From tmblue at gmail.com  Wed Feb  1 03:09:08 2017
From: tmblue at gmail.com (Tory M Blue)
Date: Tue, 31 Jan 2017 19:09:08 -0800
Subject: [squid-users] Squid error BUG 3279: HTTP reply without Date:
Message-ID: <CAEaSS0bk=hn-W7zmuo8paig7rU2LhC+=6Jm881J9cggwm-rZMg@mail.gmail.com>

I moved to a different disk today. System was down, I rsyncd the cache
directory over, including everything and the swap files etc. Squid starts
just fine but Im seeing this error once a minute or more.

Also note, I stopped the system, moved it back to the original disk and I'm
getting these errors. Nothing has changed on the original disk, so i'm
unclear what is happening here.


Any ideas?


Older version *squid*-3.3.8, on CentOS7



2017/01/31 19:06:07 kid1| WARNING: 1 swapin MD5 mismatches

2017/01/31 19:06:07 kid1| Could not parse headers from on disk object

2017/01/31 19:06:07 kid1| BUG 3279: HTTP reply without Date:

2017/01/31 19:06:07 kid1| StoreEntry->key: 1D4C79DC5531AC306DBD45596D4CC750

2017/01/31 19:06:07 kid1| StoreEntry->next: 0x7f32c7e7cb68

2017/01/31 19:06:07 kid1| StoreEntry->mem_obj: 0x7f32c3d210e0

2017/01/31 19:06:07 kid1| StoreEntry->timestamp: -1

2017/01/31 19:06:07 kid1| StoreEntry->lastref: 1485918367

2017/01/31 19:06:07 kid1| StoreEntry->expires: -1

2017/01/31 19:06:07 kid1| StoreEntry->lastmod: -1

2017/01/31 19:06:07 kid1| StoreEntry->swap_file_sz: 0

2017/01/31 19:06:07 kid1| StoreEntry->refcount: 1

2017/01/31 19:06:07 kid1| StoreEntry->flags:
CACHABLE,PRIVATE,FWD_HDR_WAIT,VALIDATED

2017/01/31 19:06:07 kid1| StoreEntry->swap_dirn: -1

2017/01/31 19:06:07 kid1| StoreEntry->swap_filen: -1

2017/01/31 19:06:07 kid1| StoreEntry->lock_count: 2

2017/01/31 19:06:07 kid1| StoreEntry->mem_status: 0

2017/01/31 19:06:07 kid1| StoreEntry->ping_status: 2

2017/01/31 19:06:07 kid1| StoreEntry->store_status: 1

2017/01/31 19:06:07 kid1| StoreEntry->swap_status: 0

2017/01/31 19:06:08 kid1| Could not parse headers from on disk object

2017/01/31 19:06:08 kid1| BUG 3279: HTTP reply without Date:

2017/01/31 19:06:08 kid1| StoreEntry->key: C495C7711862F69E6EC9FE951E20951E

2017/01/31 19:06:08 kid1| StoreEntry->next: 0x7f32ea109928

2017/01/31 19:06:08 kid1| StoreEntry->mem_obj: 0x7f32c4890060

2017/01/31 19:06:08 kid1| StoreEntry->timestamp: -1

2017/01/31 19:06:08 kid1| StoreEntry->lastref: 1485918368

2017/01/31 19:06:08 kid1| StoreEntry->expires: -1

2017/01/31 19:06:08 kid1| StoreEntry->lastmod: -1

2017/01/31 19:06:08 kid1| StoreEntry->swap_file_sz: 0

2017/01/31 19:06:08 kid1| StoreEntry->refcount: 1

2017/01/31 19:06:08 kid1| StoreEntry->flags:
CACHABLE,DISPATCHED,PRIVATE,FWD_HDR_WAIT,VALIDATED

2017/01/31 19:06:08 kid1| StoreEntry->swap_dirn: -1

2017/01/31 19:06:08 kid1| StoreEntry->swap_filen: -1

2017/01/31 19:06:08 kid1| StoreEntry->lock_count: 3

2017/01/31 19:06:08 kid1| StoreEntry->mem_status: 0

2017/01/31 19:06:08 kid1| StoreEntry->ping_status: 2

2017/01/31 19:06:08 kid1| StoreEntry->store_status: 1

2017/01/31 19:06:08 kid1| StoreEntry->swap_status: 0
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20170131/4095dffa/attachment.htm>

From squid3 at treenet.co.nz  Wed Feb  1 03:29:46 2017
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Wed, 1 Feb 2017 16:29:46 +1300
Subject: [squid-users] Squid error BUG 3279: HTTP reply without Date:
In-Reply-To: <CAEaSS0bk=hn-W7zmuo8paig7rU2LhC+=6Jm881J9cggwm-rZMg@mail.gmail.com>
References: <CAEaSS0bk=hn-W7zmuo8paig7rU2LhC+=6Jm881J9cggwm-rZMg@mail.gmail.com>
Message-ID: <eb5c8095-147d-ab50-64b3-90ad35a4f957@treenet.co.nz>

On 1/02/2017 4:09 p.m., Tory M Blue wrote:
> I moved to a different disk today. System was down, I rsyncd the cache
> directory over, including everything and the swap files etc. Squid starts
> just fine but Im seeing this error once a minute or more.
> 
> Also note, I stopped the system, moved it back to the original disk and I'm
> getting these errors. Nothing has changed on the original disk, so i'm
> unclear what is happening here.
> 
> 
> Any ideas?
> 
> 
> Older version *squid*-3.3.8, on CentOS7
> 

Squid is loading things that were provided to it without a Date header
out of the cache. It cannot handle that. Current versions will
synthesize a Date and Last-Modified if needed - which resolves most
cases of this problem. 3.3 does not have that improvement, it just
detects the bug and warns as it drops the object.

This can happen if you have refresh_pattern config options overriding
various of the caching headers. Squid can be fooled by those
override-*/ignore-* options into thinking it can re-use the object but
realy it lacks the headers to do so.

The warnings should gradually disappear as the old data in the cache
gets found and dropped. If it continues you should consider an upgrade.
You can find more up to date packages for CentOS at the links here:
<http://wiki.squid-cache.org/KnowledgeBase/CentOS>


Amos



From squid3 at treenet.co.nz  Wed Feb  1 03:37:30 2017
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Wed, 1 Feb 2017 16:37:30 +1300
Subject: [squid-users] Is it possible to modify cached object?
In-Reply-To: <201701311828.01550.Antony.Stone@squid.open.source.it>
References: <1483727739416-4681073.post@n4.nabble.com>
 <201701311744.42998.Antony.Stone@squid.open.source.it>
 <1485883154937-4681394.post@n4.nabble.com>
 <201701311828.01550.Antony.Stone@squid.open.source.it>
Message-ID: <3dca46b2-1650-cb64-fb29-79d01f1d8ca3@treenet.co.nz>

On 1/02/2017 6:28 a.m., Antony Stone wrote:
> On Tuesday 31 January 2017 at 18:19:14, boruc wrote:
> 
>> Antony Stone wrote
>>
>>> What do you get from the following:
>>> 	/etc/init.d/squid status
>>> 	/etc/init.d/squid restart
>>
>> literally nothing. I just noticed that there isn't anything with "squid" in
>> name in /etc/init.d.
> 
> Ah, right - that means the original tarball build doesn't put any init scripts 
> into place, then - I haven't built Squid like that myself for some time, so I 
> had forgotten this.
> 
> I suggest you wait until one of the developers responds to this thread and 
> reminds us how to get the appropriate init scripts into place for an Ubuntu 12 
> system - I can't help with that, sorry :(

The results seem correct. We provide some basic init scripts and such in
the tarball but do not install them. The distro packagers choose which
ones to use (or their own sytem-specific ones).

You can find the basic sysvinit script in the tarball sources at
tools/sysvinit/squid.rc
 - note that Squid-3 versions do not integrate with systemd nor upstart
very well so no .service file is provided.

This is why I usually advise people to leave the squid3 package in
place, then build the "./configure && make install" version over the
top. That leaves the distro integration but with the newer binaries.

> 
> However, you're definitely very close to your desired objective.
> 

Yes. 99.999% or so. :-)

Amos



From squid3 at treenet.co.nz  Wed Feb  1 03:42:25 2017
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Wed, 1 Feb 2017 16:42:25 +1300
Subject: [squid-users] Is it possible to modify cached object?
In-Reply-To: <1485887657504-4681396.post@n4.nabble.com>
References: <1485099242343-4681243.post@n4.nabble.com>
 <97375954-3157-92e4-f557-ea62b74a04a7@treenet.co.nz>
 <1485715924346-4681379.post@n4.nabble.com>
 <bc9151b9-df29-7c16-1460-08082e67243a@treenet.co.nz>
 <1485855330673-4681390.post@n4.nabble.com>
 <201701311052.39956.Antony.Stone@squid.open.source.it>
 <1485880095814-4681392.post@n4.nabble.com>
 <201701311744.42998.Antony.Stone@squid.open.source.it>
 <1485883154937-4681394.post@n4.nabble.com>
 <201701311828.01550.Antony.Stone@squid.open.source.it>
 <1485887657504-4681396.post@n4.nabble.com>
Message-ID: <c616d7dc-4781-2eb8-ecd8-6a976b0e1ed2@treenet.co.nz>

On 1/02/2017 7:34 a.m., boruc wrote:
> Thank you for your answers Antony.
> 
> On packages.ubuntu.com I searched for "squid3" and here's what I've found:
> 12.04LTS - 3.1.19
> 14.04LTS - 3.3.8
> 16.04LTS - 3.5.12
> 
> For now the best option would be to upgrade Ubuntu to 16.04, but I cannot do
> it now. Also Amos has written earlier: "All the newer versions should come
> pre-packaged with eCAP support with no action needed on your part." I'd like
> to know if in squid 3.5.12 eCAP is already enabled, but I cannot find it by
> myself.
> 
> I tried it on Ubuntu 14.04, by "sudo apt-get install squid3", squid 3.3.8
> was installed (and also package libecap2 (0.2.0-1ubuntu4)), the output for
> "squid3 -v" was:
> 
...
>
> So actually this squid release is already configured with "--enable-ecap"
> and all I need to do is to set "ecap_enable" in configuration file to "on"
> (and other stuff mentioned in  documentation
> <http://e-cap.org/Documentation>  ) and it'll work fine?

Correct, thats what I meant by what I wrote earlier.

Note that its the 0.2 version of eCAP though, so some things from the
1.0 version will not be exactly as documented. It should work reasonably
well for most needs.

Amos



From tmblue at gmail.com  Wed Feb  1 03:47:43 2017
From: tmblue at gmail.com (Tory M Blue)
Date: Tue, 31 Jan 2017 19:47:43 -0800
Subject: [squid-users] Squid error BUG 3279: HTTP reply without Date:
In-Reply-To: <eb5c8095-147d-ab50-64b3-90ad35a4f957@treenet.co.nz>
References: <CAEaSS0bk=hn-W7zmuo8paig7rU2LhC+=6Jm881J9cggwm-rZMg@mail.gmail.com>
 <eb5c8095-147d-ab50-64b3-90ad35a4f957@treenet.co.nz>
Message-ID: <CAEaSS0ZJUYscKyGPMwBnon+sduWb1H12okXc=T7T-Ptod87L9Q@mail.gmail.com>

On Tue, Jan 31, 2017 at 7:29 PM, Amos Jeffries <squid3 at treenet.co.nz> wrote:

> On 1/02/2017 4:09 p.m., Tory M Blue wrote:
> > I moved to a different disk today. System was down, I rsyncd the cache
> > directory over, including everything and the swap files etc. Squid starts
> > just fine but Im seeing this error once a minute or more.
> >
> > Also note, I stopped the system, moved it back to the original disk and
> I'm
> > getting these errors. Nothing has changed on the original disk, so i'm
> > unclear what is happening here.
> >
> >
> > Any ideas?
> >
> >
> > Older version *squid*-3.3.8, on CentOS7
> >
>
> Squid is loading things that were provided to it without a Date header
> out of the cache. It cannot handle that. Current versions will
> synthesize a Date and Last-Modified if needed - which resolves most
> cases of this problem. 3.3 does not have that improvement, it just
> detects the bug and warns as it drops the object.
>
> This can happen if you have refresh_pattern config options overriding
> various of the caching headers. Squid can be fooled by those
> override-*/ignore-* options into thinking it can re-use the object but
> realy it lacks the headers to do so.
>
> The warnings should gradually disappear as the old data in the cache
> gets found and dropped. If it continues you should consider an upgrade.
> You can find more up to date packages for CentOS at the links here:
> <http://wiki.squid-cache.org/KnowledgeBase/CentOS>
>
>
Thanks Amos, glad to see you are still around kicking!!

refresh_pattern . 0 0% 0 ignore-reload , that's what I have in my config.
Anything I can do here to sway it from being whiney :)

And ya i'm a bit behind for sure!!

Tory
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20170131/090643ea/attachment.htm>

From gksalil at gmail.com  Wed Feb  1 07:06:23 2017
From: gksalil at gmail.com (salil GK)
Date: Wed, 1 Feb 2017 12:36:23 +0530
Subject: [squid-users] heart beet between squid peers
Message-ID: <CAPACB-xdUX8M=rSwX1PforkGEUnhB4AMNjm4ztKYWL7r7sPRWw@mail.gmail.com>

Hi

   I am trying to build a solution with squid proxy chaining. I have a
proxy chaining through a tunnel. Every thing works fine !! But in my
framework I need to know whether the connectivity is through from squid
child to squid parent. What is the best way to know - is there any API or
utility that I can use to find it out ? I need to know in both machines
that the squid channel is active. Any help in this regard would be
appreciated.

Thanks
~S
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20170201/452c29ee/attachment.htm>

From squid3 at treenet.co.nz  Wed Feb  1 13:54:00 2017
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 2 Feb 2017 02:54:00 +1300
Subject: [squid-users] Squid error BUG 3279: HTTP reply without Date:
In-Reply-To: <CAEaSS0ZJUYscKyGPMwBnon+sduWb1H12okXc=T7T-Ptod87L9Q@mail.gmail.com>
References: <CAEaSS0bk=hn-W7zmuo8paig7rU2LhC+=6Jm881J9cggwm-rZMg@mail.gmail.com>
 <eb5c8095-147d-ab50-64b3-90ad35a4f957@treenet.co.nz>
 <CAEaSS0ZJUYscKyGPMwBnon+sduWb1H12okXc=T7T-Ptod87L9Q@mail.gmail.com>
Message-ID: <c8a82b09-2abf-912b-aa84-4285dcc0989f@treenet.co.nz>

On 1/02/2017 4:47 p.m., Tory M Blue wrote:
> 
> Thanks Amos, glad to see you are still around kicking!!
> 
> refresh_pattern . 0 0% 0 ignore-reload , that's what I have in my config.
> Anything I can do here to sway it from being whiney :)
> 
> And ya i'm a bit behind for sure!!
> 
> Tory
> 

Okay. Nothing that I'm aware of in the config.

There is a patch to silence it again, but if you are re-building you
might a well build the latest and do the upgrade.

Amos



From augustus_meyer at gmx.net  Wed Feb  1 13:51:58 2017
From: augustus_meyer at gmx.net (reinerotto)
Date: Wed, 1 Feb 2017 05:51:58 -0800 (PST)
Subject: [squid-users] heart beet between squid peers
In-Reply-To: <CAPACB-xdUX8M=rSwX1PforkGEUnhB4AMNjm4ztKYWL7r7sPRWw@mail.gmail.com>
References: <CAPACB-xdUX8M=rSwX1PforkGEUnhB4AMNjm4ztKYWL7r7sPRWw@mail.gmail.com>
Message-ID: <1485957118966-4681409.post@n4.nabble.com>

I have seen error messages in cache.log, in case conn to upstream peer
(parent proxy) was broken. However, dunno, how to do it downstream. 




--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/heart-beet-between-squid-peers-tp4681408p4681409.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From squid3 at treenet.co.nz  Wed Feb  1 14:15:47 2017
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 2 Feb 2017 03:15:47 +1300
Subject: [squid-users] irregular traffic when using proxy, not if NATed
In-Reply-To: <f14089d5-5c45-26f9-32b5-b60c7666b3ce@laposte.net>
References: <f14089d5-5c45-26f9-32b5-b60c7666b3ce@laposte.net>
Message-ID: <5a550973-4947-9df6-055d-e61772f7aff8@treenet.co.nz>

On 31/01/2017 6:10 a.m., le dahut wrote:
> Hello.
> 
> On certain upload websites, the traffic monitor shows an irregular
> traffic when uploading through squid, while uploading NATed (not using
> squid) gives a regular traffic.
> 
<snip>
> 
> 
> Can you help me find out why ?
> 

The first thing that comes to my mind is that this type of bunching in
the traffic graph is a sign of buffer bloat. Simply by using Squid you
are adding two buffers in each direction for the traffic as the client
and server connections are buffered separately. Bloat related problems
show up worst for CONNECT tunnels and uploads in Squid.

Your config shows the Squid buffers to be the default 16-64KB each
though so not exactly overly bloated on a transfer of 1 MB/s for ~30sec
straight. Could be compounding an underlying problem though.

The other thing that comes to mind is a bug in mem_node handling for
large objects. Any transfer that can fill a 1MB/s pipe for ~30sec is in
the range of objects which will start to see effects of Squid searching
memory for the next block of data to send or memory to read into.


To track down the actual reason you will need to figure out what Squid
is doing during the transfer. You do that with debug_options.

Some info on profiling Squid is mentioned at:
 <http://wiki.squid-cache.org/SquidFaq/SquidProfiling>

If you can replicate the problem on a machine that is not under a lot of
traffic load at the time you could use debug_options directive to raise
the debug level - "debug_options rotate=1 ALL,6" should give a lot of
info about whats happening.


I recommend that if you can please upgrade to the later 3.5.23 package
that is available for Ubuntu in 16.04 Xenial or later. There are quite a
few performance related issues that have been resolved in the .12
release. Not having to figure it out at all would be nice if possible.

HTH
Amos



From squid3 at treenet.co.nz  Wed Feb  1 14:46:44 2017
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 2 Feb 2017 03:46:44 +1300
Subject: [squid-users] transparent http and https filter with white-list
 only
In-Reply-To: <c25e40df-d60e-2900-d6e6-954771e56c90@gmail.com>
References: <c25e40df-d60e-2900-d6e6-954771e56c90@gmail.com>
Message-ID: <1d01efe0-83f8-2a91-c0ac-fd8ef769276f@treenet.co.nz>

On 28/01/2017 12:36 a.m., Sergey Klusov wrote:
> Hello. I'm trying to get working transparent setup allowing only certain
> domains and have problem that in order to allow https "ssl_bump splice
> allowed_domains" i have to "http_access allow all", thus allowing all
> other http traffic through. Otherwise https traffic is not allowed at all.
> 
> Here is my config:
> 

Some comments inline to improve it.

Also, what version of Squid are you using?
 I will assume that you are following the best practice advice and using
at least 3.5.19.  If not, please try to upgrade.


> =======config=======
> http_port 10.96.243.1:3128 intercept options=NO_SSLv3:NO_SSLv2
> http_port 10.96.243.1:3130 options=NO_SSLv3:NO_SSLv2

Setting SSL-related options on http_port's is not useful when they are
not doing SSL-Bump.

> https_port 10.96.243.1:3129 intercept ssl-bump
> options=ALL:NO_SSLv3:NO_SSLv2 connection-auth=off
> cert=/etc/squid/squidCA.pem
> acl localnet src 10.0.0.0/8     # RFC1918 possible internal network
> 
> acl SSL_ports port 443
> acl Safe_ports port 80          # http
> acl Safe_ports port 443         # https
> acl CONNECT method CONNECT
> 
> acl http_allow dstdomain "/etc/squid/http_allow_domains.txt"
> acl https_allow ssl::server_name "/etc/squid/https_allow_domains.txt"
> 
> sslproxy_cert_error allow all
> sslproxy_flags DONT_VERIFY_PEER

Not good. Remember this is a security protocol you are playing around with.

Both of the above lines hide critical details you need to figure out
what is going wrong. They can be useful as a spot-check (only!) to
figure out if the problem is related to cert verification or something
else. But DO NOT use them for regular traffic, not even testing traffic.

You may find that there are certain _specific_ errors that you need to
let through. Add the appropriate flags, SSL options, ACLs checks
sslproxy_cert_error lines for those as needed, dont just ignore all
possible errors like above does.

> 
> acl step1 at_step SslBump1
> ssl_bump peek step1
> ssl_bump splice https_allow
> ssl_bump terminate all
> 

Looks okay. Just to be clear you understand that:
 The above means that the TLS/SSL is spliced only if the client SNI
contains a domain in your whitelist.
 All other traffic will be terminated ... maybe with an HTTP error page.


> cache deny all
> 
> http_access deny !Safe_ports
> http_access deny CONNECT !SSL_ports
> http_access allow localhost manager
> http_access deny manager
> 
> http_access allow all http_allow
> http_access allow all https_allow

The ssl::server_name ACL will not work outside of the ssl_bump
directive. Delete the above line.


Also, I am not seeing is any line which permits the raw-IP CONNECT
message which your Squid processes first to decide whether ssl_bump will
be applied to the intercepted TCP connections.

 That is why the "allow all" makes things "work". It lets those CONNECT
request through.

You can read the details about how bumping happens at
<http://wiki.squid-cache.org/Features/SslPeekAndSplice#Processing_steps>
 The CONNECT request mentioned in step 1.ii is your problem.

To fix it in a very targeted way add these lines (mind the wrap sorry):

 acl rawIP dstdom_regex
^(([0-9]+\.[0-9]+\.[0-9]+\.[0-9]+)|(\[([0-9a-f]+)?:([0-9a-f:]+)?:([0-9a-f]+|0-9\.]+)?\])):443$

 acl bumpPort myportname 10.96.243.1:3129

 http_access allow CONNECT bumpPort rawIP


> http_access deny all
> 
> always_direct allow all
> 

That always_direct line is not useful. Remove it.

HTH
Amos


From erdosain9 at gmail.com  Wed Feb  1 15:41:04 2017
From: erdosain9 at gmail.com (erdosain9)
Date: Wed, 1 Feb 2017 07:41:04 -0800 (PST)
Subject: [squid-users] Antivirus for squid
In-Reply-To: <VI1PR0401MB2685AC837199F2249A7B13938F740@VI1PR0401MB2685.eurprd04.prod.outlook.com>
References: <1485367389664-4681323.post@n4.nabble.com>
 <3dbe39e9-87ea-8b63-73a1-63c30d3f86ae@gmail.com>
 <VI1PR0401MB2685AC837199F2249A7B13938F740@VI1PR0401MB2685.eurprd04.prod.outlook.com>
Message-ID: <1485963664148-4681413.post@n4.nabble.com>

Hi, again.
Well i installed squidclamav, c-icap, and clamav; and its working all fine,
but... the download is too slow, the download of a file. There is a way to
accelerate this?? 
Also, when the file its a virus, the message "this is a virus bla bla", go
fast... i mean the slow download its for all the other files that dosent
have a virus...

*This is squid.conf
*
# c-icap integration
icap_enable on
icap_send_client_ip on
icap_send_client_username on
icap_client_username_header X-Authenticated-User
icap_preview_enable on 
icap_preview_size 1024 
icap_service service_req reqmod_precache bypass=1
icap://127.0.0.1:1344/squidclamav
adaptation_access service_req allow all
icap_service service_resp respmod_precache bypass=1
icap://127.0.0.1:1344/squidclamav
adaptation_access service_resp allow all
# end integration


*c-icap.conf
*
PidFile /var/run/c-icap.pid
CommandsSocket /var/run/c-icap.ctl
StartServers 1
MaxServers 20
MaxRequestsPerChild  100
Port 1344 
ServerAdmin yourname at yourdomain
TmpDir /tmp
MaxMemObject 131072
DebugLevel 0 
ModulesDir /usr/local/c-icap/lib/c_icap/ 
ServicesDir /usr/local/c-icap/lib/c_icap/ 
LoadMagicFile /usr/local/etc/c-icap.magic

acl localhost src 127.0.0.1/255.255.255.255
acl PERMIT_REQUESTS type REQMOD RESPMOD
icap_access allow localhost PERMIT_REQUESTS
icap_access deny all

ServerLog /var/log/c-icap/server.log
AccessLog /var/log/c-icap/access.log 

Service squidclamav squidclamav.so


*CLAMD.CONF*
LogFile /var/log/clamd.scan
PidFile /var/run/clamd.scan/clamd.pid
TemporaryDirectory /var/tmp
DatabaseDirectory /var/lib/clamav
LocalSocket /var/run/clamd.scan/clamd.sock
TCPSocket 3310
TCPAddr 127.0.0.1
User clamscan


*SQUIDCLAMAV.CONF
*
maxsize 5000000
redirect http://squid.espaciomemoria.lan/cgi-bin/clwarn.cgi.en_EN
clamd_ip 127.0.0.1
clamd_port 3310
trust_cache 0 
timeout 1
logredir 1
dnslookup 0
safebrowsing 0

abortcontent ^video\/x-flv$
abortcontent ^video\/mp4$
# White list some sites

Somebody can give me a hand with this???
Thanks to all.



--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/Antivirus-for-squid-tp4681323p4681413.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From squid3 at treenet.co.nz  Wed Feb  1 15:48:34 2017
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 2 Feb 2017 04:48:34 +1300
Subject: [squid-users] Not all html objects are being cached
In-Reply-To: <738bb2b4-6455-ebc7-26cf-7960f5be49ba@gmail.com>
References: <1485273173646-4681293.post@n4.nabble.com>
 <201701271305.41490.Antony.Stone@squid.open.source.it>
 <e81749cc-d91b-56b7-2476-6a4ca557b094@gmail.com>
 <201701271325.18195.Antony.Stone@squid.open.source.it>
 <738bb2b4-6455-ebc7-26cf-7960f5be49ba@gmail.com>
Message-ID: <e50c2f62-f998-c601-6353-91cc05c1effa@treenet.co.nz>

On 28/01/2017 1:35 a.m., Yuri wrote:
> 
> I just want to have a choice and an opportunity to say - "F*ck you, man,
> I'm the System Administrator".

Does that go down well in parties or something?

> 
> If you do not want to violate the RFC - remove violations HTTP at all.
> If you remember, this mode is now enabled by default.

That mode does not mean what you seem to think it means.

It means that *some* *specific* things which are known not to cause much
damage are allowed which violate HTTP _a little bit_ when it helps the
traffic work better. Most things it does is enabling Squid to detect and
talk with broken software that are themselves not quite following HTTP
right.
 For example, a client forgetting to %20 some whitespace inside a URL.

> 
> You do not have to teach me that I use. I - an administrator and wish to
> be able to select tools. And do not be in a situation where the choice
> is made for me.
> 


Have you tried starting regular conversations with your friends and
family with the words "F*k you, man, I'm the System Administrator" so
they know that your way is always right no matter what. Then proceeding
to say everything else in the conversation at the loudest volume your
mouth can produce while injecting weird words randomly into each
sentence? just because you were created with those abilities you might
as well try using them. It definitely will make conversations short and
efficient (hmm.. just like 100% caching makes HTTP 'quick').


Anyhow, my point is all languages have rules and protocols of behaviour
that have to be followed for the sentences/messages to be called
"speaking" that language. If you don't follow those rules you are simply
not speaking that language. You might be speaking some other language or
just being a weirdo - either way you are not speaking that language.

HTTP is as much a language as any spoken one. It is just for Internet
software to 'talk' to each other. By not following its rules you are ...
well ... not using HTTP.

What you keep saying about how you/admin "must" be allowed to violate
HTTP just because you are administrator and want to. That makes as much
sense as being proud about shouting at everyone you talk to in real
life. It's dumb, on a scale that demonstrates one is not worthy of the
privilege of being a sysadmin and can lead to early retirement in a
small padded cell.


> 
>>
>>> Antonio, you've seen at least once, so I complained about the
>>> consequences of my own actions?
>> You seem to continually complain that people are recommending not to
>> try going
>> against standards, or trying to defeat the anti-caching directives on
>> websites
>> you find.
>>
>> It's your choice to try doing that; people are saying "but if you do
>> that, bad
>> things will happen, or things will break, or it just won't work the
>> way you
>> want it to", and then you say "but I don't like having to follow the
>> rules".
>>
>> That's what I meant about complaining about the consequences of your
>> actions.
> It is my right and my choice. Personally, I do not complain of the
> consequences, having enough tools to solve any problem.
> 

Hahahahaha "not complain about the consequences", ROFLMAO.
Thanks dude, I needed a good laugh today.

Amos



From yvoinov at gmail.com  Wed Feb  1 15:51:32 2017
From: yvoinov at gmail.com (Yuri Voinov)
Date: Wed, 1 Feb 2017 21:51:32 +0600
Subject: [squid-users] Antivirus for squid
In-Reply-To: <1485963664148-4681413.post@n4.nabble.com>
References: <1485367389664-4681323.post@n4.nabble.com>
 <3dbe39e9-87ea-8b63-73a1-63c30d3f86ae@gmail.com>
 <VI1PR0401MB2685AC837199F2249A7B13938F740@VI1PR0401MB2685.eurprd04.prod.outlook.com>
 <1485963664148-4681413.post@n4.nabble.com>
Message-ID: <fef780ab-2b98-a11c-e51b-bb11d5aa559d@gmail.com>

Squid's wiki article contains all required points about performance and
tuning.


01.02.2017 21:41, erdosain9 ?????:
> Hi, again.
> Well i installed squidclamav, c-icap, and clamav; and its working all fine,
> but... the download is too slow, the download of a file. There is a way to
> accelerate this?? 
What do you mean "too slow"? Exact data, pls. Subjective and relative
adjectives, do not say anything of substance.

I mean, i.e.: "Before I've installed clamav, download speed was 1
terabit per second for file http://bwah-bwah.com/bwahbwahbwah.tar.gz.
After - only 10 megabits. It seems too slow".
> Also, when the file its a virus, the message "this is a virus bla bla", go
This is different procedure, which is not executed by squid itself.
> fast... i mean the slow download its for all the other files that dosent
> have a virus...
>
> *This is squid.conf
> *
> # c-icap integration
> icap_enable on
> icap_send_client_ip on
> icap_send_client_username on
> icap_client_username_header X-Authenticated-User
> icap_preview_enable on 
> icap_preview_size 1024 
> icap_service service_req reqmod_precache bypass=1
> icap://127.0.0.1:1344/squidclamav
> adaptation_access service_req allow all
> icap_service service_resp respmod_precache bypass=1
> icap://127.0.0.1:1344/squidclamav
> adaptation_access service_resp allow all
> # end integration
>
>
> *c-icap.conf
> *
> PidFile /var/run/c-icap.pid
> CommandsSocket /var/run/c-icap.ctl
> StartServers 1
> MaxServers 20
> MaxRequestsPerChild  100
> Port 1344 
> ServerAdmin yourname at yourdomain
> TmpDir /tmp
> MaxMemObject 131072
> DebugLevel 0 
> ModulesDir /usr/local/c-icap/lib/c_icap/ 
> ServicesDir /usr/local/c-icap/lib/c_icap/ 
> LoadMagicFile /usr/local/etc/c-icap.magic
>
> acl localhost src 127.0.0.1/255.255.255.255
> acl PERMIT_REQUESTS type REQMOD RESPMOD
> icap_access allow localhost PERMIT_REQUESTS
> icap_access deny all
>
> ServerLog /var/log/c-icap/server.log
> AccessLog /var/log/c-icap/access.log 
>
> Service squidclamav squidclamav.so
>
>
> *CLAMD.CONF*
> LogFile /var/log/clamd.scan
> PidFile /var/run/clamd.scan/clamd.pid
> TemporaryDirectory /var/tmp
> DatabaseDirectory /var/lib/clamav
> LocalSocket /var/run/clamd.scan/clamd.sock
> TCPSocket 3310
> TCPAddr 127.0.0.1
> User clamscan
>
>
> *SQUIDCLAMAV.CONF
> *
> maxsize 5000000
> redirect http://squid.espaciomemoria.lan/cgi-bin/clwarn.cgi.en_EN
> clamd_ip 127.0.0.1
> clamd_port 3310
> trust_cache 0 
> timeout 1
> logredir 1
> dnslookup 0
> safebrowsing 0
>
> abortcontent ^video\/x-flv$
> abortcontent ^video\/mp4$
> # White list some sites
>
> Somebody can give me a hand with this???
> Thanks to all.
Thelepathy on vacation. To give your hand, it is require to have root
access to your server to make performance diagnostics during "slow
downloads". But you always can do this yourself. Pieces of configs is
not enough to diagnostics, and, therefore, for tuning.
>
>
>
> --
> View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/Antivirus-for-squid-tp4681323p4681413.html
> Sent from the Squid - Users mailing list archive at Nabble.com.
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users

-- 
Bugs to the Future
-------------- next part --------------
A non-text attachment was scrubbed...
Name: 0x613DEC46.asc
Type: application/pgp-keys
Size: 2437 bytes
Desc: not available
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20170201/8c3c288b/attachment.key>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: signature.asc
Type: application/pgp-signature
Size: 473 bytes
Desc: OpenPGP digital signature
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20170201/8c3c288b/attachment.sig>

From yvoinov at gmail.com  Wed Feb  1 15:57:14 2017
From: yvoinov at gmail.com (Yuri Voinov)
Date: Wed, 1 Feb 2017 21:57:14 +0600
Subject: [squid-users] Not all html objects are being cached
In-Reply-To: <e50c2f62-f998-c601-6353-91cc05c1effa@treenet.co.nz>
References: <1485273173646-4681293.post@n4.nabble.com>
 <201701271305.41490.Antony.Stone@squid.open.source.it>
 <e81749cc-d91b-56b7-2476-6a4ca557b094@gmail.com>
 <201701271325.18195.Antony.Stone@squid.open.source.it>
 <738bb2b4-6455-ebc7-26cf-7960f5be49ba@gmail.com>
 <e50c2f62-f998-c601-6353-91cc05c1effa@treenet.co.nz>
Message-ID: <113d3481-bb9c-6c7d-434b-7f5af62d9aee@gmail.com>

You'r welcome.

I do not understand what the hell you have clung to me. I have my own
point of view on the problem. Tell tales of the guy who started this
thread. I know the developer's position.

So, let's stop useless discussion. This is wasted time only.

01.02.2017 21:48, Amos Jeffries ?????:
> On 28/01/2017 1:35 a.m., Yuri wrote:
>> I just want to have a choice and an opportunity to say - "F*ck you, man,
>> I'm the System Administrator".
> Does that go down well in parties or something?
>
>> If you do not want to violate the RFC - remove violations HTTP at all.
>> If you remember, this mode is now enabled by default.
> That mode does not mean what you seem to think it means.
>
> It means that *some* *specific* things which are known not to cause much
> damage are allowed which violate HTTP _a little bit_ when it helps the
> traffic work better. Most things it does is enabling Squid to detect and
> talk with broken software that are themselves not quite following HTTP
> right.
>  For example, a client forgetting to %20 some whitespace inside a URL.
>
>> You do not have to teach me that I use. I - an administrator and wish to
>> be able to select tools. And do not be in a situation where the choice
>> is made for me.
>>
>
> Have you tried starting regular conversations with your friends and
> family with the words "F*k you, man, I'm the System Administrator" so
> they know that your way is always right no matter what. Then proceeding
> to say everything else in the conversation at the loudest volume your
> mouth can produce while injecting weird words randomly into each
> sentence? just because you were created with those abilities you might
> as well try using them. It definitely will make conversations short and
> efficient (hmm.. just like 100% caching makes HTTP 'quick').
>
>
> Anyhow, my point is all languages have rules and protocols of behaviour
> that have to be followed for the sentences/messages to be called
> "speaking" that language. If you don't follow those rules you are simply
> not speaking that language. You might be speaking some other language or
> just being a weirdo - either way you are not speaking that language.
>
> HTTP is as much a language as any spoken one. It is just for Internet
> software to 'talk' to each other. By not following its rules you are ...
> well ... not using HTTP.
>
> What you keep saying about how you/admin "must" be allowed to violate
> HTTP just because you are administrator and want to. That makes as much
> sense as being proud about shouting at everyone you talk to in real
> life. It's dumb, on a scale that demonstrates one is not worthy of the
> privilege of being a sysadmin and can lead to early retirement in a
> small padded cell.
>
>
>>>> Antonio, you've seen at least once, so I complained about the
>>>> consequences of my own actions?
>>> You seem to continually complain that people are recommending not to
>>> try going
>>> against standards, or trying to defeat the anti-caching directives on
>>> websites
>>> you find.
>>>
>>> It's your choice to try doing that; people are saying "but if you do
>>> that, bad
>>> things will happen, or things will break, or it just won't work the
>>> way you
>>> want it to", and then you say "but I don't like having to follow the
>>> rules".
>>>
>>> That's what I meant about complaining about the consequences of your
>>> actions.
>> It is my right and my choice. Personally, I do not complain of the
>> consequences, having enough tools to solve any problem.
>>
> Hahahahaha "not complain about the consequences", ROFLMAO.
> Thanks dude, I needed a good laugh today.
>
> Amos
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users

-- 
Bugs to the Future
-------------- next part --------------
A non-text attachment was scrubbed...
Name: 0x613DEC46.asc
Type: application/pgp-keys
Size: 2437 bytes
Desc: not available
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20170201/4d36ec21/attachment.key>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: signature.asc
Type: application/pgp-signature
Size: 473 bytes
Desc: OpenPGP digital signature
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20170201/4d36ec21/attachment.sig>

From eliezer at ngtech.co.il  Wed Feb  1 16:14:31 2017
From: eliezer at ngtech.co.il (Eliezer  Croitoru)
Date: Wed, 1 Feb 2017 18:14:31 +0200
Subject: [squid-users] Antivirus for squid
In-Reply-To: <fef780ab-2b98-a11c-e51b-bb11d5aa559d@gmail.com>
References: <1485367389664-4681323.post@n4.nabble.com>
 <3dbe39e9-87ea-8b63-73a1-63c30d3f86ae@gmail.com>
 <VI1PR0401MB2685AC837199F2249A7B13938F740@VI1PR0401MB2685.eurprd04.prod.outlook.com>
 <1485963664148-4681413.post@n4.nabble.com>
 <fef780ab-2b98-a11c-e51b-bb11d5aa559d@gmail.com>
Message-ID: <06aa01d27ca6$4540dda0$cfc298e0$@ngtech.co.il>

Hey Yuri,

What wiki article?

Thanks,
Eliezer

----
Eliezer Croitoru
Linux System Administrator
Mobile: +972-5-28704261
Email: eliezer at ngtech.co.il


-----Original Message-----
From: squid-users [mailto:squid-users-bounces at lists.squid-cache.org] On Behalf Of Yuri Voinov
Sent: Wednesday, February 1, 2017 5:52 PM
To: squid-users at lists.squid-cache.org
Subject: Re: [squid-users] Antivirus for squid

Squid's wiki article contains all required points about performance and tuning.


01.02.2017 21:41, erdosain9 ?????:
> Hi, again.
> Well i installed squidclamav, c-icap, and clamav; and its working all 
> fine, but... the download is too slow, the download of a file. There 
> is a way to accelerate this??
What do you mean "too slow"? Exact data, pls. Subjective and relative adjectives, do not say anything of substance.

I mean, i.e.: "Before I've installed clamav, download speed was 1 terabit per second for file http://bwah-bwah.com/bwahbwahbwah.tar.gz.
After - only 10 megabits. It seems too slow".
> Also, when the file its a virus, the message "this is a virus bla 
> bla", go
This is different procedure, which is not executed by squid itself.
> fast... i mean the slow download its for all the other files that 
> dosent have a virus...
>
> *This is squid.conf
> *
> # c-icap integration
> icap_enable on
> icap_send_client_ip on
> icap_send_client_username on
> icap_client_username_header X-Authenticated-User icap_preview_enable 
> on icap_preview_size 1024 icap_service service_req reqmod_precache 
> bypass=1 icap://127.0.0.1:1344/squidclamav adaptation_access 
> service_req allow all icap_service service_resp respmod_precache 
> bypass=1 icap://127.0.0.1:1344/squidclamav adaptation_access 
> service_resp allow all # end integration
>
>
> *c-icap.conf
> *
> PidFile /var/run/c-icap.pid
> CommandsSocket /var/run/c-icap.ctl
> StartServers 1
> MaxServers 20
> MaxRequestsPerChild  100
> Port 1344
> ServerAdmin yourname at yourdomain
> TmpDir /tmp
> MaxMemObject 131072
> DebugLevel 0
> ModulesDir /usr/local/c-icap/lib/c_icap/ ServicesDir 
> /usr/local/c-icap/lib/c_icap/ LoadMagicFile 
> /usr/local/etc/c-icap.magic
>
> acl localhost src 127.0.0.1/255.255.255.255 acl PERMIT_REQUESTS type 
> REQMOD RESPMOD icap_access allow localhost PERMIT_REQUESTS icap_access 
> deny all
>
> ServerLog /var/log/c-icap/server.log
> AccessLog /var/log/c-icap/access.log
>
> Service squidclamav squidclamav.so
>
>
> *CLAMD.CONF*
> LogFile /var/log/clamd.scan
> PidFile /var/run/clamd.scan/clamd.pid
> TemporaryDirectory /var/tmp
> DatabaseDirectory /var/lib/clamav
> LocalSocket /var/run/clamd.scan/clamd.sock TCPSocket 3310 TCPAddr 
> 127.0.0.1 User clamscan
>
>
> *SQUIDCLAMAV.CONF
> *
> maxsize 5000000
> redirect http://squid.espaciomemoria.lan/cgi-bin/clwarn.cgi.en_EN
> clamd_ip 127.0.0.1
> clamd_port 3310
> trust_cache 0
> timeout 1
> logredir 1
> dnslookup 0
> safebrowsing 0
>
> abortcontent ^video\/x-flv$
> abortcontent ^video\/mp4$
> # White list some sites
>
> Somebody can give me a hand with this???
> Thanks to all.
Thelepathy on vacation. To give your hand, it is require to have root access to your server to make performance diagnostics during "slow downloads". But you always can do this yourself. Pieces of configs is not enough to diagnostics, and, therefore, for tuning.
>
>
>
> --
> View this message in context: 
> http://squid-web-proxy-cache.1019090.n4.nabble.com/Antivirus-for-squid
> -tp4681323p4681413.html Sent from the Squid - Users mailing list 
> archive at Nabble.com.
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users

--
Bugs to the Future



From yvoinov at gmail.com  Wed Feb  1 16:15:41 2017
From: yvoinov at gmail.com (Yuri Voinov)
Date: Wed, 1 Feb 2017 22:15:41 +0600
Subject: [squid-users] Antivirus for squid
In-Reply-To: <06aa01d27ca6$4540dda0$cfc298e0$@ngtech.co.il>
References: <1485367389664-4681323.post@n4.nabble.com>
 <3dbe39e9-87ea-8b63-73a1-63c30d3f86ae@gmail.com>
 <VI1PR0401MB2685AC837199F2249A7B13938F740@VI1PR0401MB2685.eurprd04.prod.outlook.com>
 <1485963664148-4681413.post@n4.nabble.com>
 <fef780ab-2b98-a11c-e51b-bb11d5aa559d@gmail.com>
 <06aa01d27ca6$4540dda0$cfc298e0$@ngtech.co.il>
Message-ID: <bf3b5639-c19f-2bb1-ebdf-76a8839ccebe@gmail.com>

http://wiki.squid-cache.org/ConfigExamples/ContentAdaptation/C-ICAP


01.02.2017 22:14, Eliezer Croitoru ?????:
> Hey Yuri,
>
> What wiki article?
>
> Thanks,
> Eliezer
>
> ----
> Eliezer Croitoru
> Linux System Administrator
> Mobile: +972-5-28704261
> Email: eliezer at ngtech.co.il
>
>
> -----Original Message-----
> From: squid-users [mailto:squid-users-bounces at lists.squid-cache.org] On Behalf Of Yuri Voinov
> Sent: Wednesday, February 1, 2017 5:52 PM
> To: squid-users at lists.squid-cache.org
> Subject: Re: [squid-users] Antivirus for squid
>
> Squid's wiki article contains all required points about performance and tuning.
>
>
> 01.02.2017 21:41, erdosain9 ?????:
>> Hi, again.
>> Well i installed squidclamav, c-icap, and clamav; and its working all 
>> fine, but... the download is too slow, the download of a file. There 
>> is a way to accelerate this??
> What do you mean "too slow"? Exact data, pls. Subjective and relative adjectives, do not say anything of substance.
>
> I mean, i.e.: "Before I've installed clamav, download speed was 1 terabit per second for file http://bwah-bwah.com/bwahbwahbwah.tar.gz.
> After - only 10 megabits. It seems too slow".
>> Also, when the file its a virus, the message "this is a virus bla 
>> bla", go
> This is different procedure, which is not executed by squid itself.
>> fast... i mean the slow download its for all the other files that 
>> dosent have a virus...
>>
>> *This is squid.conf
>> *
>> # c-icap integration
>> icap_enable on
>> icap_send_client_ip on
>> icap_send_client_username on
>> icap_client_username_header X-Authenticated-User icap_preview_enable 
>> on icap_preview_size 1024 icap_service service_req reqmod_precache 
>> bypass=1 icap://127.0.0.1:1344/squidclamav adaptation_access 
>> service_req allow all icap_service service_resp respmod_precache 
>> bypass=1 icap://127.0.0.1:1344/squidclamav adaptation_access 
>> service_resp allow all # end integration
>>
>>
>> *c-icap.conf
>> *
>> PidFile /var/run/c-icap.pid
>> CommandsSocket /var/run/c-icap.ctl
>> StartServers 1
>> MaxServers 20
>> MaxRequestsPerChild  100
>> Port 1344
>> ServerAdmin yourname at yourdomain
>> TmpDir /tmp
>> MaxMemObject 131072
>> DebugLevel 0
>> ModulesDir /usr/local/c-icap/lib/c_icap/ ServicesDir 
>> /usr/local/c-icap/lib/c_icap/ LoadMagicFile 
>> /usr/local/etc/c-icap.magic
>>
>> acl localhost src 127.0.0.1/255.255.255.255 acl PERMIT_REQUESTS type 
>> REQMOD RESPMOD icap_access allow localhost PERMIT_REQUESTS icap_access 
>> deny all
>>
>> ServerLog /var/log/c-icap/server.log
>> AccessLog /var/log/c-icap/access.log
>>
>> Service squidclamav squidclamav.so
>>
>>
>> *CLAMD.CONF*
>> LogFile /var/log/clamd.scan
>> PidFile /var/run/clamd.scan/clamd.pid
>> TemporaryDirectory /var/tmp
>> DatabaseDirectory /var/lib/clamav
>> LocalSocket /var/run/clamd.scan/clamd.sock TCPSocket 3310 TCPAddr 
>> 127.0.0.1 User clamscan
>>
>>
>> *SQUIDCLAMAV.CONF
>> *
>> maxsize 5000000
>> redirect http://squid.espaciomemoria.lan/cgi-bin/clwarn.cgi.en_EN
>> clamd_ip 127.0.0.1
>> clamd_port 3310
>> trust_cache 0
>> timeout 1
>> logredir 1
>> dnslookup 0
>> safebrowsing 0
>>
>> abortcontent ^video\/x-flv$
>> abortcontent ^video\/mp4$
>> # White list some sites
>>
>> Somebody can give me a hand with this???
>> Thanks to all.
> Thelepathy on vacation. To give your hand, it is require to have root access to your server to make performance diagnostics during "slow downloads". But you always can do this yourself. Pieces of configs is not enough to diagnostics, and, therefore, for tuning.
>>
>>
>> --
>> View this message in context: 
>> http://squid-web-proxy-cache.1019090.n4.nabble.com/Antivirus-for-squid
>> -tp4681323p4681413.html Sent from the Squid - Users mailing list 
>> archive at Nabble.com.
>> _______________________________________________
>> squid-users mailing list
>> squid-users at lists.squid-cache.org
>> http://lists.squid-cache.org/listinfo/squid-users
> --
> Bugs to the Future
>

-- 
Bugs to the Future
-------------- next part --------------
A non-text attachment was scrubbed...
Name: 0x613DEC46.asc
Type: application/pgp-keys
Size: 2437 bytes
Desc: not available
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20170201/eb934ea3/attachment.key>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: signature.asc
Type: application/pgp-signature
Size: 473 bytes
Desc: OpenPGP digital signature
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20170201/eb934ea3/attachment.sig>

From gksalil at gmail.com  Wed Feb  1 16:19:12 2017
From: gksalil at gmail.com (salil GK)
Date: Wed, 1 Feb 2017 21:49:12 +0530
Subject: [squid-users] heart beet between squid peers
In-Reply-To: <1485957118966-4681409.post@n4.nabble.com>
References: <CAPACB-xdUX8M=rSwX1PforkGEUnhB4AMNjm4ztKYWL7r7sPRWw@mail.gmail.com>
 <1485957118966-4681409.post@n4.nabble.com>
Message-ID: <CAPACB-yBodguCw3QoYvZiySGW7SbduP+oKAHZuszzYJivLcJhw@mail.gmail.com>

Thanks reinerotto ..

But log file parsing to understand the state would be the last thing I
would like to do !!!!

It would be really great if some utility that will tell me if the heartbeat
is exchanged successfully !! or may be some result file where I can check
how the health !!!

Thanks
~S

On 1 February 2017 at 19:21, reinerotto <augustus_meyer at gmx.net> wrote:

> I have seen error messages in cache.log, in case conn to upstream peer
> (parent proxy) was broken. However, dunno, how to do it downstream.
>
>
>
>
> --
> View this message in context: http://squid-web-proxy-cache.
> 1019090.n4.nabble.com/heart-beet-between-squid-peers-
> tp4681408p4681409.html
> Sent from the Squid - Users mailing list archive at Nabble.com.
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20170201/665fc036/attachment.htm>

From squid3 at treenet.co.nz  Wed Feb  1 16:19:20 2017
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 2 Feb 2017 05:19:20 +1300
Subject: [squid-users] squid reverse proxy (accelerator) for MS Exchange
 OWA
In-Reply-To: <1426807003.2339496.1485505865203@mail.yahoo.com>
References: <1216522442.160455.1484870639800.ref@mail.yahoo.com>
 <1216522442.160455.1484870639800@mail.yahoo.com>
 <cdea0afa-c2a2-23c4-dbac-553a565e83f7@treenet.co.nz>
 <690032068.431187.1484905491833@mail.yahoo.com>
 <5517395f-eaa4-0b3a-34e9-66ea8d0122a1@treenet.co.nz>
 <1374872097.4095106.1485244925609@mail.yahoo.com>
 <94978436-88e5-fc84-9f3e-b8f1757402a5@measurement-factory.com>
 <2010900066.469374.1485330325639@mail.yahoo.com>
 <0f2d2e08-bbd1-656a-a83b-5bb159983cbd@measurement-factory.com>
 <757248455.1520252.1485425773189@mail.yahoo.com>
 <e13696a8-4540-bdd8-0634-902b737b82e4@measurement-factory.com>
 <1426807003.2339496.1485505865203@mail.yahoo.com>
Message-ID: <3a990a52-6b57-a4b3-c7d0-9d683828faea@treenet.co.nz>

On 27/01/2017 9:31 p.m., Vieri wrote:
> 
> 
> 
> 
> ----- Original Message ----- From: Alex Rousskov
> <rousskov at measurement-factory.com>
> 
>>> It's interesting to note that the following actually DOES give
>>> more information (unsupported
> 
>>> protocol):>
>> * If the server sent nothing, then Curl gave you potentially
>> incorrect information (i.e., Curl is just _guessing_ what went
>> wrong).
> 
> 
> I never tried telling Squid to use TLS 1.1 ONLY so I never got to see
> Squid's log when using that protocol. I'm supposing I would have seen
> the same thing in Squid as I've seen it with CURL. So I'm sure Squid
> would log useful information for the sys admin but... (see below).
> 
>>> Maybe if Squid gets an SSL negotiation error with no apparent
>>> reason then it might need to retry connecting by being more
>>> explicit, just like in my cURL and openssl binary examples
>>> above.
>> 
>> Sorry, I do not know what "retry connecting by being more
>> explicit" means. AFAICT, neither Curl nor s_client tried
>> reconnecting in your examples. Also, an appropriate default for a
>> command-line client is often a bad default for a proxy. It is
>> complicated.
> 
> 
> Let me rephrase my point but please keep in mind that I have no idea
> how Squid actually behaves.

Let me pause you right there.  What you describe is essentially how the
TLS protocol handshake is actually performed.

> Simply put, when Squid tries to connect
> for the first time, it will probably (I'm guessing here) try the most
> secure protcol known today (ie. TSL 1.2), or let OpenSSL decide by
> default which is probably the same.

That is exactly what happens.

> In my case, the server replies
> nothing. That would be like running:
> 
> # curl -k -v https://10.215.144.21 or # openssl s_client -connect
> 10.215.144.21:443
> 
> They give me the same information as Squid's log... almost nothing.
> 
> So my point is, if that first connection fails and gives me nothing
> for TLS 1.2 (or whatever the default is), two things can happen:
> either the remote site is failing or it isn't supporting the
> protocol. Why not "try again" but this time by being more specific?

Several reasons.

Reason #1 is that the TLS protocol is a security protocol for securing a
single 'hop' (just one TCP connection). So ideally TLS details would not
be remembered at all, it's a dangerous thing in security to remember
details in the middleware.


Reason #2 is that Squid has passed on the 'terminate' signal to the
client (curl).

As far as Squid is concerned, there is no "again" connection. There is a
connection, which fails. The end.

There is a connection. Which fails. The end.

There is a connection. Which fails. The end.

 ... and so on. These connections may be from the same client, or
different ones. May be to same or different servers. They are
independent of each other and only TCP at this point.

The TLS setup/handshake parts never get far enough for there to be
anything to remember. Except that TCP connection failed.

Well, Squid does remember that and tries a different IP next time. Until
it runs out of IPs, then it resets its bad-ID memory with a new DNS
lookup and the cycle begins again.


NP: if you are interested you can see the GOOD/BAD flags on IPs in the
ipcache cache manager report (squidclient mgr:ipcache).


> It would be like doing something like this:
> 
> # openssl s_client -connect 10.215.144.21:443 || openssl s_client
> -connect 10.215.144.21:443 -tls1_1 || openssl s_client -connect
> 10.215.144.21:443 -tls1
> 

Which brings us to reason #3; downgrade attacks.

You may have heard of the POODLE attack. It is basically a middleware
(like Squid) forcing the other endpoint(s) to re-try with lower TLS
versions until a version is reached and cipher selected that the
attacker can decrypt or steal the keys from.

Squid (mostly) avoids the whole class of vulnerabilities by leaving the
fallback decisions to the client whenever it can.

Since the curl command only did the one request, that is all that
happened. No retry.


Amos


From yvoinov at gmail.com  Wed Feb  1 16:29:38 2017
From: yvoinov at gmail.com (Yuri Voinov)
Date: Wed, 1 Feb 2017 22:29:38 +0600
Subject: [squid-users] squid reverse proxy (accelerator) for MS Exchange
 OWA
In-Reply-To: <3a990a52-6b57-a4b3-c7d0-9d683828faea@treenet.co.nz>
References: <1216522442.160455.1484870639800.ref@mail.yahoo.com>
 <1216522442.160455.1484870639800@mail.yahoo.com>
 <cdea0afa-c2a2-23c4-dbac-553a565e83f7@treenet.co.nz>
 <690032068.431187.1484905491833@mail.yahoo.com>
 <5517395f-eaa4-0b3a-34e9-66ea8d0122a1@treenet.co.nz>
 <1374872097.4095106.1485244925609@mail.yahoo.com>
 <94978436-88e5-fc84-9f3e-b8f1757402a5@measurement-factory.com>
 <2010900066.469374.1485330325639@mail.yahoo.com>
 <0f2d2e08-bbd1-656a-a83b-5bb159983cbd@measurement-factory.com>
 <757248455.1520252.1485425773189@mail.yahoo.com>
 <e13696a8-4540-bdd8-0634-902b737b82e4@measurement-factory.com>
 <1426807003.2339496.1485505865203@mail.yahoo.com>
 <3a990a52-6b57-a4b3-c7d0-9d683828faea@treenet.co.nz>
Message-ID: <93a965b1-4fd6-7ec6-e594-419ee89db615@gmail.com>

I'm sorry to interrupt, gentlemen - but Microsoft does not use
certificate pinning in OWA?


01.02.2017 22:19, Amos Jeffries ?????:
> On 27/01/2017 9:31 p.m., Vieri wrote:
>>
>>
>>
>> ----- Original Message ----- From: Alex Rousskov
>> <rousskov at measurement-factory.com>
>>
>>>> It's interesting to note that the following actually DOES give
>>>> more information (unsupported
>>>> protocol):>
>>> * If the server sent nothing, then Curl gave you potentially
>>> incorrect information (i.e., Curl is just _guessing_ what went
>>> wrong).
>>
>> I never tried telling Squid to use TLS 1.1 ONLY so I never got to see
>> Squid's log when using that protocol. I'm supposing I would have seen
>> the same thing in Squid as I've seen it with CURL. So I'm sure Squid
>> would log useful information for the sys admin but... (see below).
>>
>>>> Maybe if Squid gets an SSL negotiation error with no apparent
>>>> reason then it might need to retry connecting by being more
>>>> explicit, just like in my cURL and openssl binary examples
>>>> above.
>>> Sorry, I do not know what "retry connecting by being more
>>> explicit" means. AFAICT, neither Curl nor s_client tried
>>> reconnecting in your examples. Also, an appropriate default for a
>>> command-line client is often a bad default for a proxy. It is
>>> complicated.
>>
>> Let me rephrase my point but please keep in mind that I have no idea
>> how Squid actually behaves.
> Let me pause you right there.  What you describe is essentially how the
> TLS protocol handshake is actually performed.
>
>> Simply put, when Squid tries to connect
>> for the first time, it will probably (I'm guessing here) try the most
>> secure protcol known today (ie. TSL 1.2), or let OpenSSL decide by
>> default which is probably the same.
> That is exactly what happens.
>
>> In my case, the server replies
>> nothing. That would be like running:
>>
>> # curl -k -v https://10.215.144.21 or # openssl s_client -connect
>> 10.215.144.21:443
>>
>> They give me the same information as Squid's log... almost nothing.
>>
>> So my point is, if that first connection fails and gives me nothing
>> for TLS 1.2 (or whatever the default is), two things can happen:
>> either the remote site is failing or it isn't supporting the
>> protocol. Why not "try again" but this time by being more specific?
> Several reasons.
>
> Reason #1 is that the TLS protocol is a security protocol for securing a
> single 'hop' (just one TCP connection). So ideally TLS details would not
> be remembered at all, it's a dangerous thing in security to remember
> details in the middleware.
>
>
> Reason #2 is that Squid has passed on the 'terminate' signal to the
> client (curl).
>
> As far as Squid is concerned, there is no "again" connection. There is a
> connection, which fails. The end.
>
> There is a connection. Which fails. The end.
>
> There is a connection. Which fails. The end.
>
>  ... and so on. These connections may be from the same client, or
> different ones. May be to same or different servers. They are
> independent of each other and only TCP at this point.
>
> The TLS setup/handshake parts never get far enough for there to be
> anything to remember. Except that TCP connection failed.
>
> Well, Squid does remember that and tries a different IP next time. Until
> it runs out of IPs, then it resets its bad-ID memory with a new DNS
> lookup and the cycle begins again.
>
>
> NP: if you are interested you can see the GOOD/BAD flags on IPs in the
> ipcache cache manager report (squidclient mgr:ipcache).
>
>
>> It would be like doing something like this:
>>
>> # openssl s_client -connect 10.215.144.21:443 || openssl s_client
>> -connect 10.215.144.21:443 -tls1_1 || openssl s_client -connect
>> 10.215.144.21:443 -tls1
>>
> Which brings us to reason #3; downgrade attacks.
>
> You may have heard of the POODLE attack. It is basically a middleware
> (like Squid) forcing the other endpoint(s) to re-try with lower TLS
> versions until a version is reached and cipher selected that the
> attacker can decrypt or steal the keys from.
>
> Squid (mostly) avoids the whole class of vulnerabilities by leaving the
> fallback decisions to the client whenever it can.
>
> Since the curl command only did the one request, that is all that
> happened. No retry.
>
>
> Amos
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users

-- 
Bugs to the Future
-------------- next part --------------
A non-text attachment was scrubbed...
Name: 0x613DEC46.asc
Type: application/pgp-keys
Size: 2437 bytes
Desc: not available
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20170201/b61edab8/attachment.key>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: signature.asc
Type: application/pgp-signature
Size: 473 bytes
Desc: OpenPGP digital signature
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20170201/b61edab8/attachment.sig>

From erdosain9 at gmail.com  Wed Feb  1 19:06:22 2017
From: erdosain9 at gmail.com (erdosain9)
Date: Wed, 1 Feb 2017 11:06:22 -0800 (PST)
Subject: [squid-users] Two dns record fqdn pointing to different squid
	servers
Message-ID: <1485975982132-4681422.post@n4.nabble.com>

Hi.
I have running two squid servers.
One with ip access and another with users.
(the machine users are configure with "proxy.blabla.lan" (the squid with ip
access)

I want to know if it is possible do balance between them.
The problem, for me it is that the "server with ip access" it is refer with
a A dns record that point to his ip (proxy.blabla.lan)... and the "squid
with user access", the dns it is pointing with fqdn (squid.blabla.lan)....

So, i cant do a multiple A record, pointing to the two ip, because, one of
the squid servers wait a fqdn answer...

---------------------
I tried to do  CNAME but, its not working... (i tried to do
"proxy.blabla.com pointing to squid.blabla.com at the same time that the ip
of the "ip access squid server")

(hope this understood, i dont speak english)



--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/Two-dns-record-fqdn-pointing-to-different-squid-servers-tp4681422.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From Antony.Stone at squid.open.source.it  Wed Feb  1 19:41:34 2017
From: Antony.Stone at squid.open.source.it (Antony Stone)
Date: Wed, 1 Feb 2017 20:41:34 +0100
Subject: [squid-users] Two dns record fqdn pointing to different squid
	servers
In-Reply-To: <1485975982132-4681422.post@n4.nabble.com>
References: <1485975982132-4681422.post@n4.nabble.com>
Message-ID: <201702012041.34888.Antony.Stone@squid.open.source.it>

On Wednesday 01 February 2017 at 20:06:22, erdosain9 wrote:

> Hi.
> I have running two squid servers.
> One with ip access and another with users.

Sorry, what do you mean by "IP access"?

I assume both Squid servers have IP addresses.

Do you mean that only one of them has connectivity to the Internet?

What do you mean by "the other one has users"?

Is it doing some sort of authentication, or do you simply mean that this is 
the one the users have connectivity to, so that your network arrangement is:

users -> Squid box 1 -> Squid box 2 -> router -> Internet

...and that the users cannot connect to Squid box 2, and Squid box 1 cannot 
connect to the Internet.

Is that a reasonable description of your setup?

> (the machine users are configure with "proxy.blabla.lan" (the squid with ip
> access)

I don't think I uderstand that bit.

> I want to know if it is possible do balance between them.

Please define "balance"?

> The problem, for me it is that the "server with ip access" it is refer with
> a A dns record that point to his ip (proxy.blabla.lan)... and the "squid
> with user access", the dns it is pointing with fqdn (squid.blabla.lan)....

Okay, so the two machines have DNS A records for different hostnames.

> So, i cant do a multiple A record, pointing to the two ip, because, one of
> the squid servers wait a fqdn answer...

Why can't you do a multiple A record?

There's nothing wrong with:

proxy.blabla.lan	A	192.168.38.56

squid.blabla.lan	A	192.168.38.73

example.blabla.lan	A	192.168.38.56
				A	192.168.38.73

So that example.blabla.lan points to both IP addresses.

> I tried to do  CNAME but, its not working... (i tried to do
> "proxy.blabla.com pointing to squid.blabla.com at the same time that the ip
> of the "ip access squid server")

No, you are not allowed to have a CNAME in DNS as well as any other record (A, 
MX, NS, etc).  If something is a CNAME, it cannot also be anything else.

> (hope this understood, i dont speak english)

I think you're doing okay, but please clarify the things requested above :)

Also, please simply tell us: what are you trying to achieve?  It's often the 
case that someone is trying to solve a problem, thinks of a possible solution, 
and asks on a mailing list such as this for help in implementing the 
solutions, when there is actually a far better / simpler solution to the 
problem available.  However, because the problem itself was not stated, nobody 
can propose the better / simpler solution, and everyone just works towards 
making the poorer solution work somehow or other...


Regards,


Antony.


-- 
People who use Microsoft software should be certified.

                                                   Please reply to the list;
                                                         please *don't* CC me.


From eliezer at ngtech.co.il  Wed Feb  1 20:19:08 2017
From: eliezer at ngtech.co.il (Eliezer  Croitoru)
Date: Wed, 1 Feb 2017 22:19:08 +0200
Subject: [squid-users] Not all html objects are being cached
In-Reply-To: <1485273173646-4681293.post@n4.nabble.com>
References: <1485273173646-4681293.post@n4.nabble.com>
Message-ID: <06bb01d27cc8$71a99bd0$54fcd370$@ngtech.co.il>

After some time reading the thread and getting to the bottom of it I think I have an idea on how to give another angle on the caching subject.
This is an example access.log which I got to with the help of Amos:
https://gist.github.com/elico/2ea2253ef1c09872ba90becb961acd91

It can reveal to the system administrator couple good reasons why an object wasn't cached.
It can serve both academic and real world decisions.
I believe that today squid does a nice job deciding what to cache and what to not in the limits of any LRU cache based system\software.

I think wireshark was a good choice to analyze the situation.
There is a possibility that some CMS or some web site will do things the wrong way and this software will enforce an object to be "non-cachable" but from this to break the fundamentals of HTTP there is a lot to learn.
First learn(we are here to assist) and then break if required.
There are many tools to break caching "rules" but squid tries to play the role of the most "friendly" on the Internet.
>From one hand it will give you couple very good API's and configuration but requires you, the admin to know what you are doing. When you don't know or do not understand take couple minutes to find the right document or tutorial(textual or video).
If you cannot reach these we are here for the rescue to help if needed.
Just ask!!!
The answers will be there  sooner or later in the thread.

All The Bests,
Eliezer

----
Eliezer Croitoru
Linux System Administrator
Mobile: +972-5-28704261
Email: eliezer at ngtech.co.il


-----Original Message-----
From: squid-users [mailto:squid-users-bounces at lists.squid-cache.org] On Behalf Of boruc
Sent: Tuesday, January 24, 2017 5:53 PM
To: squid-users at lists.squid-cache.org
Subject: [squid-users] Not all html objects are being cached

Hi everyone,

I was wondering why some of visited pages are not being cached (I mean
"main" pages, like www.example.com). If I visit 50 pages only 10 will be
cached. Below text is from log files:

store.log:
1485272001.646 RELEASE -1 FFFFFFFF 04F7FA9EAA7FE3D531A2224F4C7DDE5A  200
1485272011        -1 375007920 text/html -1/222442 GET http://www.wykop.pl/

access.log
1485272001.646    423 10.10.10.136 TCP_MISS/200 223422 GET
http://www.wykop.pl/ - DIRECT/185.66.120.38 text/html

According to Squid Wiki: "if a RELEASE code was logged with file number
FFFFFFFF, the object existed only in memory, and was released from memory."
- I understand that requested html file wasn't saved to disk, but why?

I'm also posting my squid.conf below. I'd be grateful for your answers!


acl manager proto cache_object
acl localhost src 127.0.0.1/32 ::1
acl to_localhost dst 127.0.0.0/8 0.0.0.0/32 ::1
acl my_network src 192.168.0.0/24
acl my_phone src 192.168.54.0/24
acl my_net dst 192.168.0.0/24
acl mgr src 10.48.5.0/24
acl new_net src 10.10.10.0/24
acl ex_ft url_regex -i "/etc/squid3/excluded_filetypes.txt" 
acl ex_do url_regex -i "/etc/squid3/excluded_domains.txt" #doesnt include
any of 50 visited pages

acl SSL_ports port 443
acl Safe_ports port 80		# http
acl Safe_ports port 21		# ftp
acl Safe_ports port 443		# https
acl Safe_ports port 70		# gopher
acl Safe_ports port 210		# wais
acl Safe_ports port 1025-65535	# unregistered ports
acl Safe_ports port 280		# http-mgmt
acl Safe_ports port 488		# gss-http
acl Safe_ports port 591		# filemaker
acl Safe_ports port 777		# multiling http
acl CONNECT method CONNECT

http_access allow my_network
http_access allow my_phone
http_access allow my_net
http_access allow mgr
http_access allow new_net
http_access allow manager localhost
http_access deny manager

http_access deny !Safe_ports

http_access deny CONNECT !SSL_ports

http_access allow localhost
http_access allow all

http_port 3128

maximum_object_size_in_memory 1024 KB

cache_dir ufs /var/spool/squid3 1000 16 256

cache_store_log /var/log/squid3/store.log

coredump_dir /var/spool/squid3

cache deny ex_ft
cache deny ex_do

refresh_pattern ^ftp:		1440	20%	10080
refresh_pattern ^gopher:	1440	0%	1440
refresh_pattern -i (/cgi-bin/|\?) 0	0%	0
refresh_pattern (Release|Packages(.gz)*)$      0       20%     2880

refresh_pattern .               1000       20%     4320

request_header_access Accept-Encoding deny all



--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/Not-all-html-objects-are-being-cached-tp4681293.html
Sent from the Squid - Users mailing list archive at Nabble.com.
_______________________________________________
squid-users mailing list
squid-users at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-users



From eliezer at ngtech.co.il  Wed Feb  1 20:19:44 2017
From: eliezer at ngtech.co.il (Eliezer  Croitoru)
Date: Wed, 1 Feb 2017 22:19:44 +0200
Subject: [squid-users] Antivirus for squid
In-Reply-To: <bf3b5639-c19f-2bb1-ebdf-76a8839ccebe@gmail.com>
References: <1485367389664-4681323.post@n4.nabble.com>
 <3dbe39e9-87ea-8b63-73a1-63c30d3f86ae@gmail.com>
 <VI1PR0401MB2685AC837199F2249A7B13938F740@VI1PR0401MB2685.eurprd04.prod.outlook.com>
 <1485963664148-4681413.post@n4.nabble.com>
 <fef780ab-2b98-a11c-e51b-bb11d5aa559d@gmail.com>
 <06aa01d27ca6$4540dda0$cfc298e0$@ngtech.co.il>
 <bf3b5639-c19f-2bb1-ebdf-76a8839ccebe@gmail.com>
Message-ID: <06c001d27cc8$8719bcc0$954d3640$@ngtech.co.il>

Thanks.

----
Eliezer Croitoru
Linux System Administrator
Mobile: +972-5-28704261
Email: eliezer at ngtech.co.il


-----Original Message-----
From: Yuri Voinov [mailto:yvoinov at gmail.com] 
Sent: Wednesday, February 1, 2017 6:16 PM
To: Eliezer Croitoru <eliezer at ngtech.co.il>
Cc: squid-users at lists.squid-cache.org
Subject: Re: [squid-users] Antivirus for squid

http://wiki.squid-cache.org/ConfigExamples/ContentAdaptation/C-ICAP


01.02.2017 22:14, Eliezer Croitoru ?????:
> Hey Yuri,
>
> What wiki article?
>
> Thanks,
> Eliezer
>
> ----
> Eliezer Croitoru
> Linux System Administrator
> Mobile: +972-5-28704261
> Email: eliezer at ngtech.co.il
>
>
> -----Original Message-----
> From: squid-users [mailto:squid-users-bounces at lists.squid-cache.org] 
> On Behalf Of Yuri Voinov
> Sent: Wednesday, February 1, 2017 5:52 PM
> To: squid-users at lists.squid-cache.org
> Subject: Re: [squid-users] Antivirus for squid
>
> Squid's wiki article contains all required points about performance and tuning.
>
>
> 01.02.2017 21:41, erdosain9 ?????:
>> Hi, again.
>> Well i installed squidclamav, c-icap, and clamav; and its working all 
>> fine, but... the download is too slow, the download of a file. There 
>> is a way to accelerate this??
> What do you mean "too slow"? Exact data, pls. Subjective and relative adjectives, do not say anything of substance.
>
> I mean, i.e.: "Before I've installed clamav, download speed was 1 terabit per second for file http://bwah-bwah.com/bwahbwahbwah.tar.gz.
> After - only 10 megabits. It seems too slow".
>> Also, when the file its a virus, the message "this is a virus bla 
>> bla", go
> This is different procedure, which is not executed by squid itself.
>> fast... i mean the slow download its for all the other files that 
>> dosent have a virus...
>>
>> *This is squid.conf
>> *
>> # c-icap integration
>> icap_enable on
>> icap_send_client_ip on
>> icap_send_client_username on
>> icap_client_username_header X-Authenticated-User icap_preview_enable 
>> on icap_preview_size 1024 icap_service service_req reqmod_precache
>> bypass=1 icap://127.0.0.1:1344/squidclamav adaptation_access 
>> service_req allow all icap_service service_resp respmod_precache
>> bypass=1 icap://127.0.0.1:1344/squidclamav adaptation_access 
>> service_resp allow all # end integration
>>
>>
>> *c-icap.conf
>> *
>> PidFile /var/run/c-icap.pid
>> CommandsSocket /var/run/c-icap.ctl
>> StartServers 1
>> MaxServers 20
>> MaxRequestsPerChild  100
>> Port 1344
>> ServerAdmin yourname at yourdomain
>> TmpDir /tmp
>> MaxMemObject 131072
>> DebugLevel 0
>> ModulesDir /usr/local/c-icap/lib/c_icap/ ServicesDir 
>> /usr/local/c-icap/lib/c_icap/ LoadMagicFile 
>> /usr/local/etc/c-icap.magic
>>
>> acl localhost src 127.0.0.1/255.255.255.255 acl PERMIT_REQUESTS type 
>> REQMOD RESPMOD icap_access allow localhost PERMIT_REQUESTS 
>> icap_access deny all
>>
>> ServerLog /var/log/c-icap/server.log
>> AccessLog /var/log/c-icap/access.log
>>
>> Service squidclamav squidclamav.so
>>
>>
>> *CLAMD.CONF*
>> LogFile /var/log/clamd.scan
>> PidFile /var/run/clamd.scan/clamd.pid TemporaryDirectory /var/tmp 
>> DatabaseDirectory /var/lib/clamav LocalSocket 
>> /var/run/clamd.scan/clamd.sock TCPSocket 3310 TCPAddr
>> 127.0.0.1 User clamscan
>>
>>
>> *SQUIDCLAMAV.CONF
>> *
>> maxsize 5000000
>> redirect http://squid.espaciomemoria.lan/cgi-bin/clwarn.cgi.en_EN
>> clamd_ip 127.0.0.1
>> clamd_port 3310
>> trust_cache 0
>> timeout 1
>> logredir 1
>> dnslookup 0
>> safebrowsing 0
>>
>> abortcontent ^video\/x-flv$
>> abortcontent ^video\/mp4$
>> # White list some sites
>>
>> Somebody can give me a hand with this???
>> Thanks to all.
> Thelepathy on vacation. To give your hand, it is require to have root access to your server to make performance diagnostics during "slow downloads". But you always can do this yourself. Pieces of configs is not enough to diagnostics, and, therefore, for tuning.
>>
>>
>> --
>> View this message in context: 
>> http://squid-web-proxy-cache.1019090.n4.nabble.com/Antivirus-for-squi
>> d -tp4681323p4681413.html Sent from the Squid - Users mailing list 
>> archive at Nabble.com.
>> _______________________________________________
>> squid-users mailing list
>> squid-users at lists.squid-cache.org
>> http://lists.squid-cache.org/listinfo/squid-users
> --
> Bugs to the Future
>

--
Bugs to the Future



From rousskov at measurement-factory.com  Wed Feb  1 20:33:10 2017
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Wed, 1 Feb 2017 13:33:10 -0700
Subject: [squid-users] transparent http and https filter with white-list
 only
In-Reply-To: <1d01efe0-83f8-2a91-c0ac-fd8ef769276f@treenet.co.nz>
References: <c25e40df-d60e-2900-d6e6-954771e56c90@gmail.com>
 <1d01efe0-83f8-2a91-c0ac-fd8ef769276f@treenet.co.nz>
Message-ID: <62119c6c-9787-3edd-c59c-7be1dcad33b5@measurement-factory.com>

On 02/01/2017 07:46 AM, Amos Jeffries wrote:
> On 28/01/2017 12:36 a.m., Sergey Klusov wrote:
>> acl step1 at_step SslBump1
>> ssl_bump peek step1
>> ssl_bump splice https_allow
>> ssl_bump terminate all


>  All other traffic will be terminated ... maybe with an HTTP error page.

Bugs not withstanding, the terminate action should close the client TCP
connection without serving the error page.



> The ssl::server_name ACL will not work outside of the ssl_bump directive.

Each SslBump step gives the ACL more [reliable] information, but the ACL
is not confined to the ssl_bump rules. Using this ACL before (or without
any) ssl_bump steps is almost pointless because it can probably only
match "none", but using it during or after those steps is fine, even
outside the ssl_bump directive context. This clarification is based on
my interpretation of v5 code.

This aspect may not be relevant to your squid.conf, but I wanted to
clarify it in case somebody uses this email thread for other purposes.


Cheers,

Alex.



From vel21ripn at gmail.com  Wed Feb  1 20:55:58 2017
From: vel21ripn at gmail.com (Vitaly Lavrov)
Date: Wed, 1 Feb 2017 23:55:58 +0300
Subject: [squid-users] High utilization of CPU squid-3.5.23,squid-3.5.24
Message-ID: <9b1e504a-d14c-a025-84c8-54b5aa7d6fdd@gmail.com>

Periodically squid begins to linearly increase the use of the CPU.
Sometimes this process reaches 100%. At random moment of time the CPU usage is reduced to 5-15%,
and in the presence of client requests can again start linearly increasing use of CPU.

In the protocols are no error messages.

CPU consumption does not correlate with the number of requests and traffic.

The increase CPU consumption from 0 to 60% occurs in about 4-5 hours, and to 100% for 6-8 hours.

A typical graph of CPU usage can be viewed on http://devel.aanet.ru/tmp/squid-cpu-x.png

With the "perf record -p` pgrep -f squid-1` - sleep 30" I have received the following information:

At 100% CPU load most of the time took 3 calls

  49.15% squid squid [.] MemObject :: dump
  25.11% squid squid [.] Mem_hdr :: freeDataUpto
  20.03% squid squid [.] Mem_hdr :: copy

When loading CPU 30-60% most of the time took 3 calls

  37.26% squid squid [.] Mem_node :: dataRange
  22.61% squid squid [.] Mem_hdr :: NodeCompare
  17.31% squid squid [.] Mem_hdr :: freeDataUpto

What is it ? Is it possible to somehow fix it?

System: slackware64 14.2

sslbump not used. http only.

Part of config:

memory_pools off
memory_pools_limit 512 MB
cache_mem 768 MB
maximum_object_size_in_memory 64 KB
cache_dir ufs           /cache/sq_c1 16312 16 256
cache_dir ufs           /cache/sq_c2 16312 16 256
cache_dir ufs           /cache/sq_c3 16312 16 256


From angelvg at gmail.com  Wed Feb  1 20:58:55 2017
From: angelvg at gmail.com (angelv)
Date: Wed, 1 Feb 2017 15:58:55 -0500
Subject: [squid-users] Buy Certificates for Squid 'man in the middle'
Message-ID: <CA+wWuAxGYRCkEptmgQx6ezeH3O9MH5Z9QT4bNoo_Nnz+=G4Kig@mail.gmail.com>

Hi,

I need your advice.

I have a transparent proxy running with the self generated certificates
'myCA.pem', as it is not signed by a valid entity then I have to import the
'myCA.der' certificate in all web browsers ...

I want to know where I can buy a valid certificate that work in Squid.

PD:
The proxy is working great


----------------------------------------------------------------------------------------------
Important information for clarity (FreeBSD, squid-3.5.23 and PF):

Create self-signed certificate for Squid server

# openssl req -new -newkey rsa:2048 -sha256 -days 36500 -nodes -x509
-extensions v3_ca -keyout myCA.pem  -out
/usr/local/etc/squid/ssl_cert/myCA.pem -config
/usr/local/etc/squid/ssl_cert/openssl.cnf

# openssl dhparam -outform PEM -out
/usr/local/etc/squid/ssl_cert/dhparam.pem 2048

Create a DER-encoded certificate to import into users' browsers

# openssl x509 -in /usr/local/etc/squid/ssl_cert/myCA.pem -outform DER -out
/usr/local/etc/squid/ssl_cert/myCA.der


# edit /usr/local/etc/squid/squid.conf
...
# Squid normally listens to port 3128
http_port  3128

# Intercept HTTPS CONNECT messages with SSL-Bump
#
http_port  3129 ssl-bump intercept \
        cert=/usr/local/etc/squid/ssl_cert/myCA.pem \
        generate-host-certificates=on dynamic_cert_mem_cache_size=4MB \
        dhparams=/usr/local/etc/squid/ssl_cert/dhparam.pem
#
https_port 3130 ssl-bump intercept \
        cert=/usr/local/etc/squid/ssl_cert/myCA.pem \
        generate-host-certificates=on dynamic_cert_mem_cache_size=4MB \
        dhparams=/usr/local/etc/squid/ssl_cert/dhparam.pem
#
sslcrtd_program /usr/local/libexec/squid/ssl_crtd -s
/usr/local/etc/squid/ssl_db -M 4MB
#
acl step1 at_step SslBump1
#
ssl_bump peek step1
ssl_bump stare all
ssl_bump bump all
always_direct allow all
#
sslproxy_cert_error allow all
sslproxy_flags DONT_VERIFY_PEER
...

PF redirect the traffic to the Squid

# edit /etc/pf.conf
...
# Intercept HTTPS CONNECT messages with SSL-Bump
rdr pass on $int_if inet  proto tcp from any to port https \
        -> 127.0.0.1 port 3130
rdr pass on $int_if inet6 proto tcp from any to port https \
        -> ::1 port 3130
...
----------------------------------------------------------------------------------------------
-- 
?ngel Villa G.
US +1 (786) 233-9240 | CO +57 (300) 283-6546
angelvg at gmail.com
https://google.com/+AngelVillaG
https://angelcontents.blogspot.com

"We are all atheists about most of the gods that societies have ever
believed in. Some of us just go one god further" - Richard Dawkins
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20170201/d027bd1d/attachment.htm>

From yvoinov at gmail.com  Wed Feb  1 21:10:51 2017
From: yvoinov at gmail.com (Yuri Voinov)
Date: Thu, 2 Feb 2017 03:10:51 +0600
Subject: [squid-users] Buy Certificates for Squid 'man in the middle'
In-Reply-To: <CA+wWuAxGYRCkEptmgQx6ezeH3O9MH5Z9QT4bNoo_Nnz+=G4Kig@mail.gmail.com>
References: <CA+wWuAxGYRCkEptmgQx6ezeH3O9MH5Z9QT4bNoo_Nnz+=G4Kig@mail.gmail.com>
Message-ID: <564802ba-ce3b-d241-7b67-e1d4c45936e3@gmail.com>



02.02.2017 2:58, angelv ?????:
> Hi,
>
> I need your advice.
>
> I have a transparent proxy running with the self generated
> certificates 'myCA.pem', as it is not signed by a valid entity then I
> have to import the 'myCA.der' certificate in all web browsers ...
>
> I want to know where I can buy a valid certificate that work in Squid.
Nowhere. Due to CA's CPS.
>
> PD:
> The proxy is working great
>
>
> ----------------------------------------------------------------------------------------------
> Important information for clarity (FreeBSD, squid-3.5.23 and PF):
>
> Create self-signed certificate for Squid server
>
> # openssl req -new -newkey rsa:2048 -sha256 -days 36500 -nodes -x509
> -extensions v3_ca -keyout myCA.pem  -out
> /usr/local/etc/squid/ssl_cert/myCA.pem -config
> /usr/local/etc/squid/ssl_cert/openssl.cnf
>
> # openssl dhparam -outform PEM -out
> /usr/local/etc/squid/ssl_cert/dhparam.pem 2048
>
> Create a DER-encoded certificate to import into users' browsers
>
> # openssl x509 -in /usr/local/etc/squid/ssl_cert/myCA.pem -outform DER
> -out /usr/local/etc/squid/ssl_cert/myCA.der
>
>
> # edit /usr/local/etc/squid/squid.conf
> ...
> # Squid normally listens to port 3128
> http_port  3128
>
> # Intercept HTTPS CONNECT messages with SSL-Bump
> #
> http_port  3129 ssl-bump intercept \
>         cert=/usr/local/etc/squid/ssl_cert/myCA.pem \
>         generate-host-certificates=on dynamic_cert_mem_cache_size=4MB \
>         dhparams=/usr/local/etc/squid/ssl_cert/dhparam.pem
> #
> https_port 3130 ssl-bump intercept \
>         cert=/usr/local/etc/squid/ssl_cert/myCA.pem \
>         generate-host-certificates=on dynamic_cert_mem_cache_size=4MB \
>         dhparams=/usr/local/etc/squid/ssl_cert/dhparam.pem
> #
> sslcrtd_program /usr/local/libexec/squid/ssl_crtd -s
> /usr/local/etc/squid/ssl_db -M 4MB
> #
> acl step1 at_step SslBump1
> #
> ssl_bump peek step1
> ssl_bump stare all
> ssl_bump bump all
> always_direct allow all
> #
> sslproxy_cert_error allow all
> sslproxy_flags DONT_VERIFY_PEER
> ...
>
> PF redirect the traffic to the Squid
>
> # edit /etc/pf.conf
> ...
> # Intercept HTTPS CONNECT messages with SSL-Bump
> rdr pass on $int_if inet  proto tcp from any to port https \
>         -> 127.0.0.1 port 3130
> rdr pass on $int_if inet6 proto tcp from any to port https \
>         -> ::1 port 3130
> ...
> ----------------------------------------------------------------------------------------------
> -- 
> ?ngel Villa G.
> US +1 (786) 233-9240 | CO +57 (300) 283-6546
> angelvg at gmail.com <mailto:angelvg at gmail.com>
> https://google.com/+AngelVillaG
> https://angelcontents.blogspot.com
>
> "We are all atheists about most of the gods that societies have ever
> believed in. Some of us just go one god further" - Richard Dawkins
>
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users

-- 
Bugs to the Future
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20170202/d4930710/attachment.htm>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: 0x613DEC46.asc
Type: application/pgp-keys
Size: 2437 bytes
Desc: not available
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20170202/d4930710/attachment.key>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: signature.asc
Type: application/pgp-signature
Size: 473 bytes
Desc: OpenPGP digital signature
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20170202/d4930710/attachment.sig>

From yvoinov at gmail.com  Wed Feb  1 21:16:57 2017
From: yvoinov at gmail.com (Yuri Voinov)
Date: Thu, 2 Feb 2017 03:16:57 +0600
Subject: [squid-users] Buy Certificates for Squid 'man in the middle'
In-Reply-To: <564802ba-ce3b-d241-7b67-e1d4c45936e3@gmail.com>
References: <CA+wWuAxGYRCkEptmgQx6ezeH3O9MH5Z9QT4bNoo_Nnz+=G4Kig@mail.gmail.com>
 <564802ba-ce3b-d241-7b67-e1d4c45936e3@gmail.com>
Message-ID: <889e9e05-6f3e-c441-edf6-cf3037f47071@gmail.com>

In three words:

Forget about it.

No one in the world permit you to do Man-In-The-Middle-Attack hidden
from users.

CAs in the event of such certificates immediately include it in the list
of untrusted. And you can give up the problems up to prison for a long
time. For violation of the privacy of users. In other words, users
should be aware that there is a proxy hacking HTTPS in front of them.
All other tricks are illegal, at least it is contrary to ethics.

Forget about it.

I'm seriously.

02.02.2017 3:10, Yuri Voinov ?????:
>
>
>
> 02.02.2017 2:58, angelv ?????:
>> Hi,
>>
>> I need your advice.
>>
>> I have a transparent proxy running with the self generated
>> certificates 'myCA.pem', as it is not signed by a valid entity then I
>> have to import the 'myCA.der' certificate in all web browsers ...
>>
>> I want to know where I can buy a valid certificate that work in Squid.
> Nowhere. Due to CA's CPS.
>>
>> PD:
>> The proxy is working great
>>
>>
>> ----------------------------------------------------------------------------------------------
>> Important information for clarity (FreeBSD, squid-3.5.23 and PF):
>>
>> Create self-signed certificate for Squid server
>>
>> # openssl req -new -newkey rsa:2048 -sha256 -days 36500 -nodes -x509
>> -extensions v3_ca -keyout myCA.pem  -out
>> /usr/local/etc/squid/ssl_cert/myCA.pem -config
>> /usr/local/etc/squid/ssl_cert/openssl.cnf
>>
>> # openssl dhparam -outform PEM -out
>> /usr/local/etc/squid/ssl_cert/dhparam.pem 2048
>>
>> Create a DER-encoded certificate to import into users' browsers
>>
>> # openssl x509 -in /usr/local/etc/squid/ssl_cert/myCA.pem -outform
>> DER -out /usr/local/etc/squid/ssl_cert/myCA.der
>>
>>
>> # edit /usr/local/etc/squid/squid.conf
>> ...
>> # Squid normally listens to port 3128
>> http_port  3128
>>
>> # Intercept HTTPS CONNECT messages with SSL-Bump
>> #
>> http_port  3129 ssl-bump intercept \
>>         cert=/usr/local/etc/squid/ssl_cert/myCA.pem \
>>         generate-host-certificates=on dynamic_cert_mem_cache_size=4MB \
>>         dhparams=/usr/local/etc/squid/ssl_cert/dhparam.pem
>> #
>> https_port 3130 ssl-bump intercept \
>>         cert=/usr/local/etc/squid/ssl_cert/myCA.pem \
>>         generate-host-certificates=on dynamic_cert_mem_cache_size=4MB \
>>         dhparams=/usr/local/etc/squid/ssl_cert/dhparam.pem
>> #
>> sslcrtd_program /usr/local/libexec/squid/ssl_crtd -s
>> /usr/local/etc/squid/ssl_db -M 4MB
>> #
>> acl step1 at_step SslBump1
>> #
>> ssl_bump peek step1
>> ssl_bump stare all
>> ssl_bump bump all
>> always_direct allow all
>> #
>> sslproxy_cert_error allow all
>> sslproxy_flags DONT_VERIFY_PEER
>> ...
>>
>> PF redirect the traffic to the Squid
>>
>> # edit /etc/pf.conf
>> ...
>> # Intercept HTTPS CONNECT messages with SSL-Bump
>> rdr pass on $int_if inet  proto tcp from any to port https \
>>         -> 127.0.0.1 port 3130
>> rdr pass on $int_if inet6 proto tcp from any to port https \
>>         -> ::1 port 3130
>> ...
>> ----------------------------------------------------------------------------------------------
>> -- 
>> ?ngel Villa G.
>> US +1 (786) 233-9240 | CO +57 (300) 283-6546
>> angelvg at gmail.com <mailto:angelvg at gmail.com>
>> https://google.com/+AngelVillaG
>> https://angelcontents.blogspot.com
>>
>> "We are all atheists about most of the gods that societies have ever
>> believed in. Some of us just go one god further" - Richard Dawkins
>>
>>
>> _______________________________________________
>> squid-users mailing list
>> squid-users at lists.squid-cache.org
>> http://lists.squid-cache.org/listinfo/squid-users
>
> -- 
> Bugs to the Future

-- 
Bugs to the Future
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20170202/df678027/attachment.htm>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: 0x613DEC46.asc
Type: application/pgp-keys
Size: 2437 bytes
Desc: not available
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20170202/df678027/attachment.key>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: signature.asc
Type: application/pgp-signature
Size: 473 bytes
Desc: OpenPGP digital signature
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20170202/df678027/attachment.sig>

From yvoinov at gmail.com  Wed Feb  1 21:19:32 2017
From: yvoinov at gmail.com (Yuri Voinov)
Date: Thu, 2 Feb 2017 03:19:32 +0600
Subject: [squid-users] High utilization of CPU squid-3.5.23, squid-3.5.24
In-Reply-To: <9b1e504a-d14c-a025-84c8-54b5aa7d6fdd@gmail.com>
References: <9b1e504a-d14c-a025-84c8-54b5aa7d6fdd@gmail.com>
Message-ID: <7d9e9aac-3137-8dc9-55a5-220e75ea6820@gmail.com>

It seems as IO bottleneck at first look.

02.02.2017 2:55, Vitaly Lavrov ?????:
> Periodically squid begins to linearly increase the use of the CPU.
> Sometimes this process reaches 100%. At random moment of time the CPU usage is reduced to 5-15%,
> and in the presence of client requests can again start linearly increasing use of CPU.
>
> In the protocols are no error messages.
>
> CPU consumption does not correlate with the number of requests and traffic.
>
> The increase CPU consumption from 0 to 60% occurs in about 4-5 hours, and to 100% for 6-8 hours.
>
> A typical graph of CPU usage can be viewed on http://devel.aanet.ru/tmp/squid-cpu-x.png
>
> With the "perf record -p` pgrep -f squid-1` - sleep 30" I have received the following information:
>
> At 100% CPU load most of the time took 3 calls
>
>   49.15% squid squid [.] MemObject :: dump
>   25.11% squid squid [.] Mem_hdr :: freeDataUpto
>   20.03% squid squid [.] Mem_hdr :: copy
>
> When loading CPU 30-60% most of the time took 3 calls
>
>   37.26% squid squid [.] Mem_node :: dataRange
>   22.61% squid squid [.] Mem_hdr :: NodeCompare
>   17.31% squid squid [.] Mem_hdr :: freeDataUpto
>
> What is it ? Is it possible to somehow fix it?
>
> System: slackware64 14.2
>
> sslbump not used. http only.
>
> Part of config:
>
> memory_pools off
> memory_pools_limit 512 MB
> cache_mem 768 MB
> maximum_object_size_in_memory 64 KB
> cache_dir ufs           /cache/sq_c1 16312 16 256
> cache_dir ufs           /cache/sq_c2 16312 16 256
> cache_dir ufs           /cache/sq_c3 16312 16 256
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users

-- 
Bugs to the Future
-------------- next part --------------
A non-text attachment was scrubbed...
Name: 0x613DEC46.asc
Type: application/pgp-keys
Size: 2437 bytes
Desc: not available
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20170202/c89619b1/attachment.key>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: signature.asc
Type: application/pgp-signature
Size: 473 bytes
Desc: OpenPGP digital signature
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20170202/c89619b1/attachment.sig>

From eliezer at ngtech.co.il  Wed Feb  1 21:34:51 2017
From: eliezer at ngtech.co.il (Eliezer  Croitoru)
Date: Wed, 1 Feb 2017 23:34:51 +0200
Subject: [squid-users] High utilization of CPU squid-3.5.23, squid-3.5.24
In-Reply-To: <9b1e504a-d14c-a025-84c8-54b5aa7d6fdd@gmail.com>
References: <9b1e504a-d14c-a025-84c8-54b5aa7d6fdd@gmail.com>
Message-ID: <06e701d27cd3$058631b0$10929510$@ngtech.co.il>

I believe that the squid manager info page should give some clue about the number of concurrent requests.

If it's above some number(300-400 and above) per second then removing the cache_dir from the server for a windows of a day will answer if it's a DISK IO bottle neck or something else.

All The Bests,
Eliezer

----
Eliezer Croitoru
Linux System Administrator
Mobile: +972-5-28704261
Email: eliezer at ngtech.co.il


-----Original Message-----
From: squid-users [mailto:squid-users-bounces at lists.squid-cache.org] On Behalf Of Vitaly Lavrov
Sent: Wednesday, February 1, 2017 10:56 PM
To: squid-users at squid-cache.org
Subject: [squid-users] High utilization of CPU squid-3.5.23,squid-3.5.24

Periodically squid begins to linearly increase the use of the CPU.
Sometimes this process reaches 100%. At random moment of time the CPU usage is reduced to 5-15%, and in the presence of client requests can again start linearly increasing use of CPU.

In the protocols are no error messages.

CPU consumption does not correlate with the number of requests and traffic.

The increase CPU consumption from 0 to 60% occurs in about 4-5 hours, and to 100% for 6-8 hours.

A typical graph of CPU usage can be viewed on http://devel.aanet.ru/tmp/squid-cpu-x.png

With the "perf record -p` pgrep -f squid-1` - sleep 30" I have received the following information:

At 100% CPU load most of the time took 3 calls

  49.15% squid squid [.] MemObject :: dump
  25.11% squid squid [.] Mem_hdr :: freeDataUpto
  20.03% squid squid [.] Mem_hdr :: copy

When loading CPU 30-60% most of the time took 3 calls

  37.26% squid squid [.] Mem_node :: dataRange
  22.61% squid squid [.] Mem_hdr :: NodeCompare
  17.31% squid squid [.] Mem_hdr :: freeDataUpto

What is it ? Is it possible to somehow fix it?

System: slackware64 14.2

sslbump not used. http only.

Part of config:

memory_pools off
memory_pools_limit 512 MB
cache_mem 768 MB
maximum_object_size_in_memory 64 KB
cache_dir ufs           /cache/sq_c1 16312 16 256
cache_dir ufs           /cache/sq_c2 16312 16 256
cache_dir ufs           /cache/sq_c3 16312 16 256
_______________________________________________
squid-users mailing list
squid-users at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-users



From eliezer at ngtech.co.il  Wed Feb  1 21:42:44 2017
From: eliezer at ngtech.co.il (Eliezer  Croitoru)
Date: Wed, 1 Feb 2017 23:42:44 +0200
Subject: [squid-users] heart beet between squid peers
In-Reply-To: <CAPACB-xdUX8M=rSwX1PforkGEUnhB4AMNjm4ztKYWL7r7sPRWw@mail.gmail.com>
References: <CAPACB-xdUX8M=rSwX1PforkGEUnhB4AMNjm4ztKYWL7r7sPRWw@mail.gmail.com>
Message-ID: <06e901d27cd4$1f1b00a0$5d5101e0$@ngtech.co.il>

Hey,

You can use the cache manage interface for this.
Take a peek at:
http://wiki.squid-cache.org/Features/CacheManager#Cache_manager_Access_Control_in_squid.conf

Technically you can run something like:
http_proxy=http:///127.0.0.1:3128/ curl -X POST  http://cache-peer:3128/squid-internal-mgr/info

And see if you get and access denied.
If so then it means that your request was answered from the cache_peer.
If you need some help with it let me know.

Eliezer

----
http://ngtech.co.il/lmgtfy/
Linux System Administrator
Mobile: +972-5-28704261
Email: eliezer at ngtech.co.il


From: squid-users [mailto:squid-users-bounces at lists.squid-cache.org] On Behalf Of salil GK
Sent: Wednesday, February 1, 2017 9:06 AM
To: squid-users at lists.squid-cache.org
Subject: [squid-users] heart beet between squid peers

Hi

   I am trying to build a solution with squid proxy chaining. I have a proxy chaining through a tunnel. Every thing works fine !! But in my framework I need to know whether the connectivity is through from squid child to squid parent. What is the best way to know - is there any API or utility that I can use to find it out ? I need to know in both machines that the squid channel is active. Any help in this regard would be appreciated.

Thanks
~S



From yvoinov at gmail.com  Wed Feb  1 21:45:05 2017
From: yvoinov at gmail.com (Yuri Voinov)
Date: Thu, 2 Feb 2017 03:45:05 +0600
Subject: [squid-users] High utilization of CPU squid-3.5.23, squid-3.5.24
In-Reply-To: <06e701d27cd3$058631b0$10929510$@ngtech.co.il>
References: <9b1e504a-d14c-a025-84c8-54b5aa7d6fdd@gmail.com>
 <06e701d27cd3$058631b0$10929510$@ngtech.co.il>
Message-ID: <4fd2ac33-ee17-5f80-cda7-c02544e4e735@gmail.com>

Yes, it is require to perform extended diagnostics. Including the system
level.

BTW, it can also network IO. And, it is possible that even a slow DNS.
Have to search.


02.02.2017 3:34, Eliezer Croitoru ?????:
> I believe that the squid manager info page should give some clue about the number of concurrent requests.
>
> If it's above some number(300-400 and above) per second then removing the cache_dir from the server for a windows of a day will answer if it's a DISK IO bottle neck or something else.
>
> All The Bests,
> Eliezer
>
> ----
> Eliezer Croitoru
> Linux System Administrator
> Mobile: +972-5-28704261
> Email: eliezer at ngtech.co.il
>
>
> -----Original Message-----
> From: squid-users [mailto:squid-users-bounces at lists.squid-cache.org] On Behalf Of Vitaly Lavrov
> Sent: Wednesday, February 1, 2017 10:56 PM
> To: squid-users at squid-cache.org
> Subject: [squid-users] High utilization of CPU squid-3.5.23,squid-3.5.24
>
> Periodically squid begins to linearly increase the use of the CPU.
> Sometimes this process reaches 100%. At random moment of time the CPU usage is reduced to 5-15%, and in the presence of client requests can again start linearly increasing use of CPU.
>
> In the protocols are no error messages.
>
> CPU consumption does not correlate with the number of requests and traffic.
>
> The increase CPU consumption from 0 to 60% occurs in about 4-5 hours, and to 100% for 6-8 hours.
>
> A typical graph of CPU usage can be viewed on http://devel.aanet.ru/tmp/squid-cpu-x.png
>
> With the "perf record -p` pgrep -f squid-1` - sleep 30" I have received the following information:
>
> At 100% CPU load most of the time took 3 calls
>
>   49.15% squid squid [.] MemObject :: dump
>   25.11% squid squid [.] Mem_hdr :: freeDataUpto
>   20.03% squid squid [.] Mem_hdr :: copy
>
> When loading CPU 30-60% most of the time took 3 calls
>
>   37.26% squid squid [.] Mem_node :: dataRange
>   22.61% squid squid [.] Mem_hdr :: NodeCompare
>   17.31% squid squid [.] Mem_hdr :: freeDataUpto
>
> What is it ? Is it possible to somehow fix it?
>
> System: slackware64 14.2
>
> sslbump not used. http only.
>
> Part of config:
>
> memory_pools off
> memory_pools_limit 512 MB
> cache_mem 768 MB
> maximum_object_size_in_memory 64 KB
> cache_dir ufs           /cache/sq_c1 16312 16 256
> cache_dir ufs           /cache/sq_c2 16312 16 256
> cache_dir ufs           /cache/sq_c3 16312 16 256
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users

-- 
Bugs to the Future
-------------- next part --------------
A non-text attachment was scrubbed...
Name: 0x613DEC46.asc
Type: application/pgp-keys
Size: 2437 bytes
Desc: not available
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20170202/5e28791c/attachment.key>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: signature.asc
Type: application/pgp-signature
Size: 473 bytes
Desc: OpenPGP digital signature
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20170202/5e28791c/attachment.sig>

From garryd at comnet.uz  Thu Feb  2 04:45:13 2017
From: garryd at comnet.uz (Garri Djavadyan)
Date: Thu, 02 Feb 2017 09:45:13 +0500
Subject: [squid-users] High utilization of CPU squid-3.5.23, squid-3.5.24
In-Reply-To: <9b1e504a-d14c-a025-84c8-54b5aa7d6fdd@gmail.com>
References: <9b1e504a-d14c-a025-84c8-54b5aa7d6fdd@gmail.com>
Message-ID: <1486010713.14363.2.camel@comnet.uz>

On Wed, 2017-02-01 at 23:55 +0300, Vitaly Lavrov wrote:
> Periodically squid begins to linearly increase the use of the CPU.
> Sometimes this process reaches 100%. At random moment of time the CPU
> usage is reduced to 5-15%,
> and in the presence of client requests can again start linearly
> increasing use of CPU.
> 
> In the protocols are no error messages.
> 
> CPU consumption does not correlate with the number of requests and
> traffic.
> 
> The increase CPU consumption from 0 to 60% occurs in about 4-5 hours,
> and to 100% for 6-8 hours.
> 
> A typical graph of CPU usage can be viewed on http://devel.aanet.ru/t
> mp/squid-cpu-x.png
> 
> With the "perf record -p` pgrep -f squid-1` - sleep 30" I have
> received the following information:
> 
> At 100% CPU load most of the time took 3 calls
> 
> ? 49.15% squid squid [.] MemObject :: dump
> ? 25.11% squid squid [.] Mem_hdr :: freeDataUpto
> ? 20.03% squid squid [.] Mem_hdr :: copy
> 
> When loading CPU 30-60% most of the time took 3 calls
> 
> ? 37.26% squid squid [.] Mem_node :: dataRange
> ? 22.61% squid squid [.] Mem_hdr :: NodeCompare
> ? 17.31% squid squid [.] Mem_hdr :: freeDataUpto
> 
> What is it ? Is it possible to somehow fix it?
> 
> System: slackware64 14.2
> 
> sslbump not used. http only.
> 
> Part of config:
> 
> memory_pools off
> memory_pools_limit 512 MB
> cache_mem 768 MB
> maximum_object_size_in_memory 64 KB
> cache_dir ufs???????????/cache/sq_c1 16312 16 256
> cache_dir ufs???????????/cache/sq_c2 16312 16 256
> cache_dir ufs???????????/cache/sq_c3 16312 16 256


Hi Vitaly,

It seems you faced known issue related to linear search through in-
memory nodes. See bug report 4477 [1].

[1] http://bugs.squid-cache.org/show_bug.cgi?id=4477


Garri


From fredbmail at free.fr  Thu Feb  2 08:03:23 2017
From: fredbmail at free.fr (FredB)
Date: Thu, 2 Feb 2017 09:03:23 +0100 (CET)
Subject: [squid-users] SSL_bump and source IP
In-Reply-To: <1427329519.22421277.1484136240333.JavaMail.root@zimbra4-e1.priv.proxad.net>
Message-ID: <1897130924.71362131.1486022602999.JavaMail.root@zimbra4-e1>

So how I can manage computers without my CA ? (eg: laptop temporary connected) 
In my situation I have also some smartphones in some case, connected to my squids, how I can exclude them from SSLBump ?
I have already some ACL based on authentication (user azerty = with/without some rules)  

FredBb



From fredbmail at free.fr  Thu Feb  2 08:35:28 2017
From: fredbmail at free.fr (FredB)
Date: Thu, 2 Feb 2017 09:35:28 +0100 (CET)
Subject: [squid-users] Buy Certificates for Squid 'man in the middle'
In-Reply-To: <CA+wWuAxGYRCkEptmgQx6ezeH3O9MH5Z9QT4bNoo_Nnz+=G4Kig@mail.gmail.com>
Message-ID: <789407094.71493603.1486024528225.JavaMail.root@zimbra4-e1>


From: http://wiki.squid-cache.org/Features/DynamicSslCert

"In theory, you must either import your root certificate into browsers or instruct users on how to do that. Unfortunately, it is apparently a common practice among well-known Root CAs to issue subordinate root certificates. If you have obtained such a subordinate root certificate from a Root CA already trusted by your users, you do not need to import your certificate into browsers. However, going down this path may result in removal of the well-known Root CA certificate from browsers around the world. Such a removal will make your local SslBump-based infrastructure inoperable until you import your certificate, but that may only be the beginning of your troubles. Will the affected Root CA go after you to recoup their world-wide damages? What will your users do when they learn that you have been decrypting their traffic without their consent?" 

The last sentence is ambiguous the users can known, you can inform that you have been decrypting their traffic. 
There is no difference (from user point of view I mean) between a well-known Root CAs or a self-signed certificate with a CA injected by a local GPO. 
 
But in practice I don't how how you can do that, just hello I want a subordinate root certificates ?

FredB  


From odhiambo at gmail.com  Thu Feb  2 08:49:05 2017
From: odhiambo at gmail.com (Odhiambo Washington)
Date: Thu, 2 Feb 2017 11:49:05 +0300
Subject: [squid-users] Buy Certificates for Squid 'man in the middle'
In-Reply-To: <789407094.71493603.1486024528225.JavaMail.root@zimbra4-e1>
References: <CA+wWuAxGYRCkEptmgQx6ezeH3O9MH5Z9QT4bNoo_Nnz+=G4Kig@mail.gmail.com>
 <789407094.71493603.1486024528225.JavaMail.root@zimbra4-e1>
Message-ID: <CAAdA2WMehhLN0ZinnyrnFXQT_ig3C0ZGbpZq49cEMwJ6YZeJ=w@mail.gmail.com>

So we can't even use the free certs from letsencrypt with Squid??

On 2 February 2017 at 11:35, FredB <fredbmail at free.fr> wrote:

>
> From: http://wiki.squid-cache.org/Features/DynamicSslCert
>
> "In theory, you must either import your root certificate into browsers or
> instruct users on how to do that. Unfortunately, it is apparently a common
> practice among well-known Root CAs to issue subordinate root certificates.
> If you have obtained such a subordinate root certificate from a Root CA
> already trusted by your users, you do not need to import your certificate
> into browsers. However, going down this path may result in removal of the
> well-known Root CA certificate from browsers around the world. Such a
> removal will make your local SslBump-based infrastructure inoperable until
> you import your certificate, but that may only be the beginning of your
> troubles. Will the affected Root CA go after you to recoup their world-wide
> damages? What will your users do when they learn that you have been
> decrypting their traffic without their consent?"
>
> The last sentence is ambiguous the users can known, you can inform that
> you have been decrypting their traffic.
> There is no difference (from user point of view I mean) between a
> well-known Root CAs or a self-signed certificate with a CA injected by a
> local GPO.
>
> But in practice I don't how how you can do that, just hello I want a
> subordinate root certificates ?
>
> FredB
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>



-- 
Best regards,
Odhiambo WASHINGTON,
Nairobi,KE
+254 7 3200 0004/+254 7 2274 3223
"Oh, the cruft."
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20170202/324f42bf/attachment.htm>

From squid3 at treenet.co.nz  Thu Feb  2 09:37:48 2017
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 2 Feb 2017 22:37:48 +1300
Subject: [squid-users] Buy Certificates for Squid 'man in the middle'
In-Reply-To: <CAAdA2WMehhLN0ZinnyrnFXQT_ig3C0ZGbpZq49cEMwJ6YZeJ=w@mail.gmail.com>
References: <CA+wWuAxGYRCkEptmgQx6ezeH3O9MH5Z9QT4bNoo_Nnz+=G4Kig@mail.gmail.com>
 <789407094.71493603.1486024528225.JavaMail.root@zimbra4-e1>
 <CAAdA2WMehhLN0ZinnyrnFXQT_ig3C0ZGbpZq49cEMwJ6YZeJ=w@mail.gmail.com>
Message-ID: <1ebd64bb-a69f-0b0f-dc0b-ced3a68afaa4@treenet.co.nz>

On 2/02/2017 9:49 p.m., Odhiambo Washington wrote:
> So we can't even use the free certs from letsencrypt with Squid??
> 

Not for MITM / SSL-Bump no.

The very first clause of the purchase contract for the LetsEncrypt CA is:

"
By requesting, accepting, or using a Let?s Encrypt Certificate:

* You warrant to ISRG and the public-at-large that You are the
legitimate registrant of the Internet domain name that is, or is going
to be, the subject of Your Certificate, or that You are the duly
authorized agent of such registrant.
"

Meaning they can be used for explicit TLS-proxy or CDN reverse-proxy only.

If you have just used LetsEncrypt certs because of the hype about being
cheap, easy and everyone else is saying its good. I think it well worth
your time going to their site and reading that contract to which you
have bound your network.

For networks outside North America there are some legal implications
about signing judicial authority and your users method of legal redress
over to the USA government.

Amos



From eliezer at ngtech.co.il  Thu Feb  2 10:31:47 2017
From: eliezer at ngtech.co.il (Eliezer  Croitoru)
Date: Thu, 2 Feb 2017 12:31:47 +0200
Subject: [squid-users] SSL_bump and source IP
In-Reply-To: <1897130924.71362131.1486022602999.JavaMail.root@zimbra4-e1>
References: <1427329519.22421277.1484136240333.JavaMail.root@zimbra4-e1.priv.proxad.net>
 <1897130924.71362131.1486022602999.JavaMail.root@zimbra4-e1>
Message-ID: <074701d27d3f$926f8740$b74e95c0$@ngtech.co.il>

Have you considered an external_acl that will help you to do this by the mac address or by another way like a "bypass" portal?
With mac addresses DB you can know if the device is from one manufacturer or another.
The hackers in your network will always find a way to bypass ssl bump eventually since there are other ports but it's something.
I am not sure but if there was a way to find them by the form of the TLS hello then I believe it would be simple enough to identify these but I am not sure how possible is that.
I can write a pseudo in ruby that will help to identify vendors by MAC address based on:
https://github.com/royhills/arp-scan/blob/master/get-oui
https://github.com/joemiller/mac-to-vendor

Eliezer

----
Eliezer Croitoru
Linux System Administrator
Mobile: +972-5-28704261
Email: eliezer at ngtech.co.il


-----Original Message-----
From: squid-users [mailto:squid-users-bounces at lists.squid-cache.org] On Behalf Of FredB
Sent: Thursday, February 2, 2017 10:03 AM
To: squid-users at lists.squid-cache.org
Subject: Re: [squid-users] SSL_bump and source IP

So how I can manage computers without my CA ? (eg: laptop temporary connected) In my situation I have also some smartphones in some case, connected to my squids, how I can exclude them from SSLBump ?
I have already some ACL based on authentication (user azerty = with/without some rules)  

FredBb

_______________________________________________
squid-users mailing list
squid-users at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-users



From gksalil at gmail.com  Thu Feb  2 11:12:42 2017
From: gksalil at gmail.com (salil GK)
Date: Thu, 2 Feb 2017 16:42:42 +0530
Subject: [squid-users] can I authenticate client based on their certificate
Message-ID: <CAPACB-yaOF2bGEAjHXAtA+rtqsUrWHuB62Ss=L0RuWEK6O9gZA@mail.gmail.com>

Hello

   I have a requirement that I need to restrict access to the squid proxy (
forward proxy ) using the client certificate. All client certificates are
available in the squid servers. Could any body help me on solving this.

Thanks
~S
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20170202/c28b57eb/attachment.htm>

From fredbmail at free.fr  Thu Feb  2 11:37:59 2017
From: fredbmail at free.fr (FredB)
Date: Thu, 2 Feb 2017 12:37:59 +0100 (CET)
Subject: [squid-users] SSL_bump and source IP
In-Reply-To: <074701d27d3f$926f8740$b74e95c0$@ngtech.co.il>
Message-ID: <339708419.72111374.1486035479235.JavaMail.root@zimbra4-e1>

Thanks Eliezer

Unfortunately my "lan" is huge, many thousands of people, and MAC addresses are not known
I'm very surprised, I'm alone with this ? Nobody needs to exclude some users from SSLBump ?

Fredb 


From odhiambo at gmail.com  Thu Feb  2 11:51:02 2017
From: odhiambo at gmail.com (Odhiambo Washington)
Date: Thu, 2 Feb 2017 14:51:02 +0300
Subject: [squid-users] SSL_bump and source IP
In-Reply-To: <339708419.72111374.1486035479235.JavaMail.root@zimbra4-e1>
References: <074701d27d3f$926f8740$b74e95c0$@ngtech.co.il>
 <339708419.72111374.1486035479235.JavaMail.root@zimbra4-e1>
Message-ID: <CAAdA2WP1ewLma7WfR9vdrvTSihwKNbf1a91WUsjVpBvA0xpR=g@mail.gmail.com>

I am with you on this. Unfortunately, the way a certain subject turns out
not easy for someone in school, so does ssl_bump to me!

On 2 February 2017 at 14:37, FredB <fredbmail at free.fr> wrote:

> Thanks Eliezer
>
> Unfortunately my "lan" is huge, many thousands of people, and MAC
> addresses are not known
> I'm very surprised, I'm alone with this ? Nobody needs to exclude some
> users from SSLBump ?
>
> Fredb
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>



-- 
Best regards,
Odhiambo WASHINGTON,
Nairobi,KE
+254 7 3200 0004/+254 7 2274 3223
"Oh, the cruft."
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20170202/71a9ef3b/attachment.htm>

From snklusov at gmail.com  Thu Feb  2 12:22:34 2017
From: snklusov at gmail.com (Sergey Klusov)
Date: Thu, 2 Feb 2017 17:22:34 +0500
Subject: [squid-users] squid-users Digest, Vol 30, Issue 3
In-Reply-To: <mailman.7399.1485964312.20516.squid-users@lists.squid-cache.org>
References: <mailman.7399.1485964312.20516.squid-users@lists.squid-cache.org>
Message-ID: <a5e8d7e6-f928-173b-c6a0-d1149c35c2ef@gmail.com>


> Date: Thu, 2 Feb 2017 03:46:44 +1300
> From: Amos Jeffries <squid3 at treenet.co.nz>
> To: squid-users at lists.squid-cache.org
> Subject: Re: [squid-users] transparent http and https filter with
> 	white-list only
> Message-ID: <1d01efe0-83f8-2a91-c0ac-fd8ef769276f at treenet.co.nz>
> Content-Type: text/plain; charset=utf-8
>
> On 28/01/2017 12:36 a.m., Sergey Klusov wrote:
>> Hello. I'm trying to get working transparent setup allowing only certain
>> domains and have problem that in order to allow https "ssl_bump splice
>> allowed_domains" i have to "http_access allow all", thus allowing all
>> other http traffic through. Otherwise https traffic is not allowed at all.
>>
>> Here is my config:
>>
> Some comments inline to improve it.
>
> Also, what version of Squid are you using?
>   I will assume that you are following the best practice advice and using
> at least 3.5.19.  If not, please try to upgrade.
just installed from centos7 repo, using yum
Squid Cache: Version 3.5.20

>
>> =======config=======
>> http_port 10.96.243.1:3128 intercept options=NO_SSLv3:NO_SSLv2
>> http_port 10.96.243.1:3130 options=NO_SSLv3:NO_SSLv2
> Setting SSL-related options on http_port's is not useful when they are
> not doing SSL-Bump.

ok. just copy-pasted from some internet site about ssl_bump

>
>> https_port 10.96.243.1:3129 intercept ssl-bump
>> options=ALL:NO_SSLv3:NO_SSLv2 connection-auth=off
>> cert=/etc/squid/squidCA.pem
>> acl localnet src 10.0.0.0/8     # RFC1918 possible internal network
>>
>> acl SSL_ports port 443
>> acl Safe_ports port 80          # http
>> acl Safe_ports port 443         # https
>> acl CONNECT method CONNECT
>>
>> acl http_allow dstdomain "/etc/squid/http_allow_domains.txt"
>> acl https_allow ssl::server_name "/etc/squid/https_allow_domains.txt"
>>
>> sslproxy_cert_error allow all
>> sslproxy_flags DONT_VERIFY_PEER
> Not good. Remember this is a security protocol you are playing around with.
>
> Both of the above lines hide critical details you need to figure out
> what is going wrong. They can be useful as a spot-check (only!) to
> figure out if the problem is related to cert verification or something
> else. But DO NOT use them for regular traffic, not even testing traffic.
>
> You may find that there are certain _specific_ errors that you need to
> let through. Add the appropriate flags, SSL options, ACLs checks
> sslproxy_cert_error lines for those as needed, dont just ignore all
> possible errors like above does.

this setup only purpose is to just allow clients to connect only to 
small set of certain sites
i suppose client's browser will do all checks?

>> acl step1 at_step SslBump1
>> ssl_bump peek step1
>> ssl_bump splice https_allow
>> ssl_bump terminate all
>>
> Looks okay. Just to be clear you understand that:
>   The above means that the TLS/SSL is spliced only if the client SNI
> contains a domain in your whitelist.
>   All other traffic will be terminated ... maybe with an HTTP error page.
That's all i need. In fact i would prefer to not use squid at all for 
that purpose, but can't find any good free DPI solution.

>
>
>> cache deny all
>>
>> http_access deny !Safe_ports
>> http_access deny CONNECT !SSL_ports
>> http_access allow localhost manager
>> http_access deny manager
>>
>> http_access allow all http_allow
>> http_access allow all https_allow
> The ssl::server_name ACL will not work outside of the ssl_bump
> directive. Delete the above line.
Ok

>
> Also, I am not seeing is any line which permits the raw-IP CONNECT
> message which your Squid processes first to decide whether ssl_bump will
> be applied to the intercepted TCP connections.
>
>   That is why the "allow all" makes things "work". It lets those CONNECT
> request through.
>
> You can read the details about how bumping happens at
> <http://wiki.squid-cache.org/Features/SslPeekAndSplice#Processing_steps>
>   The CONNECT request mentioned in step 1.ii is your problem.
>
> To fix it in a very targeted way add these lines (mind the wrap sorry):
>
>   acl rawIP dstdom_regex
> ^(([0-9]+\.[0-9]+\.[0-9]+\.[0-9]+)|(\[([0-9a-f]+)?:([0-9a-f:]+)?:([0-9a-f]+|0-9\.]+)?\])):443$
>
>   acl bumpPort myportname 10.96.243.1:3129
>
>   http_access allow CONNECT bumpPort rawIP

i've worked around like this:

acl http_proto proto http
http_access allow !http

but will try your variant too
thanks.

>
>> http_access deny all
>>
>> always_direct allow all
>>
> That always_direct line is not useful. Remove it.
ok



From marcus.kool at urlfilterdb.com  Thu Feb  2 13:00:12 2017
From: marcus.kool at urlfilterdb.com (Marcus Kool)
Date: Thu, 2 Feb 2017 11:00:12 -0200
Subject: [squid-users] SSL_bump and source IP
In-Reply-To: <20170111115050.GC8862@fantomas.sk>
References: <722880849.22121122.1484130842412.JavaMail.root@zimbra4-e1.priv.proxad.net>
 <207529384.22130386.1484131031996.JavaMail.root@zimbra4-e1.priv.proxad.net>
 <20170111115050.GC8862@fantomas.sk>
Message-ID: <0bfbfccd-4e51-2f1b-a3c5-da63a29ef3ca@urlfilterdb.com>

The terminology may be confusing:
ssl_bump         means more or less "looking at HTTPS traffic"
ssl_bump splice  means "do not bump/intercept HTTPS traffic. No fake CA certificates are used"
ssl_bump bump    means "bump/intercept HTTPS traffic and use a fake CA certificate"

So the question is not about ssl_bump but about "ssl_bump bump".
To prevent the active bump, you need an acl to splice (leave the connection alone)
Something like this:

acl tls_s1_connect      at_step SslBump1

acl tls_vip_users    fill-in-your-details

ssl_bump splice    tls_vip_users	# do not peek/bump vip users
ssl_bump peek      tls_s1_connect	# peek at connections of other users
ssl_bump stare     all			# peek/stare at the server side of connections of other users
ssl_bump bump      all			# bump connections of other users

Marcus


On 11/01/17 09:50, Matus UHLAR - fantomas wrote:
> On 11.01.17 11:37, FredB wrote:
>> I'm searching a way to exclude an user (account) or an IP from my lan
>> I can exclude a destination domain to decryption with SSL_bump
>
> simply define an ACL and deny bumping it.
>
>> but not all requests from a specific source
>
> what do you mean here?
>
>> , maybe because I'm using x-forwarded ?
>
> x-forwarded-for has nothing to do with this
>
> Maybe you should rephrase the question so we understant you better.


From rentorbuy at yahoo.com  Thu Feb  2 13:05:02 2017
From: rentorbuy at yahoo.com (Vieri)
Date: Thu, 2 Feb 2017 13:05:02 +0000 (UTC)
Subject: [squid-users] squid reverse proxy (accelerator) for MS Exchange
 OWA
In-Reply-To: <3a990a52-6b57-a4b3-c7d0-9d683828faea@treenet.co.nz>
References: <1216522442.160455.1484870639800.ref@mail.yahoo.com>
 <1216522442.160455.1484870639800@mail.yahoo.com>
 <cdea0afa-c2a2-23c4-dbac-553a565e83f7@treenet.co.nz>
 <690032068.431187.1484905491833@mail.yahoo.com>
 <5517395f-eaa4-0b3a-34e9-66ea8d0122a1@treenet.co.nz>
 <1374872097.4095106.1485244925609@mail.yahoo.com>
 <94978436-88e5-fc84-9f3e-b8f1757402a5@measurement-factory.com>
 <2010900066.469374.1485330325639@mail.yahoo.com>
 <0f2d2e08-bbd1-656a-a83b-5bb159983cbd@measurement-factory.com>
 <757248455.1520252.1485425773189@mail.yahoo.com>
 <e13696a8-4540-bdd8-0634-902b737b82e4@measurement-factory.com>
 <1426807003.2339496.1485505865203@mail.yahoo.com>
 <3a990a52-6b57-a4b3-c7d0-9d683828faea@treenet.co.nz>
Message-ID: <1144746410.2885473.1486040702718@mail.yahoo.com>





----- Original Message -----
From: Amos Jeffries <squid3 at treenet.co.nz>
>
> Reason #1 is that the TLS protocol is a security protocol for securing a
> single 'hop' (just one TCP connection). So ideally TLS details would not
> be remembered at all, it's a dangerous thing in security to remember
> details in the middleware.
>
> Reason #2 is that Squid has passed on the 'terminate' signal to the
> client (curl).
> 
> As far as Squid is concerned, there is no "again" connection. There is a

> connection, which fails. The end.
>

>> # openssl s_client -connect 10.215.144.21:443 || openssl s_client
>> -connect 10.215.144.21:443 -tls1_1 || openssl s_client -connect
>> 10.215.144.21:443 -tls1> 
> Which brings us to reason #3; downgrade attacks.

> You may have heard of the POODLE attack.
>
> Squid (mostly) avoids the whole class of vulnerabilities by leaving the
> fallback decisions to the client whenever it can.


Thank you very much for explaining all this. It's quite clear now.
There's just one little thing that may be useful in the log, though.
I might be wrong or maybe everyone already knows what to try when they get a non-informative SSL handshake error but it would have helped me to get a "hint" from Squid telling me to try fiddling with the ssloptions and options flags. I realize now it's great if Squid follows the secure logic of "There is a connection, which fails. The end.", but whenever that happens (and the info is 0, only "handshake error") wouldn't it be safe to just print a hint line in the server's log?


Anyway, as I said before, I know what to do from now on so it's not a big deal. ;-)

Thanks again,

Vieri


From rentorbuy at yahoo.com  Thu Feb  2 13:09:16 2017
From: rentorbuy at yahoo.com (Vieri)
Date: Thu, 2 Feb 2017 13:09:16 +0000 (UTC)
Subject: [squid-users] renegotiation
References: <253156892.2868210.1486040956970.ref@mail.yahoo.com>
Message-ID: <253156892.2868210.1486040956970@mail.yahoo.com>

Hi,

I'm running Squid 4 beta.

# squid -v
Squid Cache: Version 4.0.17-20170122-r14968

I tested the following where Squid is listening on port 443 in accel mode.

# echo "R" | openssl s_client -connect 192.168.101.2:443 2>&1 3>&1 | grep RENEGOTIATING
RENEGOTIATING

How can I disable client renegotiation?

Vieri


From squid3 at treenet.co.nz  Thu Feb  2 13:08:40 2017
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Fri, 3 Feb 2017 02:08:40 +1300
Subject: [squid-users] squid-users Digest, Vol 30, Issue 3
In-Reply-To: <a5e8d7e6-f928-173b-c6a0-d1149c35c2ef@gmail.com>
References: <mailman.7399.1485964312.20516.squid-users@lists.squid-cache.org>
 <a5e8d7e6-f928-173b-c6a0-d1149c35c2ef@gmail.com>
Message-ID: <1389108c-962b-39b1-3c65-ca4d760bfde0@treenet.co.nz>

On 3/02/2017 1:22 a.m., Sergey Klusov wrote:
> 
>> Date: Thu, 2 Feb 2017 03:46:44 +1300
>> From: Amos Jeffries
>>
>> On 28/01/2017 12:36 a.m., Sergey Klusov wrote:
>>> Hello. I'm trying to get working transparent setup allowing only certain
>>> domains and have problem that in order to allow https "ssl_bump splice
>>> allowed_domains" i have to "http_access allow all", thus allowing all
>>> other http traffic through. Otherwise https traffic is not allowed at
>>> all.
>>>

...
>>>
>>> sslproxy_cert_error allow all
>>> sslproxy_flags DONT_VERIFY_PEER
>> Not good. Remember this is a security protocol you are playing around
>> with.
>>
>> Both of the above lines hide critical details you need to figure out
>> what is going wrong. They can be useful as a spot-check (only!) to
>> figure out if the problem is related to cert verification or something
>> else. But DO NOT use them for regular traffic, not even testing traffic.
>>
>> You may find that there are certain _specific_ errors that you need to
>> let through. Add the appropriate flags, SSL options, ACLs checks
>> sslproxy_cert_error lines for those as needed, dont just ignore all
>> possible errors like above does.
> 
> this setup only purpose is to just allow clients to connect only to
> small set of certain sites
> i suppose client's browser will do all checks?

What the browser sees is the stuff inside the spliced connections. Which
does not go near these sslproxy_* directives.

sslproxy_* are for Squid<->Internet connections. Errors here will never
get seen by any browser and in your setup will probably be from wrongly
bump'ed traffic, so you better be aware of those problems.

...
>>
>> To fix it in a very targeted way add these lines (mind the wrap sorry):
>>
>>   acl rawIP dstdom_regex
>> ^(([0-9]+\.[0-9]+\.[0-9]+\.[0-9]+)|(\[([0-9a-f]+)?:([0-9a-f:]+)?:([0-9a-f]+|0-9\.]+)?\])):443$
>>
>>
>>   acl bumpPort myportname 10.96.243.1:3129
>>
>>   http_access allow CONNECT bumpPort rawIP
> 
> i've worked around like this:
> 
> acl http_proto proto http
> http_access allow !http
> 
> but will try your variant too
> thanks.

FYI: my ACLs were being very strict. Ensuring only allow for CONNECT
requests which are coming from the port with ssl-bump, and also going to
port 443 (HTTPS).

Just allowing all non-http:// URLs through the proxy is not much better
than 'allow all'.

Amos


From fredbmail at free.fr  Thu Feb  2 13:25:57 2017
From: fredbmail at free.fr (FredB)
Date: Thu, 2 Feb 2017 14:25:57 +0100 (CET)
Subject: [squid-users] SSL_bump and source IP
In-Reply-To: <0bfbfccd-4e51-2f1b-a3c5-da63a29ef3ca@urlfilterdb.com>
Message-ID: <2077006377.72517991.1486041957274.JavaMail.root@zimbra4-e1>


> 
> acl tls_s1_connect      at_step SslBump1
> 
> acl tls_vip_users    fill-in-your-details
> 
> ssl_bump splice    tls_vip_users	# do not peek/bump vip users
> ssl_bump peek      tls_s1_connect	# peek at connections of other
> users
> ssl_bump stare     all			# peek/stare at the server side of
> connections of other users
> ssl_bump bump      all			# bump connections of other users
> 


Great, I will take a look there are some words about this in wiki ? 


From squid3 at treenet.co.nz  Thu Feb  2 13:30:48 2017
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Fri, 3 Feb 2017 02:30:48 +1300
Subject: [squid-users] renegotiation
In-Reply-To: <253156892.2868210.1486040956970@mail.yahoo.com>
References: <253156892.2868210.1486040956970.ref@mail.yahoo.com>
 <253156892.2868210.1486040956970@mail.yahoo.com>
Message-ID: <8b2fa54f-9d9c-dc03-ea04-435251e27849@treenet.co.nz>

On 3/02/2017 2:09 a.m., Vieri wrote:
> Hi,
> 
> I'm running Squid 4 beta.
> 
> # squid -v
> Squid Cache: Version 4.0.17-20170122-r14968
> 
> I tested the following where Squid is listening on port 443 in accel mode.
> 
> # echo "R" | openssl s_client -connect 192.168.101.2:443 2>&1 3>&1 | grep RENEGOTIATING
> RENEGOTIATING
> 
> How can I disable client renegotiation?
> 

For what reason is complete disable needed?

Renegotiating to an insecure version or cipher set is an issue to be
fixed by configuring tls-min-version=1.Y and tls-options= disabling
unwanted ciphers etc.

The potential DoS related to renegotiation is now prevented by rate
limiting.

The current generation of OpenSSL libraries (1.0+) all contain built-in
protection from older forms of renegotiate that had other CVE issues.

Amos



From rentorbuy at yahoo.com  Thu Feb  2 13:38:14 2017
From: rentorbuy at yahoo.com (Vieri)
Date: Thu, 2 Feb 2017 13:38:14 +0000 (UTC)
Subject: [squid-users] renegotiation
In-Reply-To: <8b2fa54f-9d9c-dc03-ea04-435251e27849@treenet.co.nz>
References: <253156892.2868210.1486040956970.ref@mail.yahoo.com>
 <253156892.2868210.1486040956970@mail.yahoo.com>
 <8b2fa54f-9d9c-dc03-ea04-435251e27849@treenet.co.nz>
Message-ID: <349877573.2882816.1486042694967@mail.yahoo.com>





----- Original Message -----
From: Amos Jeffries <squid3 at treenet.co.nz>
> Renegotiating to an insecure version or cipher set is an issue to be
> fixed by configuring tls-min-version=1.Y and tls-options= disabling
> unwanted ciphers etc.
> 
> The potential DoS related to renegotiation is now prevented by rate
> limiting.
> 
> The current generation of OpenSSL libraries (1.0+) all contain built-in
> protection from older forms of renegotiate that had other CVE issues.


Thanks again, Amos!


From creditu at eml.cc  Thu Feb  2 14:16:33 2017
From: creditu at eml.cc (creditu at eml.cc)
Date: Thu, 02 Feb 2017 07:16:33 -0700
Subject: [squid-users] Deny_Info
Message-ID: <1486044993.3500748.868029912.2259D6FA@webmail.messagingengine.com>

I have seen the use of deny_info done a few ways in regard to the
placement of the htttp_access line:

acl www dstdomain www.example.com

deny_info http://www.other.com www
http_access deny www

Or

http_access deny www
deny_info http://www.other.com www

The example on the squid acl page uses the first example.   Just curious
why I see the second in a lot of configs.


From vel21ripn at gmail.com  Thu Feb  2 14:34:49 2017
From: vel21ripn at gmail.com (Vitaly Lavrov)
Date: Thu, 2 Feb 2017 17:34:49 +0300
Subject: [squid-users] High utilization of CPU squid-3.5.23, squid-3.5.24
In-Reply-To: <4fd2ac33-ee17-5f80-cda7-c02544e4e735@gmail.com>
References: <9b1e504a-d14c-a025-84c8-54b5aa7d6fdd@gmail.com>
 <06e701d27cd3$058631b0$10929510$@ngtech.co.il>
 <4fd2ac33-ee17-5f80-cda7-c02544e4e735@gmail.com>
Message-ID: <69c87f52-0c0c-9ad7-0808-7556924d979d@gmail.com>

On 02.02.2017 00:45, Yuri Voinov wrote:
> Yes, it is require to perform extended diagnostics. Including the system
> level.
>
> BTW, it can also network IO. And, it is possible that even a slow DNS.
> Have to search.

squid-3.5.23 or squid-3.5.24 with bug-4606-v3.patch (http://bugs.squid-cache.org/show_bug.cgi?id=4606)

All cache_dir are located on SSD. Read-write up to 2 mbytes/s. IO wait < 3% - no problem.
Network average 20-40Mbit/s peak 110Mbit/s ( gigabit network ). 60-100 request/s
2 local dns-server.

This is very similar to http://bugs.squid-cache.org/show_bug.cgi?id=4477 (no solution)

>
> 02.02.2017 3:34, Eliezer Croitoru ?????:
>> I believe that the squid manager info page should give some clue about the number of concurrent requests.
>>
>> If it's above some number(300-400 and above) per second then removing the cache_dir from the server for a windows of a day will answer if it's a DISK IO bottle neck or something else.
>>
>> All The Bests,
>> Eliezer
>>
>> ----
>> Eliezer Croitoru
>> Linux System Administrator
>> Mobile: +972-5-28704261
>> Email: eliezer at ngtech.co.il
>>
>>
>> -----Original Message-----
>> From: squid-users [mailto:squid-users-bounces at lists.squid-cache.org] On Behalf Of Vitaly Lavrov
>> Sent: Wednesday, February 1, 2017 10:56 PM
>> To: squid-users at squid-cache.org
>> Subject: [squid-users] High utilization of CPU squid-3.5.23,squid-3.5.24
>>
>> Periodically squid begins to linearly increase the use of the CPU.
>> Sometimes this process reaches 100%. At random moment of time the CPU usage is reduced to 5-15%, and in the presence of client requests can again start linearly increasing use of CPU.
>>
>> In the protocols are no error messages.
>>
>> CPU consumption does not correlate with the number of requests and traffic.
>>
>> The increase CPU consumption from 0 to 60% occurs in about 4-5 hours, and to 100% for 6-8 hours.
>>
>> A typical graph of CPU usage can be viewed on http://devel.aanet.ru/tmp/squid-cpu-x.png
>>
>> With the "perf record -p` pgrep -f squid-1` - sleep 30" I have received the following information:
>>
>> At 100% CPU load most of the time took 3 calls
>>
>>   49.15% squid squid [.] MemObject :: dump
>>   25.11% squid squid [.] Mem_hdr :: freeDataUpto
>>   20.03% squid squid [.] Mem_hdr :: copy
>>
>> When loading CPU 30-60% most of the time took 3 calls
>>
>>   37.26% squid squid [.] Mem_node :: dataRange
>>   22.61% squid squid [.] Mem_hdr :: NodeCompare
>>   17.31% squid squid [.] Mem_hdr :: freeDataUpto
>>
>> What is it ? Is it possible to somehow fix it?
>>
>> System: slackware64 14.2
>>
>> sslbump not used. http only.
>>
>> Part of config:
>>
>> memory_pools off
>> memory_pools_limit 512 MB
>> cache_mem 768 MB
>> maximum_object_size_in_memory 64 KB
>> cache_dir ufs           /cache/sq_c1 16312 16 256
>> cache_dir ufs           /cache/sq_c2 16312 16 256
>> cache_dir ufs           /cache/sq_c3 16312 16 256
>> _______________________________________________
>> squid-users mailing list
>> squid-users at lists.squid-cache.org
>> http://lists.squid-cache.org/listinfo/squid-users
>>
>> _______________________________________________
>> squid-users mailing list
>> squid-users at lists.squid-cache.org
>> http://lists.squid-cache.org/listinfo/squid-users
>
>
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>



From eliezer at ngtech.co.il  Thu Feb  2 15:25:01 2017
From: eliezer at ngtech.co.il (Eliezer  Croitoru)
Date: Thu, 2 Feb 2017 17:25:01 +0200
Subject: [squid-users] SSL_bump and source IP
In-Reply-To: <339708419.72111374.1486035479235.JavaMail.root@zimbra4-e1>
References: <074701d27d3f$926f8740$b74e95c0$@ngtech.co.il>
 <339708419.72111374.1486035479235.JavaMail.root@zimbra4-e1>
Message-ID: <07a101d27d68$857517c0$905f4740$@ngtech.co.il>

You are not alone but you first need to define and understand your goals in a more technical way.
Squid can understand HTTP TLS\SSL IP and LAYER 2 MAC address.
If in one of these you can recognize that the client needs to be bypassed from SSL BUMP or interception in general you would be able to make it work.
If you have a portal that only android or mobile users can run and be identified at then you will need to first bump but give these specific users the option to somehow in the IP or LAYER 2 level be bypassed from being bumped.
If you have a WIFI network you can somehow make a trick with your radius server and usernames that will allow some clients((by IP) to be bypassed based on an external acl helper.

What do you think?

Eliezer

----
Eliezer Croitoru
Linux System Administrator
Mobile: +972-5-28704261
Email: eliezer at ngtech.co.il


-----Original Message-----
From: squid-users [mailto:squid-users-bounces at lists.squid-cache.org] On Behalf Of FredB
Sent: Thursday, February 2, 2017 1:38 PM
Cc: squid-users at lists.squid-cache.org
Subject: Re: [squid-users] SSL_bump and source IP

Thanks Eliezer

Unfortunately my "lan" is huge, many thousands of people, and MAC addresses are not known I'm very surprised, I'm alone with this ? Nobody needs to exclude some users from SSLBump ?

Fredb
_______________________________________________
squid-users mailing list
squid-users at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-users



From tmblue at gmail.com  Thu Feb  2 18:56:30 2017
From: tmblue at gmail.com (tmblue at gmail.com)
Date: Thu, 2 Feb 2017 10:56:30 -0800 (PST)
Subject: [squid-users] DiskThreadsDiskFile::openDone  squid 3.5.0.4
In-Reply-To: <1459222477090-4676833.post@n4.nabble.com>
References: <1419596549288-4668840.post@n4.nabble.com>
 <549F0575.2000801@ngtech.co.il> <1459222477090-4676833.post@n4.nabble.com>
Message-ID: <1486061790616-4681455.post@n4.nabble.com>

asnani_satish wrote
> This happens when size specified in cache_mem >= cache_dir 
> Example:
> cache_dir aufs /var/spool/squid 1000 32 512 
>    implies 1000 MB physical disk space allotted for cache in specified
> directory
> cache_mem 900 MB
>    cache size to be used by squid which must be less than the size
> specified in cache_dir directive.
> 
> Dont forget to restart squid
> 
> We cannot just ignore the error because if this error keeps on occurring
> the performance of squid degrades.

Are you sure???

i'm getting this error when turning back on a squid system that I had down
(out of production for 2 days). My config does not indicate an issue that
you advise to be the cause of the error.

cache_mem 1 GB
cache_dir aufs /cache 65000 16 256

I'm not seeing any issue but the error is rampant at this point

2017/02/02 10:58:59 kid1| 	/cache/07/49/003749FE
2017/02/02 10:59:00 kid1| DiskThreadsDiskFile::openDone: (2) No such file or
directory
2017/02/02 10:59:00 kid1| 	/cache/05/3E/00053EA0
2017/02/02 10:59:00 kid1| DiskThreadsDiskFile::openDone: (2) No such file or
directory
2017/02/02 10:59:00 kid1| 	/cache/0D/EB/005DEB19
2017/02/02 10:59:00 kid1| DiskThreadsDiskFile::openDone: (2) No such file or
directory
2017/02/02 10:59:00 kid1| 	/cache/02/27/0002273A




--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/DiskThreadsDiskFile-openDone-squid-3-5-0-4-tp4668840p4681455.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From rentorbuy at yahoo.com  Thu Feb  2 21:19:36 2017
From: rentorbuy at yahoo.com (Vieri)
Date: Thu, 2 Feb 2017 21:19:36 +0000 (UTC)
Subject: [squid-users] choose TLS version
References: <266626255.3314204.1486070376168.ref@mail.yahoo.com>
Message-ID: <266626255.3314204.1486070376168@mail.yahoo.com>

Hi,

Are the following two lines equivalent?

https_port ... options=NO_SSLv3,NO_SSLv2,NO_TLSv1_1,NO_TLSv1

https_port ... tls-min-version=1.2

Thanks,

Vieri


From rousskov at measurement-factory.com  Thu Feb  2 21:56:57 2017
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Thu, 2 Feb 2017 14:56:57 -0700
Subject: [squid-users] heart beet between squid peers
In-Reply-To: <CAPACB-xdUX8M=rSwX1PforkGEUnhB4AMNjm4ztKYWL7r7sPRWw@mail.gmail.com>
References: <CAPACB-xdUX8M=rSwX1PforkGEUnhB4AMNjm4ztKYWL7r7sPRWw@mail.gmail.com>
Message-ID: <15ddb254-3fb4-2f90-de41-56e0970da40d@measurement-factory.com>

On 02/01/2017 12:06 AM, salil GK wrote:
> I need to know whether the connectivity is through from squid
> child to squid parent.
...

> I need to know in both machines that the squid channel is active.
...

> if the heartbeat is exchanged successfully !!

It is not clear what you mean by "connectivity is through", "channel is
active", and "heartbeat is exchanged". You may want to describe what you
need in higher-level terms specific to your problem domain.

Please note that Squid peers do not normally exchange HTTP messages
unless there is a client request to be forwarded. There are various
optional features (e.g., cache_peer standby=N) that may create peer
traffic in the absence of requests, but it is not yet clear whether any
of those features are applicable to your use case.


If you just want to know whether there is at least one TCP connection
between two Squid instances, then you can use netstat or a similar tool
to find all connections to/from relevant addresses (and their state).

If you want to monitor the current peer state, I believe there is a
cache manager page for that, but that will only give you information
about one Squid's opinion, not both.

Alex.



From chip_pop at hotmail.com  Thu Feb  2 23:08:35 2017
From: chip_pop at hotmail.com (joseph)
Date: Thu, 2 Feb 2017 15:08:35 -0800 (PST)
Subject: [squid-users] DiskThreadsDiskFile::openDone  squid 3.5.0.4
In-Reply-To: <1459222477090-4676833.post@n4.nabble.com>
References: <1419596549288-4668840.post@n4.nabble.com>
 <549F0575.2000801@ngtech.co.il> <1459222477090-4676833.post@n4.nabble.com>
Message-ID: <1486076915859-4681458.post@n4.nabble.com>

lol
mostly of this ar wen vary changed lol
wen they work on vary as it should be  then most of those msg will go away
not 100% but most of them
im not going into detail but its the vary in header





--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/DiskThreadsDiskFile-openDone-squid-3-5-0-4-tp4668840p4681458.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From squid3 at treenet.co.nz  Thu Feb  2 23:34:05 2017
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Fri, 3 Feb 2017 12:34:05 +1300
Subject: [squid-users] Deny_Info
In-Reply-To: <1486044993.3500748.868029912.2259D6FA@webmail.messagingengine.com>
References: <1486044993.3500748.868029912.2259D6FA@webmail.messagingengine.com>
Message-ID: <a36c2a72-5eda-f63b-20f9-11393538afc1@treenet.co.nz>

On 3/02/2017 3:16 a.m., creditu wrote:
> I have seen the use of deny_info done a few ways in regard to the
> placement of the htttp_access line:
> 
> acl www dstdomain www.example.com
> 
> deny_info http://www.other.com www
> http_access deny www
> 
> Or
> 
> http_access deny www
> deny_info http://www.other.com www
> 
> The example on the squid acl page uses the first example.   Just curious
> why I see the second in a lot of configs.

Admin style preference. Possily one of the early config tutorials was
written that way and a lot of copy-n-paste has happened.

The only ordering requirement here is that the 'acl' line comes first.

Amos



From squid3 at treenet.co.nz  Thu Feb  2 23:38:44 2017
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Fri, 3 Feb 2017 12:38:44 +1300
Subject: [squid-users] Buy Certificates for Squid 'man in the middle'
In-Reply-To: <CA+wWuAye_3_euXZ2-L8RdEcPWFF32QymqRoR0PUUTF1rU32FVg@mail.gmail.com>
References: <CA+wWuAxGYRCkEptmgQx6ezeH3O9MH5Z9QT4bNoo_Nnz+=G4Kig@mail.gmail.com>
 <789407094.71493603.1486024528225.JavaMail.root@zimbra4-e1>
 <CAAdA2WMehhLN0ZinnyrnFXQT_ig3C0ZGbpZq49cEMwJ6YZeJ=w@mail.gmail.com>
 <1ebd64bb-a69f-0b0f-dc0b-ced3a68afaa4@treenet.co.nz>
 <CA+wWuAye_3_euXZ2-L8RdEcPWFF32QymqRoR0PUUTF1rU32FVg@mail.gmail.com>
Message-ID: <5a8424ea-4496-6f3b-974c-efe6aa9b417d@treenet.co.nz>

On 3/02/2017 1:43 a.m., angelv wrote:
> On Thu, Feb 2, 2017 at 4:37 AM, Amos Jeffries <squid3 at treenet.co.nz> wrote:
> 
>> On 2/02/2017 9:49 p.m., Odhiambo Washington wrote:
>>> So we can't even use the free certs from letsencrypt with Squid??
>>>
>>
>> Not for MITM / SSL-Bump no.
>>
>> The very first clause of the purchase contract for the LetsEncrypt CA is:
>>
>> "
>> By requesting, accepting, or using a Let?s Encrypt Certificate:
>>
>> * You warrant to ISRG and the public-at-large that You are the
>> legitimate registrant of the Internet domain name that is, or is going
>> to be, the subject of Your Certificate, or that You are the duly
>> authorized agent of such registrant.
>> "
>>
>> Meaning they can be used for explicit TLS-proxy or CDN reverse-proxy only.
>>
>> If you have just used LetsEncrypt certs because of the hype about being
>> cheap, easy and everyone else is saying its good. I think it well worth
>> your time going to their site and reading that contract to which you
>> have bound your network.
>>
>> For networks outside North America there are some legal implications
>> about signing judicial authority and your users method of legal redress
>> over to the USA government.
>>
> 
> I have certificates for my sub-domain
> 
> for example:
> 
> Proxy.subdomain.domain.com
> 
> I have the following files issued by Letsencrypt:
> 
> ca.cer
> proxy.subdomain.domain.com.conf          proxy.subdomain.domain.com.ssl.conf
> fullchain.cer                           proxy.subdomain.domain.com.csr
> proxy.subdomain.domain.com.cer           proxy.subdomain.domain.com.key
> 
> Can you use it?
> How do I make them usable for the proxy?
> 

https_port 3128 \
  cert=/path/to/proxy.subdomain.domain.com.cer \
  key=/path/to/proxy.subdomain.domain.com.key \
  cafile=/path/to/fullchain.cer

That is all. No SSL-Bump or other config.

Amos



From gksalil at gmail.com  Thu Feb  2 23:43:23 2017
From: gksalil at gmail.com (salil GK)
Date: Fri, 3 Feb 2017 05:13:23 +0530
Subject: [squid-users] heart beet between squid peers
In-Reply-To: <15ddb254-3fb4-2f90-de41-56e0970da40d@measurement-factory.com>
References: <CAPACB-xdUX8M=rSwX1PforkGEUnhB4AMNjm4ztKYWL7r7sPRWw@mail.gmail.com>
 <15ddb254-3fb4-2f90-de41-56e0970da40d@measurement-factory.com>
Message-ID: <CAPACB-yBefnyRnpqTP2vLzP=f-1kqHwsU6xF9jsK0nRqXnYfBg@mail.gmail.com>

Hello Alex

   Thanks for the reply

     what happens is - we provide an interface for the admin to set whether
forward proxy is enabled or not - and also specify which all peers need to
be involved in the squid chaining ( parent child ). If I have say 4
machines - A,B,C and D. Admin can decide machine A and B are the child  and
machine C and D as parents. Both A and B can use C and D as parents - for
load balancing and redundancy.

A -> parent -> C
A -> parent -> D
B -> parent -> C
B -> parent -> D

In the UI interface admin will specify this. When it is done, in the back
end I will create a tunnel and reconfigure squid config file and start
squid. Now in the UI I need to specify whether every thing is perfect and A
and B can talk to C and D seamlessly - or the traffic is perfect or not.
Only if the traffic between squid A and squid C and squid D are perfect, I
should mention that it is ACTIVE. For this I thought if have some mechanism
in both sides to do ping kind of functionality, it would be good.

What I found is squid client can be used for ping to other end. In the A
server I can use tunnel as the ping destination and C server I can directly
ping A. If this is through I guess set the channel as ACTIVE.  another
option is as Eliezer mentioned, I can use simple curl to verify this
connectivity

Thanks and regards
~S



On 3 February 2017 at 03:26, Alex Rousskov <rousskov at measurement-factory.com
> wrote:

> On 02/01/2017 12:06 AM, salil GK wrote:
> > I need to know whether the connectivity is through from squid
> > child to squid parent.
> ...
>
> > I need to know in both machines that the squid channel is active.
> ...
>
> > if the heartbeat is exchanged successfully !!
>
> It is not clear what you mean by "connectivity is through", "channel is
> active", and "heartbeat is exchanged". You may want to describe what you
> need in higher-level terms specific to your problem domain.
>
> Please note that Squid peers do not normally exchange HTTP messages
> unless there is a client request to be forwarded. There are various
> optional features (e.g., cache_peer standby=N) that may create peer
> traffic in the absence of requests, but it is not yet clear whether any
> of those features are applicable to your use case.
>
>
> If you just want to know whether there is at least one TCP connection
> between two Squid instances, then you can use netstat or a similar tool
> to find all connections to/from relevant addresses (and their state).
>
> If you want to monitor the current peer state, I believe there is a
> cache manager page for that, but that will only give you information
> about one Squid's opinion, not both.
>
> Alex.
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20170203/d6fb5e55/attachment.htm>

From squid3 at treenet.co.nz  Thu Feb  2 23:51:41 2017
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Fri, 3 Feb 2017 12:51:41 +1300
Subject: [squid-users] DiskThreadsDiskFile::openDone squid 3.5.0.4
In-Reply-To: <1486061790616-4681455.post@n4.nabble.com>
References: <1419596549288-4668840.post@n4.nabble.com>
 <549F0575.2000801@ngtech.co.il> <1459222477090-4676833.post@n4.nabble.com>
 <1486061790616-4681455.post@n4.nabble.com>
Message-ID: <5dbddc65-bdbf-60cf-e09c-1a334b504364@treenet.co.nz>

On 3/02/2017 7:56 a.m., tmblue at gmail.com wrote:
> asnani_satish wrote
>> This happens when size specified in cache_mem >= cache_dir 
>> Example:
>> cache_dir aufs /var/spool/squid 1000 32 512 
>>    implies 1000 MB physical disk space allotted for cache in specified
>> directory
>> cache_mem 900 MB
>>    cache size to be used by squid which must be less than the size
>> specified in cache_dir directive.
>>
>> Dont forget to restart squid
>>
>> We cannot just ignore the error because if this error keeps on occurring
>> the performance of squid degrades.
> 
> Are you sure???
> 
> i'm getting this error when turning back on a squid system that I had down
> (out of production for 2 days). My config does not indicate an issue that
> you advise to be the cause of the error.
> 
> cache_mem 1 GB
> cache_dir aufs /cache 65000 16 256
> 
> I'm not seeing any issue but the error is rampant at this point
> 
> 2017/02/02 10:58:59 kid1| 	/cache/07/49/003749FE
> 2017/02/02 10:59:00 kid1| DiskThreadsDiskFile::openDone: (2) No such file or
> directory
> 2017/02/02 10:59:00 kid1| 	/cache/05/3E/00053EA0
> 2017/02/02 10:59:00 kid1| DiskThreadsDiskFile::openDone: (2) No such file or
> directory
> 2017/02/02 10:59:00 kid1| 	/cache/0D/EB/005DEB19
> 2017/02/02 10:59:00 kid1| DiskThreadsDiskFile::openDone: (2) No such file or
> directory
> 2017/02/02 10:59:00 kid1| 	/cache/02/27/0002273A
> 

Squid uses a journal file swap.state to store changes to the cache. On
restart it is used to reconstruct the cache index fast. If you cause
Squid to shutdown very fast (kill -9, systemd crashing it, or
shutdown_lifetime too short) then it does not have time to fully update
the journal entries.

When it gets restarted after one of those abrupt shutdowns one of these
messages (not errors!) gets logged for each entry which the journal
indicates are present - but are not really there because their removal
was not recorded on shutdown.

NP: There are also things in the cache being orphaned then just
overwritten because the journal does not record them as existing. But no
messages about that because its not detected.

These messages should decrease exponentially proportional to your
traffics normal HIT rate. But in a very large cache like yours it still
may take many hours to become noticably slower and days to disappear
entirely. That is of course assuming your Squid does not have another
restart event to begin the process all over again.

Amos



From tmblue at gmail.com  Thu Feb  2 23:54:29 2017
From: tmblue at gmail.com (Tory M Blue)
Date: Thu, 2 Feb 2017 15:54:29 -0800
Subject: [squid-users] DiskThreadsDiskFile::openDone squid 3.5.0.4
In-Reply-To: <5dbddc65-bdbf-60cf-e09c-1a334b504364@treenet.co.nz>
References: <1419596549288-4668840.post@n4.nabble.com>
 <549F0575.2000801@ngtech.co.il>
 <1459222477090-4676833.post@n4.nabble.com>
 <1486061790616-4681455.post@n4.nabble.com>
 <5dbddc65-bdbf-60cf-e09c-1a334b504364@treenet.co.nz>
Message-ID: <CAEaSS0bDHi-2k45Lo6azZmrQcCsSsfJWcGeSGvtmTvFB=1tYbA@mail.gmail.com>

On Thu, Feb 2, 2017 at 3:51 PM, Amos Jeffries <squid3 at treenet.co.nz> wrote:

> On 3/02/2017 7:56 a.m., tmblue at gmail.com wrote:
> > asnani_satish wrote
> >> This happens when size specified in cache_mem >= cache_dir
> >> Example:
> >> cache_dir aufs /var/spool/squid 1000 32 512
> >>    implies 1000 MB physical disk space allotted for cache in specified
> >> directory
> >> cache_mem 900 MB
> >>    cache size to be used by squid which must be less than the size
> >> specified in cache_dir directive.
> >>
> >> Dont forget to restart squid
> >>
> >> We cannot just ignore the error because if this error keeps on occurring
> >> the performance of squid degrades.
> >
> > Are you sure???
> >
> > i'm getting this error when turning back on a squid system that I had
> down
> > (out of production for 2 days). My config does not indicate an issue that
> > you advise to be the cause of the error.
> >
> > cache_mem 1 GB
> > cache_dir aufs /cache 65000 16 256
> >
> > I'm not seeing any issue but the error is rampant at this point
> >
> > 2017/02/02 10:58:59 kid1|     /cache/07/49/003749FE
> > 2017/02/02 10:59:00 kid1| DiskThreadsDiskFile::openDone: (2) No such
> file or
> > directory
> > 2017/02/02 10:59:00 kid1|     /cache/05/3E/00053EA0
> > 2017/02/02 10:59:00 kid1| DiskThreadsDiskFile::openDone: (2) No such
> file or
> > directory
> > 2017/02/02 10:59:00 kid1|     /cache/0D/EB/005DEB19
> > 2017/02/02 10:59:00 kid1| DiskThreadsDiskFile::openDone: (2) No such
> file or
> > directory
> > 2017/02/02 10:59:00 kid1|     /cache/02/27/0002273A
> >
>
> Squid uses a journal file swap.state to store changes to the cache. On
> restart it is used to reconstruct the cache index fast. If you cause
> Squid to shutdown very fast (kill -9, systemd crashing it, or
> shutdown_lifetime too short) then it does not have time to fully update
> the journal entries.
>
> When it gets restarted after one of those abrupt shutdowns one of these
> messages (not errors!) gets logged for each entry which the journal
> indicates are present - but are not really there because their removal
> was not recorded on shutdown.
>
> NP: There are also things in the cache being orphaned then just
> overwritten because the journal does not record them as existing. But no
> messages about that because its not detected.
>
> These messages should decrease exponentially proportional to your
> traffics normal HIT rate. But in a very large cache like yours it still
> may take many hours to become noticably slower and days to disappear
> entirely. That is of course assuming your Squid does not have another
> restart event to begin the process all over again.
>
> Amos
>

As always thanks Amos

These are systemctl stop squid and systemctl start squid, so nothing
dramatic or nasty, should be shutting down cleanly

Tory
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20170202/d29bf8c7/attachment.htm>

From rousskov at measurement-factory.com  Fri Feb  3 00:19:26 2017
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Thu, 2 Feb 2017 17:19:26 -0700
Subject: [squid-users] heart beet between squid peers
In-Reply-To: <CAPACB-yBefnyRnpqTP2vLzP=f-1kqHwsU6xF9jsK0nRqXnYfBg@mail.gmail.com>
References: <CAPACB-xdUX8M=rSwX1PforkGEUnhB4AMNjm4ztKYWL7r7sPRWw@mail.gmail.com>
 <15ddb254-3fb4-2f90-de41-56e0970da40d@measurement-factory.com>
 <CAPACB-yBefnyRnpqTP2vLzP=f-1kqHwsU6xF9jsK0nRqXnYfBg@mail.gmail.com>
Message-ID: <07973c37-056a-6ddc-1798-4300e1b47dbc@measurement-factory.com>

On 02/02/2017 04:43 PM, salil GK wrote:

> we provide an interface for the admin to set
> whether forward proxy is enabled or not - and also specify which all
> peers need to be involved in the squid chaining ( parent child ). If I
> have say 4 machines - A,B,C and D. Admin can decide machine A and B are
> the child  and machine C and D as parents. Both A and B can use C and D
> as parents - for load balancing and redundancy. 
> 
> A -> parent -> C
> A -> parent -> D
> B -> parent -> C
> B -> parent -> D
> 
> In the UI interface admin will specify this. When it is done, in the
> back end I will create a tunnel and reconfigure squid config file and
> start squid. Now in the UI I need to specify whether every thing is
> perfect and A and B can talk to C and D seamlessly - or the traffic is
> perfect or not. Only if the traffic between squid A and squid C and
> squid D are perfect, I should mention that it is ACTIVE. For this I
> thought if have some mechanism in both sides to do ping kind of
> functionality, it would be good.

Thank you for detailing your problem!

Others on this list should be able to offer much better solutions, but
if you do not hear any, then I would use curl or wget(**) to send HTTP
requests to each child and have special rules in child squid.confs to
route those requests to the specified-in-the-request parent (probably by
matching a custom HTTP header). Your script can the analyze the Via
headers in each successful response to confirm that the transaction went
to the right parent.

Given your example above, four requests would be sufficient to confirm
connectivity. Please note that those requests cannot confirm whether A
is also _not_ using B as a parent and C is not using D as a parent (for
example). To prove all those "negatives", you will need a few more
requests (and possibly slightly fancier routing rules in squid.conf).

You may be able to send HTTP TRACE requests with Max-Forwards:1 header
if your Squids support that. That way, you will be testing the
connectivity between Squids without being dependent on the outside
connectivity and can analyze forwarded request headers as well, but this
is optional.


Needless to say, you can run a similar script/analysis every T minutes
if you wish to monitor connectivity continuously, after everything has
been configured. However, at runtime, your focus should be on monitoring
for errors, response times, hit ratios, and other measurements that may
reveal Squid problems.


FWIW, "pinging" in the reverse direction (e.g., from C to A) does not
make sense to me and should not be necessary to confirm connectivity
that normal requests will use.


HTH,

Alex.
(**) You may use squidclient as well but I do not recommend that because
it lacks polish and some advanced features that you may eventually find
useful.



From squid3 at treenet.co.nz  Fri Feb  3 04:29:55 2017
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Fri, 3 Feb 2017 17:29:55 +1300
Subject: [squid-users] choose TLS version
In-Reply-To: <266626255.3314204.1486070376168@mail.yahoo.com>
References: <266626255.3314204.1486070376168.ref@mail.yahoo.com>
 <266626255.3314204.1486070376168@mail.yahoo.com>
Message-ID: <0e4c379a-3db6-30c2-5d75-481e722a7454@treenet.co.nz>

On 3/02/2017 10:19 a.m., Vieri wrote:
> Hi,
> 
> Are the following two lines equivalent?
> 
> https_port ... options=NO_SSLv3,NO_SSLv2,NO_TLSv1_1,NO_TLSv1
> 
> https_port ... tls-min-version=1.2
> 

Not quite. SSL is still handled specially by options=.

The top line is equivalent to:

  options=NO_SSLv3 tls-min-version=1.2

(no NO_SSLv2 because Squid-4 does not support SSLv2 things - including
config settings.)

Amos



From snklusov at gmail.com  Fri Feb  3 12:01:00 2017
From: snklusov at gmail.com (Sergey Klusov)
Date: Fri, 3 Feb 2017 17:01:00 +0500
Subject: [squid-users] transparent http and https filter with white-list
 only
In-Reply-To: <mailman.7399.1485964312.20516.squid-users@lists.squid-cache.org>
References: <mailman.7399.1485964312.20516.squid-users@lists.squid-cache.org>
Message-ID: <6937125c-d172-c643-4b6e-7056db556727@gmail.com>


> Date: Thu, 2 Feb 2017 03:46:44 +1300
> From: Amos Jeffries <squid3 at treenet.co.nz>
> To: squid-users at lists.squid-cache.org
> Subject: Re: [squid-users] transparent http and https filter with
> 	white-list only
> Message-ID: <1d01efe0-83f8-2a91-c0ac-fd8ef769276f at treenet.co.nz>
> Content-Type: text/plain; charset=utf-8
>
> On 28/01/2017 12:36 a.m., Sergey Klusov wrote:
>> Hello. I'm trying to get working transparent setup allowing only certain
>> domains and have problem that in order to allow https "ssl_bump splice
>> allowed_domains" i have to "http_access allow all", thus allowing all
>> other http traffic through. Otherwise https traffic is not allowed at all.
>>
>> Here is my config:
>>
> Some comments inline to improve it.
>
> Also, what version of Squid are you using?
>   I will assume that you are following the best practice advice and using
> at least 3.5.19.  If not, please try to upgrade.
just installed from centos7 repo, using yum
Squid Cache: Version 3.5.20

>
>> =======config=======
>> http_port 10.96.243.1:3128 intercept options=NO_SSLv3:NO_SSLv2
>> http_port 10.96.243.1:3130 options=NO_SSLv3:NO_SSLv2
> Setting SSL-related options on http_port's is not useful when they are
> not doing SSL-Bump.

ok. just copy-pasted from some internet site about ssl_bump

>
>> https_port 10.96.243.1:3129 intercept ssl-bump
>> options=ALL:NO_SSLv3:NO_SSLv2 connection-auth=off
>> cert=/etc/squid/squidCA.pem
>> acl localnet src 10.0.0.0/8     # RFC1918 possible internal network
>>
>> acl SSL_ports port 443
>> acl Safe_ports port 80          # http
>> acl Safe_ports port 443         # https
>> acl CONNECT method CONNECT
>>
>> acl http_allow dstdomain "/etc/squid/http_allow_domains.txt"
>> acl https_allow ssl::server_name "/etc/squid/https_allow_domains.txt"
>>
>> sslproxy_cert_error allow all
>> sslproxy_flags DONT_VERIFY_PEER
> Not good. Remember this is a security protocol you are playing around with.
>
> Both of the above lines hide critical details you need to figure out
> what is going wrong. They can be useful as a spot-check (only!) to
> figure out if the problem is related to cert verification or something
> else. But DO NOT use them for regular traffic, not even testing traffic.
>
> You may find that there are certain _specific_ errors that you need to
> let through. Add the appropriate flags, SSL options, ACLs checks
> sslproxy_cert_error lines for those as needed, dont just ignore all
> possible errors like above does.

this setup only purpose is to just allow clients to connect only to 
small set of certain sites
i suppose client's browser will do all checks?

>> acl step1 at_step SslBump1
>> ssl_bump peek step1
>> ssl_bump splice https_allow
>> ssl_bump terminate all
>>
> Looks okay. Just to be clear you understand that:
>   The above means that the TLS/SSL is spliced only if the client SNI
> contains a domain in your whitelist.
>   All other traffic will be terminated ... maybe with an HTTP error page.
That's all i need. In fact i would prefer to not use squid at all for 
that purpose, but can't find any good free DPI solution.

>
>
>> cache deny all
>>
>> http_access deny !Safe_ports
>> http_access deny CONNECT !SSL_ports
>> http_access allow localhost manager
>> http_access deny manager
>>
>> http_access allow all http_allow
>> http_access allow all https_allow
> The ssl::server_name ACL will not work outside of the ssl_bump
> directive. Delete the above line.
Ok

>
> Also, I am not seeing is any line which permits the raw-IP CONNECT
> message which your Squid processes first to decide whether ssl_bump will
> be applied to the intercepted TCP connections.
>
>   That is why the "allow all" makes things "work". It lets those CONNECT
> request through.
>
> You can read the details about how bumping happens at
> <http://wiki.squid-cache.org/Features/SslPeekAndSplice#Processing_steps>
>   The CONNECT request mentioned in step 1.ii is your problem.
>
> To fix it in a very targeted way add these lines (mind the wrap sorry):
>
>   acl rawIP dstdom_regex
> ^(([0-9]+\.[0-9]+\.[0-9]+\.[0-9]+)|(\[([0-9a-f]+)?:([0-9a-f:]+)?:([0-9a-f]+|0-9\.]+)?\])):443$
>
>   acl bumpPort myportname 10.96.243.1:3129
>
>   http_access allow CONNECT bumpPort rawIP

i've worked around like this:

acl http_proto proto http
http_access allow !http

but will try your variant too
thanks.

>
>> http_access deny all
>>
>> always_direct allow all
>>
> That always_direct line is not useful. Remove it.
ok



From rafael.akchurin at diladele.com  Sat Feb  4 23:29:34 2017
From: rafael.akchurin at diladele.com (Rafael Akchurin)
Date: Sat, 4 Feb 2017 23:29:34 +0000
Subject: [squid-users] Squid 3.5.24 for Microsoft Windows 64-bit is available
Message-ID: <DB6PR0401MB268037F1E8DB412841FFB3058F4E0@DB6PR0401MB2680.eurprd04.prod.outlook.com>

Greetings everyone,

The CygWin based build of Squid proxy for Microsoft Windows version 3.5.23 is now available (amd64 only!).

* Original release notes are at http://www.squid-cache.org/Versions/v3/3.5/squid-3.5.24-RELEASENOTES.html .
* Ready to use MSI package can be downloaded from http://squid.diladele.com .
* List of open issues for the installer - https://github.com/diladele/squid-windows/issues

Thanks a lot for Squid developers for making this great software!

Please join our humble efforts to provide ready to run MSI installer for Squid on Microsoft Windows with all required dependencies at GitHub -
https://github.com/diladele/squid-windows . Please report all issues/bugs/feature requests at GitHub project. Issues about the *MSI installer only* can also be reported to support at diladele.com<mailto:support at diladele.com> .

Best regards,
Rafael Akchurin
Diladele B.V.
https://www.diladele.com

--
Please take a look at Web Safety - our ICAP based web filter server for Squid proxy.

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20170204/e8a09e28/attachment.htm>

From uhlar at fantomas.sk  Sun Feb  5 11:00:08 2017
From: uhlar at fantomas.sk (Matus UHLAR - fantomas)
Date: Sun, 5 Feb 2017 12:00:08 +0100
Subject: [squid-users] DiskThreadsDiskFile::openDone squid 3.5.0.4
In-Reply-To: <CAEaSS0bDHi-2k45Lo6azZmrQcCsSsfJWcGeSGvtmTvFB=1tYbA@mail.gmail.com>
References: <1419596549288-4668840.post@n4.nabble.com>
 <549F0575.2000801@ngtech.co.il>
 <1459222477090-4676833.post@n4.nabble.com>
 <1486061790616-4681455.post@n4.nabble.com>
 <5dbddc65-bdbf-60cf-e09c-1a334b504364@treenet.co.nz>
 <CAEaSS0bDHi-2k45Lo6azZmrQcCsSsfJWcGeSGvtmTvFB=1tYbA@mail.gmail.com>
Message-ID: <20170205110008.GA10839@fantomas.sk>

On 02.02.17 15:54, Tory M Blue wrote:
>> > I'm not seeing any issue but the error is rampant at this point
>> >
>> > 2017/02/02 10:58:59 kid1|     /cache/07/49/003749FE
>> > 2017/02/02 10:59:00 kid1| DiskThreadsDiskFile::openDone: (2) No such
>> file or
>> > directory
>> > 2017/02/02 10:59:00 kid1|     /cache/05/3E/00053EA0
>> > 2017/02/02 10:59:00 kid1| DiskThreadsDiskFile::openDone: (2) No such
>> file or
>> > directory
>> > 2017/02/02 10:59:00 kid1|     /cache/0D/EB/005DEB19
>> > 2017/02/02 10:59:00 kid1| DiskThreadsDiskFile::openDone: (2) No such
>> file or
>> > directory
>> > 2017/02/02 10:59:00 kid1|     /cache/02/27/0002273A

>> Squid uses a journal file swap.state to store changes to the cache. On
>> restart it is used to reconstruct the cache index fast. If you cause
>> Squid to shutdown very fast (kill -9, systemd crashing it, or
>> shutdown_lifetime too short) then it does not have time to fully update
>> the journal entries.

>These are systemctl stop squid and systemctl start squid, so nothing
>dramatic or nasty, should be shutting down cleanly

"should be" ... still depends on shutdown_lifetime and the systemd can still
be instructed to kill squid, look at squid.service

-- 
Matus UHLAR - fantomas, uhlar at fantomas.sk ; http://www.fantomas.sk/
Warning: I wish NOT to receive e-mail advertising to this address.
Varovanie: na tuto adresu chcem NEDOSTAVAT akukolvek reklamnu postu.
The 3 biggets disasters: Hiroshima 45, Tschernobyl 86, Windows 95


From varun.singh at gslab.com  Mon Feb  6 05:10:03 2017
From: varun.singh at gslab.com (Varun Singh)
Date: Mon, 6 Feb 2017 10:40:03 +0530
Subject: [squid-users] Basic HTTPS filtering via CONNECT in Squid
Message-ID: <CABUhpQWFcOPHS7Or9rybcs18iTjDpG=guajotsZaNc6mxykApA@mail.gmail.com>

Hi,
I have a Squid 3 installed on Ubuntu 16.04. It works perfectly as an
HTTP proxy server in transparent mode.
I wanted to know whether it can be configured to run as HTTPS proxy
server without ssl-bump i.e. without 'man in the middle attack'
technique.

I read the documentation page of HTTPS support. It says that when a
browser comes across an HTTPS website, it opens a TCP tunnel through
Squid to the origin server using CONNECT reuqest method.
With this setting the server can filter URLs based on URL scheme, URL
path and query string. The payload is still encrypted.
After that the documentation goes on to explain how can we use
SSL-bump to decrypt the payload.

Now, I only want setup basic HTTPS proxy via CONNECT tunnel in which
you can only filter URL path and string. I am not looking to setup
SSL-bump but still want to setup Squid for HTTPS filtering. I'm not
able to find a good tutorial for that.
Every tutorial I have found points to setting up SSL-bump.

If any of you have done a setup like this before please help me.

Following is my squid configuration:

acl SSL_ports port 443
acl Safe_ports port 80 # http
acl Safe_ports port 21 # ftp
acl Safe_ports port 443 # https
acl Safe_ports port 70 # gopher
acl Safe_ports port 210 # wais
acl Safe_ports port 1025-65535 # unregistered ports
acl Safe_ports port 280 # http-mgmt
acl Safe_ports port 488 # gss-http
acl Safe_ports port 591 # filemaker
acl Safe_ports port 777 # multiling http
acl CONNECT method CONNECT
acl blockads url_regex "/usr/local/squid/easylist"
http_access deny !Safe_ports
http_access deny CONNECT !SSL_ports
http_access allow localhost manager
http_access deny manager
http_access allow localhost
http_access deny blockads
http_access allow all
http_port 3128 transparent
coredump_dir /var/spool/squid
refresh_pattern ^ftp: 1440 20% 10080
refresh_pattern ^gopher: 1440 0% 1440
refresh_pattern -i (/cgi-bin/|\?) 0 0% 0
refresh_pattern (Release|Packages(.gz)*)$      0       20%     2880
refresh_pattern . 0 20% 4320

-- 
Regards,
Varun


From squid3 at treenet.co.nz  Mon Feb  6 06:09:22 2017
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Mon, 6 Feb 2017 19:09:22 +1300
Subject: [squid-users] Basic HTTPS filtering via CONNECT in Squid
In-Reply-To: <CABUhpQWFcOPHS7Or9rybcs18iTjDpG=guajotsZaNc6mxykApA@mail.gmail.com>
References: <CABUhpQWFcOPHS7Or9rybcs18iTjDpG=guajotsZaNc6mxykApA@mail.gmail.com>
Message-ID: <130414a3-f1c4-30c6-8233-c0386e2d4dcc@treenet.co.nz>

On 6/02/2017 6:10 p.m., Varun Singh wrote:
> Hi,
> I have a Squid 3 installed on Ubuntu 16.04. It works perfectly as an
> HTTP proxy server in transparent mode.
> I wanted to know whether it can be configured to run as HTTPS proxy
> server without ssl-bump i.e. without 'man in the middle attack'
> technique.

The Ubuntu package of squid/squid3 can tunnel CONNECT requests. That is
all. It has no support for anything more complicated.


> 
> I read the documentation page of HTTPS support. It says that when a
> browser comes across an HTTPS website, it opens a TCP tunnel through
> Squid to the origin server using CONNECT reuqest method.
> With this setting the server can filter URLs based on URL scheme, URL
> path and query string. The payload is still encrypted.

What documentation? it is wrong, or you are misunderstanding it. The URL
path?query is definitely *not* available without decrypting.

FWIW the squid wiki page on HTTPS documents all three of the
installation types that are all called "HTTPS".


> After that the documentation goes on to explain how can we use
> SSL-bump to decrypt the payload.
> 
> Now, I only want setup basic HTTPS proxy via CONNECT tunnel in which
> you can only filter URL path and string. I am not looking to setup
> SSL-bump but still want to setup Squid for HTTPS filtering. I'm not
> able to find a good tutorial for that.
> Every tutorial I have found points to setting up SSL-bump.

Because the only way to access more than hostname/IP and port is to decrypt.

Amos



From varun.singh at gslab.com  Mon Feb  6 13:46:07 2017
From: varun.singh at gslab.com (Varun Singh)
Date: Mon, 6 Feb 2017 19:16:07 +0530
Subject: [squid-users] Basic HTTPS filtering via CONNECT in Squid
In-Reply-To: <130414a3-f1c4-30c6-8233-c0386e2d4dcc@treenet.co.nz>
References: <CABUhpQWFcOPHS7Or9rybcs18iTjDpG=guajotsZaNc6mxykApA@mail.gmail.com>
 <130414a3-f1c4-30c6-8233-c0386e2d4dcc@treenet.co.nz>
Message-ID: <CABUhpQU5WC3ntYoNx+ZzyDmeE_=U466x_BmoNQMMQqhXsJehEA@mail.gmail.com>

On Mon, Feb 6, 2017 at 11:39 AM, Amos Jeffries <squid3 at treenet.co.nz> wrote:
> On 6/02/2017 6:10 p.m., Varun Singh wrote:
>> Hi,
>> I have a Squid 3 installed on Ubuntu 16.04. It works perfectly as an
>> HTTP proxy server in transparent mode.
>> I wanted to know whether it can be configured to run as HTTPS proxy
>> server without ssl-bump i.e. without 'man in the middle attack'
>> technique.
>
> The Ubuntu package of squid/squid3 can tunnel CONNECT requests. That is
> all. It has no support for anything more complicated.
>
>
>>
>> I read the documentation page of HTTPS support. It says that when a
>> browser comes across an HTTPS website, it opens a TCP tunnel through
>> Squid to the origin server using CONNECT reuqest method.
>> With this setting the server can filter URLs based on URL scheme, URL
>> path and query string. The payload is still encrypted.
>
> What documentation? it is wrong, or you are misunderstanding it. The URL
> path?query is definitely *not* available without decrypting.
>
> FWIW the squid wiki page on HTTPS documents all three of the
> installation types that are all called "HTTPS".
>
>
>> After that the documentation goes on to explain how can we use
>> SSL-bump to decrypt the payload.
>>
>> Now, I only want setup basic HTTPS proxy via CONNECT tunnel in which
>> you can only filter URL path and string. I am not looking to setup
>> SSL-bump but still want to setup Squid for HTTPS filtering. I'm not
>> able to find a good tutorial for that.
>> Every tutorial I have found points to setting up SSL-bump.
>
> Because the only way to access more than hostname/IP and port is to decrypt.
>
> Amos
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users


Hi,
Please find my reply inline:

> What documentation? it is wrong, or you are misunderstanding it. The URL
> path?query is definitely *not* available without decrypting.
>

Correct, I mis-read it.


> Because the only way to access more than hostname/IP and port is to decrypt.

Okay. In that, case I am okay with only being able to see hostname/IP and port.
But whenever I search for setting up HTTPS with Squid, I always come
across SSL-bump.
Could you point me to a tutorial which perform just basic HTTPS setup?

What I have tried so far is, configuring Squid to listen to port 3129
to expect HTTPS traffic. I did this by adding following line to
squid.conf:

https_port 3129

Once this was done, I redirected all the traffic coming to port 443 to
port 3129 using iptables. This is because my clients connect to proxy
via VPN.
But this had no effect. After connecting clients to proxy, when I try
to access an HTTPS website, the clients get no response and nothing
shows in access.log file. The browser behaves as if it could not
connect to internet.

Please note that this setup works perfectly for HTTP requests. Only
HTTPS requests give problems.



FYI, by documentation I was referring to below link:
http://wiki.squid-cache.org/Features/HTTPS


-- 
Regards,
Varun


From dante01010 at gmail.com  Mon Feb  6 14:40:22 2017
From: dante01010 at gmail.com (=?UTF-8?Q?Dante_F._B._Col=c3=b2?=)
Date: Mon, 6 Feb 2017 12:40:22 -0200
Subject: [squid-users] HTTPS sites specifics URL
Message-ID: <6e10d22e-004a-afe8-2e6e-9c5e8a9ae11a@gmail.com>

Hello Everyone

I have a question , probably a noob one , i 'm trying to allow some 
https sites with specific URL's  (i mean https://domain.tld/blablabla) 
but https sites are working  only with the domain part , what i have to 
do to make this work ?

Regards

Dante F. B. Col?





From karuhangajude at outlook.com  Mon Feb  6 15:49:37 2017
From: karuhangajude at outlook.com (Jude Karuhanga)
Date: Mon, 6 Feb 2017 15:49:37 +0000
Subject: [squid-users] On using Parent Proxies
In-Reply-To: <AM2PR07MB0834789FB47353AD93CA780BAE400@AM2PR07MB0834.eurprd07.prod.outlook.com>
References: <AM2PR07MB0834789FB47353AD93CA780BAE400@AM2PR07MB0834.eurprd07.prod.outlook.com>
Message-ID: <AM2PR07MB08343A6D45A789B543CD64A2AE400@AM2PR07MB0834.eurprd07.prod.outlook.com>

Hello there,


I am new to squid, and would appreciate some help on configuring my server to balance traffic between two servers that serve as Gateways to the Internet. I attach a simplified configuration file for the squid service on the server. I have managed to get the service to work using the default gateway, but the redirection through the cache_peer does not work: I am unable to access the websites included, even though both servers are visible.


Thank you.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20170206/f91f80cb/attachment.htm>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: squid.conf - parent proxies
Type: application/octet-stream
Size: 3167 bytes
Desc: squid.conf - parent proxies
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20170206/f91f80cb/attachment.obj>

From leolistas at solutti.com.br  Mon Feb  6 16:28:49 2017
From: leolistas at solutti.com.br (Leonardo Rodrigues)
Date: Mon, 6 Feb 2017 14:28:49 -0200
Subject: [squid-users] HTTPS sites specifics URL
In-Reply-To: <6e10d22e-004a-afe8-2e6e-9c5e8a9ae11a@gmail.com>
References: <6e10d22e-004a-afe8-2e6e-9c5e8a9ae11a@gmail.com>
Message-ID: <9c1de504-311b-96b8-e927-b4b2f9ee1e28@solutti.com.br>


     That's correct, when not using SSL-Bump feature (that's the one 
you're looking for), squid will only see the domain part. All the rest 
of the URL is crypted and visible only to the client (browser) and the 
server on the other side, the only two parts involved on that crypto 
session.

     To enable squid to see the whole URL and be able to do full 
filtering on HTTPS requests, you're looking for SSL-Bump feature. Google 
for it, there's a LOT of tutorials and mailing list messages on that.


Em 06/02/17 12:40, Dante F. B. Col? escreveu:
> Hello Everyone
>
> I have a question , probably a noob one , i 'm trying to allow some 
> https sites with specific URL's  (i mean https://domain.tld/blablabla) 
> but https sites are working  only with the domain part , what i have 
> to do to make this work ?
>

-- 


	Atenciosamente / Sincerily,
	Leonardo Rodrigues
	Solutti Tecnologia
	http://www.solutti.com.br

	Minha armadilha de SPAM, N?O mandem email
	gertrudes at solutti.com.br
	My SPAMTRAP, do not email it





From anonymouscross at gmail.com  Mon Feb  6 16:47:23 2017
From: anonymouscross at gmail.com (Anonymous cross)
Date: Mon, 6 Feb 2017 10:47:23 -0600
Subject: [squid-users] Logs to confirm packets dropped/not forwarded by squid
Message-ID: <CAHAQXcpeQY1hcsJBcxCL8Fk6Bs07JFr-HxJvA=P7o+yR1y4YVw@mail.gmail.com>

Hi,
Is there any way to find out the packets dropped/not forwarded by squid?
Is there any debug logs/option to enable it?


Regards,
Anonymous cross.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20170206/3477a4b8/attachment.htm>

From uhlar at fantomas.sk  Mon Feb  6 16:54:14 2017
From: uhlar at fantomas.sk (Matus UHLAR - fantomas)
Date: Mon, 6 Feb 2017 17:54:14 +0100
Subject: [squid-users] Logs to confirm packets dropped/not forwarded by
 squid
In-Reply-To: <CAHAQXcpeQY1hcsJBcxCL8Fk6Bs07JFr-HxJvA=P7o+yR1y4YVw@mail.gmail.com>
References: <CAHAQXcpeQY1hcsJBcxCL8Fk6Bs07JFr-HxJvA=P7o+yR1y4YVw@mail.gmail.com>
Message-ID: <20170206165414.GA5717@fantomas.sk>

On 06.02.17 10:47, Anonymous cross wrote:
>Is there any way to find out the packets dropped/not forwarded by squid?
>Is there any debug logs/option to enable it?

squid does not work with packets. It works with connections.

-- 
Matus UHLAR - fantomas, uhlar at fantomas.sk ; http://www.fantomas.sk/
Warning: I wish NOT to receive e-mail advertising to this address.
Varovanie: na tuto adresu chcem NEDOSTAVAT akukolvek reklamnu postu.
Linux IS user friendly, it's just selective who its friends are...


From anonymouscross at gmail.com  Mon Feb  6 17:26:13 2017
From: anonymouscross at gmail.com (Anonymous cross)
Date: Mon, 6 Feb 2017 11:26:13 -0600
Subject: [squid-users] Logs to confirm packets dropped/not forwarded by
	squid
In-Reply-To: <20170206165414.GA5717@fantomas.sk>
References: <CAHAQXcpeQY1hcsJBcxCL8Fk6Bs07JFr-HxJvA=P7o+yR1y4YVw@mail.gmail.com>
 <20170206165414.GA5717@fantomas.sk>
Message-ID: <CAHAQXcrjCRv-dYXuZk0sF3fpfdhps57Pf1bXPNhn9gxbED1-mg@mail.gmail.com>

 Is there any way to find the connections dropped/not forwarded by Squid? I
could see HTTP GET is forwarded to squid but it's not initiating a
connection with webserver

On Mon, Feb 6, 2017 at 10:54 AM, Matus UHLAR - fantomas <uhlar at fantomas.sk>
wrote:

> On 06.02.17 10:47, Anonymous cross wrote:
>
>> Is there any way to find out the packets dropped/not forwarded by squid?
>> Is there any debug logs/option to enable it?
>>
>
> squid does not work with packets. It works with connections.
>
> --
> Matus UHLAR - fantomas, uhlar at fantomas.sk ; http://www.fantomas.sk/
> Warning: I wish NOT to receive e-mail advertising to this address.
> Varovanie: na tuto adresu chcem NEDOSTAVAT akukolvek reklamnu postu.
> Linux IS user friendly, it's just selective who its friends are...
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>



-- 
Regards,
Anonymous cross.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20170206/a2cb7d98/attachment.htm>

From Antony.Stone at squid.open.source.it  Mon Feb  6 17:29:31 2017
From: Antony.Stone at squid.open.source.it (Antony Stone)
Date: Mon, 6 Feb 2017 17:29:31 +0000
Subject: [squid-users] Logs to confirm packets dropped/not forwarded by
	squid
In-Reply-To: <CAHAQXcrjCRv-dYXuZk0sF3fpfdhps57Pf1bXPNhn9gxbED1-mg@mail.gmail.com>
References: <CAHAQXcpeQY1hcsJBcxCL8Fk6Bs07JFr-HxJvA=P7o+yR1y4YVw@mail.gmail.com>
 <20170206165414.GA5717@fantomas.sk>
 <CAHAQXcrjCRv-dYXuZk0sF3fpfdhps57Pf1bXPNhn9gxbED1-mg@mail.gmail.com>
Message-ID: <201702061729.31522.Antony.Stone@squid.open.source.it>

On Monday 06 Feb 2017 at 17:26, Anonymous cross wrote:

>  Is there any way to find the connections dropped/not forwarded by Squid? I
> could see HTTP GET is forwarded to squid but it's not initiating a
> connection with webserver

Have you looked in access.log for that connection?


Antony.

-- 
Bill Gates has personally assured the Spanish Academy that he will never allow 
the upside-down question mark to disappear from Microsoft word-processing 
programs, which must be reassuring for millions of Spanish-speaking people, 
though just a piddling afterthought as far as he's concerned.

 - Lynne Truss, "Eats, Shoots and Leaves"

                                                   Please reply to the list;
                                                         please *don't* CC me.


From anonymouscross at gmail.com  Mon Feb  6 17:34:35 2017
From: anonymouscross at gmail.com (Anonymous cross)
Date: Mon, 6 Feb 2017 11:34:35 -0600
Subject: [squid-users] Logs to confirm packets dropped/not forwarded by
	squid
In-Reply-To: <201702061729.31522.Antony.Stone@squid.open.source.it>
References: <CAHAQXcpeQY1hcsJBcxCL8Fk6Bs07JFr-HxJvA=P7o+yR1y4YVw@mail.gmail.com>
 <20170206165414.GA5717@fantomas.sk>
 <CAHAQXcrjCRv-dYXuZk0sF3fpfdhps57Pf1bXPNhn9gxbED1-mg@mail.gmail.com>
 <201702061729.31522.Antony.Stone@squid.open.source.it>
Message-ID: <CAHAQXcqJqSMPCKCaHsEcS3JkDLwxBJLdmqdzpYrQepCAOvYwAg@mail.gmail.com>

I don't find any entry in access.log for that connection.


Regards,
Saravanan N

On Mon, Feb 6, 2017 at 11:29 AM, Antony Stone <
Antony.Stone at squid.open.source.it> wrote:

> On Monday 06 Feb 2017 at 17:26, Anonymous cross wrote:
>
> >  Is there any way to find the connections dropped/not forwarded by
> Squid? I
> > could see HTTP GET is forwarded to squid but it's not initiating a
> > connection with webserver
>
> Have you looked in access.log for that connection?
>
>
> Antony.
>
> --
> Bill Gates has personally assured the Spanish Academy that he will never
> allow
> the upside-down question mark to disappear from Microsoft word-processing
> programs, which must be reassuring for millions of Spanish-speaking people,
> though just a piddling afterthought as far as he's concerned.
>
>  - Lynne Truss, "Eats, Shoots and Leaves"
>
>                                                    Please reply to the
> list;
>                                                          please *don't* CC
> me.
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>



-- 
Regards,
Anonymous cross.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20170206/16f04101/attachment.htm>

From Antony.Stone at squid.open.source.it  Mon Feb  6 17:45:20 2017
From: Antony.Stone at squid.open.source.it (Antony Stone)
Date: Mon, 6 Feb 2017 17:45:20 +0000
Subject: [squid-users] Logs to confirm packets dropped/not forwarded by
	squid
In-Reply-To: <CAHAQXcqJqSMPCKCaHsEcS3JkDLwxBJLdmqdzpYrQepCAOvYwAg@mail.gmail.com>
References: <CAHAQXcpeQY1hcsJBcxCL8Fk6Bs07JFr-HxJvA=P7o+yR1y4YVw@mail.gmail.com>
 <201702061729.31522.Antony.Stone@squid.open.source.it>
 <CAHAQXcqJqSMPCKCaHsEcS3JkDLwxBJLdmqdzpYrQepCAOvYwAg@mail.gmail.com>
Message-ID: <201702061745.20818.Antony.Stone@squid.open.source.it>

On Monday 06 Feb 2017 at 17:34, Anonymous cross wrote:

> I don't find any entry in access.log for that connection.

Okay, maybe you should explain a little more about what you mean by "I could 
see HTTP GET is forwarded to Squid" - does "forwarded" mean you're using 
intercept mode, and if it does, how are you forwarding the packets to Squid?

How did you "see the HTTP GET", and where were you looking for it?


Antony.

-- 
Wanted: telepath.   You know where to apply.

                                                   Please reply to the list;
                                                         please *don't* CC me.


From anonymouscross at gmail.com  Mon Feb  6 18:04:53 2017
From: anonymouscross at gmail.com (Anonymous cross)
Date: Mon, 6 Feb 2017 12:04:53 -0600
Subject: [squid-users] Logs to confirm packets dropped/not forwarded by
	squid
In-Reply-To: <201702061745.20818.Antony.Stone@squid.open.source.it>
References: <CAHAQXcpeQY1hcsJBcxCL8Fk6Bs07JFr-HxJvA=P7o+yR1y4YVw@mail.gmail.com>
 <201702061729.31522.Antony.Stone@squid.open.source.it>
 <CAHAQXcqJqSMPCKCaHsEcS3JkDLwxBJLdmqdzpYrQepCAOvYwAg@mail.gmail.com>
 <201702061745.20818.Antony.Stone@squid.open.source.it>
Message-ID: <CAHAQXcpAZ0tA8enURCOSHeoObheqjFf73zX0ZKeh70J2NuasKA@mail.gmail.com>

We are using squid with tproxy4. All the packets destined to port 80 are
forwarded to tproxy port 3129 using the below guide

http://wiki.squid-cache.org/Features/Tproxy4

Normal HTTP GET requests forwarded to Squid are working fine. But we do see
problems with TCP segments which holds HTTP data. I am not sure how squid
TCP reassembly logic works. I need some way to find out where it's getting
dropped in squid.



On Mon, Feb 6, 2017 at 11:45 AM, Antony Stone <
Antony.Stone at squid.open.source.it> wrote:

> On Monday 06 Feb 2017 at 17:34, Anonymous cross wrote:
>
> > I don't find any entry in access.log for that connection.
>
> Okay, maybe you should explain a little more about what you mean by "I
> could
> see HTTP GET is forwarded to Squid" - does "forwarded" mean you're using
> intercept mode, and if it does, how are you forwarding the packets to
> Squid?
>
> How did you "see the HTTP GET", and where were you looking for it?
>
>
> Antony.
>
> --
> Wanted: telepath.   You know where to apply.
>
>                                                    Please reply to the
> list;
>                                                          please *don't* CC
> me.
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>



-- 
Regards,
Anonymous cross.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20170206/188dae53/attachment.htm>

From squid3 at treenet.co.nz  Mon Feb  6 22:18:36 2017
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Tue, 7 Feb 2017 11:18:36 +1300
Subject: [squid-users] Basic HTTPS filtering via CONNECT in Squid
In-Reply-To: <CABUhpQU5WC3ntYoNx+ZzyDmeE_=U466x_BmoNQMMQqhXsJehEA@mail.gmail.com>
References: <CABUhpQWFcOPHS7Or9rybcs18iTjDpG=guajotsZaNc6mxykApA@mail.gmail.com>
 <130414a3-f1c4-30c6-8233-c0386e2d4dcc@treenet.co.nz>
 <CABUhpQU5WC3ntYoNx+ZzyDmeE_=U466x_BmoNQMMQqhXsJehEA@mail.gmail.com>
Message-ID: <d9a1230e-9773-b4a2-c606-cbd1834364d0@treenet.co.nz>

On 7/02/2017 2:46 a.m., Varun Singh wrote:
> On Mon, Feb 6, 2017 at 11:39 AM, Amos Jeffries wrote:
> 
> Hi,
> Please find my reply inline:
> 
>> What documentation? it is wrong, or you are misunderstanding it. The URL
>> path?query is definitely *not* available without decrypting.
>>
> 
> Correct, I mis-read it.
> 
> 
>> Because the only way to access more than hostname/IP and port is to decrypt.
> 
> Okay. In that, case I am okay with only being able to see hostname/IP and port.
> But whenever I search for setting up HTTPS with Squid, I always come
> across SSL-bump.
> Could you point me to a tutorial which perform just basic HTTPS setup?

The Squid default config handles as much of HTTPS as can be handled
without the SSL-Bump feature.

> 
> What I have tried so far is, configuring Squid to listen to port 3129
> to expect HTTPS traffic. I did this by adding following line to
> squid.conf:
> 
> https_port 3129
> 
> Once this was done, I redirected all the traffic coming to port 443 to
> port 3129 using iptables. This is because my clients connect to proxy
> via VPN.

Since you are intercepting port 443 that port is missing the 'intercept'
flag. Also, interceptig port 443 requires SSL-Bump.


> But this had no effect. After connecting clients to proxy, when I try
> to access an HTTPS website, the clients get no response and nothing
> shows in access.log file. The browser behaves as if it could not
> connect to internet.
> 
> Please note that this setup works perfectly for HTTP requests. Only
> HTTPS requests give problems.
> 

Port 80 (HTTP) and port 443 (HTTPS) have totally different transport
protocols. The port 443 one is designed to break when being intercepted.


> 
> FYI, by documentation I was referring to below link:
> http://wiki.squid-cache.org/Features/HTTPS
> 


Amos


From baborucki at gmail.com  Mon Feb  6 22:27:21 2017
From: baborucki at gmail.com (boruc)
Date: Mon, 6 Feb 2017 14:27:21 -0800 (PST)
Subject: [squid-users] Is it possible to modify cached object?
In-Reply-To: <c616d7dc-4781-2eb8-ecd8-6a976b0e1ed2@treenet.co.nz>
References: <1485715924346-4681379.post@n4.nabble.com>
 <bc9151b9-df29-7c16-1460-08082e67243a@treenet.co.nz>
 <1485855330673-4681390.post@n4.nabble.com>
 <201701311052.39956.Antony.Stone@squid.open.source.it>
 <1485880095814-4681392.post@n4.nabble.com>
 <201701311744.42998.Antony.Stone@squid.open.source.it>
 <1485883154937-4681394.post@n4.nabble.com>
 <201701311828.01550.Antony.Stone@squid.open.source.it>
 <1485887657504-4681396.post@n4.nabble.com>
 <c616d7dc-4781-2eb8-ecd8-6a976b0e1ed2@treenet.co.nz>
Message-ID: <1486420041932-4681483.post@n4.nabble.com>

Hi again,

So I've installed squid 3.5.12, libecap 1.0.1 and sample adapter 1.0.0. I
was able to test a simple "the -> a" replacement shown in  documentation
<http://www.e-cap.org/Documentation>  . Next I tried some HTML injection,
also from documentation. However, it didn't work and I got error like below:

2017/02/06 17:56:30 kid1| Loading Squid module from
'/usr/local/lib/ecap_adapter_modifying.so'
2017/02/06 17:56:30 kid1| Squid plugin modules loaded: 1
2017/02/06 17:56:30 kid1| Adaptation support is on
2017/02/06 17:56:30 kid1| ERROR: failed to start essential eCAP service:
ecap://e-cap.org/ecap/services/sample/modifying:
Modifying Adapter: configuration error: unsupported configuration parameter:
replacement-src
2017/02/06 17:56:30 kid1| FATAL: dying from an unhandled exception:
Modifying Adapter: configuration error: unsupported configuration parameter:
replacement-src

What is more, when I try regular text replacement and I use space in
replacement text I get no error message in cache.log, but browser says that
"proxy server is refusing connections" and executing command "squid -k
check" gives me this:

2017/02/06 19:11:11| /etc/squid/squid.conf:84: Duplicate option "uri" in
adaptation service definition
FATAL: /etc/squid/squid.conf:84: malformed adaptation service configuration
Squid Cache (Version 3.5.12): Terminated abnormally.

I managed to find a  bugfix <https://bugs.launchpad.net/ecap/+bug/1634621>  
to *replacement-src* problem in  this thread
<https://answers.launchpad.net/ecap/+question/402946>  , but how should I
appy this? Just add lines with "+" at the beginning and recompile sources
one more time?

Will it work with spaces in replacement text? My main goal is to add a
paragraph to requested page, with link or image, but tags for those require
spaces in string. Or maybe I have to write my own adapter to this?

Thank you for your time.



--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/Is-it-possible-to-modify-cached-object-tp4681073p4681483.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From squid3 at treenet.co.nz  Mon Feb  6 22:44:28 2017
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Tue, 7 Feb 2017 11:44:28 +1300
Subject: [squid-users] Logs to confirm packets dropped/not forwarded by
 squid
In-Reply-To: <CAHAQXcpAZ0tA8enURCOSHeoObheqjFf73zX0ZKeh70J2NuasKA@mail.gmail.com>
References: <CAHAQXcpeQY1hcsJBcxCL8Fk6Bs07JFr-HxJvA=P7o+yR1y4YVw@mail.gmail.com>
 <201702061729.31522.Antony.Stone@squid.open.source.it>
 <CAHAQXcqJqSMPCKCaHsEcS3JkDLwxBJLdmqdzpYrQepCAOvYwAg@mail.gmail.com>
 <201702061745.20818.Antony.Stone@squid.open.source.it>
 <CAHAQXcpAZ0tA8enURCOSHeoObheqjFf73zX0ZKeh70J2NuasKA@mail.gmail.com>
Message-ID: <256d4781-8b41-91c0-8352-073811fb8cfe@treenet.co.nz>

On 7/02/2017 7:04 a.m., Anonymous cross wrote:
> We are using squid with tproxy4. All the packets destined to port 80 are
> forwarded to tproxy port 3129 using the below guide
> 
> http://wiki.squid-cache.org/Features/Tproxy4
> 
> Normal HTTP GET requests forwarded to Squid are working fine. But we do see
> problems with TCP segments which holds HTTP data. I am not sure how squid
> TCP reassembly logic works. I need some way to find out where it's getting
> dropped in squid.

As Matus said; Squid does not do any TCP level packet handling. That is
all done in the networking stack, same as for any other program.

TPROXY only affects the TCP accept() logic related to the TCP SYN
packet. Once that is over it is just a regular network socket I/O. Squid
uses the system read() for as much data as it can get. Then parses HTTP
messages out of the received data.

With "debug_options 11,2" configured Squid will log to cache.log all the
messages it is handling an the connection they occured on.


PS. Squid-4 logs to access.log connections which were received and
terminated or had errors before the HTTP message was received. Squid-3
just lets those connections die quietly.

HTH
Amos



From rousskov at measurement-factory.com  Tue Feb  7 00:55:43 2017
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Mon, 6 Feb 2017 17:55:43 -0700
Subject: [squid-users] Is it possible to modify cached object?
In-Reply-To: <1486420041932-4681483.post@n4.nabble.com>
References: <1485715924346-4681379.post@n4.nabble.com>
 <bc9151b9-df29-7c16-1460-08082e67243a@treenet.co.nz>
 <1485855330673-4681390.post@n4.nabble.com>
 <201701311052.39956.Antony.Stone@squid.open.source.it>
 <1485880095814-4681392.post@n4.nabble.com>
 <201701311744.42998.Antony.Stone@squid.open.source.it>
 <1485883154937-4681394.post@n4.nabble.com>
 <201701311828.01550.Antony.Stone@squid.open.source.it>
 <1485887657504-4681396.post@n4.nabble.com>
 <c616d7dc-4781-2eb8-ecd8-6a976b0e1ed2@treenet.co.nz>
 <1486420041932-4681483.post@n4.nabble.com>
Message-ID: <65b40acc-3dcb-8843-aefe-5150404f6eca@measurement-factory.com>

On 02/06/2017 03:27 PM, boruc wrote:

> So I've installed squid 3.5.12, libecap 1.0.1 and sample adapter 1.0.0. I
> was able to test a simple "the -> a" replacement shown in  documentation
> <http://www.e-cap.org/Documentation>  . Next I tried some HTML injection,
> also from documentation. However, it didn't work and I got error like below:

> 2017/02/06 17:56:30 kid1| FATAL: dying from an unhandled exception:
> Modifying Adapter: configuration error: unsupported configuration parameter:
> replacement-src

As you have figured out already, the following bug report adds
replacement-src support to sample adapter v1.0:
https://bugs.launchpad.net/ecap/+bug/1634621


> What is more, when I try regular text replacement and I use space in
> replacement text I get no error message in cache.log, but browser says that
> "proxy server is refusing connections" and executing command "squid -k
> check" gives me this:

Squid is picky about spaces because they often separate various
configuration tokens. You can try to quote your replacement string,
possibly after turning configuration_includes_quoted_values on.

On the other hand, virtually all production adapters that inject content
into HTML pages need a far more sophisticated injection algorithm then
the primitive sample adapter you are using, so it probably does not make
sense to spend time on making spaces work in this configuration context.


> 2017/02/06 19:11:11| /etc/squid/squid.conf:84: Duplicate option "uri" in
> adaptation service definition
> FATAL: /etc/squid/squid.conf:84: malformed adaptation service configuration
> Squid Cache (Version 3.5.12): Terminated abnormally.
> 
> I managed to find a  bugfix <https://bugs.launchpad.net/ecap/+bug/1634621>  
> to *replacement-src* problem, but how should I
> appy this? Just add lines with "+" at the beginning and recompile sources
> one more time?

You should use a "patch" program to apply the patch to the adapter
sample sources and then rebuild the adapter from scratch.


> Will it work with spaces in replacement text? 

Yes, spaces are supported in the replacement text loaded from a file.


> My main goal is to add a
> paragraph to requested page, with link or image, but tags for those require
> spaces in string.

If you have not already, please read the following FAQ page for various
problems you will need to solve. The sample adapter does _not_ solve
most of them: https://answers.launchpad.net/ecap/+faq/1793


> Or maybe I have to write my own adapter to this?

YMMV, but I speculate that most folks that do not know how to patch
source files would have to learn a lot of complex topics before they
would be able to write a quality injection adapter from scratch. That
speculation, together with the complications mentioned at the above FAQ
page, prompt me to discourage you from attempting to write your own
adapter in C++.

I have not tried it myself yet, but you might be interested in the
following Tcl wrapper for eCAP API:
https://github.com/petasis/ecap-tcl

The other standard options include, if your project allows for that,
finding a capable C++ developer to write an adapter for you and buying a
ready-to-use adapter.


HTH,

Alex.



From squid3 at treenet.co.nz  Tue Feb  7 09:05:09 2017
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Tue, 7 Feb 2017 22:05:09 +1300
Subject: [squid-users] On using Parent Proxies
In-Reply-To: <AM2PR07MB08343A6D45A789B543CD64A2AE400@AM2PR07MB0834.eurprd07.prod.outlook.com>
References: <AM2PR07MB0834789FB47353AD93CA780BAE400@AM2PR07MB0834.eurprd07.prod.outlook.com>
 <AM2PR07MB08343A6D45A789B543CD64A2AE400@AM2PR07MB0834.eurprd07.prod.outlook.com>
Message-ID: <c5f15610-5d79-8728-318e-fffb0828f5eb@treenet.co.nz>

On 7/02/2017 4:49 a.m., Jude Karuhanga wrote:
> Hello there,
> 
> 
> I am new to squid, and would appreciate some help on configuring my
> server to balance traffic between two servers that serve as Gateways
> to the Internet. I attach a simplified configuration file for the
> squid service on the server. I have managed to get the service to
> work using the default gateway, but the redirection through the
> cache_peer does not work: I am unable to access the websites
> included, even though both servers are visible.
> 
> 

Please begin by running "squid -k parse" and fix all the warnings and
errors it reports.

Also;

* "cache deny CONNECT" is pointless. CONNECT traffic cannot be cached.

* a series of "allow" lines followed by an "allow all" is usually
pointless. Just use one line doing "allow all".
 - same for a series of "deny" lines followed by a "deny all".

* For all the lines you have which do this pattern:
"
 ... allow/deny __domain__squid_2
 ... allow/deny CONNECT __domain__squid_2
"
 that second line is pointless. The first line already matches and
allows/denies the CONNECT traffic which the second might match.


Then add "nonhierarchical_direct off".


For info on how load balancing works in Squid see
<http://wiki.squid-cache.org/Features/LoadBalance>


If you still have problems after all that, pelase provide a bit more
details about what you see that is telling you its "not working".

Amos


From oguzismailuysal at gmail.com  Tue Feb  7 09:49:33 2017
From: oguzismailuysal at gmail.com (=?UTF-8?Q?O=C4=9Fuz_=C4=B0smail_Uysal?=)
Date: Tue, 7 Feb 2017 11:49:33 +0200
Subject: [squid-users] Does squid update itself ?
Message-ID: <CAH7i3Lq+gENF9hZXeHz7GHxyLgqtqhOsL1XFKJgd_ridYb3KSw@mail.gmail.com>

I have customized squid binary to understand malformed requests one month
ago. There was no problem until today, today something weird has happened.
It began not to work with malformed requests, then I reinstalled it and the
problem is solved. But I wonder, how came squid to change its behaviour ?
Is it something related to squid or anything else ?
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20170207/4e4c2b79/attachment.htm>

From uhlar at fantomas.sk  Tue Feb  7 09:55:26 2017
From: uhlar at fantomas.sk (Matus UHLAR - fantomas)
Date: Tue, 7 Feb 2017 10:55:26 +0100
Subject: [squid-users] Does squid update itself ?
In-Reply-To: <CAH7i3Lq+gENF9hZXeHz7GHxyLgqtqhOsL1XFKJgd_ridYb3KSw@mail.gmail.com>
References: <CAH7i3Lq+gENF9hZXeHz7GHxyLgqtqhOsL1XFKJgd_ridYb3KSw@mail.gmail.com>
Message-ID: <20170207095525.GA31025@fantomas.sk>

On 07.02.17 11:49, O?uz ?smail Uysal wrote:
>I have customized squid binary to understand malformed requests one month
>ago. There was no problem until today, today something weird has happened.
>It began not to work with malformed requests, then I reinstalled it and the
>problem is solved. But I wonder, how came squid to change its behaviour ?
>Is it something related to squid or anything else ?

what os/distro?
maybe it has upgraded the package because of security bug...
-- 
Matus UHLAR - fantomas, uhlar at fantomas.sk ; http://www.fantomas.sk/
Warning: I wish NOT to receive e-mail advertising to this address.
Varovanie: na tuto adresu chcem NEDOSTAVAT akukolvek reklamnu postu.
Micro$oft random number generator: 0, 0, 0, 4.33e+67, 0, 0, 0...


From oguzismailuysal at gmail.com  Tue Feb  7 09:58:40 2017
From: oguzismailuysal at gmail.com (=?UTF-8?Q?O=C4=9Fuz_=C4=B0smail_Uysal?=)
Date: Tue, 7 Feb 2017 11:58:40 +0200
Subject: [squid-users] Does squid update itself ?
In-Reply-To: <20170207095525.GA31025@fantomas.sk>
References: <CAH7i3Lq+gENF9hZXeHz7GHxyLgqtqhOsL1XFKJgd_ridYb3KSw@mail.gmail.com>
 <20170207095525.GA31025@fantomas.sk>
Message-ID: <CAH7i3Lpv-N3WfOknWE64K_xYtuT+5UMq=_pQ9p-w62rFhXud8w@mail.gmail.com>

Ubuntu 16.04, squid version 3.5.12 and 3.5.24 same thing for both

7 ?ubat 2017 Sal? tarihinde, Matus UHLAR - fantomas <uhlar at fantomas.sk>
yazd?:

> On 07.02.17 11:49, O?uz ?smail Uysal wrote:
>
>> I have customized squid binary to understand malformed requests one month
>> ago. There was no problem until today, today something weird has happened.
>> It began not to work with malformed requests, then I reinstalled it and
>> the
>> problem is solved. But I wonder, how came squid to change its behaviour ?
>> Is it something related to squid or anything else ?
>>
>
> what os/distro?
> maybe it has upgraded the package because of security bug...
> --
> Matus UHLAR - fantomas, uhlar at fantomas.sk ; http://www.fantomas.sk/
> Warning: I wish NOT to receive e-mail advertising to this address.
> Varovanie: na tuto adresu chcem NEDOSTAVAT akukolvek reklamnu postu.
> Micro$oft random number generator: 0, 0, 0, 4.33e+67, 0, 0, 0...
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20170207/dd41c9d2/attachment.htm>

From baborucki at gmail.com  Tue Feb  7 10:35:26 2017
From: baborucki at gmail.com (boruc)
Date: Tue, 7 Feb 2017 02:35:26 -0800 (PST)
Subject: [squid-users] Is it possible to modify cached object?
In-Reply-To: <65b40acc-3dcb-8843-aefe-5150404f6eca@measurement-factory.com>
References: <1485855330673-4681390.post@n4.nabble.com>
 <201701311052.39956.Antony.Stone@squid.open.source.it>
 <1485880095814-4681392.post@n4.nabble.com>
 <201701311744.42998.Antony.Stone@squid.open.source.it>
 <1485883154937-4681394.post@n4.nabble.com>
 <201701311828.01550.Antony.Stone@squid.open.source.it>
 <1485887657504-4681396.post@n4.nabble.com>
 <c616d7dc-4781-2eb8-ecd8-6a976b0e1ed2@treenet.co.nz>
 <1486420041932-4681483.post@n4.nabble.com>
 <65b40acc-3dcb-8843-aefe-5150404f6eca@measurement-factory.com>
Message-ID: <1486463726541-4681490.post@n4.nabble.com>

I managed to patch this adapter (I don't know many things in Linux, but I
don't give up), properly configured Squid (with regex and quotes) and
successfully replaced closing *body* tag with image tag and of course
closing body tag.



--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/Is-it-possible-to-modify-cached-object-tp4681073p4681490.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From uhlar at fantomas.sk  Tue Feb  7 11:07:44 2017
From: uhlar at fantomas.sk (Matus UHLAR - fantomas)
Date: Tue, 7 Feb 2017 12:07:44 +0100
Subject: [squid-users] Does squid update itself ?
In-Reply-To: <CAH7i3Lpv-N3WfOknWE64K_xYtuT+5UMq=_pQ9p-w62rFhXud8w@mail.gmail.com>
References: <CAH7i3Lq+gENF9hZXeHz7GHxyLgqtqhOsL1XFKJgd_ridYb3KSw@mail.gmail.com>
 <20170207095525.GA31025@fantomas.sk>
 <CAH7i3Lpv-N3WfOknWE64K_xYtuT+5UMq=_pQ9p-w62rFhXud8w@mail.gmail.com>
Message-ID: <20170207110744.GC31025@fantomas.sk>

On 07.02.17 11:58, O?uz ?smail Uysal wrote:
>Ubuntu 16.04, squid version 3.5.12 and 3.5.24 same thing for both

do you have both versions installed at once?
do you have installed them as ubuntu packages?
if so, did you change their versions?


>7 ?ubat 2017 Sal? tarihinde, Matus UHLAR - fantomas <uhlar at fantomas.sk>
>yazd?:
>
>> On 07.02.17 11:49, O?uz ?smail Uysal wrote:
>>
>>> I have customized squid binary to understand malformed requests one month
>>> ago. There was no problem until today, today something weird has happened.
>>> It began not to work with malformed requests, then I reinstalled it and
>>> the
>>> problem is solved. But I wonder, how came squid to change its behaviour ?
>>> Is it something related to squid or anything else ?
>>>
>>
>> what os/distro?
>> maybe it has upgraded the package because of security bug...

the changelog
http://changelogs.ubuntu.com/changelogs/pool/main/s/squid3/squid3_3.5.12-1ubuntu7.3/changelog

says that new version of squid 3.5.12 was released on Feb 3


-- 
Matus UHLAR - fantomas, uhlar at fantomas.sk ; http://www.fantomas.sk/
Warning: I wish NOT to receive e-mail advertising to this address.
Varovanie: na tuto adresu chcem NEDOSTAVAT akukolvek reklamnu postu.
REALITY.SYS corrupted. Press any key to reboot Universe.


From gozzy at yandex.ru  Tue Feb  7 13:22:42 2017
From: gozzy at yandex.ru (Alex)
Date: Tue, 07 Feb 2017 16:22:42 +0300
Subject: [squid-users] FTP relay with active client is broken?
Message-ID: <4725111486473762@web39j.yandex.ru>

  Hello.

  Recently I gave FTP relay a try and it seems that it doesn't work out of the box :(
  I've seen a topic regarding passive mode (when squid puts real server's IP into 'Entering passive mode' message), however, I've solved this by writing a kernel module with custom netfilter hooks (the module intercepts squid's reply, gets IP and port and marks corresponding incoming connection, so it's possible to write a REDIRECT rule).
  I thought that active mode will cause less problems, but it seems that what squid tries to do is illegal. As far as I understand, in active mode squid tries to connect to a client and spoofs source IP address. But it simply does not work: even if bind() succeeds after setting 'ip_nonlocal_bind' sysctl to 1, the connect() call fails with EINVAL. According to https://lkml.org/lkml/2001/6/7/17, such kernel's behaviour is legit and squid tries to do something nasty.

  Here's the excerpt from squid's log (3.5.24 on CentOS 6.5 with 4.x kernel):

017/02/07 15:24:12.262| 5,3| ConnOpener.cc(289) createFd: local=172.17.10.30 remote=172.17.11.31:56676 flags=9 will timeout in 60
2017/02/07 15:24:12.262| 5,9| comm.cc(602) comm_connect_addr: connecting socket FD 16 to 172.17.11.31:56676 (want family: 2)
2017/02/07 15:24:12.262| 5,5| comm.cc(644) comm_connect_addr: sock=16, addrinfo( flags=4, family=2, socktype=1, protocol=6, &addr=0x1bffc00, addrlen=16 )
2017/02/07 15:24:12.262| 5,9| comm.cc(645) comm_connect_addr: connect FD 16: (-1) (22) Invalid argument
2017/02/07 15:24:12.262| 14,9| comm.cc(646) comm_connect_addr: connecting to: 172.17.11.31:56676
2017/02/07 15:24:12.262| 5,7| ConnOpener.cc(357) doConnect: local=172.17.10.30 remote=172.17.11.31:56676 flags=9: failure #1 <= 0: (22) Invalid argument
2017/02/07 15:24:12.262| 5,5| ConnOpener.cc(365) doConnect: local=172.17.10.30 remote=172.17.11.31:56676 flags=9: * - ERR tried too many times already.
2017/02/07 15:24:12.262| 17,3| AsyncCall.cc(93) ScheduleCall: ConnOpener.cc(137) will call Ftp::Server::connectedForData(local=172.17.10.30 remote=172.17.11.31:56676 flags=9, errno=22, flag=-8, data=0x17d6188) [call95]

  Any thoughts?


From dante01010 at gmail.com  Tue Feb  7 15:04:11 2017
From: dante01010 at gmail.com (=?UTF-8?Q?Dante_F._B._Col=c3=b2?=)
Date: Tue, 7 Feb 2017 13:04:11 -0200
Subject: [squid-users] HTTPS sites specifics URL
In-Reply-To: <9c1de504-311b-96b8-e927-b4b2f9ee1e28@solutti.com.br>
References: <6e10d22e-004a-afe8-2e6e-9c5e8a9ae11a@gmail.com>
 <9c1de504-311b-96b8-e927-b4b2f9ee1e28@solutti.com.br>
Message-ID: <9e91819b-d5f9-5684-f221-084645f1fa0d@gmail.com>

Hi Leonardo,

Thanks for your reply,I tried SSL Bump under client-first and 
server-first modes both didn't work, Squid version is 3.4.14 running 
under OpenBSD 5.6 and 5.7 test boxes, i also increased verbosity log to 
9 of the URL Parsing debug section to see if shows something useful , i 
'll post here my squid.conf and debug output from cache.log, if you  
have some suggestion tell me please.

2016/12/06 19:32:39.446 kid1| src/cache_manager.cc(89) registerProfile: 
skipped duplicate profile: asndb
2016/12/06 19:32:39.446 kid1| src/cache_manager.cc(89) registerProfile: 
skipped duplicate profile: carp
2016/12/06 19:32:39.446 kid1| src/cache_manager.cc(89) registerProfile: 
skipped duplicate profile: userhash
2016/12/06 19:32:39.446 kid1| src/cache_manager.cc(89) registerProfile: 
skipped duplicate profile: sourcehash
2016/12/06 19:32:39.446 kid1| src/cache_manager.cc(89) registerProfile: 
skipped duplicate profile: server_list
2016/12/06 19:32:39.446 kid1| Finished loading MIME types and icons.
2016/12/06 19:32:39.469 kid1| src/base/AsyncCallQueue.cc(51) fireNext: 
entering clientListenerConnectionOpened(local=172.17.198.19:3128 
remote=[::] FD 18 flags=9, err=0, HTTP Socket port=0x8b3fb9ff418)
2016/12/06 19:32:39.470 kid1| src/base/AsyncCall.cc(30) make: make call 
clientListenerConnectionOpened [call27542]
2016/12/06 19:32:39.470 kid1| Accepting SSL bumped HTTP Socket 
connections at local=172.17.198.19:3128 remote=[::] FD 18 flags=9
2016/12/06 19:32:39.470 kid1| src/base/AsyncCallQueue.cc(53) fireNext: 
leaving clientListenerConnectionOpened(local=172.17.198.19:3128 
remote=[::] FD 18 flags=9, err=0, HTTP Socket port=0x8b3fb9ff418)
2016/12/06 19:33:05.727 kid1| src/comm/TcpAcceptor.cc(220) doAccept: New 
connection on FD 18
2016/12/06 19:33:05.727 kid1| src/comm/TcpAcceptor.cc(295) acceptNext: 
connection on local=172.17.198.19:3128 remote=[::] FD 18 flags=9
2016/12/06 19:33:05.727 kid1| src/client_side.cc(2407) parseHttpRequest: 
HTTP Client local=172.17.198.19:3128 remote=172.17.200.11:50974 FD 9 flags=1
2016/12/06 19:33:05.727 kid1| src/client_side.cc(2408) parseHttpRequest: 
HTTP Client REQUEST:
---------
CONNECT www.sans.org:443 HTTP/1.1
User-Agent: Mozilla/5.0 (Macintosh; Intel Mac OS X 10.11; rv:45.0) 
Gecko/20100101 Firefox/45.0
Proxy-Connection: keep-alive
Connection: keep-alive
Host: www.sans.org:443
Proxy-Authorization: Basic amVjYS50YXR1OjEyMzQ=


----------
2016/12/06 19:33:05.727 kid1| src/url.cc(386) urlParse: urlParse: Split 
URL 'www.sans.org:443' into proto='', host='www.sans.org', port='443', 
path=''
2016/12/06 19:33:05.727 kid1| Starting new basicauthenticator helpers...
2016/12/06 19:33:05.727 kid1| helperOpenServers: Starting 1/8 
'basic_ncsa_auth' processes
2016/12/06 19:33:05.762 kid1| src/auth/User.cc(342) addIp: user 
'jeca.tatu' has been seen at a new IP address (172.17.200.11:50974)
2016/12/06 19:33:05.763 kid1| src/client_side_request.cc(759) 
clientAccessCheckDone: The request CONNECT www.sans.org:443 is DENIED; 
last ACL checked: all
2016/12/06 19:33:05.763 kid1| src/errorpage.cc(1278) BuildContent: No 
existing error page language negotiated for ERR_ACCESS_DENIED. Using 
default error file.
2016/12/06 19:33:05.764 kid1| src/store.cc(1011) checkCachable: 
StoreEntry::checkCachable: NO: not cachable
2016/12/06 19:33:05.764 kid1| src/client_side.cc(785) setAuth: Adding 
connection-auth to local=172.17.198.19:3128 remote=172.17.200.11:50974 
FD 9 flags=1 from SSL-bumped CONNECT
2016/12/06 19:33:05.767 kid1| src/client_side.cc(3562) 
clientNegotiateSSL: clientNegotiateSSL: Session 0x8b414f73400 reused on 
FD 9 (172.17.200.11:50974)
2016/12/06 19:33:05.768 kid1| src/client_side.cc(2407) parseHttpRequest: 
HTTP Client local=172.17.198.19:3128 remote=172.17.200.11:50974 FD 9 flags=1
2016/12/06 19:33:05.768 kid1| src/client_side.cc(2408) parseHttpRequest: 
HTTP Client REQUEST:
---------
GET /programs HTTP/1.1
Host: www.sans.org
User-Agent: Mozilla/5.0 (Macintosh; Intel Mac OS X 10.11; rv:45.0) 
Gecko/20100101 Firefox/45.0
Accept: text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8
Accept-Language: en-US,en;q=0.5
Accept-Encoding: gzip, deflate
Cookie: 
QSI_HistorySession=http%3A%2F%2Fwww.sans.org%2Fprograms~1486478958014
Connection: keep-alive


----------
2016/12/06 19:33:05.768 kid1| src/url.cc(386) urlParse: urlParse: Split 
URL 'https://www.sans.org/programs' into proto='https', 
host='www.sans.org', port='443', path='/programs'
2016/12/06 19:33:05.768 kid1| src/client_side_reply.cc(1969) 
processReplyAccessResult: The reply for GET 
https://www.sans.org/programs is ALLOWED, because it matched 
'(access_log daemon:/var/squid/logs/access.log line)'
2016/12/06 19:33:05.769 kid1| src/client_side.cc(1459) 
sendStartOfMessage: HTTP Client local=172.17.198.19:3128 
remote=172.17.200.11:50974 FD 9 flags=1
2016/12/06 19:33:05.769 kid1| src/client_side.cc(1460) 
sendStartOfMessage: HTTP Client REPLY:
---------
HTTP/1.1 403 Forbidden
Server: squid/3.4.12
Mime-Version: 1.0
Date: Tue, 06 Dec 2016 21:33:05 GMT
Content-Type: text/html
Content-Length: 3342
X-Squid-Error: ERR_ACCESS_DENIED 0
Vary: Accept-Language
Content-Language: en
X-Cache: MISS from openbsd57vm01
Via: 1.1 openbsd57vm01 (squid/3.4.12)
Connection: close

#################################################################

my squid.conf

cache_dir ufs /var/squid/cache 2048 16 256
cache_log /var/squid/logs/cache.log
cache_store_log daemon:/var/squid/logs/store.log
cache_mem 256 mb
max_filedescriptors 32768
acl eu src 172.17.200.11
acl SSL_ports port 443
acl CONNECT method CONNECT
debug_options ALL,2 23,9
http_access deny !Safe_ports

http_access deny CONNECT !SSL_ports

http_access allow localhost manager
http_access deny manager
auth_param basic program /usr/local/libexec/squid/basic_ncsa_auth 
/etc/squid/squid-passwd
auth_param basic children 8
auth_param basic realm Squid proxy-caching web server
auth_param basic credentialsttl 2 hours
acl password proxy_auth REQUIRED
acl jeca.tatu proxy_auth jeca.tatu
acl restrito url_regex -i  "/etc/squid/acl/restrito"
http_access allow password jeca.tatu restrito
http_access deny all

http_port 172.17.198.19:3128 ssl-bump generate-host-certificates=on 
dynamic_cert_mem_cache_size=8MB key=/etc/squid/pki/test.private 
cert=/etc/squid/pki/test.cert
acl BadSite ssl_error SQUID_X509_V_ERR_DOMAIN_MISMATCH
always_direct allow all
ssl_bump client-first all
sslproxy_cert_error allow all
sslproxy_cert_error allow BadSite
sslproxy_flags DONT_VERIFY_PEER
sslcrtd_program /usr/local/libexec/squid/ssl_crtd -s /var/squid/ssl_db 
-M 8MB
sslcrtd_children 7 startup=1 idle=1

coredump_dir /var/squid/cache


refresh_pattern ^ftp:           1440    20%     10080
refresh_pattern ^gopher:        1440    0%      1440
refresh_pattern -i (/cgi-bin/|\?) 0     0%      0
refresh_pattern .               0       20%     4320




On 2/6/17 2:28 PM, Leonardo Rodrigues wrote:
>
>     That's correct, when not using SSL-Bump feature (that's the one 
> you're looking for), squid will only see the domain part. All the rest 
> of the URL is crypted and visible only to the client (browser) and the 
> server on the other side, the only two parts involved on that crypto 
> session.
>
>     To enable squid to see the whole URL and be able to do full 
> filtering on HTTPS requests, you're looking for SSL-Bump feature. 
> Google for it, there's a LOT of tutorials and mailing list messages on 
> that.
>
>
> Em 06/02/17 12:40, Dante F. B. Col? escreveu:
>> Hello Everyone
>>
>> I have a question , probably a noob one , i 'm trying to allow some 
>> https sites with specific URL's  (i mean 
>> https://domain.tld/blablabla) but https sites are working  only with 
>> the domain part , what i have to do to make this work ?
>>
>



From joe.mozdzer at gmail.com  Tue Feb  7 18:11:12 2017
From: joe.mozdzer at gmail.com (JoeM)
Date: Tue, 7 Feb 2017 10:11:12 -0800 (PST)
Subject: [squid-users] SMP mode fails to process sibling icp hits from peer
Message-ID: <1486491072460-4681494.post@n4.nabble.com>

I am running squid 3.5.23 in accelerator mode with a sibling and have
recently enabled SMP mode with multiple workers.  Since I have done this, I
have noticed that squid  will only fetch content from the sibling if the ICP
response was processed by the same worker which handled the original client
request.   (With the workers line omitted I have not seen the problem).   Is
this a known issue with SMP mode or maybe I am overlooking something that I
need I my config?   Any ideas would be appreciated.
Thanks,
Joe

Here are the parts of my config that I believe may be relevant (as seen by
cachemgr.cgi):

workers 2
http_port [::]:80 accel vhost name=80 connection-auth=on
cache_peer 127.0.0.1 Parent 1114 0 name=cachemgr no-digest originserver
connection-auth=auto
cache_peer 172.22.2.135 Sibling 80 3130 name=peer2 no-digest allow-miss
originserver forceddomain=172.22.2.135 connection-auth=auto

memory_cache_shared on
minimum_object_size 0 bytes
maximum_object_size 33554432 bytes
cache_dir rock /lvm/cache/squid

collapsed_forwarding on
icp_port 3130
icp_query_timeout 500


Here is a successful case of retrieving a file "test6.txt" from a peer. 
Kid1 handled both the original request and the processing of the icp reply
in this case:

2017/02/07 16:45:57.666 kid1| 44,3| peer_select.cc(137) peerSelect:
e:t6035=WV/0x7f5f73241080*2 http://172.22.2.76/test6.txt
2017/02/07 16:45:57.666 kid1| 44,3| peer_select.cc(441) peerSelectFoo: GET
172.22.2.76
2017/02/07 16:45:57.666 kid1| 44,3| peer_select.cc(464) peerSelectFoo:
peerSelectFoo: direct = DIRECT_NO (forced non-direct)
2017/02/07 16:45:57.666 kid1| 44,3| peer_select.cc(477) peerSelectFoo:
peerSelectFoo: direct = DIRECT_NO
2017/02/07 16:45:57.666 kid1| 15,5| neighbors.cc(759) peerDigestLookup:
peerDigestLookup: peer 172.22.2.135
2017/02/07 16:45:57.666 kid1| 15,5| neighbors.cc(763) peerDigestLookup:
peerDigestLookup: gone!
2017/02/07 16:45:57.666 kid1| 15,5| neighbors.cc(759) peerDigestLookup:
peerDigestLookup: peer 127.0.0.1
2017/02/07 16:45:57.666 kid1| 15,5| neighbors.cc(763) peerDigestLookup:
peerDigestLookup: gone!
2017/02/07 16:45:57.666 kid1| 15,4| neighbors.cc(847) neighborsDigestSelect:
neighborsDigestSelect: choices: 0 (0)
2017/02/07 16:45:57.666 kid1| 15,4| neighbors.cc(867) peerNoteDigestLookup:
peerNoteDigestLookup: peer <none>, lookup: LOOKUP_NONE
2017/02/07 16:45:57.666 kid1| 44,3| peer_select.cc(110) peerSelectIcpPing:
peerSelectIcpPing: http://172.22.2.76/test6.txt
2017/02/07 16:45:57.666 kid1| 15,3| neighbors.cc(289) neighborsCount:
neighborsCount: 1
2017/02/07 16:45:57.666 kid1| 44,3| peer_select.cc(121) peerSelectIcpPing:
peerSelectIcpPing: counted 1 neighbors
2017/02/07 16:45:57.666 kid1| 44,3| peer_select.cc(587) peerGetSomeNeighbor:
peerSelect: Doing ICP pings
2017/02/07 16:45:57.666 kid1| 15,5| neighbors.cc(626) neighborsUdpPing:
neighborsUdpPing: Peer 172.22.2.135
2017/02/07 16:45:57.666 kid1| 15,4| neighbors.cc(633) neighborsUdpPing:
neighborsUdpPing: pinging peer 172.22.2.135 for
'http://172.22.2.76/test6.txt'
2017/02/07 16:45:57.666 kid1| 15,3| neighbors.cc(635) neighborsUdpPing:
neighborsUdpPing: key = '68E702FD5914605C2B70B9A015BDCF7B'
2017/02/07 16:45:57.666 kid1| 15,3| neighbors.cc(637) neighborsUdpPing:
neighborsUdpPing: reqnum = 12
2017/02/07 16:45:57.666 kid1| 12,5| icp_v2.cc(286) icpUdpSend: icpUdpSend:
FD 31 sending ICP_QUERY, 53 bytes to 172.22.2.135:3130
2017/02/07 16:45:57.666 kid1| 15,5| neighbors.cc(626) neighborsUdpPing:
neighborsUdpPing: Peer 127.0.0.1
2017/02/07 16:45:57.666 kid1| 44,3| peer_select.cc(600) peerGetSomeNeighbor:
peerSelect: 1 ICP replies expected, RTT 500 msec
2017/02/07 16:45:57.667 kid1| 12,4| icp_v2.cc(621) icpHandleUdp:
icpHandleUdp: FD 31: received 49 bytes from 172.22.2.135:3130
2017/02/07 16:45:57.667 kid1| 12,3| icp_v2.cc(503) handleReply:
icpHandleIcpV2: ICP_HIT from 172.22.2.135:3130 for
'http://172.22.2.76/test6.txt'
2017/02/07 16:45:57.667 kid1| 15,6| neighbors.cc(1001) neighborsUdpAck:
neighborsUdpAck: opcode 2 '68E702FD5914605C2B70B9A015BDCF7B'
2017/02/07 16:45:57.667 kid1| 15,3| neighbors.cc(100) whichPeer: whichPeer:
from 172.22.2.135:3130
2017/02/07 16:45:57.667 kid1| 15,3| neighbors.cc(1050) neighborsUdpAck:
neighborsUdpAck: ICP_HIT for '68E702FD5914605C2B70B9A015BDCF7B' from
172.22.2.135 
2017/02/07 16:45:57.667 kid1| 44,3| peer_select.cc(823) peerHandleIcpReply:
peerHandleIcpReply: ICP_HIT http://172.22.2.76/test6.txt
2017/02/07 16:45:57.667 kid1| 44,3| peer_select.cc(441) peerSelectFoo: GET
172.22.2.76
2017/02/07 16:45:57.667 kid1| 44,3| peer_select.cc(655)
peerGetSomeNeighborReplies: peerSelect: SIBLING_HIT/172.22.2.135
2017/02/07 16:45:57.667 kid1| 44,5| peer_select.cc(938) peerAddFwdServer:
peerAddFwdServer: adding 172.22.2.135 SIBLING_HIT


Now here is an example of the failure case for file "test8.txt".   In this
case, kid1 processed the original client request but kid2 is trying to
handle the ICP reply from 172.22.2.135 and can't seem to match it up with
the original request.

2017/02/07 16:53:24.757 kid1| 44,3| peer_select.cc(137) peerSelect:
e:t15793=WV/0x7f5f72fbb660*2 http://172.22.2.76/test8.txt
2017/02/07 16:53:24.757 kid1| 44,3| peer_select.cc(441) peerSelectFoo: GET
172.22.2.76
2017/02/07 16:53:24.757 kid1| 44,3| peer_select.cc(464) peerSelectFoo:
peerSelectFoo: direct = DIRECT_NO (forced non-direct)
2017/02/07 16:53:24.757 kid1| 44,3| peer_select.cc(477) peerSelectFoo:
peerSelectFoo: direct = DIRECT_NO
2017/02/07 16:53:24.757 kid1| 15,5| neighbors.cc(759) peerDigestLookup:
peerDigestLookup: peer 172.22.2.135
2017/02/07 16:53:24.757 kid1| 15,5| neighbors.cc(763) peerDigestLookup:
peerDigestLookup: gone!
2017/02/07 16:53:24.757 kid1| 15,5| neighbors.cc(759) peerDigestLookup:
peerDigestLookup: peer 127.0.0.1
2017/02/07 16:53:24.757 kid1| 15,5| neighbors.cc(763) peerDigestLookup:
peerDigestLookup: gone!
2017/02/07 16:53:24.757 kid1| 15,4| neighbors.cc(847) neighborsDigestSelect:
neighborsDigestSelect: choices: 0 (0)
2017/02/07 16:53:24.757 kid1| 15,4| neighbors.cc(867) peerNoteDigestLookup:
peerNoteDigestLookup: peer <none>, lookup: LOOKUP_NONE
2017/02/07 16:53:24.757 kid1| 44,3| peer_select.cc(110) peerSelectIcpPing:
peerSelectIcpPing: http://172.22.2.76/test8.txt
2017/02/07 16:53:24.757 kid1| 15,3| neighbors.cc(289) neighborsCount:
neighborsCount: 1
2017/02/07 16:53:24.757 kid1| 44,3| peer_select.cc(121) peerSelectIcpPing:
peerSelectIcpPing: counted 1 neighbors
2017/02/07 16:53:24.757 kid1| 44,3| peer_select.cc(587) peerGetSomeNeighbor:
peerSelect: Doing ICP pings
2017/02/07 16:53:24.757 kid1| 15,5| neighbors.cc(626) neighborsUdpPing:
neighborsUdpPing: Peer 172.22.2.135
2017/02/07 16:53:24.758 kid1| 15,4| neighbors.cc(633) neighborsUdpPing:
neighborsUdpPing: pinging peer 172.22.2.135 for
'http://172.22.2.76/test8.txt'
2017/02/07 16:53:24.758 kid1| 15,3| neighbors.cc(635) neighborsUdpPing:
neighborsUdpPing: key = '6ED0B5DA445590DD432D23A0AA190583'
2017/02/07 16:53:24.758 kid1| 15,3| neighbors.cc(637) neighborsUdpPing:
neighborsUdpPing: reqnum = 18
2017/02/07 16:53:24.758 kid1| 12,5| icp_v2.cc(286) icpUdpSend: icpUdpSend:
FD 31 sending ICP_QUERY, 53 bytes to 172.22.2.135:3130
2017/02/07 16:53:24.758 kid1| 15,5| neighbors.cc(626) neighborsUdpPing:
neighborsUdpPing: Peer 127.0.0.1
2017/02/07 16:53:24.758 kid1| 44,3| peer_select.cc(600) peerGetSomeNeighbor:
peerSelect: 1 ICP replies expected, RTT 500 msec
2017/02/07 16:53:24.758 kid2| 12,4| icp_v2.cc(621) icpHandleUdp:
icpHandleUdp: FD 31: received 49 bytes from 172.22.2.135:3130
2017/02/07 16:53:24.758 kid2| 12,3| icp_v2.cc(503) handleReply:
icpHandleIcpV2: ICP_HIT from 172.22.2.135:3130 for
'http://172.22.2.76/test8.txt'
2017/02/07 16:53:24.758 kid2| 15,6| neighbors.cc(1001) neighborsUdpAck:
neighborsUdpAck: opcode 2 '00000000000000000000000000000000'
2017/02/07 16:53:24.758 kid2| 15,3| neighbors.cc(100) whichPeer: whichPeer:
from 172.22.2.135:3130
2017/02/07 16:53:24.758 kid2| 12,3| neighbors.cc(1019) neighborsUdpAck:
neighborsUdpAck: Cache key '00000000000000000000000000000000' not found
2017/02/07 16:53:25.259 kid1| 44,3| peer_select.cc(756) peerPingTimeout:
peerPingTimeout: 'http://172.22.2.76/test8.txt'




--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/SMP-mode-fails-to-process-sibling-icp-hits-from-peer-tp4681494.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From rousskov at measurement-factory.com  Tue Feb  7 18:33:20 2017
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Tue, 7 Feb 2017 11:33:20 -0700
Subject: [squid-users] SMP mode fails to process sibling icp hits from
 peer
In-Reply-To: <1486491072460-4681494.post@n4.nabble.com>
References: <1486491072460-4681494.post@n4.nabble.com>
Message-ID: <79011628-8c4f-10db-65da-0fe45deecb6f@measurement-factory.com>

On 02/07/2017 11:11 AM, JoeM wrote:
> I am running squid 3.5.23 in accelerator mode with a sibling and have
> recently enabled SMP mode with multiple workers.  Since I have done this, I
> have noticed that squid  will only fetch content from the sibling if the ICP
> response was processed by the same worker which handled the original client
> request. Is this a known issue with SMP mode [...]?

Yes, it is a known issue with a workaround. Please search for ICP at
    http://wiki.squid-cache.org/Features/SmpScale

Alex.



From hardikdangar+squid at gmail.com  Tue Feb  7 19:05:52 2017
From: hardikdangar+squid at gmail.com (Hardik Dangar)
Date: Wed, 8 Feb 2017 00:35:52 +0530
Subject: [squid-users] Transparent Squid issue with Appstore in MacOS Sierra
Message-ID: <CA+sSnVaARPYcKcU1Cq3s9Qme9dEd7M3ZYdMWxC=7PYjEN_wC2g@mail.gmail.com>

Hello,


Here is some information about my squid version,

Squid Cache: Version 3.5.23
Service Name: squid
configure options:  '--prefix=/usr' '--localstatedir=/var/squid'
'--libexecdir=/lib/squid' '--srcdir=.' '--datadir=/share/squid'
'--sysconfdir=/etc/squid' '--with-default-user=proxy'
'--with-logdir=/var/log/squid' '--with-pidfile=/var/run/squid.pid'
'--with-openssl' '--enable-ssl-crtd' '--enable-inline'
'--disable-arch-native' '--enable-async-io=8'
'--enable-storeio=ufs,aufs,diskd,rock'
'--enable-removal-policies=lru,heap' '--enable-delay-pools'
'--enable-follow-x-forwarded-for' '--enable-url-rewrite-helpers=fake'
'--enable-ecap'


We are running squid as transparent proxy and have certs installed in all
systems. Until recently all our systems were ubuntu or windows. Recently we
added mac os Seirra and the biggest issue we had with mac is even after
installing certificates. Few apps have problems.

Our biggest problem is Itunes Store. It just doesn't work for some reason.
if we check the log we get random ip's trying to connect via 443 port but
it doesn't connect.
Also Skype for Mac does not work. strangely this works for windows and
ubuntu in our network. Again we see the same behavior.

both of these apps does not work even in Iphone and Ipad.

I believe someone must be able to configure transparent squid with Mac. can
anyone tell me if i need to do anything extra for Mac setup.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20170208/e34b2a4d/attachment.htm>

From joe.mozdzer at gmail.com  Tue Feb  7 19:38:16 2017
From: joe.mozdzer at gmail.com (JoeM)
Date: Tue, 7 Feb 2017 11:38:16 -0800 (PST)
Subject: [squid-users] SMP mode fails to process sibling icp hits from
	peer
In-Reply-To: <79011628-8c4f-10db-65da-0fe45deecb6f@measurement-factory.com>
References: <1486491072460-4681494.post@n4.nabble.com>
 <79011628-8c4f-10db-65da-0fe45deecb6f@measurement-factory.com>
Message-ID: <1486496296290-4681497.post@n4.nabble.com>

Thank you Alex, that did the trick



--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/SMP-mode-fails-to-process-sibling-icp-hits-from-peer-tp4681494p4681497.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From squid3 at treenet.co.nz  Wed Feb  8 03:40:52 2017
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Wed, 8 Feb 2017 16:40:52 +1300
Subject: [squid-users] HTTPS sites specifics URL
In-Reply-To: <9e91819b-d5f9-5684-f221-084645f1fa0d@gmail.com>
References: <6e10d22e-004a-afe8-2e6e-9c5e8a9ae11a@gmail.com>
 <9c1de504-311b-96b8-e927-b4b2f9ee1e28@solutti.com.br>
 <9e91819b-d5f9-5684-f221-084645f1fa0d@gmail.com>
Message-ID: <2a231a09-a8ee-60cd-8b22-22abdf38b4e9@treenet.co.nz>

On 8/02/2017 4:04 a.m., Dante F. B. Col? wrote:
> Hi Leonardo,
> 
> Thanks for your reply,I tried SSL Bump under client-first and
> server-first modes both didn't work, Squid version is 3.4.14 running
> under OpenBSD 5.6 and 5.7 test boxes, i also increased verbosity log to
> 9 of the URL Parsing debug section to see if shows something useful , i
> 'll post here my squid.conf and debug output from cache.log, if you 
> have some suggestion tell me please.
> 
> 2016/12/06 19:32:39.446 kid1| src/cache_manager.cc(89) registerProfile:
> skipped duplicate profile: asndb
> 2016/12/06 19:32:39.446 kid1| src/cache_manager.cc(89) registerProfile:
> skipped duplicate profile: carp
> 2016/12/06 19:32:39.446 kid1| src/cache_manager.cc(89) registerProfile:
> skipped duplicate profile: userhash
> 2016/12/06 19:32:39.446 kid1| src/cache_manager.cc(89) registerProfile:
> skipped duplicate profile: sourcehash
> 2016/12/06 19:32:39.446 kid1| src/cache_manager.cc(89) registerProfile:
> skipped duplicate profile: server_list
> 2016/12/06 19:32:39.446 kid1| Finished loading MIME types and icons.
> 2016/12/06 19:32:39.469 kid1| src/base/AsyncCallQueue.cc(51) fireNext:
> entering clientListenerConnectionOpened(local=172.17.198.19:3128
> remote=[::] FD 18 flags=9, err=0, HTTP Socket port=0x8b3fb9ff418)
> 2016/12/06 19:32:39.470 kid1| src/base/AsyncCall.cc(30) make: make call
> clientListenerConnectionOpened [call27542]
> 2016/12/06 19:32:39.470 kid1| Accepting SSL bumped HTTP Socket
> connections at local=172.17.198.19:3128 remote=[::] FD 18 flags=9
> 2016/12/06 19:32:39.470 kid1| src/base/AsyncCallQueue.cc(53) fireNext:
> leaving clientListenerConnectionOpened(local=172.17.198.19:3128
> remote=[::] FD 18 flags=9, err=0, HTTP Socket port=0x8b3fb9ff418)
> 2016/12/06 19:33:05.727 kid1| src/comm/TcpAcceptor.cc(220) doAccept: New
> connection on FD 18
> 2016/12/06 19:33:05.727 kid1| src/comm/TcpAcceptor.cc(295) acceptNext:
> connection on local=172.17.198.19:3128 remote=[::] FD 18 flags=9
> 2016/12/06 19:33:05.727 kid1| src/client_side.cc(2407) parseHttpRequest:
> HTTP Client local=172.17.198.19:3128 remote=172.17.200.11:50974 FD 9
> flags=1
> 2016/12/06 19:33:05.727 kid1| src/client_side.cc(2408) parseHttpRequest:
> HTTP Client REQUEST:
> ---------
> CONNECT www.sans.org:443 HTTP/1.1
> User-Agent: Mozilla/5.0 (Macintosh; Intel Mac OS X 10.11; rv:45.0)
> Gecko/20100101 Firefox/45.0
> Proxy-Connection: keep-alive
> Connection: keep-alive
> Host: www.sans.org:443
> Proxy-Authorization: Basic amVjYS50YXR1OjEyMzQ=
> 
> 
> ----------
...
> 2016/12/06 19:33:05.762 kid1| src/auth/User.cc(342) addIp: user
> 'jeca.tatu' has been seen at a new IP address (172.17.200.11:50974)

... the "password" ACL works.

... the "jeca.tatu" ACL is redundant.

> 2016/12/06 19:33:05.763 kid1| src/client_side_request.cc(759)
> clientAccessCheckDone: The request CONNECT www.sans.org:443 is DENIED;
> last ACL checked: all

... the "restrito" ACL does not match "www.sans.org:443".

... the "deny all" blocks this CONNECT request.


> 2016/12/06 19:33:05.764 kid1| src/client_side.cc(785) setAuth: Adding
> connection-auth to local=172.17.198.19:3128 remote=172.17.200.11:50974
> FD 9 flags=1 from SSL-bumped CONNECT

... Squid then goes on a bumps the request. But only so that it can
deliver the error message in a way which browsers will display.

> 2016/12/06 19:33:05.767 kid1| src/client_side.cc(3562)
> clientNegotiateSSL: clientNegotiateSSL: Session 0x8b414f73400 reused on
> FD 9 (172.17.200.11:50974)
...
> 2016/12/06 19:33:05.769 kid1| src/client_side.cc(1460)
> sendStartOfMessage: HTTP Client REPLY:
> ---------
> HTTP/1.1 403 Forbidden
> Server: squid/3.4.12
> Mime-Version: 1.0
> Date: Tue, 06 Dec 2016 21:33:05 GMT
> Content-Type: text/html
> Content-Length: 3342
> X-Squid-Error: ERR_ACCESS_DENIED 0
> Vary: Accept-Language
> Content-Language: en
> X-Cache: MISS from openbsd57vm01
> Via: 1.1 openbsd57vm01 (squid/3.4.12)
> Connection: close
> 
> #################################################################
> 
> my squid.conf
> 
...
> acl password proxy_auth REQUIRED
> acl jeca.tatu proxy_auth jeca.tatu
> acl restrito url_regex -i  "/etc/squid/acl/restrito"
> http_access allow password jeca.tatu restrito
> http_access deny all
> 
> http_port 172.17.198.19:3128 ssl-bump generate-host-certificates=on
> dynamic_cert_mem_cache_size=8MB key=/etc/squid/pki/test.private
> cert=/etc/squid/pki/test.cert
> acl BadSite ssl_error SQUID_X509_V_ERR_DOMAIN_MISMATCH
> always_direct allow all

You dont need that "always_direct allow all". It was a workaround for a
3.1 bug which is long since fixed.

> ssl_bump client-first all

> sslproxy_cert_error allow all
> sslproxy_cert_error allow BadSite
> sslproxy_flags DONT_VERIFY_PEER

Remove the "allow all" and DONT_VERIFY_PEER lines. They are very bad,
partiularly for testing. You *want* to see what problems are when debugging.

Amos


From squid3 at treenet.co.nz  Wed Feb  8 04:40:55 2017
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Wed, 8 Feb 2017 17:40:55 +1300
Subject: [squid-users] On using Parent Proxies
In-Reply-To: <AM2PR07MB0834C838225511E0BFF361F7AE430@AM2PR07MB0834.eurprd07.prod.outlook.com>
References: <AM2PR07MB0834789FB47353AD93CA780BAE400@AM2PR07MB0834.eurprd07.prod.outlook.com>
 <AM2PR07MB08343A6D45A789B543CD64A2AE400@AM2PR07MB0834.eurprd07.prod.outlook.com>
 <c5f15610-5d79-8728-318e-fffb0828f5eb@treenet.co.nz>
 <AM2PR07MB0834C838225511E0BFF361F7AE430@AM2PR07MB0834.eurprd07.prod.outlook.com>
Message-ID: <08a1ffa0-b56e-7912-bdb9-c4cb8dd00b4f@treenet.co.nz>

On 8/02/2017 12:50 a.m., Jude Karuhanga wrote:
> Hello there,
> 
> 
> Thanks for the reply. There seem to be no errors in the file (there
> are none returned by "squid -k parse"). The service seems to be
> working, especially after I included the external network on the
> ACLs.
> 

Then your Squid needs an upgrade. The config file indicates it is for
3.5.20. But Squid-3 '-k parse' will absolutely complain when obsolete
Squid-2 directives are passed to them. Your config file contains at
least 3 such obsolete directives (no_cache).

It also uses QUERY ACL before defining what QUERY is.


> 
> I was looking for a schematic explanation on how to configure my
> network to effectively distribute traffic. I attach a scheme of the
> setup (one central Server and two Gateways connected to Routers).

Please define "effectively distribute" ?

You asked how to do "load balancing". The URL I referenced already
covers all the ways Squid can load balance. *Which* one is "effective"
depends on what you want to happen.

Your config file shows someone halfway through testing whether a
particular client->squid->proxy->Internet pathway works. Which is fine,
actually a good way to go about the change. One step at a time.

For info on the pieces needed for creating cache hierarchies see the top
two sections of <http://wiki.squid-cache.org/Features/CacheHierarchy>.

Amos


From hardikdangar+squid at gmail.com  Wed Feb  8 11:08:53 2017
From: hardikdangar+squid at gmail.com (Hardik Dangar)
Date: Wed, 8 Feb 2017 16:38:53 +0530
Subject: [squid-users] Transparent Squid issue with Appstore in MacOS
	Sierra
In-Reply-To: <CA+sSnVaARPYcKcU1Cq3s9Qme9dEd7M3ZYdMWxC=7PYjEN_wC2g@mail.gmail.com>
References: <CA+sSnVaARPYcKcU1Cq3s9Qme9dEd7M3ZYdMWxC=7PYjEN_wC2g@mail.gmail.com>
Message-ID: <CA+sSnVYET8o0C_MoRLsQwrp1SRbodU_KNE9Cp96w70EodwoWzA@mail.gmail.com>

here is my squid.conf http://pastebin.com/raw/9BTcpVkL

Here is what log looks like when i grep packates from Apple Devices when
app store is opened.

1486551793.635    742 192.168.1.12 TAG_NONE/200 0 CONNECT 17.110.234.27:443
- ORIGINAL_DST/17.110.234.27 -
1486551796.343  30610 192.168.1.12 TAG_NONE/200 0 CONNECT 104.113.210.17:443
- HIER_NONE/- -
1486551796.343  30605 192.168.1.12 TCP_TUNNEL/200 30574 CONNECT
init.itunes.apple.com:443 - ORIGINAL_DST/104.113.210.17 -
1486551799.097  30326 192.168.1.12 TAG_NONE/200 0 CONNECT 104.113.210.17:443
- HIER_NONE/- -
1486551799.097  30324 192.168.1.12 TCP_TUNNEL/200 30584 CONNECT
init.itunes.apple.com:443 - ORIGINAL_DST/104.113.210.17 -
1486551799.502    726 192.168.1.12 TAG_NONE/200 0 CONNECT 17.110.234.27:443
- ORIGINAL_DST/17.110.234.27 -
2017/02/08 16:33:19 kid1| SECURITY ALERT: Host header forgery detected on
local=17.173.66.101:443 remote=192.168.1.12:53158 FD 477 flags=33 (local IP
does not match any domain IP)
1486551805.013  59549 192.168.1.12 TAG_NONE/200 0 CONNECT 17.110.234.27:443
- ORIGINAL_DST/17.110.234.27 -
2017/02/08 16:33:33 kid1| SECURITY ALERT: Host header forgery detected on
local=104.113.210.17:443 remote=192.168.1.12:53159 FD 659 flags=33 (local
IP does not match any domain IP)
1486551826.441  57130 192.168.1.12 TAG_NONE/200 0 CONNECT 17.173.66.96:443
- HIER_NONE/- -
1486551826.441  57052 192.168.1.12 TCP_TUNNEL/200 6671 CONNECT
pd-st.itunes.apple.com:443 - ORIGINAL_DST/17.173.66.96 -
1486551852.061    211 192.168.1.12 TAG_NONE/200 0 CONNECT 104.113.210.11:443
- ORIGINAL_DST/104.113.210.11 -
1486551852.434    216 192.168.1.12 TCP_MISS/200 7010 GET
https://configuration.apple.com/configurations/internetservices/cloudkit/cloudkit-1.0.plist
- ORIGINAL_DST/104.113.210.11 text/xml
1486551881.425    234 192.168.1.12 TAG_NONE/200 0 CONNECT 17.252.172.5:443
- ORIGINAL_DST/17.252.172.5 -
1486551881.791    130 192.168.1.12 TCP_MISS_ABORTED/200 620 ACE
https://guzzoni.apple.com/ace - ORIGINAL_DST/17.252.172.5 -
1486551882.684    207 192.168.1.12 TAG_NONE/200 0 CONNECT 17.252.172.5:443
- ORIGINAL_DST/17.252.172.5 -
1486551882.829    348 192.168.1.12 TCP_REFRESH_MODIFIED/200 415 HEAD
http://www.apple.com/ - ORIGINAL_DST/104.113.211.46 text/html
1486551882.859     68 192.168.1.12 TCP_MISS/200 101 HEAD
https://guzzoni.apple.com/salt - ORIGINAL_DST/17.252.172.5 -
1486551883.004    207 192.168.1.12 TAG_NONE/200 0 CONNECT 17.252.172.5:443
- ORIGINAL_DST/17.252.172.5 -
1486551883.083     67 192.168.1.12 TCP_MISS/406 133 HEAD
https://guzzoni.apple.com/ace - ORIGINAL_DST/17.252.172.5 -
1486551884.123    202 192.168.1.12 TAG_NONE/200 0 CONNECT 17.252.172.5:443
- ORIGINAL_DST/17.252.172.5 -
1486551884.301     81 192.168.1.12 TCP_MISS_ABORTED/200 622 ACE
https://guzzoni.apple.com/ace - ORIGINAL_DST/17.252.172.5 -
1486551886.908     43 192.168.1.12 TCP_REFRESH_MODIFIED/200 415 HEAD
http://www.apple.com/ - ORIGINAL_DST/104.113.211.46 text/html
1486551887.085    207 192.168.1.12 TAG_NONE/200 0 CONNECT 17.252.172.5:443
- ORIGINAL_DST/17.252.172.5 -
1486551887.168     67 192.168.1.12 TCP_MISS/406 133 HEAD
https://guzzoni.apple.com/ace - ORIGINAL_DST/17.252.172.5 -
1486551887.310    200 192.168.1.12 TAG_NONE/200 0 CONNECT 17.252.172.5:443
- ORIGINAL_DST/17.252.172.5 -
1486551887.416     68 192.168.1.12 TCP_MISS/200 101 HEAD
https://guzzoni.apple.com/salt - ORIGINAL_DST/17.252.172.5 -


On Wed, Feb 8, 2017 at 12:35 AM, Hardik Dangar <hardikdangar+squid at gmail.com
> wrote:

> Hello,
>
>
> Here is some information about my squid version,
>
> Squid Cache: Version 3.5.23
> Service Name: squid
> configure options:  '--prefix=/usr' '--localstatedir=/var/squid'
> '--libexecdir=/lib/squid' '--srcdir=.' '--datadir=/share/squid'
> '--sysconfdir=/etc/squid' '--with-default-user=proxy'
> '--with-logdir=/var/log/squid' '--with-pidfile=/var/run/squid.pid'
> '--with-openssl' '--enable-ssl-crtd' '--enable-inline'
> '--disable-arch-native' '--enable-async-io=8' '--enable-storeio=ufs,aufs,diskd,rock'
> '--enable-removal-policies=lru,heap' '--enable-delay-pools'
> '--enable-follow-x-forwarded-for' '--enable-url-rewrite-helpers=fake'
> '--enable-ecap'
>
>
> We are running squid as transparent proxy and have certs installed in all
> systems. Until recently all our systems were ubuntu or windows. Recently we
> added mac os Seirra and the biggest issue we had with mac is even after
> installing certificates. Few apps have problems.
>
> Our biggest problem is Itunes Store. It just doesn't work for some reason.
> if we check the log we get random ip's trying to connect via 443 port but
> it doesn't connect.
> Also Skype for Mac does not work. strangely this works for windows and
> ubuntu in our network. Again we see the same behavior.
>
> both of these apps does not work even in Iphone and Ipad.
>
> I believe someone must be able to configure transparent squid with Mac.
> can anyone tell me if i need to do anything extra for Mac setup.
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20170208/2e50c5de/attachment.htm>

From dante01010 at gmail.com  Wed Feb  8 12:07:32 2017
From: dante01010 at gmail.com (=?UTF-8?Q?Dante_F._B._Col=c3=b2?=)
Date: Wed, 8 Feb 2017 10:07:32 -0200
Subject: [squid-users] HTTPS sites specifics URL
In-Reply-To: <2a231a09-a8ee-60cd-8b22-22abdf38b4e9@treenet.co.nz>
References: <6e10d22e-004a-afe8-2e6e-9c5e8a9ae11a@gmail.com>
 <9c1de504-311b-96b8-e927-b4b2f9ee1e28@solutti.com.br>
 <9e91819b-d5f9-5684-f221-084645f1fa0d@gmail.com>
 <2a231a09-a8ee-60cd-8b22-22abdf38b4e9@treenet.co.nz>
Message-ID: <8a1bda61-7311-1b86-b7ab-83b945444805@gmail.com>

Hi Amos,

What i'm trying to do is allow the url "www.sans.org/programs" as an 
example, the acl  file "restrito" contain this URL but it's not working, 
https urls are working only with the domain part which in this case i 
have to remove "/programs"



On 2/8/17 1:40 AM, Amos Jeffries wrote:
> On 8/02/2017 4:04 a.m., Dante F. B. Col? wrote:
>> Hi Leonardo,
>>
>> Thanks for your reply,I tried SSL Bump under client-first and
>> server-first modes both didn't work, Squid version is 3.4.14 running
>> under OpenBSD 5.6 and 5.7 test boxes, i also increased verbosity log to
>> 9 of the URL Parsing debug section to see if shows something useful , i
>> 'll post here my squid.conf and debug output from cache.log, if you
>> have some suggestion tell me please.
>>
>> 2016/12/06 19:32:39.446 kid1| src/cache_manager.cc(89) registerProfile:
>> skipped duplicate profile: asndb
>> 2016/12/06 19:32:39.446 kid1| src/cache_manager.cc(89) registerProfile:
>> skipped duplicate profile: carp
>> 2016/12/06 19:32:39.446 kid1| src/cache_manager.cc(89) registerProfile:
>> skipped duplicate profile: userhash
>> 2016/12/06 19:32:39.446 kid1| src/cache_manager.cc(89) registerProfile:
>> skipped duplicate profile: sourcehash
>> 2016/12/06 19:32:39.446 kid1| src/cache_manager.cc(89) registerProfile:
>> skipped duplicate profile: server_list
>> 2016/12/06 19:32:39.446 kid1| Finished loading MIME types and icons.
>> 2016/12/06 19:32:39.469 kid1| src/base/AsyncCallQueue.cc(51) fireNext:
>> entering clientListenerConnectionOpened(local=172.17.198.19:3128
>> remote=[::] FD 18 flags=9, err=0, HTTP Socket port=0x8b3fb9ff418)
>> 2016/12/06 19:32:39.470 kid1| src/base/AsyncCall.cc(30) make: make call
>> clientListenerConnectionOpened [call27542]
>> 2016/12/06 19:32:39.470 kid1| Accepting SSL bumped HTTP Socket
>> connections at local=172.17.198.19:3128 remote=[::] FD 18 flags=9
>> 2016/12/06 19:32:39.470 kid1| src/base/AsyncCallQueue.cc(53) fireNext:
>> leaving clientListenerConnectionOpened(local=172.17.198.19:3128
>> remote=[::] FD 18 flags=9, err=0, HTTP Socket port=0x8b3fb9ff418)
>> 2016/12/06 19:33:05.727 kid1| src/comm/TcpAcceptor.cc(220) doAccept: New
>> connection on FD 18
>> 2016/12/06 19:33:05.727 kid1| src/comm/TcpAcceptor.cc(295) acceptNext:
>> connection on local=172.17.198.19:3128 remote=[::] FD 18 flags=9
>> 2016/12/06 19:33:05.727 kid1| src/client_side.cc(2407) parseHttpRequest:
>> HTTP Client local=172.17.198.19:3128 remote=172.17.200.11:50974 FD 9
>> flags=1
>> 2016/12/06 19:33:05.727 kid1| src/client_side.cc(2408) parseHttpRequest:
>> HTTP Client REQUEST:
>> ---------
>> CONNECT www.sans.org:443 HTTP/1.1
>> User-Agent: Mozilla/5.0 (Macintosh; Intel Mac OS X 10.11; rv:45.0)
>> Gecko/20100101 Firefox/45.0
>> Proxy-Connection: keep-alive
>> Connection: keep-alive
>> Host: www.sans.org:443
>> Proxy-Authorization: Basic amVjYS50YXR1OjEyMzQ=
>>
>>
>> ----------
> ...
>> 2016/12/06 19:33:05.762 kid1| src/auth/User.cc(342) addIp: user
>> 'jeca.tatu' has been seen at a new IP address (172.17.200.11:50974)
> ... the "password" ACL works.
>
> ... the "jeca.tatu" ACL is redundant.
>
>> 2016/12/06 19:33:05.763 kid1| src/client_side_request.cc(759)
>> clientAccessCheckDone: The request CONNECT www.sans.org:443 is DENIED;
>> last ACL checked: all
> ... the "restrito" ACL does not match "www.sans.org:443".
>
> ... the "deny all" blocks this CONNECT request.
>
>
>> 2016/12/06 19:33:05.764 kid1| src/client_side.cc(785) setAuth: Adding
>> connection-auth to local=172.17.198.19:3128 remote=172.17.200.11:50974
>> FD 9 flags=1 from SSL-bumped CONNECT
> ... Squid then goes on a bumps the request. But only so that it can
> deliver the error message in a way which browsers will display.
>
>> 2016/12/06 19:33:05.767 kid1| src/client_side.cc(3562)
>> clientNegotiateSSL: clientNegotiateSSL: Session 0x8b414f73400 reused on
>> FD 9 (172.17.200.11:50974)
> ...
>> 2016/12/06 19:33:05.769 kid1| src/client_side.cc(1460)
>> sendStartOfMessage: HTTP Client REPLY:
>> ---------
>> HTTP/1.1 403 Forbidden
>> Server: squid/3.4.12
>> Mime-Version: 1.0
>> Date: Tue, 06 Dec 2016 21:33:05 GMT
>> Content-Type: text/html
>> Content-Length: 3342
>> X-Squid-Error: ERR_ACCESS_DENIED 0
>> Vary: Accept-Language
>> Content-Language: en
>> X-Cache: MISS from openbsd57vm01
>> Via: 1.1 openbsd57vm01 (squid/3.4.12)
>> Connection: close
>>
>> #################################################################
>>
>> my squid.conf
>>
> ...
>> acl password proxy_auth REQUIRED
>> acl jeca.tatu proxy_auth jeca.tatu
>> acl restrito url_regex -i  "/etc/squid/acl/restrito"
>> http_access allow password jeca.tatu restrito
>> http_access deny all
>>
>> http_port 172.17.198.19:3128 ssl-bump generate-host-certificates=on
>> dynamic_cert_mem_cache_size=8MB key=/etc/squid/pki/test.private
>> cert=/etc/squid/pki/test.cert
>> acl BadSite ssl_error SQUID_X509_V_ERR_DOMAIN_MISMATCH
>> always_direct allow all
> You dont need that "always_direct allow all". It was a workaround for a
> 3.1 bug which is long since fixed.
>
>> ssl_bump client-first all
>> sslproxy_cert_error allow all
>> sslproxy_cert_error allow BadSite
>> sslproxy_flags DONT_VERIFY_PEER
> Remove the "allow all" and DONT_VERIFY_PEER lines. They are very bad,
> partiularly for testing. You *want* to see what problems are when debugging.
>
> Amos
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users



From gozzy at yandex.ru  Wed Feb  8 13:10:44 2017
From: gozzy at yandex.ru (Alex)
Date: Wed, 08 Feb 2017 16:10:44 +0300
Subject: [squid-users] FTP relay with active client is broken?
In-Reply-To: <4725111486473762@web39j.yandex.ru>
References: <4725111486473762@web39j.yandex.ru>
Message-ID: <1069171486559444@web2m.yandex.ru>

Anyone? Is it a bug or something should be tuned? I've specified 'ftp_port 2121 intercept' and made squid intercept outgoing FTP traffic according to the following rules:

iptables -t nat -A OUTPUT -p tcp -m owner --gid-owner squid -j ACCEPT
iptables -t nat -A OUTPUT -p tcp --dport 21 -j REDIRECT --to-port 2121


07.02.2017, 16:23, "Alex" <gozzy at yandex.ru>:
> ??Hello.
>
> ??Recently I gave FTP relay a try and it seems that it doesn't work out of the box :(
> ??I've seen a topic regarding passive mode (when squid puts real server's IP into 'Entering passive mode' message), however, I've solved this by writing a kernel module with custom netfilter hooks (the module intercepts squid's reply, gets IP and port and marks corresponding incoming connection, so it's possible to write a REDIRECT rule).
> ??I thought that active mode will cause less problems, but it seems that what squid tries to do is illegal. As far as I understand, in active mode squid tries to connect to a client and spoofs source IP address. But it simply does not work: even if bind() succeeds after setting 'ip_nonlocal_bind' sysctl to 1, the connect() call fails with EINVAL. According to https://lkml.org/lkml/2001/6/7/17, such kernel's behaviour is legit and squid tries to do something nasty.
>
> ??Here's the excerpt from squid's log (3.5.24 on CentOS 6.5 with 4.x kernel):
>
> 017/02/07 15:24:12.262| 5,3| ConnOpener.cc(289) createFd: local=172.17.10.30 remote=172.17.11.31:56676 flags=9 will timeout in 60
> 2017/02/07 15:24:12.262| 5,9| comm.cc(602) comm_connect_addr: connecting socket FD 16 to 172.17.11.31:56676 (want family: 2)
> 2017/02/07 15:24:12.262| 5,5| comm.cc(644) comm_connect_addr: sock=16, addrinfo( flags=4, family=2, socktype=1, protocol=6, &addr=0x1bffc00, addrlen=16 )
> 2017/02/07 15:24:12.262| 5,9| comm.cc(645) comm_connect_addr: connect FD 16: (-1) (22) Invalid argument
> 2017/02/07 15:24:12.262| 14,9| comm.cc(646) comm_connect_addr: connecting to: 172.17.11.31:56676
> 2017/02/07 15:24:12.262| 5,7| ConnOpener.cc(357) doConnect: local=172.17.10.30 remote=172.17.11.31:56676 flags=9: failure #1 <= 0: (22) Invalid argument
> 2017/02/07 15:24:12.262| 5,5| ConnOpener.cc(365) doConnect: local=172.17.10.30 remote=172.17.11.31:56676 flags=9: * - ERR tried too many times already.
> 2017/02/07 15:24:12.262| 17,3| AsyncCall.cc(93) ScheduleCall: ConnOpener.cc(137) will call Ftp::Server::connectedForData(local=172.17.10.30 remote=172.17.11.31:56676 flags=9, errno=22, flag=-8, data=0x17d6188) [call95]
>
> ??Any thoughts?
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users


From eliezer at ngtech.co.il  Wed Feb  8 16:06:46 2017
From: eliezer at ngtech.co.il (Eliezer  Croitoru)
Date: Wed, 8 Feb 2017 18:06:46 +0200
Subject: [squid-users] Transparent Squid issue with Appstore in MacOS
	Sierra
In-Reply-To: <CA+sSnVaARPYcKcU1Cq3s9Qme9dEd7M3ZYdMWxC=7PYjEN_wC2g@mail.gmail.com>
References: <CA+sSnVaARPYcKcU1Cq3s9Qme9dEd7M3ZYdMWxC=7PYjEN_wC2g@mail.gmail.com>
Message-ID: <019b01d28225$5ce97410$16bc5c30$@ngtech.co.il>

Can you give me\us a link to instructions how you have installed the certificate on MAC OS?
I know how to do it on Windows and Linux but not MAC OS.

Also, have you tried using peek and splice? From your email it seems you have not tried to use these.(If you need instructions I would be happy to share what I am using for windows updates and it can be adapted to appstore).

Thanks,
Eliezer

----
http://ngtech.co.il/lmgtfy/
Linux System Administrator
Mobile: +972-5-28704261
Email: eliezer at ngtech.co.il


From: squid-users [mailto:squid-users-bounces at lists.squid-cache.org] On Behalf Of Hardik Dangar
Sent: Tuesday, February 7, 2017 9:06 PM
To: Squid Users <squid-users at lists.squid-cache.org>
Subject: [squid-users] Transparent Squid issue with Appstore in MacOS Sierra

Hello,


Here is some information about my squid version,

Squid Cache: Version 3.5.23
Service Name: squid
configure options:  '--prefix=/usr' '--localstatedir=/var/squid' '--libexecdir=/lib/squid' '--srcdir=.' '--datadir=/share/squid' '--sysconfdir=/etc/squid' '--with-default-user=proxy' '--with-logdir=/var/log/squid' '--with-pidfile=/var/run/squid.pid' '--with-openssl' '--enable-ssl-crtd' '--enable-inline' '--disable-arch-native' '--enable-async-io=8' '--enable-storeio=ufs,aufs,diskd,rock' '--enable-removal-policies=lru,heap' '--enable-delay-pools' '--enable-follow-x-forwarded-for' '--enable-url-rewrite-helpers=fake' '--enable-ecap'


We are running squid as transparent proxy and have certs installed in all systems. Until recently all our systems were ubuntu or windows. Recently we added mac os Seirra and the biggest issue we had with mac is even after installing certificates. Few apps have problems.

Our biggest problem is Itunes Store. It just doesn't work for some reason. if we check the log we get random ip's trying to connect via 443 port but it doesn't connect.
Also Skype for Mac does not work. strangely this works for windows and ubuntu in our network. Again we see the same behavior.

both of these apps does not work even in Iphone and Ipad.

I believe someone must be able to configure transparent squid with Mac. can anyone tell me if i need to do anything extra for Mac setup.



From rousskov at measurement-factory.com  Wed Feb  8 16:11:32 2017
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Wed, 8 Feb 2017 09:11:32 -0700
Subject: [squid-users] FTP relay with active client is broken?
In-Reply-To: <1069171486559444@web2m.yandex.ru>
References: <4725111486473762@web39j.yandex.ru>
 <1069171486559444@web2m.yandex.ru>
Message-ID: <e5f12bb5-cbd9-2f98-6002-81e708707cb1@measurement-factory.com>

On 02/08/2017 06:10 AM, Alex wrote:

> I've specified
> 'ftp_port 2121 intercept' and made squid intercept outgoing FTP
> traffic according to the following rules:

> iptables -t nat -A OUTPUT -p tcp -m owner --gid-owner squid -j ACCEPT
> iptables -t nat -A OUTPUT -p tcp --dport 21 -j REDIRECT --to-port 2121

> 07.02.2017, 16:23, "Alex" <gozzy at yandex.ru>:

>> I thought that active mode will cause less problems, but it seems
>> that what squid tries to do is illegal. As far as I understand, in
>> active mode squid tries to connect to a client and spoofs source IP
>> address.

Since spoofing client IP addresses is common for many working Squid
interception setups doing HTTP, it has to be technically possible (i.e.,
"legal" in your terminology). Unfortunately, I do not know enough
low-level details to guide you further. Most likely, the FTP-specific
Squid code facilitating IP spoofing is buggy or you are doing something
wrong (or both).

FWIW, IIRC, FTP interception code has worked for many folks.

Let's hope that somebody with a working FTP interception setup speaks up.

Alex.



From hardikdangar+squid at gmail.com  Wed Feb  8 20:17:25 2017
From: hardikdangar+squid at gmail.com (Hardik Dangar)
Date: Thu, 9 Feb 2017 01:47:25 +0530
Subject: [squid-users] Transparent Squid issue with Appstore in MacOS
	Sierra
In-Reply-To: <019b01d28225$5ce97410$16bc5c30$@ngtech.co.il>
References: <CA+sSnVaARPYcKcU1Cq3s9Qme9dEd7M3ZYdMWxC=7PYjEN_wC2g@mail.gmail.com>
 <019b01d28225$5ce97410$16bc5c30$@ngtech.co.il>
Message-ID: <CA+sSnVY=bo-R_LBMjCepVy+0GK=_k--_UDPFVQ4mqKpz1PDWyA@mail.gmail.com>

I am using following command,

i am converting pem file into cer using openssl and then putting that file
using this command into keychain.
sudo security add-trusted-cert -d -r trustRoot -k
"/Library/Keychains/System.keychain" "~/mycert.cer"

On Wed, Feb 8, 2017 at 9:36 PM, Eliezer Croitoru <eliezer at ngtech.co.il>
wrote:

> Can you give me\us a link to instructions how you have installed the
> certificate on MAC OS?
> I know how to do it on Windows and Linux but not MAC OS.
>
> Also, have you tried using peek and splice? From your email it seems you
> have not tried to use these.(If you need instructions I would be happy to
> share what I am using for windows updates and it can be adapted to
> appstore).
>
> Thanks,
> Eliezer
>
> ----
> http://ngtech.co.il/lmgtfy/
> Linux System Administrator
> Mobile: +972-5-28704261
> Email: eliezer at ngtech.co.il
>
>
> From: squid-users [mailto:squid-users-bounces at lists.squid-cache.org] On
> Behalf Of Hardik Dangar
> Sent: Tuesday, February 7, 2017 9:06 PM
> To: Squid Users <squid-users at lists.squid-cache.org>
> Subject: [squid-users] Transparent Squid issue with Appstore in MacOS
> Sierra
>
> Hello,
>
>
> Here is some information about my squid version,
>
> Squid Cache: Version 3.5.23
> Service Name: squid
> configure options:  '--prefix=/usr' '--localstatedir=/var/squid'
> '--libexecdir=/lib/squid' '--srcdir=.' '--datadir=/share/squid'
> '--sysconfdir=/etc/squid' '--with-default-user=proxy'
> '--with-logdir=/var/log/squid' '--with-pidfile=/var/run/squid.pid'
> '--with-openssl' '--enable-ssl-crtd' '--enable-inline'
> '--disable-arch-native' '--enable-async-io=8' '--enable-storeio=ufs,aufs,diskd,rock'
> '--enable-removal-policies=lru,heap' '--enable-delay-pools'
> '--enable-follow-x-forwarded-for' '--enable-url-rewrite-helpers=fake'
> '--enable-ecap'
>
>
> We are running squid as transparent proxy and have certs installed in all
> systems. Until recently all our systems were ubuntu or windows. Recently we
> added mac os Seirra and the biggest issue we had with mac is even after
> installing certificates. Few apps have problems.
>
> Our biggest problem is Itunes Store. It just doesn't work for some reason.
> if we check the log we get random ip's trying to connect via 443 port but
> it doesn't connect.
> Also Skype for Mac does not work. strangely this works for windows and
> ubuntu in our network. Again we see the same behavior.
>
> both of these apps does not work even in Iphone and Ipad.
>
> I believe someone must be able to configure transparent squid with Mac.
> can anyone tell me if i need to do anything extra for Mac setup.
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20170209/f25e7551/attachment.htm>

From Abhijit.Kottur at cba.com.au  Thu Feb  9 02:54:17 2017
From: Abhijit.Kottur at cba.com.au (Kottur, Abhijit)
Date: Thu, 9 Feb 2017 02:54:17 +0000
Subject: [squid-users] Tunnelling requests using squid-cache
Message-ID: <444C6CFE1FA6A94B98CF44217DE0E42B27DF1080@AAUNSWV34.au.cbainet.com>

Hi Team,

I am writing this email to understand the capabilities of the product 'squid-cache'.

Requirement:
I have an executable(.exe) which is trying to hit an internet website. This executable has the capability to accept proxy IP and port.
However, our enterprise proxy needs authentication credentials (NTLM Authentication) to allow any network through it. The executable that I have doesn't accept credentials.
Thus, I need to forward all requests from the executable to a local proxy which should encapsulate the request with the actual proxy details (IP, port, username & password) and route the requests to the proxy so that it will be allowed to hit internet. Likewise for response.

Please let me know if this can be achieved by 'squid-cache' and if yes, please provide me with the details as to how this can be configured on Win XP/8.1.
Also, I couldn't find a windows 32 bit installer. Please let me know if there is one available.

Thanks in advance :)


_______________________________________________________________________________________________________
Regards,
Abhijit Kottur

Level 15, 255 Pitt Street,
Sydney, NSW  2000
M: +61 414649364
mailto:abhijit.kottur at cba.com.au<mailto:abhijit.kottur at cba.com.au>
[cid:image002.png at 01CDCE17.DD5C8450]
_______________________________________________________________________________________________________
Our vision is to be Australia's finest financial services organisation through excelling in customer service


************** IMPORTANT MESSAGE *****************************       
This e-mail message is intended only for the addressee(s) and contains information which may be
confidential. 
If you are not the intended recipient please advise the sender by return email, do not use or
disclose the contents, and delete the message and any attachments from your system. Unless
specifically indicated, this email does not constitute formal advice or commitment by the sender
or the Commonwealth Bank of Australia (ABN 48 123 123 124) or its subsidiaries. 
We can be contacted through our web site: commbank.com.au. 
If you no longer wish to receive commercial electronic messages from us, please reply to this
e-mail by typing Unsubscribe in the subject line. 
**************************************************************


-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20170209/82cb313c/attachment.htm>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: image001.png
Type: image/png
Size: 2369 bytes
Desc: image001.png
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20170209/82cb313c/attachment.png>

From walter.h at mathemainzel.info  Thu Feb  9 06:56:57 2017
From: walter.h at mathemainzel.info (Walter H.)
Date: Thu, 9 Feb 2017 07:56:57 +0100
Subject: [squid-users] Object Size?
Message-ID: <e9c59af7f9ed24a254b6db229823d361.1486623417@squirrel.mail>

Hello,

the setting

maximum_object_size 4 MB

is the default;

would the following setting

maximum_object_size 2 MB

also mean,
that there would be stored much more objects on disk?

Thanks
Walter



From bpk678 at gmail.com  Thu Feb  9 12:35:36 2017
From: bpk678 at gmail.com (Brendan Kearney)
Date: Thu, 9 Feb 2017 07:35:36 -0500
Subject: [squid-users] Tunnelling requests using squid-cache
In-Reply-To: <444C6CFE1FA6A94B98CF44217DE0E42B27DF1080@AAUNSWV34.au.cbainet.com>
References: <444C6CFE1FA6A94B98CF44217DE0E42B27DF1080@AAUNSWV34.au.cbainet.com>
Message-ID: <609e43e7-4d45-b687-a900-de4cb69b465b@gmail.com>

On 02/08/2017 09:54 PM, Kottur, Abhijit wrote:
>
> Hi Team,
>
> I am writing this email to understand the capabilities of the product 
> ?squid-cache?.
>
> Requirement:
>
> I have an executable(.exe) which is trying to hit an internet website. 
> This executable has the capability to accept proxy IP and port.
>
> However, our enterprise proxy needs authentication credentials (NTLM 
> Authentication) to allow any network through it. The executable that I 
> have doesn?t accept credentials.
>
> Thus, I need to forward all requests from the executable to a /local 
> proxy/ which should encapsulate the request with the actual proxy 
> details (IP, port, username & password) and route the requests to the 
> proxy so that it will be allowed to hit internet. Likewise for response.
>
> Please let me know if this can be achieved by ?squid-cache? and if 
> yes, please provide me with the details as to how this can be 
> configured on Win XP/8.1.
>
> Also, I couldn?t find a windows 32 bit installer. Please let me know 
> if there is one available.
>
> Thanks in advance J
>
> *_________________________________________________________________________________________________________*
>
> Regards,
>
> Abhijit Kottur
>
> Level 15, 255 Pitt Street,
>
> Sydney, NSW  2000
>
> M: +61 414649364
>
> mailto:abhijit.kottur at cba.com.au <mailto:abhijit.kottur at cba.com.au>
>
> cid:image002.png at 01CDCE17.DD5C8450
>
> *_________________________________________________________________________________________________________*
> /Our vision is to be Australia's finest financial services 
> organisation through excelling in customer service/
>
>
> ************** IMPORTANT MESSAGE *****************************
> This e-mail message is intended only for the addressee(s) and contains 
> information which may be
> confidential.
> If you are not the intended recipient please advise the sender by 
> return email, do not use or
> disclose the contents, and delete the message and any attachments from 
> your system. Unless
> specifically indicated, this email does not constitute formal advice 
> or commitment by the sender
> or the Commonwealth Bank of Australia (ABN 48 123 123 124) or its 
> subsidiaries.
> We can be contacted through our web site: commbank.com.au.
> If you no longer wish to receive commercial electronic messages from 
> us, please reply to this
> e-mail by typing Unsubscribe in the subject line.
> **************************************************************
>
>
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users

Look into CNTLM.  it is a local proxy that can inject NTLM headers, to 
satisfy proxy authentication.  you would then chain CNTLM to your 
corporate proxies for internet access.

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20170209/b33303a3/attachment.htm>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: not available
Type: image/png
Size: 2369 bytes
Desc: not available
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20170209/b33303a3/attachment.png>

From squid3 at treenet.co.nz  Thu Feb  9 14:25:41 2017
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Fri, 10 Feb 2017 03:25:41 +1300
Subject: [squid-users] Object Size?
In-Reply-To: <e9c59af7f9ed24a254b6db229823d361.1486623417@squirrel.mail>
References: <e9c59af7f9ed24a254b6db229823d361.1486623417@squirrel.mail>
Message-ID: <7159dfcb-1456-a786-ed06-743715529573@treenet.co.nz>

On 9/02/2017 7:56 p.m., Walter H. wrote:
> Hello,
> 
> the setting
> 
> maximum_object_size 4 MB
> 
> is the default;
> 
> would the following setting
> 
> maximum_object_size 2 MB
> 
> also mean,
> that there would be stored much more objects on disk?
> 

Yes, but ... it depends on what object sizes your network is
transferring and whether those smaller ones will fill your cache. And on
how many 2-4 MB files are actually being stored already.

You also have to consider how popular the big objects are. It is no use
preventing big and popular objects from caching just to store smaller
and less popular ones. That would reduce bandwidth savings.

Amos



From hardikdangar+squid at gmail.com  Thu Feb  9 14:43:53 2017
From: hardikdangar+squid at gmail.com (Hardik Dangar)
Date: Thu, 9 Feb 2017 20:13:53 +0530
Subject: [squid-users] Transparent Squid issue with Appstore in MacOS
	Sierra
In-Reply-To: <023501d282be$729f3800$57dda800$@ngtech.co.il>
References: <CA+sSnVaARPYcKcU1Cq3s9Qme9dEd7M3ZYdMWxC=7PYjEN_wC2g@mail.gmail.com>
 <019b01d28225$5ce97410$16bc5c30$@ngtech.co.il>
 <CA+sSnVY=bo-R_LBMjCepVy+0GK=_k--_UDPFVQ4mqKpz1PDWyA@mail.gmail.com>
 <023501d282be$729f3800$57dda800$@ngtech.co.il>
Message-ID: <CA+sSnVZ62J9dfzgc0geHjTftOkMCx=pGb-qni1dGKaNO-fazEg@mail.gmail.com>

hey eliezer,

thanks for quick response i am actually using following,

acl DiscoverSNIHost at_step SslBump1
acl NoSSLIntercept ssl::server_name_regex -i "/etc/squid/url.nobump"
ssl_bump splice NoSSLIntercept
ssl_bump peek DiscoverSNIHost
ssl_bump bump all

contents of url.nobump file are,

update\.microsoft\.com$
update\.microsoft\.com\.akadns\.net$
v10\.vortex\-win\.data\.microsoft.com$
settings\-win\.data\.microsoft\.com$
# The next are trusted SKYPE addresses
a\.config\.skype\.com$
pipe\.skype\.com$
w[0-9]+\.web\.whatsapp\.com$
tty\.scaleway\.com$
eaadhaar\.uidai\.gov\.in$
facebook\.com$
opera\.com$
itunes\.apple\.com$


Do i need to do anything additional? or are you suggesting i remove bumping
completely and just use splice feature only.


On Thu, Feb 9, 2017 at 3:52 PM, Eliezer Croitoru <eliezer at ngtech.co.il>
wrote:

> Thanks for sharing the details.
>
> But you didn?t answered if you tried slice with ssl bump.
>
> Let me know if you have tried it.
>
>
>
> Eliezer
>
>
>
> ----
>
> Eliezer Croitoru <http://ngtech.co.il/lmgtfy/>
> Linux System Administrator
> Mobile: +972-5-28704261
> Email: eliezer at ngtech.co.il
>
>
>
> *From:* hardikdangar at gmail.com [mailto:hardikdangar at gmail.com] *On Behalf
> Of *Hardik Dangar
> *Sent:* Wednesday, February 8, 2017 10:17 PM
> *To:* Eliezer Croitoru <eliezer at ngtech.co.il>
> *Cc:* Squid Users <squid-users at lists.squid-cache.org>
> *Subject:* Re: [squid-users] Transparent Squid issue with Appstore in
> MacOS Sierra
>
>
>
> I am using following command,
>
>
>
> i am converting pem file into cer using openssl and then putting that file
> using this command into keychain.
>
> sudo security add-trusted-cert -d -r trustRoot -k
> "/Library/Keychains/System.keychain" "~/mycert.cer"
>
>
>
> On Wed, Feb 8, 2017 at 9:36 PM, Eliezer Croitoru <eliezer at ngtech.co.il>
> wrote:
>
> Can you give me\us a link to instructions how you have installed the
> certificate on MAC OS?
> I know how to do it on Windows and Linux but not MAC OS.
>
> Also, have you tried using peek and splice? From your email it seems you
> have not tried to use these.(If you need instructions I would be happy to
> share what I am using for windows updates and it can be adapted to
> appstore).
>
> Thanks,
> Eliezer
>
> ----
> http://ngtech.co.il/lmgtfy/
> Linux System Administrator
> Mobile: +972-5-28704261
> Email: eliezer at ngtech.co.il
>
>
> From: squid-users [mailto:squid-users-bounces at lists.squid-cache.org] On
> Behalf Of Hardik Dangar
> Sent: Tuesday, February 7, 2017 9:06 PM
> To: Squid Users <squid-users at lists.squid-cache.org>
> Subject: [squid-users] Transparent Squid issue with Appstore in MacOS
> Sierra
>
>
> Hello,
>
>
> Here is some information about my squid version,
>
> Squid Cache: Version 3.5.23
> Service Name: squid
> configure options:  '--prefix=/usr' '--localstatedir=/var/squid'
> '--libexecdir=/lib/squid' '--srcdir=.' '--datadir=/share/squid'
> '--sysconfdir=/etc/squid' '--with-default-user=proxy'
> '--with-logdir=/var/log/squid' '--with-pidfile=/var/run/squid.pid'
> '--with-openssl' '--enable-ssl-crtd' '--enable-inline'
> '--disable-arch-native' '--enable-async-io=8' '--enable-storeio=ufs,aufs,diskd,rock'
> '--enable-removal-policies=lru,heap' '--enable-delay-pools'
> '--enable-follow-x-forwarded-for' '--enable-url-rewrite-helpers=fake'
> '--enable-ecap'
>
>
> We are running squid as transparent proxy and have certs installed in all
> systems. Until recently all our systems were ubuntu or windows. Recently we
> added mac os Seirra and the biggest issue we had with mac is even after
> installing certificates. Few apps have problems.
>
> Our biggest problem is Itunes Store. It just doesn't work for some reason.
> if we check the log we get random ip's trying to connect via 443 port but
> it doesn't connect.
> Also Skype for Mac does not work. strangely this works for windows and
> ubuntu in our network. Again we see the same behavior.
>
> both of these apps does not work even in Iphone and Ipad.
>
> I believe someone must be able to configure transparent squid with Mac.
> can anyone tell me if i need to do anything extra for Mac setup.
>
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20170209/378c1fe0/attachment.htm>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: image002.png
Type: image/png
Size: 11297 bytes
Desc: not available
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20170209/378c1fe0/attachment.png>

From rafael.akchurin at diladele.com  Thu Feb  9 14:47:59 2017
From: rafael.akchurin at diladele.com (Rafael Akchurin)
Date: Thu, 9 Feb 2017 14:47:59 +0000
Subject: [squid-users] Transparent Squid issue with Appstore in
	MacOS	Sierra
In-Reply-To: <CA+sSnVZ62J9dfzgc0geHjTftOkMCx=pGb-qni1dGKaNO-fazEg@mail.gmail.com>
References: <CA+sSnVaARPYcKcU1Cq3s9Qme9dEd7M3ZYdMWxC=7PYjEN_wC2g@mail.gmail.com>
 <019b01d28225$5ce97410$16bc5c30$@ngtech.co.il>
 <CA+sSnVY=bo-R_LBMjCepVy+0GK=_k--_UDPFVQ4mqKpz1PDWyA@mail.gmail.com>
 <023501d282be$729f3800$57dda800$@ngtech.co.il>
 <CA+sSnVZ62J9dfzgc0geHjTftOkMCx=pGb-qni1dGKaNO-fazEg@mail.gmail.com>
Message-ID: <DB6PR0401MB26808B9DF421C29411E983BE8F450@DB6PR0401MB2680.eurprd04.prod.outlook.com>

Hello Hardik and all,

Try adding  .mzstatic.com to your exclusion from SSL bump as indicated on  https://docs.diladele.com/faq/squid/sslbump_exlusions/apple_app_store.html
Please note you need to adapt it to regex as we use it in ssl::server_name directive.

Best regards,
Rafael Akchurin
Diladele B.V.

From: squid-users [mailto:squid-users-bounces at lists.squid-cache.org] On Behalf Of Hardik Dangar
Sent: Thursday, February 9, 2017 3:44 PM
To: Eliezer Croitoru <eliezer at ngtech.co.il>; Squid Users <squid-users at lists.squid-cache.org>
Subject: Re: [squid-users] Transparent Squid issue with Appstore in MacOS Sierra

hey eliezer,

thanks for quick response i am actually using following,

acl DiscoverSNIHost at_step SslBump1
acl NoSSLIntercept ssl::server_name_regex -i "/etc/squid/url.nobump"
ssl_bump splice NoSSLIntercept
ssl_bump peek DiscoverSNIHost
ssl_bump bump all

contents of url.nobump file are,

update\.microsoft\.com$
update\.microsoft\.com\.akadns\.net$
v10\.vortex\-win\.data\.microsoft.com<http://microsoft.com>$
settings\-win\.data\.microsoft\.com$
# The next are trusted SKYPE addresses
a\.config\.skype\.com$
pipe\.skype\.com$
w[0-9]+\.web\.whatsapp\.com$
tty\.scaleway\.com$
eaadhaar\.uidai\.gov\.in$
facebook\.com$
opera\.com$
itunes\.apple\.com$


Do i need to do anything additional? or are you suggesting i remove bumping completely and just use splice feature only.


On Thu, Feb 9, 2017 at 3:52 PM, Eliezer Croitoru <eliezer at ngtech.co.il<mailto:eliezer at ngtech.co.il>> wrote:
Thanks for sharing the details.
But you didn?t answered if you tried slice with ssl bump.
Let me know if you have tried it.

Eliezer

----
Eliezer Croitoru<http://ngtech.co.il/lmgtfy/>
Linux System Administrator
Mobile: +972-5-28704261
Email: eliezer at ngtech.co.il<mailto:eliezer at ngtech.co.il>
[cid:image001.png at 01D282EB.E03F0E80]

From: hardikdangar at gmail.com<mailto:hardikdangar at gmail.com> [mailto:hardikdangar at gmail.com<mailto:hardikdangar at gmail.com>] On Behalf Of Hardik Dangar
Sent: Wednesday, February 8, 2017 10:17 PM
To: Eliezer Croitoru <eliezer at ngtech.co.il<mailto:eliezer at ngtech.co.il>>
Cc: Squid Users <squid-users at lists.squid-cache.org<mailto:squid-users at lists.squid-cache.org>>
Subject: Re: [squid-users] Transparent Squid issue with Appstore in MacOS Sierra

I am using following command,

i am converting pem file into cer using openssl and then putting that file using this command into keychain.
sudo security add-trusted-cert -d -r trustRoot -k "/Library/Keychains/System.keychain" "~/mycert.cer"

On Wed, Feb 8, 2017 at 9:36 PM, Eliezer Croitoru <eliezer at ngtech.co.il<mailto:eliezer at ngtech.co.il>> wrote:
Can you give me\us a link to instructions how you have installed the certificate on MAC OS?
I know how to do it on Windows and Linux but not MAC OS.

Also, have you tried using peek and splice? From your email it seems you have not tried to use these.(If you need instructions I would be happy to share what I am using for windows updates and it can be adapted to appstore).

Thanks,
Eliezer

----
http://ngtech.co.il/lmgtfy/
Linux System Administrator
Mobile: +972-5-28704261
Email: eliezer at ngtech.co.il<mailto:eliezer at ngtech.co.il>


From: squid-users [mailto:squid-users-bounces at lists.squid-cache.org<mailto:squid-users-bounces at lists.squid-cache.org>] On Behalf Of Hardik Dangar
Sent: Tuesday, February 7, 2017 9:06 PM
To: Squid Users <squid-users at lists.squid-cache.org<mailto:squid-users at lists.squid-cache.org>>
Subject: [squid-users] Transparent Squid issue with Appstore in MacOS Sierra

Hello,


Here is some information about my squid version,

Squid Cache: Version 3.5.23
Service Name: squid
configure options:  '--prefix=/usr' '--localstatedir=/var/squid' '--libexecdir=/lib/squid' '--srcdir=.' '--datadir=/share/squid' '--sysconfdir=/etc/squid' '--with-default-user=proxy' '--with-logdir=/var/log/squid' '--with-pidfile=/var/run/squid.pid' '--with-openssl' '--enable-ssl-crtd' '--enable-inline' '--disable-arch-native' '--enable-async-io=8' '--enable-storeio=ufs,aufs,diskd,rock' '--enable-removal-policies=lru,heap' '--enable-delay-pools' '--enable-follow-x-forwarded-for' '--enable-url-rewrite-helpers=fake' '--enable-ecap'


We are running squid as transparent proxy and have certs installed in all systems. Until recently all our systems were ubuntu or windows. Recently we added mac os Seirra and the biggest issue we had with mac is even after installing certificates. Few apps have problems.

Our biggest problem is Itunes Store. It just doesn't work for some reason. if we check the log we get random ip's trying to connect via 443 port but it doesn't connect.
Also Skype for Mac does not work. strangely this works for windows and ubuntu in our network. Again we see the same behavior.

both of these apps does not work even in Iphone and Ipad.

I believe someone must be able to configure transparent squid with Mac. can anyone tell me if i need to do anything extra for Mac setup.


-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20170209/38181d0f/attachment.htm>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: image001.png
Type: image/png
Size: 11297 bytes
Desc: image001.png
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20170209/38181d0f/attachment.png>

From philipp.wehling at megatel.de  Thu Feb  9 15:16:20 2017
From: philipp.wehling at megatel.de (Philipp Wehling)
Date: Thu, 9 Feb 2017 16:16:20 +0100 (CET)
Subject: [squid-users] Problems with sftp key re exchange
In-Reply-To: <570511680.1671816.1486653228527.JavaMail.zimbra@megatel.de>
Message-ID: <622565040.1671852.1486653380652.JavaMail.zimbra@megatel.de>

Hello at all, 

I had problems with sftp. Every 1 GiB the upload stopped. 

After trying different things on client and server side, I found out, that squid doesnt like the sftp key re exchange. 

http://www.serv-u.com/kb/1774/sftp-transfers-failing-at-1gb 
https://fogbugz.bitvise.com/default.asp?WinSSHD.1.19869.1 
https://winscp.net/eng/docs/ui_login_kex 

The last link shows how I fixed the error on client side. 

Is there a way to fix it on server side? 



Mit freundlichem Gru?/Kind regards 
i. A. Philipp Wehling 


-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20170209/b9a605b0/attachment.htm>

From squid3 at treenet.co.nz  Thu Feb  9 15:37:19 2017
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Fri, 10 Feb 2017 04:37:19 +1300
Subject: [squid-users] HTTPS sites specifics URL
In-Reply-To: <8a1bda61-7311-1b86-b7ab-83b945444805@gmail.com>
References: <6e10d22e-004a-afe8-2e6e-9c5e8a9ae11a@gmail.com>
 <9c1de504-311b-96b8-e927-b4b2f9ee1e28@solutti.com.br>
 <9e91819b-d5f9-5684-f221-084645f1fa0d@gmail.com>
 <2a231a09-a8ee-60cd-8b22-22abdf38b4e9@treenet.co.nz>
 <8a1bda61-7311-1b86-b7ab-83b945444805@gmail.com>
Message-ID: <c8d217e1-5c07-7767-75fd-f1ba2e771802@treenet.co.nz>

On 9/02/2017 1:07 a.m., Dante F. B. Col? wrote:
> Hi Amos,
> 
> What i'm trying to do is allow the url "www.sans.org/programs" as an
> example, the acl  file "restrito" contain this URL but it's not working,
> https urls are working only with the domain part which in this case i
> have to remove "/programs"
> 

You still have to first accept the client CONNECT request before you can
do anything with that requests data to find the path portion of the URL.

Amos



From rousskov at measurement-factory.com  Thu Feb  9 16:13:26 2017
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Thu, 9 Feb 2017 09:13:26 -0700
Subject: [squid-users] Problems with sftp key re exchange
In-Reply-To: <622565040.1671852.1486653380652.JavaMail.zimbra@megatel.de>
References: <622565040.1671852.1486653380652.JavaMail.zimbra@megatel.de>
Message-ID: <1bfd7930-850d-29e4-d9fe-c99101740cd3@measurement-factory.com>

On 02/09/2017 08:16 AM, Philipp Wehling wrote:
> Hello at all,
> 
> I had problems with sftp. Every 1 GiB the upload stopped.
> 
> After trying different things on client and server side, I found out,
> that squid doesnt like the sftp key re exchange.
> 
> http://www.serv-u.com/kb/1774/sftp-transfers-failing-at-1gb
> https://fogbugz.bitvise.com/default.asp?WinSSHD.1.19869.1
> https://winscp.net/eng/docs/ui_login_kex
> 
> The last link shows how I fixed the error on client side.
> 
> Is there a way to fix it on server side?

This is not my area of expertise, and I am not sure I understand the
relationship between your problem and Squid, but the following recent
improvement may be relevant to you if you need to enable support for SSL
client renegotiations in Squid:

http://lists.squid-cache.org/pipermail/squid-dev/2017-January/007817.html

Alex.



From uhlar at fantomas.sk  Thu Feb  9 18:19:50 2017
From: uhlar at fantomas.sk (Matus UHLAR - fantomas)
Date: Thu, 9 Feb 2017 19:19:50 +0100
Subject: [squid-users] Object Size?
In-Reply-To: <e9c59af7f9ed24a254b6db229823d361.1486623417@squirrel.mail>
References: <e9c59af7f9ed24a254b6db229823d361.1486623417@squirrel.mail>
Message-ID: <20170209181950.GB31007@fantomas.sk>

On 09.02.17 07:56, Walter H. wrote:
>the setting
>
>maximum_object_size 4 MB
>
>is the default;
>
>would the following setting
>
>maximum_object_size 2 MB
>
>also mean,
>that there would be stored much more objects on disk?

depends if you want to save bandwidth or hit ratio.
wieh the "heap LFUDA" replacement method you can save more space, and in
such case bigger maximum_object_size will help you.

-- 
Matus UHLAR - fantomas, uhlar at fantomas.sk ; http://www.fantomas.sk/
Warning: I wish NOT to receive e-mail advertising to this address.
Varovanie: na tuto adresu chcem NEDOSTAVAT akukolvek reklamnu postu.
2B|!2B, that's a question!


From varun.singh at gslab.com  Fri Feb 10 11:05:56 2017
From: varun.singh at gslab.com (Varun Singh)
Date: Fri, 10 Feb 2017 16:35:56 +0530
Subject: [squid-users] Basic HTTPS filtering via CONNECT in Squid
In-Reply-To: <d9a1230e-9773-b4a2-c606-cbd1834364d0@treenet.co.nz>
References: <CABUhpQWFcOPHS7Or9rybcs18iTjDpG=guajotsZaNc6mxykApA@mail.gmail.com>
 <130414a3-f1c4-30c6-8233-c0386e2d4dcc@treenet.co.nz>
 <CABUhpQU5WC3ntYoNx+ZzyDmeE_=U466x_BmoNQMMQqhXsJehEA@mail.gmail.com>
 <d9a1230e-9773-b4a2-c606-cbd1834364d0@treenet.co.nz>
Message-ID: <CABUhpQWZ+47EE7LWs=Bxok5rnbNN+Ab7WYFkj=u6-FGbm+qn+w@mail.gmail.com>

On Tue, Feb 7, 2017 at 3:48 AM, Amos Jeffries <squid3 at treenet.co.nz> wrote:
> On 7/02/2017 2:46 a.m., Varun Singh wrote:
>> On Mon, Feb 6, 2017 at 11:39 AM, Amos Jeffries wrote:
>>
>> Hi,
>> Please find my reply inline:
>>
>>> What documentation? it is wrong, or you are misunderstanding it. The URL
>>> path?query is definitely *not* available without decrypting.
>>>
>>
>> Correct, I mis-read it.
>>
>>
>>> Because the only way to access more than hostname/IP and port is to decrypt.
>>
>> Okay. In that, case I am okay with only being able to see hostname/IP and port.
>> But whenever I search for setting up HTTPS with Squid, I always come
>> across SSL-bump.
>> Could you point me to a tutorial which perform just basic HTTPS setup?
>
> The Squid default config handles as much of HTTPS as can be handled
> without the SSL-Bump feature.
>
>>
>> What I have tried so far is, configuring Squid to listen to port 3129
>> to expect HTTPS traffic. I did this by adding following line to
>> squid.conf:
>>
>> https_port 3129
>>
>> Once this was done, I redirected all the traffic coming to port 443 to
>> port 3129 using iptables. This is because my clients connect to proxy
>> via VPN.
>
> Since you are intercepting port 443 that port is missing the 'intercept'
> flag. Also, interceptig port 443 requires SSL-Bump.
>
>
>> But this had no effect. After connecting clients to proxy, when I try
>> to access an HTTPS website, the clients get no response and nothing
>> shows in access.log file. The browser behaves as if it could not
>> connect to internet.
>>
>> Please note that this setup works perfectly for HTTP requests. Only
>> HTTPS requests give problems.
>>
>
> Port 80 (HTTP) and port 443 (HTTPS) have totally different transport
> protocols. The port 443 one is designed to break when being intercepted.
>
>
>>
>> FYI, by documentation I was referring to below link:
>> http://wiki.squid-cache.org/Features/HTTPS
>>
>
>
> Amos

Thanks Amos. Sorry I couldn't reply early.

So in this case, say I want to configure HTTPS proxy from a
web-browser directly and not through VPN. In that case there will be
no port forwarding involved and hence 443 shouldn't break. To achieve
this, what configurations will have to be set in squid.conf file? I am
assuming we will have to at least provide a port number by adding
'https_port 3129'. Is there anything else I will have to do?

Thanks for your help.



-- 
Regards,
Varun


From tmblue at gmail.com  Fri Feb 10 20:52:06 2017
From: tmblue at gmail.com (Tory M Blue)
Date: Fri, 10 Feb 2017 12:52:06 -0800
Subject: [squid-users] internal image call busted?
	/squid-internal-static/icons/SN.png
Message-ID: <CAEaSS0bcf8rdtMJ5O9jWLifgPjiP0_UqgBNxYWC83ZYgC2Hk_Q@mail.gmail.com>

The request just from the browser or curl:
http://cache04.prod.ca.domain.net/squid-internal-static/icons/SN.png

Anyone know what could have changed in 3.5.20 CentOS7 to cause this check
to fail? I use it for internal load balancers to note if the system is able
to handle requests.

Not sure why this call is failing now, however it works on my stage
deployment "*squid*-3.5.20-1.el7.centos.x86_64"

Same RPM as the impacted systems: *squid*-3.5.20-1.el7.centos.x86_64

any idea?

Thanks
Tory
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20170210/be4ab193/attachment.htm>

From tmblue at gmail.com  Fri Feb 10 22:58:48 2017
From: tmblue at gmail.com (Tory M Blue)
Date: Fri, 10 Feb 2017 14:58:48 -0800
Subject: [squid-users] internal image call busted?
	/squid-internal-static/icons/SN.png
In-Reply-To: <CAEaSS0bcf8rdtMJ5O9jWLifgPjiP0_UqgBNxYWC83ZYgC2Hk_Q@mail.gmail.com>
References: <CAEaSS0bcf8rdtMJ5O9jWLifgPjiP0_UqgBNxYWC83ZYgC2Hk_Q@mail.gmail.com>
Message-ID: <CAEaSS0aae5=nLXSt5wHpbgy4vRW4kGobUYW4aGHhR_qwz2Cy0A@mail.gmail.com>

Sorry image didn't come through, i'm talking about this error

ERRORThe requested URL could not be retrieved
------------------------------

*Invalid Request* error was encountered while trying to process the request:

GET /squid-internal-static/icons/SN.png HTTP/1.1
Connection: keep-alive
Upgrade-Insecure-Requests: 1
User-Agent: Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_6)
AppleWebKit/537.36 (KHTML, like Gecko) Chrome/55.0.2883.95
Safari/537.36
Accept: text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8
DNT: 1
Accept-Encoding: gzip, deflate, sdch
Accept-Language: en-US,en;q=0.8,ru;q=0.6
Cookie: dtuid=1471486181650244744
Host: cache03

Some possible problems are:

   -

   Request is too large.
   -

   Content-Length missing for POST or PUT requests.
   -

   Illegal character in hostname; underscores are not allowed.
   -

   HTTP/1.1 Expect: feature is being asked from an HTTP/1.0 software.

Your cache administrator is webmaster
<webmaster?subject=CacheErrorInfo%20-%20ERR_INVALID_REQ&body=CacheHost%3A%20cache03%0D%0AErrPage%3A%20ERR_INVALID_REQ%0D%0AErr%3A%20%5Bnone%5D%0D%0ATimeStamp%3A%20Fri,%2010%20Feb%202017%2022%3A58%3A21%20GMT%0D%0A%0D%0AClientIP%3A%20100.99.67.26%0D%0A%0D%0AHTTP%20Request%3A%0D%0AGET%20%2Fsquid-internal-static%2Ficons%2FSN.png%20HTTP%2F1.1%0AConnection%3A%20keep-alive%0D%0AUpgrade-Insecure-Requests%3A%201%0D%0AUser-Agent%3A%20Mozilla%2F5.0%20(Macintosh%3B%20Intel%20Mac%20OS%20X%2010_11_6)%20AppleWebKit%2F537.36%20(KHTML,%20like%20Gecko)%20Chrome%2F55.0.2883.95%20Safari%2F537.36%0D%0AAccept%3A%20text%2Fhtml,application%2Fxhtml+xml,application%2Fxml%3Bq%3D0.9,image%2Fwebp,*%2F*%3Bq%3D0.8%0D%0ADNT%3A%201%0D%0AAccept-Encoding%3A%20gzip,%20deflate,%20sdch%0D%0AAccept-Language%3A%20en-US,en%3Bq%3D0.8,ru%3Bq%3D0.6%0D%0ACookie%3A%20dtuid%3D1471486181650244744%0D%0AHost%3A%20cache03%0D%0A%0D%0A%0D%0A>
.

On Fri, Feb 10, 2017 at 12:52 PM, Tory M Blue <tmblue at gmail.com> wrote:

>
> The request just from the browser or curl:
> http://cache04.prod.ca.domain.net/squid-internal-static/icons/SN.png
>
> Anyone know what could have changed in 3.5.20 CentOS7 to cause this check
> to fail? I use it for internal load balancers to note if the system is able
> to handle requests.
>
> Not sure why this call is failing now, however it works on my stage
> deployment "*squid*-3.5.20-1.el7.centos.x86_64"
>
> Same RPM as the impacted systems: *squid*-3.5.20-1.el7.centos.x86_64
>
> any idea?
>
> Thanks
> Tory
>
>
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20170210/31613a73/attachment.htm>

From tmblue at gmail.com  Sat Feb 11 01:05:07 2017
From: tmblue at gmail.com (Tory M Blue)
Date: Fri, 10 Feb 2017 17:05:07 -0800
Subject: [squid-users] internal image call busted?
	/squid-internal-static/icons/SN.png
In-Reply-To: <CAEaSS0aae5=nLXSt5wHpbgy4vRW4kGobUYW4aGHhR_qwz2Cy0A@mail.gmail.com>
References: <CAEaSS0bcf8rdtMJ5O9jWLifgPjiP0_UqgBNxYWC83ZYgC2Hk_Q@mail.gmail.com>
 <CAEaSS0aae5=nLXSt5wHpbgy4vRW4kGobUYW4aGHhR_qwz2Cy0A@mail.gmail.com>
Message-ID: <CAEaSS0ZcrLJwwjaN762fKFNhNtrMeX4pv4sz81gq+vw35YZY7w@mail.gmail.com>

On Fri, Feb 10, 2017 at 2:58 PM, Tory M Blue <tmblue at gmail.com> wrote:

> Sorry image didn't come through, i'm talking about this error
>
> ERRORThe requested URL could not be retrieved
> ------------------------------
>
> *Invalid Request* error was encountered while trying to process the
> request:
>
> GET /squid-internal-static/icons/SN.png HTTP/1.1
> Connection: keep-alive
> Upgrade-Insecure-Requests: 1
> User-Agent: Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_6) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/55.0.2883.95 Safari/537.36
> Accept: text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8
> DNT: 1
> Accept-Encoding: gzip, deflate, sdch
> Accept-Language: en-US,en;q=0.8,ru;q=0.6
> Cookie: dtuid=1471486181650244744
> Host: cache03
>
> Some possible problems are:
>
>    -
>
>    Request is too large.
>    -
>
>    Content-Length missing for POST or PUT requests.
>    -
>
>    Illegal character in hostname; underscores are not allowed.
>    -
>
>    HTTP/1.1 Expect: feature is being asked from an HTTP/1.0 software.
>
> Your cache administrator is webmaster
> <webmaster?subject=CacheErrorInfo%20-%20ERR_INVALID_REQ&body=CacheHost%3A%20cache03%0D%0AErrPage%3A%20ERR_INVALID_REQ%0D%0AErr%3A%20%5Bnone%5D%0D%0ATimeStamp%3A%20Fri,%2010%20Feb%202017%2022%3A58%3A21%20GMT%0D%0A%0D%0AClientIP%3A%20100.99.67.26%0D%0A%0D%0AHTTP%20Request%3A%0D%0AGET%20%2Fsquid-internal-static%2Ficons%2FSN.png%20HTTP%2F1.1%0AConnection%3A%20keep-alive%0D%0AUpgrade-Insecure-Requests%3A%201%0D%0AUser-Agent%3A%20Mozilla%2F5.0%20(Macintosh%3B%20Intel%20Mac%20OS%20X%2010_11_6)%20AppleWebKit%2F537.36%20(KHTML,%20like%20Gecko)%20Chrome%2F55.0.2883.95%20Safari%2F537.36%0D%0AAccept%3A%20text%2Fhtml,application%2Fxhtml+xml,application%2Fxml%3Bq%3D0.9,image%2Fwebp,*%2F*%3Bq%3D0.8%0D%0ADNT%3A%201%0D%0AAccept-Encoding%3A%20gzip,%20deflate,%20sdch%0D%0AAccept-Language%3A%20en-US,en%3Bq%3D0.8,ru%3Bq%3D0.6%0D%0ACookie%3A%20dtuid%3D1471486181650244744%0D%0AHost%3A%20cache03%0D%0A%0D%0A%0D%0A>
> .
>
> On Fri, Feb 10, 2017 at 12:52 PM, Tory M Blue <tmblue at gmail.com> wrote:
>
>>
>> The request just from the browser or curl:
>> http://cache04.prod.ca.domain.net/squid-internal-static/icons/SN.png
>>
>> Anyone know what could have changed in 3.5.20 CentOS7 to cause this check
>> to fail? I use it for internal load balancers to note if the system is able
>> to handle requests.
>>
>> Not sure why this call is failing now, however it works on my stage
>> deployment "*squid*-3.5.20-1.el7.centos.x86_64"
>>
>> Same RPM as the impacted systems: *squid*-3.5.20-1.el7.centos.x86_64
>>
>> any idea?
>>
>> Thanks
>> Tory
>>
>

Path changed, default is wrong

Where the RPM installed it, the default expects /usr/local/blah....
/usr/share/squid/icons/SN.png

Someone may want to figure out how, why this changed??

Tory
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20170210/d5a7a078/attachment.htm>

From oguzismailuysal at gmail.com  Sat Feb 11 16:53:56 2017
From: oguzismailuysal at gmail.com (=?UTF-8?Q?O=C4=9Fuz_=C4=B0smail_Uysal?=)
Date: Sat, 11 Feb 2017 18:53:56 +0200
Subject: [squid-users] Configuring squid to continue tunneling CONNECT
 requests after client disconnects and reconnects again
Message-ID: <CAH7i3Lry7y1pZQVKe-c6yWPBVJk94RDe1xpjQWVZgtedTWdgxg@mail.gmail.com>

I will explain it as clear as I can do. There is limited internet access on
a network. It lets you send data and receive reply once, then resets the
connection. Like I send CONNECT request to my server, squid replies with
200 Connection established, I receive it, and connection is reset
immediately afterwards. Is there any possible way to implement HTTPS
connection over this network ?
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20170211/35e830e3/attachment.htm>

From rousskov at measurement-factory.com  Sat Feb 11 18:03:37 2017
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Sat, 11 Feb 2017 11:03:37 -0700
Subject: [squid-users] Configuring squid to continue tunneling CONNECT
 requests after client disconnects and reconnects again
In-Reply-To: <CAH7i3Lry7y1pZQVKe-c6yWPBVJk94RDe1xpjQWVZgtedTWdgxg@mail.gmail.com>
References: <CAH7i3Lry7y1pZQVKe-c6yWPBVJk94RDe1xpjQWVZgtedTWdgxg@mail.gmail.com>
Message-ID: <707dc68f-1887-c51e-20cc-1f11938de60b@measurement-factory.com>

On 02/11/2017 09:53 AM, O?uz ?smail Uysal wrote:
> I will explain it as clear as I can do. There is limited internet access
> on a network. It lets you send data and receive reply once, then resets
> the connection. Like I send CONNECT request to my server, squid replies
> with 200 Connection established, I receive it, and connection is reset
> immediately afterwards. Is there any possible way to implement HTTPS
> connection over this network ?

What happens if you use Squid as an HTTPS proxy rather than an HTTP
proxy? In other words, can you configure your client to connect over SSL
to Squid's https_port instead of connecting over plain TCP to Squid's
http_port? After that connection is established, the client will send
CONNECT in both cases, but perhaps the "network" will not reset the SSL
connection to Squid because it would not be able to see HTTP[S]
transaction boundaries inside that SSL connection?

If SSL connections to Squid are prohibited on your network, and only one
HTTP transaction is allowed per HTTP connection, then I cannot think of
a way to send standard HTTPS requests through a standard HTTP proxy.

Alex.



From varun.singh at gslab.com  Sun Feb 12 06:40:36 2017
From: varun.singh at gslab.com (Varun Singh)
Date: Sun, 12 Feb 2017 12:10:36 +0530
Subject: [squid-users] Basic HTTPS filtering via CONNECT in Squid
In-Reply-To: <CABUhpQWZ+47EE7LWs=Bxok5rnbNN+Ab7WYFkj=u6-FGbm+qn+w@mail.gmail.com>
References: <CABUhpQWFcOPHS7Or9rybcs18iTjDpG=guajotsZaNc6mxykApA@mail.gmail.com>
 <130414a3-f1c4-30c6-8233-c0386e2d4dcc@treenet.co.nz>
 <CABUhpQU5WC3ntYoNx+ZzyDmeE_=U466x_BmoNQMMQqhXsJehEA@mail.gmail.com>
 <d9a1230e-9773-b4a2-c606-cbd1834364d0@treenet.co.nz>
 <CABUhpQWZ+47EE7LWs=Bxok5rnbNN+Ab7WYFkj=u6-FGbm+qn+w@mail.gmail.com>
Message-ID: <CABUhpQWtuEGz_xn=qS7zrCM+K62Y-sM2cDHfxNTM+aPiUB531Q@mail.gmail.com>

On Friday, February 10, 2017, Varun Singh <varun.singh at gslab.com> wrote:

> On Tue, Feb 7, 2017 at 3:48 AM, Amos Jeffries <squid3 at treenet.co.nz
> <javascript:;>> wrote:
> > On 7/02/2017 2:46 a.m., Varun Singh wrote:
> >> On Mon, Feb 6, 2017 at 11:39 AM, Amos Jeffries wrote:
> >>
> >> Hi,
> >> Please find my reply inline:
> >>
> >>> What documentation? it is wrong, or you are misunderstanding it. The
> URL
> >>> path?query is definitely *not* available without decrypting.
> >>>
> >>
> >> Correct, I mis-read it.
> >>
> >>
> >>> Because the only way to access more than hostname/IP and port is to
> decrypt.
> >>
> >> Okay. In that, case I am okay with only being able to see hostname/IP
> and port.
> >> But whenever I search for setting up HTTPS with Squid, I always come
> >> across SSL-bump.
> >> Could you point me to a tutorial which perform just basic HTTPS setup?
> >
> > The Squid default config handles as much of HTTPS as can be handled
> > without the SSL-Bump feature.
> >
> >>
> >> What I have tried so far is, configuring Squid to listen to port 3129
> >> to expect HTTPS traffic. I did this by adding following line to
> >> squid.conf:
> >>
> >> https_port 3129
> >>
> >> Once this was done, I redirected all the traffic coming to port 443 to
> >> port 3129 using iptables. This is because my clients connect to proxy
> >> via VPN.
> >
> > Since you are intercepting port 443 that port is missing the 'intercept'
> > flag. Also, interceptig port 443 requires SSL-Bump.
> >
> >
> >> But this had no effect. After connecting clients to proxy, when I try
> >> to access an HTTPS website, the clients get no response and nothing
> >> shows in access.log file. The browser behaves as if it could not
> >> connect to internet.
> >>
> >> Please note that this setup works perfectly for HTTP requests. Only
> >> HTTPS requests give problems.
> >>
> >
> > Port 80 (HTTP) and port 443 (HTTPS) have totally different transport
> > protocols. The port 443 one is designed to break when being intercepted.
> >
> >
> >>
> >> FYI, by documentation I was referring to below link:
> >> http://wiki.squid-cache.org/Features/HTTPS
> >>
> >
> >
> > Amos
>
> Thanks Amos. Sorry I couldn't reply early.
>
> So in this case, say I want to configure HTTPS proxy from a
> web-browser directly and not through VPN. In that case there will be
> no port forwarding involved and hence 443 shouldn't break. To achieve
> this, what configurations will have to be set in squid.conf file? I am
> assuming we will have to at least provide a port number by adding
> 'https_port 3129'. Is there anything else I will have to do?
>
> Thanks for your help.
>
>
>
> --
> Regards,
> Varun
>

I found this post on a StackExchange forum which is exactly what I want:

http://serverfault.com/questions/798481/squid-configuration-for-https

The answer points to installing a CA on client.
Does this mean even if I don't want Squid-in-the-middle approach, my
clients would still have to install a certificate?


-- 
Regards,
Varun Singh
Sr. Software Engineer | m: +91 20 4671 2290 |
G <https://in.linkedin.com/in/varun-singh-12b29026>reat Software Laboratory
<http://www.gslab.com/>
------------------------------------------------------------------------------
<https://twitter.com/_gslab>   <https://www.facebook.com/LifeAtGSLab/>
<https://www.linkedin.com/company/gs-lab>   <http://www.gslab.com/blogs>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20170212/1213927e/attachment.htm>

From squid3 at treenet.co.nz  Sun Feb 12 08:51:19 2017
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sun, 12 Feb 2017 21:51:19 +1300
Subject: [squid-users] Basic HTTPS filtering via CONNECT in Squid
In-Reply-To: <CABUhpQWtuEGz_xn=qS7zrCM+K62Y-sM2cDHfxNTM+aPiUB531Q@mail.gmail.com>
References: <CABUhpQWFcOPHS7Or9rybcs18iTjDpG=guajotsZaNc6mxykApA@mail.gmail.com>
 <130414a3-f1c4-30c6-8233-c0386e2d4dcc@treenet.co.nz>
 <CABUhpQU5WC3ntYoNx+ZzyDmeE_=U466x_BmoNQMMQqhXsJehEA@mail.gmail.com>
 <d9a1230e-9773-b4a2-c606-cbd1834364d0@treenet.co.nz>
 <CABUhpQWZ+47EE7LWs=Bxok5rnbNN+Ab7WYFkj=u6-FGbm+qn+w@mail.gmail.com>
 <CABUhpQWtuEGz_xn=qS7zrCM+K62Y-sM2cDHfxNTM+aPiUB531Q@mail.gmail.com>
Message-ID: <ea582b53-5cf0-e0f0-0134-8b668d40f457@treenet.co.nz>

On 12/02/2017 7:40 p.m., Varun Singh wrote:
> 
> The answer points to installing a CA on client.

The question was about how to get browsers talking TLS *directly to a
Squid reverse-proxy*. Your Ubuntu package is not capable of that and you
are not using a reverse-proxy.

> Does this mean even if I don't want Squid-in-the-middle approach, my
> clients would still have to install a certificate?

No. It is irrelevant to yrou sitation.


You began this thread with a simple question:

> Hi,
> I have a Squid 3 installed on Ubuntu 16.04. It works perfectly as an
> HTTP proxy server in transparent mode.
> I wanted to know whether it can be configured to run as HTTPS proxy
> server without ssl-bump i.e. without 'man in the middle attack'
> technique.


Everything you have been asking about since then is various ways to do
parts of the SSL-bump process. Which does not fit very well with the
"without ssl-bump" requirement.

Simply put; if you are not going to SSL-Bump then you can discard any
thoughts of doing things with the HTTPS messages or port 443 traffic.

If you have changed your mind and want to use SSL-Bump now, please
re-describe what you want to actually happen now.

Amos



From squid3 at treenet.co.nz  Sun Feb 12 08:54:00 2017
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sun, 12 Feb 2017 21:54:00 +1300
Subject: [squid-users] internal image call busted?
 /squid-internal-static/icons/SN.png
In-Reply-To: <CAEaSS0ZcrLJwwjaN762fKFNhNtrMeX4pv4sz81gq+vw35YZY7w@mail.gmail.com>
References: <CAEaSS0bcf8rdtMJ5O9jWLifgPjiP0_UqgBNxYWC83ZYgC2Hk_Q@mail.gmail.com>
 <CAEaSS0aae5=nLXSt5wHpbgy4vRW4kGobUYW4aGHhR_qwz2Cy0A@mail.gmail.com>
 <CAEaSS0ZcrLJwwjaN762fKFNhNtrMeX4pv4sz81gq+vw35YZY7w@mail.gmail.com>
Message-ID: <2ac413b4-b7fb-f2d1-1a8c-8d2ab0b4d854@treenet.co.nz>

On 11/02/2017 2:05 p.m., Tory M Blue wrote:
> 
> Path changed, default is wrong
> 
> Where the RPM installed it, the default expects /usr/local/blah....
> /usr/share/squid/icons/SN.png
> 
> Someone may want to figure out how, why this changed??
> 

Maybe. It would help if you mentioned where (which vendor) you got the
package from so the persons in charge of making it can be aware its
their package which needs attention.

Amos



From squid3 at treenet.co.nz  Sun Feb 12 10:20:21 2017
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sun, 12 Feb 2017 23:20:21 +1300
Subject: [squid-users] [squid-announce] Squid 4.0.18 beta is available
Message-ID: <01aec550-dbec-d498-21c7-ec638145e81c@treenet.co.nz>

The Squid HTTP Proxy team is very pleased to announce the availability
of the Squid-4.0.18 release!


This release is a bug fix release resolving several issues found in the
prior Squid releases.


The major changes to be aware of:

* OpenSSL v1.1 support

Many compile issues when building with OpenSSL v1.1.* have been
resolved. The way this was fixed has uncovered a bug in the LibreSSL
library - so LibreSSL will no longer build with Squid-4.


* squidclient TLS debugging

The squidclient tool when built with GnuTLS has HTTPS support. This
version extends the -v debugging mechanism to also produce debug
information from the GnuTLS library about TLS operations.


There are also some major behaviour changes shared with Squid-3.5 which
are included in this release:

* Mitigate DoS attacks that use client-initiated SSL/TLS renegotiation.

Recent alterations to the SSL-Bump feature logic were found to be
breaking the measure put in place to disable TLS renegotiation.
Since some TLSv1.2+ mechanisms actively require it and the upcoming
OpenSSL v1.1+ make it quite hard to disable, we have decided to mitigate
the vulnerability by implementing a rate limit on renegotiation instead
of an outright disable.


* SSLv2 records force SslBump bumping despite a matching step2 peek rule.

This bug shows up as SSLv2 connections being bumped to deliver an error
when they should have been spliced as configured. Squid will now splice
all connections it has been configured to regardless of whether the
obsolete SSLv2 syntax is being used.

When bumping or receiving the connection itself Squid will still reject
SSLv2. Only spliced traffic is affected by this.


* Update External ACL helpers error handling and caching

The Squid helper protocol has undergone several important changes but
the external ACL logic and bundled helpers have not kept up. The ACL
logics handling helper replies also had some bugs in the event of helper
failures.

This release fixes those various bugs and updates all the bundled
helpers to make use of the BH (BrokenHelper) status to signal internal
errors differently to ACL denial.



 All users of Squid-4.x are urged to upgrade to this release as
soon as possible.

 All users of Squid-3 are encouraged to test this release out and plan
for upgrades where possible.


 See the ChangeLog for the full list of changes in this and earlier
 releases.

Please refer to the release notes at
http://www.squid-cache.org/Versions/v4/RELEASENOTES.html
when you are ready to make the switch to Squid-4

This new release can be downloaded from our HTTP or FTP servers

 http://www.squid-cache.org/Versions/v4/
 ftp://ftp.squid-cache.org/pub/squid/
 ftp://ftp.squid-cache.org/pub/archive/4/

or the mirrors. For a list of mirror sites see

 http://www.squid-cache.org/Download/http-mirrors.html
 http://www.squid-cache.org/Download/mirrors.html

If you encounter any issues with this release please file a bug report.
http://bugs.squid-cache.org/


Amos Jeffries
_______________________________________________
squid-announce mailing list
squid-announce at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-announce

From varun.singh at gslab.com  Sun Feb 12 10:51:44 2017
From: varun.singh at gslab.com (Varun Singh)
Date: Sun, 12 Feb 2017 16:21:44 +0530
Subject: [squid-users] Basic HTTPS filtering via CONNECT in Squid
In-Reply-To: <CABUhpQVg6O0DZqdmJwKw_0B9Nb4bd-BiX4gMpnjGLa_yoRfKJQ@mail.gmail.com>
References: <CABUhpQWFcOPHS7Or9rybcs18iTjDpG=guajotsZaNc6mxykApA@mail.gmail.com>
 <130414a3-f1c4-30c6-8233-c0386e2d4dcc@treenet.co.nz>
 <CABUhpQU5WC3ntYoNx+ZzyDmeE_=U466x_BmoNQMMQqhXsJehEA@mail.gmail.com>
 <d9a1230e-9773-b4a2-c606-cbd1834364d0@treenet.co.nz>
 <CABUhpQWZ+47EE7LWs=Bxok5rnbNN+Ab7WYFkj=u6-FGbm+qn+w@mail.gmail.com>
 <CABUhpQWtuEGz_xn=qS7zrCM+K62Y-sM2cDHfxNTM+aPiUB531Q@mail.gmail.com>
 <ea582b53-5cf0-e0f0-0134-8b668d40f457@treenet.co.nz>
 <CABUhpQVg6O0DZqdmJwKw_0B9Nb4bd-BiX4gMpnjGLa_yoRfKJQ@mail.gmail.com>
Message-ID: <CABUhpQUAVXGdZ4iTvX_Qum9SUC7BwvcZjLNbQO5jSzRgpsvq8Q@mail.gmail.com>

On Feb 12, 2017 2:21 PM, "Amos Jeffries" <squid3 at treenet.co.nz> wrote:

On 12/02/2017 7:40 p.m., Varun Singh wrote:
>
> The answer points to installing a CA on client.

The question was about how to get browsers talking TLS *directly to a
Squid reverse-proxy*. Your Ubuntu package is not capable of that and you
are not using a reverse-proxy.

> Does this mean even if I don't want Squid-in-the-middle approach, my
> clients would still have to install a certificate?

No. It is irrelevant to yrou sitation.


You began this thread with a simple question:

> Hi,
> I have a Squid 3 installed on Ubuntu 16.04. It works perfectly as an
> HTTP proxy server in transparent mode.
> I wanted to know whether it can be configured to run as HTTPS proxy
> server without ssl-bump i.e. without 'man in the middle attack'
> technique.


Everything you have been asking about since then is various ways to do
parts of the SSL-bump process. Which does not fit very well with the
"without ssl-bump" requirement.


Simply put; if you are not going to SSL-Bump then you can discard any
thoughts of doing things with the HTTPS messages or port 443 traffic.

If you have changed your mind and want to use SSL-Bump now, please
re-describe what you want to actually happen now.

Amos

_______________________________________________
squid-users mailing list
squid-users at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-users


Hi,
Simply put, my question has three parts:
1. Can Squid be configured as an HTTPS proxy server without SSL-Bump?
2. If yes, then what other configurations have to performed other than
"https_port XXXX"?
3. In this configuration, can Squid filter HTTPS requests from ACL?


Thanks for you help in advance.

--
Regards,
Varun
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20170212/fab87b33/attachment.htm>

From squid3 at treenet.co.nz  Sun Feb 12 12:13:46 2017
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Mon, 13 Feb 2017 01:13:46 +1300
Subject: [squid-users] Basic HTTPS filtering via CONNECT in Squid
In-Reply-To: <CABUhpQUAVXGdZ4iTvX_Qum9SUC7BwvcZjLNbQO5jSzRgpsvq8Q@mail.gmail.com>
References: <CABUhpQWFcOPHS7Or9rybcs18iTjDpG=guajotsZaNc6mxykApA@mail.gmail.com>
 <130414a3-f1c4-30c6-8233-c0386e2d4dcc@treenet.co.nz>
 <CABUhpQU5WC3ntYoNx+ZzyDmeE_=U466x_BmoNQMMQqhXsJehEA@mail.gmail.com>
 <d9a1230e-9773-b4a2-c606-cbd1834364d0@treenet.co.nz>
 <CABUhpQWZ+47EE7LWs=Bxok5rnbNN+Ab7WYFkj=u6-FGbm+qn+w@mail.gmail.com>
 <CABUhpQWtuEGz_xn=qS7zrCM+K62Y-sM2cDHfxNTM+aPiUB531Q@mail.gmail.com>
 <ea582b53-5cf0-e0f0-0134-8b668d40f457@treenet.co.nz>
 <CABUhpQVg6O0DZqdmJwKw_0B9Nb4bd-BiX4gMpnjGLa_yoRfKJQ@mail.gmail.com>
 <CABUhpQUAVXGdZ4iTvX_Qum9SUC7BwvcZjLNbQO5jSzRgpsvq8Q@mail.gmail.com>
Message-ID: <5e564444-587c-cf7e-16d3-15e2103aad94@treenet.co.nz>

On 12/02/2017 11:51 p.m., Varun Singh wrote:
> On Feb 12, 2017 2:21 PM, "Amos Jeffries" <squid3 at treenet.co.nz> wrote:
> 
> On 12/02/2017 7:40 p.m., Varun Singh wrote:
>>
>> The answer points to installing a CA on client.
> 
> The question was about how to get browsers talking TLS *directly to a
> Squid reverse-proxy*. Your Ubuntu package is not capable of that and you
> are not using a reverse-proxy.
> 
>> Does this mean even if I don't want Squid-in-the-middle approach, my
>> clients would still have to install a certificate?
> 
> No. It is irrelevant to yrou sitation.
> 
> 
> You began this thread with a simple question:
> 
>> Hi,
>> I have a Squid 3 installed on Ubuntu 16.04. It works perfectly as an
>> HTTP proxy server in transparent mode.
>> I wanted to know whether it can be configured to run as HTTPS proxy
>> server without ssl-bump i.e. without 'man in the middle attack'
>> technique.
> 
> 
> Everything you have been asking about since then is various ways to do
> parts of the SSL-bump process. Which does not fit very well with the
> "without ssl-bump" requirement.
> 
> 
> Simply put; if you are not going to SSL-Bump then you can discard any
> thoughts of doing things with the HTTPS messages or port 443 traffic.
> 
> If you have changed your mind and want to use SSL-Bump now, please
> re-describe what you want to actually happen now.
> 

You have not described what you want to happen. Just asked how to do
this unknown thing...

> 
> Hi,
> Simply put, my question has three parts:
> 1. Can Squid be configured as an HTTPS proxy server without SSL-Bump?


* The term "HTTPS" is a generic term used to simultaneously describe two
completely different traffic syntaxes (CONNECT tunnels, and port 443 TLS).

* There are three proxy operating "modes" which may receive each of
those types of traffic (explicit/forward, intercept/tproxy, and
reverse/CDN/accel).

* For each type of traffic one mode is invalid, leaving 2x2= 4 different
sets of operations all called "proxying HTTPS".

* all 4 of those ways may be done with or without SSL-Bump feature.

When not using SSL-Bump 2 of the ways of "proxying HTTPS" will work, 2
will not.

When using SSL-Bump the non-working ways of "proxying HTTPS" will start
working, and the working ways will have an extra permutation of splice
vs bump operation that can happen. Extending the possibilities to be 6
ways of "proxying HTTPS".


So the answer(s) to your first question are:

yes, no.  yes, no.  no, yes.



> 2. If yes, then what other configurations have to performed other than
> "https_port XXXX"?

For the cases where the #1 answer was "yes" and not "no".

a) An explicit/forward or intercept proxy not using ssl-bump and
receiving CONNECT requests does not need any special configuration to
"proxy HTTPS". The proxy will simply enact the requested opaque tunnel
in accordance to HTTP rules.

b) A reverse proxy requires the 'accel' mode flag, and the cert= option
must load the cert for the domain you are hosting on that port, and the
key= option must load the private key for that certificate.

c) all other modes will not work without SSL-Bump feature.



> 3. In this configuration, can Squid filter HTTPS requests from ACL?
> 

That depends on the meaning of "this". There are 3 different
configurations in the answer to #2.

For (2a) - no. Only the CONNECT request can be filterd.

For (2b) - yes. BUT, notice that it requires private key data for certs.
This configuration is only usable when _you own the domain_ which the
client is visiting.

For (2c) - SSL-Bump feature is the mechanism which enables https://
filtering for all traffic modes other than that described by (2b).
Without using that feature - no.


Do you understand now why every path you have tried ends up with how-tos
for configuring SSL-Bump?

HTH
Amos



From varun.singh at gslab.com  Mon Feb 13 03:36:51 2017
From: varun.singh at gslab.com (Varun Singh)
Date: Mon, 13 Feb 2017 09:06:51 +0530
Subject: [squid-users] Basic HTTPS filtering via CONNECT in Squid
In-Reply-To: <5e564444-587c-cf7e-16d3-15e2103aad94@treenet.co.nz>
References: <CABUhpQWFcOPHS7Or9rybcs18iTjDpG=guajotsZaNc6mxykApA@mail.gmail.com>
 <130414a3-f1c4-30c6-8233-c0386e2d4dcc@treenet.co.nz>
 <CABUhpQU5WC3ntYoNx+ZzyDmeE_=U466x_BmoNQMMQqhXsJehEA@mail.gmail.com>
 <d9a1230e-9773-b4a2-c606-cbd1834364d0@treenet.co.nz>
 <CABUhpQWZ+47EE7LWs=Bxok5rnbNN+Ab7WYFkj=u6-FGbm+qn+w@mail.gmail.com>
 <CABUhpQWtuEGz_xn=qS7zrCM+K62Y-sM2cDHfxNTM+aPiUB531Q@mail.gmail.com>
 <ea582b53-5cf0-e0f0-0134-8b668d40f457@treenet.co.nz>
 <CABUhpQVg6O0DZqdmJwKw_0B9Nb4bd-BiX4gMpnjGLa_yoRfKJQ@mail.gmail.com>
 <CABUhpQUAVXGdZ4iTvX_Qum9SUC7BwvcZjLNbQO5jSzRgpsvq8Q@mail.gmail.com>
 <5e564444-587c-cf7e-16d3-15e2103aad94@treenet.co.nz>
Message-ID: <CABUhpQUdafg=vN1CviLQsVwffbaPjbC+ByNr-D-6J91S6qmY3A@mail.gmail.com>

On Feb 12, 2017 5:43 PM, "Amos Jeffries" <squid3 at treenet.co.nz> wrote:

On 12/02/2017 11:51 p.m., Varun Singh wrote:
> On Feb 12, 2017 2:21 PM, "Amos Jeffries" <squid3 at treenet.co.nz> wrote:
>
> On 12/02/2017 7:40 p.m., Varun Singh wrote:
>>
>> The answer points to installing a CA on client.
>
> The question was about how to get browsers talking TLS *directly to a
> Squid reverse-proxy*. Your Ubuntu package is not capable of that and you
> are not using a reverse-proxy.
>
>> Does this mean even if I don't want Squid-in-the-middle approach, my
>> clients would still have to install a certificate?
>
> No. It is irrelevant to yrou sitation.
>
>
> You began this thread with a simple question:
>
>> Hi,
>> I have a Squid 3 installed on Ubuntu 16.04. It works perfectly as an
>> HTTP proxy server in transparent mode.
>> I wanted to know whether it can be configured to run as HTTPS proxy
>> server without ssl-bump i.e. without 'man in the middle attack'
>> technique.
>
>
> Everything you have been asking about since then is various ways to do
> parts of the SSL-bump process. Which does not fit very well with the
> "without ssl-bump" requirement.
>
>
> Simply put; if you are not going to SSL-Bump then you can discard any
> thoughts of doing things with the HTTPS messages or port 443 traffic.
>
> If you have changed your mind and want to use SSL-Bump now, please
> re-describe what you want to actually happen now.
>

You have not described what you want to happen. Just asked how to do
this unknown thing...


I want to implement a URL filter using proxy server. My clients will use
this server either from their web-browsers or via strongSwan IPSec VPN
server. If they use the proxy server via VPN server, their VPN profile will
have HTTP and HTTPS proxy server configuration.

This proxy server will filter HTTP and HTTPS websites based on ACL
provided. For security reasons, I want to avoid using SSL-bump.


>
> Hi,
> Simply put, my question has three parts:
> 1. Can Squid be configured as an HTTPS proxy server without SSL-Bump?


* The term "HTTPS" is a generic term used to simultaneously describe two
completely different traffic syntaxes (CONNECT tunnels, and port 443 TLS).

* There are three proxy operating "modes" which may receive each of
those types of traffic (explicit/forward, intercept/tproxy, and
reverse/CDN/accel).

* For each type of traffic one mode is invalid, leaving 2x2= 4 different
sets of operations all called "proxying HTTPS".


This means the combinations are:
#1 CONNECT - explicit/forward
#2 443 TLS - explicit/forward

#3 CONNECT - intercept/tproxy
#4 443 TLS - intercept/tproxy

#5 CONNECT - reverse/CDN/accel
#6 443 TLS - reverse/CDN/accel

One of modes in each type is invalid. So, from Squid's HTTPS feature page,
looks like my scenario falls either in #1 or #3.

* all 4 of those ways may be done with or without SSL-Bump feature.

When not using SSL-Bump 2 of the ways of "proxying HTTPS" will work, 2
will not.

When using SSL-Bump the non-working ways of "proxying HTTPS" will start
working, and the working ways will have an extra permutation of splice
vs bump operation that can happen. Extending the possibilities to be 6
ways of "proxying HTTPS".


So the answer(s) to your first question are:

yes, no.  yes, no.  no, yes.



> 2. If yes, then what other configurations have to performed other than
> "https_port XXXX"?

For the cases where the #1 answer was "yes" and not "no".

a) An explicit/forward or intercept proxy not using ssl-bump and
receiving CONNECT requests does not need any special configuration to
"proxy HTTPS". The proxy will simply enact the requested opaque tunnel
in accordance to HTTP rules.


So this means other than specifying "https_port XXXX" no other config is
needed.
When I setup Squid with just "https_port xxxx" and configured Firefox to
use my proxy server for HTTP and HTTPS, it worked fine for HTTP but for
HTTPS it gave "Proxy server rejected connection".

So either something is wrong in my squid.conf or my assumption is incorrect
that my scenario falls in #1 or #3.


b) A reverse proxy requires the 'accel' mode flag, and the cert= option
must load the cert for the domain you are hosting on that port, and the
key= option must load the private key for that certificate.

c) all other modes will not work without SSL-Bump feature.



> 3. In this configuration, can Squid filter HTTPS requests from ACL?
>

That depends on the meaning of "this". There are 3 different
configurations in the answer to #2.

For (2a) - no. Only the CONNECT request can be filterd.


>From below links it looks like destination IP Address or hostname of a
CONNECT request is same as HTTPS request. Is that correct?

https://en.m.wikipedia.org/wiki/HTTP_tunnel#HTTP_CONNECT_tunneling

http://stackoverflow.com/a/11698002/548403


For (2b) - yes. BUT, notice that it requires private key data for certs.
This configuration is only usable when _you own the domain_ which the
client is visiting.

For (2c) - SSL-Bump feature is the mechanism which enables https://
filtering for all traffic modes other than that described by (2b).
Without using that feature - no.


Do you understand now why every path you have tried ends up with how-tos
for configuring SSL-Bump?


Yes, thanks for the elaborate explanation.


HTH
Amos
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20170213/da576b34/attachment.htm>

From eliezer at ngtech.co.il  Mon Feb 13 06:34:14 2017
From: eliezer at ngtech.co.il (Eliezer  Croitoru)
Date: Mon, 13 Feb 2017 08:34:14 +0200
Subject: [squid-users] Basic HTTPS filtering via CONNECT in Squid
In-Reply-To: <CABUhpQUdafg=vN1CviLQsVwffbaPjbC+ByNr-D-6J91S6qmY3A@mail.gmail.com>
References: <CABUhpQWFcOPHS7Or9rybcs18iTjDpG=guajotsZaNc6mxykApA@mail.gmail.com>
 <130414a3-f1c4-30c6-8233-c0386e2d4dcc@treenet.co.nz>
 <CABUhpQU5WC3ntYoNx+ZzyDmeE_=U466x_BmoNQMMQqhXsJehEA@mail.gmail.com>
 <d9a1230e-9773-b4a2-c606-cbd1834364d0@treenet.co.nz>
 <CABUhpQWZ+47EE7LWs=Bxok5rnbNN+Ab7WYFkj=u6-FGbm+qn+w@mail.gmail.com>
 <CABUhpQWtuEGz_xn=qS7zrCM+K62Y-sM2cDHfxNTM+aPiUB531Q@mail.gmail.com>
 <ea582b53-5cf0-e0f0-0134-8b668d40f457@treenet.co.nz>
 <CABUhpQVg6O0DZqdmJwKw_0B9Nb4bd-BiX4gMpnjGLa_yoRfKJQ@mail.gmail.com>
 <CABUhpQUAVXGdZ4iTvX_Qum9SUC7BwvcZjLNbQO5jSzRgpsvq8Q@mail.gmail.com>
 <5e564444-587c-cf7e-16d3-15e2103aad94@treenet.co.nz>
 <CABUhpQUdafg=vN1CviLQsVwffbaPjbC+ByNr-D-6J91S6qmY3A@mail.gmail.com>
Message-ID: <016101d285c3$356b0a10$a0411e30$@ngtech.co.il>

Hey Varun,

Filtering content based on the URL level\layer of the connection is not possible without SSL-bump.
There for you must use for some aspect of the connections SSL-bump.
However you can selectively choose which destinations would be bumped and which are not.
Most of the current browsers supports SNI which allows squid in some degree to decide if to fully bump the connection to the URL level or to decide to only proxy the connection in the TCP level.
As simple as it sounds URL level filtering requires full SSL-bump and TCP and basic TLS level filtering will not require you to fully utilize SSL-bump but will require you to fully setup squid for SSL-bump.

This is the place to clarify that SNI based filtering is not 100% bullet proof and it could be exploited to override in a way your basic SNI based SSL level filtering.

Do you have specific sites that you want to filter in the URL level or just globally?
The answer to the above question will guide us towards what might be the right path for your solution(which could be full SSL-BUMP or partial).

Eliezer

----
http://ngtech.co.il/lmgtfy/
Linux System Administrator
Mobile: +972-5-28704261
Email: eliezer at ngtech.co.il


From: squid-users [mailto:squid-users-bounces at lists.squid-cache.org] On Behalf Of Varun Singh
Sent: Monday, February 13, 2017 5:37 AM
To: Amos Jeffries <squid3 at treenet.co.nz>
Cc: squid-users at lists.squid-cache.org
Subject: Re: [squid-users] Basic HTTPS filtering via CONNECT in Squid



On Feb 12, 2017 5:43 PM, "Amos Jeffries" <mailto:squid3 at treenet.co.nz> wrote:
On 12/02/2017 11:51 p.m., Varun Singh wrote:
> On Feb 12, 2017 2:21 PM, "Amos Jeffries" <mailto:squid3 at treenet.co.nz> wrote:
>
> On 12/02/2017 7:40 p.m., Varun Singh wrote:
>>
>> The answer points to installing a CA on client.
>
> The question was about how to get browsers talking TLS *directly to a
> Squid reverse-proxy*. Your Ubuntu package is not capable of that and you
> are not using a reverse-proxy.
>
>> Does this mean even if I don't want Squid-in-the-middle approach, my
>> clients would still have to install a certificate?
>
> No. It is irrelevant to yrou sitation.
>
>
> You began this thread with a simple question:
>
>> Hi,
>> I have a Squid 3 installed on Ubuntu 16.04. It works perfectly as an
>> HTTP proxy server in transparent mode.
>> I wanted to know whether it can be configured to run as HTTPS proxy
>> server without ssl-bump i.e. without 'man in the middle attack'
>> technique.
>
>
> Everything you have been asking about since then is various ways to do
> parts of the SSL-bump process. Which does not fit very well with the
> "without ssl-bump" requirement.
>
>
> Simply put; if you are not going to SSL-Bump then you can discard any
> thoughts of doing things with the HTTPS messages or port 443 traffic.
>
> If you have changed your mind and want to use SSL-Bump now, please
> re-describe what you want to actually happen now.
>
You have not described what you want to happen. Just asked how to do
this unknown thing...

I want to implement a URL filter using proxy server. My clients will use this server either from their web-browsers or via strongSwan IPSec VPN server. If they use the proxy server via VPN server, their VPN profile will have HTTP and HTTPS proxy server configuration.

This proxy server will filter HTTP and HTTPS websites based on ACL provided. For security reasons, I want to avoid using SSL-bump.


>
> Hi,
> Simply put, my question has three parts:
> 1. Can Squid be configured as an HTTPS proxy server without SSL-Bump?

* The term "HTTPS" is a generic term used to simultaneously describe two
completely different traffic syntaxes (CONNECT tunnels, and port 443 TLS).

* There are three proxy operating "modes" which may receive each of
those types of traffic (explicit/forward, intercept/tproxy, and
reverse/CDN/accel).

* For each type of traffic one mode is invalid, leaving 2x2= 4 different
sets of operations all called "proxying HTTPS".

This means the combinations are:
#1 CONNECT - explicit/forward
#2 443 TLS - explicit/forward

#3 CONNECT - intercept/tproxy
#4 443 TLS - intercept/tproxy

#5 CONNECT - reverse/CDN/accel
#6 443 TLS - reverse/CDN/accel

One of modes in each type is invalid. So, from Squid's HTTPS feature page, looks like my scenario falls either in #1 or #3.

* all 4 of those ways may be done with or without SSL-Bump feature.

When not using SSL-Bump 2 of the ways of "proxying HTTPS" will work, 2
will not.

When using SSL-Bump the non-working ways of "proxying HTTPS" will start
working, and the working ways will have an extra permutation of splice
vs bump operation that can happen. Extending the possibilities to be 6
ways of "proxying HTTPS".


So the answer(s) to your first question are:

yes, no.  yes, no.  no, yes.



> 2. If yes, then what other configurations have to performed other than
> "https_port XXXX"?
For the cases where the #1 answer was "yes" and not "no".

a) An explicit/forward or intercept proxy not using ssl-bump and
receiving CONNECT requests does not need any special configuration to
"proxy HTTPS". The proxy will simply enact the requested opaque tunnel
in accordance to HTTP rules.

So this means other than specifying "https_port XXXX" no other config is needed. 
When I setup Squid with just "https_port xxxx" and configured Firefox to use my proxy server for HTTP and HTTPS, it worked fine for HTTP but for HTTPS it gave "Proxy server rejected connection".

So either something is wrong in my squid.conf or my assumption is incorrect that my scenario falls in #1 or #3.


b) A reverse proxy requires the 'accel' mode flag, and the cert= option
must load the cert for the domain you are hosting on that port, and the
key= option must load the private key for that certificate.

c) all other modes will not work without SSL-Bump feature.



> 3. In this configuration, can Squid filter HTTPS requests from ACL?
>
That depends on the meaning of "this". There are 3 different
configurations in the answer to #2.

For (2a) - no. Only the CONNECT request can be filterd.

>From below links it looks like destination IP Address or hostname of a CONNECT request is same as HTTPS request. Is that correct?

https://en.m.wikipedia.org/wiki/HTTP_tunnel#HTTP_CONNECT_tunneling

http://stackoverflow.com/a/11698002/548403


For (2b) - yes. BUT, notice that it requires private key data for certs.
This configuration is only usable when _you own the domain_ which the
client is visiting.

For (2c) - SSL-Bump feature is the mechanism which enables https://
filtering for all traffic modes other than that described by (2b).
Without using that feature - no.


Do you understand now why every path you have tried ends up with how-tos
for configuring SSL-Bump?

Yes, thanks for the elaborate explanation.


HTH
Amos













From eliezer at ngtech.co.il  Mon Feb 13 06:36:12 2017
From: eliezer at ngtech.co.il (Eliezer  Croitoru)
Date: Mon, 13 Feb 2017 08:36:12 +0200
Subject: [squid-users] internal image call busted?
	/squid-internal-static/icons/SN.png
In-Reply-To: <2ac413b4-b7fb-f2d1-1a8c-8d2ab0b4d854@treenet.co.nz>
References: <CAEaSS0bcf8rdtMJ5O9jWLifgPjiP0_UqgBNxYWC83ZYgC2Hk_Q@mail.gmail.com>
 <CAEaSS0aae5=nLXSt5wHpbgy4vRW4kGobUYW4aGHhR_qwz2Cy0A@mail.gmail.com>
 <CAEaSS0ZcrLJwwjaN762fKFNhNtrMeX4pv4sz81gq+vw35YZY7w@mail.gmail.com>
 <2ac413b4-b7fb-f2d1-1a8c-8d2ab0b4d854@treenet.co.nz>
Message-ID: <016301d285c3$787c3a90$6974afb0$@ngtech.co.il>

Let me know if this is the package which I have released.
I am using Squid-Cache daily with my RPM's and I have not encountered such an issue.

Let me know if I can help with it.

Eliezer

----
Eliezer Croitoru
Linux System Administrator
Mobile: +972-5-28704261
Email: eliezer at ngtech.co.il


-----Original Message-----
From: squid-users [mailto:squid-users-bounces at lists.squid-cache.org] On Behalf Of Amos Jeffries
Sent: Sunday, February 12, 2017 10:54 AM
To: squid-users at lists.squid-cache.org
Subject: Re: [squid-users] internal image call busted? /squid-internal-static/icons/SN.png

On 11/02/2017 2:05 p.m., Tory M Blue wrote:
> 
> Path changed, default is wrong
> 
> Where the RPM installed it, the default expects /usr/local/blah....
> /usr/share/squid/icons/SN.png
> 
> Someone may want to figure out how, why this changed??
> 

Maybe. It would help if you mentioned where (which vendor) you got the package from so the persons in charge of making it can be aware its their package which needs attention.

Amos

_______________________________________________
squid-users mailing list
squid-users at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-users



From gozzy at yandex.ru  Mon Feb 13 12:18:59 2017
From: gozzy at yandex.ru (Alex)
Date: Mon, 13 Feb 2017 15:18:59 +0300
Subject: [squid-users] FTP relay with active client is broken?
In-Reply-To: <e5f12bb5-cbd9-2f98-6002-81e708707cb1@measurement-factory.com>
References: <4725111486473762@web39j.yandex.ru>
 <1069171486559444@web2m.yandex.ru>
 <e5f12bb5-cbd9-2f98-6002-81e708707cb1@measurement-factory.com>
Message-ID: <7540091486988339@web21o.yandex.ru>

  Well, actually it looks like a bug in squid to me. Some points:

1. When client connects with active FTP mode, squid opens a local socket and tries to assign server's IP address to it. So far so good, however this doesn't work (tested in both 'intercept' and 'tproxy' modes):

   2017/02/13 14:58:51.234| 50,3| comm.cc(347) comm_openex: comm_openex: Attempt open socket for: 172.17.10.30
   2017/02/13 14:58:51.234| 50,3| comm.cc(388) comm_openex: comm_openex: Opened socket local=172.17.10.30 remote=[::] FD 17 flags=1 : family=2, type=1, protocol=6
   2017/02/13 14:58:51.234| 5,5| comm.cc(420) comm_init_opened: local=172.17.10.30 remote=[::] FD 17 flags=1 is a new socket
   2017/02/13 14:58:51.234| 51,3| fd.cc(198) fd_open: fd_open() FD 17
   2017/02/13 14:58:51.234| commBind: Cannot bind socket FD 17 to 172.17.10.30: (99) Cannot assign requested address

2. Ok, we've got the errno. Let's look in comm_apply_flags(). There's something like this in the middle:

    /* MUST be done before binding or face OS Error: "(99) Cannot assign requested address"... */
    if ((flags & COMM_TRANSPARENT)) {
        comm_set_transparent(new_socket);
    }

    This means that socket is not IP_TRANSPARENT and can not be bound to arbitrary IP address ('net.ipv4.ip_nonlocal_bind' can fix bind(), but connect() will obviously fail with EINVAL).

3. Let's take a look on Ftp::Server::createDataConnection():

    Comm::ConnectionPointer conn = new Comm::Connection();
    conn->flags |= COMM_DOBIND;

    // Use local IP address of the control connection as the source address
    // of the active data connection, or some clients will refuse to accept.
    conn->setAddrs(clientConnection->local, cltAddr);

    Fine, looks reasonable. However, connection has only COMM_DOBIND flag, COMM_TRANSPARENT is missing. AFAIU, we'll never be able to bind and connect a socket...

4. Finally, in restoreCapabilities():

    if (Ip::Interceptor.TransparentActive() ||
                Ip::Qos::TheConfig.isHitNfmarkActive() ||
                Ip::Qos::TheConfig.isAclNfmarkActive() ||
                Ip::Qos::TheConfig.isAclTosActive()) {
            cap_list[ncaps] = CAP_NET_ADMIN;
            ++ncaps;
    }

    Hmm... Looks like that spoofing will not work for any mode other than 'tproxy', because there's no InterceptActive() check. But anyway, without COMM_TRANSPARENT it's useless.

  To sum up, I see some possible mistakes that may cause bugs in FTP relaying. I realise that probably I do something wrong also, however due to lack of documentation for FTP relay there's no simple way to check this :)


08.02.2017, 19:11, "Alex Rousskov" <rousskov at measurement-factory.com>:
> On 02/08/2017 06:10 AM, Alex wrote:
>
>> ?I've specified
>> ?'ftp_port 2121 intercept' and made squid intercept outgoing FTP
>> ?traffic according to the following rules:
>
>> ?iptables -t nat -A OUTPUT -p tcp -m owner --gid-owner squid -j ACCEPT
>> ?iptables -t nat -A OUTPUT -p tcp --dport 21 -j REDIRECT --to-port 2121
>
>> ?07.02.2017, 16:23, "Alex" <gozzy at yandex.ru>:
>
>>> ?I thought that active mode will cause less problems, but it seems
>>> ?that what squid tries to do is illegal. As far as I understand, in
>>> ?active mode squid tries to connect to a client and spoofs source IP
>>> ?address.
>
> Since spoofing client IP addresses is common for many working Squid
> interception setups doing HTTP, it has to be technically possible (i.e.,
> "legal" in your terminology). Unfortunately, I do not know enough
> low-level details to guide you further. Most likely, the FTP-specific
> Squid code facilitating IP spoofing is buggy or you are doing something
> wrong (or both).
>
> FWIW, IIRC, FTP interception code has worked for many folks.
>
> Let's hope that somebody with a working FTP interception setup speaks up.
>
> Alex.


From philip.munaawa at appliansys.com  Mon Feb 13 15:40:16 2017
From: philip.munaawa at appliansys.com (Philip Munaawa)
Date: Mon, 13 Feb 2017 15:40:16 +0000
Subject: [squid-users] Reverse proxy for HTTPS cloudfront server
Message-ID: <CANKLRvE1ZHS28mWtQ3QxJCD-GOa8k2n71HBpsnn2eRA3N0E3eg@mail.gmail.com>

I am trying to reverse proxy a site hosted on cloudfront, using the normal
https_port accel. I have the key/cert pair for the origin. The cloudfront
uses TLS/SNI to negotiate an SSL connection. However, when I try to connect
through the proxy, I get the error below in the logs:

Error negotiating SSL on FD 39: error:14094410:SSL
routines:SSL3_READ_BYTES:sslv3 alert handshake failure (1/0/0)

I have seen a similar issie with nginx, which was resolved by adding a
switch to send the server_host_name. see:
http://stackoverflow.com/questions/25329941/nginx-caching-proxy-fails-with-ssl23-get-server-hellosslv3-alert-handshake-fail

Does squid (3.5.24) have a similar switch/functionality?
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20170213/c5da39ab/attachment.htm>

From ahmed.zaeem at netstream.ps  Mon Feb 13 18:00:48 2017
From: ahmed.zaeem at netstream.ps (--Ahmad--)
Date: Mon, 13 Feb 2017 20:00:48 +0200
Subject: [squid-users] The header: HTTP_VIA is present with the value:
Message-ID: <8F1B782E-FCC1-4DDA-9073-EE25199DCF6D@netstream.ps>

An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20170213/c365d330/attachment.htm>

From yvoinov at gmail.com  Mon Feb 13 19:35:52 2017
From: yvoinov at gmail.com (Yuri Voinov)
Date: Tue, 14 Feb 2017 01:35:52 +0600
Subject: [squid-users] The header: HTTP_VIA is present with the value:
In-Reply-To: <8F1B782E-FCC1-4DDA-9073-EE25199DCF6D@netstream.ps>
References: <8F1B782E-FCC1-4DDA-9073-EE25199DCF6D@netstream.ps>
Message-ID: <a1f8c41c-30eb-d92a-edb2-02c71d3fdbe6@gmail.com>

via off


14.02.2017 0:00, --Ahmad-- ?????:
> hi folks 
> I?m checking my proxy in 
>
> whatismyproxy.com <http://whatismyproxy.com>
>
> and it says :
>
> The header: HTTP_VIA is present with the value:HTTP/1.1
> vnnnz01msp2tser1.wnsnet.attws.com
> <http://vnnnz01msp2tser1.wnsnet.attws.com>.
>
> is there anyway that i remove that proxy detection ?
>
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users

-- 
Bugs to the Future
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20170214/19eb865d/attachment.htm>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: 0x613DEC46.asc
Type: application/pgp-keys
Size: 2437 bytes
Desc: not available
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20170214/19eb865d/attachment.key>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: signature.asc
Type: application/pgp-signature
Size: 473 bytes
Desc: OpenPGP digital signature
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20170214/19eb865d/attachment.sig>

From squid3 at treenet.co.nz  Tue Feb 14 01:58:42 2017
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Tue, 14 Feb 2017 14:58:42 +1300
Subject: [squid-users] FTP relay with active client is broken?
In-Reply-To: <7540091486988339@web21o.yandex.ru>
References: <4725111486473762@web39j.yandex.ru>
 <1069171486559444@web2m.yandex.ru>
 <e5f12bb5-cbd9-2f98-6002-81e708707cb1@measurement-factory.com>
 <7540091486988339@web21o.yandex.ru>
Message-ID: <85666142-b537-ac46-a740-3d9f1c491b7a@treenet.co.nz>

On 14/02/2017 1:18 a.m., Alex wrote:
>   Well, actually it looks like a bug in squid to me. Some points:
> 
> 1. When client connects with active FTP mode, squid opens a local socket and tries to assign server's IP address to it. So far so good, however this doesn't work (tested in both 'intercept' and 'tproxy' modes):
> 
>    2017/02/13 14:58:51.234| 50,3| comm.cc(347) comm_openex: comm_openex: Attempt open socket for: 172.17.10.30
>    2017/02/13 14:58:51.234| 50,3| comm.cc(388) comm_openex: comm_openex: Opened socket local=172.17.10.30 remote=[::] FD 17 flags=1 : family=2, type=1, protocol=6
>    2017/02/13 14:58:51.234| 5,5| comm.cc(420) comm_init_opened: local=172.17.10.30 remote=[::] FD 17 flags=1 is a new socket
>    2017/02/13 14:58:51.234| 51,3| fd.cc(198) fd_open: fd_open() FD 17
>    2017/02/13 14:58:51.234| commBind: Cannot bind socket FD 17 to 172.17.10.30: (99) Cannot assign requested address
> 
> 2. Ok, we've got the errno. Let's look in comm_apply_flags(). There's something like this in the middle:
> 
>     /* MUST be done before binding or face OS Error: "(99) Cannot assign requested address"... */
>     if ((flags & COMM_TRANSPARENT)) {
>         comm_set_transparent(new_socket);
>     }
> 
>     This means that socket is not IP_TRANSPARENT and can not be bound to arbitrary IP address ('net.ipv4.ip_nonlocal_bind' can fix bind(), but connect() will obviously fail with EINVAL).
> 
> 3. Let's take a look on Ftp::Server::createDataConnection():
> 
>     Comm::ConnectionPointer conn = new Comm::Connection();
>     conn->flags |= COMM_DOBIND;
> 
>     // Use local IP address of the control connection as the source address
>     // of the active data connection, or some clients will refuse to accept.
>     conn->setAddrs(clientConnection->local, cltAddr);
> 
>     Fine, looks reasonable. However, connection has only COMM_DOBIND flag, COMM_TRANSPARENT is missing. AFAIU, we'll never be able to bind and connect a socket...
> 

Good catch. Thank you.

So AFAICS,
* NAT intercept needs to listen on the local IP of the Squid->server
control connection.
* TPROXY needs to listen on the local IP of the client->Squid control
connection plus the TRANSPARENT flag.

Any chance of a patch?

Amos



From squid3 at treenet.co.nz  Tue Feb 14 02:40:34 2017
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Tue, 14 Feb 2017 15:40:34 +1300
Subject: [squid-users] Reverse proxy for HTTPS cloudfront server
In-Reply-To: <CANKLRvE1ZHS28mWtQ3QxJCD-GOa8k2n71HBpsnn2eRA3N0E3eg@mail.gmail.com>
References: <CANKLRvE1ZHS28mWtQ3QxJCD-GOa8k2n71HBpsnn2eRA3N0E3eg@mail.gmail.com>
Message-ID: <d8d13e22-680d-11b0-8ca4-552a20aa9a9b@treenet.co.nz>

On 14/02/2017 4:40 a.m., Philip Munaawa wrote:
> I am trying to reverse proxy a site hosted on cloudfront, using the normal
> https_port accel. I have the key/cert pair for the origin. The cloudfront
> uses TLS/SNI to negotiate an SSL connection. However, when I try to connect
> through the proxy, I get the error below in the logs:
> 
> Error negotiating SSL on FD 39: error:14094410:SSL
> routines:SSL3_READ_BYTES:sslv3 alert handshake failure (1/0/0)
> 
> I have seen a similar issie with nginx, which was resolved by adding a
> switch to send the server_host_name. see:
> http://stackoverflow.com/questions/25329941/nginx-caching-proxy-fails-with-ssl23-get-server-hellosslv3-alert-handshake-fail
> 
> Does squid (3.5.24) have a similar switch/functionality?
> 

The only thing that SSL23_SERVER_HELLO and SSL3_READ_BYTES have in
common is that they are errors. So no they are not "similar".

The server is closing the connection without reporting what the problem
actually is. Squid-3.5 should already be sending SNI using the
cache_peer hostname or request-URL hostname.

<http://openssl.6102.n7.nabble.com/error-14094410-SSL-routines-SSL3-READ-BYTES-sslv3-alert-handshake-failure-td12398.html>
has some hints from Dave Thompson on how to find out what is going on
with the server.

Amos



From gozzy at yandex.ru  Tue Feb 14 06:00:24 2017
From: gozzy at yandex.ru (Alex)
Date: Tue, 14 Feb 2017 09:00:24 +0300
Subject: [squid-users] FTP relay with active client is broken?
In-Reply-To: <85666142-b537-ac46-a740-3d9f1c491b7a@treenet.co.nz>
References: <4725111486473762@web39j.yandex.ru>
 <1069171486559444@web2m.yandex.ru>
 <e5f12bb5-cbd9-2f98-6002-81e708707cb1@measurement-factory.com>
 <7540091486988339@web21o.yandex.ru>
 <85666142-b537-ac46-a740-3d9f1c491b7a@treenet.co.nz>
Message-ID: <2880921487052024@web15h.yandex.ru>

Well, I can try to make a patch for this... Two questions:
1. I should send it to squid-dev, do I?
2. Source code for which version should I use: 4.0 or 3.5?

14.02.2017, 04:59, "Amos Jeffries" <squid3 at treenet.co.nz>:
> On 14/02/2017 1:18 a.m., Alex wrote:
>> ???Well, actually it looks like a bug in squid to me. Some points:
>>
>> ?1. When client connects with active FTP mode, squid opens a local socket and tries to assign server's IP address to it. So far so good, however this doesn't work (tested in both 'intercept' and 'tproxy' modes):
>>
>> ????2017/02/13 14:58:51.234| 50,3| comm.cc(347) comm_openex: comm_openex: Attempt open socket for: 172.17.10.30
>> ????2017/02/13 14:58:51.234| 50,3| comm.cc(388) comm_openex: comm_openex: Opened socket local=172.17.10.30 remote=[::] FD 17 flags=1 : family=2, type=1, protocol=6
>> ????2017/02/13 14:58:51.234| 5,5| comm.cc(420) comm_init_opened: local=172.17.10.30 remote=[::] FD 17 flags=1 is a new socket
>> ????2017/02/13 14:58:51.234| 51,3| fd.cc(198) fd_open: fd_open() FD 17
>> ????2017/02/13 14:58:51.234| commBind: Cannot bind socket FD 17 to 172.17.10.30: (99) Cannot assign requested address
>>
>> ?2. Ok, we've got the errno. Let's look in comm_apply_flags(). There's something like this in the middle:
>>
>> ?????/* MUST be done before binding or face OS Error: "(99) Cannot assign requested address"... */
>> ?????if ((flags & COMM_TRANSPARENT)) {
>> ?????????comm_set_transparent(new_socket);
>> ?????}
>>
>> ?????This means that socket is not IP_TRANSPARENT and can not be bound to arbitrary IP address ('net.ipv4.ip_nonlocal_bind' can fix bind(), but connect() will obviously fail with EINVAL).
>>
>> ?3. Let's take a look on Ftp::Server::createDataConnection():
>>
>> ?????Comm::ConnectionPointer conn = new Comm::Connection();
>> ?????conn->flags |= COMM_DOBIND;
>>
>> ?????// Use local IP address of the control connection as the source address
>> ?????// of the active data connection, or some clients will refuse to accept.
>> ?????conn->setAddrs(clientConnection->local, cltAddr);
>>
>> ?????Fine, looks reasonable. However, connection has only COMM_DOBIND flag, COMM_TRANSPARENT is missing. AFAIU, we'll never be able to bind and connect a socket...
>
> Good catch. Thank you.
>
> So AFAICS,
> * NAT intercept needs to listen on the local IP of the Squid->server
> control connection.
> * TPROXY needs to listen on the local IP of the client->Squid control
> connection plus the TRANSPARENT flag.
>
> Any chance of a patch?
>
> Amos
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users


From johnpearson555 at gmail.com  Tue Feb 14 06:24:52 2017
From: johnpearson555 at gmail.com (John Pearson)
Date: Mon, 13 Feb 2017 22:24:52 -0800
Subject: [squid-users] Squid on separate box and it can't see packets
Message-ID: <CAKNtY_x5YX-LAhy30Q3n1aeZ_XbeKUf7YyUd8WFKdWYkr9K_Mg@mail.gmail.com>

Hi all,
I have squid on a separate box on my network with ip address 192.168.1.2

In squid.conf I have:

http_port 0.0.0.0:3128
http_port 0.0.0.0:3129 intercept

-------

On squid box:

$ sudo netstat -lnp | grep squid
tcp        0      0 0.0.0.0:3128            0.0.0.0:*               LISTEN
     2639/(squid-1)
tcp        0      0 0.0.0.0:3129            0.0.0.0:*               LISTEN
     2639/(squid-1)
udp        0      0 0.0.0.0:37444           0.0.0.0:*
    2639/(squid-1)
udp6       0      0 :::41465                :::*
     2639/(squid-1)

-------

I followed this example:
http://wiki.squid-cache.org/ConfigExamples/Intercept/LinuxRedirect

iptables:

# your proxy IP
SQUIDIP=192.168.1.2

# your proxy listening port
SQUIDPORT=3129


iptables -t nat -A PREROUTING -s $SQUIDIP -p tcp --dport 80 -j ACCEPT
iptables -t nat -A PREROUTING -p tcp --dport 80 -j REDIRECT --to-port
$SQUIDPORT
iptables -t nat -A POSTROUTING -j MASQUERADE
iptables -t mangle -A PREROUTING -p tcp --dport $SQUIDPORT -j DROP

------

I am redirecting port 80 packets on my router to squid box

On one of the clients: 192.168.1.8, I am running
wget -v --bind-address=192.168.1.8 http://squid-cache.org:80

On squid box, I am running tcpdump and I am able to see those packets:

22:09:58.962316 IP 192.168.1.8.52219 > lists.squid-cache.org.http: Flags
[S], seq 1999822717, win 29200, options [mss 1460,sackOK,TS val 26932460
ecr 0,nop,wscale 7], length 0
22:09:59.958994 IP 192.168.1.8.52219 > lists.squid-cache.org.http: Flags
[S], seq 1999822717, win 29200, options [mss 1460,sackOK,TS val 26932560
ecr 0,nop,wscale 7], length 0
22:10:01.958981 IP 192.168.1.8.52219 > lists.squid-cache.org.http: Flags
[S], seq 1999822717, win 29200, options [mss 1460,sackOK,TS val 26932760
ecr 0,nop,wscale 7], length 0

But squid is not seeing them. Squid log is empty.

Need advice. Thanks!
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20170213/7a0884db/attachment.htm>

From eliezer at ngtech.co.il  Tue Feb 14 06:44:43 2017
From: eliezer at ngtech.co.il (Eliezer  Croitoru)
Date: Tue, 14 Feb 2017 08:44:43 +0200
Subject: [squid-users] Squid on separate box and it can't see packets
In-Reply-To: <CAKNtY_x5YX-LAhy30Q3n1aeZ_XbeKUf7YyUd8WFKdWYkr9K_Mg@mail.gmail.com>
References: <CAKNtY_x5YX-LAhy30Q3n1aeZ_XbeKUf7YyUd8WFKdWYkr9K_Mg@mail.gmail.com>
Message-ID: <0fea01d2868d$d6fab130$84f01390$@ngtech.co.il>

Hey,

There are couple missing pieces(in my eyes) in order to understand the picture.
Is this squid box a router or just a proxy?
What tcpdump command did you ran?
What is the networks that are involved?
What is the gateway and dhcp for this network?
If the client is a linux box then we need the output of:
$ ifconfig
$ route -n 
Or
$ ip route

Thanks,
Eliezer

----
http://ngtech.co.il/lmgtfy/
Linux System Administrator
Mobile: +972-5-28704261
Email: eliezer at ngtech.co.il


From: squid-users [mailto:squid-users-bounces at lists.squid-cache.org] On Behalf Of John Pearson
Sent: Tuesday, February 14, 2017 8:25 AM
To: Squid Users <squid-users at lists.squid-cache.org>
Subject: [squid-users] Squid on separate box and it can't see packets

Hi all,
I have squid on a separate box on my network with ip address 192.168.1.2

In squid.conf I have:

http_port http://0.0.0.0:3128
http_port http://0.0.0.0:3129 intercept

-------

On squid box:

$ sudo netstat -lnp | grep squid
tcp        0      0 http://0.0.0.0:3128            0.0.0.0:*               LISTEN      2639/(squid-1)
tcp        0      0 http://0.0.0.0:3129            0.0.0.0:*               LISTEN      2639/(squid-1)
udp        0      0 http://0.0.0.0:37444           0.0.0.0:*                           2639/(squid-1)
udp6       0      0 :::41465                :::*                                2639/(squid-1)

-------

I followed this example: http://wiki.squid-cache.org/ConfigExamples/Intercept/LinuxRedirect

iptables:

# your proxy IP
SQUIDIP=192.168.1.2

# your proxy listening port
SQUIDPORT=3129


iptables -t nat -A PREROUTING -s $SQUIDIP -p tcp --dport 80 -j ACCEPT
iptables -t nat -A PREROUTING -p tcp --dport 80 -j REDIRECT --to-port $SQUIDPORT
iptables -t nat -A POSTROUTING -j MASQUERADE
iptables -t mangle -A PREROUTING -p tcp --dport $SQUIDPORT -j DROP

------

I am redirecting port 80 packets on my router to squid box

On one of the clients: 192.168.1.8, I am running
wget -v --bind-address=192.168.1.8 http://squid-cache.org:80

On squid box, I am running tcpdump and I am able to see those packets:

22:09:58.962316 IP 192.168.1.8.52219 > lists.squid-cache.org.http: Flags [S], seq 1999822717, win 29200, options [mss 1460,sackOK,TS val 26932460 ecr 0,nop,wscale 7], length 0
22:09:59.958994 IP 192.168.1.8.52219 > lists.squid-cache.org.http: Flags [S], seq 1999822717, win 29200, options [mss 1460,sackOK,TS val 26932560 ecr 0,nop,wscale 7], length 0
22:10:01.958981 IP 192.168.1.8.52219 > lists.squid-cache.org.http: Flags [S], seq 1999822717, win 29200, options [mss 1460,sackOK,TS val 26932760 ecr 0,nop,wscale 7], length 0

But squid is not seeing them. Squid log is empty.

Need advice. Thanks!



From philip.munaawa at appliansys.com  Tue Feb 14 09:48:12 2017
From: philip.munaawa at appliansys.com (Philip Munaawa)
Date: Tue, 14 Feb 2017 09:48:12 +0000
Subject: [squid-users] Reverse proxy for HTTPS cloudfront server
In-Reply-To: <d8d13e22-680d-11b0-8ca4-552a20aa9a9b@treenet.co.nz>
References: <CANKLRvE1ZHS28mWtQ3QxJCD-GOa8k2n71HBpsnn2eRA3N0E3eg@mail.gmail.com>
 <d8d13e22-680d-11b0-8ca4-552a20aa9a9b@treenet.co.nz>
Message-ID: <CANKLRvFK1AQmg7n3vPdfGzCu8GiaZ7GZiuoeHFoMfGaLu0vNBQ@mail.gmail.com>

openssl test to reproduce the error:

openssl s_client  -connect www.coursera.org:443 - FAILS (Testing with
cousera since it is also hosted on cloudfront, and uses TLS/SNI)

CONNECTED(00000003)
140225331586752:error:14077410:SSL routines:SSL23_GET_SERVER_HELLO:sslv3
alert handshake failure:s23_clnt.c:757:

openssl s_client  -connect www.coursera.org:443 -servername www.coursera.org
- SUCCEEDS



Also, with nginx, if I switch off the proxy_ssl_server_name flag, I get the
same openssl error as squid,

2017/02/14 09:20:39 [error] 23604#0: *6 SSL_do_handshake() failed (SSL:
error:14077410:SSL routines:SSL23_GET_SERVER_HELLO:sslv3 alert handshake
failure) while S
SL handshaking to upstream,..."

This makes me suspect that squid is not sending the servername ...



On 14 February 2017 at 02:40, Amos Jeffries <squid3 at treenet.co.nz> wrote:

> On 14/02/2017 4:40 a.m., Philip Munaawa wrote:
> > I am trying to reverse proxy a site hosted on cloudfront, using the
> normal
> > https_port accel. I have the key/cert pair for the origin. The cloudfront
> > uses TLS/SNI to negotiate an SSL connection. However, when I try to
> connect
> > through the proxy, I get the error below in the logs:
> >
> > Error negotiating SSL on FD 39: error:14094410:SSL
> > routines:SSL3_READ_BYTES:sslv3 alert handshake failure (1/0/0)
> >
> > I have seen a similar issie with nginx, which was resolved by adding a
> > switch to send the server_host_name. see:
> > http://stackoverflow.com/questions/25329941/nginx-
> caching-proxy-fails-with-ssl23-get-server-hellosslv3-alert-handshake-fail
> >
> > Does squid (3.5.24) have a similar switch/functionality?
> >
>
> The only thing that SSL23_SERVER_HELLO and SSL3_READ_BYTES have in
> common is that they are errors. So no they are not "similar".
>
> The server is closing the connection without reporting what the problem
> actually is. Squid-3.5 should already be sending SNI using the
> cache_peer hostname or request-URL hostname.
>
> <http://openssl.6102.n7.nabble.com/error-14094410-SSL-
> routines-SSL3-READ-BYTES-sslv3-alert-handshake-failure-td12398.html>
> has some hints from Dave Thompson on how to find out what is going on
> with the server.
>
> Amos
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20170214/1c29bf8c/attachment.htm>

From craig.gowing at appliansys.com  Tue Feb 14 10:51:43 2017
From: craig.gowing at appliansys.com (Craig Gowing)
Date: Tue, 14 Feb 2017 02:51:43 -0800 (PST)
Subject: [squid-users] Reverse proxy for HTTPS cloudfront server
In-Reply-To: <CANKLRvFK1AQmg7n3vPdfGzCu8GiaZ7GZiuoeHFoMfGaLu0vNBQ@mail.gmail.com>
References: <CANKLRvE1ZHS28mWtQ3QxJCD-GOa8k2n71HBpsnn2eRA3N0E3eg@mail.gmail.com>
 <d8d13e22-680d-11b0-8ca4-552a20aa9a9b@treenet.co.nz>
 <CANKLRvFK1AQmg7n3vPdfGzCu8GiaZ7GZiuoeHFoMfGaLu0vNBQ@mail.gmail.com>
Message-ID: <1487069503551-4681542.post@n4.nabble.com>

>From what I can tell the SNI is not added for cache peers. In
Ssl::PeerConnector::initializeSsl if "peer" is set then the call to
Ssl::setClientSNI is skipped. Also the SSL context doesn't have the hostname
or a callback set, and sslCreateClientContext doesn't appear to be able to
set it either.

I've tested with a quick patch which appears to the fix the issue: (however
I feel it should take into account the forcedomain option as well)

diff --git a/src/ssl/PeerConnector.cc b/src/ssl/PeerConnector.cc
index f5d4c81..178c685 100644
--- a/src/ssl/PeerConnector.cc
+++ b/src/ssl/PeerConnector.cc
@@ -133,6 +133,7 @@ Ssl::PeerConnector::initializeSsl()
     if (peer) {
         SBuf *host = new SBuf(peer->ssldomain ? peer->ssldomain :
peer->host);
         SSL_set_ex_data(ssl, ssl_ex_index_server, host);
+        Ssl::setClientSNI(ssl, host->c_str());
 
         if (peer->sslSession)
             SSL_set_session(ssl, peer->sslSession);





--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/Reverse-proxy-for-HTTPS-cloudfront-server-tp4681533p4681542.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From craig.gowing at appliansys.com  Tue Feb 14 17:27:03 2017
From: craig.gowing at appliansys.com (Craig Gowing)
Date: Tue, 14 Feb 2017 17:27:03 +0000
Subject: [squid-users] ACL dst handled differently in intercept after rewrite
Message-ID: <CA++FD2N4ORi1NSusWeOFturFpi-BqDK4Qy0wj6EJbJFjvyzbjg@mail.gmail.com>

Hi all,

I've got a squid server running which allows direct proxy and also can
intercept traffic:

http_port 10.0.0.1:3128
http_port 10.0.0.1:3129 intercept

---

There is a URL rewriter which allows the incoming requests (this is just an
example, I don't really allow all):

url_rewrite_access allow all
url_rewrite_program /usr/bin/myrewriter

---

This rewriter will rewrite some URLs to a host on the same network, with
the intention that the request should not be cached by squid, eg
http://example.net/somefile.bin -> http://10.0.0.2/example.net/somefile.bin
So a cache_deny directive is used for this:

acl local_store dst 10.0.0.2
cache deny local_store

---

Now when requesting this URL using a defined proxy the ACL matches and the
request is not cached. If using intercept the ACL does not match and it
does get cached (which caused some storage duplication on the network)
The debug info shows the following:

Proxy:
curl -x "10.0.0.1:3128" "http://example.net/somefile.bin" > /dev/null
Ip.cc(95) aclIpAddrNetworkCompare: aclIpAddrNetworkCompare: compare:
10.0.0.2/[ffff:ffff:ffff:ffff:ffff:ffff:ffff:ffff] (10.0.0.2)  vs
10.0.0.2-[::]/[ffff:ffff:ffff:ffff:ffff:ffff:ffff:ffff]

Intercept:
curl "http://example.net/somefile.bin" > /dev/null # Intercepted on the NAT
tables
Ip.cc(95) aclIpAddrNetworkCompare: aclIpAddrNetworkCompare: compare:
93.184.216.34:80/[ffff:ffff:ffff:ffff:ffff:ffff:ffff:ffff] (93.184.216.34:80)
 vs 10.0.0.2-[::]/[ffff:ffff:ffff:ffff:ffff:ffff:ffff:ffff]

This seems to show that the ACL is processed at a different stage for the
two different modes. Now I'm wondering if this is intentional and I
shouldn't be using the 'dst' ACL here, or should it be more consistant and
give the same result regardless?

I have a solution to use the 'url_regex' ACL instead which seems consistant
between the two modes, but it may slightly affect performance.

I couldn't find a huge amount of info on what order the ACLs are processed,
so if anybody could let me know what the expected behaviour should be that
would be much appreciated.

Thanks,
Craig
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20170214/c33cd7ab/attachment.htm>

From johnpearson555 at gmail.com  Tue Feb 14 20:18:25 2017
From: johnpearson555 at gmail.com (John Pearson)
Date: Tue, 14 Feb 2017 12:18:25 -0800
Subject: [squid-users] Squid on separate box and it can't see packets
In-Reply-To: <0fea01d2868d$d6fab130$84f01390$@ngtech.co.il>
References: <CAKNtY_x5YX-LAhy30Q3n1aeZ_XbeKUf7YyUd8WFKdWYkr9K_Mg@mail.gmail.com>
 <0fea01d2868d$d6fab130$84f01390$@ngtech.co.il>
Message-ID: <CAKNtY_za61AyNijVgLv=+M4tW8JeWgHQVNwvs1pJjfU62vCWTw@mail.gmail.com>

Hi,

Is this squid box a router or just a proxy?
- just a proxy

What tcpdump command did you ran?
- sudo tcpdump -i eth0

What is the networks that are involved?
Setup:

> Client        (192.168.1.8) --->  |     Rotuer        |
>                                                | gateway/dhcp | --->
> Internet
> Squid box (192.168.1.2) --->  |  192.168.1.1   |


Here Client (debian), squid (debian) and router are three separate devices.

What is the gateway and dhcp for this network?
- Router is both gateway and dhcp server

If the client is a linux box then we need the output of:

ifconfig:

> eth0   Link encap:Ethernet  HWaddr b8:27:eb:91:83:20
>           inet addr:192.168.1.8  Bcast:192.168.1.255  Mask:255.255.255.0
>           inet6 addr: fe80::6236:7570:1f1e:d238/64 Scope:Link
>           UP BROADCAST RUNNING MULTICAST  MTU:1500  Metric:1
>           RX packets:3214 errors:0 dropped:0 overruns:0 frame:0
>           TX packets:8985 errors:0 dropped:0 overruns:0 carrier:0
>           collisions:0 txqueuelen:1000
>           RX bytes:478898 (467.6 KiB)  TX bytes:2308050 (2.2 MiB)



ip route:

> default via 192.168.1.1 dev eth0
> 169.254.0.0/16 dev eth0  proto kernel  scope link  src 169.254.219.186
>  metric 202


On Mon, Feb 13, 2017 at 10:44 PM, Eliezer Croitoru <eliezer at ngtech.co.il>
wrote:

> Hey,
>
> There are couple missing pieces(in my eyes) in order to understand the
> picture.
> Is this squid box a router or just a proxy?
> What tcpdump command did you ran?
> What is the networks that are involved?
> What is the gateway and dhcp for this network?
> If the client is a linux box then we need the output of:
> $ ifconfig
> $ route -n
> Or
> $ ip route
>
> Thanks,
> Eliezer
>
> ----
> http://ngtech.co.il/lmgtfy/
> Linux System Administrator
> Mobile: +972-5-28704261
> Email: eliezer at ngtech.co.il
>
>
> From: squid-users [mailto:squid-users-bounces at lists.squid-cache.org] On
> Behalf Of John Pearson
> Sent: Tuesday, February 14, 2017 8:25 AM
> To: Squid Users <squid-users at lists.squid-cache.org>
> Subject: [squid-users] Squid on separate box and it can't see packets
>
> Hi all,
> I have squid on a separate box on my network with ip address 192.168.1.2
>
> In squid.conf I have:
>
> http_port http://0.0.0.0:3128
> http_port http://0.0.0.0:3129 intercept
>
> -------
>
> On squid box:
>
> $ sudo netstat -lnp | grep squid
> tcp        0      0 http://0.0.0.0:3128            0.0.0.0:*
>  LISTEN      2639/(squid-1)
> tcp        0      0 http://0.0.0.0:3129            0.0.0.0:*
>  LISTEN      2639/(squid-1)
> udp        0      0 http://0.0.0.0:37444           0.0.0.0:*
>              2639/(squid-1)
> udp6       0      0 :::41465                :::*
>       2639/(squid-1)
>
> -------
>
> I followed this example: http://wiki.squid-cache.org/
> ConfigExamples/Intercept/LinuxRedirect
>
> iptables:
>
> # your proxy IP
> SQUIDIP=192.168.1.2
>
> # your proxy listening port
> SQUIDPORT=3129
>
>
> iptables -t nat -A PREROUTING -s $SQUIDIP -p tcp --dport 80 -j ACCEPT
> iptables -t nat -A PREROUTING -p tcp --dport 80 -j REDIRECT --to-port
> $SQUIDPORT
> iptables -t nat -A POSTROUTING -j MASQUERADE
> iptables -t mangle -A PREROUTING -p tcp --dport $SQUIDPORT -j DROP
>
> ------
>
> I am redirecting port 80 packets on my router to squid box
>
> On one of the clients: 192.168.1.8, I am running
> wget -v --bind-address=192.168.1.8 http://squid-cache.org:80
>
> On squid box, I am running tcpdump and I am able to see those packets:
>
> 22:09:58.962316 IP 192.168.1.8.52219 > lists.squid-cache.org.http: Flags
> [S], seq 1999822717, win 29200, options [mss 1460,sackOK,TS val 26932460
> ecr 0,nop,wscale 7], length 0
> 22:09:59.958994 IP 192.168.1.8.52219 > lists.squid-cache.org.http: Flags
> [S], seq 1999822717, win 29200, options [mss 1460,sackOK,TS val 26932560
> ecr 0,nop,wscale 7], length 0
> 22:10:01.958981 IP 192.168.1.8.52219 > lists.squid-cache.org.http: Flags
> [S], seq 1999822717, win 29200, options [mss 1460,sackOK,TS val 26932760
> ecr 0,nop,wscale 7], length 0
>
> But squid is not seeing them. Squid log is empty.
>
> Need advice. Thanks!
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20170214/837091ee/attachment.htm>

From varun.singh at gslab.com  Wed Feb 15 07:21:35 2017
From: varun.singh at gslab.com (Varun Singh)
Date: Wed, 15 Feb 2017 12:51:35 +0530
Subject: [squid-users] Basic HTTPS filtering via CONNECT in Squid
In-Reply-To: <016101d285c3$356b0a10$a0411e30$@ngtech.co.il>
References: <CABUhpQWFcOPHS7Or9rybcs18iTjDpG=guajotsZaNc6mxykApA@mail.gmail.com>
 <130414a3-f1c4-30c6-8233-c0386e2d4dcc@treenet.co.nz>
 <CABUhpQU5WC3ntYoNx+ZzyDmeE_=U466x_BmoNQMMQqhXsJehEA@mail.gmail.com>
 <d9a1230e-9773-b4a2-c606-cbd1834364d0@treenet.co.nz>
 <CABUhpQWZ+47EE7LWs=Bxok5rnbNN+Ab7WYFkj=u6-FGbm+qn+w@mail.gmail.com>
 <CABUhpQWtuEGz_xn=qS7zrCM+K62Y-sM2cDHfxNTM+aPiUB531Q@mail.gmail.com>
 <ea582b53-5cf0-e0f0-0134-8b668d40f457@treenet.co.nz>
 <CABUhpQVg6O0DZqdmJwKw_0B9Nb4bd-BiX4gMpnjGLa_yoRfKJQ@mail.gmail.com>
 <CABUhpQUAVXGdZ4iTvX_Qum9SUC7BwvcZjLNbQO5jSzRgpsvq8Q@mail.gmail.com>
 <5e564444-587c-cf7e-16d3-15e2103aad94@treenet.co.nz>
 <CABUhpQUdafg=vN1CviLQsVwffbaPjbC+ByNr-D-6J91S6qmY3A@mail.gmail.com>
 <016101d285c3$356b0a10$a0411e30$@ngtech.co.il>
Message-ID: <CABUhpQW_M9pCzC4CzbXmyhRo_djNFMgOyDJT0_uzQSsKcDZ9jw@mail.gmail.com>

On Mon, Feb 13, 2017 at 12:04 PM, Eliezer Croitoru <eliezer at ngtech.co.il> wrote:
>
> Hey Varun,
>
> Filtering content based on the URL level\layer of the connection is not possible without SSL-bump.
> There for you must use for some aspect of the connections SSL-bump.
> However you can selectively choose which destinations would be bumped and which are not.
> Most of the current browsers supports SNI which allows squid in some degree to decide if to fully bump the connection to the URL level or to decide to only proxy the connection in the TCP level.
> As simple as it sounds URL level filtering requires full SSL-bump and TCP and basic TLS level filtering will not require you to fully utilize SSL-bump but will require you to fully setup squid for SSL-bump.
>
> This is the place to clarify that SNI based filtering is not 100% bullet proof and it could be exploited to override in a way your basic SNI based SSL level filtering.
>
> Do you have specific sites that you want to filter in the URL level or just globally?
> The answer to the above question will guide us towards what might be the right path for your solution(which could be full SSL-BUMP or partial).
>
> Eliezer
>
> ----
> http://ngtech.co.il/lmgtfy/
> Linux System Administrator
> Mobile: +972-5-28704261
> Email: eliezer at ngtech.co.il
>
>
> From: squid-users [mailto:squid-users-bounces at lists.squid-cache.org] On Behalf Of Varun Singh
> Sent: Monday, February 13, 2017 5:37 AM
> To: Amos Jeffries <squid3 at treenet.co.nz>
> Cc: squid-users at lists.squid-cache.org
> Subject: Re: [squid-users] Basic HTTPS filtering via CONNECT in Squid
>
>
>
> On Feb 12, 2017 5:43 PM, "Amos Jeffries" <mailto:squid3 at treenet.co.nz> wrote:
> On 12/02/2017 11:51 p.m., Varun Singh wrote:
> > On Feb 12, 2017 2:21 PM, "Amos Jeffries" <mailto:squid3 at treenet.co.nz> wrote:
> >
> > On 12/02/2017 7:40 p.m., Varun Singh wrote:
> >>
> >> The answer points to installing a CA on client.
> >
> > The question was about how to get browsers talking TLS *directly to a
> > Squid reverse-proxy*. Your Ubuntu package is not capable of that and you
> > are not using a reverse-proxy.
> >
> >> Does this mean even if I don't want Squid-in-the-middle approach, my
> >> clients would still have to install a certificate?
> >
> > No. It is irrelevant to yrou sitation.
> >
> >
> > You began this thread with a simple question:
> >
> >> Hi,
> >> I have a Squid 3 installed on Ubuntu 16.04. It works perfectly as an
> >> HTTP proxy server in transparent mode.
> >> I wanted to know whether it can be configured to run as HTTPS proxy
> >> server without ssl-bump i.e. without 'man in the middle attack'
> >> technique.
> >
> >
> > Everything you have been asking about since then is various ways to do
> > parts of the SSL-bump process. Which does not fit very well with the
> > "without ssl-bump" requirement.
> >
> >
> > Simply put; if you are not going to SSL-Bump then you can discard any
> > thoughts of doing things with the HTTPS messages or port 443 traffic.
> >
> > If you have changed your mind and want to use SSL-Bump now, please
> > re-describe what you want to actually happen now.
> >
> You have not described what you want to happen. Just asked how to do
> this unknown thing...
>
> I want to implement a URL filter using proxy server. My clients will use this server either from their web-browsers or via strongSwan IPSec VPN server. If they use the proxy server via VPN server, their VPN profile will have HTTP and HTTPS proxy server configuration.
>
> This proxy server will filter HTTP and HTTPS websites based on ACL provided. For security reasons, I want to avoid using SSL-bump.
>
>
> >
> > Hi,
> > Simply put, my question has three parts:
> > 1. Can Squid be configured as an HTTPS proxy server without SSL-Bump?
>
> * The term "HTTPS" is a generic term used to simultaneously describe two
> completely different traffic syntaxes (CONNECT tunnels, and port 443 TLS).
>
> * There are three proxy operating "modes" which may receive each of
> those types of traffic (explicit/forward, intercept/tproxy, and
> reverse/CDN/accel).
>
> * For each type of traffic one mode is invalid, leaving 2x2= 4 different
> sets of operations all called "proxying HTTPS".
>
> This means the combinations are:
> #1 CONNECT - explicit/forward
> #2 443 TLS - explicit/forward
>
> #3 CONNECT - intercept/tproxy
> #4 443 TLS - intercept/tproxy
>
> #5 CONNECT - reverse/CDN/accel
> #6 443 TLS - reverse/CDN/accel
>
> One of modes in each type is invalid. So, from Squid's HTTPS feature page, looks like my scenario falls either in #1 or #3.
>
> * all 4 of those ways may be done with or without SSL-Bump feature.
>
> When not using SSL-Bump 2 of the ways of "proxying HTTPS" will work, 2
> will not.
>
> When using SSL-Bump the non-working ways of "proxying HTTPS" will start
> working, and the working ways will have an extra permutation of splice
> vs bump operation that can happen. Extending the possibilities to be 6
> ways of "proxying HTTPS".
>
>
> So the answer(s) to your first question are:
>
> yes, no.  yes, no.  no, yes.
>
>
>
> > 2. If yes, then what other configurations have to performed other than
> > "https_port XXXX"?
> For the cases where the #1 answer was "yes" and not "no".
>
> a) An explicit/forward or intercept proxy not using ssl-bump and
> receiving CONNECT requests does not need any special configuration to
> "proxy HTTPS". The proxy will simply enact the requested opaque tunnel
> in accordance to HTTP rules.
>
> So this means other than specifying "https_port XXXX" no other config is needed.
> When I setup Squid with just "https_port xxxx" and configured Firefox to use my proxy server for HTTP and HTTPS, it worked fine for HTTP but for HTTPS it gave "Proxy server rejected connection".
>
> So either something is wrong in my squid.conf or my assumption is incorrect that my scenario falls in #1 or #3.
>
>
> b) A reverse proxy requires the 'accel' mode flag, and the cert= option
> must load the cert for the domain you are hosting on that port, and the
> key= option must load the private key for that certificate.
>
> c) all other modes will not work without SSL-Bump feature.
>
>
>
> > 3. In this configuration, can Squid filter HTTPS requests from ACL?
> >
> That depends on the meaning of "this". There are 3 different
> configurations in the answer to #2.
>
> For (2a) - no. Only the CONNECT request can be filterd.
>
> From below links it looks like destination IP Address or hostname of a CONNECT request is same as HTTPS request. Is that correct?
>
> https://en.m.wikipedia.org/wiki/HTTP_tunnel#HTTP_CONNECT_tunneling
>
> http://stackoverflow.com/a/11698002/548403
>
>
> For (2b) - yes. BUT, notice that it requires private key data for certs.
> This configuration is only usable when _you own the domain_ which the
> client is visiting.
>
> For (2c) - SSL-Bump feature is the mechanism which enables https://
> filtering for all traffic modes other than that described by (2b).
> Without using that feature - no.
>
>
> Do you understand now why every path you have tried ends up with how-tos
> for configuring SSL-Bump?
>
> Yes, thanks for the elaborate explanation.
>
>
> HTH
> Amos
>
>
>
>
>
>
>
>
>
>
>

Thanks for your reply Eliezer. As I understand, if I want to filter
HTTPS websites based on only hostname/IP-Address, I will still have to
configure SSL-bump. However, I may not have to use the complete
feature in order to do so. Moreover, I can choose which website to
apply SSL-bump to.
Am I correct in my assumptions?


> This is the place to clarify that SNI based filtering is not 100% bullet proof and it could be exploited to override in a way your basic SNI based SSL level filtering.

The SNI solution may work with web-browsers but my solution is also
targeting clients connecting via to proxy via VPN. I think SNI won't
work in that case. Is that right?

>
> Do you have specific sites that you want to filter in the URL level or just globally?

I have a list of URL regexes. I have to filter HTTPS websites whose
URLs match the regex pattern.

> The answer to the above question will guide us towards what might be the right path for your solution(which could be full SSL-BUMP or partial).



-- 
Thanks,
Varun


From belle at bazuin.nl  Wed Feb 15 09:53:33 2017
From: belle at bazuin.nl (=?windows-1252?Q?L.P.H._van_Belle?=)
Date: Wed, 15 Feb 2017 10:53:33 +0100
Subject: [squid-users] question about : NOTICE: Authentication not
	applicable onintercepted requests.
Message-ID: <vmime.58a4251d.54d0.118595cb592d426d@ms249-lin-003.rotterdam.bazuin.nl>

Hai, 

?

In configuring my debian jessie with squid 3.5.24 ( with ssl enabled ) ?c-icap squidclamav and winbind 4.5.5 for kerberos keytab refresing. 

?

Now, im at the point of reducing my logs and i nocited : 

NOTICE: Authentication not applicable on intercepted requests. 

Messages in squid/cache.log 

?

I know this is some misconfiguration somewhere but im having a hardtime to finding/understanding it. 

Where and why, so is anyone can help me finding and understanding it, that would be very nice. 

?

I cant see my error and everything else is working fine, execept i havent tested the kerberos group acl yet. 

So i didnt set that http_access yet. 

?

Im having the following firewall rules 

?

# Not authenticated web traffice, redirected to squid in intercept mode.

-A PREROUTING -p tcp -i eth0 --dport 80 -j DNAT --to-destination 192.168.0.2:3128

-A PREROUTING -p tcp -i eth0 --dport 443 -j DNAT --to-destination 192.168.0.2:3129

Port 8080 is also open. 

?

Web traffic for pc?s which are domain joint have set the proxy by GPO to hostname.domain.tld port 8080 

Web traffic for other devices dont need to authenticate. 

WPAD and DNS wpad is also set. 

?

Below is mostly from the updated wiki pages. 

A big thank you to Amos Victor and others who changed the pages, looks good.

I have some small changed for a pure debian based setup with samba4 as addc and winbind for the squid member server. 

?

?

This is my squid config. 

# Created from a running squid version : 3.5.24

# Running os : Debian GNU/Linux 8 (jessie)

# Creation date: 2017-02-15

?

auth_param negotiate program /usr/lib/squid/negotiate_wrapper_auth --kerberos /usr/lib/squid/negotiate_kerberos_auth -s HTTP/proxy2.internal.domain.tld at INTERNAL.DOMAIN.TLD --ntlm /usr/bin/ntlm_auth --helper-protocol=gss-spnego --domain=NTDOM

auth_param negotiate children 10 startup=5 idle=5

auth_param negotiate keep_alive on

external_acl_type memberof ttl=3600 negative_ttl=3600 %LOGIN /usr/lib/squid3/ext_kerberos_ldap_group_acl -d -i -m 4 -g internet-allowed at INTERNAL.DOMAIN.TLD -N NTDOM at INTERNAL.DOMAIN.TLD -S dc1.internal.domain.tld at INTERNAL.DOMAIN.TLD -D INTERNAL.DOMAIN.TLD

acl authenticated proxy_auth REQUIRED

?

acl certificates rep_mime_type -i ^application/pkix-crl$

?

acl windows-updates dstdomain "/etc/squid/lists/updates-windows"

acl antivirus-updates dstdomain "/etc/squid/lists/updates-antivirus"

acl localnet src fc00::/7?????? # RFC 4193 local private network range

acl localnet src fe80::/10????? # RFC 4291 link-local (directly plugged) machines

acl localnet src 192.168.249.0/24??? # Company-1

acl localnet src 10.249.2.0/24?????? # Company-2

acl localnet src 10.249.3.0/24?????? # Company-3

acl localnet src 10.249.4.0/24?????? # Company-4

acl localnet src 10.249.5.0/24???? ??# Company-5

?

acl SSL_ports port 443????????? # https

acl SSL_ports port 3952???????? # CIC client

acl SSL_ports port 10443??????? # https Cisco 5506x

acl Safe_ports port 80????????? # http

acl Safe_ports port 21????????? # ftp

acl Safe_ports port 443???????? # https

acl Safe_ports port 70????????? # gopher

acl Safe_ports port 210???????? # wais

acl Safe_ports port 1025-65535? # unregistered ports

acl Safe_ports port 280???????? # http-mgmt

acl Safe_ports port 488???????? # gss-http

acl Safe_ports port 591???????? # filemaker

acl Safe_ports port 777???????? # multiling http

acl Safe_ports port 3952??????? # CIC client

acl Safe_ports port 10443?????? # https Cisco 5506x

acl CONNECT method CONNECT

?

## Added : Advertising Server Block List merge from YoYo.org and Host-file.net

acl block-asbl dstdomain "/etc/squid/lists/block-asbl-merged-dstdomain"

http_access deny block-asbl

?

acl google_recaptcha urlpath_regex ^\/recaptcha\/api.js

http_access allow google_recaptcha

?

acl NO-CACHE-SITES url_regex "/etc/squid/lists/no-cache-sites"

no_cache deny NO-CACHE-SITES

always_direct allow NO-CACHE-SITES

cache deny NO-CACHE-SITES

?

# 

http_access deny !Safe_ports

http_access deny CONNECT !SSL_ports

http_access allow localhost manager

http_access deny manager

http_access deny to_localhost

?

## allow before auth so all pc's get the needed updates

http_access allow windows-updates

http_access allow antivirus-updates

?

http_access allow authenticated

http_access allow localnet

http_access allow localhost

http_access deny all

?

http_port 192.168.249.222:3128 intercept connection-auth=off

https_port 192.168.249.222:3129 intercept connection-auth=off ssl-bump generate-host-certificates=on dynamic_cert_mem_cache_size=4MB cert=/etc/ssl/local/CAcert.pem options=NO_SSLv3 key=/etc/ssl/local/CAkey.pem

?

http_port 192.168.249.222:8080 ssl-bump generate-host-certificates=on dynamic_cert_mem_cache_size=4MB cert=/etc/ssl/local/CAcert.pem options=NO_SSLv3 key=/etc/ssl/local/CAkey.pem

sslcrtd_program /usr/lib/squid/ssl_crtd -s /var/lib/ssl_db -M 8MB

acl step1 at_step SslBump1

ssl_bump peek step1

ssl_bump bump all

sslproxy_options NO_SSLv2,NO_SSLv3,SINGLE_DH_USE

sslproxy_cipher EECDH+ECDSA+AESGCM:EECDH+aRSA+AESGCM:EECDH+ECDSA+SHA384:EECDH+ECDSA+SHA256:EECDH+aRSA+SHA384:EECDH+aRSA+SHA256:EECDH+aRSA+RC4:EECDH:EDH+aRSA:!RC4:!aNULL:!eNULL:!LOW:!3DES:!MD5:!EXP:!PSK:!SRP:!DSS

cache_mem 4096 MB

coredump_dir /var/spool/squid

ftp_user anonymousftp at domain.tld

?

# 

refresh_pattern -i windowsupdate.com/.*\.(cab|exe|ms[i|u|f|p]|[ap]sf|wm[v|a]|dat|zip|psf) 43200 80% 129600 reload-into-ims

refresh_pattern -i microsoft.com/.*\.(cab|exe|ms[i|u|f|p]|[ap]sf|wm[v|a]|dat|zip|psf) 43200 80% 129600 reload-into-ims

refresh_pattern -i windows.com/.*\.(cab|exe|ms[i|u|f|p]|[ap]sf|wm[v|a]|dat|zip|psf) 43200 80% 129600 reload-into-ims

refresh_pattern -i microsoft.com.akadns.net/.*\.(cab|exe|ms[i|u|f|p]|[ap]sf|wm[v|a]|dat|zip|psf) 43200 80% 129600 reload-into-ims

refresh_pattern -i deploy.akamaitechnologies.com/.*\.(cab|exe|ms[i|u|f|p]|[ap]sf|wm[v|a]|dat|zip|psf) 43200 80% 129600 reload-into-ims

?

## todo, make this list more complete, see icap excludes

refresh_pattern -i \.symantecliveupdate\.com\/.*\.(zip|7z|irn|[m|x][0-9][0-9])????????? 4320??? 100%??? 43200?? reload-into-ims

refresh_pattern -i .*dnl.*\.geo\.kaspersky\.(com|ru)\/.*\.(zip|avc|kdc|nhg|klz|d[at|if])??????? 4320??? 100%??? 43200?? reload-into-ims

refresh_pattern -i \.kaspersky-labs\.(com|ru)\/.*\.(cab|zip|exe|ms[i|p])??????? 4320??? 100%??? 43200?? reload-into-ims

refresh_pattern -i \.kaspersky\.(com|ru)\/.*\.(cab|zip|exe|ms[i|p]|avc) 4320??? 100%??? 43200?? reload-into-ims

refresh_pattern -i .update\.geo\.drweb\.com???? 4320??? 100%??? 43200?? reload-into-ims

refresh_pattern -i \.avast.com\/.*\.(vp[u|aa])? ????????4320??? 100%??? 43200?? reload-into-ims

refresh_pattern -i \.avg.com\/.*\.(bin)???????? 4320??? 100%??? 43200?? reload-into-ims

?

## todo, add .deb files caching

refresh_pattern ^(ht|f)tp://.*debian.*/Packages\.(bz2|gz|diff/Index)$?? 0?????? 0%????? 0

refresh_pattern ^(ht|f)tp://.*debian.*/Release(\.gpg)?$???????????????? 0?????? 0%????? 0

refresh_pattern ^(ht|f)tp://.*debian.*/Sources\.(bz2|gz|diff/Index)$??? 0?????? 0%????? 0

refresh_pattern ^(ht|f)tp://.*debian.*/Translation-en_GB\.bz2)$???????? 0?????? 0%????? 0

?

## The defaults as last.

refresh_pattern -i \.(zip|[g|b]z2?|exe|ms[i|p]|cvd|cdiff|mar)$? 43200?? 100%??? 129600? reload-into-ims

refresh_pattern ^ftp:?????????? 1440??? 20%???? 10080

refresh_pattern ^gopher:??????? 1440??? 0%????? 1440

refresh_pattern -i (/cgi-bin/|\?) 0???? 0%????? 0

refresh_pattern .?????????????? 0?????? 20%???? 4320

cache_mgr changed2protectme at somedomain.tld

mail_from proxy2 at internal.domain.tld

visible_hostname proxy2.internal.domain.tld

hostname_aliases proxy2.internal.domain.tld

?

httpd_suppress_version_string on

?

icap_enable on

icap_send_client_ip on

icap_send_client_username on

icap_client_username_header X-Authenticated-User

icap_persistent_connections on

icap_preview_enable on

icap_preview_size 1024

icap_service service_req reqmod_precache icap://127.0.0.1:1344/squidclamav bypass=off

adaptation_access service_req allow all

icap_service service_resp respmod_precache icap://127.0.0.1:1344/squidclamav bypass=off

adaptation_access service_resp allow all

?

dns_v4_first on

maximum_object_size 4096 KB

minimum_object_size 0 KB

maximum_object_size_in_memory 64 KB

cache_mem 256 MB

quick_abort_min -1 KB

fqdncache_size 4096

cache_swap_low 90

cache_swap_high 95

?

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20170215/94b6bf80/attachment.htm>

From belle at bazuin.nl  Wed Feb 15 14:38:46 2017
From: belle at bazuin.nl (=?windows-1252?Q?L.P.H._van_Belle?=)
Date: Wed, 15 Feb 2017 15:38:46 +0100
Subject: [squid-users] question about : NOTICE: Authentication not
	applicable onintercepted requests. ( SOLVED )
In-Reply-To: <>
References: <>
Message-ID: <vmime.58a467f6.2be3.78cc93ba3ef69856@ms249-lin-003.rotterdam.bazuin.nl>

If this one arived in the list. 

?

This is solved, the wpad.dat was guiding my to the other proxy while my gateway was set to me new proxy. 

This happend at the policy refresh and did not notice it. 

Sorry for the noice. 

?

But if you see anything that incorrect, or can have a better setup, please let me know. 

I always like improvements. 

?

Thanks

?

Louis

?

?


Van: L.P.H. van Belle [mailto:belle at bazuin.nl] 
Verzonden: woensdag 15 februari 2017 10:54
Aan: 'squid-users at squid-cache.org'
Onderwerp: question about : NOTICE: Authentication not applicable on intercepted requests. 


?

Hai, 

?

In configuring my debian jessie with squid 3.5.24 ( with ssl enabled ) ?c-icap squidclamav and winbind 4.5.5 for kerberos keytab refresing. 

?

Now, im at the point of reducing my logs and i nocited : 

NOTICE: Authentication not applicable on intercepted requests. 

Messages in squid/cache.log 

?

I know this is some misconfiguration somewhere but im having a hardtime to finding/understanding it. 

Where and why, so is anyone can help me finding and understanding it, that would be very nice. 

?

I cant see my error and everything else is working fine, execept i havent tested the kerberos group acl yet. 

So i didnt set that http_access yet. 

?

Im having the following firewall rules 

?

# Not authenticated web traffice, redirected to squid in intercept mode.

-A PREROUTING -p tcp -i eth0 --dport 80 -j DNAT --to-destination 192.168.0.2:3128

-A PREROUTING -p tcp -i eth0 --dport 443 -j DNAT --to-destination 192.168.0.2:3129

Port 8080 is also open. 

?

Web traffic for pc?s which are domain joint have set the proxy by GPO to hostname.domain.tld port 8080 

Web traffic for other devices dont need to authenticate. 

WPAD and DNS wpad is also set. 

?

Below is mostly from the updated wiki pages. 

A big thank you to Amos Victor and others who changed the pages, looks good.

I have some small changed for a pure debian based setup with samba4 as addc and winbind for the squid member server. 

?

?

This is my squid config. 

# Created from a running squid version : 3.5.24

# Running os : Debian GNU/Linux 8 (jessie)

# Creation date: 2017-02-15

?

auth_param negotiate program /usr/lib/squid/negotiate_wrapper_auth --kerberos /usr/lib/squid/negotiate_kerberos_auth -s HTTP/proxy2.internal.domain.tld at INTERNAL.DOMAIN.TLD --ntlm /usr/bin/ntlm_auth --helper-protocol=gss-spnego --domain=NTDOM

auth_param negotiate children 10 startup=5 idle=5

auth_param negotiate keep_alive on

external_acl_type memberof ttl=3600 negative_ttl=3600 %LOGIN /usr/lib/squid3/ext_kerberos_ldap_group_acl -d -i -m 4 -g internet-allowed at INTERNAL.DOMAIN.TLD -N NTDOM at INTERNAL.DOMAIN.TLD -S dc1.internal.domain.tld at INTERNAL.DOMAIN.TLD -D INTERNAL.DOMAIN.TLD

acl authenticated proxy_auth REQUIRED

?

acl certificates rep_mime_type -i ^application/pkix-crl$

?

acl windows-updates dstdomain "/etc/squid/lists/updates-windows"

acl antivirus-updates dstdomain "/etc/squid/lists/updates-antivirus"

acl localnet src fc00::/7?????? # RFC 4193 local private network range

acl localnet src fe80::/10????? # RFC 4291 link-local (directly plugged) machines

acl localnet src 192.168.249.0/24??? # Company-1

acl localnet src 10.249.2.0/24?????? # Company-2

acl localnet src 10.249.3.0/24?????? # Company-3

acl localnet src 10.249.4.0/24?????? # Company-4

acl localnet src 10.249.5.0/24???? ??# Company-5

?

acl SSL_ports port 443????????? # https

acl SSL_ports port 3952???????? # CIC client

acl SSL_ports port 10443??????? # https Cisco 5506x

acl Safe_ports port 80????????? # http

acl Safe_ports port 21????????? # ftp

acl Safe_ports port 443???????? # https

acl Safe_ports port 70????????? # gopher

acl Safe_ports port 210???????? # wais

acl Safe_ports port 1025-65535? # unregistered ports

acl Safe_ports port 280???????? # http-mgmt

acl Safe_ports port 488???????? # gss-http

acl Safe_ports port 591???????? # filemaker

acl Safe_ports port 777???????? # multiling http

acl Safe_ports port 3952??????? # CIC client

acl Safe_ports port 10443?????? # https Cisco 5506x

acl CONNECT method CONNECT

?

## Added : Advertising Server Block List merge from YoYo.org and Host-file.net

acl block-asbl dstdomain "/etc/squid/lists/block-asbl-merged-dstdomain"

http_access deny block-asbl

?

acl google_recaptcha urlpath_regex ^\/recaptcha\/api.js

http_access allow google_recaptcha

?

acl NO-CACHE-SITES url_regex "/etc/squid/lists/no-cache-sites"

no_cache deny NO-CACHE-SITES

always_direct allow NO-CACHE-SITES

cache deny NO-CACHE-SITES

?

# 

http_access deny !Safe_ports

http_access deny CONNECT !SSL_ports

http_access allow localhost manager

http_access deny manager

http_access deny to_localhost

?

## allow before auth so all pc's get the needed updates

http_access allow windows-updates

http_access allow antivirus-updates

?

http_access allow authenticated

http_access allow localnet

http_access allow localhost

http_access deny all

?

http_port 192.168.249.222:3128 intercept connection-auth=off

https_port 192.168.249.222:3129 intercept connection-auth=off ssl-bump generate-host-certificates=on dynamic_cert_mem_cache_size=4MB cert=/etc/ssl/local/CAcert.pem options=NO_SSLv3 key=/etc/ssl/local/CAkey.pem

?

http_port 192.168.249.222:8080 ssl-bump generate-host-certificates=on dynamic_cert_mem_cache_size=4MB cert=/etc/ssl/local/CAcert.pem options=NO_SSLv3 key=/etc/ssl/local/CAkey.pem

sslcrtd_program /usr/lib/squid/ssl_crtd -s /var/lib/ssl_db -M 8MB

acl step1 at_step SslBump1

ssl_bump peek step1

ssl_bump bump all

sslproxy_options NO_SSLv2,NO_SSLv3,SINGLE_DH_USE

sslproxy_cipher EECDH+ECDSA+AESGCM:EECDH+aRSA+AESGCM:EECDH+ECDSA+SHA384:EECDH+ECDSA+SHA256:EECDH+aRSA+SHA384:EECDH+aRSA+SHA256:EECDH+aRSA+RC4:EECDH:EDH+aRSA:!RC4:!aNULL:!eNULL:!LOW:!3DES:!MD5:!EXP:!PSK:!SRP:!DSS

cache_mem 4096 MB

coredump_dir /var/spool/squid

ftp_user anonymousftp at domain.tld

?

# 

refresh_pattern -i windowsupdate.com/.*\.(cab|exe|ms[i|u|f|p]|[ap]sf|wm[v|a]|dat|zip|psf) 43200 80% 129600 reload-into-ims

refresh_pattern -i microsoft.com/.*\.(cab|exe|ms[i|u|f|p]|[ap]sf|wm[v|a]|dat|zip|psf) 43200 80% 129600 reload-into-ims

refresh_pattern -i windows.com/.*\.(cab|exe|ms[i|u|f|p]|[ap]sf|wm[v|a]|dat|zip|psf) 43200 80% 129600 reload-into-ims

refresh_pattern -i microsoft.com.akadns.net/.*\.(cab|exe|ms[i|u|f|p]|[ap]sf|wm[v|a]|dat|zip|psf) 43200 80% 129600 reload-into-ims

refresh_pattern -i deploy.akamaitechnologies.com/.*\.(cab|exe|ms[i|u|f|p]|[ap]sf|wm[v|a]|dat|zip|psf) 43200 80% 129600 reload-into-ims

?

## todo, make this list more complete, see icap excludes

refresh_pattern -i \.symantecliveupdate\.com\/.*\.(zip|7z|irn|[m|x][0-9][0-9])????????? 4320??? 100%??? 43200?? reload-into-ims

refresh_pattern -i .*dnl.*\.geo\.kaspersky\.(com|ru)\/.*\.(zip|avc|kdc|nhg|klz|d[at|if])??????? 4320??? 100%??? 43200?? reload-into-ims

refresh_pattern -i \.kaspersky-labs\.(com|ru)\/.*\.(cab|zip|exe|ms[i|p])??????? 4320??? 100%??? 43200?? reload-into-ims

refresh_pattern -i \.kaspersky\.(com|ru)\/.*\.(cab|zip|exe|ms[i|p]|avc) 4320??? 100%??? 43200?? reload-into-ims

refresh_pattern -i .update\.geo\.drweb\.com???? 4320??? 100%??? 43200?? reload-into-ims

refresh_pattern -i \.avast.com\/.*\.(vp[u|aa])? ????????4320??? 100%??? 43200?? reload-into-ims

refresh_pattern -i \.avg.com\/.*\.(bin)???????? 4320??? 100%??? 43200?? reload-into-ims

?

## todo, add .deb files caching

refresh_pattern ^(ht|f)tp://.*debian.*/Packages\.(bz2|gz|diff/Index)$?? 0?????? 0%????? 0

refresh_pattern ^(ht|f)tp://.*debian.*/Release(\.gpg)?$???????????????? 0?????? 0%????? 0

refresh_pattern ^(ht|f)tp://.*debian.*/Sources\.(bz2|gz|diff/Index)$??? 0?????? 0%????? 0

refresh_pattern ^(ht|f)tp://.*debian.*/Translation-en_GB\.bz2)$???????? 0?????? 0%????? 0

?

## The defaults as last.

refresh_pattern -i \.(zip|[g|b]z2?|exe|ms[i|p]|cvd|cdiff|mar)$? 43200?? 100%??? 129600? reload-into-ims

refresh_pattern ^ftp:?????????? 1440??? 20%???? 10080

refresh_pattern ^gopher:??????? 1440??? 0%????? 1440

refresh_pattern -i (/cgi-bin/|\?) 0???? 0%????? 0

refresh_pattern .?????????????? 0?????? 20%???? 4320

cache_mgr changed2protectme at somedomain.tld

mail_from proxy2 at internal.domain.tld

visible_hostname proxy2.internal.domain.tld

hostname_aliases proxy2.internal.domain.tld

?

httpd_suppress_version_string on

?

icap_enable on

icap_send_client_ip on

icap_send_client_username on

icap_client_username_header X-Authenticated-User

icap_persistent_connections on

icap_preview_enable on

icap_preview_size 1024

icap_service service_req reqmod_precache icap://127.0.0.1:1344/squidclamav bypass=off

adaptation_access service_req allow all

icap_service service_resp respmod_precache icap://127.0.0.1:1344/squidclamav bypass=off

adaptation_access service_resp allow all

?

dns_v4_first on

maximum_object_size 4096 KB

minimum_object_size 0 KB

maximum_object_size_in_memory 64 KB

cache_mem 256 MB

quick_abort_min -1 KB

fqdncache_size 4096

cache_swap_low 90

cache_swap_high 95

?


-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20170215/edbf9703/attachment.htm>

From heiler.bemerguy at cinbesa.com.br  Wed Feb 15 21:40:57 2017
From: heiler.bemerguy at cinbesa.com.br (Heiler Bemerguy)
Date: Wed, 15 Feb 2017 18:40:57 -0300
Subject: [squid-users] WARNING: Ignoring cache entry due to a SIZE MISMATCH
Message-ID: <cc488323-b0b4-f6b8-dbab-e6cb3c46764a@cinbesa.com.br>


Is it normal, on every restart?

maximum_object_size 2 GB
cache_dir rock /cache  120000 min-size=0 max-size=12288 slot-size=12288 
max-swap-rate=250 swap-timeout=350
cache_dir rock /cache2 120000 min-size=10240 max-size=65536 
max-swap-rate=250 swap-timeout=360

2017/02/15 18:38:50 kid7| WARNING: Ignoring cache entry due to a SIZE 
MISMATCH 5852!=5853
2017/02/15 18:38:51 kid7| WARNING: Ignoring cache entry due to a SIZE 
MISMATCH 6223!=6222
2017/02/15 18:38:51 kid7| WARNING: Ignoring cache entry due to a SIZE 
MISMATCH 4198!=4202
2017/02/15 18:38:51 kid7| WARNING: Ignoring cache entry due to a SIZE 
MISMATCH 3822!=3825
2017/02/15 18:38:52 kid7| WARNING: Ignoring cache entry due to a SIZE 
MISMATCH 4397!=4398
2017/02/15 18:38:52 kid7| WARNING: Ignoring cache entry due to a SIZE 
MISMATCH 6929!=7077
2017/02/15 18:38:52 kid8| WARNING: Ignoring cache entry due to a SIZE 
MISMATCH 12721!=12722
2017/02/15 18:38:52 kid7| WARNING: Ignoring cache entry due to a SIZE 
MISMATCH 1483!=1467
2017/02/15 18:38:53 kid7| WARNING: Ignoring cache entry due to a SIZE 
MISMATCH 2200!=2199
2017/02/15 18:38:53 kid7| WARNING: Ignoring cache entry due to a SIZE 
MISMATCH 7911!=7920
2017/02/15 18:38:53 kid7| WARNING: Ignoring cache entry due to a SIZE 
MISMATCH 873!=872
2017/02/15 18:38:53 kid7| WARNING: Ignoring cache entry due to a SIZE 
MISMATCH 6736!=6734
2017/02/15 18:38:53 kid7| WARNING: Ignoring cache entry due to a SIZE 
MISMATCH 6214!=6198
2017/02/15 18:38:53 kid7| WARNING: Ignoring cache entry due to a SIZE 
MISMATCH 3233!=3274
2017/02/15 18:38:53 kid7| WARNING: Ignoring cache entry due to a SIZE 
MISMATCH 987!=986
2017/02/15 18:38:54 kid7| WARNING: Ignoring cache entry due to a SIZE 
MISMATCH 5152!=5150
2017/02/15 18:38:54 kid7| WARNING: Ignoring cache entry due to a SIZE 
MISMATCH 6414!=6406
2017/02/15 18:38:54 kid7| WARNING: Ignoring cache entry due to a SIZE 
MISMATCH 12086!=12078
2017/02/15 18:38:54 kid7| WARNING: Ignoring cache entry due to a SIZE 
MISMATCH 11727!=11726
2017/02/15 18:38:54 kid7| WARNING: Ignoring cache entry due to a SIZE 
MISMATCH 11460!=11458
2017/02/15 18:38:54 kid7| WARNING: Ignoring cache entry due to a SIZE 
MISMATCH 8766!=8765
2017/02/15 18:38:54 kid7| WARNING: Ignoring cache entry due to a SIZE 
MISMATCH 11455!=11447
2017/02/15 18:38:54 kid7| WARNING: Ignoring cache entry due to a SIZE 
MISMATCH 10801!=10800
2017/02/15 18:38:55 kid7| WARNING: Ignoring cache entry due to a SIZE 
MISMATCH 11453!=11454
2017/02/15 18:38:55 kid7| WARNING: Ignoring cache entry due to a SIZE 
MISMATCH 610!=645
2017/02/15 18:38:55 kid7| WARNING: Ignoring cache entry due to a SIZE 
MISMATCH 1606!=1643
2017/02/15 18:38:56 kid7| WARNING: Ignoring cache entry due to a SIZE 
MISMATCH 937!=940
2017/02/15 18:38:56 kid7| WARNING: Ignoring cache entry due to a SIZE 
MISMATCH 1599!=1601
2017/02/15 18:38:56 kid8| WARNING: Ignoring cache entry due to a SIZE 
MISMATCH 14388!=14430
2017/02/15 18:38:56 kid8| WARNING: Ignoring cache entry due to a SIZE 
MISMATCH 12261!=12262
2017/02/15 18:38:56 kid8| WARNING: Ignoring cache entry due to a SIZE 
MISMATCH 15088!=15089


-- 
Best Regards,

Heiler Bemerguy
Network Manager - CINBESA
55 91 98151-4894/3184-1751



From squid3 at treenet.co.nz  Thu Feb 16 23:12:48 2017
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Fri, 17 Feb 2017 12:12:48 +1300
Subject: [squid-users] question about : NOTICE: Authentication not
 applicable onintercepted requests. ( SOLVED )
In-Reply-To: <vmime.58a467f6.2be3.78cc93ba3ef69856@ms249-lin-003.rotterdam.bazuin.nl>
References: <>
 <vmime.58a467f6.2be3.78cc93ba3ef69856@ms249-lin-003.rotterdam.bazuin.nl>
Message-ID: <9accc789-bc94-52e0-cffb-dd6f8f1356a0@treenet.co.nz>

On 16/02/2017 3:38 a.m., L.P.H. van Belle wrote:
> If this one arived in the list. 
> 
>  
> 
> This is solved, the wpad.dat was guiding my to the other proxy while my gateway was set to me new proxy. 
> 
> This happend at the policy refresh and did not notice it. 
> 
> Sorry for the noice. 
> 
>  
> 
> But if you see anything that incorrect, or can have a better setup, please let me know. 
> 
> I always like improvements. 
> 

"no_cache" is an alias of "cache". So you can remove the "no_cache" line
from your config entirely.

>  
> 
> Thanks
> 
>  
> 
> Louis
> 
>  
> 
>  
> 
> 
> Van: L.P.H. van Belle [mailto:belle at bazuin.nl] 
> Verzonden: woensdag 15 februari 2017 10:54
> Aan: 'squid-users at squid-cache.org'
> Onderwerp: question about : NOTICE: Authentication not applicable on intercepted requests. 
> 
> 
>  
> 
> Hai, 
> 
>  
> 
> In configuring my debian jessie with squid 3.5.24 ( with ssl enabled )  c-icap squidclamav and winbind 4.5.5 for kerberos keytab refresing. 
> 
>  
> 
> Now, im at the point of reducing my logs and i nocited : 
> 
> NOTICE: Authentication not applicable on intercepted requests. 
> 
> Messages in squid/cache.log 
> 
>  
> 
> I know this is some misconfiguration somewhere but im having a hardtime to finding/understanding it. 
> 
> Where and why, so is anyone can help me finding and understanding it, that would be very nice. 
> 
>  
> 
> I cant see my error and everything else is working fine, execept i havent tested the kerberos group acl yet. 
> 
> So i didnt set that http_access yet. 
> 
>  
> 
> Im having the following firewall rules 
> 
>  
> 
> # Not authenticated web traffice, redirected to squid in intercept mode.
> 
> -A PREROUTING -p tcp -i eth0 --dport 80 -j DNAT --to-destination 192.168.0.2:3128
> 
> -A PREROUTING -p tcp -i eth0 --dport 443 -j DNAT --to-destination 192.168.0.2:3129
> 
> Port 8080 is also open. 
> 
>  
> 
> Web traffic for pc?s which are domain joint have set the proxy by GPO to hostname.domain.tld port 8080 
> 
> Web traffic for other devices dont need to authenticate. 
> 
> WPAD and DNS wpad is also set. 
> 
>  
> 
> Below is mostly from the updated wiki pages. 
> 
> A big thank you to Amos Victor and others who changed the pages, looks good.
> 
> I have some small changed for a pure debian based setup with samba4 as addc and winbind for the squid member server. 
> 
>  
> 
>  
> 
> This is my squid config. 
> 
> # Created from a running squid version : 3.5.24
> 
> # Running os : Debian GNU/Linux 8 (jessie)
> 
> # Creation date: 2017-02-15
> 
>  
> 
> auth_param negotiate program /usr/lib/squid/negotiate_wrapper_auth --kerberos /usr/lib/squid/negotiate_kerberos_auth -s HTTP/proxy2.internal.domain.tld at INTERNAL.DOMAIN.TLD --ntlm /usr/bin/ntlm_auth --helper-protocol=gss-spnego --domain=NTDOM
> 
> auth_param negotiate children 10 startup=5 idle=5
> 
> auth_param negotiate keep_alive on
> 
> external_acl_type memberof ttl=3600 negative_ttl=3600 %LOGIN /usr/lib/squid3/ext_kerberos_ldap_group_acl -d -i -m 4 -g internet-allowed at INTERNAL.DOMAIN.TLD -N NTDOM at INTERNAL.DOMAIN.TLD -S dc1.internal.domain.tld at INTERNAL.DOMAIN.TLD -D INTERNAL.DOMAIN.TLD
> 
> acl authenticated proxy_auth REQUIRED
> 
>  
> 
> acl certificates rep_mime_type -i ^application/pkix-crl$
> 
>  
> 
> acl windows-updates dstdomain "/etc/squid/lists/updates-windows"
> 
> acl antivirus-updates dstdomain "/etc/squid/lists/updates-antivirus"
> 
> acl localnet src fc00::/7       # RFC 4193 local private network range
> 
> acl localnet src fe80::/10      # RFC 4291 link-local (directly plugged) machines
> 
> acl localnet src 192.168.249.0/24    # Company-1
> 
> acl localnet src 10.249.2.0/24       # Company-2
> 
> acl localnet src 10.249.3.0/24       # Company-3
> 
> acl localnet src 10.249.4.0/24       # Company-4
> 
> acl localnet src 10.249.5.0/24       # Company-5
> 

Small optimization here. You can configure the 10/8 lines as:

  acl localnet 10.29.2.0-10.249.5.0/24

That reduces 3 IP comparisions per request.


>  
> 
> acl SSL_ports port 443          # https
> 
> acl SSL_ports port 3952         # CIC client
> 
> acl SSL_ports port 10443        # https Cisco 5506x
> 
> acl Safe_ports port 80          # http
> 
> acl Safe_ports port 21          # ftp
> 
> acl Safe_ports port 443         # https
> 
> acl Safe_ports port 70          # gopher
> 
> acl Safe_ports port 210         # wais
> 
> acl Safe_ports port 1025-65535  # unregistered ports
> 
> acl Safe_ports port 280         # http-mgmt
> 
> acl Safe_ports port 488         # gss-http
> 
> acl Safe_ports port 591         # filemaker
> 
> acl Safe_ports port 777         # multiling http
> 
> acl Safe_ports port 3952        # CIC client
> 
> acl Safe_ports port 10443       # https Cisco 5506x

Port numbers over 1024 are already included in the "unregistered ports"
entry. You can simplify by removing these last two lines of Safe_ports.

> 
> acl CONNECT method CONNECT
> 
>  
> 
> ## Added : Advertising Server Block List merge from YoYo.org and Host-file.net
> 
> acl block-asbl dstdomain "/etc/squid/lists/block-asbl-merged-dstdomain"
> 
> http_access deny block-asbl
> 
>  
> 
> acl google_recaptcha urlpath_regex ^\/recaptcha\/api.js
> 
> http_access allow google_recaptcha
> 
>  
> 
> acl NO-CACHE-SITES url_regex "/etc/squid/lists/no-cache-sites"
> 
> no_cache deny NO-CACHE-SITES
> 
> always_direct allow NO-CACHE-SITES
> 
> cache deny NO-CACHE-SITES
> 

always_direct is only relevant when you are using a cache_peer. Which
you are not. So that can be removed.

"no_cache" is an old alias for "cache". So you can remove the "no_cache"
line entirely as well.

>  
> 
> # 
> 
> http_access deny !Safe_ports
> 
> http_access deny CONNECT !SSL_ports
> 
> http_access allow localhost manager
> 
> http_access deny manager
> 
> http_access deny to_localhost
> 
>  
> 
> ## allow before auth so all pc's get the needed updates
> 
> http_access allow windows-updates
> 
> http_access allow antivirus-updates
> 
>  
> 
> http_access allow authenticated
> 
> http_access allow localnet
> 
> http_access allow localhost
> 
> http_access deny all
> 
>  
> 
> http_port 192.168.249.222:3128 intercept connection-auth=off
> 
> https_port 192.168.249.222:3129 intercept connection-auth=off ssl-bump generate-host-certificates=on dynamic_cert_mem_cache_size=4MB cert=/etc/ssl/local/CAcert.pem options=NO_SSLv3 key=/etc/ssl/local/CAkey.pem
> 
>  
> 
> http_port 192.168.249.222:8080 ssl-bump generate-host-certificates=on dynamic_cert_mem_cache_size=4MB cert=/etc/ssl/local/CAcert.pem options=NO_SSLv3 key=/etc/ssl/local/CAkey.pem
> 
> sslcrtd_program /usr/lib/squid/ssl_crtd -s /var/lib/ssl_db -M 8MB
> 
> acl step1 at_step SslBump1
> 
> ssl_bump peek step1
> 
> ssl_bump bump all
> 
> sslproxy_options NO_SSLv2,NO_SSLv3,SINGLE_DH_USE
> 
> sslproxy_cipher EECDH+ECDSA+AESGCM:EECDH+aRSA+AESGCM:EECDH+ECDSA+SHA384:EECDH+ECDSA+SHA256:EECDH+aRSA+SHA384:EECDH+aRSA+SHA256:EECDH+aRSA+RC4:EECDH:EDH+aRSA:!RC4:!aNULL:!eNULL:!LOW:!3DES:!MD5:!EXP:!PSK:!SRP:!DSS
> 
> cache_mem 4096 MB
> 
> coredump_dir /var/spool/squid
> 
> ftp_user anonymousftp at domain.tld
> 
>  
> 
> # 
> 
> refresh_pattern -i windowsupdate.com/.*\.(cab|exe|ms[i|u|f|p]|[ap]sf|wm[v|a]|dat|zip|psf) 43200 80% 129600 reload-into-ims
> 
> refresh_pattern -i microsoft.com/.*\.(cab|exe|ms[i|u|f|p]|[ap]sf|wm[v|a]|dat|zip|psf) 43200 80% 129600 reload-into-ims
> 
> refresh_pattern -i windows.com/.*\.(cab|exe|ms[i|u|f|p]|[ap]sf|wm[v|a]|dat|zip|psf) 43200 80% 129600 reload-into-ims
> 
> refresh_pattern -i microsoft.com.akadns.net/.*\.(cab|exe|ms[i|u|f|p]|[ap]sf|wm[v|a]|dat|zip|psf) 43200 80% 129600 reload-into-ims
> 
> refresh_pattern -i deploy.akamaitechnologies.com/.*\.(cab|exe|ms[i|u|f|p]|[ap]sf|wm[v|a]|dat|zip|psf) 43200 80% 129600 reload-into-ims
> 

Squid can run through testing each refresh_pattern line against objects
at several different times where processing is performance-critical.

So you can gain some speed by;
 a) manually merging the regex patterns where all the other parameters
are identical, and
 b) sorting the refresh_pattern lines by most frequently used.


>  
> 
> ## todo, make this list more complete, see icap excludes
> 
> refresh_pattern -i \.symantecliveupdate\.com\/.*\.(zip|7z|irn|[m|x][0-9][0-9])          4320    100%    43200   reload-into-ims
> 
> refresh_pattern -i .*dnl.*\.geo\.kaspersky\.(com|ru)\/.*\.(zip|avc|kdc|nhg|klz|d[at|if])        4320    100%    43200   reload-into-ims
> 
> refresh_pattern -i \.kaspersky-labs\.(com|ru)\/.*\.(cab|zip|exe|ms[i|p])        4320    100%    43200   reload-into-ims
> 
> refresh_pattern -i \.kaspersky\.(com|ru)\/.*\.(cab|zip|exe|ms[i|p]|avc) 4320    100%    43200   reload-into-ims
> 
> refresh_pattern -i .update\.geo\.drweb\.com     4320    100%    43200   reload-into-ims
> 
> refresh_pattern -i \.avast.com\/.*\.(vp[u|aa])          4320    100%    43200   reload-into-ims
> 
> refresh_pattern -i \.avg.com\/.*\.(bin)         4320    100%    43200   reload-into-ims
> 
>  
> 
> ## todo, add .deb files caching
> 
> refresh_pattern ^(ht|f)tp://.*debian.*/Packages\.(bz2|gz|diff/Index)$   0       0%      0
> 
> refresh_pattern ^(ht|f)tp://.*debian.*/Release(\.gpg)?$                 0       0%      0
> 
> refresh_pattern ^(ht|f)tp://.*debian.*/Sources\.(bz2|gz|diff/Index)$    0       0%      0
> 
> refresh_pattern ^(ht|f)tp://.*debian.*/Translation-en_GB\.bz2)$         0       0%      0
> 

Er. The min/max of 0 sets them to already expired _unless_
Cache-Controls exist and say otherwise. So if these lines do anything at
all it is prevent caching of those objects.

Squid-3.5 should be handling the .deb and related things properly
nowdays, so you can probably remove those lines.

>  
> 
> ## The defaults as last.
> 
> refresh_pattern -i \.(zip|[g|b]z2?|exe|ms[i|p]|cvd|cdiff|mar)$  43200   100%    129600  reload-into-ims
> 
> refresh_pattern ^ftp:           1440    20%     10080
> 
> refresh_pattern ^gopher:        1440    0%      1440
> 
> refresh_pattern -i (/cgi-bin/|\?) 0     0%      0
> 
> refresh_pattern .               0       20%     4320
> 
> cache_mgr changed2protectme at somedomain.tld
> 
> mail_from proxy2 at internal.domain.tld
> 
> visible_hostname proxy2.internal.domain.tld
> 
> hostname_aliases proxy2.internal.domain.tld
> 
>  
> 
> httpd_suppress_version_string on
> 
>  
> 
> icap_enable on
> 
> icap_send_client_ip on
> 
> icap_send_client_username on
> 
> icap_client_username_header X-Authenticated-User
> 
> icap_persistent_connections on
> 
> icap_preview_enable on
> 
> icap_preview_size 1024
> 
> icap_service service_req reqmod_precache icap://127.0.0.1:1344/squidclamav bypass=off
> 
> adaptation_access service_req allow all
> 
> icap_service service_resp respmod_precache icap://127.0.0.1:1344/squidclamav bypass=off
> 
> adaptation_access service_resp allow all
> 
>  
> 
> dns_v4_first on
> 
> maximum_object_size 4096 KB
> 
> minimum_object_size 0 KB
> 
> maximum_object_size_in_memory 64 KB
> 
> cache_mem 256 MB
> 
> quick_abort_min -1 KB
> 
> fqdncache_size 4096
> 
> cache_swap_low 90
> 
> cache_swap_high 95

Things which are set to their default values can be removed from squid.conf.


Amos



From splicelid at gmail.com  Fri Feb 17 11:43:17 2017
From: splicelid at gmail.com (splicelid at gmail.com)
Date: Fri, 17 Feb 2017 13:43:17 +0200
Subject: [squid-users] squid-avira-update-cache
Message-ID: <6cd017ac4ce7d5479e2cdaa7362777ad@xp1>

An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20170217/65dd7797/attachment.htm>

From squid3 at treenet.co.nz  Fri Feb 17 12:44:24 2017
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sat, 18 Feb 2017 01:44:24 +1300
Subject: [squid-users] WARNING: Ignoring cache entry due to a SIZE
 MISMATCH
In-Reply-To: <cc488323-b0b4-f6b8-dbab-e6cb3c46764a@cinbesa.com.br>
References: <cc488323-b0b4-f6b8-dbab-e6cb3c46764a@cinbesa.com.br>
Message-ID: <20ab0957-db57-b6c0-a255-8cd1c709e2e1@treenet.co.nz>

On 16/02/2017 10:40 a.m., Heiler Bemerguy wrote:
> 
> Is it normal, on every restart?
> 


Well, this is a check that is only performed on restart. So in a way it
is "normal" that it occurs on restart. It should not happen at all
though. AFAIK, it is a sign of cache corruption...


> maximum_object_size 2 GB
> cache_dir rock /cache  120000 min-size=0 max-size=12288 slot-size=12288
> max-swap-rate=250 swap-timeout=350
> cache_dir rock /cache2 120000 min-size=10240 max-size=65536
> max-swap-rate=250 swap-timeout=360
> 

These caches cannot store objects larger than 64KB. So where do you
expect the 64KB thru 2GB objects to go?


> 2017/02/15 18:38:50 kid7| WARNING: Ignoring cache entry due to a SIZE
> MISMATCH 5852!=5853
> 2017/02/15 18:38:51 kid7| WARNING: Ignoring cache entry due to a SIZE
> MISMATCH 6223!=6222
> 2017/02/15 18:38:51 kid7| WARNING: Ignoring cache entry due to a SIZE
> MISMATCH 4198!=4202
...

That last line means the rock database contains an object of size 4198
bytes, with 4202 bytes of data in it.

 4202 > 4198. So 4 bytes of what? the next database slot? padding data?


You have any custom patches applied to this Squid? what version is it?

Amos



From squid3 at treenet.co.nz  Fri Feb 17 13:21:17 2017
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sat, 18 Feb 2017 02:21:17 +1300
Subject: [squid-users] ACL dst handled differently in intercept after
 rewrite
In-Reply-To: <CA++FD2N4ORi1NSusWeOFturFpi-BqDK4Qy0wj6EJbJFjvyzbjg@mail.gmail.com>
References: <CA++FD2N4ORi1NSusWeOFturFpi-BqDK4Qy0wj6EJbJFjvyzbjg@mail.gmail.com>
Message-ID: <2bc4b30f-45c9-bc8b-daa8-47a644a5f9be@treenet.co.nz>

On 15/02/2017 6:27 a.m., Craig Gowing wrote:
> Hi all,
> 
> I've got a squid server running which allows direct proxy and also can
> intercept traffic:
> 
> http_port 10.0.0.1:3128
> http_port 10.0.0.1:3129 intercept
> 
> ---
> 
> There is a URL rewriter which allows the incoming requests (this is just an
> example, I don't really allow all):
> 
> url_rewrite_access allow all
> url_rewrite_program /usr/bin/myrewriter
> 
> ---
> 
> This rewriter will rewrite some URLs to a host on the same network, with
> the intention that the request should not be cached by squid, eg
> http://example.net/somefile.bin -> http://10.0.0.2/example.net/somefile.bin
> So a cache_deny directive is used for this:
> 
> acl local_store dst 10.0.0.2
> cache deny local_store
> 
> ---
> 
> Now when requesting this URL using a defined proxy ...

Where the client sends a URL and expects the proxy to perform DNS
lookups. The destination is any one of a set of IPs returned by DNS from
a lookup performed by Squid.

> ... the ACL matches and the request is not cached.

> If using intercept ...

Where the destination is a specific IP address provided by the NAT
system on the machine Squid is running on. It was selected by the client
from a DNS lookup from that clients machine.

> ... the ACL does not match and it
> does get cached (which caused some storage duplication on the network)
> The debug info shows the following:
> 
> Proxy:
> curl -x "10.0.0.1:3128" "http://example.net/somefile.bin" > /dev/null
> Ip.cc(95) aclIpAddrNetworkCompare: aclIpAddrNetworkCompare: compare:
> 10.0.0.2/[ffff:ffff:ffff:ffff:ffff:ffff:ffff:ffff] (10.0.0.2)  vs
> 10.0.0.2-[::]/[ffff:ffff:ffff:ffff:ffff:ffff:ffff:ffff]
> 
> Intercept:
> curl "http://example.net/somefile.bin" > /dev/null # Intercepted on the NAT
> tables
> Ip.cc(95) aclIpAddrNetworkCompare: aclIpAddrNetworkCompare: compare:
> 93.184.216.34:80/[ffff:ffff:ffff:ffff:ffff:ffff:ffff:ffff] (93.184.216.34:80)
>  vs 10.0.0.2-[::]/[ffff:ffff:ffff:ffff:ffff:ffff:ffff:ffff]
> 
> This seems to show that the ACL is processed at a different stage for the
> two different modes. Now I'm wondering if this is intentional and I
> shouldn't be using the 'dst' ACL here, or should it be more consistant and
> give the same result regardless?

It is intentional.

A NAT intercept proxy has a single specific dst-IP selected by the
client from a DNS lookup done before anythign got near to Squid.

An explicit-proxy has a whole set of potential dst-IPs found from
looking up the URL domain name (after URL rewriter changes) - any one of
which can match.

The URL-rewriter is doing exactly that. Re-writing the *URL*, not the
destination the client was attempting to reach.


> 
> I have a solution to use the 'url_regex' ACL instead which seems consistant
> between the two modes, but it may slightly affect performance.

Why not use dstdomain ACL? it was designed for matching the domain name
which is being requested by the client.


In fact whats the point of the rewriter pre-pending a raw-IP address in
the first place?

> 
> I couldn't find a huge amount of info on what order the ACLs are processed,
> so if anybody could let me know what the expected behaviour should be that
> would be much appreciated.
> 

<http://wiki.squid-cache.org/ProgrammingGuide/Architecture#Transaction_Processing>
is the best we have other than the code itself.


Amos



From squid3 at treenet.co.nz  Fri Feb 17 13:59:25 2017
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sat, 18 Feb 2017 02:59:25 +1300
Subject: [squid-users] Squid on separate box and it can't see packets
In-Reply-To: <CAKNtY_za61AyNijVgLv=+M4tW8JeWgHQVNwvs1pJjfU62vCWTw@mail.gmail.com>
References: <CAKNtY_x5YX-LAhy30Q3n1aeZ_XbeKUf7YyUd8WFKdWYkr9K_Mg@mail.gmail.com>
 <0fea01d2868d$d6fab130$84f01390$@ngtech.co.il>
 <CAKNtY_za61AyNijVgLv=+M4tW8JeWgHQVNwvs1pJjfU62vCWTw@mail.gmail.com>
Message-ID: <ba219e64-ef04-3f50-c22d-24440256d525@treenet.co.nz>

On 15/02/2017 9:18 a.m., John Pearson wrote:
> Hi,
> 
> Is this squid box a router or just a proxy?
> - just a proxy

There is the first problem.

NAT interception needs the machine Squid is running on to be configured
to operate as a router. It will be receiving packets destined to a
machine other than itself.

> 
> What tcpdump command did you ran?
> - sudo tcpdump -i eth0
> 
> What is the networks that are involved?
> Setup:
> 
>> Client        (192.168.1.8) --->  |     Rotuer        |
>>                                                | gateway/dhcp | --->
>> Internet
>> Squid box (192.168.1.2) --->  |  192.168.1.1   |
> 
> 
> Here Client (debian), squid (debian) and router are three separate devices.
> 

So the Squid machine;

requires this bit you did:
 <http://wiki.squid-cache.org/ConfigExamples/Intercept/LinuxRedirect>

PLUS the system TCP stack controls to turn it from a origin-server host
to a routing host. Otherwise the machine will silently drop packets not
destined to itself.


The router machine requires this:
 <http://wiki.squid-cache.org/ConfigExamples/Intercept/IptablesPolicyRoute#When_Squid_is_Internal_amongst_clients>

The router machine probably also needs the "Routing Setup":
 <http://wiki.squid-cache.org/ConfigExamples/Intercept/IptablesPolicyRoute#Routing_Setup>

Amos



From heiler.bemerguy at cinbesa.com.br  Fri Feb 17 14:18:30 2017
From: heiler.bemerguy at cinbesa.com.br (Heiler Bemerguy)
Date: Fri, 17 Feb 2017 11:18:30 -0300
Subject: [squid-users] WARNING: Ignoring cache entry due to a SIZE
 MISMATCH
In-Reply-To: <20ab0957-db57-b6c0-a255-8cd1c709e2e1@treenet.co.nz>
References: <cc488323-b0b4-f6b8-dbab-e6cb3c46764a@cinbesa.com.br>
 <20ab0957-db57-b6c0-a255-8cd1c709e2e1@treenet.co.nz>
Message-ID: <c80a5e4e-c3ef-3293-f3fa-f83bb78293cc@cinbesa.com.br>


Em 17/02/2017 09:44, Amos Jeffries escreveu:
> On 16/02/2017 10:40 a.m., Heiler Bemerguy wrote:
>> Is it normal, on every restart?
>>
> Well, this is a check that is only performed on restart. So in a way it
> is "normal" that it occurs on restart. It should not happen at all
> though. AFAIK, it is a sign of cache corruption...

Right. But cache corruption, by a hardware/disc error, can not be. These 
disks were formatted and tested, 3 times already, while trying to solve 
this errors and some other stuff.


>> maximum_object_size 2 GB
>> cache_dir rock /cache  120000 min-size=0 max-size=12288 slot-size=12288 max-swap-rate=250 swap-timeout=350
>> cache_dir rock /cache2 120000 min-size=10240 max-size=65536 max-swap-rate=250 swap-timeout=360
>>
> These caches cannot store objects larger than 64KB. So where do you
> expect the 64KB thru 2GB objects to go?

There are another cache_dirs just below these, like:
cache_dir rock /cache3 120000 min-size=65537 max-size=262144 
max-swap-rate=250 swap-timeout=380
cache_dir rock /cache4 120000 min-size=262145 max-swap-rate=250 
swap-timeout=500


>
>> 2017/02/15 18:38:50 kid7| WARNING: Ignoring cache entry due to a SIZE
>> MISMATCH 5852!=5853
>> 2017/02/15 18:38:51 kid7| WARNING: Ignoring cache entry due to a SIZE
>> MISMATCH 6223!=6222
>> 2017/02/15 18:38:51 kid7| WARNING: Ignoring cache entry due to a SIZE
>> MISMATCH 4198!=4202
> ...
>
> That last line means the rock database contains an object of size 4198
> bytes, with 4202 bytes of data in it.
>
>   4202 > 4198. So 4 bytes of what? the next database slot? padding data?
>
>
> You have any custom patches applied to this Squid? what version is it?
>

No patches. Squid Cache: Version 4.0.18 configure options: 
'--enable-htcp' '--disable-maintainer-mode' 
'--disable-dependency-tracking' '--disable-wccp' '--disable-snmp' 
'--enable-inline' '--enable-async-io=32' '--enable-storeio=aufs,rock' 
'--enable-underscores' '--enable-removal-policies=lru,heap' 
'--enable-http-violations' '--disable-ident-lookups' 
'--with-large-files' '--enable-ssl' '--enable-ltdl-convenience' 
'--prefix=/usr' '--localstatedir=/var' '--libexecdir=/lib/squid' 
'--srcdir=.' '--datadir=/usr/share/squid' '--sysconfdir=/etc/squid' 
'--with-default-user=proxy' '--with-logdir=/var/log' 
'--with-pidfile=/var/run/squid.pid' '--with-filedescriptors=16384' 
'--with-aufs-threads=32' '--disable-translation'

My only */Suspicion/* is the different slot-size from kid7 (/cache). But 
these messages also appears on kid8 (/cache2), although  with much, MUCH 
less frequency..... (while rebuilding/restarting, always)

-- 
Best Regards,

Heiler Bemerguy
Network Manager - CINBESA
55 91 98151-4894/3184-1751

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20170217/da6ab30c/attachment.htm>

From chiasa.men at web.de  Fri Feb 17 14:27:23 2017
From: chiasa.men at web.de (chiasa.men)
Date: Fri, 17 Feb 2017 15:27:23 +0100
Subject: [squid-users] header_access ssl multiple cache_peers? Redirect http
	to https
Message-ID: <6078163.uBiGQo67VA@march>

hello
there a two cache_peers with ssl enabled webservers. Obviously for both the 
private keys are available. 
For http_port squid can use header_access to filter headers.
Can squid make use of the known private keys in order to filter the headers for 
https_port connetions too?
How?
Is the any method at all to filter headers in ssl connections?

Is it possible to make squid redirect http to https? How?
(http://example.com redirect to  https://example.com)

regards
chiasa


From yvoinov at gmail.com  Fri Feb 17 14:52:21 2017
From: yvoinov at gmail.com (Yuri Voinov)
Date: Fri, 17 Feb 2017 20:52:21 +0600
Subject: [squid-users] squid-avira-update-cache
In-Reply-To: <6cd017ac4ce7d5479e2cdaa7362777ad@xp1>
References: <6cd017ac4ce7d5479e2cdaa7362777ad@xp1>
Message-ID: <f10fe738-46ea-7d18-22e7-dd994c32b186@gmail.com>

Any logs?


17.02.2017 17:43, splicelid at gmail.com ?????:
> Hi all, I'm trying to cache "avira updates" with squid, but no luck...
>
> my conf:
> acl aviraupdate dstdomain .avira-update.com
> range_offset_limit -1 aviraupdate
> refresh_pattern -i avira-update.com/.*\.* 4320 80% 43200 reload-into-ims
>
> any help ? 10x!
>
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users

-- 
Bugs to the Future
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20170217/515c694e/attachment.htm>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: 0x613DEC46.asc
Type: application/pgp-keys
Size: 2437 bytes
Desc: not available
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20170217/515c694e/attachment.key>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: signature.asc
Type: application/pgp-signature
Size: 473 bytes
Desc: OpenPGP digital signature
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20170217/515c694e/attachment.sig>

From yvoinov at gmail.com  Fri Feb 17 17:56:49 2017
From: yvoinov at gmail.com (Yuri Voinov)
Date: Fri, 17 Feb 2017 23:56:49 +0600
Subject: [squid-users] squid-avira-update-cache
In-Reply-To: <6cd017ac4ce7d5479e2cdaa7362777ad@xp1>
References: <6cd017ac4ce7d5479e2cdaa7362777ad@xp1>
Message-ID: <2d66c8ba-1818-ec17-535a-d10d3db3f6d4@gmail.com>

root @ khorne /patch # wget -S
http://personal.avira-update.com/update/x_vdf_sigver/7.12.155.64_8.12.155.64/xbv00050.vdf.lz
--2017-02-17 23:51:22-- 
http://personal.avira-update.com/update/x_vdf_sigver/7.12.155.64_8.12.155.64/xbv00050.vdf.lz
Connecting to 127.0.0.1:3128... connected.
Proxy request sent, awaiting response...
  HTTP/1.1 200 OK
  Server: Apache
  ETag: "934c1d64b13126c7daada1daf0fe3845:1487280555"
  Last-Modified: Thu, 16 Feb 2017 21:26:06 GMT
  Accept-Ranges: bytes
  Content-Length: 2023
  Content-Type: text/plain
  Cache-Control: max-age=45
  Expires: Fri, 17 Feb 2017 17:52:13 GMT
  Date: Fri, 17 Feb 2017 17:51:28 GMT
  X-Cache: MISS from khorne
  X-Cache-Lookup: MISS from khorne:3128
  Connection: keep-alive
Length: 2023 (2.0K) [text/plain]
Saving to: 'xbv00050.vdf.lz'

xbv00050.vdf.lz     100%[===================>]   1.98K  --.-KB/s    in
0s     

2017-02-17 23:51:28 (176 MB/s) - 'xbv00050.vdf.lz' saved [2023/2023]

root @ khorne /patch # wget -S
http://personal.avira-update.com/update/x_vdf_sigver/7.12.155.64_8.12.155.64/xbv00050.vdf.lz
--2017-02-17 23:51:40-- 
http://personal.avira-update.com/update/x_vdf_sigver/7.12.155.64_8.12.155.64/xbv00050.vdf.lz
Connecting to 127.0.0.1:3128... connected.
Proxy request sent, awaiting response...
  HTTP/1.1 200 OK
  Server: Apache
  ETag: "934c1d64b13126c7daada1daf0fe3845:1487280555"
  Last-Modified: Thu, 16 Feb 2017 21:26:06 GMT
  Accept-Ranges: bytes
  Content-Length: 2023
  Content-Type: text/plain
  Cache-Control: max-age=45
  X-Origin-Date: Fri, 17 Feb 2017 17:51:28 GMT
  Date: Fri, 17 Feb 2017 17:51:40 GMT
  X-Origin-Expires: Fri, 17 Feb 2017 17:52:13 GMT
  Expires: Fri, 17 Feb 2017 17:52:25 GMT
  X-Cache-Age: 12
  X-Cache: HIT from khorne
  X-Cache-Lookup: HIT from khorne:3128
  Connection: keep-alive
Length: 2023 (2.0K) [text/plain]
Saving to: 'xbv00050.vdf.lz.1'

xbv00050.vdf.lz.1   100%[===================>]   1.98K  --.-KB/s    in
0s     

2017-02-17 23:51:40 (233 MB/s) - 'xbv00050.vdf.lz.1' saved [2023/2023]

root @ khorne /patch # wget -S
http://personal.avira-update.com/update/x_vdf_sigver/7.12.155.64_8.12.155.64/xbv00044.vdf.lz
--2017-02-17 23:52:22-- 
http://personal.avira-update.com/update/x_vdf_sigver/7.12.155.64_8.12.155.64/xbv00044.vdf.lz
Connecting to 127.0.0.1:3128... connected.
Proxy request sent, awaiting response...
  HTTP/1.1 200 OK
  Server: Apache
  ETag: "953d632488104f11aa7252533ce6389b:1487280555"
  Last-Modified: Thu, 16 Feb 2017 21:26:06 GMT
  Accept-Ranges: bytes
  Content-Length: 13443
  Content-Type: text/plain
  Cache-Control: max-age=162
  Expires: Fri, 17 Feb 2017 17:55:04 GMT
  Date: Fri, 17 Feb 2017 17:52:22 GMT
  X-Cache: MISS from khorne
  X-Cache-Lookup: MISS from khorne:3128
  Connection: keep-alive
Length: 13443 (13K) [text/plain]
Saving to: 'xbv00044.vdf.lz'

xbv00044.vdf.lz     100%[===================>]  13.13K  --.-KB/s    in
0.003s 

2017-02-17 23:52:22 (4.20 MB/s) - 'xbv00044.vdf.lz' saved [13443/13443]

root @ khorne /patch # wget -S
http://personal.avira-update.com/update/x_vdf_sigver/7.12.155.64_8.12.155.64/xbv00044.vdf.lz
--2017-02-17 23:52:23-- 
http://personal.avira-update.com/update/x_vdf_sigver/7.12.155.64_8.12.155.64/xbv00044.vdf.lz
Connecting to 127.0.0.1:3128... connected.
Proxy request sent, awaiting response...
  HTTP/1.1 200 OK
  Server: Apache
  ETag: "953d632488104f11aa7252533ce6389b:1487280555"
  Last-Modified: Thu, 16 Feb 2017 21:26:06 GMT
  Accept-Ranges: bytes
  Content-Length: 13443
  Content-Type: text/plain
  Cache-Control: max-age=162
  X-Origin-Date: Fri, 17 Feb 2017 17:52:22 GMT
  Date: Fri, 17 Feb 2017 17:52:23 GMT
  X-Origin-Expires: Fri, 17 Feb 2017 17:55:04 GMT
  Expires: Fri, 17 Feb 2017 17:55:05 GMT
  X-Cache-Age: 1
  X-Cache: HIT from khorne
  X-Cache-Lookup: HIT from khorne:3128
  Connection: keep-alive
Length: 13443 (13K) [text/plain]
Saving to: 'xbv00044.vdf.lz.1'

xbv00044.vdf.lz.1   100%[===================>]  13.13K  --.-KB/s    in
0.03s  

2017-02-17 23:52:23 (441 KB/s) - 'xbv00044.vdf.lz.1' saved [13443/13443]

Well, let's give big Avira file:


root @ khorne /patch # wget -S
http://personal.avira-update.com/update/n_vdf/vbase005.vdf.gz
--2017-02-17 23:54:04-- 
http://personal.avira-update.com/update/n_vdf/vbase005.vdf.gz
Connecting to 127.0.0.1:3128... connected.
Proxy request sent, awaiting response...
  HTTP/1.1 200 OK
  Server: Apache
  ETag: "5af09f9e8a3d09809ce8005ddb55fa7d:1487352426"
  Last-Modified: Fri, 17 Feb 2017 17:25:36 GMT
  Accept-Ranges: bytes
  Content-Length: 11556885
  Content-Type: application/x-gzip
  Cache-Control: max-age=185
  Expires: Fri, 17 Feb 2017 17:57:09 GMT
  Date: Fri, 17 Feb 2017 17:54:04 GMT
  X-Cache: MISS from khorne
  X-Cache-Lookup: MISS from khorne:3128
  Connection: keep-alive
Length: 11556885 (11M) [application/x-gzip]
Saving to: 'vbase005.vdf.gz'

vbase005.vdf.gz     100%[===================>]  11.02M   471KB/s    in
24s    

2017-02-17 23:54:28 (471 KB/s) - 'vbase005.vdf.gz' saved [11556885/11556885]

root @ khorne /patch # wget -S
http://personal.avira-update.com/update/n_vdf/vbase005.vdf.gz
--2017-02-17 23:54:29-- 
http://personal.avira-update.com/update/n_vdf/vbase005.vdf.gz
Connecting to 127.0.0.1:3128... connected.
Proxy request sent, awaiting response...
  HTTP/1.1 200 OK
  Server: Apache
  ETag: "5af09f9e8a3d09809ce8005ddb55fa7d:1487352426"
  Last-Modified: Fri, 17 Feb 2017 17:25:36 GMT
  Accept-Ranges: bytes
  Content-Length: 11556885
  Content-Type: application/x-gzip
  Cache-Control: max-age=185
  X-Origin-Date: Fri, 17 Feb 2017 17:54:04 GMT
  Date: Fri, 17 Feb 2017 17:54:29 GMT
  X-Origin-Expires: Fri, 17 Feb 2017 17:57:09 GMT
  Expires: Fri, 17 Feb 2017 17:57:34 GMT
  X-Cache-Age: 25
  X-Cache: HIT from khorne
  X-Cache-Lookup: HIT from khorne:3128
  Connection: keep-alive
Length: 11556885 (11M) [application/x-gzip]
Saving to: 'vbase005.vdf.gz.1'

vbase005.vdf.gz.1   100%[===================>]  11.02M   1545KB/s    in
4s    

2017-02-17 23:54:53 (468 KB/s) - 'vbase005.vdf.gz.1' saved
[11556885/11556885]

See no problem to cache this. Check your config. May be you have some
stuff in squid.conf which preventing caching.

17.02.2017 17:43, splicelid at gmail.com ?????:
> Hi all, I'm trying to cache "avira updates" with squid, but no luck...
>
> my conf:
> acl aviraupdate dstdomain .avira-update.com
> range_offset_limit -1 aviraupdate
> refresh_pattern -i avira-update.com/.*\.* 4320 80% 43200 reload-into-ims
>
> any help ? 10x!
>
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users

-- 
Bugs to the Future
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20170217/b346dc21/attachment.htm>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: 0x613DEC46.asc
Type: application/pgp-keys
Size: 2437 bytes
Desc: not available
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20170217/b346dc21/attachment.key>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: signature.asc
Type: application/pgp-signature
Size: 473 bytes
Desc: OpenPGP digital signature
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20170217/b346dc21/attachment.sig>

From heiler.bemerguy at cinbesa.com.br  Fri Feb 17 19:31:11 2017
From: heiler.bemerguy at cinbesa.com.br (Heiler Bemerguy)
Date: Fri, 17 Feb 2017 16:31:11 -0300
Subject: [squid-users] squid-avira-update-cache
In-Reply-To: <f10fe738-46ea-7d18-22e7-dd994c32b186@gmail.com>
References: <6cd017ac4ce7d5479e2cdaa7362777ad@xp1>
 <f10fe738-46ea-7d18-22e7-dd994c32b186@gmail.com>
Message-ID: <d7846e49-350a-df36-b591-8743306cd5a0@cinbesa.com.br>


I've noticed this:

2017/02/17 16:28:05.632 kid4| ctx: enter level  0: 
'http://personal.avira-update.com/update/idx/localdecider_sigver-win32-int-13.0.1.12.info.lz'
2017/02/17 16:28:05.632 kid4| 22,3| http.cc(339) cacheableReply: NO 
because e:=p2XDIV/0x15c49190*3 has been released.
2017/02/17 16:28:05.797 kid4| ctx: enter level  0: 
'http://personal.avira-update.com/update/idx/weblocaldecider_sigver-win32-int-15.0.15.28.info.lz'
2017/02/17 16:28:05.797 kid4| 22,3| http.cc(339) cacheableReply: NO 
because e:=p2XDIV/0x15c49190*3 has been released.
2017/02/17 16:28:17.803 kid4| ctx: enter level  0: 
'http://personal.avira-update.com/update/x_vdf_sigver/7.12.155.132_8.12.155.132/aevdf.dat.lz'
2017/02/17 16:28:17.803 kid4| 22,3| http.cc(339) cacheableReply: NO 
because e:=p2XDIV/0x6a233d0*3 has been released.

It seems I'm not caching it too, but couldn't understand what "has been 
released" is referring too.

Squid Cache: Version 4.0.18


-- 
Best Regards,

Heiler Bemerguy
Network Manager - CINBESA
55 91 98151-4894/3184-1751


Em 17/02/2017 11:52, Yuri Voinov escreveu:
>
> Any logs?
>
>
> 17.02.2017 17:43, splicelid at gmail.com ?????:
>> Hi all, I'm trying to cache "avira updates" with squid, but no luck...
>>
>> my conf:
>> acl aviraupdate dstdomain .avira-update.com
>> range_offset_limit -1 aviraupdate
>> refresh_pattern -i avira-update.com/.*\.* 4320 80% 43200 reload-into-ims
>>
>> any help ? 10x!
>>
>>
>> _______________________________________________
>> squid-users mailing list
>> squid-users at lists.squid-cache.org
>> http://lists.squid-cache.org/listinfo/squid-users
>
> -- 
> Bugs to the Future
>
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20170217/10a256ab/attachment.htm>

From rousskov at measurement-factory.com  Fri Feb 17 20:05:53 2017
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Fri, 17 Feb 2017 13:05:53 -0700
Subject: [squid-users] squid-avira-update-cache
In-Reply-To: <d7846e49-350a-df36-b591-8743306cd5a0@cinbesa.com.br>
References: <6cd017ac4ce7d5479e2cdaa7362777ad@xp1>
 <f10fe738-46ea-7d18-22e7-dd994c32b186@gmail.com>
 <d7846e49-350a-df36-b591-8743306cd5a0@cinbesa.com.br>
Message-ID: <4c1da1ac-89cc-374e-23f6-6546cb8f720a@measurement-factory.com>

On 02/17/2017 12:31 PM, Heiler Bemerguy wrote:
> 
> I've noticed this:
> 
> 2017/02/17 16:28:05.632 kid4| ctx: enter level  0:
> 'http://personal.avira-update.com/update/idx/localdecider_sigver-win32-int-13.0.1.12.info.lz'
> 2017/02/17 16:28:05.632 kid4| 22,3| http.cc(339) cacheableReply: NO
> because e:=p2XDIV/0x15c49190*3 has been released.
> 2017/02/17 16:28:05.797 kid4| ctx: enter level  0:
> 'http://personal.avira-update.com/update/idx/weblocaldecider_sigver-win32-int-15.0.15.28.info.lz'
> 2017/02/17 16:28:05.797 kid4| 22,3| http.cc(339) cacheableReply: NO
> because e:=p2XDIV/0x15c49190*3 has been released.
> 2017/02/17 16:28:17.803 kid4| ctx: enter level  0:
> 'http://personal.avira-update.com/update/x_vdf_sigver/7.12.155.132_8.12.155.132/aevdf.dat.lz'
> 2017/02/17 16:28:17.803 kid4| 22,3| http.cc(339) cacheableReply: NO
> because e:=p2XDIV/0x6a233d0*3 has been released.
> 
> It seems I'm not caching it too, but couldn't understand what "has been
> released" is referring too.

In this debugging context, "has been released" means that the response
was marked for removal from the cache some time earlier. It will be
delivered to the current client (or several concurrent clients in some
cases) but it will not be available to future clients.

These debugging lines do not tell us why or when that marking was
applied. It is possible that the response had some anti-caching
Cache-Control headers or was too big to cache, but these are just two
examples; there are lots of other possible reasons.


HTH,

Alex.



From heiler.bemerguy at cinbesa.com.br  Fri Feb 17 20:27:46 2017
From: heiler.bemerguy at cinbesa.com.br (Heiler Bemerguy)
Date: Fri, 17 Feb 2017 17:27:46 -0300
Subject: [squid-users] squid-avira-update-cache
In-Reply-To: <4c1da1ac-89cc-374e-23f6-6546cb8f720a@measurement-factory.com>
References: <6cd017ac4ce7d5479e2cdaa7362777ad@xp1>
 <f10fe738-46ea-7d18-22e7-dd994c32b186@gmail.com>
 <d7846e49-350a-df36-b591-8743306cd5a0@cinbesa.com.br>
 <4c1da1ac-89cc-374e-23f6-6546cb8f720a@measurement-factory.com>
Message-ID: <388fba18-83b5-d2e7-b450-8c15ca74910f@cinbesa.com.br>



Em 17/02/2017 17:05, Alex Rousskov escreveu:
> On 02/17/2017 12:31 PM, Heiler Bemerguy wrote:
>> I've noticed this:
>>
>> 2017/02/17 16:28:05.632 kid4| ctx: enter level  0:
>> 'http://personal.avira-update.com/update/idx/localdecider_sigver-win32-int-13.0.1.12.info.lz'
>> 2017/02/17 16:28:05.632 kid4| 22,3| http.cc(339) cacheableReply: NO
>> because e:=p2XDIV/0x15c49190*3 has been released.
>> 2017/02/17 16:28:05.797 kid4| ctx: enter level  0:
>> 'http://personal.avira-update.com/update/idx/weblocaldecider_sigver-win32-int-15.0.15.28.info.lz'
>> 2017/02/17 16:28:05.797 kid4| 22,3| http.cc(339) cacheableReply: NO
>> because e:=p2XDIV/0x15c49190*3 has been released.
>> 2017/02/17 16:28:17.803 kid4| ctx: enter level  0:
>> 'http://personal.avira-update.com/update/x_vdf_sigver/7.12.155.132_8.12.155.132/aevdf.dat.lz'
>> 2017/02/17 16:28:17.803 kid4| 22,3| http.cc(339) cacheableReply: NO
>> because e:=p2XDIV/0x6a233d0*3 has been released.
>>
>> It seems I'm not caching it too, but couldn't understand what "has been
>> released" is referring too.
> In this debugging context, "has been released" means that the response
> was marked for removal from the cache some time earlier. It will be
> delivered to the current client (or several concurrent clients in some
> cases) but it will not be available to future clients.
>
> These debugging lines do not tell us why or when that marking was
> applied. It is possible that the response had some anti-caching
> Cache-Control headers or was too big to cache, but these are just two
> examples; there are lots of other possible reasons.

When you say "some time earlier", you mean days? Because today, every 
mention to "avira-update" just shows up those lines.. here's a bigger log:

2017/02/17 17:17:46.328 kid2| ctx: enter level  0: 
'http://img.olx.com.br/thumbsli/87/873721001890097.jpg'
2017/02/17 17:17:46.328 kid2| 22,3| refresh.cc(280) refreshCheck: 
checking freshness of URI: 
http://img.olx.com.br/thumbsli/87/873721001890097.jpg
2017/02/17 17:17:46.328 kid2| 22,3| refresh.cc(302) refreshCheck: 
Matched 
'\.(jp[eg]{1,2}|pdf|gif|pn[pg]|bmp|tiff|ico|swf|css|js|ad|png)\??$ 
604800 80%% 7257600'
2017/02/17 17:17:46.328 kid2| 22,3| refresh.cc(304) refreshCheck:       
age:    60
2017/02/17 17:17:46.328 kid2| 22,3| refresh.cc(306) refreshCheck:       
check_time:     Fri, 17 Feb 2017 20:18:46 GMT
2017/02/17 17:17:46.328 kid2| 22,3| refresh.cc(308) refreshCheck:       
entry->timestamp:       Fri, 17 Feb 2017 20:17:46 GMT
2017/02/17 17:17:46.328 kid2| 22,3| refresh.cc(156) refreshStaleness: 
FRESH: expires 1487962999 >= check_time 1487362726
2017/02/17 17:17:46.328 kid2| 22,3| refresh.cc(328) refreshCheck: 
Staleness = -1
2017/02/17 17:17:46.328 kid2| 22,3| refresh.cc(453) refreshCheck: Object 
isn't stale..
2017/02/17 17:17:46.328 kid2| 22,3| refresh.cc(455) refreshCheck: 
returning FRESH_EXPIRES
2017/02/17 17:17:46.328 kid2| 22,3| http.cc(488) cacheableReply: YES 
because HTTP status 200
2017/02/17 17:17:46.431 kid2| ctx: exit level  0*
**2017/02/17 17:17:46.432 kid2| ctx: enter level  0: 
'http://personal.avira-update.com/update/idx/webcat_sigver-common-int-2017_9.0.217.1900.info.lz'**
**2017/02/17 17:17:46.432 kid2| 22,3| http.cc(339) cacheableReply: NO 
because e:=p2XDIV/0x1fcb650*3 has been released.**
**2017/02/17 17:17:46.600 kid2| ctx: exit level  0**
**2017/02/17 17:17:46.600 kid2| ctx: enter level  0: 
'http://personal.avira-update.com/update/idx/localdecider_sigver-win32-int-13.0.1.12.info.lz'**
**2017/02/17 17:17:46.600 kid2| 22,3| http.cc(339) cacheableReply: NO 
because e:=p2XDIV/0x1fcb650*3 has been released.**
**2017/02/17 17:17:46.732 kid2| ctx: exit level  0**
**2017/02/17 17:17:46.732 kid2| 22,3| refresh.cc(646) getMaxAge: 
getMaxAge: 'http://img.olx.com.br/thumbsli/14/142717018210327.jpg'**
**2017/02/17 17:17:46.769 kid2| ctx: enter level  0: 
'http://personal.avira-update.com/update/idx/weblocaldecider_sigver-win32-int-15.0.15.28.info.lz'**
**2017/02/17 17:17:46.769 kid2| 22,3| http.cc(339) cacheableReply: NO 
because e:=p2XDIV/0x1fcb650*3 has been released.*
*2017/02/17 17:17:46.791 kid2| ctx: exit level  0*
2017/02/17 17:17:46.791 kid2| 22,3| refresh.cc(280) refreshCheck: 
checking freshness of URI: http://www.googletagservices.com/tag/js/gpt.js

I was using ALL,3 and cat cache.log |grep -C 10 avira-update |grep kid2

-- 

Best Regards,

Heiler Bemerguy
Network Manager - CINBESA
55 91 98151-4894/3184-1751

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20170217/00d7b498/attachment.htm>

From rousskov at measurement-factory.com  Fri Feb 17 20:31:37 2017
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Fri, 17 Feb 2017 13:31:37 -0700
Subject: [squid-users] squid-avira-update-cache
In-Reply-To: <388fba18-83b5-d2e7-b450-8c15ca74910f@cinbesa.com.br>
References: <6cd017ac4ce7d5479e2cdaa7362777ad@xp1>
 <f10fe738-46ea-7d18-22e7-dd994c32b186@gmail.com>
 <d7846e49-350a-df36-b591-8743306cd5a0@cinbesa.com.br>
 <4c1da1ac-89cc-374e-23f6-6546cb8f720a@measurement-factory.com>
 <388fba18-83b5-d2e7-b450-8c15ca74910f@cinbesa.com.br>
Message-ID: <87c789b9-7e1e-f408-43fb-fbd700b16fe3@measurement-factory.com>

On 02/17/2017 01:27 PM, Heiler Bemerguy wrote:
> Em 17/02/2017 17:05, Alex Rousskov escreveu:
>> On 02/17/2017 12:31 PM, Heiler Bemerguy wrote:
>>> I've noticed this:
>>>
>>> 2017/02/17 16:28:05.632 kid4| ctx: enter level  0:
>>> 'http://personal.avira-update.com/update/idx/localdecider_sigver-win32-int-13.0.1.12.info.lz'
>>> 2017/02/17 16:28:05.632 kid4| 22,3| http.cc(339) cacheableReply: NO
>>> because e:=p2XDIV/0x15c49190*3 has been released.
>>> 2017/02/17 16:28:05.797 kid4| ctx: enter level  0:
>>> 'http://personal.avira-update.com/update/idx/weblocaldecider_sigver-win32-int-15.0.15.28.info.lz'
>>> 2017/02/17 16:28:05.797 kid4| 22,3| http.cc(339) cacheableReply: NO
>>> because e:=p2XDIV/0x15c49190*3 has been released.
>>> 2017/02/17 16:28:17.803 kid4| ctx: enter level  0:
>>> 'http://personal.avira-update.com/update/x_vdf_sigver/7.12.155.132_8.12.155.132/aevdf.dat.lz'
>>> 2017/02/17 16:28:17.803 kid4| 22,3| http.cc(339) cacheableReply: NO
>>> because e:=p2XDIV/0x6a233d0*3 has been released.
>>>
>>> It seems I'm not caching it too, but couldn't understand what "has been
>>> released" is referring too.
>> In this debugging context, "has been released" means that the response
>> was marked for removal from the cache some time earlier. It will be
>> delivered to the current client (or several concurrent clients in some
>> cases) but it will not be available to future clients.

>> These debugging lines do not tell us why or when that marking was
>> applied. It is possible that the response had some anti-caching
>> Cache-Control headers or was too big to cache, but these are just two
>> examples; there are lots of other possible reasons.

> When you say "some time earlier", you mean days?

Usually seconds or milliseconds (but, in theory, it could be a lot
longer than that, even days).


> I was using ALL,3 and cat cache.log |grep -C 10 avira-update |grep kid2

Sorry, I do not have enough information to answer "why" or "when".
Others on the list may be able to guide you further.

Alex.



From splicelid at gmail.com  Sat Feb 18 07:41:44 2017
From: splicelid at gmail.com (splicelid at gmail.com)
Date: Sat, 18 Feb 2017 09:41:44 +0200
Subject: [squid-users] squid-avira-update-cache
In-Reply-To: <f10fe738-46ea-7d18-22e7-dd994c32b186@gmail.com>
References: <6cd017ac4ce7d5479e2cdaa7362777ad@xp1>
 <f10fe738-46ea-7d18-22e7-dd994c32b186@gmail.com>
Message-ID: <2F0F8B9B-5B1E-4045-8C01-B1E62C9791A3@gmail.com>

1487346706.878     60 10.3.2.254 TCP_MISS/200 265134 GET http://personal.avira-update.com/update/ave2_sigver/8.3.42.182/win32/int/aegen.dll.lz - ORIGINAL_DST/212.95.165.16 application/octet-stream

1487349903.649    129 10.3.2.254 TCP_TUNNEL/200 153 CONNECT ssldev.oes.avira.com:443 - ORIGINAL_DST/52.58.138.100 -

1487385532.212 43199639 10.3.2.254 TAG_NONE_TIMEDOUT/409 0 CONNECT ssldev.oes.avira.com:443 - HIER_NONE/- text/html;charset=utf-8

> On Feb 17, 2017, at 16:52, Yuri Voinov <yvoinov at gmail.com> wrote:
> 
> Any logs?
> 
> 17.02.2017 17:43, splicelid at gmail.com ?????:
>> Hi all, I'm trying to cache "avira updates" with squid, but no luck...
>> 
>> my conf:
>> acl aviraupdate dstdomain .avira-update.com
>> range_offset_limit -1 aviraupdate
>> refresh_pattern -i avira-update.com/.*\.* 4320 80% 43200 reload-into-ims
>> 
>> any help ? 10x!
>> 
>> 
>> _______________________________________________
>> squid-users mailing list
>> squid-users at lists.squid-cache.org
>> http://lists.squid-cache.org/listinfo/squid-users
> 
> -- 
> Bugs to the Future
> <0x613DEC46.asc>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20170218/000cb470/attachment.htm>

From squid3 at treenet.co.nz  Sat Feb 18 12:01:59 2017
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sun, 19 Feb 2017 01:01:59 +1300
Subject: [squid-users] header_access ssl multiple cache_peers? Redirect
 http to https
In-Reply-To: <6078163.uBiGQo67VA@march>
References: <6078163.uBiGQo67VA@march>
Message-ID: <4f54957e-5cfa-c2e5-a6fb-fef14d8509b9@treenet.co.nz>

On 18/02/2017 3:27 a.m., chiasa.men wrote:
> hello
> there a two cache_peers with ssl enabled webservers. Obviously for both the 
> private keys are available. 
> For http_port squid can use header_access to filter headers.
> Can squid make use of the known private keys in order to filter the headers for 
> https_port connetions too?
> How?

When you control the domains you can setup a Squid as CDN proxy in front
of the actual origin servers.

<http://wiki.squid-cache.org/ConfigExamples/Reverse/SslWithWildcardCertifiate>

Then the domains public DNS just needs to present the Squid machine IP
as the server to all visitors.

> Is the any method at all to filter headers in ssl connections?
> 
> Is it possible to make squid redirect http to https? How?
> (http://example.com redirect to  https://example.com)
> 

 acl HTTP proto HTTP
 deny_info 301:https://%H%R HTTP
 http_access deny HTTP


Amos



From squid3 at treenet.co.nz  Sat Feb 18 12:19:28 2017
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sun, 19 Feb 2017 01:19:28 +1300
Subject: [squid-users] squid-avira-update-cache
In-Reply-To: <87c789b9-7e1e-f408-43fb-fbd700b16fe3@measurement-factory.com>
References: <6cd017ac4ce7d5479e2cdaa7362777ad@xp1>
 <f10fe738-46ea-7d18-22e7-dd994c32b186@gmail.com>
 <d7846e49-350a-df36-b591-8743306cd5a0@cinbesa.com.br>
 <4c1da1ac-89cc-374e-23f6-6546cb8f720a@measurement-factory.com>
 <388fba18-83b5-d2e7-b450-8c15ca74910f@cinbesa.com.br>
 <87c789b9-7e1e-f408-43fb-fbd700b16fe3@measurement-factory.com>
Message-ID: <9413fa07-c795-2312-d4b4-4ebf0b54e685@treenet.co.nz>

On 18/02/2017 9:31 a.m., Alex Rousskov wrote:
> On 02/17/2017 01:27 PM, Heiler Bemerguy wrote:
>> Em 17/02/2017 17:05, Alex Rousskov escreveu:
>>> On 02/17/2017 12:31 PM, Heiler Bemerguy wrote:
>>>> I've noticed this:
>>>>
>>>> 2017/02/17 16:28:05.632 kid4| ctx: enter level  0:
>>>> 'http://personal.avira-update.com/update/idx/localdecider_sigver-win32-int-13.0.1.12.info.lz'
>>>> 2017/02/17 16:28:05.632 kid4| 22,3| http.cc(339) cacheableReply: NO
>>>> because e:=p2XDIV/0x15c49190*3 has been released.
>>>> 2017/02/17 16:28:05.797 kid4| ctx: enter level  0:
>>>> 'http://personal.avira-update.com/update/idx/weblocaldecider_sigver-win32-int-15.0.15.28.info.lz'
>>>> 2017/02/17 16:28:05.797 kid4| 22,3| http.cc(339) cacheableReply: NO
>>>> because e:=p2XDIV/0x15c49190*3 has been released.
>>>> 2017/02/17 16:28:17.803 kid4| ctx: enter level  0:
>>>> 'http://personal.avira-update.com/update/x_vdf_sigver/7.12.155.132_8.12.155.132/aevdf.dat.lz'
>>>> 2017/02/17 16:28:17.803 kid4| 22,3| http.cc(339) cacheableReply: NO
>>>> because e:=p2XDIV/0x6a233d0*3 has been released.
>>>>
>>>> It seems I'm not caching it too, but couldn't understand what "has been
>>>> released" is referring too.
>>> In this debugging context, "has been released" means that the response
>>> was marked for removal from the cache some time earlier. It will be
>>> delivered to the current client (or several concurrent clients in some
>>> cases) but it will not be available to future clients.
> 
>>> These debugging lines do not tell us why or when that marking was
>>> applied. It is possible that the response had some anti-caching
>>> Cache-Control headers or was too big to cache, but these are just two
>>> examples; there are lots of other possible reasons.
> 
>> When you say "some time earlier", you mean days?
> 
> Usually seconds or milliseconds (but, in theory, it could be a lot
> longer than that, even days).
> 
> 
>> I was using ALL,3 and cat cache.log |grep -C 10 avira-update |grep kid2
> 
> Sorry, I do not have enough information to answer "why" or "when".
> Others on the list may be able to guide you further.
> 

I usually point people to redbot.org at this point. But the report there
says the URL is cacheable so long as one is not using If-None-Match
headers. So it is not something the server is preventing, although ~5
min is a rather short TTL.


PS. The 'splicelid' person is showing access.log entries that indicate
they are intercepting traffic. So the Host-verify feature may be
involved with marking the objects as unsafe to cache. Avira was one
company I had a rather difficult discussion with about how their AV
client was using the Host header.

Amos



From squid-user at tlinx.org  Sat Feb 18 22:31:02 2017
From: squid-user at tlinx.org (L A Walsh)
Date: Sat, 18 Feb 2017 14:31:02 -0800
Subject: [squid-users] squid & handling/propagating certificat
	revocations...?
Message-ID: <58A8CB26.20008@tlinx.org>

How does squid 'normally' handle security revocations, like from
this test page?:

  https://revoked.grc.com/

Or how 'should' it be handling it (i.e. is my setup more broken
than most? ;^) )

Or, when squid fetches the page, does it do any checking before
sending it to the user?

Or, does it pass it through, w/o checking, to user, but check
revocation before storing it in the local disk cache.

In the above two cases, a client (say a browser) configured to
check revocations, would detect the revocations both on initial
connect as well as content served from cache.  That works, though
it _might_ be more efficient if squid didn't cache such pages.

However, in the case of squid using https-interception to allow
breaking open otherwise uncacheable streams, my configuration doesn't
seem to check if a remote site is using a revoked cert.

So question(s):  Is there anyway to configure squid to check and
either add a message to the page indicating the security revocation,
or, at least, fail in retrieving the message? 

And, ideally, _could_ squid interactively prompt the user about
whether or not the specific cert should be used/allowed anyway,
*and* whether or not the cert should be _stored_ as an "exception"?
If so, then further connects would "just work", otherwise, clients
would get an error message)?

Ideas?  Anyone else solved this problem?

Thanks!
-linda




From rousskov at measurement-factory.com  Sat Feb 18 23:17:03 2017
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Sat, 18 Feb 2017 16:17:03 -0700
Subject: [squid-users] squid & handling/propagating certificat
 revocations...?
In-Reply-To: <58A8CB26.20008@tlinx.org>
References: <58A8CB26.20008@tlinx.org>
Message-ID: <00ae3580-0047-f616-c56d-9b10e83c0b82@measurement-factory.com>

On 02/18/2017 03:31 PM, L A Walsh wrote:
> How does squid 'normally' handle security revocations,

In the simplest case without SslBump, Squid does not see the server
certificate at all and, hence, cannot validate it. Squid simply tunnels
opaque bytes, including certificate bytes, from the server to the
client. You cannot customize this aspect of Squid behavior.

If an SslBump peeking or staring rule matches at step2, then Squid
receives the server certificate and asks OpenSSL to validate it before
sending/receiving any HTTP-level data to/from that server. If your
OpenSSL installation considers the certificate revoked, Squid will treat
it as such and, by default, terminate the connection to the origin
server without receiving an HTTP response.

You can customize that default using several configuration options
and/or a certificate validation helper. Search squid.conf.documented for
"certificate" and "sslcrtvalidator_program".


> However, in the case of squid using https-interception to allow
> breaking open otherwise uncacheable streams, my configuration doesn't
> seem to check if a remote site is using a revoked cert.

Squid does not validate server certificates that it does not see. For
example, if you splice the intercepted connections at SslBump step1 or
step2, then Squid will not see the server certificate and will just
forward all certificate bytes to the client "as is".


> And, ideally, _could_ squid interactively prompt the user about
> whether or not the specific cert should be used/allowed anyway,
> *and* whether or not the cert should be _stored_ as an "exception"?

Upon receiving a revoked certificate, a _browser_ can do that. Squid is
not a User Agent and does not talk to the user directly (although it
can, in some cases, respond with an error page that a user will see).

If you are bumping (rather than splicing) SSL connections, then the fake
certificate generated by Squid will not be revoked even if the origin
server certificate was -- I do not think Squid mimics that aspect of the
origin server certificate.


HTH,

Alex.



From squid3 at treenet.co.nz  Sun Feb 19 08:03:50 2017
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sun, 19 Feb 2017 21:03:50 +1300
Subject: [squid-users] WARNING: Ignoring cache entry due to a SIZE
 MISMATCH
In-Reply-To: <c80a5e4e-c3ef-3293-f3fa-f83bb78293cc@cinbesa.com.br>
References: <cc488323-b0b4-f6b8-dbab-e6cb3c46764a@cinbesa.com.br>
 <20ab0957-db57-b6c0-a255-8cd1c709e2e1@treenet.co.nz>
 <c80a5e4e-c3ef-3293-f3fa-f83bb78293cc@cinbesa.com.br>
Message-ID: <d5baa8ad-2473-6433-3ac6-0027315ee15e@treenet.co.nz>

On 18/02/2017 3:18 a.m., Heiler Bemerguy wrote:
> 
> Em 17/02/2017 09:44, Amos Jeffries escreveu:
>> On 16/02/2017 10:40 a.m., Heiler Bemerguy wrote:
>>> Is it normal, on every restart?
>>>
>> Well, this is a check that is only performed on restart. So in a way it
>> is "normal" that it occurs on restart. It should not happen at all
>> though. AFAIK, it is a sign of cache corruption...
> 
> Right. But cache corruption, by a hardware/disc error, can not be. These
> disks were formatted and tested, 3 times already, while trying to solve
> this errors and some other stuff.
> 

Hardware errors are the most well-known, but not the only source of
corruption.

The second most common source of cache corruption is Squid being forced
to shutdown before it has fully saved the cache data. These messages are
the rock equivalent of the UFS "No such file or directory" warnings on
startup/restart.

The chances of these issues showing up increases with the size of the
cache, size of traffic going through the proxy at the shutdown time, and
shortness of the shutdown wait period.
Your 480 GB of rock caches is something I would expect to have at least
some issues with.

> 
>>> maximum_object_size 2 GB
>>> cache_dir rock /cache  120000 min-size=0 max-size=12288
>>> slot-size=12288 max-swap-rate=250 swap-timeout=350
>>> cache_dir rock /cache2 120000 min-size=10240 max-size=65536
>>> max-swap-rate=250 swap-timeout=360
>>>
>> These caches cannot store objects larger than 64KB. So where do you
>> expect the 64KB thru 2GB objects to go?
> 
> There are another cache_dirs just below these, like:
> cache_dir rock /cache3 120000 min-size=65537 max-size=262144
> max-swap-rate=250 swap-timeout=380
> cache_dir rock /cache4 120000 min-size=262145 max-swap-rate=250
> swap-timeout=500
> 

Ah. Okay.

> 
>>
>>> 2017/02/15 18:38:50 kid7| WARNING: Ignoring cache entry due to a SIZE
>>> MISMATCH 5852!=5853
>>> 2017/02/15 18:38:51 kid7| WARNING: Ignoring cache entry due to a SIZE
>>> MISMATCH 6223!=6222
>>> 2017/02/15 18:38:51 kid7| WARNING: Ignoring cache entry due to a SIZE
>>> MISMATCH 4198!=4202
>> ...
>>
>> That last line means the rock database contains an object of size 4198
>> bytes, with 4202 bytes of data in it.
>>
>>   4202 > 4198. So 4 bytes of what? the next database slot? padding data?
>>
>>
>> You have any custom patches applied to this Squid? what version is it?
>>
> 
> No patches. Squid Cache: Version 4.0.18 configure options:

Okay problems in alpha and beta code should be brought up in squid-dev
so the guys working on the changes can actually find out about problems.

"If you have any problems with a development release please write to our
... squid-dev at squid-cache.org lists. DO NOT write to squid-users with
code-related problems."


> '--enable-htcp' '--disable-maintainer-mode'
> '--disable-dependency-tracking' '--disable-wccp' '--disable-snmp'
> '--enable-inline' '--enable-async-io=32' '--enable-storeio=aufs,rock'
> '--enable-underscores' '--enable-removal-policies=lru,heap'
> '--enable-http-violations' '--disable-ident-lookups'
> '--with-large-files' '--enable-ssl' '--enable-ltdl-convenience'
> '--prefix=/usr' '--localstatedir=/var' '--libexecdir=/lib/squid'
> '--srcdir=.' '--datadir=/usr/share/squid' '--sysconfdir=/etc/squid'
> '--with-default-user=proxy' '--with-logdir=/var/log'
> '--with-pidfile=/var/run/squid.pid' '--with-filedescriptors=16384'
> '--with-aufs-threads=32' '--disable-translation'
> 
> My only */Suspicion/* is the different slot-size from kid7 (/cache). But
> these messages also appears on kid8 (/cache2), although  with much, MUCH
> less frequency..... (while rebuilding/restarting, always)
> 

I dont think that matters in this case. The slot size is a multiple (3x)
of the common disk I/O page size (4KB).

The sizes being mentioned by Squid are the amont of data stored in the
slot. The slot metadata says there are N bytes inside, the object
metadata says it is M bytes big ... " SIZE MISMATCH   M != N".

If "kill -9" or equivalent was used on kid7 during previous shutdown.
Then is will not be able to save all the slot updates it is probably
still waiting to write to disk. Resulting in what you see.

Amos



From test1964 at gmail.com  Sun Feb 19 10:22:26 2017
From: test1964 at gmail.com (Test1964)
Date: Sun, 19 Feb 2017 12:22:26 +0200
Subject: [squid-users] Squid 3.5.24 - Exclude https sites from ssl_bump in
	Transparent Mode
Message-ID: <32dc9f0e-c742-0028-70ac-96af0e963d4c@gmail.com>

Hi,

When I try to exclude some sites like Banks (or even gmail.com) for 
users using squid in TRANSPARENT Mode,
I get in Squid log : "SECURITY ALERT: On URL......." (all servers and 
users using same dns, so this not an issue).
My config file regard to this:

acl step1 at_step SslBump1
acl step2 at_step SslBump2
acl step3 at_step SslBump3

ssl_bump splice localhost


acl exclude_sites ssl::server_name "/etc/squid/exfiles.conf"

ssl_bump peek step1 all

ssl_bump splice exclude_sites
ssl_bump stare step2 all

ssl_bump all

* all users use fake ips (172.x.x.x)

Any ideas how to fix ?

Thanks Dan
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20170219/a5126541/attachment.htm>

From eliezer at ngtech.co.il  Sun Feb 19 11:12:57 2017
From: eliezer at ngtech.co.il (Eliezer  Croitoru)
Date: Sun, 19 Feb 2017 13:12:57 +0200
Subject: [squid-users] Squid 3.5.24 - Exclude https sites from ssl_bump
	in	Transparent Mode
In-Reply-To: <32dc9f0e-c742-0028-70ac-96af0e963d4c@gmail.com>
References: <32dc9f0e-c742-0028-70ac-96af0e963d4c@gmail.com>
Message-ID: <087601d28aa1$1fb21d20$5f165760$@ngtech.co.il>

What is the content of: /etc/squid/exfiles.conf

And did you tried using:
ssl::server_name_regex -i "/etc/squid/doms.nobump"

/etc/squid/doms.nobump:
##START OF FILE
update\.microsoft\.com$
update\.microsoft\.com\.akadns\.net$
v10\.vortex\-win\.data\.microsoft.com$
settings\-win\.data\.microsoft\.com$
##END OF FILE
etc?

Eliezer
----
http://ngtech.co.il/lmgtfy/
Linux System Administrator
Mobile: +972-5-28704261
Email: eliezer at ngtech.co.il


From: squid-users [mailto:squid-users-bounces at lists.squid-cache.org] On Behalf Of Test1964
Sent: Sunday, February 19, 2017 12:22 PM
To: squid-users at lists.squid-cache.org
Subject: [squid-users] Squid 3.5.24 - Exclude https sites from ssl_bump in Transparent Mode

Hi, 

When I try to exclude some sites like Banks (or even gmail.com) for users using squid in TRANSPARENT Mode,
I get in Squid log : "SECURITY ALERT: On URL......." (all servers and users using same dns, so this not an issue).
My config file regard to this:

acl step1 at_step SslBump1
acl step2 at_step SslBump2
acl step3 at_step SslBump3

ssl_bump splice localhost


acl exclude_sites ssl::server_name "/etc/squid/exfiles.conf"

ssl_bump peek step1 all

ssl_bump splice exclude_sites
ssl_bump stare step2 all

ssl_bump all

* all users use fake ips (172.x.x.x)

Any ideas how to fix ?

Thanks Dan



From oscar.segarra at gmail.com  Sun Feb 19 18:05:57 2017
From: oscar.segarra at gmail.com (Oscar Segarra)
Date: Sun, 19 Feb 2017 19:05:57 +0100
Subject: [squid-users] Proxyfy spice protocol behind nat
Message-ID: <CAJq8taEnhq79bpRw=Sw7E5c_Poa6XGZ6bori3PVRkjgo7XW59w@mail.gmail.com>

Hi,

In my environment I have deployed two KVM hypervisors. I'd like tu deploy
in my DMZ a squid proxy host in order to hide hypervisor IPs and Ports from
the clients.

Each virtual machine has a unique port but VMs can run on any hypervisor.

Is ist possible to achieve this with squid?
Is there any example how to configure this?

Any help will be wellcome!
Thanks a lot.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20170219/b0ec2dd7/attachment.htm>

From frio_cervesa at hotmail.com  Sun Feb 19 21:54:13 2017
From: frio_cervesa at hotmail.com (senor)
Date: Sun, 19 Feb 2017 21:54:13 +0000
Subject: [squid-users] On using Parent Proxies
In-Reply-To: <08a1ffa0-b56e-7912-bdb9-c4cb8dd00b4f@treenet.co.nz>
References: <AM2PR07MB0834789FB47353AD93CA780BAE400@AM2PR07MB0834.eurprd07.prod.outlook.com>
 <AM2PR07MB08343A6D45A789B543CD64A2AE400@AM2PR07MB0834.eurprd07.prod.outlook.com>
 <c5f15610-5d79-8728-318e-fffb0828f5eb@treenet.co.nz>
 <AM2PR07MB0834C838225511E0BFF361F7AE430@AM2PR07MB0834.eurprd07.prod.outlook.com>
 <08a1ffa0-b56e-7912-bdb9-c4cb8dd00b4f@treenet.co.nz>
Message-ID: <BN6PR17MB11408EBBC1EE24A7077AF6A3F75F0@BN6PR17MB1140.namprd17.prod.outlook.com>

Side note...
no_cache is not flagged as an error in 3.5.24.
If it is misconfigured you will get a notice though. It seems to be
treated as a legit alias.

Senor

On 2/7/2017 20:40, Amos Jeffries wrote:
> On 8/02/2017 12:50 a.m., Jude Karuhanga wrote:
>> Hello there,
>>
>>
>> Thanks for the reply. There seem to be no errors in the file (there
>> are none returned by "squid -k parse"). The service seems to be
>> working, especially after I included the external network on the
>> ACLs.
>>
> 
> Then your Squid needs an upgrade. The config file indicates it is for
> 3.5.20. But Squid-3 '-k parse' will absolutely complain when obsolete
> Squid-2 directives are passed to them. Your config file contains at
> least 3 such obsolete directives (no_cache).
> 
> It also uses QUERY ACL before defining what QUERY is.
> 
> 
>>
>> I was looking for a schematic explanation on how to configure my
>> network to effectively distribute traffic. I attach a scheme of the
>> setup (one central Server and two Gateways connected to Routers).
> 
> Please define "effectively distribute" ?
> 
> You asked how to do "load balancing". The URL I referenced already
> covers all the ways Squid can load balance. *Which* one is "effective"
> depends on what you want to happen.
> 
> Your config file shows someone halfway through testing whether a
> particular client->squid->proxy->Internet pathway works. Which is fine,
> actually a good way to go about the change. One step at a time.
> 
> For info on the pieces needed for creating cache hierarchies see the top
> two sections of <http://wiki.squid-cache.org/Features/CacheHierarchy>.
> 
> Amos
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
> 


From Antony.Stone at squid.open.source.it  Sun Feb 19 22:15:33 2017
From: Antony.Stone at squid.open.source.it (Antony Stone)
Date: Sun, 19 Feb 2017 23:15:33 +0100
Subject: [squid-users] Proxyfy spice protocol behind nat
In-Reply-To: <CAJq8taEnhq79bpRw=Sw7E5c_Poa6XGZ6bori3PVRkjgo7XW59w@mail.gmail.com>
References: <CAJq8taEnhq79bpRw=Sw7E5c_Poa6XGZ6bori3PVRkjgo7XW59w@mail.gmail.com>
Message-ID: <201702192315.33790.Antony.Stone@squid.open.source.it>

On Sunday 19 February 2017 at 19:05:57, Oscar Segarra wrote:

> Hi,
> 
> In my environment I have deployed two KVM hypervisors. I'd like to deploy
> in my DMZ a squid proxy host in order to hide hypervisor IPs and Ports from
> the clients.

Why?  What's the problem with the clients knowing the true values?

> Each virtual machine has a unique port but VMs can run on any hypervisor.

It doesn't sound to me like the VMs are actually part of what you're trying to 
do here?  You're just talking about client connections to hypervisors; the VMs 
are not part of that.

> Is it possible to achieve this with squid?

What protocol do the clients use to communicate with the KVM Hypervisors?

If it's HTTP, HTTPS or FTP, then you can probably configure Squid in 
accelerator mode and use it to do what you want.

However, why are you trying to do this?  What is the risk involved in the 
clients knowing the true IPs and ports of the hypervisors, which would be 
mitigated by having them connect via a proxy instead?

Have you considered using HAproxy or LVS, both of which are far more generic 
network proxies than Squid is?

> Is there any example how to configure this?

Not that I have ever heard of, however if it is a protocol which Squid can 
handle, it really doesn't matter what the specific backend system is; there are 
plenty of examples on how to do HTTP, HTTPS and FTP.



Antony.

-- 
Numerous psychological studies over the years have demonstrated that the 
majority of people genuinely believe they are not like the majority of people.

                                                   Please reply to the list;
                                                         please *don't* CC me.


From oscar.segarra at gmail.com  Sun Feb 19 22:48:42 2017
From: oscar.segarra at gmail.com (Oscar Segarra)
Date: Sun, 19 Feb 2017 23:48:42 +0100
Subject: [squid-users] Proxyfy spice protocol behind nat
In-Reply-To: <201702192315.33790.Antony.Stone@squid.open.source.it>
References: <CAJq8taEnhq79bpRw=Sw7E5c_Poa6XGZ6bori3PVRkjgo7XW59w@mail.gmail.com>
 <201702192315.33790.Antony.Stone@squid.open.source.it>
Message-ID: <CAJq8taGhxhmgNR0YQOBiLSz8SP+vg4S4AxWmLiRkU+NLzQyUrw@mail.gmail.com>

 Hi,
>
> In my environment I have deployed two KVM hypervisors. I'd like to deploy
> in my DMZ a squid proxy host in order to hide hypervisor IPs and Ports
from
> the clients.

Why?  What's the problem with the clients knowing the true values?

--> I want to publis VDI Desktops through Internet. If I have 10
hypervisors I don't want to publish 10 public IPs, I prefer just tu publish
a proxy server.

> Each virtual machine has a unique port but VMs can run on any hypervisor.

It doesn't sound to me like the VMs are actually part of what you're trying
to
do here?  You're just talking about client connections to hypervisors; the
VMs
are not part of that.

--> The hypervisor has a specific port for each VM. If you connect to the
hypervisor by that port, you are connecting directly to the virtual
machine. This is how SPICE works.

> Is it possible to achieve this with squid?

What protocol do the clients use to communicate with the KVM Hypervisors?

--> The protocol is SPICE (https://www.spice-space.org/)

If it's HTTP, HTTPS or FTP, then you can probably configure Squid in
accelerator mode and use it to do what you want.

However, why are you trying to do this?  What is the risk involved in the
clients knowing the true IPs and ports of the hypervisors, which would be
mitigated by having them connect via a proxy instead?

Have you considered using HAproxy or LVS, both of which are far more generic
network proxies than Squid is?

--> I have not considered it yet...

> Is there any example how to configure this?

Not that I have ever heard of, however if it is a protocol which Squid can
handle, it really doesn't matter what the specific backend system is; there
are
plenty of examples on how to do HTTP, HTTPS and FTP.

2017-02-19 23:15 GMT+01:00 Antony Stone <Antony.Stone at squid.open.source.it>:

> On Sunday 19 February 2017 at 19:05:57, Oscar Segarra wrote:
>
> > Hi,
> >
> > In my environment I have deployed two KVM hypervisors. I'd like to deploy
> > in my DMZ a squid proxy host in order to hide hypervisor IPs and Ports
> from
> > the clients.
>
> Why?  What's the problem with the clients knowing the true values?
>
> > Each virtual machine has a unique port but VMs can run on any hypervisor.
>
> It doesn't sound to me like the VMs are actually part of what you're
> trying to
> do here?  You're just talking about client connections to hypervisors; the
> VMs
> are not part of that.
>
> > Is it possible to achieve this with squid?
>
> What protocol do the clients use to communicate with the KVM Hypervisors?
>
> If it's HTTP, HTTPS or FTP, then you can probably configure Squid in
> accelerator mode and use it to do what you want.
>
> However, why are you trying to do this?  What is the risk involved in the
> clients knowing the true IPs and ports of the hypervisors, which would be
> mitigated by having them connect via a proxy instead?
>
> Have you considered using HAproxy or LVS, both of which are far more
> generic
> network proxies than Squid is?
>
> > Is there any example how to configure this?
>
> Not that I have ever heard of, however if it is a protocol which Squid can
> handle, it really doesn't matter what the specific backend system is;
> there are
> plenty of examples on how to do HTTP, HTTPS and FTP.
>
>
>
> Antony.
>
> --
> Numerous psychological studies over the years have demonstrated that the
> majority of people genuinely believe they are not like the majority of
> people.
>
>                                                    Please reply to the
> list;
>                                                          please *don't* CC
> me.
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20170219/bfb30dec/attachment.htm>

From test1964 at gmail.com  Mon Feb 20 07:33:12 2017
From: test1964 at gmail.com (Test1964)
Date: Mon, 20 Feb 2017 09:33:12 +0200
Subject: [squid-users] Squid 3.5.24 - Url_rewrite with ssl_bump in
	Transparent Mode
Message-ID: <ba934f59-1cfe-617e-aa3b-9c3be60730ce@gmail.com>

Hi,

When I exclude some sites (like banks)  with ssl_bump peek/splice that 
works well, Got a new problem that
sites (that I exclude)  can not be blocked using Url_Rewrite.
I use Url_rewrite to block sites based on User IP and all all other 
sites(no in exclude list) it working very well.

How to fix it? Or this another way to block excluded sites in ssl_bump 
based on User IP?

Thanks Dan.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20170220/31135057/attachment.htm>

From damir.k.bagapov at gmail.com  Mon Feb 20 21:05:36 2017
From: damir.k.bagapov at gmail.com (Damir Bagapov)
Date: Mon, 20 Feb 2017 23:05:36 +0200
Subject: [squid-users] Configure squid 3.5 to use different parent proxies
 for different listening ports
Message-ID: <CAM1T=octjBG4u0sSKU9_LH8ZQabffywkBHCr6-8E7eMeHCKxpg@mail.gmail.com>

Hi,

I have following configuration:

# Squid normally listens to port 3128
http_port 3128

cache_peer proxy1_address parent proxy1_port 0 proxy-only default
login=name1:pass1
never_direct allow all

And I need to configure squid in a way when all incoming requests to 3128
port will be redirected to proxy1 (as it works now), and all incoming
requests to 3127 will be redirected to proxy2. Is it possible to do?

I've tried to use ACL described here http://wiki.squid-cache.o
rg/ConfigExamples/Reverse/MultipleWebservers#Other_Criteria_than_Domain

http_port 3128
http_port 3127

acl port_3128 port 3128
acl port_3127 port 3127

# 3128
cache_peer proxy01 parent 3128 0 no-query originserver name=proxy3128
cache_peer_access proxy3128 allow port_3128
cache_peer_access proxy3128 deny port_3127

# 3127
cache_peer proxy02 parent 3128 0 no-query originserver name=proxy3127
cache_peer_access proxy3127 allow port_3127
cache_peer_access proxy3127 deny port_3128

But its not working =(

-- 
Best regards,
Damir
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20170220/ad577dba/attachment.htm>

From damir.k.bagapov at gmail.com  Mon Feb 20 21:08:15 2017
From: damir.k.bagapov at gmail.com (Damir Bagapov)
Date: Mon, 20 Feb 2017 23:08:15 +0200
Subject: [squid-users] Configure squid 3.5 to use different parent proxies
 for different listening ports
Message-ID: <CAM1T=odMdEtNd8ts0iQ4v6+peiRSGM6u41WvQ5obCUex5emaBw@mail.gmail.com>

Hi,

I have following configuration:

# Squid normally listens to port 3128
http_port 3128

cache_peer proxy1_address parent proxy1_port 0 proxy-only default
login=name1:pass1
never_direct allow all

And I need to configure squid in a way when all incoming requests to 3128
port will be redirected to proxy1 (as it works now), and all incoming
requests to 3127 will be redirected to proxy2. Is it possible to do?

I've tried to use ACL described here http://wiki.squid-cache.
org/ConfigExamples/Reverse/MultipleWebservers#Other_Criteria_than_Domain

http_port 3128
http_port 3127

acl port_3128 port 3128
acl port_3127 port 3127

# 3128
cache_peer proxy01 parent 3128 0 no-query originserver name=proxy3128
cache_peer_access proxy3128 allow port_3128
cache_peer_access proxy3128 deny port_3127

# 3127
cache_peer proxy02 parent 3128 0 no-query originserver name=proxy3127
cache_peer_access proxy3127 allow port_3127
cache_peer_access proxy3127 deny port_3128

But its not working =(

-- 
Best regards,
Damir
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20170220/a4edc058/attachment.htm>

From eliezer at ngtech.co.il  Mon Feb 20 21:20:20 2017
From: eliezer at ngtech.co.il (Eliezer  Croitoru)
Date: Mon, 20 Feb 2017 23:20:20 +0200
Subject: [squid-users] Configure squid 3.5 to use different parent
	proxies for different listening ports
In-Reply-To: <CAM1T=octjBG4u0sSKU9_LH8ZQabffywkBHCr6-8E7eMeHCKxpg@mail.gmail.com>
References: <CAM1T=octjBG4u0sSKU9_LH8ZQabffywkBHCr6-8E7eMeHCKxpg@mail.gmail.com>
Message-ID: <1233c01d28bbf$2780c2d0$76824870$@ngtech.co.il>

You can try  the next:
http_port 3128 name=one
http_port 3127 name=two

acl port_3128 myportname one
acl port_3127 myportname two

always_direct deny port_3128
always_direct deny port_3127

never_direct allow port_3128
never_direct allow port_3127

# 3128
cache_peer proxy01 parent 3128 0 no-query originserver name=proxy3128
cache_peer_access proxy3128 allow port_3128
cache_peer_access proxy3128 deny all

# 3127 
cache_peer proxy02 parent 3128 0 no-query originserver name=proxy3127
cache_peer_access proxy3127 allow port_3127
cache_peer_access proxy3127 deny all
##END

And see if it helps with something.

Eliezer

----
http://ngtech.co.il/lmgtfy/
Linux System Administrator
Mobile: +972-5-28704261
Email: eliezer at ngtech.co.il


From: squid-users [mailto:squid-users-bounces at lists.squid-cache.org] On Behalf Of Damir Bagapov
Sent: Monday, February 20, 2017 11:06 PM
To: squid-users at lists.squid-cache.org
Subject: [squid-users] Configure squid 3.5 to use different parent proxies for different listening ports

Hi,

I have following configuration:

# Squid normally listens to port 3128
http_port 3128

cache_peer proxy1_address parent proxy1_port 0 proxy-only default login=name1:pass1
never_direct allow all

And I need to configure squid in a way when all incoming requests to 3128 port will be redirected to proxy1 (as it works now), and all incoming requests to 3127 will be redirected to proxy2. Is it possible to do?

I've tried to use ACL described here http://wiki.squid-cache.org/ConfigExamples/Reverse/MultipleWebservers#Other_Criteria_than_Domain

http_port 3128
http_port 3127

acl port_3128 port 3128
acl port_3127 port 3127

# 3128
cache_peer proxy01 parent 3128 0 no-query originserver name=proxy3128
cache_peer_access proxy3128 allow port_3128
cache_peer_access proxy3128 deny port_3127

# 3127 
cache_peer proxy02 parent 3128 0 no-query originserver name=proxy3127
cache_peer_access proxy3127 allow port_3127
cache_peer_access proxy3127 deny port_3128

But its not working =(

-- 
Best regards,
Damir



From eliezer at ngtech.co.il  Mon Feb 20 21:21:27 2017
From: eliezer at ngtech.co.il (Eliezer  Croitoru)
Date: Mon, 20 Feb 2017 23:21:27 +0200
Subject: [squid-users] Configure squid 3.5 to use different
	parent	proxies for different listening ports
In-Reply-To: <1233c01d28bbf$2780c2d0$76824870$@ngtech.co.il>
References: <CAM1T=octjBG4u0sSKU9_LH8ZQabffywkBHCr6-8E7eMeHCKxpg@mail.gmail.com>
 <1233c01d28bbf$2780c2d0$76824870$@ngtech.co.il>
Message-ID: <1233e01d28bbf$4bdf1050$e39d30f0$@ngtech.co.il>

And forgot to add also:
nonhierarchical_direct off

----
Eliezer Croitoru
Linux System Administrator
Mobile: +972-5-28704261
Email: eliezer at ngtech.co.il


-----Original Message-----
From: squid-users [mailto:squid-users-bounces at lists.squid-cache.org] On Behalf Of Eliezer Croitoru
Sent: Monday, February 20, 2017 11:20 PM
To: 'Damir Bagapov' <damir.k.bagapov at gmail.com>; squid-users at lists.squid-cache.org
Subject: Re: [squid-users] Configure squid 3.5 to use different parent proxies for different listening ports

You can try  the next:
http_port 3128 name=one
http_port 3127 name=two

acl port_3128 myportname one
acl port_3127 myportname two

always_direct deny port_3128
always_direct deny port_3127

never_direct allow port_3128
never_direct allow port_3127

# 3128
cache_peer proxy01 parent 3128 0 no-query originserver name=proxy3128
cache_peer_access proxy3128 allow port_3128
cache_peer_access proxy3128 deny all

# 3127 
cache_peer proxy02 parent 3128 0 no-query originserver name=proxy3127
cache_peer_access proxy3127 allow port_3127
cache_peer_access proxy3127 deny all
##END

And see if it helps with something.

Eliezer

----
http://ngtech.co.il/lmgtfy/
Linux System Administrator
Mobile: +972-5-28704261
Email: eliezer at ngtech.co.il


From: squid-users [mailto:squid-users-bounces at lists.squid-cache.org] On Behalf Of Damir Bagapov
Sent: Monday, February 20, 2017 11:06 PM
To: squid-users at lists.squid-cache.org
Subject: [squid-users] Configure squid 3.5 to use different parent proxies for different listening ports

Hi,

I have following configuration:

# Squid normally listens to port 3128
http_port 3128

cache_peer proxy1_address parent proxy1_port 0 proxy-only default login=name1:pass1
never_direct allow all

And I need to configure squid in a way when all incoming requests to 3128 port will be redirected to proxy1 (as it works now), and all incoming requests to 3127 will be redirected to proxy2. Is it possible to do?

I've tried to use ACL described here http://wiki.squid-cache.org/ConfigExamples/Reverse/MultipleWebservers#Other_Criteria_than_Domain

http_port 3128
http_port 3127

acl port_3128 port 3128
acl port_3127 port 3127

# 3128
cache_peer proxy01 parent 3128 0 no-query originserver name=proxy3128
cache_peer_access proxy3128 allow port_3128
cache_peer_access proxy3128 deny port_3127

# 3127 
cache_peer proxy02 parent 3128 0 no-query originserver name=proxy3127
cache_peer_access proxy3127 allow port_3127
cache_peer_access proxy3127 deny port_3128

But its not working =(

-- 
Best regards,
Damir

_______________________________________________
squid-users mailing list
squid-users at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-users



From eliezer at ngtech.co.il  Mon Feb 20 21:25:43 2017
From: eliezer at ngtech.co.il (Eliezer  Croitoru)
Date: Mon, 20 Feb 2017 23:25:43 +0200
Subject: [squid-users] Squid on separate box and it can't see packets
In-Reply-To: <ba219e64-ef04-3f50-c22d-24440256d525@treenet.co.nz>
References: <CAKNtY_x5YX-LAhy30Q3n1aeZ_XbeKUf7YyUd8WFKdWYkr9K_Mg@mail.gmail.com>
 <0fea01d2868d$d6fab130$84f01390$@ngtech.co.il>
 <CAKNtY_za61AyNijVgLv=+M4tW8JeWgHQVNwvs1pJjfU62vCWTw@mail.gmail.com>
 <ba219e64-ef04-3f50-c22d-24440256d525@treenet.co.nz>
Message-ID: <1234001d28bbf$e4714a90$ad53dfb0$@ngtech.co.il>

And just wanted to add a note that some Linux machines will act as an HUB\BRIDGE by default in a similar scenario(will not drop packets..).
I noticed it while working on some tiny lab and it's better to have the linux machine with ipv4_forward turned on with an iptables DROP rule rather then without(with some distros and some specific kernels).

Eliezer

----
Eliezer Croitoru
Linux System Administrator
Mobile: +972-5-28704261
Email: eliezer at ngtech.co.il


-----Original Message-----
From: squid-users [mailto:squid-users-bounces at lists.squid-cache.org] On Behalf Of Amos Jeffries
Sent: Friday, February 17, 2017 3:59 PM
To: squid-users at lists.squid-cache.org
Subject: Re: [squid-users] Squid on separate box and it can't see packets

On 15/02/2017 9:18 a.m., John Pearson wrote:
> Hi,
> 
> Is this squid box a router or just a proxy?
> - just a proxy

There is the first problem.

NAT interception needs the machine Squid is running on to be configured
to operate as a router. It will be receiving packets destined to a
machine other than itself.

> 
> What tcpdump command did you ran?
> - sudo tcpdump -i eth0
> 
> What is the networks that are involved?
> Setup:
> 
>> Client        (192.168.1.8) --->  |     Rotuer        |
>>                                                | gateway/dhcp | --->
>> Internet
>> Squid box (192.168.1.2) --->  |  192.168.1.1   |
> 
> 
> Here Client (debian), squid (debian) and router are three separate devices.
> 

So the Squid machine;

requires this bit you did:
 <http://wiki.squid-cache.org/ConfigExamples/Intercept/LinuxRedirect>

PLUS the system TCP stack controls to turn it from a origin-server host
to a routing host. Otherwise the machine will silently drop packets not
destined to itself.


The router machine requires this:
 <http://wiki.squid-cache.org/ConfigExamples/Intercept/IptablesPolicyRoute#When_Squid_is_Internal_amongst_clients>

The router machine probably also needs the "Routing Setup":
 <http://wiki.squid-cache.org/ConfigExamples/Intercept/IptablesPolicyRoute#Routing_Setup>

Amos

_______________________________________________
squid-users mailing list
squid-users at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-users



From rousskov at measurement-factory.com  Mon Feb 20 21:26:47 2017
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Mon, 20 Feb 2017 14:26:47 -0700
Subject: [squid-users] Configure squid 3.5 to use different parent
 proxies for different listening ports
In-Reply-To: <CAM1T=odMdEtNd8ts0iQ4v6+peiRSGM6u41WvQ5obCUex5emaBw@mail.gmail.com>
References: <CAM1T=odMdEtNd8ts0iQ4v6+peiRSGM6u41WvQ5obCUex5emaBw@mail.gmail.com>
Message-ID: <8ee5d018-ef7c-35fe-53dd-b451c762cfe5@measurement-factory.com>

On 02/20/2017 02:08 PM, Damir Bagapov wrote:

> I need to configure squid in a way when all incoming requests to
> 3128 port will be redirected to proxy1 (as it works now), and all
> incoming requests to 3127 will be redirected to proxy2.

> http_port 3128
> http_port 3127
> 
> acl port_3128 port 3128
> acl port_3127 port 3127
> 
> # 3128
> cache_peer proxy01 parent 3128 0 no-query originserver name=proxy3128
> cache_peer_access proxy3128 allow port_3128
> cache_peer_access proxy3128 deny port_3127
> 
> # 3127 
> cache_peer proxy02 parent 3128 0 no-query originserver name=proxy3127
> cache_peer_access proxy3127 allow port_3127
> cache_peer_access proxy3127 deny port_3128


You probably want to use the "myport" ACLs instead of the "port" ACLs
because you route based on the Squid listening port, not the origin
server listening port.

I hope you do not need those "deny" rules for cache_peer_access
directives because the default -- reverse the last action [for the same
peer] -- should work fine. If you want or need to have explicit deny
rules, use "deny all" to clarify the intent and avoid the "what if none
of the cache_peer_access rules match for a peer?" concerns.


HTH,

Alex.



From eliezer at ngtech.co.il  Mon Feb 20 21:44:15 2017
From: eliezer at ngtech.co.il (Eliezer  Croitoru)
Date: Mon, 20 Feb 2017 23:44:15 +0200
Subject: [squid-users] question about : NOTICE: Authentication
	not	applicable onintercepted requests.
In-Reply-To: <vmime.58a4251d.54d0.118595cb592d426d@ms249-lin-003.rotterdam.bazuin.nl>
References: <vmime.58a4251d.54d0.118595cb592d426d@ms249-lin-003.rotterdam.bazuin.nl>
Message-ID: <1234501d28bc2$7b81b4e0$72851ea0$@ngtech.co.il>

Hey,

What you see is not a misconfiguration in the general meaning.
Squid and any other proxy cannot authenticate without some kind of special
tricks on an Intercept mode and port.
There are products which offers transparently to hijack the web traffic and
authenticate with the windows credentials to the proxy.
These agents uses some proxy but the interception does on the client side
while the connection to the proxy is similar to a regular one(non
intercept).
There are technical options to "mark" connections or requests in some levels
that will satisfy your needs but needs to be built or published but I have
yet to see one of these.
One solution which I have seen does something that is close to such a thing
is "proxifier" [ https://www.proxifier.com].
I have seen couple other Chinese developments which are doing something
similar but yet to sit on their code to say I understand what they do.
Take a peek at
https://www.raymond.cc/blog/route-all-internet-software-and-game-connection-
through-open-proxy-servers/
This is the next best option for transparently hijack connections:
https://github.com/ambrop72/badvpn

Hope it Helps,
Eliezer 

----
http://ngtech.co.il/lmgtfy/
Linux System Administrator
Mobile: +972-5-28704261
Email: eliezer at ngtech.co.il


From: squid-users [mailto:squid-users-bounces at lists.squid-cache.org] On
Behalf Of L.P.H. van Belle
Sent: Wednesday, February 15, 2017 11:54 AM
To: squid-users at squid-cache.org
Subject: [squid-users] question about : NOTICE: Authentication not
applicable onintercepted requests.

Hai, 

In configuring my debian jessie with squid 3.5.24 ( with ssl enabled )
?c-icap squidclamav and winbind 4.5.5 for kerberos keytab refresing. 

Now, im at the point of reducing my logs and i nocited : 
NOTICE: Authentication not applicable on intercepted requests. 
Messages in squid/cache.log 

I know this is some misconfiguration somewhere but im having a hardtime to
finding/understanding it. 
Where and why, so is anyone can help me finding and understanding it, that
would be very nice. 

I cant see my error and everything else is working fine, execept i havent
tested the kerberos group acl yet. 
So i didnt set that http_access yet. 

Im having the following firewall rules 

# Not authenticated web traffice, redirected to squid in intercept mode.
-A PREROUTING -p tcp -i eth0 --dport 80 -j DNAT --to-destination
192.168.0.2:3128
-A PREROUTING -p tcp -i eth0 --dport 443 -j DNAT --to-destination
192.168.0.2:3129
Port 8080 is also open. 

Web traffic for pc?s which are domain joint have set the proxy by GPO to
hostname.domain.tld port 8080 
Web traffic for other devices dont need to authenticate. 
WPAD and DNS wpad is also set. 

Below is mostly from the updated wiki pages. 
A big thank you to Amos Victor and others who changed the pages, looks good.
I have some small changed for a pure debian based setup with samba4 as addc
and winbind for the squid member server. 


This is my squid config. 
# Created from a running squid version : 3.5.24
# Running os : Debian GNU/Linux 8 (jessie)
# Creation date: 2017-02-15

auth_param negotiate program /usr/lib/squid/negotiate_wrapper_auth
--kerberos /usr/lib/squid/negotiate_kerberos_auth -s
mailto:HTTP/proxy2.internal.domain.tld at INTERNAL.DOMAIN.TLD --ntlm
/usr/bin/ntlm_auth --helper-protocol=gss-spnego --domain=NTDOM
auth_param negotiate children 10 startup=5 idle=5
auth_param negotiate keep_alive on
external_acl_type memberof ttl=3600 negative_ttl=3600 %LOGIN
/usr/lib/squid3/ext_kerberos_ldap_group_acl -d -i -m 4 -g
mailto:internet-allowed at INTERNAL.DOMAIN.TLD -N
mailto:NTDOM at INTERNAL.DOMAIN.TLD -S
mailto:dc1.internal.domain.tld at INTERNAL.DOMAIN.TLD -D INTERNAL.DOMAIN.TLD
acl authenticated proxy_auth REQUIRED

acl certificates rep_mime_type -i ^application/pkix-crl$

acl windows-updates dstdomain "/etc/squid/lists/updates-windows"
acl antivirus-updates dstdomain "/etc/squid/lists/updates-antivirus"
acl localnet src fc00::/7?????? # RFC 4193 local private network range
acl localnet src fe80::/10????? # RFC 4291 link-local (directly plugged)
machines
acl localnet src 192.168.249.0/24??? # Company-1
acl localnet src 10.249.2.0/24?????? # Company-2
acl localnet src 10.249.3.0/24?????? # Company-3
acl localnet src 10.249.4.0/24?????? # Company-4
acl localnet src 10.249.5.0/24???? ??# Company-5

acl SSL_ports port 443????????? # https
acl SSL_ports port 3952???????? # CIC client
acl SSL_ports port 10443??????? # https Cisco 5506x
acl Safe_ports port 80????????? # http
acl Safe_ports port 21????????? # ftp
acl Safe_ports port 443???????? # https
acl Safe_ports port 70????????? # gopher
acl Safe_ports port 210???????? # wais
acl Safe_ports port 1025-65535? # unregistered ports
acl Safe_ports port 280???????? # http-mgmt
acl Safe_ports port 488???????? # gss-http
acl Safe_ports port 591???????? # filemaker
acl Safe_ports port 777???????? # multiling http
acl Safe_ports port 3952??????? # CIC client
acl Safe_ports port 10443?????? # https Cisco 5506x
acl CONNECT method CONNECT

## Added : Advertising Server Block List merge from YoYo.org and
Host-file.net
acl block-asbl dstdomain "/etc/squid/lists/block-asbl-merged-dstdomain"
http_access deny block-asbl

acl google_recaptcha urlpath_regex ^\/recaptcha\/api.js
http_access allow google_recaptcha

acl NO-CACHE-SITES url_regex "/etc/squid/lists/no-cache-sites"
no_cache deny NO-CACHE-SITES
always_direct allow NO-CACHE-SITES
cache deny NO-CACHE-SITES

# 
http_access deny !Safe_ports
http_access deny CONNECT !SSL_ports
http_access allow localhost manager
http_access deny manager
http_access deny to_localhost

## allow before auth so all pc's get the needed updates
http_access allow windows-updates
http_access allow antivirus-updates

http_access allow authenticated
http_access allow localnet
http_access allow localhost
http_access deny all

http_port 192.168.249.222:3128 intercept connection-auth=off
https_port 192.168.249.222:3129 intercept connection-auth=off ssl-bump
generate-host-certificates=on dynamic_cert_mem_cache_size=4MB
cert=/etc/ssl/local/CAcert.pem options=NO_SSLv3 key=/etc/ssl/local/CAkey.pem

http_port 192.168.249.222:8080 ssl-bump generate-host-certificates=on
dynamic_cert_mem_cache_size=4MB cert=/etc/ssl/local/CAcert.pem
options=NO_SSLv3 key=/etc/ssl/local/CAkey.pem
sslcrtd_program /usr/lib/squid/ssl_crtd -s /var/lib/ssl_db -M 8MB
acl step1 at_step SslBump1
ssl_bump peek step1
ssl_bump bump all
sslproxy_options NO_SSLv2,NO_SSLv3,SINGLE_DH_USE
sslproxy_cipher
EECDH+ECDSA+AESGCM:EECDH+aRSA+AESGCM:EECDH+ECDSA+SHA384:EECDH+ECDSA+SHA256:E
ECDH+aRSA+SHA384:EECDH+aRSA+SHA256:EECDH+aRSA+RC4:EECDH:EDH+aRSA:!RC4:!aNULL
:!eNULL:!LOW:!3DES:!MD5:!EXP:!PSK:!SRP:!DSS
cache_mem 4096 MB
coredump_dir /var/spool/squid
ftp_user mailto:anonymousftp at domain.tld

# 
refresh_pattern -i
windowsupdate.com/.*\.(cab|exe|ms[i|u|f|p]|[ap]sf|wm[v|a]|dat|zip|psf) 43200
80% 129600 reload-into-ims
refresh_pattern -i
microsoft.com/.*\.(cab|exe|ms[i|u|f|p]|[ap]sf|wm[v|a]|dat|zip|psf) 43200 80%
129600 reload-into-ims
refresh_pattern -i
windows.com/.*\.(cab|exe|ms[i|u|f|p]|[ap]sf|wm[v|a]|dat|zip|psf) 43200 80%
129600 reload-into-ims
refresh_pattern -i
microsoft.com.akadns.net/.*\.(cab|exe|ms[i|u|f|p]|[ap]sf|wm[v|a]|dat|zip|psf
) 43200 80% 129600 reload-into-ims
refresh_pattern -i
deploy.akamaitechnologies.com/.*\.(cab|exe|ms[i|u|f|p]|[ap]sf|wm[v|a]|dat|zi
p|psf) 43200 80% 129600 reload-into-ims

## todo, make this list more complete, see icap excludes
refresh_pattern -i
\.symantecliveupdate\.com\/.*\.(zip|7z|irn|[m|x][0-9][0-9])????????? 4320???
100%??? 43200?? reload-into-ims
refresh_pattern -i
.*dnl.*\.geo\.kaspersky\.(com|ru)\/.*\.(zip|avc|kdc|nhg|klz|d[at|if])???????
4320??? 100%??? 43200?? reload-into-ims
refresh_pattern -i
\.kaspersky-labs\.(com|ru)\/.*\.(cab|zip|exe|ms[i|p])??????? 4320??? 100%???
43200?? reload-into-ims
refresh_pattern -i \.kaspersky\.(com|ru)\/.*\.(cab|zip|exe|ms[i|p]|avc)
4320??? 100%??? 43200?? reload-into-ims
refresh_pattern -i .update\.geo\.drweb\.com???? 4320??? 100%??? 43200??
reload-into-ims
refresh_pattern -i \.avast.com\/.*\.(vp[u|aa])? ????????4320??? 100%???
43200?? reload-into-ims
refresh_pattern -i \.avg.com\/.*\.(bin)???????? 4320??? 100%??? 43200??
reload-into-ims

## todo, add .deb files caching
refresh_pattern ^(ht|f)tp://.*debian.*/Packages\.(bz2|gz|diff/Index)$??
0?????? 0%????? 0
refresh_pattern ^(ht|f)tp://.*debian.*/Release(\.gpg)?$????????????????
0?????? 0%????? 0
refresh_pattern ^(ht|f)tp://.*debian.*/Sources\.(bz2|gz|diff/Index)$???
0?????? 0%????? 0
refresh_pattern ^(ht|f)tp://.*debian.*/Translation-en_GB\.bz2)$????????
0?????? 0%????? 0

## The defaults as last.
refresh_pattern -i \.(zip|[g|b]z2?|exe|ms[i|p]|cvd|cdiff|mar)$? 43200??
100%??? 129600? reload-into-ims
refresh_pattern ^ftp:?????????? 1440??? 20%???? 10080
refresh_pattern ^gopher:??????? 1440??? 0%????? 1440
refresh_pattern -i (/cgi-bin/|\?) 0???? 0%????? 0
refresh_pattern .?????????????? 0?????? 20%???? 4320
cache_mgr mailto:changed2protectme at somedomain.tld
mail_from mailto:proxy2 at internal.domain.tld
visible_hostname proxy2.internal.domain.tld
hostname_aliases proxy2.internal.domain.tld

httpd_suppress_version_string on

icap_enable on
icap_send_client_ip on
icap_send_client_username on
icap_client_username_header X-Authenticated-User
icap_persistent_connections on
icap_preview_enable on
icap_preview_size 1024
icap_service service_req reqmod_precache icap://127.0.0.1:1344/squidclamav
bypass=off
adaptation_access service_req allow all
icap_service service_resp respmod_precache icap://127.0.0.1:1344/squidclamav
bypass=off
adaptation_access service_resp allow all

dns_v4_first on
maximum_object_size 4096 KB
minimum_object_size 0 KB
maximum_object_size_in_memory 64 KB
cache_mem 256 MB
quick_abort_min -1 KB
fqdncache_size 4096
cache_swap_low 90
cache_swap_high 95




From damir.k.bagapov at gmail.com  Mon Feb 20 22:06:36 2017
From: damir.k.bagapov at gmail.com (Damir Bagapov)
Date: Tue, 21 Feb 2017 00:06:36 +0200
Subject: [squid-users] Configure squid 3.5 to use different parent
 proxies for different listening ports
In-Reply-To: <8ee5d018-ef7c-35fe-53dd-b451c762cfe5@measurement-factory.com>
References: <CAM1T=odMdEtNd8ts0iQ4v6+peiRSGM6u41WvQ5obCUex5emaBw@mail.gmail.com>
 <8ee5d018-ef7c-35fe-53dd-b451c762cfe5@measurement-factory.com>
Message-ID: <CAM1T=odBzacKD4Age7At=TJgDiqYtwcag=DZr33064rYo6a3QA@mail.gmail.com>

Thanks for help, finally it seems to be working with following configuration

http_port 3128 name=port_3128
http_port 3127 name=port_3127

nonhierarchical_direct off

acl port_3128_acl myportname port_3128
acl port_3127_acl myportname port_3127

always_direct deny port_3128_acl
always_direct deny port_3127_acl

never_direct allow port_3128_acl
never_direct allow port_3127_acl

# 3128
cache_peer proxy1 parent 3128 0 proxy-only default  name=proxy3128
cache_peer_access proxy3128 allow port_3128_acl
cache_peer_access proxy3128 deny all

# 3127
cache_peer proxy2 parent 3128 0 proxy-only default  name=proxy3127
cache_peer_access proxy3127 allow port_3127_acl
cache_peer_access proxy3127 deny all

For some reason 'no-query originserver' version didn't work, so I had to
change it back to 'proxy-only default'

2017-02-20 23:26 GMT+02:00 Alex Rousskov <rousskov at measurement-factory.com>:

> On 02/20/2017 02:08 PM, Damir Bagapov wrote:
>
> > I need to configure squid in a way when all incoming requests to
> > 3128 port will be redirected to proxy1 (as it works now), and all
> > incoming requests to 3127 will be redirected to proxy2.
>
> > http_port 3128
> > http_port 3127
> >
> > acl port_3128 port 3128
> > acl port_3127 port 3127
> >
> > # 3128
> > cache_peer proxy01 parent 3128 0 no-query originserver name=proxy3128
> > cache_peer_access proxy3128 allow port_3128
> > cache_peer_access proxy3128 deny port_3127
> >
> > # 3127
> > cache_peer proxy02 parent 3128 0 no-query originserver name=proxy3127
> > cache_peer_access proxy3127 allow port_3127
> > cache_peer_access proxy3127 deny port_3128
>
>
> You probably want to use the "myport" ACLs instead of the "port" ACLs
> because you route based on the Squid listening port, not the origin
> server listening port.
>
> I hope you do not need those "deny" rules for cache_peer_access
> directives because the default -- reverse the last action [for the same
> peer] -- should work fine. If you want or need to have explicit deny
> rules, use "deny all" to clarify the intent and avoid the "what if none
> of the cache_peer_access rules match for a peer?" concerns.
>
>
> HTH,
>
> Alex.
>
>


-- 
Best regards,
Damir
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20170221/37ebbf71/attachment.htm>

From squid3 at treenet.co.nz  Tue Feb 21 04:06:54 2017
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Tue, 21 Feb 2017 17:06:54 +1300
Subject: [squid-users] Squid on separate box and it can't see packets
In-Reply-To: <1234001d28bbf$e4714a90$ad53dfb0$@ngtech.co.il>
References: <CAKNtY_x5YX-LAhy30Q3n1aeZ_XbeKUf7YyUd8WFKdWYkr9K_Mg@mail.gmail.com>
 <0fea01d2868d$d6fab130$84f01390$@ngtech.co.il>
 <CAKNtY_za61AyNijVgLv=+M4tW8JeWgHQVNwvs1pJjfU62vCWTw@mail.gmail.com>
 <ba219e64-ef04-3f50-c22d-24440256d525@treenet.co.nz>
 <1234001d28bbf$e4714a90$ad53dfb0$@ngtech.co.il>
Message-ID: <de1a9318-afe0-5199-ebfd-b760d619a41b@treenet.co.nz>

On 21/02/2017 10:25 a.m., Eliezer  Croitoru wrote:
> And just wanted to add a note that some Linux machines will act as an
> HUB\BRIDGE by default in a similar scenario(will not drop
> packets..). I noticed it while working on some tiny lab and it's
> better to have the linux machine with ipv4_forward turned on with an
> iptables DROP rule rather then without(with some distros and some
> specific kernels).

Nod.

If the machine is working as a true bridge then the packets will not be
going to Squid. It still needs the routing rules to route the packets
from its bridge interface to Squid, and from Squid to its bridge
outerface. Or for that matter to pass them from the bridge
inter/outerfaces and the NAT system.

Amos



> 
> Eliezer
> ----
> Eliezer Croitoru
> Linux System Administrator
> Mobile: +972-5-28704261
> Email: eliezer at ngtech.co.il
> 
> 
> -----Original Message-----
> From: squid-users [mailto:squid-users-bounces at lists.squid-cache.org] On Behalf Of Amos Jeffries
> Sent: Friday, February 17, 2017 3:59 PM
> To: squid-users at lists.squid-cache.org
> Subject: Re: [squid-users] Squid on separate box and it can't see packets
> 
> On 15/02/2017 9:18 a.m., John Pearson wrote:
>> Hi,
>>
>> Is this squid box a router or just a proxy?
>> - just a proxy
> 
> There is the first problem.
> 
> NAT interception needs the machine Squid is running on to be configured
> to operate as a router. It will be receiving packets destined to a
> machine other than itself.
> 
>>
>> What tcpdump command did you ran?
>> - sudo tcpdump -i eth0
>>
>> What is the networks that are involved?
>> Setup:
>>
>>> Client        (192.168.1.8) --->  |     Rotuer        |
>>>                                                | gateway/dhcp | --->
>>> Internet
>>> Squid box (192.168.1.2) --->  |  192.168.1.1   |
>>
>>
>> Here Client (debian), squid (debian) and router are three separate devices.
>>
> 
> So the Squid machine;
> 
> requires this bit you did:
>  <http://wiki.squid-cache.org/ConfigExamples/Intercept/LinuxRedirect>
> 
> PLUS the system TCP stack controls to turn it from a origin-server host
> to a routing host. Otherwise the machine will silently drop packets not
> destined to itself.
> 
> 
> The router machine requires this:
>  <http://wiki.squid-cache.org/ConfigExamples/Intercept/IptablesPolicyRoute#When_Squid_is_Internal_amongst_clients>
> 
> The router machine probably also needs the "Routing Setup":
>  <http://wiki.squid-cache.org/ConfigExamples/Intercept/IptablesPolicyRoute#Routing_Setup>
> 
> Amos
> 
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
> 



From squid3 at treenet.co.nz  Tue Feb 21 04:21:54 2017
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Tue, 21 Feb 2017 17:21:54 +1300
Subject: [squid-users] Configure squid 3.5 to use different parent
 proxies for different listening ports
In-Reply-To: <CAM1T=odBzacKD4Age7At=TJgDiqYtwcag=DZr33064rYo6a3QA@mail.gmail.com>
References: <CAM1T=odMdEtNd8ts0iQ4v6+peiRSGM6u41WvQ5obCUex5emaBw@mail.gmail.com>
 <8ee5d018-ef7c-35fe-53dd-b451c762cfe5@measurement-factory.com>
 <CAM1T=odBzacKD4Age7At=TJgDiqYtwcag=DZr33064rYo6a3QA@mail.gmail.com>
Message-ID: <d9ebd65e-c670-0afc-ff0b-b7a598f3959c@treenet.co.nz>

On 21/02/2017 11:06 a.m., Damir Bagapov wrote:
> 
> For some reason 'no-query originserver' version didn't work, so I had to
> change it back to 'proxy-only default'

HTTP has multiple different URI syntaxes for talking to proxies and
origin servers. The specific details can be found at:
<https://tools.ietf.org/html/rfc7230#section-5.3>

The 'originserver' option tells Squid to use the origin-form syntax
(among other origin-specific things) when talking to this peer.

Amos



From belle at bazuin.nl  Tue Feb 21 10:00:34 2017
From: belle at bazuin.nl (=?windows-1252?Q?L.P.H._van_Belle?=)
Date: Tue, 21 Feb 2017 11:00:34 +0100
Subject: [squid-users] unable to get squid kerberos group working.
Message-ID: <vmime.58ac0fc2.d24.2a86624e5c9feee2@ms249-lin-003.rotterdam.bazuin.nl>

Hai,

?

I noticed a problem in the kerberos_ldap_group and im unable to get it working. 

I reported the bug here also : https://github.com/squid-cache/squid/issues/17 

?

Environment: Debian Jessie, Squid 3.5.24 debian rebuild from debian stretch.

kerberos_ldap_group: INFO: Starting version 1.3.1sq

?

first :? The kerberos group goes wrong with the SRV record detection. 

A and PTR records are in place and tested.

?

And a check on the SRV records shows.

?

dig SRV _ldap._tcp.internal.domain.tld.

;; ANSWER SECTION:

_ldap._tcp.internal.domain.tld. 900 IN SRV 5 100 636 dc1.internal.domain.tld.

_ldap._tcp.internal.domain.tld. 900 IN SRV 5 100 636 dc2.internal.domain.tld.

_ldap._tcp.internal.domain.tld. 900 IN SRV 10 100 389 dc1.internal.domain.tld.

_ldap._tcp.internal.domain.tld. 900 IN SRV 10 100 389 dc2.internal.domain.tld.

;; AUTHORITY SECTION:

?

dig SRV _ldaps._tcp.internal.domain.tld.

;; ANSWER SECTION:

_ldaps._tcp.internal.domain.tld. 900 IN SRV 0 100 636 dc1.internal.domain.tld.

_ldaps._tcp.internal.domain.tld. 900 IN SRV 0 100 636 dc2.internal.domain.tld.

;; AUTHORITY SECTION:

?

?

but debug logs shows. ( cache.log ) 

support_resolv.cc(407): pid=15718 :2017/02/20 08:24:03| kerberos_ldap_group: DEBUG: Adding internal.domain.tld to list

support_resolv.cc(443): pid=15718 :2017/02/20 08:24:03| kerberos_ldap_group: DEBUG: Sorted ldap server names for domain INTERNAL.DOMAIN.TLD:

support_resolv.cc(445): pid=15718 :2017/02/20 08:24:03| kerberos_ldap_group: DEBUG: Host: dc1.internal.domain.tld Port: 636 Priority: 5 Weight: 100 

support_resolv.cc(445): pid=15718 :2017/02/20 08:24:03| kerberos_ldap_group: DEBUG: Host: dc2.internal.domain.tld Port: 389 Priority: 5 Weight: 100

support_resolv.cc(445): pid=15718 :2017/02/20 08:24:03| kerberos_ldap_group: DEBUG: Host: dc1.internal.domain.tld Port: 636 Priority: 10 Weight: 100

support_resolv.cc(445): pid=15718 :2017/02/20 08:24:03| kerberos_ldap_group: DEBUG: Host: dc2.internal.domain.tld Port: 389 Priority: 10 Weight: 100

Wrong order in the debug output.

?

The hostnames and priority changes, and this changes randomly at every startup. 

I dont know it this is the cause of my problem, thats why im asking here. 

?

So Im trying to get my kerberos group checks working, but still no go and i just dont see what the problem is.

?

The Kerberos auth i use, which works fine.

?

auth_param negotiate program /usr/lib/squid/negotiate_wrapper_auth \

??? --kerberos /usr/lib/squid/negotiate_kerberos_auth -s HTTP/proxy2.internal.domain.tld at REALM \

??? --ntlm /usr/bin/ntlm_auth --helper-protocol=gss-spnego --domain=NTDOM

?

The kerberos_ldap_group line which im trying to get working. 

external_acl_type memberof-test-group ipv4 %LOGIN? /usr/lib/squid/ext_kerberos_ldap_group_acl -d -i -m 4

???? -g test-group \

??? -N NTDOM at REALM \

??? -D REALM \

??? -S dc1.internal.domain.tld at REALM:dc2.internal.domain.tld at REALM 

acl test-group external memberof-test-group

and im my config im having als test. 

http_access deny test-group

?

I tried also with the ?g test-group@ and ?g test-group@@REALM

?

?

This is the debug part of the kerberos group auth when starting squid. 

?

kerberos_ldap_group.cc(376): pid=3420 :2017/02/21 10:24:35| kerberos_ldap_group: INFO: Got User: testuser Domain: REALM

support_member.cc(63): pid=3420 :2017/02/21 10:24:35| kerberos_ldap_group: DEBUG: User domain loop: group at domain test-group at NULL

support_member.cc(91): pid=3420 :2017/02/21 10:24:35| kerberos_ldap_group: DEBUG: Default domain loop: group at domain test-group at NULL

support_member.cc(119): pid=3420 :2017/02/21 10:24:35| kerberos_ldap_group: DEBUG: Default group loop: group at domain test-group at NULL

support_member.cc(121): pid=3420 :2017/02/21 10:24:35| kerberos_ldap_group: DEBUG: Found group at domain test-group at NULL

support_ldap.cc(898): pid=3420 :2017/02/21 10:24:35| kerberos_ldap_group: DEBUG: Setup Kerberos credential cache

support_krb5.cc(127): pid=3420 :2017/02/21 10:24:35| kerberos_ldap_group: DEBUG: Set credential cache to MEMORY:squid_ldap_3420

support_krb5.cc(138): pid=3420 :2017/02/21 10:24:35| kerberos_ldap_group: DEBUG: Get default keytab file name

support_krb5.cc(144): pid=3420 :2017/02/21 10:24:35| kerberos_ldap_group: DEBUG: Got default keytab file name /etc/squid/keytab.PROXY2-HTTP

support_krb5.cc(158): pid=3420 :2017/02/21 10:24:35| kerberos_ldap_group: DEBUG: Get principal name from keytab /etc/squid/keytab.PROXY2-HTTP

support_krb5.cc(169): pid=3420 :2017/02/21 10:24:35| kerberos_ldap_group: DEBUG: Keytab entry has realm name: REALM

support_krb5.cc(181): pid=3420 :2017/02/21 10:24:35| kerberos_ldap_group: DEBUG: Found principal name: HTTP/proxy2.internal.domain.tld at REALM

support_krb5.cc(196): pid=3420 :2017/02/21 10:24:35| kerberos_ldap_group: DEBUG: Got principal name HTTP/proxy2.internal.domain.tld at REALM

support_krb5.cc(260): pid=3420 :2017/02/21 10:24:35| kerberos_ldap_group: DEBUG: Stored credentials

support_ldap.cc(927): pid=3420 :2017/02/21 10:24:35| kerberos_ldap_group: DEBUG: Initialise ldap connection

support_ldap.cc(933): pid=3420 :2017/02/21 10:24:35| kerberos_ldap_group: DEBUG: Canonicalise ldap server name for domain REALM

support_resolv.cc(379): pid=3420 :2017/02/21 10:24:35| kerberos_ldap_group: DEBUG: Resolved SRV _ldap._tcp.REALM record to dc1.internal.domain.tld

support_resolv.cc(379): pid=3420 :2017/02/21 10:24:35| kerberos_ldap_group: DEBUG: Resolved SRV _ldap._tcp.REALM record to dc2.internal.domain.tld

support_resolv.cc(379): pid=3420 :2017/02/21 10:24:35| kerberos_ldap_group: DEBUG: Resolved SRV _ldap._tcp.REALM record to dc2.internal.domain.tld

support_resolv.cc(379): pid=3420 :2017/02/21 10:24:35| kerberos_ldap_group: DEBUG: Resolved SRV _ldap._tcp.REALM record to dc1.internal.domain.tld

support_resolv.cc(207): pid=3420 :2017/02/21 10:24:35| kerberos_ldap_group: DEBUG: Resolved address 1 of REALM to dc1.internal.domain.tld

support_resolv.cc(207): pid=3420 :2017/02/21 10:24:35| kerberos_ldap_group: DEBUG: Resolved address 2 of REALM to dc1.internal.domain.tld

support_resolv.cc(207): pid=3420 :2017/02/21 10:24:35| kerberos_ldap_group: DEBUG: Resolved address 3 of REALM to dc1.internal.domain.tld

support_resolv.cc(207): pid=3420 :2017/02/21 10:24:35| kerberos_ldap_group: DEBUG: Resolved address 4 of REALM to dc2.internal.domain.tld

support_resolv.cc(207): pid=3420 :2017/02/21 10:24:35| kerberos_ldap_group: DEBUG: Resolved address 5 of REALM to dc2.internal.domain.tld

support_resolv.cc(207): pid=3420 :2017/02/21 10:24:35| kerberos_ldap_group: DEBUG: Resolved address 6 of REALM to dc2.internal.domain.tld

support_resolv.cc(407): pid=3420 :2017/02/21 10:24:35| kerberos_ldap_group: DEBUG: Adding REALM to list

support_resolv.cc(443): pid=3420 :2017/02/21 10:24:35| kerberos_ldap_group: DEBUG: Sorted ldap server names for domain REALM:

support_resolv.cc(445): pid=3420 :2017/02/21 10:24:35| kerberos_ldap_group: DEBUG: Host: dc2.internal.domain.tld Port: 389 Priority: 5 Weight: 100

support_resolv.cc(445): pid=3420 :2017/02/21 10:24:35| kerberos_ldap_group: DEBUG: Host: dc1.internal.domain.tld Port: 636 Priority: 5 Weight: 100

support_resolv.cc(445): pid=3420 :2017/02/21 10:24:35| kerberos_ldap_group: DEBUG: Host: dc2.internal.domain.tld Port: 389 Priority: 10 Weight: 100

support_resolv.cc(445): pid=3420 :2017/02/21 10:24:35| kerberos_ldap_group: DEBUG: Host: dc1.internal.domain.tld Port: 636 Priority: 10 Weight: 100

support_resolv.cc(445): pid=3420 :2017/02/21 10:24:35| kerberos_ldap_group: DEBUG: Host: REALM Port: -1 Priority: -2 Weight: -2

support_ldap.cc(942): pid=3420 :2017/02/21 10:24:35| kerberos_ldap_group: DEBUG: Setting up connection to ldap server dc2.internal.domain.tld:389

support_ldap.cc(953): pid=3420 :2017/02/21 10:24:35| kerberos_ldap_group: DEBUG: Bind to ldap server with SASL/GSSAPI

support_sasl.cc(276): pid=3420 :2017/02/21 10:24:35| kerberos_ldap_group: ERROR: ldap_sasl_interactive_bind_s error: Local error

support_ldap.cc(957): pid=3420 :2017/02/21 10:24:35| kerberos_ldap_group: ERROR: Error while binding to ldap server with SASL/GSSAPI: Local error

support_ldap.cc(942): pid=3420 :2017/02/21 10:24:35| kerberos_ldap_group: DEBUG: Setting up connection to ldap server dc1.internal.domain.tld:636

support_ldap.cc(953): pid=3420 :2017/02/21 10:24:35| kerberos_ldap_group: DEBUG: Bind to ldap server with SASL/GSSAPI

support_sasl.cc(276): pid=3420 :2017/02/21 10:24:35| kerberos_ldap_group: ERROR: ldap_sasl_interactive_bind_s error: Local error

support_ldap.cc(957): pid=3420 :2017/02/21 10:24:35| kerberos_ldap_group: ERROR: Error while binding to ldap server with SASL/GSSAPI: Local error

support_ldap.cc(942): pid=3420 :2017/02/21 10:24:35| kerberos_ldap_group: DEBUG: Setting up connection to ldap server dc2.internal.domain.tld:389

support_ldap.cc(953): pid=3420 :2017/02/21 10:24:35| kerberos_ldap_group: DEBUG: Bind to ldap server with SASL/GSSAPI

support_sasl.cc(276): pid=3420 :2017/02/21 10:24:35| kerberos_ldap_group: ERROR: ldap_sasl_interactive_bind_s error: Local error

support_ldap.cc(957): pid=3420 :2017/02/21 10:24:35| kerberos_ldap_group: ERROR: Error while binding to ldap server with SASL/GSSAPI: Local error

support_ldap.cc(942): pid=3420 :2017/02/21 10:24:35| kerberos_ldap_group: DEBUG: Setting up connection to ldap server dc1.internal.domain.tld:636

support_ldap.cc(953): pid=3420 :2017/02/21 10:24:35| kerberos_ldap_group: DEBUG: Bind to ldap server with SASL/GSSAPI

support_sasl.cc(276): pid=3420 :2017/02/21 10:24:35| kerberos_ldap_group: ERROR: ldap_sasl_interactive_bind_s error: Local error

support_ldap.cc(957): pid=3420 :2017/02/21 10:24:35| kerberos_ldap_group: ERROR: Error while binding to ldap server with SASL/GSSAPI: Local error

support_ldap.cc(942): pid=3420 :2017/02/21 10:24:35| kerberos_ldap_group: DEBUG: Setting up connection to ldap server REALM:389

support_ldap.cc(953): pid=3420 :2017/02/21 10:24:35| kerberos_ldap_group: DEBUG: Bind to ldap server with SASL/GSSAPI

support_sasl.cc(276): pid=3420 :2017/02/21 10:24:35| kerberos_ldap_group: ERROR: ldap_sasl_interactive_bind_s error: Local error

support_ldap.cc(957): pid=3420 :2017/02/21 10:24:35| kerberos_ldap_group: ERROR: Error while binding to ldap server with SASL/GSSAPI: Local error

support_ldap.cc(979): pid=3420 :2017/02/21 10:24:35| kerberos_ldap_group: DEBUG: Error during initialisation of ldap connection: Success

support_ldap.cc(1048): pid=3420 :2017/02/21 10:24:35| kerberos_ldap_group: DEBUG: Error during initialisation of ldap connection: Success

support_member.cc(132): pid=3420 :2017/02/21 10:24:35| kerberos_ldap_group: INFO: User testuser is not member of group at domain test-group at NULL

kerberos_ldap_group.cc(411): pid=3420 :2017/02/21 10:24:35| kerberos_ldap_group: DEBUG: ERR

?

?

I use samba4 AD DC?s 

?

The ssl certs are tested and correct, ssl BUMP is running and works fine. 

The ssl root-CA is also in place and also published to the pc's 

?

So im a bit lots now where to look or what im doing wrong here. 

Anyone any tips? 

?

?

Greetz, 

?

Louis

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20170221/c0b21593/attachment.htm>

From zav at tomica.ru  Tue Feb 21 10:43:38 2017
From: zav at tomica.ru (Anton)
Date: Tue, 21 Feb 2017 17:43:38 +0700
Subject: [squid-users] URL encoding in squid
Message-ID: <20170221174338.61d3d6f0@zav-wrk>

Good day.

I have squid+squidguard configuration. I need to filter a lot o URLs with national
symbols in it. My URL list consist mostly from percent-encoded URLs. So when squid
checks such URLs by squidGuard it transmits URL as-is with no percent-encoding.

SquidGuard see no URL because it has percent-encoded this URL.

URL list made from "zapret-info" if some one knows :-). It can contain non-consistent data:
%-encoded URLs can be in cp1251 or utf-8 after decoding and some URLs are not encoded at all.
I cannot to decode URLs from % in a right way.

IMHO it is better way is to %-encode not-encoded URLs to %-encoded and to use others as is.


So can squd+squdGuard make dial with percent-encoded URLs ?

Is it possible to path %-encoded URL to squidGuard ?


P.S. I use debian 8.7 and squid from debian repo (3.4.8-6+deb8u4).


From ahmed.zaeem at netstream.ps  Tue Feb 21 14:42:03 2017
From: ahmed.zaeem at netstream.ps (--Ahmad--)
Date: Tue, 21 Feb 2017 16:42:03 +0200
Subject: [squid-users] help me optimising caching or increasing hit ratio
Message-ID: <593D9177-82FF-40AE-AE3A-61E3517854AC@netstream.ps>

I?m using squid 3.5.2
and I?m browsing the same website many times but no ?HIT? in logs !!!

already enabled https and cert imported .

plz help me why i don?t see HITs in my access.log ?

there are some sites I?m very interested with like ==> https://www.ruzivodigitallearning.co.zw.com <https://www.ruzivodigitallearning.co.zw.com/>

plz have a look on my config below and advise me with best options to optimise caching and hit ratio increase

cheers 

==============
here is my config 
root at portablecloud-3011:~# cat /etc/squid/squid.conf
acl wu dstdom_regex \.download\.windowsupdate\.com$
acl wu-rejects dstdom_regex stats
acl GET method GET
cache_peer 127.0.0.1 parent 8080 0 proxy-only no-tproxy no-digest no-query no-netdb-exchange name=ms1
cache_peer_access ms1 allow GET wu !wu-rejects
cache_peer_access ms1 deny all
never_direct allow GET wu !wu-rejects
never_direct deny all

########################################
visible_hostname pcloud
acl ip1 myip 10.1.0.1
acl ip2 myip 192.168.10.210
tcp_outgoing_address 192.168.10.210 ip1
tcp_outgoing_address 192.168.10.210 ip2
#
# Recommended minimum configuration:
#

# Example rule allowing access from your local networks.
# Adapt to list your (internal) IP networks from where browsing
# should be allowed
acl localnet src 10.0.0.0/8     # RFC1918 possible internal network
acl localnet src 172.16.0.0/12  # RFC1918 possible internal network
acl localnet src 192.168.0.0/16 # RFC1918 possible internal network
acl localnet src fc00::/7       # RFC 4193 local private network range
acl localnet src fe80::/10      # RFC 4291 link-local (directly plugged) machines

acl SSL_ports port 443
acl Safe_ports port 80          # http
acl Safe_ports port 21          # ftp
acl Safe_ports port 443         # https
acl Safe_ports port 70          # gopher
acl Safe_ports port 210         # wais
acl Safe_ports port 1025-65535  # unregistered ports
acl Safe_ports port 280         # http-mgmt
acl Safe_ports port 488         # gss-http
acl Safe_ports port 591         # filemaker
acl Safe_ports port 777         # multiling http
acl CONNECT method CONNECT

#
# Recommended minimum Access Permission configuration:
#
# Deny requests to certain unsafe ports
http_access deny !Safe_ports

# Deny CONNECT to other than secure SSL ports
http_access deny CONNECT !SSL_ports
http_access allow  CONNECT 
# Only allow cachemgr access from localhost
http_access allow localhost manager
http_access deny manager

# We strongly recommend the following be uncommented to protect innocent
# web applications running on the proxy server who think the only
# one who can access services on "localhost" is a local user
#http_access deny to_localhost

#
# INSERT YOUR OWN RULE(S) HERE TO ALLOW ACCESS FROM YOUR CLIENTS
#

# Example rule allowing access from your local networks.
# Adapt localnet in the ACL section to list your (internal) IP networks
# from where browsing should be allowed
http_access allow localnet
http_access allow localhost

# And finally deny all other access to this proxy
http_access deny all

# Squid normally listens to port 3128
http_port 3128

# Uncomment and adjust the following to add a disk cache directory.
#cache_dir ufs /var/cache/squid 100 16 256

# Leave coredumps in the first cache dir
#coredump_dir /var/cache/squid

#
# Add any of your own refresh_pattern entries above these.
#
#
refresh_pattern -i \.htm 120 50% 10080 reload-into-ims
refresh_pattern -i \.html 120 50% 10080 reload-into-ims
refresh_pattern ^http://*.facebook.com/* 720 100% 4320
refresh_pattern ^https://*.ruzivodigitallearning.co.zw.com/* 720 100% 4320
refresh_pattern ^http://*.ruzivodigitallearning.co.zw.com/* 720 100% 4320
refresh_pattern ^http://mail.yahoo.com/.* 720 100% 4320
refresh_pattern ^http://*.yahoo.*/.* 720 100% 4320
refresh_pattern ^http://*.yimg.*/.* 720 100% 4320
refresh_pattern ^http://*.gmail.*/.* 720 100% 4320
refresh_pattern ^http://*.google.*/.* 720 100% 4320
refresh_pattern ^http://*.kaskus.*/.* 720 100% 4320
refresh_pattern ^http://*.googlesyndication.*/.* 720 100% 4320
refresh_pattern ^http://*.plasa.*/.* 720 100% 4320
refresh_pattern ^http://*.telkom.*/.* 720 100% 4320
##################################################
refresh_pattern -i \.fbcdn.net.*\.(jpg|gif|png|swf|mp3)                  10800 80% 10800 ignore-reload  override-expire ignore-no-cache
refresh_pattern  static\.ak\.fbcdn\.net*\.(jpg|gif|png)                  10800 80% 10800 ignore-reload  override-expire ignore-no-cache
refresh_pattern ^http:\/\/profile\.ak\.fbcdn.net*\.(jpg|gif|png)      10800 80% 10800 ignore-reload  override-expire ignore-no-cache
##############
refresh_pattern -i \.(3gp|7z|ace|asx|avi|bin|cab|dat|deb|divx|dvr-ms)      10800 80% 10800 ignore-no-cache  ignore-private override-expire override-lastmod reload-into-ims
refresh_pattern -i \.(rar|jar|gz|tgz|bz2|iso|m1v|m2(v|p)|mo(d|v))          10800 80% 10800 ignore-no-cache  ignore-private override-expire override-lastmod reload-into-ims
refresh_pattern -i \.(jp(e?g|e|2)|gif|pn[pg]|bm?|tiff?|ico|swf|css|js)     10800 80% 10800 ignore-no-cache  ignore-private override-expire override-lastmod reload-into-ims
refresh_pattern -i \.(mp(e?g|a|e|1|2|3|4)|mk(a|v)|ms(i|u|p)|og(x|v|a|g)|rar|rm|r(a|p)m|snd|vob|wav) 10800 80% 10800 ignore-no-cache ignore-private override-expire override-lastmod reload-into-ims
refresh_pattern -i \.(pp(s|t)|wax|wm(a|v)|wmx|wpl|zip|cb(r|z|t))     10800 80% 10800 ignore-no-cache ignore-private override-expire override-lastmod reload-into-ims
 #############################
refresh_pattern (cgi-bin|\?)       0      0%      0
refresh_pattern ^gopher:    1440    0%    1440
refresh_pattern ^ftp:         10080     95%     10800 override-lastmod reload-into-ims
refresh_pattern         .     180     95% 10800 override-lastmod reload-into-ims
#################
minimum_object_size 0 bytes
maximum_object_size_in_memory 500 MB
refresh_pattern ^ftp:           1440    20%     10080
refresh_pattern ^gopher:        1440    0%      1440
refresh_pattern -i (/cgi-bin/|\?) 0     0%      0
refresh_pattern .               0       20%     4320

http_port 3126
#http_port 3128
#######################################
cache_swap_low 90
cache_swap_high 95
############################
cache_effective_user squid
cache_effective_group squid
memory_replacement_policy lru
cache_replacement_policy heap LFUDA
########################
maximum_object_size 10000 MB
cache_mem 5000 MB
maximum_object_size_in_memory 10 MB
#########################
logfile_rotate 2
max_filedescriptors 131072
###############################
#cache_dir ufs /root/cache3 600000 64 128
############
cache_dir aufs /var/cache/squid 600000 64 128
#######################################
https_port 3129 intercept ssl-bump generate-host-certificates=on dynamic_cert_mem_cache_size=4MB cert=/usr/local/squid/ssl_cert/myca.pem key=/usr/local/squid/ssl_cert/myca.pem
ssl_bump server-first all
sslcrtd_program /lib/squid/ssl_crtd -s /var/lib/ssl_db -M 4MB
sslcrtd_children 8 startup=1 idle=1
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20170221/7dd6b94f/attachment.htm>

From squid3 at treenet.co.nz  Tue Feb 21 19:55:44 2017
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Wed, 22 Feb 2017 08:55:44 +1300
Subject: [squid-users] squid-avira-update-cache
In-Reply-To: <CABJB4UONHCb-hsxEU7ajRu1bf9Dd3WwXNVHt8XzR4ozMdoCwWw@mail.gmail.com>
References: <6cd017ac4ce7d5479e2cdaa7362777ad@xp1>
 <f10fe738-46ea-7d18-22e7-dd994c32b186@gmail.com>
 <d7846e49-350a-df36-b591-8743306cd5a0@cinbesa.com.br>
 <4c1da1ac-89cc-374e-23f6-6546cb8f720a@measurement-factory.com>
 <388fba18-83b5-d2e7-b450-8c15ca74910f@cinbesa.com.br>
 <87c789b9-7e1e-f408-43fb-fbd700b16fe3@measurement-factory.com>
 <9413fa07-c795-2312-d4b4-4ebf0b54e685@treenet.co.nz>
 <2D757DE7-E51B-45AE-A051-59EBA406CF5A@gmail.com>
 <d9025608-d2e8-e503-f37f-1a3dc1176d8e@treenet.co.nz>
 <CABJB4UONHCb-hsxEU7ajRu1bf9Dd3WwXNVHt8XzR4ozMdoCwWw@mail.gmail.com>
Message-ID: <39f7458c-4670-88e3-d957-fe46ed1ee643@treenet.co.nz>

On 21/02/2017 10:00 p.m., FLASH FLASH wrote:
> this is part of my cache.log:
> 
> 2017/02/21 10:54:47.908 kid1| 11,2| http.cc(2221) sendRequest: HTTP Server
> REQUEST:
> ---------
> GET /update/repair_sigver/1.0.25.18/win32/int/repair.rdf.lz HTTP/1.1
> Host: personal.avira-update.com
> User-Agent: @AUVI at 1.3;AntiVir-NGUpd/15.0.24.143 (PERS; WKS; EN; AVE
> 8.3.42.182; VDF 8.12.155.244; Windows 10 Enterprise; ; Bulgaria;
> 4e083d762df61e49122f00e3758c65355687472f; 0000149996-AVHOE-0000001; BG;
> BUILD 15.0.24.146; 0; 1; 1; iexplore; 1;
> 309afc5503fe48febffe89e27e9390322a9f97f2; 0)
> Accept: */*
> Via: 1.1 router2 (squid/3.5.10)
> X-Forwarded-For: ...
> Cache-Control: no-cache, no-store, must-revalidate

There we go, the AV updater client (or maybe that 'router2' proxy?)
forbids cached content to be delivered as a response AND the response to
this request from being cached.

Which is kind of annoying since the server indicates the reply is
actually cacheable for at least a few minutes and HTTP/1.1 revalidation
performance features are supported. :-(

If router2 is under your control you could optimize a bit by erasing
that Cache-Control header from the AV updaters request before they are
sent on to the current Squid.

Amos



From squid3 at treenet.co.nz  Tue Feb 21 20:17:27 2017
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Wed, 22 Feb 2017 09:17:27 +1300
Subject: [squid-users] URL encoding in squid
In-Reply-To: <20170221174338.61d3d6f0@zav-wrk>
References: <20170221174338.61d3d6f0@zav-wrk>
Message-ID: <44537d73-a70c-1bcb-b0fb-0011455f4bd2@treenet.co.nz>

On 21/02/2017 11:43 p.m., Anton wrote:
> Good day.
> 
> I have squid+squidguard configuration. I need to filter a lot o URLs with national
> symbols in it. My URL list consist mostly from percent-encoded URLs. So when squid
> checks such URLs by squidGuard it transmits URL as-is with no percent-encoding.
> 
> SquidGuard see no URL because it has percent-encoded this URL.
> 
> URL list made from "zapret-info" if some one knows :-). It can contain non-consistent data:
> %-encoded URLs can be in cp1251 or utf-8 after decoding and some URLs are not encoded at all.
> I cannot to decode URLs from % in a right way.

Ew.

> 
> IMHO it is better way is to %-encode not-encoded URLs to %-encoded and to use others as is.
> 
> 
> So can squd+squdGuard make dial with percent-encoded URLs ?
> 

Squid should be normalizing the %-encoding on the URLs as they arrive,
but I'm not seeing where it does that in the code so maybe not. What
SquidGuard does with them or its input data file is not under Squid control.

Also SG is very outdated and no longer maintained, you might find
ufdbGuard better able to handle this nasty input. I've bcc'd Marcus in
case there is something he can (or has) do about this type of mess in
that helper.

> Is it possible to path %-encoded URL to squidGuard ?

Not with Squid-3.4. The 3.5 releases have a url_rewrite_extras directive
which takes logformat codes. You could use that to send an extra
%-encoded copy of the URL to the helper in addition to the normal URL
input. (sorry there is no package yet in Debian 8 for 3.5).

Amos



From marcus.kool at urlfilterdb.com  Tue Feb 21 20:38:36 2017
From: marcus.kool at urlfilterdb.com (Marcus Kool)
Date: Tue, 21 Feb 2017 17:38:36 -0300
Subject: [squid-users] URL encoding in squid
In-Reply-To: <44537d73-a70c-1bcb-b0fb-0011455f4bd2@treenet.co.nz>
References: <20170221174338.61d3d6f0@zav-wrk>
 <44537d73-a70c-1bcb-b0fb-0011455f4bd2@treenet.co.nz>
Message-ID: <74d0d699-0ea1-ba5f-2b48-9ec37186304b@urlfilterdb.com>



On 21/02/17 17:17, Amos Jeffries wrote:

>> Is it possible to path %-encoded URL to squidGuard ?
>
> Not with Squid-3.4. The 3.5 releases have a url_rewrite_extras directive
> which takes logformat codes. You could use that to send an extra
> %-encoded copy of the URL to the helper in addition to the normal URL
> input. (sorry there is no package yet in Debian 8 for 3.5).
>
> Amos

ufdbGuard has a database format that supports UTF8 characters but
only the latest beta (ufdbguard 1.32.5beta9) fully supports it.
I can send you a link to the beta software if you are interested.

how it works:
ufdbGuard as a utility to convert domains+urls files into a
database file which converts all %-encoded characters.
The URLs that Squid sends to ufdbGuard are also all converted
which means that URLs with %-encoded URLs and URLs without %-encoding
match.

Marcus




From squid3 at treenet.co.nz  Tue Feb 21 21:57:47 2017
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Wed, 22 Feb 2017 10:57:47 +1300
Subject: [squid-users] help me optimising caching or increasing hit ratio
In-Reply-To: <593D9177-82FF-40AE-AE3A-61E3517854AC@netstream.ps>
References: <593D9177-82FF-40AE-AE3A-61E3517854AC@netstream.ps>
Message-ID: <f8e2e882-b9b7-6461-3233-dddd5fbcba9b@treenet.co.nz>

On 22/02/2017 3:42 a.m., --Ahmad-- wrote:
> I?m using squid 3.5.2

Please ugrade to at least 3.5.19 (current release is 3.5.24). There have
been quite a few security issues fixed, and the 3.5 does caching a *lot*
better than it did in those early releases.


> and I?m browsing the same website many times but no ?HIT? in logs !!!
> 
> already enabled https and cert imported .
> 
> plz help me why i don?t see HITs in my access.log ?
> 


3.5 supports HTTP/1.1 caching nowdays. The days of determining
performance from "HIT" being in the logs is long ago past - majority of
cached data transactison involves "REFRESH" actions these days.

If you want to see what your caching performance is like you need to use
a log analysis tool that understands the REFRESH codes, or use the
cachemgr 'info' report summary of HIT-ratio's.


> there are some sites I?m very interested with like ==> https://www.ruzivodigitallearning.co.zw.com
> 
> plz have a look on my config below and advise me with best options to optimise caching and hit ratio increase
> 
> cheers 
> 
> ==============
> here is my config 
> root at portablecloud-3011:~# cat /etc/squid/squid.conf
> acl wu dstdom_regex \.download\.windowsupdate\.com$
> acl wu-rejects dstdom_regex stats
> acl GET method GET
> cache_peer 127.0.0.1 parent 8080 0 proxy-only no-tproxy no-digest no-query no-netdb-exchange name=ms1
> cache_peer_access ms1 allow GET wu !wu-rejects
> cache_peer_access ms1 deny all
> never_direct allow GET wu !wu-rejects
> never_direct deny all
> 
> ########################################
> visible_hostname pcloud
> acl ip1 myip 10.1.0.1
> acl ip2 myip 192.168.10.210
> tcp_outgoing_address 192.168.10.210 ip1
> tcp_outgoing_address 192.168.10.210 ip2
> #
> # Recommended minimum configuration:
> #
> 
> # Example rule allowing access from your local networks.
> # Adapt to list your (internal) IP networks from where browsing
> # should be allowed
> acl localnet src 10.0.0.0/8     # RFC1918 possible internal network
> acl localnet src 172.16.0.0/12  # RFC1918 possible internal network
> acl localnet src 192.168.0.0/16 # RFC1918 possible internal network
> acl localnet src fc00::/7       # RFC 4193 local private network range
> acl localnet src fe80::/10      # RFC 4291 link-local (directly plugged) machines
> 
> acl SSL_ports port 443
> acl Safe_ports port 80          # http
> acl Safe_ports port 21          # ftp
> acl Safe_ports port 443         # https
> acl Safe_ports port 70          # gopher
> acl Safe_ports port 210         # wais
> acl Safe_ports port 1025-65535  # unregistered ports
> acl Safe_ports port 280         # http-mgmt
> acl Safe_ports port 488         # gss-http
> acl Safe_ports port 591         # filemaker
> acl Safe_ports port 777         # multiling http
> acl CONNECT method CONNECT
> 
> #
> # Recommended minimum Access Permission configuration:
> #
> # Deny requests to certain unsafe ports
> http_access deny !Safe_ports
> 
> # Deny CONNECT to other than secure SSL ports
> http_access deny CONNECT !SSL_ports
> http_access allow  CONNECT 
> # Only allow cachemgr access from localhost
> http_access allow localhost manager
> http_access deny manager
> 
> # We strongly recommend the following be uncommented to protect innocent
> # web applications running on the proxy server who think the only
> # one who can access services on "localhost" is a local user
> #http_access deny to_localhost
> 
> #
> # INSERT YOUR OWN RULE(S) HERE TO ALLOW ACCESS FROM YOUR CLIENTS
> #
> 
> # Example rule allowing access from your local networks.
> # Adapt localnet in the ACL section to list your (internal) IP networks
> # from where browsing should be allowed
> http_access allow localnet
> http_access allow localhost
> 
> # And finally deny all other access to this proxy
> http_access deny all
> 
> # Squid normally listens to port 3128
> http_port 3128
> 
> # Uncomment and adjust the following to add a disk cache directory.
> #cache_dir ufs /var/cache/squid 100 16 256
> 
> # Leave coredumps in the first cache dir
> #coredump_dir /var/cache/squid
> 
> #
> # Add any of your own refresh_pattern entries above these.
> #

Please note what that line says "above". It is the default config
comment for the final three refresh_pattern lines way, way down below.
 You can either erase it entirely, or move it back to its proper
position under your custom patterns.


Also, a note on the % values in all the refresh_pattern lines;

 One of the reasons to upgrade is that they are actually better when
used with values >100%. Some releases before 3.5.12 complained about the
% incorrectly. That bug has been fixed in the current Squid versions.

 For example; An object which is 2 hours old having a 100% pt value
configured. Has an LMF calculation indicating its cacheable for 2hrs
from time received. This sounds great, but a lot of traffic is quote
young, ie. milliseconds, and 100% of a few milliseconds is not very much
time before the object needs updating again. Values upwards to 1000% may
be useful in a busy proxy cache.



> #
> refresh_pattern -i \.htm 120 50% 10080 reload-into-ims
> refresh_pattern -i \.html 120 50% 10080 reload-into-ims
> refresh_pattern ^http://*.facebook.com/* 720 100% 4320

Where did you copy-and-paste these patterns from? they are all very,
very broken. The above will not match what you probably think it does.

1) The "/*." near the start means 0 or more _slash_ ('/') characters
followed by _only one_ character whose existence is mandatory but can
have any value.


 How many URLs you seen with "http:/blah" or "http:///////blah" ?

 If this "works" at all it is probably because the regex library
interprets the facebook URL as having 0 '/' characters for (3) and one
charater (a '/') for the mandatory position.

AFAIK a regex library that does that is buggy though. So I think this
line will never match unless you are being passed phishing-like attack
URLs where the domain "facebook.com" is prefixed with some obscure
character like "1facebook.com" to fool people into clicking links
thinking its Facebook.

The '*' at the end also means 0 or more '/' characters. This is
pointless in terms of being a wildcard. There is an implicit '.*'
pattern at the ends unless you use the special anchor code '$' to mark
the URL ending.
 But this pattern does not need that either. So you might as well erase
the whole '/*' part on the end.

Worst case you might be thinking this pattern matches only
"facebook.com" domain name because the domain is followed by a '/'.
  However since the final '*' allows omitting that '/' delimiter this
pattern _actually_ matches any URL with a *subdomain* similar to
".facebook.com"

Such as "http://_facebook_com.example.com.bwaahahaha/"


==> If that was the behaviour you actually wanted, fair enough. But I
suggest in that case adding a comment to say it is there to catch some
attack URLs, not actual facebook.com traffic.

To retain the current behaviour the correct pattern should be:
  ^http://*.facebook.com

To fix the behaviour to only match facebook.com and sub-domains, the
correct pattern would be:
  ^http://.*\.facebook\.com/

Also, I suggest adding 's?' after the '^http' bit (as demo'd below), so
the one pattern matches both HTTP and HTTPS URLs.

Also, I suggest using the -i flag. Scheme and domain are
case-insensitive URL segments. Squid should be normalizing them to lower
case, but may not.


> refresh_pattern ^https://*.ruzivodigitallearning.co.zw.com/* 720 100% 4320
> refresh_pattern ^http://*.ruzivodigitallearning.co.zw.com/* 720 100% 4320

You can merge the above two lines into one by using the regex pattern:

  ^https?://.*\.ruzivodigitallearning\.co\.zw\.com/

Note that I have corrected for the same issues the facebook pattern had.

BTW: is that .com supposed to be there? I looked up the URL in redbot to
check cachability and the .com was not found, but there is a .co.zw ccTLD.


> refresh_pattern ^http://mail.yahoo.com/.* 720 100% 4320
> refresh_pattern ^http://*.yahoo.*/.* 720 100% 4320
> refresh_pattern ^http://*.yimg.*/.* 720 100% 4320
> refresh_pattern ^http://*.gmail.*/.* 720 100% 4320
> refresh_pattern ^http://*.google.*/.* 720 100% 4320
> refresh_pattern ^http://*.kaskus.*/.* 720 100% 4320
> refresh_pattern ^http://*.googlesyndication.*/.* 720 100% 4320
> refresh_pattern ^http://*.plasa.*/.* 720 100% 4320
> refresh_pattern ^http://*.telkom.*/.* 720 100% 4320

A useful rule of thumb is that fewer refresh_pattern lines leads to
better peformance.

So a redux of the above into a single regex pattern will be faster. Do
it with (a|b|c) compounding like the "file extension" patterns below.



If you want to improve performance remove all the override-lastmod.
Last-Modified is part of HTTP/1.1 which lets Squid perform fast
revalidation - without it some (most?) things can only MISS, and it
breaks the reload-into-ims operations.

Simplify your config by removing all the 'ignore-no-cache'. It has no
effect since Squid-3.2

Also, I recommend removing the ignore-private. Squid-3.5 can store the
data relatively safely, but if the revalidation does not work well it
can also lead to users manualy forcing reloads.



> ##################################################
> refresh_pattern -i \.fbcdn.net.*\.(jpg|gif|png|swf|mp3)                  10800 80% 10800 ignore-reload  override-expire ignore-no-cache
> refresh_pattern  static\.ak\.fbcdn\.net*\.(jpg|gif|png)                  10800 80% 10800 ignore-reload  override-expire ignore-no-cache
> refresh_pattern ^http:\/\/profile\.ak\.fbcdn.net*\.(jpg|gif|png)      10800 80% 10800 ignore-reload  override-expire ignore-no-cache

Er. these last two lines are sub-sets of the tope line. I think you can
erase those 'static' and 'profile' lines.


Also, once you remove the overrides as mentioend above. You are left
with "reload-into-ims" as the only difference between the parameters of
patterns above and the patterns below which match those same
file-extensions. So you can probably improve performance a bit more by
just erasing the above lines.

However, before they went all-HTTPS facebook were becoming one of the
better sites in terms of HTTP cacheability. I do not think that has
changed, its just the HTTPS/TLS wrapper preventing most of their traffic
going to caches now.
 So override-expires is probably making things *worse* for all that
fbcdn traffic nowdays.


> ##############
> refresh_pattern -i \.(3gp|7z|ace|asx|avi|bin|cab|dat|deb|divx|dvr-ms)      10800 80% 10800 ignore-no-cache  ignore-private override-expire override-lastmod reload-into-ims
> refresh_pattern -i \.(rar|jar|gz|tgz|bz2|iso|m1v|m2(v|p)|mo(d|v))          10800 80% 10800 ignore-no-cache  ignore-private override-expire override-lastmod reload-into-ims
> refresh_pattern -i \.(jp(e?g|e|2)|gif|pn[pg]|bm?|tiff?|ico|swf|css|js)     10800 80% 10800 ignore-no-cache  ignore-private override-expire override-lastmod reload-into-ims
> refresh_pattern -i \.(mp(e?g|a|e|1|2|3|4)|mk(a|v)|ms(i|u|p)|og(x|v|a|g)|rar|rm|r(a|p)m|snd|vob|wav) 10800 80% 10800 ignore-no-cache ignore-private override-expire override-lastmod reload-into-ims
> refresh_pattern -i \.(pp(s|t)|wax|wm(a|v)|wmx|wpl|zip|cb(r|z|t))     10800 80% 10800 ignore-no-cache ignore-private override-expire override-lastmod reload-into-ims

Something to beware of:
 The above patterns will match *anywhere* within the whole URL.

So the start of domain names (aka "www.raring.com" -> \.rar ) will be
cached using these refresh parameters.
 As will any URL that happens to have a similar match in the
query-string portion. That is kind of useful if there is a filename in
the query parameters, but also dangerous since you cannot know when or
where that matching will happen.
 Note that you are caching *private* responses whenever one of these
matches. The risk you are taking is large.

If that is a problem, you can work around it by adjusting the patterns
like this:
 ^https?://[^/]+/[^?]+\.(rar|jar|gz|tgz|bz2|iso|m1v|m2(v|p)|mo(d|v))

NP: If you like them to still match inside the query portion of URLs
replace the "[^?]" with a dot "."


PS. 'rar' is listed in the 2nd and 4th lines. One of them is redundant.



>  #############################
> refresh_pattern (cgi-bin|\?)       0      0%      0

This above regex is broken. It requires the -i and the '/' path
delimiters as seen in the below refresh_pattern lines for the correct
CGI regex.


> refresh_pattern ^gopher:    1440    0%    1440
> refresh_pattern ^ftp:         10080     95%     10800 override-lastmod reload-into-ims
> refresh_pattern         .     180     95% 10800 override-lastmod reload-into-ims
> #################
> minimum_object_size 0 bytes
> maximum_object_size_in_memory 500 MB

You have placed alternative ftp, gopher and '.' refresh_patterns above.
Remove the below refresh_pattern lines.

> refresh_pattern ^ftp:           1440    20%     10080
> refresh_pattern ^gopher:        1440    0%      1440
> refresh_pattern -i (/cgi-bin/|\?) 0     0%      0
> refresh_pattern .               0       20%     4320
> 
> http_port 3126
> #http_port 3128
> #######################################
> cache_swap_low 90
> cache_swap_high 95
> ############################
> cache_effective_user squid
> cache_effective_group squid
> memory_replacement_policy lru
> cache_replacement_policy heap LFUDA
> ########################
> maximum_object_size 10000 MB
> cache_mem 5000 MB
> maximum_object_size_in_memory 10 MB
> #########################
> logfile_rotate 2
> max_filedescriptors 131072
> ###############################
> #cache_dir ufs /root/cache3 600000 64 128
> ############
> cache_dir aufs /var/cache/squid 600000 64 128


Amos



From zav at tomica.ru  Wed Feb 22 02:00:09 2017
From: zav at tomica.ru (Anton)
Date: Wed, 22 Feb 2017 09:00:09 +0700
Subject: [squid-users] URL encoding in squid
In-Reply-To: <74d0d699-0ea1-ba5f-2b48-9ec37186304b@urlfilterdb.com>
References: <20170221174338.61d3d6f0@zav-wrk>
 <44537d73-a70c-1bcb-b0fb-0011455f4bd2@treenet.co.nz>
 <74d0d699-0ea1-ba5f-2b48-9ec37186304b@urlfilterdb.com>
Message-ID: <20170222090009.282a6718@zav-wrk>

Yes, I am interesting in ufdbguard 1.32.5beta9.
Marcus pleace send me a link.

I will try to install the last 3.5.* squid and ufdbguard 1.32.5beta9 by
hands (./configure --a-lots-off-stuf... ; make ; make install to /usr/local/squid) 


On Tue, 21 Feb 2017 17:38:36 -0300
Marcus Kool <marcus.kool at urlfilterdb.com> wrote:

> On 21/02/17 17:17, Amos Jeffries wrote:
> 
> >> Is it possible to path %-encoded URL to squidGuard ?  
> >
> > Not with Squid-3.4. The 3.5 releases have a url_rewrite_extras directive
> > which takes logformat codes. You could use that to send an extra
> > %-encoded copy of the URL to the helper in addition to the normal URL
> > input. (sorry there is no package yet in Debian 8 for 3.5).
> >
> > Amos  
> 
> ufdbGuard has a database format that supports UTF8 characters but
> only the latest beta (ufdbguard 1.32.5beta9) fully supports it.
> I can send you a link to the beta software if you are interested.
> 
> how it works:
> ufdbGuard as a utility to convert domains+urls files into a
> database file which converts all %-encoded characters.
> The URLs that Squid sends to ufdbGuard are also all converted
> which means that URLs with %-encoded URLs and URLs without %-encoding
> match.
> 
> Marcus
> 
> 
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users




-- 
Anton,
??????? ?????? ?????????? ????? ?????,
??? "???" (??????) 634050 ?. ?????
??. ?????? 55, ??. 101
???: 701-855


From test1964 at gmail.com  Thu Feb 23 07:51:23 2017
From: test1964 at gmail.com (Test1964)
Date: Thu, 23 Feb 2017 09:51:23 +0200
Subject: [squid-users] Squid 3.5.24 - Ssl Bump tlsv1 alert unknown ca
Message-ID: <d0350bda-2391-e7ec-f743-e5a90947a890@gmail.com>

HI,

when using squid in transparent mode and try to access 
https://www.facebook.com from computer all working very well,
but when I try to use Facebook app (on Iphone for example), I'm getting 
this error in cache.log:

Error negotiating SSl connection on FD XX: error:14094418:SSL 
routines:SSL3_READ_BYTES:tlsv1 alert unknown ca (1/0)

and Facebook not working.

I run on Centos 7:

* run update-ca-trust
* yum update ca-certificates
* create the certificate like this:
     openssl req -newkey rsa:4096 -sha512 -days 365 -nodes -x509 -keyout 
myCA.pem -out myCA.pem

Do I need to create the certificate for smartphones  different way?
Do I need to install more ca bundle in Centos?
How to fix this problem without white list this site from ssl bump


Thanks Dan

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20170223/3b35978c/attachment.htm>

From lists at compuniverse.de  Thu Feb 23 10:08:46 2017
From: lists at compuniverse.de (Amon Ott)
Date: Thu, 23 Feb 2017 11:08:46 +0100
Subject: [squid-users] Username not passed to url_rewrite_program
Message-ID: <3c520a74-00ef-cb94-0724-c04163fd44c2@compuniverse.de>

Hello list,

we want to filter with squidGuard and decide based on the username,
identified with ident. In the bug tracker I found the old entry 2655
with this bug, which had been marked as fixed, but is still present in
3.5.24. I have recently reopened that bug with some info about our
szenario. The old bugfix seems to be in the code, but does not solve the
issue.

http://bugs.squid-cache.org/show_bug.cgi?id=2655#c11

Since we cannot continue with our setup, some help would be appreciated.

Amon Ott
-- 
Dr. Amon Ott
m-privacy GmbH           Tel: +49 30 24342334
Werner-Vo?-Damm 62       Fax: +49 30 99296856
12101 Berlin             http://www.m-privacy.de

Amtsgericht Charlottenburg, HRB 84946

Gesch?ftsf?hrer:
 Dipl.-Kfm. Holger Maczkowsky,
 Roman Maczkowsky

GnuPG-Key-ID: 0x2DD3A649




From mo at stellarise.com  Thu Feb 23 11:23:22 2017
From: mo at stellarise.com (Imaginovskiy)
Date: Thu, 23 Feb 2017 03:23:22 -0800 (PST)
Subject: [squid-users] Squid 4.0.18 question about directives
Message-ID: <1487849002972-4681599.post@n4.nabble.com>

Hi All, 

I'm in the process of upgrading some clients to the latest squid proxy
version. Coming from version 3.5 I used to specify backends in squid.conf as
follows;

cache_peer site1.domain.com parent 443 0 no-query originserver
name=server_site1 ssl proxy-only front-end-https=on login=PASS
cache_peer_domain server_site1 site1.domain.com

cache_peer site2.domain.com parent 443 0 no-query originserver
name=server_site2 ssl proxy-only front-end-https=on login=PASS
cache_peer_domain server_site2 site2.domain.com

But in Squid 4 it looks a little confusing, cache_peer_domain is superseded
by cache_peer_access but looking at the documentation it looks like
cache_peer_access isn't quite the same as cache_peer_domain. So I ended up
looking neighbour_type_domain which looks like what I want, so I've
configured as following;

cache_peer site1.domain.com parent 443 0 no-query originserver
name=server_site1 login=PASS ssl front-end-https=on
neighbor_type_domain server_site1 site1.domain.com

cache_peer site2.domain.com parent 443 0 no-query originserver
name=server_site2 login=PASS ssl front-end-https=on
neighbor_type_domain server_site2 site2.fotechsolutions.com

Which seems ok, but what I end up seeing is site1.domain.com is proxied
correctly, but when browsing for site2.domain.com I end up loading
site1.domain.com, am I missing something in this new version?



--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/Squid-4-0-18-question-about-directives-tp4681599.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From tuser6485 at gmail.com  Thu Feb 23 11:59:13 2017
From: tuser6485 at gmail.com (Test User)
Date: Thu, 23 Feb 2017 17:29:13 +0530
Subject: [squid-users] SSL-Bump: NAT/TPROXY lookup failed to locate original
	IPs
Message-ID: <CAPhcQuK77qv28BC0X+2pqNA5gOTzbj9dmmF+hkkm4VHDX+vHDQ@mail.gmail.com>

Hi,
 I am trying to setup HTTPS proxy using ssl-bump. I have followed
steps mentioned in:
http://wiki.squid-cache.org/ConfigExamples/Intercept/SslBumpExplicit

Following is my squid.conf file:

acl SSL_ports port 443
acl Safe_ports port 80 # http
acl Safe_ports port 21 # ftp
acl Safe_ports port 443 # https
acl Safe_ports port 70 # gopher
acl Safe_ports port 210 # wais
acl Safe_ports port 1025-65535 # unregistered ports
acl Safe_ports port 280 # http-mgmt
acl Safe_ports port 488 # gss-http
acl Safe_ports port 591 # filemaker
acl Safe_ports port 777 # multiling http
acl CONNECT method CONNECT
acl step1 at_step SslBump1
http_access deny !Safe_ports
http_access deny CONNECT !SSL_ports
http_access allow localhost manager
http_access deny manager
http_access allow localhost
http_access allow all
http_port 3128 ssl-bump \
  cert=/etc/squid/ssl_cert/squidCA.pem \
  generate-host-certificates=on dynamic_cert_mem_cache_size=4MB
https_port 3129 intercept ssl-bump generate-host-certificates=on \
dynamic_cert_mem_cache_size=4MB cert=/etc/squid/ssl_cert/squidCA.pem \
dhparams=/etc/squid/ssl_cert/dhparam.pem
sslproxy_options NO_SSLv2,NO_SSLv3,SINGLE_DH_USE
sslproxy_cipher
EECDH+ECDSA+AESGCM:EECDH+aRSA+AESGCM:EECDH+ECDSA+SHA384:EECDH+ECDSA+SHA256:EECDH+aRSA+SHA384:EECDH+aRSA+SHA256:EECDH+aRSA+RC4:EECDH:EDH+aRSA:!RC4:!aNULL:!eNULL:!LOW:!3DES:!MD5:!EXP:!PSK:!SRP:!DSS
sslcrtd_program /usr/lib/squid/ssl_crtd -s /var/spool/squid3_ssldb -M 4MB
debug_options ALL,1 3,5 4,5 11,5 17,5 23,5 46,5 78,5 rotate=1
coredump_dir /var/spool/squid
refresh_pattern ^ftp: 1440 20% 10080
refresh_pattern ^gopher: 1440 0% 1440
refresh_pattern -i (/cgi-bin/|\?) 0 0% 0
refresh_pattern (Release|Packages(.gz)*)$      0       20%     2880
refresh_pattern . 0 20% 4320


I get no errors while starting Squid. Following are the logs when Squid starts:

2017/02/23 09:59:53 kid1| Set Current Directory to /var/spool/squid
2017/02/23 09:59:53 kid1| Starting Squid Cache version 3.5.12 for
x86_64-pc-linux-gnu...
2017/02/23 09:59:53 kid1| Service Name: squid
2017/02/23 09:59:53 kid1| Process ID 26236
2017/02/23 09:59:53 kid1| Process Roles: worker
2017/02/23 09:59:53 kid1| With 65535 file descriptors available
2017/02/23 09:59:53 kid1| Initializing IP Cache...
2017/02/23 09:59:53.756 kid1| 78,2| dns_internal.cc(1525) dnsInit:
idnsInit: attempt open DNS socket to: [::]
2017/02/23 09:59:53.756 kid1| 78,2| dns_internal.cc(1534) dnsInit:
idnsInit: attempt open DNS socket to: 0.0.0.0
2017/02/23 09:59:53.756 kid1| DNS Socket created at [::], FD 6
2017/02/23 09:59:53.756 kid1| DNS Socket created at 0.0.0.0, FD 7
2017/02/23 09:59:53.756 kid1| Adding nameserver 172.31.0.2 from /etc/resolv.conf
2017/02/23 09:59:53.756 kid1| 78,3| dns_internal.cc(321)
idnsAddNameserver: idnsAddNameserver: Added nameserver #0
(172.31.0.2:53)
2017/02/23 09:59:53.756 kid1| Adding domain
ap-south-1.compute.internal from /etc/resolv.conf
2017/02/23 09:59:53.756 kid1| 78,3| dns_internal.cc(350)
idnsAddPathComponent: idnsAddPathComponent: Added domain #0:
ap-south-1.compute.internal
2017/02/23 09:59:53.756 kid1| helperOpenServers: Starting 5/32
'ssl_crtd' processes
2017/02/23 09:59:53.775 kid1| 46,2| Format.cc(64) parse: got
definition '%>a/%>A %un %>rm myip=%la myport=%lp'
2017/02/23 09:59:53.775 kid1| 46,5| Token.cc(380) parse: scan for
possible Misc token
2017/02/23 09:59:53.775 kid1| 46,5| Token.cc(384) parse: scan for
possible 2C token
2017/02/23 09:59:53.775 kid1| 46,5| Token.cc(389) parse: scan for
possible 1C token
2017/02/23 09:59:53.775 kid1| 46,5| Token.cc(380) parse: scan for
possible Misc token
2017/02/23 09:59:53.775 kid1| 46,5| Token.cc(384) parse: scan for
possible 2C token
2017/02/23 09:59:53.775 kid1| 46,5| Token.cc(389) parse: scan for
possible 1C token
2017/02/23 09:59:53.775 kid1| 46,5| Token.cc(380) parse: scan for
possible Misc token
2017/02/23 09:59:53.775 kid1| 46,5| Token.cc(384) parse: scan for
possible 2C token
2017/02/23 09:59:53.775 kid1| 46,5| Token.cc(380) parse: scan for
possible Misc token
2017/02/23 09:59:53.775 kid1| 46,5| Token.cc(384) parse: scan for
possible 2C token
2017/02/23 09:59:53.775 kid1| 46,5| Token.cc(380) parse: scan for
possible Misc token
2017/02/23 09:59:53.775 kid1| 46,5| Token.cc(384) parse: scan for
possible 2C token
2017/02/23 09:59:53.775 kid1| 46,5| Token.cc(380) parse: scan for
possible Misc token
2017/02/23 09:59:53.775 kid1| 46,5| Token.cc(384) parse: scan for
possible 2C token
2017/02/23 09:59:53.775 kid1| 46,2| Format.cc(64) parse: got
definition '%>a/%>A %un %>rm myip=%la myport=%lp'
2017/02/23 09:59:53.775 kid1| 46,5| Token.cc(380) parse: scan for
possible Misc token
2017/02/23 09:59:53.775 kid1| 46,5| Token.cc(384) parse: scan for
possible 2C token
2017/02/23 09:59:53.775 kid1| 46,5| Token.cc(389) parse: scan for
possible 1C token
2017/02/23 09:59:53.775 kid1| 46,5| Token.cc(380) parse: scan for
possible Misc token
2017/02/23 09:59:53.775 kid1| 46,5| Token.cc(384) parse: scan for
possible 2C token
2017/02/23 09:59:53.775 kid1| 46,5| Token.cc(389) parse: scan for
possible 1C token
2017/02/23 09:59:53.775 kid1| 46,5| Token.cc(380) parse: scan for
possible Misc token
2017/02/23 09:59:53.775 kid1| 46,5| Token.cc(384) parse: scan for
possible 2C token
2017/02/23 09:59:53.775 kid1| 46,5| Token.cc(380) parse: scan for
possible Misc token
2017/02/23 09:59:53.775 kid1| 46,5| Token.cc(384) parse: scan for
possible 2C token
2017/02/23 09:59:53.775 kid1| 46,5| Token.cc(380) parse: scan for
possible Misc token
2017/02/23 09:59:53.775 kid1| 46,5| Token.cc(384) parse: scan for
possible 2C token
2017/02/23 09:59:53.775 kid1| 46,5| Token.cc(380) parse: scan for
possible Misc token
2017/02/23 09:59:53.775 kid1| 46,5| Token.cc(384) parse: scan for
possible 2C token
2017/02/23 09:59:53.775 kid1| Logfile: opening log
daemon:/var/log/squid/access.log
2017/02/23 09:59:53.775 kid1| Logfile Daemon: opening log
/var/log/squid/access.log
2017/02/23 09:59:53.779 kid1| 23,5| url.cc(43) urlInitialize:
urlInitialize: Initializing...
2017/02/23 09:59:53.779 kid1| Local cache digest enabled;
rebuild/rewrite every 3600/3600 sec
2017/02/23 09:59:53.779 kid1| Store logging disabled
2017/02/23 09:59:53.779 kid1| Swap maxSize 0 + 262144 KB, estimated
20164 objects
2017/02/23 09:59:53.779 kid1| Target number of buckets: 1008
2017/02/23 09:59:53.779 kid1| Using 8192 Store buckets
2017/02/23 09:59:53.779 kid1| Max Mem  size: 262144 KB
2017/02/23 09:59:53.779 kid1| Max Swap size: 0 KB
2017/02/23 09:59:53.779 kid1| Using Least Load store dir selection
2017/02/23 09:59:53.779 kid1| Set Current Directory to /var/spool/squid
2017/02/23 09:59:53.785 kid1| 23,3| url.cc(357) urlParse: urlParse:
Split URL 'http://ip-172-31-25-235:3128/squid-internal-static/icons/silk/image.png'
into proto='http', host='ip-172-31-25-235', port='3128',
path='/squid-internal-static/icons/silk/image.png'
2017/02/23 09:59:53.785 kid1| 23,3| url.cc(357) urlParse: urlParse:
Split URL 'http://ip-172-31-25-235:3128/squid-internal-static/icons/silk/page_white_text.png'
into proto='http', host='ip-172-31-25-235', port='3128',
path='/squid-internal-static/icons/silk/page_white_text.png'

****several urlParse logs like above. Removing them to shorten the
email. Further logs below...****

2017/02/23 09:59:53.815 kid1| Finished loading MIME types and icons.
2017/02/23 09:59:53.815 kid1| HTCP Disabled.
2017/02/23 09:59:53.815 kid1| Pinger socket opened on FD 25
2017/02/23 09:59:53.815 kid1| Squid plugin modules loaded: 0
2017/02/23 09:59:53.815 kid1| Adaptation support is off.
2017/02/23 09:59:53.815 kid1| Accepting SSL bumped HTTP Socket
connections at local=[::]:3128 remote=[::] FD 22 flags=9
2017/02/23 09:59:53.815 kid1| Accepting NAT intercepted SSL bumped
HTTPS Socket connections at local=[::]:3129 remote=[::] FD 23 flags=41
2017/02/23 09:59:53| pinger: Initialising ICMP pinger ...
2017/02/23 09:59:53| pinger: ICMP socket opened.
2017/02/23 09:59:53| pinger: ICMPv6 socket opened
2017/02/23 09:59:54 kid1| storeLateRelease: released 0 objects



I tested this setup by providing proxy details to Firefox. Firefox was
able to show HTTP websites but when I tried to open an HTTPS website I
got following error:

2017/02/23 11:00:50 kid1| ERROR: NF getsockopt(ORIGINAL_DST) failed on
local=172.31.25.235:3129 remote=182.72.78.122:50655 FD 7 flags=33:
(92) Protocol not available
2017/02/23 11:00:50 kid1| ERROR: NAT/TPROXY lookup failed to locate
original IPs on local=172.31.25.235:3129 remote=182.72.78.122:50655 FD
7 flags=33
2017/02/23 11:00:50 kid1| ERROR: NF getsockopt(ORIGINAL_DST) failed on
local=172.31.25.235:3129 remote=182.72.78.122:50656 FD 7 flags=33:
(92) Protocol not available
2017/02/23 11:00:50 kid1| ERROR: NAT/TPROXY lookup failed to locate
original IPs on local=172.31.25.235:3129 remote=182.72.78.122:50656 FD
7 flags=33
2017/02/23 11:00:50 kid1| ERROR: NF getsockopt(ORIGINAL_DST) failed on
local=172.31.25.235:3129 remote=182.72.78.122:50657 FD 7 flags=33:
(92) Protocol not available
2017/02/23 11:00:50 kid1| ERROR: NAT/TPROXY lookup failed to locate
original IPs on local=172.31.25.235:3129 remote=182.72.78.122:50657 FD
7 flags=33

I googled this error and found this mail thread which had similar problems:
http://squid-web-proxy-cache.1019090.n4.nabble.com/NAT-TPROXY-lookup-failed-to-locate-original-IPs-td4675464.html

I found this link from the above thread. I modified the steps for
HTTPS from the below link:
http://wiki.squid-cache.org/ConfigExamples/Intercept/LinuxDnat

Now my sysctl.conf is:

net.ipv4.conf.all.rp_filter=0
net.ipv4.ip_forward = 1
net.ipv4.conf.default.rp_filter = 0
net.ipv4.conf.default.accept_source_route = 0

My iptables -t nat -L result:

Chain PREROUTING (policy ACCEPT)
target     prot opt source               destination
ACCEPT     tcp  --  ec2-35-154-101-8.ap-south-1.compute.amazonaws.com
anywhere             tcp dpt:https
DNAT       tcp  --  anywhere             anywhere             tcp
dpt:https to:35.154.101.8:3129

Chain INPUT (policy ACCEPT)
target     prot opt source               destination

Chain OUTPUT (policy ACCEPT)
target     prot opt source               destination

Chain POSTROUTING (policy ACCEPT)
target     prot opt source               destination
MASQUERADE  all  --  anywhere             anywhere


Once this was done, I tried to hit HTTPS website from Firefox and now
I get connection timeout error. Nothing shows in syslog, access.log or
cache.log. Could you please help me resolve this.

Thanks.


From squid3 at treenet.co.nz  Thu Feb 23 12:35:16 2017
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Fri, 24 Feb 2017 01:35:16 +1300
Subject: [squid-users] Squid 4.0.18 question about directives
In-Reply-To: <1487849002972-4681599.post@n4.nabble.com>
References: <1487849002972-4681599.post@n4.nabble.com>
Message-ID: <4cdda373-3926-9c20-82b8-1448bae7db80@treenet.co.nz>

On 24/02/2017 12:23 a.m., Imaginovskiy wrote:
> Hi All, 
> 
> I'm in the process of upgrading some clients to the latest squid proxy
> version. Coming from version 3.5 I used to specify backends in squid.conf as
> follows;
> 
> cache_peer site1.domain.com parent 443 0 no-query originserver
> name=server_site1 ssl proxy-only front-end-https=on login=PASS
> cache_peer_domain server_site1 site1.domain.com
> 
> cache_peer site2.domain.com parent 443 0 no-query originserver
> name=server_site2 ssl proxy-only front-end-https=on login=PASS
> cache_peer_domain server_site2 site2.domain.com
> 
> But in Squid 4 it looks a little confusing, cache_peer_domain is superseded
> by cache_peer_access but looking at the documentation it looks like
> cache_peer_access isn't quite the same as cache_peer_domain.

The syntax is a little different, but the *_domain behaviour is easily
replicated. If you want anything more you can add it easily to the
*_access rules.

The equivalent to your old config is this (which works in Squid-3 as
well, so you can test before the upgrade):

 cache_peer site1.domain.com parent 443 0 no-query originserver \
    name=server_site1 ssl proxy-only front-end-https=on login=PASS

 acl site1 dstdomain site1.domain.com
 cache_peer_access server_site1 allow site1
 # http_access allow site1


 cache_peer site2.domain.com parent 443 0 no-query originserver \
    name=server_site2 ssl proxy-only front-end-https=on login=PASS

 acl site2 dstdomain site1.domain.com
 cache_peer_access server_site2 allow site2
 # http_access allow site2



If you don't need this config to load in Squid-3 anymore I suggest
adding tls-cafile= option with the public cert of the root CA that
signed that peers cert. Then you can use tls-default-ca=off which will
protect against any problems with the irrelevant 'globally trusted' CAs
affecting your peer traffic.

Amos



From Sebastien.Boulianne at cpu.ca  Thu Feb 23 19:02:26 2017
From: Sebastien.Boulianne at cpu.ca (Sebastien.Boulianne at cpu.ca)
Date: Thu, 23 Feb 2017 14:02:26 -0500
Subject: [squid-users] My Squid stop working WITHOUT any reason.
Message-ID: <5FE0959288C73D448BB44CB7E9CC320F5E30930D73@CPUMAIL2.cpu.qc.ca>

Hi all,

Does it has changes in the configuration from Squid 3.5.20 vs 3.5.24 ?
My Squid stop working for no reason BUT i dont know why. To fix that, I have to restart the Squid process.

Do you have any idea how to know what it causes that ?

Thanks you for your answer.

S?bastien
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20170223/04ba3bb7/attachment.htm>

From eliezer at ngtech.co.il  Thu Feb 23 19:29:22 2017
From: eliezer at ngtech.co.il (Eliezer  Croitoru)
Date: Thu, 23 Feb 2017 21:29:22 +0200
Subject: [squid-users] My Squid stop working WITHOUT any reason.
In-Reply-To: <5FE0959288C73D448BB44CB7E9CC320F5E30930D73@CPUMAIL2.cpu.qc.ca>
References: <5FE0959288C73D448BB44CB7E9CC320F5E30930D73@CPUMAIL2.cpu.qc.ca>
Message-ID: <1548e01d28e0b$266e4130$734ac390$@ngtech.co.il>

What do you mean by stopped working?
100% CPU?
If so please try to add as the last line of squid:
cache deny all

and see if it happens again.

All The Bests,
Eliezer

----
http://ngtech.co.il/lmgtfy/
Linux System Administrator
Mobile: +972-5-28704261
Email: eliezer at ngtech.co.il


From: squid-users [mailto:squid-users-bounces at lists.squid-cache.org] On
Behalf Of Sebastien.Boulianne at cpu.ca
Sent: Thursday, February 23, 2017 9:02 PM
To: squid-users at lists.squid-cache.org
Subject: [squid-users] My Squid stop working WITHOUT any reason.

Hi all,

Does it has changes in the configuration from Squid 3.5.20 vs 3.5.24 ?
My Squid stop working for no reason BUT i dont know why. To fix that, I have
to restart the Squid process.

Do you have any idea how to know what it causes that ?

Thanks you for your answer.

S?bastien



From Sebastien.Boulianne at cpu.ca  Thu Feb 23 19:36:56 2017
From: Sebastien.Boulianne at cpu.ca (Sebastien.Boulianne at cpu.ca)
Date: Thu, 23 Feb 2017 14:36:56 -0500
Subject: [squid-users] My Squid stop working WITHOUT any reason.
In-Reply-To: <1548e01d28e0b$266e4130$734ac390$@ngtech.co.il>
References: <5FE0959288C73D448BB44CB7E9CC320F5E30930D73@CPUMAIL2.cpu.qc.ca>
 <1548e01d28e0b$266e4130$734ac390$@ngtech.co.il>
Message-ID: <5FE0959288C73D448BB44CB7E9CC320F5E30930D7E@CPUMAIL2.cpu.qc.ca>

Hi Eli,

It stops working like when I tried to access a website, I got a error connection reset by peer.

I will try to add the cache deny all at the end of the file.

Thanks,

S?bastien.
-----Message d'origine-----
De?: Eliezer Croitoru [mailto:eliezer at ngtech.co.il] 
Envoy??: 23 f?vrier 2017 14:29
??: Sebastien Boulianne <Sebastien.Boulianne at cpu.ca>; squid-users at lists.squid-cache.org
Objet?: RE: [squid-users] My Squid stop working WITHOUT any reason.

What do you mean by stopped working?
100% CPU?
If so please try to add as the last line of squid:
cache deny all

and see if it happens again.

All The Bests,
Eliezer

----
http://ngtech.co.il/lmgtfy/
Linux System Administrator
Mobile: +972-5-28704261
Email: eliezer at ngtech.co.il


From: squid-users [mailto:squid-users-bounces at lists.squid-cache.org] On Behalf Of Sebastien.Boulianne at cpu.ca
Sent: Thursday, February 23, 2017 9:02 PM
To: squid-users at lists.squid-cache.org
Subject: [squid-users] My Squid stop working WITHOUT any reason.

Hi all,

Does it has changes in the configuration from Squid 3.5.20 vs 3.5.24 ?
My Squid stop working for no reason BUT i dont know why. To fix that, I have to restart the Squid process.

Do you have any idea how to know what it causes that ?

Thanks you for your answer.

S?bastien



From rousskov at measurement-factory.com  Thu Feb 23 20:14:52 2017
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Thu, 23 Feb 2017 13:14:52 -0700
Subject: [squid-users] My Squid stop working WITHOUT any reason.
In-Reply-To: <5FE0959288C73D448BB44CB7E9CC320F5E30930D73@CPUMAIL2.cpu.qc.ca>
References: <5FE0959288C73D448BB44CB7E9CC320F5E30930D73@CPUMAIL2.cpu.qc.ca>
Message-ID: <3ee9f54f-3484-1d94-9a6c-ee79dad91793@measurement-factory.com>

On 02/23/2017 12:02 PM, Sebastien.Boulianne at cpu.ca wrote:

> Does it has changes in the configuration from Squid 3.5.20 vs 3.5.24 ?

I see one configuration change clearly identified in ChangeLog:

> - TLS: Make key= before cert= an error instead of quietly hiding the issue

There may be more -- I have not checked closely.


> My Squid stop working for no reason BUT i dont know why. To fix that, I
> have to restart the Squid process.

When "Squid stopped working",

* Was Squid process still running?
* What was the Squid process state (running, sleeping, blocked, etc.)?
* Was the Squid process CPU usage at or very close to 100%?
* Was the Squid process memory usage abnormal?
* Was anything potentially relevant logged at the end of cache.log?
* Was anything potentially relevant logged in system logs?
* Did telnet from localhost to Squid listening port work?
* Was Squid responding to local cache manager requests?
* Did curl/wget/squidclient from localhost to Squid listening port work?

I realize that you may not know the answer to many of these questions,
but you can consult this list when/if this happens again, so that you
can troubleshoot the problem yourself or supply relevant information to
others on the mailing list.

It is very rare for Squid to "stop working" without at least some
visible symptoms of the underlying problem.


HTH,

Alex.



From tuser6485 at gmail.com  Fri Feb 24 06:51:58 2017
From: tuser6485 at gmail.com (Test User)
Date: Fri, 24 Feb 2017 12:21:58 +0530
Subject: [squid-users] SSL-Bump: NAT/TPROXY lookup failed to locate original
	IPs
Message-ID: <CAPhcQuJU-ctRMokvU6kg+RqkfXDOU_0y+o+fEUADdoX4AstXaQ@mail.gmail.com>

Hi,
Sorry I am asking this question again. I am trying to setup HTTPS
proxy using ssl-bump. I have followed
steps mentioned in:
http://wiki.squid-cache.org/ConfigExamples/Intercept/SslBumpExplicit

Following are Squid setup details:

Squid Cache: Version 3.5.12
Service Name: squid
Ubuntu linux

configure options:  '--build=x86_64-linux-gnu' '--prefix=/usr'
'--includedir=${prefix}/include' '--mandir=${prefix}/share/man'
'--infodir=${prefix}/share/info' '--sysconfdir=/etc'
'--localstatedir=/var' '--libexecdir=${prefix}/lib/squid3'
'--srcdir=.' '--disable-maintainer-mode'
'--disable-dependency-tracking' '--disable-silent-rules'
'BUILDCXXFLAGS=-g -O2 -fPIE -fstack-protector-strong -Wformat
-Werror=format-security -Wl,-Bsymbolic-functions -fPIE -pie
-Wl,-z,relro -Wl,-z,now' '--datadir=/usr/share/squid'
'--sysconfdir=/etc/squid' '--libexecdir=/usr/lib/squid'
'--mandir=/usr/share/man' '--enable-inline' '--disable-arch-native'
'--enable-async-io=8' '--enable-storeio=ufs,aufs,diskd,rock'
'--enable-removal-policies=lru,heap' '--enable-delay-pools'
'--enable-cache-digests' '--enable-icap-client'
'--enable-follow-x-forwarded-for'
'--enable-auth-basic=DB,fake,getpwnam,LDAP,NCSA,NIS,PAM,POP3,RADIUS,SASL,SMB'
'--enable-auth-digest=file,LDAP'
'--enable-auth-negotiate=kerberos,wrapper'
'--enable-auth-ntlm=fake,smb_lm'
'--enable-external-acl-helpers=file_userip,kerberos_ldap_group,LDAP_group,session,SQL_session,unix_group,wbinfo_group'
'--enable-url-rewrite-helpers=fake' '--enable-eui' '--enable-esi'
'--enable-icmp' '--enable-zph-qos' '--enable-ecap' '--with-openssl'
'--enable-ssl-crtd' '--disable-translation'
'--with-swapdir=/var/spool/squid' '--with-logdir=/var/log/squid'
'--with-pidfile=/var/run/squid.pid' '--with-filedescriptors=65536'
'--with-large-files' '--with-default-user=proxy'
'--enable-build-info=Ubuntu linux' '--enable-linux-netfilter'
'build_alias=x86_64-linux-gnu' 'CFLAGS=-g -O2 -fPIE
-fstack-protector-strong -Wformat -Werror=format-security -Wall'
'LDFLAGS=-Wl,-Bsymbolic-functions -fPIE -pie -Wl,-z,relro -Wl,-z,now'
'CPPFLAGS=-Wdate-time -D_FORTIFY_SOURCE=2' 'CXXFLAGS=-g -O2 -fPIE
-fstack-protector-strong -Wformat -Werror=format-security'


Following is my squid.conf file:

acl SSL_ports port 443
acl Safe_ports port 80 # http
acl Safe_ports port 21 # ftp
acl Safe_ports port 443 # https
acl Safe_ports port 70 # gopher
acl Safe_ports port 210 # wais
acl Safe_ports port 1025-65535 # unregistered ports
acl Safe_ports port 280 # http-mgmt
acl Safe_ports port 488 # gss-http
acl Safe_ports port 591 # filemaker
acl Safe_ports port 777 # multiling http
acl CONNECT method CONNECT
acl step1 at_step SslBump1
http_access deny !Safe_ports
http_access deny CONNECT !SSL_ports
http_access allow localhost manager
http_access deny manager
http_access allow localhost
http_access allow all
http_port 3128 ssl-bump \
  cert=/etc/squid/ssl_cert/squidCA.pem \
  generate-host-certificates=on dynamic_cert_mem_cache_size=4MB
https_port 3129 intercept ssl-bump generate-host-certificates=on \
dynamic_cert_mem_cache_size=4MB cert=/etc/squid/ssl_cert/squidCA.pem \
dhparams=/etc/squid/ssl_cert/dhparam.pem
sslproxy_options NO_SSLv2,NO_SSLv3,SINGLE_DH_USE
sslproxy_cipher
EECDH+ECDSA+AESGCM:EECDH+aRSA+AESGCM:EECDH+ECDSA+SHA384:EECDH+ECDSA+SHA256:EECDH+aRSA+SHA384:EECDH+aRSA+SHA256:EECDH+aRSA+RC4:EECDH:EDH+aRSA:!RC4:!aNULL:!eNULL:!LOW:!3DES:!MD5:!EXP:!PSK:!SRP:!DSS
sslcrtd_program /usr/lib/squid/ssl_crtd -s /var/spool/squid3_ssldb -M 4MB
debug_options ALL,1 3,5 4,5 11,5 17,5 23,5 46,5 78,5 rotate=1
coredump_dir /var/spool/squid
refresh_pattern ^ftp: 1440 20% 10080
refresh_pattern ^gopher: 1440 0% 1440
refresh_pattern -i (/cgi-bin/|\?) 0 0% 0
refresh_pattern (Release|Packages(.gz)*)$      0       20%     2880
refresh_pattern . 0 20% 4320


I get no errors while starting Squid. Following are the logs when Squid starts:

2017/02/23 09:59:53 kid1| Set Current Directory to /var/spool/squid
2017/02/23 09:59:53 kid1| Starting Squid Cache version 3.5.12 for
x86_64-pc-linux-gnu...
2017/02/23 09:59:53 kid1| Service Name: squid
2017/02/23 09:59:53 kid1| Process ID 26236
2017/02/23 09:59:53 kid1| Process Roles: worker
2017/02/23 09:59:53 kid1| With 65535 file descriptors available
2017/02/23 09:59:53 kid1| Initializing IP Cache...
2017/02/23 09:59:53.756 kid1| 78,2| dns_internal.cc(1525) dnsInit:
idnsInit: attempt open DNS socket to: [::]
2017/02/23 09:59:53.756 kid1| 78,2| dns_internal.cc(1534) dnsInit:
idnsInit: attempt open DNS socket to: 0.0.0.0
2017/02/23 09:59:53.756 kid1| DNS Socket created at [::], FD 6
2017/02/23 09:59:53.756 kid1| DNS Socket created at 0.0.0.0, FD 7
2017/02/23 09:59:53.756 kid1| Adding nameserver 172.31.0.2 from /etc/resolv.conf
2017/02/23 09:59:53.756 kid1| 78,3| dns_internal.cc(321)
idnsAddNameserver: idnsAddNameserver: Added nameserver #0
(172.31.0.2:53)
2017/02/23 09:59:53.756 kid1| Adding domain
ap-south-1.compute.internal from /etc/resolv.conf
2017/02/23 09:59:53.756 kid1| 78,3| dns_internal.cc(350)
idnsAddPathComponent: idnsAddPathComponent: Added domain #0:
ap-south-1.compute.internal
2017/02/23 09:59:53.756 kid1| helperOpenServers: Starting 5/32
'ssl_crtd' processes
2017/02/23 09:59:53.775 kid1| 46,2| Format.cc(64) parse: got
definition '%>a/%>A %un %>rm myip=%la myport=%lp'
2017/02/23 09:59:53.775 kid1| 46,5| Token.cc(380) parse: scan for
possible Misc token
2017/02/23 09:59:53.775 kid1| 46,5| Token.cc(384) parse: scan for
possible 2C token
2017/02/23 09:59:53.775 kid1| 46,5| Token.cc(389) parse: scan for
possible 1C token
2017/02/23 09:59:53.775 kid1| 46,5| Token.cc(380) parse: scan for
possible Misc token
2017/02/23 09:59:53.775 kid1| 46,5| Token.cc(384) parse: scan for
possible 2C token
2017/02/23 09:59:53.775 kid1| 46,5| Token.cc(389) parse: scan for
possible 1C token
2017/02/23 09:59:53.775 kid1| 46,5| Token.cc(380) parse: scan for
possible Misc token
2017/02/23 09:59:53.775 kid1| 46,5| Token.cc(384) parse: scan for
possible 2C token
2017/02/23 09:59:53.775 kid1| 46,5| Token.cc(380) parse: scan for
possible Misc token
2017/02/23 09:59:53.775 kid1| 46,5| Token.cc(384) parse: scan for
possible 2C token
2017/02/23 09:59:53.775 kid1| 46,5| Token.cc(380) parse: scan for
possible Misc token
2017/02/23 09:59:53.775 kid1| 46,5| Token.cc(384) parse: scan for
possible 2C token
2017/02/23 09:59:53.775 kid1| 46,5| Token.cc(380) parse: scan for
possible Misc token
2017/02/23 09:59:53.775 kid1| 46,5| Token.cc(384) parse: scan for
possible 2C token
2017/02/23 09:59:53.775 kid1| 46,2| Format.cc(64) parse: got
definition '%>a/%>A %un %>rm myip=%la myport=%lp'
2017/02/23 09:59:53.775 kid1| 46,5| Token.cc(380) parse: scan for
possible Misc token
2017/02/23 09:59:53.775 kid1| 46,5| Token.cc(384) parse: scan for
possible 2C token
2017/02/23 09:59:53.775 kid1| 46,5| Token.cc(389) parse: scan for
possible 1C token
2017/02/23 09:59:53.775 kid1| 46,5| Token.cc(380) parse: scan for
possible Misc token
2017/02/23 09:59:53.775 kid1| 46,5| Token.cc(384) parse: scan for
possible 2C token
2017/02/23 09:59:53.775 kid1| 46,5| Token.cc(389) parse: scan for
possible 1C token
2017/02/23 09:59:53.775 kid1| 46,5| Token.cc(380) parse: scan for
possible Misc token
2017/02/23 09:59:53.775 kid1| 46,5| Token.cc(384) parse: scan for
possible 2C token
2017/02/23 09:59:53.775 kid1| 46,5| Token.cc(380) parse: scan for
possible Misc token
2017/02/23 09:59:53.775 kid1| 46,5| Token.cc(384) parse: scan for
possible 2C token
2017/02/23 09:59:53.775 kid1| 46,5| Token.cc(380) parse: scan for
possible Misc token
2017/02/23 09:59:53.775 kid1| 46,5| Token.cc(384) parse: scan for
possible 2C token
2017/02/23 09:59:53.775 kid1| 46,5| Token.cc(380) parse: scan for
possible Misc token
2017/02/23 09:59:53.775 kid1| 46,5| Token.cc(384) parse: scan for
possible 2C token
2017/02/23 09:59:53.775 kid1| Logfile: opening log
daemon:/var/log/squid/access.log
2017/02/23 09:59:53.775 kid1| Logfile Daemon: opening log
/var/log/squid/access.log
2017/02/23 09:59:53.779 kid1| 23,5| url.cc(43) urlInitialize:
urlInitialize: Initializing...
2017/02/23 09:59:53.779 kid1| Local cache digest enabled;
rebuild/rewrite every 3600/3600 sec
2017/02/23 09:59:53.779 kid1| Store logging disabled
2017/02/23 09:59:53.779 kid1| Swap maxSize 0 + 262144 KB, estimated
20164 objects
2017/02/23 09:59:53.779 kid1| Target number of buckets: 1008
2017/02/23 09:59:53.779 kid1| Using 8192 Store buckets
2017/02/23 09:59:53.779 kid1| Max Mem  size: 262144 KB
2017/02/23 09:59:53.779 kid1| Max Swap size: 0 KB
2017/02/23 09:59:53.779 kid1| Using Least Load store dir selection
2017/02/23 09:59:53.779 kid1| Set Current Directory to /var/spool/squid
2017/02/23 09:59:53.785 kid1| 23,3| url.cc(357) urlParse: urlParse:
Split URL 'http://ip-172-31-25-235:3128/squid-internal-static/icons/silk/image.png'
into proto='http', host='ip-172-31-25-235', port='3128',
path='/squid-internal-static/icons/silk/image.png'
2017/02/23 09:59:53.785 kid1| 23,3| url.cc(357) urlParse: urlParse:
Split URL 'http://ip-172-31-25-235:3128/squid-internal-static/icons/silk/page_white_text.png'
into proto='http', host='ip-172-31-25-235', port='3128',
path='/squid-internal-static/icons/silk/page_white_text.png'

****several urlParse logs like above. Removing them to shorten the
email. Further logs below...****

2017/02/23 09:59:53.815 kid1| Finished loading MIME types and icons.
2017/02/23 09:59:53.815 kid1| HTCP Disabled.
2017/02/23 09:59:53.815 kid1| Pinger socket opened on FD 25
2017/02/23 09:59:53.815 kid1| Squid plugin modules loaded: 0
2017/02/23 09:59:53.815 kid1| Adaptation support is off.
2017/02/23 09:59:53.815 kid1| Accepting SSL bumped HTTP Socket
connections at local=[::]:3128 remote=[::] FD 22 flags=9
2017/02/23 09:59:53.815 kid1| Accepting NAT intercepted SSL bumped
HTTPS Socket connections at local=[::]:3129 remote=[::] FD 23 flags=41
2017/02/23 09:59:53| pinger: Initialising ICMP pinger ...
2017/02/23 09:59:53| pinger: ICMP socket opened.
2017/02/23 09:59:53| pinger: ICMPv6 socket opened
2017/02/23 09:59:54 kid1| storeLateRelease: released 0 objects



I tested this setup by providing proxy details to Firefox. Firefox was
able to show HTTP websites but when I tried to open an HTTPS website I
got following error:

2017/02/23 11:00:50 kid1| ERROR: NF getsockopt(ORIGINAL_DST) failed on
local=172.31.25.235:3129 remote=182.72.78.122:50655 FD 7 flags=33:
(92) Protocol not available
2017/02/23 11:00:50 kid1| ERROR: NAT/TPROXY lookup failed to locate
original IPs on local=172.31.25.235:3129 remote=182.72.78.122:50655 FD
7 flags=33
2017/02/23 11:00:50 kid1| ERROR: NF getsockopt(ORIGINAL_DST) failed on
local=172.31.25.235:3129 remote=182.72.78.122:50656 FD 7 flags=33:
(92) Protocol not available
2017/02/23 11:00:50 kid1| ERROR: NAT/TPROXY lookup failed to locate
original IPs on local=172.31.25.235:3129 remote=182.72.78.122:50656 FD
7 flags=33
2017/02/23 11:00:50 kid1| ERROR: NF getsockopt(ORIGINAL_DST) failed on
local=172.31.25.235:3129 remote=182.72.78.122:50657 FD 7 flags=33:
(92) Protocol not available
2017/02/23 11:00:50 kid1| ERROR: NAT/TPROXY lookup failed to locate
original IPs on local=172.31.25.235:3129 remote=182.72.78.122:50657 FD
7 flags=33

I googled this error and found this mail thread which had similar problems:
http://squid-web-proxy-cache.1019090.n4.nabble.com/NAT-TPROXY-lookup-failed-to-locate-original-IPs-td4675464.html

I found this link from the above thread. I modified the steps for
HTTPS from the below link:
http://wiki.squid-cache.org/ConfigExamples/Intercept/LinuxDnat

Now my sysctl.conf is:

net.ipv4.conf.all.rp_filter=0
net.ipv4.ip_forward = 1
net.ipv4.conf.default.rp_filter = 0
net.ipv4.conf.default.accept_source_route = 0

My iptables -t nat -L result:

Chain PREROUTING (policy ACCEPT)
target     prot opt source               destination
ACCEPT     tcp  --  ec2-35-154-101-8.ap-south-1.compute.amazonaws.com
anywhere             tcp dpt:https
DNAT       tcp  --  anywhere             anywhere             tcp
dpt:https to:35.154.101.8:3129

Chain INPUT (policy ACCEPT)
target     prot opt source               destination

Chain OUTPUT (policy ACCEPT)
target     prot opt source               destination

Chain POSTROUTING (policy ACCEPT)
target     prot opt source               destination
MASQUERADE  all  --  anywhere             anywhere


Once this was done, I tried to hit HTTPS website from Firefox and now
I get connection timeout error. Nothing shows in syslog, access.log or
cache.log. Could you please help me resolve this.

Thanks,
Michael


From squid3 at treenet.co.nz  Fri Feb 24 10:50:48 2017
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Fri, 24 Feb 2017 23:50:48 +1300
Subject: [squid-users] SSL-Bump: NAT/TPROXY lookup failed to locate
 original IPs
In-Reply-To: <CAPhcQuJU-ctRMokvU6kg+RqkfXDOU_0y+o+fEUADdoX4AstXaQ@mail.gmail.com>
References: <CAPhcQuJU-ctRMokvU6kg+RqkfXDOU_0y+o+fEUADdoX4AstXaQ@mail.gmail.com>
Message-ID: <2b480290-3d14-df4d-cb03-3acf9338e0f1@treenet.co.nz>

On 24/02/2017 7:51 p.m., Test User wrote:
> Hi,
> Sorry I am asking this question again. 

Please dont. It has only been 7 hours. Some things (like this) are
tricky and take a while to figure out even what to suggest looking at.

Amos



From squid3 at treenet.co.nz  Fri Feb 24 11:51:00 2017
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sat, 25 Feb 2017 00:51:00 +1300
Subject: [squid-users] help me optimising caching or increasing hit ratio
In-Reply-To: <B55B4972-F909-40A6-B4E4-CF0EE64490AA@netstream.ps>
References: <593D9177-82FF-40AE-AE3A-61E3517854AC@netstream.ps>
 <f8e2e882-b9b7-6461-3233-dddd5fbcba9b@treenet.co.nz>
 <B55B4972-F909-40A6-B4E4-CF0EE64490AA@netstream.ps>
Message-ID: <a64f683a-e96c-8d3b-ccc1-1bb24d0c3848@treenet.co.nz>

On 24/02/2017 4:09 a.m., --Ahmad-- wrote:
> amos i told you i have squid version 3.5.2 version 
> but no luck with caching at all 
> 

I know. All of the ChangeLog entries below are fixing bugs or add cache
abilities to reduce MISS.

You want improved HIT ratio in todays HTTP traffic? you *need* the below
changes. It will also help with debugging if the problem remains, since
all those bugs are no longer potential causes to investigate.

You could try to patch them all in. But if you are compiling anyway, you
might as well just compile the latest anyway and get all the other major
bugs that have been fixed.


Changes to squid-3.5.23 (16 Dec 2016):

  - Bug 4169: HIT marked as MISS when If-None-Match does not match
  - Bug 3940 partial: hostHeaderVerify failures MISS when they should be HIT
  - Bug 3533: Cache still valid after HTTP/1.1 303 See Other
  - Bug 3379: Combination of If-Match and a Cache Hit result in TCP
Connection Failure
  - Bug 2258: bypassing cache but not destroying cache entry
  - HTTP/1.1: make Vary:* objects cacheable
  - HTTP/1.1: Add registered codes entry for new 103 (Early Hints)
status code

Changes to squid-3.5.22 (09 Oct 2016):

  - Bug 4471: revalidation does not work when expired cached object
lacks Last-Modified
  - Bug 2833: Collapse internal revalidation requests (SMP-unaware caches)
  - HTTP: MUST ignore a [revalidation] response with an older Date header

Changes to squid-3.5.21 (08 Sep 2016):

  - Bug 4428: mal-formed Cache-Control:stale-if-error header
  - HTTP/1.1: MUST always revalidate Cache-Control:no-cache responses
  - HTTP/1.1: do not allow Proxy-Connection to override Connection header

Changes to squid-3.5.18 (06 May 2016):

  - Bug 4501: HTTP/1.1: normalize Host header
  - Bug 4498: URL-unescape the login-info after extraction from URI

Changes to squid-3.5.17 (20 Apr 2016):

  - Regression Bug 4481: varyEvaluateMatch: Oops. Not a Vary match on
second attempt

Changes to squid-3.5.16 (02 Apr 2016):

  - Bug 2831: Cache-control: max-age not sent on TCP_IMS_HIT/304
  - RFC 7725: Add registry entry for 451 status text

Changes to squid-3.5.15 (23 Feb 2016):

  - Better handling of huge response headers. Fewer incorrect "Bug
#3279" messages.

Changes to squid-3.5.12 (28 Nov 2015):

  - Bug 4374: refresh_pattern config parser (%)

Changes to squid-3.5.11 (01 Nov 2015):

  - Bug 4279: No response from proxy for FTP-download of non-existing file
  - Fix incorrect authentication headers on cache digest requests

Changes to squid-3.5.10 (01 Oct 2015):

  - Regression Bug 4326: base64 binary encoder rejects data beginning
with nil byte

Changes to squid-3.5.8 (02 Sep 2015):

  - Bug 3553: cache_swap_high ignored and maxCapacity used instead
  - Fix truncated body length when RESPMOD service aborts
  - Reject non-chunked HTTP messages with conflicting Content-Length values

Changes to squid-3.5.4 (01 May 2015):

  - Bug 3775: Disable HTTP/1.1 pipeline feature for pinned connections
  - Fix Negotiate/Kerberos authentication request size exceeds output
buffer size

Changes to squid-3.5.3 (28 Mar 2015):

  - Regression Bug 4206: Incorrect connection close on expect:100-continue



Amos



From Sebastien.Boulianne at cpu.ca  Fri Feb 24 15:07:04 2017
From: Sebastien.Boulianne at cpu.ca (Sebastien.Boulianne at cpu.ca)
Date: Fri, 24 Feb 2017 10:07:04 -0500
Subject: [squid-users] squid squid[2541]: temporary disabling (Not Found)
Message-ID: <5FE0959288C73D448BB44CB7E9CC320F5E30930DBF@CPUMAIL2.cpu.qc.ca>

Hi all,

What squid squid[2541]: temporary disabling (Not Found) mean ?
Is it a way to fix that warning ?

Im running Squid as a reverse proxy.

Thanks.

S?bastien



-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20170224/4bb63a53/attachment.htm>

From squid3 at treenet.co.nz  Sat Feb 25 07:02:23 2017
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sat, 25 Feb 2017 20:02:23 +1300
Subject: [squid-users] Squid 3.5.24 - Url_rewrite with ssl_bump in
 Transparent Mode
In-Reply-To: <ba934f59-1cfe-617e-aa3b-9c3be60730ce@gmail.com>
References: <ba934f59-1cfe-617e-aa3b-9c3be60730ce@gmail.com>
Message-ID: <d84dfac1-a917-3568-4816-80294dcf196d@treenet.co.nz>

On 20/02/2017 8:33 p.m., Test1964 wrote:
> Hi,
> 
> When I exclude some sites (like banks)  with ssl_bump peek/splice that
> works well, Got a new problem that
> sites (that I exclude)  can not be blocked using Url_Rewrite.
> I use Url_rewrite to block sites based on User IP and all all other
> sites(no in exclude list) it working very well.
> 
> How to fix it? Or this another way to block excluded sites in ssl_bump
> based on User IP?
> 

Block things using an access control mechanism. That is what access
controls (ACLs, http_access, deny_info) are for.

If your blocking conditions are so complex or dynamic that Squid ACLs
are not able to cope; then use an external_acl_type helper to give the
allow/deny result and also consider if you can simplify the access policies.


Do not use a URL routing mechanism to do 'access control' operation.
Changing the destination of a message can *only* work if the relevant
security is equivalent for both paths the message can take.

re-write has the _appearance_ of working in HTTP because plain-text is
built on complete trust of the proxy. HTTPS is not, it contains
mechanisms to verify the honesty which is preventing your abuse of HTTP.


NP: If you were doing a proper HTTP *redirect* (with appropriate 30x
status codes) it would work, but still wrong to do access control that way.

Amos



From squid3 at treenet.co.nz  Sat Feb 25 07:21:56 2017
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sat, 25 Feb 2017 20:21:56 +1300
Subject: [squid-users] squid squid[2541]: temporary disabling (Not Found)
In-Reply-To: <5FE0959288C73D448BB44CB7E9CC320F5E30930DBF@CPUMAIL2.cpu.qc.ca>
References: <5FE0959288C73D448BB44CB7E9CC320F5E30930DBF@CPUMAIL2.cpu.qc.ca>
Message-ID: <0f86bfbe-cbf1-4443-9e24-fbeb1f0f6858@treenet.co.nz>

On 25/02/2017 4:07 a.m., Sebastien.Boulianne at cpu.ca wrote:
> Hi all,
> 
> What squid squid[2541]: temporary disabling (Not Found) mean ?
> Is it a way to fix that warning ?
> 

I'm not seeing any message with that text in the Squid code. Can you
post the exact and whole log line please? and say which log it is coming
from.

Amos



From squid3 at treenet.co.nz  Sat Feb 25 07:35:16 2017
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sat, 25 Feb 2017 20:35:16 +1300
Subject: [squid-users] Squid 3.5.24 - Url_rewrite with ssl_bump in
 Transparent Mode
In-Reply-To: <19253d41-000f-271c-30a6-08bd9bdc9a40@gmail.com>
References: <ba934f59-1cfe-617e-aa3b-9c3be60730ce@gmail.com>
 <d84dfac1-a917-3568-4816-80294dcf196d@treenet.co.nz>
 <19253d41-000f-271c-30a6-08bd9bdc9a40@gmail.com>
Message-ID: <11e767fc-fe35-445d-92bd-8cb1826902cb@treenet.co.nz>

On 25/02/2017 8:28 p.m., Test1964 wrote:
> Hi,
> 
> How  Can I use "external_acl_type helper "  to block some sites (urls) 
> based on user source ip ?
> If there is way, can I block sites even I exclude them in ssl_bump(like
> banks)?
> 

How is your URL-rewrite helper deciding to 'block' ?

PS. please keep the thread on-list.

Amos



From test1964 at gmail.com  Sat Feb 25 08:15:18 2017
From: test1964 at gmail.com (Test1964)
Date: Sat, 25 Feb 2017 10:15:18 +0200
Subject: [squid-users] Squid 3.5.24 - Url_rewrite with ssl_bump in
 Transparent Mode
In-Reply-To: <11e767fc-fe35-445d-92bd-8cb1826902cb@treenet.co.nz>
References: <ba934f59-1cfe-617e-aa3b-9c3be60730ce@gmail.com>
 <d84dfac1-a917-3568-4816-80294dcf196d@treenet.co.nz>
 <19253d41-000f-271c-30a6-08bd9bdc9a40@gmail.com>
 <11e767fc-fe35-445d-92bd-8cb1826902cb@treenet.co.nz>
Message-ID: <2e49ec12-7e42-b992-6cf8-aaf53157c1f9@gmail.com>

An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20170225/2d9b378b/attachment.htm>

From test1964 at gmail.com  Sat Feb 25 08:18:00 2017
From: test1964 at gmail.com (Test1964)
Date: Sat, 25 Feb 2017 10:18:00 +0200
Subject: [squid-users] Squid 3.5.24 - Url_rewrite with ssl_bump in
 Transparent Mode
Message-ID: <e1916b92-4f46-8715-5a3c-ee439f2acb66@gmail.com>

Hi,

  About block urls, I run Php script that get Url and User ip and check 
in  "Sqlite  DB"  if to Block (our users have fixed ip addr).

Dan.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20170225/5075d9b3/attachment.htm>

From Sebastien.Boulianne at cpu.ca  Sat Feb 25 16:14:23 2017
From: Sebastien.Boulianne at cpu.ca (Sebastien.Boulianne at cpu.ca)
Date: Sat, 25 Feb 2017 11:14:23 -0500
Subject: [squid-users] squid squid[2541]: temporary disabling (Not Found)
In-Reply-To: <0f86bfbe-cbf1-4443-9e24-fbeb1f0f6858@treenet.co.nz>
References: <5FE0959288C73D448BB44CB7E9CC320F5E30930DBF@CPUMAIL2.cpu.qc.ca>
 <0f86bfbe-cbf1-4443-9e24-fbeb1f0f6858@treenet.co.nz>
Message-ID: <5FE0959288C73D448BB44CB7E9CC320F5E30930DF4@CPUMAIL2.cpu.qc.ca>

Hi Amos,



I can find those messages in the cache.log.



# cat /var/log/squid/cache.log  | grep temp

2017/02/24 15:55:58 kid1| temporary disabling (Not Found) digest from xxxx3.cpu.qc.ca

2017/02/24 15:56:58 kid1| temporary disabling (Not Found) digest from xxxxxxx.reseaubiblio.ca

2017/02/24 15:57:58 kid1| temporary disabling (Not Found) digest from xxxxxx.reseaubiblio.ca

2017/02/24 15:58:58 kid1| temporary disabling (Not Found) digest from xxxxxx.reseaubiblio.ca

2017/02/24 15:59:58 kid1| temporary disabling (Not Found) digest from xxxx.reseaubiblio.ca



If I type ?service squid status?, I can see those messages too.



Im on the latest Squid 3.5 version.

]# squid -v

Squid Cache: Version 3.5.24

Service Name: squid



What do you think ?



Thanks



S?bastien.



-----Message d'origine-----
De : squid-users [mailto:squid-users-bounces at lists.squid-cache.org] De la part de Amos Jeffries
Envoy? : 25 f?vrier 2017 02:22
? : squid-users at lists.squid-cache.org
Objet : Re: [squid-users] squid squid[2541]: temporary disabling (Not Found)



On 25/02/2017 4:07 a.m., Sebastien.Boulianne at cpu.ca<mailto:Sebastien.Boulianne at cpu.ca> wrote:

> Hi all,

>

> What squid squid[2541]: temporary disabling (Not Found) mean ?

> Is it a way to fix that warning ?

>



I'm not seeing any message with that text in the Squid code. Can you post the exact and whole log line please? and say which log it is coming from.



Amos



_______________________________________________

squid-users mailing list

squid-users at lists.squid-cache.org<mailto:squid-users at lists.squid-cache.org>

http://lists.squid-cache.org/listinfo/squid-users
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20170225/05b6a1a2/attachment.htm>

From uhlar at fantomas.sk  Sat Feb 25 17:58:49 2017
From: uhlar at fantomas.sk (Matus UHLAR - fantomas)
Date: Sat, 25 Feb 2017 18:58:49 +0100
Subject: [squid-users] squid squid[2541]: temporary disabling (Not Found)
In-Reply-To: <5FE0959288C73D448BB44CB7E9CC320F5E30930DF4@CPUMAIL2.cpu.qc.ca>
References: <5FE0959288C73D448BB44CB7E9CC320F5E30930DBF@CPUMAIL2.cpu.qc.ca>
 <0f86bfbe-cbf1-4443-9e24-fbeb1f0f6858@treenet.co.nz>
 <5FE0959288C73D448BB44CB7E9CC320F5E30930DF4@CPUMAIL2.cpu.qc.ca>
Message-ID: <20170225175849.GA8543@fantomas.sk>

On 25.02.17 11:14, Sebastien.Boulianne at cpu.ca wrote:
># cat /var/log/squid/cache.log  | grep temp

simple "grep temp /var/log/squid/cache.log" would produce the same output

>2017/02/24 15:55:58 kid1| temporary disabling (Not Found) digest from xxxx3.cpu.qc.ca
>2017/02/24 15:56:58 kid1| temporary disabling (Not Found) digest from xxxxxxx.reseaubiblio.ca
>2017/02/24 15:57:58 kid1| temporary disabling (Not Found) digest from xxxxxx.reseaubiblio.ca
>2017/02/24 15:58:58 kid1| temporary disabling (Not Found) digest from xxxxxx.reseaubiblio.ca
>2017/02/24 15:59:58 kid1| temporary disabling (Not Found) digest from xxxx.reseaubiblio.ca

you have configured multiple caches but neither one supports cache digests.
Configure them with "no-digest" option (in cache_peer directive)


>On 25/02/2017 4:07 a.m., Sebastien.Boulianne at cpu.ca<mailto:Sebastien.Boulianne at cpu.ca> wrote:
>> What squid squid[2541]: temporary disabling (Not Found) mean ?

do you see now why providing whole line of output is important?

-- 
Matus UHLAR - fantomas, uhlar at fantomas.sk ; http://www.fantomas.sk/
Warning: I wish NOT to receive e-mail advertising to this address.
Varovanie: na tuto adresu chcem NEDOSTAVAT akukolvek reklamnu postu.
It's now safe to throw off your computer.


From tuser6485 at gmail.com  Sun Feb 26 04:29:09 2017
From: tuser6485 at gmail.com (Test User)
Date: Sun, 26 Feb 2017 09:59:09 +0530
Subject: [squid-users] SSL-Bump: NAT/TPROXY lookup failed to locate
 original IPs
In-Reply-To: <2b480290-3d14-df4d-cb03-3acf9338e0f1@treenet.co.nz>
References: <CAPhcQuJU-ctRMokvU6kg+RqkfXDOU_0y+o+fEUADdoX4AstXaQ@mail.gmail.com>
 <2b480290-3d14-df4d-cb03-3acf9338e0f1@treenet.co.nz>
Message-ID: <CAPhcQuJHPtfaOyMor6kwT-VQcyuZdJXeOZ=T2SA=J0GNjKG2RA@mail.gmail.com>

Hi,
If any of you faced this problem and were able to resolve it, your
help would be much appreciated.
Thanks in advance.

On Fri, Feb 24, 2017 at 4:20 PM, Amos Jeffries <squid3 at treenet.co.nz> wrote:
> On 24/02/2017 7:51 p.m., Test User wrote:
>> Hi,
>> Sorry I am asking this question again.
>
> Please dont. It has only been 7 hours. Some things (like this) are
> tricky and take a while to figure out even what to suggest looking at.
>
> Amos
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users


From eliezer at ngtech.co.il  Sun Feb 26 05:10:16 2017
From: eliezer at ngtech.co.il (Eliezer  Croitoru)
Date: Sun, 26 Feb 2017 07:10:16 +0200
Subject: [squid-users] SSL-Bump: NAT/TPROXY lookup failed to locate
	original	IPs
In-Reply-To: <CAPhcQuJU-ctRMokvU6kg+RqkfXDOU_0y+o+fEUADdoX4AstXaQ@mail.gmail.com>
References: <CAPhcQuJU-ctRMokvU6kg+RqkfXDOU_0y+o+fEUADdoX4AstXaQ@mail.gmail.com>
Message-ID: <544701d28fee$a1a6b050$e4f410f0$@ngtech.co.il>

Hey Michael,

You will need to clear out couple things for us.
First we will need one of the next ouputs or both:
iptables-save
iptables -L -nv

And then clear out where is this proxy sittings and the network structure.
It's not clear if the squid box is the router or a machine somewhere on AWS.
If you wish to pass traffic from a local router to a one on AWS you will need to create a tunnel like using OpenVPN or a similar solution and to use some routing rules to pass the traffic from the local LAN to AWS without removing the original destination address.

When more details on the setup will be available it will be much simpler to understand what is the root for some of the issues you are having.

All The Bests,
Eliezer

----
Eliezer Croitoru
Linux System Administrator
Mobile: +972-5-28704261
Email: eliezer at ngtech.co.il


-----Original Message-----
From: squid-users [mailto:squid-users-bounces at lists.squid-cache.org] On Behalf Of Test User
Sent: Friday, February 24, 2017 8:52 AM
To: squid-users at lists.squid-cache.org
Subject: [squid-users] SSL-Bump: NAT/TPROXY lookup failed to locate original IPs

Hi,
Sorry I am asking this question again. I am trying to setup HTTPS
proxy using ssl-bump. I have followed
steps mentioned in:
http://wiki.squid-cache.org/ConfigExamples/Intercept/SslBumpExplicit

Following are Squid setup details:

Squid Cache: Version 3.5.12
Service Name: squid
Ubuntu linux

configure options:  '--build=x86_64-linux-gnu' '--prefix=/usr'
'--includedir=${prefix}/include' '--mandir=${prefix}/share/man'
'--infodir=${prefix}/share/info' '--sysconfdir=/etc'
'--localstatedir=/var' '--libexecdir=${prefix}/lib/squid3'
'--srcdir=.' '--disable-maintainer-mode'
'--disable-dependency-tracking' '--disable-silent-rules'
'BUILDCXXFLAGS=-g -O2 -fPIE -fstack-protector-strong -Wformat
-Werror=format-security -Wl,-Bsymbolic-functions -fPIE -pie
-Wl,-z,relro -Wl,-z,now' '--datadir=/usr/share/squid'
'--sysconfdir=/etc/squid' '--libexecdir=/usr/lib/squid'
'--mandir=/usr/share/man' '--enable-inline' '--disable-arch-native'
'--enable-async-io=8' '--enable-storeio=ufs,aufs,diskd,rock'
'--enable-removal-policies=lru,heap' '--enable-delay-pools'
'--enable-cache-digests' '--enable-icap-client'
'--enable-follow-x-forwarded-for'
'--enable-auth-basic=DB,fake,getpwnam,LDAP,NCSA,NIS,PAM,POP3,RADIUS,SASL,SMB'
'--enable-auth-digest=file,LDAP'
'--enable-auth-negotiate=kerberos,wrapper'
'--enable-auth-ntlm=fake,smb_lm'
'--enable-external-acl-helpers=file_userip,kerberos_ldap_group,LDAP_group,session,SQL_session,unix_group,wbinfo_group'
'--enable-url-rewrite-helpers=fake' '--enable-eui' '--enable-esi'
'--enable-icmp' '--enable-zph-qos' '--enable-ecap' '--with-openssl'
'--enable-ssl-crtd' '--disable-translation'
'--with-swapdir=/var/spool/squid' '--with-logdir=/var/log/squid'
'--with-pidfile=/var/run/squid.pid' '--with-filedescriptors=65536'
'--with-large-files' '--with-default-user=proxy'
'--enable-build-info=Ubuntu linux' '--enable-linux-netfilter'
'build_alias=x86_64-linux-gnu' 'CFLAGS=-g -O2 -fPIE
-fstack-protector-strong -Wformat -Werror=format-security -Wall'
'LDFLAGS=-Wl,-Bsymbolic-functions -fPIE -pie -Wl,-z,relro -Wl,-z,now'
'CPPFLAGS=-Wdate-time -D_FORTIFY_SOURCE=2' 'CXXFLAGS=-g -O2 -fPIE
-fstack-protector-strong -Wformat -Werror=format-security'


Following is my squid.conf file:

acl SSL_ports port 443
acl Safe_ports port 80 # http
acl Safe_ports port 21 # ftp
acl Safe_ports port 443 # https
acl Safe_ports port 70 # gopher
acl Safe_ports port 210 # wais
acl Safe_ports port 1025-65535 # unregistered ports
acl Safe_ports port 280 # http-mgmt
acl Safe_ports port 488 # gss-http
acl Safe_ports port 591 # filemaker
acl Safe_ports port 777 # multiling http
acl CONNECT method CONNECT
acl step1 at_step SslBump1
http_access deny !Safe_ports
http_access deny CONNECT !SSL_ports
http_access allow localhost manager
http_access deny manager
http_access allow localhost
http_access allow all
http_port 3128 ssl-bump \
  cert=/etc/squid/ssl_cert/squidCA.pem \
  generate-host-certificates=on dynamic_cert_mem_cache_size=4MB
https_port 3129 intercept ssl-bump generate-host-certificates=on \
dynamic_cert_mem_cache_size=4MB cert=/etc/squid/ssl_cert/squidCA.pem \
dhparams=/etc/squid/ssl_cert/dhparam.pem
sslproxy_options NO_SSLv2,NO_SSLv3,SINGLE_DH_USE
sslproxy_cipher
EECDH+ECDSA+AESGCM:EECDH+aRSA+AESGCM:EECDH+ECDSA+SHA384:EECDH+ECDSA+SHA256:EECDH+aRSA+SHA384:EECDH+aRSA+SHA256:EECDH+aRSA+RC4:EECDH:EDH+aRSA:!RC4:!aNULL:!eNULL:!LOW:!3DES:!MD5:!EXP:!PSK:!SRP:!DSS
sslcrtd_program /usr/lib/squid/ssl_crtd -s /var/spool/squid3_ssldb -M 4MB
debug_options ALL,1 3,5 4,5 11,5 17,5 23,5 46,5 78,5 rotate=1
coredump_dir /var/spool/squid
refresh_pattern ^ftp: 1440 20% 10080
refresh_pattern ^gopher: 1440 0% 1440
refresh_pattern -i (/cgi-bin/|\?) 0 0% 0
refresh_pattern (Release|Packages(.gz)*)$      0       20%     2880
refresh_pattern . 0 20% 4320


I get no errors while starting Squid. Following are the logs when Squid starts:

2017/02/23 09:59:53 kid1| Set Current Directory to /var/spool/squid
2017/02/23 09:59:53 kid1| Starting Squid Cache version 3.5.12 for
x86_64-pc-linux-gnu...
2017/02/23 09:59:53 kid1| Service Name: squid
2017/02/23 09:59:53 kid1| Process ID 26236
2017/02/23 09:59:53 kid1| Process Roles: worker
2017/02/23 09:59:53 kid1| With 65535 file descriptors available
2017/02/23 09:59:53 kid1| Initializing IP Cache...
2017/02/23 09:59:53.756 kid1| 78,2| dns_internal.cc(1525) dnsInit:
idnsInit: attempt open DNS socket to: [::]
2017/02/23 09:59:53.756 kid1| 78,2| dns_internal.cc(1534) dnsInit:
idnsInit: attempt open DNS socket to: 0.0.0.0
2017/02/23 09:59:53.756 kid1| DNS Socket created at [::], FD 6
2017/02/23 09:59:53.756 kid1| DNS Socket created at 0.0.0.0, FD 7
2017/02/23 09:59:53.756 kid1| Adding nameserver 172.31.0.2 from /etc/resolv.conf
2017/02/23 09:59:53.756 kid1| 78,3| dns_internal.cc(321)
idnsAddNameserver: idnsAddNameserver: Added nameserver #0
(172.31.0.2:53)
2017/02/23 09:59:53.756 kid1| Adding domain
ap-south-1.compute.internal from /etc/resolv.conf
2017/02/23 09:59:53.756 kid1| 78,3| dns_internal.cc(350)
idnsAddPathComponent: idnsAddPathComponent: Added domain #0:
ap-south-1.compute.internal
2017/02/23 09:59:53.756 kid1| helperOpenServers: Starting 5/32
'ssl_crtd' processes
2017/02/23 09:59:53.775 kid1| 46,2| Format.cc(64) parse: got
definition '%>a/%>A %un %>rm myip=%la myport=%lp'
2017/02/23 09:59:53.775 kid1| 46,5| Token.cc(380) parse: scan for
possible Misc token
2017/02/23 09:59:53.775 kid1| 46,5| Token.cc(384) parse: scan for
possible 2C token
2017/02/23 09:59:53.775 kid1| 46,5| Token.cc(389) parse: scan for
possible 1C token
2017/02/23 09:59:53.775 kid1| 46,5| Token.cc(380) parse: scan for
possible Misc token
2017/02/23 09:59:53.775 kid1| 46,5| Token.cc(384) parse: scan for
possible 2C token
2017/02/23 09:59:53.775 kid1| 46,5| Token.cc(389) parse: scan for
possible 1C token
2017/02/23 09:59:53.775 kid1| 46,5| Token.cc(380) parse: scan for
possible Misc token
2017/02/23 09:59:53.775 kid1| 46,5| Token.cc(384) parse: scan for
possible 2C token
2017/02/23 09:59:53.775 kid1| 46,5| Token.cc(380) parse: scan for
possible Misc token
2017/02/23 09:59:53.775 kid1| 46,5| Token.cc(384) parse: scan for
possible 2C token
2017/02/23 09:59:53.775 kid1| 46,5| Token.cc(380) parse: scan for
possible Misc token
2017/02/23 09:59:53.775 kid1| 46,5| Token.cc(384) parse: scan for
possible 2C token
2017/02/23 09:59:53.775 kid1| 46,5| Token.cc(380) parse: scan for
possible Misc token
2017/02/23 09:59:53.775 kid1| 46,5| Token.cc(384) parse: scan for
possible 2C token
2017/02/23 09:59:53.775 kid1| 46,2| Format.cc(64) parse: got
definition '%>a/%>A %un %>rm myip=%la myport=%lp'
2017/02/23 09:59:53.775 kid1| 46,5| Token.cc(380) parse: scan for
possible Misc token
2017/02/23 09:59:53.775 kid1| 46,5| Token.cc(384) parse: scan for
possible 2C token
2017/02/23 09:59:53.775 kid1| 46,5| Token.cc(389) parse: scan for
possible 1C token
2017/02/23 09:59:53.775 kid1| 46,5| Token.cc(380) parse: scan for
possible Misc token
2017/02/23 09:59:53.775 kid1| 46,5| Token.cc(384) parse: scan for
possible 2C token
2017/02/23 09:59:53.775 kid1| 46,5| Token.cc(389) parse: scan for
possible 1C token
2017/02/23 09:59:53.775 kid1| 46,5| Token.cc(380) parse: scan for
possible Misc token
2017/02/23 09:59:53.775 kid1| 46,5| Token.cc(384) parse: scan for
possible 2C token
2017/02/23 09:59:53.775 kid1| 46,5| Token.cc(380) parse: scan for
possible Misc token
2017/02/23 09:59:53.775 kid1| 46,5| Token.cc(384) parse: scan for
possible 2C token
2017/02/23 09:59:53.775 kid1| 46,5| Token.cc(380) parse: scan for
possible Misc token
2017/02/23 09:59:53.775 kid1| 46,5| Token.cc(384) parse: scan for
possible 2C token
2017/02/23 09:59:53.775 kid1| 46,5| Token.cc(380) parse: scan for
possible Misc token
2017/02/23 09:59:53.775 kid1| 46,5| Token.cc(384) parse: scan for
possible 2C token
2017/02/23 09:59:53.775 kid1| Logfile: opening log
daemon:/var/log/squid/access.log
2017/02/23 09:59:53.775 kid1| Logfile Daemon: opening log
/var/log/squid/access.log
2017/02/23 09:59:53.779 kid1| 23,5| url.cc(43) urlInitialize:
urlInitialize: Initializing...
2017/02/23 09:59:53.779 kid1| Local cache digest enabled;
rebuild/rewrite every 3600/3600 sec
2017/02/23 09:59:53.779 kid1| Store logging disabled
2017/02/23 09:59:53.779 kid1| Swap maxSize 0 + 262144 KB, estimated
20164 objects
2017/02/23 09:59:53.779 kid1| Target number of buckets: 1008
2017/02/23 09:59:53.779 kid1| Using 8192 Store buckets
2017/02/23 09:59:53.779 kid1| Max Mem  size: 262144 KB
2017/02/23 09:59:53.779 kid1| Max Swap size: 0 KB
2017/02/23 09:59:53.779 kid1| Using Least Load store dir selection
2017/02/23 09:59:53.779 kid1| Set Current Directory to /var/spool/squid
2017/02/23 09:59:53.785 kid1| 23,3| url.cc(357) urlParse: urlParse:
Split URL 'http://ip-172-31-25-235:3128/squid-internal-static/icons/silk/image.png'
into proto='http', host='ip-172-31-25-235', port='3128',
path='/squid-internal-static/icons/silk/image.png'
2017/02/23 09:59:53.785 kid1| 23,3| url.cc(357) urlParse: urlParse:
Split URL 'http://ip-172-31-25-235:3128/squid-internal-static/icons/silk/page_white_text.png'
into proto='http', host='ip-172-31-25-235', port='3128',
path='/squid-internal-static/icons/silk/page_white_text.png'

****several urlParse logs like above. Removing them to shorten the
email. Further logs below...****

2017/02/23 09:59:53.815 kid1| Finished loading MIME types and icons.
2017/02/23 09:59:53.815 kid1| HTCP Disabled.
2017/02/23 09:59:53.815 kid1| Pinger socket opened on FD 25
2017/02/23 09:59:53.815 kid1| Squid plugin modules loaded: 0
2017/02/23 09:59:53.815 kid1| Adaptation support is off.
2017/02/23 09:59:53.815 kid1| Accepting SSL bumped HTTP Socket
connections at local=[::]:3128 remote=[::] FD 22 flags=9
2017/02/23 09:59:53.815 kid1| Accepting NAT intercepted SSL bumped
HTTPS Socket connections at local=[::]:3129 remote=[::] FD 23 flags=41
2017/02/23 09:59:53| pinger: Initialising ICMP pinger ...
2017/02/23 09:59:53| pinger: ICMP socket opened.
2017/02/23 09:59:53| pinger: ICMPv6 socket opened
2017/02/23 09:59:54 kid1| storeLateRelease: released 0 objects



I tested this setup by providing proxy details to Firefox. Firefox was
able to show HTTP websites but when I tried to open an HTTPS website I
got following error:

2017/02/23 11:00:50 kid1| ERROR: NF getsockopt(ORIGINAL_DST) failed on
local=172.31.25.235:3129 remote=182.72.78.122:50655 FD 7 flags=33:
(92) Protocol not available
2017/02/23 11:00:50 kid1| ERROR: NAT/TPROXY lookup failed to locate
original IPs on local=172.31.25.235:3129 remote=182.72.78.122:50655 FD
7 flags=33
2017/02/23 11:00:50 kid1| ERROR: NF getsockopt(ORIGINAL_DST) failed on
local=172.31.25.235:3129 remote=182.72.78.122:50656 FD 7 flags=33:
(92) Protocol not available
2017/02/23 11:00:50 kid1| ERROR: NAT/TPROXY lookup failed to locate
original IPs on local=172.31.25.235:3129 remote=182.72.78.122:50656 FD
7 flags=33
2017/02/23 11:00:50 kid1| ERROR: NF getsockopt(ORIGINAL_DST) failed on
local=172.31.25.235:3129 remote=182.72.78.122:50657 FD 7 flags=33:
(92) Protocol not available
2017/02/23 11:00:50 kid1| ERROR: NAT/TPROXY lookup failed to locate
original IPs on local=172.31.25.235:3129 remote=182.72.78.122:50657 FD
7 flags=33

I googled this error and found this mail thread which had similar problems:
http://squid-web-proxy-cache.1019090.n4.nabble.com/NAT-TPROXY-lookup-failed-to-locate-original-IPs-td4675464.html

I found this link from the above thread. I modified the steps for
HTTPS from the below link:
http://wiki.squid-cache.org/ConfigExamples/Intercept/LinuxDnat

Now my sysctl.conf is:

net.ipv4.conf.all.rp_filter=0
net.ipv4.ip_forward = 1
net.ipv4.conf.default.rp_filter = 0
net.ipv4.conf.default.accept_source_route = 0

My iptables -t nat -L result:

Chain PREROUTING (policy ACCEPT)
target     prot opt source               destination
ACCEPT     tcp  --  ec2-35-154-101-8.ap-south-1.compute.amazonaws.com
anywhere             tcp dpt:https
DNAT       tcp  --  anywhere             anywhere             tcp
dpt:https to:35.154.101.8:3129

Chain INPUT (policy ACCEPT)
target     prot opt source               destination

Chain OUTPUT (policy ACCEPT)
target     prot opt source               destination

Chain POSTROUTING (policy ACCEPT)
target     prot opt source               destination
MASQUERADE  all  --  anywhere             anywhere


Once this was done, I tried to hit HTTPS website from Firefox and now
I get connection timeout error. Nothing shows in syslog, access.log or
cache.log. Could you please help me resolve this.

Thanks,
Michael
_______________________________________________
squid-users mailing list
squid-users at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-users



From tuser6485 at gmail.com  Sun Feb 26 06:37:52 2017
From: tuser6485 at gmail.com (Test User)
Date: Sun, 26 Feb 2017 12:07:52 +0530
Subject: [squid-users] SSL-Bump: NAT/TPROXY lookup failed to locate
 original IPs
In-Reply-To: <544701d28fee$a1a6b050$e4f410f0$@ngtech.co.il>
References: <CAPhcQuJU-ctRMokvU6kg+RqkfXDOU_0y+o+fEUADdoX4AstXaQ@mail.gmail.com>
 <544701d28fee$a1a6b050$e4f410f0$@ngtech.co.il>
Message-ID: <CAPhcQuJEFmVPQ=C9=KAzuKzZjqKoQo=hN7wvR9UX2aPM98LqEw@mail.gmail.com>

On Sun, Feb 26, 2017 at 10:40 AM, Eliezer Croitoru <eliezer at ngtech.co.il> wrote:
> Hey Michael,
>
> You will need to clear out couple things for us.
> First we will need one of the next ouputs or both:
> iptables-save
> iptables -L -nv
>
> And then clear out where is this proxy sittings and the network structure.
> It's not clear if the squid box is the router or a machine somewhere on AWS.
> If you wish to pass traffic from a local router to a one on AWS you will need to create a tunnel like using OpenVPN or a similar solution and to use some routing rules to pass the traffic from the local LAN to AWS without removing the original destination address.
>
> When more details on the setup will be available it will be much simpler to understand what is the root for some of the issues you are having.
>
> All The Bests,
> Eliezer
>
> ----
> Eliezer Croitoru
> Linux System Administrator
> Mobile: +972-5-28704261
> Email: eliezer at ngtech.co.il
>
>
> -----Original Message-----
> From: squid-users [mailto:squid-users-bounces at lists.squid-cache.org] On Behalf Of Test User
> Sent: Friday, February 24, 2017 8:52 AM
> To: squid-users at lists.squid-cache.org
> Subject: [squid-users] SSL-Bump: NAT/TPROXY lookup failed to locate original IPs
>
> Hi,
> Sorry I am asking this question again. I am trying to setup HTTPS
> proxy using ssl-bump. I have followed
> steps mentioned in:
> http://wiki.squid-cache.org/ConfigExamples/Intercept/SslBumpExplicit
>
> Following are Squid setup details:
>
> Squid Cache: Version 3.5.12
> Service Name: squid
> Ubuntu linux
>
> configure options:  '--build=x86_64-linux-gnu' '--prefix=/usr'
> '--includedir=${prefix}/include' '--mandir=${prefix}/share/man'
> '--infodir=${prefix}/share/info' '--sysconfdir=/etc'
> '--localstatedir=/var' '--libexecdir=${prefix}/lib/squid3'
> '--srcdir=.' '--disable-maintainer-mode'
> '--disable-dependency-tracking' '--disable-silent-rules'
> 'BUILDCXXFLAGS=-g -O2 -fPIE -fstack-protector-strong -Wformat
> -Werror=format-security -Wl,-Bsymbolic-functions -fPIE -pie
> -Wl,-z,relro -Wl,-z,now' '--datadir=/usr/share/squid'
> '--sysconfdir=/etc/squid' '--libexecdir=/usr/lib/squid'
> '--mandir=/usr/share/man' '--enable-inline' '--disable-arch-native'
> '--enable-async-io=8' '--enable-storeio=ufs,aufs,diskd,rock'
> '--enable-removal-policies=lru,heap' '--enable-delay-pools'
> '--enable-cache-digests' '--enable-icap-client'
> '--enable-follow-x-forwarded-for'
> '--enable-auth-basic=DB,fake,getpwnam,LDAP,NCSA,NIS,PAM,POP3,RADIUS,SASL,SMB'
> '--enable-auth-digest=file,LDAP'
> '--enable-auth-negotiate=kerberos,wrapper'
> '--enable-auth-ntlm=fake,smb_lm'
> '--enable-external-acl-helpers=file_userip,kerberos_ldap_group,LDAP_group,session,SQL_session,unix_group,wbinfo_group'
> '--enable-url-rewrite-helpers=fake' '--enable-eui' '--enable-esi'
> '--enable-icmp' '--enable-zph-qos' '--enable-ecap' '--with-openssl'
> '--enable-ssl-crtd' '--disable-translation'
> '--with-swapdir=/var/spool/squid' '--with-logdir=/var/log/squid'
> '--with-pidfile=/var/run/squid.pid' '--with-filedescriptors=65536'
> '--with-large-files' '--with-default-user=proxy'
> '--enable-build-info=Ubuntu linux' '--enable-linux-netfilter'
> 'build_alias=x86_64-linux-gnu' 'CFLAGS=-g -O2 -fPIE
> -fstack-protector-strong -Wformat -Werror=format-security -Wall'
> 'LDFLAGS=-Wl,-Bsymbolic-functions -fPIE -pie -Wl,-z,relro -Wl,-z,now'
> 'CPPFLAGS=-Wdate-time -D_FORTIFY_SOURCE=2' 'CXXFLAGS=-g -O2 -fPIE
> -fstack-protector-strong -Wformat -Werror=format-security'
>
>
> Following is my squid.conf file:
>
> acl SSL_ports port 443
> acl Safe_ports port 80 # http
> acl Safe_ports port 21 # ftp
> acl Safe_ports port 443 # https
> acl Safe_ports port 70 # gopher
> acl Safe_ports port 210 # wais
> acl Safe_ports port 1025-65535 # unregistered ports
> acl Safe_ports port 280 # http-mgmt
> acl Safe_ports port 488 # gss-http
> acl Safe_ports port 591 # filemaker
> acl Safe_ports port 777 # multiling http
> acl CONNECT method CONNECT
> acl step1 at_step SslBump1
> http_access deny !Safe_ports
> http_access deny CONNECT !SSL_ports
> http_access allow localhost manager
> http_access deny manager
> http_access allow localhost
> http_access allow all
> http_port 3128 ssl-bump \
>   cert=/etc/squid/ssl_cert/squidCA.pem \
>   generate-host-certificates=on dynamic_cert_mem_cache_size=4MB
> https_port 3129 intercept ssl-bump generate-host-certificates=on \
> dynamic_cert_mem_cache_size=4MB cert=/etc/squid/ssl_cert/squidCA.pem \
> dhparams=/etc/squid/ssl_cert/dhparam.pem
> sslproxy_options NO_SSLv2,NO_SSLv3,SINGLE_DH_USE
> sslproxy_cipher
> EECDH+ECDSA+AESGCM:EECDH+aRSA+AESGCM:EECDH+ECDSA+SHA384:EECDH+ECDSA+SHA256:EECDH+aRSA+SHA384:EECDH+aRSA+SHA256:EECDH+aRSA+RC4:EECDH:EDH+aRSA:!RC4:!aNULL:!eNULL:!LOW:!3DES:!MD5:!EXP:!PSK:!SRP:!DSS
> sslcrtd_program /usr/lib/squid/ssl_crtd -s /var/spool/squid3_ssldb -M 4MB
> debug_options ALL,1 3,5 4,5 11,5 17,5 23,5 46,5 78,5 rotate=1
> coredump_dir /var/spool/squid
> refresh_pattern ^ftp: 1440 20% 10080
> refresh_pattern ^gopher: 1440 0% 1440
> refresh_pattern -i (/cgi-bin/|\?) 0 0% 0
> refresh_pattern (Release|Packages(.gz)*)$      0       20%     2880
> refresh_pattern . 0 20% 4320
>
>
> I get no errors while starting Squid. Following are the logs when Squid starts:
>
> 2017/02/23 09:59:53 kid1| Set Current Directory to /var/spool/squid
> 2017/02/23 09:59:53 kid1| Starting Squid Cache version 3.5.12 for
> x86_64-pc-linux-gnu...
> 2017/02/23 09:59:53 kid1| Service Name: squid
> 2017/02/23 09:59:53 kid1| Process ID 26236
> 2017/02/23 09:59:53 kid1| Process Roles: worker
> 2017/02/23 09:59:53 kid1| With 65535 file descriptors available
> 2017/02/23 09:59:53 kid1| Initializing IP Cache...
> 2017/02/23 09:59:53.756 kid1| 78,2| dns_internal.cc(1525) dnsInit:
> idnsInit: attempt open DNS socket to: [::]
> 2017/02/23 09:59:53.756 kid1| 78,2| dns_internal.cc(1534) dnsInit:
> idnsInit: attempt open DNS socket to: 0.0.0.0
> 2017/02/23 09:59:53.756 kid1| DNS Socket created at [::], FD 6
> 2017/02/23 09:59:53.756 kid1| DNS Socket created at 0.0.0.0, FD 7
> 2017/02/23 09:59:53.756 kid1| Adding nameserver 172.31.0.2 from /etc/resolv.conf
> 2017/02/23 09:59:53.756 kid1| 78,3| dns_internal.cc(321)
> idnsAddNameserver: idnsAddNameserver: Added nameserver #0
> (172.31.0.2:53)
> 2017/02/23 09:59:53.756 kid1| Adding domain
> ap-south-1.compute.internal from /etc/resolv.conf
> 2017/02/23 09:59:53.756 kid1| 78,3| dns_internal.cc(350)
> idnsAddPathComponent: idnsAddPathComponent: Added domain #0:
> ap-south-1.compute.internal
> 2017/02/23 09:59:53.756 kid1| helperOpenServers: Starting 5/32
> 'ssl_crtd' processes
> 2017/02/23 09:59:53.775 kid1| 46,2| Format.cc(64) parse: got
> definition '%>a/%>A %un %>rm myip=%la myport=%lp'
> 2017/02/23 09:59:53.775 kid1| 46,5| Token.cc(380) parse: scan for
> possible Misc token
> 2017/02/23 09:59:53.775 kid1| 46,5| Token.cc(384) parse: scan for
> possible 2C token
> 2017/02/23 09:59:53.775 kid1| 46,5| Token.cc(389) parse: scan for
> possible 1C token
> 2017/02/23 09:59:53.775 kid1| 46,5| Token.cc(380) parse: scan for
> possible Misc token
> 2017/02/23 09:59:53.775 kid1| 46,5| Token.cc(384) parse: scan for
> possible 2C token
> 2017/02/23 09:59:53.775 kid1| 46,5| Token.cc(389) parse: scan for
> possible 1C token
> 2017/02/23 09:59:53.775 kid1| 46,5| Token.cc(380) parse: scan for
> possible Misc token
> 2017/02/23 09:59:53.775 kid1| 46,5| Token.cc(384) parse: scan for
> possible 2C token
> 2017/02/23 09:59:53.775 kid1| 46,5| Token.cc(380) parse: scan for
> possible Misc token
> 2017/02/23 09:59:53.775 kid1| 46,5| Token.cc(384) parse: scan for
> possible 2C token
> 2017/02/23 09:59:53.775 kid1| 46,5| Token.cc(380) parse: scan for
> possible Misc token
> 2017/02/23 09:59:53.775 kid1| 46,5| Token.cc(384) parse: scan for
> possible 2C token
> 2017/02/23 09:59:53.775 kid1| 46,5| Token.cc(380) parse: scan for
> possible Misc token
> 2017/02/23 09:59:53.775 kid1| 46,5| Token.cc(384) parse: scan for
> possible 2C token
> 2017/02/23 09:59:53.775 kid1| 46,2| Format.cc(64) parse: got
> definition '%>a/%>A %un %>rm myip=%la myport=%lp'
> 2017/02/23 09:59:53.775 kid1| 46,5| Token.cc(380) parse: scan for
> possible Misc token
> 2017/02/23 09:59:53.775 kid1| 46,5| Token.cc(384) parse: scan for
> possible 2C token
> 2017/02/23 09:59:53.775 kid1| 46,5| Token.cc(389) parse: scan for
> possible 1C token
> 2017/02/23 09:59:53.775 kid1| 46,5| Token.cc(380) parse: scan for
> possible Misc token
> 2017/02/23 09:59:53.775 kid1| 46,5| Token.cc(384) parse: scan for
> possible 2C token
> 2017/02/23 09:59:53.775 kid1| 46,5| Token.cc(389) parse: scan for
> possible 1C token
> 2017/02/23 09:59:53.775 kid1| 46,5| Token.cc(380) parse: scan for
> possible Misc token
> 2017/02/23 09:59:53.775 kid1| 46,5| Token.cc(384) parse: scan for
> possible 2C token
> 2017/02/23 09:59:53.775 kid1| 46,5| Token.cc(380) parse: scan for
> possible Misc token
> 2017/02/23 09:59:53.775 kid1| 46,5| Token.cc(384) parse: scan for
> possible 2C token
> 2017/02/23 09:59:53.775 kid1| 46,5| Token.cc(380) parse: scan for
> possible Misc token
> 2017/02/23 09:59:53.775 kid1| 46,5| Token.cc(384) parse: scan for
> possible 2C token
> 2017/02/23 09:59:53.775 kid1| 46,5| Token.cc(380) parse: scan for
> possible Misc token
> 2017/02/23 09:59:53.775 kid1| 46,5| Token.cc(384) parse: scan for
> possible 2C token
> 2017/02/23 09:59:53.775 kid1| Logfile: opening log
> daemon:/var/log/squid/access.log
> 2017/02/23 09:59:53.775 kid1| Logfile Daemon: opening log
> /var/log/squid/access.log
> 2017/02/23 09:59:53.779 kid1| 23,5| url.cc(43) urlInitialize:
> urlInitialize: Initializing...
> 2017/02/23 09:59:53.779 kid1| Local cache digest enabled;
> rebuild/rewrite every 3600/3600 sec
> 2017/02/23 09:59:53.779 kid1| Store logging disabled
> 2017/02/23 09:59:53.779 kid1| Swap maxSize 0 + 262144 KB, estimated
> 20164 objects
> 2017/02/23 09:59:53.779 kid1| Target number of buckets: 1008
> 2017/02/23 09:59:53.779 kid1| Using 8192 Store buckets
> 2017/02/23 09:59:53.779 kid1| Max Mem  size: 262144 KB
> 2017/02/23 09:59:53.779 kid1| Max Swap size: 0 KB
> 2017/02/23 09:59:53.779 kid1| Using Least Load store dir selection
> 2017/02/23 09:59:53.779 kid1| Set Current Directory to /var/spool/squid
> 2017/02/23 09:59:53.785 kid1| 23,3| url.cc(357) urlParse: urlParse:
> Split URL 'http://ip-172-31-25-235:3128/squid-internal-static/icons/silk/image.png'
> into proto='http', host='ip-172-31-25-235', port='3128',
> path='/squid-internal-static/icons/silk/image.png'
> 2017/02/23 09:59:53.785 kid1| 23,3| url.cc(357) urlParse: urlParse:
> Split URL 'http://ip-172-31-25-235:3128/squid-internal-static/icons/silk/page_white_text.png'
> into proto='http', host='ip-172-31-25-235', port='3128',
> path='/squid-internal-static/icons/silk/page_white_text.png'
>
> ****several urlParse logs like above. Removing them to shorten the
> email. Further logs below...****
>
> 2017/02/23 09:59:53.815 kid1| Finished loading MIME types and icons.
> 2017/02/23 09:59:53.815 kid1| HTCP Disabled.
> 2017/02/23 09:59:53.815 kid1| Pinger socket opened on FD 25
> 2017/02/23 09:59:53.815 kid1| Squid plugin modules loaded: 0
> 2017/02/23 09:59:53.815 kid1| Adaptation support is off.
> 2017/02/23 09:59:53.815 kid1| Accepting SSL bumped HTTP Socket
> connections at local=[::]:3128 remote=[::] FD 22 flags=9
> 2017/02/23 09:59:53.815 kid1| Accepting NAT intercepted SSL bumped
> HTTPS Socket connections at local=[::]:3129 remote=[::] FD 23 flags=41
> 2017/02/23 09:59:53| pinger: Initialising ICMP pinger ...
> 2017/02/23 09:59:53| pinger: ICMP socket opened.
> 2017/02/23 09:59:53| pinger: ICMPv6 socket opened
> 2017/02/23 09:59:54 kid1| storeLateRelease: released 0 objects
>
>
>
> I tested this setup by providing proxy details to Firefox. Firefox was
> able to show HTTP websites but when I tried to open an HTTPS website I
> got following error:
>
> 2017/02/23 11:00:50 kid1| ERROR: NF getsockopt(ORIGINAL_DST) failed on
> local=172.31.25.235:3129 remote=182.72.78.122:50655 FD 7 flags=33:
> (92) Protocol not available
> 2017/02/23 11:00:50 kid1| ERROR: NAT/TPROXY lookup failed to locate
> original IPs on local=172.31.25.235:3129 remote=182.72.78.122:50655 FD
> 7 flags=33
> 2017/02/23 11:00:50 kid1| ERROR: NF getsockopt(ORIGINAL_DST) failed on
> local=172.31.25.235:3129 remote=182.72.78.122:50656 FD 7 flags=33:
> (92) Protocol not available
> 2017/02/23 11:00:50 kid1| ERROR: NAT/TPROXY lookup failed to locate
> original IPs on local=172.31.25.235:3129 remote=182.72.78.122:50656 FD
> 7 flags=33
> 2017/02/23 11:00:50 kid1| ERROR: NF getsockopt(ORIGINAL_DST) failed on
> local=172.31.25.235:3129 remote=182.72.78.122:50657 FD 7 flags=33:
> (92) Protocol not available
> 2017/02/23 11:00:50 kid1| ERROR: NAT/TPROXY lookup failed to locate
> original IPs on local=172.31.25.235:3129 remote=182.72.78.122:50657 FD
> 7 flags=33
>
> I googled this error and found this mail thread which had similar problems:
> http://squid-web-proxy-cache.1019090.n4.nabble.com/NAT-TPROXY-lookup-failed-to-locate-original-IPs-td4675464.html
>
> I found this link from the above thread. I modified the steps for
> HTTPS from the below link:
> http://wiki.squid-cache.org/ConfigExamples/Intercept/LinuxDnat
>
> Now my sysctl.conf is:
>
> net.ipv4.conf.all.rp_filter=0
> net.ipv4.ip_forward = 1
> net.ipv4.conf.default.rp_filter = 0
> net.ipv4.conf.default.accept_source_route = 0
>
> My iptables -t nat -L result:
>
> Chain PREROUTING (policy ACCEPT)
> target     prot opt source               destination
> ACCEPT     tcp  --  ec2-35-154-101-8.ap-south-1.compute.amazonaws.com
> anywhere             tcp dpt:https
> DNAT       tcp  --  anywhere             anywhere             tcp
> dpt:https to:35.154.101.8:3129
>
> Chain INPUT (policy ACCEPT)
> target     prot opt source               destination
>
> Chain OUTPUT (policy ACCEPT)
> target     prot opt source               destination
>
> Chain POSTROUTING (policy ACCEPT)
> target     prot opt source               destination
> MASQUERADE  all  --  anywhere             anywhere
>
>
> Once this was done, I tried to hit HTTPS website from Firefox and now
> I get connection timeout error. Nothing shows in syslog, access.log or
> cache.log. Could you please help me resolve this.
>
> Thanks,
> Michael
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>


Thanks for replying Eliezer. Following are the outputs you asked:

1. iptables-save:

# Generated by iptables-save v1.6.0 on Sun Feb 26 06:28:46 2017
*filter
:INPUT ACCEPT [171:12090]
:FORWARD ACCEPT [0:0]
:OUTPUT ACCEPT [106:15187]
COMMIT
# Completed on Sun Feb 26 06:28:46 2017
# Generated by iptables-save v1.6.0 on Sun Feb 26 06:28:46 2017
*mangle
:PREROUTING ACCEPT [89003:74850371]
:INPUT ACCEPT [88973:74849159]
:FORWARD ACCEPT [30:1212]
:OUTPUT ACCEPT [76710:51478183]
:POSTROUTING ACCEPT [76740:51479395]
-A PREROUTING -p tcp -m tcp --dport 3129 -j DROP
COMMIT
# Completed on Sun Feb 26 06:28:46 2017
# Generated by iptables-save v1.6.0 on Sun Feb 26 06:28:46 2017
*nat
:PREROUTING ACCEPT [7766:436942]
:INPUT ACCEPT [7766:436942]
:OUTPUT ACCEPT [952:102330]
:POSTROUTING ACCEPT [0:0]
-A PREROUTING -s 35.154.101.8/32 -p tcp -m tcp --dport 443 -j ACCEPT
-A PREROUTING -p tcp -m tcp --dport 443 -j DNAT --to-destination
35.154.101.8:3129
-A POSTROUTING -j MASQUERADE
COMMIT
# Completed on Sun Feb 26 06:28:46 2017

2. Also pasting sudo iptables -L -nv:

Chain INPUT (policy ACCEPT 216 packets, 16058 bytes)
 pkts bytes target     prot opt in     out     source
destination

Chain FORWARD (policy ACCEPT 0 packets, 0 bytes)
 pkts bytes target     prot opt in     out     source
destination

Chain OUTPUT (policy ACCEPT 161 packets, 24629 bytes)
 pkts bytes target     prot opt in     out     source               destination



> And then clear out where is this proxy sittings and the network structure.
> It's not clear if the squid box is the router or a machine somewhere on AWS.

[Michael] This proxy is installed on an AWS instance.

> If you wish to pass traffic from a local router to a one on AWS you will need to create a tunnel like using OpenVPN or a similar solution and to use some routing rules to pass the traffic from the local LAN to AWS without removing the original destination address.
>

[Michael] Does this mean, to make ssl-bump work, I will have to setup
a VPN server and configure the VPN clients to use this proxy via VPN
server?


Thanks,
Michael.


From eliezer at ngtech.co.il  Sun Feb 26 18:51:01 2017
From: eliezer at ngtech.co.il (Eliezer  Croitoru)
Date: Sun, 26 Feb 2017 20:51:01 +0200
Subject: [squid-users] SSL-Bump: NAT/TPROXY lookup failed to locate
	original IPs
In-Reply-To: <CAPhcQuJEFmVPQ=C9=KAzuKzZjqKoQo=hN7wvR9UX2aPM98LqEw@mail.gmail.com>
References: <CAPhcQuJU-ctRMokvU6kg+RqkfXDOU_0y+o+fEUADdoX4AstXaQ@mail.gmail.com>
 <544701d28fee$a1a6b050$e4f410f0$@ngtech.co.il>
 <CAPhcQuJEFmVPQ=C9=KAzuKzZjqKoQo=hN7wvR9UX2aPM98LqEw@mail.gmail.com>
Message-ID: <7a2301d29061$468aba90$d3a02fb0$@ngtech.co.il>

Hey Michael,

The details you attached explained pretty well the cause for the issues you have described.
What you will need to do in order to make this setup to work can be done in more then one way.
For a sysadmin the simplest way is to create a VPN or some kind of a tunnel between the AWS instance to the local router.
I am almost sure that you can use haproxy to do a local tproxy or interception that will forward the traffic to the remote squid with the PROXY protocol keeping original source and original destination visible to the remote squid.

The choice will depend on both:
- your skills and will to dig some time about couple subjects
- The availability of static IP addresses(both local and AWS).
- The OS on both sides

I believe that the next haproxy settings can be used as a compromise to a tunnel:
http://ngtech.co.il/paste/1605/
And some tproxy route and iptables rules ..
With a squid.conf which will be similar to:
acl frontend src 100.0.0.1
proxy_protocol_access allow frontend
http_port 3127
http_port 3128 require-proxy-header ... ssl-bump settings
##END of example

However I do still believe that the more secure way would be to use some kind of vpn tunnel like OpenVPN between the local router to the remote AWS instance.

All The Bests,
Eliezer

----
Eliezer Croitoru
Linux System Administrator
Mobile: +972-5-28704261
Email: eliezer at ngtech.co.il


-----Original Message-----
From: Test User [mailto:tuser6485 at gmail.com] 
Sent: Sunday, February 26, 2017 8:38 AM
To: Eliezer Croitoru <eliezer at ngtech.co.il>
Cc: squid-users at lists.squid-cache.org
Subject: Re: [squid-users] SSL-Bump: NAT/TPROXY lookup failed to locate original IPs

On Sun, Feb 26, 2017 at 10:40 AM, Eliezer Croitoru <eliezer at ngtech.co.il> wrote:
> Hey Michael,
>
> You will need to clear out couple things for us.
> First we will need one of the next ouputs or both:
> iptables-save
> iptables -L -nv
>
> And then clear out where is this proxy sittings and the network structure.
> It's not clear if the squid box is the router or a machine somewhere on AWS.
> If you wish to pass traffic from a local router to a one on AWS you will need to create a tunnel like using OpenVPN or a similar solution and to use some routing rules to pass the traffic from the local LAN to AWS without removing the original destination address.
>
> When more details on the setup will be available it will be much simpler to understand what is the root for some of the issues you are having.
>
> All The Bests,
> Eliezer
>
> ----
> Eliezer Croitoru
> Linux System Administrator
> Mobile: +972-5-28704261
> Email: eliezer at ngtech.co.il
>
>
> -----Original Message-----
> From: squid-users [mailto:squid-users-bounces at lists.squid-cache.org] On Behalf Of Test User
> Sent: Friday, February 24, 2017 8:52 AM
> To: squid-users at lists.squid-cache.org
> Subject: [squid-users] SSL-Bump: NAT/TPROXY lookup failed to locate original IPs
>
> Hi,
> Sorry I am asking this question again. I am trying to setup HTTPS
> proxy using ssl-bump. I have followed
> steps mentioned in:
> http://wiki.squid-cache.org/ConfigExamples/Intercept/SslBumpExplicit
>
> Following are Squid setup details:
>
> Squid Cache: Version 3.5.12
> Service Name: squid
> Ubuntu linux
>
> configure options:  '--build=x86_64-linux-gnu' '--prefix=/usr'
> '--includedir=${prefix}/include' '--mandir=${prefix}/share/man'
> '--infodir=${prefix}/share/info' '--sysconfdir=/etc'
> '--localstatedir=/var' '--libexecdir=${prefix}/lib/squid3'
> '--srcdir=.' '--disable-maintainer-mode'
> '--disable-dependency-tracking' '--disable-silent-rules'
> 'BUILDCXXFLAGS=-g -O2 -fPIE -fstack-protector-strong -Wformat
> -Werror=format-security -Wl,-Bsymbolic-functions -fPIE -pie
> -Wl,-z,relro -Wl,-z,now' '--datadir=/usr/share/squid'
> '--sysconfdir=/etc/squid' '--libexecdir=/usr/lib/squid'
> '--mandir=/usr/share/man' '--enable-inline' '--disable-arch-native'
> '--enable-async-io=8' '--enable-storeio=ufs,aufs,diskd,rock'
> '--enable-removal-policies=lru,heap' '--enable-delay-pools'
> '--enable-cache-digests' '--enable-icap-client'
> '--enable-follow-x-forwarded-for'
> '--enable-auth-basic=DB,fake,getpwnam,LDAP,NCSA,NIS,PAM,POP3,RADIUS,SASL,SMB'
> '--enable-auth-digest=file,LDAP'
> '--enable-auth-negotiate=kerberos,wrapper'
> '--enable-auth-ntlm=fake,smb_lm'
> '--enable-external-acl-helpers=file_userip,kerberos_ldap_group,LDAP_group,session,SQL_session,unix_group,wbinfo_group'
> '--enable-url-rewrite-helpers=fake' '--enable-eui' '--enable-esi'
> '--enable-icmp' '--enable-zph-qos' '--enable-ecap' '--with-openssl'
> '--enable-ssl-crtd' '--disable-translation'
> '--with-swapdir=/var/spool/squid' '--with-logdir=/var/log/squid'
> '--with-pidfile=/var/run/squid.pid' '--with-filedescriptors=65536'
> '--with-large-files' '--with-default-user=proxy'
> '--enable-build-info=Ubuntu linux' '--enable-linux-netfilter'
> 'build_alias=x86_64-linux-gnu' 'CFLAGS=-g -O2 -fPIE
> -fstack-protector-strong -Wformat -Werror=format-security -Wall'
> 'LDFLAGS=-Wl,-Bsymbolic-functions -fPIE -pie -Wl,-z,relro -Wl,-z,now'
> 'CPPFLAGS=-Wdate-time -D_FORTIFY_SOURCE=2' 'CXXFLAGS=-g -O2 -fPIE
> -fstack-protector-strong -Wformat -Werror=format-security'
>
>
> Following is my squid.conf file:
>
> acl SSL_ports port 443
> acl Safe_ports port 80 # http
> acl Safe_ports port 21 # ftp
> acl Safe_ports port 443 # https
> acl Safe_ports port 70 # gopher
> acl Safe_ports port 210 # wais
> acl Safe_ports port 1025-65535 # unregistered ports
> acl Safe_ports port 280 # http-mgmt
> acl Safe_ports port 488 # gss-http
> acl Safe_ports port 591 # filemaker
> acl Safe_ports port 777 # multiling http
> acl CONNECT method CONNECT
> acl step1 at_step SslBump1
> http_access deny !Safe_ports
> http_access deny CONNECT !SSL_ports
> http_access allow localhost manager
> http_access deny manager
> http_access allow localhost
> http_access allow all
> http_port 3128 ssl-bump \
>   cert=/etc/squid/ssl_cert/squidCA.pem \
>   generate-host-certificates=on dynamic_cert_mem_cache_size=4MB
> https_port 3129 intercept ssl-bump generate-host-certificates=on \
> dynamic_cert_mem_cache_size=4MB cert=/etc/squid/ssl_cert/squidCA.pem \
> dhparams=/etc/squid/ssl_cert/dhparam.pem
> sslproxy_options NO_SSLv2,NO_SSLv3,SINGLE_DH_USE
> sslproxy_cipher
> EECDH+ECDSA+AESGCM:EECDH+aRSA+AESGCM:EECDH+ECDSA+SHA384:EECDH+ECDSA+SHA256:EECDH+aRSA+SHA384:EECDH+aRSA+SHA256:EECDH+aRSA+RC4:EECDH:EDH+aRSA:!RC4:!aNULL:!eNULL:!LOW:!3DES:!MD5:!EXP:!PSK:!SRP:!DSS
> sslcrtd_program /usr/lib/squid/ssl_crtd -s /var/spool/squid3_ssldb -M 4MB
> debug_options ALL,1 3,5 4,5 11,5 17,5 23,5 46,5 78,5 rotate=1
> coredump_dir /var/spool/squid
> refresh_pattern ^ftp: 1440 20% 10080
> refresh_pattern ^gopher: 1440 0% 1440
> refresh_pattern -i (/cgi-bin/|\?) 0 0% 0
> refresh_pattern (Release|Packages(.gz)*)$      0       20%     2880
> refresh_pattern . 0 20% 4320
>
>
> I get no errors while starting Squid. Following are the logs when Squid starts:
>
> 2017/02/23 09:59:53 kid1| Set Current Directory to /var/spool/squid
> 2017/02/23 09:59:53 kid1| Starting Squid Cache version 3.5.12 for
> x86_64-pc-linux-gnu...
> 2017/02/23 09:59:53 kid1| Service Name: squid
> 2017/02/23 09:59:53 kid1| Process ID 26236
> 2017/02/23 09:59:53 kid1| Process Roles: worker
> 2017/02/23 09:59:53 kid1| With 65535 file descriptors available
> 2017/02/23 09:59:53 kid1| Initializing IP Cache...
> 2017/02/23 09:59:53.756 kid1| 78,2| dns_internal.cc(1525) dnsInit:
> idnsInit: attempt open DNS socket to: [::]
> 2017/02/23 09:59:53.756 kid1| 78,2| dns_internal.cc(1534) dnsInit:
> idnsInit: attempt open DNS socket to: 0.0.0.0
> 2017/02/23 09:59:53.756 kid1| DNS Socket created at [::], FD 6
> 2017/02/23 09:59:53.756 kid1| DNS Socket created at 0.0.0.0, FD 7
> 2017/02/23 09:59:53.756 kid1| Adding nameserver 172.31.0.2 from /etc/resolv.conf
> 2017/02/23 09:59:53.756 kid1| 78,3| dns_internal.cc(321)
> idnsAddNameserver: idnsAddNameserver: Added nameserver #0
> (172.31.0.2:53)
> 2017/02/23 09:59:53.756 kid1| Adding domain
> ap-south-1.compute.internal from /etc/resolv.conf
> 2017/02/23 09:59:53.756 kid1| 78,3| dns_internal.cc(350)
> idnsAddPathComponent: idnsAddPathComponent: Added domain #0:
> ap-south-1.compute.internal
> 2017/02/23 09:59:53.756 kid1| helperOpenServers: Starting 5/32
> 'ssl_crtd' processes
> 2017/02/23 09:59:53.775 kid1| 46,2| Format.cc(64) parse: got
> definition '%>a/%>A %un %>rm myip=%la myport=%lp'
> 2017/02/23 09:59:53.775 kid1| 46,5| Token.cc(380) parse: scan for
> possible Misc token
> 2017/02/23 09:59:53.775 kid1| 46,5| Token.cc(384) parse: scan for
> possible 2C token
> 2017/02/23 09:59:53.775 kid1| 46,5| Token.cc(389) parse: scan for
> possible 1C token
> 2017/02/23 09:59:53.775 kid1| 46,5| Token.cc(380) parse: scan for
> possible Misc token
> 2017/02/23 09:59:53.775 kid1| 46,5| Token.cc(384) parse: scan for
> possible 2C token
> 2017/02/23 09:59:53.775 kid1| 46,5| Token.cc(389) parse: scan for
> possible 1C token
> 2017/02/23 09:59:53.775 kid1| 46,5| Token.cc(380) parse: scan for
> possible Misc token
> 2017/02/23 09:59:53.775 kid1| 46,5| Token.cc(384) parse: scan for
> possible 2C token
> 2017/02/23 09:59:53.775 kid1| 46,5| Token.cc(380) parse: scan for
> possible Misc token
> 2017/02/23 09:59:53.775 kid1| 46,5| Token.cc(384) parse: scan for
> possible 2C token
> 2017/02/23 09:59:53.775 kid1| 46,5| Token.cc(380) parse: scan for
> possible Misc token
> 2017/02/23 09:59:53.775 kid1| 46,5| Token.cc(384) parse: scan for
> possible 2C token
> 2017/02/23 09:59:53.775 kid1| 46,5| Token.cc(380) parse: scan for
> possible Misc token
> 2017/02/23 09:59:53.775 kid1| 46,5| Token.cc(384) parse: scan for
> possible 2C token
> 2017/02/23 09:59:53.775 kid1| 46,2| Format.cc(64) parse: got
> definition '%>a/%>A %un %>rm myip=%la myport=%lp'
> 2017/02/23 09:59:53.775 kid1| 46,5| Token.cc(380) parse: scan for
> possible Misc token
> 2017/02/23 09:59:53.775 kid1| 46,5| Token.cc(384) parse: scan for
> possible 2C token
> 2017/02/23 09:59:53.775 kid1| 46,5| Token.cc(389) parse: scan for
> possible 1C token
> 2017/02/23 09:59:53.775 kid1| 46,5| Token.cc(380) parse: scan for
> possible Misc token
> 2017/02/23 09:59:53.775 kid1| 46,5| Token.cc(384) parse: scan for
> possible 2C token
> 2017/02/23 09:59:53.775 kid1| 46,5| Token.cc(389) parse: scan for
> possible 1C token
> 2017/02/23 09:59:53.775 kid1| 46,5| Token.cc(380) parse: scan for
> possible Misc token
> 2017/02/23 09:59:53.775 kid1| 46,5| Token.cc(384) parse: scan for
> possible 2C token
> 2017/02/23 09:59:53.775 kid1| 46,5| Token.cc(380) parse: scan for
> possible Misc token
> 2017/02/23 09:59:53.775 kid1| 46,5| Token.cc(384) parse: scan for
> possible 2C token
> 2017/02/23 09:59:53.775 kid1| 46,5| Token.cc(380) parse: scan for
> possible Misc token
> 2017/02/23 09:59:53.775 kid1| 46,5| Token.cc(384) parse: scan for
> possible 2C token
> 2017/02/23 09:59:53.775 kid1| 46,5| Token.cc(380) parse: scan for
> possible Misc token
> 2017/02/23 09:59:53.775 kid1| 46,5| Token.cc(384) parse: scan for
> possible 2C token
> 2017/02/23 09:59:53.775 kid1| Logfile: opening log
> daemon:/var/log/squid/access.log
> 2017/02/23 09:59:53.775 kid1| Logfile Daemon: opening log
> /var/log/squid/access.log
> 2017/02/23 09:59:53.779 kid1| 23,5| url.cc(43) urlInitialize:
> urlInitialize: Initializing...
> 2017/02/23 09:59:53.779 kid1| Local cache digest enabled;
> rebuild/rewrite every 3600/3600 sec
> 2017/02/23 09:59:53.779 kid1| Store logging disabled
> 2017/02/23 09:59:53.779 kid1| Swap maxSize 0 + 262144 KB, estimated
> 20164 objects
> 2017/02/23 09:59:53.779 kid1| Target number of buckets: 1008
> 2017/02/23 09:59:53.779 kid1| Using 8192 Store buckets
> 2017/02/23 09:59:53.779 kid1| Max Mem  size: 262144 KB
> 2017/02/23 09:59:53.779 kid1| Max Swap size: 0 KB
> 2017/02/23 09:59:53.779 kid1| Using Least Load store dir selection
> 2017/02/23 09:59:53.779 kid1| Set Current Directory to /var/spool/squid
> 2017/02/23 09:59:53.785 kid1| 23,3| url.cc(357) urlParse: urlParse:
> Split URL 'http://ip-172-31-25-235:3128/squid-internal-static/icons/silk/image.png'
> into proto='http', host='ip-172-31-25-235', port='3128',
> path='/squid-internal-static/icons/silk/image.png'
> 2017/02/23 09:59:53.785 kid1| 23,3| url.cc(357) urlParse: urlParse:
> Split URL 'http://ip-172-31-25-235:3128/squid-internal-static/icons/silk/page_white_text.png'
> into proto='http', host='ip-172-31-25-235', port='3128',
> path='/squid-internal-static/icons/silk/page_white_text.png'
>
> ****several urlParse logs like above. Removing them to shorten the
> email. Further logs below...****
>
> 2017/02/23 09:59:53.815 kid1| Finished loading MIME types and icons.
> 2017/02/23 09:59:53.815 kid1| HTCP Disabled.
> 2017/02/23 09:59:53.815 kid1| Pinger socket opened on FD 25
> 2017/02/23 09:59:53.815 kid1| Squid plugin modules loaded: 0
> 2017/02/23 09:59:53.815 kid1| Adaptation support is off.
> 2017/02/23 09:59:53.815 kid1| Accepting SSL bumped HTTP Socket
> connections at local=[::]:3128 remote=[::] FD 22 flags=9
> 2017/02/23 09:59:53.815 kid1| Accepting NAT intercepted SSL bumped
> HTTPS Socket connections at local=[::]:3129 remote=[::] FD 23 flags=41
> 2017/02/23 09:59:53| pinger: Initialising ICMP pinger ...
> 2017/02/23 09:59:53| pinger: ICMP socket opened.
> 2017/02/23 09:59:53| pinger: ICMPv6 socket opened
> 2017/02/23 09:59:54 kid1| storeLateRelease: released 0 objects
>
>
>
> I tested this setup by providing proxy details to Firefox. Firefox was
> able to show HTTP websites but when I tried to open an HTTPS website I
> got following error:
>
> 2017/02/23 11:00:50 kid1| ERROR: NF getsockopt(ORIGINAL_DST) failed on
> local=172.31.25.235:3129 remote=182.72.78.122:50655 FD 7 flags=33:
> (92) Protocol not available
> 2017/02/23 11:00:50 kid1| ERROR: NAT/TPROXY lookup failed to locate
> original IPs on local=172.31.25.235:3129 remote=182.72.78.122:50655 FD
> 7 flags=33
> 2017/02/23 11:00:50 kid1| ERROR: NF getsockopt(ORIGINAL_DST) failed on
> local=172.31.25.235:3129 remote=182.72.78.122:50656 FD 7 flags=33:
> (92) Protocol not available
> 2017/02/23 11:00:50 kid1| ERROR: NAT/TPROXY lookup failed to locate
> original IPs on local=172.31.25.235:3129 remote=182.72.78.122:50656 FD
> 7 flags=33
> 2017/02/23 11:00:50 kid1| ERROR: NF getsockopt(ORIGINAL_DST) failed on
> local=172.31.25.235:3129 remote=182.72.78.122:50657 FD 7 flags=33:
> (92) Protocol not available
> 2017/02/23 11:00:50 kid1| ERROR: NAT/TPROXY lookup failed to locate
> original IPs on local=172.31.25.235:3129 remote=182.72.78.122:50657 FD
> 7 flags=33
>
> I googled this error and found this mail thread which had similar problems:
> http://squid-web-proxy-cache.1019090.n4.nabble.com/NAT-TPROXY-lookup-failed-to-locate-original-IPs-td4675464.html
>
> I found this link from the above thread. I modified the steps for
> HTTPS from the below link:
> http://wiki.squid-cache.org/ConfigExamples/Intercept/LinuxDnat
>
> Now my sysctl.conf is:
>
> net.ipv4.conf.all.rp_filter=0
> net.ipv4.ip_forward = 1
> net.ipv4.conf.default.rp_filter = 0
> net.ipv4.conf.default.accept_source_route = 0
>
> My iptables -t nat -L result:
>
> Chain PREROUTING (policy ACCEPT)
> target     prot opt source               destination
> ACCEPT     tcp  --  ec2-35-154-101-8.ap-south-1.compute.amazonaws.com
> anywhere             tcp dpt:https
> DNAT       tcp  --  anywhere             anywhere             tcp
> dpt:https to:35.154.101.8:3129
>
> Chain INPUT (policy ACCEPT)
> target     prot opt source               destination
>
> Chain OUTPUT (policy ACCEPT)
> target     prot opt source               destination
>
> Chain POSTROUTING (policy ACCEPT)
> target     prot opt source               destination
> MASQUERADE  all  --  anywhere             anywhere
>
>
> Once this was done, I tried to hit HTTPS website from Firefox and now
> I get connection timeout error. Nothing shows in syslog, access.log or
> cache.log. Could you please help me resolve this.
>
> Thanks,
> Michael
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>


Thanks for replying Eliezer. Following are the outputs you asked:

1. iptables-save:

# Generated by iptables-save v1.6.0 on Sun Feb 26 06:28:46 2017
*filter
:INPUT ACCEPT [171:12090]
:FORWARD ACCEPT [0:0]
:OUTPUT ACCEPT [106:15187]
COMMIT
# Completed on Sun Feb 26 06:28:46 2017
# Generated by iptables-save v1.6.0 on Sun Feb 26 06:28:46 2017
*mangle
:PREROUTING ACCEPT [89003:74850371]
:INPUT ACCEPT [88973:74849159]
:FORWARD ACCEPT [30:1212]
:OUTPUT ACCEPT [76710:51478183]
:POSTROUTING ACCEPT [76740:51479395]
-A PREROUTING -p tcp -m tcp --dport 3129 -j DROP
COMMIT
# Completed on Sun Feb 26 06:28:46 2017
# Generated by iptables-save v1.6.0 on Sun Feb 26 06:28:46 2017
*nat
:PREROUTING ACCEPT [7766:436942]
:INPUT ACCEPT [7766:436942]
:OUTPUT ACCEPT [952:102330]
:POSTROUTING ACCEPT [0:0]
-A PREROUTING -s 35.154.101.8/32 -p tcp -m tcp --dport 443 -j ACCEPT
-A PREROUTING -p tcp -m tcp --dport 443 -j DNAT --to-destination
35.154.101.8:3129
-A POSTROUTING -j MASQUERADE
COMMIT
# Completed on Sun Feb 26 06:28:46 2017

2. Also pasting sudo iptables -L -nv:

Chain INPUT (policy ACCEPT 216 packets, 16058 bytes)
 pkts bytes target     prot opt in     out     source
destination

Chain FORWARD (policy ACCEPT 0 packets, 0 bytes)
 pkts bytes target     prot opt in     out     source
destination

Chain OUTPUT (policy ACCEPT 161 packets, 24629 bytes)
 pkts bytes target     prot opt in     out     source               destination



> And then clear out where is this proxy sittings and the network structure.
> It's not clear if the squid box is the router or a machine somewhere on AWS.

[Michael] This proxy is installed on an AWS instance.

> If you wish to pass traffic from a local router to a one on AWS you will need to create a tunnel like using OpenVPN or a similar solution and to use some routing rules to pass the traffic from the local LAN to AWS without removing the original destination address.
>

[Michael] Does this mean, to make ssl-bump work, I will have to setup
a VPN server and configure the VPN clients to use this proxy via VPN
server?


Thanks,
Michael.



From tuser6485 at gmail.com  Mon Feb 27 05:41:08 2017
From: tuser6485 at gmail.com (Test User)
Date: Mon, 27 Feb 2017 11:11:08 +0530
Subject: [squid-users] SSL-Bump: NAT/TPROXY lookup failed to locate
 original IPs
In-Reply-To: <7a2501d29076$9dd0c050$d97240f0$@ngtech.co.il>
References: <CAPhcQuJU-ctRMokvU6kg+RqkfXDOU_0y+o+fEUADdoX4AstXaQ@mail.gmail.com>
 <544701d28fee$a1a6b050$e4f410f0$@ngtech.co.il>
 <CAPhcQuJEFmVPQ=C9=KAzuKzZjqKoQo=hN7wvR9UX2aPM98LqEw@mail.gmail.com>
 <7a2301d29061$468aba90$d3a02fb0$@ngtech.co.il>
 <7a2501d29076$9dd0c050$d97240f0$@ngtech.co.il>
Message-ID: <CAPhcQuJCeKZYSwmefsAxsFx7kHCWCkOg6kkmgH-WE4Sz2=sXAA@mail.gmail.com>

On Mon, Feb 27, 2017 at 2:53 AM, Eliezer Croitoru <eliezer at ngtech.co.il> wrote:
> Let me know if you need some help..
>
> Eliezer
>
> ----
> Eliezer Croitoru
> Linux System Administrator
> Mobile: +972-5-28704261
> Email: eliezer at ngtech.co.il
>
>
> -----Original Message-----
> From: squid-users [mailto:squid-users-bounces at lists.squid-cache.org] On Behalf Of Eliezer Croitoru
> Sent: Sunday, February 26, 2017 8:51 PM
> To: 'Test User' <tuser6485 at gmail.com>
> Cc: squid-users at lists.squid-cache.org
> Subject: Re: [squid-users] SSL-Bump: NAT/TPROXY lookup failed to locate original IPs
>
> Hey Michael,
>
> The details you attached explained pretty well the cause for the issues you have described.
> What you will need to do in order to make this setup to work can be done in more then one way.
> For a sysadmin the simplest way is to create a VPN or some kind of a tunnel between the AWS instance to the local router.
> I am almost sure that you can use haproxy to do a local tproxy or interception that will forward the traffic to the remote squid with the PROXY protocol keeping original source and original destination visible to the remote squid.
>
> The choice will depend on both:
> - your skills and will to dig some time about couple subjects
> - The availability of static IP addresses(both local and AWS).
> - The OS on both sides
>
> I believe that the next haproxy settings can be used as a compromise to a tunnel:
> http://ngtech.co.il/paste/1605/
> And some tproxy route and iptables rules ..
> With a squid.conf which will be similar to:
> acl frontend src 100.0.0.1
> proxy_protocol_access allow frontend
> http_port 3127
> http_port 3128 require-proxy-header ... ssl-bump settings
> ##END of example
>
> However I do still believe that the more secure way would be to use some kind of vpn tunnel like OpenVPN between the local router to the remote AWS instance.
>
> All The Bests,
> Eliezer
>
> ----
> Eliezer Croitoru
> Linux System Administrator
> Mobile: +972-5-28704261
> Email: eliezer at ngtech.co.il
>
>
> -----Original Message-----
> From: Test User [mailto:tuser6485 at gmail.com]
> Sent: Sunday, February 26, 2017 8:38 AM
> To: Eliezer Croitoru <eliezer at ngtech.co.il>
> Cc: squid-users at lists.squid-cache.org
> Subject: Re: [squid-users] SSL-Bump: NAT/TPROXY lookup failed to locate original IPs
>
> On Sun, Feb 26, 2017 at 10:40 AM, Eliezer Croitoru <eliezer at ngtech.co.il> wrote:
>> Hey Michael,
>>
>> You will need to clear out couple things for us.
>> First we will need one of the next ouputs or both:
>> iptables-save
>> iptables -L -nv
>>
>> And then clear out where is this proxy sittings and the network structure.
>> It's not clear if the squid box is the router or a machine somewhere on AWS.
>> If you wish to pass traffic from a local router to a one on AWS you will need to create a tunnel like using OpenVPN or a similar solution and to use some routing rules to pass the traffic from the local LAN to AWS without removing the original destination address.
>>
>> When more details on the setup will be available it will be much simpler to understand what is the root for some of the issues you are having.
>>
>> All The Bests,
>> Eliezer
>>
>> ----
>> Eliezer Croitoru
>> Linux System Administrator
>> Mobile: +972-5-28704261
>> Email: eliezer at ngtech.co.il
>>
>>
>> -----Original Message-----
>> From: squid-users [mailto:squid-users-bounces at lists.squid-cache.org] On Behalf Of Test User
>> Sent: Friday, February 24, 2017 8:52 AM
>> To: squid-users at lists.squid-cache.org
>> Subject: [squid-users] SSL-Bump: NAT/TPROXY lookup failed to locate original IPs
>>
>> Hi,
>> Sorry I am asking this question again. I am trying to setup HTTPS
>> proxy using ssl-bump. I have followed
>> steps mentioned in:
>> http://wiki.squid-cache.org/ConfigExamples/Intercept/SslBumpExplicit
>>
>> Following are Squid setup details:
>>
>> Squid Cache: Version 3.5.12
>> Service Name: squid
>> Ubuntu linux
>>
>> configure options:  '--build=x86_64-linux-gnu' '--prefix=/usr'
>> '--includedir=${prefix}/include' '--mandir=${prefix}/share/man'
>> '--infodir=${prefix}/share/info' '--sysconfdir=/etc'
>> '--localstatedir=/var' '--libexecdir=${prefix}/lib/squid3'
>> '--srcdir=.' '--disable-maintainer-mode'
>> '--disable-dependency-tracking' '--disable-silent-rules'
>> 'BUILDCXXFLAGS=-g -O2 -fPIE -fstack-protector-strong -Wformat
>> -Werror=format-security -Wl,-Bsymbolic-functions -fPIE -pie
>> -Wl,-z,relro -Wl,-z,now' '--datadir=/usr/share/squid'
>> '--sysconfdir=/etc/squid' '--libexecdir=/usr/lib/squid'
>> '--mandir=/usr/share/man' '--enable-inline' '--disable-arch-native'
>> '--enable-async-io=8' '--enable-storeio=ufs,aufs,diskd,rock'
>> '--enable-removal-policies=lru,heap' '--enable-delay-pools'
>> '--enable-cache-digests' '--enable-icap-client'
>> '--enable-follow-x-forwarded-for'
>> '--enable-auth-basic=DB,fake,getpwnam,LDAP,NCSA,NIS,PAM,POP3,RADIUS,SASL,SMB'
>> '--enable-auth-digest=file,LDAP'
>> '--enable-auth-negotiate=kerberos,wrapper'
>> '--enable-auth-ntlm=fake,smb_lm'
>> '--enable-external-acl-helpers=file_userip,kerberos_ldap_group,LDAP_group,session,SQL_session,unix_group,wbinfo_group'
>> '--enable-url-rewrite-helpers=fake' '--enable-eui' '--enable-esi'
>> '--enable-icmp' '--enable-zph-qos' '--enable-ecap' '--with-openssl'
>> '--enable-ssl-crtd' '--disable-translation'
>> '--with-swapdir=/var/spool/squid' '--with-logdir=/var/log/squid'
>> '--with-pidfile=/var/run/squid.pid' '--with-filedescriptors=65536'
>> '--with-large-files' '--with-default-user=proxy'
>> '--enable-build-info=Ubuntu linux' '--enable-linux-netfilter'
>> 'build_alias=x86_64-linux-gnu' 'CFLAGS=-g -O2 -fPIE
>> -fstack-protector-strong -Wformat -Werror=format-security -Wall'
>> 'LDFLAGS=-Wl,-Bsymbolic-functions -fPIE -pie -Wl,-z,relro -Wl,-z,now'
>> 'CPPFLAGS=-Wdate-time -D_FORTIFY_SOURCE=2' 'CXXFLAGS=-g -O2 -fPIE
>> -fstack-protector-strong -Wformat -Werror=format-security'
>>
>>
>> Following is my squid.conf file:
>>
>> acl SSL_ports port 443
>> acl Safe_ports port 80 # http
>> acl Safe_ports port 21 # ftp
>> acl Safe_ports port 443 # https
>> acl Safe_ports port 70 # gopher
>> acl Safe_ports port 210 # wais
>> acl Safe_ports port 1025-65535 # unregistered ports
>> acl Safe_ports port 280 # http-mgmt
>> acl Safe_ports port 488 # gss-http
>> acl Safe_ports port 591 # filemaker
>> acl Safe_ports port 777 # multiling http
>> acl CONNECT method CONNECT
>> acl step1 at_step SslBump1
>> http_access deny !Safe_ports
>> http_access deny CONNECT !SSL_ports
>> http_access allow localhost manager
>> http_access deny manager
>> http_access allow localhost
>> http_access allow all
>> http_port 3128 ssl-bump \
>>   cert=/etc/squid/ssl_cert/squidCA.pem \
>>   generate-host-certificates=on dynamic_cert_mem_cache_size=4MB
>> https_port 3129 intercept ssl-bump generate-host-certificates=on \
>> dynamic_cert_mem_cache_size=4MB cert=/etc/squid/ssl_cert/squidCA.pem \
>> dhparams=/etc/squid/ssl_cert/dhparam.pem
>> sslproxy_options NO_SSLv2,NO_SSLv3,SINGLE_DH_USE
>> sslproxy_cipher
>> EECDH+ECDSA+AESGCM:EECDH+aRSA+AESGCM:EECDH+ECDSA+SHA384:EECDH+ECDSA+SHA256:EECDH+aRSA+SHA384:EECDH+aRSA+SHA256:EECDH+aRSA+RC4:EECDH:EDH+aRSA:!RC4:!aNULL:!eNULL:!LOW:!3DES:!MD5:!EXP:!PSK:!SRP:!DSS
>> sslcrtd_program /usr/lib/squid/ssl_crtd -s /var/spool/squid3_ssldb -M 4MB
>> debug_options ALL,1 3,5 4,5 11,5 17,5 23,5 46,5 78,5 rotate=1
>> coredump_dir /var/spool/squid
>> refresh_pattern ^ftp: 1440 20% 10080
>> refresh_pattern ^gopher: 1440 0% 1440
>> refresh_pattern -i (/cgi-bin/|\?) 0 0% 0
>> refresh_pattern (Release|Packages(.gz)*)$      0       20%     2880
>> refresh_pattern . 0 20% 4320
>>
>>
>> I get no errors while starting Squid. Following are the logs when Squid starts:
>>
>> 2017/02/23 09:59:53 kid1| Set Current Directory to /var/spool/squid
>> 2017/02/23 09:59:53 kid1| Starting Squid Cache version 3.5.12 for
>> x86_64-pc-linux-gnu...
>> 2017/02/23 09:59:53 kid1| Service Name: squid
>> 2017/02/23 09:59:53 kid1| Process ID 26236
>> 2017/02/23 09:59:53 kid1| Process Roles: worker
>> 2017/02/23 09:59:53 kid1| With 65535 file descriptors available
>> 2017/02/23 09:59:53 kid1| Initializing IP Cache...
>> 2017/02/23 09:59:53.756 kid1| 78,2| dns_internal.cc(1525) dnsInit:
>> idnsInit: attempt open DNS socket to: [::]
>> 2017/02/23 09:59:53.756 kid1| 78,2| dns_internal.cc(1534) dnsInit:
>> idnsInit: attempt open DNS socket to: 0.0.0.0
>> 2017/02/23 09:59:53.756 kid1| DNS Socket created at [::], FD 6
>> 2017/02/23 09:59:53.756 kid1| DNS Socket created at 0.0.0.0, FD 7
>> 2017/02/23 09:59:53.756 kid1| Adding nameserver 172.31.0.2 from /etc/resolv.conf
>> 2017/02/23 09:59:53.756 kid1| 78,3| dns_internal.cc(321)
>> idnsAddNameserver: idnsAddNameserver: Added nameserver #0
>> (172.31.0.2:53)
>> 2017/02/23 09:59:53.756 kid1| Adding domain
>> ap-south-1.compute.internal from /etc/resolv.conf
>> 2017/02/23 09:59:53.756 kid1| 78,3| dns_internal.cc(350)
>> idnsAddPathComponent: idnsAddPathComponent: Added domain #0:
>> ap-south-1.compute.internal
>> 2017/02/23 09:59:53.756 kid1| helperOpenServers: Starting 5/32
>> 'ssl_crtd' processes
>> 2017/02/23 09:59:53.775 kid1| 46,2| Format.cc(64) parse: got
>> definition '%>a/%>A %un %>rm myip=%la myport=%lp'
>> 2017/02/23 09:59:53.775 kid1| 46,5| Token.cc(380) parse: scan for
>> possible Misc token
>> 2017/02/23 09:59:53.775 kid1| 46,5| Token.cc(384) parse: scan for
>> possible 2C token
>> 2017/02/23 09:59:53.775 kid1| 46,5| Token.cc(389) parse: scan for
>> possible 1C token
>> 2017/02/23 09:59:53.775 kid1| 46,5| Token.cc(380) parse: scan for
>> possible Misc token
>> 2017/02/23 09:59:53.775 kid1| 46,5| Token.cc(384) parse: scan for
>> possible 2C token
>> 2017/02/23 09:59:53.775 kid1| 46,5| Token.cc(389) parse: scan for
>> possible 1C token
>> 2017/02/23 09:59:53.775 kid1| 46,5| Token.cc(380) parse: scan for
>> possible Misc token
>> 2017/02/23 09:59:53.775 kid1| 46,5| Token.cc(384) parse: scan for
>> possible 2C token
>> 2017/02/23 09:59:53.775 kid1| 46,5| Token.cc(380) parse: scan for
>> possible Misc token
>> 2017/02/23 09:59:53.775 kid1| 46,5| Token.cc(384) parse: scan for
>> possible 2C token
>> 2017/02/23 09:59:53.775 kid1| 46,5| Token.cc(380) parse: scan for
>> possible Misc token
>> 2017/02/23 09:59:53.775 kid1| 46,5| Token.cc(384) parse: scan for
>> possible 2C token
>> 2017/02/23 09:59:53.775 kid1| 46,5| Token.cc(380) parse: scan for
>> possible Misc token
>> 2017/02/23 09:59:53.775 kid1| 46,5| Token.cc(384) parse: scan for
>> possible 2C token
>> 2017/02/23 09:59:53.775 kid1| 46,2| Format.cc(64) parse: got
>> definition '%>a/%>A %un %>rm myip=%la myport=%lp'
>> 2017/02/23 09:59:53.775 kid1| 46,5| Token.cc(380) parse: scan for
>> possible Misc token
>> 2017/02/23 09:59:53.775 kid1| 46,5| Token.cc(384) parse: scan for
>> possible 2C token
>> 2017/02/23 09:59:53.775 kid1| 46,5| Token.cc(389) parse: scan for
>> possible 1C token
>> 2017/02/23 09:59:53.775 kid1| 46,5| Token.cc(380) parse: scan for
>> possible Misc token
>> 2017/02/23 09:59:53.775 kid1| 46,5| Token.cc(384) parse: scan for
>> possible 2C token
>> 2017/02/23 09:59:53.775 kid1| 46,5| Token.cc(389) parse: scan for
>> possible 1C token
>> 2017/02/23 09:59:53.775 kid1| 46,5| Token.cc(380) parse: scan for
>> possible Misc token
>> 2017/02/23 09:59:53.775 kid1| 46,5| Token.cc(384) parse: scan for
>> possible 2C token
>> 2017/02/23 09:59:53.775 kid1| 46,5| Token.cc(380) parse: scan for
>> possible Misc token
>> 2017/02/23 09:59:53.775 kid1| 46,5| Token.cc(384) parse: scan for
>> possible 2C token
>> 2017/02/23 09:59:53.775 kid1| 46,5| Token.cc(380) parse: scan for
>> possible Misc token
>> 2017/02/23 09:59:53.775 kid1| 46,5| Token.cc(384) parse: scan for
>> possible 2C token
>> 2017/02/23 09:59:53.775 kid1| 46,5| Token.cc(380) parse: scan for
>> possible Misc token
>> 2017/02/23 09:59:53.775 kid1| 46,5| Token.cc(384) parse: scan for
>> possible 2C token
>> 2017/02/23 09:59:53.775 kid1| Logfile: opening log
>> daemon:/var/log/squid/access.log
>> 2017/02/23 09:59:53.775 kid1| Logfile Daemon: opening log
>> /var/log/squid/access.log
>> 2017/02/23 09:59:53.779 kid1| 23,5| url.cc(43) urlInitialize:
>> urlInitialize: Initializing...
>> 2017/02/23 09:59:53.779 kid1| Local cache digest enabled;
>> rebuild/rewrite every 3600/3600 sec
>> 2017/02/23 09:59:53.779 kid1| Store logging disabled
>> 2017/02/23 09:59:53.779 kid1| Swap maxSize 0 + 262144 KB, estimated
>> 20164 objects
>> 2017/02/23 09:59:53.779 kid1| Target number of buckets: 1008
>> 2017/02/23 09:59:53.779 kid1| Using 8192 Store buckets
>> 2017/02/23 09:59:53.779 kid1| Max Mem  size: 262144 KB
>> 2017/02/23 09:59:53.779 kid1| Max Swap size: 0 KB
>> 2017/02/23 09:59:53.779 kid1| Using Least Load store dir selection
>> 2017/02/23 09:59:53.779 kid1| Set Current Directory to /var/spool/squid
>> 2017/02/23 09:59:53.785 kid1| 23,3| url.cc(357) urlParse: urlParse:
>> Split URL 'http://ip-172-31-25-235:3128/squid-internal-static/icons/silk/image.png'
>> into proto='http', host='ip-172-31-25-235', port='3128',
>> path='/squid-internal-static/icons/silk/image.png'
>> 2017/02/23 09:59:53.785 kid1| 23,3| url.cc(357) urlParse: urlParse:
>> Split URL 'http://ip-172-31-25-235:3128/squid-internal-static/icons/silk/page_white_text.png'
>> into proto='http', host='ip-172-31-25-235', port='3128',
>> path='/squid-internal-static/icons/silk/page_white_text.png'
>>
>> ****several urlParse logs like above. Removing them to shorten the
>> email. Further logs below...****
>>
>> 2017/02/23 09:59:53.815 kid1| Finished loading MIME types and icons.
>> 2017/02/23 09:59:53.815 kid1| HTCP Disabled.
>> 2017/02/23 09:59:53.815 kid1| Pinger socket opened on FD 25
>> 2017/02/23 09:59:53.815 kid1| Squid plugin modules loaded: 0
>> 2017/02/23 09:59:53.815 kid1| Adaptation support is off.
>> 2017/02/23 09:59:53.815 kid1| Accepting SSL bumped HTTP Socket
>> connections at local=[::]:3128 remote=[::] FD 22 flags=9
>> 2017/02/23 09:59:53.815 kid1| Accepting NAT intercepted SSL bumped
>> HTTPS Socket connections at local=[::]:3129 remote=[::] FD 23 flags=41
>> 2017/02/23 09:59:53| pinger: Initialising ICMP pinger ...
>> 2017/02/23 09:59:53| pinger: ICMP socket opened.
>> 2017/02/23 09:59:53| pinger: ICMPv6 socket opened
>> 2017/02/23 09:59:54 kid1| storeLateRelease: released 0 objects
>>
>>
>>
>> I tested this setup by providing proxy details to Firefox. Firefox was
>> able to show HTTP websites but when I tried to open an HTTPS website I
>> got following error:
>>
>> 2017/02/23 11:00:50 kid1| ERROR: NF getsockopt(ORIGINAL_DST) failed on
>> local=172.31.25.235:3129 remote=182.72.78.122:50655 FD 7 flags=33:
>> (92) Protocol not available
>> 2017/02/23 11:00:50 kid1| ERROR: NAT/TPROXY lookup failed to locate
>> original IPs on local=172.31.25.235:3129 remote=182.72.78.122:50655 FD
>> 7 flags=33
>> 2017/02/23 11:00:50 kid1| ERROR: NF getsockopt(ORIGINAL_DST) failed on
>> local=172.31.25.235:3129 remote=182.72.78.122:50656 FD 7 flags=33:
>> (92) Protocol not available
>> 2017/02/23 11:00:50 kid1| ERROR: NAT/TPROXY lookup failed to locate
>> original IPs on local=172.31.25.235:3129 remote=182.72.78.122:50656 FD
>> 7 flags=33
>> 2017/02/23 11:00:50 kid1| ERROR: NF getsockopt(ORIGINAL_DST) failed on
>> local=172.31.25.235:3129 remote=182.72.78.122:50657 FD 7 flags=33:
>> (92) Protocol not available
>> 2017/02/23 11:00:50 kid1| ERROR: NAT/TPROXY lookup failed to locate
>> original IPs on local=172.31.25.235:3129 remote=182.72.78.122:50657 FD
>> 7 flags=33
>>
>> I googled this error and found this mail thread which had similar problems:
>> http://squid-web-proxy-cache.1019090.n4.nabble.com/NAT-TPROXY-lookup-failed-to-locate-original-IPs-td4675464.html
>>
>> I found this link from the above thread. I modified the steps for
>> HTTPS from the below link:
>> http://wiki.squid-cache.org/ConfigExamples/Intercept/LinuxDnat
>>
>> Now my sysctl.conf is:
>>
>> net.ipv4.conf.all.rp_filter=0
>> net.ipv4.ip_forward = 1
>> net.ipv4.conf.default.rp_filter = 0
>> net.ipv4.conf.default.accept_source_route = 0
>>
>> My iptables -t nat -L result:
>>
>> Chain PREROUTING (policy ACCEPT)
>> target     prot opt source               destination
>> ACCEPT     tcp  --  ec2-35-154-101-8.ap-south-1.compute.amazonaws.com
>> anywhere             tcp dpt:https
>> DNAT       tcp  --  anywhere             anywhere             tcp
>> dpt:https to:35.154.101.8:3129
>>
>> Chain INPUT (policy ACCEPT)
>> target     prot opt source               destination
>>
>> Chain OUTPUT (policy ACCEPT)
>> target     prot opt source               destination
>>
>> Chain POSTROUTING (policy ACCEPT)
>> target     prot opt source               destination
>> MASQUERADE  all  --  anywhere             anywhere
>>
>>
>> Once this was done, I tried to hit HTTPS website from Firefox and now
>> I get connection timeout error. Nothing shows in syslog, access.log or
>> cache.log. Could you please help me resolve this.
>>
>> Thanks,
>> Michael
>> _______________________________________________
>> squid-users mailing list
>> squid-users at lists.squid-cache.org
>> http://lists.squid-cache.org/listinfo/squid-users
>>
>
>
> Thanks for replying Eliezer. Following are the outputs you asked:
>
> 1. iptables-save:
>
> # Generated by iptables-save v1.6.0 on Sun Feb 26 06:28:46 2017
> *filter
> :INPUT ACCEPT [171:12090]
> :FORWARD ACCEPT [0:0]
> :OUTPUT ACCEPT [106:15187]
> COMMIT
> # Completed on Sun Feb 26 06:28:46 2017
> # Generated by iptables-save v1.6.0 on Sun Feb 26 06:28:46 2017
> *mangle
> :PREROUTING ACCEPT [89003:74850371]
> :INPUT ACCEPT [88973:74849159]
> :FORWARD ACCEPT [30:1212]
> :OUTPUT ACCEPT [76710:51478183]
> :POSTROUTING ACCEPT [76740:51479395]
> -A PREROUTING -p tcp -m tcp --dport 3129 -j DROP
> COMMIT
> # Completed on Sun Feb 26 06:28:46 2017
> # Generated by iptables-save v1.6.0 on Sun Feb 26 06:28:46 2017
> *nat
> :PREROUTING ACCEPT [7766:436942]
> :INPUT ACCEPT [7766:436942]
> :OUTPUT ACCEPT [952:102330]
> :POSTROUTING ACCEPT [0:0]
> -A PREROUTING -s 35.154.101.8/32 -p tcp -m tcp --dport 443 -j ACCEPT
> -A PREROUTING -p tcp -m tcp --dport 443 -j DNAT --to-destination
> 35.154.101.8:3129
> -A POSTROUTING -j MASQUERADE
> COMMIT
> # Completed on Sun Feb 26 06:28:46 2017
>
> 2. Also pasting sudo iptables -L -nv:
>
> Chain INPUT (policy ACCEPT 216 packets, 16058 bytes)
>  pkts bytes target     prot opt in     out     source
> destination
>
> Chain FORWARD (policy ACCEPT 0 packets, 0 bytes)
>  pkts bytes target     prot opt in     out     source
> destination
>
> Chain OUTPUT (policy ACCEPT 161 packets, 24629 bytes)
>  pkts bytes target     prot opt in     out     source               destination
>
>
>
>> And then clear out where is this proxy sittings and the network structure.
>> It's not clear if the squid box is the router or a machine somewhere on AWS.
>
> [Michael] This proxy is installed on an AWS instance.
>
>> If you wish to pass traffic from a local router to a one on AWS you will need to create a tunnel like using OpenVPN or a similar solution and to use some routing rules to pass the traffic from the local LAN to AWS without removing the original destination address.
>>
>
> [Michael] Does this mean, to make ssl-bump work, I will have to setup
> a VPN server and configure the VPN clients to use this proxy via VPN
> server?
>
>
> Thanks,
> Michael.
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>



Thanks for replying Eliezer. Your advice is much appreciated.

> The details you attached explained pretty well the cause for the issues you have described.
> What you will need to do in order to make this setup to work can be done in more then one way.
> For a sysadmin the simplest way is to create a VPN or some kind of a tunnel between the AWS instance to the local router.
> I am almost sure that you can use haproxy to do a local tproxy or interception that will forward the traffic to the remote squid with the PROXY protocol keeping original source and original destination visible to the remote squid.
>
> The choice will depend on both:
> - your skills and will to dig some time about couple subjects
> - The availability of static IP addresses(both local and AWS).
> - The OS on both sides

[Michael] Actually, my original setup involves a VPN server. I wasn't
using it because I wanted to setup ssl-bump with simplest possible
settings. My actual setup involves:

1. strongSwan IPSec VPN server
2. Squid Proxy server
3. Clients will be IPSec VPN clients. I can specify the IP address and
port of HTTPS Proxy server in IPSec VPN client itself.

In the above setup described, will I have to do something extra to
make ssl-bump work?

Thanks,
Michael.


From odhiambo at gmail.com  Mon Feb 27 05:44:03 2017
From: odhiambo at gmail.com (Odhiambo Washington)
Date: Mon, 27 Feb 2017 08:44:03 +0300
Subject: [squid-users] SSL-Bump: NAT/TPROXY lookup failed to locate
 original IPs
In-Reply-To: <CAPhcQuJCeKZYSwmefsAxsFx7kHCWCkOg6kkmgH-WE4Sz2=sXAA@mail.gmail.com>
References: <CAPhcQuJU-ctRMokvU6kg+RqkfXDOU_0y+o+fEUADdoX4AstXaQ@mail.gmail.com>
 <544701d28fee$a1a6b050$e4f410f0$@ngtech.co.il>
 <CAPhcQuJEFmVPQ=C9=KAzuKzZjqKoQo=hN7wvR9UX2aPM98LqEw@mail.gmail.com>
 <7a2301d29061$468aba90$d3a02fb0$@ngtech.co.il>
 <7a2501d29076$9dd0c050$d97240f0$@ngtech.co.il>
 <CAPhcQuJCeKZYSwmefsAxsFx7kHCWCkOg6kkmgH-WE4Sz2=sXAA@mail.gmail.com>
Message-ID: <CAAdA2WPMEYr9ups=8L_=s-j-JT8R741UN4Sm-N7jWqS_Xac+tg@mail.gmail.com>

On 27 February 2017 at 08:41, Test User <tuser6485 at gmail.com> wrote:

> On Mon, Feb 27, 2017 at 2:53 AM, Eliezer Croitoru <eliezer at ngtech.co.il>
> wrote:
> > Let me know if you need some help..
> >
> > Eliezer
> >
> > ----
> > Eliezer Croitoru
> > Linux System Administrator
> > Mobile: +972-5-28704261
> > Email: eliezer at ngtech.co.il
> >
> >
> > -----Original Message-----
> > From: squid-users [mailto:squid-users-bounces at lists.squid-cache.org] On
> Behalf Of Eliezer Croitoru
> > Sent: Sunday, February 26, 2017 8:51 PM
> > To: 'Test User' <tuser6485 at gmail.com>
> > Cc: squid-users at lists.squid-cache.org
> > Subject: Re: [squid-users] SSL-Bump: NAT/TPROXY lookup failed to locate
> original IPs
> >
> > Hey Michael,
> >
> > The details you attached explained pretty well the cause for the issues
> you have described.
> > What you will need to do in order to make this setup to work can be done
> in more then one way.
> > For a sysadmin the simplest way is to create a VPN or some kind of a
> tunnel between the AWS instance to the local router.
> > I am almost sure that you can use haproxy to do a local tproxy or
> interception that will forward the traffic to the remote squid with the
> PROXY protocol keeping original source and original destination visible to
> the remote squid.
> >
> > The choice will depend on both:
> > - your skills and will to dig some time about couple subjects
> > - The availability of static IP addresses(both local and AWS).
> > - The OS on both sides
> >
> > I believe that the next haproxy settings can be used as a compromise to
> a tunnel:
> > http://ngtech.co.il/paste/1605/
> > And some tproxy route and iptables rules ..
> > With a squid.conf which will be similar to:
> > acl frontend src 100.0.0.1
> > proxy_protocol_access allow frontend
> > http_port 3127
> > http_port 3128 require-proxy-header ... ssl-bump settings
> > ##END of example
> >
> > However I do still believe that the more secure way would be to use some
> kind of vpn tunnel like OpenVPN between the local router to the remote AWS
> instance.
> >
> > All The Bests,
> > Eliezer
> >
> > ----
> > Eliezer Croitoru
> > Linux System Administrator
> > Mobile: +972-5-28704261
> > Email: eliezer at ngtech.co.il
> >
> >
> > -----Original Message-----
> > From: Test User [mailto:tuser6485 at gmail.com]
> > Sent: Sunday, February 26, 2017 8:38 AM
> > To: Eliezer Croitoru <eliezer at ngtech.co.il>
> > Cc: squid-users at lists.squid-cache.org
> > Subject: Re: [squid-users] SSL-Bump: NAT/TPROXY lookup failed to locate
> original IPs
> >
> > On Sun, Feb 26, 2017 at 10:40 AM, Eliezer Croitoru <eliezer at ngtech.co.il>
> wrote:
> >> Hey Michael,
> >>
> >> You will need to clear out couple things for us.
> >> First we will need one of the next ouputs or both:
> >> iptables-save
> >> iptables -L -nv
> >>
> >> And then clear out where is this proxy sittings and the network
> structure.
> >> It's not clear if the squid box is the router or a machine somewhere on
> AWS.
> >> If you wish to pass traffic from a local router to a one on AWS you
> will need to create a tunnel like using OpenVPN or a similar solution and
> to use some routing rules to pass the traffic from the local LAN to AWS
> without removing the original destination address.
> >>
> >> When more details on the setup will be available it will be much
> simpler to understand what is the root for some of the issues you are
> having.
> >>
> >> All The Bests,
> >> Eliezer
> >>
> >> ----
> >> Eliezer Croitoru
> >> Linux System Administrator
> >> Mobile: +972-5-28704261
> >> Email: eliezer at ngtech.co.il
> >>
> >>
> >> -----Original Message-----
> >> From: squid-users [mailto:squid-users-bounces at lists.squid-cache.org]
> On Behalf Of Test User
> >> Sent: Friday, February 24, 2017 8:52 AM
> >> To: squid-users at lists.squid-cache.org
> >> Subject: [squid-users] SSL-Bump: NAT/TPROXY lookup failed to locate
> original IPs
> >>
> >> Hi,
> >> Sorry I am asking this question again. I am trying to setup HTTPS
> >> proxy using ssl-bump. I have followed
> >> steps mentioned in:
> >> http://wiki.squid-cache.org/ConfigExamples/Intercept/SslBumpExplicit
> >>
> >> Following are Squid setup details:
> >>
> >> Squid Cache: Version 3.5.12
> >> Service Name: squid
> >> Ubuntu linux
> >>
> >> configure options:  '--build=x86_64-linux-gnu' '--prefix=/usr'
> >> '--includedir=${prefix}/include' '--mandir=${prefix}/share/man'
> >> '--infodir=${prefix}/share/info' '--sysconfdir=/etc'
> >> '--localstatedir=/var' '--libexecdir=${prefix}/lib/squid3'
> >> '--srcdir=.' '--disable-maintainer-mode'
> >> '--disable-dependency-tracking' '--disable-silent-rules'
> >> 'BUILDCXXFLAGS=-g -O2 -fPIE -fstack-protector-strong -Wformat
> >> -Werror=format-security -Wl,-Bsymbolic-functions -fPIE -pie
> >> -Wl,-z,relro -Wl,-z,now' '--datadir=/usr/share/squid'
> >> '--sysconfdir=/etc/squid' '--libexecdir=/usr/lib/squid'
> >> '--mandir=/usr/share/man' '--enable-inline' '--disable-arch-native'
> >> '--enable-async-io=8' '--enable-storeio=ufs,aufs,diskd,rock'
> >> '--enable-removal-policies=lru,heap' '--enable-delay-pools'
> >> '--enable-cache-digests' '--enable-icap-client'
> >> '--enable-follow-x-forwarded-for'
> >> '--enable-auth-basic=DB,fake,getpwnam,LDAP,NCSA,NIS,PAM,
> POP3,RADIUS,SASL,SMB'
> >> '--enable-auth-digest=file,LDAP'
> >> '--enable-auth-negotiate=kerberos,wrapper'
> >> '--enable-auth-ntlm=fake,smb_lm'
> >> '--enable-external-acl-helpers=file_userip,kerberos_
> ldap_group,LDAP_group,session,SQL_session,unix_group,wbinfo_group'
> >> '--enable-url-rewrite-helpers=fake' '--enable-eui' '--enable-esi'
> >> '--enable-icmp' '--enable-zph-qos' '--enable-ecap' '--with-openssl'
> >> '--enable-ssl-crtd' '--disable-translation'
> >> '--with-swapdir=/var/spool/squid' '--with-logdir=/var/log/squid'
> >> '--with-pidfile=/var/run/squid.pid' '--with-filedescriptors=65536'
> >> '--with-large-files' '--with-default-user=proxy'
> >> '--enable-build-info=Ubuntu linux' '--enable-linux-netfilter'
> >> 'build_alias=x86_64-linux-gnu' 'CFLAGS=-g -O2 -fPIE
> >> -fstack-protector-strong -Wformat -Werror=format-security -Wall'
> >> 'LDFLAGS=-Wl,-Bsymbolic-functions -fPIE -pie -Wl,-z,relro -Wl,-z,now'
> >> 'CPPFLAGS=-Wdate-time -D_FORTIFY_SOURCE=2' 'CXXFLAGS=-g -O2 -fPIE
> >> -fstack-protector-strong -Wformat -Werror=format-security'
> >>
> >>
> >> Following is my squid.conf file:
> >>
> >> acl SSL_ports port 443
> >> acl Safe_ports port 80 # http
> >> acl Safe_ports port 21 # ftp
> >> acl Safe_ports port 443 # https
> >> acl Safe_ports port 70 # gopher
> >> acl Safe_ports port 210 # wais
> >> acl Safe_ports port 1025-65535 # unregistered ports
> >> acl Safe_ports port 280 # http-mgmt
> >> acl Safe_ports port 488 # gss-http
> >> acl Safe_ports port 591 # filemaker
> >> acl Safe_ports port 777 # multiling http
> >> acl CONNECT method CONNECT
> >> acl step1 at_step SslBump1
> >> http_access deny !Safe_ports
> >> http_access deny CONNECT !SSL_ports
> >> http_access allow localhost manager
> >> http_access deny manager
> >> http_access allow localhost
> >> http_access allow all
> >> http_port 3128 ssl-bump \
> >>   cert=/etc/squid/ssl_cert/squidCA.pem \
> >>   generate-host-certificates=on dynamic_cert_mem_cache_size=4MB
> >> https_port 3129 intercept ssl-bump generate-host-certificates=on \
> >> dynamic_cert_mem_cache_size=4MB cert=/etc/squid/ssl_cert/squidCA.pem \
> >> dhparams=/etc/squid/ssl_cert/dhparam.pem
> >> sslproxy_options NO_SSLv2,NO_SSLv3,SINGLE_DH_USE
> >> sslproxy_cipher
> >> EECDH+ECDSA+AESGCM:EECDH+aRSA+AESGCM:EECDH+ECDSA+SHA384:
> EECDH+ECDSA+SHA256:EECDH+aRSA+SHA384:EECDH+aRSA+SHA256:
> EECDH+aRSA+RC4:EECDH:EDH+aRSA:!RC4:!aNULL:!eNULL:!LOW:!3DES:
> !MD5:!EXP:!PSK:!SRP:!DSS
> >> sslcrtd_program /usr/lib/squid/ssl_crtd -s /var/spool/squid3_ssldb -M
> 4MB
> >> debug_options ALL,1 3,5 4,5 11,5 17,5 23,5 46,5 78,5 rotate=1
> >> coredump_dir /var/spool/squid
> >> refresh_pattern ^ftp: 1440 20% 10080
> >> refresh_pattern ^gopher: 1440 0% 1440
> >> refresh_pattern -i (/cgi-bin/|\?) 0 0% 0
> >> refresh_pattern (Release|Packages(.gz)*)$      0       20%     2880
> >> refresh_pattern . 0 20% 4320
> >>
> >>
> >> I get no errors while starting Squid. Following are the logs when Squid
> starts:
> >>
> >> 2017/02/23 09:59:53 kid1| Set Current Directory to /var/spool/squid
> >> 2017/02/23 09:59:53 kid1| Starting Squid Cache version 3.5.12 for
> >> x86_64-pc-linux-gnu...
> >> 2017/02/23 09:59:53 kid1| Service Name: squid
> >> 2017/02/23 09:59:53 kid1| Process ID 26236
> >> 2017/02/23 09:59:53 kid1| Process Roles: worker
> >> 2017/02/23 09:59:53 kid1| With 65535 file descriptors available
> >> 2017/02/23 09:59:53 kid1| Initializing IP Cache...
> >> 2017/02/23 09:59:53.756 kid1| 78,2| dns_internal.cc(1525) dnsInit:
> >> idnsInit: attempt open DNS socket to: [::]
> >> 2017/02/23 09:59:53.756 kid1| 78,2| dns_internal.cc(1534) dnsInit:
> >> idnsInit: attempt open DNS socket to: 0.0.0.0
> >> 2017/02/23 09:59:53.756 kid1| DNS Socket created at [::], FD 6
> >> 2017/02/23 09:59:53.756 kid1| DNS Socket created at 0.0.0.0, FD 7
> >> 2017/02/23 09:59:53.756 kid1| Adding nameserver 172.31.0.2 from
> /etc/resolv.conf
> >> 2017/02/23 09:59:53.756 kid1| 78,3| dns_internal.cc(321)
> >> idnsAddNameserver: idnsAddNameserver: Added nameserver #0
> >> (172.31.0.2:53)
> >> 2017/02/23 09:59:53.756 kid1| Adding domain
> >> ap-south-1.compute.internal from /etc/resolv.conf
> >> 2017/02/23 09:59:53.756 kid1| 78,3| dns_internal.cc(350)
> >> idnsAddPathComponent: idnsAddPathComponent: Added domain #0:
> >> ap-south-1.compute.internal
> >> 2017/02/23 09:59:53.756 kid1| helperOpenServers: Starting 5/32
> >> 'ssl_crtd' processes
> >> 2017/02/23 09:59:53.775 kid1| 46,2| Format.cc(64) parse: got
> >> definition '%>a/%>A %un %>rm myip=%la myport=%lp'
> >> 2017/02/23 09:59:53.775 kid1| 46,5| Token.cc(380) parse: scan for
> >> possible Misc token
> >> 2017/02/23 09:59:53.775 kid1| 46,5| Token.cc(384) parse: scan for
> >> possible 2C token
> >> 2017/02/23 09:59:53.775 kid1| 46,5| Token.cc(389) parse: scan for
> >> possible 1C token
> >> 2017/02/23 09:59:53.775 kid1| 46,5| Token.cc(380) parse: scan for
> >> possible Misc token
> >> 2017/02/23 09:59:53.775 kid1| 46,5| Token.cc(384) parse: scan for
> >> possible 2C token
> >> 2017/02/23 09:59:53.775 kid1| 46,5| Token.cc(389) parse: scan for
> >> possible 1C token
> >> 2017/02/23 09:59:53.775 kid1| 46,5| Token.cc(380) parse: scan for
> >> possible Misc token
> >> 2017/02/23 09:59:53.775 kid1| 46,5| Token.cc(384) parse: scan for
> >> possible 2C token
> >> 2017/02/23 09:59:53.775 kid1| 46,5| Token.cc(380) parse: scan for
> >> possible Misc token
> >> 2017/02/23 09:59:53.775 kid1| 46,5| Token.cc(384) parse: scan for
> >> possible 2C token
> >> 2017/02/23 09:59:53.775 kid1| 46,5| Token.cc(380) parse: scan for
> >> possible Misc token
> >> 2017/02/23 09:59:53.775 kid1| 46,5| Token.cc(384) parse: scan for
> >> possible 2C token
> >> 2017/02/23 09:59:53.775 kid1| 46,5| Token.cc(380) parse: scan for
> >> possible Misc token
> >> 2017/02/23 09:59:53.775 kid1| 46,5| Token.cc(384) parse: scan for
> >> possible 2C token
> >> 2017/02/23 09:59:53.775 kid1| 46,2| Format.cc(64) parse: got
> >> definition '%>a/%>A %un %>rm myip=%la myport=%lp'
> >> 2017/02/23 09:59:53.775 kid1| 46,5| Token.cc(380) parse: scan for
> >> possible Misc token
> >> 2017/02/23 09:59:53.775 kid1| 46,5| Token.cc(384) parse: scan for
> >> possible 2C token
> >> 2017/02/23 09:59:53.775 kid1| 46,5| Token.cc(389) parse: scan for
> >> possible 1C token
> >> 2017/02/23 09:59:53.775 kid1| 46,5| Token.cc(380) parse: scan for
> >> possible Misc token
> >> 2017/02/23 09:59:53.775 kid1| 46,5| Token.cc(384) parse: scan for
> >> possible 2C token
> >> 2017/02/23 09:59:53.775 kid1| 46,5| Token.cc(389) parse: scan for
> >> possible 1C token
> >> 2017/02/23 09:59:53.775 kid1| 46,5| Token.cc(380) parse: scan for
> >> possible Misc token
> >> 2017/02/23 09:59:53.775 kid1| 46,5| Token.cc(384) parse: scan for
> >> possible 2C token
> >> 2017/02/23 09:59:53.775 kid1| 46,5| Token.cc(380) parse: scan for
> >> possible Misc token
> >> 2017/02/23 09:59:53.775 kid1| 46,5| Token.cc(384) parse: scan for
> >> possible 2C token
> >> 2017/02/23 09:59:53.775 kid1| 46,5| Token.cc(380) parse: scan for
> >> possible Misc token
> >> 2017/02/23 09:59:53.775 kid1| 46,5| Token.cc(384) parse: scan for
> >> possible 2C token
> >> 2017/02/23 09:59:53.775 kid1| 46,5| Token.cc(380) parse: scan for
> >> possible Misc token
> >> 2017/02/23 09:59:53.775 kid1| 46,5| Token.cc(384) parse: scan for
> >> possible 2C token
> >> 2017/02/23 09:59:53.775 kid1| Logfile: opening log
> >> daemon:/var/log/squid/access.log
> >> 2017/02/23 09:59:53.775 kid1| Logfile Daemon: opening log
> >> /var/log/squid/access.log
> >> 2017/02/23 09:59:53.779 kid1| 23,5| url.cc(43) urlInitialize:
> >> urlInitialize: Initializing...
> >> 2017/02/23 09:59:53.779 kid1| Local cache digest enabled;
> >> rebuild/rewrite every 3600/3600 sec
> >> 2017/02/23 09:59:53.779 kid1| Store logging disabled
> >> 2017/02/23 09:59:53.779 kid1| Swap maxSize 0 + 262144 KB, estimated
> >> 20164 objects
> >> 2017/02/23 09:59:53.779 kid1| Target number of buckets: 1008
> >> 2017/02/23 09:59:53.779 kid1| Using 8192 Store buckets
> >> 2017/02/23 09:59:53.779 kid1| Max Mem  size: 262144 KB
> >> 2017/02/23 09:59:53.779 kid1| Max Swap size: 0 KB
> >> 2017/02/23 09:59:53.779 kid1| Using Least Load store dir selection
> >> 2017/02/23 09:59:53.779 kid1| Set Current Directory to /var/spool/squid
> >> 2017/02/23 09:59:53.785 kid1| 23,3| url.cc(357) urlParse: urlParse:
> >> Split URL 'http://ip-172-31-25-235:3128/squid-internal-static/icons/
> silk/image.png'
> >> into proto='http', host='ip-172-31-25-235', port='3128',
> >> path='/squid-internal-static/icons/silk/image.png'
> >> 2017/02/23 09:59:53.785 kid1| 23,3| url.cc(357) urlParse: urlParse:
> >> Split URL 'http://ip-172-31-25-235:3128/squid-internal-static/icons/
> silk/page_white_text.png'
> >> into proto='http', host='ip-172-31-25-235', port='3128',
> >> path='/squid-internal-static/icons/silk/page_white_text.png'
> >>
> >> ****several urlParse logs like above. Removing them to shorten the
> >> email. Further logs below...****
> >>
> >> 2017/02/23 09:59:53.815 kid1| Finished loading MIME types and icons.
> >> 2017/02/23 09:59:53.815 kid1| HTCP Disabled.
> >> 2017/02/23 09:59:53.815 kid1| Pinger socket opened on FD 25
> >> 2017/02/23 09:59:53.815 kid1| Squid plugin modules loaded: 0
> >> 2017/02/23 09:59:53.815 kid1| Adaptation support is off.
> >> 2017/02/23 09:59:53.815 kid1| Accepting SSL bumped HTTP Socket
> >> connections at local=[::]:3128 remote=[::] FD 22 flags=9
> >> 2017/02/23 09:59:53.815 kid1| Accepting NAT intercepted SSL bumped
> >> HTTPS Socket connections at local=[::]:3129 remote=[::] FD 23 flags=41
> >> 2017/02/23 09:59:53| pinger: Initialising ICMP pinger ...
> >> 2017/02/23 09:59:53| pinger: ICMP socket opened.
> >> 2017/02/23 09:59:53| pinger: ICMPv6 socket opened
> >> 2017/02/23 09:59:54 kid1| storeLateRelease: released 0 objects
> >>
> >>
> >>
> >> I tested this setup by providing proxy details to Firefox. Firefox was
> >> able to show HTTP websites but when I tried to open an HTTPS website I
> >> got following error:
> >>
> >> 2017/02/23 11:00:50 kid1| ERROR: NF getsockopt(ORIGINAL_DST) failed on
> >> local=172.31.25.235:3129 remote=182.72.78.122:50655 FD 7 flags=33:
> >> (92) Protocol not available
> >> 2017/02/23 11:00:50 kid1| ERROR: NAT/TPROXY lookup failed to locate
> >> original IPs on local=172.31.25.235:3129 remote=182.72.78.122:50655 FD
> >> 7 flags=33
> >> 2017/02/23 11:00:50 kid1| ERROR: NF getsockopt(ORIGINAL_DST) failed on
> >> local=172.31.25.235:3129 remote=182.72.78.122:50656 FD 7 flags=33:
> >> (92) Protocol not available
> >> 2017/02/23 11:00:50 kid1| ERROR: NAT/TPROXY lookup failed to locate
> >> original IPs on local=172.31.25.235:3129 remote=182.72.78.122:50656 FD
> >> 7 flags=33
> >> 2017/02/23 11:00:50 kid1| ERROR: NF getsockopt(ORIGINAL_DST) failed on
> >> local=172.31.25.235:3129 remote=182.72.78.122:50657 FD 7 flags=33:
> >> (92) Protocol not available
> >> 2017/02/23 11:00:50 kid1| ERROR: NAT/TPROXY lookup failed to locate
> >> original IPs on local=172.31.25.235:3129 remote=182.72.78.122:50657 FD
> >> 7 flags=33
> >>
> >> I googled this error and found this mail thread which had similar
> problems:
> >> http://squid-web-proxy-cache.1019090.n4.nabble.com/NAT-
> TPROXY-lookup-failed-to-locate-original-IPs-td4675464.html
> >>
> >> I found this link from the above thread. I modified the steps for
> >> HTTPS from the below link:
> >> http://wiki.squid-cache.org/ConfigExamples/Intercept/LinuxDnat
> >>
> >> Now my sysctl.conf is:
> >>
> >> net.ipv4.conf.all.rp_filter=0
> >> net.ipv4.ip_forward = 1
> >> net.ipv4.conf.default.rp_filter = 0
> >> net.ipv4.conf.default.accept_source_route = 0
> >>
> >> My iptables -t nat -L result:
> >>
> >> Chain PREROUTING (policy ACCEPT)
> >> target     prot opt source               destination
> >> ACCEPT     tcp  --  ec2-35-154-101-8.ap-south-1.compute.amazonaws.com
> >> anywhere             tcp dpt:https
> >> DNAT       tcp  --  anywhere             anywhere             tcp
> >> dpt:https to:35.154.101.8:3129
> >>
> >> Chain INPUT (policy ACCEPT)
> >> target     prot opt source               destination
> >>
> >> Chain OUTPUT (policy ACCEPT)
> >> target     prot opt source               destination
> >>
> >> Chain POSTROUTING (policy ACCEPT)
> >> target     prot opt source               destination
> >> MASQUERADE  all  --  anywhere             anywhere
> >>
> >>
> >> Once this was done, I tried to hit HTTPS website from Firefox and now
> >> I get connection timeout error. Nothing shows in syslog, access.log or
> >> cache.log. Could you please help me resolve this.
> >>
> >> Thanks,
> >> Michael
> >> _______________________________________________
> >> squid-users mailing list
> >> squid-users at lists.squid-cache.org
> >> http://lists.squid-cache.org/listinfo/squid-users
> >>
> >
> >
> > Thanks for replying Eliezer. Following are the outputs you asked:
> >
> > 1. iptables-save:
> >
> > # Generated by iptables-save v1.6.0 on Sun Feb 26 06:28:46 2017
> > *filter
> > :INPUT ACCEPT [171:12090]
> > :FORWARD ACCEPT [0:0]
> > :OUTPUT ACCEPT [106:15187]
> > COMMIT
> > # Completed on Sun Feb 26 06:28:46 2017
> > # Generated by iptables-save v1.6.0 on Sun Feb 26 06:28:46 2017
> > *mangle
> > :PREROUTING ACCEPT [89003:74850371]
> > :INPUT ACCEPT [88973:74849159]
> > :FORWARD ACCEPT [30:1212]
> > :OUTPUT ACCEPT [76710:51478183]
> > :POSTROUTING ACCEPT [76740:51479395]
> > -A PREROUTING -p tcp -m tcp --dport 3129 -j DROP
> > COMMIT
> > # Completed on Sun Feb 26 06:28:46 2017
> > # Generated by iptables-save v1.6.0 on Sun Feb 26 06:28:46 2017
> > *nat
> > :PREROUTING ACCEPT [7766:436942]
> > :INPUT ACCEPT [7766:436942]
> > :OUTPUT ACCEPT [952:102330]
> > :POSTROUTING ACCEPT [0:0]
> > -A PREROUTING -s 35.154.101.8/32 -p tcp -m tcp --dport 443 -j ACCEPT
> > -A PREROUTING -p tcp -m tcp --dport 443 -j DNAT --to-destination
> > 35.154.101.8:3129
> > -A POSTROUTING -j MASQUERADE
> > COMMIT
> > # Completed on Sun Feb 26 06:28:46 2017
> >
> > 2. Also pasting sudo iptables -L -nv:
> >
> > Chain INPUT (policy ACCEPT 216 packets, 16058 bytes)
> >  pkts bytes target     prot opt in     out     source
> > destination
> >
> > Chain FORWARD (policy ACCEPT 0 packets, 0 bytes)
> >  pkts bytes target     prot opt in     out     source
> > destination
> >
> > Chain OUTPUT (policy ACCEPT 161 packets, 24629 bytes)
> >  pkts bytes target     prot opt in     out     source
>  destination
> >
> >
> >
> >> And then clear out where is this proxy sittings and the network
> structure.
> >> It's not clear if the squid box is the router or a machine somewhere on
> AWS.
> >
> > [Michael] This proxy is installed on an AWS instance.
> >
> >> If you wish to pass traffic from a local router to a one on AWS you
> will need to create a tunnel like using OpenVPN or a similar solution and
> to use some routing rules to pass the traffic from the local LAN to AWS
> without removing the original destination address.
> >>
> >
> > [Michael] Does this mean, to make ssl-bump work, I will have to setup
> > a VPN server and configure the VPN clients to use this proxy via VPN
> > server?
> >
> >
> > Thanks,
> > Michael.
> >
> > _______________________________________________
> > squid-users mailing list
> > squid-users at lists.squid-cache.org
> > http://lists.squid-cache.org/listinfo/squid-users
> >
>
>
>
> Thanks for replying Eliezer. Your advice is much appreciated.
>
> > The details you attached explained pretty well the cause for the issues
> you have described.
> > What you will need to do in order to make this setup to work can be done
> in more then one way.
> > For a sysadmin the simplest way is to create a VPN or some kind of a
> tunnel between the AWS instance to the local router.
> > I am almost sure that you can use haproxy to do a local tproxy or
> interception that will forward the traffic to the remote squid with the
> PROXY protocol keeping original source and original destination visible to
> the remote squid.
> >
> > The choice will depend on both:
> > - your skills and will to dig some time about couple subjects
> > - The availability of static IP addresses(both local and AWS).
> > - The OS on both sides
>
> [Michael] Actually, my original setup involves a VPN server. I wasn't
> using it because I wanted to setup ssl-bump with simplest possible
> settings. My actual setup involves:
>
> 1. strongSwan IPSec VPN server
> 2. Squid Proxy server
> 3. Clients will be IPSec VPN clients. I can specify the IP address and
> port of HTTPS Proxy server in IPSec VPN client itself.
>
> In the above setup described, will I have to do something extra to
> make ssl-bump work?
>
> Thanks,
> Michael.
>


What is the benefit of ssl-bump in this scenario?


-- 
Best regards,
Odhiambo WASHINGTON,
Nairobi,KE
+254 7 3200 0004/+254 7 2274 3223
"Oh, the cruft."
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20170227/79c1b669/attachment.htm>

From tuser6485 at gmail.com  Mon Feb 27 05:50:32 2017
From: tuser6485 at gmail.com (Test User)
Date: Mon, 27 Feb 2017 11:20:32 +0530
Subject: [squid-users] SSL-Bump: NAT/TPROXY lookup failed to locate
 original IPs
In-Reply-To: <CAAdA2WPMEYr9ups=8L_=s-j-JT8R741UN4Sm-N7jWqS_Xac+tg@mail.gmail.com>
References: <CAPhcQuJU-ctRMokvU6kg+RqkfXDOU_0y+o+fEUADdoX4AstXaQ@mail.gmail.com>
 <544701d28fee$a1a6b050$e4f410f0$@ngtech.co.il>
 <CAPhcQuJEFmVPQ=C9=KAzuKzZjqKoQo=hN7wvR9UX2aPM98LqEw@mail.gmail.com>
 <7a2301d29061$468aba90$d3a02fb0$@ngtech.co.il>
 <7a2501d29076$9dd0c050$d97240f0$@ngtech.co.il>
 <CAPhcQuJCeKZYSwmefsAxsFx7kHCWCkOg6kkmgH-WE4Sz2=sXAA@mail.gmail.com>
 <CAAdA2WPMEYr9ups=8L_=s-j-JT8R741UN4Sm-N7jWqS_Xac+tg@mail.gmail.com>
Message-ID: <CAPhcQuLPzV=PEMdn0_1_+WuZXSeS2v7VSk7thQ=szcLHcGnZKA@mail.gmail.com>

On Mon, Feb 27, 2017 at 11:14 AM, Odhiambo Washington
<odhiambo at gmail.com> wrote:
>
>
> On 27 February 2017 at 08:41, Test User <tuser6485 at gmail.com> wrote:
>>
>> On Mon, Feb 27, 2017 at 2:53 AM, Eliezer Croitoru <eliezer at ngtech.co.il>
>> wrote:
>> > Let me know if you need some help..
>> >
>> > Eliezer
>> >
>> > ----
>> > Eliezer Croitoru
>> > Linux System Administrator
>> > Mobile: +972-5-28704261
>> > Email: eliezer at ngtech.co.il
>> >
>> >
>> > -----Original Message-----
>> > From: squid-users [mailto:squid-users-bounces at lists.squid-cache.org] On
>> > Behalf Of Eliezer Croitoru
>> > Sent: Sunday, February 26, 2017 8:51 PM
>> > To: 'Test User' <tuser6485 at gmail.com>
>> > Cc: squid-users at lists.squid-cache.org
>> > Subject: Re: [squid-users] SSL-Bump: NAT/TPROXY lookup failed to locate
>> > original IPs
>> >
>> > Hey Michael,
>> >
>> > The details you attached explained pretty well the cause for the issues
>> > you have described.
>> > What you will need to do in order to make this setup to work can be done
>> > in more then one way.
>> > For a sysadmin the simplest way is to create a VPN or some kind of a
>> > tunnel between the AWS instance to the local router.
>> > I am almost sure that you can use haproxy to do a local tproxy or
>> > interception that will forward the traffic to the remote squid with the
>> > PROXY protocol keeping original source and original destination visible to
>> > the remote squid.
>> >
>> > The choice will depend on both:
>> > - your skills and will to dig some time about couple subjects
>> > - The availability of static IP addresses(both local and AWS).
>> > - The OS on both sides
>> >
>> > I believe that the next haproxy settings can be used as a compromise to
>> > a tunnel:
>> > http://ngtech.co.il/paste/1605/
>> > And some tproxy route and iptables rules ..
>> > With a squid.conf which will be similar to:
>> > acl frontend src 100.0.0.1
>> > proxy_protocol_access allow frontend
>> > http_port 3127
>> > http_port 3128 require-proxy-header ... ssl-bump settings
>> > ##END of example
>> >
>> > However I do still believe that the more secure way would be to use some
>> > kind of vpn tunnel like OpenVPN between the local router to the remote AWS
>> > instance.
>> >
>> > All The Bests,
>> > Eliezer
>> >
>> > ----
>> > Eliezer Croitoru
>> > Linux System Administrator
>> > Mobile: +972-5-28704261
>> > Email: eliezer at ngtech.co.il
>> >
>> >
>> > -----Original Message-----
>> > From: Test User [mailto:tuser6485 at gmail.com]
>> > Sent: Sunday, February 26, 2017 8:38 AM
>> > To: Eliezer Croitoru <eliezer at ngtech.co.il>
>> > Cc: squid-users at lists.squid-cache.org
>> > Subject: Re: [squid-users] SSL-Bump: NAT/TPROXY lookup failed to locate
>> > original IPs
>> >
>> > On Sun, Feb 26, 2017 at 10:40 AM, Eliezer Croitoru
>> > <eliezer at ngtech.co.il> wrote:
>> >> Hey Michael,
>> >>
>> >> You will need to clear out couple things for us.
>> >> First we will need one of the next ouputs or both:
>> >> iptables-save
>> >> iptables -L -nv
>> >>
>> >> And then clear out where is this proxy sittings and the network
>> >> structure.
>> >> It's not clear if the squid box is the router or a machine somewhere on
>> >> AWS.
>> >> If you wish to pass traffic from a local router to a one on AWS you
>> >> will need to create a tunnel like using OpenVPN or a similar solution and to
>> >> use some routing rules to pass the traffic from the local LAN to AWS without
>> >> removing the original destination address.
>> >>
>> >> When more details on the setup will be available it will be much
>> >> simpler to understand what is the root for some of the issues you are
>> >> having.
>> >>
>> >> All The Bests,
>> >> Eliezer
>> >>
>> >> ----
>> >> Eliezer Croitoru
>> >> Linux System Administrator
>> >> Mobile: +972-5-28704261
>> >> Email: eliezer at ngtech.co.il
>> >>
>> >>
>> >> -----Original Message-----
>> >> From: squid-users [mailto:squid-users-bounces at lists.squid-cache.org] On
>> >> Behalf Of Test User
>> >> Sent: Friday, February 24, 2017 8:52 AM
>> >> To: squid-users at lists.squid-cache.org
>> >> Subject: [squid-users] SSL-Bump: NAT/TPROXY lookup failed to locate
>> >> original IPs
>> >>
>> >> Hi,
>> >> Sorry I am asking this question again. I am trying to setup HTTPS
>> >> proxy using ssl-bump. I have followed
>> >> steps mentioned in:
>> >> http://wiki.squid-cache.org/ConfigExamples/Intercept/SslBumpExplicit
>> >>
>> >> Following are Squid setup details:
>> >>
>> >> Squid Cache: Version 3.5.12
>> >> Service Name: squid
>> >> Ubuntu linux
>> >>
>> >> configure options:  '--build=x86_64-linux-gnu' '--prefix=/usr'
>> >> '--includedir=${prefix}/include' '--mandir=${prefix}/share/man'
>> >> '--infodir=${prefix}/share/info' '--sysconfdir=/etc'
>> >> '--localstatedir=/var' '--libexecdir=${prefix}/lib/squid3'
>> >> '--srcdir=.' '--disable-maintainer-mode'
>> >> '--disable-dependency-tracking' '--disable-silent-rules'
>> >> 'BUILDCXXFLAGS=-g -O2 -fPIE -fstack-protector-strong -Wformat
>> >> -Werror=format-security -Wl,-Bsymbolic-functions -fPIE -pie
>> >> -Wl,-z,relro -Wl,-z,now' '--datadir=/usr/share/squid'
>> >> '--sysconfdir=/etc/squid' '--libexecdir=/usr/lib/squid'
>> >> '--mandir=/usr/share/man' '--enable-inline' '--disable-arch-native'
>> >> '--enable-async-io=8' '--enable-storeio=ufs,aufs,diskd,rock'
>> >> '--enable-removal-policies=lru,heap' '--enable-delay-pools'
>> >> '--enable-cache-digests' '--enable-icap-client'
>> >> '--enable-follow-x-forwarded-for'
>> >>
>> >> '--enable-auth-basic=DB,fake,getpwnam,LDAP,NCSA,NIS,PAM,POP3,RADIUS,SASL,SMB'
>> >> '--enable-auth-digest=file,LDAP'
>> >> '--enable-auth-negotiate=kerberos,wrapper'
>> >> '--enable-auth-ntlm=fake,smb_lm'
>> >>
>> >> '--enable-external-acl-helpers=file_userip,kerberos_ldap_group,LDAP_group,session,SQL_session,unix_group,wbinfo_group'
>> >> '--enable-url-rewrite-helpers=fake' '--enable-eui' '--enable-esi'
>> >> '--enable-icmp' '--enable-zph-qos' '--enable-ecap' '--with-openssl'
>> >> '--enable-ssl-crtd' '--disable-translation'
>> >> '--with-swapdir=/var/spool/squid' '--with-logdir=/var/log/squid'
>> >> '--with-pidfile=/var/run/squid.pid' '--with-filedescriptors=65536'
>> >> '--with-large-files' '--with-default-user=proxy'
>> >> '--enable-build-info=Ubuntu linux' '--enable-linux-netfilter'
>> >> 'build_alias=x86_64-linux-gnu' 'CFLAGS=-g -O2 -fPIE
>> >> -fstack-protector-strong -Wformat -Werror=format-security -Wall'
>> >> 'LDFLAGS=-Wl,-Bsymbolic-functions -fPIE -pie -Wl,-z,relro -Wl,-z,now'
>> >> 'CPPFLAGS=-Wdate-time -D_FORTIFY_SOURCE=2' 'CXXFLAGS=-g -O2 -fPIE
>> >> -fstack-protector-strong -Wformat -Werror=format-security'
>> >>
>> >>
>> >> Following is my squid.conf file:
>> >>
>> >> acl SSL_ports port 443
>> >> acl Safe_ports port 80 # http
>> >> acl Safe_ports port 21 # ftp
>> >> acl Safe_ports port 443 # https
>> >> acl Safe_ports port 70 # gopher
>> >> acl Safe_ports port 210 # wais
>> >> acl Safe_ports port 1025-65535 # unregistered ports
>> >> acl Safe_ports port 280 # http-mgmt
>> >> acl Safe_ports port 488 # gss-http
>> >> acl Safe_ports port 591 # filemaker
>> >> acl Safe_ports port 777 # multiling http
>> >> acl CONNECT method CONNECT
>> >> acl step1 at_step SslBump1
>> >> http_access deny !Safe_ports
>> >> http_access deny CONNECT !SSL_ports
>> >> http_access allow localhost manager
>> >> http_access deny manager
>> >> http_access allow localhost
>> >> http_access allow all
>> >> http_port 3128 ssl-bump \
>> >>   cert=/etc/squid/ssl_cert/squidCA.pem \
>> >>   generate-host-certificates=on dynamic_cert_mem_cache_size=4MB
>> >> https_port 3129 intercept ssl-bump generate-host-certificates=on \
>> >> dynamic_cert_mem_cache_size=4MB cert=/etc/squid/ssl_cert/squidCA.pem \
>> >> dhparams=/etc/squid/ssl_cert/dhparam.pem
>> >> sslproxy_options NO_SSLv2,NO_SSLv3,SINGLE_DH_USE
>> >> sslproxy_cipher
>> >>
>> >> EECDH+ECDSA+AESGCM:EECDH+aRSA+AESGCM:EECDH+ECDSA+SHA384:EECDH+ECDSA+SHA256:EECDH+aRSA+SHA384:EECDH+aRSA+SHA256:EECDH+aRSA+RC4:EECDH:EDH+aRSA:!RC4:!aNULL:!eNULL:!LOW:!3DES:!MD5:!EXP:!PSK:!SRP:!DSS
>> >> sslcrtd_program /usr/lib/squid/ssl_crtd -s /var/spool/squid3_ssldb -M
>> >> 4MB
>> >> debug_options ALL,1 3,5 4,5 11,5 17,5 23,5 46,5 78,5 rotate=1
>> >> coredump_dir /var/spool/squid
>> >> refresh_pattern ^ftp: 1440 20% 10080
>> >> refresh_pattern ^gopher: 1440 0% 1440
>> >> refresh_pattern -i (/cgi-bin/|\?) 0 0% 0
>> >> refresh_pattern (Release|Packages(.gz)*)$      0       20%     2880
>> >> refresh_pattern . 0 20% 4320
>> >>
>> >>
>> >> I get no errors while starting Squid. Following are the logs when Squid
>> >> starts:
>> >>
>> >> 2017/02/23 09:59:53 kid1| Set Current Directory to /var/spool/squid
>> >> 2017/02/23 09:59:53 kid1| Starting Squid Cache version 3.5.12 for
>> >> x86_64-pc-linux-gnu...
>> >> 2017/02/23 09:59:53 kid1| Service Name: squid
>> >> 2017/02/23 09:59:53 kid1| Process ID 26236
>> >> 2017/02/23 09:59:53 kid1| Process Roles: worker
>> >> 2017/02/23 09:59:53 kid1| With 65535 file descriptors available
>> >> 2017/02/23 09:59:53 kid1| Initializing IP Cache...
>> >> 2017/02/23 09:59:53.756 kid1| 78,2| dns_internal.cc(1525) dnsInit:
>> >> idnsInit: attempt open DNS socket to: [::]
>> >> 2017/02/23 09:59:53.756 kid1| 78,2| dns_internal.cc(1534) dnsInit:
>> >> idnsInit: attempt open DNS socket to: 0.0.0.0
>> >> 2017/02/23 09:59:53.756 kid1| DNS Socket created at [::], FD 6
>> >> 2017/02/23 09:59:53.756 kid1| DNS Socket created at 0.0.0.0, FD 7
>> >> 2017/02/23 09:59:53.756 kid1| Adding nameserver 172.31.0.2 from
>> >> /etc/resolv.conf
>> >> 2017/02/23 09:59:53.756 kid1| 78,3| dns_internal.cc(321)
>> >> idnsAddNameserver: idnsAddNameserver: Added nameserver #0
>> >> (172.31.0.2:53)
>> >> 2017/02/23 09:59:53.756 kid1| Adding domain
>> >> ap-south-1.compute.internal from /etc/resolv.conf
>> >> 2017/02/23 09:59:53.756 kid1| 78,3| dns_internal.cc(350)
>> >> idnsAddPathComponent: idnsAddPathComponent: Added domain #0:
>> >> ap-south-1.compute.internal
>> >> 2017/02/23 09:59:53.756 kid1| helperOpenServers: Starting 5/32
>> >> 'ssl_crtd' processes
>> >> 2017/02/23 09:59:53.775 kid1| 46,2| Format.cc(64) parse: got
>> >> definition '%>a/%>A %un %>rm myip=%la myport=%lp'
>> >> 2017/02/23 09:59:53.775 kid1| 46,5| Token.cc(380) parse: scan for
>> >> possible Misc token
>> >> 2017/02/23 09:59:53.775 kid1| 46,5| Token.cc(384) parse: scan for
>> >> possible 2C token
>> >> 2017/02/23 09:59:53.775 kid1| 46,5| Token.cc(389) parse: scan for
>> >> possible 1C token
>> >> 2017/02/23 09:59:53.775 kid1| 46,5| Token.cc(380) parse: scan for
>> >> possible Misc token
>> >> 2017/02/23 09:59:53.775 kid1| 46,5| Token.cc(384) parse: scan for
>> >> possible 2C token
>> >> 2017/02/23 09:59:53.775 kid1| 46,5| Token.cc(389) parse: scan for
>> >> possible 1C token
>> >> 2017/02/23 09:59:53.775 kid1| 46,5| Token.cc(380) parse: scan for
>> >> possible Misc token
>> >> 2017/02/23 09:59:53.775 kid1| 46,5| Token.cc(384) parse: scan for
>> >> possible 2C token
>> >> 2017/02/23 09:59:53.775 kid1| 46,5| Token.cc(380) parse: scan for
>> >> possible Misc token
>> >> 2017/02/23 09:59:53.775 kid1| 46,5| Token.cc(384) parse: scan for
>> >> possible 2C token
>> >> 2017/02/23 09:59:53.775 kid1| 46,5| Token.cc(380) parse: scan for
>> >> possible Misc token
>> >> 2017/02/23 09:59:53.775 kid1| 46,5| Token.cc(384) parse: scan for
>> >> possible 2C token
>> >> 2017/02/23 09:59:53.775 kid1| 46,5| Token.cc(380) parse: scan for
>> >> possible Misc token
>> >> 2017/02/23 09:59:53.775 kid1| 46,5| Token.cc(384) parse: scan for
>> >> possible 2C token
>> >> 2017/02/23 09:59:53.775 kid1| 46,2| Format.cc(64) parse: got
>> >> definition '%>a/%>A %un %>rm myip=%la myport=%lp'
>> >> 2017/02/23 09:59:53.775 kid1| 46,5| Token.cc(380) parse: scan for
>> >> possible Misc token
>> >> 2017/02/23 09:59:53.775 kid1| 46,5| Token.cc(384) parse: scan for
>> >> possible 2C token
>> >> 2017/02/23 09:59:53.775 kid1| 46,5| Token.cc(389) parse: scan for
>> >> possible 1C token
>> >> 2017/02/23 09:59:53.775 kid1| 46,5| Token.cc(380) parse: scan for
>> >> possible Misc token
>> >> 2017/02/23 09:59:53.775 kid1| 46,5| Token.cc(384) parse: scan for
>> >> possible 2C token
>> >> 2017/02/23 09:59:53.775 kid1| 46,5| Token.cc(389) parse: scan for
>> >> possible 1C token
>> >> 2017/02/23 09:59:53.775 kid1| 46,5| Token.cc(380) parse: scan for
>> >> possible Misc token
>> >> 2017/02/23 09:59:53.775 kid1| 46,5| Token.cc(384) parse: scan for
>> >> possible 2C token
>> >> 2017/02/23 09:59:53.775 kid1| 46,5| Token.cc(380) parse: scan for
>> >> possible Misc token
>> >> 2017/02/23 09:59:53.775 kid1| 46,5| Token.cc(384) parse: scan for
>> >> possible 2C token
>> >> 2017/02/23 09:59:53.775 kid1| 46,5| Token.cc(380) parse: scan for
>> >> possible Misc token
>> >> 2017/02/23 09:59:53.775 kid1| 46,5| Token.cc(384) parse: scan for
>> >> possible 2C token
>> >> 2017/02/23 09:59:53.775 kid1| 46,5| Token.cc(380) parse: scan for
>> >> possible Misc token
>> >> 2017/02/23 09:59:53.775 kid1| 46,5| Token.cc(384) parse: scan for
>> >> possible 2C token
>> >> 2017/02/23 09:59:53.775 kid1| Logfile: opening log
>> >> daemon:/var/log/squid/access.log
>> >> 2017/02/23 09:59:53.775 kid1| Logfile Daemon: opening log
>> >> /var/log/squid/access.log
>> >> 2017/02/23 09:59:53.779 kid1| 23,5| url.cc(43) urlInitialize:
>> >> urlInitialize: Initializing...
>> >> 2017/02/23 09:59:53.779 kid1| Local cache digest enabled;
>> >> rebuild/rewrite every 3600/3600 sec
>> >> 2017/02/23 09:59:53.779 kid1| Store logging disabled
>> >> 2017/02/23 09:59:53.779 kid1| Swap maxSize 0 + 262144 KB, estimated
>> >> 20164 objects
>> >> 2017/02/23 09:59:53.779 kid1| Target number of buckets: 1008
>> >> 2017/02/23 09:59:53.779 kid1| Using 8192 Store buckets
>> >> 2017/02/23 09:59:53.779 kid1| Max Mem  size: 262144 KB
>> >> 2017/02/23 09:59:53.779 kid1| Max Swap size: 0 KB
>> >> 2017/02/23 09:59:53.779 kid1| Using Least Load store dir selection
>> >> 2017/02/23 09:59:53.779 kid1| Set Current Directory to /var/spool/squid
>> >> 2017/02/23 09:59:53.785 kid1| 23,3| url.cc(357) urlParse: urlParse:
>> >> Split URL
>> >> 'http://ip-172-31-25-235:3128/squid-internal-static/icons/silk/image.png'
>> >> into proto='http', host='ip-172-31-25-235', port='3128',
>> >> path='/squid-internal-static/icons/silk/image.png'
>> >> 2017/02/23 09:59:53.785 kid1| 23,3| url.cc(357) urlParse: urlParse:
>> >> Split URL
>> >> 'http://ip-172-31-25-235:3128/squid-internal-static/icons/silk/page_white_text.png'
>> >> into proto='http', host='ip-172-31-25-235', port='3128',
>> >> path='/squid-internal-static/icons/silk/page_white_text.png'
>> >>
>> >> ****several urlParse logs like above. Removing them to shorten the
>> >> email. Further logs below...****
>> >>
>> >> 2017/02/23 09:59:53.815 kid1| Finished loading MIME types and icons.
>> >> 2017/02/23 09:59:53.815 kid1| HTCP Disabled.
>> >> 2017/02/23 09:59:53.815 kid1| Pinger socket opened on FD 25
>> >> 2017/02/23 09:59:53.815 kid1| Squid plugin modules loaded: 0
>> >> 2017/02/23 09:59:53.815 kid1| Adaptation support is off.
>> >> 2017/02/23 09:59:53.815 kid1| Accepting SSL bumped HTTP Socket
>> >> connections at local=[::]:3128 remote=[::] FD 22 flags=9
>> >> 2017/02/23 09:59:53.815 kid1| Accepting NAT intercepted SSL bumped
>> >> HTTPS Socket connections at local=[::]:3129 remote=[::] FD 23 flags=41
>> >> 2017/02/23 09:59:53| pinger: Initialising ICMP pinger ...
>> >> 2017/02/23 09:59:53| pinger: ICMP socket opened.
>> >> 2017/02/23 09:59:53| pinger: ICMPv6 socket opened
>> >> 2017/02/23 09:59:54 kid1| storeLateRelease: released 0 objects
>> >>
>> >>
>> >>
>> >> I tested this setup by providing proxy details to Firefox. Firefox was
>> >> able to show HTTP websites but when I tried to open an HTTPS website I
>> >> got following error:
>> >>
>> >> 2017/02/23 11:00:50 kid1| ERROR: NF getsockopt(ORIGINAL_DST) failed on
>> >> local=172.31.25.235:3129 remote=182.72.78.122:50655 FD 7 flags=33:
>> >> (92) Protocol not available
>> >> 2017/02/23 11:00:50 kid1| ERROR: NAT/TPROXY lookup failed to locate
>> >> original IPs on local=172.31.25.235:3129 remote=182.72.78.122:50655 FD
>> >> 7 flags=33
>> >> 2017/02/23 11:00:50 kid1| ERROR: NF getsockopt(ORIGINAL_DST) failed on
>> >> local=172.31.25.235:3129 remote=182.72.78.122:50656 FD 7 flags=33:
>> >> (92) Protocol not available
>> >> 2017/02/23 11:00:50 kid1| ERROR: NAT/TPROXY lookup failed to locate
>> >> original IPs on local=172.31.25.235:3129 remote=182.72.78.122:50656 FD
>> >> 7 flags=33
>> >> 2017/02/23 11:00:50 kid1| ERROR: NF getsockopt(ORIGINAL_DST) failed on
>> >> local=172.31.25.235:3129 remote=182.72.78.122:50657 FD 7 flags=33:
>> >> (92) Protocol not available
>> >> 2017/02/23 11:00:50 kid1| ERROR: NAT/TPROXY lookup failed to locate
>> >> original IPs on local=172.31.25.235:3129 remote=182.72.78.122:50657 FD
>> >> 7 flags=33
>> >>
>> >> I googled this error and found this mail thread which had similar
>> >> problems:
>> >>
>> >> http://squid-web-proxy-cache.1019090.n4.nabble.com/NAT-TPROXY-lookup-failed-to-locate-original-IPs-td4675464.html
>> >>
>> >> I found this link from the above thread. I modified the steps for
>> >> HTTPS from the below link:
>> >> http://wiki.squid-cache.org/ConfigExamples/Intercept/LinuxDnat
>> >>
>> >> Now my sysctl.conf is:
>> >>
>> >> net.ipv4.conf.all.rp_filter=0
>> >> net.ipv4.ip_forward = 1
>> >> net.ipv4.conf.default.rp_filter = 0
>> >> net.ipv4.conf.default.accept_source_route = 0
>> >>
>> >> My iptables -t nat -L result:
>> >>
>> >> Chain PREROUTING (policy ACCEPT)
>> >> target     prot opt source               destination
>> >> ACCEPT     tcp  --  ec2-35-154-101-8.ap-south-1.compute.amazonaws.com
>> >> anywhere             tcp dpt:https
>> >> DNAT       tcp  --  anywhere             anywhere             tcp
>> >> dpt:https to:35.154.101.8:3129
>> >>
>> >> Chain INPUT (policy ACCEPT)
>> >> target     prot opt source               destination
>> >>
>> >> Chain OUTPUT (policy ACCEPT)
>> >> target     prot opt source               destination
>> >>
>> >> Chain POSTROUTING (policy ACCEPT)
>> >> target     prot opt source               destination
>> >> MASQUERADE  all  --  anywhere             anywhere
>> >>
>> >>
>> >> Once this was done, I tried to hit HTTPS website from Firefox and now
>> >> I get connection timeout error. Nothing shows in syslog, access.log or
>> >> cache.log. Could you please help me resolve this.
>> >>
>> >> Thanks,
>> >> Michael
>> >> _______________________________________________
>> >> squid-users mailing list
>> >> squid-users at lists.squid-cache.org
>> >> http://lists.squid-cache.org/listinfo/squid-users
>> >>
>> >
>> >
>> > Thanks for replying Eliezer. Following are the outputs you asked:
>> >
>> > 1. iptables-save:
>> >
>> > # Generated by iptables-save v1.6.0 on Sun Feb 26 06:28:46 2017
>> > *filter
>> > :INPUT ACCEPT [171:12090]
>> > :FORWARD ACCEPT [0:0]
>> > :OUTPUT ACCEPT [106:15187]
>> > COMMIT
>> > # Completed on Sun Feb 26 06:28:46 2017
>> > # Generated by iptables-save v1.6.0 on Sun Feb 26 06:28:46 2017
>> > *mangle
>> > :PREROUTING ACCEPT [89003:74850371]
>> > :INPUT ACCEPT [88973:74849159]
>> > :FORWARD ACCEPT [30:1212]
>> > :OUTPUT ACCEPT [76710:51478183]
>> > :POSTROUTING ACCEPT [76740:51479395]
>> > -A PREROUTING -p tcp -m tcp --dport 3129 -j DROP
>> > COMMIT
>> > # Completed on Sun Feb 26 06:28:46 2017
>> > # Generated by iptables-save v1.6.0 on Sun Feb 26 06:28:46 2017
>> > *nat
>> > :PREROUTING ACCEPT [7766:436942]
>> > :INPUT ACCEPT [7766:436942]
>> > :OUTPUT ACCEPT [952:102330]
>> > :POSTROUTING ACCEPT [0:0]
>> > -A PREROUTING -s 35.154.101.8/32 -p tcp -m tcp --dport 443 -j ACCEPT
>> > -A PREROUTING -p tcp -m tcp --dport 443 -j DNAT --to-destination
>> > 35.154.101.8:3129
>> > -A POSTROUTING -j MASQUERADE
>> > COMMIT
>> > # Completed on Sun Feb 26 06:28:46 2017
>> >
>> > 2. Also pasting sudo iptables -L -nv:
>> >
>> > Chain INPUT (policy ACCEPT 216 packets, 16058 bytes)
>> >  pkts bytes target     prot opt in     out     source
>> > destination
>> >
>> > Chain FORWARD (policy ACCEPT 0 packets, 0 bytes)
>> >  pkts bytes target     prot opt in     out     source
>> > destination
>> >
>> > Chain OUTPUT (policy ACCEPT 161 packets, 24629 bytes)
>> >  pkts bytes target     prot opt in     out     source
>> > destination
>> >
>> >
>> >
>> >> And then clear out where is this proxy sittings and the network
>> >> structure.
>> >> It's not clear if the squid box is the router or a machine somewhere on
>> >> AWS.
>> >
>> > [Michael] This proxy is installed on an AWS instance.
>> >
>> >> If you wish to pass traffic from a local router to a one on AWS you
>> >> will need to create a tunnel like using OpenVPN or a similar solution and to
>> >> use some routing rules to pass the traffic from the local LAN to AWS without
>> >> removing the original destination address.
>> >>
>> >
>> > [Michael] Does this mean, to make ssl-bump work, I will have to setup
>> > a VPN server and configure the VPN clients to use this proxy via VPN
>> > server?
>> >
>> >
>> > Thanks,
>> > Michael.
>> >
>> > _______________________________________________
>> > squid-users mailing list
>> > squid-users at lists.squid-cache.org
>> > http://lists.squid-cache.org/listinfo/squid-users
>> >
>>
>>
>>
>> Thanks for replying Eliezer. Your advice is much appreciated.
>>
>> > The details you attached explained pretty well the cause for the issues
>> > you have described.
>> > What you will need to do in order to make this setup to work can be done
>> > in more then one way.
>> > For a sysadmin the simplest way is to create a VPN or some kind of a
>> > tunnel between the AWS instance to the local router.
>> > I am almost sure that you can use haproxy to do a local tproxy or
>> > interception that will forward the traffic to the remote squid with the
>> > PROXY protocol keeping original source and original destination visible to
>> > the remote squid.
>> >
>> > The choice will depend on both:
>> > - your skills and will to dig some time about couple subjects
>> > - The availability of static IP addresses(both local and AWS).
>> > - The OS on both sides
>>
>> [Michael] Actually, my original setup involves a VPN server. I wasn't
>> using it because I wanted to setup ssl-bump with simplest possible
>> settings. My actual setup involves:
>>
>> 1. strongSwan IPSec VPN server
>> 2. Squid Proxy server
>> 3. Clients will be IPSec VPN clients. I can specify the IP address and
>> port of HTTPS Proxy server in IPSec VPN client itself.
>>
>> In the above setup described, will I have to do something extra to
>> make ssl-bump work?
>>
>> Thanks,
>> Michael.
>
>
>
> What is the benefit of ssl-bump in this scenario?

Using ssl-bump, I will be able to filter HTTPS traffic based on either
HTTPS URL or content.

>
>
> --
> Best regards,
> Odhiambo WASHINGTON,
> Nairobi,KE
> +254 7 3200 0004/+254 7 2274 3223
> "Oh, the cruft."


From eliezer at ngtech.co.il  Mon Feb 27 11:38:33 2017
From: eliezer at ngtech.co.il (Eliezer  Croitoru)
Date: Mon, 27 Feb 2017 13:38:33 +0200
Subject: [squid-users] Two dns record fqdn pointing to different
	squid	servers
In-Reply-To: <1485975982132-4681422.post@n4.nabble.com>
References: <1485975982132-4681422.post@n4.nabble.com>
Message-ID: <7a6101d290ee$06bc0fd0$14342f70$@ngtech.co.il>

Did you got it resolved?

Eliezer

----
Eliezer Croitoru
Linux System Administrator
Mobile: +972-5-28704261
Email: eliezer at ngtech.co.il


-----Original Message-----
From: squid-users [mailto:squid-users-bounces at lists.squid-cache.org] On Behalf Of erdosain9
Sent: Wednesday, February 1, 2017 9:06 PM
To: squid-users at lists.squid-cache.org
Subject: [squid-users] Two dns record fqdn pointing to different squid servers

Hi.
I have running two squid servers.
One with ip access and another with users.
(the machine users are configure with "proxy.blabla.lan" (the squid with ip
access)

I want to know if it is possible do balance between them.
The problem, for me it is that the "server with ip access" it is refer with
a A dns record that point to his ip (proxy.blabla.lan)... and the "squid
with user access", the dns it is pointing with fqdn (squid.blabla.lan)....

So, i cant do a multiple A record, pointing to the two ip, because, one of
the squid servers wait a fqdn answer...

---------------------
I tried to do  CNAME but, its not working... (i tried to do
"proxy.blabla.com pointing to squid.blabla.com at the same time that the ip
of the "ip access squid server")

(hope this understood, i dont speak english)



--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/Two-dns-record-fqdn-pointing-to-different-squid-servers-tp4681422.html
Sent from the Squid - Users mailing list archive at Nabble.com.
_______________________________________________
squid-users mailing list
squid-users at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-users



From eliezer at ngtech.co.il  Mon Feb 27 12:02:27 2017
From: eliezer at ngtech.co.il (Eliezer  Croitoru)
Date: Mon, 27 Feb 2017 14:02:27 +0200
Subject: [squid-users] Squid 3.5.24 is available - Article and new Binaries
Message-ID: <7a6301d290f1$5d9ffd90$18dff8b0$@ngtech.co.il>

The Internet as a Talisman - SQUID 3.5.24 + 4.0.18 Released
[http://www1.ngtech.co.il/wpe/wp-content/uploads/2017/02/squid-box-s-l225.jpg]

Some have more and others have less meaning for things in their lives and specifically for objects and a objectives.
Most of the kids I have seen in my life have something embedded into them but not every eye can see the same things.
Depends on the background and nature of the person he or she can see beyond the flesh and blood.
There is some part of it in the form of genetic material but I and many others believe it?s not the only thing.
Every kid has it?s own embedded and unreplaceable soul.
We have the option to show some reflection of a fraction from our soul to others either by plain text or by some Talisman, there is meaning in things.
Even the most notorious researchers cannot deny that we all have some ?meta? things embedded into us which the genome cannot touch.
In a similar way to programming languages we can operate on the lower or the higher levels of this ?meta? world.

?What will I choose to show for all in my piece of heaven??

Take a look at the page and read the full article: http://www1.ngtech.co.il/wpe/?p=402

I have released the RPM's for:
- CentOS 6+7
- Oracle Enterprise Linux 6+7
- OpenSuse Leap
- RedHat Enterprise Linux
- SLES 12

And also tarred binaries for Debian and Ubuntu.

References:

* Squid-Cache CentOS repository details [http://wiki.squid-cache.org/KnowledgeBase/CentOS#Squid-3.5]
* Squid-Cache Binaries packages repo [http://ngtech.co.il/repo/]

Eliezer Croitoru


----
Eliezer Croitoru
Linux System Administrator
Mobile: +972-5-28704261
Email: eliezer at ngtech.co.il


-----Original Message-----
From: squid-users [mailto:squid-users-bounces at lists.squid-cache.org] On Behalf Of Amos Jeffries
Sent: Monday, January 30, 2017 9:09 AM
To: squid-announce at lists.squid-cache.org
Subject: [squid-users] [squid-announce] Squid 3.5.24 is available

The Squid HTTP Proxy team is very pleased to announce the availability of the Squid-3.5.24 release!


This release is a bug fix release resolving several issues found in the prior Squid releases.


The major changes to be aware of:

* Mitigate DoS attacks that use client-initiated SSL/TLS renegotiation.

Recent alterations to the SSL-Bump feature logic were found to be breaking the measure put in place to disable TLS renegotiation.
Since some TLSv1.2+ mechanisms actively require it and the upcoming OpenSSL v1.1+ make it quite hard to disable, we have decided to mitigate the vulnerability by implementing a rate limit on renegotiation instead of an outright disable.


* SSLv2 records force SslBump bumping despite a matching step2 peek rule.

This bug shows up as SSLv2 connections being bumped to deliver an error when they should have been spliced as configured. Squid will now splice all connections it has been configured to regardless of whether the obsolete SSLv2 syntax is being used.
 When bumping or receiving the connection itself Squid will still reject SSLv2. Only spliced traffic is affected by this.


* Update External ACL helpers error handling and caching

The Squid helper protocol has undergone several important changes but the external ACL logic and bundled helpers have not kept up. The ACL logics handling helper replies also had some bugs in the event of helper failures.

This release fixes those various bugs and updates all the bundled helpers to make use of the BH (BrokenHelper) status to signal internal errors differently to ACL denial.


* Bug #3940 pt2: Make 'cache deny' do what is documented

There was a small regression in 3.5.23 release fix for bug 3940. The 'cache deny' rules were not being obeyed. Surprisingly this has had no complaints.

Perhapse that is a sign that anyone using 'cache deny' rules should reasses whether those rules are still useful in these latest Squid releases.



 All users of Squid-3 are encouraged to upgrade to this release as soon as possible.


 See the ChangeLog for the full list of changes in this and earlier  releases.

Please refer to the release notes at
http://www.squid-cache.org/Versions/v3/3.5/RELEASENOTES.html
when you are ready to make the switch to Squid-3.5

Upgrade tip:
  "squid -k parse" is starting to display even more
   useful hints about squid.conf changes.

This new release can be downloaded from our HTTP or FTP servers

 http://www.squid-cache.org/Versions/v3/3.5/
 ftp://ftp.squid-cache.org/pub/squid/
 ftp://ftp.squid-cache.org/pub/archive/3.5/

or the mirrors. For a list of mirror sites see

 http://www.squid-cache.org/Download/http-mirrors.html
 http://www.squid-cache.org/Download/mirrors.html

If you encounter any issues with this release please file a bug report.
http://bugs.squid-cache.org/


Amos Jeffries

_______________________________________________
squid-announce mailing list
squid-announce at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-announce
_______________________________________________
squid-users mailing list
squid-users at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-users



From vel21ripn at gmail.com  Mon Feb 27 19:39:14 2017
From: vel21ripn at gmail.com (Vitaly Lavrov)
Date: Mon, 27 Feb 2017 22:39:14 +0300
Subject: [squid-users] [bug 4674] squid 4.0.18 delay_parameters for class 3
	assertion failed
Message-ID: <797dff2e-2672-4886-cb30-e1b60be2ae57@gmail.com>

[bug 4674] Regression in squid 4.0.18 (4.0.17 does not have this error)

OS: Slackware linux 14.2 / gcc 4.8.2

Simple config:

delay_pools 1
delay_class 1 3
delay_parameters 1 64000/64000 32000/32000 3000/3000

squid -k parse
----------------------------------------------------------------
2017/02/20 12:27:48| Processing: delay_pools 1
2017/02/20 12:27:48| Processing: delay_class 1 3
2017/02/20 12:27:48| assertion failed: CompositePoolNode.h:27: "byteCount == sizeof(CompositePoolNode)"
Aborted
----------------------------------------------------------------


From yvoinov at gmail.com  Tue Feb 28 00:08:44 2017
From: yvoinov at gmail.com (Yuri Voinov)
Date: Tue, 28 Feb 2017 06:08:44 +0600
Subject: [squid-users] [bug 4674] squid 4.0.18 delay_parameters for
 class 3 assertion failed
In-Reply-To: <797dff2e-2672-4886-cb30-e1b60be2ae57@gmail.com>
References: <797dff2e-2672-4886-cb30-e1b60be2ae57@gmail.com>
Message-ID: <62740373-85d9-ea35-bacb-6d11c3dc9091@gmail.com>



28.02.2017 1:39, Vitaly Lavrov ?????:
> [bug 4674] Regression in squid 4.0.18 (4.0.17 does not have this error)
>
> OS: Slackware linux 14.2 / gcc 4.8.2
May be ancient compiler. 4.8.2 is not fully C++11 compatible AFAIK.
Try at least 4.9.x. Or 5.4.
>
> Simple config:
>
> delay_pools 1
> delay_class 1 3
> delay_parameters 1 64000/64000 32000/32000 3000/3000
>
> squid -k parse
> ----------------------------------------------------------------
> 2017/02/20 12:27:48| Processing: delay_pools 1
> 2017/02/20 12:27:48| Processing: delay_class 1 3
> 2017/02/20 12:27:48| assertion failed: CompositePoolNode.h:27: "byteCount == sizeof(CompositePoolNode)"
> Aborted
> ----------------------------------------------------------------
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users

-- 
Bugs to the Future
-------------- next part --------------
A non-text attachment was scrubbed...
Name: 0x613DEC46.asc
Type: application/pgp-keys
Size: 2437 bytes
Desc: not available
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20170228/e843a66b/attachment.key>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: signature.asc
Type: application/pgp-signature
Size: 473 bytes
Desc: OpenPGP digital signature
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20170228/e843a66b/attachment.sig>

From squid at ephemeric.online  Tue Feb 28 12:53:55 2017
From: squid at ephemeric.online (Robert Gabriel)
Date: Tue, 28 Feb 2017 14:53:55 +0200
Subject: [squid-users] Huge Parent Log Lines
Message-ID: <20170228125355.GV16582@mail.ephemeric.online>

Hi,

We have below single log entry, which is HUGE!

Is there a way to decrease the log line size?

We only need something similar to default, no parents etc.

Thank you.

"Feb 22 09:50:05 3 3424 CONNECT 62.128.100.163:443 - HIER_NONE/- text/html#0121487749813.453 8317 10.224.101.132 TCP_MISS/206 89734 GET http://r3---sn-woc7en7z.gvt1.com/edgedl/release2/4ll88res6i0k5vapkhxf60lcld1qqdg1mkgbbvwg5rn8zp1tchc8np86upupsfhvasco4o167pdub21gpbtpemweza1t7u07u7i/24.0.0.221_win_PepperFlashPlayer.crx? - FIRSTUP_PARENT/10.231.221.74 application/octet-stream#0121487749813.695 225 10.224.101.132 TCP_MISS/302 1702 GET http://redirector.gvt1.com/edgedl/release2/4ll88res6i0k5vapkhxf60lcld1qqdg1mkgbbvwg5rn8zp1tchc8np86upupsfhvasco4o167pdub21gpbtpemweza1t7u07u7i/24.0.0.221_win_PepperFlashPlayer.crx - FIRSTUP_PARENT/10.231.221.74 text/html#0121487749813.764 375 10.224.100.68 TCP_MISS/200 1063 CONNECT googleads.g.doubleclick.net:443 - FIRSTUP_PARENT/10.231.221.74 -#0121487749813.822 15113 10.224.101.250 TCP_MISS/200 74954 GET http://dnl-03.geo.kaspersky.com/updates/kdb/i386/pef057.kdc - FIRSTUP_PARENT/10.231.221.74 application/octet-stream#0121487749813.851 764 10.224.100.68 TCP_MISS/200 3177 CONNECT r.turn.com:443 - FIRSTUP_PARENT/10.231.221.74 -#0121487749813.929 2006 10.224.100.68 TCP_MISS/200 4821 CONNECT 4922511.fls.doubleclick.net:443 - FIRSTUP_PARENT/10.231.221.74 -#0121487749814.021 184 10.224.101.250 TCP_MISS/200 694 GET http://dnl-03.geo.kaspersky.com/updates/wmuf/diffs/wmuf0002.dat.guh - FIRSTUP_PARENT/10.231.221.74 application/octet-stream#0121487749814.045 1084 10.224.100.68 TCP_MISS/200 664 CONNECT r.turn.com:443 - FIRSTUP_PARENT/10.231.221.74 -#0121487749814.695 644 10.224.100.68 TCP_MISS/200 4762 CONNECT maps-api-ssl.google.com:443 - FIRSTUP_PARENT/10.231.221.74 -#0121487749814.771 270 10.224.100.221 TCP_MISS/200 519 OPTIONS http://otf.msn.com/c.gif? - FIRSTUP_PARENT/10.231.221.74 text/plain#0121487749814.874 2915 10.224.100.68 TCP_MISS/200 1511 CONNECT cm.g.doubleclick.net:443 - FIRSTUP_PARENT/10.231.221.74 -#0121487749814.883 952 10.224.100.68 TCP_MISS/200 5401 CONNECT pixel.mathtag.com:443 - FIRSTUP_PARENT/10.231.221.74 -#0121487749814.893 1595 10.224.100.68 TCP_MISS/200 3177 CONNECT r.turn.com:443 - FIRSTUP_PARENT/10.231.221.74 -#0121487749814.894 806 10.224.101.250 TCP_MISS/200 684 GET http://dnl-03.geo.kaspersky.com/updates/wmuf/diffs/wmuf0004.dat.nlg - FIRSTUP_PARENT/10.231.221.74 application/octet-stream#0121487749815.056 5236 10.224.100.68 TCP_MISS/200 551 CONNECT cm.g.doubleclick.net:443 - FIRSTUP_PARENT/10.231.221.74 -#0121487749815.058 2070 10.224.100.68 TCP_MISS/200 3177 CONNECT r.turn.com:443 - FIRSTUP_PARENT/10.231.221.74 -#0121487749815.114 179 10.224.101.250 TCP_MISS/200 796 GET http://dnl-03.geo.kaspersky.com/updates/wmuf/diffs/wmuf0008.dat.ols - FIRSTUP_PARENT/10.231.221.74 application/octet-stream#0121487749815.326 176 10.224.101.250 TCP_MISS/200 735 GET http://dnl-03.geo.kaspersky.com/updates/wmuf/diffs/wmuf0009.dat.6qv - FIRSTUP_PARENT/10.231.221.74 application/octet-stream#0121487749815.384 607 10.224.100.221 TCP_MISS/200 578 POST http://otf.msn.com/c.gif? - FIRSTUP_PARENT/10.231.221.74 text/plain#0121487749815.659 241715 10.224.100.139 TCP_MISS/200 24163 CONNECT gm1.ggpht.com:443 - FIRSTUP_PARENT/10.231.221.74 -#0121487749815.726 0 10.229.71.30 TCP_DENIED/403 3435 GET http://www.google.com// - HIER_NONE/- text/html#0121487749815.757 1158 10.224.100.68 TCP_MISS/200 7896 CONNECT maps-api-ssl.google.com:443 - FIRSTUP_PARENT/10.231.221.74 -#0121487749815.862 976 10.224.100.68 TCP_MISS/200 7882 CONNECT maps-api-ssl.google.com:443 - FIRSTUP_PARENT/10.231.221.74 -#0121487749815.955 340 10.224.100.68 TCP_MISS/200 1319 CONNECT www.google.com:443 - FIRSTUP_PARENT/10.231.221.74 -#0121487749815.970 222 10.224.101.63 TCP_MISS/200 5840 CONNECT helpyouefile.sarsefiling.co.za:443 - FIRSTUP_PARENT/10.231.221.74 -#0121487749816.044 2959 10.224.100.68 TCP_MISS/200 3177 CONNECT r.turn.com:443 - FIRSTUP_PARENT/10.231.221.74 -#0121487749816.084 712 10.224.101.126 TCP_MISS/200 412 GET http://ping.chartbeat.net/ping? - FIRSTUP_PARENT/10.231.221.74 image/gif#0121487749816.084 2319 10.224.100.68 TCP_MISS/200 7092 CONNECT secure.adnxs.com:443 - FIRSTUP_PARENT/10.231.221.74 -#0121487749816.147 59 10.224.100.68 TCP_MISS/200 271 CONNECT ak1s.abmr.net:443 - FIRSTUP_PARENT/10.231.221.74 -#0121487749816.247 6550 10.224.100.68 TCP_MISS/200 5396 CONNECT secure.adnxs.com:443 - FIRSTUP_PARENT/10.231.221.74 -#0121487749816.428 570 10.224.101.250 TCP_MISS/200 658 GET http://dnl-16.geo.kaspersky.com/updaters/updater.xml.dif - FIRSTUP_PARENT/10.231.221.74 application/octet-stream#0121487749816.505 0 10.224.100.176 TCP_DENIED/403 3680 CONNECT qrtm1.instaforex.com:8443 - HIER_NONE/- text/html#0121487749816.548 0 10.224.100.176 TCP_DENIED/403 3680 CONNECT qrtm1.instaforex.com:8443 - HIER_NONE/- text/html#0121487749816.675 3414 10.224.101.129 TCP_MISS/200 7511 CONNECT login.live.com:443 - FIRSTUP_PARENT/10.231.221.74 -#0121487749816.747 596 10.224.100.68 TCP_MISS/200 647 CONNECT www.google.co.za:443 - FIRSTUP_PARENT/10.231.221.74 -#0121487749817.249 159614 10.224.100.103 TCP_MISS/200 379 GET http://su.ff.avast.com/R/A3kKIDljOWVkMmYzMGYyNzRmMDZhNWJjYjE3NGE0NjJjMzdmEgQAFQIXGNsBIgEAKgcIBBD_v7ZNKgcIAxDyvpJNMgoIBBD_v7ZNGIAKOO6RhIgBQiANFKxt1nDDEXHliIvCKJ259oVqZazWSw60wTp8jKzNwUiAg5gI - FIRSTUP_PARENT/10.231.221.74 application/octet-stream#0121487749817.328 243477 10.224.100.139 TCP_MISS/200 19261 CONNECT gm1.ggpht.com:443 - FIRSTUP_PARENT/10.231.221.74 -#0121487749817.408 241348 10.224.100.139 TCP_MISS/200 6810 CONNECT gm1.ggpht.com:443 - FIRSTUP_PARENT/10.231.221.74 -#0121487749817.489 3789 10.224.101.132 TCP_MISS/206 78226 GET http://r3---sn-woc7en7z.gvt1.com/edgedl/release2/4ll88res6i0k5vapkhxf60lcld1qqdg1mkgbbvwg5rn8zp1tchc8np86upupsfhvasco4o167pdub21gpbtpemweza1t7u07u7i/24.0.0.221_win_PepperFlashPlayer.crx? - FIRSTUP_PARENT/10.231.221.74 application/octet-stream#0121487749817.508 265 10.224.100.154 TCP_MISS/200 412 GET http://ping.chartbeat.net/ping? - FIRSTUP_PARENT/10.231.221.74 image/gif#0121487749817.653 1079 10.224.100.208 TCP_MISS/200 5566 CONNECT cupdates.trusteer.com:443 - FIRSTUP_PARENT/10.231.221.74 -#0121487749817.986 242880 10.224.100.139 TCP_MISS/200 1482 CONNECT gm1.ggpht.com:443 - FIRSTUP_PARENT/10.231.221.74 -#0121487749818.027 2952 10.224.100.68 TCP_MISS/200 5230 CONNECT beacon.krxd.net:443 - FIRSTUP_PARENT/10.231.221.74 -#0121487749818.032 4177 10.224.100.68 TCP_MISS/200 7204 CONNECT secure.adnxs.com:443 - FIRSTUP_PARENT/10.231.221.74 -#0121487749819.081 244916 10.224.100.139 TCP_MISS/200 1479 CONNECT gm1.ggpht.com:443 - FIRSTUP_PARENT/10.231.221.74 -#0121487749819.156 241146 10.224.100.139 TCP_MISS/200 793 CONNECT ssl.gstatic.com:443 - FIRSTUP_PARENT/10.231.221.74 -#0121487749819.207 4609 10.224.100.68 TCP_MISS/200 6619 CONNECT maps-api-ssl.google.com:443 - FIRSTUP_PARENT/10.231.221.74 -#0121487749819.335 560 10.224.100.154 TCP_MISS/200 577 POST http://otf.msn.com/c.gif? - FIRSTUP_PARENT/10.231.221.74 text/plain#0121487749819.396 37 10.224.100.79 TCP_MISS/200 1309 CONNECT translate.google.com:443 - FIRSTUP_PARENT/10.231.221.74 -#0121487749819.656 338 10.224.101.132 TCP_MISS/302 1702 GET http://redirector.gvt1.com/edgedl/release2/4ll88res6i0k5vapkhxf60lcld1qqdg1mkgbbvwg5rn8zp1tchc8np86upupsfhvasco4o167pdub21gpbtpemweza1t7u07u7i/24.0.0.221_win_PepperFlashPlayer.crx - FIRSTUP_PARENT/10.231.221.74 text/html#0121487749820.130 454 10.229.71.27 TCP_DENIED/403 3420 CONNECT 81.19.104.57:443 - HIER_NONE/- text/html#0121487749820.134 3527 10.224.101.250 TCP_MISS/200 844 GET http://dnl-18.geo.kaspersky.com/index/u0607g.xml.dif - FIRSTUP_PARENT/10.231.221.74 application/octet-stream#0121487749820.446 4398 10.224.100.68 TCP_MISS/200 7529 CONNECT ck.solocpm.com:443 - FIRSTUP_PARENT/10.231.221.74 -#0121487749820.532 241320 10.224.101.123 TCP_MISS/200 5966 CONNECT accounts.google.com:443 - FIRSTUP_PARENT/10.231.221.74 -#0121487749820.733 5119 10.224.100.68 TCP_MISS/200 8541 CONNECT aud.boostadvtracking.com:443 - FIRSTUP_PARENT/10.231.221.74 -#0121487749820.734 5658 10.224.100.68 TCP_MISS/200 5982 CONNECT beacon.krxd.net:443"


From lists at compuniverse.de  Tue Feb 28 14:10:11 2017
From: lists at compuniverse.de (Amon Ott)
Date: Tue, 28 Feb 2017 15:10:11 +0100
Subject: [squid-users] Username not passed to url_rewrite_program
In-Reply-To: <3c520a74-00ef-cb94-0724-c04163fd44c2@compuniverse.de>
References: <3c520a74-00ef-cb94-0724-c04163fd44c2@compuniverse.de>
Message-ID: <9e42743b-c973-a5ef-515f-e860f5faa9aa@compuniverse.de>

Am 23.02.2017 um 11:08 schrieb Amon Ott:
> we want to filter with squidGuard and decide based on the username,
> identified with ident. In the bug tracker I found the old entry 2655
> with this bug, which had been marked as fixed, but is still present in
> 3.5.24. I have recently reopened that bug with some info about our
> szenario. The old bugfix seems to be in the code, but does not solve the
> issue.
> 
> http://bugs.squid-cache.org/show_bug.cgi?id=2655#c11
> 
> Since we cannot continue with our setup, some help would be appreciated.

Bug is also present in 4.0.18, just tested.

Amon Ott.




From rousskov at measurement-factory.com  Tue Feb 28 15:28:36 2017
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Tue, 28 Feb 2017 08:28:36 -0700
Subject: [squid-users] Huge Parent Log Lines
In-Reply-To: <20170228125355.GV16582@mail.ephemeric.online>
References: <20170228125355.GV16582@mail.ephemeric.online>
Message-ID: <7f9459f1-ebd6-7d72-03c0-b4c5e53b492a@measurement-factory.com>

On 02/28/2017 05:53 AM, Robert Gabriel wrote:

> We have below single log entry, which is HUGE!

That does not look like a single log entry produced by Squid. It looks
like multiple log entries concatenated using '#' symbol and
post-processed by something outside Squid:

* The first field of the first entry starts with an imprecise
"human-friendly" time stamp (Feb 22 09:50:05) while the other
concatenated entries (after the '#' symbols) start with a Unix timestamp
(e.g., 0121487749813.453)

* The Unix timestamps increase.

* The request methods and URLs change.

* The last entry is incomplete.


Most likely, you have some custom log processing/analysis software that
mishandles Squid logs (possibly by mistreating everything it receives
from a file or a socket in a single read call as a single log entry).


HTH,

Alex.



> "Feb 22 09:50:05 3 3424 CONNECT 62.128.100.163:443 - HIER_NONE/- text/html#0121487749813.453 8317 10.224.101.132 TCP_MISS/206 89734 GET http://r3---sn-woc7en7z.gvt1.com/edgedl/release2/4ll88res6i0k5vapkhxf60lcld1qqdg1mkgbbvwg5rn8zp1tchc8np86upupsfhvasco4o167pdub21gpbtpemweza1t7u07u7i/24.0.0.221_win_PepperFlashPlayer.crx? - FIRSTUP_PARENT/10.231.221.74 application/octet-stream#0121487749813.695 225 10.224.101.132 TCP_MISS/302 1702 GET http://redirector.gvt1.com/edgedl/release2/4ll88res6i0k5vapkhxf60lcld1qqdg1mkgbbvwg5rn8zp1tchc8np86upupsfhvasco4o167pdub21gpbtpemweza1t7u07u7i/24.0.0.221_win_PepperFlashPlayer.crx - FIRSTUP_PARENT/10.231.221.74 text/html#0121487749813.764 375 10.224.100.68 TCP_MISS/200 1063 CONNECT googleads.g.doubleclick.net:443 - FIRSTUP_PARENT/10.231.221.74 -#0121487749813.822 15113 10.224.101.250 TCP_MISS/200 74954 GET http://dnl-03.geo.kaspersky.com/updates/kdb/i386/pef057.kdc - FIRSTUP_PARENT/10.231.221.74 application/octet-stream#0121487749813.851 764 10.224.100.68 TCP_MISS/200 3177 CONNECT r.turn.com:443 - FIRSTUP_PARENT/10.231.221.74 -#0121487749813.929 2006 10.224.100.68 TCP_MISS/200 4821 CONNECT 4922511.fls.doubleclick.net:443 - FIRSTUP_PARENT/10.231.221.74 -#0121487749814.021 184 10.224.101.250 TCP_MISS/200 694 GET http://dnl-03.geo.kaspersky.com/updates/wmuf/diffs/wmuf0002.dat.guh - FIRSTUP_PARENT/10.231.221.74 application/octet-stream#0121487749814.045 1084 10.224.100.68 TCP_MISS/200 664 CONNECT r.turn.com:443 - FIRSTUP_PARENT/10.231.221.74 -#0121487749814.695 644 10.224.100.68 TCP_MISS/200 4762 CONNECT maps-api-ssl.google.com:443 - FIRSTUP_PARENT/10.231.221.74 -#0121487749814.771 270 10.224.100.221 TCP_MISS/200 519 OPTIONS http://otf.msn.com/c.gif? - FIRSTUP_PARENT/10.231.221.74 text/plain#0121487749814.874 2915 10.224.100.68 TCP_MISS/200 1511 CONNECT cm.g.doubleclick.net:443 - FIRSTUP_PARENT/10.231.221.74 -#0121487749814.883 952 10.224.100.68 TCP_MISS/200 5401 CONNECT pixel.mathtag.com:443 - FIRSTUP_PARENT/10.231.221.74 -#0121487749814.893 1595 10.224.100.68 TCP_MISS/200 3177 CONNECT r.turn.com:443 - FIRSTUP_PARENT/10.231.221.74 -#0121487749814.894 806 10.224.101.250 TCP_MISS/200 684 GET http://dnl-03.geo.kaspersky.com/updates/wmuf/diffs/wmuf0004.dat.nlg - FIRSTUP_PARENT/10.231.221.74 application/octet-stream#0121487749815.056 5236 10.224.100.68 TCP_MISS/200 551 CONNECT cm.g.doubleclick.net:443 - FIRSTUP_PARENT/10.231.221.74 -#0121487749815.058 2070 10.224.100.68 TCP_MISS/200 3177 CONNECT r.turn.com:443 - FIRSTUP_PARENT/10.231.221.74 -#0121487749815.114 179 10.224.101.250 TCP_MISS/200 796 GET http://dnl-03.geo.kaspersky.com/updates/wmuf/diffs/wmuf0008.dat.ols - FIRSTUP_PARENT/10.231.221.74 application/octet-stream#0121487749815.326 176 10.224.101.250 TCP_MISS/200 735 GET http://dnl-03.geo.kaspersky.com/updates/wmuf/diffs/wmuf0009.dat.6qv - FIRSTUP_PARENT/10.231.221.74 application/octet-stream#0121487749815.384 607 10.224.100.221 TCP_MISS/200 578 POST http://otf.msn.com/c.gif? - FIRSTUP_PARENT/10.231.221.74 text/plain#0121487749815.659 241715 10.224.100.139 TCP_MISS/200 24163 CONNECT gm1.ggpht.com:443 - FIRSTUP_PARENT/10.231.221.74 -#0121487749815.726 0 10.229.71.30 TCP_DENIED/403 3435 GET http://www.google.com// - HIER_NONE/- text/html#0121487749815.757 1158 10.224.100.68 TCP_MISS/200 7896 CONNECT maps-api-ssl.google.com:443 - FIRSTUP_PARENT/10.231.221.74 -#0121487749815.862 976 10.224.100.68 TCP_MISS/200 7882 CONNECT maps-api-ssl.google.com:443 - FIRSTUP_PARENT/10.231.221.74 -#0121487749815.955 340 10.224.100.68 TCP_MISS/200 1319 CONNECT www.google.com:443 - FIRSTUP_PARENT/10.231.221.74 -#0121487749815.970 222 10.224.101.63 TCP_MISS/200 5840 CONNECT helpyouefile.sarsefiling.co.za:443 - FIRSTUP_PARENT/10.231.221.74 -#0121487749816.044 2959 10.224.100.68 TCP_MISS/200 3177 CONNECT r.turn.com:443 - FIRSTUP_PARENT/10.231.221.74 -#0121487749816.084 712 10.224.101.126 TCP_MISS/200 412 GET http://ping.chartbeat.net/ping? - FIRSTUP_PARENT/10.231.221.74 image/gif#0121487749816.084 2319 10.224.100.68 TCP_MISS/200 7092 CONNECT secure.adnxs.com:443 - FIRSTUP_PARENT/10.231.221.74 -#0121487749816.147 59 10.224.100.68 TCP_MISS/200 271 CONNECT ak1s.abmr.net:443 - FIRSTUP_PARENT/10.231.221.74 -#0121487749816.247 6550 10.224.100.68 TCP_MISS/200 5396 CONNECT secure.adnxs.com:443 - FIRSTUP_PARENT/10.231.221.74 -#0121487749816.428 570 10.224.101.250 TCP_MISS/200 658 GET http://dnl-16.geo.kaspersky.com/updaters/updater.xml.dif - FIRSTUP_PARENT/10.231.221.74 application/octet-stream#0121487749816.505 0 10.224.100.176 TCP_DENIED/403 3680 CONNECT qrtm1.instaforex.com:8443 - HIER_NONE/- text/html#0121487749816.548 0 10.224.100.176 TCP_DENIED/403 3680 CONNECT qrtm1.instaforex.com:8443 - HIER_NONE/- text/html#0121487749816.675 3414 10.224.101.129 TCP_MISS/200 7511 CONNECT login.live.com:443 - FIRSTUP_PARENT/10.231.221.74 -#0121487749816.747 596 10.224.100.68 TCP_MISS/200 647 CONNECT www.google.co.za:443 - FIRSTUP_PARENT/10.231.221.74 -#0121487749817.249 159614 10.224.100.103 TCP_MISS/200 379 GET http://su.ff.avast.com/R/A3kKIDljOWVkMmYzMGYyNzRmMDZhNWJjYjE3NGE0NjJjMzdmEgQAFQIXGNsBIgEAKgcIBBD_v7ZNKgcIAxDyvpJNMgoIBBD_v7ZNGIAKOO6RhIgBQiANFKxt1nDDEXHliIvCKJ259oVqZazWSw60wTp8jKzNwUiAg5gI - FIRSTUP_PARENT/10.231.221.74 application/octet-stream#0121487749817.328 243477 10.224.100.139 TCP_MISS/200 19261 CONNECT gm1.ggpht.com:443 - FIRSTUP_PARENT/10.231.221.74 -#0121487749817.408 241348 10.224.100.139 TCP_MISS/200 6810 CONNECT gm1.ggpht.com:443 - FIRSTUP_PARENT/10.231.221.74 -#0121487749817.489 3789 10.224.101.132 TCP_MISS/206 78226 GET http://r3---sn-woc7en7z.gvt1.com/edgedl/release2/4ll88res6i0k5vapkhxf60lcld1qqdg1mkgbbvwg5rn8zp1tchc8np86upupsfhvasco4o167pdub21gpbtpemweza1t7u07u7i/24.0.0.221_win_PepperFlashPlayer.crx? - FIRSTUP_PARENT/10.231.221.74 application/octet-stream#0121487749817.508 265 10.224.100.154 TCP_MISS/200 412 GET http://ping.chartbeat.net/ping? - FIRSTUP_PARENT/10.231.221.74 image/gif#0121487749817.653 1079 10.224.100.208 TCP_MISS/200 5566 CONNECT cupdates.trusteer.com:443 - FIRSTUP_PARENT/10.231.221.74 -#0121487749817.986 242880 10.224.100.139 TCP_MISS/200 1482 CONNECT gm1.ggpht.com:443 - FIRSTUP_PARENT/10.231.221.74 -#0121487749818.027 2952 10.224.100.68 TCP_MISS/200 5230 CONNECT beacon.krxd.net:443 - FIRSTUP_PARENT/10.231.221.74 -#0121487749818.032 4177 10.224.100.68 TCP_MISS/200 7204 CONNECT secure.adnxs.com:443 - FIRSTUP_PARENT/10.231.221.74 -#0121487749819.081 244916 10.224.100.139 TCP_MISS/200 1479 CONNECT gm1.ggpht.com:443 - FIRSTUP_PARENT/10.231.221.74 -#0121487749819.156 241146 10.224.100.139 TCP_MISS/200 793 CONNECT ssl.gstatic.com:443 - FIRSTUP_PARENT/10.231.221.74 -#0121487749819.207 4609 10.224.100.68 TCP_MISS/200 6619 CONNECT maps-api-ssl.google.com:443 - FIRSTUP_PARENT/10.231.221.74 -#0121487749819.335 560 10.224.100.154 TCP_MISS/200 577 POST http://otf.msn.com/c.gif? - FIRSTUP_PARENT/10.231.221.74 text/plain#0121487749819.396 37 10.224.100.79 TCP_MISS/200 1309 CONNECT translate.google.com:443 - FIRSTUP_PARENT/10.231.221.74 -#0121487749819.656 338 10.224.101.132 TCP_MISS/302 1702 GET http://redirector.gvt1.com/edgedl/release2/4ll88res6i0k5vapkhxf60lcld1qqdg1mkgbbvwg5rn8zp1tchc8np86upupsfhvasco4o167pdub21gpbtpemweza1t7u07u7i/24.0.0.221_win_PepperFlashPlayer.crx - FIRSTUP_PARENT/10.231.221.74 text/html#0121487749820.130 454 10.229.71.27 TCP_DENIED/403 3420 CONNECT 81.19.104.57:443 - HIER_NONE/- text/html#0121487749820.134 3527 10.224.101.250 TCP_MISS/200 844 GET http://dnl-18.geo.kaspersky.com/index/u0607g.xml.dif - FIRSTUP_PARENT/10.231.221.74 application/octet-stream#0121487749820.446 4398 10.224.100.68 TCP_MISS/200 7529 CONNECT ck.solocpm.com:443 - FIRSTUP_PARENT/10.231.221.74 -#0121487749820.532 241320 10.224.101.123 TCP_MISS/200 5966 CONNECT accounts.google.com:443 - FIRSTUP_PARENT/10.231.221.74 -#0121487749820.733 5119 10.224.100.68 TCP_MISS/200 8541 CONNECT aud.boostadvtracking.com:443 - FIRSTUP_PARENT/10.231.221.74 -#0121487749820.734 5658 10.224.100.68 TCP_MISS/200 5982 CONNECT beacon.krxd.net:443"


From squid at ephemeric.online  Tue Feb 28 15:39:11 2017
From: squid at ephemeric.online (Robert Gabriel)
Date: Tue, 28 Feb 2017 17:39:11 +0200
Subject: [squid-users] Huge Parent Log Lines
In-Reply-To: <7f9459f1-ebd6-7d72-03c0-b4c5e53b492a@measurement-factory.com>
References: <20170228125355.GV16582@mail.ephemeric.online>
 <7f9459f1-ebd6-7d72-03c0-b4c5e53b492a@measurement-factory.com>
Message-ID: <20170228153911.GZ16582@mail.ephemeric.online>

Hi,

Please pardon our ignorance and stupidity here. We got this in via syslog from Squid at a remote client site.

Thank you very much, we will ask our client what they are doing their side.

Much appreciated!

On Tue 28 Feb, 08:28, Alex Rousskov wrote:
> On 02/28/2017 05:53 AM, Robert Gabriel wrote:
> 
> > We have below single log entry, which is HUGE!
> 
> That does not look like a single log entry produced by Squid. It looks
> like multiple log entries concatenated using '#' symbol and
> post-processed by something outside Squid:
> 
> * The first field of the first entry starts with an imprecise
> "human-friendly" time stamp (Feb 22 09:50:05) while the other
> concatenated entries (after the '#' symbols) start with a Unix timestamp
> (e.g., 0121487749813.453)
> 
> * The Unix timestamps increase.
> 
> * The request methods and URLs change.
> 
> * The last entry is incomplete.
> 
> 
> Most likely, you have some custom log processing/analysis software that
> mishandles Squid logs (possibly by mistreating everything it receives
> from a file or a socket in a single read call as a single log entry).
> 
> 
> HTH,
> 
> Alex.
> 
> 
> 
> > "Feb 22 09:50:05 3 3424 CONNECT 62.128.100.163:443 - HIER_NONE/- text/html#0121487749813.453 8317 10.224.101.132 TCP_MISS/206 89734 GET http://r3---sn-woc7en7z.gvt1.com/edgedl/release2/4ll88res6i0k5vapkhxf60lcld1qqdg1mkgbbvwg5rn8zp1tchc8np86upupsfhvasco4o167pdub21gpbtpemweza1t7u07u7i/24.0.0.221_win_PepperFlashPlayer.crx? - FIRSTUP_PARENT/10.231.221.74 application/octet-stream#0121487749813.695 225 10.224.101.132 TCP_MISS/302 1702 GET http://redirector.gvt1.com/edgedl/release2/4ll88res6i0k5vapkhxf60lcld1qqdg1mkgbbvwg5rn8zp1tchc8np86upupsfhvasco4o167pdub21gpbtpemweza1t7u07u7i/24.0.0.221_win_PepperFlashPlayer.crx - FIRSTUP_PARENT/10.231.221.74 text/html#0121487749813.764 375 10.224.100.68 TCP_MISS/200 1063 CONNECT googleads.g.doubleclick.net:443 - FIRSTUP_PARENT/10.231.221.74 -#0121487749813.822 15113 10.224.101.250 TCP_MISS/200 74954 GET http://dnl-03.geo.kaspersky.com/updates/kdb/i386/pef057.kdc - FIRSTUP_PARENT/10.231.221.74 application/octet-stream#0121487749813.851 764 10.224.100.68 TCP_MISS/200 3177 CONNECT r.turn.com:443 - FIRSTUP_PARENT/10.231.221.74 -#0121487749813.929 2006 10.224.100.68 TCP_MISS/200 4821 CONNECT 4922511.fls.doubleclick.net:443 - FIRSTUP_PARENT/10.231.221.74 -#0121487749814.021 184 10.224.101.250 TCP_MISS/200 694 GET http://dnl-03.geo.kaspersky.com/updates/wmuf/diffs/wmuf0002.dat.guh - FIRSTUP_PARENT/10.231.221.74 application/octet-stream#0121487749814.045 1084 10.224.100.68 TCP_MISS/200 664 CONNECT r.turn.com:443 - FIRSTUP_PARENT/10.231.221.74 -#0121487749814.695 644 10.224.100.68 TCP_MISS/200 4762 CONNECT maps-api-ssl.google.com:443 - FIRSTUP_PARENT/10.231.221.74 -#0121487749814.771 270 10.224.100.221 TCP_MISS/200 519 OPTIONS http://otf.msn.com/c.gif? - FIRSTUP_PARENT/10.231.221.74 text/plain#0121487749814.874 2915 10.224.100.68 TCP_MISS/200 1511 CONNECT cm.g.doubleclick.net:443 - FIRSTUP_PARENT/10.231.221.74 -#0121487749814.883 952 10.224.100.68 TCP_MISS/200 5401 CONNECT pixel.mathtag.com:443 - FIRSTUP_PARENT/10.231.221.74 -#0121487749814.893 1595 10.224.100.68 TCP_MISS/200 3177 CONNECT r.turn.com:443 - FIRSTUP_PARENT/10.231.221.74 -#0121487749814.894 806 10.224.101.250 TCP_MISS/200 684 GET http://dnl-03.geo.kaspersky.com/updates/wmuf/diffs/wmuf0004.dat.nlg - FIRSTUP_PARENT/10.231.221.74 application/octet-stream#0121487749815.056 5236 10.224.100.68 TCP_MISS/200 551 CONNECT cm.g.doubleclick.net:443 - FIRSTUP_PARENT/10.231.221.74 -#0121487749815.058 2070 10.224.100.68 TCP_MISS/200 3177 CONNECT r.turn.com:443 - FIRSTUP_PARENT/10.231.221.74 -#0121487749815.114 179 10.224.101.250 TCP_MISS/200 796 GET http://dnl-03.geo.kaspersky.com/updates/wmuf/diffs/wmuf0008.dat.ols - FIRSTUP_PARENT/10.231.221.74 application/octet-stream#0121487749815.326 176 10.224.101.250 TCP_MISS/200 735 GET http://dnl-03.geo.kaspersky.com/updates/wmuf/diffs/wmuf0009.dat.6qv - FIRSTUP_PARENT/10.231.221.74 application/octet-stream#0121487749815.384 607 10.224.100.221 TCP_MISS/200 578 POST http://otf.msn.com/c.gif? - FIRSTUP_PARENT/10.231.221.74 text/plain#0121487749815.659 241715 10.224.100.139 TCP_MISS/200 24163 CONNECT gm1.ggpht.com:443 - FIRSTUP_PARENT/10.231.221.74 -#0121487749815.726 0 10.229.71.30 TCP_DENIED/403 3435 GET http://www.google.com// - HIER_NONE/- text/html#0121487749815.757 1158 10.224.100.68 TCP_MISS/200 7896 CONNECT maps-api-ssl.google.com:443 - FIRSTUP_PARENT/10.231.221.74 -#0121487749815.862 976 10.224.100.68 TCP_MISS/200 7882 CONNECT maps-api-ssl.google.com:443 - FIRSTUP_PARENT/10.231.221.74 -#0121487749815.955 340 10.224.100.68 TCP_MISS/200 1319 CONNECT www.google.com:443 - FIRSTUP_PARENT/10.231.221.74 -#0121487749815.970 222 10.224.101.63 TCP_MISS/200 5840 CONNECT helpyouefile.sarsefiling.co.za:443 - FIRSTUP_PARENT/10.231.221.74 -#0121487749816.044 2959 10.224.100.68 TCP_MISS/200 3177 CONNECT r.turn.com:443 - FIRSTUP_PARENT/10.231.221.74 -#0121487749816.084 712 10.224.101.126 TCP_MISS/200 412 GET http://ping.chartbeat.net/ping? - FIRSTUP_PARENT/10.231.221.74 image/gif#0121487749816.084 2319 10.224.100.68 TCP_MISS/200 7092 CONNECT secure.adnxs.com:443 - FIRSTUP_PARENT/10.231.221.74 -#0121487749816.147 59 10.224.100.68 TCP_MISS/200 271 CONNECT ak1s.abmr.net:443 - FIRSTUP_PARENT/10.231.221.74 -#0121487749816.247 6550 10.224.100.68 TCP_MISS/200 5396 CONNECT secure.adnxs.com:443 - FIRSTUP_PARENT/10.231.221.74 -#0121487749816.428 570 10.224.101.250 TCP_MISS/200 658 GET http://dnl-16.geo.kaspersky.com/updaters/updater.xml.dif - FIRSTUP_PARENT/10.231.221.74 application/octet-stream#0121487749816.505 0 10.224.100.176 TCP_DENIED/403 3680 CONNECT qrtm1.instaforex.com:8443 - HIER_NONE/- text/html#0121487749816.548 0 10.224.100.176 TCP_DENIED/403 3680 CONNECT qrtm1.instaforex.com:8443 - HIER_NONE/- text/html#0121487749816.675 3414 10.224.101.129 TCP_MISS/200 7511 CONNECT login.live.com:443 - FIRSTUP_PARENT/10.231.221.74 -#0121487749816.747 596 10.224.100.68 TCP_MISS/200 647 CONNECT www.google.co.za:443 - FIRSTUP_PARENT/10.231.221.74 -#0121487749817.249 159614 10.224.100.103 TCP_MISS/200 379 GET http://su.ff.avast.com/R/A3kKIDljOWVkMmYzMGYyNzRmMDZhNWJjYjE3NGE0NjJjMzdmEgQAFQIXGNsBIgEAKgcIBBD_v7ZNKgcIAxDyvpJNMgoIBBD_v7ZNGIAKOO6RhIgBQiANFKxt1nDDEXHliIvCKJ259oVqZazWSw60wTp8jKzNwUiAg5gI - FIRSTUP_PARENT/10.231.221.74 application/octet-stream#0121487749817.328 243477 10.224.100.139 TCP_MISS/200 19261 CONNECT gm1.ggpht.com:443 - FIRSTUP_PARENT/10.231.221.74 -#0121487749817.408 241348 10.224.100.139 TCP_MISS/200 6810 CONNECT gm1.ggpht.com:443 - FIRSTUP_PARENT/10.231.221.74 -#0121487749817.489 3789 10.224.101.132 TCP_MISS/206 78226 GET http://r3---sn-woc7en7z.gvt1.com/edgedl/release2/4ll88res6i0k5vapkhxf60lcld1qqdg1mkgbbvwg5rn8zp1tchc8np86upupsfhvasco4o167pdub21gpbtpemweza1t7u07u7i/24.0.0.221_win_PepperFlashPlayer.crx? - FIRSTUP_PARENT/10.231.221.74 application/octet-stream#0121487749817.508 265 10.224.100.154 TCP_MISS/200 412 GET http://ping.chartbeat.net/ping? - FIRSTUP_PARENT/10.231.221.74 image/gif#0121487749817.653 1079 10.224.100.208 TCP_MISS/200 5566 CONNECT cupdates.trusteer.com:443 - FIRSTUP_PARENT/10.231.221.74 -#0121487749817.986 242880 10.224.100.139 TCP_MISS/200 1482 CONNECT gm1.ggpht.com:443 - FIRSTUP_PARENT/10.231.221.74 -#0121487749818.027 2952 10.224.100.68 TCP_MISS/200 5230 CONNECT beacon.krxd.net:443 - FIRSTUP_PARENT/10.231.221.74 -#0121487749818.032 4177 10.224.100.68 TCP_MISS/200 7204 CONNECT secure.adnxs.com:443 - FIRSTUP_PARENT/10.231.221.74 -#0121487749819.081 244916 10.224.100.139 TCP_MISS/200 1479 CONNECT gm1.ggpht.com:443 - FIRSTUP_PARENT/10.231.221.74 -#0121487749819.156 241146 10.224.100.139 TCP_MISS/200 793 CONNECT ssl.gstatic.com:443 - FIRSTUP_PARENT/10.231.221.74 -#0121487749819.207 4609 10.224.100.68 TCP_MISS/200 6619 CONNECT maps-api-ssl.google.com:443 - FIRSTUP_PARENT/10.231.221.74 -#0121487749819.335 560 10.224.100.154 TCP_MISS/200 577 POST http://otf.msn.com/c.gif? - FIRSTUP_PARENT/10.231.221.74 text/plain#0121487749819.396 37 10.224.100.79 TCP_MISS/200 1309 CONNECT translate.google.com:443 - FIRSTUP_PARENT/10.231.221.74 -#0121487749819.656 338 10.224.101.132 TCP_MISS/302 1702 GET http://redirector.gvt1.com/edgedl/release2/4ll88res6i0k5vapkhxf60lcld1qqdg1mkgbbvwg5rn8zp1tchc8np86upupsfhvasco4o167pdub21gpbtpemweza1t7u07u7i/24.0.0.221_win_PepperFlashPlayer.crx - FIRSTUP_PARENT/10.231.221.74 text/html#0121487749820.130 454 10.229.71.27 TCP_DENIED/403 3420 CONNECT 81.19.104.57:443 - HIER_NONE/- text/html#0121487749820.134 3527 10.224.101.250 TCP_MISS/200 844 GET http://dnl-18.geo.kaspersky.com/index/u0607g.xml.dif - FIRSTUP_PARENT/10.231.221.74 application/octet-stream#0121487749820.446 4398 10.224.100.68 TCP_MISS/200 7529 CONNECT ck.solocpm.com:443 - FIRSTUP_PARENT/10.231.221.74 -#0121487749820.532 241320 10.224.101.123 TCP_MISS/200 5966 CONNECT accounts.google.com:443 - FIRSTUP_PARENT/10.231.221.74 -#0121487749820.733 5119 10.224.100.68 TCP_MISS/200 8541 CONNECT aud.boostadvtracking.com:443 - FIRSTUP_PARENT/10.231.221.74 -#0121487749820.734 5658 10.224.100.68 TCP_MISS/200 5982 CONNECT beacon.krxd.net:443"
> 


From adrian.m.miller at gmail.com  Tue Feb 28 15:58:00 2017
From: adrian.m.miller at gmail.com (stylemessiah)
Date: Tue, 28 Feb 2017 07:58:00 -0800 (PST)
Subject: [squid-users] SSL Bump and Certificate issue - RapidSSL
	Intermediate Cert
Message-ID: <1488297480314-4681635.post@n4.nabble.com>

This is driving me nuts, its the only issue ive found running ssl bump on my
home network for eons

I cant see image thumbnails on xda-developers...

When i access a thread with them, i get text links, not thumbnails, and if i
click on the links i get the following:


    (71) Protocol error (TLS code:
X509_V_ERR_UNABLE_TO_GET_ISSUER_CERT_LOCALLY)

    SSL Certficate error: certificate issuer (CA) not known:
/C=US/O=GeoTrust Inc./CN=RapidSSL SHA256 CA

I figured out by googling how to (i hope) trace the problem certificate via
s_client:


OpenSSL> s_client -showcerts -verify 32 -connect dl.xda-developers.com:443
verify depth is 32
CONNECTED(0000012C)
depth=0 CN = *.xda-developers.com
verify error:num=20:unable to get local issuer certificate
verify return:1
depth=0 CN = *.xda-developers.com
verify error:num=21:unable to verify the first certificate
verify return:1
---
Certificate chain
 0 s:/CN=*.xda-developers.com
   i:/C=US/O=GeoTrust Inc./CN=RapidSSL SHA256 CA
-----BEGIN CERTIFICATE-----
MIIFgTCCBGmgAwIBAgIQfA25Jbjbsyz/PbnaPlV5ozANBgkqhkiG9w0BAQsFADBC
MQswCQYDVQQGEwJVUzEWMBQGA1UEChMNR2VvVHJ1c3QgSW5jLjEbMBkGA1UEAxMS
UmFwaWRTU0wgU0hBMjU2IENBMB4XDTE2MTAwNDAwMDAwMFoXDTE3MTIwMzIzNTk1
OVowHzEdMBsGA1UEAwwUKi54ZGEtZGV2ZWxvcGVycy5jb20wggEiMA0GCSqGSIb3
DQEBAQUAA4IBDwAwggEKAoIBAQCtz+7A2NWVYg04JZTLCLf8+UGiJEBQXHgJENZd
bzGJpp8ue+L3a1o00uAnBYKAXzdEYYJ0cCHE4G+87okgDbSU2IO6Vvm2xf79tId/
BtQ6E6EXy4dSLya37k+fwnVo+b0c7sCnv6KRPG/z5zEQZLstY0RmUf+uS8ufoEII
Xv7HQFTXJ8by6VbA2PXKPZY+4Ok8mWMdMZx7F6kl0l+AP/pOyg59HLfvirtUElok
nwBHj20QbMg0ZF5wVYZn+7za51Ac3/Mrq0jJzs4WlofokDQWuB9pr7MZawkn2oj3
r+Ty4zeRLC4X7QMdiQAdB4OV1Uvl7sTl13g7reZoYHFUNrJ/AgMBAAGjggKUMIIC
kDAzBgNVHREELDAqghQqLnhkYS1kZXZlbG9wZXJzLmNvbYISeGRhLWRldmVsb3Bl
cnMuY29tMAkGA1UdEwQCMAAwKwYDVR0fBCQwIjAgoB6gHIYaaHR0cDovL2dwLnN5
bWNiLmNvbS9ncC5jcmwwbwYDVR0gBGgwZjBkBgZngQwBAgEwWjAqBggrBgEFBQcC
ARYeaHR0cHM6Ly93d3cucmFwaWRzc2wuY29tL2xlZ2FsMCwGCCsGAQUFBwICMCAM
Hmh0dHBzOi8vd3d3LnJhcGlkc3NsLmNvbS9sZWdhbDAfBgNVHSMEGDAWgBSXwidQ
nsLJ7AyIMsh8reKmAU/abzAOBgNVHQ8BAf8EBAMCBaAwHQYDVR0lBBYwFAYIKwYB
BQUHAwEGCCsGAQUFBwMCMFcGCCsGAQUFBwEBBEswSTAfBggrBgEFBQcwAYYTaHR0
cDovL2dwLnN5bWNkLmNvbTAmBggrBgEFBQcwAoYaaHR0cDovL2dwLnN5bWNiLmNv
bS9ncC5jcnQwggEFBgorBgEEAdZ5AgQCBIH2BIHzAPEAdwDd6x0reg1PpiCLga2B
aHB+Lo6dAdVciI09EcTNtuy+zAAAAVeRQGjoAAAEAwBIMEYCIQCGhvkj2j2G8/HS
+goN5+KUNcOo489VZB0yiuZ/i3O8EAIhAJarnN3GazZP/2MBfEK9bFaO+XTfnLSE
b+KC8+45pL65AHYAaPaY+B9kgr46jO65KB1M/HFRXWeT1ETRCmesu09P+8QAAAFX
kUBpCAAABAMARzBFAiB9Fc1GeA7oj/P31joQQbOTtlXr3v0Sy7wgg24WfcmcIQIh
ALjzk7c5ekv3D/TatIWhU249FMIOWeqs0HI9xXiC9ufwMA0GCSqGSIb3DQEBCwUA
A4IBAQCQTUYrtmdS+tgmIwnpSfufAnv4y1Zn+NuJFg9m3N1oFbNeEOoJ3C9LjzJC
jtzW5Z8HHZieT3jHAdEXGVe1uNqPX3jSQVOYNM+TXVb7rwqjUvaYYRuGp2cU4uis
pEHlsytWbMn1iGQVAr7cpJ4+wIby9c1sRXSHbFsPisR4mKzyAi2f0Dyb8CKIGLwN
6JuQw+a5k76p/ff9khjsRSdQIe6KroMrgIKltlmpqZiNaslY4YpPXMkT5Uj6RVci
JX81NejSjYGUbD1B0MXhuCzwSgjfuNKxTi73uoreQRgug1Tp3ObneM6pP/njp+sz
KI1VqiFrve2K2ebXvJ0EftQRclEi
-----END CERTIFICATE-----
---
Server certificate
subject=/CN=*.xda-developers.com
issuer=/C=US/O=GeoTrust Inc./CN=RapidSSL SHA256 CA
---
No client certificate CA names sent
Peer signing digest: SHA512
Server Temp Key: ECDH, P-256, 256 bits
---
SSL handshake has read 2067 bytes and written 302 bytes
Verification error: unable to verify the first certificate
---
New, TLSv1.2, Cipher is ECDHE-RSA-AES128-GCM-SHA256
Server public key is 2048 bit
Secure Renegotiation IS supported
Compression: NONE
Expansion: NONE
No ALPN negotiated
SSL-Session:
    Protocol  : TLSv1.2
    Cipher    : ECDHE-RSA-AES128-GCM-SHA256
    Session-ID:
733B4D29302703E57D32AB496A42CC1AB24056B9973A56F297F0B7D9429DFE0C

    Session-ID-ctx:
    Master-Key:
6B679C5560D68A9409F80DCEE91985E458A3D949CF7840F47832D75325B8DA3E
5E00C3AF2A099E51D95AC1290D1EA8C0
    PSK identity: None
    PSK identity hint: None
    SRP username: None
    TLS session ticket lifetime hint: 300 (seconds)
    TLS session ticket:
    0000 - 4c b4 25 2c 68 1a c0 fc-8c e6 d7 9c 66 37 a0 ec  
L.%,h.......f7..
    0010 - fa 2c f6 7a 78 2b 3a b0-f9 14 53 0e ed 93 21 5e  
.,.zx+:...S...!^
    0020 - 5f e6 48 db aa d5 7f c7-30 dc fe b1 e8 0d ff a5  
_.H.....0.......
    0030 - ad 50 40 ab 97 49 d8 ad-27 dc c1 e6 88 db 15 8c  
.P at ..I..'.......
    0040 - ed f6 dd d1 3f c9 70 a3-14 df a5 d6 c0 0d e2 cf  
....?.p.........
    0050 - 8f 19 3e 0c da 14 02 f1-83 83 82 61 39 bc f2 52  
..>........a9..R
    0060 - c4 92 6f cb cb 9b 05 4d-ce 96 ef 64 86 cb cb 85  
..o....M...d....
    0070 - 2d 51 0e 99 9a fd 1d b0-98 07 4e 8f c5 f7 57 ec  
-Q........N...W.
    0080 - 70 f1 28 bb d2 6a c9 57-bc f0 6d d3 e1 f5 13 c0  
p.(..j.W..m.....
    0090 - 37 ff f7 47 96 94 df eb-6a c9 f1 89 be c8 77 8a  
7..G....j.....w.

    Start Time: 1488297409
    Timeout   : 7200 (sec)
    Verify return code: 21 (unable to verify the first certificate)
    Extended master secret: no


Ive found the intermediate bundle from RapidSS, and added it to my existing
pem bundle...no change
Added as a separate pem i.e. sslproxy_foreign_intermediate_certs
/cygdrive/e/Squid/etc/ssl/extra-intermediate-CA.pem...no change

My sslbump related config lines are:

http_port 127.0.0.1:3128 ssl-bump generate-host-certificates=on
dynamic_cert_mem_cache_size=10MB cert=/cygdrive/e/Squid/etc/ssl/myCA.pem
capath=/cygdrive/e/Squid/etc/ssl
cafile=/cygdrive/e/Squid/etc/ssl/extra-intermediate-CA.pem
tls-dh=/cygdrive/e/Squid/etc/ssl/dhparam.pem
options=NO_SSLv2,NO_SSLv3,SINGLE_ECDH_USE 

acl step1 at_step SslBump1
ssl_bump peek step1
ssl_bump bump all


sslcrtd_program /cygdrive/e/Squid/lib/squid/ssl_crtd -s
/cygdrive/e/Squid/var/cache/squid_ssldb -M 4MB -b 2048
sslcrtd_children 10 startup=10 idle=1

sslproxy_cipher
EECDH+ECDSA+AESGCM:EECDH+aRSA+AESGCM:EECDH+ECDSA+SHA384:EECDH+ECDSA+SHA256:EECDH+aRSA+SHA384:EECDH+aRSA+SHA256:EECDH+aRSA+RC4:EECDH:EDH+aRSA:!RC4:!aNULL:!eNULL:!LOW:!3DES:!MD5:!EXP:!PSK:!SRP:!DSS

Im at my wits end, its the only site that has a glitch

I tried all i could think of, and google, before posting, hopefully someone
has an idea/suggestion

cheers in advance



--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/SSL-Bump-and-Certificate-issue-RapidSSL-Intermediate-Cert-tp4681635.html
Sent from the Squid - Users mailing list archive at Nabble.com.


