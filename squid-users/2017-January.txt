From rousskov at measurement-factory.com  Mon Jan  2 03:04:13 2017
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Sun, 1 Jan 2017 20:04:13 -0700
Subject: [squid-users] Problem with ssl_crtd
In-Reply-To: <58a6b9fcba54f11bfa7e21ed4256e252@treenet.co.nz>
References: <1482945386684-4680998.post@n4.nabble.com>
 <58a6b9fcba54f11bfa7e21ed4256e252@treenet.co.nz>
Message-ID: <a3e7a89c-86d1-ce2c-49a7-335a21b41154@measurement-factory.com>

On 12/28/2016 11:48 PM, Amos Jeffries wrote:
> On 2016-12-29 06:16, Eduardo Carneiro wrote:
>> 2016/12/28 12:29:17 kid1| assertion failed: Read.cc:69:
>> "fd_table[conn->fd].halfClosedReader != NULL"


> http://bugs.squid-cache.org/show_bug.cgi?id=4270

Agreed. There has not been any recent meaningful updates of that bug so
it might be fixed in the latest versions (or at least reduced to
reconfiguration races). I recommend upgrading to the latest v3.5 (at
least) before proceeding further with the triage.


>> FATAL: Ipc::Mem::Segment::open failed to
>> shm_open(/squid-ssl_session_cache.shm): (2) No such file or directory
>>
>> Squid Cache (Version 3.5.19): Terminated abnormally.


> Hmm, seems there is a part of bug 3805 still present
>  <http://bugs.squid-cache.org/show_bug.cgi?id=3805>

IMHO, the closed bug #3805 is unrelated to the Ipc::Mem::Segment::open
failure. I suspect this failure is a side effect of the earlier
assertion and restarting bugs (in Squid and/or external scripts).


HTH,

Alex.



From rousskov at measurement-factory.com  Mon Jan  2 03:16:38 2017
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Sun, 1 Jan 2017 20:16:38 -0700
Subject: [squid-users] squid 4.0.17 accept-encoding.. sending gzip?!
In-Reply-To: <318e1cd0-5245-17cf-2d19-0c34da443866@cinbesa.com.br>
References: <59a77f42-2ccf-54d5-0e3b-7578e8213b8c@cinbesa.com.br>
 <1593d9ab838.277f.0fa0e86c8d2ee43f749db760f8bca319@measurement-factory.com>
 <88c47e33f72d7917844ac09abca5df36@treenet.co.nz>
 <318e1cd0-5245-17cf-2d19-0c34da443866@cinbesa.com.br>
Message-ID: <fc2a1b0a-1fb9-dbdf-55f1-488b48d039dd@measurement-factory.com>

On 12/27/2016 06:13 AM, Heiler Bemerguy wrote:

> Why is squid compressing it

Squid does not compress because Squid does not have the code to compress.

To understand what it going on, you need to find the agent that does
compression. It could be the origin server, an ICAP service, an eCAP
adapter, or even another proxy. If you think that a packet capture or
any other information source shows that your stock Squid compresses
content, then something went wrong either when capturing data or when
interpreting it.

If it is the origin server that does compression, then what you are
seeing is simply a bad side effect of forcing Squid to cache something
it must not cache.


HTH,

Alex.



From rousskov at measurement-factory.com  Mon Jan  2 03:48:50 2017
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Sun, 1 Jan 2017 20:48:50 -0700
Subject: [squid-users] Squid Websocket Issue
In-Reply-To: <CA+sSnVYOZXznoDpAR4pA_-=xNVCQaqPxLbHPs3FdfXa4oduEcA@mail.gmail.com>
References: <CA+sSnVZo1250omWkLDzPpknEa-qqLNisA6YJeROx5BxL6pqv9Q@mail.gmail.com>
 <0577240a-6b02-0bd7-2e0d-2e58b5bb7b77@treenet.co.nz>
 <CA+sSnVZYcQkvA=-bBVSuQtoXeYxomL5Ca51cw4ax0dP3EUKF7g@mail.gmail.com>
 <5990ba78-39ea-569a-29dd-409e4912f1a6@treenet.co.nz>
 <010401d259f9$4dfcd630$e9f68290$@ngtech.co.il>
 <CA+sSnVavKQR80XNvykdecMeB1feLwBMEP29Vm+Lxp3=U8-S1dg@mail.gmail.com>
 <CA+sSnVbob7T03YvK8RCHmd4o=9Ej8NZ6vhmgKj+h75hnB1EXdQ@mail.gmail.com>
 <b858d071-bc6c-1789-c72b-d1b9343b32e5@measurement-factory.com>
 <CA+sSnVYOZXznoDpAR4pA_-=xNVCQaqPxLbHPs3FdfXa4oduEcA@mail.gmail.com>
Message-ID: <1340f864-8473-e44a-c580-49b478f5fc1c@measurement-factory.com>

On 12/27/2016 04:50 AM, Hardik Dangar wrote:

> If i remove !serverIsws somehow websockets will not work.

Then there is a bug somewhere AFAICT. It is your call whether to find
out what that bug is [while continuing to use a potentially dangerous
workaround].

Alex.


> On Tue, Dec 20, 2016 at 10:27 PM, Alex Rousskov wrote:
> 
>     On 12/20/2016 02:42 AM, Hardik Dangar wrote:
>     > Following changes in config works and whatsapp starts working,
>     >
>     > acl serverIsws ssl::server_name_regex ^w[0-9]+\.web\.whatsapp\.com$
>     >
>     > acl step1 at_step SslBump1
>     > ssl_bump peek step1
>     > ssl_bump splice serverIsws
>     > ssl_bump bump !serverIsws all
> 
>     You do not need the "!serverIsws" part because if serverIsws matches,
>     then the splice rule wins, and Squid does not reach the bump rule. This
>     configuration is sufficient:
> 
>       ssl_bump peek step1
>       ssl_bump splice serverIsws
>       ssl_bump bump all
> 
>     In theory, adding "!serverIsws" does not hurt. However, negating complex
>     ACLs is tricky/dangerous and should be avoided when possible.
> 
>     Alex.
> 
> 



From rousskov at measurement-factory.com  Mon Jan  2 05:40:00 2017
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Sun, 1 Jan 2017 22:40:00 -0700
Subject: [squid-users] acls with the same name, last wins
In-Reply-To: <c08c592a043673588b2e7d907bc142f3@treenet.co.nz>
References: <CAHvB88z33YGF8jveNmOu-Q6RuV7yNUsaAczWb3KikA4eW+tNoQ@mail.gmail.com>
 <c08c592a043673588b2e7d907bc142f3@treenet.co.nz>
Message-ID: <a6e45038-88a5-bd55-ddda-7d790bbd7b73@measurement-factory.com>

On 12/29/2016 10:44 PM, Amos Jeffries wrote:

> The intended design for ACLs is that basic/primitive tests check one
> piece of state data and get chained explicitly in the access lines for
> AND/OR conditions. That way it is clear what is being processed and
> matched (or not matched).

The intended design is to OR same-name ACL lines, just like Ivan
expected. We cannot wiggle ourselves out of that fundamental rule by
pointing to those squid.conf directives that support similar ORing
logic. The behavior reported by Ivan is probably just an unintentional
consequence of header ACL _implementation_ rather than a conscious
design error.

FWIW, IMHO, effectively ignoring some ACLs is a serious bug!


> So for now I am making Squid produce a config ERROR when this config
> situation is found.

Thank you. A non-fatal ERROR is the right temporary "change" until this
bug is properly fixed.

Alex.



From hardikdangar+squid at gmail.com  Mon Jan  2 06:46:33 2017
From: hardikdangar+squid at gmail.com (Hardik Dangar)
Date: Mon, 2 Jan 2017 12:16:33 +0530
Subject: [squid-users] Squid Websocket Issue
In-Reply-To: <1340f864-8473-e44a-c580-49b478f5fc1c@measurement-factory.com>
References: <CA+sSnVZo1250omWkLDzPpknEa-qqLNisA6YJeROx5BxL6pqv9Q@mail.gmail.com>
 <0577240a-6b02-0bd7-2e0d-2e58b5bb7b77@treenet.co.nz>
 <CA+sSnVZYcQkvA=-bBVSuQtoXeYxomL5Ca51cw4ax0dP3EUKF7g@mail.gmail.com>
 <5990ba78-39ea-569a-29dd-409e4912f1a6@treenet.co.nz>
 <010401d259f9$4dfcd630$e9f68290$@ngtech.co.il>
 <CA+sSnVavKQR80XNvykdecMeB1feLwBMEP29Vm+Lxp3=U8-S1dg@mail.gmail.com>
 <CA+sSnVbob7T03YvK8RCHmd4o=9Ej8NZ6vhmgKj+h75hnB1EXdQ@mail.gmail.com>
 <b858d071-bc6c-1789-c72b-d1b9343b32e5@measurement-factory.com>
 <CA+sSnVYOZXznoDpAR4pA_-=xNVCQaqPxLbHPs3FdfXa4oduEcA@mail.gmail.com>
 <1340f864-8473-e44a-c580-49b478f5fc1c@measurement-factory.com>
Message-ID: <CA+sSnVYuZBxnMEWPjOxcFX6H5917fxcZzpLMK6GndVo6d24pPw@mail.gmail.com>

@amos or anyone else from dev team

Can you confirm this is intentional behavior or bug ?

On Mon, Jan 2, 2017 at 9:18 AM, Alex Rousskov <
rousskov at measurement-factory.com> wrote:

> On 12/27/2016 04:50 AM, Hardik Dangar wrote:
>
> > If i remove !serverIsws somehow websockets will not work.
>
> Then there is a bug somewhere AFAICT. It is your call whether to find
> out what that bug is [while continuing to use a potentially dangerous
> workaround].
>
> Alex.
>
>
> > On Tue, Dec 20, 2016 at 10:27 PM, Alex Rousskov wrote:
> >
> >     On 12/20/2016 02:42 AM, Hardik Dangar wrote:
> >     > Following changes in config works and whatsapp starts working,
> >     >
> >     > acl serverIsws ssl::server_name_regex ^w[0-9]+\.web\.whatsapp\.com$
> >     >
> >     > acl step1 at_step SslBump1
> >     > ssl_bump peek step1
> >     > ssl_bump splice serverIsws
> >     > ssl_bump bump !serverIsws all
> >
> >     You do not need the "!serverIsws" part because if serverIsws matches,
> >     then the splice rule wins, and Squid does not reach the bump rule.
> This
> >     configuration is sufficient:
> >
> >       ssl_bump peek step1
> >       ssl_bump splice serverIsws
> >       ssl_bump bump all
> >
> >     In theory, adding "!serverIsws" does not hurt. However, negating
> complex
> >     ACLs is tricky/dangerous and should be avoided when possible.
> >
> >     Alex.
> >
> >
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20170102/1c8dc1f2/attachment.htm>

From eliezer at ngtech.co.il  Mon Jan  2 13:47:20 2017
From: eliezer at ngtech.co.il (Eliezer  Croitoru)
Date: Mon, 2 Jan 2017 15:47:20 +0200
Subject: [squid-users] Squid Websocket Issue
In-Reply-To: <CA+sSnVYuZBxnMEWPjOxcFX6H5917fxcZzpLMK6GndVo6d24pPw@mail.gmail.com>
References: <CA+sSnVZo1250omWkLDzPpknEa-qqLNisA6YJeROx5BxL6pqv9Q@mail.gmail.com>
 <0577240a-6b02-0bd7-2e0d-2e58b5bb7b77@treenet.co.nz>
 <CA+sSnVZYcQkvA=-bBVSuQtoXeYxomL5Ca51cw4ax0dP3EUKF7g@mail.gmail.com>
 <5990ba78-39ea-569a-29dd-409e4912f1a6@treenet.co.nz>
 <010401d259f9$4dfcd630$e9f68290$@ngtech.co.il>
 <CA+sSnVavKQR80XNvykdecMeB1feLwBMEP29Vm+Lxp3=U8-S1dg@mail.gmail.com>
 <CA+sSnVbob7T03YvK8RCHmd4o=9Ej8NZ6vhmgKj+h75hnB1EXdQ@mail.gmail.com>
 <b858d071-bc6c-1789-c72b-d1b9343b32e5@measurement-factory.com>
 <CA+sSnVYOZXznoDpAR4pA_-=xNVCQaqPxLbHPs3FdfXa4oduEcA@mail.gmail.com>
 <1340f864-8473-e44a-c580-49b478f5fc1c@measurement-factory.com>
 <CA+sSnVYuZBxnMEWPjOxcFX6H5917fxcZzpLMK6GndVo6d24pPw@mail.gmail.com>
Message-ID: <012a01d264fe$bd3120f0$379362d0$@ngtech.co.il>

Can we start from 0.
Currently when squid knows about the Connection being a one with websocket support it is already too late to do anything about this specific connection.
The only option for now is to identify these using some ICAP service that will for example redirect the request after a small delay that will add the destination domain ip address to a bypass list.
It?s not trivial but I have seen such implementation on ssl bump.

Can you please redirect me to the specific email with the bug details?

Eliezer

----
http://ngtech.co.il/lmgtfy/
Linux System Administrator
Mobile: +972-5-28704261
Email: eliezer at ngtech.co.il


From: squid-users [mailto:squid-users-bounces at lists.squid-cache.org] On Behalf Of Hardik Dangar
Sent: Monday, January 2, 2017 8:47 AM
To: Alex Rousskov <rousskov at measurement-factory.com>
Cc: Squid Users <squid-users at lists.squid-cache.org>
Subject: Re: [squid-users] Squid Websocket Issue

@amos or anyone else from dev team

Can you confirm this is intentional behavior or bug ?

On Mon, Jan 2, 2017 at 9:18 AM, Alex Rousskov <mailto:rousskov at measurement-factory.com> wrote:
On 12/27/2016 04:50 AM, Hardik Dangar wrote:

> If i remove !serverIsws somehow websockets will not work.

Then there is a bug somewhere AFAICT. It is your call whether to find
out what that bug is [while continuing to use a potentially dangerous
workaround].

Alex.


> On Tue, Dec 20, 2016 at 10:27 PM, Alex Rousskov wrote:
>
>     On 12/20/2016 02:42 AM, Hardik Dangar wrote:
>     > Following changes in config works and whatsapp starts working,
>     >
>     > acl serverIsws ssl::server_name_regex ^w[0-9]+\.web\.whatsapp\.com$
>     >
>     > acl step1 at_step SslBump1
>     > ssl_bump peek step1
>     > ssl_bump splice serverIsws
>     > ssl_bump bump !serverIsws all
>
>     You do not need the "!serverIsws" part because if serverIsws matches,
>     then the splice rule wins, and Squid does not reach the bump rule. This
>     configuration is sufficient:
>
>       ssl_bump peek step1
>       ssl_bump splice serverIsws
>       ssl_bump bump all
>
>     In theory, adding "!serverIsws" does not hurt. However, negating complex
>     ACLs is tricky/dangerous and should be avoided when possible.
>
>     Alex.
>
>




From vvjoshi5 at gmail.com  Mon Jan  2 16:25:24 2017
From: vvjoshi5 at gmail.com (vinay)
Date: Mon, 2 Jan 2017 08:25:24 -0800 (PST)
Subject: [squid-users] Squid 3.5.23-1 is available for Ubuntu 16.04 LTS
 (online repo ubuntu16.diladele.com)
In-Reply-To: <DB6PR0401MB26808172DDFE4A59040671378F930@DB6PR0401MB2680.eurprd04.prod.outlook.com>
References: <DB6PR0401MB26808172DDFE4A59040671378F930@DB6PR0401MB2680.eurprd04.prod.outlook.com>
Message-ID: <1483374324362-4681025.post@n4.nabble.com>

Rafael,
Am facing below 2 issues.

1.I am working on squid 3.3.8 on ubuntu 14.04 , I am trying to upgrade the
squid version to higher than the default one I.e higher than 3.4 version but
am getting an error. "Unable to locate package libecap3" on giving apt-get
install libecap3 command. Any suggestions to upgrade the version ?
2. In the existing version of squid 3.3.8 am getting the below lines in
access logs
"TCP_MISS/200" and it's not caching .
Any suggestions on this to overcome the caching issue? 



--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/Squid-3-5-23-1-is-available-for-Ubuntu-16-04-LTS-online-repo-ubuntu16-diladele-com-tp4680948p4681025.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From hardikdangar+squid at gmail.com  Mon Jan  2 16:49:56 2017
From: hardikdangar+squid at gmail.com (Hardik Dangar)
Date: Mon, 2 Jan 2017 22:19:56 +0530
Subject: [squid-users] Squid Websocket Issue
In-Reply-To: <012a01d264fe$bd3120f0$379362d0$@ngtech.co.il>
References: <CA+sSnVZo1250omWkLDzPpknEa-qqLNisA6YJeROx5BxL6pqv9Q@mail.gmail.com>
 <0577240a-6b02-0bd7-2e0d-2e58b5bb7b77@treenet.co.nz>
 <CA+sSnVZYcQkvA=-bBVSuQtoXeYxomL5Ca51cw4ax0dP3EUKF7g@mail.gmail.com>
 <5990ba78-39ea-569a-29dd-409e4912f1a6@treenet.co.nz>
 <010401d259f9$4dfcd630$e9f68290$@ngtech.co.il>
 <CA+sSnVavKQR80XNvykdecMeB1feLwBMEP29Vm+Lxp3=U8-S1dg@mail.gmail.com>
 <CA+sSnVbob7T03YvK8RCHmd4o=9Ej8NZ6vhmgKj+h75hnB1EXdQ@mail.gmail.com>
 <b858d071-bc6c-1789-c72b-d1b9343b32e5@measurement-factory.com>
 <CA+sSnVYOZXznoDpAR4pA_-=xNVCQaqPxLbHPs3FdfXa4oduEcA@mail.gmail.com>
 <1340f864-8473-e44a-c580-49b478f5fc1c@measurement-factory.com>
 <CA+sSnVYuZBxnMEWPjOxcFX6H5917fxcZzpLMK6GndVo6d24pPw@mail.gmail.com>
 <012a01d264fe$bd3120f0$379362d0$@ngtech.co.il>
Message-ID: <CA+sSnVYCk_q=YR10zaVs1oHq_usPJrrZuiRnHoDEHx_mV7dCXQ@mail.gmail.com>

Hey Eliezer,

The issue was with whatsapp web socket was not working, here is detailed
information about issue
------------

Here is some information about my squid version,

Squid Cache: Version 3.5.22-20161115-r14113
Service Name: squid
configure options:  '--prefix=/usr' '--localstatedir=/var/squid'
'--libexecdir=/lib/squid' '--srcdir=.' '--datadir=/share/squid'
'--sysconfdir=/etc/squid' '--with-default-user=proxy'
'--with-logdir=/var/log/squid' '--with-pidfile=/var/run/squid.pid'
'--with-openssl' '--enable-ssl-crtd' '--enable-inline'
'--disable-arch-native' '--enable-async-io=8'
'--enable-storeio=ufs,aufs,diskd,rock' '--enable-removal-policies=lru,heap'
'--enable-delay-pools' '--enable-follow-x-forwarded-for'
'--enable-url-rewrite-helpers=fake' '--enable-ecap'

My squid config file is located at, http://pastebin.com/raw/LvDxEF4x

Now the issue is whenever someone requests a page which contains web socket
requests response is always bad request.
Here is an example,

Request URL:wss://w4.web.whatsapp.com/ws
Request Method:GET
Status Code:400 Bad Request

Response Headers
#################
Connection:keep-alive
Date:Sat, 17 Dec 2016 09:05:36 GMT
Transfer-Encoding:chunked
X-Cache:MISS from Proxy

Request Headers
#################
Accept-Encoding:gzip, deflate, sdch, br
Accept-Language:en-US,en;q=0.8
Cache-Control:no-cache
Connection:Upgrade
Host:w4.web.whatsapp.com
Origin:https://web.whatsapp.com
Pragma:no-cache
Sec-WebSocket-Extensions:permessage-deflate; client_max_window_bits
Sec-WebSocket-Key:kzrB2ZcMHDAqvjDNXnjL/w==
Sec-WebSocket-Version:13
Upgrade:websocket
User-Agent:Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like
Gecko) Chrome/55.0.2883.75 Safari/537.36


My question is how we can work with web socket requests in squid or if not
by pass them squid. My squid instance is in interception mode and requests
are intercepted at instance via iptables and forwarded to squid using below
rules,

SQUIDIP=192.168.1.1

# your proxy listening port
SQUIDHTTPPORT=3128
SQUIDHTTPSPORT=3129


iptables -t nat -A PREROUTING -s $SQUIDIP -p tcp --dport 80 -j ACCEPT
iptables -t nat -A PREROUTING -p tcp --dport 80 -j REDIRECT --to-port
$SQUIDHTTPPORT

iptables -t nat -A PREROUTING -s $SQUIDIP -p tcp --dport 443 -j ACCEPT
iptables -t nat -A PREROUTING -p tcp --dport 443 -j REDIRECT --to-port
$SQUIDHTTPSPORT

iptables -t nat -A POSTROUTING -j MASQUERADE
iptables -t mangle -A PREROUTING -p tcp --dport $SQUIDHTTPPORT -j DROP
iptables -t mangle -A PREROUTING -p tcp --dport $SQUIDHTTPSPORT -j DROP


If anyone can help me with this it would be really awesome. Thanks for your
support.

----------------------------------------------------------

*Solution to above problem was,*

acl serverIsws ssl::server_name_regex ^w[0-9]+\.web\.whatsapp\.com$

acl step1 at_step SslBump1
ssl_bump peek step1
ssl_bump splice serverIsws
ssl_bump bump !serverIsws all

[ above is a feature of whatsapp which allows you to connect to
web.whatsapp.com from browser]


now what happens at request level is following,

Request URL:wss://w8.web.whatsapp.com/ws
Request Method:GET
Status Code:101 Switching Protocols

----------------------------------

Response Headers

Connection:Upgrade
Sec-WebSocket-Accept:Z6CC+QVdvB0cCHPbJAQMaHKL2uQ=
Upgrade:websocket

----------------------------------
Request Headers

Accept-Encoding:gzip, deflate, sdch, br
Accept-Language:en-US,en;q=0.8
Cache-Control:no-cache
Connection:Upgrade
Host:w8.web.whatsapp.com
Origin:https://web.whatsapp.com
Pragma:no-cache
Sec-WebSocket-Extensions:permessage-deflate; client_max_window_bits
Sec-WebSocket-Key:mbCFLN/Q1KMt58t6DoQI9Q==
Sec-WebSocket-Version:13
Upgrade:websocket
User-Agent:Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like
Gecko) Chrome/55.0.2883.75 Safari/537.36


So basically websockets are connected as normal https request( i think this
is a very nature of Web sockets and define somewhere in web socket
standards).


Now the problem statement is,

ssl_bump bump !serverIsws all

If i remove !serverIsws then it stops working. as per alex it shoudn't
happen and its a bug most probably.


On Mon, Jan 2, 2017 at 7:17 PM, Eliezer Croitoru <eliezer at ngtech.co.il>
wrote:

> Can we start from 0.
> Currently when squid knows about the Connection being a one with websocket
> support it is already too late to do anything about this specific
> connection.
> The only option for now is to identify these using some ICAP service that
> will for example redirect the request after a small delay that will add the
> destination domain ip address to a bypass list.
> It?s not trivial but I have seen such implementation on ssl bump.
>
> Can you please redirect me to the specific email with the bug details?
>
> Eliezer
>
> ----
> http://ngtech.co.il/lmgtfy/
> Linux System Administrator
> Mobile: +972-5-28704261
> Email: eliezer at ngtech.co.il
>
>
> From: squid-users [mailto:squid-users-bounces at lists.squid-cache.org] On
> Behalf Of Hardik Dangar
> Sent: Monday, January 2, 2017 8:47 AM
> To: Alex Rousskov <rousskov at measurement-factory.com>
> Cc: Squid Users <squid-users at lists.squid-cache.org>
> Subject: Re: [squid-users] Squid Websocket Issue
>
> @amos or anyone else from dev team
>
> Can you confirm this is intentional behavior or bug ?
>
> On Mon, Jan 2, 2017 at 9:18 AM, Alex Rousskov <mailto:
> rousskov at measurement-factory.com> wrote:
> On 12/27/2016 04:50 AM, Hardik Dangar wrote:
>
> > If i remove !serverIsws somehow websockets will not work.
>
> Then there is a bug somewhere AFAICT. It is your call whether to find
> out what that bug is [while continuing to use a potentially dangerous
> workaround].
>
> Alex.
>
>
> > On Tue, Dec 20, 2016 at 10:27 PM, Alex Rousskov wrote:
> >
> >     On 12/20/2016 02:42 AM, Hardik Dangar wrote:
> >     > Following changes in config works and whatsapp starts working,
> >     >
> >     > acl serverIsws ssl::server_name_regex ^w[0-9]+\.web\.whatsapp\.com$
> >     >
> >     > acl step1 at_step SslBump1
> >     > ssl_bump peek step1
> >     > ssl_bump splice serverIsws
> >     > ssl_bump bump !serverIsws all
> >
> >     You do not need the "!serverIsws" part because if serverIsws matches,
> >     then the splice rule wins, and Squid does not reach the bump rule.
> This
> >     configuration is sufficient:
> >
> >       ssl_bump peek step1
> >       ssl_bump splice serverIsws
> >       ssl_bump bump all
> >
> >     In theory, adding "!serverIsws" does not hurt. However, negating
> complex
> >     ACLs is tricky/dangerous and should be avoided when possible.
> >
> >     Alex.
> >
> >
>
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20170102/21cd449c/attachment.htm>

From mrghorbani2002 at gmail.com  Mon Jan  2 18:15:32 2017
From: mrghorbani2002 at gmail.com (mr ghorbani)
Date: Mon, 2 Jan 2017 21:45:32 +0330
Subject: [squid-users] squid tproxy connection time out
Message-ID: <CA+W7yKeqqG6p=-8VACH2y7CsvT_pz8jrc3Bsdk7=+koxX+L--Q@mail.gmail.com>

hello masters
I have a problem on the squid in tproxy mode and it is that  squid
return Error "110 connection
the timeout." for all Requests on port 3129, which is related to tproxy
Of course, by eliminating the code
ip route add local 0.0.0.0/0 dev lo table 100
Problem solved, but in this case, Squid does not record any activity log.

i had attached the network topology.

In this network, 185.12.32.1 is gw for both of the client and squid. it is set
to That route the client communication to the squid and then, squid
connect it to the internet by using this gw and also redirect port 80
to 3129 and tproxy to cache the data.
The gw is connected to the Internet and also it is the bgp for these
IP addresses.
-------------- next part --------------
A non-text attachment was scrubbed...
Name: Drawing1.jpg
Type: image/jpeg
Size: 32857 bytes
Desc: not available
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20170102/96c337e3/attachment.jpg>

From flashdown at data-core.org  Mon Jan  2 18:33:27 2017
From: flashdown at data-core.org (Flashdown)
Date: Mon, 02 Jan 2017 19:33:27 +0100
Subject: [squid-users] Squid 3.5.22 Bug when using Mimetype Detection?
	rep_mime_type
Message-ID: <2c3cfc7bfc490465ee71dee437ac50d4@data-core.org>

Hello together,

with Squid 3.5.22 I have switched from using a url-regex to Mime Type 
Detection, which seemed to work nicely until now... :/

OS: Debian Stretch 4.8.0-1-amd64 #1 SMP Debian 4.8.7-1 (2016-11-13) 
x86_64 GNU/Linux

I faced the following Situation:

When I globally deny specific mimetypes using a blockfile, then it 
performs as it should, so only mime types I defined in the block file 
are getting blocked, so far so good.

When I do an exception for a group I belong to like unblocking 
application/octet-stream, then I can download files, so the exception 
works in the first place.
acl mime_IT rep_mime_type application/octet-stream
http_reply_access allow IT mime_IT

Normally internal targets are excluded from the Proxy using Proxy 
Exception lists. But I do not get these settings automatically, so my 
browser did not contain this exception so I was able to discover the 
following behavior:

The Issue is occuring when browsing to an internal OTRS Web Server via 
FQDN (It's a web ticket system) through the proxy I get "Access Denied" 
from the Proxy on all requests. But when browsing to an online OTRS Demo 
site with the same OTRS version like this one: 
http://itsm-demo.otrs.com/otrs/index.pl then it works. When I now try 
again to access the internal OTRS Server through the proxy it works. 
That's strange, when I now force reload (CTRL+F5 in Firefox) the 
internal OTRS Ticketsystems webpage, I get the "access denied" again.

When I remove the exception from the global block list for the group I 
belong to,- here it's IT- then this issue does not occur and the website 
is accessible like it should.
So I just need to comment out these lines:
#acl mime_IT rep_mime_type application/octet-stream
#http_reply_access allow IT mime_IT

When I add text/html & application/xml to the global block exception, 
then this error does not occur anymore.
acl mime_IT rep_mime_type application/octet-stream text/html 
application/xml
http_reply_access allow IT mime_IT

So currently I can workaround the issue in 3 different ways:
1. Do not create a global mimetype block exception for groups I belong 
to
2. Browse to the start page of an Online Demo OTRS Site and then reload 
the internal Website
3. Add text/html & application/xml to my exception even if these 
Mimetypes are not part of the global block list, so they are not 
supposed to be blocked. (I just looked at the internal website and it 
just uses text/html and application/xml on the start page (Login Page) 
so I added them to the exception list for my group and it worked)

Conclusion: When having a global mime type block and unblocking a 
specific mime type for a specific group, then this group will most 
propably face issues with mime types that are not supposed to be 
blocked. So in case of errors, I need to unblock not blocked mimetypes 
,too.


My Squid config for mime type blocking:
---------------------------------------
## Define Default MIMETYPE ERROR Message and global block access list
acl block_mimetypes rep_mime_type "/etc/squid/mimetype_blacklist.acl"
deny_info ERR_BLOCKED_FILES block_mimetypes

# Configure Execptions
acl mime_IT rep_mime_type application/octet-stream
http_reply_access allow IT mime_IT

acl mime_SpecialGroup rep_mime_type application/octet-stream
http_reply_access allow SpecialGroup mime_SpecialGroup


#Applying Global MimeType Block
http_reply_access deny block_mimetypes
---------------------------------------
Squid main config:
---------------------------------------
http_port 8080 ssl-bump generate-host-certificates=on 
dynamic_cert_mem_cache_size=16MB cert=/*********************
ssl_bump splice localhost
ssl_bump splice SSL_Exclude
ssl_bump bump all
sslproxy_cert_error allow SSL_TrustedSites
sslproxy_cert_error deny all
---------------------------------------



Contents of mimetype_blacklist.acl:
---------------------------------------
##############
#This is the global blocklist
# Executables: bin exe com dll class
application/x-msdownload
application/octet-stream
application/exe
application/x-exe
application/dos-exe
vms/exe
application/x-winexe
application/msdos-windows
application/x-msdos-program

# .msi
application/x-msi

# .vbs
text/vbscript
text/vbs

# Archives
# .gz
application/gzip
# .z
application/x-compress
# .gtar
application/x-gtar
# .zip
#application/zip
# .tar
application/x-tar
# .rar
application/x-rar-compressed
# .7z
application/x-7z-compressed

# .torrent
application/x-bittorrent
#############
---------------------------------------

Squid Cache: Version 3.5.22
Service Name: squid
Debian linux
configure options:  '--build=x86_64-linux-gnu' '--prefix=/usr' 
'--includedir=${prefix}/include' '--mandir=${prefix}/share/man' 
'--infodir=${prefix}/share/info' '--sysconfdir=/etc' 
'--localstatedir=/var' '--libexecdir=${prefix}/lib/squid3' '--srcdir=.' 
'--disable-maintainer-mode' '--disable-dependency-tracking' 
'--disable-silent-rules' 'BUILDCXXFLAGS=-g -O2 
-fdebug-prefix-map=/usr/src/mycompile/squid3-3.5.22=. -fPIE 
-fstack-protector-strong -Wformat -Werror=format-security -Wdate-time 
-D_FORTIFY_SOURCE=2 -fPIE -pie -Wl,-z,relro -Wl,-z,now -Wl,--as-needed' 
'--datadir=/usr/share/squid' '--sysconfdir=/etc/squid' 
'--libexecdir=/usr/lib/squid' '--mandir=/usr/share/man' 
'--enable-inline' '--disable-arch-native' '--enable-async-io=8' 
'--enable-storeio=ufs,aufs,diskd,rock' 
'--enable-removal-policies=lru,heap' '--enable-delay-pools' 
'--enable-cache-digests' '--enable-icap-client' 
'--enable-follow-x-forwarded-for' 
'--enable-auth-basic=DB,fake,getpwnam,LDAP,NCSA,NIS,PAM,POP3,RADIUS,SASL,SMB' 
'--enable-auth-digest=file,LDAP' 
'--enable-auth-negotiate=kerberos,wrapper' 
'--enable-auth-ntlm=fake,smb_lm' 
'--enable-external-acl-helpers=file_userip,kerberos_ldap_group,LDAP_group,session,SQL_session,time_quota,unix_group,wbinfo_group' 
'--enable-url-rewrite-helpers=fake' '--enable-eui' '--enable-esi' 
'--enable-icmp' '--enable-zph-qos' '--enable-ecap' 
'--disable-translation' '--with-swapdir=/var/spool/squid' 
'--with-logdir=/var/log/squid' '--with-pidfile=/var/run/squid.pid' 
'--with-filedescriptors=65536' '--with-large-files' '--enable-ssl' 
'--enable-ssl-crtd' '--with-openssl' '--with-default-user=proxy' 
'--enable-build-info=Debian linux' '--enable-linux-netfilter' 
'build_alias=x86_64-linux-gnu' 'CFLAGS=-g -O2 
-fdebug-prefix-map=/usr/src/mycompile/squid3-3.5.22=. -fPIE 
-fstack-protector-strong -Wformat -Werror=format-security -Wall' 
'LDFLAGS=-fPIE -pie -Wl,-z,relro -Wl,-z,now -Wl,--as-needed' 
'CPPFLAGS=-Wdate-time -D_FORTIFY_SOURCE=2' 'CXXFLAGS=-g -O2 
-fdebug-prefix-map=/usr/src/mycompile/squid3-3.5.22=. -fPIE 
-fstack-protector-strong -Wformat -Werror=format-security'

1483381150.095    186 10.3.101.23 TCP_DENIED_REPLY/403 4493 GET 
http://otrs-server.**.**.com/otrs/index.pl - HIER_DIRECT/10.2.1.107 
text/html
1483381150.118      3 10.3.101.23 TCP_DENIED_REPLY/403 4451 GET 
http://*****-proxy.**.**.com:8080/squid-internal-static/icons/SN.png - 
HIER_NONE/- text/html

-- 
Best regards,
Enrico Heine

?This email and any files transmitted with it are confidential and 
intended solely for the use of the individual or entity to whom they are 
addressed. If you have received this email in error please notify the 
system manager. This message contains confidential information and is 
intended only for the individual named. If you are not the named 
addressee you should not disseminate, distribute or copy this e-mail. 
Please notify the sender immediately by e-mail if you have received this 
e-mail by mistake and delete this e-mail from your system. If you are 
not the intended recipient you are notified that disclosing, copying, 
distributing or taking any action in reliance on the contents of this 
information is strictly prohibited.


From eliezer at ngtech.co.il  Mon Jan  2 20:49:59 2017
From: eliezer at ngtech.co.il (Eliezer  Croitoru)
Date: Mon, 2 Jan 2017 22:49:59 +0200
Subject: [squid-users] Squid Websocket Issue
In-Reply-To: <CA+sSnVYCk_q=YR10zaVs1oHq_usPJrrZuiRnHoDEHx_mV7dCXQ@mail.gmail.com>
References: <CA+sSnVZo1250omWkLDzPpknEa-qqLNisA6YJeROx5BxL6pqv9Q@mail.gmail.com>
 <0577240a-6b02-0bd7-2e0d-2e58b5bb7b77@treenet.co.nz>
 <CA+sSnVZYcQkvA=-bBVSuQtoXeYxomL5Ca51cw4ax0dP3EUKF7g@mail.gmail.com>
 <5990ba78-39ea-569a-29dd-409e4912f1a6@treenet.co.nz>
 <010401d259f9$4dfcd630$e9f68290$@ngtech.co.il>
 <CA+sSnVavKQR80XNvykdecMeB1feLwBMEP29Vm+Lxp3=U8-S1dg@mail.gmail.com>
 <CA+sSnVbob7T03YvK8RCHmd4o=9Ej8NZ6vhmgKj+h75hnB1EXdQ@mail.gmail.com>
 <b858d071-bc6c-1789-c72b-d1b9343b32e5@measurement-factory.com>
 <CA+sSnVYOZXznoDpAR4pA_-=xNVCQaqPxLbHPs3FdfXa4oduEcA@mail.gmail.com>
 <1340f864-8473-e44a-c580-49b478f5fc1c@measurement-factory.com>
 <CA+sSnVYuZBxnMEWPjOxcFX6H5917fxcZzpLMK6GndVo6d24pPw@mail.gmail.com>
 <012a01d264fe$bd3120f0$379362d0$@ngtech.co.il>
 <CA+sSnVYCk_q=YR10zaVs1oHq_usPJrrZuiRnHoDEHx_mV7dCXQ@mail.gmail.com>
Message-ID: <000d01d26539$c87dbaf0$597930d0$@ngtech.co.il>

I am using the next ontop of squid 3.5.23 in non intercept mode:
##STARTOF SNIPPET
acl DiscoverSNIHost at_step SslBump1
acl NoSSLIntercept ssl::server_name_regex -i "/etc/squid/url.nobump"

ssl_bump splice NoSSLIntercept

ssl_bump peek DiscoverSNIHost
ssl_bump bump all
##END OF SNIPPPET

And the content of url.nobump is:##START OF QUOTE
# WU (Squid 3.5.x and above with SSL Bump)
# Only this sites must be spliced.
update\.microsoft\.com$
update\.microsoft\.com\.akadns\.net$
v10\.vortex\-win\.data\.microsoft.com$
settings\-win\.data\.microsoft\.com$
# The next are trusted SKYPE addresses
a\.config\.skype\.com$
pipe\.skype\.com$
w[0-9]+\.web\.whatsapp\.com$
##END OF  QUOTE

And whatsapp web sockets works for me.

Please be more specific whats not working and on what platform..
If it works for me and not for you there is a difference between our clients or systems.
Try using latest 3.5.23.
If you are using an RPM based system you can use one of my RPMS.

Let me know if my rules helps you and if there is a specific site that doesn?t work for you,
Eliezer

----
http://ngtech.co.il/lmgtfy/
Linux System Administrator
Mobile: +972-5-28704261
Email: eliezer at ngtech.co.il


From: hardikdangar at gmail.com [mailto:hardikdangar at gmail.com] On Behalf Of Hardik Dangar
Sent: Monday, January 2, 2017 6:50 PM
To: Eliezer Croitoru <eliezer at ngtech.co.il>
Cc: Squid Users <squid-users at lists.squid-cache.org>
Subject: Re: [squid-users] Squid Websocket Issue

Hey Eliezer,

The issue was with whatsapp web socket was not working, here is detailed information about issue
------------

Here is some information about my squid version,

Squid Cache: Version 3.5.22-20161115-r14113
Service Name: squid
configure options:  '--prefix=/usr' '--localstatedir=/var/squid' '--libexecdir=/lib/squid' '--srcdir=.' '--datadir=/share/squid' '--sysconfdir=/etc/squid' '--with-default-user=proxy' '--with-logdir=/var/log/squid' '--with-pidfile=/var/run/squid.pid' '--with-openssl' '--enable-ssl-crtd' '--enable-inline' '--disable-arch-native' '--enable-async-io=8' '--enable-storeio=ufs,aufs,diskd,rock' '--enable-removal-policies=lru,heap' '--enable-delay-pools' '--enable-follow-x-forwarded-for' '--enable-url-rewrite-helpers=fake' '--enable-ecap'

My squid config file is located at, http://pastebin.com/raw/LvDxEF4x

Now the issue is whenever someone requests a page which contains web socket requests response is always bad request. 
Here is an example,

Request URL:wss://http://w4.web.whatsapp.com/ws
Request Method:GET
Status Code:400 Bad Request

Response Headers
#################
Connection:keep-alive
Date:Sat, 17 Dec 2016 09:05:36 GMT
Transfer-Encoding:chunked
X-Cache:MISS from Proxy

Request Headers
#################
Accept-Encoding:gzip, deflate, sdch, br
Accept-Language:en-US,en;q=0.8
Cache-Control:no-cache
Connection:Upgrade
Host:http://w4.web.whatsapp.com
Origin:https://web.whatsapp.com
Pragma:no-cache
Sec-WebSocket-Extensions:permessage-deflate; client_max_window_bits
Sec-WebSocket-Key:kzrB2ZcMHDAqvjDNXnjL/w==
Sec-WebSocket-Version:13
Upgrade:websocket
User-Agent:Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/55.0.2883.75 Safari/537.36


My question is how we can work with web socket requests in squid or if not by pass them squid. My squid instance is in interception mode and requests are intercepted at instance via iptables and forwarded to squid using below rules,

SQUIDIP=192.168.1.1

# your proxy listening port
SQUIDHTTPPORT=3128
SQUIDHTTPSPORT=3129


iptables -t nat -A PREROUTING -s $SQUIDIP -p tcp --dport 80 -j ACCEPT
iptables -t nat -A PREROUTING -p tcp --dport 80 -j REDIRECT --to-port $SQUIDHTTPPORT

iptables -t nat -A PREROUTING -s $SQUIDIP -p tcp --dport 443 -j ACCEPT
iptables -t nat -A PREROUTING -p tcp --dport 443 -j REDIRECT --to-port $SQUIDHTTPSPORT

iptables -t nat -A POSTROUTING -j MASQUERADE
iptables -t mangle -A PREROUTING -p tcp --dport $SQUIDHTTPPORT -j DROP
iptables -t mangle -A PREROUTING -p tcp --dport $SQUIDHTTPSPORT -j DROP


If anyone can help me with this it would be really awesome. Thanks for your support.

----------------------------------------------------------

Solution to above problem was,

acl serverIsws ssl::server_name_regex ^w[0-9]+\.web\.whatsapp\.com$

acl step1 at_step SslBump1
ssl_bump peek step1
ssl_bump splice serverIsws
ssl_bump bump !serverIsws all

[ above is a feature of whatsapp which allows you to connect to http://web.whatsapp.com/ from browser]


now what happens at request level is following,

Request URL:wss://http://w8.web.whatsapp.com/ws
Request Method:GET
Status Code:101 Switching Protocols

----------------------------------

Response Headers

Connection:Upgrade
Sec-WebSocket-Accept:Z6CC+QVdvB0cCHPbJAQMaHKL2uQ=
Upgrade:websocket

----------------------------------
Request Headers

Accept-Encoding:gzip, deflate, sdch, br
Accept-Language:en-US,en;q=0.8
Cache-Control:no-cache
Connection:Upgrade
Host:http://w8.web.whatsapp.com/
Origin:https://web.whatsapp.com/
Pragma:no-cache
Sec-WebSocket-Extensions:permessage-deflate; client_max_window_bits
Sec-WebSocket-Key:mbCFLN/Q1KMt58t6DoQI9Q==
Sec-WebSocket-Version:13
Upgrade:websocket
User-Agent:Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/55.0.2883.75 Safari/537.36


So basically websockets are connected as normal https request( i think this is a very nature of Web sockets and define somewhere in web socket standards).


Now the problem statement is,

ssl_bump bump !serverIsws all

If i remove !serverIsws then it stops working. as per alex it shoudn't happen and its a bug most probably.


On Mon, Jan 2, 2017 at 7:17 PM, Eliezer Croitoru <mailto:eliezer at ngtech.co.il> wrote:
Can we start from 0.
Currently when squid knows about the Connection being a one with websocket support it is already too late to do anything about this specific connection.
The only option for now is to identify these using some ICAP service that will for example redirect the request after a small delay that will add the destination domain ip address to a bypass list.
It?s not trivial but I have seen such implementation on ssl bump.

Can you please redirect me to the specific email with the bug details?

Eliezer

----
http://ngtech.co.il/lmgtfy/
Linux System Administrator
Mobile: +972-5-28704261
Email: mailto:eliezer at ngtech.co.il


From: squid-users [mailto:mailto:squid-users-bounces at lists.squid-cache.org] On Behalf Of Hardik Dangar
Sent: Monday, January 2, 2017 8:47 AM
To: Alex Rousskov <mailto:rousskov at measurement-factory.com>
Cc: Squid Users <mailto:squid-users at lists.squid-cache.org>
Subject: Re: [squid-users] Squid Websocket Issue

@amos or anyone else from dev team

Can you confirm this is intentional behavior or bug ?
On Mon, Jan 2, 2017 at 9:18 AM, Alex Rousskov <mailto:mailto:rousskov at measurement-factory.com> wrote:
On 12/27/2016 04:50 AM, Hardik Dangar wrote:

> If i remove !serverIsws somehow websockets will not work.

Then there is a bug somewhere AFAICT. It is your call whether to find
out what that bug is [while continuing to use a potentially dangerous
workaround].

Alex.


> On Tue, Dec 20, 2016 at 10:27 PM, Alex Rousskov wrote:
>
>     On 12/20/2016 02:42 AM, Hardik Dangar wrote:
>     > Following changes in config works and whatsapp starts working,
>     >
>     > acl serverIsws ssl::server_name_regex ^w[0-9]+\.web\.whatsapp\.com$
>     >
>     > acl step1 at_step SslBump1
>     > ssl_bump peek step1
>     > ssl_bump splice serverIsws
>     > ssl_bump bump !serverIsws all
>
>     You do not need the "!serverIsws" part because if serverIsws matches,
>     then the splice rule wins, and Squid does not reach the bump rule. This
>     configuration is sufficient:
>
>       ssl_bump peek step1
>       ssl_bump splice serverIsws
>       ssl_bump bump all
>
>     In theory, adding "!serverIsws" does not hurt. However, negating complex
>     ACLs is tricky/dangerous and should be avoided when possible.
>
>     Alex.
>
>





From squid3 at treenet.co.nz  Tue Jan  3 04:09:14 2017
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Tue, 03 Jan 2017 17:09:14 +1300
Subject: [squid-users] Squid Websocket Issue
In-Reply-To: <CA+sSnVYuZBxnMEWPjOxcFX6H5917fxcZzpLMK6GndVo6d24pPw@mail.gmail.com>
References: <CA+sSnVZo1250omWkLDzPpknEa-qqLNisA6YJeROx5BxL6pqv9Q@mail.gmail.com>
 <0577240a-6b02-0bd7-2e0d-2e58b5bb7b77@treenet.co.nz>
 <CA+sSnVZYcQkvA=-bBVSuQtoXeYxomL5Ca51cw4ax0dP3EUKF7g@mail.gmail.com>
 <5990ba78-39ea-569a-29dd-409e4912f1a6@treenet.co.nz>
 <010401d259f9$4dfcd630$e9f68290$@ngtech.co.il>
 <CA+sSnVavKQR80XNvykdecMeB1feLwBMEP29Vm+Lxp3=U8-S1dg@mail.gmail.com>
 <CA+sSnVbob7T03YvK8RCHmd4o=9Ej8NZ6vhmgKj+h75hnB1EXdQ@mail.gmail.com>
 <b858d071-bc6c-1789-c72b-d1b9343b32e5@measurement-factory.com>
 <CA+sSnVYOZXznoDpAR4pA_-=xNVCQaqPxLbHPs3FdfXa4oduEcA@mail.gmail.com>
 <1340f864-8473-e44a-c580-49b478f5fc1c@measurement-factory.com>
 <CA+sSnVYuZBxnMEWPjOxcFX6H5917fxcZzpLMK6GndVo6d24pPw@mail.gmail.com>
Message-ID: <e58e8fa47a613405aba3b8437fc4b26b@treenet.co.nz>

On 2017-01-02 19:46, Hardik Dangar wrote:
> @amos or anyone else from dev team
> 
> Can you confirm this is intentional behavior or bug ?
> 

Alex is core dev team, and one of the designers of SSL-Bump. So if he 
says bug, it probably is.
<http://wiki.squid-cache.org/WhoWeAre>

> On Mon, Jan 2, 2017 at 9:18 AM, Alex Rousskov wrote:
> 
>> On 12/27/2016 04:50 AM, Hardik Dangar wrote:
>> 
>>> If i remove !serverIsws somehow websockets will not work.
>> 
>> Then there is a bug somewhere AFAICT. It is your call whether to
>> find
>> out what that bug is [while continuing to use a potentially
>> dangerous
>> workaround].
>> 
>> Alex.
>> 

FWIW I agree with him based on the info given. Removal of !serverIsws 
from the rules you had should have had zero effect on the visible 
behaviour of Squid.

Amos


From squid3 at treenet.co.nz  Tue Jan  3 04:59:33 2017
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Tue, 03 Jan 2017 17:59:33 +1300
Subject: [squid-users] Squid 3.5.23-1 is available for Ubuntu 16.04 LTS
	(online repo ubuntu16.diladele.com)
In-Reply-To: <1483374324362-4681025.post@n4.nabble.com>
References: <DB6PR0401MB26808172DDFE4A59040671378F930@DB6PR0401MB2680.eurprd04.prod.outlook.com>
 <1483374324362-4681025.post@n4.nabble.com>
Message-ID: <70e504824297a0221547b54a830962a4@treenet.co.nz>

On 2017-01-03 05:25, vinay wrote:
> Rafael,
> Am facing below 2 issues.
> 
> 1.I am working on squid 3.3.8 on ubuntu 14.04 , I am trying to upgrade 
> the
> squid version to higher than the default one I.e higher than 3.4 
> version but
> am getting an error. "Unable to locate package libecap3" on giving 
> apt-get
> install libecap3 command. Any suggestions to upgrade the version ?

The eCAP package needs to be rebuilt from the libecap3 source package 
from Xenial.

Or just upgrade to Xenial.


> 2. In the existing version of squid 3.3.8 am getting the below lines in
> access logs
> "TCP_MISS/200" and it's not caching .
> Any suggestions on this to overcome the caching issue?
> 

Can you provide some actual details about what you think the problem is?

There are things which are simply non-cacheable, and things which should 
never be cached, and things for which a cached response is not possible.

Amos



From squid3 at treenet.co.nz  Tue Jan  3 08:07:57 2017
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Tue, 03 Jan 2017 21:07:57 +1300
Subject: [squid-users] Squid 3.5.22 Bug when using Mimetype Detection?
	rep_mime_type
In-Reply-To: <2c3cfc7bfc490465ee71dee437ac50d4@data-core.org>
References: <2c3cfc7bfc490465ee71dee437ac50d4@data-core.org>
Message-ID: <65faf969c6c36b178491b5f319db2e40@treenet.co.nz>

On 2017-01-03 07:33, Flashdown wrote:
> Hello together,
> 
> with Squid 3.5.22 I have switched from using a url-regex to Mime Type
> Detection, which seemed to work nicely until now... :/
> 

The thing to be very wary of with this change is that when reply 
blocking the request does still make it through to the server and any 
processing done there still happens.

With things like ticketing systems that means the tickets and any auth 
related token creating has actually happened, even if the client is 
prevented from being told what the created token was.

That is a major difference from URL based blocking, which would reject 
before any server contact.


> OS: Debian Stretch 4.8.0-1-amd64 #1 SMP Debian 4.8.7-1 (2016-11-13)
> x86_64 GNU/Linux
> 


I really advise going to 3.5.23. Several major issues about sending 
wrong replies on certain occasions were fixed there. New .deb should be 
available by now to fix that.



> I faced the following Situation:
> 
> When I globally deny specific mimetypes using a blockfile, then it
> performs as it should, so only mime types I defined in the block file
> are getting blocked, so far so good.
> 
> When I do an exception for a group I belong to like unblocking
> application/octet-stream, then I can download files, so the exception
> works in the first place.
> acl mime_IT rep_mime_type application/octet-stream
> http_reply_access allow IT mime_IT
> 
> Normally internal targets are excluded from the Proxy using Proxy
> Exception lists. But I do not get these settings automatically, so my
> browser did not contain this exception so I was able to discover the
> following behavior:
> 
> The Issue is occuring when browsing to an internal OTRS Web Server via
> FQDN (It's a web ticket system) through the proxy I get "Access
> Denied" from the Proxy on all requests. But when browsing to an online
> OTRS Demo site with the same OTRS version like this one:
> http://itsm-demo.otrs.com/otrs/index.pl then it works. When I now try
> again to access the internal OTRS Server through the proxy it works.
> That's strange, when I now force reload (CTRL+F5 in Firefox) the
> internal OTRS Ticketsystems webpage, I get the "access denied" again.
> 

How is the auth happening between the users and the proxy to find out 
what the groups are?
  The reply checks may be interferring with the auth replies.


> When I remove the exception from the global block list for the group I
> belong to,- here it's IT- then this issue does not occur and the
> website is accessible like it should.
> So I just need to comment out these lines:
> #acl mime_IT rep_mime_type application/octet-stream
> #http_reply_access allow IT mime_IT
> 
> When I add text/html & application/xml to the global block exception,
> then this error does not occur anymore.
> acl mime_IT rep_mime_type application/octet-stream text/html 
> application/xml
> http_reply_access allow IT mime_IT
> 
> So currently I can workaround the issue in 3 different ways:
> 1. Do not create a global mimetype block exception for groups I belong 
> to
> 2. Browse to the start page of an Online Demo OTRS Site and then
> reload the internal Website
> 3. Add text/html & application/xml to my exception even if these
> Mimetypes are not part of the global block list, so they are not
> supposed to be blocked. (I just looked at the internal website and it
> just uses text/html and application/xml on the start page (Login Page)
> so I added them to the exception list for my group and it worked)
> 
> Conclusion: When having a global mime type block and unblocking a
> specific mime type for a specific group, then this group will most
> propably face issues with mime types that are not supposed to be
> blocked. So in case of errors, I need to unblock not blocked mimetypes
> ,too.
> 
> 
> My Squid config for mime type blocking:
> ---------------------------------------
> ## Define Default MIMETYPE ERROR Message and global block access list
> acl block_mimetypes rep_mime_type "/etc/squid/mimetype_blacklist.acl"
> deny_info ERR_BLOCKED_FILES block_mimetypes
> 
> # Configure Execptions
> acl mime_IT rep_mime_type application/octet-stream
> http_reply_access allow IT mime_IT
> 
> acl mime_SpecialGroup rep_mime_type application/octet-stream
> http_reply_access allow SpecialGroup mime_SpecialGroup
> 
> 
> #Applying Global MimeType Block
> http_reply_access deny block_mimetypes

Any other http_reply_access rules? the implicit-default behaviour may be 
having an effect if there are.


> ---------------------------------------
> Squid main config:
> ---------------------------------------
> http_port 8080 ssl-bump generate-host-certificates=on
> dynamic_cert_mem_cache_size=16MB cert=/*********************
> ssl_bump splice localhost
> ssl_bump splice SSL_Exclude

These splice rules may be related to the issue. Any connection that gets 
spliced may be used by other requests without squid being aware of them.

> ssl_bump bump all
> sslproxy_cert_error allow SSL_TrustedSites
> sslproxy_cert_error deny all
> ---------------------------------------
> 
> 
> 
> Contents of mimetype_blacklist.acl:
> ---------------------------------------

That looks okay to me, just remember that it is a regex pattern. So if a 
type entered there matches a sub-string it can block unexpectedly. That 
includes sub-strings in the type parameters.


> 1483381150.095    186 10.3.101.23 TCP_DENIED_REPLY/403 4493 GET
> http://otrs-server.**.**.com/otrs/index.pl - HIER_DIRECT/10.2.1.107
> text/html
> 1483381150.118      3 10.3.101.23 TCP_DENIED_REPLY/403 4451 GET
> http://*****-proxy.**.**.com:8080/squid-internal-static/icons/SN.png -
> HIER_NONE/- text/html

FWIW: the text/html is the type of the HTML error page being delivered 
for the 403 response. The content-type for the original response payload 
which was blocked is not logged.

You can debug with "debug_options 11,2" to get a cache.log trace of what 
messages are happening and what the original server response headers 
were.

Amos



From squid3 at treenet.co.nz  Tue Jan  3 08:47:14 2017
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Tue, 03 Jan 2017 21:47:14 +1300
Subject: [squid-users] Squid Websocket Issue
In-Reply-To: <CA+sSnVYCk_q=YR10zaVs1oHq_usPJrrZuiRnHoDEHx_mV7dCXQ@mail.gmail.com>
References: <CA+sSnVZo1250omWkLDzPpknEa-qqLNisA6YJeROx5BxL6pqv9Q@mail.gmail.com>
 <0577240a-6b02-0bd7-2e0d-2e58b5bb7b77@treenet.co.nz>
 <CA+sSnVZYcQkvA=-bBVSuQtoXeYxomL5Ca51cw4ax0dP3EUKF7g@mail.gmail.com>
 <5990ba78-39ea-569a-29dd-409e4912f1a6@treenet.co.nz>
 <010401d259f9$4dfcd630$e9f68290$@ngtech.co.il>
 <CA+sSnVavKQR80XNvykdecMeB1feLwBMEP29Vm+Lxp3=U8-S1dg@mail.gmail.com>
 <CA+sSnVbob7T03YvK8RCHmd4o=9Ej8NZ6vhmgKj+h75hnB1EXdQ@mail.gmail.com>
 <b858d071-bc6c-1789-c72b-d1b9343b32e5@measurement-factory.com>
 <CA+sSnVYOZXznoDpAR4pA_-=xNVCQaqPxLbHPs3FdfXa4oduEcA@mail.gmail.com>
 <1340f864-8473-e44a-c580-49b478f5fc1c@measurement-factory.com>
 <CA+sSnVYuZBxnMEWPjOxcFX6H5917fxcZzpLMK6GndVo6d24pPw@mail.gmail.com>
 <012a01d264fe$bd3120f0$379362d0$@ngtech.co.il>
 <CA+sSnVYCk_q=YR10zaVs1oHq_usPJrrZuiRnHoDEHx_mV7dCXQ@mail.gmail.com>
Message-ID: <62f2172bf3a18cc1f2effca6744ba3ce@treenet.co.nz>

On 2017-01-03 05:49, Hardik Dangar wrote:
> Hey Eliezer,
> 
> The issue was with whatsapp web socket was not working, here is
> detailed information about issue
> ------------
> 
> Here is some information about my squid version,
> 
> Squid Cache: Version 3.5.22-20161115-r14113
> Service Name: squid
> configure options:  '--prefix=/usr' '--localstatedir=/var/squid'
> '--libexecdir=/lib/squid' '--srcdir=.' '--datadir=/share/squid'
> '--sysconfdir=/etc/squid' '--with-default-user=proxy'
> '--with-logdir=/var/log/squid' '--with-pidfile=/var/run/squid.pid'
> '--with-openssl' '--enable-ssl-crtd' '--enable-inline'
> '--disable-arch-native' '--enable-async-io=8'
> '--enable-storeio=ufs,aufs,diskd,rock'
> '--enable-removal-policies=lru,heap' '--enable-delay-pools'
> '--enable-follow-x-forwarded-for' '--enable-url-rewrite-helpers=fake'
> '--enable-ecap'
> 
> My squid config file is located at, http://pastebin.com/raw/LvDxEF4x
> 
> Now the issue is whenever someone requests a page which contains web
> socket requests response is always bad request.
> Here is an example,
> 
> Request URL:wss://w4.web.whatsapp.com/ws [1]
> Request Method:GET
> Status Code:400 Bad Request
> 
> Response Headers
> #################
> Connection:keep-alive
> Date:Sat, 17 Dec 2016 09:05:36 GMT
> Transfer-Encoding:chunked
> X-Cache:MISS from Proxy
> 
> Request Headers
> #################
> Accept-Encoding:gzip, deflate, sdch, br
> Accept-Language:en-US,en;q=0.8
> Cache-Control:no-cache
> Connection:Upgrade
> Host:w4.web.whatsapp.com [2]
> Origin:https://web.whatsapp.com
> Pragma:no-cache
> Sec-WebSocket-Extensions:permessage-deflate; client_max_window_bits
> Sec-WebSocket-Key:kzrB2ZcMHDAqvjDNXnjL/w==
> Sec-WebSocket-Version:13
> Upgrade:websocket
> User-Agent:Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML,
> like Gecko) Chrome/55.0.2883.75 Safari/537.36
> 
> My question is how we can work with web socket requests in squid or if
> not by pass them squid. My squid instance is in interception mode and
> requests are intercepted at instance via iptables and forwarded to
> squid using below rules,

WebSockets has several ways to initiate connections:

1) native WebSocket port
  - requires any intermediary has native Websocket support

2) CONNECT tunnel (maybe with Upgrade)
  - requires only that the intermediary has HTTP CONNECT support

3) GET request with Upgrade
  - requires that the intermediary has both HTTP Upgrade and WebSockets 
native support


Squid supports method #2.

The transaction you are showing as broken is mechanism #3. Both the 
Upgrade and WebSockets support requirements are not existing in Squid.

That particular request going through a non-WebSockets proxy is supposed 
to be passed to the server as a regular GET request (without Upgrade). 
Exactly what Squid does. The server itself is producing the 400 response 
instead of using alternative things like HTTP long-polling in place of 
WebSockets. That is its choice, nothing to do with Squid and a perfectly 
valid choice on the servers part.

If WhatsApp tried the supported #2 mechanism it would work through 
Squid.


> 
> ----------------------------------------------------------
> 
> SOLUTION TO ABOVE PROBLEM WAS,
> 

... to splice instead of bump.

> 
> So basically websockets are connected as normal https request( i think
> this is a very nature of Web sockets and define somewhere in web
> socket standards).

No, that is the nature of HTTPS when not being bump'ed. Squid is not 
involved in the encrypted request when splicing. So it does not matter 
if it really is HTTPS or actually encrypted WebSockets.

The HTTPS goes through Squid as a CONNECT tunnel. Which for WebSockets 
is connection method #2 above - which is supported and works.


I think your problem is that you are expecting unrealistic things from 
decrypted HTTPS. *HTTPS* being the critical word there. Bumping decrypts 
HTTPS, it does not decrypt other protocols - including WebSockets.

Since the WhatsApp traffic is/was intercepted the client software has no 
knowledge of the proxy existing so it is not realistic to expect it to 
attempt the CONNECT method, even as a failover.

Amos



From fuckspam at wheres5.com  Tue Jan  3 09:17:54 2017
From: fuckspam at wheres5.com (Hoggins!)
Date: Tue, 3 Jan 2017 10:17:54 +0100
Subject: [squid-users] Intercept mode failing
Message-ID: <fc527d12-bb42-0621-6931-36152849bb94@wheres5.com>

Hello list,

I'm trying to do a simple intercept with Squid. Here is my setup :

I have a LAN with machines on 192.168.22.0/24. Their gateway is
192.168.22.10. On this machine, I have set the following iptables rule :

    iptables -t nat -A PREROUTING -i eth0.100 ! -d 192.168.0.0/16 -p tcp
--dport 80 -j DNAT --to 192.168.55.3:3129

    - eth0.100 because it's on a VLAN
    - 192.168.55.3 being the Squid server, directly connected to the
Internet, on a network my gateway has the routes for

On the Squid server (192.168.55.3), I have configured the following
options in squid.conf :

    - (default localnet ACLs were fine, as well as Safe_ports setting)
    - tcp_outgoing_address 1.2.3.4 (the public address the server is
attached to. There are several interfaces)
    - http_port 3129 intercept
    - http_access allow localnet
    - http_access allow localhost
    - http_access deny all

Now, if I issue a curl http://google.fr on a LAN machine
(192.168.22.129), I get the Squid error page saying "Acces Denied", and
the Squid server log shows the following :

    1483434892.803      0 1.2.3.4 TCP_DENIED/403 4032 GET
http://google.fr/ - HIER_NONE/- text/html
    1483434892.804     17 192.168.22.129 TCP_MISS/403 4146 GET
http://google.fr/ - ORIGINAL_DST/192.168.55.3 text/html


"Normal" proxying works fine with this Squid setup (I also have a
"http_port 3128" with no option, and explicitly setting the proxy
address on the LAN hosts works fine).

Do you have an idea of what are my mistakes ?

Thank you for your inputs !

    Hoggins!

-------------- next part --------------
A non-text attachment was scrubbed...
Name: signature.asc
Type: application/pgp-signature
Size: 181 bytes
Desc: OpenPGP digital signature
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20170103/db72d1da/attachment.sig>

From Antony.Stone at squid.open.source.it  Tue Jan  3 09:33:53 2017
From: Antony.Stone at squid.open.source.it (Antony Stone)
Date: Tue, 3 Jan 2017 10:33:53 +0100
Subject: [squid-users] Intercept mode failing
In-Reply-To: <fc527d12-bb42-0621-6931-36152849bb94@wheres5.com>
References: <fc527d12-bb42-0621-6931-36152849bb94@wheres5.com>
Message-ID: <201701031033.53399.Antony.Stone@squid.open.source.it>

On Tuesday 03 January 2017 at 10:17:54, Hoggins! wrote:

> Hello list,
> 
> I'm trying to do a simple intercept with Squid. Here is my setup :
> 
> I have a LAN with machines on 192.168.22.0/24. Their gateway is
> 192.168.22.10. On this machine, I have set the following iptables rule :
> 
>     iptables -t nat -A PREROUTING -i eth0.100 ! -d 192.168.0.0/16 -p tcp
> --dport 80 -j DNAT --to 192.168.55.3:3129
> 
>     - 192.168.55.3 being the Squid server

No - you must do the NAT (or REDIRECT) rule *on the Squid server*.

If you need to use policy routing to get the packets to the Squid machine in 
the first place, that's okay, but this *must* be packet routing, not address 
translation.

See http://wiki.squid-cache.org/ConfigExamples/Intercept/LinuxDnat 
http://wiki.squid-cache.org/ConfigExamples/Intercept/LinuxRedirect and 
http://wiki.squid-cache.org/ConfigExamples/Intercept/IptablesPolicyRoute


Antony.

-- 
In Heaven, the beer is Belgian, the chefs are Italian, the supermarkets are 
British, the mechanics are German, the lovers are French, the entertainment is 
American, and everything is organised by the Swiss.

In Hell, the beer is American, the chefs are British, the supermarkets are 
German, the mechanics are French, the lovers are Swiss, the entertainment is 
Belgian, and everything is organised by the Italians.

                                                   Please reply to the list;
                                                         please *don't* CC me.


From fuckspam at wheres5.com  Tue Jan  3 10:13:33 2017
From: fuckspam at wheres5.com (Hoggins!)
Date: Tue, 3 Jan 2017 11:13:33 +0100
Subject: [squid-users] Intercept mode failing
In-Reply-To: <201701031033.53399.Antony.Stone@squid.open.source.it>
References: <fc527d12-bb42-0621-6931-36152849bb94@wheres5.com>
 <201701031033.53399.Antony.Stone@squid.open.source.it>
Message-ID: <1ed37ea1-36d7-3b81-866b-ebf5a7f2437d@wheres5.com>

Okay, I get that.

Le 03/01/2017 ? 10:33, Antony Stone a ?crit :
> No - you must do the NAT (or REDIRECT) rule *on the Squid server*.

Well, my Squid server is not on the same network as my clients, so I
need something else than just a REDIRECT on the Squid itself.

>
> If you need to use policy routing to get the packets to the Squid machine in 
> the first place, that's okay, but this *must* be packet routing, not address 
> translation

Policy routing was my first choice, but there is one important detail in
my setup : between my gateway (192.168.22.10) and my Squid
(192.168.55.3), there's an IPSec tunnel. My gateway does not have a
link-local route to 192.168.55.3 so I can't add the default route to it
inside a routing table (I get "Network is unreachable", which is expected).

So I guess I'm stuck.

Thanks anyway !

    Hoggins!

-------------- next part --------------
A non-text attachment was scrubbed...
Name: signature.asc
Type: application/pgp-signature
Size: 181 bytes
Desc: OpenPGP digital signature
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20170103/70dafcd7/attachment.sig>

From omidkosari at yahoo.com  Tue Jan  3 10:02:44 2017
From: omidkosari at yahoo.com (Omid Kosari)
Date: Tue, 3 Jan 2017 02:02:44 -0800 (PST)
Subject: [squid-users] squid tproxy connection time out
In-Reply-To: <CA+W7yKeqqG6p=-8VACH2y7CsvT_pz8jrc3Bsdk7=+koxX+L--Q@mail.gmail.com>
References: <CA+W7yKeqqG6p=-8VACH2y7CsvT_pz8jrc3Bsdk7=+koxX+L--Q@mail.gmail.com>
Message-ID: <1483437764025-4681037.post@n4.nabble.com>

Hello,

I think your problem is topology . I suggest change the position of squid so
the mikrotik router stands between clients and squid box .

Also assign a private ip address to your squid and also one ip from same
range to your mikrotik router . Then try to mangle and route to that private
ip .





--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/squid-tproxy-connection-time-out-tp4681027p4681037.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From Antony.Stone at squid.open.source.it  Tue Jan  3 10:39:38 2017
From: Antony.Stone at squid.open.source.it (Antony Stone)
Date: Tue, 3 Jan 2017 11:39:38 +0100
Subject: [squid-users] Intercept mode failing
In-Reply-To: <1ed37ea1-36d7-3b81-866b-ebf5a7f2437d@wheres5.com>
References: <fc527d12-bb42-0621-6931-36152849bb94@wheres5.com>
 <201701031033.53399.Antony.Stone@squid.open.source.it>
 <1ed37ea1-36d7-3b81-866b-ebf5a7f2437d@wheres5.com>
Message-ID: <201701031139.38364.Antony.Stone@squid.open.source.it>

On Tuesday 03 January 2017 at 11:13:33, Hoggins! wrote:

> Okay, I get that.
> 
> Le 03/01/2017 ? 10:33, Antony Stone a ?crit :
> > No - you must do the NAT (or REDIRECT) rule *on the Squid server*.
> 
> Well, my Squid server is not on the same network as my clients, so I
> need something else than just a REDIRECT on the Squid itself.

I'm not sure you fully understand what REDIRECT does.  It changes the 
destination address of the packets which *were* going to random web servers 
around the Internet, and have now reached your Squid box, so thatthey go to 
the local address of your Squid machien instead (and therefore Squid can see 
them and process them).

> > If you need to use policy routing to get the packets to the Squid machine
> > in the first place, that's okay, but this *must* be packet routing, not
> > address translation
> 
> Policy routing was my first choice, but there is one important detail in
> my setup : between my gateway (192.168.22.10) and my Squid
> (192.168.55.3), there's an IPSec tunnel. My gateway does not have a
> link-local route to 192.168.55.3 so I can't add the default route to it
> inside a routing table (I get "Network is unreachable", which is expected).

So, if you can't route packets from the gateway to Squid, how was your NAT 
setup getting them there?

You said in your original posting: "192.168.55.3 being the Squid server, 
directly connected to the Internet, on a network my gateway has the routes 
for", suggesting that your gateway *can* route to the Squid server.

> So I guess I'm stuck.

Maybe you need to do policy routing on the gateway to the IPsec endpoint, and 
then further routing from there to Squid?


Antony.

-- 
"Remember: the S in IoT stands for Security."

 - Jan-Piet Mens

                                                   Please reply to the list;
                                                         please *don't* CC me.


From squid3 at treenet.co.nz  Tue Jan  3 10:45:00 2017
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Tue, 03 Jan 2017 23:45:00 +1300
Subject: [squid-users] Intercept mode failing
In-Reply-To: <1ed37ea1-36d7-3b81-866b-ebf5a7f2437d@wheres5.com>
References: <fc527d12-bb42-0621-6931-36152849bb94@wheres5.com>
 <201701031033.53399.Antony.Stone@squid.open.source.it>
 <1ed37ea1-36d7-3b81-866b-ebf5a7f2437d@wheres5.com>
Message-ID: <546f46b7bc43e3127fcdaa4e97b0c73d@treenet.co.nz>

On 2017-01-03 23:13, Hoggins! wrote:
> Okay, I get that.
> 
> Le 03/01/2017 ? 10:33, Antony Stone a ?crit :
>> No - you must do the NAT (or REDIRECT) rule *on the Squid server*.
> 
> Well, my Squid server is not on the same network as my clients, so I
> need something else than just a REDIRECT on the Squid itself.

That does not matter when the DNAT or REDIRECT is done on the Squid 
machine.

> 
>> 
>> If you need to use policy routing to get the packets to the Squid 
>> machine in
>> the first place, that's okay, but this *must* be packet routing, not 
>> address
>> translation
> 
> Policy routing was my first choice, but there is one important detail 
> in
> my setup : between my gateway (192.168.22.10) and my Squid
> (192.168.55.3), there's an IPSec tunnel. My gateway does not have a
> link-local route to 192.168.55.3 so I can't add the default route to it
> inside a routing table (I get "Network is unreachable", which is 
> expected).
> 
> So I guess I'm stuck.


So how did the packets get to the Squid machine after your DNAT ?

The route does not have to be link-local. Any type of route will do so 
long as all the routers handling the packets know which way to pass 
them, and the dst-IP address is not changed.

Amos



From mrghorbani2002 at gmail.com  Tue Jan  3 10:33:26 2017
From: mrghorbani2002 at gmail.com (mrghorbani)
Date: Tue, 3 Jan 2017 02:33:26 -0800 (PST)
Subject: [squid-users] squid tproxy connection time out
In-Reply-To: <1483437764025-4681037.post@n4.nabble.com>
References: <CA+W7yKeqqG6p=-8VACH2y7CsvT_pz8jrc3Bsdk7=+koxX+L--Q@mail.gmail.com>
 <1483437764025-4681037.post@n4.nabble.com>
Message-ID: <1483439606728-4681040.post@n4.nabble.com>

hello,
i had created the topology diagram as i get from your idea, does it that you
mentioned?
but, according to that my bgp and wireless points are connected to mikrotik
router, i can not move squid to the end point...in this network, now and in
exists network, i routed the client to the mikrotik  as gw and then routed
him by mikrotik to squid so, now, mikrotik is between the client and squid
but in virtuality
<http://squid-web-proxy-cache.1019090.n4.nabble.com/file/n4681040/Drawing1.jpg> 



--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/squid-tproxy-connection-time-out-tp4681027p4681040.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From eduardoaguiarcasanova at gmail.com  Tue Jan  3 10:36:06 2017
From: eduardoaguiarcasanova at gmail.com (hitman13)
Date: Tue, 3 Jan 2017 02:36:06 -0800 (PST)
Subject: [squid-users] Squid3 configure to force access to the facebook by
	the proxy
Message-ID: <1483439766174-4681041.post@n4.nabble.com>

Hi guys, I'm new to linux
And I need to configure squid3 on UBUNTU SERVER 16.04

But I need to force facebook access through the proxy
And it has multi-ports, I actually need many
EX:
52.xx.6.214: 3128
52.xx.6.214: 3129
52.xx.6.214: 3130
52.xx.6.214: 9999 as much as possible

So that when you need access by the mozilla can always pick up a new port

Thank you very much in advance.



--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/Squid3-configure-to-force-access-to-the-facebook-by-the-proxy-tp4681041.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From fuckspam at wheres5.com  Tue Jan  3 10:53:45 2017
From: fuckspam at wheres5.com (Hoggins!)
Date: Tue, 3 Jan 2017 11:53:45 +0100
Subject: [squid-users] Intercept mode failing
In-Reply-To: <546f46b7bc43e3127fcdaa4e97b0c73d@treenet.co.nz>
References: <fc527d12-bb42-0621-6931-36152849bb94@wheres5.com>
 <201701031033.53399.Antony.Stone@squid.open.source.it>
 <1ed37ea1-36d7-3b81-866b-ebf5a7f2437d@wheres5.com>
 <546f46b7bc43e3127fcdaa4e97b0c73d@treenet.co.nz>
Message-ID: <4ff88780-7487-6fc4-ba93-8214e3532aa7@wheres5.com>

Hello,

(answering to both Amos and Antony here, you got the same questioning ;) )

Le 03/01/2017 ? 11:45, Amos Jeffries a ?crit :
> On 2017-01-03 23:13, Hoggins! wrote:
>> Okay, I get that.
>>
>> Le 03/01/2017 ? 10:33, Antony Stone a ?crit :
>>> No - you must do the NAT (or REDIRECT) rule *on the Squid server*.
>>
>> Well, my Squid server is not on the same network as my clients, so I
>> need something else than just a REDIRECT on the Squid itself.
>
> That does not matter when the DNAT or REDIRECT is done on the Squid
> machine.

OK, I'll have a deeper look into that, indeed I'm not familiar with what
REDIRECT *exactly* does.

>
>>
>>>
>>> If you need to use policy routing to get the packets to the Squid
>>> machine in
>>> the first place, that's okay, but this *must* be packet routing, not
>>> address
>>> translation
>>
>> Policy routing was my first choice, but there is one important detail in
>> my setup : between my gateway (192.168.22.10) and my Squid
>> (192.168.55.3), there's an IPSec tunnel. My gateway does not have a
>> link-local route to 192.168.55.3 so I can't add the default route to it
>> inside a routing table (I get "Network is unreachable", which is
>> expected).
>>
>> So I guess I'm stuck.
>
>
> So how did the packets get to the Squid machine after your DNAT ?
>
> The route does not have to be link-local. Any type of route will do so
> long as all the routers handling the packets know which way to pass
> them, and the dst-IP address is not changed.

Well, xfrm routing is a lot different than "classic" routing, I learnt
it the hard way. DNAT *will* work whereas policy routing won't if I
don't explicitly declare all my subnets in my IPSec tunnel
configuration. Got a big discussion about that on StrongSwan's
mailing-list, and I believe this sums it up pretty nicely :
http://xkr47.outerspace.dyndns.org/netfilter/packet_flow/packet_flow9.png

Anyway, yes, if I try to add a route by :
    ip route add default via <IP ADDRESS> table 123

<IP ADDRESS> *has* to be directly reachable. Or it has to be in the
routing table somehow. But the routing table handling the tunnelled
packets is not managed by iproute2.

So as I can't do otherwise, I'm going to experiment a bit more with the
REDIRECT + DNAT between the gateway and the Squid server.

Thanks for your help !

>
> Amos
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>


-------------- next part --------------
A non-text attachment was scrubbed...
Name: signature.asc
Type: application/pgp-signature
Size: 181 bytes
Desc: OpenPGP digital signature
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20170103/de86e190/attachment.sig>

From hardikdangar+squid at gmail.com  Tue Jan  3 10:54:20 2017
From: hardikdangar+squid at gmail.com (Hardik Dangar)
Date: Tue, 3 Jan 2017 16:24:20 +0530
Subject: [squid-users] Squid3 configure to force access to the facebook
 by the proxy
In-Reply-To: <1483439766174-4681041.post@n4.nabble.com>
References: <1483439766174-4681041.post@n4.nabble.com>
Message-ID: <CA+sSnVZtQB9N5n4g7b+ivJBF8jgLmNT=ONZMz+r73fbYgiPA9g@mail.gmail.com>

I am not sure why you would need that many ports. if you think each port is
for each user or some other misconcept then let me tell you, you can route
all your hosts and traffic from single port only.

On Tue, Jan 3, 2017 at 4:06 PM, hitman13 <eduardoaguiarcasanova at gmail.com>
wrote:

> Hi guys, I'm new to linux
> And I need to configure squid3 on UBUNTU SERVER 16.04
>
> But I need to force facebook access through the proxy
> And it has multi-ports, I actually need many
> EX:
> 52.xx.6.214: 3128
> 52.xx.6.214: 3129
> 52.xx.6.214: 3130
> 52.xx.6.214: 9999 as much as possible
>
> So that when you need access by the mozilla can always pick up a new port
>
> Thank you very much in advance.
>
>
>
> --
> View this message in context: http://squid-web-proxy-cache.
> 1019090.n4.nabble.com/Squid3-configure-to-force-access-to-
> the-facebook-by-the-proxy-tp4681041.html
> Sent from the Squid - Users mailing list archive at Nabble.com.
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20170103/dd9b5196/attachment.htm>

From mrghorbani2002 at gmail.com  Tue Jan  3 10:53:01 2017
From: mrghorbani2002 at gmail.com (mrghorbani)
Date: Tue, 3 Jan 2017 02:53:01 -0800 (PST)
Subject: [squid-users] squid tproxy connection time out
In-Reply-To: <1483439606728-4681040.post@n4.nabble.com>
References: <CA+W7yKeqqG6p=-8VACH2y7CsvT_pz8jrc3Bsdk7=+koxX+L--Q@mail.gmail.com>
 <1483437764025-4681037.post@n4.nabble.com>
 <1483439606728-4681040.post@n4.nabble.com>
Message-ID: <1483440781802-4681044.post@n4.nabble.com>

also what about this topology?
<http://squid-web-proxy-cache.1019090.n4.nabble.com/file/n4681044/Drawing2.jpg> 



--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/squid-tproxy-connection-time-out-tp4681027p4681044.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From omidkosari at yahoo.com  Tue Jan  3 10:59:03 2017
From: omidkosari at yahoo.com (Omid Kosari)
Date: Tue, 3 Jan 2017 02:59:03 -0800 (PST)
Subject: [squid-users] squid tproxy connection time out
In-Reply-To: <1483439606728-4681040.post@n4.nabble.com>
References: <CA+W7yKeqqG6p=-8VACH2y7CsvT_pz8jrc3Bsdk7=+koxX+L--Q@mail.gmail.com>
 <1483437764025-4681037.post@n4.nabble.com>
 <1483439606728-4681040.post@n4.nabble.com>
Message-ID: <1483441143783-4681045.post@n4.nabble.com>

No it is not what i mentioned.

Your mikrotik router should be for example
ether1 -> Internet
ether2 -> Clients
ether3 -> Squid

So the mikrotik is between clients and squid .



--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/squid-tproxy-connection-time-out-tp4681027p4681045.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From squid3 at treenet.co.nz  Tue Jan  3 11:39:38 2017
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Wed, 04 Jan 2017 00:39:38 +1300
Subject: [squid-users] Intercept mode failing
In-Reply-To: <4ff88780-7487-6fc4-ba93-8214e3532aa7@wheres5.com>
References: <fc527d12-bb42-0621-6931-36152849bb94@wheres5.com>
 <201701031033.53399.Antony.Stone@squid.open.source.it>
 <1ed37ea1-36d7-3b81-866b-ebf5a7f2437d@wheres5.com>
 <546f46b7bc43e3127fcdaa4e97b0c73d@treenet.co.nz>
 <4ff88780-7487-6fc4-ba93-8214e3532aa7@wheres5.com>
Message-ID: <7b1625ee1afa6f7572724198d3c4d28e@treenet.co.nz>

On 2017-01-03 23:53, Hoggins! wrote:
> Hello,
> 
> (answering to both Amos and Antony here, you got the same questioning 
> ;) )
> 
> Le 03/01/2017 ? 11:45, Amos Jeffries a ?crit :
>> On 2017-01-03 23:13, Hoggins! wrote:
>>> Okay, I get that.
>>> 
>>> Le 03/01/2017 ? 10:33, Antony Stone a ?crit :
>>>> No - you must do the NAT (or REDIRECT) rule *on the Squid server*.
>>> 
>>> Well, my Squid server is not on the same network as my clients, so I
>>> need something else than just a REDIRECT on the Squid itself.
>> 
>> That does not matter when the DNAT or REDIRECT is done on the Squid
>> machine.
> 
> OK, I'll have a deeper look into that, indeed I'm not familiar with 
> what
> REDIRECT *exactly* does.
> 

It does the same as DNAT, but using the machines "primary" IP address 
instead of one you configure. Usually that means the first IP assigned 
to eth0 or equivalent first interface.

DNAT is best if you want a specific fixed IP:port for Squid to receive 
on.

REDIRECT is best if you want the proxy to be plug-n-play on any network. 
squid.conf and iptables not needing IP address, just a port number.


>> 
>>> 
>>>> 
>>>> If you need to use policy routing to get the packets to the Squid
>>>> machine in
>>>> the first place, that's okay, but this *must* be packet routing, not
>>>> address
>>>> translation
>>> 
>>> Policy routing was my first choice, but there is one important detail 
>>> in
>>> my setup : between my gateway (192.168.22.10) and my Squid
>>> (192.168.55.3), there's an IPSec tunnel. My gateway does not have a
>>> link-local route to 192.168.55.3 so I can't add the default route to 
>>> it
>>> inside a routing table (I get "Network is unreachable", which is
>>> expected).
>>> 
>>> So I guess I'm stuck.
>> 
>> 
>> So how did the packets get to the Squid machine after your DNAT ?
>> 
>> The route does not have to be link-local. Any type of route will do so
>> long as all the routers handling the packets know which way to pass
>> them, and the dst-IP address is not changed.
> 
> Well, xfrm routing is a lot different than "classic" routing, I learnt
> it the hard way. DNAT *will* work whereas policy routing won't if I
> don't explicitly declare all my subnets in my IPSec tunnel
> configuration. Got a big discussion about that on StrongSwan's
> mailing-list, and I believe this sums it up pretty nicely :
> http://xkr47.outerspace.dyndns.org/netfilter/packet_flow/packet_flow9.png

Looks like a simplified version of the netfilter devs official diagram 
to me. So all the required decision points are present and should be 
configurable.

> 
> Anyway, yes, if I try to add a route by :
>     ip route add default via <IP ADDRESS> table 123
> 
> <IP ADDRESS> *has* to be directly reachable. Or it has to be in the
> routing table somehow. But the routing table handling the tunnelled
> packets is not managed by iproute2.

It should "simply" be one of the tables with a value other than "123". 
But that should not be a consideration as editing it is not necessary. 
This "123" routing is only relevant *before* packets enter the tunnel, 
since it is what tells the OS to send packets to the tunnel.

IIRC the routing table for HTTP traffic (your '123' table) should 
indicate the tunnel as the gateway <IP ADDRESS>. I don't recall whether 
that gw was needing to be the local or the remote tunnel endpoint though 
sorry, been too long since I set one of these up.

> 
> So as I can't do otherwise, I'm going to experiment a bit more with the
> REDIRECT + DNAT between the gateway and the Squid server.
> 

I think you are misinterpreting that diagram in regards to the policy 
routing setup.

What we have on the squid wiki on "Policy Routing" makes packets follow 
this path;

  * the PREROUTING "mangle" table sets marks on packets to be delivered 
to Squid.

  * those packets go through the "FORWARD" to that gateway in 
'123'/'proxy' table.
   (in our wiki that is the "ip rule add fwmark 2 table ..." part).

  * after POSTROUTING the packets destined to a tunnel gateway are 
diverted at that last-minute decision point after "QoS egress" so they 
go back to "OUTPUT" which is where the tunnel related things are done to 
it.

Note that for the machine where the tunnel leads *from*, all that 
matters is routing the packets into the tunnel. Once packets are 
encapsulated by the tunnel they simply go through it to the other end. 
The receiving machine does whatever it needs to after the packets leave 
the tunnel.

HTH
Amos



From eliezer at ngtech.co.il  Tue Jan  3 12:50:27 2017
From: eliezer at ngtech.co.il (Eliezer  Croitoru)
Date: Tue, 3 Jan 2017 14:50:27 +0200
Subject: [squid-users] squid tproxy connection time out
In-Reply-To: <CA+W7yKeqqG6p=-8VACH2y7CsvT_pz8jrc3Bsdk7=+koxX+L--Q@mail.gmail.com>
References: <CA+W7yKeqqG6p=-8VACH2y7CsvT_pz8jrc3Bsdk7=+koxX+L--Q@mail.gmail.com>
Message-ID: <003601d265bf$f567dfb0$e0379f10$@ngtech.co.il>

In terms of Mikrotik this can help you:
http://wiki.bluecrow.net/index.php/Mikrotik_Connection_Tracking
http://wiki.mikrotik.com/wiki/PBR_PTP_IPIP  (needs a bit tweak to work as you expect).

Eliezer

----
Eliezer Croitoru
Linux System Administrator
Mobile: +972-5-28704261
Email: eliezer at ngtech.co.il


-----Original Message-----
From: squid-users [mailto:squid-users-bounces at lists.squid-cache.org] On Behalf Of mr ghorbani
Sent: Monday, January 2, 2017 8:16 PM
To: squid-users at lists.squid-cache.org
Subject: [squid-users] squid tproxy connection time out

hello masters
I have a problem on the squid in tproxy mode and it is that  squid return Error "110 connection the timeout." for all Requests on port 3129, which is related to tproxy Of course, by eliminating the code ip route add local 0.0.0.0/0 dev lo table 100 Problem solved, but in this case, Squid does not record any activity log.

i had attached the network topology.

In this network, 185.12.32.1 is gw for both of the client and squid. it is set to That route the client communication to the squid and then, squid connect it to the internet by using this gw and also redirect port 80 to 3129 and tproxy to cache the data.
The gw is connected to the Internet and also it is the bgp for these IP addresses.



From eliezer at ngtech.co.il  Tue Jan  3 12:53:35 2017
From: eliezer at ngtech.co.il (Eliezer  Croitoru)
Date: Tue, 3 Jan 2017 14:53:35 +0200
Subject: [squid-users] Intercept mode failing
In-Reply-To: <4ff88780-7487-6fc4-ba93-8214e3532aa7@wheres5.com>
References: <fc527d12-bb42-0621-6931-36152849bb94@wheres5.com>
 <201701031033.53399.Antony.Stone@squid.open.source.it>
 <1ed37ea1-36d7-3b81-866b-ebf5a7f2437d@wheres5.com>
 <546f46b7bc43e3127fcdaa4e97b0c73d@treenet.co.nz>
 <4ff88780-7487-6fc4-ba93-8214e3532aa7@wheres5.com>
Message-ID: <003801d265c0$656d8c10$3048a430$@ngtech.co.il>

Hey,

There is also another option.
You can open a tunnel (IPIP, GRE, OTHER) between the proxy and the router to make it possible to directly route traffic to the proxy.

If you need some help with it let me know.

Eliezer 

----
Eliezer Croitoru
Linux System Administrator
Mobile: +972-5-28704261
Email: eliezer at ngtech.co.il


-----Original Message-----
From: squid-users [mailto:squid-users-bounces at lists.squid-cache.org] On Behalf Of Hoggins!
Sent: Tuesday, January 3, 2017 12:54 PM
To: squid-users at lists.squid-cache.org
Subject: Re: [squid-users] Intercept mode failing

Hello,

(answering to both Amos and Antony here, you got the same questioning ;) )

Le 03/01/2017 ? 11:45, Amos Jeffries a ?crit :
> On 2017-01-03 23:13, Hoggins! wrote:
>> Okay, I get that.
>>
>> Le 03/01/2017 ? 10:33, Antony Stone a ?crit :
>>> No - you must do the NAT (or REDIRECT) rule *on the Squid server*.
>>
>> Well, my Squid server is not on the same network as my clients, so I 
>> need something else than just a REDIRECT on the Squid itself.
>
> That does not matter when the DNAT or REDIRECT is done on the Squid 
> machine.

OK, I'll have a deeper look into that, indeed I'm not familiar with what REDIRECT *exactly* does.

>
>>
>>>
>>> If you need to use policy routing to get the packets to the Squid 
>>> machine in the first place, that's okay, but this *must* be packet 
>>> routing, not address translation
>>
>> Policy routing was my first choice, but there is one important detail 
>> in my setup : between my gateway (192.168.22.10) and my Squid 
>> (192.168.55.3), there's an IPSec tunnel. My gateway does not have a 
>> link-local route to 192.168.55.3 so I can't add the default route to 
>> it inside a routing table (I get "Network is unreachable", which is 
>> expected).
>>
>> So I guess I'm stuck.
>
>
> So how did the packets get to the Squid machine after your DNAT ?
>
> The route does not have to be link-local. Any type of route will do so 
> long as all the routers handling the packets know which way to pass 
> them, and the dst-IP address is not changed.

Well, xfrm routing is a lot different than "classic" routing, I learnt it the hard way. DNAT *will* work whereas policy routing won't if I don't explicitly declare all my subnets in my IPSec tunnel configuration. Got a big discussion about that on StrongSwan's mailing-list, and I believe this sums it up pretty nicely :
http://xkr47.outerspace.dyndns.org/netfilter/packet_flow/packet_flow9.png

Anyway, yes, if I try to add a route by :
    ip route add default via <IP ADDRESS> table 123

<IP ADDRESS> *has* to be directly reachable. Or it has to be in the routing table somehow. But the routing table handling the tunnelled packets is not managed by iproute2.

So as I can't do otherwise, I'm going to experiment a bit more with the REDIRECT + DNAT between the gateway and the Squid server.

Thanks for your help !

>
> Amos
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>





From fuckspam at wheres5.com  Tue Jan  3 13:08:36 2017
From: fuckspam at wheres5.com (Hoggins!)
Date: Tue, 3 Jan 2017 14:08:36 +0100
Subject: [squid-users] Intercept mode failing
In-Reply-To: <003801d265c0$656d8c10$3048a430$@ngtech.co.il>
References: <fc527d12-bb42-0621-6931-36152849bb94@wheres5.com>
 <201701031033.53399.Antony.Stone@squid.open.source.it>
 <1ed37ea1-36d7-3b81-866b-ebf5a7f2437d@wheres5.com>
 <546f46b7bc43e3127fcdaa4e97b0c73d@treenet.co.nz>
 <4ff88780-7487-6fc4-ba93-8214e3532aa7@wheres5.com>
 <003801d265c0$656d8c10$3048a430$@ngtech.co.il>
Message-ID: <82d5fdf0-ce06-5191-4955-072db0a89713@wheres5.com>

Ah !

Le 03/01/2017 ? 13:53, Eliezer Croitoru a ?crit :
> Hey,
>
> There is also another option.
> You can open a tunnel (IPIP, GRE, OTHER) between the proxy and the router to make it possible to directly route traffic to the proxy.

That would actually solve a lot of my problems.

>
> If you need some help with it let me know.
>
> Eliezer 
>
> ----
> Eliezer Croitoru
> Linux System Administrator
> Mobile: +972-5-28704261
> Email: eliezer at ngtech.co.il
>
>
> -----Original Message-----
> From: squid-users [mailto:squid-users-bounces at lists.squid-cache.org] On Behalf Of Hoggins!
> Sent: Tuesday, January 3, 2017 12:54 PM
> To: squid-users at lists.squid-cache.org
> Subject: Re: [squid-users] Intercept mode failing
>
> Hello,
>
> (answering to both Amos and Antony here, you got the same questioning ;) )
>
> Le 03/01/2017 ? 11:45, Amos Jeffries a ?crit :
>> On 2017-01-03 23:13, Hoggins! wrote:
>>> Okay, I get that.
>>>
>>> Le 03/01/2017 ? 10:33, Antony Stone a ?crit :
>>>> No - you must do the NAT (or REDIRECT) rule *on the Squid server*.
>>> Well, my Squid server is not on the same network as my clients, so I 
>>> need something else than just a REDIRECT on the Squid itself.
>> That does not matter when the DNAT or REDIRECT is done on the Squid 
>> machine.
> OK, I'll have a deeper look into that, indeed I'm not familiar with what REDIRECT *exactly* does.
>
>>>> If you need to use policy routing to get the packets to the Squid 
>>>> machine in the first place, that's okay, but this *must* be packet 
>>>> routing, not address translation
>>> Policy routing was my first choice, but there is one important detail 
>>> in my setup : between my gateway (192.168.22.10) and my Squid 
>>> (192.168.55.3), there's an IPSec tunnel. My gateway does not have a 
>>> link-local route to 192.168.55.3 so I can't add the default route to 
>>> it inside a routing table (I get "Network is unreachable", which is 
>>> expected).
>>>
>>> So I guess I'm stuck.
>>
>> So how did the packets get to the Squid machine after your DNAT ?
>>
>> The route does not have to be link-local. Any type of route will do so 
>> long as all the routers handling the packets know which way to pass 
>> them, and the dst-IP address is not changed.
> Well, xfrm routing is a lot different than "classic" routing, I learnt it the hard way. DNAT *will* work whereas policy routing won't if I don't explicitly declare all my subnets in my IPSec tunnel configuration. Got a big discussion about that on StrongSwan's mailing-list, and I believe this sums it up pretty nicely :
> http://xkr47.outerspace.dyndns.org/netfilter/packet_flow/packet_flow9.png
>
> Anyway, yes, if I try to add a route by :
>     ip route add default via <IP ADDRESS> table 123
>
> <IP ADDRESS> *has* to be directly reachable. Or it has to be in the routing table somehow. But the routing table handling the tunnelled packets is not managed by iproute2.
>
> So as I can't do otherwise, I'm going to experiment a bit more with the REDIRECT + DNAT between the gateway and the Squid server.
>
> Thanks for your help !
>
>> Amos
>>
>> _______________________________________________
>> squid-users mailing list
>> squid-users at lists.squid-cache.org
>> http://lists.squid-cache.org/listinfo/squid-users
>>
>
>
>


-------------- next part --------------
A non-text attachment was scrubbed...
Name: signature.asc
Type: application/pgp-signature
Size: 181 bytes
Desc: OpenPGP digital signature
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20170103/9ba167a0/attachment.sig>

From fuckspam at wheres5.com  Tue Jan  3 13:13:25 2017
From: fuckspam at wheres5.com (Hoggins!)
Date: Tue, 3 Jan 2017 14:13:25 +0100
Subject: [squid-users] Intercept mode failing
In-Reply-To: <7b1625ee1afa6f7572724198d3c4d28e@treenet.co.nz>
References: <fc527d12-bb42-0621-6931-36152849bb94@wheres5.com>
 <201701031033.53399.Antony.Stone@squid.open.source.it>
 <1ed37ea1-36d7-3b81-866b-ebf5a7f2437d@wheres5.com>
 <546f46b7bc43e3127fcdaa4e97b0c73d@treenet.co.nz>
 <4ff88780-7487-6fc4-ba93-8214e3532aa7@wheres5.com>
 <7b1625ee1afa6f7572724198d3c4d28e@treenet.co.nz>
Message-ID: <ffa6ea99-c97a-5578-ee32-6bfe6324de63@wheres5.com>

Hello Amos,

I believe my main problem is that I'm trying to apply recipes without
understanding some of the internals, so it seems difficult to adapt.
I'll keep searching.

Thanks anyway !

    Hoggins!

Le 03/01/2017 ? 12:39, Amos Jeffries a ?crit :
> On 2017-01-03 23:53, Hoggins! wrote:
>> Hello,
>>
>> (answering to both Amos and Antony here, you got the same questioning
>> ;) )
>>
>> Le 03/01/2017 ? 11:45, Amos Jeffries a ?crit :
>>> On 2017-01-03 23:13, Hoggins! wrote:
>>>> Okay, I get that.
>>>>
>>>> Le 03/01/2017 ? 10:33, Antony Stone a ?crit :
>>>>> No - you must do the NAT (or REDIRECT) rule *on the Squid server*.
>>>>
>>>> Well, my Squid server is not on the same network as my clients, so I
>>>> need something else than just a REDIRECT on the Squid itself.
>>>
>>> That does not matter when the DNAT or REDIRECT is done on the Squid
>>> machine.
>>
>> OK, I'll have a deeper look into that, indeed I'm not familiar with what
>> REDIRECT *exactly* does.
>>
>
> It does the same as DNAT, but using the machines "primary" IP address
> instead of one you configure. Usually that means the first IP assigned
> to eth0 or equivalent first interface.
>
> DNAT is best if you want a specific fixed IP:port for Squid to receive
> on.
>
> REDIRECT is best if you want the proxy to be plug-n-play on any
> network. squid.conf and iptables not needing IP address, just a port
> number.
>
>
>>>
>>>>
>>>>>
>>>>> If you need to use policy routing to get the packets to the Squid
>>>>> machine in
>>>>> the first place, that's okay, but this *must* be packet routing, not
>>>>> address
>>>>> translation
>>>>
>>>> Policy routing was my first choice, but there is one important
>>>> detail in
>>>> my setup : between my gateway (192.168.22.10) and my Squid
>>>> (192.168.55.3), there's an IPSec tunnel. My gateway does not have a
>>>> link-local route to 192.168.55.3 so I can't add the default route
>>>> to it
>>>> inside a routing table (I get "Network is unreachable", which is
>>>> expected).
>>>>
>>>> So I guess I'm stuck.
>>>
>>>
>>> So how did the packets get to the Squid machine after your DNAT ?
>>>
>>> The route does not have to be link-local. Any type of route will do so
>>> long as all the routers handling the packets know which way to pass
>>> them, and the dst-IP address is not changed.
>>
>> Well, xfrm routing is a lot different than "classic" routing, I learnt
>> it the hard way. DNAT *will* work whereas policy routing won't if I
>> don't explicitly declare all my subnets in my IPSec tunnel
>> configuration. Got a big discussion about that on StrongSwan's
>> mailing-list, and I believe this sums it up pretty nicely :
>> http://xkr47.outerspace.dyndns.org/netfilter/packet_flow/packet_flow9.png
>>
>
> Looks like a simplified version of the netfilter devs official diagram
> to me. So all the required decision points are present and should be
> configurable.
>
>>
>> Anyway, yes, if I try to add a route by :
>>     ip route add default via <IP ADDRESS> table 123
>>
>> <IP ADDRESS> *has* to be directly reachable. Or it has to be in the
>> routing table somehow. But the routing table handling the tunnelled
>> packets is not managed by iproute2.
>
> It should "simply" be one of the tables with a value other than "123".
> But that should not be a consideration as editing it is not necessary.
> This "123" routing is only relevant *before* packets enter the tunnel,
> since it is what tells the OS to send packets to the tunnel.
>
> IIRC the routing table for HTTP traffic (your '123' table) should
> indicate the tunnel as the gateway <IP ADDRESS>. I don't recall
> whether that gw was needing to be the local or the remote tunnel
> endpoint though sorry, been too long since I set one of these up.
>
>>
>> So as I can't do otherwise, I'm going to experiment a bit more with the
>> REDIRECT + DNAT between the gateway and the Squid server.
>>
>
> I think you are misinterpreting that diagram in regards to the policy
> routing setup.
>
> What we have on the squid wiki on "Policy Routing" makes packets
> follow this path;
>
>  * the PREROUTING "mangle" table sets marks on packets to be delivered
> to Squid.
>
>  * those packets go through the "FORWARD" to that gateway in
> '123'/'proxy' table.
>   (in our wiki that is the "ip rule add fwmark 2 table ..." part).
>
>  * after POSTROUTING the packets destined to a tunnel gateway are
> diverted at that last-minute decision point after "QoS egress" so they
> go back to "OUTPUT" which is where the tunnel related things are done
> to it.
>
> Note that for the machine where the tunnel leads *from*, all that
> matters is routing the packets into the tunnel. Once packets are
> encapsulated by the tunnel they simply go through it to the other end.
> The receiving machine does whatever it needs to after the packets
> leave the tunnel.
>
> HTH
> Amos
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>


-------------- next part --------------
A non-text attachment was scrubbed...
Name: signature.asc
Type: application/pgp-signature
Size: 181 bytes
Desc: OpenPGP digital signature
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20170103/ebcdcadb/attachment.sig>

From amaury at tin.it  Tue Jan  3 14:14:45 2017
From: amaury at tin.it (amaury at tin.it)
Date: Tue, 3 Jan 2017 15:14:45 +0100 (CET)
Subject: [squid-users] problem authentication ntlm with squid 3.5.21
Message-ID: <15964aea8f2.amaury@tin.it>

Hello
I upgrade squid from 3.4.9-20141203-r13193 to 3.5.21-20160908-
r14081 and I have a problem with authentication to ntlm in a 
transparent configuration:
the squid doesn't switch to https and so it 
doesn't authentication
In my older version the configuration it so:


auth_param basic children 5
auth_param basic realm Squid proxy-caching 
web server
auth_param basic credentialsttl 2 hours

cache_peer xxx.xxx.
xxx.xxx parent 3128 0 no-query no-digest sourcehash name=PRX_ONE

cache_peer yyy.yyy.yyy.yyy parent 3128 0 no-query no-digest sourcehash 
name=PRX_TWO

that it works, but after I upgrade if I use http it 
doesn't autheticate.
Thank you

Best regards,
Mau


From mark_squid at finito.me.uk  Tue Jan  3 14:27:57 2017
From: mark_squid at finito.me.uk (Mark Hoare)
Date: Tue, 3 Jan 2017 14:27:57 +0000
Subject: [squid-users] ssl_bump - peek & splice logging IP rather than
	server name
In-Reply-To: <9A9925E4-014F-4C64-B830-1644C809084F@finito.me.uk>
References: <9A9925E4-014F-4C64-B830-1644C809084F@finito.me.uk>
Message-ID: <23AF2CA5-EEF8-4C56-84EC-FD800718D76B@finito.me.uk>

Sorry, should have included squid version details in original post:

Squid Cache: Version 3.5.20
Service Name: squid
configure options:  '--build=x86_64-redhat-linux-gnu' '--host=x86_64-redhat-linux-gnu' '--program-prefix=' '--prefix=/usr' '--exec-prefix=/usr' '--bindir=/usr/bin' '--sbindir=/usr/sbin' '--sysconfdir=/etc' '--datadir=/usr/share' '--includedir=/usr/include' '--libdir=/usr/lib64' '--libexecdir=/usr/libexec' '--sharedstatedir=/var/lib' '--mandir=/usr/share/man' '--infodir=/usr/share/info' '--disable-strict-error-checking' '--exec_prefix=/usr' '--libexecdir=/usr/lib64/squid' '--localstatedir=/var' '--datadir=/usr/share/squid' '--sysconfdir=/etc/squid' '--with-logdir=$(localstatedir)/log/squid' '--with-pidfile=$(localstatedir)/run/squid.pid' '--disable-dependency-tracking' '--enable-eui' '--enable-follow-x-forwarded-for' '--enable-auth' '--enable-auth-basic=DB,LDAP,MSNT-multi-domain,NCSA,NIS,PAM,POP3,RADIUS,SASL,SMB,SMB_LM,getpwnam' '--enable-auth-ntlm=smb_lm,fake' '--enable-auth-digest=file,LDAP,eDirectory' '--enable-auth-negotiate=kerberos' '--enable-external-acl-helpers=file_userip,LDAP_group,time_quota,session,unix_group,wbinfo_group' '--enable-cache-digests' '--enable-cachemgr-hostname=localhost' '--enable-delay-pools' '--enable-epoll' '--enable-ident-lookups' '--enable-linux-netfilter' '--enable-removal-policies=heap,lru' '--enable-snmp' '--enable-ssl-crtd' '--enable-storeio=aufs,diskd,ufs' '--enable-wccpv2' '--enable-esi' '--enable-ecap' '--with-aio' '--with-default-user=squid' '--with-dl' '--with-openssl' '--with-pthreads' '--disable-arch-native' '--disable-icap-client' 'build_alias=x86_64-redhat-linux-gnu' 'host_alias=x86_64-redhat-linux-gnu' 'CFLAGS=-O2 -g -pipe -Wall -Wp,-D_FORTIFY_SOURCE=2 -fexceptions -fstack-protector-strong --param=ssp-buffer-size=4 -grecord-gcc-switches   -m64 -mtune=generic -fpie' 'LDFLAGS=-Wl,-z,relro  -pie -Wl,-z,relro -Wl,-z,now' 'CXXFLAGS=-O2 -g -pipe -Wall -Wp,-D_FORTIFY_SOURCE=2 -fexceptions -fstack-protector-strong --param=ssp-buffer-size=4 -grecord-gcc-switches   -m64 -mtune=generic -fpie' 'PKG_CONFIG_PATH=:/usr/lib64/pkgconfig:/usr/share/pkgconfig?

Cheers

Mark

> On 31 Dec 2016, at 14:37, Mark Hoare <mark_squid at finito.me.uk> wrote:
> 
> Hi,
> 
> I?m trying to setup policy based routing on a gateway device pointing at a remote squid server to do transparent HTTP & HTTPS proxying with ssl_bump (peek & splice)
> 
> After quite a bit of pain getting policy based routing working on the gateway and local port redirection on the squid server, everything appears to be working except the access log still refers to the destination IP address in the TCP_TUNNEL rather than the SNI/TLS server name.
> 
> By increasing the debug level I can see that the SNI/TLS details are definitely being obtained during the request processing but for some reason they are not ending up in the access log.
> 
> Extract from cache log:
>> 2016/12/31 14:18:01.966 kid1| 83,7| bio.cc(1110) parseV3Hello: Found server name: www.ssllabs.com
>> 2016/12/31 14:18:02.351 kid1| 83,5| support.cc(259) ssl_verify_cb: SSL Certificate signature OK: /C=US/ST=California/L=Redwood City/O=Qualys, Inc./CN=ssllabs.com
>> 2016/12/31 14:18:02.351 kid1| 83,4| support.cc(213) check_domain: Verifying server domain www.ssllabs.com to certificate name/subjectAltName ssllabs.com
>> 2016/12/31 14:18:02.351 kid1| 83,4| support.cc(213) check_domain: Verifying server domain www.ssllabs.com to certificate name/subjectAltName *.ssllabs.com
>> 2016/12/31 14:18:02.383 kid1| 83,5| PeerConnector.cc(307) serverCertificateVerified: HTTPS server CN: ssllabs.com bumped: local=<squid IP removed>:57790 remote=64.41.200.100:443 FD 14 flags=1
> 
> Extract from access log:
>> 1483193882.790    870 <local ip removed> TCP_TUNNEL/200 5620 CONNECT 64.41.200.100:443 - ORIGINAL_DST/64.41.200.100 -
> 
> From the output above I would have expected some of the server name info to get into the access log.
> 
> Squid config below:
>> debug_options ALL,7
>> 
>> http_port 3128
>> 
>> https_port 3130 intercept ssl-bump cert=/etc/squid/ssl_cert/squidCA.pem generate-host-certificates=on dynamic_cert_mem_cache_size=4MB
>> 
>> http_port 3131 intercept ssl-bump cert=/etc/squid/ssl_cert/squidCA.pem generate-host-certificates=on dynamic_cert_mem_cache_size=4MB
>> 
>> cache_dir ufs /var/spool/squid 200 16 256
>> coredump_dir /var/spool/squid
>> 
>> acl localnet src 10.0.0.0/8	# RFC1918 possible internal network
>> acl localnet src 172.16.0.0/12	# RFC1918 possible internal network
>> acl localnet src 192.168.0.0/16	# RFC1918 possible internal network
>> acl localnet src fc00::/7       # RFC 4193 local private network range
>> acl localnet src fe80::/10      # RFC 4291 link-local (directly plugged) machines
>> 
>> acl Safe_ports port 80		# http
>> acl Safe_ports port 21		# ftp
>> acl Safe_ports port 443		# https
>> acl Safe_ports port 70		# gopher
>> acl Safe_ports port 210		# wais
>> acl Safe_ports port 1025-65535	# unregistered ports
>> acl Safe_ports port 280		# http-mgmt
>> acl Safe_ports port 488		# gss-http
>> acl Safe_ports port 591		# filemaker
>> acl Safe_ports port 777		# multiling http
>> 
>> acl SSL_ports port 443
>> acl CONNECT method CONNECT
>> 
>> http_access deny !Safe_ports
>> http_access deny CONNECT !SSL_ports
>> 
>> http_access allow localhost manager
>> http_access deny manager
>> 
>> refresh_pattern ^ftp:		1440	20%	10080
>> refresh_pattern ^gopher:	1440	0%	1440
>> refresh_pattern -i (/cgi-bin/|\?) 0	0%	0
>> refresh_pattern .		0	20%	4320
>> 
>> ssl_bump peek all
>> ssl_bump splice all
>> 
>> always_direct allow all
>> 
>> http_access allow localnet
>> http_access allow localhost
>> 
>> http_access deny all
> 
> 
> Any suggestions gratefully received.
> 
> Thanks
> 
> Mark



From eliezer at ngtech.co.il  Tue Jan  3 22:36:13 2017
From: eliezer at ngtech.co.il (Eliezer  Croitoru)
Date: Wed, 4 Jan 2017 00:36:13 +0200
Subject: [squid-users] Intercept mode failing
In-Reply-To: <ffa6ea99-c97a-5578-ee32-6bfe6324de63@wheres5.com>
References: <fc527d12-bb42-0621-6931-36152849bb94@wheres5.com>
 <201701031033.53399.Antony.Stone@squid.open.source.it>
 <1ed37ea1-36d7-3b81-866b-ebf5a7f2437d@wheres5.com>
 <546f46b7bc43e3127fcdaa4e97b0c73d@treenet.co.nz>
 <4ff88780-7487-6fc4-ba93-8214e3532aa7@wheres5.com>
 <7b1625ee1afa6f7572724198d3c4d28e@treenet.co.nz>
 <ffa6ea99-c97a-5578-ee32-6bfe6324de63@wheres5.com>
Message-ID: <002a01d26611$c9bd76c0$5d386440$@ngtech.co.il>

If you get stuck with things contact me and I will see if I can sort things out so you would be able to grasp couple basics.

Eliezer

----
Eliezer Croitoru
Linux System Administrator
Mobile: +972-5-28704261
Email: eliezer at ngtech.co.il


-----Original Message-----
From: squid-users [mailto:squid-users-bounces at lists.squid-cache.org] On Behalf Of Hoggins!
Sent: Tuesday, January 3, 2017 3:13 PM
To: squid-users at lists.squid-cache.org
Subject: Re: [squid-users] Intercept mode failing

Hello Amos,

I believe my main problem is that I'm trying to apply recipes without understanding some of the internals, so it seems difficult to adapt.
I'll keep searching.

Thanks anyway !

    Hoggins!

Le 03/01/2017 ? 12:39, Amos Jeffries a ?crit :
> On 2017-01-03 23:53, Hoggins! wrote:
>> Hello,
>>
>> (answering to both Amos and Antony here, you got the same questioning
>> ;) )
>>
>> Le 03/01/2017 ? 11:45, Amos Jeffries a ?crit :
>>> On 2017-01-03 23:13, Hoggins! wrote:
>>>> Okay, I get that.
>>>>
>>>> Le 03/01/2017 ? 10:33, Antony Stone a ?crit :
>>>>> No - you must do the NAT (or REDIRECT) rule *on the Squid server*.
>>>>
>>>> Well, my Squid server is not on the same network as my clients, so 
>>>> I need something else than just a REDIRECT on the Squid itself.
>>>
>>> That does not matter when the DNAT or REDIRECT is done on the Squid 
>>> machine.
>>
>> OK, I'll have a deeper look into that, indeed I'm not familiar with 
>> what REDIRECT *exactly* does.
>>
>
> It does the same as DNAT, but using the machines "primary" IP address 
> instead of one you configure. Usually that means the first IP assigned 
> to eth0 or equivalent first interface.
>
> DNAT is best if you want a specific fixed IP:port for Squid to receive 
> on.
>
> REDIRECT is best if you want the proxy to be plug-n-play on any 
> network. squid.conf and iptables not needing IP address, just a port 
> number.
>
>
>>>
>>>>
>>>>>
>>>>> If you need to use policy routing to get the packets to the Squid 
>>>>> machine in the first place, that's okay, but this *must* be packet 
>>>>> routing, not address translation
>>>>
>>>> Policy routing was my first choice, but there is one important 
>>>> detail in my setup : between my gateway (192.168.22.10) and my 
>>>> Squid (192.168.55.3), there's an IPSec tunnel. My gateway does not 
>>>> have a link-local route to 192.168.55.3 so I can't add the default 
>>>> route to it inside a routing table (I get "Network is unreachable", 
>>>> which is expected).
>>>>
>>>> So I guess I'm stuck.
>>>
>>>
>>> So how did the packets get to the Squid machine after your DNAT ?
>>>
>>> The route does not have to be link-local. Any type of route will do 
>>> so long as all the routers handling the packets know which way to 
>>> pass them, and the dst-IP address is not changed.
>>
>> Well, xfrm routing is a lot different than "classic" routing, I 
>> learnt it the hard way. DNAT *will* work whereas policy routing won't 
>> if I don't explicitly declare all my subnets in my IPSec tunnel 
>> configuration. Got a big discussion about that on StrongSwan's 
>> mailing-list, and I believe this sums it up pretty nicely :
>> http://xkr47.outerspace.dyndns.org/netfilter/packet_flow/packet_flow9
>> .png
>>
>
> Looks like a simplified version of the netfilter devs official diagram 
> to me. So all the required decision points are present and should be 
> configurable.
>
>>
>> Anyway, yes, if I try to add a route by :
>>     ip route add default via <IP ADDRESS> table 123
>>
>> <IP ADDRESS> *has* to be directly reachable. Or it has to be in the 
>> routing table somehow. But the routing table handling the tunnelled 
>> packets is not managed by iproute2.
>
> It should "simply" be one of the tables with a value other than "123".
> But that should not be a consideration as editing it is not necessary.
> This "123" routing is only relevant *before* packets enter the tunnel, 
> since it is what tells the OS to send packets to the tunnel.
>
> IIRC the routing table for HTTP traffic (your '123' table) should 
> indicate the tunnel as the gateway <IP ADDRESS>. I don't recall 
> whether that gw was needing to be the local or the remote tunnel 
> endpoint though sorry, been too long since I set one of these up.
>
>>
>> So as I can't do otherwise, I'm going to experiment a bit more with 
>> the REDIRECT + DNAT between the gateway and the Squid server.
>>
>
> I think you are misinterpreting that diagram in regards to the policy 
> routing setup.
>
> What we have on the squid wiki on "Policy Routing" makes packets 
> follow this path;
>
>  * the PREROUTING "mangle" table sets marks on packets to be delivered 
> to Squid.
>
>  * those packets go through the "FORWARD" to that gateway in 
> '123'/'proxy' table.
>   (in our wiki that is the "ip rule add fwmark 2 table ..." part).
>
>  * after POSTROUTING the packets destined to a tunnel gateway are 
> diverted at that last-minute decision point after "QoS egress" so they 
> go back to "OUTPUT" which is where the tunnel related things are done 
> to it.
>
> Note that for the machine where the tunnel leads *from*, all that 
> matters is routing the packets into the tunnel. Once packets are 
> encapsulated by the tunnel they simply go through it to the other end.
> The receiving machine does whatever it needs to after the packets 
> leave the tunnel.
>
> HTH
> Amos
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>





From eliezer at ngtech.co.il  Tue Jan  3 22:41:05 2017
From: eliezer at ngtech.co.il (Eliezer  Croitoru)
Date: Wed, 4 Jan 2017 00:41:05 +0200
Subject: [squid-users] ssl_bump - peek & splice logging IP rather
	than	server name
In-Reply-To: <9A9925E4-014F-4C64-B830-1644C809084F@finito.me.uk>
References: <9A9925E4-014F-4C64-B830-1644C809084F@finito.me.uk>
Message-ID: <002c01d26612$77f3b330$67db1990$@ngtech.co.il>

Hey Mark,

Squid in intercept or tproxy mode will know one thing about the tunnel\connection: IP+port.
Since you are using:
> ssl_bump peek all
> ssl_bump splice all

The connections will always be spliced and you will never see any url.(are you expecting only the SNI or also the url?)
I do not know but there might be a code that can report the SNI if exists in the logs.
Alex is better then me in this but I believe it should be possible as an addition to the IP+PORT and not replacing them.

Eliezer

----
Eliezer Croitoru
Linux System Administrator
Mobile: +972-5-28704261
Email: eliezer at ngtech.co.il


-----Original Message-----
From: squid-users [mailto:squid-users-bounces at lists.squid-cache.org] On Behalf Of Mark Hoare
Sent: Saturday, December 31, 2016 4:38 PM
To: squid-users at lists.squid-cache.org
Subject: [squid-users] ssl_bump - peek & splice logging IP rather than server name

Hi,

I?m trying to setup policy based routing on a gateway device pointing at a remote squid server to do transparent HTTP & HTTPS proxying with ssl_bump (peek & splice)

After quite a bit of pain getting policy based routing working on the gateway and local port redirection on the squid server, everything appears to be working except the access log still refers to the destination IP address in the TCP_TUNNEL rather than the SNI/TLS server name.

By increasing the debug level I can see that the SNI/TLS details are definitely being obtained during the request processing but for some reason they are not ending up in the access log.

Extract from cache log:
> 2016/12/31 14:18:01.966 kid1| 83,7| bio.cc(1110) parseV3Hello: Found server name: www.ssllabs.com
> 2016/12/31 14:18:02.351 kid1| 83,5| support.cc(259) ssl_verify_cb: SSL Certificate signature OK: /C=US/ST=California/L=Redwood City/O=Qualys, Inc./CN=ssllabs.com
> 2016/12/31 14:18:02.351 kid1| 83,4| support.cc(213) check_domain: Verifying server domain www.ssllabs.com to certificate name/subjectAltName ssllabs.com
> 2016/12/31 14:18:02.351 kid1| 83,4| support.cc(213) check_domain: Verifying server domain www.ssllabs.com to certificate name/subjectAltName *.ssllabs.com
> 2016/12/31 14:18:02.383 kid1| 83,5| PeerConnector.cc(307) serverCertificateVerified: HTTPS server CN: ssllabs.com bumped: local=<squid IP removed>:57790 remote=64.41.200.100:443 FD 14 flags=1

Extract from access log:
> 1483193882.790    870 <local ip removed> TCP_TUNNEL/200 5620 CONNECT 64.41.200.100:443 - ORIGINAL_DST/64.41.200.100 -

>From the output above I would have expected some of the server name info to get into the access log.

Squid config below:
> debug_options ALL,7
> 
> http_port 3128
> 
> https_port 3130 intercept ssl-bump cert=/etc/squid/ssl_cert/squidCA.pem generate-host-certificates=on dynamic_cert_mem_cache_size=4MB
> 
> http_port 3131 intercept ssl-bump cert=/etc/squid/ssl_cert/squidCA.pem generate-host-certificates=on dynamic_cert_mem_cache_size=4MB
> 
> cache_dir ufs /var/spool/squid 200 16 256
> coredump_dir /var/spool/squid
> 
> acl localnet src 10.0.0.0/8	# RFC1918 possible internal network
> acl localnet src 172.16.0.0/12	# RFC1918 possible internal network
> acl localnet src 192.168.0.0/16	# RFC1918 possible internal network
> acl localnet src fc00::/7       # RFC 4193 local private network range
> acl localnet src fe80::/10      # RFC 4291 link-local (directly plugged) machines
> 
> acl Safe_ports port 80		# http
> acl Safe_ports port 21		# ftp
> acl Safe_ports port 443		# https
> acl Safe_ports port 70		# gopher
> acl Safe_ports port 210		# wais
> acl Safe_ports port 1025-65535	# unregistered ports
> acl Safe_ports port 280		# http-mgmt
> acl Safe_ports port 488		# gss-http
> acl Safe_ports port 591		# filemaker
> acl Safe_ports port 777		# multiling http
> 
> acl SSL_ports port 443
> acl CONNECT method CONNECT
> 
> http_access deny !Safe_ports
> http_access deny CONNECT !SSL_ports
> 
> http_access allow localhost manager
> http_access deny manager
> 
> refresh_pattern ^ftp:		1440	20%	10080
> refresh_pattern ^gopher:	1440	0%	1440
> refresh_pattern -i (/cgi-bin/|\?) 0	0%	0
> refresh_pattern .		0	20%	4320
> 
> ssl_bump peek all
> ssl_bump splice all
> 
> always_direct allow all
> 
> http_access allow localnet
> http_access allow localhost
> 
> http_access deny all


Any suggestions gratefully received.

Thanks

Mark
_______________________________________________
squid-users mailing list
squid-users at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-users



From rousskov at measurement-factory.com  Tue Jan  3 23:10:16 2017
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Tue, 3 Jan 2017 16:10:16 -0700
Subject: [squid-users] ssl_bump - peek & splice logging IP rather than
 server name
In-Reply-To: <002c01d26612$77f3b330$67db1990$@ngtech.co.il>
References: <9A9925E4-014F-4C64-B830-1644C809084F@finito.me.uk>
 <002c01d26612$77f3b330$67db1990$@ngtech.co.il>
Message-ID: <cd02ce1f-736c-3d5d-ef77-1febddfd7b16@measurement-factory.com>

On 01/03/2017 03:41 PM, Eliezer  Croitoru wrote:

> Squid in intercept or tproxy mode will know one thing about the tunnel\connection: IP+port.

... and SSL handshake information when peeking or staring at client/server.


> Since you are using:
>> ssl_bump peek all
>> ssl_bump splice all

> The connections will always be spliced and you will never see any
> url.(are you expecting only the SNI or also the url?)

AFAICT, Mark is expecting Squid to log one of the server names, not the
HTTP request URL.


> I do not know but there might be a code that can report the SNI if exists in the logs.

According to squid.conf.documented, the following logformat %code is
supported in modern Squids:

> ssl::>sni       SSL client SNI sent to Squid. Available only
>                 after the peek, stare, or splice SSL bumping
>                 actions.

This %code is not in the default access.log line format, naturally.

Squid can also analyze CN and other server certificate fields, but there
is no code to log them IIRC.


Please note that the intercepted server IP address, the client-supplied
SNI name, the server-supplied common name (CN), the server-supplied
alternative names, and the host info in the encrypted client HTTP
request, may all be different.

Given the variety of information sources that might supply different
information, it is not clear to me whether %ru should be based on SNI
information when both TCP-level and SNI information is available. Or
should it be based on CN? Or perhaps on CN _unless_ SNI matches one of
the alternative names?? This is a complicated issue; even the smart
server_name ACL needs parameters to clarify what "server name(s)" the
admin really wants to use/trust...

According to Mark's email, %ru uses TCP-level info. We could either
change %ru to use the "latest" info (like the server_name ACL does) or
add a new logformat code that does that while leaving the old %ru and
friends alone. Given the complexity of the issue, the latter may be a
better approach.


HTH,

Alex.

> -----Original Message-----
> From: squid-users [mailto:squid-users-bounces at lists.squid-cache.org] On Behalf Of Mark Hoare
> Sent: Saturday, December 31, 2016 4:38 PM
> To: squid-users at lists.squid-cache.org
> Subject: [squid-users] ssl_bump - peek & splice logging IP rather than server name

> Extract from access log:
>> 1483193882.790    870 <local ip removed> TCP_TUNNEL/200 5620 CONNECT 64.41.200.100:443 - ORIGINAL_DST/64.41.200.100 -

> From the output above I would have expected some of the server name info to get into the access log.

>> http_port 3128
>>
>> https_port 3130 intercept ssl-bump cert=/etc/squid/ssl_cert/squidCA.pem generate-host-certificates=on dynamic_cert_mem_cache_size=4MB
>>
>> http_port 3131 intercept ssl-bump cert=/etc/squid/ssl_cert/squidCA.pem generate-host-certificates=on dynamic_cert_mem_cache_size=4MB

>> ssl_bump peek all
>> ssl_bump splice all



From mark_squid at finito.me.uk  Tue Jan  3 23:11:02 2017
From: mark_squid at finito.me.uk (Mark Hoare)
Date: Tue, 3 Jan 2017 23:11:02 +0000
Subject: [squid-users] ssl_bump - peek & splice logging IP rather
	than	server name
In-Reply-To: <002c01d26612$77f3b330$67db1990$@ngtech.co.il>
References: <9A9925E4-014F-4C64-B830-1644C809084F@finito.me.uk>
 <002c01d26612$77f3b330$67db1990$@ngtech.co.il>
Message-ID: <30CFCE0A-6C02-44E8-B989-B870D1B14277@finito.me.uk>

Thanks Eliezer,

I'm aiming for a configuration which logs all HTTP and HTTPS connections without performing any full ssl_bumping, which would need me to get devices to trust my CA cert. 

I have something similar with pfsense (which does log SNI/server name rather than IP & port) but I'm getting some strange behaviour so wanted to build an equivalent standalone squid server, with a more up to date version of squid. 

Will have another look at my pfsense ssl_bump config as there are some slight differences, but I think these are hangovers from earlier syntax (ssl_bump server-first all) which shouldn't be required under 3.5. 

Cheers

Mark

> On 3 Jan 2017, at 22:41, Eliezer Croitoru <eliezer at ngtech.co.il> wrote:
> 
> Hey Mark,
> 
> Squid in intercept or tproxy mode will know one thing about the tunnel\connection: IP+port.
> Since you are using:
>> ssl_bump peek all
>> ssl_bump splice all
> 
> The connections will always be spliced and you will never see any url.(are you expecting only the SNI or also the url?)
> I do not know but there might be a code that can report the SNI if exists in the logs.
> Alex is better then me in this but I believe it should be possible as an addition to the IP+PORT and not replacing them.
> 
> Eliezer
> 
> ----
> Eliezer Croitoru
> Linux System Administrator
> Mobile: +972-5-28704261
> Email: eliezer at ngtech.co.il
> 
> 
> -----Original Message-----
> From: squid-users [mailto:squid-users-bounces at lists.squid-cache.org] On Behalf Of Mark Hoare
> Sent: Saturday, December 31, 2016 4:38 PM
> To: squid-users at lists.squid-cache.org
> Subject: [squid-users] ssl_bump - peek & splice logging IP rather than server name
> 
> Hi,
> 
> I?m trying to setup policy based routing on a gateway device pointing at a remote squid server to do transparent HTTP & HTTPS proxying with ssl_bump (peek & splice)
> 
> After quite a bit of pain getting policy based routing working on the gateway and local port redirection on the squid server, everything appears to be working except the access log still refers to the destination IP address in the TCP_TUNNEL rather than the SNI/TLS server name.
> 
> By increasing the debug level I can see that the SNI/TLS details are definitely being obtained during the request processing but for some reason they are not ending up in the access log.
> 
> Extract from cache log:
>> 2016/12/31 14:18:01.966 kid1| 83,7| bio.cc(1110) parseV3Hello: Found server name: www.ssllabs.com
>> 2016/12/31 14:18:02.351 kid1| 83,5| support.cc(259) ssl_verify_cb: SSL Certificate signature OK: /C=US/ST=California/L=Redwood City/O=Qualys, Inc./CN=ssllabs.com
>> 2016/12/31 14:18:02.351 kid1| 83,4| support.cc(213) check_domain: Verifying server domain www.ssllabs.com to certificate name/subjectAltName ssllabs.com
>> 2016/12/31 14:18:02.351 kid1| 83,4| support.cc(213) check_domain: Verifying server domain www.ssllabs.com to certificate name/subjectAltName *.ssllabs.com
>> 2016/12/31 14:18:02.383 kid1| 83,5| PeerConnector.cc(307) serverCertificateVerified: HTTPS server CN: ssllabs.com bumped: local=<squid IP removed>:57790 remote=64.41.200.100:443 FD 14 flags=1
> 
> Extract from access log:
>> 1483193882.790    870 <local ip removed> TCP_TUNNEL/200 5620 CONNECT 64.41.200.100:443 - ORIGINAL_DST/64.41.200.100 -
> 
> From the output above I would have expected some of the server name info to get into the access log.
> 
> Squid config below:
>> debug_options ALL,7
>> 
>> http_port 3128
>> 
>> https_port 3130 intercept ssl-bump cert=/etc/squid/ssl_cert/squidCA.pem generate-host-certificates=on dynamic_cert_mem_cache_size=4MB
>> 
>> http_port 3131 intercept ssl-bump cert=/etc/squid/ssl_cert/squidCA.pem generate-host-certificates=on dynamic_cert_mem_cache_size=4MB
>> 
>> cache_dir ufs /var/spool/squid 200 16 256
>> coredump_dir /var/spool/squid
>> 
>> acl localnet src 10.0.0.0/8    # RFC1918 possible internal network
>> acl localnet src 172.16.0.0/12    # RFC1918 possible internal network
>> acl localnet src 192.168.0.0/16    # RFC1918 possible internal network
>> acl localnet src fc00::/7       # RFC 4193 local private network range
>> acl localnet src fe80::/10      # RFC 4291 link-local (directly plugged) machines
>> 
>> acl Safe_ports port 80        # http
>> acl Safe_ports port 21        # ftp
>> acl Safe_ports port 443        # https
>> acl Safe_ports port 70        # gopher
>> acl Safe_ports port 210        # wais
>> acl Safe_ports port 1025-65535    # unregistered ports
>> acl Safe_ports port 280        # http-mgmt
>> acl Safe_ports port 488        # gss-http
>> acl Safe_ports port 591        # filemaker
>> acl Safe_ports port 777        # multiling http
>> 
>> acl SSL_ports port 443
>> acl CONNECT method CONNECT
>> 
>> http_access deny !Safe_ports
>> http_access deny CONNECT !SSL_ports
>> 
>> http_access allow localhost manager
>> http_access deny manager
>> 
>> refresh_pattern ^ftp:        1440    20%    10080
>> refresh_pattern ^gopher:    1440    0%    1440
>> refresh_pattern -i (/cgi-bin/|\?) 0    0%    0
>> refresh_pattern .        0    20%    4320
>> 
>> ssl_bump peek all
>> ssl_bump splice all
>> 
>> always_direct allow all
>> 
>> http_access allow localnet
>> http_access allow localhost
>> 
>> http_access deny all
> 
> 
> Any suggestions gratefully received.
> 
> Thanks
> 
> Mark
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
> 



From rousskov at measurement-factory.com  Tue Jan  3 23:35:26 2017
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Tue, 3 Jan 2017 16:35:26 -0700
Subject: [squid-users] ssl_bump - peek & splice logging IP rather than
 server name
In-Reply-To: <30CFCE0A-6C02-44E8-B989-B870D1B14277@finito.me.uk>
References: <9A9925E4-014F-4C64-B830-1644C809084F@finito.me.uk>
 <002c01d26612$77f3b330$67db1990$@ngtech.co.il>
 <30CFCE0A-6C02-44E8-B989-B870D1B14277@finito.me.uk>
Message-ID: <d563330f-ace6-9f23-185f-8b3dd0bdb097@measurement-factory.com>

On 01/03/2017 04:11 PM, Mark Hoare wrote:

> I think these are hangovers from earlier syntax (ssl_bump
> server-first all) which shouldn't be required under 3.5.

Please note that the depricated server-first is a "bumping" (not
splicing!) action, and you may see a lot more information in the
bumping-Squid logs, naturally.

Alex.



From squid3 at treenet.co.nz  Wed Jan  4 04:41:43 2017
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Wed, 04 Jan 2017 17:41:43 +1300
Subject: [squid-users] problem authentication ntlm with squid 3.5.21
In-Reply-To: <15964aea8f2.amaury@tin.it>
References: <15964aea8f2.amaury@tin.it>
Message-ID: <0cce27878fe2a8a607a871483ea1d451@treenet.co.nz>

On 2017-01-04 03:14, amaury at tin.it wrote:
> Hello
> I upgrade squid from 3.4.9-20141203-r13193 to 3.5.21-20160908-
> r14081 and I have a problem with authentication to ntlm in a
> transparent configuration:
> the squid doesn't switch to https and so it
> doesn't authentication

This doesn't compute. Your config below has nothing to do with NTLM or 
HTTPS.


> In my older version the configuration it so:
> 
> 
> auth_param basic children 5
> auth_param basic realm Squid proxy-caching
> web server
> auth_param basic credentialsttl 2 hours

Three problems here:

1) without a "program" line specified the above do nothing.

2) the above lines are for *Basic* auth, not NTLM.

3) "transparent" interception proxy cannot perform authentication.


> 
> cache_peer xxx.xxx.
> xxx.xxx parent 3128 0 no-query no-digest sourcehash name=PRX_ONE
> 
> cache_peer yyy.yyy.yyy.yyy parent 3128 0 no-query no-digest sourcehash
> name=PRX_TWO
> 
> that it works, but after I upgrade if I use http it
> doesn't autheticate.

see above.

If the parent(s) are performing NTLM authentication you need the 
login=PASSTHRU option to be specified on the cache_peer lines and to 
remove the auth_param settings.

HTH
Amos



From amaury at tin.it  Wed Jan  4 13:51:56 2017
From: amaury at tin.it (amaury at tin.it)
Date: Wed, 4 Jan 2017 14:51:56 +0100 (CET)
Subject: [squid-users] R: problem authentication ntlm with squid 3.5.21
Message-ID: <15969c021f2.amaury@tin.it>

Thanks Amos for your answer.
Our problem is that we have a proxy chain
(remote peer is squid-2.6) where the ntlm authentication on remote site 
is working with 3.4.9-20141203-r13193 but after upgrade doesn't work. 
We have just test with login=PASSTHRU but without positive result.
We 
are using the same squid.conf configuration for 3.4.9 and 3.5.21.
What 
is it changed about ntlm ?

Regards,
Mau




From vvjoshi5 at gmail.com  Thu Jan  5 15:30:17 2017
From: vvjoshi5 at gmail.com (vinay)
Date: Thu, 5 Jan 2017 07:30:17 -0800 (PST)
Subject: [squid-users] Squid memory leak on ubuntu 14.04
In-Reply-To: <CAJGZ0h5Cf0936+5i17UCVN7vm+Y=KPg_+bM0uK1fz1+kM0sv4Q@mail.gmail.com>
References: <CAJGZ0h4bmkcMF7V6WGsfh4uLB-by27t8esB0SnRjudFPVCTt9g@mail.gmail.com>
 <5658838F.7050707@urlfilterdb.com>
 <CAJGZ0h5s9k7txGMTN1tP4wdhKvkb=z_kuK8E5t5jwBpgKj8pUg@mail.gmail.com>
 <565BC47E.6070408@treenet.co.nz>
 <CAJGZ0h4AWgyHbwkujCiUpRPf8AbhegpdxaHzRVNuOCasd-Ho0w@mail.gmail.com>
 <565C0EFE.40904@treenet.co.nz>
 <CAJGZ0h6oTCu2OJqk8u-+X4eaeDiHyawg76xJPskjG=tfv-CESA@mail.gmail.com>
 <CAJGZ0h5Cf0936+5i17UCVN7vm+Y=KPg_+bM0uK1fz1+kM0sv4Q@mail.gmail.com>
Message-ID: <1483630217278-4681060.post@n4.nabble.com>

Hi am using Squid 3.3.8 on Ubuntu 14.04. I have default configuration for
Squid config file . The request is passing via Squid but its not caching the
contents/images/css , everytime am getting TCP_MISS 200 for each request in
access logs.

########################################################
acl SSL_ports port 443
acl Safe_ports port 80          # http
acl Safe_ports port 21          # ftp
acl Safe_ports port 443         # https
acl Safe_ports port 70          # gopher
acl Safe_ports port 210         # wais
acl Safe_ports port 1025-65535  # unregistered ports
acl Safe_ports port 280         # http-mgmt
acl Safe_ports port 488         # gss-http
acl Safe_ports port 591         # filemaker
acl Safe_ports port 777         # multiling http
acl CONNECT method CONNECT
#
# Recommended minimum Access Permission configuration:
#
# Deny requests to certain unsafe ports
http_access deny !Safe_ports
# Deny CONNECT to other than secure SSL ports
http_access deny CONNECT !SSL_ports
# Only allow cachemgr access from localhost
http_access allow localhost manager
http_access deny manager
# We strongly recommend the following be uncommented to protect innocent
# web applications running on the proxy server who think the only
# one who can access services on "localhost" is a local user
http_access deny to_localhost
#
# INSERT YOUR OWN RULE(S) HERE TO ALLOW ACCESS FROM YOUR CLIENTS
#
# Example rule allowing access from your local networks.
# Adapt localnet in the ACL section to list your (internal) IP networks
# from where browsing should be allowed
http_access allow localnet
http_access allow localhost
# And finally deny all other access to this proxy
http_access deny all
# Squid normally listens to port 3128
http_port 3128
# Uncomment and adjust the following to add a disk cache directory.
cache_dir ufs /var/spool/squid 102400 16 256
# Leave coredumps in the first cache dir
coredump_dir /var/spool/squid
#
# Add any of your own refresh_pattern entries above these.
#
refresh_pattern ^ftp:           1440    20%     10080
refresh_pattern ^gopher:        1440    0%      1440
refresh_pattern -i (/cgi-bin/|\?) 0     0%      0
refresh_pattern .               0       20%     4320
# Max Object Size Cache
maximum_object_size 10240 KB
##############################################################


Let me know if Squid 3.3.8 is compatible with Ubuntu 14.04 or not? if not
what are the compatible versions with Ubuntu 14.04 as i cant change Ubuntu
version. 
what changes need to be done to cache the contents ?








--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/Squid-memory-leak-on-ubuntu-14-04-tp4674855p4681060.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From david at articatech.com  Thu Jan  5 16:10:17 2017
From: david at articatech.com (David Touzeau)
Date: Thu, 5 Jan 2017 17:10:17 +0100
Subject: [squid-users] [3.5x]: identd lookup made before proxy_protocol
	checking and failed [help]
Message-ID: <04ad01d2676e$34b64670$9e22d350$@articatech.com>


Hi,

We need to use ident daemon in order to authenticate users.

Squid works fine when computers are directly connected to the proxy.

We have added HaProxy * * * Load-balancer * * * using *proxy_protocol*
between users and 2 Squid proxies 
With the load balancer, squid want to query identd port directly on the load
balancer but not on the client source IP address.
If you see this piece of logs, you can see that the source client address is
correctly understood by Squid but * * after * *  the ident verification.


How can i fix this behaviour ?
Best regards,

Browser: 192.168.1.50
Load balancer: 192.168.1.60
Squid: 192.168.1.61


2017/01/05 16:55:19.290 kid1| 5,5| AsyncCall.cc(93) ScheduleCall:
TcpAcceptor.cc(317) will call httpAccept(local=192.168.1.62:8080
remote=192.168.1.61:54798 FD 12 flags=1, MXID_244) [call229910]
2017/01/05 16:55:19.290 kid1| 5,5| ModEpoll.cc(116) SetSelect: FD 29,
type=1, handler=1, client_data=0x18cb298, timeout=0
2017/01/05 16:55:19.290 kid1| 5,5| AsyncCallQueue.cc(55) fireNext: entering
httpAccept(local=192.168.1.62:8080 remote=192.168.1.61:54798 FD 12 flags=1,
MXID_244)
2017/01/05 16:55:19.290 kid1| 5,5| AsyncCall.cc(38) make: make call
httpAccept [call229910]
2017/01/05 16:55:19.290 kid1| 33,4| client_side.cc(3705) httpAccept:
local=192.168.1.62:8080 remote=192.168.1.61:54798 FD 12 flags=1: accepted
2017/01/05 16:55:19.290 kid1| 93,5| AsyncJob.cc(34) AsyncJob: AsyncJob
constructed, this=0x1eae810 type=Http::Server [job1085]
2017/01/05 16:55:19.290 kid1| 93,5| AsyncCall.cc(26) AsyncCall: The
AsyncCall AsyncJob::start constructed, this=0x1ab7850 [call229911]
2017/01/05 16:55:19.290 kid1| 93,5| AsyncCall.cc(93) ScheduleCall:
AsyncJob.cc(26) will call AsyncJob::start() [call229911]
2017/01/05 16:55:19.290 kid1| 5,5| AsyncCallQueue.cc(57) fireNext: leaving
httpAccept(local=192.168.1.62:8080 remote=192.168.1.61:54798 FD 12 flags=1,
MXID_244)
2017/01/05 16:55:19.290 kid1| 93,5| AsyncCallQueue.cc(55) fireNext: entering
AsyncJob::start()
2017/01/05 16:55:19.290 kid1| 93,5| AsyncCall.cc(38) make: make call
AsyncJob::start [call229911]
2017/01/05 16:55:19.290 kid1| 93,5| AsyncJob.cc(123) callStart: Http::Server
status in: [ job1085]
2017/01/05 16:55:19.290 kid1| 33,5| AsyncCall.cc(26) AsyncCall: The
AsyncCall ConnStateData::connStateClosed constructed, this=0x1e5a750
[call229912]
2017/01/05 16:55:19.290 kid1| 5,5| comm.cc(994) comm_add_close_handler:
comm_add_close_handler: FD 12, AsyncCall=0x1e5a750*1
2017/01/05 16:55:19.290 kid1| 28,3| Checklist.cc(70) preCheck:
0x7ffed50496a0 checking fast rules
2017/01/05 16:55:19.290 kid1| 28,5| Checklist.cc(346) fastCheck:
aclCheckFast: list: 0x18d4128
2017/01/05 16:55:19.290 kid1| 28,5| Acl.cc(138) matches: checking
ident_lookup_access
2017/01/05 16:55:19.290 kid1| 28,5| Checklist.cc(400) bannedAction: Action
'ALLOWED/0' is not banned
2017/01/05 16:55:19.290 kid1| 28,5| Acl.cc(138) matches: checking
ident_lookup_access#1
2017/01/05 16:55:19.290 kid1| 28,5| Acl.cc(138) matches: checking
ident_aware_hosts
2017/01/05 16:55:19.290 kid1| 28,3| Ip.cc(539) match: aclIpMatchIp:
'192.168.1.61:54798' found
2017/01/05 16:55:19.290 kid1| 28,3| Acl.cc(158) matches: checked:
ident_aware_hosts = 1
2017/01/05 16:55:19.290 kid1| 28,3| Acl.cc(158) matches: checked:
ident_lookup_access#1 = 1
2017/01/05 16:55:19.290 kid1| 28,3| Acl.cc(158) matches: checked:
ident_lookup_access = 1
2017/01/05 16:55:19.290 kid1| 28,3| Checklist.cc(63) markFinished:
0x7ffed50496a0 answer ALLOWED for match
2017/01/05 16:55:19.290 kid1| 30,3| AsyncCall.cc(26) AsyncCall: The
AsyncCall Ident::ConnectDone constructed, this=0x18c9f10 [call229913]
2017/01/05 16:55:19.290 kid1| 93,5| AsyncJob.cc(34) AsyncJob: AsyncJob
constructed, this=0x1ea4838 type=Comm::ConnOpener [job1086]
2017/01/05 16:55:19.290 kid1| 93,5| AsyncCall.cc(26) AsyncCall: The
AsyncCall AsyncJob::start constructed, this=0x18cd970 [call229914]
2017/01/05 16:55:19.290 kid1| 93,5| AsyncCall.cc(93) ScheduleCall:
AsyncJob.cc(26) will call AsyncJob::start() [call229914]
2017/01/05 16:55:19.290 kid1| 28,4| FilledChecklist.cc(66)
~ACLFilledChecklist: ACLFilledChecklist destroyed 0x7ffed50496a0
2017/01/05 16:55:19.290 kid1| 28,4| Checklist.cc(197) ~ACLChecklist:
ACLChecklist::~ACLChecklist: destroyed 0x7ffed50496a0
2017/01/05 16:55:19.290 kid1| 28,3| Checklist.cc(70) preCheck:
0x7ffed5049430 checking fast rules
2017/01/05 16:55:19.290 kid1| 28,5| Checklist.cc(346) fastCheck:
aclCheckFast: list: 0x18dd488
2017/01/05 16:55:19.290 kid1| 28,5| Acl.cc(138) matches: checking
proxy_protocol_access
2017/01/05 16:55:19.290 kid1| 28,5| Checklist.cc(400) bannedAction: Action
'ALLOWED/0' is not banned
2017/01/05 16:55:19.290 kid1| 28,5| Acl.cc(138) matches: checking
proxy_protocol_access#1
2017/01/05 16:55:19.290 kid1| 28,5| Acl.cc(138) matches: checking all
2017/01/05 16:55:19.290 kid1| 28,3| Ip.cc(539) match: aclIpMatchIp:
'192.168.1.61:54798' found
2017/01/05 16:55:19.290 kid1| 28,3| Acl.cc(158) matches: checked: all = 1
2017/01/05 16:55:19.290 kid1| 28,3| Acl.cc(158) matches: checked:
proxy_protocol_access#1 = 1
2017/01/05 16:55:19.290 kid1| 28,3| Acl.cc(158) matches: checked:
proxy_protocol_access = 1
2017/01/05 16:55:19.290 kid1| 28,3| Checklist.cc(63) markFinished:
0x7ffed5049430 answer ALLOWED for match
2017/01/05 16:55:19.290 kid1| 28,4| FilledChecklist.cc(66)
~ACLFilledChecklist: ACLFilledChecklist destroyed 0x7ffed5049430
2017/01/05 16:55:19.290 kid1| 28,4| Checklist.cc(197) ~ACLChecklist:
ACLChecklist::~ACLChecklist: destroyed 0x7ffed5049430
2017/01/05 16:55:19.290 kid1| 28,4| FilledChecklist.cc(66)
~ACLFilledChecklist: ACLFilledChecklist destroyed 0x7ffed5049860
2017/01/05 16:55:19.290 kid1| 28,4| Checklist.cc(197) ~ACLChecklist:
ACLChecklist::~ACLChecklist: destroyed 0x7ffed5049860
2017/01/05 16:55:19.290 kid1| 33,5| AsyncCall.cc(26) AsyncCall: The
AsyncCall Http::Server::requestTimeout constructed, this=0x19524c0
[call229915]
2017/01/05 16:55:19.290 kid1| 5,3| comm.cc(553) commSetConnTimeout:
local=192.168.1.62:8080 remote=192.168.1.61:54798 FD 12 flags=1 timeout 300
2017/01/05 16:55:19.290 kid1| 33,4| client_side.cc(231) readSomeData:
local=192.168.1.62:8080 remote=192.168.1.61:54798 FD 12 flags=1: reading
request...
2017/01/05 16:55:19.291 kid1| 33,5| AsyncCall.cc(26) AsyncCall: The
AsyncCall ConnStateData::clientReadRequest constructed, this=0x1f4cc10
[call229916]
2017/01/05 16:55:19.291 kid1| 5,5| Read.cc(58) comm_read_base: comm_read,
queueing read for local=192.168.1.62:8080 remote=192.168.1.61:54798 FD 12
flags=1; asynCall 0x1f4cc10*1
2017/01/05 16:55:19.291 kid1| 5,5| ModEpoll.cc(116) SetSelect: FD 12,
type=1, handler=1, client_data=0x7fe9e8ff0678, timeout=0
2017/01/05 16:55:19.291 kid1| 93,5| AsyncJob.cc(152) callEnd: Http::Server
status out: [ job1085]
2017/01/05 16:55:19.291 kid1| 93,5| AsyncCallQueue.cc(57) fireNext: leaving
AsyncJob::start()
2017/01/05 16:55:19.291 kid1| 93,5| AsyncCallQueue.cc(55) fireNext: entering
AsyncJob::start()
2017/01/05 16:55:19.291 kid1| 93,5| AsyncCall.cc(38) make: make call
AsyncJob::start [call229914]
2017/01/05 16:55:19.291 kid1| 93,5| AsyncJob.cc(123) callStart:
Comm::ConnOpener status in: [ job1086]
2017/01/05 16:55:19.291 kid1| 50,3| comm.cc(347) comm_openex: comm_openex:
Attempt open socket for: 192.168.1.62
2017/01/05 16:55:19.291 kid1| 50,3| comm.cc(388) comm_openex: comm_openex:
Opened socket local=192.168.1.62 remote=[::] FD 13 flags=1 : family=2,
type=1, protocol=6
2017/01/05 16:55:19.291 kid1| 5,5| comm.cc(420) comm_init_opened:
local=192.168.1.62 remote=[::] FD 13 flags=1 is a new socket
2017/01/05 16:55:19.291 kid1| 51,3| fd.cc(198) fd_open: fd_open() FD 13 
2017/01/05 16:55:19.291 kid1| 50,6| comm.cc(209) commBind: commBind: bind
socket FD 13 to 192.168.1.62
2017/01/05 16:55:19.291 kid1| 5,4| AsyncCall.cc(26) AsyncCall: The AsyncCall
Comm::ConnOpener::earlyAbort constructed, this=0x1fa6ea0 [call229917]
2017/01/05 16:55:19.291 kid1| 5,5| comm.cc(994) comm_add_close_handler:
comm_add_close_handler: FD 13, AsyncCall=0x1fa6ea0*1
2017/01/05 16:55:19.291 kid1| 5,4| AsyncCall.cc(26) AsyncCall: The AsyncCall
Comm::ConnOpener::timeout constructed, this=0x1fe7e90 [call229918]

* * * * Squid try to connect to the Load-Balancer on 113 port instead
192.168.1.50 * * * *

2017/01/05 16:55:19.291 kid1| 5,3| ConnOpener.cc(289) createFd:
local=192.168.1.62 remote=192.168.1.61:113 flags=1 will timeout in 3
2017/01/05 16:55:19.291 kid1| 5,5| comm.cc(644) comm_connect_addr: sock=13,
addrinfo( flags=4, family=2, socktype=1, protocol=6, &addr=0x1f788e0,
addrlen=16 )
2017/01/05 16:55:19.291 kid1| 5,5| ConnOpener.cc(343) doConnect:
local=192.168.1.62 remote=192.168.1.61:113 flags=1: Comm::INPROGRESS
2017/01/05 16:55:19.291 kid1| 5,5| ModEpoll.cc(116) SetSelect: FD 13,
type=2, handler=1, client_data=0x1f788e0, timeout=0
2017/01/05 16:55:19.291 kid1| 93,5| AsyncJob.cc(152) callEnd:
Comm::ConnOpener status out: [ job1086]
2017/01/05 16:55:19.291 kid1| 93,5| AsyncCallQueue.cc(57) fireNext: leaving
AsyncJob::start()
2017/01/05 16:55:19.291 kid1| 5,3| IoCallback.cc(116) finish: called for
local=192.168.1.62:8080 remote=192.168.1.61:54798 FD 12 flags=1 (0, 0)
2017/01/05 16:55:19.291 kid1| 33,5| AsyncCall.cc(93) ScheduleCall:
IoCallback.cc(135) will call
ConnStateData::clientReadRequest(local=192.168.1.62:8080
remote=192.168.1.61:54798 FD 12 flags=1, data=0x1eae648) [call229916]
2017/01/05 16:55:19.291 kid1| 33,5| AsyncCallQueue.cc(55) fireNext: entering
ConnStateData::clientReadRequest(local=192.168.1.62:8080
remote=192.168.1.61:54798 FD 12 flags=1, data=0x1eae648)
2017/01/05 16:55:19.291 kid1| 33,5| AsyncCall.cc(38) make: make call
ConnStateData::clientReadRequest [call229916]
2017/01/05 16:55:19.291 kid1| 33,5| AsyncJob.cc(123) callStart: Http::Server
status in: [ job1085]
2017/01/05 16:55:19.291 kid1| 33,5| client_side.cc(3283) clientReadRequest:
local=192.168.1.62:8080 remote=192.168.1.61:54798 FD 12 flags=1
2017/01/05 16:55:19.291 kid1| 24,7| SBuf.cc(193) rawSpace: reserving 16382
for SBuf18795
2017/01/05 16:55:19.291 kid1| 24,7| SBuf.cc(935) reAlloc: SBuf18795 new
store capacity: 16384
2017/01/05 16:55:19.291 kid1| 5,3| Read.cc(91) ReadNow:
local=192.168.1.62:8080 remote=192.168.1.61:54798 FD 12 flags=1, size 16382,
retval 409, errno 0
2017/01/05 16:55:19.291 kid1| 24,7| SBuf.cc(242) append: from c-string to id
SBuf18795
2017/01/05 16:55:19.291 kid1| 24,7| SBuf.cc(193) rawSpace: reserving 409 for
SBuf18795
2017/01/05 16:55:19.291 kid1| 24,7| SBuf.cc(200) rawSpace: SBuf18795 not
growing
2017/01/05 16:55:19.291 kid1| 33,5| client_side.cc(3232)
clientParseRequests: local=192.168.1.62:8080 remote=192.168.1.61:54798 FD 12
flags=1: attempting to parse
2017/01/05 16:55:19.291 kid1| 24,7| SBuf.cc(802) findFirstNotOf: first not
of characterset non-LF in id SBuf18808
2017/01/05 16:55:19.291 kid1| 24,7| SBuf.cc(139) assign: assigning SBuf18807
from SBuf18809
2017/01/05 16:55:19.291 kid1| 24,7| SBuf.cc(139) assign: assigning SBuf18795
from SBuf18803
2017/01/05 16:55:19.291 kid1| 24,7| SBuf.cc(139) assign: assigning SBuf18803
from SBuf18807
2017/01/05 16:55:19.291 kid1| 24,7| SBuf.cc(802) findFirstNotOf: first not
of characterset IP Address in id SBuf18818
2017/01/05 16:55:19.291 kid1| 24,7| SBuf.cc(139) assign: assigning SBuf18816
from SBuf18819
2017/01/05 16:55:19.291 kid1| 24,7| SBuf.cc(802) findFirstNotOf: first not
of characterset IP Address in id SBuf18821
2017/01/05 16:55:19.291 kid1| 24,7| SBuf.cc(139) assign: assigning SBuf18817
from SBuf18822
2017/01/05 16:55:19.291 kid1| 24,7| SBuf.cc(193) rawSpace: reserving 1 for
SBuf18816
2017/01/05 16:55:19.291 kid1| 24,7| SBuf.cc(935) reAlloc: SBuf18816 new
store capacity: 40
2017/01/05 16:55:19.291 kid1| 24,7| SBuf.cc(193) rawSpace: reserving 1 for
SBuf18817
2017/01/05 16:55:19.291 kid1| 24,7| SBuf.cc(935) reAlloc: SBuf18817 new
store capacity: 40

* * * * Proxy protocol checking, OK squid get the client source IP address
but ACL failed the ident checking was failed before * * * *
2017/01/05 16:55:19.291 kid1| 33,5| client_side.cc(3105) parseProxy1p0:
PROXY/1.0 protocol on connection local=192.168.1.62:8080
remote=192.168.1.61:54798 FD 12 flags=1
2017/01/05 16:55:19.291 kid1| 33,5| client_side.cc(3110) parseProxy1p0:
PROXY/1.0 upgrade: local=192.168.1.61:8080 remote=192.168.1.50:59019 FD 12
flags=1
2017/01/05 16:55:19.291 kid1| 24,7| SBuf.cc(193) rawSpace: reserving 1 for
SBuf18795
2017/01/05 16:55:19.291 kid1| 24,7| SBuf.cc(200) rawSpace: SBuf18795 not
growing
2017/01/05 16:55:19.291 kid1| 74,5| HttpParser.cc(37) reset: Request buffer
is GET http://www.squid-cache.org/Images/img4.jpg HTTP/1.1




From vvjoshi5 at gmail.com  Thu Jan  5 16:43:32 2017
From: vvjoshi5 at gmail.com (vinay)
Date: Thu, 5 Jan 2017 08:43:32 -0800 (PST)
Subject: [squid-users] Squid 3.3.8 is available
In-Reply-To: <51E163AF.2020907@treenet.co.nz>
References: <51E163AF.2020907@treenet.co.nz>
Message-ID: <1483634612879-4681062.post@n4.nabble.com>

Hi am using Squid 3.3.8 on Ubuntu 14.04. I have default configuration of
Squid config file . The request is passing via Squid but its not caching the
contents/images/css , everytime am getting TCP_MISS/200 for each request
getting logged in access logs.

###################SQUID CONF entries #####################################
acl SSL_ports port 443
acl Safe_ports port 80          # http
acl Safe_ports port 21          # ftp
acl Safe_ports port 443         # https
acl Safe_ports port 70          # gopher
acl Safe_ports port 210         # wais
acl Safe_ports port 1025-65535  # unregistered ports
acl Safe_ports port 280         # http-mgmt
acl Safe_ports port 488         # gss-http
acl Safe_ports port 591         # filemaker
acl Safe_ports port 777         # multiling http
acl CONNECT method CONNECT
#
# Recommended minimum Access Permission configuration:
#
# Deny requests to certain unsafe ports
http_access deny !Safe_ports
# Deny CONNECT to other than secure SSL ports
http_access deny CONNECT !SSL_ports
# Only allow cachemgr access from localhost
http_access allow localhost manager
http_access deny manager
# We strongly recommend the following be uncommented to protect innocent
# web applications running on the proxy server who think the only
# one who can access services on "localhost" is a local user
http_access deny to_localhost
#
# INSERT YOUR OWN RULE(S) HERE TO ALLOW ACCESS FROM YOUR CLIENTS
#
# Example rule allowing access from your local networks.
# Adapt localnet in the ACL section to list your (internal) IP networks
# from where browsing should be allowed
http_access allow localnet
http_access allow localhost
# And finally deny all other access to this proxy
http_access deny all
# Squid normally listens to port 3128
http_port 3128
# Uncomment and adjust the following to add a disk cache directory.
cache_dir ufs /var/spool/squid 102400 16 256
# Leave coredumps in the first cache dir
coredump_dir /var/spool/squid
#
# Add any of your own refresh_pattern entries above these.
#
refresh_pattern ^ftp:           1440    20%     10080
refresh_pattern ^gopher:        1440    0%      1440
refresh_pattern -i (/cgi-bin/|\?) 0     0%      0
refresh_pattern .               0       20%     4320
# Max Object Size Cache
maximum_object_size 10240 KB
##############################################################


Let me know if Squid 3.3.8 is compatible with Ubuntu 14.04 or not? if not
what are the compatible versions with Ubuntu 14.04 as i cant change Ubuntu
version. 
what changes need to be done to cache the contents ?



--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/Squid-3-3-8-is-available-tp4661037p4681062.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From yvoinov at gmail.com  Thu Jan  5 17:25:51 2017
From: yvoinov at gmail.com (Yuri Voinov)
Date: Thu, 5 Jan 2017 23:25:51 +0600
Subject: [squid-users] Squid 3.3.8 is available
In-Reply-To: <1483634612879-4681062.post@n4.nabble.com>
References: <51E163AF.2020907@treenet.co.nz>
 <1483634612879-4681062.post@n4.nabble.com>
Message-ID: <b221d9d9-b7cf-0b04-0909-efaf4d96baf1@gmail.com>



05.01.2017 22:43, vinay ?????:
> Hi am using Squid 3.3.8 on Ubuntu 14.04. I have default configuration of
> Squid config file . The request is passing via Squid but its not caching the
> contents/images/css , everytime am getting TCP_MISS/200 for each request
> getting logged in access logs.
>
> ###################SQUID CONF entries #####################################
> acl SSL_ports port 443
> acl Safe_ports port 80          # http
> acl Safe_ports port 21          # ftp
> acl Safe_ports port 443         # https
> acl Safe_ports port 70          # gopher
> acl Safe_ports port 210         # wais
> acl Safe_ports port 1025-65535  # unregistered ports
> acl Safe_ports port 280         # http-mgmt
> acl Safe_ports port 488         # gss-http
> acl Safe_ports port 591         # filemaker
> acl Safe_ports port 777         # multiling http
> acl CONNECT method CONNECT
> #
> # Recommended minimum Access Permission configuration:
> #
> # Deny requests to certain unsafe ports
> http_access deny !Safe_ports
> # Deny CONNECT to other than secure SSL ports
> http_access deny CONNECT !SSL_ports
> # Only allow cachemgr access from localhost
> http_access allow localhost manager
> http_access deny manager
> # We strongly recommend the following be uncommented to protect innocent
> # web applications running on the proxy server who think the only
> # one who can access services on "localhost" is a local user
> http_access deny to_localhost
> #
> # INSERT YOUR OWN RULE(S) HERE TO ALLOW ACCESS FROM YOUR CLIENTS
> #
> # Example rule allowing access from your local networks.
> # Adapt localnet in the ACL section to list your (internal) IP networks
> # from where browsing should be allowed
> http_access allow localnet
> http_access allow localhost
> # And finally deny all other access to this proxy
> http_access deny all
> # Squid normally listens to port 3128
> http_port 3128
> # Uncomment and adjust the following to add a disk cache directory.
> cache_dir ufs /var/spool/squid 102400 16 256
> # Leave coredumps in the first cache dir
> coredump_dir /var/spool/squid
> #
> # Add any of your own refresh_pattern entries above these.
> #
> refresh_pattern ^ftp:           1440    20%     10080
> refresh_pattern ^gopher:        1440    0%      1440
> refresh_pattern -i (/cgi-bin/|\?) 0     0%      0
> refresh_pattern .               0       20%     4320
> # Max Object Size Cache
> maximum_object_size 10240 KB
> ##############################################################
>
>
> Let me know if Squid 3.3.8 is compatible with Ubuntu 14.04 or not? if not
> what are the compatible versions with Ubuntu 14.04 as i cant change Ubuntu
> version. 
> what changes need to be done to cache the contents ?
You will never be able to achieve this with the default configuration.

To achieve this, you need to work hard. At the same time, no one will
give you a ready-made solution.
>
>
>
> --
> View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/Squid-3-3-8-is-available-tp4661037p4681062.html
> Sent from the Squid - Users mailing list archive at Nabble.com.
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users

-- 
What is the fundamental difference between the programmer and by a fag?
Fag never become five times to free the memory of one object. Fag will
not use two almost identical string libraries in the same project. Fag
will never write to a mixture of C and C ++. Fag will never pass objects
by pointer. Now you know why these two categories so often mentioned
together, and one of them is worse :)
-------------- next part --------------
A non-text attachment was scrubbed...
Name: 0x613DEC46.asc
Type: application/pgp-keys
Size: 2437 bytes
Desc: not available
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20170105/3aa09eca/attachment.key>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: signature.asc
Type: application/pgp-signature
Size: 473 bytes
Desc: OpenPGP digital signature
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20170105/3aa09eca/attachment.sig>

From frio_cervesa at hotmail.com  Thu Jan  5 23:40:29 2017
From: frio_cervesa at hotmail.com (senor)
Date: Thu, 5 Jan 2017 23:40:29 +0000
Subject: [squid-users] ssl_bump with intermediate CA
Message-ID: <BN6PR17MB11400A4762F1969D486FEEFAF7600@BN6PR17MB1140.namprd17.prod.outlook.com>

Hello All.
I'd like clarification of the documentation at
http://wiki.squid-cache.org/ConfigExamples/Intercept/SslBumpWithIntermediateCA

In section "CA certificate preparation" it is stated that a file should
be created with "intermediate CA2 followed by root CA1 in PEM format".
CA1 is the cert trusted by the clients. CA2 is used to sign the mimicked
certs. And finally the statement "Now Squid can send the intermediate
CA2 public key with root CA1 to client and does not need to install
intermediate CA2 to clients."

The specification states that the clients MUST NOT use CA1 provided in
the TLS exchange. CA1 must be (and in this scenario is) already included
in its trusted store of CAs.

As I understand it, the TLS exchange with the client for a bumped
connection should have the mimicked server cert followed by the
intermediate cert (CA2) and that's all. The client completes the chain
with the already trusted CA1.

The example file created is used for cafile= option to http_port which
is supposed to be for verifying client certs which is not part of this
scenario.

This is getting a little long-winded so I'll wait to see what anyone has
to say about my assumptions or understanding.

Thanks,
Senor


From bruce.rosenberg.au at gmail.com  Fri Jan  6 00:57:08 2017
From: bruce.rosenberg.au at gmail.com (Bruce Rosenberg)
Date: Fri, 6 Jan 2017 11:57:08 +1100
Subject: [squid-users] ssl_bump with intermediate CA
In-Reply-To: <BN6PR17MB11400A4762F1969D486FEEFAF7600@BN6PR17MB1140.namprd17.prod.outlook.com>
References: <BN6PR17MB11400A4762F1969D486FEEFAF7600@BN6PR17MB1140.namprd17.prod.outlook.com>
Message-ID: <CAHaxnUKG6-r_s_POpU7kVB8OkHnwypKFZPD8j2NxnnTF02bS6Q@mail.gmail.com>

The cafile option specifies the "chain" file squid should send back to the
client along with the cert, exactly as you would normally do with Apache
httpd or Nginx.
In the example the generated server cert is depth 0, CA2 is depth 1 and CA1
is depth 2.
If the client has CA1 installed as a trust anchor then technically you
don't need to send CA1 as it is discarded by the client once the trust
relationship for CA2 is established.
It's good practice to send the full chain though as it makes
troubleshooting easier.
>From a client perspective you can quickly grab the whole chain with openssl
s_client and check if CA1 is in the trust store.

On Fri, Jan 6, 2017 at 10:40 AM, senor <frio_cervesa at hotmail.com> wrote:

> Hello All.
> I'd like clarification of the documentation at
> http://wiki.squid-cache.org/ConfigExamples/Intercept/
> SslBumpWithIntermediateCA
>
> In section "CA certificate preparation" it is stated that a file should
> be created with "intermediate CA2 followed by root CA1 in PEM format".
> CA1 is the cert trusted by the clients. CA2 is used to sign the mimicked
> certs. And finally the statement "Now Squid can send the intermediate
> CA2 public key with root CA1 to client and does not need to install
> intermediate CA2 to clients."
>
> The specification states that the clients MUST NOT use CA1 provided in
> the TLS exchange. CA1 must be (and in this scenario is) already included
> in its trusted store of CAs.
>
> As I understand it, the TLS exchange with the client for a bumped
> connection should have the mimicked server cert followed by the
> intermediate cert (CA2) and that's all. The client completes the chain
> with the already trusted CA1.
>
> The example file created is used for cafile= option to http_port which
> is supposed to be for verifying client certs which is not part of this
> scenario.
>
> This is getting a little long-winded so I'll wait to see what anyone has
> to say about my assumptions or understanding.
>
> Thanks,
> Senor
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20170106/c128e4a3/attachment.htm>

From garryd at comnet.uz  Fri Jan  6 04:33:11 2017
From: garryd at comnet.uz (Garri Djavadyan)
Date: Fri, 06 Jan 2017 09:33:11 +0500
Subject: [squid-users] ssl_bump with intermediate CA
In-Reply-To: <BN6PR17MB11400A4762F1969D486FEEFAF7600@BN6PR17MB1140.namprd17.prod.outlook.com>
References: <BN6PR17MB11400A4762F1969D486FEEFAF7600@BN6PR17MB1140.namprd17.prod.outlook.com>
Message-ID: <1483677191.13034.1.camel@comnet.uz>

On Thu, 2017-01-05 at 23:40 +0000, senor wrote:
> Hello All.
> I'd like clarification of the documentation at
> http://wiki.squid-cache.org/ConfigExamples/Intercept/SslBumpWithInter
> mediateCA
> 
> In section "CA certificate preparation" it is stated that a file
> should
> be created with "intermediate CA2 followed by root CA1 in PEM
> format".
> CA1 is the cert trusted by the clients. CA2 is used to sign the
> mimicked
> certs. And finally the statement "Now Squid can send the intermediate
> CA2 public key with root CA1 to client and does not need to install
> intermediate CA2 to clients."
> 
> The specification states that the clients MUST NOT use CA1 provided
> in
> the TLS exchange. CA1 must be (and in this scenario is) already
> included
> in its trusted store of CAs.
> 
> As I understand it, the TLS exchange with the client for a bumped
> connection should have the mimicked server cert followed by the
> intermediate cert (CA2) and that's all. The client completes the
> chain
> with the already trusted CA1.
> 
> The example file created is used for cafile= option to http_port
> which
> is supposed to be for verifying client certs which is not part of
> this
> scenario.
> 
> This is getting a little long-winded so I'll wait to see what anyone
> has
> to say about my assumptions or understanding.
> 
> Thanks,
> Senor

Hi Senor,

You are right, it is not required to send root CA cert to a client. It
is already installed in client's cert store. You can find more details
in bug report 3426 [1] (comments 11 and 13).

[1] http://bugs.squid-cache.org/show_bug.cgi?id=3426


Garri


From frio_cervesa at hotmail.com  Fri Jan  6 08:27:40 2017
From: frio_cervesa at hotmail.com (senor)
Date: Fri, 6 Jan 2017 08:27:40 +0000
Subject: [squid-users] ssl_bump with intermediate CA
In-Reply-To: <CAHaxnUKG6-r_s_POpU7kVB8OkHnwypKFZPD8j2NxnnTF02bS6Q@mail.gmail.com>
References: <BN6PR17MB11400A4762F1969D486FEEFAF7600@BN6PR17MB1140.namprd17.prod.outlook.com>
 <CAHaxnUKG6-r_s_POpU7kVB8OkHnwypKFZPD8j2NxnnTF02bS6Q@mail.gmail.com>
Message-ID: <BN6PR17MB11404837A94EEF5C7976CBBBF7630@BN6PR17MB1140.namprd17.prod.outlook.com>

Thank you for the response but I think my question is still unanswered.
Comments below:

On 1/5/2017 16:57, Bruce Rosenberg wrote:
> The cafile option specifies the "chain" file squid should send back to
> the client along with the cert, exactly as you would normally do with
> Apache httpd or Nginx.
(For clarity: I'm using 3.5.23. cafile was replaced in squid-4)
This may be what cafile is used for but that does not match the
directive description.
http://www.squid-cache.org/Versions/v3/3.5/cfgman/http_port.html
My suspicion is that the description is a confusion between the same
option in openssl and web server options (Apache SSLCACertificateFile
and similar).
What's it really used for? Chain completion, client cert verification or
both?

> In the example the generated server cert is depth 0, CA2 is depth 1 and
> CA1 is depth 2.
> If the client has CA1 installed as a trust anchor then technically you
> don't need to send CA1 as it is discarded by the client once the trust
> relationship for CA2 is established.
> It's good practice to send the full chain though as it makes
> troubleshooting easier.
> From a client perspective you can quickly grab the whole chain with
> openssl s_client and check if CA1 is in the trust store.
I have to disagree with this. The anchor (CA1) is discarded regardless.
It cannot be used. If included it bloats the TLS handshake. Even openssl
will discard it and then look in the trusted CA store.

I see with a packet cap that the mimicked server cert and the signing
cert are both included even without the cafile option specified.

So is it safe to say that the referenced wiki page has just become
outdated? If cafile is used to fill in the cert chain it wouldn't be
needed unless there were additional intermediate certs between the mitm
cert and the trusted CA known to the client. (As in CA1 is trusted by
clients, CA1 signs CA2 which signs CA3 which is used as MITM cert,
cafile=CA2)

Thanks for comments.
Senor

> 
> On Fri, Jan 6, 2017 at 10:40 AM, senor <frio_cervesa at hotmail.com
> <mailto:frio_cervesa at hotmail.com>> wrote:
> 
>     Hello All.
>     I'd like clarification of the documentation at
>     http://wiki.squid-cache.org/ConfigExamples/Intercept/SslBumpWithIntermediateCA
>     <http://wiki.squid-cache.org/ConfigExamples/Intercept/SslBumpWithIntermediateCA>
> 
>     In section "CA certificate preparation" it is stated that a file should
>     be created with "intermediate CA2 followed by root CA1 in PEM format".
>     CA1 is the cert trusted by the clients. CA2 is used to sign the mimicked
>     certs. And finally the statement "Now Squid can send the intermediate
>     CA2 public key with root CA1 to client and does not need to install
>     intermediate CA2 to clients."
> 
>     The specification states that the clients MUST NOT use CA1 provided in
>     the TLS exchange. CA1 must be (and in this scenario is) already included
>     in its trusted store of CAs.
> 
>     As I understand it, the TLS exchange with the client for a bumped
>     connection should have the mimicked server cert followed by the
>     intermediate cert (CA2) and that's all. The client completes the chain
>     with the already trusted CA1.
> 
>     The example file created is used for cafile= option to http_port which
>     is supposed to be for verifying client certs which is not part of this
>     scenario.
> 
>     This is getting a little long-winded so I'll wait to see what anyone has
>     to say about my assumptions or understanding.
> 
>     Thanks,
>     Senor
> 
>     _______________________________________________
>     squid-users mailing list
>     squid-users at lists.squid-cache.org
>     <mailto:squid-users at lists.squid-cache.org>
>     http://lists.squid-cache.org/listinfo/squid-users
>     <http://lists.squid-cache.org/listinfo/squid-users>
> 
> 


From david at articatech.com  Fri Jan  6 09:12:46 2017
From: david at articatech.com (David Touzeau)
Date: Fri, 6 Jan 2017 10:12:46 +0100
Subject: [squid-users] [3.5x]: identd lookup made before proxy_protocol
	checking and failed [help]
Message-ID: <008901d267fd$0bb40660$231c1320$@articatech.com>

Added in bugtrack

http://bugs.squid-cache.org/show_bug.cgi?id=4657


-----Message d'origine-----
De : squid-users [mailto:squid-users-bounces at lists.squid-cache.org] De la part de David Touzeau
Envoy? : jeudi 5 janvier 2017 17:10
? : squid-users at lists.squid-cache.org
Objet : [squid-users] [3.5x]: identd lookup made before proxy_protocol checking and failed [help]


Hi,

We need to use ident daemon in order to authenticate users.

Squid works fine when computers are directly connected to the proxy.

We have added HaProxy * * * Load-balancer * * * using *proxy_protocol* between users and 2 Squid proxies With the load balancer, squid want to query identd port directly on the load balancer but not on the client source IP address.
If you see this piece of logs, you can see that the source client address is correctly understood by Squid but * * after * *  the ident verification.


How can i fix this behaviour ?
Best regards,

Browser: 192.168.1.50
Load balancer: 192.168.1.60
Squid: 192.168.1.61


2017/01/05 16:55:19.290 kid1| 5,5| AsyncCall.cc(93) ScheduleCall:
TcpAcceptor.cc(317) will call httpAccept(local=192.168.1.62:8080
remote=192.168.1.61:54798 FD 12 flags=1, MXID_244) [call229910]
2017/01/05 16:55:19.290 kid1| 5,5| ModEpoll.cc(116) SetSelect: FD 29,
type=1, handler=1, client_data=0x18cb298, timeout=0
2017/01/05 16:55:19.290 kid1| 5,5| AsyncCallQueue.cc(55) fireNext: entering
httpAccept(local=192.168.1.62:8080 remote=192.168.1.61:54798 FD 12 flags=1,
MXID_244)
2017/01/05 16:55:19.290 kid1| 5,5| AsyncCall.cc(38) make: make call
httpAccept [call229910]
2017/01/05 16:55:19.290 kid1| 33,4| client_side.cc(3705) httpAccept:
local=192.168.1.62:8080 remote=192.168.1.61:54798 FD 12 flags=1: accepted
2017/01/05 16:55:19.290 kid1| 93,5| AsyncJob.cc(34) AsyncJob: AsyncJob
constructed, this=0x1eae810 type=Http::Server [job1085]
2017/01/05 16:55:19.290 kid1| 93,5| AsyncCall.cc(26) AsyncCall: The
AsyncCall AsyncJob::start constructed, this=0x1ab7850 [call229911]
2017/01/05 16:55:19.290 kid1| 93,5| AsyncCall.cc(93) ScheduleCall:
AsyncJob.cc(26) will call AsyncJob::start() [call229911]
2017/01/05 16:55:19.290 kid1| 5,5| AsyncCallQueue.cc(57) fireNext: leaving
httpAccept(local=192.168.1.62:8080 remote=192.168.1.61:54798 FD 12 flags=1,
MXID_244)
2017/01/05 16:55:19.290 kid1| 93,5| AsyncCallQueue.cc(55) fireNext: entering
AsyncJob::start()
2017/01/05 16:55:19.290 kid1| 93,5| AsyncCall.cc(38) make: make call
AsyncJob::start [call229911]
2017/01/05 16:55:19.290 kid1| 93,5| AsyncJob.cc(123) callStart: Http::Server
status in: [ job1085]
2017/01/05 16:55:19.290 kid1| 33,5| AsyncCall.cc(26) AsyncCall: The
AsyncCall ConnStateData::connStateClosed constructed, this=0x1e5a750
[call229912]
2017/01/05 16:55:19.290 kid1| 5,5| comm.cc(994) comm_add_close_handler:
comm_add_close_handler: FD 12, AsyncCall=0x1e5a750*1
2017/01/05 16:55:19.290 kid1| 28,3| Checklist.cc(70) preCheck:
0x7ffed50496a0 checking fast rules
2017/01/05 16:55:19.290 kid1| 28,5| Checklist.cc(346) fastCheck:
aclCheckFast: list: 0x18d4128
2017/01/05 16:55:19.290 kid1| 28,5| Acl.cc(138) matches: checking
ident_lookup_access
2017/01/05 16:55:19.290 kid1| 28,5| Checklist.cc(400) bannedAction: Action
'ALLOWED/0' is not banned
2017/01/05 16:55:19.290 kid1| 28,5| Acl.cc(138) matches: checking
ident_lookup_access#1
2017/01/05 16:55:19.290 kid1| 28,5| Acl.cc(138) matches: checking
ident_aware_hosts
2017/01/05 16:55:19.290 kid1| 28,3| Ip.cc(539) match: aclIpMatchIp:
'192.168.1.61:54798' found
2017/01/05 16:55:19.290 kid1| 28,3| Acl.cc(158) matches: checked:
ident_aware_hosts = 1
2017/01/05 16:55:19.290 kid1| 28,3| Acl.cc(158) matches: checked:
ident_lookup_access#1 = 1
2017/01/05 16:55:19.290 kid1| 28,3| Acl.cc(158) matches: checked:
ident_lookup_access = 1
2017/01/05 16:55:19.290 kid1| 28,3| Checklist.cc(63) markFinished:
0x7ffed50496a0 answer ALLOWED for match
2017/01/05 16:55:19.290 kid1| 30,3| AsyncCall.cc(26) AsyncCall: The
AsyncCall Ident::ConnectDone constructed, this=0x18c9f10 [call229913]
2017/01/05 16:55:19.290 kid1| 93,5| AsyncJob.cc(34) AsyncJob: AsyncJob
constructed, this=0x1ea4838 type=Comm::ConnOpener [job1086]
2017/01/05 16:55:19.290 kid1| 93,5| AsyncCall.cc(26) AsyncCall: The
AsyncCall AsyncJob::start constructed, this=0x18cd970 [call229914]
2017/01/05 16:55:19.290 kid1| 93,5| AsyncCall.cc(93) ScheduleCall:
AsyncJob.cc(26) will call AsyncJob::start() [call229914]
2017/01/05 16:55:19.290 kid1| 28,4| FilledChecklist.cc(66)
~ACLFilledChecklist: ACLFilledChecklist destroyed 0x7ffed50496a0
2017/01/05 16:55:19.290 kid1| 28,4| Checklist.cc(197) ~ACLChecklist:
ACLChecklist::~ACLChecklist: destroyed 0x7ffed50496a0
2017/01/05 16:55:19.290 kid1| 28,3| Checklist.cc(70) preCheck:
0x7ffed5049430 checking fast rules
2017/01/05 16:55:19.290 kid1| 28,5| Checklist.cc(346) fastCheck:
aclCheckFast: list: 0x18dd488
2017/01/05 16:55:19.290 kid1| 28,5| Acl.cc(138) matches: checking
proxy_protocol_access
2017/01/05 16:55:19.290 kid1| 28,5| Checklist.cc(400) bannedAction: Action
'ALLOWED/0' is not banned
2017/01/05 16:55:19.290 kid1| 28,5| Acl.cc(138) matches: checking
proxy_protocol_access#1
2017/01/05 16:55:19.290 kid1| 28,5| Acl.cc(138) matches: checking all
2017/01/05 16:55:19.290 kid1| 28,3| Ip.cc(539) match: aclIpMatchIp:
'192.168.1.61:54798' found
2017/01/05 16:55:19.290 kid1| 28,3| Acl.cc(158) matches: checked: all = 1
2017/01/05 16:55:19.290 kid1| 28,3| Acl.cc(158) matches: checked:
proxy_protocol_access#1 = 1
2017/01/05 16:55:19.290 kid1| 28,3| Acl.cc(158) matches: checked:
proxy_protocol_access = 1
2017/01/05 16:55:19.290 kid1| 28,3| Checklist.cc(63) markFinished:
0x7ffed5049430 answer ALLOWED for match
2017/01/05 16:55:19.290 kid1| 28,4| FilledChecklist.cc(66)
~ACLFilledChecklist: ACLFilledChecklist destroyed 0x7ffed5049430
2017/01/05 16:55:19.290 kid1| 28,4| Checklist.cc(197) ~ACLChecklist:
ACLChecklist::~ACLChecklist: destroyed 0x7ffed5049430
2017/01/05 16:55:19.290 kid1| 28,4| FilledChecklist.cc(66)
~ACLFilledChecklist: ACLFilledChecklist destroyed 0x7ffed5049860
2017/01/05 16:55:19.290 kid1| 28,4| Checklist.cc(197) ~ACLChecklist:
ACLChecklist::~ACLChecklist: destroyed 0x7ffed5049860
2017/01/05 16:55:19.290 kid1| 33,5| AsyncCall.cc(26) AsyncCall: The
AsyncCall Http::Server::requestTimeout constructed, this=0x19524c0
[call229915]
2017/01/05 16:55:19.290 kid1| 5,3| comm.cc(553) commSetConnTimeout:
local=192.168.1.62:8080 remote=192.168.1.61:54798 FD 12 flags=1 timeout 300
2017/01/05 16:55:19.290 kid1| 33,4| client_side.cc(231) readSomeData:
local=192.168.1.62:8080 remote=192.168.1.61:54798 FD 12 flags=1: reading
request...
2017/01/05 16:55:19.291 kid1| 33,5| AsyncCall.cc(26) AsyncCall: The
AsyncCall ConnStateData::clientReadRequest constructed, this=0x1f4cc10
[call229916]
2017/01/05 16:55:19.291 kid1| 5,5| Read.cc(58) comm_read_base: comm_read,
queueing read for local=192.168.1.62:8080 remote=192.168.1.61:54798 FD 12
flags=1; asynCall 0x1f4cc10*1
2017/01/05 16:55:19.291 kid1| 5,5| ModEpoll.cc(116) SetSelect: FD 12,
type=1, handler=1, client_data=0x7fe9e8ff0678, timeout=0
2017/01/05 16:55:19.291 kid1| 93,5| AsyncJob.cc(152) callEnd: Http::Server
status out: [ job1085]
2017/01/05 16:55:19.291 kid1| 93,5| AsyncCallQueue.cc(57) fireNext: leaving
AsyncJob::start()
2017/01/05 16:55:19.291 kid1| 93,5| AsyncCallQueue.cc(55) fireNext: entering
AsyncJob::start()
2017/01/05 16:55:19.291 kid1| 93,5| AsyncCall.cc(38) make: make call
AsyncJob::start [call229914]
2017/01/05 16:55:19.291 kid1| 93,5| AsyncJob.cc(123) callStart:
Comm::ConnOpener status in: [ job1086]
2017/01/05 16:55:19.291 kid1| 50,3| comm.cc(347) comm_openex: comm_openex:
Attempt open socket for: 192.168.1.62
2017/01/05 16:55:19.291 kid1| 50,3| comm.cc(388) comm_openex: comm_openex:
Opened socket local=192.168.1.62 remote=[::] FD 13 flags=1 : family=2,
type=1, protocol=6
2017/01/05 16:55:19.291 kid1| 5,5| comm.cc(420) comm_init_opened:
local=192.168.1.62 remote=[::] FD 13 flags=1 is a new socket
2017/01/05 16:55:19.291 kid1| 51,3| fd.cc(198) fd_open: fd_open() FD 13 
2017/01/05 16:55:19.291 kid1| 50,6| comm.cc(209) commBind: commBind: bind
socket FD 13 to 192.168.1.62
2017/01/05 16:55:19.291 kid1| 5,4| AsyncCall.cc(26) AsyncCall: The AsyncCall
Comm::ConnOpener::earlyAbort constructed, this=0x1fa6ea0 [call229917]
2017/01/05 16:55:19.291 kid1| 5,5| comm.cc(994) comm_add_close_handler:
comm_add_close_handler: FD 13, AsyncCall=0x1fa6ea0*1
2017/01/05 16:55:19.291 kid1| 5,4| AsyncCall.cc(26) AsyncCall: The AsyncCall
Comm::ConnOpener::timeout constructed, this=0x1fe7e90 [call229918]

* * * * Squid try to connect to the Load-Balancer on 113 port instead
192.168.1.50 * * * *

2017/01/05 16:55:19.291 kid1| 5,3| ConnOpener.cc(289) createFd:
local=192.168.1.62 remote=192.168.1.61:113 flags=1 will timeout in 3
2017/01/05 16:55:19.291 kid1| 5,5| comm.cc(644) comm_connect_addr: sock=13,
addrinfo( flags=4, family=2, socktype=1, protocol=6, &addr=0x1f788e0,
addrlen=16 )
2017/01/05 16:55:19.291 kid1| 5,5| ConnOpener.cc(343) doConnect:
local=192.168.1.62 remote=192.168.1.61:113 flags=1: Comm::INPROGRESS
2017/01/05 16:55:19.291 kid1| 5,5| ModEpoll.cc(116) SetSelect: FD 13,
type=2, handler=1, client_data=0x1f788e0, timeout=0
2017/01/05 16:55:19.291 kid1| 93,5| AsyncJob.cc(152) callEnd:
Comm::ConnOpener status out: [ job1086]
2017/01/05 16:55:19.291 kid1| 93,5| AsyncCallQueue.cc(57) fireNext: leaving
AsyncJob::start()
2017/01/05 16:55:19.291 kid1| 5,3| IoCallback.cc(116) finish: called for
local=192.168.1.62:8080 remote=192.168.1.61:54798 FD 12 flags=1 (0, 0)
2017/01/05 16:55:19.291 kid1| 33,5| AsyncCall.cc(93) ScheduleCall:
IoCallback.cc(135) will call
ConnStateData::clientReadRequest(local=192.168.1.62:8080
remote=192.168.1.61:54798 FD 12 flags=1, data=0x1eae648) [call229916]
2017/01/05 16:55:19.291 kid1| 33,5| AsyncCallQueue.cc(55) fireNext: entering
ConnStateData::clientReadRequest(local=192.168.1.62:8080
remote=192.168.1.61:54798 FD 12 flags=1, data=0x1eae648)
2017/01/05 16:55:19.291 kid1| 33,5| AsyncCall.cc(38) make: make call
ConnStateData::clientReadRequest [call229916]
2017/01/05 16:55:19.291 kid1| 33,5| AsyncJob.cc(123) callStart: Http::Server
status in: [ job1085]
2017/01/05 16:55:19.291 kid1| 33,5| client_side.cc(3283) clientReadRequest:
local=192.168.1.62:8080 remote=192.168.1.61:54798 FD 12 flags=1
2017/01/05 16:55:19.291 kid1| 24,7| SBuf.cc(193) rawSpace: reserving 16382
for SBuf18795
2017/01/05 16:55:19.291 kid1| 24,7| SBuf.cc(935) reAlloc: SBuf18795 new
store capacity: 16384
2017/01/05 16:55:19.291 kid1| 5,3| Read.cc(91) ReadNow:
local=192.168.1.62:8080 remote=192.168.1.61:54798 FD 12 flags=1, size 16382,
retval 409, errno 0
2017/01/05 16:55:19.291 kid1| 24,7| SBuf.cc(242) append: from c-string to id
SBuf18795
2017/01/05 16:55:19.291 kid1| 24,7| SBuf.cc(193) rawSpace: reserving 409 for
SBuf18795
2017/01/05 16:55:19.291 kid1| 24,7| SBuf.cc(200) rawSpace: SBuf18795 not
growing
2017/01/05 16:55:19.291 kid1| 33,5| client_side.cc(3232)
clientParseRequests: local=192.168.1.62:8080 remote=192.168.1.61:54798 FD 12
flags=1: attempting to parse
2017/01/05 16:55:19.291 kid1| 24,7| SBuf.cc(802) findFirstNotOf: first not
of characterset non-LF in id SBuf18808
2017/01/05 16:55:19.291 kid1| 24,7| SBuf.cc(139) assign: assigning SBuf18807
from SBuf18809
2017/01/05 16:55:19.291 kid1| 24,7| SBuf.cc(139) assign: assigning SBuf18795
from SBuf18803
2017/01/05 16:55:19.291 kid1| 24,7| SBuf.cc(139) assign: assigning SBuf18803
from SBuf18807
2017/01/05 16:55:19.291 kid1| 24,7| SBuf.cc(802) findFirstNotOf: first not
of characterset IP Address in id SBuf18818
2017/01/05 16:55:19.291 kid1| 24,7| SBuf.cc(139) assign: assigning SBuf18816
from SBuf18819
2017/01/05 16:55:19.291 kid1| 24,7| SBuf.cc(802) findFirstNotOf: first not
of characterset IP Address in id SBuf18821
2017/01/05 16:55:19.291 kid1| 24,7| SBuf.cc(139) assign: assigning SBuf18817
from SBuf18822
2017/01/05 16:55:19.291 kid1| 24,7| SBuf.cc(193) rawSpace: reserving 1 for
SBuf18816
2017/01/05 16:55:19.291 kid1| 24,7| SBuf.cc(935) reAlloc: SBuf18816 new
store capacity: 40
2017/01/05 16:55:19.291 kid1| 24,7| SBuf.cc(193) rawSpace: reserving 1 for
SBuf18817
2017/01/05 16:55:19.291 kid1| 24,7| SBuf.cc(935) reAlloc: SBuf18817 new
store capacity: 40

* * * * Proxy protocol checking, OK squid get the client source IP address
but ACL failed the ident checking was failed before * * * *
2017/01/05 16:55:19.291 kid1| 33,5| client_side.cc(3105) parseProxy1p0:
PROXY/1.0 protocol on connection local=192.168.1.62:8080
remote=192.168.1.61:54798 FD 12 flags=1
2017/01/05 16:55:19.291 kid1| 33,5| client_side.cc(3110) parseProxy1p0:
PROXY/1.0 upgrade: local=192.168.1.61:8080 remote=192.168.1.50:59019 FD 12
flags=1
2017/01/05 16:55:19.291 kid1| 24,7| SBuf.cc(193) rawSpace: reserving 1 for
SBuf18795
2017/01/05 16:55:19.291 kid1| 24,7| SBuf.cc(200) rawSpace: SBuf18795 not
growing
2017/01/05 16:55:19.291 kid1| 74,5| HttpParser.cc(37) reset: Request buffer
is GET http://www.squid-cache.org/Images/img4.jpg HTTP/1.1


_______________________________________________
squid-users mailing list
squid-users at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-users



From squid3 at treenet.co.nz  Fri Jan  6 10:06:23 2017
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Fri, 06 Jan 2017 23:06:23 +1300
Subject: [squid-users] ssl_bump with intermediate CA
In-Reply-To: <BN6PR17MB11404837A94EEF5C7976CBBBF7630@BN6PR17MB1140.namprd17.prod.outlook.com>
References: <BN6PR17MB11400A4762F1969D486FEEFAF7600@BN6PR17MB1140.namprd17.prod.outlook.com>
 <CAHaxnUKG6-r_s_POpU7kVB8OkHnwypKFZPD8j2NxnnTF02bS6Q@mail.gmail.com>
 <BN6PR17MB11404837A94EEF5C7976CBBBF7630@BN6PR17MB1140.namprd17.prod.outlook.com>
Message-ID: <161215919c9cb0d150047cf2702de615@treenet.co.nz>

On 2017-01-06 21:27, senor wrote:
> Thank you for the response but I think my question is still unanswered.
> Comments below:
> 
> On 1/5/2017 16:57, Bruce Rosenberg wrote:
>> The cafile option specifies the "chain" file squid should send back to
>> the client along with the cert, exactly as you would normally do with
>> Apache httpd or Nginx.
> (For clarity: I'm using 3.5.23. cafile was replaced in squid-4)
> This may be what cafile is used for but that does not match the
> directive description.
> http://www.squid-cache.org/Versions/v3/3.5/cfgman/http_port.html
> My suspicion is that the description is a confusion between the same
> option in openssl and web server options (Apache SSLCACertificateFile
> and similar).
> What's it really used for? Chain completion, client cert verification 
> or
> both?

It is used for chain completion. *which* chain is being completed 
depends on the traffic mode.

clientca= should be the one used *only* for client cert verification (I 
think). But neither was used consistently in Squid-3, so YMMV based on 
the traffic modes.

> 
>> In the example the generated server cert is depth 0, CA2 is depth 1 
>> and
>> CA1 is depth 2.
>> If the client has CA1 installed as a trust anchor then technically you
>> don't need to send CA1 as it is discarded by the client once the trust
>> relationship for CA2 is established.
>> It's good practice to send the full chain though as it makes
>> troubleshooting easier.
>> From a client perspective you can quickly grab the whole chain with
>> openssl s_client and check if CA1 is in the trust store.
> I have to disagree with this. The anchor (CA1) is discarded regardless.
> It cannot be used. If included it bloats the TLS handshake. Even 
> openssl
> will discard it and then look in the trusted CA store.
> 
> I see with a packet cap that the mimicked server cert and the signing
> cert are both included even without the cafile option specified.
> 
> So is it safe to say that the referenced wiki page has just become
> outdated? If cafile is used to fill in the cert chain it wouldn't be
> needed unless there were additional intermediate certs between the mitm
> cert and the trusted CA known to the client. (As in CA1 is trusted by
> clients, CA1 signs CA2 which signs CA3 which is used as MITM cert,
> cafile=CA2)

That wiki page was incorrect at the time of creation. But the author 
refuses to agree that root cert are discarded so I left it there instead 
of inciting an edit war. Saving the root CA into the file should be 
harmless anyway.

Amos



From squid3 at treenet.co.nz  Fri Jan  6 10:26:34 2017
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Fri, 06 Jan 2017 23:26:34 +1300
Subject: [squid-users] [3.5x]: identd lookup made before proxy_protocol
	checking and failed [help]
In-Reply-To: <008901d267fd$0bb40660$231c1320$@articatech.com>
References: <008901d267fd$0bb40660$231c1320$@articatech.com>
Message-ID: <1f32a2106420fe90ddf8ab09062c36dd@treenet.co.nz>

On 2017-01-06 22:12, David Touzeau wrote:
> Added in bugtrack
> 
> http://bugs.squid-cache.org/show_bug.cgi?id=4657
> 
> 
> -----Message d'origine-----
> De : David Touzeau
> 
> Hi,
> 
> We need to use ident daemon in order to authenticate users.
> 
> Squid works fine when computers are directly connected to the proxy.
> 
> We have added HaProxy * * * Load-balancer * * * using *proxy_protocol*
> between users and 2 Squid proxies With the load balancer, squid want
> to query identd port directly on the load balancer but not on the
> client source IP address.
> If you see this piece of logs, you can see that the source client
> address is correctly understood by Squid but * * after * *  the ident
> verification.
> 
> 
> How can i fix this behaviour ?

IDENT relies on using the exact random TCP port from the connection the 
client opened to HAProxy being used as part of the IDENT connection back 
to the client.

Since there is HAProxy between Squid and the client, Squid will be 
unable to open the port already in use by the HAProxy client-connection.

So, HAProxy has to be the agent doing the IDENT lookup and sending the 
ident info to Squid - probably as part of the PROXY wrapper.

Amos


From squid3 at treenet.co.nz  Fri Jan  6 10:40:30 2017
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Fri, 06 Jan 2017 23:40:30 +1300
Subject: [squid-users] Squid memory leak on ubuntu 14.04
In-Reply-To: <1483630217278-4681060.post@n4.nabble.com>
References: <CAJGZ0h4bmkcMF7V6WGsfh4uLB-by27t8esB0SnRjudFPVCTt9g@mail.gmail.com>
 <5658838F.7050707@urlfilterdb.com>
 <CAJGZ0h5s9k7txGMTN1tP4wdhKvkb=z_kuK8E5t5jwBpgKj8pUg@mail.gmail.com>
 <565BC47E.6070408@treenet.co.nz>
 <CAJGZ0h4AWgyHbwkujCiUpRPf8AbhegpdxaHzRVNuOCasd-Ho0w@mail.gmail.com>
 <565C0EFE.40904@treenet.co.nz>
 <CAJGZ0h6oTCu2OJqk8u-+X4eaeDiHyawg76xJPskjG=tfv-CESA@mail.gmail.com>
 <CAJGZ0h5Cf0936+5i17UCVN7vm+Y=KPg_+bM0uK1fz1+kM0sv4Q@mail.gmail.com>
 <1483630217278-4681060.post@n4.nabble.com>
Message-ID: <991a6eaaac7178742e80c15045a128a5@treenet.co.nz>

On 2017-01-06 04:30, vinay wrote:

> 
> Let me know if Squid 3.3.8 is compatible with Ubuntu 14.04 or not? if 
> not
> what are the compatible versions with Ubuntu 14.04 as i cant change 
> Ubuntu
> version.

That has nothing to do with caching of HTTP data. You should be able to 
build and install a more current Squid-3.x version that Ubuntu, its just 
a matter of whether you know how to build software for Ubuntu.


> what changes need to be done to cache the contents ?
> 

And what does any of this have to do with memory leaks?

Amos


From david at articatech.com  Fri Jan  6 12:14:47 2017
From: david at articatech.com (David Touzeau)
Date: Fri, 6 Jan 2017 13:14:47 +0100
Subject: [squid-users] [3.5x]: identd lookup made before
	proxy_protocol	checking and failed [help]
In-Reply-To: <1f32a2106420fe90ddf8ab09062c36dd@treenet.co.nz>
References: <008901d267fd$0bb40660$231c1320$@articatech.com>
 <1f32a2106420fe90ddf8ab09062c36dd@treenet.co.nz>
Message-ID: <00bb01d26816$78f07740$6ad165c0$@articatech.com>



-----Message d'origine-----
De : squid-users [mailto:squid-users-bounces at lists.squid-cache.org] De la
part de Amos Jeffries
Envoy? : vendredi 6 janvier 2017 11:27
? : squid-users at lists.squid-cache.org
Objet : Re: [squid-users] [3.5x]: identd lookup made before proxy_protocol
checking and failed [help]

On 2017-01-06 22:12, David Touzeau wrote:
> Added in bugtrack
>
> http://bugs.squid-cache.org/show_bug.cgi?id=4657
>
>
> -----Message d'origine-----
> De : David Touzeau
>
> Hi,
>
> We need to use ident daemon in order to authenticate users.
>
> Squid works fine when computers are directly connected to the proxy.
>
> We have added HaProxy * * * Load-balancer * * * using *proxy_protocol*
> between users and 2 Squid proxies With the load balancer, squid want
> to query identd port directly on the load balancer but not on the
> client source IP address.
> If you see this piece of logs, you can see that the source client
> address is correctly understood by Squid but * * after * *  the ident
> verification.
>
>
> How can i fix this behaviour ?

IDENT relies on using the exact random TCP port from the connection the
client opened to HAProxy being used as part of the IDENT connection back to
the client.

Since there is HAProxy between Squid and the client, Squid will be unable to
open the port already in use by the HAProxy client-connection.

So, HAProxy has to be the agent doing the IDENT lookup and sending the ident
info to Squid - probably as part of the PROXY wrapper.

Amos
_______________________________________________
squid-users mailing list
squid-users at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-users


Hi Amos,

It is more complicated to found a "load-balancer" that supports ident 
lookup.
Are you considering this behaviour as a "Feature request" instead bug ?














From baborucki at gmail.com  Fri Jan  6 18:35:39 2017
From: baborucki at gmail.com (boruc)
Date: Fri, 6 Jan 2017 10:35:39 -0800 (PST)
Subject: [squid-users] Is it possible to modify cached object?
Message-ID: <1483727739416-4681073.post@n4.nabble.com>

Hi everyone,

I am facing a really big problem (for me). I've set up a home network (few
PCs, some mobiles) with squid proxying all requests. Is it possible to
change cached objects manually? Let's say I have an object that contains
/www.example.com/ page HTML source and what I want to do is e.g. change some
words so users will see modified pages. I won't do this with HTTPS of
course, just HTTP.

I tried to do that (I've changed one letter, so content length won't
change), but after refreshing a page that cached file disappeared. Here's my
refresh_pattern config section:



Or maybe there's another way to achieve my goal with squid?

I'd be grateful for any help.



--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/Is-it-possible-to-modify-cached-object-tp4681073.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From rousskov at measurement-factory.com  Fri Jan  6 19:21:16 2017
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Fri, 6 Jan 2017 12:21:16 -0700
Subject: [squid-users] Is it possible to modify cached object?
In-Reply-To: <1483727739416-4681073.post@n4.nabble.com>
References: <1483727739416-4681073.post@n4.nabble.com>
Message-ID: <b52d58fd-336e-b1b8-1453-38ec44e38870@measurement-factory.com>

On 01/06/2017 11:35 AM, boruc wrote:

> I am facing a really big problem (for me). I've set up a home network (few
> PCs, some mobiles) with squid proxying all requests. Is it possible to
> change cached objects manually? Let's say I have an object that contains
> /www.example.com/ page HTML source and what I want to do is e.g. change some
> words so users will see modified pages. I won't do this with HTTPS of
> course, just HTTP.
> 
> I tried to do that (I've changed one letter, so content length won't
> change), but after refreshing a page that cached file disappeared. Here's my
> refresh_pattern config section:
> 
> 
> 
> Or maybe there's another way to achieve my goal with squid?

If you just want to change one or two cached files once and do not care
that they will be replaced by fresher copies eventually (possibly
immediately; see P.S.), then you can very carefully hand-edit the cached
content on disk and restart Squid (to get rid of in-memory cached
copies, if any). You have to preserve the old content length of the
edited response and all [binary] metadata. Most editors will not do that
for you. A response may even contain a Content-MD5 checksum. This hack
is technically possible in some cases but it is unsupported.

Otherwise, what you want to do is called Content Adaptation. There are
several ways to do it as summarized at
http://wiki.squid-cache.org/SquidFaq/ContentAdaptation

HTH,

Alex.
P.S. Squid may get a fresh copy from the origin server if you tell the
browser to refresh the page or if Squid decides to refresh it for other
reasons. Your refresh patterns may combat some of these problems. Please
note that the patterns you used did not reach this mailing list.


From augustus_meyer at gmx.net  Fri Jan  6 20:33:22 2017
From: augustus_meyer at gmx.net (reinerotto)
Date: Fri, 6 Jan 2017 12:33:22 -0800 (PST)
Subject: [squid-users] Is it possible to modify cached object?
In-Reply-To: <1483727739416-4681073.post@n4.nabble.com>
References: <1483727739416-4681073.post@n4.nabble.com>
Message-ID: <1483734802168-4681075.post@n4.nabble.com>

Content adaption can also be done without squid. Mod of message body
"on-the-fly" can be achieved using commercial product(s).



--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/Is-it-possible-to-modify-cached-object-tp4681073p4681075.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From eliezer at ngtech.co.il  Sat Jan  7 04:25:02 2017
From: eliezer at ngtech.co.il (Eliezer  Croitoru)
Date: Sat, 7 Jan 2017 06:25:02 +0200
Subject: [squid-users] ssl_bump with intermediate CA
In-Reply-To: <161215919c9cb0d150047cf2702de615@treenet.co.nz>
References: <BN6PR17MB11400A4762F1969D486FEEFAF7600@BN6PR17MB1140.namprd17.prod.outlook.com>
 <CAHaxnUKG6-r_s_POpU7kVB8OkHnwypKFZPD8j2NxnnTF02bS6Q@mail.gmail.com>
 <BN6PR17MB11404837A94EEF5C7976CBBBF7630@BN6PR17MB1140.namprd17.prod.outlook.com>
 <161215919c9cb0d150047cf2702de615@treenet.co.nz>
Message-ID: <029701d2689e$043b43c0$0cb1cb40$@ngtech.co.il>

Hey Amos,

We can use the discussion section of the wiki to add some comments but the thing is that reality is almost the only answer to some questions.
IE a "cafile" in the normal world would be a certificate on the wall or in some database(back yard archive).
When in most places and cases one's diploma certificate is enough since not most of the world are certificate forgers.
Depends on the locality different measures to enforce the diploma validation are in place.
So if we would compare the real CA's world to the Computer Based one's there would be a big enough different.
Most web applications are more "aware" to these issues then any normal citizen would understand and know about.
The simple answer in most cases is that on the stage of validation there are  two things to validate:
- Authenticity
- Validity(expiration)

When a client probes for a connection it would first verify the chain in some places, but, let say I have a hospital and doctors inside, most clients of the hospital would not require validation of authenticity for the doctor diploma since the hospital is the proxy for this part of the connection and the client left alone to only present the request for care.

With the above real world "example" in hands we know that in the servers world we need to be able to inspect the certificate at-least once in period and we as sysadmins are required to present a full chain in order to meet the clients requests at-least once if not more.

I believe that a proxy should be required to be able to present the full chain certificates to be compatible with the way web applications themselves present their certificates.

And the simplest way to not open an "Edit" war is in most cases comments on the wiki discussion section..
But I do understand that there are wars which needs to be left alone.

All The Bests,
Eliezer

----
Eliezer Croitoru
Linux System Administrator
Mobile: +972-5-28704261
Email: eliezer at ngtech.co.il


-----Original Message-----
From: squid-users [mailto:squid-users-bounces at lists.squid-cache.org] On Behalf Of Amos Jeffries
Sent: Friday, January 6, 2017 12:06 PM
To: squid-users at lists.squid-cache.org
Subject: Re: [squid-users] ssl_bump with intermediate CA

On 2017-01-06 21:27, senor wrote:
> Thank you for the response but I think my question is still unanswered.
> Comments below:
> 
> On 1/5/2017 16:57, Bruce Rosenberg wrote:
>> The cafile option specifies the "chain" file squid should send back to
>> the client along with the cert, exactly as you would normally do with
>> Apache httpd or Nginx.
> (For clarity: I'm using 3.5.23. cafile was replaced in squid-4)
> This may be what cafile is used for but that does not match the
> directive description.
> http://www.squid-cache.org/Versions/v3/3.5/cfgman/http_port.html
> My suspicion is that the description is a confusion between the same
> option in openssl and web server options (Apache SSLCACertificateFile
> and similar).
> What's it really used for? Chain completion, client cert verification 
> or
> both?

It is used for chain completion. *which* chain is being completed 
depends on the traffic mode.

clientca= should be the one used *only* for client cert verification (I 
think). But neither was used consistently in Squid-3, so YMMV based on 
the traffic modes.

> 
>> In the example the generated server cert is depth 0, CA2 is depth 1 
>> and
>> CA1 is depth 2.
>> If the client has CA1 installed as a trust anchor then technically you
>> don't need to send CA1 as it is discarded by the client once the trust
>> relationship for CA2 is established.
>> It's good practice to send the full chain though as it makes
>> troubleshooting easier.
>> From a client perspective you can quickly grab the whole chain with
>> openssl s_client and check if CA1 is in the trust store.
> I have to disagree with this. The anchor (CA1) is discarded regardless.
> It cannot be used. If included it bloats the TLS handshake. Even 
> openssl
> will discard it and then look in the trusted CA store.
> 
> I see with a packet cap that the mimicked server cert and the signing
> cert are both included even without the cafile option specified.
> 
> So is it safe to say that the referenced wiki page has just become
> outdated? If cafile is used to fill in the cert chain it wouldn't be
> needed unless there were additional intermediate certs between the mitm
> cert and the trusted CA known to the client. (As in CA1 is trusted by
> clients, CA1 signs CA2 which signs CA3 which is used as MITM cert,
> cafile=CA2)

That wiki page was incorrect at the time of creation. But the author 
refuses to agree that root cert are discarded so I left it there instead 
of inciting an edit war. Saving the root CA into the file should be 
harmless anyway.

Amos

_______________________________________________
squid-users mailing list
squid-users at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-users



From eliezer at ngtech.co.il  Sat Jan  7 04:33:58 2017
From: eliezer at ngtech.co.il (Eliezer  Croitoru)
Date: Sat, 7 Jan 2017 06:33:58 +0200
Subject: [squid-users] Squid memory leak on ubuntu 14.04
In-Reply-To: <991a6eaaac7178742e80c15045a128a5@treenet.co.nz>
References: <CAJGZ0h4bmkcMF7V6WGsfh4uLB-by27t8esB0SnRjudFPVCTt9g@mail.gmail.com>
 <5658838F.7050707@urlfilterdb.com>
 <CAJGZ0h5s9k7txGMTN1tP4wdhKvkb=z_kuK8E5t5jwBpgKj8pUg@mail.gmail.com>
 <565BC47E.6070408@treenet.co.nz>
 <CAJGZ0h4AWgyHbwkujCiUpRPf8AbhegpdxaHzRVNuOCasd-Ho0w@mail.gmail.com>
 <565C0EFE.40904@treenet.co.nz>
 <CAJGZ0h6oTCu2OJqk8u-+X4eaeDiHyawg76xJPskjG=tfv-CESA@mail.gmail.com>
 <CAJGZ0h5Cf0936+5i17UCVN7vm+Y=KPg_+bM0uK1fz1+kM0sv4Q@mail.gmail.com>
 <1483630217278-4681060.post@n4.nabble.com>
 <991a6eaaac7178742e80c15045a128a5@treenet.co.nz>
Message-ID: <029901d2689f$4358fd80$ca0af880$@ngtech.co.il>

I will try to build a binary package for 12.04 and 14.04 of 3.5.23.
And as a comment I must admit that in 16.04 the kernel had couple hiccups inside a VM and I had to manually update the kernel into 4.6 branch to find out that the vanilla one was much stable then the Distro one(on Physical it worked well).

Eliezer

----
Eliezer Croitoru
Linux System Administrator
Mobile: +972-5-28704261
Email: eliezer at ngtech.co.il


-----Original Message-----
From: squid-users [mailto:squid-users-bounces at lists.squid-cache.org] On Behalf Of Amos Jeffries
Sent: Friday, January 6, 2017 12:41 PM
To: squid-users at lists.squid-cache.org
Subject: Re: [squid-users] Squid memory leak on ubuntu 14.04

On 2017-01-06 04:30, vinay wrote:

> 
> Let me know if Squid 3.3.8 is compatible with Ubuntu 14.04 or not? if 
> not what are the compatible versions with Ubuntu 14.04 as i cant 
> change Ubuntu version.

That has nothing to do with caching of HTTP data. You should be able to build and install a more current Squid-3.x version that Ubuntu, its just a matter of whether you know how to build software for Ubuntu.


> what changes need to be done to cache the contents ?
> 

And what does any of this have to do with memory leaks?

Amos
_______________________________________________
squid-users mailing list
squid-users at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-users



From squid3 at treenet.co.nz  Sat Jan  7 08:10:34 2017
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sat, 07 Jan 2017 21:10:34 +1300
Subject: [squid-users] Squid memory leak on ubuntu 14.04
In-Reply-To: <029901d2689f$4358fd80$ca0af880$@ngtech.co.il>
References: <CAJGZ0h4bmkcMF7V6WGsfh4uLB-by27t8esB0SnRjudFPVCTt9g@mail.gmail.com>
 <5658838F.7050707@urlfilterdb.com>
 <CAJGZ0h5s9k7txGMTN1tP4wdhKvkb=z_kuK8E5t5jwBpgKj8pUg@mail.gmail.com>
 <565BC47E.6070408@treenet.co.nz>
 <CAJGZ0h4AWgyHbwkujCiUpRPf8AbhegpdxaHzRVNuOCasd-Ho0w@mail.gmail.com>
 <565C0EFE.40904@treenet.co.nz>
 <CAJGZ0h6oTCu2OJqk8u-+X4eaeDiHyawg76xJPskjG=tfv-CESA@mail.gmail.com>
 <CAJGZ0h5Cf0936+5i17UCVN7vm+Y=KPg_+bM0uK1fz1+kM0sv4Q@mail.gmail.com>
 <1483630217278-4681060.post@n4.nabble.com>
 <991a6eaaac7178742e80c15045a128a5@treenet.co.nz>
 <029901d2689f$4358fd80$ca0af880$@ngtech.co.il>
Message-ID: <2b2845cf17ab7ed8a5992a2ad640747f@treenet.co.nz>

On 2017-01-07 17:33, Eliezer  Croitoru wrote:
> I will try to build a binary package for 12.04 and 14.04 of 3.5.23.

Please do that by taking the official source .deb package and just 
re-building it (and libecap3 package) for the older systems. No special 
changes.

There are several transitions tangled together in the 3.5 packages for 
both Debian and Ubuntu. These will break during future upgrades if the 
package manager detects a 3.5 install already on the machine that was 
done without the special handling in the official Debian/Ubuntu package 
scripts.

Amos



From frio_cervesa at hotmail.com  Sat Jan  7 10:32:00 2017
From: frio_cervesa at hotmail.com (senor)
Date: Sat, 7 Jan 2017 10:32:00 +0000
Subject: [squid-users] ssl_bump with intermediate CA
In-Reply-To: <161215919c9cb0d150047cf2702de615@treenet.co.nz>
References: <mailman.5.1483704002.17035.squid-users@lists.squid-cache.org>
 <mailman.5.1483704002.17035.squid-users.{6b2ffb03-f6d1-4809-88a2-ec0918ade30e}.0@lists.squid-cache.org>
 <161215919c9cb0d150047cf2702de615@treenet.co.nz>
Message-ID: <BN6PR17MB114094515485C17DE53E1DE0F7620@BN6PR17MB1140.namprd17.prod.outlook.com>

Thank you Amos. I agree that adding the anchor is generally harmless and
you've chosen your battles wisely.
Also thank you Garri. I must have missed your response confirming the same.

For current squid versions the wiki page is misleading according to all
credible references I can find. Any application failing because the root
is missing is buggy itself and that is not a squid problem. There are
several very good arguments for excluding it, including to expose bad
apps. Trusting a root cert sent from a server is like trusting a
politician because all promises end with "trust me".

I'll submit a request for the description of cafile/tls-cafile to change
and move the discussion to there.

Thanks all,
Senor

On 1/6/2017 2:06, Amos Jeffries wrote:
> On 2017-01-06 21:27, senor wrote:
>> Thank you for the response but I think my question is still unanswered.
>> Comments below:
>>
>> On 1/5/2017 16:57, Bruce Rosenberg wrote:
>>> The cafile option specifies the "chain" file squid should send back to
>>> the client along with the cert, exactly as you would normally do with
>>> Apache httpd or Nginx.
>> (For clarity: I'm using 3.5.23. cafile was replaced in squid-4)
>> This may be what cafile is used for but that does not match the
>> directive description.
>> http://www.squid-cache.org/Versions/v3/3.5/cfgman/http_port.html
>> My suspicion is that the description is a confusion between the same
>> option in openssl and web server options (Apache SSLCACertificateFile
>> and similar).
>> What's it really used for? Chain completion, client cert verification or
>> both?
> 
> It is used for chain completion. *which* chain is being completed
> depends on the traffic mode.
> 
> clientca= should be the one used *only* for client cert verification (I
> think). But neither was used consistently in Squid-3, so YMMV based on
> the traffic modes.
> 
>>
>>> In the example the generated server cert is depth 0, CA2 is depth 1 and
>>> CA1 is depth 2.
>>> If the client has CA1 installed as a trust anchor then technically you
>>> don't need to send CA1 as it is discarded by the client once the trust
>>> relationship for CA2 is established.
>>> It's good practice to send the full chain though as it makes
>>> troubleshooting easier.
>>> From a client perspective you can quickly grab the whole chain with
>>> openssl s_client and check if CA1 is in the trust store.
>> I have to disagree with this. The anchor (CA1) is discarded regardless.
>> It cannot be used. If included it bloats the TLS handshake. Even openssl
>> will discard it and then look in the trusted CA store.
>>
>> I see with a packet cap that the mimicked server cert and the signing
>> cert are both included even without the cafile option specified.
>>
>> So is it safe to say that the referenced wiki page has just become
>> outdated? If cafile is used to fill in the cert chain it wouldn't be
>> needed unless there were additional intermediate certs between the mitm
>> cert and the trusted CA known to the client. (As in CA1 is trusted by
>> clients, CA1 signs CA2 which signs CA3 which is used as MITM cert,
>> cafile=CA2)
> 
> That wiki page was incorrect at the time of creation. But the author
> refuses to agree that root cert are discarded so I left it there instead
> of inciting an edit war. Saving the root CA into the file should be
> harmless anyway.
> 
> Amos
> 
> 
> 



From ahmed.zaeem at netstream.ps  Sat Jan  7 17:25:56 2017
From: ahmed.zaeem at netstream.ps (--Ahmad--)
Date: Sat, 7 Jan 2017 19:25:56 +0200
Subject: [squid-users] keep source ip when user connect over squid using
	ip:port
Message-ID: <9C2EDD85-6314-4169-A423-EE824AB4598A@netstream.ps>

hey folks .

I?m just wondering if possible .

i have a basic squid on ip:port .

i want to connect over it using ip:port from my browser .
also ?

when i visit websites ?. i want my ip (router ip) not squid ip to be shown to the websites .

is that possible ?

i know its weird ? but i want squid as like bridging mode and i want the requests to be sourced from the original ip of mine ..like my router ip 

thank you Guys .

From uhlar at fantomas.sk  Sat Jan  7 18:20:01 2017
From: uhlar at fantomas.sk (Matus UHLAR - fantomas)
Date: Sat, 7 Jan 2017 19:20:01 +0100
Subject: [squid-users] keep source ip when user connect over squid using
 ip:port
In-Reply-To: <9C2EDD85-6314-4169-A423-EE824AB4598A@netstream.ps>
References: <9C2EDD85-6314-4169-A423-EE824AB4598A@netstream.ps>
Message-ID: <20170107182001.GA7614@fantomas.sk>

On 07.01.17 19:25, --Ahmad-- wrote:
>hey folks .
>
>I?m just wondering if possible .
>
>i have a basic squid on ip:port .
>
>i want to connect over it using ip:port from my browser .
>also ?
>
>when i visit websites ?. i want my ip (router ip) not squid ip to be shown to the websites .
>
>is that possible ?
>
>i know its weird ? but i want squid as like bridging mode and i want the requests to be sourced from the original ip of mine ..like my router ip

It's called "tproxy", http://wiki.squid-cache.org/Features/Tproxy4

- I'm not sure whether it works when you configure proxy manually 
   the wiki page describes it in intercepting mode

- your router MUST support that, so is sends incoming traffic to the proxy
   instead of your computer, while the destination IP is your compurer's
-- 
Matus UHLAR - fantomas, uhlar at fantomas.sk ; http://www.fantomas.sk/
Warning: I wish NOT to receive e-mail advertising to this address.
Varovanie: na tuto adresu chcem NEDOSTAVAT akukolvek reklamnu postu.
"One World. One Web. One Program." - Microsoft promotional advertisement
"Ein Volk, ein Reich, ein Fuhrer!" - Adolf Hitler


From ahmed.zaeem at netstream.ps  Sat Jan  7 18:23:47 2017
From: ahmed.zaeem at netstream.ps (--Ahmad--)
Date: Sat, 7 Jan 2017 20:23:47 +0200
Subject: [squid-users] keep source ip when user connect over squid using
	ip:port
In-Reply-To: <20170107182001.GA7614@fantomas.sk>
References: <9C2EDD85-6314-4169-A423-EE824AB4598A@netstream.ps>
 <20170107182001.GA7614@fantomas.sk>
Message-ID: <2B70735D-FE5F-464E-9B44-B50926BADE40@netstream.ps>

hey mate i total understand Tporxy with CISCO /wccp

but I?m asking here other way like connecting ip:port and keep squid using my original ip  as source 

cheers

> On Jan 7, 2017, at 8:20 PM, Matus UHLAR - fantomas <uhlar at fantomas.sk> wrote:
> 
> On 07.01.17 19:25, --Ahmad-- wrote:
>> hey folks .
>> 
>> I?m just wondering if possible .
>> 
>> i have a basic squid on ip:port .
>> 
>> i want to connect over it using ip:port from my browser .
>> also ?
>> 
>> when i visit websites ?. i want my ip (router ip) not squid ip to be shown to the websites .
>> 
>> is that possible ?
>> 
>> i know its weird ? but i want squid as like bridging mode and i want the requests to be sourced from the original ip of mine ..like my router ip
> 
> It's called "tproxy", http://wiki.squid-cache.org/Features/Tproxy4 <http://wiki.squid-cache.org/Features/Tproxy4>
> 
> - I'm not sure whether it works when you configure proxy manually   the wiki page describes it in intercepting mode
> 
> - your router MUST support that, so is sends incoming traffic to the proxy
>  instead of your computer, while the destination IP is your compurer's
> -- 
> Matus UHLAR - fantomas, uhlar at fantomas.sk <mailto:uhlar at fantomas.sk> ; http://www.fantomas.sk/ <http://www.fantomas.sk/>
> Warning: I wish NOT to receive e-mail advertising to this address.
> Varovanie: na tuto adresu chcem NEDOSTAVAT akukolvek reklamnu postu.
> "One World. One Web. One Program." - Microsoft promotional advertisement
> "Ein Volk, ein Reich, ein Fuhrer!" - Adolf Hitler
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org <mailto:squid-users at lists.squid-cache.org>
> http://lists.squid-cache.org/listinfo/squid-users <http://lists.squid-cache.org/listinfo/squid-users>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20170107/e505031a/attachment.htm>

From Antony.Stone at squid.open.source.it  Sat Jan  7 18:35:44 2017
From: Antony.Stone at squid.open.source.it (Antony Stone)
Date: Sat, 7 Jan 2017 19:35:44 +0100
Subject: [squid-users] keep source ip when user connect over squid using
	ip:port
In-Reply-To: <2B70735D-FE5F-464E-9B44-B50926BADE40@netstream.ps>
References: <9C2EDD85-6314-4169-A423-EE824AB4598A@netstream.ps>
 <20170107182001.GA7614@fantomas.sk>
 <2B70735D-FE5F-464E-9B44-B50926BADE40@netstream.ps>
Message-ID: <201701071935.44826.Antony.Stone@squid.open.source.it>

On Saturday 07 January 2017 at 19:23:47, --Ahmad-- wrote:

> hey mate i total understand Tporxy with CISCO /wccp
> 
> but I?m asking here other way like connecting ip:port and keep squid using
> my original ip  as source

So, where do you expect the reply packets from the remote web server to end 
up?

If you're trying to cache content, they have to arrive at your Squid server, 
which means the source of the requests has to be the Squid server's address 
(or at least, some address which gets routed from the Internet via your Squid 
server).

If you're not trying to cache content, and you want the replies to come 
directly back to your browser, what are you using Squid for in this setup?


Antony.

-- 
I wasn't sure about having a beard at first, but then it grew on me.

                                                   Please reply to the list;
                                                         please *don't* CC me.


From ahmed.zaeem at netstream.ps  Sat Jan  7 18:55:39 2017
From: ahmed.zaeem at netstream.ps (--Ahmad--)
Date: Sat, 7 Jan 2017 20:55:39 +0200
Subject: [squid-users] keep source ip when user connect over squid using
	ip:port
In-Reply-To: <201701071935.44826.Antony.Stone@squid.open.source.it>
References: <9C2EDD85-6314-4169-A423-EE824AB4598A@netstream.ps>
 <20170107182001.GA7614@fantomas.sk>
 <2B70735D-FE5F-464E-9B44-B50926BADE40@netstream.ps>
 <201701071935.44826.Antony.Stone@squid.open.source.it>
Message-ID: <241415CB-D95F-4012-A47B-6968E1DB3A44@netstream.ps>

I?m using  squid as bridge like injecting some urls

thats it 

thanks 
> On Jan 7, 2017, at 8:35 PM, Antony Stone <Antony.Stone at squid.open.source.it> wrote:
> 
> On Saturday 07 January 2017 at 19:23:47, --Ahmad-- wrote:
> 
>> hey mate i total understand Tporxy with CISCO /wccp
>> 
>> but I?m asking here other way like connecting ip:port and keep squid using
>> my original ip  as source
> 
> So, where do you expect the reply packets from the remote web server to end 
> up?
> 
> If you're trying to cache content, they have to arrive at your Squid server, 
> which means the source of the requests has to be the Squid server's address 
> (or at least, some address which gets routed from the Internet via your Squid 
> server).
> 
> If you're not trying to cache content, and you want the replies to come 
> directly back to your browser, what are you using Squid for in this setup?
> 
> 
> Antony.
> 
> -- 
> I wasn't sure about having a beard at first, but then it grew on me.
> 
>                                                   Please reply to the list;
>                                                         please *don't* CC me.
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users



From mabi at protonmail.ch  Sat Jan  7 19:50:25 2017
From: mabi at protonmail.ch (mabi)
Date: Sat, 07 Jan 2017 14:50:25 -0500
Subject: [squid-users] Citrix Receiver client with Squid transparent proxy
	to access Citrix XenApp Server
Message-ID: <tbH5RJjVmd-aTSw2MoDyJB8ph5GSgMjZFBFVsTsYqGSX_rfBhXKGstjgNTAefvCgle97itWe1DKGdomHq4sVASNocY_-gXtw_0u7Fj8rKSk=@protonmail.ch>

Hi,

Is it possible using a Citrix Receiver client behind a Squid 3.5.20 transparent proxy to connect to a Citrix XenApp server on the internet?

If someone already managed to achieve this I would be interested to know how. For me it simply does not work, the Citrix Receiver client throws the following SSL errorr: A network error occured (SSL error 4).

So before wasting time trying to figure out how to make it work I was wondering already managed to make this work...

Regards,
M.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20170107/ded1081b/attachment.htm>

From eddie at mattermedia.com  Sat Jan  7 21:59:34 2017
From: eddie at mattermedia.com (Eddie B)
Date: Sat, 7 Jan 2017 13:59:34 -0800
Subject: [squid-users] Is this proper usage of Squid?
Message-ID: <000e01d26931$55710570$00531050$@mattermedia.com>

We have embedded Vimeo videos on a site accessible only to logged in users.
Because of different firewalls, using different types of blocks, the videos
sometimes do not work for the client.

 

Aside from cases where the firewall blocks any video streaming, we want to
serve video to clients that have Vimeo blocked (by IP or DNS) via this
setup:

 

1.       our application detects Vimeo is being blocked

2.       tells the browser to load up a different player

3.       this new player loads every needed resource (video, JS, json, etc)
from domains that we control (and thus have valid SSL certificates for),
instead of Vimeo/Akamai domains.

4.       our server will download the appropriate version of the video from
Vimeo on the fly (maybe cache it for an hour) and serve it to the client.
The other resources necessary to play the video (JS, JSON) are permanently
stored on our server, and are modified to tell the client to request the
video from our servers. This is where Squid comes in.

 

In our situation, we cannot:

 

.         know which kind of block the videos trigger on these firewalls

.         ask the clients to change any settings on their browsers

.         ask the clients to request a change to firewall settings

 

Can we accomplish the above using Squid?

 

TIA

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20170107/761ad0ae/attachment.htm>

From squid3 at treenet.co.nz  Sat Jan  7 23:39:38 2017
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sun, 08 Jan 2017 12:39:38 +1300
Subject: [squid-users] Is this proper usage of Squid?
In-Reply-To: <000e01d26931$55710570$00531050$@mattermedia.com>
References: <000e01d26931$55710570$00531050$@mattermedia.com>
Message-ID: <0285bb7f8590537f5d029975103337f1@treenet.co.nz>

On 2017-01-08 10:59, Eddie B wrote:
> 
> Can we accomplish the above using Squid?
> 

A basic CDN / reverse-proxy setup does most of what you ask. You will 
need to then throw in some content adaptation (ICAP or eCAP) to handle 
URLs in the response HTML and any scripts.

Not knowing what the "firewalls" will trigger on may be a problem 
though.

Amos



From eddie at mattermedia.com  Sun Jan  8 00:33:47 2017
From: eddie at mattermedia.com (Eddie B)
Date: Sat, 7 Jan 2017 16:33:47 -0800
Subject: [squid-users] Is this proper usage of Squid?
Message-ID: <005301d26946$e0bdd490$a2397db0$@mattermedia.com>

Thanks Amos. Looks like we will be using eCAP.

 

One other possible scenario comes to mind -  I am not sure if this is
feasible. When the block is via DNS (not IP, not all video streaming block),
could we:

 

.         create CNAMEs for all vimeo domains involved in delivering video

.         serve the our modified player/resources from and pointing to these
CNAMES

.         then route thru squid all the request and response traffic running
eCAP on both headers and body, both ways

 

?

 

Thanks again

 

---------------

On 2017-01-08 10:59, Eddie B wrote:

> 

> Can we accomplish the above using Squid?

> 

 

A basic CDN / reverse-proxy setup does most of what you ask. You will 

need to then throw in some content adaptation (ICAP or eCAP) to handle 

URLs in the response HTML and any scripts.

 

Not knowing what the "firewalls" will trigger on may be a problem 

though.

 

Amos

 

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20170107/4896d89b/attachment.htm>

From matg2s at gmail.com  Sun Jan  8 09:31:51 2017
From: matg2s at gmail.com (matg2s)
Date: Sun, 8 Jan 2017 01:31:51 -0800 (PST)
Subject: [squid-users] Problem building Squid4
Message-ID: <1483867911104-4681090.post@n4.nabble.com>

Hey,

I'm trying to compile Squid 4 (on Ubuntu 14/16) and am getting the following
error:

In file included from ../../src/base/AsyncCall.h:14:0,
                 from ../../src/comm/IoCallback.h:12,
                 from ../../src/comm.h:12,
                 from ../../src/client_side.h:16,
                 from ServerBump.cc:13:
../../src/base/RefCount.h: In instantiation of ?void
RefCount<C>::dereference(const C*) [with C = AnyP::PortCfg]?:
../../src/base/RefCount.h:35:20:   required from ?RefCount<C>::~RefCount()
[with C = AnyP::PortCfg]?
../../src/servers/Server.h:31:23:   required from here
../../src/base/RefCount.h:96:40: error: invalid use of incomplete type
?const class AnyP::PortCfg?
         if (tempP_ && tempP_->unlock() == 0)
                                        ^
In file included from ../../src/MasterXaction.h:12:0,
                 from ../../src/CommCalls.h:16,
                 from ../../src/comm.h:13,
                 from ../../src/client_side.h:16,
                 from ServerBump.cc:13:
../../src/anyp/forward.h:17:7: note: forward declaration of ?class
AnyP::PortCfg?
 class PortCfg;
       ^
In file included from ../../src/base/AsyncCall.h:14:0,
                 from ../../src/comm/IoCallback.h:12,
                 from ../../src/comm.h:12,
                 from ../../src/client_side.h:16,
                 from ServerBump.cc:13:
../../src/base/RefCount.h:97:13: error: possible problem detected in
invocation of delete operator: [-Werror=delete-incomplete]
             delete tempP_;
             ^
../../src/base/RefCount.h:93:19: error: ?tempP_? has incomplete type
[-Werror]
         C const (*tempP_) (p_);
                   ^
In file included from ../../src/MasterXaction.h:12:0,
                 from ../../src/CommCalls.h:16,
                 from ../../src/comm.h:13,
                 from ../../src/client_side.h:16,
                 from ServerBump.cc:13:
../../src/anyp/forward.h:17:7: note: forward declaration of ?class
AnyP::PortCfg?
 class PortCfg;
       ^
In file included from ../../src/base/AsyncCall.h:14:0,
                 from ../../src/comm/IoCallback.h:12,
                 from ../../src/comm.h:12,
                 from ../../src/client_side.h:16,
                 from ServerBump.cc:13:
../../src/base/RefCount.h:97:13: note: neither the destructor nor the
class-specific operator delete will be called, even if they are declared
when the class is defined
             delete tempP_;
             ^
cc1plus: error: unrecognized command line option ?-Wno-deprecated-register?
[-Werror]

I'm using the following to build:
wget http://www.squid-cache.org/Versions/v4/squid-4.0.17.tar.gz
I used various configurations, from:
./configure --enable-ssl-crtd
to 
./configure --sysconfdir=/etc/squid  --mandir=/usr/share/man 
--with-default-user=proxy  --disable-maintainer-mode 
--disable-dependency-tracking  --enable-inline  --enable-async-io=8
--enable-storeio="ufs,aufs,diskd"  --enable-removal-policies="lru,heap" 
--enable-poll  --enable-delay-pools  --enable-cache-digests --enable-snmp 
--enable-htcp  --enable-select  --enable-carp  --with-large-files 
--enable-underscores  --disable-arch-native  --disable-auth
--with-krb5-config=no  --disable-external-acl-helpers  --disable-eui
--with-filedescriptors=65536  --enable-epoll  --enable-linux-netfilter
--enable-ssl-crtd --with-openssl 

and then "make" but I'm keep getting compilation errors and the problem
seems to lies with sslcrtd. 
I used the following versions:
gcc (Ubuntu 5.4.0-6ubuntu1~16.04.4) 5.4.0 20160609
g++ (Ubuntu 5.4.0-6ubuntu1~16.04.4) 5.4.0 20160609
But also other gcc versions (4.8, 4.9 and later) and even clang (3.5) but to
no avail.

Any help would be appreciated!
Thanks,
Matt



--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/Problem-building-Squid4-tp4681090.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From baborucki at gmail.com  Sun Jan  8 14:49:34 2017
From: baborucki at gmail.com (boruc)
Date: Sun, 8 Jan 2017 06:49:34 -0800 (PST)
Subject: [squid-users] Is it possible to modify cached object?
In-Reply-To: <1483734802168-4681075.post@n4.nabble.com>
References: <1483727739416-4681073.post@n4.nabble.com>
 <1483734802168-4681075.post@n4.nabble.com>
Message-ID: <CAGkbS_bsvPJhgYo+8P_op-W6Pqbm_g67nfmNvbFarzrcXD-L7g@mail.gmail.com>

Thank you for your answer.

Actually I managed to do what I want by simply editing that file and
changing content length if necessary. I don't know why sometimes I need to
restart Squid or reopen browser to see changed version of page. Sometimes
it works fine on regular browser window, sometimes I need to open it in
private mode, but that's the thing that I want to figure out now.

Would it be possible to NOT cache specific files, like images by using
refresh_pattern? Or in other words - I'd like to cache only HTML/CSS files.

2017-01-06 21:33 GMT+01:00 reinerotto [via Squid Web Proxy Cache] <
ml-node+s1019090n4681075h75 at n4.nabble.com>:

> Content adaption can also be done without squid. Mod of message body
> "on-the-fly" can be achieved using commercial product(s).
>
> ------------------------------
> If you reply to this email, your message will be added to the discussion
> below:
> http://squid-web-proxy-cache.1019090.n4.nabble.com/Is-it-
> possible-to-modify-cached-object-tp4681073p4681075.html
> To unsubscribe from Is it possible to modify cached object?, click here
> <http://squid-web-proxy-cache.1019090.n4.nabble.com/template/NamlServlet.jtp?macro=unsubscribe_by_code&node=4681073&code=YmFib3J1Y2tpQGdtYWlsLmNvbXw0NjgxMDczfDE2MjkxNjIwMDk=>
> .
> NAML
> <http://squid-web-proxy-cache.1019090.n4.nabble.com/template/NamlServlet.jtp?macro=macro_viewer&id=instant_html%21nabble%3Aemail.naml&base=nabble.naml.namespaces.BasicNamespace-nabble.view.web.template.NabbleNamespace-nabble.view.web.template.NodeNamespace&breadcrumbs=notify_subscribers%21nabble%3Aemail.naml-instant_emails%21nabble%3Aemail.naml-send_instant_email%21nabble%3Aemail.naml>
>




--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/Is-it-possible-to-modify-cached-object-tp4681073p4681091.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From yvoinov at gmail.com  Sun Jan  8 15:00:21 2017
From: yvoinov at gmail.com (Yuri Voinov)
Date: Sun, 8 Jan 2017 21:00:21 +0600
Subject: [squid-users] Is it possible to modify cached object?
In-Reply-To: <CAGkbS_bsvPJhgYo+8P_op-W6Pqbm_g67nfmNvbFarzrcXD-L7g@mail.gmail.com>
References: <1483727739416-4681073.post@n4.nabble.com>
 <1483734802168-4681075.post@n4.nabble.com>
 <CAGkbS_bsvPJhgYo+8P_op-W6Pqbm_g67nfmNvbFarzrcXD-L7g@mail.gmail.com>
Message-ID: <6b3650dc-6ea6-3f3b-655f-ef4db59b9d76@gmail.com>



08.01.2017 20:49, boruc ?????:
> Thank you for your answer.
>
> Actually I managed to do what I want by simply editing that file and
> changing content length if necessary. I don't know why sometimes I need to
> restart Squid or reopen browser to see changed version of page. Sometimes
> it works fine on regular browser window, sometimes I need to open it in
> private mode, but that's the thing that I want to figure out now.
>
> Would it be possible to NOT cache specific files, like images by using
> refresh_pattern? Or in other words - I'd like to cache only HTML/CSS files.
Sure. You can do it either using refresh_pattern, or, more selectively,
using

# No cache directives
cache deny dont_cache_url
cache allow all

> 2017-01-06 21:33 GMT+01:00 reinerotto [via Squid Web Proxy Cache] <
> ml-node+s1019090n4681075h75 at n4.nabble.com>:
>
>> Content adaption can also be done without squid. Mod of message body
>> "on-the-fly" can be achieved using commercial product(s).
>>
>> ------------------------------
>> If you reply to this email, your message will be added to the discussion
>> below:
>> http://squid-web-proxy-cache.1019090.n4.nabble.com/Is-it-
>> possible-to-modify-cached-object-tp4681073p4681075.html
>> To unsubscribe from Is it possible to modify cached object?, click here
>> <http://squid-web-proxy-cache.1019090.n4.nabble.com/template/NamlServlet.jtp?macro=unsubscribe_by_code&node=4681073&code=YmFib3J1Y2tpQGdtYWlsLmNvbXw0NjgxMDczfDE2MjkxNjIwMDk=>
>> .
>> NAML
>> <http://squid-web-proxy-cache.1019090.n4.nabble.com/template/NamlServlet.jtp?macro=macro_viewer&id=instant_html%21nabble%3Aemail.naml&base=nabble.naml.namespaces.BasicNamespace-nabble.view.web.template.NabbleNamespace-nabble.view.web.template.NodeNamespace&breadcrumbs=notify_subscribers%21nabble%3Aemail.naml-instant_emails%21nabble%3Aemail.naml-send_instant_email%21nabble%3Aemail.naml>
>>
>
>
>
> --
> View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/Is-it-possible-to-modify-cached-object-tp4681073p4681091.html
> Sent from the Squid - Users mailing list archive at Nabble.com.
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users

-- 
What is the fundamental difference between the programmer and by a fag?
Fag never become five times to free the memory of one object. Fag will
not use two almost identical string libraries in the same project. Fag
will never write to a mixture of C and C ++. Fag will never pass objects
by pointer. Now you know why these two categories so often mentioned
together, and one of them is worse :)
-------------- next part --------------
A non-text attachment was scrubbed...
Name: 0x613DEC46.asc
Type: application/pgp-keys
Size: 2437 bytes
Desc: not available
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20170108/0d646a01/attachment.key>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: signature.asc
Type: application/pgp-signature
Size: 473 bytes
Desc: OpenPGP digital signature
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20170108/0d646a01/attachment.sig>

From baborucki at gmail.com  Sun Jan  8 17:15:35 2017
From: baborucki at gmail.com (boruc)
Date: Sun, 8 Jan 2017 09:15:35 -0800 (PST)
Subject: [squid-users] Is it possible to modify cached object?
In-Reply-To: <b52d58fd-336e-b1b8-1453-38ec44e38870@measurement-factory.com>
References: <1483727739416-4681073.post@n4.nabble.com>
 <b52d58fd-336e-b1b8-1453-38ec44e38870@measurement-factory.com>
Message-ID: <1483895735144-4681093.post@n4.nabble.com>

I can change content of the HTML in cached object, but not in every way. I
have a trouble inserting an image to that site from my PC. If I simply put: 
</home/username/Images/image.png>  there's only a displayed icon which is
refering to given location but on the page server. I also tried like this: 
<file:///home/username/Images/image.png>  but also with no effect. I cannot
find any solution to my problem, you guys are my only hope :) 



--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/Is-it-possible-to-modify-cached-object-tp4681073p4681093.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From baborucki at gmail.com  Sun Jan  8 17:18:14 2017
From: baborucki at gmail.com (boruc)
Date: Sun, 8 Jan 2017 09:18:14 -0800 (PST)
Subject: [squid-users] Is it possible to modify cached object?
In-Reply-To: <1483895735144-4681093.post@n4.nabble.com>
References: <1483727739416-4681073.post@n4.nabble.com>
 <b52d58fd-336e-b1b8-1453-38ec44e38870@measurement-factory.com>
 <1483895735144-4681093.post@n4.nabble.com>
Message-ID: <1483895894250-4681094.post@n4.nabble.com>

Sorry for multiple posts. I used tag in incorrect way so you can't see the
code. Here's correct message:

I can change content of the HTML in cached object, but not in every way. I
have a trouble inserting an image to that site from my PC. If I simply put:
img src="/home/username/Images/image.png" there's only a displayed icon
which is refering to given location but on the page server. I also tried
like this: img src="file:///home/username/Images/image.png",  but also with
no effect. I cannot find any solution to my problem, you guys are my only
hope :)



--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/Is-it-possible-to-modify-cached-object-tp4681073p4681094.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From rousskov at measurement-factory.com  Sun Jan  8 17:51:53 2017
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Sun, 8 Jan 2017 10:51:53 -0700
Subject: [squid-users] Problem building Squid4
In-Reply-To: <1483867911104-4681090.post@n4.nabble.com>
References: <1483867911104-4681090.post@n4.nabble.com>
Message-ID: <781a9ef7-3070-5752-f659-c7014364d685@measurement-factory.com>

On 01/08/2017 02:31 AM, matg2s wrote:

> I'm trying to compile Squid 4 (on Ubuntu 14/16) and am getting the following
> error:

Builds OK here with similar SSL-related ./configure options. Could be an
OpenSSL library version incompatibility or a similar problem. If you are
building with OpenSSL v1.0, then I suggest posting complete build log or
at least your "./configure ..." output. Otherwise, try that version
instead of OpenSSL v1.1.

Alex.


> In file included from ../../src/base/AsyncCall.h:14:0,
>                  from ../../src/comm/IoCallback.h:12,
>                  from ../../src/comm.h:12,
>                  from ../../src/client_side.h:16,
>                  from ServerBump.cc:13:
> ../../src/base/RefCount.h: In instantiation of ?void
> RefCount<C>::dereference(const C*) [with C = AnyP::PortCfg]?:
> ../../src/base/RefCount.h:35:20:   required from ?RefCount<C>::~RefCount()
> [with C = AnyP::PortCfg]?
> ../../src/servers/Server.h:31:23:   required from here
> ../../src/base/RefCount.h:96:40: error: invalid use of incomplete type
> ?const class AnyP::PortCfg?
>          if (tempP_ && tempP_->unlock() == 0)
>                                         ^
> In file included from ../../src/MasterXaction.h:12:0,
>                  from ../../src/CommCalls.h:16,
>                  from ../../src/comm.h:13,
>                  from ../../src/client_side.h:16,
>                  from ServerBump.cc:13:
> ../../src/anyp/forward.h:17:7: note: forward declaration of ?class
> AnyP::PortCfg?
>  class PortCfg;
>        ^
> In file included from ../../src/base/AsyncCall.h:14:0,
>                  from ../../src/comm/IoCallback.h:12,
>                  from ../../src/comm.h:12,
>                  from ../../src/client_side.h:16,
>                  from ServerBump.cc:13:
> ../../src/base/RefCount.h:97:13: error: possible problem detected in
> invocation of delete operator: [-Werror=delete-incomplete]
>              delete tempP_;
>              ^
> ../../src/base/RefCount.h:93:19: error: ?tempP_? has incomplete type
> [-Werror]
>          C const (*tempP_) (p_);
>                    ^
> In file included from ../../src/MasterXaction.h:12:0,
>                  from ../../src/CommCalls.h:16,
>                  from ../../src/comm.h:13,
>                  from ../../src/client_side.h:16,
>                  from ServerBump.cc:13:
> ../../src/anyp/forward.h:17:7: note: forward declaration of ?class
> AnyP::PortCfg?
>  class PortCfg;
>        ^
> In file included from ../../src/base/AsyncCall.h:14:0,
>                  from ../../src/comm/IoCallback.h:12,
>                  from ../../src/comm.h:12,
>                  from ../../src/client_side.h:16,
>                  from ServerBump.cc:13:
> ../../src/base/RefCount.h:97:13: note: neither the destructor nor the
> class-specific operator delete will be called, even if they are declared
> when the class is defined
>              delete tempP_;
>              ^
> cc1plus: error: unrecognized command line option ?-Wno-deprecated-register?
> [-Werror]
> 
> I'm using the following to build:
> wget http://www.squid-cache.org/Versions/v4/squid-4.0.17.tar.gz
> I used various configurations, from:
> ./configure --enable-ssl-crtd
> to 
> ./configure --sysconfdir=/etc/squid  --mandir=/usr/share/man 
> --with-default-user=proxy  --disable-maintainer-mode 
> --disable-dependency-tracking  --enable-inline  --enable-async-io=8
> --enable-storeio="ufs,aufs,diskd"  --enable-removal-policies="lru,heap" 
> --enable-poll  --enable-delay-pools  --enable-cache-digests --enable-snmp 
> --enable-htcp  --enable-select  --enable-carp  --with-large-files 
> --enable-underscores  --disable-arch-native  --disable-auth
> --with-krb5-config=no  --disable-external-acl-helpers  --disable-eui
> --with-filedescriptors=65536  --enable-epoll  --enable-linux-netfilter
> --enable-ssl-crtd --with-openssl 
> 
> and then "make" but I'm keep getting compilation errors and the problem
> seems to lies with sslcrtd. 
> I used the following versions:
> gcc (Ubuntu 5.4.0-6ubuntu1~16.04.4) 5.4.0 20160609
> g++ (Ubuntu 5.4.0-6ubuntu1~16.04.4) 5.4.0 20160609
> But also other gcc versions (4.8, 4.9 and later) and even clang (3.5) but to
> no avail.
> 
> Any help would be appreciated!
> Thanks,
> Matt
> 
> 
> 
> --
> View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/Problem-building-Squid4-tp4681090.html
> Sent from the Squid - Users mailing list archive at Nabble.com.
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
> 



From rafael.akchurin at diladele.com  Sun Jan  8 19:41:24 2017
From: rafael.akchurin at diladele.com (Rafael Akchurin)
Date: Sun, 8 Jan 2017 19:41:24 +0000
Subject: [squid-users] Is it possible to modify cached object?
In-Reply-To: <1483895894250-4681094.post@n4.nabble.com>
References: <1483727739416-4681073.post@n4.nabble.com>
 <b52d58fd-336e-b1b8-1453-38ec44e38870@measurement-factory.com>
 <1483895735144-4681093.post@n4.nabble.com>
 <1483895894250-4681094.post@n4.nabble.com>
Message-ID: <DB6PR0401MB26807A472A098AA612E6ABA78F650@DB6PR0401MB2680.eurprd04.prod.outlook.com>

Hello Boruc,

Please use https://en.wikipedia.org/wiki/Data_URI_scheme

Best regards,
Rafael Akchurin
Diladele B.V.



-----Original Message-----
From: squid-users [mailto:squid-users-bounces at lists.squid-cache.org] On Behalf Of boruc
Sent: Sunday, January 8, 2017 6:18 PM
To: squid-users at lists.squid-cache.org
Subject: Re: [squid-users] Is it possible to modify cached object?

Sorry for multiple posts. I used tag in incorrect way so you can't see the code. Here's correct message:

I can change content of the HTML in cached object, but not in every way. I have a trouble inserting an image to that site from my PC. If I simply put:
img src="/home/username/Images/image.png" there's only a displayed icon which is refering to given location but on the page server. I also tried like this: img src="file:///home/username/Images/image.png",  but also with no effect. I cannot find any solution to my problem, you guys are my only hope :)



--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/Is-it-possible-to-modify-cached-object-tp4681073p4681094.html
Sent from the Squid - Users mailing list archive at Nabble.com.
_______________________________________________
squid-users mailing list
squid-users at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-users

From squid3 at treenet.co.nz  Mon Jan  9 06:18:52 2017
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Mon, 9 Jan 2017 19:18:52 +1300
Subject: [squid-users] Is it possible to modify cached object?
In-Reply-To: <CAGkbS_bsvPJhgYo+8P_op-W6Pqbm_g67nfmNvbFarzrcXD-L7g@mail.gmail.com>
References: <1483727739416-4681073.post@n4.nabble.com>
 <1483734802168-4681075.post@n4.nabble.com>
 <CAGkbS_bsvPJhgYo+8P_op-W6Pqbm_g67nfmNvbFarzrcXD-L7g@mail.gmail.com>
Message-ID: <69d58d2d-d598-7063-4164-43e28ac2d28d@treenet.co.nz>

On 9/01/2017 3:49 a.m., boruc wrote:
> Thank you for your answer.
> 
> Actually I managed to do what I want by simply editing that file and
> changing content length if necessary. I don't know why sometimes I need to
> restart Squid or reopen browser to see changed version of page. Sometimes
> it works fine on regular browser window, sometimes I need to open it in
> private mode, but that's the thing that I want to figure out now.

That is because what is on disk is only about half of the object. There
may be a whole copy of it in memory, or just the metadata about it.

Squid needs to be stopped, the swap.state file erased, then Squid
started again to scan the disk files (all of them) to load the changed
objects information.

> 
> Would it be possible to NOT cache specific files, like images by using
> refresh_pattern? Or in other words - I'd like to cache only HTML/CSS files.

Not with refresh_pattern, that is only for tuning revalidation
operations. Use 'cache deny ...' and/or 'store_miss deny ...' for
preventing things caching.

It is better though to fix the data on the server generating it = before
it goes anywhere neara a proxy or cache. Whatever you do on your own
proxy wil not fix the same problems occuring in ay of the other caches
handling it (guaranteed to be at least 1 other and probably 2-3).

Amos



From squid3 at treenet.co.nz  Mon Jan  9 06:38:45 2017
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Mon, 9 Jan 2017 19:38:45 +1300
Subject: [squid-users] Is it possible to modify cached object?
In-Reply-To: <DB6PR0401MB26807A472A098AA612E6ABA78F650@DB6PR0401MB2680.eurprd04.prod.outlook.com>
References: <1483727739416-4681073.post@n4.nabble.com>
 <b52d58fd-336e-b1b8-1453-38ec44e38870@measurement-factory.com>
 <1483895735144-4681093.post@n4.nabble.com>
 <1483895894250-4681094.post@n4.nabble.com>
 <DB6PR0401MB26807A472A098AA612E6ABA78F650@DB6PR0401MB2680.eurprd04.prod.outlook.com>
Message-ID: <5aa206ba-394e-e613-4c4f-65509c6800ce@treenet.co.nz>

On 9/01/2017 8:41 a.m., Rafael Akchurin wrote:
> Hello Boruc,
> 
> Please use https://en.wikipedia.org/wiki/Data_URI_scheme
> 

boruc:

Also you should not be manually (or even with a Script) editing cache
objects on disk as a routine operation. As Alex already mentioned use
ICAP or eCAP adaptation for that instead - thats one reason why those
adaptation APIs exist.

As I noted in the other email you have to fully stop Squid and restart
with a VERY SLOW full-disk scan of the cache after editing disk objects
to prevent the metadata being mismatched and your edit thrown away. So
dont do it unless its a one-off situation. Even then simply deleting the
bad object and letting Squid find a new copy on next request is better
than editing.

Amos



From squid3 at treenet.co.nz  Mon Jan  9 06:54:33 2017
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Mon, 9 Jan 2017 19:54:33 +1300
Subject: [squid-users] keep source ip when user connect over squid using
 ip:port
In-Reply-To: <241415CB-D95F-4012-A47B-6968E1DB3A44@netstream.ps>
References: <9C2EDD85-6314-4169-A423-EE824AB4598A@netstream.ps>
 <20170107182001.GA7614@fantomas.sk>
 <2B70735D-FE5F-464E-9B44-B50926BADE40@netstream.ps>
 <201701071935.44826.Antony.Stone@squid.open.source.it>
 <241415CB-D95F-4012-A47B-6968E1DB3A44@netstream.ps>
Message-ID: <ca5d8a00-c9a9-716d-9502-f0118c277d72@treenet.co.nz>

On 8/01/2017 7:55 a.m., --Ahmad-- wrote:
> I?m using  squid as bridge like injecting some urls
> 
> thats it 

That "Squid as bridge" and your earlier "squid using my original ip  as
source" is TPROXY.


>> On Jan 7, 2017, at 8:35 PM, Antony Stone wrote:
>>
>> On Saturday 07 January 2017 at 19:23:47, --Ahmad-- wrote:
>>
>>> hey mate i total understand Tporxy with CISCO /wccp

So you know that they are two different features and what each does.

Why are you wasting our time asking this question if you aleady know the
TPROXY feature well?

Amos



From eliezer at ngtech.co.il  Mon Jan  9 07:35:25 2017
From: eliezer at ngtech.co.il (Eliezer  Croitoru)
Date: Mon, 9 Jan 2017 09:35:25 +0200
Subject: [squid-users] Problem building Squid4
In-Reply-To: <1483867911104-4681090.post@n4.nabble.com>
References: <1483867911104-4681090.post@n4.nabble.com>
Message-ID: <030b01d26a4a$f1882000$d4986000$@ngtech.co.il>

With hope I will have time I will try to build a tar.xz of latest 4 branch on 16.04.
The last I have built was 4.0.14.

Eliezer

----
Eliezer Croitoru
Linux System Administrator
Mobile: +972-5-28704261
Email: eliezer at ngtech.co.il


-----Original Message-----
From: squid-users [mailto:squid-users-bounces at lists.squid-cache.org] On Behalf Of matg2s
Sent: Sunday, January 8, 2017 11:32 AM
To: squid-users at lists.squid-cache.org
Subject: [squid-users] Problem building Squid4

Hey,

I'm trying to compile Squid 4 (on Ubuntu 14/16) and am getting the following
error:

In file included from ../../src/base/AsyncCall.h:14:0,
                 from ../../src/comm/IoCallback.h:12,
                 from ../../src/comm.h:12,
                 from ../../src/client_side.h:16,
                 from ServerBump.cc:13:
../../src/base/RefCount.h: In instantiation of ?void RefCount<C>::dereference(const C*) [with C = AnyP::PortCfg]?:
../../src/base/RefCount.h:35:20:   required from ?RefCount<C>::~RefCount()
[with C = AnyP::PortCfg]?
../../src/servers/Server.h:31:23:   required from here
../../src/base/RefCount.h:96:40: error: invalid use of incomplete type ?const class AnyP::PortCfg?
         if (tempP_ && tempP_->unlock() == 0)
                                        ^ In file included from ../../src/MasterXaction.h:12:0,
                 from ../../src/CommCalls.h:16,
                 from ../../src/comm.h:13,
                 from ../../src/client_side.h:16,
                 from ServerBump.cc:13:
../../src/anyp/forward.h:17:7: note: forward declaration of ?class AnyP::PortCfg?
 class PortCfg;
       ^
In file included from ../../src/base/AsyncCall.h:14:0,
                 from ../../src/comm/IoCallback.h:12,
                 from ../../src/comm.h:12,
                 from ../../src/client_side.h:16,
                 from ServerBump.cc:13:
../../src/base/RefCount.h:97:13: error: possible problem detected in invocation of delete operator: [-Werror=delete-incomplete]
             delete tempP_;
             ^
../../src/base/RefCount.h:93:19: error: ?tempP_? has incomplete type [-Werror]
         C const (*tempP_) (p_);
                   ^
In file included from ../../src/MasterXaction.h:12:0,
                 from ../../src/CommCalls.h:16,
                 from ../../src/comm.h:13,
                 from ../../src/client_side.h:16,
                 from ServerBump.cc:13:
../../src/anyp/forward.h:17:7: note: forward declaration of ?class AnyP::PortCfg?
 class PortCfg;
       ^
In file included from ../../src/base/AsyncCall.h:14:0,
                 from ../../src/comm/IoCallback.h:12,
                 from ../../src/comm.h:12,
                 from ../../src/client_side.h:16,
                 from ServerBump.cc:13:
../../src/base/RefCount.h:97:13: note: neither the destructor nor the class-specific operator delete will be called, even if they are declared when the class is defined
             delete tempP_;
             ^
cc1plus: error: unrecognized command line option ?-Wno-deprecated-register?
[-Werror]

I'm using the following to build:
wget http://www.squid-cache.org/Versions/v4/squid-4.0.17.tar.gz
I used various configurations, from:
./configure --enable-ssl-crtd
to
./configure --sysconfdir=/etc/squid  --mandir=/usr/share/man --with-default-user=proxy  --disable-maintainer-mode --disable-dependency-tracking  --enable-inline  --enable-async-io=8 --enable-storeio="ufs,aufs,diskd"  --enable-removal-policies="lru,heap" 
--enable-poll  --enable-delay-pools  --enable-cache-digests --enable-snmp --enable-htcp  --enable-select  --enable-carp  --with-large-files --enable-underscores  --disable-arch-native  --disable-auth --with-krb5-config=no  --disable-external-acl-helpers  --disable-eui
--with-filedescriptors=65536  --enable-epoll  --enable-linux-netfilter --enable-ssl-crtd --with-openssl 

and then "make" but I'm keep getting compilation errors and the problem seems to lies with sslcrtd. 
I used the following versions:
gcc (Ubuntu 5.4.0-6ubuntu1~16.04.4) 5.4.0 20160609
g++ (Ubuntu 5.4.0-6ubuntu1~16.04.4) 5.4.0 20160609
But also other gcc versions (4.8, 4.9 and later) and even clang (3.5) but to no avail.

Any help would be appreciated!
Thanks,
Matt



--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/Problem-building-Squid4-tp4681090.html
Sent from the Squid - Users mailing list archive at Nabble.com.
_______________________________________________
squid-users mailing list
squid-users at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-users



From uhlar at fantomas.sk  Mon Jan  9 09:28:07 2017
From: uhlar at fantomas.sk (Matus UHLAR - fantomas)
Date: Mon, 9 Jan 2017 10:28:07 +0100
Subject: [squid-users] keep source ip when user connect over squid using
 ip:port
In-Reply-To: <ca5d8a00-c9a9-716d-9502-f0118c277d72@treenet.co.nz>
References: <9C2EDD85-6314-4169-A423-EE824AB4598A@netstream.ps>
 <20170107182001.GA7614@fantomas.sk>
 <2B70735D-FE5F-464E-9B44-B50926BADE40@netstream.ps>
 <201701071935.44826.Antony.Stone@squid.open.source.it>
 <241415CB-D95F-4012-A47B-6968E1DB3A44@netstream.ps>
 <ca5d8a00-c9a9-716d-9502-f0118c277d72@treenet.co.nz>
Message-ID: <20170109092807.GA20328@fantomas.sk>

>On 8/01/2017 7:55 a.m., --Ahmad-- wrote:
>> I?m using  squid as bridge like injecting some urls
>>
>> thats it
>
>That "Squid as bridge" and your earlier "squid using my original ip  as
>source" is TPROXY.

>>> On Jan 7, 2017, at 8:35 PM, Antony Stone wrote:
>>>
>>> On Saturday 07 January 2017 at 19:23:47, --Ahmad-- wrote:
>>>
>>>> hey mate i total understand Tporxy with CISCO /wccp

On 09.01.17 19:54, Amos Jeffries wrote:
>So you know that they are two different features and what each does.
>
>Why are you wasting our time asking this question if you aleady know the
>TPROXY feature well?

I have understood the question as "does tproxy work even with configured
proxy?" which I'm also curious about.

tproxy docs only describe intercepting/wccp.

-- 
Matus UHLAR - fantomas, uhlar at fantomas.sk ; http://www.fantomas.sk/
Warning: I wish NOT to receive e-mail advertising to this address.
Varovanie: na tuto adresu chcem NEDOSTAVAT akukolvek reklamnu postu.
Saving Private Ryan...
Private Ryan exists. Overwrite? (Y/N)


From flashdown at data-core.org  Mon Jan  9 14:31:04 2017
From: flashdown at data-core.org (Flashdown)
Date: Mon, 09 Jan 2017 15:31:04 +0100
Subject: [squid-users] Squid 3.5.22 Bug when using Mimetype Detection?
 rep_mime_type
In-Reply-To: <65faf969c6c36b178491b5f319db2e40@treenet.co.nz>
References: <2c3cfc7bfc490465ee71dee437ac50d4@data-core.org>
 <65faf969c6c36b178491b5f319db2e40@treenet.co.nz>
Message-ID: <deaf1dc121c9bce2ffa26e9b039d433c@data-core.org>

Hi Amos,

sry that my reply took that long.

I've tested with Squid 3.5.23 on Debian Stretch and the issue is still 
present. Also I was able to create the same issue with the Online OTRS 
Demo website as I had with our internal one.

I did run it with the debug options you gave me. Since you requested 
more info about my config, I stripped a lot out and made sure the issue 
is still the same. XXXXXXXXXXXX indicates that I replaced the whole line 
with XXX.. to ensure no sensitive data is leaked.

So I found out when allowing an IP without authentication and without 
group membership before the real auth is required for everything else, 
then the issue is triggered when Mimetype detection is used. I could'nt 
find a way to avoid the issue. unless I remove the http_access line for 
the target that should be accessible without authentication and without 
group membership. Or I remove the Mimetype Detection lines or better the 
exception for my group.

I hope you can confirm this as a bug or tell me what I made wrong.

This archiv includes: a small squid.conf, cache.log and access.log
https://data-core.org/debug_issue.zip

Hope you can bring light into the dark :)

Best regards,
Enrico


Am 2017-01-03 09:07, schrieb Amos Jeffries:
> On 2017-01-03 07:33, Flashdown wrote:
>> Hello together,
>> 
>> with Squid 3.5.22 I have switched from using a url-regex to Mime Type
>> Detection, which seemed to work nicely until now... :/
>> 
> 
> The thing to be very wary of with this change is that when reply
> blocking the request does still make it through to the server and any
> processing done there still happens.
> 
> With things like ticketing systems that means the tickets and any auth
> related token creating has actually happened, even if the client is
> prevented from being told what the created token was.
> 
> That is a major difference from URL based blocking, which would reject
> before any server contact.
> 
> 
>> OS: Debian Stretch 4.8.0-1-amd64 #1 SMP Debian 4.8.7-1 (2016-11-13)
>> x86_64 GNU/Linux
>> 
> 
> 
> I really advise going to 3.5.23. Several major issues about sending
> wrong replies on certain occasions were fixed there. New .deb should
> be available by now to fix that.
> 
> 
> 
>> I faced the following Situation:
>> 
>> When I globally deny specific mimetypes using a blockfile, then it
>> performs as it should, so only mime types I defined in the block file
>> are getting blocked, so far so good.
>> 
>> When I do an exception for a group I belong to like unblocking
>> application/octet-stream, then I can download files, so the exception
>> works in the first place.
>> acl mime_IT rep_mime_type application/octet-stream
>> http_reply_access allow IT mime_IT
>> 
>> Normally internal targets are excluded from the Proxy using Proxy
>> Exception lists. But I do not get these settings automatically, so my
>> browser did not contain this exception so I was able to discover the
>> following behavior:
>> 
>> The Issue is occuring when browsing to an internal OTRS Web Server via
>> FQDN (It's a web ticket system) through the proxy I get "Access
>> Denied" from the Proxy on all requests. But when browsing to an online
>> OTRS Demo site with the same OTRS version like this one:
>> http://itsm-demo.otrs.com/otrs/index.pl then it works. When I now try
>> again to access the internal OTRS Server through the proxy it works.
>> That's strange, when I now force reload (CTRL+F5 in Firefox) the
>> internal OTRS Ticketsystems webpage, I get the "access denied" again.
>> 
> 
> How is the auth happening between the users and the proxy to find out
> what the groups are?
>  The reply checks may be interferring with the auth replies.
> 
> 
>> When I remove the exception from the global block list for the group I
>> belong to,- here it's IT- then this issue does not occur and the
>> website is accessible like it should.
>> So I just need to comment out these lines:
>> #acl mime_IT rep_mime_type application/octet-stream
>> #http_reply_access allow IT mime_IT
>> 
>> When I add text/html & application/xml to the global block exception,
>> then this error does not occur anymore.
>> acl mime_IT rep_mime_type application/octet-stream text/html 
>> application/xml
>> http_reply_access allow IT mime_IT
>> 
>> So currently I can workaround the issue in 3 different ways:
>> 1. Do not create a global mimetype block exception for groups I belong 
>> to
>> 2. Browse to the start page of an Online Demo OTRS Site and then
>> reload the internal Website
>> 3. Add text/html & application/xml to my exception even if these
>> Mimetypes are not part of the global block list, so they are not
>> supposed to be blocked. (I just looked at the internal website and it
>> just uses text/html and application/xml on the start page (Login Page)
>> so I added them to the exception list for my group and it worked)
>> 
>> Conclusion: When having a global mime type block and unblocking a
>> specific mime type for a specific group, then this group will most
>> propably face issues with mime types that are not supposed to be
>> blocked. So in case of errors, I need to unblock not blocked mimetypes
>> ,too.
>> 
>> 
>> My Squid config for mime type blocking:
>> ---------------------------------------
>> ## Define Default MIMETYPE ERROR Message and global block access list
>> acl block_mimetypes rep_mime_type "/etc/squid/mimetype_blacklist.acl"
>> deny_info ERR_BLOCKED_FILES block_mimetypes
>> 
>> # Configure Execptions
>> acl mime_IT rep_mime_type application/octet-stream
>> http_reply_access allow IT mime_IT
>> 
>> acl mime_SpecialGroup rep_mime_type application/octet-stream
>> http_reply_access allow SpecialGroup mime_SpecialGroup
>> 
>> 
>> #Applying Global MimeType Block
>> http_reply_access deny block_mimetypes
> 
> Any other http_reply_access rules? the implicit-default behaviour may
> be having an effect if there are.
> 
> 
>> ---------------------------------------
>> Squid main config:
>> ---------------------------------------
>> http_port 8080 ssl-bump generate-host-certificates=on
>> dynamic_cert_mem_cache_size=16MB cert=/*********************
>> ssl_bump splice localhost
>> ssl_bump splice SSL_Exclude
> 
> These splice rules may be related to the issue. Any connection that
> gets spliced may be used by other requests without squid being aware
> of them.
> 
>> ssl_bump bump all
>> sslproxy_cert_error allow SSL_TrustedSites
>> sslproxy_cert_error deny all
>> ---------------------------------------
>> 
>> 
>> 
>> Contents of mimetype_blacklist.acl:
>> ---------------------------------------
> 
> That looks okay to me, just remember that it is a regex pattern. So if
> a type entered there matches a sub-string it can block unexpectedly.
> That includes sub-strings in the type parameters.
> 
> 
>> 1483381150.095    186 10.3.101.23 TCP_DENIED_REPLY/403 4493 GET
>> http://otrs-server.**.**.com/otrs/index.pl - HIER_DIRECT/10.2.1.107
>> text/html
>> 1483381150.118      3 10.3.101.23 TCP_DENIED_REPLY/403 4451 GET
>> http://*****-proxy.**.**.com:8080/squid-internal-static/icons/SN.png -
>> HIER_NONE/- text/html
> 
> FWIW: the text/html is the type of the HTML error page being delivered
> for the 403 response. The content-type for the original response
> payload which was blocked is not logged.
> 
> You can debug with "debug_options 11,2" to get a cache.log trace of
> what messages are happening and what the original server response
> headers were.
> 
> Amos
> 
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users


From vvjoshi5 at gmail.com  Mon Jan  9 15:56:04 2017
From: vvjoshi5 at gmail.com (vinay)
Date: Mon, 9 Jan 2017 07:56:04 -0800 (PST)
Subject: [squid-users] Squid memory leak on ubuntu 14.04
In-Reply-To: <2b2845cf17ab7ed8a5992a2ad640747f@treenet.co.nz>
References: <CAJGZ0h5s9k7txGMTN1tP4wdhKvkb=z_kuK8E5t5jwBpgKj8pUg@mail.gmail.com>
 <565BC47E.6070408@treenet.co.nz>
 <CAJGZ0h4AWgyHbwkujCiUpRPf8AbhegpdxaHzRVNuOCasd-Ho0w@mail.gmail.com>
 <565C0EFE.40904@treenet.co.nz>
 <CAJGZ0h6oTCu2OJqk8u-+X4eaeDiHyawg76xJPskjG=tfv-CESA@mail.gmail.com>
 <CAJGZ0h5Cf0936+5i17UCVN7vm+Y=KPg_+bM0uK1fz1+kM0sv4Q@mail.gmail.com>
 <1483630217278-4681060.post@n4.nabble.com>
 <991a6eaaac7178742e80c15045a128a5@treenet.co.nz>
 <029901d2689f$4358fd80$ca0af880$@ngtech.co.il>
 <2b2845cf17ab7ed8a5992a2ad640747f@treenet.co.nz>
Message-ID: <1483977364672-4681103.post@n4.nabble.com>

Thanks Amos,

I installed Squid 3.5.19 successfully on Ubuntu 14.04. The access log is
getting updated with TCP_MISS/200 for every request. Same time Cache log is
not getting updated. 
I have default squid configuration. Please let me know anything need to be
changed to Cache the contents to get "TCP_HIT" and cache log to get updated.
Thanks in advance



--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/Squid-memory-leak-on-ubuntu-14-04-tp4674855p4681103.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From squid3 at treenet.co.nz  Tue Jan 10 04:51:27 2017
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Tue, 10 Jan 2017 17:51:27 +1300
Subject: [squid-users] Squid 3.5.22 Bug when using Mimetype Detection?
 rep_mime_type
In-Reply-To: <deaf1dc121c9bce2ffa26e9b039d433c@data-core.org>
References: <2c3cfc7bfc490465ee71dee437ac50d4@data-core.org>
 <65faf969c6c36b178491b5f319db2e40@treenet.co.nz>
 <deaf1dc121c9bce2ffa26e9b039d433c@data-core.org>
Message-ID: <ec38b3d7-5d9b-6274-d8b9-f85490426a19@treenet.co.nz>

On 10/01/2017 3:31 a.m., Flashdown wrote:
> Hi Amos,
> 
> sry that my reply took that long.
> 
> I've tested with Squid 3.5.23 on Debian Stretch and the issue is still
> present. Also I was able to create the same issue with the Online OTRS
> Demo website as I had with our internal one.
> 
> I did run it with the debug options you gave me. Since you requested
> more info about my config, I stripped a lot out and made sure the issue
> is still the same. XXXXXXXXXXXX indicates that I replaced the whole line
> with XXX.. to ensure no sensitive data is leaked.
> 
> So I found out when allowing an IP without authentication and without
> group membership before the real auth is required for everything else,
> then the issue is triggered when Mimetype detection is used. I could'nt
> find a way to avoid the issue. unless I remove the http_access line for
> the target that should be accessible without authentication and without
> group membership. Or I remove the Mimetype Detection lines or better the
> exception for my group.

The problem there is that reply time is too late to begin authentication
of the group membership.

> 
> I hope you can confirm this as a bug or tell me what I made wrong.
> 

You need to have http_reply_access bypassing authentication and group
checks when http_access does.


Regarding your config:

> 
> acl SSL_ports port 443
> acl wwwports port 80 443 8082 7212
> acl CONNECT method CONNECT
> 
> # Testing against otrs-demo website http://otrs-demo.otrs.com/otrs/index.pl
> acl testing dst 178.63.99.24
> http_access allow testing
> 
> acl ntlm_users proxy_auth REQUIRED
> http_access deny !ntlm_users all

The 'all' on the end prevents authentication being done. BUT no security
aware client software will send credentials without an auth challenge.
So the above should be essentially "deny all".


> 
> dns_v4_first on
> 
> acl cache_all src all
> acl cache_out dst all
> cache deny cache_all cache_out

The above is equivalent to "cache deny all".

To properly disable HTTP caching configure:
  cache deny all
  cache_mem 0

> 
> acl IT external nt_group IT
> 
> acl block_mimetypes rep_mime_type "/etc/squid/mimetype_blacklist.acl"
> deny_info ERR_BLOCKED_FILES block_mimetypes
> 
> acl mime_IT rep_mime_type application/octet-stream
> 
> http_reply_access allow IT mime_IT
> 
> 
> http_reply_access deny block_mimetypes
> 
> #Added just for fun, somehow same behavior ;) before I used something like http_access allow IT wwwports
> http_access allow all
> 
> http_access deny all
> 
> http_access allow manager localhost
> http_access deny manager
> 
> http_access deny !wwwports
> http_access deny CONNECT !SSL_ports
> 
> http_access allow localhost
> http_access deny all

Amos



From squid3 at treenet.co.nz  Tue Jan 10 06:43:12 2017
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Tue, 10 Jan 2017 19:43:12 +1300
Subject: [squid-users] Squid memory leak on ubuntu 14.04
In-Reply-To: <1483977364672-4681103.post@n4.nabble.com>
References: <CAJGZ0h5s9k7txGMTN1tP4wdhKvkb=z_kuK8E5t5jwBpgKj8pUg@mail.gmail.com>
 <565BC47E.6070408@treenet.co.nz>
 <CAJGZ0h4AWgyHbwkujCiUpRPf8AbhegpdxaHzRVNuOCasd-Ho0w@mail.gmail.com>
 <565C0EFE.40904@treenet.co.nz>
 <CAJGZ0h6oTCu2OJqk8u-+X4eaeDiHyawg76xJPskjG=tfv-CESA@mail.gmail.com>
 <CAJGZ0h5Cf0936+5i17UCVN7vm+Y=KPg_+bM0uK1fz1+kM0sv4Q@mail.gmail.com>
 <1483630217278-4681060.post@n4.nabble.com>
 <991a6eaaac7178742e80c15045a128a5@treenet.co.nz>
 <029901d2689f$4358fd80$ca0af880$@ngtech.co.il>
 <2b2845cf17ab7ed8a5992a2ad640747f@treenet.co.nz>
 <1483977364672-4681103.post@n4.nabble.com>
Message-ID: <a5d1c6ee-8eab-cf7f-ac8c-25ee4fd91828@treenet.co.nz>

On 10/01/2017 4:56 a.m., vinay wrote:
> Thanks Amos,
> 
> I installed Squid 3.5.19 successfully on Ubuntu 14.04. The access log is

Okay. Now its time to upgrade to 3.5.23.

> getting updated with TCP_MISS/200 for every request. Same time Cache log is
> not getting updated. 
> I have default squid configuration.

Squid Project default? or Debian default? or Ubuntu default?

> Please let me know anything need to be
> changed to Cache the contents to get "TCP_HIT" and cache log to get updated.

You need to add a cache_dir to get disk hits (TCP_HIT).

But you should be seeing TCP_MEM_HIT or REFRESH traffic unless you have
disabled cachign entirely, or ar not testign it correctly. Which can
happen if:
* you are using force-reload or refresh button in a browser.
* you are using a non-cacheable website for tests.

Amos



From ahmed.zaeem at netstream.ps  Tue Jan 10 09:20:04 2017
From: ahmed.zaeem at netstream.ps (--Ahmad--)
Date: Tue, 10 Jan 2017 11:20:04 +0200
Subject: [squid-users] squid http speed/ ms
Message-ID: <0CFFEC9B-73DA-4251-BA5D-A1A1D046D884@netstream.ps>

hi folks 
i want to ask .
when i do ping  imp from my squid server  itself to website like aaa.com <http://aaa.com/> lets say i have ping over 10ms
but when i configured my server as squid and visit aaa.com <http://aaa.com/> from squid server itself ..i have like 200 ms

the question here :
why icmp timeout is 10ms  ?
but http timeout is 200 ms ?

is there  a way in squid to make it faster and few http ms like 30 ms ?


cheers 

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20170110/ae7cdb58/attachment.htm>

From Antony.Stone at squid.open.source.it  Tue Jan 10 09:29:34 2017
From: Antony.Stone at squid.open.source.it (Antony Stone)
Date: Tue, 10 Jan 2017 10:29:34 +0100
Subject: [squid-users] squid http speed/ ms
In-Reply-To: <0CFFEC9B-73DA-4251-BA5D-A1A1D046D884@netstream.ps>
References: <0CFFEC9B-73DA-4251-BA5D-A1A1D046D884@netstream.ps>
Message-ID: <201701101029.34434.Antony.Stone@squid.open.source.it>

On Tuesday 10 January 2017 at 10:20:04, --Ahmad-- wrote:

> hi folks
> i want to ask .
> when i do ping  imp from my squid server  itself to website like aaa.com
> <http://aaa.com/> lets say i have ping over 10ms but when i configured my
> server as squid and visit aaa.com <http://aaa.com/> from squid server
> itself ..i have like 200 ms

How are you measuring that 200ms?

> the question here :
> why icmp timeout is 10ms  ?
> but http timeout is 200 ms ?

1. "Timeout" is the wrong term here.  It's more accurate to say "response 
time".

"Timeout" means that something failed because it took too long, whereas what 
you're measuring here is the time taken before it succeeded.


2. Why is the HTTP response time higher than the ICMP response time?

Because compared to ping running over ICMP, HTTP is a complicated protocol, 
running over TCP, which is a complicated, multi-stage protocol, and the 
response has to be generated by an application on the remote server, which is 
probably fetching content from a storage device in order to send it to you.

An ICMP ping requires just a single packet from your machine to the target, a 
simple response in the network stack, and a single packet back to you in 
return.

> is there  a way in squid to make it faster and few http ms like 30 ms ?

It's not Squid which is taking the time - it's the remote server - try 
measuring it without Squid in place and see what the response time is.


Antony.

-- 
One tequila, two tequila, three tequila, floor.

                                                   Please reply to the list;
                                                         please *don't* CC me.


From vvjoshi5 at gmail.com  Tue Jan 10 13:34:34 2017
From: vvjoshi5 at gmail.com (vinay)
Date: Tue, 10 Jan 2017 05:34:34 -0800 (PST)
Subject: [squid-users] Squid memory leak on ubuntu 14.04
In-Reply-To: <a5d1c6ee-8eab-cf7f-ac8c-25ee4fd91828@treenet.co.nz>
References: <CAJGZ0h4AWgyHbwkujCiUpRPf8AbhegpdxaHzRVNuOCasd-Ho0w@mail.gmail.com>
 <565C0EFE.40904@treenet.co.nz>
 <CAJGZ0h6oTCu2OJqk8u-+X4eaeDiHyawg76xJPskjG=tfv-CESA@mail.gmail.com>
 <CAJGZ0h5Cf0936+5i17UCVN7vm+Y=KPg_+bM0uK1fz1+kM0sv4Q@mail.gmail.com>
 <1483630217278-4681060.post@n4.nabble.com>
 <991a6eaaac7178742e80c15045a128a5@treenet.co.nz>
 <029901d2689f$4358fd80$ca0af880$@ngtech.co.il>
 <2b2845cf17ab7ed8a5992a2ad640747f@treenet.co.nz>
 <1483977364672-4681103.post@n4.nabble.com>
 <a5d1c6ee-8eab-cf7f-ac8c-25ee4fd91828@treenet.co.nz>
Message-ID: <1484055274141-4681108.post@n4.nabble.com>

Thanks Amos , for your timely help. 

As mentioned by you, I have configured squid conf file n able to get TCP_HIT
in access logs. Thanks a lot.
My new issue is, my app has 3 types of users. Normal, Editor n Business user
, The contents are getting catched for Normal user n getting TCP_HIT for
Normal user. When I log in with other  users still it's giving TCP_MISS even
though same  contents are loaded.  It should not cache per user . 
Any idea to overcome this issue ? 
Any configurations need to be changed ? 

Thank you in advance. 



--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/Squid-memory-leak-on-ubuntu-14-04-tp4674855p4681108.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From yvoinov at gmail.com  Tue Jan 10 17:00:35 2017
From: yvoinov at gmail.com (Yuri Voinov)
Date: Tue, 10 Jan 2017 23:00:35 +0600
Subject: [squid-users] Squid memory leak on ubuntu 14.04
In-Reply-To: <1484055274141-4681108.post@n4.nabble.com>
References: <CAJGZ0h4AWgyHbwkujCiUpRPf8AbhegpdxaHzRVNuOCasd-Ho0w@mail.gmail.com>
 <565C0EFE.40904@treenet.co.nz>
 <CAJGZ0h6oTCu2OJqk8u-+X4eaeDiHyawg76xJPskjG=tfv-CESA@mail.gmail.com>
 <CAJGZ0h5Cf0936+5i17UCVN7vm+Y=KPg_+bM0uK1fz1+kM0sv4Q@mail.gmail.com>
 <1483630217278-4681060.post@n4.nabble.com>
 <991a6eaaac7178742e80c15045a128a5@treenet.co.nz>
 <029901d2689f$4358fd80$ca0af880$@ngtech.co.il>
 <2b2845cf17ab7ed8a5992a2ad640747f@treenet.co.nz>
 <1483977364672-4681103.post@n4.nabble.com>
 <a5d1c6ee-8eab-cf7f-ac8c-25ee4fd91828@treenet.co.nz>
 <1484055274141-4681108.post@n4.nabble.com>
Message-ID: <dc2071b5-0a74-f8a5-c63f-399c185a82fa@gmail.com>



10.01.2017 19:34, vinay ?????:
> Thanks Amos , for your timely help. 
>
> As mentioned by you, I have configured squid conf file n able to get TCP_HIT
> in access logs. Thanks a lot.
> My new issue is, my app has 3 types of users. Normal, Editor n Business user
> , The contents are getting catched for Normal user n getting TCP_HIT for
> Normal user. When I log in with other  users still it's giving TCP_MISS even
> though same  contents are loaded.  It should not cache per user . 
> Any idea to overcome this issue ? 
This is feature, not an issue. The "issue" name is "Vary" and (possible)
"dynamic content".
> Any configurations need to be changed ? 
Yes. All configuration should be changed.

Start from here:

http://wiki.squid-cache.org/

>
> Thank you in advance. 
>
>
>
> --
> View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/Squid-memory-leak-on-ubuntu-14-04-tp4674855p4681108.html
> Sent from the Squid - Users mailing list archive at Nabble.com.
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users

-- 
What is the fundamental difference between the programmer and by a fag?
Fag never become five times to free the memory of one object. Fag will
not use two almost identical string libraries in the same project. Fag
will never write to a mixture of C and C ++. Fag will never pass objects
by pointer. Now you know why these two categories so often mentioned
together, and one of them is worse :)
-------------- next part --------------
A non-text attachment was scrubbed...
Name: 0x613DEC46.asc
Type: application/pgp-keys
Size: 2437 bytes
Desc: not available
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20170110/168e5fc2/attachment.key>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: signature.asc
Type: application/pgp-signature
Size: 473 bytes
Desc: OpenPGP digital signature
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20170110/168e5fc2/attachment.sig>

From eliezer at ngtech.co.il  Tue Jan 10 17:45:18 2017
From: eliezer at ngtech.co.il (Eliezer  Croitoru)
Date: Tue, 10 Jan 2017 19:45:18 +0200
Subject: [squid-users] Problem building Squid4
In-Reply-To: <1483867911104-4681090.post@n4.nabble.com>
References: <1483867911104-4681090.post@n4.nabble.com>
Message-ID: <03a601d26b69$4ece5990$ec6b0cb0$@ngtech.co.il>

I have compiled squid 4.0.17 on 16.04 fully updated and it's available in tar.xz at:
http://ngtech.co.il/repo/bin/ubuntu/1604/beta/

I am still planning to put it inside a deb file but for now there are others who do that so I am just building it to make sure it builds and works as expected.

Eliezer

----
Eliezer Croitoru
Linux System Administrator
Mobile: +972-5-28704261
Email: eliezer at ngtech.co.il


-----Original Message-----
From: squid-users [mailto:squid-users-bounces at lists.squid-cache.org] On Behalf Of matg2s
Sent: Sunday, January 8, 2017 11:32 AM
To: squid-users at lists.squid-cache.org
Subject: [squid-users] Problem building Squid4

Hey,

I'm trying to compile Squid 4 (on Ubuntu 14/16) and am getting the following
error:

In file included from ../../src/base/AsyncCall.h:14:0,
                 from ../../src/comm/IoCallback.h:12,
                 from ../../src/comm.h:12,
                 from ../../src/client_side.h:16,
                 from ServerBump.cc:13:
../../src/base/RefCount.h: In instantiation of ?void RefCount<C>::dereference(const C*) [with C = AnyP::PortCfg]?:
../../src/base/RefCount.h:35:20:   required from ?RefCount<C>::~RefCount()
[with C = AnyP::PortCfg]?
../../src/servers/Server.h:31:23:   required from here
../../src/base/RefCount.h:96:40: error: invalid use of incomplete type ?const class AnyP::PortCfg?
         if (tempP_ && tempP_->unlock() == 0)
                                        ^ In file included from ../../src/MasterXaction.h:12:0,
                 from ../../src/CommCalls.h:16,
                 from ../../src/comm.h:13,
                 from ../../src/client_side.h:16,
                 from ServerBump.cc:13:
../../src/anyp/forward.h:17:7: note: forward declaration of ?class AnyP::PortCfg?
 class PortCfg;
       ^
In file included from ../../src/base/AsyncCall.h:14:0,
                 from ../../src/comm/IoCallback.h:12,
                 from ../../src/comm.h:12,
                 from ../../src/client_side.h:16,
                 from ServerBump.cc:13:
../../src/base/RefCount.h:97:13: error: possible problem detected in invocation of delete operator: [-Werror=delete-incomplete]
             delete tempP_;
             ^
../../src/base/RefCount.h:93:19: error: ?tempP_? has incomplete type [-Werror]
         C const (*tempP_) (p_);
                   ^
In file included from ../../src/MasterXaction.h:12:0,
                 from ../../src/CommCalls.h:16,
                 from ../../src/comm.h:13,
                 from ../../src/client_side.h:16,
                 from ServerBump.cc:13:
../../src/anyp/forward.h:17:7: note: forward declaration of ?class AnyP::PortCfg?
 class PortCfg;
       ^
In file included from ../../src/base/AsyncCall.h:14:0,
                 from ../../src/comm/IoCallback.h:12,
                 from ../../src/comm.h:12,
                 from ../../src/client_side.h:16,
                 from ServerBump.cc:13:
../../src/base/RefCount.h:97:13: note: neither the destructor nor the class-specific operator delete will be called, even if they are declared when the class is defined
             delete tempP_;
             ^
cc1plus: error: unrecognized command line option ?-Wno-deprecated-register?
[-Werror]

I'm using the following to build:
wget http://www.squid-cache.org/Versions/v4/squid-4.0.17.tar.gz
I used various configurations, from:
./configure --enable-ssl-crtd
to
./configure --sysconfdir=/etc/squid  --mandir=/usr/share/man --with-default-user=proxy  --disable-maintainer-mode --disable-dependency-tracking  --enable-inline  --enable-async-io=8 --enable-storeio="ufs,aufs,diskd"  --enable-removal-policies="lru,heap" 
--enable-poll  --enable-delay-pools  --enable-cache-digests --enable-snmp --enable-htcp  --enable-select  --enable-carp  --with-large-files --enable-underscores  --disable-arch-native  --disable-auth --with-krb5-config=no  --disable-external-acl-helpers  --disable-eui
--with-filedescriptors=65536  --enable-epoll  --enable-linux-netfilter --enable-ssl-crtd --with-openssl 

and then "make" but I'm keep getting compilation errors and the problem seems to lies with sslcrtd. 
I used the following versions:
gcc (Ubuntu 5.4.0-6ubuntu1~16.04.4) 5.4.0 20160609
g++ (Ubuntu 5.4.0-6ubuntu1~16.04.4) 5.4.0 20160609
But also other gcc versions (4.8, 4.9 and later) and even clang (3.5) but to no avail.

Any help would be appreciated!
Thanks,
Matt



--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/Problem-building-Squid4-tp4681090.html
Sent from the Squid - Users mailing list archive at Nabble.com.
_______________________________________________
squid-users mailing list
squid-users at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-users



From stepan.bujnak at gmail.com  Wed Jan 11 01:26:44 2017
From: stepan.bujnak at gmail.com (Stepan Bujnak)
Date: Tue, 10 Jan 2017 17:26:44 -0800
Subject: [squid-users] ERR_CANNOT_FORWARD with Squid + Privoxy
Message-ID: <CA+0ge-TCKk_Q1ceWTnUg6+wUUN7a6fLKhbjGECyTAPxcaKvLWQ@mail.gmail.com>

Hi,

I've been trying to configure intercepting proxy with privoxy as a
cache_peer. This is my Squid configuration:

acl all src all

acl SSL_ports  port 443
acl Safe_ports port 80          # http
acl Safe_ports port 21          # ftp
acl Safe_ports port 443         # https
acl Safe_ports port 70          # gopher
acl Safe_ports port 210         # wais
acl Safe_ports port 1025-65535  # unregistered ports
acl Safe_ports port 280         # http-mgmt
acl Safe_ports port 488         # gss-http
acl Safe_ports port 591         # filemaker
acl Safe_ports port 777         # multiling http
acl CONNECT    method CONNECT

#http_access deny !Safe_ports
#http_access deny CONNECT !SSL_ports
http_access allow all

# stop squid taking forever to restart.
shutdown_lifetime 3 second

client_dst_passthru off
host_verify_strict off

# IMPORTANT! squid requires at least one forward-proxy port configured
#            http://wiki.squid-cache.org/KnowledgeBase/NoForwardProxyPorts
http_port 0.0.0.0:3127
http_port 0.0.0.0:3128 intercept
https_port 0.0.0.0:3129 intercept ssl-bump
generate-host-certificates=on dynamic_cert_mem_cache_size=4MB
cert=/etc/squid/ssl_certs/squid.pem

sslcrtd_program /usr/lib/squid/ssl_crtd -s /var/lib/squid/ssl_db -M
4MB sslcrtd_children 8 startup=1 idle=1
sslproxy_capath /etc/ssl/certs

acl step1 at_step SslBump1
ssl_bump peek step1
ssl_bump bump all

cache_peer 127.0.0.1 parent 8118 7 no-query default no-digest
no-netdb-exchange proxy-only ssl
never_direct allow all

cache_mem 8 MB
maximum_object_size_in_memory 32 KB

# Disable the Via and X-Forwarded-For field from the request header to avoid
# leaking the use of a proxy and client ip address
via off
forwarded_for off
follow_x_forwarded_for deny all
request_header_access X-Forwarded-For deny all

#cache_dir ufs /var/spool/squid 1024 16 256
#coredump_dir /var/cache/squid
cache deny all

refresh_pattern ^ftp:           1440    20%     10080
refresh_pattern ^gopher:        1440    0%      1440
refresh_pattern -i (/cgi-bin/|\?) 0     0%      0
refresh_pattern .               0       20%     4320


Now when making a request, privoxy prints out following:

2017-01-11 00:36:51.420 7fe4872a4700 Connect: Accepted connection from
127.0.0.1 on socket 4
2017-01-11 00:36:51.421 7fe4872a4700 Received: from socket 4:
\x16\x03\x01\x010\x01\x00\x01,\x03\x03xfOz\xc3\xc2\xf8\xf6\xc4\x972Y\xe5w\xf0\xd7\x98\xb5\xd3\x99\xfb\x97P%\x0aX\x1f\xefs\x91\xc6d\x00\x00\xaa\xc00\xc0,\xc0(\xc0$\xc0\x14\xc0\x0a\x00\xa5\x00\xa3\x00\xa1\x00\x9f\x00k\x00j\x00i\x00h\x009\x008\x007\x006\x00\x88\x00\x87\x00\x86\x00\x85\xc02\xc0.\xc0*\xc0&\xc0\x0f\xc0\x05\x00\x9d\x00=\x005\x00\x84\xc0/\xc0+\xc0'\xc0#\xc0\x13\xc0\x09\x00\xa4\x00\xa2\x00\xa0\x00\x9e\x00g\x00@\x00?\x00>\x003\x002\x001\x000\x00\x9a\x00\x99\x00\x98\x00\x97\x00E\x00D\x00C\x00B\xc01\xc0-\xc0)\xc0%\xc0\x0e\xc0\x04\x00\x9c\x00<\x00/\x00\x96\x00A\xc0\x11\xc0\x07\xc0\x0c\xc0\x02\x00\x05\x00\x04\xc0\x12\xc0\x08\x00\x16\x00\x13\x00\x10\x00\x0d\xc0\x0d\xc0\x03\x00\x0a\x00\xff\x01\x00\x00Y\x00\x0b\x00\x04\x03\x00\x01\x02\x00\x0a\x00\x1c\x00\x1a\x00\x17\x00\x19\x00\x1c\x00\x1b\x00\x18\x00\x1a\x00\x16\x00\x0e\x00\x0d\x00\x0b\x00\x0c\x00\x09\x00\x0a\x00#\x00\x00\x00\x0d\x00
\x00\x1e\x06\x01\x06\x02\x06\x03\x05\x01\x05\x02\x05\x03\x04\x01\x04\x02\x04\x03\x03\x01\x03\x02\x03\x03\x02\x01\x02\x02\x02\x03\x00\x0f\x00\x01\x013t\x00\x00
2017-01-11 00:37:21.450 7fe4872a4700 Connect: The client side of the
connection on socket 4 got closed without sending a complete request
line.


It seems like the bumped request is missing the CONNECT line and
privoxy gets confused.

Squid version:

Squid Cache: Version 3.5.23
Service Name: squid
configure options:  'CHOST=x86_64-pc-linux-gnu' 'CFLAGS=-march=core2
-O2 -pipe' 'CXXFLAGS=' '--build=x86_64-linux-gnu' '--prefix=/usr'
'--exec-prefix=/usr' '--bindir=/usr/bin' '--sbindir=/usr/sbin'
'--libdir=/usr/lib' '--sharedstatedir=/usr/com'
'--includedir=/usr/include' '--localstatedir=/var'
'--libexecdir=/usr/lib/squid' '--srcdir=.'
'--datadir=/usr/share/squid' '--sysconfdir=/etc/squid'
'--infodir=/usr/share/info' '--mandir=/usr/share/man'
'--x-includes=/usr/include' '--x-libraries=/usr/lib'
'--with-default-user=proxy' '--with-logdir=/var/log/squid'
'--with-pidfile=/var/run/squid.pid' '--enable-storeio=ufs,aufs,diskd'
'--enable-linux-netfilter' '--enable-removal-policies=lru,heap'
'--enable-gnuregex' '--enable-follow-x-forwarded-for'
'--enable-x-accelerator-vary' '--enable-zph-qos'
'--enable-delay-pools' '--enable-snmp' '--enable-underscores'
'--with-openssl' '--enable-ssl-crtd' '--enable-http-violations'
'--enable-async-io=24' '--enable-storeid-rewrite-helpers'
'--with-large-files' '--with-libcap' '--with-netfilter-conntrack'
'--with-included-ltdl' '--with-maxfd=65536'
'--with-filedescriptors=65536' '--with-pthreads' '--without-gnutls'
'--without-mit-krb5' '--without-heimdal-krb5' '--without-gnugss'
'--disable-icap-client' '--disable-wccp' '--disable-wccpv2'
'--disable-dependency-tracking' '--disable-auth' '--disable-epoll'
'--disable-ident-lookups' '--disable-icmp'
'build_alias=x86_64-linux-gnu' --enable-ltdl-convenience


As a result, the client receives ERR_CANNOT_FORWARD. Could someone
point me to the right direction? Thank you.


From fredbmail at free.fr  Wed Jan 11 10:37:11 2017
From: fredbmail at free.fr (FredB)
Date: Wed, 11 Jan 2017 11:37:11 +0100 (CET)
Subject: [squid-users] SSL_bump and source IP
In-Reply-To: <722880849.22121122.1484130842412.JavaMail.root@zimbra4-e1.priv.proxad.net>
Message-ID: <207529384.22130386.1484131031996.JavaMail.root@zimbra4-e1.priv.proxad.net>

Hello,

I'm searching a way to exclude an user (account) or an IP from my lan 
I can exclude a destination domain to decryption with SSL_bump but not all requests from a specific source, maybe because I'm using x-forwarded ?

Thanks

Fred  


From anand at visolve.com  Wed Jan 11 11:44:19 2017
From: anand at visolve.com (anand)
Date: Wed, 11 Jan 2017 17:14:19 +0530
Subject: [squid-users] squid-users Digest, Vol 29, Issue 21
In-Reply-To: <mailman.5856.1484131042.20516.squid-users@lists.squid-cache.org>
References: <mailman.5856.1484131042.20516.squid-users@lists.squid-cache.org>
Message-ID: <24abc21d-a56b-32dd-caae-77aa77754cf9@visolve.com>

Hello Vinay,

Please verify the web content is "Static/Dynamic", if the content is 
Static, user requests will be logged TCP_HIT/TCP_REFRESH.

If the web content is "Dynamic", user requests will be refreshed from 
web server.

Thanks,
Anand P

On 1/11/2017 4:07 PM, squid-users-request at lists.squid-cache.org wrote:
> Send squid-users mailing list submissions to
> 	squid-users at lists.squid-cache.org
>
> To subscribe or unsubscribe via the World Wide Web, visit
> 	http://lists.squid-cache.org/listinfo/squid-users
> or, via email, send a message with subject or body 'help' to
> 	squid-users-request at lists.squid-cache.org
>
> You can reach the person managing the list at
> 	squid-users-owner at lists.squid-cache.org
>
> When replying, please edit your Subject line so it is more specific
> than "Re: Contents of squid-users digest..."
>
>
> Today's Topics:
>
>     1. Re: Squid memory leak on ubuntu 14.04 (vinay)
>     2. Re: Squid memory leak on ubuntu 14.04 (Yuri Voinov)
>     3. Re: Problem building Squid4 (Eliezer  Croitoru)
>     4. ERR_CANNOT_FORWARD with Squid + Privoxy (Stepan Bujnak)
>     5. SSL_bump and source IP (FredB)
>
>
> ----------------------------------------------------------------------
>
> Message: 1
> Date: Tue, 10 Jan 2017 05:34:34 -0800 (PST)
> From: vinay <vvjoshi5 at gmail.com>
> To: squid-users at lists.squid-cache.org
> Subject: Re: [squid-users] Squid memory leak on ubuntu 14.04
> Message-ID: <1484055274141-4681108.post at n4.nabble.com>
> Content-Type: text/plain; charset=us-ascii
>
> Thanks Amos , for your timely help.
>
> As mentioned by you, I have configured squid conf file n able to get TCP_HIT
> in access logs. Thanks a lot.
> My new issue is, my app has 3 types of users. Normal, Editor n Business user
> , The contents are getting catched for Normal user n getting TCP_HIT for
> Normal user. When I log in with other  users still it's giving TCP_MISS even
> though same  contents are loaded.  It should not cache per user .
> Any idea to overcome this issue ?
> Any configurations need to be changed ?
>
> Thank you in advance.
>
>
>
> --
> View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/Squid-memory-leak-on-ubuntu-14-04-tp4674855p4681108.html
> Sent from the Squid - Users mailing list archive at Nabble.com.
>
>
> ------------------------------
>
> Message: 2
> Date: Tue, 10 Jan 2017 23:00:35 +0600
> From: Yuri Voinov <yvoinov at gmail.com>
> To: squid-users at lists.squid-cache.org
> Subject: Re: [squid-users] Squid memory leak on ubuntu 14.04
> Message-ID: <dc2071b5-0a74-f8a5-c63f-399c185a82fa at gmail.com>
> Content-Type: text/plain; charset="utf-8"
>
>
>
> 10.01.2017 19:34, vinay ?????:
>> Thanks Amos , for your timely help.
>>
>> As mentioned by you, I have configured squid conf file n able to get TCP_HIT
>> in access logs. Thanks a lot.
>> My new issue is, my app has 3 types of users. Normal, Editor n Business user
>> , The contents are getting catched for Normal user n getting TCP_HIT for
>> Normal user. When I log in with other  users still it's giving TCP_MISS even
>> though same  contents are loaded.  It should not cache per user .
>> Any idea to overcome this issue ?
> This is feature, not an issue. The "issue" name is "Vary" and (possible)
> "dynamic content".
>> Any configurations need to be changed ?
> Yes. All configuration should be changed.
>
> Start from here:
>
> http://wiki.squid-cache.org/
>
>> Thank you in advance.
>>
>>
>>
>> --
>> View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/Squid-memory-leak-on-ubuntu-14-04-tp4674855p4681108.html
>> Sent from the Squid - Users mailing list archive at Nabble.com.
>> _______________________________________________
>> squid-users mailing list
>> squid-users at lists.squid-cache.org
>> http://lists.squid-cache.org/listinfo/squid-users



From uhlar at fantomas.sk  Wed Jan 11 11:50:50 2017
From: uhlar at fantomas.sk (Matus UHLAR - fantomas)
Date: Wed, 11 Jan 2017 12:50:50 +0100
Subject: [squid-users] SSL_bump and source IP
In-Reply-To: <207529384.22130386.1484131031996.JavaMail.root@zimbra4-e1.priv.proxad.net>
References: <722880849.22121122.1484130842412.JavaMail.root@zimbra4-e1.priv.proxad.net>
 <207529384.22130386.1484131031996.JavaMail.root@zimbra4-e1.priv.proxad.net>
Message-ID: <20170111115050.GC8862@fantomas.sk>

On 11.01.17 11:37, FredB wrote:
>I'm searching a way to exclude an user (account) or an IP from my lan
>I can exclude a destination domain to decryption with SSL_bump

simply define an ACL and deny bumping it.

> but not all requests from a specific source

what do you mean here?

>, maybe because I'm using x-forwarded ?

x-forwarded-for has nothing to do with this

Maybe you should rephrase the question so we understant you better.
-- 
Matus UHLAR - fantomas, uhlar at fantomas.sk ; http://www.fantomas.sk/
Warning: I wish NOT to receive e-mail advertising to this address.
Varovanie: na tuto adresu chcem NEDOSTAVAT akukolvek reklamnu postu.
I wonder how much deeper the ocean would be without sponges. 


From fredbmail at free.fr  Wed Jan 11 12:04:00 2017
From: fredbmail at free.fr (FredB)
Date: Wed, 11 Jan 2017 13:04:00 +0100 (CET)
Subject: [squid-users] SSL_bump and source IP
In-Reply-To: <207529384.22130386.1484131031996.JavaMail.root@zimbra4-e1.priv.proxad.net>
Message-ID: <1427329519.22421277.1484136240333.JavaMail.root@zimbra4-e1.priv.proxad.net>


> but not all requests from a specific source

> what do you mean here?

I mean no ssl-bump at all for a specific user, no matter the destinations
I tried some acl without success

>>, maybe because I'm using x-forwarded ?

> x-forwarded-for has nothing to do with this

There is a known bug with sslbump and x-forwarded (bug about log) maybe there is a relation, my "fake" address is not known or something like this


From rahat.khan at pribno.com  Wed Jan 11 13:20:54 2017
From: rahat.khan at pribno.com (Rahat Ali Khan)
Date: Wed, 11 Jan 2017 18:20:54 +0500
Subject: [squid-users] Squid Memory Problems
Message-ID: <966d0ce1-a8be-e1d2-541d-f8a3ea942763@pribno.com>

Hi there,

We are using Squid 3.5.14 on Ubuntu 14.04 LTS.  The squid is using 4 
worker process and is used as an explicit and intercept proxy.

We have been facing a problem of squid processes consuming the memory 
gradually and eventually it comes to a minimum threshold. The end user 
browsing experience becomes extremely slow at that time. Cache memory 
configured is 2 GB and total memory of squid machines is 8 and 16 GB on 
different boxes. The memory consumption graphs shows a gradual decrease 
in available physical RAM and we have to ultimately restart the squid 
boxed after 3-4 days.

The stats we get from cachemgr show:

Cache information for squid:
         Hits as % of all requests:      5min: 1.7%, 60min: 2.2%
         Hits as % of bytes sent:        5min: 0.2%, 60min: 0.1%
         Memory hits as % of hit requests:       5min: 91.7%, 60min: 82.8%
         Disk hits as % of hit requests: 5min: 0.0%, 60min: 0.0%
         Storage Swap size:      0 KB
         Storage Swap capacity:   0.0% used,  0.0% free
         Storage Mem size:       2097056 KB
         Storage Mem capacity:   100.0% used,  0.0% free
         Mean Object Size:       0.00 KB

We are suspecting memory leaks to be the cause of this. Please share you 
experiences .

Thanks

Rahat



---
This email has been checked for viruses by Avast antivirus software.
https://www.avast.com/antivirus
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20170111/fb7e0586/attachment.htm>

From eliezer at ngtech.co.il  Wed Jan 11 13:53:59 2017
From: eliezer at ngtech.co.il (Eliezer  Croitoru)
Date: Wed, 11 Jan 2017 15:53:59 +0200
Subject: [squid-users] Squid Memory Problems
In-Reply-To: <966d0ce1-a8be-e1d2-541d-f8a3ea942763@pribno.com>
References: <966d0ce1-a8be-e1d2-541d-f8a3ea942763@pribno.com>
Message-ID: <043001d26c12$2902dbb0$7b089310$@ngtech.co.il>

There was a report about an issue and the claim was Ubuntu Kernel issue.
I believe that the first step would be to find out if a kernel downgrade or upgrade (to xenial one) helps with the issue.

Eliezer

----
http://ngtech.co.il/lmgtfy/
Linux System Administrator
Mobile: +972-5-28704261
Email: eliezer at ngtech.co.il


From: squid-users [mailto:squid-users-bounces at lists.squid-cache.org] On Behalf Of Rahat Ali Khan
Sent: Wednesday, January 11, 2017 3:21 PM
To: squid-users at lists.squid-cache.org
Subject: [squid-users] Squid Memory Problems

Hi there,
We are using Squid 3.5.14 on Ubuntu 14.04 LTS.  The squid is using 4 worker process and is used as an explicit and intercept proxy.
We have been facing a problem of squid processes consuming the memory gradually and eventually it comes to a minimum threshold. The end user browsing experience becomes extremely slow at that time. Cache memory configured is 2 GB and total memory of squid machines is 8 and 16 GB on different boxes. The memory consumption graphs shows a gradual decrease in available physical RAM  and we have to ultimately restart the squid boxed after 3-4 days.
The stats we get from cachemgr show:
Cache information for squid:
        Hits as % of all requests:      5min: 1.7%, 60min: 2.2%
        Hits as % of bytes sent:        5min: 0.2%, 60min: 0.1%
        Memory hits as % of hit requests:       5min: 91.7%, 60min: 82.8%
        Disk hits as % of hit requests: 5min: 0.0%, 60min: 0.0%
        Storage Swap size:      0 KB
        Storage Swap capacity:   0.0% used,  0.0% free
        Storage Mem size:       2097056 KB
        Storage Mem capacity:   100.0% used,  0.0% free
        Mean Object Size:       0.00 KB

We are suspecting memory leaks to be the cause of this. Please share you experiences .
Thanks
Rahat
 

________________________________________

This email has been checked for viruses by Avast antivirus software. 
https://www.avast.com/sig-email?utm_medium=email&utm_source=link&utm_campaign=sig-email&utm_content=emailclient 




From uhlar at fantomas.sk  Wed Jan 11 14:08:13 2017
From: uhlar at fantomas.sk (Matus UHLAR - fantomas)
Date: Wed, 11 Jan 2017 15:08:13 +0100
Subject: [squid-users] Squid Memory Problems
In-Reply-To: <966d0ce1-a8be-e1d2-541d-f8a3ea942763@pribno.com>
References: <966d0ce1-a8be-e1d2-541d-f8a3ea942763@pribno.com>
Message-ID: <20170111140813.GB19926@fantomas.sk>

On 11.01.17 18:20, Rahat Ali Khan wrote:
>We are using Squid 3.5.14 on Ubuntu 14.04 LTS.  The squid is using 4 
>worker process and is used as an explicit and intercept proxy.
>
>We have been facing a problem of squid processes consuming the memory 
>gradually and eventually it comes to a minimum threshold. The end 
>user browsing experience becomes extremely slow at that time. Cache 
>memory configured is 2 GB and total memory of squid machines is 8 and 
>16 GB on different boxes. The memory consumption graphs shows a 
>gradual decrease in available physical RAM and we have to ultimately 
>restart the squid boxed after 3-4 days.

I would like to note that squid does not use memory only for cache, but for
many different uses.

do you use shared memory cache?

-- 
Matus UHLAR - fantomas, uhlar at fantomas.sk ; http://www.fantomas.sk/
Warning: I wish NOT to receive e-mail advertising to this address.
Varovanie: na tuto adresu chcem NEDOSTAVAT akukolvek reklamnu postu.
He who laughs last thinks slowest. 


From squid3 at treenet.co.nz  Wed Jan 11 14:23:37 2017
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 12 Jan 2017 03:23:37 +1300
Subject: [squid-users] ERR_CANNOT_FORWARD with Squid + Privoxy
In-Reply-To: <CA+0ge-TCKk_Q1ceWTnUg6+wUUN7a6fLKhbjGECyTAPxcaKvLWQ@mail.gmail.com>
References: <CA+0ge-TCKk_Q1ceWTnUg6+wUUN7a6fLKhbjGECyTAPxcaKvLWQ@mail.gmail.com>
Message-ID: <e18fd2e1-693e-4e26-57c9-7722e0d938b9@treenet.co.nz>

On 11/01/2017 2:26 p.m., Stepan Bujnak wrote:
> Hi,
> 
> I've been trying to configure intercepting proxy with privoxy as a
> cache_peer. This is my Squid configuration:
> 
> acl all src all
> 
> acl SSL_ports  port 443
> acl Safe_ports port 80          # http
> acl Safe_ports port 21          # ftp
> acl Safe_ports port 443         # https
> acl Safe_ports port 70          # gopher
> acl Safe_ports port 210         # wais
> acl Safe_ports port 1025-65535  # unregistered ports
> acl Safe_ports port 280         # http-mgmt
> acl Safe_ports port 488         # gss-http
> acl Safe_ports port 591         # filemaker
> acl Safe_ports port 777         # multiling http
> acl CONNECT    method CONNECT
> 
> #http_access deny !Safe_ports
> #http_access deny CONNECT !SSL_ports
> http_access allow all
> 
> # stop squid taking forever to restart.
> shutdown_lifetime 3 second
> 
> client_dst_passthru off
> host_verify_strict off

Please pay attention to the docs for these options. Specifically how it
says host_verify_strict has no effect on intercepted traffic. Also how
it says client_dst_passthru has no effect when the Host verify process
detects an origin mismatch (eg 'fails').


> 
> # IMPORTANT! squid requires at least one forward-proxy port configured
> #            http://wiki.squid-cache.org/KnowledgeBase/NoForwardProxyPorts
> http_port 0.0.0.0:3127
> http_port 0.0.0.0:3128 intercept
> https_port 0.0.0.0:3129 intercept ssl-bump
> generate-host-certificates=on dynamic_cert_mem_cache_size=4MB
> cert=/etc/squid/ssl_certs/squid.pem
> 
> sslcrtd_program /usr/lib/squid/ssl_crtd -s /var/lib/squid/ssl_db -M
> 4MB sslcrtd_children 8 startup=1 idle=1
> sslproxy_capath /etc/ssl/certs
> 
> acl step1 at_step SslBump1
> ssl_bump peek step1
> ssl_bump bump all

So what you have configured is Server-first bumping.

All clients will be presented with the Privoxy SSL certificate as if
Privoxy (at 127.0.0.1:8118) was the authoritative web server for the
HTTPS website being fetched.

"What could go wrong?" as the saying goes. A better question would be
what could possibly go _right_ in that setup. Very few websites will
work, and only where the TLS was completely broken in the first place.


> 
> cache_peer 127.0.0.1 parent 8118 7 no-query default no-digest
> no-netdb-exchange proxy-only ssl
> never_direct allow all
> 
> cache_mem 8 MB
> maximum_object_size_in_memory 32 KB
> 
> # Disable the Via and X-Forwarded-For field from the request header to avoid
> # leaking the use of a proxy and client ip address
> via off
> forwarded_for off

The above injects "X-Forwarded-For: unknown" into the traffic. Squid has
long ago moved past the point where that was the only choice.

Use "forwarded_for transparent" instead for privacy. Then you can remove
the following two lines as well...

> follow_x_forwarded_for deny all
> request_header_access X-Forwarded-For deny all
> 
> #cache_dir ufs /var/spool/squid 1024 16 256
> #coredump_dir /var/cache/squid
> cache deny all
> 
> refresh_pattern ^ftp:           1440    20%     10080
> refresh_pattern ^gopher:        1440    0%      1440
> refresh_pattern -i (/cgi-bin/|\?) 0     0%      0
> refresh_pattern .               0       20%     4320
> 
> 
> Now when making a request, privoxy prints out following:
> 
> 2017-01-11 00:36:51.420 7fe4872a4700 Connect: Accepted connection from
> 127.0.0.1 on socket 4
> 2017-01-11 00:36:51.421 7fe4872a4700 Received: from socket 4:
> \x16\x03\x01\x010\x01\x00\x01,\x03\x03xfOz\xc3\xc2\xf8\xf6\xc4\x972Y\xe5w\xf0\xd7\x98\xb5\xd3\x99\xfb\x97P%\x0aX\x1f\xefs\x91\xc6d\x00\x00\xaa\xc00\xc0,\xc0(\xc0$\xc0\x14\xc0\x0a\x00\xa5\x00\xa3\x00\xa1\x00\x9f\x00k\x00j\x00i\x00h\x009\x008\x007\x006\x00\x88\x00\x87\x00\x86\x00\x85\xc02\xc0.\xc0*\xc0&\xc0\x0f\xc0\x05\x00\x9d\x00=\x005\x00\x84\xc0/\xc0+\xc0'\xc0#\xc0\x13\xc0\x09\x00\xa4\x00\xa2\x00\xa0\x00\x9e\x00g\x00@\x00?\x00>\x003\x002\x001\x000\x00\x9a\x00\x99\x00\x98\x00\x97\x00E\x00D\x00C\x00B\xc01\xc0-\xc0)\xc0%\xc0\x0e\xc0\x04\x00\x9c\x00<\x00/\x00\x96\x00A\xc0\x11\xc0\x07\xc0\x0c\xc0\x02\x00\x05\x00\x04\xc0\x12\xc0\x08\x00\x16\x00\x13\x00\x10\x00\x0d\xc0\x0d\xc0\x03\x00\x0a\x00\xff\x01\x00\x00Y\x00\x0b\x00\x04\x03\x00\x01\x02\x00\x0a\x00\x1c\x00\x1a\x00\x17\x00\x19\x00\x1c\x00\x1b\x00\x18\x00\x1a\x00\x16\x00\x0e\x00\x0d\x00\x0b\x00\x0c\x00\x09\x00\x0a\x00#\x00\x00\x00\x0d\x00
> \x00\x1e\x06\x01\x06\x02\x06\x03\x05\x01\x05\x02\x05\x03\x04\x01\x04\x02\x04\x03\x03\x01\x03\x02\x03\x03\x02\x01\x02\x02\x02\x03\x00\x0f\x00\x01\x013t\x00\x00
> 2017-01-11 00:37:21.450 7fe4872a4700 Connect: The client side of the
> connection on socket 4 got closed without sending a complete request
> line.
> 
> 
> It seems like the bumped request is missing the CONNECT line and
> privoxy gets confused.

There is no CONNECT.

You configured Squid to SSL-Bump the HTTPS traffic. That means the
CONNECT gets absorbed by Squid and the TLS protocol to the client gets
terminated, and a new TLS connection is opened to the Privoxy to fetch
the server certificate (or not as this case shows).

The connection is native TLS between Squid and privoxy. As configured by
the 'ssl' option on cache_peer. Since Privoxy cannot handle TLS that
fails, and you get the forwarding error.

And before you ask, no Squid-3 will not send SSL-Bump'ed traffic through
a peer without that 'ssl' option. Squid-4 has some improvements for
non-HTTPS traffic, but still not to the point of Squid generating peer
CONNECT messages after bumping HTTPS.


> 
> As a result, the client receives ERR_CANNOT_FORWARD. Could someone
> point me to the right direction? Thank you.

Your best hope is to recreate in squid.conf settings the privacy
operations you are using privoxy for. Then remove privoxy from the chain
of proxies.

Amos



From squid3 at treenet.co.nz  Wed Jan 11 14:26:36 2017
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 12 Jan 2017 03:26:36 +1300
Subject: [squid-users] SSL_bump and source IP
In-Reply-To: <1427329519.22421277.1484136240333.JavaMail.root@zimbra4-e1.priv.proxad.net>
References: <1427329519.22421277.1484136240333.JavaMail.root@zimbra4-e1.priv.proxad.net>
Message-ID: <37219c2a-d8a8-b259-052a-26ce929facde@treenet.co.nz>

On 12/01/2017 1:04 a.m., FredB wrote:
> 
>> but not all requests from a specific source
> 
>> what do you mean here?
> 
> I mean no ssl-bump at all for a specific user, no matter the destinations
> I tried some acl without success

At the time of bumping Squid has no idea what a "user" is and things
like the X-Forwarded-For are probably also unknown/unavailable.

All you can assume being known about the client is the TCP detail
(IP:port), perhapse an IDENT label or TOS marking. Though I'm not sure
of the latter two.


> 
>>> , maybe because I'm using x-forwarded ?
> 
>> x-forwarded-for has nothing to do with this
> 
> There is a known bug with sslbump and x-forwarded (bug about log) maybe there is a relation, my "fake" address is not known or something like this

That bug is relevant only in the case of clients being configured to use
the proxy as a forward/explicit proxy (no intercept or tproxy). In the
non-relevant traffic types XFF header is simply not existing, period.

Amos



From vvjoshi5 at gmail.com  Wed Jan 11 14:33:48 2017
From: vvjoshi5 at gmail.com (Vidyadhish Joshi)
Date: Wed, 11 Jan 2017 20:03:48 +0530
Subject: [squid-users] squid-users Digest, Vol 29, Issue 21
In-Reply-To: <24abc21d-a56b-32dd-caae-77aa77754cf9@visolve.com>
References: <mailman.5856.1484131042.20516.squid-users@lists.squid-cache.org>
 <24abc21d-a56b-32dd-caae-77aa77754cf9@visolve.com>
Message-ID: <CAMRD5gTBLLGRY--5o3nM8jar90SJ=CjGn9XbyjKgGb+fsOCLog@mail.gmail.com>

Thanks Anand, I will go through it.

On 11-Jan-2017 5:14 PM, "anand" <anand at visolve.com> wrote:

> Hello Vinay,
>
> Please verify the web content is "Static/Dynamic", if the content is
> Static, user requests will be logged TCP_HIT/TCP_REFRESH.
>
> If the web content is "Dynamic", user requests will be refreshed from web
> server.
>
> Thanks,
> Anand P
>
> On 1/11/2017 4:07 PM, squid-users-request at lists.squid-cache.org wrote:
>
>> Send squid-users mailing list submissions to
>>         squid-users at lists.squid-cache.org
>>
>> To subscribe or unsubscribe via the World Wide Web, visit
>>         http://lists.squid-cache.org/listinfo/squid-users
>> or, via email, send a message with subject or body 'help' to
>>         squid-users-request at lists.squid-cache.org
>>
>> You can reach the person managing the list at
>>         squid-users-owner at lists.squid-cache.org
>>
>> When replying, please edit your Subject line so it is more specific
>> than "Re: Contents of squid-users digest..."
>>
>>
>> Today's Topics:
>>
>>     1. Re: Squid memory leak on ubuntu 14.04 (vinay)
>>     2. Re: Squid memory leak on ubuntu 14.04 (Yuri Voinov)
>>     3. Re: Problem building Squid4 (Eliezer  Croitoru)
>>     4. ERR_CANNOT_FORWARD with Squid + Privoxy (Stepan Bujnak)
>>     5. SSL_bump and source IP (FredB)
>>
>>
>> ----------------------------------------------------------------------
>>
>> Message: 1
>> Date: Tue, 10 Jan 2017 05:34:34 -0800 (PST)
>> From: vinay <vvjoshi5 at gmail.com>
>> To: squid-users at lists.squid-cache.org
>> Subject: Re: [squid-users] Squid memory leak on ubuntu 14.04
>> Message-ID: <1484055274141-4681108.post at n4.nabble.com>
>> Content-Type: text/plain; charset=us-ascii
>>
>> Thanks Amos , for your timely help.
>>
>> As mentioned by you, I have configured squid conf file n able to get
>> TCP_HIT
>> in access logs. Thanks a lot.
>> My new issue is, my app has 3 types of users. Normal, Editor n Business
>> user
>> , The contents are getting catched for Normal user n getting TCP_HIT for
>> Normal user. When I log in with other  users still it's giving TCP_MISS
>> even
>> though same  contents are loaded.  It should not cache per user .
>> Any idea to overcome this issue ?
>> Any configurations need to be changed ?
>>
>> Thank you in advance.
>>
>>
>>
>> --
>> View this message in context: http://squid-web-proxy-cache.1
>> 019090.n4.nabble.com/Squid-memory-leak-on-ubuntu-14-04-tp467
>> 4855p4681108.html
>> Sent from the Squid - Users mailing list archive at Nabble.com.
>>
>>
>> ------------------------------
>>
>> Message: 2
>> Date: Tue, 10 Jan 2017 23:00:35 +0600
>> From: Yuri Voinov <yvoinov at gmail.com>
>> To: squid-users at lists.squid-cache.org
>> Subject: Re: [squid-users] Squid memory leak on ubuntu 14.04
>> Message-ID: <dc2071b5-0a74-f8a5-c63f-399c185a82fa at gmail.com>
>> Content-Type: text/plain; charset="utf-8"
>>
>>
>>
>> 10.01.2017 19:34, vinay ?????:
>>
>>> Thanks Amos , for your timely help.
>>>
>>> As mentioned by you, I have configured squid conf file n able to get
>>> TCP_HIT
>>> in access logs. Thanks a lot.
>>> My new issue is, my app has 3 types of users. Normal, Editor n Business
>>> user
>>> , The contents are getting catched for Normal user n getting TCP_HIT for
>>> Normal user. When I log in with other  users still it's giving TCP_MISS
>>> even
>>> though same  contents are loaded.  It should not cache per user .
>>> Any idea to overcome this issue ?
>>>
>> This is feature, not an issue. The "issue" name is "Vary" and (possible)
>> "dynamic content".
>>
>>> Any configurations need to be changed ?
>>>
>> Yes. All configuration should be changed.
>>
>> Start from here:
>>
>> http://wiki.squid-cache.org/
>>
>> Thank you in advance.
>>>
>>>
>>>
>>> --
>>> View this message in context: http://squid-web-proxy-cache.1
>>> 019090.n4.nabble.com/Squid-memory-leak-on-ubuntu-14-04-tp467
>>> 4855p4681108.html
>>> Sent from the Squid - Users mailing list archive at Nabble.com.
>>> _______________________________________________
>>> squid-users mailing list
>>> squid-users at lists.squid-cache.org
>>> http://lists.squid-cache.org/listinfo/squid-users
>>>
>>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20170111/7c9cb882/attachment.htm>

From squid3 at treenet.co.nz  Wed Jan 11 14:42:27 2017
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 12 Jan 2017 03:42:27 +1300
Subject: [squid-users] squid-users Digest, Vol 29, Issue 21
In-Reply-To: <24abc21d-a56b-32dd-caae-77aa77754cf9@visolve.com>
References: <mailman.5856.1484131042.20516.squid-users@lists.squid-cache.org>
 <24abc21d-a56b-32dd-caae-77aa77754cf9@visolve.com>
Message-ID: <07ad859b-0dbf-c681-8905-eda3f3e7f0bf@treenet.co.nz>

On 12/01/2017 12:44 a.m., anand wrote:
> Hello Vinay,
> 
> Please verify the web content is "Static/Dynamic", if the content is
> Static, user requests will be logged TCP_HIT/TCP_REFRESH.
> 
> If the web content is "Dynamic", user requests will be refreshed from
> web server.

That is not an accurate description of current HTTP/1.1 traffic nor of
Squid behaviour.

The separation of static vs dynamic is irrelevant for the purposes of
caching and *does not* impact on the MISS situation.

REFRESH is a sign of the content being either dynamic, private or stale.
HIT is seen on both static and dynamic types of object. Neither is a
sign of being static and MISS can occur for many reasons unrelated to
data being either static or dynamic.

The only difference between them is that dynamic is HIT for shorter
times and REFRESH happens more often, compared with longer HIT period on
static content.

Amos



From vvjoshi5 at gmail.com  Wed Jan 11 14:55:16 2017
From: vvjoshi5 at gmail.com (Vidyadhish Joshi)
Date: Wed, 11 Jan 2017 20:25:16 +0530
Subject: [squid-users] squid-users Digest, Vol 29, Issue 21
In-Reply-To: <07ad859b-0dbf-c681-8905-eda3f3e7f0bf@treenet.co.nz>
References: <mailman.5856.1484131042.20516.squid-users@lists.squid-cache.org>
 <24abc21d-a56b-32dd-caae-77aa77754cf9@visolve.com>
 <07ad859b-0dbf-c681-8905-eda3f3e7f0bf@treenet.co.nz>
Message-ID: <CAMRD5gQrsmwe5zG_E5UwzUrP+epUzMH9UAFWdepASN-+dQZdwQ@mail.gmail.com>

Amos, thank you for the details.
Need pointers for caching the dynamic contents. My app has static n dynamic
cache n static am able to cache it . For dynamic the URL is getting
appended with query string.  Is there a way to cache dynamic contests n
what would be configuration changes to cache dynamic ones.
Thanks a lot.

On 11-Jan-2017 8:12 PM, "Amos Jeffries" <squid3 at treenet.co.nz> wrote:

> On 12/01/2017 12:44 a.m., anand wrote:
> > Hello Vinay,
> >
> > Please verify the web content is "Static/Dynamic", if the content is
> > Static, user requests will be logged TCP_HIT/TCP_REFRESH.
> >
> > If the web content is "Dynamic", user requests will be refreshed from
> > web server.
>
> That is not an accurate description of current HTTP/1.1 traffic nor of
> Squid behaviour.
>
> The separation of static vs dynamic is irrelevant for the purposes of
> caching and *does not* impact on the MISS situation.
>
> REFRESH is a sign of the content being either dynamic, private or stale.
> HIT is seen on both static and dynamic types of object. Neither is a
> sign of being static and MISS can occur for many reasons unrelated to
> data being either static or dynamic.
>
> The only difference between them is that dynamic is HIT for shorter
> times and REFRESH happens more often, compared with longer HIT period on
> static content.
>
> Amos
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20170111/2e3a43d4/attachment.htm>

From squid3 at treenet.co.nz  Wed Jan 11 15:52:38 2017
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 12 Jan 2017 04:52:38 +1300
Subject: [squid-users] squid-users Digest, Vol 29, Issue 21
In-Reply-To: <CAMRD5gQrsmwe5zG_E5UwzUrP+epUzMH9UAFWdepASN-+dQZdwQ@mail.gmail.com>
References: <mailman.5856.1484131042.20516.squid-users@lists.squid-cache.org>
 <24abc21d-a56b-32dd-caae-77aa77754cf9@visolve.com>
 <07ad859b-0dbf-c681-8905-eda3f3e7f0bf@treenet.co.nz>
 <CAMRD5gQrsmwe5zG_E5UwzUrP+epUzMH9UAFWdepASN-+dQZdwQ@mail.gmail.com>
Message-ID: <87eeb7e5-73e1-361a-ab87-8b702ba42e89@treenet.co.nz>

On 12/01/2017 3:55 a.m., Vidyadhish Joshi wrote:
> Amos, thank you for the details.
> Need pointers for caching the dynamic contents. My app has static n dynamic
> cache n static am able to cache it . For dynamic the URL is getting
> appended with query string.  Is there a way to cache dynamic contests n
> what would be configuration changes to cache dynamic ones.

Sure;

* Use the latest 3.5 version you can. There have been small but
important improvements across the whole series.


* Make sure you _do not_ have the old Squid-2 QUERY ACL denying storage
('cache deny QUERY' line in squid.conf) for those objects.


* Make sure your refresh_pattern lines _do not_ contain ignore-auth,
ignore-no-cache, ignore-must-revalidate, ignore-no-store or
override-lastmod.
 - you can add store-stale if you want to increase the caching further.


* Make sure you _do_ have these Squid-3 default refresh_patterns. The 0
value's are important to be 0. The other numbers you can change as wanted:

  refresh_pattern -i (/cgi-bin/|\?) 0     0%      0
  refresh_pattern .               0       20%     4320


* Make sure your server produces appropriate caching options indicating
how long the content is to be cached. Specifically either Expires or
Cache-Control:max-age=N indicating when it will next change, or
Cache-Control:must-revalidate to require constant REFRESH.
 see <https://tools.ietf.org/html/rfc7234> for more details


* Your server should also produce Last-Modified and/or ETag headers for
content it generates. And handle the If-* request headers on received
requests to produce 304 responses when the content is unchanged.
 see <https://tools.ietf.org/html/rfc7232> for more details

 - when the server properly handles these If-* headers you can add the
refresh_pattern option refresh-ims and/or reload-into-ims to further
increase caching. (Until the server properly revalidates these options
are useless.)


Amos



From stepan.bujnak at gmail.com  Wed Jan 11 15:56:09 2017
From: stepan.bujnak at gmail.com (Stepan Bujnak)
Date: Wed, 11 Jan 2017 07:56:09 -0800
Subject: [squid-users] ERR_CANNOT_FORWARD with Squid + Privoxy
In-Reply-To: <e18fd2e1-693e-4e26-57c9-7722e0d938b9@treenet.co.nz>
References: <CA+0ge-TCKk_Q1ceWTnUg6+wUUN7a6fLKhbjGECyTAPxcaKvLWQ@mail.gmail.com>
 <e18fd2e1-693e-4e26-57c9-7722e0d938b9@treenet.co.nz>
Message-ID: <CA+0ge-TOEkfO-tePJ0FbQksPRHj3w2BNvtfFzPFgKVR6gFr1pQ@mail.gmail.com>

Thank you very much for the reply!

On Wed, Jan 11, 2017 at 6:23 AM, Amos Jeffries <squid3 at treenet.co.nz> wrote:
> On 11/01/2017 2:26 p.m., Stepan Bujnak wrote:
>> Hi,
>>
>> I've been trying to configure intercepting proxy with privoxy as a
>> cache_peer. This is my Squid configuration:
>>
>> acl all src all
>>
>> acl SSL_ports  port 443
>> acl Safe_ports port 80          # http
>> acl Safe_ports port 21          # ftp
>> acl Safe_ports port 443         # https
>> acl Safe_ports port 70          # gopher
>> acl Safe_ports port 210         # wais
>> acl Safe_ports port 1025-65535  # unregistered ports
>> acl Safe_ports port 280         # http-mgmt
>> acl Safe_ports port 488         # gss-http
>> acl Safe_ports port 591         # filemaker
>> acl Safe_ports port 777         # multiling http
>> acl CONNECT    method CONNECT
>>
>> #http_access deny !Safe_ports
>> #http_access deny CONNECT !SSL_ports
>> http_access allow all
>>
>> # stop squid taking forever to restart.
>> shutdown_lifetime 3 second
>>
>> client_dst_passthru off
>> host_verify_strict off
>
> Please pay attention to the docs for these options. Specifically how it
> says host_verify_strict has no effect on intercepted traffic. Also how
> it says client_dst_passthru has no effect when the Host verify process
> detects an origin mismatch (eg 'fails').

I figured out the origin mismatch part. Unfortunately, this is very
important to me so I had to dig into the code turn the check off.

>
>
>>
>> # IMPORTANT! squid requires at least one forward-proxy port configured
>> #            http://wiki.squid-cache.org/KnowledgeBase/NoForwardProxyPorts
>> http_port 0.0.0.0:3127
>> http_port 0.0.0.0:3128 intercept
>> https_port 0.0.0.0:3129 intercept ssl-bump
>> generate-host-certificates=on dynamic_cert_mem_cache_size=4MB
>> cert=/etc/squid/ssl_certs/squid.pem
>>
>> sslcrtd_program /usr/lib/squid/ssl_crtd -s /var/lib/squid/ssl_db -M
>> 4MB sslcrtd_children 8 startup=1 idle=1
>> sslproxy_capath /etc/ssl/certs
>>
>> acl step1 at_step SslBump1
>> ssl_bump peek step1
>> ssl_bump bump all
>
> So what you have configured is Server-first bumping.
>
> All clients will be presented with the Privoxy SSL certificate as if
> Privoxy (at 127.0.0.1:8118) was the authoritative web server for the
> HTTPS website being fetched.
>
> "What could go wrong?" as the saying goes. A better question would be
> what could possibly go _right_ in that setup. Very few websites will
> work, and only where the TLS was completely broken in the first place.

Would better solution be client-first configuration where client would
be presented with squid's self generated certificate, read the traffic
and then send it to the actual destination through privoxy using
CONNECT? How could that be configured?

>
>
>>
>> cache_peer 127.0.0.1 parent 8118 7 no-query default no-digest
>> no-netdb-exchange proxy-only ssl
>> never_direct allow all
>>
>> cache_mem 8 MB
>> maximum_object_size_in_memory 32 KB
>>
>> # Disable the Via and X-Forwarded-For field from the request header to avoid
>> # leaking the use of a proxy and client ip address
>> via off
>> forwarded_for off
>
> The above injects "X-Forwarded-For: unknown" into the traffic. Squid has
> long ago moved past the point where that was the only choice.
>
> Use "forwarded_for transparent" instead for privacy. Then you can remove
> the following two lines as well...
>
>> follow_x_forwarded_for deny all
>> request_header_access X-Forwarded-For deny all
>>
>> #cache_dir ufs /var/spool/squid 1024 16 256
>> #coredump_dir /var/cache/squid
>> cache deny all
>>
>> refresh_pattern ^ftp:           1440    20%     10080
>> refresh_pattern ^gopher:        1440    0%      1440
>> refresh_pattern -i (/cgi-bin/|\?) 0     0%      0
>> refresh_pattern .               0       20%     4320
>>
>>
>> Now when making a request, privoxy prints out following:
>>
>> 2017-01-11 00:36:51.420 7fe4872a4700 Connect: Accepted connection from
>> 127.0.0.1 on socket 4
>> 2017-01-11 00:36:51.421 7fe4872a4700 Received: from socket 4:
>> \x16\x03\x01\x010\x01\x00\x01,\x03\x03xfOz\xc3\xc2\xf8\xf6\xc4\x972Y\xe5w\xf0\xd7\x98\xb5\xd3\x99\xfb\x97P%\x0aX\x1f\xefs\x91\xc6d\x00\x00\xaa\xc00\xc0,\xc0(\xc0$\xc0\x14\xc0\x0a\x00\xa5\x00\xa3\x00\xa1\x00\x9f\x00k\x00j\x00i\x00h\x009\x008\x007\x006\x00\x88\x00\x87\x00\x86\x00\x85\xc02\xc0.\xc0*\xc0&\xc0\x0f\xc0\x05\x00\x9d\x00=\x005\x00\x84\xc0/\xc0+\xc0'\xc0#\xc0\x13\xc0\x09\x00\xa4\x00\xa2\x00\xa0\x00\x9e\x00g\x00@\x00?\x00>\x003\x002\x001\x000\x00\x9a\x00\x99\x00\x98\x00\x97\x00E\x00D\x00C\x00B\xc01\xc0-\xc0)\xc0%\xc0\x0e\xc0\x04\x00\x9c\x00<\x00/\x00\x96\x00A\xc0\x11\xc0\x07\xc0\x0c\xc0\x02\x00\x05\x00\x04\xc0\x12\xc0\x08\x00\x16\x00\x13\x00\x10\x00\x0d\xc0\x0d\xc0\x03\x00\x0a\x00\xff\x01\x00\x00Y\x00\x0b\x00\x04\x03\x00\x01\x02\x00\x0a\x00\x1c\x00\x1a\x00\x17\x00\x19\x00\x1c\x00\x1b\x00\x18\x00\x1a\x00\x16\x00\x0e\x00\x0d\x00\x0b\x00\x0c\x00\x09\x00\x0a\x00#\x00\x00\x00\x0d\x00
>> \x00\x1e\x06\x01\x06\x02\x06\x03\x05\x01\x05\x02\x05\x03\x04\x01\x04\x02\x04\x03\x03\x01\x03\x02\x03\x03\x02\x01\x02\x02\x02\x03\x00\x0f\x00\x01\x013t\x00\x00
>> 2017-01-11 00:37:21.450 7fe4872a4700 Connect: The client side of the
>> connection on socket 4 got closed without sending a complete request
>> line.
>>
>>
>> It seems like the bumped request is missing the CONNECT line and
>> privoxy gets confused.
>
> There is no CONNECT.
>
> You configured Squid to SSL-Bump the HTTPS traffic. That means the
> CONNECT gets absorbed by Squid and the TLS protocol to the client gets
> terminated, and a new TLS connection is opened to the Privoxy to fetch
> the server certificate (or not as this case shows).
>
> The connection is native TLS between Squid and privoxy. As configured by
> the 'ssl' option on cache_peer. Since Privoxy cannot handle TLS that
> fails, and you get the forwarding error.
>
> And before you ask, no Squid-3 will not send SSL-Bump'ed traffic through
> a peer without that 'ssl' option. Squid-4 has some improvements for
> non-HTTPS traffic, but still not to the point of Squid generating peer
> CONNECT messages after bumping HTTPS.
>
>
>>
>> As a result, the client receives ERR_CANNOT_FORWARD. Could someone
>> point me to the right direction? Thank you.
>
> Your best hope is to recreate in squid.conf settings the privacy
> operations you are using privoxy for. Then remove privoxy from the chain
> of proxies.

I thought about this solution, but it seems that squid cannot use
socks parent yet.

>
> Amos
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users


From squid3 at treenet.co.nz  Wed Jan 11 16:18:58 2017
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 12 Jan 2017 05:18:58 +1300
Subject: [squid-users] ERR_CANNOT_FORWARD with Squid + Privoxy
In-Reply-To: <CA+0ge-TOEkfO-tePJ0FbQksPRHj3w2BNvtfFzPFgKVR6gFr1pQ@mail.gmail.com>
References: <CA+0ge-TCKk_Q1ceWTnUg6+wUUN7a6fLKhbjGECyTAPxcaKvLWQ@mail.gmail.com>
 <e18fd2e1-693e-4e26-57c9-7722e0d938b9@treenet.co.nz>
 <CA+0ge-TOEkfO-tePJ0FbQksPRHj3w2BNvtfFzPFgKVR6gFr1pQ@mail.gmail.com>
Message-ID: <0e92ed25-a96f-9637-d955-cb65039538d0@treenet.co.nz>

On 12/01/2017 4:56 a.m., Stepan Bujnak wrote:
> Thank you very much for the reply!
> 
> On Wed, Jan 11, 2017 at 6:23 AM, Amos Jeffries <squid3 at treenet.co.nz> wrote:
>> On 11/01/2017 2:26 p.m., Stepan Bujnak wrote:
>>> Hi,
>>>
>>> I've been trying to configure intercepting proxy with privoxy as a
>>> cache_peer. This is my Squid configuration:
>>>
>>> acl all src all
>>>
>>> acl SSL_ports  port 443
>>> acl Safe_ports port 80          # http
>>> acl Safe_ports port 21          # ftp
>>> acl Safe_ports port 443         # https
>>> acl Safe_ports port 70          # gopher
>>> acl Safe_ports port 210         # wais
>>> acl Safe_ports port 1025-65535  # unregistered ports
>>> acl Safe_ports port 280         # http-mgmt
>>> acl Safe_ports port 488         # gss-http
>>> acl Safe_ports port 591         # filemaker
>>> acl Safe_ports port 777         # multiling http
>>> acl CONNECT    method CONNECT
>>>
>>> #http_access deny !Safe_ports
>>> #http_access deny CONNECT !SSL_ports
>>> http_access allow all
>>>
>>> # stop squid taking forever to restart.
>>> shutdown_lifetime 3 second
>>>
>>> client_dst_passthru off
>>> host_verify_strict off
>>
>> Please pay attention to the docs for these options. Specifically how it
>> says host_verify_strict has no effect on intercepted traffic. Also how
>> it says client_dst_passthru has no effect when the Host verify process
>> detects an origin mismatch (eg 'fails').
> 
> I figured out the origin mismatch part. Unfortunately, this is very
> important to me so I had to dig into the code turn the check off.
> 

Sigh. Please don't. That makes a short web script (eg a web advert) able
to hijack your proxy and use it as a base to hijack your whole network -
hiding the attacker while doing so.

There are details and some mitigations to reduce the pain listed at
<http://wiki.squid-cache.org/KnowledgeBase/HostHeaderForgery>


>>>
>>> # IMPORTANT! squid requires at least one forward-proxy port configured
>>> #            http://wiki.squid-cache.org/KnowledgeBase/NoForwardProxyPorts
>>> http_port 0.0.0.0:3127
>>> http_port 0.0.0.0:3128 intercept
>>> https_port 0.0.0.0:3129 intercept ssl-bump
>>> generate-host-certificates=on dynamic_cert_mem_cache_size=4MB
>>> cert=/etc/squid/ssl_certs/squid.pem
>>>
>>> sslcrtd_program /usr/lib/squid/ssl_crtd -s /var/lib/squid/ssl_db -M
>>> 4MB sslcrtd_children 8 startup=1 idle=1
>>> sslproxy_capath /etc/ssl/certs
>>>
>>> acl step1 at_step SslBump1
>>> ssl_bump peek step1
>>> ssl_bump bump all
>>
>> So what you have configured is Server-first bumping.
>>
>> All clients will be presented with the Privoxy SSL certificate as if
>> Privoxy (at 127.0.0.1:8118) was the authoritative web server for the
>> HTTPS website being fetched.
>>
>> "What could go wrong?" as the saying goes. A better question would be
>> what could possibly go _right_ in that setup. Very few websites will
>> work, and only where the TLS was completely broken in the first place.
> 
> Would better solution be client-first configuration where client would
> be presented with squid's self generated certificate, read the traffic
> and then send it to the actual destination through privoxy

That can be done by making the ssl_bump directive decide to bump on
step1. It still has all the same problems as the existing config, the
issues all center around the fact that the client is not presented with
anything that looks even remotely like the real servers certificate or
TLS options. So any TLS/SSL security the client may be using is useless.

> using CONNECT?

No Squid is currently able to do that. Once decrypted the traffic has to
go to an HTTPS server directly (ORIGINAL_DST) or to a proxy which is
connected to using a secure channel, ie. TLS/SSL.


>>
>>>
>>> As a result, the client receives ERR_CANNOT_FORWARD. Could someone
>>> point me to the right direction? Thank you.
>>
>> Your best hope is to recreate in squid.conf settings the privacy
>> operations you are using privoxy for. Then remove privoxy from the chain
>> of proxies.
> 
> I thought about this solution, but it seems that squid cannot use
> socks parent yet.

You should be able to send Squids outbound traffic through a regular
SOCKS tunnel/gateway if you need to. It is just configured at the OS
routing level rather than anything in squid.conf.

Amos


From alex.tate at gmail.com  Wed Jan 11 16:32:49 2017
From: alex.tate at gmail.com (roadrage27)
Date: Wed, 11 Jan 2017 08:32:49 -0800 (PST)
Subject: [squid-users] TCP 403 Denied on new squid build out
Message-ID: <1484152369267-4681127.post@n4.nabble.com>

Built out Squid 3.5 on ubuntu 14.04  logs showing 403 denied when accessing
any resources, any help is appreciated

here is my conf file for reference.


acl localhost src 127.0.0.1/32

acl to_localhost dst 127.0.0.0/8

acl localnet src 0.0.0.0/8 10.145.68.0/24

acl myip src 10.145.68.148/32

acl to_localnet dst 10.145.68.0/24

acl search_engines dstdomain .yahoo.com .google.com

acl SSL_ports port 443

acl Safe_ports port 80          # http

acl Safe_ports port 21          # ftp

acl Safe_ports port 443         # https

acl Safe_ports port 70          # gopher

acl Safe_ports port 210         # wais

acl Safe_ports port 1025-65535  # unregistered ports

acl Safe_ports port 280         # http-mgmt

acl Safe_ports port 488         # gss-http

acl Safe_ports port 591         # filemaker

acl Safe_ports port 777         # multiling http

 

acl CONNECT method CONNECT

never_direct allow all

http_access allow search_engines

http_access allow manager localhost

http_access deny manager

http_access deny !Safe_ports

http_access allow localnet

http_access allow to_localnet

http_access allow myip

http_access allow all

http_access deny to_localhost

icp_access deny all

http_access deny all

 

http_port 3128

hierarchy_stoplist cgi-bin ?

access_log /var/log/squid3/access.log squid

 

 

#Suggested default:

refresh_pattern ^ftp:           1440    20%     10080

refresh_pattern ^gopher:        1440    0%      1440

refresh_pattern -i (/cgi-bin/|\?) 0 0% 0

refresh_pattern .               0       20%     4320

# Leave coredumps in the first cache dir

coredump_dir /var/spool/squid3



--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/TCP-403-Denied-on-new-squid-build-out-tp4681127.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From vvjoshi5 at gmail.com  Wed Jan 11 16:56:19 2017
From: vvjoshi5 at gmail.com (Vidyadhish Joshi)
Date: Wed, 11 Jan 2017 22:26:19 +0530
Subject: [squid-users] squid-users Digest, Vol 29, Issue 21
In-Reply-To: <87eeb7e5-73e1-361a-ab87-8b702ba42e89@treenet.co.nz>
References: <mailman.5856.1484131042.20516.squid-users@lists.squid-cache.org>
 <24abc21d-a56b-32dd-caae-77aa77754cf9@visolve.com>
 <07ad859b-0dbf-c681-8905-eda3f3e7f0bf@treenet.co.nz>
 <CAMRD5gQrsmwe5zG_E5UwzUrP+epUzMH9UAFWdepASN-+dQZdwQ@mail.gmail.com>
 <87eeb7e5-73e1-361a-ab87-8b702ba42e89@treenet.co.nz>
Message-ID: <CAMRD5gR=NyYyNNg07pJp1zsc9UCnFSN6REMGO9CSrxkcdreaPg@mail.gmail.com>

Thank s a lot , Amos

I will try these things

On 11-Jan-2017 9:22 PM, "Amos Jeffries" <squid3 at treenet.co.nz> wrote:

> On 12/01/2017 3:55 a.m., Vidyadhish Joshi wrote:
> > Amos, thank you for the details.
> > Need pointers for caching the dynamic contents. My app has static n
> dynamic
> > cache n static am able to cache it . For dynamic the URL is getting
> > appended with query string.  Is there a way to cache dynamic contests n
> > what would be configuration changes to cache dynamic ones.
>
> Sure;
>
> * Use the latest 3.5 version you can. There have been small but
> important improvements across the whole series.
>
>
> * Make sure you _do not_ have the old Squid-2 QUERY ACL denying storage
> ('cache deny QUERY' line in squid.conf) for those objects.
>
>
> * Make sure your refresh_pattern lines _do not_ contain ignore-auth,
> ignore-no-cache, ignore-must-revalidate, ignore-no-store or
> override-lastmod.
>  - you can add store-stale if you want to increase the caching further.
>
>
> * Make sure you _do_ have these Squid-3 default refresh_patterns. The 0
> value's are important to be 0. The other numbers you can change as wanted:
>
>   refresh_pattern -i (/cgi-bin/|\?) 0     0%      0
>   refresh_pattern .               0       20%     4320
>
>
> * Make sure your server produces appropriate caching options indicating
> how long the content is to be cached. Specifically either Expires or
> Cache-Control:max-age=N indicating when it will next change, or
> Cache-Control:must-revalidate to require constant REFRESH.
>  see <https://tools.ietf.org/html/rfc7234> for more details
>
>
> * Your server should also produce Last-Modified and/or ETag headers for
> content it generates. And handle the If-* request headers on received
> requests to produce 304 responses when the content is unchanged.
>  see <https://tools.ietf.org/html/rfc7232> for more details
>
>  - when the server properly handles these If-* headers you can add the
> refresh_pattern option refresh-ims and/or reload-into-ims to further
> increase caching. (Until the server properly revalidates these options
> are useless.)
>
>
> Amos
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20170111/d6932171/attachment.htm>

From uhlar at fantomas.sk  Wed Jan 11 19:33:13 2017
From: uhlar at fantomas.sk (Matus UHLAR - fantomas)
Date: Wed, 11 Jan 2017 20:33:13 +0100
Subject: [squid-users] TCP 403 Denied on new squid build out
In-Reply-To: <1484152369267-4681127.post@n4.nabble.com>
References: <1484152369267-4681127.post@n4.nabble.com>
Message-ID: <20170111193313.GA6491@fantomas.sk>

On 11.01.17 08:32, roadrage27 wrote:
>Built out Squid 3.5 on ubuntu 14.04  logs showing 403 denied when accessing
>any resources, any help is appreciated

please show us at least one line from logs...

>here is my conf file for reference.

no need to put empty line between all lines ...
-- 
Matus UHLAR - fantomas, uhlar at fantomas.sk ; http://www.fantomas.sk/
Warning: I wish NOT to receive e-mail advertising to this address.
Varovanie: na tuto adresu chcem NEDOSTAVAT akukolvek reklamnu postu.
Depression is merely anger without enthusiasm. 


From alex.tate at gmail.com  Wed Jan 11 19:52:25 2017
From: alex.tate at gmail.com (roadrage27)
Date: Wed, 11 Jan 2017 11:52:25 -0800 (PST)
Subject: [squid-users] TCP 403 Denied on new squid build out
In-Reply-To: <20170111193313.GA6491@fantomas.sk>
References: <1484152369267-4681127.post@n4.nabble.com>
 <20170111193313.GA6491@fantomas.sk>
Message-ID: <1484164345387-4681130.post@n4.nabble.com>

1484147626.926      3 10.145.68.148 TCP_DENIED/403 3780 GET
http://google.com/ - HIER_NONE/- text/html
1484147626.943      0 10.145.68.148 TCP_DENIED/403 3828 GET
http://www.squid-cache.org/Artwork/SN.png - HIER_NONE/- text/html



--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/TCP-403-Denied-on-new-squid-build-out-tp4681127p4681130.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From uhlar at fantomas.sk  Wed Jan 11 20:55:14 2017
From: uhlar at fantomas.sk (Matus UHLAR - fantomas)
Date: Wed, 11 Jan 2017 21:55:14 +0100
Subject: [squid-users] TCP 403 Denied on new squid build out
In-Reply-To: <1484152369267-4681127.post@n4.nabble.com>
References: <1484152369267-4681127.post@n4.nabble.com>
Message-ID: <20170111205514.GA8844@fantomas.sk>

On 11.01.17 08:32, roadrage27 wrote:
>never_direct allow all

I have missed this one.
what are you parent proxies and do you use any?

Because by "never_direct allow all" you have configured your squid to always
use parent proxy and never go direct.

You should avoid always_direct and never_direct directives if you do not use
parent or sibling proxies.
-- 
Matus UHLAR - fantomas, uhlar at fantomas.sk ; http://www.fantomas.sk/
Warning: I wish NOT to receive e-mail advertising to this address.
Varovanie: na tuto adresu chcem NEDOSTAVAT akukolvek reklamnu postu.
The early bird may get the worm, but the second mouse gets the cheese. 


From alex.tate at gmail.com  Wed Jan 11 21:09:30 2017
From: alex.tate at gmail.com (roadrage27)
Date: Wed, 11 Jan 2017 13:09:30 -0800 (PST)
Subject: [squid-users] TCP 403 Denied on new squid build out
In-Reply-To: <20170111205514.GA8844@fantomas.sk>
References: <1484152369267-4681127.post@n4.nabble.com>
 <20170111205514.GA8844@fantomas.sk>
Message-ID: <CAFwozXJ7=y_ARAAEJdd+kNWMcpDnLtU-B8zj26T3ZF137Twdbw@mail.gmail.com>

I removed the entry as i do not use those other proxies, parsed the config
file, then restarted squid, still the same error.

On Wed, Jan 11, 2017 at 2:55 PM Matus UHLAR - fantomas [via Squid Web Proxy
Cache] <ml-node+s1019090n4681131h16 at n4.nabble.com> wrote:

> On 11.01.17 08:32, roadrage27 wrote:
> >never_direct allow all
>
> I have missed this one.
> what are you parent proxies and do you use any?
>
> Because by "never_direct allow all" you have configured your squid to
> always
> use parent proxy and never go direct.
>
> You should avoid always_direct and never_direct directives if you do not
> use
> parent or sibling proxies.
> --
> Matus UHLAR - fantomas, [hidden email]
> <http:///user/SendEmail.jtp?type=node&node=4681131&i=0> ;
> http://www.fantomas.sk/
> Warning: I wish NOT to receive e-mail advertising to this address.
> Varovanie: na tuto adresu chcem NEDOSTAVAT akukolvek reklamnu postu.
> The early bird may get the worm, but the second mouse gets the cheese.
> _______________________________________________
> squid-users mailing list
> [hidden email] <http:///user/SendEmail.jtp?type=node&node=4681131&i=1>
> http://lists.squid-cache.org/listinfo/squid-users
>
>
> If you reply to this email, your message will be added to the discussion
> below:
>
> http://squid-web-proxy-cache.1019090.n4.nabble.com/TCP-403-Denied-on-new-squid-build-out-tp4681127p4681131.html
> To unsubscribe from TCP 403 Denied on new squid build out, click here
> <http://squid-web-proxy-cache.1019090.n4.nabble.com/template/NamlServlet.jtp?macro=unsubscribe_by_code&node=4681127&code=YWxleC50YXRlQGdtYWlsLmNvbXw0NjgxMTI3fDIwMjU4MDQxMw==>
> .
> NAML
> <http://squid-web-proxy-cache.1019090.n4.nabble.com/template/NamlServlet.jtp?macro=macro_viewer&id=instant_html%21nabble%3Aemail.naml&base=nabble.naml.namespaces.BasicNamespace-nabble.view.web.template.NabbleNamespace-nabble.view.web.template.NodeNamespace&breadcrumbs=notify_subscribers%21nabble%3Aemail.naml-instant_emails%21nabble%3Aemail.naml-send_instant_email%21nabble%3Aemail.naml>
>




--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/TCP-403-Denied-on-new-squid-build-out-tp4681127p4681132.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From jason_haar at trimble.com  Thu Jan 12 00:39:28 2017
From: jason_haar at trimble.com (Jason Haar)
Date: Thu, 12 Jan 2017 13:39:28 +1300
Subject: [squid-users] Transparent Proxy in AWS
In-Reply-To: <1480613233935-4680712.post@n4.nabble.com>
References: <1480368783954-4680691.post@n4.nabble.com>
 <583CF4B7.3030107@treenet.co.nz>
 <1480613233935-4680712.post@n4.nabble.com>
Message-ID: <CAFChrgL+anWFYKtP4ArVokuL8REcyyTv9S4nVyUfK7Yy9JschQ@mail.gmail.com>

On Fri, Dec 2, 2016 at 6:27 AM, klops <lo.kenneth at gmail.com> wrote:

> Does this mean the squid box has to be the overall gateway for the internal
> network for transparrancy to work?
>
> The reason the proposed setup the way it is is because AWS VPC  service has
> a service based NAT gateway which we have not low level control over and it
> is the default gateway. We want to only route http/https traffic over to
> squid and the rest via their NAT gateway
>

Couldn't you configure those VPC networks so that the AWS default route is
dead by blocking all outbound (ie of no useable value to the EC2 hosts) and
tell the EC2 hosts owners to change their boot scripts to delete the
default gateway and replace it with your squid router? (which does have
Internet access). That way you are "regaining control" of your network, and
EC2 owners are "motivated" to Do The Right Thing :-)

Then there'd be no need for iptable tricks on the clients. Also means you
could apply this to Windows EC2 systems too

I'm not an AWS guru so I have no idea if that works. I'm assuming a VPC is
like a VLAN

-- 
Cheers

Jason Haar
Information Security Manager, Trimble Navigation Ltd.
Phone: +1 408 481 8171
PGP Fingerprint: 7A2E 0407 C9A6 CAF6 2B9F 8422 C063 5EBB FE1D 66D1
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20170112/2a2f0bb1/attachment.htm>

From eliezer at ngtech.co.il  Thu Jan 12 01:03:35 2017
From: eliezer at ngtech.co.il (Eliezer  Croitoru)
Date: Thu, 12 Jan 2017 03:03:35 +0200
Subject: [squid-users] TCP 403 Denied on new squid build out
In-Reply-To: <1484152369267-4681127.post@n4.nabble.com>
References: <1484152369267-4681127.post@n4.nabble.com>
Message-ID: <045501d26c6f$b3cf1b70$1b6d5250$@ngtech.co.il>

Try the next:
Remove:
never_direct allow all

And add at the first line of the file
http_access allow all

And see if it let you surf.
If so then something is twisted in the config file order.

Eliezer

----
Eliezer Croitoru
Linux System Administrator
Mobile: +972-5-28704261
Email: eliezer at ngtech.co.il


-----Original Message-----
From: squid-users [mailto:squid-users-bounces at lists.squid-cache.org] On Behalf Of roadrage27
Sent: Wednesday, January 11, 2017 6:33 PM
To: squid-users at lists.squid-cache.org
Subject: [squid-users] TCP 403 Denied on new squid build out

Built out Squid 3.5 on ubuntu 14.04  logs showing 403 denied when accessing
any resources, any help is appreciated

here is my conf file for reference.


acl localhost src 127.0.0.1/32

acl to_localhost dst 127.0.0.0/8

acl localnet src 0.0.0.0/8 10.145.68.0/24

acl myip src 10.145.68.148/32

acl to_localnet dst 10.145.68.0/24

acl search_engines dstdomain .yahoo.com .google.com

acl SSL_ports port 443

acl Safe_ports port 80          # http

acl Safe_ports port 21          # ftp

acl Safe_ports port 443         # https

acl Safe_ports port 70          # gopher

acl Safe_ports port 210         # wais

acl Safe_ports port 1025-65535  # unregistered ports

acl Safe_ports port 280         # http-mgmt

acl Safe_ports port 488         # gss-http

acl Safe_ports port 591         # filemaker

acl Safe_ports port 777         # multiling http

 

acl CONNECT method CONNECT

never_direct allow all

http_access allow search_engines

http_access allow manager localhost

http_access deny manager

http_access deny !Safe_ports

http_access allow localnet

http_access allow to_localnet

http_access allow myip

http_access allow all

http_access deny to_localhost

icp_access deny all

http_access deny all

 

http_port 3128

hierarchy_stoplist cgi-bin ?

access_log /var/log/squid3/access.log squid

 

 

#Suggested default:

refresh_pattern ^ftp:           1440    20%     10080

refresh_pattern ^gopher:        1440    0%      1440

refresh_pattern -i (/cgi-bin/|\?) 0 0% 0

refresh_pattern .               0       20%     4320

# Leave coredumps in the first cache dir

coredump_dir /var/spool/squid3



--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/TCP-403-Denied-on-new-squid-build-out-tp4681127.html
Sent from the Squid - Users mailing list archive at Nabble.com.
_______________________________________________
squid-users mailing list
squid-users at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-users



From squid3 at treenet.co.nz  Thu Jan 12 04:06:09 2017
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 12 Jan 2017 17:06:09 +1300
Subject: [squid-users] TCP 403 Denied on new squid build out
In-Reply-To: <1484152369267-4681127.post@n4.nabble.com>
References: <1484152369267-4681127.post@n4.nabble.com>
Message-ID: <866df52c-89ef-ade6-36c1-83fd79a3d348@treenet.co.nz>

On 12/01/2017 5:32 a.m., roadrage27 wrote:
> Built out Squid 3.5 on ubuntu 14.04  logs showing 403 denied when accessing
> any resources, any help is appreciated
> 
> here is my conf file for reference.
> 
> 
> acl localhost src 127.0.0.1/32
> 
> acl to_localhost dst 127.0.0.0/8
> 

Remove the above two lines, they are built-in ACLs.

Please run 'squid -k parse' it will tell you about these things and
maybe more.


> acl localnet src 0.0.0.0/8 10.145.68.0/24
> 
> acl myip src 10.145.68.148/32
> 
> acl to_localnet dst 10.145.68.0/24
> 
> acl search_engines dstdomain .yahoo.com .google.com
> 
> acl SSL_ports port 443
> 
> acl Safe_ports port 80          # http
> 
> acl Safe_ports port 21          # ftp
> 
> acl Safe_ports port 443         # https
> 
> acl Safe_ports port 70          # gopher
> 
> acl Safe_ports port 210         # wais
> 
> acl Safe_ports port 1025-65535  # unregistered ports
> 
> acl Safe_ports port 280         # http-mgmt
> 
> acl Safe_ports port 488         # gss-http
> 
> acl Safe_ports port 591         # filemaker
> 
> acl Safe_ports port 777         # multiling http
> 
>  
> 
> acl CONNECT method CONNECT
> 
> never_direct allow all
> 

As others mentioned, remove the above line - it is preventing Squid
contacting any web server.

> http_access allow search_engines
> 
> http_access allow manager localhost
> 
> http_access deny manager
> 
> http_access deny !Safe_ports
> 
> http_access allow localnet
> 
> http_access allow to_localnet
> 
> http_access allow myip
> 
> http_access allow all
> 
> http_access deny to_localhost
> 
> icp_access deny all

You can remove the above line, you dont have ICP ports open in this proxy.


> 
> http_access deny all
> 

Your http_access lines should look like this:

 # default security checks
 http_access deny !Safe_ports
 http_access deny CONNECT !SSL_ports
 http_access allow manager localhost
 http_access deny manager

 # local network policy
 http_access allow localnet

 # default action for unidentified traffic
 http_access deny all


NOTE 1: the 'myip' ACL is not used. That is because the 'allow locanet'
already accepts the 'allow myip' traffic.

NOTE 2: the search_engines ACL is dropped. It was being used to allow
anyone anywhere on the Intenret to use your proxy to access those
domains. Which is very bad for a forward proxy to do.
 - Also, the 'allow localnet' line already allows any LAN machines to
access those domains without having to name them.

NOTE 3: the to_localnet ACL is removed because it makes your proxy an
open-proxy. Anyone on the Internet who can reach your proxy can attack
your network.
 - If you are tring to setup a CDN proxy / reverse-proxy then this is
absolutely the worst way to do it.



>  
> 
> http_port 3128
> 
> hierarchy_stoplist cgi-bin ?
> 

Also remove the above line. It is no longer good.


> access_log /var/log/squid3/access.log squid
> 
>  
> 
>  
> 
> #Suggested default:
> 
> refresh_pattern ^ftp:           1440    20%     10080
> 
> refresh_pattern ^gopher:        1440    0%      1440
> 
> refresh_pattern -i (/cgi-bin/|\?) 0 0% 0
> 
> refresh_pattern .               0       20%     4320
> 
> # Leave coredumps in the first cache dir
> 
> coredump_dir /var/spool/squid3
> 


Amos



From squid3 at treenet.co.nz  Thu Jan 12 04:07:55 2017
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 12 Jan 2017 17:07:55 +1300
Subject: [squid-users] TCP 403 Denied on new squid build out
In-Reply-To: <CAFwozXJ7=y_ARAAEJdd+kNWMcpDnLtU-B8zj26T3ZF137Twdbw@mail.gmail.com>
References: <1484152369267-4681127.post@n4.nabble.com>
 <20170111205514.GA8844@fantomas.sk>
 <CAFwozXJ7=y_ARAAEJdd+kNWMcpDnLtU-B8zj26T3ZF137Twdbw@mail.gmail.com>
Message-ID: <ecc10a1a-8ec1-9dd0-e95d-c3e694b9187f@treenet.co.nz>

On 12/01/2017 10:09 a.m., roadrage27 wrote:
> I removed the entry as i do not use those other proxies, parsed the config
> file, then restarted squid, still the same error.
> 

Are there any entries in cache.log explaining a problem?

What does the text of that 403 page say is wrong?

Amos



From anand at visolve.com  Thu Jan 12 05:22:05 2017
From: anand at visolve.com (anand)
Date: Thu, 12 Jan 2017 10:52:05 +0530
Subject: [squid-users] squid-users Digest, Vol 29, Issue 26
In-Reply-To: <mailman.5908.1484181598.20516.squid-users@lists.squid-cache.org>
References: <mailman.5908.1484181598.20516.squid-users@lists.squid-cache.org>
Message-ID: <07751b19-0cdb-1432-7745-b5d9340d7dae@visolve.com>

Hello Alex,

Please confirm the proxy is configured for parent/sibling proxy ?, if 
not please remove following lines from the squid conf.

"never_direct allow all"

"icp_access deny all"

If the issue raises again, kindly share your updated/current squid conf files in mailing list.

Thanks,
Anand P

On 1/12/2017 6:09 AM, squid-users-request at lists.squid-cache.org wrote:
> Send squid-users mailing list submissions to
> 	squid-users at lists.squid-cache.org
>
> To subscribe or unsubscribe via the World Wide Web, visit
> 	http://lists.squid-cache.org/listinfo/squid-users
> or, via email, send a message with subject or body 'help' to
> 	squid-users-request at lists.squid-cache.org
>
> You can reach the person managing the list at
> 	squid-users-owner at lists.squid-cache.org
>
> When replying, please edit your Subject line so it is more specific
> than "Re: Contents of squid-users digest..."
>
>
> Today's Topics:
>
>     1. TCP 403 Denied on new squid build out (roadrage27)
>     2. Re: squid-users Digest, Vol 29, Issue 21 (Vidyadhish Joshi)
>     3. Re: TCP 403 Denied on new squid build out (Matus UHLAR - fantomas)
>     4. Re: TCP 403 Denied on new squid build out (roadrage27)
>     5. Re: TCP 403 Denied on new squid build out (Matus UHLAR - fantomas)
>     6. Re: TCP 403 Denied on new squid build out (roadrage27)
>     7. Re: Transparent Proxy in AWS (Jason Haar)
>
>
> ----------------------------------------------------------------------
>
> Message: 1
> Date: Wed, 11 Jan 2017 08:32:49 -0800 (PST)
> From: roadrage27 <alex.tate at gmail.com>
> To: squid-users at lists.squid-cache.org
> Subject: [squid-users] TCP 403 Denied on new squid build out
> Message-ID: <1484152369267-4681127.post at n4.nabble.com>
> Content-Type: text/plain; charset=us-ascii
>
> Built out Squid 3.5 on ubuntu 14.04  logs showing 403 denied when accessing
> any resources, any help is appreciated
>
> here is my conf file for reference.
>
>
> acl localhost src 127.0.0.1/32
>
> acl to_localhost dst 127.0.0.0/8
>
> acl localnet src 0.0.0.0/8 10.145.68.0/24
>
> acl myip src 10.145.68.148/32
>
> acl to_localnet dst 10.145.68.0/24
>
> acl search_engines dstdomain .yahoo.com .google.com
>
> acl SSL_ports port 443
>
> acl Safe_ports port 80          # http
>
> acl Safe_ports port 21          # ftp
>
> acl Safe_ports port 443         # https
>
> acl Safe_ports port 70          # gopher
>
> acl Safe_ports port 210         # wais
>
> acl Safe_ports port 1025-65535  # unregistered ports
>
> acl Safe_ports port 280         # http-mgmt
>
> acl Safe_ports port 488         # gss-http
>
> acl Safe_ports port 591         # filemaker
>
> acl Safe_ports port 777         # multiling http
>
>   
>
> acl CONNECT method CONNECT
>
> never_direct allow all
>
> http_access allow search_engines
>
> http_access allow manager localhost
>
> http_access deny manager
>
> http_access deny !Safe_ports
>
> http_access allow localnet
>
> http_access allow to_localnet
>
> http_access allow myip
>
> http_access allow all
>
> http_access deny to_localhost
>
> icp_access deny all
>
> http_access deny all
>
>   
>
> http_port 3128
>
> hierarchy_stoplist cgi-bin ?
>
> access_log /var/log/squid3/access.log squid
>
>   
>
>   
>
> #Suggested default:
>
> refresh_pattern ^ftp:           1440    20%     10080
>
> refresh_pattern ^gopher:        1440    0%      1440
>
> refresh_pattern -i (/cgi-bin/|\?) 0 0% 0
>
> refresh_pattern .               0       20%     4320
>
> # Leave coredumps in the first cache dir
>
> coredump_dir /var/spool/squid3
>
>
>
> --
> View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/TCP-403-Denied-on-new-squid-build-out-tp4681127.html
> Sent from the Squid - Users mailing list archive at Nabble.com.
>
>
> ------------------------------
>
> Message: 2
> Date: Wed, 11 Jan 2017 22:26:19 +0530
> From: Vidyadhish Joshi <vvjoshi5 at gmail.com>
> To: Amos Jeffries <squid3 at treenet.co.nz>
> Cc: squid-users at lists.squid-cache.org
> Subject: Re: [squid-users] squid-users Digest, Vol 29, Issue 21
> Message-ID:
> 	<CAMRD5gR=NyYyNNg07pJp1zsc9UCnFSN6REMGO9CSrxkcdreaPg at mail.gmail.com>
> Content-Type: text/plain; charset="utf-8"
>
> Thank s a lot , Amos
>
> I will try these things
>
> On 11-Jan-2017 9:22 PM, "Amos Jeffries" <squid3 at treenet.co.nz> wrote:
>
>> On 12/01/2017 3:55 a.m., Vidyadhish Joshi wrote:
>>> Amos, thank you for the details.
>>> Need pointers for caching the dynamic contents. My app has static n
>> dynamic
>>> cache n static am able to cache it . For dynamic the URL is getting
>>> appended with query string.  Is there a way to cache dynamic contests n
>>> what would be configuration changes to cache dynamic ones.
>> Sure;
>>
>> * Use the latest 3.5 version you can. There have been small but
>> important improvements across the whole series.
>>
>>
>> * Make sure you _do not_ have the old Squid-2 QUERY ACL denying storage
>> ('cache deny QUERY' line in squid.conf) for those objects.
>>
>>
>> * Make sure your refresh_pattern lines _do not_ contain ignore-auth,
>> ignore-no-cache, ignore-must-revalidate, ignore-no-store or
>> override-lastmod.
>>   - you can add store-stale if you want to increase the caching further.
>>
>>
>> * Make sure you _do_ have these Squid-3 default refresh_patterns. The 0
>> value's are important to be 0. The other numbers you can change as wanted:
>>
>>    refresh_pattern -i (/cgi-bin/|\?) 0     0%      0
>>    refresh_pattern .               0       20%     4320
>>
>>
>> * Make sure your server produces appropriate caching options indicating
>> how long the content is to be cached. Specifically either Expires or
>> Cache-Control:max-age=N indicating when it will next change, or
>> Cache-Control:must-revalidate to require constant REFRESH.
>>   see <https://tools.ietf.org/html/rfc7234> for more details
>>
>>
>> * Your server should also produce Last-Modified and/or ETag headers for
>> content it generates. And handle the If-* request headers on received
>> requests to produce 304 responses when the content is unchanged.
>>   see <https://tools.ietf.org/html/rfc7232> for more details
>>
>>   - when the server properly handles these If-* headers you can add the
>> refresh_pattern option refresh-ims and/or reload-into-ims to further
>> increase caching. (Until the server properly revalidates these options
>> are useless.)
>>
>>
>> Amos
>>
>>
> -------------- next part --------------
> An HTML attachment was scrubbed...
> URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20170111/d6932171/attachment-0001.html>
>
> ------------------------------
>
> Message: 3
> Date: Wed, 11 Jan 2017 20:33:13 +0100
> From: Matus UHLAR - fantomas <uhlar at fantomas.sk>
> To: squid-users at lists.squid-cache.org
> Subject: Re: [squid-users] TCP 403 Denied on new squid build out
> Message-ID: <20170111193313.GA6491 at fantomas.sk>
> Content-Type: text/plain; charset=us-ascii; format=flowed
>
> On 11.01.17 08:32, roadrage27 wrote:
>> Built out Squid 3.5 on ubuntu 14.04  logs showing 403 denied when accessing
>> any resources, any help is appreciated
> please show us at least one line from logs...
>
>> here is my conf file for reference.
> no need to put empty line between all lines ...



From vbvbrj at gmail.com  Thu Jan 12 07:43:40 2017
From: vbvbrj at gmail.com (Mimiko)
Date: Thu, 12 Jan 2017 09:43:40 +0200
Subject: [squid-users] logformat %tl and %tg are same.
Message-ID: <186d6a61-9a7a-dbe7-31b1-ff407c18cf05@gmail.com>

Hello.

I use Squid Cache: Version 3.1.20

In order to view the log in a readable format I use this:

logformat squid %{%Y/%m/%d %H:%M:%S}tl %ts.%03tu %6tr %>a %Ss/%03>Hs 
%<st %rm %ru %un %Sh/%<A %mt

The problem is that the time in log is in GMT despite of %tl option. 
Server is configured to local time. Squid for both, %tl and %tg, shows 
same time.

What could be the problem?


From matg2s at gmail.com  Thu Jan 12 07:59:27 2017
From: matg2s at gmail.com (matg2s)
Date: Wed, 11 Jan 2017 23:59:27 -0800 (PST)
Subject: [squid-users] Problem building Squid4
In-Reply-To: <03a601d26b69$4ece5990$ec6b0cb0$@ngtech.co.il>
References: <1483867911104-4681090.post@n4.nabble.com>
 <03a601d26b69$4ece5990$ec6b0cb0$@ngtech.co.il>
Message-ID: <1484207967991-4681139.post@n4.nabble.com>

Hi,

Ok, I'm not sure why - but it seems that using the following configure fixed
this:
./configure --disable-external-acl-helpers --enable-ssl-crtd --with-openssl
--datadir=/usr/share/squid --sysconfdir=/etc/squid
--libexecdir=/usr/lib/squid --mandir=/usr/share/man
--with-swapdir=/var/spool/squid --with-logdir=/var/log/squid
--with-pidfile=/var/run/squid.pid --without-mit-krb5 --without-heimdal-krb5
--with-default-user=proxy 'PKG_CONFIG_PATH=/usr/local/lib'

Eliezer - thanks! I'd love to try and build/get a deb package for Squid 4. 
Could you please point me to either the deb packages others are building or
if you happen to have scripts that do that?

Thanks!
Matt



--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/Problem-building-Squid4-tp4681090p4681139.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From rafael.akchurin at diladele.com  Thu Jan 12 08:02:12 2017
From: rafael.akchurin at diladele.com (Rafael Akchurin)
Date: Thu, 12 Jan 2017 08:02:12 +0000
Subject: [squid-users] Problem building Squid4
In-Reply-To: <1484207967991-4681139.post@n4.nabble.com>
References: <1483867911104-4681090.post@n4.nabble.com>
 <03a601d26b69$4ece5990$ec6b0cb0$@ngtech.co.il>
 <1484207967991-4681139.post@n4.nabble.com>
Message-ID: <DB6PR0401MB268086C8A5B0D3FA7DAC01CC8F790@DB6PR0401MB2680.eurprd04.prod.outlook.com>

Hello Matt,

This is how we repackage Debian 8's 3.5.23 into Ubuntu 16. It is pretty easy - https://docs.diladele.com/howtos/build_squid_ubuntu16/index.html
I hope you can adapt the build instructions for Squid 4. 

Best regards,
Rafael Akchurin
Diladele B.V.

-----Original Message-----
From: squid-users [mailto:squid-users-bounces at lists.squid-cache.org] On Behalf Of matg2s
Sent: Thursday, January 12, 2017 8:59 AM
To: squid-users at lists.squid-cache.org
Subject: Re: [squid-users] Problem building Squid4

Hi,

Ok, I'm not sure why - but it seems that using the following configure fixed
this:
./configure --disable-external-acl-helpers --enable-ssl-crtd --with-openssl --datadir=/usr/share/squid --sysconfdir=/etc/squid --libexecdir=/usr/lib/squid --mandir=/usr/share/man --with-swapdir=/var/spool/squid --with-logdir=/var/log/squid --with-pidfile=/var/run/squid.pid --without-mit-krb5 --without-heimdal-krb5 --with-default-user=proxy 'PKG_CONFIG_PATH=/usr/local/lib'

Eliezer - thanks! I'd love to try and build/get a deb package for Squid 4. 
Could you please point me to either the deb packages others are building or if you happen to have scripts that do that?

Thanks!
Matt



--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/Problem-building-Squid4-tp4681090p4681139.html
Sent from the Squid - Users mailing list archive at Nabble.com.
_______________________________________________
squid-users mailing list
squid-users at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-users

From matg2s at gmail.com  Thu Jan 12 09:02:33 2017
From: matg2s at gmail.com (matg2s)
Date: Thu, 12 Jan 2017 01:02:33 -0800 (PST)
Subject: [squid-users] Problem building Squid4
In-Reply-To: <DB6PR0401MB268086C8A5B0D3FA7DAC01CC8F790@DB6PR0401MB2680.eurprd04.prod.outlook.com>
References: <1483867911104-4681090.post@n4.nabble.com>
 <03a601d26b69$4ece5990$ec6b0cb0$@ngtech.co.il>
 <1484207967991-4681139.post@n4.nabble.com>
 <DB6PR0401MB268086C8A5B0D3FA7DAC01CC8F790@DB6PR0401MB2680.eurprd04.prod.outlook.com>
Message-ID: <1484211753258-4681141.post@n4.nabble.com>

Hi Rafael,

I actually stumbled on your project (and I've gotta say it's very impressive
and well made/documented) in the past.
I did try adapting it to Squid 4.0 but wasn't able to. The dsc file expects
a "3.5.23" tar.gz, and then there some problems with the patches and other
errors so I gave up :-)

I will try it again unless someone succeeded with this and will of course
update.
Thanks!
Matt



--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/Problem-building-Squid4-tp4681090p4681141.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From rahat.khan at pribno.com  Thu Jan 12 09:28:52 2017
From: rahat.khan at pribno.com (Rahat Ali Khan)
Date: Thu, 12 Jan 2017 14:28:52 +0500
Subject: [squid-users] squid-users Digest, Vol 29, Issue 24
In-Reply-To: <mailman.5866.1484145238.20516.squid-users@lists.squid-cache.org>
References: <mailman.5866.1484145238.20516.squid-users@lists.squid-cache.org>
Message-ID: <bdea3345-f1ec-ace0-ec2e-98f0e860bee8@pribno.com>

Hi

Thanks guys for replies.

@Matus UHLA,

Yes the mem cache is shared among the 4 squid worker processes.

Rahat



On 11-Jan-17 7:33 PM, squid-users-request at lists.squid-cache.org wrote:
> Send squid-users mailing list submissions to
> 	squid-users at lists.squid-cache.org
>
> To subscribe or unsubscribe via the World Wide Web, visit
> 	http://lists.squid-cache.org/listinfo/squid-users
> or, via email, send a message with subject or body 'help' to
> 	squid-users-request at lists.squid-cache.org
>
> You can reach the person managing the list at
> 	squid-users-owner at lists.squid-cache.org
>
> When replying, please edit your Subject line so it is more specific
> than "Re: Contents of squid-users digest..."
>
>
> Today's Topics:
>
>     1. Re: Squid Memory Problems (Eliezer  Croitoru)
>     2. Re: Squid Memory Problems (Matus UHLAR - fantomas)
>     3. Re: ERR_CANNOT_FORWARD with Squid + Privoxy (Amos Jeffries)
>     4. Re: SSL_bump and source IP (Amos Jeffries)
>     5. Re: squid-users Digest, Vol 29, Issue 21 (Vidyadhish Joshi)
>
>
> ----------------------------------------------------------------------
>
> Message: 1
> Date: Wed, 11 Jan 2017 15:53:59 +0200
> From: "Eliezer  Croitoru" <eliezer at ngtech.co.il>
> To: "'Rahat Ali Khan'" <rahat.khan at pribno.com>,
> 	<squid-users at lists.squid-cache.org>
> Subject: Re: [squid-users] Squid Memory Problems
> Message-ID: <043001d26c12$2902dbb0$7b089310$@ngtech.co.il>
> Content-Type: text/plain;	charset="utf-8"
>
> There was a report about an issue and the claim was Ubuntu Kernel issue.
> I believe that the first step would be to find out if a kernel downgrade or upgrade (to xenial one) helps with the issue.
>
> Eliezer
>
> ----
> http://ngtech.co.il/lmgtfy/
> Linux System Administrator
> Mobile: +972-5-28704261
> Email: eliezer at ngtech.co.il
>
>
> From: squid-users [mailto:squid-users-bounces at lists.squid-cache.org] On Behalf Of Rahat Ali Khan
> Sent: Wednesday, January 11, 2017 3:21 PM
> To: squid-users at lists.squid-cache.org
> Subject: [squid-users] Squid Memory Problems
>
> Hi there,
> We are using Squid 3.5.14 on Ubuntu 14.04 LTS.  The squid is using 4 worker process and is used as an explicit and intercept proxy.
> We have been facing a problem of squid processes consuming the memory gradually and eventually it comes to a minimum threshold. The end user browsing experience becomes extremely slow at that time. Cache memory configured is 2 GB and total memory of squid machines is 8 and 16 GB on different boxes. The memory consumption graphs shows a gradual decrease in available physical RAM  and we have to ultimately restart the squid boxed after 3-4 days.
> The stats we get from cachemgr show:
> Cache information for squid:
>          Hits as % of all requests:      5min: 1.7%, 60min: 2.2%
>          Hits as % of bytes sent:        5min: 0.2%, 60min: 0.1%
>          Memory hits as % of hit requests:       5min: 91.7%, 60min: 82.8%
>          Disk hits as % of hit requests: 5min: 0.0%, 60min: 0.0%
>          Storage Swap size:      0 KB
>          Storage Swap capacity:   0.0% used,  0.0% free
>          Storage Mem size:       2097056 KB
>          Storage Mem capacity:   100.0% used,  0.0% free
>          Mean Object Size:       0.00 KB
>
> We are suspecting memory leaks to be the cause of this. Please share you experiences .
> Thanks
> Rahat
>   
>
> ________________________________________
>
> This email has been checked for viruses by Avast antivirus software.
> https://www.avast.com/sig-email?utm_medium=email&utm_source=link&utm_campaign=sig-email&utm_content=emailclient
>
>
>
>
> ------------------------------
>
> Message: 2
> Date: Wed, 11 Jan 2017 15:08:13 +0100
> From: Matus UHLAR - fantomas <uhlar at fantomas.sk>
> To: squid-users at lists.squid-cache.org
> Subject: Re: [squid-users] Squid Memory Problems
> Message-ID: <20170111140813.GB19926 at fantomas.sk>
> Content-Type: text/plain; charset=us-ascii; format=flowed
>
> On 11.01.17 18:20, Rahat Ali Khan wrote:
>> We are using Squid 3.5.14 on Ubuntu 14.04 LTS.  The squid is using 4
>> worker process and is used as an explicit and intercept proxy.
>>
>> We have been facing a problem of squid processes consuming the memory
>> gradually and eventually it comes to a minimum threshold. The end
>> user browsing experience becomes extremely slow at that time. Cache
>> memory configured is 2 GB and total memory of squid machines is 8 and
>> 16 GB on different boxes. The memory consumption graphs shows a
>> gradual decrease in available physical RAM and we have to ultimately
>> restart the squid boxed after 3-4 days.
> I would like to note that squid does not use memory only for cache, but for
> many different uses.
>
> do you use shared memory cache?
>


---
This email has been checked for viruses by Avast antivirus software.
https://www.avast.com/antivirus



From squid3 at treenet.co.nz  Thu Jan 12 11:27:39 2017
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Fri, 13 Jan 2017 00:27:39 +1300
Subject: [squid-users] logformat %tl and %tg are same.
In-Reply-To: <186d6a61-9a7a-dbe7-31b1-ff407c18cf05@gmail.com>
References: <186d6a61-9a7a-dbe7-31b1-ff407c18cf05@gmail.com>
Message-ID: <4c7887d5-822d-2d99-e351-c0495e50934d@treenet.co.nz>

On 12/01/2017 8:43 p.m., Mimiko wrote:
> Hello.
> 
> I use Squid Cache: Version 3.1.20
> 
> In order to view the log in a readable format I use this:
> 
> logformat squid %{%Y/%m/%d %H:%M:%S}tl %ts.%03tu %6tr %>a %Ss/%03>Hs
> %<st %rm %ru %un %Sh/%<A %mt
> 
> The problem is that the time in log is in GMT despite of %tl option.
> Server is configured to local time. Squid for both, %tl and %tg, shows
> same time.
> 
> What could be the problem?

The problem is very probably that "Server is configured to local time".

Local time is a political value decided by your countries government,
politicos get to change it whenever they please. And some countries do
play around with it for ridiculous reasons - mine for example changed
local daylight savings time recently so the national sports game can be
played during daylight and broadcast "outside of work hours". So
essentially what time our whole country sees now depends on the
corporate owners of sports stadiums deciding when the games occur. :-(

Internet connected machinery (ie everything nowdays) is supposed to be
using UTC. It would not be a good situation for two adjacent machines to
display for example, ping times of 12hrs just because their admin wanted
different _displays_ on their calendar applications (eg maintained by
people in different countries).

By setting your server to use "local time" (whatever that may be for
you) you are making it tell Squid that GMT == local time.

Your proxy is probably also getting very confused about cacheability of
things because the objects timestamps delivered are always off by
whatever your governments local timezone differential is.


The right thing to do is make your machines operate with UTC and only
use local time for *display* purposes to humans.

Amos



From vbvbrj at gmail.com  Thu Jan 12 11:40:22 2017
From: vbvbrj at gmail.com (Mimiko)
Date: Thu, 12 Jan 2017 13:40:22 +0200
Subject: [squid-users] logformat %tl and %tg are same.
In-Reply-To: <4c7887d5-822d-2d99-e351-c0495e50934d@treenet.co.nz>
References: <186d6a61-9a7a-dbe7-31b1-ff407c18cf05@gmail.com>
 <4c7887d5-822d-2d99-e351-c0495e50934d@treenet.co.nz>
Message-ID: <bf0be821-229a-78f4-0c4a-fa63432f5d9e@gmail.com>

On 12.01.2017 13:27, Amos Jeffries wrote:
> The problem is very probably that "Server is configured to local time".
>
> By setting your server to use "local time" (whatever that may be for
> you) you are making it tell Squid that GMT == local time.

Ok. I understand your point. But why squid shows time in log in GMT? All 
other applications log ok, only squid logs time as if server's local 
time was configured to GMT.


From oguzismailuysal at gmail.com  Thu Jan 12 11:54:33 2017
From: oguzismailuysal at gmail.com (=?UTF-8?Q?O=C4=9Fuz_=C4=B0smail_Uysal?=)
Date: Thu, 12 Jan 2017 03:54:33 -0800
Subject: [squid-users] Linker error while compiling version 3.5.12 after
	applying a patch
Message-ID: <CAH7i3Lqr=-gdvXy2SiAAP1kNgNqmYYCzeX2ghkFxFB+WmaHuQg@mail.gmail.com>

I have downloaded source tarball from
http://www.squid-cache.org/Versions/v3/3.5/squid-3.5.12.tar.gz .
I have downloaded the patch I have mentioned from
http://www.squid-cache.org/Versions/v3/3.5/changesets/SQUID-2016_5.patch
then applied to source.
I have installed building tools by typing
>apt build-dep squid
to terminal.
While compiling source on Ubuntu 16.04 x64 I got these errors;
/bin/bash ../libtool  --tag=CXX   --mode=link g++
-DDEFAULT_CACHEMGR_CONFIG=\"/etc/squid/cachemgr.conf\"
-I/usr/include/libxml2 -Wall -Wpointer-arith -Wwrite-strings -Wcomments
-Wshadow -Werror -pipe -D_REENTRANT -m64 -I/usr/include/p11-kit-1 -g -O2
-fPIE -fstack-protector-strong -Wformat -Werror=format-security -std=c++11
 -m64 -Wl,-Bsymbolic-functions -fPIE -pie -Wl,-z,relro -Wl,-z,now -o
cachemgr.cgi cachemgr__CGIEXT_-cachemgr.o cachemgr__CGIEXT_-stub_debug.o
cachemgr__CGIEXT_-test_tools.o cachemgr__CGIEXT_-time.o ../src/ip/libip.la
../lib/libmiscencoding.la ../lib/libmiscutil.la ../compat/libcompat-squid.la
 -lnettle -L/usr/lib/x86_64-linux-gnu/mit-krb5 -lgssapi_krb5 -lkrb5
-lk5crypto -lcom_err  -lm -lnsl -lresolv -lcap -lnetfilter_conntrack -lrt
-ldl -ldl
libtool: link: g++ -DDEFAULT_CACHEMGR_CONFIG=\"/etc/squid/cachemgr.conf\"
-I/usr/include/libxml2 -Wall -Wpointer-arith -Wwrite-strings -Wcomments
-Wshadow -Werror -pipe -D_REENTRANT -m64 -I/usr/include/p11-kit-1 -g -O2
-fPIE -fstack-protector-strong -Wformat -Werror=format-security -std=c++11
-m64 -Wl,-Bsymbolic-functions -fPIE -pie -Wl,-z -Wl,relro -Wl,-z -Wl,now -o
cachemgr.cgi cachemgr__CGIEXT_-cachemgr.o cachemgr__CGIEXT_-stub_debug.o
cachemgr__CGIEXT_-test_tools.o cachemgr__CGIEXT_-time.o
 ../src/ip/.libs/libip.a ../lib/.libs/libmiscencoding.a
../lib/.libs/libmiscutil.a ../compat/.libs/libcompat-squid.a -lnettle
-L/usr/lib/x86_64-linux-gnu/mit-krb5 -lgssapi_krb5 -lkrb5 -lk5crypto
-lcom_err -lm -lnsl -lresolv -lcap -lnetfilter_conntrack -lrt -ldl
cachemgr__CGIEXT_-cachemgr.o: In function `munge_menu_line':
/root/squid-3.5.12/tools/cachemgr.cc:439: undefined reference to
`MemBuf::append(char const*, long)'
cachemgr__CGIEXT_-cachemgr.o: In function `MemBuf::~MemBuf()':
/root/squid-3.5.12/tools/../src/MemBuf.cci:17: undefined reference to
`MemBuf::clean()'
cachemgr__CGIEXT_-cachemgr.o: In function `read_reply(int,
cachemgr_request*)':
/root/squid-3.5.12/tools/cachemgr.cc:701: undefined reference to
`MemBuf::init()'
cachemgr__CGIEXT_-cachemgr.o: In function `munge_other_line':
/root/squid-3.5.12/tools/cachemgr.cc:514: undefined reference to
`MemBuf::append(char const*, long)'
/root/squid-3.5.12/tools/cachemgr.cc:534: undefined reference to
`MemBuf::Printf(char const*, ...)'
/root/squid-3.5.12/tools/cachemgr.cc:539: undefined reference to
`MemBuf::append(char const*, long)'
/root/squid-3.5.12/tools/cachemgr.cc:497: undefined reference to
`MemBuf::Printf(char const*, ...)'
cachemgr__CGIEXT_-cachemgr.o: In function `munge_menu_line':
/root/squid-3.5.12/tools/cachemgr.cc:469: undefined reference to
`MemBuf::Printf(char const*, ...)'
cachemgr__CGIEXT_-cachemgr.o: In function `munge_other_line':
/root/squid-3.5.12/tools/cachemgr.cc:504: undefined reference to
`MemBuf::append(char const*, long)'
/root/squid-3.5.12/tools/cachemgr.cc:496: undefined reference to
`MemBuf::append(char const*, long)'
cachemgr__CGIEXT_-cachemgr.o: In function `munge_menu_line':
/root/squid-3.5.12/tools/cachemgr.cc:455: undefined reference to
`MemBuf::Printf(char const*, ...)'
/root/squid-3.5.12/tools/cachemgr.cc:473: undefined reference to
`MemBuf::Printf(char const*, ...)'
/root/squid-3.5.12/tools/cachemgr.cc:459: undefined reference to
`MemBuf::Printf(char const*, ...)'
cachemgr__CGIEXT_-cachemgr.o: In function `MemBuf::~MemBuf()':
/root/squid-3.5.12/tools/../src/MemBuf.cci:17: undefined reference to
`MemBuf::clean()'
cachemgr__CGIEXT_-cachemgr.o: In function `munge_menu_line':
/root/squid-3.5.12/tools/cachemgr.cc:464: undefined reference to
`MemBuf::Printf(char const*, ...)'
collect2: error: ld returned 1 exit status
Makefile:868: recipe for target 'cachemgr.cgi' failed
make[2]: *** [cachemgr.cgi] Error 1
make[2]: Leaving directory '/root/squid-3.5.12/tools'
Makefile:1053: recipe for target 'all-recursive' failed
make[1]: *** [all-recursive] Error 1
make[1]: Leaving directory '/root/squid-3.5.12/tools'
Makefile:579: recipe for target 'all-recursive' failed
make: *** [all-recursive] Error 1
What am I doing wrong ?
Without that patch it will not show any error.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20170112/d32dcde9/attachment.htm>

From squid3 at treenet.co.nz  Thu Jan 12 12:39:09 2017
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Fri, 13 Jan 2017 01:39:09 +1300
Subject: [squid-users] Linker error while compiling version 3.5.12 after
 applying a patch
In-Reply-To: <CAH7i3Lqr=-gdvXy2SiAAP1kNgNqmYYCzeX2ghkFxFB+WmaHuQg@mail.gmail.com>
References: <CAH7i3Lqr=-gdvXy2SiAAP1kNgNqmYYCzeX2ghkFxFB+WmaHuQg@mail.gmail.com>
Message-ID: <6e6bd691-22b0-8c5e-b7e4-8d39c6e74813@treenet.co.nz>

On 13/01/2017 12:54 a.m., O?uz ?smail Uysal wrote:
> I have downloaded source tarball from
> http://www.squid-cache.org/Versions/v3/3.5/squid-3.5.12.tar.gz .
> I have downloaded the patch I have mentioned from
> http://www.squid-cache.org/Versions/v3/3.5/changesets/SQUID-2016_5.patch
> then applied to source.

Er, why? Ubuntu are already providing a 3.5.12-1ubuntu7.2 package in
their 16.04 main repository which contains this and many other CVE fixes
relevant to that version of Squid.


> I have installed building tools by typing
>> apt build-dep squid
> to terminal.
> While compiling source on Ubuntu 16.04 x64 I got these errors;
<snip>
> /root/squid-3.5.12/tools/cachemgr.cc:439: undefined reference to
> `MemBuf::append(char const*, long)'
<snip>

> What am I doing wrong ?
> Without that patch it will not show any error.

That security patch contains changes to the automake build files. IIRC
you need to autoreconf the sources after applying that kind of change.

BUT...

If you are just trying to build your own Squid please either use the
source package available from your vendor (Ubuntu) or the latest minor
release we have published for the appropriate series (3.5.23 currently).

The ubuntu source package can be obtained with "apt-get source squid",
use the -b option before 'source' to build it on retrieval.


Amos



From squid3 at treenet.co.nz  Thu Jan 12 13:01:35 2017
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Fri, 13 Jan 2017 02:01:35 +1300
Subject: [squid-users] logformat %tl and %tg are same.
In-Reply-To: <bf0be821-229a-78f4-0c4a-fa63432f5d9e@gmail.com>
References: <186d6a61-9a7a-dbe7-31b1-ff407c18cf05@gmail.com>
 <4c7887d5-822d-2d99-e351-c0495e50934d@treenet.co.nz>
 <bf0be821-229a-78f4-0c4a-fa63432f5d9e@gmail.com>
Message-ID: <c4c4b5d2-4898-bd62-c2a6-39eda3517d8f@treenet.co.nz>

On 13/01/2017 12:40 a.m., Mimiko wrote:
> On 12.01.2017 13:27, Amos Jeffries wrote:
>> The problem is very probably that "Server is configured to local time".
>>
>> By setting your server to use "local time" (whatever that may be for
>> you) you are making it tell Squid that GMT == local time.
> 
> Ok. I understand your point. But why squid shows time in log in GMT? All
> other applications log ok, only squid logs time as if server's local
> time was configured to GMT.

Not sure, and 3.1 is so obsolete a version I'm not even able to test it
anymore.

FWIW; I am not seeing your problem on the current Squid versions. %tl
and %tg show different values. Though I do have my machine set to
operate in UTC properly.

Amos



From rahat.khan at pribno.com  Thu Jan 12 13:19:00 2017
From: rahat.khan at pribno.com (Rahat Ali Khan)
Date: Thu, 12 Jan 2017 18:19:00 +0500
Subject: [squid-users] Squid Memory Problems
In-Reply-To: <mailman.5866.1484145238.20516.squid-users@lists.squid-cache.org>
References: <mailman.5866.1484145238.20516.squid-users@lists.squid-cache.org>
Message-ID: <63cc39df-dd77-6849-6890-94da9b98c51a@pribno.com>

Hi

Thanks guys for replies.

@Matus UHLA,

Yes the mem cache is shared among the 4 squid worker processes.

Rahat


On 11-Jan-17 7:33 PM, squid-users-request at lists.squid-cache.org wrote:
> Send squid-users mailing list submissions to
> 	squid-users at lists.squid-cache.org
>
> To subscribe or unsubscribe via the World Wide Web, visit
> 	http://lists.squid-cache.org/listinfo/squid-users
> or, via email, send a message with subject or body 'help' to
> 	squid-users-request at lists.squid-cache.org
>
> You can reach the person managing the list at
> 	squid-users-owner at lists.squid-cache.org
>
> When replying, please edit your Subject line so it is more specific
> than "Re: Contents of squid-users digest..."
>
>
> Today's Topics:
>
>     1. Re: Squid Memory Problems (Eliezer  Croitoru)
>     2. Re: Squid Memory Problems (Matus UHLAR - fantomas)
>     3. Re: ERR_CANNOT_FORWARD with Squid + Privoxy (Amos Jeffries)
>     4. Re: SSL_bump and source IP (Amos Jeffries)
>     5. Re: squid-users Digest, Vol 29, Issue 21 (Vidyadhish Joshi)
>
>
> ----------------------------------------------------------------------
>
> Message: 1
> Date: Wed, 11 Jan 2017 15:53:59 +0200
> From: "Eliezer  Croitoru" <eliezer at ngtech.co.il>
> To: "'Rahat Ali Khan'" <rahat.khan at pribno.com>,
> 	<squid-users at lists.squid-cache.org>
> Subject: Re: [squid-users] Squid Memory Problems
> Message-ID: <043001d26c12$2902dbb0$7b089310$@ngtech.co.il>
> Content-Type: text/plain;	charset="utf-8"
>
> There was a report about an issue and the claim was Ubuntu Kernel issue.
> I believe that the first step would be to find out if a kernel downgrade or upgrade (to xenial one) helps with the issue.
>
> Eliezer
>
> ----
> http://ngtech.co.il/lmgtfy/
> Linux System Administrator
> Mobile: +972-5-28704261
> Email: eliezer at ngtech.co.il
>
>
> From: squid-users [mailto:squid-users-bounces at lists.squid-cache.org] On Behalf Of Rahat Ali Khan
> Sent: Wednesday, January 11, 2017 3:21 PM
> To: squid-users at lists.squid-cache.org
> Subject: [squid-users] Squid Memory Problems
>
> Hi there,
> We are using Squid 3.5.14 on Ubuntu 14.04 LTS.  The squid is using 4 worker process and is used as an explicit and intercept proxy.
> We have been facing a problem of squid processes consuming the memory gradually and eventually it comes to a minimum threshold. The end user browsing experience becomes extremely slow at that time. Cache memory configured is 2 GB and total memory of squid machines is 8 and 16 GB on different boxes. The memory consumption graphs shows a gradual decrease in available physical RAM  and we have to ultimately restart the squid boxed after 3-4 days.
> The stats we get from cachemgr show:
> Cache information for squid:
>          Hits as % of all requests:      5min: 1.7%, 60min: 2.2%
>          Hits as % of bytes sent:        5min: 0.2%, 60min: 0.1%
>          Memory hits as % of hit requests:       5min: 91.7%, 60min: 82.8%
>          Disk hits as % of hit requests: 5min: 0.0%, 60min: 0.0%
>          Storage Swap size:      0 KB
>          Storage Swap capacity:   0.0% used,  0.0% free
>          Storage Mem size:       2097056 KB
>          Storage Mem capacity:   100.0% used,  0.0% free
>          Mean Object Size:       0.00 KB
>
> We are suspecting memory leaks to be the cause of this. Please share you experiences .
> Thanks
> Rahat
>   
>
> ________________________________________
>
> This email has been checked for viruses by Avast antivirus software.
> https://www.avast.com/sig-email?utm_medium=email&utm_source=link&utm_campaign=sig-email&utm_content=emailclient
>
>
>
>
> ------------------------------
>
> Message: 2
> Date: Wed, 11 Jan 2017 15:08:13 +0100
> From: Matus UHLAR - fantomas <uhlar at fantomas.sk>
> To: squid-users at lists.squid-cache.org
> Subject: Re: [squid-users] Squid Memory Problems
> Message-ID: <20170111140813.GB19926 at fantomas.sk>
> Content-Type: text/plain; charset=us-ascii; format=flowed
>
> On 11.01.17 18:20, Rahat Ali Khan wrote:
>> We are using Squid 3.5.14 on Ubuntu 14.04 LTS.  The squid is using 4
>> worker process and is used as an explicit and intercept proxy.
>>
>> We have been facing a problem of squid processes consuming the memory
>> gradually and eventually it comes to a minimum threshold. The end
>> user browsing experience becomes extremely slow at that time. Cache
>> memory configured is 2 GB and total memory of squid machines is 8 and
>> 16 GB on different boxes. The memory consumption graphs shows a
>> gradual decrease in available physical RAM and we have to ultimately
>> restart the squid boxed after 3-4 days.
> I would like to note that squid does not use memory only for cache, but for
> many different uses.
>
> do you use shared memory cache?
>


---
This email has been checked for viruses by Avast antivirus software.
https://www.avast.com/antivirus



From alex.tate at gmail.com  Thu Jan 12 14:44:12 2017
From: alex.tate at gmail.com (roadrage27)
Date: Thu, 12 Jan 2017 06:44:12 -0800 (PST)
Subject: [squid-users] TCP 403 Denied on new squid build out
In-Reply-To: <ecc10a1a-8ec1-9dd0-e95d-c3e694b9187f@treenet.co.nz>
References: <1484152369267-4681127.post@n4.nabble.com>
 <20170111205514.GA8844@fantomas.sk>
 <CAFwozXJ7=y_ARAAEJdd+kNWMcpDnLtU-B8zj26T3ZF137Twdbw@mail.gmail.com>
 <ecc10a1a-8ec1-9dd0-e95d-c3e694b9187f@treenet.co.nz>
Message-ID: <1484232252343-4681149.post@n4.nabble.com>

I made the changes suggested and still no love from squid for browsing the
web using it as a proxy.

the error screen reads

Access Denied

Access control configuration prevents your request from being allowed at
this time.  Please contact your service provider if you feel this is
incorrect.


nothing in cache.log that has an error associated with it



--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/TCP-403-Denied-on-new-squid-build-out-tp4681127p4681149.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From squid3 at treenet.co.nz  Thu Jan 12 15:28:50 2017
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Fri, 13 Jan 2017 04:28:50 +1300
Subject: [squid-users] TCP 403 Denied on new squid build out
In-Reply-To: <1484232252343-4681149.post@n4.nabble.com>
References: <1484152369267-4681127.post@n4.nabble.com>
 <20170111205514.GA8844@fantomas.sk>
 <CAFwozXJ7=y_ARAAEJdd+kNWMcpDnLtU-B8zj26T3ZF137Twdbw@mail.gmail.com>
 <ecc10a1a-8ec1-9dd0-e95d-c3e694b9187f@treenet.co.nz>
 <1484232252343-4681149.post@n4.nabble.com>
Message-ID: <e49381e0-3590-aa18-4c8e-dab1142ecf75@treenet.co.nz>

On 13/01/2017 3:44 a.m., roadrage27 wrote:
> I made the changes suggested and still no love from squid for browsing the
> web using it as a proxy.
> 
> the error screen reads
> 
> Access Denied
> 
> Access control configuration prevents your request from being allowed at
> this time.  Please contact your service provider if you feel this is
> incorrect.
> 

Okay. Would you mind re-posting your updated squid.conf so we can check
teh changes were done right?

Amos



From alex.tate at gmail.com  Thu Jan 12 16:16:19 2017
From: alex.tate at gmail.com (roadrage27)
Date: Thu, 12 Jan 2017 08:16:19 -0800 (PST)
Subject: [squid-users] TCP 403 Denied on new squid build out
In-Reply-To: <e49381e0-3590-aa18-4c8e-dab1142ecf75@treenet.co.nz>
References: <1484152369267-4681127.post@n4.nabble.com>
 <20170111205514.GA8844@fantomas.sk>
 <CAFwozXJ7=y_ARAAEJdd+kNWMcpDnLtU-B8zj26T3ZF137Twdbw@mail.gmail.com>
 <ecc10a1a-8ec1-9dd0-e95d-c3e694b9187f@treenet.co.nz>
 <1484232252343-4681149.post@n4.nabble.com>
 <e49381e0-3590-aa18-4c8e-dab1142ecf75@treenet.co.nz>
Message-ID: <CAFwozX+9k1ucQ8+Bg+P=89X9iVLqC-bp9q=_V+Bv3nRdf28Pyw@mail.gmail.com>

sure thing here it is

http_access allow all



#Recommended minimum configuration:

#acl manager proto cache_object

#acl localhost src 127.0.0.1/32

#acl to_localhost dst 127.0.0.0/8

acl localnet src 0.0.0.0/8 10.145.68.0/24

#acl myip src 10.145.68.148/32



acl SSL_ports port 443

acl Safe_ports port 80          # http

acl Safe_ports port 21          # ftp

acl Safe_ports port 443         # https

acl Safe_ports port 70          # gopher

acl Safe_ports port 210         # wais

acl Safe_ports port 1025-65535  # unregistered ports

acl Safe_ports port 280         # http-mgmt

acl Safe_ports port 488         # gss-http

acl Safe_ports port 591         # filemaker

acl Safe_ports port 777         # multiling http



acl CONNECT method CONNECT



http_access deny !Safe_ports

http_access allow manager localhost

http_access allow localnet

http_access deny manager



http_port 3128

hierarchy_stoplist cgi-bin ?

access_log /var/log/squid3/access.log squid


On Thu, Jan 12, 2017 at 9:29 AM Amos Jeffries [via Squid Web Proxy Cache] <
ml-node+s1019090n4681150h5 at n4.nabble.com> wrote:

On 13/01/2017 3:44 a.m., roadrage27 wrote:

> I made the changes suggested and still no love from squid for browsing
the
> web using it as a proxy.
>
> the error screen reads
>
> Access Denied
>
> Access control configuration prevents your request from being allowed at
> this time.  Please contact your service provider if you feel this is
> incorrect.
>

Okay. Would you mind re-posting your updated squid.conf so we can check
teh changes were done right?

Amos

_______________________________________________
squid-users mailing list
[hidden email] <http:///user/SendEmail.jtp?type=node&node=4681150&i=0>
http://lists.squid-cache.org/listinfo/squid-users


If you reply to this email, your message will be added to the discussion
below:
http://squid-web-proxy-cache.1019090.n4.nabble.com/TCP-403-Denied-on-new-squid-build-out-tp4681127p4681150.html
To unsubscribe from TCP 403 Denied on new squid build out, click here
<http://squid-web-proxy-cache.1019090.n4.nabble.com/template/NamlServlet.jtp?macro=unsubscribe_by_code&node=4681127&code=YWxleC50YXRlQGdtYWlsLmNvbXw0NjgxMTI3fDIwMjU4MDQxMw==>
.
NAML
<http://squid-web-proxy-cache.1019090.n4.nabble.com/template/NamlServlet.jtp?macro=macro_viewer&id=instant_html%21nabble%3Aemail.naml&base=nabble.naml.namespaces.BasicNamespace-nabble.view.web.template.NabbleNamespace-nabble.view.web.template.NodeNamespace&breadcrumbs=notify_subscribers%21nabble%3Aemail.naml-instant_emails%21nabble%3Aemail.naml-send_instant_email%21nabble%3Aemail.naml>




--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/TCP-403-Denied-on-new-squid-build-out-tp4681127p4681151.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From uhlar at fantomas.sk  Thu Jan 12 18:12:49 2017
From: uhlar at fantomas.sk (Matus UHLAR - fantomas)
Date: Thu, 12 Jan 2017 19:12:49 +0100
Subject: [squid-users] TCP 403 Denied on new squid build out
In-Reply-To: <CAFwozX+9k1ucQ8+Bg+P=89X9iVLqC-bp9q=_V+Bv3nRdf28Pyw@mail.gmail.com>
References: <1484152369267-4681127.post@n4.nabble.com>
 <20170111205514.GA8844@fantomas.sk>
 <CAFwozXJ7=y_ARAAEJdd+kNWMcpDnLtU-B8zj26T3ZF137Twdbw@mail.gmail.com>
 <ecc10a1a-8ec1-9dd0-e95d-c3e694b9187f@treenet.co.nz>
 <1484232252343-4681149.post@n4.nabble.com>
 <e49381e0-3590-aa18-4c8e-dab1142ecf75@treenet.co.nz>
 <CAFwozX+9k1ucQ8+Bg+P=89X9iVLqC-bp9q=_V+Bv3nRdf28Pyw@mail.gmail.com>
Message-ID: <20170112181249.GA7479@fantomas.sk>

On 12.01.17 08:16, roadrage27 wrote:
>sure thing here it is
>
>http_access allow all

you allow everything at very beginning...

are you sure squid uses _this_ configuration file?
if so, are you sure squid has access to internet?


-- 
Matus UHLAR - fantomas, uhlar at fantomas.sk ; http://www.fantomas.sk/
Warning: I wish NOT to receive e-mail advertising to this address.
Varovanie: na tuto adresu chcem NEDOSTAVAT akukolvek reklamnu postu.
I drive way too fast to worry about cholesterol. 


From alex.tate at gmail.com  Thu Jan 12 18:19:37 2017
From: alex.tate at gmail.com (roadrage27)
Date: Thu, 12 Jan 2017 10:19:37 -0800 (PST)
Subject: [squid-users] TCP 403 Denied on new squid build out
In-Reply-To: <20170112181249.GA7479@fantomas.sk>
References: <1484152369267-4681127.post@n4.nabble.com>
 <20170111205514.GA8844@fantomas.sk>
 <CAFwozXJ7=y_ARAAEJdd+kNWMcpDnLtU-B8zj26T3ZF137Twdbw@mail.gmail.com>
 <ecc10a1a-8ec1-9dd0-e95d-c3e694b9187f@treenet.co.nz>
 <1484232252343-4681149.post@n4.nabble.com>
 <e49381e0-3590-aa18-4c8e-dab1142ecf75@treenet.co.nz>
 <CAFwozX+9k1ucQ8+Bg+P=89X9iVLqC-bp9q=_V+Bv3nRdf28Pyw@mail.gmail.com>
 <20170112181249.GA7479@fantomas.sk>
Message-ID: <1484245177642-4681153.post@n4.nabble.com>

if i run squid-k parse it shows me the output of the processing of that
config file i posted.

from the server where squid is running i can browse the internet and have
disabled the firewall running on the server as well.



--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/TCP-403-Denied-on-new-squid-build-out-tp4681127p4681153.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From sameh.onaissi at solcv.com  Thu Jan 12 21:09:58 2017
From: sameh.onaissi at solcv.com (Sameh Onaissi)
Date: Thu, 12 Jan 2017 21:09:58 +0000
Subject: [squid-users] A bunch of SSL errors I am not sure why
Message-ID: <2EBAFFFF-752F-45BC-B06A-E5513CE7D7BA@solcv.com>

System info:
Squid Cache: Version 3.5.22
Ubuntu linux 16.04

Hello,

Last couple of days I have started seeing SSL errors in my cache.log which I don?t really understand: http://pastebin.com/mDHVm7cQ

My SSL bump configs:

http_port 3127 intercept
http_port 3128
https_port 3129 intercept ssl-bump generate-host-certificates=on dynamic_cert_mem_cache_size=4MB cert=/etc/squid/ssl_certs/squid.crt key=/etc/squid/ssl_certs/squid.key cipher=ECDHE-RSA-RC4-SHA:ECDHE-RSA-AES128-SHA:DHE-RSA-AES128-SHA:DHE-RSA-CAMELLIA128-SHA:AES128-SHA:RC4-SHA:HIGH:!aNULL:!MD5:!ADH

acl step1 at_step SslBump1
acl step2 at_step SslBump2
acl step3 at_step SslBump3
ssl_bump peek step1 all
ssl_bump bump all


I have tried deleting /var/lib/ssh_db and recreating a fresh one, restarted squid, and no luck.

While the service still works fine, some websites like https://web.dlinkla.com/websys were showing a handshake error until I added the site IP into a bypass list.
The internet speed also drops every now and then due to this.

Any help is appreciated with these errors.

Thanks,
Sam


-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20170112/1b46b130/attachment.htm>

From eliezer at ngtech.co.il  Fri Jan 13 00:28:41 2017
From: eliezer at ngtech.co.il (Eliezer  Croitoru)
Date: Fri, 13 Jan 2017 02:28:41 +0200
Subject: [squid-users] A bunch of SSL errors I am not sure why
In-Reply-To: <2EBAFFFF-752F-45BC-B06A-E5513CE7D7BA@solcv.com>
References: <2EBAFFFF-752F-45BC-B06A-E5513CE7D7BA@solcv.com>
Message-ID: <049c01d26d33$fe058520$fa108f60$@ngtech.co.il>

Try removing:
cipher=ECDHE-RSA-RC4-SHA:ECDHE-RSA-AES128-SHA:DHE-RSA-AES128-SHA:DHE-RSA-CAMELLIA128-SHA:AES128-SHA:RC4-SHA:HIGH:!aNULL:!MD5:!ADH

>From the ssl-bump line and see what happens.

----
http://ngtech.co.il/lmgtfy/
Linux System Administrator
Mobile: +972-5-28704261
Email: eliezer at ngtech.co.il


From: squid-users [mailto:squid-users-bounces at lists.squid-cache.org] On Behalf Of Sameh Onaissi
Sent: Thursday, January 12, 2017 11:10 PM
To: squid-users at lists.squid-cache.org
Subject: [squid-users] A bunch of SSL errors I am not sure why

System info: 
Squid Cache: Version 3.5.22
Ubuntu linux 16.04


Hello,


Last couple of days I have started seeing SSL errors in my cache.log which I don?t really understand: http://pastebin.com/mDHVm7cQ

My SSL bump configs:

http_port 3127 intercept
http_port 3128 
https_port 3129 intercept ssl-bump generate-host-certificates=on dynamic_cert_mem_cache_size=4MB cert=/etc/squid/ssl_certs/squid.crt key=/etc/squid/ssl_certs/squid.key cipher=ECDHE-RSA-RC4-SHA:ECDHE-RSA-AES128-SHA:DHE-RSA-AES128-SHA:DHE-RSA-CAMELLIA128-SHA:AES128-SHA:RC4-SHA:HIGH:!aNULL:!MD5:!ADH

acl step1 at_step SslBump1
acl step2 at_step SslBump2
acl step3 at_step SslBump3
ssl_bump peek step1 all
ssl_bump bump all




I have tried deleting /var/lib/ssh_db and recreating a fresh one, restarted squid, and no luck.


While the service still works fine, some websites like https://web.dlinkla.com/websys were showing a handshake error until I added the site IP into a bypass list.
The internet speed also drops every now and then due to this.


Any help is appreciated with these errors.


Thanks,
Sam







From squid3 at treenet.co.nz  Fri Jan 13 05:37:30 2017
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Fri, 13 Jan 2017 18:37:30 +1300
Subject: [squid-users] squid-users Digest, Vol 29, Issue 21
In-Reply-To: <CAMRD5gRa1JYdAdRJgXYMc9A3OWzqm1P7rvymsoGN+4-FAR7BZQ@mail.gmail.com>
References: <mailman.5856.1484131042.20516.squid-users@lists.squid-cache.org>
 <24abc21d-a56b-32dd-caae-77aa77754cf9@visolve.com>
 <07ad859b-0dbf-c681-8905-eda3f3e7f0bf@treenet.co.nz>
 <CAMRD5gQrsmwe5zG_E5UwzUrP+epUzMH9UAFWdepASN-+dQZdwQ@mail.gmail.com>
 <87eeb7e5-73e1-361a-ab87-8b702ba42e89@treenet.co.nz>
 <CAMRD5gR=NyYyNNg07pJp1zsc9UCnFSN6REMGO9CSrxkcdreaPg@mail.gmail.com>
 <CAMRD5gRa1JYdAdRJgXYMc9A3OWzqm1P7rvymsoGN+4-FAR7BZQ@mail.gmail.com>
Message-ID: <e110f3c0-845d-f466-7f40-5b9249c3e7fe@treenet.co.nz>

On 13/01/2017 4:54 a.m., Vidyadhish Joshi wrote:
> Hi Amos,
> 
> I appreciate your effort in answering the queries .
> Thank you Amos for resolving most of my queries since from last week.
> 
> Sorry to disturb you again. I am facing an issue in my application.
> 
> The Squid is caching the contents and getting TCP_HIT and TCP_MEM_HIT in
> access logs. If i keep the browser idle for 10 min and later if i try to
> navigate to same tabs in application which were cached earlier, then
> instead of serving from cache its taking more time to load nearly 10 to 15
> seconds and again hitting the server again.
> 
> I have default squid cache with refresh pattern for my application URL
> cached for one day i.e 1440 i.e
> 
> refresh_pattern ^http://*.applicationurl.net.*/.* 1440 100% 4320
> 

The regex on that line is not correct.

The "http://*." part requires 0 or more '/' characters before the
"application.net" domain name. It *does not* match sub-domain segments.

Also Squid does not need a trailing .* pattern, that is implicit.

I think the pattern you were trying to achieve is:
  ^http://(.*\.)?applicationurl\.net(\..*)?/


Also be aware that refresh_pattern does not force those parameters on
any URL. It is simpy providing default values *if* (and only if) the
response for matching URLs does not contain the relevant Cache-Control
settings.
 If the server provides caching values, those will be used instead.

> Is there a way to overcome this issue? Did you face this issue in web
> application ?
> 

There does not appear to be any issue in the caching. Dynamic content
can have very short storage times. That is why it is called dynamic.
>From your description it sounds like the application updates its content
more often than each 10mins.

Take the URL from your access.log and paste it into the tool at
http://redbot.org. It will tell what cacheability behaviour is expected.

The 10-15 second delay is not something Squid can do much about, if the
server takes that long to generate a reply thats what it takes.

Amos



From stephen.baynes at smoothwall.net  Fri Jan 13 10:10:32 2017
From: stephen.baynes at smoothwall.net (Stephen Baynes)
Date: Fri, 13 Jan 2017 10:10:32 +0000
Subject: [squid-users] =?utf-8?q?Squid_performance_3=2E5=2E20_=E2=86=92_3?=
	=?utf-8?b?LjUuMjM=?=
Message-ID: <CABrJVoz5WcSC-kWgskrbMEeosq_uVQy5h3x7wA_2FJkRPbNstA@mail.gmail.com>

Is there a known performance fall off going 3.5.20 ? 3.5.23?

I am seeing a 15% to 20% performance drop on my normal download benchmark
and a crude test of uploading shows a few percent slowdown.

Running on a Linux derived from Debian.

Thanks

-- 

Stephen Baynes
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20170113/12007891/attachment.htm>

From yvoinov at gmail.com  Fri Jan 13 10:59:05 2017
From: yvoinov at gmail.com (Yuri)
Date: Fri, 13 Jan 2017 16:59:05 +0600
Subject: [squid-users]
 =?utf-8?q?Squid_performance_3=2E5=2E20_=E2=86=92_3?=
 =?utf-8?b?LjUuMjM=?=
In-Reply-To: <CABrJVoz5WcSC-kWgskrbMEeosq_uVQy5h3x7wA_2FJkRPbNstA@mail.gmail.com>
References: <CABrJVoz5WcSC-kWgskrbMEeosq_uVQy5h3x7wA_2FJkRPbNstA@mail.gmail.com>
Message-ID: <5b23e857-0879-b38f-e716-8f95ab0c3e3a@gmail.com>

"Premature optimization is root of all evlis".


13.01.2017 16:10, Stephen Baynes ?????:
> Is there a known performance fall off going 3.5.20 ? 3.5.23?
>
> I am seeing a 15% to 20% performance drop on my normal download 
> benchmark and a crude test of uploading shows a few percent slowdown.
>
> Running on a Linux derived from Debian.
>
> Thanks
>
> -- 
>
> Stephen Baynes
>
>
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20170113/8397ba1e/attachment.htm>

From sameh.onaissi at solcv.com  Fri Jan 13 15:27:05 2017
From: sameh.onaissi at solcv.com (Sameh Onaissi)
Date: Fri, 13 Jan 2017 15:27:05 +0000
Subject: [squid-users] A bunch of SSL errors I am not sure why
In-Reply-To: <049c01d26d33$fe058520$fa108f60$@ngtech.co.il>
References: <2EBAFFFF-752F-45BC-B06A-E5513CE7D7BA@solcv.com>
 <049c01d26d33$fe058520$fa108f60$@ngtech.co.il>
Message-ID: <2403F106-B288-4780-BF4F-06AE58E1A52F@solcv.com>

Hello Eliezer, all,


I removed the cipher and the problem is still there:


2017/01/13 10:20:50 kid1| Error negotiating SSL connection on FD 138: error:14094418:SSL routines:ssl3_read_bytes:tlsv1 alert unknown ca (1/0)
2017/01/13 10:21:05 kid1| Error negotiating SSL connection on FD 191: error:14094416:SSL routines:ssl3_read_bytes:sslv3 alert certificate unknown (1/0)
2017/01/13 10:21:17 kid1| Error negotiating SSL connection on FD 194: error:14094416:SSL routines:ssl3_read_bytes:sslv3 alert certificate unknown (1/0)
2017/01/13 10:21:17 kid1| Error negotiating SSL connection on FD 198: error:14094416:SSL routines:ssl3_read_bytes:sslv3 alert certificate unknown (1/0)
2017/01/13 10:21:18 kid1| Error negotiating SSL connection on FD 194: error:14094416:SSL routines:ssl3_read_bytes:sslv3 alert certificate unknown (1/0)
2017/01/13 10:21:18 kid1| Error negotiating SSL connection on FD 194: error:14094416:SSL routines:ssl3_read_bytes:sslv3 alert certificate unknown (1/0)
2017/01/13 10:21:19 kid1| Error negotiating SSL connection on FD 194: error:14094416:SSL routines:ssl3_read_bytes:sslv3 alert certificate unknown (1/0)
2017/01/13 10:21:24 kid1| Error negotiating SSL connection on FD 163: Closed by client
2017/01/13 10:21:39 kid1| Error negotiating SSL connection on FD 250: error:14094418:SSL routines:ssl3_read_bytes:tlsv1 alert unknown ca (1/0)
2017/01/13 10:21:42 kid1| Error negotiating SSL on FD 298: error:14090086:SSL routines:ssl3_get_server_certificate:certificate verify failed (1/-1/0)
2017-01-13 10:21:53 [29866] Request(everyone/deny/-) https://accounts.youtube.com/accounts/CheckConnection?pmpo=https://accounts.google.com&v=-1574475776&timestamp=1484320896449 10.0.0.127/10.0.0.127 - GET REDIRECT
2017/01/13 10:21:56 kid1| Error negotiating SSL connection on FD 109: error:14094418:SSL routines:ssl3_read_bytes:tlsv1 alert unknown ca (1/0)
2017/01/13 10:21:56 kid1| Error negotiating SSL connection on FD 309: error:14094416:SSL routines:ssl3_read_bytes:sslv3 alert certificate unknown (1/0)
2017/01/13 10:22:25 kid1| Error negotiating SSL connection on FD 155: Closed by client




Thanks.

On Jan 12, 2017, at 7:28 PM, Eliezer Croitoru <eliezer at ngtech.co.il<mailto:eliezer at ngtech.co.il>> wrote:

Try removing:
cipher=ECDHE-RSA-RC4-SHA:ECDHE-RSA-AES128-SHA:DHE-RSA-AES128-SHA:DHE-RSA-CAMELLIA128-SHA:AES128-SHA:RC4-SHA:HIGH:!aNULL:!MD5:!ADH

From the ssl-bump line and see what happens.

----
http://ngtech.co.il/lmgtfy/
Linux System Administrator
Mobile: +972-5-28704261
Email: eliezer at ngtech.co.il


From: squid-users [mailto:squid-users-bounces at lists.squid-cache.org] On Behalf Of Sameh Onaissi
Sent: Thursday, January 12, 2017 11:10 PM
To: squid-users at lists.squid-cache.org
Subject: [squid-users] A bunch of SSL errors I am not sure why

System info:
Squid Cache: Version 3.5.22
Ubuntu linux 16.04


Hello,


Last couple of days I have started seeing SSL errors in my cache.log which I don?t really understand: http://pastebin.com/mDHVm7cQ

My SSL bump configs:

http_port 3127 intercept
http_port 3128
https_port 3129 intercept ssl-bump generate-host-certificates=on dynamic_cert_mem_cache_size=4MB cert=/etc/squid/ssl_certs/squid.crt key=/etc/squid/ssl_certs/squid.key cipher=ECDHE-RSA-RC4-SHA:ECDHE-RSA-AES128-SHA:DHE-RSA-AES128-SHA:DHE-RSA-CAMELLIA128-SHA:AES128-SHA:RC4-SHA:HIGH:!aNULL:!MD5:!ADH

acl step1 at_step SslBump1
acl step2 at_step SslBump2
acl step3 at_step SslBump3
ssl_bump peek step1 all
ssl_bump bump all




I have tried deleting /var/lib/ssh_db and recreating a fresh one, restarted squid, and no luck.


While the service still works fine, some websites like https://web.dlinkla.com/websys were showing a handshake error until I added the site IP into a bypass list.
The internet speed also drops every now and then due to this.


Any help is appreciated with these errors.


Thanks,
Sam






-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20170113/147c2d7a/attachment.htm>

From jester at optimera.us  Fri Jan 13 21:32:41 2017
From: jester at optimera.us (Jester Purtteman)
Date: Fri, 13 Jan 2017 13:32:41 -0800
Subject: [squid-users] Will squid core dump with worker threads?
	Investigating squid crash, 3.5.23
Message-ID: <000001d26de4$93742840$ba5c78c0$@optimera.us>

Hello,

 

I am having period crashes of my squid server, and I am not getting core
dump files.  I have set "workers 6" in my squid.conf, and I know that
threads can cause trouble from reading the debugging wiki page.  I have
confirmed permissions on the directory I'm dumping to, so I don't *think*
that is the issue.

 

FWIW (core dump to follow, I'll retry without workers and see what happens)
I am having squid crashes.  Details I have so far as are as follows:

 

I am running Squid-3.5.23-R14129 on a stock Ubuntu 16.04 configured with:

 

./configure --prefix=/usr   --localstatedir=/var
--libexecdir=/usr/lib/squid    --srcdir=.   --datadir=/usr/share/squid
--sysconfdir=/etc/squid   --with-default-user=proxy   --with-logdir=/var/log
--with-pidfile=/var/run/squid.pid --enable-linux-netfilter
--enable-cache-digests --enable-storeio=ufs,aufs,diskd,rock
--enable-async-io=30 --enable-http-violations --enable-zph-qos
--with-netfilter-conntrack --with-filedescriptors=65536 --with-large-files

 

About once a day it is crashing with the following line as about my only
lead in the cache.log:

 

assertion failed: MemBuf.cc:216: "0 <= tailSize && tailSize <= cSize"

 

>From the possibly interesting-but-who-knows-maybe-unrelated-files, there is
one additional detail.  I had this version running on a Ubuntu 14.04 machine
until last week, which I had installed GCC-4.9 on (so I could test squid
4.0), and that had ran stable from December 20th to January 5th without a
any crashes.  Then something totally outside of squid went horribly off the
rails.  Ubuntu dropped support for the 3.x series kernels, so I updated to
4.4 (from the Ubuntu repositories) and that caused /proc/sys/net/bridge to
go away.  While testing an unrelated issue, I ran a script that I adapted
from http://wiki.squid-cache.org/ConfigExamples/Intercept/LinuxBridge which
contains a dangerous couple lines I had not before contemplated:

 

cd /proc/sys/net/bridge/

for i in *

do

   echo 0 > $i

done

unset i

 

When /proc/sys/net/bridge went away, the change directory failed, then the
script proceeded to turn everything in that directory into 0's.  OOPS!  I
tell this bit so that my fellow admins get a laugh at my expense, and as a
cautionary tale.  CHECK the status of that command before you let it do
other things!  As it turns out, tproxy works fine without echoing '0' at all
those files, but if you want to leave it on the page, may I suggest the
following revision to the wiki page:

 

#!/bin/bash

cd /proc/sys/net/bridge

if [ $? -eq 0 ]

then

for i in *

do

  echo 0 > $i

done

unset i

else

echo "WARNING! /proc/sys/net/bridge does not exist, you can 'sudo modprobe
br_netfilter' to get it, but you may not need it"

fi

 

That just checks whether the changedir worked, and if it didn't it issues a
warning instead of cooking all the files in your current directory, which is
nice!

 

Anyway, after that happened, for reasons completely unknown, but I suspect
related to bridging, the machine that had been my squid server completely
seized, so I installed Ubuntu 16, and have since run into this crash.  Once
I have a core dump, I'll post it and my configuration, which is pretty stock
with the exception of a couple ACLs and "workers 6".  I am wondering if the
move from gcc 4.9 to gcc 5.4 (the stock gcc in Ubuntu 16) may be a culprit,
might recompile with the downgrade and see if the issue resolves.  

 

In any event, is there a way to get a core with worker threads?  My system
benefits from them, so I'd rather not turn them off but I want a core dump.
Also let me know if there are other details that would be useful.  Adding
much in the way of debugging is going to be a challenge because it takes a
day or so to get to a crash, and I don't necessarily have the disk space to
hold the volume of cache log generated over that period of time (I have a
60-gb log partition).  If there is some clever way of limiting cache.log and
causing it to round-robin or something, I'm happy to try things.  Thank you!

 

Jester Purtteman

 

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20170113/e8723072/attachment.htm>

From squid3 at treenet.co.nz  Sat Jan 14 04:50:43 2017
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sat, 14 Jan 2017 17:50:43 +1300
Subject: [squid-users] A bunch of SSL errors I am not sure why
In-Reply-To: <2403F106-B288-4780-BF4F-06AE58E1A52F@solcv.com>
References: <2EBAFFFF-752F-45BC-B06A-E5513CE7D7BA@solcv.com>
 <049c01d26d33$fe058520$fa108f60$@ngtech.co.il>
 <2403F106-B288-4780-BF4F-06AE58E1A52F@solcv.com>
Message-ID: <59772e12-aa11-d1c7-5543-13c98bb83419@treenet.co.nz>

On 14/01/2017 4:27 a.m., Sameh Onaissi wrote:
> Hello Eliezer, all,
> 
> 
> I removed the cipher and the problem is still there:
> 
> 
> 2017/01/13 10:20:50 kid1| Error negotiating SSL connection on FD 138: error:14094418:SSL routines:ssl3_read_bytes:tlsv1 alert unknown ca (1/0)

The CA used to sign the remote endpoints certificate is not trusted. Or
an intermediary certificate is missing.

* Check that the set of "global trusted CA" installed on your Squid
machiene is up to date.

* Try the latest Squid-4, which can auto-download intermediate certificates.


> 2017/01/13 10:21:05 kid1| Error negotiating SSL connection on FD 191: error:14094416:SSL routines:ssl3_read_bytes:sslv3 alert certificate unknown (1/0)
> 2017/01/13 10:21:17 kid1| Error negotiating SSL connection on FD 194: error:14094416:SSL routines:ssl3_read_bytes:sslv3 alert certificate unknown (1/0)
> 2017/01/13 10:21:17 kid1| Error negotiating SSL connection on FD 198: error:14094416:SSL routines:ssl3_read_bytes:sslv3 alert certificate unknown (1/0)
> 2017/01/13 10:21:18 kid1| Error negotiating SSL connection on FD 194: error:14094416:SSL routines:ssl3_read_bytes:sslv3 alert certificate unknown (1/0)
> 2017/01/13 10:21:18 kid1| Error negotiating SSL connection on FD 194: error:14094416:SSL routines:ssl3_read_bytes:sslv3 alert certificate unknown (1/0)
> 2017/01/13 10:21:19 kid1| Error negotiating SSL connection on FD 194: error:14094416:SSL routines:ssl3_read_bytes:sslv3 alert certificate unknown (1/0)

The obsolete SSL protocol is being used.


> 2017/01/13 10:21:24 kid1| Error negotiating SSL connection on FD 163: Closed by client

The client disconnected. You can do nothing about that.

> 2017/01/13 10:21:39 kid1| Error negotiating SSL connection on FD 250: error:14094418:SSL routines:ssl3_read_bytes:tlsv1 alert unknown ca (1/0)
> 2017/01/13 10:21:42 kid1| Error negotiating SSL on FD 298: error:14090086:SSL routines:ssl3_get_server_certificate:certificate verify failed (1/-1/0)

"certificate verify failed" says what it means.

> 2017-01-13 10:21:53 [29866] Request(everyone/deny/-) https://accounts.youtube.com/accounts/CheckConnection?pmpo=https://accounts.google.com&v=-1574475776&timestamp=1484320896449 10.0.0.127/10.0.0.127 - GET REDIRECT
> 2017/01/13 10:21:56 kid1| Error negotiating SSL connection on FD 109: error:14094418:SSL routines:ssl3_read_bytes:tlsv1 alert unknown ca (1/0)
> 2017/01/13 10:21:56 kid1| Error negotiating SSL connection on FD 309: error:14094416:SSL routines:ssl3_read_bytes:sslv3 alert certificate unknown (1/0)
> 2017/01/13 10:22:25 kid1| Error negotiating SSL connection on FD 155: Closed by client
> 

Amos



From squid3 at treenet.co.nz  Sat Jan 14 05:37:11 2017
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sat, 14 Jan 2017 18:37:11 +1300
Subject: [squid-users] Will squid core dump with worker threads?
 Investigating squid crash, 3.5.23
In-Reply-To: <000001d26de4$93742840$ba5c78c0$@optimera.us>
References: <000001d26de4$93742840$ba5c78c0$@optimera.us>
Message-ID: <f03d70e4-0c65-79f0-0a49-54cf14b19406@treenet.co.nz>

On 14/01/2017 10:32 a.m., Jester Purtteman wrote:
> Hello,
> 
>  
> 
> I am having period crashes of my squid server, and I am not getting core
> dump files.  I have set "workers 6" in my squid.conf, and I know that
> threads can cause trouble from reading the debugging wiki page.  I have
> confirmed permissions on the directory I'm dumping to, so I don't *think*
> that is the issue.

Core dumps are done by your OS when programs crash. You may need to turn
it on explicitly.
<http://wiki.squid-cache.org/SquidFaq/BugReporting#Resource_Limits>

> 
> FWIW (core dump to follow, I'll retry without workers and see what happens)
> I am having squid crashes.  Details I have so far as are as follows:
> 
>  
> 
> I am running Squid-3.5.23-R14129 on a stock Ubuntu 16.04 configured with:
> 
>  
> 
> ./configure --prefix=/usr   --localstatedir=/var
> --libexecdir=/usr/lib/squid    --srcdir=.   --datadir=/usr/share/squid
> --sysconfdir=/etc/squid   --with-default-user=proxy   --with-logdir=/var/log
> --with-pidfile=/var/run/squid.pid --enable-linux-netfilter
> --enable-cache-digests --enable-storeio=ufs,aufs,diskd,rock
> --enable-async-io=30 --enable-http-violations --enable-zph-qos
> --with-netfilter-conntrack --with-filedescriptors=65536 --with-large-files
> 
>  
> 
> About once a day it is crashing with the following line as about my only
> lead in the cache.log:
> 
>  
> 
> assertion failed: MemBuf.cc:216: "0 <= tailSize && tailSize <= cSize"
> 

This is <http://bugs.squid-cache.org/show_bug.cgi?id=4606>. We have
narrowed it down to something about the collapsed revalidation behaviour
that became visible after the recent security fix.

> 
> From the possibly interesting-but-who-knows-maybe-unrelated-files, there is
> one additional detail.  I had this version running on a Ubuntu 14.04 machine
> until last week, which I had installed GCC-4.9 on (so I could test squid
> 4.0), and that had ran stable from December 20th to January 5th without a
> any crashes.  Then something totally outside of squid went horribly off the
> rails.  Ubuntu dropped support for the 3.x series kernels, so I updated to
> 4.4 (from the Ubuntu repositories) and that caused /proc/sys/net/bridge to
> go away.  While testing an unrelated issue, I ran a script that I adapted
> from http://wiki.squid-cache.org/ConfigExamples/Intercept/LinuxBridge which
> contains a dangerous couple lines I had not before contemplated:
> 
> 
> cd /proc/sys/net/bridge/
> for i in *
> do
>    echo 0 > $i
> done
> unset i
> 
> When /proc/sys/net/bridge went away, the change directory failed, then the
> script proceeded to turn everything in that directory into 0's.  OOPS!  I
> tell this bit so that my fellow admins get a laugh at my expense, and as a
> cautionary tale.  CHECK the status of that command before you let it do
> other things!  As it turns out, tproxy works fine without echoing '0' at all
> those files, but if you want to leave it on the page, may I suggest the
> following revision to the wiki page:
> 

Thank you. There is no need for the cd or the * to be without a fixed
path. I have updated the wiki to prevent this.

It it true that TPROXY does not require bridging, and bridging has
nothing particularly requiring TPROXY. Except that for real transparency
they are usually both wanted.

>  
> 
> In any event, is there a way to get a core with worker threads?  My system
> benefits from them, so I'd rather not turn them off but I want a core dump.

<http://wiki.squid-cache.org/SquidFaq/BugReporting#Coredump_Location>
<http://wiki.squid-cache.org/SquidFaq/BugReporting#Resource_Limits>

workers are not threads. They are processes so each have their own PID
which the core file should be associated with. Also, only the individual
worker which crashed will have a core dump generated, then it should be
restarted automatically by the master process.


> Also let me know if there are other details that would be useful.  Adding
> much in the way of debugging is going to be a challenge because it takes a
> day or so to get to a crash, and I don't necessarily have the disk space to
> hold the volume of cache log generated over that period of time (I have a
> 60-gb log partition).  If there is some clever way of limiting cache.log and
> causing it to round-robin or something, I'm happy to try things.  Thank you!
> 

Using the rotate=N option on the debug_options directive will rotate
cache.log a different (usually smaller) number of times to access.log.

Or, if you use logrotate you can set it to rotate when the cache.log
gets to a certain size.
 see
<http://stackoverflow.com/questions/20162176/centos-linux-setting-logrotate-to-maximum-file-size-for-all-logs>

Or, you could also setup the cache.log to be a pipe to somewhere else
with more disk space.

Amos



From david at articatech.com  Sat Jan 14 11:55:49 2017
From: david at articatech.com (David Touzeau)
Date: Sat, 14 Jan 2017 12:55:49 +0100
Subject: [squid-users] 3.5.23: Retreive pairs in note acl
Message-ID: <001d01d26e5d$262ccd90$728668b0$@articatech.com>


Hi

I have created an external helper that return  OK a=note1

By adding tags in logs I see correctly that squid writes in log,
"a:%20note1"

But I cannot match this note in acls both test1 and test2 test3 not matches
the added tag

Acl test1 note a:note1
Acl test2 note a=note1
Acl test3 note:%20note1

http_access deny test1
http_access deny test2
http_access deny test3

What is the correct line to retrieve the correct note ?





From rousskov at measurement-factory.com  Sat Jan 14 16:25:50 2017
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Sat, 14 Jan 2017 09:25:50 -0700
Subject: [squid-users] 3.5.23: Retreive pairs in note acl
In-Reply-To: <001d01d26e5d$262ccd90$728668b0$@articatech.com>
References: <001d01d26e5d$262ccd90$728668b0$@articatech.com>
Message-ID: <1c33e73a-b8cf-2d4e-40b2-6cfd3e60631f@measurement-factory.com>

On 01/14/2017 04:55 AM, David Touzeau wrote:

> I have created an external helper that return  OK a=note1
> 
> What is the correct line to retrieve the correct note ?

  acl annotatedWithANote1 note a note1
  http_access deny annotatedWithANote1

Alex.

> acl aclname note [-m[=delimiters]] name [value ...]
>   # match transaction annotation [fast]
>   # Without values, matches any annotation with a given name.
>   # With value(s), matches any annotation with a given name that
>   # also has one of the given values.



From eliezer at ngtech.co.il  Sat Jan 14 17:07:13 2017
From: eliezer at ngtech.co.il (Eliezer  Croitoru)
Date: Sat, 14 Jan 2017 19:07:13 +0200
Subject: [squid-users] A bunch of SSL errors I am not sure why
In-Reply-To: <59772e12-aa11-d1c7-5543-13c98bb83419@treenet.co.nz>
References: <2EBAFFFF-752F-45BC-B06A-E5513CE7D7BA@solcv.com>
 <049c01d26d33$fe058520$fa108f60$@ngtech.co.il>
 <2403F106-B288-4780-BF4F-06AE58E1A52F@solcv.com>
 <59772e12-aa11-d1c7-5543-13c98bb83419@treenet.co.nz>
Message-ID: <04fd01d26e88$a68b7290$f3a257b0$@ngtech.co.il>

I have not experienced this issue on my testing lab when accessing:
https://web.dlinkla.com/websys

$ squid -v
Squid Cache: Version 3.5.23
Service Name: squid
configure options:  '--build=x86_64-redhat-linux-gnu' '--host=x86_64-redhat-linux-gnu' '--program-prefix=' '--prefix=/usr' '--exec-prefix=/usr' '--bindir=/usr/bin' '--sbindir=/usr/sbin' '--sysconfdir=/etc' '--datadir=/usr/share' '--includedir=/usr/include' '--libdir=/usr/lib64' '--libexecdir=/usr/libexec' '--sharedstatedir=/var/lib' '--mandir=/usr/share/man' '--infodir=/usr/share/info' '--verbose' '--exec_prefix=/usr' '--libexecdir=/usr/lib64/squid' '--localstatedir=/var' '--datadir=/usr/share/squid' '--sysconfdir=/etc/squid' '--with-logdir=$(localstatedir)/log/squid' '--with-pidfile=$(localstatedir)/run/squid.pid' '--disable-dependency-tracking' '--enable-follow-x-forwarded-for' '--enable-auth' '--enable-auth-basic=DB,LDAP,NCSA,NIS,PAM,POP3,RADIUS,SASL,SMB,getpwnam,fake' '--enable-auth-ntlm=smb_lm,fake' '--enable-auth-digest=file,LDAP,eDirectory' '--enable-auth-negotiate=kerberos,wrapper' '--enable-external-acl-helpers=wbinfo_group,kerberos_ldap_group,LDAP_group,delayer,file_userip,SQL_session,unix_group,session,time_quota' '--enable-cache-digests' '--enable-cachemgr-hostname=localhost' '--enable-delay-pools' '--enable-epoll' '--enable-icap-client' '--enable-ident-lookups' '--enable-linux-netfilter' '--enable-removal-policies=heap,lru' '--enable-snmp' '--enable-storeio=aufs,diskd,ufs,rock' '--enable-wccpv2' '--enable-esi' '--enable-ssl-crtd' '--enable-icmp' '--with-aio' '--with-default-user=squid' '--with-filedescriptors=16384' '--with-dl' '--with-openssl' '--with-pthreads' '--with-included-ltdl' '--disable-arch-native' '--enable-ecap' '--without-nettle' 'build_alias=x86_64-redhat-linux-gnu' 'host_alias=x86_64-redhat-linux-gnu' 'CFLAGS=-O2 -g -pipe -Wall -Wp,-D_FORTIFY_SOURCE=2 -fexceptions -fstack-protector-strong --param=ssp-buffer-size=4 -grecord-gcc-switches   -m64 -mtune=generic' 'LDFLAGS=-Wl,-z,relro ' 'CXXFLAGS=-O2 -g -pipe -Wall -Wp,-D_FORTIFY_SOURCE=2 -fexceptions -fstack-protector-strong --param=ssp-buffer-size=4 -grecord-gcc-switches   -m64 -mtune=generic -fPIC' 'PKG_CONFIG_PATH=:/usr/lib64/pkgconfig:/usr/share/pkgconfig' --enable-ltdl-convenience

When the proxy is defined in the browser.
Can you verify if it affects only intercepted connections or also non-intercepted ones?

Thanks,
Eliezer

----
Eliezer Croitoru
Linux System Administrator
Mobile: +972-5-28704261
Email: eliezer at ngtech.co.il


-----Original Message-----
From: squid-users [mailto:squid-users-bounces at lists.squid-cache.org] On Behalf Of Amos Jeffries
Sent: Saturday, January 14, 2017 6:51 AM
To: squid-users at lists.squid-cache.org
Subject: Re: [squid-users] A bunch of SSL errors I am not sure why

On 14/01/2017 4:27 a.m., Sameh Onaissi wrote:
> Hello Eliezer, all,
> 
> 
> I removed the cipher and the problem is still there:
> 
> 
> 2017/01/13 10:20:50 kid1| Error negotiating SSL connection on FD 138: 
> error:14094418:SSL routines:ssl3_read_bytes:tlsv1 alert unknown ca 
> (1/0)

The CA used to sign the remote endpoints certificate is not trusted. Or an intermediary certificate is missing.

* Check that the set of "global trusted CA" installed on your Squid machiene is up to date.

* Try the latest Squid-4, which can auto-download intermediate certificates.


> 2017/01/13 10:21:05 kid1| Error negotiating SSL connection on FD 191: 
> error:14094416:SSL routines:ssl3_read_bytes:sslv3 alert certificate 
> unknown (1/0)
> 2017/01/13 10:21:17 kid1| Error negotiating SSL connection on FD 194: 
> error:14094416:SSL routines:ssl3_read_bytes:sslv3 alert certificate 
> unknown (1/0)
> 2017/01/13 10:21:17 kid1| Error negotiating SSL connection on FD 198: 
> error:14094416:SSL routines:ssl3_read_bytes:sslv3 alert certificate 
> unknown (1/0)
> 2017/01/13 10:21:18 kid1| Error negotiating SSL connection on FD 194: 
> error:14094416:SSL routines:ssl3_read_bytes:sslv3 alert certificate 
> unknown (1/0)
> 2017/01/13 10:21:18 kid1| Error negotiating SSL connection on FD 194: 
> error:14094416:SSL routines:ssl3_read_bytes:sslv3 alert certificate 
> unknown (1/0)
> 2017/01/13 10:21:19 kid1| Error negotiating SSL connection on FD 194: 
> error:14094416:SSL routines:ssl3_read_bytes:sslv3 alert certificate 
> unknown (1/0)

The obsolete SSL protocol is being used.


> 2017/01/13 10:21:24 kid1| Error negotiating SSL connection on FD 163: 
> Closed by client

The client disconnected. You can do nothing about that.

> 2017/01/13 10:21:39 kid1| Error negotiating SSL connection on FD 250: 
> error:14094418:SSL routines:ssl3_read_bytes:tlsv1 alert unknown ca 
> (1/0)
> 2017/01/13 10:21:42 kid1| Error negotiating SSL on FD 298: 
> error:14090086:SSL routines:ssl3_get_server_certificate:certificate 
> verify failed (1/-1/0)

"certificate verify failed" says what it means.

> 2017-01-13 10:21:53 [29866] Request(everyone/deny/-) 
> https://accounts.youtube.com/accounts/CheckConnection?pmpo=https://acc
> ounts.google.com&v=-1574475776&timestamp=1484320896449 
> 10.0.0.127/10.0.0.127 - GET REDIRECT
> 2017/01/13 10:21:56 kid1| Error negotiating SSL connection on FD 109: 
> error:14094418:SSL routines:ssl3_read_bytes:tlsv1 alert unknown ca 
> (1/0)
> 2017/01/13 10:21:56 kid1| Error negotiating SSL connection on FD 309: 
> error:14094416:SSL routines:ssl3_read_bytes:sslv3 alert certificate 
> unknown (1/0)
> 2017/01/13 10:22:25 kid1| Error negotiating SSL connection on FD 155: 
> Closed by client
> 

Amos

_______________________________________________
squid-users mailing list
squid-users at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-users



From david at articatech.com  Sat Jan 14 21:22:13 2017
From: david at articatech.com (David Touzeau)
Date: Sat, 14 Jan 2017 22:22:13 +0100
Subject: [squid-users] 3.5.23: Retreive pairs in note acl
In-Reply-To: <1c33e73a-b8cf-2d4e-40b2-6cfd3e60631f@measurement-factory.com>
References: <001d01d26e5d$262ccd90$728668b0$@articatech.com>
 <1c33e73a-b8cf-2d4e-40b2-6cfd3e60631f@measurement-factory.com>
Message-ID: <001401d26eac$464ff080$d2efd180$@articatech.com>




> I have created an external helper that return  OK a=note1
> 
> What is the correct line to retrieve the correct note ?

  acl annotatedWithANote1 note a note1
  http_access deny annotatedWithANote1

Alex.

> acl aclname note [-m[=delimiters]] name [value ...]
>   # match transaction annotation [fast]
>   # Without values, matches any annotation with a given name.
>   # With value(s), matches any annotation with a given name that
>   # also has one of the given values.


Thanks, works like a charm

Helper answer OK mykey=note1
Acl mynote note mykey note1
http_access deny mynote




From sameh.onaissi at solcv.com  Sun Jan 15 01:57:01 2017
From: sameh.onaissi at solcv.com (Sameh Onaissi)
Date: Sun, 15 Jan 2017 01:57:01 +0000
Subject: [squid-users] A bunch of SSL errors I am not sure why
In-Reply-To: <04fd01d26e88$a68b7290$f3a257b0$@ngtech.co.il>
References: <2EBAFFFF-752F-45BC-B06A-E5513CE7D7BA@solcv.com>
 <049c01d26d33$fe058520$fa108f60$@ngtech.co.il>
 <2403F106-B288-4780-BF4F-06AE58E1A52F@solcv.com>
 <59772e12-aa11-d1c7-5543-13c98bb83419@treenet.co.nz>
 <04fd01d26e88$a68b7290$f3a257b0$@ngtech.co.il>
Message-ID: <8CC18AE2-E097-4E61-B1CC-FDA9E4EF7A22@solcv.com>

Hello,

Bypassed is non-intercepted right? The site worked fine when it was added to the bypass list.
Other .gov.co<http://gov.co> sites have the same issue, and I have to add them to bypass list for clients to be able to access them.

Here?s the error page before adding to the bypass list:
https://imagebin.ca/v/38uz8akvWayM

My squid info:
squid -v
Squid Cache: Version 3.5.22
Service Name: squid
Ubuntu linux
configure options:  '--build=x86_64-linux-gnu' '--prefix=/usr' '--includedir=${prefix}/include' '--mandir=${prefix}/share/man' '--infodir=${prefix}/share/info' '--sysconfdir=/etc' '--localstatedir=/var' '--libexecdir=${prefix}/lib/squid3' '--srcdir=.' '--disable-maintainer-mode' '--disable-dependency-tracking' '--disable-silent-rules' 'BUILDCXXFLAGS=-g -O2 -fPIE -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -Wl,-Bsymbolic-functions -fPIE -pie -Wl,-z,relro -Wl,-z,now -Wl,--as-needed' '--datadir=/usr/share/squid' '--sysconfdir=/etc/squid' '--libexecdir=/usr/lib/squid' '--mandir=/usr/share/man' '--enable-inline' '--disable-arch-native' '--enable-async-io=8' '--enable-storeio=ufs,aufs,diskd,rock' '--enable-removal-policies=lru,heap' '--enable-delay-pools' '--enable-cache-digests' '--enable-icap-client' '--enable-follow-x-forwarded-for' '--enable-auth-basic=DB,fake,getpwnam,LDAP,NCSA,NIS,PAM,POP3,RADIUS,SASL,SMB' '--enable-auth-digest=file,LDAP' '--enable-auth-negotiate=kerberos,wrapper' '--enable-auth-ntlm=fake,smb_lm' '--enable-external-acl-helpers=file_userip,kerberos_ldap_group,LDAP_group,session,SQL_session,time_quota,unix_group,wbinfo_group' '--enable-url-rewrite-helpers=fake' '--enable-eui' '--enable-esi' '--enable-icmp' '--enable-zph-qos' '--enable-ecap' '--disable-translation' '--with-swapdir=/var/spool/squid' '--with-logdir=/var/log/squid' '--with-pidfile=/var/run/squid.pid' '--with-filedescriptors=65536' '--with-large-files' '--with-default-user=proxy' '--with-openssl' '--enable-ssl' '--enable-ssl-crtd' '--enable-build-info=Ubuntu linux' '--enable-linux-netfilter' 'build_alias=x86_64-linux-gnu' 'CFLAGS=-g -O2 -fPIE -fstack-protector-strong -Wformat -Werror=format-security -Wall' 'LDFLAGS=-Wl,-Bsymbolic-functions -fPIE -pie -Wl,-z,relro -Wl,-z,now -Wl,--as-needed' 'CPPFLAGS=-Wdate-time -D_FORTIFY_SOURCE=2' 'CXXFLAGS=-g -O2 -fPIE -fstack-protector-strong -Wformat -Werror=format-security'


@Amos:


* Check that the set of "global trusted CA" installed on your Squid machiene is up to date.

How can I do that? I recreated the Certs DB and no luck.


* Try the latest Squid-4, which can auto-download intermediate certificates.

Is squid 4 stable for production?


Thanks,
[cid:DA9A1E3F-3876-4EF2-BBA2-D3942A06ACE1 at routerb408e2.com]

Sameh Onaissi
Sol Cable Visi?n
Cel: 316-3023424
Email: sameh.onaissi at solcv.com<mailto:sameh.onaissi at solcv.com>



[cid:2FD1C3AB-E45C-49F0-84AB-0F8AC658BD11 at routerb408e2.com]Piensa en el medio ambiente antes de imprimir este email.

On Jan 14, 2017, at 12:07 PM, Eliezer Croitoru <eliezer at ngtech.co.il<mailto:eliezer at ngtech.co.il>> wrote:

I have not experienced this issue on my testing lab when accessing:
https://web.dlinkla.com/websys

$ squid -v
Squid Cache: Version 3.5.23
Service Name: squid
configure options:  '--build=x86_64-redhat-linux-gnu' '--host=x86_64-redhat-linux-gnu' '--program-prefix=' '--prefix=/usr' '--exec-prefix=/usr' '--bindir=/usr/bin' '--sbindir=/usr/sbin' '--sysconfdir=/etc' '--datadir=/usr/share' '--includedir=/usr/include' '--libdir=/usr/lib64' '--libexecdir=/usr/libexec' '--sharedstatedir=/var/lib' '--mandir=/usr/share/man' '--infodir=/usr/share/info' '--verbose' '--exec_prefix=/usr' '--libexecdir=/usr/lib64/squid' '--localstatedir=/var' '--datadir=/usr/share/squid' '--sysconfdir=/etc/squid' '--with-logdir=$(localstatedir)/log/squid' '--with-pidfile=$(localstatedir)/run/squid.pid' '--disable-dependency-tracking' '--enable-follow-x-forwarded-for' '--enable-auth' '--enable-auth-basic=DB,LDAP,NCSA,NIS,PAM,POP3,RADIUS,SASL,SMB,getpwnam,fake' '--enable-auth-ntlm=smb_lm,fake' '--enable-auth-digest=file,LDAP,eDirectory' '--enable-auth-negotiate=kerberos,wrapper' '--enable-external-acl-helpers=wbinfo_group,kerberos_ldap_group,LDAP_group,delayer,file_userip,SQL_session,unix_group,session,time_quota' '--enable-cache-digests' '--enable-cachemgr-hostname=localhost' '--enable-delay-pools' '--enable-epoll' '--enable-icap-client' '--enable-ident-lookups' '--enable-linux-netfilter' '--enable-removal-policies=heap,lru' '--enable-snmp' '--enable-storeio=aufs,diskd,ufs,rock' '--enable-wccpv2' '--enable-esi' '--enable-ssl-crtd' '--enable-icmp' '--with-aio' '--with-default-user=squid' '--with-filedescriptors=16384' '--with-dl' '--with-openssl' '--with-pthreads' '--with-included-ltdl' '--disable-arch-native' '--enable-ecap' '--without-nettle' 'build_alias=x86_64-redhat-linux-gnu' 'host_alias=x86_64-redhat-linux-gnu' 'CFLAGS=-O2 -g -pipe -Wall -Wp,-D_FORTIFY_SOURCE=2 -fexceptions -fstack-protector-strong --param=ssp-buffer-size=4 -grecord-gcc-switches   -m64 -mtune=generic' 'LDFLAGS=-Wl,-z,relro ' 'CXXFLAGS=-O2 -g -pipe -Wall -Wp,-D_FORTIFY_SOURCE=2 -fexceptions -fstack-protector-strong --param=ssp-buffer-size=4 -grecord-gcc-switches   -m64 -mtune=generic -fPIC' 'PKG_CONFIG_PATH=:/usr/lib64/pkgconfig:/usr/share/pkgconfig' --enable-ltdl-convenience

When the proxy is defined in the browser.
Can you verify if it affects only intercepted connections or also non-intercepted ones?

Thanks,
Eliezer

----
Eliezer Croitoru
Linux System Administrator
Mobile: +972-5-28704261
Email: eliezer at ngtech.co.il


-----Original Message-----
From: squid-users [mailto:squid-users-bounces at lists.squid-cache.org] On Behalf Of Amos Jeffries
Sent: Saturday, January 14, 2017 6:51 AM
To: squid-users at lists.squid-cache.org
Subject: Re: [squid-users] A bunch of SSL errors I am not sure why

On 14/01/2017 4:27 a.m., Sameh Onaissi wrote:
Hello Eliezer, all,


I removed the cipher and the problem is still there:


2017/01/13 10:20:50 kid1| Error negotiating SSL connection on FD 138:
error:14094418:SSL routines:ssl3_read_bytes:tlsv1 alert unknown ca
(1/0)

The CA used to sign the remote endpoints certificate is not trusted. Or an intermediary certificate is missing.

* Check that the set of "global trusted CA" installed on your Squid machiene is up to date.

* Try the latest Squid-4, which can auto-download intermediate certificates.


2017/01/13 10:21:05 kid1| Error negotiating SSL connection on FD 191:
error:14094416:SSL routines:ssl3_read_bytes:sslv3 alert certificate
unknown (1/0)
2017/01/13 10:21:17 kid1| Error negotiating SSL connection on FD 194:
error:14094416:SSL routines:ssl3_read_bytes:sslv3 alert certificate
unknown (1/0)
2017/01/13 10:21:17 kid1| Error negotiating SSL connection on FD 198:
error:14094416:SSL routines:ssl3_read_bytes:sslv3 alert certificate
unknown (1/0)
2017/01/13 10:21:18 kid1| Error negotiating SSL connection on FD 194:
error:14094416:SSL routines:ssl3_read_bytes:sslv3 alert certificate
unknown (1/0)
2017/01/13 10:21:18 kid1| Error negotiating SSL connection on FD 194:
error:14094416:SSL routines:ssl3_read_bytes:sslv3 alert certificate
unknown (1/0)
2017/01/13 10:21:19 kid1| Error negotiating SSL connection on FD 194:
error:14094416:SSL routines:ssl3_read_bytes:sslv3 alert certificate
unknown (1/0)

The obsolete SSL protocol is being used.


2017/01/13 10:21:24 kid1| Error negotiating SSL connection on FD 163:
Closed by client

The client disconnected. You can do nothing about that.

2017/01/13 10:21:39 kid1| Error negotiating SSL connection on FD 250:
error:14094418:SSL routines:ssl3_read_bytes:tlsv1 alert unknown ca
(1/0)
2017/01/13 10:21:42 kid1| Error negotiating SSL on FD 298:
error:14090086:SSL routines:ssl3_get_server_certificate:certificate
verify failed (1/-1/0)

"certificate verify failed" says what it means.

2017-01-13 10:21:53 [29866] Request(everyone/deny/-)
https://accounts.youtube.com/accounts/CheckConnection?pmpo=https://acc
ounts.google.com&v=-1574475776&timestamp=1484320896449
10.0.0.127/10.0.0.127 - GET REDIRECT
2017/01/13 10:21:56 kid1| Error negotiating SSL connection on FD 109:
error:14094418:SSL routines:ssl3_read_bytes:tlsv1 alert unknown ca
(1/0)
2017/01/13 10:21:56 kid1| Error negotiating SSL connection on FD 309:
error:14094416:SSL routines:ssl3_read_bytes:sslv3 alert certificate
unknown (1/0)
2017/01/13 10:22:25 kid1| Error negotiating SSL connection on FD 155:
Closed by client


Amos

_______________________________________________
squid-users mailing list
squid-users at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-users

_______________________________________________
squid-users mailing list
squid-users at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-users

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20170115/5c4991d6/attachment.htm>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: 58.png
Type: image/png
Size: 7419 bytes
Desc: 58.png
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20170115/5c4991d6/attachment.png>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: Image 5-5-16 at 11.48 AM.jpg
Type: image/jpeg
Size: 4083 bytes
Desc: Image 5-5-16 at 11.48 AM.jpg
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20170115/5c4991d6/attachment.jpg>

From squid3 at treenet.co.nz  Sun Jan 15 07:14:55 2017
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sun, 15 Jan 2017 20:14:55 +1300
Subject: [squid-users] A bunch of SSL errors I am not sure why
In-Reply-To: <07335B2B-5956-42F8-A2DD-3CFA7E6E3158@solcv.com>
References: <2EBAFFFF-752F-45BC-B06A-E5513CE7D7BA@solcv.com>
 <049c01d26d33$fe058520$fa108f60$@ngtech.co.il>
 <2403F106-B288-4780-BF4F-06AE58E1A52F@solcv.com>
 <59772e12-aa11-d1c7-5543-13c98bb83419@treenet.co.nz>
 <04fd01d26e88$a68b7290$f3a257b0$@ngtech.co.il>
 <07335B2B-5956-42F8-A2DD-3CFA7E6E3158@solcv.com>
Message-ID: <f170cd65-c39b-09df-1d91-b8853d069c5e@treenet.co.nz>

On 15/01/2017 2:25 p.m., Sameh Onaissi wrote:
> Hello,
> 
> I assume bypassed are non intercepted?

That depends on whether the bypass is bypassing interception or
something else.


> Once the site IP is on the bypass list, it opened without an issue.
There are a few other .gov.co<http://gov.co> sites who have the same
problem too.
> 
> Attached is a screenshot of the error before I added the site to the bypass list.
> 

If you actually read that error message it tells you exactly what the
problem is.

 "Handshake with SSL server failed: [blah blah codes]: dh key too small"

The server is trying to use a Diffi-Helman cipher with a too-short key.
DH cipher with short keys has recently been broken. By recently I mean
about a whole year ago.

Amos



From eliezer at ngtech.co.il  Sun Jan 15 08:59:27 2017
From: eliezer at ngtech.co.il (Eliezer  Croitoru)
Date: Sun, 15 Jan 2017 10:59:27 +0200
Subject: [squid-users] A bunch of SSL errors I am not sure why
In-Reply-To: <07335B2B-5956-42F8-A2DD-3CFA7E6E3158@solcv.com>
References: <2EBAFFFF-752F-45BC-B06A-E5513CE7D7BA@solcv.com>
 <049c01d26d33$fe058520$fa108f60$@ngtech.co.il>
 <2403F106-B288-4780-BF4F-06AE58E1A52F@solcv.com>
 <59772e12-aa11-d1c7-5543-13c98bb83419@treenet.co.nz>
 <04fd01d26e88$a68b7290$f3a257b0$@ngtech.co.il>
 <07335B2B-5956-42F8-A2DD-3CFA7E6E3158@solcv.com>
Message-ID: <051401d26f0d$ad266bc0$07734340$@ngtech.co.il>

Non intercepted is not bypassed?
Squid has coupe options for the ?http_port? option.
One that you are using is intercept and the other is without intercept.
What happens when you try to connect to this website when you are defining another port without ?Intercept?  and define the proxy in the browser settings?
Let me know if something is missing in the picture.

Eliezer

----
http://ngtech.co.il/lmgtfy/
Linux System Administrator
Mobile: +972-5-28704261
Email: eliezer at ngtech.co.il


From: Sameh Onaissi [mailto:sameh.onaissi at solcv.com] 
Sent: Sunday, January 15, 2017 3:25 AM
To: Eliezer Croitoru <eliezer at ngtech.co.il>
Cc: Amos Jeffries <squid3 at treenet.co.nz>; squid-users at lists.squid-cache.org
Subject: Re: [squid-users] A bunch of SSL errors I am not sure why

Hello, 

I assume bypassed are non intercepted? Once the site IP is on the bypass list, it opened without an issue. There are a few other .http://gov.co sites who have the same problem too. 

Attached is a screenshot of the error before I added the site to the bypass list.

squid -v
Squid Cache: Version 3.5.22
Service Name: squid
Ubuntu linux
configure options:  '--build=x86_64-linux-gnu' '--prefix=/usr' '--includedir=${prefix}/include' '--mandir=${prefix}/share/man' '--infodir=${prefix}/share/info' '--sysconfdir=/etc' '--localstatedir=/var' '--libexecdir=${prefix}/lib/squid3' '--srcdir=.' '--disable-maintainer-mode' '--disable-dependency-tracking' '--disable-silent-rules' 'BUILDCXXFLAGS=-g -O2 -fPIE -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -Wl,-Bsymbolic-functions -fPIE -pie -Wl,-z,relro -Wl,-z,now -Wl,--as-needed' '--datadir=/usr/share/squid' '--sysconfdir=/etc/squid' '--libexecdir=/usr/lib/squid' '--mandir=/usr/share/man' '--enable-inline' '--disable-arch-native' '--enable-async-io=8' '--enable-storeio=ufs,aufs,diskd,rock' '--enable-removal-policies=lru,heap' '--enable-delay-pools' '--enable-cache-digests' '--enable-icap-client' '--enable-follow-x-forwarded-for' '--enable-auth-basic=DB,fake,getpwnam,LDAP,NCSA,NIS,PAM,POP3,RADIUS,SASL,SMB' '--enable-auth-digest=file,LDAP' '--enable-auth-negotiate=kerberos,wrapper' '--enable-auth-ntlm=fake,smb_lm' '--enable-external-acl-helpers=file_userip,kerberos_ldap_group,LDAP_group,session,SQL_session,time_quota,unix_group,wbinfo_group' '--enable-url-rewrite-helpers=fake' '--enable-eui' '--enable-esi' '--enable-icmp' '--enable-zph-qos' '--enable-ecap' '--disable-translation' '--with-swapdir=/var/spool/squid' '--with-logdir=/var/log/squid' '--with-pidfile=/var/run/squid.pid' '--with-filedescriptors=65536' '--with-large-files' '--with-default-user=proxy' '--with-openssl' '--enable-ssl' '--enable-ssl-crtd' '--enable-build-info=Ubuntu linux' '--enable-linux-netfilter' 'build_alias=x86_64-linux-gnu' 'CFLAGS=-g -O2 -fPIE -fstack-protector-strong -Wformat -Werror=format-security -Wall' 'LDFLAGS=-Wl,-Bsymbolic-functions -fPIE -pie -Wl,-z,relro -Wl,-z,now -Wl,--as-needed' 'CPPFLAGS=-Wdate-time -D_FORTIFY_SOURCE=2' 'CXXFLAGS=-g -O2 -fPIE -fstack-protector-strong -Wformat -Werror=format-security'





@ Amos:

"* Check that the set of "global trusted CA" installed on your Squid machiene is up to date.? 
I recreated the set recently.


* Try the latest Squid-4, which can auto-download intermediate certificates.

Is squid-4 stable for production?

Thank you,



Sameh Onaissi
Sol Cable Visi?n
Cel: 316-3023424
Email: mailto:sameh.onaissi at solcv.com



Piensa en el medio ambiente antes de imprimir este email. 

On Jan 14, 2017, at 12:07 PM, Eliezer Croitoru <mailto:eliezer at ngtech.co.il> wrote:

I have not experienced this issue on my testing lab when accessing:
https://web.dlinkla.com/websys

$ squid -v
Squid Cache: Version 3.5.23
Service Name: squid
configure options:  '--build=x86_64-redhat-linux-gnu' '--host=x86_64-redhat-linux-gnu' '--program-prefix=' '--prefix=/usr' '--exec-prefix=/usr' '--bindir=/usr/bin' '--sbindir=/usr/sbin' '--sysconfdir=/etc' '--datadir=/usr/share' '--includedir=/usr/include' '--libdir=/usr/lib64' '--libexecdir=/usr/libexec' '--sharedstatedir=/var/lib' '--mandir=/usr/share/man' '--infodir=/usr/share/info' '--verbose' '--exec_prefix=/usr' '--libexecdir=/usr/lib64/squid' '--localstatedir=/var' '--datadir=/usr/share/squid' '--sysconfdir=/etc/squid' '--with-logdir=$(localstatedir)/log/squid' '--with-pidfile=$(localstatedir)/run/squid.pid' '--disable-dependency-tracking' '--enable-follow-x-forwarded-for' '--enable-auth' '--enable-auth-basic=DB,LDAP,NCSA,NIS,PAM,POP3,RADIUS,SASL,SMB,getpwnam,fake' '--enable-auth-ntlm=smb_lm,fake' '--enable-auth-digest=file,LDAP,eDirectory' '--enable-auth-negotiate=kerberos,wrapper' '--enable-external-acl-helpers=wbinfo_group,kerberos_ldap_group,LDAP_group,delayer,file_userip,SQL_session,unix_group,session,time_quota' '--enable-cache-digests' '--enable-cachemgr-hostname=localhost' '--enable-delay-pools' '--enable-epoll' '--enable-icap-client' '--enable-ident-lookups' '--enable-linux-netfilter' '--enable-removal-policies=heap,lru' '--enable-snmp' '--enable-storeio=aufs,diskd,ufs,rock' '--enable-wccpv2' '--enable-esi' '--enable-ssl-crtd' '--enable-icmp' '--with-aio' '--with-default-user=squid' '--with-filedescriptors=16384' '--with-dl' '--with-openssl' '--with-pthreads' '--with-included-ltdl' '--disable-arch-native' '--enable-ecap' '--without-nettle' 'build_alias=x86_64-redhat-linux-gnu' 'host_alias=x86_64-redhat-linux-gnu' 'CFLAGS=-O2 -g -pipe -Wall -Wp,-D_FORTIFY_SOURCE=2 -fexceptions -fstack-protector-strong --param=ssp-buffer-size=4 -grecord-gcc-switches   -m64 -mtune=generic' 'LDFLAGS=-Wl,-z,relro ' 'CXXFLAGS=-O2 -g -pipe -Wall -Wp,-D_FORTIFY_SOURCE=2 -fexceptions -fstack-protector-strong --param=ssp-buffer-size=4 -grecord-gcc-switches   -m64 -mtune=generic -fPIC' 'PKG_CONFIG_PATH=:/usr/lib64/pkgconfig:/usr/share/pkgconfig' --enable-ltdl-convenience

When the proxy is defined in the browser.
Can you verify if it affects only intercepted connections or also non-intercepted ones?

Thanks,
Eliezer

----
Eliezer Croitoru
Linux System Administrator
Mobile: +972-5-28704261
Email: mailto:eliezer at ngtech.co.il


-----Original Message-----
From: squid-users [mailto:squid-users-bounces at lists.squid-cache.org] On Behalf Of Amos Jeffries
Sent: Saturday, January 14, 2017 6:51 AM
To: mailto:squid-users at lists.squid-cache.org
Subject: Re: [squid-users] A bunch of SSL errors I am not sure why

On 14/01/2017 4:27 a.m., Sameh Onaissi wrote:

Hello Eliezer, all,


I removed the cipher and the problem is still there:


2017/01/13 10:20:50 kid1| Error negotiating SSL connection on FD 138: 
error:14094418:SSL routines:ssl3_read_bytes:tlsv1 alert unknown ca 
(1/0)

The CA used to sign the remote endpoints certificate is not trusted. Or an intermediary certificate is missing.

* Check that the set of "global trusted CA" installed on your Squid machiene is up to date.

* Try the latest Squid-4, which can auto-download intermediate certificates.



2017/01/13 10:21:05 kid1| Error negotiating SSL connection on FD 191: 
error:14094416:SSL routines:ssl3_read_bytes:sslv3 alert certificate 
unknown (1/0)
2017/01/13 10:21:17 kid1| Error negotiating SSL connection on FD 194: 
error:14094416:SSL routines:ssl3_read_bytes:sslv3 alert certificate 
unknown (1/0)
2017/01/13 10:21:17 kid1| Error negotiating SSL connection on FD 198: 
error:14094416:SSL routines:ssl3_read_bytes:sslv3 alert certificate 
unknown (1/0)
2017/01/13 10:21:18 kid1| Error negotiating SSL connection on FD 194: 
error:14094416:SSL routines:ssl3_read_bytes:sslv3 alert certificate 
unknown (1/0)
2017/01/13 10:21:18 kid1| Error negotiating SSL connection on FD 194: 
error:14094416:SSL routines:ssl3_read_bytes:sslv3 alert certificate 
unknown (1/0)
2017/01/13 10:21:19 kid1| Error negotiating SSL connection on FD 194: 
error:14094416:SSL routines:ssl3_read_bytes:sslv3 alert certificate 
unknown (1/0)

The obsolete SSL protocol is being used.



2017/01/13 10:21:24 kid1| Error negotiating SSL connection on FD 163: 
Closed by client

The client disconnected. You can do nothing about that.


2017/01/13 10:21:39 kid1| Error negotiating SSL connection on FD 250: 
error:14094418:SSL routines:ssl3_read_bytes:tlsv1 alert unknown ca 
(1/0)
2017/01/13 10:21:42 kid1| Error negotiating SSL on FD 298: 
error:14090086:SSL routines:ssl3_get_server_certificate:certificate 
verify failed (1/-1/0)

"certificate verify failed" says what it means.


2017-01-13 10:21:53 [29866] Request(everyone/deny/-) 
https://accounts.youtube.com/accounts/CheckConnection?pmpo=https://acc
ounts.google.com&v=-1574475776&timestamp=1484320896449 
10.0.0.127/10.0.0.127 - GET REDIRECT
2017/01/13 10:21:56 kid1| Error negotiating SSL connection on FD 109: 
error:14094418:SSL routines:ssl3_read_bytes:tlsv1 alert unknown ca 
(1/0)
2017/01/13 10:21:56 kid1| Error negotiating SSL connection on FD 309: 
error:14094416:SSL routines:ssl3_read_bytes:sslv3 alert certificate 
unknown (1/0)
2017/01/13 10:22:25 kid1| Error negotiating SSL connection on FD 155: 
Closed by client

Amos

_______________________________________________
squid-users mailing list
mailto:squid-users at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-users

_______________________________________________
squid-users mailing list
mailto:squid-users at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-users




From oguzismailuysal at gmail.com  Mon Jan 16 08:03:52 2017
From: oguzismailuysal at gmail.com (=?UTF-8?Q?O=C4=9Fuz_=C4=B0smail_Uysal?=)
Date: Mon, 16 Jan 2017 00:03:52 -0800
Subject: [squid-users] Customize squid to make it understand malformed
	requests
Message-ID: <CAH7i3LoFF6=1knu+RxPHbbcy6butuot9F75AdqkCXOQvGYfUiQ@mail.gmail.com>

For a private reason, I want to customize squid version 3.5.12 the way I
stated above. For example I have customized it already to make it
understand \r\n /\r\n instead of \r\n\r\n as request's end, (I know that
this decreases the reliability of squid but I need.). By the way this
customization was kinda easy, now I want it to remove the characters after
a spesific character in request uri, and to remove a spesific character
which is placed at the end of all headers (before \r\n). Which source files
I need to edit, do anyone know ? Or is there any link in which I can find
references to source code which are related to request parsing ?
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20170116/2f3b217f/attachment.htm>

From Antony.Stone at squid.open.source.it  Mon Jan 16 09:10:14 2017
From: Antony.Stone at squid.open.source.it (Antony Stone)
Date: Mon, 16 Jan 2017 10:10:14 +0100
Subject: [squid-users] Customize squid to make it understand malformed
	requests
In-Reply-To: <CAH7i3LoFF6=1knu+RxPHbbcy6butuot9F75AdqkCXOQvGYfUiQ@mail.gmail.com>
References: <CAH7i3LoFF6=1knu+RxPHbbcy6butuot9F75AdqkCXOQvGYfUiQ@mail.gmail.com>
Message-ID: <201701161010.14327.Antony.Stone@squid.open.source.it>

On Monday 16 January 2017 at 09:03:52, O?uz ?smail Uysal wrote:

> For a private reason, I want to customize squid version 3.5.12 the way I
> stated above. For example I have customized it already to make it
> understand \r\n /\r\n instead of \r\n\r\n as request's end

> now I want it to remove the characters after a spesific character in request
> uri, and to remove a spesific character which is placed at the end of all
> headers (before \r\n).

Wouldn't this be easier to achieve using content adaptation?

http://wiki.squid-cache.org/Features/eCAP
http://wiki.squid-cache.org/Features/ICAP

Antony.

-- 
"There is no reason for any individual to have a computer in their home."

 - Ken Olsen, President of Digital Equipment Corporation (DEC, later consumed 
by Compaq, later merged with HP)

                                                   Please reply to the list;
                                                         please *don't* CC me.


From squid3 at treenet.co.nz  Mon Jan 16 09:22:19 2017
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Mon, 16 Jan 2017 22:22:19 +1300
Subject: [squid-users] Customize squid to make it understand malformed
 requests
In-Reply-To: <CAH7i3LoFF6=1knu+RxPHbbcy6butuot9F75AdqkCXOQvGYfUiQ@mail.gmail.com>
References: <CAH7i3LoFF6=1knu+RxPHbbcy6butuot9F75AdqkCXOQvGYfUiQ@mail.gmail.com>
Message-ID: <c0dbcb89-466f-dac9-e9b1-d5c70ad7e9a4@treenet.co.nz>

On 16/01/2017 9:03 p.m., O?uz ?smail Uysal wrote:
> For a private reason, I want to customize squid version 3.5.12 the way I

Note: That is a long-ago obsoleted Squid version.

> stated above. For example I have customized it already to make it
> understand \r\n /\r\n instead of \r\n\r\n as request's end, (I know that
> this decreases the reliability of squid but I need.). By the way this

"reliability" has nothing to do with it. You are talking about
redesigning the fundamental definition of how a message is framed. In
direct violation of how the rest of the HTTP talking Internet communicates.
 <https://tools.ietf.org/html/rfc7230#section-3>

DO NOT do that.

> customization was kinda easy,

Of course, changing code is the easy part. Getting it right is much harder.

For example you seem to think that \r\n\r\n is the end of a request.

It is actually the MIME terminator and occurs at what the syntax defines
as the *middle* of an HTTP message frame - between the MIME and payload
sections.

The MIME section is a sub-frame within HTTP messages. It is optional
(either non-existing or empty) and may be used in any of the three major
HTTP message types.

Payload is a rare occurance on GET requests, which I suspect is where
your mistakes is coming from. But there is actually an implicit
Content-Length:0 header in GET requests making the payload section not
visible when looking at traffic.

Beyond that, mime sections are also used by several other protocols that
Squid implements. We re-use code for common protocol parsing so the code
you are changing probably affects _all_ of those protocols.


> now I want it to remove the characters after
> a spesific character in request uri, and to remove a spesific character
> which is placed at the end of all headers (before \r\n).

So what you actually have is your very own non-HTTP custom messaging
syntax. Please stop modifying Squid's HTTP parsing logics.

For a custom protocol (lets call it Foo for now) you should create your
own Foo namespace and one (or more) Foo::Parser classes that do the
message decapsulation in accordance with your protocols syntax.

You can inherit from or use an instance of the Http1::RequestParser if
you want to re-use some of its HTTP logics. But do not re-write the HTTP
logic to do non-HTTP things. Because, as I mentioned above there are
other protocols also re-using that same HTTP logic.


> Which source files
> I need to edit, do anyone know ? Or is there any link in which I can find
> references to source code which are related to request parsing ?
> 

The Squid developers can be contacted through the squid-dev mailing
list. This list is for network administrators to discuss and get help
*operating* Squid properly.

Amos



From squid3 at treenet.co.nz  Mon Jan 16 09:40:39 2017
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Mon, 16 Jan 2017 22:40:39 +1300
Subject: [squid-users] Customize squid to make it understand malformed
 requests
In-Reply-To: <201701161010.14327.Antony.Stone@squid.open.source.it>
References: <CAH7i3LoFF6=1knu+RxPHbbcy6butuot9F75AdqkCXOQvGYfUiQ@mail.gmail.com>
 <201701161010.14327.Antony.Stone@squid.open.source.it>
Message-ID: <fd87553e-a601-2eaa-0425-fd9804fc4842@treenet.co.nz>

On 16/01/2017 10:10 p.m., Antony Stone wrote:
> On Monday 16 January 2017 at 09:03:52, O?uz ?smail Uysal wrote:
> 
>> For a private reason, I want to customize squid version 3.5.12 the way I
>> stated above. For example I have customized it already to make it
>> understand \r\n /\r\n instead of \r\n\r\n as request's end
> 
>> now I want it to remove the characters after a spesific character in request
>> uri, and to remove a spesific character which is placed at the end of all
>> headers (before \r\n).
> 
> Wouldn't this be easier to achieve using content adaptation?
> 

Not if the malformation screws up the HTTP framing syntax, like the
above describes. See my other post about \r\n\r\n being the middle *not*
the end of a request.


To reach ICAP/eCAP Squid has to be able to parse the message and there
are a limited range of frame malformations which are tolerated before
the message is too mangled and simply gets rejected as non-HTTP.

Also, in passing to ICAP the message has to be delivered in correct HTTP
format to the service with Encapsulated header indicating the sizes of
each HTTP frame sub-section. If the malformation screws with the framing
those sizes will be incorrect and ICAP service gets screwed over as well
as Squid.


FWIW: By replacing the end-of-mime terminator with ' \' Ozguy is making
Squid smuggle request messages. The "private reasons" is obviously an
intention to turn a Squid binary into a piece of malware.
 <https://devcentral.f5.com/articles/jedi-mind-tricks-http-request-smuggling>

Amos



From jester at optimera.us  Tue Jan 17 00:44:34 2017
From: jester at optimera.us (Jester Purtteman)
Date: Mon, 16 Jan 2017 16:44:34 -0800
Subject: [squid-users] Will squid core dump with worker threads?
	Investigating squid crash, 3.5.23
In-Reply-To: <f03d70e4-0c65-79f0-0a49-54cf14b19406@treenet.co.nz>
References: <000001d26de4$93742840$ba5c78c0$@optimera.us>
 <f03d70e4-0c65-79f0-0a49-54cf14b19406@treenet.co.nz>
Message-ID: <003201d2705a$e34dcce0$a9e966a0$@optimera.us>

> -----Original Message-----
> From: squid-users [mailto:squid-users-bounces at lists.squid-cache.org] On
> Behalf Of Amos Jeffries
> Sent: Friday, January 13, 2017 9:37 PM
> To: squid-users at lists.squid-cache.org
> Subject: Re: [squid-users] Will squid core dump with worker threads?
> Investigating squid crash, 3.5.23
> 
> On 14/01/2017 10:32 a.m., Jester Purtteman wrote:
> > Hello,
> >
> >
> >
> > I am having period crashes of my squid server, and I am not getting
> > core dump files.  I have set "workers 6" in my squid.conf, and I know
> > that threads can cause trouble from reading the debugging wiki page.
> > I have confirmed permissions on the directory I'm dumping to, so I
> > don't *think* that is the issue.
> 
> Core dumps are done by your OS when programs crash. You may need to
> turn it on explicitly.
> <http://wiki.squid-cache.org/SquidFaq/BugReporting#Resource_Limits>

Will continue prodding.  I think systemd is doing something funny, because the process isn't getting the same ulimits as I am running it with somehow.  I used to get dumps and I don't anymore, and systemd is one of the big changes between how I have this setup and how it was before.
> 
> >
> > FWIW (core dump to follow, I'll retry without workers and see what
> > happens) I am having squid crashes.  Details I have so far as are as follows:
> >
> >
> >
> > I am running Squid-3.5.23-R14129 on a stock Ubuntu 16.04 configured with:
> >
> >
> >
> > ./configure --prefix=/usr   --localstatedir=/var
> > --libexecdir=/usr/lib/squid    --srcdir=.   --datadir=/usr/share/squid
> > --sysconfdir=/etc/squid   --with-default-user=proxy   --with-logdir=/var/log
> > --with-pidfile=/var/run/squid.pid --enable-linux-netfilter
> > --enable-cache-digests --enable-storeio=ufs,aufs,diskd,rock
> > --enable-async-io=30 --enable-http-violations --enable-zph-qos
> > --with-netfilter-conntrack --with-filedescriptors=65536
> > --with-large-files
> >
> >
> >
> > About once a day it is crashing with the following line as about my
> > only lead in the cache.log:
> >
> >
> >
> > assertion failed: MemBuf.cc:216: "0 <= tailSize && tailSize <= cSize"
> >
> 
> This is <http://bugs.squid-cache.org/show_bug.cgi?id=4606>. We have
> narrowed it down to something about the collapsed revalidation behaviour
> that became visible after the recent security fix.

If I do not use collapsed forwarding, would it be safe to revert 3.5.22?  Crashes are happening roughly daily and I don't really want to put a "babysitter" script in to keep it running if I have better options.  For right now, leaving collapsed forwarding off and not applying that patch seems like a better answer.

> 
> >
> > From the possibly interesting-but-who-knows-maybe-unrelated-files,
> > there is one additional detail.  I had this version running on a
> > Ubuntu 14.04 machine until last week, which I had installed GCC-4.9 on
> > (so I could test squid 4.0), and that had ran stable from December
> > 20th to January 5th without a any crashes.  Then something totally
> > outside of squid went horribly off the rails.  Ubuntu dropped support
> > for the 3.x series kernels, so I updated to
> > 4.4 (from the Ubuntu repositories) and that caused
> > /proc/sys/net/bridge to go away.  While testing an unrelated issue, I
> > ran a script that I adapted from
> > http://wiki.squid-cache.org/ConfigExamples/Intercept/LinuxBridge which
> contains a dangerous couple lines I had not before contemplated:
> >
> >
> > cd /proc/sys/net/bridge/
> > for i in *
> > do
> >    echo 0 > $i
> > done
> > unset i
> >
> > When /proc/sys/net/bridge went away, the change directory failed, then
> > the script proceeded to turn everything in that directory into 0's.
> > OOPS!  I tell this bit so that my fellow admins get a laugh at my
> > expense, and as a cautionary tale.  CHECK the status of that command
> > before you let it do other things!  As it turns out, tproxy works fine
> > without echoing '0' at all those files, but if you want to leave it on
> > the page, may I suggest the following revision to the wiki page:
> >
> 
> Thank you. There is no need for the cd or the * to be without a fixed path. I
> have updated the wiki to prevent this.

Glad my foolish mistakes can make the world better :)

> 
> It it true that TPROXY does not require bridging, and bridging has nothing
> particularly requiring TPROXY. Except that for real transparency they are
> usually both wanted.

I am running a TPROXY system on a bridge, and my script had the snippet in it before.  But, it kept working even after the bridging code was migrated to a module, and it kept working even when the module was not loaded.  So, what effect setting everything in that directory to 0 has is beyond my understanding, and (at least appears) to not be necessary in the latest Ubuntu at least.  So, I guess that is part of my question, is something silently broken that you know of because I am not loading the kernel module for that?

> 
> >
> >
> > In any event, is there a way to get a core with worker threads?  My
> > system benefits from them, so I'd rather not turn them off but I want a
> core dump.
> 
> <http://wiki.squid-cache.org/SquidFaq/BugReporting#Coredump_Location>
> <http://wiki.squid-cache.org/SquidFaq/BugReporting#Resource_Limits>
> 
> workers are not threads. They are processes so each have their own PID
> which the core file should be associated with. Also, only the individual worker
> which crashed will have a core dump generated, then it should be restarted
> automatically by the master process.
> 
> 
> > Also let me know if there are other details that would be useful.
> > Adding much in the way of debugging is going to be a challenge because
> > it takes a day or so to get to a crash, and I don't necessarily have
> > the disk space to hold the volume of cache log generated over that
> > period of time (I have a 60-gb log partition).  If there is some
> > clever way of limiting cache.log and causing it to round-robin or something,
> I'm happy to try things.  Thank you!
> >
> 
> Using the rotate=N option on the debug_options directive will rotate
> cache.log a different (usually smaller) number of times to access.log.
> 
> Or, if you use logrotate you can set it to rotate when the cache.log gets to a
> certain size.
>  see
> <http://stackoverflow.com/questions/20162176/centos-linux-setting-
> logrotate-to-maximum-file-size-for-all-logs>
> 
> Or, you could also setup the cache.log to be a pipe to somewhere else with
> more disk space.

I'll dig in, thanks for the ideas.  It sounds like I don't need to work too hard on debug at this time, because it appears to be a well enough known bug.  But, it would be handy to be able to generate usable debug logs without swamping my system so I'll see what I can come up with.  Thank you again!

--Jester



From mustafamohammad92 at gmail.com  Tue Jan 17 19:27:25 2017
From: mustafamohammad92 at gmail.com (Mustafa Mohammad)
Date: Tue, 17 Jan 2017 13:27:25 -0600
Subject: [squid-users] Help with Certificate validation
Message-ID: <CAOUmaCRCjzQxb794SthctPOg8+fzqpjnn5oo_MCEq4E3Z85oBw@mail.gmail.com>

I?m using squid proxy to connect to our regression server. When our
configuration file is doing a CRLCheck, I?m unable to connect to the
server.  I have tried SSL bump and ssl_proxy option but was unable to make
it work. When I checked the logs, It says it was unable to validate
certificate. This is a high priority issue for our company. Please respond
as soon as possible.

Thanks,
Mustafa
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20170117/6839827e/attachment.htm>

From yvoinov at gmail.com  Tue Jan 17 19:31:33 2017
From: yvoinov at gmail.com (Yuri Voinov)
Date: Wed, 18 Jan 2017 01:31:33 +0600
Subject: [squid-users] Help with Certificate validation
In-Reply-To: <CAOUmaCRCjzQxb794SthctPOg8+fzqpjnn5oo_MCEq4E3Z85oBw@mail.gmail.com>
References: <CAOUmaCRCjzQxb794SthctPOg8+fzqpjnn5oo_MCEq4E3Z85oBw@mail.gmail.com>
Message-ID: <1f74793d-65e4-2ce6-7ecc-ebe7fcfdaed1@gmail.com>

Put your regression server to SSL Bump splice rule.


18.01.2017 1:27, Mustafa Mohammad ?????:
> I?m using squid proxy to connect to our regression server. When our
> configuration file is doing a CRLCheck, I?m unable to connect to the
> server.  I have tried SSL bump and ssl_proxy option but was unable to
> make it work. When I checked the logs, It says it was unable to
> validate certificate. This is a high priority issue for our company.
> Please respond as soon as possible.
>
> Thanks,
> Mustafa
>
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users

-- 
What is the fundamental difference between the programmer and by a fag?
Fag never become five times to free the memory of one object. Fag will
not use two almost identical string libraries in the same project. Fag
will never write to a mixture of C and C ++. Fag will never pass objects
by pointer. Now you know why these two categories so often mentioned
together, and one of them is worse :)
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20170118/5a3966f0/attachment.htm>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: 0x613DEC46.asc
Type: application/pgp-keys
Size: 2437 bytes
Desc: not available
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20170118/5a3966f0/attachment.key>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: signature.asc
Type: application/pgp-signature
Size: 473 bytes
Desc: OpenPGP digital signature
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20170118/5a3966f0/attachment.sig>

From oguzismailuysal at gmail.com  Wed Jan 18 10:01:28 2017
From: oguzismailuysal at gmail.com (=?UTF-8?Q?O=C4=9Fuz_=C4=B0smail_Uysal?=)
Date: Wed, 18 Jan 2017 02:01:28 -0800
Subject: [squid-users] Limit clients per port
Message-ID: <CAH7i3Lo=Yq5_vr7TBYK6C3uEt6FfA37O5G7FWNSOG4kdi8hr8w@mail.gmail.com>

I want to configure squid not to let more than one client per port at a
time. Is it possible ? There are 10 users who use my proxy server and I
have given them different ports to connect to. But I also want to block
multiple clients at one port. How could I do this ?
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20170118/1208da90/attachment.htm>

From squid3 at treenet.co.nz  Wed Jan 18 11:37:01 2017
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 19 Jan 2017 00:37:01 +1300
Subject: [squid-users] Help with Certificate validation
In-Reply-To: <1f74793d-65e4-2ce6-7ecc-ebe7fcfdaed1@gmail.com>
References: <CAOUmaCRCjzQxb794SthctPOg8+fzqpjnn5oo_MCEq4E3Z85oBw@mail.gmail.com>
 <1f74793d-65e4-2ce6-7ecc-ebe7fcfdaed1@gmail.com>
Message-ID: <8038eec9-d31a-1804-6e05-da0482e81b07@treenet.co.nz>

On 18/01/2017 8:31 a.m., Yuri Voinov wrote:
> Put your regression server to SSL Bump splice rule.
> 

If the situation requires SSL-Bump at all then there is no good
solution, because the browser itself is doing CRL checks and rejection.
Squid cannot change browsers internal coding.

> 
> 18.01.2017 1:27, Mustafa Mohammad ?????:
>> I?m using squid proxy to connect to our regression server. When our
>> configuration file is doing a CRLCheck, I?m unable to connect to the
>> server.  I have tried SSL bump and ssl_proxy option but was unable to
>> make it work. When I checked the logs, It says it was unable to
>> validate certificate. This is a high priority issue for our company.
>> Please respond as soon as possible.
>>

How are browser(s) getting to the proxy?

Amos



From squid3 at treenet.co.nz  Wed Jan 18 11:43:34 2017
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 19 Jan 2017 00:43:34 +1300
Subject: [squid-users] Limit clients per port
In-Reply-To: <CAH7i3Lo=Yq5_vr7TBYK6C3uEt6FfA37O5G7FWNSOG4kdi8hr8w@mail.gmail.com>
References: <CAH7i3Lo=Yq5_vr7TBYK6C3uEt6FfA37O5G7FWNSOG4kdi8hr8w@mail.gmail.com>
Message-ID: <8a78462c-3915-7fc2-f7ce-1979883aa197@treenet.co.nz>

On 18/01/2017 11:01 p.m., O?uz ?smail Uysal wrote:
> I want to configure squid not to let more than one client per port at a
> time. Is it possible ? There are 10 users who use my proxy server and I
> have given them different ports to connect to. But I also want to block
> multiple clients at one port. How could I do this ?
> 

With an external ACL helper you can do almost any access conditions you
want.

Amos



From zoltanflavius at yahoo.com  Wed Jan 18 11:50:40 2017
From: zoltanflavius at yahoo.com (Zoltan Flavius)
Date: Wed, 18 Jan 2017 11:50:40 +0000 (UTC)
Subject: [squid-users] Squid as Reverse Proxy for Windows
In-Reply-To: <1137675929.8378199.1484739277837@mail.yahoo.com>
References: <1137675929.8378199.1484739277837.ref@mail.yahoo.com>
 <1137675929.8378199.1484739277837@mail.yahoo.com>
Message-ID: <1764339193.8360808.1484740240696@mail.yahoo.com>

Hello all,



I have an API for which we would like to implement a reverse proxy caching with squid on Windows Server 2008.As I can see here KnowledgeBase/Windows - Squid Web Proxy Wiki?there are some known limitations and I would like to ask you some questions:
  
|  
|  
|  
|   |    |

  |

  |
|  
|   |  
KnowledgeBase/Windows - Squid Web Proxy Wiki
   |   |

  |

  |

 
1. What do you mean by "Some code sections can make blocking calls". Please give me ?details on this.2. ?Also what do you mean by "Some external helpers may not work"?Is the Squid for Windows a stable solution for reverse proxy caching or do you recommend using UNIX based operations system instead??
Could you give me more details in regards to the license, since it is licensed under GNU General Public License and I use it as a reverse proxy??Do I have to give to my end users the freedom to run, study, share and modify our software for which I use Squid as reverse proxy?
Regards,Zoltan Flavius



   
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20170118/c3d2ca8d/attachment.htm>

From yvoinov at gmail.com  Wed Jan 18 12:03:59 2017
From: yvoinov at gmail.com (Yuri)
Date: Wed, 18 Jan 2017 18:03:59 +0600
Subject: [squid-users] Help with Certificate validation
In-Reply-To: <8038eec9-d31a-1804-6e05-da0482e81b07@treenet.co.nz>
References: <CAOUmaCRCjzQxb794SthctPOg8+fzqpjnn5oo_MCEq4E3Z85oBw@mail.gmail.com>
 <1f74793d-65e4-2ce6-7ecc-ebe7fcfdaed1@gmail.com>
 <8038eec9-d31a-1804-6e05-da0482e81b07@treenet.co.nz>
Message-ID: <df795a73-7171-c62c-4b26-4231d66887ae@gmail.com>



18.01.2017 17:37, Amos Jeffries ?????:
> On 18/01/2017 8:31 a.m., Yuri Voinov wrote:
>> Put your regression server to SSL Bump splice rule.
>>
> If the situation requires SSL-Bump at all then there is no good
> solution, because the browser itself is doing CRL checks and rejection.
> Squid cannot change browsers internal coding.
Agreed.
>
>> 18.01.2017 1:27, Mustafa Mohammad ?????:
>>> I?m using squid proxy to connect to our regression server. When our
>>> configuration file is doing a CRLCheck, I?m unable to connect to the
>>> server.  I have tried SSL bump and ssl_proxy option but was unable to
>>> make it work. When I checked the logs, It says it was unable to
>>> validate certificate. This is a high priority issue for our company.
>>> Please respond as soon as possible.
>>>
> How are browser(s) getting to the proxy?
>
> Amos
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users



From sameh.onaissi at solcv.com  Wed Jan 18 14:50:48 2017
From: sameh.onaissi at solcv.com (Sameh Onaissi)
Date: Wed, 18 Jan 2017 14:50:48 +0000
Subject: [squid-users] A bunch of SSL errors I am not sure why
In-Reply-To: <067001d27198$b664e940$232ebbc0$@ngtech.co.il>
References: <0E8D807B-C74E-476A-8F66-8BFE54429E4E@solcv.com>
 <97C413A4-5A8E-4C5C-9440-D9306328EFAE@solcv.com>
 <067001d27198$b664e940$232ebbc0$@ngtech.co.il>
Message-ID: <97F8DE4F-F967-4282-8CAF-C3703C2E4052@solcv.com>

The server is ubuntu 16.04

Clients are mostly Windows 7 Pro, Windows 8.1 Pro, Windows 10 Pro and a few Mac OS El Capitan 10.11





[cid:2FD1C3AB-E45C-49F0-84AB-0F8AC658BD11 at routerb408e2.com]Piensa en el medio ambiente antes de imprimir este email.

On Jan 18, 2017, at 9:39 AM, Eliezer Croitoru <eliezer at ngtech.co.il<mailto:eliezer at ngtech.co.il>> wrote:

You will need to verify if there is an update to the certificates of the OS.
I know that couple authorities certificates was removed in the last month or two and it might be because of this.
What OS are you using?

----
Eliezer Croitoru<http://ngtech.co.il/lmgtfy/>
Linux System Administrator
Mobile: +972-5-28704261
Email: eliezer at ngtech.co.il<mailto:eliezer at ngtech.co.il>
<image004.png>

From: Sameh Onaissi [mailto:sameh.onaissi at solcv.com]
Sent: Wednesday, January 18, 2017 4:32 PM
To: Eliezer Croitoru <eliezer at ngtech.co.il<mailto:eliezer at ngtech.co.il>>
Subject: Fwd: [squid-users] A bunch of SSL errors I am not sure why

Hello Eliezer, all

Sorry for the late reply.

When I configure the browser to access a non intercept port, the errors do not show up and the site is accessed without a problem.

The client machine has the .crt file installed, but still shows the error.

Other pages with errors:
http://pasteboard.co/nA20FD7om.png
http://pasteboard.co/nA2yWRyTE.png

Here is the second page in a browser without an intercepted port:
http://pasteboard.co/nA39CEFGU.png



Thanks in advance.
Some of these sites are used to pay company bills, so it?s important to get this issue resolves ASAP.
Worth mentioning that this was not a problem about 10 days ago.

Thanks again!
<image002.png>

Sameh Onaissi
Ingeniero de Soporte
Sol Cable Visi?n
Cel: 316-3023424
Email: sameh.onaissi at solcv.com<mailto:sameh.onaissi at solcv.com>



<image003.jpg>Piensa en el medio ambiente antes de imprimir este email.

On Jan 15, 2017, at 3:59 AM, Eliezer Croitoru <eliezer at ngtech.co.il<mailto:eliezer at ngtech.co.il>> wrote:

Non intercepted is not bypassed?
Squid has coupe options for the ?http_port? option.
One that you are using is intercept and the other is without intercept.
What happens when you try to connect to this website when you are defining another port without ?Intercept?  and define the proxy in the browser settings?
Let me know if something is missing in the picture.

Eliezer

----
http://ngtech.co.il/lmgtfy/
Linux System Administrator
Mobile: +972-5-28704261
Email: eliezer at ngtech.co.il<mailto:eliezer at ngtech.co.il>


From: Sameh Onaissi [mailto:sameh.onaissi at solcv.com]
Sent: Sunday, January 15, 2017 3:25 AM
To: Eliezer Croitoru <eliezer at ngtech.co.il<mailto:eliezer at ngtech.co.il>>
Cc: Amos Jeffries <squid3 at treenet.co.nz<mailto:squid3 at treenet.co.nz>>; squid-users at lists.squid-cache.org<mailto:squid-users at lists.squid-cache.org>
Subject: Re: [squid-users] A bunch of SSL errors I am not sure why

Hello,

I assume bypassed are non intercepted? Once the site IP is on the bypass list, it opened without an issue. There are a few other .http://gov.co<http://gov.co/> sites who have the same problem too.

Attached is a screenshot of the error before I added the site to the bypass list.

squid -v
Squid Cache: Version 3.5.22
Service Name: squid
Ubuntu linux
configure options:  '--build=x86_64-linux-gnu' '--prefix=/usr' '--includedir=${prefix}/include' '--mandir=${prefix}/share/man' '--infodir=${prefix}/share/info' '--sysconfdir=/etc' '--localstatedir=/var' '--libexecdir=${prefix}/lib/squid3' '--srcdir=.' '--disable-maintainer-mode' '--disable-dependency-tracking' '--disable-silent-rules' 'BUILDCXXFLAGS=-g -O2 -fPIE -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -Wl,-Bsymbolic-functions -fPIE -pie -Wl,-z,relro -Wl,-z,now -Wl,--as-needed' '--datadir=/usr/share/squid' '--sysconfdir=/etc/squid' '--libexecdir=/usr/lib/squid' '--mandir=/usr/share/man' '--enable-inline' '--disable-arch-native' '--enable-async-io=8' '--enable-storeio=ufs,aufs,diskd,rock' '--enable-removal-policies=lru,heap' '--enable-delay-pools' '--enable-cache-digests' '--enable-icap-client' '--enable-follow-x-forwarded-for' '--enable-auth-basic=DB,fake,getpwnam,LDAP,NCSA,NIS,PAM,POP3,RADIUS,SASL,SMB' '--enable-auth-digest=file,LDAP' '--enable-auth-negotiate=kerberos,wrapper' '--enable-auth-ntlm=fake,smb_lm' '--enable-external-acl-helpers=file_userip,kerberos_ldap_group,LDAP_group,session,SQL_session,time_quota,unix_group,wbinfo_group' '--enable-url-rewrite-helpers=fake' '--enable-eui' '--enable-esi' '--enable-icmp' '--enable-zph-qos' '--enable-ecap' '--disable-translation' '--with-swapdir=/var/spool/squid' '--with-logdir=/var/log/squid' '--with-pidfile=/var/run/squid.pid' '--with-filedescriptors=65536' '--with-large-files' '--with-default-user=proxy' '--with-openssl' '--enable-ssl' '--enable-ssl-crtd' '--enable-build-info=Ubuntu linux' '--enable-linux-netfilter' 'build_alias=x86_64-linux-gnu' 'CFLAGS=-g -O2 -fPIE -fstack-protector-strong -Wformat -Werror=format-security -Wall' 'LDFLAGS=-Wl,-Bsymbolic-functions -fPIE -pie -Wl,-z,relro -Wl,-z,now -Wl,--as-needed' 'CPPFLAGS=-Wdate-time -D_FORTIFY_SOURCE=2' 'CXXFLAGS=-g -O2 -fPIE -fstack-protector-strong -Wformat -Werror=format-security'





@ Amos:

"* Check that the set of "global trusted CA" installed on your Squid machiene is up to date.?
I recreated the set recently.


* Try the latest Squid-4, which can auto-download intermediate certificates.

Is squid-4 stable for production?

Thank you,



Sameh Onaissi
Sol Cable Visi?n
Cel: 316-3023424
Email: mailto:sameh.onaissi at solcv.com



Piensa en el medio ambiente antes de imprimir este email.

On Jan 14, 2017, at 12:07 PM, Eliezer Croitoru <mailto:eliezer at ngtech.co.il> wrote:

I have not experienced this issue on my testing lab when accessing:
https://web.dlinkla.com/websys

$ squid -v
Squid Cache: Version 3.5.23
Service Name: squid
configure options:  '--build=x86_64-redhat-linux-gnu' '--host=x86_64-redhat-linux-gnu' '--program-prefix=' '--prefix=/usr' '--exec-prefix=/usr' '--bindir=/usr/bin' '--sbindir=/usr/sbin' '--sysconfdir=/etc' '--datadir=/usr/share' '--includedir=/usr/include' '--libdir=/usr/lib64' '--libexecdir=/usr/libexec' '--sharedstatedir=/var/lib' '--mandir=/usr/share/man' '--infodir=/usr/share/info' '--verbose' '--exec_prefix=/usr' '--libexecdir=/usr/lib64/squid' '--localstatedir=/var' '--datadir=/usr/share/squid' '--sysconfdir=/etc/squid' '--with-logdir=$(localstatedir)/log/squid' '--with-pidfile=$(localstatedir)/run/squid.pid' '--disable-dependency-tracking' '--enable-follow-x-forwarded-for' '--enable-auth' '--enable-auth-basic=DB,LDAP,NCSA,NIS,PAM,POP3,RADIUS,SASL,SMB,getpwnam,fake' '--enable-auth-ntlm=smb_lm,fake' '--enable-auth-digest=file,LDAP,eDirectory' '--enable-auth-negotiate=kerberos,wrapper' '--enable-external-acl-helpers=wbinfo_group,kerberos_ldap_group,LDAP_group,delayer,file_userip,SQL_session,unix_group,session,time_quota' '--enable-cache-digests' '--enable-cachemgr-hostname=localhost' '--enable-delay-pools' '--enable-epoll' '--enable-icap-client' '--enable-ident-lookups' '--enable-linux-netfilter' '--enable-removal-policies=heap,lru' '--enable-snmp' '--enable-storeio=aufs,diskd,ufs,rock' '--enable-wccpv2' '--enable-esi' '--enable-ssl-crtd' '--enable-icmp' '--with-aio' '--with-default-user=squid' '--with-filedescriptors=16384' '--with-dl' '--with-openssl' '--with-pthreads' '--with-included-ltdl' '--disable-arch-native' '--enable-ecap' '--without-nettle' 'build_alias=x86_64-redhat-linux-gnu' 'host_alias=x86_64-redhat-linux-gnu' 'CFLAGS=-O2 -g -pipe -Wall -Wp,-D_FORTIFY_SOURCE=2 -fexceptions -fstack-protector-strong --param=ssp-buffer-size=4 -grecord-gcc-switches   -m64 -mtune=generic' 'LDFLAGS=-Wl,-z,relro ' 'CXXFLAGS=-O2 -g -pipe -Wall -Wp,-D_FORTIFY_SOURCE=2 -fexceptions -fstack-protector-strong --param=ssp-buffer-size=4 -grecord-gcc-switches   -m64 -mtune=generic -fPIC' 'PKG_CONFIG_PATH=:/usr/lib64/pkgconfig:/usr/share/pkgconfig' --enable-ltdl-convenience

When the proxy is defined in the browser.
Can you verify if it affects only intercepted connections or also non-intercepted ones?

Thanks,
Eliezer

----
Eliezer Croitoru
Linux System Administrator
Mobile: +972-5-28704261
Email: mailto:eliezer at ngtech.co.il


-----Original Message-----
From: squid-users [mailto:squid-users-bounces at lists.squid-cache.org] On Behalf Of Amos Jeffries
Sent: Saturday, January 14, 2017 6:51 AM
To: mailto:squid-users at lists.squid-cache.org
Subject: Re: [squid-users] A bunch of SSL errors I am not sure why

On 14/01/2017 4:27 a.m., Sameh Onaissi wrote:

Hello Eliezer, all,


I removed the cipher and the problem is still there:


2017/01/13 10:20:50 kid1| Error negotiating SSL connection on FD 138:
error:14094418:SSL routines:ssl3_read_bytes:tlsv1 alert unknown ca
(1/0)

The CA used to sign the remote endpoints certificate is not trusted. Or an intermediary certificate is missing.

* Check that the set of "global trusted CA" installed on your Squid machiene is up to date.

* Try the latest Squid-4, which can auto-download intermediate certificates.



2017/01/13 10:21:05 kid1| Error negotiating SSL connection on FD 191:
error:14094416:SSL routines:ssl3_read_bytes:sslv3 alert certificate
unknown (1/0)
2017/01/13 10:21:17 kid1| Error negotiating SSL connection on FD 194:
error:14094416:SSL routines:ssl3_read_bytes:sslv3 alert certificate
unknown (1/0)
2017/01/13 10:21:17 kid1| Error negotiating SSL connection on FD 198:
error:14094416:SSL routines:ssl3_read_bytes:sslv3 alert certificate
unknown (1/0)
2017/01/13 10:21:18 kid1| Error negotiating SSL connection on FD 194:
error:14094416:SSL routines:ssl3_read_bytes:sslv3 alert certificate
unknown (1/0)
2017/01/13 10:21:18 kid1| Error negotiating SSL connection on FD 194:
error:14094416:SSL routines:ssl3_read_bytes:sslv3 alert certificate
unknown (1/0)
2017/01/13 10:21:19 kid1| Error negotiating SSL connection on FD 194:
error:14094416:SSL routines:ssl3_read_bytes:sslv3 alert certificate
unknown (1/0)

The obsolete SSL protocol is being used.



2017/01/13 10:21:24 kid1| Error negotiating SSL connection on FD 163:
Closed by client

The client disconnected. You can do nothing about that.


2017/01/13 10:21:39 kid1| Error negotiating SSL connection on FD 250:
error:14094418:SSL routines:ssl3_read_bytes:tlsv1 alert unknown ca
(1/0)
2017/01/13 10:21:42 kid1| Error negotiating SSL on FD 298:
error:14090086:SSL routines:ssl3_get_server_certificate:certificate
verify failed (1/-1/0)

"certificate verify failed" says what it means.


2017-01-13 10:21:53 [29866] Request(everyone/deny/-)
https://accounts.youtube.com/accounts/CheckConnection?pmpo=https://acc
ounts.google.com<http://ounts.google.com/>&v=-1574475776&timestamp=1484320896449
10.0.0.127/10.0.0.127 - GET REDIRECT
2017/01/13 10:21:56 kid1| Error negotiating SSL connection on FD 109:
error:14094418:SSL routines:ssl3_read_bytes:tlsv1 alert unknown ca
(1/0)
2017/01/13 10:21:56 kid1| Error negotiating SSL connection on FD 309:
error:14094416:SSL routines:ssl3_read_bytes:sslv3 alert certificate
unknown (1/0)
2017/01/13 10:22:25 kid1| Error negotiating SSL connection on FD 155:
Closed by client

Amos

_______________________________________________
squid-users mailing list
mailto:squid-users at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-users

_______________________________________________
squid-users mailing list
mailto:squid-users at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-users



-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20170118/624cbab8/attachment.htm>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: Image 5-5-16 at 11.48 AM.jpg
Type: image/jpeg
Size: 4083 bytes
Desc: Image 5-5-16 at 11.48 AM.jpg
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20170118/624cbab8/attachment.jpg>

From squid3 at treenet.co.nz  Wed Jan 18 15:41:37 2017
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 19 Jan 2017 04:41:37 +1300
Subject: [squid-users] Squid as Reverse Proxy for Windows
In-Reply-To: <1764339193.8360808.1484740240696@mail.yahoo.com>
References: <1137675929.8378199.1484739277837.ref@mail.yahoo.com>
 <1137675929.8378199.1484739277837@mail.yahoo.com>
 <1764339193.8360808.1484740240696@mail.yahoo.com>
Message-ID: <37451dce-078e-7349-de5b-75a70546f786@treenet.co.nz>

On 19/01/2017 12:50 a.m., Zoltan Flavius wrote:
> Hello all,
> 
> 
> 
> I have an API for which we would like to implement a reverse proxy
> caching with squid on Windows Server 2008.As I can see here
> KnowledgeBase/Windows - Squid Web Proxy Wiki there are some known
> limitations and I would like to ask you some questions:
> 
> 
> 1. What do you mean by "Some code sections can make blocking calls".
> Please give me  details on this.

One of them was use of poll() for I/O. Most likely someone familiar with
the Windows APIs reading through the compat/os/windows.* code files
would spot a few more.

Sorry, it has been too long since I last hacked away at the MinGW
project I have forgotten the specific API calls behind that statement.

Given your other questions and stated use, the details are probably not
that important for you anyway. It essentially means native Windows
builds are *slow*, which brings me to your next Q...



> 2.  Also what do you mean by "Some
> external helpers may not work"? Is the Squid for Windows a stable
> solution for reverse proxy caching or do you recommend using UNIX
> based operations system instead?

We do recommend using non-Windows operating systems. But primarily for
performance reasons. Windows is just plain slow, and I dont mean by a
little - its peak request per second capacity (top traffic speed) is an
order of magnitude lower than any other OS.

Diladele B.V. linked from that wiki page are providing stable /
producion suitable binaries built with Cygwin for the latest Squid
releases. I dare say that is the best Windows version you will be able
to find anytime soon.

NP: Squid-3 and later still do not build with Visual Studio or MinGW. So
those builds are very much non-stable.


> Could you give me more details in
> regards to the license, since it is licensed under GNU General Public
> License and I use it as a reverse proxy?

If you just build the source code we provide as-is you can *use* the
resulting binaries in any way you wish.

The GPL requirements start to have effect if you make changes to the
Squid code or copy bits of it for use elsewhere.


> Do I have to give to my end
> users the freedom to run, study, share and modify our software for
> which I use Squid as reverse proxy?

The traffic messages (HTTP etc.) going through Squid to the public are
not affected by the Squid license (and vice versa).


HTH
Amos


From squid3 at treenet.co.nz  Wed Jan 18 16:05:53 2017
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 19 Jan 2017 05:05:53 +1300
Subject: [squid-users] A bunch of SSL errors I am not sure why
In-Reply-To: <0E8D807B-C74E-476A-8F66-8BFE54429E4E@solcv.com>
References: <2EBAFFFF-752F-45BC-B06A-E5513CE7D7BA@solcv.com>
 <049c01d26d33$fe058520$fa108f60$@ngtech.co.il>
 <2403F106-B288-4780-BF4F-06AE58E1A52F@solcv.com>
 <59772e12-aa11-d1c7-5543-13c98bb83419@treenet.co.nz>
 <04fd01d26e88$a68b7290$f3a257b0$@ngtech.co.il>
 <07335B2B-5956-42F8-A2DD-3CFA7E6E3158@solcv.com>
 <051401d26f0d$ad266bc0$07734340$@ngtech.co.il>
 <0E8D807B-C74E-476A-8F66-8BFE54429E4E@solcv.com>
Message-ID: <7290de22-7fbc-2857-9a30-b05e33e3adfa@treenet.co.nz>

On 19/01/2017 3:29 a.m., Sameh Onaissi wrote:
> Hello Eliezer, all
> 
> Sorry for the late reply.
> 
> When I configure the browser to access a non intercept port, the errors do not show up and the site is accessed without a problem.
> 
> The client machine has the .crt file installed, but still shows the error.
> 
> Other pages with errors:
> http://pasteboard.co/nA20FD7om.png
> http://pasteboard.co/nA2yWRyTE.png
> 
> Here is the second page in a browser without an intercepted port:
> http://pasteboard.co/nA39CEFGU.png
> 
> 
> Thanks in advance.
> Some of these sites are used to pay company bills, so it?s important to get this issue resolves ASAP.

I assume from that first part that the most important of these sites are
a small enough set to deal with as a special case without becoming a
maintenance nightmare.

The error messages both show that Squid at least cannot find one of the
CA required to verify the servers cert.

Soo...
 you can probably use the openssl client tool to identify and fetch the
certs manually; then

1a) add the root CA (only if needed) into your machines global CA set,

1b) add any intermediary certs to the file Squid loads through
sslproxy_foreign_intermediate_certs directive.
<http://www.squid-cache.org/Doc/config/sslproxy_foreign_intermediate_certs/>

OR

2) create a cache_peer to the domains server port 443, using the
originserver option and sslcafile= option to specify what its CA chain
is supposed to be.
<http://www.squid-cache.org/Doc/config/cache_peer/>


> Worth mentioning that this was not a problem about 10 days ago.

Nod, these types of things can appear out of nowhere as servers certs
expire or get blacklisted, ciphers etc suddenly get rejected by browsers
as insecure. TLS advocates deny it, but F*ups happen far too often in
reality when dealing with certs.


> 
> 
> * Try the latest Squid-4, which can auto-download intermediate certificates.
> 
> Is squid-4 stable for production?
> 

Sorry I missed this in your earlier post.

Well strictly speaking no. It still has a handful of critical bugs to be
tracked down and quashed. But whether those affect you, or if they do
whether its worth an occasional crash to avoid these SSL isues is a
different matter.

Amos



From eliezer at ngtech.co.il  Wed Jan 18 17:40:39 2017
From: eliezer at ngtech.co.il (Eliezer  Croitoru)
Date: Wed, 18 Jan 2017 19:40:39 +0200
Subject: [squid-users] A bunch of SSL errors I am not sure why
In-Reply-To: <7290de22-7fbc-2857-9a30-b05e33e3adfa@treenet.co.nz>
References: <2EBAFFFF-752F-45BC-B06A-E5513CE7D7BA@solcv.com>
 <049c01d26d33$fe058520$fa108f60$@ngtech.co.il>
 <2403F106-B288-4780-BF4F-06AE58E1A52F@solcv.com>
 <59772e12-aa11-d1c7-5543-13c98bb83419@treenet.co.nz>
 <04fd01d26e88$a68b7290$f3a257b0$@ngtech.co.il>
 <07335B2B-5956-42F8-A2DD-3CFA7E6E3158@solcv.com>
 <051401d26f0d$ad266bc0$07734340$@ngtech.co.il>
 <0E8D807B-C74E-476A-8F66-8BFE54429E4E@solcv.com>
 <7290de22-7fbc-2857-9a30-b05e33e3adfa@treenet.co.nz>
Message-ID: <000c01d271b1$fbe0ced0$f3a26c70$@ngtech.co.il>

Thanks for the detail Amos,

I noticed that couple major Root CA certificates was revoked so it could be one thing.
And can you give some more details on how to fetch the certificated using the openssl tools?
(Maybe redirect towards an article about it)
I think that if some sites are have issues then a simple script that will run the openssl tools to fetch the certificates and add them to the system can be useful for those which are running 3.5 and yet to jump into the 4.0 testing.
I can write the script that will do come of the work for these admins.

Eliezer

----
Eliezer Croitoru
Linux System Administrator
Mobile: +972-5-28704261
Email: eliezer at ngtech.co.il


-----Original Message-----
From: squid-users [mailto:squid-users-bounces at lists.squid-cache.org] On Behalf Of Amos Jeffries
Sent: Wednesday, January 18, 2017 6:06 PM
To: squid-users at lists.squid-cache.org
Subject: Re: [squid-users] A bunch of SSL errors I am not sure why

On 19/01/2017 3:29 a.m., Sameh Onaissi wrote:
> Hello Eliezer, all
> 
> Sorry for the late reply.
> 
> When I configure the browser to access a non intercept port, the errors do not show up and the site is accessed without a problem.
> 
> The client machine has the .crt file installed, but still shows the error.
> 
> Other pages with errors:
> http://pasteboard.co/nA20FD7om.png
> http://pasteboard.co/nA2yWRyTE.png
> 
> Here is the second page in a browser without an intercepted port:
> http://pasteboard.co/nA39CEFGU.png
> 
> 
> Thanks in advance.
> Some of these sites are used to pay company bills, so it?s important to get this issue resolves ASAP.

I assume from that first part that the most important of these sites are a small enough set to deal with as a special case without becoming a maintenance nightmare.

The error messages both show that Squid at least cannot find one of the CA required to verify the servers cert.

Soo...
 you can probably use the openssl client tool to identify and fetch the certs manually; then

1a) add the root CA (only if needed) into your machines global CA set,

1b) add any intermediary certs to the file Squid loads through sslproxy_foreign_intermediate_certs directive.
<http://www.squid-cache.org/Doc/config/sslproxy_foreign_intermediate_certs/>

OR

2) create a cache_peer to the domains server port 443, using the originserver option and sslcafile= option to specify what its CA chain is supposed to be.
<http://www.squid-cache.org/Doc/config/cache_peer/>


> Worth mentioning that this was not a problem about 10 days ago.

Nod, these types of things can appear out of nowhere as servers certs expire or get blacklisted, ciphers etc suddenly get rejected by browsers as insecure. TLS advocates deny it, but F*ups happen far too often in reality when dealing with certs.


> 
> 
> * Try the latest Squid-4, which can auto-download intermediate certificates.
> 
> Is squid-4 stable for production?
> 

Sorry I missed this in your earlier post.

Well strictly speaking no. It still has a handful of critical bugs to be tracked down and quashed. But whether those affect you, or if they do whether its worth an occasional crash to avoid these SSL isues is a different matter.

Amos

_______________________________________________
squid-users mailing list
squid-users at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-users



From yvoinov at gmail.com  Wed Jan 18 17:44:25 2017
From: yvoinov at gmail.com (Yuri Voinov)
Date: Wed, 18 Jan 2017 23:44:25 +0600
Subject: [squid-users] A bunch of SSL errors I am not sure why
In-Reply-To: <000c01d271b1$fbe0ced0$f3a26c70$@ngtech.co.il>
References: <2EBAFFFF-752F-45BC-B06A-E5513CE7D7BA@solcv.com>
 <049c01d26d33$fe058520$fa108f60$@ngtech.co.il>
 <2403F106-B288-4780-BF4F-06AE58E1A52F@solcv.com>
 <59772e12-aa11-d1c7-5543-13c98bb83419@treenet.co.nz>
 <04fd01d26e88$a68b7290$f3a257b0$@ngtech.co.il>
 <07335B2B-5956-42F8-A2DD-3CFA7E6E3158@solcv.com>
 <051401d26f0d$ad266bc0$07734340$@ngtech.co.il>
 <0E8D807B-C74E-476A-8F66-8BFE54429E4E@solcv.com>
 <7290de22-7fbc-2857-9a30-b05e33e3adfa@treenet.co.nz>
 <000c01d271b1$fbe0ced0$f3a26c70$@ngtech.co.il>
Message-ID: <7f0acc05-8744-c8a3-070e-104d726fd88a@gmail.com>



18.01.2017 23:40, Eliezer Croitoru ?????:
> Thanks for the detail Amos,
>
> I noticed that couple major Root CA certificates was revoked so it could be one thing.
> And can you give some more details on how to fetch the certificated using the openssl tools?
> (Maybe redirect towards an article about it)
There is no article about trivial things.

root @ khorne / # openssl s_client -connect symantec.com:443
CONNECTED(00000003)
depth=2 C = US, O = "VeriSign, Inc.", OU = VeriSign Trust Network, OU =
"(c) 2006 VeriSign, Inc. - For authorized use only", CN = VeriSign Class
3 Public Primary Certification Authority - G5
verify return:1
depth=1 C = US, O = Symantec Corporation, OU = Symantec Trust Network,
CN = Symantec Class 3 EV SSL CA - G3
verify return:1
depth=0 1.3.6.1.4.1.311.60.2.1.3 = US, 1.3.6.1.4.1.311.60.2.1.2 =
Delaware, businessCategory = Private Organization, serialNumber =
2158113, C = US, postalCode = 94043, ST = California, L = Mountain View,
street = 350 Ellis Street, O = Symantec Corporation, OU = Symantec Web -
Redir, CN = symantec.com
verify return:1
---
Certificate chain
 0
s:/1.3.6.1.4.1.311.60.2.1.3=US/1.3.6.1.4.1.311.60.2.1.2=Delaware/businessCategory=Private
Organization/serialNumber=2158113/C=US/postalCode=94043/ST=California/L=Mountain
View/street=350 Ellis Street/O=Symantec Corporation/OU=Symantec Web -
Redir/CN=symantec.com
   i:/C=US/O=Symantec Corporation/OU=Symantec Trust Network/CN=Symantec
Class 3 EV SSL CA - G3
 1 s:/C=US/O=Symantec Corporation/OU=Symantec Trust Network/CN=Symantec
Class 3 EV SSL CA - G3
   i:/C=US/O=VeriSign, Inc./OU=VeriSign Trust Network/OU=(c) 2006
VeriSign, Inc. - For authorized use only/CN=VeriSign Class 3 Public
Primary Certification Authority - G5
 2 s:/C=US/O=VeriSign, Inc./OU=VeriSign Trust Network/OU=(c) 2006
VeriSign, Inc. - For authorized use only/CN=VeriSign Class 3 Public
Primary Certification Authority - G5
   i:/C=US/O=VeriSign, Inc./OU=VeriSign Trust Network/OU=(c) 2006
VeriSign, Inc. - For authorized use only/CN=VeriSign Class 3 Public
Primary Certification Authority - G5
---
Server certificate
-----BEGIN CERTIFICATE-----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-----END CERTIFICATE-----
subject=/1.3.6.1.4.1.311.60.2.1.3=US/1.3.6.1.4.1.311.60.2.1.2=Delaware/businessCategory=Private
Organization/serialNumber=2158113/C=US/postalCode=94043/ST=California/L=Mountain
View/street=350 Ellis Street/O=Symantec Corporation/OU=Symantec Web -
Redir/CN=symantec.com
issuer=/C=US/O=Symantec Corporation/OU=Symantec Trust
Network/CN=Symantec Class 3 EV SSL CA - G3
---
No client certificate CA names sent
---
SSL handshake has read 5624 bytes and written 433 bytes
---
New, TLSv1/SSLv3, Cipher is ECDHE-RSA-AES128-SHA
Server public key is 2048 bit
Secure Renegotiation IS supported
Compression: NONE
Expansion: NONE
SSL-Session:
    Protocol  : TLSv1.2
    Cipher    : ECDHE-RSA-AES128-SHA
    Session-ID:
7ED48810D697DDAE5C591942755CF47E3D96431EC46C074641B5E1363ABE812E
    Session-ID-ctx:
    Master-Key:
68B42DE89E49E2F16E7461853B9CD8F5393955C9A8C3B6DB27A560CD753669285C51FEA33C2324694F1AA43B833021E8
    Key-Arg   : None
    PSK identity: None
    PSK identity hint: None
    SRP username: None
    Start Time: 1484761429
    Timeout   : 300 (sec)
    Verify return code: 0 (ok)
---
^C

That's all.
> I think that if some sites are have issues then a simple script that will run the openssl tools to fetch the certificates and add them to the system can be useful for those which are running 3.5 and yet to jump into the 4.0 testing.
> I can write the script that will do come of the work for these admins.
>
> Eliezer
>
> ----
> Eliezer Croitoru
> Linux System Administrator
> Mobile: +972-5-28704261
> Email: eliezer at ngtech.co.il
>
>
> -----Original Message-----
> From: squid-users [mailto:squid-users-bounces at lists.squid-cache.org] On Behalf Of Amos Jeffries
> Sent: Wednesday, January 18, 2017 6:06 PM
> To: squid-users at lists.squid-cache.org
> Subject: Re: [squid-users] A bunch of SSL errors I am not sure why
>
> On 19/01/2017 3:29 a.m., Sameh Onaissi wrote:
>> Hello Eliezer, all
>>
>> Sorry for the late reply.
>>
>> When I configure the browser to access a non intercept port, the errors do not show up and the site is accessed without a problem.
>>
>> The client machine has the .crt file installed, but still shows the error.
>>
>> Other pages with errors:
>> http://pasteboard.co/nA20FD7om.png
>> http://pasteboard.co/nA2yWRyTE.png
>>
>> Here is the second page in a browser without an intercepted port:
>> http://pasteboard.co/nA39CEFGU.png
>>
>>
>> Thanks in advance.
>> Some of these sites are used to pay company bills, so it?s important to get this issue resolves ASAP.
> I assume from that first part that the most important of these sites are a small enough set to deal with as a special case without becoming a maintenance nightmare.
>
> The error messages both show that Squid at least cannot find one of the CA required to verify the servers cert.
>
> Soo...
>  you can probably use the openssl client tool to identify and fetch the certs manually; then
>
> 1a) add the root CA (only if needed) into your machines global CA set,
>
> 1b) add any intermediary certs to the file Squid loads through sslproxy_foreign_intermediate_certs directive.
> <http://www.squid-cache.org/Doc/config/sslproxy_foreign_intermediate_certs/>
>
> OR
>
> 2) create a cache_peer to the domains server port 443, using the originserver option and sslcafile= option to specify what its CA chain is supposed to be.
> <http://www.squid-cache.org/Doc/config/cache_peer/>
>
>
>> Worth mentioning that this was not a problem about 10 days ago.
> Nod, these types of things can appear out of nowhere as servers certs expire or get blacklisted, ciphers etc suddenly get rejected by browsers as insecure. TLS advocates deny it, but F*ups happen far too often in reality when dealing with certs.
>
>
>>
>> * Try the latest Squid-4, which can auto-download intermediate certificates.
>>
>> Is squid-4 stable for production?
>>
> Sorry I missed this in your earlier post.
>
> Well strictly speaking no. It still has a handful of critical bugs to be tracked down and quashed. But whether those affect you, or if they do whether its worth an occasional crash to avoid these SSL isues is a different matter.
>
> Amos
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users

-- 
OpenSource should be called "Fifty shades of Brown"
-------------- next part --------------
A non-text attachment was scrubbed...
Name: 0x613DEC46.asc
Type: application/pgp-keys
Size: 2437 bytes
Desc: not available
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20170118/d7f9fac7/attachment.key>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: signature.asc
Type: application/pgp-signature
Size: 473 bytes
Desc: OpenPGP digital signature
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20170118/d7f9fac7/attachment.sig>

From goal81 at gmail.com  Wed Jan 18 19:07:13 2017
From: goal81 at gmail.com (Alexander)
Date: Wed, 18 Jan 2017 22:07:13 +0300
Subject: [squid-users] Native FTP relay - connection closes when FTP data
	connection is used (?)
Message-ID: <CACMUsGxcd9csLvhRb2TUoh45CYw3knPtm7UYjZ1oG4TfJgfW=Q@mail.gmail.com>

Hello, I have a question regarding a native FTP relay.

I have tried to test this feature like this:

[Filezilla Client, 1.1.1.2] <-----> [ Router: iptables + squid ]
<-----> [vsftpd server, 5.5.5.10]

Firewall settings on the router are:

ip route flush table 100
ip rule add fwmark 1 lookup 100
ip route add local 0.0.0.0/0 dev lo table 100

iptables -t mangle -N DIVERT
iptables -t mangle -A DIVERT -j MARK --set-mark 0x01/0x01
iptables -t mangle -A DIVERT -j ACCEPT
iptables -t mangle -A PREROUTING -p tcp -m socket -j DIVERT
iptables -t mangle -A PREROUTING -p tcp --dport 21 -j TPROXY
--tproxy-mark 0x1/0x1 --on-port 2121
iptables -t mangle -A PREROUTING -p tcp --dport 80 -j TPROXY
--tproxy-mark 0x1/0x1 --on-port 3128

No other rules are defined, default policies in chains is ACCEPT.

Squid's configuration file is attached.

With HTTP traffic everything works fine, however FTP causes a problem.
A client successfully connects and authenticates, but when it tries to
execute LIST or RETR (when data connection should be established),
Filezilla says "Connection closed by server". In squid's log I have
noticed some errors when establishing data connection (?), like
"failed to connect FTP server data channel". The log is also attached.

What can be wrong with this setup?
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20170118/cb529175/attachment.htm>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: cache2.log
Type: application/octet-stream
Size: 43809 bytes
Desc: not available
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20170118/cb529175/attachment.obj>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: squid.conf
Type: application/octet-stream
Size: 1485 bytes
Desc: not available
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20170118/cb529175/attachment-0001.obj>

From sameh.onaissi at solcv.com  Wed Jan 18 23:53:30 2017
From: sameh.onaissi at solcv.com (Sameh Onaissi)
Date: Wed, 18 Jan 2017 23:53:30 +0000
Subject: [squid-users] A bunch of SSL errors I am not sure why
In-Reply-To: <7f0acc05-8744-c8a3-070e-104d726fd88a@gmail.com>
References: <2EBAFFFF-752F-45BC-B06A-E5513CE7D7BA@solcv.com>
 <049c01d26d33$fe058520$fa108f60$@ngtech.co.il>
 <2403F106-B288-4780-BF4F-06AE58E1A52F@solcv.com>
 <59772e12-aa11-d1c7-5543-13c98bb83419@treenet.co.nz>
 <04fd01d26e88$a68b7290$f3a257b0$@ngtech.co.il>
 <07335B2B-5956-42F8-A2DD-3CFA7E6E3158@solcv.com>
 <051401d26f0d$ad266bc0$07734340$@ngtech.co.il>
 <0E8D807B-C74E-476A-8F66-8BFE54429E4E@solcv.com>
 <7290de22-7fbc-2857-9a30-b05e33e3adfa@treenet.co.nz>
 <000c01d271b1$fbe0ced0$f3a26c70$@ngtech.co.il>
 <7f0acc05-8744-c8a3-070e-104d726fd88a@gmail.com>
Message-ID: <5E0F128E-DEA3-41AA-9BD7-311953C6784F@solcv.com>

Hello, Amos? all

Yuri, thanks for the reply.


Amos,

I added: Thanks to Eliezer)
sslproxy_cert_error allow all
sslproxy_flags DONT_VERIFY_PEER
to the config file, I am not too worried about the verification since the accessed sites showing problems are government site or local paying services/partners.

However, some sites are still showing the Handshake problem. https://ibin.co/38uz8akvWayM.png

You had previously replied to this saying:

"If you actually read that error message it tells you exactly what the
problem is.

"Handshake with SSL server failed: [blah blah codes]: dh key too small"

The server is trying to use a Diffi-Helman cipher with a too-short key.
DH cipher with short keys has recently been broken. By recently I mean
about a whole year ago.?

However, I still wonder what the solution is? is it possible to fix this? and who needs to fix it? is it a squid side error? is it an OS level error?

Any more information is greatly appreciated.






Thanks again,
Sam


On Jan 18, 2017, at 12:44 PM, Yuri Voinov <yvoinov at gmail.com<mailto:yvoinov at gmail.com>> wrote:



18.01.2017 23:40, Eliezer Croitoru ?????:
Thanks for the detail Amos,

I noticed that couple major Root CA certificates was revoked so it could be one thing.
And can you give some more details on how to fetch the certificated using the openssl tools?
(Maybe redirect towards an article about it)
There is no article about trivial things.

root @ khorne / # openssl s_client -connect symantec.com<http://symantec.com>:443
CONNECTED(00000003)
depth=2 C = US, O = "VeriSign, Inc.", OU = VeriSign Trust Network, OU =
"(c) 2006 VeriSign, Inc. - For authorized use only", CN = VeriSign Class
3 Public Primary Certification Authority - G5
verify return:1
depth=1 C = US, O = Symantec Corporation, OU = Symantec Trust Network,
CN = Symantec Class 3 EV SSL CA - G3
verify return:1
depth=0 1.3.6.1.4.1.311.60.2.1.3 = US, 1.3.6.1.4.1.311.60.2.1.2 =
Delaware, businessCategory = Private Organization, serialNumber =
2158113, C = US, postalCode = 94043, ST = California, L = Mountain View,
street = 350 Ellis Street, O = Symantec Corporation, OU = Symantec Web -
Redir, CN = symantec.com
verify return:1
---
Certificate chain
0
s:/1.3.6.1.4.1.311.60.2.1.3=US/1.3.6.1.4.1.311.60.2.1.2=Delaware/businessCategory=Private
Organization/serialNumber=2158113/C=US/postalCode=94043/ST=California/L=Mountain
View/street=350 Ellis Street/O=Symantec Corporation/OU=Symantec Web -
Redir/CN=symantec.com
  i:/C=US/O=Symantec Corporation/OU=Symantec Trust Network/CN=Symantec
Class 3 EV SSL CA - G3
1 s:/C=US/O=Symantec Corporation/OU=Symantec Trust Network/CN=Symantec
Class 3 EV SSL CA - G3
  i:/C=US/O=VeriSign, Inc./OU=VeriSign Trust Network/OU=(c) 2006
VeriSign, Inc. - For authorized use only/CN=VeriSign Class 3 Public
Primary Certification Authority - G5
2 s:/C=US/O=VeriSign, Inc./OU=VeriSign Trust Network/OU=(c) 2006
VeriSign, Inc. - For authorized use only/CN=VeriSign Class 3 Public
Primary Certification Authority - G5
  i:/C=US/O=VeriSign, Inc./OU=VeriSign Trust Network/OU=(c) 2006
VeriSign, Inc. - For authorized use only/CN=VeriSign Class 3 Public
Primary Certification Authority - G5
---
Server certificate
-----BEGIN CERTIFICATE-----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-----END CERTIFICATE-----
subject=/1.3.6.1.4.1.311.60.2.1.3=US/1.3.6.1.4.1.311.60.2.1.2=Delaware/businessCategory=Private
Organization/serialNumber=2158113/C=US/postalCode=94043/ST=California/L=Mountain
View/street=350 Ellis Street/O=Symantec Corporation/OU=Symantec Web -
Redir/CN=symantec.com
issuer=/C=US/O=Symantec Corporation/OU=Symantec Trust
Network/CN=Symantec Class 3 EV SSL CA - G3
---
No client certificate CA names sent
---
SSL handshake has read 5624 bytes and written 433 bytes
---
New, TLSv1/SSLv3, Cipher is ECDHE-RSA-AES128-SHA
Server public key is 2048 bit
Secure Renegotiation IS supported
Compression: NONE
Expansion: NONE
SSL-Session:
   Protocol  : TLSv1.2
   Cipher    : ECDHE-RSA-AES128-SHA
   Session-ID:
7ED48810D697DDAE5C591942755CF47E3D96431EC46C074641B5E1363ABE812E
   Session-ID-ctx:
   Master-Key:
68B42DE89E49E2F16E7461853B9CD8F5393955C9A8C3B6DB27A560CD753669285C51FEA33C2324694F1AA43B833021E8
   Key-Arg   : None
   PSK identity: None
   PSK identity hint: None
   SRP username: None
   Start Time: 1484761429
   Timeout   : 300 (sec)
   Verify return code: 0 (ok)
---
^C

That's all.
I think that if some sites are have issues then a simple script that will run the openssl tools to fetch the certificates and add them to the system can be useful for those which are running 3.5 and yet to jump into the 4.0 testing.
I can write the script that will do come of the work for these admins.

Eliezer

----
Eliezer Croitoru
Linux System Administrator
Mobile: +972-5-28704261
Email: eliezer at ngtech.co.il


-----Original Message-----
From: squid-users [mailto:squid-users-bounces at lists.squid-cache.org] On Behalf Of Amos Jeffries
Sent: Wednesday, January 18, 2017 6:06 PM
To: squid-users at lists.squid-cache.org
Subject: Re: [squid-users] A bunch of SSL errors I am not sure why

On 19/01/2017 3:29 a.m., Sameh Onaissi wrote:
Hello Eliezer, all

Sorry for the late reply.

When I configure the browser to access a non intercept port, the errors do not show up and the site is accessed without a problem.

The client machine has the .crt file installed, but still shows the error.

Other pages with errors:
http://pasteboard.co/nA20FD7om.png
http://pasteboard.co/nA2yWRyTE.png

Here is the second page in a browser without an intercepted port:
http://pasteboard.co/nA39CEFGU.png


Thanks in advance.
Some of these sites are used to pay company bills, so it?s important to get this issue resolves ASAP.
I assume from that first part that the most important of these sites are a small enough set to deal with as a special case without becoming a maintenance nightmare.

The error messages both show that Squid at least cannot find one of the CA required to verify the servers cert.

Soo...
you can probably use the openssl client tool to identify and fetch the certs manually; then

1a) add the root CA (only if needed) into your machines global CA set,

1b) add any intermediary certs to the file Squid loads through sslproxy_foreign_intermediate_certs directive.
<http://www.squid-cache.org/Doc/config/sslproxy_foreign_intermediate_certs/>

OR

2) create a cache_peer to the domains server port 443, using the originserver option and sslcafile= option to specify what its CA chain is supposed to be.
<http://www.squid-cache.org/Doc/config/cache_peer/>


Worth mentioning that this was not a problem about 10 days ago.
Nod, these types of things can appear out of nowhere as servers certs expire or get blacklisted, ciphers etc suddenly get rejected by browsers as insecure. TLS advocates deny it, but F*ups happen far too often in reality when dealing with certs.



* Try the latest Squid-4, which can auto-download intermediate certificates.

Is squid-4 stable for production?

Sorry I missed this in your earlier post.

Well strictly speaking no. It still has a handful of critical bugs to be tracked down and quashed. But whether those affect you, or if they do whether its worth an occasional crash to avoid these SSL isues is a different matter.

Amos

_______________________________________________
squid-users mailing list
squid-users at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-users

_______________________________________________
squid-users mailing list
squid-users at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-users

--
OpenSource should be called "Fifty shades of Brown"
<0x613DEC46.asc>_______________________________________________
squid-users mailing list
squid-users at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-users

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20170118/462e673a/attachment.htm>

From varun.singh at gslab.com  Thu Jan 19 07:00:54 2017
From: varun.singh at gslab.com (Varun Singh)
Date: Thu, 19 Jan 2017 12:30:54 +0530
Subject: [squid-users] Connect strongSwan and Squid on same server
Message-ID: <CABUhpQU1QN3hvX0xqxO51r8u7GrQcjZ8yjqfDP0RLnLu06YAFw@mail.gmail.com>

Hi,
I have installed strongSwan and Squid HTTP Proxy on the same Ubuntu
16.04 server and I am trying to connect both. By connect I mean, I am
trying to achieve following:

[VPN Client] <------> [VPN Server] <-> [Squid] <------> [Internet]

My objective is to connect a VPN client to VPN server and use Squid
for filtering out blocked Urls. strongSwan and Squid work fine on
their own. I can access internet when connected to VPN server and also
when configured HTTP Proxy without VPN.

>From what I understand, to achieve what I want, I am supposed to
redirect incoming HTTP traffic from port 80 to port using IPTables. I
enter following IPTables rule:

iptables -t nat -A PREROUTING -i eth0 -p tcp --dport 80 -j REDIRECT
--to-port 3128

Once I do this and try to access internet from a connected VPN client,
I get error. Pasting a log of /var/log/squid/access.log


1484738365.632      0 114.143.194.190 TCP_DENIED/403 4066 CONNECT
api-glb-sin.smoot.apple.com:443 - HIER_NONE/- text/html
1484738365.642      0 114.143.194.190 TCP_DENIED/403 4870 GET
http://www.apple.com/ac/globalfooter/2.0/en_US/styles/ac-globalfooter.built.css
- HIER_NONE/- text/html
1484738365.643      0 114.143.194.190 TCP_DENIED/403 4852 GET
http://www.apple.com/ac/globalnav/2.0/en_US/styles/ac-globalnav.built.css
- HIER_NONE/- text/html
1484738365.731      0 114.143.194.190 TCP_DENIED/403 4753 GET
http://www.apple.com/wss/fonts/? - HIER_NONE/- text/html
1484738365.760      0 114.143.194.190 TCP_DENIED/403 4817 GET
http://www.apple.com/metrics/ac-analytics/1.1/scripts/ac-analytics.js
- HIER_NONE/- text/html
1484738367.798      0 114.143.194.190 TCP_DENIED/403 4066 CONNECT
init.itunes.apple.com:443 - HIER_NONE/- text/html
1484738367.922      0 114.143.194.190 TCP_DENIED/403 4334 GET
http://www.apple.com/apple-touch-icon-76x76-precomposed.png -
HIER_NONE/- text/html
1484738367.963      0 114.143.194.190 TCP_DENIED/403 4025 CONNECT
gsp10-ssl.apple.com:443 - HIER_NONE/- text/html
1484738368.036      0 114.143.194.190 TCP_DENIED/403 4298 GET
http://www.apple.com/apple-touch-icon-76x76.png - HIER_NONE/-
text/html
1484738368.148      0 114.143.194.190 TCP_DENIED/403 4352 GET
http://www.apple.com/apple-touch-icon.png - HIER_NONE/- text/html
1484738368.255      0 114.143.194.190 TCP_DENIED/403 4352 GET
http://www.apple.com/apple-touch-icon.png - HIER_NONE/- text/html
1484738368.296      0 114.143.194.190 TCP_DENIED/403 4316 GET
http://www.apple.com/apple-touch-icon-precomposed.png - HIER_NONE/-
text/html
1484738368.348      0 114.143.194.190 TCP_DENIED/403 4253 GET
http://www.apple.com/favicon.ico - HIER_NONE/- text/html
1484738376.374      0 114.143.194.190 TCP_DENIED/403 4655 GET
http://www.apple.com/ - HIER_NONE/- text/html
1484738376.456      0 114.143.194.190 TCP_DENIED/403 4711 GET
http://ip-172-31-9-90:3128/squid-internal-static/icons/SN.png -
HIER_NONE/- text/html
1484738385.761      0 114.143.194.190 TCP_DENIED/403 4655 GET
http://www.apple.com/ - HIER_NONE/- text/html
1484738385.828      0 114.143.194.190 TCP_DENIED/403 4747 GET
http://ip-172-31-9-90:3128/squid-internal-static/icons/SN.png -
HIER_NONE/- text/html
1484738858.272      0 10.99.1.1 TAG_NONE/400 4154 GET
/assets/com_apple_MobileAsset_SafariCloudHistoryConfiguration/com_apple_MobileAsset_SafariCloudHistoryConfiguration.xml
- HIER_NONE/- text/html
1484738858.990      0 10.99.1.1 TAG_NONE/400 4004 GET
/us/shop/bag/status?apikey=SFX9YPYY9PPXCU9KH - HIER_NONE/- text/html
1484738860.362      0 10.99.1.1 TAG_NONE/400 5350 GET
/b/ss/appleglobal,applehome,applestoreww,applestoreamr,applestoreus/1/H.27/s5505031635984?AQB=1&ndh=1&t=18%2F0%2F2017%2016%3A57%3A40%203%20-330&fid=21A4DCCB11396F92-26B205C305B2B2DF&pageName=apple%20-%20index%2Ftab%20%28us%29&g=http%3A%2F%2Fwww.apple.com%2F&cc=USD&ch=www.us.homepage&server=new%20approach%20ac-analytics&v3=aos%3A%20us&c4=D%3Dg&c5=ipad&c9=ios%209.3.5&c19=aos%3A%20us%3A%20apple%20-%20index%2Ftab%20%28us%29&c20=aos%3A%20us&c25=direct%20entry&c48=4&c49=D%3D2C39962A85032063-4000118780008FDC&v54=http%3A%2F%2Fwww.apple.com%2F&h1=www.us.homepage&s=768x1024&c=32&j=1.6&v=N&k=Y&bw=768&bh=960&AQE=1
- HIER_NONE/- text/html
1484739056.258      0 10.99.1.1 TAG_NONE/400 3918 GET / - HIER_NONE/- text/html
1484739056.480      0 10.99.1.1 TCP_DENIED/403 4290 GET
http://ip-172-31-9-90:3128/squid-internal-static/icons/SN.png -
HIER_NONE/- text/html
1484739057.106      0 10.99.1.1 TAG_NONE/400 3994 GET
/apple-touch-icon-76x76-precomposed.png - HIER_NONE/- text/html
1484739057.166      0 10.99.1.1 TAG_NONE/400 3970 GET
/apple-touch-icon-76x76.png - HIER_NONE/- text/html
1484739057.211      0 10.99.1.1 TAG_NONE/400 3958 GET
/apple-touch-icon.png - HIER_NONE/- text/html
1484739057.267      0 10.99.1.1 TAG_NONE/400 3958 GET
/apple-touch-icon.png - HIER_NONE/- text/html
1484739057.340      0 10.99.1.1 TAG_NONE/400 3982 GET
/apple-touch-icon-precomposed.png - HIER_NONE/- text/html
1484739057.436      0 10.99.1.1 TAG_NONE/400 3940 GET /favicon.ico -
HIER_NONE/- text/html
1484739060.563      0 10.99.1.1 TAG_NONE/400 3924 GET /bag -
HIER_NONE/- text/html
1484739071.241      0 10.99.1.1 TAG_NONE/400 3918 GET / - HIER_NONE/- text/html
1484739071.439      0 10.99.1.1 TCP_DENIED/403 4290 GET
http://ip-172-31-9-90:3128/squid-internal-static/icons/SN.png -
HIER_NONE/- text/html
1484739092.972      0 10.99.1.1 TAG_NONE/400 3918 GET / - HIER_NONE/- text/html
1484739093.151      0 10.99.1.1 TCP_DENIED/403 4621 GET
http://ip-172-31-9-90:3128/squid-internal-static/icons/SN.png -
HIER_NONE/- text/html
1484739093.306      0 10.99.1.1 TAG_NONE/400 3994 GET
/apple-touch-icon-76x76-precomposed.png - HIER_NONE/- text/html
1484739093.364      0 10.99.1.1 TAG_NONE/400 3970 GET
/apple-touch-icon-76x76.png - HIER_NONE/- text/html
1484739093.427      0 10.99.1.1 TAG_NONE/400 3958 GET
/apple-touch-icon.png - HIER_NONE/- text/html
1484739093.480      0 10.99.1.1 TAG_NONE/400 3958 GET
/apple-touch-icon.png - HIER_NONE/- text/html
1484739093.529      0 10.99.1.1 TAG_NONE/400 3982 GET
/apple-touch-icon-precomposed.png - HIER_NONE/- text/html
1484739093.578      0 10.99.1.1 TAG_NONE/400 3940 GET /favicon.ico -
HIER_NONE/- text/html
1484741172.545      0 123.240.104.249 TAG_NONE/400 3924 GET / -
HIER_NONE/- text/html
1484742330.250      0 10.99.1.2 TAG_NONE/400 4444 NONE
error:invalid-request - HIER_NONE/- text/html
1484742335.479      0 10.99.1.2 TAG_NONE/400 4220
%E1%89%C5%01%DCd%95A-%D0%16%9B%98%7F7%D3%12%80%F3%BB%A4mm%13%60%B4%E1%B7%D9%C0j%11
- HIER_NONE/- text/html
1484742335.538      0 10.99.1.2 TAG_NONE/400 4234
%BB%E1%89%C5%01%DCd%95A-%D0%16%9B%98%7F7%D3%12%80%F3%BB%A4mm%13%60%B4%E1%B7%D9%C0j%11
- HIER_NONE/- text/html
1484742335.605      0 10.99.1.2 TAG_NONE/400 4444 NONE
error:invalid-request - HIER_NONE/- text/html
1484742335.691      0 10.99.1.2 TAG_NONE/400 4444 NONE
error:invalid-request - HIER_NONE/- text/html
1484742339.640      0 10.99.1.2 TAG_NONE/400 4022
%C6%CF%91Pv%85%82l%DEbD%1F%E0 - HIER_NONE/- text/html
1484742339.697      0 10.99.1.2 TAG_NONE/400 3918 GET / - HIER_NONE/- text/html
1484742339.885      0 10.99.1.2 TCP_DENIED/403 4556 GET
http://ip-172-31-9-90:3128/squid-internal-static/icons/SN.png -
HIER_NONE/- text/html
1484742340.105      0 10.99.1.2 TAG_NONE/400 3994 GET
/apple-touch-icon-76x76-precomposed.png - HIER_NONE/- text/html
1484742340.195      0 10.99.1.2 TAG_NONE/400 3970 GET
/apple-touch-icon-76x76.png - HIER_NONE/- text/html
1484742340.258      0 10.99.1.2 TAG_NONE/400 3958 GET
/apple-touch-icon.png - HIER_NONE/- text/html
1484742340.309      0 10.99.1.2 TAG_NONE/400 3958 GET
/apple-touch-icon.png - HIER_NONE/- text/html
1484742340.359      0 10.99.1.2 TAG_NONE/400 3982 GET
/apple-touch-icon-precomposed.png - HIER_NONE/- text/html
1484742340.413      0 10.99.1.2 TAG_NONE/400 3940 GET /favicon.ico -
HIER_NONE/- text/html
1484742378.858      0 10.99.1.2 TAG_NONE/400 4444 NONE
error:invalid-request - HIER_NONE/- text/html
1484742510.612      0 10.99.1.2 TAG_NONE/400 4444 NONE
error:invalid-request - HIER_NONE/- text/html
1484742517.730      0 10.99.1.2 TAG_NONE/400 4444 NONE
error:invalid-request - HIER_NONE/- text/html
1484744550.653      0 10.99.1.2 TAG_NONE/400 4174 GET
/MFYwVKADAgEAME0wSzBJMAkGBSsOAwIaBQAEFHQkFGcGn%2FXgmD9ePhproGUqVBV1BBQBWavn3ToLWaZkY9bPIAdX1ZHnagIQBHT%2BRrNCtgO6lb6fVDjflA%3D%3D
- HIER_NONE/- text/html
1484744597.163      0 10.99.1.1 TAG_NONE/400 4022 GET
/ac/globalnav/2.0/en_US/styles/ac-globalnav.built.css - HIER_NONE/-
text/html
1484744597.361      0 10.99.1.1 TAG_NONE/400 4034 GET
/ac/globalfooter/2.0/en_US/scripts/ac-globalfooter.built.js -
HIER_NONE/- text/html
1484744599.970      0 10.99.1.1 TAG_NONE/400 5352 GET
/b/ss/appleglobal,applehome,applestoreww,applestoreamr,applestoreus/1/H.27/s62860188740305?AQB=1&ndh=1&t=18%2F0%2F2017%2018%3A33%3A19%203%20-330&fid=21A4DCCB11396F92-26B205C305B2B2DF&pageName=apple%20-%20index%2Ftab%20%28us%29&g=http%3A%2F%2Fwww.apple.com%2F&cc=USD&ch=www.us.homepage&server=new%20approach%20ac-analytics&v3=aos%3A%20us&c4=D%3Dg&c5=ipad&c9=ios%209.3.5&c19=aos%3A%20us%3A%20apple%20-%20index%2Ftab%20%28us%29&c20=aos%3A%20us&c25=direct%20entry&c48=2&c49=D%3D2C39962A85032063-4000118780008FDC&v54=http%3A%2F%2Fwww.apple.com%2F&h1=www.us.homepage&s=768x1024&c=32&j=1.6&v=N&k=Y&bw=768&bh=960&AQE=1
- HIER_NONE/- text/html
1484744606.878      0 10.99.1.1 TAG_NONE/400 4022 GET
/ac/globalnav/2.0/en_US/styles/ac-globalnav.built.css - HIER_NONE/-
text/html
1484744606.879      0 10.99.1.1 TAG_NONE/400 4034 GET
/ac/globalfooter/2.0/en_US/scripts/ac-globalfooter.built.js -
HIER_NONE/- text/html
1484744608.852      0 10.99.1.1 TAG_NONE/400 5352 GET
/b/ss/appleglobal,applehome,applestoreww,applestoreamr,applestoreus/1/H.27/s68294376337435?AQB=1&ndh=1&t=18%2F0%2F2017%2018%3A33%3A28%203%20-330&fid=21A4DCCB11396F92-26B205C305B2B2DF&pageName=apple%20-%20index%2Ftab%20%28us%29&g=http%3A%2F%2Fwww.apple.com%2F&cc=USD&ch=www.us.homepage&server=new%20approach%20ac-analytics&v3=aos%3A%20us&c4=D%3Dg&c5=ipad&c9=ios%209.3.5&c19=aos%3A%20us%3A%20apple%20-%20index%2Ftab%20%28us%29&c20=aos%3A%20us&c25=direct%20entry&c48=3&c49=D%3D2C39962A85032063-4000118780008FDC&v54=http%3A%2F%2Fwww.apple.com%2F&h1=www.us.homepage&s=768x1024&c=32&j=1.6&v=N&k=Y&bw=768&bh=960&AQE=1
- HIER_NONE/- text/html
1484744615.457      0 10.99.1.1 TAG_NONE/400 4022 GET
/ac/globalnav/2.0/en_US/styles/ac-globalnav.built.css - HIER_NONE/-
text/html
1484744615.526      0 10.99.1.1 TAG_NONE/400 4008 GET
/metrics/ac-analytics/1.1/scripts/auto-init.js - HIER_NONE/- text/html
1484744615.587      0 10.99.1.1 TAG_NONE/400 4034 GET
/ac/globalfooter/2.0/en_US/scripts/ac-globalfooter.built.js -
HIER_NONE/- text/html
1484744625.891      0 10.99.1.1 TAG_NONE/400 3952 GET
/retail/geniusbar/ - HIER_NONE/- text/html
1484744626.062      0 10.99.1.1 TCP_MEM_HIT/200 11731 GET
http://ip-172-31-9-90:3128/squid-internal-static/icons/SN.png -
HIER_NONE/- image/png
1484744643.114      0 10.99.1.1 TAG_NONE/400 3918 GET / - HIER_NONE/- text/html
1484744643.268      0 10.99.1.1 TCP_MEM_HIT/200 11731 GET
http://ip-172-31-9-90:3128/squid-internal-static/icons/SN.png -
HIER_NONE/- image/png
1484746410.764      0 108.189.96.202 TAG_NONE/400 3923 GET / -
HIER_NONE/- text/html
1484751091.543      0 153.142.43.105 TAG_NONE/400 3923 GET / -
HIER_NONE/- text/html


My /etc/squid/squid.conf file has only one change and that is:
http_access allow all



Following is my /etc/ipsec.conf file:
config setup
 strictcrlpolicy=no
 uniqueids = no

conn %default
 mobike=yes
 dpdaction=clear
 dpddelay=35s
 dpdtimeout=200s
 fragmentation=yes

conn iOS-IKEV2
 auto=add
 keyexchange=ike
 eap_identity=%any
 left=%any
 leftsubnet=0.0.0.0/0
 rightsubnet=10.99.1.0/24
 leftauth=psk
 leftid=%any
 right=%any
 rightsourceip=10.99.1.0/24
 rightauth=eap-mschapv2
 rightid=%any

Following is NAT IPTables entries. I get this by entering sudo
iptables -t nat -L

Chain PREROUTING (policy ACCEPT)
target     prot opt source               destination
REDIRECT   tcp  --  anywhere             anywhere             tcp
dpt:http redir ports 3128

Chain INPUT (policy ACCEPT)
target     prot opt source               destination

Chain OUTPUT (policy ACCEPT)
target     prot opt source               destination

Chain POSTROUTING (policy ACCEPT)
target     prot opt source               destination
MASQUERADE  all  --  10.99.1.0/24  anywhere



If any of you have faced this problem before and was able to resolve
it, can you please help me? Thanks.

-- 
Regards,
Varun


From squid3 at treenet.co.nz  Thu Jan 19 07:40:16 2017
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 19 Jan 2017 20:40:16 +1300
Subject: [squid-users] A bunch of SSL errors I am not sure why
In-Reply-To: <5E0F128E-DEA3-41AA-9BD7-311953C6784F@solcv.com>
References: <2EBAFFFF-752F-45BC-B06A-E5513CE7D7BA@solcv.com>
 <049c01d26d33$fe058520$fa108f60$@ngtech.co.il>
 <2403F106-B288-4780-BF4F-06AE58E1A52F@solcv.com>
 <59772e12-aa11-d1c7-5543-13c98bb83419@treenet.co.nz>
 <04fd01d26e88$a68b7290$f3a257b0$@ngtech.co.il>
 <07335B2B-5956-42F8-A2DD-3CFA7E6E3158@solcv.com>
 <051401d26f0d$ad266bc0$07734340$@ngtech.co.il>
 <0E8D807B-C74E-476A-8F66-8BFE54429E4E@solcv.com>
 <7290de22-7fbc-2857-9a30-b05e33e3adfa@treenet.co.nz>
 <000c01d271b1$fbe0ced0$f3a26c70$@ngtech.co.il>
 <7f0acc05-8744-c8a3-070e-104d726fd88a@gmail.com>
 <5E0F128E-DEA3-41AA-9BD7-311953C6784F@solcv.com>
Message-ID: <3d6e409f-014e-d990-546c-3372c28a50f7@treenet.co.nz>

On 19/01/2017 12:53 p.m., Sameh Onaissi wrote:
> Hello, Amos? all
> 
> Yuri, thanks for the reply.
> 
> 
> Amos,
> 
> I added: Thanks to Eliezer)
> sslproxy_cert_error allow all
> sslproxy_flags DONT_VERIFY_PEER

That is a spot-check config to see if TLS is fully broken or if the fix
can be done in Squid. It should never, ever, ever, be used in a
production proxy.

> to the config file, I am not too worried about the verification since the accessed sites showing problems are government site or local paying services/partners.
> 

The peer verify is not about whether communication to them is safe (it
might not be even when verify succeeds).

It is about whether you are actually communicating with the right
destination or with some hijacker responding to your TCP connections.

In other words, to check that the endpoint you are sending those
financial details actually is your bank. Not mine.


The situation I am trying to get you to is checking the certs actually
belong to the right entity. But ignoring some minor(-ish) details like
missing CA in their cert chain, their bad choice of cipher etc.


> However, some sites are still showing the Handshake problem. https://ibin.co/38uz8akvWayM.png
> 
> You had previously replied to this saying:
> 
> "If you actually read that error message it tells you exactly what the
> problem is.
> 
> "Handshake with SSL server failed: [blah blah codes]: dh key too small"
> 
> The server is trying to use a Diffi-Helman cipher with a too-short key.
> DH cipher with short keys has recently been broken. By recently I mean
> about a whole year ago.?
> 
> However, I still wonder what the solution is? is it possible to fix this? and who needs to fix it? is it a squid side error? is it an OS level error?
> 

The only solution for that one is for the server admin to change/fix
their DH key settings to make it longer.

You are unlikely to be the only one having such problem, so with any
luck they will fix it soon. You can try to contact their admin and tell
them about the problem.

Amos



From squid at peralex.com  Thu Jan 19 09:13:42 2017
From: squid at peralex.com (squid at peralex.com)
Date: Thu, 19 Jan 2017 11:13:42 +0200
Subject: [squid-users] Will squid core dump with worker threads?
 Investigating squid crash, 3.5.23
In-Reply-To: <f03d70e4-0c65-79f0-0a49-54cf14b19406@treenet.co.nz>
References: <000001d26de4$93742840$ba5c78c0$@optimera.us>
 <f03d70e4-0c65-79f0-0a49-54cf14b19406@treenet.co.nz>
Message-ID: <o5q004$4hp$1@blaine.gmane.org>


>>
>> assertion failed: MemBuf.cc:216: "0 <= tailSize && tailSize <= cSize"
>>
> 
> This is <http://bugs.squid-cache.org/show_bug.cgi?id=4606>. We have


Is there a workaround for this - something that I can put in the config
perhaps?  I'm getting the same issue a few times a day.  I suspect it's
mainly due to clients accessing Windows Updates, but difficult to tell.

I am automatically restarting squid, but the delays for other users
while all this is happening can generate a poor browsing experience.

Thanks
Mark






From squid3 at treenet.co.nz  Thu Jan 19 09:29:09 2017
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 19 Jan 2017 22:29:09 +1300
Subject: [squid-users] Connect strongSwan and Squid on same server
In-Reply-To: <CABUhpQU1QN3hvX0xqxO51r8u7GrQcjZ8yjqfDP0RLnLu06YAFw@mail.gmail.com>
References: <CABUhpQU1QN3hvX0xqxO51r8u7GrQcjZ8yjqfDP0RLnLu06YAFw@mail.gmail.com>
Message-ID: <4a585c07-afc3-e0e4-6a67-45e1a1198448@treenet.co.nz>

On 19/01/2017 8:00 p.m., Varun Singh wrote:
> Hi,
> I have installed strongSwan and Squid HTTP Proxy on the same Ubuntu
> 16.04 server and I am trying to connect both. By connect I mean, I am
> trying to achieve following:
> 
> [VPN Client] <------> [VPN Server] <-> [Squid] <------> [Internet]
> 
> My objective is to connect a VPN client to VPN server and use Squid
> for filtering out blocked Urls. strongSwan and Squid work fine on
> their own. I can access internet when connected to VPN server and also
> when configured HTTP Proxy without VPN.
> 

Is the VPN acting as an interface on the client machine through which
trafffic is gatewayed?
 or as a transparent tunnel to the proxy?


> From what I understand, to achieve what I want, I am supposed to
> redirect incoming HTTP traffic from port 80 to port using IPTables. I
> enter following IPTables rule:
> 
> iptables -t nat -A PREROUTING -i eth0 -p tcp --dport 80 -j REDIRECT
> --to-port 3128
> 

What are the squid.conf ports configured as?

> Once I do this and try to access internet from a connected VPN client,
> I get error. Pasting a log of /var/log/squid/access.log
> 
> 

These are explicit-proxy requests (port 3128 syntax):

> 1484738365.632      0 114.143.194.190 TCP_DENIED/403 4066 CONNECT
> api-glb-sin.smoot.apple.com:443 - HIER_NONE/- text/html
> 1484738365.642      0 114.143.194.190 TCP_DENIED/403 4870 GET
> http://www.apple.com/ac/globalfooter/2.0/en_US/styles/ac-globalfooter.built.css
> - HIER_NONE/- text/html
> 1484738365.643      0 114.143.194.190 TCP_DENIED/403 4852 GET
> http://www.apple.com/ac/globalnav/2.0/en_US/styles/ac-globalnav.built.css
> - HIER_NONE/- text/html
> 1484738365.731      0 114.143.194.190 TCP_DENIED/403 4753 GET
> http://www.apple.com/wss/fonts/? - HIER_NONE/- text/html
> 1484738365.760      0 114.143.194.190 TCP_DENIED/403 4817 GET
> http://www.apple.com/metrics/ac-analytics/1.1/scripts/ac-analytics.js
> - HIER_NONE/- text/html
> 1484738367.798      0 114.143.194.190 TCP_DENIED/403 4066 CONNECT
> init.itunes.apple.com:443 - HIER_NONE/- text/html
> 1484738367.922      0 114.143.194.190 TCP_DENIED/403 4334 GET
> http://www.apple.com/apple-touch-icon-76x76-precomposed.png -
> HIER_NONE/- text/html
> 1484738367.963      0 114.143.194.190 TCP_DENIED/403 4025 CONNECT
> gsp10-ssl.apple.com:443 - HIER_NONE/- text/html
> 1484738368.036      0 114.143.194.190 TCP_DENIED/403 4298 GET
> http://www.apple.com/apple-touch-icon-76x76.png - HIER_NONE/-
> text/html
<snip>


What you are expected by to do on Debian and Ubuntu installs is setup
the "localnet" ACL to be apropriate for your LAN. It is commented out by
default.
 Search squid.conf for "#http_access allow localnet" and "#acl localnet"

When that is done the above should work. No NAT needed.


These are origin requests (port 80 syntax):

> 1484738858.272      0 10.99.1.1 TAG_NONE/400 4154 GET
> /assets/com_apple_MobileAsset_SafariCloudHistoryConfiguration/com_apple_MobileAsset_SafariCloudHistoryConfiguration.xml
> - HIER_NONE/- text/html
> 1484738858.990      0 10.99.1.1 TAG_NONE/400 4004 GET
> /us/shop/bag/status?apikey=SFX9YPYY9PPXCU9KH - HIER_NONE/- text/html
> 1484738860.362      0 10.99.1.1 TAG_NONE/400 5350 GET
> /b/ss/appleglobal,applehome,applestoreww,applestoreamr,applestoreus/1/H.27/s5505031635984?AQB=1&ndh=1&t=18%2F0%2F2017%2016%3A57%3A40%203%20-330&fid=21A4DCCB11396F92-26B205C305B2B2DF&pageName=apple%20-%20index%2Ftab%20%28us%29&g=http%3A%2F%2Fwww.apple.com%2F&cc=USD&ch=www.us.homepage&server=new%20approach%20ac-analytics&v3=aos%3A%20us&c4=D%3Dg&c5=ipad&c9=ios%209.3.5&c19=aos%3A%20us%3A%20apple%20-%20index%2Ftab%20%28us%29&c20=aos%3A%20us&c25=direct%20entry&c48=4&c49=D%3D2C39962A85032063-4000118780008FDC&v54=http%3A%2F%2Fwww.apple.com%2F&h1=www.us.homepage&s=768x1024&c=32&j=1.6&v=N&k=Y&bw=768&bh=960&AQE=1
> - HIER_NONE/- text/html
> 1484739056.258      0 10.99.1.1 TAG_NONE/400 3918 GET / - HIER_NONE/- text/html
> 1484739056.480      0 10.99.1.1 TCP_DENIED/403 4290 GET
> http://ip-172-31-9-90:3128/squid-internal-static/icons/SN.png -
> HIER_NONE/- text/html
> 1484739057.106      0 10.99.1.1 TAG_NONE/400 3994 GET
> /apple-touch-icon-76x76-precomposed.png - HIER_NONE/- text/html
<snip>

Notice how both those sets of requests are reaching your proxy properly.
The VPN is still working just fine.

> 
> 
> My /etc/squid/squid.conf file has only one change and that is:
> http_access allow all
> 

Where? order and position is important.


You have not added the squid.conf line required for Squid to receive the
iptables packets from NAT.
  <http://wiki.squid-cache.org/ConfigExamples/Intercept/LinuxRedirect>

Amos



From varun.singh at gslab.com  Thu Jan 19 10:13:27 2017
From: varun.singh at gslab.com (Varun Singh)
Date: Thu, 19 Jan 2017 15:43:27 +0530
Subject: [squid-users] Connect strongSwan and Squid on same server
In-Reply-To: <4a585c07-afc3-e0e4-6a67-45e1a1198448@treenet.co.nz>
References: <CABUhpQU1QN3hvX0xqxO51r8u7GrQcjZ8yjqfDP0RLnLu06YAFw@mail.gmail.com>
 <4a585c07-afc3-e0e4-6a67-45e1a1198448@treenet.co.nz>
Message-ID: <CABUhpQXoJMHs7NJN7JPuPqTv0BJ+wq8dg6JUN_GOO5XC9jW1pA@mail.gmail.com>

On Thu, Jan 19, 2017 at 2:59 PM, Amos Jeffries <squid3 at treenet.co.nz> wrote:
> On 19/01/2017 8:00 p.m., Varun Singh wrote:
>> Hi,
>> I have installed strongSwan and Squid HTTP Proxy on the same Ubuntu
>> 16.04 server and I am trying to connect both. By connect I mean, I am
>> trying to achieve following:
>>
>> [VPN Client] <------> [VPN Server] <-> [Squid] <------> [Internet]
>>
>> My objective is to connect a VPN client to VPN server and use Squid
>> for filtering out blocked Urls. strongSwan and Squid work fine on
>> their own. I can access internet when connected to VPN server and also
>> when configured HTTP Proxy without VPN.
>>
>
> Is the VPN acting as an interface on the client machine through which
> trafffic is gatewayed?
>  or as a transparent tunnel to the proxy?
>
>
>> From what I understand, to achieve what I want, I am supposed to
>> redirect incoming HTTP traffic from port 80 to port using IPTables. I
>> enter following IPTables rule:
>>
>> iptables -t nat -A PREROUTING -i eth0 -p tcp --dport 80 -j REDIRECT
>> --to-port 3128
>>
>
> What are the squid.conf ports configured as?
>
>> Once I do this and try to access internet from a connected VPN client,
>> I get error. Pasting a log of /var/log/squid/access.log
>>
>>
>
> These are explicit-proxy requests (port 3128 syntax):
>
>> 1484738365.632      0 114.143.194.190 TCP_DENIED/403 4066 CONNECT
>> api-glb-sin.smoot.apple.com:443 - HIER_NONE/- text/html
>> 1484738365.642      0 114.143.194.190 TCP_DENIED/403 4870 GET
>> http://www.apple.com/ac/globalfooter/2.0/en_US/styles/ac-globalfooter.built.css
>> - HIER_NONE/- text/html
>> 1484738365.643      0 114.143.194.190 TCP_DENIED/403 4852 GET
>> http://www.apple.com/ac/globalnav/2.0/en_US/styles/ac-globalnav.built.css
>> - HIER_NONE/- text/html
>> 1484738365.731      0 114.143.194.190 TCP_DENIED/403 4753 GET
>> http://www.apple.com/wss/fonts/? - HIER_NONE/- text/html
>> 1484738365.760      0 114.143.194.190 TCP_DENIED/403 4817 GET
>> http://www.apple.com/metrics/ac-analytics/1.1/scripts/ac-analytics.js
>> - HIER_NONE/- text/html
>> 1484738367.798      0 114.143.194.190 TCP_DENIED/403 4066 CONNECT
>> init.itunes.apple.com:443 - HIER_NONE/- text/html
>> 1484738367.922      0 114.143.194.190 TCP_DENIED/403 4334 GET
>> http://www.apple.com/apple-touch-icon-76x76-precomposed.png -
>> HIER_NONE/- text/html
>> 1484738367.963      0 114.143.194.190 TCP_DENIED/403 4025 CONNECT
>> gsp10-ssl.apple.com:443 - HIER_NONE/- text/html
>> 1484738368.036      0 114.143.194.190 TCP_DENIED/403 4298 GET
>> http://www.apple.com/apple-touch-icon-76x76.png - HIER_NONE/-
>> text/html
> <snip>
>
>
> What you are expected by to do on Debian and Ubuntu installs is setup
> the "localnet" ACL to be apropriate for your LAN. It is commented out by
> default.
>  Search squid.conf for "#http_access allow localnet" and "#acl localnet"
>
> When that is done the above should work. No NAT needed.
>
>
> These are origin requests (port 80 syntax):
>
>> 1484738858.272      0 10.99.1.1 TAG_NONE/400 4154 GET
>> /assets/com_apple_MobileAsset_SafariCloudHistoryConfiguration/com_apple_MobileAsset_SafariCloudHistoryConfiguration.xml
>> - HIER_NONE/- text/html
>> 1484738858.990      0 10.99.1.1 TAG_NONE/400 4004 GET
>> /us/shop/bag/status?apikey=SFX9YPYY9PPXCU9KH - HIER_NONE/- text/html
>> 1484738860.362      0 10.99.1.1 TAG_NONE/400 5350 GET
>> /b/ss/appleglobal,applehome,applestoreww,applestoreamr,applestoreus/1/H.27/s5505031635984?AQB=1&ndh=1&t=18%2F0%2F2017%2016%3A57%3A40%203%20-330&fid=21A4DCCB11396F92-26B205C305B2B2DF&pageName=apple%20-%20index%2Ftab%20%28us%29&g=http%3A%2F%2Fwww.apple.com%2F&cc=USD&ch=www.us.homepage&server=new%20approach%20ac-analytics&v3=aos%3A%20us&c4=D%3Dg&c5=ipad&c9=ios%209.3.5&c19=aos%3A%20us%3A%20apple%20-%20index%2Ftab%20%28us%29&c20=aos%3A%20us&c25=direct%20entry&c48=4&c49=D%3D2C39962A85032063-4000118780008FDC&v54=http%3A%2F%2Fwww.apple.com%2F&h1=www.us.homepage&s=768x1024&c=32&j=1.6&v=N&k=Y&bw=768&bh=960&AQE=1
>> - HIER_NONE/- text/html
>> 1484739056.258      0 10.99.1.1 TAG_NONE/400 3918 GET / - HIER_NONE/- text/html
>> 1484739056.480      0 10.99.1.1 TCP_DENIED/403 4290 GET
>> http://ip-172-31-9-90:3128/squid-internal-static/icons/SN.png -
>> HIER_NONE/- text/html
>> 1484739057.106      0 10.99.1.1 TAG_NONE/400 3994 GET
>> /apple-touch-icon-76x76-precomposed.png - HIER_NONE/- text/html
> <snip>
>
> Notice how both those sets of requests are reaching your proxy properly.
> The VPN is still working just fine.
>
>>
>>
>> My /etc/squid/squid.conf file has only one change and that is:
>> http_access allow all
>>
>
> Where? order and position is important.
>
>
> You have not added the squid.conf line required for Squid to receive the
> iptables packets from NAT.
>   <http://wiki.squid-cache.org/ConfigExamples/Intercept/LinuxRedirect>
>
> Amos
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users


Thanks. Doing the following solved the problem:
"You have not added the squid.conf line required for Squid to receive the
> iptables packets from NAT."

I think that is why Squid was not able to infer the packets received
on port 3128.

-- 
Regards,
Varun


From eduardoocarneiro at gmail.com  Thu Jan 19 13:37:00 2017
From: eduardoocarneiro at gmail.com (Eduardo Carneiro)
Date: Thu, 19 Jan 2017 05:37:00 -0800 (PST)
Subject: [squid-users] Users inserted incorrectly in access.log
Message-ID: <1484833020698-4681196.post@n4.nabble.com>

Hi everyone.

I have a environment with one frontend server and three parent servers in
culster. The frontend server receives all client connections and forward
them to parent servers. There is no exist any authentication method in the
frontend server. In the parent servers the requests are authenticated via
KERBEROS.

The problem is, when there are simultaneous accesses to any site, usernames,
many times, are inserted incorrectly in the access.log. Per example, the
user "userA" accesses microsoft.com, but on access.log, shows "userB".

On the frontend server, there are these three lines:

cache_peer server1.domain.com parent 8080 3130 round-robin sourcehash
no-query login=PASSTHRU connection-auth=on
cache_peer server2.domain.com parent 8080 3130 round-robin sourcehash
no-query login=PASSTHRU connection-auth=on
cache_peer server3.domain.com parent 8080 3130 round-robin sourcehash
no-query login=PASSTHRU connection-auth=on

I noticed that when the access is direct to some parent server, this problem
do not occurs. Only if connection pass by frontend.

Is this a bug?

Someone could help me? 




--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/Users-inserted-incorrectly-in-access-log-tp4681196.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From gksalil at gmail.com  Thu Jan 19 15:39:16 2017
From: gksalil at gmail.com (salil GK)
Date: Thu, 19 Jan 2017 21:09:16 +0530
Subject: [squid-users] Configuration for cache_peer doesn't work
Message-ID: <CAPACB-xfzwPhEkguSDOAxX4p6o8V8EiAc0MYnoTg=9WkL7Ws4Q@mail.gmail.com>

Hello

  I am new to squid and I have a use case that I need to configure a
forward proxy with squid. But there will be two squid servers chained to
isolate the networks. So when client machine wanted to access some internet
site, they will specify proxy as my first squid server. This proxy in turn
will forward the packet to squid server 2 and from there traffic will be
forwarded to origin server and response will come through the same path.

  I could achieve this by configuring cache_peer.

>>>>>  configuration in SquidServer1

http_port 3223

include "/etc/squid3/blockedhosts.lst"

http_access allow all

cache_peer 10.106.251.90 parent 3223 0 no-query default

<<<<<

So this will forward packets to SquidServer2 ( 10.106.251.90 ) and then
will be forwarded further from there to origin server


Now I want to make ssl connection between SquidServer1 and SquidServer2.

I tried the following line for cache_peer

>>>>

cache_peer 10.106.251.90 parent 3223 0 no-query default ssl
sslcert="/tmp/server_90.pem" sslkey="/tmp/privkey_90.pem"

<<<<<

But this doesn't work.

when I try to start quid - it gives the following error

>>>>>>

~ # /usr/sbin/squid3 -N -Y -d 5 -f /tmp/minsquid.conf

2017/01/19 21:04:24| parse_peer: token='ssl'

FATAL: Bungled minsquid.conf line 12: cache_peer 10.106.251.90 parent 3223
0 no-query default ssl sslcert="/tmp/server_90.pem"
sslkey="/tmp/privkey_90.pem"

Squid Cache (Version 3.1.19): Terminated abnormally.

CPU Usage: 0.004 seconds = 0.004 user + 0.000 sys

Maximum Resident Size: 28224 KB

Page faults with physical i/o: 0

<<<<<<

what could be the issue .

-----

In SquidServer2 I think I need to specify https port for the client to
access. I have put this line in config file

>>>>>

https_port 3224  cert=self_s_cert.pem key=key.pem

<<<<<

There while executing squid, getting the following error


>>>>

~ # /usr/sbin/squid3 -N -Y -d 5 -f /tmp/minsquid.conf

2017/01/19 15:37:40| cache_cf.cc(381) parseOneConfigFile: minsquid.conf:4
unrecognized: 'https_port'

<<<<


Thanks

~S
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20170119/474f9848/attachment.htm>

From alex.tate at gmail.com  Thu Jan 19 20:32:47 2017
From: alex.tate at gmail.com (roadrage27)
Date: Thu, 19 Jan 2017 12:32:47 -0800 (PST)
Subject: [squid-users] HTTPS site filtering
Message-ID: <1484857967970-4681198.post@n4.nabble.com>

I was able to solve my previous issue of no connections and now have a
working squid along with http site filtering and regex working nicely.

My current issue is the need to allow only certain sites which do include
some HTTPS sites.  If i leave the line

http_access deny CONNECT !SSL_ports

within my conf file, no HTTPS traffic works, commenting it out and putting
in

http_access allow CONNECT SSL_ports 

allows SSL but it allows all sites that are available to work with SSL to be
accessed.  

Is there a way to limit this access with an ACL and if so what is they
syntax?  I have been crawling over the squid docs and cannot get it figured
out at this point.

Thanks



--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/HTTPS-site-filtering-tp4681198.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From gksalil at gmail.com  Thu Jan 19 22:31:46 2017
From: gksalil at gmail.com (salil GK)
Date: Fri, 20 Jan 2017 04:01:46 +0530
Subject: [squid-users] Configuration for cache_peer doesn't work
Message-ID: <CAPACB-zvHJRcOtUybohf1mT+5XKKg9RpzszkWsEAJ=5Q4=MM4A@mail.gmail.com>

Could some one please provide me some information on this. This is a kind
of urgent for me now. Sorry for bothering too much.

Thanks
~S

On 19 January 2017 at 21:09, salil GK <gksalil at gmail.com> wrote:

> Hello
>
>   I am new to squid and I have a use case that I need to configure a
> forward proxy with squid. But there will be two squid servers chained to
> isolate the networks. So when client machine wanted to access some internet
> site, they will specify proxy as my first squid server. This proxy in turn
> will forward the packet to squid server 2 and from there traffic will be
> forwarded to origin server and response will come through the same path.
>
>   I could achieve this by configuring cache_peer.
>
> >>>>>  configuration in SquidServer1
>
> http_port 3223
>
> include "/etc/squid3/blockedhosts.lst"
>
> http_access allow all
>
> cache_peer 10.106.251.90 parent 3223 0 no-query default
>
> <<<<<
>
> So this will forward packets to SquidServer2 ( 10.106.251.90 ) and then
> will be forwarded further from there to origin server
>
>
> Now I want to make ssl connection between SquidServer1 and SquidServer2.
>
> I tried the following line for cache_peer
>
> >>>>
>
> cache_peer 10.106.251.90 parent 3223 0 no-query default ssl
> sslcert="/tmp/server_90.pem" sslkey="/tmp/privkey_90.pem"
>
> <<<<<
>
> But this doesn't work.
>
> when I try to start quid - it gives the following error
>
> >>>>>>
>
> ~ # /usr/sbin/squid3 -N -Y -d 5 -f /tmp/minsquid.conf
>
> 2017/01/19 21:04:24| parse_peer: token='ssl'
>
> FATAL: Bungled minsquid.conf line 12: cache_peer 10.106.251.90 parent 3223
> 0 no-query default ssl sslcert="/tmp/server_90.pem"
> sslkey="/tmp/privkey_90.pem"
>
> Squid Cache (Version 3.1.19): Terminated abnormally.
>
> CPU Usage: 0.004 seconds = 0.004 user + 0.000 sys
>
> Maximum Resident Size: 28224 KB
>
> Page faults with physical i/o: 0
>
> <<<<<<
>
> what could be the issue .
>
> -----
>
> In SquidServer2 I think I need to specify https port for the client to
> access. I have put this line in config file
>
> >>>>>
>
> https_port 3224  cert=self_s_cert.pem key=key.pem
>
> <<<<<
>
> There while executing squid, getting the following error
>
>
> >>>>
>
> ~ # /usr/sbin/squid3 -N -Y -d 5 -f /tmp/minsquid.conf
>
> 2017/01/19 15:37:40| cache_cf.cc(381) parseOneConfigFile: minsquid.conf:4
> unrecognized: 'https_port'
>
> <<<<
>
>
> Thanks
>
> ~S
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20170120/d5a3b5fb/attachment.htm>

From eliezer at ngtech.co.il  Thu Jan 19 22:32:59 2017
From: eliezer at ngtech.co.il (Eliezer  Croitoru)
Date: Fri, 20 Jan 2017 00:32:59 +0200
Subject: [squid-users] Squid 3.5.23 is available - Article and new tools by
	NgTech
Message-ID: <!&!AAAAAAAAAAAuAAAAAAAAAJAi8qAYKFtBkRXMK4znJhMBANlTnCJhprtFudq2LHCBs8EBACQA//8AABAAAADcFVnm0SrJR5Miw4+VMNVGAQAAAAA=@ngtech.co.il>

When is the kid considered stable?
Or
When is the software stable enough?
[http://www1.ngtech.co.il/wpe/wp-content/uploads/2017/01/dessert_sushi_by_outlawxvega-300x225.jpg]

Take a look at the page to get the full article: http://www1.ngtech.co.il/wpe/?p=374


Specially for this release I am releasing couple new tools based on the DRBL peers library I wrote.
On the plate:

* CA certificate test and installation html page[https://github.com/elico/ca-cert-test-page] (example page[http://moodle.ngtech.co.il/ca-test/])
* Windows Root CA installation script[https://github.com/elico/windows-rootca-autodeploy-create] (example page[http://ngtech.co.il/myca/])
* Debian and Ubuntu Stable  and Beta versions repository(without ecap support).. takes time to prepare
ICAP DRBL query service[http://moodle.ngtech.co.il/drbl-icap-service/]
? Package of Binaries Sources and scripts[http://moodle.ngtech.co.il/drbl-icap-service/]
? Sources and startup scripts on github[https://github.com/elico/drbl-icap-service]
? I have hope to publish the tool in RPM and DEB format
* Squid 4.0.17 Basic functionality tests .. takes time to prepare

References:
* Squid-Cache CentOS repository details[http://wiki.squid-cache.org/KnowledgeBase/CentOS#Squid-3.5]

Eliezer Croitoru

* If you need some help with the new tools contact me.

----
Eliezer Croitoru
Linux System Administrator
Mobile: +972-5-28704261
Email: eliezer at ngtech.co.il


-----Original Message-----
From: squid-users [mailto:squid-users-bounces at lists.squid-cache.org] On Behalf Of Amos Jeffries
Sent: Saturday, December 17, 2016 6:05 PM
To: squid-announce at lists.squid-cache.org
Subject: [squid-users] [squid-announce] Squid 3.5.23 is available

The Squid HTTP Proxy team is very pleased to announce the availability of the Squid-3.5.23 release!

<SNIP>
Amos Jeffries



From rentorbuy at yahoo.com  Fri Jan 20 00:03:59 2017
From: rentorbuy at yahoo.com (Vieri)
Date: Fri, 20 Jan 2017 00:03:59 +0000 (UTC)
Subject: [squid-users] squid reverse proxy (accelerator) for MS Exchange OWA
References: <1216522442.160455.1484870639800.ref@mail.yahoo.com>
Message-ID: <1216522442.160455.1484870639800@mail.yahoo.com>

Hi,

I'm trying to set up Squid as a reverse proxy on a host with IP address 10.215.144.91 so that web browsers can connect to it on port 443 and request pages from an OWA server at 10.215.144.21:443.

I have this in my squid.conf:

https_port 10.215.144.91:443 accel cert=/etc/ssl/squid/owa_cert.cer key=/etc/ssl/squid/owa_key.pem defaultsite=webmail2.mydomain.org

cache_peer 10.215.144.21 parent 443 0 no-query originserver login=PASS ssl sslcert=/etc/ssl/squid/client.cer sslkey=/etc/ssl/squid/client_key.pem ssloptions=ALL sslflags=DONT_VERIFY_PEER,DONT_VERIFY_DOMAIN name=owaServer
# cache_peer 10.215.144.21 parent 80 0 no-query originserver login=PASS front-end-https=on name=owaServer

acl OWA dstdomain webmail2.mydomain.org
cache_peer_access owaServer allow OWA
never_direct allow OWA

http_access allow OWA
http_access deny all
miss_access allow OWA
miss_access deny all

Note that if I comment out the "cache_peer parent 443" line above and uncomment the "cache_peer parent 80" line then the web browser client successfully connects and can view the OWA pages after logging in.

However, the connection fails if I use 443 between squid at 10.215.144.91 and the OWA backend at 10.215.144.21. The client views a Squid error page with an SSL handshake error.

Here's the cache log when I try to connect with a client:

2017/01/20 00:10:42.284 kid1| Error negotiating SSL on FD 16: error:00000000:lib(0):func(0):reason(0) (5/0/0)
2017/01/20 00:10:42.284 kid1| TCP connection to 10.215.144.21/443 failed
2017/01/20 00:10:42.285 kid1| 5,5| comm.cc(1038) comm_remove_close_handler: comm_remove_close_handler: FD 16, AsyncCall=0x80d93a00*2
2017/01/20 00:10:42.285 kid1| 9,5| AsyncCall.cc(56) cancel: will not call Ssl::PeerConnector::commCloseHandler [call453] because comm_remove_close_handler
2017/01/20 00:10:42.285 kid1| 17,4| AsyncCall.cc(93) ScheduleCall: PeerConnector.cc(742) will call FwdState::ConnectedToPeer(0x80d8b9f0, local=10.215.144.91:55948 remote=10.215.144.21:443 FD 16 flags=1, 0x809d49a0/0x809d49a0) [call451]
2017/01/20 00:10:42.285 kid1| 93,5| AsyncJob.cc(137) callEnd: Ssl::PeerConnector::negotiateSsl() ends job [ FD 16 job42]
2017/01/20 00:10:42.285 kid1| 83,5| PeerConnector.cc(58) ~PeerConnector: Peer connector 0x80d8b590 gone
2017/01/20 00:10:42.285 kid1| 93,5| AsyncJob.cc(40) ~AsyncJob: AsyncJob destructed, this=0x80d8b5b4 type=Ssl::PeerConnector [job42]
2017/01/20 00:10:42.285 kid1| 17,4| AsyncCallQueue.cc(55) fireNext: entering FwdState::ConnectedToPeer(0x80d8b9f0, local=10.215.144.91:55948 remote=10.215.144.21:443 FD 16 flags=1, 0x809d49a0/0x809d49a0)
2017/01/20 00:10:42.285 kid1| 17,4| AsyncCall.cc(38) make: make call FwdState::ConnectedToPeer [call451]
2017/01/20 00:10:42.285 kid1| 17,3| FwdState.cc(415) fail: ERR_SECURE_CONNECT_FAIL "Service Unavailable"
https://webmail2.mydomain.org/Exchange2/
2017/01/20 00:10:42.285 kid1| TCP connection to 10.215.144.21/443 failed

I don't understand the "Service Unavailable" bit above.
I can connect just fine from the command line on the squid server at 10.215.144.91 as you can see below.

# wget --no-check-certificate -O -  https://10.215.144.21 
--2017-01-20 00:41:10--  https://10.215.144.21/
Connecting to 10.215.144.21:443... connected.
WARNING: cannot verify 10.215.144.21's certificate, issued by '/C=xx/ST=xx/O=xx/OU=xx/CN=xxx/emailAddress=xx at xx.xxx':
Unable to locally verify the issuer's authority.
WARNING: certificate common name 'XYZ' doesn't match requested host name '10.215.144.21'.
HTTP request sent, awaiting response... 200 OK
Length: 1546 (1.5K) [text/html]

What can I try?

Thanks,

Vieri


From gksalil at gmail.com  Fri Jan 20 00:08:57 2017
From: gksalil at gmail.com (salil GK)
Date: Fri, 20 Jan 2017 05:38:57 +0530
Subject: [squid-users] Configuration for cache_peer doesn't work
In-Reply-To: <CAPACB-zvHJRcOtUybohf1mT+5XKKg9RpzszkWsEAJ=5Q4=MM4A@mail.gmail.com>
References: <CAPACB-zvHJRcOtUybohf1mT+5XKKg9RpzszkWsEAJ=5Q4=MM4A@mail.gmail.com>
Message-ID: <CAPACB-yA+g-6JWyK_E2ECk8ufTUdCvhzRyzm5mjsHo7KkwEtvA@mail.gmail.com>

could there be a problem with the ssl support
the output of `squid3 -v`

Squid Cache: Version 3.1.19

configure options:  '--build=x86_64-linux-gnu' '--prefix=/usr'
'--includedir=${prefix}/include' '--mandir=${prefix}/share/man'
'--infodir=${prefix}/share/info' '--sysconfdir=/etc' '--localstatedir=/var'
'--libexecdir=${prefix}/lib/squid3' '--srcdir=.'
'--disable-maintainer-mode' '--disable-dependency-tracking'
'--disable-silent-rules' '--datadir=/usr/share/squid3'
'--sysconfdir=/etc/squid3' '--mandir=/usr/share/man'
'--with-cppunit-basedir=/usr' '--enable-inline' '--enable-async-io=8'
'--enable-storeio=ufs,aufs,diskd' '--enable-removal-policies=lru,heap'
'--enable-delay-pools' '--enable-cache-digests' '--enable-underscores'
'--enable-icap-client' '--enable-follow-x-forwarded-for'
'--enable-auth=basic,digest,ntlm,negotiate'
'--enable-basic-auth-helpers=LDAP,MSNT,NCSA,PAM,SASL,SMB,YP,DB,POP3,getpwnam,squid_radius_auth,multi-domain-NTLM'
'--enable-ntlm-auth-helpers=smb_lm,'
'--enable-digest-auth-helpers=ldap,password'
'--enable-negotiate-auth-helpers=squid_kerb_auth'
'--enable-external-acl-helpers=ip_user,ldap_group,session,unix_group,wbinfo_group'
'--enable-arp-acl' '--enable-esi' '--enable-zph-qos' '--enable-wccpv2'
'--disable-translation' '--with-logdir=/var/log/squid3'
'--with-pidfile=/var/run/squid3.pid' '--with-filedescriptors=65536'
'--with-large-files' '--with-default-user=proxy' '--enable-linux-netfilter'
'build_alias=x86_64-linux-gnu' 'CFLAGS=-g -O2 -fPIE -fstack-protector
--param=ssp-buffer-size=4 -Wformat -Wformat-security
-Werror=format-security' 'LDFLAGS=-Wl,-Bsymbolic-functions -fPIE -pie
-Wl,-z,relro -Wl,-z,now' 'CPPFLAGS=-D_FORTIFY_SOURCE=2' 'CXXFLAGS=-g -O2
-fPIE -fstack-protector --param=ssp-buffer-size=4 -Wformat
-Wformat-security -Werror=format-security'
--with-squid=/build/squid3-nkylXD/squid3-3.1.19


I tried to recompile squid source with the following options


./configure --with-openssl --enable-ssl


When I build this binary and run, it throws error like this

~ # ~/squid -N -Y -d 5 -f /tmp/minsquid.conf

/tandberg/squid: /lib/x86_64/libcrypto.so.1.0.0: no version information
available (required by /tandberg/squid)

/tandberg/squid: /lib/x86_64/libssl.so.1.0.0: no version information
available (required by /tandberg/squid)

2017/01/20 05:35:57| ERROR: MIME Config Table
/usr/local/squid/etc/mime.conf: (2) No such file or directory

FATAL: MIME Config Table /usr/local/squid/etc/mime.conf: (2) No such file
or directory

Squid Cache (Version 3.1.23): Terminated abnormally.

CPU Usage: 0.032 seconds = 0.031 user + 0.001 sys

Maximum Resident Size: 28368 KB

Page faults with physical i/o: 0



On 20 January 2017 at 04:01, salil GK <gksalil at gmail.com> wrote:

> Could some one please provide me some information on this. This is a kind
> of urgent for me now. Sorry for bothering too much.
>
> Thanks
> ~S
>
>
> On 19 January 2017 at 21:09, salil GK <gksalil at gmail.com> wrote:
>
>> Hello
>>
>>   I am new to squid and I have a use case that I need to configure a
>> forward proxy with squid. But there will be two squid servers chained to
>> isolate the networks. So when client machine wanted to access some internet
>> site, they will specify proxy as my first squid server. This proxy in turn
>> will forward the packet to squid server 2 and from there traffic will be
>> forwarded to origin server and response will come through the same path.
>>
>>   I could achieve this by configuring cache_peer.
>>
>> >>>>>  configuration in SquidServer1
>>
>> http_port 3223
>>
>> include "/etc/squid3/blockedhosts.lst"
>>
>> http_access allow all
>>
>> cache_peer 10.106.251.90 parent 3223 0 no-query default
>>
>> <<<<<
>>
>> So this will forward packets to SquidServer2 ( 10.106.251.90 ) and then
>> will be forwarded further from there to origin server
>>
>>
>> Now I want to make ssl connection between SquidServer1 and SquidServer2.
>>
>> I tried the following line for cache_peer
>>
>> >>>>
>>
>> cache_peer 10.106.251.90 parent 3223 0 no-query default ssl
>> sslcert="/tmp/server_90.pem" sslkey="/tmp/privkey_90.pem"
>>
>> <<<<<
>>
>> But this doesn't work.
>>
>> when I try to start quid - it gives the following error
>>
>> >>>>>>
>>
>> ~ # /usr/sbin/squid3 -N -Y -d 5 -f /tmp/minsquid.conf
>>
>> 2017/01/19 21:04:24| parse_peer: token='ssl'
>>
>> FATAL: Bungled minsquid.conf line 12: cache_peer 10.106.251.90 parent
>> 3223 0 no-query default ssl sslcert="/tmp/server_90.pem"
>> sslkey="/tmp/privkey_90.pem"
>>
>> Squid Cache (Version 3.1.19): Terminated abnormally.
>>
>> CPU Usage: 0.004 seconds = 0.004 user + 0.000 sys
>>
>> Maximum Resident Size: 28224 KB
>>
>> Page faults with physical i/o: 0
>>
>> <<<<<<
>>
>> what could be the issue .
>>
>> -----
>>
>> In SquidServer2 I think I need to specify https port for the client to
>> access. I have put this line in config file
>>
>> >>>>>
>>
>> https_port 3224  cert=self_s_cert.pem key=key.pem
>>
>> <<<<<
>>
>> There while executing squid, getting the following error
>>
>>
>> >>>>
>>
>> ~ # /usr/sbin/squid3 -N -Y -d 5 -f /tmp/minsquid.conf
>>
>> 2017/01/19 15:37:40| cache_cf.cc(381) parseOneConfigFile: minsquid.conf:4
>> unrecognized: 'https_port'
>>
>> <<<<
>>
>>
>> Thanks
>>
>> ~S
>>
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20170120/90fc45f1/attachment.htm>

From xeron.oskom at gmail.com  Fri Jan 20 00:23:54 2017
From: xeron.oskom at gmail.com (Ivan Larionov)
Date: Thu, 19 Jan 2017 16:23:54 -0800
Subject: [squid-users] squid 3.5.23 memory usage
Message-ID: <CAHvB88xx6-p2fzTRc77+meeYrZqB1rZzNnuSfruu_9k1e0tNZA@mail.gmail.com>

Hello.

I'm pretty sure this question has been asked multiple times already, but
after reading everything I found I still can't figure out squid memory
usage patterns.

We're currently trying to upgrade from squid 2.7 to squid 3.5 and memory
usage on squid 3 is much much higher compared to squid 2 with the same
configuration.

What do I see:

squid running for several days with low traffic:

# top
 PID USER      PR  NI  VIRT  RES  SHR S %CPU %MEM    TIME+  COMMAND
 7367 squid     20   0 4780m 4.4g 5224 S  6.0 60.6 105:01.76 squid -N

So it uses 4.4GB resident memory. Ok, let's see important config options:

cache_mem 2298756 KB
maximum_object_size_in_memory 8 KB
memory_replacement_policy lru
cache_replacement_policy lru

cache_dir aufs /mnt/services/squid/cache 445644 16 256

minimum_object_size 64 bytes # none-zero so we dont cache mistakes
maximum_object_size 102400 KB

So we configured 2.2GB memory cache and 500GB disk cache. Disk cache is
quite big but current usage is only 3GB:

# du -sh /mnt/services/squid/cache # cache_dir
3.0G  /mnt/services/squid/cache

Now I'm looking into this page
http://wiki.squid-cache.org/SquidFaq/SquidMemory and see:

14 MB of memory per 1 GB on disk for 64-bit Squid

Which means disk cache should use ~50MB of RAM.

All these means we have ~2.2GB ram used for everything else except
cache_mem and disk cache index.

Let's see top pools from mgr:mem:

Pool                  (KB)     %Tot
mem_node              2298833  55.082
Short Strings         622365   14.913
HttpHeaderEntry       404531   9.693
Long Strings          284520   6.817
MemObject             182288   4.368
HttpReply             155612   3.729
StoreEntry            73965    1.772
Medium Strings        71152    1.705
cbdata MemBuf (12)    35573    0.852
LRU policy node       30403    0.728
MD5 digest            11380    0.273
16K Buffer            1056     0.025

These pools consume ~35% of total squid memory usage: Short Strings,
HttpHeaderEntry, Long Strings, HttpReply. Looks suspicious. On squid 2 same
pools use 10 times less memory.

I found a bug which looks similar to our experience:
http://bugs.squid-cache.org/show_bug.cgi?id=4084.

I'm attaching our config, mgr:info, mgr:mem and some system info I
collected.

Could someone say if this is normal and why it's so much different from
squid 2?

-- 
With best regards, Ivan Larionov.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20170119/6acfe4e2/attachment.htm>
-------------- next part --------------
HTTP/1.1 200 OK
Server: squid/3.5.23
Mime-Version: 1.0
Date: Thu, 19 Jan 2017 23:39:50 GMT
Content-Type: text/plain;charset=utf-8
Expires: Thu, 19 Jan 2017 23:39:50 GMT
Last-Modified: Thu, 19 Jan 2017 23:39:50 GMT
X-Cache: MISS from ip-172-22-10-120
X-Cache-Lookup: MISS from ip-172-22-10-120:3128
Connection: close

Squid Object Cache: Version 3.5.23
Build Info: 
Service Name: squid
Start Time:	Fri, 13 Jan 2017 23:35:32 GMT
Current Time:	Thu, 19 Jan 2017 23:39:50 GMT
Connection information for squid:
	Number of clients accessing cache:	(client_db off)
	Number of HTTP requests received:	8195690
	Number of ICP messages received:	0
	Number of ICP messages sent:	0
	Number of queued ICP replies:	0
	Number of HTCP messages received:	0
	Number of HTCP messages sent:	0
	Request failure ratio:	 0.00
	Average HTTP requests per minute since start:	948.1
	Average ICP messages per minute since start:	0.0
	Select loop called: 73529108 times, 7.054 ms avg
Cache information for squid:
	Hits as % of all requests:	5min: 29.2%, 60min: 28.9%
	Hits as % of bytes sent:	5min: 89.0%, 60min: 89.1%
	Memory hits as % of hit requests:	5min: 0.0%, 60min: 0.0%
	Disk hits as % of hit requests:	5min: 100.0%, 60min: 100.0%
	Storage Swap size:	2915344 KB
	Storage Swap capacity:	 0.6% used, 99.4% free
	Storage Mem size:	2276524 KB
	Storage Mem capacity:	99.0% used,  1.0% free
	Mean Object Size:	4.00 KB
	Requests given to unlinkd:	0
Median Service Times (seconds)  5 min    60 min:
	HTTP Requests (All):   0.01745  0.01745
	Cache Misses:          0.02899  0.02451
	Cache Hits:            0.00091  0.00091
	Near Hits:             0.00000  0.00000
	Not-Modified Replies:  0.00000  0.00000
	DNS Lookups:           0.00000  0.00094
	ICP Queries:           0.00000  0.00000
Resource usage for squid:
	UP Time:	518657.265 seconds
	CPU Time:	6265.444 seconds
	CPU Usage:	1.21%
	CPU Usage, 5 minute avg:	6.43%
	CPU Usage, 60 minute avg:	5.11%
	Maximum Resident Size: 18579360 KB
	Page faults with physical i/o: 0
Memory accounted for:
	Total accounted:       -20826 KB
	memPoolAlloc calls: 2192400061
	memPoolFree calls:  2194290230
File descriptor usage for squid:
	Maximum number of file descriptors:   524288
	Largest file desc currently in use:     70
	Number of file desc currently in use:   44
	Files queued for open:                   0
	Available number of file descriptors: 524244
	Reserved number of file descriptors:   100
	Store Disk files open:                   0
Internal Data Structures:
	728426 StoreEntries
	569027 StoreEntries with MemObjects
	569025 Hot Object Cache Items
	728372 on-disk objects
-------------- next part --------------
A non-text attachment was scrubbed...
Name: squid_memory_3.tsv
Type: text/tab-separated-values
Size: 14636 bytes
Desc: not available
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20170119/6acfe4e2/attachment.tsv>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: squid.conf
Type: application/octet-stream
Size: 3298 bytes
Desc: not available
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20170119/6acfe4e2/attachment.obj>
-------------- next part --------------
# free -m

             total       used       free     shared    buffers     cached
Mem:          7482       7349        133          0        189       1354
-/+ buffers/cache:       5805       1677
Swap:            0          0          0

# df -h

Filesystem                   Size  Used Avail Use% Mounted on
/dev/xvda1                   9.8G  3.3G  6.4G  34% /
devtmpfs                     3.7G   68K  3.7G   1% /dev
tmpfs                        3.7G   44K  3.7G   1% /dev/shm
/dev/mapper/vg-data--master   29G  1.1G   28G   4% /mnt
/dev/xvdh                    504G  3.0G  476G   1% /mnt/services/squid/cache

# du -sh /mnt/services/squid/cache

3.0G  /mnt/services/squid/cache

# top

 PID USER      PR  NI  VIRT  RES  SHR S %CPU %MEM    TIME+  COMMAND
 7367 squid     20   0 4780m 4.4g 5224 S  6.0 60.6 105:01.76 squid -N

# squid -v
Squid Cache: Version 3.5.23
Service Name: squid
configure options:  '--program-prefix=' '--prefix=/usr' '--exec-prefix=/usr' '--bindir=/usr/sbin' '--sbindir=/usr/sbin' '--sysconfdir=/etc/squid' '--libdir=/usr/lib' '--libexecdir=/usr/lib/squid' '--includedir=/usr/include' '--datadir=/usr/share' '--sharedstatedir=/usr/com' '--localstatedir=/var' '--mandir=/usr/share/man' '--infodir=/usr/share/info' '--enable-epoll' '--enable-removal-policies=heap,lru' '--enable-storeio=aufs' '--enable-delay-pools' '--with-pthreads' '--enable-cache-digests' '--enable-useragent-log' '--enable-referer-log' '--with-large-files' '--with-maxfd=16384' '--enable-err-languages=English' '--enable-htcp'

From creditu at eml.cc  Fri Jan 20 02:01:11 2017
From: creditu at eml.cc (creditu at eml.cc)
Date: Thu, 19 Jan 2017 19:01:11 -0700
Subject: [squid-users] Dst and dstdomain ACLs
Message-ID: <1484877671.2918253.853578648.210B72F5@webmail.messagingengine.com>

Had a question about dst and dstdomain acls.  Given the sample below:

http_port 192.168.100.1:80 accel defaultsite=www.example.com vhost
acl www dstdomain www.example.com dev.example.com
cache_peer 10.10.10.1 parent 80 0 no-query no-digest originserver
round-robin
cache_peer_access 10.10.10.1 allow www
cache_peer_access 10.10.10.1 deny all
.......
http_access allow www
http_access deny all

When someone tries to access the site by specifying an IP
(192.168.100.1) instead of the name the client gets a standard access
denied squid page.  It seems that a separate acl needs to be defined for
when someone tries to access the site using an IP?  For instance:
acl dst www_ip 192.168.100.1
 
If we wanted to pass to the backend we would need to add a extra
cache_peer_access statement
 cache_peer_access 10.10.10.1 allow www_ip

Then add:
http_access allow www_ip

Is that correct?  If we wanted to not allow IP based requests we would
still define the acl and use a http_access deny www_ip  and then use
deny_info to redirect or send a TCP Reset?  Thanks.


From squid3 at treenet.co.nz  Fri Jan 20 06:19:54 2017
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Fri, 20 Jan 2017 19:19:54 +1300
Subject: [squid-users] Users inserted incorrectly in access.log
In-Reply-To: <1484833020698-4681196.post@n4.nabble.com>
References: <1484833020698-4681196.post@n4.nabble.com>
Message-ID: <f59d8ff9-6ea5-cc28-9675-ff47225042a1@treenet.co.nz>

On 20/01/2017 2:37 a.m., Eduardo Carneiro wrote:
> Hi everyone.
> 
> I have a environment with one frontend server and three parent servers in
> culster. The frontend server receives all client connections and forward
> them to parent servers. There is no exist any authentication method in the
> frontend server. In the parent servers the requests are authenticated via
> KERBEROS.
> 
> The problem is, when there are simultaneous accesses to any site, usernames,
> many times, are inserted incorrectly in the access.log. Per example, the
> user "userA" accesses microsoft.com, but on access.log, shows "userB".
> 
> On the frontend server, there are these three lines:
> 
> cache_peer server1.domain.com parent 8080 3130 round-robin sourcehash
> no-query login=PASSTHRU connection-auth=on
> cache_peer server2.domain.com parent 8080 3130 round-robin sourcehash
> no-query login=PASSTHRU connection-auth=on
> cache_peer server3.domain.com parent 8080 3130 round-robin sourcehash
> no-query login=PASSTHRU connection-auth=on


Please start by selecting one of round-robin and sourcehash. They are
very different selection algorithms.

Given that Kerberos auth requires HTTP/1 multiplexing to be disabled for
the auth to work I suggest that you drop the round-robin. It forces
multiplexing to be used.

If the problem still remains try adding the connection-auth=on to those
Squid's listening ports as well.


> 
> I noticed that when the access is direct to some parent server, this problem
> do not occurs. Only if connection pass by frontend.
> 
> Is this a bug?
> 

Maybe. What version of Squid are you using?
This was a problem back in 3.2 and older IIRC.

Amos



From squid3 at treenet.co.nz  Fri Jan 20 06:45:31 2017
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Fri, 20 Jan 2017 19:45:31 +1300
Subject: [squid-users] Configuration for cache_peer doesn't work
In-Reply-To: <CAPACB-xfzwPhEkguSDOAxX4p6o8V8EiAc0MYnoTg=9WkL7Ws4Q@mail.gmail.com>
References: <CAPACB-xfzwPhEkguSDOAxX4p6o8V8EiAc0MYnoTg=9WkL7Ws4Q@mail.gmail.com>
Message-ID: <8f8c4ab9-f4d3-4050-79c1-18e5936ffa17@treenet.co.nz>

On 20/01/2017 4:39 a.m., salil GK wrote:
> Hello
> 
>   I am new to squid and I have a use case that I need to configure a
> forward proxy with squid. But there will be two squid servers chained to
> isolate the networks. So when client machine wanted to access some internet
> site, they will specify proxy as my first squid server. This proxy in turn
> will forward the packet to squid server 2 and from there traffic will be
> forwarded to origin server and response will come through the same path.
> 

Okay. Reasonable.

<snip>
> ~ # /usr/sbin/squid3 -N -Y -d 5 -f /tmp/minsquid.conf
> 
> 2017/01/19 21:04:24| parse_peer: token='ssl'
> 
> FATAL: Bungled minsquid.conf line 12: cache_peer 10.106.251.90 parent 3223
> 0 no-query default ssl sslcert="/tmp/server_90.pem"
> sslkey="/tmp/privkey_90.pem"
> 
> Squid Cache (Version 3.1.19): Terminated abnormally.

You appear to be using an extremely old verson of Squid. Is this the
Ubuntu Precise by chance?

The Debian/Ubuntu packages are not (for legal reasons) linked to
OpenSSL. So they cannot be configured with any TLS/SSL settings like this.

You will have to make a custom build of Squid.

Amos



From goal81 at gmail.com  Fri Jan 20 08:40:16 2017
From: goal81 at gmail.com (Alexander)
Date: Fri, 20 Jan 2017 11:40:16 +0300
Subject: [squid-users] Native FTP relay: connection closes (?) after 'cannot
 assign requested address' error
Message-ID: <CACMUsGz1rDaY4ffe7vRp-u-cOANuufPfVo+BgvNTmi3t8ywOMA@mail.gmail.com>

Hello, I have a question regarding a native FTP relay (squid's version is
3.5.23).

I've tried to test this feature like this:

[Filezilla Client, 1.1.1.2] <-----> [ Router: iptables + squid ]
<-----> [vsftpd server, 5.5.5.10]

The router is CentOS 6.5 machine. Firewall settings are:

ip route flush table 100
ip rule add fwmark 1 lookup 100
ip route add local 0.0.0.0/0 dev lo table 100

iptables -t mangle -N DIVERT
iptables -t mangle -A DIVERT -j MARK --set-mark 0x01/0x01
iptables -t mangle -A DIVERT -j ACCEPT
iptables -t mangle -A PREROUTING -p tcp -m socket -j DIVERT
iptables -t mangle -A PREROUTING -p tcp --dport 21 -j TPROXY
--tproxy-mark 0x1/0x1 --on-port 2121
iptables -t mangle -A PREROUTING -p tcp --dport 80 -j TPROXY
--tproxy-mark 0x1/0x1 --on-port 3128

No other rules are defined and default policy for INPUT/OUTPUT/FORWARD is
ACCEPT. The rp_filter is disabled.

Squid's configuration file is attached.

With HTTP everything works fine, however FTP causes a problem. A client
successfully connects and authenticates, but when it tries to execute LIST
or RETR (when data connection should be established), Filezilla says
"Connection closed by server". Meanwhile squid says the following:

commBind: Cannot bind socket FD 17 to 1.1.1.2: (99) Cannot assign requested
address

What can be wrong with this setup?
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20170120/0542ce77/attachment.htm>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: squid.conf
Type: application/octet-stream
Size: 1485 bytes
Desc: not available
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20170120/0542ce77/attachment.obj>

From squid3 at treenet.co.nz  Fri Jan 20 08:42:19 2017
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Fri, 20 Jan 2017 21:42:19 +1300
Subject: [squid-users] Dst and dstdomain ACLs
In-Reply-To: <1484877671.2918253.853578648.210B72F5@webmail.messagingengine.com>
References: <1484877671.2918253.853578648.210B72F5@webmail.messagingengine.com>
Message-ID: <d1e9824d-9c48-a1aa-535d-b77c3446dff7@treenet.co.nz>

On 20/01/2017 3:01 p.m., creditu wrote:
> Had a question about dst and dstdomain acls.  Given the sample below:
> 
> http_port 192.168.100.1:80 accel defaultsite=www.example.com vhost
> acl www dstdomain www.example.com dev.example.com
> cache_peer 10.10.10.1 parent 80 0 no-query no-digest originserver
> round-robin
> cache_peer_access 10.10.10.1 allow www
> cache_peer_access 10.10.10.1 deny all
> .......
> http_access allow www
> http_access deny all
> 
> When someone tries to access the site by specifying an IP
> (192.168.100.1) instead of the name the client gets a standard access
> denied squid page.

What is the rDNS for 192.168.100.1 ?

The dstdomain you have configured only the exact two domains listed to
match.

>  It seems that a separate acl needs to be defined for
> when someone tries to access the site using an IP?  For instance:
> acl dst www_ip 192.168.100.1

You could add the raw-IP to the www ACL:
 acl www dstdomain -n 192.168.100.1

 ... but what will 10.10.10.1 do when asked for the site hosted at
192.168.100.1 ?


>  
> If we wanted to pass to the backend we would need to add a extra
> cache_peer_access statement
>  cache_peer_access 10.10.10.1 allow www_ip
> 
> Then add:
> http_access allow www_ip
> 
> Is that correct?

Not for matching raw-IP. The dst will match also for any domain name
that resolves to the IP given.

If you want an ACL that matches the textual representation of the raw-IP
you need to use dsdomain with the -n (no DNS lookup) flag, or the
dstdom_regex type.

>  If we wanted to not allow IP based requests we would
> still define the acl and use a http_access deny www_ip  and then use
> deny_info to redirect or send a TCP Reset?

That is another way, and somewhat better than just accepting the raw-IP
URLs to the backend server.


Amos




From squid3 at treenet.co.nz  Fri Jan 20 08:51:38 2017
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Fri, 20 Jan 2017 21:51:38 +1300
Subject: [squid-users] HTTPS site filtering
In-Reply-To: <1484857967970-4681198.post@n4.nabble.com>
References: <1484857967970-4681198.post@n4.nabble.com>
Message-ID: <35739dd3-a2b7-d6e5-9099-9503edc891c5@treenet.co.nz>

On 20/01/2017 9:32 a.m., roadrage27 wrote:
> I was able to solve my previous issue of no connections and now have a
> working squid along with http site filtering and regex working nicely.
> 
> My current issue is the need to allow only certain sites which do include
> some HTTPS sites.  If i leave the line
> 
> http_access deny CONNECT !SSL_ports
> 
> within my conf file, no HTTPS traffic works,

That tells me either you have screwed up the CONNECT ACL definition. Or
the SSL_ports one.

I suspect that whatever you have done is making HTTPS no longer use port
443. That needs to be fixed.


> commenting it out and putting
> in
> 
> http_access allow CONNECT SSL_ports 
> 
> allows SSL but it allows all sites that are available to work with SSL to be
> accessed.  
>

Quite. The security protection intended by that rule is to deny the
identifiably bad things and let your custom rules that follow decide
what is allowed.


> Is there a way to limit this access with an ACL and if so what is they
> syntax?

The required syntax is the default:

 acl SSL_Ports port 443
 acl CONNECT method CONNECT
 http_access deny CONNECT !SSL_Ports

Since you say that is not working, the problem is elsewhere and ACL
definition will not solve the breakage.

If you still need help, we will need to see what your squid.conf
contains currently. And if you are intercepting, the rules used for
doing that.

Amos



From squid3 at treenet.co.nz  Fri Jan 20 09:13:19 2017
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Fri, 20 Jan 2017 22:13:19 +1300
Subject: [squid-users] squid reverse proxy (accelerator) for MS Exchange
 OWA
In-Reply-To: <1216522442.160455.1484870639800@mail.yahoo.com>
References: <1216522442.160455.1484870639800.ref@mail.yahoo.com>
 <1216522442.160455.1484870639800@mail.yahoo.com>
Message-ID: <cdea0afa-c2a2-23c4-dbac-553a565e83f7@treenet.co.nz>

On 20/01/2017 1:03 p.m., Vieri wrote:
> Hi,
> 
> I'm trying to set up Squid as a reverse proxy on a host with IP address 10.215.144.91 so that web browsers can connect to it on port 443 and request pages from an OWA server at 10.215.144.21:443.
> 
> I have this in my squid.conf:
> 
> https_port 10.215.144.91:443 accel cert=/etc/ssl/squid/owa_cert.cer key=/etc/ssl/squid/owa_key.pem defaultsite=webmail2.mydomain.org
> 
> cache_peer 10.215.144.21 parent 443 0 no-query originserver login=PASS ssl sslcert=/etc/ssl/squid/client.cer sslkey=/etc/ssl/squid/client_key.pem ssloptions=ALL sslflags=DONT_VERIFY_PEER,DONT_VERIFY_DOMAIN name=owaServer
> # cache_peer 10.215.144.21 parent 80 0 no-query originserver login=PASS front-end-https=on name=owaServer
> 

The ssloptions and sslflags values are bad for debugging.
 * The ssloptions=ALL may be adding unknown new issues OWA does not like
from all the insecure things it turns on in Squids TLS handshake.
 * The sslflags disabling verify actions is hiding from you any issues
Squid will have with the OWA handshake security settings.

Knowing what all that hidden stuff does is important to finding the
right fixes or workarounds.



> acl OWA dstdomain webmail2.mydomain.org
> cache_peer_access owaServer allow OWA
> never_direct allow OWA
> 
> http_access allow OWA
> http_access deny all
> miss_access allow OWA
> miss_access deny all

The miss_access is not useful. Your earlier *_access rules already
prevent the unwanted things happening.

> 
> Note that if I comment out the "cache_peer parent 443" line above and uncomment the "cache_peer parent 80" line then the web browser client successfully connects and can view the OWA pages after logging in.
> 
> However, the connection fails if I use 443 between squid at 10.215.144.91 and the OWA backend at 10.215.144.21. The client views a Squid error page with an SSL handshake error.
> 
> Here's the cache log when I try to connect with a client:
> 
> 2017/01/20 00:10:42.284 kid1| Error negotiating SSL on FD 16: error:00000000:lib(0):func(0):reason(0) (5/0/0)
> 2017/01/20 00:10:42.284 kid1| TCP connection to 10.215.144.21/443 failed
> 2017/01/20 00:10:42.285 kid1| 5,5| comm.cc(1038) comm_remove_close_handler: comm_remove_close_handler: FD 16, AsyncCall=0x80d93a00*2
> 2017/01/20 00:10:42.285 kid1| 9,5| AsyncCall.cc(56) cancel: will not call Ssl::PeerConnector::commCloseHandler [call453] because comm_remove_close_handler
> 2017/01/20 00:10:42.285 kid1| 17,4| AsyncCall.cc(93) ScheduleCall: PeerConnector.cc(742) will call FwdState::ConnectedToPeer(0x80d8b9f0, local=10.215.144.91:55948 remote=10.215.144.21:443 FD 16 flags=1, 0x809d49a0/0x809d49a0) [call451]
> 2017/01/20 00:10:42.285 kid1| 93,5| AsyncJob.cc(137) callEnd: Ssl::PeerConnector::negotiateSsl() ends job [ FD 16 job42]
> 2017/01/20 00:10:42.285 kid1| 83,5| PeerConnector.cc(58) ~PeerConnector: Peer connector 0x80d8b590 gone
> 2017/01/20 00:10:42.285 kid1| 93,5| AsyncJob.cc(40) ~AsyncJob: AsyncJob destructed, this=0x80d8b5b4 type=Ssl::PeerConnector [job42]
> 2017/01/20 00:10:42.285 kid1| 17,4| AsyncCallQueue.cc(55) fireNext: entering FwdState::ConnectedToPeer(0x80d8b9f0, local=10.215.144.91:55948 remote=10.215.144.21:443 FD 16 flags=1, 0x809d49a0/0x809d49a0)
> 2017/01/20 00:10:42.285 kid1| 17,4| AsyncCall.cc(38) make: make call FwdState::ConnectedToPeer [call451]
> 2017/01/20 00:10:42.285 kid1| 17,3| FwdState.cc(415) fail: ERR_SECURE_CONNECT_FAIL "Service Unavailable"
> https://webmail2.mydomain.org/Exchange2/
> 2017/01/20 00:10:42.285 kid1| TCP connection to 10.215.144.21/443 failed
> 
> I don't understand the "Service Unavailable" bit above.

It is Squid telling the client it cannot proxy the request. Because the
cache_peer failed. Just one of the symptoms of whatever the problem is.

The key part is the "Error negotiating SSL on FD 16:
error:00000000:lib(0):func(0):reason(0) (5/0/0)"

Which is OpenSSL's very obtuse way of telling Squid "an error
rhappened". With no helpful details about what error it was.

> I can connect just fine from the command line on the squid server at 10.215.144.91 as you can see below.
> 
> # wget --no-check-certificate -O -  https://10.215.144.21 
> --2017-01-20 00:41:10--  https://10.215.144.21/
> Connecting to 10.215.144.21:443... connected.
> WARNING: cannot verify 10.215.144.21's certificate, issued by '/C=xx/ST=xx/O=xx/OU=xx/CN=xxx/emailAddress=xx at xx.xxx':
> Unable to locally verify the issuer's authority.
> WARNING: certificate common name 'XYZ' doesn't match requested host name '10.215.144.21'.

Firstly remove the ssloptions=ALL from your config.

Traffic should be able to go through at that point. But dont take that
as "working", the TLS layer is not in any way secure yet.
 - if not try the front-end-https setting I mentioned earlier.

Then add a sslcafile= option to tell Squid the CA cert which signed the
OWA servers certificate.

Then remove the sslflags option. Traffic should stay working with that.

Amos



From squid3 at treenet.co.nz  Fri Jan 20 09:23:11 2017
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Fri, 20 Jan 2017 22:23:11 +1300
Subject: [squid-users] Native FTP relay: connection closes (?) after
 'cannot assign requested address' error
In-Reply-To: <CACMUsGz1rDaY4ffe7vRp-u-cOANuufPfVo+BgvNTmi3t8ywOMA@mail.gmail.com>
References: <CACMUsGz1rDaY4ffe7vRp-u-cOANuufPfVo+BgvNTmi3t8ywOMA@mail.gmail.com>
Message-ID: <832f83c5-dc2b-574b-9ab0-80142862bec9@treenet.co.nz>

On 20/01/2017 9:40 p.m., Alexander wrote:
> Hello, I have a question regarding a native FTP relay (squid's version is
> 3.5.23).

Have you tried NAT intercept for the FTP port?
TPROXY has some low-level things including socket mapping that might not
go so well with how FTP uses multiple connections.

Amos



From rentorbuy at yahoo.com  Fri Jan 20 09:44:51 2017
From: rentorbuy at yahoo.com (Vieri)
Date: Fri, 20 Jan 2017 09:44:51 +0000 (UTC)
Subject: [squid-users] squid reverse proxy (accelerator) for MS Exchange
 OWA
In-Reply-To: <cdea0afa-c2a2-23c4-dbac-553a565e83f7@treenet.co.nz>
References: <1216522442.160455.1484870639800.ref@mail.yahoo.com>
 <1216522442.160455.1484870639800@mail.yahoo.com>
 <cdea0afa-c2a2-23c4-dbac-553a565e83f7@treenet.co.nz>
Message-ID: <690032068.431187.1484905491833@mail.yahoo.com>





----- Original Message -----
From: Amos Jeffries <squid3 at treenet.co.nz>

> Firstly remove the ssloptions=ALL from your config.
> 

> Traffic should be able to go through at that point.

Thanks for the feedback.

I tried it again, but this time with a non-OWA IIS HTTPS server.

Here's the squid.conf:

https_port 10.215.144.91:35443 accel cert=/etc/ssl/squid/cert.cer key=/etc/ssl/squid/key.pem defaultsite=www.mydomain.org

cache_peer 10.215.144.66 parent 443 0 no-query originserver login=PASS ssl sslcert=/etc/ssl/squid/client.cer sslkey=/etc/ssl/squid/client_key.pem front-end-https=on name=httpsServer

acl HTTPSACL dstdomain www.mydomain.org
cache_peer_access httpsServer allow HTTPSACL
never_direct allow HTTPSACL

http_access allow HTTPSACL
http_access deny all

And here's the log when trying to connect from a web browser:

2017/01/20 10:31:06.724 kid1| 5,3| comm.cc(553) commSetConnTimeout: local=10.215.144.91:57753 remote=10.215.144.66:443 FD 14 flags=1 timeout 30
2017/01/20 10:31:06.724 kid1| 5,5| ModEpoll.cc(116) SetSelect: FD 14, type=1, handler=1, client_data=0x80cb86e0, timeout=0
2017/01/20 10:31:06.724 kid1| 93,5| AsyncJob.cc(152) callEnd: Ssl::PeerConnector status out: [ FD 14 job16]
2017/01/20 10:31:06.724 kid1| 93,5| AsyncCallQueue.cc(57) fireNext: leaving AsyncJob::start()
2017/01/20 10:31:06.724 kid1| 83,5| bio.cc(118) read: FD 14 read 0 <= 7
2017/01/20 10:31:06.724 kid1| Error negotiating SSL on FD 14: error:00000000:lib(0):func(0):reason(0) (5/0/0)
2017/01/20 10:31:06.724 kid1| TCP connection to 10.215.144.66/443 failed
2017/01/20 10:31:06.724 kid1| 5,5| comm.cc(1038) comm_remove_close_handler: comm_remove_close_handler: FD 14, AsyncCall=0x80cd0ff8*2
2017/01/20 10:31:06.724 kid1| 9,5| AsyncCall.cc(56) cancel: will not call Ssl::PeerConnector::commCloseHandler [call117] because comm_remove_close_handler
2017/01/20 10:31:06.724 kid1| 17,4| AsyncCall.cc(93) ScheduleCall: PeerConnector.cc(742) will call FwdState::ConnectedToPeer(0x80cae868, local=10.215.144.91:57753 remote=10.215.144.66:443 FD 14 flags=1, 0x80cd0ed0/0x80cd0ed0) [call115]
2017/01/20 10:31:06.724 kid1| 93,5| AsyncJob.cc(137) callEnd: Ssl::PeerConnector::negotiateSsl() ends job [ FD 14 job16]
2017/01/20 10:31:06.724 kid1| 83,5| PeerConnector.cc(58) ~PeerConnector: Peer connector 0x80cb86e0 gone
2017/01/20 10:31:06.724 kid1| 93,5| AsyncJob.cc(40) ~AsyncJob: AsyncJob destructed, this=0x80cb8704 type=Ssl::PeerConnector [job16]
2017/01/20 10:31:06.725 kid1| 17,4| AsyncCallQueue.cc(55) fireNext: entering FwdState::ConnectedToPeer(0x80cae868, local=10.215.144.91:57753 remote=10.215.144.66:443 FD 14 flags=1, 0x80cd0ed0/0x80cd0ed0)
2017/01/20 10:31:06.725 kid1| 17,4| AsyncCall.cc(38) make: make call FwdState::ConnectedToPeer [call115]
2017/01/20 10:31:06.725 kid1| 17,3| FwdState.cc(415) fail: ERR_SECURE_CONNECT_FAIL "Service Unavailable"

I'm not getting any useful debug information, at least not the one I can understand.

Maybe I should rebuild Squid?

# squid -v
Squid Cache: Version 3.5.14
Service Name: squid
configure options:  '--prefix=/usr' '--build=i686-pc-linux-gnu' '--host=i686-pc-linux-gnu' '--mandir=/usr/share/man' '--infodir=/usr/share/info' '--datadir=/usr/share' '--sysconfdir=/etc' '--localstatedir=/var/lib' '--disable-dependency-tracking' '--disable-silent-rules' '--libdir=/usr/lib' '--sysconfdir=/etc/squid' '--libexecdir=/usr/libexec/squid' '--localstatedir=/var' '--with-pidfile=/run/squid.pid' '--datadir=/usr/share/squid' '--with-logdir=/var/log/squid' '--with-default-user=squid' '--enable-removal-policies=lru,heap' '--enable-storeio=aufs,diskd,rock,ufs' '--enable-disk-io' '--enable-auth-basic=MSNT-multi-domain,NCSA,POP3,getpwnam,SMB,LDAP,PAM,RADIUS' '--enable-auth-digest=file,LDAP,eDirectory' '--enable-auth-ntlm=smb_lm' '--enable-auth-negotiate=kerberos,wrapper' '--enable-external-acl-helpers=file_userip,session,unix_group,wbinfo_group,LDAP_group,eDirectory_userip,kerberos_ldap_group' '--enable-log-daemon-helpers' '--enable-url-rewrite-helpers' '--enable-cache-digests' '--enable-delay-pools' '--enable-eui' '--enable-icmp' '--enable-follow-x-forwarded-for' '--with-large-files' '--disable-strict-error-checking' '--disable-arch-native' '--with-ltdl-includedir=/usr/include' '--with-ltdl-libdir=/usr/lib' '--with-libcap' '--enable-ipv6' '--disable-snmp' '--with-openssl' '--with-nettle' '--with-gnutls' '--enable-ssl-crtd' '--disable-ecap' '--disable-esi' '--enable-htcp' '--enable-wccp' '--enable-wccpv2' '--enable-linux-netfilter' '--with-mit-krb5' '--without-heimdal-krb5' 'build_alias=i686-pc-linux-gnu' 'host_alias=i686-pc-linux-gnu' 'CC=i686-pc-linux-gnu-gcc' 'CFLAGS=-O2 -march=i686 -pipe' 'LDFLAGS=-Wl,-O1 -Wl,--as-needed' 'CXXFLAGS=-O2 -march=i686 -pipe' 'PKG_CONFIG_PATH=/usr/lib/pkgconfig'

Thanks,

Vieri


From squid3 at treenet.co.nz  Fri Jan 20 09:51:48 2017
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Fri, 20 Jan 2017 22:51:48 +1300
Subject: [squid-users] squid 3.5.23 memory usage
In-Reply-To: <CAHvB88xx6-p2fzTRc77+meeYrZqB1rZzNnuSfruu_9k1e0tNZA@mail.gmail.com>
References: <CAHvB88xx6-p2fzTRc77+meeYrZqB1rZzNnuSfruu_9k1e0tNZA@mail.gmail.com>
Message-ID: <af672811-ac6f-3d88-db09-6a0e3e712e79@treenet.co.nz>

On 20/01/2017 1:23 p.m., Ivan Larionov wrote:
> Hello.
> 
> I'm pretty sure this question has been asked multiple times already, but
> after reading everything I found I still can't figure out squid memory
> usage patterns.
> 
> We're currently trying to upgrade from squid 2.7 to squid 3.5 and memory
> usage on squid 3 is much much higher compared to squid 2 with the same
> configuration.

One thing to be aware of with this big step in versions is that 3.x has
a lot more things 64-bit enabled where 2.x was more 32-bit oriented. It
is minor in any one place, but does add up when dealing with large
numbers of objects.


> 
> What do I see:
> 
> squid running for several days with low traffic:
> 
> # top
>  PID USER      PR  NI  VIRT  RES  SHR S %CPU %MEM    TIME+  COMMAND
>  7367 squid     20   0 4780m 4.4g 5224 S  6.0 60.6 105:01.76 squid -N
> 
> So it uses 4.4GB resident memory. Ok, let's see important config options:
> 
> cache_mem 2298756 KB
> maximum_object_size_in_memory 8 KB
> memory_replacement_policy lru
> cache_replacement_policy lru
> 
> cache_dir aufs /mnt/services/squid/cache 445644 16 256
> 
> minimum_object_size 64 bytes # none-zero so we dont cache mistakes
> maximum_object_size 102400 KB
> 
> So we configured 2.2GB memory cache and 500GB disk cache. Disk cache is
> quite big but current usage is only 3GB:
> 
> # du -sh /mnt/services/squid/cache # cache_dir
> 3.0G  /mnt/services/squid/cache
> 
> Now I'm looking into this page
> http://wiki.squid-cache.org/SquidFaq/SquidMemory and see:
> 
> 14 MB of memory per 1 GB on disk for 64-bit Squid
> 

These wiki numbers are based on an average object size of 32KB.

By setting "maximum_object_size_in_memory 8 KB" you reduce that by 3x so
need to multiply the overhead per object by (3x more objects in same
space) for the cache_mem value to get a better estimate.

So,
 ... up to 100 MB of index for cache_mem
 ... up to 6 GB of index for cache_dir

Another difference is the buffers in Squid-3 are a bit bigger than those
used for Squid-2. Up to 256 KB per FD (Squid-2 stopped at 64KB).
 BUT, your pool details below show only the 16KB buffer being used much.
So I doubt it is client connections related.



> Which means disk cache should use ~50MB of RAM.
> 
> All these means we have ~2.2GB ram used for everything else except
> cache_mem and disk cache index.

No, the index is also in that 2.2 GB which is not being used by the
cache_mem.


> 
> Let's see top pools from mgr:mem:
> 
> Pool                  (KB)     %Tot
> mem_node              2298833  55.082
> Short Strings         622365   14.913
> HttpHeaderEntry       404531   9.693
> Long Strings          284520   6.817
> MemObject             182288   4.368
> HttpReply             155612   3.729
> StoreEntry            73965    1.772
> Medium Strings        71152    1.705
> cbdata MemBuf (12)    35573    0.852
> LRU policy node       30403    0.728
> MD5 digest            11380    0.273
> 16K Buffer            1056     0.025
> 
> These pools consume ~35% of total squid memory usage: Short Strings,
> HttpHeaderEntry, Long Strings, HttpReply. Looks suspicious. On squid 2 same
> pools use 10 times less memory.


The mem_node is the cache_mem space itself, plus active transactions data.

The StoreEntry is the index entry for each object (cache_dir, cache_mem
and in-transit).
The MemObject is the index entry for each in-memory object (cache_mem
and in-transit).
The HttpReply are those cached objects in parsed format.
The HttpHeaderEntry are all the headers in those reply objects.
The various Strings are the individual words/lines etc in those headers.

So we are under 1% values by the time we are done eliminating data
stored in cache_mem objects and active transaction data.


> 
> I found a bug which looks similar to our experience:
> http://bugs.squid-cache.org/show_bug.cgi?id=4084.
> 

Since you have configured your cache_mem to be 2.2 GB and total memory
usage is 4.4 GB the report saying 55% of memory is used for mem_node
looks fine to me. 50% of 4.4 GB is your 2.2 GB cache_mem setting, and
the extra 5% is probably active transactions and maybe some nodes for
the cache_dir data.

So I dont think it is the issue I mentioned in comments 8.
That said we have not fully identified what the bug problem was.


> I'm attaching our config, mgr:info, mgr:mem and some system info I
> collected.
> 
> Could someone say if this is normal and why it's so much different from
> squid 2?
> 

Well, tentatively yes. The Squid-3 numbers all looks reasonably
accurate. So there is no obvious sign of any problem from this one point
in time.

But if it worries you keep an eye on it for a week or so and see if
anything starts to skew. Graphs like Martin had in comment 5 on that bug
report would be a good indicator of whether there is a problem or just a
new "normal" level.

Amos



From squid3 at treenet.co.nz  Fri Jan 20 09:59:03 2017
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Fri, 20 Jan 2017 22:59:03 +1300
Subject: [squid-users] Will squid core dump with worker threads?
 Investigating squid crash, 3.5.23
In-Reply-To: <o5q004$4hp$1@blaine.gmane.org>
References: <000001d26de4$93742840$ba5c78c0$@optimera.us>
 <f03d70e4-0c65-79f0-0a49-54cf14b19406@treenet.co.nz>
 <o5q004$4hp$1@blaine.gmane.org>
Message-ID: <a155e660-623f-1469-e9bd-7bb825ccdfd7@treenet.co.nz>

On 19/01/2017 10:13 p.m., squid wrote:
> 
>>>
>>> assertion failed: MemBuf.cc:216: "0 <= tailSize && tailSize <= cSize"
>>>
>>
>> This is <http://bugs.squid-cache.org/show_bug.cgi?id=4606>. We have
> 
> 
> Is there a workaround for this - something that I can put in the config
> perhaps?  I'm getting the same issue a few times a day.  I suspect it's
> mainly due to clients accessing Windows Updates, but difficult to tell.
> 
> I am automatically restarting squid, but the delays for other users
> while all this is happening can generate a poor browsing experience.
> 

All that is known is in that bug report, sorry.

If you can assist with the debugging to find out the cause it would be a
great step toward a fix.

Amos



From squid3 at treenet.co.nz  Fri Jan 20 10:07:54 2017
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Fri, 20 Jan 2017 23:07:54 +1300
Subject: [squid-users] squid reverse proxy (accelerator) for MS Exchange
 OWA
In-Reply-To: <690032068.431187.1484905491833@mail.yahoo.com>
References: <1216522442.160455.1484870639800.ref@mail.yahoo.com>
 <1216522442.160455.1484870639800@mail.yahoo.com>
 <cdea0afa-c2a2-23c4-dbac-553a565e83f7@treenet.co.nz>
 <690032068.431187.1484905491833@mail.yahoo.com>
Message-ID: <5517395f-eaa4-0b3a-34e9-66ea8d0122a1@treenet.co.nz>

On 20/01/2017 10:44 p.m., Vieri wrote:
> 
> ----- Original Message -----
> From: Amos Jeffries
> 
>> Firstly remove the ssloptions=ALL from your config.
>>
> 
>> Traffic should be able to go through at that point.
> 
> Thanks for the feedback.
> 
> I tried it again, but this time with a non-OWA IIS HTTPS server.
> 
> Here's the squid.conf:
> 

<snip>
> 
> I'm not getting any useful debug information, at least not the one I can understand.
> 
> Maybe I should rebuild Squid?
> 

You could try with a newer Squid version since the bio.cc code might be
making something else happen in 3.5.23. If that still fails the 4.0 beta
has different logic and far better debug info in this area.

Amos



From eduardoocarneiro at gmail.com  Fri Jan 20 11:15:12 2017
From: eduardoocarneiro at gmail.com (Eduardo Carneiro)
Date: Fri, 20 Jan 2017 03:15:12 -0800 (PST)
Subject: [squid-users] Users inserted incorrectly in access.log
In-Reply-To: <f59d8ff9-6ea5-cc28-9675-ff47225042a1@treenet.co.nz>
References: <1484833020698-4681196.post@n4.nabble.com>
 <f59d8ff9-6ea5-cc28-9675-ff47225042a1@treenet.co.nz>
Message-ID: <1484910912043-4681217.post@n4.nabble.com>

Amos Jeffries wrote
> Please start by selecting one of round-robin and sourcehash. They are
> very different selection algorithms.
> 
> Given that Kerberos auth requires HTTP/1 multiplexing to be disabled for
> the auth to work I suggest that you drop the round-robin. It forces
> multiplexing to be used.
> 
> If the problem still remains try adding the connection-auth=on to those
> Squid's listening ports as well.

Hi Amos, I disabled the round-robin in my frontend server and added
connection-auth = on config in my parents as you suggested me. But the
problem persists. I use squid 3.5.19. 

I have a system developed by myself that gets the squid logs inserted in a
postgres database, and shows them in a PHP page. So, if the accesses are
coming with a wrong user, you can imagine the disorder that causes them. All
the system reports aren't trusted.

Thanks in advance.
Eduardo



--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/Users-inserted-incorrectly-in-access-log-tp4681196p4681217.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From creditu at eml.cc  Fri Jan 20 14:19:41 2017
From: creditu at eml.cc (creditu at eml.cc)
Date: Fri, 20 Jan 2017 07:19:41 -0700
Subject: [squid-users] Dst and dstdomain ACLs
In-Reply-To: <d1e9824d-9c48-a1aa-535d-b77c3446dff7@treenet.co.nz>
References: <1484877671.2918253.853578648.210B72F5@webmail.messagingengine.com>
 <d1e9824d-9c48-a1aa-535d-b77c3446dff7@treenet.co.nz>
Message-ID: <1484921981.4054934.854050648.71FEFBE0@webmail.messagingengine.com>

On Fri, Jan 20, 2017, at 01:42 AM, Amos Jeffries wrote:
> On 20/01/2017 3:01 p.m., creditu wrote:
> > Had a question about dst and dstdomain acls.  Given the sample below:
> > 
> > http_port 192.168.100.1:80 accel defaultsite=www.example.com vhost
> > acl www dstdomain www.example.com dev.example.com
> > cache_peer 10.10.10.1 parent 80 0 no-query no-digest originserver
> > round-robin
> > cache_peer_access 10.10.10.1 allow www
> > cache_peer_access 10.10.10.1 deny all
> > .......
> > http_access allow www
> > http_access deny all
> > 
> > When someone tries to access the site by specifying an IP
> > (192.168.100.1) instead of the name the client gets a standard access
> > denied squid page.
> 
> What is the rDNS for 192.168.100.1 ?

Shoot and thanks.  It's a rDNS issue.  We were using vport in a previous
config and it may have not been noticed because of that.

> 
> The dstdomain you have configured only the exact two domains listed to
> match.
> 
> >  It seems that a separate acl needs to be defined for
> > when someone tries to access the site using an IP?  For instance:
> > acl dst www_ip 192.168.100.1
> 
> You could add the raw-IP to the www ACL:
>  acl www dstdomain -n 192.168.100.1
> 
>  ... but what will 10.10.10.1 do when asked for the site hosted at
> 192.168.100.1 ?

10.10.10.1 doesn't allow it, so might as well stop at squid. So, is the
best way be to create an ACL and deny cache peer access then do
something with deny info?  Something like:

acl dstdomain -n 192.168.100.1
cache_peer_access 10.10.10.1 deny www_ip
....
deny_info http://.... www_ip
http_access deny www_ip

> 
> 
> >  
> > If we wanted to pass to the backend we would need to add a extra
> > cache_peer_access statement
> >  cache_peer_access 10.10.10.1 allow www_ip
> > 
> > Then add:
> > http_access allow www_ip
> > 
> > Is that correct?
> 
> Not for matching raw-IP. The dst will match also for any domain name
> that resolves to the IP given.
> 
> If you want an ACL that matches the textual representation of the raw-IP
> you need to use dsdomain with the -n (no DNS lookup) flag, or the
> dstdom_regex type.
> 
> >  If we wanted to not allow IP based requests we would
> > still define the acl and use a http_access deny www_ip  and then use
> > deny_info to redirect or send a TCP Reset?
> 
> That is another way, and somewhat better than just accepting the raw-IP
> URLs to the backend server.
> 
> 
> Amos
> 
> 
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users


From alex.tate at gmail.com  Fri Jan 20 14:35:05 2017
From: alex.tate at gmail.com (roadrage27)
Date: Fri, 20 Jan 2017 06:35:05 -0800 (PST)
Subject: [squid-users] HTTPS site filtering
In-Reply-To: <35739dd3-a2b7-d6e5-9099-9503edc891c5@treenet.co.nz>
References: <1484857967970-4681198.post@n4.nabble.com>
 <35739dd3-a2b7-d6e5-9099-9503edc891c5@treenet.co.nz>
Message-ID: <1484922905263-4681219.post@n4.nabble.com>

>That tells me either you have screwed up the CONNECT ACL definition. Or
>the SSL_ports one.
Very possible as im pretty green on squid, my current conf file is below. 
with that conf the SSL sites just sit and spin until the eventually time
out.

acl site_squid_art url_regex ^http://www.squid-cache.org/Artwork
acl keepgoing dstdomain .plateau.com .skillwsa.com .successfactors.com

acl SSL_ports port 443
acl Safe_ports port 80		# http
acl Safe_ports port 21		# ftp
acl Safe_ports port 443		# https
acl Safe_ports port 70		# gopher
acl Safe_ports port 210		# wais
acl Safe_ports port 1025-65535	# unregistered ports
acl Safe_ports port 280		# http-mgmt
acl Safe_ports port 488		# gss-http
acl Safe_ports port 591		# filemaker
acl Safe_ports port 777		# multiling http
acl CONNECT method CONNECT

http_access allow keepgoing
http_access deny !Safe_ports
http_access deny CONNECT !SSL_ports
#http_access allow CONNECT SSL_ports
http_access allow localhost manager
http_access allow site_squid_art
http_access allow localhost


http_port 3132


access_log /var/log/squid3/squid3132.log squid

pid_filename /var/run/squid3132.pid
coredump_dir /var/spool/squid3

refresh_pattern ^ftp:		1440	20%	10080
refresh_pattern ^gopher:	1440	0%	1440
#refresh_pattern -i (/cgi-bin/|\?) 0	0%	0
refresh_pattern (Release|Packages(.gz)*)$      0       20%     2880
#refresh_pattern .		0	20%	4320



--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/HTTPS-site-filtering-tp4681198p4681219.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From rousskov at measurement-factory.com  Fri Jan 20 15:59:31 2017
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Fri, 20 Jan 2017 08:59:31 -0700
Subject: [squid-users] squid reverse proxy (accelerator) for MS Exchange
 OWA
In-Reply-To: <cdea0afa-c2a2-23c4-dbac-553a565e83f7@treenet.co.nz>
References: <1216522442.160455.1484870639800.ref@mail.yahoo.com>
 <1216522442.160455.1484870639800@mail.yahoo.com>
 <cdea0afa-c2a2-23c4-dbac-553a565e83f7@treenet.co.nz>
Message-ID: <6f5e3f4c-0459-b9ea-cd63-ec556c7d0f4d@measurement-factory.com>

On 01/20/2017 02:13 AM, Amos Jeffries wrote:

> The key part is the "Error negotiating SSL on FD 16:
> error:00000000:lib(0):func(0):reason(0) (5/0/0)"
> 
> Which is OpenSSL's very obtuse way of telling Squid "an error
> rhappened". With no helpful details about what error it was.

Actually, this is Squid's very obtuse way of telling us that peer closed
the connection while violating the SSL protocol (i.e., a
protocol-violating EOF during an SSL_connect() network read).

OpenSSL error reporting is ugly indeed, but we should not blame it for
our own lack of code to render OpenSSL-supplied details in a
human-friendly way (or for losing critical information along the way).


Cheers,

Alex.



From mustafamohammad92 at gmail.com  Fri Jan 20 16:12:04 2017
From: mustafamohammad92 at gmail.com (Mustafa Mohammad)
Date: Fri, 20 Jan 2017 10:12:04 -0600
Subject: [squid-users] SSL Bump
Message-ID: <CAOUmaCTBd-dXON=RVaY92Ckj66kvkcWN4wteiygpLcb_dPO2og@mail.gmail.com>

What are the steps to setup SSL Bump?


Thanks,

Mustafa Mohammad
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20170120/cda5fe21/attachment.htm>

From giles at coochey.net  Fri Jan 20 16:21:55 2017
From: giles at coochey.net (Giles Coochey)
Date: Fri, 20 Jan 2017 16:21:55 +0000
Subject: [squid-users] SSL Bump
In-Reply-To: <CAOUmaCTBd-dXON=RVaY92Ckj66kvkcWN4wteiygpLcb_dPO2og@mail.gmail.com>
References: <CAOUmaCTBd-dXON=RVaY92Ckj66kvkcWN4wteiygpLcb_dPO2og@mail.gmail.com>
Message-ID: <a88897d5-1f19-7327-8d78-cdd2206c137a@coochey.net>



On 20/01/17 16:12, Mustafa Mohammad wrote:
> What are the steps to setup SSL Bump?
http://lmgtfy.com/?iie=1&q=What+are+the+steps+to+setup+SSL+Bump%3F

-- 
Regards,

Giles Coochey
+44 (0) 7584 634 135
+44 (0) 1803 529 451
giles at coochey.net


-------------- next part --------------
A non-text attachment was scrubbed...
Name: smime.p7s
Type: application/pkcs7-signature
Size: 3819 bytes
Desc: S/MIME Cryptographic Signature
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20170120/fb60f617/attachment.bin>

From Antony.Stone at squid.open.source.it  Fri Jan 20 16:22:44 2017
From: Antony.Stone at squid.open.source.it (Antony Stone)
Date: Fri, 20 Jan 2017 17:22:44 +0100
Subject: [squid-users] SSL Bump
In-Reply-To: <CAOUmaCTBd-dXON=RVaY92Ckj66kvkcWN4wteiygpLcb_dPO2og@mail.gmail.com>
References: <CAOUmaCTBd-dXON=RVaY92Ckj66kvkcWN4wteiygpLcb_dPO2og@mail.gmail.com>
Message-ID: <201701201722.45018.Antony.Stone@squid.open.source.it>

On Friday 20 January 2017 at 17:12:04, Mustafa Mohammad wrote:

> What are the steps to setup SSL Bump?

Don't.

Use peek and splice instead.

See http://wiki.squid-cache.org/Features/SslBump for info, then 
http://wiki.squid-cache.org/Features/SslPeekAndSplice for guidance.


Antony.

-- 
If at first you don't succeed, destroy all the evidence that you tried.

                                                   Please reply to the list;
                                                         please *don't* CC me.


From alex.tate at gmail.com  Fri Jan 20 16:52:00 2017
From: alex.tate at gmail.com (roadrage27)
Date: Fri, 20 Jan 2017 08:52:00 -0800 (PST)
Subject: [squid-users] HTTPS site filtering
In-Reply-To: <1484922905263-4681219.post@n4.nabble.com>
References: <1484857967970-4681198.post@n4.nabble.com>
 <35739dd3-a2b7-d6e5-9099-9503edc891c5@treenet.co.nz>
 <1484922905263-4681219.post@n4.nabble.com>
Message-ID: <CAFwozXKHz6fFNNfBx-Fr2rh30be+=zRHS78mB0aZ6MsUP4xbUw@mail.gmail.com>

I was able to resolve my issue partially.  I burned down the server and
rebuilt it clean so all previous changes that were made attempting to make
SSL work were gone.  Once i reloaded squid and the config files i was able
to allow SSL traffic using the dstdomain acl type.  I currently have a few
URLS that are regex type that need to be allowed so im currently cranking
out those.

On Fri, Jan 20, 2017 at 8:36 AM roadrage27 [via Squid Web Proxy Cache] <
ml-node+s1019090n4681219h44 at n4.nabble.com> wrote:

> >That tells me either you have screwed up the CONNECT ACL definition. Or
> >the SSL_ports one.
> Very possible as im pretty green on squid, my current conf file is below.
>  with that conf the SSL sites just sit and spin until the eventually time
> out.
>
> acl site_squid_art url_regex ^http://www.squid-cache.org/Artwork
> acl keepgoing dstdomain .plateau.com .skillwsa.com .successfactors.com
>
> acl SSL_ports port 443
> acl Safe_ports port 80 # http
> acl Safe_ports port 21 # ftp
> acl Safe_ports port 443 # https
> acl Safe_ports port 70 # gopher
> acl Safe_ports port 210 # wais
> acl Safe_ports port 1025-65535 # unregistered ports
> acl Safe_ports port 280 # http-mgmt
> acl Safe_ports port 488 # gss-http
> acl Safe_ports port 591 # filemaker
> acl Safe_ports port 777 # multiling http
> acl CONNECT method CONNECT
>
> http_access allow keepgoing
> http_access deny !Safe_ports
> http_access deny CONNECT !SSL_ports
> #http_access allow CONNECT SSL_ports
> http_access allow localhost manager
> http_access allow site_squid_art
> http_access allow localhost
>
>
> http_port 3132
>
>
> access_log /var/log/squid3/squid3132.log squid
>
> pid_filename /var/run/squid3132.pid
> coredump_dir /var/spool/squid3
>
> refresh_pattern ^ftp: 1440 20% 10080
> refresh_pattern ^gopher: 1440 0% 1440
> #refresh_pattern -i (/cgi-bin/|\?) 0 0% 0
> refresh_pattern (Release|Packages(.gz)*)$      0       20%     2880
> #refresh_pattern . 0 20% 4320
>
> ------------------------------
> If you reply to this email, your message will be added to the discussion
> below:
>
> http://squid-web-proxy-cache.1019090.n4.nabble.com/HTTPS-site-filtering-tp4681198p4681219.html
> To unsubscribe from HTTPS site filtering, click here
> <http://squid-web-proxy-cache.1019090.n4.nabble.com/template/NamlServlet.jtp?macro=unsubscribe_by_code&node=4681198&code=YWxleC50YXRlQGdtYWlsLmNvbXw0NjgxMTk4fDIwMjU4MDQxMw==>
> .
> NAML
> <http://squid-web-proxy-cache.1019090.n4.nabble.com/template/NamlServlet.jtp?macro=macro_viewer&id=instant_html%21nabble%3Aemail.naml&base=nabble.naml.namespaces.BasicNamespace-nabble.view.web.template.NabbleNamespace-nabble.view.web.template.NodeNamespace&breadcrumbs=notify_subscribers%21nabble%3Aemail.naml-instant_emails%21nabble%3Aemail.naml-send_instant_email%21nabble%3Aemail.naml>
>




--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/HTTPS-site-filtering-tp4681198p4681224.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From squid3 at treenet.co.nz  Fri Jan 20 17:33:11 2017
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sat, 21 Jan 2017 06:33:11 +1300
Subject: [squid-users] Dst and dstdomain ACLs
In-Reply-To: <1484921981.4054934.854050648.71FEFBE0@webmail.messagingengine.com>
References: <1484877671.2918253.853578648.210B72F5@webmail.messagingengine.com>
 <d1e9824d-9c48-a1aa-535d-b77c3446dff7@treenet.co.nz>
 <1484921981.4054934.854050648.71FEFBE0@webmail.messagingengine.com>
Message-ID: <5858ef8d-3de2-677e-fe14-ebe4e1de67f8@treenet.co.nz>

On 21/01/2017 3:19 a.m., creditu at eml.cc wrote:
> On Fri, Jan 20, 2017, at 01:42 AM, Amos Jeffries wrote:
>> On 20/01/2017 3:01 p.m., creditu wrote:
>>> Had a question about dst and dstdomain acls.  Given the sample below:
>>>
>>> http_port 192.168.100.1:80 accel defaultsite=www.example.com vhost
>>> acl www dstdomain www.example.com dev.example.com
>>> cache_peer 10.10.10.1 parent 80 0 no-query no-digest originserver
>>> round-robin
>>> cache_peer_access 10.10.10.1 allow www
>>> cache_peer_access 10.10.10.1 deny all
>>> .......
>>> http_access allow www
>>> http_access deny all
>>>
>>> When someone tries to access the site by specifying an IP
>>> (192.168.100.1) instead of the name the client gets a standard access
>>> denied squid page.
>>
>> What is the rDNS for 192.168.100.1 ?
> 
> Shoot and thanks.  It's a rDNS issue.  We were using vport in a previous
> config and it may have not been noticed because of that.
> 
>>
>> The dstdomain you have configured only the exact two domains listed to
>> match.
>>
>>>  It seems that a separate acl needs to be defined for
>>> when someone tries to access the site using an IP?  For instance:
>>> acl dst www_ip 192.168.100.1
>>
>> You could add the raw-IP to the www ACL:
>>  acl www dstdomain -n 192.168.100.1
>>
>>  ... but what will 10.10.10.1 do when asked for the site hosted at
>> 192.168.100.1 ?
> 
> 10.10.10.1 doesn't allow it, so might as well stop at squid. So, is the
> best way be to create an ACL and deny cache peer access then do
> something with deny info?  Something like:
> 
> acl www_ip dstdomain -n 192.168.100.1
> cache_peer_access 10.10.10.1 deny www_ip
> ....
> deny_info http://.... www_ip
> http_access deny www_ip
> 

Pretty much. But without the cache_peer_access bit. The denied request
never gets near the cache_peer.

Amos



From squid3 at treenet.co.nz  Fri Jan 20 17:45:37 2017
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sat, 21 Jan 2017 06:45:37 +1300
Subject: [squid-users] HTTPS site filtering
In-Reply-To: <CAFwozXKHz6fFNNfBx-Fr2rh30be+=zRHS78mB0aZ6MsUP4xbUw@mail.gmail.com>
References: <1484857967970-4681198.post@n4.nabble.com>
 <35739dd3-a2b7-d6e5-9099-9503edc891c5@treenet.co.nz>
 <1484922905263-4681219.post@n4.nabble.com>
 <CAFwozXKHz6fFNNfBx-Fr2rh30be+=zRHS78mB0aZ6MsUP4xbUw@mail.gmail.com>
Message-ID: <d8e6bb40-2471-4607-37b6-24adf91d1dd6@treenet.co.nz>

On 21/01/2017 5:52 a.m., roadrage27 wrote:
> I was able to resolve my issue partially.  I burned down the server and
> rebuilt it clean so all previous changes that were made attempting to make
> SSL work were gone.  Once i reloaded squid and the config files i was able
> to allow SSL traffic using the dstdomain acl type.  I currently have a few
> URLS that are regex type that need to be allowed so im currently cranking
> out those.
> 
> On Fri, Jan 20, 2017 at 8:36 AM roadrage27 wrote:
> 
>>> That tells me either you have screwed up the CONNECT ACL definition. Or
>>> the SSL_ports one.
>> Very possible as im pretty green on squid, my current conf file is below.
>>  with that conf the SSL sites just sit and spin until the eventually time
>> out.
>>
>> acl site_squid_art url_regex ^http://www.squid-cache.org/Artwork
>> acl keepgoing dstdomain .plateau.com .skillwsa.com .successfactors.com
>>

Whats the idea behind this "keepgoing" ACL ?
 Is this proxy supposed to have reverse-proxy duties for them?

>> acl SSL_ports port 443
>> acl Safe_ports port 80 # http
>> acl Safe_ports port 21 # ftp
>> acl Safe_ports port 443 # https
>> acl Safe_ports port 70 # gopher
>> acl Safe_ports port 210 # wais
>> acl Safe_ports port 1025-65535 # unregistered ports
>> acl Safe_ports port 280 # http-mgmt
>> acl Safe_ports port 488 # gss-http
>> acl Safe_ports port 591 # filemaker
>> acl Safe_ports port 777 # multiling http
>> acl CONNECT method CONNECT
>>
>> http_access allow keepgoing
>> http_access deny !Safe_ports
>> http_access deny CONNECT !SSL_ports
>> #http_access allow CONNECT SSL_ports
>> http_access allow localhost manager
>> http_access allow site_squid_art
>> http_access allow localhost
>>

I see no 'localnet' ACL use. If this proxy is supposed to be servicing
LAN clients, that will be needed and the keepgoing and artwork ACLs
probably not needed.

The final "http_access deny all" is missing as well. Squid is just doing
that impicitly anyway. So its more needed to remind you of what is
happening and prevent possible mistakes implicitly allowing lots of
unexpected things through the proxy later.


>>
>> http_port 3132
>>
>>
>> access_log /var/log/squid3/squid3132.log squid
>>
>> pid_filename /var/run/squid3132.pid
>> coredump_dir /var/spool/squid3
>>
>> refresh_pattern ^ftp: 1440 20% 10080
>> refresh_pattern ^gopher: 1440 0% 1440
>> #refresh_pattern -i (/cgi-bin/|\?) 0 0% 0

FYI: The above commented out line is rather critical to the correct
behaviour for dynamic web content.

If the server is not producing the required cache controls dynamically
changing data should not be allowed to store for one second, let alone
the default 7 days.

>> refresh_pattern (Release|Packages(.gz)*)$      0       20%     2880
>> #refresh_pattern . 0 20% 4320
>>

Whats the point of commenting that out?

Amos


From alex.tate at gmail.com  Fri Jan 20 17:59:04 2017
From: alex.tate at gmail.com (roadrage27)
Date: Fri, 20 Jan 2017 09:59:04 -0800 (PST)
Subject: [squid-users] HTTPS site filtering
In-Reply-To: <d8e6bb40-2471-4607-37b6-24adf91d1dd6@treenet.co.nz>
References: <1484857967970-4681198.post@n4.nabble.com>
 <35739dd3-a2b7-d6e5-9099-9503edc891c5@treenet.co.nz>
 <1484922905263-4681219.post@n4.nabble.com>
 <CAFwozXKHz6fFNNfBx-Fr2rh30be+=zRHS78mB0aZ6MsUP4xbUw@mail.gmail.com>
 <d8e6bb40-2471-4607-37b6-24adf91d1dd6@treenet.co.nz>
Message-ID: <CAFwozXJVq=qtQfe-=taywWjs9jAgBaaJAP1SDWAqVtbjgtiG-A@mail.gmail.com>

When I add the final deny all then no traffic traverses squid.  When I
removed it then squid started passing traffic

On Fri, Jan 20, 2017, 11:46 AM Amos Jeffries [via Squid Web Proxy Cache] <
ml-node+s1019090n4681226h61 at n4.nabble.com> wrote:

> On 21/01/2017 5:52 a.m., roadrage27 wrote:
>
> > I was able to resolve my issue partially.  I burned down the server and
> > rebuilt it clean so all previous changes that were made attempting to
> make
> > SSL work were gone.  Once i reloaded squid and the config files i was
> able
> > to allow SSL traffic using the dstdomain acl type.  I currently have a
> few
> > URLS that are regex type that need to be allowed so im currently
> cranking
> > out those.
> >
> > On Fri, Jan 20, 2017 at 8:36 AM roadrage27 wrote:
> >
> >>> That tells me either you have screwed up the CONNECT ACL definition.
> Or
> >>> the SSL_ports one.
> >> Very possible as im pretty green on squid, my current conf file is
> below.
> >>  with that conf the SSL sites just sit and spin until the eventually
> time
> >> out.
> >>
> >> acl site_squid_art url_regex ^http://www.squid-cache.org/Artwork
> >> acl keepgoing dstdomain .plateau.com .skillwsa.com .successfactors.com
> >>
>
> Whats the idea behind this "keepgoing" ACL ?
>  Is this proxy supposed to have reverse-proxy duties for them?
>
> >> acl SSL_ports port 443
> >> acl Safe_ports port 80 # http
> >> acl Safe_ports port 21 # ftp
> >> acl Safe_ports port 443 # https
> >> acl Safe_ports port 70 # gopher
> >> acl Safe_ports port 210 # wais
> >> acl Safe_ports port 1025-65535 # unregistered ports
> >> acl Safe_ports port 280 # http-mgmt
> >> acl Safe_ports port 488 # gss-http
> >> acl Safe_ports port 591 # filemaker
> >> acl Safe_ports port 777 # multiling http
> >> acl CONNECT method CONNECT
> >>
> >> http_access allow keepgoing
> >> http_access deny !Safe_ports
> >> http_access deny CONNECT !SSL_ports
> >> #http_access allow CONNECT SSL_ports
> >> http_access allow localhost manager
> >> http_access allow site_squid_art
> >> http_access allow localhost
> >>
>
> I see no 'localnet' ACL use. If this proxy is supposed to be servicing
> LAN clients, that will be needed and the keepgoing and artwork ACLs
> probably not needed.
>
> The final "http_access deny all" is missing as well. Squid is just doing
> that impicitly anyway. So its more needed to remind you of what is
> happening and prevent possible mistakes implicitly allowing lots of
> unexpected things through the proxy later.
>
>
> >>
> >> http_port 3132
> >>
> >>
> >> access_log /var/log/squid3/squid3132.log squid
> >>
> >> pid_filename /var/run/squid3132.pid
> >> coredump_dir /var/spool/squid3
> >>
> >> refresh_pattern ^ftp: 1440 20% 10080
> >> refresh_pattern ^gopher: 1440 0% 1440
> >> #refresh_pattern -i (/cgi-bin/|\?) 0 0% 0
>
> FYI: The above commented out line is rather critical to the correct
> behaviour for dynamic web content.
>
> If the server is not producing the required cache controls dynamically
> changing data should not be allowed to store for one second, let alone
> the default 7 days.
>
> >> refresh_pattern (Release|Packages(.gz)*)$      0       20%     2880
> >> #refresh_pattern . 0 20% 4320
> >>
>
> Whats the point of commenting that out?
>
> Amos
> _______________________________________________
> squid-users mailing list
> [hidden email] <http:///user/SendEmail.jtp?type=node&node=4681226&i=0>
> http://lists.squid-cache.org/listinfo/squid-users
>
>
> If you reply to this email, your message will be added to the discussion
> below:
>
> http://squid-web-proxy-cache.1019090.n4.nabble.com/HTTPS-site-filtering-tp4681198p4681226.html
> To unsubscribe from HTTPS site filtering, click here
> <http://squid-web-proxy-cache.1019090.n4.nabble.com/template/NamlServlet.jtp?macro=unsubscribe_by_code&node=4681198&code=YWxleC50YXRlQGdtYWlsLmNvbXw0NjgxMTk4fDIwMjU4MDQxMw==>
> .
> NAML
> <http://squid-web-proxy-cache.1019090.n4.nabble.com/template/NamlServlet.jtp?macro=macro_viewer&id=instant_html%21nabble%3Aemail.naml&base=nabble.naml.namespaces.BasicNamespace-nabble.view.web.template.NabbleNamespace-nabble.view.web.template.NodeNamespace&breadcrumbs=notify_subscribers%21nabble%3Aemail.naml-instant_emails%21nabble%3Aemail.naml-send_instant_email%21nabble%3Aemail.naml>
>




--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/HTTPS-site-filtering-tp4681198p4681227.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From alex.tate at gmail.com  Fri Jan 20 18:30:36 2017
From: alex.tate at gmail.com (roadrage27)
Date: Fri, 20 Jan 2017 10:30:36 -0800 (PST)
Subject: [squid-users] HTTPS site filtering
In-Reply-To: <d8e6bb40-2471-4607-37b6-24adf91d1dd6@treenet.co.nz>
References: <1484857967970-4681198.post@n4.nabble.com>
 <35739dd3-a2b7-d6e5-9099-9503edc891c5@treenet.co.nz>
 <1484922905263-4681219.post@n4.nabble.com>
 <CAFwozXKHz6fFNNfBx-Fr2rh30be+=zRHS78mB0aZ6MsUP4xbUw@mail.gmail.com>
 <d8e6bb40-2471-4607-37b6-24adf91d1dd6@treenet.co.nz>
Message-ID: <CAFwozXLs8xyL1V_BZjY9H1JnYTHDh14ODzhJXRVCNi6Up8QnxA@mail.gmail.com>

>I see no 'localnet' ACL use. If this proxy is supposed to be servicing
>LAN clients, that will be needed and the keepgoing and artwork ACLs
>probably not needed.

I am connecting on a LAN to it now with no issues and multiple testers on
the same subnet can also use it.  why would i add a directive if its
already working?

I uncommented out the other lines cant recall why i commented them but yeah
mistake there.

>Whats the idea behind this "keepgoing" ACL ?
Once i put that in with those domain it allowed them to connect as those
were domains needed for access via SSL
 >Is this proxy supposed to have reverse-proxy duties for them?
Nope, just a simple proxy that locks out the web unless the ACL allows it.

On Fri, Jan 20, 2017 at 12:00 PM Alex Tate <alex.tate at gmail.com> wrote:

> When I add the final deny all then no traffic traverses squid.  When I
> removed it then squid started passing traffic
>
> On Fri, Jan 20, 2017, 11:46 AM Amos Jeffries [via Squid Web Proxy Cache] <
> ml-node+s1019090n4681226h61 at n4.nabble.com> wrote:
>
> On 21/01/2017 5:52 a.m., roadrage27 wrote:
>
> > I was able to resolve my issue partially.  I burned down the server and
> > rebuilt it clean so all previous changes that were made attempting to
> make
> > SSL work were gone.  Once i reloaded squid and the config files i was
> able
> > to allow SSL traffic using the dstdomain acl type.  I currently have a
> few
> > URLS that are regex type that need to be allowed so im currently
> cranking
> > out those.
> >
> > On Fri, Jan 20, 2017 at 8:36 AM roadrage27 wrote:
> >
> >>> That tells me either you have screwed up the CONNECT ACL definition.
> Or
> >>> the SSL_ports one.
> >> Very possible as im pretty green on squid, my current conf file is
> below.
> >>  with that conf the SSL sites just sit and spin until the eventually
> time
> >> out.
> >>
> >> acl site_squid_art url_regex ^http://www.squid-cache.org/Artwork
> >> acl keepgoing dstdomain .plateau.com .skillwsa.com .successfactors.com
> >>
>
> Whats the idea behind this "keepgoing" ACL ?
>  Is this proxy supposed to have reverse-proxy duties for them?
>
> >> acl SSL_ports port 443
> >> acl Safe_ports port 80 # http
> >> acl Safe_ports port 21 # ftp
> >> acl Safe_ports port 443 # https
> >> acl Safe_ports port 70 # gopher
> >> acl Safe_ports port 210 # wais
> >> acl Safe_ports port 1025-65535 # unregistered ports
> >> acl Safe_ports port 280 # http-mgmt
> >> acl Safe_ports port 488 # gss-http
> >> acl Safe_ports port 591 # filemaker
> >> acl Safe_ports port 777 # multiling http
> >> acl CONNECT method CONNECT
> >>
> >> http_access allow keepgoing
> >> http_access deny !Safe_ports
> >> http_access deny CONNECT !SSL_ports
> >> #http_access allow CONNECT SSL_ports
> >> http_access allow localhost manager
> >> http_access allow site_squid_art
> >> http_access allow localhost
> >>
>
> I see no 'localnet' ACL use. If this proxy is supposed to be servicing
> LAN clients, that will be needed and the keepgoing and artwork ACLs
> probably not needed.
>
> The final "http_access deny all" is missing as well. Squid is just doing
> that impicitly anyway. So its more needed to remind you of what is
> happening and prevent possible mistakes implicitly allowing lots of
> unexpected things through the proxy later.
>
>
> >>
> >> http_port 3132
> >>
> >>
> >> access_log /var/log/squid3/squid3132.log squid
> >>
> >> pid_filename /var/run/squid3132.pid
> >> coredump_dir /var/spool/squid3
> >>
> >> refresh_pattern ^ftp: 1440 20% 10080
> >> refresh_pattern ^gopher: 1440 0% 1440
> >> #refresh_pattern -i (/cgi-bin/|\?) 0 0% 0
>
> FYI: The above commented out line is rather critical to the correct
> behaviour for dynamic web content.
>
> If the server is not producing the required cache controls dynamically
> changing data should not be allowed to store for one second, let alone
> the default 7 days.
>
> >> refresh_pattern (Release|Packages(.gz)*)$      0       20%     2880
> >> #refresh_pattern . 0 20% 4320
> >>
>
> Whats the point of commenting that out?
>
> Amos
> _______________________________________________
> squid-users mailing list
> [hidden email] <http:///user/SendEmail.jtp?type=node&node=4681226&i=0>
> http://lists.squid-cache.org/listinfo/squid-users
>
>
> If you reply to this email, your message will be added to the discussion
> below:
>
> http://squid-web-proxy-cache.1019090.n4.nabble.com/HTTPS-site-filtering-tp4681198p4681226.html
> To unsubscribe from HTTPS site filtering, click here
> <http://squid-web-proxy-cache.1019090.n4.nabble.com/template/NamlServlet.jtp?macro=unsubscribe_by_code&node=4681198&code=YWxleC50YXRlQGdtYWlsLmNvbXw0NjgxMTk4fDIwMjU4MDQxMw==>
> .
> NAML
> <http://squid-web-proxy-cache.1019090.n4.nabble.com/template/NamlServlet.jtp?macro=macro_viewer&id=instant_html%21nabble%3Aemail.naml&base=nabble.naml.namespaces.BasicNamespace-nabble.view.web.template.NabbleNamespace-nabble.view.web.template.NodeNamespace&breadcrumbs=notify_subscribers%21nabble%3Aemail.naml-instant_emails%21nabble%3Aemail.naml-send_instant_email%21nabble%3Aemail.naml>
>
>




--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/HTTPS-site-filtering-tp4681198p4681228.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From squid3 at treenet.co.nz  Fri Jan 20 18:35:42 2017
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sat, 21 Jan 2017 07:35:42 +1300
Subject: [squid-users] HTTPS site filtering
In-Reply-To: <CAFwozXJVq=qtQfe-=taywWjs9jAgBaaJAP1SDWAqVtbjgtiG-A@mail.gmail.com>
References: <1484857967970-4681198.post@n4.nabble.com>
 <35739dd3-a2b7-d6e5-9099-9503edc891c5@treenet.co.nz>
 <1484922905263-4681219.post@n4.nabble.com>
 <CAFwozXKHz6fFNNfBx-Fr2rh30be+=zRHS78mB0aZ6MsUP4xbUw@mail.gmail.com>
 <d8e6bb40-2471-4607-37b6-24adf91d1dd6@treenet.co.nz>
 <CAFwozXJVq=qtQfe-=taywWjs9jAgBaaJAP1SDWAqVtbjgtiG-A@mail.gmail.com>
Message-ID: <a548cae1-0987-6339-effe-5c1ec66c408d@treenet.co.nz>

On 21/01/2017 6:59 a.m., roadrage27 wrote:
> When I add the final deny all then no traffic traverses squid.  When I
> removed it then squid started passing traffic
> 

That is odd. Because Squid ACL logics implicitly use the inverse of the
last line as the default action.

So your "allow localhost" should be causing an impicit "deny all" to
exist at that point in the processing anyway.

(/me wonders who broke what.)

Amos



From squid3 at treenet.co.nz  Fri Jan 20 18:41:42 2017
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sat, 21 Jan 2017 07:41:42 +1300
Subject: [squid-users] HTTPS site filtering
In-Reply-To: <CAFwozXLs8xyL1V_BZjY9H1JnYTHDh14ODzhJXRVCNi6Up8QnxA@mail.gmail.com>
References: <1484857967970-4681198.post@n4.nabble.com>
 <35739dd3-a2b7-d6e5-9099-9503edc891c5@treenet.co.nz>
 <1484922905263-4681219.post@n4.nabble.com>
 <CAFwozXKHz6fFNNfBx-Fr2rh30be+=zRHS78mB0aZ6MsUP4xbUw@mail.gmail.com>
 <d8e6bb40-2471-4607-37b6-24adf91d1dd6@treenet.co.nz>
 <CAFwozXLs8xyL1V_BZjY9H1JnYTHDh14ODzhJXRVCNi6Up8QnxA@mail.gmail.com>
Message-ID: <3e495f6d-e3ee-eaac-cae4-4565034a1fa8@treenet.co.nz>

On 21/01/2017 7:30 a.m., roadrage27 wrote:
>> I see no 'localnet' ACL use. If this proxy is supposed to be servicing
>> LAN clients, that will be needed and the keepgoing and artwork ACLs
>> probably not needed.
> 
> I am connecting on a LAN to it now with no issues and multiple testers on
> the same subnet can also use it.  why would i add a directive if its
> already working?

Because your config file says the only traffic allowed is those specific
keepgoing domains, the squid artwork file, and traffic was generated by
locahost (aka. 127.0.0.1 on the proxy machine itself).

How is that LAN traffic getting to Squid?

Amos



From alex.tate at gmail.com  Fri Jan 20 18:57:26 2017
From: alex.tate at gmail.com (roadrage27)
Date: Fri, 20 Jan 2017 10:57:26 -0800 (PST)
Subject: [squid-users] HTTPS site filtering
In-Reply-To: <3e495f6d-e3ee-eaac-cae4-4565034a1fa8@treenet.co.nz>
References: <1484857967970-4681198.post@n4.nabble.com>
 <35739dd3-a2b7-d6e5-9099-9503edc891c5@treenet.co.nz>
 <1484922905263-4681219.post@n4.nabble.com>
 <CAFwozXKHz6fFNNfBx-Fr2rh30be+=zRHS78mB0aZ6MsUP4xbUw@mail.gmail.com>
 <d8e6bb40-2471-4607-37b6-24adf91d1dd6@treenet.co.nz>
 <CAFwozXLs8xyL1V_BZjY9H1JnYTHDh14ODzhJXRVCNi6Up8QnxA@mail.gmail.com>
 <3e495f6d-e3ee-eaac-cae4-4565034a1fa8@treenet.co.nz>
Message-ID: <CAFwozXKpov=a8h_0agcFRuyuq_MK+6vVHcg-XX=yo42w=vOLcA@mail.gmail.com>

>How is that LAN traffic getting to Squid?
Squid is sitting on the internal LAN, not an external facing server

>That is odd. Because Squid ACL logics implicitly use the inverse of the
l>ast line as the default action.

>So your "allow localhost" sho>uld be causing an impicit "deny all" to
>exist at that point in the processing anyway.
>(/me wonders who broke what.)

I read the documentation which suggested the same but it goes nowhere.  I
thought i snapped it in half which is why i burned the server down and
rebuilt it from scratch then brought the config back over.  I attempted the
implicit deny on the new build with the same result.


On Fri, Jan 20, 2017 at 12:42 PM Amos Jeffries [via Squid Web Proxy Cache] <
ml-node+s1019090n4681230h85 at n4.nabble.com> wrote:

> On 21/01/2017 7:30 a.m., roadrage27 wrote:
> >> I see no 'localnet' ACL use. If this proxy is supposed to be servicing
> >> LAN clients, that will be needed and the keepgoing and artwork ACLs
> >> probably not needed.
> >
> > I am connecting on a LAN to it now with no issues and multiple testers
> on
> > the same subnet can also use it.  why would i add a directive if its
> > already working?
>
> Because your config file says the only traffic allowed is those specific
> keepgoing domains, the squid artwork file, and traffic was generated by
> locahost (aka. 127.0.0.1 on the proxy machine itself).
>
> How is that LAN traffic getting to Squid?
>
> Amos
>
> _______________________________________________
> squid-users mailing list
> [hidden email] <http:///user/SendEmail.jtp?type=node&node=4681230&i=0>
> http://lists.squid-cache.org/listinfo/squid-users
>
>
> If you reply to this email, your message will be added to the discussion
> below:
>
> http://squid-web-proxy-cache.1019090.n4.nabble.com/HTTPS-site-filtering-tp4681198p4681230.html
> To unsubscribe from HTTPS site filtering, click here
> <http://squid-web-proxy-cache.1019090.n4.nabble.com/template/NamlServlet.jtp?macro=unsubscribe_by_code&node=4681198&code=YWxleC50YXRlQGdtYWlsLmNvbXw0NjgxMTk4fDIwMjU4MDQxMw==>
> .
> NAML
> <http://squid-web-proxy-cache.1019090.n4.nabble.com/template/NamlServlet.jtp?macro=macro_viewer&id=instant_html%21nabble%3Aemail.naml&base=nabble.naml.namespaces.BasicNamespace-nabble.view.web.template.NabbleNamespace-nabble.view.web.template.NodeNamespace&breadcrumbs=notify_subscribers%21nabble%3Aemail.naml-instant_emails%21nabble%3Aemail.naml-send_instant_email%21nabble%3Aemail.naml>
>




--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/HTTPS-site-filtering-tp4681198p4681231.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From vladimir.v.kostyukov at gmail.com  Sat Jan 21 19:29:13 2017
From: vladimir.v.kostyukov at gmail.com (Vladimir Kostyukov)
Date: Sat, 21 Jan 2017 11:29:13 -0800
Subject: [squid-users] HTTP CONNECT ipv6
Message-ID: <CAOtr1_OXE_GrpHO_BeUt6xJeN7L7arfRUfaKiMUjwixR8prwcA@mail.gmail.com>

Hello!

I have a quick question and I apologize upfront if there is already a doc
covering this. Somehow I wasn't able to find one.

Does Squid support establishing a TCP tunneling (eg: HTTPS
<http://wiki.squid-cache.org/Features/HTTPS>) session to IPV6 address
instead of a hostname/ipv4 address? Basically, what happens in a client
sends something between the lines:

CONNECT <IPV6>:443 HTTP/1.1 \r\n

-- 
Vladimir Kostyukov | @vkostyukov <https://twitter.com/vkostyukov> |
http://vkostyukov.ru
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20170121/d92dfc81/attachment.htm>

From squid3 at treenet.co.nz  Sat Jan 21 19:36:28 2017
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sun, 22 Jan 2017 08:36:28 +1300
Subject: [squid-users] HTTP CONNECT ipv6
In-Reply-To: <CAOtr1_OXE_GrpHO_BeUt6xJeN7L7arfRUfaKiMUjwixR8prwcA@mail.gmail.com>
References: <CAOtr1_OXE_GrpHO_BeUt6xJeN7L7arfRUfaKiMUjwixR8prwcA@mail.gmail.com>
Message-ID: <cff5aae2-066d-26c5-2c77-7ec26e7a67bc@treenet.co.nz>

On 22/01/2017 8:29 a.m., Vladimir Kostyukov wrote:
> Hello!
> 
> I have a quick question and I apologize upfront if there is already a doc
> covering this. Somehow I wasn't able to find one.
> 
> Does Squid support establishing a TCP tunneling (eg: HTTPS
> <http://wiki.squid-cache.org/Features/HTTPS>) session to IPV6 address
> instead of a hostname/ipv4 address? Basically, what happens in a client
> sends something between the lines:
> 
> CONNECT <IPV6>:443 HTTP/1.1 \r\n
> 

Yes it does.

Amos



From vladimir.v.kostyukov at gmail.com  Sat Jan 21 20:50:04 2017
From: vladimir.v.kostyukov at gmail.com (Vladimir Kostyukov)
Date: Sat, 21 Jan 2017 12:50:04 -0800
Subject: [squid-users] HTTP CONNECT ipv6
In-Reply-To: <cff5aae2-066d-26c5-2c77-7ec26e7a67bc@treenet.co.nz>
References: <CAOtr1_OXE_GrpHO_BeUt6xJeN7L7arfRUfaKiMUjwixR8prwcA@mail.gmail.com>
 <cff5aae2-066d-26c5-2c77-7ec26e7a67bc@treenet.co.nz>
Message-ID: <CAOtr1_M-eXTMBVOgiHws7MP1+dMLnu=tAMg2uOd5krah2Fkwkw@mail.gmail.com>

Thanks Amos! Do you have to enable that somehow? From what I'm seeing now
it Squid (3.5.15) fails with HTTP 503 and err_connect_fail 65.

On Sat, Jan 21, 2017 at 11:36 AM, Amos Jeffries <squid3 at treenet.co.nz>
wrote:

> On 22/01/2017 8:29 a.m., Vladimir Kostyukov wrote:
> > Hello!
> >
> > I have a quick question and I apologize upfront if there is already a doc
> > covering this. Somehow I wasn't able to find one.
> >
> > Does Squid support establishing a TCP tunneling (eg: HTTPS
> > <http://wiki.squid-cache.org/Features/HTTPS>) session to IPV6 address
> > instead of a hostname/ipv4 address? Basically, what happens in a client
> > sends something between the lines:
> >
> > CONNECT <IPV6>:443 HTTP/1.1 \r\n
> >
>
> Yes it does.
>
> Amos
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>



-- 
Vladimir Kostyukov | @vkostyukov <https://twitter.com/vkostyukov> |
http://vkostyukov.ru
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20170121/7e7c9d76/attachment.htm>

From eliezer at ngtech.co.il  Sat Jan 21 21:33:45 2017
From: eliezer at ngtech.co.il (Eliezer  Croitoru)
Date: Sat, 21 Jan 2017 23:33:45 +0200
Subject: [squid-users] HTTP CONNECT ipv6
In-Reply-To: <CAOtr1_OXE_GrpHO_BeUt6xJeN7L7arfRUfaKiMUjwixR8prwcA@mail.gmail.com>
References: <CAOtr1_OXE_GrpHO_BeUt6xJeN7L7arfRUfaKiMUjwixR8prwcA@mail.gmail.com>
Message-ID: <0da101d2742e$0b78a7e0$2269f7a0$@ngtech.co.il>

Hey Vladimir,

It supports IPV6 for a very long time.(3.2 and up)

Regards,
Eliezer

----
http://ngtech.co.il/lmgtfy/
Linux System Administrator
Mobile: +972-5-28704261
Email: eliezer at ngtech.co.il


From: squid-users [mailto:squid-users-bounces at lists.squid-cache.org] On Behalf Of Vladimir Kostyukov
Sent: Saturday, January 21, 2017 9:29 PM
To: squid-users at lists.squid-cache.org
Subject: [squid-users] HTTP CONNECT ipv6

Hello!

I have a quick question and I apologize upfront if there is already a doc covering this. Somehow I wasn't able to find one.

Does Squid support establishing a TCP tunneling (eg: http://wiki.squid-cache.org/Features/HTTPS) session to IPV6 address instead of a hostname/ipv4 address? Basically, what happens in a client sends something between the lines:

CONNECT <IPV6>:443 HTTP/1.1 \r\n

-- 
Vladimir Kostyukov | https://twitter.com/vkostyukov | http://vkostyukov.ru



From squid3 at treenet.co.nz  Sun Jan 22 02:09:51 2017
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sun, 22 Jan 2017 15:09:51 +1300
Subject: [squid-users] HTTP CONNECT ipv6
In-Reply-To: <CAOtr1_M-eXTMBVOgiHws7MP1+dMLnu=tAMg2uOd5krah2Fkwkw@mail.gmail.com>
References: <CAOtr1_OXE_GrpHO_BeUt6xJeN7L7arfRUfaKiMUjwixR8prwcA@mail.gmail.com>
 <cff5aae2-066d-26c5-2c77-7ec26e7a67bc@treenet.co.nz>
 <CAOtr1_M-eXTMBVOgiHws7MP1+dMLnu=tAMg2uOd5krah2Fkwkw@mail.gmail.com>
Message-ID: <98ed87c3-2da4-39b2-7811-c464f9a4e430@treenet.co.nz>

On 22/01/2017 9:50 a.m., Vladimir Kostyukov wrote:
> Thanks Amos! Do you have to enable that somehow? From what I'm seeing now
> it Squid (3.5.15) fails with HTTP 503 and err_connect_fail 65.
> 

Check that the network you are running this Squid on is IPv6-enabled. It
is all very well having Squid understand the address, but you wont be
going anywhere if the network does not also have support working.

Also, the URL needs to have it in proper IPv6 URL format with []
brackets. eg.
 CONNECT [::1]:443 HTTP/1.0


Just in case, check the 'squid -v' output for --disable-ipv6. But I
expect you would be seeing a parse error if that were the problem.

Amos



From squid3 at treenet.co.nz  Sun Jan 22 02:12:28 2017
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sun, 22 Jan 2017 15:12:28 +1300
Subject: [squid-users] HTTPS site filtering
In-Reply-To: <CAFwozXKpov=a8h_0agcFRuyuq_MK+6vVHcg-XX=yo42w=vOLcA@mail.gmail.com>
References: <1484857967970-4681198.post@n4.nabble.com>
 <35739dd3-a2b7-d6e5-9099-9503edc891c5@treenet.co.nz>
 <1484922905263-4681219.post@n4.nabble.com>
 <CAFwozXKHz6fFNNfBx-Fr2rh30be+=zRHS78mB0aZ6MsUP4xbUw@mail.gmail.com>
 <d8e6bb40-2471-4607-37b6-24adf91d1dd6@treenet.co.nz>
 <CAFwozXLs8xyL1V_BZjY9H1JnYTHDh14ODzhJXRVCNi6Up8QnxA@mail.gmail.com>
 <3e495f6d-e3ee-eaac-cae4-4565034a1fa8@treenet.co.nz>
 <CAFwozXKpov=a8h_0agcFRuyuq_MK+6vVHcg-XX=yo42w=vOLcA@mail.gmail.com>
Message-ID: <b410afab-ea14-06ed-e09f-759247902acf@treenet.co.nz>

On 21/01/2017 7:57 a.m., roadrage27 wrote:
>> How is that LAN traffic getting to Squid?
> Squid is sitting on the internal LAN, not an external facing server
> 

Sure, but that does not answer the question.

Do you manually configure the browsers? use WPAD/PAC? DHCP? NAT? TPROXY?
DNS? or something else?

Amos



From alex.tate at gmail.com  Sun Jan 22 02:12:55 2017
From: alex.tate at gmail.com (roadrage27)
Date: Sat, 21 Jan 2017 18:12:55 -0800 (PST)
Subject: [squid-users] HTTPS site filtering
In-Reply-To: <b410afab-ea14-06ed-e09f-759247902acf@treenet.co.nz>
References: <1484857967970-4681198.post@n4.nabble.com>
 <35739dd3-a2b7-d6e5-9099-9503edc891c5@treenet.co.nz>
 <1484922905263-4681219.post@n4.nabble.com>
 <CAFwozXKHz6fFNNfBx-Fr2rh30be+=zRHS78mB0aZ6MsUP4xbUw@mail.gmail.com>
 <d8e6bb40-2471-4607-37b6-24adf91d1dd6@treenet.co.nz>
 <CAFwozXLs8xyL1V_BZjY9H1JnYTHDh14ODzhJXRVCNi6Up8QnxA@mail.gmail.com>
 <3e495f6d-e3ee-eaac-cae4-4565034a1fa8@treenet.co.nz>
 <CAFwozXKpov=a8h_0agcFRuyuq_MK+6vVHcg-XX=yo42w=vOLcA@mail.gmail.com>
 <b410afab-ea14-06ed-e09f-759247902acf@treenet.co.nz>
Message-ID: <CAFwozX+3_oOboi9gbGqoRjN9pe0zypEqDW-YkwKYRRKBBmEcAA@mail.gmail.com>

manual browser config

On Sat, Jan 21, 2017 at 8:13 PM Amos Jeffries [via Squid Web Proxy Cache] <
ml-node+s1019090n4681237h2 at n4.nabble.com> wrote:

> On 21/01/2017 7:57 a.m., roadrage27 wrote:
> >> How is that LAN traffic getting to Squid?
> > Squid is sitting on the internal LAN, not an external facing server
> >
>
> Sure, but that does not answer the question.
>
> Do you manually configure the browsers? use WPAD/PAC? DHCP? NAT? TPROXY?
> DNS? or something else?
>
> Amos
>
> _______________________________________________
> squid-users mailing list
> [hidden email] <http:///user/SendEmail.jtp?type=node&node=4681237&i=0>
> http://lists.squid-cache.org/listinfo/squid-users
>
>
> If you reply to this email, your message will be added to the discussion
> below:
>
> http://squid-web-proxy-cache.1019090.n4.nabble.com/HTTPS-site-filtering-tp4681198p4681237.html
> To unsubscribe from HTTPS site filtering, click here
> <http://squid-web-proxy-cache.1019090.n4.nabble.com/template/NamlServlet.jtp?macro=unsubscribe_by_code&node=4681198&code=YWxleC50YXRlQGdtYWlsLmNvbXw0NjgxMTk4fDIwMjU4MDQxMw==>
> .
> NAML
> <http://squid-web-proxy-cache.1019090.n4.nabble.com/template/NamlServlet.jtp?macro=macro_viewer&id=instant_html%21nabble%3Aemail.naml&base=nabble.naml.namespaces.BasicNamespace-nabble.view.web.template.NabbleNamespace-nabble.view.web.template.NodeNamespace&breadcrumbs=notify_subscribers%21nabble%3Aemail.naml-instant_emails%21nabble%3Aemail.naml-send_instant_email%21nabble%3Aemail.naml>
>




--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/HTTPS-site-filtering-tp4681198p4681238.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From alex.tate at gmail.com  Sun Jan 22 02:13:35 2017
From: alex.tate at gmail.com (roadrage27)
Date: Sat, 21 Jan 2017 18:13:35 -0800 (PST)
Subject: [squid-users] HTTPS site filtering
In-Reply-To: <b410afab-ea14-06ed-e09f-759247902acf@treenet.co.nz>
References: <1484857967970-4681198.post@n4.nabble.com>
 <35739dd3-a2b7-d6e5-9099-9503edc891c5@treenet.co.nz>
 <1484922905263-4681219.post@n4.nabble.com>
 <CAFwozXKHz6fFNNfBx-Fr2rh30be+=zRHS78mB0aZ6MsUP4xbUw@mail.gmail.com>
 <d8e6bb40-2471-4607-37b6-24adf91d1dd6@treenet.co.nz>
 <CAFwozXLs8xyL1V_BZjY9H1JnYTHDh14ODzhJXRVCNi6Up8QnxA@mail.gmail.com>
 <3e495f6d-e3ee-eaac-cae4-4565034a1fa8@treenet.co.nz>
 <CAFwozXKpov=a8h_0agcFRuyuq_MK+6vVHcg-XX=yo42w=vOLcA@mail.gmail.com>
 <b410afab-ea14-06ed-e09f-759247902acf@treenet.co.nz>
Message-ID: <CAFwozXKpagpoFcrA8n=Yt7epVx9zaVeTq3L+HTuWA-GJGW0_Eg@mail.gmail.com>

i got it all working this morning just forgot to chime in on the thread.
ACLs working properly and doing everything they need to.  thanks for all
the help

On Sat, Jan 21, 2017 at 8:14 PM Alex Tate <alex.tate at gmail.com> wrote:

> manual browser config
>
> On Sat, Jan 21, 2017 at 8:13 PM Amos Jeffries [via Squid Web Proxy Cache] <
> ml-node+s1019090n4681237h2 at n4.nabble.com> wrote:
>
>> On 21/01/2017 7:57 a.m., roadrage27 wrote:
>> >> How is that LAN traffic getting to Squid?
>> > Squid is sitting on the internal LAN, not an external facing server
>> >
>>
>> Sure, but that does not answer the question.
>>
>> Do you manually configure the browsers? use WPAD/PAC? DHCP? NAT? TPROXY?
>> DNS? or something else?
>>
>> Amos
>>
>> _______________________________________________
>> squid-users mailing list
>> [hidden email] <http:///user/SendEmail.jtp?type=node&node=4681237&i=0>
>> http://lists.squid-cache.org/listinfo/squid-users
>>
>>
>> If you reply to this email, your message will be added to the discussion
>> below:
>>
>> http://squid-web-proxy-cache.1019090.n4.nabble.com/HTTPS-site-filtering-tp4681198p4681237.html
>> To unsubscribe from HTTPS site filtering, click here
>> <http://squid-web-proxy-cache.1019090.n4.nabble.com/template/NamlServlet.jtp?macro=unsubscribe_by_code&node=4681198&code=YWxleC50YXRlQGdtYWlsLmNvbXw0NjgxMTk4fDIwMjU4MDQxMw==>
>> .
>> NAML
>> <http://squid-web-proxy-cache.1019090.n4.nabble.com/template/NamlServlet.jtp?macro=macro_viewer&id=instant_html%21nabble%3Aemail.naml&base=nabble.naml.namespaces.BasicNamespace-nabble.view.web.template.NabbleNamespace-nabble.view.web.template.NodeNamespace&breadcrumbs=notify_subscribers%21nabble%3Aemail.naml-instant_emails%21nabble%3Aemail.naml-send_instant_email%21nabble%3Aemail.naml>
>>
>




--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/HTTPS-site-filtering-tp4681198p4681239.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From oguzismailuysal at gmail.com  Sun Jan 22 09:20:51 2017
From: oguzismailuysal at gmail.com (=?UTF-8?Q?O=C4=9Fuz_=C4=B0smail_Uysal?=)
Date: Sun, 22 Jan 2017 11:20:51 +0200
Subject: [squid-users] external acl helper
Message-ID: <CAH7i3LpcUZ4D3yfb8+=PpnrFo=0vPzb7pdLhhXibs1kXO8Ofpw@mail.gmail.com>

Can I use a python script as external acl helper ? I have tested it quickly
by adding these lines to squid.conf:

external_acl_type python %SRC /usr/bin/python ~/rekt.py
acl external_acl_helpers external python
http_access allow external_acl_helpers

And this is rekt.py:

#!/usr/bin/env python
print "OK"

As I sadly expected it did not work after restarting squid. Excuse my
inadequate knowledge guys, how can I do this ?
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20170122/a5848b15/attachment.htm>

From eliezer at ngtech.co.il  Sun Jan 22 11:58:57 2017
From: eliezer at ngtech.co.il (Eliezer  Croitoru)
Date: Sun, 22 Jan 2017 13:58:57 +0200
Subject: [squid-users] external acl helper
In-Reply-To: <CAH7i3LpcUZ4D3yfb8+=PpnrFo=0vPzb7pdLhhXibs1kXO8Ofpw@mail.gmail.com>
References: <CAH7i3LpcUZ4D3yfb8+=PpnrFo=0vPzb7pdLhhXibs1kXO8Ofpw@mail.gmail.com>
Message-ID: <0e8301d274a6$e9bf57b0$bd3e0710$@ngtech.co.il>

An example for a helper in python.
You will need to remove the whole memcache stuff and this is the basic structure:
http://wiki.squid-cache.org/EliezerCroitoru/SessionHelper/Python


----
http://ngtech.co.il/lmgtfy/
Linux System Administrator
Mobile: +972-5-28704261
Email: eliezer at ngtech.co.il


From: squid-users [mailto:squid-users-bounces at lists.squid-cache.org] On Behalf Of O?uz ?smail Uysal
Sent: Sunday, January 22, 2017 11:21 AM
To: squid-users at lists.squid-cache.org
Subject: [squid-users] external acl helper

Can I use a python script as external acl helper ? I have tested it quickly by adding these lines to squid.conf:

external_acl_type python %SRC /usr/bin/python ~/rekt.py
acl external_acl_helpers external python
http_access allow external_acl_helpers

And this is rekt.py:

#!/usr/bin/env python
print "OK"

As I sadly expected it did not work after restarting squid. Excuse my inadequate knowledge guys, how can I do this ?



From goal81 at gmail.com  Sun Jan 22 13:49:47 2017
From: goal81 at gmail.com (Alexander)
Date: Sun, 22 Jan 2017 05:49:47 -0800 (PST)
Subject: [squid-users] Native FTP relay: connection closes (?) after
 'cannot assign requested address' error
In-Reply-To: <832f83c5-dc2b-574b-9ab0-80142862bec9@treenet.co.nz>
References: <CACMUsGz1rDaY4ffe7vRp-u-cOANuufPfVo+BgvNTmi3t8ywOMA@mail.gmail.com>
 <832f83c5-dc2b-574b-9ab0-80142862bec9@treenet.co.nz>
Message-ID: <1485092987028-4681242.post@n4.nabble.com>

As far as I remember, I have tried both options, REDIRECT and TPROXY, but
TPROXY is the preferred one for us. I will try one more time on Monday.
However, I suppose that something else prevents squid from working properly.
Maybe on of sysctls, like net.ipv4.ip_nonlocal_bind, will do the trick.



--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/Native-FTP-relay-connection-closes-after-cannot-assign-requested-address-error-tp4681208p4681242.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From baborucki at gmail.com  Sun Jan 22 15:34:02 2017
From: baborucki at gmail.com (boruc)
Date: Sun, 22 Jan 2017 07:34:02 -0800 (PST)
Subject: [squid-users] Is it possible to modify cached object?
In-Reply-To: <b52d58fd-336e-b1b8-1453-38ec44e38870@measurement-factory.com>
References: <1483727739416-4681073.post@n4.nabble.com>
 <b52d58fd-336e-b1b8-1453-38ec44e38870@measurement-factory.com>
Message-ID: <1485099242343-4681243.post@n4.nabble.com>

So basically eCAP will allow me to modify any pages that is in response? What
about pages that are gzipped? Would I have to decode, modify it as I want
and encode? If you could write a "lifecycle" of object that is going to be
cached, what would it look like?

HTTP Request -> HTTP Response -> ContentAdaptation -> Cache ->
Refresh/Delete/OtherStuff



--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/Is-it-possible-to-modify-cached-object-tp4681073p4681243.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From david at articatech.com  Sun Jan 22 18:48:47 2017
From: david at articatech.com (David Touzeau)
Date: Sun, 22 Jan 2017 19:48:47 +0100
Subject: [squid-users] [3.5.23]: mozilla.org failed using SSL transparent
	SSL23_GET_SERVER_HELLO:unknown protocol
Message-ID: <006f01d274e0$2a10b500$7e321f00$@articatech.com>


Hi

I'm using SSL transparent method :

https_port 0.0.0.0:53695  intercept disable-pmtu-discovery=transparent
name=MyPortNameID22 ssl-bump  generate-host-certificates=on
dynamic_cert_mem_cache_size=4MB
cert=/etc/squid3/ssl/cb623e9bfc65772f68b84393604cd6ea.dyn

sslproxy_foreign_intermediate_certs /etc/squid3/intermediate_ca.pem
sslcrtd_program /lib/squid3/ssl_crtd -s /var/lib/squid/session/ssl/ssl_db -M
8MB
sslcrtd_children 16 startup=5 idle=1

acl ssl_step1 at_step SslBump1
acl ssl_step2 at_step SslBump2
acl ssl_step3 at_step SslBump3
ssl_bump peek ssl_step1
ssl_bump splice all

sslproxy_flags DONT_VERIFY_PEER
sslproxy_cert_error allow all

As you can see squid just intercept ssl queries and bump nothing ( just to
filter connections from url_rewrite program  and log ssl connections )

When connecting to mozilla.org using transparent, we receive this error:

* About to connect() to www.mozilla.org port 443 (#0)
*   Trying 104.16.41.2...
* connected
* Connected to www.mozilla.org (104.16.41.2) port 443 (#0)
* successfully set certificate verify locations:
*   CAfile: none
  CApath: /etc/ssl/certs
* SSLv3, TLS handshake, Client hello (1):
* error:140770FC:SSL routines:SSL23_GET_SERVER_HELLO:unknown protocol
* Closing connection #0
curl: (35) error:140770FC:SSL routines:SSL23_GET_SERVER_HELLO:unknown
protocol


And squid access.log

1485110919.564      3 192.168.1.236 TAG_NONE/403 6263 CONNECT
104.16.41.2:443 - HIER_NONE/- text/html

When using squid using standard port ( connected port/TUNNEL ) mozilla is
correctly dispalyed without any error.


How to whitelist mozilla.org without create a bypass iptables rule  ?


Best regards






From squid3 at treenet.co.nz  Sun Jan 22 20:00:14 2017
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Mon, 23 Jan 2017 09:00:14 +1300
Subject: [squid-users] Native FTP relay: connection closes (?) after
 'cannot assign requested address' error
In-Reply-To: <1485092987028-4681242.post@n4.nabble.com>
References: <CACMUsGz1rDaY4ffe7vRp-u-cOANuufPfVo+BgvNTmi3t8ywOMA@mail.gmail.com>
 <832f83c5-dc2b-574b-9ab0-80142862bec9@treenet.co.nz>
 <1485092987028-4681242.post@n4.nabble.com>
Message-ID: <5a1e3e82-4f84-7246-0437-113dd99e957b@treenet.co.nz>

On 23/01/2017 2:49 a.m., Alexander wrote:
> As far as I remember, I have tried both options, REDIRECT and TPROXY, but
> TPROXY is the preferred one for us. I will try one more time on Monday.
> However, I suppose that something else prevents squid from working properly.
> Maybe on of sysctls, like net.ipv4.ip_nonlocal_bind, will do the trick.

Maybe.

I expect that REDIRECT will be the required way for FTP at present,
since TPROXY has requirements that there is a client socket state to
associate with the non-local binding. Essentially that sockets are
opened directionally in sequence client->proxy->server - whereas FTP
data connections are opened in the opposite sequencing order:
server->squid first, then squid->client.

Amos



From squid3 at treenet.co.nz  Sun Jan 22 20:33:23 2017
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Mon, 23 Jan 2017 09:33:23 +1300
Subject: [squid-users] Is it possible to modify cached object?
In-Reply-To: <1485099242343-4681243.post@n4.nabble.com>
References: <1483727739416-4681073.post@n4.nabble.com>
 <b52d58fd-336e-b1b8-1453-38ec44e38870@measurement-factory.com>
 <1485099242343-4681243.post@n4.nabble.com>
Message-ID: <97375954-3157-92e4-f557-ea62b74a04a7@treenet.co.nz>

On 23/01/2017 4:34 a.m., boruc wrote:
> So basically eCAP will allow me to modify any pages that is in response? What
> about pages that are gzipped? Would I have to decode, modify it as I want
> and encode?

Yes you would. Squid just passes the data it gets.

> If you could write a "lifecycle" of object that is going to be
> cached, what would it look like?
> 
> HTTP Request -> HTTP Response -> ContentAdaptation -> Cache ->
> Refresh/Delete/OtherStuff

Not even close.

A request goes through REQMOD-precache adaptation and other stages
before the cache lookup happens.

 request -> stuff -> REQMOD adaptation -> stuff -> cache [ -> upstream
server ]


There is a 1:1 relationship between request and response on the client
connection. But multiple transactions may be occuring with server(s) to
produce that response.
RESPMOD-precache adaptation happens for each server response, before the
adapted content is delivered to the cache or the client.

 server response -> RESPMOD adaptation -> cache [ -> client response ]


In both chains the last entry is in [] since it may not exist.


Amos



From rentorbuy at yahoo.com  Sun Jan 22 23:02:50 2017
From: rentorbuy at yahoo.com (Vieri)
Date: Sun, 22 Jan 2017 23:02:50 +0000 (UTC)
Subject: [squid-users] squid reverse proxy (accelerator) for MS Exchange
 OWA
In-Reply-To: <5517395f-eaa4-0b3a-34e9-66ea8d0122a1@treenet.co.nz>
References: <1216522442.160455.1484870639800.ref@mail.yahoo.com>
 <1216522442.160455.1484870639800@mail.yahoo.com>
 <cdea0afa-c2a2-23c4-dbac-553a565e83f7@treenet.co.nz>
 <690032068.431187.1484905491833@mail.yahoo.com>
 <5517395f-eaa4-0b3a-34e9-66ea8d0122a1@treenet.co.nz>
Message-ID: <1060030591.1598935.1485126170814@mail.yahoo.com>





----- Original Message -----
From: Amos Jeffries <squid3 at treenet.co.nz>
>
> You could try with a newer Squid version since the bio.cc code might be

> making something else happen in 3.5.23. If that still fails the 4.0 beta
> has different logic and far better debug info in this area.

I tried 3.5.23 and I finally got a clear hint.
Basically, I was missing sslcafile.
My setup works now.

Thanks

Vieri


From frio_cervesa at hotmail.com  Sun Jan 22 23:08:28 2017
From: frio_cervesa at hotmail.com (senor)
Date: Sun, 22 Jan 2017 23:08:28 +0000
Subject: [squid-users] cert mem cache
Message-ID: <BN6PR17MB114081C2458ADB8C2FEC446EF7730@BN6PR17MB1140.namprd17.prod.outlook.com>

Hello all,
Is the use of dynamic_cert_mem_cache_size=SIZE on the http_port
directive any different with and without using sslcrtd_program?

Should there be a specific relationship between the amount of memory or
disk configured for the two?

On a slight tangent, what performance improvement could be expected by
using ssl_crtd? What metrics would be best to view if comparing with and
without?

Thanks in advance.
Senor

From goal81 at gmail.com  Mon Jan 23 10:11:24 2017
From: goal81 at gmail.com (Alexander)
Date: Mon, 23 Jan 2017 02:11:24 -0800 (PST)
Subject: [squid-users] Native FTP relay: connection closes (?) after
 'cannot assign requested address' error
In-Reply-To: <5a1e3e82-4f84-7246-0437-113dd99e957b@treenet.co.nz>
References: <CACMUsGz1rDaY4ffe7vRp-u-cOANuufPfVo+BgvNTmi3t8ywOMA@mail.gmail.com>
 <832f83c5-dc2b-574b-9ab0-80142862bec9@treenet.co.nz>
 <1485092987028-4681242.post@n4.nabble.com>
 <5a1e3e82-4f84-7246-0437-113dd99e957b@treenet.co.nz>
Message-ID: <1485166284501-4681250.post@n4.nabble.com>

Just tried it out with REDIRECT rule. Still no luck, but now Filezilla client
reports ECONNREFUSED error. I do not see any critical errors in squid's
output, however the following thing is suspicious:

2017/01/20 19:10:11.604| 33,3| FtpServer.cc(1655) checkDataConnPost: missing
client data conn: 
2017/01/20 19:10:11.604| 33,7| FtpServer.cc(1190)
writeForwardedReplyAndCall: wait for the client to establish a data
connection

I have tcpdump'ed client's and vsftpd's interfaces and seen the following:
1. A client successfully connects to a server: squid forwards requests
properly.
2. When entering passive mode and executing the LIST command, squid opens a
data connection and receives data from vsftpd.
3. Squid opens a local port and sends it back to client via the "Entering
passive mode" reply. Seems to be ok, but a client sees a real server's IP
address, not a squid's one. So when a client tries to connect to a server,
it gets ECONNREFUSED because no-one is listening on a requested port.

Probably there means an issue with handlePasvReply(), but it's just a guess.

Taking 'wait for the client to establish a data connection' into account, it
seems that squid wants a client to connect, just to forward a data received
from a server. However a client attempts to make a direct connection and
things break down.



--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/Native-FTP-relay-connection-closes-after-cannot-assign-requested-address-error-tp4681208p4681250.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From yvoinov at gmail.com  Mon Jan 23 11:28:30 2017
From: yvoinov at gmail.com (Yuri)
Date: Mon, 23 Jan 2017 17:28:30 +0600
Subject: [squid-users] Squid 4.x: Intermediate certificates downloader
Message-ID: <4198a8d7-07c7-8c74-80ac-eb75ca61e62c@gmail.com>

Hi, gents.

I have some stupid questions about subject.

1. How does it work? I.e., where downloaded certs stored, how it 
handles, does it saves anywhere to disk? Because of this feature is 
completely undocumented and it did not follow from the source code.

2. How this feature is related to sslproxy_foreign_intermediate_certs, 
how it can interfere with it? Because of this one also completely 
undocunemted and nobody has enough unlimited free time to study complete 
sources of squid to understand features.

If somebody knows answers, please, I ask them to voice it.

Release notes contains nothing about this feature. Wiki contains only 
one mention in passing that this functionality exists in principle.

Thanks in advance.






From cf at utc.fr  Mon Jan 23 11:57:32 2017
From: cf at utc.fr (Christophe Fillot)
Date: Mon, 23 Jan 2017 12:57:32 +0100
Subject: [squid-users] Strange delays (30 seconds) with TLS connections in
 WCCP/Transparent mode
Message-ID: <f3591d3f-0c99-8a89-0455-7fc1bcecafbb@utc.fr>

Hello all,

I have a strange problem where some TLS connections are delayed by 30 
seconds when going through my transparent proxy with WCCP. This occurs 
typically with sites behind Cloudflare (for example, 
https://www.wireshark.org). No problem for Google websites for example.

I only want to log the SNI hostname, I do *not* want to 
intercept/decrypt/re-encrypt connections with fake certificates.

Here is the setup:

     - Linux Debian 7
     - OpenSSL 1.0.1t
     - Squid 3.5.23 (also tested Squid 4.0.17).

The Squid configure options are:

./configure --enable-delay-pools --with-large-files --enable-async-io 
--enable-icmp --enable-kill-parent-hack
--enable-htpc --enable-forw-via-db --enable-cache-digests 
--enable-dl-malloc --with-large-files
--enable-linux-netfilter --enable-ssl --enable-ssl-crtd --with-openssl

IPTables configuration (the routing device sending WCCP frames is a 
Cisco ASA):

iptables -t nat -A PREROUTING -i wccp0 -p tcp -m tcp --dport 443 -j DNAT 
--to-destination 195.83.155.53:3130

The output/public interface is eth0, the traffic returns to clients 
through eth1 with the following:

iptables -t mangle -A OUTPUT -p tcp --sport 3130  -j MARK --set-mark 900
ip rule add fwmark 900 table 1

(The table 1 allows direct access to client networks with the 
appropriate routes, this is needed because the return traffic must not 
go through the ASA).

The SQUID configuration:

https_port 3130 intercept ssl-bump cert=/usr/local/squid/etc/proxyCA.pem

acl step1 at_step SslBump1
acl step2 at_step SslBump2
ssl_bump peek step1 all
ssl_bump splice step2 all

sslcrtd_program  /usr/local/squid/libexec/ssl_crtd -s 
/usr/local/squid/var/ssl_db -M 40MB
sslcrtd_children 5
wccp2_service dynamic 70 password=XXXXXXXXX
wccp2_service_info 70 protocol=tcp flags=src_ip_hash,dst_ip_hash 
priority=240 ports=443

Even if I use only "ssl_bump splice all" the 30-second delay happens.

I have an example of captured traffic here, on the client and the 
various network interfaces on the proxy server: 
http://www.utc.fr/filex/get?k=6Dt169xGsHMswCKEF5L

As you can see in the captures, Squid returns the "Server Hello" 30 
seconds (in cap_eth1.cap) after it has received it from the server (in 
cap_eth0.cap).

This behavior is not systematic, sometimes the data is returned 
immediately. What could cause this delay ? This looks like some timeout, 
but for what reason ?

Thanks in advance for any suggestion !

Christophe




From emmanuel.fuste at thalesgroup.com  Mon Jan 23 14:58:07 2017
From: emmanuel.fuste at thalesgroup.com (FUSTE Emmanuel)
Date: Mon, 23 Jan 2017 15:58:07 +0100
Subject: [squid-users] Squid 3.x never_direct and DNS requests problem.
Message-ID: <69f299e4-c302-c693-9a37-1aabf96b1cc0@thalesgroup.com>

Hello,

I'm in a context where I have a lot of Squid installation without direct 
internet access.
All queries are forwarded to an Internet connected peer.

Recently, I migrate my old 2.x Squid to 3.x and take responsibility for 
some other 3.x existing installations.
- my Debian based Squid 3.4.8 start doing DNS request for each requested 
domain
- Ubuntu 14.04 based Squid 3.3.8 behave the same
- Ubuntu 16.04 based Squid 3.5.12 behave the same
The internal DNS setup is completely private with it's own hierarchy an 
with no Internet link/relation.
Internet "like" request are banned on this infrastructure and could 
raise alarms.

On the Ubuntu installations, the problem was worked around with a local 
nsd daemon responsible to answer "nxdomain" to all requests.

All was carefully checked and nothing in my configuration (acl etc ...) 
explain why Squid insist to do DNS requests for requests forwarded to 
the peer(s).

I was able to reproduce the "bug" with all squid versions up to 3.5.23 
with this minimalist config test file:
----------------------------
http_access allow all

http_port 3128
cache_peer 10.xx.xx.xx parent 8000 0 default no-query no-digest 
login=login:password
never_direct allow all

cache_mem 256 MB
maximum_object_size_in_memory 16384 KB
cache_dir aufs /var/spool/squid3 100000 32 256
maximum_object_size 400 MB
access_log stdio:/var/log/squid/access.log squid

refresh_pattern ^ftp:           1440    20%     10080
refresh_pattern ^gopher:        1440    0%      1440
refresh_pattern -i (/cgi-bin/|\?) 0     0%      0
refresh_pattern .               0       20%     4320

quick_abort_pct 55
read_ahead_gap 128 KB
hosts_file none
coredump_dir /var/spool/squid3

#bug #4575
url_rewrite_extras XXX
store_id_extras XXX
------------------------------------

Since the switch from 3.5.12 to 3.5.19/23, I am able to use a simpler 
work around (I switched directly from 3.5.12 to 3.5.19 so I don't know 
when the behavior changed):
Instead of installing a fake local DNS server and using
dns_nameservers 127.0.0.1
I could use
dns_nameservers none
Squid warn about non usable DNS and proceed normally. Before (tested 
with 3.5.12 and lower) Squid hang.

So, I am missing something ? Is it a know problem ?
With the work around, things work but I could not logs things based on 
Internal DNS for the client side, and this is something that was working 
in the old 2.x versions.
Should I open a bug report ?

Thank you,
Emmanuel.



From rousskov at measurement-factory.com  Mon Jan 23 17:31:49 2017
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Mon, 23 Jan 2017 10:31:49 -0700
Subject: [squid-users] Squid 4.x: Intermediate certificates downloader
In-Reply-To: <4198a8d7-07c7-8c74-80ac-eb75ca61e62c@gmail.com>
References: <4198a8d7-07c7-8c74-80ac-eb75ca61e62c@gmail.com>
Message-ID: <35afc9c9-dd43-fb17-d0d6-c90866fd5c28@measurement-factory.com>

On 01/23/2017 04:28 AM, Yuri wrote:

> 1. How does it work? 

My response below and the following commit message might answer some of
your questions:

    http://bazaar.launchpad.net/~squid/squid/5/revision/14769

> I.e., where downloaded certs stored, how it
> handles, does it saves anywhere to disk?

Missing certificates are fetched using HTTP[S]. Certificate responses
should be treated as any other HTTP[S] responses with regard to caching.
For example, if you have disk caching enabled and your caching rules
(including defaults) allow certificate response caching, then the
response should be cached. Similarly, the cached certificate will
eventually be evicted from the cache following regular cache maintenance
rules. When that happens, Squid will try to fetch the certificate again
(if it becomes needed again).


> 2. How this feature is related to sslproxy_foreign_intermediate_certs,
> how it can interfere with it?

AFAICT by looking at the code, Squid only downloads certificates that
Squid is missing when trying to build a complete certificate chain for a
given server connection. Any sslproxy_foreign_intermediate_certs are
used as needed during the chain building process (i.e., they are _not_
"missing").


> Release notes contains nothing about this feature. Wiki contains only
> one mention in passing that this functionality exists in principle.

I agree that this feature lacks documentation. This is, in part, because
the feature has no configuration options that normally force developers
to document at least some of the code logic. We should add a few words
about it to sslproxy_foreign_intermediate_certs documentation.


FWIW, we are also adding an ACL to identify internal transactions that
fetch missing certificates.


HTH,

Alex.



From yvoinov at gmail.com  Mon Jan 23 17:41:12 2017
From: yvoinov at gmail.com (Yuri Voinov)
Date: Mon, 23 Jan 2017 23:41:12 +0600
Subject: [squid-users] Squid 4.x: Intermediate certificates downloader
In-Reply-To: <35afc9c9-dd43-fb17-d0d6-c90866fd5c28@measurement-factory.com>
References: <4198a8d7-07c7-8c74-80ac-eb75ca61e62c@gmail.com>
 <35afc9c9-dd43-fb17-d0d6-c90866fd5c28@measurement-factory.com>
Message-ID: <b3e34d8e-ae0d-d59c-1f1e-07643934b81b@gmail.com>



23.01.2017 23:31, Alex Rousskov ?????:
> On 01/23/2017 04:28 AM, Yuri wrote:
>
>> 1. How does it work? 
> My response below and the following commit message might answer some of
> your questions:
>
>     http://bazaar.launchpad.net/~squid/squid/5/revision/14769
>
>> I.e., where downloaded certs stored, how it
>> handles, does it saves anywhere to disk?
> Missing certificates are fetched using HTTP[S]. Certificate responses
> should be treated as any other HTTP[S] responses with regard to caching.
> For example, if you have disk caching enabled and your caching rules
> (including defaults) allow certificate response caching, then the
> response should be cached. Similarly, the cached certificate will
> eventually be evicted from the cache following regular cache maintenance
> rules. When that happens, Squid will try to fetch the certificate again
> (if it becomes needed again).
I.e., fetchesd intermediate certificate stores only in memory cache for

sslcrtd_program /usr/local/squid/libexec/security_file_certgen -s
/var/lib/ssl_db -M 4MB

daemon, right? And never stores anywhere on disk?
>
>
>> 2. How this feature is related to sslproxy_foreign_intermediate_certs,
>> how it can interfere with it?
> AFAICT by looking at the code, Squid only downloads certificates that
> Squid is missing when trying to build a complete certificate chain for a
> given server connection. Any sslproxy_foreign_intermediate_certs are
> used as needed during the chain building process (i.e., they are _not_
> "missing").
Ok, so, this file uses for complete chains, and it contains statically
added (manually) certs only, right?

I.e., downloader should not save fetched intermediate CA's here, which
will be logically, isn't it?
>
>
>> Release notes contains nothing about this feature. Wiki contains only
>> one mention in passing that this functionality exists in principle.
> I agree that this feature lacks documentation. This is, in part, because
> the feature has no configuration options that normally force developers
> to document at least some of the code logic. We should add a few words
> about it to sslproxy_foreign_intermediate_certs documentation.
>
>
> FWIW, we are also adding an ACL to identify internal transactions that
> fetch missing certificates.
>
>
> HTH,
>
> Alex.
>

-------------- next part --------------
A non-text attachment was scrubbed...
Name: 0x613DEC46.asc
Type: application/pgp-keys
Size: 2437 bytes
Desc: not available
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20170123/d50257b0/attachment.key>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: signature.asc
Type: application/pgp-signature
Size: 473 bytes
Desc: OpenPGP digital signature
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20170123/d50257b0/attachment.sig>

From marcus.kool at urlfilterdb.com  Mon Jan 23 18:06:12 2017
From: marcus.kool at urlfilterdb.com (Marcus Kool)
Date: Mon, 23 Jan 2017 16:06:12 -0200
Subject: [squid-users] Squid 4.x: Intermediate certificates downloader
In-Reply-To: <35afc9c9-dd43-fb17-d0d6-c90866fd5c28@measurement-factory.com>
References: <4198a8d7-07c7-8c74-80ac-eb75ca61e62c@gmail.com>
 <35afc9c9-dd43-fb17-d0d6-c90866fd5c28@measurement-factory.com>
Message-ID: <52b0aec8-f61d-1bec-de18-1c3be06494a0@urlfilterdb.com>



On 23/01/17 15:31, Alex Rousskov wrote:
> On 01/23/2017 04:28 AM, Yuri wrote:
>
>> 1. How does it work?
>
> My response below and the following commit message might answer some of
> your questions:
>
>     http://bazaar.launchpad.net/~squid/squid/5/revision/14769

This seems that the feature only goes to Squid 5.  Will it be ported to Squid 4 ?

>> I.e., where downloaded certs stored, how it
>> handles, does it saves anywhere to disk?
>
> Missing certificates are fetched using HTTP[S]. Certificate responses
> should be treated as any other HTTP[S] responses with regard to caching.
> For example, if you have disk caching enabled and your caching rules
> (including defaults) allow certificate response caching, then the
> response should be cached. Similarly, the cached certificate will
> eventually be evicted from the cache following regular cache maintenance
> rules. When that happens, Squid will try to fetch the certificate again
> (if it becomes needed again).
>
>
>> 2. How this feature is related to sslproxy_foreign_intermediate_certs,
>> how it can interfere with it?
>
> AFAICT by looking at the code, Squid only downloads certificates that
> Squid is missing when trying to build a complete certificate chain for a
> given server connection. Any sslproxy_foreign_intermediate_certs are
> used as needed during the chain building process (i.e., they are _not_
> "missing").

I created bug report http://bugs.squid-cache.org/show_bug.cgi?id=4659
a week ago but there has not been any activity.
Is there someone who has sslproxy_foreign_intermediate_certs
working in Squid 4.0.17 ?

Thanks,
Marcus

[snip]

> HTH,
>
> Alex.
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>


From rousskov at measurement-factory.com  Mon Jan 23 18:06:47 2017
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Mon, 23 Jan 2017 11:06:47 -0700
Subject: [squid-users] Squid 4.x: Intermediate certificates downloader
In-Reply-To: <b3e34d8e-ae0d-d59c-1f1e-07643934b81b@gmail.com>
References: <4198a8d7-07c7-8c74-80ac-eb75ca61e62c@gmail.com>
 <35afc9c9-dd43-fb17-d0d6-c90866fd5c28@measurement-factory.com>
 <b3e34d8e-ae0d-d59c-1f1e-07643934b81b@gmail.com>
Message-ID: <03abbc4e-79a7-6388-782f-c94d3c1333c9@measurement-factory.com>

On 01/23/2017 10:41 AM, Yuri Voinov wrote:
> 23.01.2017 23:31, Alex Rousskov ?????:
>> On 01/23/2017 04:28 AM, Yuri wrote:
>>> I.e., where downloaded certs stored, how it
>>> handles, does it saves anywhere to disk?

>> Missing certificates are fetched using HTTP[S]. Certificate responses
>> should be treated as any other HTTP[S] responses with regard to caching.
>> For example, if you have disk caching enabled and your caching rules
>> (including defaults) allow certificate response caching, then the
>> response should be cached. Similarly, the cached certificate will
>> eventually be evicted from the cache following regular cache maintenance
>> rules. When that happens, Squid will try to fetch the certificate again
>> (if it becomes needed again).

> I.e., fetchesd intermediate certificate stores only in memory cache for
> 
> sslcrtd_program /usr/local/squid/libexec/security_file_certgen -s
> /var/lib/ssl_db -M 4MB
> 
> daemon, right? And never stores anywhere on disk?

No, this is incorrect -- sslcrtd_program settings are independent from
fetching missing certificates. The ssl_crtd helper is about fake
certificate generation. The helper does not use the Squid cache to cache
its results. The "missing certificates" features are about the virgin
server certificates that are necessary to complete/validate the server
chain but absent from the server's ServerHello response.

The only relationship between the ssl_crtd helper and fetching of the
missing certificates (that I can think of) is that the helper will mimic
the fetched certificates (in some cases). However, I am not even sure
whether the helper gets the virgin incomplete certificate chain or the
completed-by-Squid certificate chain in such cases. I only suspect that
it is the latter. @Christos, please correct me if my suspicion is wrong.


>>> 2. How this feature is related to sslproxy_foreign_intermediate_certs,
>>> how it can interfere with it?
>> AFAICT by looking at the code, Squid only downloads certificates that
>> Squid is missing when trying to build a complete certificate chain for a
>> given server connection. Any sslproxy_foreign_intermediate_certs are
>> used as needed during the chain building process (i.e., they are _not_
>> "missing").

> Ok, so, this file uses for complete chains, and it contains statically
> added (manually) certs only, right?

Yes, the sslproxy_foreign_intermediate_certs file is maintained by the
Squid administrator. Squid does not update it.


> I.e., downloader should not save fetched intermediate CA's here,

Correct.


> which will be logically, isn't it?

I believe it is better to use the regular Squid cache for storing the
fetched missing certificates. I would not call abusing the
sslproxy_foreign_intermediate_certs file for this purpose completely
illogical, but such abuse would create more problems than it would solve
IMO. We have also considered using a dedicated storage for the fetched
missing certificates, but have decided (for many reasons) that it would
be worse than reusing the existing caching infrastructure.

FWIW, IMO, storing the generated fake certificates in the regular Squid
cache would also be better than using an OpenSSL-administered database.


HTH,

Alex.



From goal81 at gmail.com  Mon Jan 23 18:11:31 2017
From: goal81 at gmail.com (Alexander)
Date: Mon, 23 Jan 2017 10:11:31 -0800 (PST)
Subject: [squid-users] Native FTP relay: connection closes (?) after
 'cannot assign requested address' error
In-Reply-To: <5a1e3e82-4f84-7246-0437-113dd99e957b@treenet.co.nz>
References: <CACMUsGz1rDaY4ffe7vRp-u-cOANuufPfVo+BgvNTmi3t8ywOMA@mail.gmail.com>
 <832f83c5-dc2b-574b-9ab0-80142862bec9@treenet.co.nz>
 <1485092987028-4681242.post@n4.nabble.com>
 <5a1e3e82-4f84-7246-0437-113dd99e957b@treenet.co.nz>
Message-ID: <1485195091976-4681258.post@n4.nabble.com>

Actually, a PASV-handling logic looks a bit strange to me. In
Ftp::Server::handlePasvReply() there is a comment:

"In interception setups, we combine remote server address with a local port
number and hope that traffic will be redirected to us."

How is it supposed to work? A client receives server's IP and squid's port
and tries to make a connection, which obviously ends with ECONNREFUSED. I do
not have any idea on how a traffic could be redirected to squid (redirecting
everything from A to B is not an option). Also, why squid needs to intercept
a data connection?

If I hardcode one of squid's IP in handlePasvReply(), everything works fine.
However I am not sure if it is a correct way because a client opens a data
connection not to an FTP server...




--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/Native-FTP-relay-connection-closes-after-cannot-assign-requested-address-error-tp4681208p4681258.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From rousskov at measurement-factory.com  Mon Jan 23 18:23:27 2017
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Mon, 23 Jan 2017 11:23:27 -0700
Subject: [squid-users] Native FTP relay: connection closes (?) after
 'cannot assign requested address' error
In-Reply-To: <1485166284501-4681250.post@n4.nabble.com>
References: <CACMUsGz1rDaY4ffe7vRp-u-cOANuufPfVo+BgvNTmi3t8ywOMA@mail.gmail.com>
 <832f83c5-dc2b-574b-9ab0-80142862bec9@treenet.co.nz>
 <1485092987028-4681242.post@n4.nabble.com>
 <5a1e3e82-4f84-7246-0437-113dd99e957b@treenet.co.nz>
 <1485166284501-4681250.post@n4.nabble.com>
Message-ID: <8b885b8b-a6ed-17be-cac7-b120de1cd1bc@measurement-factory.com>

On 01/23/2017 03:11 AM, Alexander wrote:

> 3. Squid opens a local port and sends it back to client via the "Entering
> passive mode" reply. Seems to be ok, but a client sees a real server's IP
> address, not a squid's one. So when a client tries to connect to a server,
> it gets ECONNREFUSED because no-one is listening on a requested port.


This Squid behavior is intentional:

>     // In interception setups, we combine remote server address with a
>     // local port number and hope that traffic will be redirected to us.
...
>     mb.appendf("227 Entering Passive Mode (%s,%i,%i).\r\n",


> So when a client tries to connect to a server,

... your networking rules should redirect that connection to Squid in
order to avoid the problem you are describing:


> it gets ECONNREFUSED because no-one is listening on a requested port.

Please note that I am _not_ claiming that the intentional Squid behavior
is correct in all cases. I only know that we made Squid do what it does
now to fix a (most likely real) problem:

> revno: 12742.1.11
> branch nick: ftp-gw
> timestamp: Wed 2013-08-21 09:39:09 -0600
> message:
>   Fixed address handling for PASV responses in interception cases.


HTH,

Alex.



From rousskov at measurement-factory.com  Mon Jan 23 18:41:53 2017
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Mon, 23 Jan 2017 11:41:53 -0700
Subject: [squid-users] Native FTP relay: connection closes (?) after
 'cannot assign requested address' error
In-Reply-To: <1485195091976-4681258.post@n4.nabble.com>
References: <CACMUsGz1rDaY4ffe7vRp-u-cOANuufPfVo+BgvNTmi3t8ywOMA@mail.gmail.com>
 <832f83c5-dc2b-574b-9ab0-80142862bec9@treenet.co.nz>
 <1485092987028-4681242.post@n4.nabble.com>
 <5a1e3e82-4f84-7246-0437-113dd99e957b@treenet.co.nz>
 <1485195091976-4681258.post@n4.nabble.com>
Message-ID: <8d4fc039-bbd8-3a23-cae1-84ff43ff40e6@measurement-factory.com>

On 01/23/2017 11:11 AM, Alexander wrote:
> Actually, a PASV-handling logic looks a bit strange to me. In
> Ftp::Server::handlePasvReply() there is a comment:
> 
> "In interception setups, we combine remote server address with a local port
> number and hope that traffic will be redirected to us."
> 
> How is it supposed to work? I do
> not have any idea on how a traffic could be redirected to squid (redirecting
> everything from A to B is not an option).

You should only redirect FTP traffic, of course. Sorry, I do not know
how you can identify FTP data traffic in your environment, but I am sure
there are tools that can do that in some environments (e.g., by
monitoring FTP 227 responses on the already redirected connections).
There are also some ideas for future work below in case nobody can
suggest anything better.


> Also, why squid needs to intercept a data connection?

For the same set of reasons Squid needs to intercept everything else --
traffic logging, blocking, and adaptation. If you want Squid to proxy a
"message", Squid expects to proxy the entire "message". In FTP, a single
"message" (from high-level point of view) is often split among two or
more connections (from TCP point of view).

Needless to say, your specific needs may differ from that general
principle. It is possible that Squid needs a knob to handle your use
case differently. However, I am pretty sure that somebody does want
Squid to do what it does know so we should not change Squid behavior to
satisfy your use case.


> If I hardcode one of squid's IP in handlePasvReply(), everything works fine.
> However I am not sure if it is a correct way because a client opens a data
> connection not to an FTP server...

I agree that mixing intercepted [control] and direct [data] connections
is a bad design in general, even if it works in your use case. In many
cases, Squid IP address is not even reachable from the client!
Hopefully, you can find a better way to handle this.

What if you can restrict the set of ports that Squid uses to accept
passive FTP data connections? That way, you can redirect only those data
connections that match those ports. This is not an ideal solution, and
Squid does not support that directly right now, but it might work in
principle.

Another option is to modify Squid to report the expected data connection
IP:ports to some helper so that you can write a script that dynamically
modifies your network redirection rules.

Others may know a better way to handle this (short of deploying an
FTP-aware L7 networking gear).


Cheers,

Alex.



From goal81 at gmail.com  Mon Jan 23 19:18:15 2017
From: goal81 at gmail.com (Alexander)
Date: Mon, 23 Jan 2017 22:18:15 +0300
Subject: [squid-users] Native FTP relay: connection closes (?) after
 'cannot assign requested address' error
In-Reply-To: <8d4fc039-bbd8-3a23-cae1-84ff43ff40e6@measurement-factory.com>
References: <CACMUsGz1rDaY4ffe7vRp-u-cOANuufPfVo+BgvNTmi3t8ywOMA@mail.gmail.com>
 <832f83c5-dc2b-574b-9ab0-80142862bec9@treenet.co.nz>
 <1485092987028-4681242.post@n4.nabble.com>
 <5a1e3e82-4f84-7246-0437-113dd99e957b@treenet.co.nz>
 <1485195091976-4681258.post@n4.nabble.com>
 <8d4fc039-bbd8-3a23-cae1-84ff43ff40e6@measurement-factory.com>
Message-ID: <CACMUsGw1CxzsPBA6-Tz7zy-RqpnxFn_UqLugz19o9VXR-cLEqg@mail.gmail.com>

2017-01-23 21:41 GMT+03:00 Alex Rousskov <rousskov at measurement-factory.com>:

>
> Needless to say, your specific needs may differ from that general
> principle. It is possible that Squid needs a knob to handle your use
> case differently. However, I am pretty sure that somebody does want
> Squid to do what it does know so we should not change Squid behavior to
> satisfy your use case.
>

I understand that, however the first and foremost reason I asked the
question was that my use case pretends to be pretty typical :)


> What if you can restrict the set of ports that Squid uses to accept
> passive FTP data connections? That way, you can redirect only those data
> connections that match those ports. This is not an ideal solution, and
> Squid does not support that directly right now, but it might work in
> principle.
>

I have thought about it, however these ports may interfere with real
server's ports.


> Another option is to modify Squid to report the expected data connection
> IP:ports to some helper so that you can write a script that dynamically
> modifies your network redirection rules.
>

I like this one more. It looks like a kind of ip_conntrack_ftp.

Thank you for the explanation, I will try to do something.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20170123/cf3ad42e/attachment.htm>

From yvoinov at gmail.com  Mon Jan 23 19:22:31 2017
From: yvoinov at gmail.com (Yuri Voinov)
Date: Tue, 24 Jan 2017 01:22:31 +0600
Subject: [squid-users] Squid 4.x: Intermediate certificates downloader
In-Reply-To: <03abbc4e-79a7-6388-782f-c94d3c1333c9@measurement-factory.com>
References: <4198a8d7-07c7-8c74-80ac-eb75ca61e62c@gmail.com>
 <35afc9c9-dd43-fb17-d0d6-c90866fd5c28@measurement-factory.com>
 <b3e34d8e-ae0d-d59c-1f1e-07643934b81b@gmail.com>
 <03abbc4e-79a7-6388-782f-c94d3c1333c9@measurement-factory.com>
Message-ID: <5ca1a459-ac17-2b4a-3bb2-2041525b4df9@gmail.com>



24.01.2017 0:06, Alex Rousskov ?????:
> On 01/23/2017 10:41 AM, Yuri Voinov wrote:
>> 23.01.2017 23:31, Alex Rousskov ?????:
>>> On 01/23/2017 04:28 AM, Yuri wrote:
>>>> I.e., where downloaded certs stored, how it
>>>> handles, does it saves anywhere to disk?
>>> Missing certificates are fetched using HTTP[S]. Certificate responses
>>> should be treated as any other HTTP[S] responses with regard to caching.
>>> For example, if you have disk caching enabled and your caching rules
>>> (including defaults) allow certificate response caching, then the
>>> response should be cached. Similarly, the cached certificate will
>>> eventually be evicted from the cache following regular cache maintenance
>>> rules. When that happens, Squid will try to fetch the certificate again
>>> (if it becomes needed again).
>> I.e., fetchesd intermediate certificate stores only in memory cache for
>>
>> sslcrtd_program /usr/local/squid/libexec/security_file_certgen -s
>> /var/lib/ssl_db -M 4MB
>>
>> daemon, right? And never stores anywhere on disk?
> No, this is incorrect -- sslcrtd_program settings are independent from
> fetching missing certificates. The ssl_crtd helper is about fake
> certificate generation. The helper does not use the Squid cache to cache
> its results. The "missing certificates" features are about the virgin
> server certificates that are necessary to complete/validate the server
> chain but absent from the server's ServerHello response.
>
> The only relationship between the ssl_crtd helper and fetching of the
> missing certificates (that I can think of) is that the helper will mimic
> the fetched certificates (in some cases). However, I am not even sure
> whether the helper gets the virgin incomplete certificate chain or the
> completed-by-Squid certificate chain in such cases. I only suspect that
> it is the latter. @Christos, please correct me if my suspicion is wrong.
>
>
>>>> 2. How this feature is related to sslproxy_foreign_intermediate_certs,
>>>> how it can interfere with it?
>>> AFAICT by looking at the code, Squid only downloads certificates that
>>> Squid is missing when trying to build a complete certificate chain for a
>>> given server connection. Any sslproxy_foreign_intermediate_certs are
>>> used as needed during the chain building process (i.e., they are _not_
>>> "missing").
>> Ok, so, this file uses for complete chains, and it contains statically
>> added (manually) certs only, right?
> Yes, the sslproxy_foreign_intermediate_certs file is maintained by the
> Squid administrator. Squid does not update it.
>
>
>> I.e., downloader should not save fetched intermediate CA's here,
> Correct.
>
>
>> which will be logically, isn't it?
> I believe it is better to use the regular Squid cache for storing the
> fetched missing certificates. I would not call abusing the
> sslproxy_foreign_intermediate_certs file for this purpose completely
> illogical, but such abuse would create more problems than it would solve
> IMO. We have also considered using a dedicated storage for the fetched
> missing certificates, but have decided (for many reasons) that it would
> be worse than reusing the existing caching infrastructure.
>
> FWIW, IMO, storing the generated fake certificates in the regular Squid
> cache would also be better than using an OpenSSL-administered database.
Exactly.
>
>
> HTH,
>
> Alex.
>

-------------- next part --------------
A non-text attachment was scrubbed...
Name: 0x613DEC46.asc
Type: application/pgp-keys
Size: 2437 bytes
Desc: not available
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20170124/84983bf9/attachment.key>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: signature.asc
Type: application/pgp-signature
Size: 473 bytes
Desc: OpenPGP digital signature
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20170124/84983bf9/attachment.sig>

From yvoinov at gmail.com  Mon Jan 23 19:23:34 2017
From: yvoinov at gmail.com (Yuri Voinov)
Date: Tue, 24 Jan 2017 01:23:34 +0600
Subject: [squid-users] Squid 4.x: Intermediate certificates downloader
In-Reply-To: <52b0aec8-f61d-1bec-de18-1c3be06494a0@urlfilterdb.com>
References: <4198a8d7-07c7-8c74-80ac-eb75ca61e62c@gmail.com>
 <35afc9c9-dd43-fb17-d0d6-c90866fd5c28@measurement-factory.com>
 <52b0aec8-f61d-1bec-de18-1c3be06494a0@urlfilterdb.com>
Message-ID: <2b5d58f2-c338-70ac-f2a5-0664de64d88e@gmail.com>



24.01.2017 0:06, Marcus Kool ?????:
>
>
> On 23/01/17 15:31, Alex Rousskov wrote:
>> On 01/23/2017 04:28 AM, Yuri wrote:
>>
>>> 1. How does it work?
>>
>> My response below and the following commit message might answer some of
>> your questions:
>>
>>     http://bazaar.launchpad.net/~squid/squid/5/revision/14769
>
> This seems that the feature only goes to Squid 5.  Will it be ported
> to Squid 4 ?
>
>>> I.e., where downloaded certs stored, how it
>>> handles, does it saves anywhere to disk?
>>
>> Missing certificates are fetched using HTTP[S]. Certificate responses
>> should be treated as any other HTTP[S] responses with regard to caching.
>> For example, if you have disk caching enabled and your caching rules
>> (including defaults) allow certificate response caching, then the
>> response should be cached. Similarly, the cached certificate will
>> eventually be evicted from the cache following regular cache maintenance
>> rules. When that happens, Squid will try to fetch the certificate again
>> (if it becomes needed again).
>>
>>
>>> 2. How this feature is related to sslproxy_foreign_intermediate_certs,
>>> how it can interfere with it?
>>
>> AFAICT by looking at the code, Squid only downloads certificates that
>> Squid is missing when trying to build a complete certificate chain for a
>> given server connection. Any sslproxy_foreign_intermediate_certs are
>> used as needed during the chain building process (i.e., they are _not_
>> "missing").
>
> I created bug report http://bugs.squid-cache.org/show_bug.cgi?id=4659
> a week ago but there has not been any activity.
> Is there someone who has sslproxy_foreign_intermediate_certs
> working in Squid 4.0.17 ?
Seems works as by as in 3.5.x. As I can see.
>
> Thanks,
> Marcus
>
> [snip]
>
>> HTH,
>>
>> Alex.
>>
>> _______________________________________________
>> squid-users mailing list
>> squid-users at lists.squid-cache.org
>> http://lists.squid-cache.org/listinfo/squid-users
>>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users

-------------- next part --------------
A non-text attachment was scrubbed...
Name: 0x613DEC46.asc
Type: application/pgp-keys
Size: 2437 bytes
Desc: not available
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20170124/ddb7090d/attachment.key>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: signature.asc
Type: application/pgp-signature
Size: 473 bytes
Desc: OpenPGP digital signature
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20170124/ddb7090d/attachment.sig>

From marcus.kool at urlfilterdb.com  Mon Jan 23 20:25:18 2017
From: marcus.kool at urlfilterdb.com (Marcus Kool)
Date: Mon, 23 Jan 2017 18:25:18 -0200
Subject: [squid-users] Squid 4.x: Intermediate certificates downloader
In-Reply-To: <2b5d58f2-c338-70ac-f2a5-0664de64d88e@gmail.com>
References: <4198a8d7-07c7-8c74-80ac-eb75ca61e62c@gmail.com>
 <35afc9c9-dd43-fb17-d0d6-c90866fd5c28@measurement-factory.com>
 <52b0aec8-f61d-1bec-de18-1c3be06494a0@urlfilterdb.com>
 <2b5d58f2-c338-70ac-f2a5-0664de64d88e@gmail.com>
Message-ID: <1eb1d7d3-add6-a489-f545-454645b3b478@urlfilterdb.com>



On 23/01/17 17:23, Yuri Voinov wrote:
[snip]

>> I created bug report http://bugs.squid-cache.org/show_bug.cgi?id=4659
>> a week ago but there has not been any activity.
>> Is there someone who has sslproxy_foreign_intermediate_certs
>> working in Squid 4.0.17 ?
> Seems works as by as in 3.5.x. As I can see.

3.5.x works fine but 4.0.17 fails on my servers.

>>
>> Thanks,
>> Marcus


From yvoinov at gmail.com  Mon Jan 23 20:27:17 2017
From: yvoinov at gmail.com (Yuri Voinov)
Date: Tue, 24 Jan 2017 02:27:17 +0600
Subject: [squid-users] Squid 4.x: Intermediate certificates downloader
In-Reply-To: <1eb1d7d3-add6-a489-f545-454645b3b478@urlfilterdb.com>
References: <4198a8d7-07c7-8c74-80ac-eb75ca61e62c@gmail.com>
 <35afc9c9-dd43-fb17-d0d6-c90866fd5c28@measurement-factory.com>
 <52b0aec8-f61d-1bec-de18-1c3be06494a0@urlfilterdb.com>
 <2b5d58f2-c338-70ac-f2a5-0664de64d88e@gmail.com>
 <1eb1d7d3-add6-a489-f545-454645b3b478@urlfilterdb.com>
Message-ID: <f48709f6-1b26-ecee-e018-ec60fc76d651@gmail.com>



24.01.2017 2:25, Marcus Kool ?????:
>
>
> On 23/01/17 17:23, Yuri Voinov wrote:
> [snip]
>
>>> I created bug report http://bugs.squid-cache.org/show_bug.cgi?id=4659
>>> a week ago but there has not been any activity.
>>> Is there someone who has sslproxy_foreign_intermediate_certs
>>> working in Squid 4.0.17 ?
>> Seems works as by as in 3.5.x. As I can see.
>
> 3.5.x works fine but 4.0.17 fails on my servers.
My test 4.0.17 works. Seems same like 3.5.
>
>>>
>>> Thanks,
>>> Marcus
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users

-------------- next part --------------
A non-text attachment was scrubbed...
Name: 0x613DEC46.asc
Type: application/pgp-keys
Size: 2437 bytes
Desc: not available
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20170124/eb78b5ae/attachment.key>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: signature.asc
Type: application/pgp-signature
Size: 473 bytes
Desc: OpenPGP digital signature
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20170124/eb78b5ae/attachment.sig>

From rousskov at measurement-factory.com  Mon Jan 23 20:36:48 2017
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Mon, 23 Jan 2017 13:36:48 -0700
Subject: [squid-users] Native FTP relay: connection closes (?) after
 'cannot assign requested address' error
In-Reply-To: <CACMUsGw1CxzsPBA6-Tz7zy-RqpnxFn_UqLugz19o9VXR-cLEqg@mail.gmail.com>
References: <CACMUsGz1rDaY4ffe7vRp-u-cOANuufPfVo+BgvNTmi3t8ywOMA@mail.gmail.com>
 <832f83c5-dc2b-574b-9ab0-80142862bec9@treenet.co.nz>
 <1485092987028-4681242.post@n4.nabble.com>
 <5a1e3e82-4f84-7246-0437-113dd99e957b@treenet.co.nz>
 <1485195091976-4681258.post@n4.nabble.com>
 <8d4fc039-bbd8-3a23-cae1-84ff43ff40e6@measurement-factory.com>
 <CACMUsGw1CxzsPBA6-Tz7zy-RqpnxFn_UqLugz19o9VXR-cLEqg@mail.gmail.com>
Message-ID: <c5c6ec70-983f-03f5-bb3f-a1db4ffddb17@measurement-factory.com>

On 01/23/2017 12:18 PM, Alexander wrote:
> 2017-01-23 21:41 GMT+03:00 Alex Rousskov: 
>     It is possible that Squid needs a knob to handle your use
>     case differently. However, I am pretty sure that somebody does want
>     Squid to do what it does know so we should not change Squid behavior to
>     satisfy your use case.

Clarification: ... should not satisfy your use case alone (i.e., at the
expense of the other known use case). It is perfectly fine to change
Squid to satisfy more than one legitimate use case, of course.


> I understand that, however the first and foremost reason I asked the
> question was that my use case pretends to be pretty typical :)

That may be true, but please keep in mind that:

a) The "whole message or nothing" principle is fairly fundamental to
Squid and correctly accommodating exceptions to that principle may be
difficult, on many levels.

b) Native FTP relaying is a very recent feature so any "lots of Squid
users need FTP relay to do X" argument can be paired with "the vast and
increasing majority of Squid users do need FTP relay at all, so FTP code
should not inconvenience the other code much" argument followed by the
same "whole message or nothing" principle discussed earlier.

Neither (a) nor (b) means that your use case should not be supported,
one way or the other. I am just cautioning against a rushed judgment to
change Squid without thinking of other users and long-term
effects/consequences.


Cheers,

Alex.



From mustafamohammad92 at gmail.com  Mon Jan 23 22:27:25 2017
From: mustafamohammad92 at gmail.com (Mustafa Mohammad)
Date: Mon, 23 Jan 2017 16:27:25 -0600
Subject: [squid-users] Enable SSL bump
Message-ID: <CAOUmaCSpK0_pE2HfA5k+J0YzhO0s2QOeDemY1CgZ5MawUwfBsw@mail.gmail.com>

I'm trying to enable ssl bump but it says that
FATAL: No valid signing SSL certificate configured for HTTP_port [::]:the
port I'm listening on. I did a lot of research and I couldn't find the
answer. Any help would be deeply appreciated.

Thanks,
Mustafa Mohammad
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20170123/0a20a0f7/attachment.htm>

From squid3 at treenet.co.nz  Mon Jan 23 22:39:16 2017
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Tue, 24 Jan 2017 11:39:16 +1300
Subject: [squid-users] Enable SSL bump
In-Reply-To: <CAOUmaCSpK0_pE2HfA5k+J0YzhO0s2QOeDemY1CgZ5MawUwfBsw@mail.gmail.com>
References: <CAOUmaCSpK0_pE2HfA5k+J0YzhO0s2QOeDemY1CgZ5MawUwfBsw@mail.gmail.com>
Message-ID: <7b7fa1c8-a209-5272-25ba-14853178061f@treenet.co.nz>

On 24/01/2017 11:27 a.m., Mustafa Mohammad wrote:
> I'm trying to enable ssl bump but it says that
> FATAL: No valid signing SSL certificate configured for HTTP_port [::]:the
> port I'm listening on. I did a lot of research and I couldn't find the
> answer. Any help would be deeply appreciated.
> 

SSL-Bump feature requires the TLS/SSL options which are normally only
mandatory on https_port.

Specifically the cert= option needs to be pointing Squid at a CA cert
with privileges to sign the auto-generated certs SSL-Bump creates.
 NP: a normal server cert such as one receives from the global root CAs
is not sufficient.


Also, please ensure you are using the latest versions of Squid with this
feature (today that is 3.5.23 or later, the 4.0 beta if possible).
SSL-Bump has gone through a lot of change and older implementations have
some quite nasty limitations and side effects.

Amos



From squid3 at treenet.co.nz  Mon Jan 23 22:41:05 2017
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Tue, 24 Jan 2017 11:41:05 +1300
Subject: [squid-users] Squid 3.x never_direct and DNS requests problem.
In-Reply-To: <69f299e4-c302-c693-9a37-1aabf96b1cc0@thalesgroup.com>
References: <69f299e4-c302-c693-9a37-1aabf96b1cc0@thalesgroup.com>
Message-ID: <88028f43-ae4b-966a-1297-7daf22a4a9b6@treenet.co.nz>

On 24/01/2017 3:58 a.m., FUSTE Emmanuel wrote:
> 
> All was carefully checked and nothing in my configuration (acl etc ...) 
> explain why Squid insist to do DNS requests for requests forwarded to 
> the peer(s).
> 
<snip>
> 
> #bug #4575
> url_rewrite_extras XXX
> store_id_extras XXX

I dont think that workaround is working.

> ------------------------------------
> 
> Since the switch from 3.5.12 to 3.5.19/23, I am able to use a simpler 
> work around (I switched directly from 3.5.12 to 3.5.19 so I don't know 
> when the behavior changed):
> Instead of installing a fake local DNS server and using
> dns_nameservers 127.0.0.1
> I could use
> dns_nameservers none
> Squid warn about non usable DNS and proceed normally. Before (tested 
> with 3.5.12 and lower) Squid hang.
> 

:-) nice.

I'm prety sure this is still bug 4575. I've added a comment there to
mention how the workaround is broken, and your improved one.

Amos



From squid3 at treenet.co.nz  Mon Jan 23 22:59:17 2017
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Tue, 24 Jan 2017 11:59:17 +1300
Subject: [squid-users] Squid 4.x: Intermediate certificates downloader
In-Reply-To: <5ca1a459-ac17-2b4a-3bb2-2041525b4df9@gmail.com>
References: <4198a8d7-07c7-8c74-80ac-eb75ca61e62c@gmail.com>
 <35afc9c9-dd43-fb17-d0d6-c90866fd5c28@measurement-factory.com>
 <b3e34d8e-ae0d-d59c-1f1e-07643934b81b@gmail.com>
 <03abbc4e-79a7-6388-782f-c94d3c1333c9@measurement-factory.com>
 <5ca1a459-ac17-2b4a-3bb2-2041525b4df9@gmail.com>
Message-ID: <72ea7116-3e24-7caf-ffcc-dc39c360c4aa@treenet.co.nz>

On 24/01/2017 8:22 a.m., Yuri Voinov wrote:
> 
> 
> 24.01.2017 0:06, Alex Rousskov ?????:
>> On 01/23/2017 10:41 AM, Yuri Voinov wrote:
>>> 23.01.2017 23:31, Alex Rousskov ?????:
>>>> On 01/23/2017 04:28 AM, Yuri wrote:
>>
>>>>> 2. How this feature is related to sslproxy_foreign_intermediate_certs,
>>>>> how it can interfere with it?
>>>> AFAICT by looking at the code, Squid only downloads certificates that
>>>> Squid is missing when trying to build a complete certificate chain for a
>>>> given server connection. Any sslproxy_foreign_intermediate_certs are
>>>> used as needed during the chain building process (i.e., they are _not_
>>>> "missing").
>>> Ok, so, this file uses for complete chains, and it contains statically
>>> added (manually) certs only, right?
>> Yes, the sslproxy_foreign_intermediate_certs file is maintained by the
>> Squid administrator. Squid does not update it.
>>
>>
>>> I.e., downloader should not save fetched intermediate CA's here,
>> Correct.
>>
>>
>>> which will be logically, isn't it?
>> I believe it is better to use the regular Squid cache for storing the
>> fetched missing certificates. I would not call abusing the
>> sslproxy_foreign_intermediate_certs file for this purpose completely
>> illogical, but such abuse would create more problems than it would solve
>> IMO. We have also considered using a dedicated storage for the fetched
>> missing certificates, but have decided (for many reasons) that it would
>> be worse than reusing the existing caching infrastructure.
>>
>> FWIW, IMO, storing the generated fake certificates in the regular Squid
>> cache would also be better than using an OpenSSL-administered database.
> Exactly.

There is one drawback to that suggestion though.

The certs which are downloaded are publicly available information and
intended to be such. Anyone can download them from source just like
browsers and Squid-4 do. So there is no harm in having the data stored
in a semi-insecure cache.

The cert generated by Squid are pollution as far as TLS is concerned.
Intended for use only by that proxy installation with the specific set
of details involved with the origin certificate on that connection.
Re-usability is purely a bonus. People could get into connectivity
trouble if they were stored long-term like other cache items.

Amos



From squid3 at treenet.co.nz  Mon Jan 23 23:08:10 2017
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Tue, 24 Jan 2017 12:08:10 +1300
Subject: [squid-users] Squid 4.x: Intermediate certificates downloader
In-Reply-To: <52b0aec8-f61d-1bec-de18-1c3be06494a0@urlfilterdb.com>
References: <4198a8d7-07c7-8c74-80ac-eb75ca61e62c@gmail.com>
 <35afc9c9-dd43-fb17-d0d6-c90866fd5c28@measurement-factory.com>
 <52b0aec8-f61d-1bec-de18-1c3be06494a0@urlfilterdb.com>
Message-ID: <81cf3b07-28a0-39b6-2690-a6cc320d653f@treenet.co.nz>

On 24/01/2017 7:06 a.m., Marcus Kool wrote:
> 
> 
> On 23/01/17 15:31, Alex Rousskov wrote:
>> On 01/23/2017 04:28 AM, Yuri wrote:
>>
>>> 1. How does it work?
>>
>> My response below and the following commit message might answer some of
>> your questions:
>>
>>     http://bazaar.launchpad.net/~squid/squid/5/revision/14769
> 
> This seems that the feature only goes to Squid 5.  Will it be ported to
> Squid 4 ?

rev.14769 is from before Squid-5 existed (rev.14932). The commits
labeled 'trunk' at that time were Squid-4.

Amos



From david at articatech.com  Mon Jan 23 23:28:45 2017
From: david at articatech.com (David Touzeau)
Date: Tue, 24 Jan 2017 00:28:45 +0100
Subject: [squid-users] [3.5.23]: mozilla.org failed using SSL
	transparent	SSL23_GET_SERVER_HELLO:unknown protocol
In-Reply-To: <006f01d274e0$2a10b500$7e321f00$@articatech.com>
References: <006f01d274e0$2a10b500$7e321f00$@articatech.com>
Message-ID: <036f01d275d0$712da5b0$5388f110$@articatech.com>

Same issue with https://www.digitalocean.com/
is somebody did not encounter the issue using Squid in transparent mode with SSL ??


-----Message d'origine-----
De : squid-users [mailto:squid-users-bounces at lists.squid-cache.org] De la part de David Touzeau
Envoy? : dimanche 22 janvier 2017 19:49
? : squid-users at lists.squid-cache.org
Objet : [squid-users] [3.5.23]: mozilla.org failed using SSL transparent SSL23_GET_SERVER_HELLO:unknown protocol


Hi

I'm using SSL transparent method :

https_port 0.0.0.0:53695  intercept disable-pmtu-discovery=transparent
name=MyPortNameID22 ssl-bump  generate-host-certificates=on dynamic_cert_mem_cache_size=4MB cert=/etc/squid3/ssl/cb623e9bfc65772f68b84393604cd6ea.dyn

sslproxy_foreign_intermediate_certs /etc/squid3/intermediate_ca.pem sslcrtd_program /lib/squid3/ssl_crtd -s /var/lib/squid/session/ssl/ssl_db -M 8MB sslcrtd_children 16 startup=5 idle=1

acl ssl_step1 at_step SslBump1
acl ssl_step2 at_step SslBump2
acl ssl_step3 at_step SslBump3
ssl_bump peek ssl_step1
ssl_bump splice all

sslproxy_flags DONT_VERIFY_PEER
sslproxy_cert_error allow all

As you can see squid just intercept ssl queries and bump nothing ( just to filter connections from url_rewrite program  and log ssl connections )

When connecting to mozilla.org using transparent, we receive this error:

* About to connect() to www.mozilla.org port 443 (#0)
*   Trying 104.16.41.2...
* connected
* Connected to www.mozilla.org (104.16.41.2) port 443 (#0)
* successfully set certificate verify locations:
*   CAfile: none
  CApath: /etc/ssl/certs
* SSLv3, TLS handshake, Client hello (1):
* error:140770FC:SSL routines:SSL23_GET_SERVER_HELLO:unknown protocol
* Closing connection #0
curl: (35) error:140770FC:SSL routines:SSL23_GET_SERVER_HELLO:unknown
protocol


And squid access.log

1485110919.564      3 192.168.1.236 TAG_NONE/403 6263 CONNECT
104.16.41.2:443 - HIER_NONE/- text/html

When using squid using standard port ( connected port/TUNNEL ) mozilla is correctly dispalyed without any error.


How to whitelist mozilla.org without create a bypass iptables rule  ?


Best regards




_______________________________________________
squid-users mailing list
squid-users at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-users



From squid3 at treenet.co.nz  Tue Jan 24 00:01:09 2017
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Tue, 24 Jan 2017 13:01:09 +1300
Subject: [squid-users] [3.5.23]: mozilla.org failed using SSL
 transparent SSL23_GET_SERVER_HELLO:unknown protocol
In-Reply-To: <036f01d275d0$712da5b0$5388f110$@articatech.com>
References: <006f01d274e0$2a10b500$7e321f00$@articatech.com>
 <036f01d275d0$712da5b0$5388f110$@articatech.com>
Message-ID: <ea091606-7634-ee94-6ff0-c3ee89a360e8@treenet.co.nz>

On 24/01/2017 12:28 p.m., David Touzeau wrote:
> Same issue with https://www.digitalocean.com/
> is somebody did not encounter the issue using Squid in transparent mode with SSL ??
> 

The TLS / HTTP Senvironment is in the process of stabilizing, but still
quite volatile.

Since the error message says "unknown protocol" I suspect it is
something like WebSockets, HTTP/2 or SPDY which you are actually
intercepting on port 443. Not HTTP/1 which Squid supports.

Or maybe it is some non-TLS traffic that OpenSSL does not support.

Mozilla do cert pinning, so teh bump/intercept should probably not work
anyway. I'm not sure about digitalocean.

Amos



From rousskov at measurement-factory.com  Tue Jan 24 00:45:10 2017
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Mon, 23 Jan 2017 17:45:10 -0700
Subject: [squid-users] Squid 4.x: Intermediate certificates downloader
In-Reply-To: <72ea7116-3e24-7caf-ffcc-dc39c360c4aa@treenet.co.nz>
References: <4198a8d7-07c7-8c74-80ac-eb75ca61e62c@gmail.com>
 <35afc9c9-dd43-fb17-d0d6-c90866fd5c28@measurement-factory.com>
 <b3e34d8e-ae0d-d59c-1f1e-07643934b81b@gmail.com>
 <03abbc4e-79a7-6388-782f-c94d3c1333c9@measurement-factory.com>
 <5ca1a459-ac17-2b4a-3bb2-2041525b4df9@gmail.com>
 <72ea7116-3e24-7caf-ffcc-dc39c360c4aa@treenet.co.nz>
Message-ID: <23db6c50-5959-72b2-0b41-951d4e542b51@measurement-factory.com>

On 01/23/2017 03:59 PM, Amos Jeffries wrote:
> On 24/01/2017 8:22 a.m., Yuri Voinov wrote:
>> 24.01.2017 0:06, Alex Rousskov ?????:
>>> FWIW, IMO, storing the generated fake certificates in the regular Squid
>>> cache would also be better than using an OpenSSL-administered database.

>> Exactly.

> There is one drawback to that suggestion though.

> The cert generated by Squid are pollution as far as TLS is concerned.

TLS has nothing to do with this decision though. Whether it is a good
idea to store something in a cache is determined primarily by two
factors: Reusability (i.e., the probability of a hit) and hit cost
(relative to a miss cost).


> Intended for use only by that proxy installation with the specific set
> of details involved with the origin certificate on that connection.
> Re-usability is purely a bonus. People could get into connectivity
> trouble if they were stored long-term like other cache items.

Sorry, I do not understand your argument. It seems to be revolving
around re-usability and "connectivity trouble". Both problems may be
about the same underlying concern, but I am addressing them separately:

* Both fake certificates and many other cached items are reusable, often
short- and sometimes long-term so there does not seem to be an important
difference as far as the probability of a hit is concerned. In fact,
fake certificates may be more reusable long-term than an average cached
HTTP response because the certificates they mimic and the mimicking
parameters are often stable for many days or even weeks.

* As for the "connectivity trouble", I assume that you are talking about
the cache user getting a stale (i.e., no longer applicable) fake
certificate from the cache without realizing it. I believe HTTP has
solutions for that problem already; any implementation using HTTP cache
for fake certificates would need to use the right HTTP knobs to ensure
freshness/applicability of the previously cached fake certificate to its
current user. Moreover, the existing fake certificate cache has to solve
exactly the same problem anyway, and does not have more appropriate
tools than an HTTP cache can offer.


AFAICT, the only real doubt regarding fake certificate caching is
whether the cost of generating the correct cache request (and/or
validating the hit response) is significantly lower than the cost of
generating a new fake certificate from scratch (i.e., the miss-vs-hit
cost factor). That problem exist for any cache, generic HTTP or
certificate-specific one. AFAIK, it needs studying/experiments.


Cheers,

Alex.



From eliezer at ngtech.co.il  Tue Jan 24 01:06:21 2017
From: eliezer at ngtech.co.il (Eliezer  Croitoru)
Date: Tue, 24 Jan 2017 03:06:21 +0200
Subject: [squid-users] BUG Notification. RE: Squid 3.5.23 is available -
	Article and new tools by	NgTech
Message-ID: <0fdf01d275de$1386b330$3a941990$@ngtech.co.il>

I didn't expected it but it happens to the best of us and the tools used the drbl-peer library that has a very huge memory leak that was found in a production environment(more then 10k queries per second).
I fixed the library and I will publish the new and updated binaries for the squid external acl helper and the ICAP service in the next 24-48 hours.

Thanks and Sorry,
Eliezer

----
Eliezer Croitoru
Linux System Administrator
Mobile: +972-5-28704261
Email: eliezer at ngtech.co.il


-----Original Message-----
From: squid-users [mailto:squid-users-bounces at lists.squid-cache.org] On Behalf Of Eliezer Croitoru
Sent: Friday, January 20, 2017 12:33 AM
To: squid-users at lists.squid-cache.org
Subject: [squid-users] Squid 3.5.23 is available - Article and new tools by NgTech

When is the kid considered stable?
Or
When is the software stable enough?
[http://www1.ngtech.co.il/wpe/wp-content/uploads/2017/01/dessert_sushi_by_outlawxvega-300x225.jpg]

Take a look at the page to get the full article: http://www1.ngtech.co.il/wpe/?p=374


Specially for this release I am releasing couple new tools based on the DRBL peers library I wrote.
On the plate:

* CA certificate test and installation html page[https://github.com/elico/ca-cert-test-page] (example page[http://moodle.ngtech.co.il/ca-test/])
* Windows Root CA installation script[https://github.com/elico/windows-rootca-autodeploy-create] (example page[http://ngtech.co.il/myca/])
* Debian and Ubuntu Stable  and Beta versions repository(without ecap support).. takes time to prepare ICAP DRBL query service[http://moodle.ngtech.co.il/drbl-icap-service/]
? Package of Binaries Sources and scripts[http://moodle.ngtech.co.il/drbl-icap-service/]
? Sources and startup scripts on github[https://github.com/elico/drbl-icap-service]
? I have hope to publish the tool in RPM and DEB format
* Squid 4.0.17 Basic functionality tests .. takes time to prepare

References:
* Squid-Cache CentOS repository details[http://wiki.squid-cache.org/KnowledgeBase/CentOS#Squid-3.5]

Eliezer Croitoru

* If you need some help with the new tools contact me.

----
Eliezer Croitoru
Linux System Administrator
Mobile: +972-5-28704261
Email: eliezer at ngtech.co.il


-----Original Message-----
From: squid-users [mailto:squid-users-bounces at lists.squid-cache.org] On Behalf Of Amos Jeffries
Sent: Saturday, December 17, 2016 6:05 PM
To: squid-announce at lists.squid-cache.org
Subject: [squid-users] [squid-announce] Squid 3.5.23 is available

The Squid HTTP Proxy team is very pleased to announce the availability of the Squid-3.5.23 release!

<SNIP>
Amos Jeffries

_______________________________________________
squid-users mailing list
squid-users at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-users



From david at articatech.com  Tue Jan 24 01:11:49 2017
From: david at articatech.com (David Touzeau)
Date: Tue, 24 Jan 2017 02:11:49 +0100
Subject: [squid-users] [3.5.23]: mozilla.org failed using SSL
	transparent SSL23_GET_SERVER_HELLO:unknown protocol
In-Reply-To: <ea091606-7634-ee94-6ff0-c3ee89a360e8@treenet.co.nz>
References: <006f01d274e0$2a10b500$7e321f00$@articatech.com>
 <036f01d275d0$712da5b0$5388f110$@articatech.com>
 <ea091606-7634-ee94-6ff0-c3ee89a360e8@treenet.co.nz>
Message-ID: <037401d275de$d6c9bae0$845d30a0$@articatech.com>

De : squid-users [mailto:squid-users-bounces at lists.squid-cache.org] De la
part de Amos Jeffries
Envoy? : mardi 24 janvier 2017 01:01
? : squid-users at lists.squid-cache.org
Objet : Re: [squid-users] [3.5.23]: mozilla.org failed using SSL transparent
SSL23_GET_SERVER_HELLO:unknown protocol

On 24/01/2017 12:28 p.m., David Touzeau wrote:
> Same issue with https://www.digitalocean.com/ is somebody did not
> encounter the issue using Squid in transparent mode with SSL ??
>

The TLS / HTTP Senvironment is in the process of stabilizing, but still
quite volatile.

Since the error message says "unknown protocol" I suspect it is something
like WebSockets, HTTP/2 or SPDY which you are actually intercepting on port
443. Not HTTP/1 which Squid supports.

Or maybe it is some non-TLS traffic that OpenSSL does not support.

Mozilla do cert pinning, so teh bump/intercept should probably not work
anyway. I'm not sure about digitalocean.
------------------------------------------------------------------------------------------------------------------------------------

Thanks Amos for the answer but...

I did not want to bump these sites, only pass trough the squid
port and process the request without try decrypting the protocol.

Tried :

acl nossl dstdomain -i .mozilla.org
ssl_bump none nossl
acl ssl_step1 at_step SslBump1
acl ssl_step2 at_step SslBump2
acl ssl_step3 at_step SslBump3
ssl_bump peek ssl_step1
ssl_bump splice all

or

acl nossl dst 104.16.40.2
ssl_bump none nossl
acl ssl_step1 at_step SslBump1
acl ssl_step2 at_step SslBump2
acl ssl_step3 at_step SslBump3
ssl_bump peek ssl_step1
ssl_bump splice all


But squid is still unable to process the request.

Any workaround ?




From squid3 at treenet.co.nz  Tue Jan 24 01:35:32 2017
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Tue, 24 Jan 2017 14:35:32 +1300
Subject: [squid-users] Enable SSL bump
In-Reply-To: <CAOUmaCRwTh2_CNTe-51OPE9AGvdbqp3_RKPGAn=oBwnmNBxH1A@mail.gmail.com>
References: <CAOUmaCSpK0_pE2HfA5k+J0YzhO0s2QOeDemY1CgZ5MawUwfBsw@mail.gmail.com>
 <7b7fa1c8-a209-5272-25ba-14853178061f@treenet.co.nz>
 <CAOUmaCRwTh2_CNTe-51OPE9AGvdbqp3_RKPGAn=oBwnmNBxH1A@mail.gmail.com>
Message-ID: <33df082f-60a3-55de-bc2e-0febb23dde90@treenet.co.nz>

[ Please reply to the list, not to me personally. ]

On 24/01/2017 11:54 a.m., Mustafa Mohammad wrote:
> I'm using 3.5.23 version. My problem is that I'm trying to hit our
> regression server and after doing research, I found that SSL bump might
> work for me but I'm not sure.

We (the squid-users list people) can probably answer that. But will need
to know a bit more details about what exactly your situation is.

I have been assuming that by "regression" you actually mean "legacy
server" - as in; 'a server running old software'. Is that correct?

If so, then the CRL check failing usually means that the CA who issued
that certificate has formally published an advisory (CRL) indicating
that certificate as invalid and must never be used again. Why can't you
just change the cert?


> When my config file is not doing a crl check,
> I was able to hit the server but I can't hit the server if my crlcheck is
> set to yes. I'm very new to squid.

Okay. Sounds like you just need to disable the some checks. But lets put
that aside until its clear whether Squid is the right solution for your
need.

Amos



From squid3 at treenet.co.nz  Tue Jan 24 01:54:28 2017
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Tue, 24 Jan 2017 14:54:28 +1300
Subject: [squid-users] cert mem cache
In-Reply-To: <BN6PR17MB114081C2458ADB8C2FEC446EF7730@BN6PR17MB1140.namprd17.prod.outlook.com>
References: <BN6PR17MB114081C2458ADB8C2FEC446EF7730@BN6PR17MB1140.namprd17.prod.outlook.com>
Message-ID: <42e1a724-a25e-19ba-b830-52ef483f2deb@treenet.co.nz>

On 23/01/2017 12:08 p.m., senor wrote:
> Hello all,
> Is the use of dynamic_cert_mem_cache_size=SIZE on the http_port
> directive any different with and without using sslcrtd_program?
> 

As far as I'm aware they are different. But Squid passes some of the
prot parameters to the helper, and with SMP there are shared blocks of
memory involved, so best to keep them the same.
 - At the very least that is the normal well-tested way of using them.

The helper uses an on-disk database/cache managed by OpenSSL as well as
the in-memory copies of popular things.
The Squid internal generator only uses in-memory AFAIK. But that may be
incorrect now, things in that area have changed a few times.

[ FYI, If Alex or Christos have differing info they know it best. ]


> Should there be a specific relationship between the amount of memory or
> disk configured for the two?

Disk - no. Memory - maybe.

> 
> On a slight tangent, what performance improvement could be expected by
> using ssl_crtd? What metrics would be best to view if comparing with and
> without?
> 

Without the helper the CPU timeslots assigned to Squid by the kernel
have to handle both traffic and cert generation tasks. This will
naturally be slower and add jitter to the traffic handling. However,
using a helper adds serialization overheads. So YMMV.


I'm not aware of anyone having done proper (or even rough) measurements.
Results will be traffic dependent though, since the certs are cached and
have HIT/MISS type behaviour just like any other cache data.

Amos



From squid3 at treenet.co.nz  Tue Jan 24 02:04:47 2017
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Tue, 24 Jan 2017 15:04:47 +1300
Subject: [squid-users] [3.5.23]: mozilla.org failed using SSL
 transparent SSL23_GET_SERVER_HELLO:unknown protocol
In-Reply-To: <037401d275de$d6c9bae0$845d30a0$@articatech.com>
References: <006f01d274e0$2a10b500$7e321f00$@articatech.com>
 <036f01d275d0$712da5b0$5388f110$@articatech.com>
 <ea091606-7634-ee94-6ff0-c3ee89a360e8@treenet.co.nz>
 <037401d275de$d6c9bae0$845d30a0$@articatech.com>
Message-ID: <e82c7ae4-f16e-63ec-6191-49c57133db8c@treenet.co.nz>

On 24/01/2017 2:11 p.m., David Touzeau wrote:
> De :  Amos Jeffries
> 
> On 24/01/2017 12:28 p.m., David Touzeau wrote:
>> Same issue with https://www.digitalocean.com/ is somebody did not
>> encounter the issue using Squid in transparent mode with SSL ??
>>
> 
> The TLS / HTTP Senvironment is in the process of stabilizing, but still
> quite volatile.
> 
> Since the error message says "unknown protocol" I suspect it is something
> like WebSockets, HTTP/2 or SPDY which you are actually intercepting on port
> 443. Not HTTP/1 which Squid supports.
> 
> Or maybe it is some non-TLS traffic that OpenSSL does not support.
> 
> Mozilla do cert pinning, so teh bump/intercept should probably not work
> anyway. I'm not sure about digitalocean.
> ------------------------------------------------------------------------------------------------------------------------------------
> 
> Thanks Amos for the answer but...
> 
> I did not want to bump these sites, only pass trough the squid
> port and process the request without try decrypting the protocol.

Aye. Sorry I noticed that just after pressing send.

Which means it is probably OpenSSL itself that does not know the
protocol. Could be TLS/1.3, from a big name like Mozilla port 443 is
unlikely to be non-TLS unless somebody upstream of you is doing
something funky for that specific traffic.


> 
> Tried :
> 
> acl nossl dstdomain -i .mozilla.org
> ssl_bump none nossl
> acl ssl_step1 at_step SslBump1
> acl ssl_step2 at_step SslBump2
> acl ssl_step3 at_step SslBump3
> ssl_bump peek ssl_step1
> ssl_bump splice all
> 
> or
> 
> acl nossl dst 104.16.40.2
> ssl_bump none nossl
> acl ssl_step1 at_step SslBump1
> acl ssl_step2 at_step SslBump2
> acl ssl_step3 at_step SslBump3
> ssl_bump peek ssl_step1
> ssl_bump splice all
> 
> 
> But squid is still unable to process the request.
> 
> Any workaround ?

Use 'splice' instead of 'none'. Mixing the SSL-Bump feature versions
like that is bad.

Also, what is showing up in your access.log for these transactions? The
ACL deciding not to even peek needs to correcly match the faked CONNECT
request containing only the dst-IP address.


I'm not aware of any other workaround for Squid-3. The
on_unsupported_protocol might be able to handle it in Squid-4.


Amos


From mustafamohammad92 at gmail.com  Tue Jan 24 02:38:11 2017
From: mustafamohammad92 at gmail.com (Mustafa Mohammad)
Date: Mon, 23 Jan 2017 20:38:11 -0600
Subject: [squid-users] Enable SSL bump
In-Reply-To: <33df082f-60a3-55de-bc2e-0febb23dde90@treenet.co.nz>
References: <CAOUmaCSpK0_pE2HfA5k+J0YzhO0s2QOeDemY1CgZ5MawUwfBsw@mail.gmail.com>
 <7b7fa1c8-a209-5272-25ba-14853178061f@treenet.co.nz>
 <CAOUmaCRwTh2_CNTe-51OPE9AGvdbqp3_RKPGAn=oBwnmNBxH1A@mail.gmail.com>
 <33df082f-60a3-55de-bc2e-0febb23dde90@treenet.co.nz>
Message-ID: <CAOUmaCQi0DjGBVZhZqZzpYs1YvSSU5aWDf+fnq6jvHgShDKdLA@mail.gmail.com>

By regression...I mean our QA testing server. Let me explain this in
detail: I have a squid proxy running which is needed to connect to the
server so we can get back if the transaction was approved or not. It is a
point of sale application that send transaction data to the server to
receive response about the transaction and that's when the problem is
occurring when It is trying to communicate to that server. I received some
help and I think ssl splice and ssl peek might work but I don't know how to
use them. I don't the rules to apply in this situation.

On Mon, Jan 23, 2017 at 7:35 PM, Amos Jeffries <squid3 at treenet.co.nz> wrote:

> [ Please reply to the list, not to me personally. ]
>
> On 24/01/2017 11:54 a.m., Mustafa Mohammad wrote:
> > I'm using 3.5.23 version. My problem is that I'm trying to hit our
> > regression server and after doing research, I found that SSL bump might
> > work for me but I'm not sure.
>
> We (the squid-users list people) can probably answer that. But will need
> to know a bit more details about what exactly your situation is.
>
> I have been assuming that by "regression" you actually mean "legacy
> server" - as in; 'a server running old software'. Is that correct?
>
> If so, then the CRL check failing usually means that the CA who issued
> that certificate has formally published an advisory (CRL) indicating
> that certificate as invalid and must never be used again. Why can't you
> just change the cert?
>
>
> > When my config file is not doing a crl check,
> > I was able to hit the server but I can't hit the server if my crlcheck is
> > set to yes. I'm very new to squid.
>
> Okay. Sounds like you just need to disable the some checks. But lets put
> that aside until its clear whether Squid is the right solution for your
> need.
>
> Amos
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20170123/4cc369b4/attachment.htm>

From rousskov at measurement-factory.com  Tue Jan 24 02:54:26 2017
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Mon, 23 Jan 2017 19:54:26 -0700
Subject: [squid-users] [3.5.23]: mozilla.org failed using SSL
 transparent SSL23_GET_SERVER_HELLO:unknown protocol
In-Reply-To: <036f01d275d0$712da5b0$5388f110$@articatech.com>
References: <006f01d274e0$2a10b500$7e321f00$@articatech.com>
 <036f01d275d0$712da5b0$5388f110$@articatech.com>
Message-ID: <32d2bc2b-e94e-5d8f-737c-eff69307e390@measurement-factory.com>

On 01/23/2017 04:28 PM, David Touzeau wrote:
> ssl_bump peek ssl_step1
> ssl_bump splice all
> 
> sslproxy_flags DONT_VERIFY_PEER
> sslproxy_cert_error allow all


> When connecting to mozilla.org using transparent, we receive this error:
> 
> * About to connect() to www.mozilla.org port 443 (#0)
> *   Trying 104.16.41.2...
> * connected
> * Connected to www.mozilla.org (104.16.41.2) port 443 (#0)
> * successfully set certificate verify locations:
> *   CAfile: none
>   CApath: /etc/ssl/certs
> * SSLv3, TLS handshake, Client hello (1):
> * error:140770FC:SSL routines:SSL23_GET_SERVER_HELLO:unknown protocol
> * Closing connection #0
> curl: (35) error:140770FC:SSL routines:SSL23_GET_SERVER_HELLO:unknown
> protocol
> 
> 
> And squid access.log
> 
> 1485110919.564      3 192.168.1.236 TAG_NONE/403 6263 CONNECT
> 104.16.41.2:443 - HIER_NONE/- text/html

Amos, please note that the above failing test is done using curl, not
some fancy/non-HTTP/websocket traffic from a "browser".

David, you need to figure out why Squid is denying the intercepted
connection attempt (the /403 part in your access.log). Check your
http_access rules to start with. They were applied to the denied fake
CONNECT request shown above.

AFAICT, Squid denies the [fake] CONNECT without bumping the client
connection to serve a secure error message. That is _not_ what I would
expect because usually Squid bumps to serve errors, even when dealing
with non-bumping ssl_bump rules. However, I may be misinterpreting the
"unknown protocol" part; perhaps OpenSSL can use that phrase for an
unsupported TLS version as well? Or perhaps Squid failed to bump the
client for some reason?

Capture packets to see what Squid is sending to curl.


HTH,

Alex.



From jlay at slave-tothe-box.net  Tue Jan 24 03:15:43 2017
From: jlay at slave-tothe-box.net (James Lay)
Date: Mon, 23 Jan 2017 20:15:43 -0700
Subject: [squid-users] [3.5.23]: mozilla.org failed using SSL
 transparent SSL23_GET_SERVER_HELLO:unknown protocol
In-Reply-To: <32d2bc2b-e94e-5d8f-737c-eff69307e390@measurement-factory.com>
References: <006f01d274e0$2a10b500$7e321f00$@articatech.com>
 <036f01d275d0$712da5b0$5388f110$@articatech.com>
 <32d2bc2b-e94e-5d8f-737c-eff69307e390@measurement-factory.com>
Message-ID: <1485227743.3362.1.camel@slave-tothe-box.net>

On Mon, 2017-01-23 at 19:54 -0700, Alex Rousskov wrote:
> On 01/23/2017 04:28 PM, David Touzeau wrote:
> > 
> > ssl_bump peek ssl_step1
> > ssl_bump splice all
> > 
> > sslproxy_flags DONT_VERIFY_PEER
> > sslproxy_cert_error allow all
> 
> > 
> > When connecting to mozilla.org using transparent, we receive this
> > error:
> > 
> > * About to connect() to www.mozilla.org port 443 (#0)
> > *???Trying 104.16.41.2...
> > * connected
> > * Connected to www.mozilla.org (104.16.41.2) port 443 (#0)
> > * successfully set certificate verify locations:
> > *???CAfile: none
> > ? CApath: /etc/ssl/certs
> > * SSLv3, TLS handshake, Client hello (1):
> > * error:140770FC:SSL routines:SSL23_GET_SERVER_HELLO:unknown
> > protocol
> > * Closing connection #0
> > curl: (35) error:140770FC:SSL
> > routines:SSL23_GET_SERVER_HELLO:unknown
> > protocol
> > 
> > 
> > And squid access.log
> > 
> > 1485110919.564??????3 192.168.1.236 TAG_NONE/403 6263 CONNECT
> > 104.16.41.2:443 - HIER_NONE/- text/html
> Amos, please note that the above failing test is done using curl, not
> some fancy/non-HTTP/websocket traffic from a "browser".
> 
> David, you need to figure out why Squid is denying the intercepted
> connection attempt (the /403 part in your access.log). Check your
> http_access rules to start with. They were applied to the denied fake
> CONNECT request shown above.
> 
> AFAICT, Squid denies the [fake] CONNECT without bumping the client
> connection to serve a secure error message. That is _not_ what I
> would
> expect because usually Squid bumps to serve errors, even when dealing
> with non-bumping ssl_bump rules. However, I may be misinterpreting
> the
> "unknown protocol" part; perhaps OpenSSL can use that phrase for an
> unsupported TLS version as well? Or perhaps Squid failed to bump the
> client for some reason?
> 
> Capture packets to see what Squid is sending to curl.
> 
> 
> HTH,
> 
> Alex.
> 
> 
Seems like pretty standard stuff:
Jan 23 20:09:04 (squid): 192.168.1.109 - - [23/Jan/2017:20:09:04 -0700]
"CONNECT 104.16.40.2:443 HTTP/1.1" www.mozilla.org - 200 916167
TCP_TUNNEL:ORIGINAL_DST
TLSv12	TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256	secp256r1
James
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20170123/2da966d1/attachment.htm>

From squid3 at treenet.co.nz  Tue Jan 24 06:35:29 2017
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Tue, 24 Jan 2017 19:35:29 +1300
Subject: [squid-users] [3.5.23]: mozilla.org failed using SSL
 transparent SSL23_GET_SERVER_HELLO:unknown protocol
In-Reply-To: <1485227743.3362.1.camel@slave-tothe-box.net>
References: <006f01d274e0$2a10b500$7e321f00$@articatech.com>
 <036f01d275d0$712da5b0$5388f110$@articatech.com>
 <32d2bc2b-e94e-5d8f-737c-eff69307e390@measurement-factory.com>
 <1485227743.3362.1.camel@slave-tothe-box.net>
Message-ID: <bc98a2fe-77f4-2099-c8a1-8e1ada28309e@treenet.co.nz>

> On Mon, 2017-01-23 at 19:54 -0700, Alex Rousskov wrote:
>> On 01/23/2017 04:28 PM, David Touzeau wrote:
>>>
>>> ssl_bump peek ssl_step1
>>> ssl_bump splice all
>>>
>>> sslproxy_flags DONT_VERIFY_PEER
>>> sslproxy_cert_error allow all
>>
>>>
>>> When connecting to mozilla.org using transparent, we receive this
>>> error:
>>>
>>> * About to connect() to www.mozilla.org port 443 (#0)
>>> *   Trying 104.16.41.2...
>>> * connected
>>> * Connected to www.mozilla.org (104.16.41.2) port 443 (#0)
>>> * successfully set certificate verify locations:
>>> *   CAfile: none
>>>   CApath: /etc/ssl/certs
>>> * SSLv3, TLS handshake, Client hello (1):
>>> * error:140770FC:SSL routines:SSL23_GET_SERVER_HELLO:unknown
>>> protocol
>>> * Closing connection #0
>>> curl: (35) error:140770FC:SSL
>>> routines:SSL23_GET_SERVER_HELLO:unknown
>>> protocol
>>>
>>>
>>> And squid access.log
>>>
>>> 1485110919.564      3 192.168.1.236 TAG_NONE/403 6263 CONNECT
>>> 104.16.41.2:443 - HIER_NONE/- text/html
>> Amos, please note that the above failing test is done using curl, not
>> some fancy/non-HTTP/websocket traffic from a "browser".

Doh! Thats why its weird. The error is coming from curls OpenSSL
library, not Squid's one.


>>
>> David, you need to figure out why Squid is denying the intercepted
>> connection attempt (the /403 part in your access.log). Check your
>> http_access rules to start with. They were applied to the denied fake
>> CONNECT request shown above.
>>
>> AFAICT, Squid denies the [fake] CONNECT without bumping the client
>> connection to serve a secure error message. That is _not_ what I
>> would
>> expect because usually Squid bumps to serve errors, even when dealing
>> with non-bumping ssl_bump rules. However, I may be misinterpreting
>> the
>> "unknown protocol" part; perhaps OpenSSL can use that phrase for an
>> unsupported TLS version as well? Or perhaps Squid failed to bump the
>> client for some reason?
>>
>> Capture packets to see what Squid is sending to curl.
>>


On 24/01/2017 4:15 p.m., James Lay wrote:
> Seems like pretty standard stuff:
> Jan 23 20:09:04 (squid): 192.168.1.109 - - [23/Jan/2017:20:09:04 -0700]
> "CONNECT 104.16.40.2:443 HTTP/1.1" www.mozilla.org - 200 916167
> TCP_TUNNEL:ORIGINAL_DST
> TLSv12	TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256	secp256r1
> James

This is a different log trace from David's.

Here Squid is setting up a TUNNEL to the clients original dst-IP,
successfully. Any TLS funky stuff going on for this transaction is done
directly between server and client. Squid's only involvement is to peek
at the Hello messages and record them for its log.

But some of those details (ie the agreed cipher) come from the
ServerHello on successful TLS setup. So I think no errors happened in
that log entries transaction.

Amos



From squid3 at treenet.co.nz  Tue Jan 24 06:56:50 2017
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Tue, 24 Jan 2017 19:56:50 +1300
Subject: [squid-users] Enable SSL bump
In-Reply-To: <CAOUmaCQi0DjGBVZhZqZzpYs1YvSSU5aWDf+fnq6jvHgShDKdLA@mail.gmail.com>
References: <CAOUmaCSpK0_pE2HfA5k+J0YzhO0s2QOeDemY1CgZ5MawUwfBsw@mail.gmail.com>
 <7b7fa1c8-a209-5272-25ba-14853178061f@treenet.co.nz>
 <CAOUmaCRwTh2_CNTe-51OPE9AGvdbqp3_RKPGAn=oBwnmNBxH1A@mail.gmail.com>
 <33df082f-60a3-55de-bc2e-0febb23dde90@treenet.co.nz>
 <CAOUmaCQi0DjGBVZhZqZzpYs1YvSSU5aWDf+fnq6jvHgShDKdLA@mail.gmail.com>
Message-ID: <6fb3dc49-1d06-07ed-f03e-044de83d2991@treenet.co.nz>

On 24/01/2017 3:38 p.m., Mustafa Mohammad wrote:
> By regression...I mean our QA testing server. Let me explain this in
> detail: I have a squid proxy running which is needed to connect to the
> server so we can get back if the transaction was approved or not. It is a
> point of sale application that send transaction data to the server to
> receive response about the transaction and that's when the problem is
> occurring when It is trying to communicate to that server. I received some
> help and I think ssl splice and ssl peek might work but I don't know how to
> use them. I don't the rules to apply in this situation.

Whats usually needed in these setups is a reverse-proxy (aka "load
balancer", CDN frontend, etc.). But for that to be Squid it would
require the POS application to be messaging with HTTP.
 Is that the case?

The peek-and-splice form of SSL-Bump MITM might work anyway so long as
the application is actually using real TLS. But you need to be aware the
splice action is just blindly tunneling the TLS data through Squid. It
is not being touched, so anything like CRL issues is a problem between
the endpoints - Squid cannot help unless its actually HTTP messages,
then 'bump' action is needed to fully decrypt and modify the TLS.


(That said, there have been some weird issues showing up even when the
tunnel is spliced. see the threads about 30sec delays to cloudeflare, or
curl rejecting tunneled traffic.)

Amos



From rentorbuy at yahoo.com  Tue Jan 24 08:02:05 2017
From: rentorbuy at yahoo.com (Vieri)
Date: Tue, 24 Jan 2017 08:02:05 +0000 (UTC)
Subject: [squid-users] squid reverse proxy (accelerator) for MS Exchange
 OWA
In-Reply-To: <5517395f-eaa4-0b3a-34e9-66ea8d0122a1@treenet.co.nz>
References: <1216522442.160455.1484870639800.ref@mail.yahoo.com>
 <1216522442.160455.1484870639800@mail.yahoo.com>
 <cdea0afa-c2a2-23c4-dbac-553a565e83f7@treenet.co.nz>
 <690032068.431187.1484905491833@mail.yahoo.com>
 <5517395f-eaa4-0b3a-34e9-66ea8d0122a1@treenet.co.nz>
Message-ID: <1374872097.4095106.1485244925609@mail.yahoo.com>





----- Original Message -----
From: Amos Jeffries <squid3 at treenet.co.nz>
>
> You could try with a newer Squid version since the bio.cc code might be
> making something else happen in 3.5.23. If that still fails the 4.0 beta

> has different logic and far better debug info in this area.

Hi again,

I'm still struggling with my reverse proxy setup. As stated in my previous post, after upgrading to squid 3.5.23, I successfully connected with a web browser FROM an external location (internet) to the Squid proxy which in turn connected via https on port 443 to an internal MS Exchange OWA server.
Apparently, all I was missing was sslcafile in cache_peer.

However, I'm now trying (but failing) to do the same thing locally. I'm connecting from a web browser in the local network to the Squid reverse proxy and that, in turn, is trying to connect to the OWA server on the same LAN.
I set up the client's hosts file to point webmail2.mydomain.org to Squid's IP address.

Here's squid.conf:

https_port 0.0.0.0:443 accel cert=/etc/ssl/squid/accel_cert.cer key=/etc/ssl/squid/accel_key.pem defaultsite=webmail2.mydomain.org

cache_peer 10.215.144.21 parent 443 0 no-query originserver login=PASS ssl sslcert=/etc/ssl/squid/client_cert.cer sslkey=/etc/ssl/squid/client_key.pem sslcafile=/etc/ssl/CA/cacert.pem front-end-https=on name=owaServer

acl OWA dstdomain webmail2.mydomain.org
cache_peer_access owaServer allow OWA
never_direct allow OWA

http_access allow OWA
http_access deny all

Since it didn't work with Squid 3, I updated to the latest Squid 4 version.

I get this in the log when trying to connect: 

2017/01/24 07:58:57.075 kid1| 83,5| bio.cc(116) write: FD 18 wrote 312 <= 312
2017/01/24 07:58:57.075 kid1| 83,5| bio.cc(139) read: FD 18 read -1 <= 65535
2017/01/24 07:58:57.076 kid1| 83,5| bio.cc(144) read: error: 11 ignored: 1
2017/01/24 07:58:57.076 kid1| 5,3| comm.cc(559) commSetConnTimeout: local=10.215.144.92:56236 remote=10.215.144.21:443 FD 18 flags=1 timeout 30
2017/01/24 07:58:57.076 kid1| 5,5| ModEpoll.cc(117) SetSelect: FD 18, type=1, handler=1, client_data=0xb7cf8be8, timeout=0
2017/01/24 07:58:57.076 kid1| 93,5| AsyncJob.cc(154) callEnd: Security::BlindPeerConnector status out: [ FD 18 job60]
2017/01/24 07:58:57.076 kid1| 93,5| AsyncCallQueue.cc(57) fireNext: leaving AsyncJob::start()
2017/01/24 07:58:57.076 kid1| 83,5| bio.cc(139) read: FD 18 read 0 <= 65535
2017/01/24 07:58:57.076 kid1| 83,5| NegotiationHistory.cc(83) retrieveNegotiatedInfo: SSL connection info on FD 18 SSL version NONE/0.0 negotiated cipher
2017/01/24 07:58:57.076 kid1| Error negotiating SSL on FD 18: error:00000000:lib(0):func(0):reason(0) (5/0/0)
2017/01/24 07:58:57.076 kid1| TCP connection to 10.215.144.21/443 failed
2017/01/24 07:58:57.077 kid1| 15,2| neighbors.cc(1246) peerConnectFailedSilent: TCP connection to 10.215.144.21/443 dead

# squid -v
Squid Cache: Version 4.0.17-20170122-r14968
Service Name: squid
configure options:  '--prefix=/usr' '--build=i686-pc-linux-gnu' '--host=i686-pc-linux-gnu' '--mandir=/usr/share/man' '--infodir=/usr/share/info' '--datadir=/usr/share' '--sysconfdir=/etc' '--localstatedir=/var/lib' '--disable-dependency-tracking' '--disable-silent-rules' '--docdir=/usr/share/doc/squid-4.0.17_beta_p2017012214968' '--htmldir=/usr/share/doc/squid-4.0.17_beta_p2017012214968/html' '--libdir=/usr/lib' '--sysconfdir=/etc/squid' '--libexecdir=/usr/libexec/squid' '--localstatedir=/var' '--with-pidfile=/run/squid.pid' '--datadir=/usr/share/squid' '--with-logdir=/var/log/squid' '--with-default-user=squid' '--enable-removal-policies=lru,heap' '--enable-storeio=aufs,diskd,rock,ufs' '--enable-disk-io' '--enable-auth-basic=NCSA,POP3,getpwnam,SMB,LDAP,PAM,RADIUS' '--enable-auth-digest=file,LDAP,eDirectory' '--enable-auth-ntlm=SMB_LM' '--enable-auth-negotiate=kerberos,wrapper' '--enable-external-acl-helpers=file_userip,session,unix_group,wbinfo_group,LDAP_group,eDirectory_userip,kerberos_ldap_group' '--enable-log-daemon-helpers' '--enable-url-rewrite-helpers' '--enable-cache-digests' '--enable-delay-pools' '--enable-eui' '--enable-icmp' '--enable-follow-x-forwarded-for' '--with-large-files' '--disable-strict-error-checking' '--disable-arch-native' '--with-ltdl-includedir=/usr/include' '--with-ltdl-libdir=/usr/lib' '--with-libcap' '--enable-ipv6' '--disable-snmp' '--with-openssl' '--with-nettle' '--with-gnutls' '--disable-ssl-crtd' '--disable-ecap' '--disable-esi' '--enable-htcp' '--enable-wccp' '--enable-wccpv2' '--enable-linux-netfilter' '--with-mit-krb5' '--without-heimdal-krb5' 'build_alias=i686-pc-linux-gnu' 'host_alias=i686-pc-linux-gnu' 'CC=i686-pc-linux-gnu-gcc' 'CFLAGS=-O2 -march=i686 -pipe' 'LDFLAGS=-Wl,-O1 -Wl,--as-needed' 'CXXFLAGS=-O2 -march=i686 -pipe' 'PKG_CONFIG_PATH=/usr/lib/pkgconfig'

# openssl version
OpenSSL 1.0.2j  26 Sep 2016

Unfortunately, Squid's or OpenSSL's log message isn't too informative, even in Squid 4.
Also, I'm not sure why the SSL version isn't picked up (NONE/0.0) but I don't think it changes anything.

What else can I try?

Thanks,

Vieri


From fredbmail at free.fr  Tue Jan 24 08:26:38 2017
From: fredbmail at free.fr (FredB)
Date: Tue, 24 Jan 2017 09:26:38 +0100 (CET)
Subject: [squid-users] Squid 3.5.23 little fixes
In-Reply-To: <749076102.42388451.1485246167428.JavaMail.root@zimbra4-e1>
Message-ID: <132593023.42397520.1485246398235.JavaMail.root@zimbra4-e1>

Hello,

FI, I'm reading some parts of code and I found two little spelling errors

FredB

---------------------------------------------------------------

--- src/client_side.cc	2016-10-09 21:58:01.000000000 +0200
+++ src/client_side.cc	2016-12-14 10:57:12.915469723 +0100
@@ -2736,10 +2736,10 @@ clientProcessRequest(ConnStateData *conn
 
     request->flags.internal = http->flags.internal;
     setLogUri (http, urlCanonicalClean(request.getRaw()));
-    request->client_addr = conn->clientConnection->remote; // XXX: remove reuest->client_addr member.
+    request->client_addr = conn->clientConnection->remote; // XXX: remove request->client_addr member.
 #if FOLLOW_X_FORWARDED_FOR
     // indirect client gets stored here because it is an HTTP header result (from X-Forwarded-For:)
-    // not a details about teh TCP connection itself
+    // not a details about the TCP connection itself
     request->indirect_client_addr = conn->clientConnection->remote;
 #endif /* FOLLOW_X_FORWARDED_FOR */
     request->my_addr = conn->clientConnection->local;


From yvoinov at gmail.com  Tue Jan 24 08:52:39 2017
From: yvoinov at gmail.com (Yuri Voinov)
Date: Tue, 24 Jan 2017 14:52:39 +0600
Subject: [squid-users] Squid 3.5.23 little fixes
In-Reply-To: <132593023.42397520.1485246398235.JavaMail.root@zimbra4-e1>
References: <132593023.42397520.1485246398235.JavaMail.root@zimbra4-e1>
Message-ID: <ce77c79b-a1d4-5398-f5f5-663c4edbabd8@gmail.com>

teh TCP :-D teh drama :-D

Nice shoot :-D


24.01.2017 14:26, FredB ?????:
> Hello,
>
> FI, I'm reading some parts of code and I found two little spelling errors
>
> FredB
>
> ---------------------------------------------------------------
>
> --- src/client_side.cc	2016-10-09 21:58:01.000000000 +0200
> +++ src/client_side.cc	2016-12-14 10:57:12.915469723 +0100
> @@ -2736,10 +2736,10 @@ clientProcessRequest(ConnStateData *conn
>  
>      request->flags.internal = http->flags.internal;
>      setLogUri (http, urlCanonicalClean(request.getRaw()));
> -    request->client_addr = conn->clientConnection->remote; // XXX: remove reuest->client_addr member.
> +    request->client_addr = conn->clientConnection->remote; // XXX: remove request->client_addr member.
>  #if FOLLOW_X_FORWARDED_FOR
>      // indirect client gets stored here because it is an HTTP header result (from X-Forwarded-For:)
> -    // not a details about teh TCP connection itself
> +    // not a details about the TCP connection itself
>      request->indirect_client_addr = conn->clientConnection->remote;
>  #endif /* FOLLOW_X_FORWARDED_FOR */
>      request->my_addr = conn->clientConnection->local;
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users

-------------- next part --------------
A non-text attachment was scrubbed...
Name: 0x613DEC46.asc
Type: application/pgp-keys
Size: 2437 bytes
Desc: not available
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20170124/601b71e0/attachment.key>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: signature.asc
Type: application/pgp-signature
Size: 473 bytes
Desc: OpenPGP digital signature
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20170124/601b71e0/attachment.sig>

From squid3 at treenet.co.nz  Tue Jan 24 09:11:36 2017
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Tue, 24 Jan 2017 22:11:36 +1300
Subject: [squid-users] Squid 3.5.23 little fixes
In-Reply-To: <132593023.42397520.1485246398235.JavaMail.root@zimbra4-e1>
References: <132593023.42397520.1485246398235.JavaMail.root@zimbra4-e1>
Message-ID: <b9502513-4920-ffe2-f471-09f7cdb8f0b8@treenet.co.nz>

On 24/01/2017 9:26 p.m., FredB wrote:
> Hello,
> 
> FI, I'm reading some parts of code and I found two little spelling errors
> 

Thanks. Applied.

Amos



From emmanuel.fuste at thalesgroup.com  Tue Jan 24 09:55:21 2017
From: emmanuel.fuste at thalesgroup.com (FUSTE Emmanuel)
Date: Tue, 24 Jan 2017 10:55:21 +0100
Subject: [squid-users] Squid 3.x never_direct and DNS requests problem.
In-Reply-To: <88028f43-ae4b-966a-1297-7daf22a4a9b6@treenet.co.nz>
References: <69f299e4-c302-c693-9a37-1aabf96b1cc0@thalesgroup.com>
 <88028f43-ae4b-966a-1297-7daf22a4a9b6@treenet.co.nz>
Message-ID: <ab0784ec-0318-f293-382b-37972b45141d@thalesgroup.com>

Le 23/01/2017 ? 23:41, Amos Jeffries a ?crit :
> On 24/01/2017 3:58 a.m., FUSTE Emmanuel wrote:
>> All was carefully checked and nothing in my configuration (acl etc ...)
>> explain why Squid insist to do DNS requests for requests forwarded to
>> the peer(s).
>>
> <snip>
>> #bug #4575
>> url_rewrite_extras XXX
>> store_id_extras XXX
> I dont think that workaround is working.
>
>> ------------------------------------
>>
>> Since the switch from 3.5.12 to 3.5.19/23, I am able to use a simpler
>> work around (I switched directly from 3.5.12 to 3.5.19 so I don't know
>> when the behavior changed):
>> Instead of installing a fake local DNS server and using
>> dns_nameservers 127.0.0.1
>> I could use
>> dns_nameservers none
>> Squid warn about non usable DNS and proceed normally. Before (tested
>> with 3.5.12 and lower) Squid hang.
>>
> :-) nice.
>
> I'm prety sure this is still bug 4575. I've added a comment there to
> mention how the workaround is broken, and your improved one.
>
Thank you !
If there's anything I can help with to solve this bug, I'd be happy to.

Emmanuel.

From david at articatech.com  Tue Jan 24 10:42:03 2017
From: david at articatech.com (David Touzeau)
Date: Tue, 24 Jan 2017 11:42:03 +0100
Subject: [squid-users] [3.5.23]: mozilla.org failed using SSL
	transparent SSL23_GET_SERVER_HELLO:unknown protocol
In-Reply-To: <bc98a2fe-77f4-2099-c8a1-8e1ada28309e@treenet.co.nz>
References: <006f01d274e0$2a10b500$7e321f00$@articatech.com>
 <036f01d275d0$712da5b0$5388f110$@articatech.com>
 <32d2bc2b-e94e-5d8f-737c-eff69307e390@measurement-factory.com>
 <1485227743.3362.1.camel@slave-tothe-box.net>
 <bc98a2fe-77f4-2099-c8a1-8e1ada28309e@treenet.co.nz>
Message-ID: <03d901d2762e$80028e30$8007aa90$@articatech.com>


This is a different log trace from David's.

Here Squid is setting up a TUNNEL to the clients original dst-IP,
successfully. Any TLS funky stuff going on for this transaction is done
directly between server and client. Squid's only involvement is to peek at
the Hello messages and record them for its log.

But some of those details (ie the agreed cipher) come from the ServerHello
on successful TLS setup. So I think no errors happened in that log entries
transaction.

Amos

______________________________________________________________________________________________


Hi tried with

acl nossl dst 104.16.41.2
acl nossl2 dstdomain -i .mozilla.org
ssl_bump splice nossl
ssl_bump splice nossl2
acl ssl_step1 at_step SslBump1
acl ssl_step2 at_step SslBump2
acl ssl_step3 at_step SslBump3
ssl_bump peek ssl_step1
ssl_bump splice all
sslproxy_flags DONT_VERIFY_PEER
sslproxy_cert_error allow all

1485252508.663      2 192.168.1.236 TAG_NONE/403 6263 CONNECT 
104.16.41.2:443 - HIER_NONE/- text/html
1485252509.385      2 192.168.1.236 TAG_NONE/403 6263 CONNECT 
104.16.41.2:443 - HIER_NONE/- text/html

Using squid port 3128 without any bump allow accessing to mozilla 

So if there are any acl it will be blocked on both.

Return back to list with a full debug mode..




From cf at utc.fr  Tue Jan 24 11:18:00 2017
From: cf at utc.fr (Christophe Fillot)
Date: Tue, 24 Jan 2017 12:18:00 +0100
Subject: [squid-users] Strange delays (30 seconds) with TLS connections
 in WCCP/Transparent mode
In-Reply-To: <f3591d3f-0c99-8a89-0455-7fc1bcecafbb@utc.fr>
References: <f3591d3f-0c99-8a89-0455-7fc1bcecafbb@utc.fr>
Message-ID: <8364002a-3f3f-6fd1-05d9-b0fabbb5e98a@utc.fr>

Sorry for the noise, I was able to find the cause: we use "dstdomain" 
ACLs and Squid does reverse lookups.

It seems that Cloudflare DNS servers do not respond to PTR requests, and 
since Squid has the default "dns_timeout" value to 30 seconds...:

$ host www.wireshark.org
www.wireshark.org has address 104.25.219.21
www.wireshark.org has address 104.25.218.21
www.wireshark.org has IPv6 address 2400:cb00:2048:1::6819:da15
www.wireshark.org has IPv6 address 2400:cb00:2048:1::6819:db15

$ host 104.25.219.21
Host 21.219.25.104.in-addr.arpa not found: 2(SERVFAIL)

$ dig @arin.authdns.ripe.net. in ns 21.219.25.104.in-addr.arpa.
[...]
;; AUTHORITY SECTION:
25.104.in-addr.arpa.    86400    IN    NS ns1.cloudflare.com.
25.104.in-addr.arpa.    86400    IN    NS ns2.cloudflare.com.

$ dig @ns1.cloudflare.com. in ptr 21.219.25.104.in-addr.arpa.
[...]
;; connection timed out; no servers could be reached

Best regards,

Christophe



From david at articatech.com  Tue Jan 24 11:50:00 2017
From: david at articatech.com (David Touzeau)
Date: Tue, 24 Jan 2017 12:50:00 +0100
Subject: [squid-users] [3.5.23]: mozilla.org failed using
	SSL	transparent SSL23_GET_SERVER_HELLO:unknown protocol
In-Reply-To: <03d901d2762e$80028e30$8007aa90$@articatech.com>
References: <006f01d274e0$2a10b500$7e321f00$@articatech.com>
 <036f01d275d0$712da5b0$5388f110$@articatech.com>
 <32d2bc2b-e94e-5d8f-737c-eff69307e390@measurement-factory.com>
 <1485227743.3362.1.camel@slave-tothe-box.net>
 <bc98a2fe-77f4-2099-c8a1-8e1ada28309e@treenet.co.nz>
 <03d901d2762e$80028e30$8007aa90$@articatech.com>
Message-ID: <042201d27637$fe1c1da0$fa5458e0$@articatech.com>



-----Message d'origine-----
De : squid-users [mailto:squid-users-bounces at lists.squid-cache.org] De la part de David Touzeau
Envoy? : mardi 24 janvier 2017 11:42
? : squid-users at lists.squid-cache.org
Objet : Re: [squid-users] [3.5.23]: mozilla.org failed using SSL transparent SSL23_GET_SERVER_HELLO:unknown protocol


This is a different log trace from David's.

Here Squid is setting up a TUNNEL to the clients original dst-IP, successfully. Any TLS funky stuff going on for this transaction is done directly between server and client. Squid's only involvement is to peek at the Hello messages and record them for its log.

But some of those details (ie the agreed cipher) come from the ServerHello on successful TLS setup. So I think no errors happened in that log entries transaction.

Amos

______________________________________________________________________________________________


Hi tried with

acl nossl dst 104.16.41.2
acl nossl2 dstdomain -i .mozilla.org
ssl_bump splice nossl
ssl_bump splice nossl2
acl ssl_step1 at_step SslBump1
acl ssl_step2 at_step SslBump2
acl ssl_step3 at_step SslBump3
ssl_bump peek ssl_step1
ssl_bump splice all
sslproxy_flags DONT_VERIFY_PEER
sslproxy_cert_error allow all

1485252508.663      2 192.168.1.236 TAG_NONE/403 6263 CONNECT 
104.16.41.2:443 - HIER_NONE/- text/html
1485252509.385      2 192.168.1.236 TAG_NONE/403 6263 CONNECT 
104.16.41.2:443 - HIER_NONE/- text/html

Using squid port 3128 without any bump allow accessing to mozilla 

So if there are any acl it will be blocked on both.

Return back to list with a full debug mode..


_______________________________________________
squid-users mailing list
squid-users at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-users



Using acl nossl ssl::server_name working like a charme.
Also after restarting C-ICAP everything is fine.
Thanks everyone 

* * * TOPIC CLOSED * * * 




From baborucki at gmail.com  Tue Jan 24 15:52:53 2017
From: baborucki at gmail.com (boruc)
Date: Tue, 24 Jan 2017 07:52:53 -0800 (PST)
Subject: [squid-users] Not all html objects are being cached
Message-ID: <1485273173646-4681293.post@n4.nabble.com>

Hi everyone,

I was wondering why some of visited pages are not being cached (I mean
"main" pages, like www.example.com). If I visit 50 pages only 10 will be
cached. Below text is from log files:

store.log:
1485272001.646 RELEASE -1 FFFFFFFF 04F7FA9EAA7FE3D531A2224F4C7DDE5A  200
1485272011        -1 375007920 text/html -1/222442 GET http://www.wykop.pl/

access.log
1485272001.646    423 10.10.10.136 TCP_MISS/200 223422 GET
http://www.wykop.pl/ - DIRECT/185.66.120.38 text/html

According to Squid Wiki: "if a RELEASE code was logged with file number
FFFFFFFF, the object existed only in memory, and was released from memory."
- I understand that requested html file wasn't saved to disk, but why?

I'm also posting my squid.conf below. I'd be grateful for your answers!


acl manager proto cache_object
acl localhost src 127.0.0.1/32 ::1
acl to_localhost dst 127.0.0.0/8 0.0.0.0/32 ::1
acl my_network src 192.168.0.0/24
acl my_phone src 192.168.54.0/24
acl my_net dst 192.168.0.0/24
acl mgr src 10.48.5.0/24
acl new_net src 10.10.10.0/24
acl ex_ft url_regex -i "/etc/squid3/excluded_filetypes.txt" 
acl ex_do url_regex -i "/etc/squid3/excluded_domains.txt" #doesnt include
any of 50 visited pages

acl SSL_ports port 443
acl Safe_ports port 80		# http
acl Safe_ports port 21		# ftp
acl Safe_ports port 443		# https
acl Safe_ports port 70		# gopher
acl Safe_ports port 210		# wais
acl Safe_ports port 1025-65535	# unregistered ports
acl Safe_ports port 280		# http-mgmt
acl Safe_ports port 488		# gss-http
acl Safe_ports port 591		# filemaker
acl Safe_ports port 777		# multiling http
acl CONNECT method CONNECT

http_access allow my_network
http_access allow my_phone
http_access allow my_net
http_access allow mgr
http_access allow new_net
http_access allow manager localhost
http_access deny manager

http_access deny !Safe_ports

http_access deny CONNECT !SSL_ports

http_access allow localhost
http_access allow all

http_port 3128

maximum_object_size_in_memory 1024 KB

cache_dir ufs /var/spool/squid3 1000 16 256

cache_store_log /var/log/squid3/store.log

coredump_dir /var/spool/squid3

cache deny ex_ft
cache deny ex_do

refresh_pattern ^ftp:		1440	20%	10080
refresh_pattern ^gopher:	1440	0%	1440
refresh_pattern -i (/cgi-bin/|\?) 0	0%	0
refresh_pattern (Release|Packages(.gz)*)$      0       20%     2880

refresh_pattern .               1000       20%     4320

request_header_access Accept-Encoding deny all



--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/Not-all-html-objects-are-being-cached-tp4681293.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From mustafamohammad92 at gmail.com  Tue Jan 24 16:08:00 2017
From: mustafamohammad92 at gmail.com (Mustafa Mohammad)
Date: Tue, 24 Jan 2017 10:08:00 -0600
Subject: [squid-users] Enable SSL bump
In-Reply-To: <6fb3dc49-1d06-07ed-f03e-044de83d2991@treenet.co.nz>
References: <CAOUmaCSpK0_pE2HfA5k+J0YzhO0s2QOeDemY1CgZ5MawUwfBsw@mail.gmail.com>
 <7b7fa1c8-a209-5272-25ba-14853178061f@treenet.co.nz>
 <CAOUmaCRwTh2_CNTe-51OPE9AGvdbqp3_RKPGAn=oBwnmNBxH1A@mail.gmail.com>
 <33df082f-60a3-55de-bc2e-0febb23dde90@treenet.co.nz>
 <CAOUmaCQi0DjGBVZhZqZzpYs1YvSSU5aWDf+fnq6jvHgShDKdLA@mail.gmail.com>
 <6fb3dc49-1d06-07ed-f03e-044de83d2991@treenet.co.nz>
Message-ID: <CAOUmaCSrTqeotb0EmWTdek=5-r4ktkPEL2FR4j+Gv-ONNcQrvw@mail.gmail.com>

No, It is messaging with HTTPS. If I were to use splice and peek, do I need
a self signed certificate or any type of certificate?

On Tue, Jan 24, 2017 at 12:56 AM, Amos Jeffries <squid3 at treenet.co.nz>
wrote:

> On 24/01/2017 3:38 p.m., Mustafa Mohammad wrote:
> > By regression...I mean our QA testing server. Let me explain this in
> > detail: I have a squid proxy running which is needed to connect to the
> > server so we can get back if the transaction was approved or not. It is a
> > point of sale application that send transaction data to the server to
> > receive response about the transaction and that's when the problem is
> > occurring when It is trying to communicate to that server. I received
> some
> > help and I think ssl splice and ssl peek might work but I don't know how
> to
> > use them. I don't the rules to apply in this situation.
>
> Whats usually needed in these setups is a reverse-proxy (aka "load
> balancer", CDN frontend, etc.). But for that to be Squid it would
> require the POS application to be messaging with HTTP.
>  Is that the case?
>
> The peek-and-splice form of SSL-Bump MITM might work anyway so long as
> the application is actually using real TLS. But you need to be aware the
> splice action is just blindly tunneling the TLS data through Squid. It
> is not being touched, so anything like CRL issues is a problem between
> the endpoints - Squid cannot help unless its actually HTTP messages,
> then 'bump' action is needed to fully decrypt and modify the TLS.
>
>
> (That said, there have been some weird issues showing up even when the
> tunnel is spliced. see the threads about 30sec delays to cloudeflare, or
> curl rejecting tunneled traffic.)
>
> Amos
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20170124/b576c4d7/attachment.htm>

From mustafamohammad92 at gmail.com  Tue Jan 24 16:08:37 2017
From: mustafamohammad92 at gmail.com (Mustafa Mohammad)
Date: Tue, 24 Jan 2017 10:08:37 -0600
Subject: [squid-users] Enable SSL bump
In-Reply-To: <CAOUmaCSrTqeotb0EmWTdek=5-r4ktkPEL2FR4j+Gv-ONNcQrvw@mail.gmail.com>
References: <CAOUmaCSpK0_pE2HfA5k+J0YzhO0s2QOeDemY1CgZ5MawUwfBsw@mail.gmail.com>
 <7b7fa1c8-a209-5272-25ba-14853178061f@treenet.co.nz>
 <CAOUmaCRwTh2_CNTe-51OPE9AGvdbqp3_RKPGAn=oBwnmNBxH1A@mail.gmail.com>
 <33df082f-60a3-55de-bc2e-0febb23dde90@treenet.co.nz>
 <CAOUmaCQi0DjGBVZhZqZzpYs1YvSSU5aWDf+fnq6jvHgShDKdLA@mail.gmail.com>
 <6fb3dc49-1d06-07ed-f03e-044de83d2991@treenet.co.nz>
 <CAOUmaCSrTqeotb0EmWTdek=5-r4ktkPEL2FR4j+Gv-ONNcQrvw@mail.gmail.com>
Message-ID: <CAOUmaCT8iuy7fdkP5zbE26fm_GFpqX0pOje8tdjzpM=XMbMGyg@mail.gmail.com>

What TLS option. I don't know how to configure that?

On Tue, Jan 24, 2017 at 10:08 AM, Mustafa Mohammad <
mustafamohammad92 at gmail.com> wrote:

> No, It is messaging with HTTPS. If I were to use splice and peek, do I
> need a self signed certificate or any type of certificate?
>
> On Tue, Jan 24, 2017 at 12:56 AM, Amos Jeffries <squid3 at treenet.co.nz>
> wrote:
>
>> On 24/01/2017 3:38 p.m., Mustafa Mohammad wrote:
>> > By regression...I mean our QA testing server. Let me explain this in
>> > detail: I have a squid proxy running which is needed to connect to the
>> > server so we can get back if the transaction was approved or not. It is
>> a
>> > point of sale application that send transaction data to the server to
>> > receive response about the transaction and that's when the problem is
>> > occurring when It is trying to communicate to that server. I received
>> some
>> > help and I think ssl splice and ssl peek might work but I don't know
>> how to
>> > use them. I don't the rules to apply in this situation.
>>
>> Whats usually needed in these setups is a reverse-proxy (aka "load
>> balancer", CDN frontend, etc.). But for that to be Squid it would
>> require the POS application to be messaging with HTTP.
>>  Is that the case?
>>
>> The peek-and-splice form of SSL-Bump MITM might work anyway so long as
>> the application is actually using real TLS. But you need to be aware the
>> splice action is just blindly tunneling the TLS data through Squid. It
>> is not being touched, so anything like CRL issues is a problem between
>> the endpoints - Squid cannot help unless its actually HTTP messages,
>> then 'bump' action is needed to fully decrypt and modify the TLS.
>>
>>
>> (That said, there have been some weird issues showing up even when the
>> tunnel is spliced. see the threads about 30sec delays to cloudeflare, or
>> curl rejecting tunneled traffic.)
>>
>> Amos
>>
>> _______________________________________________
>> squid-users mailing list
>> squid-users at lists.squid-cache.org
>> http://lists.squid-cache.org/listinfo/squid-users
>>
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20170124/9ed4aaa8/attachment.htm>

From mustafamohammad92 at gmail.com  Tue Jan 24 17:07:54 2017
From: mustafamohammad92 at gmail.com (Mustafa Mohammad)
Date: Tue, 24 Jan 2017 11:07:54 -0600
Subject: [squid-users] Enable SSL bump
In-Reply-To: <6fb3dc49-1d06-07ed-f03e-044de83d2991@treenet.co.nz>
References: <CAOUmaCSpK0_pE2HfA5k+J0YzhO0s2QOeDemY1CgZ5MawUwfBsw@mail.gmail.com>
 <7b7fa1c8-a209-5272-25ba-14853178061f@treenet.co.nz>
 <CAOUmaCRwTh2_CNTe-51OPE9AGvdbqp3_RKPGAn=oBwnmNBxH1A@mail.gmail.com>
 <33df082f-60a3-55de-bc2e-0febb23dde90@treenet.co.nz>
 <CAOUmaCQi0DjGBVZhZqZzpYs1YvSSU5aWDf+fnq6jvHgShDKdLA@mail.gmail.com>
 <6fb3dc49-1d06-07ed-f03e-044de83d2991@treenet.co.nz>
Message-ID: <CAOUmaCTzGfVTxfL58cCWmo-uGYXxRRyVn-AaarcMhVOTBB9F=w@mail.gmail.com>

I just received the news from my team that squid is working at first but
when they restart the service, It doesn't work. Has anyone encountered
issues like that?

On Tue, Jan 24, 2017 at 12:56 AM, Amos Jeffries <squid3 at treenet.co.nz>
wrote:

> On 24/01/2017 3:38 p.m., Mustafa Mohammad wrote:
> > By regression...I mean our QA testing server. Let me explain this in
> > detail: I have a squid proxy running which is needed to connect to the
> > server so we can get back if the transaction was approved or not. It is a
> > point of sale application that send transaction data to the server to
> > receive response about the transaction and that's when the problem is
> > occurring when It is trying to communicate to that server. I received
> some
> > help and I think ssl splice and ssl peek might work but I don't know how
> to
> > use them. I don't the rules to apply in this situation.
>
> Whats usually needed in these setups is a reverse-proxy (aka "load
> balancer", CDN frontend, etc.). But for that to be Squid it would
> require the POS application to be messaging with HTTP.
>  Is that the case?
>
> The peek-and-splice form of SSL-Bump MITM might work anyway so long as
> the application is actually using real TLS. But you need to be aware the
> splice action is just blindly tunneling the TLS data through Squid. It
> is not being touched, so anything like CRL issues is a problem between
> the endpoints - Squid cannot help unless its actually HTTP messages,
> then 'bump' action is needed to fully decrypt and modify the TLS.
>
>
> (That said, there have been some weird issues showing up even when the
> tunnel is spliced. see the threads about 30sec delays to cloudeflare, or
> curl rejecting tunneled traffic.)
>
> Amos
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20170124/e48de408/attachment.htm>

From yvoinov at gmail.com  Tue Jan 24 17:48:46 2017
From: yvoinov at gmail.com (Yuri Voinov)
Date: Tue, 24 Jan 2017 23:48:46 +0600
Subject: [squid-users] Squid 4.x: Intermediate certificates downloader
In-Reply-To: <81cf3b07-28a0-39b6-2690-a6cc320d653f@treenet.co.nz>
References: <4198a8d7-07c7-8c74-80ac-eb75ca61e62c@gmail.com>
 <35afc9c9-dd43-fb17-d0d6-c90866fd5c28@measurement-factory.com>
 <52b0aec8-f61d-1bec-de18-1c3be06494a0@urlfilterdb.com>
 <81cf3b07-28a0-39b6-2690-a6cc320d653f@treenet.co.nz>
Message-ID: <02aff9e7-c248-88f4-4a24-75f68448c378@gmail.com>

Hm. Another question.

It seems 4.0.17 tries to download certs:

1485279884.648      0 - TCP_DENIED/403 3574 GET
http://repository.certum.pl/ca.cer - HIER_NONE/- text/html;charset=utf-8

but gives deny somewhere.

However, same URL with wget via same proxy works:

root @ khorne /patch # wget -S http://repository.certum.pl/ca.cer
--2017-01-24 23:46:37--  http://repository.certum.pl/ca.cer
Connecting to 127.0.0.1:3128... connected.
Proxy request sent, awaiting response...
  HTTP/1.1 200 OK
  Content-Type: text/plain; charset=UTF-8
  Content-Length: 784
  Last-Modified: Fri, 07 Mar 2014 10:05:14 GMT
  ETag: "34231-310-63d6aa80"
  X-Cached: MISS
  Server: NetDNA-cache/2.2
  X-Cache: HIT
  Accept-Ranges: bytes
  X-Origin-Date: Mon, 23 Jan 2017 06:12:38 GMT
  Date: Tue, 24 Jan 2017 17:46:37 GMT
  X-Cache-Age: 128039
  X-Cache: HIT from khorne
  X-Cache-Lookup: HIT from khorne:3128
  Connection: keep-alive
Length: 784 [text/plain]
Saving to: 'ca.cer.2'

ca.cer.2            100%[==================>]     784  --.-KB/s    in
0s     

2017-01-24 23:46:37 (95.7 MB/s) - 'ca.cer.2' saved [784/784]

Why? Downloader requires special ACL? Or something else undocumented?


24.01.2017 5:08, Amos Jeffries ?????:
> On 24/01/2017 7:06 a.m., Marcus Kool wrote:
>>
>> On 23/01/17 15:31, Alex Rousskov wrote:
>>> On 01/23/2017 04:28 AM, Yuri wrote:
>>>
>>>> 1. How does it work?
>>> My response below and the following commit message might answer some of
>>> your questions:
>>>
>>>     http://bazaar.launchpad.net/~squid/squid/5/revision/14769
>> This seems that the feature only goes to Squid 5.  Will it be ported to
>> Squid 4 ?
> rev.14769 is from before Squid-5 existed (rev.14932). The commits
> labeled 'trunk' at that time were Squid-4.
>
> Amos
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users

-------------- next part --------------
A non-text attachment was scrubbed...
Name: 0x613DEC46.asc
Type: application/pgp-keys
Size: 2437 bytes
Desc: not available
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20170124/2f5026e8/attachment.key>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: signature.asc
Type: application/pgp-signature
Size: 473 bytes
Desc: OpenPGP digital signature
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20170124/2f5026e8/attachment.sig>

From rousskov at measurement-factory.com  Tue Jan 24 18:16:48 2017
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Tue, 24 Jan 2017 11:16:48 -0700
Subject: [squid-users] Squid 4.x: Intermediate certificates downloader
In-Reply-To: <02aff9e7-c248-88f4-4a24-75f68448c378@gmail.com>
References: <4198a8d7-07c7-8c74-80ac-eb75ca61e62c@gmail.com>
 <35afc9c9-dd43-fb17-d0d6-c90866fd5c28@measurement-factory.com>
 <52b0aec8-f61d-1bec-de18-1c3be06494a0@urlfilterdb.com>
 <81cf3b07-28a0-39b6-2690-a6cc320d653f@treenet.co.nz>
 <02aff9e7-c248-88f4-4a24-75f68448c378@gmail.com>
Message-ID: <4df5b398-9c23-87ad-81c4-4d34c16a231c@measurement-factory.com>

On 01/24/2017 10:48 AM, Yuri Voinov wrote:

> It seems 4.0.17 tries to download certs but gives deny somewhere.
> However, same URL with wget via same proxy works
> Why?

Most likely, your http_access or similar rules deny internal download
transactions but allow external ones. This is possible, for example, if
your access rules use client information. Internal transactions (ESI,
missing certificate fetching, Cache Digests, etc.) do not have an
associated client.

The standard denial troubleshooting procedure applies here: Start with
finding out which directive/ACL denies access. I am _not_ implying that
this is easy to do.


HTH,

Alex.



From yvoinov at gmail.com  Tue Jan 24 18:19:24 2017
From: yvoinov at gmail.com (Yuri Voinov)
Date: Wed, 25 Jan 2017 00:19:24 +0600
Subject: [squid-users] Squid 4.x: Intermediate certificates downloader
In-Reply-To: <4df5b398-9c23-87ad-81c4-4d34c16a231c@measurement-factory.com>
References: <4198a8d7-07c7-8c74-80ac-eb75ca61e62c@gmail.com>
 <35afc9c9-dd43-fb17-d0d6-c90866fd5c28@measurement-factory.com>
 <52b0aec8-f61d-1bec-de18-1c3be06494a0@urlfilterdb.com>
 <81cf3b07-28a0-39b6-2690-a6cc320d653f@treenet.co.nz>
 <02aff9e7-c248-88f4-4a24-75f68448c378@gmail.com>
 <4df5b398-9c23-87ad-81c4-4d34c16a231c@measurement-factory.com>
Message-ID: <42af5a38-f4f9-19a5-7397-b3c7084ce1a4@gmail.com>

Mmmmmm, hardly.

It is downloads directly via proxy from localhost:

root @ khorne /patch # http_proxy=localhost:3128 curl
http://repository.certum.pl/ca.cer
0
0>1     *H
   0    UPL1U
270611104639Z0>1o.10U   Certum CA0
                0       UPL1U
0       *H. z o.o.10U   Certum CA0"0
AK?jk??g?&_O????n??n9??? r?[?????Vn?S    ^Uc??.0h???nZN4?P?mB      ?
?O)?B^?
?????Dl?9>?n??!w?w???c?7?v?$L?go-??e1p?
{mXI?c2
       k???;??   Q???`'l2w??r????B????T(>?M
:;#c??'y??];?????nd???t.q;?io?|R??g?p??i?@Hj5?f!,?J@??,s

root @ khorne /patch #

root @ khorne /patch # wget -S http://repository.certum.pl/ca.cer
--2017-01-24 23:59:54--  http://repository.certum.pl/ca.cer
Connecting to 127.0.0.1:3128... connected.
Proxy request sent, awaiting response...
  HTTP/1.1 200 OK
  Content-Type: text/plain; charset=UTF-8
  Content-Length: 784
  Last-Modified: Fri, 07 Mar 2014 10:05:14 GMT
  ETag: "34231-310-63d6aa80"
  X-Cached: MISS
  Server: NetDNA-cache/2.2
  X-Cache: HIT
  Accept-Ranges: bytes
  X-Origin-Date: Mon, 23 Jan 2017 06:12:38 GMT
  Date: Tue, 24 Jan 2017 17:59:54 GMT
  X-Cache-Age: 128836
  X-Cache: HIT from khorne
  X-Cache-Lookup: HIT from khorne:3128
  Connection: keep-alive
Length: 784 [text/plain]
Saving to: 'ca.cer'

ca.cer              100%[==================>]     784  --.-KB/s    in
0s     

2017-01-24 23:59:54 (86.2 MB/s) - 'ca.cer' saved [784/784]

As I understand, downloader also access via localhost, right? So, it
should work.

Either from localnet, or from localhost download occurs.


25.01.2017 0:16, Alex Rousskov ?????:
> On 01/24/2017 10:48 AM, Yuri Voinov wrote:
>
>> It seems 4.0.17 tries to download certs but gives deny somewhere.
>> However, same URL with wget via same proxy works
>> Why?
> Most likely, your http_access or similar rules deny internal download
> transactions but allow external ones. This is possible, for example, if
> your access rules use client information. Internal transactions (ESI,
> missing certificate fetching, Cache Digests, etc.) do not have an
> associated client.
>
> The standard denial troubleshooting procedure applies here: Start with
> finding out which directive/ACL denies access. I am _not_ implying that
> this is easy to do.
>
>
> HTH,
>
> Alex.
>

-------------- next part --------------
A non-text attachment was scrubbed...
Name: 0x613DEC46.asc
Type: application/pgp-keys
Size: 2437 bytes
Desc: not available
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20170125/95e65199/attachment.key>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: signature.asc
Type: application/pgp-signature
Size: 473 bytes
Desc: OpenPGP digital signature
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20170125/95e65199/attachment.sig>

From yvoinov at gmail.com  Tue Jan 24 18:22:24 2017
From: yvoinov at gmail.com (Yuri Voinov)
Date: Wed, 25 Jan 2017 00:22:24 +0600
Subject: [squid-users] Squid 4.x: Intermediate certificates downloader
In-Reply-To: <42af5a38-f4f9-19a5-7397-b3c7084ce1a4@gmail.com>
References: <4198a8d7-07c7-8c74-80ac-eb75ca61e62c@gmail.com>
 <35afc9c9-dd43-fb17-d0d6-c90866fd5c28@measurement-factory.com>
 <52b0aec8-f61d-1bec-de18-1c3be06494a0@urlfilterdb.com>
 <81cf3b07-28a0-39b6-2690-a6cc320d653f@treenet.co.nz>
 <02aff9e7-c248-88f4-4a24-75f68448c378@gmail.com>
 <4df5b398-9c23-87ad-81c4-4d34c16a231c@measurement-factory.com>
 <42af5a38-f4f9-19a5-7397-b3c7084ce1a4@gmail.com>
Message-ID: <646eaf46-2e3e-0796-f9bd-0b90e8c8cee8@gmail.com>

May be, this feature is mutually exclusive with
sslproxy_foreign_intermediate_certs option?


25.01.2017 0:19, Yuri Voinov ?????:
> Mmmmmm, hardly.
>
> It is downloads directly via proxy from localhost:
>
> root @ khorne /patch # http_proxy=localhost:3128 curl
> http://repository.certum.pl/ca.cer
> 0
> 0>1     *H
>    0    UPL1U
> 270611104639Z0>1o.10U   Certum CA0
>                 0       UPL1U
> 0       *H. z o.o.10U   Certum CA0"0
> AK?jk??g?&_O????n??n9??? r?[?????Vn?S    ^Uc??.0h???nZN4?P?mB      ?
> ?O)?B^?
> ?????Dl?9>?n??!w?w???c?7?v?$L?go-??e1p?
> {mXI?c2
>        k???;??   Q???`'l2w??r????B????T(>?M
> :;#c??'y??];?????nd???t.q;?io?|R??g?p??i?@Hj5?f!,?J@??,s
>
> root @ khorne /patch #
>
> root @ khorne /patch # wget -S http://repository.certum.pl/ca.cer
> --2017-01-24 23:59:54--  http://repository.certum.pl/ca.cer
> Connecting to 127.0.0.1:3128... connected.
> Proxy request sent, awaiting response...
>   HTTP/1.1 200 OK
>   Content-Type: text/plain; charset=UTF-8
>   Content-Length: 784
>   Last-Modified: Fri, 07 Mar 2014 10:05:14 GMT
>   ETag: "34231-310-63d6aa80"
>   X-Cached: MISS
>   Server: NetDNA-cache/2.2
>   X-Cache: HIT
>   Accept-Ranges: bytes
>   X-Origin-Date: Mon, 23 Jan 2017 06:12:38 GMT
>   Date: Tue, 24 Jan 2017 17:59:54 GMT
>   X-Cache-Age: 128836
>   X-Cache: HIT from khorne
>   X-Cache-Lookup: HIT from khorne:3128
>   Connection: keep-alive
> Length: 784 [text/plain]
> Saving to: 'ca.cer'
>
> ca.cer              100%[==================>]     784  --.-KB/s    in
> 0s     
>
> 2017-01-24 23:59:54 (86.2 MB/s) - 'ca.cer' saved [784/784]
>
> As I understand, downloader also access via localhost, right? So, it
> should work.
>
> Either from localnet, or from localhost download occurs.
>
>
> 25.01.2017 0:16, Alex Rousskov ?????:
>> On 01/24/2017 10:48 AM, Yuri Voinov wrote:
>>
>>> It seems 4.0.17 tries to download certs but gives deny somewhere.
>>> However, same URL with wget via same proxy works
>>> Why?
>> Most likely, your http_access or similar rules deny internal download
>> transactions but allow external ones. This is possible, for example, if
>> your access rules use client information. Internal transactions (ESI,
>> missing certificate fetching, Cache Digests, etc.) do not have an
>> associated client.
>>
>> The standard denial troubleshooting procedure applies here: Start with
>> finding out which directive/ACL denies access. I am _not_ implying that
>> this is easy to do.
>>
>>
>> HTH,
>>
>> Alex.
>>

-------------- next part --------------
A non-text attachment was scrubbed...
Name: 0x613DEC46.asc
Type: application/pgp-keys
Size: 2437 bytes
Desc: not available
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20170125/0b217275/attachment.key>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: signature.asc
Type: application/pgp-signature
Size: 473 bytes
Desc: OpenPGP digital signature
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20170125/0b217275/attachment.sig>

From rousskov at measurement-factory.com  Tue Jan 24 18:27:46 2017
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Tue, 24 Jan 2017 11:27:46 -0700
Subject: [squid-users] Squid 4.x: Intermediate certificates downloader
In-Reply-To: <42af5a38-f4f9-19a5-7397-b3c7084ce1a4@gmail.com>
References: <4198a8d7-07c7-8c74-80ac-eb75ca61e62c@gmail.com>
 <35afc9c9-dd43-fb17-d0d6-c90866fd5c28@measurement-factory.com>
 <52b0aec8-f61d-1bec-de18-1c3be06494a0@urlfilterdb.com>
 <81cf3b07-28a0-39b6-2690-a6cc320d653f@treenet.co.nz>
 <02aff9e7-c248-88f4-4a24-75f68448c378@gmail.com>
 <4df5b398-9c23-87ad-81c4-4d34c16a231c@measurement-factory.com>
 <42af5a38-f4f9-19a5-7397-b3c7084ce1a4@gmail.com>
Message-ID: <e9b275f4-f9b1-7eda-e102-420bb9eda7ad@measurement-factory.com>

On 01/24/2017 11:19 AM, Yuri Voinov wrote:

> It is downloads directly via proxy from localhost:

> As I understand, downloader also access via localhost, right? 

This is incorrect. Downloader does not have a concept of an HTTP client
which sends the request to Squid so "via localhost" or "via any client
source address" does not apply to Downloader transactions. In other
words, there is no client [source address] for Downloader requests.

Unfortunately, I do not know exactly what effect that lack of info has
on what ACLs (in part because there are too many of them and because
lack of info is often treated inconsistently by various ACLs). Thus, I
continue to recommend finding out which directive/ACL denied Downloader
access as the first step.

Alex.


> 25.01.2017 0:16, Alex Rousskov ?????:
>> On 01/24/2017 10:48 AM, Yuri Voinov wrote:
>>
>>> It seems 4.0.17 tries to download certs but gives deny somewhere.
>>> However, same URL with wget via same proxy works
>>> Why?
>> Most likely, your http_access or similar rules deny internal download
>> transactions but allow external ones. This is possible, for example, if
>> your access rules use client information. Internal transactions (ESI,
>> missing certificate fetching, Cache Digests, etc.) do not have an
>> associated client.
>>
>> The standard denial troubleshooting procedure applies here: Start with
>> finding out which directive/ACL denies access. I am _not_ implying that
>> this is easy to do.



From yvoinov at gmail.com  Tue Jan 24 18:33:37 2017
From: yvoinov at gmail.com (Yuri Voinov)
Date: Wed, 25 Jan 2017 00:33:37 +0600
Subject: [squid-users] Squid 4.x: Intermediate certificates downloader
In-Reply-To: <e9b275f4-f9b1-7eda-e102-420bb9eda7ad@measurement-factory.com>
References: <4198a8d7-07c7-8c74-80ac-eb75ca61e62c@gmail.com>
 <35afc9c9-dd43-fb17-d0d6-c90866fd5c28@measurement-factory.com>
 <52b0aec8-f61d-1bec-de18-1c3be06494a0@urlfilterdb.com>
 <81cf3b07-28a0-39b6-2690-a6cc320d653f@treenet.co.nz>
 <02aff9e7-c248-88f4-4a24-75f68448c378@gmail.com>
 <4df5b398-9c23-87ad-81c4-4d34c16a231c@measurement-factory.com>
 <42af5a38-f4f9-19a5-7397-b3c7084ce1a4@gmail.com>
 <e9b275f4-f9b1-7eda-e102-420bb9eda7ad@measurement-factory.com>
Message-ID: <6d9fb82c-a83d-1215-7df7-04259c9a3997@gmail.com>

This is working production server. I've checked configuration twice. See
no problem.

Here:


# -------------------------------------
# Access parameters
# -------------------------------------
# Deny requests to unsafe ports
http_access deny !Safe_ports

# Instant messengers include
include "/usr/local/squid/etc/acl.im.include"

# Deny CONNECT to other than SSL ports
http_access deny CONNECT !SSL_ports

# Only allow cachemgr access from localhost
http_access allow localhost manager
http_access deny manager
http_access deny to_localhost
# Allow purge from localhost
http_access allow PURGE localhost
http_access deny PURGE

# Block torrent files
acl TorrentFiles rep_mime_type mime-type application/x-bittorrent
http_reply_access deny TorrentFiles
deny_info TCP_RESET TorrentFiles

# No cache directives
cache deny dont_cache_url
cache allow all

# 302 loop
acl text_mime rep_mime_type text/html text/plain
acl http302 http_status 302
store_miss deny text_mime http302
send_hit deny text_mime http302

# Windows updates rules
http_access allow CONNECT wuCONNECT localnet
http_access allow CONNECT wuCONNECT localhost
http_access allow windowsupdate localnet
http_access allow windowsupdate localhost

# SSL bump rules
acl DiscoverSNIHost at_step SslBump1
acl NoSSLIntercept ssl::server_name_regex
"/usr/local/squid/etc/acl.url.nobump"
ssl_bump peek DiscoverSNIHost
ssl_bump splice DiscoverSNIHost icq
ssl_bump splice DiscoverSNIHost icqip icqport
ssl_bump splice NoSSLIntercept
ssl_bump bump all

# Rule allowing access from local networks
http_access allow localnet
http_access allow localhost

# And finally deny all other access to this proxy
http_access deny all

is ok.

I'm only on doubt about this:

http_access deny to_localhost

but it is recommended to use long time, as documented:

# We strongly recommend the following be uncommented to protect innocent
# web applications running on the proxy server who think the only
# one who can access services on "localhost" is a local user
#http_access deny to_localhost

and has no visible effect to comment out this line.

I have no idea, what can block access.

25.01.2017 0:27, Alex Rousskov ?????:
> On 01/24/2017 11:19 AM, Yuri Voinov wrote:
>
>> It is downloads directly via proxy from localhost:
>> As I understand, downloader also access via localhost, right? 
> This is incorrect. Downloader does not have a concept of an HTTP client
> which sends the request to Squid so "via localhost" or "via any client
> source address" does not apply to Downloader transactions. In other
> words, there is no client [source address] for Downloader requests.
>
> Unfortunately, I do not know exactly what effect that lack of info has
> on what ACLs (in part because there are too many of them and because
> lack of info is often treated inconsistently by various ACLs). Thus, I
> continue to recommend finding out which directive/ACL denied Downloader
> access as the first step.
>
> Alex.
>
>
>> 25.01.2017 0:16, Alex Rousskov ?????:
>>> On 01/24/2017 10:48 AM, Yuri Voinov wrote:
>>>
>>>> It seems 4.0.17 tries to download certs but gives deny somewhere.
>>>> However, same URL with wget via same proxy works
>>>> Why?
>>> Most likely, your http_access or similar rules deny internal download
>>> transactions but allow external ones. This is possible, for example, if
>>> your access rules use client information. Internal transactions (ESI,
>>> missing certificate fetching, Cache Digests, etc.) do not have an
>>> associated client.
>>>
>>> The standard denial troubleshooting procedure applies here: Start with
>>> finding out which directive/ACL denies access. I am _not_ implying that
>>> this is easy to do.

-------------- next part --------------
A non-text attachment was scrubbed...
Name: 0x613DEC46.asc
Type: application/pgp-keys
Size: 2437 bytes
Desc: not available
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20170125/7d606d04/attachment.key>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: signature.asc
Type: application/pgp-signature
Size: 473 bytes
Desc: OpenPGP digital signature
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20170125/7d606d04/attachment.sig>

From rousskov at measurement-factory.com  Tue Jan 24 18:41:54 2017
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Tue, 24 Jan 2017 11:41:54 -0700
Subject: [squid-users] squid reverse proxy (accelerator) for MS Exchange
 OWA
In-Reply-To: <1374872097.4095106.1485244925609@mail.yahoo.com>
References: <1216522442.160455.1484870639800.ref@mail.yahoo.com>
 <1216522442.160455.1484870639800@mail.yahoo.com>
 <cdea0afa-c2a2-23c4-dbac-553a565e83f7@treenet.co.nz>
 <690032068.431187.1484905491833@mail.yahoo.com>
 <5517395f-eaa4-0b3a-34e9-66ea8d0122a1@treenet.co.nz>
 <1374872097.4095106.1485244925609@mail.yahoo.com>
Message-ID: <94978436-88e5-fc84-9f3e-b8f1757402a5@measurement-factory.com>

On 01/24/2017 01:02 AM, Vieri wrote:
> 2017/01/24 07:58:57.076 kid1| 83,5| bio.cc(139) read: FD 18 read 0 <= 65535

The peer at 10.215.144.21:443 accepted Squid connection and then closed
it, probably before sending anything to Squid (you did not show enough
FD 18 history to confirm that with certainty, but it is likely).


> 2017/01/24 07:58:57.076 kid1| 83,5| NegotiationHistory.cc(83) retrieveNegotiatedInfo: SSL connection info on FD 18 SSL version NONE/0.0 negotiated cipher

Nothing was negotiated.


> 2017/01/24 07:58:57.076 kid1| Error negotiating SSL on FD 18: error:00000000:lib(0):func(0):reason(0) (5/0/0)

More-or-less confirms the above -- an SSL-violating end of TCP connection.


> 2017/01/24 07:58:57.076 kid1| TCP connection to 10.215.144.21/443 failed
> 2017/01/24 07:58:57.077 kid1| 15,2| neighbors.cc(1246) peerConnectFailedSilent: TCP connection to 10.215.144.21/443 dead

More echoes of the same error.


> Also, I'm not sure why the SSL version isn't picked up (NONE/0.0)

Probably because the server did not send its SSL version to Squid.


> What else can I try?

I would start by capturing TCP packets between Squid and the server in
question to confirm that the server (a) receives SSL ClientHello from
Squid and (b) closes the connection before sending SSL ServerHello to Squid.

If that is confirmed, you could interrogate the server about its
decision to close the connection. For example, it may not like the
ciphers offered by Squid. Establishing similar SSL connections from
Squid IP address to the server using OpenSSL command-line client may
help triage this further.


HTH,

Alex.



From rousskov at measurement-factory.com  Tue Jan 24 19:10:23 2017
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Tue, 24 Jan 2017 12:10:23 -0700
Subject: [squid-users] Squid 4.x: Intermediate certificates downloader
In-Reply-To: <6d9fb82c-a83d-1215-7df7-04259c9a3997@gmail.com>
References: <4198a8d7-07c7-8c74-80ac-eb75ca61e62c@gmail.com>
 <35afc9c9-dd43-fb17-d0d6-c90866fd5c28@measurement-factory.com>
 <52b0aec8-f61d-1bec-de18-1c3be06494a0@urlfilterdb.com>
 <81cf3b07-28a0-39b6-2690-a6cc320d653f@treenet.co.nz>
 <02aff9e7-c248-88f4-4a24-75f68448c378@gmail.com>
 <4df5b398-9c23-87ad-81c4-4d34c16a231c@measurement-factory.com>
 <42af5a38-f4f9-19a5-7397-b3c7084ce1a4@gmail.com>
 <e9b275f4-f9b1-7eda-e102-420bb9eda7ad@measurement-factory.com>
 <6d9fb82c-a83d-1215-7df7-04259c9a3997@gmail.com>
Message-ID: <42f09caf-ee99-0f5e-b187-7468b736a04d@measurement-factory.com>

On 01/24/2017 11:33 AM, Yuri Voinov wrote:

>> 1485279884.648      0 - TCP_DENIED/403 3574 GET
>> http://repository.certum.pl/ca.cer - HIER_NONE/- text/html;charset=utf-8


> http_access deny !Safe_ports

Probably does not match -- 80 is a safe port.


> # Instant messengers include
> include "/usr/local/squid/etc/acl.im.include"

I am guessing these do not match or are irrelevant.


> # Deny CONNECT to other than SSL ports
> http_access deny CONNECT !SSL_ports

Does not match. This is a GET request.


> # Only allow cachemgr access from localhost
> http_access allow localhost manager
> http_access deny manager

Probably do not match. This is not a cache manager request although I
have not checked how Squid identifies those exactly.


> http_access deny to_localhost

Does not match. The destination is not localhost.


> # Allow purge from localhost
> http_access allow PURGE localhost
> http_access deny PURGE

Do not match. This is a GET request, not PURGE.


> # Block torrent files
> acl TorrentFiles rep_mime_type mime-type application/x-bittorrent
> http_reply_access deny TorrentFiles

Does not match. There was no response [with an application/x-bittorrent
MIME type].


> # Windows updates rules
> http_access allow CONNECT wuCONNECT localnet
> http_access allow CONNECT wuCONNECT localhost

Do not match. This is a GET request, not CONNECT.


> http_access allow windowsupdate localnet
> http_access allow windowsupdate localhost

Probably do not match. The internal transaction is not associated with a
to-Squid connection coming from localnet or localhost.


> # Rule allowing access from local networks
> http_access allow localnet
> http_access allow localhost

Probably do not match. The internal transaction is not associated with a
to-Squid connection coming from localnet or localhost.


> # And finally deny all other access to this proxy
> http_access deny all

Matches!


> I have no idea, what can block access.

That much was clear from the time you asked the question. I bet your
last http_access rule that denies all other connection matches, but I
would still ask Squid. Squid knows why it blocks (or does not allow)
access. There are several ways to ask Squid, including increasing
debugging verbosity when reproducing the problem, adding the matching
ACL to the error message, using custom error messages for different
http_access deny lines, etc.

These methods are not easy, pleasant, quick, or human-friendly,
unfortunately, but you are a very capable sysadmin with more than enough
Squid knowledge to find the blocking directive/ACL, especially for a
problem that can be isolated to two HTTP transactions.

Once we know what directive/ACL blocks, we may be able to figure out a
workaround, propose a bug fix, etc. For example, if my guess is correct
-- the "deny all" rule has matched -- then you would need to add a rule
to allow internal requests, including the ones that fetch those missing
certificates.


HTH,

Alex.


> 25.01.2017 0:27, Alex Rousskov ?????:
>> On 01/24/2017 11:19 AM, Yuri Voinov wrote:
>>
>>> It is downloads directly via proxy from localhost:
>>> As I understand, downloader also access via localhost, right? 
>> This is incorrect. Downloader does not have a concept of an HTTP client
>> which sends the request to Squid so "via localhost" or "via any client
>> source address" does not apply to Downloader transactions. In other
>> words, there is no client [source address] for Downloader requests.
>>
>> Unfortunately, I do not know exactly what effect that lack of info has
>> on what ACLs (in part because there are too many of them and because
>> lack of info is often treated inconsistently by various ACLs). Thus, I
>> continue to recommend finding out which directive/ACL denied Downloader
>> access as the first step.
>>
>> Alex.
>>
>>
>>> 25.01.2017 0:16, Alex Rousskov ?????:
>>>> On 01/24/2017 10:48 AM, Yuri Voinov wrote:
>>>>
>>>>> It seems 4.0.17 tries to download certs but gives deny somewhere.
>>>>> However, same URL with wget via same proxy works
>>>>> Why?
>>>> Most likely, your http_access or similar rules deny internal download
>>>> transactions but allow external ones. This is possible, for example, if
>>>> your access rules use client information. Internal transactions (ESI,
>>>> missing certificate fetching, Cache Digests, etc.) do not have an
>>>> associated client.
>>>>
>>>> The standard denial troubleshooting procedure applies here: Start with
>>>> finding out which directive/ACL denies access. I am _not_ implying that
>>>> this is easy to do.
> 



From yvoinov at gmail.com  Tue Jan 24 19:20:17 2017
From: yvoinov at gmail.com (Yuri Voinov)
Date: Wed, 25 Jan 2017 01:20:17 +0600
Subject: [squid-users] Squid 4.x: Intermediate certificates downloader
In-Reply-To: <42f09caf-ee99-0f5e-b187-7468b736a04d@measurement-factory.com>
References: <4198a8d7-07c7-8c74-80ac-eb75ca61e62c@gmail.com>
 <35afc9c9-dd43-fb17-d0d6-c90866fd5c28@measurement-factory.com>
 <52b0aec8-f61d-1bec-de18-1c3be06494a0@urlfilterdb.com>
 <81cf3b07-28a0-39b6-2690-a6cc320d653f@treenet.co.nz>
 <02aff9e7-c248-88f4-4a24-75f68448c378@gmail.com>
 <4df5b398-9c23-87ad-81c4-4d34c16a231c@measurement-factory.com>
 <42af5a38-f4f9-19a5-7397-b3c7084ce1a4@gmail.com>
 <e9b275f4-f9b1-7eda-e102-420bb9eda7ad@measurement-factory.com>
 <6d9fb82c-a83d-1215-7df7-04259c9a3997@gmail.com>
 <42f09caf-ee99-0f5e-b187-7468b736a04d@measurement-factory.com>
Message-ID: <c4e25904-5c86-e3cb-c49c-f1f7d3f4583e@gmail.com>



25.01.2017 1:10, Alex Rousskov ?????:
> On 01/24/2017 11:33 AM, Yuri Voinov wrote:
>
>>> 1485279884.648      0 - TCP_DENIED/403 3574 GET
>>> http://repository.certum.pl/ca.cer - HIER_NONE/- text/html;charset=utf-8
>
>> http_access deny !Safe_ports
> Probably does not match -- 80 is a safe port.
>
>
>> # Instant messengers include
>> include "/usr/local/squid/etc/acl.im.include"
> I am guessing these do not match or are irrelevant.
Yes, irrelevant.
>
>
>> # Deny CONNECT to other than SSL ports
>> http_access deny CONNECT !SSL_ports
> Does not match. This is a GET request.
Exactly.
>
>
>> # Only allow cachemgr access from localhost
>> http_access allow localhost manager
>> http_access deny manager
> Probably do not match. This is not a cache manager request although I
> have not checked how Squid identifies those exactly.
>
>
>> http_access deny to_localhost
> Does not match. The destination is not localhost.
Yes, destination is squid itself. From squid to squid.
>
>
>> # Allow purge from localhost
>> http_access allow PURGE localhost
>> http_access deny PURGE
> Do not match. This is a GET request, not PURGE.
>
>
>> # Block torrent files
>> acl TorrentFiles rep_mime_type mime-type application/x-bittorrent
>> http_reply_access deny TorrentFiles
> Does not match. There was no response [with an application/x-bittorrent
> MIME type].
>
>
>> # Windows updates rules
>> http_access allow CONNECT wuCONNECT localnet
>> http_access allow CONNECT wuCONNECT localhost
> Do not match. This is a GET request, not CONNECT.
>
>
>> http_access allow windowsupdate localnet
>> http_access allow windowsupdate localhost
> Probably do not match. The internal transaction is not associated with a
> to-Squid connection coming from localnet or localhost.
>
>
>> # Rule allowing access from local networks
>> http_access allow localnet
>> http_access allow localhost
> Probably do not match. The internal transaction is not associated with a
> to-Squid connection coming from localnet or localhost.
Exactly.
>
>
>> # And finally deny all other access to this proxy
>> http_access deny all
> Matches!
But! This is recommended final deny rule, if I omit it - squid will adds
it silently by default, like normal firewall!
>
>
>> I have no idea, what can block access.
> That much was clear from the time you asked the question. I bet your
> last http_access rule that denies all other connection matches, but I
> would still ask Squid. Squid knows why it blocks (or does not allow)
> access. There are several ways to ask Squid, including increasing
> debugging verbosity when reproducing the problem, adding the matching
> ACL to the error message, using custom error messages for different
> http_access deny lines, etc.
Yes. I've tought about debugging.
>
> These methods are not easy, pleasant, quick, or human-friendly,
> unfortunately, but you are a very capable sysadmin with more than enough
> Squid knowledge to find the blocking directive/ACL, especially for a
> problem that can be isolated to two HTTP transactions.
>
> Once we know what directive/ACL blocks, we may be able to figure out a
> workaround, propose a bug fix, etc. For example, if my guess is correct
> -- the "deny all" rule has matched -- then you would need to add a rule
> to allow internal requests, including the ones that fetch those missing
> certificates.
Here is in doubt. I tought I've good knowlegde about squid's acl. But I
don't know (on first look) special ACL rule to permit internal access
from squid to squid.

I'm reading documented config - there is no special ACL type to
permit/deny internal access.

Hmmmmmmmmmm. It looks squid blocks own internal access to itself, but
permits the same request from outside.

Can this be bug? Or I lost something?
>
>
> HTH,
>
> Alex.
>
>
>> 25.01.2017 0:27, Alex Rousskov ?????:
>>> On 01/24/2017 11:19 AM, Yuri Voinov wrote:
>>>
>>>> It is downloads directly via proxy from localhost:
>>>> As I understand, downloader also access via localhost, right? 
>>> This is incorrect. Downloader does not have a concept of an HTTP client
>>> which sends the request to Squid so "via localhost" or "via any client
>>> source address" does not apply to Downloader transactions. In other
>>> words, there is no client [source address] for Downloader requests.
>>>
>>> Unfortunately, I do not know exactly what effect that lack of info has
>>> on what ACLs (in part because there are too many of them and because
>>> lack of info is often treated inconsistently by various ACLs). Thus, I
>>> continue to recommend finding out which directive/ACL denied Downloader
>>> access as the first step.
>>>
>>> Alex.
>>>
>>>
>>>> 25.01.2017 0:16, Alex Rousskov ?????:
>>>>> On 01/24/2017 10:48 AM, Yuri Voinov wrote:
>>>>>
>>>>>> It seems 4.0.17 tries to download certs but gives deny somewhere.
>>>>>> However, same URL with wget via same proxy works
>>>>>> Why?
>>>>> Most likely, your http_access or similar rules deny internal download
>>>>> transactions but allow external ones. This is possible, for example, if
>>>>> your access rules use client information. Internal transactions (ESI,
>>>>> missing certificate fetching, Cache Digests, etc.) do not have an
>>>>> associated client.
>>>>>
>>>>> The standard denial troubleshooting procedure applies here: Start with
>>>>> finding out which directive/ACL denies access. I am _not_ implying that
>>>>> this is easy to do.

-------------- next part --------------
A non-text attachment was scrubbed...
Name: 0x613DEC46.asc
Type: application/pgp-keys
Size: 2437 bytes
Desc: not available
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20170125/41151124/attachment.key>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: signature.asc
Type: application/pgp-signature
Size: 473 bytes
Desc: OpenPGP digital signature
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20170125/41151124/attachment.sig>

From yvoinov at gmail.com  Tue Jan 24 19:45:23 2017
From: yvoinov at gmail.com (Yuri Voinov)
Date: Wed, 25 Jan 2017 01:45:23 +0600
Subject: [squid-users] Squid 4.x: Intermediate certificates downloader
In-Reply-To: <42f09caf-ee99-0f5e-b187-7468b736a04d@measurement-factory.com>
References: <4198a8d7-07c7-8c74-80ac-eb75ca61e62c@gmail.com>
 <35afc9c9-dd43-fb17-d0d6-c90866fd5c28@measurement-factory.com>
 <52b0aec8-f61d-1bec-de18-1c3be06494a0@urlfilterdb.com>
 <81cf3b07-28a0-39b6-2690-a6cc320d653f@treenet.co.nz>
 <02aff9e7-c248-88f4-4a24-75f68448c378@gmail.com>
 <4df5b398-9c23-87ad-81c4-4d34c16a231c@measurement-factory.com>
 <42af5a38-f4f9-19a5-7397-b3c7084ce1a4@gmail.com>
 <e9b275f4-f9b1-7eda-e102-420bb9eda7ad@measurement-factory.com>
 <6d9fb82c-a83d-1215-7df7-04259c9a3997@gmail.com>
 <42f09caf-ee99-0f5e-b187-7468b736a04d@measurement-factory.com>
Message-ID: <d57fdcf5-30b4-c258-f506-630becbdad79@gmail.com>

Under detailed ACL debug got this transaction:

2017/01/25 01:36:35.772 kid1| 28,3| DomainData.cc(110) match:
aclMatchDomainList: checking 'repository.certum.pl'
2017/01/25 01:36:35.772 kid1| 28,3| DomainData.cc(115) match:
aclMatchDomainList: 'repository.certum.pl' NOT found
2017/01/25 01:36:35.772 kid1| 28,3| Acl.cc(290) matches: checked:
block_tld = 0
2017/01/25 01:36:35.772 kid1| 28,3| Acl.cc(290) matches: checked:
http_access#11 = 0
2017/01/25 01:36:35.772 kid1| 28,5| Checklist.cc(397) bannedAction:
Action 'ALLOWED/0' is not banned
2017/01/25 01:36:35.772 kid1| 28,5| Acl.cc(263) matches: checking
http_access#12
2017/01/25 01:36:35.772 kid1| 28,5| Acl.cc(263) matches: checking CONNECT
2017/01/25 01:36:35.772 kid1| 28,3| Acl.cc(290) matches: checked:
CONNECT = 0
2017/01/25 01:36:35.772 kid1| 28,3| Acl.cc(290) matches: checked:
http_access#12 = 0
2017/01/25 01:36:35.772 kid1| 28,5| Checklist.cc(397) bannedAction:
Action 'ALLOWED/0' is not banned
2017/01/25 01:36:35.772 kid1| 28,5| Acl.cc(263) matches: checking
http_access#13
2017/01/25 01:36:35.772 kid1| 28,5| Acl.cc(263) matches: checking CONNECT
2017/01/25 01:36:35.772 kid1| 28,3| Acl.cc(290) matches: checked:
CONNECT = 0
2017/01/25 01:36:35.772 kid1| 28,3| Acl.cc(290) matches: checked:
http_access#13 = 0
2017/01/25 01:36:35.772 kid1| 28,5| Checklist.cc(397) bannedAction:
Action 'ALLOWED/0' is not banned
2017/01/25 01:36:35.772 kid1| 28,5| Acl.cc(263) matches: checking
http_access#14
2017/01/25 01:36:35.772 kid1| 28,5| Acl.cc(263) matches: checking
windowsupdate
2017/01/25 01:36:35.772 kid1| 28,3| DomainData.cc(110) match:
aclMatchDomainList: checking 'repository.certum.pl'
2017/01/25 01:36:35.772 kid1| 28,3| DomainData.cc(115) match:
aclMatchDomainList: 'repository.certum.pl' NOT found
2017/01/25 01:36:35.772 kid1| 28,3| Acl.cc(290) matches: checked:
windowsupdate = 0
2017/01/25 01:36:35.772 kid1| 28,3| Acl.cc(290) matches: checked:
http_access#14 = 0
2017/01/25 01:36:35.772 kid1| 28,5| Checklist.cc(397) bannedAction:
Action 'ALLOWED/0' is not banned
2017/01/25 01:36:35.772 kid1| 28,5| Acl.cc(263) matches: checking
http_access#15
2017/01/25 01:36:35.772 kid1| 28,5| Acl.cc(263) matches: checking
windowsupdate
2017/01/25 01:36:35.773 kid1| 28,3| DomainData.cc(110) match:
aclMatchDomainList: checking 'repository.certum.pl'
2017/01/25 01:36:35.773 kid1| 28,3| DomainData.cc(115) match:
aclMatchDomainList: 'repository.certum.pl' NOT found
2017/01/25 01:36:35.773 kid1| 28,3| Acl.cc(290) matches: checked:
windowsupdate = 0
2017/01/25 01:36:35.773 kid1| 28,3| Acl.cc(290) matches: checked:
http_access#15 = 0
2017/01/25 01:36:35.773 kid1| 28,5| Checklist.cc(397) bannedAction:
Action 'ALLOWED/0' is not banned
2017/01/25 01:36:35.773 kid1| 28,5| Acl.cc(263) matches: checking
http_access#16
2017/01/25 01:36:35.773 kid1| 28,5| Acl.cc(263) matches: checking localnet
2017/01/25 01:36:35.773 kid1| 28,9| Ip.cc(96) aclIpAddrNetworkCompare:
aclIpAddrNetworkCompare: compare:
[ffff:ffff:ffff:ffff:ffff:ffff:ffff:ffff]/[ffff:ffff:ffff:ffff:ffff:ffff:ffc0:0]
([ffff:ffff:ffff:ffff:ffff:ffff:ffc0:0])  vs
100.64.0.0-[::]/[ffff:ffff:ffff:ffff:ffff:ffff:ffc0:0]
2017/01/25 01:36:35.773 kid1| 28,9| Ip.cc(96) aclIpAddrNetworkCompare:
aclIpAddrNetworkCompare: compare:
[ffff:ffff:ffff:ffff:ffff:ffff:ffff:ffff]/[ffff:ffff:ffff:ffff:ffff:ffff:fff0:0]
([ffff:ffff:ffff:ffff:ffff:ffff:fff0:0])  vs
172.16.0.0-[::]/[ffff:ffff:ffff:ffff:ffff:ffff:fff0:0]
2017/01/25 01:36:35.773 kid1| 28,9| Ip.cc(96) aclIpAddrNetworkCompare:
aclIpAddrNetworkCompare: compare:
[ffff:ffff:ffff:ffff:ffff:ffff:ffff:ffff]/[ffff:ffff:ffff:ffff:ffff:ffff:ffff:0]
([ffff:ffff:ffff:ffff:ffff:ffff:ffff:0])  vs
192.168.0.0-[::]/[ffff:ffff:ffff:ffff:ffff:ffff:ffff:0]
2017/01/25 01:36:35.773 kid1| 28,3| Ip.cc(540) match: aclIpMatchIp:
'[ffff:ffff:ffff:ffff:ffff:ffff:ffff:ffff]' NOT found
2017/01/25 01:36:35.773 kid1| 28,3| Acl.cc(290) matches: checked:
localnet = 0
2017/01/25 01:36:35.773 kid1| 28,3| Acl.cc(290) matches: checked:
http_access#16 = 0
2017/01/25 01:36:35.773 kid1| 28,5| Checklist.cc(397) bannedAction:
Action 'ALLOWED/0' is not banned
2017/01/25 01:36:35.773 kid1| 28,5| Acl.cc(263) matches: checking
http_access#17
2017/01/25 01:36:35.773 kid1| 28,5| Acl.cc(263) matches: checking localhost
2017/01/25 01:36:35.773 kid1| 28,9| Ip.cc(96) aclIpAddrNetworkCompare:
aclIpAddrNetworkCompare: compare:
[ffff:ffff:ffff:ffff:ffff:ffff:ffff:ffff]/[ffff:ffff:ffff:ffff:ffff:ffff:ffff:ffff]
([ffff:ffff:ffff:ffff:ffff:ffff:ffff:ffff])  vs
127.0.0.1-[::]/[ffff:ffff:ffff:ffff:ffff:ffff:ffff:ffff]
2017/01/25 01:36:35.773 kid1| 28,3| Ip.cc(540) match: aclIpMatchIp:
'[ffff:ffff:ffff:ffff:ffff:ffff:ffff:ffff]' NOT found
2017/01/25 01:36:35.773 kid1| 28,3| Acl.cc(290) matches: checked:
localhost = 0
2017/01/25 01:36:35.773 kid1| 28,3| Acl.cc(290) matches: checked:
http_access#17 = 0
2017/01/25 01:36:35.773 kid1| 28,5| Checklist.cc(397) bannedAction:
Action 'DENIED/0' is not banned
2017/01/25 01:36:35.773 kid1| 28,5| Acl.cc(263) matches: checking
http_access#18
2017/01/25 01:36:35.773 kid1| 28,5| Acl.cc(263) matches: checking all
2017/01/25 01:36:35.773 kid1| 28,9| Ip.cc(96) aclIpAddrNetworkCompare:
aclIpAddrNetworkCompare: compare:
[ffff:ffff:ffff:ffff:ffff:ffff:ffff:ffff]/[::] ([::])  vs [::]-[::]/[::]
2017/01/25 01:36:35.773 kid1| 28,3| Ip.cc(540) match: aclIpMatchIp:
'[ffff:ffff:ffff:ffff:ffff:ffff:ffff:ffff]' found
2017/01/25 01:36:35.773 kid1| 28,3| Acl.cc(290) matches: checked: all = 1
2017/01/25 01:36:35.773 kid1| 28,3| Acl.cc(290) matches: checked:
http_access#18 = 1
2017/01/25 01:36:35.773 kid1| 28,3| Acl.cc(290) matches: checked:
http_access = 1
2017/01/25 01:36:35.773 kid1| 28,3| Checklist.cc(63) markFinished:
0x4b781938 answer DENIED for match
2017/01/25 01:36:35.773 kid1| 28,3| Checklist.cc(163) checkCallback:
ACLChecklist::checkCallback: 0x4b781938 answer=DENIED

It seems like bug.

25.01.2017 1:10, Alex Rousskov ?????:
> On 01/24/2017 11:33 AM, Yuri Voinov wrote:
>
>>> 1485279884.648      0 - TCP_DENIED/403 3574 GET
>>> http://repository.certum.pl/ca.cer - HIER_NONE/- text/html;charset=utf-8
>
>> http_access deny !Safe_ports
> Probably does not match -- 80 is a safe port.
>
>
>> # Instant messengers include
>> include "/usr/local/squid/etc/acl.im.include"
> I am guessing these do not match or are irrelevant.
>
>
>> # Deny CONNECT to other than SSL ports
>> http_access deny CONNECT !SSL_ports
> Does not match. This is a GET request.
>
>
>> # Only allow cachemgr access from localhost
>> http_access allow localhost manager
>> http_access deny manager
> Probably do not match. This is not a cache manager request although I
> have not checked how Squid identifies those exactly.
>
>
>> http_access deny to_localhost
> Does not match. The destination is not localhost.
>
>
>> # Allow purge from localhost
>> http_access allow PURGE localhost
>> http_access deny PURGE
> Do not match. This is a GET request, not PURGE.
>
>
>> # Block torrent files
>> acl TorrentFiles rep_mime_type mime-type application/x-bittorrent
>> http_reply_access deny TorrentFiles
> Does not match. There was no response [with an application/x-bittorrent
> MIME type].
>
>
>> # Windows updates rules
>> http_access allow CONNECT wuCONNECT localnet
>> http_access allow CONNECT wuCONNECT localhost
> Do not match. This is a GET request, not CONNECT.
>
>
>> http_access allow windowsupdate localnet
>> http_access allow windowsupdate localhost
> Probably do not match. The internal transaction is not associated with a
> to-Squid connection coming from localnet or localhost.
>
>
>> # Rule allowing access from local networks
>> http_access allow localnet
>> http_access allow localhost
> Probably do not match. The internal transaction is not associated with a
> to-Squid connection coming from localnet or localhost.
>
>
>> # And finally deny all other access to this proxy
>> http_access deny all
> Matches!
>
>
>> I have no idea, what can block access.
> That much was clear from the time you asked the question. I bet your
> last http_access rule that denies all other connection matches, but I
> would still ask Squid. Squid knows why it blocks (or does not allow)
> access. There are several ways to ask Squid, including increasing
> debugging verbosity when reproducing the problem, adding the matching
> ACL to the error message, using custom error messages for different
> http_access deny lines, etc.
>
> These methods are not easy, pleasant, quick, or human-friendly,
> unfortunately, but you are a very capable sysadmin with more than enough
> Squid knowledge to find the blocking directive/ACL, especially for a
> problem that can be isolated to two HTTP transactions.
>
> Once we know what directive/ACL blocks, we may be able to figure out a
> workaround, propose a bug fix, etc. For example, if my guess is correct
> -- the "deny all" rule has matched -- then you would need to add a rule
> to allow internal requests, including the ones that fetch those missing
> certificates.
>
>
> HTH,
>
> Alex.
>
>
>> 25.01.2017 0:27, Alex Rousskov ?????:
>>> On 01/24/2017 11:19 AM, Yuri Voinov wrote:
>>>
>>>> It is downloads directly via proxy from localhost:
>>>> As I understand, downloader also access via localhost, right? 
>>> This is incorrect. Downloader does not have a concept of an HTTP client
>>> which sends the request to Squid so "via localhost" or "via any client
>>> source address" does not apply to Downloader transactions. In other
>>> words, there is no client [source address] for Downloader requests.
>>>
>>> Unfortunately, I do not know exactly what effect that lack of info has
>>> on what ACLs (in part because there are too many of them and because
>>> lack of info is often treated inconsistently by various ACLs). Thus, I
>>> continue to recommend finding out which directive/ACL denied Downloader
>>> access as the first step.
>>>
>>> Alex.
>>>
>>>
>>>> 25.01.2017 0:16, Alex Rousskov ?????:
>>>>> On 01/24/2017 10:48 AM, Yuri Voinov wrote:
>>>>>
>>>>>> It seems 4.0.17 tries to download certs but gives deny somewhere.
>>>>>> However, same URL with wget via same proxy works
>>>>>> Why?
>>>>> Most likely, your http_access or similar rules deny internal download
>>>>> transactions but allow external ones. This is possible, for example, if
>>>>> your access rules use client information. Internal transactions (ESI,
>>>>> missing certificate fetching, Cache Digests, etc.) do not have an
>>>>> associated client.
>>>>>
>>>>> The standard denial troubleshooting procedure applies here: Start with
>>>>> finding out which directive/ACL denies access. I am _not_ implying that
>>>>> this is easy to do.

-------------- next part --------------
A non-text attachment was scrubbed...
Name: 0x613DEC46.asc
Type: application/pgp-keys
Size: 2437 bytes
Desc: not available
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20170125/8ba51f13/attachment.key>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: signature.asc
Type: application/pgp-signature
Size: 473 bytes
Desc: OpenPGP digital signature
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20170125/8ba51f13/attachment.sig>

From yvoinov at gmail.com  Tue Jan 24 20:09:08 2017
From: yvoinov at gmail.com (Yuri Voinov)
Date: Wed, 25 Jan 2017 02:09:08 +0600
Subject: [squid-users] Squid 4.x: Intermediate certificates downloader
In-Reply-To: <d57fdcf5-30b4-c258-f506-630becbdad79@gmail.com>
References: <4198a8d7-07c7-8c74-80ac-eb75ca61e62c@gmail.com>
 <35afc9c9-dd43-fb17-d0d6-c90866fd5c28@measurement-factory.com>
 <52b0aec8-f61d-1bec-de18-1c3be06494a0@urlfilterdb.com>
 <81cf3b07-28a0-39b6-2690-a6cc320d653f@treenet.co.nz>
 <02aff9e7-c248-88f4-4a24-75f68448c378@gmail.com>
 <4df5b398-9c23-87ad-81c4-4d34c16a231c@measurement-factory.com>
 <42af5a38-f4f9-19a5-7397-b3c7084ce1a4@gmail.com>
 <e9b275f4-f9b1-7eda-e102-420bb9eda7ad@measurement-factory.com>
 <6d9fb82c-a83d-1215-7df7-04259c9a3997@gmail.com>
 <42f09caf-ee99-0f5e-b187-7468b736a04d@measurement-factory.com>
 <d57fdcf5-30b4-c258-f506-630becbdad79@gmail.com>
Message-ID: <2a246077-c8af-d916-5afa-3c1f8c4cb30b@gmail.com>

This is mentioned debug for this transaction.

I see no anomalies. Just DENIED finally.


25.01.2017 1:45, Yuri Voinov ?????:
> Under detailed ACL debug got this transaction:
>
> 2017/01/25 01:36:35.772 kid1| 28,3| DomainData.cc(110) match:
> aclMatchDomainList: checking 'repository.certum.pl'
> 2017/01/25 01:36:35.772 kid1| 28,3| DomainData.cc(115) match:
> aclMatchDomainList: 'repository.certum.pl' NOT found
> 2017/01/25 01:36:35.772 kid1| 28,3| Acl.cc(290) matches: checked:
> block_tld = 0
> 2017/01/25 01:36:35.772 kid1| 28,3| Acl.cc(290) matches: checked:
> http_access#11 = 0
> 2017/01/25 01:36:35.772 kid1| 28,5| Checklist.cc(397) bannedAction:
> Action 'ALLOWED/0' is not banned
> 2017/01/25 01:36:35.772 kid1| 28,5| Acl.cc(263) matches: checking
> http_access#12
> 2017/01/25 01:36:35.772 kid1| 28,5| Acl.cc(263) matches: checking CONNECT
> 2017/01/25 01:36:35.772 kid1| 28,3| Acl.cc(290) matches: checked:
> CONNECT = 0
> 2017/01/25 01:36:35.772 kid1| 28,3| Acl.cc(290) matches: checked:
> http_access#12 = 0
> 2017/01/25 01:36:35.772 kid1| 28,5| Checklist.cc(397) bannedAction:
> Action 'ALLOWED/0' is not banned
> 2017/01/25 01:36:35.772 kid1| 28,5| Acl.cc(263) matches: checking
> http_access#13
> 2017/01/25 01:36:35.772 kid1| 28,5| Acl.cc(263) matches: checking CONNECT
> 2017/01/25 01:36:35.772 kid1| 28,3| Acl.cc(290) matches: checked:
> CONNECT = 0
> 2017/01/25 01:36:35.772 kid1| 28,3| Acl.cc(290) matches: checked:
> http_access#13 = 0
> 2017/01/25 01:36:35.772 kid1| 28,5| Checklist.cc(397) bannedAction:
> Action 'ALLOWED/0' is not banned
> 2017/01/25 01:36:35.772 kid1| 28,5| Acl.cc(263) matches: checking
> http_access#14
> 2017/01/25 01:36:35.772 kid1| 28,5| Acl.cc(263) matches: checking
> windowsupdate
> 2017/01/25 01:36:35.772 kid1| 28,3| DomainData.cc(110) match:
> aclMatchDomainList: checking 'repository.certum.pl'
> 2017/01/25 01:36:35.772 kid1| 28,3| DomainData.cc(115) match:
> aclMatchDomainList: 'repository.certum.pl' NOT found
> 2017/01/25 01:36:35.772 kid1| 28,3| Acl.cc(290) matches: checked:
> windowsupdate = 0
> 2017/01/25 01:36:35.772 kid1| 28,3| Acl.cc(290) matches: checked:
> http_access#14 = 0
> 2017/01/25 01:36:35.772 kid1| 28,5| Checklist.cc(397) bannedAction:
> Action 'ALLOWED/0' is not banned
> 2017/01/25 01:36:35.772 kid1| 28,5| Acl.cc(263) matches: checking
> http_access#15
> 2017/01/25 01:36:35.772 kid1| 28,5| Acl.cc(263) matches: checking
> windowsupdate
> 2017/01/25 01:36:35.773 kid1| 28,3| DomainData.cc(110) match:
> aclMatchDomainList: checking 'repository.certum.pl'
> 2017/01/25 01:36:35.773 kid1| 28,3| DomainData.cc(115) match:
> aclMatchDomainList: 'repository.certum.pl' NOT found
> 2017/01/25 01:36:35.773 kid1| 28,3| Acl.cc(290) matches: checked:
> windowsupdate = 0
> 2017/01/25 01:36:35.773 kid1| 28,3| Acl.cc(290) matches: checked:
> http_access#15 = 0
> 2017/01/25 01:36:35.773 kid1| 28,5| Checklist.cc(397) bannedAction:
> Action 'ALLOWED/0' is not banned
> 2017/01/25 01:36:35.773 kid1| 28,5| Acl.cc(263) matches: checking
> http_access#16
> 2017/01/25 01:36:35.773 kid1| 28,5| Acl.cc(263) matches: checking localnet
> 2017/01/25 01:36:35.773 kid1| 28,9| Ip.cc(96) aclIpAddrNetworkCompare:
> aclIpAddrNetworkCompare: compare:
> [ffff:ffff:ffff:ffff:ffff:ffff:ffff:ffff]/[ffff:ffff:ffff:ffff:ffff:ffff:ffc0:0]
> ([ffff:ffff:ffff:ffff:ffff:ffff:ffc0:0])  vs
> 100.64.0.0-[::]/[ffff:ffff:ffff:ffff:ffff:ffff:ffc0:0]
> 2017/01/25 01:36:35.773 kid1| 28,9| Ip.cc(96) aclIpAddrNetworkCompare:
> aclIpAddrNetworkCompare: compare:
> [ffff:ffff:ffff:ffff:ffff:ffff:ffff:ffff]/[ffff:ffff:ffff:ffff:ffff:ffff:fff0:0]
> ([ffff:ffff:ffff:ffff:ffff:ffff:fff0:0])  vs
> 172.16.0.0-[::]/[ffff:ffff:ffff:ffff:ffff:ffff:fff0:0]
> 2017/01/25 01:36:35.773 kid1| 28,9| Ip.cc(96) aclIpAddrNetworkCompare:
> aclIpAddrNetworkCompare: compare:
> [ffff:ffff:ffff:ffff:ffff:ffff:ffff:ffff]/[ffff:ffff:ffff:ffff:ffff:ffff:ffff:0]
> ([ffff:ffff:ffff:ffff:ffff:ffff:ffff:0])  vs
> 192.168.0.0-[::]/[ffff:ffff:ffff:ffff:ffff:ffff:ffff:0]
> 2017/01/25 01:36:35.773 kid1| 28,3| Ip.cc(540) match: aclIpMatchIp:
> '[ffff:ffff:ffff:ffff:ffff:ffff:ffff:ffff]' NOT found
> 2017/01/25 01:36:35.773 kid1| 28,3| Acl.cc(290) matches: checked:
> localnet = 0
> 2017/01/25 01:36:35.773 kid1| 28,3| Acl.cc(290) matches: checked:
> http_access#16 = 0
> 2017/01/25 01:36:35.773 kid1| 28,5| Checklist.cc(397) bannedAction:
> Action 'ALLOWED/0' is not banned
> 2017/01/25 01:36:35.773 kid1| 28,5| Acl.cc(263) matches: checking
> http_access#17
> 2017/01/25 01:36:35.773 kid1| 28,5| Acl.cc(263) matches: checking localhost
> 2017/01/25 01:36:35.773 kid1| 28,9| Ip.cc(96) aclIpAddrNetworkCompare:
> aclIpAddrNetworkCompare: compare:
> [ffff:ffff:ffff:ffff:ffff:ffff:ffff:ffff]/[ffff:ffff:ffff:ffff:ffff:ffff:ffff:ffff]
> ([ffff:ffff:ffff:ffff:ffff:ffff:ffff:ffff])  vs
> 127.0.0.1-[::]/[ffff:ffff:ffff:ffff:ffff:ffff:ffff:ffff]
> 2017/01/25 01:36:35.773 kid1| 28,3| Ip.cc(540) match: aclIpMatchIp:
> '[ffff:ffff:ffff:ffff:ffff:ffff:ffff:ffff]' NOT found
> 2017/01/25 01:36:35.773 kid1| 28,3| Acl.cc(290) matches: checked:
> localhost = 0
> 2017/01/25 01:36:35.773 kid1| 28,3| Acl.cc(290) matches: checked:
> http_access#17 = 0
> 2017/01/25 01:36:35.773 kid1| 28,5| Checklist.cc(397) bannedAction:
> Action 'DENIED/0' is not banned
> 2017/01/25 01:36:35.773 kid1| 28,5| Acl.cc(263) matches: checking
> http_access#18
> 2017/01/25 01:36:35.773 kid1| 28,5| Acl.cc(263) matches: checking all
> 2017/01/25 01:36:35.773 kid1| 28,9| Ip.cc(96) aclIpAddrNetworkCompare:
> aclIpAddrNetworkCompare: compare:
> [ffff:ffff:ffff:ffff:ffff:ffff:ffff:ffff]/[::] ([::])  vs [::]-[::]/[::]
> 2017/01/25 01:36:35.773 kid1| 28,3| Ip.cc(540) match: aclIpMatchIp:
> '[ffff:ffff:ffff:ffff:ffff:ffff:ffff:ffff]' found
> 2017/01/25 01:36:35.773 kid1| 28,3| Acl.cc(290) matches: checked: all = 1
> 2017/01/25 01:36:35.773 kid1| 28,3| Acl.cc(290) matches: checked:
> http_access#18 = 1
> 2017/01/25 01:36:35.773 kid1| 28,3| Acl.cc(290) matches: checked:
> http_access = 1
> 2017/01/25 01:36:35.773 kid1| 28,3| Checklist.cc(63) markFinished:
> 0x4b781938 answer DENIED for match
> 2017/01/25 01:36:35.773 kid1| 28,3| Checklist.cc(163) checkCallback:
> ACLChecklist::checkCallback: 0x4b781938 answer=DENIED
>
> It seems like bug.
>
> 25.01.2017 1:10, Alex Rousskov ?????:
>> On 01/24/2017 11:33 AM, Yuri Voinov wrote:
>>
>>>> 1485279884.648      0 - TCP_DENIED/403 3574 GET
>>>> http://repository.certum.pl/ca.cer - HIER_NONE/- text/html;charset=utf-8
>>> http_access deny !Safe_ports
>> Probably does not match -- 80 is a safe port.
>>
>>
>>> # Instant messengers include
>>> include "/usr/local/squid/etc/acl.im.include"
>> I am guessing these do not match or are irrelevant.
>>
>>
>>> # Deny CONNECT to other than SSL ports
>>> http_access deny CONNECT !SSL_ports
>> Does not match. This is a GET request.
>>
>>
>>> # Only allow cachemgr access from localhost
>>> http_access allow localhost manager
>>> http_access deny manager
>> Probably do not match. This is not a cache manager request although I
>> have not checked how Squid identifies those exactly.
>>
>>
>>> http_access deny to_localhost
>> Does not match. The destination is not localhost.
>>
>>
>>> # Allow purge from localhost
>>> http_access allow PURGE localhost
>>> http_access deny PURGE
>> Do not match. This is a GET request, not PURGE.
>>
>>
>>> # Block torrent files
>>> acl TorrentFiles rep_mime_type mime-type application/x-bittorrent
>>> http_reply_access deny TorrentFiles
>> Does not match. There was no response [with an application/x-bittorrent
>> MIME type].
>>
>>
>>> # Windows updates rules
>>> http_access allow CONNECT wuCONNECT localnet
>>> http_access allow CONNECT wuCONNECT localhost
>> Do not match. This is a GET request, not CONNECT.
>>
>>
>>> http_access allow windowsupdate localnet
>>> http_access allow windowsupdate localhost
>> Probably do not match. The internal transaction is not associated with a
>> to-Squid connection coming from localnet or localhost.
>>
>>
>>> # Rule allowing access from local networks
>>> http_access allow localnet
>>> http_access allow localhost
>> Probably do not match. The internal transaction is not associated with a
>> to-Squid connection coming from localnet or localhost.
>>
>>
>>> # And finally deny all other access to this proxy
>>> http_access deny all
>> Matches!
>>
>>
>>> I have no idea, what can block access.
>> That much was clear from the time you asked the question. I bet your
>> last http_access rule that denies all other connection matches, but I
>> would still ask Squid. Squid knows why it blocks (or does not allow)
>> access. There are several ways to ask Squid, including increasing
>> debugging verbosity when reproducing the problem, adding the matching
>> ACL to the error message, using custom error messages for different
>> http_access deny lines, etc.
>>
>> These methods are not easy, pleasant, quick, or human-friendly,
>> unfortunately, but you are a very capable sysadmin with more than enough
>> Squid knowledge to find the blocking directive/ACL, especially for a
>> problem that can be isolated to two HTTP transactions.
>>
>> Once we know what directive/ACL blocks, we may be able to figure out a
>> workaround, propose a bug fix, etc. For example, if my guess is correct
>> -- the "deny all" rule has matched -- then you would need to add a rule
>> to allow internal requests, including the ones that fetch those missing
>> certificates.
>>
>>
>> HTH,
>>
>> Alex.
>>
>>
>>> 25.01.2017 0:27, Alex Rousskov ?????:
>>>> On 01/24/2017 11:19 AM, Yuri Voinov wrote:
>>>>
>>>>> It is downloads directly via proxy from localhost:
>>>>> As I understand, downloader also access via localhost, right? 
>>>> This is incorrect. Downloader does not have a concept of an HTTP client
>>>> which sends the request to Squid so "via localhost" or "via any client
>>>> source address" does not apply to Downloader transactions. In other
>>>> words, there is no client [source address] for Downloader requests.
>>>>
>>>> Unfortunately, I do not know exactly what effect that lack of info has
>>>> on what ACLs (in part because there are too many of them and because
>>>> lack of info is often treated inconsistently by various ACLs). Thus, I
>>>> continue to recommend finding out which directive/ACL denied Downloader
>>>> access as the first step.
>>>>
>>>> Alex.
>>>>
>>>>
>>>>> 25.01.2017 0:16, Alex Rousskov ?????:
>>>>>> On 01/24/2017 10:48 AM, Yuri Voinov wrote:
>>>>>>
>>>>>>> It seems 4.0.17 tries to download certs but gives deny somewhere.
>>>>>>> However, same URL with wget via same proxy works
>>>>>>> Why?
>>>>>> Most likely, your http_access or similar rules deny internal download
>>>>>> transactions but allow external ones. This is possible, for example, if
>>>>>> your access rules use client information. Internal transactions (ESI,
>>>>>> missing certificate fetching, Cache Digests, etc.) do not have an
>>>>>> associated client.
>>>>>>
>>>>>> The standard denial troubleshooting procedure applies here: Start with
>>>>>> finding out which directive/ACL denies access. I am _not_ implying that
>>>>>> this is easy to do.

-------------- next part --------------
A non-text attachment was scrubbed...
Name: cache.zip
Type: application/x-zip-compressed
Size: 18074 bytes
Desc: not available
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20170125/50f3679a/attachment.bin>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: 0x613DEC46.asc
Type: application/pgp-keys
Size: 2437 bytes
Desc: not available
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20170125/50f3679a/attachment.key>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: signature.asc
Type: application/pgp-signature
Size: 473 bytes
Desc: OpenPGP digital signature
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20170125/50f3679a/attachment.sig>

From yvoinov at gmail.com  Tue Jan 24 20:40:44 2017
From: yvoinov at gmail.com (Yuri Voinov)
Date: Wed, 25 Jan 2017 02:40:44 +0600
Subject: [squid-users] Squid 4.x: Intermediate certificates downloader
In-Reply-To: <2a246077-c8af-d916-5afa-3c1f8c4cb30b@gmail.com>
References: <4198a8d7-07c7-8c74-80ac-eb75ca61e62c@gmail.com>
 <35afc9c9-dd43-fb17-d0d6-c90866fd5c28@measurement-factory.com>
 <52b0aec8-f61d-1bec-de18-1c3be06494a0@urlfilterdb.com>
 <81cf3b07-28a0-39b6-2690-a6cc320d653f@treenet.co.nz>
 <02aff9e7-c248-88f4-4a24-75f68448c378@gmail.com>
 <4df5b398-9c23-87ad-81c4-4d34c16a231c@measurement-factory.com>
 <42af5a38-f4f9-19a5-7397-b3c7084ce1a4@gmail.com>
 <e9b275f4-f9b1-7eda-e102-420bb9eda7ad@measurement-factory.com>
 <6d9fb82c-a83d-1215-7df7-04259c9a3997@gmail.com>
 <42f09caf-ee99-0f5e-b187-7468b736a04d@measurement-factory.com>
 <d57fdcf5-30b4-c258-f506-630becbdad79@gmail.com>
 <2a246077-c8af-d916-5afa-3c1f8c4cb30b@gmail.com>
Message-ID: <e6dd3ea8-9229-f6d3-e61d-612b65ad0629@gmail.com>

On my setup it is easy to reproduce.

It is enough to execute with wget:

wget -S https://yandex.com/company/

access.log immediately shows

0 - TCP_DENIED/403 3574 GET http://repository.certum.pl/ca.cer -
HIER_NONE/- text/html;charset=utf-8

before request to Yandex destination.

However it executes:

root @ khorne /patch # wget -S https://yandex.com/company/
--2017-01-25 02:37:52--  https://yandex.com/company/
Connecting to 127.0.0.1:3128... connected.
Proxy request sent, awaiting response...
  HTTP/1.1 200 OK
  Date: Tue, 24 Jan 2017 20:37:54 GMT
  Content-Type: text/html; charset="UTF-8"
  Set-Cookie: yandexuid=15112434331485290274; Domain=.yandex.com; Path=/
  Content-Security-Policy: default-src 'none'; frame-src 'self'
yastatic.net yandex.st music.yandex.ru download.yandex.ru
static.video.yandex.ru video.yandex.ru player.vimeo.com www.youtube.com
*.cdn.yandex.net; script-src 'unsafe-eval' 'unsafe-inline'
clck.yandex.ru pass.yandex.com yastatic.net mc.yandex.ru
api-maps.yandex.ru social.yandex.com
'nonce-728706f8-8c12-4f25-a00f-27d1e8c36f6f'; style-src 'unsafe-inline'
yastatic.net; connect-src 'self' yandex.st mail.yandex.com mc.yandex.ru
mc.yandex.com; font-src yastatic.net; img-src 'self' data:
avatars.yandex.net avatars.mds.yandex.net avatars.mdst.yandex.net
http://avatars.mds.yandex.net jing.yandex-team.ru download.yandex.ru
yandex.st mc.yandex.ru yastatic.net www.tns-counter.ru
yandexgacom.hit.gemius.pl *.cdn.yandex.net api-maps.yandex.ru
static-maps.yandex.ru *.maps.yandex.net i.ytimg.com company.yandex.com
yandex.com http://img-fotki.yandex.ru img-fotki.yandex.ru; media-src
*.cdn.yandex.net
  X-Content-Security-Policy: default-src 'none'; frame-src 'self'
yastatic.net yandex.st music.yandex.ru download.yandex.ru
static.video.yandex.ru video.yandex.ru player.vimeo.com www.youtube.com
*.cdn.yandex.net; script-src 'unsafe-eval' 'unsafe-inline'
clck.yandex.ru pass.yandex.com yastatic.net mc.yandex.ru
api-maps.yandex.ru social.yandex.com
'nonce-728706f8-8c12-4f25-a00f-27d1e8c36f6f'; style-src 'unsafe-inline'
yastatic.net; connect-src 'self' yandex.st mail.yandex.com mc.yandex.ru
mc.yandex.com; font-src yastatic.net; img-src 'self' data:
avatars.yandex.net avatars.mds.yandex.net avatars.mdst.yandex.net
http://avatars.mds.yandex.net jing.yandex-team.ru download.yandex.ru
yandex.st mc.yandex.ru yastatic.net www.tns-counter.ru
yandexgacom.hit.gemius.pl *.cdn.yandex.net api-maps.yandex.ru
static-maps.yandex.ru *.maps.yandex.net i.ytimg.com company.yandex.com
yandex.com http://img-fotki.yandex.ru img-fotki.yandex.ru; media-src
*.cdn.yandex.net
  X-WebKit-CSP: default-src 'none'; frame-src 'self' yastatic.net
yandex.st music.yandex.ru download.yandex.ru static.video.yandex.ru
video.yandex.ru player.vimeo.com www.youtube.com *.cdn.yandex.net;
script-src 'unsafe-eval' 'unsafe-inline' clck.yandex.ru pass.yandex.com
yastatic.net mc.yandex.ru api-maps.yandex.ru social.yandex.com
'nonce-728706f8-8c12-4f25-a00f-27d1e8c36f6f'; style-src 'unsafe-inline'
yastatic.net; connect-src 'self' yandex.st mail.yandex.com mc.yandex.ru
mc.yandex.com; font-src yastatic.net; img-src 'self' data:
avatars.yandex.net avatars.mds.yandex.net avatars.mdst.yandex.net
http://avatars.mds.yandex.net jing.yandex-team.ru download.yandex.ru
yandex.st mc.yandex.ru yastatic.net www.tns-counter.ru
yandexgacom.hit.gemius.pl *.cdn.yandex.net api-maps.yandex.ru
static-maps.yandex.ru *.maps.yandex.net i.ytimg.com company.yandex.com
yandex.com http://img-fotki.yandex.ru img-fotki.yandex.ru; media-src
*.cdn.yandex.net
  Content-Encoding: gzip
  X-XSS-Protection: 1; mode=block
  X-Content-Type-Options: nosniff
  Strict-Transport-Security: max-age=0; includeSubDomains
  X-Cache: MISS from khorne
  X-Cache-Lookup: HIT from khorne:3128
  Transfer-Encoding: chunked
  Connection: keep-alive
Length: unspecified [text/html]
Saving to: 'index.html'

index.html              [ <=>               ]   3.60K  --.-KB/s    in
0s     

2017-01-25 02:37:54 (7.44 MB/s) - 'index.html' saved [3688]

because intermediate certificates file is exists and configured.

25.01.2017 2:09, Yuri Voinov ?????:
> This is mentioned debug for this transaction.
>
> I see no anomalies. Just DENIED finally.
>
>
> 25.01.2017 1:45, Yuri Voinov ?????:
>> Under detailed ACL debug got this transaction:
>>
>> 2017/01/25 01:36:35.772 kid1| 28,3| DomainData.cc(110) match:
>> aclMatchDomainList: checking 'repository.certum.pl'
>> 2017/01/25 01:36:35.772 kid1| 28,3| DomainData.cc(115) match:
>> aclMatchDomainList: 'repository.certum.pl' NOT found
>> 2017/01/25 01:36:35.772 kid1| 28,3| Acl.cc(290) matches: checked:
>> block_tld = 0
>> 2017/01/25 01:36:35.772 kid1| 28,3| Acl.cc(290) matches: checked:
>> http_access#11 = 0
>> 2017/01/25 01:36:35.772 kid1| 28,5| Checklist.cc(397) bannedAction:
>> Action 'ALLOWED/0' is not banned
>> 2017/01/25 01:36:35.772 kid1| 28,5| Acl.cc(263) matches: checking
>> http_access#12
>> 2017/01/25 01:36:35.772 kid1| 28,5| Acl.cc(263) matches: checking CONNECT
>> 2017/01/25 01:36:35.772 kid1| 28,3| Acl.cc(290) matches: checked:
>> CONNECT = 0
>> 2017/01/25 01:36:35.772 kid1| 28,3| Acl.cc(290) matches: checked:
>> http_access#12 = 0
>> 2017/01/25 01:36:35.772 kid1| 28,5| Checklist.cc(397) bannedAction:
>> Action 'ALLOWED/0' is not banned
>> 2017/01/25 01:36:35.772 kid1| 28,5| Acl.cc(263) matches: checking
>> http_access#13
>> 2017/01/25 01:36:35.772 kid1| 28,5| Acl.cc(263) matches: checking CONNECT
>> 2017/01/25 01:36:35.772 kid1| 28,3| Acl.cc(290) matches: checked:
>> CONNECT = 0
>> 2017/01/25 01:36:35.772 kid1| 28,3| Acl.cc(290) matches: checked:
>> http_access#13 = 0
>> 2017/01/25 01:36:35.772 kid1| 28,5| Checklist.cc(397) bannedAction:
>> Action 'ALLOWED/0' is not banned
>> 2017/01/25 01:36:35.772 kid1| 28,5| Acl.cc(263) matches: checking
>> http_access#14
>> 2017/01/25 01:36:35.772 kid1| 28,5| Acl.cc(263) matches: checking
>> windowsupdate
>> 2017/01/25 01:36:35.772 kid1| 28,3| DomainData.cc(110) match:
>> aclMatchDomainList: checking 'repository.certum.pl'
>> 2017/01/25 01:36:35.772 kid1| 28,3| DomainData.cc(115) match:
>> aclMatchDomainList: 'repository.certum.pl' NOT found
>> 2017/01/25 01:36:35.772 kid1| 28,3| Acl.cc(290) matches: checked:
>> windowsupdate = 0
>> 2017/01/25 01:36:35.772 kid1| 28,3| Acl.cc(290) matches: checked:
>> http_access#14 = 0
>> 2017/01/25 01:36:35.772 kid1| 28,5| Checklist.cc(397) bannedAction:
>> Action 'ALLOWED/0' is not banned
>> 2017/01/25 01:36:35.772 kid1| 28,5| Acl.cc(263) matches: checking
>> http_access#15
>> 2017/01/25 01:36:35.772 kid1| 28,5| Acl.cc(263) matches: checking
>> windowsupdate
>> 2017/01/25 01:36:35.773 kid1| 28,3| DomainData.cc(110) match:
>> aclMatchDomainList: checking 'repository.certum.pl'
>> 2017/01/25 01:36:35.773 kid1| 28,3| DomainData.cc(115) match:
>> aclMatchDomainList: 'repository.certum.pl' NOT found
>> 2017/01/25 01:36:35.773 kid1| 28,3| Acl.cc(290) matches: checked:
>> windowsupdate = 0
>> 2017/01/25 01:36:35.773 kid1| 28,3| Acl.cc(290) matches: checked:
>> http_access#15 = 0
>> 2017/01/25 01:36:35.773 kid1| 28,5| Checklist.cc(397) bannedAction:
>> Action 'ALLOWED/0' is not banned
>> 2017/01/25 01:36:35.773 kid1| 28,5| Acl.cc(263) matches: checking
>> http_access#16
>> 2017/01/25 01:36:35.773 kid1| 28,5| Acl.cc(263) matches: checking localnet
>> 2017/01/25 01:36:35.773 kid1| 28,9| Ip.cc(96) aclIpAddrNetworkCompare:
>> aclIpAddrNetworkCompare: compare:
>> [ffff:ffff:ffff:ffff:ffff:ffff:ffff:ffff]/[ffff:ffff:ffff:ffff:ffff:ffff:ffc0:0]
>> ([ffff:ffff:ffff:ffff:ffff:ffff:ffc0:0])  vs
>> 100.64.0.0-[::]/[ffff:ffff:ffff:ffff:ffff:ffff:ffc0:0]
>> 2017/01/25 01:36:35.773 kid1| 28,9| Ip.cc(96) aclIpAddrNetworkCompare:
>> aclIpAddrNetworkCompare: compare:
>> [ffff:ffff:ffff:ffff:ffff:ffff:ffff:ffff]/[ffff:ffff:ffff:ffff:ffff:ffff:fff0:0]
>> ([ffff:ffff:ffff:ffff:ffff:ffff:fff0:0])  vs
>> 172.16.0.0-[::]/[ffff:ffff:ffff:ffff:ffff:ffff:fff0:0]
>> 2017/01/25 01:36:35.773 kid1| 28,9| Ip.cc(96) aclIpAddrNetworkCompare:
>> aclIpAddrNetworkCompare: compare:
>> [ffff:ffff:ffff:ffff:ffff:ffff:ffff:ffff]/[ffff:ffff:ffff:ffff:ffff:ffff:ffff:0]
>> ([ffff:ffff:ffff:ffff:ffff:ffff:ffff:0])  vs
>> 192.168.0.0-[::]/[ffff:ffff:ffff:ffff:ffff:ffff:ffff:0]
>> 2017/01/25 01:36:35.773 kid1| 28,3| Ip.cc(540) match: aclIpMatchIp:
>> '[ffff:ffff:ffff:ffff:ffff:ffff:ffff:ffff]' NOT found
>> 2017/01/25 01:36:35.773 kid1| 28,3| Acl.cc(290) matches: checked:
>> localnet = 0
>> 2017/01/25 01:36:35.773 kid1| 28,3| Acl.cc(290) matches: checked:
>> http_access#16 = 0
>> 2017/01/25 01:36:35.773 kid1| 28,5| Checklist.cc(397) bannedAction:
>> Action 'ALLOWED/0' is not banned
>> 2017/01/25 01:36:35.773 kid1| 28,5| Acl.cc(263) matches: checking
>> http_access#17
>> 2017/01/25 01:36:35.773 kid1| 28,5| Acl.cc(263) matches: checking localhost
>> 2017/01/25 01:36:35.773 kid1| 28,9| Ip.cc(96) aclIpAddrNetworkCompare:
>> aclIpAddrNetworkCompare: compare:
>> [ffff:ffff:ffff:ffff:ffff:ffff:ffff:ffff]/[ffff:ffff:ffff:ffff:ffff:ffff:ffff:ffff]
>> ([ffff:ffff:ffff:ffff:ffff:ffff:ffff:ffff])  vs
>> 127.0.0.1-[::]/[ffff:ffff:ffff:ffff:ffff:ffff:ffff:ffff]
>> 2017/01/25 01:36:35.773 kid1| 28,3| Ip.cc(540) match: aclIpMatchIp:
>> '[ffff:ffff:ffff:ffff:ffff:ffff:ffff:ffff]' NOT found
>> 2017/01/25 01:36:35.773 kid1| 28,3| Acl.cc(290) matches: checked:
>> localhost = 0
>> 2017/01/25 01:36:35.773 kid1| 28,3| Acl.cc(290) matches: checked:
>> http_access#17 = 0
>> 2017/01/25 01:36:35.773 kid1| 28,5| Checklist.cc(397) bannedAction:
>> Action 'DENIED/0' is not banned
>> 2017/01/25 01:36:35.773 kid1| 28,5| Acl.cc(263) matches: checking
>> http_access#18
>> 2017/01/25 01:36:35.773 kid1| 28,5| Acl.cc(263) matches: checking all
>> 2017/01/25 01:36:35.773 kid1| 28,9| Ip.cc(96) aclIpAddrNetworkCompare:
>> aclIpAddrNetworkCompare: compare:
>> [ffff:ffff:ffff:ffff:ffff:ffff:ffff:ffff]/[::] ([::])  vs [::]-[::]/[::]
>> 2017/01/25 01:36:35.773 kid1| 28,3| Ip.cc(540) match: aclIpMatchIp:
>> '[ffff:ffff:ffff:ffff:ffff:ffff:ffff:ffff]' found
>> 2017/01/25 01:36:35.773 kid1| 28,3| Acl.cc(290) matches: checked: all = 1
>> 2017/01/25 01:36:35.773 kid1| 28,3| Acl.cc(290) matches: checked:
>> http_access#18 = 1
>> 2017/01/25 01:36:35.773 kid1| 28,3| Acl.cc(290) matches: checked:
>> http_access = 1
>> 2017/01/25 01:36:35.773 kid1| 28,3| Checklist.cc(63) markFinished:
>> 0x4b781938 answer DENIED for match
>> 2017/01/25 01:36:35.773 kid1| 28,3| Checklist.cc(163) checkCallback:
>> ACLChecklist::checkCallback: 0x4b781938 answer=DENIED
>>
>> It seems like bug.
>>
>> 25.01.2017 1:10, Alex Rousskov ?????:
>>> On 01/24/2017 11:33 AM, Yuri Voinov wrote:
>>>
>>>>> 1485279884.648      0 - TCP_DENIED/403 3574 GET
>>>>> http://repository.certum.pl/ca.cer - HIER_NONE/- text/html;charset=utf-8
>>>> http_access deny !Safe_ports
>>> Probably does not match -- 80 is a safe port.
>>>
>>>
>>>> # Instant messengers include
>>>> include "/usr/local/squid/etc/acl.im.include"
>>> I am guessing these do not match or are irrelevant.
>>>
>>>
>>>> # Deny CONNECT to other than SSL ports
>>>> http_access deny CONNECT !SSL_ports
>>> Does not match. This is a GET request.
>>>
>>>
>>>> # Only allow cachemgr access from localhost
>>>> http_access allow localhost manager
>>>> http_access deny manager
>>> Probably do not match. This is not a cache manager request although I
>>> have not checked how Squid identifies those exactly.
>>>
>>>
>>>> http_access deny to_localhost
>>> Does not match. The destination is not localhost.
>>>
>>>
>>>> # Allow purge from localhost
>>>> http_access allow PURGE localhost
>>>> http_access deny PURGE
>>> Do not match. This is a GET request, not PURGE.
>>>
>>>
>>>> # Block torrent files
>>>> acl TorrentFiles rep_mime_type mime-type application/x-bittorrent
>>>> http_reply_access deny TorrentFiles
>>> Does not match. There was no response [with an application/x-bittorrent
>>> MIME type].
>>>
>>>
>>>> # Windows updates rules
>>>> http_access allow CONNECT wuCONNECT localnet
>>>> http_access allow CONNECT wuCONNECT localhost
>>> Do not match. This is a GET request, not CONNECT.
>>>
>>>
>>>> http_access allow windowsupdate localnet
>>>> http_access allow windowsupdate localhost
>>> Probably do not match. The internal transaction is not associated with a
>>> to-Squid connection coming from localnet or localhost.
>>>
>>>
>>>> # Rule allowing access from local networks
>>>> http_access allow localnet
>>>> http_access allow localhost
>>> Probably do not match. The internal transaction is not associated with a
>>> to-Squid connection coming from localnet or localhost.
>>>
>>>
>>>> # And finally deny all other access to this proxy
>>>> http_access deny all
>>> Matches!
>>>
>>>
>>>> I have no idea, what can block access.
>>> That much was clear from the time you asked the question. I bet your
>>> last http_access rule that denies all other connection matches, but I
>>> would still ask Squid. Squid knows why it blocks (or does not allow)
>>> access. There are several ways to ask Squid, including increasing
>>> debugging verbosity when reproducing the problem, adding the matching
>>> ACL to the error message, using custom error messages for different
>>> http_access deny lines, etc.
>>>
>>> These methods are not easy, pleasant, quick, or human-friendly,
>>> unfortunately, but you are a very capable sysadmin with more than enough
>>> Squid knowledge to find the blocking directive/ACL, especially for a
>>> problem that can be isolated to two HTTP transactions.
>>>
>>> Once we know what directive/ACL blocks, we may be able to figure out a
>>> workaround, propose a bug fix, etc. For example, if my guess is correct
>>> -- the "deny all" rule has matched -- then you would need to add a rule
>>> to allow internal requests, including the ones that fetch those missing
>>> certificates.
>>>
>>>
>>> HTH,
>>>
>>> Alex.
>>>
>>>
>>>> 25.01.2017 0:27, Alex Rousskov ?????:
>>>>> On 01/24/2017 11:19 AM, Yuri Voinov wrote:
>>>>>
>>>>>> It is downloads directly via proxy from localhost:
>>>>>> As I understand, downloader also access via localhost, right? 
>>>>> This is incorrect. Downloader does not have a concept of an HTTP client
>>>>> which sends the request to Squid so "via localhost" or "via any client
>>>>> source address" does not apply to Downloader transactions. In other
>>>>> words, there is no client [source address] for Downloader requests.
>>>>>
>>>>> Unfortunately, I do not know exactly what effect that lack of info has
>>>>> on what ACLs (in part because there are too many of them and because
>>>>> lack of info is often treated inconsistently by various ACLs). Thus, I
>>>>> continue to recommend finding out which directive/ACL denied Downloader
>>>>> access as the first step.
>>>>>
>>>>> Alex.
>>>>>
>>>>>
>>>>>> 25.01.2017 0:16, Alex Rousskov ?????:
>>>>>>> On 01/24/2017 10:48 AM, Yuri Voinov wrote:
>>>>>>>
>>>>>>>> It seems 4.0.17 tries to download certs but gives deny somewhere.
>>>>>>>> However, same URL with wget via same proxy works
>>>>>>>> Why?
>>>>>>> Most likely, your http_access or similar rules deny internal download
>>>>>>> transactions but allow external ones. This is possible, for example, if
>>>>>>> your access rules use client information. Internal transactions (ESI,
>>>>>>> missing certificate fetching, Cache Digests, etc.) do not have an
>>>>>>> associated client.
>>>>>>>
>>>>>>> The standard denial troubleshooting procedure applies here: Start with
>>>>>>> finding out which directive/ACL denies access. I am _not_ implying that
>>>>>>> this is easy to do.

-------------- next part --------------
A non-text attachment was scrubbed...
Name: 0x613DEC46.asc
Type: application/pgp-keys
Size: 2437 bytes
Desc: not available
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20170125/639ded32/attachment.key>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: signature.asc
Type: application/pgp-signature
Size: 473 bytes
Desc: OpenPGP digital signature
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20170125/639ded32/attachment.sig>

From rousskov at measurement-factory.com  Tue Jan 24 20:50:11 2017
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Tue, 24 Jan 2017 13:50:11 -0700
Subject: [squid-users] Squid 4.x: Intermediate certificates downloader
In-Reply-To: <c4e25904-5c86-e3cb-c49c-f1f7d3f4583e@gmail.com>
References: <4198a8d7-07c7-8c74-80ac-eb75ca61e62c@gmail.com>
 <35afc9c9-dd43-fb17-d0d6-c90866fd5c28@measurement-factory.com>
 <52b0aec8-f61d-1bec-de18-1c3be06494a0@urlfilterdb.com>
 <81cf3b07-28a0-39b6-2690-a6cc320d653f@treenet.co.nz>
 <02aff9e7-c248-88f4-4a24-75f68448c378@gmail.com>
 <4df5b398-9c23-87ad-81c4-4d34c16a231c@measurement-factory.com>
 <42af5a38-f4f9-19a5-7397-b3c7084ce1a4@gmail.com>
 <e9b275f4-f9b1-7eda-e102-420bb9eda7ad@measurement-factory.com>
 <6d9fb82c-a83d-1215-7df7-04259c9a3997@gmail.com>
 <42f09caf-ee99-0f5e-b187-7468b736a04d@measurement-factory.com>
 <c4e25904-5c86-e3cb-c49c-f1f7d3f4583e@gmail.com>
Message-ID: <c1090b61-7bad-5940-c5d9-10eb829cc595@measurement-factory.com>

On 01/24/2017 12:20 PM, Yuri Voinov wrote:
> 25.01.2017 1:10, Alex Rousskov ?????:
>> On 01/24/2017 11:33 AM, Yuri Voinov wrote:

>>> http_access deny to_localhost

>> Does not match. The destination is not localhost.

> Yes, destination is squid itself. From squid to squid.

No, not "to squid": The destination of the request is
http://repository.certum.pl/ca.cer.

As for the "from Squid" part, it is dangerous to think that way because,
in most Squid contexts, "from" applies to the source of the request
_received_ by Squid. In this case, there is no received request at all.

So this transaction is _not_ "from Squid to Squid" in a sense that some
other, regular transaction is "from user X to site Y".



>>> # And finally deny all other access to this proxy
>>> http_access deny all

>> Matches!

> But! This is recommended final deny rule, if I omit it - squid will adds
> it silently by default, like normal firewall!

Nobody is suggesting to omit this catch-all rule (which you have
confirmed as matching in a follow-up email, thank you). The solution is
to add a new "allow" rule (at least) above this last "deny all" one. In
other words, your configuration is missing an http_access rule to allow
internally-generated requests [to benign destinations].


> I don't know (on first look) special ACL rule to permit internal access
> from squid to squid.

A short-term hack: I have seen folks successfully solving somewhat
similar problems using a localport ACL with an "impossible" value of
zero. Please try this hack and update this thread if it works for you:

> # Allow ESI, certificate fetching, Cache Digests, etc. internal requests
> # XXX: Sooner or later, this unsupported hack will stop working!
> acl generatedBySquid localport 0
> http_access allow generatedBySquid


A possible long-term solution: Factory is working on adding support for
the following new ACL which may help solve this and a few other problems:

> 	acl aclname transaction_initiator initiator...
> 	  # Matches transaction's initiator [fast]
> 	  # Supported initiators are:
> 	  #   esi: matches transactions fetching ESI resources
> 	  #   certificate-fetching: matches transactions fetching
> 	  #       a missing intermediate TLS certificate
> 	  #   cache-digest: matches transactions fetching Cache Digests
> 	  #       from a cache_peer
> 	  #   icp: matches icp requests to peers
> 	  #   icmp: matches icmp requests to peers
> 	  #   asn: matches asns db requests
> 	  #   internal: matches any of the above
> 	  #   client: matches transactions containing an HTTP or FTP
> 	  #       client request received at a Squid *_port
> 	  #  all: matches all transactions
> 	  # Multiple initiators are ORed.

Please note that these are draft specs that are subject to change. We
may not be able to support many of the initiators listed above. We will
support the certificate-fetching initiator though.


> It looks squid blocks own internal access to itself, but
> permits the same request from outside.

As we discussed earlier, the destination here is not "Squid itself".
Squid permits transaction with the same destination upon receiving a
request from "outside" because your http_access rules permit such
requests from "outside" while prohibiting transactions that are not
based on "outside" requests.

One aspect that complicates this problem in general is that we cannot
simply allow internally-initiated transactions without an explicit admin
permission because many of them, including the ones that fetch missing
certificates, may go to completely arbitrary destinations (determined by
origin servers, not users). Many admins care about where their Squid
sends requests as much as they care about where their Squid is receiving
the requests from!


HTH,

Alex.



From yvoinov at gmail.com  Tue Jan 24 21:11:05 2017
From: yvoinov at gmail.com (Yuri Voinov)
Date: Wed, 25 Jan 2017 03:11:05 +0600
Subject: [squid-users] Squid 4.x: Intermediate certificates downloader
In-Reply-To: <c1090b61-7bad-5940-c5d9-10eb829cc595@measurement-factory.com>
References: <4198a8d7-07c7-8c74-80ac-eb75ca61e62c@gmail.com>
 <35afc9c9-dd43-fb17-d0d6-c90866fd5c28@measurement-factory.com>
 <52b0aec8-f61d-1bec-de18-1c3be06494a0@urlfilterdb.com>
 <81cf3b07-28a0-39b6-2690-a6cc320d653f@treenet.co.nz>
 <02aff9e7-c248-88f4-4a24-75f68448c378@gmail.com>
 <4df5b398-9c23-87ad-81c4-4d34c16a231c@measurement-factory.com>
 <42af5a38-f4f9-19a5-7397-b3c7084ce1a4@gmail.com>
 <e9b275f4-f9b1-7eda-e102-420bb9eda7ad@measurement-factory.com>
 <6d9fb82c-a83d-1215-7df7-04259c9a3997@gmail.com>
 <42f09caf-ee99-0f5e-b187-7468b736a04d@measurement-factory.com>
 <c4e25904-5c86-e3cb-c49c-f1f7d3f4583e@gmail.com>
 <c1090b61-7bad-5940-c5d9-10eb829cc595@measurement-factory.com>
Message-ID: <c48783fd-02ad-d9e7-f2cc-164908dd0293@gmail.com>



25.01.2017 2:50, Alex Rousskov ?????:
> On 01/24/2017 12:20 PM, Yuri Voinov wrote:
>> 25.01.2017 1:10, Alex Rousskov ?????:
>>> On 01/24/2017 11:33 AM, Yuri Voinov wrote:
>>>> http_access deny to_localhost
>>> Does not match. The destination is not localhost.
>> Yes, destination is squid itself. From squid to squid.
> No, not "to squid": The destination of the request is
> http://repository.certum.pl/ca.cer.
>
> As for the "from Squid" part, it is dangerous to think that way because,
> in most Squid contexts, "from" applies to the source of the request
> _received_ by Squid. In this case, there is no received request at all.
>
> So this transaction is _not_ "from Squid to Squid" in a sense that some
> other, regular transaction is "from user X to site Y".
>
>
>
>>>> # And finally deny all other access to this proxy
>>>> http_access deny all
>>> Matches!
>> But! This is recommended final deny rule, if I omit it - squid will adds
>> it silently by default, like normal firewall!
> Nobody is suggesting to omit this catch-all rule (which you have
> confirmed as matching in a follow-up email, thank you). The solution is
> to add a new "allow" rule (at least) above this last "deny all" one. In
> other words, your configuration is missing an http_access rule to allow
> internally-generated requests [to benign destinations].
>
>
>> I don't know (on first look) special ACL rule to permit internal access
>> from squid to squid.
> A short-term hack: I have seen folks successfully solving somewhat
> similar problems using a localport ACL with an "impossible" value of
> zero. Please try this hack and update this thread if it works for you:
>
>> # Allow ESI, certificate fetching, Cache Digests, etc. internal requests
>> # XXX: Sooner or later, this unsupported hack will stop working!
>> acl generatedBySquid localport 0
>> http_access allow generatedBySquid
Sadly, but with this hack squid dies at request:

root @ khorne /patch # wget -S https://yandex.com/company/
--2017-01-25 02:53:58--  https://yandex.com/company/
Connecting to 127.0.0.1:3128... connected.

2017/01/25 02:53:51 kid1| Accepting SSL bumped HTTP Socket connections
at local=[::]:3128 remote=[::] FD 76 flags=9
FATAL: Received Segment Violation...dying.
2017/01/25 02:54:00 kid1| Closing HTTP(S) port [::]:3126
2017/01/25 02:54:00 kid1| Closing HTTP(S) port [::]:3127
2017/01/25 02:54:00 kid1| Closing HTTP(S) port [::]:3128
2017/01/25 02:54:00 kid1| storeDirWriteCleanLogs: Starting...
2017/01/25 02:54:00 kid1|     65536 entries written so far.
2017/01/25 02:54:00 kid1|    131072 entries written so far.
2017/01/25 02:54:01 kid1|    196608 entries written so far.
2017/01/25 02:54:01 kid1|    262144 entries written so far.
2017/01/25 02:54:01 kid1|    327680 entries written so far.
2017/01/25 02:54:01 kid1|    393216 entries written so far.
2017/01/25 02:54:01 kid1|    458752 entries written so far.
2017/01/25 02:54:01 kid1|    524288 entries written so far.
2017/01/25 02:54:01 kid1|    589824 entries written so far.
2017/01/25 02:54:01 kid1|   Finished.  Wrote 622687 entries.
2017/01/25 02:54:01 kid1|   Took 0.43 seconds (1438401.76 entries/sec).
CPU Usage: 1762.455 seconds = 1490.509 user + 271.946 sys
Maximum Resident Size: 0 KB
Page faults with physical i/o: 0

t at 1 (l at 1) program terminated by signal ABRT (Abort)
0xfffffd7ffe7c362a: __lwp_kill+0x000a:  jae      __lwp_kill+0x18       
[ 0xfffffd7ffe7c3638, .+0xe ]
(dbx)
where                                                                 
current thread: t at 1
=>[1] __lwp_kill(0x1, 0x6, 0xffffffffd1df63a0, 0xfffffd7ffe7c3f1e, 0x0,
0xfffffd7ffe823c80), at 0xfffffd7ffe7c362a
  [2] _thr_kill(), at 0xfffffd7ffe7bbf23
  [3] raise(), at 0xfffffd7ffe7681c9
  [4] abort(), at 0xfffffd7ffe746bc0
  [5] death(), at 0x6f463d
  [6] __sighndlr(), at 0xfffffd7ffe7bde26
  [7] call_user_handler(), at 0xfffffd7ffe7b26f2
  [8] sigacthandler(), at 0xfffffd7ffe7b291e
  ---- called from signal handler with signal 11 (SIGSEGV) ------
  [9] Ip::Qos::doTosLocalHit(), at 0x7d800e
  [10] clientReplyContext::doGetMoreData(), at 0x5d3009
  [11] clientReplyContext::identifyFoundObject(), at 0x5d323c
  [12] clientReplyContext::created(), at 0x5d3a45
  [13] StoreEntry::getPublicByRequest(), at 0x6cae49
  [14] clientStreamRead(), at 0x5edcc4
  [15] ClientHttpRequest::httpStart(), at 0x5d917e
  [16] ClientHttpRequest::processRequest(), at 0x5db570
  [17] ClientHttpRequest::doCallouts(), at 0x5dd1be
  [18] ACLChecklist::checkCallback(), at 0x76224a
  [19] ClientHttpRequest::doCallouts(), at 0x5ddd69
  [20] ClientRequestContext::clientStoreIdDone(), at 0x5e33bf
  [21] 0x5e3810(), at 0x5e3810
  [22] ACLChecklist::checkCallback(), at 0x76224a
  [23] ClientRequestContext::clientStoreIdStart(), at 0x5d8f78
  [24] ClientHttpRequest::doCallouts(), at 0x5dda59
  [25] ClientRequestContext::clientAccessCheckDone(), at 0x5e15b4
  [26] ClientRequestContext::clientAccessCheck2(), at 0x5e2318
  [27] ClientHttpRequest::doCallouts(), at 0x5dd8ec
  [28] ClientRequestContext::clientRedirectDone(), at 0x5e296b
  [29] 0x6b35dd(), at 0x6b35dd
  [30] 0x638393(), at 0x638393
  [31] AsyncCall::make(), at 0x7c0579
  [32] AsyncCallQueue::fireNext(), at 0x7c4cf3
  [33] AsyncCallQueue::fire(), at 0x7c508a
  [34] EventLoop::runOnce(), at 0x612229
  [35] EventLoop::run(), at 0x612348
  [36] SquidMain(), at 0x683518
  [37] main(), at 0x937f3c

>
> A possible long-term solution: Factory is working on adding support for
> the following new ACL which may help solve this and a few other problems:
>
>> 	acl aclname transaction_initiator initiator...
>> 	  # Matches transaction's initiator [fast]
>> 	  # Supported initiators are:
>> 	  #   esi: matches transactions fetching ESI resources
>> 	  #   certificate-fetching: matches transactions fetching
>> 	  #       a missing intermediate TLS certificate
>> 	  #   cache-digest: matches transactions fetching Cache Digests
>> 	  #       from a cache_peer
>> 	  #   icp: matches icp requests to peers
>> 	  #   icmp: matches icmp requests to peers
>> 	  #   asn: matches asns db requests
>> 	  #   internal: matches any of the above
>> 	  #   client: matches transactions containing an HTTP or FTP
>> 	  #       client request received at a Squid *_port
>> 	  #  all: matches all transactions
>> 	  # Multiple initiators are ORed.
> Please note that these are draft specs that are subject to change. We
> may not be able to support many of the initiators listed above. We will
> support the certificate-fetching initiator though.
Very good. When it will be ready to use, or, at least, to test?
>
>
>> It looks squid blocks own internal access to itself, but
>> permits the same request from outside.
> As we discussed earlier, the destination here is not "Squid itself".
> Squid permits transaction with the same destination upon receiving a
> request from "outside" because your http_access rules permit such
> requests from "outside" while prohibiting transactions that are not
> based on "outside" requests.
>
> One aspect that complicates this problem in general is that we cannot
> simply allow internally-initiated transactions without an explicit admin
> permission because many of them, including the ones that fetch missing
> certificates, may go to completely arbitrary destinations (determined by
> origin servers, not users). Many admins care about where their Squid
> sends requests as much as they care about where their Squid is receiving
> the requests from!
I think it's a necessary evil. As soon as we actually do attack, "Man in
the middle", whether to worry about lost virginity withnon-controlled
requests? No download intermediate certificates, we return to the local
and accompanied by a certificate-based hand. Here it is necessary to
decide - to go to smart or go to beautiful.

For smart is always remains sslproxy_foreign_intermediate_certs and all
the beauties of a manual tracking certificates. As I know from personal
experience, it is very entertaining organization of unlimited personal
time for proxy administrator. No girls, just clock-around support,
seriously.

So personally I'm willing to put up with uncontrollable requests to the
certificate stores.

Anyway, always exists an alternative. Either full automated
zero-administrated proxy appliance - or, handmade day-by-day entertained
and funny support.
>
>
> HTH,
>
> Alex.
>

-------------- next part --------------
A non-text attachment was scrubbed...
Name: 0x613DEC46.asc
Type: application/pgp-keys
Size: 2437 bytes
Desc: not available
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20170125/222ffb70/attachment.key>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: signature.asc
Type: application/pgp-signature
Size: 473 bytes
Desc: OpenPGP digital signature
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20170125/222ffb70/attachment.sig>

From rousskov at measurement-factory.com  Tue Jan 24 23:25:29 2017
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Tue, 24 Jan 2017 16:25:29 -0700
Subject: [squid-users] Squid 4.x: Intermediate certificates downloader
In-Reply-To: <c48783fd-02ad-d9e7-f2cc-164908dd0293@gmail.com>
References: <4198a8d7-07c7-8c74-80ac-eb75ca61e62c@gmail.com>
 <35afc9c9-dd43-fb17-d0d6-c90866fd5c28@measurement-factory.com>
 <52b0aec8-f61d-1bec-de18-1c3be06494a0@urlfilterdb.com>
 <81cf3b07-28a0-39b6-2690-a6cc320d653f@treenet.co.nz>
 <02aff9e7-c248-88f4-4a24-75f68448c378@gmail.com>
 <4df5b398-9c23-87ad-81c4-4d34c16a231c@measurement-factory.com>
 <42af5a38-f4f9-19a5-7397-b3c7084ce1a4@gmail.com>
 <e9b275f4-f9b1-7eda-e102-420bb9eda7ad@measurement-factory.com>
 <6d9fb82c-a83d-1215-7df7-04259c9a3997@gmail.com>
 <42f09caf-ee99-0f5e-b187-7468b736a04d@measurement-factory.com>
 <c4e25904-5c86-e3cb-c49c-f1f7d3f4583e@gmail.com>
 <c1090b61-7bad-5940-c5d9-10eb829cc595@measurement-factory.com>
 <c48783fd-02ad-d9e7-f2cc-164908dd0293@gmail.com>
Message-ID: <741d243a-1750-30c9-f676-45a772038a69@measurement-factory.com>

On 01/24/2017 02:11 PM, Yuri Voinov wrote:
> 25.01.2017 2:50, Alex Rousskov ?????:
>> A short-term hack: I have seen folks successfully solving somewhat
>> similar problems using a localport ACL with an "impossible" value of
>> zero. Please try this hack and update this thread if it works for you:
>>
>>> # Allow ESI, certificate fetching, Cache Digests, etc. internal requests
>>> # XXX: Sooner or later, this unsupported hack will stop working!
>>> acl generatedBySquid localport 0
>>> http_access allow generatedBySquid


> Sadly, but with this hack squid dies at request:

That death is unlikely to be related to the ACL hack itself IMO. You can
test my theory by temporary replacing "generatedBySquid" with "all" on
the http_access line. If Squid still dies with "all", please consider
properly reporting the crash; it will probably continue to bite you even
after the long-term solution (e.g., transaction_initiator ACL) is available.


>> A possible long-term solution: Factory is working on adding support for
>> the following new ACL which may help solve this and a few other problems:
>>
>>> 	acl aclname transaction_initiator initiator...
>>> 	  # Matches transaction's initiator [fast]

> Very good. When it will be ready to use, or, at least, to test?

I expect the first public implementation to be posted for squid-dev
review within a week but this is not a promise.


> So personally I'm willing to put up with uncontrollable requests to the
> certificate stores.

... including any site pretending to be a certificate store.

I respect that choice, but it must remain as a choice rather than
hard-coded behavior IMO. I am sure that sooner or later somebody will
come up with a "certificate" response that crashes Squid, and we do not
want to leave the admins without a way to combat that attack vector.

We could change Squid to ignore http_access and lots of other existing
directives while adding an increasing number of new control directives
dedicated to certificate fetching transactions, but I think that
segregation approach would be a strategic mistake because there are too
many potential segregation areas with partially overlapping and complex
configuration requirements. It will get messy and too error-prone.


Cheers,

Alex.



From rentorbuy at yahoo.com  Wed Jan 25 07:45:25 2017
From: rentorbuy at yahoo.com (Vieri)
Date: Wed, 25 Jan 2017 07:45:25 +0000 (UTC)
Subject: [squid-users] squid reverse proxy (accelerator) for MS Exchange
 OWA
In-Reply-To: <94978436-88e5-fc84-9f3e-b8f1757402a5@measurement-factory.com>
References: <1216522442.160455.1484870639800.ref@mail.yahoo.com>
 <1216522442.160455.1484870639800@mail.yahoo.com>
 <cdea0afa-c2a2-23c4-dbac-553a565e83f7@treenet.co.nz>
 <690032068.431187.1484905491833@mail.yahoo.com>
 <5517395f-eaa4-0b3a-34e9-66ea8d0122a1@treenet.co.nz>
 <1374872097.4095106.1485244925609@mail.yahoo.com>
 <94978436-88e5-fc84-9f3e-b8f1757402a5@measurement-factory.com>
Message-ID: <2010900066.469374.1485330325639@mail.yahoo.com>





----- Original Message -----
From: Alex Rousskov <rousskov at measurement-factory.com>
>
> The peer at 10.215.144.21:443 accepted Squid connection and then closed

> it, probably before sending anything to Squid

Thanks Alex.

I was lucky enough to try the following options in cache_peer:
ssloptions=NO_SSLv3,NO_SSLv2,NO_TLSv1_2,NO_TLSv1_1

This solves the issue. I understand it forces using TLS 1.0. In fact, the OWA origin server is a Windows server 2003 and only supports SSLv{2,3} and TLS 1.0.

It seems that Squid delegates SSL to OpenSSL and it's really too bad the latter can't be a little bit more verbose. I know this isn't the right list for this but couldn't OpenSSL simply have logged something regarding "unsupported TLS/SSL versions"? I'm only supposing that without the ssloptions I posted above, openssl will try TLS 1.2 and silently fail if that doesn't succeed.

Regardless, it all seems to be working now, even with Squid 3.5.14.

Thanks again,

Vieri


From hoper at free.fr  Wed Jan 25 09:29:42 2017
From: hoper at free.fr (hoper at free.fr)
Date: Wed, 25 Jan 2017 10:29:42 +0100 (CET)
Subject: [squid-users] squid3 : Really need to use external (slow) acl with
 peer_cache_access
In-Reply-To: <2062955437.99592803.1485331797299.JavaMail.root@zimbra84-e15.priv.proxad.net>
Message-ID: <351688258.99968148.1485336582829.JavaMail.root@zimbra84-e15.priv.proxad.net>


Hi everybody,

I really try to find a answer with google, and within
the archives of this mailing list but couldn't find anything
so... here I am...

I need to select a squid parent based on the login of the
user (and others things). With squid 2.7, I had a configuration
like this one :

-------------------------------------------------------------
cache_peer 169.254.1.1 parent 3128 0 default name=parent1
cache_peer 169.254.1.2 parent 3128 0 default name=parent2
[...] (many parents)

external_acl_type choose_parent ttl=60,children-max=1 %EXT_USER %SRC %LOGIN %ACL /home/user/myhelper.sh
acl p0 external choose_parent

external_acl_type myparent1 ttl=60,children-max=1 %ACL %EXT_USER  /home/user/another_helper
acl p1 external myparent1
external_acl_type myparent2 ttl=60,children-max=1 %ACL %EXT_USER  /home/user/another_helper
acl p2 external myparent2
[...]

cache_peer_access parent1 allow p1
cache_peer_access parent2 allow p2
[...]

cache_peer_access path1 deny all
cache_peer_access path2 deny all
[...]

---------------------------------------------------------------

The idea is to deny all squid parents except the one I want this user
(with this specific IP and so on) to use.

But with squid3, I just have lot's of error in cache.log:

2017/01/25 10:22:16.053 kid1| external_acl.cc(868) aclMatchExternal: myparent1("p1 p1") = lookup needed
2017/01/25 10:22:16.053 kid1| external_acl.cc(871) aclMatchExternal: "p1 p1": queueing a call.
2017/01/25 10:22:16.053 kid1| Checklist.cc(115) goAsync: 0x7fff415cf470 a fast-only directive uses a slow ACL!
2017/01/25 10:22:16.053 kid1| external_acl.cc(873) aclMatchExternal: "p1 p1": no async support!
2017/01/25 10:22:16.053 kid1| external_acl.cc(874) aclMatchExternal: "p1 p1": return -1.

The documentation made it perfectly clear that "cache_peer_acccess" is a "fast ACL" that can only use fast ones...
But I really need to use external "slow" acl. Please, is there a way to do it ?
Again, this was working in 2.7 :(

Thanks you very much.


From stephen.baynes at smoothwall.net  Wed Jan 25 10:00:42 2017
From: stephen.baynes at smoothwall.net (Stephen Baynes)
Date: Wed, 25 Jan 2017 10:00:42 +0000
Subject: [squid-users]
	=?utf-8?q?Squid_performance_3=2E5=2E20_=E2=86=92_3?=
	=?utf-8?b?LjUuMjM=?=
In-Reply-To: <CABrJVoz5WcSC-kWgskrbMEeosq_uVQy5h3x7wA_2FJkRPbNstA@mail.gmail.com>
References: <CABrJVoz5WcSC-kWgskrbMEeosq_uVQy5h3x7wA_2FJkRPbNstA@mail.gmail.com>
Message-ID: <CABrJVozz+C_zVnf9J42jdB0RGJ1K+exmmB6XqZ5wjo10XebUpQ@mail.gmail.com>

Looks like this was a false alarm. The test environment I was using had
some random fluctuations in the results. I assumed they averaged out over
multiple test runs. However it looks as if I got some bad rolls of the dice
and they did not.
I have now a modified test which has <1% variation over four runs each
taking 3 hours. Testing both versions, I now see negligible difference
between them.

On 13 January 2017 at 10:10, Stephen Baynes <stephen.baynes at smoothwall.net>
wrote:

> Is there a known performance fall off going 3.5.20 ? 3.5.23?
>
> I am seeing a 15% to 20% performance drop on my normal download benchmark
> and a crude test of uploading shows a few percent slowdown.
>
> Running on a Linux derived from Debian.
>
> Thanks
>
> --
>
> Stephen Baynes
>
>


-- 

Stephen Baynes
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20170125/ea8a7908/attachment.htm>

From javier.sanchez at coam.org  Wed Jan 25 12:00:26 2017
From: javier.sanchez at coam.org (javier.sanchez at coam.org)
Date: Wed, 25 Jan 2017 13:00:26 +0100
Subject: [squid-users] URL bad rewritten using directive "deny_info"
Message-ID: <3dd83a67-cd31-a611-bb58-6610b3e3497e@coam.org>


     Hi all.


     I using squid 3.4.8 over Debian as reverse proxy in order to 
protect with SSL some of our servers.


     I have an issue when I try to "redirect" to port 443 users that try 
to connect to port 80.


     As is said here 
(http://lists.squid-cache.org/pipermail/squid-users/2016-September/012669.html), 
we use this lines in our configuration.

[...]
http_port 80 accel vport
acl HTTP proto HTTP
deny_info 301:https://%H%R HTTP
http_access deny HTTP
[...]


	It works and send a 301 reply to the client, but the problem is that it changes the requested URL. Particularly it changes "&" for "&amp;"

	
	This is a real example:

	The client request next URL on port 80:

http://masivos.coam.org/Masivos/URLProcesador?id_envio=2689&token=20161202122625195AMVnxAmHS2lwgc5JoFd2HCHi9fBipvoesVlpMWw3RpJ5yratkcx3ucbBRwpHPs1pNouvpP99UuWWxn359pnXnetQKrekCDZjCo65&id=0&semilla=20150710135955907GDEjFCZNveaoPsXs34Npc2JUQWF3PcCFjcAadFbToAFCqfQqX3cS6xeyh5NVEb31tNa4YqtE9pWFFxoqnT6wWhTrbMNEVKp7guyx&email=prueba2 at coam.org

	The client receives 301 error code requesting him to go to:

https://masivos.coam.org/Masivos/URLProcesador?id_envio=2689&amp;token=20161202122625195AMVnxAmHS2lwgc5JoFd2HCHi9fBipvoesVlpMWw3RpJ5yratkcx3ucbBRwpHPs1pNouvpP99UuWWxn359pnXnetQKrekCDZjCo65&amp;id=0&amp;semilla=20150710135955907GDEjFCZNveaoPsXs34Npc2JUQWF3PcCFjcAadFbToAFCqfQqX3cS6xeyh5NVEb31tNa4YqtE9pWFFxoqnT6wWhTrbMNEVKp7guyx&amp;email=prueba2 at coam.org


     How can I solve that? Is this a bug or something that can be solved 
changing configuration.


     Thank you.


-- 

Javier
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20170125/2a073a29/attachment.htm>

From erdosain9 at gmail.com  Wed Jan 25 13:12:31 2017
From: erdosain9 at gmail.com (erdosain9)
Date: Wed, 25 Jan 2017 05:12:31 -0800 (PST)
Subject: [squid-users] Strange behavior - reload service failed,
	but not start....
Message-ID: <1485349951724-4681317.post@n4.nabble.com>

Hi,
I'm having this problem:
if i reload the service (systemctl reload squid)... the service failed and
dont reload... but, if i do systemctl start squid, all is working fine...
this begin to happend after a bad reboot... (and after the bad reboot, squid
dosent work for a moment giving this error:
"2017/01/24 11:58:30 kid1| WARNING: Disk space over limit: 19244596.00 KB >
15360000 KB
2017/01/24 11:58:57 kid1| WARNING: Disk space over limit: 19134980.00 KB >
15360000 KB
2017/01/24 11:59:21 kid1| WARNING: Disk space over limit: 19021112.00 KB >
15360000 KB
2017/01/24 12:00:06 kid1| WARNING: Disk space over limit: 18804760.00 KB >
15360000 KB
2017/01/24 12:00:21 kid1| WARNING: Disk space over limit: 18640860.00 KB >
15360000 KB"

After some stop start process, again is working but with that problem of
"reload" that i mentioned before.

this is some log

[root at squid squid]# *systemctl status squid.service*
? squid.service - Squid Web Proxy Server
   Loaded: loaded (/usr/lib/systemd/system/squid.service; enabled; vendor
preset: disabled)
   Active: failed (Result: exit-code) since mi? 2017-01-25 09:49:52 ART; 41s
ago
     Docs: man:squid(8)
  Process: 4057 ExecStop=/usr/sbin/squidshut.sh (code=exited,
status=1/FAILURE)
  Process: 4056 ExecReload=/usr/sbin/squid -kreconf (code=exited,
status=1/FAILURE)
  Process: 3995 ExecStart=/usr/sbin/squid -sYC (code=exited,
status=0/SUCCESS)
  Process: 3993 ExecStartPre=/usr/bin/chown squid.squid /var/run/squid
(code=exited, status=0/SUCCESS)
  Process: 3992 ExecStartPre=/usr/bin/mkdir -p /var/run/squid (code=exited,
status=0/SUCCESS)
 Main PID: 4020
   CGroup: /system.slice/squid.service
           ??2228 /usr/sbin/squid -sYC
           ??2230 (squid-1) -sYC
           ??2231 (ssl_crtd) -s /var/lib/ssl_db -M 4MB
           ??2232 (ssl_crtd) -s /var/lib/ssl_db -M 4MB
           ??2233 (ssl_crtd) -s /var/lib/ssl_db -M 4MB
           ??2234 (ssl_crtd) -s /var/lib/ssl_db -M 4MB
           ??2235 (ssl_crtd) -s /var/lib/ssl_db -M 4MB
           ??2236 (logfile-daemon) /var/log/squid/access.log
           ??2237 (unlinkd)
           ??2238 diskd 2283524 2283525 2283526
           ??2239 (ssl_crtd) -s /var/lib/ssl_db -M 4MB
           ??2240 (ssl_crtd) -s /var/lib/ssl_db -M 4MB
           ??2241 (ssl_crtd) -s /var/lib/ssl_db -M 4MB
           ??2242 (ssl_crtd) -s /var/lib/ssl_db -M 4MB
           ??2243 (ssl_crtd) -s /var/lib/ssl_db -M 4MB
           ??2244 (ssl_crtd) -s /var/lib/ssl_db -M 4MB
           ??2245 (ssl_crtd) -s /var/lib/ssl_db -M 4MB
           ??2246 (ssl_crtd) -s /var/lib/ssl_db -M 4MB
           ??2247 (ssl_crtd) -s /var/lib/ssl_db -M 4MB
           ??2248 (ssl_crtd) -s /var/lib/ssl_db -M 4MB
           ??2278 (ssl_crtd) -s /var/lib/ssl_db -M 4MB
           ??2325 (ssl_crtd) -s /var/lib/ssl_db -M 4MB
           ??2368 (ssl_crtd) -s /var/lib/ssl_db -M 4MB
           ??2369 (ssl_crtd) -s /var/lib/ssl_db -M 4MB
           ??2371 (ssl_crtd) -s /var/lib/ssl_db -M 4MB
           ??2397 (ssl_crtd) -s /var/lib/ssl_db -M 4MB
           ??2398 (ssl_crtd) -s /var/lib/ssl_db -M 4MB
           ??2399 (ssl_crtd) -s /var/lib/ssl_db -M 4MB
           ??2400 (ssl_crtd) -s /var/lib/ssl_db -M 4MB
           ??2401 (ssl_crtd) -s /var/lib/ssl_db -M 4MB
           ??2402 (ssl_crtd) -s /var/lib/ssl_db -M 4MB
           ??2403 (ssl_crtd) -s /var/lib/ssl_db -M 4MB
           ??2404 (ssl_crtd) -s /var/lib/ssl_db -M 4MB
           ??2405 (ssl_crtd) -s /var/lib/ssl_db -M 4MB
           ??2406 (ssl_crtd) -s /var/lib/ssl_db -M 4MB
           ??2407 (ssl_crtd) -s /var/lib/ssl_db -M 4MB
           ??2408 (ssl_crtd) -s /var/lib/ssl_db -M 4MB

ene 25 09:49:40 squid.domain.lan squid[4040]: Rebuilding storage in
/var/spool/squid (no log)
ene 25 09:49:40 squid.domain.lan squid[4040]: Using Least Load store dir
selection
ene 25 09:49:40 squid.domain.lan squid[4040]: Set Current Directory to
/var/spool/squid
ene 25 09:49:52 squid.domain.lan squid[4056]: squid: ERROR: Could not send
signal 1 to process 4040: (3) No such process
ene 25 09:49:52 squid.domain.lan systemd[1]: squid.service: control process
exited, code=exited status=1
ene 25 09:49:52 squid.domain.lan squidshut.sh[4057]: Stopping Squid: Squid
settings file Falied the check
ene 25 09:49:52 squid.domain.lan systemd[1]: squid.service: control process
exited, code=exited status=1
ene 25 09:49:52 squid.domain.lan systemd[1]: Reload failed for Squid Web
Proxy Server.
ene 25 09:49:52 squid.domain.lan systemd[1]: Unit squid.service entered
failed state.
ene 25 09:49:52 squid.domain.lan systemd[1]: squid.service failed

[root at squid squid]# *journalctl -xe*
ene 25 09:49:40 squid.domain.lan squid[4040]: Squid plugin modules loade
ene 25 09:49:40 squid.domain.lan squid[4040]: Adaptation support is off.
ene 25 09:49:40 squid.domain.lan squid[4040]: Closing HTTP port 192.168.
ene 25 09:49:40 squid.domain.lan squid[4040]: Unable to open HTTP Socket
ene 25 09:49:40 squid.domain.lan squid[3997]: Squid Parent: (squid-1) pr
ene 25 09:49:40 squid.domain.lan squid[3997]: Squid Parent: (squid-1) pr
ene 25 09:49:40 squid.domain.lan squid[3997]: Exiting due to repeated, f
ene 25 09:49:52 squid.domain.lan polkitd[667]: Registered Authentication
ene 25 09:49:52 squid.domain.lan squid[4056]: squid: ERROR: Could not se
ene 25 09:49:52 squid.domain.lan systemd[1]: squid.service: control proc
ene 25 09:49:52 squid.domain.lan squidshut.sh[4057]: Stopping Squid: Squ
ene 25 09:49:52 squid.domain.lan systemd[1]: squid.service: control proc
ene 25 09:49:52 squid.domain.lan systemd[1]: Reload failed for Squid Web
-- Subject: Unit squid.service has finished reloading its configuration
-- Defined-By: systemd
-- Support: http://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit squid.service has finished reloading its configuration
-- 
-- The result is failed.
ene 25 09:49:52 squid.domain.lan systemd[1]: Unit squid.service entered 
ene 25 09:49:52 squid.domain.lan systemd[1]: squid.service failed.
ene 25 09:49:52 squid.domain.lan polkitd[667]: Unregistered Authenticati
lines 1462-1484/1484 (END)
ene 25 09:49:40 squid.domain.lan squid[4040]: Squid plugin modules loade
ene 25 09:49:40 squid.domain.lan squid[4040]: Adaptation support is off.
ene 25 09:49:40 squid.domain.lan squid[4040]: Closing HTTP port 192.168.
ene 25 09:49:40 squid.domain.lan squid[4040]: Unable to open HTTP Socket
ene 25 09:49:40 squid.domain.lan squid[3997]: Squid Parent: (squid-1) pr
ene 25 09:49:40 squid.domain.lan squid[3997]: Squid Parent: (squid-1) pr
ene 25 09:49:40 squid.domain.lan squid[3997]: Exiting due to repeated, f
ene 25 09:49:52 squid.domain.lan polkitd[667]: Registered Authentication
ene 25 09:49:52 squid.domain.lan squid[4056]: squid: ERROR: Could not se
ene 25 09:49:52 squid.domain.lan systemd[1]: squid.service: control proc
ene 25 09:49:52 squid.domain.lan squidshut.sh[4057]: Stopping Squid: Squ
ene 25 09:49:52 squid.domain.lan systemd[1]: squid.service: control proc
ene 25 09:49:52 squid.domain.lan systemd[1]: Reload failed for Squid Web
-- Subject: Unit squid.service has finished reloading its configuration
-- Defined-By: systemd
-- Support: http://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit squid.service has finished reloading its configuration
-- 
-- The result is failed.
ene 25 09:49:52 squid.domain.lan systemd[1]: Unit squid.service entered 
ene 25 09:49:52 squid.domain.lan systemd[1]: squid.service failed.
lines 1462-1483/1484 100%
ene 25 09:49:40 squid.domain.lan squid[4040]: Squid plugin modules loaded:
ene 25 09:49:40 squid.domain.lan squid[4040]: Adaptation support is off.
ene 25 09:49:40 squid.domain.lan squid[4040]: Closing HTTP port 192.168.1.
ene 25 09:49:40 squid.domain.lan squid[4040]: Unable to open HTTP Socket
ene 25 09:49:40 squid.domain.lan squid[3997]: Squid Parent: (squid-1) proc
ene 25 09:49:40 squid.domain.lan squid[3997]: Squid Parent: (squid-1) proc
ene 25 09:49:40 squid.domain.lan squid[3997]: Exiting due to repeated, fre
ene 25 09:49:52 squid.domain.lan polkitd[667]: Registered Authentication A
ene 25 09:49:52 squid.domain.lan squid[4056]: squid: ERROR: Could not send
ene 25 09:49:52 squid.domain.lan systemd[1]: squid.service: control proces
ene 25 09:49:52 squid.domain.lan squidshut.sh[4057]: Stopping Squid: Squid
ene 25 09:49:52 squid.domain.lan systemd[1]: squid.service: control proces
ene 25 09:49:52 squid.domain.lan systemd[1]: Reload failed for Squid Web P
-- Subject: Unit squid.service has finished reloading its configuration
-- Defined-By: systemd
-- Support: http://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit squid.service has finished reloading its configuration
-- 
-- The result is failed.
ene 25 09:49:52 squid.domain.lan systemd[1]: Unit squid.service entered fa
ene 25 09:49:52 squid.domain.lan systemd[1]: squid.service failed.
lines 1462-1483/1484 100%
ene 25 09:49:40 squid.domain.lan squid[4040]: Squid plugin modules loaded: 0
ene 25 09:49:40 squid.domain.lan squid[4040]: Adaptation support is off.
ene 25 09:49:40 squid.domain.lan squid[4040]: Closing HTTP port
192.168.1.97:3128
ene 25 09:49:40 squid.domain.lan squid[4040]: Unable to open HTTP Socket
ene 25 09:49:40 squid.domain.lan squid[3997]: Squid Parent: (squid-1)
process 4040 exited wit
ene 25 09:49:40 squid.domain.lan squid[3997]: Squid Parent: (squid-1)
process 4040 will not b
ene 25 09:49:40 squid.domain.lan squid[3997]: Exiting due to repeated,
frequent failures
ene 25 09:49:52 squid.domain.lan polkitd[667]: Registered Authentication
Agent for unix-proce
ene 25 09:49:52 squid.domain.lan squid[4056]: squid: ERROR: Could not send
signal 1 to proces
ene 25 09:49:52 squid.domain.lan systemd[1]: squid.service: control process
exited, code=exit
ene 25 09:49:52 squid.domain.lan squidshut.sh[4057]: Stopping Squid: Squid
settings file Fali
ene 25 09:49:52 squid.domain.lan systemd[1]: squid.service: control process
exited, code=exit
ene 25 09:49:52 squid.domain.lan systemd[1]: Reload failed for Squid Web
Proxy Server.
-- Subject: Unit squid.service has finished reloading its configuration
-- Defined-By: systemd
-- Support: http://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit squid.service has finished reloading its configuration
-- 
-- The result is failed.
ene 25 09:49:52 squid.domain.lan systemd[1]: Unit squid.service entered
failed state.
ene 25 09:49:52 squid.domain.lan systemd[1]: squid.service failed.
lines 1462-1483/1484 100%
ene 25 09:49:40 squid.domain.lan squid[4040]: Squid plugin modules loaded: 0
ene 25 09:49:40 squid.domain.lan squid[4040]: Adaptation support is off.
ene 25 09:49:40 squid.domain.lan squid[4040]: Closing HTTP port
192.168.1.97:3128
ene 25 09:49:40 squid.domain.lan squid[4040]: Unable to open HTTP Socket
ene 25 09:49:40 squid.domain.lan squid[3997]: Squid Parent: (squid-1)
process 4040 exited with stat
ene 25 09:49:40 squid.domain.lan squid[3997]: Squid Parent: (squid-1)
process 4040 will not be rest
ene 25 09:49:40 squid.domain.lan squid[3997]: Exiting due to repeated,
frequent failures
ene 25 09:49:52 squid.domain.lan polkitd[667]: Registered Authentication
Agent for unix-process:405
ene 25 09:49:52 squid.domain.lan squid[4056]: squid: ERROR: Could not send
signal 1 to process 4040
ene 25 09:49:52 squid.domain.lan systemd[1]: squid.service: control process
exited, code=exited sta
ene 25 09:49:52 squid.domain.lan squidshut.sh[4057]: Stopping Squid: Squid
settings file Falied the
ene 25 09:49:52 squid.domain.lan systemd[1]: squid.service: control process
exited, code=exited sta
ene 25 09:49:52 squid.domain.lan systemd[1]: Reload failed for Squid Web
Proxy Server.
-- Subject: Unit squid.service has finished reloading its configuration
-- Defined-By: systemd
-- Support: http://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit squid.service has finished reloading its configuration
-- 
-- The result is failed.
ene 25 09:49:52 squid.domain.lan systemd[1]: Unit squid.service entered
failed state.
ene 25 09:49:52 squid.domain.lan systemd[1]: squid.service failed.
lines 1462-1483/1484 100%
ene 25 09:49:40 squid.domain.lan squid[4040]: Squid plugin modules loaded: 0
ene 25 09:49:40 squid.domain.lan squid[4040]: Adaptation support is off.
ene 25 09:49:40 squid.domain.lan squid[4040]: Closing HTTP port
192.168.1.97:3128
ene 25 09:49:40 squid.domain.lan squid[4040]: Unable to open HTTP Socket
ene 25 09:49:40 squid.domain.lan squid[3997]: Squid Parent: (squid-1)
process 4040 exited with status 1
ene 25 09:49:40 squid.domain.lan squid[3997]: Squid Parent: (squid-1)
process 4040 will not be restarted due 
ene 25 09:49:40 squid.domain.lan squid[3997]: Exiting due to repeated,
frequent failures
ene 25 09:49:52 squid.domain.lan polkitd[667]: Registered Authentication
Agent for unix-process:4051:7880379 
ene 25 09:49:52 squid.domain.lan squid[4056]: squid: ERROR: Could not send
signal 1 to process 4040: (3) No s
ene 25 09:49:52 squid.domain.lan systemd[1]: squid.service: control process
exited, code=exited status=1
ene 25 09:49:52 squid.domain.lan squidshut.sh[4057]: Stopping Squid: Squid
settings file Falied the check
ene 25 09:49:52 squid.domain.lan systemd[1]: squid.service: control process
exited, code=exited status=1
ene 25 09:49:52 squid.domain.lan systemd[1]: Reload failed for Squid Web
Proxy Server.
-- Subject: Unit squid.service has finished reloading its configuration
-- Defined-By: systemd
-- Support: http://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit squid.service has finished reloading its configuration
-- 
-- The result is failed.
ene 25 09:49:52 squid.domain.lan systemd[1]: Unit squid.service entered
failed state.
ene 25 09:49:52 squid.domain.lan systemd[1]: squid.service failed.
lines 1462-1483/1484 100%
ene 25 09:49:40 squid.domain.lan squid[4040]: Squid plugin modules loaded: 0
ene 25 09:49:40 squid.domain.lan squid[4040]: Adaptation support is off.
ene 25 09:49:40 squid.domain.lan squid[4040]: Closing HTTP port
192.168.1.97:3128
ene 25 09:49:40 squid.domain.lan squid[4040]: Unable to open HTTP Socket
ene 25 09:49:40 squid.domain.lan squid[3997]: Squid Parent: (squid-1)
process 4040 exited with status 1
ene 25 09:49:40 squid.domain.lan squid[3997]: Squid Parent: (squid-1)
process 4040 will not be restarted due to rep
ene 25 09:49:40 squid.domain.lan squid[3997]: Exiting due to repeated,
frequent failures
ene 25 09:49:52 squid.domain.lan polkitd[667]: Registered Authentication
Agent for unix-process:4051:7880379 (syste
ene 25 09:49:52 squid.domain.lan squid[4056]: squid: ERROR: Could not send
signal 1 to process 4040: (3) No such pr
ene 25 09:49:52 squid.domain.lan systemd[1]: squid.service: control process
exited, code=exited status=1
ene 25 09:49:52 squid.domain.lan squidshut.sh[4057]: Stopping Squid: Squid
settings file Falied the check
ene 25 09:49:52 squid.domain.lan systemd[1]: squid.service: control process
exited, code=exited status=1
ene 25 09:49:52 squid.domain.lan systemd[1]: Reload failed for Squid Web
Proxy Server.
-- Subject: Unit squid.service has finished reloading its configuration
-- Defined-By: systemd
-- Support: http://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit squid.service has finished reloading its configuration
-- 
-- The result is failed.
ene 25 09:49:52 squid.domain.lan systemd[1]: Unit squid.service entered
failed state.
ene 25 09:49:52 squid.domain.lan systemd[1]: squid.service failed.
lines 1462-1483/1484 100%
ene 25 09:49:40 squid.domain.lan squid[4040]: Squid plugin modules loaded: 0
ene 25 09:49:40 squid.domain.lan squid[4040]: Adaptation support is off.
ene 25 09:49:40 squid.domain.lan squid[4040]: Closing HTTP port
192.168.1.97:3128
ene 25 09:49:40 squid.domain.lan squid[4040]: Unable to open HTTP Socket
ene 25 09:49:40 squid.domain.lan squid[3997]: Squid Parent: (squid-1)
process 4040 exited with status 1
ene 25 09:49:40 squid.domain.lan squid[3997]: Squid Parent: (squid-1)
process 4040 will not be restarted due to repe
ene 25 09:49:40 squid.domain.lan squid[3997]: Exiting due to repeated,
frequent failures
ene 25 09:49:52 squid.domain.lan polkitd[667]: Registered Authentication
Agent for unix-process:4051:7880379 (system
ene 25 09:49:52 squid.domain.lan squid[4056]: squid: ERROR: Could not send
signal 1 to process 4040: (3) No such pro
ene 25 09:49:52 squid.domain.lan systemd[1]: squid.service: control process
exited, code=exited status=1
ene 25 09:49:52 squid.domain.lan squidshut.sh[4057]: Stopping Squid: Squid
settings file Falied the check
ene 25 09:49:52 squid.domain.lan systemd[1]: squid.service: control process
exited, code=exited status=1
ene 25 09:49:52 squid.domain.lan systemd[1]: Reload failed for Squid Web
Proxy Server.
-- Subject: Unit squid.service has finished reloading its configuration
-- Defined-By: systemd
-- Support: http://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit squid.service has finished reloading its configuration
-- 
-- The result is failed.
ene 25 09:49:52 squid.domain.lan systemd[1]: Unit squid.service entered
failed state.
ene 25 09:49:52 squid.domain.lan systemd[1]: squid.service failed.
lines 1462-1483/1484 100%
ene 25 09:49:40 squid.domain.lan squid[4040]: Squid plugin modules loaded: 0
ene 25 09:49:40 squid.domain.lan squid[4040]: Adaptation support is off.
ene 25 09:49:40 squid.domain.lan squid[4040]: Closing HTTP port
192.168.1.97:3128
ene 25 09:49:40 squid.domain.lan squid[4040]: Unable to open HTTP Socket
ene 25 09:49:40 squid.domain.lan squid[3997]: Squid Parent: (squid-1)
process 4040 exited with status 1
ene 25 09:49:40 squid.domain.lan squid[3997]: Squid Parent: (squid-1)
process 4040 will not be restarted due to repea
ene 25 09:49:40 squid.domain.lan squid[3997]: Exiting due to repeated,
frequent failures
ene 25 09:49:52 squid.domain.lan polkitd[667]: Registered Authentication
Agent for unix-process:4051:7880379 (system 
ene 25 09:49:52 squid.domain.lan squid[4056]: squid: ERROR: Could not send
signal 1 to process 4040: (3) No such proc
ene 25 09:49:52 squid.domain.lan systemd[1]: squid.service: control process
exited, code=exited status=1
ene 25 09:49:52 squid.domain.lan squidshut.sh[4057]: Stopping Squid: Squid
settings file Falied the check
ene 25 09:49:52 squid.domain.lan systemd[1]: squid.service: control process
exited, code=exited status=1
ene 25 09:49:52 squid.domain.lan systemd[1]: Reload failed for Squid Web
Proxy Server.
-- Subject: Unit squid.service has finished reloading its configuration
-- Defined-By: systemd
-- Support: http://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit squid.service has finished reloading its configuration
-- 
-- The result is failed.
ene 25 09:49:52 squid.domain.lan systemd[1]: Unit squid.service entered
failed state.
ene 25 09:49:52 squid.domain.lan systemd[1]: squid.service failed.
lines 1462-1483/1484 100%
ene 25 09:49:40 squid.domain.lan squid[4040]: Squid plugin modules loaded: 0
ene 25 09:49:40 squid.domain.lan squid[4040]: Adaptation support is off.
ene 25 09:49:40 squid.domain.lan squid[4040]: Closing HTTP port
192.168.1.97:3128
ene 25 09:49:40 squid.domain.lan squid[4040]: Unable to open HTTP Socket
ene 25 09:49:40 squid.domain.lan squid[3997]: Squid Parent: (squid-1)
process 4040 exited with status 1
ene 25 09:49:40 squid.domain.lan squid[3997]: Squid Parent: (squid-1)
process 4040 will not be restarted due to repeated
ene 25 09:49:40 squid.domain.lan squid[3997]: Exiting due to repeated,
frequent failures
ene 25 09:49:52 squid.domain.lan polkitd[667]: Registered Authentication
Agent for unix-process:4051:7880379 (system bus
ene 25 09:49:52 squid.domain.lan squid[4056]: squid: ERROR: Could not send
signal 1 to process 4040: (3) No such process
ene 25 09:49:52 squid.domain.lan systemd[1]: squid.service: control process
exited, code=exited status=1
ene 25 09:49:52 squid.domain.lan squidshut.sh[4057]: Stopping Squid: Squid
settings file Falied the check
ene 25 09:49:52 squid.domain.lan systemd[1]: squid.service: control process
exited, code=exited status=1
ene 25 09:49:52 squid.domain.lan systemd[1]: Reload failed for Squid Web
Proxy Server.
-- Subject: Unit squid.service has finished reloading its configuration
-- Defined-By: systemd
-- Support: http://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit squid.service has finished reloading its configuration
-- 
-- The result is failed.
ene 25 09:49:52 squid.domain.lan systemd[1]: Unit squid.service entered
failed state.
ene 25 09:49:52 squid.domain.lan systemd[1]: squid.service failed.
lines 1462-1483/1484 100%
ene 25 09:49:40 squid.domain.lan squid[4040]: Squid plugin modules loaded: 0
ene 25 09:49:40 squid.domain.lan squid[4040]: Adaptation support is off.
ene 25 09:49:40 squid.domain.lan squid[4040]: Closing HTTP port
192.168.1.97:3128
ene 25 09:49:40 squid.domain.lan squid[4040]: Unable to open HTTP Socket
ene 25 09:49:40 squid.domain.lan squid[3997]: Squid Parent: (squid-1)
process 4040 exited with status 1
ene 25 09:49:40 squid.domain.lan squid[3997]: Squid Parent: (squid-1)
process 4040 will not be restarted due to repeated, f
ene 25 09:49:40 squid.domain.lan squid[3997]: Exiting due to repeated,
frequent failures
ene 25 09:49:52 squid.domain.lan polkitd[667]: Registered Authentication
Agent for unix-process:4051:7880379 (system bus na
ene 25 09:49:52 squid.domain.lan squid[4056]: squid: ERROR: Could not send
signal 1 to process 4040: (3) No such process
ene 25 09:49:52 squid.domain.lan systemd[1]: squid.service: control process
exited, code=exited status=1
ene 25 09:49:52 squid.domain.lan squidshut.sh[4057]: Stopping Squid: Squid
settings file Falied the check
ene 25 09:49:52 squid.domain.lan systemd[1]: squid.service: control process
exited, code=exited status=1
ene 25 09:49:52 squid.domain.lan systemd[1]: Reload failed for Squid Web
Proxy Server.
-- Subject: Unit squid.service has finished reloading its configuration
-- Defined-By: systemd
-- Support: http://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit squid.service has finished reloading its configuration
-- 
-- The result is failed.
ene 25 09:49:52 squid.domain.lan systemd[1]: Unit squid.service entered
failed state.
ene 25 09:49:52 squid.domain.lan systemd[1]: squid.service failed.
lines 1462-1483/1484 100%
ene 25 09:49:40 squid.domain.lan squid[4040]: Squid plugin modules loaded: 0
ene 25 09:49:40 squid.domain.lan squid[4040]: Adaptation support is off.
ene 25 09:49:40 squid.domain.lan squid[4040]: Closing HTTP port
192.168.1.97:3128
ene 25 09:49:40 squid.domain.lan squid[4040]: Unable to open HTTP Socket
ene 25 09:49:40 squid.domain.lan squid[3997]: Squid Parent: (squid-1)
process 4040 exited with status 1
ene 25 09:49:40 squid.domain.lan squid[3997]: Squid Parent: (squid-1)
process 4040 will not be restarted due to repeated, fr
ene 25 09:49:40 squid.domain.lan squid[3997]: Exiting due to repeated,
frequent failures
ene 25 09:49:52 squid.domain.lan polkitd[667]: Registered Authentication
Agent for unix-process:4051:7880379 (system bus nam
ene 25 09:49:52 squid.domain.lan squid[4056]: squid: ERROR: Could not send
signal 1 to process 4040: (3) No such process
ene 25 09:49:52 squid.domain.lan systemd[1]: squid.service: control process
exited, code=exited status=1
ene 25 09:49:52 squid.domain.lan squidshut.sh[4057]: Stopping Squid: Squid
settings file Falied the check
ene 25 09:49:52 squid.domain.lan systemd[1]: squid.service: control process
exited, code=exited status=1
ene 25 09:49:52 squid.domain.lan systemd[1]: Reload failed for Squid Web
Proxy Server.
-- Subject: Unit squid.service has finished reloading its configuration
-- Defined-By: systemd
-- Support: http://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit squid.service has finished reloading its configuration
-- 
-- The result is failed.
ene 25 09:49:52 squid.domain.lan systemd[1]: Unit squid.service entered
failed state.
ene 25 09:49:52 squid.domain.lan systemd[1]: squid.service failed.
lines 1462-1483/1484 100%
ene 25 09:49:40 squid.domain.lan squid[4040]: Squid plugin modules loaded: 0
ene 25 09:49:40 squid.domain.lan squid[4040]: Adaptation support is off.
ene 25 09:49:40 squid.domain.lan squid[4040]: Closing HTTP port
192.168.1.97:3128
ene 25 09:49:40 squid.domain.lan squid[4040]: Unable to open HTTP Socket
ene 25 09:49:40 squid.domain.lan squid[3997]: Squid Parent: (squid-1)
process 4040 exited with status 1
ene 25 09:49:40 squid.domain.lan squid[3997]: Squid Parent: (squid-1)
process 4040 will not be restarted due to repeated, freq
ene 25 09:49:40 squid.domain.lan squid[3997]: Exiting due to repeated,
frequent failures
ene 25 09:49:52 squid.domain.lan polkitd[667]: Registered Authentication
Agent for unix-process:4051:7880379 (system bus name 
ene 25 09:49:52 squid.domain.lan squid[4056]: squid: ERROR: Could not send
signal 1 to process 4040: (3) No such process
ene 25 09:49:52 squid.domain.lan systemd[1]: squid.service: control process
exited, code=exited status=1
ene 25 09:49:52 squid.domain.lan squidshut.sh[4057]: Stopping Squid: Squid
settings file Falied the check
ene 25 09:49:52 squid.domain.lan systemd[1]: squid.service: control process
exited, code=exited status=1
ene 25 09:49:52 squid.domain.lan systemd[1]: Reload failed for Squid Web
Proxy Server.
-- Subject: Unit squid.service has finished reloading its configuration
-- Defined-By: systemd
-- Support: http://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit squid.service has finished reloading its configuration
-- 
-- The result is failed.
ene 25 09:49:52 squid.domain.lan systemd[1]: Unit squid.service entered
failed state.
ene 25 09:49:52 squid.domain.lan systemd[1]: squid.service failed.
lines 1462-1483/1484 100%
ene 25 09:49:40 squid.domain.lan squid[4040]: Squid plugin modules loaded: 0
ene 25 09:49:40 squid.domain.lan squid[4040]: Adaptation support is off.
ene 25 09:49:40 squid.domain.lan squid[4040]: Closing HTTP port
192.168.1.97:3128
ene 25 09:49:40 squid.domain.lan squid[4040]: Unable to open HTTP Socket
ene 25 09:49:40 squid.domain.lan squid[3997]: Squid Parent: (squid-1)
process 4040 exited with status 1
ene 25 09:49:40 squid.domain.lan squid[3997]: Squid Parent: (squid-1)
process 4040 will not be restarted due to repeated, freque
ene 25 09:49:40 squid.domain.lan squid[3997]: Exiting due to repeated,
frequent failures
ene 25 09:49:52 squid.domain.lan polkitd[667]: Registered Authentication
Agent for unix-process:4051:7880379 (system bus name :1
ene 25 09:49:52 squid.domain.lan squid[4056]: squid: ERROR: Could not send
signal 1 to process 4040: (3) No such process
ene 25 09:49:52 squid.domain.lan systemd[1]: squid.service: control process
exited, code=exited status=1
ene 25 09:49:52 squid.domain.lan squidshut.sh[4057]: Stopping Squid: Squid
settings file Falied the check
ene 25 09:49:52 squid.domain.lan systemd[1]: squid.service: control process
exited, code=exited status=1
ene 25 09:49:52 squid.domain.lan systemd[1]: Reload failed for Squid Web
Proxy Server.
-- Subject: Unit squid.service has finished reloading its configuration
-- Defined-By: systemd
-- Support: http://lists.freedesktop.org/mailman/listinfo/systemd-devel
-- 
-- Unit squid.service has finished reloading its configuration
-- 
-- The result is failed.
ene 25 09:49:52 squid.domain.lan systemd[1]: Unit squid.service entered
failed state.
ene 25 09:49:52 squid.domain.lan systemd[1]: squid.service failed.
ene 25 09:49:52 squid.domain.lan polkitd[667]: Unregistered Authentication
Agent for unix-process:4051:7880379 (system bus name 

.


*cache.log*

2017/01/25 09:50:53 kid1| Service Name: squid
2017/01/25 09:50:53 kid1| Process ID 4118
2017/01/25 09:50:53 kid1| Process Roles: worker
2017/01/25 09:50:53 kid1| With 16384 file descriptors available
2017/01/25 09:50:53 kid1| Initializing IP Cache...
2017/01/25 09:50:53 kid1| DNS Socket created at [::], FD 9
2017/01/25 09:50:53 kid1| DNS Socket created at 0.0.0.0, FD 10
2017/01/25 09:50:53 kid1| Adding nameserver 8.8.8.8 from squid.conf
2017/01/25 09:50:53 kid1| Adding nameserver 8.8.4.4 from squid.conf
2017/01/25 09:50:53 kid1| helperOpenServers: Starting 5/32 'ssl_crtd'
processes
2017/01/25 09:50:54 kid1| '/usr/share/squid/errors/es-mod/ERR_READ_TIMEOUT':
(2) No such file or directory
2017/01/25 09:50:54 kid1| Logfile: opening log
daemon:/var/log/squid/access.log
2017/01/25 09:50:54 kid1| Logfile Daemon: opening log
/var/log/squid/access.log
2017/01/25 09:50:54 kid1| Unlinkd pipe opened on FD 27
2017/01/25 09:50:54 kid1| Local cache digest enabled; rebuild/rewrite every
3600/3600 sec
2017/01/25 09:50:54 kid1| Store logging disabled
2017/01/25 09:50:54 kid1| Swap maxSize 15360000 + 262144 KB, estimated
1201703 objects
2017/01/25 09:50:54 kid1| Target number of buckets: 60085
2017/01/25 09:50:54 kid1| Using 65536 Store buckets
2017/01/25 09:50:54 kid1| Max Mem  size: 262144 KB
2017/01/25 09:50:54 kid1| Max Swap size: 15360000 KB
2017/01/25 09:50:54 kid1| Rebuilding storage in /var/spool/squid (no log)
2017/01/25 09:50:54 kid1| Using Least Load store dir selection
2017/01/25 09:50:54 kid1| Set Current Directory to /var/spool/squid
2017/01/25 09:50:54 kid1| Finished loading MIME types and icons.
*2017/01/25 09:50:54 kid1| commBind: Cannot bind socket FD 31 to
192.168.1.97:3128: (98) Address already in use
*2017/01/25 09:50:54 kid1| HTCP Disabled.
2017/01/25 09:50:54 kid1| Squid plugin modules loaded: 0
2017/01/25 09:50:54 kid1| Adaptation support is off.
2017/01/25 09:50:54 kid1| Closing HTTP port 192.168.1.97:3128
FATAL: Unable to open HTTP Socket
Squid Cache (Version 3.5.20): Terminated abnormally.
CPU Usage: 0.110 seconds = 0.090 user + 0.020 sys
Maximum Resident Size: 151392 KB
Page faults with physical i/o: 0

What's going on?????????
Thanks to all.



--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/Strange-behavior-reload-service-failed-but-not-start-tp4681317.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From hardikdangar+squid at gmail.com  Wed Jan 25 13:38:53 2017
From: hardikdangar+squid at gmail.com (Hardik Dangar)
Date: Wed, 25 Jan 2017 19:08:53 +0530
Subject: [squid-users] Strange behavior - reload service failed,
	but not start....
In-Reply-To: <1485349951724-4681317.post@n4.nabble.com>
References: <1485349951724-4681317.post@n4.nabble.com>
Message-ID: <CA+sSnVbSgDGBSzTVNbomVOKzH5oAd+vKB0JGVdjxai3rjihHYA@mail.gmail.com>

see the output of,
df -h

and check any of your drives are full.

Because this message says,"WARNING: Disk space over limit: 19244596.00
KB > 15360000
KB"

On Wed, Jan 25, 2017 at 6:42 PM, erdosain9 <erdosain9 at gmail.com> wrote:

> Hi,
> I'm having this problem:
> if i reload the service (systemctl reload squid)... the service failed and
> dont reload... but, if i do systemctl start squid, all is working fine...
> this begin to happend after a bad reboot... (and after the bad reboot,
> squid
> dosent work for a moment giving this error:
> "2017/01/24 11:58:30 kid1| WARNING: Disk space over limit: 19244596.00 KB >
> 15360000 KB
> 2017/01/24 11:58:57 kid1| WARNING: Disk space over limit: 19134980.00 KB >
> 15360000 KB
> 2017/01/24 11:59:21 kid1| WARNING: Disk space over limit: 19021112.00 KB >
> 15360000 KB
> 2017/01/24 12:00:06 kid1| WARNING: Disk space over limit: 18804760.00 KB >
> 15360000 KB
> 2017/01/24 12:00:21 kid1| WARNING: Disk space over limit: 18640860.00 KB >
> 15360000 KB"
>
> After some stop start process, again is working but with that problem of
> "reload" that i mentioned before.
>
> this is some log
>
> [root at squid squid]# *systemctl status squid.service*
> ? squid.service - Squid Web Proxy Server
>    Loaded: loaded (/usr/lib/systemd/system/squid.service; enabled; vendor
> preset: disabled)
>    Active: failed (Result: exit-code) since mi? 2017-01-25 09:49:52 ART;
> 41s
> ago
>      Docs: man:squid(8)
>   Process: 4057 ExecStop=/usr/sbin/squidshut.sh (code=exited,
> status=1/FAILURE)
>   Process: 4056 ExecReload=/usr/sbin/squid -kreconf (code=exited,
> status=1/FAILURE)
>   Process: 3995 ExecStart=/usr/sbin/squid -sYC (code=exited,
> status=0/SUCCESS)
>   Process: 3993 ExecStartPre=/usr/bin/chown squid.squid /var/run/squid
> (code=exited, status=0/SUCCESS)
>   Process: 3992 ExecStartPre=/usr/bin/mkdir -p /var/run/squid (code=exited,
> status=0/SUCCESS)
>  Main PID: 4020
>    CGroup: /system.slice/squid.service
>            ??2228 /usr/sbin/squid -sYC
>            ??2230 (squid-1) -sYC
>            ??2231 (ssl_crtd) -s /var/lib/ssl_db -M 4MB
>            ??2232 (ssl_crtd) -s /var/lib/ssl_db -M 4MB
>            ??2233 (ssl_crtd) -s /var/lib/ssl_db -M 4MB
>            ??2234 (ssl_crtd) -s /var/lib/ssl_db -M 4MB
>            ??2235 (ssl_crtd) -s /var/lib/ssl_db -M 4MB
>            ??2236 (logfile-daemon) /var/log/squid/access.log
>            ??2237 (unlinkd)
>            ??2238 diskd 2283524 2283525 2283526
>            ??2239 (ssl_crtd) -s /var/lib/ssl_db -M 4MB
>            ??2240 (ssl_crtd) -s /var/lib/ssl_db -M 4MB
>            ??2241 (ssl_crtd) -s /var/lib/ssl_db -M 4MB
>            ??2242 (ssl_crtd) -s /var/lib/ssl_db -M 4MB
>            ??2243 (ssl_crtd) -s /var/lib/ssl_db -M 4MB
>            ??2244 (ssl_crtd) -s /var/lib/ssl_db -M 4MB
>            ??2245 (ssl_crtd) -s /var/lib/ssl_db -M 4MB
>            ??2246 (ssl_crtd) -s /var/lib/ssl_db -M 4MB
>            ??2247 (ssl_crtd) -s /var/lib/ssl_db -M 4MB
>            ??2248 (ssl_crtd) -s /var/lib/ssl_db -M 4MB
>            ??2278 (ssl_crtd) -s /var/lib/ssl_db -M 4MB
>            ??2325 (ssl_crtd) -s /var/lib/ssl_db -M 4MB
>            ??2368 (ssl_crtd) -s /var/lib/ssl_db -M 4MB
>            ??2369 (ssl_crtd) -s /var/lib/ssl_db -M 4MB
>            ??2371 (ssl_crtd) -s /var/lib/ssl_db -M 4MB
>            ??2397 (ssl_crtd) -s /var/lib/ssl_db -M 4MB
>            ??2398 (ssl_crtd) -s /var/lib/ssl_db -M 4MB
>            ??2399 (ssl_crtd) -s /var/lib/ssl_db -M 4MB
>            ??2400 (ssl_crtd) -s /var/lib/ssl_db -M 4MB
>            ??2401 (ssl_crtd) -s /var/lib/ssl_db -M 4MB
>            ??2402 (ssl_crtd) -s /var/lib/ssl_db -M 4MB
>            ??2403 (ssl_crtd) -s /var/lib/ssl_db -M 4MB
>            ??2404 (ssl_crtd) -s /var/lib/ssl_db -M 4MB
>            ??2405 (ssl_crtd) -s /var/lib/ssl_db -M 4MB
>            ??2406 (ssl_crtd) -s /var/lib/ssl_db -M 4MB
>            ??2407 (ssl_crtd) -s /var/lib/ssl_db -M 4MB
>            ??2408 (ssl_crtd) -s /var/lib/ssl_db -M 4MB
>
> ene 25 09:49:40 squid.domain.lan squid[4040]: Rebuilding storage in
> /var/spool/squid (no log)
> ene 25 09:49:40 squid.domain.lan squid[4040]: Using Least Load store dir
> selection
> ene 25 09:49:40 squid.domain.lan squid[4040]: Set Current Directory to
> /var/spool/squid
> ene 25 09:49:52 squid.domain.lan squid[4056]: squid: ERROR: Could not send
> signal 1 to process 4040: (3) No such process
> ene 25 09:49:52 squid.domain.lan systemd[1]: squid.service: control process
> exited, code=exited status=1
> ene 25 09:49:52 squid.domain.lan squidshut.sh[4057]: Stopping Squid: Squid
> settings file Falied the check
> ene 25 09:49:52 squid.domain.lan systemd[1]: squid.service: control process
> exited, code=exited status=1
> ene 25 09:49:52 squid.domain.lan systemd[1]: Reload failed for Squid Web
> Proxy Server.
> ene 25 09:49:52 squid.domain.lan systemd[1]: Unit squid.service entered
> failed state.
> ene 25 09:49:52 squid.domain.lan systemd[1]: squid.service failed
>
> [root at squid squid]# *journalctl -xe*
> ene 25 09:49:40 squid.domain.lan squid[4040]: Squid plugin modules loade
> ene 25 09:49:40 squid.domain.lan squid[4040]: Adaptation support is off.
> ene 25 09:49:40 squid.domain.lan squid[4040]: Closing HTTP port 192.168.
> ene 25 09:49:40 squid.domain.lan squid[4040]: Unable to open HTTP Socket
> ene 25 09:49:40 squid.domain.lan squid[3997]: Squid Parent: (squid-1) pr
> ene 25 09:49:40 squid.domain.lan squid[3997]: Squid Parent: (squid-1) pr
> ene 25 09:49:40 squid.domain.lan squid[3997]: Exiting due to repeated, f
> ene 25 09:49:52 squid.domain.lan polkitd[667]: Registered Authentication
> ene 25 09:49:52 squid.domain.lan squid[4056]: squid: ERROR: Could not se
> ene 25 09:49:52 squid.domain.lan systemd[1]: squid.service: control proc
> ene 25 09:49:52 squid.domain.lan squidshut.sh[4057]: Stopping Squid: Squ
> ene 25 09:49:52 squid.domain.lan systemd[1]: squid.service: control proc
> ene 25 09:49:52 squid.domain.lan systemd[1]: Reload failed for Squid Web
> -- Subject: Unit squid.service has finished reloading its configuration
> -- Defined-By: systemd
> -- Support: http://lists.freedesktop.org/mailman/listinfo/systemd-devel
> --
> -- Unit squid.service has finished reloading its configuration
> --
> -- The result is failed.
> ene 25 09:49:52 squid.domain.lan systemd[1]: Unit squid.service entered
> ene 25 09:49:52 squid.domain.lan systemd[1]: squid.service failed.
> ene 25 09:49:52 squid.domain.lan polkitd[667]: Unregistered Authenticati
> lines 1462-1484/1484 (END)
> ene 25 09:49:40 squid.domain.lan squid[4040]: Squid plugin modules loade
> ene 25 09:49:40 squid.domain.lan squid[4040]: Adaptation support is off.
> ene 25 09:49:40 squid.domain.lan squid[4040]: Closing HTTP port 192.168.
> ene 25 09:49:40 squid.domain.lan squid[4040]: Unable to open HTTP Socket
> ene 25 09:49:40 squid.domain.lan squid[3997]: Squid Parent: (squid-1) pr
> ene 25 09:49:40 squid.domain.lan squid[3997]: Squid Parent: (squid-1) pr
> ene 25 09:49:40 squid.domain.lan squid[3997]: Exiting due to repeated, f
> ene 25 09:49:52 squid.domain.lan polkitd[667]: Registered Authentication
> ene 25 09:49:52 squid.domain.lan squid[4056]: squid: ERROR: Could not se
> ene 25 09:49:52 squid.domain.lan systemd[1]: squid.service: control proc
> ene 25 09:49:52 squid.domain.lan squidshut.sh[4057]: Stopping Squid: Squ
> ene 25 09:49:52 squid.domain.lan systemd[1]: squid.service: control proc
> ene 25 09:49:52 squid.domain.lan systemd[1]: Reload failed for Squid Web
> -- Subject: Unit squid.service has finished reloading its configuration
> -- Defined-By: systemd
> -- Support: http://lists.freedesktop.org/mailman/listinfo/systemd-devel
> --
> -- Unit squid.service has finished reloading its configuration
> --
> -- The result is failed.
> ene 25 09:49:52 squid.domain.lan systemd[1]: Unit squid.service entered
> ene 25 09:49:52 squid.domain.lan systemd[1]: squid.service failed.
> lines 1462-1483/1484 100%
> ene 25 09:49:40 squid.domain.lan squid[4040]: Squid plugin modules loaded:
> ene 25 09:49:40 squid.domain.lan squid[4040]: Adaptation support is off.
> ene 25 09:49:40 squid.domain.lan squid[4040]: Closing HTTP port 192.168.1.
> ene 25 09:49:40 squid.domain.lan squid[4040]: Unable to open HTTP Socket
> ene 25 09:49:40 squid.domain.lan squid[3997]: Squid Parent: (squid-1) proc
> ene 25 09:49:40 squid.domain.lan squid[3997]: Squid Parent: (squid-1) proc
> ene 25 09:49:40 squid.domain.lan squid[3997]: Exiting due to repeated, fre
> ene 25 09:49:52 squid.domain.lan polkitd[667]: Registered Authentication A
> ene 25 09:49:52 squid.domain.lan squid[4056]: squid: ERROR: Could not send
> ene 25 09:49:52 squid.domain.lan systemd[1]: squid.service: control proces
> ene 25 09:49:52 squid.domain.lan squidshut.sh[4057]: Stopping Squid: Squid
> ene 25 09:49:52 squid.domain.lan systemd[1]: squid.service: control proces
> ene 25 09:49:52 squid.domain.lan systemd[1]: Reload failed for Squid Web P
> -- Subject: Unit squid.service has finished reloading its configuration
> -- Defined-By: systemd
> -- Support: http://lists.freedesktop.org/mailman/listinfo/systemd-devel
> --
> -- Unit squid.service has finished reloading its configuration
> --
> -- The result is failed.
> ene 25 09:49:52 squid.domain.lan systemd[1]: Unit squid.service entered fa
> ene 25 09:49:52 squid.domain.lan systemd[1]: squid.service failed.
> lines 1462-1483/1484 100%
> ene 25 09:49:40 squid.domain.lan squid[4040]: Squid plugin modules loaded:
> 0
> ene 25 09:49:40 squid.domain.lan squid[4040]: Adaptation support is off.
> ene 25 09:49:40 squid.domain.lan squid[4040]: Closing HTTP port
> 192.168.1.97:3128
> ene 25 09:49:40 squid.domain.lan squid[4040]: Unable to open HTTP Socket
> ene 25 09:49:40 squid.domain.lan squid[3997]: Squid Parent: (squid-1)
> process 4040 exited wit
> ene 25 09:49:40 squid.domain.lan squid[3997]: Squid Parent: (squid-1)
> process 4040 will not b
> ene 25 09:49:40 squid.domain.lan squid[3997]: Exiting due to repeated,
> frequent failures
> ene 25 09:49:52 squid.domain.lan polkitd[667]: Registered Authentication
> Agent for unix-proce
> ene 25 09:49:52 squid.domain.lan squid[4056]: squid: ERROR: Could not send
> signal 1 to proces
> ene 25 09:49:52 squid.domain.lan systemd[1]: squid.service: control process
> exited, code=exit
> ene 25 09:49:52 squid.domain.lan squidshut.sh[4057]: Stopping Squid: Squid
> settings file Fali
> ene 25 09:49:52 squid.domain.lan systemd[1]: squid.service: control process
> exited, code=exit
> ene 25 09:49:52 squid.domain.lan systemd[1]: Reload failed for Squid Web
> Proxy Server.
> -- Subject: Unit squid.service has finished reloading its configuration
> -- Defined-By: systemd
> -- Support: http://lists.freedesktop.org/mailman/listinfo/systemd-devel
> --
> -- Unit squid.service has finished reloading its configuration
> --
> -- The result is failed.
> ene 25 09:49:52 squid.domain.lan systemd[1]: Unit squid.service entered
> failed state.
> ene 25 09:49:52 squid.domain.lan systemd[1]: squid.service failed.
> lines 1462-1483/1484 100%
> ene 25 09:49:40 squid.domain.lan squid[4040]: Squid plugin modules loaded:
> 0
> ene 25 09:49:40 squid.domain.lan squid[4040]: Adaptation support is off.
> ene 25 09:49:40 squid.domain.lan squid[4040]: Closing HTTP port
> 192.168.1.97:3128
> ene 25 09:49:40 squid.domain.lan squid[4040]: Unable to open HTTP Socket
> ene 25 09:49:40 squid.domain.lan squid[3997]: Squid Parent: (squid-1)
> process 4040 exited with stat
> ene 25 09:49:40 squid.domain.lan squid[3997]: Squid Parent: (squid-1)
> process 4040 will not be rest
> ene 25 09:49:40 squid.domain.lan squid[3997]: Exiting due to repeated,
> frequent failures
> ene 25 09:49:52 squid.domain.lan polkitd[667]: Registered Authentication
> Agent for unix-process:405
> ene 25 09:49:52 squid.domain.lan squid[4056]: squid: ERROR: Could not send
> signal 1 to process 4040
> ene 25 09:49:52 squid.domain.lan systemd[1]: squid.service: control process
> exited, code=exited sta
> ene 25 09:49:52 squid.domain.lan squidshut.sh[4057]: Stopping Squid: Squid
> settings file Falied the
> ene 25 09:49:52 squid.domain.lan systemd[1]: squid.service: control process
> exited, code=exited sta
> ene 25 09:49:52 squid.domain.lan systemd[1]: Reload failed for Squid Web
> Proxy Server.
> -- Subject: Unit squid.service has finished reloading its configuration
> -- Defined-By: systemd
> -- Support: http://lists.freedesktop.org/mailman/listinfo/systemd-devel
> --
> -- Unit squid.service has finished reloading its configuration
> --
> -- The result is failed.
> ene 25 09:49:52 squid.domain.lan systemd[1]: Unit squid.service entered
> failed state.
> ene 25 09:49:52 squid.domain.lan systemd[1]: squid.service failed.
> lines 1462-1483/1484 100%
> ene 25 09:49:40 squid.domain.lan squid[4040]: Squid plugin modules loaded:
> 0
> ene 25 09:49:40 squid.domain.lan squid[4040]: Adaptation support is off.
> ene 25 09:49:40 squid.domain.lan squid[4040]: Closing HTTP port
> 192.168.1.97:3128
> ene 25 09:49:40 squid.domain.lan squid[4040]: Unable to open HTTP Socket
> ene 25 09:49:40 squid.domain.lan squid[3997]: Squid Parent: (squid-1)
> process 4040 exited with status 1
> ene 25 09:49:40 squid.domain.lan squid[3997]: Squid Parent: (squid-1)
> process 4040 will not be restarted due
> ene 25 09:49:40 squid.domain.lan squid[3997]: Exiting due to repeated,
> frequent failures
> ene 25 09:49:52 squid.domain.lan polkitd[667]: Registered Authentication
> Agent for unix-process:4051:7880379
> ene 25 09:49:52 squid.domain.lan squid[4056]: squid: ERROR: Could not send
> signal 1 to process 4040: (3) No s
> ene 25 09:49:52 squid.domain.lan systemd[1]: squid.service: control process
> exited, code=exited status=1
> ene 25 09:49:52 squid.domain.lan squidshut.sh[4057]: Stopping Squid: Squid
> settings file Falied the check
> ene 25 09:49:52 squid.domain.lan systemd[1]: squid.service: control process
> exited, code=exited status=1
> ene 25 09:49:52 squid.domain.lan systemd[1]: Reload failed for Squid Web
> Proxy Server.
> -- Subject: Unit squid.service has finished reloading its configuration
> -- Defined-By: systemd
> -- Support: http://lists.freedesktop.org/mailman/listinfo/systemd-devel
> --
> -- Unit squid.service has finished reloading its configuration
> --
> -- The result is failed.
> ene 25 09:49:52 squid.domain.lan systemd[1]: Unit squid.service entered
> failed state.
> ene 25 09:49:52 squid.domain.lan systemd[1]: squid.service failed.
> lines 1462-1483/1484 100%
> ene 25 09:49:40 squid.domain.lan squid[4040]: Squid plugin modules loaded:
> 0
> ene 25 09:49:40 squid.domain.lan squid[4040]: Adaptation support is off.
> ene 25 09:49:40 squid.domain.lan squid[4040]: Closing HTTP port
> 192.168.1.97:3128
> ene 25 09:49:40 squid.domain.lan squid[4040]: Unable to open HTTP Socket
> ene 25 09:49:40 squid.domain.lan squid[3997]: Squid Parent: (squid-1)
> process 4040 exited with status 1
> ene 25 09:49:40 squid.domain.lan squid[3997]: Squid Parent: (squid-1)
> process 4040 will not be restarted due to rep
> ene 25 09:49:40 squid.domain.lan squid[3997]: Exiting due to repeated,
> frequent failures
> ene 25 09:49:52 squid.domain.lan polkitd[667]: Registered Authentication
> Agent for unix-process:4051:7880379 (syste
> ene 25 09:49:52 squid.domain.lan squid[4056]: squid: ERROR: Could not send
> signal 1 to process 4040: (3) No such pr
> ene 25 09:49:52 squid.domain.lan systemd[1]: squid.service: control process
> exited, code=exited status=1
> ene 25 09:49:52 squid.domain.lan squidshut.sh[4057]: Stopping Squid: Squid
> settings file Falied the check
> ene 25 09:49:52 squid.domain.lan systemd[1]: squid.service: control process
> exited, code=exited status=1
> ene 25 09:49:52 squid.domain.lan systemd[1]: Reload failed for Squid Web
> Proxy Server.
> -- Subject: Unit squid.service has finished reloading its configuration
> -- Defined-By: systemd
> -- Support: http://lists.freedesktop.org/mailman/listinfo/systemd-devel
> --
> -- Unit squid.service has finished reloading its configuration
> --
> -- The result is failed.
> ene 25 09:49:52 squid.domain.lan systemd[1]: Unit squid.service entered
> failed state.
> ene 25 09:49:52 squid.domain.lan systemd[1]: squid.service failed.
> lines 1462-1483/1484 100%
> ene 25 09:49:40 squid.domain.lan squid[4040]: Squid plugin modules loaded:
> 0
> ene 25 09:49:40 squid.domain.lan squid[4040]: Adaptation support is off.
> ene 25 09:49:40 squid.domain.lan squid[4040]: Closing HTTP port
> 192.168.1.97:3128
> ene 25 09:49:40 squid.domain.lan squid[4040]: Unable to open HTTP Socket
> ene 25 09:49:40 squid.domain.lan squid[3997]: Squid Parent: (squid-1)
> process 4040 exited with status 1
> ene 25 09:49:40 squid.domain.lan squid[3997]: Squid Parent: (squid-1)
> process 4040 will not be restarted due to repe
> ene 25 09:49:40 squid.domain.lan squid[3997]: Exiting due to repeated,
> frequent failures
> ene 25 09:49:52 squid.domain.lan polkitd[667]: Registered Authentication
> Agent for unix-process:4051:7880379 (system
> ene 25 09:49:52 squid.domain.lan squid[4056]: squid: ERROR: Could not send
> signal 1 to process 4040: (3) No such pro
> ene 25 09:49:52 squid.domain.lan systemd[1]: squid.service: control process
> exited, code=exited status=1
> ene 25 09:49:52 squid.domain.lan squidshut.sh[4057]: Stopping Squid: Squid
> settings file Falied the check
> ene 25 09:49:52 squid.domain.lan systemd[1]: squid.service: control process
> exited, code=exited status=1
> ene 25 09:49:52 squid.domain.lan systemd[1]: Reload failed for Squid Web
> Proxy Server.
> -- Subject: Unit squid.service has finished reloading its configuration
> -- Defined-By: systemd
> -- Support: http://lists.freedesktop.org/mailman/listinfo/systemd-devel
> --
> -- Unit squid.service has finished reloading its configuration
> --
> -- The result is failed.
> ene 25 09:49:52 squid.domain.lan systemd[1]: Unit squid.service entered
> failed state.
> ene 25 09:49:52 squid.domain.lan systemd[1]: squid.service failed.
> lines 1462-1483/1484 100%
> ene 25 09:49:40 squid.domain.lan squid[4040]: Squid plugin modules loaded:
> 0
> ene 25 09:49:40 squid.domain.lan squid[4040]: Adaptation support is off.
> ene 25 09:49:40 squid.domain.lan squid[4040]: Closing HTTP port
> 192.168.1.97:3128
> ene 25 09:49:40 squid.domain.lan squid[4040]: Unable to open HTTP Socket
> ene 25 09:49:40 squid.domain.lan squid[3997]: Squid Parent: (squid-1)
> process 4040 exited with status 1
> ene 25 09:49:40 squid.domain.lan squid[3997]: Squid Parent: (squid-1)
> process 4040 will not be restarted due to repea
> ene 25 09:49:40 squid.domain.lan squid[3997]: Exiting due to repeated,
> frequent failures
> ene 25 09:49:52 squid.domain.lan polkitd[667]: Registered Authentication
> Agent for unix-process:4051:7880379 (system
> ene 25 09:49:52 squid.domain.lan squid[4056]: squid: ERROR: Could not send
> signal 1 to process 4040: (3) No such proc
> ene 25 09:49:52 squid.domain.lan systemd[1]: squid.service: control process
> exited, code=exited status=1
> ene 25 09:49:52 squid.domain.lan squidshut.sh[4057]: Stopping Squid: Squid
> settings file Falied the check
> ene 25 09:49:52 squid.domain.lan systemd[1]: squid.service: control process
> exited, code=exited status=1
> ene 25 09:49:52 squid.domain.lan systemd[1]: Reload failed for Squid Web
> Proxy Server.
> -- Subject: Unit squid.service has finished reloading its configuration
> -- Defined-By: systemd
> -- Support: http://lists.freedesktop.org/mailman/listinfo/systemd-devel
> --
> -- Unit squid.service has finished reloading its configuration
> --
> -- The result is failed.
> ene 25 09:49:52 squid.domain.lan systemd[1]: Unit squid.service entered
> failed state.
> ene 25 09:49:52 squid.domain.lan systemd[1]: squid.service failed.
> lines 1462-1483/1484 100%
> ene 25 09:49:40 squid.domain.lan squid[4040]: Squid plugin modules loaded:
> 0
> ene 25 09:49:40 squid.domain.lan squid[4040]: Adaptation support is off.
> ene 25 09:49:40 squid.domain.lan squid[4040]: Closing HTTP port
> 192.168.1.97:3128
> ene 25 09:49:40 squid.domain.lan squid[4040]: Unable to open HTTP Socket
> ene 25 09:49:40 squid.domain.lan squid[3997]: Squid Parent: (squid-1)
> process 4040 exited with status 1
> ene 25 09:49:40 squid.domain.lan squid[3997]: Squid Parent: (squid-1)
> process 4040 will not be restarted due to repeated
> ene 25 09:49:40 squid.domain.lan squid[3997]: Exiting due to repeated,
> frequent failures
> ene 25 09:49:52 squid.domain.lan polkitd[667]: Registered Authentication
> Agent for unix-process:4051:7880379 (system bus
> ene 25 09:49:52 squid.domain.lan squid[4056]: squid: ERROR: Could not send
> signal 1 to process 4040: (3) No such process
> ene 25 09:49:52 squid.domain.lan systemd[1]: squid.service: control process
> exited, code=exited status=1
> ene 25 09:49:52 squid.domain.lan squidshut.sh[4057]: Stopping Squid: Squid
> settings file Falied the check
> ene 25 09:49:52 squid.domain.lan systemd[1]: squid.service: control process
> exited, code=exited status=1
> ene 25 09:49:52 squid.domain.lan systemd[1]: Reload failed for Squid Web
> Proxy Server.
> -- Subject: Unit squid.service has finished reloading its configuration
> -- Defined-By: systemd
> -- Support: http://lists.freedesktop.org/mailman/listinfo/systemd-devel
> --
> -- Unit squid.service has finished reloading its configuration
> --
> -- The result is failed.
> ene 25 09:49:52 squid.domain.lan systemd[1]: Unit squid.service entered
> failed state.
> ene 25 09:49:52 squid.domain.lan systemd[1]: squid.service failed.
> lines 1462-1483/1484 100%
> ene 25 09:49:40 squid.domain.lan squid[4040]: Squid plugin modules loaded:
> 0
> ene 25 09:49:40 squid.domain.lan squid[4040]: Adaptation support is off.
> ene 25 09:49:40 squid.domain.lan squid[4040]: Closing HTTP port
> 192.168.1.97:3128
> ene 25 09:49:40 squid.domain.lan squid[4040]: Unable to open HTTP Socket
> ene 25 09:49:40 squid.domain.lan squid[3997]: Squid Parent: (squid-1)
> process 4040 exited with status 1
> ene 25 09:49:40 squid.domain.lan squid[3997]: Squid Parent: (squid-1)
> process 4040 will not be restarted due to repeated, f
> ene 25 09:49:40 squid.domain.lan squid[3997]: Exiting due to repeated,
> frequent failures
> ene 25 09:49:52 squid.domain.lan polkitd[667]: Registered Authentication
> Agent for unix-process:4051:7880379 (system bus na
> ene 25 09:49:52 squid.domain.lan squid[4056]: squid: ERROR: Could not send
> signal 1 to process 4040: (3) No such process
> ene 25 09:49:52 squid.domain.lan systemd[1]: squid.service: control process
> exited, code=exited status=1
> ene 25 09:49:52 squid.domain.lan squidshut.sh[4057]: Stopping Squid: Squid
> settings file Falied the check
> ene 25 09:49:52 squid.domain.lan systemd[1]: squid.service: control process
> exited, code=exited status=1
> ene 25 09:49:52 squid.domain.lan systemd[1]: Reload failed for Squid Web
> Proxy Server.
> -- Subject: Unit squid.service has finished reloading its configuration
> -- Defined-By: systemd
> -- Support: http://lists.freedesktop.org/mailman/listinfo/systemd-devel
> --
> -- Unit squid.service has finished reloading its configuration
> --
> -- The result is failed.
> ene 25 09:49:52 squid.domain.lan systemd[1]: Unit squid.service entered
> failed state.
> ene 25 09:49:52 squid.domain.lan systemd[1]: squid.service failed.
> lines 1462-1483/1484 100%
> ene 25 09:49:40 squid.domain.lan squid[4040]: Squid plugin modules loaded:
> 0
> ene 25 09:49:40 squid.domain.lan squid[4040]: Adaptation support is off.
> ene 25 09:49:40 squid.domain.lan squid[4040]: Closing HTTP port
> 192.168.1.97:3128
> ene 25 09:49:40 squid.domain.lan squid[4040]: Unable to open HTTP Socket
> ene 25 09:49:40 squid.domain.lan squid[3997]: Squid Parent: (squid-1)
> process 4040 exited with status 1
> ene 25 09:49:40 squid.domain.lan squid[3997]: Squid Parent: (squid-1)
> process 4040 will not be restarted due to repeated, fr
> ene 25 09:49:40 squid.domain.lan squid[3997]: Exiting due to repeated,
> frequent failures
> ene 25 09:49:52 squid.domain.lan polkitd[667]: Registered Authentication
> Agent for unix-process:4051:7880379 (system bus nam
> ene 25 09:49:52 squid.domain.lan squid[4056]: squid: ERROR: Could not send
> signal 1 to process 4040: (3) No such process
> ene 25 09:49:52 squid.domain.lan systemd[1]: squid.service: control process
> exited, code=exited status=1
> ene 25 09:49:52 squid.domain.lan squidshut.sh[4057]: Stopping Squid: Squid
> settings file Falied the check
> ene 25 09:49:52 squid.domain.lan systemd[1]: squid.service: control process
> exited, code=exited status=1
> ene 25 09:49:52 squid.domain.lan systemd[1]: Reload failed for Squid Web
> Proxy Server.
> -- Subject: Unit squid.service has finished reloading its configuration
> -- Defined-By: systemd
> -- Support: http://lists.freedesktop.org/mailman/listinfo/systemd-devel
> --
> -- Unit squid.service has finished reloading its configuration
> --
> -- The result is failed.
> ene 25 09:49:52 squid.domain.lan systemd[1]: Unit squid.service entered
> failed state.
> ene 25 09:49:52 squid.domain.lan systemd[1]: squid.service failed.
> lines 1462-1483/1484 100%
> ene 25 09:49:40 squid.domain.lan squid[4040]: Squid plugin modules loaded:
> 0
> ene 25 09:49:40 squid.domain.lan squid[4040]: Adaptation support is off.
> ene 25 09:49:40 squid.domain.lan squid[4040]: Closing HTTP port
> 192.168.1.97:3128
> ene 25 09:49:40 squid.domain.lan squid[4040]: Unable to open HTTP Socket
> ene 25 09:49:40 squid.domain.lan squid[3997]: Squid Parent: (squid-1)
> process 4040 exited with status 1
> ene 25 09:49:40 squid.domain.lan squid[3997]: Squid Parent: (squid-1)
> process 4040 will not be restarted due to repeated, freq
> ene 25 09:49:40 squid.domain.lan squid[3997]: Exiting due to repeated,
> frequent failures
> ene 25 09:49:52 squid.domain.lan polkitd[667]: Registered Authentication
> Agent for unix-process:4051:7880379 (system bus name
> ene 25 09:49:52 squid.domain.lan squid[4056]: squid: ERROR: Could not send
> signal 1 to process 4040: (3) No such process
> ene 25 09:49:52 squid.domain.lan systemd[1]: squid.service: control process
> exited, code=exited status=1
> ene 25 09:49:52 squid.domain.lan squidshut.sh[4057]: Stopping Squid: Squid
> settings file Falied the check
> ene 25 09:49:52 squid.domain.lan systemd[1]: squid.service: control process
> exited, code=exited status=1
> ene 25 09:49:52 squid.domain.lan systemd[1]: Reload failed for Squid Web
> Proxy Server.
> -- Subject: Unit squid.service has finished reloading its configuration
> -- Defined-By: systemd
> -- Support: http://lists.freedesktop.org/mailman/listinfo/systemd-devel
> --
> -- Unit squid.service has finished reloading its configuration
> --
> -- The result is failed.
> ene 25 09:49:52 squid.domain.lan systemd[1]: Unit squid.service entered
> failed state.
> ene 25 09:49:52 squid.domain.lan systemd[1]: squid.service failed.
> lines 1462-1483/1484 100%
> ene 25 09:49:40 squid.domain.lan squid[4040]: Squid plugin modules loaded:
> 0
> ene 25 09:49:40 squid.domain.lan squid[4040]: Adaptation support is off.
> ene 25 09:49:40 squid.domain.lan squid[4040]: Closing HTTP port
> 192.168.1.97:3128
> ene 25 09:49:40 squid.domain.lan squid[4040]: Unable to open HTTP Socket
> ene 25 09:49:40 squid.domain.lan squid[3997]: Squid Parent: (squid-1)
> process 4040 exited with status 1
> ene 25 09:49:40 squid.domain.lan squid[3997]: Squid Parent: (squid-1)
> process 4040 will not be restarted due to repeated, freque
> ene 25 09:49:40 squid.domain.lan squid[3997]: Exiting due to repeated,
> frequent failures
> ene 25 09:49:52 squid.domain.lan polkitd[667]: Registered Authentication
> Agent for unix-process:4051:7880379 (system bus name :1
> ene 25 09:49:52 squid.domain.lan squid[4056]: squid: ERROR: Could not send
> signal 1 to process 4040: (3) No such process
> ene 25 09:49:52 squid.domain.lan systemd[1]: squid.service: control process
> exited, code=exited status=1
> ene 25 09:49:52 squid.domain.lan squidshut.sh[4057]: Stopping Squid: Squid
> settings file Falied the check
> ene 25 09:49:52 squid.domain.lan systemd[1]: squid.service: control process
> exited, code=exited status=1
> ene 25 09:49:52 squid.domain.lan systemd[1]: Reload failed for Squid Web
> Proxy Server.
> -- Subject: Unit squid.service has finished reloading its configuration
> -- Defined-By: systemd
> -- Support: http://lists.freedesktop.org/mailman/listinfo/systemd-devel
> --
> -- Unit squid.service has finished reloading its configuration
> --
> -- The result is failed.
> ene 25 09:49:52 squid.domain.lan systemd[1]: Unit squid.service entered
> failed state.
> ene 25 09:49:52 squid.domain.lan systemd[1]: squid.service failed.
> ene 25 09:49:52 squid.domain.lan polkitd[667]: Unregistered Authentication
> Agent for unix-process:4051:7880379 (system bus name
>
> .
>
>
> *cache.log*
>
> 2017/01/25 09:50:53 kid1| Service Name: squid
> 2017/01/25 09:50:53 kid1| Process ID 4118
> 2017/01/25 09:50:53 kid1| Process Roles: worker
> 2017/01/25 09:50:53 kid1| With 16384 file descriptors available
> 2017/01/25 09:50:53 kid1| Initializing IP Cache...
> 2017/01/25 09:50:53 kid1| DNS Socket created at [::], FD 9
> 2017/01/25 09:50:53 kid1| DNS Socket created at 0.0.0.0, FD 10
> 2017/01/25 09:50:53 kid1| Adding nameserver 8.8.8.8 from squid.conf
> 2017/01/25 09:50:53 kid1| Adding nameserver 8.8.4.4 from squid.conf
> 2017/01/25 09:50:53 kid1| helperOpenServers: Starting 5/32 'ssl_crtd'
> processes
> 2017/01/25 09:50:54 kid1| '/usr/share/squid/errors/es-
> mod/ERR_READ_TIMEOUT':
> (2) No such file or directory
> 2017/01/25 09:50:54 kid1| Logfile: opening log
> daemon:/var/log/squid/access.log
> 2017/01/25 09:50:54 kid1| Logfile Daemon: opening log
> /var/log/squid/access.log
> 2017/01/25 09:50:54 kid1| Unlinkd pipe opened on FD 27
> 2017/01/25 09:50:54 kid1| Local cache digest enabled; rebuild/rewrite every
> 3600/3600 sec
> 2017/01/25 09:50:54 kid1| Store logging disabled
> 2017/01/25 09:50:54 kid1| Swap maxSize 15360000 + 262144 KB, estimated
> 1201703 objects
> 2017/01/25 09:50:54 kid1| Target number of buckets: 60085
> 2017/01/25 09:50:54 kid1| Using 65536 Store buckets
> 2017/01/25 09:50:54 kid1| Max Mem  size: 262144 KB
> 2017/01/25 09:50:54 kid1| Max Swap size: 15360000 KB
> 2017/01/25 09:50:54 kid1| Rebuilding storage in /var/spool/squid (no log)
> 2017/01/25 09:50:54 kid1| Using Least Load store dir selection
> 2017/01/25 09:50:54 kid1| Set Current Directory to /var/spool/squid
> 2017/01/25 09:50:54 kid1| Finished loading MIME types and icons.
> *2017/01/25 09:50:54 kid1| commBind: Cannot bind socket FD 31 to
> 192.168.1.97:3128: (98) Address already in use
> *2017/01/25 09:50:54 kid1| HTCP Disabled.
> 2017/01/25 09:50:54 kid1| Squid plugin modules loaded: 0
> 2017/01/25 09:50:54 kid1| Adaptation support is off.
> 2017/01/25 09:50:54 kid1| Closing HTTP port 192.168.1.97:3128
> FATAL: Unable to open HTTP Socket
> Squid Cache (Version 3.5.20): Terminated abnormally.
> CPU Usage: 0.110 seconds = 0.090 user + 0.020 sys
> Maximum Resident Size: 151392 KB
> Page faults with physical i/o: 0
>
> What's going on?????????
> Thanks to all.
>
>
>
> --
> View this message in context: http://squid-web-proxy-cache.
> 1019090.n4.nabble.com/Strange-behavior-reload-service-
> failed-but-not-start-tp4681317.html
> Sent from the Squid - Users mailing list archive at Nabble.com.
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20170125/56fa5a7c/attachment.htm>

From yvoinov at gmail.com  Wed Jan 25 14:24:33 2017
From: yvoinov at gmail.com (Yuri Voinov)
Date: Wed, 25 Jan 2017 20:24:33 +0600
Subject: [squid-users] Squid 4.x: Intermediate certificates downloader
In-Reply-To: <741d243a-1750-30c9-f676-45a772038a69@measurement-factory.com>
References: <4198a8d7-07c7-8c74-80ac-eb75ca61e62c@gmail.com>
 <35afc9c9-dd43-fb17-d0d6-c90866fd5c28@measurement-factory.com>
 <52b0aec8-f61d-1bec-de18-1c3be06494a0@urlfilterdb.com>
 <81cf3b07-28a0-39b6-2690-a6cc320d653f@treenet.co.nz>
 <02aff9e7-c248-88f4-4a24-75f68448c378@gmail.com>
 <4df5b398-9c23-87ad-81c4-4d34c16a231c@measurement-factory.com>
 <42af5a38-f4f9-19a5-7397-b3c7084ce1a4@gmail.com>
 <e9b275f4-f9b1-7eda-e102-420bb9eda7ad@measurement-factory.com>
 <6d9fb82c-a83d-1215-7df7-04259c9a3997@gmail.com>
 <42f09caf-ee99-0f5e-b187-7468b736a04d@measurement-factory.com>
 <c4e25904-5c86-e3cb-c49c-f1f7d3f4583e@gmail.com>
 <c1090b61-7bad-5940-c5d9-10eb829cc595@measurement-factory.com>
 <c48783fd-02ad-d9e7-f2cc-164908dd0293@gmail.com>
 <741d243a-1750-30c9-f676-45a772038a69@measurement-factory.com>
Message-ID: <d0c57913-c13e-b27b-512b-42de7d9110ea@gmail.com>



25.01.2017 5:25, Alex Rousskov ?????:
> On 01/24/2017 02:11 PM, Yuri Voinov wrote:
>> 25.01.2017 2:50, Alex Rousskov ?????:
>>> A short-term hack: I have seen folks successfully solving somewhat
>>> similar problems using a localport ACL with an "impossible" value of
>>> zero. Please try this hack and update this thread if it works for you:
>>>
>>>> # Allow ESI, certificate fetching, Cache Digests, etc. internal requests
>>>> # XXX: Sooner or later, this unsupported hack will stop working!
>>>> acl generatedBySquid localport 0
>>>> http_access allow generatedBySquid
>
>> Sadly, but with this hack squid dies at request:
> That death is unlikely to be related to the ACL hack itself IMO. You can
> test my theory by temporary replacing "generatedBySquid" with "all" on
> the http_access line. If Squid still dies with "all", please consider
> properly reporting the crash; it will probably continue to bite you even
> after the long-term solution (e.g., transaction_initiator ACL) is available.
Yes, it also dies when I've changed final deny to allow all. I
understand that is another bug (probably), but rught now has not enought
time to build debug-enabled squid and reproduce death to backtrace. May
be later.
>
>
>>> A possible long-term solution: Factory is working on adding support for
>>> the following new ACL which may help solve this and a few other problems:
>>>
>>>> 	acl aclname transaction_initiator initiator...
>>>> 	  # Matches transaction's initiator [fast]
>> Very good. When it will be ready to use, or, at least, to test?
> I expect the first public implementation to be posted for squid-dev
> review within a week but this is not a promise.
Excellent, will wait.
>
>
>> So personally I'm willing to put up with uncontrollable requests to the
>> certificate stores.
> ... including any site pretending to be a certificate store.
As I said, it's silly to cry about lost virginity by performing "Man of
the middle." In addition, as I understand it, Squid takes the addresses
of sites with certificates not from the Higher Mind and not from
subspace - if my memory does not deceive, they are encoded in the
certificates themselves, that is, a priori, be regarded as relatively
safe. Also, it seemed to me, the intermediate certificates Squid sees as
untrusted.
>
> I respect that choice, but it must remain as a choice rather than
> hard-coded behavior IMO. I am sure that sooner or later somebody will
> come up with a "certificate" response that crashes Squid, and we do not
> want to leave the admins without a way to combat that attack vector.
Exactly, I'm not talking about the absence of choice. Everyone should be
able to meet their most foolish desire to scratch paranoia.
>
> We could change Squid to ignore http_access and lots of other existing
> directives while adding an increasing number of new control directives
> dedicated to certificate fetching transactions, but I think that
> segregation approach would be a strategic mistake because there are too
> many potential segregation areas with partially overlapping and complex
> configuration requirements. It will get messy and too error-prone.
No-no-no, no segregation. Aliens are the same as homo primates. :-D

Incidentally, when it was Squid configure simple and easy? :-!
>
>
> Cheers,
>
> Alex.
>

-------------- next part --------------
A non-text attachment was scrubbed...
Name: 0x613DEC46.asc
Type: application/pgp-keys
Size: 2437 bytes
Desc: not available
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20170125/7c2df2e7/attachment.key>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: signature.asc
Type: application/pgp-signature
Size: 473 bytes
Desc: OpenPGP digital signature
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20170125/7c2df2e7/attachment.sig>

From rousskov at measurement-factory.com  Wed Jan 25 15:10:25 2017
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Wed, 25 Jan 2017 08:10:25 -0700
Subject: [squid-users] squid reverse proxy (accelerator) for MS Exchange
 OWA
In-Reply-To: <2010900066.469374.1485330325639@mail.yahoo.com>
References: <1216522442.160455.1484870639800.ref@mail.yahoo.com>
 <1216522442.160455.1484870639800@mail.yahoo.com>
 <cdea0afa-c2a2-23c4-dbac-553a565e83f7@treenet.co.nz>
 <690032068.431187.1484905491833@mail.yahoo.com>
 <5517395f-eaa4-0b3a-34e9-66ea8d0122a1@treenet.co.nz>
 <1374872097.4095106.1485244925609@mail.yahoo.com>
 <94978436-88e5-fc84-9f3e-b8f1757402a5@measurement-factory.com>
 <2010900066.469374.1485330325639@mail.yahoo.com>
Message-ID: <0f2d2e08-bbd1-656a-a83b-5bb159983cbd@measurement-factory.com>

On 01/25/2017 12:45 AM, Vieri wrote:
> From: Alex Rousskov
>> The peer at 10.215.144.21:443 accepted Squid connection and then closed
>> it, probably before sending anything to Squid

> It seems that Squid delegates SSL to OpenSSL and it's really too bad
> the latter can't be a little bit more verbose. I know this isn't the
> right list for this but couldn't OpenSSL simply have logged something
> regarding "unsupported TLS/SSL versions"? 

If my reconstruction of the events was correct, then OpenSSL supplied as
much information as it could -- the "unsupported TLS/SSL versions" is
_your_ conclusion based on the information that neither Squid nor
OpenSSL had access to.


> I'm only supposing that
> without the ssloptions I posted above, openssl will try TLS 1.2 and
> silently fail if that doesn't succeed.

It takes two to tango. How silent that failure is depends, in part, on
the server. AFAICT, your server was 100% silent about the reason behind
its abrupt connection closure, and OpenSSL correctly declined to
speculate about those reasons due to lack of info. From OpenSSL/client
point of view, it could have been anything from an unsupported TLS
version to a crashed server.

Glad you figured it out!

Alex.



From leonardo at lbasolutions.com  Wed Jan 25 15:28:41 2017
From: leonardo at lbasolutions.com (Leonardo Bacha Abrantes)
Date: Wed, 25 Jan 2017 13:28:41 -0200
Subject: [squid-users] LDAP acl groups
Message-ID: <CAG+8EEapWQ+HxL1=r66oLFphpL7-f0SCcCTUjwncnYcaAPBsEg@mail.gmail.com>

Hi guys,

I have an active directory running on windows server 2008 r2 and squid
(version 3.5.20 - CentOS 7) authenticating via LDAP (without kerberos).
The ldap authentication is working, the trouble is to create ACLs based on
active directory groups.


OBS: When I run both basic_ldap_auth and ext_ldap_group_acl commands
manually as squid user in console to test, I receive 'OK' as answer.


--->>> My squid.conf:

auth_param basic program /usr/lib64/squid/basic_ldap_auth -P -R -b
ou=Users,ou=city,ou=country,dc=company,dc=local -D
CN=bindUser,DC=company,DC=local -W PasswdFile -f sAMAccountName=%s -h
192.168.1.9
auth_param basic children 10
auth_param basic realm XXXXX
auth_param basic credentialsttl 10 minutes

external_acl_type memberof %LOGIN /usr/lib64/squid/ext_ldap_group_acl -P -R
-b OU=city,OU=country,DC=company,DC=local -D
CN=bindUser,DC=company,DC=local -W PasswdFile -h 192.168.1.9 -f
'(&(objectClass=person)(sAMAccountName=%v)(memberOf=CN=%a,OU=Groups,OU=city,OU=country,dc=company,dc=local))'

#Also tried memberOf=CN=%*g*

acl fullaccess  external memberof squid_fullaccess

acl LdapUsers proxy_auth REQUIRED
http_access allow fullaccess LdapUsers

###

When I try to authenticate on proxy it still prompting for user/password
and any ldap query was done in domain controller looking to check if user
is member of squid_fullaccess group.


Can you give me some help please ?

Many thanks!
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20170125/222f7557/attachment.htm>

From erdosain9 at gmail.com  Wed Jan 25 16:38:51 2017
From: erdosain9 at gmail.com (erdosain9)
Date: Wed, 25 Jan 2017 08:38:51 -0800 (PST)
Subject: [squid-users] Strange behavior - reload service failed,
	but not start....
In-Reply-To: <CA+sSnVbSgDGBSzTVNbomVOKzH5oAd+vKB0JGVdjxai3rjihHYA@mail.gmail.com>
References: <1485349951724-4681317.post@n4.nabble.com>
 <CA+sSnVbSgDGBSzTVNbomVOKzH5oAd+vKB0JGVdjxai3rjihHYA@mail.gmail.com>
Message-ID: <1485362331939-4681322.post@n4.nabble.com>

Hi,........ no

[root at squid ~]# df -h
S.ficheros              Tama?o Usados  Disp Uso% Montado en
/dev/mapper/centos-root    48G    16G   33G  32% /
devtmpfs                  896M      0  896M   0% /dev
tmpfs                     906M   2,1M  904M   1% /dev/shm
tmpfs                     906M   8,5M  898M   1% /run
tmpfs                     906M      0  906M   0% /sys/fs/cgroup
/dev/sda1                 497M   141M  356M  29% /boot
tmpfs                     182M      0  182M   0% /run/user/0


by the way, this error dosent appear anymore, but was the first error i
noticed after the bad reboot. (i think that maybe i fix that with "squid
-z"....

some other approach??

(another "log", that maybe help)

[root at squid squid]# netstat -antp
Active Internet connections (servers and established)
Proto Recv-Q Send-Q Local Address           Foreign Address         State      
PID/Program name    
tcp        0      0 0.0.0.0:22              0.0.0.0:*               LISTEN     
971/sshd            
tcp        0      0 192.168.1.97:3128       0.0.0.0:*               LISTEN     
2230/(squid-1)      
tcp        0      0 127.0.0.1:25            0.0.0.0:*               LISTEN     
2001/master         
tcp        0      0 0.0.0.0:5666            0.0.0.0:*               LISTEN     
1027/nrpe           
tcp        0      0 192.168.1.97:3128       192.168.1.22:49350     
ESTABLISHED 2230/(squid-1)      
tcp        0      0 192.168.1.97:3128       192.168.6.20:53850     
ESTABLISHED 2230/(squid-1)      
tcp        0      0 192.168.1.97:3128       192.168.1.91:51600     
ESTABLISHED 2230/(squid-1)      
tcp        0      0 192.168.1.97:45465      104.16.127.226:443     
TIME_WAIT   -                   
tcp        0      0 192.168.1.97:38009      104.16.133.65:443      
TIME_WAIT   -                   
tcp   1689601      0 192.168.1.97:58852      204.79.197.213:443     
ESTABLISHED 2230/(squid-1)      
tcp        0      0 192.168.1.97:

       192.168.1.147:49249     ESTABLISHED 2230/(squid-1)      
tcp        0      0 192.168.1.97:52295      64.233.186.94:443      
TIME_WAIT   -                   
tcp        0      0 192.168.1.97:3128       192.168.1.147:49867    
ESTABLISHED 2230/(squid-1)      
tcp        0      0 192.168.1.97:39331      172.217.28.227:443     
TIME_WAIT   -                   
tcp        0      0 192.168.1.97:3128       192.168.6.20:37590     
TIME_WAIT   -                   
tcp        0      0 192.168.1.97:54678      54.230.81.192:443      
TIME_WAIT   -                   
tcp        0      0 192.168.1.97:41725      64.233.186.189:443     
ESTABLISHED 2230/(squid-1)      
tcp        0      0 192.168.1.97:39358      172.217.28.227:443     
TIME_WAIT   -                   
tcp        0      0 192.168.1.97:3128       192.168.1.18:54890     
ESTABLISHED 2230/(squid-1)      
tcp        0      0 192.168.1.97:3128       192.168.6.20:37646     
TIME_WAIT   -                   
tcp        0      0 192.168.1.97:56268      64.233.186.102:443     
TIME_WAIT   -                   
tcp        0      0 192.168.1.97:3128       192.168.1.164:49749    
ESTABLISHED 2230/(squid-1)      
tcp        0      0 192.168.1.97:3128       192.168.1.164:49707    
ESTABLISHED 2230/(squid-1)      
tcp        0      0 192.168.1.97:45542      104.16.127.226:443     
TIME_WAIT   -                   
tcp        0      1 192.168.1.97:3128       192.168.1.112:63933    
FIN_WAIT1   -                   
tcp        0      0 192.168.1.97:52912      172.217.29.5:443       
ESTABLISHED 2230/(squid-1)      
tcp        0      0 192.168.1.97:3128       192.168.1.91:51606     
ESTABLISHED 2230/(squid-1)      
tcp        0      0 192.168.1.97:3128       192.168.1.62:51074     
ESTABLISHED 2230/(squid-1)      
tcp        0      0 192.168.1.97:59908      31.13.85.8:443         
ESTABLISHED 2230/(squid-1)      
tcp        0      0 192.168.1.97:34688      104.16.32.227:443      
TIME_WAIT   -                   
tcp        0      0 192.168.1.97:48964      172.217.29.5:443       
ESTABLISHED 2230/(squid-1)      
tcp        0      0 192.168.1.97:56055      64.233.186.102:443     
ESTABLISHED 2230/(squid-1)      
tcp        0      0 192.168.1.97:39352      172.217.28.227:443     
TIME_WAIT   -                   
tcp        0      0 192.168.1.97:3128       192.168.1.164:49763    
ESTABLISHED 2230/(squid-1)      
tcp   181822      0 192.168.1.97:40542      170.51.244.15:443      
ESTABLISHED 2230/(squid-1)      
tcp        0      0 192.168.1.97:45466      104.16.127.226:443     
TIME_WAIT   -                   
tcp        0      0 192.168.1.97:3128       192.168.1.164:49761    
ESTABLISHED 2230/(squid-1)      
tcp        0      0 192.168.1.97:56139      64.233.186.139:443     
TIME_WAIT   -                   
tcp        0      0 192.168.1.97:3128       192.168.1.91:51590     
ESTABLISHED 2230/(squid-1)      
tcp        0      0 192.168.1.97:3128       192.168.1.91:51368     
ESTABLISHED 2230/(squid-1)      
tcp        0      0 192.168.1.97:35640      23.76.60.41:80         
TIME_WAIT   -                   
tcp        0      0 192.168.1.97:3128       192.168.6.70:40526     
TIME_WAIT   -                   
tcp        0      0 192.168.1.97:3128       192.168.1.91:51599     
ESTABLISHED 2230/(squid-1)      
tcp        0      0 192.168.1.97:39379      172.217.28.227:443     
TIME_WAIT   -                   
tcp        0      0 192.168.1.97:58081      172.217.29.5:443       
TIME_WAIT   -                   
tcp        0      0 192.168.1.97:3128       192.168.1.95:51831     
ESTABLISHED 2230/(squid-1)      
tcp        0      0 192.168.1.97:58708      104.27.149.90:443      
TIME_WAIT   -                   
tcp        0      0 192.168.1.97:57719      104.244.43.209:443     
ESTABLISHED 2230/(squid-1)      
tcp        0      0 192.168.1.97:3128       192.168.1.164:49760    
ESTABLISHED 2230/(squid-1)      
tcp        0      0 192.168.1.97:3128       192.168.1.164:49742    
ESTABLISHED 2230/(squid-1)      
tcp        0      0 192.168.1.97:50373      190.221.162.24:443     
TIME_WAIT   -                   
tcp        0      0 192.168.1.97:38656      158.85.224.174:443     
ESTABLISHED 2230/(squid-1)      
tcp        0      0 192.168.1.97:38005      104.16.133.65:443      
TIME_WAIT   -                   
tcp        0      0 192.168.1.97:36946      31.13.85.8:443         
ESTABLISHED 2230/(squid-1)      
tcp        0      0 192.168.1.97:58322      190.94.187.8:80        
ESTABLISHED 2230/(squid-1)      
tcp        0      0 192.168.1.97:38004      104.16.133.65:443      
TIME_WAIT   -                   
tcp        0      0 192.168.1.97:3128       192.168.1.120:49758    
ESTABLISHED 2230/(squid-1)      
tcp        0      0 192.168.1.97:3128       192.168.1.112:63939    
ESTABLISHED 2230/(squid-1)      
tcp        0      0 192.168.1.97:3128       192.168.1.91:51585     
ESTABLISHED 2230/(squid-1)      
tcp        0      0 192.168.1.97:3128       192.168.1.164:49752    
ESTABLISHED 2230/(squid-1)      
tcp        0      0 192.168.1.97:3128       192.168.1.91:51580     
ESTABLISHED 2230/(squid-1)      
tcp        0      0 192.168.1.97:58913      207.244.90.212:80      
TIME_WAIT   -                   
tcp   771534      0 192.168.1.97:58649      104.27.149.90:443      
ESTABLISHED 2230/(squid-1)      
tcp        0      0 192.168.1.97:50398      77.234.42.60:443       
ESTABLISHED 2230/(squid-1)      
tcp        0      0 192.168.1.97:33305      216.58.212.131:443     
TIME_WAIT   -                   
tcp        0      0 192.168.1.97:43271      64.233.186.95:443      
TIME_WAIT   -                   
tcp        0      0 192.168.1.97:41581      64.233.186.189:443     
ESTABLISHED 2230/(squid-1)      
tcp        0      0 192.168.1.97:35275      64.233.186.94:80       
ESTABLISHED 2230/(squid-1)      
tcp        0      0 192.168.1.97:3128       192.168.1.112:63937    
ESTABLISHED 2230/(squid-1)      
tcp        0      0 192.168.1.97:56115      64.233.186.102:443     
TIME_WAIT   -                   
tcp        0      0 192.168.1.97:46514      64.233.186.95:80       
TIME_WAIT   -                   
tcp        0      0 192.168.1.97:3128       192.168.1.18:54926     
ESTABLISHED 2230/(squid-1)      
tcp        0      0 192.168.1.97:34691      104.16.32.227:443      
TIME_WAIT   -                   
tcp        0      0 192.168.1.97:3128       192.168.1.62:50835     
ESTABLISHED 2230/(squid-1)      
tcp        0      0 192.168.1.97:58282      172.217.29.5:443       
TIME_WAIT   -                   
tcp        0      0 192.168.1.97:22         192.168.6.20:35336     
ESTABLISHED 3946/sshd: root at pts 
tcp        0      0 192.168.1.97:3128       192.168.1.129:63728    
ESTABLISHED 2230/(squid-1)      
tcp        0      0 192.168.1.97:3128       192.168.6.20:37594     
FIN_WAIT2   -                   
tcp        0      1 192.168.1.97:3128       192.168.1.112:63934    
FIN_WAIT1   -                   
tcp        0      0 192.168.1.97:60339      104.20.62.49:443       
TIME_WAIT   -                   
tcp        0      0 192.168.1.97:58796      200.69.47.99:1935      
ESTABLISHED 2230/(squid-1)      
tcp        0      0 192.168.1.97:36663      23.76.32.140:443       
ESTABLISHED 2230/(squid-1)      
tcp        0      0 192.168.1.97:3128       192.168.1.26:51158     
ESTABLISHED 2230/(squid-1)      
tcp        0      0 192.168.1.97:57357      104.244.42.72:443      
ESTABLISHED 2230/(squid-1)      
tcp        0      0 192.168.1.97:3128       192.168.1.120:49412    
ESTABLISHED 2230/(squid-1)      
tcp        0      0 192.168.1.97:3128       192.168.6.20:37642     
TIME_WAIT   -                   
tcp        0      0 192.168.1.97:32932      23.76.32.81:443        
ESTABLISHED 2230/(squid-1)      
tcp        0      0 192.168.1.97:38246      172.217.28.229:443     
ESTABLISHED 2230/(squid-1)      
tcp        0      0 192.168.1.97:56148      64.233.186.102:443     
TIME_WAIT   -                   
tcp        0      0 192.168.1.97:39353      172.217.28.227:443     
TIME_WAIT   -                   
tcp        0      0 192.168.1.97:3128       192.168.1.113:51078    
ESTABLISHED 2230/(squid-1)      
tcp        0      0 192.168.1.97:50375      190.221.162.24:443     
TIME_WAIT   -                   
tcp        0      0 192.168.1.97:3128       192.168.1.164:49751    
ESTABLISHED 2230/(squid-1)      
tcp        0      0 192.168.1.97:3128       192.168.1.167:51066    
ESTABLISHED 2230/(squid-1)      
tcp        0      0 192.168.1.97:42039      64.233.186.101:443     
TIME_WAIT   -                   
tcp        0      0 192.168.1.97:56103      64.233.186.102:443     
TIME_WAIT   -                   
tcp        0      0 192.168.1.97:3128       192.168.1.91:51597     
ESTABLISHED 2230/(squid-1)      
tcp        0      0 192.168.1.97:58893      207.244.90.212:80      
TIME_WAIT   -                   
tcp        0      0 192.168.1.97:3128       192.168.1.120:49759    
ESTABLISHED 2230/(squid-1)      
tcp        0      0 192.168.1.97:56387      162.125.18.133:443     
ESTABLISHED 2230/(squid-1)      
tcp        0      0 192.168.1.97:55660      64.233.186.139:443     
TIME_WAIT   -                   
tcp        0      0 192.168.1.97:42062      199.38.85.43:80        
TIME_WAIT   -                   
tcp        0  66570 192.168.1.97:3128       192.168.1.95:51808     
ESTABLISHED 2230/(squid-1)      
tcp        0      0 192.168.1.97:3128       192.168.1.155:49503    
ESTABLISHED 2230/(squid-1)      
tcp        0      0 192.168.1.97:38937      172.217.28.227:443     
TIME_WAIT   -                   
tcp        0      0 192.168.1.97:45470      104.16.127.226:443     
TIME_WAIT   -                   
tcp        0      0 192.168.1.97:3128       192.168.1.164:49733    
ESTABLISHED 2230/(squid-1)      
tcp        0      0 192.168.1.97:3128       192.168.1.164:49754    
ESTABLISHED 2230/(squid-1)      
tcp        0      0 192.168.1.97:39407      172.217.28.227:443     
TIME_WAIT   -                   
tcp        0      0 192.168.1.97:3128       192.168.1.113:51080    
ESTABLISHED 2230/(squid-1)      
tcp        0      0 192.168.1.97:39972      64.233.186.189:443     
ESTABLISHED 2230/(squid-1)      
tcp        0      0 192.168.1.97:3128       192.168.1.19:49543     
ESTABLISHED 2230/(squid-1)      
tcp        0      0 192.168.1.97:3128       192.168.1.164:49737    
ESTABLISHED 2230/(squid-1)      
tcp        0      0 192.168.1.97:57639      162.125.18.133:443     
ESTABLISHED 2230/(squid-1)      
tcp        0      0 192.168.1.97:43894      104.16.130.65:443      
TIME_WAIT   -                   
tcp        0      0 192.168.1.97:3128       192.168.6.20:37654     
FIN_WAIT2   -                   
tcp        0      0 192.168.1.97:35975      172.217.28.228:443     
TIME_WAIT   -                   
tcp        0      0 192.168.1.97:60892      104.20.62.49:443       
ESTABLISHED 2230/(squid-1)      
tcp        0      0 192.168.1.97:3128       192.168.1.164:49739    
ESTABLISHED 2230/(squid-1)      
tcp        0      0 192.168.1.97:34264      64.233.186.97:443      
TIME_WAIT   -                   
tcp        0      0 192.168.1.97:3128       192.168.1.62:51028     
ESTABLISHED 2230/(squid-1)      
tcp        0      0 192.168.1.97:55701      200.32.56.49:443       
ESTABLISHED 2230/(squid-1)      
tcp        0      1 192.168.1.97:3128       192.168.1.112:63935    
FIN_WAIT1   -                   
tcp        0      0 192.168.1.97:37945      104.16.133.65:443      
TIME_WAIT   -                   
tcp        0      0 192.168.1.97:38649      172.217.28.229:443     
TIME_WAIT   -                   
tcp        0      0 192.168.1.97:55748      172.217.29.5:443       
ESTABLISHED 2230/(squid-1)      
tcp        0      0 192.168.1.97:3128       192.168.1.66:49353     
ESTABLISHED 2230/(squid-1)      
tcp        0      0 192.168.1.97:56076      64.233.186.102:443     
TIME_WAIT   -                   
tcp        0      0 192.168.1.97:3128       192.168.1.120:49761    
FIN_WAIT2   -                   
tcp        0      0 192.168.1.97:36493      98.138.81.72:443       
TIME_WAIT   -                   
tcp        0      0 192.168.1.97:3128       192.168.1.164:49732    
ESTABLISHED 2230/(squid-1)      
tcp        0      0 192.168.1.97:3128       192.168.1.91:51576     
ESTABLISHED 2230/(squid-1)      
tcp        0      0 192.168.1.97:45468      104.16.127.226:443     
TIME_WAIT   -                   
tcp        0      0 192.168.1.97:46932      190.94.187.17:80       
TIME_WAIT   -                   
tcp        0      0 192.168.1.97:36442      98.138.140.94:443      
TIME_WAIT   -                   
tcp        0      0 192.168.1.97:3128       192.168.1.91:51514     
ESTABLISHED 2230/(squid-1)      
tcp        0      0 192.168.1.97:58272      172.217.29.5:443       
ESTABLISHED 2230/(squid-1)      
tcp        0      0 192.168.1.97:45638      64.233.190.94:443      
TIME_WAIT   -                   
tcp        0      0 192.168.1.97:38899      209.225.49.65:443      
TIME_WAIT   -                   
tcp        0      0 192.168.1.97:37950      104.16.133.65:443      
TIME_WAIT   -                   
tcp        0      0 192.168.1.97:3128       192.168.1.91:51589     
ESTABLISHED 2230/(squid-1)      
tcp        0      0 192.168.1.97:35671      31.13.85.8:443         
ESTABLISHED 2230/(squid-1)      
tcp        0      0 192.168.1.97:40018      209.225.49.57:80       
ESTABLISHED 2230/(squid-1)      
tcp        0      0 192.168.1.97:38979      172.217.28.229:443     
ESTABLISHED 2230/(squid-1)      
tcp        0      0 192.168.1.97:3128       192.168.1.91:51601     
ESTABLISHED 2230/(squid-1)      
tcp        0      0 192.168.1.97:52205      64.233.186.94:443      
TIME_WAIT   -                   
tcp        0      0 192.168.1.97:43325      192.150.19.174:443     
TIME_WAIT   -                   
tcp        0      0 192.168.1.97:3128       192.168.6.20:37604     
FIN_WAIT2   -                   
tcp        0      0 192.168.1.97:3128       192.168.6.20:37606     
FIN_WAIT2   -                   
tcp        0      0 192.168.1.97:3128       192.168.1.91:50908     
ESTABLISHED 2230/(squid-1)      
tcp        0      0 192.168.1.97:3128       192.168.1.91:51605     
ESTABLISHED 2230/(squid-1)      
tcp        0      0 192.168.1.97:3128       192.168.1.164:49715    
ESTABLISHED 2230/(squid-1)      
tcp        0      0 192.168.1.97:58820      104.16.132.65:443      
TIME_WAIT   -                   
tcp        0      0 192.168.1.97:56140      64.233.186.139:443     
TIME_WAIT   -                   
tcp        0      0 192.168.1.97:52706      64.233.186.102:80      
ESTABLISHED 2230/(squid-1)      
tcp        0      0 192.168.1.97:3128       192.168.1.112:63941    
ESTABLISHED 2230/(squid-1)      
tcp        0      0 192.168.1.97:34438      77.234.44.35:80        
ESTABLISHED 2230/(squid-1)      
tcp        0      0 192.168.1.97:34730      104.16.126.226:443     
TIME_WAIT   -                   
tcp        0      0 192.168.1.97:58707      104.27.149.90:443      
TIME_WAIT   -                   
tcp        0      0 192.168.1.97:3128       192.168.1.113:51077    
ESTABLISHED 2230/(squid-1)      
tcp        0      0 192.168.1.97:3128       192.168.1.26:51043     
ESTABLISHED 2230/(squid-1)      
tcp        0      0 192.168.1.97:3128       192.168.1.91:51586     
ESTABLISHED 2230/(squid-1)      
tcp        0      0 192.168.1.97:3128       192.168.6.20:37644     
TIME_WAIT   -                   
tcp        0      0 192.168.1.97:52816      151.101.192.246:443    
ESTABLISHED 2230/(squid-1)      
tcp        0      0 192.168.1.97:3128       192.168.1.113:51083    
ESTABLISHED 2230/(squid-1)      
tcp        0      0 192.168.1.97:43399      54.231.49.186:443      
ESTABLISHED 2230/(squid-1)      
tcp        0      0 192.168.1.97:34736      104.16.126.226:443     
TIME_WAIT   -                   
tcp        0      0 192.168.1.97:3128       192.168.1.69:50616     
ESTABLISHED 2230/(squid-1)      
tcp        0      0 192.168.1.97:34544      104.16.32.227:443      
TIME_WAIT   -                   
tcp        0      0 192.168.1.97:52214      64.233.186.94:443      
TIME_WAIT   -                   
tcp        0      0 192.168.1.97:34583      104.16.126.226:443     
TIME_WAIT   -                   
tcp        0      0 192.168.1.97:36455      64.233.186.91:443      
ESTABLISHED 2230/(squid-1)      
tcp        0      0 192.168.1.97:39378      172.217.28.227:443     
TIME_WAIT   -                   
tcp        0      0 192.168.1.97:54521      54.230.81.192:443      
TIME_WAIT   -                   
tcp        0      0 192.168.1.97:34689      104.16.32.227:443      
TIME_WAIT   -                   
tcp        0      0 192.168.1.97:34545      104.16.32.227:443      
TIME_WAIT   -                   
tcp        0      0 192.168.1.97:3128       192.168.1.164:49741    
ESTABLISHED 2230/(squid-1)      
tcp        0      0 192.168.1.97:43676      209.225.49.78:80       
ESTABLISHED 2230/(squid-1)      
tcp        0      0 192.168.1.97:42298      199.38.85.43:80        
ESTABLISHED 2230/(squid-1)      
tcp        0      0 192.168.1.97:57377      31.13.85.4:443         
ESTABLISHED 2230/(squid-1)      
tcp        0      0 192.168.1.97:3128       192.168.6.20:37618     
FIN_WAIT2   -                   
tcp        0      0 192.168.1.97:54554      168.63.200.92:443      
ESTABLISHED 2230/(squid-1)      
tcp        0      0 192.168.1.97:58298      172.217.29.5:443       
TIME_WAIT   -                   
tcp        0      0 192.168.1.97:45097      23.76.59.231:80        
TIME_WAIT   -                   
tcp        0      0 192.168.1.97:43397      54.231.49.186:443      
ESTABLISHED 2230/(squid-1)      
tcp        0      0 192.168.1.97:3128       192.168.6.20:37628     
FIN_WAIT2   -                   
tcp        0      0 192.168.1.97:37951      104.16.133.65:443      
TIME_WAIT   -                   
tcp        0      0 192.168.1.97:56097      64.233.186.102:443     
TIME_WAIT   -                   
tcp        0      0 192.168.1.97:34729      104.16.126.226:443     
TIME_WAIT   -                   
tcp        0      0 192.168.1.97:3128       192.168.1.164:49714    
ESTABLISHED 2230/(squid-1)      
tcp        0      0 192.168.1.97:3128       192.168.1.129:65466    
ESTABLISHED 2230/(squid-1)      
tcp        0      0 192.168.1.97:3128       192.168.1.85:49594     
ESTABLISHED 2230/(squid-1)      
tcp        0      0 192.168.1.97:37957      104.16.133.65:443      
TIME_WAIT   -                   
tcp        0      0 192.168.1.97:3128       192.168.1.112:63938    
ESTABLISHED 2230/(squid-1)      
tcp        0      0 192.168.1.97:3128       192.168.1.85:49796     
ESTABLISHED 2230/(squid-1)      
tcp        0      0 192.168.1.97:52255      64.233.186.94:443      
TIME_WAIT   -                   
tcp        0      0 192.168.1.97:3128       192.168.1.164:49738    
ESTABLISHED 2230/(squid-1)      
tcp        0      0 192.168.1.97:3128       192.168.1.167:49966    
ESTABLISHED 2230/(squid-1)      
tcp        0      0 192.168.1.97:58280      172.217.29.5:443       
TIME_WAIT   -                   
tcp        0      0 192.168.1.97:3128       192.168.6.20:37608     
FIN_WAIT2   -                   
tcp        0      0 192.168.1.97:59522      52.22.200.230:80       
ESTABLISHED 2230/(squid-1)      
tcp        0      0 192.168.1.97:41985      64.233.186.101:443     
TIME_WAIT   -                   
tcp        0      0 192.168.1.97:41972      64.233.186.101:443     
TIME_WAIT   -                   
tcp        0      0 192.168.1.97:36661      23.76.32.140:443       
ESTABLISHED 2230/(squid-1)      
tcp        0      0 192.168.1.97:56314      64.233.186.102:443     
TIME_WAIT   -                   
tcp        0      0 192.168.1.97:3128       192.168.1.91:51593     
ESTABLISHED 2230/(squid-1)      
tcp        0      0 192.168.1.97:43448      64.233.186.95:443      
TIME_WAIT   -                   
tcp        0      0 192.168.1.97:43764      64.233.186.95:443      
TIME_WAIT   -                   
tcp        0      0 192.168.1.97:3128       192.168.1.63:58649     
TIME_WAIT   -                   
tcp        0      0 192.168.1.97:45525      209.225.49.77:443      
TIME_WAIT   -                   
tcp        0      0 192.168.1.97:43660      200.152.162.161:443    
TIME_WAIT   -                   
tcp        0      0 192.168.1.97:55709      200.32.56.49:443       
ESTABLISHED 2230/(squid-1)      
tcp        0      0 192.168.1.97:3128       192.168.1.164:49745    
ESTABLISHED 2230/(squid-1)      
tcp        0      0 192.168.1.97:37948      104.16.133.65:443      
TIME_WAIT   -                   
tcp        0      0 192.168.1.97:33234      172.217.28.238:443     
TIME_WAIT   -                   
tcp        0      0 192.168.1.97:3128       192.168.1.164:49671    
ESTABLISHED 2230/(squid-1)      
tcp        0      0 192.168.1.97:56185      64.233.186.102:443     
TIME_WAIT   -                   
tcp        0      0 192.168.1.97:43270      64.233.186.95:443      
TIME_WAIT   -                   
tcp        0      0 192.168.1.97:34687      104.16.32.227:443      
TIME_WAIT   -                   
tcp        0      0 192.168.1.97:36365      98.137.201.232:443     
ESTABLISHED 2230/(squid-1)      
tcp        0   1788 192.168.1.97:55091      31.13.85.36:443        
ESTABLISHED 2230/(squid-1)      
tcp        0      0 192.168.1.97:3128       192.168.1.19:49563     
ESTABLISHED 2230/(squid-1)      
tcp        0      0 192.168.1.97:36821      64.233.186.132:443     
TIME_WAIT   -                   
tcp        0      0 192.168.1.97:52219      64.233.186.94:443      
TIME_WAIT   -                   
tcp        0      0 192.168.1.97:56312      64.233.186.102:443     
TIME_WAIT   -                   
tcp        0      0 192.168.1.97:3128       192.168.1.91:51594     
ESTABLISHED 2230/(squid-1)      
tcp        0      0 192.168.1.97:3128       192.168.1.164:49740    
ESTABLISHED 2230/(squid-1)      
tcp        0      0 192.168.1.97:56135      64.233.186.139:443     
TIME_WAIT   -                   
tcp        0      0 192.168.1.97:3128       192.168.1.18:53848     
ESTABLISHED 2230/(squid-1)      
tcp        0      0 192.168.1.97:3128       192.168.1.164:49717    
ESTABLISHED 2230/(squid-1)      
tcp        0      0 192.168.1.97:3128       192.168.1.95:51822     
TIME_WAIT   -                   
tcp        0      0 192.168.1.97:37941      104.16.133.65:443      
TIME_WAIT   -                   
tcp        0      0 192.168.1.97:3128       192.168.6.20:37616     
FIN_WAIT2   -                   
tcp        0      0 192.168.1.97:3128       192.168.1.91:51602     
ESTABLISHED 2230/(squid-1)      
tcp        0      0 192.168.1.97:56151      64.233.186.102:443     
TIME_WAIT   -                   
tcp        0      0 192.168.1.97:3128       192.168.1.91:51581     
ESTABLISHED 2230/(squid-1)      
tcp        0      0 192.168.1.97:36662      23.76.32.140:443       
ESTABLISHED 2230/(squid-1)      
tcp        0      0 192.168.1.97:3128       192.168.1.112:63942    
ESTABLISHED 2230/(squid-1)      
tcp        0      0 192.168.1.97:3128       192.168.1.112:63940    
ESTABLISHED 2230/(squid-1)      
tcp        0      0 192.168.1.97:3128       192.168.1.164:49736    
ESTABLISHED 2230/(squid-1)      
tcp        0      0 192.168.1.97:57492      172.217.29.5:443       
ESTABLISHED 2230/(squid-1)      
tcp        0      0 192.168.1.97:3128       192.168.6.20:37610     
FIN_WAIT2   -                   
tcp        0      0 192.168.1.97:35611      40.97.145.186:443      
ESTABLISHED 2230/(squid-1)      
tcp        0      0 192.168.1.97:3128       192.168.1.164:49743    
ESTABLISHED 2230/(squid-1)      
tcp        0      0 192.168.1.97:3128       192.168.1.18:53453     
ESTABLISHED 2230/(squid-1)      
tcp        0      0 192.168.1.97:3128       192.168.6.71:49751     
ESTABLISHED 2230/(squid-1)      
tcp        0      0 192.168.1.97:3128       192.168.1.164:49716    
ESTABLISHED 2230/(squid-1)      
tcp        0      0 192.168.1.97:46084      64.233.186.113:443     
TIME_WAIT   -                   
tcp        0      0 192.168.1.97:3128       192.168.1.164:49746    
ESTABLISHED 2230/(squid-1)      
tcp        0      0 192.168.1.97:43793      172.217.29.5:443       
ESTABLISHED 2230/(squid-1)      
tcp        0      0 192.168.1.97:3128       192.168.1.164:49734    
ESTABLISHED 2230/(squid-1)      
tcp        0      0 192.168.1.97:38007      104.16.133.65:443      
TIME_WAIT   -                   
tcp        0      0 192.168.1.97:36664      23.76.32.140:443       
ESTABLISHED 2230/(squid-1)      
tcp        0      0 192.168.1.97:3128       192.168.1.155:49721    
ESTABLISHED 2230/(squid-1)      
tcp        0      0 192.168.1.97:58151      65.55.252.71:443       
TIME_WAIT   -                   
tcp        0      0 192.168.1.97:44351      64.233.186.136:443     
TIME_WAIT   -                   
tcp        0      0 192.168.1.97:40101      23.196.18.178:443      
ESTABLISHED 2230/(squid-1)      
tcp        0      0 192.168.1.97:3128       192.168.6.20:37624     
TIME_WAIT   -                   
tcp        0      0 192.168.1.97:56142      64.233.186.102:443     
TIME_WAIT   -                   
tcp        0      0 192.168.1.97:34734      104.16.126.226:443     
TIME_WAIT   -                   
tcp        0      0 192.168.1.97:3128       192.168.6.20:37636     
TIME_WAIT   -                   
tcp        0      0 192.168.1.97:56144      64.233.186.102:443     
TIME_WAIT   -                   
tcp        0      0 192.168.1.97:3128       192.168.1.91:51596     
ESTABLISHED 2230/(squid-1)      
tcp        0      0 192.168.1.97:46228      104.208.165.109:443    
TIME_WAIT   -                   
tcp        0      0 192.168.1.97:3128       192.168.1.164:49713    
ESTABLISHED 2230/(squid-1)      
tcp        0      0 192.168.1.97:36473      64.233.186.189:443     
ESTABLISHED 2230/(squid-1)      
tcp        0      0 192.168.1.97:38759      172.217.28.227:443     
ESTABLISHED 2230/(squid-1)      
tcp        0      0 192.168.1.97:3128       192.168.1.91:51604     
ESTABLISHED 2230/(squid-1)      
tcp        0      0 192.168.1.97:58291      172.217.29.5:443       
ESTABLISHED 2230/(squid-1)      
tcp        0      0 192.168.1.97:3128       192.168.1.62:51298     
ESTABLISHED 2230/(squid-1)      
tcp        0      0 192.168.1.97:3128       192.168.1.132:50098    
ESTABLISHED 2230/(squid-1)      
tcp        0      0 192.168.1.97:55419      104.16.254.64:80       
ESTABLISHED 2230/(squid-1)      
tcp        0      0 192.168.1.97:3128       192.168.6.20:37612     
FIN_WAIT2   -                   
tcp        0      0 192.168.1.97:3128       192.168.1.147:49516    
ESTABLISHED 2230/(squid-1)      
tcp        0      0 192.168.1.97:37961      104.16.133.65:443      
TIME_WAIT   -                   
tcp        0      0 192.168.1.97:3128       192.168.6.70:40514     
ESTABLISHED 2230/(squid-1)      
tcp        0      0 192.168.1.97:35641      23.76.60.41:80         
TIME_WAIT   -                   
tcp        0      0 192.168.1.97:55703      200.32.56.49:443       
ESTABLISHED 2230/(squid-1)      
tcp        0      0 192.168.1.97:50516      190.221.162.24:443     
TIME_WAIT   -                   
tcp        0      0 192.168.1.97:3128       192.168.1.91:51578     
ESTABLISHED 2230/(squid-1)      
tcp        0      0 192.168.1.97:53326      162.125.18.133:443     
ESTABLISHED 2230/(squid-1)      
tcp        0      0 192.168.1.97:38008      104.16.133.65:443      
TIME_WAIT   -                   
tcp        0      0 192.168.1.97:3128       192.168.1.113:65415    
ESTABLISHED 2230/(squid-1)      
tcp        0      0 192.168.1.97:45541      104.16.127.226:443     
TIME_WAIT   -                   
tcp        0      0 192.168.1.97:34584      104.16.126.226:443     
TIME_WAIT   -                   
tcp        0      0 192.168.1.97:58855      65.55.252.169:443      
TIME_WAIT   -                   
tcp        0      0 192.168.1.97:54208      172.217.28.2:443       
ESTABLISHED 2230/(squid-1)      
tcp        0      0 192.168.1.97:50337      190.221.162.24:443     
TIME_WAIT   -                   
tcp        0      0 192.168.1.97:3128       192.168.1.129:65467    
ESTABLISHED 2230/(squid-1)      
tcp        0      0 192.168.1.97:3128       192.168.6.20:36964     
ESTABLISHED 2230/(squid-1)      
tcp        0      0 192.168.1.97:43398      54.231.49.186:443      
ESTABLISHED 2230/(squid-1)      
tcp        0      0 192.168.1.97:49107      172.217.29.5:443       
ESTABLISHED 2230/(squid-1)      
tcp        0      0 192.168.1.97:3128       192.168.1.164:49551    
ESTABLISHED 2230/(squid-1)      
tcp        0      0 192.168.1.97:40570      172.217.28.229:443     
TIME_WAIT   -                   
tcp        0      0 192.168.1.97:3128       192.168.1.164:49757    
ESTABLISHED 2230/(squid-1)      
tcp        0      0 192.168.1.97:3128       192.168.6.70:40524     
ESTABLISHED 2230/(squid-1)      
tcp        0      0 192.168.1.97:40572      172.217.28.229:443     
TIME_WAIT   -                   
tcp        0      0 192.168.1.97:3128       192.168.1.132:50747    
ESTABLISHED 2230/(squid-1)      
tcp        0      0 192.168.1.97:55038      31.13.85.36:443        
ESTABLISHED 2230/(squid-1)      
tcp        0      0 192.168.1.97:40575      172.217.28.229:443     
TIME_WAIT   -                   
tcp        0      0 192.168.1.97:3128       192.168.1.95:51832     
ESTABLISHED 2230/(squid-1)      
tcp        0      0 192.168.1.97:44062      162.125.18.133:443     
ESTABLISHED 2230/(squid-1)      
tcp        0      0 192.168.1.97:38006      104.16.133.65:443      
TIME_WAIT   -                   
tcp        0      0 192.168.1.97:3128       192.168.1.91:51584     
ESTABLISHED 2230/(squid-1)      
tcp        0      0 192.168.1.97:3128       192.168.1.164:49759    
ESTABLISHED 2230/(squid-1)      
tcp        0      0 192.168.1.97:3128       192.168.1.91:51603     
ESTABLISHED 2230/(squid-1)      
tcp        0      0 192.168.1.97:40541      170.51.244.15:443      
TIME_WAIT   -                   
tcp        0      0 192.168.1.97:56393      172.217.28.229:443     
ESTABLISHED 2230/(squid-1)      
tcp        0      0 192.168.1.97:60893      104.20.62.49:443       
ESTABLISHED 2230/(squid-1)      
tcp        0      0 192.168.1.97:3128       192.168.1.85:49655     
ESTABLISHED 2230/(squid-1)      
tcp        0      0 192.168.1.97:3128       192.168.1.112:63946    
ESTABLISHED 2230/(squid-1)      
tcp        0      0 192.168.1.97:3128       192.168.1.93:54088     
ESTABLISHED 2230/(squid-1)      
tcp        0      0 192.168.1.97:3128       192.168.1.114:58881    
ESTABLISHED 2230/(squid-1)      
tcp        0      0 192.168.1.97:56171      64.233.186.102:443     
TIME_WAIT   -                   
tcp        0      0 192.168.1.97:50556      104.16.131.65:443      
TIME_WAIT   -                   
tcp        0      0 192.168.1.97:3128       192.168.1.4:49315      
ESTABLISHED 2230/(squid-1)      
tcp        0      0 192.168.1.97:58988      31.13.85.8:443         
ESTABLISHED 2230/(squid-1)      
tcp        0      0 192.168.1.97:56282      104.244.43.16:443      
TIME_WAIT   -                   
tcp        0      0 192.168.1.97:3128       192.168.1.112:63945    
ESTABLISHED 2230/(squid-1)      
tcp        0      0 192.168.1.97:55705      200.32.56.49:443       
ESTABLISHED 2230/(squid-1)      
tcp        0      0 192.168.1.97:52488      64.233.186.94:443      
ESTABLISHED 2230/(squid-1)      
tcp        0      0 192.168.1.97:56161      64.233.186.102:443     
TIME_WAIT   -                   
tcp        0      0 192.168.1.97:3128       192.168.1.164:49554    
ESTABLISHED 2230/(squid-1)      
tcp        0      0 192.168.1.97:59085      65.55.252.169:443      
ESTABLISHED 2230/(squid-1)      
tcp        0      0 192.168.1.97:3128       192.168.1.91:51587     
ESTABLISHED 2230/(squid-1)      
tcp        0      0 192.168.1.97:3128       192.168.1.63:58655     
ESTABLISHED 2230/(squid-1)      
tcp        0      0 192.168.1.97:3128       192.168.1.18:54517     
ESTABLISHED 2230/(squid-1)      
tcp        0      0 192.168.1.97:54736      72.21.202.25:80        
TIME_WAIT   -                   
tcp        0      0 192.168.1.97:35291      13.107.42.11:443       
ESTABLISHED 2230/(squid-1)      
tcp        0      0 192.168.1.97:59795      64.233.186.128:80      
ESTABLISHED 2230/(squid-1)      
tcp        0      0 192.168.1.97:3128       192.168.1.164:49735    
ESTABLISHED 2230/(squid-1)      
tcp        0      0 192.168.1.97:36443      98.138.140.94:443      
TIME_WAIT   -                   
tcp        0      0 192.168.1.97:3128       192.168.1.164:49750    
ESTABLISHED 2230/(squid-1)      
tcp        0      0 192.168.1.97:57378      31.13.85.4:443         
ESTABLISHED 2230/(squid-1)      
tcp        0      0 192.168.1.97:3128       192.168.1.164:49731    
ESTABLISHED 2230/(squid-1)      
tcp        0      0 192.168.1.97:3128       192.168.6.70:40528     
ESTABLISHED 2230/(squid-1)      
tcp        0      0 192.168.1.97:3128       192.168.1.164:49748    
ESTABLISHED 2230/(squid-1)      
tcp        0      0 192.168.1.97:56104      64.233.186.102:443     
TIME_WAIT   -                   
tcp        0      0 192.168.1.97:38010      104.16.133.65:443      
TIME_WAIT   -                   
tcp        0      0 192.168.1.97:3128       192.168.1.169:50544    
ESTABLISHED 2230/(squid-1)      
tcp        0      0 192.168.1.97:3128       192.168.1.93:54213     
FIN_WAIT2   -                   
tcp        0      0 192.168.1.97:39376      172.217.28.227:443     
TIME_WAIT   -                   
tcp        0      0 192.168.1.97:3128       192.168.1.91:51579     
ESTABLISHED 2230/(squid-1)      
tcp        0      0 192.168.1.97:49963      172.217.29.5:443       
ESTABLISHED 2230/(squid-1)      
tcp        0      0 192.168.1.97:3128       192.168.1.91:51588     
ESTABLISHED 2230/(squid-1)      
tcp        0      0 192.168.1.97:39356      172.217.28.227:443     
TIME_WAIT   -                   
tcp        0      0 192.168.1.97:58324      172.217.29.5:443       
TIME_WAIT   -                   
tcp        0      0 192.168.1.97:56141      64.233.186.139:443     
TIME_WAIT   -                   
tcp        0      0 192.168.1.97:34630      31.13.85.8:443         
ESTABLISHED 2230/(squid-1)      
tcp        0      0 192.168.1.97:3128       192.168.1.164:49758    
ESTABLISHED 2230/(squid-1)      
tcp        0      0 192.168.1.97:3128       192.168.1.91:51592     
ESTABLISHED 2230/(squid-1)      
tcp        0      0 192.168.1.97:56317      64.233.186.102:443     
ESTABLISHED 2230/(squid-1)      
tcp        0      0 192.168.1.97:3128       192.168.1.113:51081    
ESTABLISHED 2230/(squid-1)      
tcp        0      0 192.168.1.97:3128       192.168.1.69:50710     
ESTABLISHED 2230/(squid-1)      
tcp        0      0 192.168.1.97:3128       192.168.1.63:56419     
ESTABLISHED 2230/(squid-1)      
tcp      397      0 192.168.1.97:39200      190.221.162.16:80      
ESTABLISHED 2230/(squid-1)      
tcp        0    190 192.168.1.97:38986      209.225.49.65:443      
ESTABLISHED 2230/(squid-1)      
tcp        0      0 192.168.1.97:58791      104.16.132.65:443      
TIME_WAIT   -                   
tcp        0      0 192.168.1.97:55337      104.16.254.64:80       
TIME_WAIT   -                   
tcp        0      0 192.168.1.97:34732      104.16.126.226:443     
TIME_WAIT   -                   
tcp        0      0 192.168.1.97:37959      104.16.133.65:443      
TIME_WAIT   -                   
tcp        0      0 192.168.1.97:3128       192.168.6.20:37600     
FIN_WAIT2   -                   
tcp        0      0 192.168.1.97:3128       192.168.1.164:49755    
ESTABLISHED 2230/(squid-1)      
tcp        0      0 192.168.1.97:49411      104.27.139.135:443     
ESTABLISHED 2230/(squid-1)      
tcp        0      0 192.168.1.97:34690      104.16.32.227:443      
TIME_WAIT   -                   
tcp        0      0 192.168.1.97:3128       192.168.6.20:37614     
FIN_WAIT2   -                   
tcp        0      0 192.168.1.97:59026      65.55.252.169:443      
TIME_WAIT   -                   
tcp        0      0 192.168.1.97:42895      64.233.186.100:443     
TIME_WAIT   -                   
tcp        0      0 192.168.1.97:39372      172.217.28.227:443     
TIME_WAIT   -                   
tcp        0      0 192.168.1.97:3128       192.168.1.164:49756    
ESTABLISHED 2230/(squid-1)      
tcp        0      0 192.168.1.97:58260      172.217.29.5:443       
ESTABLISHED 2230/(squid-1)      
tcp        0      0 192.168.1.97:3128       192.168.1.112:63944    
ESTABLISHED 2230/(squid-1)      
tcp        0      0 192.168.1.97:3128       192.168.1.113:51084    
ESTABLISHED 2230/(squid-1)      
tcp        0      0 192.168.1.97:3128       192.168.1.164:49675    
ESTABLISHED 2230/(squid-1)      
tcp        0      0 192.168.1.97:57194      172.217.29.5:443       
ESTABLISHED 2230/(squid-1)      
tcp        0      0 192.168.1.97:59196      162.125.18.133:443     
ESTABLISHED 2230/(squid-1)      
tcp        0      0 192.168.1.97:45477      104.16.127.226:443     
TIME_WAIT   -                   
tcp        0      0 192.168.1.97:50379      190.221.162.24:443     
TIME_WAIT   -                   
tcp        0      0 192.168.1.97:39389      172.217.28.227:443     
TIME_WAIT   -                   
tcp        0      0 192.168.1.97:3128       192.168.6.70:40482     
ESTABLISHED 2230/(squid-1)      
tcp        0      0 192.168.1.97:3128       192.168.1.112:63936    
ESTABLISHED 2230/(squid-1)      
tcp        0      0 192.168.1.97:3128       192.168.1.113:51085    
ESTABLISHED 2230/(squid-1)      
tcp        0      0 192.168.1.97:34255      192.95.23.56:8080      
TIME_WAIT   -                   
tcp        0      0 192.168.1.97:40632      172.217.28.229:443     
ESTABLISHED 2230/(squid-1)      
tcp        0      0 192.168.1.97:3128       192.168.6.20:53852     
ESTABLISHED 2230/(squid-1)      
tcp        0      0 192.168.1.97:56250      64.233.186.102:443     
TIME_WAIT   -                   
tcp        0      0 192.168.1.97:46664      172.217.29.13:443      
ESTABLISHED 2230/(squid-1)      
tcp        0      0 192.168.1.97:34731      104.16.126.226:443     
TIME_WAIT   -                   
tcp        0      0 192.168.1.97:44667      190.94.187.19:80       
TIME_WAIT   -                   
tcp        0      0 192.168.1.97:56045      64.233.186.102:443     
ESTABLISHED 2230/(squid-1)      
tcp        0      0 192.168.1.97:3128       192.168.1.147:49841    
ESTABLISHED 2230/(squid-1)      
tcp        0      0 192.168.1.97:57983      172.217.29.5:443       
ESTABLISHED 2230/(squid-1)      
tcp        0      0 192.168.1.97:3128       192.168.1.113:51079    
ESTABLISHED 2230/(squid-1)      
tcp        0      0 192.168.1.97:3128       192.168.1.112:63943    
ESTABLISHED 2230/(squid-1)      
tcp        0      0 192.168.1.97:38932      172.217.28.227:443     
TIME_WAIT   -                   
tcp        0      0 192.168.1.97:3128       192.168.1.167:49492    
ESTABLISHED 2230/(squid-1)      
tcp        0      0 192.168.1.97:45467      104.16.127.226:443     
TIME_WAIT   -                   
tcp        0      0 192.168.1.97:37942      104.16.133.65:443      
TIME_WAIT   -                   
tcp        0      0 192.168.1.97:34686      104.16.32.227:443      
TIME_WAIT   -                   
tcp        0      0 192.168.1.97:38876      209.225.49.65:443      
TIME_WAIT   -                   
tcp        0      0 192.168.1.97:52329      151.101.192.69:80      
ESTABLISHED 2230/(squid-1)      
tcp        0      0 192.168.1.97:57376      31.13.85.4:443         
ESTABLISHED 2230/(squid-1)      
tcp        0      0 192.168.1.97:55700      200.32.56.49:443       
ESTABLISHED 2230/(squid-1)      
tcp        0      0 192.168.1.97:56074      64.233.186.102:443     
TIME_WAIT   -                   
tcp        0      0 192.168.1.97:57873      172.217.29.5:443       
ESTABLISHED 2230/(squid-1)      
tcp        0      0 192.168.1.97:3128       192.168.1.164:49744    
ESTABLISHED 2230/(squid-1)      
tcp        0      0 192.168.1.97:46243      52.71.199.94:80        
ESTABLISHED 2230/(squid-1)      
tcp        0      0 192.168.1.97:53603      162.125.18.133:443     
ESTABLISHED 2230/(squid-1)      
tcp        0      0 192.168.1.97:3128       192.168.6.20:37620     
FIN_WAIT2   -                   
tcp        0      0 192.168.1.97:58296      172.217.29.5:443       
TIME_WAIT   -                   
tcp        0      0 192.168.1.97:3128       192.168.1.112:63947    
ESTABLISHED 2230/(squid-1)      
tcp        0      0 192.168.1.97:43854      64.233.186.189:443     
ESTABLISHED 2230/(squid-1)      
tcp        0      0 192.168.1.97:34735      104.16.126.226:443     
TIME_WAIT   -                   
tcp        0      0 192.168.1.97:51055      162.247.242.19:443     
TIME_WAIT   -                   
tcp        0      0 192.168.1.97:36860      64.233.186.132:443     
TIME_WAIT   -                   
tcp        0      0 192.168.1.97:46131      52.71.199.94:80        
TIME_WAIT   -                   
tcp        0      0 192.168.1.97:36996      31.13.85.8:443         
ESTABLISHED 2230/(squid-1)      
tcp        0      0 192.168.1.97:52216      64.233.186.94:443      
TIME_WAIT   -                   
tcp        0      0 192.168.1.97:45469      104.16.127.226:443     
TIME_WAIT   -                   
tcp        0      0 192.168.1.97:3128       192.168.6.20:37648     
TIME_WAIT   -                   
tcp        0      0 192.168.1.97:46969      216.33.197.78:80       
ESTABLISHED 2230/(squid-1)      
tcp        0      0 192.168.1.97:36246      172.217.28.229:443     
ESTABLISHED 2230/(squid-1)      
tcp        0      0 192.168.1.97:3128       192.168.1.91:51598     
ESTABLISHED 2230/(squid-1)      
tcp        0      0 192.168.1.97:39328      172.217.28.227:443     
TIME_WAIT   -                   
tcp        0      0 192.168.1.97:3128       192.168.6.20:37666     
TIME_WAIT   -                   
tcp        0      0 192.168.1.97:3128       192.168.6.70:40516     
ESTABLISHED 2230/(squid-1)      
tcp        0      0 192.168.1.97:41598      64.233.186.189:443     
ESTABLISHED 2230/(squid-1)      
tcp        0      0 192.168.1.97:43934      64.233.186.95:443      
ESTABLISHED 2230/(squid-1)      
tcp        0    191 192.168.1.97:55093      31.13.85.36:443        
ESTABLISHED 2230/(squid-1)      
tcp        0      0 192.168.1.97:47690      172.217.29.5:443       
ESTABLISHED 2230/(squid-1)      
tcp        0      0 192.168.1.97:55702      200.32.56.49:443       
ESTABLISHED 2230/(squid-1)      
tcp        0      0 192.168.1.97:3128       192.168.1.164:49762    
ESTABLISHED 2230/(squid-1)      
tcp        0      0 192.168.1.97:55956      104.28.26.91:443       
TIME_WAIT   -                   
tcp        0      0 192.168.1.97:3128       192.168.1.169:50577    
ESTABLISHED 2230/(squid-1)      
tcp        0      0 192.168.1.97:3128       192.168.1.62:51248     
ESTABLISHED 2230/(squid-1)      
tcp        0      0 192.168.1.97:47428      104.209.214.88:443     
ESTABLISHED 2230/(squid-1)      
tcp        0      0 192.168.1.97:3128       192.168.1.164:49747    
ESTABLISHED 2230/(squid-1)      
tcp        0      0 192.168.1.97:41494      216.58.212.131:80      
TIME_WAIT   -                   
tcp        0      0 192.168.1.97:56290      64.233.186.102:443     
TIME_WAIT   -                   
tcp        0      0 192.168.1.97:3128       192.168.6.20:37640     
TIME_WAIT   -                   
tcp        0      0 192.168.1.97:32931      23.76.32.81:443        
ESTABLISHED 2230/(squid-1)      
tcp        0      0 192.168.1.97:3128       192.168.1.113:51087    
ESTABLISHED 2230/(squid-1)      
tcp        0      0 192.168.1.97:34733      104.16.126.226:443     
TIME_WAIT   -                   
tcp        0      0 192.168.1.97:3128       192.168.1.164:49730    
ESTABLISHED 2230/(squid-1)      
tcp        0      0 192.168.1.97:3128       192.168.1.113:51086    
ESTABLISHED 2230/(squid-1)      
tcp        0      0 192.168.1.97:3128       192.168.1.85:49808     
ESTABLISHED 2230/(squid-1)      
tcp        0      0 192.168.1.97:3128       192.168.1.132:50179    
ESTABLISHED 2230/(squid-1)      
tcp        0      0 192.168.1.97:56136      64.233.186.139:443     
TIME_WAIT   -                   
tcp        0      0 192.168.1.97:3128       192.168.1.26:51157     
ESTABLISHED 2230/(squid-1)      
tcp        0      0 192.168.1.97:3128       192.168.1.91:51583     
ESTABLISHED 2230/(squid-1)      
tcp        0      0 192.168.1.97:52296      64.233.186.94:443      
TIME_WAIT   -                   
tcp        0      0 192.168.1.97:3128       192.168.1.18:54112     
ESTABLISHED 2230/(squid-1)      
tcp        0      0 192.168.1.97:3128       192.168.1.91:51582     
ESTABLISHED 2230/(squid-1)      
tcp        0      0 192.168.1.97:52198      64.233.186.94:443      
TIME_WAIT   -                   
tcp        0      0 192.168.1.97:38934      172.217.28.227:443     
TIME_WAIT   -                   
tcp        0      0 192.168.1.97:52329      172.217.29.2:443       
TIME_WAIT   -                   
tcp        0      0 192.168.1.97:38344      172.217.29.1:443       
TIME_WAIT   -                   
tcp        0      0 192.168.1.97:3128       192.168.1.91:51595     
ESTABLISHED 2230/(squid-1)      
tcp        0      0 192.168.1.97:60894      104.20.62.49:443       
ESTABLISHED 2230/(squid-1)      
tcp        0      0 192.168.1.97:3128       192.168.1.91:51591     
ESTABLISHED 2230/(squid-1)      
tcp        0      0 192.168.1.97:3128       192.168.1.26:51156     
ESTABLISHED 2230/(squid-1)      
tcp6       0      0 :::80                   :::*                    LISTEN     
965/httpd           
tcp6       0      0 :::22                   :::*                    LISTEN     
971/sshd            
tcp6       0      0 :::5666                 :::*                    LISTEN     
1027/nrpe 



--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/Strange-behavior-reload-service-failed-but-not-start-tp4681317p4681322.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From erdosain9 at gmail.com  Wed Jan 25 18:03:09 2017
From: erdosain9 at gmail.com (erdosain9)
Date: Wed, 25 Jan 2017 10:03:09 -0800 (PST)
Subject: [squid-users] Antivirus for squid
Message-ID: <1485367389664-4681323.post@n4.nabble.com>

Hi to all.
Im a little confuse about this... i just want "antivirus", i dont care block
some web, filter, etc. (at least no more that what i get with squid)... so,
just for antivirus, what recommend???
clamav
squidclamav
squidguard
????
Somebody have a tutorial to install something of this on Centos7??
Thanks



--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/Antivirus-for-squid-tp4681323.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From yvoinov at gmail.com  Wed Jan 25 18:08:15 2017
From: yvoinov at gmail.com (Yuri Voinov)
Date: Thu, 26 Jan 2017 00:08:15 +0600
Subject: [squid-users] Antivirus for squid
In-Reply-To: <1485367389664-4681323.post@n4.nabble.com>
References: <1485367389664-4681323.post@n4.nabble.com>
Message-ID: <3dbe39e9-87ea-8b63-73a1-63c30d3f86ae@gmail.com>



26.01.2017 0:03, erdosain9 ?????:
> Hi to all.
> Im a little confuse about this... i just want "antivirus", i dont care block
> some web, filter, etc. (at least no more that what i get with squid)... so,
> just for antivirus, what recommend???
> clamav
You thing you have a choise? All others AV is commercial.
> squidclamav
squidclamav is not AV, it is ICAP adapter for AV.
> squidguard
This is not AV at all.
> ????
> Somebody have a tutorial to install something of this on Centos7??
Common example on Squid's wiki. There is no tutorial for all and any
"OS" on the Earth. Adapt wiki example yourself to any OS you want.
> Thanks
>
>
>
> --
> View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/Antivirus-for-squid-tp4681323.html
> Sent from the Squid - Users mailing list archive at Nabble.com.
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users

-------------- next part --------------
A non-text attachment was scrubbed...
Name: 0x613DEC46.asc
Type: application/pgp-keys
Size: 2437 bytes
Desc: not available
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20170126/ec388c20/attachment.key>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: signature.asc
Type: application/pgp-signature
Size: 473 bytes
Desc: OpenPGP digital signature
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20170126/ec388c20/attachment.sig>

From rafael.akchurin at diladele.com  Wed Jan 25 18:36:20 2017
From: rafael.akchurin at diladele.com (Rafael Akchurin)
Date: Wed, 25 Jan 2017 18:36:20 +0000
Subject: [squid-users] Antivirus for squid
In-Reply-To: <3dbe39e9-87ea-8b63-73a1-63c30d3f86ae@gmail.com>
References: <1485367389664-4681323.post@n4.nabble.com>
 <3dbe39e9-87ea-8b63-73a1-63c30d3f86ae@gmail.com>
Message-ID: <VI1PR0401MB2685AC837199F2249A7B13938F740@VI1PR0401MB2685.eurprd04.prod.outlook.com>

Greetings all,

One possible tutorial for Squid Clam AV on Ubuntu 16 - https://docs.diladele.com/administrator_guide_4_9/antivirus/index.html
Unfortunately it references our Web UI - but I guess people familiar with inner directives of squid.conf can easily adapt it for their needs.

It works basically but sometimes ICAP chain fails on some sites - although both ICAP services working not in a chain have no such errors... still looking for a solution.

Best regards,
Rafael Akchurin
Diladele B.V.

-----Original Message-----
From: squid-users [mailto:squid-users-bounces at lists.squid-cache.org] On Behalf Of Yuri Voinov
Sent: Wednesday, January 25, 2017 7:08 PM
To: squid-users at lists.squid-cache.org
Subject: Re: [squid-users] Antivirus for squid



26.01.2017 0:03, erdosain9 ?????:
> Hi to all.
> Im a little confuse about this... i just want "antivirus", i dont care 
> block some web, filter, etc. (at least no more that what i get with 
> squid)... so, just for antivirus, what recommend???
> clamav
You thing you have a choise? All others AV is commercial.
> squidclamav
squidclamav is not AV, it is ICAP adapter for AV.
> squidguard
This is not AV at all.
> ????
> Somebody have a tutorial to install something of this on Centos7??
Common example on Squid's wiki. There is no tutorial for all and any "OS" on the Earth. Adapt wiki example yourself to any OS you want.
> Thanks
>
>
>
> --
> View this message in context: 
> http://squid-web-proxy-cache.1019090.n4.nabble.com/Antivirus-for-squid
> -tp4681323.html Sent from the Squid - Users mailing list archive at 
> Nabble.com.
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users


From baborucki at gmail.com  Wed Jan 25 20:22:04 2017
From: baborucki at gmail.com (boruc)
Date: Wed, 25 Jan 2017 12:22:04 -0800 (PST)
Subject: [squid-users] Not all html objects are being cached
In-Reply-To: <1485273173646-4681293.post@n4.nabble.com>
References: <1485273173646-4681293.post@n4.nabble.com>
Message-ID: <1485375724827-4681326.post@n4.nabble.com>

After a little bit of analyzing requests and responses with WireShark I
noticed that many sites that weren't cached had different combination of
below parameters:

Cache-Control: no-cache, no-store, must-revalidate, post-check, pre-check,
private, public, max-age, public
Pragma: no-cache

There is a possibility to disable this in squid by using
request_header_access and reply_header_access, however it doesn't work for
me, many pages aren't still in cache. I am currently using lines below:

request_header_access Cache-Control deny all
request_header_access Pragma deny all
request_header_access Accept-Encoding deny all
reply_header_access Cache-Control deny all
reply_header_access Pragma deny all
reply_header_access Accept-Encoding deny all

I could also try refresh_pattern, but I don't think that code below will
work because not every URL ends with .html or .htm (because you visit
/www.example.com/, not /www.example.com/index.html/)
refresh_pattern -i \.(html|htm)$          1440   40% 40320 ignore-no-cache
ignore-no-store ignore-private override-expire reload-into-ims

Thank you in advance.



--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/Not-all-html-objects-are-being-cached-tp4681293p4681326.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From yvoinov at gmail.com  Wed Jan 25 20:44:57 2017
From: yvoinov at gmail.com (Yuri Voinov)
Date: Thu, 26 Jan 2017 02:44:57 +0600
Subject: [squid-users] Not all html objects are being cached
In-Reply-To: <1485375724827-4681326.post@n4.nabble.com>
References: <1485273173646-4681293.post@n4.nabble.com>
 <1485375724827-4681326.post@n4.nabble.com>
Message-ID: <85b108fb-1f3e-d7d5-48e1-37edbb2ae2d9@gmail.com>



26.01.2017 2:22, boruc ?????:
> After a little bit of analyzing requests and responses with WireShark I
> noticed that many sites that weren't cached had different combination of
> below parameters:
>
> Cache-Control: no-cache, no-store, must-revalidate, post-check, pre-check,
> private, public, max-age, public
> Pragma: no-cache
If the webmaster has done this - he had good reason to. Trying to break
the RFC in this way, you break the Internet.
>
> There is a possibility to disable this in squid by using
Don't do it.
> request_header_access and reply_header_access, however it doesn't work for
> me, many pages aren't still in cache. I am currently using lines below:
>
> request_header_access Cache-Control deny all
> request_header_access Pragma deny all
> request_header_access Accept-Encoding deny all
> reply_header_access Cache-Control deny all
> reply_header_access Pragma deny all
> reply_header_access Accept-Encoding deny all
>
> I could also try refresh_pattern, but I don't think that code below will
> work because not every URL ends with .html or .htm (because you visit
> /www.example.com/, not /www.example.com/index.html/)
> refresh_pattern -i \.(html|htm)$          1440   40% 40320 ignore-no-cache
> ignore-no-store ignore-private override-expire reload-into-ims
>
> Thank you in advance.
You're welcome.
>
>
>
> --
> View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/Not-all-html-objects-are-being-cached-tp4681293p4681326.html
> Sent from the Squid - Users mailing list archive at Nabble.com.
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users

-------------- next part --------------
A non-text attachment was scrubbed...
Name: 0x613DEC46.asc
Type: application/pgp-keys
Size: 2437 bytes
Desc: not available
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20170126/09f86cb3/attachment.key>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: signature.asc
Type: application/pgp-signature
Size: 473 bytes
Desc: OpenPGP digital signature
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20170126/09f86cb3/attachment.sig>

From squid3 at treenet.co.nz  Wed Jan 25 23:18:18 2017
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 26 Jan 2017 12:18:18 +1300
Subject: [squid-users] LDAP acl groups
In-Reply-To: <CAG+8EEapWQ+HxL1=r66oLFphpL7-f0SCcCTUjwncnYcaAPBsEg@mail.gmail.com>
References: <CAG+8EEapWQ+HxL1=r66oLFphpL7-f0SCcCTUjwncnYcaAPBsEg@mail.gmail.com>
Message-ID: <219f3c68-59f5-51b9-77b1-4b38d34a2d3d@treenet.co.nz>

On 26/01/2017 4:28 a.m., Leonardo Bacha Abrantes wrote:
> Hi guys,
> 
> I have an active directory running on windows server 2008 r2 and squid
> (version 3.5.20 - CentOS 7) authenticating via LDAP (without kerberos).
> The ldap authentication is working, the trouble is to create ACLs based on
> active directory groups.
> 
> 
> OBS: When I run both basic_ldap_auth and ext_ldap_group_acl commands
> manually as squid user in console to test, I receive 'OK' as answer.
> 
> 
> --->>> My squid.conf:
> 
> auth_param basic program /usr/lib64/squid/basic_ldap_auth -P -R -b
> ou=Users,ou=city,ou=country,dc=company,dc=local -D
> CN=bindUser,DC=company,DC=local -W PasswdFile -f sAMAccountName=%s -h
> 192.168.1.9
> auth_param basic children 10
> auth_param basic realm XXXXX
> auth_param basic credentialsttl 10 minutes
> 
> external_acl_type memberof %LOGIN /usr/lib64/squid/ext_ldap_group_acl -P -R
> -b OU=city,OU=country,DC=company,DC=local -D
> CN=bindUser,DC=company,DC=local -W PasswdFile -h 192.168.1.9 -f
> '(&(objectClass=person)(sAMAccountName=%v)(memberOf=CN=%a,OU=Groups,OU=city,OU=country,dc=company,dc=local))'
> 
> #Also tried memberOf=CN=%*g*
> 
> acl fullaccess  external memberof squid_fullaccess
> 
> acl LdapUsers proxy_auth REQUIRED
> http_access allow fullaccess LdapUsers


Two things that you really NEED to know:

1) Order is important.
<http://wiki.squid-cache.org/SquidFaq/OrderIsImportant>

So the http_access line you are using tells Squid to check the group and
only for members of the group is authentication to be performed.

How exactly do you expect Squid to know what user to check the groups
for *before* authentication has happened?


2) the popup is a browser decision.

All Squid is doing is telling the browser that credentials are needed to
use the proxy, and what types it can accept. If the browser were
properly doing its SSO the popup would not happen. There is nothing we
can do about that.


What you should do is deny non-authenticated users and only then check
the groups for people who have authenticated:

 http_access deny !LdapUsers
 http_access allow fullaccess

If the popup still occurs, then consder whether the browser was supposed
to have the right credentials to begin with (ie. registered to the AD
domin controller). If it does why is it not sending them instead of
doing the popup.


Amos



From squid3 at treenet.co.nz  Thu Jan 26 01:01:54 2017
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 26 Jan 2017 14:01:54 +1300
Subject: [squid-users] Not all html objects are being cached
In-Reply-To: <85b108fb-1f3e-d7d5-48e1-37edbb2ae2d9@gmail.com>
References: <1485273173646-4681293.post@n4.nabble.com>
 <1485375724827-4681326.post@n4.nabble.com>
 <85b108fb-1f3e-d7d5-48e1-37edbb2ae2d9@gmail.com>
Message-ID: <609d443c-daeb-8c0e-b4b8-ab5be13f8d76@treenet.co.nz>

On 26/01/2017 9:44 a.m., Yuri Voinov wrote:
> 
> 
> 26.01.2017 2:22, boruc ?????:
>> After a little bit of analyzing requests and responses with WireShark I
>> noticed that many sites that weren't cached had different combination of
>> below parameters:
>>
>> Cache-Control: no-cache, no-store, must-revalidate, post-check, pre-check,
>> private, public, max-age, public
>> Pragma: no-cache
> If the webmaster has done this - he had good reason to. Trying to break
> the RFC in this way, you break the Internet.

Instead use the latest Squid you can. Squid by default caches as much as
it can within the restrictions imposed by the web environment. But
'latest is best' etc. since we are still working on support for HTTP/1.1
features.


I recommend you use the tool at <http://redbot.org> to check URLs
cacheability instead of wireshark. It will tell you what those controls
actually *mean* in regards to cacheability, not just that they are used.
And whether there are other problems you may not have noticed in the
various different ways there are to fetch any given URL.


The Squid options available are mostly for disabling some caching
operation - so that if you are in a situation where disabling operation
X causes operation Y to cache better you can tune the behaviour.

You can't really *force* things which are not cacheable to be stored.
They will just be replaced with a newer copy shortly after with no
benefit gained - just some possibly nasty side effects, or real monetary
costs.


>>
>> There is a possibility to disable this in squid by using
> Don't do it.
>> request_header_access and reply_header_access, however it doesn't work for
>> me, many pages aren't still in cache. I am currently using lines below:
>>
>> request_header_access Cache-Control deny all
>> request_header_access Pragma deny all
>> request_header_access Accept-Encoding deny all
>> reply_header_access Cache-Control deny all
>> reply_header_access Pragma deny all
>> reply_header_access Accept-Encoding deny all
>>

Ah, changing the headers on the *outgoing* traffic does not in any way
affect how Squid interprets the _previously_ received inbound messages.

==> In other words; doing the above is pointless and screws everybody
using your proxy over. Dont do that.


By erasing the Cache-Controls response header delivered along with that
content you are technically in violation of International copyright laws.
==> Dont do that.


By removing the Accept-Encoding on requests (only) you can improve HIT
ratio (only a small amount), but at cost of 50-90% bandwidth increase on
each MISS - so the cost increase usually swamps the gains.

==> Making this change lead to the opposite of what you intended. Dont
do that.


Removing the Accept-Encoding header on responses. Is just pointless. It
controls POST/PUT payload data, which Squid cannot cache anyway. So all
you did was prevent the clients using less bandwidth.

==> More bandwidth, more costs. Dont do that.


Removing the Pragma header is also pointless. It's used by very ancient
software from the 1990's and such.

==> if the web application was actually using the Pragma for anything
important (some do) you just screwed them over with no gains to
yourself. Dont do that.


>> I could also try refresh_pattern, but I don't think that code below will
>> work because not every URL ends with .html or .htm (because you visit
>> /www.example.com/, not /www.example.com/index.html/)
>> refresh_pattern -i \.(html|htm)$          1440   40% 40320 ignore-no-cache
>> ignore-no-store ignore-private override-expire reload-into-ims
>>


Quite. So configure the correct options.

No software is psychic enough to do operation X which you want, when you
configure it to do *only* some other non-X operation.


Amos



From squid3 at treenet.co.nz  Thu Jan 26 02:57:48 2017
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 26 Jan 2017 15:57:48 +1300
Subject: [squid-users] Strange behavior - reload service failed,
 but not start....
In-Reply-To: <1485362331939-4681322.post@n4.nabble.com>
References: <1485349951724-4681317.post@n4.nabble.com>
 <CA+sSnVbSgDGBSzTVNbomVOKzH5oAd+vKB0JGVdjxai3rjihHYA@mail.gmail.com>
 <1485362331939-4681322.post@n4.nabble.com>
Message-ID: <33d99c9f-8c63-65c2-9cfb-bb217ba5c176@treenet.co.nz>

On 26/01/2017 5:38 a.m., erdosain9 wrote:
> Hi,........ no
> 
> [root at squid ~]# df -h
> S.ficheros              Tama?o Usados  Disp Uso% Montado en
> /dev/mapper/centos-root    48G    16G   33G  32% /
> devtmpfs                  896M      0  896M   0% /dev
> tmpfs                     906M   2,1M  904M   1% /dev/shm
> tmpfs                     906M   8,5M  898M   1% /run
> tmpfs                     906M      0  906M   0% /sys/fs/cgroup
> /dev/sda1                 497M   141M  356M  29% /boot
> tmpfs                     182M      0  182M   0% /run/user/0
> 
> 
> by the way, this error dosent appear anymore, but was the first error i
> noticed after the bad reboot. (i think that maybe i fix that with "squid
> -z"....

I think you changed the size of a cache_dir right?
The log messages seems to be Squid busily trying to clear the directory
size down to what is configured, while systemd keeps trying to force
start and restart of overlapping processes mid-action.


> 
> some other approach??
> 

Not using systemd to control Squid-3. The two are not compatible. As you
just found out the hard way.

Squid is not a daemon, it is a Daemon + Manager in one binary/process.
systemd is based around the naive assumption that everything is a simple
daemon and gets horribly confuzled when reality bites. It is not alone,
upstart has the same issues. Basically only start/stop work, and even
those only most of the time if done very carefully.

Your choices with systemd are (1) use the 'squid -k' commands, or (2)
upgrade to Squid-4 and install the tools/systemd/squid.service file we
provide for that version.

Amos



From rentorbuy at yahoo.com  Thu Jan 26 10:16:13 2017
From: rentorbuy at yahoo.com (Vieri)
Date: Thu, 26 Jan 2017 10:16:13 +0000 (UTC)
Subject: [squid-users] squid reverse proxy (accelerator) for MS Exchange
 OWA
In-Reply-To: <0f2d2e08-bbd1-656a-a83b-5bb159983cbd@measurement-factory.com>
References: <1216522442.160455.1484870639800.ref@mail.yahoo.com>
 <1216522442.160455.1484870639800@mail.yahoo.com>
 <cdea0afa-c2a2-23c4-dbac-553a565e83f7@treenet.co.nz>
 <690032068.431187.1484905491833@mail.yahoo.com>
 <5517395f-eaa4-0b3a-34e9-66ea8d0122a1@treenet.co.nz>
 <1374872097.4095106.1485244925609@mail.yahoo.com>
 <94978436-88e5-fc84-9f3e-b8f1757402a5@measurement-factory.com>
 <2010900066.469374.1485330325639@mail.yahoo.com>
 <0f2d2e08-bbd1-656a-a83b-5bb159983cbd@measurement-factory.com>
Message-ID: <757248455.1520252.1485425773189@mail.yahoo.com>



----- Original Message -----
From: Alex Rousskov <rousskov at measurement-factory.com>
> If my reconstruction of the events was correct, then OpenSSL supplied as
> much information as it could -- the "unsupported TLS/SSL versions" is
> _your_ conclusion based on the information that neither Squid nor
> OpenSSL had access to.
>
> 
>> I'm only supposing that
>> without the ssloptions I posted above, openssl will try TLS 1.2 and
>> silently fail if that doesn't succeed.
>
> It takes two to tango. How silent that failure is depends, in part, on
> the server. AFAICT, your server was 100% silent about the reason behind
> its abrupt connection closure, and OpenSSL correctly declined to
> speculate about those reasons due to lack of info. From OpenSSL/client
> point of view, it could have been anything from an unsupported TLS
> version to a crashed server.


Thanks for taking the time to explain. I understand the point you make but I'm still a bit scepticle, probably due to my lack of knowledge in this domain.

I haven't read the RFCs for TLSv1*, SSLv*, etc. However, let's try to give a simple and straightforward example, just to clear things up. Suppose you (the client) meet someone (the server) and ask her/him out. That person can turn away and refuse your proposal without saying a word so you'll never know the reason. That's what you explained and I get that. However, you must "obviously" know what you told that person. Maybe it's not that "obvious" in the case of Squid & OpenSSL, but I'm guessing that it should be possible for Squid to tell OpenSSL to report what it actually said to the server without the need for an admin to do a traffic dump and analysis.

Let's take this simple example into consideration where I use cURL to connect to the same MS Exchange server:

# curl -k -v https://10.215.144.21
* Rebuilt URL to: https://10.215.144.21/
*   Trying 10.215.144.21...
* Connected to 10.215.144.21 (10.215.144.21) port 443 (#0)
* ALPN, offering http/1.1
* Cipher selection: ALL:!EXPORT:!EXPORT40:!EXPORT56:!aNULL:!LOW:!RC4:@STRENGTH
* successfully set certificate verify locations:
*   CAfile: /etc/ssl/certs/ca-certificates.crt
CApath: /etc/ssl/certs
* TLSv1.2 (OUT), TLS header, Certificate Status (22):
* TLSv1.2 (OUT), TLS handshake, Client hello (1):
* Unknown SSL protocol error in connection to 10.215.144.21:443 
* Closing connection 0
curl: (35) Unknown SSL protocol error in connection to 10.215.144.21:443

Now that's clear. cURL tried to use TLS1.2 and failed. The nasty server didn't even say hello.

It's interesting to note that the following actually DOES give more information (unsupported protocol):

# curl --tlsv1.1 -k -v https://10.215.144.21/
*   Trying 10.215.144.21...
* Connected to 10.215.144.21 (10.215.144.21) port 443 (#0)
* ALPN, offering http/1.1
* Cipher selection: ALL:!EXPORT:!EXPORT40:!EXPORT56:!aNULL:!LOW:!RC4:@STRENGTH
* successfully set certificate verify locations:
*   CAfile: /etc/ssl/certs/ca-certificates.crt
CApath: /etc/ssl/certs
* TLSv1.1 (OUT), TLS header, Certificate Status (22):
* TLSv1.1 (OUT), TLS handshake, Client hello (1):
* error:14077102:SSL routines:SSL23_GET_SERVER_HELLO:unsupported protocol
* Closing connection 0
curl: (35) error:14077102:SSL routines:SSL23_GET_SERVER_HELLO:unsupported protocol


Of course, the following test succeeds:

# curl --tlsv1.0 -k -v https://10.215.144.21/
*   Trying 10.215.144.21...
* Connected to 10.215.144.21 (10.215.144.21) port 443 (#0)
* ALPN, offering http/1.1
* Cipher selection: ALL:!EXPORT:!EXPORT40:!EXPORT56:!aNULL:!LOW:!RC4:@STRENGTH
* successfully set certificate verify locations:
*   CAfile: /etc/ssl/certs/ca-certificates.crt
CApath: /etc/ssl/certs
* TLSv1.0 (OUT), TLS header, Certificate Status (22):
* TLSv1.0 (OUT), TLS handshake, Client hello (1):
* TLSv1.0 (IN), TLS handshake, Server hello (2):
* TLSv1.0 (IN), TLS handshake, Certificate (11):
* TLSv1.0 (IN), TLS handshake, Server finished (14):
* TLSv1.0 (OUT), TLS handshake, Client key exchange (16):
* TLSv1.0 (OUT), TLS change cipher, Client hello (1):
* TLSv1.0 (OUT), TLS handshake, Finished (20):
* TLSv1.0 (IN), TLS change cipher, Client hello (1):
* TLSv1.0 (IN), TLS handshake, Finished (20):
* SSL connection using TLSv1.0 / DES-CBC3-SHA


Now with the openssl command-line client:

#  openssl s_client -connect 10.215.144.21:443 -tls1_2
CONNECTED(00000003)
3072153276:error:1409E0E5:SSL routines:ssl3_write_bytes:ssl handshake failure:s3_pkt.c:656:


#  openssl s_client -connect 10.215.144.21:443 -tls1_1
CONNECTED(00000003)
3072530108:error:1408F10B:SSL routines:SSL3_GET_RECORD:wrong version number:s3_pkt.c:362:


...and it obviously works with -tls1.

I haven't looked at the source code but I guess Squid uses the OpenSSL library.

I searched the Squid log above and below these lines:

2017/01/24 17:20:28.997 kid1| 83,5| NegotiationHistory.cc(83) retrieveNegotiatedInfo: SSL connection info on FD 18 SSL version NONE/0.0 negotiated cipher 
2017/01/24 17:20:28.997 kid1| Error negotiating SSL on FD 18: error:00000000:lib(0):func(0):reason(0) (5/0/0)
2017/01/24 17:20:28.997 kid1| TCP connection to 10.215.144.21/443 failed


However, I haven't found any hint of what the client (cache_peer) tried to offer.

Maybe if Squid gets an SSL negotiation error with no apparent reason then it might need to retry connecting by being more explicit, just like in my cURL and openssl binary examples above.


I used the latest Squid 4 beta by the way.

I would have understood earlier the reason of the connection failure if Squid/OpenSSL had logged how they were actually hitting on the server.

Anyway, it's not a big deal now that I know what to do if this kind of connection issue comes back up. It could be useful to others though if the logging could be a tad more verbose or if Squid could retry connections by explictly specifying protocols (and logging them).

Thanks,

Vieri


From goal81 at gmail.com  Thu Jan 26 16:41:21 2017
From: goal81 at gmail.com (Alexander)
Date: Thu, 26 Jan 2017 08:41:21 -0800 (PST)
Subject: [squid-users] Native FTP relay: connection closes (?) after
 'cannot assign requested address' error
In-Reply-To: <c5c6ec70-983f-03f5-bb3f-a1db4ffddb17@measurement-factory.com>
References: <CACMUsGz1rDaY4ffe7vRp-u-cOANuufPfVo+BgvNTmi3t8ywOMA@mail.gmail.com>
 <832f83c5-dc2b-574b-9ab0-80142862bec9@treenet.co.nz>
 <1485092987028-4681242.post@n4.nabble.com>
 <5a1e3e82-4f84-7246-0437-113dd99e957b@treenet.co.nz>
 <1485195091976-4681258.post@n4.nabble.com>
 <8d4fc039-bbd8-3a23-cae1-84ff43ff40e6@measurement-factory.com>
 <CACMUsGw1CxzsPBA6-Tz7zy-RqpnxFn_UqLugz19o9VXR-cLEqg@mail.gmail.com>
 <c5c6ec70-983f-03f5-bb3f-a1db4ffddb17@measurement-factory.com>
Message-ID: <1485448881841-4681332.post@n4.nabble.com>

It seems that I have solved the issue by using nf_conntrack_ftp and
redirecting "NEW,RELATED" traffic to squid:

ftp_port 2121 intercept

modprobe nf_conntrack_ftp ports=2121

iptables -t nat -A PREROUTING -p tcp --dport 21 -j REDIRECT --to-port 2121
iptables -t nat -A PREROUTING -p tcp -m state --state NEW,RELATED -j
REDIRECT

Thank you for your time.



--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/Native-FTP-relay-connection-closes-after-cannot-assign-requested-address-error-tp4681208p4681332.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From Antony.Stone at squid.open.source.it  Thu Jan 26 17:01:27 2017
From: Antony.Stone at squid.open.source.it (Antony Stone)
Date: Thu, 26 Jan 2017 18:01:27 +0100
Subject: [squid-users] Native FTP relay: connection closes (?) after
	'cannot assign requested address' error
In-Reply-To: <1485448881841-4681332.post@n4.nabble.com>
References: <CACMUsGz1rDaY4ffe7vRp-u-cOANuufPfVo+BgvNTmi3t8ywOMA@mail.gmail.com>
 <c5c6ec70-983f-03f5-bb3f-a1db4ffddb17@measurement-factory.com>
 <1485448881841-4681332.post@n4.nabble.com>
Message-ID: <201701261801.27407.Antony.Stone@squid.open.source.it>

On Thursday 26 January 2017 at 17:41:21, Alexander wrote:

> It seems that I have solved the issue by using nf_conntrack_ftp and
> redirecting "NEW,RELATED" traffic to squid:

Excellent news.

> ftp_port 2121 intercept
> 
> modprobe nf_conntrack_ftp ports=2121
> 
> iptables -t nat -A PREROUTING -p tcp --dport 21 -j REDIRECT --to-port 2121
> iptables -t nat -A PREROUTING -p tcp -m state --state NEW,RELATED -j
> REDIRECT

Just out of interest, how are you getting the FTP traffic to the Squid box in 
the first place?

I assume you're not routing all Internet-bound traffic via this machine 
(otherwise that second REDIRECT rule would cause problems for SSH, SMTP, IMAP, 
etc), so how are you identifying the FTP traffic to get it from your router to 
the Squid box?


Antony.

-- 
Police have found a cartoonist dead in his house.  They say that details are 
currently sketchy.

                                                   Please reply to the list;
                                                         please *don't* CC me.


From goal81 at gmail.com  Thu Jan 26 17:47:15 2017
From: goal81 at gmail.com (Alexander)
Date: Thu, 26 Jan 2017 09:47:15 -0800 (PST)
Subject: [squid-users] Native FTP relay: connection closes (?) after
 'cannot assign requested address' error
In-Reply-To: <201701261801.27407.Antony.Stone@squid.open.source.it>
References: <CACMUsGz1rDaY4ffe7vRp-u-cOANuufPfVo+BgvNTmi3t8ywOMA@mail.gmail.com>
 <832f83c5-dc2b-574b-9ab0-80142862bec9@treenet.co.nz>
 <1485092987028-4681242.post@n4.nabble.com>
 <5a1e3e82-4f84-7246-0437-113dd99e957b@treenet.co.nz>
 <1485195091976-4681258.post@n4.nabble.com>
 <8d4fc039-bbd8-3a23-cae1-84ff43ff40e6@measurement-factory.com>
 <CACMUsGw1CxzsPBA6-Tz7zy-RqpnxFn_UqLugz19o9VXR-cLEqg@mail.gmail.com>
 <c5c6ec70-983f-03f5-bb3f-a1db4ffddb17@measurement-factory.com>
 <1485448881841-4681332.post@n4.nabble.com>
 <201701261801.27407.Antony.Stone@squid.open.source.it>
Message-ID: <1485452835118-4681334.post@n4.nabble.com>

Well, actually these rules are just a kind of proof of concept and there is
something to think about later. The redirection rule should be more precise
and include destination address. Also, 'NEW' state should probably be
excluded from the list.



--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/Native-FTP-relay-connection-closes-after-cannot-assign-requested-address-error-tp4681208p4681334.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From rousskov at measurement-factory.com  Thu Jan 26 18:18:01 2017
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Thu, 26 Jan 2017 11:18:01 -0700
Subject: [squid-users] squid reverse proxy (accelerator) for MS Exchange
 OWA
In-Reply-To: <757248455.1520252.1485425773189@mail.yahoo.com>
References: <1216522442.160455.1484870639800.ref@mail.yahoo.com>
 <1216522442.160455.1484870639800@mail.yahoo.com>
 <cdea0afa-c2a2-23c4-dbac-553a565e83f7@treenet.co.nz>
 <690032068.431187.1484905491833@mail.yahoo.com>
 <5517395f-eaa4-0b3a-34e9-66ea8d0122a1@treenet.co.nz>
 <1374872097.4095106.1485244925609@mail.yahoo.com>
 <94978436-88e5-fc84-9f3e-b8f1757402a5@measurement-factory.com>
 <2010900066.469374.1485330325639@mail.yahoo.com>
 <0f2d2e08-bbd1-656a-a83b-5bb159983cbd@measurement-factory.com>
 <757248455.1520252.1485425773189@mail.yahoo.com>
Message-ID: <e13696a8-4540-bdd8-0634-902b737b82e4@measurement-factory.com>

On 01/26/2017 03:16 AM, Vieri wrote:

> I'm guessing that it
> should be possible for Squid to tell OpenSSL to report what it
> actually said to the server without the need for an admin to do a
> traffic dump and analysis.

Your are correct, but, in most cases, it is a lot easier to dump and
analyze traffic than to ask OpenSSL inside Squid to report what it
actually said. This is, in part, Squid's own fault, but OpenSSL does not
make it easy either. More on that below at (**).


> Let's take this simple example into consideration where I use cURL to connect to the same MS Exchange server:
> 
> # curl -k -v https://10.215.144.21
> * Rebuilt URL to: https://10.215.144.21/
> *   Trying 10.215.144.21...
> * Connected to 10.215.144.21 (10.215.144.21) port 443 (#0)
> * ALPN, offering http/1.1
> * Cipher selection: ALL:!EXPORT:!EXPORT40:!EXPORT56:!aNULL:!LOW:!RC4:@STRENGTH
> * successfully set certificate verify locations:
> *   CAfile: /etc/ssl/certs/ca-certificates.crt
> CApath: /etc/ssl/certs
> * TLSv1.2 (OUT), TLS header, Certificate Status (22):
> * TLSv1.2 (OUT), TLS handshake, Client hello (1):
> * Unknown SSL protocol error in connection to 10.215.144.21:443 
> * Closing connection 0
> curl: (35) Unknown SSL protocol error in connection to 10.215.144.21:443
> 
> Now that's clear. cURL tried to use TLS1.2 and failed. The nasty server didn't even say hello.

Please note that the information you see above details what Curl did,
not what OpenSSL did.

(**) Squid prints many similar details as well, but because Squid
generally deals with many concurrent transactions over a long time, and
does a lot more than Curl does, those details are not as easy to
find/isolate in Squid debugging logs. You found some of them. Again,
Squid could do a lot better in this area, but nobody is working on that
right now AFAIK, and even my attempts to form consensus on how this
should be done have failed so far.


> It's interesting to note that the following actually DOES give more information (unsupported protocol):

* If the server sent nothing, then Curl gave you potentially incorrect
information (i.e., Curl is just _guessing_ what went wrong).

* If the server sent something, then either Squid situation was
different or I did not see that additional info in the logs you have
posted.

Based on everything I have seen so far, it is probably the former -- the
server sent nothing.


> I haven't looked at the source code but I guess Squid uses the OpenSSL library.

Yes, in your case, it does.


> However, I haven't found any hint of what the client (cache_peer) tried to offer.

Cache_peer is not the client here, but yes, see (**) above.


> Maybe if Squid gets an SSL negotiation error with no apparent reason
> then it might need to retry connecting by being more explicit, just
> like in my cURL and openssl binary examples above.

Sorry, I do not know what "retry connecting by being more explicit"
means. AFAICT, neither Curl nor s_client tried reconnecting in your
examples. Also, an appropriate default for a command-line client is
often a bad default for a proxy. It is complicated.


> I would have understood earlier the reason of the connection failure
> if Squid/OpenSSL had logged how they were actually hitting on the
> server.

Agreed.


> Anyway, it's not a big deal now that I know what to do if this kind
> of connection issue comes back up. It could be useful to others
> though if the logging could be a tad more verbose or if Squid could
> retry connections by explictly specifying protocols (and logging
> them).

Agreed in general, but the devil is in the details. Improving this is
difficult, and nobody is working on it at the moment AFAIK.

http://wiki.squid-cache.org/SquidFaq/AboutSquid#How_to_add_a_new_Squid_feature.2C_enhance.2C_of_fix_something.3F


Cheers,

Alex.



From uhlar at fantomas.sk  Thu Jan 26 20:44:50 2017
From: uhlar at fantomas.sk (Matus UHLAR - fantomas)
Date: Thu, 26 Jan 2017 21:44:50 +0100
Subject: [squid-users] Not all html objects are being cached
In-Reply-To: <85b108fb-1f3e-d7d5-48e1-37edbb2ae2d9@gmail.com>
References: <1485273173646-4681293.post@n4.nabble.com>
 <1485375724827-4681326.post@n4.nabble.com>
 <85b108fb-1f3e-d7d5-48e1-37edbb2ae2d9@gmail.com>
Message-ID: <20170126204449.GB25104@fantomas.sk>

>26.01.2017 2:22, boruc ?????:
>> After a little bit of analyzing requests and responses with WireShark I
>> noticed that many sites that weren't cached had different combination of
>> below parameters:
>>
>> Cache-Control: no-cache, no-store, must-revalidate, post-check, pre-check,
>> private, public, max-age, public
>> Pragma: no-cache

On 26.01.17 02:44, Yuri Voinov wrote:
>If the webmaster has done this - he had good reason to. Trying to break
>the RFC in this way, you break the Internet.

Actually, no. If the webmaster has done the above - he has no damn idea what
those mean (private and public?) , and how to provide properly cacheable
content.

Which is very common and also a reason why many proxy admins tend to ignore
those controls...

-- 
Matus UHLAR - fantomas, uhlar at fantomas.sk ; http://www.fantomas.sk/
Warning: I wish NOT to receive e-mail advertising to this address.
Varovanie: na tuto adresu chcem NEDOSTAVAT akukolvek reklamnu postu.
There's a long-standing bug relating to the x86 architecture that
allows you to install Windows.   -- Matthew D. Fuller


From uhlar at fantomas.sk  Thu Jan 26 20:46:32 2017
From: uhlar at fantomas.sk (Matus UHLAR - fantomas)
Date: Thu, 26 Jan 2017 21:46:32 +0100
Subject: [squid-users] Native FTP relay: connection closes (?) after
 'cannot assign requested address' error
In-Reply-To: <1485448881841-4681332.post@n4.nabble.com>
References: <CACMUsGz1rDaY4ffe7vRp-u-cOANuufPfVo+BgvNTmi3t8ywOMA@mail.gmail.com>
 <832f83c5-dc2b-574b-9ab0-80142862bec9@treenet.co.nz>
 <1485092987028-4681242.post@n4.nabble.com>
 <5a1e3e82-4f84-7246-0437-113dd99e957b@treenet.co.nz>
 <1485195091976-4681258.post@n4.nabble.com>
 <8d4fc039-bbd8-3a23-cae1-84ff43ff40e6@measurement-factory.com>
 <CACMUsGw1CxzsPBA6-Tz7zy-RqpnxFn_UqLugz19o9VXR-cLEqg@mail.gmail.com>
 <c5c6ec70-983f-03f5-bb3f-a1db4ffddb17@measurement-factory.com>
 <1485448881841-4681332.post@n4.nabble.com>
Message-ID: <20170126204632.GC25104@fantomas.sk>

On 26.01.17 08:41, Alexander wrote:
>It seems that I have solved the issue by using nf_conntrack_ftp and
>redirecting "NEW,RELATED" traffic to squid:
>
>ftp_port 2121 intercept
>
>modprobe nf_conntrack_ftp ports=2121
>
>iptables -t nat -A PREROUTING -p tcp --dport 21 -j REDIRECT --to-port 2121
>iptables -t nat -A PREROUTING -p tcp -m state --state NEW,RELATED -j
>REDIRECT

just note that connections may be related to different connections than
FTP...

-- 
Matus UHLAR - fantomas, uhlar at fantomas.sk ; http://www.fantomas.sk/
Warning: I wish NOT to receive e-mail advertising to this address.
Varovanie: na tuto adresu chcem NEDOSTAVAT akukolvek reklamnu postu.
BSE = Mad Cow Desease ... BSA = Mad Software Producents Desease


From yvoinov at gmail.com  Thu Jan 26 20:46:21 2017
From: yvoinov at gmail.com (Yuri Voinov)
Date: Fri, 27 Jan 2017 02:46:21 +0600
Subject: [squid-users] Not all html objects are being cached
In-Reply-To: <20170126204449.GB25104@fantomas.sk>
References: <1485273173646-4681293.post@n4.nabble.com>
 <1485375724827-4681326.post@n4.nabble.com>
 <85b108fb-1f3e-d7d5-48e1-37edbb2ae2d9@gmail.com>
 <20170126204449.GB25104@fantomas.sk>
Message-ID: <7fa88d69-1964-0eeb-9e5d-5f539e9a0805@gmail.com>



27.01.2017 2:44, Matus UHLAR - fantomas ?????:
>> 26.01.2017 2:22, boruc ?????:
>>> After a little bit of analyzing requests and responses with WireShark I
>>> noticed that many sites that weren't cached had different
>>> combination of
>>> below parameters:
>>>
>>> Cache-Control: no-cache, no-store, must-revalidate, post-check,
>>> pre-check,
>>> private, public, max-age, public
>>> Pragma: no-cache
>
> On 26.01.17 02:44, Yuri Voinov wrote:
>> If the webmaster has done this - he had good reason to. Trying to break
>> the RFC in this way, you break the Internet.
>
> Actually, no. If the webmaster has done the above - he has no damn
> idea what
> those mean (private and public?) , and how to provide properly cacheable
> content.
It was sarcasm.
>
> Which is very common and also a reason why many proxy admins tend to
> ignore
> those controls...
>

-- 
Bugs to the Future
-------------- next part --------------
A non-text attachment was scrubbed...
Name: 0x613DEC46.asc
Type: application/pgp-keys
Size: 2437 bytes
Desc: not available
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20170127/ff86549a/attachment.key>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: signature.asc
Type: application/pgp-signature
Size: 473 bytes
Desc: OpenPGP digital signature
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20170127/ff86549a/attachment.sig>

From augustus_meyer at gmx.net  Thu Jan 26 22:08:31 2017
From: augustus_meyer at gmx.net (reinerotto)
Date: Thu, 26 Jan 2017 14:08:31 -0800 (PST)
Subject: [squid-users] Not all html objects are being cached
In-Reply-To: <609d443c-daeb-8c0e-b4b8-ab5be13f8d76@treenet.co.nz>
References: <1485273173646-4681293.post@n4.nabble.com>
 <1485375724827-4681326.post@n4.nabble.com>
 <85b108fb-1f3e-d7d5-48e1-37edbb2ae2d9@gmail.com>
 <609d443c-daeb-8c0e-b4b8-ab5be13f8d76@treenet.co.nz>
Message-ID: <1485468511911-4681339.post@n4.nabble.com>

>reply_header_access Cache-Control deny all<
Will this only affect downstream caches, or will this squid itself also
ignore any Cache-Control header info
received from upstream ? 




--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/Not-all-html-objects-are-being-cached-tp4681293p4681339.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From mark_squid at finito.me.uk  Thu Jan 26 22:38:39 2017
From: mark_squid at finito.me.uk (Mark Hoare)
Date: Thu, 26 Jan 2017 22:38:39 +0000
Subject: [squid-users] ssl_bump - peek & splice logging IP rather than
	server name
In-Reply-To: <cd02ce1f-736c-3d5d-ef77-1febddfd7b16@measurement-factory.com>
References: <9A9925E4-014F-4C64-B830-1644C809084F@finito.me.uk>
 <002c01d26612$77f3b330$67db1990$@ngtech.co.il>
 <cd02ce1f-736c-3d5d-ef77-1febddfd7b16@measurement-factory.com>
Message-ID: <CA531091-9440-4E43-AD2E-DED0C78C5109@finito.me.uk>

Alex/Eliezer,

Thanks for you earlier comments and apologies for not responding (and saying thank you previously, squid got back-burnered unfortunately)

Getting logging working with transparent proxying was my initial step prior to looking at restricting specific sites via either ACLs or a URL rewriter (ufdbGuard, SquidGuard etc - although I don?t think SquidGuard works with SNI) 

To reiterate, my desire is to have Squid running and capable of blocking access to http and https sites primarily based on the server name requested by the client (so no need to go beyond a peek)
For HTTP requests this is obviously out of the box stuff but for HTTPS it seems more complicated.

From everything I?ve read, it looks like the following ssl_bump lines should provide access to the SNI server name requested by the client. 
	ssl_bump peek all
	ssl_bump splice all

I can?t help thinking that I must have something wrong with my config:
- Log output correctly shows 
	- SNI server name via ssl::>sni 
	- Bump mode via ssl::bump_mode 
- Implies my ssl_bump config is working
- Restricting access via a squid ACL doesn?t use the SNI server name for an HTTPS request 
- Works fine for HTTP

Example ACL:
    acl blocked_sites ssl::server_name .apple.com
    http_access deny blocked_sites

Example access log output:
%ts.%03tu 	  %6tr  %>a        %Ss/%03>Hs 	   %<st  %rm      %ru                         %ssl::>sni        %ssl::bump_mode %[un  %Sh/%<a                     %mt
1485468402.401  575   10.1.0.1  TCP_TUNNEL/200 592  CONNECT  23.63.86.92:443          store.apple.com  peek           -    ORIGINAL_DST/23.63.86.92  -
1485469054.633  51    10.1.0.1  TCP_DENIED/403 3962 GET      http://store.apple.com/  -                -              -    HIER_NONE/-               text/html

Example cache log output:
2017/01/26 21:54:21.745 kid1| 28,5| Acl.cc(138) matches: checking blocked_sites
2017/01/26 21:54:21.745 kid1| 28,3| ServerName.cc(42) match: checking '23.63.86.92'
2017/01/26 21:54:21.745 kid1| 28,7| ServerName.cc(32) aclHostDomainCompare: Match:23.63.86.92 <>  .apple.com
2017/01/26 21:54:21.745 kid1| 28,3| ServerName.cc(47) match: '23.63.86.92' NOT found
2017/01/26 21:54:21.745 kid1| 28,3| ServerName.cc(42) match: checking 'none'
2017/01/26 21:54:21.745 kid1| 28,7| ServerName.cc(32) aclHostDomainCompare: Match:none <>  .apple.com
2017/01/26 21:54:21.745 kid1| 28,3| ServerName.cc(47) match: 'none' NOT found
2017/01/26 21:54:21.745 kid1| 28,3| Acl.cc(158) matches: checked: blocked_sites = 0
2017/01/26 21:54:21.745 kid1| 28,3| Acl.cc(158) matches: checked: http_access#5 = 0
2017/01/26 21:54:21.745 kid1| 28,5| Checklist.cc(400) bannedAction: Action 'ALLOWED/0is not banned

squid -v output:
Squid Cache: Version 3.5.20
Service Name: squid
configure options:  '--build=x86_64-redhat-linux-gnu' '--host=x86_64-redhat-linux-gnu' '--program-prefix=' '--prefix=/usr' '--exec-prefix=/usr' '--bindir=/usr/bin' '--sbindir=/usr/sbin' '--sysconfdir=/etc' '--datadir=/usr/share' '--includedir=/usr/include' '--libdir=/usr/lib64' '--libexecdir=/usr/libexec' '--sharedstatedir=/var/lib' '--mandir=/usr/share/man' '--infodir=/usr/share/info' '--disable-strict-error-checking' '--exec_prefix=/usr' '--libexecdir=/usr/lib64/squid' '--localstatedir=/var' '--datadir=/usr/share/squid' '--sysconfdir=/etc/squid' '--with-logdir=$(localstatedir)/log/squid' '--with-pidfile=$(localstatedir)/run/squid.pid' '--disable-dependency-tracking' '--enable-eui' '--enable-follow-x-forwarded-for' '--enable-auth' '--enable-auth-basic=DB,LDAP,MSNT-multi-domain,NCSA,NIS,PAM,POP3,RADIUS,SASL,SMB,SMB_LM,getpwnam' '--enable-auth-ntlm=smb_lm,fake' '--enable-auth-digest=file,LDAP,eDirectory' '--enable-auth-negotiate=kerberos' '--enable-external-acl-helpers=file_userip,LDAP_group,time_quota,session,unix_group,wbinfo_group' '--enable-cache-digests' '--enable-cachemgr-hostname=localhost' '--enable-delay-pools' '--enable-epoll' '--enable-ident-lookups' '--enable-linux-netfilter' '--enable-removal-policies=heap,lru' '--enable-snmp' '--enable-ssl-crtd' '--enable-storeio=aufs,diskd,ufs' '--enable-wccpv2' '--enable-esi' '--enable-ecap' '--with-aio' '--with-default-user=squid' '--with-dl' '--with-openssl' '--with-pthreads' '--disable-arch-native' '--disable-icap-client' 'build_alias=x86_64-redhat-linux-gnu' 'host_alias=x86_64-redhat-linux-gnu' 'CFLAGS=-O2 -g -pipe -Wall -Wp,-D_FORTIFY_SOURCE=2 -fexceptions -fstack-protector-strong --param=ssp-buffer-size=4 -grecord-gcc-switches   -m64 -mtune=generic -fpie' 'LDFLAGS=-Wl,-z,relro  -pie -Wl,-z,relro -Wl,-z,now' 'CXXFLAGS=-O2 -g -pipe -Wall -Wp,-D_FORTIFY_SOURCE=2 -fexceptions -fstack-protector-strong --param=ssp-buffer-size=4 -grecord-gcc-switches   -m64 -mtune=generic -fpie' 'PKG_CONFIG_PATH=:/usr/lib64/pkgconfig:/usr/share/pkgconfig'

Is there anything obvious that I am missing as I?m a bit stumped now.

Thanks again

Mark

> On 3 Jan 2017, at 23:35, Alex Rousskov <rousskov at measurement-factory.com> wrote:
> 
> On 01/03/2017 04:11 PM, Mark Hoare wrote:
> 

>> I think these are hangovers from earlier syntax (ssl_bump
>> server-first all) which shouldn't be required under 3.5.
> 

> Please note that the depricated server-first is a "bumping" (not
> splicing!) action, and you may see a lot more information in the
> bumping-Squid logs, naturally.
> 
> Alex.
> 

> On 3 Jan 2017, at 23:10, Alex Rousskov <rousskov at measurement-factory.com> wrote:
> 
> On 01/03/2017 03:41 PM, Eliezer  Croitoru wrote:
> 
>> Squid in intercept or tproxy mode will know one thing about the tunnel\connection: IP+port.
> 
> ... and SSL handshake information when peeking or staring at client/server.
> 
> 
>> Since you are using:
>>> ssl_bump peek all
>>> ssl_bump splice all
> 
>> The connections will always be spliced and you will never see any
>> url.(are you expecting only the SNI or also the url?)
> 
> AFAICT, Mark is expecting Squid to log one of the server names, not the
> HTTP request URL.
> 
> 
>> I do not know but there might be a code that can report the SNI if exists in the logs.
> 
> According to squid.conf.documented, the following logformat %code is
> supported in modern Squids:
> 
>> ssl::>sni       SSL client SNI sent to Squid. Available only
>>                after the peek, stare, or splice SSL bumping
>>                actions.
> 
> This %code is not in the default access.log line format, naturally.
> 
> Squid can also analyze CN and other server certificate fields, but there
> is no code to log them IIRC.
> 
> 
> Please note that the intercepted server IP address, the client-supplied
> SNI name, the server-supplied common name (CN), the server-supplied
> alternative names, and the host info in the encrypted client HTTP
> request, may all be different.
> 
> Given the variety of information sources that might supply different
> information, it is not clear to me whether %ru should be based on SNI
> information when both TCP-level and SNI information is available. Or
> should it be based on CN? Or perhaps on CN _unless_ SNI matches one of
> the alternative names?? This is a complicated issue; even the smart
> server_name ACL needs parameters to clarify what "server name(s)" the
> admin really wants to use/trust...
> 
> According to Mark's email, %ru uses TCP-level info. We could either
> change %ru to use the "latest" info (like the server_name ACL does) or
> add a new logformat code that does that while leaving the old %ru and
> friends alone. Given the complexity of the issue, the latter may be a
> better approach.
> 
> 
> HTH,
> 
> Alex.
> 
>> -----Original Message-----
>> From: squid-users [mailto:squid-users-bounces at lists.squid-cache.org] On Behalf Of Mark Hoare
>> Sent: Saturday, December 31, 2016 4:38 PM
>> To: squid-users at lists.squid-cache.org
>> Subject: [squid-users] ssl_bump - peek & splice logging IP rather than server name
> 
>> Extract from access log:
>>> 1483193882.790    870 <local ip removed> TCP_TUNNEL/200 5620 CONNECT 64.41.200.100:443 - ORIGINAL_DST/64.41.200.100 -
> 
>> From the output above I would have expected some of the server name info to get into the access log.
> 
>>> http_port 3128
>>> 
>>> https_port 3130 intercept ssl-bump cert=/etc/squid/ssl_cert/squidCA.pem generate-host-certificates=on dynamic_cert_mem_cache_size=4MB
>>> 
>>> http_port 3131 intercept ssl-bump cert=/etc/squid/ssl_cert/squidCA.pem generate-host-certificates=on dynamic_cert_mem_cache_size=4MB
> 
>>> ssl_bump peek all
>>> ssl_bump splice all
> 

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20170126/08be0150/attachment.htm>

From rousskov at measurement-factory.com  Thu Jan 26 23:57:19 2017
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Thu, 26 Jan 2017 16:57:19 -0700
Subject: [squid-users] ssl_bump - peek & splice logging IP rather than
 server name
In-Reply-To: <CA531091-9440-4E43-AD2E-DED0C78C5109@finito.me.uk>
References: <9A9925E4-014F-4C64-B830-1644C809084F@finito.me.uk>
 <002c01d26612$77f3b330$67db1990$@ngtech.co.il>
 <cd02ce1f-736c-3d5d-ef77-1febddfd7b16@measurement-factory.com>
 <CA531091-9440-4E43-AD2E-DED0C78C5109@finito.me.uk>
Message-ID: <3b94995a-c7df-f36a-65ab-c24e7fb775d9@measurement-factory.com>

On 01/26/2017 03:38 PM, Mark Hoare wrote:

> To reiterate, my desire is to have Squid running and capable of blocking
> access to http and https sites primarily based on the server name
> requested by the client (so no need to go beyond a peek)

... or even beyond a peek at the client.


> From everything I?ve read, it looks like the following ssl_bump lines
> should provide access to the SNI server name requested by the client. 
> ssl_bump peek all
> ssl_bump splice all

Yes, but you are also telling Squid to peek at the server certificate.
If you want to avoid doing that, then replace "peek all" with "peek
step1" while providing the right step1 ACL. The SNI-based denial you
want should happen earlier anyway, but if you do peek at the server
certificate, then you may also deny later, based on server-supplied
information (e.g., when SNI was missing or was not matching). Whether
more denials based on server info is a good thing is your decision.


> I can?t help thinking that I must have something wrong with my config:
> - Log output correctly shows 
> - SNI server name via ssl::>sni 
> - Bump mode via ssl::bump_mode 
> - Implies my ssl_bump config is working
> - Works fine for HTTP

Great!


> - Restricting access via a squid ACL doesn?t use the SNI server name for
> an HTTPS request 

You may be right, but not for the reasons you think. The output you have
shown does not necessarily confirm any problems.


> Example ACL:
>     acl blocked_sites ssl::server_name .apple.com
>     http_access deny blocked_sites

Please note that your Squid version is missing a critical
ssl::server_name fix detailed below.


> Example access log output:
> %ts.%03tu   %6tr  %>a        %Ss/%03>Hs    %<st  %rm      %ru
>     %ssl::>sni       %ssl::bump_mode %[un  %Sh/%<a %mt

> 1485468402.401  575   10.1.0.1  TCP_TUNNEL/200 592  CONNECT 23.63.86.92:443
>     store.apple.com  peek -   ORIGINAL_DST/23.63.86.92  -

> 1485469054.633  51    10.1.0.1  TCP_DENIED/403 3962 GET http://store.apple.com/
>     -                -    -   HIER_NONE/-               text/html

The above shows that Squid peeked and denied access. To serve the error
page to the client, Squid bumped the client connection first and then
denied the first encrypted HTTP request. This is normal/expected.


> Example cache log output:
> 2017/01/26 21:54:21.745 kid1| 28,5| Acl.cc(138) matches: checking blocked_sites
> 2017/01/26 21:54:21.745 kid1| 28,3| ServerName.cc(42) match: checking '23.63.86.92'
> 2017/01/26 21:54:21.745 kid1| 28,7| ServerName.cc(32) aclHostDomainCompare: Match:23.63.86.92 <> .apple.com
> 2017/01/26 21:54:21.745 kid1| 28,3| ServerName.cc(47) match: '23.63.86.92' NOT found
> 2017/01/26 21:54:21.745 kid1| 28,3| ServerName.cc(42) match: checking 'none'
> 2017/01/26 21:54:21.745 kid1| 28,7| ServerName.cc(32) aclHostDomainCompare: Match:none <> .apple.com
> 2017/01/26 21:54:21.745 kid1| 28,3| ServerName.cc(47) match: 'none' NOT found
> 2017/01/26 21:54:21.745 kid1| 28,5| Checklist.cc(400) bannedAction: Action 'ALLOWED/0is not banned

If the above was for step1 checks, then it makes sense: The access was
not banned based on TCP level information. Proceed to step2 (extract SNI
and test again). There should be more checks like the above, and then
Squid decided to deny access. However, the timing of that decision and
the sources of information used for that decision may change after the
ssl::server_name fix mentioned below.


> Squid Cache: Version 3.5.20

You are missing the following server_name fix (among other things):

> revno: 14110
> branch nick: 3.5
> timestamp: Mon 2016-11-14 23:51:24 +1300
> message:
>   Fix ssl::server_name ACL badly broken since inception.
>   
>   The original server_name code mishandled all SNI checks and some rare
>   host checks:
>   
>   * The SNI-derived value was pointing to an already freed memory storage.
>   * Missing host-derived values were not detected (host() is never nil).
>   * Mismatches were re-checked with an undocumented "none" value
>     instead of being treated as mismatches.
>   
>   Same for ssl::server_name_regex.
>   
>   Also set SNI for more server-first and client-first transactions.
>   
>   This is a Measurement Factory project.

The first rule of SslBumping: Use the latest code.


HTH,

Alex.
P.S. Please avoid HTMLifying your emails, especially when quoting logs.



>> On 3 Jan 2017, at 23:35, Alex Rousskov wrote:
>>
>> On 01/03/2017 04:11 PM, Mark Hoare wrote:
>>
>>> I think these are hangovers from earlier syntax (ssl_bump
>>> server-first all) which shouldn't be required under 3.5.
>>
>> Please note that the depricated server-first is a "bumping" (not
>> splicing!) action, and you may see a lot more information in the
>> bumping-Squid logs, naturally.
>>
>> Alex.
>>
> 
>> On 3 Jan 2017, at 23:10, Alex Rousskov
>> <rousskov at measurement-factory.com
>> <mailto:rousskov at measurement-factory.com>> wrote:
>>
>> On 01/03/2017 03:41 PM, Eliezer  Croitoru wrote:
>>
>>> Squid in intercept or tproxy mode will know one thing about the
>>> tunnel\connection: IP+port.
>>
>> ... and SSL handshake information when peeking or staring at
>> client/server.
>>
>>
>>> Since you are using:
>>>> ssl_bump peek all
>>>> ssl_bump splice all
>>
>>> The connections will always be spliced and you will never see any
>>> url.(are you expecting only the SNI or also the url?)
>>
>> AFAICT, Mark is expecting Squid to log one of the server names, not the
>> HTTP request URL.
>>
>>
>>> I do not know but there might be a code that can report the SNI if
>>> exists in the logs.
>>
>> According to squid.conf.documented, the following logformat %code is
>> supported in modern Squids:
>>
>>> ssl::>sni       SSL client SNI sent to Squid. Available only
>>>                after the peek, stare, or splice SSL bumping
>>>                actions.
>>
>> This %code is not in the default access.log line format, naturally.
>>
>> Squid can also analyze CN and other server certificate fields, but there
>> is no code to log them IIRC.
>>
>>
>> Please note that the intercepted server IP address, the client-supplied
>> SNI name, the server-supplied common name (CN), the server-supplied
>> alternative names, and the host info in the encrypted client HTTP
>> request, may all be different.
>>
>> Given the variety of information sources that might supply different
>> information, it is not clear to me whether %ru should be based on SNI
>> information when both TCP-level and SNI information is available. Or
>> should it be based on CN? Or perhaps on CN _unless_ SNI matches one of
>> the alternative names?? This is a complicated issue; even the smart
>> server_name ACL needs parameters to clarify what "server name(s)" the
>> admin really wants to use/trust...
>>
>> According to Mark's email, %ru uses TCP-level info. We could either
>> change %ru to use the "latest" info (like the server_name ACL does) or
>> add a new logformat code that does that while leaving the old %ru and
>> friends alone. Given the complexity of the issue, the latter may be a
>> better approach.
>>
>>
>> HTH,
>>
>> Alex.
>>
>>> -----Original Message-----
>>> From: squid-users [mailto:squid-users-bounces at lists.squid-cache.org]
>>> On Behalf Of Mark Hoare
>>> Sent: Saturday, December 31, 2016 4:38 PM
>>> To: squid-users at lists.squid-cache.org
>>> <mailto:squid-users at lists.squid-cache.org>
>>> Subject: [squid-users] ssl_bump - peek & splice logging IP rather
>>> than server name
>>
>>> Extract from access log:
>>>> 1483193882.790    870 <local ip removed> TCP_TUNNEL/200 5620 CONNECT
>>>> 64.41.200.100:443 - ORIGINAL_DST/64.41.200.100 -
>>
>>> From the output above I would have expected some of the server name
>>> info to get into the access log.
>>
>>>> http_port 3128
>>>>
>>>> https_port 3130 intercept ssl-bump
>>>> cert=/etc/squid/ssl_cert/squidCA.pem generate-host-certificates=on
>>>> dynamic_cert_mem_cache_size=4MB
>>>>
>>>> http_port 3131 intercept ssl-bump
>>>> cert=/etc/squid/ssl_cert/squidCA.pem generate-host-certificates=on
>>>> dynamic_cert_mem_cache_size=4MB
>>
>>>> ssl_bump peek all
>>>> ssl_bump splice all
>>
> 



From squid3 at treenet.co.nz  Fri Jan 27 02:15:29 2017
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Fri, 27 Jan 2017 15:15:29 +1300
Subject: [squid-users] Not all html objects are being cached
In-Reply-To: <1485468511911-4681339.post@n4.nabble.com>
References: <1485273173646-4681293.post@n4.nabble.com>
 <1485375724827-4681326.post@n4.nabble.com>
 <85b108fb-1f3e-d7d5-48e1-37edbb2ae2d9@gmail.com>
 <609d443c-daeb-8c0e-b4b8-ab5be13f8d76@treenet.co.nz>
 <1485468511911-4681339.post@n4.nabble.com>
Message-ID: <569ed228-e343-357c-cece-4dcfa79e6c46@treenet.co.nz>

On 27/01/2017 11:08 a.m., reinerotto wrote:
>> reply_header_access Cache-Control deny all<
> Will this only affect downstream caches, or will this squid itself also
> ignore any Cache-Control header info
> received from upstream ? 
> 

It will only affect the clients caches. eg. their browser cache.

Amos



From eliezer at ngtech.co.il  Fri Jan 27 02:22:02 2017
From: eliezer at ngtech.co.il (Eliezer  Croitoru)
Date: Fri, 27 Jan 2017 04:22:02 +0200
Subject: [squid-users] ssl_bump - peek & splice logging IP rather than
	server name
In-Reply-To: <3b94995a-c7df-f36a-65ab-c24e7fb775d9@measurement-factory.com>
References: <9A9925E4-014F-4C64-B830-1644C809084F@finito.me.uk>
 <002c01d26612$77f3b330$67db1990$@ngtech.co.il>
 <cd02ce1f-736c-3d5d-ef77-1febddfd7b16@measurement-factory.com>
 <CA531091-9440-4E43-AD2E-DED0C78C5109@finito.me.uk>
 <3b94995a-c7df-f36a-65ab-c24e7fb775d9@measurement-factory.com>
Message-ID: <017d01d27844$252a15b0$6f7e4110$@ngtech.co.il>

Is it stock REDHAT or CentOS or Other?
You can use the RPM's of squid I am building which are quite generic and works very well.
I have just released 3.5.23 and 4.0.17.
I have built it both for RH and CentOS:
http://wiki.squid-cache.org/KnowledgeBase/CentOS#Squid-3.5
And you can take a peek and browse at:
http://www1.ngtech.co.il/repo/
If what you are running is not there let me know and I will try to build a binary for it.

Eliezer

----
Eliezer Croitoru
Linux System Administrator
Mobile: +972-5-28704261
Email: eliezer at ngtech.co.il


-----Original Message-----
From: squid-users [mailto:squid-users-bounces at lists.squid-cache.org] On Behalf Of Alex Rousskov
Sent: Friday, January 27, 2017 1:57 AM
To: squid-users at lists.squid-cache.org
Subject: Re: [squid-users] ssl_bump - peek & splice logging IP rather than server name

On 01/26/2017 03:38 PM, Mark Hoare wrote:

> To reiterate, my desire is to have Squid running and capable of 
> blocking access to http and https sites primarily based on the server 
> name requested by the client (so no need to go beyond a peek)

... or even beyond a peek at the client.


> From everything I?ve read, it looks like the following ssl_bump lines 
> should provide access to the SNI server name requested by the client.
> ssl_bump peek all
> ssl_bump splice all

Yes, but you are also telling Squid to peek at the server certificate.
If you want to avoid doing that, then replace "peek all" with "peek step1" while providing the right step1 ACL. The SNI-based denial you want should happen earlier anyway, but if you do peek at the server certificate, then you may also deny later, based on server-supplied information (e.g., when SNI was missing or was not matching). Whether more denials based on server info is a good thing is your decision.


> I can?t help thinking that I must have something wrong with my config:
> - Log output correctly shows
> - SNI server name via ssl::>sni
> - Bump mode via ssl::bump_mode
> - Implies my ssl_bump config is working
> - Works fine for HTTP

Great!


> - Restricting access via a squid ACL doesn?t use the SNI server name 
> for an HTTPS request

You may be right, but not for the reasons you think. The output you have shown does not necessarily confirm any problems.


> Example ACL:
>     acl blocked_sites ssl::server_name .apple.com
>     http_access deny blocked_sites

Please note that your Squid version is missing a critical ssl::server_name fix detailed below.


> Example access log output:
> %ts.%03tu   %6tr  %>a        %Ss/%03>Hs    %<st  %rm      %ru
>     %ssl::>sni       %ssl::bump_mode %[un  %Sh/%<a %mt

> 1485468402.401  575   10.1.0.1  TCP_TUNNEL/200 592  CONNECT 23.63.86.92:443
>     store.apple.com  peek -   ORIGINAL_DST/23.63.86.92  -

> 1485469054.633  51    10.1.0.1  TCP_DENIED/403 3962 GET http://store.apple.com/
>     -                -    -   HIER_NONE/-               text/html

The above shows that Squid peeked and denied access. To serve the error page to the client, Squid bumped the client connection first and then denied the first encrypted HTTP request. This is normal/expected.


> Example cache log output:
> 2017/01/26 21:54:21.745 kid1| 28,5| Acl.cc(138) matches: checking 
> blocked_sites
> 2017/01/26 21:54:21.745 kid1| 28,3| ServerName.cc(42) match: checking '23.63.86.92'
> 2017/01/26 21:54:21.745 kid1| 28,7| ServerName.cc(32) 
> aclHostDomainCompare: Match:23.63.86.92 <> .apple.com
> 2017/01/26 21:54:21.745 kid1| 28,3| ServerName.cc(47) match: 
> '23.63.86.92' NOT found
> 2017/01/26 21:54:21.745 kid1| 28,3| ServerName.cc(42) match: checking 'none'
> 2017/01/26 21:54:21.745 kid1| 28,7| ServerName.cc(32) 
> aclHostDomainCompare: Match:none <> .apple.com
> 2017/01/26 21:54:21.745 kid1| 28,3| ServerName.cc(47) match: 'none' 
> NOT found
> 2017/01/26 21:54:21.745 kid1| 28,5| Checklist.cc(400) bannedAction: 
> Action 'ALLOWED/0is not banned

If the above was for step1 checks, then it makes sense: The access was not banned based on TCP level information. Proceed to step2 (extract SNI and test again). There should be more checks like the above, and then Squid decided to deny access. However, the timing of that decision and the sources of information used for that decision may change after the ssl::server_name fix mentioned below.


> Squid Cache: Version 3.5.20

You are missing the following server_name fix (among other things):

> revno: 14110
> branch nick: 3.5
> timestamp: Mon 2016-11-14 23:51:24 +1300
> message:
>   Fix ssl::server_name ACL badly broken since inception.
>   
>   The original server_name code mishandled all SNI checks and some rare
>   host checks:
>   
>   * The SNI-derived value was pointing to an already freed memory storage.
>   * Missing host-derived values were not detected (host() is never nil).
>   * Mismatches were re-checked with an undocumented "none" value
>     instead of being treated as mismatches.
>   
>   Same for ssl::server_name_regex.
>   
>   Also set SNI for more server-first and client-first transactions.
>   
>   This is a Measurement Factory project.

The first rule of SslBumping: Use the latest code.


HTH,

Alex.
P.S. Please avoid HTMLifying your emails, especially when quoting logs.



>> On 3 Jan 2017, at 23:35, Alex Rousskov wrote:
>>
>> On 01/03/2017 04:11 PM, Mark Hoare wrote:
>>
>>> I think these are hangovers from earlier syntax (ssl_bump 
>>> server-first all) which shouldn't be required under 3.5.
>>
>> Please note that the depricated server-first is a "bumping" (not
>> splicing!) action, and you may see a lot more information in the 
>> bumping-Squid logs, naturally.
>>
>> Alex.
>>
> 
>> On 3 Jan 2017, at 23:10, Alex Rousskov 
>> <rousskov at measurement-factory.com 
>> <mailto:rousskov at measurement-factory.com>> wrote:
>>
>> On 01/03/2017 03:41 PM, Eliezer  Croitoru wrote:
>>
>>> Squid in intercept or tproxy mode will know one thing about the
>>> tunnel\connection: IP+port.
>>
>> ... and SSL handshake information when peeking or staring at 
>> client/server.
>>
>>
>>> Since you are using:
>>>> ssl_bump peek all
>>>> ssl_bump splice all
>>
>>> The connections will always be spliced and you will never see any 
>>> url.(are you expecting only the SNI or also the url?)
>>
>> AFAICT, Mark is expecting Squid to log one of the server names, not 
>> the HTTP request URL.
>>
>>
>>> I do not know but there might be a code that can report the SNI if 
>>> exists in the logs.
>>
>> According to squid.conf.documented, the following logformat %code is 
>> supported in modern Squids:
>>
>>> ssl::>sni       SSL client SNI sent to Squid. Available only
>>>                after the peek, stare, or splice SSL bumping
>>>                actions.
>>
>> This %code is not in the default access.log line format, naturally.
>>
>> Squid can also analyze CN and other server certificate fields, but 
>> there is no code to log them IIRC.
>>
>>
>> Please note that the intercepted server IP address, the 
>> client-supplied SNI name, the server-supplied common name (CN), the 
>> server-supplied alternative names, and the host info in the encrypted 
>> client HTTP request, may all be different.
>>
>> Given the variety of information sources that might supply different 
>> information, it is not clear to me whether %ru should be based on SNI 
>> information when both TCP-level and SNI information is available. Or 
>> should it be based on CN? Or perhaps on CN _unless_ SNI matches one 
>> of the alternative names?? This is a complicated issue; even the 
>> smart server_name ACL needs parameters to clarify what "server 
>> name(s)" the admin really wants to use/trust...
>>
>> According to Mark's email, %ru uses TCP-level info. We could either 
>> change %ru to use the "latest" info (like the server_name ACL does) 
>> or add a new logformat code that does that while leaving the old %ru 
>> and friends alone. Given the complexity of the issue, the latter may 
>> be a better approach.
>>
>>
>> HTH,
>>
>> Alex.
>>
>>> -----Original Message-----
>>> From: squid-users [mailto:squid-users-bounces at lists.squid-cache.org]
>>> On Behalf Of Mark Hoare
>>> Sent: Saturday, December 31, 2016 4:38 PM
>>> To: squid-users at lists.squid-cache.org 
>>> <mailto:squid-users at lists.squid-cache.org>
>>> Subject: [squid-users] ssl_bump - peek & splice logging IP rather 
>>> than server name
>>
>>> Extract from access log:
>>>> 1483193882.790    870 <local ip removed> TCP_TUNNEL/200 5620 CONNECT
>>>> 64.41.200.100:443 - ORIGINAL_DST/64.41.200.100 -
>>
>>> From the output above I would have expected some of the server name 
>>> info to get into the access log.
>>
>>>> http_port 3128
>>>>
>>>> https_port 3130 intercept ssl-bump
>>>> cert=/etc/squid/ssl_cert/squidCA.pem generate-host-certificates=on 
>>>> dynamic_cert_mem_cache_size=4MB
>>>>
>>>> http_port 3131 intercept ssl-bump
>>>> cert=/etc/squid/ssl_cert/squidCA.pem generate-host-certificates=on 
>>>> dynamic_cert_mem_cache_size=4MB
>>
>>>> ssl_bump peek all
>>>> ssl_bump splice all
>>
> 

_______________________________________________
squid-users mailing list
squid-users at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-users



From squid3 at treenet.co.nz  Fri Jan 27 03:08:01 2017
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Fri, 27 Jan 2017 16:08:01 +1300
Subject: [squid-users] Not all html objects are being cached
In-Reply-To: <20170126204449.GB25104@fantomas.sk>
References: <1485273173646-4681293.post@n4.nabble.com>
 <1485375724827-4681326.post@n4.nabble.com>
 <85b108fb-1f3e-d7d5-48e1-37edbb2ae2d9@gmail.com>
 <20170126204449.GB25104@fantomas.sk>
Message-ID: <fa9f1060-f591-ae15-1e45-0305824e97e2@treenet.co.nz>

On 27/01/2017 9:44 a.m., Matus UHLAR - fantomas wrote:
>> 26.01.2017 2:22, boruc ?????:
>>> After a little bit of analyzing requests and responses with WireShark I
>>> noticed that many sites that weren't cached had different combination of
>>> below parameters:
>>>
>>> Cache-Control: no-cache, no-store, must-revalidate, post-check,
>>> pre-check,
>>> private, public, max-age, public
>>> Pragma: no-cache
> 
> On 26.01.17 02:44, Yuri Voinov wrote:
>> If the webmaster has done this - he had good reason to. Trying to break
>> the RFC in this way, you break the Internet.
> 
> Actually, no. If the webmaster has done the above - he has no damn idea
> what
> those mean (private and public?) , and how to provide properly cacheable
> content.
> 


I think boruc has just listed all the cache controls he has noticed in
one line. Not actually what is being seen ...


> Which is very common and also a reason why many proxy admins tend to ignore
> those controls...
> 

... the URLs used for expanded details show the usual combos webmasters
use to 'fix' broken behaviour of such proxies. For example adding
"no-cache, private, max-age=0" to get around proxies ignoring various of
the controls.

Amos


From squid3 at treenet.co.nz  Fri Jan 27 03:10:59 2017
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Fri, 27 Jan 2017 16:10:59 +1300
Subject: [squid-users] Not all html objects are being cached
In-Reply-To: <7fa88d69-1964-0eeb-9e5d-5f539e9a0805@gmail.com>
References: <1485273173646-4681293.post@n4.nabble.com>
 <1485375724827-4681326.post@n4.nabble.com>
 <85b108fb-1f3e-d7d5-48e1-37edbb2ae2d9@gmail.com>
 <20170126204449.GB25104@fantomas.sk>
 <7fa88d69-1964-0eeb-9e5d-5f539e9a0805@gmail.com>
Message-ID: <bc726969-ca5e-912b-7600-b4d4408c0c02@treenet.co.nz>

On 27/01/2017 9:46 a.m., Yuri Voinov wrote:
> 
> 
> 27.01.2017 2:44, Matus UHLAR - fantomas ?????:
>>> 26.01.2017 2:22, boruc ?????:
>>>> After a little bit of analyzing requests and responses with WireShark I
>>>> noticed that many sites that weren't cached had different
>>>> combination of
>>>> below parameters:
>>>>
>>>> Cache-Control: no-cache, no-store, must-revalidate, post-check,
>>>> pre-check,
>>>> private, public, max-age, public
>>>> Pragma: no-cache
>>
>> On 26.01.17 02:44, Yuri Voinov wrote:
>>> If the webmaster has done this - he had good reason to. Trying to break
>>> the RFC in this way, you break the Internet.
>>
>> Actually, no. If the webmaster has done the above - he has no damn
>> idea what
>> those mean (private and public?) , and how to provide properly cacheable
>> content.
> It was sarcasm.


You may have intended it to be. But you spoke the simple truth.

Other than 'public' there really are situations which have "good reason"
to send that set of controls all at once.

For example; any admin who wants a RESTful or SaaS application to
actually work for all their potential customers.


I have been watching the below cycle take place for the past 20 years in
HTTP:

Webmaster: dont cache this please.

  "Cache-Control: no-store"

Proxy Admin: ignore-no-store


Webmaster: I meant it. Dont deliver anything you cached without fetching
a updated version.

  ... "no-store, no-cache"

Proxy Admin: ignore-no-cache


Webmaster: really you MUST revalidate before using ths data.

 ... "no-store, no-cache, must-revalidate"

Proxy Admin: ignore-must-revalidate


Webmaster: Really I meant it. This is non-storable PRIVATE DATA!

... "no-store, no-cache, must-revalidate, private"

Proxy Admin: ignore-private


Webmaster: Seriously. I'm changing it on EVERY request! dont store it.

... "no-store, no-cache, must-revalidate, private, max-age=0"
"Expires: -1"

Proxy Admin: ignore-expires


Webmaster: are you one of those dumb HTTP/1.0 proxies who dont
understand Cache-Control?

"Pragma: no-cache"
"Expires: 1 Jan 1970"

Proxy Admin: hehe! I already ignore-no-cache ignore-expires


Webmaster: F*U!  May your clients batch up their traffic to slam you
with it all at once!

... "no-store, no-cache, must-revalidate, private, max-age=0,
pre-check=1, post-check=1"


Proxy Admin: My bandwidth! I need to cache more!

Webmaster: Doh! Oh well, so I have to write my application to force new
content then.

Proxy Admin: ignore-reload


Webmaster: Now What? Oh HTTPS wont have any damn proxies in the way....

... the cycle repeats again within HTTPS. Took all of 5 years this time.

... the cycle repeats again within SPDY. That took only ~1 year.

... the cycle repeats again within CoAP. The standards are not even
finished yet and its underway.


Stop this cycle of stupidity. It really HAS "broken the Internet".


HTH
Amos


From squid3 at treenet.co.nz  Fri Jan 27 03:39:45 2017
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Fri, 27 Jan 2017 16:39:45 +1300
Subject: [squid-users] squid3 : Really need to use external (slow) acl
 with peer_cache_access
In-Reply-To: <351688258.99968148.1485336582829.JavaMail.root@zimbra84-e15.priv.proxad.net>
References: <351688258.99968148.1485336582829.JavaMail.root@zimbra84-e15.priv.proxad.net>
Message-ID: <d038527c-fe3a-26d8-8b1a-8ea93a03bea9@treenet.co.nz>

On 25/01/2017 10:29 p.m., hoper at free.fr wrote:
> 
> Hi everybody,
> 
> I really try to find a answer with google, and within
> the archives of this mailing list but couldn't find anything
> so... here I am...
> 
> I need to select a squid parent based on the login of the
> user (and others things). With squid 2.7, I had a configuration
> like this one :
> 
> -------------------------------------------------------------
> cache_peer 169.254.1.1 parent 3128 0 default name=parent1
> cache_peer 169.254.1.2 parent 3128 0 default name=parent2
> [...] (many parents)
> 
> external_acl_type choose_parent ttl=60,children-max=1 %EXT_USER %SRC %LOGIN %ACL /home/user/myhelper.sh
> acl p0 external choose_parent
> 
> external_acl_type myparent1 ttl=60,children-max=1 %ACL %EXT_USER  /home/user/another_helper
> acl p1 external myparent1
> external_acl_type myparent2 ttl=60,children-max=1 %ACL %EXT_USER  /home/user/another_helper
> acl p2 external myparent2
> [...]
> 
> cache_peer_access parent1 allow p1
> cache_peer_access parent2 allow p2
> [...]
> 
> cache_peer_access path1 deny all
> cache_peer_access path2 deny all
> [...]
> 
> ---------------------------------------------------------------
> 
> The idea is to deny all squid parents except the one I want this user
> (with this specific IP and so on) to use.
> 
> But with squid3, I just have lot's of error in cache.log:
> 
> 2017/01/25 10:22:16.053 kid1| external_acl.cc(868) aclMatchExternal: myparent1("p1 p1") = lookup needed
> 2017/01/25 10:22:16.053 kid1| external_acl.cc(871) aclMatchExternal: "p1 p1": queueing a call.
> 2017/01/25 10:22:16.053 kid1| Checklist.cc(115) goAsync: 0x7fff415cf470 a fast-only directive uses a slow ACL!
> 2017/01/25 10:22:16.053 kid1| external_acl.cc(873) aclMatchExternal: "p1 p1": no async support!
> 2017/01/25 10:22:16.053 kid1| external_acl.cc(874) aclMatchExternal: "p1 p1": return -1.
> 
> The documentation made it perfectly clear that "cache_peer_acccess" is a "fast ACL" that can only use fast ones...
> But I really need to use external "slow" acl. Please, is there a way to do it ?
> Again, this was working in 2.7 :(


Well, no. 2.7 was just being silent about the situation and guessing
whether you wanted OK/ERR result. Whereas Squid-3 tells you when the
fast-only cannot handle the ACL check results.

What you need to do is perform the external ACL check during one of the
*_access checks that permites slow lookups. eg. http_access.

Then use the 'note' ACL type in your fast-only access controls to check
some annotation that the helper returns to Squid.

Amos



From squid3 at treenet.co.nz  Fri Jan 27 03:47:27 2017
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Fri, 27 Jan 2017 16:47:27 +1300
Subject: [squid-users] URL bad rewritten using directive "deny_info"
In-Reply-To: <3dd83a67-cd31-a611-bb58-6610b3e3497e@coam.org>
References: <3dd83a67-cd31-a611-bb58-6610b3e3497e@coam.org>
Message-ID: <922603cf-a9d1-9d5b-e1bd-d7f3fa015380@treenet.co.nz>

On 26/01/2017 1:00 a.m., javier.sanchez wrote:
> 
>     Hi all.
> 
> 
>     I using squid 3.4.8 over Debian as reverse proxy in order to protect
> with SSL some of our servers.
> 

<snip>
> 
>     How can I solve that? Is this a bug or something that can be solved
> changing configuration.

Please try the Debian backports package of the 3.5 version.

If the issue remains, then you may have to use url_rewrite_program with
a helper to do the redirection instead of deny_info.

Have the helper produce the same 301:https://... URLs that deny_info
should have, and the http_access to allow HTTP traffic.
 That helper interface is a bit slower but will definitely work.

Amos



From johnpearson555 at gmail.com  Fri Jan 27 04:17:28 2017
From: johnpearson555 at gmail.com (John Pearson)
Date: Thu, 26 Jan 2017 20:17:28 -0800
Subject: [squid-users] squid on it's own server
Message-ID: <CAKNtY_yJmVZyE34H5Z6aw4z+_0cDmmKQxRoqDJqfMCr21AXexQ@mail.gmail.com>

hi all, my current setup: laptop(10.0.1.10) and squid-box(10.0.1.11) and
debian router(10.0.1.1).

I am doing wget on laptop

wget squid-cache.org

I am redirecting packets on the router to squid-box by changing the
destination MAC address and destination IP and port address. I am able to
see the packets reaching the squid-box and in squid log I am seeing many

10.0.1.11 TCP_MISS/503 47502 GET http://squid-cache.org/ - ORIGINAL_DST/
10.0.1.11 text/html

The log stream is really fast. All I see on laptop is ?HTTP request sent,
awaiting response ?" Any advice? thanks!
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20170126/b2403615/attachment.htm>

From rentorbuy at yahoo.com  Fri Jan 27 08:31:05 2017
From: rentorbuy at yahoo.com (Vieri)
Date: Fri, 27 Jan 2017 08:31:05 +0000 (UTC)
Subject: [squid-users] squid reverse proxy (accelerator) for MS Exchange
 OWA
In-Reply-To: <e13696a8-4540-bdd8-0634-902b737b82e4@measurement-factory.com>
References: <1216522442.160455.1484870639800.ref@mail.yahoo.com>
 <1216522442.160455.1484870639800@mail.yahoo.com>
 <cdea0afa-c2a2-23c4-dbac-553a565e83f7@treenet.co.nz>
 <690032068.431187.1484905491833@mail.yahoo.com>
 <5517395f-eaa4-0b3a-34e9-66ea8d0122a1@treenet.co.nz>
 <1374872097.4095106.1485244925609@mail.yahoo.com>
 <94978436-88e5-fc84-9f3e-b8f1757402a5@measurement-factory.com>
 <2010900066.469374.1485330325639@mail.yahoo.com>
 <0f2d2e08-bbd1-656a-a83b-5bb159983cbd@measurement-factory.com>
 <757248455.1520252.1485425773189@mail.yahoo.com>
 <e13696a8-4540-bdd8-0634-902b737b82e4@measurement-factory.com>
Message-ID: <1426807003.2339496.1485505865203@mail.yahoo.com>





----- Original Message -----
From: Alex Rousskov <rousskov at measurement-factory.com>

>> It's interesting to note that the following actually DOES give more information (unsupported 

>> protocol):>
> * If the server sent nothing, then Curl gave you potentially incorrect
> information (i.e., Curl is just _guessing_ what went wrong).


I never tried telling Squid to use TLS 1.1 ONLY so I never got to see Squid's log when using that protocol. I'm supposing I would have seen the same thing in Squid as I've seen it with CURL.
So I'm sure Squid would log useful information for the sys admin but... (see below).

>> Maybe if Squid gets an SSL negotiation error with no apparent reason
>> then it might need to retry connecting by being more explicit, just
>> like in my cURL and openssl binary examples above.
>
> Sorry, I do not know what "retry connecting by being more explicit"
> means. AFAICT, neither Curl nor s_client tried reconnecting in your
> examples. Also, an appropriate default for a command-line client is
> often a bad default for a proxy. It is complicated.


Let me rephrase my point but please keep in mind that I have no idea how Squid actually behaves. Simply put, when Squid tries to connect for the first time, it will probably (I'm guessing here) try the most secure protcol known today (ie. TSL 1.2), or let OpenSSL decide by default which is probably the same. In my case, the server replies nothing. That would be like running:

# curl -k -v https://10.215.144.21
or
# openssl s_client -connect 10.215.144.21:443

They give me the same information as Squid's log... almost nothing.

So my point is, if that first connection fails and gives me nothing for TLS 1.2 (or whatever the default is), two things can happen: either the remote site is failing or it isn't supporting the protocol. Why not "try again" but this time by being more specific? It would be like doing something like this:

# openssl s_client -connect 10.215.144.21:443 || openssl s_client -connect 10.215.144.21:443 -tls1_1 || openssl s_client -connect 10.215.144.21:443 -tls1
 

Of course, this shouldn't be done each and every time it tries to connect because it would probably give performance issues. If Squid successfully connects with TSL 1.0 then it could "remember" that for later connections to the same peer. It could also forget it after a sensible timeout, in case the remote peer starts supporting a safer protocol.

> Agreed in general, but the devil is in the details. Improving this is
> difficult, and nobody is working on it at the moment AFAIK.


I can imagine it must be difficult...


Instead of improving the source code, maybe a FAQ or some doc related to "squid error negotiating SSL" which would describe what to try when the error message is a mere "handshake failure". In the end, it's as simple as setting ssloptions correctly (in my case, NO_SSLv3,NO_SSLv2,NO_TLSv1_2,NO_TLSv1_1). I know there could be many other reasons for such a failure but at least that would be a good starting point.


Or even better... if Squid detects an SSL handshake failure with no extra info like in my case, can't it simply log an extra string that would look something like "Failed to negotiate SSL for unknown reason. Try setting ssloptions (cache_peer) or options (https_port) with a combination of NO_SSLv2 NO_SSLv3 
NO_TLSv1 NO_TLSv1_1 NO_TLSv1_2. Find out which SSL protocol is supported by the remote peer. If the connection still fails then you will need to analyze traffic with the peer to find out the reason."

In my case, that would have been enough info in Squid's log to fix the issue.

Thanks again.

Vieri


From Antony.Stone at squid.open.source.it  Fri Jan 27 09:18:13 2017
From: Antony.Stone at squid.open.source.it (Antony Stone)
Date: Fri, 27 Jan 2017 10:18:13 +0100
Subject: [squid-users] squid on it's own server
In-Reply-To: <CAKNtY_yJmVZyE34H5Z6aw4z+_0cDmmKQxRoqDJqfMCr21AXexQ@mail.gmail.com>
References: <CAKNtY_yJmVZyE34H5Z6aw4z+_0cDmmKQxRoqDJqfMCr21AXexQ@mail.gmail.com>
Message-ID: <201701271018.13548.Antony.Stone@squid.open.source.it>

On Friday 27 January 2017 at 05:17:28, John Pearson wrote:

> hi all, my current setup: laptop(10.0.1.10) and squid-box(10.0.1.11) and
> debian router(10.0.1.1).
> 
> I am doing wget on laptop
> 
> wget squid-cache.org
> 
> I am redirecting packets on the router to squid-box by changing the
> destination MAC address

Well, that's a novel way of doing policy routiong...

> and destination IP and port address.

Oh dear.

> I am able to see the packets reaching the squid-box and in squid log I am
> seeing many
> 
> 10.0.1.11 TCP_MISS/503 47502 GET http://squid-cache.org/ - ORIGINAL_DST/
> 10.0.1.11 text/html
> 
> The log stream is really fast. All I see on laptop is ?HTTP request sent,
> awaiting response ?" Any advice? thanks!

Yes, do NOT change the destination IP address on ANY machine except the one 
which Squid is running on.

See http://wiki.squid-cache.org/ConfigExamples/Intercept/LinuxRedirect and pay 
attention to the part which says "This configuration is given for use *on the 
squid box*."

Get the packets *to* that box however you like, but don't change them along 
the way.


Antony.

-- 
It may not seem obvious, but (6 x 5 + 5) x 5 - 55 equals 5!

                                                   Please reply to the list;
                                                         please *don't* CC me.


From yvoinov at gmail.com  Fri Jan 27 09:47:06 2017
From: yvoinov at gmail.com (Yuri)
Date: Fri, 27 Jan 2017 15:47:06 +0600
Subject: [squid-users] Not all html objects are being cached
In-Reply-To: <bc726969-ca5e-912b-7600-b4d4408c0c02@treenet.co.nz>
References: <1485273173646-4681293.post@n4.nabble.com>
 <1485375724827-4681326.post@n4.nabble.com>
 <85b108fb-1f3e-d7d5-48e1-37edbb2ae2d9@gmail.com>
 <20170126204449.GB25104@fantomas.sk>
 <7fa88d69-1964-0eeb-9e5d-5f539e9a0805@gmail.com>
 <bc726969-ca5e-912b-7600-b4d4408c0c02@treenet.co.nz>
Message-ID: <a9ab7ffd-8f89-b78d-df6e-bdb49cc0c27f@gmail.com>



27.01.2017 9:10, Amos Jeffries ?????:
> On 27/01/2017 9:46 a.m., Yuri Voinov wrote:
>>
>> 27.01.2017 2:44, Matus UHLAR - fantomas ?????:
>>>> 26.01.2017 2:22, boruc ?????:
>>>>> After a little bit of analyzing requests and responses with WireShark I
>>>>> noticed that many sites that weren't cached had different
>>>>> combination of
>>>>> below parameters:
>>>>>
>>>>> Cache-Control: no-cache, no-store, must-revalidate, post-check,
>>>>> pre-check,
>>>>> private, public, max-age, public
>>>>> Pragma: no-cache
>>> On 26.01.17 02:44, Yuri Voinov wrote:
>>>> If the webmaster has done this - he had good reason to. Trying to break
>>>> the RFC in this way, you break the Internet.
>>> Actually, no. If the webmaster has done the above - he has no damn
>>> idea what
>>> those mean (private and public?) , and how to provide properly cacheable
>>> content.
>> It was sarcasm.
>
> You may have intended it to be. But you spoke the simple truth.
>
> Other than 'public' there really are situations which have "good reason"
> to send that set of controls all at once.
>
> For example; any admin who wants a RESTful or SaaS application to
> actually work for all their potential customers.
>
>
> I have been watching the below cycle take place for the past 20 years in
> HTTP:
>
> Webmaster: dont cache this please.
>
>    "Cache-Control: no-store"
>
> Proxy Admin: ignore-no-store
>
>
> Webmaster: I meant it. Dont deliver anything you cached without fetching
> a updated version.
>
>    ... "no-store, no-cache"
>
> Proxy Admin: ignore-no-cache
>
>
> Webmaster: really you MUST revalidate before using ths data.
>
>   ... "no-store, no-cache, must-revalidate"
>
> Proxy Admin: ignore-must-revalidate
>
>
> Webmaster: Really I meant it. This is non-storable PRIVATE DATA!
>
> ... "no-store, no-cache, must-revalidate, private"
>
> Proxy Admin: ignore-private
>
>
> Webmaster: Seriously. I'm changing it on EVERY request! dont store it.
>
> ... "no-store, no-cache, must-revalidate, private, max-age=0"
> "Expires: -1"
>
> Proxy Admin: ignore-expires
>
>
> Webmaster: are you one of those dumb HTTP/1.0 proxies who dont
> understand Cache-Control?
>
> "Pragma: no-cache"
> "Expires: 1 Jan 1970"
>
> Proxy Admin: hehe! I already ignore-no-cache ignore-expires
>
>
> Webmaster: F*U!  May your clients batch up their traffic to slam you
> with it all at once!
>
> ... "no-store, no-cache, must-revalidate, private, max-age=0,
> pre-check=1, post-check=1"
>
>
> Proxy Admin: My bandwidth! I need to cache more!
>
> Webmaster: Doh! Oh well, so I have to write my application to force new
> content then.
>
> Proxy Admin: ignore-reload
>
>
> Webmaster: Now What? Oh HTTPS wont have any damn proxies in the way....
>
> ... the cycle repeats again within HTTPS. Took all of 5 years this time.
>
> ... the cycle repeats again within SPDY. That took only ~1 year.
>
> ... the cycle repeats again within CoAP. The standards are not even
> finished yet and its underway.
>
>
> Stop this cycle of stupidity. It really HAS "broken the Internet".
All that would be just great if a webmaster was conscientious. I will 
give just one example.

Only one example.

root @ khorne /patch # wget -S http://www.microsoft.com
--2017-01-27 15:29:54--  http://www.microsoft.com/
Connecting to 127.0.0.1:3128... connected.
Proxy request sent, awaiting response...
   HTTP/1.1 302 Found
   Server: AkamaiGHost
   Content-Length: 0
   Location: http://www.microsoft.com/ru-kz/
   Date: Fri, 27 Jan 2017 09:29:54 GMT
   X-CCC: NL
   X-CID: 2
   X-Cache: MISS from khorne
   X-Cache-Lookup: MISS from khorne:3128
   Connection: keep-alive
Location: http://www.microsoft.com/ru-kz/ [following]
--2017-01-27 15:29:54--  http://www.microsoft.com/ru-kz/
Reusing existing connection to 127.0.0.1:3128.
Proxy request sent, awaiting response...
   HTTP/1.1 301 Moved Permanently
   Server: AkamaiGHost
   Content-Length: 0
   Location: https://www.microsoft.com/ru-kz/
   Date: Fri, 27 Jan 2017 09:29:54 GMT
   Set-Cookie: 
akacd_OneRF=1493285394~rv=7~id=6a2316770abdbb58a85c16676a0f84fd; path=/; 
Expires=Thu, 27 Apr 2017 09:29:54 GMT
   X-CCC: NL
   X-CID: 2
   X-Cache: MISS from khorne
   X-Cache-Lookup: MISS from khorne:3128
   Connection: keep-alive
Location: https://www.microsoft.com/ru-kz/ [following]
--2017-01-27 15:29:54--  https://www.microsoft.com/ru-kz/
Connecting to 127.0.0.1:3128... connected.
Proxy request sent, awaiting response...
   HTTP/1.1 200 OK
   Cache-Control: no-cache, no-store
   Pragma: no-cache
   Content-Type: text/html
   Expires: -1
   Server: Microsoft-IIS/8.0
   CorrelationVector: BzssVwiBIUaXqyOh.1.1
   X-AspNet-Version: 4.0.30319
   X-Powered-By: ASP.NET
   Access-Control-Allow-Headers: Origin, X-Requested-With, Content-Type, 
Accept
   Access-Control-Allow-Methods: GET, POST, PUT, DELETE, OPTIONS
   Access-Control-Allow-Credentials: true
   P3P: CP="ALL IND DSP COR ADM CONo CUR CUSo IVAo IVDo PSA PSD TAI TELo 
OUR SAMo CNT COM INT NAV ONL PHY PRE PUR UNI"
   X-Frame-Options: SAMEORIGIN
   Vary: Accept-Encoding
   Content-Encoding: gzip
   Date: Fri, 27 Jan 2017 09:29:56 GMT
   Content-Length: 13322
   Set-Cookie: MS-CV=BzssVwiBIUaXqyOh.1; domain=.microsoft.com; 
expires=Sat, 28-Jan-2017 09:29:56 GMT; path=/
   Set-Cookie: MS-CV=BzssVwiBIUaXqyOh.2; domain=.microsoft.com; 
expires=Sat, 28-Jan-2017 09:29:56 GMT; path=/
   Strict-Transport-Security: max-age=0; includeSubDomains
   X-CCC: NL
   X-CID: 2
   X-Cache: MISS from khorne
   X-Cache-Lookup: MISS from khorne:3128
   Connection: keep-alive
Length: 13322 (13K) [text/html]
Saving to: 'index.html'

index.html          100%[==================>]  13.01K --.-KB/s    in 0s

2017-01-27 15:29:57 (32.2 MB/s) - 'index.html' saved [13322/13322]

Can you explain me - for what static index.html has this:

Cache-Control: no-cache, no-store
Pragma: no-cache

?

What can be broken to ignore CC in this page?


Yes, saving traffic is the most important, because not all and not 
everywhere has terabit links with unlimited calling. Moreover, the 
number of users increases and the capacity is finite. In any case, the 
decision on how to deal with the content in such a situation should 
remain behind the proxy administrator. And not for the developers of 
this proxy, which is hardcoded own vision, even with RFC. Because the 
byte-hit 10% (vanilla Squid, after very hadr work it will be up to 30%, 
but no more) - this is ridiculous. In such a situation would be more 
honest nothing at all cache - only let's not say that the squid - a 
caching proxy. Set the path of the secondary server that requires a lot 
of attention, despite the fact that it gives a gain only 10% - a mockery 
of users.

Let me explain the situation as I see it. Webmaster hanging everywhere 
ban caching in any way possible, because on its pages full of 
advertising. For that pays money. This is the same reason that Google 
prevents caching Youtube. Big money. We do not get the money, in fact, 
our goal - to minimize the costs of traffic. We choose Squid as a tool. 
And you, with your point of view, deprived us of weapons against 
unscrupulous webmasters. So it looks.

Again. Breaking the Internet - it should be my choice, not yours. Or 
follow the RFC at 100% - or do not have to break it in part. You either 
wear pants or remove the cross, as they say.
>
>
> HTH
> Amos
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users



From snklusov at gmail.com  Fri Jan 27 11:36:52 2017
From: snklusov at gmail.com (Sergey Klusov)
Date: Fri, 27 Jan 2017 16:36:52 +0500
Subject: [squid-users] transparent http and https filter with white-list only
Message-ID: <c25e40df-d60e-2900-d6e6-954771e56c90@gmail.com>

Hello. I'm trying to get working transparent setup allowing only certain 
domains and have problem that in order to allow https "ssl_bump splice 
allowed_domains" i have to "http_access allow all", thus allowing all 
other http traffic through. Otherwise https traffic is not allowed at all.

Here is my config:

=======config=======
http_port 10.96.243.1:3128 intercept options=NO_SSLv3:NO_SSLv2
http_port 10.96.243.1:3130 options=NO_SSLv3:NO_SSLv2
https_port 10.96.243.1:3129 intercept ssl-bump 
options=ALL:NO_SSLv3:NO_SSLv2 connection-auth=off 
cert=/etc/squid/squidCA.pem
acl localnet src 10.0.0.0/8     # RFC1918 possible internal network

acl SSL_ports port 443
acl Safe_ports port 80          # http
acl Safe_ports port 443         # https
acl CONNECT method CONNECT

acl http_allow dstdomain "/etc/squid/http_allow_domains.txt"
acl https_allow ssl::server_name "/etc/squid/https_allow_domains.txt"

sslproxy_cert_error allow all
sslproxy_flags DONT_VERIFY_PEER

acl step1 at_step SslBump1
ssl_bump peek step1
ssl_bump splice https_allow
ssl_bump terminate all

cache deny all

http_access deny !Safe_ports
http_access deny CONNECT !SSL_ports
http_access allow localhost manager
http_access deny manager

http_access allow all http_allow
http_access allow all https_allow
http_access deny all

always_direct allow all

coredump_dir /var/spool/squid

refresh_pattern .               0       0%      0

logformat ssl %ts.%03tu %6tr %>a %la:%lp %Ss/%03>Hs %<st %rm %ssl::>sni 
%ru %[un %Sh/%<a %mt
access_log daemon:/var/log/squid/access.log logformat=ssl
================cut==============

files with domain names:
=====================
# cat http_allow_domains.txt
.google.com
# cat https_allow_domains.txt
.google.com
=====================

With this config http filtering works and https://google.com request 
gets replied with self-signed squid deny message.
If i replace "http_access deny all" with "http_access allow all", https 
filtering starts working, allowing https://google.com and resetting 
other https requests, BUT it allows any http traffic as well!

What do i do wrong?
I need my server to pass "/etc/squid/http_allow_domains.txt" HTTP and 
"/etc/squid/https_allow_domains.txt" HTTPS domains ONLY.


From garryd at comnet.uz  Fri Jan 27 11:54:10 2017
From: garryd at comnet.uz (Garri Djavadyan)
Date: Fri, 27 Jan 2017 16:54:10 +0500
Subject: [squid-users] Not all html objects are being cached
In-Reply-To: <a9ab7ffd-8f89-b78d-df6e-bdb49cc0c27f@gmail.com>
References: <1485273173646-4681293.post@n4.nabble.com>
 <1485375724827-4681326.post@n4.nabble.com>
 <85b108fb-1f3e-d7d5-48e1-37edbb2ae2d9@gmail.com>
 <20170126204449.GB25104@fantomas.sk>
 <7fa88d69-1964-0eeb-9e5d-5f539e9a0805@gmail.com>
 <bc726969-ca5e-912b-7600-b4d4408c0c02@treenet.co.nz>
 <a9ab7ffd-8f89-b78d-df6e-bdb49cc0c27f@gmail.com>
Message-ID: <1485518050.5283.2.camel@comnet.uz>

On Fri, 2017-01-27 at 15:47 +0600, Yuri wrote:
> --2017-01-27 15:29:54--??https://www.microsoft.com/ru-kz/
> Connecting to 127.0.0.1:3128... connected.
> Proxy request sent, awaiting response...
> ???HTTP/1.1 200 OK
> ???Cache-Control: no-cache, no-store
> ???Pragma: no-cache
> ???Content-Type: text/html
> ???Expires: -1
> ???Server: Microsoft-IIS/8.0
> ???CorrelationVector: BzssVwiBIUaXqyOh.1.1
> ???X-AspNet-Version: 4.0.30319
> ???X-Powered-By: ASP.NET
> ???Access-Control-Allow-Headers: Origin, X-Requested-With, Content-
> Type,?
> Accept
> ???Access-Control-Allow-Methods: GET, POST, PUT, DELETE, OPTIONS
> ???Access-Control-Allow-Credentials: true
> ???P3P: CP="ALL IND DSP COR ADM CONo CUR CUSo IVAo IVDo PSA PSD TAI
> TELo?
> OUR SAMo CNT COM INT NAV ONL PHY PRE PUR UNI"
> ???X-Frame-Options: SAMEORIGIN
> ???Vary: Accept-Encoding
> ???Content-Encoding: gzip
> ???Date: Fri, 27 Jan 2017 09:29:56 GMT
> ???Content-Length: 13322
> ???Set-Cookie: MS-CV=BzssVwiBIUaXqyOh.1; domain=.microsoft.com;?
> expires=Sat, 28-Jan-2017 09:29:56 GMT; path=/
> ???Set-Cookie: MS-CV=BzssVwiBIUaXqyOh.2; domain=.microsoft.com;?
> expires=Sat, 28-Jan-2017 09:29:56 GMT; path=/
> ???Strict-Transport-Security: max-age=0; includeSubDomains
> ???X-CCC: NL
> ???X-CID: 2
> ???X-Cache: MISS from khorne
> ???X-Cache-Lookup: MISS from khorne:3128
> ???Connection: keep-alive
> Length: 13322 (13K) [text/html]
> Saving to: 'index.html'
> 
> index.html??????????100%[==================>]??13.01K --.-KB/s????in
> 0s
> 
> 2017-01-27 15:29:57 (32.2 MB/s) - 'index.html' saved [13322/13322]
> 
> Can you explain me - for what static index.html has this:
> 
> Cache-Control: no-cache, no-store
> Pragma: no-cache
> 
> ?
> 
> What can be broken to ignore CC in this page?

Hi Yuri,


Why do you think the page returned for URL [https://www.microsoft.com/r
u-kz/] is static and not dynamically generated one?

The index.html file is default file name for wget.

man wget:
  --default-page=name
???????Use name as the default file name when it isn't known (i.e., for
       URLs that end in a slash), instead of index.html.

In fact the https://www.microsoft.com/ru-kz/index.html is a stub page
(The page you requested cannot be found.).


Garri


From yvoinov at gmail.com  Fri Jan 27 11:58:52 2017
From: yvoinov at gmail.com (Yuri)
Date: Fri, 27 Jan 2017 17:58:52 +0600
Subject: [squid-users] Not all html objects are being cached
In-Reply-To: <1485518050.5283.2.camel@comnet.uz>
References: <1485273173646-4681293.post@n4.nabble.com>
 <1485375724827-4681326.post@n4.nabble.com>
 <85b108fb-1f3e-d7d5-48e1-37edbb2ae2d9@gmail.com>
 <20170126204449.GB25104@fantomas.sk>
 <7fa88d69-1964-0eeb-9e5d-5f539e9a0805@gmail.com>
 <bc726969-ca5e-912b-7600-b4d4408c0c02@treenet.co.nz>
 <a9ab7ffd-8f89-b78d-df6e-bdb49cc0c27f@gmail.com>
 <1485518050.5283.2.camel@comnet.uz>
Message-ID: <5b4106a3-7143-33ce-a4c8-ecb21f64f6fa@gmail.com>



27.01.2017 17:54, Garri Djavadyan ?????:
> On Fri, 2017-01-27 at 15:47 +0600, Yuri wrote:
>> --2017-01-27 15:29:54--  https://www.microsoft.com/ru-kz/
>> Connecting to 127.0.0.1:3128... connected.
>> Proxy request sent, awaiting response...
>>     HTTP/1.1 200 OK
>>     Cache-Control: no-cache, no-store
>>     Pragma: no-cache
>>     Content-Type: text/html
>>     Expires: -1
>>     Server: Microsoft-IIS/8.0
>>     CorrelationVector: BzssVwiBIUaXqyOh.1.1
>>     X-AspNet-Version: 4.0.30319
>>     X-Powered-By: ASP.NET
>>     Access-Control-Allow-Headers: Origin, X-Requested-With, Content-
>> Type,
>> Accept
>>     Access-Control-Allow-Methods: GET, POST, PUT, DELETE, OPTIONS
>>     Access-Control-Allow-Credentials: true
>>     P3P: CP="ALL IND DSP COR ADM CONo CUR CUSo IVAo IVDo PSA PSD TAI
>> TELo
>> OUR SAMo CNT COM INT NAV ONL PHY PRE PUR UNI"
>>     X-Frame-Options: SAMEORIGIN
>>     Vary: Accept-Encoding
>>     Content-Encoding: gzip
>>     Date: Fri, 27 Jan 2017 09:29:56 GMT
>>     Content-Length: 13322
>>     Set-Cookie: MS-CV=BzssVwiBIUaXqyOh.1; domain=.microsoft.com;
>> expires=Sat, 28-Jan-2017 09:29:56 GMT; path=/
>>     Set-Cookie: MS-CV=BzssVwiBIUaXqyOh.2; domain=.microsoft.com;
>> expires=Sat, 28-Jan-2017 09:29:56 GMT; path=/
>>     Strict-Transport-Security: max-age=0; includeSubDomains
>>     X-CCC: NL
>>     X-CID: 2
>>     X-Cache: MISS from khorne
>>     X-Cache-Lookup: MISS from khorne:3128
>>     Connection: keep-alive
>> Length: 13322 (13K) [text/html]
>> Saving to: 'index.html'
>>
>> index.html          100%[==================>]  13.01K --.-KB/s    in
>> 0s
>>
>> 2017-01-27 15:29:57 (32.2 MB/s) - 'index.html' saved [13322/13322]
>>
>> Can you explain me - for what static index.html has this:
>>
>> Cache-Control: no-cache, no-store
>> Pragma: no-cache
>>
>> ?
>>
>> What can be broken to ignore CC in this page?
> Hi Yuri,
>
>
> Why do you think the page returned for URL [https://www.microsoft.com/r
> u-kz/] is static and not dynamically generated one?
And for me, what's the difference? Does it change anything? In addition, 
it is easy to see on the page and even the eyes - strangely enough - to 
open its code. And? What do you see there?
>
> The index.html file is default file name for wget.
And also the name of the default home page in the web. Imagine - I know 
the obvious things. But the question was about something else.
>
> man wget:
>    --default-page=name
>         Use name as the default file name when it isn't known (i.e., for
>         URLs that end in a slash), instead of index.html.
>
> In fact the https://www.microsoft.com/ru-kz/index.html is a stub page
> (The page you requested cannot be found.).
You living in wrong region. This is geo-dependent page, as obvious, yes?

Again. What is the difference? I open it from different workstations, 
from different browsers - I see the same thing. The code is identical. I 
can is to cache? Yes or no?
>
>
> Garri
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users



From yvoinov at gmail.com  Fri Jan 27 12:03:09 2017
From: yvoinov at gmail.com (Yuri)
Date: Fri, 27 Jan 2017 18:03:09 +0600
Subject: [squid-users] Not all html objects are being cached
In-Reply-To: <1485518050.5283.2.camel@comnet.uz>
References: <1485273173646-4681293.post@n4.nabble.com>
 <1485375724827-4681326.post@n4.nabble.com>
 <85b108fb-1f3e-d7d5-48e1-37edbb2ae2d9@gmail.com>
 <20170126204449.GB25104@fantomas.sk>
 <7fa88d69-1964-0eeb-9e5d-5f539e9a0805@gmail.com>
 <bc726969-ca5e-912b-7600-b4d4408c0c02@treenet.co.nz>
 <a9ab7ffd-8f89-b78d-df6e-bdb49cc0c27f@gmail.com>
 <1485518050.5283.2.camel@comnet.uz>
Message-ID: <c512898d-190e-edc0-add1-52b89c344fce@gmail.com>

I understand that I want to conclusively prove its case. But for the 
sake of objectivity - dynamically generated only dynamic pages? Maybe 
the solution is still the administrator to leave? If I see that 
something is broken or users complain about me - directive *cache deny* 
already canceled?


27.01.2017 17:54, Garri Djavadyan ?????:
> On Fri, 2017-01-27 at 15:47 +0600, Yuri wrote:
>> --2017-01-27 15:29:54--  https://www.microsoft.com/ru-kz/
>> Connecting to 127.0.0.1:3128... connected.
>> Proxy request sent, awaiting response...
>>     HTTP/1.1 200 OK
>>     Cache-Control: no-cache, no-store
>>     Pragma: no-cache
>>     Content-Type: text/html
>>     Expires: -1
>>     Server: Microsoft-IIS/8.0
>>     CorrelationVector: BzssVwiBIUaXqyOh.1.1
>>     X-AspNet-Version: 4.0.30319
>>     X-Powered-By: ASP.NET
>>     Access-Control-Allow-Headers: Origin, X-Requested-With, Content-
>> Type,
>> Accept
>>     Access-Control-Allow-Methods: GET, POST, PUT, DELETE, OPTIONS
>>     Access-Control-Allow-Credentials: true
>>     P3P: CP="ALL IND DSP COR ADM CONo CUR CUSo IVAo IVDo PSA PSD TAI
>> TELo
>> OUR SAMo CNT COM INT NAV ONL PHY PRE PUR UNI"
>>     X-Frame-Options: SAMEORIGIN
>>     Vary: Accept-Encoding
>>     Content-Encoding: gzip
>>     Date: Fri, 27 Jan 2017 09:29:56 GMT
>>     Content-Length: 13322
>>     Set-Cookie: MS-CV=BzssVwiBIUaXqyOh.1; domain=.microsoft.com;
>> expires=Sat, 28-Jan-2017 09:29:56 GMT; path=/
>>     Set-Cookie: MS-CV=BzssVwiBIUaXqyOh.2; domain=.microsoft.com;
>> expires=Sat, 28-Jan-2017 09:29:56 GMT; path=/
>>     Strict-Transport-Security: max-age=0; includeSubDomains
>>     X-CCC: NL
>>     X-CID: 2
>>     X-Cache: MISS from khorne
>>     X-Cache-Lookup: MISS from khorne:3128
>>     Connection: keep-alive
>> Length: 13322 (13K) [text/html]
>> Saving to: 'index.html'
>>
>> index.html          100%[==================>]  13.01K --.-KB/s    in
>> 0s
>>
>> 2017-01-27 15:29:57 (32.2 MB/s) - 'index.html' saved [13322/13322]
>>
>> Can you explain me - for what static index.html has this:
>>
>> Cache-Control: no-cache, no-store
>> Pragma: no-cache
>>
>> ?
>>
>> What can be broken to ignore CC in this page?
> Hi Yuri,
>
>
> Why do you think the page returned for URL [https://www.microsoft.com/r
> u-kz/] is static and not dynamically generated one?
>
> The index.html file is default file name for wget.
>
> man wget:
>    --default-page=name
>         Use name as the default file name when it isn't known (i.e., for
>         URLs that end in a slash), instead of index.html.
>
> In fact the https://www.microsoft.com/ru-kz/index.html is a stub page
> (The page you requested cannot be found.).
>
>
> Garri
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20170127/e93da604/attachment.htm>

From Antony.Stone at squid.open.source.it  Fri Jan 27 12:05:41 2017
From: Antony.Stone at squid.open.source.it (Antony Stone)
Date: Fri, 27 Jan 2017 13:05:41 +0100
Subject: [squid-users] Not all html objects are being cached
In-Reply-To: <5b4106a3-7143-33ce-a4c8-ecb21f64f6fa@gmail.com>
References: <1485273173646-4681293.post@n4.nabble.com>
 <1485518050.5283.2.camel@comnet.uz>
 <5b4106a3-7143-33ce-a4c8-ecb21f64f6fa@gmail.com>
Message-ID: <201701271305.41490.Antony.Stone@squid.open.source.it>

On Friday 27 January 2017 at 12:58:52, Yuri wrote:

> Again. What is the difference? I open it from different workstations,
> from different browsers - I see the same thing. The code is identical. I
> can is to cache? Yes or no?

You're entitled to do whatever you want to, following standards and 
recommendations or not - just don't complain when choosing not to follow those 
standards and recommendations results in behaviour different from what you 
wanted (or what someone else intended).

Oh, and by the way, what did you mean earlier when you said:

> You either wear pants or remove the cross, as they say.

	?


Antony.

-- 
"640 kilobytes (of RAM) should be enough for anybody."

 - Bill Gates

                                                   Please reply to the list;
                                                         please *don't* CC me.


From yvoinov at gmail.com  Fri Jan 27 12:15:21 2017
From: yvoinov at gmail.com (Yuri)
Date: Fri, 27 Jan 2017 18:15:21 +0600
Subject: [squid-users] Not all html objects are being cached
In-Reply-To: <201701271305.41490.Antony.Stone@squid.open.source.it>
References: <1485273173646-4681293.post@n4.nabble.com>
 <1485518050.5283.2.camel@comnet.uz>
 <5b4106a3-7143-33ce-a4c8-ecb21f64f6fa@gmail.com>
 <201701271305.41490.Antony.Stone@squid.open.source.it>
Message-ID: <e81749cc-d91b-56b7-2476-6a4ca557b094@gmail.com>



27.01.2017 18:05, Antony Stone ?????:
> On Friday 27 January 2017 at 12:58:52, Yuri wrote:
>
>> Again. What is the difference? I open it from different workstations,
>> from different browsers - I see the same thing. The code is identical. I
>> can is to cache? Yes or no?
> You're entitled to do whatever you want to, following standards and
> recommendations or not - just don't complain when choosing not to follow those
> standards and recommendations results in behaviour different from what you
> wanted (or what someone else intended).
All this crazy debate reminds me of Microsoft Windows. Windows is better 
to know why the administrator should not have full access. Windows is 
better to know how to work. Windows is better to know how to tell the 
system administrator so that he called the system administrator.

Antonio, you've seen at least once, so I complained about the 
consequences of my own actions?

>
> Oh, and by the way, what did you mean earlier when you said:
>
>> You either wear pants or remove the cross, as they say.
> 	?
This is the end of a good Russian joke about a priest who had sex.
I meant that we should ever stop having sex - or remove the pectoral 
cross. This is to ensure that the need to be consistent.
>
>
> Antony.
>



From Antony.Stone at squid.open.source.it  Fri Jan 27 12:25:17 2017
From: Antony.Stone at squid.open.source.it (Antony Stone)
Date: Fri, 27 Jan 2017 13:25:17 +0100
Subject: [squid-users] Not all html objects are being cached
In-Reply-To: <e81749cc-d91b-56b7-2476-6a4ca557b094@gmail.com>
References: <1485273173646-4681293.post@n4.nabble.com>
 <201701271305.41490.Antony.Stone@squid.open.source.it>
 <e81749cc-d91b-56b7-2476-6a4ca557b094@gmail.com>
Message-ID: <201701271325.18195.Antony.Stone@squid.open.source.it>

On Friday 27 January 2017 at 13:15:21, Yuri wrote:

> 27.01.2017 18:05, Antony Stone ?????:
> 
> > You're entitled to do whatever you want to, following standards and
> > recommendations or not - just don't complain when choosing not to follow
> > those standards and recommendations results in behaviour different from
> > what you wanted (or what someone else intended).
> 
> All this crazy debate reminds me of Microsoft Windows. Windows is better
> to know why the administrator should not have full access. Windows is
> better to know how to work. Windows is better to know how to tell the
> system administrator so that he called the system administrator.

That should remind you of OS X and Android as well, at the very least (and 
quite possibly systemd as well)

My opinion is that it's your choice whether to run Microsoft Windows (or Apple 
OS X, or Google Android) or not - but you have to accept it as a whole 
package; you can't say "I want some of the neat features, but I want them to 
work *my* way".

If you don't accept all aspects of the package, then don't use it.

> Antonio, you've seen at least once, so I complained about the
> consequences of my own actions?

You seem to continually complain that people are recommending not to try going 
against standards, or trying to defeat the anti-caching directives on websites 
you find.

It's your choice to try doing that; people are saying "but if you do that, bad 
things will happen, or things will break, or it just won't work the way you 
want it to", and then you say "but I don't like having to follow the rules".

That's what I meant about complaining about the consequences of your actions.


Antony.

-- 
"Life is just a lot better if you feel you're having 10 [small] wins a day 
rather than a [big] win every 10 years or so."

 - Chris Hadfield, former skiing (and ski racing) instructor

                                                   Please reply to the list;
                                                         please *don't* CC me.


From yvoinov at gmail.com  Fri Jan 27 12:35:11 2017
From: yvoinov at gmail.com (Yuri)
Date: Fri, 27 Jan 2017 18:35:11 +0600
Subject: [squid-users] Not all html objects are being cached
In-Reply-To: <201701271325.18195.Antony.Stone@squid.open.source.it>
References: <1485273173646-4681293.post@n4.nabble.com>
 <201701271305.41490.Antony.Stone@squid.open.source.it>
 <e81749cc-d91b-56b7-2476-6a4ca557b094@gmail.com>
 <201701271325.18195.Antony.Stone@squid.open.source.it>
Message-ID: <738bb2b4-6455-ebc7-26cf-7960f5be49ba@gmail.com>



27.01.2017 18:25, Antony Stone ?????:
> On Friday 27 January 2017 at 13:15:21, Yuri wrote:
>
>> 27.01.2017 18:05, Antony Stone ?????:
>>
>>> You're entitled to do whatever you want to, following standards and
>>> recommendations or not - just don't complain when choosing not to follow
>>> those standards and recommendations results in behaviour different from
>>> what you wanted (or what someone else intended).
>> All this crazy debate reminds me of Microsoft Windows. Windows is better
>> to know why the administrator should not have full access. Windows is
>> better to know how to work. Windows is better to know how to tell the
>> system administrator so that he called the system administrator.
> That should remind you of OS X and Android as well, at the very least (and
> quite possibly systemd as well)
>
> My opinion is that it's your choice whether to run Microsoft Windows (or Apple
> OS X, or Google Android) or not - but you have to accept it as a whole
> package; you can't say "I want some of the neat features, but I want them to
> work *my* way".
>
> If you don't accept all aspects of the package, then don't use it.
I just want to have a choice and an opportunity to say - "F*ck you, man, 
I'm the System Administrator".

If you do not want to violate the RFC - remove violations HTTP at all. 
If you remember, this mode is now enabled by default.

You do not have to teach me that I use. I - an administrator and wish to 
be able to select tools. And do not be in a situation where the choice 
is made for me.


>
>> Antonio, you've seen at least once, so I complained about the
>> consequences of my own actions?
> You seem to continually complain that people are recommending not to try going
> against standards, or trying to defeat the anti-caching directives on websites
> you find.
>
> It's your choice to try doing that; people are saying "but if you do that, bad
> things will happen, or things will break, or it just won't work the way you
> want it to", and then you say "but I don't like having to follow the rules".
>
> That's what I meant about complaining about the consequences of your actions.
It is my right and my choice. Personally, I do not complain of the 
consequences, having enough tools to solve any problem.

Enough to learn me. Op asked why he did not cached static html. That 
explains to him that in fact there live dragons and why he is wrong in 
desires to cache any and all.
>
>
> Antony.
>



From erdosain9 at gmail.com  Fri Jan 27 13:13:55 2017
From: erdosain9 at gmail.com (erdosain9)
Date: Fri, 27 Jan 2017 05:13:55 -0800 (PST)
Subject: [squid-users] Strange behavior - reload service failed,
	but not start....
In-Reply-To: <33d99c9f-8c63-65c2-9cfb-bb217ba5c176@treenet.co.nz>
References: <1485349951724-4681317.post@n4.nabble.com>
 <CA+sSnVbSgDGBSzTVNbomVOKzH5oAd+vKB0JGVdjxai3rjihHYA@mail.gmail.com>
 <1485362331939-4681322.post@n4.nabble.com>
 <33d99c9f-8c63-65c2-9cfb-bb217ba5c176@treenet.co.nz>
Message-ID: <1485522835174-4681360.post@n4.nabble.com>

Ok, thanks.
But something more its wrong.... look up this:

[root at squid ips]# squid -k restart
squid: ERROR: Could not send signal 21 to process 8083: (3) No such process

[root at squid ips]# squid -k shutdown
squid: ERROR: Could not send signal 15 to process 8083: (3) No such process

[root at squid ips]# squid -k kill
squid: ERROR: Could not send signal 9 to process 8083: (3) No such process

[root at squid ips]# squid -k debug
squid: ERROR: Could not send signal 12 to process 8083: (3) No such process

......mmm... what's going on here???

But actually squid is running and working, so.... 
Also, if i do a change in squid.conf... it dosent take it. neither
systemctl, or like you see any squid -k command







--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/Strange-behavior-reload-service-failed-but-not-start-tp4681317p4681360.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From garryd at comnet.uz  Fri Jan 27 13:35:46 2017
From: garryd at comnet.uz (Garri Djavadyan)
Date: Fri, 27 Jan 2017 18:35:46 +0500
Subject: [squid-users] Not all html objects are being cached
In-Reply-To: <5b4106a3-7143-33ce-a4c8-ecb21f64f6fa@gmail.com>
References: <1485273173646-4681293.post@n4.nabble.com>
 <1485375724827-4681326.post@n4.nabble.com>
 <85b108fb-1f3e-d7d5-48e1-37edbb2ae2d9@gmail.com>
 <20170126204449.GB25104@fantomas.sk>
 <7fa88d69-1964-0eeb-9e5d-5f539e9a0805@gmail.com>
 <bc726969-ca5e-912b-7600-b4d4408c0c02@treenet.co.nz>
 <a9ab7ffd-8f89-b78d-df6e-bdb49cc0c27f@gmail.com>
 <1485518050.5283.2.camel@comnet.uz>
 <5b4106a3-7143-33ce-a4c8-ecb21f64f6fa@gmail.com>
Message-ID: <1485524146.5283.5.camel@comnet.uz>

On Fri, 2017-01-27 at 17:58 +0600, Yuri wrote:
> 
> 27.01.2017 17:54, Garri Djavadyan ?????:
> > On Fri, 2017-01-27 at 15:47 +0600, Yuri wrote:
> > > --2017-01-27 15:29:54--??https://www.microsoft.com/ru-kz/
> > > Connecting to 127.0.0.1:3128... connected.
> > > Proxy request sent, awaiting response...
> > > ????HTTP/1.1 200 OK
> > > ????Cache-Control: no-cache, no-store
> > > ????Pragma: no-cache
> > > ????Content-Type: text/html
> > > ????Expires: -1
> > > ????Server: Microsoft-IIS/8.0
> > > ????CorrelationVector: BzssVwiBIUaXqyOh.1.1
> > > ????X-AspNet-Version: 4.0.30319
> > > ????X-Powered-By: ASP.NET
> > > ????Access-Control-Allow-Headers: Origin, X-Requested-With,
> > > Content-
> > > Type,
> > > Accept
> > > ????Access-Control-Allow-Methods: GET, POST, PUT, DELETE, OPTIONS
> > > ????Access-Control-Allow-Credentials: true
> > > ????P3P: CP="ALL IND DSP COR ADM CONo CUR CUSo IVAo IVDo PSA PSD
> > > TAI
> > > TELo
> > > OUR SAMo CNT COM INT NAV ONL PHY PRE PUR UNI"
> > > ????X-Frame-Options: SAMEORIGIN
> > > ????Vary: Accept-Encoding
> > > ????Content-Encoding: gzip
> > > ????Date: Fri, 27 Jan 2017 09:29:56 GMT
> > > ????Content-Length: 13322
> > > ????Set-Cookie: MS-CV=BzssVwiBIUaXqyOh.1; domain=.microsoft.com;
> > > expires=Sat, 28-Jan-2017 09:29:56 GMT; path=/
> > > ????Set-Cookie: MS-CV=BzssVwiBIUaXqyOh.2; domain=.microsoft.com;
> > > expires=Sat, 28-Jan-2017 09:29:56 GMT; path=/
> > > ????Strict-Transport-Security: max-age=0; includeSubDomains
> > > ????X-CCC: NL
> > > ????X-CID: 2
> > > ????X-Cache: MISS from khorne
> > > ????X-Cache-Lookup: MISS from khorne:3128
> > > ????Connection: keep-alive
> > > Length: 13322 (13K) [text/html]
> > > Saving to: 'index.html'
> > > 
> > > index.html??????????100%[==================>]??13.01K --.-
> > > KB/s????in
> > > 0s
> > > 
> > > 2017-01-27 15:29:57 (32.2 MB/s) - 'index.html' saved
> > > [13322/13322]
> > > 
> > > Can you explain me - for what static index.html has this:
> > > 
> > > Cache-Control: no-cache, no-store
> > > Pragma: no-cache
> > > 
> > > ?
> > > 
> > > What can be broken to ignore CC in this page?
> > 
> > Hi Yuri,
> > 
> > 
> > Why do you think the page returned for URL
> > [https://www.microsot.cpom/r
> > u-kz/] is static and not dynamically generated one?
> 
> And for me, what's the difference? Does it change anything? In
> addition,?
> it is easy to see on the page and even the eyes - strangely enough -
> to?
> open its code. And? What do you see there?

I see an official home page of Microsoft company for KZ region. The
page is full of javascripts and products offer. It makes sense to
expect that the page could be changed intensively enough.


> > The index.html file is default file name for wget.
> 
> And also the name of the default home page in the web. Imagine - I
> know?
> the obvious things. But the question was about something else.
> > 
> > man wget:
> > ???--default-page=name
> > ????????Use name as the default file name when it isn't known
> > (i.e., for
> > ????????URLs that end in a slash), instead of index.html.
> > 
> > In fact the https://www.microsoft.com/ru-kz/index.html is a stub
> > page
> > (The page you requested cannot be found.).
> 
> You living in wrong region. This is geo-dependent page, as obvious,
> yes?

What I mean is the pages https://www.microsoft.com/ru-kz/ and https://w
ww.microsoft.com/ru-kz/index.html are not same. You can easily confirm
it.


> Again. What is the difference? I open it from different
> workstations,?
> from different browsers - I see the same thing. The code is
> identical. I?
> can is to cache? Yes or no?

I'm a new member of Squid community (about 1 year). While tracking for
community activity I found that you can't grasp the advantages of
HTTP/1.1 over HTTP/1.0 for caching systems. Especially, its ability to
_safely_ cache and serve same amount (but I believe even more) of the
objects as HTTP/1.0 compliant caches do (while not breaking internet).
The main tool of HTTP/1.1 compliant proxies is _revalidation_ process.
HTTP/1.1 compliant caches like Squid tend to cache all possible objects
but later use revalidation for dubious requests. In fact the
revalidation is not costly process, especially using conditional GET
requests.

I found that most of your complains in the mail list and Bugzilla are
related to HTTPS scheme. FYI: The primary tool (revalidation) does not
work for HTTPS scheme using all current Squid branches at the moment.
See bug 4648.

Try to apply the proposed patch and update all related bug reports.

HTH


Garri


From Antony.Stone at squid.open.source.it  Fri Jan 27 13:35:48 2017
From: Antony.Stone at squid.open.source.it (Antony Stone)
Date: Fri, 27 Jan 2017 14:35:48 +0100
Subject: [squid-users] Strange behavior - reload service failed,
	but not start....
In-Reply-To: <1485522835174-4681360.post@n4.nabble.com>
References: <1485349951724-4681317.post@n4.nabble.com>
 <33d99c9f-8c63-65c2-9cfb-bb217ba5c176@treenet.co.nz>
 <1485522835174-4681360.post@n4.nabble.com>
Message-ID: <201701271435.49269.Antony.Stone@squid.open.source.it>

On Friday 27 January 2017 at 14:13:55, erdosain9 wrote:

> Ok, thanks.
> But something more its wrong.... look up this:
> 
> [root at squid ips]# squid -k restart
> squid: ERROR: Could not send signal 21 to process 8083: (3) No such process
> 
> [root at squid ips]# squid -k shutdown
> squid: ERROR: Could not send signal 15 to process 8083: (3) No such process
> 
> [root at squid ips]# squid -k kill
> squid: ERROR: Could not send signal 9 to process 8083: (3) No such process
> 
> [root at squid ips]# squid -k debug
> squid: ERROR: Could not send signal 12 to process 8083: (3) No such process
> 
> ......mmm... what's going on here???
> 
> But actually squid is running and working,

What does ps -ax tell you the process ID for it is?

I bet it's not 8083...

> Also, if i do a change in squid.conf... it dosent take it. neither
> systemctl, or like you see any squid -k command

Sounds like a permissions problem to me - what are the ownerships and 
permissions on your squid.conf file, and on the Squid PID file?


Antony.

-- 
I want to build a machine that will be proud of me.

 - Danny Hillis, creator of The Connection Machine

                                                   Please reply to the list;
                                                         please *don't* CC me.


From erdosain9 at gmail.com  Fri Jan 27 13:36:01 2017
From: erdosain9 at gmail.com (erdosain9)
Date: Fri, 27 Jan 2017 05:36:01 -0800 (PST)
Subject: [squid-users] Strange behavior - reload service failed,
 but not start.... (solved)
In-Reply-To: <1485522835174-4681360.post@n4.nabble.com>
References: <1485349951724-4681317.post@n4.nabble.com>
 <CA+sSnVbSgDGBSzTVNbomVOKzH5oAd+vKB0JGVdjxai3rjihHYA@mail.gmail.com>
 <1485362331939-4681322.post@n4.nabble.com>
 <33d99c9f-8c63-65c2-9cfb-bb217ba5c176@treenet.co.nz>
 <1485522835174-4681360.post@n4.nabble.com>
Message-ID: <1485524161802-4681363.post@n4.nabble.com>

Hi, again.
Now, i do this

[root at squid ips]# ps aux | grep squid
root      2228  0.0  0.0 130900   344 ?        Ss   ene24   0:00
/usr/sbin/squid -sYC
squid     2230  6.2 64.9 1341864 1205160 ?     R    ene24 263:30 (squid-1)
-sYC
squid     2231  0.4  0.1  68196  1948 ?        S    ene24  20:35 (ssl_crtd)
-s /var/lib/ssl_db -M 4MB
squid     2232  0.0  0.1  68196  1944 ?        S    ene24   1:21 (ssl_crtd)
-s /var/lib/ssl_db -M 4MB
squid     2233  0.0  0.1  68196  1948 ?        S    ene24   0:32 (ssl_crtd)
-s /var/lib/ssl_db -M 4MB
squid     2234  0.0  0.1  68196  1952 ?        S    ene24   0:17 (ssl_crtd)
-s /var/lib/ssl_db -M 4MB
squid     2235  0.0  0.1  68196  1944 ?        S    ene24   0:11 (ssl_crtd)
-s /var/lib/ssl_db -M 4MB
squid     2236  0.0  0.0  33712   216 ?        S    ene24   1:48
(logfile-daemon) /var/log/squid/access.log
squid     2237  0.0  0.0  33560   220 ?        S    ene24   0:20 (unlinkd)
squid     2238  0.8  0.0  34084   484 ?        S    ene24  34:55 diskd
2283524 2283525 2283526
squid     2239  0.0  0.1  68196  1944 ?        S    ene24   0:06 (ssl_crtd)
-s /var/lib/ssl_db -M 4MB
squid     2240  0.0  0.1  68196  1944 ?        S    ene24   0:04 (ssl_crtd)
-s /var/lib/ssl_db -M 4MB
squid     2241  0.0  0.1  68196  1944 ?        S    ene24   0:02 (ssl_crtd)
-s /var/lib/ssl_db -M 4MB
squid     2242  0.0  0.1  68196  1944 ?        S    ene24   0:01 (ssl_crtd)
-s /var/lib/ssl_db -M 4MB
squid     2243  0.0  0.1  68196  1940 ?        S    ene24   0:01 (ssl_crtd)
-s /var/lib/ssl_db -M 4MB
squid     2244  0.0  0.1  68184  1932 ?        S    ene24   0:01 (ssl_crtd)
-s /var/lib/ssl_db -M 4MB
squid     2245  0.0  0.1  68196  1948 ?        S    ene24   0:01 (ssl_crtd)
-s /var/lib/ssl_db -M 4MB
squid     2246  0.0  0.1  68196  1940 ?        S    ene24   0:00 (ssl_crtd)
-s /var/lib/ssl_db -M 4MB
squid     2247  0.0  0.1  68196  1940 ?        S    ene24   0:00 (ssl_crtd)
-s /var/lib/ssl_db -M 4MB
squid     2248  0.0  0.1  68196  2076 ?        S    ene24   0:00 (ssl_crtd)
-s /var/lib/ssl_db -M 4MB
squid     2278  0.0  0.1  68196  1940 ?        S    ene24   0:00 (ssl_crtd)
-s /var/lib/ssl_db -M 4MB
squid     2325  0.0  0.1  68196  2064 ?        S    ene24   0:00 (ssl_crtd)
-s /var/lib/ssl_db -M 4MB
squid     2368  0.0  0.1  68196  1984 ?        S    ene24   0:00 (ssl_crtd)
-s /var/lib/ssl_db -M 4MB
squid     2369  0.0  0.1  68196  2168 ?        S    ene24   0:00 (ssl_crtd)
-s /var/lib/ssl_db -M 4MB
squid     2371  0.0  0.0  68152  1656 ?        S    ene24   0:00 (ssl_crtd)
-s /var/lib/ssl_db -M 4MB
squid     2397  0.0  0.1  68180  1920 ?        S    ene24   0:00 (ssl_crtd)
-s /var/lib/ssl_db -M 4MB
squid     2398  0.0  0.1  68188  1920 ?        S    ene24   0:00 (ssl_crtd)
-s /var/lib/ssl_db -M 4MB
squid     2399  0.0  0.1  68184  1924 ?        S    ene24   0:00 (ssl_crtd)
-s /var/lib/ssl_db -M 4MB
squid     2400  0.0  0.1  68184  1932 ?        S    ene24   0:00 (ssl_crtd)
-s /var/lib/ssl_db -M 4MB
squid     2401  0.0  0.1  68180  2032 ?        S    ene24   0:00 (ssl_crtd)
-s /var/lib/ssl_db -M 4MB
squid     2402  0.0  0.1  68180  2032 ?        S    ene24   0:00 (ssl_crtd)
-s /var/lib/ssl_db -M 4MB
squid     2403  0.0  0.0  68152  1648 ?        S    ene24   0:00 (ssl_crtd)
-s /var/lib/ssl_db -M 4MB
squid     2404  0.0  0.0  68152  1620 ?        S    ene24   0:00 (ssl_crtd)
-s /var/lib/ssl_db -M 4MB
squid     2405  0.0  0.0  68152  1612 ?        S    ene24   0:00 (ssl_crtd)
-s /var/lib/ssl_db -M 4MB
squid     2406  0.0  0.1  68188  1920 ?        S    ene24   0:00 (ssl_crtd)
-s /var/lib/ssl_db -M 4MB
squid     2407  0.0  0.0  68152  1612 ?        S    ene24   0:00 (ssl_crtd)
-s /var/lib/ssl_db -M 4MB
squid     2408  0.0  0.0  68152  1608 ?        S    ene24   0:00 (ssl_crtd)
-s /var/lib/ssl_db -M 4MB
root      8128  0.0  0.0 112672   972 pts/0    S+   10:24   0:00 grep
--color=auto squid
[root at squid ips]# systemctl stop squid
[root at squid ips]# pkill squid
[root at squid ips]# squid -z


And now is working, also with the command systemctl.... but, anyway you
recommend more the use of squid -k commands no??

Thanks again.

pd: this is process now. 
[root at squid ips]# ps aux | grep squid
root      8156  0.0  1.3 130900 25272 ?        Ss   10:26   0:00
/usr/sbin/squid -sYC
squid     8158  6.5 18.7 452532 347580 ?       S    10:26   0:42 (squid-1)
-sYC
squid     8165  0.0  0.0  33560  1300 ?        S    10:26   0:00 (unlinkd)
squid     8166  1.0  0.0  34084  1572 ?        S    10:26   0:06 diskd
8353796 8353797 8353798
squid     8182  0.0  0.0  33712  1304 ?        S    10:28   0:00
(logfile-daemon) /var/log/squid/access.log
squid     8183  0.5  0.2  68188  4940 ?        S    10:28   0:02 (ssl_crtd)
-s /var/lib/ssl_db -M 4MB
squid     8184  0.0  0.2  68152  4708 ?        S    10:28   0:00 (ssl_crtd)
-s /var/lib/ssl_db -M 4MB
squid     8185  0.0  0.2  68192  4936 ?        S    10:28   0:00 (ssl_crtd)
-s /var/lib/ssl_db -M 4MB
squid     8186  0.0  0.2  68152  4708 ?        S    10:28   0:00 (ssl_crtd)
-s /var/lib/ssl_db -M 4MB
squid     8187  0.0  0.2  68152  4712 ?        S    10:28   0:00 (ssl_crtd)
-s /var/lib/ssl_db -M 4MB
squid     8190  0.0  0.2  68152  4708 ?        S    10:30   0:00 (ssl_crtd)
-s /var/lib/ssl_db -M 4MB
squid     8191  0.0  0.2  68152  4712 ?        S    10:30   0:00 (ssl_crtd)
-s /var/lib/ssl_db -M 4MB
squid     8201  0.0  0.2  68152  4576 ?        S    10:35   0:00 (ssl_crtd)
-s /var/lib/ssl_db -M 4MB
squid     8202  0.0  0.2  68152  4580 ?        S    10:35   0:00 (ssl_crtd)
-s /var/lib/ssl_db -M 4MB
squid     8203  0.0  0.2  68152  4576 ?        S    10:35   0:00 (ssl_crtd)
-s /var/lib/ssl_db -M 4MB
root      8205  0.0  0.0 112668   976 pts/0    R+   10:37   0:00 grep
--color=auto squid




--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/Strange-behavior-reload-service-failed-but-not-start-tp4681317p4681363.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From Antony.Stone at squid.open.source.it  Fri Jan 27 14:01:11 2017
From: Antony.Stone at squid.open.source.it (Antony Stone)
Date: Fri, 27 Jan 2017 15:01:11 +0100
Subject: [squid-users] Strange behavior - reload service failed,
	but not start.... (solved)
In-Reply-To: <1485524161802-4681363.post@n4.nabble.com>
References: <1485349951724-4681317.post@n4.nabble.com>
 <1485522835174-4681360.post@n4.nabble.com>
 <1485524161802-4681363.post@n4.nabble.com>
Message-ID: <201701271501.11492.Antony.Stone@squid.open.source.it>

On Friday 27 January 2017 at 14:36:01, erdosain9 wrote:

> Hi, again.
> Now, i do this
> 
> [root at squid ips]# ps aux | grep squid
> root      2228  0.0  0.0 130900   344 ?        Ss   ene24   0:00
> /usr/sbin/squid -sYC

... snip ...

> [root at squid ips]# systemctl stop squid
> [root at squid ips]# pkill squid
> [root at squid ips]# squid -z
> 
> And now is working, also with the command systemctl.... but, anyway you
> recommend more the use of squid -k commands no??

Well, if you started it with systemctl / systemd, then it's a good idea to 
stop it with systemctl / systemd.

However:

On Thursday 26 January 2017 at 03:57:48, Amos Jeffries wrote:

> On 26/01/2017 5:38 a.m., erdosain9 wrote:
> 
> > some other approach??
> 
> Not using systemd to control Squid-3. The two are not compatible. As you
> just found out the hard way.
> 
> Squid is not a daemon, it is a Daemon + Manager in one binary/process.
> systemd is based around the naive assumption that everything is a simple
> daemon and gets horribly confuzled when reality bites. It is not alone,
> upstart has the same issues. Basically only start/stop work, and even
> those only most of the time if done very carefully.
> 
> Your choices with systemd are (1) use the 'squid -k' commands, or (2)
> upgrade to Squid-4 and install the tools/systemd/squid.service file we
> provide for that version.

Therefore avoid using systemd with Squid, and you should be able to manage it 
normally.


Antony.

-- 
A user interface is like a joke.
If you have to explain it, it didn't work.

                                                   Please reply to the list;
                                                         please *don't* CC me.


From chip_pop at hotmail.com  Fri Jan 27 14:15:04 2017
From: chip_pop at hotmail.com (joseph)
Date: Fri, 27 Jan 2017 06:15:04 -0800 (PST)
Subject: [squid-users] Not all html objects are being cached
In-Reply-To: <1485524146.5283.5.camel@comnet.uz>
References: <1485273173646-4681293.post@n4.nabble.com>
 <1485375724827-4681326.post@n4.nabble.com>
 <85b108fb-1f3e-d7d5-48e1-37edbb2ae2d9@gmail.com>
 <20170126204449.GB25104@fantomas.sk>
 <7fa88d69-1964-0eeb-9e5d-5f539e9a0805@gmail.com>
 <bc726969-ca5e-912b-7600-b4d4408c0c02@treenet.co.nz>
 <a9ab7ffd-8f89-b78d-df6e-bdb49cc0c27f@gmail.com>
 <1485518050.5283.2.camel@comnet.uz>
 <5b4106a3-7143-33ce-a4c8-ecb21f64f6fa@gmail.com>
 <1485524146.5283.5.camel@comnet.uz>
Message-ID: <1485526504486-4681365.post@n4.nabble.com>

hi its not about https scheme its about evrything
i decide not to involve with arg...
but why not its the last one i should say it once
they ar right most of the ppl admin have no knwoleg so its ok to baby sit
them as its
but
--enable-http-violations should be fully ignore cache control and in refresh
pattern  admin shuld control the behavior of his need else they should  take
of  ?enable-http-violations or alow us to do so
controlling the 
Pragma: no-cache     and  Cache-Control: no-cache + + ++ +
in both request reply     
and its up to us to fix broke site   since almost 80% or more from the web
admin programmer using them just to prevent caching not becaus it brake the
page
has nothing to do with old damen page that we can fix the obj to be fresh
soon all web programmer will use those control and squid will become suks
end up having cache server not being able to cache all lool soooooooo
let other admin use squid without --enable-http-violations  if they ar worry
about braking web shit bad site
and let other good admin that know wat they ar doing control wat they need
using --enable-http-violations fully open no restriction at all
https is  rarely used not everywhere can use depend on country 
bye
joseph
so as my structure i have http only and as its squid  save me only 5% from
all the http bandwith




--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/Not-all-html-objects-are-being-cached-tp4681293p4681365.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From garryd at comnet.uz  Fri Jan 27 15:05:38 2017
From: garryd at comnet.uz (Garri Djavadyan)
Date: Fri, 27 Jan 2017 20:05:38 +0500
Subject: [squid-users] Not all html objects are being cached
In-Reply-To: <1485526504486-4681365.post@n4.nabble.com>
References: <1485273173646-4681293.post@n4.nabble.com>
 <1485375724827-4681326.post@n4.nabble.com>
 <85b108fb-1f3e-d7d5-48e1-37edbb2ae2d9@gmail.com>
 <20170126204449.GB25104@fantomas.sk>
 <7fa88d69-1964-0eeb-9e5d-5f539e9a0805@gmail.com>
 <bc726969-ca5e-912b-7600-b4d4408c0c02@treenet.co.nz>
 <a9ab7ffd-8f89-b78d-df6e-bdb49cc0c27f@gmail.com>
 <1485518050.5283.2.camel@comnet.uz>
 <5b4106a3-7143-33ce-a4c8-ecb21f64f6fa@gmail.com>
 <1485524146.5283.5.camel@comnet.uz>
 <1485526504486-4681365.post@n4.nabble.com>
Message-ID: <1485529538.5283.7.camel@comnet.uz>

On Fri, 2017-01-27 at 06:15 -0800, joseph wrote:
> hi its not about https scheme its about evrything

Hi,

First of all, I can't brag about my English and writing style, but your
writing style is _very_ offensive to other members. Please, try it
better. First of all, it is very difficult to catch the idea of many
sentences. I believe punctuation marks could help a lot. Thanks in
advance.

> i decide not to involve with arg...
> but why not its the last one i should say it once
> they ar right most of the ppl admin have no knwoleg so its ok to baby
> sit
> them as its
> but
> --enable-http-violations should be fully ignore cache control and in
> refresh
> pattern??admin shuld control the behavior of his need else they
> should??take
> of???enable-http-violations or alow us to do so
> controlling the?
> Pragma: no-cache?????and??Cache-Control: no-cache + + ++ +
> in both request reply

Squid, as HTTP/1.1 compliant cache successfully caches and serves
CC:no-cache replies. Below is excerpt from the RFC7234:

5.2.2.2.??no-cache

???The "no-cache" response directive indicates that the response MUST
???NOT be used to satisfy a subsequent request without successful
???validation on the origin server.

The key word is _validation_. There is nothing bad with revalidation.
It is inexpensive but saves us from possible problems. The log entry
'TCP_REFRESH_UNMODIFIED' should be welcomed as TCP_HIT or TCP_MEM_HIT.

Example:

$ curl -v -s -x http://127.0.0.1:3128 http://sandbox.comnet.local/test.
bin >/dev/null

< HTTP/1.1 200 OK
< Last-Modified: Wed, 31 Aug 2016 19:00:00 GMT
< Accept-Ranges: bytes
< Content-Length: 262146
< Content-Type: application/octet-stream
< Expires: Thu, 01 Dec 1994 16:00:00 GMT
< Date: Fri, 27 Jan 2017 14:55:09 GMT
< Server: Apache
< ETag: "ea0cd5-40002-53b62b438ac00"
< Cache-Control: no-cache
< Age: 3
< X-Cache: HIT from gentoo.comnet.uz
< Via: 1.1 gentoo.comnet.uz (squid/3.5.23-BZR)
< Connection: keep-alive

1485528912.222?????18 127.0.0.1 TCP_REFRESH_UNMODIFIED/200 262565 GET h
ttp://sandbox.comnet.local/test.bin - HIER_DIRECT/192.168.24.5
application/octet-stream


As you can see, there are no problems with the no-cache reply.


I advise you to consider every specific case where you believe Squid's
transition to HTTP/1.1 compliance restricts you to cache something.


Garri


From yvoinov at gmail.com  Fri Jan 27 16:45:06 2017
From: yvoinov at gmail.com (Yuri Voinov)
Date: Fri, 27 Jan 2017 22:45:06 +0600
Subject: [squid-users] Not all html objects are being cached
In-Reply-To: <1485524146.5283.5.camel@comnet.uz>
References: <1485273173646-4681293.post@n4.nabble.com>
 <1485375724827-4681326.post@n4.nabble.com>
 <85b108fb-1f3e-d7d5-48e1-37edbb2ae2d9@gmail.com>
 <20170126204449.GB25104@fantomas.sk>
 <7fa88d69-1964-0eeb-9e5d-5f539e9a0805@gmail.com>
 <bc726969-ca5e-912b-7600-b4d4408c0c02@treenet.co.nz>
 <a9ab7ffd-8f89-b78d-df6e-bdb49cc0c27f@gmail.com>
 <1485518050.5283.2.camel@comnet.uz>
 <5b4106a3-7143-33ce-a4c8-ecb21f64f6fa@gmail.com>
 <1485524146.5283.5.camel@comnet.uz>
Message-ID: <fe6dc292-bc76-f1d8-70fd-1ab3d9c5074e@gmail.com>



27.01.2017 19:35, Garri Djavadyan ?????:
> On Fri, 2017-01-27 at 17:58 +0600, Yuri wrote:
>> 27.01.2017 17:54, Garri Djavadyan ?????:
>>> On Fri, 2017-01-27 at 15:47 +0600, Yuri wrote:
>>>> --2017-01-27 15:29:54--  https://www.microsoft.com/ru-kz/
>>>> Connecting to 127.0.0.1:3128... connected.
>>>> Proxy request sent, awaiting response...
>>>>     HTTP/1.1 200 OK
>>>>     Cache-Control: no-cache, no-store
>>>>     Pragma: no-cache
>>>>     Content-Type: text/html
>>>>     Expires: -1
>>>>     Server: Microsoft-IIS/8.0
>>>>     CorrelationVector: BzssVwiBIUaXqyOh.1.1
>>>>     X-AspNet-Version: 4.0.30319
>>>>     X-Powered-By: ASP.NET
>>>>     Access-Control-Allow-Headers: Origin, X-Requested-With,
>>>> Content-
>>>> Type,
>>>> Accept
>>>>     Access-Control-Allow-Methods: GET, POST, PUT, DELETE, OPTIONS
>>>>     Access-Control-Allow-Credentials: true
>>>>     P3P: CP="ALL IND DSP COR ADM CONo CUR CUSo IVAo IVDo PSA PSD
>>>> TAI
>>>> TELo
>>>> OUR SAMo CNT COM INT NAV ONL PHY PRE PUR UNI"
>>>>     X-Frame-Options: SAMEORIGIN
>>>>     Vary: Accept-Encoding
>>>>     Content-Encoding: gzip
>>>>     Date: Fri, 27 Jan 2017 09:29:56 GMT
>>>>     Content-Length: 13322
>>>>     Set-Cookie: MS-CV=BzssVwiBIUaXqyOh.1; domain=.microsoft.com;
>>>> expires=Sat, 28-Jan-2017 09:29:56 GMT; path=/
>>>>     Set-Cookie: MS-CV=BzssVwiBIUaXqyOh.2; domain=.microsoft.com;
>>>> expires=Sat, 28-Jan-2017 09:29:56 GMT; path=/
>>>>     Strict-Transport-Security: max-age=0; includeSubDomains
>>>>     X-CCC: NL
>>>>     X-CID: 2
>>>>     X-Cache: MISS from khorne
>>>>     X-Cache-Lookup: MISS from khorne:3128
>>>>     Connection: keep-alive
>>>> Length: 13322 (13K) [text/html]
>>>> Saving to: 'index.html'
>>>>
>>>> index.html          100%[==================>]  13.01K --.-
>>>> KB/s    in
>>>> 0s
>>>>
>>>> 2017-01-27 15:29:57 (32.2 MB/s) - 'index.html' saved
>>>> [13322/13322]
>>>>
>>>> Can you explain me - for what static index.html has this:
>>>>
>>>> Cache-Control: no-cache, no-store
>>>> Pragma: no-cache
>>>>
>>>> ?
>>>>
>>>> What can be broken to ignore CC in this page?
>>> Hi Yuri,
>>>
>>>
>>> Why do you think the page returned for URL
>>> [https://www.microsot.cpom/r
>>> u-kz/] is static and not dynamically generated one?
>> And for me, what's the difference? Does it change anything? In
>> addition, 
>> it is easy to see on the page and even the eyes - strangely enough -
>> to 
>> open its code. And? What do you see there?
> I see an official home page of Microsoft company for KZ region. The
> page is full of javascripts and products offer. It makes sense to
> expect that the page could be changed intensively enough.
In essence, the question is, what to say? In addition to the general
discussion of particulars or examples? As I said - this is just one
example. A lot of them. And I think sometimes it's better to chew than talk.
>
>
>>> The index.html file is default file name for wget.
>> And also the name of the default home page in the web. Imagine - I
>> know 
>> the obvious things. But the question was about something else.
>>> man wget:
>>>    --default-page=name
>>>         Use name as the default file name when it isn't known
>>> (i.e., for
>>>         URLs that end in a slash), instead of index.html.
>>>
>>> In fact the https://www.microsoft.com/ru-kz/index.html is a stub
>>> page
>>> (The page you requested cannot be found.).
>> You living in wrong region. This is geo-dependent page, as obvious,
>> yes?
> What I mean is the pages https://www.microsoft.com/ru-kz/ and https://w
> ww.microsoft.com/ru-kz/index.html are not same. You can easily confirm
> it.
>
>
>> Again. What is the difference? I open it from different
>> workstations, 
>> from different browsers - I see the same thing. The code is
>> identical. I 
>> can is to cache? Yes or no?
> I'm a new member of Squid community (about 1 year). While tracking for
> community activity I found that you can't grasp the advantages of
> HTTP/1.1 over HTTP/1.0 for caching systems. Especially, its ability to
> _safely_ cache and serve same amount (but I believe even more) of the
> objects as HTTP/1.0 compliant caches do (while not breaking internet).
> The main tool of HTTP/1.1 compliant proxies is _revalidation_ process.
> HTTP/1.1 compliant caches like Squid tend to cache all possible objects
> but later use revalidation for dubious requests. In fact the
> revalidation is not costly process, especially using conditional GET
> requests.
Nuff said. Let's stop waste time. Take a look on attachement.
>
> I found that most of your complains in the mail list and Bugzilla are
> related to HTTPS scheme. FYI: The primary tool (revalidation) does not
> work for HTTPS scheme using all current Squid branches at the moment.
> See bug 4648.
Forgot about it. Now I've solved all of my problems.
>
> Try to apply the proposed patch and update all related bug reports.
I have no unresolved problems with caching. For me personally, this
debate - only of academic interest. You can continue to spend their time
in the reasoning, whose side take - smart or beautiful. Let me leave
you, I have what to do with my free time.
>
> HTH
>
>
> Garri
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users

-- 
Bugs to the Future
-------------- next part --------------
A non-text attachment was scrubbed...
Name: 861wNcx.png
Type: image/png
Size: 10152 bytes
Desc: not available
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20170127/5b4d4105/attachment.png>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: 0x613DEC46.asc
Type: application/pgp-keys
Size: 2437 bytes
Desc: not available
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20170127/5b4d4105/attachment.key>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: signature.asc
Type: application/pgp-signature
Size: 473 bytes
Desc: OpenPGP digital signature
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20170127/5b4d4105/attachment.sig>

From chip_pop at hotmail.com  Fri Jan 27 16:53:18 2017
From: chip_pop at hotmail.com (joseph)
Date: Fri, 27 Jan 2017 08:53:18 -0800 (PST)
Subject: [squid-users] Not all html objects are being cached
In-Reply-To: <1485529538.5283.7.camel@comnet.uz>
References: <85b108fb-1f3e-d7d5-48e1-37edbb2ae2d9@gmail.com>
 <20170126204449.GB25104@fantomas.sk>
 <7fa88d69-1964-0eeb-9e5d-5f539e9a0805@gmail.com>
 <bc726969-ca5e-912b-7600-b4d4408c0c02@treenet.co.nz>
 <a9ab7ffd-8f89-b78d-df6e-bdb49cc0c27f@gmail.com>
 <1485518050.5283.2.camel@comnet.uz>
 <5b4106a3-7143-33ce-a4c8-ecb21f64f6fa@gmail.com>
 <1485524146.5283.5.camel@comnet.uz>
 <1485526504486-4681365.post@n4.nabble.com>
 <1485529538.5283.7.camel@comnet.uz>
Message-ID: <1485535998164-4681368.post@n4.nabble.com>

im not here to fight dont mention RFC caus its alredy violating RFC just
using enable-http-violations
pls re read my post or get someone to translate the structure of it 
else no benefit explaining or protecting RFC shit
so pls careful reading my point of view else waisting time with one year
experienced guy
bye folx





--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/Not-all-html-objects-are-being-cached-tp4681293p4681368.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From charlie at charlie.is  Fri Jan 27 18:24:17 2017
From: charlie at charlie.is (Charlie Orford)
Date: Fri, 27 Jan 2017 18:24:17 +0000
Subject: [squid-users] Clarity on sending intercepted HTTPS traffic upstream
	to a cache_peer
Message-ID: <43c4ec96-c100-149e-d2c2-78cceb4ce530@charlie.is>

Hi list

We're using squid 3.5.23 and trying to achieve the following:

client https request (not proxy aware) -> squid1 (https NAT intercept) 
-> upstream squid2 (configured as a cache_peer in squid1) -> origin 
server (e.g. www.google.com)

Amos mentioned in this thread 
http://lists.squid-cache.org/pipermail/squid-users/2016-March/009468.html 
that:

 > Squid can:
 >
 >  A) relay CONNECT message from client to any upstream proxy.
 >
 >  B) generate CONNECT message on arriving intercepted HTTPS and relay
 > that to upstream proxy *IF* (and only if) ssl_bump selects the 'splice'
 > action.
 >
 >  C) relay https:// URLs to an upstream TLS proxy.
 >
 >
 > That is all at present.
 >
 > Squid cannot (yet) generate CONNECT messages to try and fetch TLS
 > details via a non-TLS cache_peer. If you are able to sponsor that
 > enhancement work patches are welcome, or sponsorship $$ to help pay
 > persons working on these things (Christos / measurement-factory) are
 > also welcome.

Option B seems to cover what we need i.e. squid1 wraps arriving 
intercepted HTTPS traffic in a CONNECT and sends it upstream to squid2 
which in turn tunnels it to the origin server. Unfortunately, we can't 
get it to work: as soon as squid1 receives a client HTTPS request it 
exits with "assertion failed: PeerConnector.cc:116: "peer->use_ssl"" in 
cache.log

Relevant config for squid1:
######################################
acl localnet src 10.100.0.0/24
acl SSL_ports port 443
acl Safe_ports port 80          # http
acl Safe_ports port 443         # https
acl Safe_ports port 777         # multiling http
acl CONNECT method CONNECT
acl Blocked_domains dstdomain "/etc/squid3/blocked.domains.acl"
acl step1 at_step SslBump1
acl step2 at_step SslBump2
acl step3 at_step SslBump3
acl MITM_TRAFFIC myportname MITM_port

http_access allow manager localhost
http_access deny manager
http_access deny !Safe_ports
http_access deny CONNECT !SSL_ports
http_access deny to_localhost
http_access deny Blocked_domains
http_access allow localhost
http_access allow localnet
http_access deny all
http_reply_access allow all

https_port 8443 ssl-bump intercept cert=/etc/squid3/root_ca.combined.pem 
generate-host-certificates=on dynamic_cert_mem_cache_size=8MB 
name=MITM_port
sslcrtd_program /usr/lib/squid3/ssl_crtd -s /var/lib/squid3/ssl_db -M 4MB

ssl_bump peek all
ssl_bump splice all

nonhierarchical_direct off
never_direct allow all
prefer_direct off
cache_peer 192.168.0.1 parent 3128 0 no-query no-digest 
no-netdb-exchange name=WWW_GATEWAY


Relevant config for squid2:
######################################
acl localnet src 192.168.0.0/24
acl SSL_ports port 443
acl Safe_ports port 80          # http
acl Safe_ports port 443         # https
acl Safe_ports port 777         # multiling http
acl CONNECT method CONNECT

http_port 3128

http_access allow manager localhost
http_access deny manager
http_access deny !Safe_ports
http_access deny CONNECT !SSL_ports
http_access deny to_localhost
http_access allow localnet
http_access deny all

http_reply_access allow all


Is what we want to do currently achievable with the latest 3.5 branch or 
have we misunderstood what Amos was stating (some of his posts about ssl 
interception and cache_peer support can be fairly cryptic)?

If it is achievable, does the cache_peer link itself also need to be 
encrypted (via the ssl flag) to make it work?

Thanks,
Charlie



-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20170127/de795d81/attachment.htm>

From charlie at charlie.is  Fri Jan 27 23:04:45 2017
From: charlie at charlie.is (Charlie Orford)
Date: Fri, 27 Jan 2017 23:04:45 +0000
Subject: [squid-users] Clarity on sending intercepted HTTPS traffic
 upstream to a cache_peer
In-Reply-To: <43c4ec96-c100-149e-d2c2-78cceb4ce530@charlie.is>
References: <43c4ec96-c100-149e-d2c2-78cceb4ce530@charlie.is>
Message-ID: <69ed2073-eb71-c535-fb44-9911f9c35973@charlie.is>

To follow up:

Adding ssl to the cache_peer directive on squid1 (and changing squid2 so 
it listens for connections on an https_port) gets us a little further 
but still doesn't work.

Clients get a SQUID_X509_V_ERR_DOMAIN_MISMATCH error (because the 
auto-generated cert squid1 gives to the client contains the domain of 
the cache_peer *not* the ultimate origin server).

The above is with the following ssl_bump directives set in squid1's config:

ssl_bump peek step1
ssl_bump peek step2
ssl_bump splice step3

A post from another user on this list seems to suggest they successfully 
got squid to do what we want 
(http://lists.squid-cache.org/pipermail/squid-users/2015-November/007955.html) 
but when emulating their setup (i.e. peeking at step1, staring at step2 
and then bumping at step3) we get the same 
SQUID_X509_V_ERR_DOMAIN_MISMATCH error.

Setting sslflags=DONT_VERIFY_DOMAIN on the cache_peer directive has no 
effect.

Connecting to squid1 with a proxy aware client (on a standard http_port 
with the ssl-bump flag set but no intercept) also results in the same 
problem.

Where are we going wrong?

Charlie

On 27/01/2017 18:24, Charlie Orford wrote:
> Hi list
>
> We're using squid 3.5.23 and trying to achieve the following:
>
> client https request (not proxy aware) -> squid1 (https NAT intercept) 
> -> upstream squid2 (configured as a cache_peer in squid1) -> origin 
> server (e.g. www.google.com)
>
> Amos mentioned in this thread 
> http://lists.squid-cache.org/pipermail/squid-users/2016-March/009468.html 
> that:
>
> > Squid can:
> >
> >  A) relay CONNECT message from client to any upstream proxy.
> >
> >  B) generate CONNECT message on arriving intercepted HTTPS and relay
> > that to upstream proxy *IF* (and only if) ssl_bump selects the 'splice'
> > action.
> >
> >  C) relay https:// URLs to an upstream TLS proxy.
> >
> >
> > That is all at present.
> >
> > Squid cannot (yet) generate CONNECT messages to try and fetch TLS
> > details via a non-TLS cache_peer. If you are able to sponsor that
> > enhancement work patches are welcome, or sponsorship $$ to help pay
> > persons working on these things (Christos / measurement-factory) are
> > also welcome.
>
> Option B seems to cover what we need i.e. squid1 wraps arriving 
> intercepted HTTPS traffic in a CONNECT and sends it upstream to squid2 
> which in turn tunnels it to the origin server. Unfortunately, we can't 
> get it to work: as soon as squid1 receives a client HTTPS request it 
> exits with "assertion failed: PeerConnector.cc:116: "peer->use_ssl"" 
> in cache.log
>
> Relevant config for squid1:
> ######################################
> acl localnet src 10.100.0.0/24
> acl SSL_ports port 443
> acl Safe_ports port 80          # http
> acl Safe_ports port 443         # https
> acl Safe_ports port 777         # multiling http
> acl CONNECT method CONNECT
> acl Blocked_domains dstdomain "/etc/squid3/blocked.domains.acl"
> acl step1 at_step SslBump1
> acl step2 at_step SslBump2
> acl step3 at_step SslBump3
> acl MITM_TRAFFIC myportname MITM_port
>
> http_access allow manager localhost
> http_access deny manager
> http_access deny !Safe_ports
> http_access deny CONNECT !SSL_ports
> http_access deny to_localhost
> http_access deny Blocked_domains
> http_access allow localhost
> http_access allow localnet
> http_access deny all
> http_reply_access allow all
>
> https_port 8443 ssl-bump intercept 
> cert=/etc/squid3/root_ca.combined.pem generate-host-certificates=on 
> dynamic_cert_mem_cache_size=8MB name=MITM_port
> sslcrtd_program /usr/lib/squid3/ssl_crtd -s /var/lib/squid3/ssl_db -M 4MB
>
> ssl_bump peek all
> ssl_bump splice all
>
> nonhierarchical_direct off
> never_direct allow all
> prefer_direct off
> cache_peer 192.168.0.1 parent 3128 0 no-query no-digest 
> no-netdb-exchange name=WWW_GATEWAY
>
>
> Relevant config for squid2:
> ######################################
> acl localnet src 192.168.0.0/24
> acl SSL_ports port 443
> acl Safe_ports port 80          # http
> acl Safe_ports port 443         # https
> acl Safe_ports port 777         # multiling http
> acl CONNECT method CONNECT
>
> http_port 3128
>
> http_access allow manager localhost
> http_access deny manager
> http_access deny !Safe_ports
> http_access deny CONNECT !SSL_ports
> http_access deny to_localhost
> http_access allow localnet
> http_access deny all
>
> http_reply_access allow all
>
>
> Is what we want to do currently achievable with the latest 3.5 branch 
> or have we misunderstood what Amos was stating (some of his posts 
> about ssl interception and cache_peer support can be fairly cryptic)?
>
> If it is achievable, does the cache_peer link itself also need to be 
> encrypted (via the ssl flag) to make it work?
>
> Thanks,
> Charlie
>
>
>
>
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20170127/2c9ea6e6/attachment.htm>

From rousskov at measurement-factory.com  Fri Jan 27 23:43:04 2017
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Fri, 27 Jan 2017 16:43:04 -0700
Subject: [squid-users] Clarity on sending intercepted HTTPS traffic
 upstream to a cache_peer
In-Reply-To: <69ed2073-eb71-c535-fb44-9911f9c35973@charlie.is>
References: <43c4ec96-c100-149e-d2c2-78cceb4ce530@charlie.is>
 <69ed2073-eb71-c535-fb44-9911f9c35973@charlie.is>
Message-ID: <5cee91b5-3ca7-eb45-07f1-5152233ee58b@measurement-factory.com>

On 01/27/2017 04:04 PM, Charlie Orford wrote:

> Clients get a SQUID_X509_V_ERR_DOMAIN_MISMATCH error (because the
> auto-generated cert squid1 gives to the client contains the domain of
> the cache_peer *not* the ultimate origin server).

Under normal circumstances, Squid should generate no certificates in
your setup AFAICT.


> The above is with the following ssl_bump directives set in squid1's config:
> 
> ssl_bump peek step1
> ssl_bump peek step2
> ssl_bump splice step3

In other words:

  ssl_bump peek all
  ssl_bump splice all

Why not just do this instead:

  ssl_bump splice all

I have not tested this, but I do not understand why you want a
three-step SslBump to blindly forward an SSL connection to a peer. I
would use the minimal number of steps possible: one if it works or two
if I have to because of some Squid bugs/missing features.

When everything goes OK, Squid should generate no certificates in either
case, but with three-step SslBump, there are a lot more opportunities
for Squid to detect problems and want to send an error response to the
client. To send an error message, Squid bumps the connection to the
client (which does require fake certificate generation).

Finally, I do not know whether Squid is capable of peeking at the origin
server through a peer, but I doubt it is. If my guess is correct, then
three-step splicing will not work for you because during step3 your
Squid will already be talking to the origin server rather than a
cache_peer (or will fail because it cannot do that).


> A post from another user on this list seems to suggest they successfully
> got squid to do what we want
> (http://lists.squid-cache.org/pipermail/squid-users/2015-November/007955.html)
> but when emulating their setup (i.e. peeking at step1, staring at step2
> and then bumping at step3) we get the same
> SQUID_X509_V_ERR_DOMAIN_MISMATCH error.

I suggest the following order:

  1. Decide whether your Squid should bump or splice.
  2. Find the configuration that does what you decided in #1.

So far, you have given no reasons to warrant bumping so I assume you do
not need or want to bump anything. Thus, you should ignore any
configurations that contain "stare", "bump", or deprecated "*-first"
ssl_bump actions.


HTH,

Alex.


> On 27/01/2017 18:24, Charlie Orford wrote:
>> Hi list
>>
>> We're using squid 3.5.23 and trying to achieve the following:
>>
>> client https request (not proxy aware) -> squid1 (https NAT intercept)
>> -> upstream squid2 (configured as a cache_peer in squid1) -> origin
>> server (e.g. www.google.com)
>>
>> Amos mentioned in this thread
>> http://lists.squid-cache.org/pipermail/squid-users/2016-March/009468.html
>> that:
>>
>> > Squid can:
>> >
>> >  A) relay CONNECT message from client to any upstream proxy.
>> >
>> >  B) generate CONNECT message on arriving intercepted HTTPS and relay
>> > that to upstream proxy *IF* (and only if) ssl_bump selects the 'splice'
>> > action.
>> >
>> >  C) relay https:// URLs to an upstream TLS proxy.
>> >
>> >
>> > That is all at present.
>> >
>> > Squid cannot (yet) generate CONNECT messages to try and fetch TLS
>> > details via a non-TLS cache_peer. If you are able to sponsor that
>> > enhancement work patches are welcome, or sponsorship $$ to help pay
>> > persons working on these things (Christos / measurement-factory) are
>> > also welcome.
>>
>> Option B seems to cover what we need i.e. squid1 wraps arriving
>> intercepted HTTPS traffic in a CONNECT and sends it upstream to squid2
>> which in turn tunnels it to the origin server. Unfortunately, we can't
>> get it to work: as soon as squid1 receives a client HTTPS request it
>> exits with "assertion failed: PeerConnector.cc:116: "peer->use_ssl""
>> in cache.log
>>
>> Relevant config for squid1:
>> ######################################
>> acl localnet src 10.100.0.0/24
>> acl SSL_ports port 443
>> acl Safe_ports port 80          # http
>> acl Safe_ports port 443         # https
>> acl Safe_ports port 777         # multiling http
>> acl CONNECT method CONNECT
>> acl Blocked_domains dstdomain "/etc/squid3/blocked.domains.acl"
>> acl step1 at_step SslBump1
>> acl step2 at_step SslBump2
>> acl step3 at_step SslBump3
>> acl MITM_TRAFFIC myportname MITM_port
>>
>> http_access allow manager localhost
>> http_access deny manager
>> http_access deny !Safe_ports
>> http_access deny CONNECT !SSL_ports
>> http_access deny to_localhost
>> http_access deny Blocked_domains
>> http_access allow localhost
>> http_access allow localnet
>> http_access deny all
>> http_reply_access allow all
>>
>> https_port 8443 ssl-bump intercept
>> cert=/etc/squid3/root_ca.combined.pem generate-host-certificates=on
>> dynamic_cert_mem_cache_size=8MB name=MITM_port
>> sslcrtd_program /usr/lib/squid3/ssl_crtd -s /var/lib/squid3/ssl_db -M 4MB
>>
>> ssl_bump peek all
>> ssl_bump splice all
>>
>> nonhierarchical_direct off
>> never_direct allow all
>> prefer_direct off
>> cache_peer 192.168.0.1 parent 3128 0 no-query no-digest
>> no-netdb-exchange name=WWW_GATEWAY
>>
>>
>> Relevant config for squid2:
>> ######################################
>> acl localnet src 192.168.0.0/24
>> acl SSL_ports port 443
>> acl Safe_ports port 80          # http
>> acl Safe_ports port 443         # https
>> acl Safe_ports port 777         # multiling http
>> acl CONNECT method CONNECT
>>
>> http_port 3128
>>
>> http_access allow manager localhost
>> http_access deny manager
>> http_access deny !Safe_ports
>> http_access deny CONNECT !SSL_ports
>> http_access deny to_localhost
>> http_access allow localnet
>> http_access deny all
>>
>> http_reply_access allow all
>>
>>
>> Is what we want to do currently achievable with the latest 3.5 branch
>> or have we misunderstood what Amos was stating (some of his posts
>> about ssl interception and cache_peer support can be fairly cryptic)?
>>
>> If it is achievable, does the cache_peer link itself also need to be
>> encrypted (via the ssl flag) to make it work?
>>
>> Thanks,
>> Charlie
>>
>>
>>
>>
>>
>> _______________________________________________
>> squid-users mailing list
>> squid-users at lists.squid-cache.org
>> http://lists.squid-cache.org/listinfo/squid-users
> 
> 
> 
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
> 



From charlie at charlie.is  Sat Jan 28 00:32:44 2017
From: charlie at charlie.is (Charlie Orford)
Date: Sat, 28 Jan 2017 00:32:44 +0000
Subject: [squid-users] Clarity on sending intercepted HTTPS traffic
 upstream to a cache_peer
In-Reply-To: <5cee91b5-3ca7-eb45-07f1-5152233ee58b@measurement-factory.com>
References: <43c4ec96-c100-149e-d2c2-78cceb4ce530@charlie.is>
 <69ed2073-eb71-c535-fb44-9911f9c35973@charlie.is>
 <5cee91b5-3ca7-eb45-07f1-5152233ee58b@measurement-factory.com>
Message-ID: <da435a9f-904a-b438-4fa5-309a1671b3bb@charlie.is>

On 27/01/2017 23:43, Alex Rousskov wrote:
> On 01/27/2017 04:04 PM, Charlie Orford wrote:
>> A post from another user on this list seems to suggest they successfully
>> got squid to do what we want
>> (http://lists.squid-cache.org/pipermail/squid-users/2015-November/007955.html)
>> but when emulating their setup (i.e. peeking at step1, staring at step2
>> and then bumping at step3) we get the same
>> SQUID_X509_V_ERR_DOMAIN_MISMATCH error.
> I suggest the following order:
>
>    1. Decide whether your Squid should bump or splice.
>    2. Find the configuration that does what you decided in #1.
>
> So far, you have given no reasons to warrant bumping so I assume you do
> not need or want to bump anything. Thus, you should ignore any
> configurations that contain "stare", "bump", or deprecated "*-first"
> ssl_bump actions.

Sorry if my original intent wasn't clear. Obviously it makes no sense 
intercepting ssl traffic if we're going to splice everything.

Our design goal is: intercept and bump local client https traffic on 
squid1 (so we can filter certain urls, cache content etc.) and then 
forward the request on to the origin server via an upstream squid2 
(which has internet access).

The user who posted 
http://lists.squid-cache.org/pipermail/squid-users/2015-November/007955.html 
seems to have successfully done this but I can't replicate it. After 
doing a lot of googling (and semi-successfully trying to interpret Amos' 
various replies whenever bumping and cache_peers come up on this list) 
I'm beginning to wonder if it is indeed possible or if that user simple 
mistook what he was seeing when he posted that message (e.g. didn't 
notice that squid was actually not bumping his client connections).

Charlie











From squid3 at treenet.co.nz  Sat Jan 28 02:52:27 2017
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sat, 28 Jan 2017 15:52:27 +1300
Subject: [squid-users] Clarity on sending intercepted HTTPS traffic
 upstream to a cache_peer
In-Reply-To: <da435a9f-904a-b438-4fa5-309a1671b3bb@charlie.is>
References: <43c4ec96-c100-149e-d2c2-78cceb4ce530@charlie.is>
 <69ed2073-eb71-c535-fb44-9911f9c35973@charlie.is>
 <5cee91b5-3ca7-eb45-07f1-5152233ee58b@measurement-factory.com>
 <da435a9f-904a-b438-4fa5-309a1671b3bb@charlie.is>
Message-ID: <cf621900-5af7-7c2c-6122-5a28ba0349a0@treenet.co.nz>

On 28/01/2017 1:32 p.m., Charlie Orford wrote:
> On 27/01/2017 23:43, Alex Rousskov wrote:
>> On 01/27/2017 04:04 PM, Charlie Orford wrote:
>>> A post from another user on this list seems to suggest they successfully
>>> got squid to do what we want
>>> (http://lists.squid-cache.org/pipermail/squid-users/2015-November/007955.html)
>>>
>>> but when emulating their setup (i.e. peeking at step1, staring at step2
>>> and then bumping at step3) we get the same
>>> SQUID_X509_V_ERR_DOMAIN_MISMATCH error.
>> I suggest the following order:
>>
>>    1. Decide whether your Squid should bump or splice.
>>    2. Find the configuration that does what you decided in #1.
>>
>> So far, you have given no reasons to warrant bumping so I assume you do
>> not need or want to bump anything. Thus, you should ignore any
>> configurations that contain "stare", "bump", or deprecated "*-first"
>> ssl_bump actions.
> 
> Sorry if my original intent wasn't clear. Obviously it makes no sense
> intercepting ssl traffic if we're going to splice everything.
> 
> Our design goal is: intercept and bump local client https traffic on
> squid1 (so we can filter certain urls, cache content etc.) and then
> forward the request on to the origin server via an upstream squid2
> (which has internet access).

Under a narrow set of splice conditions you can get traffic through the
2-proxy heirarchy. But that is a very limited set of circumstances and
definitely not working with 'bump' anywhere involved.

As Alex pointed out step3 also eliminates the CONNECT ability. Which I
was not aware of a year ago when I wrote that original email you referenced.


The problem is that *any* server or peer TLS communication prior to
deciding to splice eliminates the ability to use a fake-CONNECT. That is
absolute because *all* TLS server/peer communication has to go through
the CONNECT tunnel - or none can. Anything happening prior to its
existence wont be TLS authenticated with the origin server.

> 
> The user who posted
> http://lists.squid-cache.org/pipermail/squid-users/2015-November/007955.html
> seems to have successfully done this but I can't replicate it. After

They did not because Squid _cannot_ do it.

Note that their cache_peer has 'ssl' flag enabled. So their transparent
traffic is using the peer certificate to base the auto-generated certs
on. Which you already tried and decided was not workable for you.

Amos



From chip_pop at hotmail.com  Sat Jan 28 11:10:55 2017
From: chip_pop at hotmail.com (joseph)
Date: Sat, 28 Jan 2017 03:10:55 -0800 (PST)
Subject: [squid-users] re squid V5
Message-ID: <1485601854857-4681374.post@n4.nabble.com>

is this patch in correct place :)
http://www.squid-cache.org/Versions/v5/changesets/squid-5-15021.patch

------------------------------------------------------------
revno: 15021
revision-id: squid3 at treenet.co.nz-20170128033532-yuuv6hosxtsh040f
parent: chtsanti at users.sourceforge.net-20170126162230-ukt52s3ms42vlji2
committer: Amos Jeffries <squid3 at treenet.co.nz>
branch nick: 5
timestamp: Sat 2017-01-28 16:35:32 +1300
message:
  Prep for 3.5.24
------------------------------------------------------------
# Bazaar merge directive format 2 (Bazaar 0.90)
# revision_id: squid3 at treenet.co.nz-20170128033532-yuuv6hosxtsh040f
# target_branch: http://bzr.squid-cache.org/bzr/squid3/5
# testament_sha1: d58152c5ba6a3a12a3ff9c8660b33e4984a97c0e
# timestamp: 2017-01-28 03:52:23 +0000
# source_branch: http://bzr.squid-cache.org/bzr/squid3/5
# base_revision_id: chtsanti at users.sourceforge.net-20170126162230-\
#   ukt52s3ms42vlji2
# 
# Begin patch
=== modified file 'ChangeLog'
--- ChangeLog	2016-12-19 14:10:52 +0000
+++ ChangeLog	2017-01-28 03:35:32 +0000
@@ -255,6 +255,17 @@
 	- ... and many documentation changes
 	- ... and much code cleanup and polishing
 
+Changes to squid-3.5.24 (28 Jan 2017):
+
+	- Regression Bug 3940: Make 'cache deny' do what is documented
+	- TLS: Fix SSLv2 records bumping despite a matching step2 peek rule
+	- TLS: Mitigate DoS attacks that use client-initiated SSL/TLS
renegotiation
+	- Fix "Source and destination overlap in memcpy" Valgrind errors
+	- Reduce crashes due to unexpected ClientHttpRequest termination
+	- Update External ACL helpers error handling and caching
+	- Detect HTTP header ACL issues
+	- ... and some documentation fixes
+
 Changes to squid-3.5.23 (16 Dec 2016):
 
 	- Bug 4627: fix generate-host-certificates and dynamic_cert_mem_cache_size
docs

=== modified file 'doc/release-notes/release-3.5.sgml'
--- doc/release-notes/release-3.5.sgml	2017-01-01 00:12:22 +0000
+++ doc/release-notes/release-3.5.sgml	2017-01-28 03:35:32 +0000
@@ -1,6 +1,6 @@
 <!doctype linuxdoc system>
 <article>
-<title>Squid 3.5.23 release notes</title>
+<title>Squid 3.5.24 release notes</title>
 <author>Squid Developers</author>
 
 <abstract>
@@ -13,7 +13,7 @@
 
 <sect>Notice
 <p>
-The Squid Team are pleased to announce the release of Squid-3.5.23.
+The Squid Team are pleased to announce the release of Squid-3.5.24.
 
 This new release is available for download from <url
url="http://www.squid-cache.org/Versions/v3/3.5/"> or the
  <url url="http://www.squid-cache.org/Download/http-mirrors.html"
name="mirrors">.





--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/re-squid-V5-tp4681374.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From chip_pop at hotmail.com  Sat Jan 28 11:30:46 2017
From: chip_pop at hotmail.com (joseph)
Date: Sat, 28 Jan 2017 03:30:46 -0800 (PST)
Subject: [squid-users] re squid V5
In-Reply-To: <1485601854857-4681374.post@n4.nabble.com>
References: <1485601854857-4681374.post@n4.nabble.com>
Message-ID: <1485603046919-4681375.post@n4.nabble.com>

v4 also
http://www.squid-cache.org/Versions/v4/changesets/squid-4-14979.patch



--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/re-squid-V5-tp4681374p4681375.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From rousskov at measurement-factory.com  Sat Jan 28 17:47:00 2017
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Sat, 28 Jan 2017 10:47:00 -0700
Subject: [squid-users] Clarity on sending intercepted HTTPS traffic
 upstream to a cache_peer
In-Reply-To: <da435a9f-904a-b438-4fa5-309a1671b3bb@charlie.is>
References: <43c4ec96-c100-149e-d2c2-78cceb4ce530@charlie.is>
 <69ed2073-eb71-c535-fb44-9911f9c35973@charlie.is>
 <5cee91b5-3ca7-eb45-07f1-5152233ee58b@measurement-factory.com>
 <da435a9f-904a-b438-4fa5-309a1671b3bb@charlie.is>
Message-ID: <0534e50e-ff0a-9538-3e36-a02085970760@measurement-factory.com>

On 01/27/2017 05:32 PM, Charlie Orford wrote:
> Obviously it makes no sense
> intercepting ssl traffic if we're going to splice everything.

It actually does make a lot of sense in many environments, but not
necessarily yours.


> Our design goal is: intercept and bump local client https traffic on
> squid1 (so we can filter certain urls, cache content etc.) and then
> forward the request on to the origin server via an upstream squid2
> (which has internet access).

Understood. Squid can be enhanced to do what you want. There is nothing
fundamentally impossible in what you are describing AFAICT. We need to
add an insecure peer connector, and then using that connector code on
the regular request forwarding path. The low-level code to do that
already exists in tunnel.cc, but needs to be refactored/moved. This is
an architecturally challenging work, but it is certainly doable.

After those Squid modifications, in the simplest case (ignoring that you
cannot bump some sites and that you may not want to bump some of the
clients either), your squid1 configuration would be something like this:

  ssl_bump stare all
  ssl_bump bump all

Your squid2 will not do SslBump. In fact, to achieve the stated goal,
squid2 does not need to support SSL at all -- it can blindly forward
[encrypted] traffic from squid1 to the internet.

Your next steps to make the above happen are outlined at
http://wiki.squid-cache.org/SquidFaq/AboutSquid#How_to_add_a_new_Squid_feature.2C_enhance.2C_of_fix_something.3F


> http://lists.squid-cache.org/pipermail/squid-users/2015-November/007955.html
> seems to have successfully done this but I can't replicate it.

The configuration posted at the above URL is broken because it does not
tell Squid what to do after step1. If it did work, it was a bug like,
for example, bug 3209. Most likely, Squid just spliced everything (as
you suspect). Ignore that email. To learn why that configuration makes
no sense, study http://wiki.squid-cache.org/Features/SslPeekAndSplice


HTH,

Alex.



From squid3 at treenet.co.nz  Sun Jan 29 03:02:30 2017
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sun, 29 Jan 2017 16:02:30 +1300
Subject: [squid-users] re squid V5
In-Reply-To: <1485603046919-4681375.post@n4.nabble.com>
References: <1485601854857-4681374.post@n4.nabble.com>
 <1485603046919-4681375.post@n4.nabble.com>
Message-ID: <a8362b74-116b-d8d9-88f7-b168ca780234@treenet.co.nz>

On 29/01/2017 12:30 a.m., joseph wrote:
> v4 also
> http://www.squid-cache.org/Versions/v4/changesets/squid-4-14979.patch
> 

Yes they are correct.

All changes** to Squid go into the development release branch, then
filter 'down' through the branches until they reach the oldest version
they are going to be living.

In this case a 3.5 erelease means v5 / v4 have doumentation updates to
their Changelogs and release notes historic documents. The 3.5 patch has
the release tags etc.

** Well almost all changes, some exceptions take place where the
relevant code has been removed from a later release, or the bug was
created by the backporting process itself.

Amos



From charlie at charlie.is  Sun Jan 29 11:34:25 2017
From: charlie at charlie.is (Charlie Orford)
Date: Sun, 29 Jan 2017 11:34:25 +0000
Subject: [squid-users] Clarity on sending intercepted HTTPS traffic
 upstream to a cache_peer
In-Reply-To: <0534e50e-ff0a-9538-3e36-a02085970760@measurement-factory.com>
References: <43c4ec96-c100-149e-d2c2-78cceb4ce530@charlie.is>
 <69ed2073-eb71-c535-fb44-9911f9c35973@charlie.is>
 <5cee91b5-3ca7-eb45-07f1-5152233ee58b@measurement-factory.com>
 <da435a9f-904a-b438-4fa5-309a1671b3bb@charlie.is>
 <0534e50e-ff0a-9538-3e36-a02085970760@measurement-factory.com>
Message-ID: <3a07acbb-c7d6-c2ad-89d8-0821896603b1@charlie.is>

On 28/01/2017 17:47, Alex Rousskov wrote:
>
>> Our design goal is: intercept and bump local client https traffic on
>> squid1 (so we can filter certain urls, cache content etc.) and then
>> forward the request on to the origin server via an upstream squid2
>> (which has internet access).
> Understood. Squid can be enhanced to do what you want. There is nothing
> fundamentally impossible in what you are describing AFAICT. We need to
> add an insecure peer connector, and then using that connector code on
> the regular request forwarding path. The low-level code to do that
> already exists in tunnel.cc, but needs to be refactored/moved. This is
> an architecturally challenging work, but it is certainly doable.
>
> After those Squid modifications, in the simplest case (ignoring that you
> cannot bump some sites and that you may not want to bump some of the
> clients either), your squid1 configuration would be something like this:
>
>    ssl_bump stare all
>    ssl_bump bump all
>
> Your squid2 will not do SslBump. In fact, to achieve the stated goal,
> squid2 does not need to support SSL at all -- it can blindly forward
> [encrypted] traffic from squid1 to the internet.
>
> Your next steps to make the above happen are outlined at
> http://wiki.squid-cache.org/SquidFaq/AboutSquid#How_to_add_a_new_Squid_feature.2C_enhance.2C_of_fix_something.3F
>
>
>> http://lists.squid-cache.org/pipermail/squid-users/2015-November/007955.html
>> seems to have successfully done this but I can't replicate it.
> The configuration posted at the above URL is broken because it does not
> tell Squid what to do after step1. If it did work, it was a bug like,
> for example, bug 3209. Most likely, Squid just spliced everything (as
> you suspect). Ignore that email. To learn why that configuration makes
> no sense, study http://wiki.squid-cache.org/Features/SslPeekAndSplice
>
>
> HTH,
>
> Alex.
>

Thanks Alex and Amos for your thoughtful replies, they've given me the 
clarity I was seeking.

In terms of next steps, patching this functionality in to squid is 
certainly well beyond my skill level. Sponsorship of the work may be 
possible but not something I could  arrange quickly. I'll see how viable 
this is and get in contact with Measurement Factory directly if I'm able 
to secure the budget.

Kind regards,
Charlie





From baborucki at gmail.com  Sun Jan 29 18:52:04 2017
From: baborucki at gmail.com (boruc)
Date: Sun, 29 Jan 2017 10:52:04 -0800 (PST)
Subject: [squid-users] Is it possible to modify cached object?
In-Reply-To: <97375954-3157-92e4-f557-ea62b74a04a7@treenet.co.nz>
References: <1483727739416-4681073.post@n4.nabble.com>
 <b52d58fd-336e-b1b8-1453-38ec44e38870@measurement-factory.com>
 <1485099242343-4681243.post@n4.nabble.com>
 <97375954-3157-92e4-f557-ea62b74a04a7@treenet.co.nz>
Message-ID: <1485715924346-4681379.post@n4.nabble.com>

What would be the safest way to rebuild squid and enable eCAP?

I wanted to install libecap and some examples from  e-cap.org/Documentation
<http://e-cap.org/Documentation>   for my squid. My version is 3.1.19,  wiki
<http://wiki.squid-cache.org/Features/eCAP>   says that suitable version of
both /libecap/ and /adapter/ should be 0.0.3. First I wanted to do this on
VirtualBox so installing new stuff and rebuilding won't break my work.
1. sudo apt-get install squid3 (I did to recreate everything like it's on my
PC)
2. sudo apt-get remove --purge squid3
3. install libecap from downloaded source (./configure && make && sudo make
install)
4. install adapter (just like above)
5. run "squid3 -v" on PC to get configuration used to install with /apt-get/
6. configure squid3 with that config and add "--enable-ecap"
7. sudo make && sudo make install

After quite long installation I am unable to do commands like "squid3 -v",
"squid3 -z", I get message:

bash: /usr/sbin/squid3: No such file or directory

when I try "sudo service squid restart then I get "squid3: unrecognized
service". However, I am able to run "squid -v" (without 3 in name), but
there's no "squid" service.

Has anyone faced similar problem? What can I do to have squid service?

Thank you.



--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/Is-it-possible-to-modify-cached-object-tp4681073p4681379.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From squid3 at treenet.co.nz  Mon Jan 30 04:43:35 2017
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Mon, 30 Jan 2017 17:43:35 +1300
Subject: [squid-users] Is it possible to modify cached object?
In-Reply-To: <1485715924346-4681379.post@n4.nabble.com>
References: <1483727739416-4681073.post@n4.nabble.com>
 <b52d58fd-336e-b1b8-1453-38ec44e38870@measurement-factory.com>
 <1485099242343-4681243.post@n4.nabble.com>
 <97375954-3157-92e4-f557-ea62b74a04a7@treenet.co.nz>
 <1485715924346-4681379.post@n4.nabble.com>
Message-ID: <bc9151b9-df29-7c16-1460-08082e67243a@treenet.co.nz>

On 30/01/2017 7:52 a.m., boruc wrote:
> What would be the safest way to rebuild squid and enable eCAP?
> 
> I wanted to install libecap and some examples from  e-cap.org/Documentation
> <http://e-cap.org/Documentation>   for my squid. My version is 3.1.19,  wiki

Please upgrade. 3.1 is over 5 years outdated and the OS it was written
for wont even have LTS support for very much longer.

All the newer versions should come pre-packaged with eCAP support with
no action needed on your part.

Amos



From squid3 at treenet.co.nz  Mon Jan 30 07:08:47 2017
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Mon, 30 Jan 2017 20:08:47 +1300
Subject: [squid-users] [squid-announce] Squid 3.5.24 is available
Message-ID: <c7359734-1428-1e58-e9a4-3f617c579639@treenet.co.nz>

The Squid HTTP Proxy team is very pleased to announce the availability
of the Squid-3.5.24 release!


This release is a bug fix release resolving several issues found in the
prior Squid releases.


The major changes to be aware of:

* Mitigate DoS attacks that use client-initiated SSL/TLS renegotiation.

Recent alterations to the SSL-Bump feature logic were found to be
breaking the measure put in place to disable TLS renegotiation.
Since some TLSv1.2+ mechanisms actively require it and the upcoming
OpenSSL v1.1+ make it quite hard to disable, we have decided to mitigate
the vulnerability by implementing a rate limit on renegotiation instead
of an outright disable.


* SSLv2 records force SslBump bumping despite a matching step2 peek rule.

This bug shows up as SSLv2 connections being bumped to deliver an error
when they should have been spliced as configured. Squid will now splice
all connections it has been configured to regardless of whether the
obsolete SSLv2 syntax is being used.
 When bumping or receiving the connection itself Squid will still reject
SSLv2. Only spliced traffic is affected by this.


* Update External ACL helpers error handling and caching

The Squid helper protocol has undergone several important changes but
the external ACL logic and bundled helpers have not kept up. The ACL
logics handling helper replies also had some bugs in the event of helper
failures.

This release fixes those various bugs and updates all the bundled
helpers to make use of the BH (BrokenHelper) status to signal internal
errors differently to ACL denial.


* Bug #3940 pt2: Make 'cache deny' do what is documented

There was a small regression in 3.5.23 release fix for bug 3940. The
'cache deny' rules were not being obeyed. Surprisingly this has had no
complaints.

Perhapse that is a sign that anyone using 'cache deny' rules should
reasses whether those rules are still useful in these latest Squid releases.



 All users of Squid-3 are encouraged to upgrade to this release as
soon as possible.


 See the ChangeLog for the full list of changes in this and earlier
 releases.

Please refer to the release notes at
http://www.squid-cache.org/Versions/v3/3.5/RELEASENOTES.html
when you are ready to make the switch to Squid-3.5

Upgrade tip:
  "squid -k parse" is starting to display even more
   useful hints about squid.conf changes.

This new release can be downloaded from our HTTP or FTP servers

 http://www.squid-cache.org/Versions/v3/3.5/
 ftp://ftp.squid-cache.org/pub/squid/
 ftp://ftp.squid-cache.org/pub/archive/3.5/

or the mirrors. For a list of mirror sites see

 http://www.squid-cache.org/Download/http-mirrors.html
 http://www.squid-cache.org/Download/mirrors.html

If you encounter any issues with this release please file a bug report.
http://bugs.squid-cache.org/


Amos Jeffries

_______________________________________________
squid-announce mailing list
squid-announce at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-announce

From ml at netfence.it  Mon Jan 30 11:09:34 2017
From: ml at netfence.it (Andrea Venturoli)
Date: Mon, 30 Jan 2017 12:09:34 +0100
Subject: [squid-users] X-Forwarded-For breaks a site
Message-ID: <2222c554-4cca-f0b6-5169-f85b0a6eb9dc@netfence.it>

Hello.

I've been invited to visit a web site and I couldn't see it.
Bypassing squid would solve the problem, so I made some some researches 
and saw that adding "forwarded_for transparent" to my config would do.

I'm wondering what the reason might be...

tcpdump showed that:
1) initial connection to http:/www.xxxxxxx.com yields a 302 redirect to 
http:/www.xxxxxxx.com/md;
2) so a second request goes out to http:/www.xxxxxxx.com/md and yields a 
301, again redirecting to http:/www.xxxxxxx.com/md/ (notice the last slash);
3) finally a request goes out for http:/www.xxxxxxx.com/md/ and here's 
where a difference arises between a direct connection and one through 
Squid (without "forwarded_for transparent").

The answer to a direct connection (or to Squid with "forwarded_for 
transparent") is:
> HTTP/1.1 303 See other
> Date: Mon, 30 Jan 2017 09:56:18 GMT
> Server: Apache
> X-Powered-By: PHP/5.3.29
> Expires: Thu, 19 Nov 1981 08:52:00 GMT
> Cache-Control: no-store, no-cache, must-revalidate, post-check=0, pre-check=0
> Pragma: no-cache
> Set-Cookie: PHPSESSID=wwwwwwwwwww; path=/
> Set-Cookie: yyyyyyyyyyyyyy=zzzzzzzzzzzzz; path=/; HttpOnly
> Location: http://www.xxxxxxx.com/md/it/
> Content-Length: 0
> Connection: close
> Content-Type: text/html; charset=utf-8

The answer to Squid without "forwarded_for transparent") is:
> HTTP/1.1 200 OK
> Date: Mon, 30 Jan 2017 09:33:51 GMT
> Server: Apache
> X-Powered-By: PHP/5.3.29
> Expires: Thu, 19 Nov 1981 08:52:00 GMT
> Cache-Control: no-store, no-cache, must-revalidate, post-check=0, pre-check=0
> Pragma: no-cache
> Set-Cookie: PHPSESSID=vvvvvvvvvvvvvvvvvvvvvv; path=/
> Content-Length: 0
> Keep-Alive: timeout=15, max=98
> Connection: Keep-Alive
> Content-Type: text/html


The site is a commercial one and, altough it features a reserved area, I 
don't see any point in loosing visibility to corporate users.
Also the webserver belongs to a famous ISP which should also hosts 
thousands of other sites, so I guess it should have nothing fancy.



Anyone can shed some light on this behaviour?
Is this Squid's fault (I don't think so, but I'll just ask)?
Is this a known bug in some version of Apache or PHP or whatever?
Is it dangerous to keep "forwarded_for transparent" in my config?



  bye & Thanks
	av.


From uhlar at fantomas.sk  Mon Jan 30 13:25:25 2017
From: uhlar at fantomas.sk (Matus UHLAR - fantomas)
Date: Mon, 30 Jan 2017 14:25:25 +0100
Subject: [squid-users] X-Forwarded-For breaks a site
In-Reply-To: <2222c554-4cca-f0b6-5169-f85b0a6eb9dc@netfence.it>
References: <2222c554-4cca-f0b6-5169-f85b0a6eb9dc@netfence.it>
Message-ID: <20170130132525.GA23252@fantomas.sk>

On 30.01.17 12:09, Andrea Venturoli wrote:
>The answer to a direct connection (or to Squid with "forwarded_for 
>transparent") is:
>>HTTP/1.1 303 See other
>>Date: Mon, 30 Jan 2017 09:56:18 GMT
>>Server: Apache
>>X-Powered-By: PHP/5.3.29
>>Expires: Thu, 19 Nov 1981 08:52:00 GMT
>>Cache-Control: no-store, no-cache, must-revalidate, post-check=0, pre-check=0
>>Pragma: no-cache
>>Set-Cookie: PHPSESSID=wwwwwwwwwww; path=/
>>Set-Cookie: yyyyyyyyyyyyyy=zzzzzzzzzzzzz; path=/; HttpOnly
>>Location: http://www.xxxxxxx.com/md/it/
>>Content-Length: 0
>>Connection: close
>>Content-Type: text/html; charset=utf-8
>
>The answer to Squid without "forwarded_for transparent") is:
>>HTTP/1.1 200 OK
>>Date: Mon, 30 Jan 2017 09:33:51 GMT
>>Server: Apache
>>X-Powered-By: PHP/5.3.29
>>Expires: Thu, 19 Nov 1981 08:52:00 GMT
>>Cache-Control: no-store, no-cache, must-revalidate, post-check=0, pre-check=0
>>Pragma: no-cache
>>Set-Cookie: PHPSESSID=vvvvvvvvvvvvvvvvvvvvvv; path=/
>>Content-Length: 0
>>Keep-Alive: timeout=15, max=98
>>Connection: Keep-Alive
>>Content-Type: text/html
>
>
>The site is a commercial one and, altough it features a reserved 
>area, I don't see any point in loosing visibility to corporate users.
>Also the webserver belongs to a famous ISP which should also hosts 
>thousands of other sites, so I guess it should have nothing fancy.

>Anyone can shed some light on this behaviour?

it's quite common that some pages break on x-forwarded-for header.
It's mostly fault of those pages, not clients or webserver.

>Is this Squid's fault (I don't think so, but I'll just ask)?

no

>Is this a known bug in some version of Apache or PHP or whatever?

no

>Is it dangerous to keep "forwarded_for transparent" in my config?

might be, if you let private internal data to pass out.

you should study what does the directive do and decide what to do with XFF
header. See:
http://www.squid-cache.org/Doc/config/forwarded_for/

if there's possibility of contacting the page owner with a complaint, do that.

-- 
Matus UHLAR - fantomas, uhlar at fantomas.sk ; http://www.fantomas.sk/
Warning: I wish NOT to receive e-mail advertising to this address.
Varovanie: na tuto adresu chcem NEDOSTAVAT akukolvek reklamnu postu.
We are but packets in the Internet of life (userfriendly.org)


From creditu at eml.cc  Mon Jan 30 14:46:30 2017
From: creditu at eml.cc (creditu at eml.cc)
Date: Mon, 30 Jan 2017 07:46:30 -0700
Subject: [squid-users] Log Setup
Message-ID: <1485787590.659241.864187792.016BB4DC@webmail.messagingengine.com>

I have a 3.1 accelerator and have set the logs up to emulate _httpd_log.
 On the standard squid log I can see which backend server the request
was sent to:
 
1481343537.601      1 192.168.1.227 TCP_MISS/200 496 GET
https://192.168.1.102/ - ROUNDROBIN_PARENT/192.168.1.21 text/html

When I turn on httpd emulation I can't seem to come up with the
setting/option that will give me the IP of the backend the request was
sent to.  Is there a way to do this in 3.1?  I can get the
ROUNDROBIN_PARENT part, but not the IP.

192.168.1.227 - - [09/Dec/2016:21:21:48 -0700] "GET
https://192.168.1.102/ HTTP/1.1" 200 498 "-" "Mozilla/5.0 (X11; Linux
i686; rv:45.0) Gecko/20100101 Firefox/45.0" "-" "-" "192.168.1.102" "-"
"-" "1" TCP_MISS:ROUNDROBIN_PARENT


From rousskov at measurement-factory.com  Mon Jan 30 16:38:42 2017
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Mon, 30 Jan 2017 09:38:42 -0700
Subject: [squid-users] Squid 3.5.24 is available
In-Reply-To: <c7359734-1428-1e58-e9a4-3f617c579639@treenet.co.nz>
References: <c7359734-1428-1e58-e9a4-3f617c579639@treenet.co.nz>
Message-ID: <b7ca5e84-e91c-7dff-97ec-79334b02dc8f@measurement-factory.com>

On 01/30/2017 12:08 AM, Amos Jeffries wrote:
> * Bug #3940 pt2: Make 'cache deny' do what is documented
>
> There was a small regression in 3.5.23 release fix for bug 3940. The
> 'cache deny' rules were not being obeyed. Surprisingly this has had no
> complaints.

There were complaints. That "small regression" had significant costs for
some admins and some developers.


Alex.



From le.dahut at laposte.net  Mon Jan 30 17:10:20 2017
From: le.dahut at laposte.net (le dahut)
Date: Mon, 30 Jan 2017 18:10:20 +0100
Subject: [squid-users] irregular traffic when using proxy, not if NATed
Message-ID: <f14089d5-5c45-26f9-32b5-b60c7666b3ce@laposte.net>

Hello.

On certain upload websites, the traffic monitor shows an irregular 
traffic when uploading through squid, while uploading NATed (not using 
squid) gives a regular traffic.

Traffic monitoring on the client gives this when using squid :
https://dev-eole.ac-dijon.fr/attachments/download/1978/trafic-en-ligne.ac-aix-marseille.png

And this when connecting directly to the website :
https://dev-eole.ac-dijon.fr/attachments/download/1998/trafic-en-ligne.ac-aix-marseille-NAT.png

"squid.conf" is here :
http://paste.ubuntu.com/23894668/


# squid -v
Squid Cache: Version 3.5.12
Service Name: squid
Ubuntu linux
configure options:  '--build=x86_64-linux-gnu' '--prefix=/usr' 
'--includedir=${prefix}/include' '--mandir=${prefix}/share/man' 
'--infodir=${prefix}/share/info' '--sysconfdir=/etc' 
'--localstatedir=/var' '--libexecdir=${prefix}/lib/squid3' '--srcdir=.' 
'--disable-maintainer-mode' '--disable-dependency-tracking' 
'--disable-silent-rules' 'BUILDCXXFLAGS=-g -O2 -fPIE 
-fstack-protector-strong -Wformat -Werror=format-security 
-Wl,-Bsymbolic-functions -fPIE -pie -Wl,-z,relro -Wl,-z,now' 
'--datadir=/usr/share/squid' '--sysconfdir=/etc/squid' 
'--libexecdir=/usr/lib/squid' '--mandir=/usr/share/man' 
'--enable-inline' '--disable-arch-native' '--enable-async-io=8' 
'--enable-storeio=ufs,aufs,diskd,rock' 
'--enable-removal-policies=lru,heap' '--enable-delay-pools' 
'--enable-cache-digests' '--enable-icap-client' 
'--enable-follow-x-forwarded-for' 
'--enable-auth-basic=DB,fake,getpwnam,LDAP,NCSA,NIS,PAM,POP3,RADIUS,SASL,SMB' 
'--enable-auth-digest=file,LDAP' 
'--enable-auth-negotiate=kerberos,wrapper' 
'--enable-auth-ntlm=fake,smb_lm' 
'--enable-external-acl-helpers=file_userip,kerberos_ldap_group,LDAP_group,session,SQL_session,unix_group,wbinfo_group' 
'--enable-url-rewrite-helpers=fake' '--enable-eui' '--enable-esi' 
'--enable-icmp' '--enable-zph-qos' '--enable-ecap' 
'--disable-translation' '--with-swapdir=/var/spool/squid' 
'--with-logdir=/var/log/squid' '--with-pidfile=/var/run/squid.pid' 
'--with-filedescriptors=65536' '--with-large-files' 
'--with-default-user=proxy' '--enable-build-info=Ubuntu linux' 
'--enable-linux-netfilter' 'build_alias=x86_64-linux-gnu' 'CFLAGS=-g -O2 
-fPIE -fstack-protector-strong -Wformat -Werror=format-security -Wall' 
'LDFLAGS=-Wl,-Bsymbolic-functions -fPIE -pie -Wl,-z,relro -Wl,-z,now' 
'CPPFLAGS=-Wdate-time -D_FORTIFY_SOURCE=2' 'CXXFLAGS=-g -O2 -fPIE 
-fstack-protector-strong -Wformat -Werror=format-security'


Can you help me find out why ?


Regards,
  Klaas



From squid3 at treenet.co.nz  Tue Jan 31 02:22:46 2017
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Tue, 31 Jan 2017 15:22:46 +1300
Subject: [squid-users] Log Setup
In-Reply-To: <1485787590.659241.864187792.016BB4DC@webmail.messagingengine.com>
References: <1485787590.659241.864187792.016BB4DC@webmail.messagingengine.com>
Message-ID: <ab645c2f-8d16-b236-3692-c35152d06022@treenet.co.nz>

On 31/01/2017 3:46 a.m., creditu wrote:
> I have a 3.1 accelerator and have set the logs up to emulate _httpd_log.

Apache ("httpd") is a web server. Its logging format was not designed
for use by proxies nor to display the type of information routinely
handled by proxies (and CDN in your case).

The rest of your problems follow naturally from that simple fact.


>  On the standard squid log I can see which backend server the request
> was sent to:
>  
> 1481343537.601      1 192.168.1.227 TCP_MISS/200 496 GET
> https://192.168.1.102/ - ROUNDROBIN_PARENT/192.168.1.21 text/html
> 
> When I turn on httpd emulation I can't seem to come up with the
> setting/option that will give me the IP of the backend the request was
> sent to.  Is there a way to do this in 3.1?  I can get the
> ROUNDROBIN_PARENT part, but not the IP.

You can try "log_fqdn off", but really you should stop using web-server
format to record proxy information.

PS. Also you should upgrade your proxy, Squid 3.1 is past 5 years old in
an Internet that has seen massive changes in those years.

Amos



From baborucki at gmail.com  Tue Jan 31 09:35:30 2017
From: baborucki at gmail.com (boruc)
Date: Tue, 31 Jan 2017 01:35:30 -0800 (PST)
Subject: [squid-users] Is it possible to modify cached object?
In-Reply-To: <bc9151b9-df29-7c16-1460-08082e67243a@treenet.co.nz>
References: <1483727739416-4681073.post@n4.nabble.com>
 <b52d58fd-336e-b1b8-1453-38ec44e38870@measurement-factory.com>
 <1485099242343-4681243.post@n4.nabble.com>
 <97375954-3157-92e4-f557-ea62b74a04a7@treenet.co.nz>
 <1485715924346-4681379.post@n4.nabble.com>
 <bc9151b9-df29-7c16-1460-08082e67243a@treenet.co.nz>
Message-ID: <1485855330673-4681390.post@n4.nabble.com>

I ran command "sudo apt-get remove --purge squid3", then I downloaded
squid-3.5.24 in browser, unpacked it, then "sudo auto-apt run ./configure &&
sudo make && sudo checkinstall". deb package is created, message says that
it was also installed. So i ran command to see installed packages and that's
what I get:

squid-common
squid-langpack
squid3
squid3-common

When I run any command with "squid3" it says that squid3 is not installed,
but "squid -v" gives me "Squid Cache: Version 3.1.19". Also there is no
"squid" or "squid3" service.

What am I doing wrong? What should I do to get this 3.5.24 version to work
and also have squid service?

Thanks in advance.



--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/Is-it-possible-to-modify-cached-object-tp4681073p4681390.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From Antony.Stone at squid.open.source.it  Tue Jan 31 09:52:39 2017
From: Antony.Stone at squid.open.source.it (Antony Stone)
Date: Tue, 31 Jan 2017 10:52:39 +0100
Subject: [squid-users] Is it possible to modify cached object?
In-Reply-To: <1485855330673-4681390.post@n4.nabble.com>
References: <1483727739416-4681073.post@n4.nabble.com>
 <bc9151b9-df29-7c16-1460-08082e67243a@treenet.co.nz>
 <1485855330673-4681390.post@n4.nabble.com>
Message-ID: <201701311052.39956.Antony.Stone@squid.open.source.it>

On Tuesday 31 January 2017 at 10:35:30, boruc wrote:

> I ran command "sudo apt-get remove --purge squid3"

Okay, that will remove the distribution (Debian or Debian-based) package.

> then I downloaded squid-3.5.24 in browser

What exactly did you download from where?

> unpacked it

So, presumably this was some source code tarball?

> then "sudo auto-apt run ./configure && sudo make && sudo checkinstall".

That looks like a rather bizarre combination of a Distribution command (auto-
apt run) and a distribution-independent configure & build (but not an 
installation of the resulting binary), followed by a checkinstall (which is 
designed to create an installable object, but dow not install it).

My biggest question here is "where did you find the instructions telling you to 
run that combination of commands?"

> deb package is created, message says that it was also installed.

Show us the message.

> So i ran command

What command?

> to see installed packages and that's what I get:
> 
> squid-common
> squid-langpack
> squid3
> squid3-common
> 
> When I run any command with "squid3" it says that squid3 is not installed,
> but "squid -v" gives me "Squid Cache: Version 3.1.19". Also there is no
> "squid" or "squid3" service.
> 
> What am I doing wrong?

You are getting horribly confused between installing a distribution-packaged 
version, and compiling your own binary from source.

> What should I do to get this 3.5.24 version to work and also have squid
> service?

1. Tell us which Linux distribution and version you are running.

2. Use you distribution's package manager to remove the packages you currently 
have installed:

	squid-common
	squid-langpack
	squid3
	squid3-common

3. Make sure they really have been removed, by running:

	find / -name "squid*"

Remove anything that command finds on your system.

4. If you really want version 3.5.24, check whether that is available for your 
distribution and if it is, install it through your package manager.

If that version is not available, and you're not happy to install whatever the 
latest version for your distribution is, follow the instructions at 
http://wiki.squid-cache.org/SquidFaq/CompilingSquid

5. Return here with copy/pasted commands and output if you need further help.


Regards,


Antony.

-- 
There's no such thing as bad weather - only the wrong clothes.

 - Billy Connolly

                                                   Please reply to the list;
                                                         please *don't* CC me.


From baborucki at gmail.com  Tue Jan 31 16:28:15 2017
From: baborucki at gmail.com (boruc)
Date: Tue, 31 Jan 2017 08:28:15 -0800 (PST)
Subject: [squid-users] Is it possible to modify cached object?
In-Reply-To: <201701311052.39956.Antony.Stone@squid.open.source.it>
References: <1483727739416-4681073.post@n4.nabble.com>
 <b52d58fd-336e-b1b8-1453-38ec44e38870@measurement-factory.com>
 <1485099242343-4681243.post@n4.nabble.com>
 <97375954-3157-92e4-f557-ea62b74a04a7@treenet.co.nz>
 <1485715924346-4681379.post@n4.nabble.com>
 <bc9151b9-df29-7c16-1460-08082e67243a@treenet.co.nz>
 <1485855330673-4681390.post@n4.nabble.com>
 <201701311052.39956.Antony.Stone@squid.open.source.it>
Message-ID: <1485880095814-4681392.post@n4.nabble.com>

1. Ubuntu 12.04.5 LTS
2. Squid downloaded from
http://www.squid-cache.org/Versions/v3/3.5/squid-3.5.24.tar.gz
3. About "sudo auto-apt run ./configure && sudo make && sudo checkinstall",
I just wanted to give it a shot, original command was "sudo ./configure &&
make && sudo make install"
4. Command to list packages: dpkg --get-selections | grep -v deinstall
5. Like Amon has written: "Please upgrade. 3.1 is over 5 years outdated and
the OS it was written 
for wont even have LTS support for very much longer. All the newer versions
should come pre-packaged with eCAP support with no action needed on your
part."
So I wanted the newest stable release and that is 3.5.24
6. I've deleted every squid-related package with dpkg (what about this one,
should I delete it too?: /usr/share/vim/vim73/syntax/squid.vim)
7. Inside unpacked squid folder I used ./configure that is at the end of
this post && sudo make && sudo make install 
8. Command from 4. doesn't show that squid is installed, however, "squid -v"
shows
Squid Cache: Version 3.5.24
Service Name: squid
9. When I go to /etc/init.d and type "squid" I get no error (I got earlier
because there was no cache.log file in /var/log/squid3)
10. I can run squid -z to create directories
11. there is no squid service on the list using "service --status-all"


./configure --build=x86_64-linux-gnu --prefix=/usr
--includedir=${prefix}/include --mandir=${prefix}/share/man
--infodir=${prefix}/share/info --sysconfdir=/etc --localstatedir=/var
--libexecdir=${prefix}/lib/squid3 --srcdir=. --disable-maintainer-mode
--disable-dependency-tracking --disable-silent-rules
--datadir=/usr/share/squid3 --sysconfdir=/etc/squid3 --mandir=/usr/share/man
--with-cppunit-basedir=/usr --enable-inline --enable-async-io=8
--enable-ecap --enable-storeio=ufs,aufs,diskd
--enable-removal-policies=lru,heap --enable-delay-pools
--enable-cache-digests --enable-underscores --enable-icap-client
--enable-follow-x-forwarded-for
--enable-basic-auth-helpers=LDAP,MSNT,NCSA,PAM,SASL,SMB,YP,DB,POP3,getpwnam,squid_radius_auth,multi-domain-NTLM
--enable-ntlm-auth-helpers=smb_lm,
--enable-digest-auth-helpers=ldap,password
--enable-negotiate-auth-helpers=squid_kerb_auth  --enable-arp-acl
--enable-esi --enable-zph-qos --enable-wccpv2 --disable-translation
--with-logdir=/var/log/squid3 --with-pidfile=/var/run/squid3.pid
--with-filedescriptors=65536 --with-large-files --with-default-user=proxy
--enable-linux-netfilter



--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/Is-it-possible-to-modify-cached-object-tp4681073p4681392.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From Antony.Stone at squid.open.source.it  Tue Jan 31 16:44:42 2017
From: Antony.Stone at squid.open.source.it (Antony Stone)
Date: Tue, 31 Jan 2017 17:44:42 +0100
Subject: [squid-users] Is it possible to modify cached object?
In-Reply-To: <1485880095814-4681392.post@n4.nabble.com>
References: <1483727739416-4681073.post@n4.nabble.com>
 <201701311052.39956.Antony.Stone@squid.open.source.it>
 <1485880095814-4681392.post@n4.nabble.com>
Message-ID: <201701311744.42998.Antony.Stone@squid.open.source.it>

On Tuesday 31 January 2017 at 17:28:15, boruc wrote:

> 1. Ubuntu 12.04.5 LTS
> 2. Squid downloaded from
> http://www.squid-cache.org/Versions/v3/3.5/squid-3.5.24.tar.gz

Okay, so that's an official source tarball, good.

> 3. About "sudo auto-apt run ./configure && sudo make && sudo checkinstall",
> I just wanted to give it a shot, original command was "sudo ./configure &&
> make && sudo make install"

That latter command would have been *far* more sensible (provided you remember 
the extra sudo in the middle) - it would configure, build and install the 
version you just downloaded from the Squid website.

> 4. Command to list packages: dpkg --get-selections | grep -v deinstall

Okay, that'll tell you what Ubuntu thinks has been installed via the package 
manager.

> 5. Like Amon has written: "Please upgrade. 3.1 is over 5 years outdated and
> the OS it was written for wont even have LTS support for very much longer.
> All the newer versions should come pre-packaged with eCAP support with no
> action needed on your part."

Yes, I completely agree you should upgrade from Squid 3.1

You might also consider upgrading from Ubuntu 12.04... :)

> So I wanted the newest stable release and that is 3.5.24

> 6. I've deleted every squid-related package with dpkg (what about this one,
> should I delete it too?: /usr/share/vim/vim73/syntax/squid.vim)

No, that's just an editor syntax rules file, for "intelligent" highlighting 
when you edit the Squid config file.

Leave it or delete it; it's nothing to do with installing or running Squid.

> 7. Inside unpacked squid folder I used ./configure that is at the end of
> this post && sudo make && sudo make install

Now *that* sounds good.

> 8. Command from 4. doesn't show that squid is installed

No, it won't, because you didn't install it through the package manager.

> however, "squid -v" shows
> Squid Cache: Version 3.5.24
> Service Name: squid

Excellent.

> 9. When I go to /etc/init.d and type "squid" I get no error (I got earlier
> because there was no cache.log file in /var/log/squid3)

Do you really mean "squid" or do you mean "./squid"?

If it's the first, I don't understand the relevance of being in /etc/init.d

> 10. I can run squid -z to create directories

Good.

> 11. there is no squid service on the list using "service --status-all"

What do you get from the following:

	/etc/init.d/squid status
	/etc/init.d/squid restart


Basically I think you are doing very well now.


Antony.

-- 
The next sentence is true.
The previous sentence is untrue.

                                                   Please reply to the list;
                                                         please *don't* CC me.


From baborucki at gmail.com  Tue Jan 31 17:19:14 2017
From: baborucki at gmail.com (boruc)
Date: Tue, 31 Jan 2017 09:19:14 -0800 (PST)
Subject: [squid-users] Is it possible to modify cached object?
In-Reply-To: <201701311744.42998.Antony.Stone@squid.open.source.it>
References: <1483727739416-4681073.post@n4.nabble.com>
 <b52d58fd-336e-b1b8-1453-38ec44e38870@measurement-factory.com>
 <1485099242343-4681243.post@n4.nabble.com>
 <97375954-3157-92e4-f557-ea62b74a04a7@treenet.co.nz>
 <1485715924346-4681379.post@n4.nabble.com>
 <bc9151b9-df29-7c16-1460-08082e67243a@treenet.co.nz>
 <1485855330673-4681390.post@n4.nabble.com>
 <201701311052.39956.Antony.Stone@squid.open.source.it>
 <1485880095814-4681392.post@n4.nabble.com>
 <201701311744.42998.Antony.Stone@squid.open.source.it>
Message-ID: <1485883154937-4681394.post@n4.nabble.com>

Antony Stone wrote
> What do you get from the following:
> 
> 	/etc/init.d/squid status
> 	/etc/init.d/squid restart

literally nothing. I just noticed that there isn't anything with "squid" in
name in /etc/init.d. 

Here are some locations and files with "squid" in name:
/usr/share/vim/vim73/syntax/squid.vim
/usr/share/man/man1/squidclient.1
/usr/share/man/man8/squid.8
/usr/share/squid3
/usr/sbin/squid
/usr/bin/squidclient
/etc/squid3/
/lib/squid3
/var/log/squid3
/var/spool/squid3
/var/cache/squid

Running "squid -k restart" gives me this: "squid: ERROR: No running copy"
Running "sudo service squid restart" gives me: "squid: unrecognized service"



--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/Is-it-possible-to-modify-cached-object-tp4681073p4681394.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From Antony.Stone at squid.open.source.it  Tue Jan 31 17:28:01 2017
From: Antony.Stone at squid.open.source.it (Antony Stone)
Date: Tue, 31 Jan 2017 18:28:01 +0100
Subject: [squid-users] Is it possible to modify cached object?
In-Reply-To: <1485883154937-4681394.post@n4.nabble.com>
References: <1483727739416-4681073.post@n4.nabble.com>
 <201701311744.42998.Antony.Stone@squid.open.source.it>
 <1485883154937-4681394.post@n4.nabble.com>
Message-ID: <201701311828.01550.Antony.Stone@squid.open.source.it>

On Tuesday 31 January 2017 at 18:19:14, boruc wrote:

> Antony Stone wrote
> 
> > What do you get from the following:
> > 	/etc/init.d/squid status
> > 	/etc/init.d/squid restart
> 
> literally nothing. I just noticed that there isn't anything with "squid" in
> name in /etc/init.d.

Ah, right - that means the original tarball build doesn't put any init scripts 
into place, then - I haven't built Squid like that myself for some time, so I 
had forgotten this.

I suggest you wait until one of the developers responds to this thread and 
reminds us how to get the appropriate init scripts into place for an Ubuntu 12 
system - I can't help with that, sorry :(

However, you're definitely very close to your desired objective.


Antony.

-- 
.evah I serutangis sseltniop tsom eht fo eno eb tsum sihT

                                                   Please reply to the list;
                                                         please *don't* CC me.


From baborucki at gmail.com  Tue Jan 31 18:34:17 2017
From: baborucki at gmail.com (boruc)
Date: Tue, 31 Jan 2017 10:34:17 -0800 (PST)
Subject: [squid-users] Is it possible to modify cached object?
In-Reply-To: <201701311828.01550.Antony.Stone@squid.open.source.it>
References: <1485099242343-4681243.post@n4.nabble.com>
 <97375954-3157-92e4-f557-ea62b74a04a7@treenet.co.nz>
 <1485715924346-4681379.post@n4.nabble.com>
 <bc9151b9-df29-7c16-1460-08082e67243a@treenet.co.nz>
 <1485855330673-4681390.post@n4.nabble.com>
 <201701311052.39956.Antony.Stone@squid.open.source.it>
 <1485880095814-4681392.post@n4.nabble.com>
 <201701311744.42998.Antony.Stone@squid.open.source.it>
 <1485883154937-4681394.post@n4.nabble.com>
 <201701311828.01550.Antony.Stone@squid.open.source.it>
Message-ID: <1485887657504-4681396.post@n4.nabble.com>

Thank you for your answers Antony.

On packages.ubuntu.com I searched for "squid3" and here's what I've found:
12.04LTS - 3.1.19
14.04LTS - 3.3.8
16.04LTS - 3.5.12

For now the best option would be to upgrade Ubuntu to 16.04, but I cannot do
it now. Also Amos has written earlier: "All the newer versions should come
pre-packaged with eCAP support with no action needed on your part." I'd like
to know if in squid 3.5.12 eCAP is already enabled, but I cannot find it by
myself.

I tried it on Ubuntu 14.04, by "sudo apt-get install squid3", squid 3.3.8
was installed (and also package libecap2 (0.2.0-1ubuntu4)), the output for
"squid3 -v" was:

configure options:  '--build=x86_64-linux-gnu' '--prefix=/usr'
'--includedir=${prefix}/include' '--mandir=${prefix}/share/man'
'--infodir=${prefix}/share/info' '--sysconfdir=/etc' '--localstatedir=/var'
'--libexecdir=${prefix}/lib/squid3' '--srcdir=.' '--disable-maintainer-mode'
'--disable-dependency-tracking' '--disable-silent-rules'
'--datadir=/usr/share/squid3' '--sysconfdir=/etc/squid3'
'--mandir=/usr/share/man' '--enable-inline' '--enable-async-io=8'
'--enable-storeio=ufs,aufs,diskd,rock' '--enable-removal-policies=lru,heap'
'--enable-delay-pools' '--enable-cache-digests' '--enable-underscores'
'--enable-icap-client' '--enable-follow-x-forwarded-for'
'--enable-auth-basic=DB,fake,getpwnam,LDAP,MSNT,MSNT-multi-domain,NCSA,NIS,PAM,POP3,RADIUS,SASL,SMB'
'--enable-auth-digest=file,LDAP' '--enable-auth-negotiate=kerberos,wrapper'
'--enable-auth-ntlm=fake,smb_lm'
'--enable-external-acl-helpers=file_userip,kerberos_ldap_group,LDAP_group,session,SQL_session,unix_group,wbinfo_group'
'--enable-url-rewrite-helpers=fake' '--enable-eui' '--enable-esi'
'--enable-icmp' '--enable-zph-qos' *'--enable-ecap'* '--disable-translation'
'--with-swapdir=/var/spool/squid3' '--with-logdir=/var/log/squid3'
'--with-pidfile=/var/run/squid3.pid' '--with-filedescriptors=65536'
'--with-large-files' '--with-default-user=proxy' '--enable-linux-netfilter'
'build_alias=x86_64-linux-gnu' 'CFLAGS=-g -O2 -fPIE -fstack-protector
--param=ssp-buffer-size=4 -Wformat -Werror=format-security -Wall'
'LDFLAGS=-Wl,-Bsymbolic-functions -fPIE -pie -Wl,-z,relro -Wl,-z,now'
'CPPFLAGS=-D_FORTIFY_SOURCE=2' 'CXXFLAGS=-g -O2 -fPIE -fstack-protector
--param=ssp-buffer-size=4 -Wformat -Werror=format-security'

So actually this squid release is already configured with "--enable-ecap"
and all I need to do is to set "ecap_enable" in configuration file to "on"
(and other stuff mentioned in  documentation
<http://e-cap.org/Documentation>  ) and it'll work fine?



--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/Is-it-possible-to-modify-cached-object-tp4681073p4681396.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From yvoinov at gmail.com  Tue Jan 31 19:02:56 2017
From: yvoinov at gmail.com (Yuri Voinov)
Date: Wed, 1 Feb 2017 01:02:56 +0600
Subject: [squid-users] Is it possible to modify cached object?
In-Reply-To: <1485887657504-4681396.post@n4.nabble.com>
References: <1485099242343-4681243.post@n4.nabble.com>
 <97375954-3157-92e4-f557-ea62b74a04a7@treenet.co.nz>
 <1485715924346-4681379.post@n4.nabble.com>
 <bc9151b9-df29-7c16-1460-08082e67243a@treenet.co.nz>
 <1485855330673-4681390.post@n4.nabble.com>
 <201701311052.39956.Antony.Stone@squid.open.source.it>
 <1485880095814-4681392.post@n4.nabble.com>
 <201701311744.42998.Antony.Stone@squid.open.source.it>
 <1485883154937-4681394.post@n4.nabble.com>
 <201701311828.01550.Antony.Stone@squid.open.source.it>
 <1485887657504-4681396.post@n4.nabble.com>
Message-ID: <56bbfdb7-6f2c-eccc-a01e-da977a2d972b@gmail.com>



01.02.2017 0:34, boruc ?????:
> Thank you for your answers Antony.
>
> On packages.ubuntu.com I searched for "squid3" and here's what I've found:
> 12.04LTS - 3.1.19
> 14.04LTS - 3.3.8
> 16.04LTS - 3.5.12
>
> For now the best option would be to upgrade Ubuntu to 16.04, but I cannot do
> it now. Also Amos has written earlier: "All the newer versions should come
> pre-packaged with eCAP support with no action needed on your part." I'd like
> to know if in squid 3.5.12 eCAP is already enabled, but I cannot find it by
> myself.
What an idea, dude! To upgrade one package - upgrade OS at whole! Wow!
You have unlimited free time and not responsible to the all datacenter,
right? :)
> I tried it on Ubuntu 14.04, by "sudo apt-get install squid3", squid 3.3.8
> was installed (and also package libecap2 (0.2.0-1ubuntu4)), the output for
> "squid3 -v" was:
>
> configure options:  '--build=x86_64-linux-gnu' '--prefix=/usr'
> '--includedir=${prefix}/include' '--mandir=${prefix}/share/man'
> '--infodir=${prefix}/share/info' '--sysconfdir=/etc' '--localstatedir=/var'
> '--libexecdir=${prefix}/lib/squid3' '--srcdir=.' '--disable-maintainer-mode'
> '--disable-dependency-tracking' '--disable-silent-rules'
> '--datadir=/usr/share/squid3' '--sysconfdir=/etc/squid3'
> '--mandir=/usr/share/man' '--enable-inline' '--enable-async-io=8'
> '--enable-storeio=ufs,aufs,diskd,rock' '--enable-removal-policies=lru,heap'
> '--enable-delay-pools' '--enable-cache-digests' '--enable-underscores'
> '--enable-icap-client' '--enable-follow-x-forwarded-for'
> '--enable-auth-basic=DB,fake,getpwnam,LDAP,MSNT,MSNT-multi-domain,NCSA,NIS,PAM,POP3,RADIUS,SASL,SMB'
> '--enable-auth-digest=file,LDAP' '--enable-auth-negotiate=kerberos,wrapper'
> '--enable-auth-ntlm=fake,smb_lm'
> '--enable-external-acl-helpers=file_userip,kerberos_ldap_group,LDAP_group,session,SQL_session,unix_group,wbinfo_group'
> '--enable-url-rewrite-helpers=fake' '--enable-eui' '--enable-esi'
> '--enable-icmp' '--enable-zph-qos' *'--enable-ecap'* '--disable-translation'
> '--with-swapdir=/var/spool/squid3' '--with-logdir=/var/log/squid3'
> '--with-pidfile=/var/run/squid3.pid' '--with-filedescriptors=65536'
> '--with-large-files' '--with-default-user=proxy' '--enable-linux-netfilter'
> 'build_alias=x86_64-linux-gnu' 'CFLAGS=-g -O2 -fPIE -fstack-protector
> --param=ssp-buffer-size=4 -Wformat -Werror=format-security -Wall'
> 'LDFLAGS=-Wl,-Bsymbolic-functions -fPIE -pie -Wl,-z,relro -Wl,-z,now'
> 'CPPFLAGS=-D_FORTIFY_SOURCE=2' 'CXXFLAGS=-g -O2 -fPIE -fstack-protector
> --param=ssp-buffer-size=4 -Wformat -Werror=format-security'
>
> So actually this squid release is already configured with "--enable-ecap"
> and all I need to do is to set "ecap_enable" in configuration file to "on"
> (and other stuff mentioned in  documentation
> <http://e-cap.org/Documentation>  ) and it'll work fine?
>
>
>
> --
> View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/Is-it-possible-to-modify-cached-object-tp4681073p4681396.html
> Sent from the Squid - Users mailing list archive at Nabble.com.
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users

-- 
Bugs to the Future
-------------- next part --------------
A non-text attachment was scrubbed...
Name: 0x613DEC46.asc
Type: application/pgp-keys
Size: 2437 bytes
Desc: not available
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20170201/6ffc35ce/attachment.key>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: signature.asc
Type: application/pgp-signature
Size: 473 bytes
Desc: OpenPGP digital signature
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20170201/6ffc35ce/attachment.sig>

From baborucki at gmail.com  Tue Jan 31 19:05:56 2017
From: baborucki at gmail.com (boruc)
Date: Tue, 31 Jan 2017 11:05:56 -0800 (PST)
Subject: [squid-users] Is it possible to modify cached object?
In-Reply-To: <56bbfdb7-6f2c-eccc-a01e-da977a2d972b@gmail.com>
References: <1485715924346-4681379.post@n4.nabble.com>
 <bc9151b9-df29-7c16-1460-08082e67243a@treenet.co.nz>
 <1485855330673-4681390.post@n4.nabble.com>
 <201701311052.39956.Antony.Stone@squid.open.source.it>
 <1485880095814-4681392.post@n4.nabble.com>
 <201701311744.42998.Antony.Stone@squid.open.source.it>
 <1485883154937-4681394.post@n4.nabble.com>
 <201701311828.01550.Antony.Stone@squid.open.source.it>
 <1485887657504-4681396.post@n4.nabble.com>
 <56bbfdb7-6f2c-eccc-a01e-da977a2d972b@gmail.com>
Message-ID: <1485889556741-4681398.post@n4.nabble.com>

Well, basically I'm working on virtual machine with nothing special installed
on it so I don't have to worry about all of this. I wanted to give squid a
try, look how it works, learn something new. Being here, reading all your
answers and suggestions is a great experience for me :)



--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/Is-it-possible-to-modify-cached-object-tp4681073p4681398.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From yvoinov at gmail.com  Tue Jan 31 19:09:35 2017
From: yvoinov at gmail.com (Yuri Voinov)
Date: Wed, 1 Feb 2017 01:09:35 +0600
Subject: [squid-users] Is it possible to modify cached object?
In-Reply-To: <1485889556741-4681398.post@n4.nabble.com>
References: <1485715924346-4681379.post@n4.nabble.com>
 <bc9151b9-df29-7c16-1460-08082e67243a@treenet.co.nz>
 <1485855330673-4681390.post@n4.nabble.com>
 <201701311052.39956.Antony.Stone@squid.open.source.it>
 <1485880095814-4681392.post@n4.nabble.com>
 <201701311744.42998.Antony.Stone@squid.open.source.it>
 <1485883154937-4681394.post@n4.nabble.com>
 <201701311828.01550.Antony.Stone@squid.open.source.it>
 <1485887657504-4681396.post@n4.nabble.com>
 <56bbfdb7-6f2c-eccc-a01e-da977a2d972b@gmail.com>
 <1485889556741-4681398.post@n4.nabble.com>
Message-ID: <f226824b-fadf-0e23-a638-14929768428d@gmail.com>

Exactly, localhost system administrators can do what they want ;-)


01.02.2017 1:05, boruc ?????:
> Well, basically I'm working on virtual machine with nothing special installed
> on it so I don't have to worry about all of this. I wanted to give squid a
> try, look how it works, learn something new. Being here, reading all your
> answers and suggestions is a great experience for me :)
>
>
>
> --
> View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/Is-it-possible-to-modify-cached-object-tp4681073p4681398.html
> Sent from the Squid - Users mailing list archive at Nabble.com.
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users

-- 
Bugs to the Future
-------------- next part --------------
A non-text attachment was scrubbed...
Name: 0x613DEC46.asc
Type: application/pgp-keys
Size: 2437 bytes
Desc: not available
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20170201/15af6e7d/attachment.key>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: signature.asc
Type: application/pgp-signature
Size: 473 bytes
Desc: OpenPGP digital signature
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20170201/15af6e7d/attachment.sig>

From rousskov at measurement-factory.com  Tue Jan 31 21:49:51 2017
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Tue, 31 Jan 2017 14:49:51 -0700
Subject: [squid-users] QA Pilots
Message-ID: <8168c881-0d81-8c05-4c45-b8231f71626b@measurement-factory.com>

Hello,

    The Squid Software Foundation plans to hire a part-time remote QA
engineer to help us address systemic quality problems with Squid
releases and development snapshots. This position will be funded by your
donations to the Foundation. Thank you!

Before a regular QA Engineer position is filled, we offer several paid
pilot projects to select the most suitable candidate. These projects
focus on building QA infrastructure to enforce existing (but
underutilized) quality controls. A lot more details are available at

    http://wiki.squid-cache.org/QA/Pilots

If you would like to apply, please see that page for the application
procedure. If you know somebody who should apply, please point them to
the above page. If you have constructive feedback regarding the planned
QA improvements, please share it on this mailing list.


Thank you,

Alex.


From yvoinov at gmail.com  Tue Jan 31 22:00:47 2017
From: yvoinov at gmail.com (Yuri Voinov)
Date: Wed, 1 Feb 2017 04:00:47 +0600
Subject: [squid-users] QA Pilots
In-Reply-To: <8168c881-0d81-8c05-4c45-b8231f71626b@measurement-factory.com>
References: <8168c881-0d81-8c05-4c45-b8231f71626b@measurement-factory.com>
Message-ID: <dc2df36b-d279-7cf9-4757-95174b808701@gmail.com>



01.02.2017 3:49, Alex Rousskov ?????:
> Hello,
>
>     The Squid Software Foundation plans to hire a part-time remote QA
> engineer to help us address systemic quality problems with Squid
> releases and development snapshots. This position will be funded by your
> donations to the Foundation. Thank you!
Do you mean the volunteers?
>
> Before a regular QA Engineer position is filled, we offer several paid
> pilot projects to select the most suitable candidate. These projects
> focus on building QA infrastructure to enforce existing (but
> underutilized) quality controls. A lot more details are available at
>
>     http://wiki.squid-cache.org/QA/Pilots
>
> If you would like to apply, please see that page for the application
> procedure. If you know somebody who should apply, please point them to
> the above page. If you have constructive feedback regarding the planned
> QA improvements, please share it on this mailing list.
>
>
> Thank you,
>
> Alex.
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users

-- 
Bugs to the Future
-------------- next part --------------
A non-text attachment was scrubbed...
Name: 0x613DEC46.asc
Type: application/pgp-keys
Size: 2437 bytes
Desc: not available
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20170201/dd641f07/attachment.key>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: signature.asc
Type: application/pgp-signature
Size: 473 bytes
Desc: OpenPGP digital signature
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20170201/dd641f07/attachment.sig>

From rousskov at measurement-factory.com  Tue Jan 31 22:21:48 2017
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Tue, 31 Jan 2017 15:21:48 -0700
Subject: [squid-users] QA Pilots
In-Reply-To: <dc2df36b-d279-7cf9-4757-95174b808701@gmail.com>
References: <8168c881-0d81-8c05-4c45-b8231f71626b@measurement-factory.com>
 <dc2df36b-d279-7cf9-4757-95174b808701@gmail.com>
Message-ID: <812f6813-d917-c40e-c8a1-b8dd8c732345@measurement-factory.com>

On 01/31/2017 03:00 PM, Yuri Voinov wrote:
> 01.02.2017 3:49, Alex Rousskov ?????:
>>     The Squid Software Foundation plans to hire a part-time remote QA
>> engineer to help us address systemic quality problems with Squid
>> releases and development snapshots. This position will be funded by your
>> donations to the Foundation. Thank you!

> Do you mean the volunteers?

The money to pay the engineer will come from the Foundation. Most of the
Foundation existing funds consist of donations from various people and
organizations, many of which subscribe to this mailing list. The
position (and the pilots) will be essentially funded by past (and
hopefully future) donations. That is why I thanked those who donated
(and/or will donate in the future). They made it possible.

The position is _not_ an unpaid volunteer position and the pilots are
not unpaid throw-away projects. FWIW, we have already tried the
volunteer approach and failed because volunteers (and their volunteer
managers!) often run out of free time, lose interest, develop work
conflicts, resist doing unpleasant tasks, etc. BTW, this would be the
first substantial paid contractor position with the Foundation. All
Foundation directors are and will remain unpaid volunteers.


Hope this clarifies,

Alex.

>>     http://wiki.squid-cache.org/QA/Pilots



