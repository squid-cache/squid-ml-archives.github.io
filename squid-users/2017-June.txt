From rousskov at measurement-factory.com  Thu Jun  1 00:10:18 2017
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Wed, 31 May 2017 18:10:18 -0600
Subject: [squid-users] Help troubleshooting proxy<-->client https
In-Reply-To: <CA+8Eki1-a7jqQzGrM0XYwDz6d-VYi1Q6PPNK5g34HpdmF2-_Sg@mail.gmail.com>
References: <CA+8Eki1-a7jqQzGrM0XYwDz6d-VYi1Q6PPNK5g34HpdmF2-_Sg@mail.gmail.com>
Message-ID: <abf01a59-b7f0-9b51-494f-9707a69a754f@measurement-factory.com>

On 05/31/2017 02:42 PM, Masha Lifshin wrote:

> What I am trying to achieve is 


> 1. an https connection between the client and squid proxy, as well as

> 2. listen on port 80 for http traffic,

> 3. on port 443 for ssl traffic, and

> 4. apply ssl-bump to the ssl traffic.


If I parsed your query correctly, and by "listen" you mean "intercept",
and you want to apply SslBump to proxied SSL traffic on all ports, then
it looks like you will need three ports, each doing ssl-bump. In other
words, instead of

> http_port 80 ssl-bump cert=some.cert.pem ...
> https_port 443 cert=other.cert.pem ...

You will need something like this:

# HTTPS proxy; clients establish TLS connections to 31443 (your item #1)
https_port 31443 ssl-bump ...

# HTTP-intercepting proxy (your item #2)
http_port 80 intercept ssl-bump ...

# SSL-intercepting proxy (your item #3)
https_port 443 intercept ssl-bump ...

You may need "tproxy" instead of "intercept", depending on how you are
intercepting/forwarding traffic.

The actual port numbers do no matter.


> Also wondering what, if any, are the security issues with using port 80
> for the http traffic?

Anybody with access to that traffic will be able to easily see
everything and, with a monumental effort, potentially occasionally
modify unencrypted traffic, including plain CONNECT requests that
establish secure channels.


HTH,

Alex.


> On Fri, May 26, 2017 at 7:19 AM, Alex Rousskov wrote:
> 
>     On 05/26/2017 12:00 AM, Masha Lifshin wrote:
>     > I have added an https_port directive
>     > to squid.conf, but it must be misconfigured.
> 
>     > http_port 172.30.0.67:443 <http://172.30.0.67:443> ...
>     > https_port 172.30.0.67:443 <http://172.30.0.67:443> ...
> 
>     You are right -- your Squid is misconfigured. You cannot use the same
>     address for two ports. Unfortunately, Squid thinks that port binding
>     errors are a minor inconvenience and continues running after logging an
>     error message (that looks like many other benign error messages).
> 
>     Changing one of the ports will solve the "same address" problem
>     described above.
> 
>     Do not use port 443 for http_port. It makes triage extremely confusing
>     because port 443 usually implies SSL. Consider using port 3128 instead.
> 
> 
>     HTH,
> 
>     Alex.
> 
> 
> 



From mlifshin at phantomdesign.com  Thu Jun  1 02:15:12 2017
From: mlifshin at phantomdesign.com (Masha Lifshin)
Date: Wed, 31 May 2017 19:15:12 -0700
Subject: [squid-users] Help troubleshooting proxy<-->client https
In-Reply-To: <abf01a59-b7f0-9b51-494f-9707a69a754f@measurement-factory.com>
References: <CA+8Eki1-a7jqQzGrM0XYwDz6d-VYi1Q6PPNK5g34HpdmF2-_Sg@mail.gmail.com>
 <abf01a59-b7f0-9b51-494f-9707a69a754f@measurement-factory.com>
Message-ID: <CA+8Eki3DcvqkXaH7S_eWo-SgUP+KP5a5qZsUViruAesMsr9POg@mail.gmail.com>

Thank you, very helpful.  Some more clarifying questions for you.

Sorry for the imprecise language, I mean not interception but rather
accepting connections to that port.  Our browsers will be explicitly
configured to connect our proxy, so I believe that is not interception?

If we want to only allow encrypted traffic between the browser and proxy,
does that mean we'd only want to use the following line from your example?

# HTTPS proxy; clients establish TLS connections to 31443 (your item #1)
https_port 31443 ssl-bump ...

Once a handshake is done and tls connection is established here, would it
be possible to have all http and https traffic from the browser go through
31443?  So squid would not need to have ports 80 and 443 open?

Thank you,
-Masha


On Wed, May 31, 2017 at 5:10 PM, Alex Rousskov <
rousskov at measurement-factory.com> wrote:

> On 05/31/2017 02:42 PM, Masha Lifshin wrote:
>
> > What I am trying to achieve is
>
>
> > 1. an https connection between the client and squid proxy, as well as
>
> > 2. listen on port 80 for http traffic,
>
> > 3. on port 443 for ssl traffic, and
>
> > 4. apply ssl-bump to the ssl traffic.
>
>
> If I parsed your query correctly, and by "listen" you mean "intercept",
> and you want to apply SslBump to proxied SSL traffic on all ports, then
> it looks like you will need three ports, each doing ssl-bump. In other
> words, instead of
>
> > http_port 80 ssl-bump cert=some.cert.pem ...
> > https_port 443 cert=other.cert.pem ...
>
> You will need something like this:
>
> # HTTPS proxy; clients establish TLS connections to 31443 (your item #1)
> https_port 31443 ssl-bump ...
>
> # HTTP-intercepting proxy (your item #2)
> http_port 80 intercept ssl-bump ...
>
> # SSL-intercepting proxy (your item #3)
> https_port 443 intercept ssl-bump ...
>
> You may need "tproxy" instead of "intercept", depending on how you are
> intercepting/forwarding traffic.
>
> The actual port numbers do no matter.
>
>
> > Also wondering what, if any, are the security issues with using port 80
> > for the http traffic?
>
> Anybody with access to that traffic will be able to easily see
> everything and, with a monumental effort, potentially occasionally
> modify unencrypted traffic, including plain CONNECT requests that
> establish secure channels.
>
>
> HTH,
>
> Alex.
>
>
> > On Fri, May 26, 2017 at 7:19 AM, Alex Rousskov wrote:
> >
> >     On 05/26/2017 12:00 AM, Masha Lifshin wrote:
> >     > I have added an https_port directive
> >     > to squid.conf, but it must be misconfigured.
> >
> >     > http_port 172.30.0.67:443 <http://172.30.0.67:443> ...
> >     > https_port 172.30.0.67:443 <http://172.30.0.67:443> ...
> >
> >     You are right -- your Squid is misconfigured. You cannot use the same
> >     address for two ports. Unfortunately, Squid thinks that port binding
> >     errors are a minor inconvenience and continues running after logging
> an
> >     error message (that looks like many other benign error messages).
> >
> >     Changing one of the ports will solve the "same address" problem
> >     described above.
> >
> >     Do not use port 443 for http_port. It makes triage extremely
> confusing
> >     because port 443 usually implies SSL. Consider using port 3128
> instead.
> >
> >
> >     HTH,
> >
> >     Alex.
> >
> >
> >
>
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20170531/de5006a8/attachment.htm>

From chip_pop at hotmail.com  Thu Jun  1 11:06:51 2017
From: chip_pop at hotmail.com (joseph)
Date: Thu, 1 Jun 2017 04:06:51 -0700 (PDT)
Subject: [squid-users] request future option
Message-ID: <1496315211819-4682648.post@n4.nabble.com>

hi dev. guys  i dont know if its ok to post here or in squid or ecap or i-cap
my proposal is  simple to understand
to be able to control  qos_flows tos local-hit=0x?? miss=0??
from adapter ecap or i-cap
example  i have file in locale separate drive  wen the code in adapter
detect and send that file
i need to control the qos bit  send  hit  or miss
tks in advanced.


regard joseph



--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/request-future-option-tp4682648.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From squid3 at treenet.co.nz  Thu Jun  1 12:07:24 2017
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Fri, 2 Jun 2017 00:07:24 +1200
Subject: [squid-users] Help troubleshooting proxy<-->client https
In-Reply-To: <CA+8Eki3DcvqkXaH7S_eWo-SgUP+KP5a5qZsUViruAesMsr9POg@mail.gmail.com>
References: <CA+8Eki1-a7jqQzGrM0XYwDz6d-VYi1Q6PPNK5g34HpdmF2-_Sg@mail.gmail.com>
 <abf01a59-b7f0-9b51-494f-9707a69a754f@measurement-factory.com>
 <CA+8Eki3DcvqkXaH7S_eWo-SgUP+KP5a5qZsUViruAesMsr9POg@mail.gmail.com>
Message-ID: <988c6f80-52e6-d59c-79f9-3e778ee8b7a5@treenet.co.nz>

On 01/06/17 14:15, Masha Lifshin wrote:
> Thank you, very helpful.  Some more clarifying questions for you.
>
> Sorry for the imprecise language, I mean not interception but rather 
> accepting connections to that port.  Our browsers will be explicitly 
> configured to connect our proxy, so I believe that is not interception?

Yes, however for HTTP/1.x it is also not the same traffic syntax as port 
80 or 443. It is a proxy specific syntax which port 3128 is registered for.


>
> If we want to only allow encrypted traffic between the browser and 
> proxy, does that mean we'd only want to use the following line from 
> your example?

Depends on what you mean by encrypted in context of the proxy syntax.


>
> # HTTPS proxy; clients establish TLS connections to 31443 (your item #1)
> https_port 31443 ssl-bump ...
>
> Once a handshake is done and tls connection is established here, would 
> it be possible to have all http and https traffic from the browser go 
> through 31443?  So squid would not need to have ports 80 and 443 open?
>

Think about it as two separate things.

  1) the transport used to get HTTP(S) to the proxy.
   That can be TCP (http_port) or TLS (https_port).

  2) the protocol the browser is talking to some origin on the other 
side of the proxy.
  That can be HTTP or HTTPS (aka HTTP-over-TLS).

These layers stack. So Squid supports:

http_port 3128
# HTTP-over-TCP
# HTTPS-over-TCP (aka HTTP-over-TLS-over-TCP)

https_port 3129
# HTTP-over-TLS   (aka HTTP-over-TLS-over-TCP)
# HTTPS-over-TLS (aka HTTP-over-TLS-over-TLS-over-TCP)


SSL-Bump is about decrypting the HTTPS traffic destined to some origin 
(not the proxy itself), it has effectively no relation to the transport 
(TCP or TLS) used to send that traffic to the proxy. ie. It peeks down 
to get the HTTP message layer in those 'aka' stacks.


Amos



From squid3 at treenet.co.nz  Thu Jun  1 12:15:25 2017
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Fri, 2 Jun 2017 00:15:25 +1200
Subject: [squid-users] request future option
In-Reply-To: <1496315211819-4682648.post@n4.nabble.com>
References: <1496315211819-4682648.post@n4.nabble.com>
Message-ID: <b583b16d-7e81-8107-5021-fdafd2be88f7@treenet.co.nz>

On 01/06/17 23:06, joseph wrote:
> hi dev. guys  i dont know if its ok to post here or in squid or ecap or i-cap
> my proposal is  simple to understand
> to be able to control  qos_flows tos local-hit=0x?? miss=0??
> from adapter ecap or i-cap
> example  i have file in locale separate drive  wen the code in adapter
> detect and send that file
> i need to control the qos bit  send  hit  or miss
> tks in advanced.

qos_flows HIT/MISS is whether the Squid cache contains the object.

Why are you doing caching buried inside an adapter?

Amos



From chip_pop at hotmail.com  Thu Jun  1 12:11:45 2017
From: chip_pop at hotmail.com (joseph)
Date: Thu, 1 Jun 2017 05:11:45 -0700 (PDT)
Subject: [squid-users] request future option
In-Reply-To: <b583b16d-7e81-8107-5021-fdafd2be88f7@treenet.co.nz>
References: <1496315211819-4682648.post@n4.nabble.com>
 <b583b16d-7e81-8107-5021-fdafd2be88f7@treenet.co.nz>
Message-ID: <1496319105333-4682651.post@n4.nabble.com>

its privet file wen matching  url
i need to control  qos_flows tos local-hit=0x??  to a diferent bit then
squid  if it will be done



--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/request-future-option-tp4682648p4682651.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From squid3 at treenet.co.nz  Thu Jun  1 12:29:40 2017
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Fri, 2 Jun 2017 00:29:40 +1200
Subject: [squid-users] failed to bump Twitter
In-Reply-To: <1759077679.133810.1496267351414@mail.yahoo.com>
References: <1759077679.133810.1496267351414.ref@mail.yahoo.com>
 <1759077679.133810.1496267351414@mail.yahoo.com>
Message-ID: <267bedf7-ad6c-1974-7d7d-0fd3d7f05eb2@treenet.co.nz>

On 01/06/17 09:49, Vieri wrote:
> Hi,
>
> I can't seem to be able to bump Twitter.
>
> Whenever a client tries to browse https://twitter.com there's a connection refusal error page (111).
>
> Any clue as to what I could try?

Squid is simply not able to make outbound TCP connections to twitter.com 
(which according to your OS is hosted by 199.16.156.6). Without a TCP 
connection it cannot get the TLS cert details to use when bumping.


> # cat /var/log/squid/access.test.log
>
> 1496266616.296    200 10.215.144.48 TAG_NONE/200 0 CONNECT 199.16.156.6:443 - ORIGINAL_DST/199.16.156.6 -
> 1496266616.322      2 10.215.144.48 TAG_NONE/503 3902 GET https://twitter.com/ - HIER_NONE/- text/html
>
> # cat /var/log/squid/cache.test.log
>
> 2017/05/31 23:36:56.095 kid1| 11,2| client_side.cc(2345) parseHttpRequest: HTTP Client local=199.16.156.6:443 remote=10.215.144.48 FD 13 flags=17
> 2017/05/31 23:36:56.095 kid1| 11,2| client_side.cc(2346) parseHttpRequest: HTTP Client REQUEST:
> ---------
> CONNECT 199.16.156.6:443 HTTP/1.1
> Host: 199.16.156.6:443
>
>
> ----------

> 2017/05/31 23:36:56.293 kid1| 5,5| ConnOpener.cc(365) doConnect: local=10.215.144.48 remote=199.16.156.6:443 flags=25: * - ERR tried too many times already.

> 2017/05/31 23:36:56.293 kid1| 17,3| FwdState.cc(415) fail: ERR_CONNECT_FAIL "Service Unavailable"
> 199.16.156.6:443
> 2017/05/31 23:36:56.293 kid1| 17,3| FwdState.cc(616) retryOrBail: re-forwarding (0 tries, 0 secs)
> 2017/05/31 23:36:56.293 kid1| 17,3| FwdState.cc(386) startConnectionOrFail: 199.16.156.6:443
> 2017/05/31 23:36:56.293 kid1| 17,3| FwdState.cc(403) startConnectionOrFail: Connection failed: 199.16.156.6:443


Amos



From erdosain9 at gmail.com  Thu Jun  1 13:10:44 2017
From: erdosain9 at gmail.com (erdosain9)
Date: Thu, 1 Jun 2017 06:10:44 -0700 (PDT)
Subject: [squid-users] this config is ok? is ok the order?
In-Reply-To: <1f29d3f2-3d4d-c55d-ad3b-4d06748b857f@treenet.co.nz>
References: <1496157812155-4682631.post@n4.nabble.com>
 <1f29d3f2-3d4d-c55d-ad3b-4d06748b857f@treenet.co.nz>
Message-ID: <1496322644043-4682653.post@n4.nabble.com>

"If I assume that its doing what you want there are still two major
issues that can be seen."................. i think it was...

"1) Mixing interception and authentication (ssl-bump is a type of
interception, at least on the https:// traffic). Intercepted messages
cannot be authenticated - though there are some workarounds in place for
ssl-bump to authenticate the CONNECT tunnel and label all the bumped
traffic with that username."

how it's that?, maybe i wrong (probably) but, for example a connection to
youtube, it is ssl, and i see (in access.log, who do that (its
authenticate). So? im wrong no? why?

2)........ we have a dns server (192.168.1.222) that just have our internal
dns names and then points to 8.8.8.8... that (192.168.1.222) dns server
would it not be useful either?

sorry for ignorance and thanks



--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/this-config-is-ok-is-ok-the-order-tp4682631p4682653.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From squid3 at treenet.co.nz  Thu Jun  1 15:17:14 2017
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Fri, 2 Jun 2017 03:17:14 +1200
Subject: [squid-users] this config is ok? is ok the order?
In-Reply-To: <1496322644043-4682653.post@n4.nabble.com>
References: <1496157812155-4682631.post@n4.nabble.com>
 <1f29d3f2-3d4d-c55d-ad3b-4d06748b857f@treenet.co.nz>
 <1496322644043-4682653.post@n4.nabble.com>
Message-ID: <3ef299a1-df56-4913-38f7-f9627cded853@treenet.co.nz>

On 02/06/17 01:10, erdosain9 wrote:
> "If I assume that its doing what you want there are still two major
> issues that can be seen."................. i think it was...
>
> "1) Mixing interception and authentication (ssl-bump is a type of
> interception, at least on the https:// traffic). Intercepted messages
> cannot be authenticated - though there are some workarounds in place for
> ssl-bump to authenticate the CONNECT tunnel and label all the bumped
> traffic with that username."
>
> how it's that?, maybe i wrong (probably) but, for example a connection to
> youtube, it is ssl, and i see (in access.log, who do that (its
> authenticate). So? im wrong no? why?

That is the hack workaround doing its thing. Squid is authenticating the 
CONNECT message, then simply reporting that authenticated username for 
all the bumped https:// log entries. In its current form/code it sort-of 
works most of the time, but can break (start rejecting everything) if 
there is ever even a slightest wobble in the credentials validity while 
the bump of that tunnel is ongoing.


> 2)........ we have a dns server (192.168.1.222) that just have our internal
> dns names and then points to 8.8.8.8... that (192.168.1.222) dns server
> would it not be useful either?

The core issue is the speed at which that service rotates its response 
IP lists, which is directly related to each request going to entirely 
different server in their farm. Simply having a single (and maybe more 
sane regarding TTLs) resolver as a networks focal point for the traffic 
before it reaches out to the Google service seems to bring sanity back 
to the performance.

Amos



From rentorbuy at yahoo.com  Thu Jun  1 15:20:24 2017
From: rentorbuy at yahoo.com (Vieri)
Date: Thu, 1 Jun 2017 15:20:24 +0000 (UTC)
Subject: [squid-users] squid sslbump and certificates
In-Reply-To: <028101d2da60$b51b0c50$1f5124f0$@ngtech.co.il>
References: <1666824574.1494153.1496061384978.ref@mail.yahoo.com>
 <1666824574.1494153.1496061384978@mail.yahoo.com>
 <DB6PR0401MB2680BA1E032D170A3D770D3F8FF30@DB6PR0401MB2680.eurprd04.prod.outlook.com>
 <548387612.1952806.1496095904224@mail.yahoo.com>
 <a982180a-e389-9d05-9a74-6b951064b9c7@treenet.co.nz>
 <820815053.47827.1496265847163@mail.yahoo.com>
 <028101d2da60$b51b0c50$1f5124f0$@ngtech.co.il>
Message-ID: <1390109515.397665.1496330424813@mail.yahoo.com>


________________________________
From: Eliezer Croitoru <eliezer at ngtech.co.il>
>
> What OS?


Linux 4.8.17-hardened


From rentorbuy at yahoo.com  Thu Jun  1 15:21:35 2017
From: rentorbuy at yahoo.com (Vieri)
Date: Thu, 1 Jun 2017 15:21:35 +0000 (UTC)
Subject: [squid-users] failed to bump Twitter
In-Reply-To: <267bedf7-ad6c-1974-7d7d-0fd3d7f05eb2@treenet.co.nz>
References: <1759077679.133810.1496267351414.ref@mail.yahoo.com>
 <1759077679.133810.1496267351414@mail.yahoo.com>
 <267bedf7-ad6c-1974-7d7d-0fd3d7f05eb2@treenet.co.nz>
Message-ID: <1637527701.424756.1496330495976@mail.yahoo.com>


________________________________
From: Amos Jeffries <squid3 at treenet.co.nz>
>
> Squid is simply not able to make outbound TCP connections to twitter.com 

> (which according to your OS is hosted by 199.16.156.6). 


It seems to be a DNS issue.

Thanks

Vieri


From squid3 at treenet.co.nz  Thu Jun  1 15:43:07 2017
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Fri, 2 Jun 2017 03:43:07 +1200
Subject: [squid-users] request future option
In-Reply-To: <1496319105333-4682651.post@n4.nabble.com>
References: <1496315211819-4682648.post@n4.nabble.com>
 <b583b16d-7e81-8107-5021-fdafd2be88f7@treenet.co.nz>
 <1496319105333-4682651.post@n4.nabble.com>
Message-ID: <a51df170-0fc9-636d-d17b-a52eb154fc9f@treenet.co.nz>

On 02/06/17 00:11, joseph wrote:
> its privet file wen matching  url
> i need to control  qos_flows tos local-hit=0x??  to a diferent bit then
> squid  if it will be done

Okay. You probably want to use clientside_tos then instead of qos_flows.
<http://www.squid-cache.org/Doc/config/clientside_tos/>

Amos



From A.Madonna at rechtspraak.nl  Thu Jun  1 16:09:21 2017
From: A.Madonna at rechtspraak.nl (Madonna, A. (spir-it))
Date: Thu, 1 Jun 2017 16:09:21 +0000
Subject: [squid-users] squid proxy 3.5 redhat 7.3
Message-ID: <382fd8bbff524df9971d461b69ae4eeb@rechtspraak.nl>

Hello,

Due to all the documentation on the internet, we still do not have the answer to the question or whether we can use ssl_bump https traffic to intercept https traffic using a cache_peer.

So our question is, can we use ssl_bump to intercept https traffic with a parent proxy (cache_peer).

Config example:

http_port 10.**********:8080 ssl-bump cert=/etc/squid/ssl_cert/myCA.pem generate-host-certificates=on options=NO_SSLv2,NO_SSLv3,SINGLE_DH_USE,SINGLE_ECDH_USE dynamic_cert_mem_cache_size=4MB
http_port 127.0.0.1:8080 intercept ssl-bump cert=/etc/squid/ssl_cert/myCA.pem generate-host-certificates=on options=NO_SSLv2,NO_SSLv3,SINGLE_DH_USE,SINGLE_ECDH_USE dynamic_cert_mem_cache_size=4MB
https_port 127.0.0.1:8081 intercept ssl-bump cert=/etc/squid/ssl_cert/myCA.pem generate-host-certificates=on options=NO_SSLv2,NO_SSLv3,SINGLE_DH_USE,SINGLE_ECDH_USE dynamic_cert_mem_cache_size=4MB
sslcrtd_program /usr/lib64/squid/ssl_crtd -s /var/lib/ssl_db -M 4MB
sslcrtd_children 32 startup=5 idle=1
acl step1 at_step SslBump1

ssl_bump peek step1

cache_peer ************** parent 8080 0 no-query no-netdb-exchange no-digest name=*******
never_direct allow all


squid -v squid-3.5.20-2.el7.x86_64

configure options:  '--build=x86_64-redhat-linux-gnu' '--host=x86_64-redhat-linux-gnu' '--program-prefix=' '--prefix=/usr' '--exec-prefix=/usr' '--bindir=/usr/bin' '--sbindir=/usr/sbin' '--sysconfdir=/etc' '--datadir=/usr/share' '--includedir=/usr/include' '--libdir=/usr/lib64' '--libexecdir=/usr/libexec' '--sharedstatedir=/var/lib' '--mandir=/usr/share/man' '--infodir=/usr/share/info' '--disable-strict-error-checking' '--exec_prefix=/usr' '--libexecdir=/usr/lib64/squid' '--localstatedir=/var' '--datadir=/usr/share/squid' '--sysconfdir=/etc/squid' '--with-logdir=$(localstatedir)/log/squid' '--with-pidfile=$(localstatedir)/run/squid.pid' '--disable-dependency-tracking' '--enable-eui' '--enable-follow-x-forwarded-for' '--enable-auth' '--enable-auth-basic=DB,LDAP,MSNT-multi-domain,NCSA,NIS,PAM,POP3,RADIUS,SASL,SMB,SMB_LM,getpwnam' '--enable-auth-ntlm=smb_lm,fake' '--enable-auth-digest=file,LDAP,eDirectory' '--enable-auth-negotiate=kerberos' '--enable-external-acl-helpers=file_userip,LDAP_group,time_quota,session,unix_group,wbinfo_group' '--enable-cache-digests' '--enable-cachemgr-hostname=localhost' '--enable-delay-pools' '--enable-epoll' '--enable-ident-lookups' '--enable-linux-netfilter' '--enable-removal-policies=heap,lru' '--enable-snmp' '--enable-ssl-crtd' '--enable-storeio=aufs,diskd,ufs' '--enable-wccpv2' '--enable-esi' '--enable-ecap' '--with-aio' '--with-default-user=squid' '--with-dl' '--with-openssl' '--with-pthreads' '--disable-arch-native' '--disable-icap-client' 'build_alias=x86_64-redhat-linux-gnu' 'host_alias=x86_64-redhat-linux-gnu' 'CFLAGS=-O2 -g -pipe -Wall -Wp,-D_FORTIFY_SOURCE=2 -fexceptions -fstack-protector-strong --param=ssp-buffer-size=4 -grecord-gcc-switches   -m64 -mtune=generic -fpie' 'LDFLAGS=-Wl,-z,relro  -pie -Wl,-z,relro -Wl,-z,now' 'CXXFLAGS=-O2 -g -pipe -Wall -Wp,-D_FORTIFY_SOURCE=2 -fexceptions -fstack-protector-strong --param=ssp-buffer-size=4 -grecord-gcc-switches   -m64 -mtune=generic -fpie' 'PKG_CONFIG_PATH=:/usr/lib64/pkgconfig:/usr/share/pkgconfig'


kind regards,

Sandro

________________________________

Informatie van de Raad voor de rechtspraak, de rechtbanken, de gerechtshoven en de bijzondere colleges vindt u op www.rechtspraak.nl.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20170601/c431b918/attachment.htm>

From rousskov at measurement-factory.com  Thu Jun  1 16:48:59 2017
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Thu, 1 Jun 2017 10:48:59 -0600
Subject: [squid-users] squid proxy 3.5 redhat 7.3
In-Reply-To: <382fd8bbff524df9971d461b69ae4eeb@rechtspraak.nl>
References: <382fd8bbff524df9971d461b69ae4eeb@rechtspraak.nl>
Message-ID: <46ea5daf-a067-0090-d22d-fd3d04b803e8@measurement-factory.com>

On 06/01/2017 10:09 AM, Madonna, A. (spir-it) wrote:
> can we use ssl_bump to intercept https traffic with a parent proxy (cache_peer).

IIRC, you may be able to use limited SslBump features, but not the full
SslBump functionality: Peeking or staring at the origin server through a
cache_peer is not supported (yet).


> ssl_bump peek step1
> cache_peer ... parent 8080 0 no-query no-netdb-exchange no-digest

Bugs notwithstanding, the above combination should work because peeking
at step1 does not require communication with a cache_peer and splicing
at step2 should follow the regular (non-SslBump) tunneling path for
CONNECTs, where modern Squids do support cache peers.


I recommend that you make everything work without a cache_peer and then
add a cache_peer.

Alex.


From rousskov at measurement-factory.com  Thu Jun  1 17:08:29 2017
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Thu, 1 Jun 2017 11:08:29 -0600
Subject: [squid-users] request future option
In-Reply-To: <1496315211819-4682648.post@n4.nabble.com>
References: <1496315211819-4682648.post@n4.nabble.com>
Message-ID: <42d8e0b2-006f-1b80-d3bd-e1ac439e9857@measurement-factory.com>

On 06/01/2017 05:06 AM, joseph wrote:
> to be able to control  qos_flows tos local-hit=0x?? miss=0??
> from adapter ecap or i-cap

> example  i have file in locale separate drive  wen the code in adapter
> detect and send that file
> i need to control the qos bit  send  hit  or miss

I believe what you want is supported in modern Squids:

1. Your eCAP adapter can set transaction annotations.
   https://answers.launchpad.net/ecap/+question/253497

2. Your clientside_tos rules can select the right TOS marks
   based on those annotations using the "note" ACL

IIRC, ICAP code does not support setting transaction annotations yet.


HTH,

Alex.
P.S. Thanks to Amos for pointing out that you want clientside_tos
instead of qos_flows.


From rousskov at measurement-factory.com  Thu Jun  1 17:16:51 2017
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Thu, 1 Jun 2017 11:16:51 -0600
Subject: [squid-users] this config is ok? is ok the order?
In-Reply-To: <3ef299a1-df56-4913-38f7-f9627cded853@treenet.co.nz>
References: <1496157812155-4682631.post@n4.nabble.com>
 <1f29d3f2-3d4d-c55d-ad3b-4d06748b857f@treenet.co.nz>
 <1496322644043-4682653.post@n4.nabble.com>
 <3ef299a1-df56-4913-38f7-f9627cded853@treenet.co.nz>
Message-ID: <b286bc8e-2ca0-fae0-7327-0de4a8722fe3@measurement-factory.com>

On 06/01/2017 09:17 AM, Amos Jeffries wrote:
> On 02/06/17 01:10, erdosain9 wrote:
>> "If I assume that its doing what you want there are still two major
>> issues that can be seen."................. i think it was...
>>
>> "1) Mixing interception and authentication (ssl-bump is a type of
>> interception, at least on the https:// traffic). Intercepted messages
>> cannot be authenticated - though there are some workarounds in place for
>> ssl-bump to authenticate the CONNECT tunnel and label all the bumped
>> traffic with that username."

Bumped messages cannot be proxy-authenticated but the CONNECT tunnels
that carry bumped messages can be, and such proxy authentication does
not violate any rules or principles. It is perfectly fine to use.
Furthermore, logging the authenticated tunnel user when logging
transactions inside that tunnel is the right thing to do IMO.


>> how it's that?, maybe i wrong (probably) but, for example a connection to
>> youtube, it is ssl, and i see (in access.log, who do that (its
>> authenticate).
> 
> That is the hack workaround doing its thing. Squid is authenticating the
> CONNECT message, then simply reporting that authenticated username for
> all the bumped https:// log entries. 

FWIW, I do not think this is a hack. It is exactly what Squid should be
doing in this context. There may be bugs in the implementation of that
functionality, of course, but the functionality itself is a legitimate
feature, not a workaround.

Alex.


From rousskov at measurement-factory.com  Thu Jun  1 17:29:23 2017
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Thu, 1 Jun 2017 11:29:23 -0600
Subject: [squid-users] Help troubleshooting proxy<-->client https
In-Reply-To: <CA+8Eki3DcvqkXaH7S_eWo-SgUP+KP5a5qZsUViruAesMsr9POg@mail.gmail.com>
References: <CA+8Eki1-a7jqQzGrM0XYwDz6d-VYi1Q6PPNK5g34HpdmF2-_Sg@mail.gmail.com>
 <abf01a59-b7f0-9b51-494f-9707a69a754f@measurement-factory.com>
 <CA+8Eki3DcvqkXaH7S_eWo-SgUP+KP5a5qZsUViruAesMsr9POg@mail.gmail.com>
Message-ID: <f964fc92-c8b1-9053-14e3-72c510d71a43@measurement-factory.com>

On 05/31/2017 08:15 PM, Masha Lifshin wrote:
> 
> Sorry for the imprecise language, I mean not interception but rather
> accepting connections to that port.  Our browsers will be explicitly
> configured to connect our proxy, so I believe that is not interception?

You are correct. It is explicit proxying, not interception.


> If we want to only allow encrypted traffic between the browser and
> proxy, does that mean we'd only want to use the following line from your
> example?
> 
> # HTTPS proxy; clients establish TLS connections to 31443 (your item #1)
> https_port 31443 ssl-bump ...

Yes.

Again, the actual port number does not matter here as long as the
browser is configured to use the same port. I would not use 3128, 80, or
443 because, to many people, those numbers mean something different than
an "HTTPS proxy" port. My 31443 suggestion is based on an
unregistered(?) port that "looks like" a combination of registered or
well-known ports 3128 and 443. FWIW, many products use ports ending with
443 for similar purposes AFAICT.


> Once a handshake is done and tls connection is established here, would
> it be possible to have all http and https traffic from the browser go
> through 31443?  So squid would not need to have ports 80 and 443 open?

Yes, provided the browser supports HTTPS proxies, of course. Several
popular browsers do, but not all HTTP clients support HTTPS proxies.
Also, it is difficult to configure the popular browsers to do what you
want; the required configuration changes are _not_ available through the
regular browser configuration interface. After you figure browser
configuration out, please consider writing a Squid wiki entry to
document your findings.


Thank you,

Alex.



> On Wed, May 31, 2017 at 5:10 PM, Alex Rousskov wrote:
> 
>     On 05/31/2017 02:42 PM, Masha Lifshin wrote:
> 
>     > What I am trying to achieve is
> 
> 
>     > 1. an https connection between the client and squid proxy, as well as
> 
>     > 2. listen on port 80 for http traffic,
> 
>     > 3. on port 443 for ssl traffic, and
> 
>     > 4. apply ssl-bump to the ssl traffic.
> 
> 
>     If I parsed your query correctly, and by "listen" you mean "intercept",
>     and you want to apply SslBump to proxied SSL traffic on all ports, then
>     it looks like you will need three ports, each doing ssl-bump. In other
>     words, instead of
> 
>     > http_port 80 ssl-bump cert=some.cert.pem ...
>     > https_port 443 cert=other.cert.pem ...
> 
>     You will need something like this:
> 
>     # HTTPS proxy; clients establish TLS connections to 31443 (your item #1)
>     https_port 31443 ssl-bump ...
> 
>     # HTTP-intercepting proxy (your item #2)
>     http_port 80 intercept ssl-bump ...
> 
>     # SSL-intercepting proxy (your item #3)
>     https_port 443 intercept ssl-bump ...
> 
>     You may need "tproxy" instead of "intercept", depending on how you are
>     intercepting/forwarding traffic.
> 
>     The actual port numbers do no matter.
> 
> 
>     > Also wondering what, if any, are the security issues with using port 80
>     > for the http traffic?
> 
>     Anybody with access to that traffic will be able to easily see
>     everything and, with a monumental effort, potentially occasionally
>     modify unencrypted traffic, including plain CONNECT requests that
>     establish secure channels.
> 
> 
>     HTH,
> 
>     Alex.
> 
> 
>     > On Fri, May 26, 2017 at 7:19 AM, Alex Rousskov wrote:
>     >
>     >     On 05/26/2017 12:00 AM, Masha Lifshin wrote:
>     >     > I have added an https_port directive
>     >     > to squid.conf, but it must be misconfigured.
>     >
>     >     > http_port 172.30.0.67:443 <http://172.30.0.67:443>
>     <http://172.30.0.67:443> ...
>     >     > https_port 172.30.0.67:443 <http://172.30.0.67:443>
>     <http://172.30.0.67:443> ...
>     >
>     >     You are right -- your Squid is misconfigured. You cannot use
>     the same
>     >     address for two ports. Unfortunately, Squid thinks that port
>     binding
>     >     errors are a minor inconvenience and continues running after
>     logging an
>     >     error message (that looks like many other benign error messages).
>     >
>     >     Changing one of the ports will solve the "same address" problem
>     >     described above.
>     >
>     >     Do not use port 443 for http_port. It makes triage extremely
>     confusing
>     >     because port 443 usually implies SSL. Consider using port 3128
>     instead.
>     >
>     >
>     >     HTH,
>     >
>     >     Alex.
>     >
>     >
>     >
> 
> 
> 



From chip_pop at hotmail.com  Thu Jun  1 17:29:46 2017
From: chip_pop at hotmail.com (joseph)
Date: Thu, 1 Jun 2017 10:29:46 -0700 (PDT)
Subject: [squid-users] request future option
In-Reply-To: <a51df170-0fc9-636d-d17b-a52eb154fc9f@treenet.co.nz>
References: <1496315211819-4682648.post@n4.nabble.com>
 <b583b16d-7e81-8107-5021-fdafd2be88f7@treenet.co.nz>
 <1496319105333-4682651.post@n4.nabble.com>
 <a51df170-0fc9-636d-d17b-a52eb154fc9f@treenet.co.nz>
Message-ID: <1496338186615-4682663.post@n4.nabble.com>

well  another good idea
	clientside_tos ds-field [!]aclname ...
--------------------------------------
	acl normal_service_net src 10.0.0.0/24
	acl good_service_net src 10.0.1.0/24
	clientside_tos 0x00 normal_service_net
	clientside_tos 0x20 good_service_net

almost got me never seen that befor but  it will be wondeful if  normal acl
regex  matching url
example: its good to send mark on acl base for limiting the speed in router
to thgose DSCP
or puch hi speed to those as that example
acl match_updates url_regex -i  \.picaso\.org.*\.avi
clientside_tos 0x20 match_updates

but wat i need is diferent  if a file come from ecap the adapter will send
command to squid to force dscp tos  to a value that in adapter itself like

virtual void SetTos(std::string &value);
std::string dscpTmp = 0x50;
SetTos(dscpTmp );

that wen the file sent from the adapter set tos to xx
so i can tell if the file exist or not
tks

 




--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/request-future-option-tp4682648p4682663.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From rousskov at measurement-factory.com  Thu Jun  1 18:24:10 2017
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Thu, 1 Jun 2017 12:24:10 -0600
Subject: [squid-users] request future option
In-Reply-To: <1496338186615-4682663.post@n4.nabble.com>
References: <1496315211819-4682648.post@n4.nabble.com>
 <b583b16d-7e81-8107-5021-fdafd2be88f7@treenet.co.nz>
 <1496319105333-4682651.post@n4.nabble.com>
 <a51df170-0fc9-636d-d17b-a52eb154fc9f@treenet.co.nz>
 <1496338186615-4682663.post@n4.nabble.com>
Message-ID: <2a55721b-3fa9-e864-bd16-e8351ac27260@measurement-factory.com>

On 06/01/2017 11:29 AM, joseph wrote:
> if a file come from ecap the adapter will send
> command to squid to force dscp tos  to a value that in adapter itself like
> 
> virtual void SetTos(std::string &value);
> std::string dscpTmp = 0x50;
> SetTos(dscpTmp );

You cannot convert an eCAP-set annotation into a TOS mark, but you can
write an ACL that will detect a specific eCAP-set annotation value and
then use that ACL to set the corresponding mark. If you have 100
different TOS mark values, it will be a little tedious (without
automation) but if you only have a few, then no automation is needed:

  acl tosMark50 note tosMark 50
  acl tosMark60 note tosMark 60
  ...

  clientside_tos 0x50 toMark50
  clientside_tos 0x60 toMark60
  ...

Alex.


From chip_pop at hotmail.com  Thu Jun  1 18:24:57 2017
From: chip_pop at hotmail.com (joseph)
Date: Thu, 1 Jun 2017 11:24:57 -0700 (PDT)
Subject: [squid-users] request future option
In-Reply-To: <2a55721b-3fa9-e864-bd16-e8351ac27260@measurement-factory.com>
References: <1496315211819-4682648.post@n4.nabble.com>
 <b583b16d-7e81-8107-5021-fdafd2be88f7@treenet.co.nz>
 <1496319105333-4682651.post@n4.nabble.com>
 <a51df170-0fc9-636d-d17b-a52eb154fc9f@treenet.co.nz>
 <1496338186615-4682663.post@n4.nabble.com>
 <2a55721b-3fa9-e864-bd16-e8351ac27260@measurement-factory.com>
Message-ID: <1496341497176-4682665.post@n4.nabble.com>

tks guys



--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/request-future-option-tp4682648p4682665.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From rousskov at measurement-factory.com  Thu Jun  1 19:26:22 2017
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Thu, 1 Jun 2017 13:26:22 -0600
Subject: [squid-users] Help troubleshooting proxy<-->client https
In-Reply-To: <f964fc92-c8b1-9053-14e3-72c510d71a43@measurement-factory.com>
References: <CA+8Eki1-a7jqQzGrM0XYwDz6d-VYi1Q6PPNK5g34HpdmF2-_Sg@mail.gmail.com>
 <abf01a59-b7f0-9b51-494f-9707a69a754f@measurement-factory.com>
 <CA+8Eki3DcvqkXaH7S_eWo-SgUP+KP5a5qZsUViruAesMsr9POg@mail.gmail.com>
 <f964fc92-c8b1-9053-14e3-72c510d71a43@measurement-factory.com>
Message-ID: <bc105ba2-11fd-d028-35c8-b8e7ef8a6191@measurement-factory.com>

On 06/01/2017 11:29 AM, Alex Rousskov wrote:
> On 05/31/2017 08:15 PM, Masha Lifshin wrote:
>> If we want to only allow encrypted traffic between the browser and
>> proxy, does that mean we'd only want to use the following line from your
>> example?

>> # HTTPS proxy; clients establish TLS connections to 31443 (your item #1)
>> https_port 31443 ssl-bump ...

> Yes.


* HTTPS proxy is a rarely used feature that works well for some.
* SslBump is a frequently used feature that works well enough for some.

Disclaimer: I do not know of anybody using the _combination_ of the
above two features, and I do not recall whether such a combination is
already supported. Please post once you figure it out.

Alex.


From A.Madonna at rechtspraak.nl  Fri Jun  2 07:37:54 2017
From: A.Madonna at rechtspraak.nl (Madonna, A. (spir-it))
Date: Fri, 2 Jun 2017 07:37:54 +0000
Subject: [squid-users] FW:  squid proxy 3.5 redhat 7.3
References: <382fd8bbff524df9971d461b69ae4eeb@rechtspraak.nl>
 <46ea5daf-a067-0090-d22d-fd3d04b803e8@measurement-factory.com> 
Message-ID: <448797274e5f40d781241fcc3e4b08e9@rechtspraak.nl>

Hello Alex,

Our setup is as follows:

Clients -> squid proxy -> internet.
This works with the config as previously mentioned.

Clients -> squid proxy (with cache_peer) -> Parent Proxy (not Squid) -> internet

Does not work.

However I've also setup the following:

Cleints -> Squid Proxy (with cache_peer) -> Parent Proxy (Squid Proxy) -> internet

This seems at least to work for http traffic, however, I don't see any HTTPS traffic coming into the Parent Proxy (Squid).

Now this morning I will do some more tcpdumping to see where that traffic is going, but maybe you can already shed some light on this?


Kind regards,

-----Oorspronkelijk bericht-----
Van: Alex Rousskov [mailto:rousskov at measurement-factory.com]
Verzonden: donderdag 1 juni 2017 18:49
Aan: Madonna, A. (spir-it) <A.Madonna at rechtspraak.nl>; squid-users at lists.squid-cache.org
Onderwerp: Re: [squid-users] squid proxy 3.5 redhat 7.3

On 06/01/2017 10:09 AM, Madonna, A. (spir-it) wrote:
> can we use ssl_bump to intercept https traffic with a parent proxy (cache_peer).

IIRC, you may be able to use limited SslBump features, but not the full SslBump functionality: Peeking or staring at the origin server through a cache_peer is not supported (yet).


> ssl_bump peek step1
> cache_peer ... parent 8080 0 no-query no-netdb-exchange no-digest

Bugs notwithstanding, the above combination should work because peeking at step1 does not require communication with a cache_peer and splicing at step2 should follow the regular (non-SslBump) tunneling path for CONNECTs, where modern Squids do support cache peers.


I recommend that you make everything work without a cache_peer and then add a cache_peer.

Alex.


________________________________

Informatie van de Raad voor de rechtspraak, de rechtbanken, de gerechtshoven en de bijzondere colleges vindt u op www.rechtspraak.nl.


From rousskov at measurement-factory.com  Fri Jun  2 15:59:12 2017
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Fri, 2 Jun 2017 09:59:12 -0600
Subject: [squid-users] FW: squid proxy 3.5 redhat 7.3
In-Reply-To: <448797274e5f40d781241fcc3e4b08e9@rechtspraak.nl>
References: <382fd8bbff524df9971d461b69ae4eeb@rechtspraak.nl>
 <46ea5daf-a067-0090-d22d-fd3d04b803e8@measurement-factory.com>
 <448797274e5f40d781241fcc3e4b08e9@rechtspraak.nl>
Message-ID: <5ac71a9d-1fd6-7a89-5f28-c5925189364e@measurement-factory.com>

On 06/02/2017 01:37 AM, Madonna, A. (spir-it) wrote:

> Clients -> squid proxy -> internet.
> This works with the config as previously mentioned.

OK.


> Clients -> squid proxy (with cache_peer) -> Parent Proxy (not Squid) -> internet
> Does not work.

Even for regular HTTP traffic and non-bumped HTTPS traffic? If that
traffic does not work, then you have misconfigured something or the
Parent Proxy is badly broken. There is nothing special in the above
setup as far as regular traffic is concerned.


> However I've also setup the following:
> 
> Cleints -> Squid Proxy (with cache_peer) -> Parent Proxy (Squid Proxy) -> internet
> 
> This seems at least to work for http traffic, however, I don't see any HTTPS traffic coming into the Parent Proxy (Squid).

Squid does not know who made the parent proxy. The fact that one
(presumably production-quality) proxy "does not work" and another "seems
to work" implies that something is seriously misconfigured in one or
both cases.


> Now this morning I will do some more tcpdumping to see where that traffic is going, but maybe you can already shed some light on this?

I cannot shed more light on problems described only as "does not work"
and "no traffic".

Alex.


> -----Oorspronkelijk bericht-----
> Van: Alex Rousskov [mailto:rousskov at measurement-factory.com]
> Verzonden: donderdag 1 juni 2017 18:49
> Aan: Madonna, A. (spir-it) <A.Madonna at rechtspraak.nl>; squid-users at lists.squid-cache.org
> Onderwerp: Re: [squid-users] squid proxy 3.5 redhat 7.3
> 
> On 06/01/2017 10:09 AM, Madonna, A. (spir-it) wrote:
>> can we use ssl_bump to intercept https traffic with a parent proxy (cache_peer).
> 
> IIRC, you may be able to use limited SslBump features, but not the full SslBump functionality: Peeking or staring at the origin server through a cache_peer is not supported (yet).
> 
> 
>> ssl_bump peek step1
>> cache_peer ... parent 8080 0 no-query no-netdb-exchange no-digest
> 
> Bugs notwithstanding, the above combination should work because peeking at step1 does not require communication with a cache_peer and splicing at step2 should follow the regular (non-SslBump) tunneling path for CONNECTs, where modern Squids do support cache peers.
> 
> 
> I recommend that you make everything work without a cache_peer and then add a cache_peer.
> 
> Alex.
> 
> 
> ________________________________
> 
> Informatie van de Raad voor de rechtspraak, de rechtbanken, de gerechtshoven en de bijzondere colleges vindt u op www.rechtspraak.nl.
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
> 



From rousskov at measurement-factory.com  Fri Jun  2 16:05:11 2017
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Fri, 2 Jun 2017 10:05:11 -0600
Subject: [squid-users] Help troubleshooting proxy<-->client https
In-Reply-To: <bc105ba2-11fd-d028-35c8-b8e7ef8a6191@measurement-factory.com>
References: <CA+8Eki1-a7jqQzGrM0XYwDz6d-VYi1Q6PPNK5g34HpdmF2-_Sg@mail.gmail.com>
 <abf01a59-b7f0-9b51-494f-9707a69a754f@measurement-factory.com>
 <CA+8Eki3DcvqkXaH7S_eWo-SgUP+KP5a5qZsUViruAesMsr9POg@mail.gmail.com>
 <f964fc92-c8b1-9053-14e3-72c510d71a43@measurement-factory.com>
 <bc105ba2-11fd-d028-35c8-b8e7ef8a6191@measurement-factory.com>
Message-ID: <446f67cb-5a57-c969-cb85-23573c662905@measurement-factory.com>

On 06/01/2017 01:26 PM, Alex Rousskov wrote:
> On 06/01/2017 11:29 AM, Alex Rousskov wrote:

> * HTTPS proxy is a rarely used feature that works well for some.
> * SslBump is a frequently used feature that works well enough for some.

> Disclaimer: I do not know of anybody using the _combination_ of the
> above two features, and I do not recall whether such a combination is
> already supported. Please post once you figure it out.

I just confirmed that Squid does _not_ support the above combination. An
https_port with an ssl-bump option requires either "tproxy" or
"intercept" mode, which are both incompatible with HTTPS proxy mode.
Until the above combination is supported, you have to pick between using
HTTPS proxy and using SslBump.

http://wiki.squid-cache.org/SquidFaq/AboutSquid#How_to_add_a_new_Squid_feature.2C_enhance.2C_of_fix_something.3F

Alex.


From elrico22 at msn.com  Fri Jun  2 17:38:07 2017
From: elrico22 at msn.com (Rick W)
Date: Fri, 2 Jun 2017 17:38:07 +0000
Subject: [squid-users] Squid + Shibboleth/SAML Authentication
Message-ID: <HK2PR0601MB1331F9F5BFD379C2756584C6DBF70@HK2PR0601MB1331.apcprd06.prod.outlook.com>

Hi,

Can you tell me if it's possible to configure squid to allow access to some websites protected by Shibboleth/SAML Authentication.

Without Squid, access works correctly.

In my access.log the connection stop after this line :

TCP_MISS/502 5050 POST http://abonnes.efl.fr/Shibboleth.sso/SAML/POST DIRECT/82.112.98.23 text/html



Thank You for your Help.


Regards.

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20170602/6fcf7cb0/attachment.htm>

From mlifshin at phantomdesign.com  Fri Jun  2 22:27:58 2017
From: mlifshin at phantomdesign.com (Masha Lifshin)
Date: Fri, 2 Jun 2017 15:27:58 -0700
Subject: [squid-users] Help troubleshooting proxy<-->client https
In-Reply-To: <446f67cb-5a57-c969-cb85-23573c662905@measurement-factory.com>
References: <CA+8Eki1-a7jqQzGrM0XYwDz6d-VYi1Q6PPNK5g34HpdmF2-_Sg@mail.gmail.com>
 <abf01a59-b7f0-9b51-494f-9707a69a754f@measurement-factory.com>
 <CA+8Eki3DcvqkXaH7S_eWo-SgUP+KP5a5qZsUViruAesMsr9POg@mail.gmail.com>
 <f964fc92-c8b1-9053-14e3-72c510d71a43@measurement-factory.com>
 <bc105ba2-11fd-d028-35c8-b8e7ef8a6191@measurement-factory.com>
 <446f67cb-5a57-c969-cb85-23573c662905@measurement-factory.com>
Message-ID: <CA+8Eki1=P85-OPeZLKEmmTUEzrc4eOrHPctqQqXQQg+iHgT2jA@mail.gmail.com>

Thank you very much Amos and Alex for the helpful explanations, high level
of detail, and for tracking down that this combo is not possible at this
time.

We're going to evaluate what to do next with this info.  I'll probably be
following up with more questions soon.
-M

On Fri, Jun 2, 2017 at 9:05 AM, Alex Rousskov <rousskov at measurement-factory.
com> wrote:

> On 06/01/2017 01:26 PM, Alex Rousskov wrote:
> > On 06/01/2017 11:29 AM, Alex Rousskov wrote:
>
> > * HTTPS proxy is a rarely used feature that works well for some.
> > * SslBump is a frequently used feature that works well enough for some.
>
> > Disclaimer: I do not know of anybody using the _combination_ of the
> > above two features, and I do not recall whether such a combination is
> > already supported. Please post once you figure it out.
>
> I just confirmed that Squid does _not_ support the above combination. An
> https_port with an ssl-bump option requires either "tproxy" or
> "intercept" mode, which are both incompatible with HTTPS proxy mode.
> Until the above combination is supported, you have to pick between using
> HTTPS proxy and using SslBump.
>
> http://wiki.squid-cache.org/SquidFaq/AboutSquid#How_to_add_a
> _new_Squid_feature.2C_enhance.2C_of_fix_something.3F
>
> Alex.
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20170602/a061ceeb/attachment.htm>

From lan at zato.ru  Sun Jun  4 07:27:11 2017
From: lan at zato.ru (alexander lunev)
Date: Sun, 4 Jun 2017 10:27:11 +0300
Subject: [squid-users] assertion failed: store.cc "EBIT_TEST(flags,
	ENTRY_ABORTED)"
Message-ID: <a9963434-bddb-ed7c-48cf-db97203906d4@zato.ru>

Hello everyone!
I have two almost identical cache servers, both FreeBSD 10.3, both 
running latest squid-3.2.25 from ports in transparent mode, one runs OK 
and another is throwing this error:


2017/06/04 10:19:08 kid1| storeLateRelease: released 0 objects
2017/06/04 10:19:19 kid1| assertion failed: store.cc:1086: 
"EBIT_TEST(flags, ENTRY_ABORTED)"

After this squid is exiting.

Beside some default configuration config contains:

http_port 127.0.0.1:3127
http_port  127.0.0.1:3128 intercept
https_port 127.0.0.1:3129 intercept ssl-bump 
options=ALL:NO_SSLv3:NO_SSLv2 connection-auth=off 
generate-host-certificates=on dynamic_cert_mem_cache_size=4MB 
cert=/usr/local/etc/squid/squid.pem key=/usr/local/etc/squid/squid.key

sslcrtd_program /usr/local/libexec/squid/ssl_crtd -s 
/usr/local/etc/squid/ssl_db -M 4MB
sslcrtd_children 35

cache deny all
url_rewrite_program /usr/local/bin/squidGuard -c 
/usr/local/etc/squid/squidGuard.conf


# Leave coredumps in the first cache dir
coredump_dir /var/squid/cache
#ssl_bump client-first all

always_direct allow all

acl step1 at_step SslBump1
acl ssldomains ssl::server_name "/usr/local/etc/squid/ssldomains.txt"
ssl_bump peek step1
ssl_bump bump ssldomains
ssl_bump splice all

sslproxy_cert_error allow all
sslproxy_flags DONT_VERIFY_PEER


Why is this and how it can be fixed?

-- 
Best regards


From ahmed.zaeem at netstream.ps  Mon Jun  5 10:08:12 2017
From: ahmed.zaeem at netstream.ps (--Ahmad--)
Date: Mon, 5 Jun 2017 13:08:12 +0300
Subject: [squid-users] squid 3.5.24 HTTPS/HTTP caching optimisation  Help
Message-ID: <D94BF1A4-596A-443E-B72F-B48EB79151D8@netstream.ps>

Hola Folks .
i have been trying to optimise caching website that is http and has many images in its content like cnn.com <http://cnn.com/>
and trying to cache https static website like https://www.alwatanvoice.com <https://www.alwatanvoice.com/>


i have squid enabled with https and i installed certificates on clients pcs :

i have  been fighting to have better HIT ration but it is vey poor .
i hope someone give me a hand and focous on caching images contents of the 2 websites below .

here is what i have in my config file :



########################################
visible_hostname V6Proxies

#
# Recommended minimum configuration:
#

# Example rule allowing access from your local networks.
# Adapt to list your (internal) IP networks from where browsing
# should be allowed
acl localnet src 10.0.0.0/8	# RFC1918 possible internal network
acl localnet src 172.16.0.0/12	# RFC1918 possible internal network
acl localnet src 192.168.0.0/16	# RFC1918 possible internal network
acl localnet src fc00::/7       # RFC 4193 local private network range
acl localnet src fe80::/10      # RFC 4291 link-local (directly plugged) machines

acl SSL_ports port 443
acl Safe_ports port 80		# http
acl Safe_ports port 21		# ftp
acl Safe_ports port 443		# https
acl Safe_ports port 70		# gopher
acl Safe_ports port 210		# wais
acl Safe_ports port 1025-65535	# unregistered ports
acl Safe_ports port 280		# http-mgmt
acl Safe_ports port 488		# gss-http
acl Safe_ports port 591		# filemaker
acl Safe_ports port 777		# multiling http
acl CONNECT method CONNECT
cache_mgr a at a
cachemgr_passwd a all
http_access allow localnet manager
#
# Recommended minimum Access Permission configuration:
#
# Deny requests to certain unsafe ports
http_access deny !Safe_ports

# Deny CONNECT to other than secure SSL ports
http_access deny CONNECT !SSL_ports

# Only allow cachemgr access from localhost
http_access allow localhost manager
http_access deny manager

# We strongly recommend the following be uncommented to protect innocent
# web applications running on the proxy server who think the only
# one who can access services on "localhost" is a local user
#http_access deny to_localhost

#
# INSERT YOUR OWN RULE(S) HERE TO ALLOW ACCESS FROM YOUR CLIENTS
#

# Example rule allowing access from your local networks.
# Adapt localnet in the ACL section to list your (internal) IP networks
# from where browsing should be allowed
http_access allow localnet
http_access allow localhost

# And finally deny all other access to this proxy
http_access deny all

# Squid normally listens to port 3128
http_port 3128
#######################################################################################################
https_port 3129 intercept ssl-bump generate-host-certificates=on dynamic_cert_mem_cache_size=4MB cert=/usr/local/squid/ssl_cert/myca.pem key=/usr/local/squid/ssl_cert/myca.pem
ssl_bump server-first all
sslcrtd_program /lib/squid/ssl_crtd -s /var/lib/ssl_db -M 4MB
sslcrtd_children 8 startup=1 idle=1
################################################################################################
#coredump_dir /usr/local/squid/var/cache/squid
###########################
# Uncomment and adjust the following to add a disk cache directory.
#cache_dir ufs /var/cache/squid 100 16 256

# Leave coredumps in the first cache dir
#coredump_dir /var/cache/squid

#
# Add any of your own refresh_pattern entries above these.

refresh_pattern ^http://*.cnn.com/* <http://*.cnn.com/*> 720 100% 4320
refresh_pattern ^https://*.cnn.com/* <https://*.cnn.com/*> 720 100% 4320
#############################
refresh_pattern ^http://*.alwatanvoice.com/* <http://*.alwatanvoice.com/*> 720 100% 4320
refresh_pattern ^https://*.alwatanvoice.com/* <https://*.alwatanvoice.com/*> 720 100% 4320
refresh_pattern ^ftp:		1440	20%	10080
refresh_pattern ^gopher:	1440	0%	1440
refresh_pattern -i (/cgi-bin/|\?) 0	0%	0
refresh_pattern .		0	20%	4320

http_port 3126 transparent

http_port 3128

#######################################

cache_swap_low 90
cache_swap_high 95
############################
cache_effective_user squid
cache_effective_group squid
memory_replacement_policy heap GDSF
cache_replacement_policy heap LFUDA
########################
cache_mem 5000 MB
maximum_object_size_in_memory 10 MB
#########################
logfile_rotate 2
max_filedescriptors 131072
###############################
#cache_dir ufs /root/cache3 600000 64 128
############
cache_dir aufs /var/cache/squid 600000 64 128
#######################################






Squid Cache: Version 3.5.24
Service Name: squid
configure options:  '--prefix=/usr' '--includedir=/include' '--mandir=/share/man' '--infodir=/share/info' '--sysconfdir=/etc' '--enable-cachemgr-hostname=Ahmad-Allzaeem' '--localstatedir=/var' '--libexecdir=/lib/squid' '--disable-maintainer-mode' '--disable-dependency-tracking' '--disable-silent-rules' '--srcdir=.' '--datadir=/usr/share/squid' '--sysconfdir=/etc/squid' '--mandir=/usr/share/man' '--enable-inline' '--enable-async-io=8' '--enable-storeio=ufs,aufs,diskd,rock' '--enable-removal-policies=lru,heap' '--enable-delay-pools' '--enable-cache-digests' '--enable-underscores' '--enable-icap-client' '--enable-follow-x-forwarded-for' '--enable-auth' '--enable-basic-auth-helpers=LDAP,MSNT,NCSA,PAM,SASL,SMB,YP,DB,POP3,getpwnam,squid_radius_auth,multi-domain-NTLM' '--enable-ntlm-auth-helpers=smb_lm' '--enable-digest-auth-helpers=ldap,password' '--enable-negotiate-auth-helpers=squid_kerb_auth' '--enable-esi' '--disable-translation' '--with-logdir=/var/log/squid' '--with-pidfile=/var/run/squid.pid' '--with-filedescriptors=11311072' '--with-large-files' '--with-default-user=squid' '--enable-linux-netfilter' '--enable-ltdl-convenience' '--enable-ssl' '--enable-ssl-crtd' '--enable-arp-acl' 'CXXFLAGS=-DMAXTCPLISTENPORTS=200000' '--with-openssl' '--enable-snmp' '--with-included-ltdl' '--disable-arch-native'
root at portablecloud-3010:/var/cache/squid#







Any optimisation hints ?

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20170605/3f95a374/attachment.htm>

From lyningg at gmail.com  Mon Jun  5 11:54:56 2017
From: lyningg at gmail.com (LIU Yaning)
Date: Mon, 5 Jun 2017 13:54:56 +0200
Subject: [squid-users] Squid issue of caching the m3u8 file
Message-ID: <CAEip7fzdjxuchvn3YUrdh1jnXOvfn=bUqkWLko_X_-rVq2Wn5Q@mail.gmail.com>

Dear All,

I would like to cache the .m3u8 file to be able to provide offline caching
service by Squid. The played HLS video streaming is the link as below:
http://qthttp.apple.com.edgesuite.net/1010qwoeiuryfg/0150_vod.m3u8

However, the .m3u8 file is not be cached probably because it is mentioned
as a no-cache, no-store, max-age=0 in the "Cache-Control" in the HTTP
header.

HTTP/1.1 200 OK

Server: Apache

ETag: "1d7168b4f49e75f76f3182f24bf075f6:1299516751"

Last-Modified: Mon, 07 Mar 2011 16:52:31 GMT

Expires: Fri, 02 Jun 2017 14:26:52 GMT

Cache-Control: max-age=0, no-cache, no-store

Pragma: no-cache

Date: Fri, 02 Jun 2017 14:26:52 GMT

Content-Length: 16046

Set-Cookie: AKID=77F9F1316ECCE780566608C5E514DE0A;expires=Fri, 26 Aug 2016
00:01:00 GMT; path=/; domain=qthttp.apple.com.edgesuite.net

Content-Type: application/x-mpegURL

Access-Control-Allow-Origin: *

I added a new rule for .m3u8 file in squid.conf, however, it is still not
working.

refresh_pattern -i \.(ts|m3u8)$ 120 90% 1000 override-expire
override-lastmod ignore-no-cache ignore-no-store

Does anyone know how to allow Squid caching the .m3u8 file? Thanks a lot in
advance.

Best Regards,
--
Yaning.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20170605/f8289063/attachment.htm>

From rentorbuy at yahoo.com  Mon Jun  5 12:49:25 2017
From: rentorbuy at yahoo.com (Vieri)
Date: Mon, 5 Jun 2017 12:49:25 +0000 (UTC)
Subject: [squid-users] squid ssl bump and Adobe Connect
References: <2085804462.2620283.1496666965861.ref@mail.yahoo.com>
Message-ID: <2085804462.2620283.1496666965861@mail.yahoo.com>

Hi,


I'm reposting this message because my previous email was too big.


I'm unable to connect to Adobe Connect through Squid TPROXY.

The URL is:

https://emeacmsd.acms.com/common/help/en/support/meeting_test.htm

# grep -v ^# squid.test.conf | grep -v ^$
http_access allow localhost manager
http_access deny manager
http_port 3227
http_port 3228 tproxy
https_port 3229 tproxy ssl-bump generate-host-certificates=on dynamic_cert_mem_cache_size=16MB cert=/etc/ssl/squid/proxyserver.pem
acl localnet src 10.0.0.0/8     # RFC1918 possible internal network
acl localnet src 192.168.0.0/16 # RFC1918 possible internal network
acl localnet src fc00::/7       # RFC 4193 local private network range
acl interceptedhttp myportname 3228
acl interceptedhttps myportname 3229
http_access deny interceptedhttp !localnet
http_access deny interceptedhttps !localnet
sslcrtd_program /usr/libexec/squid/ssl_crtd -s /var/lib/squid/ssl_db_test -M 16MB
sslcrtd_children 10
reply_header_access Alternate-Protocol deny all
ssl_bump stare all
ssl_bump bump all
cache_dir diskd /var/cache/squid.test 100 16 256
http_access allow localnet
http_access allow localhost
http_access deny all
coredump_dir /var/cache/squid
refresh_pattern ^ftp:           1440    20%     10080
refresh_pattern ^gopher:        1440    0%      1440
refresh_pattern -i (/cgi-bin/|\?) 0     0%      0
refresh_pattern .               0       20%     4320
pid_filename /run/squid.test.pid
access_log daemon:/var/log/squid/access.test.log squid
cache_log /var/log/squid/cache.test.log
debug_options rotate=1 ALL,5

# cat /var/log/squid/access.test.log

1496665078.340    223 10.215.145.187 TAG_NONE/200 0 CONNECT 216.58.201.142:443 - ORIGINAL_DST/216.58.201.142 -
1496665079.387   1003 10.215.145.187 TCP_MISS/200 4623 POST https://safebrowsing.google.com/safebrowsing/downloads? - ORIGINAL_DST/216.58.201.142 application/vnd.google.safebrowsing-update
1496665080.000    541 10.215.145.187 TAG_NONE/200 0 CONNECT 216.58.211.238:443 - ORIGINAL_DST/216.58.211.238 -
1496665080.129     85 10.215.145.187 TCP_MISS/200 550 GET https://safebrowsing-cache.google.com/safebrowsing/rd/ChVnb29nLWJhZGJpbnVybC1zaGF2YXI4AEACSgwIABCD9QcYg_UHIAE - ORIGINAL_DST/216.58.211.238 application/vnd.google.safebrowsing-chunk
1496665080.241    107 10.215.145.187 TCP_MISS/200 3069 GET https://safebrowsing-cache.google.com/safebrowsing/rd/ChNnb29nLW1hbHdhcmUtc2hhdmFyOABAAkoMCAEQy4YQGMuGECABSgwIARC3hhAYt4YQIAFKDAgBEKOFEBijhRAgAUoMCAEQ2IMQGNiDECABSgwIARCEghAYhIIQIAFKDAgBEOv5Dxjr-Q8gAUoMCAEQ9ugPGPboDyABSgwIARDg6A8Y4OgPIAFKDAgBEI3cDxiN3A8gAUoMCAEQitsPGIrbDyABSgwIARD42g8Y-NoPIAFKDAgBEITaDxiF2g8gAUoMCAEQ9NYPGPTWDyABSgwIARCc1Q8YnNUPIAFKDAgBELHLDxixyw8gAUoMCAEQmMoPGJjKDyABSgwIARDLyQ8Yy8kPIAFKDAgBEN7EDxjexA8gAUoMCAEQyb4PGMm-DyABSgwIARCkug8YpLoPIAFKDAgBEIG5DxiBuQ8gAUoMCAEQ-bgPGPm4DyABSgwIARC1uA8YtbgPIAFKDAgBEIq3DxiKtw8gAUoMCAEQobYPGKG2DyABSgwIARCDtg8Yg7YPIAFKDAgBEJa1DxiWtQ8gAUoMCAEQ07QPGNO0DyABSgwIARDGsw8YxrMPIAFKDAgBENuyDxjbsg8gAUoMCAEQmrIPGJqyDyABSgwIARD5sQ8Y-bEPIAFKDAgBEOuxDxjrsQ8gAUoQCAAQltUPGJ3VDyABKgIBBg - ORIGINAL_DST/216.58.211.238 application/vnd.google.safebrowsing-chunk
1496665080.711    466 10.215.145.187 TCP_MISS/200 186018 GET https://safebrowsing-cache.google.com/safebrowsing/rd/ChNnb29nLW1hbHdhcmUtc2hhdmFyOABAAkoMCAAQvqgQGL6oECABSiQIABCC2Q8YnNsPIAEqFsUBxgHkAegB9gH3AfgB-QH7Af0BgwI - ORIGINAL_DST/216.58.211.238 application/vnd.google.safebrowsing-chunk
1496665081.067    355 10.215.145.187 TCP_MISS/200 185194 GET https://safebrowsing-cache.google.com/safebrowsing/rd/ChNnb29nLW1hbHdhcmUtc2hhdmFyOABAAkp_CAAQntUPGIHZDyABKnEICQoLDBIkLzxCUFJeX2BhZmhqbG5ydnx9mwGqAbEBtAG4AbsBwAHBAcIB6AHuAfAB9gH5AY4CwwLJAs0C3gLhAuIC5ALmAusC7QLuAvAC8QLyAv4C_wKEA4gDigOLA5IDmQOaA5sDpQOoA6kDqwOtAw - ORIGINAL_DST/216.58.211.238 application/vnd.google.safebrowsing-chunk
1496665081.263    193 10.215.145.187 TCP_MISS/200 36504 GET https://safebrowsing-cache.google.com/safebrowsing/rd/ChFnb29nLXBoaXNoLXNoYXZhcjgAQAJKDAgBEMrlExjK5RMgAUoMCAEQueMTGLnjEyABSgwIARCR4xMYkuMTIAFKDAgBEOHiExji4hMgAUoMCAEQz-ITGNLiEyABSgwIARDL4hMYzeITIAFKDAgBEP_hExiD4hMgAUoMCAEQ9-ETGP3hEyABSgwIARDk4RMY9OETIAFKDAgBEM7hExji4RMgAUoMCAEQit4TGIreEyABSgwIARDZ2hMY2doTIAFKDAgBELLZExiy2RMgAUoMCAEQ1NgTGNTYEyABSgwIARCy1RMYuNUTIAFKDAgBEMLUExjE1BMgAUoMCAEQjdQTGI3UEyABSgwIARC-0RMYvtETIAFKDAgBELzRExi80RMgAUoMCAEQstETGLPREyABSgwIARCo0RMYqNETIAFKDAgBEOjQExjo0BMgAUoMCAEQ5NATGOTQEyABSgwIARCd0BMYntATIAFKDAgBEObPExjmzxMgAUoMCAEQ288TGNvPEyABSgwIARDGzxMY0M8TIAFKDAgBELvPExjDzxMgAUoMCAEQqM8TGKjPEyABSgwIARCVzxMYlc8TIAFKDAgBEJHPExiRzxMgAUoMCAEQ684TGO7OEyABSgwIARDazhMY584TIAFKDAgBEM_OExjTzhMgAUoMCAEQns4TGJ7OEyABSgwIARDYzRMY2M0TIAFKDAgBELzNExi8zRMgAUoMCAEQ8MwTGPDMEyABSgwIARDtzBMY7cwTIAFKDAgBEOjMExjrzBMgAUoMCAEQ4cwTGOTMEyABSgwIARDfzBMY38wTIAFKDAgBENfMExjYzBMgAUoMCAEQ08wTGNPMEyABSgwIARDPzBMYz8wTIAFKDAgBEM3MExjNzBMgAUoMCAEQv8wTGL_MEyABSgwIARC9zBMYvcwTIAFKDAgBELvMExi7zBMgAUoMCAEQuMwTGLjMEyABSgwIARCyzBMYsswTIAFKDAgBEK3MExiuzBMgAUoMCAEQpswTGKbMEyABSgwIARCWzBMYlswTIAFKDAgBEIrMExiKzBMgAUoMCAEQ_8sTGP_LEyABSgwIARD6yxMY-ssTIAFKDAgBEPfLExj4yxMgAUoMCAEQ88sTGPPLEyABSgwIARDryxMY68sTIAFKDAgBEObLExjnyxMgAUoMCAEQ2csTGNnLEyABSgwIARDPyxMYz8sTIAFKDAgBEL3LExi9yxMgAUoMCAEQu8sTGLvLEyABSgwIARC2yxMYt8sTIAFKDAgBEKLLExiiyxMgAUoMCAEQoMsTGKDLEyABSgwIARCdyxMYncsTIAFKDAgBEJnLExiZyxMgAUoMCAEQjMsTGI3LEyABSgwIARCByxMYiMsTIAFKDAgBEPvKExj-yhMgAUoMCAEQ6soTGPnKEyABSgwIARDnyhMY58oTIAFKDAgBEOLKExjiyhMgAUoMCAEQ1soTGODKEyABSgwIARDOyhMYzsoTIAFKDAgBEMfKExjHyhMgAUoMCAEQxcoTGMXKEyABSgwIARC9yhMYvcoTIAFKDAgBELrKExi6yhMgAUoMCAEQsMoTGLHKEyABSgwIARCsyhMYrMoTIAFKDAgBEKrKExiqyhMgAUoMCAEQosoTGKLKEyABSgwIARCLyhMYi8oTIAFKDAgBEIDKExiAyhMgAUoMCAEQ-8kTGPzJEyABSgwIARD3yRMY-MkTIAFKDAgBEO3JExjyyRMgAUoMCAEQ6ckTGOnJEyABSgwIARDjyRMY48kTIAFKDAgBEN7JExjfyRMgAUoMCAEQ18kTGNjJEyABSgwIARDUyRMY1MkTIAFKDAgBENHJExjRyRMgAUoMCAEQzckTGM3JEyABSgwIARDGyRMYyMkTIAFKDAgBEMLJExjCyRMgAUoMCAEQvskTGL7JEyAB - ORIGINAL_DST/216.58.211.238 application/vnd.google.safebrowsing-chunk
1496665081.389    123 10.215.145.187 TCP_MISS/200 10706 GET https://safebrowsing-cache.google.com/safebrowsing/rd/ChFnb29nLXBoaXNoLXNoYXZhcjgAQAJKDAgBELnJExi5yRMgAUoMCAEQt8kTGLfJEyABSgwIARCzyRMYs8kTIAFKDAgBELHJExixyRMgAUoMCAEQr8kTGK_JEyABSgwIARCnyRMYp8kTIAFKDAgBEKTJExikyRMgAUoMCAEQmskTGKHJEyABSgwIARCYyRMYmMkTIAFKDAgBEJHJExiSyRMgAUoMCAEQjskTGI7JEyABSgwIARCGyRMYi8kTIAFKDAgBEITJExiEyRMgAUoMCAEQgskTGILJEyABSgwIARCAyRMYgMkTIAFKDAgBEOvIExjryBMgAUoMCAEQ5cgTGObIEyABSgwIARDgyBMY4MgTIAFKDAgBENHIExjRyBMgAUoMCAEQzsgTGM_IEyAB - ORIGINAL_DST/216.58.211.238 application/vnd.google.safebrowsing-chunk
1496665081.812    419 10.215.145.187 TCP_MISS/200 189122 GET https://safebrowsing-cache.google.com/safebrowsing/rd/ChFnb29nLXBoaXNoLXNoYXZhcjgAQAJKDAgAEIKqHRiCqh0gAUoMCAAQ1JsdGJ2dHSAB - ORIGINAL_DST/216.58.211.238 application/vnd.google.safebrowsing-chunk
1496665082.104    290 10.215.145.187 TCP_MISS/200 192024 GET https://safebrowsing-cache.google.com/safebrowsing/rd/ChFnb29nLXBoaXNoLXNoYXZhcjgAQAJKEAgAEJyaHRjTmx0gASoCrwE - ORIGINAL_DST/216.58.211.238 application/vnd.google.safebrowsing-chunk
1496665082.406    298 10.215.145.187 TCP_MISS/200 190496 GET https://safebrowsing-cache.google.com/safebrowsing/rd/ChFnb29nLXBoaXNoLXNoYXZhcjgAQAJKDAgAEM-YHRibmh0gAQ - ORIGINAL_DST/216.58.211.238 application/vnd.google.safebrowsing-chunk
1496665082.678    263 10.215.145.187 TCP_MISS/200 191042 GET https://safebrowsing-cache.google.com/safebrowsing/rd/ChFnb29nLXBoaXNoLXNoYXZhcjgAQAJKDAgAEN6WHRjOmB0gAQ - ORIGINAL_DST/216.58.211.238 application/vnd.google.safebrowsing-chunk
1496665085.543    124 10.215.145.187 TCP_MISS/200 7481 GET https://emeacmsd.acms.com/common/help/en/support/css/globalnav.css - ORIGINAL_DST/54.247.125.57 text/html
1496665085.717    292 10.215.145.187 TAG_NONE/200 0 CONNECT 46.137.190.100:443 - ORIGINAL_DST/46.137.190.100 -
1496665086.800    751 10.215.145.187 TCP_MISS/200 493387 GET https://emeacmsd.acms.com/common/intro/test.swf - ORIGINAL_DST/54.247.125.57 application/x-shockwave-flash
1496665087.217    114 10.215.145.187 TCP_MISS/200 1433 GET https://emeacmsd.acms.com/common/AddInInfo.xml - ORIGINAL_DST/54.247.125.57 application/xml
1496665087.404    113 10.215.145.187 TCP_MISS/200 407 POST https://emeacmsd.acms.com/messagebroker/amf - ORIGINAL_DST/54.247.125.57 application/x-amf
1496665088.098    472 10.215.145.187 TAG_NONE/200 0 CONNECT 46.51.187.18:443 - ORIGINAL_DST/46.51.187.18 -
1496665088.143      6 10.215.145.187 TAG_NONE/400 4428 NONE error:invalid-request - HIER_NONE/- text/html
1496665177.661    209 10.215.145.187 TCP_MISS/200 324 POST https://emeacmsd.acms.com/messagebroker/amf - ORIGINAL_DST/54.247.125.57 application/x-amf

Note the "error:invalid-request" message.

# cat /var/log/squid/cache.test.log (part of it)

---------
HTTP/1.1 200 OK
Accept-Ranges: bytes
Cache-Control: max-age=3600
Content-Type: application/x-shockwave-flash
Date: Mon, 05 Jun 2017 12:18:05 GMT
Last-Modified: Wed, 16 Sep 2015 16:36:14 GMT
Server: Apache-Coyote/1.1
Set-Cookie: BreezeCCookie=FDYC-UW78-J9K9-SZ93-YUQP-F4B4-ZRNY-UVXE; Path=/; Secure; HttpOnly
X-Breeze-Cache: appserv/common/intro/test.swf
X-Breeze-Public-Map: /common/,appserv/common/
Content-Length: 492826
X-Cache: MISS from inf-fw1
X-Cache-Lookup: MISS from inf-fw1:3227
Via: 1.1 inf-fw1 (squid/3.5.14)
Connection: keep-alive


----------
2017/06/05 14:18:06.175 kid1| 33,5| AsyncCall.cc(26) AsyncCall: The AsyncCall clientWriteComplete constructed, this=0x80e081e8 [call2315]
2017/06/05 14:18:06.175 kid1| 5,5| Write.cc(35) Write: local=54.247.125.57:443 remote=10.215.145.187 FD 13 flags=17: sz 561: asynCall 0x80e081e8*1
2017/06/05 14:18:06.175 kid1| 5,5| ModEpoll.cc(116) SetSelect: FD 13, type=2, handler=1, client_data=0x808b43a0, timeout=0
2017/06/05 14:18:06.175 kid1| 11,5| http.cc(1399) processReplyBody: adaptationAccessCheckPending=0
2017/06/05 14:18:06.175 kid1| 20,5| store.cc(834) write: storeWrite: writing 8192 bytes for 'B04729E56EF0FD97349B176C475C4F1B'
2017/06/05 14:18:06.175 kid1| 20,3| store_swapout.cc(376) mayStartSwapOut: already allowed
2017/06/05 14:18:06.175 kid1| 20,5| store_swapout.cc(47) storeSwapOutStart: storeSwapOutStart: Begin SwapOut 'https://emeacmsd.acms.com/common/intro/test.swf' to dirno -1, fileno FFFFFFFF
2017/06/05 14:18:06.175 kid1| 73,3| HttpRequest.cc(689) storeId: sent back canonicalUrl:https://emeacmsd.acms.com/common/intro/test.swf
2017/06/05 14:18:06.175 kid1| 20,3| store_swapmeta.cc(54) storeSwapMetaBuild: storeSwapMetaBuild URL: https://emeacmsd.acms.com/common/intro/test.swf
2017/06/05 14:18:06.175 kid1| 20,2| store_io.cc(42) storeCreate: storeCreate: Selected dir 0 for e:=w1p2DV/0x80d8d2e8*4
2017/06/05 14:18:06.175 kid1| 79,3| ufs/UFSStrategy.cc(100) create: fileno 0000002D
2017/06/05 14:18:06.175 kid1| 79,3| DiskIO/DiskDaemon/DiskdFile.cc(40) DiskdFile: DiskdFile::DiskdFile: /var/cache/squid.test/00/00/0000002D
2017/06/05 14:18:06.175 kid1| 79,3| DiskIO/DiskDaemon/DiskdFile.cc(86) create: DiskdFile::create: 0x80e08bf0 creating for 0x80e0585c
2017/06/05 14:18:06.175 kid1| 47,4| ufs/UFSSwapDir.cc(1206) replacementAdd: added node 0x80d8d2e8 to dir 0
2017/06/05 14:18:06.175 kid1| 20,3| store.cc(484) lock: storeSwapOutStart locked key B04729E56EF0FD97349B176C475C4F1B e:=w1p2DV/0x80d8d2e8*5
2017/06/05 14:18:06.175 kid1| 79,3| ufs/UFSStoreState.cc(161) write: UFSStoreState::write: dirn 0, fileno 0000002D
2017/06/05 14:18:06.175 kid1| 79,3| ufs/UFSStoreState.cc(469) queueWrite: 0x80e05828 UFSStoreState::queueWrite: queueing write of size 125
2017/06/05 14:18:06.175 kid1| 79,3| ufs/UFSStoreState.cc(184) doWrite: 0x80e05828 UFSStoreState::doWrite
2017/06/05 14:18:06.175 kid1| 79,3| ufs/UFSStoreState.cc(219) doWrite: 0x80e05828 calling theFile->write(125)
2017/06/05 14:18:06.175 kid1| 79,3| DiskIO/DiskDaemon/DiskdFile.cc(278) write: DiskdFile::write: this 0x80e08bf0, buf 0x80e05480, off 0, len 125
2017/06/05 14:18:06.175 kid1| 20,3| store_swapout.cc(132) doPages: storeSwapOut: swap_buf_len = 4096
2017/06/05 14:18:06.175 kid1| 20,3| store_swapout.cc(136) doPages: storeSwapOut: swapping out 4096 bytes from 0
2017/06/05 14:18:06.175 kid1| 79,3| ufs/UFSStoreState.cc(161) write: UFSStoreState::write: dirn 0, fileno 0000002D
2017/06/05 14:18:06.175 kid1| 79,3| ufs/UFSStoreState.cc(469) queueWrite: 0x80e05828 UFSStoreState::queueWrite: queueing write of size 4096
2017/06/05 14:18:06.175 kid1| 79,3| ufs/UFSStoreState.cc(184) doWrite: 0x80e05828 UFSStoreState::doWrite
2017/06/05 14:18:06.175 kid1| 79,3| ufs/UFSStoreState.cc(219) doWrite: 0x80e05828 calling theFile->write(4096)
2017/06/05 14:18:06.175 kid1| 79,3| DiskIO/DiskDaemon/DiskdFile.cc(278) write: DiskdFile::write: this 0x80e08bf0, buf 0x80daa56c, off -1, len 4096
2017/06/05 14:18:06.175 kid1| 20,3| store_swapout.cc(132) doPages: storeSwapOut: swap_buf_len = 4096
2017/06/05 14:18:06.175 kid1| 20,3| store_swapout.cc(136) doPages: storeSwapOut: swapping out 4096 bytes from 4096
2017/06/05 14:18:06.175 kid1| 79,3| ufs/UFSStoreState.cc(161) write: UFSStoreState::write: dirn 0, fileno 0000002D
2017/06/05 14:18:06.175 kid1| 79,3| ufs/UFSStoreState.cc(469) queueWrite: 0x80e05828 UFSStoreState::queueWrite: queueing write of size 4096
2017/06/05 14:18:06.176 kid1| 79,3| ufs/UFSStoreState.cc(184) doWrite: 0x80e05828 UFSStoreState::doWrite
2017/06/05 14:18:06.176 kid1| 79,3| ufs/UFSStoreState.cc(219) doWrite: 0x80e05828 calling theFile->write(4096)
2017/06/05 14:18:06.176 kid1| 79,3| DiskIO/DiskDaemon/DiskdFile.cc(278) write: DiskdFile::write: this 0x80e08bf0, buf 0x80da954c, off -1, len 4096
2017/06/05 14:18:06.176 kid1| 90,3| store_client.cc(732) invokeHandlers: InvokeHandlers: B04729E56EF0FD97349B176C475C4F1B
2017/06/05 14:18:06.176 kid1| 90,3| store_client.cc(738) invokeHandlers: StoreEntry::InvokeHandlers: checking client #0
2017/06/05 14:18:06.176 kid1| 11,3| http.cc(1054) persistentConnStatus: local=10.215.145.187:60291 remote=54.247.125.57:443 FD 15 flags=25 eof=0
2017/06/05 14:18:06.176 kid1| 11,5| http.cc(1074) persistentConnStatus: persistentConnStatus: content_length=492826
2017/06/05 14:18:06.176 kid1| 11,5| http.cc(1078) persistentConnStatus: persistentConnStatus: clen=492826
2017/06/05 14:18:06.176 kid1| 11,5| http.cc(1091) persistentConnStatus: persistentConnStatus: body_bytes_read=8192 content_length=492826
2017/06/05 14:18:06.176 kid1| 11,5| http.cc(1428) processReplyBody: processReplyBody: INCOMPLETE_MSG from local=10.215.145.187:60291 remote=54.247.125.57:443 FD 15 flags=25

Any ideas?

Vieri


From rousskov at measurement-factory.com  Mon Jun  5 14:52:01 2017
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Mon, 5 Jun 2017 08:52:01 -0600
Subject: [squid-users] squid ssl bump and Adobe Connect
In-Reply-To: <2085804462.2620283.1496666965861@mail.yahoo.com>
References: <2085804462.2620283.1496666965861.ref@mail.yahoo.com>
 <2085804462.2620283.1496666965861@mail.yahoo.com>
Message-ID: <03181dbd-0880-e4a5-5e50-aa4f4b3a94e4@measurement-factory.com>

On 06/05/2017 06:49 AM, Vieri wrote:

> 1496665088.143      6 10.215.145.187 TAG_NONE/400 4428 NONE error:invalid-request - HIER_NONE/- text/html

> Any ideas?

I recommend finding the place in the debugging cache.log where Squid
generates the above error response and then going backwards to find the
cause. Please note that this error may not be the reason you cannot
connect to Adobe Connect, but if it is the only red flag, it is worth
investigating.

Logging the source port of the client connection to access.log can help
you narrow down your search.

In general, copy-pasting debugging cache.logs here is rarely useful
_unless_ you isolate the problematic lines first. If you cannot isolate
those lines, post a link to a compressed full log. In fact, posting such
a link is a good idea even if you copy-paste (what you think are) the
relevant lines here.


HTH,

Alex.


From erdosain9 at gmail.com  Mon Jun  5 15:24:00 2017
From: erdosain9 at gmail.com (erdosain9)
Date: Mon, 5 Jun 2017 08:24:00 -0700 (PDT)
Subject: [squid-users] this config is ok? is ok the order?
In-Reply-To: <3ef299a1-df56-4913-38f7-f9627cded853@treenet.co.nz>
References: <1496157812155-4682631.post@n4.nabble.com>
 <1f29d3f2-3d4d-c55d-ad3b-4d06748b857f@treenet.co.nz>
 <1496322644043-4682653.post@n4.nabble.com>
 <3ef299a1-df56-4913-38f7-f9627cded853@treenet.co.nz>
Message-ID: <1496676240770-4682677.post@n4.nabble.com>

Amos Jeffries wrote
> The core issue is the speed at which that service rotates its response 
> IP lists, which is directly related to each request going to entirely 
> different server in their farm. Simply having a single (and maybe more 
> sane regarding TTLs) resolver as a networks focal point for the traffic 
> before it reaches out to the Google service seems to bring sanity back 
> to the performance.

Ok, thanks.
mmm... and what you think about this

dig -x google.com

; <<>> DiG 9.9.4-RedHat-9.9.4-29.el7_2.3 <<>> -x google.com
;; global options: +cmd
;; Got answer:
;; ->>HEADER<<- opcode: QUERY, status: NXDOMAIN, id: 25260
;; flags: qr rd ra; QUERY: 1, ANSWER: 0, AUTHORITY: 1, ADDITIONAL: 1

;; OPT PSEUDOSECTION:
; EDNS: version: 0, flags:; udp: 4000
;; QUESTION SECTION:
;com.google.in-addr.arpa.	IN	PTR

;; AUTHORITY SECTION:
in-addr.arpa.		900	IN	SOA	b.in-addr-servers.arpa. nstld.iana.org. 2017042647
1800 900 604800 3600

;; Query time: 1 msec
;; SERVER: 192.168.1.222#53(192.168.1.222)
;; WHEN: lun jun 05 12:37:03 ART 2017
;; MSG SIZE  rcvd: 120

We have, little time? about 15', this is a problem, dont you think?
Or im using bad dig??
what would be a good value???
Thanks againg.



--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/this-config-is-ok-is-ok-the-order-tp4682631p4682677.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From Antony.Stone at squid.open.source.it  Mon Jun  5 15:47:57 2017
From: Antony.Stone at squid.open.source.it (Antony Stone)
Date: Mon, 05 Jun 2017 17:47:57 +0200
Subject: [squid-users] this config is ok? is ok the order?
In-Reply-To: <1496676240770-4682677.post@n4.nabble.com>
References: <1496157812155-4682631.post@n4.nabble.com>
 <3ef299a1-df56-4913-38f7-f9627cded853@treenet.co.nz>
 <1496676240770-4682677.post@n4.nabble.com>
Message-ID: <1793820.seS1fI72I3@pikantusdevuan>

On Monday 05 June 2017 08:24:00 erdosain9 wrote:

> mmm... and what you think about this
> 
> dig -x google.com

Firstly, I think "what is this supposed to mean?"

dig -x is for reverse lookups - you give it an address and it tells you what 
name has been assigned in a PTR record (as opposed to a forward lookup, which 
takes a name and tells you what address/s have been assigned in A or AAAA 
records).

Therefore "dig -x" with a name makes no sense to me.

> ;; AUTHORITY SECTION:
> in-addr.arpa.		900	IN	SOA	b.in-addr-servers.arpa. nstld.iana.org. 
2017042647
> 1800 900 604800 3600

> We have, little time? about 15', this is a problem, dont you think?

That's the TTL for lookups on a *root* name server - in fact I'm slightly 
surprised it's even as long as 15 minutes.

A large TTL on root name servers would prevent the rest of the Internet from 
finding out about new name/address assignments in a timely manner.

> Or im using bad dig??

Yes, I think that's the problem.

> what would be a good value???

What would be a good value for what?

What are you trying to achieve / find out?


Antony.

-- 
I lay awake all night wondering where the sun went, and then it dawned on me.

                                                   Please reply to the list;
                                                         please *don't* CC me.



From erdosain9 at gmail.com  Mon Jun  5 18:50:42 2017
From: erdosain9 at gmail.com (erdosain9)
Date: Mon, 5 Jun 2017 11:50:42 -0700 (PDT)
Subject: [squid-users] this config is ok? is ok the order?
In-Reply-To: <1793820.seS1fI72I3@pikantusdevuan>
References: <1496157812155-4682631.post@n4.nabble.com>
 <1f29d3f2-3d4d-c55d-ad3b-4d06748b857f@treenet.co.nz>
 <1496322644043-4682653.post@n4.nabble.com>
 <3ef299a1-df56-4913-38f7-f9627cded853@treenet.co.nz>
 <1496676240770-4682677.post@n4.nabble.com>
 <1793820.seS1fI72I3@pikantusdevuan>
Message-ID: <1496688642885-4682679.post@n4.nabble.com>

Hi. For what I understood. It is important ttl of dns names. So, I wanted to
know when the squid server would ask for resolution again. That is, how long
was the record kept.
Thanks

pd.:whitout -x

[root at squid ~]# dig yahoo.com

; <<>> DiG 9.9.4-RedHat-9.9.4-29.el7_2.3 <<>> yahoo.com
;; global options: +cmd
;; Got answer:
;; ->>HEADER<<- opcode: QUERY, status: NOERROR, id: 6258
;; flags: qr rd ra; QUERY: 1, ANSWER: 3, AUTHORITY: 0, ADDITIONAL: 1

;; OPT PSEUDOSECTION:
; EDNS: version: 0, flags:; udp: 4000
;; QUESTION SECTION:
;yahoo.com.			IN	A

;; ANSWER SECTION:
yahoo.com.		590	IN	A	98.138.253.109
yahoo.com.		590	IN	A	98.139.183.24
yahoo.com.		590	IN	A	206.190.36.45

;; Query time: 4 msec
;; SERVER: 192.168.1.222#53(192.168.1.222)
;; WHEN: lun jun 05 16:00:44 ART 2017
;; MSG SIZE  rcvd: 86

[root at squid ~]# dig pijamasurf.com

; <<>> DiG 9.9.4-RedHat-9.9.4-29.el7_2.3 <<>> pijamasurf.com
;; global options: +cmd
;; Got answer:
;; ->>HEADER<<- opcode: QUERY, status: NOERROR, id: 17497
;; flags: qr rd ra; QUERY: 1, ANSWER: 2, AUTHORITY: 0, ADDITIONAL: 1

;; OPT PSEUDOSECTION:
; EDNS: version: 0, flags:; udp: 4000
;; QUESTION SECTION:
;pijamasurf.com.			IN	A

;; ANSWER SECTION:
pijamasurf.com.		299	IN	A	104.24.25.112
pijamasurf.com.		299	IN	A	104.24.26.112

;; Query time: 71 msec
;; SERVER: 192.168.1.222#53(192.168.1.222)
;; WHEN: lun jun 05 16:02:15 ART 2017
;; MSG SIZE  rcvd: 75


I wish I could put a bigger ttl to avoid being asked every "little amount of
time" by one address. For example pijamasurf.com = 299 and yahoo = 590, so
who manage that time?? how can i put more time to live?
Or does this make no sense?
Maybe I did not understand Amos's comment. (I thought I read better English
:-))
Thanks



--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/this-config-is-ok-is-ok-the-order-tp4682631p4682679.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From Antony.Stone at squid.open.source.it  Mon Jun  5 19:24:05 2017
From: Antony.Stone at squid.open.source.it (Antony Stone)
Date: Mon, 05 Jun 2017 21:24:05 +0200
Subject: [squid-users] this config is ok? is ok the order?
In-Reply-To: <1496688642885-4682679.post@n4.nabble.com>
References: <1496157812155-4682631.post@n4.nabble.com>
 <1793820.seS1fI72I3@pikantusdevuan>
 <1496688642885-4682679.post@n4.nabble.com>
Message-ID: <8009368.3uC88zRBUU@pikantusdevuan>

On Monday 05 June 2017 11:50:42 erdosain9 wrote:

> Hi. For what I understood. It is important ttl of dns names.

Yes, TTL is important.  It tells caching DNS servers how long they may 
remember the last answer they got from the authoritative server, before they 
need to ask the authoritative server again.

> So, I wanted to know when the squid server would ask for resolution again.

Well, that's a different question.

Q: When will Squid ask [its configured name server] for resolution again?
A: When it needs to know the answer again.

Q: When will the [recursive] DNS server which Squid asks, ask for resolution 
again?
A: When the TTL has expired.

> That is, how long was the record kept.

That is the TTL.

> ;; ANSWER SECTION:
> yahoo.com.		590	IN	A	98.138.253.109

> ;; ANSWER SECTION:
> pijamasurf.com.		299	IN	A	104.24.25.112

> I wish I could put a bigger ttl to avoid being asked every "little amount of
> time" by one address.

Why?  What does it matter to you that Yahoo asks your DNS server to refresh 
its results no more than 30 minutes after the last time (your example of 590 
fails to mention that you clearly asked your local name server for yahoo.com 
1210 seconds previously).  If you want to know the real TTL, ask an 
authoritative name server:

$ dig @ns1.yahoo.com. yahoo.com

;; ANSWER SECTION:
yahoo.com.              1800    IN      A       98.139.183.24

If you only ask your local caching server, all you are finding out is how much 
longer its cached answer is valid for, before it will ask (the authoritative 
servers) again.

> For example pijamasurf.com = 299 and yahoo = 590, so
> who manage that time??

Whoever maintains the zone files (DNS records) for those domains.

> how can i put more time to live?

You cannot (and should not).

> Or does this make no sense?

Why do you want to change the TTL on somebody else's domain?

What (do you think) is the benefit for you?

> Maybe I did not understand Amos's comment.

Please repeat the comment which led you to trying to change the TTL of other 
people's domains - maybe that will help us better understand what you are 
trying to achieve,


Antony.

-- 
Never automate fully anything that does not have a manual override capability. 
Never design anything that cannot work under degraded conditions in emergency.

                                                   Please reply to the list;
                                                         please *don't* CC me.



From rousskov at measurement-factory.com  Mon Jun  5 21:33:07 2017
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Mon, 5 Jun 2017 15:33:07 -0600
Subject: [squid-users] this config is ok? is ok the order?
In-Reply-To: <8009368.3uC88zRBUU@pikantusdevuan>
References: <1496157812155-4682631.post@n4.nabble.com>
 <1793820.seS1fI72I3@pikantusdevuan>
 <1496688642885-4682679.post@n4.nabble.com>
 <8009368.3uC88zRBUU@pikantusdevuan>
Message-ID: <0934616a-0984-b565-afff-14e440c90439@measurement-factory.com>

On 06/05/2017 01:24 PM, Antony Stone wrote:
> On Monday 05 June 2017 11:50:42 erdosain9 wrote:
>> So, I wanted to know when the squid server would ask for resolution again.

> Q: When will Squid ask [its configured name server] for resolution again?
> A: When it needs to know the answer again.

This answer is incorrect because Squid has its own DNS cache.

Squid pays attention to DNS record TTLs. I believe there are several
bugs in that code, but, in many cases, Squid will not cache the answer
for less than negative_dns_ttl and for longer than the minimum of the
received TTL and positive_dns_ttl. DNS answers may be purged from the
Squid DNS cache for reasons other than TTL, of course.


>> how can i put more time to live?

> You cannot (and should not).

One can increase effective TTL in Squid DNS cache by increasing
negative_dns_ttl. Whether one _should_ do that is a different question
that I cannot answer in general.


HTH,

Alex.


From squid3 at treenet.co.nz  Tue Jun  6 01:29:58 2017
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Tue, 6 Jun 2017 13:29:58 +1200
Subject: [squid-users] this config is ok? is ok the order?
In-Reply-To: <8009368.3uC88zRBUU@pikantusdevuan>
References: <1496157812155-4682631.post@n4.nabble.com>
 <1793820.seS1fI72I3@pikantusdevuan>
 <1496688642885-4682679.post@n4.nabble.com>
 <8009368.3uC88zRBUU@pikantusdevuan>
Message-ID: <723b8c10-d7bf-5539-70aa-f7e32ccc05c9@treenet.co.nz>

On 06/06/17 07:24, Antony Stone wrote:
> On Monday 05 June 2017 11:50:42 erdosain9 wrote:
>
>> Hi. For what I understood. It is important ttl of dns names.
> Yes, TTL is important.  It tells caching DNS servers how long they may
> remember the last answer they got from the authoritative server, before they
> need to ask the authoritative server again.
>
>> So, I wanted to know when the squid server would ask for resolution again.
> Well, that's a different question.
>
> Q: When will Squid ask [its configured name server] for resolution again?
> A: When it needs to know the answer again.
>
> Q: When will the [recursive] DNS server which Squid asks, ask for resolution
> again?
> A: When the TTL has expired.
>
>> That is, how long was the record kept.
> That is the TTL.
>
>> ;; ANSWER SECTION:
>> yahoo.com.		590	IN	A	98.138.253.109
>> ;; ANSWER SECTION:
>> pijamasurf.com.		299	IN	A	104.24.25.112
>> I wish I could put a bigger ttl to avoid being asked every "little amount of
>> time" by one address.
> Why?  What does it matter to you that Yahoo asks your DNS server to refresh
> its results no more than 30 minutes after the last time (your example of 590
> fails to mention that you clearly asked your local name server for yahoo.com
> 1210 seconds previously).  If you want to know the real TTL, ask an
> authoritative name server:
>
> $ dig @ns1.yahoo.com. yahoo.com
>
> ;; ANSWER SECTION:
> yahoo.com.              1800    IN      A       98.139.183.24
>
> If you only ask your local caching server, all you are finding out is how much
> longer its cached answer is valid for, before it will ask (the authoritative
> servers) again.
>
>> For example pijamasurf.com = 299 and yahoo = 590, so
>> who manage that time??
> Whoever maintains the zone files (DNS records) for those domains.
>
>> how can i put more time to live?
> You cannot (and should not).
>
>> Or does this make no sense?
> Why do you want to change the TTL on somebody else's domain?
>
> What (do you think) is the benefit for you?
>
>> Maybe I did not understand Amos's comment.
> Please repeat the comment which led you to trying to change the TTL of other
> people's domains - maybe that will help us better understand what you are
> trying to achieve,
>

I suspect it was this comment:

> The core issue is the speed at which that service rotates its response 
> IP lists, which is directly related to each request going to entirely 
> different server in their farm. Simply having a single (and maybe more 
> sane regarding TTLs) resolver as a networks focal point for the 
> traffic before it reaches out to the Google service seems to bring 
> sanity back to the performance. 

What I meant there was using a resolver that obeys the domain TTL it 
gets given and stores the result until that TTL expires.

The way the Google service load balances does not allow that to happen - 
your users query will reach a different server to your Squid query and 
to your test query later - all of which probably have different TTL 
values coming back (as you saw in that dig result 590 != 1800 and 299 != 
1800 ... the Google service is just a farm of recursive resolvers all 
with different cache contents). By having your own resolver between you 
and the Google service/farm that resolver takes only _one_ TTL period at 
a time from Google - and delivers all your clients and Squid etc that 
result until its single TTL period expires. After which it asks Google 
again and gets just one TTL to follow, and so on.


Amos



From squid3 at treenet.co.nz  Tue Jun  6 02:05:06 2017
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Tue, 6 Jun 2017 14:05:06 +1200
Subject: [squid-users] squid 3.5.24 HTTPS/HTTP caching optimisation Help
In-Reply-To: <D94BF1A4-596A-443E-B72F-B48EB79151D8@netstream.ps>
References: <D94BF1A4-596A-443E-B72F-B48EB79151D8@netstream.ps>
Message-ID: <2196d3f9-2017-b099-b46d-526983dc6762@treenet.co.nz>

On 05/06/17 22:08, --Ahmad-- wrote:
> Hola Folks .
> i have been trying to optimise caching website that is http and has 
> many images in its content like cnn.com <http://cnn.com>
> and trying to cache https static website like https://www.alwatanvoice.com
>
>
> i have squid enabled with https and i installed certificates 
> on clients pcs :
>
> i have  been fighting to have better HIT ration but it is vey poor .
> i hope someone give me a hand and focous on caching images contents of 
> the 2 websites below .
>

"poor" is not a sufficient metric, especially when you are optimizing. 
Numbers please, and the details of how you obtained those numbers.



Amos



From squid3 at treenet.co.nz  Tue Jun  6 02:08:11 2017
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Tue, 6 Jun 2017 14:08:11 +1200
Subject: [squid-users] Squid issue of caching the m3u8 file
In-Reply-To: <CAEip7fzdjxuchvn3YUrdh1jnXOvfn=bUqkWLko_X_-rVq2Wn5Q@mail.gmail.com>
References: <CAEip7fzdjxuchvn3YUrdh1jnXOvfn=bUqkWLko_X_-rVq2Wn5Q@mail.gmail.com>
Message-ID: <5af5e6e9-4880-f58e-106b-9f11dd0a4a7b@treenet.co.nz>

On 05/06/17 23:54, LIU Yaning wrote:
> Dear All,
>
> I would like to cache the .m3u8 file to be able to provide offline 
> caching service by Squid. The played HLS video streaming is the link 
> as below:
> http://qthttp.apple.com.edgesuite.net/1010qwoeiuryfg/0150_vod.m3u8 
> <http://qthttp.apple.com.edgesuite.net/1010qwoeiuryfg/0150_vod.m3u8>
> However, the .m3u8 file is not be cached probably because it is 
> mentioned as a no-cache, no-store, max-age=0 in the "Cache-Control" in 
> the HTTP header.
Nope. Only the CC:no-store is preventing caching. The other headers 
simply put boundaries on what is to be done with the content in the 
cache. In particular the "no-cache", max-age=0 and Expires values mean 
it has to be revalidated (REFRESH in your access.log) before any future 
uses - probably because of that Set-Cookie needing to be changed for 
different end-users.
>
> HTTP/1.1 200 OK
>
> Server: Apache
>
> ETag: "1d7168b4f49e75f76f3182f24bf075f6:1299516751"
>
> Last-Modified: Mon, 07 Mar 2011 16:52:31 GMT
>
> Expires: Fri, 02 Jun 2017 14:26:52 GMT
>
> Cache-Control: max-age=0, no-cache, no-store
>
> Pragma: no-cache
>
> Date: Fri, 02 Jun 2017 14:26:52 GMT
>
> Content-Length: 16046
>
> Set-Cookie: AKID=77F9F1316ECCE780566608C5E514DE0A;expires=Fri, 26 Aug 
> 2016 00:01:00 GMT; path=/; domain=qthttp.apple.com.edgesuite.net 
> <http://qthttp.apple.com.edgesuite.net/>
>
> Content-Type: application/x-mpegURL
>
> Access-Control-Allow-Origin: *
>
> I added a new rule for .m3u8 file in squid.conf, however, it is still 
> not working.
>
> refresh_pattern -i \.(ts|m3u8)$ 120 90% 1000 override-expire 
> override-lastmod ignore-no-cache ignore-no-store
>
> Does anyone know how to allow Squid caching the .m3u8 file? Thanks a 
> lot in advance.
What makes you think it is not caching? The ignore-no-store alone should 
be sufficient to allow current Squid versions to cache that object. You 
could perhapse add "store-stale" option on that config line. Which 
should make Squid cache object containing an Expires header with current 
or past values. refresh_pattern settings do not affect that cacheable vs 
non-cacheable decision. The no-cache header tells Squid the object needs 
revalidating before every use. However, be aware the tool at redbot.org 
tells me that this URL is badly broken in how it is using the ETag and 
Vary headers - in a way which can break the revalidation when these 
things are cached. Some of your clients may see very broken behaviour 
accessing this object unless you follow the no-store requirement or the 
server stops its broken ETag behaviour. PS. if you are using a Squid 
version much older than 3.5.24 I recommend an upgrade. With an urgency 
increasing the older your Squid is. Amos


From squid3 at treenet.co.nz  Tue Jun  6 02:38:46 2017
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Tue, 6 Jun 2017 14:38:46 +1200
Subject: [squid-users] assertion failed: store.cc "EBIT_TEST(flags,
 ENTRY_ABORTED)"
In-Reply-To: <a9963434-bddb-ed7c-48cf-db97203906d4@zato.ru>
References: <a9963434-bddb-ed7c-48cf-db97203906d4@zato.ru>
Message-ID: <c3277d32-1149-8ba3-e54a-91976f883278@treenet.co.nz>

On 04/06/17 19:27, alexander lunev wrote:
> Hello everyone!
> I have two almost identical cache servers, both FreeBSD 10.3, both 
> running latest squid-3.2.25 from ports in transparent mode, one runs 
> OK and another is throwing this error:

Do you mean 3.5.25?  (3.2 series ended at 3.2.14)

>
>
> 2017/06/04 10:19:08 kid1| storeLateRelease: released 0 objects
> 2017/06/04 10:19:19 kid1| assertion failed: store.cc:1086: 
> "EBIT_TEST(flags, ENTRY_ABORTED)"
>

If you can obtain an updated stack/back-trace from that assertion it 
would be a help in identifying how it is happening. 
<http://wiki.squid-cache.org/SquidFaq/BugReporting> has info on how to 
report this type of bug, and how to obtain traces from production 
proxies with minimal service impact if you need it.



> After this squid is exiting.
>
> Beside some default configuration config contains:
>
> http_port 127.0.0.1:3127
> http_port  127.0.0.1:3128 intercept
> https_port 127.0.0.1:3129 intercept ssl-bump 
> options=ALL:NO_SSLv3:NO_SSLv2 connection-auth=off 
> generate-host-certificates=on dynamic_cert_mem_cache_size=4MB 
> cert=/usr/local/etc/squid/squid.pem key=/usr/local/etc/squid/squid.key
>
> sslcrtd_program /usr/local/libexec/squid/ssl_crtd -s 
> /usr/local/etc/squid/ssl_db -M 4MB
> sslcrtd_children 35
>
> cache deny all
> url_rewrite_program /usr/local/bin/squidGuard -c 
> /usr/local/etc/squid/squidGuard.conf
>
>
> # Leave coredumps in the first cache dir
> coredump_dir /var/squid/cache
> #ssl_bump client-first all
>
> always_direct allow all

You can/should remove that above line. It is unnecessary for bumping 
since 3.1 series.

>
> acl step1 at_step SslBump1
> acl ssldomains ssl::server_name "/usr/local/etc/squid/ssldomains.txt"
> ssl_bump peek step1
> ssl_bump bump ssldomains
> ssl_bump splice all
>
> sslproxy_cert_error allow all
> sslproxy_flags DONT_VERIFY_PEER
>

You should definitely remove both the above lines. They are hiding many 
potential TLS/SSL problems from *you* (not your users). The errors which 
may appear are real security problems with potentially major impacts on 
your users. They should usually be solved in ways other than simply 
hiding ones head in the sand.


>
> Why is this and how it can be fixed?
>

Something being cached is not being aborted when it was supposed to have 
been. More details are needed, please follow the instructions above.

Amos



From rentorbuy at yahoo.com  Tue Jun  6 06:08:12 2017
From: rentorbuy at yahoo.com (Vieri)
Date: Tue, 6 Jun 2017 06:08:12 +0000 (UTC)
Subject: [squid-users] squid ssl bump and Adobe Connect
In-Reply-To: <03181dbd-0880-e4a5-5e50-aa4f4b3a94e4@measurement-factory.com>
References: <2085804462.2620283.1496666965861.ref@mail.yahoo.com>
 <2085804462.2620283.1496666965861@mail.yahoo.com>
 <03181dbd-0880-e4a5-5e50-aa4f4b3a94e4@measurement-factory.com>
Message-ID: <782975790.3626570.1496729292503@mail.yahoo.com>

________________________________
From: Alex Rousskov <rousskov at measurement-factory.com>
>

>> 1496665088.143      6 10.215.145.187 TAG_NONE/400 4428 NONE error:invalid-request - HIER_NONE/- 

>> text/html>
> I recommend finding the place in the debugging cache.log where Squid

> generates the above error response and then going backwards to find the
> cause.

OK Alex, got it.
In the meantime, I searched for the events around the time this happened.
BTW as a side question I'd like to know if I can change the timestamp in cache.log so it can print the unixtime as in access.log.

In any case, here's the relevant part:

[Mon Jun  5 14:18:08 2017].143      6 10.215.145.187 TAG_NONE/400 4428 NONE error:invalid-request - HIER_NONE/- text/html

cache.log within 14:18:08:

2017/06/05 14:18:08 kid1| hold write on SSL connection on FD 30
2017/06/05 14:18:08.000 kid1| 28,3| Checklist.cc(70) preCheck: 0x80d21df8 checking slow rules
2017/06/05 14:18:08.000 kid1| 28,5| Acl.cc(138) matches: checking (ssl_bump rules)
2017/06/05 14:18:08.000 kid1| 28,5| Checklist.cc(400) bannedAction: Action 'ALLOWED/4 is  banned
2017/06/05 14:18:08.000 kid1| 28,5| Checklist.cc(400) bannedAction: Action 'ALLOWED/5is not banned
2017/06/05 14:18:08.000 kid1| 28,5| Acl.cc(138) matches: checking (ssl_bump rule)
2017/06/05 14:18:08.000 kid1| 28,5| Acl.cc(138) matches: checking all
2017/06/05 14:18:08.000 kid1| 28,3| Ip.cc(539) match: aclIpMatchIp: '10.215.145.187' found
2017/06/05 14:18:08.000 kid1| 28,3| Acl.cc(158) matches: checked: all = 1
2017/06/05 14:18:08.000 kid1| 28,3| Acl.cc(158) matches: checked: (ssl_bump rule) = 1
2017/06/05 14:18:08.000 kid1| 28,3| Acl.cc(158) matches: checked: (ssl_bump rules) = 1
2017/06/05 14:18:08.000 kid1| 28,3| Checklist.cc(63) markFinished: 0x80d21df8 answer ALLOWED for match
2017/06/05 14:18:08.000 kid1| 28,3| Checklist.cc(163) checkCallback: ACLChecklist::checkCallback: 0x80d21df8 answer=ALLOWED
2017/06/05 14:18:08.000 kid1| 28,4| FilledChecklist.cc(66) ~ACLFilledChecklist: ACLFilledChecklist destroyed 0x80d21df8
2017/06/05 14:18:08.000 kid1| 28,4| Checklist.cc(197) ~ACLChecklist: ACLChecklist::~ACLChecklist: destroyed 0x80d21df8
2017/06/05 14:18:08.000 kid1| 83,5| PeerConnector.cc(418) checkForPeekAndSpliceMatched: Will check for peek and splice on FD 30
2017/06/05 14:18:08.000 kid1| 5,5| ModEpoll.cc(116) SetSelect: FD 30, type=2, handler=1, client_data=0x80d36e50, timeout=0
2017/06/05 14:18:08.000 kid1| 83,5| PeerConnector.cc(436) checkForPeekAndSpliceMatched: Retry the fwdNegotiateSSL on FD 30
2017/06/05 14:18:08.000 kid1| 83,5| bio.cc(95) write: FD 30 wrote 150 <= 150
2017/06/05 14:18:08.000 kid1| 83,5| bio.cc(576) squid_bio_ctrl: 0x80e60900 11(0, 0)
2017/06/05 14:18:08.000 kid1| 83,5| bio.cc(118) read: FD 30 read -1 <= 5
2017/06/05 14:18:08.000 kid1| 83,5| bio.cc(123) read: error: 11 ignored: 1
2017/06/05 14:18:08.000 kid1| 5,3| comm.cc(553) commSetConnTimeout: local=10.215.145.187:39368 remote=46.51.187.18:443 FD 30 flags=25 timeout 59
2017/06/05 14:18:08.000 kid1| 5,5| ModEpoll.cc(116) SetSelect: FD 30, type=1, handler=1, client_data=0x80d36e50, timeout=0
2017/06/05 14:18:08.000 kid1| 5,5| ModEpoll.cc(116) SetSelect: FD 30, type=2, handler=0, client_data=0, timeout=0
2017/06/05 14:18:08.096 kid1| 83,5| bio.cc(118) read: FD 30 read 5 <= 5
2017/06/05 14:18:08.096 kid1| 83,5| bio.cc(118) read: FD 30 read 1 <= 1
2017/06/05 14:18:08.096 kid1| 83,5| bio.cc(118) read: FD 30 read 5 <= 5
2017/06/05 14:18:08.096 kid1| 83,5| bio.cc(118) read: FD 30 read 64 <= 64
2017/06/05 14:18:08.096 kid1| 83,5| bio.cc(576) squid_bio_ctrl: 0x80e60900 7(0, 0x80e6f868)
2017/06/05 14:18:08.096 kid1| 83,5| PeerConnector.cc(307) serverCertificateVerified: HTTPS server CN: *.acms.com bumped: local=10.215.145.187:39368 remote=46.51.187.18:443 FD 30 flags=25
2017/06/05 14:18:08.096 kid1| 5,5| comm.cc(1038) comm_remove_close_handler: comm_remove_close_handler: FD 30, AsyncCall=0x80e60820*2
2017/06/05 14:18:08.096 kid1| 9,5| AsyncCall.cc(56) cancel: will not call Ssl::PeerConnector::commCloseHandler [call2554] because comm_remove_close_handler
2017/06/05 14:18:08.096 kid1| 17,4| AsyncCall.cc(93) ScheduleCall: PeerConnector.cc(742) will call FwdState::ConnectedToPeer(0x80d4b730, local=10.215.145.187:39368 remote=46.51.187.18:443 FD 30 flags=25, 0/0) [call2552]
2017/06/05 14:18:08.096 kid1| 93,5| AsyncJob.cc(137) callEnd: Ssl::PeerConnector::negotiateSsl() ends job [ FD 30 job59]
2017/06/05 14:18:08.096 kid1| 83,5| PeerConnector.cc(58) ~PeerConnector: Peer connector 0x80d36e50 gone
2017/06/05 14:18:08.096 kid1| 93,5| AsyncJob.cc(40) ~AsyncJob: AsyncJob destructed, this=0x80d36e74 type=Ssl::PeerConnector [job59]
2017/06/05 14:18:08.096 kid1| 17,4| AsyncCallQueue.cc(55) fireNext: entering FwdState::ConnectedToPeer(0x80d4b730, local=10.215.145.187:39368 remote=46.51.187.18:443 FD 30 flags=25, 0/0)
2017/06/05 14:18:08.096 kid1| 17,4| AsyncCall.cc(38) make: make call FwdState::ConnectedToPeer [call2552]
2017/06/05 14:18:08.096 kid1| 17,3| FwdState.cc(905) dispatch: local=46.51.187.18:443 remote=10.215.145.187 FD 29 flags=17: Fetching CONNECT 46.51.187.18:443
2017/06/05 14:18:08.096 kid1| 14,4| ipcache.cc(501) ipcache_nbgethostbyname: ipcache_nbgethostbyname: Name '46.51.187.18'.
2017/06/05 14:18:08.096 kid1| 14,4| ipcache.cc(810) ipcacheCheckNumeric: ipcacheCheckNumeric: HIT_BYPASS for '46.51.187.18' == 46.51.187.18
2017/06/05 14:18:08.096 kid1| 14,4| ipcache.cc(514) ipcache_nbgethostbyname: ipcache_nbgethostbyname: BYPASS for '46.51.187.18' (already numeric)
2017/06/05 14:18:08.096 kid1| 14,5| net_db.cc(365) networkFromInaddr: networkFromInaddr : Masked IPv4 Address to 46.51.187.18/24.
2017/06/05 14:18:08.096 kid1| 14,5| net_db.cc(369) networkFromInaddr: networkFromInaddr : Masked IPv4 Address to 46.51.187.18/24.
2017/06/05 14:18:08.096 kid1| 14,5| net_db.cc(365) networkFromInaddr: networkFromInaddr : Masked IPv4 Address to 46.51.187.18/24.
2017/06/05 14:18:08.096 kid1| 14,5| net_db.cc(369) networkFromInaddr: networkFromInaddr : Masked IPv4 Address to 46.51.187.18/24.
2017/06/05 14:18:08.097 kid1| 38,3| net_db.cc(325) netdbSendPing: netdbSendPing: pinging 46.51.187.18
2017/06/05 14:18:08.097 kid1| 37,4| IcmpSquid.cc(181) DomainPing: '46.51.187.18' (46.51.187.18)
2017/06/05 14:18:08.097 kid1| 37,2| IcmpSquid.cc(90) SendEcho: to 46.51.187.18, opcode 3, len 12
2017/06/05 14:18:08.097| 42,2| IcmpPinger.cc(198) Recv:  Pass 46.51.187.18 off to ICMPv4 module.
2017/06/05 14:18:08.097| 42,5| Icmp4.cc(135) SendEcho: Send ICMP packet to 46.51.187.18.
2017/06/05 14:18:08.097| 42,2| Icmp.cc(95) Log: pingerLog: 1496665088.097213 46.51.187.18                                  32 
2017/06/05 14:18:08.097 kid1| 17,4| AsyncCall.cc(26) AsyncCall: The AsyncCall ConnStateData::ConnStateData::httpsPeeked constructed, this=0x80e60728 [call2561]
2017/06/05 14:18:08.097 kid1| 17,4| AsyncCall.cc(93) ScheduleCall: FwdState.cc(952) will call ConnStateData::ConnStateData::httpsPeeked(local=10.215.145.187:39368 remote=46.51.187.18:443 FD 30 flags=25) [call2561]
2017/06/05 14:18:08.097 kid1| 17,3| FwdState.cc(446) unregister: 46.51.187.18:443
2017/06/05 14:18:08.097 kid1| 5,5| comm.cc(1011) comm_remove_close_handler: comm_remove_close_handler: FD 30, handler=1, data=0x80d4b730
2017/06/05 14:18:08.097 kid1| 5,4| AsyncCall.cc(56) cancel: will not call SomeCloseHandler [call2551] because comm_remove_close_handler
2017/06/05 14:18:08.097 kid1| 17,3| FwdState.cc(471) complete: 46.51.187.18:443
2017/06/05 14:18:08.097 kid1| 17,3| FwdState.cc(1043) reforward: 46.51.187.18:443?
2017/06/05 14:18:08.097 kid1| 17,3| FwdState.cc(1058) reforward: No alternative forwarding paths left
2017/06/05 14:18:08.097 kid1| 17,3| FwdState.cc(495) complete: server (FD closed) not re-forwarding status 0
2017/06/05 14:18:08.097 kid1| 20,3| store.cc(1053) complete: storeComplete: '52E847A2F8AF07D9783AAE4A91F2E9E6'
2017/06/05 14:18:08.097 kid1| 20,3| store.cc(1342) validLength: storeEntryValidLength: Checking '52E847A2F8AF07D9783AAE4A91F2E9E6'
2017/06/05 14:18:08.097 kid1| 20,5| store.cc(1344) validLength: storeEntryValidLength:     object_len = 0
2017/06/05 14:18:08.097 kid1| 20,5| store.cc(1345) validLength: storeEntryValidLength:         hdr_sz = 0
2017/06/05 14:18:08.097 kid1| 20,5| store.cc(1346) validLength: storeEntryValidLength: content_length = -1
2017/06/05 14:18:08.097 kid1| 20,5| store.cc(1349) validLength: storeEntryValidLength: Unspecified content length: 52E847A2F8AF07D9783AAE4A91F2E9E6
2017/06/05 14:18:08.097 kid1| 20,3| store.cc(985) checkCachable: StoreEntry::checkCachable: NO: private key
2017/06/05 14:18:08.097 kid1| 20,3| store.cc(500) setReleaseFlag: StoreEntry::setReleaseFlag: '52E847A2F8AF07D9783AAE4A91F2E9E6'
2017/06/05 14:18:08.097 kid1| 20,3| store_swapout.cc(381) mayStartSwapOut: not cachable
2017/06/05 14:18:08.097 kid1| 20,2| store.cc(954) checkCachable: StoreEntry::checkCachable: NO: not cachable
2017/06/05 14:18:08.097 kid1| 90,3| store_client.cc(732) invokeHandlers: InvokeHandlers: 52E847A2F8AF07D9783AAE4A91F2E9E6
2017/06/05 14:18:08.097 kid1| 90,3| store_client.cc(738) invokeHandlers: StoreEntry::InvokeHandlers: checking client #0
2017/06/05 14:18:08.097 kid1| 46,5| access_log.cc(289) stopPeerClock: First connection started: 1496665087.724223, current total response time value: -1
2017/06/05 14:18:08.097 kid1| 90,3| store_client.cc(758) storePendingNClients: storePendingNClients: returning 1
2017/06/05 14:18:08.097 kid1| 17,3| FwdState.cc(265) ~FwdState: FwdState destructor starting
2017/06/05 14:18:08.097 kid1| 20,3| store.cc(522) unlock: FwdState unlocking key 52E847A2F8AF07D9783AAE4A91F2E9E6 e:=sp2XDIV/0x80e0a448*2
2017/06/05 14:18:08.097 kid1| 17,3| AsyncCall.cc(56) cancel: will not call fwdConnectDoneWrapper [call2544] because FwdState destructed
2017/06/05 14:18:08.097 kid1| 17,3| FwdState.cc(292) ~FwdState: FwdState destructor done
2017/06/05 14:18:08.098 kid1| 17,4| AsyncCallQueue.cc(57) fireNext: leaving FwdState::ConnectedToPeer(0, local=10.215.145.187:39368 remote=46.51.187.18:443 FD 30 flags=25, 0/0)
2017/06/05 14:18:08.098 kid1| 17,4| AsyncCallQueue.cc(55) fireNext: entering ConnStateData::ConnStateData::httpsPeeked(local=10.215.145.187:39368 remote=46.51.187.18:443 FD 30 flags=25)
2017/06/05 14:18:08.098 kid1| 17,4| AsyncCall.cc(38) make: make call ConnStateData::ConnStateData::httpsPeeked [call2561]
2017/06/05 14:18:08.098 kid1| 17,4| AsyncJob.cc(123) callStart: Http::Server status in: [ job56]
2017/06/05 14:18:08.098 kid1| 33,3| client_side.cc(5106) unpinConnection: 
2017/06/05 14:18:08.098 kid1| 33,3| client_side.cc(4938) pinNewConnection: local=10.215.145.187:39368 remote=46.51.187.18:443 FD 30 flags=25
2017/06/05 14:18:08.098 kid1| 33,5| AsyncCall.cc(26) AsyncCall: The AsyncCall ConnStateData::clientPinnedConnectionClosed constructed, this=0x80e84ea0 [call2562]
2017/06/05 14:18:08.098 kid1| 5,5| comm.cc(993) comm_add_close_handler: comm_add_close_handler: FD 30, AsyncCall=0x80e84ea0*1
2017/06/05 14:18:08.098 kid1| 33,3| AsyncCall.cc(26) AsyncCall: The AsyncCall ConnStateData::clientPinnedConnectionRead constructed, this=0x80e84790 [call2563]
2017/06/05 14:18:08.098 kid1| 5,5| Read.cc(58) comm_read_base: comm_read, queueing read for local=10.215.145.187:39368 remote=46.51.187.18:443 FD 30 flags=25; asynCall 0x80e84790*1
2017/06/05 14:18:08.098 kid1| 5,5| ModEpoll.cc(116) SetSelect: FD 30, type=1, handler=1, client_data=0x808b490c, timeout=0
2017/06/05 14:18:08.098 kid1| 33,5| client_side.cc(4409) httpsPeeked: bumped HTTPS server: 46.51.187.18
2017/06/05 14:18:08.098 kid1| 87,3| clientStream.cc(202) clientStreamDetach: clientStreamDetach: Detaching node 0x80d39c38
2017/06/05 14:18:08.098 kid1| 87,3| clientStream.cc(287) clientStreamFree: Freeing clientStreamNode 0x80d39c38
2017/06/05 14:18:08.098 kid1| 87,3| clientStream.cc(223) clientStreamDetach: clientStreamDetach: Calling 1 with cbdata 0x80d39bc8
2017/06/05 14:18:08.098 kid1| 87,3| clientStream.cc(202) clientStreamDetach: clientStreamDetach: Detaching node 0x80d39be8
2017/06/05 14:18:08.098 kid1| 87,3| clientStream.cc(287) clientStreamFree: Freeing clientStreamNode 0x80d39be8
2017/06/05 14:18:08.098 kid1| 33,3| client_side_request.cc(246) ~ClientHttpRequest: httpRequestFree: 46.51.187.18:443
2017/06/05 14:18:08.098 kid1| 33,5| client_side.cc(576) logRequest: logging half-baked transaction: 46.51.187.18:443
2017/06/05 14:18:08.098 kid1| 28,3| Checklist.cc(70) preCheck: 0xbf839e8c checking fast ACLs
2017/06/05 14:18:08.098 kid1| 28,5| Acl.cc(138) matches: checking access_log daemon:/var/log/squid/access.test.log
2017/06/05 14:18:08.098 kid1| 28,5| Acl.cc(138) matches: checking (access_log daemon:/var/log/squid/access.test.log line)
2017/06/05 14:18:08.098 kid1| 28,3| Acl.cc(158) matches: checked: (access_log daemon:/var/log/squid/access.test.log line) = 1
2017/06/05 14:18:08.098 kid1| 28,3| Acl.cc(158) matches: checked: access_log daemon:/var/log/squid/access.test.log = 1
2017/06/05 14:18:08.098 kid1| 28,3| Checklist.cc(63) markFinished: 0xbf839e8c answer ALLOWED for match
2017/06/05 14:18:08.098 kid1| 50,5| ModDaemon.cc(65) logfileNewBuffer: logfileNewBuffer: daemon:/var/log/squid/access.test.log: new buffer
2017/06/05 14:18:08.098 kid1| 50,3| ModDaemon.cc(172) logfile_mod_daemon_append: logfile_mod_daemon_append: daemon:/var/log/squid/access.test.log: appending 1 bytes
2017/06/05 14:18:08.098 kid1| 50,3| ModDaemon.cc(176) logfile_mod_daemon_append: logfile_mod_daemon_append: current buffer has 0 of 32768 bytes before append
2017/06/05 14:18:08.098 kid1| 50,3| ModDaemon.cc(172) logfile_mod_daemon_append: logfile_mod_daemon_append: daemon:/var/log/squid/access.test.log: appending 107 bytes
2017/06/05 14:18:08.098 kid1| 50,3| ModDaemon.cc(176) logfile_mod_daemon_append: logfile_mod_daemon_append: current buffer has 1 of 32768 bytes before append
2017/06/05 14:18:08.098 kid1| 50,3| ModDaemon.cc(172) logfile_mod_daemon_append: logfile_mod_daemon_append: daemon:/var/log/squid/access.test.log: appending 2 bytes
2017/06/05 14:18:08.098 kid1| 50,3| ModDaemon.cc(176) logfile_mod_daemon_append: logfile_mod_daemon_append: current buffer has 108 of 32768 bytes before append
2017/06/05 14:18:08.098 kid1| 5,5| ModEpoll.cc(116) SetSelect: FD 21, type=2, handler=1, client_data=0x808fb030, timeout=0
2017/06/05 14:18:08.098 kid1| 28,4| FilledChecklist.cc(66) ~ACLFilledChecklist: ACLFilledChecklist destroyed 0xbf839e8c
2017/06/05 14:18:08.098 kid1| 28,4| Checklist.cc(197) ~ACLChecklist: ACLChecklist::~ACLChecklist: destroyed 0xbf839e8c
2017/06/05 14:18:08.098 kid1| 93,5| AsyncCall.cc(26) AsyncCall: The AsyncCall Initiate::noteInitiatorAborted constructed, this=0x80e84cb0 [call2564]
2017/06/05 14:18:08.098 kid1| 93,5| AsyncCall.cc(93) ScheduleCall: Initiator.cc(40) will call Initiate::noteInitiatorAborted() [call2564]
2017/06/05 14:18:08.098 kid1| 93,5| AsyncJob.cc(40) ~AsyncJob: AsyncJob destructed, this=0x80d37708 type=ClientHttpRequest [job57]
2017/06/05 14:18:08.098 kid1| 28,3| Checklist.cc(70) preCheck: 0xbf83a14c checking fast ACLs
2017/06/05 14:18:08.099 kid1| 28,5| Acl.cc(138) matches: checking sslproxy_cert_sign signUntrusted
2017/06/05 14:18:08.099 kid1| 28,5| Acl.cc(138) matches: checking (sslproxy_cert_sign signUntrusted line)
2017/06/05 14:18:08.099 kid1| 28,5| Acl.cc(138) matches: checking ssl::certUntrusted
2017/06/05 14:18:08.099 kid1| 28,3| Acl.cc(158) matches: checked: ssl::certUntrusted = 0
2017/06/05 14:18:08.099 kid1| 28,3| Acl.cc(158) matches: checked: (sslproxy_cert_sign signUntrusted line) = 0
2017/06/05 14:18:08.099 kid1| 28,3| Acl.cc(158) matches: checked: sslproxy_cert_sign signUntrusted = 0
2017/06/05 14:18:08.099 kid1| 28,3| Checklist.cc(63) markFinished: 0xbf83a14c answer DENIED for ACLs failed to match
2017/06/05 14:18:08.099 kid1| 28,3| Checklist.cc(70) preCheck: 0xbf83a14c checking fast ACLs
2017/06/05 14:18:08.099 kid1| 28,5| Acl.cc(138) matches: checking sslproxy_cert_sign signSelf
2017/06/05 14:18:08.099 kid1| 28,5| Acl.cc(138) matches: checking (sslproxy_cert_sign signSelf line)
2017/06/05 14:18:08.099 kid1| 28,5| Acl.cc(138) matches: checking ssl::certSelfSigned
2017/06/05 14:18:08.099 kid1| 28,3| Acl.cc(158) matches: checked: ssl::certSelfSigned = 0
2017/06/05 14:18:08.099 kid1| 28,3| Acl.cc(158) matches: checked: (sslproxy_cert_sign signSelf line) = 0
2017/06/05 14:18:08.099 kid1| 28,3| Acl.cc(158) matches: checked: sslproxy_cert_sign signSelf = 0
2017/06/05 14:18:08.099 kid1| 28,3| Checklist.cc(63) markFinished: 0xbf83a14c answer DENIED for ACLs failed to match
2017/06/05 14:18:08.099 kid1| 28,3| Checklist.cc(70) preCheck: 0xbf83a14c checking fast ACLs
2017/06/05 14:18:08.099 kid1| 28,5| Acl.cc(138) matches: checking sslproxy_cert_sign signTrusted
2017/06/05 14:18:08.099 kid1| 28,5| Acl.cc(138) matches: checking (sslproxy_cert_sign signTrusted line)
2017/06/05 14:18:08.099 kid1| 28,5| Acl.cc(138) matches: checking all
2017/06/05 14:18:08.099 kid1| 28,3| Ip.cc(539) match: aclIpMatchIp: '10.215.145.187' found
2017/06/05 14:18:08.099 kid1| 28,3| Acl.cc(158) matches: checked: all = 1
2017/06/05 14:18:08.099 kid1| 28,3| Acl.cc(158) matches: checked: (sslproxy_cert_sign signTrusted line) = 1
2017/06/05 14:18:08.099 kid1| 28,3| Acl.cc(158) matches: checked: sslproxy_cert_sign signTrusted = 1
2017/06/05 14:18:08.099 kid1| 28,3| Checklist.cc(63) markFinished: 0xbf83a14c answer ALLOWED for match
2017/06/05 14:18:08.099 kid1| 28,4| FilledChecklist.cc(66) ~ACLFilledChecklist: ACLFilledChecklist destroyed 0xbf83a14c
2017/06/05 14:18:08.099 kid1| 28,4| Checklist.cc(197) ~ACLChecklist: ACLChecklist::~ACLChecklist: destroyed 0xbf83a14c
2017/06/05 14:18:08.099 kid1| 33,5| client_side.cc(4135) getSslContextStart: Generating SSL certificate for *.acms.com using ssl_crtd.
2017/06/05 14:18:08.099 kid1| 33,5| client_side.cc(4139) getSslContextStart: SSL crtd request: new_certificate 5112 host=*.acms.com
2017/06/05 14:18:08.099 kid1| 84,5| helper.cc(1167) GetFirstAvailable: GetFirstAvailable: Running servers 5
2017/06/05 14:18:08.099 kid1| 5,5| AsyncCall.cc(26) AsyncCall: The AsyncCall helperDispatchWriteDone constructed, this=0x80e6f6b8 [call2565]
2017/06/05 14:18:08.100 kid1| 5,5| Write.cc(35) Write: local=[::] remote=[::] FD 10 flags=1: sz 5134: asynCall 0x80e6f6b8*1
2017/06/05 14:18:08.100 kid1| 5,5| ModEpoll.cc(116) SetSelect: FD 10, type=2, handler=1, client_data=0x808b42a4, timeout=0
2017/06/05 14:18:08.100 kid1| 84,5| helper.cc(1309) helperDispatch: helperDispatch: Request sent to ssl_crtd #Hlpr1, 5134 bytes
2017/06/05 14:18:08.100 kid1| 17,4| AsyncJob.cc(152) callEnd: Http::Server status out: [ job56]
2017/06/05 14:18:08.100 kid1| 17,4| AsyncCallQueue.cc(57) fireNext: leaving ConnStateData::ConnStateData::httpsPeeked(local=10.215.145.187:39368 remote=46.51.187.18:443 FD 30 flags=25)
2017/06/05 14:18:08.100 kid1| 93,5| AsyncCallQueue.cc(55) fireNext: entering Initiate::noteInitiatorAborted()
2017/06/05 14:18:08.100 kid1| 93,5| AsyncCall.cc(38) make: make call Initiate::noteInitiatorAborted [call2564]
2017/06/05 14:18:08.100 kid1| 93,5| AsyncCall.cc(56) cancel: will not call Initiate::noteInitiatorAborted [call2564] because job gone
2017/06/05 14:18:08.100 kid1| 93,5| AsyncCall.cc(48) make: will not call Initiate::noteInitiatorAborted [call2564] because of job gone
2017/06/05 14:18:08.100 kid1| 93,5| AsyncCallQueue.cc(57) fireNext: leaving Initiate::noteInitiatorAborted()
2017/06/05 14:18:08.100 kid1| 50,3| ModDaemon.cc(108) logfileHandleWrite: daemon:/var/log/squid/access.test.log: write returned 110
2017/06/05 14:18:08.100 kid1| 5,5| Write.cc(66) HandleWrite: local=[::] remote=[::] FD 10 flags=1: off 0, sz 5134.
2017/06/05 14:18:08.100 kid1| 5,5| Write.cc(108) HandleWrite: write() returns 5134
2017/06/05 14:18:08.100 kid1| 5,3| IoCallback.cc(116) finish: called for local=[::] remote=[::] FD 10 flags=1 (0, 0)
2017/06/05 14:18:08.100 kid1| 5,5| AsyncCall.cc(93) ScheduleCall: IoCallback.cc(135) will call helperDispatchWriteDone(local=[::] remote=[::] FD 10 flags=1, data=0x808d9390, size=5134, buf=0x80dd0608) [call2565]
2017/06/05 14:18:08.100 kid1| 5,5| AsyncCallQueue.cc(55) fireNext: entering helperDispatchWriteDone(local=[::] remote=[::] FD 10 flags=1, data=0x808d9390, size=5134, buf=0x80dd0608)
2017/06/05 14:18:08.100 kid1| 5,5| AsyncCall.cc(38) make: make call helperDispatchWriteDone [call2565]
2017/06/05 14:18:08.100 kid1| 5,5| AsyncCallQueue.cc(57) fireNext: leaving helperDispatchWriteDone(local=[::] remote=[::] FD 10 flags=1, data=0x808d9390, size=5134, buf=0x80dd0608)
2017/06/05 14:18:08.100 kid1| 5,5| ModEpoll.cc(116) SetSelect: FD 21, type=2, handler=0, client_data=0, timeout=0
2017/06/05 14:18:08.100 kid1| 5,5| ModEpoll.cc(116) SetSelect: FD 10, type=2, handler=0, client_data=0, timeout=0
2017/06/05 14:18:08.106 kid1| 5,3| Read.cc(144) HandleRead: FD 10, size 8191, retval 3161, errno 0
2017/06/05 14:18:08.106 kid1| 5,3| IoCallback.cc(116) finish: called for local=[::] remote=[::] FD 10 flags=1 (0, 0)
2017/06/05 14:18:08.106 kid1| 5,4| AsyncCall.cc(93) ScheduleCall: IoCallback.cc(135) will call helperHandleRead(local=[::] remote=[::] FD 10 flags=1, data=0x808d9390, size=3161, buf=0x80d904b8) [call2299]
2017/06/05 14:18:08.106 kid1| 5,4| AsyncCallQueue.cc(55) fireNext: entering helperHandleRead(local=[::] remote=[::] FD 10 flags=1, data=0x808d9390, size=3161, buf=0x80d904b8)
2017/06/05 14:18:08.106 kid1| 5,4| AsyncCall.cc(38) make: make call helperHandleRead [call2299]
2017/06/05 14:18:08.106 kid1| 84,5| helper.cc(866) helperHandleRead: helperHandleRead: 3161 bytes from ssl_crtd #Hlpr1
2017/06/05 14:18:08.106 kid1| 84,3| helper.cc(892) helperHandleRead: helperHandleRead: end of reply found
2017/06/05 14:18:08.106 kid1| 84,3| Reply.cc(29) parse: Parsing helper buffer
2017/06/05 14:18:08.106 kid1| 84,3| Reply.cc(48) parse: Buff length is larger than 2
2017/06/05 14:18:08.106 kid1| 84,3| Reply.cc(52) parse: helper Result = OK
2017/06/05 14:18:08.106 kid1| 33,5| client_side.cc(3992) sslCrtdHandleReply: Certificate for 46.51.187.18 was successfully recieved from ssl_crtd
2017/06/05 14:18:08.106 kid1| 33,5| client_side.cc(4394) doPeekAndSpliceStep: PeekAndSplice mode, proceed with client negotiation. Currrent state:SSLv2/v3 read client hello A
2017/06/05 14:18:08.106 kid1| 5,5| ModEpoll.cc(116) SetSelect: FD 29, type=2, handler=1, client_data=0x80e07e00, timeout=0
2017/06/05 14:18:08.106 kid1| 84,5| helper.cc(1167) GetFirstAvailable: GetFirstAvailable: Running servers 5
2017/06/05 14:18:08.106 kid1| 5,4| AsyncCall.cc(26) AsyncCall: The AsyncCall helperHandleRead constructed, this=0x80e84cb0 [call2566]
2017/06/05 14:18:08.106 kid1| 5,5| Read.cc(58) comm_read_base: comm_read, queueing read for local=[::] remote=[::] FD 10 flags=1; asynCall 0x80e84cb0*1
2017/06/05 14:18:08.107 kid1| 5,5| ModEpoll.cc(116) SetSelect: FD 10, type=1, handler=1, client_data=0x808b427c, timeout=0
2017/06/05 14:18:08.107 kid1| 5,4| AsyncCallQueue.cc(57) fireNext: leaving helperHandleRead(local=[::] remote=[::] FD 10 flags=1, data=0x808d9390, size=3161, buf=0x80d904b8)
2017/06/05 14:18:08.107 kid1| 83,5| bio.cc(576) squid_bio_ctrl: 0x80e07048 6(0, 0x80e14ad0)
2017/06/05 14:18:08.107 kid1| 83,5| bio.cc(95) write: FD 29 wrote 1135 <= 1135
2017/06/05 14:18:08.107 kid1| 83,5| bio.cc(576) squid_bio_ctrl: 0x80e07048 11(0, 0)
2017/06/05 14:18:08.107 kid1| 83,5| bio.cc(118) read: FD 29 read -1 <= 5
2017/06/05 14:18:08.107 kid1| 83,5| bio.cc(123) read: error: 11 ignored: 1
2017/06/05 14:18:08.107 kid1| 5,5| ModEpoll.cc(116) SetSelect: FD 29, type=1, handler=1, client_data=0x80e07e00, timeout=0
2017/06/05 14:18:08.107 kid1| 5,5| ModEpoll.cc(116) SetSelect: FD 29, type=2, handler=0, client_data=0, timeout=0
2017/06/05 14:18:08.109 kid1| 83,5| bio.cc(118) read: FD 29 read 5 <= 5
2017/06/05 14:18:08.109 kid1| 83,5| bio.cc(118) read: FD 29 read 262 <= 262
2017/06/05 14:18:08.128 kid1| 83,5| bio.cc(118) read: FD 29 read 5 <= 5
2017/06/05 14:18:08.128 kid1| 83,5| bio.cc(118) read: FD 29 read 1 <= 1
2017/06/05 14:18:08.128 kid1| 83,5| bio.cc(118) read: FD 29 read 5 <= 5
2017/06/05 14:18:08.128 kid1| 83,5| bio.cc(118) read: FD 29 read 48 <= 48
2017/06/05 14:18:08.129 kid1| 83,5| bio.cc(95) write: FD 29 wrote 59 <= 59
2017/06/05 14:18:08.129 kid1| 83,5| bio.cc(576) squid_bio_ctrl: 0x80e07048 11(0, 0)
2017/06/05 14:18:08.129 kid1| 83,5| bio.cc(576) squid_bio_ctrl: 0x80e07048 7(0, 0x80e14ad0)
2017/06/05 14:18:08.129 kid1| 83,5| support.cc(2097) store_session_cb: Request to store SSL Session 
2017/06/05 14:18:08.129 kid1| 54,5| MemMap.cc(45) openForWriting: trying to open slot for key 9E7C52DD3CBFFBD0304B94480415CC7C for writing in map [ssl_session_cache]
2017/06/05 14:18:08.129 kid1| 54,5| MemMap.cc(81) openForWritingAt: opened slot at 41 for writing in map [ssl_session_cache]
2017/06/05 14:18:08.129 kid1| 54,5| MemMap.cc(94) closeForWriting: closing slot at 41 for writing and openning for reading in map [ssl_session_cache]
2017/06/05 14:18:08.129 kid1| 83,5| support.cc(2119) store_session_cb: wrote an ssl session entry of size 157 at pos 41
2017/06/05 14:18:08.129 kid1| 83,2| client_side.cc(3796) clientNegotiateSSL: clientNegotiateSSL: New session 0x80e80e30 on FD 29 (10.215.145.187:54815)
2017/06/05 14:18:08.129 kid1| 83,3| client_side.cc(3800) clientNegotiateSSL: clientNegotiateSSL: FD 29 negotiated cipher AES128-SHA
2017/06/05 14:18:08.129 kid1| 83,5| client_side.cc(3816) clientNegotiateSSL: clientNegotiateSSL: FD 29 has no certificate.
2017/06/05 14:18:08.129 kid1| 33,4| client_side.cc(231) readSomeData: local=46.51.187.18:443 remote=10.215.145.187 FD 29 flags=17: reading request...
2017/06/05 14:18:08.129 kid1| 33,5| AsyncCall.cc(26) AsyncCall: The AsyncCall ConnStateData::clientReadRequest constructed, this=0x80e80578 [call2567]
2017/06/05 14:18:08.129 kid1| 5,5| Read.cc(58) comm_read_base: comm_read, queueing read for local=46.51.187.18:443 remote=10.215.145.187 FD 29 flags=17; asynCall 0x80e80578*1
2017/06/05 14:18:08.129 kid1| 5,5| ModEpoll.cc(116) SetSelect: FD 29, type=1, handler=1, client_data=0x808b48b8, timeout=0
2017/06/05 14:18:08.135 kid1| 5,3| IoCallback.cc(116) finish: called for local=46.51.187.18:443 remote=10.215.145.187 FD 29 flags=17 (0, 0)
2017/06/05 14:18:08.135 kid1| 33,5| AsyncCall.cc(93) ScheduleCall: IoCallback.cc(135) will call ConnStateData::clientReadRequest(local=46.51.187.18:443 remote=10.215.145.187 FD 29 flags=17, data=0x80e07e00) [call2567]
2017/06/05 14:18:08.135 kid1| 33,5| AsyncCallQueue.cc(55) fireNext: entering ConnStateData::clientReadRequest(local=46.51.187.18:443 remote=10.215.145.187 FD 29 flags=17, data=0x80e07e00)
2017/06/05 14:18:08.135 kid1| 33,5| AsyncCall.cc(38) make: make call ConnStateData::clientReadRequest [call2567]
2017/06/05 14:18:08.135 kid1| 33,5| AsyncJob.cc(123) callStart: Http::Server status in: [ job56]
2017/06/05 14:18:08.136 kid1| 33,5| client_side.cc(3251) clientReadRequest: local=46.51.187.18:443 remote=10.215.145.187 FD 29 flags=17
2017/06/05 14:18:08.136 kid1| 83,5| bio.cc(118) read: FD 29 read 5 <= 5
2017/06/05 14:18:08.136 kid1| 83,5| bio.cc(118) read: FD 29 read 1568 <= 1568
2017/06/05 14:18:08.136 kid1| 83,2| support.cc(1364) ssl_read_method: SSL FD 29 is pending
2017/06/05 14:18:08.136 kid1| 5,3| Read.cc(91) ReadNow: local=46.51.187.18:443 remote=10.215.145.187 FD 29 flags=17, size 66, retval 66, errno 0
2017/06/05 14:18:08.136 kid1| 33,5| client_side.cc(3200) clientParseRequests: local=46.51.187.18:443 remote=10.215.145.187 FD 29 flags=17: attempting to parse
2017/06/05 14:18:08.136 kid1| 74,5| HttpParser.cc(37) reset: Request buffer is 
2017/06/05 14:18:08.136 kid1| 74,5| HttpParser.cc(47) parseRequestFirstLine: parsing possible request: 
2017/06/05 14:18:08.136 kid1| 74,5| HttpParser.cc(257) HttpParserParseReqLine: Parser: retval -1: from 0->4: method 0->-1; url -1->-1; version -1->-1 (0/0)
2017/06/05 14:18:08.136 kid1| 93,5| AsyncJob.cc(34) AsyncJob: AsyncJob constructed, this=0x80d227d0 type=ClientHttpRequest [job60]
2017/06/05 14:18:08.136 kid1| 87,3| clientStream.cc(144) clientStreamInsertHead: clientStreamInsertHead: Inserted node 0x80d39c38 with data 0x80d38b44 after head
2017/06/05 14:18:08.136 kid1| 33,5| client_side.cc(3221) clientParseRequests: local=46.51.187.18:443 remote=10.215.145.187 FD 29 flags=17: done parsing a request
2017/06/05 14:18:08.136 kid1| 5,4| AsyncCall.cc(26) AsyncCall: The AsyncCall clientLifetimeTimeout constructed, this=0x80e14ad0 [call2568]
2017/06/05 14:18:08.136 kid1| 5,3| comm.cc(553) commSetConnTimeout: local=46.51.187.18:443 remote=10.215.145.187 FD 29 flags=17 timeout 86400
2017/06/05 14:18:08.136 kid1| 33,2| client_side.cc(2584) clientProcessRequest: clientProcessRequest: Invalid Request
2017/06/05 14:18:08.136 kid1| 33,4| client_side.cc(2455) quitAfterError: Will close after error: local=46.51.187.18:443 remote=10.215.145.187 FD 29 flags=17
2017/06/05 14:18:08.136 kid1| 20,3| store.cc(779) storeCreatePureEntry: storeCreateEntry: 'error:invalid-request'
2017/06/05 14:18:08.136 kid1| 20,5| store.cc(371) StoreEntry: StoreEntry constructed, this=0x80e05f48
2017/06/05 14:18:08.136 kid1| 20,3| MemObject.cc(97) MemObject: new MemObject 0x80e71638
2017/06/05 14:18:08.136 kid1| 20,3| store.cc(500) setReleaseFlag: StoreEntry::setReleaseFlag: '[null_store_key]'
2017/06/05 14:18:08.136 kid1| 20,3| store_key_md5.cc(89) storeKeyPrivate: storeKeyPrivate: NONE error:invalid-request
2017/06/05 14:18:08.136 kid1| 20,3| store.cc(448) hashInsert: StoreEntry::hashInsert: Inserting Entry e:=XI/0x80e05f48*0 key '908891323D546174B25B4E7FDFB415C7'
2017/06/05 14:18:08.136 kid1| 20,3| store.cc(484) lock: storeCreateEntry locked key 908891323D546174B25B4E7FDFB415C7 e:=XIV/0x80e05f48*1
2017/06/05 14:18:08.137 kid1| 4,4| errorpage.cc(603) errorAppendEntry: Creating an error page for entry 0x80e05f48 with errorstate 0x80e80440 page id 20
2017/06/05 14:18:08.137 kid1| 4,2| errorpage.cc(1262) BuildContent: No existing error page language negotiated for ERR_INVALID_REQ. Using default error file.
2017/06/05 14:18:08.137 kid1| 4,3| errorpage.cc(1101) Convert: errorConvert: %%l --> '/*
2017/06/05 14:18:08.137 kid1| 4,3| errorpage.cc(1101) Convert: errorConvert: %%; --> '%;'
2017/06/05 14:18:08.137 kid1| 4,3| errorpage.cc(1101) Convert: errorConvert: %%c --> 'ERR_INVALID_REQ'
2017/06/05 14:18:08.137 kid1| 4,3| errorpage.cc(1101) Convert: errorConvert: %%R --> ''
2017/06/05 14:18:08.137 kid1| 4,3| errorpage.cc(1101) Convert: errorConvert: %%w --> 'root'
2017/06/05 14:18:08.137 kid1| 4,3| errorpage.cc(1101) Convert: errorConvert: %%W --> '?subject=CacheErrorInfo%20-%20ERR_INVALID_REQ&body=CacheHost%3A%20inf-fw1%0D%0AErrPage%3A%20ERR_INVALID_REQ%0D%0AErr%3A%20%5Bnone%5D%0D%0ATimeStamp%3A%20Mon,%2005%20Jun%202017%2012%3A18%3A08%20GMT%0D%0A%0D%0AClientIP%3A%2010.215.145.187%0D%0A%0D%0AHTTP%20Request%3A%0D%0A%0D%0A%0D%0A'
2017/06/05 14:18:08.137 kid1| 4,3| errorpage.cc(1101) Convert: errorConvert: %%w --> 'root'
2017/06/05 14:18:08.137 kid1| 4,3| errorpage.cc(1101) Convert: errorConvert: %%M --> '[unknown method]'
2017/06/05 14:18:08.137 kid1| 4,3| errorpage.cc(1101) Convert: errorConvert: %%u --> 'error:invalid-request'
2017/06/05 14:18:08.137 kid1| 4,3| errorpage.cc(1101) Convert: errorConvert: %%P --> '[unknown protocol]'
2017/06/05 14:18:08.137 kid1| 4,3| errorpage.cc(1101) Convert: errorConvert: %%T --> 'Mon, 05 Jun 2017 12:18:08 GMT'
2017/06/05 14:18:08.137 kid1| 4,3| errorpage.cc(1101) Convert: errorConvert: %%h --> 'inf-fw1'
2017/06/05 14:18:08.137 kid1| 4,3| errorpage.cc(1101) Convert: errorConvert: %%s --> 'squid/3.5.14'
2017/06/05 14:18:08.137 kid1| 4,3| errorpage.cc(1101) Convert: errorConvert: %%c --> 'ERR_INVALID_REQ'
2017/06/05 14:18:08.137 kid1| 20,3| store.cc(484) lock: StoreEntry::storeErrorResponse locked key 908891323D546174B25B4E7FDFB415C7 e:=XIV/0x80e05f48*2
2017/06/05 14:18:08.137 kid1| 20,3| store.cc(1867) replaceHttpReply: StoreEntry::replaceHttpReply: error:invalid-request
2017/06/05 14:18:08.137 kid1| 20,5| store.cc(834) write: storeWrite: writing 26 bytes for '908891323D546174B25B4E7FDFB415C7'
2017/06/05 14:18:08.137 kid1| 20,5| store.cc(834) write: storeWrite: writing 6 bytes for '908891323D546174B25B4E7FDFB415C7'
2017/06/05 14:18:08.137 kid1| 20,5| store.cc(834) write: storeWrite: writing 2 bytes for '908891323D546174B25B4E7FDFB415C7'
2017/06/05 14:18:08.137 kid1| 20,5| store.cc(834) write: storeWrite: writing 12 bytes for '908891323D546174B25B4E7FDFB415C7'
2017/06/05 14:18:08.137 kid1| 20,5| store.cc(834) write: storeWrite: writing 2 bytes for '908891323D546174B25B4E7FDFB415C7'
2017/06/05 14:18:08.137 kid1| 20,5| store.cc(834) write: storeWrite: writing 12 bytes for '908891323D546174B25B4E7FDFB415C7'
2017/06/05 14:18:08.137 kid1| 20,5| store.cc(834) write: storeWrite: writing 2 bytes for '908891323D546174B25B4E7FDFB415C7'
2017/06/05 14:18:08.137 kid1| 20,5| store.cc(834) write: storeWrite: writing 3 bytes for '908891323D546174B25B4E7FDFB415C7'
2017/06/05 14:18:08.137 kid1| 20,5| store.cc(834) write: storeWrite: writing 2 bytes for '908891323D546174B25B4E7FDFB415C7'
2017/06/05 14:18:08.137 kid1| 20,5| store.cc(834) write: storeWrite: writing 4 bytes for '908891323D546174B25B4E7FDFB415C7'
2017/06/05 14:18:08.138 kid1| 20,5| store.cc(834) write: storeWrite: writing 2 bytes for '908891323D546174B25B4E7FDFB415C7'
2017/06/05 14:18:08.138 kid1| 20,5| store.cc(834) write: storeWrite: writing 29 bytes for '908891323D546174B25B4E7FDFB415C7'
2017/06/05 14:18:08.138 kid1| 20,5| store.cc(834) write: storeWrite: writing 2 bytes for '908891323D546174B25B4E7FDFB415C7'
2017/06/05 14:18:08.138 kid1| 20,5| store.cc(834) write: storeWrite: writing 12 bytes for '908891323D546174B25B4E7FDFB415C7'
2017/06/05 14:18:08.138 kid1| 20,5| store.cc(834) write: storeWrite: writing 2 bytes for '908891323D546174B25B4E7FDFB415C7'
2017/06/05 14:18:08.138 kid1| 20,5| store.cc(834) write: storeWrite: writing 23 bytes for '908891323D546174B25B4E7FDFB415C7'
2017/06/05 14:18:08.138 kid1| 20,5| store.cc(834) write: storeWrite: writing 2 bytes for '908891323D546174B25B4E7FDFB415C7'
2017/06/05 14:18:08.138 kid1| 20,5| store.cc(834) write: storeWrite: writing 14 bytes for '908891323D546174B25B4E7FDFB415C7'
2017/06/05 14:18:08.138 kid1| 20,5| store.cc(834) write: storeWrite: writing 2 bytes for '908891323D546174B25B4E7FDFB415C7'
2017/06/05 14:18:08.138 kid1| 20,5| store.cc(834) write: storeWrite: writing 4 bytes for '908891323D546174B25B4E7FDFB415C7'
2017/06/05 14:18:08.138 kid1| 20,5| store.cc(834) write: storeWrite: writing 2 bytes for '908891323D546174B25B4E7FDFB415C7'
2017/06/05 14:18:08.138 kid1| 20,5| store.cc(834) write: storeWrite: writing 13 bytes for '908891323D546174B25B4E7FDFB415C7'
2017/06/05 14:18:08.138 kid1| 20,5| store.cc(834) write: storeWrite: writing 2 bytes for '908891323D546174B25B4E7FDFB415C7'
2017/06/05 14:18:08.138 kid1| 20,5| store.cc(834) write: storeWrite: writing 17 bytes for '908891323D546174B25B4E7FDFB415C7'
2017/06/05 14:18:08.138 kid1| 20,5| store.cc(834) write: storeWrite: writing 2 bytes for '908891323D546174B25B4E7FDFB415C7'
2017/06/05 14:18:08.138 kid1| 20,5| store.cc(834) write: storeWrite: writing 4 bytes for '908891323D546174B25B4E7FDFB415C7'
2017/06/05 14:18:08.138 kid1| 20,5| store.cc(834) write: storeWrite: writing 2 bytes for '908891323D546174B25B4E7FDFB415C7'
2017/06/05 14:18:08.138 kid1| 20,5| store.cc(834) write: storeWrite: writing 15 bytes for '908891323D546174B25B4E7FDFB415C7'
2017/06/05 14:18:08.138 kid1| 20,5| store.cc(834) write: storeWrite: writing 2 bytes for '908891323D546174B25B4E7FDFB415C7'
2017/06/05 14:18:08.138 kid1| 20,5| store.cc(834) write: storeWrite: writing 16 bytes for '908891323D546174B25B4E7FDFB415C7'
2017/06/05 14:18:08.138 kid1| 20,5| store.cc(834) write: storeWrite: writing 2 bytes for '908891323D546174B25B4E7FDFB415C7'
2017/06/05 14:18:08.138 kid1| 20,5| store.cc(834) write: storeWrite: writing 2 bytes for '908891323D546174B25B4E7FDFB415C7'
2017/06/05 14:18:08.138 kid1| 20,5| store.cc(834) write: storeWrite: writing 2 bytes for '908891323D546174B25B4E7FDFB415C7'
2017/06/05 14:18:08.138 kid1| 20,5| store.cc(834) write: storeWrite: writing 2 bytes for '908891323D546174B25B4E7FDFB415C7'
2017/06/05 14:18:08.138 kid1| 20,5| store.cc(834) write: storeWrite: writing 4062 bytes for '908891323D546174B25B4E7FDFB415C7'
2017/06/05 14:18:08.138 kid1| 20,2| store.cc(954) checkCachable: StoreEntry::checkCachable: NO: not cachable
2017/06/05 14:18:08.138 kid1| 20,3| store_swapout.cc(381) mayStartSwapOut: not cachable
2017/06/05 14:18:08.138 kid1| 20,2| store.cc(954) checkCachable: StoreEntry::checkCachable: NO: not cachable
2017/06/05 14:18:08.138 kid1| 90,3| store_client.cc(732) invokeHandlers: InvokeHandlers: 908891323D546174B25B4E7FDFB415C7
2017/06/05 14:18:08.138 kid1| 90,3| store_client.cc(738) invokeHandlers: StoreEntry::InvokeHandlers: checking client #0
2017/06/05 14:18:08.138 kid1| 20,3| store.cc(1053) complete: storeComplete: '908891323D546174B25B4E7FDFB415C7'
2017/06/05 14:18:08.138 kid1| 20,3| store.cc(1342) validLength: storeEntryValidLength: Checking '908891323D546174B25B4E7FDFB415C7'
2017/06/05 14:18:08.138 kid1| 20,5| store.cc(1344) validLength: storeEntryValidLength:     object_len = 4308
2017/06/05 14:18:08.138 kid1| 20,5| store.cc(1345) validLength: storeEntryValidLength:         hdr_sz = 246
2017/06/05 14:18:08.139 kid1| 20,5| store.cc(1346) validLength: storeEntryValidLength: content_length = 4062
2017/06/05 14:18:08.139 kid1| 20,3| store_swapout.cc(356) mayStartSwapOut:  already rejected
2017/06/05 14:18:08.139 kid1| 20,2| store.cc(954) checkCachable: StoreEntry::checkCachable: NO: not cachable
2017/06/05 14:18:08.139 kid1| 90,3| store_client.cc(732) invokeHandlers: InvokeHandlers: 908891323D546174B25B4E7FDFB415C7
2017/06/05 14:18:08.139 kid1| 90,3| store_client.cc(738) invokeHandlers: StoreEntry::InvokeHandlers: checking client #0
2017/06/05 14:18:08.139 kid1| 20,3| store.cc(522) unlock: StoreEntry::storeErrorResponse unlocking key 908891323D546174B25B4E7FDFB415C7 e:=sXINV/0x80e05f48*2
2017/06/05 14:18:08.139 kid1| 33,5| client_side.cc(1709) pullData: 0 written 0 into local=46.51.187.18:443 remote=10.215.145.187 FD 29 flags=17
2017/06/05 14:18:08.139 kid1| 33,5| client_side.cc(1665) getNextRangeOffset: range: 0; http offset 0; reply 0
2017/06/05 14:18:08.139 kid1| 87,3| clientStream.cc(184) clientStreamRead: clientStreamRead: Calling 1 with cbdata 0x80d39bc8 from node 0x80d39c38
2017/06/05 14:18:08.139 kid1| 90,3| store_client.cc(200) copy: store_client::copy: 908891323D546174B25B4E7FDFB415C7, from 0, for length 4096, cb 1, cbdata 0x80d38b60
2017/06/05 14:18:08.139 kid1| 20,3| store.cc(484) lock: store_client::copy locked key 908891323D546174B25B4E7FDFB415C7 e:=sXINV/0x80e05f48*2
2017/06/05 14:18:08.139 kid1| 90,3| store_client.cc(297) storeClientCopy2: storeClientCopy2: 908891323D546174B25B4E7FDFB415C7
2017/06/05 14:18:08.139 kid1| 33,5| store_client.cc(329) doCopy: store_client::doCopy: co: 0, hi: 4308
2017/06/05 14:18:08.139 kid1| 90,3| store_client.cc(433) scheduleMemRead: store_client::doCopy: Copying normal from memory
2017/06/05 14:18:08.139 kid1| 88,5| client_side_reply.cc(2154) sendMoreData: clientReplyContext::sendMoreData: error:invalid-request, 4096 bytes (4096 new bytes)
2017/06/05 14:18:08.139 kid1| 88,5| client_side_reply.cc(2158) sendMoreData: clientReplyContext::sendMoreData:local=46.51.187.18:443 remote=10.215.145.187 FD 29 flags=17 'error:invalid-request' out.offset=0
2017/06/05 14:18:08.139 kid1| 88,2| client_side_reply.cc(2001) processReplyAccessResult: The reply for NONE error:invalid-request is ALLOWED, because it matched all
2017/06/05 14:18:08.139 kid1| 20,3| store.cc(484) lock: ClientHttpRequest::loggingEntry locked key 908891323D546174B25B4E7FDFB415C7 e:=sXINV/0x80e05f48*3
2017/06/05 14:18:08.139 kid1| 88,3| client_side_reply.cc(2039) processReplyAccessResult: clientReplyContext::sendMoreData: Appending 3850 bytes after 246 bytes of headers
2017/06/05 14:18:08.139 kid1| 87,3| clientStream.cc(162) clientStreamCallback: clientStreamCallback: Calling 1 with cbdata 0x80d38b44 from node 0x80d39be8
2017/06/05 14:18:08.139 kid1| 11,2| client_side.cc(1391) sendStartOfMessage: HTTP Client local=46.51.187.18:443 remote=10.215.145.187 FD 29 flags=17
2017/06/05 14:18:08.139 kid1| 11,2| client_side.cc(1392) sendStartOfMessage: HTTP Client REPLY:
2017/06/05 14:18:08.139 kid1| 33,5| AsyncCall.cc(26) AsyncCall: The AsyncCall clientWriteComplete constructed, this=0x80e73b48 [call2569]
2017/06/05 14:18:08.139 kid1| 5,5| Write.cc(35) Write: local=46.51.187.18:443 remote=10.215.145.187 FD 29 flags=17: sz 4216: asynCall 0x80e73b48*1
2017/06/05 14:18:08.139 kid1| 5,5| ModEpoll.cc(116) SetSelect: FD 29, type=2, handler=1, client_data=0x808b48e0, timeout=0
2017/06/05 14:18:08.139 kid1| 20,3| store.cc(522) unlock: store_client::copy unlocking key 908891323D546174B25B4E7FDFB415C7 e:=sXINV/0x80e05f48*3
2017/06/05 14:18:08.139 kid1| 33,5| client_side.cc(2422) consumeInput: in.buf has 0 unused bytes
2017/06/05 14:18:08.139 kid1| 33,5| AsyncJob.cc(152) callEnd: Http::Server status out: [ job56]
2017/06/05 14:18:08.139 kid1| 33,5| AsyncCallQueue.cc(57) fireNext: leaving ConnStateData::clientReadRequest(local=46.51.187.18:443 remote=10.215.145.187 FD 29 flags=17, data=0x80e07e00)
2017/06/05 14:18:08.140 kid1| 5,5| ModEpoll.cc(116) SetSelect: FD 29, type=1, handler=0, client_data=0, timeout=0
2017/06/05 14:18:08.140 kid1| 5,5| Write.cc(66) HandleWrite: local=46.51.187.18:443 remote=10.215.145.187 FD 29 flags=17: off 0, sz 4216.
2017/06/05 14:18:08.140 kid1| 83,5| bio.cc(95) write: FD 29 wrote 4282 <= 4282
2017/06/05 14:18:08.140 kid1| 5,5| Write.cc(108) HandleWrite: write() returns 4216
2017/06/05 14:18:08.140 kid1| 5,3| IoCallback.cc(116) finish: called for local=46.51.187.18:443 remote=10.215.145.187 FD 29 flags=17 (0, 0)
2017/06/05 14:18:08.140 kid1| 33,5| AsyncCall.cc(93) ScheduleCall: IoCallback.cc(135) will call clientWriteComplete(local=46.51.187.18:443 remote=10.215.145.187 FD 29 flags=17, data=0x80d37b00) [call2569]
2017/06/05 14:18:08.140 kid1| 33,5| AsyncCallQueue.cc(55) fireNext: entering clientWriteComplete(local=46.51.187.18:443 remote=10.215.145.187 FD 29 flags=17, data=0x80d37b00)
2017/06/05 14:18:08.140 kid1| 33,5| AsyncCall.cc(38) make: make call clientWriteComplete [call2569]
2017/06/05 14:18:08.140 kid1| 33,5| client_side.cc(1845) writeComplete: local=46.51.187.18:443 remote=10.215.145.187 FD 29 flags=17, sz 4216, err 0, off 4216, len 4308
2017/06/05 14:18:08.140 kid1| 58,3| HttpReply.cc(520) receivedBodyTooLarge: -246 >? -1
2017/06/05 14:18:08.140 kid1| 33,5| client_side.cc(1709) pullData: 0x80e07350 written 4216 into local=46.51.187.18:443 remote=10.215.145.187 FD 29 flags=17
2017/06/05 14:18:08.140 kid1| 33,5| client_side.cc(1665) getNextRangeOffset: range: 0; http offset 3850; reply 0x80e07350
2017/06/05 14:18:08.140 kid1| 87,3| clientStream.cc(184) clientStreamRead: clientStreamRead: Calling 1 with cbdata 0x80d39bc8 from node 0x80d39c38
2017/06/05 14:18:08.140 kid1| 90,3| store_client.cc(200) copy: store_client::copy: 908891323D546174B25B4E7FDFB415C7, from 4096, for length 4096, cb 1, cbdata 0x80d38b60
2017/06/05 14:18:08.140 kid1| 20,3| store.cc(484) lock: store_client::copy locked key 908891323D546174B25B4E7FDFB415C7 e:=sXINV/0x80e05f48*3
2017/06/05 14:18:08.140 kid1| 90,3| store_client.cc(297) storeClientCopy2: storeClientCopy2: 908891323D546174B25B4E7FDFB415C7
2017/06/05 14:18:08.140 kid1| 33,5| store_client.cc(329) doCopy: store_client::doCopy: co: 4096, hi: 4308
2017/06/05 14:18:08.140 kid1| 90,3| store_client.cc(433) scheduleMemRead: store_client::doCopy: Copying normal from memory
2017/06/05 14:18:08.140 kid1| 88,5| client_side_reply.cc(2154) sendMoreData: clientReplyContext::sendMoreData: error:invalid-request, 4308 bytes (212 new bytes)
2017/06/05 14:18:08.140 kid1| 88,5| client_side_reply.cc(2158) sendMoreData: clientReplyContext::sendMoreData:local=46.51.187.18:443 remote=10.215.145.187 FD 29 flags=17 'error:invalid-request' out.offset=3850
2017/06/05 14:18:08.140 kid1| 87,3| clientStream.cc(162) clientStreamCallback: clientStreamCallback: Calling 1 with cbdata 0x80d38b44 from node 0x80d39be8
2017/06/05 14:18:08.140 kid1| 33,5| AsyncCall.cc(26) AsyncCall: The AsyncCall clientWriteBodyComplete constructed, this=0x80e80578 [call2570]
2017/06/05 14:18:08.140 kid1| 5,5| Write.cc(35) Write: local=46.51.187.18:443 remote=10.215.145.187 FD 29 flags=17: sz 212: asynCall 0x80e80578*1
2017/06/05 14:18:08.140 kid1| 5,5| ModEpoll.cc(116) SetSelect: FD 29, type=2, handler=1, client_data=0x808b48e0, timeout=0
2017/06/05 14:18:08.140 kid1| 20,3| store.cc(522) unlock: store_client::copy unlocking key 908891323D546174B25B4E7FDFB415C7 e:=sXINV/0x80e05f48*3
2017/06/05 14:18:08.140 kid1| 33,5| AsyncCallQueue.cc(57) fireNext: leaving clientWriteComplete(local=46.51.187.18:443 remote=10.215.145.187 FD 29 flags=17, data=0x80d37b00)
2017/06/05 14:18:08.140 kid1| 5,5| ModEpoll.cc(116) SetSelect: FD 29, type=1, handler=0, client_data=0, timeout=0
2017/06/05 14:18:08.140 kid1| 5,5| Write.cc(66) HandleWrite: local=46.51.187.18:443 remote=10.215.145.187 FD 29 flags=17: off 0, sz 212.
2017/06/05 14:18:08.140 kid1| 83,5| bio.cc(95) write: FD 29 wrote 282 <= 282
2017/06/05 14:18:08.140 kid1| 5,5| Write.cc(108) HandleWrite: write() returns 212
2017/06/05 14:18:08.140 kid1| 5,3| IoCallback.cc(116) finish: called for local=46.51.187.18:443 remote=10.215.145.187 FD 29 flags=17 (0, 0)
2017/06/05 14:18:08.140 kid1| 33,5| AsyncCall.cc(93) ScheduleCall: IoCallback.cc(135) will call clientWriteBodyComplete(local=46.51.187.18:443 remote=10.215.145.187 FD 29 flags=17, data=0x80d37b00, size=212, buf=0x80d37b10) [call2570]
2017/06/05 14:18:08.140 kid1| 33,5| AsyncCallQueue.cc(55) fireNext: entering clientWriteBodyComplete(local=46.51.187.18:443 remote=10.215.145.187 FD 29 flags=17, data=0x80d37b00, size=212, buf=0x80d37b10)
2017/06/05 14:18:08.141 kid1| 33,5| AsyncCall.cc(38) make: make call clientWriteBodyComplete [call2570]
2017/06/05 14:18:08.141 kid1| 33,5| client_side.cc(1845) writeComplete: local=46.51.187.18:443 remote=10.215.145.187 FD 29 flags=17, sz 212, err 0, off 4428, len 4308
2017/06/05 14:18:08.141 kid1| 88,3| client_side_reply.cc(1098) storeOKTransferDone: storeOKTransferDone  out.offset=4062 objectLen()=4308 headers_sz=246
2017/06/05 14:18:08.141 kid1| 88,5| client_side_reply.cc(1210) replyStatus: clientReplyStatus: transfer is DONE: 10
2017/06/05 14:18:08.141 kid1| 88,5| client_side_reply.cc(1236) replyStatus: clientReplyStatus: stream complete; keepalive=0
2017/06/05 14:18:08.141 kid1| 33,5| client_side.cc(1866) writeComplete: local=46.51.187.18:443 remote=10.215.145.187 FD 29 flags=17 Stream complete, keepalive is 0
2017/06/05 14:18:08.141 kid1| 33,4| client_side.cc(1819) stopSending: sending error (local=46.51.187.18:443 remote=10.215.145.187 FD 29 flags=17): STREAM_COMPLETE NOKEEPALIVE; old receiving error: none
2017/06/05 14:18:08.141 kid1| 5,3| comm.cc(868) _comm_close: comm_close: start closing FD 29
2017/06/05 14:18:08.141 kid1| 5,4| AsyncCall.cc(26) AsyncCall: The AsyncCall commStartSslClose constructed, this=0x80e73b48 [call2571]
2017/06/05 14:18:08.141 kid1| 5,4| AsyncCall.cc(93) ScheduleCall: comm.cc(902) will call commStartSslClose(FD 29) [call2571]
2017/06/05 14:18:08.141 kid1| 5,3| comm.cc(540) commUnsetFdTimeout: Remove timeout for FD 29
2017/06/05 14:18:08.141 kid1| 5,5| comm.cc(721) commCallCloseHandlers: commCallCloseHandlers: FD 29
2017/06/05 14:18:08.141 kid1| 5,5| comm.cc(729) commCallCloseHandlers: commCallCloseHandlers: ch->handler=0x80dfaaf8*1
2017/06/05 14:18:08.141 kid1| 33,5| AsyncCall.cc(93) ScheduleCall: comm.cc(730) will call ConnStateData::connStateClosed(FD -1, data=0x80e07e00) [call2541]
2017/06/05 14:18:08.141 kid1| 5,4| AsyncCall.cc(26) AsyncCall: The AsyncCall comm_close_complete constructed, this=0x80e14ad0 [call2572]
2017/06/05 14:18:08.141 kid1| 5,4| AsyncCall.cc(93) ScheduleCall: comm.cc(941) will call comm_close_complete(FD 29) [call2572]
2017/06/05 14:18:08.141 kid1| 33,5| AsyncCallQueue.cc(57) fireNext: leaving clientWriteBodyComplete(local=46.51.187.18:443 remote=10.215.145.187 flags=17, data=0x80d37b00, size=212, buf=0x80d37b10)
2017/06/05 14:18:08.141 kid1| 5,4| AsyncCallQueue.cc(55) fireNext: entering commStartSslClose(FD 29)
2017/06/05 14:18:08.141 kid1| 5,4| AsyncCall.cc(38) make: make call commStartSslClose [call2571]
2017/06/05 14:18:08.141 kid1| 83,5| bio.cc(95) write: FD 29 wrote 37 <= 37
2017/06/05 14:18:08.141 kid1| 5,4| AsyncCallQueue.cc(57) fireNext: leaving commStartSslClose(FD 29)
2017/06/05 14:18:08.141 kid1| 33,5| AsyncCallQueue.cc(55) fireNext: entering ConnStateData::connStateClosed(FD -1, data=0x80e07e00)
2017/06/05 14:18:08.141 kid1| 33,5| AsyncCall.cc(38) make: make call ConnStateData::connStateClosed [call2541]
2017/06/05 14:18:08.141 kid1| 33,5| AsyncJob.cc(123) callStart: Http::Server status in: [ job56]
2017/06/05 14:18:08.141 kid1| 93,4| AsyncJob.cc(55) deleteThis: Http::Server will NOT delete in-call job, reason: ConnStateData::connStateClosed
2017/06/05 14:18:08.142 kid1| 93,5| AsyncJob.cc(137) callEnd: ConnStateData::connStateClosed(FD -1, data=0x80e07e00) ends job [Stopped, reason:ConnStateData::connStateClosed job56]
2017/06/05 14:18:08.142 kid1| 33,2| client_side.cc(815) swanSong: local=46.51.187.18:443 remote=10.215.145.187 flags=17
2017/06/05 14:18:08.142 kid1| 87,3| clientStream.cc(202) clientStreamDetach: clientStreamDetach: Detaching node 0x80d39c38
2017/06/05 14:18:08.142 kid1| 87,3| clientStream.cc(287) clientStreamFree: Freeing clientStreamNode 0x80d39c38
2017/06/05 14:18:08.142 kid1| 87,3| clientStream.cc(223) clientStreamDetach: clientStreamDetach: Calling 1 with cbdata 0x80d39bc8
2017/06/05 14:18:08.142 kid1| 87,3| clientStream.cc(202) clientStreamDetach: clientStreamDetach: Detaching node 0x80d39be8
2017/06/05 14:18:08.142 kid1| 87,3| clientStream.cc(287) clientStreamFree: Freeing clientStreamNode 0x80d39be8
2017/06/05 14:18:08.142 kid1| 90,3| store_client.cc(664) storeUnregister: storeUnregister: called for '908891323D546174B25B4E7FDFB415C7'
2017/06/05 14:18:08.142 kid1| 20,3| store_swapout.cc(356) mayStartSwapOut:  already rejected
2017/06/05 14:18:08.142 kid1| 20,2| store.cc(954) checkCachable: StoreEntry::checkCachable: NO: not cachable
2017/06/05 14:18:08.142 kid1| 20,3| store.cc(484) lock: storeUnregister locked key 908891323D546174B25B4E7FDFB415C7 e:=sXINV/0x80e05f48*3
2017/06/05 14:18:08.142 kid1| 90,3| store_client.cc(758) storePendingNClients: storePendingNClients: returning 0
2017/06/05 14:18:08.142 kid1| 20,3| store.cc(522) unlock: storeUnregister unlocking key 908891323D546174B25B4E7FDFB415C7 e:=sXINV/0x80e05f48*3
2017/06/05 14:18:08.142 kid1| 20,3| store.cc(522) unlock: clientReplyContext::removeStoreReference unlocking key 908891323D546174B25B4E7FDFB415C7 e:=sXINV/0x80e05f48*2
2017/06/05 14:18:08.142 kid1| 33,3| client_side_request.cc(246) ~ClientHttpRequest: httpRequestFree: error:invalid-request
2017/06/05 14:18:08.142 kid1| 28,3| Checklist.cc(70) preCheck: 0xbf839f2c checking fast ACLs
2017/06/05 14:18:08.143 kid1| 28,5| Acl.cc(138) matches: checking access_log daemon:/var/log/squid/access.test.log
2017/06/05 14:18:08.143 kid1| 28,5| Acl.cc(138) matches: checking (access_log daemon:/var/log/squid/access.test.log line)
2017/06/05 14:18:08.143 kid1| 28,3| Acl.cc(158) matches: checked: (access_log daemon:/var/log/squid/access.test.log line) = 1
2017/06/05 14:18:08.143 kid1| 28,3| Acl.cc(158) matches: checked: access_log daemon:/var/log/squid/access.test.log = 1
2017/06/05 14:18:08.143 kid1| 28,3| Checklist.cc(63) markFinished: 0xbf839f2c answer ALLOWED for match
2017/06/05 14:18:08.143 kid1| 50,5| ModDaemon.cc(65) logfileNewBuffer: logfileNewBuffer: daemon:/var/log/squid/access.test.log: new buffer
2017/06/05 14:18:08.143 kid1| 50,3| ModDaemon.cc(172) logfile_mod_daemon_append: logfile_mod_daemon_append: daemon:/var/log/squid/access.test.log: appending 1 bytes
2017/06/05 14:18:08.143 kid1| 50,3| ModDaemon.cc(176) logfile_mod_daemon_append: logfile_mod_daemon_append: current buffer has 0 of 32768 bytes before append
2017/06/05 14:18:08.143 kid1| 50,3| ModDaemon.cc(172) logfile_mod_daemon_append: logfile_mod_daemon_append: daemon:/var/log/squid/access.test.log: appending 106 bytes
2017/06/05 14:18:08.143 kid1| 50,3| ModDaemon.cc(176) logfile_mod_daemon_append: logfile_mod_daemon_append: current buffer has 1 of 32768 bytes before append
2017/06/05 14:18:08.143 kid1| 50,3| ModDaemon.cc(172) logfile_mod_daemon_append: logfile_mod_daemon_append: daemon:/var/log/squid/access.test.log: appending 2 bytes
2017/06/05 14:18:08.143 kid1| 50,3| ModDaemon.cc(176) logfile_mod_daemon_append: logfile_mod_daemon_append: current buffer has 107 of 32768 bytes before append
2017/06/05 14:18:08.143 kid1| 5,5| ModEpoll.cc(116) SetSelect: FD 21, type=2, handler=1, client_data=0x808fb030, timeout=0
2017/06/05 14:18:08.143 kid1| 28,4| FilledChecklist.cc(66) ~ACLFilledChecklist: ACLFilledChecklist destroyed 0xbf839f2c
2017/06/05 14:18:08.143 kid1| 28,4| Checklist.cc(197) ~ACLChecklist: ACLChecklist::~ACLChecklist: destroyed 0xbf839f2c
2017/06/05 14:18:08.143 kid1| 20,3| store.cc(522) unlock: ClientHttpRequest::loggingEntry unlocking key 908891323D546174B25B4E7FDFB415C7 e:=sXINV/0x80e05f48*1
2017/06/05 14:18:08.143 kid1| 90,3| store_client.cc(758) storePendingNClients: storePendingNClients: returning 0
2017/06/05 14:18:08.143 kid1| 20,3| store.cc(1244) release: releasing e:=sXINV/0x80e05f48*0 908891323D546174B25B4E7FDFB415C7
2017/06/05 14:18:08.143 kid1| 20,3| store.cc(403) destroyMemObject: destroyMemObject 0x80e71638
2017/06/05 14:18:08.143 kid1| 20,3| MemObject.cc(110) ~MemObject: del MemObject 0x80e71638
2017/06/05 14:18:08.144 kid1| 20,3| store.cc(421) destroyStoreEntry: destroyStoreEntry: destroying 0x80e05f4c
2017/06/05 14:18:08.144 kid1| 20,3| store.cc(403) destroyMemObject: destroyMemObject 0
2017/06/05 14:18:08.144 kid1| 20,5| store.cc(376) ~StoreEntry: StoreEntry destructed, this=0x80e05f48
2017/06/05 14:18:08.144 kid1| 93,5| AsyncCall.cc(26) AsyncCall: The AsyncCall Initiate::noteInitiatorAborted constructed, this=0x80e717d0 [call2573]
2017/06/05 14:18:08.144 kid1| 93,5| AsyncCall.cc(93) ScheduleCall: Initiator.cc(40) will call Initiate::noteInitiatorAborted() [call2573]
2017/06/05 14:18:08.144 kid1| 93,5| AsyncJob.cc(40) ~AsyncJob: AsyncJob destructed, this=0x80d227d0 type=ClientHttpRequest [job60]
2017/06/05 14:18:08.144 kid1| 33,3| client_side.cc(5106) unpinConnection: local=10.215.145.187:39368 remote=46.51.187.18:443 FD 30 flags=25
2017/06/05 14:18:08.144 kid1| 5,5| comm.cc(1038) comm_remove_close_handler: comm_remove_close_handler: FD 30, AsyncCall=0x80e84ea0*2
2017/06/05 14:18:08.144 kid1| 33,5| AsyncCall.cc(56) cancel: will not call ConnStateData::clientPinnedConnectionClosed [call2562] because comm_remove_close_handler
2017/06/05 14:18:08.144 kid1| 33,3| AsyncCall.cc(56) cancel: will not call ConnStateData::clientPinnedConnectionRead [call2563] because comm_read_cancel
2017/06/05 14:18:08.144 kid1| 33,3| AsyncCall.cc(56) cancel: will not call ConnStateData::clientPinnedConnectionRead [call2563] also because comm_read_cancel
2017/06/05 14:18:08.144 kid1| 5,5| ModEpoll.cc(116) SetSelect: FD 30, type=1, handler=0, client_data=0, timeout=0
2017/06/05 14:18:08.144 kid1| 5,3| comm.cc(868) _comm_close: comm_close: start closing FD 30
2017/06/05 14:18:08.144 kid1| 5,4| AsyncCall.cc(26) AsyncCall: The AsyncCall commStartSslClose constructed, this=0x80e73a38 [call2574]
2017/06/05 14:18:08.144 kid1| 5,4| AsyncCall.cc(93) ScheduleCall: comm.cc(902) will call commStartSslClose(FD 30) [call2574]
2017/06/05 14:18:08.144 kid1| 5,3| comm.cc(540) commUnsetFdTimeout: Remove timeout for FD 30
2017/06/05 14:18:08.144 kid1| 5,5| comm.cc(721) commCallCloseHandlers: commCallCloseHandlers: FD 30
2017/06/05 14:18:08.144 kid1| 5,4| AsyncCall.cc(26) AsyncCall: The AsyncCall comm_close_complete constructed, this=0x80e84ea0 [call2575]
2017/06/05 14:18:08.144 kid1| 5,4| AsyncCall.cc(93) ScheduleCall: comm.cc(941) will call comm_close_complete(FD 30) [call2575]
2017/06/05 14:18:08.144 kid1| 33,3| client_side.cc(846) ~ConnStateData: local=46.51.187.18:443 remote=10.215.145.187 flags=17
2017/06/05 14:18:08.144 kid1| 33,4| ServerBump.cc(44) ~ServerBump: destroying
2017/06/05 14:18:08.144 kid1| 33,4| ServerBump.cc(46) ~ServerBump: e:=sp2XDIV/0x80e0a448*1
2017/06/05 14:18:08.144 kid1| 90,3| store_client.cc(664) storeUnregister: storeUnregister: called for '52E847A2F8AF07D9783AAE4A91F2E9E6'
2017/06/05 14:18:08.144 kid1| 20,3| store_swapout.cc(356) mayStartSwapOut:  already rejected
2017/06/05 14:18:08.144 kid1| 20,2| store.cc(954) checkCachable: StoreEntry::checkCachable: NO: not cachable
2017/06/05 14:18:08.144 kid1| 20,3| store.cc(484) lock: storeUnregister locked key 52E847A2F8AF07D9783AAE4A91F2E9E6 e:=sp2XDIV/0x80e0a448*2
2017/06/05 14:18:08.144 kid1| 90,3| store_client.cc(758) storePendingNClients: storePendingNClients: returning 0
2017/06/05 14:18:08.144 kid1| 20,3| store.cc(522) unlock: storeUnregister unlocking key 52E847A2F8AF07D9783AAE4A91F2E9E6 e:=sp2XDIV/0x80e0a448*2
2017/06/05 14:18:08.145 kid1| 20,3| store.cc(522) unlock: Ssl::ServerBump unlocking key 52E847A2F8AF07D9783AAE4A91F2E9E6 e:=sp2XDIV/0x80e0a448*1
2017/06/05 14:18:08.145 kid1| 90,3| store_client.cc(758) storePendingNClients: storePendingNClients: returning 0
2017/06/05 14:18:08.145 kid1| 20,3| store.cc(1244) release: releasing e:=sp2XDIV/0x80e0a448*0 52E847A2F8AF07D9783AAE4A91F2E9E6
2017/06/05 14:18:08.145 kid1| 20,3| store.cc(403) destroyMemObject: destroyMemObject 0x80e09f08
2017/06/05 14:18:08.145 kid1| 20,3| MemObject.cc(110) ~MemObject: del MemObject 0x80e09f08
2017/06/05 14:18:08.145 kid1| 20,3| store.cc(421) destroyStoreEntry: destroyStoreEntry: destroying 0x80e0a44c
2017/06/05 14:18:08.145 kid1| 20,3| store.cc(403) destroyMemObject: destroyMemObject 0
2017/06/05 14:18:08.145 kid1| 20,5| store.cc(376) ~StoreEntry: StoreEntry destructed, this=0x80e0a448
2017/06/05 14:18:08.145 kid1| 93,5| AsyncJob.cc(40) ~AsyncJob: AsyncJob destructed, this=0x80e07f24 type=Http::Server [job56]
2017/06/05 14:18:08.145 kid1| 33,5| AsyncCallQueue.cc(57) fireNext: leaving ConnStateData::connStateClosed(FD -1, data=0x80e07e00)
2017/06/05 14:18:08.145 kid1| 5,4| AsyncCallQueue.cc(55) fireNext: entering comm_close_complete(FD 29)
2017/06/05 14:18:08.145 kid1| 5,4| AsyncCall.cc(38) make: make call comm_close_complete [call2572]
2017/06/05 14:18:08.145 kid1| 51,3| fd.cc(93) fd_close: fd_close FD 29 Reading next request
2017/06/05 14:18:08.145 kid1| 5,5| ModEpoll.cc(116) SetSelect: FD 29, type=1, handler=0, client_data=0, timeout=0
2017/06/05 14:18:08.145 kid1| 5,5| ModEpoll.cc(116) SetSelect: FD 29, type=2, handler=0, client_data=0, timeout=0
2017/06/05 14:18:08.145 kid1| 5,5| AcceptLimiter.cc(55) kick: size=0
2017/06/05 14:18:08.145 kid1| 5,4| AsyncCallQueue.cc(57) fireNext: leaving comm_close_complete(FD 29)
2017/06/05 14:18:08.145 kid1| 93,5| AsyncCallQueue.cc(55) fireNext: entering Initiate::noteInitiatorAborted()
2017/06/05 14:18:08.145 kid1| 93,5| AsyncCall.cc(38) make: make call Initiate::noteInitiatorAborted [call2573]
2017/06/05 14:18:08.145 kid1| 93,5| AsyncCall.cc(56) cancel: will not call Initiate::noteInitiatorAborted [call2573] because job gone
2017/06/05 14:18:08.145 kid1| 93,5| AsyncCall.cc(48) make: will not call Initiate::noteInitiatorAborted [call2573] because of job gone
2017/06/05 14:18:08.145 kid1| 93,5| AsyncCallQueue.cc(57) fireNext: leaving Initiate::noteInitiatorAborted()
2017/06/05 14:18:08.145 kid1| 5,4| AsyncCallQueue.cc(55) fireNext: entering commStartSslClose(FD 30)
2017/06/05 14:18:08.145 kid1| 5,4| AsyncCall.cc(38) make: make call commStartSslClose [call2574]
2017/06/05 14:18:08.146 kid1| 83,5| bio.cc(95) write: FD 30 wrote 53 <= 53
2017/06/05 14:18:08.146 kid1| 5,4| AsyncCallQueue.cc(57) fireNext: leaving commStartSslClose(FD 30)
2017/06/05 14:18:08.146 kid1| 5,4| AsyncCallQueue.cc(55) fireNext: entering comm_close_complete(FD 30)
2017/06/05 14:18:08.146 kid1| 5,4| AsyncCall.cc(38) make: make call comm_close_complete [call2575]
2017/06/05 14:18:08.146 kid1| 51,3| fd.cc(93) fd_close: fd_close FD 30 [unknown] pinned connection for 10.215.145.187 (29)
2017/06/05 14:18:08.146 kid1| 5,5| ModEpoll.cc(116) SetSelect: FD 30, type=1, handler=0, client_data=0, timeout=0
2017/06/05 14:18:08.146 kid1| 5,5| ModEpoll.cc(116) SetSelect: FD 30, type=2, handler=0, client_data=0, timeout=0
2017/06/05 14:18:08.146 kid1| 5,5| AcceptLimiter.cc(55) kick: size=0
2017/06/05 14:18:08.146 kid1| 5,4| AsyncCallQueue.cc(57) fireNext: leaving comm_close_complete(FD 30)
2017/06/05 14:18:08.146 kid1| 50,3| ModDaemon.cc(108) logfileHandleWrite: daemon:/var/log/squid/access.test.log: write returned 109
2017/06/05 14:18:08.146 kid1| 5,5| ModEpoll.cc(116) SetSelect: FD 21, type=2, handler=0, client_data=0, timeout=0
2017/06/05 14:18:08.786 kid1| 41,5| AsyncCall.cc(26) AsyncCall: The AsyncCall MaintainSwapSpace constructed, this=0x80d3d960 [call2576]
2017/06/05 14:18:08.786 kid1| 41,5| AsyncCall.cc(93) ScheduleCall: event.cc(237) will call MaintainSwapSpace() [call2576]
2017/06/05 14:18:08.786 kid1| 41,5| AsyncCallQueue.cc(55) fireNext: entering MaintainSwapSpace()
2017/06/05 14:18:08.786 kid1| 41,5| AsyncCall.cc(38) make: make call MaintainSwapSpace [call2576]
2017/06/05 14:18:08.786 kid1| 47,5| ufs/UFSSwapDir.cc(447) maintain: space still available in /var/cache/squid.test
2017/06/05 14:18:08.786 kid1| 41,5| AsyncCallQueue.cc(57) fireNext: leaving MaintainSwapSpace()
2017/06/05 14:18:08.786 kid1| 41,5| AsyncCall.cc(26) AsyncCall: The AsyncCall logfileFlush constructed, this=0x80d3d960 [call2577]
2017/06/05 14:18:08.786 kid1| 41,5| AsyncCall.cc(93) ScheduleCall: event.cc(237) will call logfileFlush(0x808fb030*?) [call2577]
2017/06/05 14:18:08.787 kid1| 41,5| AsyncCallQueue.cc(55) fireNext: entering logfileFlush(0x808fb030*?)
2017/06/05 14:18:08.787 kid1| 41,5| AsyncCall.cc(38) make: make call logfileFlush [call2577]
2017/06/05 14:18:08.787 kid1| 41,5| AsyncCallQueue.cc(57) fireNext: leaving logfileFlush(0x808fb030*?)

I tried to make something out of the log before the generation of ERR_INVALID_REQ, but I'm a bit lost here.

Thanks,

Vieri


From janis.heller at outlook.de  Tue Jun  6 09:36:58 2017
From: janis.heller at outlook.de (Janis Heller)
Date: Tue, 6 Jun 2017 09:36:58 +0000
Subject: [squid-users] retrieve amount of traffic by username
Message-ID: <AM5P194MB0210C7865BA463B7BB8A3079F7CB0@AM5P194MB0210.EURP194.PROD.OUTLOOK.COM>

Dear SQUID users;

what?s the best way to retrieve the amount of a user connected to SQUID server by using his username as an identifier?
Would be necessary for me to do so for including some traffic based limitations for each user (would send traffic amount generated by a user back to some backend for further processing), so all I?m looking for is some way to retrieve the amount generated by a user using a custom script.

All the best;

From fredbmail at free.fr  Tue Jun  6 11:01:46 2017
From: fredbmail at free.fr (FredB)
Date: Tue, 6 Jun 2017 13:01:46 +0200 (CEST)
Subject: [squid-users] retrieve amount of traffic by username
In-Reply-To: <AM5P194MB0210C7865BA463B7BB8A3079F7CB0@AM5P194MB0210.EURP194.PROD.OUTLOOK.COM>
Message-ID: <1560790630.138618806.1496746906658.JavaMail.root@zimbra4-e1>

delay_pool mixed with an acl like this acl ldap_auth proxy_auth REQUIRED

delay_access 1 allow ldap_auth
delay_access 1 deny all

A delay_class 4 should be good 

Fred



From squid3 at treenet.co.nz  Tue Jun  6 11:09:11 2017
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Tue, 6 Jun 2017 23:09:11 +1200
Subject: [squid-users] retrieve amount of traffic by username
In-Reply-To: <AM5P194MB0210C7865BA463B7BB8A3079F7CB0@AM5P194MB0210.EURP194.PROD.OUTLOOK.COM>
References: <AM5P194MB0210C7865BA463B7BB8A3079F7CB0@AM5P194MB0210.EURP194.PROD.OUTLOOK.COM>
Message-ID: <fdd0625d-792b-fd12-00e7-9c165caec55a@treenet.co.nz>

On 06/06/17 21:36, Janis Heller wrote:
> Dear SQUID users;
>
> what?s the best way to retrieve the amount of a user connected to SQUID server by using his username as an identifier?
> Would be necessary for me to do so for including some traffic based limitations for each user (would send traffic amount generated by a user back to some backend for further processing), so all I?m looking for is some way to retrieve the amount generated by a user using a custom script.

What you are looking for seems to be a logging daemon.
<http://wiki.squid-cache.org/action/show/Features/LogModules#Module:_Daemon>


Amos



From squid3 at treenet.co.nz  Tue Jun  6 12:02:22 2017
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Wed, 7 Jun 2017 00:02:22 +1200
Subject: [squid-users] squid ssl bump and Adobe Connect
In-Reply-To: <782975790.3626570.1496729292503@mail.yahoo.com>
References: <2085804462.2620283.1496666965861.ref@mail.yahoo.com>
 <2085804462.2620283.1496666965861@mail.yahoo.com>
 <03181dbd-0880-e4a5-5e50-aa4f4b3a94e4@measurement-factory.com>
 <782975790.3626570.1496729292503@mail.yahoo.com>
Message-ID: <8d9d84ad-0560-3d44-700b-a82707ffd355@treenet.co.nz>

On 06/06/17 18:08, Vieri wrote:
> ________________________________
> From: Alex Rousskov <rousskov at measurement-factory.com>
>>> 1496665088.143      6 10.215.145.187 TAG_NONE/400 4428 NONE error:invalid-request - HIER_NONE/-
>>> text/html>
>> I recommend finding the place in the debugging cache.log where Squid
>> generates the above error response and then going backwards to find the
>> cause.
> OK Alex, got it.
> In the meantime, I searched for the events around the time this happened.
> BTW as a side question I'd like to know if I can change the timestamp in cache.log so it can print the unixtime as in access.log.
>
> In any case, here's the relevant part:
>
> [Mon Jun  5 14:18:08 2017].143      6 10.215.145.187 TAG_NONE/400 4428 NONE error:invalid-request - HIER_NONE/- text/html
>
> cache.log within 14:18:08:

<snip>
> 2017/06/05 14:18:08.129 kid1| 83,2| client_side.cc(3796) clientNegotiateSSL: clientNegotiateSSL: New session 0x80e80e30 on FD 29 (10.215.145.187:54815)
> 2017/06/05 14:18:08.129 kid1| 83,3| client_side.cc(3800) clientNegotiateSSL: clientNegotiateSSL: FD 29 negotiated cipher AES128-SHA
> 2017/06/05 14:18:08.129 kid1| 83,5| client_side.cc(3816) clientNegotiateSSL: clientNegotiateSSL: FD 29 has no certificate.
> 2017/06/05 14:18:08.129 kid1| 33,4| client_side.cc(231) readSomeData: local=46.51.187.18:443 remote=10.215.145.187 FD 29 flags=17: reading request...
> 2017/06/05 14:18:08.129 kid1| 33,5| AsyncCall.cc(26) AsyncCall: The AsyncCall ConnStateData::clientReadRequest constructed, this=0x80e80578 [call2567]
> 2017/06/05 14:18:08.129 kid1| 5,5| Read.cc(58) comm_read_base: comm_read, queueing read for local=46.51.187.18:443 remote=10.215.145.187 FD 29 flags=17; asynCall 0x80e80578*1
> 2017/06/05 14:18:08.129 kid1| 5,5| ModEpoll.cc(116) SetSelect: FD 29, type=1, handler=1, client_data=0x808b48b8, timeout=0
> 2017/06/05 14:18:08.135 kid1| 5,3| IoCallback.cc(116) finish: called for local=46.51.187.18:443 remote=10.215.145.187 FD 29 flags=17 (0, 0)
> 2017/06/05 14:18:08.135 kid1| 33,5| AsyncCall.cc(93) ScheduleCall: IoCallback.cc(135) will call ConnStateData::clientReadRequest(local=46.51.187.18:443 remote=10.215.145.187 FD 29 flags=17, data=0x80e07e00) [call2567]
> 2017/06/05 14:18:08.135 kid1| 33,5| AsyncCallQueue.cc(55) fireNext: entering ConnStateData::clientReadRequest(local=46.51.187.18:443 remote=10.215.145.187 FD 29 flags=17, data=0x80e07e00)
> 2017/06/05 14:18:08.135 kid1| 33,5| AsyncCall.cc(38) make: make call ConnStateData::clientReadRequest [call2567]
> 2017/06/05 14:18:08.135 kid1| 33,5| AsyncJob.cc(123) callStart: Http::Server status in: [ job56]
> 2017/06/05 14:18:08.136 kid1| 33,5| client_side.cc(3251) clientReadRequest: local=46.51.187.18:443 remote=10.215.145.187 FD 29 flags=17
> 2017/06/05 14:18:08.136 kid1| 83,5| bio.cc(118) read: FD 29 read 5 <= 5
> 2017/06/05 14:18:08.136 kid1| 83,5| bio.cc(118) read: FD 29 read 1568 <= 1568
> 2017/06/05 14:18:08.136 kid1| 83,2| support.cc(1364) ssl_read_method: SSL FD 29 is pending

FD 29 went through SSL-Bump processing and was bump'ed. An HTTPS request 
message is now expected to arrive.

> 2017/06/05 14:18:08.136 kid1| 5,3| Read.cc(91) ReadNow: local=46.51.187.18:443 remote=10.215.145.187 FD 29 flags=17, size 66, retval 66, errno 0
> 2017/06/05 14:18:08.136 kid1| 33,5| client_side.cc(3200) clientParseRequests: local=46.51.187.18:443 remote=10.215.145.187 FD 29 flags=17: attempting to parse

... 66 bytes arrive ...

> 2017/06/05 14:18:08.136 kid1| 74,5| HttpParser.cc(37) reset: Request buffer is
> 2017/06/05 14:18:08.136 kid1| 74,5| HttpParser.cc(47) parseRequestFirstLine: parsing possible request:
> 2017/06/05 14:18:08.136 kid1| 74,5| HttpParser.cc(257) HttpParserParseReqLine: Parser: retval -1: from 0->4: method 0->-1; url -1->-1; version -1->-1 (0/0)

The first 4 bytes were ones which cannot exist at the start if any HTTP 
message - so the protocol being bumped on port 443 was not HTTPS.

Squid then proceeds to generate and send the invalid-request error 
response to this non-HTTPS over port 443.


Amos


From janis.heller at outlook.de  Tue Jun  6 12:10:39 2017
From: janis.heller at outlook.de (Janis Heller)
Date: Tue, 6 Jun 2017 12:10:39 +0000
Subject: [squid-users] retrieve amount of traffic by username
In-Reply-To: <293872742.138644268.1496747362175.JavaMail.root@zimbra4-e1>
References: <AM5P194MB0210BE3D3050745AFDF93C08F7CB0@AM5P194MB0210.EURP194.PROD.OUTLOOK.COM>,
 <293872742.138644268.1496747362175.JavaMail.root@zimbra4-e1>
Message-ID: <AM5P194MB0210C9CA75C7E4CBD7AA6D2AF7CB0@AM5P194MB0210.EURP194.PROD.OUTLOOK.COM>

Dear Fred,


thanks for your reply, let me explain a bit more. I'm using the RADIUS module for authentication, RADIUS is using a custom backend script to validate the provided login data (received from SQUID server). How to keep track on traffic usage of each user (for example OpenVPN config allows calling a script with used traffic + username each time a user is disconnecting). Is there some easy way, since I would need to store the generated traffic of a user per month (for being able to deny access on my custom backend script). So all I need to get is the traffic generated by a user when he's disconnecting. Polling the generated traffic in an interval won't be the best way I think.


All the best;

Janis

________________________________
Von: FredB <fredbmail at free.fr>
Gesendet: Dienstag, 6. Juni 2017 13:09:22
An: Janis Heller
Betreff: Re: AW: [squid-users] retrieve amount of traffic by username

Ldap as an example with basic identification, but it can works with NTLM or DIGEST
Ldap is the identification mode here, this is not a ldap setup to limit the bandwidth.


Please let the group in Cc:

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20170606/7453d554/attachment.htm>

From fredbmail at free.fr  Tue Jun  6 12:30:58 2017
From: fredbmail at free.fr (FredB)
Date: Tue, 6 Jun 2017 14:30:58 +0200 (CEST)
Subject: [squid-users] retrieve amount of traffic by username
In-Reply-To: <AM5P194MB0210C9CA75C7E4CBD7AA6D2AF7CB0@AM5P194MB0210.EURP194.PROD.OUTLOOK.COM>
Message-ID: <94924396.138882539.1496752258592.JavaMail.root@zimbra4-e1>


My answer was only for this point 

> Would be necessary for me to do so for including some traffic based limitations for each user 

I don't known radius with Squid but I guess you have an acl like this
acl radius-auth proxy_auth REQUIRED ?? (or something close)

In this case I guess you can easily mixed this acl with delay_pool http://wiki.squid-cache.org/Features/DelayPools

Here, I have bandwidth limitation for each account also based on time (not limitation at night)




From alex.delgado at crg.eu  Tue Jun  6 13:19:05 2017
From: alex.delgado at crg.eu (Alejandro Delgado Moreno)
Date: Tue, 6 Jun 2017 13:19:05 +0000
Subject: [squid-users] Cache peer help
Message-ID: <c84b8794fa3e4b118d128a687b7047d6@UPF-BORN-MBX.crg.es>

Hi,
I need to set up a proxy server to filter the Gateway used by sites.
I've created a file called sites.txt, that contains the list of sites that our squid proxy should forward the request to another proxy outside our lan.
If the address typed is not in the list, it should be request by our proxy.
I've tried different configurations with peer_cache directive, but haven't been able to route it successfully because all traffic is going by the peer proxy or by our own Gateway, without having into account the file contents.

This is a part of the configuration:

acl journals dstdomain "/etc/squid/xx_LIST.txt"
cache_peer xxx.xxx.xxx.xxx parent 9090 0 no-query no-digest default
cache_peer_access proxy-inst.upf.edu allow journals
Does anybody has a similar configuration and share it with me?

Regards,
Alex.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20170606/303f932f/attachment.htm>

From lyningg at gmail.com  Tue Jun  6 13:33:22 2017
From: lyningg at gmail.com (LIU Yaning)
Date: Tue, 6 Jun 2017 15:33:22 +0200
Subject: [squid-users] Squid issue of caching the m3u8 file
Message-ID: <CAEip7fwgrViUQA2vCM4Q5BXE5HmhKS373o=FV=zToE9HLw9uuw@mail.gmail.com>

Dear Amos,

Thanks a lot for your explanation and suggestion. I added the "store-stale"
to the refresh_pattern rule as:
refresh_pattern -i \.(ts|m3u8)$ 120 90% 1000 override-expire override-lastmod
ignore-no-cache ignore-no-store store-stale

However, I have checked the access.log, I am still getting TCP_Miss.
1496754869.963 13 192.168.0.100 TCP_MISS/200 16636 GET
http://qthttp.apple.com.edgesuite.net/1010qwoeiuryfg/0150_vod.m3u8
<http://www.google.com/url?q=http%3A%2F%2Fqthttp.apple.com.edgesuite.net%2F1010qwoeiuryfg%2F0150_vod.m3u8&sa=D&sntz=1&usg=AFQjCNHpiHy55EMeBIaMGhgEKRHanTrXxg>
-
HIER_DIRECT/95.101.182.201
<http://www.google.com/url?q=http%3A%2F%2F95.101.182.201&sa=D&sntz=1&usg=AFQjCNE9ZWXH7sOJgbqIA--MJwxobSp76Q>
 application/x-mpegURL
1496754870.605 4 192.168.0.100 TCP_MISS/200 16636 GET
http://qthttp.apple.com.edgesuite.net/1010qwoeiuryfg/0150_vod.m3u8
<http://www.google.com/url?q=http%3A%2F%2Fqthttp.apple.com.edgesuite.net%2F1010qwoeiuryfg%2F0150_vod.m3u8&sa=D&sntz=1&usg=AFQjCNHpiHy55EMeBIaMGhgEKRHanTrXxg>
-
HIER_DIRECT/95.101.182.201
<http://www.google.com/url?q=http%3A%2F%2F95.101.182.201&sa=D&sntz=1&usg=AFQjCNE9ZWXH7sOJgbqIA--MJwxobSp76Q>
 application/x-mpegURL
1496754871.194 15 192.168.0.100 TCP_MISS/200 16636 GET
http://qthttp.apple.com.edgesuite.net/1010qwoeiuryfg/0150_vod.m3u8
<http://www.google.com/url?q=http%3A%2F%2Fqthttp.apple.com.edgesuite.net%2F1010qwoeiuryfg%2F0150_vod.m3u8&sa=D&sntz=1&usg=AFQjCNHpiHy55EMeBIaMGhgEKRHanTrXxg>
-
HIER_DIRECT/95.101.182.201
<http://www.google.com/url?q=http%3A%2F%2F95.101.182.201&sa=D&sntz=1&usg=AFQjCNE9ZWXH7sOJgbqIA--MJwxobSp76Q>
 application/x-mpegURL
1496754871.715 4 192.168.0.100 TCP_MISS/200 16636 GET
http://qthttp.apple.com.edgesuite.net/1010qwoeiuryfg/0150_vod.m3u8
<http://www.google.com/url?q=http%3A%2F%2Fqthttp.apple.com.edgesuite.net%2F1010qwoeiuryfg%2F0150_vod.m3u8&sa=D&sntz=1&usg=AFQjCNHpiHy55EMeBIaMGhgEKRHanTrXxg>
-
HIER_DIRECT/95.101.182.201
<http://www.google.com/url?q=http%3A%2F%2F95.101.182.201&sa=D&sntz=1&usg=AFQjCNE9ZWXH7sOJgbqIA--MJwxobSp76Q>
 application/x-mpegURL

If I understand well, TCP_MISS/200 shows the content is not cached by
Squid. Could you please help me to see if anything I did wrong to make
.m3u8 not cached by Squid?


Date: Tue, 6 Jun 2017 14:08:11 +1200
From: Amos Jeffries <squid3 at treenet.co.nz>
To: squid-users at lists.squid-cache.org
Subject: Re: [squid-users] Squid issue of caching the m3u8 file
Message-ID: <5af5e6e9-4880-f58e-106b-9f11dd0a4a7b at treenet.co.nz>
Content-Type: text/plain; charset=utf-8; format=flowed

On 05/06/17 23:54, LIU Yaning wrote:
> Dear All,
>
> I would like to cache the .m3u8 file to be able to provide offline
> caching service by Squid. The played HLS video streaming is the link
> as below:
> http://qthttp.apple.com.edgesuite.net/1010qwoeiuryfg/0150_vod.m3u8
> <http://qthttp.apple.com.edgesuite.net/1010qwoeiuryfg/0150_vod.m3u8>
> However, the .m3u8 file is not be cached probably because it is
> mentioned as a no-cache, no-store, max-age=0 in the "Cache-Control" in
> the HTTP header.
Nope. Only the CC:no-store is preventing caching. The other headers
simply put boundaries on what is to be done with the content in the
cache. In particular the "no-cache", max-age=0 and Expires values mean
it has to be revalidated (REFRESH in your access.log) before any future
uses - probably because of that Set-Cookie needing to be changed for
different end-users.
>
> HTTP/1.1 200 OK
>
> Server: Apache
>
> ETag: "1d7168b4f49e75f76f3182f24bf075f6:1299516751"
>
> Last-Modified: Mon, 07 Mar 2011 16:52:31 GMT
>
> Expires: Fri, 02 Jun 2017 14:26:52 GMT
>
> Cache-Control: max-age=0, no-cache, no-store
>
> Pragma: no-cache
>
> Date: Fri, 02 Jun 2017 14:26:52 GMT
>
> Content-Length: 16046
>
> Set-Cookie: AKID=77F9F1316ECCE780566608C5E514DE0A;expires=Fri, 26 Aug
> 2016 00:01:00 GMT; path=/; domain=qthttp.apple.com.edgesuite.net
> <http://qthttp.apple.com.edgesuite.net/>
>
> Content-Type: application/x-mpegURL
>
> Access-Control-Allow-Origin: *
>
> I added a new rule for .m3u8 file in squid.conf, however, it is still
> not working.
>
> refresh_pattern -i \.(ts|m3u8)$ 120 90% 1000 override-expire
> override-lastmod ignore-no-cache ignore-no-store
>
> Does anyone know how to allow Squid caching the .m3u8 file? Thanks a
> lot in advance.
What makes you think it is not caching? The ignore-no-store alone should
be sufficient to allow current Squid versions to cache that object. You
could perhapse add "store-stale" option on that config line. Which
should make Squid cache object containing an Expires header with current
or past values. refresh_pattern settings do not affect that cacheable vs
non-cacheable decision. The no-cache header tells Squid the object needs
revalidating before every use. However, be aware the tool at redbot.org
tells me that this URL is badly broken in how it is using the ETag and
Vary headers - in a way which can break the revalidation when these
things are cached. Some of your clients may see very broken behaviour
accessing this object unless you follow the no-store requirement or the
server stops its broken ETag behaviour. PS. if you are using a Squid
version much older than 3.5.24 I recommend an upgrade. With an urgency
increasing the older your Squid is. Amos



-- 
Best Regards,
--
Yaning.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20170606/5cf5752a/attachment.htm>

From rousskov at measurement-factory.com  Tue Jun  6 13:40:18 2017
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Tue, 6 Jun 2017 07:40:18 -0600
Subject: [squid-users] retrieve amount of traffic by username
In-Reply-To: <AM5P194MB0210C9CA75C7E4CBD7AA6D2AF7CB0@AM5P194MB0210.EURP194.PROD.OUTLOOK.COM>
References: <AM5P194MB0210BE3D3050745AFDF93C08F7CB0@AM5P194MB0210.EURP194.PROD.OUTLOOK.COM>
 <293872742.138644268.1496747362175.JavaMail.root@zimbra4-e1>
 <AM5P194MB0210C9CA75C7E4CBD7AA6D2AF7CB0@AM5P194MB0210.EURP194.PROD.OUTLOOK.COM>
Message-ID: <bb1d0cc9-4650-ad16-dd7e-be310464fa6a@measurement-factory.com>

On 06/06/2017 06:10 AM, Janis Heller wrote:
> Is there some easy way, since I would need to store the
> generated traffic of a user per month (for being able to deny access on
> my custom backend script).

If parsing Squid access.log entries is easy for you, then there is an
easy way -- either parse the log Squid writes to disk or, as Amos,
suggested, add your own logging daemon that will parse the log entries
on the fly.


> So all I need to get is the traffic generated
> by a user when he's disconnecting.

HTTP does not have a concept of "user disconnect" beyond a single HTTP
transaction or, in some cases, a single HTTP connection. Each access log
entry is logged at the end of an HTTP transaction.

Alex.


From janis.heller at outlook.de  Tue Jun  6 13:45:53 2017
From: janis.heller at outlook.de (Janis Heller)
Date: Tue, 6 Jun 2017 13:45:53 +0000
Subject: [squid-users] retrieve amount of traffic by username
In-Reply-To: <bb1d0cc9-4650-ad16-dd7e-be310464fa6a@measurement-factory.com>
References: <AM5P194MB0210BE3D3050745AFDF93C08F7CB0@AM5P194MB0210.EURP194.PROD.OUTLOOK.COM>
 <293872742.138644268.1496747362175.JavaMail.root@zimbra4-e1>
 <AM5P194MB0210C9CA75C7E4CBD7AA6D2AF7CB0@AM5P194MB0210.EURP194.PROD.OUTLOOK.COM>,
 <bb1d0cc9-4650-ad16-dd7e-be310464fa6a@measurement-factory.com>
Message-ID: <AM5P194MB02102B9E1EFAF1EEDB0BF933F7CB0@AM5P194MB0210.EURP194.PROD.OUTLOOK.COM>

Seems like parsing would be what I need. Is the size (consumed bandwith) and the usernams (timestamp can be generated by my parser) being written to this file?
Could you show me a sample output of this file?
________________________________
Von: Alex Rousskov <rousskov at measurement-factory.com>
Gesendet: Dienstag, 6. Juni 2017 15:40:18
An: Janis Heller; squid-users at lists.squid-cache.org
Betreff: Re: [squid-users] retrieve amount of traffic by username

On 06/06/2017 06:10 AM, Janis Heller wrote:
> Is there some easy way, since I would need to store the
> generated traffic of a user per month (for being able to deny access on
> my custom backend script).

If parsing Squid access.log entries is easy for you, then there is an
easy way -- either parse the log Squid writes to disk or, as Amos,
suggested, add your own logging daemon that will parse the log entries
on the fly.


> So all I need to get is the traffic generated
> by a user when he's disconnecting.

HTTP does not have a concept of "user disconnect" beyond a single HTTP
transaction or, in some cases, a single HTTP connection. Each access log
entry is logged at the end of an HTTP transaction.

Alex.


From squid3 at treenet.co.nz  Tue Jun  6 14:17:24 2017
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Wed, 7 Jun 2017 02:17:24 +1200
Subject: [squid-users] Cache peer help
In-Reply-To: <c84b8794fa3e4b118d128a687b7047d6@UPF-BORN-MBX.crg.es>
References: <c84b8794fa3e4b118d128a687b7047d6@UPF-BORN-MBX.crg.es>
Message-ID: <a7ea26a7-6f65-c68b-8fd2-5b0f1b9b1c29@treenet.co.nz>

On 07/06/17 01:19, Alejandro Delgado Moreno wrote:
>
> Hi,
>
> I need to set up a proxy server to filter the Gateway used by sites.
>
> I?ve created a file called sites.txt, that contains the list of sites 
> that our squid proxy should forward the request to another proxy 
> outside our lan.
>
> If the address typed is not in the list, it should be request by our 
> proxy.
>
> I?ve tried different configurations with peer_cache directive, but 
> haven?t been able to route it successfully because all traffic is 
> going by the peer proxy or by our own Gateway, without having into 
> account the file contents.
>
> This is a part of the configuration:
>
> acl journals dstdomain "/etc/squid/xx_LIST.txt"
>
> cache_peer xxx.xxx.xxx.xxx parent 9090 0 no-query no-digest default
>
> cache_peer_access proxy-inst.upf.edu allow journals
>
> Does anybody has a similar configuration and share it with me?
>

In your lines above, you have a cache_peer named "xxx.xxx.xxx.xxx".

Your cache_peer_access rule is applied to a different cache_peer line 
containing a peer named "proxy-inst.upf.edu".


Amos



From A.Madonna at rechtspraak.nl  Tue Jun  6 14:22:27 2017
From: A.Madonna at rechtspraak.nl (Madonna, A. (spir-it))
Date: Tue, 6 Jun 2017 14:22:27 +0000
Subject: [squid-users] FW:  FW: squid proxy 3.5 redhat 7.3
In-Reply-To: <7bc2697e57bb450e9ba716e54de290fa@rechtspraak.nl>
References: <382fd8bbff524df9971d461b69ae4eeb@rechtspraak.nl>
 <46ea5daf-a067-0090-d22d-fd3d04b803e8@measurement-factory.com>
 <448797274e5f40d781241fcc3e4b08e9@rechtspraak.nl>
 <5ac71a9d-1fd6-7a89-5f28-c5925189364e@measurement-factory.com>
 <b4e2cf45755442cbacbb00746b8ed817@rechtspraak.nl>
 <7bc2697e57bb450e9ba716e54de290fa@rechtspraak.nl>
Message-ID: <bbf7651c9be648cba439245cf8e8bd00@rechtspraak.nl>

Hello,


Know issue 2012 squid proxy 3.2

http://www.squid-cache.org/Versions/v3/3.2/RELEASENOTES.html#ss1.1
?SSL-Bump not re-wrapping decrypted traffic in CONNECT for peers.

+ 5 years ago this already  was a known issue. Apparently even after + 5 years there is still proper solution. Can we expect anything regarding this in the near future?

This person already describes the issue in his blog and  offers a solution although its not perfect. 

https://www.mydlp.com/using-parent-proxy-ssl-bump-enabled-squid-3-2/


also it is still not clear to me if the traffic is encrypted again after leaving the squid proxy when doing ssl bump when using a parent proxy. 
Ssl_bump according to your wiki states that it decrypts and encrypts. However is it true if you are using a parent proxy (cache_peer) that the decrypted traffic does not get re-encrypted anymore, but is send clear text through the cache_peer ? 


-----Oorspronkelijk bericht-----
Van: Alex Rousskov [mailto:rousskov at measurement-factory.com] 
Verzonden: vrijdag 2 juni 2017 17:59
Aan: Madonna, A. (spir-it) <A.Madonna at rechtspraak.nl>; squid-users at lists.squid-cache.org
Onderwerp: Re: [squid-users] FW: squid proxy 3.5 redhat 7.3

On 06/02/2017 01:37 AM, Madonna, A. (spir-it) wrote:

> Clients -> squid proxy -> internet.
> This works with the config as previously mentioned.

OK.


> Clients -> squid proxy (with cache_peer) -> Parent Proxy (not Squid) 
> -> internet Does not work.

Even for regular HTTP traffic and non-bumped HTTPS traffic? If that traffic does not work, then you have misconfigured something or the Parent Proxy is badly broken. There is nothing special in the above setup as far as regular traffic is concerned.


> However I've also setup the following:
> 
> Cleints -> Squid Proxy (with cache_peer) -> Parent Proxy (Squid Proxy) 
> -> internet
> 
> This seems at least to work for http traffic, however, I don't see any HTTPS traffic coming into the Parent Proxy (Squid).

Squid does not know who made the parent proxy. The fact that one (presumably production-quality) proxy "does not work" and another "seems to work" implies that something is seriously misconfigured in one or both cases.


> Now this morning I will do some more tcpdumping to see where that traffic is going, but maybe you can already shed some light on this?

I cannot shed more light on problems described only as "does not work"
and "no traffic".

Alex.


> -----Oorspronkelijk bericht-----
> Van: Alex Rousskov [mailto:rousskov at measurement-factory.com]
> Verzonden: donderdag 1 juni 2017 18:49
> Aan: Madonna, A. (spir-it) <A.Madonna at rechtspraak.nl>; 
> squid-users at lists.squid-cache.org
> Onderwerp: Re: [squid-users] squid proxy 3.5 redhat 7.3
> 
> On 06/01/2017 10:09 AM, Madonna, A. (spir-it) wrote:
>> can we use ssl_bump to intercept https traffic with a parent proxy (cache_peer).
> 
> IIRC, you may be able to use limited SslBump features, but not the full SslBump functionality: Peeking or staring at the origin server through a cache_peer is not supported (yet).
> 
> 
>> ssl_bump peek step1
>> cache_peer ... parent 8080 0 no-query no-netdb-exchange no-digest
> 
> Bugs notwithstanding, the above combination should work because peeking at step1 does not require communication with a cache_peer and splicing at step2 should follow the regular (non-SslBump) tunneling path for CONNECTs, where modern Squids do support cache peers.
> 
> 
> I recommend that you make everything work without a cache_peer and then add a cache_peer.
> 
> Alex.
> 
> 
> ________________________________
> 
> Informatie van de Raad voor de rechtspraak, de rechtbanken, de gerechtshoven en de bijzondere colleges vindt u op www.rechtspraak.nl.
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
> 


From alex.delgado at crg.eu  Tue Jun  6 14:24:56 2017
From: alex.delgado at crg.eu (Alejandro Delgado Moreno)
Date: Tue, 6 Jun 2017 14:24:56 +0000
Subject: [squid-users] Cache peer help
In-Reply-To: <a7ea26a7-6f65-c68b-8fd2-5b0f1b9b1c29@treenet.co.nz>
References: <c84b8794fa3e4b118d128a687b7047d6@UPF-BORN-MBX.crg.es>
 <a7ea26a7-6f65-c68b-8fd2-5b0f1b9b1c29@treenet.co.nz>
Message-ID: <c898a3a31c0d496a91d7e199431d9302@UPF-BORN-MBX.crg.es>

Sorry for this mistake,

It's:

acl journals dstdomain "/etc/squid/xx_LIST.txt"

 cache_peer xxx.xxx.xxx.xxx parent 9090 0 no-query no-digest default

 cache_peer_access xxx.xxx.xxx.xxx allow journals

and it's the same, in both lines.

Regards,

-----Original Message-----
From: squid-users [mailto:squid-users-bounces at lists.squid-cache.org] On Behalf Of Amos Jeffries
Sent: martes, 6 de junio de 2017 16:17
To: squid-users at lists.squid-cache.org
Subject: Re: [squid-users] Cache peer help

On 07/06/17 01:19, Alejandro Delgado Moreno wrote:
>
> Hi,
>
> I need to set up a proxy server to filter the Gateway used by sites.
>
> I?ve created a file called sites.txt, that contains the list of sites 
> that our squid proxy should forward the request to another proxy 
> outside our lan.
>
> If the address typed is not in the list, it should be request by our 
> proxy.
>
> I?ve tried different configurations with peer_cache directive, but 
> haven?t been able to route it successfully because all traffic is 
> going by the peer proxy or by our own Gateway, without having into 
> account the file contents.
>
> This is a part of the configuration:
>
> acl journals dstdomain "/etc/squid/xx_LIST.txt"
>
> cache_peer xxx.xxx.xxx.xxx parent 9090 0 no-query no-digest default
>
> cache_peer_access proxy-inst.upf.edu allow journals
>
> Does anybody has a similar configuration and share it with me?
>

In your lines above, you have a cache_peer named "xxx.xxx.xxx.xxx".

Your cache_peer_access rule is applied to a different cache_peer line containing a peer named "proxy-inst.upf.edu".


Amos

_______________________________________________
squid-users mailing list
squid-users at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-users

From leolistas at solutti.com.br  Tue Jun  6 14:26:12 2017
From: leolistas at solutti.com.br (Leonardo Rodrigues)
Date: Tue, 6 Jun 2017 11:26:12 -0300
Subject: [squid-users] retrieve amount of traffic by username
In-Reply-To: <AM5P194MB02102B9E1EFAF1EEDB0BF933F7CB0@AM5P194MB0210.EURP194.PROD.OUTLOOK.COM>
References: <AM5P194MB0210BE3D3050745AFDF93C08F7CB0@AM5P194MB0210.EURP194.PROD.OUTLOOK.COM>
 <293872742.138644268.1496747362175.JavaMail.root@zimbra4-e1>
 <AM5P194MB0210C9CA75C7E4CBD7AA6D2AF7CB0@AM5P194MB0210.EURP194.PROD.OUTLOOK.COM>
 <bb1d0cc9-4650-ad16-dd7e-be310464fa6a@measurement-factory.com>
 <AM5P194MB02102B9E1EFAF1EEDB0BF933F7CB0@AM5P194MB0210.EURP194.PROD.OUTLOOK.COM>
Message-ID: <47920b6e-fd89-7a59-c985-075738c8247f@solutti.com.br>

Em 06/06/17 10:45, Janis Heller escreveu:
> Seems like parsing would be what I need. Is the size (consumed bandwith) and the usernams (timestamp can be generated by my parser) being written to this file?
> Could you show me a sample output of this file?

     the already existing documentation is your friend :)

http://wiki.squid-cache.org/SquidFaq/SquidLogs


-- 


	Atenciosamente / Sincerily,
	Leonardo Rodrigues
	Solutti Tecnologia
	http://www.solutti.com.br

	Minha armadilha de SPAM, N?O mandem email
	gertrudes at solutti.com.br
	My SPAMTRAP, do not email it





From erdosain9 at gmail.com  Tue Jun  6 14:16:19 2017
From: erdosain9 at gmail.com (erdosain9)
Date: Tue, 6 Jun 2017 07:16:19 -0700 (PDT)
Subject: [squid-users] this config is ok? is ok the order?
In-Reply-To: <723b8c10-d7bf-5539-70aa-f7e32ccc05c9@treenet.co.nz>
References: <1496157812155-4682631.post@n4.nabble.com>
 <1f29d3f2-3d4d-c55d-ad3b-4d06748b857f@treenet.co.nz>
 <1496322644043-4682653.post@n4.nabble.com>
 <3ef299a1-df56-4913-38f7-f9627cded853@treenet.co.nz>
 <1496676240770-4682677.post@n4.nabble.com>
 <1793820.seS1fI72I3@pikantusdevuan>
 <1496688642885-4682679.post@n4.nabble.com>
 <8009368.3uC88zRBUU@pikantusdevuan>
 <723b8c10-d7bf-5539-70aa-f7e32ccc05c9@treenet.co.nz>
Message-ID: <1496758579768-4682702.post@n4.nabble.com>

oh ok!
so... dosent have any sense try to have a big ttl?
because ok, if i use just a own dns resolver then "they" have just one ttl
and no one for each user. 
But, would not be better have long ttl??? 
the ip attached to a domain name it's changing so quickly (15', for
example)?? i dont understand that. because if it is not changing so quickly
why those values so lows??
Thanks (and again... sorry... for...my...ignorance... and my bad writing)



--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/this-config-is-ok-is-ok-the-order-tp4682631p4682702.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From rousskov at measurement-factory.com  Tue Jun  6 15:19:27 2017
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Tue, 6 Jun 2017 09:19:27 -0600
Subject: [squid-users] this config is ok? is ok the order?
In-Reply-To: <1496758579768-4682702.post@n4.nabble.com>
References: <1496157812155-4682631.post@n4.nabble.com>
 <1f29d3f2-3d4d-c55d-ad3b-4d06748b857f@treenet.co.nz>
 <1496322644043-4682653.post@n4.nabble.com>
 <3ef299a1-df56-4913-38f7-f9627cded853@treenet.co.nz>
 <1496676240770-4682677.post@n4.nabble.com>
 <1793820.seS1fI72I3@pikantusdevuan>
 <1496688642885-4682679.post@n4.nabble.com>
 <8009368.3uC88zRBUU@pikantusdevuan>
 <723b8c10-d7bf-5539-70aa-f7e32ccc05c9@treenet.co.nz>
 <1496758579768-4682702.post@n4.nabble.com>
Message-ID: <72da3678-3f72-dd64-977f-5e6e06a34a61@measurement-factory.com>

On 06/06/2017 08:16 AM, erdosain9 wrote:

> if it is not changing so quickly why those values so lows??

Low TTLs for stable DNS records can be used for several reasons, including:

* To track DNS resolvers asking for those records.
* To be able to change those DNS records on a short notice.
* To improve round-robin effects of multiple IP addresses.

Alex.


From rousskov at measurement-factory.com  Tue Jun  6 15:34:21 2017
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Tue, 6 Jun 2017 09:34:21 -0600
Subject: [squid-users] FW:  FW: squid proxy 3.5 redhat 7.3
In-Reply-To: <bbf7651c9be648cba439245cf8e8bd00@rechtspraak.nl>
References: <382fd8bbff524df9971d461b69ae4eeb@rechtspraak.nl>
 <46ea5daf-a067-0090-d22d-fd3d04b803e8@measurement-factory.com>
 <448797274e5f40d781241fcc3e4b08e9@rechtspraak.nl>
 <5ac71a9d-1fd6-7a89-5f28-c5925189364e@measurement-factory.com>
 <b4e2cf45755442cbacbb00746b8ed817@rechtspraak.nl>
 <7bc2697e57bb450e9ba716e54de290fa@rechtspraak.nl>
 <bbf7651c9be648cba439245cf8e8bd00@rechtspraak.nl>
Message-ID: <c1c9c27f-2fa0-eba9-9dca-31698817fbde@measurement-factory.com>

On 06/06/2017 08:22 AM, Madonna, A. (spir-it) wrote:

> Know issue 2012 squid proxy 3.2
> 
> http://www.squid-cache.org/Versions/v3/3.2/RELEASENOTES.html#ss1.1
> ?SSL-Bump not re-wrapping decrypted traffic in CONNECT for peers.

> + 5 years ago this already  was a known issue. Apparently even after
> + 5 years there is still proper solution. Can we expect anything
> regarding this in the near future?

FWIW, I am not aware of anybody working on this problem. Going forward,
your options include those outlined at the following FAQ entry:

http://wiki.squid-cache.org/SquidFaq/AboutSquid#How_to_add_a_new_Squid_feature.2C_enhance.2C_of_fix_something.3F


> This person already describes the issue in his blog and  offers a solution although its not perfect. 

> https://www.mydlp.com/using-parent-proxy-ssl-bump-enabled-squid-3-2/

Yes, one can replace one problem with another. Or, to be more precise,
since we are apparently talking about going back to Squid v3.2, one can
replace one problem with a large bag of different problems. Pick your
poison.


> also it is still not clear to me if the traffic is encrypted again
> after leaving the squid proxy when doing ssl bump when using a parent
> proxy.

Bugs notwithstanding, the HTTPS traffic leaving moderns Squids is
encrypted. The workaround at the above link re-introduces an old bug
that allows Squid to emit decrypted traffic.

Alex.


> -----Oorspronkelijk bericht-----
> Van: Alex Rousskov [mailto:rousskov at measurement-factory.com] 
> Verzonden: vrijdag 2 juni 2017 17:59
> Aan: Madonna, A. (spir-it) <A.Madonna at rechtspraak.nl>; squid-users at lists.squid-cache.org
> Onderwerp: Re: [squid-users] FW: squid proxy 3.5 redhat 7.3
> 
> On 06/02/2017 01:37 AM, Madonna, A. (spir-it) wrote:
> 
>> Clients -> squid proxy -> internet.
>> This works with the config as previously mentioned.
> 
> OK.
> 
> 
>> Clients -> squid proxy (with cache_peer) -> Parent Proxy (not Squid) 
>> -> internet Does not work.
> 
> Even for regular HTTP traffic and non-bumped HTTPS traffic? If that traffic does not work, then you have misconfigured something or the Parent Proxy is badly broken. There is nothing special in the above setup as far as regular traffic is concerned.
> 
> 
>> However I've also setup the following:
>>
>> Cleints -> Squid Proxy (with cache_peer) -> Parent Proxy (Squid Proxy) 
>> -> internet
>>
>> This seems at least to work for http traffic, however, I don't see any HTTPS traffic coming into the Parent Proxy (Squid).
> 
> Squid does not know who made the parent proxy. The fact that one (presumably production-quality) proxy "does not work" and another "seems to work" implies that something is seriously misconfigured in one or both cases.
> 
> 
>> Now this morning I will do some more tcpdumping to see where that traffic is going, but maybe you can already shed some light on this?
> 
> I cannot shed more light on problems described only as "does not work"
> and "no traffic".
> 
> Alex.
> 
> 
>> -----Oorspronkelijk bericht-----
>> Van: Alex Rousskov [mailto:rousskov at measurement-factory.com]
>> Verzonden: donderdag 1 juni 2017 18:49
>> Aan: Madonna, A. (spir-it) <A.Madonna at rechtspraak.nl>; 
>> squid-users at lists.squid-cache.org
>> Onderwerp: Re: [squid-users] squid proxy 3.5 redhat 7.3
>>
>> On 06/01/2017 10:09 AM, Madonna, A. (spir-it) wrote:
>>> can we use ssl_bump to intercept https traffic with a parent proxy (cache_peer).
>>
>> IIRC, you may be able to use limited SslBump features, but not the full SslBump functionality: Peeking or staring at the origin server through a cache_peer is not supported (yet).
>>
>>
>>> ssl_bump peek step1
>>> cache_peer ... parent 8080 0 no-query no-netdb-exchange no-digest
>>
>> Bugs notwithstanding, the above combination should work because peeking at step1 does not require communication with a cache_peer and splicing at step2 should follow the regular (non-SslBump) tunneling path for CONNECTs, where modern Squids do support cache peers.
>>
>>
>> I recommend that you make everything work without a cache_peer and then add a cache_peer.
>>
>> Alex.
>>
>>
>> ________________________________
>>
>> Informatie van de Raad voor de rechtspraak, de rechtbanken, de gerechtshoven en de bijzondere colleges vindt u op www.rechtspraak.nl.
>> _______________________________________________
>> squid-users mailing list
>> squid-users at lists.squid-cache.org
>> http://lists.squid-cache.org/listinfo/squid-users
>>
> 



From squid3 at treenet.co.nz  Tue Jun  6 16:18:25 2017
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Wed, 7 Jun 2017 04:18:25 +1200
Subject: [squid-users] Cache peer help
In-Reply-To: <c898a3a31c0d496a91d7e199431d9302@UPF-BORN-MBX.crg.es>
References: <c84b8794fa3e4b118d128a687b7047d6@UPF-BORN-MBX.crg.es>
 <a7ea26a7-6f65-c68b-8fd2-5b0f1b9b1c29@treenet.co.nz>
 <c898a3a31c0d496a91d7e199431d9302@UPF-BORN-MBX.crg.es>
Message-ID: <5a9bc1c6-cbce-a4d7-907f-719ad1d5b827@treenet.co.nz>

On 07/06/17 02:24, Alejandro Delgado Moreno wrote:
> Sorry for this mistake,
>
> It's:
>
> acl journals dstdomain "/etc/squid/xx_LIST.txt"
>
>   cache_peer xxx.xxx.xxx.xxx parent 9090 0 no-query no-digest default
>
>   cache_peer_access xxx.xxx.xxx.xxx allow journals
>
> and it's the same, in both lines.

Okay then the issue is something else, those lines in isolation are 
correct for allowing traffic to use that peer, but there are many other 
things that may make other routes either required or preferred.

So what is the rest of your squid.conf and can you provide a sample of 
the access.log for the traffic going wrong?

Amos



From squid3 at treenet.co.nz  Tue Jun  6 16:37:36 2017
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Wed, 7 Jun 2017 04:37:36 +1200
Subject: [squid-users] Squid issue of caching the m3u8 file
In-Reply-To: <CAEip7fwgrViUQA2vCM4Q5BXE5HmhKS373o=FV=zToE9HLw9uuw@mail.gmail.com>
References: <CAEip7fwgrViUQA2vCM4Q5BXE5HmhKS373o=FV=zToE9HLw9uuw@mail.gmail.com>
Message-ID: <eb6634a4-7ebf-4276-bcc4-0ae9dca50a39@treenet.co.nz>

On 07/06/17 01:33, LIU Yaning wrote:
> Dear Amos,
>
> Thanks a lot for your explanation and suggestion. I added the 
> "store-stale" to the refresh_pattern rule as:
> refresh_pattern -i \.(ts|m3u8)$ 120 90% 1000 override-expire 
> override-lastmod ignore-no-cache ignore-no-store store-stale
>
> However, I have checked the access.log, I am still getting TCP_Miss.
> 1496754869.963 13 192.168.0.100 TCP_MISS/200 16636 GET 
> http://qthttp.apple.com.edgesuite.net/1010qwoeiuryfg/0150_vod.m3u8 
> <http://www.google.com/url?q=http%3A%2F%2Fqthttp.apple.com.edgesuite.net%2F1010qwoeiuryfg%2F0150_vod.m3u8&sa=D&sntz=1&usg=AFQjCNHpiHy55EMeBIaMGhgEKRHanTrXxg> - 
> HIER_DIRECT/95.101.182.201 
> <http://www.google.com/url?q=http%3A%2F%2F95.101.182.201&sa=D&sntz=1&usg=AFQjCNE9ZWXH7sOJgbqIA--MJwxobSp76Q> application/x-mpegURL
> 1496754870.605 4 192.168.0.100 TCP_MISS/200 16636 GET 
> http://qthttp.apple.com.edgesuite.net/1010qwoeiuryfg/0150_vod.m3u8 
> <http://www.google.com/url?q=http%3A%2F%2Fqthttp.apple.com.edgesuite.net%2F1010qwoeiuryfg%2F0150_vod.m3u8&sa=D&sntz=1&usg=AFQjCNHpiHy55EMeBIaMGhgEKRHanTrXxg> - 
> HIER_DIRECT/95.101.182.201 
> <http://www.google.com/url?q=http%3A%2F%2F95.101.182.201&sa=D&sntz=1&usg=AFQjCNE9ZWXH7sOJgbqIA--MJwxobSp76Q> application/x-mpegURL
> 1496754871.194 15 192.168.0.100 TCP_MISS/200 16636 GET 
> http://qthttp.apple.com.edgesuite.net/1010qwoeiuryfg/0150_vod.m3u8 
> <http://www.google.com/url?q=http%3A%2F%2Fqthttp.apple.com.edgesuite.net%2F1010qwoeiuryfg%2F0150_vod.m3u8&sa=D&sntz=1&usg=AFQjCNHpiHy55EMeBIaMGhgEKRHanTrXxg> - 
> HIER_DIRECT/95.101.182.201 
> <http://www.google.com/url?q=http%3A%2F%2F95.101.182.201&sa=D&sntz=1&usg=AFQjCNE9ZWXH7sOJgbqIA--MJwxobSp76Q> application/x-mpegURL
> 1496754871.715 4 192.168.0.100 TCP_MISS/200 16636 GET 
> http://qthttp.apple.com.edgesuite.net/1010qwoeiuryfg/0150_vod.m3u8 
> <http://www.google.com/url?q=http%3A%2F%2Fqthttp.apple.com.edgesuite.net%2F1010qwoeiuryfg%2F0150_vod.m3u8&sa=D&sntz=1&usg=AFQjCNHpiHy55EMeBIaMGhgEKRHanTrXxg> - 
> HIER_DIRECT/95.101.182.201 
> <http://www.google.com/url?q=http%3A%2F%2F95.101.182.201&sa=D&sntz=1&usg=AFQjCNE9ZWXH7sOJgbqIA--MJwxobSp76Q> application/x-mpegURL
>
> If I understand well, TCP_MISS/200 shows the content is not cached by 
> Squid. Could you please help me to see if anything I did wrong to make 
> .m3u8 not cached by Squid?

I'm out of ideas sorry.


FWIW: this is what I get from my test setup using your refresh_pattern:

1496766607.433   1284 ::1 TCP_MISS/200 16628 GET 
http://qthttp.apple.com.edgesuite.net/1010qwoeiuryfg/0150_vod.m3u8 - 
HIER_DIRECT/119.224.143.41 application/x-mpegURL
1496766610.254    902 ::1 TCP_REFRESH_UNMODIFIED/200 16499 GET 
http://qthttp.apple.com.edgesuite.net/1010qwoeiuryfg/0150_vod.m3u8 - 
HIER_DIRECT/119.224.143.41 application/x-mpegURL

I am currently testing Squid-4.0.20, but it should behave the same in 
the recent few v3.5 releases.

Amos



From xeron.oskom at gmail.com  Wed Jun  7 00:13:05 2017
From: xeron.oskom at gmail.com (Ivan Larionov)
Date: Tue, 6 Jun 2017 17:13:05 -0700
Subject: [squid-users] Huge amount of time_wait connections after upgrade
	from v2 to v3
Message-ID: <CAHvB88x+nNYMtmNZWqrZo-p==UykYDj5dZF+5N1eC3rGR8ybGw@mail.gmail.com>

Hi!

We recently updated from squid v2 to v3 and now see huge increase in
connections in TIME_WAIT state on our squid servers (verified that this is
clients connections).

See versions and amount of such connections under the same load with the
same configs (except some incompatible stuff):

squid 2.7.STABLE9

configure options:  '--program-prefix=' '--prefix=/usr'
'--exec-prefix=/usr' '--bindir=/usr/bin' '--sbindir=/usr/sbin'
'--sysconfdir=/etc' '--includedir=/usr/include' '--libdir=/usr/lib'
'--libexecdir=/usr/libexec' '--sharedstatedir=/usr/com'
'--mandir=/usr/share/man' '--infodir=/usr/share/info' '--exec_prefix=/usr'
'--bindir=/usr/sbin' '--libexecdir=/usr/lib/squid' '--localstatedir=/var'
'--datadir=/usr/share' '--sysconfdir=/etc/squid' '--enable-epoll'
'--enable-removal-policies=heap,lru' '--enable-storeio=aufs'
'--enable-delay-pools' '--with-pthreads' '--enable-cache-digests'
'--enable-useragent-log' '--enable-referer-log' '--with-large-files'
'--with-maxfd=16384' '--enable-err-languages=English'

# netstat -tn | grep TIME_WAIT | grep 3128 | wc -l
95

squid 3.5.25

configure options:  '--program-prefix=' '--prefix=/usr'
'--exec-prefix=/usr' '--bindir=/usr/sbin' '--sbindir=/usr/sbin'
'--sysconfdir=/etc/squid' '--libdir=/usr/lib' '--libexecdir=/usr/lib/squid'
'--includedir=/usr/include' '--datadir=/usr/share'
'--sharedstatedir=/usr/com' '--localstatedir=/var'
'--mandir=/usr/share/man' '--infodir=/usr/share/info' '--enable-epoll'
'--enable-removal-policies=heap,lru' '--enable-storeio=aufs'
'--enable-delay-pools' '--with-pthreads' '--enable-cache-digests'
'--enable-useragent-log' '--enable-referer-log' '--with-large-files'
'--with-maxfd=16384' '--enable-err-languages=English' '--enable-htcp'

# netstat -tn | grep TIME_WAIT | grep 3128 | wc -l
11277

Config:

http_port 0.0.0.0:3128

acl localnet src 10.0.0.0/8     # RFC1918 possible internal network
acl localnet src 172.16.0.0/12  # RFC1918 possible internal network
acl localnet src 192.168.0.0/16 # RFC1918 possible internal network
acl localnet src fc00::/7       # RFC 4193 local private network range
acl localnet src fe80::/10      # RFC 4291 link-local (directly plugged)
machines

acl SSL_ports port 443

acl Safe_ports port 80          # http
acl Safe_ports port 21          # ftp
acl Safe_ports port 443         # https
acl Safe_ports port 70          # gopher
acl Safe_ports port 210         # wais
acl Safe_ports port 280         # http-mgmt
acl Safe_ports port 488         # gss-http
acl Safe_ports port 591         # filemaker
acl Safe_ports port 777         # multiling http
acl Safe_ports port 1025-65535  # unregistered ports

acl CONNECT method CONNECT

### START CUSTOM
acl Purge_method method PURGE

# Allow localhost to selectively flush the cache
http_access allow localhost Purge_method
http_access deny Purge_method
### END CUSTOM

### ALLOW ACCESS TO ALL PORTS
# http_access deny !Safe_ports
http_access deny CONNECT !SSL_ports
http_access allow localhost manager
http_access deny manager

http_access allow localnet
http_access allow localhost
http_access deny all

### START CUSTOM
# Disable icp
icp_port 0
# Allow ICP queries from local networks only
icp_access allow localnet
icp_access allow localhost
icp_access deny all

# Disable htcp
htcp_port 0
# Allow HTCP queries from local networks only
htcp_access allow localnet
htcp_access allow localhost
htcp_access deny all

# Check for custom request header
acl custom_acl req_header x-use-custom-proxy -i true
# Check for x-use-new-proxy request header
acl custom_new_acl req_header x-use-new-proxy -i true

# first_proxy
cache_peer 127.0.0.1 parent 18070 0 no-query no-digest name=first_proxy
cache_peer_access first_proxy deny custom_acl
cache_peer_access first_proxy deny custom_new_acl

# second_proxy
cache_peer 127.0.0.1 parent 18079 0 no-query no-digest name=second_proxy
cache_peer_access second_proxy allow custom_acl
cache_peer_access second_proxy allow custom_new_acl
cache_peer_access second_proxy deny all

never_direct allow all

cache_mem 4620591 KB
maximum_object_size_in_memory 8 KB
memory_replacement_policy heap LRU
cache_replacement_policy heap LRU

cache_dir aufs /mnt/services/squid/cache 891289 16 256

minimum_object_size 64 bytes # none-zero so we dont cache mistakes
maximum_object_size 102400 KB

logformat combined %>a %ui %un [%tl] "%rm %ru HTTP/%rv" %>Hs %<st %tr
"%{Referer}>h" "%{User-Agent}>h" %Ss:%Sh
logformat squid %ts.%03tu %6tr %>a %Ss/%03>Hs %<st %rm %ru %un %Sh/%<A %mt

access_log stdio:/var/log/squid/access.log combined
cache_log /var/log/squid/cache.log
cache_store_log none
logfile_rotate 0

client_db off

pid_filename /var/run/squid.pid


coredump_dir /var/cache
### END CUSTOM

refresh_pattern ^ftp:           1440    20%     10080
refresh_pattern ^gopher:        1440    0%      1440
# refresh_pattern -i (/cgi-bin/|\?) 0     0%      0
refresh_pattern .               0       20%     4320

### START CUSTOM
# don't cache errors
negative_ttl 0 minutes
# always fetch object from the beginning regardless of Range requests
range_offset_limit none
cache_effective_user squid
cache_effective_group squid
max_filedescriptors 524288
via off
forwarded_for delete
### END CUSTOM

We tried "half_closed_clients on" but it didn't help.

Any ideas?

Thanks.

-- 
With best regards, Ivan Larionov.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20170606/fbabe730/attachment.htm>

From makleking at yandex.ru  Wed Jun  7 01:00:15 2017
From: makleking at yandex.ru (=?utf-8?B?0JzQuNGF0LDQuNC7?=)
Date: Wed, 07 Jun 2017 09:00:15 +0800
Subject: [squid-users] CacheManager::ParseUrl: action 'digestauthenticator'
	not found
Message-ID: <634341496797215@web32g.yandex.ru>

Hi everybody!
Sometimes i get in cache.log this message:
"CacheManager::ParseUrl: action 'digestauthenticator' not found"

Tell me what it means?

About my systems:
Squid Cache: Version 3.5.25
Service Name: squid
configure options:  '--prefix=/usr' '--with-logdir=/var/log/squid/' '--includedir=/usr/include' '--datadir=/usr/share' '--bindir=/usr/sbin' '--libexecdir=/usr/lib/squid' '--localstatedir=/var' '--sysconfdir=/etc/squid' '--with-default-user=squid' '--disable-ipv6' '--with-filedescriptors=32768' '--enable-default-err-language=Russian' '--enable-err-languages=Russian' '--enable-delay-pools' '--enable-ssl' --enable-ltdl-convenience

# uname -a
Linux proxy.***.corp 3.10.0-514.16.1.el7.x86_64 #1 SMP Fri Mar 10 13:12:32 EST 2017 x86_64 x86_64 x86_64 GNU/Linux

Best regards, Micheal.


From squid at bloms.de  Wed Jun  7 08:30:24 2017
From: squid at bloms.de (Dieter Bloms)
Date: Wed, 7 Jun 2017 10:30:24 +0200
Subject: [squid-users] Huge amount of time_wait connections after
 upgrade from v2 to v3
In-Reply-To: <CAHvB88x+nNYMtmNZWqrZo-p==UykYDj5dZF+5N1eC3rGR8ybGw@mail.gmail.com>
References: <CAHvB88x+nNYMtmNZWqrZo-p==UykYDj5dZF+5N1eC3rGR8ybGw@mail.gmail.com>
Message-ID: <20170607083024.GA62@bloms.de>

Hi Ivan,

On Tue, Jun 06, Ivan Larionov wrote:

> We recently updated from squid v2 to v3 and now see huge increase in
> connections in TIME_WAIT state on our squid servers (verified that this is
> clients connections).

I can confirm that since 3.5.22 to our ICAP scanners.
with 3.5.21 we had no problems on SLES11 SP4 operating system.
We did some tests with RHEL7 and we had much less TIME_WAIT.
Do you use an older operation system ?


-- 
Regards

  Dieter

--
I do not get viruses because I do not use MS software.
If you use Outlook then please do not put my email address in your
address-book so that WHEN you get a virus it won't use my address in the
>From field.


From f6253283 at hotmail.com  Wed Jun  7 09:37:42 2017
From: f6253283 at hotmail.com (Jason Chiu)
Date: Wed, 7 Jun 2017 02:37:42 -0700 (PDT)
Subject: [squid-users] squid 3.5 ssl-bump intercept TCP_DENIED/200 on bridge
	mode
Message-ID: <1496828262040-4682712.post@n4.nabble.com>

I had a FreeBSD 9.1 bridge (em0, em1) environment,
Use "pf rdr to" redirect HTTPS (port 443) packets to squid (squid 127.0.0.1:
3129)

Squid *3.3.11* ssl bump is OK.


The following is the setting of squid 3.3.11

Squid Cache: Version 3.3.11-20140220-r12672
Configure options: '--prefix = / usr / local / squid' '--sysconfdir = / etc
/ squid' '--localstatedir = / var / squid' '--datadir = / usr / share /
squid' Enable-icap-client '' --enable-ssl '' --with-pthreads ''
--enable-pf-transparent '' --enable-ssl-crtd '' --enable-ecap ''
PKG_CONFIG_PATH = / usr / Local / lib / pkgconfig '--enable-ltdl-convenience


Recently in order to allow squid can signing generated sha256 certificates 
, 
upgrade squid to 3.5.24 version.


But ssl bump * is not OK *

Access.log always appears the following message:
1495699856.074      0 192.168.95.81 TCP_DENIED/200 0 CONNECT 127.0.0.1:3129
- HIER_NONE/- -
1495699857.720      0 192.168.95.81 TCP_DENIED/200 0 CONNECT 127.0.0.1:3129
- HIER_NONE/- -
1495701676.054      0 192.168.95.81 TCP_DENIED/200 0 CONNECT 127.0.0.1:3129
- HIER_NONE/- -
1495701676.717      0 192.168.95.81 TCP_DENIED/200 0 CONNECT 127.0.0.1:3129
- HIER_NONE/- -
1495701677.060      0 192.168.95.81 TCP_DENIED/200 0 CONNECT 127.0.0.1:3129
- HIER_NONE/- -
1495701677.354      0 192.168.95.81 TCP_DENIED/200 0 CONNECT 127.0.0.1:3129
- HIER_NONE/- -

*Need to adjust which part of the settings?*



The following is my settings: 

Squid Cache: Version 3.5.24-20170331-r14150
Service Name: squid
configure options:  '--prefix=/usr/local/squid' '--sysconfdir=/etc/squid'
'--localstatedir=/var/squid' '--datadir=/usr/share/squid'
'--enable-icap-client' '--enable-ssl' '--with-pthreads'
'--enable-pf-transparent' '--enable-ssl-crtd' '--enable-ecap'
'--with-openssl' 'PKG_CONFIG_PATH=/usr/local/lib/pkgconfig'
--enable-ltdl-convenience

------------
squid.conf
------------
#
# Recommended minimum configuration:
#

# Example rule allowing access from your local networks.
# Adapt to list your (internal) IP networks from where browsing
# should be allowed
acl localnet src 10.0.0.0/8     # RFC1918 possible internal network
acl localnet src 172.16.0.0/12  # RFC1918 possible internal network
acl localnet src 192.168.0.0/16 # RFC1918 possible internal network
acl localnet src fc00::/7       # RFC 4193 local private network range
acl localnet src fe80::/10      # RFC 4291 link-local (directly plugged)
machines

acl SSL_ports port 443
acl Safe_ports port 80          # http
acl Safe_ports port 21          # ftp
acl Safe_ports port 443         # https
acl Safe_ports port 70          # gopher
acl Safe_ports port 210         # wais
acl Safe_ports port 1025-65535  # unregistered ports
acl Safe_ports port 280         # http-mgmt
acl Safe_ports port 488         # gss-http
acl Safe_ports port 591         # filemaker
acl Safe_ports port 777         # multiling http
acl CONNECT method CONNECT

#
# Recommended minimum Access Permission configuration:
#
# Deny requests to certain unsafe ports
http_access deny !Safe_ports

# Deny CONNECT to other than secure SSL ports
http_access deny CONNECT !SSL_ports

# Only allow cachemgr access from localhost
http_access allow localhost manager
http_access deny manager

# We strongly recommend the following be uncommented to protect innocent
# web applications running on the proxy server who think the only
# one who can access services on "localhost" is a local user
#http_access deny to_localhost

#
# INSERT YOUR OWN RULE(S) HERE TO ALLOW ACCESS FROM YOUR CLIENTS
#

# Example rule allowing access from your local networks.
# Adapt localnet in the ACL section to list your (internal) IP networks
# from where browsing should be allowed
http_access allow localnet
http_access allow localhost

# And finally deny all other access to this proxy
http_access deny all

# Squid normally listens to port 3128
http_port 3128

# Uncomment and adjust the following to add a disk cache directory.
#cache_dir ufs /var/squid/cache/squid 100 16 256

# Leave coredumps in the first cache dir
coredump_dir /var/squid/cache/squid

#
# Add any of your own refresh_pattern entries above these.
#
refresh_pattern ^ftp:           1440    20%     10080
refresh_pattern ^gopher:        1440    0%      1440
refresh_pattern -i (/cgi-bin/|\?) 0     0%      0
refresh_pattern .               0       20%     4320

#http_port 3129 ssl-bump cert=/usr/local/squid/ssl_cert/myCA.pem
generate-host-certificates=on dynamic_cert_mem_cache_size=4MB
https_port 3129 intercept ssl-bump cert=/usr/local/squid/ssl_cert/myCA.pem
generate-host-certificates=on dynamic_cert_mem_cache_size=4MB

acl step1 at_step SslBump1
acl step2 at_step SslBump2
acl step3 at_step SslBump3
ssl_bump peek step1 all
ssl_bump bump all

# sslcrtd
sslcrtd_program /usr/local/squid/libexec/ssl_crtd -s /var/squid/ssl_db -M
10MB
sslcrtd_children 5

# sslproxy setting
sslproxy_capath /var/squid/ssl_db/certs
sslproxy_options NO_SSLv2,NO_SSLv3,NO_TLSv1,SINGLE_DH_USE,SINGLE_ECDH_USE
#sslproxy_cipher
EECDH+ECDSA+AESGCM:EECDH+aRSA+AESGCM:EECDH+ECDSA+SHA384:EECDH+ECDSA+SHA256:EECDH+aRSA+SHA384:EECDH+aRSA+SHA256:EECDH+aRSA+RC4:EECDH:EDH+aRSA:!RC4:!aNULL:!eNULL:!LOW:!3DES:!MD5:!EXP:!PSK:!SRP:!DSS
sslproxy_cert_error allow all
sslproxy_cert_adapt setValidAfter all

sslproxy_flags DONT_VERIFY_PEER

----------------------------------------
pf.conf
---------------------------------------
#internal interface
int_if = '{em1}'

# Normalization: reassemble fragments resolve or reduce traffic ambiguities.
scrub in all
set skip on lo0

#sslTP rdr setting
rdr_from = 'any'
rdr_to = 'any;
rdr on $int_if inet proto tcp from $rdr_from to $rdr_to port 443 ->
127.0.0.1 port 3129
pass in all no state
pass out all no state
pass in quick on $int_if route-to lo0 inet proto tcp from $rdr_from to any
keep state



--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/squid-3-5-ssl-bump-intercept-TCP-DENIED-200-on-bridge-mode-tp4682712.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From f6253283 at hotmail.com  Wed Jun  7 09:39:33 2017
From: f6253283 at hotmail.com (Jason Chiu)
Date: Wed, 7 Jun 2017 02:39:33 -0700 (PDT)
Subject: [squid-users] squid 3.5 ssl-bump intercept TCP_DENIED/200 on
	bridge mode
In-Reply-To: <1496828262040-4682712.post@n4.nabble.com>
References: <1496828262040-4682712.post@n4.nabble.com>
Message-ID: <1496828373361-4682713.post@n4.nabble.com>

I also tested the following cases 
test case 1: 

add the following settings in squid.conf 

acl bumpedPorts myportname 3129 
http_access allow CONNECT bumpedPorts 

test results:  ssl bump is failed
1. access.log no record 
2. web browser has been waiting , no response 

---------------------------------------------- 

test case 2: 
1. squid.conf  use  http_port 3129 ssl-bump
cert=/usr/local/squid/ssl_cert/myCA.pem generate-host-certificates=on
dynamic_cert_mem_cache_size=4MB 
2. web browser use proxy server x.x.x.x 3129 

test result :  ssl bump is OK 



--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/squid-3-5-ssl-bump-intercept-TCP-DENIED-200-on-bridge-mode-tp4682712p4682713.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From chip_pop at hotmail.com  Wed Jun  7 09:42:25 2017
From: chip_pop at hotmail.com (joseph)
Date: Wed, 7 Jun 2017 02:42:25 -0700 (PDT)
Subject: [squid-users] Squid issue of caching the m3u8 file
In-Reply-To: <eb6634a4-7ebf-4276-bcc4-0ae9dca50a39@treenet.co.nz>
References: <CAEip7fzdjxuchvn3YUrdh1jnXOvfn=bUqkWLko_X_-rVq2Wn5Q@mail.gmail.com>
 <CAEip7fwgrViUQA2vCM4Q5BXE5HmhKS373o=FV=zToE9HLw9uuw@mail.gmail.com>
 <eb6634a4-7ebf-4276-bcc4-0ae9dca50a39@treenet.co.nz>
Message-ID: <1496828545709-4682714.post@n4.nabble.com>

is Set-Cookie:  saved in cached file as well ?? amos




--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/Squid-issue-of-caching-the-m3u8-file-tp4682674p4682714.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From ishayahu at mail.ru  Wed Jun  7 10:54:46 2017
From: ishayahu at mail.ru (=?UTF-8?B?0JjRiNCw0Y9o0YMg0JvQsNGB0YLQvtCy?=)
Date: Wed, 07 Jun 2017 13:54:46 +0300
Subject: [squid-users] =?utf-8?q?Error_while_writing_to_TCP_socket=3A_Perm?=
 =?utf-8?q?ission_denied?=
Message-ID: <1496832886.263517883@f227.i.mail.ru>


I use squid on freebsd 10.3. When I set proxy adress in web brouser, I can't attach files to mail on mail.ru. Whithout proxy it works. In access.log there are no errors. In cache.log I've got:
2017/05/29 21:12:16 kid1| local=217.151.68.36:34572 remote=217.69.139.216:443 FD 44 flags=1: read/write failure: (13) Permission denied
2017/05/29 21:12:16 kid1| local=217.151.68.36:36057 remote=217.69.139.216:443 FD 44 flags=1: read/write failure: (13) Permission denied
FD can change If I understand right, how to use lson, then
root at bkp_router:/home/ishayahu # lsof -d44
COMMAND   PID  USER   FD   TYPE             DEVICE SIZE/OFF NODE NAME
squid   30197 squid   44u  IPv4 0xfffff80029788810      0t0  TCP broadband-68-36.clients.extel.ru:48387->e.mail.ru:https (ESTABLISHED)
broadband-68-36.clients.extel.ru is ISP's router
How can I solve that problem?
PS: settings like
#request_body_max_size 0
#reply_body_max_size 0
#request_header_max_size 64 KB
#reply_header_max_size 64 KB
#client_request_buffer_max_size 50 Mb
#acl post method POST
#http_access allow post localnet
doesn't help
#ktrace -d -p 667 kdump>kdump.out
Searching in kdump.out for denied:
4529011-  6776 squid    GIO   fd 5 read 32 bytes
4529052-       0x0000 1b00 0000 0000 0000 feff 1000 0000 0000 400f 0000 0000 0000 0000 0000 0000 0000                 |................ at ...............|
4529197-
4529198-  6776 squid    RET   kevent 1
4529229-  6776 squid    CALL  write(0x1b,0x8057c3000,0x5b4)
4529281:  6776 squid    RET   write -1 errno 13 Permission denied
4529339-  6776 squid    CALL  write(0x4,0x802dbf000,0x88)
4529389-  6776 squid    GIO   fd 4 wrote 136 bytes
4529432:       "2017/06/04 17:34:06 kid1| local=217.151.68.36:42442 remote=217.69.139.216:443 FD 27 flags=1: read/write failure: (13) Permission denied
4529576-       "
4529585-  6776 squid    RET   write 136/0x88
4529622-  6776 squid    CALL  close(0x1b)
4529656-  6776 squid    RET   close 0
4529686-  6776 squid    CALL  close(0x19)
In kdump.out I see, that socekt was opened, was bunch of reading/writing to it, and without any seeing reason it ends with error.
Here socket opens:

root at bkp_router:/home/ishayahu # cat kdump.out | grep -b10 "RET socket 27/0x1b"
4534031-  6776 squid    GIO   fd 25 read 196 bytes
4534074-       "CONNECT e.mail.ru:443 HTTP/1.1\r
4534115-        User-Agent: Mozilla/5.0 (Windows NT 10.0; WOW64; rv:55.0) Gecko/20100101 Firefox/55.0\r
4534204-        Proxy-Connection: keep-alive\r
4534236-        Connection: keep-alive\r
4534262-        Host: e.mail.ru:443\r
4534285-        \r
4534289-       "
4534298-  6776 squid    RET   read 196/0xc4
4534334-  6776 squid    CALL  socket(PF_INET,SOCK_STREAM,IPPROTO_TCP)
4534396:  6776 squid    RET   socket 27/0x1b
4534433-  6776 squid    CALL  fcntl(0x1b,F_GETFD,0)
4534477-  6776 squid    RET   fcntl 0
4534507-  6776 squid    CALL  fcntl(0x1b,F_SETFD,FD_CLOEXEC)
4534560-  6776 squid    RET   fcntl 0
4534590-  6776 squid    CALL  fcntl(0x1b,F_GETFL,0)
4534634-  6776 squid    RET   fcntl 2
4534664-  6776 squid    CALL  fcntl(0x1b,F_SETFL,0x6<O_RDWR|O_NONBLOCK>)
4534729-  6776 squid    RET   fcntl 0
4534759-  6776 squid    CALL  setsockopt(0x1b,0x6,0x1,0x7fffffffe484,0x4)
4534825-  6776 squid    RET   setsockopt 0


-- 
????h? ??????

+7-906-772-88-86
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20170607/c38c0cff/attachment.htm>

From squid3 at treenet.co.nz  Wed Jun  7 11:34:03 2017
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Wed, 7 Jun 2017 23:34:03 +1200
Subject: [squid-users] Huge amount of time_wait connections after
 upgrade from v2 to v3
In-Reply-To: <CAHvB88x+nNYMtmNZWqrZo-p==UykYDj5dZF+5N1eC3rGR8ybGw@mail.gmail.com>
References: <CAHvB88x+nNYMtmNZWqrZo-p==UykYDj5dZF+5N1eC3rGR8ybGw@mail.gmail.com>
Message-ID: <ca593b00-8727-c816-8169-d6c528c51d93@treenet.co.nz>

On 07/06/17 12:13, Ivan Larionov wrote:
> Hi!
>
> We recently updated from squid v2 to v3 and now see huge increase in 
> connections in TIME_WAIT state on our squid servers (verified that 
> this is clients connections).

The biggest change between 2.7 and 3.5 in this area is that 2.7 was 
HTTP/1.0 which closed TCP connections after each request by default, and 
3.5 is HTTP/1.1 which does not. So connections are more likely to 
persist until they hit some TCP timeout then enter the slow TIME_WAIT 
process.

There were also some other bugs identified in older 3.5 releases which 
increased the TIME_WAIT specifically. I thought those were almost all 
fixed by now, but YMMV whether you hit the remaining issues.
  A workaround it to set 
<http://www.squid-cache.org/Doc/config/client_idle_pconn_timeout/> to a 
shorter value than the default  2min. eg you might want it to be 30sec 
or so.



>
> See versions and amount of such connections under the same load with 
> the same configs (except some incompatible stuff):
>
> squid 2.7.STABLE9
>
> configure options:  '--program-prefix=' '--prefix=/usr' 
> '--exec-prefix=/usr' '--bindir=/usr/bin' '--sbindir=/usr/sbin' 
> '--sysconfdir=/etc' '--includedir=/usr/include' '--libdir=/usr/lib' 
> '--libexecdir=/usr/libexec' '--sharedstatedir=/usr/com' 
> '--mandir=/usr/share/man' '--infodir=/usr/share/info' 
> '--exec_prefix=/usr' '--bindir=/usr/sbin' 
> '--libexecdir=/usr/lib/squid' '--localstatedir=/var' 
> '--datadir=/usr/share' '--sysconfdir=/etc/squid' '--enable-epoll' 
> '--enable-removal-policies=heap,lru' '--enable-storeio=aufs' 
> '--enable-delay-pools' '--with-pthreads' '--enable-cache-digests' 
> '--enable-useragent-log' '--enable-referer-log' '--with-large-files' 
> '--with-maxfd=16384' '--enable-err-languages=English'
>
> # netstat -tn | grep TIME_WAIT | grep 3128 | wc -l
> 95
>
> squid 3.5.25
>
> configure options:  '--program-prefix=' '--prefix=/usr' 
> '--exec-prefix=/usr' '--bindir=/usr/sbin' '--sbindir=/usr/sbin' 
> '--sysconfdir=/etc/squid' '--libdir=/usr/lib' 
> '--libexecdir=/usr/lib/squid' '--includedir=/usr/include' 
> '--datadir=/usr/share' '--sharedstatedir=/usr/com' 
> '--localstatedir=/var' '--mandir=/usr/share/man' 
> '--infodir=/usr/share/info' '--enable-epoll' 
> '--enable-removal-policies=heap,lru' '--enable-storeio=aufs' 
> '--enable-delay-pools' '--with-pthreads' '--enable-cache-digests' 
> '--enable-useragent-log' '--enable-referer-log' '--with-large-files' 
> '--with-maxfd=16384' '--enable-err-languages=English' '--enable-htcp'

FYI, these options are not doing anything for Squid-3:
   '--enable-useragent-log' '--enable-referer-log' 
'--enable-err-languages=English'


>
> # netstat -tn | grep TIME_WAIT | grep 3128 | wc -l
> 11277
>
> Config:
>
> http_port 0.0.0.0:3128 <http://0.0.0.0:3128>
>
> acl localnet src 10.0.0.0/8 <http://10.0.0.0/8>     # RFC1918 possible 
> internal network
> acl localnet src 172.16.0.0/12 <http://172.16.0.0/12>  # RFC1918 
> possible internal network
> acl localnet src 192.168.0.0/16 <http://192.168.0.0/16> # RFC1918 
> possible internal network
> acl localnet src fc00::/7       # RFC 4193 local private network range
> acl localnet src fe80::/10      # RFC 4291 link-local (directly 
> plugged) machines
>
> acl SSL_ports port 443
>
> acl Safe_ports port 80          # http
> acl Safe_ports port 21          # ftp
> acl Safe_ports port 443         # https
> acl Safe_ports port 70          # gopher
> acl Safe_ports port 210         # wais
> acl Safe_ports port 280         # http-mgmt
> acl Safe_ports port 488         # gss-http
> acl Safe_ports port 591         # filemaker
> acl Safe_ports port 777         # multiling http
> acl Safe_ports port 1025-65535  # unregistered ports
>
> acl CONNECT method CONNECT
>
> ### START CUSTOM
> acl Purge_method method PURGE
>
> # Allow localhost to selectively flush the cache
> http_access allow localhost Purge_method
> http_access deny Purge_method
> ### END CUSTOM
>
> ### ALLOW ACCESS TO ALL PORTS
> # http_access deny !Safe_ports
> http_access deny CONNECT !SSL_ports
> http_access allow localhost manager
> http_access deny manager
>
> http_access allow localnet
> http_access allow localhost
> http_access deny all
>
> ### START CUSTOM
> # Disable icp
> icp_port 0
> # Allow ICP queries from local networks only
> icp_access allow localnet
> icp_access allow localhost
> icp_access deny all
>
> # Disable htcp
> htcp_port 0
> # Allow HTCP queries from local networks only
> htcp_access allow localnet
> htcp_access allow localhost
> htcp_access deny all

FYI: setting icp_access and htcp_access is pointless when the relevant 
port is 0. That port 0 disables the entire component.

>
> # Check for custom request header
> acl custom_acl req_header x-use-custom-proxy -i true
> # Check for x-use-new-proxy request header
> acl custom_new_acl req_header x-use-new-proxy -i true
>
> # first_proxy
> cache_peer 127.0.0.1 parent 18070 0 no-query no-digest name=first_proxy
> cache_peer_access first_proxy deny custom_acl
> cache_peer_access first_proxy deny custom_new_acl
>
> # second_proxy
> cache_peer 127.0.0.1 parent 18079 0 no-query no-digest name=second_proxy
> cache_peer_access second_proxy allow custom_acl
> cache_peer_access second_proxy allow custom_new_acl
> cache_peer_access second_proxy deny all
>
> never_direct allow all
>
> cache_mem 4620591 KB
> maximum_object_size_in_memory 8 KB
> memory_replacement_policy heap LRU
> cache_replacement_policy heap LRU
>
> cache_dir aufs /mnt/services/squid/cache 891289 16 256
>
> minimum_object_size 64 bytes # none-zero so we dont cache mistakes
> maximum_object_size 102400 KB
>
> logformat combined %>a %ui %un [%tl] "%rm %ru HTTP/%rv" %>Hs %<st %tr 
> "%{Referer}>h" "%{User-Agent}>h" %Ss:%Sh
> logformat squid %ts.%03tu %6tr %>a %Ss/%03>Hs %<st %rm %ru %un %Sh/%<A %mt

Please do not re-define these formats. If you want to use the default 
format they are defined internally by Squid3, if you want any 
customizations use a different format name.

>
> access_log stdio:/var/log/squid/access.log combined
> cache_log /var/log/squid/cache.log
> cache_store_log none
> logfile_rotate 0
>
> client_db off
>
> pid_filename /var/run/squid.pid
>
>
> coredump_dir /var/cache
> ### END CUSTOM
>
> refresh_pattern ^ftp:           1440    20%     10080
> refresh_pattern ^gopher:        1440    0%      1440
> # refresh_pattern -i (/cgi-bin/|\?) 0     0%      0

Please do not remove that cgi-bin pattern. It is there to protect the 
cache against servers with broken/ancient CGI engines. It is designed 
explicitly so modern dynamic sites that provide proper cacheability 
headers can still be stored. So no harm and only benefits from in 
leaving it there.


Amos



From squid3 at treenet.co.nz  Wed Jun  7 12:21:26 2017
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 8 Jun 2017 00:21:26 +1200
Subject: [squid-users] CacheManager::ParseUrl: action
 'digestauthenticator' not found
In-Reply-To: <634341496797215@web32g.yandex.ru>
References: <634341496797215@web32g.yandex.ru>
Message-ID: <e4fa2717-9616-1d05-fd22-12070a3ea36a@treenet.co.nz>

On 07/06/17 13:00, ?????? wrote:
> Hi everybody!
> Sometimes i get in cache.log this message:
> "CacheManager::ParseUrl: action 'digestauthenticator' not found"
>
> Tell me what it means?

For some reason the Squid process answering a manager HTTP(S) request 
does not have the Digest authentication component registered. Your build 
options do not prohibit that component from existing, so it should be 
registered but that is still process dependent.

The parts of the log line which you omitted contain the information 
about which process that was reported by, what type of process it was 
and when the manager request happened.

Amos



From adielp at estereocentro.icrt.cu  Wed Jun  7 15:28:47 2017
From: adielp at estereocentro.icrt.cu (Adiel Plasencia Herrera)
Date: Wed, 07 Jun 2017 08:28:47 -0700
Subject: [squid-users] https_port
Message-ID: <WC20170607152847.780003@estereocentro.icrt.cu>

Hello,


They would help me with a configuration of my squid that I want to     
  implement.



My proxy passes all traffic to a parent proxy and I want clients to    
   connect to my proxy via https.



Can you help me how to implement the connection to my proxy via       
https?



To better explain what I want attached 2 pictures. The    
   image with 1.jpg name shows my proxy configuration with type HTTp that    

   connects well to internet.



What I want is for the connection to my proxy to be by the form of the 
      2.jpg image that uses the HTTPS type.



Or if it is possible then leave the 2 forms.





This is my current configuration:







acl trabajadores src 10.5.7.3 10.5.7.5



acl SSL_ports port 443

acl Safe_ports port 3128    # proxy server

acl Safe_ports port 80        # http

acl Safe_ports port 21        # ftp

acl Safe_ports port 443        # https

acl Safe_ports port 70        # gopher

acl Safe_ports port 210        # wais

acl Safe_ports port 1025-65535    # unregistered ports

acl Safe_ports port 280        # http-mgmt

acl Safe_ports port 488        # gss-http


acl Safe_ports port 591        # filemaker

acl Safe_ports port 777        # multiling     
  http

acl CONNECT method CONNECT



http_access allow trabajadores

http_access deny !Safe_ports

http_access deny CONNECT !SSL_ports

http_access deny all





http_port 3128





cache_peer 10.5.7.2   parent  3128 0  no-query default   
    login=PASS

forwarded_for on



#hierarchy_stoplist cgi-bin ?



cache_swap_low 90

cache_swap_high 95



#update_headers on

cache_mem 128 MB

#cache_access_log 

cache_dir ufs /var/spool/squid3 512 16 256



access_log daemon:/var/log/squid3/access.log squid

cache_log /var/log/squid3/cache.log

cache_store_log daemon:/var/log/squid3/store.log





refresh_pattern ^ftp:              
1440    20%    10080

refresh_pattern ^gopher:    1440          
0%    1440

refresh_pattern -i (/cgi-bin/|\?) 0    0%      
    0

refresh_pattern .        0      
    20%    4320







cache_mgr admin at example.com

#visible_hostname proxy.example.com

#unique_hostname proxy.example.com





nonhierarchical_direct off



dns_nameservers 10.5.7.2

coredump_dir /var/spool/squid3



max_filedescriptors 3200
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20170607/c06a9f1e/attachment.htm>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: 1.jpg
Type: image/jpeg
Size: 34086 bytes
Desc: not available
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20170607/c06a9f1e/attachment.jpg>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: 2.jpg
Type: image/jpeg
Size: 36593 bytes
Desc: not available
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20170607/c06a9f1e/attachment-0001.jpg>

From squid3 at treenet.co.nz  Wed Jun  7 12:35:22 2017
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 8 Jun 2017 00:35:22 +1200
Subject: [squid-users] Squid issue of caching the m3u8 file
In-Reply-To: <1496828545709-4682714.post@n4.nabble.com>
References: <CAEip7fzdjxuchvn3YUrdh1jnXOvfn=bUqkWLko_X_-rVq2Wn5Q@mail.gmail.com>
 <CAEip7fwgrViUQA2vCM4Q5BXE5HmhKS373o=FV=zToE9HLw9uuw@mail.gmail.com>
 <eb6634a4-7ebf-4276-bcc4-0ae9dca50a39@treenet.co.nz>
 <1496828545709-4682714.post@n4.nabble.com>
Message-ID: <3ed7b1ec-79ba-222c-1413-6ee71836ddc3@treenet.co.nz>

On 07/06/17 21:42, joseph wrote:
> is Set-Cookie:  saved in cached file as well ?? amos

Yes it is. The header on the cached object only gets removed on delivery 
to a client.
Squid does not comply with the Cookie specifications in this regard.

With the extra wrinkle that the mandatory revalidation on these objects 
may provide a new Set-Cookie header that gets added for delivery to the 
new client after the old/cached header was removed. So you may still see 
Set-Cookie on the revalidated HIT (aka REFRESH_UNMODIFIED). Adding a new 
Set-Cookie is usually the purpose of the revalidation being required on 
these sort of objects.

Amos



From squid3 at treenet.co.nz  Wed Jun  7 12:44:35 2017
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 8 Jun 2017 00:44:35 +1200
Subject: [squid-users] Error while writing to TCP socket: Permission
 denied
In-Reply-To: <1496832886.263517883@f227.i.mail.ru>
References: <1496832886.263517883@f227.i.mail.ru>
Message-ID: <d58887a2-864f-bed8-4982-1d1ffacdf67e@treenet.co.nz>

On 07/06/17 22:54, ????h? ?????? wrote:
>
> I use squid on freebsd 10.3. When I set proxy adress in web brouser, I 
> can't attach files to mail on mail.ru. Whithout proxy it works. In 
> access.log there are no errors. In cache.log I've got:
>
> |2017/05/29 21:12:16 kid1| local=217.151.68.36:34572 
> remote=217.69.139.216:443 FD 44 flags=1: read/write failure: (13) 
> Permission denied 2017/05/29 21:12:16 kid1| local=217.151.68.36:36057 
> remote=217.69.139.216:443 FD 44 flags=1: read/write failure: (13) 
> Permission denied |
>
> FD can change If I understand right, how to use lson, then
>

FD in this case is an open network connection. Each TCP connection has 
exactly one FD in Squid. They get re-used like ports, but it should 
remain unchanged as long as the TCP connection exists. Another identical 
TCP connection made later may get a different FD.

 > How can I solve that problem?


It is rather odd to have "Permission denied" (filesystem error!) on a 
network socket. The network equivalent is a failure to connect in the 
first place, or a sudden close event if already open. I suspect a bug in 
your OS kernel or whatever security system it has controlling access to 
system resources.

If this were Linux I would point at SELinux misconfiguration, but I'm 
not sure what (if anything) FreeBSD has doing that sort of control.


Sorry
Amos



From squid3 at treenet.co.nz  Wed Jun  7 13:04:31 2017
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 8 Jun 2017 01:04:31 +1200
Subject: [squid-users] https_port
In-Reply-To: <WC20170607152847.780003@estereocentro.icrt.cu>
References: <WC20170607152847.780003@estereocentro.icrt.cu>
Message-ID: <764ecd5f-6f6c-0eb5-90b4-5591ab5e1920@treenet.co.nz>

On 08/06/17 03:28, Adiel Plasencia Herrera wrote:
>
> Hello,
>
> They would help me with a configuration of my squid that I want to 
> implement.
>
> My proxy passes all traffic to a parent proxy and I want clients to 
> connect to my proxy via https.
>
> Can you help me how to implement the connection to my proxy via https?
>
> To better explain what I want attached 2 pictures. The image with 
> 1.jpg name shows my proxy configuration with type HTTp that connects 
> well to internet.
>
> What I want is for the connection to my proxy to be by the form of the 
> 2.jpg image that uses the HTTPS type.
>
> Or if it is possible then leave the 2 forms.

What operating system are you using, and what applications are you 
wanting to use this proxy connection?

The normal configuration is simply to add an https_port line with cert= 
parameter to your squid.conf. More details on that below.


>
>
> This is my current configuration:
> acl trabajadores src 10.5.7.3 10.5.7.5
>
<snip>
>
> http_access allow trabajadores
> http_access deny !Safe_ports
> http_access deny CONNECT !SSL_ports

You custom http_access rules ("allow trabajadores") should be down here 
after the basic security checks.

> http_access deny all
>
>
> http_port 3128

The above port is for receiving plain-text connections to the proxy. 
Most software supports this, with a few exceptions (usually Java apps).


To accept TLS connections to the proxy (not HTTPS *over* the proxy), 
what you do is add an https_port line here. That https_port line needs a 
cert= parameter containing the proxy server certificate. You may need 
other TLS/SSL parameters to fine tune what the TLS does, but just start 
with getting that basic setup to work.
  <http://www.squid-cache.org/Doc/config/https_port/>

For example:
   https_port 3129 cert=/etc/squid/proxy.pem

(the proxy.pem file here contains both the public server cert and 
private server key for that cert).

Many GUI applications (most notably browsers) do not support this type 
of connection to a proxy (or not well if they do). Which is where the 
Q's about your OS and applications come in. You may need to setup 
environment variables or PAC files to get the applications to work.


Note that this is *very* different situation to intercepting port 443 
traffic. Much more different than port 3128 vs. intercepted port 80. 
HTTPS traffic goes through these TLS proxy connections with 
double-layered encryption, so this setup does *not* magically make the 
proxy able to see inside HTTPS if that is what you are really after.

Amos



From chip_pop at hotmail.com  Wed Jun  7 13:00:37 2017
From: chip_pop at hotmail.com (joseph)
Date: Wed, 7 Jun 2017 06:00:37 -0700 (PDT)
Subject: [squid-users] Squid issue of caching the m3u8 file
In-Reply-To: <3ed7b1ec-79ba-222c-1413-6ee71836ddc3@treenet.co.nz>
References: <CAEip7fzdjxuchvn3YUrdh1jnXOvfn=bUqkWLko_X_-rVq2Wn5Q@mail.gmail.com>
 <CAEip7fwgrViUQA2vCM4Q5BXE5HmhKS373o=FV=zToE9HLw9uuw@mail.gmail.com>
 <eb6634a4-7ebf-4276-bcc4-0ae9dca50a39@treenet.co.nz>
 <1496828545709-4682714.post@n4.nabble.com>
 <3ed7b1ec-79ba-222c-1413-6ee71836ddc3@treenet.co.nz>
Message-ID: <1496840437112-4682722.post@n4.nabble.com>

so if the  server send   same obj  with new Cookie it will be miss  since the
Cookie dose not match in cached obj

regarding the topping sorry if i ask it will clear  all those question  in
all the question has Ben asked before
example cached file header

link   = 
http://sa.bbc.co.uk/bbc/bbc/s?name=SET-COUNTER.page&ml_name=webmodule&ml_version=63
header

Server: nginx
Date: Thu, 04 May 2017 16:14:41 GMT
Content-Type: image/gif
Content-Length: 43
Connection: close
Expires: Sat, 01 Jan 2000 00:00:00 GMT
Pragma: no-cache
Cache-Control: no-cache
P3P: policyref="http://www.nedstat.com/w3c/p3p.xml", CP="NOI DSP COR NID PSA
ADM OUR IND NAV COM"
so all those  should match right  to have hit  not  just the link and  some
of the header  like Cache-Control or so

even cookie and   p3p   am i right here
or  its just specific header should match ???

like his situation  for miss not only  the no-cache prevent HIT  its  cookie
as well
cookie on the link he provide  change on every single  click to that link

another question  what about  deleting  cookie  before save the content 
1 since  its same object they deliver so it will be  hit  dose it violate ??
2 or  damage the clients  web view ?? or anything bad







--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/Squid-issue-of-caching-the-m3u8-file-tp4682674p4682722.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From squid3 at treenet.co.nz  Wed Jun  7 13:56:42 2017
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 8 Jun 2017 01:56:42 +1200
Subject: [squid-users] Squid issue of caching the m3u8 file
In-Reply-To: <1496840437112-4682722.post@n4.nabble.com>
References: <CAEip7fzdjxuchvn3YUrdh1jnXOvfn=bUqkWLko_X_-rVq2Wn5Q@mail.gmail.com>
 <CAEip7fwgrViUQA2vCM4Q5BXE5HmhKS373o=FV=zToE9HLw9uuw@mail.gmail.com>
 <eb6634a4-7ebf-4276-bcc4-0ae9dca50a39@treenet.co.nz>
 <1496828545709-4682714.post@n4.nabble.com>
 <3ed7b1ec-79ba-222c-1413-6ee71836ddc3@treenet.co.nz>
 <1496840437112-4682722.post@n4.nabble.com>
Message-ID: <7f54b83e-4188-21c3-50ae-e1c2a8dec421@treenet.co.nz>

On 08/06/17 01:00, joseph wrote:
> so if the  server send   same obj  with new Cookie it will be miss  since the
> Cookie dose not match in cached obj

No, the Vary header does not say Cookie is part of the variance AFAICT. 
Just URL plus Accept-Encoding.

The Cookie/Set-Cookie being accurate is just related to whether the 
client browsing session is kept continuous or breaks. Depending on 
whether this transaction is part of a purchase or something similar that 
may be significant, or not.

At a guess since this is a media related object from Apple I suspect it 
is linked to an iTunes account of some sort. So the Cookie might be 
needed by the client for something. In that case getting a reply without 
one (as a pure cache HIT would appear to the client) may have problems.

That is just speculation though to show that this traffic behaviour is 
not completely unreasonable. Only the site and client software authors 
actually know for sure what is intended to be going on and why.


So, to get back on topic. Yes Squid should be caching it. But don't 
expect to see the letters "HIT" in the log anymore for this particular URL.

Amos



From chip_pop at hotmail.com  Wed Jun  7 14:04:36 2017
From: chip_pop at hotmail.com (joseph)
Date: Wed, 7 Jun 2017 07:04:36 -0700 (PDT)
Subject: [squid-users] Squid issue of caching the m3u8 file
In-Reply-To: <7f54b83e-4188-21c3-50ae-e1c2a8dec421@treenet.co.nz>
References: <CAEip7fzdjxuchvn3YUrdh1jnXOvfn=bUqkWLko_X_-rVq2Wn5Q@mail.gmail.com>
 <CAEip7fwgrViUQA2vCM4Q5BXE5HmhKS373o=FV=zToE9HLw9uuw@mail.gmail.com>
 <eb6634a4-7ebf-4276-bcc4-0ae9dca50a39@treenet.co.nz>
 <1496828545709-4682714.post@n4.nabble.com>
 <3ed7b1ec-79ba-222c-1413-6ee71836ddc3@treenet.co.nz>
 <1496840437112-4682722.post@n4.nabble.com>
 <7f54b83e-4188-21c3-50ae-e1c2a8dec421@treenet.co.nz>
Message-ID: <1496844276813-4682724.post@n4.nabble.com>

right 

lets say i have obj  with this header
if server  send the same  obj  with different Set-Cookie value that will be
MISS i was refairing to this
not to the vary if it has cookie sorry if i did not explain it correctly

Server: nginx
Date: Mon, 22 May 2017 15:44:59 GMT
Content-Type: image/gif
Content-Length: 43
Expires: Fri, 20 Mar 2009 00:00:00 GMT
P3P: CP="CUR ADM OUR NOR STA NID"
Set-Cookie:
ljtrtb=eJyrrgUAAXUA%2BQ%3D%3D;Path=/;Domain=.lijit.com;Expires=Thu,
01-Jan-1970 00:00:00 GMT
Set-Cookie:
lijit_retarget=eJyrrgUAAXUA%2BQ%3D%3D;Path=/;Domain=.lijit.com;Expires=Thu,
01-Jan-1970 00:00:00 GMT
X-Sovrn-Pod: ap3iad3



--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/Squid-issue-of-caching-the-m3u8-file-tp4682674p4682724.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From rousskov at measurement-factory.com  Wed Jun  7 14:32:23 2017
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Wed, 7 Jun 2017 08:32:23 -0600
Subject: [squid-users] squid 3.5 ssl-bump intercept TCP_DENIED/200 on
 bridge mode
In-Reply-To: <1496828262040-4682712.post@n4.nabble.com>
References: <1496828262040-4682712.post@n4.nabble.com>
Message-ID: <f36e4969-a9d4-8e01-b99f-362d84e53e02@measurement-factory.com>

On 06/07/2017 03:37 AM, Jason Chiu wrote:

> 1495699856.074      0 192.168.95.81 TCP_DENIED/200 0 CONNECT 127.0.0.1:3129

> *Need to adjust which part of the settings?*

If that connection is really trying to connect to 127.0.0.1:3129 from
Squid point of view, then your interception setup is probably deficient.
Intercepted to-port 443 connections should be seen by Squid as going to
port 443 (while being received at Squid port 3129). Interception is not
(or should not be) just port redirection. This has nothing to do with
Squid configuration though.

Once you fix interception (or if you refuse to fix it), if Squid is
denying access, then you should adjust your http_access rules. Your
rules must allow fake CONNECT request that represent intercepted HTTPS
connections. For example, the above TCP_DENIED line is probably logged
because your current interception setup triggers this (correct) rule:

> # Deny CONNECT to other than secure SSL ports
> http_access deny CONNECT !SSL_ports


And yes, it might have "worked" in the past because earlier Squids were
doing fewer checks that they should be doing.

Alex.


> #
> # Recommended minimum Access Permission configuration:
> #
> # Deny requests to certain unsafe ports
> http_access deny !Safe_ports
> 
> # Deny CONNECT to other than secure SSL ports
> http_access deny CONNECT !SSL_ports
> 
> # Only allow cachemgr access from localhost
> http_access allow localhost manager
> http_access deny manager
> 
> # We strongly recommend the following be uncommented to protect innocent
> # web applications running on the proxy server who think the only
> # one who can access services on "localhost" is a local user
> #http_access deny to_localhost
> 
> #
> # INSERT YOUR OWN RULE(S) HERE TO ALLOW ACCESS FROM YOUR CLIENTS
> #
> 
> # Example rule allowing access from your local networks.
> # Adapt localnet in the ACL section to list your (internal) IP networks
> # from where browsing should be allowed
> http_access allow localnet
> http_access allow localhost
> 
> # And finally deny all other access to this proxy
> http_access deny all
> 
> # Squid normally listens to port 3128
> http_port 3128
> 
> # Uncomment and adjust the following to add a disk cache directory.
> #cache_dir ufs /var/squid/cache/squid 100 16 256
> 
> # Leave coredumps in the first cache dir
> coredump_dir /var/squid/cache/squid
> 
> #
> # Add any of your own refresh_pattern entries above these.
> #
> refresh_pattern ^ftp:           1440    20%     10080
> refresh_pattern ^gopher:        1440    0%      1440
> refresh_pattern -i (/cgi-bin/|\?) 0     0%      0
> refresh_pattern .               0       20%     4320
> 
> #http_port 3129 ssl-bump cert=/usr/local/squid/ssl_cert/myCA.pem
> generate-host-certificates=on dynamic_cert_mem_cache_size=4MB
> https_port 3129 intercept ssl-bump cert=/usr/local/squid/ssl_cert/myCA.pem
> generate-host-certificates=on dynamic_cert_mem_cache_size=4MB
> 
> acl step1 at_step SslBump1
> acl step2 at_step SslBump2
> acl step3 at_step SslBump3
> ssl_bump peek step1 all
> ssl_bump bump all
> 
> # sslcrtd
> sslcrtd_program /usr/local/squid/libexec/ssl_crtd -s /var/squid/ssl_db -M
> 10MB
> sslcrtd_children 5
> 
> # sslproxy setting
> sslproxy_capath /var/squid/ssl_db/certs
> sslproxy_options NO_SSLv2,NO_SSLv3,NO_TLSv1,SINGLE_DH_USE,SINGLE_ECDH_USE
> #sslproxy_cipher
> EECDH+ECDSA+AESGCM:EECDH+aRSA+AESGCM:EECDH+ECDSA+SHA384:EECDH+ECDSA+SHA256:EECDH+aRSA+SHA384:EECDH+aRSA+SHA256:EECDH+aRSA+RC4:EECDH:EDH+aRSA:!RC4:!aNULL:!eNULL:!LOW:!3DES:!MD5:!EXP:!PSK:!SRP:!DSS
> sslproxy_cert_error allow all
> sslproxy_cert_adapt setValidAfter all
> 
> sslproxy_flags DONT_VERIFY_PEER
> 
> ----------------------------------------
> pf.conf
> ---------------------------------------
> #internal interface
> int_if = '{em1}'
> 
> # Normalization: reassemble fragments resolve or reduce traffic ambiguities.
> scrub in all
> set skip on lo0
> 
> #sslTP rdr setting
> rdr_from = 'any'
> rdr_to = 'any;
> rdr on $int_if inet proto tcp from $rdr_from to $rdr_to port 443 ->
> 127.0.0.1 port 3129
> pass in all no state
> pass out all no state
> pass in quick on $int_if route-to lo0 inet proto tcp from $rdr_from to any
> keep state
> 
> 
> 
> --
> View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/squid-3-5-ssl-bump-intercept-TCP-DENIED-200-on-bridge-mode-tp4682712.html
> Sent from the Squid - Users mailing list archive at Nabble.com.
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
> 



From squid3 at treenet.co.nz  Wed Jun  7 15:11:50 2017
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 8 Jun 2017 03:11:50 +1200
Subject: [squid-users] Squid issue of caching the m3u8 file
In-Reply-To: <1496844276813-4682724.post@n4.nabble.com>
References: <CAEip7fzdjxuchvn3YUrdh1jnXOvfn=bUqkWLko_X_-rVq2Wn5Q@mail.gmail.com>
 <CAEip7fwgrViUQA2vCM4Q5BXE5HmhKS373o=FV=zToE9HLw9uuw@mail.gmail.com>
 <eb6634a4-7ebf-4276-bcc4-0ae9dca50a39@treenet.co.nz>
 <1496828545709-4682714.post@n4.nabble.com>
 <3ed7b1ec-79ba-222c-1413-6ee71836ddc3@treenet.co.nz>
 <1496840437112-4682722.post@n4.nabble.com>
 <7f54b83e-4188-21c3-50ae-e1c2a8dec421@treenet.co.nz>
 <1496844276813-4682724.post@n4.nabble.com>
Message-ID: <12696ed4-6725-83dd-8d5c-67ca31f66614@treenet.co.nz>

On 08/06/17 02:04, joseph wrote:
> right
>
> lets say i have obj  with this header
> if server  send the same  obj  with different Set-Cookie value that will be
> MISS i was refairing to this
> not to the vary if it has cookie sorry if i did not explain it correctly

Cookie(s) have nothing to do with MISS unless it is listed in Vary.

The fact that the object is coming from a server without involving any 
object in the proxies own cache is what makes a transaction be 
classified as MISS.

By comparison; things that involve both an object in the proxies cache 
and a server are REFRESH, and things that _only_ involve the proxy cache 
are HIT.

Amos



From alex.delgado at crg.eu  Thu Jun  8 07:51:03 2017
From: alex.delgado at crg.eu (Alejandro Delgado Moreno)
Date: Thu, 8 Jun 2017 07:51:03 +0000
Subject: [squid-users] Cache peer help
In-Reply-To: <5a9bc1c6-cbce-a4d7-907f-719ad1d5b827@treenet.co.nz>
References: <c84b8794fa3e4b118d128a687b7047d6@UPF-BORN-MBX.crg.es>
 <a7ea26a7-6f65-c68b-8fd2-5b0f1b9b1c29@treenet.co.nz>
 <c898a3a31c0d496a91d7e199431d9302@UPF-BORN-MBX.crg.es>
 <5a9bc1c6-cbce-a4d7-907f-719ad1d5b827@treenet.co.nz>
Message-ID: <65c3699721d44d77bec274dae9c00b93@UPF-BORN-MBX.crg.es>

Hi Amos,

Here is the squid.conf file:

acl localnet src 172.16.0.0/16

acl SSL_ports port 443
acl Safe_ports port 80          # http
acl Safe_ports port 21          # ftp
acl Safe_ports port 443         # https
acl Safe_ports port 70          # gopher
acl Safe_ports port 210         # wais
acl Safe_ports port 1025-65535  # unregistered ports
acl Safe_ports port 280         # http-mgmt
acl Safe_ports port 488         # gss-http
acl Safe_ports port 591         # filemaker
acl Safe_ports port 777         # multiling http
acl CONNECT method CONNECT


acl journals dstdomain "/etc/squid/UPF_LIST.txt"

cache_peer proxy-inst.upf.edu parent 9090 0 no-query no-digest default

cache_peer_access proxy-inst.upf.edu allow journals
always_direct allow journals


# Deny requests to certain unsafe ports
http_access deny !Safe_ports

# Deny CONNECT to other than secure SSL ports
http_access deny CONNECT !SSL_ports

# Only allow cachemgr access from localhost
http_access allow localhost manager
http_access deny manager

# INSERT YOUR OWN RULE(S) HERE TO ALLOW ACCESS FROM YOUR CLIENTS

# from where browsing should be allowed
http_access allow localnet
http_access allow localhost

# And finally deny all other access to this proxy
http_access deny all

# Squid normally listens to port 3128
http_port 8881

coredump_dir /var/spool/squid

# Add any of your own refresh_pattern entries above these.
refresh_pattern ^ftp:           1440    20%     10080
refresh_pattern ^gopher:        1440    0%      1440
refresh_pattern -i (/cgi-bin/|\?) 0     0%      0
refresh_pattern .               0       20%     4320


And this is an extract of the log:

[Thu Jun  8 09:47:15 2017].269     57 172.18.2.45 TCP_MISS/200 874 POST http://clients1.google.com/ocsp - HIER_DIRECT/216.58.204.142 application/ocsp-response
[Thu Jun  8 09:47:16 2017].128     57 172.18.2.45 TCP_MISS/200 874 POST http://clients1.google.com/ocsp - HIER_DIRECT/216.58.204.142 application/ocsp-response
[Thu Jun  8 09:47:16 2017].331     56 172.18.2.45 TCP_MISS/200 874 POST http://clients1.google.com/ocsp - HIER_DIRECT/216.58.204.142 application/ocsp-response
[Thu Jun  8 09:47:20 2017].258    111 172.18.2.45 TCP_MISS/200 967 POST http://ocsp.usertrust.com/ - HIER_DIRECT/178.255.83.1 application/ocsp-response
[Thu Jun  8 09:47:21 2017].250     56 172.18.2.45 TCP_MISS/200 874 POST http://clients1.google.com/ocsp - HIER_DIRECT/216.58.204.142 application/ocsp-response
[Thu Jun  8 09:47:21 2017].459     47 172.18.2.45 TCP_MISS/200 924 POST http://ocsp.digicert.com/ - HIER_DIRECT/93.184.220.29 application/ocsp-response
[Thu Jun  8 09:47:23 2017].744    185 172.18.2.45 TCP_MISS/302 615 GET http://wos.fecyt.es/ - HIER_DIRECT/185.79.129.106 text/html
[Thu Jun  8 09:47:24 2017].005    104 172.18.2.45 TCP_MISS/200 2067 POST http://ss.symcd.com/ - HIER_DIRECT/23.37.171.27 application/ocsp-response
[Thu Jun  8 09:47:25 2017].902   5105 172.18.2.45 TCP_TUNNEL/200 5792 CONNECT www.recursoscientificos.fecyt.es:443 - HIER_DIRECT/185.79.129.106 -
[Thu Jun  8 09:47:27 2017].980     65 172.18.2.45 TCP_MISS/200 924 POST http://ocsp.digicert.com/ - HIER_DIRECT/93.184.220.29 application/ocsp-response
[Thu Jun  8 09:47:28 2017].394    211 172.18.2.45 TCP_MISS/200 488 GET http://detectportal.firefox.com/success.txt - HIER_DIRECT/88.221.254.202 text/plain
[Thu Jun  8 09:47:28 2017].786     46 172.18.2.45 TCP_MISS/200 924 POST http://ocsp.digicert.com/ - HIER_DIRECT/93.184.220.29 application/ocsp-response
[Thu Jun  8 09:47:28 2017].809   8785 172.18.2.45 TCP_TUNNEL/200 54093 CONNECT www.recursoscientificos.fecyt.es:443 - HIER_DIRECT/185.79.129.106 -
[Thu Jun  8 09:47:30 2017].094   5079 172.18.2.45 TCP_TUNNEL/200 333 CONNECT idp.fecyt.es:443 - HIER_DIRECT/185.79.129.106 -
[Thu Jun  8 09:47:30 2017].094   5079 172.18.2.45 TCP_TUNNEL/200 331 CONNECT idp.fecyt.es:443 - HIER_DIRECT/185.79.129.106 -
[Thu Jun  8 09:47:30 2017].120   5106 172.18.2.45 TCP_TUNNEL/200 331 CONNECT idp.fecyt.es:443 - HIER_DIRECT/185.79.129.106 -
[Thu Jun  8 09:47:30 2017].144   5130 172.18.2.45 TCP_TUNNEL/200 332 CONNECT idp.fecyt.es:443 - HIER_DIRECT/185.79.129.106 -
[Thu Jun  8 09:47:30 2017].147   5133 172.18.2.45 TCP_TUNNEL/200 333 CONNECT idp.fecyt.es:443 - HIER_DIRECT/185.79.129.106 -
[Thu Jun  8 09:47:30 2017].374   6567 172.18.2.45 TCP_TUNNEL/200 108115 CONNECT idp.fecyt.es:443 - HIER_DIRECT/185.79.129.106 -

As you can see, always is going direct, but when going to idp.fecyt.es should be going through the peer, as the file UPF_LIST.txt has:

https://idp.fecyt.es
https://idp.fecyt.es/
https://idp.fecyt.es/*
 
among other lines.

Regards,

-----Original Message-----
From: squid-users [mailto:squid-users-bounces at lists.squid-cache.org] On Behalf Of Amos Jeffries
Sent: martes, 6 de junio de 2017 18:18
To: squid-users at lists.squid-cache.org
Subject: Re: [squid-users] Cache peer help

On 07/06/17 02:24, Alejandro Delgado Moreno wrote:
> Sorry for this mistake,
>
> It's:
>
> acl journals dstdomain "/etc/squid/xx_LIST.txt"
>
>   cache_peer xxx.xxx.xxx.xxx parent 9090 0 no-query no-digest default
>
>   cache_peer_access xxx.xxx.xxx.xxx allow journals
>
> and it's the same, in both lines.

Okay then the issue is something else, those lines in isolation are correct for allowing traffic to use that peer, but there are many other things that may make other routes either required or preferred.

So what is the rest of your squid.conf and can you provide a sample of the access.log for the traffic going wrong?

Amos

_______________________________________________
squid-users mailing list
squid-users at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-users

From alex.delgado at crg.eu  Thu Jun  8 09:31:19 2017
From: alex.delgado at crg.eu (Alejandro Delgado Moreno)
Date: Thu, 8 Jun 2017 09:31:19 +0000
Subject: [squid-users]  Cache peer help
References: <c84b8794fa3e4b118d128a687b7047d6@UPF-BORN-MBX.crg.es>
 <a7ea26a7-6f65-c68b-8fd2-5b0f1b9b1c29@treenet.co.nz>
 <c898a3a31c0d496a91d7e199431d9302@UPF-BORN-MBX.crg.es>
 <5a9bc1c6-cbce-a4d7-907f-719ad1d5b827@treenet.co.nz> 
Message-ID: <f443203fa9d84e47893fed87001bcd20@UPF-BORN-MBX.crg.es>

Hi Amos,

Here is the squid.conf file:

acl localnet src 172.16.0.0/16

acl SSL_ports port 443
acl Safe_ports port 80          # http
acl Safe_ports port 21          # ftp
acl Safe_ports port 443         # https
acl Safe_ports port 70          # gopher
acl Safe_ports port 210         # wais
acl Safe_ports port 1025-65535  # unregistered ports
acl Safe_ports port 280         # http-mgmt
acl Safe_ports port 488         # gss-http
acl Safe_ports port 591         # filemaker
acl Safe_ports port 777         # multiling http
acl CONNECT method CONNECT


acl journals dstdomain "/etc/squid/UPF_LIST.txt"

cache_peer proxy-inst.upf.edu parent 9090 0 no-query no-digest default

cache_peer_access proxy-inst.upf.edu allow journals always_direct allow journals


# Deny requests to certain unsafe ports
http_access deny !Safe_ports

# Deny CONNECT to other than secure SSL ports http_access deny CONNECT !SSL_ports

# Only allow cachemgr access from localhost http_access allow localhost manager http_access deny manager

# INSERT YOUR OWN RULE(S) HERE TO ALLOW ACCESS FROM YOUR CLIENTS

# from where browsing should be allowed
http_access allow localnet
http_access allow localhost

# And finally deny all other access to this proxy http_access deny all

# Squid normally listens to port 3128
http_port 8881

coredump_dir /var/spool/squid

# Add any of your own refresh_pattern entries above these.
refresh_pattern ^ftp:           1440    20%     10080
refresh_pattern ^gopher:        1440    0%      1440
refresh_pattern -i (/cgi-bin/|\?) 0     0%      0
refresh_pattern .               0       20%     4320


And this is an extract of the log:

[Thu Jun  8 09:47:15 2017].269     57 172.18.2.45 TCP_MISS/200 874 POST http://clients1.google.com/ocsp - HIER_DIRECT/216.58.204.142 application/ocsp-response
[Thu Jun  8 09:47:16 2017].128     57 172.18.2.45 TCP_MISS/200 874 POST http://clients1.google.com/ocsp - HIER_DIRECT/216.58.204.142 application/ocsp-response
[Thu Jun  8 09:47:16 2017].331     56 172.18.2.45 TCP_MISS/200 874 POST http://clients1.google.com/ocsp - HIER_DIRECT/216.58.204.142 application/ocsp-response
[Thu Jun  8 09:47:20 2017].258    111 172.18.2.45 TCP_MISS/200 967 POST http://ocsp.usertrust.com/ - HIER_DIRECT/178.255.83.1 application/ocsp-response
[Thu Jun  8 09:47:21 2017].250     56 172.18.2.45 TCP_MISS/200 874 POST http://clients1.google.com/ocsp - HIER_DIRECT/216.58.204.142 application/ocsp-response
[Thu Jun  8 09:47:21 2017].459     47 172.18.2.45 TCP_MISS/200 924 POST http://ocsp.digicert.com/ - HIER_DIRECT/93.184.220.29 application/ocsp-response
[Thu Jun  8 09:47:23 2017].744    185 172.18.2.45 TCP_MISS/302 615 GET http://wos.fecyt.es/ - HIER_DIRECT/185.79.129.106 text/html
[Thu Jun  8 09:47:24 2017].005    104 172.18.2.45 TCP_MISS/200 2067 POST http://ss.symcd.com/ - HIER_DIRECT/23.37.171.27 application/ocsp-response
[Thu Jun  8 09:47:25 2017].902   5105 172.18.2.45 TCP_TUNNEL/200 5792 CONNECT www.recursoscientificos.fecyt.es:443 - HIER_DIRECT/185.79.129.106 -
[Thu Jun  8 09:47:27 2017].980     65 172.18.2.45 TCP_MISS/200 924 POST http://ocsp.digicert.com/ - HIER_DIRECT/93.184.220.29 application/ocsp-response
[Thu Jun  8 09:47:28 2017].394    211 172.18.2.45 TCP_MISS/200 488 GET http://detectportal.firefox.com/success.txt - HIER_DIRECT/88.221.254.202 text/plain
[Thu Jun  8 09:47:28 2017].786     46 172.18.2.45 TCP_MISS/200 924 POST http://ocsp.digicert.com/ - HIER_DIRECT/93.184.220.29 application/ocsp-response
[Thu Jun  8 09:47:28 2017].809   8785 172.18.2.45 TCP_TUNNEL/200 54093 CONNECT www.recursoscientificos.fecyt.es:443 - HIER_DIRECT/185.79.129.106 -
[Thu Jun  8 09:47:30 2017].094   5079 172.18.2.45 TCP_TUNNEL/200 333 CONNECT idp.fecyt.es:443 - HIER_DIRECT/185.79.129.106 -
[Thu Jun  8 09:47:30 2017].094   5079 172.18.2.45 TCP_TUNNEL/200 331 CONNECT idp.fecyt.es:443 - HIER_DIRECT/185.79.129.106 -
[Thu Jun  8 09:47:30 2017].120   5106 172.18.2.45 TCP_TUNNEL/200 331 CONNECT idp.fecyt.es:443 - HIER_DIRECT/185.79.129.106 -
[Thu Jun  8 09:47:30 2017].144   5130 172.18.2.45 TCP_TUNNEL/200 332 CONNECT idp.fecyt.es:443 - HIER_DIRECT/185.79.129.106 -
[Thu Jun  8 09:47:30 2017].147   5133 172.18.2.45 TCP_TUNNEL/200 333 CONNECT idp.fecyt.es:443 - HIER_DIRECT/185.79.129.106 -
[Thu Jun  8 09:47:30 2017].374   6567 172.18.2.45 TCP_TUNNEL/200 108115 CONNECT idp.fecyt.es:443 - HIER_DIRECT/185.79.129.106 -

As you can see, always is going direct, but when going to idp.fecyt.es should be going through the peer, as the file UPF_LIST.txt has:

https://idp.fecyt.es
https://idp.fecyt.es/
https://idp.fecyt.es/*
 
among other lines.

Regards,

-----Original Message-----
From: squid-users [mailto:squid-users-bounces at lists.squid-cache.org] On Behalf Of Amos Jeffries
Sent: martes, 6 de junio de 2017 18:18
To: squid-users at lists.squid-cache.org
Subject: Re: [squid-users] Cache peer help

On 07/06/17 02:24, Alejandro Delgado Moreno wrote:
> Sorry for this mistake,
>
> It's:
>
> acl journals dstdomain "/etc/squid/xx_LIST.txt"
>
>   cache_peer xxx.xxx.xxx.xxx parent 9090 0 no-query no-digest default
>
>   cache_peer_access xxx.xxx.xxx.xxx allow journals
>
> and it's the same, in both lines.

Okay then the issue is something else, those lines in isolation are correct for allowing traffic to use that peer, but there are many other things that may make other routes either required or preferred.

So what is the rest of your squid.conf and can you provide a sample of the access.log for the traffic going wrong?

Amos

_______________________________________________
squid-users mailing list
squid-users at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-users

From squid3 at treenet.co.nz  Thu Jun  8 10:55:00 2017
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 8 Jun 2017 22:55:00 +1200
Subject: [squid-users] Cache peer help
In-Reply-To: <65c3699721d44d77bec274dae9c00b93@UPF-BORN-MBX.crg.es>
References: <c84b8794fa3e4b118d128a687b7047d6@UPF-BORN-MBX.crg.es>
 <a7ea26a7-6f65-c68b-8fd2-5b0f1b9b1c29@treenet.co.nz>
 <c898a3a31c0d496a91d7e199431d9302@UPF-BORN-MBX.crg.es>
 <5a9bc1c6-cbce-a4d7-907f-719ad1d5b827@treenet.co.nz>
 <65c3699721d44d77bec274dae9c00b93@UPF-BORN-MBX.crg.es>
Message-ID: <b71539b0-0833-3d63-ef7a-b05493e4f9e3@treenet.co.nz>

On 08/06/17 19:51, Alejandro Delgado Moreno wrote:
> Hi Amos,
>
> Here is the squid.conf file:
>
> acl localnet src 172.16.0.0/16
>
> acl SSL_ports port 443
> acl Safe_ports port 80          # http
> acl Safe_ports port 21          # ftp
> acl Safe_ports port 443         # https
> acl Safe_ports port 70          # gopher
> acl Safe_ports port 210         # wais
> acl Safe_ports port 1025-65535  # unregistered ports
> acl Safe_ports port 280         # http-mgmt
> acl Safe_ports port 488         # gss-http
> acl Safe_ports port 591         # filemaker
> acl Safe_ports port 777         # multiling http
> acl CONNECT method CONNECT
>
>
> acl journals dstdomain "/etc/squid/UPF_LIST.txt"
>
> cache_peer proxy-inst.upf.edu parent 9090 0 no-query no-digest default
>
> cache_peer_access proxy-inst.upf.edu allow journals
> always_direct allow journals

There you go. Problem #1:  "always_direct allow" prohibits any 
cache_peer being used by that request (by requiring that DIRECT be used, 
mandatory). Remove that and some of the journal traffic will start going 
to the peer.

> And this is an extract of the log:
>
> [Thu Jun  8 09:47:30 2017].094   5079 172.18.2.45 TCP_TUNNEL/200 333 CONNECT idp.fecyt.es:443 - HIER_DIRECT/185.79.129.106 -
> [Thu Jun  8 09:47:30 2017].094   5079 172.18.2.45 TCP_TUNNEL/200 331 CONNECT idp.fecyt.es:443 - HIER_DIRECT/185.79.129.106 -
> [Thu Jun  8 09:47:30 2017].120   5106 172.18.2.45 TCP_TUNNEL/200 331 CONNECT idp.fecyt.es:443 - HIER_DIRECT/185.79.129.106 -
> [Thu Jun  8 09:47:30 2017].144   5130 172.18.2.45 TCP_TUNNEL/200 332 CONNECT idp.fecyt.es:443 - HIER_DIRECT/185.79.129.106 -
> [Thu Jun  8 09:47:30 2017].147   5133 172.18.2.45 TCP_TUNNEL/200 333 CONNECT idp.fecyt.es:443 - HIER_DIRECT/185.79.129.106 -
> [Thu Jun  8 09:47:30 2017].374   6567 172.18.2.45 TCP_TUNNEL/200 108115 CONNECT idp.fecyt.es:443 - HIER_DIRECT/185.79.129.106 -

CONNECT and a few other things are normally sent DIRECT because that is 
way faster than doing another hop.

To make those prefer going through the peer add this line:

   nonhierarchical_direct off

And if that is not enough, you can add "never_direct allow journals" to 
forbid DIRECT being used. They will then fail completely if the peer is 
not used for any reason.


> As you can see, always is going direct, but when going to idp.fecyt.es should be going through the peer, as the file UPF_LIST.txt has:
>
> https://idp.fecyt.es
> https://idp.fecyt.es/
> https://idp.fecyt.es/*

Your squid.conf said these were being loaded into a dstdomain ACL. But 
the above lines are URLs, not domain names.

dstdomain syntax is a domain name with maybe a wildcard to match all 
sub-domains. see 
<http://wiki.squid-cache.org/SquidFaq/SquidAcl#Squid_doesn.27t_match_my_subdomains>


HTH
Amos



From eliezer at ngtech.co.il  Fri Jun  9 03:43:49 2017
From: eliezer at ngtech.co.il (Eliezer  Croitoru)
Date: Fri, 9 Jun 2017 06:43:49 +0300
Subject: [squid-users] A youtube acl tool sketch
Message-ID: <001f01d2e0d2$9adbb650$d09322f0$@ngtech.co.il>

I have been working on couple tools that will help to filter content on
youtube.
If you are not an education facility you don't have the ability to manage
youtube content or force some policy on your users.
The basic issue is that you don't want any video to be seen by your users.
We can start categorizing and build a DB that will contain a black and
whitelilst but I have written a sketch for a tool that will help to build a
squid external_acl helper to acl youtube videos access.
The tool is at:
https://gist.github.com/elico/cbda8a6918cb71918616d39b560c90d8

It's BSD licensed and free for all.

There is a cgi script in ruby that can be patched to response with a pretty
json output and help external tools to use it as an API. There is also an
example in GoLang which receives a videoid argument and returns the channel
or the user which it belongs to.

Let say you are a business and you want to allow access to youtube videos
which was published by mikrotik or another company or a specific tutorial
maker but not news or other distracting things you can build an external_acl
helper based on this.

If someone is interested that  will complete this tool to work with a
specific DB or a specific text file that will be the whitelist and all the
others are banned, just contact me and with hope that I will schedule it to
the next squid release.

I am now working on the article for the release of squid 3.5.26 and 4.0.20
RPM's
(The RPM's are  out..)

Eliezer

----
Eliezer Croitoru
Linux System Administrator
Mobile: +972-5-28704261
Email: eliezer at ngtech.co.il





From f6253283 at hotmail.com  Fri Jun  9 03:43:15 2017
From: f6253283 at hotmail.com (Jason Chiu)
Date: Thu, 8 Jun 2017 20:43:15 -0700 (PDT)
Subject: [squid-users] squid 3.5 ssl-bump intercept TCP_DENIED/200 on
	bridge mode
In-Reply-To: <f36e4969-a9d4-8e01-b99f-362d84e53e02@measurement-factory.com>
References: <1496828262040-4682712.post@n4.nabble.com>
 <f36e4969-a9d4-8e01-b99f-362d84e53e02@measurement-factory.com>
Message-ID: <1496979795835-4682734.post@n4.nabble.com>

I also tested the following cases 
test case 1: 

add the following settings in squid.conf 

acl bumpedPorts myportname 3129 
http_access allow CONNECT bumpedPorts 

test results:  ssl bump is failed 
1. access.log no record 
2. web browser has been waiting , no response 

---------------------------------------------- 

test case 2: 
1. squid.conf  use  http_port 3129 ssl-bump
cert=/usr/local/squid/ssl_cert/myCA.pem generate-host-certificates=on
dynamic_cert_mem_cache_size=4MB 
2. web browser use proxy server x.x.x.x 3129 

test result :  ssl bump is OK



--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/squid-3-5-ssl-bump-intercept-TCP-DENIED-200-on-bridge-mode-tp4682712p4682734.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From f6253283 at hotmail.com  Fri Jun  9 04:05:33 2017
From: f6253283 at hotmail.com (Jason Chiu)
Date: Thu, 8 Jun 2017 21:05:33 -0700 (PDT)
Subject: [squid-users] squid 3.5 ssl-bump intercept TCP_DENIED/200 on
	bridge mode
In-Reply-To: <f36e4969-a9d4-8e01-b99f-362d84e53e02@measurement-factory.com>
References: <1496828262040-4682712.post@n4.nabble.com>
 <f36e4969-a9d4-8e01-b99f-362d84e53e02@measurement-factory.com>
Message-ID: <1496981133148-4682735.post@n4.nabble.com>

test case 1 : 
-----------------------------------------
I changed my squid setting (don't use intercept mode)

http_port 3129 ssl-bump cert=/usr/local/squid/ssl_cert/myCA.pem
generate-host-certificates=on dynamic_cert_mem_cache_size=4MB

thab client Web Browser set proxy to 192.168.95.81:3129

squid ssl-bump * OK *
squid access.log has the client access log.

test case 2:
-----------------------------------------
but I want use transparent mode (intercept with PF rdr).
intercept mode add the following acl rule :

acl bumpedPorts myportname 3129
http_access allow CONNECT bumpedPorts
.....
https_port 3129 intercept ssl-bump cert=/usr/local/squid/ssl_cert/myCA.pem
generate-host-certificates=on dynamic_cert_mem_cache_size=4MB 

access.log no appear TCP_DENIED/200 0 CONNECT 127.0.0.1:3129 
but client web browser has been waiting and no response.





--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/squid-3-5-ssl-bump-intercept-TCP-DENIED-200-on-bridge-mode-tp4682712p4682735.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From fredbmail at free.fr  Fri Jun  9 07:33:27 2017
From: fredbmail at free.fr (FredB)
Date: Fri, 9 Jun 2017 09:33:27 +0200 (CEST)
Subject: [squid-users] Squid and SSLBump
In-Reply-To: <1921200353.147344029.1496992798130.JavaMail.root@zimbra4-e1>
Message-ID: <1466175444.147389827.1496993607646.JavaMail.root@zimbra4-e1>

Hi all,

There is way to approximately estimate the "cost" of CPU/Memory usage of SSLbump ?
What do you see in practice ? 
Some features are incompatibles with SMP so I'm using a single process, Squid is using more or less 30/40 % of CPU

I have approximately 1000 users simultaneously connected 
Squid 3.5.25

Fred


From innovature.arun.xavier at gmail.com  Fri Jun  9 07:39:37 2017
From: innovature.arun.xavier at gmail.com (Arun Xavier)
Date: Fri, 09 Jun 2017 07:39:37 +0000
Subject: [squid-users] Unable log log mac address in Server
Message-ID: <CAAU89bT7xhiDCUhJEoB23a_M4fYyoOgbFvkGrP7hMRozY69Q0A@mail.gmail.com>

Hi all,

I set up squid in 2 different environments.

One in my local network and One in aws ec2.

I am getting device mac address in local network, but I
get 00:00:00:00:00:00 in aws.

Is it a network issue ? or Is does this works this way?

Squid Version Details:

squid -v

Squid Cache: Version 4.0.19-20170508-r15031
Service Name: squid
Ubuntu linux
configure options:  '--prefix=/usr' '--localstatedir=/var'
'--datadir=/usr/share/squid' '--sysconfdir=/etc/squid'
'--libexecdir=/usr/lib/squid' '--with-swapdir=/var/spool/squid'
'--with-logdir=/var/log/squid' '--with-pidfile=/var/run/squid.pid'
'--with-default-user=proxy' '--enable-inline' '--enable-delay-pools'
'--enable-cache-digests' '--enable-icap-client'
'--enable-follow-x-forwarded-for' '--enable-eui' '--enable-esi'
'--enable-icmp' '--enable-zph-qos' '--enable-ecap' '--disable-translation'
'--with-filedescriptors=65536' '--with-large-files' '--with-openssl'
'--enable-ssl' '--enable-ssl-crtd' '--enable-build-info=Ubuntu linux'
'--enable-linux-netfilter'

Differences in Configuration file:

acl allnetwork src 0.0.0.0/0
http_access allow allnetwork
ssl_bump bump all
always_direct allow all
http_port 3128 ssl-bump generate-host-certificates=on
cert=/etc/squid/cert/cert.pem key=/etc/squid/cert/key.pem
strip_query_terms off
logformat squid %{%Y-%m-%d,%H:%M:%S}tl.%03tu %6tr %>a %Ss/%03>Hs %<st %rm
%ru %[un %Sh/%<a %mt %>eui


I have same configurations in both aws and local, but squid in aws fails to
log mac addres.

What might be the reason ?

Regards
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20170609/44b7bfe2/attachment.htm>

From flashdown at data-core.org  Fri Jun  9 11:08:11 2017
From: flashdown at data-core.org (Flashdown)
Date: Fri, 09 Jun 2017 13:08:11 +0200
Subject: [squid-users] Unable log log mac address in Server
In-Reply-To: <CAAU89bT7xhiDCUhJEoB23a_M4fYyoOgbFvkGrP7hMRozY69Q0A@mail.gmail.com>
References: <CAAU89bT7xhiDCUhJEoB23a_M4fYyoOgbFvkGrP7hMRozY69Q0A@mail.gmail.com>
Message-ID: <84055CEA-3429-4F25-8BDD-96610A050C03@data-core.org>

Hi Xavier,

it is a normal network behavior. The reason is we might believe that we use IP's in the local network. That of course is true, but in the local network the real end to end communication is done from & to MAC address (OSI Layer 2) when a IP-packet (layer 3) leaves the local network then target and source mac are unknown to the client and server. So they only know the IP's.

Am 9. Juni 2017 09:39:37 MESZ schrieb Arun Xavier <innovature.arun.xavier at gmail.com>:
>Hi all,
>
>I set up squid in 2 different environments.
>
>One in my local network and One in aws ec2.
>
>I am getting device mac address in local network, but I
>get 00:00:00:00:00:00 in aws.
>
>Is it a network issue ? or Is does this works this way?
>
>Squid Version Details:
>
>squid -v
>
>Squid Cache: Version 4.0.19-20170508-r15031
>Service Name: squid
>Ubuntu linux
>configure options:  '--prefix=/usr' '--localstatedir=/var'
>'--datadir=/usr/share/squid' '--sysconfdir=/etc/squid'
>'--libexecdir=/usr/lib/squid' '--with-swapdir=/var/spool/squid'
>'--with-logdir=/var/log/squid' '--with-pidfile=/var/run/squid.pid'
>'--with-default-user=proxy' '--enable-inline' '--enable-delay-pools'
>'--enable-cache-digests' '--enable-icap-client'
>'--enable-follow-x-forwarded-for' '--enable-eui' '--enable-esi'
>'--enable-icmp' '--enable-zph-qos' '--enable-ecap'
>'--disable-translation'
>'--with-filedescriptors=65536' '--with-large-files' '--with-openssl'
>'--enable-ssl' '--enable-ssl-crtd' '--enable-build-info=Ubuntu linux'
>'--enable-linux-netfilter'
>
>Differences in Configuration file:
>
>acl allnetwork src 0.0.0.0/0
>http_access allow allnetwork
>ssl_bump bump all
>always_direct allow all
>http_port 3128 ssl-bump generate-host-certificates=on
>cert=/etc/squid/cert/cert.pem key=/etc/squid/cert/key.pem
>strip_query_terms off
>logformat squid %{%Y-%m-%d,%H:%M:%S}tl.%03tu %6tr %>a %Ss/%03>Hs %<st
>%rm
>%ru %[un %Sh/%<a %mt %>eui
>
>
>I have same configurations in both aws and local, but squid in aws
>fails to
>log mac addres.
>
>What might be the reason ?
>
>Regards
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20170609/bc57c388/attachment.htm>

From squid3 at treenet.co.nz  Fri Jun  9 11:10:35 2017
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Fri, 9 Jun 2017 23:10:35 +1200
Subject: [squid-users] Unable log log mac address in Server
In-Reply-To: <CAAU89bT7xhiDCUhJEoB23a_M4fYyoOgbFvkGrP7hMRozY69Q0A@mail.gmail.com>
References: <CAAU89bT7xhiDCUhJEoB23a_M4fYyoOgbFvkGrP7hMRozY69Q0A@mail.gmail.com>
Message-ID: <c8fe5240-809f-66bd-9a31-f490964dd8b6@treenet.co.nz>

On 09/06/17 19:39, Arun Xavier wrote:
> Hi all,
>
> I set up squid in 2 different environments.
>
> One in my local network and One in aws ec2.
>
> I am getting device mac address in local network, but I 
> get 00:00:00:00:00:00 in aws.
>
> Is it a network issue ? or Is does this works this way?
>

Not sure about the answer to those. If those machines have the same OS 
then it is probably related to one being a real machine with real 
hardware and one a VM on fake hardware.


> Squid Version Details:
>
> squid -v
>
> Squid Cache: Version 4.0.19-20170508-r15031
> Service Name: squid
> Ubuntu linux
> configure options:  '--prefix=/usr' '--localstatedir=/var' 
> '--datadir=/usr/share/squid' '--sysconfdir=/etc/squid' 
> '--libexecdir=/usr/lib/squid' '--with-swapdir=/var/spool/squid' 
> '--with-logdir=/var/log/squid' '--with-pidfile=/var/run/squid.pid' 
> '--with-default-user=proxy' '--enable-inline' '--enable-delay-pools' 
> '--enable-cache-digests' '--enable-icap-client' 
> '--enable-follow-x-forwarded-for' '--enable-eui' '--enable-esi' 
> '--enable-icmp' '--enable-zph-qos' '--enable-ecap' 
> '--disable-translation' '--with-filedescriptors=65536' 
> '--with-large-files' '--with-openssl' '--enable-ssl' 
> '--enable-ssl-crtd' '--enable-build-info=Ubuntu linux' 
> '--enable-linux-netfilter'
>
> Differences in Configuration file:
>
> acl allnetwork src 0.0.0.0/0 <http://0.0.0.0/0>
> http_access allow allnetwork

Why are you calling that ACL "allnetwork" ? it matches many machines on 
the Internet - which is far more than your network. But then it only 
matches IPv4, which is not even close to 'all' sources.


> ssl_bump bump all
> always_direct allow all
> http_port 3128 ssl-bump generate-host-certificates=on 
> cert=/etc/squid/cert/cert.pem key=/etc/squid/cert/key.pem
> strip_query_terms off
> logformat squid %{%Y-%m-%d,%H:%M:%S}tl.%03tu %6tr %>a %Ss/%03>Hs %<st 
> %rm %ru %[un %Sh/%<a %mt %>eui
>
>
> I have same configurations in both aws and local, but squid in aws 
> fails to log mac addres.
>
> What might be the reason ?


Amos



From uhlar at fantomas.sk  Fri Jun  9 17:08:50 2017
From: uhlar at fantomas.sk (Matus UHLAR - fantomas)
Date: Fri, 9 Jun 2017 19:08:50 +0200
Subject: [squid-users] Unable log log mac address in Server
In-Reply-To: <c8fe5240-809f-66bd-9a31-f490964dd8b6@treenet.co.nz>
References: <CAAU89bT7xhiDCUhJEoB23a_M4fYyoOgbFvkGrP7hMRozY69Q0A@mail.gmail.com>
 <c8fe5240-809f-66bd-9a31-f490964dd8b6@treenet.co.nz>
Message-ID: <20170609170850.GB26700@fantomas.sk>

>On 09/06/17 19:39, Arun Xavier wrote:
>>I set up squid in 2 different environments.
>>
>>One in my local network and One in aws ec2.
>>
>>I am getting device mac address in local network, but I get 
>>00:00:00:00:00:00 in aws.
>>
>>Is it a network issue ? or Is does this works this way?

On 09.06.17 23:10, Amos Jeffries wrote:
>Not sure about the answer to those. If those machines have the same 
>OS then it is probably related to one being a real machine with real 
>hardware and one a VM on fake hardware.

most probably. While virtual machines in clouds do have ethernet addresses
if they have (virtual) ethernet cards, amazon apparently hides them from
each other.

-- 
Matus UHLAR - fantomas, uhlar at fantomas.sk ; http://www.fantomas.sk/
Warning: I wish NOT to receive e-mail advertising to this address.
Varovanie: na tuto adresu chcem NEDOSTAVAT akukolvek reklamnu postu.
Remember half the people you know are below average. 


From Walter.H at mathemainzel.info  Fri Jun  9 18:01:08 2017
From: Walter.H at mathemainzel.info (Walter H.)
Date: Fri, 09 Jun 2017 20:01:08 +0200
Subject: [squid-users] Squid and SSLBump
In-Reply-To: <1466175444.147389827.1496993607646.JavaMail.root@zimbra4-e1>
References: <1466175444.147389827.1496993607646.JavaMail.root@zimbra4-e1>
Message-ID: <593AE264.5080103@mathemainzel.info>

On 09.06.2017 09:33, FredB wrote:
> Hi all,
>
> There is way to approximately estimate the "cost" of CPU/Memory usage of SSLbump ?
be careful, if there is a "cost" value now, this will be very probably 
wrong when SSL gets more common ...


-------------- next part --------------
A non-text attachment was scrubbed...
Name: smime.p7s
Type: application/pkcs7-signature
Size: 3491 bytes
Desc: S/MIME Cryptographic Signature
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20170609/a41206f0/attachment.bin>

From squid3 at treenet.co.nz  Sat Jun 10 03:42:23 2017
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sat, 10 Jun 2017 15:42:23 +1200
Subject: [squid-users] Squid and SSLBump
In-Reply-To: <593AE264.5080103@mathemainzel.info>
References: <1466175444.147389827.1496993607646.JavaMail.root@zimbra4-e1>
 <593AE264.5080103@mathemainzel.info>
Message-ID: <e3c9585c-e8c9-b134-0c6a-1ab5fc68e612@treenet.co.nz>

On 10/06/17 06:01, Walter H. wrote:
> On 09.06.2017 09:33, FredB wrote:
>> Hi all,
>>
>> There is way to approximately estimate the "cost" of CPU/Memory usage 
>> of SSLbump ?
> be careful, if there is a "cost" value now, this will be very probably 
> wrong when SSL gets more common ...
>


As far as I have seen there are simply not enough published numbers for 
the SSL-Bump and HTTPS proxying to get even a good ballpark estimate 
yet. Additional info for the 
<http://wiki.squid-cache.org/KnowledgeBase/Benchmarks> page are welcome, 
how to take measurements to minimize comparison issues are listed there.

IMO, the situation is much the same with plain-text as well as Squid has 
constantly improving support for HTTP/1.1 and performance updates 
changing things there too. For any given machine you should be able to 
get your own numbers by comparing the RPS numbers to the CPU and memory 
load patterns, regardless of what the feature sets in use are.

Amos



From rousskov at measurement-factory.com  Sat Jun 10 17:20:26 2017
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Sat, 10 Jun 2017 11:20:26 -0600
Subject: [squid-users] Squid and SSLBump
In-Reply-To: <1466175444.147389827.1496993607646.JavaMail.root@zimbra4-e1>
References: <1466175444.147389827.1496993607646.JavaMail.root@zimbra4-e1>
Message-ID: <97b20b47-02c8-ac25-32fd-ba9c9a201431@measurement-factory.com>

On 06/09/2017 01:33 AM, FredB wrote:

> There is way to approximately estimate the "cost" of CPU/Memory usage of SSLbump ?

Ballpark splicing speed/CPU estimates[1,2] for Squid v4+:

  * splicing during step 2: 75% of splicing during step1 performance
  * splicing during step 3: 25% of splicing during step1 performance

Squid v3.5 numbers for splicing during step 2 are much worse (~20%)
because the SNI peeking code is not optimized in v3.5 [1].

I do not recall bumping numbers, but expect them to be approximately 10%
of baseline plain text performance.

The above info is based on lab benchmarks that do not reflect _your_
deployment environment. You can collect much more reliable performance
data for your use case by measuring your actual Squid performance while
turning features on and off (or at least by running lab benchmarks that
are tuned to represent your use case).


Please also note that there is currently no regular Squid performance
regression testing so individual releases may experience significant and
surprising changes[3]. If the Squid Foundation has enough money, the
Squid Project will fix that [4].


[1] http://lists.squid-cache.org/pipermail/squid-dev/2016-May/005659.html

[2] http://lists.squid-cache.org/pipermail/squid-dev/2016-May/005660.html

[3] http://lists.squid-cache.org/pipermail/squid-dev/2016-August/006637.html

[4] http://wiki.squid-cache.org/QA/Pilots


HTH,

Alex.


From squid3 at treenet.co.nz  Sun Jun 11 11:48:07 2017
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sun, 11 Jun 2017 23:48:07 +1200
Subject: [squid-users] squid 3.5 ssl-bump intercept TCP_DENIED/200 on
 bridge mode
In-Reply-To: <1496981133148-4682735.post@n4.nabble.com>
References: <1496828262040-4682712.post@n4.nabble.com>
 <f36e4969-a9d4-8e01-b99f-362d84e53e02@measurement-factory.com>
 <1496981133148-4682735.post@n4.nabble.com>
Message-ID: <ad28ca01-851a-fced-dc8b-af6e8ecf1e07@treenet.co.nz>

On 09/06/17 16:05, Jason Chiu wrote:
> test case 2:
> -----------------------------------------
> but I want use transparent mode (intercept with PF rdr).
> intercept mode add the following acl rule :
>
> acl bumpedPorts myportname 3129
> http_access allow CONNECT bumpedPorts
> .....
> https_port 3129 intercept ssl-bump cert=/usr/local/squid/ssl_cert/myCA.pem
> generate-host-certificates=on dynamic_cert_mem_cache_size=4MB
>
> access.log no appear TCP_DENIED/200 0 CONNECT 127.0.0.1:3129
> but client web browser has been waiting and no response.

Ah, sorry I should have remembered this earlier:
<http://www.squid-cache.org/Versions/v3/3.4/RELEASENOTES.html#ss2.4>

TL;DR:  Add --with-nat-devpf to your build options for FreeBSD.

Amos



From thesnable at gmail.com  Sun Jun 11 12:31:37 2017
From: thesnable at gmail.com (snable snable)
Date: Sun, 11 Jun 2017 14:31:37 +0200
Subject: [squid-users] telegram app on android
In-Reply-To: <CADYcWGQejzUcDBzd_5UKoqJ83hpQjshwg2XveW70pZkT=H5AFA@mail.gmail.com>
References: <CADYcWGQejzUcDBzd_5UKoqJ83hpQjshwg2XveW70pZkT=H5AFA@mail.gmail.com>
Message-ID: <CADYcWGSnNG=Wvwv2VYdQyEAa3OmVcfpWA96jHW54h_e3DVnYgw@mail.gmail.com>

hi

i get these error messages and telegram cant connect:

squid 4.0.20
bumping only specific sites


1497184119.235      1 192.168.1.200 NONE_ABORTED/200 0 CONNECT 149.154.167.
51:443 - HIER_NONE/- -
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20170611/2f32f944/attachment.htm>

From yvoinov at gmail.com  Sun Jun 11 12:33:34 2017
From: yvoinov at gmail.com (Yuri)
Date: Sun, 11 Jun 2017 18:33:34 +0600
Subject: [squid-users] telegram app on android
In-Reply-To: <CADYcWGSnNG=Wvwv2VYdQyEAa3OmVcfpWA96jHW54h_e3DVnYgw@mail.gmail.com>
References: <CADYcWGQejzUcDBzd_5UKoqJ83hpQjshwg2XveW70pZkT=H5AFA@mail.gmail.com>
 <CADYcWGSnNG=Wvwv2VYdQyEAa3OmVcfpWA96jHW54h_e3DVnYgw@mail.gmail.com>
Message-ID: <d39e4986-5391-e837-4e6c-a75810afa0bb@gmail.com>

http://wiki.squid-cache.org/ConfigExamples/Chat/Telegram


11.06.2017 18:31, snable snable ?????:
> hi 
>
> i get these error messages and telegram cant connect:
>
> squid 4.0.20
> bumping only specific sites
>
>
> 1497184119.235      1 192.168.1.200 NONE_ABORTED/200 0 CONNECT
> 149.154.167.
> 51:443 - HIER_NONE/- -
>
>
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20170611/8efc254f/attachment.htm>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: signature.asc
Type: application/pgp-signature
Size: 473 bytes
Desc: OpenPGP digital signature
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20170611/8efc254f/attachment.sig>

From yvoinov at gmail.com  Sun Jun 11 21:34:26 2017
From: yvoinov at gmail.com (Yuri)
Date: Mon, 12 Jun 2017 03:34:26 +0600
Subject: [squid-users] telegram app on android
In-Reply-To: <d39e4986-5391-e837-4e6c-a75810afa0bb@gmail.com>
References: <CADYcWGQejzUcDBzd_5UKoqJ83hpQjshwg2XveW70pZkT=H5AFA@mail.gmail.com>
 <CADYcWGSnNG=Wvwv2VYdQyEAa3OmVcfpWA96jHW54h_e3DVnYgw@mail.gmail.com>
 <d39e4986-5391-e837-4e6c-a75810afa0bb@gmail.com>
Message-ID: <757abfc1-d2eb-1a36-1d80-fb20eaa853c2@gmail.com>

Do not thank :)


11.06.2017 18:33, Yuri ?????:
>
> http://wiki.squid-cache.org/ConfigExamples/Chat/Telegram
>
>
> 11.06.2017 18:31, snable snable ?????:
>> hi 
>>
>> i get these error messages and telegram cant connect:
>>
>> squid 4.0.20
>> bumping only specific sites
>>
>>
>> 1497184119.235      1 192.168.1.200 NONE_ABORTED/200 0 CONNECT
>> 149.154.167.
>> 51:443 - HIER_NONE/- -
>>
>>
>>
>> _______________________________________________
>> squid-users mailing list
>> squid-users at lists.squid-cache.org
>> http://lists.squid-cache.org/listinfo/squid-users
>

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20170612/2cf3083c/attachment.htm>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: signature.asc
Type: application/pgp-signature
Size: 473 bytes
Desc: OpenPGP digital signature
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20170612/2cf3083c/attachment.sig>

From f6253283 at hotmail.com  Mon Jun 12 03:54:23 2017
From: f6253283 at hotmail.com (Jason Chiu)
Date: Sun, 11 Jun 2017 20:54:23 -0700 (PDT)
Subject: [squid-users] squid 3.5 ssl-bump intercept TCP_DENIED/200 on
	bridge mode
In-Reply-To: <ad28ca01-851a-fced-dc8b-af6e8ecf1e07@treenet.co.nz>
References: <1496828262040-4682712.post@n4.nabble.com>
 <f36e4969-a9d4-8e01-b99f-362d84e53e02@measurement-factory.com>
 <1496981133148-4682735.post@n4.nabble.com>
 <ad28ca01-851a-fced-dc8b-af6e8ecf1e07@treenet.co.nz>
Message-ID: <1497239663910-4682748.post@n4.nabble.com>

I reconfigured  add " --with-nat-devpf " (squid-3.5.24 on FreeBSD 9.1)

This issue *has been resolved*
thanks to Amos Jeffries

The follow is my squid version and configure.

Squid Cache: Version 3.5.24-20170331-r14150
Service Name: squid
configure options:  '--prefix=/usr/local/squid' '--sysconfdir=/etc/squid'
'--localstatedir=/var/squid' '--datadir=/usr/share/squid'
'--enable-icap-client' '--enable-ssl' '--with-pthreads'
'--enable-pf-transparent' '--with-nat-devpf' '--enable-ssl-crtd'
'--enable-ecap' '--with-openssl' 'PKG_CONFIG_PATH=/usr/local/lib/pkgconfig'
--enable-ltdl-convenience






--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/squid-3-5-ssl-bump-intercept-TCP-DENIED-200-on-bridge-mode-tp4682712p4682748.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From blason16 at gmail.com  Mon Jun 12 09:05:06 2017
From: blason16 at gmail.com (Blason R)
Date: Mon, 12 Jun 2017 14:35:06 +0530
Subject: [squid-users] Office 365 Support for Squid Proxy
Message-ID: <CAPPXLT-nXbTHdmWBQPWX-FbA0RWrJsCx54EUqcRTz=57+0_Miw@mail.gmail.com>

Hello All,

If someone can confirm if squid can very well work with Office 365? If
anyone has any documentation can someone please forward that to me? I do
have almost around 400 Office 365 users hence wanted to know what
configuration I might need for Office 365 traffic?
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20170612/5b62dd11/attachment.htm>

From norbert.naveen at tayana.in  Mon Jun 12 16:36:34 2017
From: norbert.naveen at tayana.in (Norbert Naveen)
Date: Mon, 12 Jun 2017 22:06:34 +0530
Subject: [squid-users] Error Compiling squid-3.5.26 with libecap-1.0.1 on
	CentOS Linux release 7.3.1611
Message-ID: <00ae01d2e39a$0e0038d0$2a00aa70$@tayana.in>

Hello 

 

I have CentOS Linux release 7.3.1611 64 bit 

 On the same I tried to Install the below 

 

cd libecap-1.0.1

./configure 'CXXFLAGS=-O2 -m64 -pipe' 'CFLAGS=-O2 -m64 -pipe'

gmake

gmake install-strip

 

cd squid-3.5.26

./configure '--enable-ecap' 'PKG_CONFIG_PATH=/usr/local/lib/pkgconfig'

make 

 

Towards the end receive the error as below . Attaching the complete 

 

 

adaptation/.libs/libadaptation.a(libsquid_ecap_la-Host.o): In function
`Adaptation::Ecap::Host::Host()':

/root/install/squid-3.5.26/src/adaptation/ecap/Host.cc:41: undefined
reference to `libecap::headerTransferEncoding'

/root/install/squid-3.5.26/src/adaptation/ecap/Host.cc:41: undefined
reference to `libecap::Name::assignHostId(int) const'

/root/install/squid-3.5.26/src/adaptation/ecap/Host.cc:42: undefined
reference to `libecap::headerReferer'

/root/install/squid-3.5.26/src/adaptation/ecap/Host.cc:42: undefined
reference to `libecap::Name::assignHostId(int) const'

/root/install/squid-3.5.26/src/adaptation/ecap/Host.cc:43: undefined
reference to `libecap::headerContentLength'

/root/install/squid-3.5.26/src/adaptation/ecap/Host.cc:43: undefined
reference to `libecap::Name::assignHostId(int) const'

/root/install/squid-3.5.26/src/adaptation/ecap/Host.cc:44: undefined
reference to `libecap::headerVia'

/root/install/squid-3.5.26/src/adaptation/ecap/Host.cc:44: undefined
reference to `libecap::Name::assignHostId(int) const'

/root/install/squid-3.5.26/src/adaptation/ecap/Host.cc:48: undefined
reference to `libecap::protocolHttp'

/root/install/squid-3.5.26/src/adaptation/ecap/Host.cc:48: undefined
reference to `libecap::Name::assignHostId(int) const'

/root/install/squid-3.5.26/src/adaptation/ecap/Host.cc:49: undefined
reference to `libecap::protocolHttps'

/root/install/squid-3.5.26/src/adaptation/ecap/Host.cc:49: undefined
reference to `libecap::Name::assignHostId(int) const'

/root/install/squid-3.5.26/src/adaptation/ecap/Host.cc:50: undefined
reference to `libecap::protocolFtp'

/root/install/squid-3.5.26/src/adaptation/ecap/Host.cc:50: undefined
reference to `libecap::Name::assignHostId(int) const'

/root/install/squid-3.5.26/src/adaptation/ecap/Host.cc:51: undefined
reference to `libecap::protocolGopher'

/root/install/squid-3.5.26/src/adaptation/ecap/Host.cc:51: undefined
reference to `libecap::Name::assignHostId(int) const'

/root/install/squid-3.5.26/src/adaptation/ecap/Host.cc:52: undefined
reference to `libecap::protocolWais'

/root/install/squid-3.5.26/src/adaptation/ecap/Host.cc:52: undefined
reference to `libecap::Name::assignHostId(int) const'

/root/install/squid-3.5.26/src/adaptation/ecap/Host.cc:53: undefined
reference to `libecap::protocolUrn'

/root/install/squid-3.5.26/src/adaptation/ecap/Host.cc:53: undefined
reference to `libecap::Name::assignHostId(int) const'

/root/install/squid-3.5.26/src/adaptation/ecap/Host.cc:54: undefined
reference to `libecap::protocolWhois'

/root/install/squid-3.5.26/src/adaptation/ecap/Host.cc:54: undefined
reference to `libecap::Name::assignHostId(int) const'

/root/install/squid-3.5.26/src/adaptation/ecap/Host.cc:55: undefined
reference to `libecap::Name::assignHostId(int) const'

/root/install/squid-3.5.26/src/adaptation/ecap/Host.cc:56: undefined
reference to `libecap::Name::assignHostId(int) const'

/root/install/squid-3.5.26/src/adaptation/ecap/Host.cc:58: undefined
reference to `libecap::Name::assignHostId(int) const'

/root/install/squid-3.5.26/src/adaptation/ecap/Host.cc:60: undefined
reference to `libecap::Name::assignHostId(int) const'

adaptation/.libs/libadaptation.a(libsquid_ecap_la-Host.o):/root/install/squi
d-3.5.26/src/adaptation/ecap/Host.cc:61: more undefined references to
`libecap::Name::assignHostId(int) const' follow

adaptation/.libs/libadaptation.a(libsquid_ecap_la-Host.o): In function
`Adaptation::Ecap::Host::Register()':

/root/install/squid-3.5.26/src/adaptation/ecap/Host.cc:178: undefined
reference to `libecap::VersionString()'

/root/install/squid-3.5.26/src/adaptation/ecap/Host.cc:180: undefined
reference to
`libecap::RegisterHost(std::tr1::shared_ptr<libecap::host::Host> const&)'

adaptation/.libs/libadaptation.a(libsquid_ecap_la-Host.o): In function
`_GLOBAL__sub_I_Host.cc':

/root/install/squid-3.5.26/src/adaptation/ecap/Host.cc:22: undefined
reference to `libecap::Name::NextId()'

/root/install/squid-3.5.26/src/adaptation/ecap/Host.cc:22: undefined
reference to `libecap::Name::Name(std::string const&, int)'

/root/install/squid-3.5.26/src/adaptation/ecap/Host.cc:23: undefined
reference to `libecap::Name::NextId()'

/root/install/squid-3.5.26/src/adaptation/ecap/Host.cc:23: undefined
reference to `libecap::Name::Name(std::string const&, int)'

/root/install/squid-3.5.26/src/adaptation/ecap/Host.cc:24: undefined
reference to `libecap::Name::NextId()'

/root/install/squid-3.5.26/src/adaptation/ecap/Host.cc:24: undefined
reference to `libecap::Name::Name(std::string const&, int)'

/root/install/squid-3.5.26/src/adaptation/ecap/Host.cc:26: undefined
reference to `libecap::Name::NextId()'

/root/install/squid-3.5.26/src/adaptation/ecap/Host.cc:26: undefined
reference to `libecap::Name::Name(std::string const&, int)'

/root/install/squid-3.5.26/src/adaptation/ecap/Host.cc:28: undefined
reference to `libecap::Name::NextId()'

/root/install/squid-3.5.26/src/adaptation/ecap/Host.cc:28: undefined
reference to `libecap::Name::Name(std::string const&, int)'

/root/install/squid-3.5.26/src/adaptation/ecap/Host.cc:29: undefined
reference to `libecap::Name::NextId()'

/root/install/squid-3.5.26/src/adaptation/ecap/Host.cc:29: undefined
reference to `libecap::Name::Name(std::string const&, int)'

/root/install/squid-3.5.26/src/adaptation/ecap/Host.cc:31: undefined
reference to `libecap::Name::NextId()'

/root/install/squid-3.5.26/src/adaptation/ecap/Host.cc:31: undefined
reference to `libecap::Name::Name(std::string const&, int)'

adaptation/.libs/libadaptation.a(libsquid_ecap_la-MessageRep.o): In function
`Adaptation::Ecap::MessageRep::~MessageRep()':

/usr/local/include/libecap/common/message.h:16: undefined reference to
`vtable for libecap::Message'

adaptation/.libs/libadaptation.a(libsquid_ecap_la-MessageRep.o): In function
`Adaptation::Ecap::RequestLineRep::method(libecap::Name const&)':

/root/install/squid-3.5.26/src/adaptation/ecap/MessageRep.cc:231: undefined
reference to `libecap::Name::assignedHostId() const'

adaptation/.libs/libadaptation.a(libsquid_ecap_la-MessageRep.o): In function
`Adaptation::Ecap::RequestLineRep::uri() const':

/root/install/squid-3.5.26/src/adaptation/ecap/MessageRep.cc:225: undefined
reference to `libecap::Area::FromTempBuffer(char const*, unsigned long)'

adaptation/.libs/libadaptation.a(libsquid_ecap_la-MessageRep.o): In function
`Adaptation::Ecap::StatusLineRep::reasonPhrase() const':

/root/install/squid-3.5.26/src/adaptation/ecap/MessageRep.cc:321: undefined
reference to `libecap::Area::FromTempString(std::string const&)'

adaptation/.libs/libadaptation.a(libsquid_ecap_la-MessageRep.o): In function
`Adaptation::Ecap::RequestLineRep::uri(libecap::Area const&)':

/root/install/squid-3.5.26/src/adaptation/ecap/MessageRep.cc:213: undefined
reference to `libecap::Area::toString() const'

adaptation/.libs/libadaptation.a(libsquid_ecap_la-MessageRep.o): In function
`Adaptation::Ecap::RequestLineRep::method() const':

/usr/local/include/libecap/common/name.h:14: undefined reference to
`libecap::methodDelete'

/usr/local/include/libecap/common/name.h:14: undefined reference to
`libecap::methodGet'

/usr/local/include/libecap/common/name.h:14: undefined reference to
`libecap::methodPost'

/usr/local/include/libecap/common/name.h:14: undefined reference to
`libecap::methodPut'

/usr/local/include/libecap/common/name.h:14: undefined reference to
`libecap::methodHead'

/usr/local/include/libecap/common/name.h:14: undefined reference to
`libecap::methodConnect'

/usr/local/include/libecap/common/name.h:14: undefined reference to
`libecap::methodTrace'

adaptation/.libs/libadaptation.a(libsquid_ecap_la-MessageRep.o): In function
`Adaptation::Ecap::RequestLineRep::method() const':

/root/install/squid-3.5.26/src/adaptation/ecap/MessageRep.cc:262: undefined
reference to `libecap::Name::Name(std::string const&)'

adaptation/.libs/libadaptation.a(libsquid_ecap_la-MessageRep.o): In function
`Adaptation::Ecap::HeaderRep::image() const':

/root/install/squid-3.5.26/src/adaptation/ecap/MessageRep.cc:99: undefined
reference to `libecap::Area::FromTempBuffer(char const*, unsigned long)'

adaptation/.libs/libadaptation.a(libsquid_ecap_la-MessageRep.o): In function
`Adaptation::Ecap::HeaderRep::TranslateHeaderId(libecap::Name const&)':

/root/install/squid-3.5.26/src/adaptation/ecap/MessageRep.cc:116: undefined
reference to `libecap::Name::assignedHostId() const'

adaptation/.libs/libadaptation.a(libsquid_ecap_la-MessageRep.o): In function
`Adaptation::Ecap::HeaderRep::add(libecap::Name const&, libecap::Area
const&)':

/root/install/squid-3.5.26/src/adaptation/ecap/MessageRep.cc:58: undefined
reference to `libecap::Area::toString() const'

adaptation/.libs/libadaptation.a(libsquid_ecap_la-MessageRep.o): In function
`Adaptation::Ecap::HeaderRep::value(libecap::Name const&) const':

/root/install/squid-3.5.26/src/adaptation/ecap/MessageRep.cc:50: undefined
reference to `libecap::Area::FromTempString(std::string const&)'

adaptation/.libs/libadaptation.a(libsquid_ecap_la-MessageRep.o): In function
`Adaptation::Ecap::FirstLineRep::protocol() const':

/root/install/squid-3.5.26/src/adaptation/ecap/MessageRep.cc:182: undefined
reference to `libecap::Name::Name()'

adaptation/.libs/libadaptation.a(libsquid_ecap_la-MessageRep.o): In function
`Adaptation::Ecap::FirstLineRep::protocol() const':

/usr/local/include/libecap/common/name.h:14: undefined reference to
`libecap::protocolHttp'

/usr/local/include/libecap/common/name.h:14: undefined reference to
`libecap::protocolFtp'

/usr/local/include/libecap/common/name.h:14: undefined reference to
`libecap::protocolHttps'

/usr/local/include/libecap/common/name.h:14: undefined reference to
`libecap::protocolGopher'

/usr/local/include/libecap/common/name.h:14: undefined reference to
`libecap::protocolWais'

/usr/local/include/libecap/common/name.h:14: undefined reference to
`libecap::protocolUrn'

/usr/local/include/libecap/common/name.h:14: undefined reference to
`libecap::protocolWhois'

adaptation/.libs/libadaptation.a(libsquid_ecap_la-MessageRep.o): In function
`Adaptation::Ecap::FirstLineRep::TranslateProtocolId(libecap::Name const&)':

/root/install/squid-3.5.26/src/adaptation/ecap/MessageRep.cc:195: undefined
reference to `libecap::Name::assignedHostId() const'

adaptation/.libs/libadaptation.a(libsquid_ecap_la-MessageRep.o): In function
`Adaptation::Ecap::MessageRep::MessageRep(HttpMsg*)':

/usr/local/include/libecap/common/message.h:16: undefined reference to
`vtable for libecap::Message'

adaptation/.libs/libadaptation.a(libsquid_ecap_la-MessageRep.o): In function
`Adaptation::Ecap::HeaderRep::visitEach(libecap::NamedValueVisitor&) const':

/root/install/squid-3.5.26/src/adaptation/ecap/MessageRep.cc:83: undefined
reference to `libecap::Name::Name(std::string const&)'

/root/install/squid-3.5.26/src/adaptation/ecap/MessageRep.cc:84: undefined
reference to `libecap::Name::assignHostId(int) const'

adaptation/.libs/libadaptation.a(libsquid_ecap_la-MessageRep.o):(.data.rel.r
o._ZTIN10Adaptation4Ecap10MessageRepE[_ZTIN10Adaptation4Ecap10MessageRepE]+0
x10): undefined reference to `typeinfo for libecap::Message'

adaptation/.libs/libadaptation.a(libsquid_ecap_la-MessageRep.o):(.data.rel.r
o._ZTVN10Adaptation4Ecap10MessageRepE[_ZTVN10Adaptation4Ecap10MessageRepE]+0
x60): undefined reference to `libecap::Message::addTrailer()'

adaptation/.libs/libadaptation.a(libsquid_ecap_la-ServiceRep.o): In function
`Adaptation::Ecap::ConfigRep::option(libecap::Name const&) const':

/root/install/squid-3.5.26/src/adaptation/ecap/ServiceRep.cc:86: undefined
reference to `libecap::Name::Name(std::string const&)'

adaptation/.libs/libadaptation.a(libsquid_ecap_la-ServiceRep.o): In function
`Adaptation::Ecap::ConfigRep::visitEachOption(libecap::NamedValueVisitor&)
const':

/root/install/squid-3.5.26/src/adaptation/ecap/ServiceRep.cc:102: undefined
reference to `libecap::Area::FromTempString(std::string const&)'

/root/install/squid-3.5.26/src/adaptation/ecap/ServiceRep.cc:102: undefined
reference to `libecap::Name::Name(std::string const&)'

adaptation/.libs/libadaptation.a(libsquid_ecap_la-XactionRep.o): In function
`Adaptation::Ecap::XactionRep::clientIpValue() const':

/root/install/squid-3.5.26/src/adaptation/ecap/XactionRep.cc:140: undefined
reference to `libecap::Area::FromTempBuffer(char const*, unsigned long)'

adaptation/.libs/libadaptation.a(libsquid_ecap_la-XactionRep.o): In function
`Adaptation::Ecap::XactionRep::usernameValue() const':

/root/install/squid-3.5.26/src/adaptation/ecap/XactionRep.cc:155: undefined
reference to `libecap::Area::FromTempBuffer(char const*, unsigned long)'

/root/install/squid-3.5.26/src/adaptation/ecap/XactionRep.cc:158: undefined
reference to `libecap::Area::FromTempBuffer(char const*, unsigned long)'

adaptation/.libs/libadaptation.a(libsquid_ecap_la-XactionRep.o): In function
`Adaptation::Ecap::XactionRep::metaValue(libecap::Name const&) const':

/root/install/squid-3.5.26/src/adaptation/ecap/XactionRep.cc:194: undefined
reference to `libecap::Area::FromTempString(std::string const&)'

adaptation/.libs/libadaptation.a(libsquid_ecap_la-XactionRep.o): In function
`Adaptation::Ecap::XactionRep::answer()':

/root/install/squid-3.5.26/src/adaptation/ecap/XactionRep.cc:328: undefined
reference to `typeinfo for libecap::Message'

adaptation/.libs/libadaptation.a(libsquid_ecap_la-XactionRep.o): In function
`Adaptation::Ecap::XactionRep::status() const':

/root/install/squid-3.5.26/src/adaptation/ecap/XactionRep.cc:718: undefined
reference to `typeinfo for libecap::Message'

adaptation/.libs/libadaptation.a(libsquid_ecap_la-XactionRep.o): In function
`Adaptation::Ecap::XactionRep::moveAbContent()':

/root/install/squid-3.5.26/src/adaptation/ecap/XactionRep.cc:681: undefined
reference to `libecap::nsize'

adaptation/.libs/libadaptation.a(libsquid_ecap_la-XactionRep.o): In function
`Adaptation::Ecap::XactionRep::visitEachMetaHeader(libecap::NamedValueVisito
r&) const':

/root/install/squid-3.5.26/src/adaptation/ecap/XactionRep.cc:216: undefined
reference to `libecap::Name::Name(std::string const&)'

/root/install/squid-3.5.26/src/adaptation/ecap/XactionRep.cc:217: undefined
reference to `libecap::Area::FromTempString(std::string const&)'

adaptation/.libs/libadaptation.a(libsquid_ecap_la-XactionRep.o): In function
`Adaptation::Ecap::XactionRep::updateHistory(HttpMsg*)':

/root/install/squid-3.5.26/src/adaptation/ecap/XactionRep.cc:476: undefined
reference to `libecap::Name::Name(std::string const&)'

/root/install/squid-3.5.26/src/adaptation/ecap/XactionRep.cc:478: undefined
reference to `libecap::Area::toString() const'

/root/install/squid-3.5.26/src/adaptation/ecap/XactionRep.cc:485: undefined
reference to `libecap::metaNextServices'

/root/install/squid-3.5.26/src/adaptation/ecap/XactionRep.cc:488: undefined
reference to `libecap::Area::toString() const'

adaptation/.libs/libadaptation.a(libsquid_ecap_la-XactionRep.o): In function
`Adaptation::Ecap::XactionRep::masterxSharedValue(libecap::Name const&)
const':

/root/install/squid-3.5.26/src/adaptation/ecap/XactionRep.cc:175: undefined
reference to `libecap::Area::FromTempBuffer(char const*, unsigned long)'

adaptation/.libs/libadaptation.a(libsquid_ecap_la-XactionRep.o): In function
`Adaptation::Ecap::XactionRep::visitEachOption(libecap::NamedValueVisitor&)
const':

/root/install/squid-3.5.26/src/adaptation/ecap/XactionRep.cc:106: undefined
reference to `libecap::metaClientIp'

/root/install/squid-3.5.26/src/adaptation/ecap/XactionRep.cc:108: undefined
reference to `libecap::metaUserName'

/root/install/squid-3.5.26/src/adaptation/ecap/XactionRep.cc:111: undefined
reference to `libecap::Name::Name(std::string const&)'

adaptation/.libs/libadaptation.a(libsquid_ecap_la-XactionRep.o): In function
`libecap::Name::operator==(libecap::Name const&) const':

/usr/local/include/libecap/common/name.h:27: undefined reference to
`libecap::metaClientIp'

/usr/local/include/libecap/common/name.h:27: undefined reference to
`libecap::metaUserName'

adaptation/.libs/libadaptation.a(libsquid_ecap_la-XactionRep.o): In function
`std::string::_M_data() const':

/usr/include/c++/4.8.2/bits/basic_string.h:293: undefined reference to
`libecap::metaClientIp'

/usr/include/c++/4.8.2/bits/basic_string.h:293: undefined reference to
`libecap::metaUserName'

adaptation/.libs/libadaptation.a(libsquid_ecap_la-XactionRep.o): In function
`Adaptation::Ecap::XactionRep::vbContent(unsigned long, unsigned long)':

/root/install/squid-3.5.26/src/adaptation/ecap/XactionRep.cc:562: undefined
reference to `libecap::nsize'

/root/install/squid-3.5.26/src/adaptation/ecap/XactionRep.cc:567: undefined
reference to `libecap::Area::FromTempBuffer(char const*, unsigned long)'

/root/install/squid-3.5.26/src/adaptation/ecap/XactionRep.cc:562: undefined
reference to `libecap::nsize'

adaptation/.libs/libadaptation.a(libsquid_ecap_la-XactionRep.o): In function
`Adaptation::Ecap::XactionRep::useAdapted(std::tr1::shared_ptr<libecap::Mess
age> const&)':

/root/install/squid-3.5.26/src/adaptation/ecap/XactionRep.cc:430: undefined
reference to `typeinfo for libecap::Message'

adaptation/.libs/libadaptation.a(libsquid_ecap_la-XactionRep.o): In function
`OptionsExtractor::visit(libecap::Name const&, libecap::Area const&)':

/root/install/squid-3.5.26/src/adaptation/ecap/XactionRep.cc:41: undefined
reference to `libecap::Area::toString() const'

collect2: error: ld returned 1 exit status

libtool: link: rm -f ".libs/squidS.o"

make[3]: *** [squid] Error 1

make[3]: Leaving directory `/root/install/squid-3.5.26/src'

make[2]: *** [all-recursive] Error 1

make[2]: Leaving directory `/root/install/squid-3.5.26/src'

make[1]: *** [all] Error 2

make[1]: Leaving directory `/root/install/squid-3.5.26/src'

make: *** [all-recursive] Error 1

 

Shall attach the complete output of make subsequently if required

 

 

Thanks for the help in Advance 

 

Rgds

Naveen

 

 

 

 

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20170612/b5095697/attachment.htm>

From rousskov at measurement-factory.com  Mon Jun 12 16:49:37 2017
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Mon, 12 Jun 2017 10:49:37 -0600
Subject: [squid-users] Error Compiling squid-3.5.26 with libecap-1.0.1
 on CentOS Linux release 7.3.1611
In-Reply-To: <00ae01d2e39a$0e0038d0$2a00aa70$@tayana.in>
References: <00ae01d2e39a$0e0038d0$2a00aa70$@tayana.in>
Message-ID: <ed4a5498-9d07-bef0-c304-6653ebec2851@measurement-factory.com>

On 06/12/2017 10:36 AM, Norbert Naveen wrote:
> I have CentOS Linux release 7.3.1611 64 bit

> cd libecap-1.0.1
> ./configure 'CXXFLAGS=-O2 -m64 -pipe' 'CFLAGS=-O2 -m64 -pipe'
> gmake
> gmake install-strip


> cd squid-3.5.26
> ./configure '--enable-ecap' 'PKG_CONFIG_PATH=/usr/local/lib/pkgconfig'
> make

> Towards the end receive the error as below . Attaching the complete
> /root/install/squid-3.5.26/src/adaptation/ecap/Host.cc:41: undefined
> reference to `libecap::headerTransferEncoding'

It looks like Squid found eCAP header files but did not find libecap.
Posting ./configure output and the failed linker command (should be
right above the first "undefined reference" error) may help triage this
further. Linking to a complete build log (starting with ./configure) may
reduce the number of follow up questions.

Also, what does /usr/local/lib/pkgconfig/libecap.pc contain?

HTH,

Alex.


From norbert.naveen at tayana.in  Mon Jun 12 17:29:11 2017
From: norbert.naveen at tayana.in (Norbert Naveen)
Date: Mon, 12 Jun 2017 22:59:11 +0530
Subject: [squid-users] Error Compiling squid-3.5.26 with libecap-1.0.1
	on CentOS Linux release 7.3.1611
In-Reply-To: <ed4a5498-9d07-bef0-c304-6653ebec2851@measurement-factory.com>
References: <00ae01d2e39a$0e0038d0$2a00aa70$@tayana.in>
 <ed4a5498-9d07-bef0-c304-6653ebec2851@measurement-factory.com>
Message-ID: <00cb01d2e3a1$67cc7ca0$376575e0$@tayana.in>

Hello Alex
  Sorry and Thanks for the response , I have attached Outputs of both
Configure and Make
  And  libecap.pc contents are as below 

# cat /usr/local/lib/pkgconfig/libecap.pc
prefix=/usr/local
exec_prefix=${prefix}
libdir=${exec_prefix}/lib
includedir=${prefix}/include

Name: eCAP
Description: Allows a network application to outsource content analysis and
adaptation to a loadable module.
URL: http://www.e-cap.org/
Version: 1.0.1
Libs: -L${libdir} -lecap
Cflags: -I${includedir}


&&& From make the first instance would be 

libtool: link: rm -f .libs/squid.nm .libs/squid.nmS .libs/squid.nmT
libtool: link: rm -f ".libs/squid.nmI"
libtool: link: (cd .libs && gcc -Wall -g -O2 -c -fno-builtin "squidS.c")
libtool: link: rm -f ".libs/squidS.c" ".libs/squid.nm" ".libs/squid.nmS"
".libs/squid.nmT" ".libs/squid.nmI"

libtool: link: g++ -Wall -Wpointer-arith -Wwrite-strings -Wcomments -Wshadow
-Woverloaded-virtual -Werror -pipe -D_REENTRANT -g -O2 -march=native
-std=c++11 .libs/squidS.o -g -o squid AclRegs.o AuthReg.o AccessLogEntry.o
AsyncEngine.o YesNoNone.o cache_cf.o CacheDigest.o cache_manager.o carp.o
cbdata.o ChunkedCodingParser.o client_db.o client_side.o client_side_reply.o
client_side_request.o BodyPipe.o clientStream.o CollapsedForwarding.o
CompletionDispatcher.o ConfigOption.o ConfigParser.o CpuAffinity.o
CpuAffinityMap.o CpuAffinitySet.o debug.o disk.o DiskIO/DiskIOModule.o
DiskIO/ReadRequest.o DiskIO/WriteRequest.o dlink.o dns_internal.o
DnsLookupDetails.o errorpage.o ETag.o event.o EventLoop.o external_acl.o
ExternalACLEntry.o FadingCounter.o fatal.o fd.o fde.o filemap.o fqdncache.o
FwdState.o gopher.o helper.o htcp.o http.o HttpHdrCc.o HttpHdrRange.o
HttpHdrSc.o HttpHdrScTarget.o HttpHdrContRange.o HttpHeader.o
HttpHeaderTools.o HttpBody.o HttpMsg.o HttpParser.o HttpReply.o
RequestFlags.o HttpRequest.o HttpRequestMethod.o icp_v2.o icp_v3.o int.o
internal.o ipc.o ipcache.o SquidList.o main.o MasterXaction.o mem.o
mem_node.o MemBuf.o MemObject.o mime.o mime_header.o multicast.o neighbors.o
Notes.o Packer.o Parsing.o pconn.o peer_digest.o peer_proxy_negotiate_auth.o
peer_select.o peer_sourcehash.o peer_userhash.o PeerPoolMgr.o redirect.o
refresh.o RemovalPolicy.o send-announce.o MemBlob.o SBuf.o SBufExceptions.o
SBufDetailedStats.o SBufStatsAction.o snmp_core.o snmp_agent.o SquidMath.o
SquidNew.o stat.o StatCounters.o StatHist.o String.o StrList.o stmem.o
store.o StoreFileSystem.o store_io.o StoreIOState.o store_client.o
store_digest.o store_dir.o store_key_md5.o store_log.o store_rebuild.o
store_swapin.o store_swapmeta.o store_swapout.o StoreMetaUnpacker.o
StoreMeta.o StoreMetaMD5.o StoreMetaSTD.o StoreMetaSTDLFS.o StoreMetaURL.o
StoreMetaVary.o StoreStats.o StoreSwapLogData.o SwapDir.o Transients.o
MemStore.o time.o tools.o tunnel.o unlinkd.o url.o urn.o wccp.o wccp2.o
whois.o wordlist.o LoadableModule.o LoadableModules.o
DiskIO/DiskIOModules_gen.o err_type.o err_detail_type.o globals.o
hier_code.o icp_opcode.o LogTags.o lookup_t.o repl_modules.o swap_log_op.o
DiskIO/AIO/AIODiskIOModule.o DiskIO/Blocking/BlockingDiskIOModule.o
DiskIO/DiskDaemon/DiskDaemonDiskIOModule.o
DiskIO/DiskThreads/DiskThreadsDiskIOModule.o
DiskIO/IpcIo/IpcIoDiskIOModule.o DiskIO/Mmapped/MmappedDiskIOModule.o
-Wl,--export-dynamic  /root/install/squid-3.5.26/libltdl/./.libs/dlopen.a
auth/.libs/libacls.a ident/.libs/libident.a acl/.libs/libacls.a
acl/.libs/libstate.a auth/.libs/libauth.a libAIO.a libBlocking.a
libDiskDaemon.a libDiskThreads.a libIpcIo.a libMmapped.a acl/.libs/libapi.a
base/.libs/libbase.a ./.libs/libsquid.a ip/.libs/libip.a fs/.libs/libfs.a
ipc/.libs/libipc.a mgr/.libs/libmgr.a anyp/.libs/libanyp.a
comm/.libs/libcomm.a eui/.libs/libeui.a helper/.libs/libhelper.a
http/.libs/libsquid-http.a icmp/.libs/libicmp.a icmp/.libs/libicmp-core.a
log/.libs/liblog.a format/.libs/libformat.a clients/.libs/libclients.a
servers/.libs/libservers.a ftp/.libs/libftp.a repl/liblru.a -lpthread
-lcrypt adaptation/.libs/libadaptation.a snmp/.libs/libsnmp.a
../lib/snmplib/.libs/libsnmplib.a parser/.libs/libsquid-parser.a
../lib/.libs/libmisccontainers.a ../lib/.libs/libmiscencoding.a
../lib/.libs/libmiscutil.a -lgssapi_krb5 -lkrb5 -lk5crypto -lcom_err
../compat/.libs/libcompat-squid.a -lm -lnsl -lresolv -lrt -L..
../libltdl/.libs/libltdlc.a -ldl
adaptation/.libs/libadaptation.a(libsquid_ecap_la-Host.o): In function
`Adaptation::Ecap::Host::Host()':
/root/install/squid-3.5.26/src/adaptation/ecap/Host.cc:41: undefined
reference to `libecap::headerTransferEncoding'
/root/install/squid-3.5.26/src/adaptation/ecap/Host.cc:41: undefined
reference to `libecap::Name::assignHostId(int) const'
/root/install/squid-3.5.26/src/adaptation/ecap/Host.cc:42: undefined
reference to `libecap::headerReferer'
/root/install/squid-3.5.26/src/adaptation/ecap/Host.cc:42: undefined
reference to `libecap::Name::assignHostId(int) const'
/root/install/squid-3.5.26/src/adaptation/ecap/Host.cc:43: undefined
reference to `libecap::headerContentLength'
/root/install/squid-3.5.26/src/adaptation/ecap/Host.cc:43: undefined
reference to `libecap::Name::assignHostId(int) const'
/root/install/squid-3.5.26/src/adaptation/ecap/Host.cc:44: undefined
reference to `libecap::headerVia'
/root/install/squid-3.5.26/src/adaptation/ecap/Host.cc:44: undefined
reference to `libecap::Name::assignHostId(int) const'
/root/install/squid-3.5.26/src/adaptation/ecap/Host.cc:48: undefined
reference to `libecap::protocolHttp'
/root/install/squid-3.5.26/src/adaptation/ecap/Host.cc:48: undefined
reference to `libecap::Name::assignHostId(int) const'
/root/install/squid-3.5.26/src/adaptation/ecap/Host.cc:49: undefined
reference to `libecap::protocolHttps'
/root/install/squid-3.5.26/src/adaptation/ecap/Host.cc:49: undefined
reference to `libecap::Name::assignHostId(int) const'
/root/install/squid-3.5.26/src/adaptation/ecap/Host.cc:50: undefined
reference to `libecap::protocolFtp'
/root/install/squid-3.5.26/src/adaptation/ecap/Host.cc:50: undefined
reference to `libecap::Name::assignHostId(int) const'
/root/install/squid-3.5.26/src/adaptation/ecap/Host.cc:51: undefined
reference to `libecap::protocolGopher'
/root/install/squid-3.5.26/src/adaptation/ecap/Host.cc:51: undefined
reference to `libecap::Name::assignHostId(int) const'
/root/install/squid-3.5.26/src/adaptation/ecap/Host.cc:52: undefined
reference to `libecap::protocolWais'
/root/install/squid-3.5.26/src/adaptation/ecap/Host.cc:52: undefined
reference to `libecap::Name::assignHostId(int) const'
/root/install/squid-3.5.26/src/adaptation/ecap/Host.cc:53: undefined
reference to `libecap::protocolUrn'
/root/install/squid-3.5.26/src/adaptation/ecap/Host.cc:53: undefined
reference to `libecap::Name::assignHostId(int) const'
/root/install/squid-3.5.26/src/adaptation/ecap/Host.cc:54: undefined
reference to `libecap::protocolWhois'
/root/install/squid-3.5.26/src/adaptation/ecap/Host.cc:54: undefined
reference to `libecap::Name::assignHostId(int) const'
/root/install/squid-3.5.26/src/adaptation/ecap/Host.cc:55: undefined
reference to `libecap::Name::assignHostId(int) const'
/root/install/squid-3.5.26/src/adaptation/ecap/Host.cc:56: undefined
reference to `libecap::Name::assignHostId(int) const'
/root/install/squid-3.5.26/src/adaptation/ecap/Host.cc:58: undefined
reference to `libecap::Name::assignHostId(int) const'
/root/install/squid-3.5.26/src/adaptation/ecap/Host.cc:60: undefined
reference to `libecap::Name::assignHostId(int) const'
adaptation/.libs/libadaptation.a(libsquid_ecap_la-Host.o):/root/install/squi
d-3.5.26/src/adaptation/ecap/Host.cc:61: more undefined references to
`libecap::Name::assignHostId(int) const' follow
adaptation/.libs/libadaptation.a(libsquid_ecap_la-Host.o): In function
`Adaptation::Ecap::Host::Register()':
/root/install/squid-3.5.26/src/adaptation/ecap/Host.cc:178: undefined
reference to `libecap::VersionString()'
/root/install/squid-3.5.26/src/adaptation/ecap/Host.cc:180: undefined
reference to
`libecap::RegisterHost(std::tr1::shared_ptr<libecap::host::Host> const&)'
adaptation/.libs/libadaptation.a(libsquid_ecap_la-Host.o): In function
`_GLOBAL__sub_I_Host.cc':
/root/install/squid-3.5.26/src/adaptation/ecap/Host.cc:22: undefined
reference to `libecap::Name::NextId()'
/root/install/squid-3.5.26/src/adaptation/ecap/Host.cc:22: undefined
reference to `libecap::Name::Name(std::string const&, int)'
/root/install/squid-3.5.26/src/adaptation/ecap/Host.cc:23: undefined
reference to `libecap::Name::NextId()'
/root/install/squid-3.5.26/src/adaptation/ecap/Host.cc:23: undefined
reference to `libecap::Name::Name(std::string const&, int)'
/root/install/squid-3.5.26/src/adaptation/ecap/Host.cc:24: undefined
reference to `libecap::Name::NextId()'
/root/install/squid-3.5.26/src/adaptation/ecap/Host.cc:24: undefined
reference to `libecap::Name::Name(std::string const&, int)'
/root/install/squid-3.5.26/src/adaptation/ecap/Host.cc:26: undefined
reference to `libecap::Name::NextId()'
/root/install/squid-3.5.26/src/adaptation/ecap/Host.cc:26: undefined
reference to `libecap::Name::Name(std::string const&, int)'
/root/install/squid-3.5.26/src/adaptation/ecap/Host.cc:28: undefined
reference to `libecap::Name::NextId()'
/root/install/squid-3.5.26/src/adaptation/ecap/Host.cc:28: undefined
reference to `libecap::Name::Name(std::string const&, int)'
/root/install/squid-3.5.26/src/adaptation/ecap/Host.cc:29: undefined
reference to `libecap::Name::NextId()'
/root/install/squid-3.5.26/src/adaptation/ecap/Host.cc:29: undefined
reference to `libecap::Name::Name(std::string const&, int)'
/root/install/squid-3.5.26/src/adaptation/ecap/Host.cc:31: undefined
reference to `libecap::Name::NextId()'
/root/install/squid-3.5.26/src/adaptation/ecap/Host.cc:31: undefined
reference to `libecap::Name::Name(std::string const&, int)'
adaptation/.libs/libadaptation.a(libsquid_ecap_la-MessageRep.o): In function
`Adaptation::Ecap::MessageRep::~MessageRep()':
/usr/local/include/libecap/common/message.h:16: undefined reference to
`vtable for libecap::Message'
adaptation/.libs/libadaptation.a(libsquid_ecap_la-MessageRep.o): In function
`Adaptation::Ecap::RequestLineRep::method(libecap::Name const&)':
/root/install/squid-3.5.26/src/adaptation/ecap/MessageRep.cc:231: undefined
reference to `libecap::Name::assignedHostId() const'
adaptation/.libs/libadaptation.a(libsquid_ecap_la-MessageRep.o): In function
`Adaptation::Ecap::RequestLineRep::uri() const':
/root/install/squid-3.5.26/src/adaptation/ecap/MessageRep.cc:225: undefined
reference to `libecap::Area::FromTempBuffer(char const*, unsigned long)'
adaptation/.libs/libadaptation.a(libsquid_ecap_la-MessageRep.o): In function
`Adaptation::Ecap::StatusLineRep::reasonPhrase() const':
/root/install/squid-3.5.26/src/adaptation/ecap/MessageRep.cc:321: undefined
reference to `libecap::Area::FromTempString(std::string const&)'
adaptation/.libs/libadaptation.a(libsquid_ecap_la-MessageRep.o): In function
`Adaptation::Ecap::RequestLineRep::uri(libecap::Area const&)':
/root/install/squid-3.5.26/src/adaptation/ecap/MessageRep.cc:213: undefined
reference to `libecap::Area::toString() const'
adaptation/.libs/libadaptation.a(libsquid_ecap_la-MessageRep.o): In function
`Adaptation::Ecap::RequestLineRep::method() const':
/usr/local/include/libecap/common/name.h:14: undefined reference to
`libecap::methodDelete'
/usr/local/include/libecap/common/name.h:14: undefined reference to
`libecap::methodGet'
/usr/local/include/libecap/common/name.h:14: undefined reference to
`libecap::methodPost'
/usr/local/include/libecap/common/name.h:14: undefined reference to
`libecap::methodPut'
/usr/local/include/libecap/common/name.h:14: undefined reference to
`libecap::methodHead'
/usr/local/include/libecap/common/name.h:14: undefined reference to
`libecap::methodConnect'
/usr/local/include/libecap/common/name.h:14: undefined reference to
`libecap::methodTrace'
adaptation/.libs/libadaptation.a(libsquid_ecap_la-MessageRep.o): In function
`Adaptation::Ecap::RequestLineRep::method() const':
/root/install/squid-3.5.26/src/adaptation/ecap/MessageRep.cc:262: undefined
reference to `libecap::Name::Name(std::string const&)'
adaptation/.libs/libadaptation.a(libsquid_ecap_la-MessageRep.o): In function
`Adaptation::Ecap::HeaderRep::image() const':
/root/install/squid-3.5.26/src/adaptation/ecap/MessageRep.cc:99: undefined
reference to `libecap::Area::FromTempBuffer(char const*, unsigned long)'
adaptation/.libs/libadaptation.a(libsquid_ecap_la-MessageRep.o): In function
`Adaptation::Ecap::HeaderRep::TranslateHeaderId(libecap::Name const&)':
/root/install/squid-3.5.26/src/adaptation/ecap/MessageRep.cc:116: undefined
reference to `libecap::Name::assignedHostId() const'
adaptation/.libs/libadaptation.a(libsquid_ecap_la-MessageRep.o): In function
`Adaptation::Ecap::HeaderRep::add(libecap::Name const&, libecap::Area
const&)':
/root/install/squid-3.5.26/src/adaptation/ecap/MessageRep.cc:58: undefined
reference to `libecap::Area::toString() const'
adaptation/.libs/libadaptation.a(libsquid_ecap_la-MessageRep.o): In function
`Adaptation::Ecap::HeaderRep::value(libecap::Name const&) const':
/root/install/squid-3.5.26/src/adaptation/ecap/MessageRep.cc:50: undefined
reference to `libecap::Area::FromTempString(std::string const&)'
adaptation/.libs/libadaptation.a(libsquid_ecap_la-MessageRep.o): In function
`Adaptation::Ecap::FirstLineRep::protocol() const':
/root/install/squid-3.5.26/src/adaptation/ecap/MessageRep.cc:182: undefined
reference to `libecap::Name::Name()'
adaptation/.libs/libadaptation.a(libsquid_ecap_la-MessageRep.o): In function
`Adaptation::Ecap::FirstLineRep::protocol() const':
/usr/local/include/libecap/common/name.h:14: undefined reference to
`libecap::protocolHttp'
/usr/local/include/libecap/common/name.h:14: undefined reference to
`libecap::protocolFtp'
/usr/local/include/libecap/common/name.h:14: undefined reference to
`libecap::protocolHttps'
/usr/local/include/libecap/common/name.h:14: undefined reference to
`libecap::protocolGopher'
/usr/local/include/libecap/common/name.h:14: undefined reference to
`libecap::protocolWais'
/usr/local/include/libecap/common/name.h:14: undefined reference to
`libecap::protocolUrn'
/usr/local/include/libecap/common/name.h:14: undefined reference to
`libecap::protocolWhois'
adaptation/.libs/libadaptation.a(libsquid_ecap_la-MessageRep.o): In function
`Adaptation::Ecap::FirstLineRep::TranslateProtocolId(libecap::Name const&)':
/root/install/squid-3.5.26/src/adaptation/ecap/MessageRep.cc:195: undefined
reference to `libecap::Name::assignedHostId() const'
adaptation/.libs/libadaptation.a(libsquid_ecap_la-MessageRep.o): In function
`Adaptation::Ecap::MessageRep::MessageRep(HttpMsg*)':
/usr/local/include/libecap/common/message.h:16: undefined reference to
`vtable for libecap::Message'
adaptation/.libs/libadaptation.a(libsquid_ecap_la-MessageRep.o): In function
`Adaptation::Ecap::HeaderRep::visitEach(libecap::NamedValueVisitor&) const':
/root/install/squid-3.5.26/src/adaptation/ecap/MessageRep.cc:83: undefined
reference to `libecap::Name::Name(std::string const&)'
/root/install/squid-3.5.26/src/adaptation/ecap/MessageRep.cc:84: undefined
reference to `libecap::Name::assignHostId(int) const'
adaptation/.libs/libadaptation.a(libsquid_ecap_la-MessageRep.o):(.data.rel.r
o._ZTIN10Adaptation4Ecap10MessageRepE[_ZTIN10Adaptation4Ecap10MessageRepE]+0
x10): undefined reference to `typeinfo for libecap::Message'
adaptation/.libs/libadaptation.a(libsquid_ecap_la-MessageRep.o):(.data.rel.r
o._ZTVN10Adaptation4Ecap10MessageRepE[_ZTVN10Adaptation4Ecap10MessageRepE]+0
x60): undefined reference to `libecap::Message::addTrailer()'
adaptation/.libs/libadaptation.a(libsquid_ecap_la-ServiceRep.o): In function
`Adaptation::Ecap::ConfigRep::option(libecap::Name const&) const':
/root/install/squid-3.5.26/src/adaptation/ecap/ServiceRep.cc:86: undefined
reference to `libecap::Name::Name(std::string const&)'
adaptation/.libs/libadaptation.a(libsquid_ecap_la-ServiceRep.o): In function
`Adaptation::Ecap::ConfigRep::visitEachOption(libecap::NamedValueVisitor&)
const':
/root/install/squid-3.5.26/src/adaptation/ecap/ServiceRep.cc:102: undefined
reference to `libecap::Area::FromTempString(std::string const&)'
/root/install/squid-3.5.26/src/adaptation/ecap/ServiceRep.cc:102: undefined
reference to `libecap::Name::Name(std::string const&)'
adaptation/.libs/libadaptation.a(libsquid_ecap_la-XactionRep.o): In function
`Adaptation::Ecap::XactionRep::clientIpValue() const':
/root/install/squid-3.5.26/src/adaptation/ecap/XactionRep.cc:140: undefined
reference to `libecap::Area::FromTempBuffer(char const*, unsigned long)'
adaptation/.libs/libadaptation.a(libsquid_ecap_la-XactionRep.o): In function
`Adaptation::Ecap::XactionRep::usernameValue() const':
/root/install/squid-3.5.26/src/adaptation/ecap/XactionRep.cc:155: undefined
reference to `libecap::Area::FromTempBuffer(char const*, unsigned long)'
/root/install/squid-3.5.26/src/adaptation/ecap/XactionRep.cc:158: undefined
reference to `libecap::Area::FromTempBuffer(char const*, unsigned long)'
adaptation/.libs/libadaptation.a(libsquid_ecap_la-XactionRep.o): In function
`Adaptation::Ecap::XactionRep::metaValue(libecap::Name const&) const':
/root/install/squid-3.5.26/src/adaptation/ecap/XactionRep.cc:194: undefined
reference to `libecap::Area::FromTempString(std::string const&)'
adaptation/.libs/libadaptation.a(libsquid_ecap_la-XactionRep.o): In function
`Adaptation::Ecap::XactionRep::answer()':
/root/install/squid-3.5.26/src/adaptation/ecap/XactionRep.cc:328: undefined
reference to `typeinfo for libecap::Message'
adaptation/.libs/libadaptation.a(libsquid_ecap_la-XactionRep.o): In function
`Adaptation::Ecap::XactionRep::status() const':
/root/install/squid-3.5.26/src/adaptation/ecap/XactionRep.cc:718: undefined
reference to `typeinfo for libecap::Message'
adaptation/.libs/libadaptation.a(libsquid_ecap_la-XactionRep.o): In function
`Adaptation::Ecap::XactionRep::moveAbContent()':
/root/install/squid-3.5.26/src/adaptation/ecap/XactionRep.cc:681: undefined
reference to `libecap::nsize'
adaptation/.libs/libadaptation.a(libsquid_ecap_la-XactionRep.o): In function
`Adaptation::Ecap::XactionRep::visitEachMetaHeader(libecap::NamedValueVisito
r&) const':
/root/install/squid-3.5.26/src/adaptation/ecap/XactionRep.cc:216: undefined
reference to `libecap::Name::Name(std::string const&)'
/root/install/squid-3.5.26/src/adaptation/ecap/XactionRep.cc:217: undefined
reference to `libecap::Area::FromTempString(std::string const&)'
adaptation/.libs/libadaptation.a(libsquid_ecap_la-XactionRep.o): In function
`Adaptation::Ecap::XactionRep::updateHistory(HttpMsg*)':
/root/install/squid-3.5.26/src/adaptation/ecap/XactionRep.cc:476: undefined
reference to `libecap::Name::Name(std::string const&)'
/root/install/squid-3.5.26/src/adaptation/ecap/XactionRep.cc:478: undefined
reference to `libecap::Area::toString() const'
/root/install/squid-3.5.26/src/adaptation/ecap/XactionRep.cc:485: undefined
reference to `libecap::metaNextServices'
/root/install/squid-3.5.26/src/adaptation/ecap/XactionRep.cc:488: undefined
reference to `libecap::Area::toString() const'
adaptation/.libs/libadaptation.a(libsquid_ecap_la-XactionRep.o): In function
`Adaptation::Ecap::XactionRep::masterxSharedValue(libecap::Name const&)
const':
/root/install/squid-3.5.26/src/adaptation/ecap/XactionRep.cc:175: undefined
reference to `libecap::Area::FromTempBuffer(char const*, unsigned long)'
adaptation/.libs/libadaptation.a(libsquid_ecap_la-XactionRep.o): In function
`Adaptation::Ecap::XactionRep::visitEachOption(libecap::NamedValueVisitor&)
const':
/root/install/squid-3.5.26/src/adaptation/ecap/XactionRep.cc:106: undefined
reference to `libecap::metaClientIp'
/root/install/squid-3.5.26/src/adaptation/ecap/XactionRep.cc:108: undefined
reference to `libecap::metaUserName'
/root/install/squid-3.5.26/src/adaptation/ecap/XactionRep.cc:111: undefined
reference to `libecap::Name::Name(std::string const&)'
adaptation/.libs/libadaptation.a(libsquid_ecap_la-XactionRep.o): In function
`libecap::Name::operator==(libecap::Name const&) const':
/usr/local/include/libecap/common/name.h:27: undefined reference to
`libecap::metaClientIp'
/usr/local/include/libecap/common/name.h:27: undefined reference to
`libecap::metaUserName'
adaptation/.libs/libadaptation.a(libsquid_ecap_la-XactionRep.o): In function
`std::string::_M_data() const':
/usr/include/c++/4.8.2/bits/basic_string.h:293: undefined reference to
`libecap::metaClientIp'
/usr/include/c++/4.8.2/bits/basic_string.h:293: undefined reference to
`libecap::metaUserName'
adaptation/.libs/libadaptation.a(libsquid_ecap_la-XactionRep.o): In function
`Adaptation::Ecap::XactionRep::vbContent(unsigned long, unsigned long)':
/root/install/squid-3.5.26/src/adaptation/ecap/XactionRep.cc:562: undefined
reference to `libecap::nsize'
/root/install/squid-3.5.26/src/adaptation/ecap/XactionRep.cc:567: undefined
reference to `libecap::Area::FromTempBuffer(char const*, unsigned long)'
/root/install/squid-3.5.26/src/adaptation/ecap/XactionRep.cc:562: undefined
reference to `libecap::nsize'
adaptation/.libs/libadaptation.a(libsquid_ecap_la-XactionRep.o): In function
`Adaptation::Ecap::XactionRep::useAdapted(std::tr1::shared_ptr<libecap::Mess
age> const&)':
/root/install/squid-3.5.26/src/adaptation/ecap/XactionRep.cc:430: undefined
reference to `typeinfo for libecap::Message'
adaptation/.libs/libadaptation.a(libsquid_ecap_la-XactionRep.o): In function
`OptionsExtractor::visit(libecap::Name const&, libecap::Area const&)':
/root/install/squid-3.5.26/src/adaptation/ecap/XactionRep.cc:41: undefined
reference to `libecap::Area::toString() const'
collect2: error: ld returned 1 exit status
libtool: link: rm -f ".libs/squidS.o"
make[3]: *** [squid] Error 1
make[3]: Leaving directory `/root/install/squid-3.5.26/src'
make[2]: *** [all-recursive] Error 1
make[2]: Leaving directory `/root/install/squid-3.5.26/src'
make[1]: *** [all] Error 2
make[1]: Leaving directory `/root/install/squid-3.5.26/src'
make: *** [all-recursive] Error 1
Thanks
Naveen

-----Original Message-----
From: Alex Rousskov [mailto:rousskov at measurement-factory.com] 
Sent: Monday, June 12, 2017 10:20 PM
To: norbert.naveen at tayana.in; squid-users at lists.squid-cache.org
Subject: Re: [squid-users] Error Compiling squid-3.5.26 with libecap-1.0.1
on CentOS Linux release 7.3.1611

On 06/12/2017 10:36 AM, Norbert Naveen wrote:
> I have CentOS Linux release 7.3.1611 64 bit

> cd libecap-1.0.1
> ./configure 'CXXFLAGS=-O2 -m64 -pipe' 'CFLAGS=-O2 -m64 -pipe'
> gmake
> gmake install-strip


> cd squid-3.5.26
> ./configure '--enable-ecap' 'PKG_CONFIG_PATH=/usr/local/lib/pkgconfig'
> make

> Towards the end receive the error as below . Attaching the complete
> /root/install/squid-3.5.26/src/adaptation/ecap/Host.cc:41: undefined 
> reference to `libecap::headerTransferEncoding'

It looks like Squid found eCAP header files but did not find libecap.
Posting ./configure output and the failed linker command (should be right
above the first "undefined reference" error) may help triage this further.
Linking to a complete build log (starting with ./configure) may reduce the
number of follow up questions.

Also, what does /usr/local/lib/pkgconfig/libecap.pc contain?

HTH,

Alex.
-------------- next part --------------
An embedded and charset-unspecified text was scrubbed...
Name: ConfigureOutput.txt
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20170612/370e861f/attachment.txt>

From norbert.naveen at tayana.in  Mon Jun 12 17:32:09 2017
From: norbert.naveen at tayana.in (Norbert Naveen)
Date: Mon, 12 Jun 2017 23:02:09 +0530
Subject: [squid-users] Error Compiling squid-3.5.26 with libecap-1.0.1
	on CentOS Linux release 7.3.1611
References: <00ae01d2e39a$0e0038d0$2a00aa70$@tayana.in>
 <ed4a5498-9d07-bef0-c304-6653ebec2851@measurement-factory.com> 
Message-ID: <00cf01d2e3a1$d1ddf5b0$7599e110$@tayana.in>

Hello Alex, 
  Output of configure as below 

squid-3.5.26]# ./configure '--enable-ecap'
'PKG_CONFIG_PATH=/usr/local/lib/pkgconfig'

checking for a BSD-compatible install... /usr/bin/install -c
checking whether build environment is sane... yes
checking for a thread-safe mkdir -p... /usr/bin/mkdir -p
checking for gawk... gawk
checking whether make sets $(MAKE)... yes
checking whether make supports nested variables... yes
checking whether UID '0' is supported by ustar format... yes
checking whether GID '0' is supported by ustar format... yes
checking how to create a ustar tar archive... gnutar
checking whether to enable maintainer-specific portions of Makefiles... no
checking for gcc... gcc
checking whether the C compiler works... yes
checking for C compiler default output file name... a.out
checking for suffix of executables...
checking whether we are cross compiling... no
checking for suffix of object files... o
checking whether we are using the GNU C compiler... yes
checking whether gcc accepts -g... yes
checking for gcc option to accept ISO C89... none needed
checking whether gcc understands -c and -o together... yes
checking for style of include used by make... GNU
checking dependency style of gcc... gcc3
checking for g++... g++
checking whether we are using the GNU C++ compiler... yes
checking whether g++ accepts -g... yes
checking dependency style of g++... gcc3
checking build system type... x86_64-pc-linux-gnu
checking host system type... x86_64-pc-linux-gnu
configure: CPU arch native optimization enabled: auto
checking whether compiler accepts -march=native... yes
checking simplified host os... linux (version )
checking what kind of compiler we're using... gcc
checking whether g++ supports C++11 features by default... no
checking whether g++ supports C++11 features with -std=c++11... yes
checking for ranlib... ranlib
checking how to run the C preprocessor... gcc -E
checking whether ln -s works... yes
checking for egrep... /usr/bin/egrep
checking for sh... /usr/bin/sh
checking for false... /usr/bin/false
checking for true... /usr/bin/true
checking for mv... /usr/bin/mv
checking for mkdir... /usr/bin/mkdir
checking for ln... /usr/bin/ln
checking for chmod... /usr/bin/chmod
checking for tr... /usr/bin/tr
checking for rm... /usr/bin/rm
checking for cppunit-config... false
checking for perl... /usr/bin/perl
checking for pod2man... /usr/bin/pod2man
checking for ar... /usr/bin/ar
checking for linuxdoc... /usr/bin/false
configure: strict error checking enabled: yes
checking whether to use loadable modules... yes
checking how to print strings... printf
checking for a sed that does not truncate output... /usr/bin/sed
checking for fgrep... /usr/bin/fgrep
checking for ld used by gcc... /usr/bin/ld
checking if the linker (/usr/bin/ld) is GNU ld... yes
checking for BSD- or MS-compatible name lister (nm)... /usr/bin/nm -B
checking the name lister (/usr/bin/nm -B) interface... BSD nm
checking the maximum length of command line arguments... 1572864
checking how to convert x86_64-pc-linux-gnu file names to
x86_64-pc-linux-gnu format... func_convert_file_noop
checking how to convert x86_64-pc-linux-gnu file names to toolchain
format... func_convert_file_noop
checking for /usr/bin/ld option to reload object files... -r
checking for objdump... objdump
checking how to recognize dependent libraries... pass_all
checking for dlltool... no
checking how to associate runtime and link libraries... printf %s\n
checking for archiver @FILE support... @
checking for strip... strip
checking for ranlib... (cached) ranlib
checking command to parse /usr/bin/nm -B output from gcc object... ok
checking for sysroot... no
checking for a working dd... /usr/bin/dd
checking how to truncate binary pipes... /usr/bin/dd bs=4096 count=1
checking for mt... mt
checking if mt is a manifest tool... no
checking for dlfcn.h... yes
checking for objdir... .libs
checking if gcc supports -fno-rtti -fno-exceptions... no
checking for gcc option to produce PIC... -fPIC -DPIC
checking if gcc PIC flag -fPIC -DPIC works... yes
checking if gcc static flag -static works... no
checking if gcc supports -c -o file.o... yes
checking if gcc supports -c -o file.o... (cached) yes
checking whether the gcc linker (/usr/bin/ld -m elf_x86_64) supports shared
libraries... yes
checking whether -lc should be explicitly linked in... no
checking dynamic linker characteristics... GNU/Linux ld.so
checking how to hardcode library paths into programs... immediate
checking for shl_load... no
checking for shl_load in -ldld... no
checking for dlopen... no
checking for dlopen in -ldl... yes
checking whether a program can dlopen itself... yes
checking whether a statically linked program can dlopen itself... yes
checking whether stripping libraries is possible... yes
checking if libtool supports shared libraries... yes
checking whether to build shared libraries... yes
checking whether to build static libraries... yes
checking how to run the C++ preprocessor... g++ -E
checking for ld used by g++... /usr/bin/ld -m elf_x86_64
checking if the linker (/usr/bin/ld -m elf_x86_64) is GNU ld... yes
checking whether the g++ linker (/usr/bin/ld -m elf_x86_64) supports shared
libraries... yes
checking for g++ option to produce PIC... -fPIC -DPIC
checking if g++ PIC flag -fPIC -DPIC works... yes
checking if g++ static flag -static works... no
checking if g++ supports -c -o file.o... yes
checking if g++ supports -c -o file.o... (cached) yes
checking whether the g++ linker (/usr/bin/ld -m elf_x86_64) supports shared
libraries... yes
checking dynamic linker characteristics... (cached) GNU/Linux ld.so
checking how to hardcode library paths into programs... immediate
checking what extension is used for runtime loadable modules... .so
checking what variable specifies run-time module search path...
LD_LIBRARY_PATH
checking for the default library search path... /lib /usr/lib
/usr/lib64/dyninst /usr/lib64/iscsi /usr/lib64/mysql /usr/lib64/qt-3.3/lib
checking for library containing dlopen... -ldl
checking for dlerror... yes
checking for shl_load... (cached) no
checking for shl_load in -ldld... (cached) no
checking for dld_link in -ldld... no
checking for _ prefix in compiled symbols... no
checking whether deplibs are loaded by dlopen... yes
checking for argz.h... yes
checking for error_t... yes
checking for argz_add... yes
checking for argz_append... yes
checking for argz_count... yes
checking for argz_create_sep... yes
checking for argz_insert... yes
checking for argz_next... yes
checking for argz_stringify... yes
checking if argz actually works... yes
checking whether libtool supports -dlopen/-dlpreopen... yes
checking for ltdl.h... no
checking where to find libltdl headers... -I$(top_srcdir)/libltdl
checking where to find libltdl library...
$(top_build_prefix)libltdl/libltdlc.la
checking for unistd.h... yes
checking for dl.h... no
checking for sys/dl.h... no
checking for dld.h... no
checking for mach-o/dyld.h... no
checking for dirent.h... yes
checking for closedir... yes
checking for opendir... yes
checking for readdir... yes
checking for strlcat... no
checking for strlcpy... no
checking for library containing dlopen... (cached) -ldl
checking for dlerror... (cached) yes
checking for shl_load... (cached) no
checking for shl_load in -ldld... (cached) no
checking for dld_link in -ldld... (cached) no
checking for compiler variant... gcc
configure: inlining optimizations enabled: yes
checking for GNU atomic operations support... yes
configure: cbdata debugging enabled: no
configure: xmalloc stats display: no
checking for library containing shm_open... -lrt
checking for DiskIO modules to be enabled...  AIO Blocking DiskDaemon
DiskThreads IpcIo Mmapped
checking aio.h usability... yes
checking aio.h presence... yes
checking for aio.h... yes
checking for aio_read in -lrt... yes
configure: Native POSIX AIO support detected.
configure: Enabling AIO DiskIO module
configure: Enabling Blocking DiskIO module
configure: Enabling DiskDaemon DiskIO module
checking for pthread_create  in -lpthread... yes
configure: Enabling DiskThreads DiskIO module
configure: Enabling IpcIo DiskIO module
configure: Enabling Mmapped DiskIO module
configure: IO Modules built:  AIO Blocking DiskDaemon DiskThreads IpcIo
Mmapped
checking for available StoreIO modules...  aufs diskd rock ufs
configure: Store modules built:  aufs diskd rock ufs
configure: Removal policies to build: lru
configure: Disabling ESI processor
checking whether to support eCAP... yes, explicitly
checking for pkg-config... /usr/bin/pkg-config
checking pkg-config is at least version 0.9.0... yes
checking for EXT_LIBECAP... yes
configure: Web Cache Coordination Protocol enabled: yes
configure: Web Cache Coordination V2 Protocol enabled: yes
configure: Kill parent on shutdown hack enabled: no
configure: SNMP support enabled: yes
checking for windows.h... no
checking for sys/sockio.h... no
checking for sys/param.h... yes
checking for net/if_arp.h... yes
checking for net/route.h... yes
checking for net/if_dl.h... no
checking for sys/sysctl.h... yes
configure: EUI (MAC address) controls enabled: yes
configure: HTCP support enabled: yes
checking for nettle_md5_init in -lnettle... no
configure: Using Nettle cryptographic library: no
checking for crypt in -lcrypt... yes
checking for MD5Init in -lmd5... no
checking for LIBGNUTLS... no
checking for gnutls_certificate_verify_peers3 in -lgnutls... no
checking gnutls/gnutls.h usability... no
checking gnutls/gnutls.h presence... no
checking for gnutls/gnutls.h... no
checking gnutls/x509.h usability... no
checking gnutls/x509.h presence... no
checking for gnutls/x509.h... no
configure: GnuTLS library support: no
configure: OpenSSL library support: no
checking for LIB_KRB5... yes
configure: Try to find Kerberos headers in given path
checking gssapi.h usability... yes
checking gssapi.h presence... yes
checking for gssapi.h... yes
checking gssapi/gssapi.h usability... yes
checking gssapi/gssapi.h presence... yes
checking for gssapi/gssapi.h... yes
checking gssapi/gssapi_krb5.h usability... yes
checking gssapi/gssapi_krb5.h presence... yes
checking for gssapi/gssapi_krb5.h... yes
checking gssapi/gssapi_generic.h usability... yes
checking gssapi/gssapi_generic.h presence... yes
checking for gssapi/gssapi_generic.h... yes
checking krb5.h usability... yes
checking krb5.h presence... yes
checking for krb5.h... yes
checking com_err.h usability... yes
checking com_err.h presence... yes
checking for com_err.h... yes
checking et/com_err.h usability... yes
checking et/com_err.h presence... yes
checking for et/com_err.h... yes
checking profile.h usability... yes
checking profile.h presence... yes
checking for profile.h... yes
checking for error_message in -lcom_err... yes
checking for krb5_get_err_text in -lkrb5... no
checking for krb5_get_error_message in -lkrb5... yes
checking for krb5_free_error_message in -lkrb5... yes
checking for krb5_free_error_string in -lkrb5... no
checking whether krb5_kt_free_entry is declared... yes
checking for krb5_pac... yes
checking for krb5_kt_free_entry in -lkrb5... yes
checking for krb5_get_init_creds_keytab in -lkrb5... yes
checking for krb5_get_max_time_skew in -lkrb5... no
checking for krb5_get_profile in -lkrb5... yes
checking for profile_get_integer in -lkrb5... yes
checking for profile_release in -lkrb5... yes
checking for krb5_get_renewed_creds in -lkrb5... yes
checking for krb5_principal_get_realm in -lkrb5... no
checking for krb5_get_init_creds_opt_alloc in -lkrb5... yes
checking for krb5_get_init_creds_free requires krb5_context... yes
checking for gss_map_name_to_any... yes
checking for gsskrb5_extract_authz_data_from_sec_context... yes
checking for memory cache... yes
checking for memory keytab... yes
checking for working gssapi... yes
checking for spnego support... yes
checking for working krb5... yes
configure: MIT Kerberos library support: yes  -lgssapi_krb5 -lkrb5
-lk5crypto -lcom_err
checking for ldap_init in -lldap... no
checking for ber_init in -llber... no
checking for ldap_init in -lldap60... no
checking for prldap_init in -lprldap60... no
checking for ldapssl_init in -lssldap60... no
checking ldap.h usability... no
checking ldap.h presence... no
checking for ldap.h... no
checking lber.h usability... no
checking lber.h presence... no
checking for lber.h... no
checking mozldap/ldap.h usability... no
checking mozldap/ldap.h presence... no
checking for mozldap/ldap.h... no
checking for LDAP_OPT_DEBUG_LEVEL... no
checking for working ldap... no
checking for OpenLDAP... no
checking for Sun LDAP SDK... no
checking for Mozilla LDAP SDK... no
checking for LDAP_REBINDPROC_CALLBACK... no
checking for LDAP_REBIND_PROC... no
checking for LDAP_REBIND_FUNCTION... no
checking for LDAP_SCOPE_DEFAULT... no
checking for struct ldap_url_desc.lud_scheme... no
checking for ldapssl_client_init in -lldap... no
checking for ldap_url_desc2str in -lldap... no
checking for ldap_url_parse in -lldap... no
checking for ldap_start_tls_s in -lldap... no
configure: Forw/Via database enabled: no
configure: Cache Digests enabled: no
configure: enabling select syscall for net I/O: auto
configure: enabling poll syscall for net I/O: auto
checking sys/event.h usability... no
checking sys/event.h presence... no
checking for sys/event.h... no
checking for kqueue... no
configure: enabling kqueue for net I/O: no
configure: enabling epoll syscall for net I/O: auto
checking for library containing epoll_ctl... none required
checking sys/epoll.h usability... yes
checking sys/epoll.h presence... yes
checking for sys/epoll.h... yes
checking if epoll works... yes
configure: enabling /dev/poll for net I/O: auto
checking for ioctl... yes
checking for write... yes
checking sys/devpoll.h usability... no
checking sys/devpoll.h presence... no
checking for sys/devpoll.h... no
configure: HTTP violations support enabled: yes
configure: FreeBSD IPFW-based transparent proxying enabled: no
configure: IPF-based transparent proxying requested: no
configure: PF-based transparent proxying requested: no
configure: NAT lookups via /dev/pf: no
configure: Linux Netfilter support requested: auto
configure: Linux Netfilter Conntrack support requested: auto
checking for library containing nfct_query... no
checking libnetfilter_conntrack/libnetfilter_conntrack.h usability... no
checking libnetfilter_conntrack/libnetfilter_conntrack.h presence... no
checking for libnetfilter_conntrack/libnetfilter_conntrack.h... no
checking libnetfilter_conntrack/libnetfilter_conntrack_tcp.h usability... no
checking libnetfilter_conntrack/libnetfilter_conntrack_tcp.h presence... no
checking for libnetfilter_conntrack/libnetfilter_conntrack_tcp.h... no
checking size of long... 8
configure: Leak Finder enabled: no
configure: Support for X-Forwarded-For enabled: yes
configure: Support for Ident lookups enabled: yes
configure: Default hosts file set to: /etc/hosts
configure: Authentication support enabled: yes
checking for ldap.h... (cached) no
checking winldap.h usability... no
checking winldap.h presence... no
checking for winldap.h... no
configure: Basic auth helper LDAP ... found but cannot be built
checking for crypt... yes
checking for sys/types.h... yes
checking for rpc/rpc.h... yes
checking for rpcsvc/ypclnt.h... yes
checking for rpcsvc/yp_prot.h... yes
checking for crypt.h... yes
checking security/pam_appl.h usability... no
checking security/pam_appl.h presence... no
checking for security/pam_appl.h... no
configure: Basic auth helper PAM ... found but cannot be built
checking sasl/sasl.h usability... no
checking sasl/sasl.h presence... no
checking for sasl/sasl.h... no
checking sasl.h usability... no
checking sasl.h presence... no
checking for sasl.h... no
checking for sasl_errstring in -lsasl2... no
checking for sasl_errstring in -lsasl... no
configure: WARNING: Neither SASL nor SASL2 found
configure: Basic auth helper SASL ... found but cannot be built
checking for smbclient... no
configure: WARNING: Samba smbclient not found in default location.
basic_smb_auth may not work on this machine
checking w32api/windows.h usability... no
checking w32api/windows.h presence... no
checking for w32api/windows.h... no
checking for windows.h... (cached) no
checking for w32api/windows.h... (cached) no
checking for windows.h... (cached) no
configure: Basic auth helper SSPI ... found but cannot be built
checking pwd.h usability... yes
checking pwd.h presence... yes
checking for pwd.h... yes
checking for crypt... (cached) yes
checking for unistd.h... (cached) yes
checking for crypt.h... (cached) yes
checking shadow.h usability... yes
checking shadow.h presence... yes
checking for shadow.h... yes
configure: Basic auth helpers to be built:  DB MSNT-multi-domain NCSA NIS
POP3 RADIUS SMB SMB_LM fake getpwnam
checking for ldap.h... (cached) no
checking for winldap.h... (cached) no
configure: Digest auth helper LDAP ... found but cannot be built
checking for ldap.h... (cached) no
checking for winldap.h... (cached) no
configure: Digest auth helper eDirectory ... found but cannot be built
configure: Digest auth helpers to be built:  file
checking for w32api/windows.h... (cached) no
checking for windows.h... (cached) no
configure: Negotiate auth helper SSPI ... found but cannot be built
checking for vfork... yes
configure: Negotiate auth helpers to be built:  kerberos wrapper
checking for w32api/windows.h... (cached) no
checking for windows.h... (cached) no
configure: NTLM auth helper SSPI ... found but cannot be built
checking for w32api/windows.h... (cached) no
checking for windows.h... (cached) no
configure: NTLM auth helpers to be built:  fake smb_lm
checking machine/byte_swap.h usability... no
checking machine/byte_swap.h presence... no
checking for machine/byte_swap.h... no
checking sys/bswap.h usability... no
checking sys/bswap.h presence... no
checking for sys/bswap.h... no
checking endian.h usability... yes
checking endian.h presence... yes
checking for endian.h... yes
checking sys/endian.h usability... no
checking sys/endian.h presence... no
checking for sys/endian.h... no
checking for bswap_16... no
checking for bswap16... no
checking for bswap_32... no
checking for bswap32... no
checking for htole16... no
checking for __htole16... no
checking for htole32... no
checking for __htole32... no
checking for le16toh... no
checking for __le16toh... no
checking for le32toh... no
checking for __le32toh... no
configure: Log daemon helpers to be built:  DB file
configure: external acl helper AD_group ... found but cannot be built
checking for ldap.h... (cached) no
checking for winldap.h... (cached) no
configure: external acl helper LDAP_group ... found but cannot be built
checking for w32api/windows.h... (cached) no
checking for windows.h... (cached) no
configure: external acl helper LM_group ... found but cannot be built
checking for ldap.h... (cached) no
checking for winldap.h... (cached) no
configure: external acl helper eDirectory_userip ... found but cannot be
built
checking sys/socket.h usability... yes
checking sys/socket.h presence... yes
checking for sys/socket.h... yes
checking for sasl/sasl.h... (cached) no
checking for sasl.h... (cached) no
checking for sasl_errstring in -lsasl2... (cached) no
checking for sasl_errstring in -lsasl... (cached) no
configure: WARNING: Neither SASL nor SASL2 found
checking db_185.h usability... yes
checking db_185.h presence... yes
checking for db_185.h... yes
checking for pwd.h... (cached) yes
checking for wbinfo... no
configure: WARNING: Samba wbinfo not found in default location.
ext_wbinfo_group_acl may not work on this machine
configure: External acl helpers to be built:  SQL_session delayer
file_userip kerberos_ldap_group session time_quota unix_group wbinfo_group
configure: URL rewrite helper candidates: fake
configure: URL rewrite helpers to be built:  fake
configure: Store-ID rewrite helper candidates: file
configure: Store-ID rewrite helpers to be built:  file
configure: Valgrind debug support enabled: no
configure: MS Windows service mode enabled: no
configure: unlinkd enabled: yes
configure: Automatically print stack trace on fatal errors: no
configure: CPU profiling enabled: no
configure: X-Accelerator-Vary support enabled: no
configure: WARNING: cppunit does not appear to be installed. squid does not
require this, but code testing with 'make check' will fail.
checking cppunit/extensions/HelperMacros.h usability... no
checking cppunit/extensions/HelperMacros.h presence... no
checking for cppunit/extensions/HelperMacros.h... no
checking for dirent.h that defines DIR... yes
checking for library containing opendir... none required
checking for ANSI C header files... yes
checking arpa/inet.h usability... yes
checking arpa/inet.h presence... yes
checking for arpa/inet.h... yes
checking arpa/nameser.h usability... yes
checking arpa/nameser.h presence... yes
checking for arpa/nameser.h... yes
checking assert.h usability... yes
checking assert.h presence... yes
checking for assert.h... yes
checking bstring.h usability... no
checking bstring.h presence... no
checking for bstring.h... no
checking for crypt.h... (cached) yes
checking ctype.h usability... yes
checking ctype.h presence... yes
checking for ctype.h... yes
checking direct.h usability... no
checking direct.h presence... no
checking for direct.h... no
checking errno.h usability... yes
checking errno.h presence... yes
checking for errno.h... yes
checking execinfo.h usability... yes
checking execinfo.h presence... yes
checking for execinfo.h... yes
checking fcntl.h usability... yes
checking fcntl.h presence... yes
checking for fcntl.h... yes
checking fnmatch.h usability... yes
checking fnmatch.h presence... yes
checking for fnmatch.h... yes
checking getopt.h usability... yes
checking getopt.h presence... yes
checking for getopt.h... yes
checking glob.h usability... yes
checking glob.h presence... yes
checking for glob.h... yes
checking gnumalloc.h usability... no
checking gnumalloc.h presence... no
checking for gnumalloc.h... no
checking grp.h usability... yes
checking grp.h presence... yes
checking for grp.h... yes
checking ipl.h usability... no
checking ipl.h presence... no
checking for ipl.h... no
checking for lber.h... (cached) no
checking for ldap.h... (cached) no
checking libc.h usability... no
checking libc.h presence... no
checking for libc.h... no
checking limits.h usability... yes
checking limits.h presence... yes
checking for limits.h... yes
checking linux/posix_types.h usability... yes
checking linux/posix_types.h presence... yes
checking for linux/posix_types.h... yes
checking linux/types.h usability... yes
checking linux/types.h presence... yes
checking for linux/types.h... yes
checking malloc.h usability... yes
checking malloc.h presence... yes
checking for malloc.h... yes
checking math.h usability... yes
checking math.h presence... yes
checking for math.h... yes
checking memory.h usability... yes
checking memory.h presence... yes
checking for memory.h... yes
checking mount.h usability... no
checking mount.h presence... no
checking for mount.h... no
checking netdb.h usability... yes
checking netdb.h presence... yes
checking for netdb.h... yes
checking netinet/in.h usability... yes
checking netinet/in.h presence... yes
checking for netinet/in.h... yes
checking netinet/in_systm.h usability... yes
checking netinet/in_systm.h presence... yes
checking for netinet/in_systm.h... yes
checking netinet/tcp.h usability... yes
checking netinet/tcp.h presence... yes
checking for netinet/tcp.h... yes
checking paths.h usability... yes
checking paths.h presence... yes
checking for paths.h... yes
checking poll.h usability... yes
checking poll.h presence... yes
checking for poll.h... yes
checking for pwd.h... (cached) yes
checking regex.h usability... yes
checking regex.h presence... yes
checking for regex.h... yes
checking sched.h usability... yes
checking sched.h presence... yes
checking for sched.h... yes
checking siginfo.h usability... no
checking siginfo.h presence... no
checking for siginfo.h... no
checking signal.h usability... yes
checking signal.h presence... yes
checking for signal.h... yes
checking stdarg.h usability... yes
checking stdarg.h presence... yes
checking for stdarg.h... yes
checking stdbool.h usability... yes
checking stdbool.h presence... yes
checking for stdbool.h... yes
checking stddef.h usability... yes
checking stddef.h presence... yes
checking for stddef.h... yes
checking stdio.h usability... yes
checking stdio.h presence... yes
checking for stdio.h... yes
checking stdlib.h usability... yes
checking stdlib.h presence... yes
checking for stdlib.h... yes
checking string.h usability... yes
checking string.h presence... yes
checking for string.h... yes
checking strings.h usability... yes
checking strings.h presence... yes
checking for strings.h... yes
checking sys/bitypes.h usability... yes
checking sys/bitypes.h presence... yes
checking for sys/bitypes.h... yes
checking sys/file.h usability... yes
checking sys/file.h presence... yes
checking for sys/file.h... yes
checking sys/ioctl.h usability... yes
checking sys/ioctl.h presence... yes
checking for sys/ioctl.h... yes
checking sys/ipc.cc usability... no
checking sys/ipc.cc presence... no
checking for sys/ipc.cc... no
checking for sys/param.h... (cached) yes
checking sys/prctl.h usability... yes
checking sys/prctl.h presence... yes
checking for sys/prctl.h... yes
checking sys/md5.h usability... no
checking sys/md5.h presence... no
checking for sys/md5.h... no
checking sys/mman.h usability... yes
checking sys/mman.h presence... yes
checking for sys/mman.h... yes
checking sys/msg.h usability... yes
checking sys/msg.h presence... yes
checking for sys/msg.h... yes
checking sys/resource.h usability... yes
checking sys/resource.h presence... yes
checking for sys/resource.h... yes
checking sys/select.h usability... yes
checking sys/select.h presence... yes
checking for sys/select.h... yes
checking sys/shm.h usability... yes
checking sys/shm.h presence... yes
checking for sys/shm.h... yes
checking for sys/socket.h... (cached) yes
checking sys/stat.h usability... yes
checking sys/stat.h presence... yes
checking for sys/stat.h... yes
checking syscall.h usability... yes
checking syscall.h presence... yes
checking for syscall.h... yes
checking sys/syscall.h usability... yes
checking sys/syscall.h presence... yes
checking for sys/syscall.h... yes
checking sys/time.h usability... yes
checking sys/time.h presence... yes
checking for sys/time.h... yes
checking for sys/types.h... (cached) yes
checking sys/uio.h usability... yes
checking sys/uio.h presence... yes
checking for sys/uio.h... yes
checking sys/un.h usability... yes
checking sys/un.h presence... yes
checking for sys/un.h... yes
checking sys/vfs.h usability... yes
checking sys/vfs.h presence... yes
checking for sys/vfs.h... yes
checking sys/wait.h usability... yes
checking sys/wait.h presence... yes
checking for sys/wait.h... yes
checking syslog.h usability... yes
checking syslog.h presence... yes
checking for syslog.h... yes
checking time.h usability... yes
checking time.h presence... yes
checking for time.h... yes
checking for unistd.h... (cached) yes
checking utime.h usability... yes
checking utime.h presence... yes
checking for utime.h... yes
checking varargs.h usability... no
checking varargs.h presence... no
checking for varargs.h... no
checking byteswap.h usability... yes
checking byteswap.h presence... yes
checking for byteswap.h... yes
checking glib.h usability... no
checking glib.h presence... no
checking for glib.h... no
checking stdint.h usability... yes
checking stdint.h presence... yes
checking for stdint.h... yes
checking inttypes.h usability... yes
checking inttypes.h presence... yes
checking for inttypes.h... yes
checking db.h usability... yes
checking db.h presence... yes
checking for db.h... yes
checking for db_185.h... (cached) yes
checking wchar.h usability... yes
checking wchar.h presence... yes
checking for wchar.h... yes
checking for linux/netfilter_ipv4.h... yes
checking for linux/netfilter_ipv6/ip6_tables.h... no
checking for net/if.h... yes
checking for netinet/if_ether.h... yes
checking for netinet/icmp6.h... yes
checking for netinet/in.h... (cached) yes
checking for netinet/ip.h... yes
checking for netinet/ip6.h... yes
checking for netinet/ip_icmp.h... yes
checking for netinet/ipl.h... no
checking for net/pf/pfvar.h... no
checking for net/pfvar.h... no
checking for sys/mount.h... yes
checking for resolv.h... yes
checking for an ANSI C-conforming const... yes
checking whether byte ordering is bigendian... no
checking whether struct tm is in sys/time.h or time.h... time.h
checking for struct tm.tm_gmtoff... yes
checking for struct rusage... yes
checking for struct iphdr.ip_hl... yes
checking size of void *... 8
checking for int8_t... yes
checking for uint8_t... yes
checking for int16_t... yes
checking for uint16_t... yes
checking for int32_t... yes
checking for uint32_t... yes
checking for int64_t... yes
checking for uint64_t... yes
checking for pid_t... yes
checking for size_t... yes
checking for ssize_t... yes
checking for off_t... yes
checking for uid_t in sys/types.h... yes
checking for bool... yes
checking size of int64_t... 8
checking size of long... (cached) 8
checking size of size_t... 8
checking size of off_t... 8
checking size of size_t... (cached) 8
checking whether nullptr is supported... yes
checking whether nullptr_t is supported... yes
checking whether std::unique_ptr<T> is supported... yes
checking for pad128_t... no
checking for upad128_t... no
checking for mode_t... yes
checking for fd_mask... yes
checking for socklen_t... yes
checking for mtyp_t... no
checking for compiler %zu support... yes
checking for working alloca.h... yes
checking for alloca... yes
checking sys/capability.h usability... no
checking sys/capability.h presence... no
checking for sys/capability.h... no
checking for cap_clear_flag in -lcap... no
checking for operational libcap2 headers... no
configure: libcap support enabled: no
configure: libcap2 headers are ok: no
checking for library containing gethostbyname... none required
checking for library containing res_init... no
checking for library containing __res_search... -lresolv
checking for library containing bind... none required
checking for library containing opcom_stack_trace... no
checking for library containing strlcpy... no
checking for library containing yp_match... -lnsl
checking for unix domain sockets... yes
checking for malloc in -lgnumalloc... no
checking for main in -lmalloc... no
checking for library containing rint... -lm
checking for library containing log... none required
checking Default FD_SETSIZE value... 1024
checking for setrlimit... yes
checking Maximum number of filedescriptors we can open... 4096
configure: Default number of fieldescriptors: 4096
checking whether to enable IPv6... yes
checking for sin6_len field in struct sockaddr_in6... no
checking for ss_len field in struct sockaddr_storage... no
checking for sin_len field in struct sockaddr_in... no
checking whether dbopen is declared... yes
checking if dbopen needs -ldb... yes
checking for backtrace_symbols_fd... yes
checking for bcopy... yes
checking for eui64_aton... no
checking for fchmod... yes
checking for getdtablesize... yes
checking for getpagesize... yes
checking for getpass... yes
checking for getrlimit... yes
checking for getrusage... yes
checking for getspnam... yes
checking for gettimeofday... yes
checking for glob... yes
checking for lrand48... yes
checking for mallocblksize... no
checking for mallopt... yes
checking for memcpy... yes
checking for memmove... yes
checking for memrchr... yes
checking for memset... yes
checking for mkstemp... yes
checking for mktime... yes
checking for mstats... no
checking for poll... yes
checking for prctl... yes
checking for pthread_attr_setschedparam... yes
checking for pthread_attr_setscope... yes
checking for pthread_setschedparam... yes
checking for pthread_sigmask... no
checking for putenv... yes
checking for random... yes
checking for regcomp... yes
checking for regexec... yes
checking for regfree... yes
checking for res_init... no
checking for __res_init... yes
checking for rint... yes
checking for sched_getaffinity... yes
checking for sched_setaffinity... yes
checking for select... yes
checking for seteuid... yes
checking for setgroups... yes
checking for setpgrp... yes
checking for setsid... yes
checking for sigaction... yes
checking for snprintf... yes
checking for socketpair... yes
checking for srand48... yes
checking for srandom... yes
checking for sysconf... yes
checking for syslog... yes
checking for timegm... yes
checking for vsnprintf... yes
checking for drand48... yes
checking for initgroups... yes
checking for psignal... yes
checking for strerror... yes
checking for strtoll... yes
checking for tempnam... yes
checking whether getaddrinfo is declared... yes
checking whether getnameinfo is declared... yes
checking whether inet_ntop is declared... yes
checking whether inet_pton is declared... yes
configure: Using epoll for the IO loop.
checking if setresuid is actually implemented... yes
checking mswsock.h usability... no
checking mswsock.h presence... no
checking for mswsock.h... no
checking for constant CMSG_SPACE... yes
checking for struct cmsghdr... yes
checking for struct iovec... yes
checking for struct msghdr... yes
checking for struct sockaddr_un... yes
checking if strnstr is well implemented... no
checking if va_copy is implemented... yes
checking if __va_copy is implemented... yes
configure: IPF-based transparent proxying enabled: no
configure: Support for Netfilter-based interception proxy requested: yes
configure: WARNING: Missing needed capabilities (libcap 2.09+) for TPROXY
configure: WARNING: Linux Transparent Proxy (version 4+) support WILL NOT be
enabled
configure: WARNING: Reduced support to NAT Interception Proxy
configure: Linux Netfilter Conntrack support enabled: no
configure: ZPH QOS enabled: yes
configure: QOS netfilter mark preservation enabled: no
checking for regexec in -lregex... no
checking if the system-supplied regex lib actually works... yes
checking if GNUregex needs to be compiled... no
checking Default UDP send buffer size... 212992
checking Default UDP receive buffer size... 212992
checking Default TCP send buffer size... 16384
checking Default TCP receive buffer size... 87380
configure: Limiting receive buffer size to 64K
checking whether recv takes a pointer to void or char as second argument...
char*
char
checking if sys_errlist is already defined... yes
checking for system-provided MAXPATHLEN... yes
checking for libresolv _dns_ttl_ hack... no
checking for _res_ext.nsaddr_list... no
checking for _res._u._ext.nsaddrs... no
checking for _res.nsaddr_list... yes
checking for _res.ns_list... no
checking sys/statvfs.h usability... yes
checking sys/statvfs.h presence... yes
checking for sys/statvfs.h... yes
checking for working statvfs() interface... yes
configure: Multi-Language support enabled: yes
configure: BUILD LIBRARIES:
configure: BUILD EXTRA LIBRARIES: -lm -lnsl -lresolv -lrt
configure: BUILD OBJECTS:
configure: BUILD EXTRA OBJECTS:
configure: BUILD C FLAGS: -Wall -g -O2
configure: BUILD EXTRA C FLAGS: -Wall -Wpointer-arith -Wwrite-strings
-Wmissing-prototypes -Wmissing-declarations -Wcomments -Wshadow -Werror
-pipe -D_REENTRANT
configure: BUILD C++ FLAGS: -g -O2 -march=native -std=c++11
configure: BUILD EXTRA C++ FLAGS: -Wall -Wpointer-arith -Wwrite-strings
-Wcomments -Wshadow -Woverloaded-virtual -Werror -pipe -D_REENTRANT
configure: BUILD Tools C++ FLAGS: -march=native -g -O2 -march=native
-std=c++11
checking that generated files are newer than configure... done
configure: creating ./config.status
config.status: creating Makefile
config.status: creating compat/Makefile
config.status: creating lib/Makefile
config.status: creating lib/ntlmauth/Makefile
config.status: creating lib/libTrie/Makefile
config.status: creating lib/libTrie/test/Makefile
config.status: creating lib/profiler/Makefile
config.status: creating lib/rfcnb/Makefile
config.status: creating lib/smblib/Makefile
config.status: creating lib/snmplib/Makefile
config.status: creating scripts/Makefile
config.status: creating src/Makefile
config.status: creating src/anyp/Makefile
config.status: creating src/ftp/Makefile
config.status: creating src/base/Makefile
config.status: creating src/acl/Makefile
config.status: creating src/clients/Makefile
config.status: creating src/servers/Makefile
config.status: creating src/fs/Makefile
config.status: creating src/repl/Makefile
config.status: creating src/auth/Makefile
config.status: creating src/auth/basic/Makefile
config.status: creating src/auth/digest/Makefile
config.status: creating src/auth/negotiate/Makefile
config.status: creating src/auth/ntlm/Makefile
config.status: creating src/adaptation/Makefile
config.status: creating src/adaptation/icap/Makefile
config.status: creating src/adaptation/ecap/Makefile
config.status: creating src/comm/Makefile
config.status: creating src/esi/Makefile
config.status: creating src/eui/Makefile
config.status: creating src/format/Makefile
config.status: creating src/helper/Makefile
config.status: creating src/http/Makefile
config.status: creating src/icmp/Makefile
config.status: creating src/ident/Makefile
config.status: creating src/ip/Makefile
config.status: creating src/log/Makefile
config.status: creating src/ipc/Makefile
config.status: creating src/ssl/Makefile
config.status: creating src/mgr/Makefile
config.status: creating src/parser/Makefile
config.status: creating src/snmp/Makefile
config.status: creating contrib/Makefile
config.status: creating icons/Makefile
config.status: creating errors/Makefile
config.status: creating test-suite/Makefile
config.status: creating doc/Makefile
config.status: creating doc/manuals/Makefile
config.status: creating doc/release-notes/Makefile
config.status: creating helpers/Makefile
config.status: creating helpers/basic_auth/Makefile
config.status: creating helpers/basic_auth/DB/Makefile
config.status: creating helpers/basic_auth/fake/Makefile
config.status: creating helpers/basic_auth/getpwnam/Makefile
config.status: creating helpers/basic_auth/LDAP/Makefile
config.status: creating helpers/basic_auth/MSNT-multi-domain/Makefile
config.status: creating helpers/basic_auth/NCSA/Makefile
config.status: creating helpers/basic_auth/NIS/Makefile
config.status: creating helpers/basic_auth/PAM/Makefile
config.status: creating helpers/basic_auth/POP3/Makefile
config.status: creating helpers/basic_auth/RADIUS/Makefile
config.status: creating helpers/basic_auth/SASL/Makefile
config.status: creating helpers/basic_auth/SMB/Makefile
config.status: creating helpers/basic_auth/SMB_LM/Makefile
config.status: creating helpers/basic_auth/SSPI/Makefile
config.status: creating helpers/digest_auth/Makefile
config.status: creating helpers/digest_auth/eDirectory/Makefile
config.status: creating helpers/digest_auth/file/Makefile
config.status: creating helpers/digest_auth/LDAP/Makefile
config.status: creating helpers/ntlm_auth/Makefile
config.status: creating helpers/ntlm_auth/fake/Makefile
config.status: creating helpers/ntlm_auth/smb_lm/Makefile
config.status: creating helpers/ntlm_auth/SSPI/Makefile
config.status: creating helpers/negotiate_auth/Makefile
config.status: creating helpers/negotiate_auth/kerberos/Makefile
config.status: creating helpers/negotiate_auth/SSPI/Makefile
config.status: creating helpers/negotiate_auth/wrapper/Makefile
config.status: creating helpers/external_acl/Makefile
config.status: creating helpers/external_acl/AD_group/Makefile
config.status: creating helpers/external_acl/delayer/Makefile
config.status: creating helpers/external_acl/eDirectory_userip/Makefile
config.status: creating helpers/external_acl/file_userip/Makefile
config.status: creating helpers/external_acl/kerberos_ldap_group/Makefile
config.status: creating helpers/external_acl/LDAP_group/Makefile
config.status: creating helpers/external_acl/LM_group/Makefile
config.status: creating helpers/external_acl/session/Makefile
config.status: creating helpers/external_acl/SQL_session/Makefile
config.status: creating helpers/external_acl/unix_group/Makefile
config.status: creating helpers/external_acl/wbinfo_group/Makefile
config.status: creating helpers/external_acl/time_quota/Makefile
config.status: creating helpers/log_daemon/Makefile
config.status: creating helpers/log_daemon/DB/Makefile
config.status: creating helpers/log_daemon/file/Makefile
config.status: creating helpers/url_rewrite/Makefile
config.status: creating helpers/url_rewrite/fake/Makefile
config.status: creating helpers/ssl/Makefile
config.status: creating helpers/storeid_rewrite/Makefile
config.status: creating helpers/storeid_rewrite/file/Makefile
config.status: creating tools/Makefile
config.status: creating tools/purge/Makefile
config.status: creating tools/squidclient/Makefile
config.status: creating tools/systemd/Makefile
config.status: creating tools/sysvinit/Makefile
config.status: creating include/autoconf.h
config.status: include/autoconf.h is unchanged
config.status: executing depfiles commands
config.status: executing libtool commands
=== configuring in libltdl (/root/install/squid-3.5.26/libltdl)
configure: running /bin/sh ./configure --disable-option-checking
'--prefix=/usr/local/squid'  '--enable-ecap'
'PKG_CONFIG_PATH=/usr/local/lib/pkgconfig'
'EXT_LIBECAP_CFLAGS=/usr/local/lib' 'EXT_LIBECAP_LIBS=/usr/local/lib'
'--enable-ltdl-convenience' --cache-file=/dev/null --srcdir=.
checking for a BSD-compatible install... /usr/bin/install -c
checking whether build environment is sane... yes
checking for a thread-safe mkdir -p... /usr/bin/mkdir -p
checking for gawk... gawk
checking whether make sets $(MAKE)... yes
checking whether make supports nested variables... yes
checking whether make supports nested variables... (cached) yes
checking build system type... x86_64-pc-linux-gnu
checking host system type... x86_64-pc-linux-gnu
checking how to print strings... printf
checking for style of include used by make... GNU
checking for gcc... gcc
checking whether the C compiler works... yes
checking for C compiler default output file name... a.out
checking for suffix of executables...
checking whether we are cross compiling... no
checking for suffix of object files... o
checking whether we are using the GNU C compiler... yes
checking whether gcc accepts -g... yes
checking for gcc option to accept ISO C89... none needed
checking whether gcc understands -c and -o together... yes
checking dependency style of gcc... gcc3
checking for a sed that does not truncate output... /usr/bin/sed
checking for grep that handles long lines and -e... /usr/bin/grep
checking for egrep... /usr/bin/grep -E
checking for fgrep... /usr/bin/grep -F
checking for ld used by gcc... /usr/bin/ld
checking if the linker (/usr/bin/ld) is GNU ld... yes
checking for BSD- or MS-compatible name lister (nm)... /usr/bin/nm -B
checking the name lister (/usr/bin/nm -B) interface... BSD nm
checking whether ln -s works... yes
checking the maximum length of command line arguments... 1572864
checking how to convert x86_64-pc-linux-gnu file names to
x86_64-pc-linux-gnu format... func_convert_file_noop
checking how to convert x86_64-pc-linux-gnu file names to toolchain
format... func_convert_file_noop
checking for /usr/bin/ld option to reload object files... -r
checking for objdump... objdump
checking how to recognize dependent libraries... pass_all
checking for dlltool... no
checking how to associate runtime and link libraries... printf %s\n
checking for ar... ar
checking for archiver @FILE support... @
checking for strip... strip
checking for ranlib... ranlib
checking command to parse /usr/bin/nm -B output from gcc object... ok
checking for sysroot... no
checking for a working dd... /usr/bin/dd
checking how to truncate binary pipes... /usr/bin/dd bs=4096 count=1
checking for mt... mt
checking if mt is a manifest tool... no
checking how to run the C preprocessor... gcc -E
checking for ANSI C header files... yes
checking for sys/types.h... yes
checking for sys/stat.h... yes
checking for stdlib.h... yes
checking for string.h... yes
checking for memory.h... yes
checking for strings.h... yes
checking for inttypes.h... yes
checking for stdint.h... yes
checking for unistd.h... yes
checking for dlfcn.h... yes
checking for objdir... .libs
checking if gcc supports -fno-rtti -fno-exceptions... no
checking for gcc option to produce PIC... -fPIC -DPIC
checking if gcc PIC flag -fPIC -DPIC works... yes
checking if gcc static flag -static works... no
checking if gcc supports -c -o file.o... yes
checking if gcc supports -c -o file.o... (cached) yes
checking whether the gcc linker (/usr/bin/ld -m elf_x86_64) supports shared
libraries... yes
checking whether -lc should be explicitly linked in... no
checking dynamic linker characteristics... GNU/Linux ld.so
checking how to hardcode library paths into programs... immediate
checking for shl_load... no
checking for shl_load in -ldld... no
checking for dlopen... no
checking for dlopen in -ldl... yes
checking whether a program can dlopen itself... yes
checking whether a statically linked program can dlopen itself... yes
checking whether stripping libraries is possible... yes
checking if libtool supports shared libraries... yes
checking whether to build shared libraries... yes
checking whether to build static libraries... yes
checking what extension is used for runtime loadable modules... .so
checking what variable specifies run-time module search path...
LD_LIBRARY_PATH
checking for the default library search path... /lib /usr/lib
/usr/lib64/dyninst /usr/lib64/iscsi /usr/lib64/mysql /usr/lib64/qt-3.3/lib
checking for library containing dlopen... -ldl
checking for dlerror... yes
checking for shl_load... (cached) no
checking for shl_load in -ldld... (cached) no
checking for dld_link in -ldld... no
checking for _ prefix in compiled symbols... no
checking whether deplibs are loaded by dlopen... yes
checking for argz.h... yes
checking for error_t... yes
checking for argz_add... yes
checking for argz_append... yes
checking for argz_count... yes
checking for argz_create_sep... yes
checking for argz_insert... yes
checking for argz_next... yes
checking for argz_stringify... yes
checking if argz actually works... yes
checking whether libtool supports -dlopen/-dlpreopen... yes
checking for unistd.h... (cached) yes
checking for dl.h... no
checking for sys/dl.h... no
checking for dld.h... no
checking for mach-o/dyld.h... no
checking for dirent.h... yes
checking for closedir... yes
checking for opendir... yes
checking for readdir... yes
checking for strlcat... no
checking for strlcpy... no
checking that generated files are newer than configure... done
configure: creating ./config.status
config.status: creating Makefile
config.status: creating config.h
config.status: config.h is unchanged
config.status: executing depfiles commands
config.status: executing libtool commands

-----Original Message-----
From: Norbert Naveen [mailto:norbert.naveen at tayana.in] 
Sent: Monday, June 12, 2017 10:59 PM
To: 'Alex Rousskov' <rousskov at measurement-factory.com>;
'squid-users at lists.squid-cache.org' <squid-users at lists.squid-cache.org>
Subject: RE: [squid-users] Error Compiling squid-3.5.26 with libecap-1.0.1
on CentOS Linux release 7.3.1611

Hello Alex
  Sorry and Thanks for the response , I have attached Outputs of both
Configure and Make
  And  libecap.pc contents are as below 

# cat /usr/local/lib/pkgconfig/libecap.pc
prefix=/usr/local
exec_prefix=${prefix}
libdir=${exec_prefix}/lib
includedir=${prefix}/include

Name: eCAP
Description: Allows a network application to outsource content analysis and
adaptation to a loadable module.
URL: http://www.e-cap.org/
Version: 1.0.1
Libs: -L${libdir} -lecap
Cflags: -I${includedir}


&&& From make the first instance would be 

libtool: link: rm -f .libs/squid.nm .libs/squid.nmS .libs/squid.nmT
libtool: link: rm -f ".libs/squid.nmI"
libtool: link: (cd .libs && gcc -Wall -g -O2 -c -fno-builtin "squidS.c")
libtool: link: rm -f ".libs/squidS.c" ".libs/squid.nm" ".libs/squid.nmS"
".libs/squid.nmT" ".libs/squid.nmI"

libtool: link: g++ -Wall -Wpointer-arith -Wwrite-strings -Wcomments -Wshadow
-Woverloaded-virtual -Werror -pipe -D_REENTRANT -g -O2 -march=native
-std=c++11 .libs/squidS.o -g -o squid AclRegs.o AuthReg.o AccessLogEntry.o
AsyncEngine.o YesNoNone.o cache_cf.o CacheDigest.o cache_manager.o carp.o
cbdata.o ChunkedCodingParser.o client_db.o client_side.o client_side_reply.o
client_side_request.o BodyPipe.o clientStream.o CollapsedForwarding.o
CompletionDispatcher.o ConfigOption.o ConfigParser.o CpuAffinity.o
CpuAffinityMap.o CpuAffinitySet.o debug.o disk.o DiskIO/DiskIOModule.o
DiskIO/ReadRequest.o DiskIO/WriteRequest.o dlink.o dns_internal.o
DnsLookupDetails.o errorpage.o ETag.o event.o EventLoop.o external_acl.o
ExternalACLEntry.o FadingCounter.o fatal.o fd.o fde.o filemap.o fqdncache.o
FwdState.o gopher.o helper.o htcp.o http.o HttpHdrCc.o HttpHdrRange.o
HttpHdrSc.o HttpHdrScTarget.o HttpHdrContRange.o HttpHeader.o
HttpHeaderTools.o HttpBody.o HttpMsg.o HttpParser.o HttpReply.o
RequestFlags.o HttpRequest.o HttpRequestMethod.o icp_v2.o icp_v3.o int.o
internal.o ipc.o ipcache.o SquidList.o main.o MasterXaction.o mem.o
mem_node.o MemBuf.o MemObject.o mime.o mime_header.o multicast.o neighbors.o
Notes.o Packer.o Parsing.o pconn.o peer_digest.o peer_proxy_negotiate_auth.o
peer_select.o peer_sourcehash.o peer_userhash.o PeerPoolMgr.o redirect.o
refresh.o RemovalPolicy.o send-announce.o MemBlob.o SBuf.o SBufExceptions.o
SBufDetailedStats.o SBufStatsAction.o snmp_core.o snmp_agent.o SquidMath.o
SquidNew.o stat.o StatCounters.o StatHist.o String.o StrList.o stmem.o
store.o StoreFileSystem.o store_io.o StoreIOState.o store_client.o
store_digest.o store_dir.o store_key_md5.o store_log.o store_rebuild.o
store_swapin.o store_swapmeta.o store_swapout.o StoreMetaUnpacker.o
StoreMeta.o StoreMetaMD5.o StoreMetaSTD.o StoreMetaSTDLFS.o StoreMetaURL.o
StoreMetaVary.o StoreStats.o StoreSwapLogData.o SwapDir.o Transients.o
MemStore.o time.o tools.o tunnel.o unlinkd.o url.o urn.o wccp.o wccp2.o
whois.o wordlist.o LoadableModule.o LoadableModules.o
DiskIO/DiskIOModules_gen.o err_type.o err_detail_type.o globals.o
hier_code.o icp_opcode.o LogTags.o lookup_t.o repl_modules.o swap_log_op.o
DiskIO/AIO/AIODiskIOModule.o DiskIO/Blocking/BlockingDiskIOModule.o
DiskIO/DiskDaemon/DiskDaemonDiskIOModule.o
DiskIO/DiskThreads/DiskThreadsDiskIOModule.o
DiskIO/IpcIo/IpcIoDiskIOModule.o DiskIO/Mmapped/MmappedDiskIOModule.o
-Wl,--export-dynamic  /root/install/squid-3.5.26/libltdl/./.libs/dlopen.a
auth/.libs/libacls.a ident/.libs/libident.a acl/.libs/libacls.a
acl/.libs/libstate.a auth/.libs/libauth.a libAIO.a libBlocking.a
libDiskDaemon.a libDiskThreads.a libIpcIo.a libMmapped.a acl/.libs/libapi.a
base/.libs/libbase.a ./.libs/libsquid.a ip/.libs/libip.a fs/.libs/libfs.a
ipc/.libs/libipc.a mgr/.libs/libmgr.a anyp/.libs/libanyp.a
comm/.libs/libcomm.a eui/.libs/libeui.a helper/.libs/libhelper.a
http/.libs/libsquid-http.a icmp/.libs/libicmp.a icmp/.libs/libicmp-core.a
log/.libs/liblog.a format/.libs/libformat.a clients/.libs/libclients.a
servers/.libs/libservers.a ftp/.libs/libftp.a repl/liblru.a -lpthread
-lcrypt adaptation/.libs/libadaptation.a snmp/.libs/libsnmp.a
../lib/snmplib/.libs/libsnmplib.a parser/.libs/libsquid-parser.a
../lib/.libs/libmisccontainers.a ../lib/.libs/libmiscencoding.a
../lib/.libs/libmiscutil.a -lgssapi_krb5 -lkrb5 -lk5crypto -lcom_err
../compat/.libs/libcompat-squid.a -lm -lnsl -lresolv -lrt -L..
../libltdl/.libs/libltdlc.a -ldl
adaptation/.libs/libadaptation.a(libsquid_ecap_la-Host.o): In function
`Adaptation::Ecap::Host::Host()':
/root/install/squid-3.5.26/src/adaptation/ecap/Host.cc:41: undefined
reference to `libecap::headerTransferEncoding'
/root/install/squid-3.5.26/src/adaptation/ecap/Host.cc:41: undefined
reference to `libecap::Name::assignHostId(int) const'
/root/install/squid-3.5.26/src/adaptation/ecap/Host.cc:42: undefined
reference to `libecap::headerReferer'
/root/install/squid-3.5.26/src/adaptation/ecap/Host.cc:42: undefined
reference to `libecap::Name::assignHostId(int) const'
/root/install/squid-3.5.26/src/adaptation/ecap/Host.cc:43: undefined
reference to `libecap::headerContentLength'
/root/install/squid-3.5.26/src/adaptation/ecap/Host.cc:43: undefined
reference to `libecap::Name::assignHostId(int) const'
/root/install/squid-3.5.26/src/adaptation/ecap/Host.cc:44: undefined
reference to `libecap::headerVia'
/root/install/squid-3.5.26/src/adaptation/ecap/Host.cc:44: undefined
reference to `libecap::Name::assignHostId(int) const'
/root/install/squid-3.5.26/src/adaptation/ecap/Host.cc:48: undefined
reference to `libecap::protocolHttp'
/root/install/squid-3.5.26/src/adaptation/ecap/Host.cc:48: undefined
reference to `libecap::Name::assignHostId(int) const'
/root/install/squid-3.5.26/src/adaptation/ecap/Host.cc:49: undefined
reference to `libecap::protocolHttps'
/root/install/squid-3.5.26/src/adaptation/ecap/Host.cc:49: undefined
reference to `libecap::Name::assignHostId(int) const'
/root/install/squid-3.5.26/src/adaptation/ecap/Host.cc:50: undefined
reference to `libecap::protocolFtp'
/root/install/squid-3.5.26/src/adaptation/ecap/Host.cc:50: undefined
reference to `libecap::Name::assignHostId(int) const'
/root/install/squid-3.5.26/src/adaptation/ecap/Host.cc:51: undefined
reference to `libecap::protocolGopher'
/root/install/squid-3.5.26/src/adaptation/ecap/Host.cc:51: undefined
reference to `libecap::Name::assignHostId(int) const'
/root/install/squid-3.5.26/src/adaptation/ecap/Host.cc:52: undefined
reference to `libecap::protocolWais'
/root/install/squid-3.5.26/src/adaptation/ecap/Host.cc:52: undefined
reference to `libecap::Name::assignHostId(int) const'
/root/install/squid-3.5.26/src/adaptation/ecap/Host.cc:53: undefined
reference to `libecap::protocolUrn'
/root/install/squid-3.5.26/src/adaptation/ecap/Host.cc:53: undefined
reference to `libecap::Name::assignHostId(int) const'
/root/install/squid-3.5.26/src/adaptation/ecap/Host.cc:54: undefined
reference to `libecap::protocolWhois'
/root/install/squid-3.5.26/src/adaptation/ecap/Host.cc:54: undefined
reference to `libecap::Name::assignHostId(int) const'
/root/install/squid-3.5.26/src/adaptation/ecap/Host.cc:55: undefined
reference to `libecap::Name::assignHostId(int) const'
/root/install/squid-3.5.26/src/adaptation/ecap/Host.cc:56: undefined
reference to `libecap::Name::assignHostId(int) const'
/root/install/squid-3.5.26/src/adaptation/ecap/Host.cc:58: undefined
reference to `libecap::Name::assignHostId(int) const'
/root/install/squid-3.5.26/src/adaptation/ecap/Host.cc:60: undefined
reference to `libecap::Name::assignHostId(int) const'
adaptation/.libs/libadaptation.a(libsquid_ecap_la-Host.o):/root/install/squi
d-3.5.26/src/adaptation/ecap/Host.cc:61: more undefined references to
`libecap::Name::assignHostId(int) const' follow
adaptation/.libs/libadaptation.a(libsquid_ecap_la-Host.o): In function
`Adaptation::Ecap::Host::Register()':
/root/install/squid-3.5.26/src/adaptation/ecap/Host.cc:178: undefined
reference to `libecap::VersionString()'
/root/install/squid-3.5.26/src/adaptation/ecap/Host.cc:180: undefined
reference to
`libecap::RegisterHost(std::tr1::shared_ptr<libecap::host::Host> const&)'
adaptation/.libs/libadaptation.a(libsquid_ecap_la-Host.o): In function
`_GLOBAL__sub_I_Host.cc':
/root/install/squid-3.5.26/src/adaptation/ecap/Host.cc:22: undefined
reference to `libecap::Name::NextId()'
/root/install/squid-3.5.26/src/adaptation/ecap/Host.cc:22: undefined
reference to `libecap::Name::Name(std::string const&, int)'
/root/install/squid-3.5.26/src/adaptation/ecap/Host.cc:23: undefined
reference to `libecap::Name::NextId()'
/root/install/squid-3.5.26/src/adaptation/ecap/Host.cc:23: undefined
reference to `libecap::Name::Name(std::string const&, int)'
/root/install/squid-3.5.26/src/adaptation/ecap/Host.cc:24: undefined
reference to `libecap::Name::NextId()'
/root/install/squid-3.5.26/src/adaptation/ecap/Host.cc:24: undefined
reference to `libecap::Name::Name(std::string const&, int)'
/root/install/squid-3.5.26/src/adaptation/ecap/Host.cc:26: undefined
reference to `libecap::Name::NextId()'
/root/install/squid-3.5.26/src/adaptation/ecap/Host.cc:26: undefined
reference to `libecap::Name::Name(std::string const&, int)'
/root/install/squid-3.5.26/src/adaptation/ecap/Host.cc:28: undefined
reference to `libecap::Name::NextId()'
/root/install/squid-3.5.26/src/adaptation/ecap/Host.cc:28: undefined
reference to `libecap::Name::Name(std::string const&, int)'
/root/install/squid-3.5.26/src/adaptation/ecap/Host.cc:29: undefined
reference to `libecap::Name::NextId()'
/root/install/squid-3.5.26/src/adaptation/ecap/Host.cc:29: undefined
reference to `libecap::Name::Name(std::string const&, int)'
/root/install/squid-3.5.26/src/adaptation/ecap/Host.cc:31: undefined
reference to `libecap::Name::NextId()'
/root/install/squid-3.5.26/src/adaptation/ecap/Host.cc:31: undefined
reference to `libecap::Name::Name(std::string const&, int)'
adaptation/.libs/libadaptation.a(libsquid_ecap_la-MessageRep.o): In function
`Adaptation::Ecap::MessageRep::~MessageRep()':
/usr/local/include/libecap/common/message.h:16: undefined reference to
`vtable for libecap::Message'
adaptation/.libs/libadaptation.a(libsquid_ecap_la-MessageRep.o): In function
`Adaptation::Ecap::RequestLineRep::method(libecap::Name const&)':
/root/install/squid-3.5.26/src/adaptation/ecap/MessageRep.cc:231: undefined
reference to `libecap::Name::assignedHostId() const'
adaptation/.libs/libadaptation.a(libsquid_ecap_la-MessageRep.o): In function
`Adaptation::Ecap::RequestLineRep::uri() const':
/root/install/squid-3.5.26/src/adaptation/ecap/MessageRep.cc:225: undefined
reference to `libecap::Area::FromTempBuffer(char const*, unsigned long)'
adaptation/.libs/libadaptation.a(libsquid_ecap_la-MessageRep.o): In function
`Adaptation::Ecap::StatusLineRep::reasonPhrase() const':
/root/install/squid-3.5.26/src/adaptation/ecap/MessageRep.cc:321: undefined
reference to `libecap::Area::FromTempString(std::string const&)'
adaptation/.libs/libadaptation.a(libsquid_ecap_la-MessageRep.o): In function
`Adaptation::Ecap::RequestLineRep::uri(libecap::Area const&)':
/root/install/squid-3.5.26/src/adaptation/ecap/MessageRep.cc:213: undefined
reference to `libecap::Area::toString() const'
adaptation/.libs/libadaptation.a(libsquid_ecap_la-MessageRep.o): In function
`Adaptation::Ecap::RequestLineRep::method() const':
/usr/local/include/libecap/common/name.h:14: undefined reference to
`libecap::methodDelete'
/usr/local/include/libecap/common/name.h:14: undefined reference to
`libecap::methodGet'
/usr/local/include/libecap/common/name.h:14: undefined reference to
`libecap::methodPost'
/usr/local/include/libecap/common/name.h:14: undefined reference to
`libecap::methodPut'
/usr/local/include/libecap/common/name.h:14: undefined reference to
`libecap::methodHead'
/usr/local/include/libecap/common/name.h:14: undefined reference to
`libecap::methodConnect'
/usr/local/include/libecap/common/name.h:14: undefined reference to
`libecap::methodTrace'
adaptation/.libs/libadaptation.a(libsquid_ecap_la-MessageRep.o): In function
`Adaptation::Ecap::RequestLineRep::method() const':
/root/install/squid-3.5.26/src/adaptation/ecap/MessageRep.cc:262: undefined
reference to `libecap::Name::Name(std::string const&)'
adaptation/.libs/libadaptation.a(libsquid_ecap_la-MessageRep.o): In function
`Adaptation::Ecap::HeaderRep::image() const':
/root/install/squid-3.5.26/src/adaptation/ecap/MessageRep.cc:99: undefined
reference to `libecap::Area::FromTempBuffer(char const*, unsigned long)'
adaptation/.libs/libadaptation.a(libsquid_ecap_la-MessageRep.o): In function
`Adaptation::Ecap::HeaderRep::TranslateHeaderId(libecap::Name const&)':
/root/install/squid-3.5.26/src/adaptation/ecap/MessageRep.cc:116: undefined
reference to `libecap::Name::assignedHostId() const'
adaptation/.libs/libadaptation.a(libsquid_ecap_la-MessageRep.o): In function
`Adaptation::Ecap::HeaderRep::add(libecap::Name const&, libecap::Area
const&)':
/root/install/squid-3.5.26/src/adaptation/ecap/MessageRep.cc:58: undefined
reference to `libecap::Area::toString() const'
adaptation/.libs/libadaptation.a(libsquid_ecap_la-MessageRep.o): In function
`Adaptation::Ecap::HeaderRep::value(libecap::Name const&) const':
/root/install/squid-3.5.26/src/adaptation/ecap/MessageRep.cc:50: undefined
reference to `libecap::Area::FromTempString(std::string const&)'
adaptation/.libs/libadaptation.a(libsquid_ecap_la-MessageRep.o): In function
`Adaptation::Ecap::FirstLineRep::protocol() const':
/root/install/squid-3.5.26/src/adaptation/ecap/MessageRep.cc:182: undefined
reference to `libecap::Name::Name()'
adaptation/.libs/libadaptation.a(libsquid_ecap_la-MessageRep.o): In function
`Adaptation::Ecap::FirstLineRep::protocol() const':
/usr/local/include/libecap/common/name.h:14: undefined reference to
`libecap::protocolHttp'
/usr/local/include/libecap/common/name.h:14: undefined reference to
`libecap::protocolFtp'
/usr/local/include/libecap/common/name.h:14: undefined reference to
`libecap::protocolHttps'
/usr/local/include/libecap/common/name.h:14: undefined reference to
`libecap::protocolGopher'
/usr/local/include/libecap/common/name.h:14: undefined reference to
`libecap::protocolWais'
/usr/local/include/libecap/common/name.h:14: undefined reference to
`libecap::protocolUrn'
/usr/local/include/libecap/common/name.h:14: undefined reference to
`libecap::protocolWhois'
adaptation/.libs/libadaptation.a(libsquid_ecap_la-MessageRep.o): In function
`Adaptation::Ecap::FirstLineRep::TranslateProtocolId(libecap::Name const&)':
/root/install/squid-3.5.26/src/adaptation/ecap/MessageRep.cc:195: undefined
reference to `libecap::Name::assignedHostId() const'
adaptation/.libs/libadaptation.a(libsquid_ecap_la-MessageRep.o): In function
`Adaptation::Ecap::MessageRep::MessageRep(HttpMsg*)':
/usr/local/include/libecap/common/message.h:16: undefined reference to
`vtable for libecap::Message'
adaptation/.libs/libadaptation.a(libsquid_ecap_la-MessageRep.o): In function
`Adaptation::Ecap::HeaderRep::visitEach(libecap::NamedValueVisitor&) const':
/root/install/squid-3.5.26/src/adaptation/ecap/MessageRep.cc:83: undefined
reference to `libecap::Name::Name(std::string const&)'
/root/install/squid-3.5.26/src/adaptation/ecap/MessageRep.cc:84: undefined
reference to `libecap::Name::assignHostId(int) const'
adaptation/.libs/libadaptation.a(libsquid_ecap_la-MessageRep.o):(.data.rel.r
o._ZTIN10Adaptation4Ecap10MessageRepE[_ZTIN10Adaptation4Ecap10MessageRepE]+0
x10): undefined reference to `typeinfo for libecap::Message'
adaptation/.libs/libadaptation.a(libsquid_ecap_la-MessageRep.o):(.data.rel.r
o._ZTVN10Adaptation4Ecap10MessageRepE[_ZTVN10Adaptation4Ecap10MessageRepE]+0
x60): undefined reference to `libecap::Message::addTrailer()'
adaptation/.libs/libadaptation.a(libsquid_ecap_la-ServiceRep.o): In function
`Adaptation::Ecap::ConfigRep::option(libecap::Name const&) const':
/root/install/squid-3.5.26/src/adaptation/ecap/ServiceRep.cc:86: undefined
reference to `libecap::Name::Name(std::string const&)'
adaptation/.libs/libadaptation.a(libsquid_ecap_la-ServiceRep.o): In function
`Adaptation::Ecap::ConfigRep::visitEachOption(libecap::NamedValueVisitor&)
const':
/root/install/squid-3.5.26/src/adaptation/ecap/ServiceRep.cc:102: undefined
reference to `libecap::Area::FromTempString(std::string const&)'
/root/install/squid-3.5.26/src/adaptation/ecap/ServiceRep.cc:102: undefined
reference to `libecap::Name::Name(std::string const&)'
adaptation/.libs/libadaptation.a(libsquid_ecap_la-XactionRep.o): In function
`Adaptation::Ecap::XactionRep::clientIpValue() const':
/root/install/squid-3.5.26/src/adaptation/ecap/XactionRep.cc:140: undefined
reference to `libecap::Area::FromTempBuffer(char const*, unsigned long)'
adaptation/.libs/libadaptation.a(libsquid_ecap_la-XactionRep.o): In function
`Adaptation::Ecap::XactionRep::usernameValue() const':
/root/install/squid-3.5.26/src/adaptation/ecap/XactionRep.cc:155: undefined
reference to `libecap::Area::FromTempBuffer(char const*, unsigned long)'
/root/install/squid-3.5.26/src/adaptation/ecap/XactionRep.cc:158: undefined
reference to `libecap::Area::FromTempBuffer(char const*, unsigned long)'
adaptation/.libs/libadaptation.a(libsquid_ecap_la-XactionRep.o): In function
`Adaptation::Ecap::XactionRep::metaValue(libecap::Name const&) const':
/root/install/squid-3.5.26/src/adaptation/ecap/XactionRep.cc:194: undefined
reference to `libecap::Area::FromTempString(std::string const&)'
adaptation/.libs/libadaptation.a(libsquid_ecap_la-XactionRep.o): In function
`Adaptation::Ecap::XactionRep::answer()':
/root/install/squid-3.5.26/src/adaptation/ecap/XactionRep.cc:328: undefined
reference to `typeinfo for libecap::Message'
adaptation/.libs/libadaptation.a(libsquid_ecap_la-XactionRep.o): In function
`Adaptation::Ecap::XactionRep::status() const':
/root/install/squid-3.5.26/src/adaptation/ecap/XactionRep.cc:718: undefined
reference to `typeinfo for libecap::Message'
adaptation/.libs/libadaptation.a(libsquid_ecap_la-XactionRep.o): In function
`Adaptation::Ecap::XactionRep::moveAbContent()':
/root/install/squid-3.5.26/src/adaptation/ecap/XactionRep.cc:681: undefined
reference to `libecap::nsize'
adaptation/.libs/libadaptation.a(libsquid_ecap_la-XactionRep.o): In function
`Adaptation::Ecap::XactionRep::visitEachMetaHeader(libecap::NamedValueVisito
r&) const':
/root/install/squid-3.5.26/src/adaptation/ecap/XactionRep.cc:216: undefined
reference to `libecap::Name::Name(std::string const&)'
/root/install/squid-3.5.26/src/adaptation/ecap/XactionRep.cc:217: undefined
reference to `libecap::Area::FromTempString(std::string const&)'
adaptation/.libs/libadaptation.a(libsquid_ecap_la-XactionRep.o): In function
`Adaptation::Ecap::XactionRep::updateHistory(HttpMsg*)':
/root/install/squid-3.5.26/src/adaptation/ecap/XactionRep.cc:476: undefined
reference to `libecap::Name::Name(std::string const&)'
/root/install/squid-3.5.26/src/adaptation/ecap/XactionRep.cc:478: undefined
reference to `libecap::Area::toString() const'
/root/install/squid-3.5.26/src/adaptation/ecap/XactionRep.cc:485: undefined
reference to `libecap::metaNextServices'
/root/install/squid-3.5.26/src/adaptation/ecap/XactionRep.cc:488: undefined
reference to `libecap::Area::toString() const'
adaptation/.libs/libadaptation.a(libsquid_ecap_la-XactionRep.o): In function
`Adaptation::Ecap::XactionRep::masterxSharedValue(libecap::Name const&)
const':
/root/install/squid-3.5.26/src/adaptation/ecap/XactionRep.cc:175: undefined
reference to `libecap::Area::FromTempBuffer(char const*, unsigned long)'
adaptation/.libs/libadaptation.a(libsquid_ecap_la-XactionRep.o): In function
`Adaptation::Ecap::XactionRep::visitEachOption(libecap::NamedValueVisitor&)
const':
/root/install/squid-3.5.26/src/adaptation/ecap/XactionRep.cc:106: undefined
reference to `libecap::metaClientIp'
/root/install/squid-3.5.26/src/adaptation/ecap/XactionRep.cc:108: undefined
reference to `libecap::metaUserName'
/root/install/squid-3.5.26/src/adaptation/ecap/XactionRep.cc:111: undefined
reference to `libecap::Name::Name(std::string const&)'
adaptation/.libs/libadaptation.a(libsquid_ecap_la-XactionRep.o): In function
`libecap::Name::operator==(libecap::Name const&) const':
/usr/local/include/libecap/common/name.h:27: undefined reference to
`libecap::metaClientIp'
/usr/local/include/libecap/common/name.h:27: undefined reference to
`libecap::metaUserName'
adaptation/.libs/libadaptation.a(libsquid_ecap_la-XactionRep.o): In function
`std::string::_M_data() const':
/usr/include/c++/4.8.2/bits/basic_string.h:293: undefined reference to
`libecap::metaClientIp'
/usr/include/c++/4.8.2/bits/basic_string.h:293: undefined reference to
`libecap::metaUserName'
adaptation/.libs/libadaptation.a(libsquid_ecap_la-XactionRep.o): In function
`Adaptation::Ecap::XactionRep::vbContent(unsigned long, unsigned long)':
/root/install/squid-3.5.26/src/adaptation/ecap/XactionRep.cc:562: undefined
reference to `libecap::nsize'
/root/install/squid-3.5.26/src/adaptation/ecap/XactionRep.cc:567: undefined
reference to `libecap::Area::FromTempBuffer(char const*, unsigned long)'
/root/install/squid-3.5.26/src/adaptation/ecap/XactionRep.cc:562: undefined
reference to `libecap::nsize'
adaptation/.libs/libadaptation.a(libsquid_ecap_la-XactionRep.o): In function
`Adaptation::Ecap::XactionRep::useAdapted(std::tr1::shared_ptr<libecap::Mess
age> const&)':
/root/install/squid-3.5.26/src/adaptation/ecap/XactionRep.cc:430: undefined
reference to `typeinfo for libecap::Message'
adaptation/.libs/libadaptation.a(libsquid_ecap_la-XactionRep.o): In function
`OptionsExtractor::visit(libecap::Name const&, libecap::Area const&)':
/root/install/squid-3.5.26/src/adaptation/ecap/XactionRep.cc:41: undefined
reference to `libecap::Area::toString() const'
collect2: error: ld returned 1 exit status
libtool: link: rm -f ".libs/squidS.o"
make[3]: *** [squid] Error 1
make[3]: Leaving directory `/root/install/squid-3.5.26/src'
make[2]: *** [all-recursive] Error 1
make[2]: Leaving directory `/root/install/squid-3.5.26/src'
make[1]: *** [all] Error 2
make[1]: Leaving directory `/root/install/squid-3.5.26/src'
make: *** [all-recursive] Error 1
Thanks
Naveen

-----Original Message-----
From: Alex Rousskov [mailto:rousskov at measurement-factory.com]
Sent: Monday, June 12, 2017 10:20 PM
To: norbert.naveen at tayana.in; squid-users at lists.squid-cache.org
Subject: Re: [squid-users] Error Compiling squid-3.5.26 with libecap-1.0.1
on CentOS Linux release 7.3.1611

On 06/12/2017 10:36 AM, Norbert Naveen wrote:
> I have CentOS Linux release 7.3.1611 64 bit

> cd libecap-1.0.1
> ./configure 'CXXFLAGS=-O2 -m64 -pipe' 'CFLAGS=-O2 -m64 -pipe'
> gmake
> gmake install-strip


> cd squid-3.5.26
> ./configure '--enable-ecap' 'PKG_CONFIG_PATH=/usr/local/lib/pkgconfig'
> make

> Towards the end receive the error as below . Attaching the complete
> /root/install/squid-3.5.26/src/adaptation/ecap/Host.cc:41: undefined 
> reference to `libecap::headerTransferEncoding'

It looks like Squid found eCAP header files but did not find libecap.
Posting ./configure output and the failed linker command (should be right
above the first "undefined reference" error) may help triage this further.
Linking to a complete build log (starting with ./configure) may reduce the
number of follow up questions.

Also, what does /usr/local/lib/pkgconfig/libecap.pc contain?

HTH,

Alex.



From norbert.naveen at tayana.in  Mon Jun 12 17:48:58 2017
From: norbert.naveen at tayana.in (Norbert Naveen)
Date: Mon, 12 Jun 2017 23:18:58 +0530
Subject: [squid-users] Content / Message JS Injection for HTTP Pages Only
Message-ID: <00d701d2e3a4$2bb464f0$831d2ed0$@tayana.in>

Hello Squid Admins 

Squid 3.5 acts as HTTP Proxy only and does not allow any other kind of
Traffic 

For Such a Setup where only HTTP traffic flows through Squid 

I want to Insert a Banner / Java Script based Dynamic Page in the HTTP
response , to notify the users or alert the users based on their usage 

 

Would like to know how to go about the same 

Should I use ICAP or ECAP or any other suggestions ? 

Has anybody set up anything similar to this ? 

Any words of advice is welcome 

 

Thanks 

Naveen

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20170612/5950de03/attachment.htm>

From leolistas at solutti.com.br  Mon Jun 12 18:06:55 2017
From: leolistas at solutti.com.br (Leonardo Rodrigues)
Date: Mon, 12 Jun 2017 15:06:55 -0300
Subject: [squid-users] Office 365 Support for Squid Proxy
In-Reply-To: <CAPPXLT-nXbTHdmWBQPWX-FbA0RWrJsCx54EUqcRTz=57+0_Miw@mail.gmail.com>
References: <CAPPXLT-nXbTHdmWBQPWX-FbA0RWrJsCx54EUqcRTz=57+0_Miw@mail.gmail.com>
Message-ID: <2719e1d4-592a-845b-c365-5506596c485c@solutti.com.br>


     i have a lot of customers who access Office 365 through squid 
proxies and have no problem at all. Office 365 is just another website, 
there's absolutely no need for special configurations for it to simply work.


Em 12/06/17 06:05, Blason R escreveu:
> Hello All,
>
> If someone can confirm if squid can very well work with Office 365? If 
> anyone has any documentation can someone please forward that to me? I 
> do have almost around 400 Office 365 users hence wanted to know what 
> configuration I might need for Office 365 traffic?
>

-- 


	Atenciosamente / Sincerily,
	Leonardo Rodrigues
	Solutti Tecnologia
	http://www.solutti.com.br

	Minha armadilha de SPAM, N?O mandem email
	gertrudes at solutti.com.br
	My SPAMTRAP, do not email it





From eliezer at ngtech.co.il  Mon Jun 12 19:11:45 2017
From: eliezer at ngtech.co.il (Eliezer  Croitoru)
Date: Mon, 12 Jun 2017 22:11:45 +0300
Subject: [squid-users] Office 365 Support for Squid Proxy
In-Reply-To: <CAPPXLT-nXbTHdmWBQPWX-FbA0RWrJsCx54EUqcRTz=57+0_Miw@mail.gmail.com>
References: <CAPPXLT-nXbTHdmWBQPWX-FbA0RWrJsCx54EUqcRTz=57+0_Miw@mail.gmail.com>
Message-ID: <007701d2e3af$bbefc2c0$33cf4840$@ngtech.co.il>

The main question is if it uses websockets or not and if you are using SSL-BUMP or not.
If you are using SSL-BUMP it's one thing while if you are not it?s another story.
Also it will be different if you are using the proxy in INTERCEPT mode or a regular forward proxy mode.
We would be able to answer you more with more details on your setup.

Eliezer

----
http://ngtech.co.il/lmgtfy/
Linux System Administrator
Mobile: +972-5-28704261
Email: eliezer at ngtech.co.il


From: squid-users [mailto:squid-users-bounces at lists.squid-cache.org] On Behalf Of Blason R
Sent: Monday, June 12, 2017 12:05 PM
To: squid-users at lists.squid-cache.org
Subject: [squid-users] Office 365 Support for Squid Proxy

Hello All,

If someone can confirm if squid can very well work with Office 365? If anyone has any documentation can someone please forward that to me? I do have almost around 400 Office 365 users hence wanted to know what configuration I might need for Office 365 traffic?



From eliezer at ngtech.co.il  Mon Jun 12 20:36:53 2017
From: eliezer at ngtech.co.il (Eliezer  Croitoru)
Date: Mon, 12 Jun 2017 23:36:53 +0300
Subject: [squid-users] Content / Message JS Injection for HTTP Pages Only
In-Reply-To: <00d701d2e3a4$2bb464f0$831d2ed0$@tayana.in>
References: <00d701d2e3a4$2bb464f0$831d2ed0$@tayana.in>
Message-ID: <009201d2e3bb$a046f500$e0d4df00$@ngtech.co.il>

Hey Naveen,

ECAP and ICAP been in use for such things very long ago.
This is one of the reasons for the movement in the network to encrypt
traffic and also to pin certificates in the applications.
You will need to choose to go either with ECAP or ICAP and from there the
direction is pretty simple.
I was thinking of publishing an example using an ICAP service which is based
on this example:
https://github.com/elico/icap/blob/master/example/redirect.go

But I am not sure if it would be a good idea to do so.
I believe that if you will be able to grasp the example you would be able to
mangle it a bit to act as "proxy" in a way and then to inject content into
the body of the response.
You will need some skills and experimentation with the GoLang language and
concepts to make it work.
It's not a beginner task so don't expect it to be accomplished in one week.

Let me know if you need some more guidance.

Eliezer 

----
http://ngtech.co.il/lmgtfy/
Linux System Administrator
Mobile: +972-5-28704261
Email: eliezer at ngtech.co.il


From: squid-users [mailto:squid-users-bounces at lists.squid-cache.org] On
Behalf Of Norbert Naveen
Sent: Monday, June 12, 2017 8:49 PM
To: squid-users at lists.squid-cache.org
Subject: [squid-users] Content / Message JS Injection for HTTP Pages Only

Hello Squid Admins 
Squid 3.5 acts as HTTP Proxy only and does not allow any other kind of
Traffic 
For Such a Setup where only HTTP traffic flows through Squid 
I want to Insert a Banner / Java Script based Dynamic Page in the HTTP
response , to notify the users or alert the users based on their usage 

Would like to know how to go about the same 
Should I use ICAP or ECAP or any other suggestions ? 
Has anybody set up anything similar to this ? 
Any words of advice is welcome 

Thanks 
Naveen



From rousskov at measurement-factory.com  Mon Jun 12 20:41:38 2017
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Mon, 12 Jun 2017 14:41:38 -0600
Subject: [squid-users] Error Compiling squid-3.5.26 with libecap-1.0.1
 on CentOS Linux release 7.3.1611
In-Reply-To: <00cb01d2e3a1$67cc7ca0$376575e0$@tayana.in>
References: <00ae01d2e39a$0e0038d0$2a00aa70$@tayana.in>
 <ed4a5498-9d07-bef0-c304-6653ebec2851@measurement-factory.com>
 <00cb01d2e3a1$67cc7ca0$376575e0$@tayana.in>
Message-ID: <6c8ea344-5001-00fb-ad2e-9c89e5ceb7d1@measurement-factory.com>

On 06/12/2017 11:29 AM, Norbert Naveen wrote:

> I have attached Outputs of both Configure and Make
> And  libecap.pc contents are as below 

Your ./configure output and libecap.pc contents look good to me, but
there is no sign of the eCAP library (-lecap) being linked with Squid
during "make". If running "make clean; ./configure ...; make" does not
fix this, then please find a way to post a link to the _entire_ build
log. Something along these lines should be able to capture it:

  $ make -k clean; (./configure ...; make) > /tmp/build.log 2>&1

You may want to compress /tmp/build.log. Please post a link to some
public file sharing site here instead of the file itself. Even
compressed, it may be too big to email.

Also, what does the following command output?

  $ grep 'EXT_LIBECAP_.*=' src/adaptation/ecap/Makefile

(adjust the Makefile path if necessary if you are not building Squid its
source directory)


Cheers,

Alex.


From ext.jcook at keywcorp.com  Mon Jun 12 20:33:05 2017
From: ext.jcook at keywcorp.com (JerylCook)
Date: Mon, 12 Jun 2017 13:33:05 -0700 (PDT)
Subject: [squid-users] client-->iptables-->squid-proxy->another-proxy
Message-ID: <1497299585897-4682759.post@n4.nabble.com>

I've been stuck on this for a few days :P...

 I 'thought' I had a fairly good understanding of squid + ssl_bump but not
so sure.

In a nutshell i am having an issue linking a second proxy server via
cache_peer.

we have 2 boxes.

*Configuration:*
1 box, has iptables configured to send all outbound traffic to 10.0.0.1:8999
which is the second box's squid server and port(8999)

2nd box, has squid running on 8999, we have another server running on 8998. 
both proxy servers are using the same 'CA'.

https 10.0.0.1:8999 transparent ssl-bump generate-host-certificates=on.....

cache_peer 10.0.0.1:8998 8998 0 ssl default no-query no-digest
sslflags=DONT_VERIFY_PEER....

use-case:
wget https://facebook.com --ca-cert=/dat/sharedCa.cer  , on box 1 through
iptables..
1. squid on box 2 generates and signs a certificate with CN=facebook.com for
the client
2. client trusts the CA and cert.
3.we want squid to send this proxied https request to the second proxy
server on :8998. this proxy server is set to generate impersonation certs as
well using the same rootCAKey that squid uses...

however, we keep getting 
"Failed to establish a secure connection, SQUID_ERR_SSL_HANDSHAKE",
Handshake with SSL Server failed: error:140770FC:SSL routines
SSL23_GET_SERVER_HELLO: unknown protocol" 

Does squid 3.5.20 support PROXY Protocol in cache_peer if you need to link a
second proxy? or is my configuration messed up.



--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/client-iptables-squid-proxy-another-proxy-tp4682759.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From dkewley at uci.edu  Tue Jun 13 01:48:01 2017
From: dkewley at uci.edu (David Kewley)
Date: Mon, 12 Jun 2017 18:48:01 -0700
Subject: [squid-users] source spoofing without tproxy?
Message-ID: <CAHKjJqgCTjrGb6YR6Jj7Zo-VgjXFkDeq36uFvVwTFe=EQRKJ-w@mail.gmail.com>

I want my clients to explicitly address squid as a proxy (not use tproxy),
but have squid spoof the source addresses in the forwarded connection, so
that further hops know the original source address from the IPv4 headers.

I could find no indication that anyone else has done this, and when I tried
various things, I could not get it working.

Is this possible today? If not, is it worth considering as a future
feature? Or am I overlooking a reason that this cannot work even in theory?

I got the nearly-equivalent functionality working for reverse proxying
using nginx, but so far I've found no way to do it with forward proxying.
Nginx doesn't do https forward proxying (no handling of CONNECT).

If squid can't do what I'm looking for today, I would welcome pointers to
other possible approaches.

Thanks,
David
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20170612/bedabc4e/attachment.htm>

From squid3 at treenet.co.nz  Tue Jun 13 03:41:21 2017
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Tue, 13 Jun 2017 15:41:21 +1200
Subject: [squid-users] client-->iptables-->squid-proxy->another-proxy
In-Reply-To: <1497299585897-4682759.post@n4.nabble.com>
References: <1497299585897-4682759.post@n4.nabble.com>
Message-ID: <063c7194-e2a2-4ecb-a429-dded3bc8afdb@treenet.co.nz>

On 13/06/17 08:33, JerylCook wrote:
> I've been stuck on this for a few days :P...
>
>   I 'thought' I had a fairly good understanding of squid + ssl_bump but not
> so sure.
>
> In a nutshell i am having an issue linking a second proxy server via
> cache_peer.
>
> we have 2 boxes.
>
> *Configuration:*
> 1 box, has iptables configured to send all outbound traffic to 10.0.0.1:8999
> which is the second box's squid server and port(8999)
>
> 2nd box, has squid running on 8999, we have another server running on 8998.
> both proxy servers are using the same 'CA'.
>
> https 10.0.0.1:8999 transparent ssl-bump generate-host-certificates=on.....
>
> cache_peer 10.0.0.1:8998 8998 0 ssl default no-query no-digest
> sslflags=DONT_VERIFY_PEER....
>
> use-case:
> wget https://facebook.com --ca-cert=/dat/sharedCa.cer  , on box 1 through
> iptables..
> 1. squid on box 2 generates and signs a certificate with CN=facebook.com for
> the client

That sounds a little suspicious to me. FB have a more complicated CN in 
their real certs. You omitted your ssl_bump rules, so the type of 
bumping and details available are unknown - but I suspect they may not 
be doing what you expect in that case.

> 2. client trusts the CA and cert.

Which if the three CA involved? they need to trust the one being used by 
the frontend Squid cert generator.
  Only frontend Squid needs to trust the backend peer CA. And likewise, 
only the backend peer needs to trust the origin CA.

> 3.we want squid to send this proxied https request to the second proxy
> server on :8998. this proxy server is set to generate impersonation certs as
> well using the same rootCAKey that squid uses...

This is where the current behaviour is lacking AFAIK. SSL-Bump assumes 
the client (frontend Squid) is either sending a CONNECT request to get 
the server details from, or that it is working with intercepted TLS 
rather than a TLS explicit proxy connection. So the backend behaviour is 
still very much just receive a request for https:// URL and do the serve 
TLS thing - no mimicing on its client connection (AFAIK).

> however, we keep getting
> "Failed to establish a secure connection, SQUID_ERR_SSL_HANDSHAKE",
> Handshake with SSL Server failed: error:140770FC:SSL routines
> SSL23_GET_SERVER_HELLO: unknown protocol"
>
> Does squid 3.5.20 support PROXY Protocol in cache_peer if you need to link a
> second proxy? or is my configuration messed up.

Squid only supports receiving PROXY Protocol on the http_port directive. 
Not yet sending to a cache_peer. Though I don't see any relevance to 
PROXY Protocol in anything you have described about your configuration.

If the peer is sending an error back to Squid when it gets TLS instead 
of PROXY intro octets that would explain the SSL errors. It also would 
if the peer was sending back HTTP messages instead of TLS (HTTPS), which 
is a more common problem when the peer is an older Squid.


SSL-Bump is supported to cache_peer when the peer connection is a 
TLS/SSL connection. Though be aware that the "server" frontend Squid 
mimics would then be the backend peer's certificate, not the origin server.

Also, avoid DONT_VERIFY_PEER, it is really doing more harm than anything 
useful. Since this is a peer you know about you should also know its CA 
in advance. So use "sslflags=NO_DEFAULT_CA sslcafile=..." and Squid can 
do all the security checks just fine regardless of whether its a custom 
CA or not.

Amos



From norbert.naveen at tayana.in  Tue Jun 13 03:47:34 2017
From: norbert.naveen at tayana.in (Norbert Naveen)
Date: Tue, 13 Jun 2017 09:17:34 +0530
Subject: [squid-users] Content / Message JS Injection for HTTP Pages Only
In-Reply-To: <009201d2e3bb$a046f500$e0d4df00$@ngtech.co.il>
References: <00d701d2e3a4$2bb464f0$831d2ed0$@tayana.in>
 <009201d2e3bb$a046f500$e0d4df00$@ngtech.co.il>
Message-ID: <015101d2e3f7$cb0e3960$612aac20$@tayana.in>

Hello Eliezer 
  Thanks for your response shall have a look at it and revert 
  Between ECAP & ICAP which one is optimal ? 
  Something I have for reference is  below 
  http://wiki.squid-cache.org/SquidFaq/ContentAdaptation#Summary
 Kindly suggest 

Thanks 
Naveen

-----Original Message-----
From: Eliezer Croitoru [mailto:eliezer at ngtech.co.il] 
Sent: Tuesday, June 13, 2017 2:07 AM
To: norbert.naveen at tayana.in
Cc: squid-users at lists.squid-cache.org
Subject: RE: [squid-users] Content / Message JS Injection for HTTP Pages
Only

Hey Naveen,

ECAP and ICAP been in use for such things very long ago.
This is one of the reasons for the movement in the network to encrypt
traffic and also to pin certificates in the applications.
You will need to choose to go either with ECAP or ICAP and from there the
direction is pretty simple.
I was thinking of publishing an example using an ICAP service which is based
on this example:
https://github.com/elico/icap/blob/master/example/redirect.go

But I am not sure if it would be a good idea to do so.
I believe that if you will be able to grasp the example you would be able to
mangle it a bit to act as "proxy" in a way and then to inject content into
the body of the response.
You will need some skills and experimentation with the GoLang language and
concepts to make it work.
It's not a beginner task so don't expect it to be accomplished in one week.

Let me know if you need some more guidance.

Eliezer 

----
http://ngtech.co.il/lmgtfy/
Linux System Administrator
Mobile: +972-5-28704261
Email: eliezer at ngtech.co.il


From: squid-users [mailto:squid-users-bounces at lists.squid-cache.org] On
Behalf Of Norbert Naveen
Sent: Monday, June 12, 2017 8:49 PM
To: squid-users at lists.squid-cache.org
Subject: [squid-users] Content / Message JS Injection for HTTP Pages Only

Hello Squid Admins
Squid 3.5 acts as HTTP Proxy only and does not allow any other kind of
Traffic For Such a Setup where only HTTP traffic flows through Squid I want
to Insert a Banner / Java Script based Dynamic Page in the HTTP response ,
to notify the users or alert the users based on their usage 

Would like to know how to go about the same Should I use ICAP or ECAP or any
other suggestions ? 
Has anybody set up anything similar to this ? 
Any words of advice is welcome 

Thanks
Naveen



From squid3 at treenet.co.nz  Tue Jun 13 03:50:54 2017
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Tue, 13 Jun 2017 15:50:54 +1200
Subject: [squid-users] Content / Message JS Injection for HTTP Pages Only
In-Reply-To: <009201d2e3bb$a046f500$e0d4df00$@ngtech.co.il>
References: <00d701d2e3a4$2bb464f0$831d2ed0$@tayana.in>
 <009201d2e3bb$a046f500$e0d4df00$@ngtech.co.il>
Message-ID: <206ba3a9-bee4-01b2-995f-263b327df051@treenet.co.nz>

On 13/06/17 08:36, Eliezer Croitoru wrote:
> Hey Naveen,
>
> ECAP and ICAP been in use for such things very long ago.
> This is one of the reasons for the movement in the network to encrypt
> traffic and also to pin certificates in the applications.
> You will need to choose to go either with ECAP or ICAP and from there the
> direction is pretty simple.
> I was thinking of publishing an example using an ICAP service which is based
> on this example:
> https://github.com/elico/icap/blob/master/example/redirect.go
>
> But I am not sure if it would be a good idea to do so.
> I believe that if you will be able to grasp the example you would be able to
> mangle it a bit to act as "proxy" in a way and then to inject content into
> the body of the response.
> You will need some skills and experimentation with the GoLang language and
> concepts to make it work.
> It's not a beginner task so don't expect it to be accomplished in one week.
>
> Let me know if you need some more guidance.
>
> Eliezer

And don't forget that the content going through is often copyrighted by 
someone else. Being publicly accessible does not make it any less 
proprietary or restricted by its owners. Under copyright legislation 
altering other peoples content can be a form of theft.

Naveen: please get some legal advice about the above before going any 
further. This idea of injecting JS banners and notices into other 
peoples pages has a history lined with companies having major PR fallout 
or collapsing into bankruptcy in a few cases as a direct result of 
consumer and copyrigth content owners reaction to the injection.

Amos


> ----
> http://ngtech.co.il/lmgtfy/
> Linux System Administrator
> Mobile: +972-5-28704261
> Email: eliezer at ngtech.co.il
>
>
> From: squid-users [mailto:squid-users-bounces at lists.squid-cache.org] On
> Behalf Of Norbert Naveen
> Sent: Monday, June 12, 2017 8:49 PM
> To: squid-users at lists.squid-cache.org
> Subject: [squid-users] Content / Message JS Injection for HTTP Pages Only
>
> Hello Squid Admins
> Squid 3.5 acts as HTTP Proxy only and does not allow any other kind of
> Traffic
> For Such a Setup where only HTTP traffic flows through Squid
> I want to Insert a Banner / Java Script based Dynamic Page in the HTTP
> response , to notify the users or alert the users based on their usage
>
> Would like to know how to go about the same
> Should I use ICAP or ECAP or any other suggestions ?
> Has anybody set up anything similar to this ?
> Any words of advice is welcome
>
> Thanks
> Naveen
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users



From squid3 at treenet.co.nz  Tue Jun 13 04:50:45 2017
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Tue, 13 Jun 2017 16:50:45 +1200
Subject: [squid-users] source spoofing without tproxy?
In-Reply-To: <CAHKjJqgCTjrGb6YR6Jj7Zo-VgjXFkDeq36uFvVwTFe=EQRKJ-w@mail.gmail.com>
References: <CAHKjJqgCTjrGb6YR6Jj7Zo-VgjXFkDeq36uFvVwTFe=EQRKJ-w@mail.gmail.com>
Message-ID: <e40c77cc-c58c-d3a1-f194-93a255c8cb30@treenet.co.nz>

On 13/06/17 13:48, David Kewley wrote:
> I want my clients to explicitly address squid as a proxy (not use 
> tproxy), but have squid spoof the source addresses in the forwarded 
> connection, so that further hops know the original source address from 
> the IPv4 headers.
>
> I could find no indication that anyone else has done this, and when I 
> tried various things, I could not get it working.
>
> Is this possible today? If not, is it worth considering as a future 
> feature? Or am I overlooking a reason that this cannot work even in 
> theory?

It is not possible.

No, it is a terrible idea.

It is prohibited by the OS kernel as part of the anti-malware 
protections, in this case to prevent the local machine being used to 
attack its surrounding network nodes. And by Squid to make it harder to 
use Squid as viral payload and damage the brand reputation.


Also, HTTP contains multiplexing and persistent connections. So there is 
no particular relation between one incoming/client connection and the 
outgoing/server connection(s) the traffic from that client goes out on. 
Added to that, a client request may generate multiple outgoing requests 
of various types, or Squid may itself generate traffic for its own needs 
without any client interaction.

So doing this just degrades the proxy performance. And not in a small 
way - intercepted traffic pinning everything as this would need comes 
out about 10% nominal (90% reduction), and at the extreme end proxies 
with NTLM going through to an origin see only 1% of nominal performance. 
Nominal for me being what I clocked a big clients network doing in 
real-world traffic a few years back: ~20000 requests per second a few 
years back (Squid Project got approx 2x that in controlled lab tests).

>
> I got the nearly-equivalent functionality working for reverse proxying 
> using nginx, but so far I've found no way to do it with forward 
> proxying. Nginx doesn't do https forward proxying (no handling of 
> CONNECT).

So Nginx can be used to attack networks from inside. Good no know we now 
have to watch out for that in viral payloads too.

>
> If squid can't do what I'm looking for today, I would welcome pointers 
> to other possible approaches.

Squid supports X-Forwarded-For fully - it was invented by Squid devs 
back in the day, and Squid is still the authoritative implementation for 
how it is supposed to work. As an old feature just about all other HTTP 
server and intermediary software have support for that too so you should 
have no issue pulling the data out at the receiving end, or in HTTP 
processing DPI software / firewalls etc. It is sent on all outgoing 
Squid messages unless you explicitly configure something else to happen 
with the forwarded_for directive.
  <http://www.squid-cache.org/Doc/config/forwarded_for/>


There is also newer HTTP "Forwarded" header which supercedes 
X-Forwarded-For and some very newly written servers might only support 
that. Squid lacks the built-in support for that directive so its no good 
on received traffic. But if you have to it can be sent to an upstream 
server fine with the request_header_add directive, like so:
   request_header_add Forwarded for=%>a


HTH
Amos



From norbert.naveen at tayana.in  Tue Jun 13 05:13:56 2017
From: norbert.naveen at tayana.in (Norbert Naveen)
Date: Tue, 13 Jun 2017 10:43:56 +0530
Subject: [squid-users] Error Compiling squid-3.5.26 with libecap-1.0.1
	on CentOS Linux release 7.3.1611
In-Reply-To: <6c8ea344-5001-00fb-ad2e-9c89e5ceb7d1@measurement-factory.com>
References: <00ae01d2e39a$0e0038d0$2a00aa70$@tayana.in>
 <ed4a5498-9d07-bef0-c304-6653ebec2851@measurement-factory.com>
 <00cb01d2e3a1$67cc7ca0$376575e0$@tayana.in>
 <6c8ea344-5001-00fb-ad2e-9c89e5ceb7d1@measurement-factory.com>
Message-ID: <006201d2e403$dc16eb10$9444c130$@tayana.in>

Hello 
  The Make output can be found at 
  https://drive.google.com/open?id=0B_dDVNpzSGEKcFlMSlBVZWs5c2c

Thanks 
Naveen

-----Original Message-----
From: Alex Rousskov [mailto:rousskov at measurement-factory.com] 
Sent: Tuesday, June 13, 2017 2:12 AM
To: norbert.naveen at tayana.in; squid-users at lists.squid-cache.org
Subject: Re: [squid-users] Error Compiling squid-3.5.26 with libecap-1.0.1
on CentOS Linux release 7.3.1611

On 06/12/2017 11:29 AM, Norbert Naveen wrote:

> I have attached Outputs of both Configure and Make And  libecap.pc 
> contents are as below

Your ./configure output and libecap.pc contents look good to me, but there
is no sign of the eCAP library (-lecap) being linked with Squid during
"make". If running "make clean; ./configure ...; make" does not fix this,
then please find a way to post a link to the _entire_ build log. Something
along these lines should be able to capture it:

  $ make -k clean; (./configure ...; make) > /tmp/build.log 2>&1

You may want to compress /tmp/build.log. Please post a link to some public
file sharing site here instead of the file itself. Even compressed, it may
be too big to email.

Also, what does the following command output?

  $ grep 'EXT_LIBECAP_.*=' src/adaptation/ecap/Makefile

(adjust the Makefile path if necessary if you are not building Squid its
source directory)


Cheers,

Alex.



From dkewley at uci.edu  Tue Jun 13 06:14:55 2017
From: dkewley at uci.edu (David Kewley)
Date: Mon, 12 Jun 2017 23:14:55 -0700
Subject: [squid-users] source spoofing without tproxy?
In-Reply-To: <e40c77cc-c58c-d3a1-f194-93a255c8cb30@treenet.co.nz>
References: <CAHKjJqgCTjrGb6YR6Jj7Zo-VgjXFkDeq36uFvVwTFe=EQRKJ-w@mail.gmail.com>
 <e40c77cc-c58c-d3a1-f194-93a255c8cb30@treenet.co.nz>
Message-ID: <CAHKjJqj=GoEnXo2uqKKisSYubn5D9UhG5g4fV+q5vpyB+nXGHA@mail.gmail.com>

Thanks for your reply, Amos.

On Mon, Jun 12, 2017 at 9:50 PM, Amos Jeffries <squid3 at treenet.co.nz> wrote:

> On 13/06/17 13:48, David Kewley wrote:
>
>> I want my clients to explicitly address squid as a proxy (not use
>> tproxy), but have squid spoof the source addresses in the forwarded
>> connection, so that further hops know the original source address from the
>> IPv4 headers.
>>
>> I could find no indication that anyone else has done this, and when I
>> tried various things, I could not get it working.
>>
>> Is this possible today? If not, is it worth considering as a future
>> feature? Or am I overlooking a reason that this cannot work even in theory?
>>
>
> It is not possible.
>
> No, it is a terrible idea.
>
> It is prohibited by the OS kernel as part of the anti-malware protections,
> in this case to prevent the local machine being used to attack its
> surrounding network nodes. And by Squid to make it harder to use Squid as
> viral payload and damage the brand reputation.
>

What exactly is the "it" that you're saying is prohibited by the OS kernel?
Source spoofing alone, or something else?

Also, HTTP contains multiplexing and persistent connections. So there is no
> particular relation between one incoming/client connection and the
> outgoing/server connection(s) the traffic from that client goes out on.
> Added to that, a client request may generate multiple outgoing requests of
> various types, or Squid may itself generate traffic for its own needs
> without any client interaction.
>
> So doing this just degrades the proxy performance. And not in a small way
> - intercepted traffic pinning everything as this would need comes out about
> 10% nominal (90% reduction), and at the extreme end proxies with NTLM going
> through to an origin see only 1% of nominal performance. Nominal for me
> being what I clocked a big clients network doing in real-world traffic a
> few years back: ~20000 requests per second a few years back (Squid Project
> got approx 2x that in controlled lab tests).


Good to know there are strong performance implications, thanks. I don't
understand these systems deeply enough to have anticipated this, so I
appreciate the heads-up. Too many systems to learn, too quickly...

I got the nearly-equivalent functionality working for reverse proxying
>> using nginx, but so far I've found no way to do it with forward proxying.
>> Nginx doesn't do https forward proxying (no handling of CONNECT).
>>
>
> So Nginx can be used to attack networks from inside. Good no know we now
> have to watch out for that in viral payloads too.


"Can be used to attack" because of source spoofing, or something else?

If squid can't do what I'm looking for today, I would welcome pointers to
>> other possible approaches.
>>
>
> Squid supports X-Forwarded-For fully - it was invented by Squid devs back
> in the day, and Squid is still the authoritative implementation for how it
> is supposed to work. As an old feature just about all other HTTP server and
> intermediary software have support for that too so you should have no issue
> pulling the data out at the receiving end, or in HTTP processing DPI
> software / firewalls etc. It is sent on all outgoing Squid messages unless
> you explicitly configure something else to happen with the forwarded_for
> directive.
>  <http://www.squid-cache.org/Doc/config/forwarded_for/>
>

I'll ask the team managing the next-hop device to evaluate that
possibility; it looks to me from the docs like it might work. Thanks for
the suggestion.

David
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20170612/00e27b48/attachment.htm>

From uhlar at fantomas.sk  Tue Jun 13 07:34:58 2017
From: uhlar at fantomas.sk (Matus UHLAR - fantomas)
Date: Tue, 13 Jun 2017 09:34:58 +0200
Subject: [squid-users] source spoofing without tproxy?
In-Reply-To: <e40c77cc-c58c-d3a1-f194-93a255c8cb30@treenet.co.nz>
References: <CAHKjJqgCTjrGb6YR6Jj7Zo-VgjXFkDeq36uFvVwTFe=EQRKJ-w@mail.gmail.com>
 <e40c77cc-c58c-d3a1-f194-93a255c8cb30@treenet.co.nz>
Message-ID: <20170613073458.GA4704@fantomas.sk>

>On 13/06/17 13:48, David Kewley wrote:
>>I want my clients to explicitly address squid as a proxy (not use 
>>tproxy), but have squid spoof the source addresses in the forwarded 
>>connection, so that further hops know the original source address 
>>from the IPv4 headers.
>>
>>I could find no indication that anyone else has done this, and when 
>>I tried various things, I could not get it working.
>>
>>Is this possible today? If not, is it worth considering as a future 
>>feature? Or am I overlooking a reason that this cannot work even in 
>>theory?

On 13.06.17 16:50, Amos Jeffries wrote:
>It is not possible.
>
>No, it is a terrible idea.
>
>It is prohibited by the OS kernel as part of the anti-malware 
>protections, in this case to prevent the local machine being used to 
>attack its surrounding network nodes. And by Squid to make it harder 
>to use Squid as viral payload and damage the brand reputation.

For me to fully understand (I was curious about this some time ago), it is
allowed to fake clients' IPs when intercepting their connections, but not
when connections are done to proxy server directly?

What's the difference that makes it more terrible than spoofing IPs of
intercepted connections?

-- 
Matus UHLAR - fantomas, uhlar at fantomas.sk ; http://www.fantomas.sk/
Warning: I wish NOT to receive e-mail advertising to this address.
Varovanie: na tuto adresu chcem NEDOSTAVAT akukolvek reklamnu postu.
Eagles may soar, but weasels don't get sucked into jet engines. 


From A.Madonna at rechtspraak.nl  Tue Jun 13 07:35:29 2017
From: A.Madonna at rechtspraak.nl (Madonna, A. (spir-it))
Date: Tue, 13 Jun 2017 07:35:29 +0000
Subject: [squid-users] client-->iptables-->squid-proxy->another-proxy
In-Reply-To: <063c7194-e2a2-4ecb-a429-dded3bc8afdb@treenet.co.nz>
References: <1497299585897-4682759.post@n4.nabble.com>
 <063c7194-e2a2-4ecb-a429-dded3bc8afdb@treenet.co.nz>
Message-ID: <f6d464cd87cf4fb98380e988eb57e95e@rechtspraak.nl>

Hello Jeryl,

If you look on the mailing list we and many before us have this problem.

Client ----> Squid proxy ----> Parent proxy ----> Internets (http / HTTPS)

As already stated by 1 of the developers before, the code simply does not exist to handle this. cache_peer can't do a "HTTP CONNECT", simulating the first client connection with a parent proxy.

This has been so for at least the last 5+ years.

We are now looking into a solution where we put something between the squid and the parent proxy which can provide a "HTTP CONNECT" in combination with ssl_bump preserving the original SNI(server name indication).

Client ----> Squid proxy ----> "HTTP CONNECT" solution---->Parent proxy ----> Internets (http / HTTPS)

Kind regards,



-----Oorspronkelijk bericht-----
Van: squid-users [mailto:squid-users-bounces at lists.squid-cache.org] Namens Amos Jeffries
Verzonden: dinsdag 13 juni 2017 5:41
Aan: squid-users at lists.squid-cache.org
Onderwerp: Re: [squid-users] client-->iptables-->squid-proxy->another-proxy

On 13/06/17 08:33, JerylCook wrote:
> I've been stuck on this for a few days :P...
>
>   I 'thought' I had a fairly good understanding of squid + ssl_bump
> but not so sure.
>
> In a nutshell i am having an issue linking a second proxy server via
> cache_peer.
>
> we have 2 boxes.
>
> *Configuration:*
> 1 box, has iptables configured to send all outbound traffic to
> 10.0.0.1:8999 which is the second box's squid server and port(8999)
>
> 2nd box, has squid running on 8999, we have another server running on 8998.
> both proxy servers are using the same 'CA'.
>
> https 10.0.0.1:8999 transparent ssl-bump generate-host-certificates=on.....
>
> cache_peer 10.0.0.1:8998 8998 0 ssl default no-query no-digest
> sslflags=DONT_VERIFY_PEER....
>
> use-case:
> wget https://facebook.com --ca-cert=/dat/sharedCa.cer  , on box 1
> through iptables..
> 1. squid on box 2 generates and signs a certificate with
> CN=facebook.com for the client

That sounds a little suspicious to me. FB have a more complicated CN in their real certs. You omitted your ssl_bump rules, so the type of bumping and details available are unknown - but I suspect they may not be doing what you expect in that case.

> 2. client trusts the CA and cert.

Which if the three CA involved? they need to trust the one being used by the frontend Squid cert generator.
  Only frontend Squid needs to trust the backend peer CA. And likewise, only the backend peer needs to trust the origin CA.

> 3.we want squid to send this proxied https request to the second proxy
> server on :8998. this proxy server is set to generate impersonation
> certs as well using the same rootCAKey that squid uses...

This is where the current behaviour is lacking AFAIK. SSL-Bump assumes the client (frontend Squid) is either sending a CONNECT request to get the server details from, or that it is working with intercepted TLS rather than a TLS explicit proxy connection. So the backend behaviour is still very much just receive a request for https:// URL and do the serve TLS thing - no mimicing on its client connection (AFAIK).

> however, we keep getting
> "Failed to establish a secure connection, SQUID_ERR_SSL_HANDSHAKE",
> Handshake with SSL Server failed: error:140770FC:SSL routines
> SSL23_GET_SERVER_HELLO: unknown protocol"
>
> Does squid 3.5.20 support PROXY Protocol in cache_peer if you need to
> link a second proxy? or is my configuration messed up.

Squid only supports receiving PROXY Protocol on the http_port directive.
Not yet sending to a cache_peer. Though I don't see any relevance to PROXY Protocol in anything you have described about your configuration.

If the peer is sending an error back to Squid when it gets TLS instead of PROXY intro octets that would explain the SSL errors. It also would if the peer was sending back HTTP messages instead of TLS (HTTPS), which is a more common problem when the peer is an older Squid.


SSL-Bump is supported to cache_peer when the peer connection is a TLS/SSL connection. Though be aware that the "server" frontend Squid mimics would then be the backend peer's certificate, not the origin server.

Also, avoid DONT_VERIFY_PEER, it is really doing more harm than anything useful. Since this is a peer you know about you should also know its CA in advance. So use "sslflags=NO_DEFAULT_CA sslcafile=..." and Squid can do all the security checks just fine regardless of whether its a custom CA or not.

Amos

_______________________________________________
squid-users mailing list
squid-users at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-users

________________________________

Informatie van de Raad voor de rechtspraak, de rechtbanken, de gerechtshoven en de bijzondere colleges vindt u op www.rechtspraak.nl.

From squid3 at treenet.co.nz  Tue Jun 13 09:49:15 2017
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Tue, 13 Jun 2017 21:49:15 +1200
Subject: [squid-users] source spoofing without tproxy?
In-Reply-To: <20170613073458.GA4704@fantomas.sk>
References: <CAHKjJqgCTjrGb6YR6Jj7Zo-VgjXFkDeq36uFvVwTFe=EQRKJ-w@mail.gmail.com>
 <e40c77cc-c58c-d3a1-f194-93a255c8cb30@treenet.co.nz>
 <20170613073458.GA4704@fantomas.sk>
Message-ID: <b2003801-7c31-ea31-62cc-fb0ebb2e99a4@treenet.co.nz>

On 13/06/17 19:34, Matus UHLAR - fantomas wrote:
>> On 13/06/17 13:48, David Kewley wrote:
>>> I want my clients to explicitly address squid as a proxy (not use 
>>> tproxy), but have squid spoof the source addresses in the forwarded 
>>> connection, so that further hops know the original source address 
>>> from the IPv4 headers.
>>>
>>> I could find no indication that anyone else has done this, and when 
>>> I tried various things, I could not get it working.
>>>
>>> Is this possible today? If not, is it worth considering as a future 
>>> feature? Or am I overlooking a reason that this cannot work even in 
>>> theory?
>
> On 13.06.17 16:50, Amos Jeffries wrote:
>> It is not possible.
>>
>> No, it is a terrible idea.
>>
>> It is prohibited by the OS kernel as part of the anti-malware 
>> protections, in this case to prevent the local machine being used to 
>> attack its surrounding network nodes. And by Squid to make it harder 
>> to use Squid as viral payload and damage the brand reputation.
>
> For me to fully understand (I was curious about this some time ago), 
> it is
> allowed to fake clients' IPs when intercepting their connections, but not
> when connections are done to proxy server directly?

Yes.

> What's the difference that makes it more terrible than spoofing IPs of
> intercepted connections?

If you take a close look at the packets you should see the incoming ones 
as (client-IP:server-IP:server-port) and outgoing from Squid has 
identical (client-IP:server-IP:server-port). Only the src-port differs.
  - As far as the rest of the network is concerned Squid is acting as if 
it were a TCP router. The kernel is also configured as a router in order 
to do TPROXY, so the whole environment except for the proxy and a few 
rules in the networking stack is setup for routing.


By comparison, without TPROXY the incoming packets have 
(client-IP:client-port:squid-IP:squid-port) and the server connection 
packets would have (client-IP:random-port:server-IP:server-port). The 
machine is setup as a server host, not a router. Which is where Ingress 
and Egress Filtering both stomp on it hard for using other machines IPs.


PS. if you spoof (with or without TPROXY) and have not implemented 
BCP-38 ingress filtering (AND its equivalent egress filtering) on your 
network then you are part of the DoS attack system, guaranteed, whether 
you know it or not. Very likely you will _not_ be aware of it because 
all the information you can log or gather from TCP is spoofed (duh!) and 
appears to be your own innocent clients doing things that clients tend 
to do (NTP lookups? DNS lookups? sending email? using HTTP? sure no harm 
there ... unless it wasn't them).

Amos



From squid3 at treenet.co.nz  Tue Jun 13 10:15:49 2017
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Tue, 13 Jun 2017 22:15:49 +1200
Subject: [squid-users] source spoofing without tproxy?
In-Reply-To: <CAHKjJqj=GoEnXo2uqKKisSYubn5D9UhG5g4fV+q5vpyB+nXGHA@mail.gmail.com>
References: <CAHKjJqgCTjrGb6YR6Jj7Zo-VgjXFkDeq36uFvVwTFe=EQRKJ-w@mail.gmail.com>
 <e40c77cc-c58c-d3a1-f194-93a255c8cb30@treenet.co.nz>
 <CAHKjJqj=GoEnXo2uqKKisSYubn5D9UhG5g4fV+q5vpyB+nXGHA@mail.gmail.com>
Message-ID: <bbd1961a-f491-85a0-a9c3-c82c704900da@treenet.co.nz>

On 13/06/17 18:14, David Kewley wrote:
> Thanks for your reply, Amos.
>
> On Mon, Jun 12, 2017 at 9:50 PM, Amos Jeffries <squid3 at treenet.co.nz 
> <mailto:squid3 at treenet.co.nz>> wrote:
>
>     On 13/06/17 13:48, David Kewley wrote:
>
>         I want my clients to explicitly address squid as a proxy (not
>         use tproxy), but have squid spoof the source addresses in the
>         forwarded connection, so that further hops know the original
>         source address from the IPv4 headers.
>
>         I could find no indication that anyone else has done this, and
>         when I tried various things, I could not get it working.
>
>         Is this possible today? If not, is it worth considering as a
>         future feature? Or am I overlooking a reason that this cannot
>         work even in theory?
>
>
>     It is not possible.
>
>     No, it is a terrible idea.
>
>     It is prohibited by the OS kernel as part of the anti-malware
>     protections, in this case to prevent the local machine being used
>     to attack its surrounding network nodes. And by Squid to make it
>     harder to use Squid as viral payload and damage the brand reputation.
>
>
> What exactly is the "it" that you're saying is prohibited by the OS 
> kernel? Source spoofing alone, or something else?

The "it" that was the subject of your question "it" - spoofing.
>
>     Also, HTTP contains multiplexing and persistent connections. So
>     there is no particular relation between one incoming/client
>     connection and the outgoing/server connection(s) the traffic from
>     that client goes out on. Added to that, a client request may
>     generate multiple outgoing requests of various types, or Squid may
>     itself generate traffic for its own needs without any client
>     interaction.
>
>     So doing this just degrades the proxy performance. And not in a
>     small way - intercepted traffic pinning everything as this would
>     need comes out about 10% nominal (90% reduction), and at the
>     extreme end proxies with NTLM going through to an origin see only
>     1% of nominal performance. Nominal for me being what I clocked a
>     big clients network doing in real-world traffic a few years back:
>     ~20000 requests per second a few years back (Squid Project got
>     approx 2x that in controlled lab tests).
>
>
> Good to know there are strong performance implications, thanks. I 
> don't understand these systems deeply enough to have anticipated this, 
> so I appreciate the heads-up. Too many systems to learn, too quickly...
>
>         I got the nearly-equivalent functionality working for reverse
>         proxying using nginx, but so far I've found no way to do it
>         with forward proxying. Nginx doesn't do https forward proxying
>         (no handling of CONNECT).
>
>
>     So Nginx can be used to attack networks from inside. Good no know
>     we now have to watch out for that in viral payloads too.
>
>
> "Can be used to attack" because of source spoofing, or something else?
>

Directly because of source spoofing, or equivalent if you got it sending 
from any non-local IP address. "local" meaning an IP address explicitly 
assigned for use by the machine the proxy runs on.

This might be of help if you are not already aware of the risks and 
issues involved with spoofing and handling of non-local IPs; 
<http://www.bcp38.info/>


>         If squid can't do what I'm looking for today, I would welcome
>         pointers to other possible approaches.
>
>
>     Squid supports X-Forwarded-For fully - it was invented by Squid
>     devs back in the day, and Squid is still the authoritative
>     implementation for how it is supposed to work. As an old feature
>     just about all other HTTP server and intermediary software have
>     support for that too so you should have no issue pulling the data
>     out at the receiving end, or in HTTP processing DPI software /
>     firewalls etc. It is sent on all outgoing Squid messages unless
>     you explicitly configure something else to happen with the
>     forwarded_for directive.
>      <http://www.squid-cache.org/Doc/config/forwarded_for/
>     <http://www.squid-cache.org/Doc/config/forwarded_for/>>
>
>
> I'll ask the team managing the next-hop device to evaluate that 
> possibility; it looks to me from the docs like it might work. Thanks 
> for the suggestion.
>
> David

That would be best if it works.

I came up with a bodgy workaround using NAT after sending the earlier 
mail. So if there is no other way than delivering the client-IP on the 
packets there is still something that might be done. But, that would 
still run up against HTTP multiplexing and also add all sorts of NAT 
related issues as well. So only a last resort really.

Amos


From alex.delgado at crg.eu  Tue Jun 13 11:30:10 2017
From: alex.delgado at crg.eu (Alejandro Delgado Moreno)
Date: Tue, 13 Jun 2017 11:30:10 +0000
Subject: [squid-users] Cache peer help
In-Reply-To: <b71539b0-0833-3d63-ef7a-b05493e4f9e3@treenet.co.nz>
References: <c84b8794fa3e4b118d128a687b7047d6@UPF-BORN-MBX.crg.es>
 <a7ea26a7-6f65-c68b-8fd2-5b0f1b9b1c29@treenet.co.nz>
 <c898a3a31c0d496a91d7e199431d9302@UPF-BORN-MBX.crg.es>
 <5a9bc1c6-cbce-a4d7-907f-719ad1d5b827@treenet.co.nz>
 <65c3699721d44d77bec274dae9c00b93@UPF-BORN-MBX.crg.es>
 <b71539b0-0833-3d63-ef7a-b05493e4f9e3@treenet.co.nz>
Message-ID: <a3d8bfb6301647158f447b4821d43546@UPF-BORN-MBX.crg.es>

Hi Amos, 

I've applied your suggestions, but still every request is sent directly, bypassing the peer proxy for sites specified on file UPF_List.txt:

[Tue Jun 13 13:25:58 2017].905    111 172.18.2.45 TCP_MISS/200 968 POST http://ocsp.usertrust.com/ - HIER_DIRECT/178.255.83.1 application/ocsp-response
[Tue Jun 13 13:26:00 2017].173     56 172.18.2.45 TCP_MISS/200 874 POST http://clients1.google.com/ocsp - HIER_DIRECT/216.58.208.238 application/ocsp-response
[Tue Jun 13 13:26:00 2017].283     47 172.18.2.45 TCP_MISS/200 924 POST http://ocsp.digicert.com/ - HIER_DIRECT/93.184.220.29 application/ocsp-response
[Tue Jun 13 13:26:00 2017].618    211 172.18.2.45 TCP_TUNNEL/200 5147 CONNECT www.facebook.com:443 - HIER_DIRECT/31.13.90.36 -
[Tue Jun 13 13:26:01 2017].691  65863 172.18.2.43 TCP_TUNNEL/200 4946 CONNECT d.dropbox.com:443 - HIER_DIRECT/162.125.32.5 -
[Tue Jun 13 13:26:03 2017].821     68 172.18.2.45 TCP_MISS/302 615 GET http://wos.fecyt.es/ - HIER_DIRECT/185.79.129.106 text/html
[Tue Jun 13 13:26:04 2017].014     29 172.18.2.45 TCP_MISS/200 2068 POST http://ss.symcd.com/ - HIER_DIRECT/23.37.171.27 application/ocsp-response
[Tue Jun 13 13:26:05 2017].151   5079 172.18.2.45 TCP_TUNNEL/200 404 CONNECT www.recursoscientificos.fecyt.es:443 - HIER_DIRECT/185.79.129.106 -
[Tue Jun 13 13:26:05 2017].239   5163 172.18.2.45 TCP_TUNNEL/200 404 CONNECT www.recursoscientificos.fecyt.es:443 - HIER_DIRECT/185.79.129.106 -
[Tue Jun 13 13:26:08 2017].878  10313 172.18.2.45 TCP_TUNNEL/200 54835 CONNECT www.recursoscientificos.fecyt.es:443 - HIER_DIRECT/185.79.129.106 -
[Tue Jun 13 13:26:10 2017].281   5202 172.18.2.45 TCP_TUNNEL/200 526 CONNECT idp.fecyt.es:443 - HIER_DIRECT/185.79.129.106 -
[Tue Jun 13 13:26:10 2017].365   5107 172.18.2.45 TCP_TUNNEL/200 331 CONNECT idp.fecyt.es:443 - HIER_DIRECT/185.79.129.106 -
[Tue Jun 13 13:26:10 2017].372  10219 172.18.2.45 TCP_TUNNEL/200 38460 CONNECT platform.twitter.com:443 - HIER_DIRECT/199.96.57.6 -
[Tue Jun 13 13:26:10 2017].391   5135 172.18.2.45 TCP_TUNNEL/200 331 CONNECT idp.fecyt.es:443 - HIER_DIRECT/185.79.129.106 -
[Tue Jun 13 13:26:10 2017].454   6580 172.18.2.45 TCP_TUNNEL/200 106738 CONNECT idp.fecyt.es:443 - HIER_DIRECT/185.79.129.106 -

This is the squid.conf file settings:

#
# Recommended minimum configuration:
#

# Example rule allowing access from your local networks.
# Adapt to list your (internal) IP networks from where browsing
# should be allowed
acl localnet src 10.0.0.0/8	# RFC1918 possible internal network
#acl localnet src 172.16.0.0/12	# RFC1918 possible internal network
acl localnet src 192.168.0.0/16	# RFC1918 possible internal network
acl localnet src fc00::/7       # RFC 4193 local private network range
acl localnet src fe80::/10      # RFC 4291 link-local (directly plugged) machines
acl localnet src 172.17.0.0/16
acl localnet src 172.18.0.0/16
acl localnet src 172.16.0.0/16

acl SSL_ports port 443
acl Safe_ports port 80		# http
acl Safe_ports port 21		# ftp
acl Safe_ports port 443		# https
acl Safe_ports port 70		# gopher
acl Safe_ports port 210		# wais
acl Safe_ports port 1025-65535	# unregistered ports
acl Safe_ports port 280		# http-mgmt
acl Safe_ports port 488		# gss-http
acl Safe_ports port 591		# filemaker
acl Safe_ports port 777		# multiling http
acl CONNECT method CONNECT

acl journals dstdomain "/etc/squid/UPF_LIST.txt"

cache_peer proxy-inst.upf.edu parent 9090 0 no-query  no-digest default
cache_peer_access proxy-inst.upf.edu allow journals
#originserver name=proxyupf
# dstdomain "/etc/squid/UPF_LIST.txt"
#cache_peer_access server_upf allow upf
#cache_peer_access proxyupf allow upf
#cache_peer_access proxyupf deny all
nonhierarchical_direct off
#never_direct deny upf
never_direct allow journals

#never_direct allow upf

#never_direct deny !upf
#never_direct allow all
#cache_peer_access allow upf
#cache_peer_access deny all

#never_direct allow !upf
#never_direct deny all
#
# Recommended minimum Access Permission configuration:
#
# Deny requests to certain unsafe ports
http_access deny !Safe_ports

# Deny CONNECT to other than secure SSL ports
http_access deny CONNECT !SSL_ports

# Only allow cachemgr access from localhost
http_access allow localhost manager
http_access deny manager

# We strongly recommend the following be uncommented to protect innocent
# web applications running on the proxy server who think the only
# one who can access services on "localhost" is a local user
#http_access deny to_localhost

#
# INSERT YOUR OWN RULE(S) HERE TO ALLOW ACCESS FROM YOUR CLIENTS
#
http_access allow journals
#cache_peer_access proxyupf allow upf
#cache_peer_access proxyupf deny all
# Example rule allowing access from your local networks.
# Adapt localnet in the ACL section to list your (internal) IP networks
# from where browsing should be allowed
http_access allow localnet
http_access allow localhost

# And finally deny all other access to this proxy
http_access deny all

# Squid normally listens to port 3128
#http_port 3128
http_port 8881

# Uncomment and adjust the following to add a disk cache directory.
#cache_dir ufs /var/spool/squid 100 16 256

# Leave coredumps in the first cache dir
coredump_dir /var/spool/squid

#
# Add any of your own refresh_pattern entries above these.
#
refresh_pattern ^ftp:		1440	20%	10080
refresh_pattern ^gopher:	1440	0%	1440
refresh_pattern -i (/cgi-bin/|\?) 0	0%	0
refresh_pattern .		0	20%	4320

Any other suggestions? Do you need the contents of UPF_LIST.txt?

Regards,

-----Original Message-----
From: squid-users [mailto:squid-users-bounces at lists.squid-cache.org] On Behalf Of Amos Jeffries
Sent: jueves, 8 de junio de 2017 12:55
To: squid-users at lists.squid-cache.org
Subject: Re: [squid-users] Cache peer help

On 08/06/17 19:51, Alejandro Delgado Moreno wrote:
> Hi Amos,
>
> Here is the squid.conf file:
>
> acl localnet src 172.16.0.0/16
>
> acl SSL_ports port 443
> acl Safe_ports port 80          # http
> acl Safe_ports port 21          # ftp
> acl Safe_ports port 443         # https
> acl Safe_ports port 70          # gopher
> acl Safe_ports port 210         # wais
> acl Safe_ports port 1025-65535  # unregistered ports
> acl Safe_ports port 280         # http-mgmt
> acl Safe_ports port 488         # gss-http
> acl Safe_ports port 591         # filemaker
> acl Safe_ports port 777         # multiling http
> acl CONNECT method CONNECT
>
>
> acl journals dstdomain "/etc/squid/UPF_LIST.txt"
>
> cache_peer proxy-inst.upf.edu parent 9090 0 no-query no-digest default
>
> cache_peer_access proxy-inst.upf.edu allow journals always_direct 
> allow journals

There you go. Problem #1:  "always_direct allow" prohibits any cache_peer being used by that request (by requiring that DIRECT be used, mandatory). Remove that and some of the journal traffic will start going to the peer.

> And this is an extract of the log:
>
> [Thu Jun  8 09:47:30 2017].094   5079 172.18.2.45 TCP_TUNNEL/200 333 CONNECT idp.fecyt.es:443 - HIER_DIRECT/185.79.129.106 -
> [Thu Jun  8 09:47:30 2017].094   5079 172.18.2.45 TCP_TUNNEL/200 331 CONNECT idp.fecyt.es:443 - HIER_DIRECT/185.79.129.106 -
> [Thu Jun  8 09:47:30 2017].120   5106 172.18.2.45 TCP_TUNNEL/200 331 CONNECT idp.fecyt.es:443 - HIER_DIRECT/185.79.129.106 -
> [Thu Jun  8 09:47:30 2017].144   5130 172.18.2.45 TCP_TUNNEL/200 332 CONNECT idp.fecyt.es:443 - HIER_DIRECT/185.79.129.106 -
> [Thu Jun  8 09:47:30 2017].147   5133 172.18.2.45 TCP_TUNNEL/200 333 CONNECT idp.fecyt.es:443 - HIER_DIRECT/185.79.129.106 -
> [Thu Jun  8 09:47:30 2017].374   6567 172.18.2.45 TCP_TUNNEL/200 108115 CONNECT idp.fecyt.es:443 - HIER_DIRECT/185.79.129.106 -

CONNECT and a few other things are normally sent DIRECT because that is way faster than doing another hop.

To make those prefer going through the peer add this line:

   nonhierarchical_direct off

And if that is not enough, you can add "never_direct allow journals" to forbid DIRECT being used. They will then fail completely if the peer is not used for any reason.


> As you can see, always is going direct, but when going to idp.fecyt.es should be going through the peer, as the file UPF_LIST.txt has:
>
> https://idp.fecyt.es
> https://idp.fecyt.es/
> https://idp.fecyt.es/*

Your squid.conf said these were being loaded into a dstdomain ACL. But the above lines are URLs, not domain names.

dstdomain syntax is a domain name with maybe a wildcard to match all sub-domains. see <http://wiki.squid-cache.org/SquidFaq/SquidAcl#Squid_doesn.27t_match_my_subdomains>


HTH
Amos

_______________________________________________
squid-users mailing list
squid-users at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-users

From squid3 at treenet.co.nz  Tue Jun 13 11:49:51 2017
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Tue, 13 Jun 2017 23:49:51 +1200
Subject: [squid-users] Cache peer help
In-Reply-To: <a3d8bfb6301647158f447b4821d43546@UPF-BORN-MBX.crg.es>
References: <c84b8794fa3e4b118d128a687b7047d6@UPF-BORN-MBX.crg.es>
 <a7ea26a7-6f65-c68b-8fd2-5b0f1b9b1c29@treenet.co.nz>
 <c898a3a31c0d496a91d7e199431d9302@UPF-BORN-MBX.crg.es>
 <5a9bc1c6-cbce-a4d7-907f-719ad1d5b827@treenet.co.nz>
 <65c3699721d44d77bec274dae9c00b93@UPF-BORN-MBX.crg.es>
 <b71539b0-0833-3d63-ef7a-b05493e4f9e3@treenet.co.nz>
 <a3d8bfb6301647158f447b4821d43546@UPF-BORN-MBX.crg.es>
Message-ID: <7127e6ca-f314-d64f-b6d9-c0bc41a47ccd@treenet.co.nz>

On 13/06/17 23:30, Alejandro Delgado Moreno wrote:
> Hi Amos,
>
> I've applied your suggestions, but still every request is sent directly, bypassing the peer proxy for sites specified on file UPF_List.txt:
>
> [Tue Jun 13 13:25:58 2017].905    111 172.18.2.45 TCP_MISS/200 968 POST http://ocsp.usertrust.com/ - HIER_DIRECT/178.255.83.1 application/ocsp-response
> [Tue Jun 13 13:26:00 2017].173     56 172.18.2.45 TCP_MISS/200 874 POST http://clients1.google.com/ocsp - HIER_DIRECT/216.58.208.238 application/ocsp-response
> [Tue Jun 13 13:26:00 2017].283     47 172.18.2.45 TCP_MISS/200 924 POST http://ocsp.digicert.com/ - HIER_DIRECT/93.184.220.29 application/ocsp-response
> [Tue Jun 13 13:26:00 2017].618    211 172.18.2.45 TCP_TUNNEL/200 5147 CONNECT www.facebook.com:443 - HIER_DIRECT/31.13.90.36 -
> [Tue Jun 13 13:26:01 2017].691  65863 172.18.2.43 TCP_TUNNEL/200 4946 CONNECT d.dropbox.com:443 - HIER_DIRECT/162.125.32.5 -
> [Tue Jun 13 13:26:03 2017].821     68 172.18.2.45 TCP_MISS/302 615 GET http://wos.fecyt.es/ - HIER_DIRECT/185.79.129.106 text/html
> [Tue Jun 13 13:26:04 2017].014     29 172.18.2.45 TCP_MISS/200 2068 POST http://ss.symcd.com/ - HIER_DIRECT/23.37.171.27 application/ocsp-response
> [Tue Jun 13 13:26:05 2017].151   5079 172.18.2.45 TCP_TUNNEL/200 404 CONNECT www.recursoscientificos.fecyt.es:443 - HIER_DIRECT/185.79.129.106 -
> [Tue Jun 13 13:26:05 2017].239   5163 172.18.2.45 TCP_TUNNEL/200 404 CONNECT www.recursoscientificos.fecyt.es:443 - HIER_DIRECT/185.79.129.106 -
> [Tue Jun 13 13:26:08 2017].878  10313 172.18.2.45 TCP_TUNNEL/200 54835 CONNECT www.recursoscientificos.fecyt.es:443 - HIER_DIRECT/185.79.129.106 -
> [Tue Jun 13 13:26:10 2017].281   5202 172.18.2.45 TCP_TUNNEL/200 526 CONNECT idp.fecyt.es:443 - HIER_DIRECT/185.79.129.106 -
> [Tue Jun 13 13:26:10 2017].365   5107 172.18.2.45 TCP_TUNNEL/200 331 CONNECT idp.fecyt.es:443 - HIER_DIRECT/185.79.129.106 -
> [Tue Jun 13 13:26:10 2017].372  10219 172.18.2.45 TCP_TUNNEL/200 38460 CONNECT platform.twitter.com:443 - HIER_DIRECT/199.96.57.6 -
> [Tue Jun 13 13:26:10 2017].391   5135 172.18.2.45 TCP_TUNNEL/200 331 CONNECT idp.fecyt.es:443 - HIER_DIRECT/185.79.129.106 -
> [Tue Jun 13 13:26:10 2017].454   6580 172.18.2.45 TCP_TUNNEL/200 106738 CONNECT idp.fecyt.es:443 - HIER_DIRECT/185.79.129.106 -

Hmm. Your squid.conf now looks fine to me.

> Any other suggestions? Do you need the contents of UPF_LIST.txt?

I think so, yes. It is the last bit of the config I can think of right 
now as maybe problematic.

PS. If it contains anything you want to keep private or is bigger than 
50KB then you may mail me off-list.

Amos



From kevinmuehlparzer at hotmail.de  Tue Jun 13 11:59:33 2017
From: kevinmuehlparzer at hotmail.de (=?utf-8?B?S2V2aW4gTe+/vWhscGFyemVy?=)
Date: Tue, 13 Jun 2017 11:59:33 +0000
Subject: [squid-users] Negotiate Kerberos Auth - BH Invalid request
In-Reply-To: <bbd1961a-f491-85a0-a9c3-c82c704900da@treenet.co.nz>
References: <CAHKjJqgCTjrGb6YR6Jj7Zo-VgjXFkDeq36uFvVwTFe=EQRKJ-w@mail.gmail.com>
 <e40c77cc-c58c-d3a1-f194-93a255c8cb30@treenet.co.nz>
 <CAHKjJqj=GoEnXo2uqKKisSYubn5D9UhG5g4fV+q5vpyB+nXGHA@mail.gmail.com>,
 <bbd1961a-f491-85a0-a9c3-c82c704900da@treenet.co.nz>
Message-ID: <VI1PR0501MB1967B271DA4B512686C87452A7C20@VI1PR0501MB1967.eurprd05.prod.outlook.com>

Hello list,


I asked about a problem with NTLM-Authentication before. (BH SPNEGO request invalid prefix; thats the error of the helper protocol "helper-protocol=squid-2.5-ntlmssp" I used with NTLM, while basic works fine)

A user told me I should use negotiate_kerberos_auth instead of ntlm_auth.

Now here's my new problem:


root at x-x-testproxy01:/etc/squid# /usr/lib/squid/negotiate_kerberos_auth -d -s HTTP/x-x-testproxy01.x-xxx.local at X-XXX.LOCAL
negotiate_kerberos_auth.cc(487): pid=5305 :2017/06/13 13:29:41| negotiate_kerberos_auth: INFO: Starting version 3.0.4sq
negotiate_kerberos_auth.cc(546): pid=5305 :2017/06/13 13:29:41| negotiate_kerberos_auth: INFO: Setting keytab to FILE:/etc/squid/HTTP.keytab
negotiate_kerberos_auth.cc(570): pid=5305 :2017/06/13 13:29:41| negotiate_kerberos_auth: INFO: Changed keytab to MEMORY:negotiate_kerberos_auth_5305
testuser xxxxxxx
negotiate_kerberos_auth.cc(610): pid=5305 :2017/06/13 13:29:47| negotiate_kerberos_auth: DEBUG: Got 'testuser xxxxxx' from squid (length: 18).
negotiate_kerberos_auth.cc(647): pid=5305 :2017/06/13 13:29:47| negotiate_kerberos_auth: ERROR: Invalid request [testuser xxxxxxx]
BH Invalid request
So my configuration has mistakes, but I can't find them. I don't really know where to search, or what works for sure. I tried many tutorials on krb5 and samba. Every form of testing I tried works fine except indeed using the required kerberos authentication of my squid-proxy.


Tests that come to my mind:

kinit a user

Warning: Your password will expire in 36 days on Don 20 Jul 2017 13:23:54 CEST



klist

Ticket cache: FILE:/tmp/krb5cc_0
Default principal: testuser at X-XXX.LOCAL

Valid starting       Expires              Service principal
2017-06-13 13:38:37  2017-06-13 23:38:37  krbtgt/X-XXX.LOCAL at X-XXX.LOCAL
    renew until 2017-06-14 13:38:34


klist -k on my HTTP.keytab

Keytab name: FILE:/etc/squid/HTTP.keytab
KVNO Principal
---- --------------------------------------------------------------------------
   1 host/x-x-testproxy01.x-xxx.local at X-XXX.LOCAL
   1 host/x-x-testproxy01.x-xxx.local at X-XXX.LOCAL
   1 host/x-x-testproxy01.x-xxx.local at X-XXX.LOCAL
   1 host/x-x-testproxy01.x-xxx.local at X-XXX.LOCAL
   1 host/x-x-testproxy01.x-xxx.local at X-XXX.LOCAL
   1 host/X-X-TESTPROXY01 at X-XXX.LOCAL
   1 host/X-X-TESTPROXY01 at X-XXX.LOCAL
   1 host/X-X-TESTPROXY01 at X-XXX.LOCAL
   1 host/X-X-TESTPROXY01 at X-XXX.LOCAL
   1 host/X-X-TESTPROXY01 at X-XXX.LOCAL
   1 X-X-TESTPROXY01$@X-XXX.LOCAL
   1 X-X-TESTPROXY01$@X-XXX.LOCAL
   1 X-X-TESTPROXY01$@X-XXX.LOCAL
   1 X-X-TESTPROXY01$@X-XXX.LOCAL
   1 X-X-TESTPROXY01$@X-XXX.LOCAL


basic-auth using ntlm

root at x-x-testproxy01:/etc/squid# /usr/bin/ntlm_auth --helper-protocol=squid-2.5-basic --username=testuser --password=xxxxxxxx
testuser xxxxxxxxxx
OK
testuser at x-xxx.local xxxxxxxx
OK

wbinfo -u
administrator
testuser
...
wbinfo -g
allowed rodc password replication group
enterprise read-only domain controllers
...

wbinfo --krb5auth=testuser%xxxxxxx
plaintext kerberos password authentication for [testuser%xxxxxxx] succeeded (requesting cctype: FILE)

wbinfo -t
checking the trust secret for domain X-XXX via RPC calls succeeded

wbinfo --authenticate=testuser%xxxxxxxx
plaintext password authentication succeeded
challenge/response password authentication succeeded

/usr/lib/squid/negotiate_kerberos_auth_test x-x-testproxy01.x-xxx.local
Token: YIIFOgYGKwYBBQUCoIIFLjCCBSqgJzAlBgkqhkiG9xIBAgIGBSsFAQUCBgkqhkiC9xIBAgIGBisGAQUCBaKCBP0EggT5YIIE9QYJKoZIhvcSAQICAQBuggTkMIIE4KADAgEFoQMCAQ6iBwMFAAAAAACjggP2YYID8jCCA+6gAwIBBaENGwtYLU5FVC5MT0NBTKIuMCygAwIBA6ElMCMbBEhUVFAbG3gtbC10ZXN0cHJveHkwMS54LW5ldC5sb2NhbKOCA6YwggOioAMCARKhAwIBAaKCA5QEggOQIMtincRDtWjh44pew3twk26Gm9rTC7CbkobNrzaRq/weljVl5TSbMQTFIVRQXVe4CQBWJ/Gcg472cgLA3mjOH8Z30zxQFP8fsK46wAtTEzJhonzXLImhaPtXvCVz94xaCVG7cBlNJCUmZQHsQMxFsGJZfKCkDvztiNplXEEwRgT7S6f8HQNm62xPAyz9aK8Wqfz9suW5cSBk8wdRAQNleKP1Xe/2LqZ4jfDpodPdcy9A8vh1dKu4tmbz+EJ/bKvWA+/twuXiOhhGq4W39TlOu/3zD87pXAh65ka1QsepkCWgUMHImDw8nUr8Zvi4j4vI9WhyMyLFYBya8BvAX9kLg73zl80g82bQVIAb8QU3gS2Akhpd7r1flfUbfRDUQfuS/bsaHspZoP+2c8+Bxy38OML4Gg29y6fJvRNfDaCnqmTQdiPtyqELVUS+4x7r260mM1wQKzD2Jb5pcz4wMHUK0sdw8xmMARGxB7XdyGSbo759GD6tOaTAKkNwccno6i9wyOoLqfhVRjE/K9FLCvCnzEFI07q1/dFz1Ce/ZzroW3nOwNQe3V6qqBELwuTvHgxIdGq4HEPeLqAUkVWxneXemRNbLKiOs/BIe3qkXxizgAkFLqRO5az2pVOD7/KBevxYZKeAgIuDsbIYG/u3Ic+KtCDaaM/to2b41SB8ZOFKJau3BuMPOvZ4ipiMbv0N/Svk4Tg61BIhN3CJYKA3ep/3p3wSfJlkcYeRVkDpasQnmFnjxV2YS7Q8nvmf9LIz+KIYBIT8X9yRPuV/E2lSELZlxJ8CySLFLUKgtMj97GPMlacc+UN3lJjyoExUKHpMZtUmaKrw7ueT3wnLMWgx0BBPkiAebUAedKj9u9sEscFylmI/+PdCCraMNbkOckCsggYXfJm6LxFZJhnDvw1+Z87xsJFDs5fasF6j8REiG8aHTmKHgt2M9TmIRNo/PsYrZGvuVQhkk2fuyFxwwyfs7ysNEkOmBWlFlTEjddjT9YShHfV8Fo8+M2UYY5nYiUIQq5BBfZ679ntivs7F80lKMOqhc7SOY63VwRJOwoq35+bnsIB08b9cttySiOcFsZb6uTnYvHzUFVSUha4nxrg3zW3fL9KVu4XY+lgCRZrBZMxioy6vbAOJBmpqXOJvel2gBbGN6PEd2ReeX43l1gcn+Bd3mQykmUIEzMMuRpSHda9233aWHbwEZ9H9rOdJdhgX4U+upIHQMIHNoAMCARKigcUEgcJu7kcC1zuJdhOQk+YA0Hw2G9kyg46tpaNIgj1CEgkD/KE28kaKivCTZTnHfrNOpIOJaaiYw4RMwJsbgZdRU+fz/jUwxvXdUSGon6JrU7S2XZ9CjXRXfdpXc4HjP0QW6Cql1SE95MpkcbMRH8FQvGNryBzsqIkELnvXceTGCmwlN3n60nqkoR5/41p2PtSz4hFMOVdT6AkPlNC5VTCtCZUj7YbrYVYImPnG3aAfQxXEHRy19/v0mL2845jZFA7Xw96s1A==



Sorry for posting so many output...
I already read many documentations, but no one really tests in small steps, they just assume that it works for everyone out of the box...

Does anyone have a clue what could be my mistake?

Thanks in advance.


-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20170613/5015e8a2/attachment.htm>

From belle at bazuin.nl  Tue Jun 13 12:54:38 2017
From: belle at bazuin.nl (=?windows-1252?Q?L.P.H._van_Belle?=)
Date: Tue, 13 Jun 2017 14:54:38 +0200
Subject: [squid-users] Negotiate Kerberos Auth - BH Invalid request
In-Reply-To: <VI1PR0501MB1967B271DA4B512686C87452A7C20@VI1PR0501MB1967.eurprd05.prod.outlook.com>
References: <bbd1961a-f491-85a0-a9c3-c82c704900da@treenet.co.nz>
Message-ID: <vmime.593fe08e.2928.415f4fe258757e6e@ms249-lin-003.rotterdam.bazuin.nl>

First, it very handy to know your os and samba and squid versions used. 
?
Second, 
Squid/radius etc anything that uses NTLMv1 with samba stopped working after 4.5.0 
I think your main problem can be explained by this extract from the release notes for 4.5.0:
?

NTLMv1 authentication disabled by default

-----------------------------------------

?

In order to improve security we have changed the default value for the "ntlm auth" option from "yes" to "no".?
This may have impact on very old clients which doesn't support NTLMv2 yet.

?

The primary user of NTLMv1 is MSCHAPv2 for VPNs and 802.1x.

?

By default, Samba will only allow NTLMv2 via NTLMSSP now, 
as we have the following default "lanman auth = no", "ntlm auth = no" and "raw NTLMv2 auth = no".

?

?

Greetz, 

?

Louis

?

?

?

Van: squid-users [mailto:squid-users-bounces at lists.squid-cache.org] Namens Kevin M???hlparzer
Verzonden: dinsdag 13 juni 2017 14:00
Aan: squid-users at lists.squid-cache.org
Onderwerp: [squid-users] Negotiate Kerberos Auth - BH Invalid request




Hello list,




I asked about a problem with NTLM-Authentication before. (BH SPNEGO request invalid prefix; thats the error of the helper protocol "helper-protocol=squid-2.5-ntlmssp" I used with NTLM, while basic works fine)

A user told me I should use negotiate_kerberos_auth instead of ntlm_auth.

Now here's my new problem:





root at x-x-testproxy01:/etc/squid# /usr/lib/squid/negotiate_kerberos_auth -d -s HTTP/x-x-testproxy01.x-xxx.local at X-XXX.LOCAL
negotiate_kerberos_auth.cc(487): pid=5305 :2017/06/13 13:29:41| negotiate_kerberos_auth: INFO: Starting version 3.0.4sq
negotiate_kerberos_auth.cc(546): pid=5305 :2017/06/13 13:29:41| negotiate_kerberos_auth: INFO: Setting keytab to FILE:/etc/squid/HTTP.keytab
negotiate_kerberos_auth.cc(570): pid=5305 :2017/06/13 13:29:41| negotiate_kerberos_auth: INFO: Changed keytab to MEMORY:negotiate_kerberos_auth_5305
testuser xxxxxxx
negotiate_kerberos_auth.cc(610): pid=5305 :2017/06/13 13:29:47| negotiate_kerberos_auth: DEBUG: Got 'testuser xxxxxx' from squid (length: 18).
negotiate_kerberos_auth.cc(647): pid=5305 :2017/06/13 13:29:47| negotiate_kerberos_auth: ERROR: Invalid request [testuser xxxxxxx]
BH Invalid request
So my configuration has mistakes, but I can't find them. I don't really know where to search, or what works for sure. I tried many tutorials on krb5 and samba. Every form of testing I tried works fine except indeed using the required kerberos authentication of my squid-proxy.






Tests that come to my mind:

kinit a user

Warning: Your password will expire in 36 days on Don 20 Jul 2017 13:23:54 CEST










klist

Ticket cache: FILE:/tmp/krb5cc_0
Default principal: testuser at X-XXX.LOCAL

Valid starting?????? Expires????????????? Service principal
2017-06-13 13:38:37? 2017-06-13 23:38:37? krbtgt/X-XXX.LOCAL at X-XXX.LOCAL
?? ?renew until 2017-06-14 13:38:34





klist -k on my HTTP.keytab



Keytab name: FILE:/etc/squid/HTTP.keytab
KVNO Principal
---- --------------------------------------------------------------------------
?? 1 host/x-x-testproxy01.x-xxx.local at X-XXX.LOCAL
?? 1 host/x-x-testproxy01.x-xxx.local at X-XXX.LOCAL
?? 1 host/x-x-testproxy01.x-xxx.local at X-XXX.LOCAL
?? 1 host/x-x-testproxy01.x-xxx.local at X-XXX.LOCAL
?? 1 host/x-x-testproxy01.x-xxx.local at X-XXX.LOCAL
?? 1 host/X-X-TESTPROXY01 at X-XXX.LOCAL
?? 1 host/X-X-TESTPROXY01 at X-XXX.LOCAL
?? 1 host/X-X-TESTPROXY01 at X-XXX.LOCAL
?? 1 host/X-X-TESTPROXY01 at X-XXX.LOCAL
?? 1 host/X-X-TESTPROXY01 at X-XXX.LOCAL
?? 1 X-X-TESTPROXY01$@X-XXX.LOCAL
?? 1 X-X-TESTPROXY01$@X-XXX.LOCAL
?? 1 X-X-TESTPROXY01$@X-XXX.LOCAL
?? 1 X-X-TESTPROXY01$@X-XXX.LOCAL
?? 1 X-X-TESTPROXY01$@X-XXX.LOCAL





basic-auth using ntlm


root at x-x-testproxy01:/etc/squid# /usr/bin/ntlm_auth --helper-protocol=squid-2.5-basic --username=testuser --password=xxxxxxxx
testuser xxxxxxxxxx
OK
testuser at x-xxx.local xxxxxxxx
OK

wbinfo -u
administrator
testuser
...
wbinfo -g
allowed rodc password replication group
enterprise read-only domain controllers
...

wbinfo --krb5auth=testuser%xxxxxxx
plaintext kerberos password authentication for [testuser%xxxxxxx] succeeded (requesting cctype: FILE)

wbinfo -t
checking the trust secret for domain X-XXX via RPC calls succeeded

wbinfo --authenticate=testuser%xxxxxxxx
plaintext password authentication succeeded
challenge/response password authentication succeeded

/usr/lib/squid/negotiate_kerberos_auth_test x-x-testproxy01.x-xxx.local
Token: YIIFOgYGKwYBBQUCoIIFLjCCBSqgJzAlBgkqhkiG9xIBAgIGBSsFAQUCBgkqhkiC9xIBAgIGBisGAQUCBaKCBP0EggT5YIIE9QYJKoZIhvcSAQICAQBuggTkMIIE4KADAgEFoQMCAQ6iBwMFAAAAAACjggP2YYID8jCCA+6gAwIBBaENGwtYLU5FVC5MT0NBTKIuMCygAwIBA6ElMCMbBEhUVFAbG3gtbC10ZXN0cHJveHkwMS54LW5ldC5sb2NhbKOCA6YwggOioAMCARKhAwIBAaKCA5QEggOQIMtincRDtWjh44pew3twk26Gm9rTC7CbkobNrzaRq/weljVl5TSbMQTFIVRQXVe4CQBWJ/Gcg472cgLA3mjOH8Z30zxQFP8fsK46wAtTEzJhonzXLImhaPtXvCVz94xaCVG7cBlNJCUmZQHsQMxFsGJZfKCkDvztiNplXEEwRgT7S6f8HQNm62xPAyz9aK8Wqfz9suW5cSBk8wdRAQNleKP1Xe/2LqZ4jfDpodPdcy9A8vh1dKu4tmbz+EJ/bKvWA+/twuXiOhhGq4W39TlOu/3zD87pXAh65ka1QsepkCWgUMHImDw8nUr8Zvi4j4vI9WhyMyLFYBya8BvAX9kLg73zl80g82bQVIAb8QU3gS2Akhpd7r1flfUbfRDUQfuS/bsaHspZoP+2c8+Bxy38OML4Gg29y6fJvRNfDaCnqmTQdiPtyqELVUS+4x7r260mM1wQKzD2Jb5pcz4wMHUK0sdw8xmMARGxB7XdyGSbo759GD6tOaTAKkNwccno6i9wyOoLqfhVRjE/K9FLCvCnzEFI07q1/dFz1Ce/ZzroW3nOwNQe3V6qqBELwuTvHgxIdGq4HEPeLqAUkVWxneXemRNbLKiOs/BIe3qkXxizgAkFLqRO5az2pVOD7/KBevxYZKeAgIuDsbIYG/u3Ic+KtCDaaM/to2b41SB8ZOFKJau3BuMPOvZ4ipiMbv0N/Svk4Tg61BIhN3CJYKA3ep/3p3wSfJlkcYeRVkDpasQnmFnjxV2YS7Q8nvmf9LIz+KIYBIT8X9yRPuV/E2lSELZlxJ8CySLFLUKgtMj97GPMlacc+UN3lJjyoExUKHpMZtUmaKrw7ueT3wnLMWgx0BBPkiAebUAedKj9u9sEscFylmI/+PdCCraMNbkOckCsggYXfJm6LxFZJhnDvw1+Z87xsJFDs5fasF6j8REiG8aHTmKHgt2M9TmIRNo/PsYrZGvuVQhkk2fuyFxwwyfs7ysNEkOmBWlFlTEjddjT9YShHfV8Fo8+M2UYY5nYiUIQq5BBfZ679ntivs7F80lKMOqhc7SOY63VwRJOwoq35+bnsIB08b9cttySiOcFsZb6uTnYvHzUFVSUha4nxrg3zW3fL9KVu4XY+lgCRZrBZMxioy6vbAOJBmpqXOJvel2gBbGN6PEd2ReeX43l1gcn+Bd3mQykmUIEzMMuRpSHda9233aWHbwEZ9H9rOdJdhgX4U+upIHQMIHNoAMCARKigcUEgcJu7kcC1zuJdhOQk+YA0Hw2G9kyg46tpaNIgj1CEgkD/KE28kaKivCTZTnHfrNOpIOJaaiYw4RMwJsbgZdRU+fz/jUwxvXdUSGon6JrU7S2XZ9CjXRXfdpXc4HjP0QW6Cql1SE95MpkcbMRH8FQvGNryBzsqIkELnvXceTGCmwlN3n60nqkoR5/41p2PtSz4hFMOVdT6AkPlNC5VTCtCZUj7YbrYVYImPnG3aAfQxXEHRy19/v0mL2845jZFA7Xw96s1A==




Sorry for posting so many output...
I already read many documentations, but no one really tests in small steps, they just assume that it works for everyone out of the box...


Does anyone have a clue what could be my mistake?



Thanks in advance.








-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20170613/8a4f419b/attachment.htm>

From chip_pop at hotmail.com  Tue Jun 13 13:35:33 2017
From: chip_pop at hotmail.com (joseph)
Date: Tue, 13 Jun 2017 06:35:33 -0700 (PDT)
Subject: [squid-users] cacheable object dose not match
Message-ID: <1497360933302-4682776.post@n4.nabble.com>

if you open the hex cached file
first header or so should be HTTP/1.1 200 OK
right ???
is this correct in one line will be cacheable hit or corrupted 
 accept-encodingHTTP/1.1 200 OK         ???

good cache header file example

HTTP/1.1 200 OK
Via: cache-yes
Content-Type: image/x-icon
Last-Modified: Thu, 08 Jun 2017 16:42:39 GMT
Content-Length: 1406
Server: Jetty(7.6.0.v20120127)
---------------------------------------

bad cached header file

accept-encodingHTTP/1.1 200 OK
Accept-Ranges: bytes
Content-Length: 158484
Last-Modified: Fri, 31 Mar 2017 23:01:45 GMT
Server: ATS
Vary: Accept-Encoding
Connection: keep-alive
Content-Type: video/mp2t
Cache-Control: public, max-age=604800
Access-Control-Allow-Origin: *
Access-Control-Expose-Headers: Content-Range, X-ATLAS-MARKERS
Timing-Allow-Origin: *
X-ATLAS-MARKERS: colodeb,cslrtt,rtt80_100ms,csbrtt,ar2,dist2000_3000km




--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/cacheable-object-dose-not-match-tp4682776.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From squid3 at treenet.co.nz  Tue Jun 13 14:20:08 2017
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Wed, 14 Jun 2017 02:20:08 +1200
Subject: [squid-users] cacheable object dose not match
In-Reply-To: <1497360933302-4682776.post@n4.nabble.com>
References: <1497360933302-4682776.post@n4.nabble.com>
Message-ID: <c44b7938-6f11-2ed8-9b7f-520d52252a14@treenet.co.nz>

On 14/06/17 01:35, joseph wrote:
> if you open the hex cached file
> first header or so should be HTTP/1.1 200 OK
> right ???

No. The cache file contains a TLV structure of metadata followed by the 
response 'on-wire' syntax.
You appear not to be able to see (or cut-n-paste) the binary TLV 
prefixes, just the Vary metadata since it is a string.


Amos



From chip_pop at hotmail.com  Tue Jun 13 14:11:57 2017
From: chip_pop at hotmail.com (joseph)
Date: Tue, 13 Jun 2017 07:11:57 -0700 (PDT)
Subject: [squid-users] cacheable object dose not match
In-Reply-To: <c44b7938-6f11-2ed8-9b7f-520d52252a14@treenet.co.nz>
References: <1497360933302-4682776.post@n4.nabble.com>
 <c44b7938-6f11-2ed8-9b7f-520d52252a14@treenet.co.nz>
Message-ID: <1497363117488-4682778.post@n4.nabble.com>

>>No. The cache file contains a TLV structure of metadata followed by the 
right  but so it should be a TLV bindery and after that ??  HTTP/1.1 200 OK
wish is text clear or anything betwean thim as this  -->>   
accept-encodingHTTP/1.1 200 OK
accept-encoding and status line  on one line also 

1 accept-encoding should be befor status line ??
2 they should be  on one line  without  cr ??

so i need to know befor reporting bug tks



--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/cacheable-object-dose-not-match-tp4682776p4682778.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From norbert.naveen at tayana.in  Tue Jun 13 14:46:04 2017
From: norbert.naveen at tayana.in (Norbert Naveen)
Date: Tue, 13 Jun 2017 20:16:04 +0530
Subject: [squid-users] Error Compiling squid-3.5.26 with libecap-1.0.1
	on CentOS Linux release 7.3.1611
References: <00ae01d2e39a$0e0038d0$2a00aa70$@tayana.in>
 <ed4a5498-9d07-bef0-c304-6653ebec2851@measurement-factory.com>
 <00cb01d2e3a1$67cc7ca0$376575e0$@tayana.in>
 <6c8ea344-5001-00fb-ad2e-9c89e5ceb7d1@measurement-factory.com> 
Message-ID: <003e01d2e453$c8d24e50$5a76eaf0$@tayana.in>

Hello
  The Make output can be found at
  https://drive.google.com/open?id=0B_dDVNpzSGEKcFlMSlBVZWs5c2c


 And the output of # grep 'EXT_LIBECAP_.*=' src/adaptation/ecap/Makefile
EXT_LIBECAP_CFLAGS = /usr/local/lib
EXT_LIBECAP_LIBS = /usr/local/lib


 I did the below to no effect 
make -k clean; 
./configure  --enable-ecap
 make > /tmp/build.log 2>&1

Build Log is available at 
https://drive.google.com/open?id=0B_dDVNpzSGEKM3JSUHNJLWJlWjA

Thanks
Naveen

-----Original Message-----
From: Alex Rousskov [mailto:rousskov at measurement-factory.com]
Sent: Tuesday, June 13, 2017 2:12 AM
To: norbert.naveen at tayana.in; squid-users at lists.squid-cache.org
Subject: Re: [squid-users] Error Compiling squid-3.5.26 with libecap-1.0.1
on CentOS Linux release 7.3.1611

On 06/12/2017 11:29 AM, Norbert Naveen wrote:

> I have attached Outputs of both Configure and Make And  libecap.pc 
> contents are as below

Your ./configure output and libecap.pc contents look good to me, but there
is no sign of the eCAP library (-lecap) being linked with Squid during
"make". If running "make clean; ./configure ...; make" does not fix this,
then please find a way to post a link to the _entire_ build log. Something
along these lines should be able to capture it:

  $ make -k clean; (./configure ...; make) > /tmp/build.log 2>&1

You may want to compress /tmp/build.log. Please post a link to some public
file sharing site here instead of the file itself. Even compressed, it may
be too big to email.

Also, what does the following command output?

  $ grep 'EXT_LIBECAP_.*=' src/adaptation/ecap/Makefile

(adjust the Makefile path if necessary if you are not building Squid its
source directory)


Cheers,

Alex.



From belle at bazuin.nl  Tue Jun 13 15:03:01 2017
From: belle at bazuin.nl (=?windows-1252?Q?L.P.H._van_Belle?=)
Date: Tue, 13 Jun 2017 17:03:01 +0200
Subject: [squid-users] Error Compiling squid-3.5.26 with libecap-1.0.1
 on CentOS Linux release 7.3.1611
In-Reply-To: <003e01d2e453$c8d24e50$5a76eaf0$@tayana.in>
References: <003e01d2e453$c8d24e50$5a76eaf0$@tayana.in>
Message-ID: <vmime.593ffea5.6ca6.3e11588364ca0075@ms249-lin-003.rotterdam.bazuin.nl>

Looks the same like.

http://squid-web-proxy-cache.1019090.n4.nabble.com/Compiling-squid-3-5-4-with-ecap-enabled-td4671325.html 

Greetz, 

Louis



> -----Oorspronkelijk bericht-----
> Van: squid-users 
> [mailto:squid-users-bounces at lists.squid-cache.org] Namens 
> Norbert Naveen
> Verzonden: dinsdag 13 juni 2017 16:46
> Aan: 'Alex Rousskov'; squid-users at lists.squid-cache.org
> Onderwerp: Re: [squid-users] Error Compiling squid-3.5.26 
> with libecap-1.0.1 on CentOS Linux release 7.3.1611
> 
> Hello
>   The Make output can be found at
>   https://drive.google.com/open?id=0B_dDVNpzSGEKcFlMSlBVZWs5c2c
> 
> 
>  And the output of # grep 'EXT_LIBECAP_.*=' 
> src/adaptation/ecap/Makefile EXT_LIBECAP_CFLAGS = 
> /usr/local/lib EXT_LIBECAP_LIBS = /usr/local/lib
> 
> 
>  I did the below to no effect
> make -k clean;
> ./configure  --enable-ecap
>  make > /tmp/build.log 2>&1
> 
> Build Log is available at
> https://drive.google.com/open?id=0B_dDVNpzSGEKM3JSUHNJLWJlWjA
> 
> Thanks
> Naveen
> 
> -----Original Message-----
> From: Alex Rousskov [mailto:rousskov at measurement-factory.com]
> Sent: Tuesday, June 13, 2017 2:12 AM
> To: norbert.naveen at tayana.in; squid-users at lists.squid-cache.org
> Subject: Re: [squid-users] Error Compiling squid-3.5.26 with 
> libecap-1.0.1 on CentOS Linux release 7.3.1611
> 
> On 06/12/2017 11:29 AM, Norbert Naveen wrote:
> 
> > I have attached Outputs of both Configure and Make And  libecap.pc 
> > contents are as below
> 
> Your ./configure output and libecap.pc contents look good to 
> me, but there is no sign of the eCAP library (-lecap) being 
> linked with Squid during "make". If running "make clean; 
> ./configure ...; make" does not fix this, then please find a 
> way to post a link to the _entire_ build log. Something along 
> these lines should be able to capture it:
> 
>   $ make -k clean; (./configure ...; make) > /tmp/build.log 2>&1
> 
> You may want to compress /tmp/build.log. Please post a link 
> to some public file sharing site here instead of the file 
> itself. Even compressed, it may be too big to email.
> 
> Also, what does the following command output?
> 
>   $ grep 'EXT_LIBECAP_.*=' src/adaptation/ecap/Makefile
> 
> (adjust the Makefile path if necessary if you are not 
> building Squid its source directory)
> 
> 
> Cheers,
> 
> Alex.
> 
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
> 



From rousskov at measurement-factory.com  Tue Jun 13 15:38:36 2017
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Tue, 13 Jun 2017 09:38:36 -0600
Subject: [squid-users] Error Compiling squid-3.5.26 with libecap-1.0.1
 on CentOS Linux release 7.3.1611
In-Reply-To: <003e01d2e453$c8d24e50$5a76eaf0$@tayana.in>
References: <00ae01d2e39a$0e0038d0$2a00aa70$@tayana.in>
 <ed4a5498-9d07-bef0-c304-6653ebec2851@measurement-factory.com>
 <00cb01d2e3a1$67cc7ca0$376575e0$@tayana.in>
 <6c8ea344-5001-00fb-ad2e-9c89e5ceb7d1@measurement-factory.com>
 <003e01d2e453$c8d24e50$5a76eaf0$@tayana.in>
Message-ID: <8c1103ed-138e-8918-0127-5e612d3bd64f@measurement-factory.com>

On 06/13/2017 08:46 AM, Norbert Naveen wrote:

> # grep 'EXT_LIBECAP_.*=' src/adaptation/ecap/Makefile
> EXT_LIBECAP_CFLAGS = /usr/local/lib
> EXT_LIBECAP_LIBS = /usr/local/lib

Strange. These variables are malformed and do not match the contents of
your /usr/local/lib/pkgconfig/libecap.pc file!

What output does the following one-line command produce?

PKG_CONFIG_PATH=/usr/local/lib/pkgconfig /usr/bin/pkg-config
--print-errors --debug "libecap >= 1.0 libecap < 1.1"



> I did the below to no effect 
> make -k clean; 
> ./configure  --enable-ecap
>  make > /tmp/build.log 2>&1

FYI: The above is missing parenthesis and captures make output without
capturing ./configure output. I doubt that lost ./configure output would
be useful right now, but please be more careful next time.

Alex.

> -----Original Message-----
> From: Alex Rousskov [mailto:rousskov at measurement-factory.com]
> Sent: Tuesday, June 13, 2017 2:12 AM
> To: norbert.naveen at tayana.in; squid-users at lists.squid-cache.org
> Subject: Re: [squid-users] Error Compiling squid-3.5.26 with libecap-1.0.1
> on CentOS Linux release 7.3.1611
> 
> On 06/12/2017 11:29 AM, Norbert Naveen wrote:
> 
>> I have attached Outputs of both Configure and Make And  libecap.pc 
>> contents are as below
> 
> Your ./configure output and libecap.pc contents look good to me, but there
> is no sign of the eCAP library (-lecap) being linked with Squid during
> "make". If running "make clean; ./configure ...; make" does not fix this,
> then please find a way to post a link to the _entire_ build log. Something
> along these lines should be able to capture it:
> 
>   $ make -k clean; (./configure ...; make) > /tmp/build.log 2>&1
> 
> You may want to compress /tmp/build.log. Please post a link to some public
> file sharing site here instead of the file itself. Even compressed, it may
> be too big to email.
> 
> Also, what does the following command output?
> 
>   $ grep 'EXT_LIBECAP_.*=' src/adaptation/ecap/Makefile
> 
> (adjust the Makefile path if necessary if you are not building Squid its
> source directory)
> 
> 
> Cheers,
> 
> Alex.
> 



From rousskov at measurement-factory.com  Tue Jun 13 15:49:44 2017
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Tue, 13 Jun 2017 09:49:44 -0600
Subject: [squid-users] Error Compiling squid-3.5.26 with libecap-1.0.1
 on CentOS Linux release 7.3.1611
In-Reply-To: <vmime.593ffea5.6ca6.3e11588364ca0075@ms249-lin-003.rotterdam.bazuin.nl>
References: <003e01d2e453$c8d24e50$5a76eaf0$@tayana.in>
 <vmime.593ffea5.6ca6.3e11588364ca0075@ms249-lin-003.rotterdam.bazuin.nl>
Message-ID: <3453f05d-a7f1-913a-baac-1085d166c64f@measurement-factory.com>

On 06/13/2017 09:03 AM, L.P.H. van Belle wrote:
> Looks the same like.
> 
> http://squid-web-proxy-cache.1019090.n4.nabble.com/Compiling-squid-3-5-4-with-ecap-enabled-td4671325.html 

The "undefined references" problem on that thread looks the same, but
the "PKG_CONFIG_PATH is the path to the pkg-config binary" solution(?)
to that problem sounds wrong because PKG_CONFIG_PATH is where pkg-config
looks for .pc files, not where pgk-config binary itself lives.

AFAICT, Naveen is using PKG_CONFIG_PATH correctly, and I see no serious
problems with Naveen's build commands. I do not know why the generated
Makefile gets bogus variable values. Could be some kind of pkg-config or
PKG_CHECK_MODULES bug or compatibility problem. Hopefully, pkg-config
debugging I asked for in my previous email will point us in the right
direction.

Alex.


>> -----Oorspronkelijk bericht-----
>> Van: squid-users 
>> [mailto:squid-users-bounces at lists.squid-cache.org] Namens 
>> Norbert Naveen
>> Verzonden: dinsdag 13 juni 2017 16:46
>> Aan: 'Alex Rousskov'; squid-users at lists.squid-cache.org
>> Onderwerp: Re: [squid-users] Error Compiling squid-3.5.26 
>> with libecap-1.0.1 on CentOS Linux release 7.3.1611
>>
>> Hello
>>   The Make output can be found at
>>   https://drive.google.com/open?id=0B_dDVNpzSGEKcFlMSlBVZWs5c2c
>>
>>
>>  And the output of # grep 'EXT_LIBECAP_.*=' 
>> src/adaptation/ecap/Makefile EXT_LIBECAP_CFLAGS = 
>> /usr/local/lib EXT_LIBECAP_LIBS = /usr/local/lib
>>
>>
>>  I did the below to no effect
>> make -k clean;
>> ./configure  --enable-ecap
>>  make > /tmp/build.log 2>&1
>>
>> Build Log is available at
>> https://drive.google.com/open?id=0B_dDVNpzSGEKM3JSUHNJLWJlWjA
>>
>> Thanks
>> Naveen
>>
>> -----Original Message-----
>> From: Alex Rousskov [mailto:rousskov at measurement-factory.com]
>> Sent: Tuesday, June 13, 2017 2:12 AM
>> To: norbert.naveen at tayana.in; squid-users at lists.squid-cache.org
>> Subject: Re: [squid-users] Error Compiling squid-3.5.26 with 
>> libecap-1.0.1 on CentOS Linux release 7.3.1611
>>
>> On 06/12/2017 11:29 AM, Norbert Naveen wrote:
>>
>>> I have attached Outputs of both Configure and Make And  libecap.pc 
>>> contents are as below
>>
>> Your ./configure output and libecap.pc contents look good to 
>> me, but there is no sign of the eCAP library (-lecap) being 
>> linked with Squid during "make". If running "make clean; 
>> ./configure ...; make" does not fix this, then please find a 
>> way to post a link to the _entire_ build log. Something along 
>> these lines should be able to capture it:
>>
>>   $ make -k clean; (./configure ...; make) > /tmp/build.log 2>&1
>>
>> You may want to compress /tmp/build.log. Please post a link 
>> to some public file sharing site here instead of the file 
>> itself. Even compressed, it may be too big to email.
>>
>> Also, what does the following command output?
>>
>>   $ grep 'EXT_LIBECAP_.*=' src/adaptation/ecap/Makefile
>>
>> (adjust the Makefile path if necessary if you are not 
>> building Squid its source directory)
>>
>>
>> Cheers,
>>
>> Alex.
>>
>> _______________________________________________
>> squid-users mailing list
>> squid-users at lists.squid-cache.org
>> http://lists.squid-cache.org/listinfo/squid-users
>>
> 
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
> 



From squid3 at treenet.co.nz  Tue Jun  6 04:04:38 2017
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Tue, 6 Jun 2017 16:04:38 +1200
Subject: [squid-users] [squid-announce] Squid 4.0.20 beta is available
Message-ID: <ee86ca5a-983b-3a15-840f-6a55de3dc101@treenet.co.nz>

The Squid HTTP Proxy team is very pleased to announce the availability
of the Squid-4.0.20 release!


This release is a bug fix release resolving several issues found in the
prior Squid releases.


The major changes to be aware of:

* Regression Bug 4692: SSL-Bump breaks intercepted IPv6 connections

This bug applies to all IPv6 intercepted traffic (TPROXY, etc.). It is 
especially visible with SSL/TLS (port 443) traffic. It affects Google 
searches, YouTube videos, and many other websites. With non-TLS/SSL 
requests, it can cause what appear to be timeouts as well as other 
problems. It is a regression specific to the Squid-4 release series, not 
affecting any other installations.


* Regression Bug 4659: sslproxy_foreign_intermediate_certs does not work

This bug appears as loading of custom intermediate certificates not 
working since the auto-download feature was implemented  in Squid-4. 
This release is now able to verify a certificate chain with both 
configured intermediates and auto-downloaded CA certificates.


* Bug 4662: build errors with LibreSSL 2.4.4

This release updates the OpenSSL v1.1 support to use API feature 
detection to resolve many issues identified with LibreSSL and 
potentially other OpenSSL derived libraries. New tests have been added, 
existing feature tests have been updated to obey the --with-openssl=PATH 
parameter more accurately for custom library locations, and the squid -v 
output is updated to report which library is being loaded and used at 
run-time.

As such there are some potentially significant changes to the code being 
used by LibreSSL and other derivative libraries. These should build and 
work now, but are not specifically tested by the Squid team developing 
the TLS/SSL code. Community testing and feedback is very welcome.


* Bug 4321: ssl_bump terminate does not terminate at step1

This release adds support for terminating TLS connections before any TLS 
protocol has been received. Previous versions of Squid would require 
some of the handshake to be received before terminate would work. This 
also causes non-TLS connections to be able to properly terminate before 
step1 of the SSL-Bump process.


* Improved cache_peer handling

This release updates the DEAD peer probe behaviour and handling to 
reduce HTTP response times when a cache_peer previously marked DEAD is 
involved as a potential destination for the request. For example as a 
failover destination after an initial attempt to a LIVE peer failed, or 
as a probe to investigate peer recovery when ICP, HTCP, Digest, NetDB 
and ICMP are all disabled.

Also, as of this release a new DNS query no longer revives DEAD peers 
unconditionally. This prevents periodic timeouts on transactions when 
DNS TTL is short and a peer is unavailable for extended periods of time 
relative to that TTL.

These changes will impact all Squid installations depending on these 
passive DNS or HTTP revival methods as the sole ways for peers to be 
detected as usable once they go down. An active probe of at least one 
type mentioned above is now required to avoid an increase in user 
visible connection failures.


* Make PID file check/creation atomic and earlier

This release adds further improvements to the Squid startup process for 
better PID file related behaviour to set the file contents earlier and 
in an atomic manner. Fixing many race condition issues when SMP workers 
are involved or an init system such as systemd, upstart, and OpenRC with 
potentially parallel startup procedures is used.


* OpenSSL support better compliance with license requirements

The OpenSSL license requires that all binaries which are built to 
utilize the library API (that includes any library derived from OpenSSL) 
must publicly advertise that OpenSSL or derivative library in all 
documentation detailing features of that software.

This release of Squid will now include the required OpenSSL 
advertisement on builds -v output where features are displayed. This is
primarily intended as a way to easily identify which library is being 
used by Squid at run-time when multiple libraries are present on a system.

Please note even with this update Squid is still not directly compatible 
with the OpenSSL terms of distribution. Distributors of OpenSSL enabled 
Squid are required to ensure they meet both GPL and OpenSSL licensing 
requirements.



  All users of Squid-4.x are urged to upgrade to this release as
soon as possible.

  All users of Squid-3 are encouraged to test this release out and plan
for upgrades where possible.


  See the ChangeLog for the full list of changes in this and earlier
  releases.

Please refer to the release notes at
http://www.squid-cache.org/Versions/v4/RELEASENOTES.html
when you are ready to make the switch to Squid-4

This new release can be downloaded from our HTTP or FTP servers

  http://www.squid-cache.org/Versions/v4/
  ftp://ftp.squid-cache.org/pub/squid/
  ftp://ftp.squid-cache.org/pub/archive/4/

or the mirrors. For a list of mirror sites see

  http://www.squid-cache.org/Download/http-mirrors.html
  http://www.squid-cache.org/Download/mirrors.html

If you encounter any issues with this release please file a bug report.
http://bugs.squid-cache.org/


Amos Jeffries

_______________________________________________
squid-announce mailing list
squid-announce at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-announce

From squid3 at treenet.co.nz  Tue Jun  6 04:04:49 2017
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Tue, 6 Jun 2017 16:04:49 +1200
Subject: [squid-users] [squid-announce] Squid 3.5.26 is available
Message-ID: <67fef825-dea4-ecf0-817e-590ae2b29082@treenet.co.nz>

The Squid HTTP Proxy team is very pleased to announce the availability
of the Squid-3.5.26 release!


This release is a bug fix release resolving several issues found in the
prior Squid releases.


The major changes to be aware of:

* Bug 4711: SubjectAlternativeNames is missing in some generated 
certificates

Previous releases of Squid were not able to generate valid mimic 
certificates from AltName server certificate field only. This leads to 
security error [missing_subjectAltName] in modern browsers (both 
Chrome/Firefox this time), and, net::ERR_CERT_COMMON_NAME_INVALID errors 
visible to users.


* Bug 4682: ignoring http_access deny when client-first bumping mode is used

This bug appears as Squid failing to identify some HTTP requests which 
are tunneled inside an already established client-first bumped tunnel, 
and this is results in ignoring http_access denied for these requests.


* Bug 4589: ssl_crtd: returning zero on failure

This bug has been affecting some init scripts that were depending on the 
tool return values to detect when it failed to initialize the 
certificate database. This does not resolve any initialization issues 
directly,  merely allows init scripts to be made aware of them before 
Squid is started.


* Bug 3102 and 3772: FTP directory listings display issues

These bugs appears as line wrap and path truncation errors in FTP 
directory listings from some FTP servers.


* OpenSSL support better compliance with license requirements

The OpenSSL license requires that all binaries which are built to 
utilize the library API (that includes any library derived from OpenSSL) 
must publicly advertise that OpenSSL or derivative library in all 
documentation detailing features of that software.

This release of Squid will now include the required OpenSSL 
advertisement on builds -v output where features are displayed. This is 
primarily intended as a way to easily identify which library is being 
used by Squid at run-time when multiple libraries are present on a system.

Please note even with this update Squid is still not directly compatible 
with the OpenSSL terms of distribution. Distributors of OpenSSL enabled 
Squid are required to ensure they meet both GPL and OpenSSL licensing 
requirements.



  All users of Squid-3 with SSL-Bump functionality are encouraged to
upgrade to this release as soon as possible.

  All other users of Squid-3 are encouraged to upgrade to this release as
time permits.


  See the ChangeLog for the full list of changes in this and earlier
  releases.

Please refer to the release notes at
http://www.squid-cache.org/Versions/v3/3.5/RELEASENOTES.html
when you are ready to make the switch to Squid-3.5

Upgrade tip:
   "squid -k parse" is starting to display even more
    useful hints about squid.conf changes.

This new release can be downloaded from our HTTP or FTP servers

  http://www.squid-cache.org/Versions/v3/3.5/
  ftp://ftp.squid-cache.org/pub/squid/
  ftp://ftp.squid-cache.org/pub/archive/3.5/

or the mirrors. For a list of mirror sites see

  http://www.squid-cache.org/Download/http-mirrors.html
  http://www.squid-cache.org/Download/mirrors.html

If you encounter any issues with this release please file a bug report.
http://bugs.squid-cache.org/


Amos Jeffries

_______________________________________________
squid-announce mailing list
squid-announce at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-announce

From jramirez.uy at gmail.com  Tue Jun 13 16:16:06 2017
From: jramirez.uy at gmail.com (=?UTF-8?Q?Juan_Ram=C3=ADrez?=)
Date: Tue, 13 Jun 2017 13:16:06 -0300
Subject: [squid-users] Error Compiling squid-3.5.26 with libecap-1.0.1
 on CentOS Linux release 7.3.1611
In-Reply-To: <3453f05d-a7f1-913a-baac-1085d166c64f@measurement-factory.com>
References: <003e01d2e453$c8d24e50$5a76eaf0$@tayana.in>
 <vmime.593ffea5.6ca6.3e11588364ca0075@ms249-lin-003.rotterdam.bazuin.nl>
 <3453f05d-a7f1-913a-baac-1085d166c64f@measurement-factory.com>
Message-ID: <CAEK0aNsFZeiFdCQYBe-26EGJxRA8Shp+0yCObse8oox7Dk_9DQ@mail.gmail.com>

>From this output:

    === configuring in libltdl (/root/install/squid-3.5.26/libltdl)
    configure: running /bin/sh ./configure --disable-option-checking
    '--prefix=/usr/local/squid'  '--enable-ecap'
    'PKG_CONFIG_PATH=/usr/local/lib/pkgconfig'
    'EXT_LIBECAP_CFLAGS=/usr/local/lib' 'EXT_LIBECAP_LIBS=/usr/local/lib'

I would think that the environment variables EXT_LIBECAP_CFLAGS and
EXT_LIBECAP_LIBS
have been set before calling ./configure.

Would you mind checking if those variables are set?

Another option would be to bypass pkgconfig and set the compiler flags
directly, like this (asumming libecap is installed in /usr/local as the .pc
file shows):

    export EXT_LIBECAP_CFLAGS=-I/usr/local/include
    export EXT_LIBECAP_LIBS=-L/usr/local/lib -lecap



On Tue, Jun 13, 2017 at 12:49 PM, Alex Rousskov <
rousskov at measurement-factory.com> wrote:

> On 06/13/2017 09:03 AM, L.P.H. van Belle wrote:
> > Looks the same like.
> >
> > http://squid-web-proxy-cache.1019090.n4.nabble.com/
> Compiling-squid-3-5-4-with-ecap-enabled-td4671325.html
>
> The "undefined references" problem on that thread looks the same, but
> the "PKG_CONFIG_PATH is the path to the pkg-config binary" solution(?)
> to that problem sounds wrong because PKG_CONFIG_PATH is where pkg-config
> looks for .pc files, not where pgk-config binary itself lives.
>
> AFAICT, Naveen is using PKG_CONFIG_PATH correctly, and I see no serious
> problems with Naveen's build commands. I do not know why the generated
> Makefile gets bogus variable values. Could be some kind of pkg-config or
> PKG_CHECK_MODULES bug or compatibility problem. Hopefully, pkg-config
> debugging I asked for in my previous email will point us in the right
> direction.
>
> Alex.
>
>
> >> -----Oorspronkelijk bericht-----
> >> Van: squid-users
> >> [mailto:squid-users-bounces at lists.squid-cache.org] Namens
> >> Norbert Naveen
> >> Verzonden: dinsdag 13 juni 2017 16:46
> >> Aan: 'Alex Rousskov'; squid-users at lists.squid-cache.org
> >> Onderwerp: Re: [squid-users] Error Compiling squid-3.5.26
> >> with libecap-1.0.1 on CentOS Linux release 7.3.1611
> >>
> >> Hello
> >>   The Make output can be found at
> >>   https://drive.google.com/open?id=0B_dDVNpzSGEKcFlMSlBVZWs5c2c
> >>
> >>
> >>  And the output of # grep 'EXT_LIBECAP_.*='
> >> src/adaptation/ecap/Makefile EXT_LIBECAP_CFLAGS =
> >> /usr/local/lib EXT_LIBECAP_LIBS = /usr/local/lib
> >>
> >>
> >>  I did the below to no effect
> >> make -k clean;
> >> ./configure  --enable-ecap
> >>  make > /tmp/build.log 2>&1
> >>
> >> Build Log is available at
> >> https://drive.google.com/open?id=0B_dDVNpzSGEKM3JSUHNJLWJlWjA
> >>
> >> Thanks
> >> Naveen
> >>
> >> -----Original Message-----
> >> From: Alex Rousskov [mailto:rousskov at measurement-factory.com]
> >> Sent: Tuesday, June 13, 2017 2:12 AM
> >> To: norbert.naveen at tayana.in; squid-users at lists.squid-cache.org
> >> Subject: Re: [squid-users] Error Compiling squid-3.5.26 with
> >> libecap-1.0.1 on CentOS Linux release 7.3.1611
> >>
> >> On 06/12/2017 11:29 AM, Norbert Naveen wrote:
> >>
> >>> I have attached Outputs of both Configure and Make And  libecap.pc
> >>> contents are as below
> >>
> >> Your ./configure output and libecap.pc contents look good to
> >> me, but there is no sign of the eCAP library (-lecap) being
> >> linked with Squid during "make". If running "make clean;
> >> ./configure ...; make" does not fix this, then please find a
> >> way to post a link to the _entire_ build log. Something along
> >> these lines should be able to capture it:
> >>
> >>   $ make -k clean; (./configure ...; make) > /tmp/build.log 2>&1
> >>
> >> You may want to compress /tmp/build.log. Please post a link
> >> to some public file sharing site here instead of the file
> >> itself. Even compressed, it may be too big to email.
> >>
> >> Also, what does the following command output?
> >>
> >>   $ grep 'EXT_LIBECAP_.*=' src/adaptation/ecap/Makefile
> >>
> >> (adjust the Makefile path if necessary if you are not
> >> building Squid its source directory)
> >>
> >>
> >> Cheers,
> >>
> >> Alex.
> >>
> >> _______________________________________________
> >> squid-users mailing list
> >> squid-users at lists.squid-cache.org
> >> http://lists.squid-cache.org/listinfo/squid-users
> >>
> >
> > _______________________________________________
> > squid-users mailing list
> > squid-users at lists.squid-cache.org
> > http://lists.squid-cache.org/listinfo/squid-users
> >
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>



-- 
Juan
:wq
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20170613/a6fbba0f/attachment.htm>

From norbert.naveen at tayana.in  Tue Jun 13 19:07:24 2017
From: norbert.naveen at tayana.in (Norbert Naveen)
Date: Wed, 14 Jun 2017 00:37:24 +0530
Subject: [squid-users] Error Compiling squid-3.5.26 with libecap-1.0.1
	on CentOS Linux release 7.3.1611
In-Reply-To: <8c1103ed-138e-8918-0127-5e612d3bd64f@measurement-factory.com>
References: <00ae01d2e39a$0e0038d0$2a00aa70$@tayana.in>
 <ed4a5498-9d07-bef0-c304-6653ebec2851@measurement-factory.com>
 <00cb01d2e3a1$67cc7ca0$376575e0$@tayana.in>
 <6c8ea344-5001-00fb-ad2e-9c89e5ceb7d1@measurement-factory.com>
 <003e01d2e453$c8d24e50$5a76eaf0$@tayana.in>
 <8c1103ed-138e-8918-0127-5e612d3bd64f@measurement-factory.com>
Message-ID: <007a01d2e478$4aecd850$e0c688f0$@tayana.in>

Hello Alex 

Sorry for the mistake of missing the o/p of configure 
# PKG_CONFIG_PATH=/usr/local/lib/pkgconfig /usr/bin/pkg-config --print-errors --debug "libecap >= 1.0 libecap < 1.1"
Error printing disabled by default, value of --print-errors: 1
Error printing enabled
Adding virtual 'pkg-config' package to list of known packages
Scanning directory '/usr/local/lib/pkgconfig'
Ignoring file '.' in search directory; not a .pc file
Ignoring file '..' in search directory; not a .pc file
File 'libecap.pc' appears to be a .pc file
Will find package 'libecap' in file '/usr/local/lib/pkgconfig/libecap.pc'
Scanning directory '/usr/lib64/pkgconfig'
Ignoring file '.' in search directory; not a .pc file
Ignoring file '..' in search directory; not a .pc file
File 'fontutil.pc' appears to be a .pc file
Will find package 'fontutil' in file '/usr/lib64/pkgconfig/fontutil.pc'
File 'systemd.pc' appears to be a .pc file
Will find package 'systemd' in file '/usr/lib64/pkgconfig/systemd.pc'
File 'libpcre.pc' appears to be a .pc file
Will find package 'libpcre' in file '/usr/lib64/pkgconfig/libpcre.pc'
File 'libpcre16.pc' appears to be a .pc file
Will find package 'libpcre16' in file '/usr/lib64/pkgconfig/libpcre16.pc'
File 'libpcre32.pc' appears to be a .pc file
Will find package 'libpcre32' in file '/usr/lib64/pkgconfig/libpcre32.pc'
File 'libpcrecpp.pc' appears to be a .pc file
Will find package 'libpcrecpp' in file '/usr/lib64/pkgconfig/libpcrecpp.pc'
File 'libpcreposix.pc' appears to be a .pc file
Will find package 'libpcreposix' in file '/usr/lib64/pkgconfig/libpcreposix.pc'
File 'libsepol.pc' appears to be a .pc file
Will find package 'libsepol' in file '/usr/lib64/pkgconfig/libsepol.pc'
File 'libselinux.pc' appears to be a .pc file
Will find package 'libselinux' in file '/usr/lib64/pkgconfig/libselinux.pc'
File 'com_err.pc' appears to be a .pc file
Will find package 'com_err' in file '/usr/lib64/pkgconfig/com_err.pc'
File 'libverto.pc' appears to be a .pc file
Will find package 'libverto' in file '/usr/lib64/pkgconfig/libverto.pc'
File 'gssrpc.pc' appears to be a .pc file
Will find package 'gssrpc' in file '/usr/lib64/pkgconfig/gssrpc.pc'
File 'kadm-client.pc' appears to be a .pc file
Will find package 'kadm-client' in file '/usr/lib64/pkgconfig/kadm-client.pc'
File 'kadm-server.pc' appears to be a .pc file
Will find package 'kadm-server' in file '/usr/lib64/pkgconfig/kadm-server.pc'
File 'kdb.pc' appears to be a .pc file
Will find package 'kdb' in file '/usr/lib64/pkgconfig/kdb.pc'
File 'krb5-gssapi.pc' appears to be a .pc file
Will find package 'krb5-gssapi' in file '/usr/lib64/pkgconfig/krb5-gssapi.pc'
File 'krb5.pc' appears to be a .pc file
Will find package 'krb5' in file '/usr/lib64/pkgconfig/krb5.pc'
File 'mit-krb5-gssapi.pc' appears to be a .pc file
Will find package 'mit-krb5-gssapi' in file '/usr/lib64/pkgconfig/mit-krb5-gssapi.pc'
File 'mit-krb5.pc' appears to be a .pc file
Will find package 'mit-krb5' in file '/usr/lib64/pkgconfig/mit-krb5.pc'
File 'zlib.pc' appears to be a .pc file
Will find package 'zlib' in file '/usr/lib64/pkgconfig/zlib.pc'
File 'libcrypto.pc' appears to be a .pc file
Will find package 'libcrypto' in file '/usr/lib64/pkgconfig/libcrypto.pc'
File 'libssl.pc' appears to be a .pc file
Will find package 'libssl' in file '/usr/lib64/pkgconfig/libssl.pc'
File 'openssl.pc' appears to be a .pc file
Will find package 'openssl' in file '/usr/lib64/pkgconfig/openssl.pc'
Scanning directory '/usr/share/pkgconfig'
Ignoring file '.' in search directory; not a .pc file
Ignoring file '..' in search directory; not a .pc file
File 'shared-mime-info.pc' appears to be a .pc file
Will find package 'shared-mime-info' in file '/usr/share/pkgconfig/shared-mime-info.pc'
File 'spice-protocol.pc' appears to be a .pc file
Will find package 'spice-protocol' in file '/usr/share/pkgconfig/spice-protocol.pc'
File 'gnome-video-effects.pc' appears to be a .pc file
Will find package 'gnome-video-effects' in file '/usr/share/pkgconfig/gnome-video-effects.pc'
File 'udev.pc' appears to be a .pc file
Will find package 'udev' in file '/usr/share/pkgconfig/udev.pc'
File 'gnome-keybindings.pc' appears to be a .pc file
Will find package 'gnome-keybindings' in file '/usr/share/pkgconfig/gnome-keybindings.pc'
File 'usbutils.pc' appears to be a .pc file
Will find package 'usbutils' in file '/usr/share/pkgconfig/usbutils.pc'
File 'gnome-icon-theme-symbolic.pc' appears to be a .pc file
Will find package 'gnome-icon-theme-symbolic' in file '/usr/share/pkgconfig/gnome-icon-theme-symbolic.pc'
File 'bash-completion.pc' appears to be a .pc file
Will find package 'bash-completion' in file '/usr/share/pkgconfig/bash-completion.pc'
Looking for package 'libecap'
Looking for package 'libecap-uninstalled'
Reading 'libecap' from file '/usr/local/lib/pkgconfig/libecap.pc'
Parsing package file '/usr/local/lib/pkgconfig/libecap.pc'
  line>prefix=/usr/local
 Variable declaration, 'prefix' has value '/usr/local'
  line>exec_prefix=${prefix}
 Variable declaration, 'exec_prefix' has value '/usr/local'
  line>libdir=${exec_prefix}/lib
 Variable declaration, 'libdir' has value '/usr/local/lib'
  line>includedir=${prefix}/include
 Variable declaration, 'includedir' has value '/usr/local/include'
  line>
  line>Name: eCAP
  line>Description: Allows a network application to outsource content analysis and adaptation to a loadable module.
  line>URL: http://www.e-cap.org/
  line>Version: 1.0.1
  line>Libs: -L${libdir} -lecap
  line>Cflags: -I${includedir}
Path position of 'eCAP' is 1
Adding 'libecap' to list of known packages, returning as package 'libecap'

Shall make clean and send all the details 

Thanks 


-----Original Message-----
From: Alex Rousskov [mailto:rousskov at measurement-factory.com] 
Sent: Tuesday, June 13, 2017 9:09 PM
To: norbert.naveen at tayana.in; squid-users at lists.squid-cache.org
Subject: Re: [squid-users] Error Compiling squid-3.5.26 with libecap-1.0.1 on CentOS Linux release 7.3.1611

On 06/13/2017 08:46 AM, Norbert Naveen wrote:

> # grep 'EXT_LIBECAP_.*=' src/adaptation/ecap/Makefile 
> EXT_LIBECAP_CFLAGS = /usr/local/lib EXT_LIBECAP_LIBS = /usr/local/lib

Strange. These variables are malformed and do not match the contents of your /usr/local/lib/pkgconfig/libecap.pc file!

What output does the following one-line command produce?

PKG_CONFIG_PATH=/usr/local/lib/pkgconfig /usr/bin/pkg-config --print-errors --debug "libecap >= 1.0 libecap < 1.1"



> I did the below to no effect
> make -k clean;
> ./configure  --enable-ecap
>  make > /tmp/build.log 2>&1

FYI: The above is missing parenthesis and captures make output without capturing ./configure output. I doubt that lost ./configure output would be useful right now, but please be more careful next time.

Alex.

> -----Original Message-----
> From: Alex Rousskov [mailto:rousskov at measurement-factory.com]
> Sent: Tuesday, June 13, 2017 2:12 AM
> To: norbert.naveen at tayana.in; squid-users at lists.squid-cache.org
> Subject: Re: [squid-users] Error Compiling squid-3.5.26 with 
> libecap-1.0.1 on CentOS Linux release 7.3.1611
> 
> On 06/12/2017 11:29 AM, Norbert Naveen wrote:
> 
>> I have attached Outputs of both Configure and Make And  libecap.pc 
>> contents are as below
> 
> Your ./configure output and libecap.pc contents look good to me, but 
> there is no sign of the eCAP library (-lecap) being linked with Squid 
> during "make". If running "make clean; ./configure ...; make" does not 
> fix this, then please find a way to post a link to the _entire_ build 
> log. Something along these lines should be able to capture it:
> 
>   $ make -k clean; (./configure ...; make) > /tmp/build.log 2>&1
> 
> You may want to compress /tmp/build.log. Please post a link to some 
> public file sharing site here instead of the file itself. Even 
> compressed, it may be too big to email.
> 
> Also, what does the following command output?
> 
>   $ grep 'EXT_LIBECAP_.*=' src/adaptation/ecap/Makefile
> 
> (adjust the Makefile path if necessary if you are not building Squid 
> its source directory)
> 
> 
> Cheers,
> 
> Alex.
> 



From norbert.naveen at tayana.in  Tue Jun 13 19:37:30 2017
From: norbert.naveen at tayana.in (Norbert Naveen)
Date: Wed, 14 Jun 2017 01:07:30 +0530
Subject: [squid-users] Error Compiling squid-3.5.26 with libecap-1.0.1
	on CentOS Linux release 7.3.1611
References: <00ae01d2e39a$0e0038d0$2a00aa70$@tayana.in>
 <ed4a5498-9d07-bef0-c304-6653ebec2851@measurement-factory.com>
 <00cb01d2e3a1$67cc7ca0$376575e0$@tayana.in>
 <6c8ea344-5001-00fb-ad2e-9c89e5ceb7d1@measurement-factory.com>
 <003e01d2e453$c8d24e50$5a76eaf0$@tayana.in>
 <8c1103ed-138e-8918-0127-5e612d3bd64f@measurement-factory.com> 
Message-ID: <001001d2e47c$7f7f1c00$7e7d5400$@tayana.in>

Hello Alex 
 I guess I know the culprit 
 In bashrc 
 The following 4 lines were present ,, I have commented it out now . 
 Shall try to make -k clean and then redo 
Thanks 

-----Original Message-----
From: Norbert Naveen [mailto:norbert.naveen at tayana.in] 
Sent: Wednesday, June 14, 2017 12:37 AM
To: 'Alex Rousskov' <rousskov at measurement-factory.com>; 'squid-users at lists.squid-cache.org' <squid-users at lists.squid-cache.org>
Subject: RE: [squid-users] Error Compiling squid-3.5.26 with libecap-1.0.1 on CentOS Linux release 7.3.1611

Hello Alex 

Sorry for the mistake of missing the o/p of configure # PKG_CONFIG_PATH=/usr/local/lib/pkgconfig /usr/bin/pkg-config --print-errors --debug "libecap >= 1.0 libecap < 1.1"
Error printing disabled by default, value of --print-errors: 1 Error printing enabled Adding virtual 'pkg-config' package to list of known packages Scanning directory '/usr/local/lib/pkgconfig'
Ignoring file '.' in search directory; not a .pc file Ignoring file '..' in search directory; not a .pc file File 'libecap.pc' appears to be a .pc file Will find package 'libecap' in file '/usr/local/lib/pkgconfig/libecap.pc'
Scanning directory '/usr/lib64/pkgconfig'
Ignoring file '.' in search directory; not a .pc file Ignoring file '..' in search directory; not a .pc file File 'fontutil.pc' appears to be a .pc file Will find package 'fontutil' in file '/usr/lib64/pkgconfig/fontutil.pc'
File 'systemd.pc' appears to be a .pc file Will find package 'systemd' in file '/usr/lib64/pkgconfig/systemd.pc'
File 'libpcre.pc' appears to be a .pc file Will find package 'libpcre' in file '/usr/lib64/pkgconfig/libpcre.pc'
File 'libpcre16.pc' appears to be a .pc file Will find package 'libpcre16' in file '/usr/lib64/pkgconfig/libpcre16.pc'
File 'libpcre32.pc' appears to be a .pc file Will find package 'libpcre32' in file '/usr/lib64/pkgconfig/libpcre32.pc'
File 'libpcrecpp.pc' appears to be a .pc file Will find package 'libpcrecpp' in file '/usr/lib64/pkgconfig/libpcrecpp.pc'
File 'libpcreposix.pc' appears to be a .pc file Will find package 'libpcreposix' in file '/usr/lib64/pkgconfig/libpcreposix.pc'
File 'libsepol.pc' appears to be a .pc file Will find package 'libsepol' in file '/usr/lib64/pkgconfig/libsepol.pc'
File 'libselinux.pc' appears to be a .pc file Will find package 'libselinux' in file '/usr/lib64/pkgconfig/libselinux.pc'
File 'com_err.pc' appears to be a .pc file Will find package 'com_err' in file '/usr/lib64/pkgconfig/com_err.pc'
File 'libverto.pc' appears to be a .pc file Will find package 'libverto' in file '/usr/lib64/pkgconfig/libverto.pc'
File 'gssrpc.pc' appears to be a .pc file Will find package 'gssrpc' in file '/usr/lib64/pkgconfig/gssrpc.pc'
File 'kadm-client.pc' appears to be a .pc file Will find package 'kadm-client' in file '/usr/lib64/pkgconfig/kadm-client.pc'
File 'kadm-server.pc' appears to be a .pc file Will find package 'kadm-server' in file '/usr/lib64/pkgconfig/kadm-server.pc'
File 'kdb.pc' appears to be a .pc file
Will find package 'kdb' in file '/usr/lib64/pkgconfig/kdb.pc'
File 'krb5-gssapi.pc' appears to be a .pc file Will find package 'krb5-gssapi' in file '/usr/lib64/pkgconfig/krb5-gssapi.pc'
File 'krb5.pc' appears to be a .pc file
Will find package 'krb5' in file '/usr/lib64/pkgconfig/krb5.pc'
File 'mit-krb5-gssapi.pc' appears to be a .pc file Will find package 'mit-krb5-gssapi' in file '/usr/lib64/pkgconfig/mit-krb5-gssapi.pc'
File 'mit-krb5.pc' appears to be a .pc file Will find package 'mit-krb5' in file '/usr/lib64/pkgconfig/mit-krb5.pc'
File 'zlib.pc' appears to be a .pc file
Will find package 'zlib' in file '/usr/lib64/pkgconfig/zlib.pc'
File 'libcrypto.pc' appears to be a .pc file Will find package 'libcrypto' in file '/usr/lib64/pkgconfig/libcrypto.pc'
File 'libssl.pc' appears to be a .pc file Will find package 'libssl' in file '/usr/lib64/pkgconfig/libssl.pc'
File 'openssl.pc' appears to be a .pc file Will find package 'openssl' in file '/usr/lib64/pkgconfig/openssl.pc'
Scanning directory '/usr/share/pkgconfig'
Ignoring file '.' in search directory; not a .pc file Ignoring file '..' in search directory; not a .pc file File 'shared-mime-info.pc' appears to be a .pc file Will find package 'shared-mime-info' in file '/usr/share/pkgconfig/shared-mime-info.pc'
File 'spice-protocol.pc' appears to be a .pc file Will find package 'spice-protocol' in file '/usr/share/pkgconfig/spice-protocol.pc'
File 'gnome-video-effects.pc' appears to be a .pc file Will find package 'gnome-video-effects' in file '/usr/share/pkgconfig/gnome-video-effects.pc'
File 'udev.pc' appears to be a .pc file
Will find package 'udev' in file '/usr/share/pkgconfig/udev.pc'
File 'gnome-keybindings.pc' appears to be a .pc file Will find package 'gnome-keybindings' in file '/usr/share/pkgconfig/gnome-keybindings.pc'
File 'usbutils.pc' appears to be a .pc file Will find package 'usbutils' in file '/usr/share/pkgconfig/usbutils.pc'
File 'gnome-icon-theme-symbolic.pc' appears to be a .pc file Will find package 'gnome-icon-theme-symbolic' in file '/usr/share/pkgconfig/gnome-icon-theme-symbolic.pc'
File 'bash-completion.pc' appears to be a .pc file Will find package 'bash-completion' in file '/usr/share/pkgconfig/bash-completion.pc'
Looking for package 'libecap'
Looking for package 'libecap-uninstalled'
Reading 'libecap' from file '/usr/local/lib/pkgconfig/libecap.pc'
Parsing package file '/usr/local/lib/pkgconfig/libecap.pc'
  line>prefix=/usr/local
 Variable declaration, 'prefix' has value '/usr/local'
  line>exec_prefix=${prefix}
 Variable declaration, 'exec_prefix' has value '/usr/local'
  line>libdir=${exec_prefix}/lib
 Variable declaration, 'libdir' has value '/usr/local/lib'
  line>includedir=${prefix}/include
 Variable declaration, 'includedir' has value '/usr/local/include'
  line>
  line>Name: eCAP
  line>Description: Allows a network application to outsource content analysis and adaptation to a loadable module.
  line>URL: http://www.e-cap.org/
  line>Version: 1.0.1
  line>Libs: -L${libdir} -lecap
  line>Cflags: -I${includedir}
Path position of 'eCAP' is 1
Adding 'libecap' to list of known packages, returning as package 'libecap'

Shall make clean and send all the details 

Thanks 


-----Original Message-----
From: Alex Rousskov [mailto:rousskov at measurement-factory.com]
Sent: Tuesday, June 13, 2017 9:09 PM
To: norbert.naveen at tayana.in; squid-users at lists.squid-cache.org
Subject: Re: [squid-users] Error Compiling squid-3.5.26 with libecap-1.0.1 on CentOS Linux release 7.3.1611

On 06/13/2017 08:46 AM, Norbert Naveen wrote:

> # grep 'EXT_LIBECAP_.*=' src/adaptation/ecap/Makefile 
> EXT_LIBECAP_CFLAGS = /usr/local/lib EXT_LIBECAP_LIBS = /usr/local/lib

Strange. These variables are malformed and do not match the contents of your /usr/local/lib/pkgconfig/libecap.pc file!

What output does the following one-line command produce?

PKG_CONFIG_PATH=/usr/local/lib/pkgconfig /usr/bin/pkg-config --print-errors --debug "libecap >= 1.0 libecap < 1.1"



> I did the below to no effect
> make -k clean;
> ./configure  --enable-ecap
>  make > /tmp/build.log 2>&1

FYI: The above is missing parenthesis and captures make output without capturing ./configure output. I doubt that lost ./configure output would be useful right now, but please be more careful next time.

Alex.

> -----Original Message-----
> From: Alex Rousskov [mailto:rousskov at measurement-factory.com]
> Sent: Tuesday, June 13, 2017 2:12 AM
> To: norbert.naveen at tayana.in; squid-users at lists.squid-cache.org
> Subject: Re: [squid-users] Error Compiling squid-3.5.26 with
> libecap-1.0.1 on CentOS Linux release 7.3.1611
> 
> On 06/12/2017 11:29 AM, Norbert Naveen wrote:
> 
>> I have attached Outputs of both Configure and Make And  libecap.pc 
>> contents are as below
> 
> Your ./configure output and libecap.pc contents look good to me, but 
> there is no sign of the eCAP library (-lecap) being linked with Squid 
> during "make". If running "make clean; ./configure ...; make" does not 
> fix this, then please find a way to post a link to the _entire_ build 
> log. Something along these lines should be able to capture it:
> 
>   $ make -k clean; (./configure ...; make) > /tmp/build.log 2>&1
> 
> You may want to compress /tmp/build.log. Please post a link to some 
> public file sharing site here instead of the file itself. Even 
> compressed, it may be too big to email.
> 
> Also, what does the following command output?
> 
>   $ grep 'EXT_LIBECAP_.*=' src/adaptation/ecap/Makefile
> 
> (adjust the Makefile path if necessary if you are not building Squid 
> its source directory)
> 
> 
> Cheers,
> 
> Alex.
> 



From norbert.naveen at tayana.in  Tue Jun 13 19:40:23 2017
From: norbert.naveen at tayana.in (Norbert Naveen)
Date: Wed, 14 Jun 2017 01:10:23 +0530
Subject: [squid-users] Error Compiling squid-3.5.26 with libecap-1.0.1
	on CentOS Linux release 7.3.1611
References: <00ae01d2e39a$0e0038d0$2a00aa70$@tayana.in>
 <ed4a5498-9d07-bef0-c304-6653ebec2851@measurement-factory.com>
 <00cb01d2e3a1$67cc7ca0$376575e0$@tayana.in>
 <6c8ea344-5001-00fb-ad2e-9c89e5ceb7d1@measurement-factory.com>
 <003e01d2e453$c8d24e50$5a76eaf0$@tayana.in>
 <8c1103ed-138e-8918-0127-5e612d3bd64f@measurement-factory.com> 
Message-ID: <001101d2e47c$e678cdc0$b36a6940$@tayana.in>

Hello Alex and Squid Admins
Managed to Compile with ecap successfully 
Looks like the Error was due to earlier added export in bashrc 

Have questions on the 4 patches of ecap .. but I shall open a different Thread 


Thanks & Rgds
Naveen

-----Original Message-----
From: Norbert Naveen [mailto:norbert.naveen at tayana.in] 
Sent: Wednesday, June 14, 2017 1:08 AM
To: 'Alex Rousskov' <rousskov at measurement-factory.com>; 'squid-users at lists.squid-cache.org' <squid-users at lists.squid-cache.org>
Subject: RE: [squid-users] Error Compiling squid-3.5.26 with libecap-1.0.1 on CentOS Linux release 7.3.1611

Hello Alex
 I guess I know the culprit
 In bashrc
 The following 4 lines were present ,, I have commented it out now . 
 Shall try to make -k clean and then redo Thanks 

-----Original Message-----
From: Norbert Naveen [mailto:norbert.naveen at tayana.in]
Sent: Wednesday, June 14, 2017 12:37 AM
To: 'Alex Rousskov' <rousskov at measurement-factory.com>; 'squid-users at lists.squid-cache.org' <squid-users at lists.squid-cache.org>
Subject: RE: [squid-users] Error Compiling squid-3.5.26 with libecap-1.0.1 on CentOS Linux release 7.3.1611

Hello Alex 

Sorry for the mistake of missing the o/p of configure # PKG_CONFIG_PATH=/usr/local/lib/pkgconfig /usr/bin/pkg-config --print-errors --debug "libecap >= 1.0 libecap < 1.1"
Error printing disabled by default, value of --print-errors: 1 Error printing enabled Adding virtual 'pkg-config' package to list of known packages Scanning directory '/usr/local/lib/pkgconfig'
Ignoring file '.' in search directory; not a .pc file Ignoring file '..' in search directory; not a .pc file File 'libecap.pc' appears to be a .pc file Will find package 'libecap' in file '/usr/local/lib/pkgconfig/libecap.pc'
Scanning directory '/usr/lib64/pkgconfig'
Ignoring file '.' in search directory; not a .pc file Ignoring file '..' in search directory; not a .pc file File 'fontutil.pc' appears to be a .pc file Will find package 'fontutil' in file '/usr/lib64/pkgconfig/fontutil.pc'
File 'systemd.pc' appears to be a .pc file Will find package 'systemd' in file '/usr/lib64/pkgconfig/systemd.pc'
File 'libpcre.pc' appears to be a .pc file Will find package 'libpcre' in file '/usr/lib64/pkgconfig/libpcre.pc'
File 'libpcre16.pc' appears to be a .pc file Will find package 'libpcre16' in file '/usr/lib64/pkgconfig/libpcre16.pc'
File 'libpcre32.pc' appears to be a .pc file Will find package 'libpcre32' in file '/usr/lib64/pkgconfig/libpcre32.pc'
File 'libpcrecpp.pc' appears to be a .pc file Will find package 'libpcrecpp' in file '/usr/lib64/pkgconfig/libpcrecpp.pc'
File 'libpcreposix.pc' appears to be a .pc file Will find package 'libpcreposix' in file '/usr/lib64/pkgconfig/libpcreposix.pc'
File 'libsepol.pc' appears to be a .pc file Will find package 'libsepol' in file '/usr/lib64/pkgconfig/libsepol.pc'
File 'libselinux.pc' appears to be a .pc file Will find package 'libselinux' in file '/usr/lib64/pkgconfig/libselinux.pc'
File 'com_err.pc' appears to be a .pc file Will find package 'com_err' in file '/usr/lib64/pkgconfig/com_err.pc'
File 'libverto.pc' appears to be a .pc file Will find package 'libverto' in file '/usr/lib64/pkgconfig/libverto.pc'
File 'gssrpc.pc' appears to be a .pc file Will find package 'gssrpc' in file '/usr/lib64/pkgconfig/gssrpc.pc'
File 'kadm-client.pc' appears to be a .pc file Will find package 'kadm-client' in file '/usr/lib64/pkgconfig/kadm-client.pc'
File 'kadm-server.pc' appears to be a .pc file Will find package 'kadm-server' in file '/usr/lib64/pkgconfig/kadm-server.pc'
File 'kdb.pc' appears to be a .pc file
Will find package 'kdb' in file '/usr/lib64/pkgconfig/kdb.pc'
File 'krb5-gssapi.pc' appears to be a .pc file Will find package 'krb5-gssapi' in file '/usr/lib64/pkgconfig/krb5-gssapi.pc'
File 'krb5.pc' appears to be a .pc file
Will find package 'krb5' in file '/usr/lib64/pkgconfig/krb5.pc'
File 'mit-krb5-gssapi.pc' appears to be a .pc file Will find package 'mit-krb5-gssapi' in file '/usr/lib64/pkgconfig/mit-krb5-gssapi.pc'
File 'mit-krb5.pc' appears to be a .pc file Will find package 'mit-krb5' in file '/usr/lib64/pkgconfig/mit-krb5.pc'
File 'zlib.pc' appears to be a .pc file
Will find package 'zlib' in file '/usr/lib64/pkgconfig/zlib.pc'
File 'libcrypto.pc' appears to be a .pc file Will find package 'libcrypto' in file '/usr/lib64/pkgconfig/libcrypto.pc'
File 'libssl.pc' appears to be a .pc file Will find package 'libssl' in file '/usr/lib64/pkgconfig/libssl.pc'
File 'openssl.pc' appears to be a .pc file Will find package 'openssl' in file '/usr/lib64/pkgconfig/openssl.pc'
Scanning directory '/usr/share/pkgconfig'
Ignoring file '.' in search directory; not a .pc file Ignoring file '..' in search directory; not a .pc file File 'shared-mime-info.pc' appears to be a .pc file Will find package 'shared-mime-info' in file '/usr/share/pkgconfig/shared-mime-info.pc'
File 'spice-protocol.pc' appears to be a .pc file Will find package 'spice-protocol' in file '/usr/share/pkgconfig/spice-protocol.pc'
File 'gnome-video-effects.pc' appears to be a .pc file Will find package 'gnome-video-effects' in file '/usr/share/pkgconfig/gnome-video-effects.pc'
File 'udev.pc' appears to be a .pc file
Will find package 'udev' in file '/usr/share/pkgconfig/udev.pc'
File 'gnome-keybindings.pc' appears to be a .pc file Will find package 'gnome-keybindings' in file '/usr/share/pkgconfig/gnome-keybindings.pc'
File 'usbutils.pc' appears to be a .pc file Will find package 'usbutils' in file '/usr/share/pkgconfig/usbutils.pc'
File 'gnome-icon-theme-symbolic.pc' appears to be a .pc file Will find package 'gnome-icon-theme-symbolic' in file '/usr/share/pkgconfig/gnome-icon-theme-symbolic.pc'
File 'bash-completion.pc' appears to be a .pc file Will find package 'bash-completion' in file '/usr/share/pkgconfig/bash-completion.pc'
Looking for package 'libecap'
Looking for package 'libecap-uninstalled'
Reading 'libecap' from file '/usr/local/lib/pkgconfig/libecap.pc'
Parsing package file '/usr/local/lib/pkgconfig/libecap.pc'
  line>prefix=/usr/local
 Variable declaration, 'prefix' has value '/usr/local'
  line>exec_prefix=${prefix}
 Variable declaration, 'exec_prefix' has value '/usr/local'
  line>libdir=${exec_prefix}/lib
 Variable declaration, 'libdir' has value '/usr/local/lib'
  line>includedir=${prefix}/include
 Variable declaration, 'includedir' has value '/usr/local/include'
  line>
  line>Name: eCAP
  line>Description: Allows a network application to outsource content analysis and adaptation to a loadable module.
  line>URL: http://www.e-cap.org/
  line>Version: 1.0.1
  line>Libs: -L${libdir} -lecap
  line>Cflags: -I${includedir}
Path position of 'eCAP' is 1
Adding 'libecap' to list of known packages, returning as package 'libecap'

Shall make clean and send all the details 

Thanks 


-----Original Message-----
From: Alex Rousskov [mailto:rousskov at measurement-factory.com]
Sent: Tuesday, June 13, 2017 9:09 PM
To: norbert.naveen at tayana.in; squid-users at lists.squid-cache.org
Subject: Re: [squid-users] Error Compiling squid-3.5.26 with libecap-1.0.1 on CentOS Linux release 7.3.1611

On 06/13/2017 08:46 AM, Norbert Naveen wrote:

> # grep 'EXT_LIBECAP_.*=' src/adaptation/ecap/Makefile 
> EXT_LIBECAP_CFLAGS = /usr/local/lib EXT_LIBECAP_LIBS = /usr/local/lib

Strange. These variables are malformed and do not match the contents of your /usr/local/lib/pkgconfig/libecap.pc file!

What output does the following one-line command produce?

PKG_CONFIG_PATH=/usr/local/lib/pkgconfig /usr/bin/pkg-config --print-errors --debug "libecap >= 1.0 libecap < 1.1"



> I did the below to no effect
> make -k clean;
> ./configure  --enable-ecap
>  make > /tmp/build.log 2>&1

FYI: The above is missing parenthesis and captures make output without capturing ./configure output. I doubt that lost ./configure output would be useful right now, but please be more careful next time.

Alex.

> -----Original Message-----
> From: Alex Rousskov [mailto:rousskov at measurement-factory.com]
> Sent: Tuesday, June 13, 2017 2:12 AM
> To: norbert.naveen at tayana.in; squid-users at lists.squid-cache.org
> Subject: Re: [squid-users] Error Compiling squid-3.5.26 with
> libecap-1.0.1 on CentOS Linux release 7.3.1611
> 
> On 06/12/2017 11:29 AM, Norbert Naveen wrote:
> 
>> I have attached Outputs of both Configure and Make And  libecap.pc 
>> contents are as below
> 
> Your ./configure output and libecap.pc contents look good to me, but 
> there is no sign of the eCAP library (-lecap) being linked with Squid 
> during "make". If running "make clean; ./configure ...; make" does not 
> fix this, then please find a way to post a link to the _entire_ build 
> log. Something along these lines should be able to capture it:
> 
>   $ make -k clean; (./configure ...; make) > /tmp/build.log 2>&1
> 
> You may want to compress /tmp/build.log. Please post a link to some 
> public file sharing site here instead of the file itself. Even 
> compressed, it may be too big to email.
> 
> Also, what does the following command output?
> 
>   $ grep 'EXT_LIBECAP_.*=' src/adaptation/ecap/Makefile
> 
> (adjust the Makefile path if necessary if you are not building Squid 
> its source directory)
> 
> 
> Cheers,
> 
> Alex.
> 



From rousskov at measurement-factory.com  Tue Jun 13 19:47:35 2017
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Tue, 13 Jun 2017 13:47:35 -0600
Subject: [squid-users] Error Compiling squid-3.5.26 with libecap-1.0.1
 on CentOS Linux release 7.3.1611
In-Reply-To: <001001d2e47c$7f7f1c00$7e7d5400$@tayana.in>
References: <00ae01d2e39a$0e0038d0$2a00aa70$@tayana.in>
 <ed4a5498-9d07-bef0-c304-6653ebec2851@measurement-factory.com>
 <00cb01d2e3a1$67cc7ca0$376575e0$@tayana.in>
 <6c8ea344-5001-00fb-ad2e-9c89e5ceb7d1@measurement-factory.com>
 <003e01d2e453$c8d24e50$5a76eaf0$@tayana.in>
 <8c1103ed-138e-8918-0127-5e612d3bd64f@measurement-factory.com>
 <001001d2e47c$7f7f1c00$7e7d5400$@tayana.in>
Message-ID: <6bdb72d1-395c-0353-b7a1-c6ced2afe3c6@measurement-factory.com>

On 06/13/2017 01:37 PM, Norbert Naveen wrote:
>  I guess I know the culprit 

Glad you found a solution!


>  In bashrc 
>  The following 4 lines were present ,, I have commented it out now . 

Please try to post those four lines again, as plain text. Others may
find those wrong lines useful, but the lines were stripped from your
email (sent to the mailing list subscribers).


> Have questions on the 4 patches of ecap .. but I shall open a
> different Thread

Please do. And if those eCAP questions are not specifically about Squid,
then please do not post them here but use eCAP support channels instead:
http://www.e-cap.org/Support


Cheers,

Alex.
P.S. For the record, your pkg-config output looked fine.


From dkewley at uci.edu  Tue Jun 13 20:41:27 2017
From: dkewley at uci.edu (David Kewley)
Date: Tue, 13 Jun 2017 13:41:27 -0700
Subject: [squid-users] source spoofing without tproxy?
In-Reply-To: <bbd1961a-f491-85a0-a9c3-c82c704900da@treenet.co.nz>
References: <CAHKjJqgCTjrGb6YR6Jj7Zo-VgjXFkDeq36uFvVwTFe=EQRKJ-w@mail.gmail.com>
 <e40c77cc-c58c-d3a1-f194-93a255c8cb30@treenet.co.nz>
 <CAHKjJqj=GoEnXo2uqKKisSYubn5D9UhG5g4fV+q5vpyB+nXGHA@mail.gmail.com>
 <bbd1961a-f491-85a0-a9c3-c82c704900da@treenet.co.nz>
Message-ID: <CAHKjJqiDi=MObezZore05Hhy7gOm9NFJt6ZWx0ab0qGo0dUVmA@mail.gmail.com>

On Tue, Jun 13, 2017 at 3:15 AM, Amos Jeffries <squid3 at treenet.co.nz> wrote:

> On 13/06/17 18:14, David Kewley wrote:
>
>> This might be of help if you are not already aware of the risks and
>> issues involved with spoofing and handling of non-local IPs; <
>> http://www.bcp38.info/>
>>
>
I was aware of at least most of those issues, though I'm not an expert on
them. So the reference is useful, thanks.

Our squid server will only accept connections from its internal IP spaces,
and I only wanted it to spoof the actual client source IPs to make
downstream decision making easier based on the IP headers (but
X-Forwarded-For might be sufficient, as you pointed out). Also the actual
egress point to the internet is a NAT device that always uses its external
IP as the source IP. So I see zero risk of us leaking foreign spoofed
source IP addresses.

I will take a look at our ingress filtering to make sure we're rejecting
external inbound packets claiming to be from our internal network.

Do you see anything obvious I'm missing around what I should do for BCP38?

I'm taking seriously your warning about a significant performance impact.
I'll be curious to see if similar issues impact our nginx reverse proxies
that spoof the original source IP in the proxied connection. Makes sense it
might.

    Squid supports X-Forwarded-For fully - it was invented by Squid
>>     devs back in the day, and Squid is still the authoritative
>>     implementation for how it is supposed to work. As an old feature
>>     just about all other HTTP server and intermediary software have
>>     support for that too so you should have no issue pulling the data
>>     out at the receiving end, or in HTTP processing DPI software /
>>     firewalls etc. It is sent on all outgoing Squid messages unless
>>     you explicitly configure something else to happen with the
>>     forwarded_for directive.
>>      <http://www.squid-cache.org/Doc/config/forwarded_for/
>>     <http://www.squid-cache.org/Doc/config/forwarded_for/>>
>>
>>
>> I'll ask the team managing the next-hop device to evaluate that
>> possibility; it looks to me from the docs like it might work. Thanks for
>> the suggestion.
>>
>
> That would be best if it works.
>
> I came up with a bodgy workaround using NAT after sending the earlier
> mail. So if there is no other way than delivering the client-IP on the
> packets there is still something that might be done. But, that would still
> run up against HTTP multiplexing and also add all sorts of NAT related
> issues as well. So only a last resort really.


Thanks! I'll come back to you if I think that might be useful. For now I'll
proceed with using squid in a more normal fashion, and work with the owners
of our next-hop-device about using X-Forwarded-For for any decision making.

I will proceed assuming that squid will never support the sort of spoofing
I was hoping for (since it would probably simplify things greatly for us),
even though I believe in our design that spoofing would have been safe.

David
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20170613/1970d282/attachment.htm>

From rousskov at measurement-factory.com  Tue Jun 13 21:43:04 2017
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Tue, 13 Jun 2017 15:43:04 -0600
Subject: [squid-users] source spoofing without tproxy?
In-Reply-To: <CAHKjJqiDi=MObezZore05Hhy7gOm9NFJt6ZWx0ab0qGo0dUVmA@mail.gmail.com>
References: <CAHKjJqgCTjrGb6YR6Jj7Zo-VgjXFkDeq36uFvVwTFe=EQRKJ-w@mail.gmail.com>
 <e40c77cc-c58c-d3a1-f194-93a255c8cb30@treenet.co.nz>
 <CAHKjJqj=GoEnXo2uqKKisSYubn5D9UhG5g4fV+q5vpyB+nXGHA@mail.gmail.com>
 <bbd1961a-f491-85a0-a9c3-c82c704900da@treenet.co.nz>
 <CAHKjJqiDi=MObezZore05Hhy7gOm9NFJt6ZWx0ab0qGo0dUVmA@mail.gmail.com>
Message-ID: <a15d50f6-a6ec-071c-1c3e-c64c42d86162@measurement-factory.com>

On 06/13/2017 02:41 PM, David Kewley wrote:

> I will proceed assuming that squid will never support the sort of
> spoofing I was hoping for (since it would probably simplify things
> greatly for us), even though I believe in our design that spoofing would
> have been safe.

If you have a legitimate use case, Squid may address it. You just need
to convince developers that your use case does not violate basic
internet principles (more than the existing code does) and is generally
useful (i.e., many folks may find the new feature useful).

In such discussions, claims of RFC or BCP violations are often made.
Sometimes, those claims are correct. Sometimes, they are smoke and
mirrors. Sometimes, Squid already violates those documents. The onus of
distinguishing these cases while defending your use case is on you.

If you believe that your feature does not violate an RFC that is being
thrown against it, then you have to convince others that it does not.
You may request that others cite specific MUST-level requirements that
the feature would violate and then build a logical argument proving that
those MUSTs will not be violated or that those MUSTs are already
violated by other Squid features.


Please do not misinterpret the above as veiled support for the feature
you are requesting. I am just clarifying the rules of the game because
your current assumptions about feature request triage may not match the
reality. I do not know whether your feature violates any important RFCs
(more than other features do) or is generally useful.


HTH,

Alex.


From dkewley at uci.edu  Tue Jun 13 22:16:51 2017
From: dkewley at uci.edu (David Kewley)
Date: Tue, 13 Jun 2017 15:16:51 -0700
Subject: [squid-users] source spoofing without tproxy?
In-Reply-To: <a15d50f6-a6ec-071c-1c3e-c64c42d86162@measurement-factory.com>
References: <CAHKjJqgCTjrGb6YR6Jj7Zo-VgjXFkDeq36uFvVwTFe=EQRKJ-w@mail.gmail.com>
 <e40c77cc-c58c-d3a1-f194-93a255c8cb30@treenet.co.nz>
 <CAHKjJqj=GoEnXo2uqKKisSYubn5D9UhG5g4fV+q5vpyB+nXGHA@mail.gmail.com>
 <bbd1961a-f491-85a0-a9c3-c82c704900da@treenet.co.nz>
 <CAHKjJqiDi=MObezZore05Hhy7gOm9NFJt6ZWx0ab0qGo0dUVmA@mail.gmail.com>
 <a15d50f6-a6ec-071c-1c3e-c64c42d86162@measurement-factory.com>
Message-ID: <CAHKjJqi0X8BP9dFzRaHxRWFrU-RVFp34zP60bGKZj6nmgNgciw@mail.gmail.com>

That's very helpful guidance, Alex. Thank you.

It's probably not in scope currently for me to take on championing such an
effort, but I'll keep it in mind as an option for the future.

David

On Tue, Jun 13, 2017 at 2:43 PM, Alex Rousskov <
rousskov at measurement-factory.com> wrote:

> On 06/13/2017 02:41 PM, David Kewley wrote:
>
> > I will proceed assuming that squid will never support the sort of
> > spoofing I was hoping for (since it would probably simplify things
> > greatly for us), even though I believe in our design that spoofing would
> > have been safe.
>
> If you have a legitimate use case, Squid may address it. You just need
> to convince developers that your use case does not violate basic
> internet principles (more than the existing code does) and is generally
> useful (i.e., many folks may find the new feature useful).
>
> In such discussions, claims of RFC or BCP violations are often made.
> Sometimes, those claims are correct. Sometimes, they are smoke and
> mirrors. Sometimes, Squid already violates those documents. The onus of
> distinguishing these cases while defending your use case is on you.
>
> If you believe that your feature does not violate an RFC that is being
> thrown against it, then you have to convince others that it does not.
> You may request that others cite specific MUST-level requirements that
> the feature would violate and then build a logical argument proving that
> those MUSTs will not be violated or that those MUSTs are already
> violated by other Squid features.
>
>
> Please do not misinterpret the above as veiled support for the feature
> you are requesting. I am just clarifying the rules of the game because
> your current assumptions about feature request triage may not match the
> reality. I do not know whether your feature violates any important RFCs
> (more than other features do) or is generally useful.
>
>
> HTH,
>
> Alex.
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20170613/c9ba0409/attachment.htm>

From eliezer at ngtech.co.il  Wed Jun 14 08:09:16 2017
From: eliezer at ngtech.co.il (Eliezer  Croitoru)
Date: Wed, 14 Jun 2017 11:09:16 +0300
Subject: [squid-users] source spoofing without tproxy?
In-Reply-To: <CAHKjJqgCTjrGb6YR6Jj7Zo-VgjXFkDeq36uFvVwTFe=EQRKJ-w@mail.gmail.com>
References: <CAHKjJqgCTjrGb6YR6Jj7Zo-VgjXFkDeq36uFvVwTFe=EQRKJ-w@mail.gmail.com>
Message-ID: <001101d2e4e5$84909fc0$8db1df40$@ngtech.co.il>

Hey,

This is a library I wrote that uses tproxy:
https://github.com/elico/go-linux-tproxy

It?s doable using some enthusiasm but technically you cannot spoof just any IP since you need to be able to receive back this traffic.
You cannot really "cheap nationally" the BGP protocol but only for specific small areas which are all under your "domain" and management.

All The Bests,
Eliezer

----
http://ngtech.co.il/lmgtfy/
Linux System Administrator
Mobile: +972-5-28704261
Email: eliezer at ngtech.co.il


From: squid-users [mailto:squid-users-bounces at lists.squid-cache.org] On Behalf Of David Kewley
Sent: Tuesday, June 13, 2017 4:48 AM
To: squid-users at lists.squid-cache.org
Subject: [squid-users] source spoofing without tproxy?

I want my clients to explicitly address squid as a proxy (not use tproxy), but have squid spoof the source addresses in the forwarded connection, so that further hops know the original source address from the IPv4 headers.

I could find no indication that anyone else has done this, and when I tried various things, I could not get it working.

Is this possible today? If not, is it worth considering as a future feature? Or am I overlooking a reason that this cannot work even in theory?

I got the nearly-equivalent functionality working for reverse proxying using nginx, but so far I've found no way to do it with forward proxying. Nginx doesn't do https forward proxying (no handling of CONNECT).

If squid can't do what I'm looking for today, I would welcome pointers to other possible approaches.

Thanks,
David



From codemarauder at gmail.com  Wed Jun 14 10:36:53 2017
From: codemarauder at gmail.com (Nishant Sharma)
Date: Wed, 14 Jun 2017 16:06:53 +0530
Subject: [squid-users] Introducing Charcoal - Centralised URL Filter for
	squid
Message-ID: <452c9346-331e-e9ee-7f5d-65c330b5db16@gmail.com>

Hi,

We are excited to invite early users to test drive Charcoal 
(http://charcoal.io) - a Squid URL Rewriter for distributed proxies.

Charcoal is designed to help administrators manage access rules for the 
proxies at just one place with a GUI, instead of editing configuration 
of individual proxy servers.

It has come out of our need of managing ACLs for 100+ proxy servers on 
embedded devices (OpenWRT/LEDE) running at our customer offices across 
the geography of India. We are releasing it in the hope that it will be 
useful for Squid users who have to manage multiple proxy servers everyday.

The architecture is API key driven client-server, where a squid 
url-rewrite helper contacts server to query access controls for the 
incoming requests.

Current features:
-----------------
- Supports Squid 2.x and 3.x
- 70+ pre-existing domains blacklist
- Custom destination groups/categories
- Custom source groups for IPs and Networks (usernames in the pipeline)
- As of now only domain filter support (no full url filtering)
- API key driven

Configuration:
--------------
- Download the helper from 
https://raw.githubusercontent.com/Hopbox/charcoal-helper/master/squid/charcoal-helper.pl.
- Make sure IO::Socket module for Perl is installed.
- Add following lines to squid.conf after downloading the helper:

url_rewrite_program /path/to/charcoal-helper.pl YOUR_API_KEY
url_rewrite_children X startup=Y idle=Z concurrency=1

YOUR_API_KEY for our hosted Charcoal service can be requested by filling 
in the form at http://charcoal.io or writing in to charcoal at hopbox.in. 
The credentials for login to https://active.charcoal.io to manage the 
ACL will be emailed along with YOUR_API_KEY.

License:
--------
URL Rewrite helper for squid is licensed under GPLv2.0 while Charcoal 
Server is licensed under AGPLv3.0.

GIT Repo:
---------
Squid URL Rewrite helper can be downloaded from 
https://github.com/Hopbox/charcoal-helper

Git repository for Charcoal Server is at https://github.com/Hopbox/charcoal

Regards,
Nishant


From eliezer at ngtech.co.il  Wed Jun 14 13:28:00 2017
From: eliezer at ngtech.co.il (Eliezer  Croitoru)
Date: Wed, 14 Jun 2017 16:28:00 +0300
Subject: [squid-users] source spoofing without tproxy?
In-Reply-To: <001101d2e4e5$84909fc0$8db1df40$@ngtech.co.il>
References: <CAHKjJqgCTjrGb6YR6Jj7Zo-VgjXFkDeq36uFvVwTFe=EQRKJ-w@mail.gmail.com>
 <001101d2e4e5$84909fc0$8db1df40$@ngtech.co.il>
Message-ID: <002e01d2e512$0b3c29f0$21b47dd0$@ngtech.co.il>

Rephrase the "cheap nationally" into "cheat inernationally".

----
Eliezer Croitoru
Linux System Administrator
Mobile: +972-5-28704261
Email: eliezer at ngtech.co.il


-----Original Message-----
From: squid-users [mailto:squid-users-bounces at lists.squid-cache.org] On Behalf Of Eliezer Croitoru
Sent: Wednesday, June 14, 2017 11:09 AM
To: 'David Kewley' <dkewley at uci.edu>; squid-users at lists.squid-cache.org
Subject: Re: [squid-users] source spoofing without tproxy?

Hey,

This is a library I wrote that uses tproxy:
https://github.com/elico/go-linux-tproxy

It?s doable using some enthusiasm but technically you cannot spoof just any IP since you need to be able to receive back this traffic.
You cannot really "cheap nationally" the BGP protocol but only for specific small areas which are all under your "domain" and management.

All The Bests,
Eliezer

----
http://ngtech.co.il/lmgtfy/
Linux System Administrator
Mobile: +972-5-28704261
Email: eliezer at ngtech.co.il


From: squid-users [mailto:squid-users-bounces at lists.squid-cache.org] On Behalf Of David Kewley
Sent: Tuesday, June 13, 2017 4:48 AM
To: squid-users at lists.squid-cache.org
Subject: [squid-users] source spoofing without tproxy?

I want my clients to explicitly address squid as a proxy (not use tproxy), but have squid spoof the source addresses in the forwarded connection, so that further hops know the original source address from the IPv4 headers.

I could find no indication that anyone else has done this, and when I tried various things, I could not get it working.

Is this possible today? If not, is it worth considering as a future feature? Or am I overlooking a reason that this cannot work even in theory?

I got the nearly-equivalent functionality working for reverse proxying using nginx, but so far I've found no way to do it with forward proxying. Nginx doesn't do https forward proxying (no handling of CONNECT).

If squid can't do what I'm looking for today, I would welcome pointers to other possible approaches.

Thanks,
David

_______________________________________________
squid-users mailing list
squid-users at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-users



From yvoinov at gmail.com  Wed Jun 14 13:29:32 2017
From: yvoinov at gmail.com (Yuri)
Date: Wed, 14 Jun 2017 19:29:32 +0600
Subject: [squid-users] source spoofing without tproxy?
In-Reply-To: <002e01d2e512$0b3c29f0$21b47dd0$@ngtech.co.il>
References: <CAHKjJqgCTjrGb6YR6Jj7Zo-VgjXFkDeq36uFvVwTFe=EQRKJ-w@mail.gmail.com>
 <001101d2e4e5$84909fc0$8db1df40$@ngtech.co.il>
 <002e01d2e512$0b3c29f0$21b47dd0$@ngtech.co.il>
Message-ID: <6096373a-cf74-ca7b-0558-646f9055791f@gmail.com>

Nice shoot, Eliezer :-D


14.06.2017 19:28, Eliezer Croitoru ?????:
> Rephrase the "cheap nationally" into "cheat inernationally".
>
> ----
> Eliezer Croitoru
> Linux System Administrator
> Mobile: +972-5-28704261
> Email: eliezer at ngtech.co.il
>
>
> -----Original Message-----
> From: squid-users [mailto:squid-users-bounces at lists.squid-cache.org] On Behalf Of Eliezer Croitoru
> Sent: Wednesday, June 14, 2017 11:09 AM
> To: 'David Kewley' <dkewley at uci.edu>; squid-users at lists.squid-cache.org
> Subject: Re: [squid-users] source spoofing without tproxy?
>
> Hey,
>
> This is a library I wrote that uses tproxy:
> https://github.com/elico/go-linux-tproxy
>
> It?s doable using some enthusiasm but technically you cannot spoof just any IP since you need to be able to receive back this traffic.
> You cannot really "cheap nationally" the BGP protocol but only for specific small areas which are all under your "domain" and management.
>
> All The Bests,
> Eliezer
>
> ----
> http://ngtech.co.il/lmgtfy/
> Linux System Administrator
> Mobile: +972-5-28704261
> Email: eliezer at ngtech.co.il
>
>
> From: squid-users [mailto:squid-users-bounces at lists.squid-cache.org] On Behalf Of David Kewley
> Sent: Tuesday, June 13, 2017 4:48 AM
> To: squid-users at lists.squid-cache.org
> Subject: [squid-users] source spoofing without tproxy?
>
> I want my clients to explicitly address squid as a proxy (not use tproxy), but have squid spoof the source addresses in the forwarded connection, so that further hops know the original source address from the IPv4 headers.
>
> I could find no indication that anyone else has done this, and when I tried various things, I could not get it working.
>
> Is this possible today? If not, is it worth considering as a future feature? Or am I overlooking a reason that this cannot work even in theory?
>
> I got the nearly-equivalent functionality working for reverse proxying using nginx, but so far I've found no way to do it with forward proxying. Nginx doesn't do https forward proxying (no handling of CONNECT).
>
> If squid can't do what I'm looking for today, I would welcome pointers to other possible approaches.
>
> Thanks,
> David
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users


-------------- next part --------------
A non-text attachment was scrubbed...
Name: signature.asc
Type: application/pgp-signature
Size: 473 bytes
Desc: OpenPGP digital signature
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20170614/e00ff440/attachment.sig>

From norbert.naveen at tayana.in  Wed Jun 14 13:30:19 2017
From: norbert.naveen at tayana.in (Norbert Naveen)
Date: Wed, 14 Jun 2017 19:00:19 +0530
Subject: [squid-users] Squid Transparent HTTP Proxy - 2 ETH Links - HTTP
	Proxy
Message-ID: <01a201d2e512$5dd492b0$197db810$@tayana.in>

Hello Admins , 

 

Pls refer to the Image as in 

 

https://drive.google.com/open?id=0B_dDVNpzSGEKZmFPWHFLWlJJMUU

 

The Setup will be as attached  in URL Above . 

Server which will Host Squid will have Two Interfaces with 2 Different VLAN
Tags 

Content Inspection Engine will REROUTE all HTTP Traffic Through the Links
coming to Squid Server . 

Squid Server has to act as TRANSPARENT PROXY

 

One Possible way of doing it IP tables and Masquerading SRC IP 

But . Without Changing Src or Dst IP address . How to achieve the same ? 

 

ALL HTTP Traffic will be forward from 1 to 2 and Squid will be in between 

We will have to Forward all traffic on 1 to 2 .. ?

 

 

Thanks 

Naveen

 

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20170614/e521fe89/attachment.htm>

From eliezer at ngtech.co.il  Wed Jun 14 13:58:01 2017
From: eliezer at ngtech.co.il (Eliezer  Croitoru)
Date: Wed, 14 Jun 2017 16:58:01 +0300
Subject: [squid-users] Squid Transparent HTTP Proxy - 2 ETH Links -
	HTTP	Proxy
In-Reply-To: <01a201d2e512$5dd492b0$197db810$@tayana.in>
References: <01a201d2e512$5dd492b0$197db810$@tayana.in>
Message-ID: <003b01d2e516$3c8276f0$b58764d0$@ngtech.co.il>

It depends on the equipment..
What you should do is to use the switch to pass all traffic to the squid mac
address and mirror all traffic to the probe node.
What switch do you have there?

Eliezer

----
http://ngtech.co.il/lmgtfy/
Linux System Administrator
Mobile: +972-5-28704261
Email: eliezer at ngtech.co.il


From: squid-users [mailto:squid-users-bounces at lists.squid-cache.org] On
Behalf Of Norbert Naveen
Sent: Wednesday, June 14, 2017 4:30 PM
To: squid-users at lists.squid-cache.org
Subject: [squid-users] Squid Transparent HTTP Proxy - 2 ETH Links - HTTP
Proxy

Hello Admins , 

Pls refer to the Image as in 

https://drive.google.com/open?id=0B_dDVNpzSGEKZmFPWHFLWlJJMUU

The Setup will be as attached ?in URL Above ? 
Server which will Host Squid will have Two Interfaces with 2 Different VLAN
Tags 
Content Inspection Engine will REROUTE all HTTP Traffic Through the Links
coming to Squid Server . 
Squid Server has to act as TRANSPARENT PROXY

One Possible way of doing it IP tables and Masquerading SRC IP 
But ? Without Changing Src or Dst IP address . How to achieve the same ? 

ALL HTTP Traffic will be forward from 1 to 2 and Squid will be in between 
We will have to Forward all traffic on 1 to 2 .. ?


Thanks 
Naveen




From Antony.Stone at squid.open.source.it  Wed Jun 14 14:16:01 2017
From: Antony.Stone at squid.open.source.it (Antony Stone)
Date: Wed, 14 Jun 2017 16:16:01 +0200
Subject: [squid-users] Squid Transparent HTTP Proxy - 2 ETH Links -
	HTTP	Proxy
In-Reply-To: <003b01d2e516$3c8276f0$b58764d0$@ngtech.co.il>
References: <01a201d2e512$5dd492b0$197db810$@tayana.in>
 <003b01d2e516$3c8276f0$b58764d0$@ngtech.co.il>
Message-ID: <8276426.yLjuPAjmXo@pikantusdevuan>

On Wednesday 14 June 2017 16:58:01 Eliezer  Croitoru wrote:

> It depends on the equipment..
> What you should do is to use the switch to pass all traffic to the squid mac
> address and mirror all traffic to the probe node.

http://wiki.squid-cache.org/ConfigExamples/#Interception may give you some 
useful guidelines, depending on what your equipment is.

Alternatively you could do policy routing on the "Core Router", giving the 
internal IP address of the Squid server as the gateway for HTTP/S traffic, and 
then you do the standard Intercept NAT on the Squid machine so that it gets 
processed.

http://wiki.squid-cache.org/ConfigExamples/Intercept/LinuxRedirect

Squid then has the "Internet Router" as its gateway to the outside.

The important thing is *not* to do any Destination NAT on traffic to try to get 
it to hit the Squid box.  The destination IPs of the packets must remain 
unchanged (ie: wherever they were trying to get to on the Internet).


Regards,


Antony.

> From: squid-users [mailto:squid-users-bounces at lists.squid-cache.org] On
> Behalf Of Norbert Naveen
> Sent: Wednesday, June 14, 2017 4:30 PM
> To: squid-users at lists.squid-cache.org
> Subject: [squid-users] Squid Transparent HTTP Proxy - 2 ETH Links - HTTP
> Proxy
> 
> Hello Admins ,
> 
> Pls refer to the Image as in
> 
> https://drive.google.com/open?id=0B_dDVNpzSGEKZmFPWHFLWlJJMUU
> 
> The Setup will be as attached  in URL Above ?
> Server which will Host Squid will have Two Interfaces with 2 Different VLAN
> Tags
> Content Inspection Engine will REROUTE all HTTP Traffic Through the Links
> coming to Squid Server .
> Squid Server has to act as TRANSPARENT PROXY
> 
> One Possible way of doing it IP tables and Masquerading SRC IP
> But ? Without Changing Src or Dst IP address . How to achieve the same ?
> 
> ALL HTTP Traffic will be forward from 1 to 2 and Squid will be in between
> We will have to Forward all traffic on 1 to 2 .. ?

-- 
If you want to be happy for an hour, get drunk.
If you want to be happy for a year, get married.
If you want to be happy for a lifetime, get a garden.

                                                   Please reply to the list;
                                                         please *don't* CC me.



From squid3 at treenet.co.nz  Wed Jun 14 14:41:30 2017
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 15 Jun 2017 02:41:30 +1200
Subject: [squid-users] cacheable object dose not match
In-Reply-To: <1497363117488-4682778.post@n4.nabble.com>
References: <1497360933302-4682776.post@n4.nabble.com>
 <c44b7938-6f11-2ed8-9b7f-520d52252a14@treenet.co.nz>
 <1497363117488-4682778.post@n4.nabble.com>
Message-ID: <b830af6b-4944-452f-a97b-07e604c8043f@treenet.co.nz>

On 14/06/17 02:11, joseph wrote:
>>> No. The cache file contains a TLV structure of metadata followed by the
> right  but so it should be a TLV bindery and after that ??  HTTP/1.1 200 OK
> wish is text clear or anything betwean thim as this  -->>
> accept-encodingHTTP/1.1 200 OK
> accept-encoding and status line  on one line also
>
> 1 accept-encoding should be befor status line ??
> 2 they should be  on one line  without  cr ??

The "accept-encoding" you see there is part of the metadata TLV for the 
"Vary" header. The letter "H" is the actual start of the ASCII portion 
of the file.


> so i need to know befor reporting bug tks

I suggest you investigate the file(s) with the squid-purge and/or 
ufsdump tools. Both of those should be able to identify and validate the 
UFS/AUFS/diskd cache file contents for you in an easier to read format - 
such as displaying what the binary parts mean.

Amos



From webmaster at squidblacklist.org  Wed Jun 14 14:52:44 2017
From: webmaster at squidblacklist.org (Benjamin E. Nichols)
Date: Wed, 14 Jun 2017 09:52:44 -0500
Subject: [squid-users] Introducing Charcoal - Centralised URL Filter for
 squid
In-Reply-To: <452c9346-331e-e9ee-7f5d-65c330b5db16@gmail.com>
References: <452c9346-331e-e9ee-7f5d-65c330b5db16@gmail.com>
Message-ID: <6d2128fe-1d4d-d9b8-77e7-1fbd1182b14d@squidblacklist.org>

This sounds great, and would you mind specifying the source of the 
blacklist data at the core of your services?

In other words, what I dare ask you is this, and im sure others might 
want to know, are you using the blacklists from shalla, UT1, or 
urlblacklist? Or have you developed your own domain management technology?


-- 
Signed,

Benjamin E. Nichols

http://www.squidblacklist.org


On 6/14/2017 5:36 AM, Nishant Sharma wrote:
> Hi,
>
> We are excited to invite early users to test drive Charcoal 
> (http://charcoal.io) - a Squid URL Rewriter for distributed proxies.
>
> Charcoal is designed to help administrators manage access rules for 
> the proxies at just one place with a GUI, instead of editing 
> configuration of individual proxy servers.
>
> It has come out of our need of managing ACLs for 100+ proxy servers on 
> embedded devices (OpenWRT/LEDE) running at our customer offices across 
> the geography of India. We are releasing it in the hope that it will 
> be useful for Squid users who have to manage multiple proxy servers 
> everyday.
>
> The architecture is API key driven client-server, where a squid 
> url-rewrite helper contacts server to query access controls for the 
> incoming requests.
>
> Current features:
> -----------------
> - Supports Squid 2.x and 3.x
> - 70+ pre-existing domains blacklist
> - Custom destination groups/categories
> - Custom source groups for IPs and Networks (usernames in the pipeline)
> - As of now only domain filter support (no full url filtering)
> - API key driven
>
> Configuration:
> --------------
> - Download the helper from 
> https://raw.githubusercontent.com/Hopbox/charcoal-helper/master/squid/charcoal-helper.pl.
> - Make sure IO::Socket module for Perl is installed.
> - Add following lines to squid.conf after downloading the helper:
>
> url_rewrite_program /path/to/charcoal-helper.pl YOUR_API_KEY
> url_rewrite_children X startup=Y idle=Z concurrency=1
>
> YOUR_API_KEY for our hosted Charcoal service can be requested by 
> filling in the form at http://charcoal.io or writing in to 
> charcoal at hopbox.in. The credentials for login to 
> https://active.charcoal.io to manage the ACL will be emailed along 
> with YOUR_API_KEY.
>
> License:
> --------
> URL Rewrite helper for squid is licensed under GPLv2.0 while Charcoal 
> Server is licensed under AGPLv3.0.
>
> GIT Repo:
> ---------
> Squid URL Rewrite helper can be downloaded from 
> https://github.com/Hopbox/charcoal-helper
>
> Git repository for Charcoal Server is at 
> https://github.com/Hopbox/charcoal
>
> Regards,
> Nishant
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users

-- 
Signed,

Benjamin E. Nichols

http://www.squidblacklist.org



From codemarauder at gmail.com  Wed Jun 14 15:04:50 2017
From: codemarauder at gmail.com (Nishant Sharma)
Date: Wed, 14 Jun 2017 20:34:50 +0530
Subject: [squid-users] Introducing Charcoal - Centralised URL Filter for
 squid
In-Reply-To: <6d2128fe-1d4d-d9b8-77e7-1fbd1182b14d@squidblacklist.org>
References: <452c9346-331e-e9ee-7f5d-65c330b5db16@gmail.com>
 <6d2128fe-1d4d-d9b8-77e7-1fbd1182b14d@squidblacklist.org>
Message-ID: <50d402ad-35fe-3244-e5dc-6a3de808fd50@gmail.com>

Hi Benjamin,

On Wednesday 14 June 2017 08:22 PM, Benjamin E. Nichols wrote:
> This sounds great, and would you mind specifying the source of the 
> blacklist data at the core of your services?
> 
> In other words, what I dare ask you is this, and im sure others might 
> want to know, are you using the blacklists from shalla, UT1, or 
> urlblacklist? Or have you developed your own domain management technology?
> 

Thanks for the kind words.

For the test run, we are using Shalla.

I understand that quality of blacklists matters. It is also possible to 
mix-match multiple blacklists and that should be the ideal scenario with 
most of the bases covered. And that depends on the user-base and the 
financial aspects of sourcing the blacklists.

Right now, our first priority is to fix a handful of bugs reported just 
after the announcement.

Thanks & Regards,
Nishant


From eliezer at ngtech.co.il  Wed Jun 14 17:37:16 2017
From: eliezer at ngtech.co.il (Eliezer  Croitoru)
Date: Wed, 14 Jun 2017 20:37:16 +0300
Subject: [squid-users] Introducing Charcoal - Centralised URL Filter
	for	squid
In-Reply-To: <452c9346-331e-e9ee-7f5d-65c330b5db16@gmail.com>
References: <452c9346-331e-e9ee-7f5d-65c330b5db16@gmail.com>
Message-ID: <003601d2e534$dd7729c0$98657d40$@ngtech.co.il>

Hey Nishant,

I want to offer you a more advanced helper that supports actual concurrency compared to the current perl helper on github,
which understands the protocol but do not use threads or any other method of concurrency.

Let me know if it's of any interest for you.
The skeleton is at:
http://wiki.squid-cache.org/EliezerCroitoru/GolangFakeHelper

I am willing to take my time and write the code for you. So..

Eliezer

----
Eliezer Croitoru
Linux System Administrator
Mobile: +972-5-28704261
Email: eliezer at ngtech.co.il


-----Original Message-----
From: squid-users [mailto:squid-users-bounces at lists.squid-cache.org] On Behalf Of Nishant Sharma
Sent: Wednesday, June 14, 2017 1:37 PM
To: squid-users at lists.squid-cache.org
Subject: [squid-users] Introducing Charcoal - Centralised URL Filter for squid

Hi,

We are excited to invite early users to test drive Charcoal
(http://charcoal.io) - a Squid URL Rewriter for distributed proxies.

Charcoal is designed to help administrators manage access rules for the proxies at just one place with a GUI, instead of editing configuration of individual proxy servers.

It has come out of our need of managing ACLs for 100+ proxy servers on embedded devices (OpenWRT/LEDE) running at our customer offices across the geography of India. We are releasing it in the hope that it will be useful for Squid users who have to manage multiple proxy servers everyday.

The architecture is API key driven client-server, where a squid url-rewrite helper contacts server to query access controls for the incoming requests.

Current features:
-----------------
- Supports Squid 2.x and 3.x
- 70+ pre-existing domains blacklist
- Custom destination groups/categories
- Custom source groups for IPs and Networks (usernames in the pipeline)
- As of now only domain filter support (no full url filtering)
- API key driven

Configuration:
--------------
- Download the helper from
https://raw.githubusercontent.com/Hopbox/charcoal-helper/master/squid/charcoal-helper.pl.
- Make sure IO::Socket module for Perl is installed.
- Add following lines to squid.conf after downloading the helper:

url_rewrite_program /path/to/charcoal-helper.pl YOUR_API_KEY url_rewrite_children X startup=Y idle=Z concurrency=1

YOUR_API_KEY for our hosted Charcoal service can be requested by filling in the form at http://charcoal.io or writing in to charcoal at hopbox.in. 
The credentials for login to https://active.charcoal.io to manage the ACL will be emailed along with YOUR_API_KEY.

License:
--------
URL Rewrite helper for squid is licensed under GPLv2.0 while Charcoal Server is licensed under AGPLv3.0.

GIT Repo:
---------
Squid URL Rewrite helper can be downloaded from https://github.com/Hopbox/charcoal-helper

Git repository for Charcoal Server is at https://github.com/Hopbox/charcoal

Regards,
Nishant
_______________________________________________
squid-users mailing list
squid-users at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-users



From blason16 at gmail.com  Thu Jun 15 03:20:22 2017
From: blason16 at gmail.com (Blason R)
Date: Thu, 15 Jun 2017 03:20:22 +0000
Subject: [squid-users] Office 365 Support for Squid Proxy
In-Reply-To: <007701d2e3af$bbefc2c0$33cf4840$@ngtech.co.il>
References: <CAPPXLT-nXbTHdmWBQPWX-FbA0RWrJsCx54EUqcRTz=57+0_Miw@mail.gmail.com>
 <007701d2e3af$bbefc2c0$33cf4840$@ngtech.co.il>
Message-ID: <CAPPXLT8enxeOmEPCWKQ9XvGH3x04P1WBDcJ5vfRwgCYNsghUSA@mail.gmail.com>

I am thinking to put it in forwarding only mode. And being a office 365 I
don't see any reason for ssl-bump since I do have other device for handling
web traffic.

On Tue, Jun 13, 2017, 12:41 AM Eliezer Croitoru <eliezer at ngtech.co.il>
wrote:

> The main question is if it uses websockets or not and if you are using
> SSL-BUMP or not.
> If you are using SSL-BUMP it's one thing while if you are not it?s another
> story.
> Also it will be different if you are using the proxy in INTERCEPT mode or
> a regular forward proxy mode.
> We would be able to answer you more with more details on your setup.
>
> Eliezer
>
> ----
> http://ngtech.co.il/lmgtfy/
> Linux System Administrator
> Mobile: +972-5-28704261
> Email: eliezer at ngtech.co.il
>
>
> From: squid-users [mailto:squid-users-bounces at lists.squid-cache.org] On
> Behalf Of Blason R
> Sent: Monday, June 12, 2017 12:05 PM
> To: squid-users at lists.squid-cache.org
> Subject: [squid-users] Office 365 Support for Squid Proxy
>
> Hello All,
>
> If someone can confirm if squid can very well work with Office 365? If
> anyone has any documentation can someone please forward that to me? I do
> have almost around 400 Office 365 users hence wanted to know what
> configuration I might need for Office 365 traffic?
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20170615/cf0cb024/attachment.htm>

From eliezer at ngtech.co.il  Thu Jun 15 07:54:03 2017
From: eliezer at ngtech.co.il (Eliezer  Croitoru)
Date: Thu, 15 Jun 2017 10:54:03 +0300
Subject: [squid-users] Office 365 Support for Squid Proxy
In-Reply-To: <CAPPXLT8enxeOmEPCWKQ9XvGH3x04P1WBDcJ5vfRwgCYNsghUSA@mail.gmail.com>
References: <CAPPXLT-nXbTHdmWBQPWX-FbA0RWrJsCx54EUqcRTz=57+0_Miw@mail.gmail.com>
 <007701d2e3af$bbefc2c0$33cf4840$@ngtech.co.il>
 <CAPPXLT8enxeOmEPCWKQ9XvGH3x04P1WBDcJ5vfRwgCYNsghUSA@mail.gmail.com>
Message-ID: <002601d2e5ac$8e64c4f0$ab2e4ed0$@ngtech.co.il>

In a simple forward proxy mode which enforces acl's it should be pretty simple and easy to use.

All The Bests,
Eliezer

----
http://ngtech.co.il/lmgtfy/
Linux System Administrator
Mobile: +972-5-28704261
Email: eliezer at ngtech.co.il


From: Blason R [mailto:blason16 at gmail.com] 
Sent: Thursday, June 15, 2017 6:20 AM
To: Eliezer Croitoru <eliezer at ngtech.co.il>
Cc: squid-users at lists.squid-cache.org
Subject: Re: [squid-users] Office 365 Support for Squid Proxy

I am thinking to put it in forwarding only mode. And being a office 365 I don't see any reason for ssl-bump since I do have other device for handling web traffic.
On Tue, Jun 13, 2017, 12:41 AM Eliezer Croitoru <mailto:eliezer at ngtech.co.il> wrote:
The main question is if it uses websockets or not and if you are using SSL-BUMP or not.
If you are using SSL-BUMP it's one thing while if you are not it?s another story.
Also it will be different if you are using the proxy in INTERCEPT mode or a regular forward proxy mode.
We would be able to answer you more with more details on your setup.

Eliezer

----
http://ngtech.co.il/lmgtfy/
Linux System Administrator
Mobile: +972-5-28704261
Email: mailto:eliezer at ngtech.co.il


From: squid-users [mailto:mailto:squid-users-bounces at lists.squid-cache.org] On Behalf Of Blason R
Sent: Monday, June 12, 2017 12:05 PM
To: mailto:squid-users at lists.squid-cache.org
Subject: [squid-users] Office 365 Support for Squid Proxy

Hello All,

If someone can confirm if squid can very well work with Office 365? If anyone has any documentation can someone please forward that to me? I do have almost around 400 Office 365 users hence wanted to know what configuration I might need for Office 365 traffic?



From javier.perez at accelya.com  Thu Jun 15 10:52:44 2017
From: javier.perez at accelya.com (javier perez)
Date: Thu, 15 Jun 2017 16:22:44 +0530 (IST)
Subject: [squid-users] RV: squid
Message-ID: <01ff01d2e5c5$840788d0$8c169a70$@accelya.com>

Good morning squid users,

 

I'm facing a weird situation in my Company. let me explain:

 

I installed squid(3.5.20) on CentOS 7 minimal to perform as an ftp-proxy. 

My configuration file looks like this:

 

/etc/squid/squid.conf

##########################################################################
####

 

acl SSL_ports port 443 21

ftp_port 21

ftp_passive off

 

##########################################################################
####

 

acl Safe_ports port 80          # http

acl Safe_ports port 443         # https

acl Safe_ports port 70          # gopher

acl Safe_ports port 21

acl Safe_ports port 210         # wais

acl Safe_ports port 1025-65535  # unregistered ports

acl Safe_ports port 280         # http-mgmt

acl Safe_ports port 488         # gss-http

acl Safe_ports port 591         # filemaker

acl Safe_ports port 777         # multiling http

acl CONNECT method CONNECT

 

##########################################################################
####

 

acl FTP proto FTP

always_direct allow FTP

 

##########################################################################
####

 

http_access allow CONNECT SSL_ports

http_access allow CONNECT Safe_ports

http_access allow SSL_ports

http_access allow Safe_ports

http_access allow all

http_access allow FTP

 

##########################################################################
####

 

http_port 3128

 

refresh_pattern ^ftp:           1440    20%     10080

refresh_pattern ^gopher:        1440    0%      1440

refresh_pattern -i (/cgi-bin/|\?) 0     0%      0

refresh_pattern .               0       20%     4320

 

##########################################################################
####

 

The thing is that the parameter "ftp_passive off" seems not to be working.


Due to security measures we have to use non-passive mode to be able to
transfer anything.

 

The connection works fine with the remote hosts, the login works, but I
have to enter "passive" every single time to swap the mode to non-passive.


I don't know whether the "ftp_passive" is not working or I need to do
something else.

 

After doing a deep research I cannot find much information related with
this kind of problems, so I decided to get in touch with you. Please
help!!

 

Thanks in advance.

Regards

 

 

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20170615/3037cd98/attachment.htm>

From norbert.naveen at tayana.in  Thu Jun 15 11:00:31 2017
From: norbert.naveen at tayana.in (Norbert Naveen)
Date: Thu, 15 Jun 2017 16:30:31 +0530
Subject: [squid-users] Squid Transparent HTTP Proxy - 2 ETH Links -
	HTTP	Proxy
In-Reply-To: <003b01d2e516$3c8276f0$b58764d0$@ngtech.co.il>
References: <01a201d2e512$5dd492b0$197db810$@tayana.in>
 <003b01d2e516$3c8276f0$b58764d0$@ngtech.co.il>
Message-ID: <013501d2e5c6$9d101d50$d73057f0$@tayana.in>

Hello Eliezer, 
 Switch - Cisco 3750 
 Did not understand the " mirror all traffic to the probe node."
Thanks 
Naveen

-----Original Message-----
From: Eliezer Croitoru [mailto:eliezer at ngtech.co.il] 
Sent: Wednesday, June 14, 2017 7:28 PM
To: norbert.naveen at tayana.in
Cc: squid-users at lists.squid-cache.org
Subject: RE: [squid-users] Squid Transparent HTTP Proxy - 2 ETH Links - HTTP
Proxy

It depends on the equipment..
What you should do is to use the switch to pass all traffic to the squid mac
address and mirror all traffic to the probe node.
What switch do you have there?

Eliezer

----
http://ngtech.co.il/lmgtfy/
Linux System Administrator
Mobile: +972-5-28704261
Email: eliezer at ngtech.co.il


From: squid-users [mailto:squid-users-bounces at lists.squid-cache.org] On
Behalf Of Norbert Naveen
Sent: Wednesday, June 14, 2017 4:30 PM
To: squid-users at lists.squid-cache.org
Subject: [squid-users] Squid Transparent HTTP Proxy - 2 ETH Links - HTTP
Proxy

Hello Admins , 

Pls refer to the Image as in 

https://drive.google.com/open?id=0B_dDVNpzSGEKZmFPWHFLWlJJMUU

The Setup will be as attached ?in URL Above 
 Server which will Host Squid
will have Two Interfaces with 2 Different VLAN Tags Content Inspection
Engine will REROUTE all HTTP Traffic Through the Links coming to Squid
Server . 
Squid Server has to act as TRANSPARENT PROXY

One Possible way of doing it IP tables and Masquerading SRC IP But 
 Without
Changing Src or Dst IP address . How to achieve the same ? 

ALL HTTP Traffic will be forward from 1 to 2 and Squid will be in between We
will have to Forward all traffic on 1 to 2 .. ?


Thanks 
Naveen




From Antony.Stone at squid.open.source.it  Thu Jun 15 11:03:51 2017
From: Antony.Stone at squid.open.source.it (Antony Stone)
Date: Thu, 15 Jun 2017 13:03:51 +0200
Subject: [squid-users] RV: squid
In-Reply-To: <01ff01d2e5c5$840788d0$8c169a70$@accelya.com>
References: <01ff01d2e5c5$840788d0$8c169a70$@accelya.com>
Message-ID: <2686958.ZNAhSqxdWU@pikantusdevuan>

On Thursday 15 June 2017 16:22:44 javier perez wrote:

> I installed squid(3.5.20) on CentOS 7 minimal to perform as an ftp-proxy.
> 
> My configuration file looks like this:

...snip...

> acl SSL_ports port 443 21

Er, what?

Why are you specifying port 21 as SSL?

> ftp_passive off

...snip...

> The thing is that the parameter "ftp_passive off" seems not to be working.

> The connection works fine with the remote hosts, the login works, but I
> have to enter "passive" every single time to swap the mode to non-passive.

Surely the option merely tells Squid whether to allow active or passive FTP 
connections - it doesn't tell the client application what to ask for.

"ftp_passive off" should mean that you can't do passive FTP through the Squid 
server, but it won't stop the client application from trying.

You need to tell the client system/s always to use active FTP (which will go 
through Squid) - Squid can't do that for you - it will simply allow or block 
whatever requests come its way.


Antony.

-- 
Under UK law, no VAT is charged on biscuits and cakes - they are "zero rated".  
Chocolate covered biscuits, however, are classed as "luxury items" and are 
subject to VAT.  McVitie's classed its Jaffa Cakes as cakes, but in 1991 this 
was challenged by Her Majesty's Customs and Excise in court.

The question which had to be answered was what criteria should be used to 
class something as a cake or a biscuit.  McVitie's defended the classification 
of Jaffa Cakes as a cake by arguing that cakes go hard when stale, whereas 
biscuits go soft.  It was demonstrated that Jaffa Cakes become hard when stale 
and McVitie's won the case.

                                                   Please reply to the list;
                                                         please *don't* CC me.



From javier.perez at accelya.com  Thu Jun 15 11:38:27 2017
From: javier.perez at accelya.com (javier perez)
Date: Thu, 15 Jun 2017 17:08:27 +0530 (IST)
Subject: [squid-users] RV: squid
In-Reply-To: <2686958.ZNAhSqxdWU@pikantusdevuan>
References: <01ff01d2e5c5$840788d0$8c169a70$@accelya.com>
 <2686958.ZNAhSqxdWU@pikantusdevuan>
Message-ID: <022b01d2e5cb$e76b5ae0$b64210a0$@accelya.com>


>> I installed squid(3.5.20) on CentOS 7 minimal to perform as an ftp-proxy.
>>
>> My configuration file looks like this:

>...snip...
snip?

>> acl SSL_ports port 443 21

>Er, what?

>Why are you specifying port 21 as SSL?

I saw many guides that ask for it e.g. 
https://unix.stackexchange.com/questions/15484/connecting-to-ftp-sites-via-squid

I understand thet its in order to apply acls to those ports invoking 
"SSL_ports".

>> ftp_passive off

>...snip...
snip?
>> The thing is that the parameter "ftp_passive off" seems not to be 
>> working.

>> The connection works fine with the remote hosts, the login works, but
>> I have to enter "passive" every single time to swap the mode to 
>> non-passive.

>Surely the option merely tells Squid whether to allow active or passive FTP 
>connections - it doesn't tell the client application what to ask for.

>"ftp_passive off" should mean that you can't do passive FTP through the 
>Squid server, but it won't stop the client application from trying.

>You need to tell the client system/s always to use active FTP (which will 
>go through Squid) - Squid can't do that for you - it will simply allow or 
>block whatever requests come its way.

The thing is that my destination hosts are only listening on port 21 
(active) and my source hosts have the passive mode disabled...

Here you have an example of some other weird stuff:

With a Windows host (passive mode disabled) I do an ftp through CMD to my 
proxy, then I enter user at host(this host accepts active and passive mode), I 
enter the password, Access granted. But when I try dir/ls the host 
disconnects me.
But if I remove " ftp_passive off" it works!! Non-sense to me...

Thank you Anthony for your quick answer.

Regards




From blason16 at gmail.com  Thu Jun 15 11:44:53 2017
From: blason16 at gmail.com (Blason R)
Date: Thu, 15 Jun 2017 11:44:53 +0000
Subject: [squid-users] Office 365 Support for Squid Proxy
In-Reply-To: <002601d2e5ac$8e64c4f0$ab2e4ed0$@ngtech.co.il>
References: <CAPPXLT-nXbTHdmWBQPWX-FbA0RWrJsCx54EUqcRTz=57+0_Miw@mail.gmail.com>
 <007701d2e3af$bbefc2c0$33cf4840$@ngtech.co.il>
 <CAPPXLT8enxeOmEPCWKQ9XvGH3x04P1WBDcJ5vfRwgCYNsghUSA@mail.gmail.com>
 <002601d2e5ac$8e64c4f0$ab2e4ed0$@ngtech.co.il>
Message-ID: <CAPPXLT9ty2qrp3GywDrED-0NF_rFZDegDzF3wPt2mBW6dEzKnQ@mail.gmail.com>

So it Would work for all Office  365 applications right? Have you tried
that before?

On Thu, Jun 15, 2017, 1:24 PM Eliezer Croitoru <eliezer at ngtech.co.il> wrote:

> In a simple forward proxy mode which enforces acl's it should be pretty
> simple and easy to use.
>
> All The Bests,
> Eliezer
>
> ----
> http://ngtech.co.il/lmgtfy/
> Linux System Administrator
> Mobile: +972-5-28704261
> Email: eliezer at ngtech.co.il
>
>
> From: Blason R [mailto:blason16 at gmail.com]
> Sent: Thursday, June 15, 2017 6:20 AM
> To: Eliezer Croitoru <eliezer at ngtech.co.il>
> Cc: squid-users at lists.squid-cache.org
> Subject: Re: [squid-users] Office 365 Support for Squid Proxy
>
> I am thinking to put it in forwarding only mode. And being a office 365 I
> don't see any reason for ssl-bump since I do have other device for handling
> web traffic.
> On Tue, Jun 13, 2017, 12:41 AM Eliezer Croitoru <mailto:
> eliezer at ngtech.co.il> wrote:
> The main question is if it uses websockets or not and if you are using
> SSL-BUMP or not.
> If you are using SSL-BUMP it's one thing while if you are not it?s another
> story.
> Also it will be different if you are using the proxy in INTERCEPT mode or
> a regular forward proxy mode.
> We would be able to answer you more with more details on your setup.
>
> Eliezer
>
> ----
> http://ngtech.co.il/lmgtfy/
> Linux System Administrator
> Mobile: +972-5-28704261
> Email: mailto:eliezer at ngtech.co.il
>
>
> From: squid-users [mailto:mailto:squid-users-bounces at lists.squid-cache.org]
> On Behalf Of Blason R
> Sent: Monday, June 12, 2017 12:05 PM
> To: mailto:squid-users at lists.squid-cache.org
> Subject: [squid-users] Office 365 Support for Squid Proxy
>
> Hello All,
>
> If someone can confirm if squid can very well work with Office 365? If
> anyone has any documentation can someone please forward that to me? I do
> have almost around 400 Office 365 users hence wanted to know what
> configuration I might need for Office 365 traffic?
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20170615/1b4cd1f9/attachment.htm>

From uhlar at fantomas.sk  Thu Jun 15 12:50:49 2017
From: uhlar at fantomas.sk (Matus UHLAR - fantomas)
Date: Thu, 15 Jun 2017 14:50:49 +0200
Subject: [squid-users] RV: squid
In-Reply-To: <2686958.ZNAhSqxdWU@pikantusdevuan>
References: <01ff01d2e5c5$840788d0$8c169a70$@accelya.com>
 <2686958.ZNAhSqxdWU@pikantusdevuan>
Message-ID: <20170615125049.GA5653@fantomas.sk>

>On Thursday 15 June 2017 16:22:44 javier perez wrote:
>
>> I installed squid(3.5.20) on CentOS 7 minimal to perform as an ftp-proxy.
>>
>> My configuration file looks like this:
>
>...snip...
>
>> acl SSL_ports port 443 21

On 15.06.17 13:03, Antony Stone wrote:
>Why are you specifying port 21 as SSL?

apparently result of windows settings "enable folder view for FTP sites" that
causes explorer avoid using proxy for ftp:// and connect directly as FTP
client.

maybe IE in this case uses CONNECT tunnels for FTP protocol.

I wonder how would it behave if you enabled SOCKS server.

>"ftp_passive off" should mean that you can't do passive FTP through the Squid
>server, but it won't stop the client application from trying.
>
>You need to tell the client system/s always to use active FTP (which will go
>through Squid) - Squid can't do that for you - it will simply allow or block
>whatever requests come its way.

clients using squid as CONNECT proxy technically can't use PORT mode, since
HTTP does not contain anything like LISTEN.

intercepted FTP connections are something different, although support for
this is relatively new (since 3.5)

there is SOCKS protocol that supports listening required by PORT/EPRT mode,
although most of FTP clients use passive by default

(not sure about windows commandline FTP client - at least in XP is only
supported PORT mode)


-- 
Matus UHLAR - fantomas, uhlar at fantomas.sk ; http://www.fantomas.sk/
Warning: I wish NOT to receive e-mail advertising to this address.
Varovanie: na tuto adresu chcem NEDOSTAVAT akukolvek reklamnu postu.
I intend to live forever - so far so good. 


From javier.perez at accelya.com  Thu Jun 15 14:28:59 2017
From: javier.perez at accelya.com (javier perez)
Date: Thu, 15 Jun 2017 19:58:59 +0530 (IST)
Subject: [squid-users] RV: squid
In-Reply-To: <20170615125049.GA5653@fantomas.sk>
References: <01ff01d2e5c5$840788d0$8c169a70$@accelya.com>
 <2686958.ZNAhSqxdWU@pikantusdevuan> <20170615125049.GA5653@fantomas.sk>
Message-ID: <027301d2e5e3$b9eadec0$2dc09c40$@accelya.com>

I found this on the oficial documentation:

ftp://ftp.fu-berlin.de/unix/www/squid/archive/3.5/squid-3.5.0.1-RELEASENOTES.html

Section 2.6 Relay FTP
FTP Relay highlights:
2nd line:

" Active and passive FTP support on the user-facing side; require passive 
connections to come from the control connection source IP address."

Does this mean that no active connections will be stablished between the 
dest. Host and squid?????

Thank you all in advance.

Regards

>On Thursday 15 June 2017 16:22:44 javier perez wrote:
>
>> I installed squid(3.5.20) on CentOS 7 minimal to perform as an ftp-proxy.
>>
>> My configuration file looks like this:
>
>...snip...
>
>> acl SSL_ports port 443 21

On 15.06.17 13:03, Antony Stone wrote:
>Why are you specifying port 21 as SSL?

apparently result of windows settings "enable folder view for FTP sites" 
that causes explorer avoid using proxy for ftp:// and connect directly as 
FTP client.

maybe IE in this case uses CONNECT tunnels for FTP protocol.

I wonder how would it behave if you enabled SOCKS server.

>"ftp_passive off" should mean that you can't do passive FTP through the
>Squid server, but it won't stop the client application from trying.
>
>You need to tell the client system/s always to use active FTP (which
>will go through Squid) - Squid can't do that for you - it will simply
>allow or block whatever requests come its way.

clients using squid as CONNECT proxy technically can't use PORT mode, since 
HTTP does not contain anything like LISTEN.

intercepted FTP connections are something different, although support for 
this is relatively new (since 3.5)

there is SOCKS protocol that supports listening required by PORT/EPRT mode, 
although most of FTP clients use passive by default

(not sure about windows commandline FTP client - at least in XP is only 
supported PORT mode)


--
Matus UHLAR - fantomas, uhlar at fantomas.sk ; http://www.fantomas.sk/
Warning: I wish NOT to receive e-mail advertising to this address.
Varovanie: na tuto adresu chcem NEDOSTAVAT akukolvek reklamnu postu.
I intend to live forever - so far so good.
_______________________________________________
squid-users mailing list
squid-users at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-users


From Antony.Stone at squid.open.source.it  Thu Jun 15 14:35:07 2017
From: Antony.Stone at squid.open.source.it (Antony Stone)
Date: Thu, 15 Jun 2017 16:35:07 +0200
Subject: [squid-users] RV: squid
In-Reply-To: <027301d2e5e3$b9eadec0$2dc09c40$@accelya.com>
References: <01ff01d2e5c5$840788d0$8c169a70$@accelya.com>
 <20170615125049.GA5653@fantomas.sk>
 <027301d2e5e3$b9eadec0$2dc09c40$@accelya.com>
Message-ID: <2321392.JSWA5hPb23@pikantusdevuan>

On Thursday 15 June 2017 19:58:59 javier perez wrote:

> I found this on the oficial documentation:
> 
> ftp://ftp.fu-berlin.de/unix/www/squid/archive/3.5/squid-3.5.0.1-RELEASENOTES
> .html
> 
> Section 2.6 Relay FTP
> FTP Relay highlights:
> 2nd line:
> 
> " Active and passive FTP support on the user-facing side; require passive
> connections to come from the control connection source IP address."
> 
> Does this mean that no active connections will be stablished between the
> dest. Host and squid?????

Well, yes - but only if you're using the "Native FTP Relay" feature, which 
that same documentation lists as being "a new, experimental, complex feature 
that has seen limited production exposure".

Therefore, given your situation, you might be best not using this new feature 
for the time being (I have no idea whether it's planned to allow active 
connections on the server side in future).


Antony.

-- 
"If I've told you once, I've told you a million times - stop exaggerating!"

                                                   Please reply to the list;
                                                         please *don't* CC me.



From eliezer at ngtech.co.il  Thu Jun 15 15:49:42 2017
From: eliezer at ngtech.co.il (Eliezer  Croitoru)
Date: Thu, 15 Jun 2017 18:49:42 +0300
Subject: [squid-users] Office 365 Support for Squid Proxy
In-Reply-To: <CAPPXLT9ty2qrp3GywDrED-0NF_rFZDegDzF3wPt2mBW6dEzKnQ@mail.gmail.com>
References: <CAPPXLT-nXbTHdmWBQPWX-FbA0RWrJsCx54EUqcRTz=57+0_Miw@mail.gmail.com>
 <007701d2e3af$bbefc2c0$33cf4840$@ngtech.co.il>
 <CAPPXLT8enxeOmEPCWKQ9XvGH3x04P1WBDcJ5vfRwgCYNsghUSA@mail.gmail.com>
 <002601d2e5ac$8e64c4f0$ab2e4ed0$@ngtech.co.il>
 <CAPPXLT9ty2qrp3GywDrED-0NF_rFZDegDzF3wPt2mBW6dEzKnQ@mail.gmail.com>
Message-ID: <001901d2e5ef$022e2a10$068a7e30$@ngtech.co.il>

Well I do not need to test it specifically since it's the most basic function of squid as forward proxy.
It tunnels HTTPS connections as a TCP connection.
Every instance of squid since 1.X did it in a very good way.
There shouldn't be any issues since squid only allows or disallows the connection and splice the traffic between the clients to the destination host.
If you would have used SSL-BUMP you would be required to run some tests to make sure there are no new and special things that MS invented and pushed into their HTTPs stack.

All The Bests,
Eliezer

----
http://ngtech.co.il/lmgtfy/
Linux System Administrator
Mobile: +972-5-28704261
Email: eliezer at ngtech.co.il


From: Blason R [mailto:blason16 at gmail.com] 
Sent: Thursday, June 15, 2017 2:45 PM
To: Eliezer Croitoru <eliezer at ngtech.co.il>
Cc: squid-users at lists.squid-cache.org
Subject: Re: [squid-users] Office 365 Support for Squid Proxy

So it Would work for all Office  365 applications right? Have you tried that before?
On Thu, Jun 15, 2017, 1:24 PM Eliezer Croitoru <mailto:eliezer at ngtech.co.il> wrote:
In a simple forward proxy mode which enforces acl's it should be pretty simple and easy to use.

All The Bests,
Eliezer

----
http://ngtech.co.il/lmgtfy/
Linux System Administrator
Mobile: +972-5-28704261
Email: mailto:eliezer at ngtech.co.il


From: Blason R [mailto:mailto:blason16 at gmail.com]
Sent: Thursday, June 15, 2017 6:20 AM
To: Eliezer Croitoru <mailto:eliezer at ngtech.co.il>
Cc: mailto:squid-users at lists.squid-cache.org
Subject: Re: [squid-users] Office 365 Support for Squid Proxy

I am thinking to put it in forwarding only mode. And being a office 365 I don't see any reason for ssl-bump since I do have other device for handling web traffic.
On Tue, Jun 13, 2017, 12:41 AM Eliezer Croitoru <mailto:mailto:eliezer at ngtech.co.il> wrote:
The main question is if it uses websockets or not and if you are using SSL-BUMP or not.
If you are using SSL-BUMP it's one thing while if you are not it?s another story.
Also it will be different if you are using the proxy in INTERCEPT mode or a regular forward proxy mode.
We would be able to answer you more with more details on your setup.

Eliezer

----
http://ngtech.co.il/lmgtfy/
Linux System Administrator
Mobile: +972-5-28704261
Email: mailto:mailto:eliezer at ngtech.co.il


From: squid-users [mailto:mailto:mailto:mailto:squid-users-bounces at lists.squid-cache.org] On Behalf Of Blason R
Sent: Monday, June 12, 2017 12:05 PM
To: mailto:mailto:squid-users at lists.squid-cache.org
Subject: [squid-users] Office 365 Support for Squid Proxy

Hello All,

If someone can confirm if squid can very well work with Office 365? If anyone has any documentation can someone please forward that to me? I do have almost around 400 Office 365 users hence wanted to know what configuration I might need for Office 365 traffic?



From uhlar at fantomas.sk  Thu Jun 15 15:55:02 2017
From: uhlar at fantomas.sk (Matus UHLAR - fantomas)
Date: Thu, 15 Jun 2017 17:55:02 +0200
Subject: [squid-users] RV: squid
In-Reply-To: <027301d2e5e3$b9eadec0$2dc09c40$@accelya.com>
References: <01ff01d2e5c5$840788d0$8c169a70$@accelya.com>
 <2686958.ZNAhSqxdWU@pikantusdevuan>
 <20170615125049.GA5653@fantomas.sk>
 <027301d2e5e3$b9eadec0$2dc09c40$@accelya.com>
Message-ID: <20170615155502.GA14831@fantomas.sk>

On 15.06.17 19:58, javier perez wrote:
>I found this on the oficial documentation:
>
>ftp://ftp.fu-berlin.de/unix/www/squid/archive/3.5/squid-3.5.0.1-RELEASENOTES.html
>
>Section 2.6 Relay FTP
>FTP Relay highlights:
>2nd line:
>
>" Active and passive FTP support on the user-facing side; require passive
>connections to come from the control connection source IP address."

IMHO

that means, if you open FTP control connection to squid, the passive data
connection to it must come from the same IP as control connection.

That in fact means, you can't use squid for FXP (server-server transfers).

>Does this mean that no active connections will be stablished between the
>dest. Host and squid?????

IMHO

that one is still managed by ftp_passive option.


-- 
Matus UHLAR - fantomas, uhlar at fantomas.sk ; http://www.fantomas.sk/
Warning: I wish NOT to receive e-mail advertising to this address.
Varovanie: na tuto adresu chcem NEDOSTAVAT akukolvek reklamnu postu.
If Barbie is so popular, why do you have to buy her friends? 


From eliezer at ngtech.co.il  Thu Jun 15 15:57:25 2017
From: eliezer at ngtech.co.il (Eliezer  Croitoru)
Date: Thu, 15 Jun 2017 18:57:25 +0300
Subject: [squid-users] RV: squid
In-Reply-To: <01ff01d2e5c5$840788d0$8c169a70$@accelya.com>
References: <01ff01d2e5c5$840788d0$8c169a70$@accelya.com>
Message-ID: <001b01d2e5f0$1543ead0$3fcbc070$@ngtech.co.il>

Hey,

Can you re-define your scenario?
Squid in it's basic form is merely a http proxy which you can use to fetch
ftp requests.
I do not know exactly what you expect but when you use squid as a FTP proxy
it would convert the ftp connection into http.
If you are using a specific ftp client it might be different and then you
would need the port 21 on the Safe_Ports list.
But you cannot force a server to use an active or passive connection since
the ftp service will be defined only for active or for both active and
passive connections.
This is not in the hand of squid...
If you have a specific ftp address which we can use to test it would help a
lot.

Eliezer

----
http://ngtech.co.il/lmgtfy/
Linux System Administrator
Mobile: +972-5-28704261
Email: eliezer at ngtech.co.il


From: squid-users [mailto:squid-users-bounces at lists.squid-cache.org] On
Behalf Of javier perez
Sent: Thursday, June 15, 2017 1:53 PM
To: squid-users at lists.squid-cache.org
Subject: [squid-users] RV: squid

Good morning squid users,

I?m facing a weird situation in my Company? let me explain:

I installed squid(3.5.20) on CentOS 7 minimal to perform as an ftp-proxy. 
My configuration file looks like this:

/etc/squid/squid.conf
############################################################################
##

acl SSL_ports port 443 21
ftp_port 21
ftp_passive off

############################################################################
##

acl Safe_ports port 80????????? # http
acl Safe_ports port 443???????? # https
acl Safe_ports port 70????????? # gopher
acl Safe_ports port 21
acl Safe_ports port 210???????? # wais
acl Safe_ports port 1025-65535? # unregistered ports
acl Safe_ports port 280???????? # http-mgmt
acl Safe_ports port 488???????? # gss-http
acl Safe_ports port 591???????? # filemaker
acl Safe_ports port 777??? ?????# multiling http
acl CONNECT method CONNECT

############################################################################
##

acl FTP proto FTP
always_direct allow FTP

############################################################################
##

http_access allow CONNECT SSL_ports
http_access allow CONNECT Safe_ports
http_access allow SSL_ports
http_access allow Safe_ports
http_access allow all
http_access allow FTP

############################################################################
##

http_port 3128

refresh_pattern ^ftp:?????????? 1440??? 20%???? 10080
refresh_pattern ^gopher:??????? 1440??? 0%????? 1440
refresh_pattern -i (/cgi-bin/|\?) 0???? 0%???? ?0
refresh_pattern .?????????????? 0?????? 20%???? 4320

############################################################################
##

The thing is that the parameter ?ftp_passive off? seems not to be working? 
Due to security measures we have to use non-passive mode to be able to
transfer anything.

The connection works fine with the remote hosts, the login works, but I have
to enter ?passive? every single time to swap the mode to non-passive. 
I don?t know whether the ?ftp_passive? is not working or I need to do
something else.

After doing a deep research I cannot find much information related with this
kind of problems, so I decided to get in touch with you. Please help!!

Thanks in advance.
Regards





From rousskov at measurement-factory.com  Thu Jun 15 16:06:28 2017
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Thu, 15 Jun 2017 10:06:28 -0600
Subject: [squid-users] RV: squid
In-Reply-To: <20170615155502.GA14831@fantomas.sk>
References: <01ff01d2e5c5$840788d0$8c169a70$@accelya.com>
 <2686958.ZNAhSqxdWU@pikantusdevuan> <20170615125049.GA5653@fantomas.sk>
 <027301d2e5e3$b9eadec0$2dc09c40$@accelya.com>
 <20170615155502.GA14831@fantomas.sk>
Message-ID: <ea0958e0-70ff-75fb-be2c-4b9444dbe6a9@measurement-factory.com>

On 06/15/2017 09:55 AM, Matus UHLAR - fantomas wrote:
>> ftp://ftp.fu-berlin.de/unix/www/squid/archive/3.5/squid-3.5.0.1-RELEASENOTES.html
>> " Active and passive FTP support on the user-facing side; require passive
>> connections to come from the control connection source IP address."

> that means, if you open FTP control connection to squid, the passive data
> connection to it must come from the same IP as control connection.

IIRC, the above interpretation is the right one:

* We support both active and passive FTP between an FTP client (a.k.a.
user) and Squid.

* When an FTP client is using passive mode, the data connection must
come from the same IP as the control connection. This restriction blocks
attacks that steal data connection of legitimate FTP users.

AFAIK, there are currently no plans (or even strong demand) to support
active FTP mode between Squid and FTP origin servers.


Alex.


From Antony.Stone at squid.open.source.it  Thu Jun 15 16:21:43 2017
From: Antony.Stone at squid.open.source.it (Antony Stone)
Date: Thu, 15 Jun 2017 18:21:43 +0200
Subject: [squid-users] RV: squid
In-Reply-To: <01ff01d2e5c5$840788d0$8c169a70$@accelya.com>
References: <01ff01d2e5c5$840788d0$8c169a70$@accelya.com>
Message-ID: <9602862.vTdPJ2O8yu@pikantusdevuan>

On Thursday 15 June 2017 16:22:44 javier perez wrote:

> I installed squid(3.5.20) on CentOS 7 minimal to perform as an ftp-proxy.

Why?

What are you trying to achieve by doing this, instead of simply allowing 
clients inside to connect to servers outside?


Antony.

-- 
I lay awake all night wondering where the sun went, and then it dawned on me.

                                                   Please reply to the list;
                                                         please *don't* CC me.



From javier.perez at accelya.com  Fri Jun 16 06:27:35 2017
From: javier.perez at accelya.com (javier perez)
Date: Fri, 16 Jun 2017 11:57:35 +0530 (IST)
Subject: [squid-users] RV: squid
In-Reply-To: <9602862.vTdPJ2O8yu@pikantusdevuan>
References: <01ff01d2e5c5$840788d0$8c169a70$@accelya.com>
 <9602862.vTdPJ2O8yu@pikantusdevuan>
Message-ID: <002f01d2e669$a3ffca90$ebff5fb0$@accelya.com>

Hi Anthony,

My server acts as a focal point for all ftp transfer on a highly securized 
network.

I have more tan 100 static routes pointing to different gateways deppending 
on our client addresses.

The thing is that only 2 of our customers have old fashioned active-ftp 
sites, so only bcz of them my work is ruined.

We were trying to change the Windows server that currently do the task 
(native active-ftp).

Anyway, thank you so much for your time and interest.

__________________________________________________________________________________________________________


I installed squid(3.5.20) on CentOS 7 minimal to perform as an ftp-proxy.

Why?

What are you trying to achieve by doing this, instead of simply allowing 
clients inside to connect to servers outside?


Antony.

-- 
I lay awake all night wondering where the sun went, and then it dawned on 
me.

                                                   Please reply to the list;
                                                         please *don't* CC 
me.

_______________________________________________
squid-users mailing list
squid-users at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-users


From javier.perez at accelya.com  Fri Jun 16 06:33:44 2017
From: javier.perez at accelya.com (javier perez)
Date: Fri, 16 Jun 2017 12:03:44 +0530 (IST)
Subject: [squid-users] RV: squid
In-Reply-To: <20170615155502.GA14831@fantomas.sk>
References: <01ff01d2e5c5$840788d0$8c169a70$@accelya.com>
 <2686958.ZNAhSqxdWU@pikantusdevuan> <20170615125049.GA5653@fantomas.sk>
 <027301d2e5e3$b9eadec0$2dc09c40$@accelya.com>
 <20170615155502.GA14831@fantomas.sk>
Message-ID: <003501d2e66a$8026e440$8074acc0$@accelya.com>

Hello Matus,

You are right, the thing is that our clients are not going to open any other 
port than 20 and 21 for security meassures (or lazyness).

So, if We can't use a dinamic data- port on the destination, passive ftp is 
discarded.

The thing is that with the "ftp_passive off" directive the most of my 
clients don't work at all, just a couple of them demand active ftp and make 
my life a bit more complicated bcz of this deprecated way of ftp-ing.

We are working with highly securized environments that make very difficult 
any kind of modification.

Thank you very much for your time and effort.
Regards


On 15.06.17 19:58, javier perez wrote:
>I found this on the oficial documentation:
>
>ftp://ftp.fu-berlin.de/unix/www/squid/archive/3.5/squid-3.5.0.1-RELEASE
>NOTES.html
>
>Section 2.6 Relay FTP
>FTP Relay highlights:
>2nd line:
>
>" Active and passive FTP support on the user-facing side; require
>passive connections to come from the control connection source IP address."

IMHO

that means, if you open FTP control connection to squid, the passive data 
connection to it must come from the same IP as control connection.

That in fact means, you can't use squid for FXP (server-server transfers).

>Does this mean that no active connections will be stablished between
>the dest. Host and squid?????

IMHO

that one is still managed by ftp_passive option.


--
Matus UHLAR - fantomas, uhlar at fantomas.sk ; http://www.fantomas.sk/
Warning: I wish NOT to receive e-mail advertising to this address.
Varovanie: na tuto adresu chcem NEDOSTAVAT akukolvek reklamnu postu.
If Barbie is so popular, why do you have to buy her friends?
_______________________________________________
squid-users mailing list
squid-users at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-users


From javier.perez at accelya.com  Fri Jun 16 06:39:50 2017
From: javier.perez at accelya.com (javier perez)
Date: Fri, 16 Jun 2017 12:09:50 +0530 (IST)
Subject: [squid-users] RV: squid
In-Reply-To: <001b01d2e5f0$1543ead0$3fcbc070$@ngtech.co.il>
References: <01ff01d2e5c5$840788d0$8c169a70$@accelya.com>
 <001b01d2e5f0$1543ead0$3fcbc070$@ngtech.co.il>
Message-ID: <003e01d2e66b$5a0dd1f0$0e2975d0$@accelya.com>

Hello Eliezer,

I have more tan 100 clients and only 2 of them demand active ftp, so only
bcz of them we are rolling-back to Windows.
Squid works perfectly with the other 100+ clients, so I am really happy
with Squid, and I will use it in the future for sure.

I can't redefine the thing bcz it depends on my clients, not on me.

Thank you very much for your interest and feedback.

Regards
__________________________________________________________________________
__________________________________________

Hey,

Can you re-define your scenario?
Squid in it's basic form is merely a http proxy which you can use to fetch
ftp requests.
I do not know exactly what you expect but when you use squid as a FTP
proxy it would convert the ftp connection into http.
If you are using a specific ftp client it might be different and then you
would need the port 21 on the Safe_Ports list.
But you cannot force a server to use an active or passive connection since
the ftp service will be defined only for active or for both active and
passive connections.
This is not in the hand of squid...
If you have a specific ftp address which we can use to test it would help
a lot.

Eliezer

----
http://ngtech.co.il/lmgtfy/
Linux System Administrator
Mobile: +972-5-28704261
Email: eliezer at ngtech.co.il


From: squid-users [mailto:squid-users-bounces at lists.squid-cache.org] On
Behalf Of javier perez
Sent: Thursday, June 15, 2017 1:53 PM
To: squid-users at lists.squid-cache.org
Subject: [squid-users] RV: squid

Good morning squid users,

I?m facing a weird situation in my Company
 let me explain:

I installed squid(3.5.20) on CentOS 7 minimal to perform as an ftp-proxy.
My configuration file looks like this:

/etc/squid/squid.conf
##########################################################################
##
##

acl SSL_ports port 443 21
ftp_port 21
ftp_passive off

##########################################################################
##
##

acl Safe_ports port 80????????? # http
acl Safe_ports port 443???????? # https
acl Safe_ports port 70????????? # gopher acl Safe_ports port 21 acl
Safe_ports port 210???????? # wais acl Safe_ports port 1025-65535? #
unregistered ports acl Safe_ports port 280???????? # http-mgmt acl
Safe_ports port 488???????? # gss-http acl Safe_ports port 591???????? #
filemaker acl Safe_ports port 777??? ?????# multiling http acl CONNECT
method CONNECT

##########################################################################
##
##

acl FTP proto FTP
always_direct allow FTP

##########################################################################
##
##

http_access allow CONNECT SSL_ports
http_access allow CONNECT Safe_ports
http_access allow SSL_ports
http_access allow Safe_ports
http_access allow all
http_access allow FTP

##########################################################################
##
##

http_port 3128

refresh_pattern ^ftp:?????????? 1440??? 20%???? 10080 refresh_pattern
^gopher:??????? 1440??? 0%????? 1440 refresh_pattern -i (/cgi-bin/|\?)
0???? 0%???? ?0 refresh_pattern .?????????????? 0?????? 20%???? 4320

##########################################################################
##
##

The thing is that the parameter ?ftp_passive off? seems not to be working

Due to security measures we have to use non-passive mode to be able to
transfer anything.

The connection works fine with the remote hosts, the login works, but I
have to enter ?passive? every single time to swap the mode to non-passive.

I don?t know whether the ?ftp_passive? is not working or I need to do
something else.

After doing a deep research I cannot find much information related with
this kind of problems, so I decided to get in touch with you. Please
help!!

Thanks in advance.
Regards





From javier.perez at accelya.com  Fri Jun 16 06:42:52 2017
From: javier.perez at accelya.com (javier perez)
Date: Fri, 16 Jun 2017 12:12:52 +0530 (IST)
Subject: [squid-users] RV: squid
In-Reply-To: <ea0958e0-70ff-75fb-be2c-4b9444dbe6a9@measurement-factory.com>
References: <01ff01d2e5c5$840788d0$8c169a70$@accelya.com>
 <2686958.ZNAhSqxdWU@pikantusdevuan> <20170615125049.GA5653@fantomas.sk>
 <027301d2e5e3$b9eadec0$2dc09c40$@accelya.com>
 <20170615155502.GA14831@fantomas.sk>
 <ea0958e0-70ff-75fb-be2c-4b9444dbe6a9@measurement-factory.com>
Message-ID: <004201d2e66b$c6d71800$54854800$@accelya.com>

Hi Alex,

I totally understand it, and I know that active ftp is being deprecated, so 
It's logic that no further development It's gonna take place.

I'm happy with Squid, and it works perfectly on 99% of my clients but two.

Thank you for your time.
Regards.

On 06/15/2017 09:55 AM, Matus UHLAR - fantomas wrote:
>> ftp://ftp.fu-berlin.de/unix/www/squid/archive/3.5/squid-3.5.0.1-RELEA
>> SENOTES.html " Active and passive FTP support on the user-facing
>> side; require passive connections to come from the control connection
>> source IP address."

> that means, if you open FTP control connection to squid, the passive
> data connection to it must come from the same IP as control connection.

IIRC, the above interpretation is the right one:

* We support both active and passive FTP between an FTP client (a.k.a.
user) and Squid.

* When an FTP client is using passive mode, the data connection must come 
from the same IP as the control connection. This restriction blocks attacks 
that steal data connection of legitimate FTP users.

AFAIK, there are currently no plans (or even strong demand) to support 
active FTP mode between Squid and FTP origin servers.


Alex.
_______________________________________________
squid-users mailing list
squid-users at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-users


From norbert.naveen at tayana.in  Fri Jun 16 08:30:51 2017
From: norbert.naveen at tayana.in (Norbert Naveen)
Date: Fri, 16 Jun 2017 14:00:51 +0530
Subject: [squid-users] Squid Transparent HTTP Proxy - 2 ETH Links -
	HTTP	Proxy
References: <01a201d2e512$5dd492b0$197db810$@tayana.in>
 <003b01d2e516$3c8276f0$b58764d0$@ngtech.co.il> 
Message-ID: <003201d2e67a$dd33e510$979baf30$@tayana.in>

Hello Eliezer, 
 Switch - Cisco 3750 
 Did not understand the " mirror all traffic to the probe node."
Thanks 
Naveen

-----Original Message-----
From: Eliezer Croitoru [mailto:eliezer at ngtech.co.il] 
Sent: Wednesday, June 14, 2017 7:28 PM
To: norbert.naveen at tayana.in
Cc: squid-users at lists.squid-cache.org
Subject: RE: [squid-users] Squid Transparent HTTP Proxy - 2 ETH Links - HTTP
Proxy

It depends on the equipment..
What you should do is to use the switch to pass all traffic to the squid mac
address and mirror all traffic to the probe node.
What switch do you have there?

Eliezer

----
http://ngtech.co.il/lmgtfy/
Linux System Administrator
Mobile: +972-5-28704261
Email: eliezer at ngtech.co.il


From: squid-users [mailto:squid-users-bounces at lists.squid-cache.org] On
Behalf Of Norbert Naveen
Sent: Wednesday, June 14, 2017 4:30 PM
To: squid-users at lists.squid-cache.org
Subject: [squid-users] Squid Transparent HTTP Proxy - 2 ETH Links - HTTP
Proxy

Hello Admins , 

Pls refer to the Image as in 

https://drive.google.com/open?id=0B_dDVNpzSGEKZmFPWHFLWlJJMUU

The Setup will be as attached ?in URL Above 
 Server which will Host Squid
will have Two Interfaces with 2 Different VLAN Tags Content Inspection
Engine will REROUTE all HTTP Traffic Through the Links coming to Squid
Server . 
Squid Server has to act as TRANSPARENT PROXY

One Possible way of doing it IP tables and Masquerading SRC IP But 
 Without
Changing Src or Dst IP address . How to achieve the same ? 

ALL HTTP Traffic will be forward from 1 to 2 and Squid will be in between We
will have to Forward all traffic on 1 to 2 .. ?


Thanks 
Naveen




From squid3 at treenet.co.nz  Fri Jun 16 10:10:07 2017
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Fri, 16 Jun 2017 22:10:07 +1200
Subject: [squid-users] RV: squid
In-Reply-To: <004201d2e66b$c6d71800$54854800$@accelya.com>
References: <01ff01d2e5c5$840788d0$8c169a70$@accelya.com>
 <2686958.ZNAhSqxdWU@pikantusdevuan> <20170615125049.GA5653@fantomas.sk>
 <027301d2e5e3$b9eadec0$2dc09c40$@accelya.com>
 <20170615155502.GA14831@fantomas.sk>
 <ea0958e0-70ff-75fb-be2c-4b9444dbe6a9@measurement-factory.com>
 <004201d2e66b$c6d71800$54854800$@accelya.com>
Message-ID: <efe2b5d7-c37f-9740-17f9-9e536c04d842@treenet.co.nz>

On 16/06/17 18:42, javier perez wrote:
> Hi Alex,
>
> I totally understand it, and I know that active ftp is being deprecated, so
> It's logic that no further development It's gonna take place.

That reason just makes it unlikely, not impossible. Squid being FOSS 
anyone can contribute patches at any time that make things like this happen.

Patches to make Squid accept active-FTP from clients and convert that to 
passive-FTP on the server connection would be welcome.


> I'm happy with Squid, and it works perfectly on 99% of my clients but two.

FYI; you might want to look into foxyproxy or similar dedicated FTP 
proxies rather than Squid., They have a much better focus on proxying 
FTP and a longer history than Squid in the area, so probably less 
missing features.

Amos



From uhlar at fantomas.sk  Fri Jun 16 10:40:37 2017
From: uhlar at fantomas.sk (Matus UHLAR - fantomas)
Date: Fri, 16 Jun 2017 12:40:37 +0200
Subject: [squid-users] RV: squid
In-Reply-To: <ea0958e0-70ff-75fb-be2c-4b9444dbe6a9@measurement-factory.com>
References: <01ff01d2e5c5$840788d0$8c169a70$@accelya.com>
 <2686958.ZNAhSqxdWU@pikantusdevuan>
 <20170615125049.GA5653@fantomas.sk>
 <027301d2e5e3$b9eadec0$2dc09c40$@accelya.com>
 <20170615155502.GA14831@fantomas.sk>
 <ea0958e0-70ff-75fb-be2c-4b9444dbe6a9@measurement-factory.com>
Message-ID: <20170616104037.GA5851@fantomas.sk>

>>> ftp://ftp.fu-berlin.de/unix/www/squid/archive/3.5/squid-3.5.0.1-RELEASENOTES.html
>>> " Active and passive FTP support on the user-facing side; require passive
>>> connections to come from the control connection source IP address."

>On 06/15/2017 09:55 AM, Matus UHLAR - fantomas wrote:
>> that means, if you open FTP control connection to squid, the passive data
>> connection to it must come from the same IP as control connection.

On 15.06.17 10:06, Alex Rousskov wrote:
>IIRC, the above interpretation is the right one:

just for sure: my one?

>* We support both active and passive FTP between an FTP client (a.k.a.
>user) and Squid.
>
>* When an FTP client is using passive mode, the data connection must
>come from the same IP as the control connection. This restriction blocks
>attacks that steal data connection of legitimate FTP users.
>
>AFAIK, there are currently no plans (or even strong demand) to support
>active FTP mode between Squid and FTP origin servers.

what is ftp_passive for then?

btw I suggest calling it "port" FTP mode instead of active


-- 
Matus UHLAR - fantomas, uhlar at fantomas.sk ; http://www.fantomas.sk/
Warning: I wish NOT to receive e-mail advertising to this address.
Varovanie: na tuto adresu chcem NEDOSTAVAT akukolvek reklamnu postu.
Quantum mechanics: The dreams stuff is made of. 


From javier.perez at accelya.com  Fri Jun 16 11:26:01 2017
From: javier.perez at accelya.com (javier perez)
Date: Fri, 16 Jun 2017 16:56:01 +0530 (IST)
Subject: [squid-users] RV: squid
In-Reply-To: <efe2b5d7-c37f-9740-17f9-9e536c04d842@treenet.co.nz>
References: <01ff01d2e5c5$840788d0$8c169a70$@accelya.com>
 <2686958.ZNAhSqxdWU@pikantusdevuan> <20170615125049.GA5653@fantomas.sk>
 <027301d2e5e3$b9eadec0$2dc09c40$@accelya.com>
 <20170615155502.GA14831@fantomas.sk>
 <ea0958e0-70ff-75fb-be2c-4b9444dbe6a9@measurement-factory.com>
 <004201d2e66b$c6d71800$54854800$@accelya.com>
 <efe2b5d7-c37f-9740-17f9-9e536c04d842@treenet.co.nz>
Message-ID: <002901d2e693$55004df0$ff00e9d0$@accelya.com>

Thank you very much Amos for your suggestion, I'm gonna study it straight 
away.

Regards!

________________________________________________________________________________________________________________________________

On 16/06/17 18:42, javier perez wrote:
> Hi Alex,
>
> I totally understand it, and I know that active ftp is being
> deprecated, so It's logic that no further development It's gonna take 
> place.

That reason just makes it unlikely, not impossible. Squid being FOSS anyone 
can contribute patches at any time that make things like this happen.

Patches to make Squid accept active-FTP from clients and convert that to 
passive-FTP on the server connection would be welcome.


> I'm happy with Squid, and it works perfectly on 99% of my clients but two.

FYI; you might want to look into foxyproxy or similar dedicated FTP
proxies rather than Squid., They have a much better focus on proxying
FTP and a longer history than Squid in the area, so probably less
missing features.

Amos

_______________________________________________
squid-users mailing list
squid-users at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-users


From squid3 at treenet.co.nz  Fri Jun 16 11:49:53 2017
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Fri, 16 Jun 2017 23:49:53 +1200
Subject: [squid-users] RV: squid
In-Reply-To: <20170616104037.GA5851@fantomas.sk>
References: <01ff01d2e5c5$840788d0$8c169a70$@accelya.com>
 <2686958.ZNAhSqxdWU@pikantusdevuan> <20170615125049.GA5653@fantomas.sk>
 <027301d2e5e3$b9eadec0$2dc09c40$@accelya.com>
 <20170615155502.GA14831@fantomas.sk>
 <ea0958e0-70ff-75fb-be2c-4b9444dbe6a9@measurement-factory.com>
 <20170616104037.GA5851@fantomas.sk>
Message-ID: <2502fce8-6a7a-9f4e-d1da-84bdb7d8c9ad@treenet.co.nz>

On 16/06/17 22:40, Matus UHLAR - fantomas wrote:
>>>> ftp://ftp.fu-berlin.de/unix/www/squid/archive/3.5/squid-3.5.0.1-RELEASENOTES.html 
>>>>
>>>> " Active and passive FTP support on the user-facing side; require 
>>>> passive
>>>> connections to come from the control connection source IP address."
>
>> On 06/15/2017 09:55 AM, Matus UHLAR - fantomas wrote:
>>> that means, if you open FTP control connection to squid, the passive 
>>> data
>>> connection to it must come from the same IP as control connection.
>
> On 15.06.17 10:06, Alex Rousskov wrote:
>> IIRC, the above interpretation is the right one:
>
> just for sure: my one?
>
>> * We support both active and passive FTP between an FTP client (a.k.a.
>> user) and Squid.
>>
>> * When an FTP client is using passive mode, the data connection must
>> come from the same IP as the control connection. This restriction blocks
>> attacks that steal data connection of legitimate FTP users.
>>
>> AFAIK, there are currently no plans (or even strong demand) to support
>> active FTP mode between Squid and FTP origin servers.
>
> what is ftp_passive for then?

For controlling how Squid gateways  "GET ftp://example.com/ HTTP/1.1" 
requests to an FTP server. Whether it attempts PASV / EPSV mode commands 
at all, or skips straight to the fallback "active" PORT/EPRT commands.


> btw I suggest calling it "port" FTP mode instead of active

active vs passive are well-known terms for how DATA connections in FTP 
work (<http://slacksite.com/other/ftp.html> to pick the top result in 
from Google claiming to be *the* definition of the terms). AFAIK, the 
words come from RFC 959 itself:

  "server-DTP

          The data transfer process, in its normal "active" state,
          establishes the data connection with the "listening" data port.
          It sets up parameters for transfer and storage, and transfers
          data on command from its PI.  The DTP can be placed in a
          "passive" state to listen for, rather than initiate a
          connection on the data port.
"

They refer to whether the server is actively initiating TCP connections 
to the client, or passively waiting for the client to connect to a 
random listener port the server sets up.


Amos



From squid3 at treenet.co.nz  Fri Jun 16 11:55:25 2017
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Fri, 16 Jun 2017 23:55:25 +1200
Subject: [squid-users] RV: squid
In-Reply-To: <002901d2e693$55004df0$ff00e9d0$@accelya.com>
References: <01ff01d2e5c5$840788d0$8c169a70$@accelya.com>
 <2686958.ZNAhSqxdWU@pikantusdevuan> <20170615125049.GA5653@fantomas.sk>
 <027301d2e5e3$b9eadec0$2dc09c40$@accelya.com>
 <20170615155502.GA14831@fantomas.sk>
 <ea0958e0-70ff-75fb-be2c-4b9444dbe6a9@measurement-factory.com>
 <004201d2e66b$c6d71800$54854800$@accelya.com>
 <efe2b5d7-c37f-9740-17f9-9e536c04d842@treenet.co.nz>
 <002901d2e693$55004df0$ff00e9d0$@accelya.com>
Message-ID: <a2409036-19d5-82f4-50ac-8b38b12edcc2@treenet.co.nz>

On 16/06/17 23:26, javier perez wrote:
> Thank you very much Amos for your suggestion, I'm gonna study it straight
> away.
>

Ouch. Sorry I thought one thing and typed another. What I meant to 
suggest was FROX and similar. FoxyProxy is the browser integration thing 
for proxying.

<http://frox.sourceforge.net/>

Amos



From squid3 at treenet.co.nz  Fri Jun 16 11:55:33 2017
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Fri, 16 Jun 2017 23:55:33 +1200
Subject: [squid-users] RV: squid
In-Reply-To: <003501d2e66a$8026e440$8074acc0$@accelya.com>
References: <01ff01d2e5c5$840788d0$8c169a70$@accelya.com>
 <2686958.ZNAhSqxdWU@pikantusdevuan> <20170615125049.GA5653@fantomas.sk>
 <027301d2e5e3$b9eadec0$2dc09c40$@accelya.com>
 <20170615155502.GA14831@fantomas.sk>
 <003501d2e66a$8026e440$8074acc0$@accelya.com>
Message-ID: <ce5f6f79-34bb-81a8-a68a-fb3583a77907@treenet.co.nz>

On 16/06/17 18:33, javier perez wrote:
> Hello Matus,
>
> You are right, the thing is that our clients are not going to open any other
> port than 20 and 21 for security meassures (or lazyness).

FYI: The "for security" argument is bogus because;

a)  allowing any random client to determine their own arbitrary port 
number(s) is strictly worse for security than having your control point 
(Squid) select the port, and

b) limiting that client-selected port to 20/21 makes the data between 
client and Squid go over a port which is more easily predicted and 
therefore interceptable by passive attack.

Amos



From javier.perez at accelya.com  Fri Jun 16 11:57:04 2017
From: javier.perez at accelya.com (javier perez)
Date: Fri, 16 Jun 2017 17:27:04 +0530 (IST)
Subject: [squid-users] RV: squid
In-Reply-To: <a2409036-19d5-82f4-50ac-8b38b12edcc2@treenet.co.nz>
References: <01ff01d2e5c5$840788d0$8c169a70$@accelya.com>
 <2686958.ZNAhSqxdWU@pikantusdevuan> <20170615125049.GA5653@fantomas.sk>
 <027301d2e5e3$b9eadec0$2dc09c40$@accelya.com>
 <20170615155502.GA14831@fantomas.sk>
 <ea0958e0-70ff-75fb-be2c-4b9444dbe6a9@measurement-factory.com>
 <004201d2e66b$c6d71800$54854800$@accelya.com>
 <efe2b5d7-c37f-9740-17f9-9e536c04d842@treenet.co.nz>
 <002901d2e693$55004df0$ff00e9d0$@accelya.com>
 <a2409036-19d5-82f4-50ac-8b38b12edcc2@treenet.co.nz>
Message-ID: <003401d2e697$ab8e8520$02ab8f60$@accelya.com>

Yes!! I was wondering wtf is this xD!!

I will check FROX, and ty again!

Regards


On 16/06/17 23:26, javier perez wrote:
> Thank you very much Amos for your suggestion, I'm gonna study it
> straight away.
>

Ouch. Sorry I thought one thing and typed another. What I meant to suggest 
was FROX and similar. FoxyProxy is the browser integration thing for 
proxying.

<http://frox.sourceforge.net/>

Amos

_______________________________________________
squid-users mailing list
squid-users at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-users


From javier.perez at accelya.com  Fri Jun 16 11:57:43 2017
From: javier.perez at accelya.com (javier perez)
Date: Fri, 16 Jun 2017 17:27:43 +0530 (IST)
Subject: [squid-users] RV: squid
In-Reply-To: <ce5f6f79-34bb-81a8-a68a-fb3583a77907@treenet.co.nz>
References: <01ff01d2e5c5$840788d0$8c169a70$@accelya.com>
 <2686958.ZNAhSqxdWU@pikantusdevuan> <20170615125049.GA5653@fantomas.sk>
 <027301d2e5e3$b9eadec0$2dc09c40$@accelya.com>
 <20170615155502.GA14831@fantomas.sk>
 <003501d2e66a$8026e440$8074acc0$@accelya.com>
 <ce5f6f79-34bb-81a8-a68a-fb3583a77907@treenet.co.nz>
Message-ID: <003601d2e697$c2e58e80$48b0ab80$@accelya.com>

They could open just a range of 5 dinamic ports and monitor them 
intensively...

> Hello Matus,
>
> You are right, the thing is that our clients are not going to open any
> other port than 20 and 21 for security meassures (or lazyness).

FYI: The "for security" argument is bogus because;

a)  allowing any random client to determine their own arbitrary port
number(s) is strictly worse for security than having your control point
(Squid) select the port, and

b) limiting that client-selected port to 20/21 makes the data between client 
and Squid go over a port which is more easily predicted and therefore 
interceptable by passive attack.

Amos

_______________________________________________
squid-users mailing list
squid-users at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-users


From uhlar at fantomas.sk  Fri Jun 16 12:22:59 2017
From: uhlar at fantomas.sk (Matus UHLAR - fantomas)
Date: Fri, 16 Jun 2017 14:22:59 +0200
Subject: [squid-users] RV: squid
In-Reply-To: <2502fce8-6a7a-9f4e-d1da-84bdb7d8c9ad@treenet.co.nz>
References: <01ff01d2e5c5$840788d0$8c169a70$@accelya.com>
 <2686958.ZNAhSqxdWU@pikantusdevuan>
 <20170615125049.GA5653@fantomas.sk>
 <027301d2e5e3$b9eadec0$2dc09c40$@accelya.com>
 <20170615155502.GA14831@fantomas.sk>
 <ea0958e0-70ff-75fb-be2c-4b9444dbe6a9@measurement-factory.com>
 <20170616104037.GA5851@fantomas.sk>
 <2502fce8-6a7a-9f4e-d1da-84bdb7d8c9ad@treenet.co.nz>
Message-ID: <20170616122259.GA6571@fantomas.sk>

Citing:

>>>AFAIK, there are currently no plans (or even strong demand) to support
>>>active FTP mode between Squid and FTP origin servers.

>On 16/06/17 22:40, Matus UHLAR - fantomas wrote:
>>what is ftp_passive for then?

On 16.06.17 23:49, Amos Jeffries wrote:
>For controlling how Squid gateways  "GET ftp://example.com/ HTTP/1.1" 
>requests to an FTP server. Whether it attempts PASV / EPSV mode 
>commands at all, or skips straight to the fallback "active" PORT/EPRT 
>commands.

so, where exactly does squid NOT support active mode, when it does support
it on client's side and on the server side when gatewaying FTP commands?


>>btw I suggest calling it "port" FTP mode instead of active
>
>active vs passive are well-known terms for how DATA connections in 
>FTP work (<http://slacksite.com/other/ftp.html> to pick the top 
>result in from Google claiming to be *the* definition of the terms). 
>AFAIK, the words come from RFC 959 itself:

ok, got it.

-- 
Matus UHLAR - fantomas, uhlar at fantomas.sk ; http://www.fantomas.sk/
Warning: I wish NOT to receive e-mail advertising to this address.
Varovanie: na tuto adresu chcem NEDOSTAVAT akukolvek reklamnu postu.
The only substitute for good manners is fast reflexes. 


From eliezer at ngtech.co.il  Fri Jun 16 12:49:37 2017
From: eliezer at ngtech.co.il (Eliezer  Croitoru)
Date: Fri, 16 Jun 2017 15:49:37 +0300
Subject: [squid-users] Squid 3.5.26 and 4.0.20 RPM's avaliable
Message-ID: <044601d2e69f$03f6a510$0be3ef30$@ngtech.co.il>

First something nice for the weekend:
http://geekshumor.com/wp-content/uploads/2013/04/You-said-I-should-spend.gif

And:

I am happy to release the RPM's for Squid-Cache versions: 3.5.26 and 4.0.20.
The RPM's included are for: CentOS 6+7, RHEL 7, SLES 12, Oracle Enterprise Linux 6+7.
I have also released DEB packages for Ubuntu 16.04 and Debian 8(Jessie) at:
- http://ngtech.co.il/repo/debian/jessie/
- http://ngtech.co.il/repo/ubuntu/16.04/

I am also releasing the article: "How accurate Statistics are? The SquidBlocker Way." Special for this release.
The article is at: http://www1.ngtech.co.il/wpe/?p=457
This is a very special article and shows the very unique way that a url blacklist DB is being used in the Medical and Therapy world.
The regular way of using SquidGuard has one single purpose but SquidBlocker was written to also be a Therapy tool.
This is one big difference that I can say about this tool, more details are in the article.

All The Bests,
Eliezer

* A nice caching test site: https://www.pinterest.com/rwlibrary/tech-chuckles/
* A YouTube Categorizing public DB on the way, will post some details on it in the next couple days\weeks.

----
Eliezer Croitoru
Linux System Administrator
Mobile: +972-5-28704261
Email: eliezer at ngtech.co.il



-----Original Message-----
From: squid-users [mailto:squid-users-bounces at lists.squid-cache.org] On Behalf Of Amos Jeffries
Sent: Tuesday, June 6, 2017 7:05 AM
To: squid-announce at lists.squid-cache.org
Subject: [squid-users] [squid-announce] Squid 3.5.26 is available

The Squid HTTP Proxy team is very pleased to announce the availability of the Squid-3.5.26 release!


This release is a bug fix release resolving several issues found in the prior Squid releases.


<SNIP>

Amos Jeffries



From squid3 at treenet.co.nz  Fri Jun 16 12:59:51 2017
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sat, 17 Jun 2017 00:59:51 +1200
Subject: [squid-users] RV: squid
In-Reply-To: <003601d2e697$c2e58e80$48b0ab80$@accelya.com>
References: <01ff01d2e5c5$840788d0$8c169a70$@accelya.com>
 <2686958.ZNAhSqxdWU@pikantusdevuan> <20170615125049.GA5653@fantomas.sk>
 <027301d2e5e3$b9eadec0$2dc09c40$@accelya.com>
 <20170615155502.GA14831@fantomas.sk>
 <003501d2e66a$8026e440$8074acc0$@accelya.com>
 <ce5f6f79-34bb-81a8-a68a-fb3583a77907@treenet.co.nz>
 <003601d2e697$c2e58e80$48b0ab80$@accelya.com>
Message-ID: <1c106ac8-de18-6c06-f524-f0a93de8280a@treenet.co.nz>

On 16/06/17 23:57, javier perez wrote:
> They could open just a range of 5 dinamic ports and monitor them
> intensively...

I take it by "they" you mean the passive attacker? the server may open 
any of (2^N) * (2^15) ports, where N is the number of IPs assigned to 
the server both IPv4 and IPv6. A range of 5 has very miniscule 
probability of success.

My point was that "for security" is bogus. In the end neither mode is 
actually "secure" because the CTRL channel leaks like a seive.

The reasons for choosing one over the other are solely about whether 
your network design and that of all networks your clients traffic goes 
through allow that mode to work properly. NAT and similar things 
existing all over the place nowdays invariably means passive mode is the 
only way to get working FTP connections, so even lazyness is 
self-inflicted pain.


>
>> Hello Matus,
>>
>> You are right, the thing is that our clients are not going to open any
>> other port than 20 and 21 for security meassures (or lazyness).
> FYI: The "for security" argument is bogus because;
>
> a)  allowing any random client to determine their own arbitrary port
> number(s) is strictly worse for security than having your control point
> (Squid) select the port, and
>
> b) limiting that client-selected port to 20/21 makes the data between client
> and Squid go over a port which is more easily predicted and therefore
> interceptable by passive attack.
>
> Amos
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users



From rousskov at measurement-factory.com  Fri Jun 16 15:51:56 2017
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Fri, 16 Jun 2017 09:51:56 -0600
Subject: [squid-users] RV: squid
In-Reply-To: <20170616122259.GA6571@fantomas.sk>
References: <01ff01d2e5c5$840788d0$8c169a70$@accelya.com>
 <2686958.ZNAhSqxdWU@pikantusdevuan> <20170615125049.GA5653@fantomas.sk>
 <027301d2e5e3$b9eadec0$2dc09c40$@accelya.com>
 <20170615155502.GA14831@fantomas.sk>
 <ea0958e0-70ff-75fb-be2c-4b9444dbe6a9@measurement-factory.com>
 <20170616104037.GA5851@fantomas.sk>
 <2502fce8-6a7a-9f4e-d1da-84bdb7d8c9ad@treenet.co.nz>
 <20170616122259.GA6571@fantomas.sk>
Message-ID: <5360d84e-5756-9685-f431-9375bbe03774@measurement-factory.com>

On 06/16/2017 06:22 AM, Matus UHLAR - fantomas wrote:
>>> Alex Rousskov wrote:
>>>> AFAIK, there are currently no plans (or even strong demand) to support
>>>> active FTP mode between Squid and FTP origin servers.

>> On 16/06/17 22:40, Matus UHLAR - fantomas wrote:
>>> what is ftp_passive for then?

> On 16.06.17 23:49, Amos Jeffries wrote:
>> For controlling how Squid gateways  "GET ftp://example.com/ HTTP/1.1"
>> requests to an FTP server. Whether it attempts PASV / EPSV mode
>> commands at all, or skips straight to the fallback "active" PORT/EPRT
>> commands.

> so, where exactly does squid NOT support active mode, when it does support
> it on client's side and on the server side when gatewaying FTP commands?

It is easy to confuse the ftp_passive intent (what Amos have described
and what Squid once supported) with the actual implementation in modern
Squids: The ftp_passive option can be used to enable buggy code that
does not work.

IIRC, when adding FTP relaying, after wasting a lot of time looking for
bugs in our new code, we realized that active Squid-origin mode has been
badly broken for a while. As Amos has said, quality patches fixing that
old bug are welcomed.

Alex.


From codemarauder at gmail.com  Sat Jun 17 03:39:32 2017
From: codemarauder at gmail.com (Nishant Sharma)
Date: Sat, 17 Jun 2017 09:09:32 +0530
Subject: [squid-users] Introducing Charcoal - Centralised URL Filter
	for	squid
In-Reply-To: <003601d2e534$dd7729c0$98657d40$@ngtech.co.il>
References: <452c9346-331e-e9ee-7f5d-65c330b5db16@gmail.com>
 <003601d2e534$dd7729c0$98657d40$@ngtech.co.il>
Message-ID: <AE76D0CE-CBBE-4D68-9C3C-316C0697A4EC@gmail.com>



Hi Eliezer,

On 14 June 2017 11:07:16 PM IST, Eliezer  Croitoru <eliezer at ngtech.co.il> wrote:

>I want to offer you a more advanced helper that supports actual
>concurrency compared to the current perl helper on github,
>which understands the protocol but do not use threads or any other
>method of concurrency.
>
>Let me know if it's of any interest for you.
>The skeleton is at:
>http://wiki.squid-cache.org/EliezerCroitoru/GolangFakeHelper

Thanks a lot for the offer. It surely is interesting.

The current state of helper is due to the fact that it was written for embedded/low powered devices running Linux. OpenWrt doesn't cross-compile Go as of now, so we had to go for Perl. It is good enough for low request proxies at small offices.

We are modifying it as per recommendations by Amos and will check-in the updated code soon.

>I am willing to take my time and write the code for you. So..

Glad to know about your willingness to write it in Go. It will help the community at large to run it on more powerful machines that serve a lot of requests.

Another version of helper that we are writing will use memcached on local proxy to cache the access granted from the cloud server and will greatly increase the speed.

Regards,
Nishant
-- 
Sent from my Android device with K-9 Mail. Please excuse my brevity.


From eliezer at ngtech.co.il  Sat Jun 17 07:07:18 2017
From: eliezer at ngtech.co.il (Eliezer  Croitoru)
Date: Sat, 17 Jun 2017 10:07:18 +0300
Subject: [squid-users] Introducing Charcoal - Centralised URL Filter
	for	squid
In-Reply-To: <AE76D0CE-CBBE-4D68-9C3C-316C0697A4EC@gmail.com>
References: <452c9346-331e-e9ee-7f5d-65c330b5db16@gmail.com>
 <003601d2e534$dd7729c0$98657d40$@ngtech.co.il>
 <AE76D0CE-CBBE-4D68-9C3C-316C0697A4EC@gmail.com>
Message-ID: <000201d2e738$5d5f7ed0$181e7c70$@ngtech.co.il>

I wanted to be sure I am not day-dreaming but from the code it seems that every request is given a single TCP connection.
Am I right?
If so there is much to improve.
You can use the same tcp connection for more then a single request and also have a reconnect option for the very far from realiy case of a closed connection.

All The Bests,
Eliezer

----
Eliezer Croitoru
Linux System Administrator
Mobile: +972-5-28704261
Email: eliezer at ngtech.co.il


-----Original Message-----
From: Nishant Sharma [mailto:codemarauder at gmail.com] 
Sent: Saturday, June 17, 2017 06:40
To: Eliezer Croitoru <eliezer at ngtech.co.il>
Cc: squid-users at lists.squid-cache.org
Subject: RE: [squid-users] Introducing Charcoal - Centralised URL Filter for squid



Hi Eliezer,

On 14 June 2017 11:07:16 PM IST, Eliezer  Croitoru <eliezer at ngtech.co.il> wrote:

>I want to offer you a more advanced helper that supports actual 
>concurrency compared to the current perl helper on github, which 
>understands the protocol but do not use threads or any other method of 
>concurrency.
>
>Let me know if it's of any interest for you.
>The skeleton is at:
>http://wiki.squid-cache.org/EliezerCroitoru/GolangFakeHelper

Thanks a lot for the offer. It surely is interesting.

The current state of helper is due to the fact that it was written for embedded/low powered devices running Linux. OpenWrt doesn't cross-compile Go as of now, so we had to go for Perl. It is good enough for low request proxies at small offices.

We are modifying it as per recommendations by Amos and will check-in the updated code soon.

>I am willing to take my time and write the code for you. So..

Glad to know about your willingness to write it in Go. It will help the community at large to run it on more powerful machines that serve a lot of requests.

Another version of helper that we are writing will use memcached on local proxy to cache the access granted from the cloud server and will greatly increase the speed.

Regards,
Nishant
--
Sent from my Android device with K-9 Mail. Please excuse my brevity.



From squid3 at treenet.co.nz  Sat Jun 17 07:47:41 2017
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sat, 17 Jun 2017 19:47:41 +1200
Subject: [squid-users] Introducing Charcoal - Centralised URL Filter for
 squid
In-Reply-To: <000201d2e738$5d5f7ed0$181e7c70$@ngtech.co.il>
References: <452c9346-331e-e9ee-7f5d-65c330b5db16@gmail.com>
 <003601d2e534$dd7729c0$98657d40$@ngtech.co.il>
 <AE76D0CE-CBBE-4D68-9C3C-316C0697A4EC@gmail.com>
 <000201d2e738$5d5f7ed0$181e7c70$@ngtech.co.il>
Message-ID: <cc73dfa1-8a2f-5b33-fa90-b0a0f3c69941@treenet.co.nz>

On 17/06/17 19:07, Eliezer Croitoru wrote:
> I wanted to be sure I am not day-dreaming but from the code it seems that every request is given a single TCP connection.
> Am I right?
> If so there is much to improve.

You are seeing correct. That is one of the things I brought up and is 
being worked on already. see issue #3 in their tracker.

Amos



From codemarauder at gmail.com  Sat Jun 17 09:59:21 2017
From: codemarauder at gmail.com (Nishant Sharma)
Date: Sat, 17 Jun 2017 15:29:21 +0530
Subject: [squid-users] Introducing Charcoal - Centralised URL Filter for
 squid
In-Reply-To: <000201d2e738$5d5f7ed0$181e7c70$@ngtech.co.il>
References: <452c9346-331e-e9ee-7f5d-65c330b5db16@gmail.com>
 <003601d2e534$dd7729c0$98657d40$@ngtech.co.il>
 <AE76D0CE-CBBE-4D68-9C3C-316C0697A4EC@gmail.com>
 <000201d2e738$5d5f7ed0$181e7c70$@ngtech.co.il>
Message-ID: <810529fc-b922-fa1f-4ee4-37107b99be6e@gmail.com>

Hi Eliezer,

On Saturday 17 June 2017 12:37 PM, Eliezer  Croitoru wrote:
> I wanted to be sure I am not day-dreaming but from the code it seems that every request is given a single TCP connection.
> Am I right?
> If so there is much to improve.
> You can use the same tcp connection for more then a single request and also have a reconnect option for the very far from realiy case of a closed connection.

Your observation is correct.

We have updated the helper with the latest commit 
https://github.com/Hopbox/charcoal-helper/commit/2cd3a0f985c2083046267eee82f6c7df16113113

It tries to address the issues you mentioned, but is not yet ideal. 
Since, it is invoked by squid, number of children started depends on 
squid. Total no. of sockets in states ESTABLISHED & CLOSE_WAIT are equal 
to the number of helper children started by squid.

May be, the helper architecture could be changed such that a parent 
process creates a pool of network connections that children use. Thus, 
limiting the number of sockets being used at any moment. And squid 
controls the number of those parent processes.

Regards,
Nishant


From blason16 at gmail.com  Sat Jun 17 10:07:07 2017
From: blason16 at gmail.com (Blason R)
Date: Sat, 17 Jun 2017 15:37:07 +0530
Subject: [squid-users] Office 365 Support for Squid Proxy
In-Reply-To: <001901d2e5ef$022e2a10$068a7e30$@ngtech.co.il>
References: <CAPPXLT-nXbTHdmWBQPWX-FbA0RWrJsCx54EUqcRTz=57+0_Miw@mail.gmail.com>
 <007701d2e3af$bbefc2c0$33cf4840$@ngtech.co.il>
 <CAPPXLT8enxeOmEPCWKQ9XvGH3x04P1WBDcJ5vfRwgCYNsghUSA@mail.gmail.com>
 <002601d2e5ac$8e64c4f0$ab2e4ed0$@ngtech.co.il>
 <CAPPXLT9ty2qrp3GywDrED-0NF_rFZDegDzF3wPt2mBW6dEzKnQ@mail.gmail.com>
 <001901d2e5ef$022e2a10$068a7e30$@ngtech.co.il>
Message-ID: <CAPPXLT9S0pd+qrQdwL9Xkpvr-WG1VUqxipO3hrDVjUH5otqs=Q@mail.gmail.com>

Agree and yes even I considered that as well. This should work without any
issues. However since I am testing I am looking for ready-made ACLs for
office 365  and wanted to check every applications i.e. Skype, Voice
Calling and other functionalities.

On Thu, Jun 15, 2017 at 9:19 PM, Eliezer Croitoru <eliezer at ngtech.co.il>
wrote:

> Well I do not need to test it specifically since it's the most basic
> function of squid as forward proxy.
> It tunnels HTTPS connections as a TCP connection.
> Every instance of squid since 1.X did it in a very good way.
> There shouldn't be any issues since squid only allows or disallows the
> connection and splice the traffic between the clients to the destination
> host.
> If you would have used SSL-BUMP you would be required to run some tests to
> make sure there are no new and special things that MS invented and pushed
> into their HTTPs stack.
>
> All The Bests,
> Eliezer
>
> ----
> http://ngtech.co.il/lmgtfy/
> Linux System Administrator
> Mobile: +972-5-28704261
> Email: eliezer at ngtech.co.il
>
>
> From: Blason R [mailto:blason16 at gmail.com]
> Sent: Thursday, June 15, 2017 2:45 PM
> To: Eliezer Croitoru <eliezer at ngtech.co.il>
> Cc: squid-users at lists.squid-cache.org
> Subject: Re: [squid-users] Office 365 Support for Squid Proxy
>
> So it Would work for all Office  365 applications right? Have you tried
> that before?
> On Thu, Jun 15, 2017, 1:24 PM Eliezer Croitoru <mailto:
> eliezer at ngtech.co.il> wrote:
> In a simple forward proxy mode which enforces acl's it should be pretty
> simple and easy to use.
>
> All The Bests,
> Eliezer
>
> ----
> http://ngtech.co.il/lmgtfy/
> Linux System Administrator
> Mobile: +972-5-28704261
> Email: mailto:eliezer at ngtech.co.il
>
>
> From: Blason R [mailto:mailto:blason16 at gmail.com]
> Sent: Thursday, June 15, 2017 6:20 AM
> To: Eliezer Croitoru <mailto:eliezer at ngtech.co.il>
> Cc: mailto:squid-users at lists.squid-cache.org
> Subject: Re: [squid-users] Office 365 Support for Squid Proxy
>
> I am thinking to put it in forwarding only mode. And being a office 365 I
> don't see any reason for ssl-bump since I do have other device for handling
> web traffic.
> On Tue, Jun 13, 2017, 12:41 AM Eliezer Croitoru <mailto:mailto:
> eliezer at ngtech.co.il> wrote:
> The main question is if it uses websockets or not and if you are using
> SSL-BUMP or not.
> If you are using SSL-BUMP it's one thing while if you are not it?s another
> story.
> Also it will be different if you are using the proxy in INTERCEPT mode or
> a regular forward proxy mode.
> We would be able to answer you more with more details on your setup.
>
> Eliezer
>
> ----
> http://ngtech.co.il/lmgtfy/
> Linux System Administrator
> Mobile: +972-5-28704261
> Email: mailto:mailto:eliezer at ngtech.co.il
>
>
> From: squid-users [mailto:mailto:mailto:mailto:squid-users-bounces at lists.
> squid-cache.org] On Behalf Of Blason R
> Sent: Monday, June 12, 2017 12:05 PM
> To: mailto:mailto:squid-users at lists.squid-cache.org
> Subject: [squid-users] Office 365 Support for Squid Proxy
>
> Hello All,
>
> If someone can confirm if squid can very well work with Office 365? If
> anyone has any documentation can someone please forward that to me? I do
> have almost around 400 Office 365 users hence wanted to know what
> configuration I might need for Office 365 traffic?
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20170617/cdd7cd95/attachment.htm>

From squid3 at treenet.co.nz  Sat Jun 17 12:12:09 2017
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sun, 18 Jun 2017 00:12:09 +1200
Subject: [squid-users] Office 365 Support for Squid Proxy
In-Reply-To: <CAPPXLT9S0pd+qrQdwL9Xkpvr-WG1VUqxipO3hrDVjUH5otqs=Q@mail.gmail.com>
References: <CAPPXLT-nXbTHdmWBQPWX-FbA0RWrJsCx54EUqcRTz=57+0_Miw@mail.gmail.com>
 <007701d2e3af$bbefc2c0$33cf4840$@ngtech.co.il>
 <CAPPXLT8enxeOmEPCWKQ9XvGH3x04P1WBDcJ5vfRwgCYNsghUSA@mail.gmail.com>
 <002601d2e5ac$8e64c4f0$ab2e4ed0$@ngtech.co.il>
 <CAPPXLT9ty2qrp3GywDrED-0NF_rFZDegDzF3wPt2mBW6dEzKnQ@mail.gmail.com>
 <001901d2e5ef$022e2a10$068a7e30$@ngtech.co.il>
 <CAPPXLT9S0pd+qrQdwL9Xkpvr-WG1VUqxipO3hrDVjUH5otqs=Q@mail.gmail.com>
Message-ID: <062f205d-1045-083a-cd0c-bce7a164a0e4@treenet.co.nz>

On 17/06/17 22:07, Blason R wrote:
> Agree and yes even I considered that as well. This should work without 
> any issues. However since I am testing I am looking for ready-made 
> ACLs for office 365  and wanted to check every applications i.e. 
> Skype, Voice Calling and other functionalities.

It helps a lot to mention minor details like what you actually want in 
your initial message. Then you do not end up with days of waiting 5 days 
for answers to the "does it work? yes/no" stage for every piece of 
software in existence.

The answer to all of the above is that Squid is an HTTP proxy. If the 
application uses HTTP it almost certainly works with Squid in one way or 
another.

Now you can move on to the actually important things like looking up 
whether those applications use HTTP or not (VoIP almost certainly does 
not; Skype is a mixed situation with some HTTP some non-HTTP), and 
testing the ones that do HTTP. Note that squid.conf needs to match 
*your* chosen network policy, not someone elses' policy.

Amos



From ahmed.zaeem at netstream.ps  Sat Jun 17 13:11:26 2017
From: ahmed.zaeem at netstream.ps (--Ahmad--)
Date: Sat, 17 Jun 2017 16:11:26 +0300
Subject: [squid-users] rotated squid with multi instance with non default
	config file
Message-ID: <DB8F8745-CC1F-4968-9367-0A55AAF4E8E7@netstream.ps>

Hi folks 
i have squid 3.5.24

i run it like :

squid -n 90 -f /etc/squid/xsquid.conf 


but i need to reload squid not kill then start it .


i cant use the cmd
squid -n 90 -f /etc/squid/xsquid.conf  -k rec


any way to do that ?

cheers

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20170617/f14e5f85/attachment.htm>

From rousskov at measurement-factory.com  Sat Jun 17 15:56:10 2017
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Sat, 17 Jun 2017 09:56:10 -0600
Subject: [squid-users] rotated squid with multi instance with non
 default config file
In-Reply-To: <DB8F8745-CC1F-4968-9367-0A55AAF4E8E7@netstream.ps>
References: <DB8F8745-CC1F-4968-9367-0A55AAF4E8E7@netstream.ps>
Message-ID: <7a5bbfee-abcd-e2b9-0337-6c3a6c5cdd56@measurement-factory.com>

On 06/17/2017 07:11 AM, --Ahmad-- wrote:
> Hi folks 
> i have squid 3.5.24
> 
> i run it like :
> 
> squid -n 90 -f /etc/squid/xsquid.conf 
> 
> 
> but i need to reload squid not kill then start it .
> 
> 
> i cant use the cmd
> squid -n 90 -f /etc/squid/xsquid.conf  -k rec
> 
> 
> any way to do that ?


The following command should trigger Squid instance reconfiguration:

  squid -n 90 -f /etc/squid/xsquid.conf -k reconfigure

If it does not, then please double check that there is a valid Squid PID
file and, if there is, open a bug report that details what happens when
you use that command.

Please note that your email subject says "rotated" while your email body
says "reload" and "rec". You may want to clarify whether you want to
reconfigure Squid or rotate its logs. There is "-k rotate" for the latter.


HTH,

Alex.


From meym at nym.mixmin.net  Sat Jun 17 16:09:08 2017
From: meym at nym.mixmin.net (meym)
Date: Sat, 17 Jun 2017 17:09:08 +0100 (BST)
Subject: [squid-users] squid 4.0.20 does not recognize ssl-bump option.
Message-ID: <20170617160908.E2D711200DE@fleegle.mixmin.net>

Squid Cache: Version 4.0.20
"FATAL: Unknown http_port option 'ssl-bump'."

ssl-bump mode still exist, according to documentation for the Squid
configuration file.

-- 
0x9E688B76E4064CD0



From rousskov at measurement-factory.com  Sat Jun 17 16:31:52 2017
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Sat, 17 Jun 2017 10:31:52 -0600
Subject: [squid-users] squid 4.0.20 does not recognize ssl-bump option.
In-Reply-To: <20170617160908.E2D711200DE@fleegle.mixmin.net>
References: <20170617160908.E2D711200DE@fleegle.mixmin.net>
Message-ID: <5955ce99-f680-5462-da56-2ef830ac1f1a@measurement-factory.com>

On 06/17/2017 10:09 AM, meym wrote:
> Squid Cache: Version 4.0.20
> "FATAL: Unknown http_port option 'ssl-bump'."

Your Squid thinks it was built without OpenSSL support. OpenSSL support
is required for SslBump. Examine your ./configure options and output.


> ssl-bump mode still exist, according to documentation for the Squid
> configuration file.

Unfortunately, squid.conf.documented generator is not flexible enough to
exclude portions of a supported directive based on build options. It can
only exclude whole directives. For example, you should see not ssl_bump
directive in your squid.conf.documented.

HTH,

Alex.




From squid3 at treenet.co.nz  Sat Jun 17 17:47:38 2017
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sun, 18 Jun 2017 05:47:38 +1200
Subject: [squid-users] Introducing Charcoal - Centralised URL Filter for
 squid
In-Reply-To: <810529fc-b922-fa1f-4ee4-37107b99be6e@gmail.com>
References: <452c9346-331e-e9ee-7f5d-65c330b5db16@gmail.com>
 <003601d2e534$dd7729c0$98657d40$@ngtech.co.il>
 <AE76D0CE-CBBE-4D68-9C3C-316C0697A4EC@gmail.com>
 <000201d2e738$5d5f7ed0$181e7c70$@ngtech.co.il>
 <810529fc-b922-fa1f-4ee4-37107b99be6e@gmail.com>
Message-ID: <86653f1f-ac87-1444-5dd2-72cf8804a452@treenet.co.nz>

On 17/06/17 21:59, Nishant Sharma wrote:
> May be, the helper architecture could be changed such that a parent 
> process creates a pool of network connections that children use. Thus, 
> limiting the number of sockets being used at any moment. And squid 
> controls the number of those parent processes.

That would mean making Squid aware of the internal workings of the 
helper. Namely that it uses connections to a specific server, port and 
which transport. One of the major points of flexibility with helpers is 
that this kind of thing is kept completely separate from Squid.

The URL-rewrite API being used by charcoal has the purpose of altering 
the URI which Squid fetches content for a client from. Doing access 
control through it instead of the access control API (external ACL 
helper) is kind of borked from the start.

Amos



From codemarauder at gmail.com  Sat Jun 17 18:30:08 2017
From: codemarauder at gmail.com (Nishant Sharma)
Date: Sun, 18 Jun 2017 00:00:08 +0530
Subject: [squid-users] Introducing Charcoal - Centralised URL Filter for
	squid
In-Reply-To: <86653f1f-ac87-1444-5dd2-72cf8804a452@treenet.co.nz>
References: <452c9346-331e-e9ee-7f5d-65c330b5db16@gmail.com>
 <003601d2e534$dd7729c0$98657d40$@ngtech.co.il>
 <AE76D0CE-CBBE-4D68-9C3C-316C0697A4EC@gmail.com>
 <000201d2e738$5d5f7ed0$181e7c70$@ngtech.co.il>
 <810529fc-b922-fa1f-4ee4-37107b99be6e@gmail.com>
 <86653f1f-ac87-1444-5dd2-72cf8804a452@treenet.co.nz>
Message-ID: <2CCBCBC1-2216-4BA8-9B6C-6759AA4D0BAD@gmail.com>



On 17 June 2017 11:17:38 PM IST, Amos Jeffries <squid3 at treenet.co.nz> wrote:

>That would mean making Squid aware of the internal workings of the 
>helper. Namely that it uses connections to a specific server, port and 
>which transport. One of the major points of flexibility with helpers is
>
>that this kind of thing is kept completely separate from Squid.

Re-reading my mail made me realise that it conveyed that helper architecture of squid be modified,  instead I wanted to say that we can modify the architecture of our helper, where it internally manages its own children which may speed-up the URL rewrite process.

>The URL-rewrite API being used by charcoal has the purpose of altering 
>the URI which Squid fetches content for a client from. Doing access 
>control through it instead of the access control API (external ACL 
>helper) is kind of borked from the start.

I agree, external ACL helper will also allow to have access to  additional information like user-agent, reply content-type etc. to have more granular control.

Regards,
Nishant
-- 
Sent from my Android device with K-9 Mail. Please excuse my brevity.


From eliezer at ngtech.co.il  Sat Jun 17 20:42:35 2017
From: eliezer at ngtech.co.il (Eliezer  Croitoru)
Date: Sat, 17 Jun 2017 23:42:35 +0300
Subject: [squid-users] Introducing Charcoal - Centralised URL Filter
	for	squid
In-Reply-To: <2CCBCBC1-2216-4BA8-9B6C-6759AA4D0BAD@gmail.com>
References: <452c9346-331e-e9ee-7f5d-65c330b5db16@gmail.com>
 <003601d2e534$dd7729c0$98657d40$@ngtech.co.il>
 <AE76D0CE-CBBE-4D68-9C3C-316C0697A4EC@gmail.com>
 <000201d2e738$5d5f7ed0$181e7c70$@ngtech.co.il>
 <810529fc-b922-fa1f-4ee4-37107b99be6e@gmail.com>
 <86653f1f-ac87-1444-5dd2-72cf8804a452@treenet.co.nz>
 <2CCBCBC1-2216-4BA8-9B6C-6759AA4D0BAD@gmail.com>
Message-ID: <007d01d2e7aa$40369060$c0a3b120$@ngtech.co.il>

Hey Nishant,

Responding to your idea and the whole concept of the helper and also comparing to GoLang binaries.

About the software design to run on-top of embedded hardware and a GoLang binary:
A GoLang helper can be compiled to almost any modern embedded device(else them mips based)
Also its more efficient then any software you will write with perl.
The only "limit" is CPU comparability and Binary size vs device free space.(in any case a perl helper would use more then a GoLang one).

The idea of writing a software that will implement concurrency in perl or python is nice and noble but I believe that probably
for small embedded devices you won't need a "robust" helper that supports concurrency or any other more complex solutions.

I believe that you should aim for the more standard hardware devices which squid can be built on-top such as:
- x86
- x86_64
- arm64
- arm5
- arm8

The above will benefit from a good and robust helper which supports concurrency.
Now that it's clear that your socket can handle more then only one request I will write a helper in GoLang that works with:
- concurrency
- better connection handling(being able to handle responses whenever they received)

I already wrote most of the code so I believe it's a matter of days for the helper to be ready.
Would I be able to receive some testing api key\token once the helper will be ready?

Thanks,
Eliezer

----
Eliezer Croitoru
Linux System Administrator
Mobile: +972-5-28704261
Email: eliezer at ngtech.co.il


-----Original Message-----
From: squid-users [mailto:squid-users-bounces at lists.squid-cache.org] On Behalf Of Nishant Sharma
Sent: Saturday, June 17, 2017 21:30
To: squid-users at lists.squid-cache.org
Subject: Re: [squid-users] Introducing Charcoal - Centralised URL Filter for squid



On 17 June 2017 11:17:38 PM IST, Amos Jeffries <squid3 at treenet.co.nz> wrote:

>That would mean making Squid aware of the internal workings of the 
>helper. Namely that it uses connections to a specific server, port and 
>which transport. One of the major points of flexibility with helpers is
>
>that this kind of thing is kept completely separate from Squid.

Re-reading my mail made me realise that it conveyed that helper architecture of squid be modified,  instead I wanted to say that we can modify the architecture of our helper, where it internally manages its own children which may speed-up the URL rewrite process.

>The URL-rewrite API being used by charcoal has the purpose of altering 
>the URI which Squid fetches content for a client from. Doing access 
>control through it instead of the access control API (external ACL 
>helper) is kind of borked from the start.

I agree, external ACL helper will also allow to have access to  additional information like user-agent, reply content-type etc. to have more granular control.

Regards,
Nishant
-- 
Sent from my Android device with K-9 Mail. Please excuse my brevity.
_______________________________________________
squid-users mailing list
squid-users at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-users



From sonyaroy75 at gmail.com  Sun Jun 18 05:50:02 2017
From: sonyaroy75 at gmail.com (Sonya Roy)
Date: Sun, 18 Jun 2017 11:20:02 +0530
Subject: [squid-users] Squid authentication problem
Message-ID: <CALSaDe1rjgTWRGX8S7Vv-x0KTh65ggONm763A=Mjf_bos4_i3Q@mail.gmail.com>

Hi,

I am running squid on a server with multiple public IPs and I want some
users to be able to access the proxy through some of the IPs and other
users through other IPs.

At the moment I have acl rules of the form:-
acl abcd myip x.x.x.x

and for these acl rules I have these tcp_outgoing_address:-
tcp_outgoing_address x.x.x.x abcd

And earlier I had proxy_auth acl rules separately, but that allowed any
authenticated users to be able to access the proxy through any of those
IPs. Since I wanted some users to be able to use the server through some
IPs and others through different IPs, I tried this in those acl rules:-

acl abcd myip x.x.x.x proxy_auth user1

and so on. But this doesn't seem to work and I realized that the proxy_auth
acl rules need to be separate than these. Is there any workaround for this?

With regards,
Sonya Roy
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20170618/e8233bdd/attachment.htm>

From squid3 at treenet.co.nz  Sun Jun 18 12:56:31 2017
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Mon, 19 Jun 2017 00:56:31 +1200
Subject: [squid-users] Squid authentication problem
In-Reply-To: <CALSaDe1rjgTWRGX8S7Vv-x0KTh65ggONm763A=Mjf_bos4_i3Q@mail.gmail.com>
References: <CALSaDe1rjgTWRGX8S7Vv-x0KTh65ggONm763A=Mjf_bos4_i3Q@mail.gmail.com>
Message-ID: <343154ec-dbd6-aa55-f867-216d3c261423@treenet.co.nz>


On 18/06/17 17:50, Sonya Roy wrote:
> Hi,
>
> I am running squid on a server with multiple public IPs and I want 
> some users to be able to access the proxy through some of the IPs and 
> other users through other IPs.
>
> At the moment I have acl rules of the form:-
> acl abcd myip x.x.x.x
>

What you need is an ACL that compares the username to the IP.

<http://www.squid-cache.org/Versions/v3/3.5/manuals/ext_file_userip_acl.html>
<http://www.squid-cache.org/Versions/v3/3.5/manuals/ext_edirectory_userip_acl.html>
<http://www.squid-cache.org/Versions/v3/3.5/manuals/ext_sql_session_acl.html>

or the new 'extras' feature for authenticators in Squid-3.5 that lets 
them use the IP as part of the auth approval. Though with this the thing 
to be aware of is that the IP becomes like a scope for the user login - 
the wrong IP being used to login from results in re-auth challenge just 
as would be seen if the password was wrong. So use carefully.
  <http://www.squid-cache.org/Doc/config/auth_param/>
  <http://www.squid-cache.org/Versions/v3/3.5/RELEASENOTES.html#ss2.2>

> and for these acl rules I have these tcp_outgoing_address:-
> tcp_outgoing_address x.x.x.x abcd
>

Why limit the outgoing? in HTTP that is independent to the incoming 
connection and restricting it will lower performance.

> And earlier I had proxy_auth acl rules separately, but that allowed 
> any authenticated users to be able to access the proxy through any of 
> those IPs. Since I wanted some users to be able to use the server 
> through some IPs and others through different IPs, I tried this in 
> those acl rules:-
>
> acl abcd myip x.x.x.x proxy_auth user1

FTR: that will match the IP address x.x.x.x and the IP address(es) of 
the servers with hostnames "proxy_auth" and "user1" in your local DNS.

Also, the myip ACL is deprecated because it matched different things 
based on the traffic type. myportname or localip ACLs are better if you 
need to do this at all. Your "squid -k parse" config checks should warn 
you about that.

Amos


From rousskov at measurement-factory.com  Sun Jun 18 22:53:15 2017
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Sun, 18 Jun 2017 16:53:15 -0600
Subject: [squid-users] squid 4.0.20 does not recognize ssl-bump option.
In-Reply-To: <20170618154920.6E38A1200CE@fleegle.mixmin.net>
References: <20170617160908.E2D711200DE@fleegle.mixmin.net>
 <5955ce99-f680-5462-da56-2ef830ac1f1a@measurement-factory.com>
 <20170618154920.6E38A1200CE@fleegle.mixmin.net>
Message-ID: <9e834f7b-b20b-2cb5-e439-3fa0eaf1223e@measurement-factory.com>

On 06/18/2017 09:49 AM, meym wrote:
>> On 06/17/2017 10:09 AM, meym wrote:
>>> Squid Cache: Version 4.0.20
>>> "FATAL: Unknown http_port option 'ssl-bump'."
>>
>> Your Squid thinks it was built without OpenSSL support. OpenSSL support
>> is required for SslBump. Examine your ./configure options and output.

> With libressl actually.

I do not know what you mean by that remark exactly, but what I said
applies to any library providing OpenSSL API, including LibreSSL. Moreover:

* Squid does not know anything about LibreSSL. Somebody added the
letters "LibreSSL" to squid.conf.documented, but that was a mistake IMO.

* Primary SslBump developers do not normally use or test with LibreSSL.

* LibreSSL provides OpenSSL API so you can tell Squid to use LibreSSL as
if it was OpenSSL, and things should work as well as with OpenSSL itself
if (and only if) LibreSSL does a good job providing that OpenSSL API.

* LibreSSL does not do a good job providing OpenSSL API and/or Squid
does not do a good job detecting OpenSSL API variations in a
LibreSSL-compatible way (depending on your point of view). See bug #4662
for more details.

There have been recent improvements in LibreSSL-compatibility area, but
I am not sure those improvements (or the problems) are in your Squid
version and, at any rate, are taking significant additional risks by
using LibreSSL with SslBump. Whether those risks are worth using
something other than OpenSSL is your call, of course.

Alex.


From squid3 at treenet.co.nz  Mon Jun 19 09:12:57 2017
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Mon, 19 Jun 2017 21:12:57 +1200
Subject: [squid-users] squid 4.0.20 does not recognize ssl-bump option.
In-Reply-To: <9e834f7b-b20b-2cb5-e439-3fa0eaf1223e@measurement-factory.com>
References: <20170617160908.E2D711200DE@fleegle.mixmin.net>
 <5955ce99-f680-5462-da56-2ef830ac1f1a@measurement-factory.com>
 <20170618154920.6E38A1200CE@fleegle.mixmin.net>
 <9e834f7b-b20b-2cb5-e439-3fa0eaf1223e@measurement-factory.com>
Message-ID: <999533d5-0efa-8310-d32d-0ac0a10f34cd@treenet.co.nz>

On 19/06/17 10:53, Alex Rousskov wrote:
> On 06/18/2017 09:49 AM, meym wrote:
>>> On 06/17/2017 10:09 AM, meym wrote:
>>>> Squid Cache: Version 4.0.20
>>>> "FATAL: Unknown http_port option 'ssl-bump'."
>>>
>>> Your Squid thinks it was built without OpenSSL support. OpenSSL support
>>> is required for SslBump. Examine your ./configure options and output.
>
>> With libressl actually.
>
> I do not know what you mean by that remark exactly, but what I said
> applies to any library providing OpenSSL API, including LibreSSL.

To clarify that. This Squid is missing the --with-openssl build option, 
which is required both by OpenSSL and any library derived from it.

see "squid -v" for the details of a specific squid binary. This will now 
distinguish between the OpenSSL vs LibreSSL vs other situation.


> Moreover:
>
> * Squid does not know anything about LibreSSL. Somebody added the
> letters "LibreSSL" to squid.conf.documented, but that was a mistake IMO.

The mentions of LibreSSL in the current file are for things which were 
tested before the recent round of LibreSSL issues. Specifically loading 
CA certs from a file. AFAIK that should still be working.

ssl-bump is correctly not one of those options mentioning it. Also, note 
that the fatal error message does not mention any particular library. It 
is about lack of support from *any* library in the current build.

>
> * Primary SslBump developers do not normally use or test with LibreSSL.
>
> * LibreSSL provides OpenSSL API so you can tell Squid to use LibreSSL as
> if it was OpenSSL, and things should work as well as with OpenSSL itself
> if (and only if) LibreSSL does a good job providing that OpenSSL API.
>
> * LibreSSL does not do a good job providing OpenSSL API and/or Squid
> does not do a good job detecting OpenSSL API variations in a
> LibreSSL-compatible way (depending on your point of view). See bug #4662
> for more details.
>
> There have been recent improvements in LibreSSL-compatibility area, but
> I am not sure those improvements (or the problems) are in your Squid
> version and,

They are. Though the release notes still say "This release does not 
support LibreSSL" at present since we have had no positive feedback on 
anything actually working yet.


> at any rate, are taking significant additional risks by
> using LibreSSL with SslBump. Whether those risks are worth using
> something other than OpenSSL is your call, of course.
>

Since the risk here is due to lack of testing... More testing is very 
welcome of course. Especially with feedback about what works and what 
does not.

Amos


From sonyaroy75 at gmail.com  Mon Jun 19 12:09:27 2017
From: sonyaroy75 at gmail.com (Sonya Roy)
Date: Mon, 19 Jun 2017 17:39:27 +0530
Subject: [squid-users] Squid authentication problem (Amos Jeffries)
Message-ID: <CALSaDe14oG=szaNhVWqii14XxY0wS6LdaguuoAza_r8obGKddQ@mail.gmail.com>

Hi,

>From what I saw with using IP as part of then authentication, it checks
which IP the user is connecting to the server from. What I want to check is
which public IP of the server the user is connecting to.

If someone connects to the server's IP address x.x.x.x, I want the outgoing
traffic to go through the same IP address x.x.x.x. That's why I put an acl
rule for each public IP of the server and specified the
tcp_outgoing_address for each of them.

So, for example, if the server has say 50 public IP address, I want to
create an user who will be able to connect to 25 of them and another to
another 25.

I hope this clarifies my original question.

With regards,
Sonya Roy.

On Mon, Jun 19, 2017 at 5:30 PM, <squid-users-request at lists.squid-cache.org>
wrote:

> Send squid-users mailing list submissions to
>         squid-users at lists.squid-cache.org
>
> To subscribe or unsubscribe via the World Wide Web, visit
>         http://lists.squid-cache.org/listinfo/squid-users
> or, via email, send a message with subject or body 'help' to
>         squid-users-request at lists.squid-cache.org
>
> You can reach the person managing the list at
>         squid-users-owner at lists.squid-cache.org
>
> When replying, please edit your Subject line so it is more specific
> than "Re: Contents of squid-users digest..."
>
>
> Today's Topics:
>
>    1. Re: Squid authentication problem (Amos Jeffries)
>    2. Re: squid 4.0.20 does not recognize ssl-bump option.
>       (Alex Rousskov)
>    3. Re: squid 4.0.20 does not recognize ssl-bump option.
>       (Amos Jeffries)
>
>
> ----------------------------------------------------------------------
>
> Message: 1
> Date: Mon, 19 Jun 2017 00:56:31 +1200
> From: Amos Jeffries <squid3 at treenet.co.nz>
> To: squid-users at lists.squid-cache.org
> Subject: Re: [squid-users] Squid authentication problem
> Message-ID: <343154ec-dbd6-aa55-f867-216d3c261423 at treenet.co.nz>
> Content-Type: text/plain; charset=utf-8; format=flowed
>
>
> On 18/06/17 17:50, Sonya Roy wrote:
> > Hi,
> >
> > I am running squid on a server with multiple public IPs and I want
> > some users to be able to access the proxy through some of the IPs and
> > other users through other IPs.
> >
> > At the moment I have acl rules of the form:-
> > acl abcd myip x.x.x.x
> >
>
> What you need is an ACL that compares the username to the IP.
>
> <http://www.squid-cache.org/Versions/v3/3.5/manuals/ext_
> file_userip_acl.html>
> <http://www.squid-cache.org/Versions/v3/3.5/manuals/ext_
> edirectory_userip_acl.html>
> <http://www.squid-cache.org/Versions/v3/3.5/manuals/ext_
> sql_session_acl.html>
>
> or the new 'extras' feature for authenticators in Squid-3.5 that lets
> them use the IP as part of the auth approval. Though with this the thing
> to be aware of is that the IP becomes like a scope for the user login -
> the wrong IP being used to login from results in re-auth challenge just
> as would be seen if the password was wrong. So use carefully.
>   <http://www.squid-cache.org/Doc/config/auth_param/>
>   <http://www.squid-cache.org/Versions/v3/3.5/RELEASENOTES.html#ss2.2>
>
> > and for these acl rules I have these tcp_outgoing_address:-
> > tcp_outgoing_address x.x.x.x abcd
> >
>
> Why limit the outgoing? in HTTP that is independent to the incoming
> connection and restricting it will lower performance.
>
> > And earlier I had proxy_auth acl rules separately, but that allowed
> > any authenticated users to be able to access the proxy through any of
> > those IPs. Since I wanted some users to be able to use the server
> > through some IPs and others through different IPs, I tried this in
> > those acl rules:-
> >
> > acl abcd myip x.x.x.x proxy_auth user1
>
> FTR: that will match the IP address x.x.x.x and the IP address(es) of
> the servers with hostnames "proxy_auth" and "user1" in your local DNS.
>
> Also, the myip ACL is deprecated because it matched different things
> based on the traffic type. myportname or localip ACLs are better if you
> need to do this at all. Your "squid -k parse" config checks should warn
> you about that.
>
> Amos
>
>
> ------------------------------
>
> Message: 2
> Date: Sun, 18 Jun 2017 16:53:15 -0600
> From: Alex Rousskov <rousskov at measurement-factory.com>
> To: meym <meym at nym.mixmin.net>, Squid Users
>         <squid-users at lists.squid-cache.org>
> Subject: Re: [squid-users] squid 4.0.20 does not recognize ssl-bump
>         option.
> Message-ID:
>         <9e834f7b-b20b-2cb5-e439-3fa0eaf1223e at measurement-factory.com>
> Content-Type: text/plain; charset=koi8-r
>
> On 06/18/2017 09:49 AM, meym wrote:
> >> On 06/17/2017 10:09 AM, meym wrote:
> >>> Squid Cache: Version 4.0.20
> >>> "FATAL: Unknown http_port option 'ssl-bump'."
> >>
> >> Your Squid thinks it was built without OpenSSL support. OpenSSL support
> >> is required for SslBump. Examine your ./configure options and output.
>
> > With libressl actually.
>
> I do not know what you mean by that remark exactly, but what I said
> applies to any library providing OpenSSL API, including LibreSSL. Moreover:
>
> * Squid does not know anything about LibreSSL. Somebody added the
> letters "LibreSSL" to squid.conf.documented, but that was a mistake IMO.
>
> * Primary SslBump developers do not normally use or test with LibreSSL.
>
> * LibreSSL provides OpenSSL API so you can tell Squid to use LibreSSL as
> if it was OpenSSL, and things should work as well as with OpenSSL itself
> if (and only if) LibreSSL does a good job providing that OpenSSL API.
>
> * LibreSSL does not do a good job providing OpenSSL API and/or Squid
> does not do a good job detecting OpenSSL API variations in a
> LibreSSL-compatible way (depending on your point of view). See bug #4662
> for more details.
>
> There have been recent improvements in LibreSSL-compatibility area, but
> I am not sure those improvements (or the problems) are in your Squid
> version and, at any rate, are taking significant additional risks by
> using LibreSSL with SslBump. Whether those risks are worth using
> something other than OpenSSL is your call, of course.
>
> Alex.
>
>
> ------------------------------
>
> Message: 3
> Date: Mon, 19 Jun 2017 21:12:57 +1200
> From: Amos Jeffries <squid3 at treenet.co.nz>
> To: squid-users at lists.squid-cache.org
> Subject: Re: [squid-users] squid 4.0.20 does not recognize ssl-bump
>         option.
> Message-ID: <999533d5-0efa-8310-d32d-0ac0a10f34cd at treenet.co.nz>
> Content-Type: text/plain; charset=utf-8; format=flowed
>
> On 19/06/17 10:53, Alex Rousskov wrote:
> > On 06/18/2017 09:49 AM, meym wrote:
> >>> On 06/17/2017 10:09 AM, meym wrote:
> >>>> Squid Cache: Version 4.0.20
> >>>> "FATAL: Unknown http_port option 'ssl-bump'."
> >>>
> >>> Your Squid thinks it was built without OpenSSL support. OpenSSL support
> >>> is required for SslBump. Examine your ./configure options and output.
> >
> >> With libressl actually.
> >
> > I do not know what you mean by that remark exactly, but what I said
> > applies to any library providing OpenSSL API, including LibreSSL.
>
> To clarify that. This Squid is missing the --with-openssl build option,
> which is required both by OpenSSL and any library derived from it.
>
> see "squid -v" for the details of a specific squid binary. This will now
> distinguish between the OpenSSL vs LibreSSL vs other situation.
>
>
> > Moreover:
> >
> > * Squid does not know anything about LibreSSL. Somebody added the
> > letters "LibreSSL" to squid.conf.documented, but that was a mistake IMO.
>
> The mentions of LibreSSL in the current file are for things which were
> tested before the recent round of LibreSSL issues. Specifically loading
> CA certs from a file. AFAIK that should still be working.
>
> ssl-bump is correctly not one of those options mentioning it. Also, note
> that the fatal error message does not mention any particular library. It
> is about lack of support from *any* library in the current build.
>
> >
> > * Primary SslBump developers do not normally use or test with LibreSSL.
> >
> > * LibreSSL provides OpenSSL API so you can tell Squid to use LibreSSL as
> > if it was OpenSSL, and things should work as well as with OpenSSL itself
> > if (and only if) LibreSSL does a good job providing that OpenSSL API.
> >
> > * LibreSSL does not do a good job providing OpenSSL API and/or Squid
> > does not do a good job detecting OpenSSL API variations in a
> > LibreSSL-compatible way (depending on your point of view). See bug #4662
> > for more details.
> >
> > There have been recent improvements in LibreSSL-compatibility area, but
> > I am not sure those improvements (or the problems) are in your Squid
> > version and,
>
> They are. Though the release notes still say "This release does not
> support LibreSSL" at present since we have had no positive feedback on
> anything actually working yet.
>
>
> > at any rate, are taking significant additional risks by
> > using LibreSSL with SslBump. Whether those risks are worth using
> > something other than OpenSSL is your call, of course.
> >
>
> Since the risk here is due to lack of testing... More testing is very
> welcome of course. Especially with feedback about what works and what
> does not.
>
> Amos
>
>
> ------------------------------
>
> Subject: Digest Footer
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>
>
> ------------------------------
>
> End of squid-users Digest, Vol 34, Issue 46
> *******************************************
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20170619/1e214406/attachment.htm>

From anon.amish at gmail.com  Mon Jun 19 12:16:36 2017
From: anon.amish at gmail.com (Amish)
Date: Mon, 19 Jun 2017 17:46:36 +0530
Subject: [squid-users] Do peek and stare function exact same at step 1? Also
 does dstdom_regex work in ssl_bump?
Message-ID: <811cc17e-8dcf-0372-5c00-579d9e5f5b1a@gmail.com>

Hello,

I was referring to:
http://wiki.squid-cache.org/Features/SslPeekAndSplice#Actions

Based on explanation I wonder if peek and stare are exactly same at step 1?

If yes, which one should I use at step 1? peek or stare?

I am asking because in future their function may change (at step 1).

My intention is to bump as much traffic as can be done. (at step 3)

Currently:
At step 1 I peek most traffic (and splice traffic originating from some IPs)
At step 2 I stare most traffic (and splice exempted domains)
At step 3 everything is bumped.

If peek and stare are same at step 1, I may change peek to stare so that 
it looks consistent.


My 2nd question is:

In the above link it is mentioned under "Configuration Examples" that:
"At no point during ssl_bump processing will dstdomain ACL work. That 
ACL relies on HTTP message details that are not yet decrypted"

Does it hold true for dstdom_regex as well? Because both seem to apply 
to same thing.

Thanks and regards,

Amish.



From squid3 at treenet.co.nz  Mon Jun 19 13:13:00 2017
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Tue, 20 Jun 2017 01:13:00 +1200
Subject: [squid-users] Squid authentication problem (Amos Jeffries)
In-Reply-To: <CALSaDe14oG=szaNhVWqii14XxY0wS6LdaguuoAza_r8obGKddQ@mail.gmail.com>
References: <CALSaDe14oG=szaNhVWqii14XxY0wS6LdaguuoAza_r8obGKddQ@mail.gmail.com>
Message-ID: <460da941-2142-289a-cdec-985e82d42be9@treenet.co.nz>

On 20/06/17 00:09, Sonya Roy wrote:
> Hi,
>
> From what I saw with using IP as part of then authentication, it checks
> which IP the user is connecting to the server from. What I want to check
> is which public IP of the server the user is connecting to.

The IP is whichever one you pass to the various helpers. That is 
configurable.

>
> If someone connects to the server's IP address x.x.x.x, I want the
> outgoing traffic to go through the same IP address x.x.x.x. That's why I
> put an acl rule for each public IP of the server and specified the
> tcp_outgoing_address for each of them.
>
> So, for example, if the server has say 50 public IP address, I want to
> create an user who will be able to connect to 25 of them and another to
> another 25.

That is _what_ you are wanting.

My question was _why_ you wanted to do that?

>
> I hope this clarifies my original question.

Your original question was whether there was any workaround for 
authentication requiring credentials. I believe my previous post 
answered that already.


Cheers
Amos


From sonyaroy75 at gmail.com  Mon Jun 19 15:20:57 2017
From: sonyaroy75 at gmail.com (Sonya Roy)
Date: Mon, 19 Jun 2017 20:50:57 +0530
Subject: [squid-users] Squid authentication problem (Amos Jeffries)
Message-ID: <CALSaDe3hdXHgCnzzsRzOxLcQ0=UZsaNDfLbp3=LR+bD+aMP1-Q@mail.gmail.com>

Since you are saying the IP that can be passed to the helpers is
configurable, how would I pass the local IP of the server that the client
connected to?

I checked out the helpers you mentioned, there they check which IP the
connection is coming from. Not the local IP of the server that the client
is connected to and they are using %SRC for that.

With regards,
Sonya Roy.

On Mon, Jun 19, 2017 at 6:43 PM, Amos Jeffries <squid3 at treenet.co.nz> wrote:

> On 20/06/17 00:09, Sonya Roy wrote:
>
>> Hi,
>>
>> From what I saw with using IP as part of then authentication, it checks
>> which IP the user is connecting to the server from. What I want to check
>> is which public IP of the server the user is connecting to.
>>
>
> The IP is whichever one you pass to the various helpers. That is
> configurable.
>
>
>> If someone connects to the server's IP address x.x.x.x, I want the
>> outgoing traffic to go through the same IP address x.x.x.x. That's why I
>> put an acl rule for each public IP of the server and specified the
>> tcp_outgoing_address for each of them.
>>
>> So, for example, if the server has say 50 public IP address, I want to
>> create an user who will be able to connect to 25 of them and another to
>> another 25.
>>
>
> That is _what_ you are wanting.
>
> My question was _why_ you wanted to do that?
>
>
>> I hope this clarifies my original question.
>>
>
> Your original question was whether there was any workaround for
> authentication requiring credentials. I believe my previous post answered
> that already.
>
>
> Cheers
> Amos
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20170619/da771758/attachment.htm>

From rousskov at measurement-factory.com  Mon Jun 19 17:23:16 2017
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Mon, 19 Jun 2017 11:23:16 -0600
Subject: [squid-users] squid 4.0.20 does not recognize ssl-bump option.
In-Reply-To: <999533d5-0efa-8310-d32d-0ac0a10f34cd@treenet.co.nz>
References: <20170617160908.E2D711200DE@fleegle.mixmin.net>
 <5955ce99-f680-5462-da56-2ef830ac1f1a@measurement-factory.com>
 <20170618154920.6E38A1200CE@fleegle.mixmin.net>
 <9e834f7b-b20b-2cb5-e439-3fa0eaf1223e@measurement-factory.com>
 <999533d5-0efa-8310-d32d-0ac0a10f34cd@treenet.co.nz>
Message-ID: <05248566-7095-527c-843e-e3d23366cd01@measurement-factory.com>

On 06/19/2017 03:12 AM, Amos Jeffries wrote:
> On 19/06/17 10:53, Alex Rousskov wrote:
>> * Squid does not know anything about LibreSSL. Somebody added the
>> letters "LibreSSL" to squid.conf.documented, but that was a mistake IMO.

> The mentions of LibreSSL in the current file are for things which were
> tested before the recent round of LibreSSL issues. Specifically loading
> CA certs from a file. AFAIK that should still be working.

IMO, regardless of whether LibreSSL works for loading CA certs from a
file, it is a mistake for Squid documentation to potentially imply,
however indirectly, that Squid supports LibreSSL today. Besides, I do
not think that loading CA is somehow meaningful in isolation from 100
other actions participating in TLS traffic processing.

It may be possible to meaningfully divide TLS-related code into SslBump
and everything else, but Squid offers proper LibreSSL support for
neither SslBump nor "everything else" IMO.


> the release notes still say "This release does not
> support LibreSSL" at present since we have had no positive feedback on
> anything actually working yet.

Please do not remove that "does not support" disclaimer even if somebody
says that they are using LibreSSL successfully.


>> are taking significant additional risks by
>> using LibreSSL with SslBump. Whether those risks are worth using
>> something other than OpenSSL is your call, of course.

> Since the risk here is due to lack of testing... More testing is very
> welcome of course. Especially with feedback about what works and what
> does not.

I disagree. The Project should not welcome more bug reports about an
unsupported library unless we want to spend our cycles on actually
supporting that library. IMHO, we must spend those cycles on other, more
important/higher priority things.

Alex.


From rousskov at measurement-factory.com  Mon Jun 19 17:41:53 2017
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Mon, 19 Jun 2017 11:41:53 -0600
Subject: [squid-users] Do peek and stare function exact same at step 1?
 Also does dstdom_regex work in ssl_bump?
In-Reply-To: <811cc17e-8dcf-0372-5c00-579d9e5f5b1a@gmail.com>
References: <811cc17e-8dcf-0372-5c00-579d9e5f5b1a@gmail.com>
Message-ID: <aabf5704-dbb2-bfd7-d38e-808b863ac11e@measurement-factory.com>

On 06/19/2017 06:16 AM, Amish wrote:

> I was referring to:
> http://wiki.squid-cache.org/Features/SslPeekAndSplice#Actions
> 
> Based on explanation I wonder if peek and stare are exactly same at step 1?

Both look at the same Client Hello bytes but have at least one different
side effect:

* If you use "peek" during step 1 and Squid cannot decide what you want
to do during step 2, then Squid should splice.

* If you use "stare" during step 1 and Squid cannot decide what you want
to do during step 2, then Squid should bump.

IIRC, there were implementation bugs in the above algorithm but they may
have been fixed since then. As a rule of thumb, always tell Squid what
to do by making sure that at least one applicable ssl_bump rule matches,
regardless of the step.



> If yes, which one should I use at step 1? peek or stare?

* If you intend to splice, use peek.
* If you intend to bump, use stare.
* If you are not yet sure, it is a gray area. Use whatever you think is
best.


> My 2nd question is:
> 
> In the above link it is mentioned under "Configuration Examples" that:
> "At no point during ssl_bump processing will dstdomain ACL work. That
> ACL relies on HTTP message details that are not yet decrypted"

Hm.. AFAICT, that comment is misleading: dstdomain (and dstdomain_regex)
"work" as expected in some SslBump cases, sometimes even during step1.
However, you should use server_name if possible instead because
server_name should work as expected in all SslBump cases. And the latest
Squids (v5 r15189) can be used to fine-tune server_name behavior to
match based on SNI, server certificate, and other critically important
cases.


HTH,

Alex.


From squid3 at treenet.co.nz  Mon Jun 19 21:15:32 2017
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Tue, 20 Jun 2017 09:15:32 +1200
Subject: [squid-users] Squid authentication problem (Amos Jeffries)
In-Reply-To: <CALSaDe3hdXHgCnzzsRzOxLcQ0=UZsaNDfLbp3=LR+bD+aMP1-Q@mail.gmail.com>
References: <CALSaDe3hdXHgCnzzsRzOxLcQ0=UZsaNDfLbp3=LR+bD+aMP1-Q@mail.gmail.com>
Message-ID: <9a36c159-0ec5-4f32-51f1-509b2761c9e3@treenet.co.nz>

On 20/06/17 03:20, Sonya Roy wrote:
> Since you are saying the IP that can be passed to the helpers is
> configurable, how would I pass the local IP of the server that the
> client connected to?
>
> I checked out the helpers you mentioned, there they check which IP the
> connection is coming from. Not the local IP of the server that the
> client is connected to and they are using %SRC for that.

The external ACL helpers don't know one IP from any other. They simply 
check what is given to them against some form of username+ip mapping.


In Squid-3.5 that would be %MYADDR 
<http://ww.squid-cache.org/Versions/v3/3.5/cfgman/external_acl_type.html>.

In Squid-4+ it would be %>la 
<http://ww.squid-cache.org/Versions/v3/3.5/cfgman/logformat.html>

Amos


From squid3 at treenet.co.nz  Mon Jun 19 21:19:18 2017
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Tue, 20 Jun 2017 09:19:18 +1200
Subject: [squid-users] Squid authentication problem (Amos Jeffries)
In-Reply-To: <9a36c159-0ec5-4f32-51f1-509b2761c9e3@treenet.co.nz>
References: <CALSaDe3hdXHgCnzzsRzOxLcQ0=UZsaNDfLbp3=LR+bD+aMP1-Q@mail.gmail.com>
 <9a36c159-0ec5-4f32-51f1-509b2761c9e3@treenet.co.nz>
Message-ID: <7d5ab7a8-0d01-9801-81a4-2776f6b55fd7@treenet.co.nz>

On 20/06/17 09:15, Amos Jeffries wrote:
> On 20/06/17 03:20, Sonya Roy wrote:
>> Since you are saying the IP that can be passed to the helpers is
>> configurable, how would I pass the local IP of the server that the
>> client connected to?
>>
>> I checked out the helpers you mentioned, there they check which IP the
>> connection is coming from. Not the local IP of the server that the
>> client is connected to and they are using %SRC for that.
>
> The external ACL helpers don't know one IP from any other. They simply
> check what is given to them against some form of username+ip mapping.
>

[ with the correct links ]
>
> In Squid-3.5 that would be %MYADDR
> <http://www.squid-cache.org/Versions/v3/3.5/cfgman/external_acl_type.html>.
>
> In Squid-4+ it would be %>la
> <http://www.squid-cache.org/Versions/v3/3.5/cfgman/logformat.html>
>

Amos


From sonyaroy75 at gmail.com  Mon Jun 19 22:50:49 2017
From: sonyaroy75 at gmail.com (Sonya Roy)
Date: Tue, 20 Jun 2017 04:20:49 +0530
Subject: [squid-users] Squid authentication problem (Amos Jeffries)
Message-ID: <CALSaDe0nN=fEx-xSvP+Ayc=W6RL_aPLWGhJsmkzPKdbEDi-+rQ@mail.gmail.com>

Hi,

Thanks for the links. So I tried what you suggested and for testing, I was
using this simple config:-

http_port 8080
auth_param basic program /usr/lib/squid/basic_ncsa_auth /etc/squid/passwords
auth_param basic realm proxy
external_acl_type checkclient children-max=20 %MYADDR %LOGIN
/usr/local/squidauth.py
acl authenticated external checkclient
http_access allow authenticated
cache deny all
forwarded_for delete
request_header_access Via deny all

I made sure that the squidauth.py file was executable and when debugging, I
found that the helper processes were running. But nothing was getting
passed to the helper processes. In the python code, I was running a loop
which reads lines from the stdin and parses them and writes output to the
stdout. I checked and it wasn't getting anything from stdin. (I added a
line which reads the input line from stdin and sends it to another server
through a http request to make sure if it was getting anything from stdin
at all)

But, when I tried to use the proxy(and of course I was using a username and
password that was stored in /etc/squid/passwords), I kept getting the error
that authentication required, i.e. the server was sending back the header
Proxy-Authenticate: Basic realm="proxy". I am not sure what I am doing
wrong here.

With regards,
Sonya Roy

On Tue, Jun 20, 2017 at 2:49 AM, Amos Jeffries <squid3 at treenet.co.nz> wrote:

> On 20/06/17 09:15, Amos Jeffries wrote:
>
>> On 20/06/17 03:20, Sonya Roy wrote:
>>
>>> Since you are saying the IP that can be passed to the helpers is
>>> configurable, how would I pass the local IP of the server that the
>>> client connected to?
>>>
>>> I checked out the helpers you mentioned, there they check which IP the
>>> connection is coming from. Not the local IP of the server that the
>>> client is connected to and they are using %SRC for that.
>>>
>>
>> The external ACL helpers don't know one IP from any other. They simply
>> check what is given to them against some form of username+ip mapping.
>>
>>
> [ with the correct links ]
>
>>
>> In Squid-3.5 that would be %MYADDR
>> <http://www.squid-cache.org/Versions/v3/3.5/cfgman/external_acl_type.html
>> >.
>>
>> In Squid-4+ it would be %>la
>> <http://www.squid-cache.org/Versions/v3/3.5/cfgman/logformat.html>
>>
>>
> Amos
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20170620/f10ce2f1/attachment.htm>

From sabu.thaliyath at gmail.com  Tue Jun 20 01:46:03 2017
From: sabu.thaliyath at gmail.com (Sabu Thaliyath)
Date: Tue, 20 Jun 2017 07:16:03 +0530
Subject: [squid-users] Squid tcp_outgoing_mark issue
Message-ID: <CAC=ab-_8y_1xmWcqnmVkGj73MGr_kEfFWiH3V=ELmUGy4D_fZg@mail.gmail.com>

Hi,

I was using the squid directive "tcp_outgoing_mark" in squid 3.3 and it was
working for me. However, in 3.5 version, it is not marking the packets.

I was using it as

tcp_outgoing_mark 0x5 all

 I was compiling squid using following options for configure

./configure --disable-arch-native --disable-dependency-tracking
--enable-delay-pools --enable-icap-client --enable-cachemgr-hostname=localhost
--with-openssl --enable-cache-digests --enable-linux-netfilter
--enable-follow-x-forwarded-for --enable-inline --enable-ssl-crtd

The same squid.conf and directive works for me in squid version 3.3.

Any hints ?


Regards,

Sabu
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20170620/2fdf9325/attachment.htm>

From squid3 at treenet.co.nz  Tue Jun 20 09:42:26 2017
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Tue, 20 Jun 2017 21:42:26 +1200
Subject: [squid-users] Squid tcp_outgoing_mark issue
In-Reply-To: <CAC=ab-_8y_1xmWcqnmVkGj73MGr_kEfFWiH3V=ELmUGy4D_fZg@mail.gmail.com>
References: <CAC=ab-_8y_1xmWcqnmVkGj73MGr_kEfFWiH3V=ELmUGy4D_fZg@mail.gmail.com>
Message-ID: <711f07e3-27fe-6a5e-d940-58816fd42b6a@treenet.co.nz>

On 20/06/17 13:46, Sabu Thaliyath wrote:
> Hi,
>
> I was using the squid directive "tcp_outgoing_mark" in squid 3.3 and it
> was working for me. However, in 3.5 version, it is not marking the packets.
>
> I was using it as
>
> tcp_outgoing_mark 0x5 all
>
>  I was compiling squid using following options for configure
>
> ./configure --disable-arch-native --disable-dependency-tracking
> --enable-delay-pools --enable-icap-client
> --enable-cachemgr-hostname=localhost --with-openssl
> --enable-cache-digests --enable-linux-netfilter
> --enable-follow-x-forwarded-for --enable-inline --enable-ssl-crtd
>
> The same squid.conf and directive works for me in squid version 3.3.
>
> Any hints ?

Which exact squid Squid-3.5 release are you using?

IIRC there was a bug where ssl-bump'ed traffic was loosing TOS/mark values.

Amos


From squid3 at treenet.co.nz  Tue Jun 20 10:48:23 2017
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Tue, 20 Jun 2017 22:48:23 +1200
Subject: [squid-users] Squid authentication problem (Amos Jeffries)
In-Reply-To: <CALSaDe0nN=fEx-xSvP+Ayc=W6RL_aPLWGhJsmkzPKdbEDi-+rQ@mail.gmail.com>
References: <CALSaDe0nN=fEx-xSvP+Ayc=W6RL_aPLWGhJsmkzPKdbEDi-+rQ@mail.gmail.com>
Message-ID: <3fe5aad2-d484-57dd-b80d-d3cbac70a80d@treenet.co.nz>


On 20/06/17 10:50, Sonya Roy wrote:
> Hi,
>
> Thanks for the links. So I tried what you suggested and for testing, I
> was using this simple config:-
>
> http_port 8080
> auth_param basic program /usr/lib/squid/basic_ncsa_auth /etc/squid/passwords
> auth_param basic realm proxy
> external_acl_type checkclient children-max=20 %MYADDR %LOGIN
> /usr/local/squidauth.py
> acl authenticated external checkclient
> http_access allow authenticated
> cache deny all
> forwarded_for delete
> request_header_access Via deny all
>
> I made sure that the squidauth.py file was executable and when
> debugging, I found that the helper processes were running. But nothing
> was getting passed to the helper processes. In the python code, I was
> running a loop which reads lines from the stdin and parses them and
> writes output to the stdout. I checked and it wasn't getting anything
> from stdin. (I added a line which reads the input line from stdin and
> sends it to another server through a http request to make sure if it was
> getting anything from stdin at all)
>
> But, when I tried to use the proxy(and of course I was using a username
> and password that was stored in /etc/squid/passwords), I kept getting
> the error that authentication required, i.e. the server was sending back
> the header Proxy-Authenticate: Basic realm="proxy". I am not sure what I
> am doing wrong here.

Sounds to me like the auth_param helper is not accepting the credentials 
you are testing with. The %LOGIN parameter needs auth to be completed 
successfully before the ACL helper is called with the resulting username.

Note that the NCSA helper uses a database file (/etc/squid/passwords) of 
hashes encoded by the Apache htpasswd tool. It is not a plain-text nor 
Unix passwd file, that difference catches some people out.


To simplify what is going on I would use the following config sequence:

  acl login proxy_auth REQUIRED
  http_access deny !login

  acl userip_check external checkclient
  http_access allow userip_check

  http_access deny all


Amos


From emmanuel.fuste at thalesgroup.com  Tue Jun 20 10:55:21 2017
From: emmanuel.fuste at thalesgroup.com (FUSTE Emmanuel)
Date: Tue, 20 Jun 2017 12:55:21 +0200
Subject: [squid-users] annotation and fast / slow acl
Message-ID: <d39110f9-fa86-6a50-756f-8ea0fab848f9@thalesgroup.com>

Hello,

I need to select a cache peer based on the user group.
As cache_peer_access need a fast acl to have predicable result, I tried to
- annotate transactions with "note"
- match the annotation with a fast acl
- use the acl in the cache_peer_access directive

But I still got warning about slow acl in use where fast are required.
I am missing something ?
I saw a proper configuration for something like that in the mailing list 
but no longer find it.

Log:

2017/06/20 12:13:37.024 kid1| 82,2| external_acl.cc(788) aclMatchExternal: ldap_group("anne.test ACCESINTERNET") = lookup needed
2017/06/20 12:13:37.025 kid1| 82,2| external_acl.cc(791) aclMatchExternal: "anne.test ACCESINTERNET": queueing a call.
2017/06/20 12:13:37.025 kid1| 28,2| Checklist.cc(123) goAsync: 0x7ffde8afc0e0 a fast-only directive uses a slow ACL!
2017/06/20 12:13:37.025 kid1| 82,2| external_acl.cc(793) aclMatchExternal: "anne.test ACCESINTERNET": no async support!
2017/06/20 12:13:37.025 kid1| 82,2| external_acl.cc(794) aclMatchExternal: "anne.test ACCESINTERNET": return -1.
2017/06/20 12:13:37.025 kid1| 82,2| external_acl.cc(788) aclMatchExternal: ldap_group("anne.test ACCESCHARGEDECOM") = lookup needed
2017/06/20 12:13:37.025 kid1| 82,2| external_acl.cc(791) aclMatchExternal: "anne.test ACCESCHARGEDECOM": queueing a call.
2017/06/20 12:13:37.025 kid1| 28,2| Checklist.cc(123) goAsync: 0x7ffde8afc0e0 a fast-only directive uses a slow ACL!
2017/06/20 12:13:37.025 kid1| 82,2| external_acl.cc(793) aclMatchExternal: "anne.test ACCESCHARGEDECOM": no async support!
2017/06/20 12:13:37.025 kid1| 82,2| external_acl.cc(794) aclMatchExternal: "anne.test ACCESCHARGEDECOM": return -1.
2017/06/20 12:13:37.025 kid1| 82,2| external_acl.cc(788) aclMatchExternal: ldap_group("anne.test INITIAL") = lookup needed
2017/06/20 12:13:37.025 kid1| 82,2| external_acl.cc(791) aclMatchExternal: "anne.test INITIAL": queueing a call.
2017/06/20 12:13:37.025 kid1| 28,2| Checklist.cc(123) goAsync: 0x7ffde8afc0e0 a fast-only directive uses a slow ACL!
2017/06/20 12:13:37.025 kid1| 82,2| external_acl.cc(793) aclMatchExternal: "anne.test INITIAL": no async support!
2017/06/20 12:13:37.026 kid1| 82,2| external_acl.cc(794) aclMatchExternal: "anne.test INITIAL": return -1.

conf:

acl SSL_ports port 443
acl Safe_ports port 80          # http
acl Safe_ports port 21          # ftp
acl Safe_ports port 443         # https
acl Safe_ports port 70          # gopher
acl Safe_ports port 210         # wais
acl Safe_ports port 1025-65535  # unregistered ports
acl Safe_ports port 280         # http-mgmt
acl Safe_ports port 488         # gss-http
acl Safe_ports port 591         # filemaker
acl Safe_ports port 777         # multiling http
acl Safe_ports port 8002        # multiling http
acl Safe_ports port 8080        # multiling http
acl CONNECT method CONNECT
acl AuthorizedUsers proxy_auth REQUIRED
acl StandardUser external ldap_group ACCESINTERNET
acl VIPUser external ldap_group ACCESCHARGEDECOM
acl NoNetUser external ldap_group INITIAL
acl hostnoauth src "/etc/squid/hosts_noauth"
acl urlnoauth url_regex "/etc/squid/urls_noauth"

note profil StdUser StandardUser
note profil VIP VIPUser
note profil NoNet NoNetUser
acl match-StandardUser note profil StdUser
acl match-VIPUser note profil VIP
acl match-NoNetUser note profil NoNet

http_access deny !Safe_ports
http_access deny CONNECT !SSL_ports
http_access allow localhost manager
http_access deny manager
http_access allow localhost
http_access allow urlnoauth hostnoauth
http_access allow AuthorizedUsers
http_access deny all
http_port 3128
http_port 10.10.10.10:8080
http_port 10.10.10.10:8002
http_port 10.10.10.10:8001

nonhierarchical_direct off

cache_peer 10.10.10.10         parent   8080     0  name=server_std
cache_peer 10.10.10.10         parent   8002     0  name=server_vip
cache_peer 10.10.10.10         parent   8002     0  name=server_urlnoauth
cache_peer 127.0.0.1             parent     80     0  name=server_nonet

never_direct allow all
always_direct deny all

cache_peer_access server_std allow match-StandardUser
cache_peer_access server_std deny all
cache_peer_access server_vip allow match-VIPUser
cache_peer_access server_vip deny all
cache_peer_access server_nonet allow match-NoNetUser
cache_peer_access server_nonet deny all
cache_peer_access server_urlnoauth allow urlnoauth
cache_peer_access server_urlnoauth deny all
cache_mem 2048 MB

maximum_object_size_in_memory 50 MB
logformat squid [%tl] %6tr %>a %Ss/%03>Hs %<st %rm %ru %[un %Sh/%<a %mt

debug_options ALL,2


From emmanuel.fuste at thalesgroup.com  Tue Jun 20 11:02:56 2017
From: emmanuel.fuste at thalesgroup.com (FUSTE Emmanuel)
Date: Tue, 20 Jun 2017 13:02:56 +0200
Subject: [squid-users] annotation and fast / slow acl
In-Reply-To: <d39110f9-fa86-6a50-756f-8ea0fab848f9@thalesgroup.com>
References: <d39110f9-fa86-6a50-756f-8ea0fab848f9@thalesgroup.com>
Message-ID: <8017ab06-3021-cee5-8461-579c3ef7f4fd@thalesgroup.com>

Le 20/06/2017 ? 12:55, FUSTE Emmanuel a ?crit :
> Hello,
>
> I need to select a cache peer based on the user group.
> As cache_peer_access need a fast acl to have predicable result, I tried to
> - annotate transactions with "note"
> - match the annotation with a fast acl
> - use the acl in the cache_peer_access directive
>
> But I still got warning about slow acl in use where fast are required.
> I am missing something ?
> I saw a proper configuration for something like that in the mailing list
> but no longer find it.
>
> Log:
>
> 2017/06/20 12:13:37.024 kid1| 82,2| external_acl.cc(788) aclMatchExternal: ldap_group("anne.test ACCESINTERNET") = lookup needed
> 2017/06/20 12:13:37.025 kid1| 82,2| external_acl.cc(791) aclMatchExternal: "anne.test ACCESINTERNET": queueing a call.
> 2017/06/20 12:13:37.025 kid1| 28,2| Checklist.cc(123) goAsync: 0x7ffde8afc0e0 a fast-only directive uses a slow ACL!
> 2017/06/20 12:13:37.025 kid1| 82,2| external_acl.cc(793) aclMatchExternal: "anne.test ACCESINTERNET": no async support!
> 2017/06/20 12:13:37.025 kid1| 82,2| external_acl.cc(794) aclMatchExternal: "anne.test ACCESINTERNET": return -1.
> 2017/06/20 12:13:37.025 kid1| 82,2| external_acl.cc(788) aclMatchExternal: ldap_group("anne.test ACCESCHARGEDECOM") = lookup needed
> 2017/06/20 12:13:37.025 kid1| 82,2| external_acl.cc(791) aclMatchExternal: "anne.test ACCESCHARGEDECOM": queueing a call.
> 2017/06/20 12:13:37.025 kid1| 28,2| Checklist.cc(123) goAsync: 0x7ffde8afc0e0 a fast-only directive uses a slow ACL!
> 2017/06/20 12:13:37.025 kid1| 82,2| external_acl.cc(793) aclMatchExternal: "anne.test ACCESCHARGEDECOM": no async support!
> 2017/06/20 12:13:37.025 kid1| 82,2| external_acl.cc(794) aclMatchExternal: "anne.test ACCESCHARGEDECOM": return -1.
> 2017/06/20 12:13:37.025 kid1| 82,2| external_acl.cc(788) aclMatchExternal: ldap_group("anne.test INITIAL") = lookup needed
> 2017/06/20 12:13:37.025 kid1| 82,2| external_acl.cc(791) aclMatchExternal: "anne.test INITIAL": queueing a call.
> 2017/06/20 12:13:37.025 kid1| 28,2| Checklist.cc(123) goAsync: 0x7ffde8afc0e0 a fast-only directive uses a slow ACL!
> 2017/06/20 12:13:37.025 kid1| 82,2| external_acl.cc(793) aclMatchExternal: "anne.test INITIAL": no async support!
> 2017/06/20 12:13:37.026 kid1| 82,2| external_acl.cc(794) aclMatchExternal: "anne.test INITIAL": return -1.
>
> conf:
>
> acl SSL_ports port 443
> acl Safe_ports port 80          # http
> acl Safe_ports port 21          # ftp
> acl Safe_ports port 443         # https
> acl Safe_ports port 70          # gopher
> acl Safe_ports port 210         # wais
> acl Safe_ports port 1025-65535  # unregistered ports
> acl Safe_ports port 280         # http-mgmt
> acl Safe_ports port 488         # gss-http
> acl Safe_ports port 591         # filemaker
> acl Safe_ports port 777         # multiling http
> acl Safe_ports port 8002        # multiling http
> acl Safe_ports port 8080        # multiling http
> acl CONNECT method CONNECT
> acl AuthorizedUsers proxy_auth REQUIRED
> acl StandardUser external ldap_group ACCESINTERNET
> acl VIPUser external ldap_group ACCESCHARGEDECOM
> acl NoNetUser external ldap_group INITIAL
> acl hostnoauth src "/etc/squid/hosts_noauth"
> acl urlnoauth url_regex "/etc/squid/urls_noauth"
>
> note profil StdUser StandardUser
> note profil VIP VIPUser
> note profil NoNet NoNetUser
> acl match-StandardUser note profil StdUser
> acl match-VIPUser note profil VIP
> acl match-NoNetUser note profil NoNet
>
> http_access deny !Safe_ports
> http_access deny CONNECT !SSL_ports
> http_access allow localhost manager
> http_access deny manager
> http_access allow localhost
> http_access allow urlnoauth hostnoauth
> http_access allow AuthorizedUsers
> http_access deny all
> http_port 3128
--------
> http_port 10.10.10.10:8080
> http_port 10.10.10.10:8002
> http_port 10.10.10.10:8001
Forget this block : anonymization  error....
-------
>
> nonhierarchical_direct off
>
> cache_peer 10.10.10.10         parent   8080     0  name=server_std
> cache_peer 10.10.10.10         parent   8002     0  name=server_vip
> cache_peer 10.10.10.10         parent   8002     0  name=server_urlnoauth
> cache_peer 127.0.0.1             parent     80     0  name=server_nonet
>
> never_direct allow all
> always_direct deny all
>
> cache_peer_access server_std allow match-StandardUser
> cache_peer_access server_std deny all
> cache_peer_access server_vip allow match-VIPUser
> cache_peer_access server_vip deny all
> cache_peer_access server_nonet allow match-NoNetUser
> cache_peer_access server_nonet deny all
> cache_peer_access server_urlnoauth allow urlnoauth
> cache_peer_access server_urlnoauth deny all
> cache_mem 2048 MB
>
> maximum_object_size_in_memory 50 MB
> logformat squid [%tl] %6tr %>a %Ss/%03>Hs %<st %rm %ru %[un %Sh/%<a %mt
>
> debug_options ALL,2
>

From sonyaroy75 at gmail.com  Tue Jun 20 11:32:07 2017
From: sonyaroy75 at gmail.com (Sonya Roy)
Date: Tue, 20 Jun 2017 17:02:07 +0530
Subject: [squid-users] Squid authentication problem (Amos Jeffries)
Message-ID: <CALSaDe0k=OFc9KSaNbdmex7v+UMpyuVALGGawpe+9y26E7=TBw@mail.gmail.com>

Thanks for all the help. I just checked the /etc/squid/passwords file,
turns out I mistakenly used htpasswd -c when saving the last username,
password and all the previous ones got overwritten.

After fixing that, the config file I wrote earlier worked fine.

With regards,
Sonya Roy

On Tue, Jun 20, 2017 at 4:18 PM, Amos Jeffries <squid3 at treenet.co.nz> wrote:

>
> On 20/06/17 10:50, Sonya Roy wrote:
>
>> Hi,
>>
>> Thanks for the links. So I tried what you suggested and for testing, I
>> was using this simple config:-
>>
>> http_port 8080
>> auth_param basic program /usr/lib/squid/basic_ncsa_auth
>> /etc/squid/passwords
>> auth_param basic realm proxy
>> external_acl_type checkclient children-max=20 %MYADDR %LOGIN
>> /usr/local/squidauth.py
>> acl authenticated external checkclient
>> http_access allow authenticated
>> cache deny all
>> forwarded_for delete
>> request_header_access Via deny all
>>
>> I made sure that the squidauth.py file was executable and when
>> debugging, I found that the helper processes were running. But nothing
>> was getting passed to the helper processes. In the python code, I was
>> running a loop which reads lines from the stdin and parses them and
>> writes output to the stdout. I checked and it wasn't getting anything
>> from stdin. (I added a line which reads the input line from stdin and
>> sends it to another server through a http request to make sure if it was
>> getting anything from stdin at all)
>>
>> But, when I tried to use the proxy(and of course I was using a username
>> and password that was stored in /etc/squid/passwords), I kept getting
>> the error that authentication required, i.e. the server was sending back
>> the header Proxy-Authenticate: Basic realm="proxy". I am not sure what I
>> am doing wrong here.
>>
>
> Sounds to me like the auth_param helper is not accepting the credentials
> you are testing with. The %LOGIN parameter needs auth to be completed
> successfully before the ACL helper is called with the resulting username.
>
> Note that the NCSA helper uses a database file (/etc/squid/passwords) of
> hashes encoded by the Apache htpasswd tool. It is not a plain-text nor Unix
> passwd file, that difference catches some people out.
>
>
> To simplify what is going on I would use the following config sequence:
>
>  acl login proxy_auth REQUIRED
>  http_access deny !login
>
>  acl userip_check external checkclient
>  http_access allow userip_check
>
>  http_access deny all
>
>
>
> Amos
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20170620/9404f37e/attachment.htm>

From Ralf.Hildebrandt at charite.de  Tue Jun 20 11:35:08 2017
From: Ralf.Hildebrandt at charite.de (Ralf Hildebrandt)
Date: Tue, 20 Jun 2017 13:35:08 +0200
Subject: [squid-users] Reverse DNS Lookup for client IPs
Message-ID: <20170620113508.ycg4iqvdq442kska@charite.de>

I have to chime in on the "Reverse DNS Lookup for client IPs" thread
back in Feb 2016. I tried redefining the logging format for
url_rewrite_extras and store_id_extras in the config, but that
wouldn't work.

I had to change the file src/cf.data.pre and recompiled, after that
the number of reverse lookups dropped considerably.

-- 
Ralf Hildebrandt                   Charite Universit?tsmedizin Berlin
ralf.hildebrandt at charite.de        Campus Benjamin Franklin
https://www.charite.de             Hindenburgdamm 30, 12203 Berlin
Gesch?ftsbereich IT, Abt. Netzwerk fon: +49-30-450.570.155


From squid3 at treenet.co.nz  Tue Jun 20 12:46:19 2017
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Wed, 21 Jun 2017 00:46:19 +1200
Subject: [squid-users] annotation and fast / slow acl
In-Reply-To: <d39110f9-fa86-6a50-756f-8ea0fab848f9@thalesgroup.com>
References: <d39110f9-fa86-6a50-756f-8ea0fab848f9@thalesgroup.com>
Message-ID: <597649ba-3928-1e4d-a16e-a09f01248bca@treenet.co.nz>

On 20/06/17 22:55, FUSTE Emmanuel wrote:
> Hello,
>
> I need to select a cache peer based on the user group.
> As cache_peer_access need a fast acl to have predicable result, I tried to
> - annotate transactions with "note"
> - match the annotation with a fast acl
> - use the acl in the cache_peer_access directive
>
> But I still got warning about slow acl in use where fast are required.
> I am missing something ?

The 'note' directive (different from the note ACL type) itself is a 
"fast" access control whose purpose is to add things into the log file. 
It only does its thing at the termination of a transaction right before 
logging.


What you are wanting is to alter the external_acl_type helper (or write 
a script wrapper for it that changes the output). Such that when Squid 
sends it a lookup it generates an response to Squid saying something 
like this:

  OK profil="$group_name"

(where $group_name, is the group which matched)


When that is working you can also vastly simplify your squid.conf by 
replacing all these:

   acl StandardUser external ldap_group ACCESINTERNET
   acl VIPUser external ldap_group ACCESCHARGEDECOM
   acl NoNetUser external ldap_group INITIAL

... with a single helper ACL test:
   acl group external ldap_group ACCESINTERNET ACCESCHARGEDECOM INITIAL

... which gets run only for authenticated users:
   http_access deny !AuthorizedUsers
   http_access allow group

... and use the note ACLs to do all your other access controls:
   acl StandardUser note profil ACCESINTERNET
   acl VIPUser note profil ACCESCHARGEDECOM
   acl NoNetUser note profil INITIAL



PS.
>
> maximum_object_size_in_memory 50 MB
> logformat squid [%tl] %6tr %>a %Ss/%03>Hs %<st %rm %ru %[un %Sh/%<a %mt

FYI: please do not try to define that "squid" log format in squid.conf. 
Squid does not follow that instruction, and may do unexpected things as 
a result. The latest releases will refuse to start if squid.conf 
contains these.


Amos


From emmanuel.fuste at thalesgroup.com  Tue Jun 20 12:55:26 2017
From: emmanuel.fuste at thalesgroup.com (FUSTE Emmanuel)
Date: Tue, 20 Jun 2017 14:55:26 +0200
Subject: [squid-users] annotation and fast / slow acl
In-Reply-To: <597649ba-3928-1e4d-a16e-a09f01248bca@treenet.co.nz>
References: <d39110f9-fa86-6a50-756f-8ea0fab848f9@thalesgroup.com>
 <597649ba-3928-1e4d-a16e-a09f01248bca@treenet.co.nz>
Message-ID: <ee5346c1-864f-84ac-de98-abb789f1af5e@thalesgroup.com>

Hello,

Thank you, it help a lot and clarify  things.


Emmanuel.

Le 20/06/2017 ? 14:46, Amos Jeffries a ?crit :
> On 20/06/17 22:55, FUSTE Emmanuel wrote:
>> Hello,
>>
>> I need to select a cache peer based on the user group.
>> As cache_peer_access need a fast acl to have predicable result, I tried to
>> - annotate transactions with "note"
>> - match the annotation with a fast acl
>> - use the acl in the cache_peer_access directive
>>
>> But I still got warning about slow acl in use where fast are required.
>> I am missing something ?
> The 'note' directive (different from the note ACL type) itself is a
> "fast" access control whose purpose is to add things into the log file.
> It only does its thing at the termination of a transaction right before
> logging.
>
>
> What you are wanting is to alter the external_acl_type helper (or write
> a script wrapper for it that changes the output). Such that when Squid
> sends it a lookup it generates an response to Squid saying something
> like this:
>
>    OK profil="$group_name"
>
> (where $group_name, is the group which matched)
>
>
> When that is working you can also vastly simplify your squid.conf by
> replacing all these:
>
>     acl StandardUser external ldap_group ACCESINTERNET
>     acl VIPUser external ldap_group ACCESCHARGEDECOM
>     acl NoNetUser external ldap_group INITIAL
>
> ... with a single helper ACL test:
>     acl group external ldap_group ACCESINTERNET ACCESCHARGEDECOM INITIAL
>
> ... which gets run only for authenticated users:
>     http_access deny !AuthorizedUsers
>     http_access allow group
>
> ... and use the note ACLs to do all your other access controls:
>     acl StandardUser note profil ACCESINTERNET
>     acl VIPUser note profil ACCESCHARGEDECOM
>     acl NoNetUser note profil INITIAL
>
>
>
> PS.
>> maximum_object_size_in_memory 50 MB
>> logformat squid [%tl] %6tr %>a %Ss/%03>Hs %<st %rm %ru %[un %Sh/%<a %mt
> FYI: please do not try to define that "squid" log format in squid.conf.
> Squid does not follow that instruction, and may do unexpected things as
> a result. The latest releases will refuse to start if squid.conf
> contains these.
>
>
> Amos
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users

From eric.kom83 at gmail.com  Tue Jun 20 13:47:23 2017
From: eric.kom83 at gmail.com (Eric C. Kom)
Date: Tue, 20 Jun 2017 15:47:23 +0200
Subject: [squid-users] FATAL error while using dstdomain directive
Message-ID: <CAMQUnrxypXpAozJdMO=Ryk_iKd6fNq61M-xhdEtOy0sXHqQzrw@mail.gmail.com>

Good day Folks,
Please I tried to block websites like youtube.com and facebook.com  using
acl dstdomain directive without success.

Squid replied the above error:
erickom at proxy:/etc/squid3$ sudo squid3 -z
2017/06/20 15:37:37| ERROR: '.google.com' is a subdomain of 'google.com'
2017/06/20 15:37:37| ERROR: You need to remove '.google.com' from the ACL
named 'ban_list'
FATAL: Bungled /etc/squid3/squid.conf line 57: acl ban_list dstdomain
"/etc/squid3/ban_list"
Squid Cache (Version 3.4.8): Terminated abnormally.
CPU Usage: 0.004 seconds = 0.004 user + 0.000 sys
Maximum Resident Size: 37520 KB


See below my ban_list file:
erickom at proxy:/etc/squid3$ cat ban_list
google.com
.google.com
youtube.com
.youtube.com
facebook.com
.facebook.com

Please assist

Thanks
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20170620/8086282d/attachment.htm>

From Ralf.Hildebrandt at charite.de  Tue Jun 20 14:09:19 2017
From: Ralf.Hildebrandt at charite.de (Ralf Hildebrandt)
Date: Tue, 20 Jun 2017 16:09:19 +0200
Subject: [squid-users] FATAL error while using dstdomain directive
In-Reply-To: <CAMQUnrxypXpAozJdMO=Ryk_iKd6fNq61M-xhdEtOy0sXHqQzrw@mail.gmail.com>
References: <CAMQUnrxypXpAozJdMO=Ryk_iKd6fNq61M-xhdEtOy0sXHqQzrw@mail.gmail.com>
Message-ID: <20170620140919.fy7vsuw2fwwjxv3l@charite.de>

* Eric C. Kom <eric.kom83 at gmail.com>:
> Good day Folks,
> Please I tried to block websites like youtube.com and facebook.com  using
> acl dstdomain directive without success.
> 
> Squid replied the above error:
> erickom at proxy:/etc/squid3$ sudo squid3 -z
> 2017/06/20 15:37:37| ERROR: '.google.com' is a subdomain of 'google.com'
> 2017/06/20 15:37:37| ERROR: You need to remove '.google.com' from the ACL named 'ban_list'

It SAYS what you need to do...

> erickom at proxy:/etc/squid3$ cat ban_list
> google.com

Remove .google.com

> youtube.com

Remove .youtube.com

> facebook.com

Remove .facebook.com

-- 
Ralf Hildebrandt                   Charite Universit?tsmedizin Berlin
ralf.hildebrandt at charite.de        Campus Benjamin Franklin
https://www.charite.de             Hindenburgdamm 30, 12203 Berlin
Gesch?ftsbereich IT, Abt. Netzwerk fon: +49-30-450.570.155


From squid at bloms.de  Tue Jun 20 14:13:56 2017
From: squid at bloms.de (Dieter Bloms)
Date: Tue, 20 Jun 2017 16:13:56 +0200
Subject: [squid-users] customize timeformat in error pages
Message-ID: <20170620141356.GB13@bloms.de>

Hello,

I want to customize the time format for %t in my error pages.
For the logfiles it is in strftime format like %{%d.%m:%Y %H:%M:%S}tl,
but when I put it in my error page templates like %{%d.%m:%Y %H:%M:%S}t,
squid doesn't consider it.
Is there any way to define the timeformat for %t in the error pages ?

Thank you very much!

-- 
Regards

  Dieter

--
I do not get viruses because I do not use MS software.
If you use Outlook then please do not put my email address in your
address-book so that WHEN you get a virus it won't use my address in the
>From field.


From eric.kom83 at gmail.com  Tue Jun 20 14:24:09 2017
From: eric.kom83 at gmail.com (Eric C. Kom)
Date: Tue, 20 Jun 2017 16:24:09 +0200
Subject: [squid-users] FATAL error while using dstdomain directive
In-Reply-To: <20170620140919.fy7vsuw2fwwjxv3l@charite.de>
References: <CAMQUnrxypXpAozJdMO=Ryk_iKd6fNq61M-xhdEtOy0sXHqQzrw@mail.gmail.com>
 <20170620140919.fy7vsuw2fwwjxv3l@charite.de>
Message-ID: <CAMQUnrwU6cH3-Et_UMDFJzaomtFe-eBNxAb=RtqeDiQdh5zaTQ@mail.gmail.com>

?Sorry for not seen?

On 20 June 2017 at 16:09, Ralf Hildebrandt <Ralf.Hildebrandt at charite.de>
wrote:

> * Eric C. Kom <eric.kom83 at gmail.com>:
> > Good day Folks,
> > Please I tried to block websites like youtube.com and facebook.com
> using
> > acl dstdomain directive without success.
> >
> > Squid replied the above error:
> > erickom at proxy:/etc/squid3$ sudo squid3 -z
> > 2017/06/20 15:37:37| ERROR: '.google.com' is a subdomain of 'google.com'
> > 2017/06/20 15:37:37| ERROR: You need to remove '.google.com' from the
> ACL named 'ban_list'
>
> It SAYS what you need to do...
>
> > erickom at proxy:/etc/squid3$ cat ban_list
> > google.com
>
> Remove .google.com
>
> > youtube.com
>
> Remove .youtube.com
>
> > facebook.com
>
> Remove .facebook.com
>
> --
> Ralf Hildebrandt                   Charite Universit?tsmedizin Berlin
> ralf.hildebrandt at charite.de        Campus Benjamin Franklin
> https://www.charite.de             Hindenburgdamm 30, 12203 Berlin
> Gesch?ftsbereich IT, Abt. Netzwerk fon: +49-30-450.570.155
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20170620/d22f710c/attachment.htm>

From eric.kom83 at gmail.com  Tue Jun 20 14:25:47 2017
From: eric.kom83 at gmail.com (Eric C. Kom)
Date: Tue, 20 Jun 2017 16:25:47 +0200
Subject: [squid-users] FATAL error while using dstdomain directive
In-Reply-To: <CAMQUnrwU6cH3-Et_UMDFJzaomtFe-eBNxAb=RtqeDiQdh5zaTQ@mail.gmail.com>
References: <CAMQUnrxypXpAozJdMO=Ryk_iKd6fNq61M-xhdEtOy0sXHqQzrw@mail.gmail.com>
 <20170620140919.fy7vsuw2fwwjxv3l@charite.de>
 <CAMQUnrwU6cH3-Et_UMDFJzaomtFe-eBNxAb=RtqeDiQdh5zaTQ@mail.gmail.com>
Message-ID: <CAMQUnryc80hA8rxzvvPjQBUxFK=vJcr6bXV0rSP8Zxm+WYphpQ@mail.gmail.com>

Sorry for not seen it.
Running fine now:
erickom at proxy:/etc/squid3$ sudo squid3 -z
2017/06/20 16:30:15| Squid is already running!  Process ID 1584

But still the block websites are still accessible.



On 20 June 2017 at 16:24, Eric C. Kom <eric.kom83 at gmail.com> wrote:

> ?Sorry for not seen?
>
> On 20 June 2017 at 16:09, Ralf Hildebrandt <Ralf.Hildebrandt at charite.de>
> wrote:
>
>> * Eric C. Kom <eric.kom83 at gmail.com>:
>> > Good day Folks,
>> > Please I tried to block websites like youtube.com and facebook.com
>> using
>> > acl dstdomain directive without success.
>> >
>> > Squid replied the above error:
>> > erickom at proxy:/etc/squid3$ sudo squid3 -z
>> > 2017/06/20 15:37:37| ERROR: '.google.com' is a subdomain of 'google.com
>> '
>> > 2017/06/20 15:37:37| ERROR: You need to remove '.google.com' from the
>> ACL named 'ban_list'
>>
>> It SAYS what you need to do...
>>
>> > erickom at proxy:/etc/squid3$ cat ban_list
>> > google.com
>>
>> Remove .google.com
>>
>> > youtube.com
>>
>> Remove .youtube.com
>>
>> > facebook.com
>>
>> Remove .facebook.com
>>
>> --
>> Ralf Hildebrandt                   Charite Universit?tsmedizin Berlin
>> ralf.hildebrandt at charite.de        Campus Benjamin Franklin
>> https://www.charite.de             Hindenburgdamm 30, 12203 Berlin
>> Gesch?ftsbereich IT, Abt. Netzwerk fon: +49-30-450.570.155
>> _______________________________________________
>> squid-users mailing list
>> squid-users at lists.squid-cache.org
>> http://lists.squid-cache.org/listinfo/squid-users
>>
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20170620/9e37e71c/attachment.htm>

From Antony.Stone at squid.open.source.it  Tue Jun 20 14:46:30 2017
From: Antony.Stone at squid.open.source.it (Antony Stone)
Date: Tue, 20 Jun 2017 16:46:30 +0200
Subject: [squid-users] FATAL error while using dstdomain directive
In-Reply-To: <CAMQUnryc80hA8rxzvvPjQBUxFK=vJcr6bXV0rSP8Zxm+WYphpQ@mail.gmail.com>
References: <CAMQUnrxypXpAozJdMO=Ryk_iKd6fNq61M-xhdEtOy0sXHqQzrw@mail.gmail.com>
 <CAMQUnrwU6cH3-Et_UMDFJzaomtFe-eBNxAb=RtqeDiQdh5zaTQ@mail.gmail.com>
 <CAMQUnryc80hA8rxzvvPjQBUxFK=vJcr6bXV0rSP8Zxm+WYphpQ@mail.gmail.com>
Message-ID: <201706201646.30903.Antony.Stone@squid.open.source.it>

On Tuesday 20 June 2017 at 16:25:47, Eric C. Kom wrote:

> Sorry for not seen it.
> Running fine now:
> erickom at proxy:/etc/squid3$ sudo squid3 -z
> 2017/06/20 16:30:15| Squid is already running!  Process ID 1584
> 
> But still the block websites are still accessible.

Please, either:

1. show us an example of an access log entry for a user visiting "google.com" 
or "youtube.com" or "facebook.com"

or

2. tell us whether you really are trying to block "google.com" (plus the other 
two) or whether you really mean to block "anything ending in .google.com" 
(plus the other two).

This may help: http://wiki.squid-
cache.org/SquidFaq/SquidAcl#Squid_doesn.27t_match_my_subdomains


Antony.

> On 20 June 2017 at 16:24, Eric C. Kom <eric.kom83 at gmail.com> wrote:
> > ?Sorry for not seen?
> > 
> > On 20 June 2017 at 16:09, Ralf Hildebrandt <Ralf.Hildebrandt at charite.de>
> > 
> > wrote:
> >> * Eric C. Kom <eric.kom83 at gmail.com>:
> >> > Good day Folks,
> >> > Please I tried to block websites like youtube.com and facebook.com
> >> 
> >> using
> >> 
> >> > acl dstdomain directive without success.
> >> > 
> >> > Squid replied the above error:
> >> > erickom at proxy:/etc/squid3$ sudo squid3 -z
> >> > 2017/06/20 15:37:37| ERROR: '.google.com' is a subdomain of
> >> > 'google.com
> >> 
> >> '
> >> 
> >> > 2017/06/20 15:37:37| ERROR: You need to remove '.google.com' from the
> >> 
> >> ACL named 'ban_list'
> >> 
> >> It SAYS what you need to do...
> >> 
> >> > erickom at proxy:/etc/squid3$ cat ban_list
> >> > google.com
> >> 
> >> Remove .google.com
> >> 
> >> > youtube.com
> >> 
> >> Remove .youtube.com
> >> 
> >> > facebook.com
> >> 
> >> Remove .facebook.com
> >> 
> >> --
> >> Ralf Hildebrandt                   Charite Universit?tsmedizin Berlin
> >> ralf.hildebrandt at charite.de        Campus Benjamin Franklin
> >> https://www.charite.de             Hindenburgdamm 30, 12203 Berlin
> >> Gesch?ftsbereich IT, Abt. Netzwerk fon: +49-30-450.570.155
> >> _______________________________________________
> >> squid-users mailing list
> >> squid-users at lists.squid-cache.org
> >> http://lists.squid-cache.org/listinfo/squid-users

-- 
If the human brain were so simple that we could understand it,
we'd be so simple that we couldn't.

                                                   Please reply to the list;
                                                         please *don't* CC me.


From uhlar at fantomas.sk  Tue Jun 20 15:58:20 2017
From: uhlar at fantomas.sk (Matus UHLAR - fantomas)
Date: Tue, 20 Jun 2017 17:58:20 +0200
Subject: [squid-users] FATAL error while using dstdomain directive
In-Reply-To: <CAMQUnrxypXpAozJdMO=Ryk_iKd6fNq61M-xhdEtOy0sXHqQzrw@mail.gmail.com>
References: <CAMQUnrxypXpAozJdMO=Ryk_iKd6fNq61M-xhdEtOy0sXHqQzrw@mail.gmail.com>
Message-ID: <20170620155820.GB29690@fantomas.sk>

On 20.06.17 15:47, Eric C. Kom wrote:
>Squid replied the above error:
>erickom at proxy:/etc/squid3$ sudo squid3 -z
>2017/06/20 15:37:37| ERROR: '.google.com' is a subdomain of 'google.com'
>2017/06/20 15:37:37| ERROR: You need to remove '.google.com' from the ACL
>named 'ban_list'
>FATAL: Bungled /etc/squid3/squid.conf line 57: acl ban_list dstdomain
>"/etc/squid3/ban_list"
>Squid Cache (Version 3.4.8): Terminated abnormally.
>CPU Usage: 0.004 seconds = 0.004 user + 0.000 sys
>Maximum Resident Size: 37520 KB
>
>
>See below my ban_list file:
>erickom at proxy:/etc/squid3$ cat ban_list
>google.com
>.google.com
>youtube.com
>.youtube.com
>facebook.com
>.facebook.com

remove google.com, youtube.com and facebook.com from the list.
the ".google.com" applies for "google.com" alone too.
the same applies for the rest.

-- 
Matus UHLAR - fantomas, uhlar at fantomas.sk ; http://www.fantomas.sk/
Warning: I wish NOT to receive e-mail advertising to this address.
Varovanie: na tuto adresu chcem NEDOSTAVAT akukolvek reklamnu postu.
Boost your system's speed by 500% - DEL C:\WINDOWS\*.*


From squid3 at treenet.co.nz  Tue Jun 20 16:26:58 2017
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Wed, 21 Jun 2017 04:26:58 +1200
Subject: [squid-users] customize timeformat in error pages
In-Reply-To: <20170620141356.GB13@bloms.de>
References: <20170620141356.GB13@bloms.de>
Message-ID: <f775ceb6-b45a-60f5-fcc8-ba16055e22cb@treenet.co.nz>

On 21/06/17 02:13, Dieter Bloms wrote:
> Hello,
>
> I want to customize the time format for %t in my error pages.
> For the logfiles it is in strftime format like %{%d.%m:%Y %H:%M:%S}tl,
> but when I put it in my error page templates like %{%d.%m:%Y %H:%M:%S}t,
> squid doesn't consider it.
> Is there any way to define the timeformat for %t in the error pages ?
>

Not yet. Error templates using the logformat codes is still on the wishlist.

%T uses international standard syntax that should be readable to anyone 
on the planet. If you want anything more complex I suggest using 
embedded javascript to mangle that into whatever.

Amos


From jne100 at gmail.com  Wed Jun 21 16:15:21 2017
From: jne100 at gmail.com (Nikita)
Date: Wed, 21 Jun 2017 19:15:21 +0300
Subject: [squid-users] Squid reject self-signed SSL certificate of ICAP
	server
Message-ID: <CAMCZdezyg8aVNgBtCsbgJp92sgvSu31cx4cSACQxkeVVR3jsbg@mail.gmail.com>

Hello, I'm trying to integrate Squid with secure ICAP server over icaps://
protocol for two-way authentication (icap_service configuration directive).

I find out that Squid reject self-signed certificate of ICAP server and
there is no obvious workaround.

There is tls-flags=DONT_VERIFY_PEER flag, but in this case Squid don't send
it's own certificate to ICAP server, so more accurate workaround needed.
sslproxy_cert_error configuration directive with ssl_error acltype don't
help as well.

Is it possible to allow self-signed SSL certificates for ICAP server
connections somehow? Probably I miss some obvious solution since I have no
experience in Squid configuration.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20170621/61e53686/attachment.htm>

From rousskov at measurement-factory.com  Wed Jun 21 16:46:46 2017
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Wed, 21 Jun 2017 10:46:46 -0600
Subject: [squid-users] Squid reject self-signed SSL certificate of ICAP
 server
In-Reply-To: <CAMCZdezyg8aVNgBtCsbgJp92sgvSu31cx4cSACQxkeVVR3jsbg@mail.gmail.com>
References: <CAMCZdezyg8aVNgBtCsbgJp92sgvSu31cx4cSACQxkeVVR3jsbg@mail.gmail.com>
Message-ID: <aad4601d-8b3e-ef6c-b195-ddd66c124a5b@measurement-factory.com>

On 06/21/2017 10:15 AM, Nikita wrote:

> Is it possible to allow self-signed SSL certificates for ICAP server
> connections somehow?

Can you configure your OpenSSL library (or equivalent) to trust the ICAP
server certificate? Squid deletages most of the certificate validation
work to OpenSSL (or equivalent).


> There is tls-flags=DONT_VERIFY_PEER flag, but in this case Squid
> don't send it's own certificate to ICAP server

Why do you think tls-flags=DONT_VERIFY_PEER only works if Squid sends
its own certificate? The two actions (from-peer certificate validation
and sending of a certificate to a peer) seem unrelated to me.

Alex.


From emmanuel.fuste at thalesgroup.com  Wed Jun 21 16:51:11 2017
From: emmanuel.fuste at thalesgroup.com (FUSTE Emmanuel)
Date: Wed, 21 Jun 2017 18:51:11 +0200
Subject: [squid-users] annotation and fast / slow acl
In-Reply-To: <ee5346c1-864f-84ac-de98-abb789f1af5e@thalesgroup.com>
References: <d39110f9-fa86-6a50-756f-8ea0fab848f9@thalesgroup.com>
 <597649ba-3928-1e4d-a16e-a09f01248bca@treenet.co.nz>
 <ee5346c1-864f-84ac-de98-abb789f1af5e@thalesgroup.com>
Message-ID: <ec74b885-0a3f-f1e7-d29d-8092a076b6de@thalesgroup.com>

Hello,

One more question to be sure to understand some details:

> Le 20/06/2017 ? 14:46, Amos Jeffries a ?crit :
>> On 20/06/17 22:55, FUSTE Emmanuel wrote:
>>> Hello,
>>>
>>> I need to select a cache peer based on the user group.
>>> As cache_peer_access need a fast acl to have predicable result, I tried to
>>> - annotate transactions with "note"
>>> - match the annotation with a fast acl
>>> - use the acl in the cache_peer_access directive
>>>
>>> But I still got warning about slow acl in use where fast are required.
>>> I am missing something ?
>> The 'note' directive (different from the note ACL type) itself is a
>> "fast" access control whose purpose is to add things into the log file.
>> It only does its thing at the termination of a transaction right before
>> logging.
>>
>>
>> What you are wanting is to alter the external_acl_type helper (or write
>> a script wrapper for it that changes the output). Such that when Squid
>> sends it a lookup it generates an response to Squid saying something
>> like this:
>>
>>     OK profil="$group_name"
>>
>> (where $group_name, is the group which matched)
>>
>>
>> When that is working you can also vastly simplify your squid.conf by
>> replacing all these:
>>
>>      acl StandardUser external ldap_group ACCESINTERNET
>>      acl VIPUser external ldap_group ACCESCHARGEDECOM
>>      acl NoNetUser external ldap_group INITIAL
>>
>> ... with a single helper ACL test:
>>      acl group external ldap_group ACCESINTERNET ACCESCHARGEDECOM INITIAL
>>
>> ... which gets run only for authenticated users:
>>      http_access deny !AuthorizedUsers
>>      http_access allow group
>>
>> ... and use the note ACLs to do all your other access controls:
>>      acl StandardUser note profil ACCESINTERNET
>>      acl VIPUser note profil ACCESCHARGEDECOM
>>      acl NoNetUser note profil INITIAL
So arbitrary k- v pair not used by the acl helper protocol could be 
matched against with the note acl ?
How it relate to the defined/reserved tag= and clt_conn_tag= keywords of 
the acl helper protocol ?

The helper is modified to return profil="$group_name" when the group 
match. It work.
Will test it on a squid instance with note acl tomorrow.

Emmanuel.

From squid3 at treenet.co.nz  Wed Jun 21 17:17:06 2017
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 22 Jun 2017 05:17:06 +1200
Subject: [squid-users] annotation and fast / slow acl
In-Reply-To: <ec74b885-0a3f-f1e7-d29d-8092a076b6de@thalesgroup.com>
References: <d39110f9-fa86-6a50-756f-8ea0fab848f9@thalesgroup.com>
 <597649ba-3928-1e4d-a16e-a09f01248bca@treenet.co.nz>
 <ee5346c1-864f-84ac-de98-abb789f1af5e@thalesgroup.com>
 <ec74b885-0a3f-f1e7-d29d-8092a076b6de@thalesgroup.com>
Message-ID: <a82ca9cc-b8be-87ea-28f0-57e5afebfa9e@treenet.co.nz>

On 22/06/17 04:51, FUSTE Emmanuel wrote:
>
> So arbitrary k- v pair not used by the acl helper protocol could be
> matched against with the note acl ?
> How it relate to the defined/reserved tag= and clt_conn_tag= keywords of
> the acl helper protocol ?

They are all attached as annotations on the transaction. The only ones 
excluded are the security tokens in Kerberos/NTLM/Digest auth.

Amos


From jne100 at gmail.com  Thu Jun 22 09:23:14 2017
From: jne100 at gmail.com (Nikita)
Date: Thu, 22 Jun 2017 12:23:14 +0300
Subject: [squid-users] Squid reject self-signed SSL certificate of ICAP
	server
In-Reply-To: <aad4601d-8b3e-ef6c-b195-ddd66c124a5b@measurement-factory.com>
References: <CAMCZdezyg8aVNgBtCsbgJp92sgvSu31cx4cSACQxkeVVR3jsbg@mail.gmail.com>
 <aad4601d-8b3e-ef6c-b195-ddd66c124a5b@measurement-factory.com>
Message-ID: <CAMCZdewLBJDm7hMt25FnjEADYJ54hVEZz7WrK03VDSXHFAtHUQ@mail.gmail.com>

2017-06-21 19:46 GMT+03:00 Alex Rousskov <rousskov at measurement-factory.com>:

> On 06/21/2017 10:15 AM, Nikita wrote:
>
> > Is it possible to allow self-signed SSL certificates for ICAP server
> > connections somehow?
>
> Can you configure your OpenSSL library (or equivalent) to trust the ICAP
> server certificate? Squid deletages most of the certificate validation
> work to OpenSSL (or equivalent).
>
>
Probably worth a try, but generally it is undesirable in my case to modify
global OpenSSL config.


> > There is tls-flags=DONT_VERIFY_PEER flag, but in this case Squid
> > don't send it's own certificate to ICAP server
>
> Why do you think tls-flags=DONT_VERIFY_PEER only works if Squid sends
> its own certificate? The two actions (from-peer certificate validation
> and sending of a certificate to a peer) seem unrelated to me.
>
>
In my case for some unknown reasons Squid don't send its own certificate to
ICAP server, probably because of DONT_VERIFY_PEER flag, but not sure here.
BIO_do_handshake fails with "no certificate returned" on ICAP server side
despite the fact that squid certificate was specified via tls-cert and
tls-key options of icap_service config directive and ICAP server was
configured to request client certificate. It seems need to investigate
Squid source code in more detail to find some answers, thanks for advices.


> Alex.
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20170622/9833e502/attachment.htm>

From squid3 at treenet.co.nz  Thu Jun 22 13:33:02 2017
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Fri, 23 Jun 2017 01:33:02 +1200
Subject: [squid-users] Squid reject self-signed SSL certificate of ICAP
 server
In-Reply-To: <CAMCZdewLBJDm7hMt25FnjEADYJ54hVEZz7WrK03VDSXHFAtHUQ@mail.gmail.com>
References: <CAMCZdezyg8aVNgBtCsbgJp92sgvSu31cx4cSACQxkeVVR3jsbg@mail.gmail.com>
 <aad4601d-8b3e-ef6c-b195-ddd66c124a5b@measurement-factory.com>
 <CAMCZdewLBJDm7hMt25FnjEADYJ54hVEZz7WrK03VDSXHFAtHUQ@mail.gmail.com>
Message-ID: <6452fd04-d550-2ff9-e084-f9ab479e487a@treenet.co.nz>

On 22/06/17 21:23, Nikita wrote:
> 
> 2017-06-21 19:46 GMT+03:00 Alex Rousskov:
> 
>     On 06/21/2017 10:15 AM, Nikita wrote:
> 
>     > Is it possible to allow self-signed SSL certificates for ICAP server
>     > connections somehow?
> 
>     Can you configure your OpenSSL library (or equivalent) to trust the ICAP
>     server certificate? Squid deletages most of the certificate validation
>     work to OpenSSL (or equivalent).
> 
> 
> Probably worth a try, but generally it is undesirable in my case to 
> modify global OpenSSL config.
> 
> 
>     > There is tls-flags=DONT_VERIFY_PEER flag, but in this case Squid
>     > don't send it's own certificate to ICAP server
> 
>     Why do you think tls-flags=DONT_VERIFY_PEER only works if Squid sends
>     its own certificate? The two actions (from-peer certificate validation
>     and sending of a certificate to a peer) seem unrelated to me.
> 
> 
> In my case for some unknown reasons Squid don't send its own certificate 
> to ICAP server, probably because of DONT_VERIFY_PEER flag, but not sure 
> here. BIO_do_handshake fails with "no certificate returned" on ICAP 
> server side despite the fact that squid certificate was specified via 
> tls-cert and tls-key options of icap_service config directive and ICAP 
> server was configured to request client certificate. It seems need to 
> investigate Squid source code in more detail to find some answers, 
> thanks for advices.


What DONT_VERIFY_PEER does is prevent Squid checking any of the TLS 
server details that ensure it is actually talking to the ICAP server you 
configured it to use. As Alex said it does not directly prevent Squid 
code from sending a client cert, but it *does* allow your Squid traffic 
to be diverted to a completely irrelevant ICAP server and you will never 
know.

It is quite possible the behaviour you are seeing is simply because 
Squid is not even connected to the ICAP server you are trying to test with.

I hope this clarifies why I strongly recommend people erase the 
DONT_VERIFY_PEER option from their configs, and get things going without 
it. It is not a useful debugging tool, let along production setting.


Amos


From sonyaroy75 at gmail.com  Thu Jun 22 17:49:36 2017
From: sonyaroy75 at gmail.com (Sonya Roy)
Date: Thu, 22 Jun 2017 23:19:36 +0530
Subject: [squid-users] Header order in squid proxy
Message-ID: <CALSaDe1UHsVMoRLCr3CzyuhNPocPcKdc9FM7i5oLuCCJ6BD0+Q@mail.gmail.com>

Hi,

I noticed that squid changes the header order received from the client
before sending it to the origin server.

I assume this is because squid parses the header data and adds some headers
depending on the config file and then recreates the header data.

Is there any way to prevent this? To keep the header order received from
the client and only just remove headers like Proxy-Connection,
Proxy-Authorization,... which squid does anyway.

I am asking because some sites detect bots using the header order and they
drop any such connection. So they unintentionally block squid proxies even
if its not being used by a bot.

With regards,
Sonya Roy.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20170622/7f965837/attachment.htm>

From rousskov at measurement-factory.com  Thu Jun 22 18:32:14 2017
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Thu, 22 Jun 2017 12:32:14 -0600
Subject: [squid-users] Header order in squid proxy
In-Reply-To: <CALSaDe1UHsVMoRLCr3CzyuhNPocPcKdc9FM7i5oLuCCJ6BD0+Q@mail.gmail.com>
References: <CALSaDe1UHsVMoRLCr3CzyuhNPocPcKdc9FM7i5oLuCCJ6BD0+Q@mail.gmail.com>
Message-ID: <880a7094-14cc-2cb2-947b-73a4b9f5c603@measurement-factory.com>

On 06/22/2017 11:49 AM, Sonya Roy wrote:

> I noticed that squid changes the header order received from the client
> before sending it to the origin server.
> 
> I assume this is because squid parses the header data and adds some
> headers depending on the config file and then recreates the header data.

IIRC, modern Squids change a header field position when the received
field is deleted and then added back. This is typical for hop-by-hop
headers such as Connection, but there are other reasons for Squid to
delete and add a header field. When the value of the added field is the
same as the value of the removed field, such pointless "editing" looks
like mindless "reordering" to the outside observer.

The two actions (field deletion and addition) may happen in a single
piece of code or may be separated by lots of code and even time.
Preventing pointless editing in the former cases is straightforward, but
the latter cases are difficult to handle. Correct avoidance of pointless
editing may improve performance and, if it does, can be considered a
useful optimization on its own, regardless of your use case.


> Is there any way to prevent this?

Not without changing Squid code (or adding more proxies). However,
before we even talk about code changes, we should clarify the problem we
are dealing with. The questions below will guide you.

It is probably much easier to ensure some fixed field send order
(regardless of the received order) than to preserve the received order.
Will a fixed order (e.g., always alphabetical) address your use case?
This feature will hurt performance, but you might be able to convince
others to accept it if you have a very compelling/specific/detailed use
case because it can be disabled by default.


> I am asking because some sites detect bots using the header order and
> they drop any such connection. So they unintentionally block squid
> proxies even if its not being used by a bot.

Are you implying that bots often change header field order between their
requests? Or that bots often use a different (fixed) header field order
than the (fixed) field order used by non-bots? Preserving received order
may help in the former case but not in the latter case.

Also, do those blocking sites pay attention to all headers or just
end-to-end headers?

Please note that there are many other ways to detect a proxy so if a
site wants to block proxies rather than bots, then it is probably
pointless to fight it (or, at least, the Squid Project should not).


HTH,

Alex.


From sonyaroy75 at gmail.com  Thu Jun 22 18:54:25 2017
From: sonyaroy75 at gmail.com (Sonya Roy)
Date: Fri, 23 Jun 2017 00:24:25 +0530
Subject: [squid-users] Header order in squid proxy
Message-ID: <CALSaDe0TD=sctLfc8xP_URsfRyvSo1cNm6p9+Tdsohi2sgXkhg@mail.gmail.com>

The sites I am talking about check the User-Agent header and makes sure the
user-agent is for a well-known browser, i.e. a browser that they support.
And any browser like Firefox, Chrome, Safari, Edge for example, sends the
headers in a certain order and the order depends on the browser. And this
header order for well-known headers like Accept, Accept-Language,
Accept-Encoding, Content-Length, Host, Connection, Referer, Cookie, etc.
And they match the order of the received request with the standard header
order for the browser for that user-agent.

This detects bots like a poorly written bot(i.e ones that don't consider
this header order) using python requests or in any language for that matter
where the requests are handled using a low level http requests library.

So, keeping the header order sent from the client intact would prevent them
from dropping proxied requests(ones that use squid). I know for a fact that
they don't intend to block proxies.

Could you point me in the direction to where I should look for in the
source code of squid? the part that handles the header data sent from the
client.

With regards,
Sonya Roy.

On Fri, Jun 23, 2017 at 12:02 AM, Alex Rousskov <
rousskov at measurement-factory.com> wrote:

> On 06/22/2017 11:49 AM, Sonya Roy wrote:
>
> > I noticed that squid changes the header order received from the client
> > before sending it to the origin server.
> >
> > I assume this is because squid parses the header data and adds some
> > headers depending on the config file and then recreates the header data.
>
> IIRC, modern Squids change a header field position when the received
> field is deleted and then added back. This is typical for hop-by-hop
> headers such as Connection, but there are other reasons for Squid to
> delete and add a header field. When the value of the added field is the
> same as the value of the removed field, such pointless "editing" looks
> like mindless "reordering" to the outside observer.
>
> The two actions (field deletion and addition) may happen in a single
> piece of code or may be separated by lots of code and even time.
> Preventing pointless editing in the former cases is straightforward, but
> the latter cases are difficult to handle. Correct avoidance of pointless
> editing may improve performance and, if it does, can be considered a
> useful optimization on its own, regardless of your use case.
>
>
> > Is there any way to prevent this?
>
> Not without changing Squid code (or adding more proxies). However,
> before we even talk about code changes, we should clarify the problem we
> are dealing with. The questions below will guide you.
>
> It is probably much easier to ensure some fixed field send order
> (regardless of the received order) than to preserve the received order.
> Will a fixed order (e.g., always alphabetical) address your use case?
> This feature will hurt performance, but you might be able to convince
> others to accept it if you have a very compelling/specific/detailed use
> case because it can be disabled by default.
>
>
> > I am asking because some sites detect bots using the header order and
> > they drop any such connection. So they unintentionally block squid
> > proxies even if its not being used by a bot.
>
> Are you implying that bots often change header field order between their
> requests? Or that bots often use a different (fixed) header field order
> than the (fixed) field order used by non-bots? Preserving received order
> may help in the former case but not in the latter case.
>
> Also, do those blocking sites pay attention to all headers or just
> end-to-end headers?
>
> Please note that there are many other ways to detect a proxy so if a
> site wants to block proxies rather than bots, then it is probably
> pointless to fight it (or, at least, the Squid Project should not).
>
>
> HTH,
>
> Alex.
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20170623/43740d9e/attachment.htm>

From rousskov at measurement-factory.com  Thu Jun 22 20:06:22 2017
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Thu, 22 Jun 2017 14:06:22 -0600
Subject: [squid-users] Header order in squid proxy
In-Reply-To: <CALSaDe0TD=sctLfc8xP_URsfRyvSo1cNm6p9+Tdsohi2sgXkhg@mail.gmail.com>
References: <CALSaDe0TD=sctLfc8xP_URsfRyvSo1cNm6p9+Tdsohi2sgXkhg@mail.gmail.com>
Message-ID: <ef76db48-2b28-683e-2bb9-10a1698d3f62@measurement-factory.com>

On 06/22/2017 12:54 PM, Sonya Roy wrote:
> The sites I am talking about check the User-Agent header and makes sure
> the user-agent is for a well-known browser, i.e. a browser that they
> support. And any browser like Firefox, Chrome, Safari, Edge for example,
> sends the headers in a certain order and the order depends on the
> browser. And this header order for well-known headers like Accept,
> Accept-Language, Accept-Encoding, Content-Length, Host, Connection,
> Referer, Cookie, etc. And they match the order of the received request
> with the standard header order for the browser for that user-agent.

FWIW, Connection and possibly some "etc." headers are hop-by-hop headers
so if a blocking site really pays attention to them, it should be told
to exclude them.


> Could you point me in the direction to where I should look for in the
> source code of squid?

The answer depends on whether you want to:

A) prevent pointless edits (difficult and less effective but has a
fighting chance of official acceptance because it is a useful
performance optimization) or

B) simply reorder all the fields just before sending them, based on a
User-Agent field-driven order table (easy to hack in and effective but
less likely to be officially accepted due to performance overheads and
configuration/support complexities).

If you want a general vague answer, search for calls to non-const
HttpHeader methods like HttpHeader::delByName() and
HttpHeader::insertEntry(). There are about 20-30 potentially relevant
methods AFAICT. And examine the sending code in
HttpStateData::httpBuildRequestHeader().


Please note that the discussion about Squid code belongs to squid-dev,
not squid-users.


HTH,

Alex.


> On Fri, Jun 23, 2017 at 12:02 AM, Alex Rousskov wrote:
> 
>     On 06/22/2017 11:49 AM, Sonya Roy wrote:
> 
>     > I noticed that squid changes the header order received from the client
>     > before sending it to the origin server.
>     >
>     > I assume this is because squid parses the header data and adds some
>     > headers depending on the config file and then recreates the header data.
> 
>     IIRC, modern Squids change a header field position when the received
>     field is deleted and then added back. This is typical for hop-by-hop
>     headers such as Connection, but there are other reasons for Squid to
>     delete and add a header field. When the value of the added field is the
>     same as the value of the removed field, such pointless "editing" looks
>     like mindless "reordering" to the outside observer.
> 
>     The two actions (field deletion and addition) may happen in a single
>     piece of code or may be separated by lots of code and even time.
>     Preventing pointless editing in the former cases is straightforward, but
>     the latter cases are difficult to handle. Correct avoidance of pointless
>     editing may improve performance and, if it does, can be considered a
>     useful optimization on its own, regardless of your use case.
> 
> 
>     > Is there any way to prevent this?
> 
>     Not without changing Squid code (or adding more proxies). However,
>     before we even talk about code changes, we should clarify the problem we
>     are dealing with. The questions below will guide you.
> 
>     It is probably much easier to ensure some fixed field send order
>     (regardless of the received order) than to preserve the received order.
>     Will a fixed order (e.g., always alphabetical) address your use case?
>     This feature will hurt performance, but you might be able to convince
>     others to accept it if you have a very compelling/specific/detailed use
>     case because it can be disabled by default.
> 
> 
>     > I am asking because some sites detect bots using the header order and
>     > they drop any such connection. So they unintentionally block squid
>     > proxies even if its not being used by a bot.
> 
>     Are you implying that bots often change header field order between their
>     requests? Or that bots often use a different (fixed) header field order
>     than the (fixed) field order used by non-bots? Preserving received order
>     may help in the former case but not in the latter case.
> 
>     Also, do those blocking sites pay attention to all headers or just
>     end-to-end headers?
> 
>     Please note that there are many other ways to detect a proxy so if a
>     site wants to block proxies rather than bots, then it is probably
>     pointless to fight it (or, at least, the Squid Project should not).
> 
> 
>     HTH,
> 
>     Alex.
> 
> 
> 
> 
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
> 



From ahmed.zaeem at netstream.ps  Fri Jun 23 11:21:30 2017
From: ahmed.zaeem at netstream.ps (--Ahmad--)
Date: Fri, 23 Jun 2017 14:21:30 +0300
Subject: [squid-users] i want squid return value or log something when
	authentication of user succeeded
Message-ID: <6B4F1523-4F91-4FDF-B00A-AE6BD197B270@netstream.ps>

hello folks ,


basically i need to have any log or return value from squid once any user get authenticated .

as an example , if user A authenticated i can see that in access_log the username in default log format .

but what i want is like more details like the 1st time the user get authenticated .

and if the user closed his session and opened his browser and put the user/pwd again i want to be logged on that action .

is there anything in log format help me view that action in cache.log ? 


Do you recommend other external helpers for that ?




kindly advise me 

cheers 



From eduardoocarneiro at gmail.com  Fri Jun 23 16:05:11 2017
From: eduardoocarneiro at gmail.com (Eduardo Carneiro)
Date: Fri, 23 Jun 2017 09:05:11 -0700 (PDT)
Subject: [squid-users] Shared cache directory.
Message-ID: <1498233911793-4682894.post@n4.nabble.com>

Hi everyone.

Squid version 3.5.19.

I need to set up a load balancing. It would be something like, 3 servers
decentralizing the accesses. I already have that solution.

But when I decentralize the cache directories, my "HIT rates" decrease.

I'd like to know if there is any way to have more than one squid server
sharing the same cache directory.

I have already tried it using cache_peer, with "follow_x_forwarded_for"
function to do this. But, due to the use of ssl_bump, this solution did not
answer me because, in https requests, the client IP was not shown.

Best regards.



--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/Shared-cache-directory-tp4682894.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From squid3 at treenet.co.nz  Fri Jun 23 19:43:38 2017
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sat, 24 Jun 2017 07:43:38 +1200
Subject: [squid-users] i want squid return value or log something when
 authentication of user succeeded
In-Reply-To: <6B4F1523-4F91-4FDF-B00A-AE6BD197B270@netstream.ps>
References: <6B4F1523-4F91-4FDF-B00A-AE6BD197B270@netstream.ps>
Message-ID: <0c2dda8c-b4eb-8177-f8fa-926705e3854b@treenet.co.nz>

On 23/06/17 23:21, --Ahmad-- wrote:
> hello folks ,
> 
> 
> basically i need to have any log or return value from squid once any user get authenticated .
> 
> as an example , if user A authenticated i can see that in access_log the username in default log format .
> 
> but what i want is like more details like the 1st time the user get authenticated .
> 
> and if the user closed his session and opened his browser and put the user/pwd again i want to be logged on that action .

HTTP is a stateless protocol. There is no session concept at the level 
Squid operates. That is a purely web-application-layer thing in the browser.

A browser being shutdown and re-opened is indistinguishable to Squid 
from a browser simply opening a new TCP connection. Which happens 
routinely without the browser being shutdown anyway, since browsers use 
8+ (up to several hundred) of TCP connections in parallel.

So while there is a "first" authentication (as in "first ever" or "first 
since N minutes ago"), there is no "last".



> 
> is there anything in log format help me view that action in cache.log ?
> 
> 
> Do you recommend other external helpers for that ?
> 

To get the details you ask for, look at what the helper can report in 
its debug output, or adding a wrapper script around it that does the 
logging you want.

BUT, it should be clear from the info I gave above that what you seek is 
simply not going to be visible in any information Squid or its helper 
can log.
  What the access.log entry is saying is just "this request had 
credentials for X, they were checked and valid when this transaction 
started.". Nothing about any session thing.

Amos


From ncherukuri at partycity.com  Fri Jun 23 20:33:37 2017
From: ncherukuri at partycity.com (Cherukuri, Naresh)
Date: Fri, 23 Jun 2017 20:33:37 +0000
Subject: [squid-users] Squid Version 3.5.20
Message-ID: <89638057A560FB458C01C197F81C7F5D13EDCCD0@ROCKETS.amscan.corp>

Hello All,

I installed Squid version 3.5.20 on RHEL 7 and generated selfsigned CA certificates, can you shed some light on how to "Configure regular expression of the Google ReCaptcha URL with ACL".

My requirement :

This requirement is to allow Google's ReCaptcha URL (HTTPS) so associates can successfully use ADP which now utilizes Google's ReCaptcha which is called via an HTTPS URL, without allowing users to access other Google-related services such as Gmail or Google Drive.

Any ideas much appreciated!

Thanks,
Naresh
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20170623/a43df4a8/attachment.htm>

From squid3 at treenet.co.nz  Fri Jun 23 20:47:29 2017
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sat, 24 Jun 2017 08:47:29 +1200
Subject: [squid-users] Shared cache directory.
In-Reply-To: <1498233911793-4682894.post@n4.nabble.com>
References: <1498233911793-4682894.post@n4.nabble.com>
Message-ID: <3c358725-5e1a-b164-9efc-bb89fd8a523a@treenet.co.nz>

On 24/06/17 04:05, Eduardo Carneiro wrote:
> Hi everyone.
> 
> Squid version 3.5.19.
> 
> I need to set up a load balancing. It would be something like, 3 servers
> decentralizing the accesses. I already have that solution.
> 
> But when I decentralize the cache directories, my "HIT rates" decrease.

Correct.


> 
> I'd like to know if there is any way to have more than one squid server
> sharing the same cache directory.

Not in the way you seem to be thinking of.


> 
> I have already tried it using cache_peer, with "follow_x_forwarded_for"
> function to do this. But, due to the use of ssl_bump, this solution did not
> answer me because, in https requests, the client IP was not shown.

Doing SSL-Bump effectively requires that the proxy terminating the TLS 
be the one caching. Passing the traffic in to a peer has major problems 
with cert mimic'ing.


If you are intercepting port 443, you should be able to LB by 
destination-IP to maximize the hit ratio.

That implies a traditional CARP cache_peer installation which is the 
solution for this problem in plain-HTTP should work almost as well for 
HTTPS. Just do the CARP based on destination-IP for the fake CONNECT 
requests the 'intercept' https_port generates - and put SSL-Bump in the 
backends which are caching.


Amos


From rentorbuy at yahoo.com  Mon Jun 26 08:46:56 2017
From: rentorbuy at yahoo.com (Vieri)
Date: Mon, 26 Jun 2017 08:46:56 +0000 (UTC)
Subject: [squid-users] ACLs allow/deny logic
References: <338112759.2261098.1498466816031.ref@mail.yahoo.com>
Message-ID: <338112759.2261098.1498466816031@mail.yahoo.com>

Hi,

I'd like to allow by default and deny only according to the ACLs I define.

Here's an example with Telegram. I'd like to deny all application/octet-stream mime types in requests and replies except for a set of IP addresses or domains.

acl denied_restricted1_mimetypes_req req_mime_type -i "/usr/local/proxy-settings/denied.restricted1.mimetypes"
acl denied_restricted1_mimetypes_rep rep_mime_type -i "/usr/local/proxy-settings/denied.restricted1.mimetypes"
acl allowed_restricted1_domains dstdomain -i "/usr/local/proxy-settings/allowed.restricted1.domains"
acl allowed_restricted1_ips dst "/usr/local/proxy-settings/allowed.restricted1.ips"
http_access deny denied_restricted1_mimetypes_req !allowed_restricted1_domains !allowed_restricted1_ips
http_reply_access deny denied_restricted1_mimetypes_rep !allowed_restricted1_domains !allowed_restricted1_ips

# cat /usr/local/proxy-settings/allowed.restricted1.domains
.telegram.org

# cat /usr/local/proxy-settings/allowed.restricted1.ips
149.154.167.91
149.154.165.120

# cat /usr/local/proxy-settings/denied.restricted1.mimetypes
^application/octet-stream$

I see this in access.log:

1498463484.530    413 10.215.144.237 TCP_DENIED_REPLY/403 4085 POST http://149.154.165.120/api - ORIGINAL_DST/149.154.165.120 text/html

I searched for the relevant parts in cache.log:

# grep -e "^2017/06/26 09:51:24.48[0-4]" /var/log/squid/cache.test.log_JL
2017/06/26 09:51:24.480 kid1| 28,3| Checklist.cc(70) preCheck: 0x80de0648 checking slow rules
2017/06/26 09:51:24.480 kid1| 28,5| Acl.cc(138) matches: checking http_reply_access
2017/06/26 09:51:24.480 kid1| 28,5| Checklist.cc(400) bannedAction: Action 'DENIED/0is not banned
2017/06/26 09:51:24.480 kid1| 28,5| Acl.cc(138) matches: checking http_reply_access#1
2017/06/26 09:51:24.480 kid1| 28,5| Acl.cc(138) matches: checking denied_filetypes
2017/06/26 09:51:24.480 kid1| 28,3| RegexData.cc(51) match: aclRegexData::match: checking '/api'
2017/06/26 09:51:24.480 kid1| 28,3| RegexData.cc(62) match: aclRegexData::match: looking for '(\.ade(\?.*)?$)|(\.adp(\?.*)?$)|(\.app(\?.*)?$)|(\.asd(\?.*)?$)|(\.asf(\?.*)?$)|(\.asx(\?.*)?$)|(\.avi(\?.*)?$)|(\.bas(\?.*)?$)|(\.bat(\?.*)?$)|(\.cab(\?.*)?$)|(\.chm(\?.*)?$)|(\.cmd(\?.*)?$)|(\.cpl(\?.*)?$)|(\.dll$)|(\.exe(\?.*)?$)|(\.fxp(\?.*)?$)|(\.hlp(\?.*)?$)|(\.hta(\?.*)?$)|(\.hto(\?.*)?$)|(\.inf(\?.*)?$)|(\.ini(\?.*)?$)|(\.ins(\?.*)?$)|(\.iso(\?.*)?$)|(\.isp(\?.*)?$)|(\.jse(.?)(\?.*)?$)|(\.jse(\?.*)?$)|(\.lib(\?.*)?$)|(\.lnk(\?.*)?$)|(\.mar(\?.*)?$)|(\.mdb(\?.*)?$)|(\.mde(\?.*)?$)|(\.mp3(\?.*)?$)|(\.mpeg(\?.*)?$)|(\.mpg(\?.*)?$)|(\.msc(\?.*)?$)|(\.msi(\?.*)?$)|(\.msp(\?.*)?$)|(\.mst(\?.*)?$)|(\.ocx(\?.*)?$)|(\.pcd(\?.*)?$)|(\.pif(\?.*)?$)|(\.prg(\?.*)?$)|(\.reg(\?.*)?$)|(\.scr(\?.*)?$)|(\.sct(\?.*)?$)|(\.sh(\?.*)?$)|(\.shb(\?.*)?$)|(\.shs(\?.*)?$)|(\.sys(\?.*)?$)|(\.url(\?.*)?$)|(\.vb(\?.*)?$)|(\.vbe(\?.*)?$)|(\.vbs(\?.*)?$)|(\.vcs(\?.*)?$)|(\.vxd(\?.*)?$)|(\.wmd(\?.*)?$)|(\.wms(\?.*)?$)|(\.wmv(\?.*)?$)|(\.wmz(\?.*)?$)|(\.wsc(\?.*)?$)|(\.wsf(\?.*)?$)|(\.wsh(\?.*)?$)'
2017/06/26 09:51:24.480 kid1| 28,3| Acl.cc(158) matches: checked: denied_filetypes = 0
2017/06/26 09:51:24.480 kid1| 28,3| Acl.cc(158) matches: checked: http_reply_access#1 = 0
2017/06/26 09:51:24.480 kid1| 28,5| Checklist.cc(400) bannedAction: Action 'DENIED/0is not banned
2017/06/26 09:51:24.480 kid1| 28,5| Acl.cc(138) matches: checking http_reply_access#2
2017/06/26 09:51:24.480 kid1| 28,5| Acl.cc(138) matches: checking denied_mimetypes_rep
2017/06/26 09:51:24.480 kid1| 28,3| RegexData.cc(51) match: aclRegexData::match: checking 'application/octet-stream'
2017/06/26 09:51:24.480 kid1| 28,3| RegexData.cc(62) match: aclRegexData::match: looking for '(^application/ecmascript$)|(^application/oebps-package+xml$)|(^application/vnd.amazon.ebook$)|(^application/vnd.android.package-archive$)|(^application/vnd.gmx$)|(^application/vnd.google-earth.kml+xml$)|(^application/vnd.google-earth.kmz$)|(^application/vnd.ms-cab-compressed$)|(^application/vnd.ms-excel.addin.macroenabled.12$)|(^application/vnd.ms-excel.sheet.binary.macroenabled.12$)|(^application/vnd.ms-excel.sheet.macroenabled.12$)|(^application/vnd.ms-excel.template.macroenabled.12$)|(^application/vnd.ms-powerpoint.addin.macroenabled.12$)|(^application/vnd.ms-powerpoint.presentation.macroenabled.12$)|(^application/vnd.ms-powerpoint.slide.macroenabled.12$)|(^application/vnd.ms-powerpoint.slideshow.macroenabled.12$)|(^application/vnd.ms-powerpoint.template.macroenabled.12$)|(^application/vnd.ms-wpl$)|(^application/vnd.ms.wms-hdr.asfv1$)|(^application/vnd.realvnc.bed$)|(^application/vnd.tmobile-livetv$)|(^application/x-authorware-bin$)|(^application/x-cab$)|(^application/x-iso9660-image$)|(^application/x-mms-framed$)|(^application/x-ms-wm$)|(^application/x-msdos-program$)|(^application/x-msdownload$)|(^application/x-shar$)|(^application/x-vbs$)|(^text/vbs$)|(^text/vbscript$)'
2017/06/26 09:51:24.480 kid1| 28,3| Acl.cc(158) matches: checked: denied_mimetypes_rep = 0
2017/06/26 09:51:24.480 kid1| 28,3| Acl.cc(158) matches: checked: http_reply_access#2 = 0
2017/06/26 09:51:24.480 kid1| 28,5| Checklist.cc(400) bannedAction: Action 'DENIED/0is not banned
2017/06/26 09:51:24.480 kid1| 28,5| Acl.cc(138) matches: checking http_reply_access#3
2017/06/26 09:51:24.480 kid1| 28,5| Acl.cc(138) matches: checking denied_extra1_mimetypes_rep
2017/06/26 09:51:24.480 kid1| 28,3| RegexData.cc(51) match: aclRegexData::match: checking 'application/octet-stream'
2017/06/26 09:51:24.480 kid1| 28,3| RegexData.cc(62) match: aclRegexData::match: looking for '(^application/mp21$)|(^application/mp4$)|(^application/vnd.rn-realmedia$)|(^application/vnd.tmobile-livetv$)|(^audio/)|(^video/)'
2017/06/26 09:51:24.480 kid1| 28,3| Acl.cc(158) matches: checked: denied_extra1_mimetypes_rep = 0
2017/06/26 09:51:24.480 kid1| 28,3| Acl.cc(158) matches: checked: http_reply_access#3 = 0
2017/06/26 09:51:24.480 kid1| 28,5| Checklist.cc(400) bannedAction: Action 'DENIED/0is not banned
2017/06/26 09:51:24.480 kid1| 28,5| Acl.cc(138) matches: checking http_reply_access#4
2017/06/26 09:51:24.480 kid1| 28,5| Acl.cc(138) matches: checking denied_restricted1_mimetypes_rep
2017/06/26 09:51:24.480 kid1| 28,3| RegexData.cc(51) match: aclRegexData::match: checking 'application/octet-stream'
2017/06/26 09:51:24.480 kid1| 28,3| RegexData.cc(62) match: aclRegexData::match: looking for '(^application/octet-stream$)'
2017/06/26 09:51:24.480 kid1| 28,2| RegexData.cc(73) match: aclRegexData::match: match '(^application/octet-stream$)' found in 'application/octet-stream'
2017/06/26 09:51:24.480 kid1| 28,3| Acl.cc(158) matches: checked: denied_restricted1_mimetypes_rep = 1
2017/06/26 09:51:24.480 kid1| 28,5| Acl.cc(138) matches: checking !allowed_ips
2017/06/26 09:51:24.480 kid1| 28,5| Acl.cc(138) matches: checking allowed_ips
2017/06/26 09:51:24.480 kid1| 28,3| Ip.cc(539) match: aclIpMatchIp: '10.215.144.237' NOT found
2017/06/26 09:51:24.480 kid1| 28,3| Acl.cc(158) matches: checked: allowed_ips = 0
2017/06/26 09:51:24.480 kid1| 28,3| Acl.cc(158) matches: checked: !allowed_ips = 1
2017/06/26 09:51:24.480 kid1| 28,5| Acl.cc(138) matches: checking !allowed_restricted1_domains
2017/06/26 09:51:24.480 kid1| 28,5| Acl.cc(138) matches: checking allowed_restricted1_domains
2017/06/26 09:51:24.480 kid1| 28,3| DomainData.cc(108) match: aclMatchDomainList: checking '149.154.165.120'
2017/06/26 09:51:24.480 kid1| 28,3| DomainData.cc(113) match: aclMatchDomainList: '149.154.165.120' NOT found
2017/06/26 09:51:24.481 kid1| 14,4| ipcache.cc(810) ipcacheCheckNumeric: ipcacheCheckNumeric: HIT_BYPASS for '149.154.165.120' == 149.154.165.120
2017/06/26 09:51:24.481 kid1| 28,3| DestinationDomain.cc(85) match: aclMatchAcl: Can't yet compare 'allowed_restricted1_domains' ACL for '149.154.165.120'
2017/06/26 09:51:24.481 kid1| 35,4| fqdncache.cc(425) fqdncache_nbgethostbyaddr: fqdncache_nbgethostbyaddr: Name '149.154.165.120'.
2017/06/26 09:51:24.481 kid1| 35,4| fqdncache.cc(447) fqdncache_nbgethostbyaddr: fqdncache_nbgethostbyaddr: HIT for '149.154.165.120'
2017/06/26 09:51:24.481 kid1| 28,3| DomainData.cc(108) match: aclMatchDomainList: checking 'none'
2017/06/26 09:51:24.481 kid1| 28,3| DomainData.cc(113) match: aclMatchDomainList: 'none' NOT found
2017/06/26 09:51:24.481 kid1| 28,3| Acl.cc(158) matches: checked: allowed_restricted1_domains = 0
2017/06/26 09:51:24.481 kid1| 28,3| Acl.cc(158) matches: checked: !allowed_restricted1_domains = 1
2017/06/26 09:51:24.481 kid1| 28,5| Acl.cc(138) matches: checking !allowed_restricted1_ips
2017/06/26 09:51:24.481 kid1| 28,5| Acl.cc(138) matches: checking allowed_restricted1_ips
2017/06/26 09:51:24.481 kid1| 28,3| Ip.cc(539) match: aclIpMatchIp: '149.154.165.120:80' NOT found
2017/06/26 09:51:24.481 kid1| 28,3| Acl.cc(158) matches: checked: allowed_restricted1_ips = 0
2017/06/26 09:51:24.481 kid1| 28,3| Acl.cc(158) matches: checked: !allowed_restricted1_ips = 1
2017/06/26 09:51:24.481 kid1| 28,3| Acl.cc(158) matches: checked: http_reply_access#4 = 1
2017/06/26 09:51:24.481 kid1| 28,3| Acl.cc(158) matches: checked: http_reply_access = 1
2017/06/26 09:51:24.481 kid1| 28,3| Checklist.cc(63) markFinished: 0x80de0648 answer DENIED for match
2017/06/26 09:51:24.481 kid1| 28,3| Checklist.cc(163) checkCallback: ACLChecklist::checkCallback: 0x80de0648 answer=DENIED
2017/06/26 09:51:24.481 kid1| 88,2| client_side_reply.cc(2001) processReplyAccessResult: The reply for POST http://149.154.165.120/api is DENIED, because it matched allowed_restricted1_ips
2017/06/26 09:51:24.481 kid1| 90,3| store_client.cc(664) storeUnregister: storeUnregister: called for '3333CC1501BBE277B139F5F07A4F1141'
2017/06/26 09:51:24.481 kid1| 20,3| store.cc(484) lock: storeUnregister locked key 3333CC1501BBE277B139F5F07A4F1141 e:=p2XDIV/0x80d96640*4
2017/06/26 09:51:24.481 kid1| 90,3| store_client.cc(758) storePendingNClients: storePendingNClients: returning 0
2017/06/26 09:51:24.481 kid1| 90,3| store_client.cc(768) CheckQuickAbortIsReasonable: entry=0x80d96640, mem=0x814b8720
2017/06/26 09:51:24.481 kid1| 90,3| store_client.cc(771) CheckQuickAbortIsReasonable: quick-abort? YES !mem->request->flags.cachable
2017/06/26 09:51:24.481 kid1| 20,3| store.cc(484) lock: StoreEntry::abort locked key 3333CC1501BBE277B139F5F07A4F1141 e:=p2XDIV/0x80d96640*5
2017/06/26 09:51:24.481 kid1| 90,3| store_client.cc(732) invokeHandlers: InvokeHandlers: 3333CC1501BBE277B139F5F07A4F1141
2017/06/26 09:51:24.481 kid1| 20,3| store_swapout.cc(273) swapOutFileClose: storeSwapOutFileClose: 3333CC1501BBE277B139F5F07A4F1141 how=1
2017/06/26 09:51:24.481 kid1| 20,3| store_swapout.cc(274) swapOutFileClose: storeSwapOutFileClose: sio = 0
2017/06/26 09:51:24.481 kid1| 20,3| store.cc(522) unlock: StoreEntry::abort unlocking key 3333CC1501BBE277B139F5F07A4F1141 e:=sp2XDINVA/0x80d96640*5
2017/06/26 09:51:24.481 kid1| 20,3| store.cc(522) unlock: storeUnregister unlocking key 3333CC1501BBE277B139F5F07A4F1141 e:=sp2XDINVA/0x80d96640*4
2017/06/26 09:51:24.481 kid1| 20,3| store.cc(522) unlock: clientReplyContext::removeStoreReference unlocking key 3333CC1501BBE277B139F5F07A4F1141 e:=sp2XDINVA/0x80d96640*3
2017/06/26 09:51:24.481 kid1| 20,3| store.cc(779) storeCreatePureEntry: storeCreateEntry: 'http://149.154.165.120/api'
2017/06/26 09:51:24.481 kid1| 20,5| store.cc(371) StoreEntry: StoreEntry constructed, this=0x80ba5460
2017/06/26 09:51:24.481 kid1| 20,3| MemObject.cc(97) MemObject: new MemObject 0x80b902e8
2017/06/26 09:51:24.481 kid1| 20,3| store.cc(500) setReleaseFlag: StoreEntry::setReleaseFlag: '[null_store_key]'
2017/06/26 09:51:24.481 kid1| 20,3| store_key_md5.cc(89) storeKeyPrivate: storeKeyPrivate: POST http://149.154.165.120/api
2017/06/26 09:51:24.481 kid1| 20,3| store.cc(448) hashInsert: StoreEntry::hashInsert: Inserting Entry e:=XI/0x80ba5460*0 key 'CCEA5776796B6352934736B5664CDAEA'
2017/06/26 09:51:24.481 kid1| 20,3| store.cc(484) lock: storeCreateEntry locked key CCEA5776796B6352934736B5664CDAEA e:=XIV/0x80ba5460*1
2017/06/26 09:51:24.481 kid1| 90,3| store_client.cc(200) copy: store_client::copy: CCEA5776796B6352934736B5664CDAEA, from 0, for length 4096, cb 1, cbdata 0x8172e1a0
2017/06/26 09:51:24.481 kid1| 20,3| store.cc(484) lock: store_client::copy locked key CCEA5776796B6352934736B5664CDAEA e:=XIV/0x80ba5460*2
2017/06/26 09:51:24.481 kid1| 90,3| store_client.cc(297) storeClientCopy2: storeClientCopy2: CCEA5776796B6352934736B5664CDAEA
2017/06/26 09:51:24.482 kid1| 33,5| store_client.cc(329) doCopy: store_client::doCopy: co: 0, hi: 0
2017/06/26 09:51:24.482 kid1| 90,3| store_client.cc(341) doCopy: store_client::doCopy: Waiting for more
2017/06/26 09:51:24.482 kid1| 20,3| store.cc(522) unlock: store_client::copy unlocking key CCEA5776796B6352934736B5664CDAEA e:=XIV/0x80ba5460*2
2017/06/26 09:51:24.482 kid1| 4,4| errorpage.cc(603) errorAppendEntry: Creating an error page for entry 0x80ba5460 with errorstate 0x80e430e0 page id 1
2017/06/26 09:51:24.482 kid1| 6,5| disk.cc(71) file_open: file_open: FD 79
2017/06/26 09:51:24.482 kid1| 51,3| fd.cc(198) fd_open: fd_open() FD 79 /usr/share/squid/errors/ERR_ACCESS_DENIED
2017/06/26 09:51:24.482 kid1| 6,5| disk.cc(126) file_close: file_close: FD 79 really closing
2017/06/26 09:51:24.482 kid1| 51,3| fd.cc(93) fd_close: fd_close FD 79 /usr/share/squid/errors/ERR_ACCESS_DENIED
2017/06/26 09:51:24.482 kid1| 5,5| ModEpoll.cc(116) SetSelect: FD 79, type=1, handler=0, client_data=0, timeout=0
2017/06/26 09:51:24.482 kid1| 5,5| ModEpoll.cc(116) SetSelect: FD 79, type=2, handler=0, client_data=0, timeout=0
2017/06/26 09:51:24.482 kid1| 4,3| errorpage.cc(1101) Convert: errorConvert: %%l --> '/*
2017/06/26 09:51:24.482 kid1| 4,3| errorpage.cc(1101) Convert: errorConvert: %%; --> '%;'
2017/06/26 09:51:24.482 kid1| 4,3| errorpage.cc(1101) Convert: errorConvert: %%c --> 'ERR_ACCESS_DENIED'
2017/06/26 09:51:24.482 kid1| 4,3| errorpage.cc(1101) Convert: errorConvert: %%U --> 'http://149.154.165.120/api'
[trimmed]
2017/06/26 09:51:24.483 kid1| 20,2| store.cc(954) checkCachable: StoreEntry::checkCachable: NO: not cachable
2017/06/26 09:51:24.483 kid1| 20,3| store_swapout.cc(381) mayStartSwapOut: not cachable
2017/06/26 09:51:24.483 kid1| 20,2| store.cc(954) checkCachable: StoreEntry::checkCachable: NO: not cachable
2017/06/26 09:51:24.483 kid1| 90,3| store_client.cc(732) invokeHandlers: InvokeHandlers: CCEA5776796B6352934736B5664CDAEA
2017/06/26 09:51:24.483 kid1| 90,3| store_client.cc(738) invokeHandlers: StoreEntry::InvokeHandlers: checking client #0
2017/06/26 09:51:24.483 kid1| 90,3| store_client.cc(297) storeClientCopy2: storeClientCopy2: CCEA5776796B6352934736B5664CDAEA
2017/06/26 09:51:24.483 kid1| 33,5| store_client.cc(329) doCopy: store_client::doCopy: co: 0, hi: 3960
2017/06/26 09:51:24.483 kid1| 90,3| store_client.cc(433) scheduleMemRead: store_client::doCopy: Copying normal from memory
2017/06/26 09:51:24.483 kid1| 88,5| client_side_reply.cc(2154) sendMoreData: clientReplyContext::sendMoreData: http://149.154.165.120/api, 3960 bytes (3960 new bytes)
2017/06/26 09:51:24.483 kid1| 88,5| client_side_reply.cc(2158) sendMoreData: clientReplyContext::sendMoreData:local=149.154.165.120:80 remote=10.215.144.237 FD 56 flags=17 'http://149.154.165.120/api' out.offset=0
2017/06/26 09:51:24.483 kid1| 88,2| client_side_reply.cc(2001) processReplyAccessResult: The reply for POST http://149.154.165.120/api is ALLOWED, because it matched allowed_restricted1_ips
2017/06/26 09:51:24.483 kid1| 20,3| store.cc(484) lock: ClientHttpRequest::loggingEntry locked key CCEA5776796B6352934736B5664CDAEA e:=XIV/0x80ba5460*3
2017/06/26 09:51:24.483 kid1| 88,3| client_side_reply.cc(2039) processReplyAccessResult: clientReplyContext::sendMoreData: Appending 3711 bytes after 249 bytes of headers
2017/06/26 09:51:24.484 kid1| 87,3| clientStream.cc(162) clientStreamCallback: clientStreamCallback: Calling 1 with cbdata 0x8172e184 from node 0x80b74508
2017/06/26 09:51:24.484 kid1| 11,2| client_side.cc(1391) sendStartOfMessage: HTTP Client local=149.154.165.120:80 remote=10.215.144.237 FD 56 flags=17
2017/06/26 09:51:24.484 kid1| 11,2| client_side.cc(1392) sendStartOfMessage: HTTP Client REPLY:

I see 2 apparently contradictory log messages (well, for me that is -- I'm still learning how to read the log):
The reply for POST http://149.154.165.120/api is DENIED, because it matched allowed_restricted1_ips
The reply for POST http://149.154.165.120/api is ALLOWED, because it matched allowed_restricted1_ips

Why is this happening?

Thanks,

Vieri


From emmanuel.fuste at thalesgroup.com  Mon Jun 26 09:53:18 2017
From: emmanuel.fuste at thalesgroup.com (FUSTE Emmanuel)
Date: Mon, 26 Jun 2017 11:53:18 +0200
Subject: [squid-users] annotation and fast / slow acl
In-Reply-To: <a82ca9cc-b8be-87ea-28f0-57e5afebfa9e@treenet.co.nz>
References: <d39110f9-fa86-6a50-756f-8ea0fab848f9@thalesgroup.com>
 <597649ba-3928-1e4d-a16e-a09f01248bca@treenet.co.nz>
 <ee5346c1-864f-84ac-de98-abb789f1af5e@thalesgroup.com>
 <ec74b885-0a3f-f1e7-d29d-8092a076b6de@thalesgroup.com>
 <a82ca9cc-b8be-87ea-28f0-57e5afebfa9e@treenet.co.nz>
Message-ID: <f01e3b1e-2ea1-77c9-3a86-58774b57e8b3@thalesgroup.com>

Le 21/06/2017 ? 19:17, Amos Jeffries a ?crit :
> On 22/06/17 04:51, FUSTE Emmanuel wrote:
>> So arbitrary k- v pair not used by the acl helper protocol could be
>> matched against with the note acl ?
>> How it relate to the defined/reserved tag= and clt_conn_tag= keywords of
>> the acl helper protocol ?
> They are all attached as annotations on the transaction. The only ones
> excluded are the security tokens in Kerberos/NTLM/Digest auth.
>
> Amos
>
Hello,

One more time, thank you!
All is working beautifully now with annotation  and note acl for peer 
selection.

Emmanuel.

From squid3 at treenet.co.nz  Mon Jun 26 09:53:46 2017
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Mon, 26 Jun 2017 21:53:46 +1200
Subject: [squid-users] ACLs allow/deny logic
In-Reply-To: <338112759.2261098.1498466816031@mail.yahoo.com>
References: <338112759.2261098.1498466816031.ref@mail.yahoo.com>
 <338112759.2261098.1498466816031@mail.yahoo.com>
Message-ID: <2ea4a067-fb39-c820-5823-769711294482@treenet.co.nz>

On 26/06/17 20:46, Vieri wrote:
> Hi,
> 
> I'd like to allow by default and deny only according to the ACLs I define.
> 
> Here's an example with Telegram. I'd like to deny all application/octet-stream mime types in requests and replies except for a set of IP addresses or domains.

Er, deny is the opposite of allow. So your "example" is to demonstrate 
the _opposite_ of what you want?


Not to mention that what you want is the opposite of a well-known 
Security Best-Practice. Well, your call, but when things go terribly 
wrong don't say you weren't warned.


Anyhow ...

> 
> acl denied_restricted1_mimetypes_req req_mime_type -i "/usr/local/proxy-settings/denied.restricted1.mimetypes"
> acl denied_restricted1_mimetypes_rep rep_mime_type -i "/usr/local/proxy-settings/denied.restricted1.mimetypes"
> acl allowed_restricted1_domains dstdomain -i "/usr/local/proxy-settings/allowed.restricted1.domains"
> acl allowed_restricted1_ips dst "/usr/local/proxy-settings/allowed.restricted1.ips"
> http_access deny denied_restricted1_mimetypes_req !allowed_restricted1_domains !allowed_restricted1_ips
> http_reply_access deny denied_restricted1_mimetypes_rep !allowed_restricted1_domains !allowed_restricted1_ips
> 

A line of ACLS is an "AND" condition:

  http_access deny (if) X (and) Y (and) Z

This configuration will deny the mime types except when they come from 
certain IPs *AND* are going to certain domains.


To enact your stated policy you need to do it this way:

  http_access allow allowed_restricted1_domains \
     denied_restricted1_mimetypes_req

  http_access allow allowed_restricted1_ips \
     denied_restricted1_mimetypes_req

  http_access deny denied_restricted1_mimetypes_req


.. same sort of thing for the reply lines.


> # cat /usr/local/proxy-settings/allowed.restricted1.domains
> .telegram.org
> 
> # cat /usr/local/proxy-settings/allowed.restricted1.ips
> 149.154.167.91
> 149.154.165.120
> 
> # cat /usr/local/proxy-settings/denied.restricted1.mimetypes
> ^application/octet-stream$
> 
> I see this in access.log:
> 
> 1498463484.530    413 10.215.144.237 TCP_DENIED_REPLY/403 4085 POST http://149.154.165.120/api - ORIGINAL_DST/149.154.165.120 text/html
> 
> I searched for the relevant parts in cache.log:
> 
<snip>

> I see 2 apparently contradictory log messages (well, for me that is -- I'm still learning how to read the log):
> The reply for POST http://149.154.165.120/api is DENIED, because it matched allowed_restricted1_ips
> The reply for POST http://149.154.165.120/api is ALLOWED, because it matched allowed_restricted1_ips
> 
> Why is this happening?

The servers reply (application/octet-stream) is being denied, and the 
Squid generated error page (text/html) is being allowed.

When a default / implicit action is being done the "matched X" shows the 
name of the last ACL processed - that ACL non-matching was the reason 
for the default/implicit action happening.

Amos


From rafael.akchurin at diladele.com  Mon Jun 26 11:24:15 2017
From: rafael.akchurin at diladele.com (Rafael Akchurin)
Date: Mon, 26 Jun 2017 11:24:15 +0000
Subject: [squid-users] Squid 3.5.26 for Microsoft Windows 64-bit is available
Message-ID: <DB6PR0401MB26800D631A2DBA3DFC4D207F8FDF0@DB6PR0401MB2680.eurprd04.prod.outlook.com>

Greetings everyone,

The CygWin based build of Squid proxy for Microsoft Windows version 3.5.26 is now available (amd64 only!).

* Original release notes are at http://www.squid-cache.org/Versions/v3/3.5/squid-3.5.26-RELEASENOTES.html .
* Ready to use MSI package can be downloaded from http://squid.diladele.com .
* List of open issues for the installer - https://github.com/diladele/squid-windows/issues

Thanks a lot for Squid developers for making this great software!

Please join our humble efforts to provide ready to run MSI installer for Squid on Microsoft Windows with all required dependencies at GitHub -
https://github.com/diladele/squid-windows . Report all issues/bugs/feature requests at GitHub project.
Issues about the *MSI installer only* can also be reported to support at diladele.com<mailto:support at diladele.com> .

Best regards,
Rafael Akchurin
Diladele B.V.
https://www.diladele.com

--
Please take a look at Web Safety - our ICAP based web filter server for Squid proxy.
The upcoming version 5.1 with "Bypass Blocked Page" functionality is available from https://www.diladele.com/download_next_version.html



-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20170626/47dd095d/attachment.htm>

From webmaster at squidblacklist.org  Mon Jun 26 11:25:09 2017
From: webmaster at squidblacklist.org (Benjamin E. Nichols)
Date: Mon, 26 Jun 2017 06:25:09 -0500
Subject: [squid-users] Squid 3.5.26 for Microsoft Windows 64-bit is
 available
In-Reply-To: <DB6PR0401MB26800D631A2DBA3DFC4D207F8FDF0@DB6PR0401MB2680.eurprd04.prod.outlook.com>
References: <DB6PR0401MB26800D631A2DBA3DFC4D207F8FDF0@DB6PR0401MB2680.eurprd04.prod.outlook.com>
Message-ID: <72e67c24-baa0-f8a2-fe2b-f216b8a32db7@squidblacklist.org>

Thank you!  :P

Greetz everyone!


On 6/26/2017 6:24 AM, Rafael Akchurin wrote:
>
> Greetings everyone,
>
> The CygWin based build of Squid proxy for Microsoft Windows version 
> 3.5.26 is now available (amd64 only!).
>
> * Original release notes are at 
> http://www.squid-cache.org/Versions/v3/3.5/squid-3.5.26-RELEASENOTES.html 
> .
>
> * Ready to use MSI package can be downloaded from 
> http://squid.diladele.com <http://squid.diladele.com> .
>
> * List of open issues for the installer - 
> https://github.com/diladele/squid-windows/issues 
> <https://github.com/diladele/squid-windows/issues>
>
> Thanks a lot for Squid developers for making this great software!
>
> Please join our humble efforts to provide ready to run MSI installer 
> for Squid on Microsoft Windows with all required dependencies at GitHub -
>
> https://github.com/diladele/squid-windows . Report all 
> issues/bugs/feature requests at GitHub project.
>
> Issues about the *MSI installer only* can also be reported to 
> support at diladele.com <mailto:support at diladele.com> .
>
> Best regards,
>
> Rafael Akchurin
>
> Diladele B.V.
>
> https://www.diladele.com
>
> --
>
> Please take a look at Web Safety - our ICAP based web filter server 
> for Squid proxy.
>
> The upcoming version 5.1 with ?Bypass Blocked Page? functionality is 
> available from https://www.diladele.com/download_next_version.html
>
>
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users

-- 
--

Signed,

Benjamin E. Nichols
http://www.squidblacklist.org

1-405-397-1360 - Call Anytime.

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20170626/f085e9f8/attachment.htm>

From rentorbuy at yahoo.com  Mon Jun 26 13:01:23 2017
From: rentorbuy at yahoo.com (Vieri)
Date: Mon, 26 Jun 2017 13:01:23 +0000 (UTC)
Subject: [squid-users] ACLs allow/deny logic
In-Reply-To: <2ea4a067-fb39-c820-5823-769711294482@treenet.co.nz>
References: <338112759.2261098.1498466816031.ref@mail.yahoo.com>
 <338112759.2261098.1498466816031@mail.yahoo.com>
 <2ea4a067-fb39-c820-5823-769711294482@treenet.co.nz>
Message-ID: <1580316897.2345568.1498482083127@mail.yahoo.com>

________________________________
From: Amos Jeffries <squid3 at treenet.co.nz>
>> I'd like to allow by default and deny only according to the ACLs I define.
>> 

>> Here's an example with Telegram. I'd like to deny all application/octet-stream mime types in requests 

>> and replies except for a set of IP addresses or domains.>
> Er, deny is the opposite of allow. So your "example" is to demonstrate 
> the _opposite_ of what you want?
> 

> Not to mention that what you want is the opposite of a well-known 

> Security Best-Practice. Well, your call, but when things go terribly 
> wrong don't say you weren't warned.

My sentence was misleading, I suppose.
My squid.conf has the following structure (which I believe is close to the default for a caching http proxy):

ACL definitions

http_access deny ...
http_reply_access deny ...

http_access deny intercepted !localnet

http_access allow localnethttp_access deny all

Is there anything wrong with this?

Vieri


From bloggerrazorcross at gmail.com  Mon Jun 26 16:11:49 2017
From: bloggerrazorcross at gmail.com (Razor Cross)
Date: Mon, 26 Jun 2017 11:11:49 -0500
Subject: [squid-users] Squid caching bad objects
Message-ID: <CANRy4AV5n4m8PC7fKQJfaZboKxyUG7o7ASuJABRhVfFFfmzjhg@mail.gmail.com>

Hi, Amos,
We are using squid 3.5. for our server. Recently we have noticed that squid
is caching incomplete objects in case of chunked response.

We have gone through the squid code. It looks likes squid is caching
incomplete response in case of EOF from the server even though it does not
receive the last empty chunk.


 if (eof) // already reached EOF
        return COMPLETE_NONPERSISTENT_MSG;


As per RFC,

 "The chunked encoding is ended by any chunk whose size is
   zero, followed by the trailer, which is terminated by an empty line."



Is this expected? Because of this problem, our server ends up serving bad
objects to the user.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20170626/ee3f106c/attachment.htm>

From rousskov at measurement-factory.com  Mon Jun 26 17:06:47 2017
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Mon, 26 Jun 2017 11:06:47 -0600
Subject: [squid-users] Squid caching bad objects
In-Reply-To: <CANRy4AV5n4m8PC7fKQJfaZboKxyUG7o7ASuJABRhVfFFfmzjhg@mail.gmail.com>
References: <CANRy4AV5n4m8PC7fKQJfaZboKxyUG7o7ASuJABRhVfFFfmzjhg@mail.gmail.com>
Message-ID: <a494df2a-eb3d-acd4-ea08-2bc615348c45@measurement-factory.com>

On 06/26/2017 10:11 AM, Razor Cross wrote:

> We are using squid 3.5. for our server. Recently we have noticed that
> squid is caching incomplete objects in case of chunked response.
> 
> We have gone through the squid code. It looks likes squid is caching
> incomplete response in case of EOF from the server even though it does
> not receive the last empty chunk.
> 
> 
>  if (eof) // already reached EOF
>         return COMPLETE_NONPERSISTENT_MSG;

You are looking at the wrong code. HttpStateData::persistentConnStatus()
and related *_MSG codes do not determine whether the entire object was
received. They determine whether

(a) Squid should expect more response bytes and

(b) The connection can be kept open if no more response bytes are expected.

The COMPLETE_NONPERSISTENT_MSG return value is correct here (I am
ignoring the sad fact that we are abusing the word "complete" to cover
both whole and truncated responses).


> Is this expected? Because of this problem, our server ends up serving
> bad objects to the user.

What you describe sounds like a bug, but the exact code you are quoting
is not responsible for that bug. I di not study this in detail, but I
suspect that the COMPLETE_NONPERSISTENT_MSG case in
HttpStateData::processReplyBody() should be changed to call
StoreEntry::lengthWentBad("missing last-chunk") when lastChunk is false
and HttpStateData::flags.chunked is true.


HTH,

Alex.


From rtpearson at yahoo.com  Tue Jun 27 00:06:25 2017
From: rtpearson at yahoo.com (Todd Pearson)
Date: Tue, 27 Jun 2017 00:06:25 +0000 (UTC)
Subject: [squid-users] NTLM authentication worked in Squid 2.7.STABLE8 Squid
 Web Proxy, now need it in v3.5 hosted on Windows server 2k12
References: <799349610.3174840.1498521985549.ref@mail.yahoo.com>
Message-ID: <799349610.3174840.1498521985549@mail.yahoo.com>


I am hosting the squid proxy on Windows 2K12 server. ??Squid 2.7.STABLE8 Squid Web Proxy version worked well for authentication until recent Windows 10 update killed Sha1. ?Now I am upgrading to squid proxy version 3.5.x.x to restore authentication. ?
The below settings are longer available in the 3.5.x.x version since the progams do not exist for the new version:
auth_param ntlm program c:/squid/libexec/mswin_ntlm_auth.exe
external_acl_type win_domain_group %LOGIN c:/squid/libexec/mswin_check_ad_group.exe -G

What are the equivalent setting for v 3.5. ?Once again I am in windows environment. ?



-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20170627/f533378a/attachment.htm>

From bumpski3r at gmail.com  Tue Jun 27 03:28:07 2017
From: bumpski3r at gmail.com (bump skier)
Date: Mon, 26 Jun 2017 23:28:07 -0400
Subject: [squid-users] HIER_NONE on TCP_MISS?
Message-ID: <CAJDqJL8pnoXCYadBxaff37iXkcxCjfXVK0TbXNj00iHoA6khLg@mail.gmail.com>

Hi,

I'm trying to understand the following behavior I'm seeing with Squid
running in accelerator mode. In short, I'm seeing some TCP_MISS for
requests to a static javascript file which is initially cached and returned
as a cache hit. I suspect the missed cache hits are due to the cache size
being too small and the file eventually getting evicted. However, I'm
confused about what I'm seeing in the Squid access log. For some of the
cache misses I can see in the access log that Squid fetches the file from
the configured origin server but for a vast majority of them I see
HIER_NONE even though Squid is actually returning the file.

Under what situations would Squid fetch content from the origin server
during a cache miss but print HIER_NONE?

Sample Requests from Access Log (actual values have been replaced with
placeholders/fake values)

#What I would expect to see for TCP_MISS - Squid fetches from the origin
server
TIMESTAMP 1.2.3.4 TCP_MISS/200 17310 GET http://www.domain.com/myFile.js
FIRSTUP_PARENT/2.4.6.8 application/javascript

#Squid still appears to be fetching from the origin server however it
prints HIER_NONE in the access log
TIMESTAMP 4.3.2.1 TCP_MISS/200 17310 GET http://www.domain.com/myFile.js
HIER_NONE/- application/javascript

Thanks in advance for any help you can provide.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20170626/79f778a1/attachment.htm>

From rentorbuy at yahoo.com  Tue Jun 27 06:31:41 2017
From: rentorbuy at yahoo.com (Vieri)
Date: Tue, 27 Jun 2017 06:31:41 +0000 (UTC)
Subject: [squid-users] ACLs allow/deny logic
In-Reply-To: <2ea4a067-fb39-c820-5823-769711294482@treenet.co.nz>
References: <338112759.2261098.1498466816031.ref@mail.yahoo.com>
 <338112759.2261098.1498466816031@mail.yahoo.com>
 <2ea4a067-fb39-c820-5823-769711294482@treenet.co.nz>
Message-ID: <1903999605.3040033.1498545101578@mail.yahoo.com>

Please bear with me because I still don't quite grasp the AND logic with ACLs.

Let's consider the logic "http_access deny (if) X (and) Y (and) Z" and the following squid configuration section:

[squid.conf - start]
acl denied_restricted1_mimetypes_req req_mime_type -i "/usr/local/proxy-settings/denied.restricted1.mimetypes"
acl denied_restricted1_mimetypes_rep rep_mime_type -i "/usr/local/proxy-settings/denied.restricted1.mimetypes"
acl allowed_restricted1_domains dstdomain -i "/usr/local/proxy-settings/allowed.restricted1.domains"
acl allowed_restricted1_ips dst "/usr/local/proxy-settings/allowed.restricted1.ips"

http_access deny denied_restricted1_mimetypes_req !allowed_restricted1_domains !allowed_restricted1_ips
http_reply_access deny denied_restricted1_mimetypes_rep !allowed_restricted1_domains !allowed_restricted1_ips

http_access deny intercepted !localnet

http_access allow localnet

http_access deny all
[squid.conf - finish]

In particular:

http_reply_access deny (if) denied_restricted1_mimetypes_rep (and not) allowed_restricted1_domains (and not) allowed_restricted1_ips

where 

denied_restricted1_mimetypes_rep: matches mime type application/octet-stream
allowed_restricted1_domains: matches DESTINATION domain .telegram.org
allowed_restricted1_ips: matches DESTINATION IP addresses (any one of 149.154.167.91 or 149.154.165.120)

So, it should translate to something like this:

http_reply_access deny (if) (mime type is application/octet-stream) (and) (DESTINATION domain is NOT .telegram.org) (and) (DESTINATION IP address is NOT any of 149.154.167.91 or 149.154.165.120)

Correct?
If so, then I'm still struggling to understand the first message in the log:

"The reply for POST http://149.154.165.120/api is DENIED, because it matched allowed_restricted1_ips"

I don't think "the server's reply (application/octet-stream) should be denied" if it comes from one of 149.154.167.91 or 149.154.165.120.

Anyway, I'll try out the configuration directives you suggested and see if that logic applies correctly (at least to my undertsanding ;-) ).

Thanks for your valuable help,

Vieri


From danielrieken89 at gmail.com  Tue Jun 27 11:53:26 2017
From: danielrieken89 at gmail.com (Daniel Rieken)
Date: Tue, 27 Jun 2017 13:53:26 +0200
Subject: [squid-users] Block doc documents
Message-ID: <CAJtq-03UDVB462bzqG+5q0DDVVwXFqSmfUw_TrjWNeWskBmcnw@mail.gmail.com>

Hello,

I would like to block my users from downloading doc- and docm-files,
but not docx.

So this works fine for me:
/etc/squid3/blockExtensions.acl:
\.doc(\?.*)?$
\.docm(\?.*)?$

acl blockExtensions urlpath_regex -i "/etc/squid3/blockExtensions.acl"
http_access deny blockExtensions


But in some cases the URL doesn't contain the extension (e.g. doc).
For URLs like this the above ACL doesn't work:
- http://www.example.org/download.pl?file=wordfile
- http://www.example.org/invoice-5479657415/

Here I need to work with mime-types:
acl blockMime rep_mime_type application/msword
acl blockMime rep_mime_type application/vnd.ms-word.document.macroEnabled.12
http_reply_access deny blockMime

This works fine, too. But I see a problem: The mime-type is defined on
the webserver. So the badguy could configure his webserver to serve a
doc-file as application/i.am.not.a.docfile and the above ACL isn't
working anymore.
Is there any way to make squid block doc- and docm files based on the
response-headers file-type?
Or in other words: Is squid able to match the "doc" in the
Content-Disposition header of the response?

HTTP/1.0 200 OK
Date: Tue, 27 Jun 2017 11:40:57 GMT
Server: Apache Phusion_Passenger/4.0.10 mod_bwlimited/1.4
Cache-Control: no-cache, no-store, max-age=0, must-revalidate
Pragma: no-cache
Content-Type: application/baddoc
Content-Disposition: attachment;
filename="gescanntes-Dokument-VPPAW-072-JCD3032.doc"
Content-Transfer-Encoding: binary
X-Powered-By: PHP/5.3.29
Connection: close


Regards, Daniel


From bpk678 at gmail.com  Tue Jun 27 13:36:50 2017
From: bpk678 at gmail.com (brendan kearney)
Date: Tue, 27 Jun 2017 09:36:50 -0400
Subject: [squid-users] Block doc documents
In-Reply-To: <CAJtq-03UDVB462bzqG+5q0DDVVwXFqSmfUw_TrjWNeWskBmcnw@mail.gmail.com>
References: <CAJtq-03UDVB462bzqG+5q0DDVVwXFqSmfUw_TrjWNeWskBmcnw@mail.gmail.com>
Message-ID: <CAARxGtg8Vv=822V-GxWv2cDU_mC1giZ7R3mBu=2_PKR4eMx7kQ@mail.gmail.com>

You need an ICAP server intelligent enough to differentiate between the
file types.  Squid is a proxy and can only deal with the protocol.  An ICAP
server can deal with the content.  C-icap and ecap are a couple options
that seem to be available.  I havr no experience with either.

On Jun 27, 2017 7:53 AM, "Daniel Rieken" <danielrieken89 at gmail.com> wrote:

> Hello,
>
> I would like to block my users from downloading doc- and docm-files,
> but not docx.
>
> So this works fine for me:
> /etc/squid3/blockExtensions.acl:
> \.doc(\?.*)?$
> \.docm(\?.*)?$
>
> acl blockExtensions urlpath_regex -i "/etc/squid3/blockExtensions.acl"
> http_access deny blockExtensions
>
>
> But in some cases the URL doesn't contain the extension (e.g. doc).
> For URLs like this the above ACL doesn't work:
> - http://www.example.org/download.pl?file=wordfile
> - http://www.example.org/invoice-5479657415/
>
> Here I need to work with mime-types:
> acl blockMime rep_mime_type application/msword
> acl blockMime rep_mime_type application/vnd.ms-word.
> document.macroEnabled.12
> http_reply_access deny blockMime
>
> This works fine, too. But I see a problem: The mime-type is defined on
> the webserver. So the badguy could configure his webserver to serve a
> doc-file as application/i.am.not.a.docfile and the above ACL isn't
> working anymore.
> Is there any way to make squid block doc- and docm files based on the
> response-headers file-type?
> Or in other words: Is squid able to match the "doc" in the
> Content-Disposition header of the response?
>
> HTTP/1.0 200 OK
> Date: Tue, 27 Jun 2017 11:40:57 GMT
> Server: Apache Phusion_Passenger/4.0.10 mod_bwlimited/1.4
> Cache-Control: no-cache, no-store, max-age=0, must-revalidate
> Pragma: no-cache
> Content-Type: application/baddoc
> Content-Disposition: attachment;
> filename="gescanntes-Dokument-VPPAW-072-JCD3032.doc"
> Content-Transfer-Encoding: binary
> X-Powered-By: PHP/5.3.29
> Connection: close
>
>
> Regards, Daniel
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20170627/ada845f1/attachment.htm>

From eliezer at ngtech.co.il  Tue Jun 27 14:16:12 2017
From: eliezer at ngtech.co.il (Eliezer Croitoru)
Date: Tue, 27 Jun 2017 17:16:12 +0300
Subject: [squid-users] Squid Version 3.5.20
In-Reply-To: <89638057A560FB458C01C197F81C7F5D13EDCCD0@ROCKETS.amscan.corp>
References: <89638057A560FB458C01C197F81C7F5D13EDCCD0@ROCKETS.amscan.corp>
Message-ID: <066401d2ef4f$ee3dd450$cab97cf0$@ngtech.co.il>

Hey,

I can try to help you but I do not have enough logs for it.
Also it's not so simple.
Basically you will need to block gmail and google drive themselves in one
rule that will not include other google services.

All The Bests,
Eliezer

----
http://ngtech.co.il/lmgtfy/
Linux System Administrator
Mobile: +972-5-28704261
Email: eliezer at ngtech.co.il


From: squid-users [mailto:squid-users-bounces at lists.squid-cache.org] On
Behalf Of Cherukuri, Naresh
Sent: Friday, June 23, 2017 23:34
To: squid-users at lists.squid-cache.org
Subject: [squid-users] Squid Version 3.5.20

Hello All,

I installed Squid version 3.5.20 on RHEL 7 and generated selfsigned CA
certificates, can you shed some light on how to "Configure regular
expression of the Google ReCaptcha URL with ACL".

My requirement :

This requirement is to allow Google's ReCaptcha URL (HTTPS) so associates
can successfully use ADP which now utilizes Google's ReCaptcha which is
called via an HTTPS URL, without allowing users to access other
Google-related services such as Gmail or Google Drive.

Any ideas much appreciated!

Thanks,
Naresh



From eliezer at ngtech.co.il  Tue Jun 27 14:19:16 2017
From: eliezer at ngtech.co.il (Eliezer Croitoru)
Date: Tue, 27 Jun 2017 17:19:16 +0300
Subject: [squid-users] Reverse DNS Lookup for client IPs
In-Reply-To: <20170620113508.ycg4iqvdq442kska@charite.de>
References: <20170620113508.ycg4iqvdq442kska@charite.de>
Message-ID: <066601d2ef50$5c2e8360$148b8a20$@ngtech.co.il>

Hey,

Can you put a link to the thread here?
Are you talking about this thread:
http://lists.squid-cache.org/pipermail/squid-users/2016-February/008999.html

http://squid-web-proxy-cache.1019090.n4.nabble.com/Reverse-DNS-Lookup-for-client-IPs-td4675872.html

Thanks,
Eliezer

----
Eliezer Croitoru
Linux System Administrator
Mobile: +972-5-28704261
Email: eliezer at ngtech.co.il


-----Original Message-----
From: squid-users [mailto:squid-users-bounces at lists.squid-cache.org] On Behalf Of Ralf Hildebrandt
Sent: Tuesday, June 20, 2017 14:35
To: squid-users at squid-cache.org
Subject: [squid-users] Reverse DNS Lookup for client IPs

I have to chime in on the "Reverse DNS Lookup for client IPs" thread back in Feb 2016. I tried redefining the logging format for url_rewrite_extras and store_id_extras in the config, but that wouldn't work.

I had to change the file src/cf.data.pre and recompiled, after that the number of reverse lookups dropped considerably.

-- 
Ralf Hildebrandt                   Charite Universit?tsmedizin Berlin
ralf.hildebrandt at charite.de        Campus Benjamin Franklin
https://www.charite.de             Hindenburgdamm 30, 12203 Berlin
Gesch?ftsbereich IT, Abt. Netzwerk fon: +49-30-450.570.155 _______________________________________________
squid-users mailing list
squid-users at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-users



From eliezer at ngtech.co.il  Tue Jun 27 14:32:20 2017
From: eliezer at ngtech.co.il (Eliezer Croitoru)
Date: Tue, 27 Jun 2017 17:32:20 +0300
Subject: [squid-users] Header order in squid proxy
In-Reply-To: <CALSaDe0TD=sctLfc8xP_URsfRyvSo1cNm6p9+Tdsohi2sgXkhg@mail.gmail.com>
References: <CALSaDe0TD=sctLfc8xP_URsfRyvSo1cNm6p9+Tdsohi2sgXkhg@mail.gmail.com>
Message-ID: <066801d2ef52$2f351390$8d9f3ab0$@ngtech.co.il>

If I may add a word or two:
If sites are securing their systems based on headers order then I believe they are aiming at the wrong target.
It's a "nice to have" but not actual deep application level defense.(based on my low level in the subject)
One example I have seen of a DOS\DDOS issue is:
"Hey, We are having high CPU usage, what should we do?"
- The bot was hammering the service from an AWS instance ... so block it..
- How many requests per second from a single IP is considered normal?
- Then, how many *new* cookies requests per second is considered normal?
- What about NAT? would a Chinese client be considered legit despite to him being under one big NAT?
- Would you be able to differentiate between a specific single ip or subnet that is considered legit?
- What about RBL?

The above are a things I heard here or there which I think are more important than headers order.
Take my words as coming from a person which is not an expert in the security area.

All The Bests,
Eliezer

----
http://ngtech.co.il/lmgtfy/
Linux System Administrator
Mobile: +972-5-28704261
Email: eliezer at ngtech.co.il


From: squid-users [mailto:squid-users-bounces at lists.squid-cache.org] On Behalf Of Sonya Roy
Sent: Thursday, June 22, 2017 21:54
To: squid-users at lists.squid-cache.org
Subject: Re: [squid-users] Header order in squid proxy

The sites I am talking about check the User-Agent header and makes sure the user-agent is for a well-known browser, i.e. a browser that they support. And any browser like Firefox, Chrome, Safari, Edge for example, sends the headers in a certain order and the order depends on the browser. And this header order for well-known headers like Accept, Accept-Language, Accept-Encoding, Content-Length, Host, Connection, Referer, Cookie, etc. And they match the order of the received request with the standard header order for the browser for that user-agent.

This detects bots like a poorly written bot(i.e ones that don't consider this header order) using python requests or in any language for that matter where the requests are handled using a low level http requests library. 

So, keeping the header order sent from the client intact would prevent them from dropping proxied requests(ones that use squid). I know for a fact that they don't intend to block proxies.

Could you point me in the direction to where I should look for in the source code of squid? the part that handles the header data sent from the client.

With regards,
Sonya Roy.

On Fri, Jun 23, 2017 at 12:02 AM, Alex Rousskov <mailto:rousskov at measurement-factory.com> wrote:
On 06/22/2017 11:49 AM, Sonya Roy wrote:

> I noticed that squid changes the header order received from the client
> before sending it to the origin server.
>
> I assume this is because squid parses the header data and adds some
> headers depending on the config file and then recreates the header data.

IIRC, modern Squids change a header field position when the received
field is deleted and then added back. This is typical for hop-by-hop
headers such as Connection, but there are other reasons for Squid to
delete and add a header field. When the value of the added field is the
same as the value of the removed field, such pointless "editing" looks
like mindless "reordering" to the outside observer.

The two actions (field deletion and addition) may happen in a single
piece of code or may be separated by lots of code and even time.
Preventing pointless editing in the former cases is straightforward, but
the latter cases are difficult to handle. Correct avoidance of pointless
editing may improve performance and, if it does, can be considered a
useful optimization on its own, regardless of your use case.


> Is there any way to prevent this?

Not without changing Squid code (or adding more proxies). However,
before we even talk about code changes, we should clarify the problem we
are dealing with. The questions below will guide you.

It is probably much easier to ensure some fixed field send order
(regardless of the received order) than to preserve the received order.
Will a fixed order (e.g., always alphabetical) address your use case?
This feature will hurt performance, but you might be able to convince
others to accept it if you have a very compelling/specific/detailed use
case because it can be disabled by default.


> I am asking because some sites detect bots using the header order and
> they drop any such connection. So they unintentionally block squid
> proxies even if its not being used by a bot.

Are you implying that bots often change header field order between their
requests? Or that bots often use a different (fixed) header field order
than the (fixed) field order used by non-bots? Preserving received order
may help in the former case but not in the latter case.

Also, do those blocking sites pay attention to all headers or just
end-to-end headers?

Please note that there are many other ways to detect a proxy so if a
site wants to block proxies rather than bots, then it is probably
pointless to fight it (or, at least, the Squid Project should not).


HTH,

Alex.




From sonyaroy75 at gmail.com  Tue Jun 27 14:45:43 2017
From: sonyaroy75 at gmail.com (Sonya Roy)
Date: Tue, 27 Jun 2017 20:15:43 +0530
Subject: [squid-users] Header order in squid proxy
In-Reply-To: <066801d2ef52$2f351390$8d9f3ab0$@ngtech.co.il>
References: <CALSaDe0TD=sctLfc8xP_URsfRyvSo1cNm6p9+Tdsohi2sgXkhg@mail.gmail.com>
 <066801d2ef52$2f351390$8d9f3ab0$@ngtech.co.il>
Message-ID: <CALSaDe1cQvdrwXqJzZ0BXT_q6iXcS6X9i0STJ0A8TfW52CosqA@mail.gmail.com>

The sites I was talking about don't just target the header order. That's
just one of the things they check. Of course, they have their own system to
protect themselves again ddos attacks or use services like akamai or
cloudflare. The header order is just one of the common bot-detection
techniques that they use to filter out unwanted traffic.

For example, akamai's bot detection system checks header order as well
among a lot of other things.

Anyway, after Alex pointed me to the right direction, I managed to edit
couple lines in squid to prevent the change in header order.

With regards,

On Tue, Jun 27, 2017 at 8:02 PM, Eliezer Croitoru <eliezer at ngtech.co.il>
wrote:

> If I may add a word or two:
> If sites are securing their systems based on headers order then I believe
> they are aiming at the wrong target.
> It's a "nice to have" but not actual deep application level defense.(based
> on my low level in the subject)
> One example I have seen of a DOS\DDOS issue is:
> "Hey, We are having high CPU usage, what should we do?"
> - The bot was hammering the service from an AWS instance ... so block it..
> - How many requests per second from a single IP is considered normal?
> - Then, how many *new* cookies requests per second is considered normal?
> - What about NAT? would a Chinese client be considered legit despite to
> him being under one big NAT?
> - Would you be able to differentiate between a specific single ip or
> subnet that is considered legit?
> - What about RBL?
>
> The above are a things I heard here or there which I think are more
> important than headers order.
> Take my words as coming from a person which is not an expert in the
> security area.
>
> All The Bests,
> Eliezer
>
> ----
> http://ngtech.co.il/lmgtfy/
> Linux System Administrator
> Mobile: +972-5-28704261
> Email: eliezer at ngtech.co.il
>
>
> From: squid-users [mailto:squid-users-bounces at lists.squid-cache.org] On
> Behalf Of Sonya Roy
> Sent: Thursday, June 22, 2017 21:54
> To: squid-users at lists.squid-cache.org
> Subject: Re: [squid-users] Header order in squid proxy
>
> The sites I am talking about check the User-Agent header and makes sure
> the user-agent is for a well-known browser, i.e. a browser that they
> support. And any browser like Firefox, Chrome, Safari, Edge for example,
> sends the headers in a certain order and the order depends on the browser.
> And this header order for well-known headers like Accept, Accept-Language,
> Accept-Encoding, Content-Length, Host, Connection, Referer, Cookie, etc.
> And they match the order of the received request with the standard header
> order for the browser for that user-agent.
>
> This detects bots like a poorly written bot(i.e ones that don't consider
> this header order) using python requests or in any language for that matter
> where the requests are handled using a low level http requests library.
>
> So, keeping the header order sent from the client intact would prevent
> them from dropping proxied requests(ones that use squid). I know for a fact
> that they don't intend to block proxies.
>
> Could you point me in the direction to where I should look for in the
> source code of squid? the part that handles the header data sent from the
> client.
>
> With regards,
> Sonya Roy.
>
> On Fri, Jun 23, 2017 at 12:02 AM, Alex Rousskov <mailto:
> rousskov at measurement-factory.com> wrote:
> On 06/22/2017 11:49 AM, Sonya Roy wrote:
>
> > I noticed that squid changes the header order received from the client
> > before sending it to the origin server.
> >
> > I assume this is because squid parses the header data and adds some
> > headers depending on the config file and then recreates the header data.
>
> IIRC, modern Squids change a header field position when the received
> field is deleted and then added back. This is typical for hop-by-hop
> headers such as Connection, but there are other reasons for Squid to
> delete and add a header field. When the value of the added field is the
> same as the value of the removed field, such pointless "editing" looks
> like mindless "reordering" to the outside observer.
>
> The two actions (field deletion and addition) may happen in a single
> piece of code or may be separated by lots of code and even time.
> Preventing pointless editing in the former cases is straightforward, but
> the latter cases are difficult to handle. Correct avoidance of pointless
> editing may improve performance and, if it does, can be considered a
> useful optimization on its own, regardless of your use case.
>
>
> > Is there any way to prevent this?
>
> Not without changing Squid code (or adding more proxies). However,
> before we even talk about code changes, we should clarify the problem we
> are dealing with. The questions below will guide you.
>
> It is probably much easier to ensure some fixed field send order
> (regardless of the received order) than to preserve the received order.
> Will a fixed order (e.g., always alphabetical) address your use case?
> This feature will hurt performance, but you might be able to convince
> others to accept it if you have a very compelling/specific/detailed use
> case because it can be disabled by default.
>
>
> > I am asking because some sites detect bots using the header order and
> > they drop any such connection. So they unintentionally block squid
> > proxies even if its not being used by a bot.
>
> Are you implying that bots often change header field order between their
> requests? Or that bots often use a different (fixed) header field order
> than the (fixed) field order used by non-bots? Preserving received order
> may help in the former case but not in the latter case.
>
> Also, do those blocking sites pay attention to all headers or just
> end-to-end headers?
>
> Please note that there are many other ways to detect a proxy so if a
> site wants to block proxies rather than bots, then it is probably
> pointless to fight it (or, at least, the Squid Project should not).
>
>
> HTH,
>
> Alex.
>
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20170627/22f58ffa/attachment.htm>

From ncherukuri at partycity.com  Tue Jun 27 15:07:19 2017
From: ncherukuri at partycity.com (Cherukuri, Naresh)
Date: Tue, 27 Jun 2017 15:07:19 +0000
Subject: [squid-users] Squid Version 3.5.20
In-Reply-To: <066401d2ef4f$ee3dd450$cab97cf0$@ngtech.co.il>
References: <89638057A560FB458C01C197F81C7F5D13EDCCD0@ROCKETS.amscan.corp>
 <066401d2ef4f$ee3dd450$cab97cf0$@ngtech.co.il>
Message-ID: <89638057A560FB458C01C197F81C7F5D13EE401F@ROCKETS.amscan.corp>

Hi Eliezer,

We successfully blocked gmail, google images, google drive and rest all google related. Now we allowing www.google.com and www. google/Recaptcha. We still need to block www.google.com and just allow www.google/recaptcha. Is there a way to do that?

Appreciate your quick turnover!

Thanks&Regards,
Naresh

 
-----Original Message-----
From: Eliezer Croitoru [mailto:eliezer at ngtech.co.il] 
Sent: Tuesday, June 27, 2017 10:16 AM
To: Cherukuri, Naresh; squid-users at lists.squid-cache.org
Subject: RE: [squid-users] Squid Version 3.5.20

Hey,

I can try to help you but I do not have enough logs for it.
Also it's not so simple.
Basically you will need to block gmail and google drive themselves in one rule that will not include other google services.

All The Bests,
Eliezer

----
http://ngtech.co.il/lmgtfy/
Linux System Administrator
Mobile: +972-5-28704261
Email: eliezer at ngtech.co.il


From: squid-users [mailto:squid-users-bounces at lists.squid-cache.org] On Behalf Of Cherukuri, Naresh
Sent: Friday, June 23, 2017 23:34
To: squid-users at lists.squid-cache.org
Subject: [squid-users] Squid Version 3.5.20

Hello All,

I installed Squid version 3.5.20 on RHEL 7 and generated selfsigned CA certificates, can you shed some light on how to "Configure regular expression of the Google ReCaptcha URL with ACL".

My requirement :

This requirement is to allow Google's ReCaptcha URL (HTTPS) so associates can successfully use ADP which now utilizes Google's ReCaptcha which is called via an HTTPS URL, without allowing users to access other Google-related services such as Gmail or Google Drive.

Any ideas much appreciated!

Thanks,
Naresh



From flashdown at data-core.org  Tue Jun 27 15:37:25 2017
From: flashdown at data-core.org (Flashdown)
Date: Tue, 27 Jun 2017 17:37:25 +0200
Subject: [squid-users] Squid Version 3.5.20
In-Reply-To: <89638057A560FB458C01C197F81C7F5D13EE401F@ROCKETS.amscan.corp>
References: <89638057A560FB458C01C197F81C7F5D13EDCCD0@ROCKETS.amscan.corp>
 <066401d2ef4f$ee3dd450$cab97cf0$@ngtech.co.il>
 <89638057A560FB458C01C197F81C7F5D13EE401F@ROCKETS.amscan.corp>
Message-ID: <C6A90989-34B7-4B7E-86AB-D3049A07F4A5@data-core.org>

Well, I know that issue very good and google is the issue since they should put their captcha on a own subdomain. Then we could effectivley allow only the access to the captcha.

Until that there is no good way to achive this. But there is a non reliable way of blocking google.com

First allow the Connect method for google.com
Acl CONNECT method CONNECT
acl sslconnect dstdomain -i www.google.com
http_access allow CONNECT sslconnect
Then use an url regex and allow google.com/recaptcha

This way sometimes www.google.com is blocked, sometimes not. But access to recaptcha will always work.

Why we can't block it reliable? Well when   browser/client wants to connect to  https website then the firsr thing the browser trie is open a ssl tunnel to the FQDN
As soon as the tunnel is up it will request the ressource. May it helps if you add a url regex deny between allowing the connect method and allowing the url www.google.com/recaptcha

Written on  my mobile..

Br,
Flashdown



Am 27. Juni 2017 17:07:19 MESZ schrieb "Cherukuri, Naresh" <ncherukuri at partycity.com>:
>Hi Eliezer,
>
>We successfully blocked gmail, google images, google drive and rest all
>google related. Now we allowing www.google.com and www.
>google/Recaptcha. We still need to block www.google.com and just allow
>www.google/recaptcha. Is there a way to do that?
>
>Appreciate your quick turnover!
>
>Thanks&Regards,
>Naresh
>
> 
>-----Original Message-----
>From: Eliezer Croitoru [mailto:eliezer at ngtech.co.il] 
>Sent: Tuesday, June 27, 2017 10:16 AM
>To: Cherukuri, Naresh; squid-users at lists.squid-cache.org
>Subject: RE: [squid-users] Squid Version 3.5.20
>
>Hey,
>
>I can try to help you but I do not have enough logs for it.
>Also it's not so simple.
>Basically you will need to block gmail and google drive themselves in
>one rule that will not include other google services.
>
>All The Bests,
>Eliezer
>
>----
>http://ngtech.co.il/lmgtfy/
>Linux System Administrator
>Mobile: +972-5-28704261
>Email: eliezer at ngtech.co.il
>
>
>From: squid-users [mailto:squid-users-bounces at lists.squid-cache.org] On
>Behalf Of Cherukuri, Naresh
>Sent: Friday, June 23, 2017 23:34
>To: squid-users at lists.squid-cache.org
>Subject: [squid-users] Squid Version 3.5.20
>
>Hello All,
>
>I installed Squid version 3.5.20 on RHEL 7 and generated selfsigned CA
>certificates, can you shed some light on how to "Configure regular
>expression of the Google ReCaptcha URL with ACL".
>
>My requirement :
>
>This requirement is to allow Google's ReCaptcha URL (HTTPS) so
>associates can successfully use ADP which now utilizes Google's
>ReCaptcha which is called via an HTTPS URL, without allowing users to
>access other Google-related services such as Gmail or Google Drive.
>
>Any ideas much appreciated!
>
>Thanks,
>Naresh
>
>_______________________________________________
>squid-users mailing list
>squid-users at lists.squid-cache.org
>http://lists.squid-cache.org/listinfo/squid-users
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20170627/6b153a4c/attachment.htm>

From squid3 at treenet.co.nz  Tue Jun 27 15:39:54 2017
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Wed, 28 Jun 2017 03:39:54 +1200
Subject: [squid-users] NTLM authentication worked in Squid 2.7.STABLE8
 Squid Web Proxy, now need it in v3.5 hosted on Windows server 2k12
In-Reply-To: <799349610.3174840.1498521985549@mail.yahoo.com>
References: <799349610.3174840.1498521985549.ref@mail.yahoo.com>
 <799349610.3174840.1498521985549@mail.yahoo.com>
Message-ID: <52ad05f0-14cb-0b98-bb77-38333c25c988@treenet.co.nz>

On 27/06/17 12:06, Todd Pearson wrote:
> 
> I am hosting the squid proxy on Windows 2K12 server.   Squid 2.7.STABLE8 
> Squid Web Proxy version worked well for authentication until recent 
> Windows 10 update killed Sha1.  Now I am upgrading to squid proxy 
> version 3.5.x.x to restore authentication.

FYI: upgrading to Squid-3 will not solve that problem by itself. The 
helpers in both Squid series are performing the same logic, with the 
same crypto limitations.

The core problem is that NTLM protocol itself is not capable of anything 
actually considered secure these days. It was declared EOL by MS more 
then 11 years ago, so loss of NTLM related things in Win10 is hardly a 
surprise.

To solve your auth problem what you need is actually a migration to 
Kerberos authentication (Negotiate auth). You might find that slightly 
easier after the Squid-3 upgrade, but the two are really independent 
changes.


> 
> The below settings are longer available in the 3.5.x.x version since the 
> progams do not exist for the new version:
> 
> auth_param ntlm program c:/squid/libexec/mswin_ntlm_auth.exe
> 
> external_acl_type win_domain_group %LOGIN 
> c:/squid/libexec/mswin_check_ad_group.exe -G
> 
> 
> What are the equivalent setting for v 3.5.  Once again I am in windows 
> environment.

The helpers still exist, they just got renamed to follow a structured 
taxonomy:
<http://www.squid-cache.org/Versions/v3/3.2/RELEASENOTES.html#ss2.6>


Amos


From ncherukuri at partycity.com  Tue Jun 27 15:46:05 2017
From: ncherukuri at partycity.com (Cherukuri, Naresh)
Date: Tue, 27 Jun 2017 15:46:05 +0000
Subject: [squid-users] Squid Version 3.5.20
In-Reply-To: <C6A90989-34B7-4B7E-86AB-D3049A07F4A5@data-core.org>
References: <89638057A560FB458C01C197F81C7F5D13EDCCD0@ROCKETS.amscan.corp>
 <066401d2ef4f$ee3dd450$cab97cf0$@ngtech.co.il>
 <89638057A560FB458C01C197F81C7F5D13EE401F@ROCKETS.amscan.corp>
 <C6A90989-34B7-4B7E-86AB-D3049A07F4A5@data-core.org>
Message-ID: <89638057A560FB458C01C197F81C7F5D13EE4192@ROCKETS.amscan.corp>

Hi,

Thank You for quick turnover, as per your request I changed squid config like below, still I going to www.google.com<http://www.google.com>
acl CONNECT method CONNECT
acl sslconnect dstdomain -i https://www.google.com
acl GoogleRecaptcha url_regex ^https://www.google.com/recaptcha/$
http_access allow CONNECT sslconnect
http_access allow backoffice_users GoogleRecaptcha


Thanks& Regards,
Naresh
From: Flashdown [mailto:flashdown at data-core.org]
Sent: Tuesday, June 27, 2017 11:37 AM
To: squid-users at lists.squid-cache.org; Cherukuri, Naresh; Eliezer Croitoru
Subject: Re: [squid-users] Squid Version 3.5.20

Well, I know that issue very good and google is the issue since they should put their captcha on a own subdomain. Then we could effectivley allow only the access to the captcha.

Until that there is no good way to achive this. But there is a non reliable way of blocking google.com<http://google.com>

First allow the Connect method for google.com<http://google.com>
Acl CONNECT method CONNECT
acl sslconnect dstdomain -i www.google.com<http://www.google.com>
http_access allow CONNECT sslconnect
Then use an url regex and allow google.com/recaptcha<http://google.com/recaptcha>

This way sometimes www.google.com<http://www.google.com> is blocked, sometimes not. But access to recaptcha will always work.

Why we can't block it reliable? Well when browser/client wants to connect to https website then the firsr thing the browser trie is open a ssl tunnel to the FQDN
As soon as the tunnel is up it will request the ressource. May it helps if you add a url regex deny between allowing the connect method and allowing the url www.google.com/recaptcha<http://www.google.com/recaptcha>

Written on my mobile..

Br,
Flashdown


Am 27. Juni 2017 17:07:19 MESZ schrieb "Cherukuri, Naresh" <ncherukuri at partycity.com<mailto:ncherukuri at partycity.com>>:

Hi Eliezer,

We successfully blocked gmail, google images, google drive and rest all google related. Now we allowing www.google.com<http://www.google.com> and www. google/Recaptcha. We still need to block www.google.com<http://www.google.com> and just allow www.google/recaptcha<http://www.google/recaptcha>. Is there a way to do that?

Appreciate your quick turnover!

Thanks&Regards,
Naresh


-----Original Message-----
From: Eliezer Croitoru [mailto:eliezer at ngtech.co.il]
Sent: Tuesday, June 27, 2017 10:16 AM
To: Cherukuri, Naresh; squid-users at lists.squid-cache.org<mailto:squid-users at lists.squid-cache.org>
Subject: RE: [squid-users] Squid Version 3.5.20

Hey,

I can try to help you but I do not have enough logs for it.
Also it's not so simple.
Basically you will need to block gmail and google drive themselves in one rule that will not include other google services.

All The Bests,
Eliezer

----
http://ngtech.co.il/lmgtfy/
Linux System Administrator
Mobile: +972-5-28704261
Email: eliezer at ngtech.co.il<mailto:eliezer at ngtech.co.il>


From: squid-users [mailto:squid-users-bounces at lists.squid-cache.org] On Behalf Of Cherukuri, Naresh
Sent: Friday, June 23, 2017 23:34
To: squid-users at lists.squid-cache.org<mailto:squid-users at lists.squid-cache.org>
Subject: [squid-users] Squid Version 3.5.20

Hello All,

I installed Squid version 3.5.20 on RHEL 7 and generated selfsigned CA certificates, can you shed some light on how to "Configure regular expression of the Google ReCaptcha URL with ACL".

My requirement :

This requirement is to allow Google's ReCaptcha URL (HTTPS) so associates can successfully use ADP which now utilizes Google's ReCaptcha which is called via an HTTPS URL, without allowing users to access other Google-related services such as Gmail or Google Drive.

Any ideas much appreciated!

Thanks,
Naresh

________________________________

squid-users mailing list
squid-users at lists.squid-cache.org<mailto:squid-users at lists.squid-cache.org>
http://lists.squid-cache.org/listinfo/squid-users
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20170627/67d25b8e/attachment.htm>

From squid3 at treenet.co.nz  Tue Jun 27 15:55:04 2017
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Wed, 28 Jun 2017 03:55:04 +1200
Subject: [squid-users] HIER_NONE on TCP_MISS?
In-Reply-To: <CAJDqJL8pnoXCYadBxaff37iXkcxCjfXVK0TbXNj00iHoA6khLg@mail.gmail.com>
References: <CAJDqJL8pnoXCYadBxaff37iXkcxCjfXVK0TbXNj00iHoA6khLg@mail.gmail.com>
Message-ID: <d10eabc0-2e7d-c6c7-2682-58297d00d51b@treenet.co.nz>

On 27/06/17 15:28, bump skier wrote:
> Hi,
> 
> I'm trying to understand the following behavior I'm seeing with Squid 
> running in accelerator mode. In short, I'm seeing some TCP_MISS for 
> requests to a static javascript file which is initially cached and 
> returned as a cache hit. I suspect the missed cache hits are due to the 
> cache size being too small and the file eventually getting evicted. 
> However, I'm confused about what I'm seeing in the Squid access log. For 
> some of the cache misses I can see in the access log that Squid fetches 
> the file from the configured origin server but for a vast majority of 
> them I see HIER_NONE even though Squid is actually returning the file.
> 
> Under what situations would Squid fetch content from the origin server 
> during a cache miss but print HIER_NONE?


It may happen if you have content adaptation (ICAP/eCAP) providing a 
response instead of either cache or origin server.

Maybe also if the collapsed forwarding feature is in use. AFAIK, we have 
not got the log entries quite right there yet.

Amos


From bloggerrazorcross at gmail.com  Tue Jun 27 16:11:10 2017
From: bloggerrazorcross at gmail.com (Razor Cross)
Date: Tue, 27 Jun 2017 11:11:10 -0500
Subject: [squid-users] Squid caching bad objects
In-Reply-To: <a494df2a-eb3d-acd4-ea08-2bc615348c45@measurement-factory.com>
References: <CANRy4AV5n4m8PC7fKQJfaZboKxyUG7o7ASuJABRhVfFFfmzjhg@mail.gmail.com>
 <a494df2a-eb3d-acd4-ea08-2bc615348c45@measurement-factory.com>
Message-ID: <CANRy4AU=5VR+8kmcFdKLbpnxbdTgUCvRopxHV5ypD+0JmujOAA@mail.gmail.com>

On Mon, Jun 26, 2017 at 12:06 PM, Alex Rousskov <
rousskov at measurement-factory.com> wrote:

> On 06/26/2017 10:11 AM, Razor Cross wrote:
>
> > We are using squid 3.5. for our server. Recently we have noticed that
> > squid is caching incomplete objects in case of chunked response.
> >
> > We have gone through the squid code. It looks likes squid is caching
> > incomplete response in case of EOF from the server even though it does
> > not receive the last empty chunk.
> >
> >
> >  if (eof) // already reached EOF
> >         return COMPLETE_NONPERSISTENT_MSG;
>
> You are looking at the wrong code. HttpStateData::persistentConnStatus()
> and related *_MSG codes do not determine whether the entire object was
> received. They determine whether
>
> (a) Squid should expect more response bytes and
>
> (b) The connection can be kept open if no more response bytes are expected.
>
> The COMPLETE_NONPERSISTENT_MSG return value is correct here (I am
> ignoring the sad fact that we are abusing the word "complete" to cover
> both whole and truncated responses).
>
>
> > Is this expected? Because of this problem, our server ends up serving
> > bad objects to the user.
>
> >What you describe sounds like a bug, but the exact code you are quoting
> >is not responsible for that bug. I di not study this in detail, but I
> >suspect that the COMPLETE_NONPERSISTENT_MSG case in
> >HttpStateData::processReplyBody() should be changed to call
> >StoreEntry::lengthWentBad("missing last-chunk") when lastChunk is false
> > and HttpStateData::flags.chunked is true.
>
>       We are able to reproduce the issue . If server socket is closed
after sending first chunk of data, squid is caching the partial object even
though it did not receive the remaining chunks. I feel it has to make sure
that lastchunk has received before caching the data.

>
> - Cross
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20170627/77e3805d/attachment.htm>

From rafael.akchurin at diladele.com  Tue Jun 27 16:11:23 2017
From: rafael.akchurin at diladele.com (Rafael Akchurin)
Date: Tue, 27 Jun 2017 16:11:23 +0000
Subject: [squid-users] Block doc documents
In-Reply-To: <CAJtq-03UDVB462bzqG+5q0DDVVwXFqSmfUw_TrjWNeWskBmcnw@mail.gmail.com>
References: <CAJtq-03UDVB462bzqG+5q0DDVVwXFqSmfUw_TrjWNeWskBmcnw@mail.gmail.com>
Message-ID: <DB6PR0401MB268031CD2FB67D8431A8BEAE8FDC0@DB6PR0401MB2680.eurprd04.prod.outlook.com>

Hello Daniel,

We have something like this - but I am unsure if it is possible to differentiate the doc types you mentioned using first 256 bytes of contents. Also think about zips - may it be your users will be able to pack a file into zip and get through your protection.

See https://docs.diladele.com/administrator_guide_5_1/web_filter/policies/blocking_file_downloads.html

Best regards,
Rafael

-----Original Message-----
From: squid-users [mailto:squid-users-bounces at lists.squid-cache.org] On Behalf Of Daniel Rieken
Sent: Tuesday, June 27, 2017 1:53 PM
To: squid-users at lists.squid-cache.org
Subject: [squid-users] Block doc documents

Hello,

I would like to block my users from downloading doc- and docm-files, but not docx.

So this works fine for me:
/etc/squid3/blockExtensions.acl:
\.doc(\?.*)?$
\.docm(\?.*)?$

acl blockExtensions urlpath_regex -i "/etc/squid3/blockExtensions.acl"
http_access deny blockExtensions


But in some cases the URL doesn't contain the extension (e.g. doc).
For URLs like this the above ACL doesn't work:
- http://www.example.org/download.pl?file=wordfile
- http://www.example.org/invoice-5479657415/

Here I need to work with mime-types:
acl blockMime rep_mime_type application/msword acl blockMime rep_mime_type application/vnd.ms-word.document.macroEnabled.12
http_reply_access deny blockMime

This works fine, too. But I see a problem: The mime-type is defined on the webserver. So the badguy could configure his webserver to serve a doc-file as application/i.am.not.a.docfile and the above ACL isn't working anymore.
Is there any way to make squid block doc- and docm files based on the response-headers file-type?
Or in other words: Is squid able to match the "doc" in the Content-Disposition header of the response?

HTTP/1.0 200 OK
Date: Tue, 27 Jun 2017 11:40:57 GMT
Server: Apache Phusion_Passenger/4.0.10 mod_bwlimited/1.4
Cache-Control: no-cache, no-store, max-age=0, must-revalidate
Pragma: no-cache
Content-Type: application/baddoc
Content-Disposition: attachment;
filename="gescanntes-Dokument-VPPAW-072-JCD3032.doc"
Content-Transfer-Encoding: binary
X-Powered-By: PHP/5.3.29
Connection: close


Regards, Daniel
_______________________________________________
squid-users mailing list
squid-users at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-users

From rousskov at measurement-factory.com  Tue Jun 27 16:26:48 2017
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Tue, 27 Jun 2017 10:26:48 -0600
Subject: [squid-users] ACLs allow/deny logic
In-Reply-To: <1903999605.3040033.1498545101578@mail.yahoo.com>
References: <338112759.2261098.1498466816031.ref@mail.yahoo.com>
 <338112759.2261098.1498466816031@mail.yahoo.com>
 <2ea4a067-fb39-c820-5823-769711294482@treenet.co.nz>
 <1903999605.3040033.1498545101578@mail.yahoo.com>
Message-ID: <8b860ae3-d52a-98ec-6d32-f939a6839e06@measurement-factory.com>

On 06/27/2017 12:31 AM, Vieri wrote:

> http_access deny denied_restricted1_mimetypes_req !allowed_restricted1_domains !allowed_restricted1_ips
> http_reply_access deny denied_restricted1_mimetypes_rep !allowed_restricted1_domains !allowed_restricted1_ips
> http_access deny intercepted !localnet
> http_access allow localnet
> http_access deny all

> "The reply for POST http://149.154.165.120/api is DENIED, because it matched allowed_restricted1_ips"

Squid "matched ACL" reporting code is badly designed and often leads to
misleading results. In this particular case, Squid wanted to say "it
matched !allowed_restricted1_ips" but could not. Older Squids were
especially broken in this area, but even modern ones suffer from the
same design flaw. This flaw is a known problem:

> // XXX: AclMatchedName does not contain a matched ACL name when the acl
> // does not match. It contains the last (usually leaf) ACL name checked
> // (or is NULL if no ACLs were checked).

You can work around most of these problems by appending an
always-matching ACL to every http_access rule you want to identify and
making sure that at least one rule always matches. The former can be
done using an any-of ACL in older Squids or annotate_transaction ACL in
modern Squids. You are already doing the latter with "deny all".


HTH,

Alex.


From rousskov at measurement-factory.com  Tue Jun 27 16:34:35 2017
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Tue, 27 Jun 2017 10:34:35 -0600
Subject: [squid-users] Squid caching bad objects
In-Reply-To: <CANRy4AU=5VR+8kmcFdKLbpnxbdTgUCvRopxHV5ypD+0JmujOAA@mail.gmail.com>
References: <CANRy4AV5n4m8PC7fKQJfaZboKxyUG7o7ASuJABRhVfFFfmzjhg@mail.gmail.com>
 <a494df2a-eb3d-acd4-ea08-2bc615348c45@measurement-factory.com>
 <CANRy4AU=5VR+8kmcFdKLbpnxbdTgUCvRopxHV5ypD+0JmujOAA@mail.gmail.com>
Message-ID: <bdb4c8bc-8d14-b442-53d0-e39ac4b5470e@measurement-factory.com>

On 06/27/2017 10:11 AM, Razor Cross wrote:
> On Mon, Jun 26, 2017 at 12:06 PM, Alex Rousskov wrote:

>     >I suspect that the COMPLETE_NONPERSISTENT_MSG case in
>     >HttpStateData::processReplyBody() should be changed to call
>     >StoreEntry::lengthWentBad("missing last-chunk") when lastChunk is false
>     >and HttpStateData::flags.chunked is true.

>       We are able to reproduce the issue . If server socket is closed
> after sending first chunk of data, squid is caching the partial object
> even though it did not receive the remaining chunks.

If you are not going to fix this yourself, please consider filing a bug
report, citing this email thread.


> I feel it has to
> make sure that lastchunk has received before caching the data.

That is impossible in general (the response may be too big to buffer)
but is also unnecessary in most cases (because Squid can stop caching
and delete the being-cached object in-flight). My paragraph quoted above
has the blueprint for a possible fix.

Alex.


From squid3 at treenet.co.nz  Tue Jun 27 16:59:36 2017
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Wed, 28 Jun 2017 04:59:36 +1200
Subject: [squid-users] Block doc documents
In-Reply-To: <CAJtq-03UDVB462bzqG+5q0DDVVwXFqSmfUw_TrjWNeWskBmcnw@mail.gmail.com>
References: <CAJtq-03UDVB462bzqG+5q0DDVVwXFqSmfUw_TrjWNeWskBmcnw@mail.gmail.com>
Message-ID: <b4b9241f-b4fc-57b1-f47b-b276c32bf314@treenet.co.nz>

On 27/06/17 23:53, Daniel Rieken wrote:
> Hello,
> 
> I would like to block my users from downloading doc- and docm-files,
> but not docx.
> 
> So this works fine for me:
> /etc/squid3/blockExtensions.acl:
> \.doc(\?.*)?$
> \.docm(\?.*)?$
> 
> acl blockExtensions urlpath_regex -i "/etc/squid3/blockExtensions.acl"
> http_access deny blockExtensions
> 
> 
> But in some cases the URL doesn't contain the extension (e.g. doc).
> For URLs like this the above ACL doesn't work:
> - http://www.example.org/download.pl?file=wordfile
> - http://www.example.org/invoice-5479657415/
> 
> Here I need to work with mime-types:
> acl blockMime rep_mime_type application/msword
> acl blockMime rep_mime_type application/vnd.ms-word.document.macroEnabled.12
> http_reply_access deny blockMime
> 
> This works fine, too. But I see a problem: The mime-type is defined on
> the webserver. So the badguy could configure his webserver to serve a
> doc-file as application/i.am.not.a.docfile and the above ACL isn't
> working anymore.


HTTP contains no concept of "file". That is a human concept. All of what 
you mention above are the consequences of that difference.

I recommend you drop this concept of "file" from your thinking and 
concentrate on detecting what HTTP details represent a bad HTTP message. 
The "file" related things should be dealt with at other layers by other 
software like AV scanning or as Brendan suggested ICAP payload scanners.


Amos


From bloggerrazorcross at gmail.com  Tue Jun 27 17:04:22 2017
From: bloggerrazorcross at gmail.com (Razor Cross)
Date: Tue, 27 Jun 2017 12:04:22 -0500
Subject: [squid-users] Squid caching bad objects
In-Reply-To: <bdb4c8bc-8d14-b442-53d0-e39ac4b5470e@measurement-factory.com>
References: <CANRy4AV5n4m8PC7fKQJfaZboKxyUG7o7ASuJABRhVfFFfmzjhg@mail.gmail.com>
 <a494df2a-eb3d-acd4-ea08-2bc615348c45@measurement-factory.com>
 <CANRy4AU=5VR+8kmcFdKLbpnxbdTgUCvRopxHV5ypD+0JmujOAA@mail.gmail.com>
 <bdb4c8bc-8d14-b442-53d0-e39ac4b5470e@measurement-factory.com>
Message-ID: <CANRy4AVDOhpG6X-u2OKkPEMC0fhDmPvjubDwNU_fSHo3kzJeQw@mail.gmail.com>

On Tue, Jun 27, 2017 at 11:34 AM, Alex Rousskov <
rousskov at measurement-factory.com> wrote:

> On 06/27/2017 10:11 AM, Razor Cross wrote:
> > On Mon, Jun 26, 2017 at 12:06 PM, Alex Rousskov wrote:
>
> >     >I suspect that the COMPLETE_NONPERSISTENT_MSG case in
> >     >HttpStateData::processReplyBody() should be changed to call
> >     >StoreEntry::lengthWentBad("missing last-chunk") when lastChunk is
> false
> >     >and HttpStateData::flags.chunked is true.
>
> >       We are able to reproduce the issue . If server socket is closed
> > after sending first chunk of data, squid is caching the partial object
> > even though it did not receive the remaining chunks.
>
> If you are not going to fix this yourself, please consider filing a bug
> report, citing this email thread.
>
>
> > I feel it has to
> > make sure that lastchunk has received before caching the data.
>
> That is impossible in general (the response may be too big to buffer)
> but is also unnecessary in most cases (because Squid can stop caching
> and delete the being-cached object in-flight). My paragraph quoted above
> has the blueprint for a possible fix.
>
> Thanks for your inputs..
I just want to hear from squid official forum/owner whether it has fixed in
any recent squid releases so that we can upgrade/patch the fix.

- Cross

>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20170627/747a3ada/attachment.htm>

From squid3 at treenet.co.nz  Tue Jun 27 17:09:10 2017
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Wed, 28 Jun 2017 05:09:10 +1200
Subject: [squid-users] Squid Version 3.5.20
In-Reply-To: <89638057A560FB458C01C197F81C7F5D13EE4192@ROCKETS.amscan.corp>
References: <89638057A560FB458C01C197F81C7F5D13EDCCD0@ROCKETS.amscan.corp>
 <066401d2ef4f$ee3dd450$cab97cf0$@ngtech.co.il>
 <89638057A560FB458C01C197F81C7F5D13EE401F@ROCKETS.amscan.corp>
 <C6A90989-34B7-4B7E-86AB-D3049A07F4A5@data-core.org>
 <89638057A560FB458C01C197F81C7F5D13EE4192@ROCKETS.amscan.corp>
Message-ID: <abdeacde-2942-136b-604b-58bae0593e46@treenet.co.nz>

On 28/06/17 03:46, Cherukuri, Naresh wrote:
> Hi,
> 
> Thank You for quick turnover, as per your request I changed squid config 
> like below, still I going to www.google.com
> 
> acl CONNECT method CONNECT
> 
> acl sslconnect dstdomain -i https://www.google.com
> 
> acl GoogleRecaptcha url_regex ^https://www.google.com/recaptcha/$
> 
> http_access allow CONNECT sslconnect
> 

Er. That will never work.

* Firstly because "https://..." are not valid dstdomain values.

* Secondly because as the CONNECT message uses an authority-form URL 
structure, not an absolute-form URL.

Your Squid will simply not see the https:// URL unless you are 
decrypting the TLS tunnel inside the CONNECT payload.  That means 
SSL-Bump functionality is mandatory for what you are attempting to do.

Also, be aware that Google services are using HSTS and certificate 
pinning. So SSL-Bump is much more likely not to work for their URLs.

Amos


From rtpearson at yahoo.com  Tue Jun 27 17:12:41 2017
From: rtpearson at yahoo.com (Todd Pearson)
Date: Tue, 27 Jun 2017 17:12:41 +0000 (UTC)
Subject: [squid-users] NTLM authentication worked in Squid 2.7.STABLE8
 Squid Web Proxy, now need it in v3.5 hosted on Windows server 2k12
In-Reply-To: <52ad05f0-14cb-0b98-bb77-38333c25c988@treenet.co.nz>
References: <799349610.3174840.1498521985549.ref@mail.yahoo.com>
 <799349610.3174840.1498521985549@mail.yahoo.com>
 <52ad05f0-14cb-0b98-bb77-38333c25c988@treenet.co.nz>
Message-ID: <1483595053.3721106.1498583561886@mail.yahoo.com>


Thank you for the information. ?Is there any place to download the helper binaries for NTLM? ?Or do I need to build them myself?
Is there additional information on kerberos configuration in a windows environment. ?Trying to wrap my head around the keytab and creation of it in a windows only environment.      From: Amos Jeffries <squid3 at treenet.co.nz>
 To: squid-users at lists.squid-cache.org 
 Sent: Tuesday, June 27, 2017 8:40 AM
 Subject: Re: [squid-users] NTLM authentication worked in Squid 2.7.STABLE8 Squid Web Proxy, now need it in v3.5 hosted on Windows server 2k12
   
On 27/06/17 12:06, Todd Pearson wrote:
> 
> I am hosting the squid proxy on Windows 2K12 server.? Squid 2.7.STABLE8 
> Squid Web Proxy version worked well for authentication until recent 
> Windows 10 update killed Sha1.? Now I am upgrading to squid proxy 
> version 3.5.x.x to restore authentication.

FYI: upgrading to Squid-3 will not solve that problem by itself. The 
helpers in both Squid series are performing the same logic, with the 
same crypto limitations.

The core problem is that NTLM protocol itself is not capable of anything 
actually considered secure these days. It was declared EOL by MS more 
then 11 years ago, so loss of NTLM related things in Win10 is hardly a 
surprise.

To solve your auth problem what you need is actually a migration to 
Kerberos authentication (Negotiate auth). You might find that slightly 
easier after the Squid-3 upgrade, but the two are really independent 
changes.


> 
> The below settings are longer available in the 3.5.x.x version since the 
> progams do not exist for the new version:
> 
> auth_param ntlm program c:/squid/libexec/mswin_ntlm_auth.exe
> 
> external_acl_type win_domain_group %LOGIN 
> c:/squid/libexec/mswin_check_ad_group.exe -G
> 
> 
> What are the equivalent setting for v 3.5.? Once again I am in windows 
> environment.

The helpers still exist, they just got renamed to follow a structured 
taxonomy:
<http://www.squid-cache.org/Versions/v3/3.2/RELEASENOTES.html#ss2.6>


Amos
_______________________________________________
squid-users mailing list
squid-users at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-users


   
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20170627/3262745d/attachment.htm>

From squid3 at treenet.co.nz  Tue Jun 27 17:36:56 2017
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Wed, 28 Jun 2017 05:36:56 +1200
Subject: [squid-users] NTLM authentication worked in Squid 2.7.STABLE8
 Squid Web Proxy, now need it in v3.5 hosted on Windows server 2k12
In-Reply-To: <1483595053.3721106.1498583561886@mail.yahoo.com>
References: <799349610.3174840.1498521985549.ref@mail.yahoo.com>
 <799349610.3174840.1498521985549@mail.yahoo.com>
 <52ad05f0-14cb-0b98-bb77-38333c25c988@treenet.co.nz>
 <1483595053.3721106.1498583561886@mail.yahoo.com>
Message-ID: <2905cb64-c905-584c-b5de-d812a3c3a1ca@treenet.co.nz>

On 28/06/17 05:12, Todd Pearson wrote:
> 
> Thank you for the information.  Is there any place to download the 
> helper binaries for NTLM?  Or do I need to build them myself?
> 

Since you were using the SSPI helper for NTLM you should have the 
Negotiate/Kerberos equivalent already. It is mswin_sspi in Squid-2 or 
negotiate_sspi_auth in Squid-3.2+. The group checking helpers work with 
both auth types.

Diladele provide Squid-3 builds for Windows 
(<http://squid.diladele.com/>) if you are still going that way.


> Is there additional information on kerberos configuration in a windows 
> environment.  Trying to wrap my head around the keytab and creation of 
> it in a windows only environment.


This may be of help understanding what the Kerberos process is:
<http://wiki.squid-cache.org/ConfigExamples/Authenticate/Kerberos>

though the config examples and setup commands we have are all for 
non-Windows Squid machines it seems.


PS. I don't use Windows Squid servers myself, so cant be much help here. 
Maybe someone more familiar can help out.

Amos


From bumpski3r at gmail.com  Tue Jun 27 17:52:07 2017
From: bumpski3r at gmail.com (bump skier)
Date: Tue, 27 Jun 2017 17:52:07 +0000
Subject: [squid-users] HIER_NONE on TCP_MISS?
In-Reply-To: <d10eabc0-2e7d-c6c7-2682-58297d00d51b@treenet.co.nz>
References: <CAJDqJL8pnoXCYadBxaff37iXkcxCjfXVK0TbXNj00iHoA6khLg@mail.gmail.com>
 <d10eabc0-2e7d-c6c7-2682-58297d00d51b@treenet.co.nz>
Message-ID: <CAJDqJL_TC9thCPDWMt-3EvNYrL=7uc0SOcnJTTvQYQy8VUW_Sw@mail.gmail.com>

Hmm. I don't have ICAP/eCAP or collapsed forwarding configured. Are there
any situations where something similar to collapsed forwarding can happen
by default?

On Tue, Jun 27, 2017 at 11:55 AM Amos Jeffries <squid3 at treenet.co.nz> wrote:

> On 27/06/17 15:28, bump skier wrote:
> > Hi,
> >
> > I'm trying to understand the following behavior I'm seeing with Squid
> > running in accelerator mode. In short, I'm seeing some TCP_MISS for
> > requests to a static javascript file which is initially cached and
> > returned as a cache hit. I suspect the missed cache hits are due to the
> > cache size being too small and the file eventually getting evicted.
> > However, I'm confused about what I'm seeing in the Squid access log. For
> > some of the cache misses I can see in the access log that Squid fetches
> > the file from the configured origin server but for a vast majority of
> > them I see HIER_NONE even though Squid is actually returning the file.
> >
> > Under what situations would Squid fetch content from the origin server
> > during a cache miss but print HIER_NONE?
>
>
> It may happen if you have content adaptation (ICAP/eCAP) providing a
> response instead of either cache or origin server.
>
> Maybe also if the collapsed forwarding feature is in use. AFAIK, we have
> not got the log entries quite right there yet.
>
> Amos
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20170627/7584dcd3/attachment.htm>

From rousskov at measurement-factory.com  Tue Jun 27 19:43:44 2017
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Tue, 27 Jun 2017 13:43:44 -0600
Subject: [squid-users] Reverse DNS Lookup for client IPs
In-Reply-To: <066601d2ef50$5c2e8360$148b8a20$@ngtech.co.il>
References: <20170620113508.ycg4iqvdq442kska@charite.de>
 <066601d2ef50$5c2e8360$148b8a20$@ngtech.co.il>
Message-ID: <96ce1358-83ea-d2a4-db25-f291175e7748@measurement-factory.com>

On 06/27/2017 08:19 AM, Eliezer Croitoru wrote:

> Can you put a link to the thread here?

The best relevant link is probably bug #4575:

  http://bugs.squid-cache.org/show_bug.cgi?id=4575

Alex.


> Are you talking about this thread:
> http://lists.squid-cache.org/pipermail/squid-users/2016-February/008999.html
> 
> http://squid-web-proxy-cache.1019090.n4.nabble.com/Reverse-DNS-Lookup-for-client-IPs-td4675872.html
> 
> Thanks,
> Eliezer
> 
> ----
> Eliezer Croitoru
> Linux System Administrator
> Mobile: +972-5-28704261
> Email: eliezer at ngtech.co.il
> 
> 
> -----Original Message-----
> From: squid-users [mailto:squid-users-bounces at lists.squid-cache.org] On Behalf Of Ralf Hildebrandt
> Sent: Tuesday, June 20, 2017 14:35
> To: squid-users at squid-cache.org
> Subject: [squid-users] Reverse DNS Lookup for client IPs
> 
> I have to chime in on the "Reverse DNS Lookup for client IPs" thread back in Feb 2016. I tried redefining the logging format for url_rewrite_extras and store_id_extras in the config, but that wouldn't work.
> 
> I had to change the file src/cf.data.pre and recompiled, after that the number of reverse lookups dropped considerably.
> 



From rtpearson at yahoo.com  Tue Jun 27 20:13:46 2017
From: rtpearson at yahoo.com (Todd Pearson)
Date: Tue, 27 Jun 2017 20:13:46 +0000 (UTC)
Subject: [squid-users] NTLM authentication worked in Squid 2.7.STABLE8
 Squid Web Proxy, now need it in v3.5 hosted on Windows server 2k12
In-Reply-To: <2905cb64-c905-584c-b5de-d812a3c3a1ca@treenet.co.nz>
References: <799349610.3174840.1498521985549.ref@mail.yahoo.com>
 <799349610.3174840.1498521985549@mail.yahoo.com>
 <52ad05f0-14cb-0b98-bb77-38333c25c988@treenet.co.nz>
 <1483595053.3721106.1498583561886@mail.yahoo.com>
 <2905cb64-c905-584c-b5de-d812a3c3a1ca@treenet.co.nz>
Message-ID: <1560325870.3867241.1498594426480@mail.yahoo.com>

I appreciate the input. ?Do you (or anyone else) know if keytab is required in a windows only environment for kerberos authentication?

      From: Amos Jeffries <squid3 at treenet.co.nz>
 To: Todd Pearson <rtpearson at yahoo.com>; "squid-users at lists.squid-cache.org" <squid-users at lists.squid-cache.org> 
 Sent: Tuesday, June 27, 2017 10:37 AM
 Subject: Re: [squid-users] NTLM authentication worked in Squid 2.7.STABLE8 Squid Web Proxy, now need it in v3.5 hosted on Windows server 2k12
   
On 28/06/17 05:12, Todd Pearson wrote:
> 
> Thank you for the information.? Is there any place to download the 
> helper binaries for NTLM?? Or do I need to build them myself?
> 

Since you were using the SSPI helper for NTLM you should have the 
Negotiate/Kerberos equivalent already. It is mswin_sspi in Squid-2 or 
negotiate_sspi_auth in Squid-3.2+. The group checking helpers work with 
both auth types.

Diladele provide Squid-3 builds for Windows 
(<http://squid.diladele.com/>) if you are still going that way.


> Is there additional information on kerberos configuration in a windows 
> environment.? Trying to wrap my head around the keytab and creation of 
> it in a windows only environment.


This may be of help understanding what the Kerberos process is:
<http://wiki.squid-cache.org/ConfigExamples/Authenticate/Kerberos>

though the config examples and setup commands we have are all for 
non-Windows Squid machines it seems.


PS. I don't use Windows Squid servers myself, so cant be much help here. 
Maybe someone more familiar can help out.

Amos


   
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20170627/b4c5877a/attachment.htm>

From eliezer at ngtech.co.il  Tue Jun 27 21:08:16 2017
From: eliezer at ngtech.co.il (Eliezer Croitoru)
Date: Wed, 28 Jun 2017 00:08:16 +0300
Subject: [squid-users] Reverse DNS Lookup for client IPs
In-Reply-To: <96ce1358-83ea-d2a4-db25-f291175e7748@measurement-factory.com>
References: <20170620113508.ycg4iqvdq442kska@charite.de>
 <066601d2ef50$5c2e8360$148b8a20$@ngtech.co.il>
 <96ce1358-83ea-d2a4-db25-f291175e7748@measurement-factory.com>
Message-ID: <07d701d2ef89$7f1b76b0$7d526410$@ngtech.co.il>

Thanks Alex,

Now it makes more sense and I will try to follow there.

Eliezer

----
Eliezer Croitoru
Linux System Administrator
Mobile: +972-5-28704261
Email: eliezer at ngtech.co.il



-----Original Message-----
From: Alex Rousskov [mailto:rousskov at measurement-factory.com] 
Sent: Tuesday, June 27, 2017 22:44
To: Eliezer Croitoru <eliezer at ngtech.co.il>; 'Ralf Hildebrandt' <Ralf.Hildebrandt at charite.de>; squid-users at squid-cache.org
Subject: Re: [squid-users] Reverse DNS Lookup for client IPs

On 06/27/2017 08:19 AM, Eliezer Croitoru wrote:

> Can you put a link to the thread here?

The best relevant link is probably bug #4575:

  http://bugs.squid-cache.org/show_bug.cgi?id=4575

Alex.


> Are you talking about this thread:
> http://lists.squid-cache.org/pipermail/squid-users/2016-February/008999.html
> 
> http://squid-web-proxy-cache.1019090.n4.nabble.com/Reverse-DNS-Lookup-for-client-IPs-td4675872.html
> 
> Thanks,
> Eliezer
> 
> ----
> Eliezer Croitoru
> Linux System Administrator
> Mobile: +972-5-28704261
> Email: eliezer at ngtech.co.il
> 
> 
> -----Original Message-----
> From: squid-users [mailto:squid-users-bounces at lists.squid-cache.org] On Behalf Of Ralf Hildebrandt
> Sent: Tuesday, June 20, 2017 14:35
> To: squid-users at squid-cache.org
> Subject: [squid-users] Reverse DNS Lookup for client IPs
> 
> I have to chime in on the "Reverse DNS Lookup for client IPs" thread back in Feb 2016. I tried redefining the logging format for url_rewrite_extras and store_id_extras in the config, but that wouldn't work.
> 
> I had to change the file src/cf.data.pre and recompiled, after that the number of reverse lookups dropped considerably.
> 




From flashdown at data-core.org  Wed Jun 28 11:02:19 2017
From: flashdown at data-core.org (Enrico Heine)
Date: Wed, 28 Jun 2017 13:02:19 +0200
Subject: [squid-users] When will Squid 3.5.26 be available on Debian?
Message-ID: <153bff1e769167b77a607300d0b373d2@data-core.org>

Hello together,

anybody knows when Squid 3.5.26 will enter Debian testing nor unstable?

Best regards,
Flashdown


From Antony.Stone at squid.open.source.it  Wed Jun 28 11:06:41 2017
From: Antony.Stone at squid.open.source.it (Antony Stone)
Date: Wed, 28 Jun 2017 13:06:41 +0200
Subject: [squid-users] When will Squid 3.5.26 be available on Debian?
In-Reply-To: <153bff1e769167b77a607300d0b373d2@data-core.org>
References: <153bff1e769167b77a607300d0b373d2@data-core.org>
Message-ID: <201706281306.41962.Antony.Stone@squid.open.source.it>

On Wednesday 28 June 2017 at 13:02:19, Enrico Heine wrote:

> Hello together,
> 
> anybody knows when Squid 3.5.26 will enter Debian testing nor unstable?

Best asked on a Debian list?

Alternatively according to the package details, the maintainer is:
	Luigi Gangitano <luigi at debian.org>


Antony.

-- 
I don't know, maybe if we all waited then cosmic rays would write all our 
software for us. Of course it might take a while.

 - Ron Minnich, Los Alamos National Laboratory

                                                   Please reply to the list;
                                                         please *don't* CC me.


From squid3 at treenet.co.nz  Wed Jun 28 13:25:30 2017
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 29 Jun 2017 01:25:30 +1200
Subject: [squid-users] When will Squid 3.5.26 be available on Debian?
In-Reply-To: <153bff1e769167b77a607300d0b373d2@data-core.org>
References: <153bff1e769167b77a607300d0b373d2@data-core.org>
Message-ID: <11a22bfa-cd0d-e113-22a7-0a77a83a5563@treenet.co.nz>

On 28/06/17 23:02, Enrico Heine wrote:
> Hello together,
> 
> anybody knows when Squid 3.5.26 will enter Debian testing nor unstable?
> 

The Debian pkg-squid team has some repository alterations and updates to 
do now that Debian 9 is done. I'm not sure how long that will take as 
Luigi has been a bit occupied with other things recently.

Amos


From eliezer at ngtech.co.il  Wed Jun 28 14:20:03 2017
From: eliezer at ngtech.co.il (Eliezer Croitoru)
Date: Wed, 28 Jun 2017 17:20:03 +0300
Subject: [squid-users] When will Squid 3.5.26 be available on Debian?
In-Reply-To: <153bff1e769167b77a607300d0b373d2@data-core.org>
References: <153bff1e769167b77a607300d0b373d2@data-core.org>
Message-ID: <0a6c01d2f019$a27f0e10$e77d2a30$@ngtech.co.il>

Hey Enrico,

I didn't got any response from users about the debian package I am releasing,
Probably because it's not officially fully tested.
You can try to use the repo:
http://ngtech.co.il/repo/debian/jessie/

or download manually the deb package:
- http://ngtech.co.il/repo/debian/jessie/amd64/squid_3.5.26_amd64.deb
- http://ngtech.co.il/repo/debian/jessie/i386/squid_3.5.26_i386.deb

If you are up for testing it.

Thanks,
Eliezer

----
Eliezer Croitoru
Linux System Administrator
Mobile: +972-5-28704261
Email: eliezer at ngtech.co.il



-----Original Message-----
From: squid-users [mailto:squid-users-bounces at lists.squid-cache.org] On Behalf Of Enrico Heine
Sent: Wednesday, June 28, 2017 14:02
To: squid-users at squid-cache.org
Subject: [squid-users] When will Squid 3.5.26 be available on Debian?

Hello together,

anybody knows when Squid 3.5.26 will enter Debian testing nor unstable?

Best regards,
Flashdown
_______________________________________________
squid-users mailing list
squid-users at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-users



From flashdown at data-core.org  Wed Jun 28 14:44:00 2017
From: flashdown at data-core.org (Enrico Heine)
Date: Wed, 28 Jun 2017 16:44:00 +0200
Subject: [squid-users] When will Squid 3.5.26 be available on Debian?
In-Reply-To: <0a6c01d2f019$a27f0e10$e77d2a30$@ngtech.co.il>
References: <153bff1e769167b77a607300d0b373d2@data-core.org>
 <0a6c01d2f019$a27f0e10$e77d2a30$@ngtech.co.il>
Message-ID: <d7852a24d25a0e3ae95dffd31d117e48@data-core.org>

Dear Eliezer,

thank you for the offer, but unfortunately there is a trust issue if 
it's not from the official repo and I always build from source grapping 
the debian source and build rules. This way the possibility of overseen 
a needed adjustment for debian is very low and it is very handy to do 
so. I have many squid servers running in a large environment.

best regards,
Enrico

Am 2017-06-28 16:20, schrieb Eliezer Croitoru:
> Hey Enrico,
> 
> I didn't got any response from users about the debian package I am 
> releasing,
> Probably because it's not officially fully tested.
> You can try to use the repo:
> http://ngtech.co.il/repo/debian/jessie/
> 
> or download manually the deb package:
> - http://ngtech.co.il/repo/debian/jessie/amd64/squid_3.5.26_amd64.deb
> - http://ngtech.co.il/repo/debian/jessie/i386/squid_3.5.26_i386.deb
> 
> If you are up for testing it.
> 
> Thanks,
> Eliezer
> 
> ----
> Eliezer Croitoru
> Linux System Administrator
> Mobile: +972-5-28704261
> Email: eliezer at ngtech.co.il
> 
> 
> 
> -----Original Message-----
> From: squid-users [mailto:squid-users-bounces at lists.squid-cache.org]
> On Behalf Of Enrico Heine
> Sent: Wednesday, June 28, 2017 14:02
> To: squid-users at squid-cache.org
> Subject: [squid-users] When will Squid 3.5.26 be available on Debian?
> 
> Hello together,
> 
> anybody knows when Squid 3.5.26 will enter Debian testing nor unstable?
> 
> Best regards,
> Flashdown
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users


From mikes at surcouf.co.uk  Wed Jun 28 16:08:48 2017
From: mikes at surcouf.co.uk (Mike Surcouf)
Date: Wed, 28 Jun 2017 16:08:48 +0000
Subject: [squid-users] When will Squid 3.5.26 be available on Debian?
In-Reply-To: <d7852a24d25a0e3ae95dffd31d117e48@data-core.org>
References: <153bff1e769167b77a607300d0b373d2@data-core.org>
 <0a6c01d2f019$a27f0e10$e77d2a30$@ngtech.co.il>
 <d7852a24d25a0e3ae95dffd31d117e48@data-core.org>
Message-ID: <2197768425D7F5479A0FFB3FEC212F7FF5EBFADC@aesmail.surcouf.local>

Just to say I have been using Eliezers centos repo for a few years as the centos/rhel repos are always slow to react to new versions.
I think Eliezers repos are well respected out there.

Regards

Mike

-----Original Message-----
From: squid-users [mailto:squid-users-bounces at lists.squid-cache.org] On Behalf Of Enrico Heine
Sent: 28 June 2017 15:44
To: Eliezer Croitoru
Cc: squid-users at squid-cache.org
Subject: Re: [squid-users] When will Squid 3.5.26 be available on Debian?

Dear Eliezer,

thank you for the offer, but unfortunately there is a trust issue if it's not from the official repo and I always build from source grapping the debian source and build rules. This way the possibility of overseen a needed adjustment for debian is very low and it is very handy to do so. I have many squid servers running in a large environment.

best regards,
Enrico

Am 2017-06-28 16:20, schrieb Eliezer Croitoru:
> Hey Enrico,
> 
> I didn't got any response from users about the debian package I am 
> releasing, Probably because it's not officially fully tested.
> You can try to use the repo:
> http://ngtech.co.il/repo/debian/jessie/
> 
> or download manually the deb package:
> - http://ngtech.co.il/repo/debian/jessie/amd64/squid_3.5.26_amd64.deb
> - http://ngtech.co.il/repo/debian/jessie/i386/squid_3.5.26_i386.deb
> 
> If you are up for testing it.
> 
> Thanks,
> Eliezer
> 
> ----
> Eliezer Croitoru
> Linux System Administrator
> Mobile: +972-5-28704261
> Email: eliezer at ngtech.co.il
> 
> 
> 
> -----Original Message-----
> From: squid-users [mailto:squid-users-bounces at lists.squid-cache.org]
> On Behalf Of Enrico Heine
> Sent: Wednesday, June 28, 2017 14:02
> To: squid-users at squid-cache.org
> Subject: [squid-users] When will Squid 3.5.26 be available on Debian?
> 
> Hello together,
> 
> anybody knows when Squid 3.5.26 will enter Debian testing nor unstable?
> 
> Best regards,
> Flashdown
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
_______________________________________________
squid-users mailing list
squid-users at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-users

From flashdown at data-core.org  Wed Jun 28 16:43:43 2017
From: flashdown at data-core.org (Flashdown)
Date: Wed, 28 Jun 2017 18:43:43 +0200
Subject: [squid-users] When will Squid 3.5.26 be available on Debian?
In-Reply-To: <2197768425D7F5479A0FFB3FEC212F7FF5EBFADC@aesmail.surcouf.local>
References: <153bff1e769167b77a607300d0b373d2@data-core.org>
 <0a6c01d2f019$a27f0e10$e77d2a30$@ngtech.co.il>
 <d7852a24d25a0e3ae95dffd31d117e48@data-core.org>
 <2197768425D7F5479A0FFB3FEC212F7FF5EBFADC@aesmail.surcouf.local>
Message-ID: <62E2D62D-C579-4194-8022-93868DF39CE2@data-core.org>

Dear Mike,

Sure, but that changes nothing and has nothing to do with Eliezer. 

It's about Policies and security. So don't get it wrong. I would never take "binaries" from inofficial sources, it doesn't matter who released them.

I also can compile it on my own, no issue. But the only binaries that will go into our systems will come from official repos. 

Not everyone can choose freely as you can especially in a corporate network.

Br, Enrico



Am 28. Juni 2017 18:08:48 MESZ schrieb Mike Surcouf <mikes at surcouf.co.uk>:
>Just to say I have been using Eliezers centos repo for a few years as
>the centos/rhel repos are always slow to react to new versions.
>I think Eliezers repos are well respected out there.
>
>Regards
>
>Mike
>
>-----Original Message-----
>From: squid-users [mailto:squid-users-bounces at lists.squid-cache.org] On
>Behalf Of Enrico Heine
>Sent: 28 June 2017 15:44
>To: Eliezer Croitoru
>Cc: squid-users at squid-cache.org
>Subject: Re: [squid-users] When will Squid 3.5.26 be available on
>Debian?
>
>Dear Eliezer,
>
>thank you for the offer, but unfortunately there is a trust issue if
>it's not from the official repo and I always build from source grapping
>the debian source and build rules. This way the possibility of overseen
>a needed adjustment for debian is very low and it is very handy to do
>so. I have many squid servers running in a large environment.
>
>best regards,
>Enrico
>
>Am 2017-06-28 16:20, schrieb Eliezer Croitoru:
>> Hey Enrico,
>> 
>> I didn't got any response from users about the debian package I am 
>> releasing, Probably because it's not officially fully tested.
>> You can try to use the repo:
>> http://ngtech.co.il/repo/debian/jessie/
>> 
>> or download manually the deb package:
>> - http://ngtech.co.il/repo/debian/jessie/amd64/squid_3.5.26_amd64.deb
>> - http://ngtech.co.il/repo/debian/jessie/i386/squid_3.5.26_i386.deb
>> 
>> If you are up for testing it.
>> 
>> Thanks,
>> Eliezer
>> 
>> ----
>> Eliezer Croitoru
>> Linux System Administrator
>> Mobile: +972-5-28704261
>> Email: eliezer at ngtech.co.il
>> 
>> 
>> 
>> -----Original Message-----
>> From: squid-users [mailto:squid-users-bounces at lists.squid-cache.org]
>> On Behalf Of Enrico Heine
>> Sent: Wednesday, June 28, 2017 14:02
>> To: squid-users at squid-cache.org
>> Subject: [squid-users] When will Squid 3.5.26 be available on Debian?
>> 
>> Hello together,
>> 
>> anybody knows when Squid 3.5.26 will enter Debian testing nor
>unstable?
>> 
>> Best regards,
>> Flashdown
>> _______________________________________________
>> squid-users mailing list
>> squid-users at lists.squid-cache.org
>> http://lists.squid-cache.org/listinfo/squid-users
>_______________________________________________
>squid-users mailing list
>squid-users at lists.squid-cache.org
>http://lists.squid-cache.org/listinfo/squid-users
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20170628/5447d8ef/attachment.htm>

From rtpearson at yahoo.com  Thu Jun 29 04:11:21 2017
From: rtpearson at yahoo.com (Todd Pearson)
Date: Thu, 29 Jun 2017 04:11:21 +0000 (UTC)
Subject: [squid-users] Windows binary files for Squid Helpers download
 available anywhere for 3.5.x.x?
References: <1479744613.1088233.1498709481846.ref@mail.yahoo.com>
Message-ID: <1479744613.1088233.1498709481846@mail.yahoo.com>

Does anyone have the windows squid helper binaries built and available for download? ?If not, are there any complete instructions on how to build them? ?I am working on getting authentication working in a windows only environment and need to build the helper binaries if I can find them to download.Thanks,Todd?
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20170629/b946a625/attachment.htm>

From eliezer at ngtech.co.il  Thu Jun 29 14:11:29 2017
From: eliezer at ngtech.co.il (Eliezer Croitoru)
Date: Thu, 29 Jun 2017 17:11:29 +0300
Subject: [squid-users] Windows binary files for Squid Helpers download
	available anywhere for 3.5.x.x?
In-Reply-To: <1479744613.1088233.1498709481846@mail.yahoo.com>
References: <1479744613.1088233.1498709481846.ref@mail.yahoo.com>
 <1479744613.1088233.1498709481846@mail.yahoo.com>
Message-ID: <0e8c01d2f0e1$9aa0c120$cfe24360$@ngtech.co.il>

Hey Todd,

Can you be more specific what helper?
It's not a big issue to write something if I will know what you need it for.
A basic authentication helper like against some SQL DB or apache htpasswd file?

Eliezer

----
http://ngtech.co.il/lmgtfy/
Linux System Administrator
Mobile: +972-5-28704261
Email: eliezer at ngtech.co.il


From: squid-users [mailto:squid-users-bounces at lists.squid-cache.org] On Behalf Of Todd Pearson
Sent: Thursday, June 29, 2017 07:11
To: squid-users at lists.squid-cache.org
Subject: [squid-users] Windows binary files for Squid Helpers download available anywhere for 3.5.x.x?

Does anyone have the windows squid helper binaries built and available for download?
If not, are there any complete instructions on how to build them?
I am working on getting authentication working in a windows only environment and need to build the helper binaries if I can find them to download.
Thanks,
Todd 



From marc.boschma at gmail.com  Thu Jun 29 22:18:31 2017
From: marc.boschma at gmail.com (Marc Boschma)
Date: Fri, 30 Jun 2017 08:18:31 +1000
Subject: [squid-users] sending proxy protocol
Message-ID: <80611C94-5E1E-4D11-9593-13BB9EB5A5B6@gmail.com>

I?ve been trying to work out if Squid can inject a proxy protocol to down stream destinations? hopefully proxy protocol version 2 [1].

The scenario is as follows:

Server ? CONNECT request + out of bound request ID in HTTP header ?> ELB on 3128 added proxy protocol v1 with Server?s IP details ?> SQUID extracts proxy protocol v1 to get Server?s IP details, extracts request ID, processes CONNECT request, injects proxy-protocol v2 in front of CONNECT TLS stream with either PP2_TYPE_NETNS TLV populated with request ID or better yet a custom TLV with the same info ?> Reverse Proxy that accepts proxy-protocol v2.

The main aim of the game is to:
* be able to log as much detail as each hop to allow for correlation of logs through the infrastructure. Especially since the TLS is end to end between Server and Reverse Proxy.
* give the Reverse Proxy as much detail about the origin of the request.

I?ve been able to find that SQUID can accept the proxy protocol from the ELB? I?m curious as to whether or not the proxy protocol and TLV (NETNS or another) can be set?

Regards,

Marc


[1] https://www.haproxy.org/download/1.8/doc/proxy-protocol.txt

From squid3 at treenet.co.nz  Thu Jun 29 22:40:40 2017
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Fri, 30 Jun 2017 10:40:40 +1200
Subject: [squid-users] sending proxy protocol
In-Reply-To: <80611C94-5E1E-4D11-9593-13BB9EB5A5B6@gmail.com>
References: <80611C94-5E1E-4D11-9593-13BB9EB5A5B6@gmail.com>
Message-ID: <e6f98cbf-1a61-fc74-555f-b7c985b5db68@treenet.co.nz>

On 30/06/17 10:18, Marc Boschma wrote:
> I?ve been trying to work out if Squid can inject a proxy protocol to down stream destinations? hopefully proxy protocol version 2 [1].
> 

At present Squid can only receive PROXYv1/2 protocol.

The infrastructure is now in place to add it relatively easily if you 
want to develop a patch.

PS. If you want to sponsor development instead of writing it yourself I 
am available for that at present.

Amos


From chip_pop at hotmail.com  Fri Jun 30 09:35:02 2017
From: chip_pop at hotmail.com (joseph)
Date: Fri, 30 Jun 2017 02:35:02 -0700 (PDT)
Subject: [squid-users] error:transaction-end-before-headers
Message-ID: <1498815302455-4682945.post@n4.nabble.com>

i have over then 10000  daily this error befor was 0 or at least not much
using squid 5   latest patch  up to  r15228
my setup same nothing change 
ps.. not only those  clients ip   the hole range affected  so its not from
specific client that has some trouble so all my client produce this   

1498793967.385      0 10.3.252.229 NONE/000 0 NONE
error:transaction-end-before-headers - HIER_NONE/- -
1498793967.385      0 10.3.252.229 NONE/000 0 NONE
error:transaction-end-before-headers - HIER_NONE/- -
1498793967.385      0 10.3.252.229 NONE/000 0 NONE
error:transaction-end-before-headers - HIER_NONE/- -
1498793967.385      0 10.3.252.229 NONE/000 0 NONE
error:transaction-end-before-headers - HIER_NONE/- -
1498793967.385      0 10.3.252.229 NONE/000 0 NONE
error:transaction-end-before-headers - HIER_NONE/- -
1498793967.385      0 10.3.252.229 NONE/000 0 NONE
error:transaction-end-before-headers - HIER_NONE/- -
1498793967.387      0 10.3.252.229 NONE/000 0 NONE
error:transaction-end-before-headers - HIER_NONE/- -
1498793967.387      0 10.3.252.229 NONE/000 0 NONE
error:transaction-end-before-headers - HIER_NONE/- -
1498793967.387      0 10.3.252.229 NONE/000 0 NONE
error:transaction-end-before-headers - HIER_NONE/- -
1498793967.387      0 10.3.252.229 NONE/000 0 NONE
error:transaction-end-before-headers - HIER_NONE/- -
1498793967.387      0 10.3.252.229 NONE/000 0 NONE
error:transaction-end-before-headers - HIER_NONE/- -
1498793967.387      0 10.3.252.229 NONE/000 0 NONE
error:transaction-end-before-headers - HIER_NONE/- -
1498793967.387      0 10.3.252.229 NONE/000 0 NONE
error:transaction-end-before-headers - HIER_NONE/- -
1498793967.387      0 10.3.252.229 NONE/000 0 NONE
error:transaction-end-before-headers - HIER_NONE/- -
1498793967.391      0 10.3.252.229 NONE/000 0 NONE
error:transaction-end-before-headers - HIER_NONE/- -
1498793967.391      0 10.3.252.229 NONE/000 0 NONE
error:transaction-end-before-headers - HIER_NONE/- -
1498793967.391      0 10.3.252.229 NONE/000 0 NONE
error:transaction-end-before-headers - HIER_NONE/- -
1498793967.391      0 10.3.252.229 NONE/000 0 NONE
error:transaction-end-before-headers - HIER_NONE/- -
1498793970.692      0 10.3.255.106 NONE/000 0 NONE
error:transaction-end-before-headers - HIER_NONE/- -
1498793973.205      0 10.3.254.96 NONE/000 0 NONE
error:transaction-end-before-headers - HIER_NONE/- -
1498793979.237      0 10.3.254.152 NONE/000 0 NONE
error:transaction-end-before-headers - HIER_NONE/- -
1498793979.702      0 10.3.252.174 NONE/000 0 NONE
error:transaction-end-before-headers - HIER_NONE/- -
1498793980.841      0 10.3.254.96 NONE/000 0 NONE
error:transaction-end-before-headers - HIER_NONE/- -
1498793980.841      0 10.3.254.96 NONE/000 0 NONE
error:transaction-end-before-headers - HIER_NONE/- -
1498793980.841      0 10.3.254.96 NONE/000 0 NONE
error:transaction-end-before-headers - HIER_NONE/- -
1498793980.841      0 10.3.254.96 NONE/000 0 NONE
error:transaction-end-before-headers - HIER_NONE/- -
1498793982.845      0 10.3.254.96 NONE/000 0 NONE
error:transaction-end-before-headers - HIER_NONE/- -
1498793982.845      0 10.3.254.96 NONE/000 0 NONE
error:transaction-end-before-headers - HIER_NONE/- -
1498793988.355      0 10.3.254.96 NONE/000 0 NONE
error:transaction-end-before-headers - HIER_NONE/- -
1498793988.356      0 10.3.254.96 NONE/000 0 NONE
error:transaction-end-before-headers - HIER_NONE/- -
1498793988.569      0 10.3.252.174 NONE/000 0 NONE
error:transaction-end-before-headers - HIER_NONE/- -
1498793991.236      0 10.3.254.96 NONE/000 0 NONE
error:transaction-end-before-headers - HIER_NONE/- -
1498793991.236      0 10.3.254.96 NONE/000 0 NONE
error:transaction-end-before-headers - HIER_NONE/- -
1498793992.237      0 10.3.254.96 NONE/000 0 NONE
error:transaction-end-before-headers - HIER_NONE/- -
1498793992.237      0 10.3.254.96 NONE/000 0 NONE
error:transaction-end-before-headers - HIER_NONE/- -
1498793993.237      0 10.3.254.96 NONE/000 0 NONE
error:transaction-end-before-headers - HIER_NONE/- -
1498793994.239      0 10.3.254.96 NONE/000 0 NONE
error:transaction-end-before-headers - HIER_NONE/- -
1498793994.239      0 10.3.254.96 NONE/000 0 NONE
error:transaction-end-before-headers - HIER_NONE/- -
1498793995.241      0 10.3.254.96 NONE/000 0 NONE
error:transaction-end-before-headers - HIER_NONE/- -
1498793995.242      0 10.3.254.96 NONE/000 0 NONE
error:transaction-end-before-headers - HIER_NONE/- -
1498793996.146      0 10.3.252.174 NONE/000 0 NONE
error:transaction-end-before-headers - HIER_NONE/- -
1498794028.229      0 10.3.254.96 NONE/000 0 NONE
error:transaction-end-before-headers - HIER_NONE/- -
1498794028.229      0 10.3.254.96 NONE/000 0 NONE
error:transaction-end-before-headers - HIER_NONE/- -
1498794034.188      0 10.3.252.174 NONE/000 0 NONE
error:transaction-end-before-headers - HIER_NONE/- -
1498794037.041      0 10.3.252.174 NONE/000 0 NONE
error:transaction-end-before-headers - HIER_NONE/- -
1498794037.042      0 10.3.252.174 NONE/000 0 NONE
error:transaction-end-before-headers - HIER_NONE/- -
1498794037.042      0 10.3.252.174 NONE/000 0 NONE
error:transaction-end-before-headers - HIER_NONE/- -
1498794058.094      0 10.3.252.174 NONE/000 0 NONE
error:transaction-end-before-headers - HIER_NONE/- -
1498794064.108      0 10.3.252.174 NONE/000 0 NONE
error:transaction-end-before-headers - HIER_NONE/- -
1498794069.832      0 10.3.252.174 NONE/000 0 NONE
error:transaction-end-before-headers - HIER_NONE/- -



--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/error-transaction-end-before-headers-tp4682945.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From rousskov at measurement-factory.com  Fri Jun 30 15:30:06 2017
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Fri, 30 Jun 2017 09:30:06 -0600
Subject: [squid-users] error:transaction-end-before-headers
In-Reply-To: <1498815302455-4682945.post@n4.nabble.com>
References: <1498815302455-4682945.post@n4.nabble.com>
Message-ID: <1f310a8a-ae81-c4a6-463b-318bb92be401@measurement-factory.com>

On 06/30/2017 03:35 AM, joseph wrote:
> i have over then 10000  daily this error befor was 0 or at least not much
> using squid 5   latest patch  up to  r15228
> my setup same nothing change

What Squid version did you run "before"?


> ps.. all my client produce this   
> 
> 1498793967.385      0 10.3.252.229 NONE/000 0 NONE
> error:transaction-end-before-headers - HIER_NONE/- -

These errors are meant to be logged for clients that open and close
connections without sending any HTTP headers (or without sending
complete HTTP headers -- you can log HTTP request size to distinguish
these two cases).

I do not know whether Squid has a bug identifying these cases or you
just have a lot of these cases in your environment. If these are
frequent, then you should be able to learn more about them by capturing
network packets and matching them with the errors. Logging the
client/source TCP port number would help with that. There are a couple
of known problems with the detection code, but if this is some
new/unknown problem, it would be useful for developers to know more
about it. Please investigate if you can.

I do not know whether it is possible to match these cases with existing
ACLs so that you can stop logging them (if that is what you want).
Adding an ACL for this purpose is a known TODO, but I am not aware of
anybody working on that or sponsoring that work.

If you want to modify Squid to stop logging these cases, add "return;"
at the very beginning of ConnStateData::checkLogging() in
src/client_side.cc.

You forgot to ask a question, but I hope the above info is useful.


HTH,

Alex.


From chip_pop at hotmail.com  Fri Jun 30 15:23:57 2017
From: chip_pop at hotmail.com (joseph)
Date: Fri, 30 Jun 2017 08:23:57 -0700 (PDT)
Subject: [squid-users] error:transaction-end-before-headers
In-Reply-To: <1f310a8a-ae81-c4a6-463b-318bb92be401@measurement-factory.com>
References: <1498815302455-4682945.post@n4.nabble.com>
 <1f310a8a-ae81-c4a6-463b-318bb92be401@measurement-factory.com>
Message-ID: <1498836237112-4682947.post@n4.nabble.com>

tks alex  i will investigate more i will capture port and link to identify
the causes and packet as well
i dont remember on wish  patch start  getting those more and more 
i may have to start  removing patch  one by one and test

its ben a one weak since i monitor the acsess.log error  before that was
less  huge difference 



--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/error-transaction-end-before-headers-tp4682945p4682947.html
Sent from the Squid - Users mailing list archive at Nabble.com.


