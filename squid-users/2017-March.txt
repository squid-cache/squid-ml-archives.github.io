From erdosain9 at gmail.com  Wed Mar  1 12:55:37 2017
From: erdosain9 at gmail.com (erdosain9)
Date: Wed, 1 Mar 2017 04:55:37 -0800 (PST)
Subject: [squid-users] Unspecified GSS failure ERROR
Message-ID: <1488372937217-4681636.post@n4.nabble.com>

Hi.
Today the users cant go to internet because the web browser asking everytime
the user and pass, constantly.

This is the error in cache.log:

2017/03/01 08:22:16 kid1| ERROR: Negotiate Authentication validating user.
Result: {result=BH, notes={message: gss_accept_sec_context() failed:
Unspecified GSS failure.  Minor code may provide more information. Cannot
find key for HTTP/squid.xxxxxxxxxxxxxx.lan at XXXXXXXXXXXXXX.LAN kvno 22 in
keytab; }}

Just stop and start the service put squid working again... but.... what
happend???

Thanks for all.
(i dont speak english.....)




--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/Unspecified-GSS-failure-ERROR-tp4681636.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From carlopmart at gmail.com  Wed Mar  1 13:44:41 2017
From: carlopmart at gmail.com (C. L. Martinez)
Date: Wed, 1 Mar 2017 13:44:41 +0000
Subject: [squid-users] Build errors with Squid 3.5.24 under Debian
Message-ID: <20170301134441.wukqkkwatxjbeix2@scotland.uxdom.org>

Hi all

 I am trying to build Squid 3.5.24 release under a Debian Testing and returns the following errors:

depbase=`echo PortCfg.lo | sed 's|[^/]*$|.deps/&|;s|\.lo$||'`;\
/bin/bash ../../libtool  --tag=CXX   --mode=compile g++ -DHAVE_CONFIG_H   -I../.. -I../../include -I../../lib -I../../src -I../../include  -isystem /usr/include/mit-krb5  -I/usr/include/libxml2  -I/usr/include/libxml2 -Wall -Wpointer-arith -Wwrite-strings -Wcomments -Wshadow -Woverloaded-virtual -Werror -pipe -D_REENTRANT -m64  -g -O2 -MT PortCfg.lo -MD -MP -MF $depbase.Tpo -c -o PortCfg.lo PortCfg.cc &&\
mv -f $depbase.Tpo $depbase.Plo
libtool: compile:  g++ -DHAVE_CONFIG_H -I../.. -I../../include -I../../lib -I../../src -I../../include -isystem /usr/include/mit-krb5 -I/usr/include/libxml2 -I/usr/include/libxml2 -Wall -Wpointer-arith -Wwrite-strings -Wcomments -Wshadow -Woverloaded-virtual -Werror -pipe -D_REENTRANT -m64 -g -O2 -MT PortCfg.lo -MD -MP -MF .deps/PortCfg.Tpo -c PortCfg.cc  -fPIC -DPIC -o .libs/PortCfg.o
In file included from ../../src/anyp/PortCfg.h:18:0,
                 from PortCfg.cc:10:
../../src/ssl/gadgets.h:83:45: error: ?CRYPTO_LOCK_X509? was not declared in this scope
 typedef LockingPointer<X509, X509_free_cpp, CRYPTO_LOCK_X509> X509_Pointer;
                                             ^~~~~~~~~~~~~~~~
../../src/ssl/gadgets.h:83:61: error: template argument 3 is invalid
 typedef LockingPointer<X509, X509_free_cpp, CRYPTO_LOCK_X509> X509_Pointer;
                                                             ^
../../src/ssl/gadgets.h:89:53: error: ?CRYPTO_LOCK_EVP_PKEY? was not declared in this scope
 typedef LockingPointer<EVP_PKEY, EVP_PKEY_free_cpp, CRYPTO_LOCK_EVP_PKEY> EVP_PKEY_Pointer;
                                                     ^~~~~~~~~~~~~~~~~~~~
../../src/ssl/gadgets.h:89:73: error: template argument 3 is invalid
 typedef LockingPointer<EVP_PKEY, EVP_PKEY_free_cpp, CRYPTO_LOCK_EVP_PKEY> EVP_PKEY_Pointer;
                                                                         ^
../../src/ssl/gadgets.h:116:43: error: ?CRYPTO_LOCK_SSL? was not declared in this scope
 typedef LockingPointer<SSL, SSL_free_cpp, CRYPTO_LOCK_SSL> SSL_Pointer;
                                           ^~~~~~~~~~~~~~~
../../src/ssl/gadgets.h:116:58: error: template argument 3 is invalid
 typedef LockingPointer<SSL, SSL_free_cpp, CRYPTO_LOCK_SSL> SSL_Pointer;
                                                          ^
Makefile:791: recipe for target 'PortCfg.lo' failed
make[3]: *** [PortCfg.lo] Error 1
make[3]: Leaving directory '/tmp/o/squid-3.5.24/src/anyp'
Makefile:7296: recipe for target 'all-recursive' failed
make[2]: *** [all-recursive] Error 1
make[2]: Leaving directory '/tmp/o/squid-3.5.24/src'
Makefile:6157: recipe for target 'all' failed
make[1]: *** [all] Error 2
make[1]: Leaving directory '/tmp/o/squid-3.5.24/src'
Makefile:581: recipe for target 'all-recursive' failed
make: *** [all-recursive] Error 1

 I have installed the following packages:

 build-essential autoconf automake libtool flex bison autoconf-archive gnu-standards libecap3-dev libssl-dev libcurl4-openssl-dev pkg-config libsasl2-dev libxml2-dev libkrb5-dev libnetfilter-conntrack-dev libcppunit-dev libcap-dev libevent-dev nettle-dev

 And my configure options are:

 ./configure --prefix=/opt/squid --sysconfdir=/etc/squid --enable-inline --disable-arch-native --enable-async-io=8 --enable-storeio="ufs,aufs,diskd,rock" --enable-removal-policies="lru,heap" --enable-delay-pools --enable-cache-digests --enable-icap-client --enable-follow-x-forwarded-for --enable-eui --enable-esi --enable-icmp --enable-zph-qos --enable-ecap --disable-translation --with-swapdir=/var/spool/squid --with-logdir=/var/log/squid --with-pidfile=/var/run/squid.pid --with-filedescriptors=65536 --with-large-files --with-default-user=proxy --enable-ssl-crtd  --enable-linux-netfilter --with-openssl

 Where am I doing the mistake??

-- 
Greetings,
C. L. Martinez


From belle at bazuin.nl  Wed Mar  1 14:42:01 2017
From: belle at bazuin.nl (=?windows-1252?Q?L.P.H._van_Belle?=)
Date: Wed, 1 Mar 2017 15:42:01 +0100
Subject: [squid-users] Build errors with Squid 3.5.24 under Debian
In-Reply-To: <20170301134441.wukqkkwatxjbeix2@scotland.uxdom.org>
References: <20170301134441.wukqkkwatxjbeix2@scotland.uxdom.org>
Message-ID: <vmime.58b6ddb9.100c.6f5bfa0f7f7deec9@ms249-lin-003.rotterdam.bazuin.nl>

Hai, 

>I am trying to build Squid 3.5.24 release under a Debian Testing
Debian testing already has 3.5.23 so this should be very easy... 

# check if you have all dependecies.
apt-get build-deb squid

# Install you missing files if you did not see them. 
apt-get install libssl-dev libcrypto++-dev

apt-get source squid

#extract the 3.5.24 from squid-cache. 
#Copy the debian folder from 3.5.23 to 3.5.24

Now change : 
debian/rules 
( add --enable-ssl --with-openssl ) to the DEB_CONFIGURE_EXTRA_FLAGS
Change the  debian/changelog. 

cd squid-3.5.24
dpkg-buildpackage -us -uc -b

Something like that, if you need more info, just mail me. 

Im runnig 3.5.24-ssl in jessie. 
If you want these packages (source and debs), same.. just mail me. 

Greetz, 

Louis


> -----Oorspronkelijk bericht-----
> Van: squid-users [mailto:squid-users-bounces at lists.squid-cache.org] Namens
> C. L. Martinez
> Verzonden: woensdag 1 maart 2017 14:45
> Aan: squid-users at lists.squid-cache.org
> Onderwerp: [squid-users] Build errors with Squid 3.5.24 under Debian
> 
> Hi all
> 
>  I am trying to build Squid 3.5.24 release under a Debian Testing and
> returns the following errors:
> 
> depbase=`echo PortCfg.lo | sed 's|[^/]*$|.deps/&|;s|\.lo$||'`;\
> /bin/bash ../../libtool  --tag=CXX   --mode=compile g++ -DHAVE_CONFIG_H
> -I../.. -I../../include -I../../lib -I../../src -I../../include  -isystem
> /usr/include/mit-krb5  -I/usr/include/libxml2  -I/usr/include/libxml2 -
> Wall -Wpointer-arith -Wwrite-strings -Wcomments -Wshadow -Woverloaded-
> virtual -Werror -pipe -D_REENTRANT -m64  -g -O2 -MT PortCfg.lo -MD -MP -MF
> $depbase.Tpo -c -o PortCfg.lo PortCfg.cc &&\
> mv -f $depbase.Tpo $depbase.Plo
> libtool: compile:  g++ -DHAVE_CONFIG_H -I../.. -I../../include -I../../lib
> -I../../src -I../../include -isystem /usr/include/mit-krb5 -
> I/usr/include/libxml2 -I/usr/include/libxml2 -Wall -Wpointer-arith -
> Wwrite-strings -Wcomments -Wshadow -Woverloaded-virtual -Werror -pipe -
> D_REENTRANT -m64 -g -O2 -MT PortCfg.lo -MD -MP -MF .deps/PortCfg.Tpo -c
> PortCfg.cc  -fPIC -DPIC -o .libs/PortCfg.o
> In file included from ../../src/anyp/PortCfg.h:18:0,
>                  from PortCfg.cc:10:
> ../../src/ssl/gadgets.h:83:45: error: ?CRYPTO_LOCK_X509? was not declared
> in this scope
>  typedef LockingPointer<X509, X509_free_cpp, CRYPTO_LOCK_X509>
> X509_Pointer;
>                                              ^~~~~~~~~~~~~~~~
> ../../src/ssl/gadgets.h:83:61: error: template argument 3 is invalid
>  typedef LockingPointer<X509, X509_free_cpp, CRYPTO_LOCK_X509>
> X509_Pointer;
>                                                              ^
> ../../src/ssl/gadgets.h:89:53: error: ?CRYPTO_LOCK_EVP_PKEY? was not
> declared in this scope
>  typedef LockingPointer<EVP_PKEY, EVP_PKEY_free_cpp, CRYPTO_LOCK_EVP_PKEY>
> EVP_PKEY_Pointer;
>                                                      ^~~~~~~~~~~~~~~~~~~~
> ../../src/ssl/gadgets.h:89:73: error: template argument 3 is invalid
>  typedef LockingPointer<EVP_PKEY, EVP_PKEY_free_cpp, CRYPTO_LOCK_EVP_PKEY>
> EVP_PKEY_Pointer;
>                                                                          ^
> ../../src/ssl/gadgets.h:116:43: error: ?CRYPTO_LOCK_SSL? was not declared
> in this scope
>  typedef LockingPointer<SSL, SSL_free_cpp, CRYPTO_LOCK_SSL> SSL_Pointer;
>                                            ^~~~~~~~~~~~~~~
> ../../src/ssl/gadgets.h:116:58: error: template argument 3 is invalid
>  typedef LockingPointer<SSL, SSL_free_cpp, CRYPTO_LOCK_SSL> SSL_Pointer;
>                                                           ^
> Makefile:791: recipe for target 'PortCfg.lo' failed
> make[3]: *** [PortCfg.lo] Error 1
> make[3]: Leaving directory '/tmp/o/squid-3.5.24/src/anyp'
> Makefile:7296: recipe for target 'all-recursive' failed
> make[2]: *** [all-recursive] Error 1
> make[2]: Leaving directory '/tmp/o/squid-3.5.24/src'
> Makefile:6157: recipe for target 'all' failed
> make[1]: *** [all] Error 2
> make[1]: Leaving directory '/tmp/o/squid-3.5.24/src'
> Makefile:581: recipe for target 'all-recursive' failed
> make: *** [all-recursive] Error 1
> 
>  I have installed the following packages:
> 
>  build-essential autoconf automake libtool flex bison autoconf-archive
> gnu-standards libecap3-dev libssl-dev libcurl4-openssl-dev pkg-config
> libsasl2-dev libxml2-dev libkrb5-dev libnetfilter-conntrack-dev
> libcppunit-dev libcap-dev libevent-dev nettle-dev
> 
>  And my configure options are:
> 
>  ./configure --prefix=/opt/squid --sysconfdir=/etc/squid --enable-inline -
> -disable-arch-native --enable-async-io=8 --enable-
> storeio="ufs,aufs,diskd,rock" --enable-removal-policies="lru,heap" --
> enable-delay-pools --enable-cache-digests --enable-icap-client --enable-
> follow-x-forwarded-for --enable-eui --enable-esi --enable-icmp --enable-
> zph-qos --enable-ecap --disable-translation --with-
> swapdir=/var/spool/squid --with-logdir=/var/log/squid --with-
> pidfile=/var/run/squid.pid --with-filedescriptors=65536 --with-large-files
> --with-default-user=proxy --enable-ssl-crtd  --enable-linux-netfilter --
> with-openssl
> 
>  Where am I doing the mistake??
> 
> --
> Greetings,
> C. L. Martinez
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users



From erdosain9 at gmail.com  Wed Mar  1 15:06:37 2017
From: erdosain9 at gmail.com (erdosain9)
Date: Wed, 1 Mar 2017 07:06:37 -0800 (PST)
Subject: [squid-users] Failed to shm_open
Message-ID: <1488380797187-4681639.post@n4.nabble.com>

Hi.
Now squid stop... abnormaly.

2017/03/01 12:04:31 kid1| helperOpenServers: Starting 5/32 'ssl_crtd'
processes
FATAL: Ipc::Mem::Segment::open failed to
shm_open(/squid-ssl_session_cache.shm): (2) No such file or directory

Squid Cache (Version 3.5.20): Terminated abnormally.
CPU Usage: 0.095 seconds = 0.074 user + 0.021 sys
Maximum Resident Size: 134144 KB
Page faults with physical i/o: 0
2017/03/01 12:04:31| Set Current Directory to /var/spool/squid

What is happend??



--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/Failed-to-shm-open-tp4681639.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From erdosain9 at gmail.com  Wed Mar  1 15:21:14 2017
From: erdosain9 at gmail.com (erdosain9)
Date: Wed, 1 Mar 2017 07:21:14 -0800 (PST)
Subject: [squid-users] Failed to shm_open
In-Reply-To: <1488380797187-4681639.post@n4.nabble.com>
References: <1488380797187-4681639.post@n4.nabble.com>
Message-ID: <1488381674522-4681640.post@n4.nabble.com>

Im having this problem too
mar 01 12:23:37 squid.xxxxxxxxxxxxxxxxx.lan squid[17628]: Error negotiating
SSL connection on FD 181: error:1408A0C1:SSL
routines:SSL3_GET_CLIENT_HELLO:no shared cipher (1/-1)
mar 01 12:23:38 squid.xxxxxxxxxxxxxxxxx.lan squid[17628]: Error negotiating
SSL connection on FD 467: error:1408A0C1:SSL
routines:SSL3_GET_CLIENT_HELLO:no shared cipher (1/-1)
mar 01 12:23:38 squid.xxxxxxxxxxxxxxxxx.lan squid[17628]: Error negotiating
SSL connection on FD 471: error:1408A0C1:SSL
routines:SSL3_GET_CLIENT_HELLO:no shared cipher (1/-1)
mar 01 12:23:39 squid.xxxxxxxxxxxxxxxxx.lan squid[17628]: Error negotiating
SSL connection on FD 414: error:1408A0C1:SSL
routines:SSL3_GET_CLIENT_HELLO:no shared cipher (1/-1)
mar 01 12:23:39 squid.xxxxxxxxxxxxxxxxx.lan squid[17628]: Error negotiating
SSL connection on FD 446: error:1408A0C1:SSL
routines:SSL3_GET_CLIENT_HELLO:no shared cipher (1/-1)
mar 01 12:23:41 squid.xxxxxxxxxxxxxxxxx.lan squid[17628]: Error negotiating
SSL connection on FD 266: error:1408A0C1:SSL
routines:SSL3_GET_CLIENT_HELLO:no shared cipher (1/-1)
mar 01 12:23:42 squid.xxxxxxxxxxxxxxxxx.lan squid[17628]: Error negotiating
SSL connection on FD 276: error:1408A0C1:SSL
routines:SSL3_GET_CLIENT_HELLO:no shared cipher (1/-1)
mar 01 12:23:42 squid.xxxxxxxxxxxxxxxxx.lan squid[17628]: Error negotiating
SSL connection on FD 266: error:1408A0C1:SSL
routines:SSL3_GET_CLIENT_HELLO:no shared cipher (1/-1)
mar 01 12:23:43 squid.xxxxxxxxxxxxxxxxx.lan squid[17628]: Error negotiating
SSL connection on FD 211: error:1408A0C1:SSL
routines:SSL3_GET_CLIENT_HELLO:no shared cipher (1/-1)
mar 01 12:23:43 squid.xxxxxxxxxxxxxxxxx.lan squid[17628]: Error negotiating
SSL connection on FD 136: error:1408A0C1:SSL
routines:SSL3_GET_CLIENT_HELLO:no shared cipher (1/-1)



--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/Failed-to-shm-open-tp4681639p4681640.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From squid3 at treenet.co.nz  Wed Mar  1 16:31:18 2017
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 2 Mar 2017 05:31:18 +1300
Subject: [squid-users] Build errors with Squid 3.5.24 under Debian
In-Reply-To: <vmime.58b6ddb9.100c.6f5bfa0f7f7deec9@ms249-lin-003.rotterdam.bazuin.nl>
References: <20170301134441.wukqkkwatxjbeix2@scotland.uxdom.org>
 <vmime.58b6ddb9.100c.6f5bfa0f7f7deec9@ms249-lin-003.rotterdam.bazuin.nl>
Message-ID: <e4287dd4-ceb1-3d58-c80e-6e7b315db838@treenet.co.nz>

On 2/03/2017 3:42 a.m., L.P.H. van Belle wrote:
> Hai, 
> 
>> I am trying to build Squid 3.5.24 release under a Debian Testing
> Debian testing already has 3.5.23 so this should be very easy... 
> 

Debian Testing/sretch and sid contain OpenSSL v1.1. BUt Squid-3 only
supports OpenSSL v1.0.

You need to install libssl1.0-dev there instead of libssl-dev.

Amos



From squid3 at treenet.co.nz  Wed Mar  1 17:05:29 2017
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 2 Mar 2017 06:05:29 +1300
Subject: [squid-users] SSL Bump and Certificate issue - RapidSSL
 Intermediate Cert
In-Reply-To: <1488297480314-4681635.post@n4.nabble.com>
References: <1488297480314-4681635.post@n4.nabble.com>
Message-ID: <899f1712-9b39-e052-a075-80609be34275@treenet.co.nz>

On 1/03/2017 4:58 a.m., stylemessiah wrote:
> This is driving me nuts, its the only issue ive found running ssl bump on my
> home network for eons
> 
> I cant see image thumbnails on xda-developers...
> 
> When i access a thread with them, i get text links, not thumbnails, and if i
> click on the links i get the following:
> 
> 
>     (71) Protocol error (TLS code:
> X509_V_ERR_UNABLE_TO_GET_ISSUER_CERT_LOCALLY)
> 
>     SSL Certficate error: certificate issuer (CA) not known:
> /C=US/O=GeoTrust Inc./CN=RapidSSL SHA256 CA
> 
> I figured out by googling how to (i hope) trace the problem certificate via
> s_client:
> 
> 
> OpenSSL> s_client -showcerts -verify 32 -connect dl.xda-developers.com:443
> verify depth is 32
> CONNECTED(0000012C)
> depth=0 CN = *.xda-developers.com
> verify error:num=20:unable to get local issuer certificate
> verify return:1
> depth=0 CN = *.xda-developers.com
> verify error:num=21:unable to verify the first certificate
> verify return:1

That command you used does not send data through the proxy. So that
confirms that the servers TLS is broken in a way unrelated to Squid.



> ---
> Certificate chain
>  0 s:/CN=*.xda-developers.com
>    i:/C=US/O=GeoTrust Inc./CN=RapidSSL SHA256 CA
...
> ---
> Server certificate
> subject=/CN=*.xda-developers.com
> issuer=/C=US/O=GeoTrust Inc./CN=RapidSSL SHA256 CA
> ---
> No client certificate CA names sent
> Peer signing digest: SHA512
> Server Temp Key: ECDH, P-256, 256 bits
> ---
> SSL handshake has read 2067 bytes and written 302 bytes
> Verification error: unable to verify the first certificate

> 
> Ive found the intermediate bundle from RapidSS, and added it to my existing
> pem bundle...no change

You need to locate the root CA and/or intermediate CA certificates used
to sign the domain servers certificate.

You then need to identify *why* they are not being trusted by your OS
library.

Be sure to determine whether the CA which is missing is actually
trustworthy before adding it to your trusted set. More than a few of the
CA which are around are not trusted because they have been hacked or
caught signing forged certificates they should not have.


> Added as a separate pem i.e. sslproxy_foreign_intermediate_certs
> /cygdrive/e/Squid/etc/ssl/extra-intermediate-CA.pem...no change
> 
> My sslbump related config lines are:
> 
> http_port 127.0.0.1:3128 ssl-bump generate-host-certificates=on
> dynamic_cert_mem_cache_size=10MB cert=/cygdrive/e/Squid/etc/ssl/myCA.pem
> capath=/cygdrive/e/Squid/etc/ssl
> cafile=/cygdrive/e/Squid/etc/ssl/extra-intermediate-CA.pem
> tls-dh=/cygdrive/e/Squid/etc/ssl/dhparam.pem
> options=NO_SSLv2,NO_SSLv3,SINGLE_ECDH_USE 

PS.  EECDH will not work unless you configure a curve name in the
tls-dh= option. Just having dhparam.pem alone will only enable the less
secure DH ciphers.

Amos



From adrian.m.miller at gmail.com  Wed Mar  1 17:03:47 2017
From: adrian.m.miller at gmail.com (stylemessiah)
Date: Wed, 1 Mar 2017 09:03:47 -0800 (PST)
Subject: [squid-users] SSL Bump and Certificate issue - RapidSSL
	Intermediate Cert
In-Reply-To: <899f1712-9b39-e052-a075-80609be34275@treenet.co.nz>
References: <1488297480314-4681635.post@n4.nabble.com>
 <899f1712-9b39-e052-a075-80609be34275@treenet.co.nz>
Message-ID: <CAOLOQx36wSy24sDDS-Qm=BSAeGsS5oiT5kGK5kP7s=sMQEffpQ@mail.gmail.com>

Thanks Amos for the info, appreciate your tireless assistance for us
numpties :)

On 2 Mar. 2017 4:06 am, "Amos Jeffries [via Squid Web Proxy Cache]" <
ml-node+s1019090n4681642h47 at n4.nabble.com> wrote:

> On 1/03/2017 4:58 a.m., stylemessiah wrote:
>
> > This is driving me nuts, its the only issue ive found running ssl bump
> on my
> > home network for eons
> >
> > I cant see image thumbnails on xda-developers...
> >
> > When i access a thread with them, i get text links, not thumbnails, and
> if i
> > click on the links i get the following:
> >
> >
> >     (71) Protocol error (TLS code:
> > X509_V_ERR_UNABLE_TO_GET_ISSUER_CERT_LOCALLY)
> >
> >     SSL Certficate error: certificate issuer (CA) not known:
> > /C=US/O=GeoTrust Inc./CN=RapidSSL SHA256 CA
> >
> > I figured out by googling how to (i hope) trace the problem certificate
> via
> > s_client:
> >
> >
> > OpenSSL> s_client -showcerts -verify 32 -connect
> dl.xda-developers.com:443
> > verify depth is 32
> > CONNECTED(0000012C)
> > depth=0 CN = *.xda-developers.com
> > verify error:num=20:unable to get local issuer certificate
> > verify return:1
> > depth=0 CN = *.xda-developers.com
> > verify error:num=21:unable to verify the first certificate
> > verify return:1
>
> That command you used does not send data through the proxy. So that
> confirms that the servers TLS is broken in a way unrelated to Squid.
>
>
>
> > ---
> > Certificate chain
> >  0 s:/CN=*.xda-developers.com
> >    i:/C=US/O=GeoTrust Inc./CN=RapidSSL SHA256 CA
> ...
>
> > ---
> > Server certificate
> > subject=/CN=*.xda-developers.com
> > issuer=/C=US/O=GeoTrust Inc./CN=RapidSSL SHA256 CA
> > ---
> > No client certificate CA names sent
> > Peer signing digest: SHA512
> > Server Temp Key: ECDH, P-256, 256 bits
> > ---
> > SSL handshake has read 2067 bytes and written 302 bytes
> > Verification error: unable to verify the first certificate
>
> >
> > Ive found the intermediate bundle from RapidSS, and added it to my
> existing
> > pem bundle...no change
>
> You need to locate the root CA and/or intermediate CA certificates used
> to sign the domain servers certificate.
>
> You then need to identify *why* they are not being trusted by your OS
> library.
>
> Be sure to determine whether the CA which is missing is actually
> trustworthy before adding it to your trusted set. More than a few of the
> CA which are around are not trusted because they have been hacked or
> caught signing forged certificates they should not have.
>
>
> > Added as a separate pem i.e. sslproxy_foreign_intermediate_certs
> > /cygdrive/e/Squid/etc/ssl/extra-intermediate-CA.pem...no change
> >
> > My sslbump related config lines are:
> >
> > http_port 127.0.0.1:3128 ssl-bump generate-host-certificates=on
> > dynamic_cert_mem_cache_size=10MB cert=/cygdrive/e/Squid/etc/ssl/myCA.pem
>
> > capath=/cygdrive/e/Squid/etc/ssl
> > cafile=/cygdrive/e/Squid/etc/ssl/extra-intermediate-CA.pem
> > tls-dh=/cygdrive/e/Squid/etc/ssl/dhparam.pem
> > options=NO_SSLv2,NO_SSLv3,SINGLE_ECDH_USE
>
> PS.  EECDH will not work unless you configure a curve name in the
> tls-dh= option. Just having dhparam.pem alone will only enable the less
> secure DH ciphers.
>
> Amos
>
> _______________________________________________
> squid-users mailing list
> [hidden email] <http:///user/SendEmail.jtp?type=node&node=4681642&i=0>
> http://lists.squid-cache.org/listinfo/squid-users
>
>
> ------------------------------
> If you reply to this email, your message will be added to the discussion
> below:
> http://squid-web-proxy-cache.1019090.n4.nabble.com/SSL-
> Bump-and-Certificate-issue-RapidSSL-Intermediate-Cert-
> tp4681635p4681642.html
> To unsubscribe from SSL Bump and Certificate issue - RapidSSL Intermediate
> Cert, click here
> <http://squid-web-proxy-cache.1019090.n4.nabble.com/template/NamlServlet.jtp?macro=unsubscribe_by_code&node=4681635&code=YWRyaWFuLm0ubWlsbGVyQGdtYWlsLmNvbXw0NjgxNjM1fDE5ODY3MjIyMDI=>
> .
> NAML
> <http://squid-web-proxy-cache.1019090.n4.nabble.com/template/NamlServlet.jtp?macro=macro_viewer&id=instant_html%21nabble%3Aemail.naml&base=nabble.naml.namespaces.BasicNamespace-nabble.view.web.template.NabbleNamespace-nabble.view.web.template.NodeNamespace&breadcrumbs=notify_subscribers%21nabble%3Aemail.naml-instant_emails%21nabble%3Aemail.naml-send_instant_email%21nabble%3Aemail.naml>
>




--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/SSL-Bump-and-Certificate-issue-RapidSSL-Intermediate-Cert-tp4681635p4681643.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From squid3 at treenet.co.nz  Wed Mar  1 17:19:27 2017
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 2 Mar 2017 06:19:27 +1300
Subject: [squid-users] Failed to shm_open
In-Reply-To: <1488380797187-4681639.post@n4.nabble.com>
References: <1488380797187-4681639.post@n4.nabble.com>
Message-ID: <97f1176a-f88d-9fbf-28dc-a8c2341dc612@treenet.co.nz>

On 2/03/2017 4:06 a.m., erdosain9 wrote:
> Hi.
> Now squid stop... abnormaly.
> 
> 2017/03/01 12:04:31 kid1| helperOpenServers: Starting 5/32 'ssl_crtd'
> processes
> FATAL: Ipc::Mem::Segment::open failed to
> shm_open(/squid-ssl_session_cache.shm): (2) No such file or directory
> 
> Squid Cache (Version 3.5.20): Terminated abnormally.
> CPU Usage: 0.095 seconds = 0.074 user + 0.021 sys
> Maximum Resident Size: 134144 KB
> Page faults with physical i/o: 0
> 2017/03/01 12:04:31| Set Current Directory to /var/spool/squid
> 
> What is happend??
> 

One of three things, in order of likelihood:

a) your OS does not have /dev/shm running.

b) your Squid was not started with appropriate privileges to access
/dev/shm and create the shared-memory area. ie root.

c) a previous Squid process that was supposed to create that
shared-memory area is not running.


Amos



From squid3 at treenet.co.nz  Wed Mar  1 17:24:35 2017
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 2 Mar 2017 06:24:35 +1300
Subject: [squid-users] Failed to shm_open
In-Reply-To: <1488381674522-4681640.post@n4.nabble.com>
References: <1488380797187-4681639.post@n4.nabble.com>
 <1488381674522-4681640.post@n4.nabble.com>
Message-ID: <3245dc48-4a84-ffe4-5952-ee09921efd8f@treenet.co.nz>

On 2/03/2017 4:21 a.m., erdosain9 wrote:
> no shared cipher

Exactly what it says. There are no ciphers which both the client  and
the server are allowing to be used.

One example of this is a client that only speaks SSLv2 and a server that
speaks only TLS/1.3.

You will have to dig a bit deeper to figure out what ciphers are needed.
Unfortunately Squid does not have much useful debug information in this
area yet.

Amos



From adrian.m.miller at gmail.com  Thu Mar  2 01:57:30 2017
From: adrian.m.miller at gmail.com (stylemessiah)
Date: Wed, 1 Mar 2017 17:57:30 -0800 (PST)
Subject: [squid-users] SSL Bump and Certificate issue - RapidSSL
	Intermediate Cert
In-Reply-To: <899f1712-9b39-e052-a075-80609be34275@treenet.co.nz>
References: <1488297480314-4681635.post@n4.nabble.com>
 <899f1712-9b39-e052-a075-80609be34275@treenet.co.nz>
Message-ID: <CAOLOQx3n5MSOZHTiKSZJ8BfA=Q=LNd7KCVsncgLz2QZt0XaEOQ@mail.gmail.com>

>That command you used does not send data through the proxy. So that
>confirms that the servers TLS is broken in a way unrelated to Squid.

As that may be, when i go direct (sans proxy) i get thumbnails...no issues
Toggle the proxy back on and no thumbnails, and opening an image link gives
the
error initially reported.

(71) Protocol error (TLS code:
 X509_V_ERR_UNABLE_TO_GET_ISSUER_CERT_LOCALLY)

SSL Certficate error: certificate issuer (CA) not known:
/C=US/O=GeoTrust Inc./CN=RapidSSL SHA256 CA

So both Ie and FF will just load anything from dl.xda-developers.com and not
register an issue, but squid will refuse to load the content and generate
the error

>You need to locate the root CA and/or intermediate CA certificates used
>to sign the domain servers certificate.

>You then need to identify *why* they are not being trusted by your OS
>library.

>Be sure to determine whether the CA which is missing is actually
>trustworthy before adding it to your trusted set. More than a few of the
>CA which are around are not trusted because they have been hacked or
>caught signing forged certificates they should not have.

I aalways learn something when youre silly enough to reply :)

When i ran dl.xda-developers.com through ssllabs (thanks google), it gave
me a less than glowing report, including
an incomplete cert chain (i say that like i understand it :) ) or as it put
it:

This server is vulnerable to the OpenSSL CCS vulnerability (CVE-2014-0224)
<https://community.qualys.com/blogs/securitylabs/2014/06/13/ssl-pulse-49-vulnerable-to-cve-2014-0224-14-exploitable>
and exploitable. Grade set to F.
This server is vulnerable to the OpenSSL Padding Oracle vulnerability
(CVE-2016-2107)
<https://blog.cloudflare.com/yet-another-padding-oracle-in-openssl-cbc-ciphersuites/>
and insecure. Grade set to F.
This server accepts RC4 cipher, but only with older browsers. Grade capped
to B.  MORE INFO ?
<https://community.qualys.com/blogs/securitylabs/2013/03/19/rc4-in-tls-is-broken-now-what>
This server's certificate chain is incomplete. Grade capped to B.

Full report here for the curious:
https://globalsign.ssllabs.com/analyze.html?d=dl.xda-developers.com&hideResults=on

For a few thumbnails im not going to torture myself, maybe ill send the
forum admin a note instead :)

>PS.  EECDH will not work unless you configure a curve name in the
>tls-dh= option. Just having dhparam.pem alone will only enable the less
>secure DH ciphers.

I did add a curve to the tls-dh param, im guessing tis correct, little info
on which one to use (grabbing the list from my local openssl had me going
what the hell)

tls-dh=prime256v1:/cygdrive/e/Squid/etc/ssl/dhparam.pem

Note: this made no difference whatsoever with my issue

Cheers,

Adrian Miller



On 2 March 2017 at 04:08, Adrian Miller <adrian.m.miller at gmail.com> wrote:

> Thanks Amos for the info, appreciate your tireless assistance for us
> numpties :)
>
> On 2 Mar. 2017 4:06 am, "Amos Jeffries [via Squid Web Proxy Cache]" <
> ml-node+s1019090n4681642h47 at n4.nabble.com> wrote:
>
>> On 1/03/2017 4:58 a.m., stylemessiah wrote:
>>
>> > This is driving me nuts, its the only issue ive found running ssl bump
>> on my
>> > home network for eons
>> >
>> > I cant see image thumbnails on xda-developers...
>> >
>> > When i access a thread with them, i get text links, not thumbnails, and
>> if i
>> > click on the links i get the following:
>> >
>> >
>> >     (71) Protocol error (TLS code:
>> > X509_V_ERR_UNABLE_TO_GET_ISSUER_CERT_LOCALLY)
>> >
>> >     SSL Certficate error: certificate issuer (CA) not known:
>> > /C=US/O=GeoTrust Inc./CN=RapidSSL SHA256 CA
>> >
>> > I figured out by googling how to (i hope) trace the problem certificate
>> via
>> > s_client:
>> >
>> >
>> > OpenSSL> s_client -showcerts -verify 32 -connect
>> dl.xda-developers.com:443
>> > verify depth is 32
>> > CONNECTED(0000012C)
>> > depth=0 CN = *.xda-developers.com
>> > verify error:num=20:unable to get local issuer certificate
>> > verify return:1
>> > depth=0 CN = *.xda-developers.com
>> > verify error:num=21:unable to verify the first certificate
>> > verify return:1
>>
>> That command you used does not send data through the proxy. So that
>> confirms that the servers TLS is broken in a way unrelated to Squid.
>>
>>
>>
>> > ---
>> > Certificate chain
>> >  0 s:/CN=*.xda-developers.com
>> >    i:/C=US/O=GeoTrust Inc./CN=RapidSSL SHA256 CA
>> ...
>>
>> > ---
>> > Server certificate
>> > subject=/CN=*.xda-developers.com
>> > issuer=/C=US/O=GeoTrust Inc./CN=RapidSSL SHA256 CA
>> > ---
>> > No client certificate CA names sent
>> > Peer signing digest: SHA512
>> > Server Temp Key: ECDH, P-256, 256 bits
>> > ---
>> > SSL handshake has read 2067 bytes and written 302 bytes
>> > Verification error: unable to verify the first certificate
>>
>> >
>> > Ive found the intermediate bundle from RapidSS, and added it to my
>> existing
>> > pem bundle...no change
>>
>> You need to locate the root CA and/or intermediate CA certificates used
>> to sign the domain servers certificate.
>>
>> You then need to identify *why* they are not being trusted by your OS
>> library.
>>
>> Be sure to determine whether the CA which is missing is actually
>> trustworthy before adding it to your trusted set. More than a few of the
>> CA which are around are not trusted because they have been hacked or
>> caught signing forged certificates they should not have.
>>
>>
>> > Added as a separate pem i.e. sslproxy_foreign_intermediate_certs
>> > /cygdrive/e/Squid/etc/ssl/extra-intermediate-CA.pem...no change
>> >
>> > My sslbump related config lines are:
>> >
>> > http_port 127.0.0.1:3128 ssl-bump generate-host-certificates=on
>> > dynamic_cert_mem_cache_size=10MB cert=/cygdrive/e/Squid/etc/ssl/myCA.pem
>>
>> > capath=/cygdrive/e/Squid/etc/ssl
>> > cafile=/cygdrive/e/Squid/etc/ssl/extra-intermediate-CA.pem
>> > tls-dh=/cygdrive/e/Squid/etc/ssl/dhparam.pem
>> > options=NO_SSLv2,NO_SSLv3,SINGLE_ECDH_USE
>>
>> PS.  EECDH will not work unless you configure a curve name in the
>> tls-dh= option. Just having dhparam.pem alone will only enable the less
>> secure DH ciphers.
>>
>> Amos
>>
>> _______________________________________________
>> squid-users mailing list
>> [hidden email] <http:///user/SendEmail.jtp?type=node&node=4681642&i=0>
>> http://lists.squid-cache.org/listinfo/squid-users
>>
>>
>> ------------------------------
>> If you reply to this email, your message will be added to the discussion
>> below:
>> http://squid-web-proxy-cache.1019090.n4.nabble.com/SSL-Bump-
>> and-Certificate-issue-RapidSSL-Intermediate-Cert-tp4681635p4681642.html
>> To unsubscribe from SSL Bump and Certificate issue - RapidSSL
>> Intermediate Cert, click here
>> <http://squid-web-proxy-cache.1019090.n4.nabble.com/template/NamlServlet.jtp?macro=unsubscribe_by_code&node=4681635&code=YWRyaWFuLm0ubWlsbGVyQGdtYWlsLmNvbXw0NjgxNjM1fDE5ODY3MjIyMDI=>
>> .
>> NAML
>> <http://squid-web-proxy-cache.1019090.n4.nabble.com/template/NamlServlet.jtp?macro=macro_viewer&id=instant_html%21nabble%3Aemail.naml&base=nabble.naml.namespaces.BasicNamespace-nabble.view.web.template.NabbleNamespace-nabble.view.web.template.NodeNamespace&breadcrumbs=notify_subscribers%21nabble%3Aemail.naml-instant_emails%21nabble%3Aemail.naml-send_instant_email%21nabble%3Aemail.naml>
>>
>


-- 
I hate to advocate *drugs*, *alcohol*,* violence *or
*insanity* to anyone, *but* they've *always* worked for* me*

- Hunter S. Thompson




--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/SSL-Bump-and-Certificate-issue-RapidSSL-Intermediate-Cert-tp4681635p4681646.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From adrian.m.miller at gmail.com  Thu Mar  2 02:59:08 2017
From: adrian.m.miller at gmail.com (stylemessiah)
Date: Wed, 1 Mar 2017 18:59:08 -0800 (PST)
Subject: [squid-users] SSL Bump and Certificate issue - RapidSSL
	Intermediate Cert
In-Reply-To: <899f1712-9b39-e052-a075-80609be34275@treenet.co.nz>
References: <1488297480314-4681635.post@n4.nabble.com>
 <899f1712-9b39-e052-a075-80609be34275@treenet.co.nz>
Message-ID: <CAOLOQx1-wRQ4RZcTjg6CqAT-mYyaBu-nCaPNkYFg4tW66E=F+w@mail.gmail.com>

Decided to fiddle with it one last time....

If i change my cipher entries from

EECDH+ECDSA+AESGCM:EECDH+aRSA+AESGCM:EECDH+ECDSA+SHA384:EECDH+ECDSA+SHA256:EECDH+aRSA+SHA384:EECDH+aRSA+SHA256:EECDH+aRSA+RC4:EECDH:EDH+aRSA:HIGH:MEDIUM:!RC4:!aNULL:!eNULL:!LOW:!3DES:!MD5:!EXP:!PSK:!SRP:!DSS

to

ECDHE-RSA-AES256-GCM-SHA384:ECDHE-RSA-AES128-GCM-SHA256:DHE-RSA-AES256-GCM-SHA384:DHE-RSA-AES128-GCM-SHA256:ECDHE-RSA-AES256-SHA384:ECDHE-RSA-AES128-SHA256:ECDHE-RSA-AES256-SHA:ECDHE-RSA-AES128-SHA:DHE-RSA-AES256-SHA256:DHE-RSA-AES128-SHA256:DHE-RSA-AES256-SHA:DHE-RSA-AES128-SHA:ECDHE-RSA-DES-CBC3-SHA:EDH-RSA-DES-CBC3-SHA:AES256-GCM-SHA384:AES128-GCM-SHA256:AES256-SHA256:AES128-SHA256:AES256-SHA:AES128-SHA:DES-CBC3-SHA:HIGH:!aNULL:!eNULL:!EXPORT:!DES:!MD5:!PSK:!RC4

I get content from dl.xda-developers.com just fine

But i wont pretend i understand the cipher chain, or whether the change is
a good thing


On 2 March 2017 at 13:01, Adrian Miller <adrian.m.miller at gmail.com> wrote:

> >That command you used does not send data through the proxy. So that
> >confirms that the servers TLS is broken in a way unrelated to Squid.
>
> As that may be, when i go direct (sans proxy) i get thumbnails...no issues
> Toggle the proxy back on and no thumbnails, and opening an image link
> gives the
> error initially reported.
>
> (71) Protocol error (TLS code:
>  X509_V_ERR_UNABLE_TO_GET_ISSUER_CERT_LOCALLY)
>
> SSL Certficate error: certificate issuer (CA) not known:
> /C=US/O=GeoTrust Inc./CN=RapidSSL SHA256 CA
>
> So both Ie and FF will just load anything from dl.xda-developers.com and
> not
> register an issue, but squid will refuse to load the content and generate
> the error
>
> >You need to locate the root CA and/or intermediate CA certificates used
> >to sign the domain servers certificate.
>
> >You then need to identify *why* they are not being trusted by your OS
> >library.
>
> >Be sure to determine whether the CA which is missing is actually
> >trustworthy before adding it to your trusted set. More than a few of the
> >CA which are around are not trusted because they have been hacked or
> >caught signing forged certificates they should not have.
>
> I aalways learn something when youre silly enough to reply :)
>
> When i ran dl.xda-developers.com through ssllabs (thanks google), it gave
> me a less than glowing report, including
> an incomplete cert chain (i say that like i understand it :) ) or as it
> put it:
>
> This server is vulnerable to the OpenSSL CCS vulnerability (CVE-2014-0224)
> <https://community.qualys.com/blogs/securitylabs/2014/06/13/ssl-pulse-49-vulnerable-to-cve-2014-0224-14-exploitable>
> and exploitable. Grade set to F.
> This server is vulnerable to the OpenSSL Padding Oracle vulnerability
> (CVE-2016-2107)
> <https://blog.cloudflare.com/yet-another-padding-oracle-in-openssl-cbc-ciphersuites/>
> and insecure. Grade set to F.
> This server accepts RC4 cipher, but only with older browsers. Grade capped
> to B.  MORE INFO ?
> <https://community.qualys.com/blogs/securitylabs/2013/03/19/rc4-in-tls-is-broken-now-what>
> This server's certificate chain is incomplete. Grade capped to B.
>
> Full report here for the curious: https://globalsign.ssllabs.
> com/analyze.html?d=dl.xda-developers.com&hideResults=on
>
> For a few thumbnails im not going to torture myself, maybe ill send the
> forum admin a note instead :)
>
> >PS.  EECDH will not work unless you configure a curve name in the
> >tls-dh= option. Just having dhparam.pem alone will only enable the less
> >secure DH ciphers.
>
> I did add a curve to the tls-dh param, im guessing tis correct, little
> info on which one to use (grabbing the list from my local openssl had me
> going what the hell)
>
> tls-dh=prime256v1:/cygdrive/e/Squid/etc/ssl/dhparam.pem
>
> Note: this made no difference whatsoever with my issue
>
> Cheers,
>
> Adrian Miller
>
>
>
> On 2 March 2017 at 04:08, Adrian Miller <adrian.m.miller at gmail.com> wrote:
>
>> Thanks Amos for the info, appreciate your tireless assistance for us
>> numpties :)
>>
>> On 2 Mar. 2017 4:06 am, "Amos Jeffries [via Squid Web Proxy Cache]" <
>> ml-node+s1019090n4681642h47 at n4.nabble.com> wrote:
>>
>>> On 1/03/2017 4:58 a.m., stylemessiah wrote:
>>>
>>> > This is driving me nuts, its the only issue ive found running ssl bump
>>> on my
>>> > home network for eons
>>> >
>>> > I cant see image thumbnails on xda-developers...
>>> >
>>> > When i access a thread with them, i get text links, not thumbnails,
>>> and if i
>>> > click on the links i get the following:
>>> >
>>> >
>>> >     (71) Protocol error (TLS code:
>>> > X509_V_ERR_UNABLE_TO_GET_ISSUER_CERT_LOCALLY)
>>> >
>>> >     SSL Certficate error: certificate issuer (CA) not known:
>>> > /C=US/O=GeoTrust Inc./CN=RapidSSL SHA256 CA
>>> >
>>> > I figured out by googling how to (i hope) trace the problem
>>> certificate via
>>> > s_client:
>>> >
>>> >
>>> > OpenSSL> s_client -showcerts -verify 32 -connect
>>> dl.xda-developers.com:443
>>> > verify depth is 32
>>> > CONNECTED(0000012C)
>>> > depth=0 CN = *.xda-developers.com
>>> > verify error:num=20:unable to get local issuer certificate
>>> > verify return:1
>>> > depth=0 CN = *.xda-developers.com
>>> > verify error:num=21:unable to verify the first certificate
>>> > verify return:1
>>>
>>> That command you used does not send data through the proxy. So that
>>> confirms that the servers TLS is broken in a way unrelated to Squid.
>>>
>>>
>>>
>>> > ---
>>> > Certificate chain
>>> >  0 s:/CN=*.xda-developers.com
>>> >    i:/C=US/O=GeoTrust Inc./CN=RapidSSL SHA256 CA
>>> ...
>>>
>>> > ---
>>> > Server certificate
>>> > subject=/CN=*.xda-developers.com
>>> > issuer=/C=US/O=GeoTrust Inc./CN=RapidSSL SHA256 CA
>>> > ---
>>> > No client certificate CA names sent
>>> > Peer signing digest: SHA512
>>> > Server Temp Key: ECDH, P-256, 256 bits
>>> > ---
>>> > SSL handshake has read 2067 bytes and written 302 bytes
>>> > Verification error: unable to verify the first certificate
>>>
>>> >
>>> > Ive found the intermediate bundle from RapidSS, and added it to my
>>> existing
>>> > pem bundle...no change
>>>
>>> You need to locate the root CA and/or intermediate CA certificates used
>>> to sign the domain servers certificate.
>>>
>>> You then need to identify *why* they are not being trusted by your OS
>>> library.
>>>
>>> Be sure to determine whether the CA which is missing is actually
>>> trustworthy before adding it to your trusted set. More than a few of the
>>> CA which are around are not trusted because they have been hacked or
>>> caught signing forged certificates they should not have.
>>>
>>>
>>> > Added as a separate pem i.e. sslproxy_foreign_intermediate_certs
>>> > /cygdrive/e/Squid/etc/ssl/extra-intermediate-CA.pem...no change
>>> >
>>> > My sslbump related config lines are:
>>> >
>>> > http_port 127.0.0.1:3128 ssl-bump generate-host-certificates=on
>>> > dynamic_cert_mem_cache_size=10MB cert=/cygdrive/e/Squid/etc/ssl/myCA.pem
>>>
>>> > capath=/cygdrive/e/Squid/etc/ssl
>>> > cafile=/cygdrive/e/Squid/etc/ssl/extra-intermediate-CA.pem
>>> > tls-dh=/cygdrive/e/Squid/etc/ssl/dhparam.pem
>>> > options=NO_SSLv2,NO_SSLv3,SINGLE_ECDH_USE
>>>
>>> PS.  EECDH will not work unless you configure a curve name in the
>>> tls-dh= option. Just having dhparam.pem alone will only enable the less
>>> secure DH ciphers.
>>>
>>> Amos
>>>
>>> _______________________________________________
>>> squid-users mailing list
>>> [hidden email] <http:///user/SendEmail.jtp?type=node&node=4681642&i=0>
>>> http://lists.squid-cache.org/listinfo/squid-users
>>>
>>>
>>> ------------------------------
>>> If you reply to this email, your message will be added to the discussion
>>> below:
>>> http://squid-web-proxy-cache.1019090.n4.nabble.com/SSL-Bump-
>>> and-Certificate-issue-RapidSSL-Intermediate-Cert-tp4681635p4681642.html
>>> To unsubscribe from SSL Bump and Certificate issue - RapidSSL
>>> Intermediate Cert, click here
>>> <http://squid-web-proxy-cache.1019090.n4.nabble.com/template/NamlServlet.jtp?macro=unsubscribe_by_code&node=4681635&code=YWRyaWFuLm0ubWlsbGVyQGdtYWlsLmNvbXw0NjgxNjM1fDE5ODY3MjIyMDI=>
>>> .
>>> NAML
>>> <http://squid-web-proxy-cache.1019090.n4.nabble.com/template/NamlServlet.jtp?macro=macro_viewer&id=instant_html%21nabble%3Aemail.naml&base=nabble.naml.namespaces.BasicNamespace-nabble.view.web.template.NabbleNamespace-nabble.view.web.template.NodeNamespace&breadcrumbs=notify_subscribers%21nabble%3Aemail.naml-instant_emails%21nabble%3Aemail.naml-send_instant_email%21nabble%3Aemail.naml>
>>>
>>
>
>
> --
> I hate to advocate *drugs*, *alcohol*,* violence *or
> *insanity* to anyone, *but* they've *always* worked for* me*
>
> - Hunter S. Thompson
>



-- 
I hate to advocate *drugs*, *alcohol*,* violence *or
*insanity* to anyone, *but* they've *always* worked for* me*

- Hunter S. Thompson




--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/SSL-Bump-and-Certificate-issue-RapidSSL-Intermediate-Cert-tp4681635p4681647.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From adrian.m.miller at gmail.com  Thu Mar  2 03:14:53 2017
From: adrian.m.miller at gmail.com (Adrian Miller)
Date: Thu, 2 Mar 2017 14:14:53 +1100
Subject: [squid-users] squid-users Digest, Vol 31, Issue 3
In-Reply-To: <mailman.10124.1488423886.20516.squid-users@lists.squid-cache.org>
References: <mailman.10124.1488423886.20516.squid-users@lists.squid-cache.org>
Message-ID: <CAOLOQx0URyG3NMhjaDhq12ahFA==-h1_FT7WLnoMnHBO4XhWvQ@mail.gmail.com>

Disregard last message, it seemed to work...once - quite possible i had the
proxy toggled off at the time...sheesh

Reverted my cipher chain back to the original and leaving the hell alone,
will send the site admin an email instead of fiddling further

On 2 March 2017 at 14:04, <squid-users-request at lists.squid-cache.org> wrote:

> Send squid-users mailing list submissions to
>         squid-users at lists.squid-cache.org
>
> To subscribe or unsubscribe via the World Wide Web, visit
>         http://lists.squid-cache.org/listinfo/squid-users
> or, via email, send a message with subject or body 'help' to
>         squid-users-request at lists.squid-cache.org
>
> You can reach the person managing the list at
>         squid-users-owner at lists.squid-cache.org
>
> When replying, please edit your Subject line so it is more specific
> than "Re: Contents of squid-users digest..."
>
>
> Today's Topics:
>
>    1. Re: SSL Bump and Certificate issue - RapidSSL     Intermediate
>       Cert (stylemessiah)
>    2. Re: Failed to shm_open (Amos Jeffries)
>    3. Re: Failed to shm_open (Amos Jeffries)
>    4. Re: SSL Bump and Certificate issue - RapidSSL     Intermediate
>       Cert (stylemessiah)
>    5. Re: SSL Bump and Certificate issue - RapidSSL     Intermediate
>       Cert (stylemessiah)
>
>
> ----------------------------------------------------------------------
>
> Message: 1
> Date: Wed, 1 Mar 2017 09:03:47 -0800 (PST)
> From: stylemessiah <adrian.m.miller at gmail.com>
> To: squid-users at lists.squid-cache.org
> Subject: Re: [squid-users] SSL Bump and Certificate issue - RapidSSL
>         Intermediate Cert
> Message-ID:
>         <CAOLOQx36wSy24sDDS-Qm=BSAeGsS5oiT5kGK5kP7s=sMQEffpQ@
> mail.gmail.com>
> Content-Type: text/plain; charset=us-ascii
>
> Thanks Amos for the info, appreciate your tireless assistance for us
> numpties :)
>
> On 2 Mar. 2017 4:06 am, "Amos Jeffries [via Squid Web Proxy Cache]" <
> ml-node+s1019090n4681642h47 at n4.nabble.com> wrote:
>
> > On 1/03/2017 4:58 a.m., stylemessiah wrote:
> >
> > > This is driving me nuts, its the only issue ive found running ssl bump
> > on my
> > > home network for eons
> > >
> > > I cant see image thumbnails on xda-developers...
> > >
> > > When i access a thread with them, i get text links, not thumbnails, and
> > if i
> > > click on the links i get the following:
> > >
> > >
> > >     (71) Protocol error (TLS code:
> > > X509_V_ERR_UNABLE_TO_GET_ISSUER_CERT_LOCALLY)
> > >
> > >     SSL Certficate error: certificate issuer (CA) not known:
> > > /C=US/O=GeoTrust Inc./CN=RapidSSL SHA256 CA
> > >
> > > I figured out by googling how to (i hope) trace the problem certificate
> > via
> > > s_client:
> > >
> > >
> > > OpenSSL> s_client -showcerts -verify 32 -connect
> > dl.xda-developers.com:443
> > > verify depth is 32
> > > CONNECTED(0000012C)
> > > depth=0 CN = *.xda-developers.com
> > > verify error:num=20:unable to get local issuer certificate
> > > verify return:1
> > > depth=0 CN = *.xda-developers.com
> > > verify error:num=21:unable to verify the first certificate
> > > verify return:1
> >
> > That command you used does not send data through the proxy. So that
> > confirms that the servers TLS is broken in a way unrelated to Squid.
> >
> >
> >
> > > ---
> > > Certificate chain
> > >  0 s:/CN=*.xda-developers.com
> > >    i:/C=US/O=GeoTrust Inc./CN=RapidSSL SHA256 CA
> > ...
> >
> > > ---
> > > Server certificate
> > > subject=/CN=*.xda-developers.com
> > > issuer=/C=US/O=GeoTrust Inc./CN=RapidSSL SHA256 CA
> > > ---
> > > No client certificate CA names sent
> > > Peer signing digest: SHA512
> > > Server Temp Key: ECDH, P-256, 256 bits
> > > ---
> > > SSL handshake has read 2067 bytes and written 302 bytes
> > > Verification error: unable to verify the first certificate
> >
> > >
> > > Ive found the intermediate bundle from RapidSS, and added it to my
> > existing
> > > pem bundle...no change
> >
> > You need to locate the root CA and/or intermediate CA certificates used
> > to sign the domain servers certificate.
> >
> > You then need to identify *why* they are not being trusted by your OS
> > library.
> >
> > Be sure to determine whether the CA which is missing is actually
> > trustworthy before adding it to your trusted set. More than a few of the
> > CA which are around are not trusted because they have been hacked or
> > caught signing forged certificates they should not have.
> >
> >
> > > Added as a separate pem i.e. sslproxy_foreign_intermediate_certs
> > > /cygdrive/e/Squid/etc/ssl/extra-intermediate-CA.pem...no change
> > >
> > > My sslbump related config lines are:
> > >
> > > http_port 127.0.0.1:3128 ssl-bump generate-host-certificates=on
> > > dynamic_cert_mem_cache_size=10MB cert=/cygdrive/e/Squid/etc/
> ssl/myCA.pem
> >
> > > capath=/cygdrive/e/Squid/etc/ssl
> > > cafile=/cygdrive/e/Squid/etc/ssl/extra-intermediate-CA.pem
> > > tls-dh=/cygdrive/e/Squid/etc/ssl/dhparam.pem
> > > options=NO_SSLv2,NO_SSLv3,SINGLE_ECDH_USE
> >
> > PS.  EECDH will not work unless you configure a curve name in the
> > tls-dh= option. Just having dhparam.pem alone will only enable the less
> > secure DH ciphers.
> >
> > Amos
> >
> > _______________________________________________
> > squid-users mailing list
> > [hidden email] <http:///user/SendEmail.jtp?type=node&node=4681642&i=0>
> > http://lists.squid-cache.org/listinfo/squid-users
> >
> >
> > ------------------------------
> > If you reply to this email, your message will be added to the discussion
> > below:
> > http://squid-web-proxy-cache.1019090.n4.nabble.com/SSL-
> > Bump-and-Certificate-issue-RapidSSL-Intermediate-Cert-
> > tp4681635p4681642.html
> > To unsubscribe from SSL Bump and Certificate issue - RapidSSL
> Intermediate
> > Cert, click here
> > <http://squid-web-proxy-cache.1019090.n4.nabble.com/
> template/NamlServlet.jtp?macro=unsubscribe_by_code&node=4681635&code=
> YWRyaWFuLm0ubWlsbGVyQGdtYWlsLmNvbXw0NjgxNjM1fDE5ODY3MjIyMDI=>
> > .
> > NAML
> > <http://squid-web-proxy-cache.1019090.n4.nabble.com/
> template/NamlServlet.jtp?macro=macro_viewer&id=instant_
> html%21nabble%3Aemail.naml&base=nabble.naml.namespaces.
> BasicNamespace-nabble.view.web.template.NabbleNamespace-
> nabble.view.web.template.NodeNamespace&breadcrumbs=
> notify_subscribers%21nabble%3Aemail.naml-instant_emails%
> 21nabble%3Aemail.naml-send_instant_email%21nabble%3Aemail.naml>
> >
>
>
>
>
> --
> View this message in context: http://squid-web-proxy-cache.
> 1019090.n4.nabble.com/SSL-Bump-and-Certificate-issue-
> RapidSSL-Intermediate-Cert-tp4681635p4681643.html
> Sent from the Squid - Users mailing list archive at Nabble.com.
>
>
> ------------------------------
>
> Message: 2
> Date: Thu, 2 Mar 2017 06:19:27 +1300
> From: Amos Jeffries <squid3 at treenet.co.nz>
> To: squid-users at lists.squid-cache.org
> Subject: Re: [squid-users] Failed to shm_open
> Message-ID: <97f1176a-f88d-9fbf-28dc-a8c2341dc612 at treenet.co.nz>
> Content-Type: text/plain; charset=utf-8
>
> On 2/03/2017 4:06 a.m., erdosain9 wrote:
> > Hi.
> > Now squid stop... abnormaly.
> >
> > 2017/03/01 12:04:31 kid1| helperOpenServers: Starting 5/32 'ssl_crtd'
> > processes
> > FATAL: Ipc::Mem::Segment::open failed to
> > shm_open(/squid-ssl_session_cache.shm): (2) No such file or directory
> >
> > Squid Cache (Version 3.5.20): Terminated abnormally.
> > CPU Usage: 0.095 seconds = 0.074 user + 0.021 sys
> > Maximum Resident Size: 134144 KB
> > Page faults with physical i/o: 0
> > 2017/03/01 12:04:31| Set Current Directory to /var/spool/squid
> >
> > What is happend??
> >
>
> One of three things, in order of likelihood:
>
> a) your OS does not have /dev/shm running.
>
> b) your Squid was not started with appropriate privileges to access
> /dev/shm and create the shared-memory area. ie root.
>
> c) a previous Squid process that was supposed to create that
> shared-memory area is not running.
>
>
> Amos
>
>
>
> ------------------------------
>
> Message: 3
> Date: Thu, 2 Mar 2017 06:24:35 +1300
> From: Amos Jeffries <squid3 at treenet.co.nz>
> To: squid-users at lists.squid-cache.org
> Subject: Re: [squid-users] Failed to shm_open
> Message-ID: <3245dc48-4a84-ffe4-5952-ee09921efd8f at treenet.co.nz>
> Content-Type: text/plain; charset=utf-8
>
> On 2/03/2017 4:21 a.m., erdosain9 wrote:
> > no shared cipher
>
> Exactly what it says. There are no ciphers which both the client  and
> the server are allowing to be used.
>
> One example of this is a client that only speaks SSLv2 and a server that
> speaks only TLS/1.3.
>
> You will have to dig a bit deeper to figure out what ciphers are needed.
> Unfortunately Squid does not have much useful debug information in this
> area yet.
>
> Amos
>
>
>
> ------------------------------
>
> Message: 4
> Date: Wed, 1 Mar 2017 17:57:30 -0800 (PST)
> From: stylemessiah <adrian.m.miller at gmail.com>
> To: squid-users at lists.squid-cache.org
> Subject: Re: [squid-users] SSL Bump and Certificate issue - RapidSSL
>         Intermediate Cert
> Message-ID:
>         <CAOLOQx3n5MSOZHTiKSZJ8BfA=Q=LNd7KCVsncgLz2QZt0XaEOQ at mail.
> gmail.com>
> Content-Type: text/plain; charset=UTF-8
>
> >That command you used does not send data through the proxy. So that
> >confirms that the servers TLS is broken in a way unrelated to Squid.
>
> As that may be, when i go direct (sans proxy) i get thumbnails...no issues
> Toggle the proxy back on and no thumbnails, and opening an image link gives
> the
> error initially reported.
>
> (71) Protocol error (TLS code:
>  X509_V_ERR_UNABLE_TO_GET_ISSUER_CERT_LOCALLY)
>
> SSL Certficate error: certificate issuer (CA) not known:
> /C=US/O=GeoTrust Inc./CN=RapidSSL SHA256 CA
>
> So both Ie and FF will just load anything from dl.xda-developers.com and
> not
> register an issue, but squid will refuse to load the content and generate
> the error
>
> >You need to locate the root CA and/or intermediate CA certificates used
> >to sign the domain servers certificate.
>
> >You then need to identify *why* they are not being trusted by your OS
> >library.
>
> >Be sure to determine whether the CA which is missing is actually
> >trustworthy before adding it to your trusted set. More than a few of the
> >CA which are around are not trusted because they have been hacked or
> >caught signing forged certificates they should not have.
>
> I aalways learn something when youre silly enough to reply :)
>
> When i ran dl.xda-developers.com through ssllabs (thanks google), it gave
> me a less than glowing report, including
> an incomplete cert chain (i say that like i understand it :) ) or as it put
> it:
>
> This server is vulnerable to the OpenSSL CCS vulnerability (CVE-2014-0224)
> <https://community.qualys.com/blogs/securitylabs/2014/06/13/
> ssl-pulse-49-vulnerable-to-cve-2014-0224-14-exploitable>
> and exploitable. Grade set to F.
> This server is vulnerable to the OpenSSL Padding Oracle vulnerability
> (CVE-2016-2107)
> <https://blog.cloudflare.com/yet-another-padding-oracle-in-
> openssl-cbc-ciphersuites/>
> and insecure. Grade set to F.
> This server accepts RC4 cipher, but only with older browsers. Grade capped
> to B.  MORE INFO ?
> <https://community.qualys.com/blogs/securitylabs/2013/03/19/
> rc4-in-tls-is-broken-now-what>
> This server's certificate chain is incomplete. Grade capped to B.
>
> Full report here for the curious:
> https://globalsign.ssllabs.com/analyze.html?d=dl.xda-
> developers.com&hideResults=on
>
> For a few thumbnails im not going to torture myself, maybe ill send the
> forum admin a note instead :)
>
> >PS.  EECDH will not work unless you configure a curve name in the
> >tls-dh= option. Just having dhparam.pem alone will only enable the less
> >secure DH ciphers.
>
> I did add a curve to the tls-dh param, im guessing tis correct, little info
> on which one to use (grabbing the list from my local openssl had me going
> what the hell)
>
> tls-dh=prime256v1:/cygdrive/e/Squid/etc/ssl/dhparam.pem
>
> Note: this made no difference whatsoever with my issue
>
> Cheers,
>
> Adrian Miller
>
>
>
> On 2 March 2017 at 04:08, Adrian Miller <adrian.m.miller at gmail.com> wrote:
>
> > Thanks Amos for the info, appreciate your tireless assistance for us
> > numpties :)
> >
> > On 2 Mar. 2017 4:06 am, "Amos Jeffries [via Squid Web Proxy Cache]" <
> > ml-node+s1019090n4681642h47 at n4.nabble.com> wrote:
> >
> >> On 1/03/2017 4:58 a.m., stylemessiah wrote:
> >>
> >> > This is driving me nuts, its the only issue ive found running ssl bump
> >> on my
> >> > home network for eons
> >> >
> >> > I cant see image thumbnails on xda-developers...
> >> >
> >> > When i access a thread with them, i get text links, not thumbnails,
> and
> >> if i
> >> > click on the links i get the following:
> >> >
> >> >
> >> >     (71) Protocol error (TLS code:
> >> > X509_V_ERR_UNABLE_TO_GET_ISSUER_CERT_LOCALLY)
> >> >
> >> >     SSL Certficate error: certificate issuer (CA) not known:
> >> > /C=US/O=GeoTrust Inc./CN=RapidSSL SHA256 CA
> >> >
> >> > I figured out by googling how to (i hope) trace the problem
> certificate
> >> via
> >> > s_client:
> >> >
> >> >
> >> > OpenSSL> s_client -showcerts -verify 32 -connect
> >> dl.xda-developers.com:443
> >> > verify depth is 32
> >> > CONNECTED(0000012C)
> >> > depth=0 CN = *.xda-developers.com
> >> > verify error:num=20:unable to get local issuer certificate
> >> > verify return:1
> >> > depth=0 CN = *.xda-developers.com
> >> > verify error:num=21:unable to verify the first certificate
> >> > verify return:1
> >>
> >> That command you used does not send data through the proxy. So that
> >> confirms that the servers TLS is broken in a way unrelated to Squid.
> >>
> >>
> >>
> >> > ---
> >> > Certificate chain
> >> >  0 s:/CN=*.xda-developers.com
> >> >    i:/C=US/O=GeoTrust Inc./CN=RapidSSL SHA256 CA
> >> ...
> >>
> >> > ---
> >> > Server certificate
> >> > subject=/CN=*.xda-developers.com
> >> > issuer=/C=US/O=GeoTrust Inc./CN=RapidSSL SHA256 CA
> >> > ---
> >> > No client certificate CA names sent
> >> > Peer signing digest: SHA512
> >> > Server Temp Key: ECDH, P-256, 256 bits
> >> > ---
> >> > SSL handshake has read 2067 bytes and written 302 bytes
> >> > Verification error: unable to verify the first certificate
> >>
> >> >
> >> > Ive found the intermediate bundle from RapidSS, and added it to my
> >> existing
> >> > pem bundle...no change
> >>
> >> You need to locate the root CA and/or intermediate CA certificates used
> >> to sign the domain servers certificate.
> >>
> >> You then need to identify *why* they are not being trusted by your OS
> >> library.
> >>
> >> Be sure to determine whether the CA which is missing is actually
> >> trustworthy before adding it to your trusted set. More than a few of the
> >> CA which are around are not trusted because they have been hacked or
> >> caught signing forged certificates they should not have.
> >>
> >>
> >> > Added as a separate pem i.e. sslproxy_foreign_intermediate_certs
> >> > /cygdrive/e/Squid/etc/ssl/extra-intermediate-CA.pem...no change
> >> >
> >> > My sslbump related config lines are:
> >> >
> >> > http_port 127.0.0.1:3128 ssl-bump generate-host-certificates=on
> >> > dynamic_cert_mem_cache_size=10MB cert=/cygdrive/e/Squid/etc/
> ssl/myCA.pem
> >>
> >> > capath=/cygdrive/e/Squid/etc/ssl
> >> > cafile=/cygdrive/e/Squid/etc/ssl/extra-intermediate-CA.pem
> >> > tls-dh=/cygdrive/e/Squid/etc/ssl/dhparam.pem
> >> > options=NO_SSLv2,NO_SSLv3,SINGLE_ECDH_USE
> >>
> >> PS.  EECDH will not work unless you configure a curve name in the
> >> tls-dh= option. Just having dhparam.pem alone will only enable the less
> >> secure DH ciphers.
> >>
> >> Amos
> >>
> >> _______________________________________________
> >> squid-users mailing list
> >> [hidden email] <http:///user/SendEmail.jtp?type=node&node=4681642&i=0>
> >> http://lists.squid-cache.org/listinfo/squid-users
> >>
> >>
> >> ------------------------------
> >> If you reply to this email, your message will be added to the discussion
> >> below:
> >> http://squid-web-proxy-cache.1019090.n4.nabble.com/SSL-Bump-
> >> and-Certificate-issue-RapidSSL-Intermediate-Cert-tp4681635p4681642.html
> >> To unsubscribe from SSL Bump and Certificate issue - RapidSSL
> >> Intermediate Cert, click here
> >> <http://squid-web-proxy-cache.1019090.n4.nabble.com/
> template/NamlServlet.jtp?macro=unsubscribe_by_code&node=4681635&code=
> YWRyaWFuLm0ubWlsbGVyQGdtYWlsLmNvbXw0NjgxNjM1fDE5ODY3MjIyMDI=>
> >> .
> >> NAML
> >> <http://squid-web-proxy-cache.1019090.n4.nabble.com/
> template/NamlServlet.jtp?macro=macro_viewer&id=instant_
> html%21nabble%3Aemail.naml&base=nabble.naml.namespaces.
> BasicNamespace-nabble.view.web.template.NabbleNamespace-
> nabble.view.web.template.NodeNamespace&breadcrumbs=
> notify_subscribers%21nabble%3Aemail.naml-instant_emails%
> 21nabble%3Aemail.naml-send_instant_email%21nabble%3Aemail.naml>
> >>
> >
>
>
> --
> I hate to advocate *drugs*, *alcohol*,* violence *or
> *insanity* to anyone, *but* they've *always* worked for* me*
>
> - Hunter S. Thompson
>
>
>
>
> --
> View this message in context: http://squid-web-proxy-cache.
> 1019090.n4.nabble.com/SSL-Bump-and-Certificate-issue-
> RapidSSL-Intermediate-Cert-tp4681635p4681646.html
> Sent from the Squid - Users mailing list archive at Nabble.com.
>
>
> ------------------------------
>
> Message: 5
> Date: Wed, 1 Mar 2017 18:59:08 -0800 (PST)
> From: stylemessiah <adrian.m.miller at gmail.com>
> To: squid-users at lists.squid-cache.org
> Subject: Re: [squid-users] SSL Bump and Certificate issue - RapidSSL
>         Intermediate Cert
> Message-ID:
>         <CAOLOQx1-wRQ4RZcTjg6CqAT-mYyaBu-nCaPNkYFg4tW66E=F+w@
> mail.gmail.com>
> Content-Type: text/plain; charset=UTF-8
>
> Decided to fiddle with it one last time....
>
> If i change my cipher entries from
>
> EECDH+ECDSA+AESGCM:EECDH+aRSA+AESGCM:EECDH+ECDSA+SHA384:
> EECDH+ECDSA+SHA256:EECDH+aRSA+SHA384:EECDH+aRSA+SHA256:
> EECDH+aRSA+RC4:EECDH:EDH+aRSA:HIGH:MEDIUM:!RC4:!aNULL:!
> eNULL:!LOW:!3DES:!MD5:!EXP:!PSK:!SRP:!DSS
>
> to
>
> ECDHE-RSA-AES256-GCM-SHA384:ECDHE-RSA-AES128-GCM-SHA256:
> DHE-RSA-AES256-GCM-SHA384:DHE-RSA-AES128-GCM-SHA256:ECDHE-
> RSA-AES256-SHA384:ECDHE-RSA-AES128-SHA256:ECDHE-RSA-
> AES256-SHA:ECDHE-RSA-AES128-SHA:DHE-RSA-AES256-SHA256:DHE-
> RSA-AES128-SHA256:DHE-RSA-AES256-SHA:DHE-RSA-AES128-SHA:
> ECDHE-RSA-DES-CBC3-SHA:EDH-RSA-DES-CBC3-SHA:AES256-GCM-
> SHA384:AES128-GCM-SHA256:AES256-SHA256:AES128-SHA256:
> AES256-SHA:AES128-SHA:DES-CBC3-SHA:HIGH:!aNULL:!eNULL:!
> EXPORT:!DES:!MD5:!PSK:!RC4
>
> I get content from dl.xda-developers.com just fine
>
> But i wont pretend i understand the cipher chain, or whether the change is
> a good thing
>
>
> On 2 March 2017 at 13:01, Adrian Miller <adrian.m.miller at gmail.com> wrote:
>
> > >That command you used does not send data through the proxy. So that
> > >confirms that the servers TLS is broken in a way unrelated to Squid.
> >
> > As that may be, when i go direct (sans proxy) i get thumbnails...no
> issues
> > Toggle the proxy back on and no thumbnails, and opening an image link
> > gives the
> > error initially reported.
> >
> > (71) Protocol error (TLS code:
> >  X509_V_ERR_UNABLE_TO_GET_ISSUER_CERT_LOCALLY)
> >
> > SSL Certficate error: certificate issuer (CA) not known:
> > /C=US/O=GeoTrust Inc./CN=RapidSSL SHA256 CA
> >
> > So both Ie and FF will just load anything from dl.xda-developers.com and
> > not
> > register an issue, but squid will refuse to load the content and generate
> > the error
> >
> > >You need to locate the root CA and/or intermediate CA certificates used
> > >to sign the domain servers certificate.
> >
> > >You then need to identify *why* they are not being trusted by your OS
> > >library.
> >
> > >Be sure to determine whether the CA which is missing is actually
> > >trustworthy before adding it to your trusted set. More than a few of the
> > >CA which are around are not trusted because they have been hacked or
> > >caught signing forged certificates they should not have.
> >
> > I aalways learn something when youre silly enough to reply :)
> >
> > When i ran dl.xda-developers.com through ssllabs (thanks google), it
> gave
> > me a less than glowing report, including
> > an incomplete cert chain (i say that like i understand it :) ) or as it
> > put it:
> >
> > This server is vulnerable to the OpenSSL CCS vulnerability
> (CVE-2014-0224)
> > <https://community.qualys.com/blogs/securitylabs/2014/06/13/
> ssl-pulse-49-vulnerable-to-cve-2014-0224-14-exploitable>
> > and exploitable. Grade set to F.
> > This server is vulnerable to the OpenSSL Padding Oracle vulnerability
> > (CVE-2016-2107)
> > <https://blog.cloudflare.com/yet-another-padding-oracle-in-
> openssl-cbc-ciphersuites/>
> > and insecure. Grade set to F.
> > This server accepts RC4 cipher, but only with older browsers. Grade
> capped
> > to B.  MORE INFO ?
> > <https://community.qualys.com/blogs/securitylabs/2013/03/19/
> rc4-in-tls-is-broken-now-what>
> > This server's certificate chain is incomplete. Grade capped to B.
> >
> > Full report here for the curious: https://globalsign.ssllabs.
> > com/analyze.html?d=dl.xda-developers.com&hideResults=on
> >
> > For a few thumbnails im not going to torture myself, maybe ill send the
> > forum admin a note instead :)
> >
> > >PS.  EECDH will not work unless you configure a curve name in the
> > >tls-dh= option. Just having dhparam.pem alone will only enable the less
> > >secure DH ciphers.
> >
> > I did add a curve to the tls-dh param, im guessing tis correct, little
> > info on which one to use (grabbing the list from my local openssl had me
> > going what the hell)
> >
> > tls-dh=prime256v1:/cygdrive/e/Squid/etc/ssl/dhparam.pem
> >
> > Note: this made no difference whatsoever with my issue
> >
> > Cheers,
> >
> > Adrian Miller
> >
> >
> >
> > On 2 March 2017 at 04:08, Adrian Miller <adrian.m.miller at gmail.com>
> wrote:
> >
> >> Thanks Amos for the info, appreciate your tireless assistance for us
> >> numpties :)
> >>
> >> On 2 Mar. 2017 4:06 am, "Amos Jeffries [via Squid Web Proxy Cache]" <
> >> ml-node+s1019090n4681642h47 at n4.nabble.com> wrote:
> >>
> >>> On 1/03/2017 4:58 a.m., stylemessiah wrote:
> >>>
> >>> > This is driving me nuts, its the only issue ive found running ssl
> bump
> >>> on my
> >>> > home network for eons
> >>> >
> >>> > I cant see image thumbnails on xda-developers...
> >>> >
> >>> > When i access a thread with them, i get text links, not thumbnails,
> >>> and if i
> >>> > click on the links i get the following:
> >>> >
> >>> >
> >>> >     (71) Protocol error (TLS code:
> >>> > X509_V_ERR_UNABLE_TO_GET_ISSUER_CERT_LOCALLY)
> >>> >
> >>> >     SSL Certficate error: certificate issuer (CA) not known:
> >>> > /C=US/O=GeoTrust Inc./CN=RapidSSL SHA256 CA
> >>> >
> >>> > I figured out by googling how to (i hope) trace the problem
> >>> certificate via
> >>> > s_client:
> >>> >
> >>> >
> >>> > OpenSSL> s_client -showcerts -verify 32 -connect
> >>> dl.xda-developers.com:443
> >>> > verify depth is 32
> >>> > CONNECTED(0000012C)
> >>> > depth=0 CN = *.xda-developers.com
> >>> > verify error:num=20:unable to get local issuer certificate
> >>> > verify return:1
> >>> > depth=0 CN = *.xda-developers.com
> >>> > verify error:num=21:unable to verify the first certificate
> >>> > verify return:1
> >>>
> >>> That command you used does not send data through the proxy. So that
> >>> confirms that the servers TLS is broken in a way unrelated to Squid.
> >>>
> >>>
> >>>
> >>> > ---
> >>> > Certificate chain
> >>> >  0 s:/CN=*.xda-developers.com
> >>> >    i:/C=US/O=GeoTrust Inc./CN=RapidSSL SHA256 CA
> >>> ...
> >>>
> >>> > ---
> >>> > Server certificate
> >>> > subject=/CN=*.xda-developers.com
> >>> > issuer=/C=US/O=GeoTrust Inc./CN=RapidSSL SHA256 CA
> >>> > ---
> >>> > No client certificate CA names sent
> >>> > Peer signing digest: SHA512
> >>> > Server Temp Key: ECDH, P-256, 256 bits
> >>> > ---
> >>> > SSL handshake has read 2067 bytes and written 302 bytes
> >>> > Verification error: unable to verify the first certificate
> >>>
> >>> >
> >>> > Ive found the intermediate bundle from RapidSS, and added it to my
> >>> existing
> >>> > pem bundle...no change
> >>>
> >>> You need to locate the root CA and/or intermediate CA certificates used
> >>> to sign the domain servers certificate.
> >>>
> >>> You then need to identify *why* they are not being trusted by your OS
> >>> library.
> >>>
> >>> Be sure to determine whether the CA which is missing is actually
> >>> trustworthy before adding it to your trusted set. More than a few of
> the
> >>> CA which are around are not trusted because they have been hacked or
> >>> caught signing forged certificates they should not have.
> >>>
> >>>
> >>> > Added as a separate pem i.e. sslproxy_foreign_intermediate_certs
> >>> > /cygdrive/e/Squid/etc/ssl/extra-intermediate-CA.pem...no change
> >>> >
> >>> > My sslbump related config lines are:
> >>> >
> >>> > http_port 127.0.0.1:3128 ssl-bump generate-host-certificates=on
> >>> > dynamic_cert_mem_cache_size=10MB cert=/cygdrive/e/Squid/etc/
> ssl/myCA.pem
> >>>
> >>> > capath=/cygdrive/e/Squid/etc/ssl
> >>> > cafile=/cygdrive/e/Squid/etc/ssl/extra-intermediate-CA.pem
> >>> > tls-dh=/cygdrive/e/Squid/etc/ssl/dhparam.pem
> >>> > options=NO_SSLv2,NO_SSLv3,SINGLE_ECDH_USE
> >>>
> >>> PS.  EECDH will not work unless you configure a curve name in the
> >>> tls-dh= option. Just having dhparam.pem alone will only enable the less
> >>> secure DH ciphers.
> >>>
> >>> Amos
> >>>
> >>> _______________________________________________
> >>> squid-users mailing list
> >>> [hidden email] <http:///user/SendEmail.jtp?type=node&node=4681642&i=0>
> >>> http://lists.squid-cache.org/listinfo/squid-users
> >>>
> >>>
> >>> ------------------------------
> >>> If you reply to this email, your message will be added to the
> discussion
> >>> below:
> >>> http://squid-web-proxy-cache.1019090.n4.nabble.com/SSL-Bump-
> >>> and-Certificate-issue-RapidSSL-Intermediate-Cert-
> tp4681635p4681642.html
> >>> To unsubscribe from SSL Bump and Certificate issue - RapidSSL
> >>> Intermediate Cert, click here
> >>> <http://squid-web-proxy-cache.1019090.n4.nabble.com/
> template/NamlServlet.jtp?macro=unsubscribe_by_code&node=4681635&code=
> YWRyaWFuLm0ubWlsbGVyQGdtYWlsLmNvbXw0NjgxNjM1fDE5ODY3MjIyMDI=>
> >>> .
> >>> NAML
> >>> <http://squid-web-proxy-cache.1019090.n4.nabble.com/
> template/NamlServlet.jtp?macro=macro_viewer&id=instant_
> html%21nabble%3Aemail.naml&base=nabble.naml.namespaces.
> BasicNamespace-nabble.view.web.template.NabbleNamespace-
> nabble.view.web.template.NodeNamespace&breadcrumbs=
> notify_subscribers%21nabble%3Aemail.naml-instant_emails%
> 21nabble%3Aemail.naml-send_instant_email%21nabble%3Aemail.naml>
> >>>
> >>
> >
> >
> > --
> > I hate to advocate *drugs*, *alcohol*,* violence *or
> > *insanity* to anyone, *but* they've *always* worked for* me*
> >
> > - Hunter S. Thompson
> >
>
>
>
> --
> I hate to advocate *drugs*, *alcohol*,* violence *or
> *insanity* to anyone, *but* they've *always* worked for* me*
>
> - Hunter S. Thompson
>
>
>
>
> --
> View this message in context: http://squid-web-proxy-cache.
> 1019090.n4.nabble.com/SSL-Bump-and-Certificate-issue-
> RapidSSL-Intermediate-Cert-tp4681635p4681647.html
> Sent from the Squid - Users mailing list archive at Nabble.com.
>
>
> ------------------------------
>
> Subject: Digest Footer
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>
>
> ------------------------------
>
> End of squid-users Digest, Vol 31, Issue 3
> ******************************************
>



-- 
I hate to advocate *drugs*, *alcohol*,* violence *or
*insanity* to anyone, *but* they've *always* worked for* me*

- Hunter S. Thompson
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20170302/50651c8c/attachment.htm>

From belle at bazuin.nl  Thu Mar  2 07:00:34 2017
From: belle at bazuin.nl (=?windows-1252?Q?L.P.H._van_Belle?=)
Date: Thu, 2 Mar 2017 08:00:34 +0100
Subject: [squid-users] Build errors with Squid 3.5.24 under Debian
In-Reply-To: <e4287dd4-ceb1-3d58-c80e-6e7b315db838@treenet.co.nz>
References: <vmime.58b6ddb9.100c.6f5bfa0f7f7deec9@ms249-lin-003.rotterdam.bazuin.nl>
Message-ID: <vmime.58b7c312.d41.4b78697e4e55cf32@ms249-lin-003.rotterdam.bazuin.nl>

Hai Amos, 

Thank you for that info, i didnt know that.
Will this be fixed for 3.5 or is squid4 going to enter debian? 
I know i'll stay a bit longer on Debian Jessie to avoid this. 


Greetz, 

Louis

> -----Oorspronkelijk bericht-----
> Van: squid-users [mailto:squid-users-bounces at lists.squid-cache.org] Namens
> Amos Jeffries
> Verzonden: woensdag 1 maart 2017 17:31
> Aan: squid-users at lists.squid-cache.org
> Onderwerp: Re: [squid-users] Build errors with Squid 3.5.24 under Debian
> 
> On 2/03/2017 3:42 a.m., L.P.H. van Belle wrote:
> > Hai,
> >
> >> I am trying to build Squid 3.5.24 release under a Debian Testing
> > Debian testing already has 3.5.23 so this should be very easy...
> >
> 
> Debian Testing/sretch and sid contain OpenSSL v1.1. BUt Squid-3 only
> supports OpenSSL v1.0.
> 
> You need to install libssl1.0-dev there instead of libssl-dev.
> 
> Amos
> 
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users



From squid3 at treenet.co.nz  Thu Mar  2 07:16:57 2017
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 2 Mar 2017 20:16:57 +1300
Subject: [squid-users] Build errors with Squid 3.5.24 under Debian
In-Reply-To: <vmime.58b7c312.d41.4b78697e4e55cf32@ms249-lin-003.rotterdam.bazuin.nl>
References: <vmime.58b6ddb9.100c.6f5bfa0f7f7deec9@ms249-lin-003.rotterdam.bazuin.nl>
 <vmime.58b7c312.d41.4b78697e4e55cf32@ms249-lin-003.rotterdam.bazuin.nl>
Message-ID: <8c6b7c61-0edb-eb73-fb3b-6ee5de1cc5c9@treenet.co.nz>

On 2/03/2017 8:00 p.m., L.P.H. van Belle wrote:
> Hai Amos, 
> 
> Thank you for that info, i didnt know that.
> Will this be fixed for 3.5 or is squid4 going to enter debian? 
> I know i'll stay a bit longer on Debian Jessie to avoid this. 

This is just a matter of what *-dev package you install for your custom
build requirements. There is nothing in relevant for Debian to do.

Amos



From test1964 at gmail.com  Thu Mar  2 07:33:19 2017
From: test1964 at gmail.com (Test1964)
Date: Thu, 2 Mar 2017 09:33:19 +0200
Subject: [squid-users] Exclude users ip from white listed Ssl bump sites
Message-ID: <4b881f25-d01e-0ca1-0d09-9105238d76be@gmail.com>

Hi,

I  exclude some sites like Banks (or even gmail.com) from ssl bump like 
this:

acl exclude_sites ssl::server_name .banks.com
ssl_bump peek step1 all
ssl_bump splice exclude_sites
ssl_bump stare step2 all
ssl_bump all

and this works for all users.
How can I exclude some users based on users ip, so when they use 
banks.com they still using ssl bump?


Thanks Dan


From belle at bazuin.nl  Thu Mar  2 07:38:57 2017
From: belle at bazuin.nl (=?windows-1252?Q?L.P.H._van_Belle?=)
Date: Thu, 2 Mar 2017 08:38:57 +0100
Subject: [squid-users] Build errors with Squid 3.5.24 under Debian
In-Reply-To: <8c6b7c61-0edb-eb73-fb3b-6ee5de1cc5c9@treenet.co.nz>
References: <vmime.58b7c312.d41.4b78697e4e55cf32@ms249-lin-003.rotterdam.bazuin.nl>
Message-ID: <vmime.58b7cc11.1fa4.28626e77564246a1@ms249-lin-003.rotterdam.bazuin.nl>

Hai Amos, 

Now im bit confused. ( sorry english is not my native language ) 

In my situation. 
1) i (normaly) only use debian packages. 
2) if i build newer that supplied by debian, like squid, 
i use the debian packages as base for the setup then i build a debian package and install that. 

Now with debian stretch i see libssl1.0-dev and libssl1.1 in the tree.
I can still use libssl1.0-dev to build squid with ssl support? 
Even when debian stretch installs openssl 1.1.0? 

And thank you for clarifing in advanced. 
And now i understand at least why i had problems building on stretch. :-) 

Greetz, 

Louis

> -----Oorspronkelijk bericht-----
> Van: squid-users [mailto:squid-users-bounces at lists.squid-cache.org] Namens
> Amos Jeffries
> Verzonden: donderdag 2 maart 2017 8:17
> Aan: squid-users at lists.squid-cache.org
> Onderwerp: Re: [squid-users] Build errors with Squid 3.5.24 under Debian
> 
> On 2/03/2017 8:00 p.m., L.P.H. van Belle wrote:
> > Hai Amos,
> >
> > Thank you for that info, i didnt know that.
> > Will this be fixed for 3.5 or is squid4 going to enter debian?
> > I know i'll stay a bit longer on Debian Jessie to avoid this.
> 
> This is just a matter of what *-dev package you install for your custom
> build requirements. There is nothing in relevant for Debian to do.
> 
> Amos
> 
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users



From squid3 at treenet.co.nz  Thu Mar  2 08:03:48 2017
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 2 Mar 2017 21:03:48 +1300
Subject: [squid-users] Exclude users ip from white listed Ssl bump sites
In-Reply-To: <4b881f25-d01e-0ca1-0d09-9105238d76be@gmail.com>
References: <4b881f25-d01e-0ca1-0d09-9105238d76be@gmail.com>
Message-ID: <7f816980-bf0f-c65e-817c-7e4d1ee14b30@treenet.co.nz>

On 2/03/2017 8:33 p.m., Test1964 wrote:
> Hi,
> 
> I  exclude some sites like Banks (or even gmail.com) from ssl bump like
> this:
> 
> acl exclude_sites ssl::server_name .banks.com
> ssl_bump peek step1 all
> ssl_bump splice exclude_sites
> ssl_bump stare step2 all
> ssl_bump all
> 
> and this works for all users.
> How can I exclude some users based on users ip, so when they use
> banks.com they still using ssl bump?
> 

First you learn that there is no "user". Then you define what "user ip"
means in a time and place without user's.

Hint: client != user.

<http://wiki.squid-cache.org/SquidFaq/SquidAcl#The_Basics:_How_the_parts_fit_together>

Amos



From squid3 at treenet.co.nz  Thu Mar  2 08:14:18 2017
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 2 Mar 2017 21:14:18 +1300
Subject: [squid-users] Build errors with Squid 3.5.24 under Debian
In-Reply-To: <vmime.58b7cc11.1fa4.28626e77564246a1@ms249-lin-003.rotterdam.bazuin.nl>
References: <vmime.58b7c312.d41.4b78697e4e55cf32@ms249-lin-003.rotterdam.bazuin.nl>
 <vmime.58b7cc11.1fa4.28626e77564246a1@ms249-lin-003.rotterdam.bazuin.nl>
Message-ID: <5936739f-691e-e253-7cb0-89a8baa3ce65@treenet.co.nz>

On 2/03/2017 8:38 p.m., L.P.H. van Belle wrote:
> Hai Amos, 
> 
> Now im bit confused. ( sorry english is not my native language ) 
> 
> In my situation. 
> 1) i (normaly) only use debian packages. 
> 2) if i build newer that supplied by debian, like squid, 
> i use the debian packages as base for the setup then i build a debian package and install that. 
> 
> Now with debian stretch i see libssl1.0-dev and libssl1.1 in the tree.
> I can still use libssl1.0-dev to build squid with ssl support? 

Yes. Exactly so.

On stretch Squid-3 builds with libssl1.0-dev and Squid-4 builds with
libssl-dev.


> Even when debian stretch installs openssl 1.1.0? 

Only the *-dev package matters when building. The run-time stuff is
handled automatically by the packaging system and apt.

Due to other packages you will probably end up with both the 1.0 and 1.1
runtime libraries installed anyway, but that does not matter.

> 
> And thank you for clarifing in advanced. 
> And now i understand at least why i had problems building on stretch. :-) 
> 

Welcome.

Amos



From ahmed.zaeem at netstream.ps  Thu Mar  2 20:47:19 2017
From: ahmed.zaeem at netstream.ps (--Ahmad--)
Date: Thu, 2 Mar 2017 22:47:19 +0200
Subject: [squid-users] squid 3.5.2==> HTTPS FATAL: The ssl_crtd helpers are
	crashing too rapidly, need help!
Message-ID: <592CB65D-536B-40F8-B887-12127542DCB5@netstream.ps>

hey folks .
i have a problem with squid it get crashed after i enabled https !
cache log error => FATAL: The ssl_crtd helpers are crashing too rapidly, need help!

i googled many topics and relevant pages and couldnt find a clear solution .

the quick solution i made was i  removed the certs in file :
rm -rfv /var/lib/ssl_db/

then reinitiated the DB using cmd below :
/lib/squid/ssl_crtd -c -s /var/lib/ssl_db
chown -R squid.squid /var/lib/ssl_db
chown -R squid.squid /var/lib/ssl_db

the restarted squid .

but this is not a solution becuase squid get crashed again after certain time and i don?t know why !
my version is 3.5.2

here is squid.conf :
 /etc/squid/squid.conf
visible_hostname pcloud
acl ip1 myip 10.1.0.1
acl ip2 myip 192.168.10.210
tcp_outgoing_address 192.168.10.210 ip1
tcp_outgoing_address 192.168.10.210 ip2
#
# Recommended minimum configuration:
#

# Example rule allowing access from your local networks.
# Adapt to list your (internal) IP networks from where browsing
# should be allowed
acl localnet src 10.0.0.0/8     # RFC1918 possible internal network
acl localnet src 172.16.0.0/12  # RFC1918 possible internal network
acl localnet src 192.168.0.0/16 # RFC1918 possible internal network
acl localnet src fc00::/7       # RFC 4193 local private network range
acl localnet src fe80::/10      # RFC 4291 link-local (directly plugged) machines

acl SSL_ports port 443
acl Safe_ports port 80          # http
acl Safe_ports port 21          # ftp
acl Safe_ports port 443         # https
acl Safe_ports port 70          # gopher
acl Safe_ports port 210         # wais
acl Safe_ports port 1025-65535  # unregistered ports
acl Safe_ports port 280         # http-mgmt
acl Safe_ports port 488         # gss-http
acl Safe_ports port 591         # filemaker
acl Safe_ports port 777         # multiling http
acl CONNECT method CONNECT

#
# Recommended minimum Access Permission configuration:
#
# Deny requests to certain unsafe ports
http_access deny !Safe_ports

# Deny CONNECT to other than secure SSL ports
http_access deny CONNECT !SSL_ports
http_access allow  CONNECT 
# Only allow cachemgr access from localhost
http_access allow localhost manager
http_access deny manager

# We strongly recommend the following be uncommented to protect innocent
# web applications running on the proxy server who think the only
# one who can access services on "localhost" is a local user
#http_access deny to_localhost

#
# INSERT YOUR OWN RULE(S) HERE TO ALLOW ACCESS FROM YOUR CLIENTS
#

# Example rule allowing access from your local networks.
# Adapt localnet in the ACL section to list your (internal) IP networks
# from where browsing should be allowed
http_access allow localnet
http_access allow localhost

# And finally deny all other access to this proxy
http_access deny all

# Squid normally listens to port 3128
http_port 3128

# Uncomment and adjust the following to add a disk cache directory.
#cache_dir ufs /var/cache/squid 100 16 256

# Leave coredumps in the first cache dir
#coredump_dir /var/cache/squid

#
# Add any of your own refresh_pattern entries above these.
#
#

http_port 3126
#http_port 3128
#######################################
#cache_swap_low 90
#cache_swap_high 95
############################
cache_effective_user squid
cache_effective_group squid
memory_replacement_policy lru
cache_replacement_policy heap LFUDA
########################
maximum_object_size 10000 MB
#cache_mem 5000 MB
maximum_object_size_in_memory 10 MB
#########################
logfile_rotate 2
max_filedescriptors 131072
###############################
############
cache_dir aufs /var/cache/squid 600000 64 128
#######################################
https_port 3129 intercept ssl-bump generate-host-certificates=on dynamic_cert_mem_cache_size=4MB cert=/usr/local/squid/ssl_cert/myca.pem key=/usr/local/squid/ssl_cert/myca.pem
ssl_bump server-first all
sslcrtd_program /lib/squid/ssl_crtd -s /var/lib/ssl_db -M 4MB
sslcrtd_children 1000 startup=1 idle=1
###
minimum_object_size 0 bytes
#refresh patterns for caching static files
refresh_pattern ^ftp: 1440 20% 10080
refresh_pattern ^gopher: 1440 0% 1440
refresh_pattern -i .(gif|png|jpg|jpeg|ico)$ 10080 90% 43200 override-expire ignore-no-cache ignore-no-store ignore-private
refresh_pattern -i .(iso|avi|wav|mp3|mp4|mpeg|swf|flv|x-flv)$ 43200 90% 432000 override-expire ignore-no-cache ignore-no-store ignore-private
refresh_pattern -i .(deb|rpm|exe|zip|tar|tgz|ram|rar|bin|ppt|doc|tiff)$ 10080 90% 43200 override-expire ignore-no-cache ignore-no-store ignore-private
refresh_pattern -i .index.(html|htm)$ 0 40% 10080
refresh_pattern -i .(html|htm|css|js)$ 1440 40% 40320
refresh_pattern . 0 40% 40320








any Joy Guys ?

should i update squid ? or downgrade squid ?

kind regards 

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20170302/07688c6e/attachment.htm>

From yvoinov at gmail.com  Thu Mar  2 21:34:43 2017
From: yvoinov at gmail.com (Yuri Voinov)
Date: Fri, 3 Mar 2017 03:34:43 +0600
Subject: [squid-users] squid 3.5.2==> HTTPS FATAL: The ssl_crtd helpers
 are crashing too rapidly, need help!
In-Reply-To: <592CB65D-536B-40F8-B887-12127542DCB5@netstream.ps>
References: <592CB65D-536B-40F8-B887-12127542DCB5@netstream.ps>
Message-ID: <38a07e38-7347-313a-b27f-08fd7eddfa3f@gmail.com>

This error is usually preceded by another error in cache.log associated
with the certificates.

I will show you the direction. Then go himself.

This software will useful for you to solve:

http://www.crypt.gen.nz/logsurfer/

HTH, Yuri


03.03.2017 2:47, --Ahmad-- ?????:
> hey folks .
> i have a problem with squid it get crashed after i enabled https !
> cache log error => FATAL: The ssl_crtd helpers are crashing too
> rapidly, need help!
>
> i googled many topics and relevant pages and couldnt find a
> clear solution .
>
> the quick solution i made was i  removed the certs in file :
> *rm -rfv /var/lib/ssl_db/*
> *
> *
> *then reinitiated the DB using cmd below :*
> /lib/squid/ssl_crtd -c -s /var/lib/ssl_db
> chown -R squid.squid /var/lib/ssl_db
> chown-R squid.squid /var/lib/ssl_db
>
> the restarted squid .
>
> but this is not a solution becuase squid get crashed again after
> certain time and i don?t know why !
> my version is 3.5.2
>
> here is squid.conf :
>  /etc/squid/squid.conf
> visible_hostname pcloud
> acl ip1 myip 10.1.0.1
> acl ip2 myip 192.168.10.210
> tcp_outgoing_address 192.168.10.210 ip1
> tcp_outgoing_address 192.168.10.210 ip2
> #
> # Recommended minimum configuration:
> #
>
> # Example rule allowing access from your local networks.
> # Adapt to list your (internal) IP networks from where browsing
> # should be allowed
> acl localnet src 10.0.0.0/8     # RFC1918 possible internal network
> acl localnet src 172.16.0.0/12  # RFC1918 possible internal network
> acl localnet src 192.168.0.0/16 # RFC1918 possible internal network
> acl localnet src fc00::/7       # RFC 4193 local private network range
> acl localnet src fe80::/10      # RFC 4291 link-local (directly
> plugged) machines
>
> acl SSL_ports port 443
> acl Safe_ports port 80          # http
> acl Safe_ports port 21          # ftp
> acl Safe_ports port 443         # https
> acl Safe_ports port 70          # gopher
> acl Safe_ports port 210         # wais
> acl Safe_ports port 1025-65535  # unregistered ports
> acl Safe_ports port 280         # http-mgmt
> acl Safe_ports port 488         # gss-http
> acl Safe_ports port 591         # filemaker
> acl Safe_ports port 777         # multiling http
> acl CONNECT method CONNECT
>
> #
> # Recommended minimum Access Permission configuration:
> #
> # Deny requests to certain unsafe ports
> http_access deny !Safe_ports
>
> # Deny CONNECT to other than secure SSL ports
> http_access deny CONNECT !SSL_ports
> http_access allow  CONNECT 
> # Only allow cachemgr access from localhost
> http_access allow localhost manager
> http_access deny manager
>
> # We strongly recommend the following be uncommented to protect innocent
> # web applications running on the proxy server who think the only
> # one who can access services on "localhost" is a local user
> #http_access deny to_localhost
>
> #
> # INSERT YOUR OWN RULE(S) HERE TO ALLOW ACCESS FROM YOUR CLIENTS
> #
>
> # Example rule allowing access from your local networks.
> # Adapt localnet in the ACL section to list your (internal) IP networks
> # from where browsing should be allowed
> http_access allow localnet
> http_access allow localhost
>
> # And finally deny all other access to this proxy
> http_access deny all
>
> # Squid normally listens to port 3128
> http_port 3128
>
> # Uncomment and adjust the following to add a disk cache directory.
> #cache_dir ufs /var/cache/squid 100 16 256
>
> # Leave coredumps in the first cache dir
> #coredump_dir /var/cache/squid
>
> #
> # Add any of your own refresh_pattern entries above these.
> #
> #
>
> http_port 3126
> #http_port 3128
> #######################################
> #cache_swap_low 90
> #cache_swap_high 95
> ############################
> cache_effective_user squid
> cache_effective_group squid
> memory_replacement_policy lru
> cache_replacement_policy heap LFUDA
> ########################
> maximum_object_size 10000 MB
> #cache_mem 5000 MB
> maximum_object_size_in_memory 10 MB
> #########################
> logfile_rotate 2
> max_filedescriptors 131072
> ###############################
> ############
> cache_dir aufs /var/cache/squid 600000 64 128
> #######################################
> https_port 3129 intercept ssl-bump generate-host-certificates=on
> dynamic_cert_mem_cache_size=4MB
> cert=/usr/local/squid/ssl_cert/myca.pem
> key=/usr/local/squid/ssl_cert/myca.pem
> ssl_bump server-first all
> sslcrtd_program /lib/squid/ssl_crtd -s /var/lib/ssl_db -M 4MB
> sslcrtd_children 1000 startup=1 idle=1
> ###
> minimum_object_size 0 bytes
> #refresh patterns for caching static files
> refresh_pattern ^ftp: 1440 20% 10080
> refresh_pattern ^gopher: 1440 0% 1440
> refresh_pattern -i .(gif|png|jpg|jpeg|ico)$ 10080 90% 43200
> override-expire ignore-no-cache ignore-no-store ignore-private
> refresh_pattern -i .(iso|avi|wav|mp3|mp4|mpeg|swf|flv|x-flv)$ 43200
> 90% 432000 override-expire ignore-no-cache ignore-no-store ignore-private
> refresh_pattern -i
> .(deb|rpm|exe|zip|tar|tgz|ram|rar|bin|ppt|doc|tiff)$ 10080 90% 43200
> override-expire ignore-no-cache ignore-no-store ignore-private
> refresh_pattern -i .index.(html|htm)$ 0 40% 10080
> refresh_pattern -i .(html|htm|css|js)$ 1440 40% 40320
> refresh_pattern . 0 40% 40320
>
>
>
>
>
>
>
>
> any Joy Guys ?
>
> should i update squid ? or downgrade squid ?
>
> kind regards 
>
>
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users

-- 
Bugs to the Future
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20170303/d6e0ee84/attachment.htm>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: 0x613DEC46.asc
Type: application/pgp-keys
Size: 2437 bytes
Desc: not available
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20170303/d6e0ee84/attachment.key>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: signature.asc
Type: application/pgp-signature
Size: 473 bytes
Desc: OpenPGP digital signature
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20170303/d6e0ee84/attachment.sig>

From eliezer at ngtech.co.il  Thu Mar  2 21:35:57 2017
From: eliezer at ngtech.co.il (Eliezer  Croitoru)
Date: Thu, 2 Mar 2017 23:35:57 +0200
Subject: [squid-users] squid 3.5.2==> HTTPS FATAL: The ssl_crtd helpers
	are	crashing too rapidly, need help!
In-Reply-To: <592CB65D-536B-40F8-B887-12127542DCB5@netstream.ps>
References: <592CB65D-536B-40F8-B887-12127542DCB5@netstream.ps>
Message-ID: <7ba701d2939c$fa9adcc0$efd09640$@ngtech.co.il>

Really???
sslcrtd_children 1000 startup=1 idle=1

Are you series?
You don't need to google.
And before you delete the directory you need to save it first so you would be able to analyze what happen wrong with it.
There is a reason for these helpers to crash.
If they crash it's because of something...
You need to either find why they crash and to produce the issue.
How is it that my helpers do not crash but yours do crash?

For future testing save the /var/lib/ssl_db/ into a tar and then run offline the helper and see if it's crashing.
Also  on what OS are you running this?

Eliezer 

----
http://ngtech.co.il/lmgtfy/
Linux System Administrator
Mobile: +972-5-28704261
Email: eliezer at ngtech.co.il


From: squid-users [mailto:squid-users-bounces at lists.squid-cache.org] On Behalf Of --Ahmad--
Sent: Thursday, March 2, 2017 10:47 PM
To: Squid Users <squid-users at lists.squid-cache.org>
Subject: [squid-users] squid 3.5.2==> HTTPS FATAL: The ssl_crtd helpers are crashing too rapidly, need help!

hey folks .
i have a problem with squid it get crashed after i enabled https !
cache log error => FATAL: The ssl_crtd helpers are crashing too rapidly, need help!

i googled many topics and relevant pages and couldnt find a clear solution .

the quick solution i made was i  removed the certs in file :
rm -rfv /var/lib/ssl_db/


then reinitiated the DB using cmd below :
/lib/squid/ssl_crtd -c -s /var/lib/ssl_db
chown -R squid.squid /var/lib/ssl_db
chown -R squid.squid /var/lib/ssl_db


the restarted squid .


but this is not a solution becuase squid get crashed again after certain time and i don?t know why !
my version is 3.5.2


here is squid.conf :
 /etc/squid/squid.conf
visible_hostname pcloud
acl ip1 myip 10.1.0.1
acl ip2 myip 192.168.10.210
tcp_outgoing_address 192.168.10.210 ip1
tcp_outgoing_address 192.168.10.210 ip2
#
# Recommended minimum configuration:
#

# Example rule allowing access from your local networks.
# Adapt to list your (internal) IP networks from where browsing
# should be allowed
acl localnet src 10.0.0.0/8     # RFC1918 possible internal network
acl localnet src 172.16.0.0/12  # RFC1918 possible internal network
acl localnet src 192.168.0.0/16 # RFC1918 possible internal network
acl localnet src fc00::/7       # RFC 4193 local private network range
acl localnet src fe80::/10      # RFC 4291 link-local (directly plugged) machines

acl SSL_ports port 443
acl Safe_ports port 80          # http
acl Safe_ports port 21          # ftp
acl Safe_ports port 443         # https
acl Safe_ports port 70          # gopher
acl Safe_ports port 210         # wais
acl Safe_ports port 1025-65535  # unregistered ports
acl Safe_ports port 280         # http-mgmt
acl Safe_ports port 488         # gss-http
acl Safe_ports port 591         # filemaker
acl Safe_ports port 777         # multiling http
acl CONNECT method CONNECT

#
# Recommended minimum Access Permission configuration:
#
# Deny requests to certain unsafe ports
http_access deny !Safe_ports

# Deny CONNECT to other than secure SSL ports
http_access deny CONNECT !SSL_ports
http_access allow  CONNECT 
# Only allow cachemgr access from localhost
http_access allow localhost manager
http_access deny manager

# We strongly recommend the following be uncommented to protect innocent
# web applications running on the proxy server who think the only
# one who can access services on "localhost" is a local user
#http_access deny to_localhost

#
# INSERT YOUR OWN RULE(S) HERE TO ALLOW ACCESS FROM YOUR CLIENTS
#

# Example rule allowing access from your local networks.
# Adapt localnet in the ACL section to list your (internal) IP networks
# from where browsing should be allowed
http_access allow localnet
http_access allow localhost

# And finally deny all other access to this proxy
http_access deny all

# Squid normally listens to port 3128
http_port 3128

# Uncomment and adjust the following to add a disk cache directory.
#cache_dir ufs /var/cache/squid 100 16 256

# Leave coredumps in the first cache dir
#coredump_dir /var/cache/squid

#
# Add any of your own refresh_pattern entries above these.
#
#

http_port 3126
#http_port 3128
#######################################
#cache_swap_low 90
#cache_swap_high 95
############################
cache_effective_user squid
cache_effective_group squid
memory_replacement_policy lru
cache_replacement_policy heap LFUDA
########################
maximum_object_size 10000 MB
#cache_mem 5000 MB
maximum_object_size_in_memory 10 MB
#########################
logfile_rotate 2
max_filedescriptors 131072
###############################
############
cache_dir aufs /var/cache/squid 600000 64 128
#######################################
https_port 3129 intercept ssl-bump generate-host-certificates=on dynamic_cert_mem_cache_size=4MB cert=/usr/local/squid/ssl_cert/myca.pem key=/usr/local/squid/ssl_cert/myca.pem
ssl_bump server-first all
sslcrtd_program /lib/squid/ssl_crtd -s /var/lib/ssl_db -M 4MB
sslcrtd_children 1000 startup=1 idle=1
###
minimum_object_size 0 bytes
#refresh patterns for caching static files
refresh_pattern ^ftp: 1440 20% 10080
refresh_pattern ^gopher: 1440 0% 1440
refresh_pattern -i .(gif|png|jpg|jpeg|ico)$ 10080 90% 43200 override-expire ignore-no-cache ignore-no-store ignore-private
refresh_pattern -i .(iso|avi|wav|mp3|mp4|mpeg|swf|flv|x-flv)$ 43200 90% 432000 override-expire ignore-no-cache ignore-no-store ignore-private
refresh_pattern -i .(deb|rpm|exe|zip|tar|tgz|ram|rar|bin|ppt|doc|tiff)$ 10080 90% 43200 override-expire ignore-no-cache ignore-no-store ignore-private
refresh_pattern -i .index.(html|htm)$ 0 40% 10080
refresh_pattern -i .(html|htm|css|js)$ 1440 40% 40320
refresh_pattern . 0 40% 40320
















any Joy Guys ?


should i update squid ? or downgrade squid ?


kind regards 




From yvoinov at gmail.com  Thu Mar  2 21:40:40 2017
From: yvoinov at gmail.com (Yuri Voinov)
Date: Fri, 3 Mar 2017 03:40:40 +0600
Subject: [squid-users] squid 3.5.2==> HTTPS FATAL: The ssl_crtd helpers
 are crashing too rapidly, need help!
In-Reply-To: <38a07e38-7347-313a-b27f-08fd7eddfa3f@gmail.com>
References: <592CB65D-536B-40F8-B887-12127542DCB5@netstream.ps>
 <38a07e38-7347-313a-b27f-08fd7eddfa3f@gmail.com>
Message-ID: <97215197-78fc-e987-9a9d-51a697010e7b@gmail.com>

One hint finally:

'([^ ]*) helper database ([^ ]*) failed: The SSL certificate database
([^ ]*) is corrupted. Please rebuild' - - - 0    exec
"/usr/local/bin/crtd_create.sh -r >/dev/null 2>&1"
'FATAL: ([^ ]*) helpers are crashing too rapidly, need help!' - - - 0   
exec "/usr/local/bin/crtd_create.sh -r >/dev/null 2>&1"
'Cannot add certificate to db.' - - - 0        exec
"/usr/local/bin/crtd_create.sh -r >/dev/null 2>&1"

PS. This is from logsurfer.conf.


03.03.2017 3:34, Yuri Voinov ?????:
>
> This error is usually preceded by another error in cache.log
> associated with the certificates.
>
> I will show you the direction. Then go himself.
>
> This software will useful for you to solve:
>
> http://www.crypt.gen.nz/logsurfer/
>
> HTH, Yuri
>
>
> 03.03.2017 2:47, --Ahmad-- ?????:
>> hey folks .
>> i have a problem with squid it get crashed after i enabled https !
>> cache log error => FATAL: The ssl_crtd helpers are crashing too
>> rapidly, need help!
>>
>> i googled many topics and relevant pages and couldnt find a
>> clear solution .
>>
>> the quick solution i made was i  removed the certs in file :
>> *rm -rfv /var/lib/ssl_db/*
>> *
>> *
>> *then reinitiated the DB using cmd below :*
>> /lib/squid/ssl_crtd -c -s /var/lib/ssl_db
>> chown -R squid.squid /var/lib/ssl_db
>> chown-R squid.squid /var/lib/ssl_db
>>
>> the restarted squid .
>>
>> but this is not a solution becuase squid get crashed again after
>> certain time and i don?t know why !
>> my version is 3.5.2
>>
>> here is squid.conf :
>>  /etc/squid/squid.conf
>> visible_hostname pcloud
>> acl ip1 myip 10.1.0.1
>> acl ip2 myip 192.168.10.210
>> tcp_outgoing_address 192.168.10.210 ip1
>> tcp_outgoing_address 192.168.10.210 ip2
>> #
>> # Recommended minimum configuration:
>> #
>>
>> # Example rule allowing access from your local networks.
>> # Adapt to list your (internal) IP networks from where browsing
>> # should be allowed
>> acl localnet src 10.0.0.0/8     # RFC1918 possible internal network
>> acl localnet src 172.16.0.0/12  # RFC1918 possible internal network
>> acl localnet src 192.168.0.0/16 # RFC1918 possible internal network
>> acl localnet src fc00::/7       # RFC 4193 local private network range
>> acl localnet src fe80::/10      # RFC 4291 link-local (directly
>> plugged) machines
>>
>> acl SSL_ports port 443
>> acl Safe_ports port 80          # http
>> acl Safe_ports port 21          # ftp
>> acl Safe_ports port 443         # https
>> acl Safe_ports port 70          # gopher
>> acl Safe_ports port 210         # wais
>> acl Safe_ports port 1025-65535  # unregistered ports
>> acl Safe_ports port 280         # http-mgmt
>> acl Safe_ports port 488         # gss-http
>> acl Safe_ports port 591         # filemaker
>> acl Safe_ports port 777         # multiling http
>> acl CONNECT method CONNECT
>>
>> #
>> # Recommended minimum Access Permission configuration:
>> #
>> # Deny requests to certain unsafe ports
>> http_access deny !Safe_ports
>>
>> # Deny CONNECT to other than secure SSL ports
>> http_access deny CONNECT !SSL_ports
>> http_access allow  CONNECT 
>> # Only allow cachemgr access from localhost
>> http_access allow localhost manager
>> http_access deny manager
>>
>> # We strongly recommend the following be uncommented to protect innocent
>> # web applications running on the proxy server who think the only
>> # one who can access services on "localhost" is a local user
>> #http_access deny to_localhost
>>
>> #
>> # INSERT YOUR OWN RULE(S) HERE TO ALLOW ACCESS FROM YOUR CLIENTS
>> #
>>
>> # Example rule allowing access from your local networks.
>> # Adapt localnet in the ACL section to list your (internal) IP networks
>> # from where browsing should be allowed
>> http_access allow localnet
>> http_access allow localhost
>>
>> # And finally deny all other access to this proxy
>> http_access deny all
>>
>> # Squid normally listens to port 3128
>> http_port 3128
>>
>> # Uncomment and adjust the following to add a disk cache directory.
>> #cache_dir ufs /var/cache/squid 100 16 256
>>
>> # Leave coredumps in the first cache dir
>> #coredump_dir /var/cache/squid
>>
>> #
>> # Add any of your own refresh_pattern entries above these.
>> #
>> #
>>
>> http_port 3126
>> #http_port 3128
>> #######################################
>> #cache_swap_low 90
>> #cache_swap_high 95
>> ############################
>> cache_effective_user squid
>> cache_effective_group squid
>> memory_replacement_policy lru
>> cache_replacement_policy heap LFUDA
>> ########################
>> maximum_object_size 10000 MB
>> #cache_mem 5000 MB
>> maximum_object_size_in_memory 10 MB
>> #########################
>> logfile_rotate 2
>> max_filedescriptors 131072
>> ###############################
>> ############
>> cache_dir aufs /var/cache/squid 600000 64 128
>> #######################################
>> https_port 3129 intercept ssl-bump generate-host-certificates=on
>> dynamic_cert_mem_cache_size=4MB
>> cert=/usr/local/squid/ssl_cert/myca.pem
>> key=/usr/local/squid/ssl_cert/myca.pem
>> ssl_bump server-first all
>> sslcrtd_program /lib/squid/ssl_crtd -s /var/lib/ssl_db -M 4MB
>> sslcrtd_children 1000 startup=1 idle=1
>> ###
>> minimum_object_size 0 bytes
>> #refresh patterns for caching static files
>> refresh_pattern ^ftp: 1440 20% 10080
>> refresh_pattern ^gopher: 1440 0% 1440
>> refresh_pattern -i .(gif|png|jpg|jpeg|ico)$ 10080 90% 43200
>> override-expire ignore-no-cache ignore-no-store ignore-private
>> refresh_pattern -i .(iso|avi|wav|mp3|mp4|mpeg|swf|flv|x-flv)$ 43200
>> 90% 432000 override-expire ignore-no-cache ignore-no-store ignore-private
>> refresh_pattern -i
>> .(deb|rpm|exe|zip|tar|tgz|ram|rar|bin|ppt|doc|tiff)$ 10080 90% 43200
>> override-expire ignore-no-cache ignore-no-store ignore-private
>> refresh_pattern -i .index.(html|htm)$ 0 40% 10080
>> refresh_pattern -i .(html|htm|css|js)$ 1440 40% 40320
>> refresh_pattern . 0 40% 40320
>>
>>
>>
>>
>>
>>
>>
>>
>> any Joy Guys ?
>>
>> should i update squid ? or downgrade squid ?
>>
>> kind regards 
>>
>>
>>
>> _______________________________________________
>> squid-users mailing list
>> squid-users at lists.squid-cache.org
>> http://lists.squid-cache.org/listinfo/squid-users
>
> -- 
> Bugs to the Future

-- 
Bugs to the Future
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20170303/cabc3dc2/attachment.htm>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: 0x613DEC46.asc
Type: application/pgp-keys
Size: 2437 bytes
Desc: not available
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20170303/cabc3dc2/attachment.key>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: signature.asc
Type: application/pgp-signature
Size: 473 bytes
Desc: OpenPGP digital signature
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20170303/cabc3dc2/attachment.sig>

From yvoinov at gmail.com  Thu Mar  2 21:46:10 2017
From: yvoinov at gmail.com (Yuri Voinov)
Date: Fri, 3 Mar 2017 03:46:10 +0600
Subject: [squid-users] squid 3.5.2==> HTTPS FATAL: The ssl_crtd helpers
 are crashing too rapidly, need help!
In-Reply-To: <97215197-78fc-e987-9a9d-51a697010e7b@gmail.com>
References: <592CB65D-536B-40F8-B887-12127542DCB5@netstream.ps>
 <38a07e38-7347-313a-b27f-08fd7eddfa3f@gmail.com>
 <97215197-78fc-e987-9a9d-51a697010e7b@gmail.com>
Message-ID: <714528e5-a6d5-e72a-2bc7-9950a8eecb73@gmail.com>

This problem, in principle, is common to all versions of ssl-bumped
Squid from version 3.4 and 5.0, inclusive, and occurs when the stored
certificate is damaged for any reason. The only thing vorkeraund that I
could find - a monitor kesh.log and initialize the certificate database
again with squid restart automatically.

In some installations, this problem does not occur over the years. In
other - almost daily. I have no desire to find out why this is happening
exactly. For me it was easier to make the watchdog, which will follow up
on this.

03.03.2017 3:40, Yuri Voinov ?????:
>
> One hint finally:
>
> '([^ ]*) helper database ([^ ]*) failed: The SSL certificate database
> ([^ ]*) is corrupted. Please rebuild' - - - 0    exec
> "/usr/local/bin/crtd_create.sh -r >/dev/null 2>&1"
> 'FATAL: ([^ ]*) helpers are crashing too rapidly, need help!' - - -
> 0    exec "/usr/local/bin/crtd_create.sh -r >/dev/null 2>&1"
> 'Cannot add certificate to db.' - - - 0        exec
> "/usr/local/bin/crtd_create.sh -r >/dev/null 2>&1"
>
> PS. This is from logsurfer.conf.
>
>
> 03.03.2017 3:34, Yuri Voinov ?????:
>>
>> This error is usually preceded by another error in cache.log
>> associated with the certificates.
>>
>> I will show you the direction. Then go himself.
>>
>> This software will useful for you to solve:
>>
>> http://www.crypt.gen.nz/logsurfer/
>>
>> HTH, Yuri
>>
>>
>> 03.03.2017 2:47, --Ahmad-- ?????:
>>> hey folks .
>>> i have a problem with squid it get crashed after i enabled https !
>>> cache log error => FATAL: The ssl_crtd helpers are crashing too
>>> rapidly, need help!
>>>
>>> i googled many topics and relevant pages and couldnt find a
>>> clear solution .
>>>
>>> the quick solution i made was i  removed the certs in file :
>>> *rm -rfv /var/lib/ssl_db/*
>>> *
>>> *
>>> *then reinitiated the DB using cmd below :*
>>> /lib/squid/ssl_crtd -c -s /var/lib/ssl_db
>>> chown -R squid.squid /var/lib/ssl_db
>>> chown-R squid.squid /var/lib/ssl_db
>>>
>>> the restarted squid .
>>>
>>> but this is not a solution becuase squid get crashed again after
>>> certain time and i don?t know why !
>>> my version is 3.5.2
>>>
>>> here is squid.conf :
>>>  /etc/squid/squid.conf
>>> visible_hostname pcloud
>>> acl ip1 myip 10.1.0.1
>>> acl ip2 myip 192.168.10.210
>>> tcp_outgoing_address 192.168.10.210 ip1
>>> tcp_outgoing_address 192.168.10.210 ip2
>>> #
>>> # Recommended minimum configuration:
>>> #
>>>
>>> # Example rule allowing access from your local networks.
>>> # Adapt to list your (internal) IP networks from where browsing
>>> # should be allowed
>>> acl localnet src 10.0.0.0/8     # RFC1918 possible internal network
>>> acl localnet src 172.16.0.0/12  # RFC1918 possible internal network
>>> acl localnet src 192.168.0.0/16 # RFC1918 possible internal network
>>> acl localnet src fc00::/7       # RFC 4193 local private network range
>>> acl localnet src fe80::/10      # RFC 4291 link-local (directly
>>> plugged) machines
>>>
>>> acl SSL_ports port 443
>>> acl Safe_ports port 80          # http
>>> acl Safe_ports port 21          # ftp
>>> acl Safe_ports port 443         # https
>>> acl Safe_ports port 70          # gopher
>>> acl Safe_ports port 210         # wais
>>> acl Safe_ports port 1025-65535  # unregistered ports
>>> acl Safe_ports port 280         # http-mgmt
>>> acl Safe_ports port 488         # gss-http
>>> acl Safe_ports port 591         # filemaker
>>> acl Safe_ports port 777         # multiling http
>>> acl CONNECT method CONNECT
>>>
>>> #
>>> # Recommended minimum Access Permission configuration:
>>> #
>>> # Deny requests to certain unsafe ports
>>> http_access deny !Safe_ports
>>>
>>> # Deny CONNECT to other than secure SSL ports
>>> http_access deny CONNECT !SSL_ports
>>> http_access allow  CONNECT 
>>> # Only allow cachemgr access from localhost
>>> http_access allow localhost manager
>>> http_access deny manager
>>>
>>> # We strongly recommend the following be uncommented to protect innocent

>>> # web applications running on the proxy server who think the only
>>> # one who can access services on "localhost" is a local user
>>> #http_access deny to_localhost
>>>
>>> #
>>> # INSERT YOUR OWN RULE(S) HERE TO ALLOW ACCESS FROM YOUR CLIENTS
>>> #
>>>
>>> # Example rule allowing access from your local networks.
>>> # Adapt localnet in the ACL section to list your (internal) IP networks
>>> # from where browsing should be allowed
>>> http_access allow localnet
>>> http_access allow localhost
>>>
>>> # And finally deny all other access to this proxy
>>> http_access deny all
>>>
>>> # Squid normally listens to port 3128
>>> http_port 3128
>>>
>>> # Uncomment and adjust the following to add a disk cache directory.
>>> #cache_dir ufs /var/cache/squid 100 16 256
>>>
>>> # Leave coredumps in the first cache dir
>>> #coredump_dir /var/cache/squid
>>>
>>> #
>>> # Add any of your own refresh_pattern entries above these.
>>> #
>>> #
>>>
>>> http_port 3126
>>> #http_port 3128
>>> #######################################
>>> #cache_swap_low 90
>>> #cache_swap_high 95
>>> ############################
>>> cache_effective_user squid
>>> cache_effective_group squid
>>> memory_replacement_policy lru
>>> cache_replacement_policy heap LFUDA
>>> ########################
>>> maximum_object_size 10000 MB
>>> #cache_mem 5000 MB
>>> maximum_object_size_in_memory 10 MB
>>> #########################
>>> logfile_rotate 2
>>> max_filedescriptors 131072
>>> ###############################
>>> ############
>>> cache_dir aufs /var/cache/squid 600000 64 128
>>> #######################################
>>> https_port 3129 intercept ssl-bump generate-host-certificates=on
>>> dynamic_cert_mem_cache_size=4MB
>>> cert=/usr/local/squid/ssl_cert/myca.pem
>>> key=/usr/local/squid/ssl_cert/myca.pem
>>> ssl_bump server-first all
>>> sslcrtd_program /lib/squid/ssl_crtd -s /var/lib/ssl_db -M 4MB
>>> sslcrtd_children 1000 startup=1 idle=1
>>> ###
>>> minimum_object_size 0 bytes
>>> #refresh patterns for caching static files
>>> refresh_pattern ^ftp: 1440 20% 10080
>>> refresh_pattern ^gopher: 1440 0% 1440
>>> refresh_pattern -i .(gif|png|jpg|jpeg|ico)$ 10080 90% 43200
>>> override-expire ignore-no-cache ignore-no-store ignore-private
>>> refresh_pattern -i .(iso|avi|wav|mp3|mp4|mpeg|swf|flv|x-flv)$ 43200
>>> 90% 432000 override-expire ignore-no-cache ignore-no-store
>>> ignore-private
>>> refresh_pattern -i
>>> .(deb|rpm|exe|zip|tar|tgz|ram|rar|bin|ppt|doc|tiff)$ 10080 90% 43200
>>> override-expire ignore-no-cache ignore-no-store ignore-private
>>> refresh_pattern -i .index.(html|htm)$ 0 40% 10080
>>> refresh_pattern -i .(html|htm|css|js)$ 1440 40% 40320
>>> refresh_pattern . 0 40% 40320
>>>
>>>
>>>
>>>
>>>
>>>
>>>
>>>
>>> any Joy Guys ?
>>>
>>> should i update squid ? or downgrade squid ?
>>>
>>> kind regards 
>>>
>>>
>>>
>>> _______________________________________________
>>> squid-users mailing list
>>> squid-users at lists.squid-cache.org
>>> http://lists.squid-cache.org/listinfo/squid-users
>>
>> -- 
>> Bugs to the Future
>
> -- 
> Bugs to the Future

-- 
Bugs to the Future
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20170303/f5499462/attachment.htm>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: 0x613DEC46.asc
Type: application/pgp-keys
Size: 2437 bytes
Desc: not available
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20170303/f5499462/attachment.key>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: signature.asc
Type: application/pgp-signature
Size: 473 bytes
Desc: OpenPGP digital signature
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20170303/f5499462/attachment.sig>

From eliezer at ngtech.co.il  Thu Mar  2 21:47:56 2017
From: eliezer at ngtech.co.il (Eliezer  Croitoru)
Date: Thu, 2 Mar 2017 23:47:56 +0200
Subject: [squid-users] Username not passed to url_rewrite_program
In-Reply-To: <9e42743b-c973-a5ef-515f-e860f5faa9aa@compuniverse.de>
References: <3c520a74-00ef-cb94-0724-c04163fd44c2@compuniverse.de>
 <9e42743b-c973-a5ef-515f-e860f5faa9aa@compuniverse.de>
Message-ID: <7bb501d2939e$a6e99f10$f4bcdd30$@ngtech.co.il>

Hey Amon Ott(which is the first name?),

It's not clear what the issue is to me yet.
The current bug report is:
http://bugs.squid-cache.org/show_bug.cgi?id=4679

>From the cache logs what I see is that squid sends:
"2017/03/02 13:49:27.799 kid1| 61,6| redirect.cc(285) constructHelperQuery: sending 'http://clients1.google.com/ocsp 127.0.0.1/localhost - POST myip=127.0.0.1 myport=3129
' to the redirector helper"
IE without a username.
Is it the case?
( I will write the rest in the bug report)

----
Eliezer Croitoru
Linux System Administrator
Mobile: +972-5-28704261
Email: eliezer at ngtech.co.il


-----Original Message-----
From: squid-users [mailto:squid-users-bounces at lists.squid-cache.org] On Behalf Of Amon Ott
Sent: Tuesday, February 28, 2017 4:10 PM
To: squid-users at lists.squid-cache.org
Subject: Re: [squid-users] Username not passed to url_rewrite_program

Am 23.02.2017 um 11:08 schrieb Amon Ott:
> we want to filter with squidGuard and decide based on the username,
> identified with ident. In the bug tracker I found the old entry 2655
> with this bug, which had been marked as fixed, but is still present in
> 3.5.24. I have recently reopened that bug with some info about our
> szenario. The old bugfix seems to be in the code, but does not solve the
> issue.
> 
> http://bugs.squid-cache.org/show_bug.cgi?id=2655#c11
> 
> Since we cannot continue with our setup, some help would be appreciated.

Bug is also present in 4.0.18, just tested.

Amon Ott.


_______________________________________________
squid-users mailing list
squid-users at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-users



From eliezer at ngtech.co.il  Fri Mar  3 00:32:14 2017
From: eliezer at ngtech.co.il (Eliezer  Croitoru)
Date: Fri, 3 Mar 2017 02:32:14 +0200
Subject: [squid-users] squid 3.5.2==> HTTPS FATAL: The ssl_crtd helpers
	are crashing too rapidly, need help!
In-Reply-To: <714528e5-a6d5-e72a-2bc7-9950a8eecb73@gmail.com>
References: <592CB65D-536B-40F8-B887-12127542DCB5@netstream.ps>
 <38a07e38-7347-313a-b27f-08fd7eddfa3f@gmail.com>
 <97215197-78fc-e987-9a9d-51a697010e7b@gmail.com>
 <714528e5-a6d5-e72a-2bc7-9950a8eecb73@gmail.com>
Message-ID: <7c1d01d293b5$9b22d2c0$d1687840$@ngtech.co.il>

Hey Yuri,

This issue is not 100% squid but I think it's related to the way ssl_crtd works.
I am not sure if it has some locking or other things to prevent such issues.
The first solution is to somehow defend the DB from corruption, like in a case that more then a dozen identical requests are being done towards a single site and two ssl_crtd helpers are trying to do the same things.
I believe that something to fence this should already be inside squid and ssl_crtd but I am pretty sure this is the main issue.

Alex and his team should know the answer for this subject and if I'm not wrong theoretically there are couple ways to prevent the mentioned issues.
I had a plan to try and understand the ssl_crtd code and interface but yet to do so.

I hope this issue will be resolved in a way that it can be backported to 3.5 in the worst case.

Eliezer

----
http://ngtech.co.il/lmgtfy/
Linux System Administrator
Mobile: +972-5-28704261
Email: eliezer at ngtech.co.il


From: squid-users [mailto:squid-users-bounces at lists.squid-cache.org] On Behalf Of Yuri Voinov
Sent: Thursday, March 2, 2017 11:46 PM
To: squid-users at lists.squid-cache.org
Subject: Re: [squid-users] squid 3.5.2==> HTTPS FATAL: The ssl_crtd helpers are crashing too rapidly, need help!

This problem, in principle, is common to all versions of ssl-bumped Squid from version 3.4 and 5.0, inclusive, and occurs when the stored certificate is damaged for any reason. The only thing vorkeraund that I could find - a monitor kesh.log and initialize the certificate database again with squid restart automatically.
In some installations, this problem does not occur over the years. In other - almost daily. I have no desire to find out why this is happening exactly. For me it was easier to make the watchdog, which will follow up on this.
03.03.2017 3:40, Yuri Voinov ?????:
One hint finally:
'([^ ]*) helper database ([^ ]*) failed: The SSL certificate database ([^ ]*) is corrupted. Please rebuild' - - - 0    exec "/usr/local/bin/crtd_create.sh -r >/dev/null 2>&1"
'FATAL: ([^ ]*) helpers are crashing too rapidly, need help!' - - - 0    exec "/usr/local/bin/crtd_create.sh -r >/dev/null 2>&1"
'Cannot add certificate to db.' - - - 0        exec "/usr/local/bin/crtd_create.sh -r >/dev/null 2>&1"
PS. This is from logsurfer.conf.

03.03.2017 3:34, Yuri Voinov ?????:
This error is usually preceded by another error in cache.log associated with the certificates.
I will show you the direction. Then go himself.
This software will useful for you to solve:
http://www.crypt.gen.nz/logsurfer/
HTH, Yuri

03.03.2017 2:47, --Ahmad-- ?????:
hey folks . 
i have a problem with squid it get crashed after i enabled https !
cache log error => FATAL: The ssl_crtd helpers are crashing too rapidly, need help!

i googled many topics and relevant pages and couldnt find a clear solution .

the quick solution i made was i  removed the certs in file :
rm -rfv /var/lib/ssl_db/


then reinitiated the DB using cmd below :
/lib/squid/ssl_crtd -c -s /var/lib/ssl_db
chown -R squid.squid /var/lib/ssl_db
chown -R squid.squid /var/lib/ssl_db


the restarted squid .


but this is not a solution becuase squid get crashed again after certain time and i don?t know why !
my version is 3.5.2


here is squid.conf :
 /etc/squid/squid.conf
visible_hostname pcloud
acl ip1 myip 10.1.0.1
acl ip2 myip 192.168.10.210
tcp_outgoing_address 192.168.10.210 ip1
tcp_outgoing_address 192.168.10.210 ip2
#
# Recommended minimum configuration:
#

# Example rule allowing access from your local networks.
# Adapt to list your (internal) IP networks from where browsing
# should be allowed
acl localnet src 10.0.0.0/8     # RFC1918 possible internal network
acl localnet src 172.16.0.0/12  # RFC1918 possible internal network
acl localnet src 192.168.0.0/16 # RFC1918 possible internal network
acl localnet src fc00::/7       # RFC 4193 local private network range
acl localnet src fe80::/10      # RFC 4291 link-local (directly plugged) machines

acl SSL_ports port 443
acl Safe_ports port 80          # http
acl Safe_ports port 21          # ftp
acl Safe_ports port 443         # https
acl Safe_ports port 70          # gopher
acl Safe_ports port 210         # wais
acl Safe_ports port 1025-65535  # unregistered ports
acl Safe_ports port 280         # http-mgmt
acl Safe_ports port 488         # gss-http
acl Safe_ports port 591         # filemaker
acl Safe_ports port 777         # multiling http
acl CONNECT method CONNECT

#
# Recommended minimum Access Permission configuration:
#
# Deny requests to certain unsafe ports
http_access deny !Safe_ports

# Deny CONNECT to other than secure SSL ports
http_access deny CONNECT !SSL_ports
http_access allow  CONNECT 
# Only allow cachemgr access from localhost
http_access allow localhost manager
http_access deny manager

# We strongly recommend the following be uncommented to protect innocent
# web applications running on the proxy server who think the only
# one who can access services on "localhost" is a local user
#http_access deny to_localhost

#
# INSERT YOUR OWN RULE(S) HERE TO ALLOW ACCESS FROM YOUR CLIENTS
#

# Example rule allowing access from your local networks.
# Adapt localnet in the ACL section to list your (internal) IP networks
# from where browsing should be allowed
http_access allow localnet
http_access allow localhost

# And finally deny all other access to this proxy
http_access deny all

# Squid normally listens to port 3128
http_port 3128

# Uncomment and adjust the following to add a disk cache directory.
#cache_dir ufs /var/cache/squid 100 16 256

# Leave coredumps in the first cache dir
#coredump_dir /var/cache/squid

#
# Add any of your own refresh_pattern entries above these.
#
#

http_port 3126
#http_port 3128
#######################################
#cache_swap_low 90
#cache_swap_high 95
############################
cache_effective_user squid
cache_effective_group squid
memory_replacement_policy lru
cache_replacement_policy heap LFUDA
########################
maximum_object_size 10000 MB
#cache_mem 5000 MB
maximum_object_size_in_memory 10 MB
#########################
logfile_rotate 2
max_filedescriptors 131072
###############################
############
cache_dir aufs /var/cache/squid 600000 64 128
#######################################
https_port 3129 intercept ssl-bump generate-host-certificates=on dynamic_cert_mem_cache_size=4MB cert=/usr/local/squid/ssl_cert/myca.pem key=/usr/local/squid/ssl_cert/myca.pem
ssl_bump server-first all
sslcrtd_program /lib/squid/ssl_crtd -s /var/lib/ssl_db -M 4MB
sslcrtd_children 1000 startup=1 idle=1
###
minimum_object_size 0 bytes
#refresh patterns for caching static files
refresh_pattern ^ftp: 1440 20% 10080
refresh_pattern ^gopher: 1440 0% 1440
refresh_pattern -i .(gif|png|jpg|jpeg|ico)$ 10080 90% 43200 override-expire ignore-no-cache ignore-no-store ignore-private
refresh_pattern -i .(iso|avi|wav|mp3|mp4|mpeg|swf|flv|x-flv)$ 43200 90% 432000 override-expire ignore-no-cache ignore-no-store ignore-private
refresh_pattern -i .(deb|rpm|exe|zip|tar|tgz|ram|rar|bin|ppt|doc|tiff)$ 10080 90% 43200 override-expire ignore-no-cache ignore-no-store ignore-private
refresh_pattern -i .index.(html|htm)$ 0 40% 10080
refresh_pattern -i .(html|htm|css|js)$ 1440 40% 40320
refresh_pattern . 0 40% 40320
















any Joy Guys ?


should i update squid ? or downgrade squid ?


kind regards 




_______________________________________________
squid-users mailing list
mailto:squid-users at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-users

-- 
Bugs to the Future

-- 
Bugs to the Future

-- 
Bugs to the Future



From rousskov at measurement-factory.com  Fri Mar  3 00:45:17 2017
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Thu, 2 Mar 2017 17:45:17 -0700
Subject: [squid-users] squid 3.5.2==> HTTPS FATAL: The ssl_crtd helpers
 are crashing too rapidly, need help!
In-Reply-To: <592CB65D-536B-40F8-B887-12127542DCB5@netstream.ps>
References: <592CB65D-536B-40F8-B887-12127542DCB5@netstream.ps>
Message-ID: <53845e00-50bc-e26c-f640-ff920591b8ca@measurement-factory.com>

On 03/02/2017 01:47 PM, --Ahmad-- wrote:

> i have a problem with squid it get crashed after i enabled https !
> my version is 3.5.2
> should i update squid ?

Yes, you should. The change log for Squid v3.5 clearly shows several
relevant bugs fixed since v3.5.2, including:

>         - Fix some cases of ssl_crtd SSL certificate DB corruption

According to Yury, an upgrade may not fix all db corruption cases, so an
upgrade alone may not be enough, but it is very unlikely that somebody
is going to triage v3.5.2 problems for you.


HTH,

Alex.



From adrian.m.miller at gmail.com  Fri Mar  3 04:24:59 2017
From: adrian.m.miller at gmail.com (Adrian Miller)
Date: Fri, 3 Mar 2017 15:24:59 +1100
Subject: [squid-users] squid-users Digest, Vol 31, Issue 9
In-Reply-To: <mailman.10193.1488491185.20516.squid-users@lists.squid-cache.org>
References: <mailman.10193.1488491185.20516.squid-users@lists.squid-cache.org>
Message-ID: <CAOLOQx1v5Fd2-9RF2pnKwcAsJYKoyS67EseTB1mcoNsL_nEpOA@mail.gmail.com>

Are you creating the database as root or the squid user.....try as the
squid user

On 3 March 2017 at 08:46, <squid-users-request at lists.squid-cache.org> wrote:

> Send squid-users mailing list submissions to
>         squid-users at lists.squid-cache.org
>
> To subscribe or unsubscribe via the World Wide Web, visit
>         http://lists.squid-cache.org/listinfo/squid-users
> or, via email, send a message with subject or body 'help' to
>         squid-users-request at lists.squid-cache.org
>
> You can reach the person managing the list at
>         squid-users-owner at lists.squid-cache.org
>
> When replying, please edit your Subject line so it is more specific
> than "Re: Contents of squid-users digest..."
>
>
> Today's Topics:
>
>    1. Re: squid 3.5.2==> HTTPS FATAL: The ssl_crtd helpers are
>       crashing too rapidly, need help! (Yuri Voinov)
>
>
> ----------------------------------------------------------------------
>
> Message: 1
> Date: Fri, 3 Mar 2017 03:46:10 +0600
> From: Yuri Voinov <yvoinov at gmail.com>
> To: squid-users at lists.squid-cache.org
> Subject: Re: [squid-users] squid 3.5.2==> HTTPS FATAL: The ssl_crtd
>         helpers are crashing too rapidly, need help!
> Message-ID: <714528e5-a6d5-e72a-2bc7-9950a8eecb73 at gmail.com>
> Content-Type: text/plain; charset="utf-8"
>
> This problem, in principle, is common to all versions of ssl-bumped
> Squid from version 3.4 and 5.0, inclusive, and occurs when the stored
> certificate is damaged for any reason. The only thing vorkeraund that I
> could find - a monitor kesh.log and initialize the certificate database
> again with squid restart automatically.
>
> In some installations, this problem does not occur over the years. In
> other - almost daily. I have no desire to find out why this is happening
> exactly. For me it was easier to make the watchdog, which will follow up
> on this.
>
> 03.03.2017 3:40, Yuri Voinov ?????:
> >
> > One hint finally:
> >
> > '([^ ]*) helper database ([^ ]*) failed: The SSL certificate database
> > ([^ ]*) is corrupted. Please rebuild' - - - 0    exec
> > "/usr/local/bin/crtd_create.sh -r >/dev/null 2>&1"
> > 'FATAL: ([^ ]*) helpers are crashing too rapidly, need help!' - - -
> > 0    exec "/usr/local/bin/crtd_create.sh -r >/dev/null 2>&1"
> > 'Cannot add certificate to db.' - - - 0        exec
> > "/usr/local/bin/crtd_create.sh -r >/dev/null 2>&1"
> >
> > PS. This is from logsurfer.conf.
> >
> >
> > 03.03.2017 3:34, Yuri Voinov ?????:
> >>
> >> This error is usually preceded by another error in cache.log
> >> associated with the certificates.
> >>
> >> I will show you the direction. Then go himself.
> >>
> >> This software will useful for you to solve:
> >>
> >> http://www.crypt.gen.nz/logsurfer/
> >>
> >> HTH, Yuri
> >>
> >>
> >> 03.03.2017 2:47, --Ahmad-- ?????:
> >>> hey folks .
> >>> i have a problem with squid it get crashed after i enabled https !
> >>> cache log error => FATAL: The ssl_crtd helpers are crashing too
> >>> rapidly, need help!
> >>>
> >>> i googled many topics and relevant pages and couldnt find a
> >>> clear solution .
> >>>
> >>> the quick solution i made was i  removed the certs in file :
> >>> *rm -rfv /var/lib/ssl_db/*
> >>> *
> >>> *
> >>> *then reinitiated the DB using cmd below :*
> >>> /lib/squid/ssl_crtd -c -s /var/lib/ssl_db
> >>> chown -R squid.squid /var/lib/ssl_db
> >>> chown-R squid.squid /var/lib/ssl_db
> >>>
> >>> the restarted squid .
> >>>
> >>> but this is not a solution becuase squid get crashed again after
> >>> certain time and i don?t know why !
> >>> my version is 3.5.2
> >>>
> >>> here is squid.conf :
> >>>  /etc/squid/squid.conf
> >>> visible_hostname pcloud
> >>> acl ip1 myip 10.1.0.1
> >>> acl ip2 myip 192.168.10.210
> >>> tcp_outgoing_address 192.168.10.210 ip1
> >>> tcp_outgoing_address 192.168.10.210 ip2
> >>> #
> >>> # Recommended minimum configuration:
> >>> #
> >>>
> >>> # Example rule allowing access from your local networks.
> >>> # Adapt to list your (internal) IP networks from where browsing
> >>> # should be allowed
> >>> acl localnet src 10.0.0.0/8     # RFC1918 possible internal network
> >>> acl localnet src 172.16.0.0/12  # RFC1918 possible internal network
> >>> acl localnet src 192.168.0.0/16 # RFC1918 possible internal network
> >>> acl localnet src fc00::/7       # RFC 4193 local private network range
> >>> acl localnet src fe80::/10      # RFC 4291 link-local (directly
> >>> plugged) machines
> >>>
> >>> acl SSL_ports port 443
> >>> acl Safe_ports port 80          # http
> >>> acl Safe_ports port 21          # ftp
> >>> acl Safe_ports port 443         # https
> >>> acl Safe_ports port 70          # gopher
> >>> acl Safe_ports port 210         # wais
> >>> acl Safe_ports port 1025-65535  # unregistered ports
> >>> acl Safe_ports port 280         # http-mgmt
> >>> acl Safe_ports port 488         # gss-http
> >>> acl Safe_ports port 591         # filemaker
> >>> acl Safe_ports port 777         # multiling http
> >>> acl CONNECT method CONNECT
> >>>
> >>> #
> >>> # Recommended minimum Access Permission configuration:
> >>> #
> >>> # Deny requests to certain unsafe ports
> >>> http_access deny !Safe_ports
> >>>
> >>> # Deny CONNECT to other than secure SSL ports
> >>> http_access deny CONNECT !SSL_ports
> >>> http_access allow  CONNECT
> >>> # Only allow cachemgr access from localhost
> >>> http_access allow localhost manager
> >>> http_access deny manager
> >>>
> >>> # We strongly recommend the following be uncommented to protect
> innocent
>
> >>> # web applications running on the proxy server who think the only
> >>> # one who can access services on "localhost" is a local user
> >>> #http_access deny to_localhost
> >>>
> >>> #
> >>> # INSERT YOUR OWN RULE(S) HERE TO ALLOW ACCESS FROM YOUR CLIENTS
> >>> #
> >>>
> >>> # Example rule allowing access from your local networks.
> >>> # Adapt localnet in the ACL section to list your (internal) IP networks
> >>> # from where browsing should be allowed
> >>> http_access allow localnet
> >>> http_access allow localhost
> >>>
> >>> # And finally deny all other access to this proxy
> >>> http_access deny all
> >>>
> >>> # Squid normally listens to port 3128
> >>> http_port 3128
> >>>
> >>> # Uncomment and adjust the following to add a disk cache directory.
> >>> #cache_dir ufs /var/cache/squid 100 16 256
> >>>
> >>> # Leave coredumps in the first cache dir
> >>> #coredump_dir /var/cache/squid
> >>>
> >>> #
> >>> # Add any of your own refresh_pattern entries above these.
> >>> #
> >>> #
> >>>
> >>> http_port 3126
> >>> #http_port 3128
> >>> #######################################
> >>> #cache_swap_low 90
> >>> #cache_swap_high 95
> >>> ############################
> >>> cache_effective_user squid
> >>> cache_effective_group squid
> >>> memory_replacement_policy lru
> >>> cache_replacement_policy heap LFUDA
> >>> ########################
> >>> maximum_object_size 10000 MB
> >>> #cache_mem 5000 MB
> >>> maximum_object_size_in_memory 10 MB
> >>> #########################
> >>> logfile_rotate 2
> >>> max_filedescriptors 131072
> >>> ###############################
> >>> ############
> >>> cache_dir aufs /var/cache/squid 600000 64 128
> >>> #######################################
> >>> https_port 3129 intercept ssl-bump generate-host-certificates=on
> >>> dynamic_cert_mem_cache_size=4MB
> >>> cert=/usr/local/squid/ssl_cert/myca.pem
> >>> key=/usr/local/squid/ssl_cert/myca.pem
> >>> ssl_bump server-first all
> >>> sslcrtd_program /lib/squid/ssl_crtd -s /var/lib/ssl_db -M 4MB
> >>> sslcrtd_children 1000 startup=1 idle=1
> >>> ###
> >>> minimum_object_size 0 bytes
> >>> #refresh patterns for caching static files
> >>> refresh_pattern ^ftp: 1440 20% 10080
> >>> refresh_pattern ^gopher: 1440 0% 1440
> >>> refresh_pattern -i .(gif|png|jpg|jpeg|ico)$ 10080 90% 43200
> >>> override-expire ignore-no-cache ignore-no-store ignore-private
> >>> refresh_pattern -i .(iso|avi|wav|mp3|mp4|mpeg|swf|flv|x-flv)$ 43200
> >>> 90% 432000 override-expire ignore-no-cache ignore-no-store
> >>> ignore-private
> >>> refresh_pattern -i
> >>> .(deb|rpm|exe|zip|tar|tgz|ram|rar|bin|ppt|doc|tiff)$ 10080 90% 43200
> >>> override-expire ignore-no-cache ignore-no-store ignore-private
> >>> refresh_pattern -i .index.(html|htm)$ 0 40% 10080
> >>> refresh_pattern -i .(html|htm|css|js)$ 1440 40% 40320
> >>> refresh_pattern . 0 40% 40320
> >>>
> >>>
> >>>
> >>>
> >>>
> >>>
> >>>
> >>>
> >>> any Joy Guys ?
> >>>
> >>> should i update squid ? or downgrade squid ?
> >>>
> >>> kind regards
> >>>
> >>>
> >>>
> >>> _______________________________________________
> >>> squid-users mailing list
> >>> squid-users at lists.squid-cache.org
> >>> http://lists.squid-cache.org/listinfo/squid-users
> >>
> >> --
> >> Bugs to the Future
> >
> > --
> > Bugs to the Future
>
> --
> Bugs to the Future
> -------------- next part --------------
> An HTML attachment was scrubbed...
> URL: <http://lists.squid-cache.org/pipermail/squid-users/
> attachments/20170303/f5499462/attachment.html>
> -------------- next part --------------
> A non-text attachment was scrubbed...
> Name: 0x613DEC46.asc
> Type: application/pgp-keys
> Size: 2437 bytes
> Desc: not available
> URL: <http://lists.squid-cache.org/pipermail/squid-users/
> attachments/20170303/f5499462/attachment.key>
> -------------- next part --------------
> A non-text attachment was scrubbed...
> Name: signature.asc
> Type: application/pgp-signature
> Size: 473 bytes
> Desc: OpenPGP digital signature
> URL: <http://lists.squid-cache.org/pipermail/squid-users/
> attachments/20170303/f5499462/attachment.sig>
>
> ------------------------------
>
> Subject: Digest Footer
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>
>
> ------------------------------
>
> End of squid-users Digest, Vol 31, Issue 9
> ******************************************
>



-- 
I hate to advocate *drugs*, *alcohol*,* violence *or
*insanity* to anyone, *but* they've *always* worked for* me*

- Hunter S. Thompson
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20170303/efabbb57/attachment.htm>

From lists at compuniverse.de  Fri Mar  3 08:07:59 2017
From: lists at compuniverse.de (Amon Ott)
Date: Fri, 3 Mar 2017 09:07:59 +0100
Subject: [squid-users] Username not passed to url_rewrite_program
In-Reply-To: <7bb501d2939e$a6e99f10$f4bcdd30$@ngtech.co.il>
References: <3c520a74-00ef-cb94-0724-c04163fd44c2@compuniverse.de>
 <9e42743b-c973-a5ef-515f-e860f5faa9aa@compuniverse.de>
 <7bb501d2939e$a6e99f10$f4bcdd30$@ngtech.co.il>
Message-ID: <707c7d7e-a972-4390-b18e-a40aebe03036@compuniverse.de>

Hi Eliezer!

Am 02.03.2017 um 22:47 schrieb Eliezer Croitoru:
> It's not clear what the issue is to me yet.
> The current bug report is:
> http://bugs.squid-cache.org/show_bug.cgi?id=4679
> 
> From the cache logs what I see is that squid sends:
> "2017/03/02 13:49:27.799 kid1| 61,6| redirect.cc(285) constructHelperQuery: sending 'http://clients1.google.com/ocsp 127.0.0.1/localhost - POST myip=127.0.0.1 myport=3129
> ' to the redirector helper"
> IE without a username.
> Is it the case?
> ( I will write the rest in the bug report)

Yes, that is the problem. Ident identifies the username correctly, but
it does not get passed to the rewriter. I will go on in the bugtracker.

Amon.


From sothy.e98 at gmail.com  Fri Mar  3 09:02:13 2017
From: sothy.e98 at gmail.com (sothy shan)
Date: Fri, 3 Mar 2017 10:02:13 +0100
Subject: [squid-users] Reverse proxy with HTTPS
Message-ID: <CAHcF_0ZdYzE_7Rc70Pi0BTbAwDYs=J_ZaTGjnYBPkhCaFZtDyQ@mail.gmail.com>

Hello,

I am trying to test reverse proxy with HTTPS. For example, client makes
HTTPS request to squid server which make another HTTPS request to web
server.
To test the scenario, what setup I need to do?
If u have any document , please share me or point me key step.

However, after reading on web, I found  information to create reverse proxy
using sslwithwildcard certificate. In the setup, I dont know how to create
wildcard certificate. But I created certificate authority certificate.

Thanks for your help.

Best regards
Sothy
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20170303/96ea393b/attachment.htm>

From yvoinov at gmail.com  Fri Mar  3 11:33:58 2017
From: yvoinov at gmail.com (Yuri Voinov)
Date: Fri, 3 Mar 2017 17:33:58 +0600
Subject: [squid-users] squid-users Digest, Vol 31, Issue 9
In-Reply-To: <CAOLOQx1v5Fd2-9RF2pnKwcAsJYKoyS67EseTB1mcoNsL_nEpOA@mail.gmail.com>
References: <mailman.10193.1488491185.20516.squid-users@lists.squid-cache.org>
 <CAOLOQx1v5Fd2-9RF2pnKwcAsJYKoyS67EseTB1mcoNsL_nEpOA@mail.gmail.com>
Message-ID: <ea62888b-536f-eb51-3dbb-e75d52e1196c@gmail.com>



03.03.2017 10:24, Adrian Miller ?????:
> Are you creating the database as root or the squid user.....try as the
> squid user
It will not work when created as root. Will be permission denied. crtd
runs as squid, not as root.
>
> On 3 March 2017 at 08:46, <squid-users-request at lists.squid-cache.org
> <mailto:squid-users-request at lists.squid-cache.org>> wrote:
>
>     Send squid-users mailing list submissions to
>             squid-users at lists.squid-cache.org
>     <mailto:squid-users at lists.squid-cache.org>
>
>     To subscribe or unsubscribe via the World Wide Web, visit
>             http://lists.squid-cache.org/listinfo/squid-users
>     <http://lists.squid-cache.org/listinfo/squid-users>
>     or, via email, send a message with subject or body 'help' to
>             squid-users-request at lists.squid-cache.org
>     <mailto:squid-users-request at lists.squid-cache.org>
>
>     You can reach the person managing the list at
>             squid-users-owner at lists.squid-cache.org
>     <mailto:squid-users-owner at lists.squid-cache.org>
>
>     When replying, please edit your Subject line so it is more specific
>     than "Re: Contents of squid-users digest..."
>
>
>     Today's Topics:
>
>        1. Re: squid 3.5.2==> HTTPS FATAL: The ssl_crtd helpers are
>           crashing too rapidly, need help! (Yuri Voinov)
>
>
>     ----------------------------------------------------------------------
>
>     Message: 1
>     Date: Fri, 3 Mar 2017 03:46:10 +0600
>     From: Yuri Voinov <yvoinov at gmail.com <mailto:yvoinov at gmail.com>>
>     To: squid-users at lists.squid-cache.org
>     <mailto:squid-users at lists.squid-cache.org>
>     Subject: Re: [squid-users] squid 3.5.2==> HTTPS FATAL: The ssl_crtd
>             helpers are crashing too rapidly, need help!
>     Message-ID: <714528e5-a6d5-e72a-2bc7-9950a8eecb73 at gmail.com
>     <mailto:714528e5-a6d5-e72a-2bc7-9950a8eecb73 at gmail.com>>
>     Content-Type: text/plain; charset="utf-8"
>
>     This problem, in principle, is common to all versions of ssl-bumped
>     Squid from version 3.4 and 5.0, inclusive, and occurs when the stored
>     certificate is damaged for any reason. The only thing vorkeraund
>     that I
>     could find - a monitor kesh.log and initialize the certificate
>     database
>     again with squid restart automatically.
>
>     In some installations, this problem does not occur over the years. In
>     other - almost daily. I have no desire to find out why this is
>     happening
>     exactly. For me it was easier to make the watchdog, which will
>     follow up
>     on this.
>
>     03.03.2017 3:40, Yuri Voinov ?????:
>     >
>     > One hint finally:
>     >
>     > '([^ ]*) helper database ([^ ]*) failed: The SSL certificate
>     database
>     > ([^ ]*) is corrupted. Please rebuild' - - - 0    exec
>     > "/usr/local/bin/crtd_create.sh -r >/dev/null 2>&1"
>     > 'FATAL: ([^ ]*) helpers are crashing too rapidly, need help!' - - -
>     > 0    exec "/usr/local/bin/crtd_create.sh -r >/dev/null 2>&1"
>     > 'Cannot add certificate to db.' - - - 0        exec
>     > "/usr/local/bin/crtd_create.sh -r >/dev/null 2>&1"
>     >
>     > PS. This is from logsurfer.conf.
>     >
>     >
>     > 03.03.2017 3:34, Yuri Voinov ?????:
>     >>
>     >> This error is usually preceded by another error in cache.log
>     >> associated with the certificates.
>     >>
>     >> I will show you the direction. Then go himself.
>     >>
>     >> This software will useful for you to solve:
>     >>
>     >> http://www.crypt.gen.nz/logsurfer/
>     <http://www.crypt.gen.nz/logsurfer/>
>     >>
>     >> HTH, Yuri
>     >>
>     >>
>     >> 03.03.2017 2:47, --Ahmad-- ?????:
>     >>> hey folks .
>     >>> i have a problem with squid it get crashed after i enabled https !
>     >>> cache log error => FATAL: The ssl_crtd helpers are crashing too
>     >>> rapidly, need help!
>     >>>
>     >>> i googled many topics and relevant pages and couldnt find a
>     >>> clear solution .
>     >>>
>     >>> the quick solution i made was i  removed the certs in file :
>     >>> *rm -rfv /var/lib/ssl_db/*
>     >>> *
>     >>> *
>     >>> *then reinitiated the DB using cmd below :*
>     >>> /lib/squid/ssl_crtd -c -s /var/lib/ssl_db
>     >>> chown -R squid.squid /var/lib/ssl_db
>     >>> chown-R squid.squid /var/lib/ssl_db
>     >>>
>     >>> the restarted squid .
>     >>>
>     >>> but this is not a solution becuase squid get crashed again after
>     >>> certain time and i don?t know why !
>     >>> my version is 3.5.2
>     >>>
>     >>> here is squid.conf :
>     >>>  /etc/squid/squid.conf
>     >>> visible_hostname pcloud
>     >>> acl ip1 myip 10.1.0.1
>     >>> acl ip2 myip 192.168.10.210
>     >>> tcp_outgoing_address 192.168.10.210 ip1
>     >>> tcp_outgoing_address 192.168.10.210 ip2
>     >>> #
>     >>> # Recommended minimum configuration:
>     >>> #
>     >>>
>     >>> # Example rule allowing access from your local networks.
>     >>> # Adapt to list your (internal) IP networks from where browsing
>     >>> # should be allowed
>     >>> acl localnet src 10.0.0.0/8 <http://10.0.0.0/8>     # RFC1918
>     possible internal network
>     >>> acl localnet src 172.16.0.0/12 <http://172.16.0.0/12>  #
>     RFC1918 possible internal network
>     >>> acl localnet src 192.168.0.0/16 <http://192.168.0.0/16> #
>     RFC1918 possible internal network
>     >>> acl localnet src fc00::/7       # RFC 4193 local private
>     network range
>     >>> acl localnet src fe80::/10      # RFC 4291 link-local (directly
>     >>> plugged) machines
>     >>>
>     >>> acl SSL_ports port 443
>     >>> acl Safe_ports port 80          # http
>     >>> acl Safe_ports port 21          # ftp
>     >>> acl Safe_ports port 443         # https
>     >>> acl Safe_ports port 70          # gopher
>     >>> acl Safe_ports port 210         # wais
>     >>> acl Safe_ports port 1025-65535  # unregistered ports
>     >>> acl Safe_ports port 280         # http-mgmt
>     >>> acl Safe_ports port 488         # gss-http
>     >>> acl Safe_ports port 591         # filemaker
>     >>> acl Safe_ports port 777         # multiling http
>     >>> acl CONNECT method CONNECT
>     >>>
>     >>> #
>     >>> # Recommended minimum Access Permission configuration:
>     >>> #
>     >>> # Deny requests to certain unsafe ports
>     >>> http_access deny !Safe_ports
>     >>>
>     >>> # Deny CONNECT to other than secure SSL ports
>     >>> http_access deny CONNECT !SSL_ports
>     >>> http_access allow  CONNECT
>     >>> # Only allow cachemgr access from localhost
>     >>> http_access allow localhost manager
>     >>> http_access deny manager
>     >>>
>     >>> # We strongly recommend the following be uncommented to
>     protect innocent
>
>     >>> # web applications running on the proxy server who think the only
>     >>> # one who can access services on "localhost" is a local user
>     >>> #http_access deny to_localhost
>     >>>
>     >>> #
>     >>> # INSERT YOUR OWN RULE(S) HERE TO ALLOW ACCESS FROM YOUR CLIENTS
>     >>> #
>     >>>
>     >>> # Example rule allowing access from your local networks.
>     >>> # Adapt localnet in the ACL section to list your (internal) IP
>     networks
>     >>> # from where browsing should be allowed
>     >>> http_access allow localnet
>     >>> http_access allow localhost
>     >>>
>     >>> # And finally deny all other access to this proxy
>     >>> http_access deny all
>     >>>
>     >>> # Squid normally listens to port 3128
>     >>> http_port 3128
>     >>>
>     >>> # Uncomment and adjust the following to add a disk cache
>     directory.
>     >>> #cache_dir ufs /var/cache/squid 100 16 256
>     >>>
>     >>> # Leave coredumps in the first cache dir
>     >>> #coredump_dir /var/cache/squid
>     >>>
>     >>> #
>     >>> # Add any of your own refresh_pattern entries above these.
>     >>> #
>     >>> #
>     >>>
>     >>> http_port 3126
>     >>> #http_port 3128
>     >>> #######################################
>     >>> #cache_swap_low 90
>     >>> #cache_swap_high 95
>     >>> ############################
>     >>> cache_effective_user squid
>     >>> cache_effective_group squid
>     >>> memory_replacement_policy lru
>     >>> cache_replacement_policy heap LFUDA
>     >>> ########################
>     >>> maximum_object_size 10000 MB
>     >>> #cache_mem 5000 MB
>     >>> maximum_object_size_in_memory 10 MB
>     >>> #########################
>     >>> logfile_rotate 2
>     >>> max_filedescriptors 131072
>     >>> ###############################
>     >>> ############
>     >>> cache_dir aufs /var/cache/squid 600000 64 128
>     >>> #######################################
>     >>> https_port 3129 intercept ssl-bump generate-host-certificates=on
>     >>> dynamic_cert_mem_cache_size=4MB
>     >>> cert=/usr/local/squid/ssl_cert/myca.pem
>     >>> key=/usr/local/squid/ssl_cert/myca.pem
>     >>> ssl_bump server-first all
>     >>> sslcrtd_program /lib/squid/ssl_crtd -s /var/lib/ssl_db -M 4MB
>     >>> sslcrtd_children 1000 startup=1 idle=1
>     >>> ###
>     >>> minimum_object_size 0 bytes
>     >>> #refresh patterns for caching static files
>     >>> refresh_pattern ^ftp: 1440 20% 10080
>     >>> refresh_pattern ^gopher: 1440 0% 1440
>     >>> refresh_pattern -i .(gif|png|jpg|jpeg|ico)$ 10080 90% 43200
>     >>> override-expire ignore-no-cache ignore-no-store ignore-private
>     >>> refresh_pattern -i .(iso|avi|wav|mp3|mp4|mpeg|swf|flv|x-flv)$
>     43200
>     >>> 90% 432000 override-expire ignore-no-cache ignore-no-store
>     >>> ignore-private
>     >>> refresh_pattern -i
>     >>> .(deb|rpm|exe|zip|tar|tgz|ram|rar|bin|ppt|doc|tiff)$ 10080 90%
>     43200
>     >>> override-expire ignore-no-cache ignore-no-store ignore-private
>     >>> refresh_pattern -i .index.(html|htm)$ 0 40% 10080
>     >>> refresh_pattern -i .(html|htm|css|js)$ 1440 40% 40320
>     >>> refresh_pattern . 0 40% 40320
>     >>>
>     >>>
>     >>>
>     >>>
>     >>>
>     >>>
>     >>>
>     >>>
>     >>> any Joy Guys ?
>     >>>
>     >>> should i update squid ? or downgrade squid ?
>     >>>
>     >>> kind regards
>     >>>
>     >>>
>     >>>
>     >>> _______________________________________________
>     >>> squid-users mailing list
>     >>> squid-users at lists.squid-cache.org
>     <mailto:squid-users at lists.squid-cache.org>
>     >>> http://lists.squid-cache.org/listinfo/squid-users
>     <http://lists.squid-cache.org/listinfo/squid-users>
>     >>
>     >> --
>     >> Bugs to the Future
>     >
>     > --
>     > Bugs to the Future
>
>     --
>     Bugs to the Future
>     -------------- next part --------------
>     An HTML attachment was scrubbed...
>     URL:
>     <http://lists.squid-cache.org/pipermail/squid-users/attachments/20170303/f5499462/attachment.html
>     <http://lists.squid-cache.org/pipermail/squid-users/attachments/20170303/f5499462/attachment.html>>
>     -------------- next part --------------
>     A non-text attachment was scrubbed...
>     Name: 0x613DEC46.asc
>     Type: application/pgp-keys
>     Size: 2437 bytes
>     Desc: not available
>     URL:
>     <http://lists.squid-cache.org/pipermail/squid-users/attachments/20170303/f5499462/attachment.key
>     <http://lists.squid-cache.org/pipermail/squid-users/attachments/20170303/f5499462/attachment.key>>
>     -------------- next part --------------
>     A non-text attachment was scrubbed...
>     Name: signature.asc
>     Type: application/pgp-signature
>     Size: 473 bytes
>     Desc: OpenPGP digital signature
>     URL:
>     <http://lists.squid-cache.org/pipermail/squid-users/attachments/20170303/f5499462/attachment.sig
>     <http://lists.squid-cache.org/pipermail/squid-users/attachments/20170303/f5499462/attachment.sig>>
>
>     ------------------------------
>
>     Subject: Digest Footer
>
>     _______________________________________________
>     squid-users mailing list
>     squid-users at lists.squid-cache.org
>     <mailto:squid-users at lists.squid-cache.org>
>     http://lists.squid-cache.org/listinfo/squid-users
>     <http://lists.squid-cache.org/listinfo/squid-users>
>
>
>     ------------------------------
>
>     End of squid-users Digest, Vol 31, Issue 9
>     ******************************************
>
>
>
>
> -- 
> I hate to advocate *drugs*, *alcohol*,*violence *or
> *insanity* to anyone,*but* they've *always* worked for*me*
>
> - Hunter S. Thompson
>
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users

-- 
Bugs to the Future
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20170303/fb06df78/attachment.htm>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: 0x613DEC46.asc
Type: application/pgp-keys
Size: 2437 bytes
Desc: not available
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20170303/fb06df78/attachment.key>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: signature.asc
Type: application/pgp-signature
Size: 473 bytes
Desc: OpenPGP digital signature
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20170303/fb06df78/attachment.sig>

From yvoinov at gmail.com  Fri Mar  3 11:37:59 2017
From: yvoinov at gmail.com (Yuri Voinov)
Date: Fri, 3 Mar 2017 17:37:59 +0600
Subject: [squid-users] squid 3.5.2==> HTTPS FATAL: The ssl_crtd helpers
 are crashing too rapidly, need help!
In-Reply-To: <7c1d01d293b5$9b22d2c0$d1687840$@ngtech.co.il>
References: <592CB65D-536B-40F8-B887-12127542DCB5@netstream.ps>
 <38a07e38-7347-313a-b27f-08fd7eddfa3f@gmail.com>
 <97215197-78fc-e987-9a9d-51a697010e7b@gmail.com>
 <714528e5-a6d5-e72a-2bc7-9950a8eecb73@gmail.com>
 <7c1d01d293b5$9b22d2c0$d1687840$@ngtech.co.il>
Message-ID: <a5a63820-0afd-563b-f627-3819733a5680@gmail.com>



03.03.2017 6:32, Eliezer Croitoru ?????:
> Hey Yuri,
>
> This issue is not 100% squid but I think it's related to the way ssl_crtd works.
> I am not sure if it has some locking or other things to prevent such issues.
> The first solution is to somehow defend the DB from corruption, like in a case that more then a dozen identical requests are being done towards a single site and two ssl_crtd helpers are trying to do the same things.
> I believe that something to fence this should already be inside squid and ssl_crtd but I am pretty sure this is the main issue.
I suggests this can be external reason to occurs this issue. Somehow,
for example, BlueCoat on ISP upstream, tcp packets corruption, etc. I
dont know, just guessing.
> Alex and his team should know the answer for this subject and if I'm not wrong theoretically there are couple ways to prevent the mentioned issues.
> I had a plan to try and understand the ssl_crtd code and interface but yet to do so.
>
> I hope this issue will be resolved in a way that it can be backported to 3.5 in the worst case.
I hope too, but if it external..... fewwwwwwwwwww.

Anyway, watchdog is good backup to preventing manual interventions by SA.
>
> Eliezer
>
> ----
> http://ngtech.co.il/lmgtfy/
> Linux System Administrator
> Mobile: +972-5-28704261
> Email: eliezer at ngtech.co.il
>
>
> From: squid-users [mailto:squid-users-bounces at lists.squid-cache.org] On Behalf Of Yuri Voinov
> Sent: Thursday, March 2, 2017 11:46 PM
> To: squid-users at lists.squid-cache.org
> Subject: Re: [squid-users] squid 3.5.2==> HTTPS FATAL: The ssl_crtd helpers are crashing too rapidly, need help!
>
> This problem, in principle, is common to all versions of ssl-bumped Squid from version 3.4 and 5.0, inclusive, and occurs when the stored certificate is damaged for any reason. The only thing vorkeraund that I could find - a monitor kesh.log and initialize the certificate database again with squid restart automatically.
> In some installations, this problem does not occur over the years. In other - almost daily. I have no desire to find out why this is happening exactly. For me it was easier to make the watchdog, which will follow up on this.
> 03.03.2017 3:40, Yuri Voinov ?????:
> One hint finally:
> '([^ ]*) helper database ([^ ]*) failed: The SSL certificate database ([^ ]*) is corrupted. Please rebuild' - - - 0    exec "/usr/local/bin/crtd_create.sh -r >/dev/null 2>&1"
> 'FATAL: ([^ ]*) helpers are crashing too rapidly, need help!' - - - 0    exec "/usr/local/bin/crtd_create.sh -r >/dev/null 2>&1"
> 'Cannot add certificate to db.' - - - 0        exec "/usr/local/bin/crtd_create.sh -r >/dev/null 2>&1"
> PS. This is from logsurfer.conf.
>
> 03.03.2017 3:34, Yuri Voinov ?????:
> This error is usually preceded by another error in cache.log associated with the certificates.
> I will show you the direction. Then go himself.
> This software will useful for you to solve:
> http://www.crypt.gen.nz/logsurfer/
> HTH, Yuri
>
> 03.03.2017 2:47, --Ahmad-- ?????:
> hey folks . 
> i have a problem with squid it get crashed after i enabled https !
> cache log error => FATAL: The ssl_crtd helpers are crashing too rapidly, need help!
>
> i googled many topics and relevant pages and couldnt find a clear solution .
>
> the quick solution i made was i  removed the certs in file :
> rm -rfv /var/lib/ssl_db/
>
>
> then reinitiated the DB using cmd below :
> /lib/squid/ssl_crtd -c -s /var/lib/ssl_db
> chown -R squid.squid /var/lib/ssl_db
> chown -R squid.squid /var/lib/ssl_db
>
>
> the restarted squid .
>
>
> but this is not a solution becuase squid get crashed again after certain time and i don?t know why !
> my version is 3.5.2
>
>
> here is squid.conf :
>  /etc/squid/squid.conf
> visible_hostname pcloud
> acl ip1 myip 10.1.0.1
> acl ip2 myip 192.168.10.210
> tcp_outgoing_address 192.168.10.210 ip1
> tcp_outgoing_address 192.168.10.210 ip2
> #
> # Recommended minimum configuration:
> #
>
> # Example rule allowing access from your local networks.
> # Adapt to list your (internal) IP networks from where browsing
> # should be allowed
> acl localnet src 10.0.0.0/8     # RFC1918 possible internal network
> acl localnet src 172.16.0.0/12  # RFC1918 possible internal network
> acl localnet src 192.168.0.0/16 # RFC1918 possible internal network
> acl localnet src fc00::/7       # RFC 4193 local private network range
> acl localnet src fe80::/10      # RFC 4291 link-local (directly plugged) machines
>
> acl SSL_ports port 443
> acl Safe_ports port 80          # http
> acl Safe_ports port 21          # ftp
> acl Safe_ports port 443         # https
> acl Safe_ports port 70          # gopher
> acl Safe_ports port 210         # wais
> acl Safe_ports port 1025-65535  # unregistered ports
> acl Safe_ports port 280         # http-mgmt
> acl Safe_ports port 488         # gss-http
> acl Safe_ports port 591         # filemaker
> acl Safe_ports port 777         # multiling http
> acl CONNECT method CONNECT
>
> #
> # Recommended minimum Access Permission configuration:
> #
> # Deny requests to certain unsafe ports
> http_access deny !Safe_ports
>
> # Deny CONNECT to other than secure SSL ports
> http_access deny CONNECT !SSL_ports
> http_access allow  CONNECT 
> # Only allow cachemgr access from localhost
> http_access allow localhost manager
> http_access deny manager
>
> # We strongly recommend the following be uncommented to protect innocent
> # web applications running on the proxy server who think the only
> # one who can access services on "localhost" is a local user
> #http_access deny to_localhost
>
> #
> # INSERT YOUR OWN RULE(S) HERE TO ALLOW ACCESS FROM YOUR CLIENTS
> #
>
> # Example rule allowing access from your local networks.
> # Adapt localnet in the ACL section to list your (internal) IP networks
> # from where browsing should be allowed
> http_access allow localnet
> http_access allow localhost
>
> # And finally deny all other access to this proxy
> http_access deny all
>
> # Squid normally listens to port 3128
> http_port 3128
>
> # Uncomment and adjust the following to add a disk cache directory.
> #cache_dir ufs /var/cache/squid 100 16 256
>
> # Leave coredumps in the first cache dir
> #coredump_dir /var/cache/squid
>
> #
> # Add any of your own refresh_pattern entries above these.
> #
> #
>
> http_port 3126
> #http_port 3128
> #######################################
> #cache_swap_low 90
> #cache_swap_high 95
> ############################
> cache_effective_user squid
> cache_effective_group squid
> memory_replacement_policy lru
> cache_replacement_policy heap LFUDA
> ########################
> maximum_object_size 10000 MB
> #cache_mem 5000 MB
> maximum_object_size_in_memory 10 MB
> #########################
> logfile_rotate 2
> max_filedescriptors 131072
> ###############################
> ############
> cache_dir aufs /var/cache/squid 600000 64 128
> #######################################
> https_port 3129 intercept ssl-bump generate-host-certificates=on dynamic_cert_mem_cache_size=4MB cert=/usr/local/squid/ssl_cert/myca.pem key=/usr/local/squid/ssl_cert/myca.pem
> ssl_bump server-first all
> sslcrtd_program /lib/squid/ssl_crtd -s /var/lib/ssl_db -M 4MB
> sslcrtd_children 1000 startup=1 idle=1
> ###
> minimum_object_size 0 bytes
> #refresh patterns for caching static files
> refresh_pattern ^ftp: 1440 20% 10080
> refresh_pattern ^gopher: 1440 0% 1440
> refresh_pattern -i .(gif|png|jpg|jpeg|ico)$ 10080 90% 43200 override-expire ignore-no-cache ignore-no-store ignore-private
> refresh_pattern -i .(iso|avi|wav|mp3|mp4|mpeg|swf|flv|x-flv)$ 43200 90% 432000 override-expire ignore-no-cache ignore-no-store ignore-private
> refresh_pattern -i .(deb|rpm|exe|zip|tar|tgz|ram|rar|bin|ppt|doc|tiff)$ 10080 90% 43200 override-expire ignore-no-cache ignore-no-store ignore-private
> refresh_pattern -i .index.(html|htm)$ 0 40% 10080
> refresh_pattern -i .(html|htm|css|js)$ 1440 40% 40320
> refresh_pattern . 0 40% 40320
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
> any Joy Guys ?
>
>
> should i update squid ? or downgrade squid ?
>
>
> kind regards 
>
>
>
>
> _______________________________________________
> squid-users mailing list
> mailto:squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>

-- 
Bugs to the Future
-------------- next part --------------
A non-text attachment was scrubbed...
Name: 0x613DEC46.asc
Type: application/pgp-keys
Size: 2437 bytes
Desc: not available
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20170303/119ec9c5/attachment.key>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: signature.asc
Type: application/pgp-signature
Size: 473 bytes
Desc: OpenPGP digital signature
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20170303/119ec9c5/attachment.sig>

From uhlar at fantomas.sk  Fri Mar  3 11:59:20 2017
From: uhlar at fantomas.sk (Matus UHLAR - fantomas)
Date: Fri, 3 Mar 2017 12:59:20 +0100
Subject: [squid-users] Reverse proxy with HTTPS
In-Reply-To: <CAHcF_0ZdYzE_7Rc70Pi0BTbAwDYs=J_ZaTGjnYBPkhCaFZtDyQ@mail.gmail.com>
References: <CAHcF_0ZdYzE_7Rc70Pi0BTbAwDYs=J_ZaTGjnYBPkhCaFZtDyQ@mail.gmail.com>
Message-ID: <20170303115920.GA28103@fantomas.sk>

On 03.03.17 10:02, sothy shan wrote:
>I am trying to test reverse proxy with HTTPS. For example, client makes
>HTTPS request to squid server which make another HTTPS request to web
>server.

what point does this have, except disabling client certificates?

-- 
Matus UHLAR - fantomas, uhlar at fantomas.sk ; http://www.fantomas.sk/
Warning: I wish NOT to receive e-mail advertising to this address.
Varovanie: na tuto adresu chcem NEDOSTAVAT akukolvek reklamnu postu.
Honk if you love peace and quiet. 


From ahmed.zaeem at netstream.ps  Fri Mar  3 12:20:24 2017
From: ahmed.zaeem at netstream.ps (--Ahmad--)
Date: Fri, 3 Mar 2017 14:20:24 +0200
Subject: [squid-users] squid 3.5.2==> HTTPS FATAL: The ssl_crtd helpers
	are crashing too rapidly, need help!
In-Reply-To: <a5a63820-0afd-563b-f627-3819733a5680@gmail.com>
References: <592CB65D-536B-40F8-B887-12127542DCB5@netstream.ps>
 <38a07e38-7347-313a-b27f-08fd7eddfa3f@gmail.com>
 <97215197-78fc-e987-9a9d-51a697010e7b@gmail.com>
 <714528e5-a6d5-e72a-2bc7-9950a8eecb73@gmail.com>
 <7c1d01d293b5$9b22d2c0$d1687840$@ngtech.co.il>
 <a5a63820-0afd-563b-f627-3819733a5680@gmail.com>
Message-ID: <4F85658B-A32E-4D74-98E6-E1CEE90168EA@netstream.ps>

@ eliezer 
i was using children as 10
ans faced the problem 


so i trued to increase children to 1000 to see if this was the reason 
and unfortunately the same problem .

ys I?m using debian 6 os .

i appreciable the helping from all the replies below but so far i havent got any clear solution .

now i updated to 3.5.24 last one .
and will see it if comes back ?i will update the list with result .

if it failed ? I?m forced to create cron job to remove the certs like every 24 hours .

thank you  guys all of you .
thanks amos , thanks eliezer , thanks yuri

kind regards
> On Mar 3, 2017, at 1:37 PM, Yuri Voinov <yvoinov at gmail.com> wrote:
> 
> 
> 
> 03.03.2017 6:32, Eliezer Croitoru ?????:
>> Hey Yuri,
>> 
>> This issue is not 100% squid but I think it's related to the way ssl_crtd works.
>> I am not sure if it has some locking or other things to prevent such issues.
>> The first solution is to somehow defend the DB from corruption, like in a case that more then a dozen identical requests are being done towards a single site and two ssl_crtd helpers are trying to do the same things.
>> I believe that something to fence this should already be inside squid and ssl_crtd but I am pretty sure this is the main issue.
> I suggests this can be external reason to occurs this issue. Somehow,
> for example, BlueCoat on ISP upstream, tcp packets corruption, etc. I
> dont know, just guessing.
>> Alex and his team should know the answer for this subject and if I'm not wrong theoretically there are couple ways to prevent the mentioned issues.
>> I had a plan to try and understand the ssl_crtd code and interface but yet to do so.
>> 
>> I hope this issue will be resolved in a way that it can be backported to 3.5 in the worst case.
> I hope too, but if it external..... fewwwwwwwwwww.
> 
> Anyway, watchdog is good backup to preventing manual interventions by SA.
>> 
>> Eliezer
>> 
>> ----
>> http://ngtech.co.il/lmgtfy/
>> Linux System Administrator
>> Mobile: +972-5-28704261
>> Email: eliezer at ngtech.co.il
>> 
>> 
>> From: squid-users [mailto:squid-users-bounces at lists.squid-cache.org] On Behalf Of Yuri Voinov
>> Sent: Thursday, March 2, 2017 11:46 PM
>> To: squid-users at lists.squid-cache.org
>> Subject: Re: [squid-users] squid 3.5.2==> HTTPS FATAL: The ssl_crtd helpers are crashing too rapidly, need help!
>> 
>> This problem, in principle, is common to all versions of ssl-bumped Squid from version 3.4 and 5.0, inclusive, and occurs when the stored certificate is damaged for any reason. The only thing vorkeraund that I could find - a monitor kesh.log and initialize the certificate database again with squid restart automatically.
>> In some installations, this problem does not occur over the years. In other - almost daily. I have no desire to find out why this is happening exactly. For me it was easier to make the watchdog, which will follow up on this.
>> 03.03.2017 3:40, Yuri Voinov ?????:
>> One hint finally:
>> '([^ ]*) helper database ([^ ]*) failed: The SSL certificate database ([^ ]*) is corrupted. Please rebuild' - - - 0    exec "/usr/local/bin/crtd_create.sh -r >/dev/null 2>&1"
>> 'FATAL: ([^ ]*) helpers are crashing too rapidly, need help!' - - - 0    exec "/usr/local/bin/crtd_create.sh -r >/dev/null 2>&1"
>> 'Cannot add certificate to db.' - - - 0        exec "/usr/local/bin/crtd_create.sh -r >/dev/null 2>&1"
>> PS. This is from logsurfer.conf.
>> 
>> 03.03.2017 3:34, Yuri Voinov ?????:
>> This error is usually preceded by another error in cache.log associated with the certificates.
>> I will show you the direction. Then go himself.
>> This software will useful for you to solve:
>> http://www.crypt.gen.nz/logsurfer/
>> HTH, Yuri
>> 
>> 03.03.2017 2:47, --Ahmad-- ?????:
>> hey folks . 
>> i have a problem with squid it get crashed after i enabled https !
>> cache log error => FATAL: The ssl_crtd helpers are crashing too rapidly, need help!
>> 
>> i googled many topics and relevant pages and couldnt find a clear solution .
>> 
>> the quick solution i made was i  removed the certs in file :
>> rm -rfv /var/lib/ssl_db/
>> 
>> 
>> then reinitiated the DB using cmd below :
>> /lib/squid/ssl_crtd -c -s /var/lib/ssl_db
>> chown -R squid.squid /var/lib/ssl_db
>> chown -R squid.squid /var/lib/ssl_db
>> 
>> 
>> the restarted squid .
>> 
>> 
>> but this is not a solution becuase squid get crashed again after certain time and i don?t know why !
>> my version is 3.5.2
>> 
>> 
>> here is squid.conf :
>> /etc/squid/squid.conf
>> visible_hostname pcloud
>> acl ip1 myip 10.1.0.1
>> acl ip2 myip 192.168.10.210
>> tcp_outgoing_address 192.168.10.210 ip1
>> tcp_outgoing_address 192.168.10.210 ip2
>> #
>> # Recommended minimum configuration:
>> #
>> 
>> # Example rule allowing access from your local networks.
>> # Adapt to list your (internal) IP networks from where browsing
>> # should be allowed
>> acl localnet src 10.0.0.0/8     # RFC1918 possible internal network
>> acl localnet src 172.16.0.0/12  # RFC1918 possible internal network
>> acl localnet src 192.168.0.0/16 # RFC1918 possible internal network
>> acl localnet src fc00::/7       # RFC 4193 local private network range
>> acl localnet src fe80::/10      # RFC 4291 link-local (directly plugged) machines
>> 
>> acl SSL_ports port 443
>> acl Safe_ports port 80          # http
>> acl Safe_ports port 21          # ftp
>> acl Safe_ports port 443         # https
>> acl Safe_ports port 70          # gopher
>> acl Safe_ports port 210         # wais
>> acl Safe_ports port 1025-65535  # unregistered ports
>> acl Safe_ports port 280         # http-mgmt
>> acl Safe_ports port 488         # gss-http
>> acl Safe_ports port 591         # filemaker
>> acl Safe_ports port 777         # multiling http
>> acl CONNECT method CONNECT
>> 
>> #
>> # Recommended minimum Access Permission configuration:
>> #
>> # Deny requests to certain unsafe ports
>> http_access deny !Safe_ports
>> 
>> # Deny CONNECT to other than secure SSL ports
>> http_access deny CONNECT !SSL_ports
>> http_access allow  CONNECT 
>> # Only allow cachemgr access from localhost
>> http_access allow localhost manager
>> http_access deny manager
>> 
>> # We strongly recommend the following be uncommented to protect innocent
>> # web applications running on the proxy server who think the only
>> # one who can access services on "localhost" is a local user
>> #http_access deny to_localhost
>> 
>> #
>> # INSERT YOUR OWN RULE(S) HERE TO ALLOW ACCESS FROM YOUR CLIENTS
>> #
>> 
>> # Example rule allowing access from your local networks.
>> # Adapt localnet in the ACL section to list your (internal) IP networks
>> # from where browsing should be allowed
>> http_access allow localnet
>> http_access allow localhost
>> 
>> # And finally deny all other access to this proxy
>> http_access deny all
>> 
>> # Squid normally listens to port 3128
>> http_port 3128
>> 
>> # Uncomment and adjust the following to add a disk cache directory.
>> #cache_dir ufs /var/cache/squid 100 16 256
>> 
>> # Leave coredumps in the first cache dir
>> #coredump_dir /var/cache/squid
>> 
>> #
>> # Add any of your own refresh_pattern entries above these.
>> #
>> #
>> 
>> http_port 3126
>> #http_port 3128
>> #######################################
>> #cache_swap_low 90
>> #cache_swap_high 95
>> ############################
>> cache_effective_user squid
>> cache_effective_group squid
>> memory_replacement_policy lru
>> cache_replacement_policy heap LFUDA
>> ########################
>> maximum_object_size 10000 MB
>> #cache_mem 5000 MB
>> maximum_object_size_in_memory 10 MB
>> #########################
>> logfile_rotate 2
>> max_filedescriptors 131072
>> ###############################
>> ############
>> cache_dir aufs /var/cache/squid 600000 64 128
>> #######################################
>> https_port 3129 intercept ssl-bump generate-host-certificates=on dynamic_cert_mem_cache_size=4MB cert=/usr/local/squid/ssl_cert/myca.pem key=/usr/local/squid/ssl_cert/myca.pem
>> ssl_bump server-first all
>> sslcrtd_program /lib/squid/ssl_crtd -s /var/lib/ssl_db -M 4MB
>> sslcrtd_children 1000 startup=1 idle=1
>> ###
>> minimum_object_size 0 bytes
>> #refresh patterns for caching static files
>> refresh_pattern ^ftp: 1440 20% 10080
>> refresh_pattern ^gopher: 1440 0% 1440
>> refresh_pattern -i .(gif|png|jpg|jpeg|ico)$ 10080 90% 43200 override-expire ignore-no-cache ignore-no-store ignore-private
>> refresh_pattern -i .(iso|avi|wav|mp3|mp4|mpeg|swf|flv|x-flv)$ 43200 90% 432000 override-expire ignore-no-cache ignore-no-store ignore-private
>> refresh_pattern -i .(deb|rpm|exe|zip|tar|tgz|ram|rar|bin|ppt|doc|tiff)$ 10080 90% 43200 override-expire ignore-no-cache ignore-no-store ignore-private
>> refresh_pattern -i .index.(html|htm)$ 0 40% 10080
>> refresh_pattern -i .(html|htm|css|js)$ 1440 40% 40320
>> refresh_pattern . 0 40% 40320
>> 
>> 
>> 
>> 
>> 
>> 
>> 
>> 
>> 
>> 
>> 
>> 
>> 
>> 
>> 
>> 
>> any Joy Guys ?
>> 
>> 
>> should i update squid ? or downgrade squid ?
>> 
>> 
>> kind regards 
>> 
>> 
>> 
>> 
>> _______________________________________________
>> squid-users mailing list
>> mailto:squid-users at lists.squid-cache.org
>> http://lists.squid-cache.org/listinfo/squid-users
>> 
> 
> -- 
> Bugs to the Future
> <0x613DEC46.asc>_______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org <mailto:squid-users at lists.squid-cache.org>
> http://lists.squid-cache.org/listinfo/squid-users <http://lists.squid-cache.org/listinfo/squid-users>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20170303/33d475d4/attachment.htm>

From yvoinov at gmail.com  Fri Mar  3 12:53:01 2017
From: yvoinov at gmail.com (Yuri Voinov)
Date: Fri, 3 Mar 2017 18:53:01 +0600
Subject: [squid-users] squid 3.5.2==> HTTPS FATAL: The ssl_crtd helpers
 are crashing too rapidly, need help!
In-Reply-To: <4F85658B-A32E-4D74-98E6-E1CEE90168EA@netstream.ps>
References: <592CB65D-536B-40F8-B887-12127542DCB5@netstream.ps>
 <38a07e38-7347-313a-b27f-08fd7eddfa3f@gmail.com>
 <97215197-78fc-e987-9a9d-51a697010e7b@gmail.com>
 <714528e5-a6d5-e72a-2bc7-9950a8eecb73@gmail.com>
 <7c1d01d293b5$9b22d2c0$d1687840$@ngtech.co.il>
 <a5a63820-0afd-563b-f627-3819733a5680@gmail.com>
 <4F85658B-A32E-4D74-98E6-E1CEE90168EA@netstream.ps>
Message-ID: <bfbda1c1-0b1e-37ed-2bf5-2ddcc8b56bd1@gmail.com>



03.03.2017 18:20, --Ahmad-- ?????:
> @ eliezer 
> i was using children as 10
> ans faced the problem 
>
>
> so i trued to increase children to 1000 to see if this was the reason 
> and unfortunately the same problem .
>
> ys I?m using debian 6 os .
>
> i appreciable the helping from all the replies below but so far i
> havent got any clear solution .
>
> now i updated to 3.5.24 last one .
> and will see it if comes back ?i will update the list with result .
>
> if it failed ? I?m forced to create cron job to remove the certs like
> every 24 hours .
Cron is not the best solution. Logsurf will be better.
>
> thank you  guys all of you .
> thanks amos , thanks eliezer , thanks yuri
>
> kind regards
>> On Mar 3, 2017, at 1:37 PM, Yuri Voinov <yvoinov at gmail.com
>> <mailto:yvoinov at gmail.com>> wrote:
>>
>>
>>
>> 03.03.2017 6:32, Eliezer Croitoru ?????:
>>> Hey Yuri,
>>>
>>> This issue is not 100% squid but I think it's related to the way
>>> ssl_crtd works.
>>> I am not sure if it has some locking or other things to prevent such
>>> issues.
>>> The first solution is to somehow defend the DB from corruption, like
>>> in a case that more then a dozen identical requests are being done
>>> towards a single site and two ssl_crtd helpers are trying to do the
>>> same things.
>>> I believe that something to fence this should already be inside
>>> squid and ssl_crtd but I am pretty sure this is the main issue.
>> I suggests this can be external reason to occurs this issue. Somehow,
>> for example, BlueCoat on ISP upstream, tcp packets corruption, etc. I
>> dont know, just guessing.
>>> Alex and his team should know the answer for this subject and if I'm
>>> not wrong theoretically there are couple ways to prevent the
>>> mentioned issues.
>>> I had a plan to try and understand the ssl_crtd code and interface
>>> but yet to do so.
>>>
>>> I hope this issue will be resolved in a way that it can be
>>> backported to 3.5 in the worst case.
>> I hope too, but if it external..... fewwwwwwwwwww.
>>
>> Anyway, watchdog is good backup to preventing manual interventions by SA.
>>>
>>> Eliezer
>>>
>>> ----
>>> http://ngtech.co.il/lmgtfy/
>>> Linux System Administrator
>>> Mobile: +972-5-28704261
>>> Email: eliezer at ngtech.co.il
>>>
>>>
>>> From: squid-users [mailto:squid-users-bounces at lists.squid-cache.org]
>>> On Behalf Of Yuri Voinov
>>> Sent: Thursday, March 2, 2017 11:46 PM
>>> To: squid-users at lists.squid-cache.org
>>> Subject: Re: [squid-users] squid 3.5.2==> HTTPS FATAL: The ssl_crtd
>>> helpers are crashing too rapidly, need help!
>>>
>>> This problem, in principle, is common to all versions of ssl-bumped
>>> Squid from version 3.4 and 5.0, inclusive, and occurs when the
>>> stored certificate is damaged for any reason. The only thing
>>> vorkeraund that I could find - a monitor kesh.log and initialize the
>>> certificate database again with squid restart automatically.
>>> In some installations, this problem does not occur over the years.
>>> In other - almost daily. I have no desire to find out why this is
>>> happening exactly. For me it was easier to make the watchdog, which
>>> will follow up on this.
>>> 03.03.2017 3:40, Yuri Voinov ?????:
>>> One hint finally:
>>> '([^ ]*) helper database ([^ ]*) failed: The SSL certificate
>>> database ([^ ]*) is corrupted. Please rebuild' - - - 0    exec
>>> "/usr/local/bin/crtd_create.sh -r >/dev/null 2>&1"
>>> 'FATAL: ([^ ]*) helpers are crashing too rapidly, need help!' - - -
>>> 0    exec "/usr/local/bin/crtd_create.sh -r >/dev/null 2>&1"
>>> 'Cannot add certificate to db.' - - - 0        exec
>>> "/usr/local/bin/crtd_create.sh -r >/dev/null 2>&1"
>>> PS. This is from logsurfer.conf.
>>>
>>> 03.03.2017 3:34, Yuri Voinov ?????:
>>> This error is usually preceded by another error in cache.log
>>> associated with the certificates.
>>> I will show you the direction. Then go himself.
>>> This software will useful for you to solve:
>>> http://www.crypt.gen.nz/logsurfer/
>>> HTH, Yuri
>>>
>>> 03.03.2017 2:47, --Ahmad-- ?????:
>>> hey folks . 
>>> i have a problem with squid it get crashed after i enabled https !
>>> cache log error => FATAL: The ssl_crtd helpers are crashing too
>>> rapidly, need help!
>>>
>>> i googled many topics and relevant pages and couldnt find a clear
>>> solution .
>>>
>>> the quick solution i made was i  removed the certs in file :
>>> rm -rfv /var/lib/ssl_db/
>>>
>>>
>>> then reinitiated the DB using cmd below :
>>> /lib/squid/ssl_crtd -c -s /var/lib/ssl_db
>>> chown -R squid.squid /var/lib/ssl_db
>>> chown -R squid.squid /var/lib/ssl_db
>>>
>>>
>>> the restarted squid .
>>>
>>>
>>> but this is not a solution becuase squid get crashed again after
>>> certain time and i don?t know why !
>>> my version is 3.5.2
>>>
>>>
>>> here is squid.conf :
>>> /etc/squid/squid.conf
>>> visible_hostname pcloud
>>> acl ip1 myip 10.1.0.1
>>> acl ip2 myip 192.168.10.210
>>> tcp_outgoing_address 192.168.10.210 ip1
>>> tcp_outgoing_address 192.168.10.210 ip2
>>> #
>>> # Recommended minimum configuration:
>>> #
>>>
>>> # Example rule allowing access from your local networks.
>>> # Adapt to list your (internal) IP networks from where browsing
>>> # should be allowed
>>> acl localnet src 10.0.0.0/8     # RFC1918 possible internal network
>>> acl localnet src 172.16.0.0/12  # RFC1918 possible internal network
>>> acl localnet src 192.168.0.0/16 # RFC1918 possible internal network
>>> acl localnet src fc00::/7       # RFC 4193 local private network range
>>> acl localnet src fe80::/10      # RFC 4291 link-local (directly
>>> plugged) machines
>>>
>>> acl SSL_ports port 443
>>> acl Safe_ports port 80          # http
>>> acl Safe_ports port 21          # ftp
>>> acl Safe_ports port 443         # https
>>> acl Safe_ports port 70          # gopher
>>> acl Safe_ports port 210         # wais
>>> acl Safe_ports port 1025-65535  # unregistered ports
>>> acl Safe_ports port 280         # http-mgmt
>>> acl Safe_ports port 488         # gss-http
>>> acl Safe_ports port 591         # filemaker
>>> acl Safe_ports port 777         # multiling http
>>> acl CONNECT method CONNECT
>>>
>>> #
>>> # Recommended minimum Access Permission configuration:
>>> #
>>> # Deny requests to certain unsafe ports
>>> http_access deny !Safe_ports
>>>
>>> # Deny CONNECT to other than secure SSL ports
>>> http_access deny CONNECT !SSL_ports
>>> http_access allow  CONNECT 
>>> # Only allow cachemgr access from localhost
>>> http_access allow localhost manager
>>> http_access deny manager
>>>
>>> # We strongly recommend the following be uncommented to protect innocent
>>> # web applications running on the proxy server who think the only
>>> # one who can access services on "localhost" is a local user
>>> #http_access deny to_localhost
>>>
>>> #
>>> # INSERT YOUR OWN RULE(S) HERE TO ALLOW ACCESS FROM YOUR CLIENTS
>>> #
>>>
>>> # Example rule allowing access from your local networks.
>>> # Adapt localnet in the ACL section to list your (internal) IP networks
>>> # from where browsing should be allowed
>>> http_access allow localnet
>>> http_access allow localhost
>>>
>>> # And finally deny all other access to this proxy
>>> http_access deny all
>>>
>>> # Squid normally listens to port 3128
>>> http_port 3128
>>>
>>> # Uncomment and adjust the following to add a disk cache directory.
>>> #cache_dir ufs /var/cache/squid 100 16 256
>>>
>>> # Leave coredumps in the first cache dir
>>> #coredump_dir /var/cache/squid
>>>
>>> #
>>> # Add any of your own refresh_pattern entries above these.
>>> #
>>> #
>>>
>>> http_port 3126
>>> #http_port 3128
>>> #######################################
>>> #cache_swap_low 90
>>> #cache_swap_high 95
>>> ############################
>>> cache_effective_user squid
>>> cache_effective_group squid
>>> memory_replacement_policy lru
>>> cache_replacement_policy heap LFUDA
>>> ########################
>>> maximum_object_size 10000 MB
>>> #cache_mem 5000 MB
>>> maximum_object_size_in_memory 10 MB
>>> #########################
>>> logfile_rotate 2
>>> max_filedescriptors 131072
>>> ###############################
>>> ############
>>> cache_dir aufs /var/cache/squid 600000 64 128
>>> #######################################
>>> https_port 3129 intercept ssl-bump generate-host-certificates=on
>>> dynamic_cert_mem_cache_size=4MB
>>> cert=/usr/local/squid/ssl_cert/myca.pem
>>> key=/usr/local/squid/ssl_cert/myca.pem
>>> ssl_bump server-first all
>>> sslcrtd_program /lib/squid/ssl_crtd -s /var/lib/ssl_db -M 4MB
>>> sslcrtd_children 1000 startup=1 idle=1
>>> ###
>>> minimum_object_size 0 bytes
>>> #refresh patterns for caching static files
>>> refresh_pattern ^ftp: 1440 20% 10080
>>> refresh_pattern ^gopher: 1440 0% 1440
>>> refresh_pattern -i .(gif|png|jpg|jpeg|ico)$ 10080 90% 43200
>>> override-expire ignore-no-cache ignore-no-store ignore-private
>>> refresh_pattern -i .(iso|avi|wav|mp3|mp4|mpeg|swf|flv|x-flv)$ 43200
>>> 90% 432000 override-expire ignore-no-cache ignore-no-store
>>> ignore-private
>>> refresh_pattern -i
>>> .(deb|rpm|exe|zip|tar|tgz|ram|rar|bin|ppt|doc|tiff)$ 10080 90% 43200
>>> override-expire ignore-no-cache ignore-no-store ignore-private
>>> refresh_pattern -i .index.(html|htm)$ 0 40% 10080
>>> refresh_pattern -i .(html|htm|css|js)$ 1440 40% 40320
>>> refresh_pattern . 0 40% 40320
>>>
>>>
>>>
>>>
>>>
>>>
>>>
>>>
>>>
>>>
>>>
>>>
>>>
>>>
>>>
>>>
>>> any Joy Guys ?
>>>
>>>
>>> should i update squid ? or downgrade squid ?
>>>
>>>
>>> kind regards 
>>>
>>>
>>>
>>>
>>> _______________________________________________
>>> squid-users mailing list
>>> mailto:squid-users at lists.squid-cache.org
>>> http://lists.squid-cache.org/listinfo/squid-users
>>>
>>
>> -- 
>> Bugs to the Future
>> <0x613DEC46.asc>_______________________________________________
>> squid-users mailing list
>> squid-users at lists.squid-cache.org
>> <mailto:squid-users at lists.squid-cache.org>
>> http://lists.squid-cache.org/listinfo/squid-users
>

-- 
Bugs to the Future
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20170303/cfd6e483/attachment.htm>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: 0x613DEC46.asc
Type: application/pgp-keys
Size: 2437 bytes
Desc: not available
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20170303/cfd6e483/attachment.key>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: signature.asc
Type: application/pgp-signature
Size: 473 bytes
Desc: OpenPGP digital signature
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20170303/cfd6e483/attachment.sig>

From sothy.e98 at gmail.com  Fri Mar  3 13:09:37 2017
From: sothy.e98 at gmail.com (sothy shan)
Date: Fri, 3 Mar 2017 14:09:37 +0100
Subject: [squid-users] Reverse proxy with HTTPS
In-Reply-To: <20170303115920.GA28103@fantomas.sk>
References: <CAHcF_0ZdYzE_7Rc70Pi0BTbAwDYs=J_ZaTGjnYBPkhCaFZtDyQ@mail.gmail.com>
 <20170303115920.GA28103@fantomas.sk>
Message-ID: <CAHcF_0acuQ28rzE7sLVw1K+wa3LDUpsV+=VB7iTYDErKRRBKbQ@mail.gmail.com>

On Fri, Mar 3, 2017 at 12:59 PM, Matus UHLAR - fantomas <uhlar at fantomas.sk>
wrote:

> On 03.03.17 10:02, sothy shan wrote:
>
>> I am trying to test reverse proxy with HTTPS. For example, client makes
>> HTTPS request to squid server which make another HTTPS request to web
>> server.
>>
>
> what point does this have, except disabling client certificates?
>

Will it work as expect (i.e. reverse HTTPS Proxy) when I disable client
certificates?


> --
> Matus UHLAR - fantomas, uhlar at fantomas.sk ; http://www.fantomas.sk/
> Warning: I wish NOT to receive e-mail advertising to this address.
> Varovanie: na tuto adresu chcem NEDOSTAVAT akukolvek reklamnu postu.
> Honk if you love peace and quiet. ______________________________
> _________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20170303/79ac31b5/attachment.htm>

From eliezer at ngtech.co.il  Fri Mar  3 13:17:59 2017
From: eliezer at ngtech.co.il (Eliezer  Croitoru)
Date: Fri, 3 Mar 2017 15:17:59 +0200
Subject: [squid-users] squid 3.5.2==> HTTPS FATAL: The ssl_crtd helpers
	are crashing too rapidly, need help!
In-Reply-To: <4F85658B-A32E-4D74-98E6-E1CEE90168EA@netstream.ps>
References: <592CB65D-536B-40F8-B887-12127542DCB5@netstream.ps>
 <38a07e38-7347-313a-b27f-08fd7eddfa3f@gmail.com>
 <97215197-78fc-e987-9a9d-51a697010e7b@gmail.com>
 <714528e5-a6d5-e72a-2bc7-9950a8eecb73@gmail.com>
 <7c1d01d293b5$9b22d2c0$d1687840$@ngtech.co.il>
 <a5a63820-0afd-563b-f627-3819733a5680@gmail.com>
 <4F85658B-A32E-4D74-98E6-E1CEE90168EA@netstream.ps>
Message-ID: <7c6801d29420$9481bb00$bd853100$@ngtech.co.il>

Lets hope for the best.
When you will be with 3.5.24 we would be able to handle things in a simpler manner.
There are couple options to handle a situation like this and one of the options is to fence the ssl_crtd with some kind of lock mechanism for the DB rebuild time.
For example a bash script that will run the ssl_crtd and will try to trace if there is an exit code else then 0.
If such a case exists it will check for some lock file.
If it doesn't exit it will create it and will send a healing request to another daemon or will start the self healing by itself.
There are pros and cons but I believe that 3.5.24 will help more then many other options.

All The Bests,
Eliezer

----
http://ngtech.co.il/lmgtfy/
Linux System Administrator
Mobile: +972-5-28704261
Email: eliezer at ngtech.co.il


From: --Ahmad-- [mailto:ahmed.zaeem at netstream.ps] 
Sent: Friday, March 3, 2017 2:20 PM
To: Yuri Voinov <yvoinov at gmail.com>
Cc: Eliezer Croitoru <eliezer at ngtech.co.il>; squid-users at lists.squid-cache.org
Subject: Re: [squid-users] squid 3.5.2==> HTTPS FATAL: The ssl_crtd helpers are crashing too rapidly, need help!

@ eliezer 
i was using children as 10
ans faced the problem 


so i trued to increase children to 1000 to see if this was the reason 
and unfortunately the same problem .

ys I?m using debian 6 os .

i appreciable the helping from all the replies below but so far i havent got any clear solution .

now i updated to 3.5.24 last one .
and will see it if comes back ?i will update the list with result .

if it failed ? I?m forced to create cron job to remove the certs like every 24 hours .

thank you  guys all of you .
thanks amos , thanks eliezer , thanks yuri

kind regards
On Mar 3, 2017, at 1:37 PM, Yuri Voinov <mailto:yvoinov at gmail.com> wrote:



03.03.2017 6:32, Eliezer Croitoru ?????:

Hey Yuri,

This issue is not 100% squid but I think it's related to the way ssl_crtd works.
I am not sure if it has some locking or other things to prevent such issues.
The first solution is to somehow defend the DB from corruption, like in a case that more then a dozen identical requests are being done towards a single site and two ssl_crtd helpers are trying to do the same things.
I believe that something to fence this should already be inside squid and ssl_crtd but I am pretty sure this is the main issue.
I suggests this can be external reason to occurs this issue. Somehow,
for example, BlueCoat on ISP upstream, tcp packets corruption, etc. I
dont know, just guessing.

Alex and his team should know the answer for this subject and if I'm not wrong theoretically there are couple ways to prevent the mentioned issues.
I had a plan to try and understand the ssl_crtd code and interface but yet to do so.

I hope this issue will be resolved in a way that it can be backported to 3.5 in the worst case.
I hope too, but if it external..... fewwwwwwwwwww.

Anyway, watchdog is good backup to preventing manual interventions by SA.


Eliezer

----
http://ngtech.co.il/lmgtfy/
Linux System Administrator
Mobile: +972-5-28704261
Email: mailto:eliezer at ngtech.co.il


From: squid-users [mailto:squid-users-bounces at lists.squid-cache.org] On Behalf Of Yuri Voinov
Sent: Thursday, March 2, 2017 11:46 PM
To: mailto:squid-users at lists.squid-cache.org
Subject: Re: [squid-users] squid 3.5.2==> HTTPS FATAL: The ssl_crtd helpers are crashing too rapidly, need help!

This problem, in principle, is common to all versions of ssl-bumped Squid from version 3.4 and 5.0, inclusive, and occurs when the stored certificate is damaged for any reason. The only thing vorkeraund that I could find - a monitor kesh.log and initialize the certificate database again with squid restart automatically.
In some installations, this problem does not occur over the years. In other - almost daily. I have no desire to find out why this is happening exactly. For me it was easier to make the watchdog, which will follow up on this.
03.03.2017 3:40, Yuri Voinov ?????:
One hint finally:
'([^ ]*) helper database ([^ ]*) failed: The SSL certificate database ([^ ]*) is corrupted. Please rebuild' - - - 0    exec "/usr/local/bin/crtd_create.sh -r >/dev/null 2>&1"
'FATAL: ([^ ]*) helpers are crashing too rapidly, need help!' - - - 0    exec "/usr/local/bin/crtd_create.sh -r >/dev/null 2>&1"
'Cannot add certificate to db.' - - - 0        exec "/usr/local/bin/crtd_create.sh -r >/dev/null 2>&1"
PS. This is from logsurfer.conf.

03.03.2017 3:34, Yuri Voinov ?????:
This error is usually preceded by another error in cache.log associated with the certificates.
I will show you the direction. Then go himself.
This software will useful for you to solve:
http://www.crypt.gen.nz/logsurfer/
HTH, Yuri

03.03.2017 2:47, --Ahmad-- ?????:
hey folks . 
i have a problem with squid it get crashed after i enabled https !
cache log error => FATAL: The ssl_crtd helpers are crashing too rapidly, need help!

i googled many topics and relevant pages and couldnt find a clear solution .

the quick solution i made was i  removed the certs in file :
rm -rfv /var/lib/ssl_db/


then reinitiated the DB using cmd below :
/lib/squid/ssl_crtd -c -s /var/lib/ssl_db
chown -R squid.squid /var/lib/ssl_db
chown -R squid.squid /var/lib/ssl_db


the restarted squid .


but this is not a solution becuase squid get crashed again after certain time and i don?t know why !
my version is 3.5.2


here is squid.conf :
/etc/squid/squid.conf
visible_hostname pcloud
acl ip1 myip 10.1.0.1
acl ip2 myip 192.168.10.210
tcp_outgoing_address 192.168.10.210 ip1
tcp_outgoing_address 192.168.10.210 ip2
#
# Recommended minimum configuration:
#

# Example rule allowing access from your local networks.
# Adapt to list your (internal) IP networks from where browsing
# should be allowed
acl localnet src 10.0.0.0/8     # RFC1918 possible internal network
acl localnet src 172.16.0.0/12  # RFC1918 possible internal network
acl localnet src 192.168.0.0/16 # RFC1918 possible internal network
acl localnet src fc00::/7       # RFC 4193 local private network range
acl localnet src fe80::/10      # RFC 4291 link-local (directly plugged) machines

acl SSL_ports port 443
acl Safe_ports port 80          # http
acl Safe_ports port 21          # ftp
acl Safe_ports port 443         # https
acl Safe_ports port 70          # gopher
acl Safe_ports port 210         # wais
acl Safe_ports port 1025-65535  # unregistered ports
acl Safe_ports port 280         # http-mgmt
acl Safe_ports port 488         # gss-http
acl Safe_ports port 591         # filemaker
acl Safe_ports port 777         # multiling http
acl CONNECT method CONNECT

#
# Recommended minimum Access Permission configuration:
#
# Deny requests to certain unsafe ports
http_access deny !Safe_ports

# Deny CONNECT to other than secure SSL ports
http_access deny CONNECT !SSL_ports
http_access allow  CONNECT 
# Only allow cachemgr access from localhost
http_access allow localhost manager
http_access deny manager

# We strongly recommend the following be uncommented to protect innocent
# web applications running on the proxy server who think the only
# one who can access services on "localhost" is a local user
#http_access deny to_localhost

#
# INSERT YOUR OWN RULE(S) HERE TO ALLOW ACCESS FROM YOUR CLIENTS
#

# Example rule allowing access from your local networks.
# Adapt localnet in the ACL section to list your (internal) IP networks
# from where browsing should be allowed
http_access allow localnet
http_access allow localhost

# And finally deny all other access to this proxy
http_access deny all

# Squid normally listens to port 3128
http_port 3128

# Uncomment and adjust the following to add a disk cache directory.
#cache_dir ufs /var/cache/squid 100 16 256

# Leave coredumps in the first cache dir
#coredump_dir /var/cache/squid

#
# Add any of your own refresh_pattern entries above these.
#
#

http_port 3126
#http_port 3128
#######################################
#cache_swap_low 90
#cache_swap_high 95
############################
cache_effective_user squid
cache_effective_group squid
memory_replacement_policy lru
cache_replacement_policy heap LFUDA
########################
maximum_object_size 10000 MB
#cache_mem 5000 MB
maximum_object_size_in_memory 10 MB
#########################
logfile_rotate 2
max_filedescriptors 131072
###############################
############
cache_dir aufs /var/cache/squid 600000 64 128
#######################################
https_port 3129 intercept ssl-bump generate-host-certificates=on dynamic_cert_mem_cache_size=4MB cert=/usr/local/squid/ssl_cert/myca.pem key=/usr/local/squid/ssl_cert/myca.pem
ssl_bump server-first all
sslcrtd_program /lib/squid/ssl_crtd -s /var/lib/ssl_db -M 4MB
sslcrtd_children 1000 startup=1 idle=1
###
minimum_object_size 0 bytes
#refresh patterns for caching static files
refresh_pattern ^ftp: 1440 20% 10080
refresh_pattern ^gopher: 1440 0% 1440
refresh_pattern -i .(gif|png|jpg|jpeg|ico)$ 10080 90% 43200 override-expire ignore-no-cache ignore-no-store ignore-private
refresh_pattern -i .(iso|avi|wav|mp3|mp4|mpeg|swf|flv|x-flv)$ 43200 90% 432000 override-expire ignore-no-cache ignore-no-store ignore-private
refresh_pattern -i .(deb|rpm|exe|zip|tar|tgz|ram|rar|bin|ppt|doc|tiff)$ 10080 90% 43200 override-expire ignore-no-cache ignore-no-store ignore-private
refresh_pattern -i .index.(html|htm)$ 0 40% 10080
refresh_pattern -i .(html|htm|css|js)$ 1440 40% 40320
refresh_pattern . 0 40% 40320
















any Joy Guys ?


should i update squid ? or downgrade squid ?


kind regards 




_______________________________________________
squid-users mailing list
mailto:squid-users at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-users

-- 
Bugs to the Future
<0x613DEC46.asc>_______________________________________________
squid-users mailing list
mailto:squid-users at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-users




From uhlar at fantomas.sk  Fri Mar  3 13:56:31 2017
From: uhlar at fantomas.sk (Matus UHLAR - fantomas)
Date: Fri, 3 Mar 2017 14:56:31 +0100
Subject: [squid-users] Reverse proxy with HTTPS
In-Reply-To: <CAHcF_0acuQ28rzE7sLVw1K+wa3LDUpsV+=VB7iTYDErKRRBKbQ@mail.gmail.com>
References: <CAHcF_0ZdYzE_7Rc70Pi0BTbAwDYs=J_ZaTGjnYBPkhCaFZtDyQ@mail.gmail.com>
 <20170303115920.GA28103@fantomas.sk>
 <CAHcF_0acuQ28rzE7sLVw1K+wa3LDUpsV+=VB7iTYDErKRRBKbQ@mail.gmail.com>
Message-ID: <20170303135631.GC28103@fantomas.sk>

>> On 03.03.17 10:02, sothy shan wrote:
>>> I am trying to test reverse proxy with HTTPS. For example, client makes
>>> HTTPS request to squid server which make another HTTPS request to web
>>> server.

>On Fri, Mar 3, 2017 at 12:59 PM, Matus UHLAR - fantomas <uhlar at fantomas.sk>
>wrote:
>> what point does this have, except disabling client certificates?

On 03.03.17 14:09, sothy shan wrote:
>Will it work as expect (i.e. reverse HTTPS Proxy) when I disable client
>certificates?

It should work even without disabling client certificates, it just makes
little sense.

Talking to servers using HTTP and thus behaving like SSL accelerator makes
sense.

Behaving like caching accelerator while using SSL on both sides makes little
sense, of course depending on cacheability of the content.

-- 
Matus UHLAR - fantomas, uhlar at fantomas.sk ; http://www.fantomas.sk/
Warning: I wish NOT to receive e-mail advertising to this address.
Varovanie: na tuto adresu chcem NEDOSTAVAT akukolvek reklamnu postu.
Nothing is fool-proof to a talented fool. 


From sothy.e98 at gmail.com  Fri Mar  3 14:53:23 2017
From: sothy.e98 at gmail.com (sothy shan)
Date: Fri, 3 Mar 2017 15:53:23 +0100
Subject: [squid-users] Reverse proxy with HTTPS
In-Reply-To: <20170303135631.GC28103@fantomas.sk>
References: <CAHcF_0ZdYzE_7Rc70Pi0BTbAwDYs=J_ZaTGjnYBPkhCaFZtDyQ@mail.gmail.com>
 <20170303115920.GA28103@fantomas.sk>
 <CAHcF_0acuQ28rzE7sLVw1K+wa3LDUpsV+=VB7iTYDErKRRBKbQ@mail.gmail.com>
 <20170303135631.GC28103@fantomas.sk>
Message-ID: <CAHcF_0a=0yyNLGbHyWSU-xPmANDUu7k=8mrtvfWaBcSPsEM8wQ@mail.gmail.com>

On Fri, Mar 3, 2017 at 2:56 PM, Matus UHLAR - fantomas <uhlar at fantomas.sk>
wrote:

> On 03.03.17 10:02, sothy shan wrote:
>>>
>>>> I am trying to test reverse proxy with HTTPS. For example, client makes
>>>> HTTPS request to squid server which make another HTTPS request to web
>>>> server.
>>>>
>>>
> On Fri, Mar 3, 2017 at 12:59 PM, Matus UHLAR - fantomas <uhlar at fantomas.sk
>> >
>> wrote:
>>
>>> what point does this have, except disabling client certificates?
>>>
>>
> On 03.03.17 14:09, sothy shan wrote:
>
>> Will it work as expect (i.e. reverse HTTPS Proxy) when I disable client
>> certificates?
>>
>
> It should work even without disabling client certificates, it just makes
> little sense.
>
> Talking to servers using HTTP and thus behaving like SSL accelerator makes
> sense.
>
> Behaving like caching accelerator while using SSL on both sides makes
> little
> sense, of course depending on cacheability of the content.
>

In order to check first rever proxy with HTTP, I am using squid-4.0.18.
The following changes are added into /etc/squid/squid.conf
+++++++++++++++++++++++++++++++++++++++++++++++++++++++

http_port 192.168.1.69:80 accel defaultsite=www.xxxx.fr
cache_peer X.Y.W.Z parent 80 0 no-query originserver name=myAccel

acl our_sites dstdomain www.lemonde.fr
http_access allow our_sites
cache_peer_acces myAccel allow our_sites
cache_peer_access myAccel deny all

+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

After that I am manually running
$sudo squid -d 2 -a 80 -N -X

Squid is not running and stopped in the middle without any error msg.

Do you see where is problem?

Best regards
Sothy


> --
> Matus UHLAR - fantomas, uhlar at fantomas.sk ; http://www.fantomas.sk/
> Warning: I wish NOT to receive e-mail advertising to this address.
> Varovanie: na tuto adresu chcem NEDOSTAVAT akukolvek reklamnu postu.
> Nothing is fool-proof to a talented fool. ______________________________
> _________________
>
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20170303/7330f4bf/attachment.htm>

From rousskov at measurement-factory.com  Fri Mar  3 15:29:10 2017
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Fri, 3 Mar 2017 08:29:10 -0700
Subject: [squid-users] Reverse proxy with HTTPS
In-Reply-To: <20170303115920.GA28103@fantomas.sk>
References: <CAHcF_0ZdYzE_7Rc70Pi0BTbAwDYs=J_ZaTGjnYBPkhCaFZtDyQ@mail.gmail.com>
 <20170303115920.GA28103@fantomas.sk>
Message-ID: <97ab1200-347b-6390-ea06-23bafb6b8aba@measurement-factory.com>

On 03/03/2017 04:59 AM, Matus UHLAR - fantomas wrote:
> On 03.03.17 10:02, sothy shan wrote:
>> I am trying to test reverse proxy with HTTPS. For example, client makes
>> HTTPS request to squid server which make another HTTPS request to web
>> server.

> what point does this have, except disabling client certificates?

This setup may be useful for several reasons, including:

* caching
* access controls
* content adaptation
* logging
* using different encryption policies with clients and servers

Alex.



From sothy.e98 at gmail.com  Fri Mar  3 15:41:37 2017
From: sothy.e98 at gmail.com (sothy shan)
Date: Fri, 3 Mar 2017 16:41:37 +0100
Subject: [squid-users] Reverse proxy with HTTPS
In-Reply-To: <97ab1200-347b-6390-ea06-23bafb6b8aba@measurement-factory.com>
References: <CAHcF_0ZdYzE_7Rc70Pi0BTbAwDYs=J_ZaTGjnYBPkhCaFZtDyQ@mail.gmail.com>
 <20170303115920.GA28103@fantomas.sk>
 <97ab1200-347b-6390-ea06-23bafb6b8aba@measurement-factory.com>
Message-ID: <CAHcF_0bJsp0rmxZ1dXSJKa9XrsWKN97YJ39-Jv+BEd4AP6nSgQ@mail.gmail.com>

On Fri, Mar 3, 2017 at 4:29 PM, Alex Rousskov <
rousskov at measurement-factory.com> wrote:

> On 03/03/2017 04:59 AM, Matus UHLAR - fantomas wrote:
> > On 03.03.17 10:02, sothy shan wrote:
> >> I am trying to test reverse proxy with HTTPS. For example, client makes
> >> HTTPS request to squid server which make another HTTPS request to web
> >> server.
>
> > what point does this have, except disabling client certificates?
>
> This setup may be useful for several reasons, including:
>
> * caching
>
My question is caching will work during reverse HTTPS proxy? I just want
configuration informaton squid-4.0.5?

> * access controls
> * content adaptation
> * logging
> * using different encryption policies with clients and servers
>
> Alex.
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20170303/a4918ffe/attachment.htm>

From squid3 at treenet.co.nz  Fri Mar  3 15:55:20 2017
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sat, 4 Mar 2017 04:55:20 +1300
Subject: [squid-users] Reverse proxy with HTTPS
In-Reply-To: <CAHcF_0a=0yyNLGbHyWSU-xPmANDUu7k=8mrtvfWaBcSPsEM8wQ@mail.gmail.com>
References: <CAHcF_0ZdYzE_7Rc70Pi0BTbAwDYs=J_ZaTGjnYBPkhCaFZtDyQ@mail.gmail.com>
 <20170303115920.GA28103@fantomas.sk>
 <CAHcF_0acuQ28rzE7sLVw1K+wa3LDUpsV+=VB7iTYDErKRRBKbQ@mail.gmail.com>
 <20170303135631.GC28103@fantomas.sk>
 <CAHcF_0a=0yyNLGbHyWSU-xPmANDUu7k=8mrtvfWaBcSPsEM8wQ@mail.gmail.com>
Message-ID: <c4e9e868-98fe-48f4-9561-ee2a6d4240ed@treenet.co.nz>

On 4/03/2017 3:53 a.m., sothy shan wrote:
> On Fri, Mar 3, 2017 at 2:56 PM, Matus UHLAR - fantomas <uhlar at fantomas.sk>
> wrote:
> 
>> On 03.03.17 10:02, sothy shan wrote:
>>>>
>>>>> I am trying to test reverse proxy with HTTPS. For example, client makes
>>>>> HTTPS request to squid server which make another HTTPS request to web
>>>>> server.
>>>>>
>>>>
>> On Fri, Mar 3, 2017 at 12:59 PM, Matus UHLAR - fantomas <uhlar at fantomas.sk
>>>>
>>> wrote:
>>>
>>>> what point does this have, except disabling client certificates?
>>>>
>>>
>> On 03.03.17 14:09, sothy shan wrote:
>>
>>> Will it work as expect (i.e. reverse HTTPS Proxy) when I disable client
>>> certificates?
>>>
>>
>> It should work even without disabling client certificates, it just makes
>> little sense.
>>
>> Talking to servers using HTTP and thus behaving like SSL accelerator makes
>> sense.
>>
>> Behaving like caching accelerator while using SSL on both sides makes
>> little
>> sense, of course depending on cacheability of the content.
>>
> 
> In order to check first rever proxy with HTTP, I am using squid-4.0.18.
> The following changes are added into /etc/squid/squid.conf
> +++++++++++++++++++++++++++++++++++++++++++++++++++++++
> 
> http_port 192.168.1.69:80 accel defaultsite=www.xxxx.fr
> cache_peer X.Y.W.Z parent 80 0 no-query originserver name=myAccel
> 
> acl our_sites dstdomain www.lemonde.fr
> http_access allow our_sites
> cache_peer_acces myAccel allow our_sites
> cache_peer_access myAccel deny all
> 
> +++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
> 
> After that I am manually running
> $sudo squid -d 2 -a 80 -N -X
> 
> Squid is not running and stopped in the middle without any error msg.
> 

I very much doubt there is *no* error message. Much more likely that it
is just being sent to a place you are not noticing.

> Do you see where is problem?

You have told Squid to open port 80 on *all* IPs of the machine for
generic proxy traffic (command line parameter "-a 80"), then you have
told Squid to *also* open port 80 on IP 192.168.1.69 for reverse-proxy
traffic.

You cannot open a IP:port twice. Not even with the same application.

Amos



From rousskov at measurement-factory.com  Fri Mar  3 15:56:04 2017
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Fri, 3 Mar 2017 08:56:04 -0700
Subject: [squid-users] squid 3.5.2==> HTTPS FATAL: The ssl_crtd helpers
 are crashing too rapidly, need help!
In-Reply-To: <7c6801d29420$9481bb00$bd853100$@ngtech.co.il>
References: <592CB65D-536B-40F8-B887-12127542DCB5@netstream.ps>
 <38a07e38-7347-313a-b27f-08fd7eddfa3f@gmail.com>
 <97215197-78fc-e987-9a9d-51a697010e7b@gmail.com>
 <714528e5-a6d5-e72a-2bc7-9950a8eecb73@gmail.com>
 <7c1d01d293b5$9b22d2c0$d1687840$@ngtech.co.il>
 <a5a63820-0afd-563b-f627-3819733a5680@gmail.com>
 <4F85658B-A32E-4D74-98E6-E1CEE90168EA@netstream.ps>
 <7c6801d29420$9481bb00$bd853100$@ngtech.co.il>
Message-ID: <b6eee0af-ea58-105b-f6f6-dd6f25c58277@measurement-factory.com>

On 03/03/2017 06:17 AM, Eliezer Croitoru wrote:

> one of the options is to fence the ssl_crtd with some kind of lock
> mechanism for the DB rebuild time.

ssl_crtd already has a lock mechanism. If that mechanism is buggy, it
needs to be fixed, but it does not make sense to add another one.

There is still a lot of room for improvements, of course. For example,
compared to a log-monitoring watchdog mentioned by Yuri:

* Teaching ssl_crtd to run a sysadmin-provided script on database
failures will allow the sysadmin to handle such failures almost
transparently to the users and may reduce configuration headaches
associated with ssl_crtd message text changes.

* Teaching Squid to run a sysadmin-provided script on helper crashes
will allow the sysadmin to handle such failures more transparently to
the users and may reduce configuration headaches associated with helper
message text changes.


Alex.



From sothy.e98 at gmail.com  Fri Mar  3 15:58:05 2017
From: sothy.e98 at gmail.com (sothy shan)
Date: Fri, 3 Mar 2017 16:58:05 +0100
Subject: [squid-users] Reverse proxy with HTTPS
In-Reply-To: <c4e9e868-98fe-48f4-9561-ee2a6d4240ed@treenet.co.nz>
References: <CAHcF_0ZdYzE_7Rc70Pi0BTbAwDYs=J_ZaTGjnYBPkhCaFZtDyQ@mail.gmail.com>
 <20170303115920.GA28103@fantomas.sk>
 <CAHcF_0acuQ28rzE7sLVw1K+wa3LDUpsV+=VB7iTYDErKRRBKbQ@mail.gmail.com>
 <20170303135631.GC28103@fantomas.sk>
 <CAHcF_0a=0yyNLGbHyWSU-xPmANDUu7k=8mrtvfWaBcSPsEM8wQ@mail.gmail.com>
 <c4e9e868-98fe-48f4-9561-ee2a6d4240ed@treenet.co.nz>
Message-ID: <CAHcF_0bVE4y70adxEkJhJjwjdL8+GQiy7-HbvWVm1a7pL534+A@mail.gmail.com>

On Fri, Mar 3, 2017 at 4:55 PM, Amos Jeffries <squid3 at treenet.co.nz> wrote:

> On 4/03/2017 3:53 a.m., sothy shan wrote:
> > On Fri, Mar 3, 2017 at 2:56 PM, Matus UHLAR - fantomas <
> uhlar at fantomas.sk>
> > wrote:
> >
> >> On 03.03.17 10:02, sothy shan wrote:
> >>>>
> >>>>> I am trying to test reverse proxy with HTTPS. For example, client
> makes
> >>>>> HTTPS request to squid server which make another HTTPS request to web
> >>>>> server.
> >>>>>
> >>>>
> >> On Fri, Mar 3, 2017 at 12:59 PM, Matus UHLAR - fantomas <
> uhlar at fantomas.sk
> >>>>
> >>> wrote:
> >>>
> >>>> what point does this have, except disabling client certificates?
> >>>>
> >>>
> >> On 03.03.17 14:09, sothy shan wrote:
> >>
> >>> Will it work as expect (i.e. reverse HTTPS Proxy) when I disable client
> >>> certificates?
> >>>
> >>
> >> It should work even without disabling client certificates, it just makes
> >> little sense.
> >>
> >> Talking to servers using HTTP and thus behaving like SSL accelerator
> makes
> >> sense.
> >>
> >> Behaving like caching accelerator while using SSL on both sides makes
> >> little
> >> sense, of course depending on cacheability of the content.
> >>
> >
> > In order to check first rever proxy with HTTP, I am using squid-4.0.18.
> > The following changes are added into /etc/squid/squid.conf
> > +++++++++++++++++++++++++++++++++++++++++++++++++++++++
> >
> > http_port 192.168.1.69:80 accel defaultsite=www.xxxx.fr
> > cache_peer X.Y.W.Z parent 80 0 no-query originserver name=myAccel
> >
> > acl our_sites dstdomain www.lemonde.fr
> > http_access allow our_sites
> > cache_peer_acces myAccel allow our_sites
> > cache_peer_access myAccel deny all
> >
> > +++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
> >
> > After that I am manually running
> > $sudo squid -d 2 -a 80 -N -X
> >
> > Squid is not running and stopped in the middle without any error msg.
> >
>
> I very much doubt there is *no* error message. Much more likely that it
> is just being sent to a place you are not noticing.
>
> > Do you see where is problem?
>
> You have told Squid to open port 80 on *all* IPs of the machine for
> generic proxy traffic (command line parameter "-a 80"), then you have
> told Squid to *also* open port 80 on IP 192.168.1.69 for reverse-proxy
> traffic.
>
> You cannot open a IP:port twice. Not even with the same application.
>

I changed the configuration
++++++++++++++++++++++++++++++++++++
http_port 192.168.1.69:80 accel defaultsite=www.AAAAA.com
cache_peer 192.168.1.31 parent 80 0 no-query originserver


http_access allow all
 ++++++++++++++++++++++++++++++++++++++++++
It worked well now for HTTP reverse proxy.

> Amos
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20170303/ae9a20e8/attachment.htm>

From squid3 at treenet.co.nz  Fri Mar  3 16:06:58 2017
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sat, 4 Mar 2017 05:06:58 +1300
Subject: [squid-users] Reverse proxy with HTTPS
In-Reply-To: <CAHcF_0bVE4y70adxEkJhJjwjdL8+GQiy7-HbvWVm1a7pL534+A@mail.gmail.com>
References: <CAHcF_0ZdYzE_7Rc70Pi0BTbAwDYs=J_ZaTGjnYBPkhCaFZtDyQ@mail.gmail.com>
 <20170303115920.GA28103@fantomas.sk>
 <CAHcF_0acuQ28rzE7sLVw1K+wa3LDUpsV+=VB7iTYDErKRRBKbQ@mail.gmail.com>
 <20170303135631.GC28103@fantomas.sk>
 <CAHcF_0a=0yyNLGbHyWSU-xPmANDUu7k=8mrtvfWaBcSPsEM8wQ@mail.gmail.com>
 <c4e9e868-98fe-48f4-9561-ee2a6d4240ed@treenet.co.nz>
 <CAHcF_0bVE4y70adxEkJhJjwjdL8+GQiy7-HbvWVm1a7pL534+A@mail.gmail.com>
Message-ID: <8c25c345-0cea-40c7-0fd4-2e20859c0e26@treenet.co.nz>

On 4/03/2017 4:58 a.m., sothy shan wrote:
> I changed the configuration
> ++++++++++++++++++++++++++++++++++++
> http_port 192.168.1.69:80 accel defaultsite=www.AAAAA.com
> cache_peer 192.168.1.31 parent 80 0 no-query originserver
> 
> 
> http_access allow all
>  ++++++++++++++++++++++++++++++++++++++++++
> It worked well now for HTTP reverse proxy.

"allow all" is *BAD*. Your server just delivered successful relayed
responses when I asked it for google.com, example.com and some other
domains which do not belong to you.
 It is an open-proxy, not a reverse-proxy.

You should know what domains your system is serving and keep the
dstdomain ACL to allow only that traffic through the proxy.

My point earlier was that you need to choose your method of configuring
the Squid ports. Either use the -a command option, or http_port. Do not
use both for the same port number.
 I suggest removing the -a use, since it cannot be used to configure
reverse-proxy port options.

Amos



From rousskov at measurement-factory.com  Fri Mar  3 16:23:55 2017
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Fri, 3 Mar 2017 09:23:55 -0700
Subject: [squid-users] Reverse proxy with HTTPS
In-Reply-To: <CAHcF_0bJsp0rmxZ1dXSJKa9XrsWKN97YJ39-Jv+BEd4AP6nSgQ@mail.gmail.com>
References: <CAHcF_0ZdYzE_7Rc70Pi0BTbAwDYs=J_ZaTGjnYBPkhCaFZtDyQ@mail.gmail.com>
 <20170303115920.GA28103@fantomas.sk>
 <97ab1200-347b-6390-ea06-23bafb6b8aba@measurement-factory.com>
 <CAHcF_0bJsp0rmxZ1dXSJKa9XrsWKN97YJ39-Jv+BEd4AP6nSgQ@mail.gmail.com>
Message-ID: <fde0a1f9-89c3-959f-29fb-729cc3de5846@measurement-factory.com>

On 03/03/2017 08:41 AM, sothy shan wrote:

> On Fri, Mar 3, 2017 at 4:29 PM, Alex Rousskov wrote:
> 
>     On 03/03/2017 04:59 AM, Matus UHLAR - fantomas wrote:
>     > On 03.03.17 10:02, sothy shan wrote:
>     >> I am trying to test reverse proxy with HTTPS. For example, client makes
>     >> HTTPS request to squid server which make another HTTPS request to web
>     >> server.
> 
>     > what point does this have, except disabling client certificates?
> 
>     This setup may be useful for several reasons, including:
> 
>     * caching
> 
> My question is caching will work during reverse HTTPS proxy?

Yes, it should work.


>     * access controls
>     * content adaptation
>     * logging
>     * using different encryption policies with clients and servers

Alex.



From carlopmart at gmail.com  Fri Mar  3 21:29:29 2017
From: carlopmart at gmail.com (C. L. Martinez)
Date: Fri, 3 Mar 2017 21:29:29 +0000
Subject: [squid-users] Squid with SSL-Bump on Debian testing:
	SSL_ERROR_RX_RECORD_TOO_LONG
Message-ID: <20170303212929.4mxju5jziye3mo2a@scotland.uxdom.org>

Hi all,

 After installing Squid 3.5.24 in my Debian testing (many thanks Amos for your help), I am trying to configure Squid as https intercept proxy. My config actually is:

http_port 127.0.0.1:8080
http_port 127.0.0.1:8081 intercept
http_port 127.0.0.1:8082 ssl-bump cert=/opt/squid/etc/certs/myCA.pem generate-host-certificates=on \
	dynamic_cert_mem_cache_size=4MB tls-dh=/opt/squid/etc/certs/dhparam.pem
https_port 127.0.0.1:8083 ssl-bump intercept cert=/opt/squid/etc/certs/myCA.pem generate-host-certificates=on \
	dynamic_cert_mem_cache_size=4MB tls-dh=/opt/squid/etc/certs/dhparam.pem
sslcrtd_program /opt/squid/libexec/ssl_crtd -s /var/squid/ssldb -M 4MB

# SSL-Bump
acl step1 at_step SslBump1
acl step2 at_step SslBump2
acl step3 at_step SslBump3
ssl_bump splice localhost
acl exclude_sites ssl::server_name_regex -i "/usr/local/etc/squid/doms.nobump"
ssl_bump peek step1 all
ssl_bump splice exclude_sites
ssl_bump stare step2 all
ssl_bump bump all

 Content of "/usr/local/etc/squid/doms.nobump" is:

update\.microsoft\.com$
update\.microsoft\.com\.akadns\.net$

 But every time I have receiving Error code: SSL_ERROR_RX_RECORD_TOO_LONG in Firefox's browsers when I visit any web using https like https://www.debian.org, https://www.redhat.com, etc.. Some time ago, I have setup same config under OpenBSD and all works ok.

 Where am I doing the mistake?
-- 
Greetings,
C. L. Martinez


From yvoinov at gmail.com  Fri Mar  3 22:21:19 2017
From: yvoinov at gmail.com (Yuri Voinov)
Date: Sat, 4 Mar 2017 04:21:19 +0600
Subject: [squid-users] Squid with SSL-Bump on Debian testing:
 SSL_ERROR_RX_RECORD_TOO_LONG
In-Reply-To: <20170303212929.4mxju5jziye3mo2a@scotland.uxdom.org>
References: <20170303212929.4mxju5jziye3mo2a@scotland.uxdom.org>
Message-ID: <5e65eff2-9976-0e0e-95ce-b602635e347d@gmail.com>



04.03.2017 3:29, C. L. Martinez ?????:
> Hi all,
>
>  After installing Squid 3.5.24 in my Debian testing (many thanks Amos for your help), I am trying to configure Squid as https intercept proxy. My config actually is:
>
> http_port 127.0.0.1:8080
> http_port 127.0.0.1:8081 intercept
> http_port 127.0.0.1:8082 ssl-bump cert=/opt/squid/etc/certs/myCA.pem generate-host-certificates=on \
> 	dynamic_cert_mem_cache_size=4MB tls-dh=/opt/squid/etc/certs/dhparam.pem
> https_port 127.0.0.1:8083 ssl-bump intercept cert=/opt/squid/etc/certs/myCA.pem generate-host-certificates=on \
> 	dynamic_cert_mem_cache_size=4MB tls-dh=/opt/squid/etc/certs/dhparam.pem
> sslcrtd_program /opt/squid/libexec/ssl_crtd -s /var/squid/ssldb -M 4MB
>
> # SSL-Bump
> acl step1 at_step SslBump1
> acl step2 at_step SslBump2
> acl step3 at_step SslBump3
> ssl_bump splice localhost
> acl exclude_sites ssl::server_name_regex -i "/usr/local/etc/squid/doms.nobump"
> ssl_bump peek step1 all
> ssl_bump splice exclude_sites
> ssl_bump stare step2 all
> ssl_bump bump all
>
>  Content of "/usr/local/etc/squid/doms.nobump" is:
>
> update\.microsoft\.com$
> update\.microsoft\.com\.akadns\.net$
>
>  But every time I have receiving Error code: SSL_ERROR_RX_RECORD_TOO_LONG in Firefox's browsers when I visit any web using https like https://www.debian.org, https://www.redhat.com, etc.. Some time ago, I have setup same config under OpenBSD and all works ok.
>
>  Where am I doing the mistake?
Hardly this is mistake. Most probably this is platform-specific
non-squid bug.
-- 
Bugs to the Future
-------------- next part --------------
A non-text attachment was scrubbed...
Name: 0x613DEC46.asc
Type: application/pgp-keys
Size: 2437 bytes
Desc: not available
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20170304/a85fce18/attachment.key>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: signature.asc
Type: application/pgp-signature
Size: 473 bytes
Desc: OpenPGP digital signature
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20170304/a85fce18/attachment.sig>

From carlopmart at gmail.com  Sat Mar  4 08:24:32 2017
From: carlopmart at gmail.com (C. L. Martinez)
Date: Sat, 4 Mar 2017 08:24:32 +0000
Subject: [squid-users] Squid with SSL-Bump on Debian testing:
 SSL_ERROR_RX_RECORD_TOO_LONG
In-Reply-To: <5e65eff2-9976-0e0e-95ce-b602635e347d@gmail.com>
References: <20170303212929.4mxju5jziye3mo2a@scotland.uxdom.org>
 <5e65eff2-9976-0e0e-95ce-b602635e347d@gmail.com>
Message-ID: <20170304082432.fw7w5froa2rjwuvj@scotland.uxdom.org>

On Sat, Mar 04, 2017 at 04:21:19AM +0600, Yuri Voinov wrote:
> 
> 
> 04.03.2017 3:29, C. L. Martinez ?????:
> > Hi all,
> >
> >  After installing Squid 3.5.24 in my Debian testing (many thanks Amos for your help), I am trying to configure Squid as https intercept proxy. My config actually is:
> >
> > http_port 127.0.0.1:8080
> > http_port 127.0.0.1:8081 intercept
> > http_port 127.0.0.1:8082 ssl-bump cert=/opt/squid/etc/certs/myCA.pem generate-host-certificates=on \
> > 	dynamic_cert_mem_cache_size=4MB tls-dh=/opt/squid/etc/certs/dhparam.pem
> > https_port 127.0.0.1:8083 ssl-bump intercept cert=/opt/squid/etc/certs/myCA.pem generate-host-certificates=on \
> > 	dynamic_cert_mem_cache_size=4MB tls-dh=/opt/squid/etc/certs/dhparam.pem
> > sslcrtd_program /opt/squid/libexec/ssl_crtd -s /var/squid/ssldb -M 4MB
> >
> > # SSL-Bump
> > acl step1 at_step SslBump1
> > acl step2 at_step SslBump2
> > acl step3 at_step SslBump3
> > ssl_bump splice localhost
> > acl exclude_sites ssl::server_name_regex -i "/usr/local/etc/squid/doms.nobump"
> > ssl_bump peek step1 all
> > ssl_bump splice exclude_sites
> > ssl_bump stare step2 all
> > ssl_bump bump all
> >
> >  Content of "/usr/local/etc/squid/doms.nobump" is:
> >
> > update\.microsoft\.com$
> > update\.microsoft\.com\.akadns\.net$
> >
> >  But every time I have receiving Error code: SSL_ERROR_RX_RECORD_TOO_LONG in Firefox's browsers when I visit any web using https like https://www.debian.org, https://www.redhat.com, etc.. Some time ago, I have setup same config under OpenBSD and all works ok.
> >
> >  Where am I doing the mistake?
> Hardly this is mistake. Most probably this is platform-specific
> non-squid bug.
> -- 
> Bugs to the Future

Uhmm ... You are right Yuri. There is some problem with LibreSSL :( ... Generating certificate with Debian's openssl package works ok.

Many thanks Yuri.

-- 
Greetings,
C. L. Martinez


From hardikdangar+squid at gmail.com  Sat Mar  4 09:58:14 2017
From: hardikdangar+squid at gmail.com (Hardik Dangar)
Date: Sat, 4 Mar 2017 15:28:14 +0530
Subject: [squid-users] squid errors in error.log and slow internet from squid
Message-ID: <CA+sSnVZpNBKBVP5Njv5Yby4-z7K1R9MfbkAoFf9NHyVLc23ndQ@mail.gmail.com>

Hello,

I have been facing few squid issues lately. we are about 40 users mostly
linux, some mac and some windows system.

squid has become very slow for some reason. we can check that by bypassing
squid and opening same urls. Second thing we are seeing swap mismatch
errors and only solution we found is to remove swap file and use squid -z
that will make things work for a day. other than that there are other
errors in squid log too.

/var/cache/squid directory has became around 80 gb.

Here is our squid version and conf file

Squid Cache: Version 3.5.23
Service Name: squid
configure options:  '--prefix=/usr' '--localstatedir=/var/squid'
'--libexecdir=/lib/squid' '--srcdir=.' '--datadir=/share/squid'
'--sysconfdir=/etc/squid' '--with-default-user=proxy'
'--with-logdir=/var/log/squid' '--with-pidfile=/var/run/squid.pid'
'--with-openssl' '--enable-ssl-crtd' '--enable-inline'
'--disable-arch-native' '--enable-async-io=8'
'--enable-storeio=ufs,aufs,diskd,rock' '--enable-removal-policies=lru,heap'
'--enable-delay-pools' '--enable-follow-x-forwarded-for'
'--enable-url-rewrite-helpers=fake' '--enable-ecap' '--enable-arp-acl'
'--disable-translation'


Squid.conf file
https://gist.githubusercontent.com/hardikdangar/44f424e5a0be0a2a12ebc53a25a29629/raw/6e70682601b03732b662094ea7f4e79fb80a3851/squid.conf

Squid error log
http://pastebin.com/raw/J7Ws2WKT
if you see error log file you can see  Error negotiating SSL errors and
 Ignoring malformed cache entry errors frequently.


squid access also have lot of entries like,
1488621158.416   1316 192.168.1.51 TAG_NONE/200 0 CONNECT 35.154.95.34:443
- ORIGINAL_DST/35.154.95.34 -
1488621158.416   1315 192.168.1.51 TAG_NONE/200 0 CONNECT 35.154.95.34:443
- ORIGINAL_DST/35.154.95.34
which i believe https requests which have certificate issues. this has
became growing for mac machines.

Anybody else have similar issues?

We are not sure weather it was the 3.5.23 upgrade or adding Mac systems to
our network is causing these issues. but squid has certainly become slow
for lots of people to the point we have to bypass them.


Thank you very much for reading through the post. Let me know if you find
any issues with our config or anything that can help us figuring out whats
the issue.

Have a good day.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20170304/5f126888/attachment.htm>

From eliezer at ngtech.co.il  Sat Mar  4 17:36:50 2017
From: eliezer at ngtech.co.il (Eliezer  Croitoru)
Date: Sat, 4 Mar 2017 19:36:50 +0200
Subject: [squid-users] squid 3.5.2==> HTTPS FATAL: The ssl_crtd helpers
	are crashing too rapidly, need help!
In-Reply-To: <b6eee0af-ea58-105b-f6f6-dd6f25c58277@measurement-factory.com>
References: <592CB65D-536B-40F8-B887-12127542DCB5@netstream.ps>
 <38a07e38-7347-313a-b27f-08fd7eddfa3f@gmail.com>
 <97215197-78fc-e987-9a9d-51a697010e7b@gmail.com>
 <714528e5-a6d5-e72a-2bc7-9950a8eecb73@gmail.com>
 <7c1d01d293b5$9b22d2c0$d1687840$@ngtech.co.il>
 <a5a63820-0afd-563b-f627-3819733a5680@gmail.com>
 <4F85658B-A32E-4D74-98E6-E1CEE90168EA@netstream.ps>
 <7c6801d29420$9481bb00$bd853100$@ngtech.co.il>
 <b6eee0af-ea58-105b-f6f6-dd6f25c58277@measurement-factory.com>
Message-ID: <96d301d2950d$e83beac0$b8b3c040$@ngtech.co.il>

Hey Alex,

I still believe that an upgrade will improve since it's hard for me to imagine that only two admins are having trouble with it.
What will scare me is that If indeed many admins are having such issues and they do not ask or report about these.
I hope that with this thread we will be one step smarter and one step closer to a satisfying solution rather then the currently used options.

Eliezer

----
Eliezer Croitoru
Linux System Administrator
Mobile: +972-5-28704261
Email: eliezer at ngtech.co.il


-----Original Message-----
From: Alex Rousskov [mailto:rousskov at measurement-factory.com] 
Sent: Friday, March 3, 2017 5:56 PM
To: squid-users at lists.squid-cache.org
Cc: Eliezer Croitoru <eliezer at ngtech.co.il>
Subject: Re: [squid-users] squid 3.5.2==> HTTPS FATAL: The ssl_crtd helpers are crashing too rapidly, need help!

On 03/03/2017 06:17 AM, Eliezer Croitoru wrote:

> one of the options is to fence the ssl_crtd with some kind of lock
> mechanism for the DB rebuild time.

ssl_crtd already has a lock mechanism. If that mechanism is buggy, it
needs to be fixed, but it does not make sense to add another one.

There is still a lot of room for improvements, of course. For example,
compared to a log-monitoring watchdog mentioned by Yuri:

* Teaching ssl_crtd to run a sysadmin-provided script on database
failures will allow the sysadmin to handle such failures almost
transparently to the users and may reduce configuration headaches
associated with ssl_crtd message text changes.

* Teaching Squid to run a sysadmin-provided script on helper crashes
will allow the sysadmin to handle such failures more transparently to
the users and may reduce configuration headaches associated with helper
message text changes.


Alex.




From eliezer at ngtech.co.il  Sat Mar  4 17:50:56 2017
From: eliezer at ngtech.co.il (Eliezer  Croitoru)
Date: Sat, 4 Mar 2017 19:50:56 +0200
Subject: [squid-users] squid errors in error.log and slow internet from
	squid
In-Reply-To: <CA+sSnVZpNBKBVP5Njv5Yby4-z7K1R9MfbkAoFf9NHyVLc23ndQ@mail.gmail.com>
References: <CA+sSnVZpNBKBVP5Njv5Yby4-z7K1R9MfbkAoFf9NHyVLc23ndQ@mail.gmail.com>
Message-ID: <96d601d2950f$e04f57f0$a0ee07d0$@ngtech.co.il>

Two things:
First change the:
quick_abort_min -1

or remove it since it can do much harm on many cases and is useful only for specific ones.
Also try to remove all the refresh_pattern lines except for the defaults just to make sure they do not cause the server to move slow.
To understand the size of your service please attach the squid manager info page.
Have you checked CPU or memory usage when the service is slow?

using the cache manager can be seen at: http://wiki.squid-cache.org/Features/CacheManager#default
And you can use curl with something like http://mycache.example.com:3128/squid-internal-mgr/info

Eliezer

----
http://ngtech.co.il/lmgtfy/
Linux System Administrator
Mobile: +972-5-28704261
Email: eliezer at ngtech.co.il


From: squid-users [mailto:squid-users-bounces at lists.squid-cache.org] On Behalf Of Hardik Dangar
Sent: Saturday, March 4, 2017 11:58 AM
To: Squid Users <squid-users at lists.squid-cache.org>
Subject: [squid-users] squid errors in error.log and slow internet from squid

Hello,

I have been facing few squid issues lately. we are about 40 users mostly linux, some mac and some windows system.

squid has become very slow for some reason. we can check that by bypassing squid and opening same urls. Second thing we are seeing swap mismatch errors and only solution we found is to remove swap file and use squid -z that will make things work for a day. other than that there are other errors in squid log too.

/var/cache/squid directory has became around 80 gb.

Here is our squid version and conf file 

Squid Cache: Version 3.5.23
Service Name: squid
configure options:  '--prefix=/usr' '--localstatedir=/var/squid' '--libexecdir=/lib/squid' '--srcdir=.' '--datadir=/share/squid' '--sysconfdir=/etc/squid' '--with-default-user=proxy' '--with-logdir=/var/log/squid' '--with-pidfile=/var/run/squid.pid' '--with-openssl' '--enable-ssl-crtd' '--enable-inline' '--disable-arch-native' '--enable-async-io=8' '--enable-storeio=ufs,aufs,diskd,rock' '--enable-removal-policies=lru,heap' '--enable-delay-pools' '--enable-follow-x-forwarded-for' '--enable-url-rewrite-helpers=fake' '--enable-ecap' '--enable-arp-acl' '--disable-translation'


Squid.conf file
https://gist.githubusercontent.com/hardikdangar/44f424e5a0be0a2a12ebc53a25a29629/raw/6e70682601b03732b662094ea7f4e79fb80a3851/squid.conf

Squid error log
http://pastebin.com/raw/J7Ws2WKT
if you see error log file you can see  Error negotiating SSL errors and  Ignoring malformed cache entry errors frequently.


squid access also have lot of entries like,
1488621158.416   1316 192.168.1.51 TAG_NONE/200 0 CONNECT http://35.154.95.34:443 - ORIGINAL_DST/http://35.154.95.34 -
1488621158.416   1315 192.168.1.51 TAG_NONE/200 0 CONNECT http://35.154.95.34:443 - ORIGINAL_DST/http://35.154.95.34 
which i believe https requests which have certificate issues. this has became growing for mac machines.

Anybody else have similar issues?

We are not sure weather it was the 3.5.23 upgrade or adding Mac systems to our network is causing these issues. but squid has certainly become slow for lots of people to the point we have to bypass them.


Thank you very much for reading through the post. Let me know if you find any issues with our config or anything that can help us figuring out whats the issue.

Have a good day.



From sothy.e98 at gmail.com  Mon Mar  6 09:44:49 2017
From: sothy.e98 at gmail.com (sothy shan)
Date: Mon, 6 Mar 2017 10:44:49 +0100
Subject: [squid-users] reverse proxy HTTPS
Message-ID: <CAHcF_0b8PKGxJFobuE2NJd6r1eFGw9BXUf9nJX6_gCL3ma5OUA@mail.gmail.com>

Hello friends,

I am using squid 4.0.18. It works for reverse proxy HTTP.

Now I need to make HTTPS. I am not sure configure squid server and ssl keys.

If you have any pointer or procedures in Ubuntu 16.04, please let me know.

Thanks for your reponse.


Best regards
Sothy
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20170306/2f1e52bc/attachment.htm>

From sothy.e98 at gmail.com  Mon Mar  6 10:21:21 2017
From: sothy.e98 at gmail.com (sothy shan)
Date: Mon, 6 Mar 2017 11:21:21 +0100
Subject: [squid-users] Fwd: reverse proxy HTTPS
In-Reply-To: <CAHcF_0b8PKGxJFobuE2NJd6r1eFGw9BXUf9nJX6_gCL3ma5OUA@mail.gmail.com>
References: <CAHcF_0b8PKGxJFobuE2NJd6r1eFGw9BXUf9nJX6_gCL3ma5OUA@mail.gmail.com>
Message-ID: <CAHcF_0a7Jpj-VXdtuLx6E8L7Psm_U959R=KAA338DVB8isuO4Q@mail.gmail.com>

Hi,

I can give precise what I am doing on this part.See the previous mail below
for my exact requirement.

//create the keys.

$openssl req -new -keyout key.pem -nodes -x509 -days 365 -out cert.pem

Both keys(cert.pem and key.pem) are places in /etc/squid/.

Then, I make following in squid.
+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
https_port 192.168.1.69:443 cert=/etc/squid/cert.pem key=/etc/squid/key.pem
cache_peer X.Y.Z.Z parent 443 0 no-query originserver


http_access allow all
++++++++++++++++++++++++++++++++++++++++++++++

When I type in browser like this https://192.168.1.69

I got an error in browser

The requested URL could not be retrieved
When see terminal, I show this error:

HTTP/1.1 400 Bad Request
Server: squid/4.0.18
Mime-Version: 1.0
Date: Mon, 06 Mar 2017 10:19:42 GMT
Content-Type: text/html;charset=utf-8
Content-Length: 3522
X-Squid-Error: ERR_INVALID_URL 0
Vary: Accept-Language
Content-Language: en
X-Cache: MISS from Host-004
Via: 1.1 Host-004 (squid/4.0.18)
Connection: close
///////////////////////////////////////////////////////////////////

May I ask why I'm getting this error?

Thanks for your help.

Best regards
Sothy


---------- Forwarded message ----------
From: sothy shan <sothy.e98 at gmail.com>
Date: Mon, Mar 6, 2017 at 10:44 AM
Subject: reverse proxy HTTPS
To: squid-users at lists.squid-cache.org


Hello friends,

I am using squid 4.0.18. It works for reverse proxy HTTP.

Now I need to make HTTPS. I am not sure configure squid server and ssl keys.

If you have any pointer or procedures in Ubuntu 16.04, please let me know.

Thanks for your reponse.


Best regards
Sothy
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20170306/5e8e72af/attachment.htm>

From alex at tech.jahtoe.com  Mon Mar  6 11:31:09 2017
From: alex at tech.jahtoe.com (Alex Muir)
Date: Mon, 6 Mar 2017 11:31:09 +0000
Subject: [squid-users] anonymous squid setup on digital ocean centos 6 but
	my IP still detected
Message-ID: <CAFtPEJYVL7mmEbAGYAM_DpXyEr5gYm5EzYrKNN=8p8vMxtgPow@mail.gmail.com>

Greetings,

I have implemented the following tutorial
<https://www.digitalocean.com/community/tutorials/how-to-install-squid-proxy-on-centos-6>
from
digital ocean which sets up squid with settings to not have my IP address
forwarded.

I'm finding however that my local IP is still detected when I google what
my local ip address is.  I've configured firefox browser to use the proxy
and have confirmed that it is using the proxy. Additionally https goes
through the proxy however http is blocked. I'd like to ensure that http is
not blocked.

What changes do I need to make to get this working as desired?

Here is the squid config setup:

[root at CENTOSMASTER ~]# cat /etc/squid/squid.conf
#
# Recommended minimum configuration:
#
acl manager proto cache_object
acl localhost src 127.0.0.1/32 ::1
acl to_localhost dst 127.0.0.0/8 0.0.0.0/32 ::1

# Example rule allowing access from your local networks.
# Adapt to list your (internal) IP networks from where browsing
# should be allowed
acl localnet src 10.0.0.0/8 # RFC1918 possible internal network
acl localnet src 172.16.0.0/12  # RFC1918 possible internal network
acl localnet src 192.168.0.0/16 # RFC1918 possible internal network
acl localnet src fc00::/7       # RFC 4193 local private network range
acl localnet src fe80::/10      # RFC 4291 link-local (directly
plugged) machines

acl SSL_ports port 443
acl Safe_ports port 80      # http
acl Safe_ports port 21      # ftp
acl Safe_ports port 443     # https
acl Safe_ports port 70      # gopher
acl Safe_ports port 210     # wais
acl Safe_ports port 1025-65535  # unregistered ports
acl Safe_ports port 280     # http-mgmt
acl Safe_ports port 488     # gss-http
acl Safe_ports port 591     # filemaker
acl Safe_ports port 777     # multiling http
acl CONNECT method CONNECT

#
# Recommended minimum Access Permission configuration:
#
# Only allow cachemgr access from localhost
http_access allow manager localhost
http_access deny manager

# Deny requests to certain unsafe ports
http_access deny !Safe_ports

# Deny CONNECT to other than secure SSL ports
#http_access deny CONNECT !SSL_ports

# We strongly recommend the following be uncommented to protect innocent
# web applications running on the proxy server who think the only
# one who can access services on "localhost" is a local user
#http_access deny to_localhost

#
# INSERT YOUR OWN RULE(S) HERE TO ALLOW ACCESS FROM YOUR CLIENTS
#

# Example rule allowing access from your local networks.
# Adapt localnet in the ACL section to list your (internal) IP networks
# from where browsing should be allowed
http_access allow localnet
http_access allow localhost

# And finally deny all other access to this proxy
http_access deny all

# Squid normally listens to port 3128
http_port 3128

# Uncomment and adjust the following to add a disk cache directory.
#cache_dir ufs /var/spool/squid 100 16 256

# Leave coredumps in the first cache dir
coredump_dir /var/spool/squid

# Add any of your own refresh_pattern entries above these.
refresh_pattern ^ftp:       1440    20% 10080
refresh_pattern ^gopher:    1440    0%  1440
refresh_pattern -i (/cgi-bin/|\?) 0 0%  0
refresh_pattern .       0   20%

via off
forwarded_for off

request_header_access Allow allow all
request_header_access Authorization allow all
request_header_access WWW-Authenticate allow all
request_header_access Proxy-Authorization allow all
request_header_access Proxy-Authenticate allow all
request_header_access Cache-Control allow all
request_header_access Content-Encoding allow all
request_header_access Content-Length allow all
request_header_access Content-Type allow all
request_header_access Date allow all
request_header_access Expires allow all
request_header_access Host allow all
request_header_access If-Modified-Since allow all
request_header_access Last-Modified allow all
request_header_access Location allow all
request_header_access Pragma allow all
request_header_access Accept allow all
request_header_access Accept-Charset allow all
request_header_access Accept-Encoding allow all
request_header_access Accept-Language allow all
request_header_access Content-Language allow all
request_header_access Mime-Version allow all
request_header_access Retry-After allow all
request_header_access Title allow all
request_header_access Connection allow all
request_header_access Proxy-Connection allow all
request_header_access User-Agent allow all
request_header_access Cookie allow all
request_header_access All deny all


I've posted the question on stackoverflow

http://serverfault.com/questions/836385/anonymous-
squid-setup-on-digital-ocean-centos-6-but-my-ip-still-detected

Regards

Alex Muir
Chief Data Engineer/Architect
Jahtoe Technology
tech.jahtoe.com
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20170306/55150b64/attachment.htm>

From hellname at bk.ru  Mon Mar  6 11:43:42 2017
From: hellname at bk.ru (=?UTF-8?B?aGVsbG5hbWUgaGVsbG5hbWU=?=)
Date: Mon, 06 Mar 2017 14:43:42 +0300
Subject: [squid-users] =?utf-8?q?squid_3=2E5=2E24_Host_header_forgery_dete?=
	=?utf-8?q?cted?=
Message-ID: <1488800622.869641941@f68.i.mail.ru>

Hello. I have the?squid 3.5.24 from source:
configure options: ?'--build=x86_64-linux-gnu' '--prefix=/usr' '--includedir=/include' '--mandir=/share/man' '--infodir=/share/info' '--sysconfdir=/etc' '--localstatedir=/var' '--libexecdir=/lib/squid' '--srcdir=.' '--disable-maintainer-mode' '--disable-dependency-tracking' '--disable-silent-rules' '--datadir=/usr/share/squid' '--sysconfdir=/etc/squid' '--mandir=/usr/share/man' '--enable-inline' '--disable-arch-native' '--enable-async-io=8' '--enable-storeio=ufs,aufs,diskd,rock' '--enable-removal-policies=lru,heap' '--enable-delay-pools' '--enable-cache-digests' '--enable-icap-client' '--enable-follow-x-forwarded-for' '--enable-auth-basic=DB,fake,getpwnam,LDAP,NCSA,NIS,PAM,POP3,RADIUS,SASL,SMB' '--enable-auth-digest=file,LDAP' '--enable-auth-negotiate=kerberos,wrapper' '--enable-auth-ntlm=fake,smb_lm' '--enable-external-acl-helpers=file_userip,kerberos_ldap_group,LDAP_group,session,SQL_session,unix_group,wbinfo_group' '--enable-url-rewrite-helpers=fake' '--enable-eui' '--enable-esi' '--enable-icmp' '--enable-zph-qos' '--enable-ecap' '--disable-translation' '--with-swapdir=/var/spool/squid' '--with-logdir=/var/log/squid' '--with-pidfile=/var/run/squid.pid' '--with-filedescriptors=65536' '--with-large-files' '--with-default-user=proxy' '--enable-ssl' '--enable-ssl-crtd' '--with-openssl' '--enable-linux-netfilter' 'CFLAGS=-g -O2 -fPIE -fstack-protector-strong -Wformat -Werror=format-security -Wall' 'LDFLAGS=-fPIE -pie -Wl,-z,relro -Wl,-z,now' 'CPPFLAGS=-D_FORTIFY_SOURCE=2' 'CXXFLAGS=-g -O2 -fPIE -fstack-protector-strong -Wformat -Werror=format-security' 'build_alias=x86_64-linux-gnu'

I also have this configuration:
max_filedesc 35000
acl localnet src 172.16.16.0/24
acl localnet src 172.22.1.0/24
acl SSL_ports port 443
acl CONNECT method CONNECT
acl AdminsIP src "/etc/squid/admips.txt"
acl BlackList dstdomain "/etc/squid/BL.txt"
acl BlockInetExlWhite src "/etc/squid/BI.txt"
acl WhiteList dstdomain "/etc/squid/WL.txt"
acl manager proto manager
via off
forwarded_for off
follow_x_forwarded_for deny all
visible_hostname my.server.com
hosts_file /etc/hosts
dns_nameservers 172.16.16.11
ipcache_size 10240
negative_dns_ttl 5 minutes
fqdncache_size 10240
http_access deny !Safe_ports
http_access deny CONNECT !SSL_ports
http_access allow manager
http_access deny manager
cachemgr_passwd password all
http_access allow localhost
http_access allow AdminsIP
http_access deny BlockInetExlWhite !WhiteList
http_access deny BlackList
http_access allow localnet
http_access deny all
cache deny all
http_port 0.0.0.0:3128 intercept
http_port 0.0.0.0:3130
https_port 0.0.0.0:3129 intercept ssl-bump generate-host-certificates=on dynamic_cert_mem_cache_size=4MB cert=/etc/squid/ssl/myCA.pem
always_direct allow all
sslproxy_cert_error allow all
sslproxy_flags DONT_VERIFY_PEER
acl blocked ssl::server_name "/etc/squid/BL.txt"
acl step1 at_step SslBump1
ssl_bump peek step1
ssl_bump terminate blocked
ssl_bump splice all
sslcrtd_program /lib/squid/ssl_crtd -s /var/lib/ssl_db -M 4MB
coredump_dir /var/spool/squid
refresh_pattern ^ftp: 1440 20% 10080
refresh_pattern ^gopher: 1440 0% 1440
refresh_pattern -i (/cgi-bin/|\?) 0 0% 0
refresh_pattern . 0 20% 4320
maximum_object_size 61440 KB
minimum_object_size 3 KB
cache_swap_low 90
cache_swap_high 95
maximum_object_size_in_memory 512 KB
memory_replacement_policy lru
logfile_rotate 0
cache_mgr  admin at my.server.com ?

All users in my company have enabled auto-proxy-config in OS with wpad url configuration, below you can see wpad.dat:
function FindProxyForURL(url, host) {
var proxy_on = "PROXY 172.16.16.30:3130";
var proxy_off = "DIRECT";
var network = "172.16.16.0";
var network1 = "192.168.100.0";
var subnet = "255.255.255.0";

var proxy_bypass = new Array(
"*.slack.com",
"*.slack-edge.com",
"*.slack-msgs.com",
"*slack-files.com",
"*slack-imgs.com",
"*slack-edge.com",
"*slack-core.com",
"*slack-redir.net",
"192.168.100.0",
"127.0.0.1",
"localhost"
);
if (isInNet(host, network, subnet)) {
return proxy_off;
}
if (isInNet(host, network1, subnet)) {
return proxy_off;
}

for (var i = 0; i < proxy_bypass.length; i++) {
if (shExpMatch(host, proxy_bypass[i])) {
return DIRECT;
}
}

if (shExpMatch(url, "http:*") ||
shExpMatch(url, "https:*") ||
shExpMatch(url, "ftp:*")) {
return proxy_on;
}

// Finally, send all other requests direct.
return proxy_off;
}

In the wpad web server I can see logs:
188.xxx.xxx.xxx - - [06/Mar/2017:11:20:21 +0300] "GET /wpad.dat HTTP/1.1" 200 1324 "-" "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Slack/2.5.1 Chrome/53.0.2785.143 Electron/1.4.15 Safari/537.36"
188.xxx.xxx.xxx - - [06/Mar/2017:11:57:29 +0300] "GET /wpad.dat HTTP/1.1" 200 1324 "-" "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/56.0.2924.87 Safari/537.36"
188.xxx.xxx.xxx - - [06/Mar/2017:11:57:29 +0300] "GET /wpad.dat HTTP/1.1" 200 1324 "-" "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/56.0.2924.87 Safari/537.36"
188.xxx.xxx.xxx - - [06/Mar/2017:11:57:29 +0300] "GET /wpad.dat HTTP/1.1" 200 1324 "-" "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Slack/2.5.1 Chrome/53.0.2785.143 Electron/1.4.15 Safari/537.36"
188.xxx.xxx.xxx - - [06/Mar/2017:13:13:01 +0300] "GET /wpad.dat HTTP/1.1" 200 1324 "-" "Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/56.0.2924.87 Safari/537.36" I see the problems when to try work with slack app or gmail apps (e.g. calendar), in the squid log I see many entries:
2017/03/06 14:38:03| SECURITY ALERT: Host header forgery detected on local=54.230.99.128:443 remote=172.16.16.123:64857 FD 748 flags=33 (local IP does not match any domain IP)
2017/03/06 14:38:03| SECURITY ALERT: on URL: slack.com:443
2017/03/06 14:38:30| SECURITY ALERT: Host header forgery detected on local=54.230.99.128:443 remote=172.16.16.123:64875 FD 505 flags=33 (local IP does not match any domain IP)
2017/03/06 14:38:30| SECURITY ALERT: on URL: slack.com:443
2017/03/06 14:38:37| SECURITY ALERT: Host header forgery detected on local=54.230.99.128:443 remote=172.16.16.123:64881 FD 678 flags=33 (local IP does not match any domain IP)
Could you help me please?
P.S. Sorry for my bad english.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20170306/a757c0ac/attachment.htm>

From for.work2920 at gmail.com  Mon Mar  6 13:46:42 2017
From: for.work2920 at gmail.com (Hanoch Hanoch K)
Date: Mon, 6 Mar 2017 15:46:42 +0200
Subject: [squid-users] Ssl bump tunneling connection by using Common Name
Message-ID: <CAHWcLuOGMmnPA=teUUSzycw_CFbfmMxQGp2XvjiaBHE52PyEBQ@mail.gmail.com>

Greetings

We're using Squid 3.5.19 with ssl bump,
and we want to tunnel (not bump) applications such as skype, that use
pinned ssl,
so we defined an acl for splicing skype's ssl_server_name.

However skype's client app uses client certificates that don't have SNI.
The only way to identify skype is its Common Name: *.dc.trouter.io

But the Common Name is available only in step3 of ssl bump,
where tunneling the connection is no longer possible (as documented in peek
and splice step3 docs).
What we get is bumping.

Is there a way we can tunnel an acl based on Common Name?

ty


http_port 3127
http_port 3128 intercept
https_port 3129 ssl-bump intercept generate-host-certificates=on
dynamic_cert_mem_cache_size=4MB cert=/etc/squid/ssl_cert/myCA.pem
always_direct allow all
acl DiscoverSNIHost at_step SslBump1
acl NoSSLIntercept ssl::server_name_regex -i (microsoft|msn|windows|update|
skype.com|go.trouter.io|secure.adnxs.compipe.skype.com|skype-m.hotmail.com|
mobile.pipe.aria.microsoft.com|edge.skype.com|api.cc.skype.com|a.config.sky
pe.com|clientlogin.cdn.skype.com|.dc.trouter.io|ui.skype.com|apps.skype.com|
registrar-rr.prod.registrar.skype.com|secure.skypeassets.com|c1.skype.com)
ssl_bump splice NoSSLIntercept
ssl_bump peek DiscoverSNIHost
ssl_bump bump all

sslproxy_cert_error allow all
sslproxy_flags DONT_VERIFY_PEER
sslcrtd_program /usr/lib/squid/ssl_crtd -s /var/lib/ssl_db -M 4MB
sslcrtd_children 5
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20170306/85700560/attachment.htm>

From guy.helmer at gmail.com  Mon Mar  6 14:33:23 2017
From: guy.helmer at gmail.com (Guy Helmer)
Date: Mon, 6 Mar 2017 08:33:23 -0600
Subject: [squid-users] squid 3.5.2==> HTTPS FATAL: The ssl_crtd helpers
	are crashing too rapidly, need help!
In-Reply-To: <96d301d2950d$e83beac0$b8b3c040$@ngtech.co.il>
References: <592CB65D-536B-40F8-B887-12127542DCB5@netstream.ps>
 <38a07e38-7347-313a-b27f-08fd7eddfa3f@gmail.com>
 <97215197-78fc-e987-9a9d-51a697010e7b@gmail.com>
 <714528e5-a6d5-e72a-2bc7-9950a8eecb73@gmail.com>
 <7c1d01d293b5$9b22d2c0$d1687840$@ngtech.co.il>
 <a5a63820-0afd-563b-f627-3819733a5680@gmail.com>
 <4F85658B-A32E-4D74-98E6-E1CEE90168EA@netstream.ps>
 <7c6801d29420$9481bb00$bd853100$@ngtech.co.il>
 <b6eee0af-ea58-105b-f6f6-dd6f25c58277@measurement-factory.com>
 <96d301d2950d$e83beac0$b8b3c040$@ngtech.co.il>
Message-ID: <BF13D4F4-ABE5-4A60-87C6-11FB66A02686@gmail.com>

Hi, all,

A couple of years ago, I wrote a perl script that ran a specified number of ssl_crtd processes with simultaneous requests to expose the problem and test the resolution. I?ve attached it below in case it would help test/diagnose the situation. It has hard-coded paths at the top of the script that would need to be updated for any particular test environment, and its testing certificate directory would need to be prepared in advance just as for a regular instance of squid running ssl_crtd.

I initially investigated the problem and helped improve the database file locking problem by changing the locking protocol from lockf() to flock(). I haven?t seen the problem occur on FreeBSD since the flock() changes went in. However, when I last looked at the code, some operating systems were still configured to use lockf() locking which does not function as needed due to its unintuitive POSIX / X/Open semantics: the first close() on the locked file (even via a different file descriptor) releases the lock. MacOS?s man page notes "File locks are released on first close by the locking process of any file descriptor for the file?.  There are situations in the ssl_crtd code where the database file is open using multiple file descriptors simultaneously, and close() calls occur that would cause read/write hazards due to loss of the lockf() lock.

My $0.02,
Guy


> On Mar 4, 2017, at 11:36 AM, Eliezer Croitoru <eliezer at ngtech.co.il> wrote:
> 
> Hey Alex,
> 
> I still believe that an upgrade will improve since it's hard for me to imagine that only two admins are having trouble with it.
> What will scare me is that If indeed many admins are having such issues and they do not ask or report about these.
> I hope that with this thread we will be one step smarter and one step closer to a satisfying solution rather then the currently used options.
> 
> Eliezer
> 
> ----
> Eliezer Croitoru
> Linux System Administrator
> Mobile: +972-5-28704261
> Email: eliezer at ngtech.co.il
> 
> 
> -----Original Message-----
> From: Alex Rousskov [mailto:rousskov at measurement-factory.com] 
> Sent: Friday, March 3, 2017 5:56 PM
> To: squid-users at lists.squid-cache.org
> Cc: Eliezer Croitoru <eliezer at ngtech.co.il>
> Subject: Re: [squid-users] squid 3.5.2==> HTTPS FATAL: The ssl_crtd helpers are crashing too rapidly, need help!
> 
> On 03/03/2017 06:17 AM, Eliezer Croitoru wrote:
> 
>> one of the options is to fence the ssl_crtd with some kind of lock
>> mechanism for the DB rebuild time.
> 
> ssl_crtd already has a lock mechanism. If that mechanism is buggy, it
> needs to be fixed, but it does not make sense to add another one.
> 
> There is still a lot of room for improvements, of course. For example,
> compared to a log-monitoring watchdog mentioned by Yuri:
> 
> * Teaching ssl_crtd to run a sysadmin-provided script on database
> failures will allow the sysadmin to handle such failures almost
> transparently to the users and may reduce configuration headaches
> associated with ssl_crtd message text changes.
> 
> * Teaching Squid to run a sysadmin-provided script on helper crashes
> will allow the sysadmin to handle such failures more transparently to
> the users and may reduce configuration headaches associated with helper
> message text changes.
> 
> 
> Alex.
> 
> 
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users

-------------- next part --------------
A non-text attachment was scrubbed...
Name: stress-sslcrtd.perl
Type: application/octet-stream
Size: 3288 bytes
Desc: not available
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20170306/9560821f/attachment.obj>
-------------- next part --------------



From rousskov at measurement-factory.com  Mon Mar  6 16:36:33 2017
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Mon, 6 Mar 2017 09:36:33 -0700
Subject: [squid-users] Ssl bump tunneling connection by using Common Name
In-Reply-To: <CAHWcLuOGMmnPA=teUUSzycw_CFbfmMxQGp2XvjiaBHE52PyEBQ@mail.gmail.com>
References: <CAHWcLuOGMmnPA=teUUSzycw_CFbfmMxQGp2XvjiaBHE52PyEBQ@mail.gmail.com>
Message-ID: <941f65cb-fc84-0bb6-a0ca-e94c9515b3ac@measurement-factory.com>

On 03/06/2017 06:46 AM, Hanoch Hanoch K wrote:

> However skype's client app uses client certificates that don't have SNI.

SNI is not a property of a client certificate. It is a property of a
client Hello message. I do not know whether some Skype clients do not
send SNI with their Hellos, but I wanted to correct the above
misconception for the record.


> the Common Name is available only in step3 of ssl bump,

This part is correct. Squid receives the server handshake messages,
including the server certificate during step3.


> where tunneling the connection is no longer possible (as documented in
> peek and splice step3 docs).

This is somewhat misleading: Splicing after step3 works fine, provided
the "peek" action matched at step2. In other words, splicing works after
peeking.

Needless to say, bumping configurations cannot peek during step3 -- they
have to either stare or bump instead.

> acl DiscoverSNIHost at_step SslBump1
> acl NoSSLIntercept ssl::server_name_regex -i ...
> ssl_bump splice NoSSLIntercept
> ssl_bump peek DiscoverSNIHost
> ssl_bump bump all

Your configuration usually peeks [at the client Hello] during step2 and
either splices or bumps during step3.


> Is there a way we can tunnel an acl based on Common Name?

Yes, but you will not be able to bump then.

If some Skype clients do not send SNI, then your options include:

* do not bump any connections;

* match Skype connections using destination IP addresses (lots of
maintenance headaches and some errors, but doable);

* enhance Squid to detect something that is unique to Skype client
handshake messages sent _before_ the client receives the server Hello.
For example, if (and only if) the Skype client sends its certificate
before receiving the server Hello, then Squid can be enhanced to detect
and interrogate that client certificate using ACLs.

Pick your poison.

Alex.



From eliezer at ngtech.co.il  Mon Mar  6 16:41:02 2017
From: eliezer at ngtech.co.il (Eliezer  Croitoru)
Date: Mon, 6 Mar 2017 18:41:02 +0200
Subject: [squid-users] Ssl bump tunneling connection by using Common Name
In-Reply-To: <CAHWcLuOGMmnPA=teUUSzycw_CFbfmMxQGp2XvjiaBHE52PyEBQ@mail.gmail.com>
References: <CAHWcLuOGMmnPA=teUUSzycw_CFbfmMxQGp2XvjiaBHE52PyEBQ@mail.gmail.com>
Message-ID: <002301d29698$7149c0e0$53dd42a0$@ngtech.co.il>

Hey,

There was something about it but I believe it's only on squid version 4.0.X.
The other options for such a thing is to use an external_acl helper that will try to initiate a connection to the destination host (like what is done in the happy eyeballs) to and to inspect the certificate to match a specific criteria.
I was working on such a helper a year ago but stopped touch it since there was something I didn't expected.
I can try to dig in my repository and see if I find the helper.

Let me know if to bother with it.

Eliezer

----
http://ngtech.co.il/lmgtfy/
Linux System Administrator
Mobile: +972-5-28704261
Email: eliezer at ngtech.co.il


From: squid-users [mailto:squid-users-bounces at lists.squid-cache.org] On Behalf Of Hanoch Hanoch K
Sent: Monday, March 6, 2017 3:47 PM
To: squid-users at lists.squid-cache.org
Subject: [squid-users] Ssl bump tunneling connection by using Common Name

Greetings

We're using Squid 3.5.19 with ssl bump,
and we want to tunnel (not bump) applications such as skype, that use pinned ssl,
so we defined an acl for splicing skype's ssl_server_name.

However skype's client app uses client certificates that don't have SNI.
The only way to identify skype is its Common Name: *.http://dc.trouter.io/

But the Common Name is available only in step3 of ssl bump,
where tunneling the connection is no longer possible (as documented in peek and splice step3 docs).
What we get is bumping.

Is there a way we can tunnel an acl based on Common Name?

ty


http_port 3127
http_port 3128 intercept
https_port 3129 ssl-bump intercept generate-host-certificates=on dynamic_cert_mem_cache_size=4MB cert=/etc/squid/ssl_cert/myCA.pem
always_direct allow all
acl DiscoverSNIHost at_step SslBump1
acl NoSSLIntercept ssl::server_name_regex -i (microsoft|msn|windows|update|http://skype.com/|http://go.trouter.io/|http://secure.adnxs.compipe.skype.com/|http://skype-m.hotmail.com/|http://mobile.pipe.aria.microsoft.com/|http://edge.skype.com/|http://api.cc.skype.com/|http://a.config.skype.com/|http://clientlogin.cdn.skype.com/|.http://dc.trouter.io/|http://ui.skype.com/|http://apps.skype.com/|http://registrar-rr.prod.registrar.skype.com/|http://secure.skypeassets.com/|http://c1.skype.com/)
ssl_bump splice NoSSLIntercept
ssl_bump peek DiscoverSNIHost
ssl_bump bump all

sslproxy_cert_error allow all
sslproxy_flags DONT_VERIFY_PEER
sslcrtd_program /usr/lib/squid/ssl_crtd -s /var/lib/ssl_db -M 4MB
sslcrtd_children 5




From eliezer at ngtech.co.il  Mon Mar  6 17:30:10 2017
From: eliezer at ngtech.co.il (Eliezer  Croitoru)
Date: Mon, 6 Mar 2017 19:30:10 +0200
Subject: [squid-users] anonymous squid setup on digital ocean centos 6
	but	my IP still detected
In-Reply-To: <CAFtPEJYVL7mmEbAGYAM_DpXyEr5gYm5EzYrKNN=8p8vMxtgPow@mail.gmail.com>
References: <CAFtPEJYVL7mmEbAGYAM_DpXyEr5gYm5EzYrKNN=8p8vMxtgPow@mail.gmail.com>
Message-ID: <002701d2969f$4e9acce0$ebd066a0$@ngtech.co.il>

Hey Alex,

First goes first: If you spin any version of CentOS these days I recommend to use CentOS 7 and not 6.
This is also based on many use cases which proved(to me and many others) that the kernel and many other components improved performance.
If you already spinoff a server to mask your IP I believe that the more "perfected" way to do so is using a VPN.
Specifically OpenVPN and a CentOS 7 with pritunl on it as the management web interface for OpenVPN.
The instructions on how to install it are at:
https://docs.pritunl.com/docs/installation

The client can be found at:
https://openvpn.net/index.php/open-source/downloads.html

And it would mask your IP address for all of your connections from the machine you are working on.
Notice that some that the reason your IP is not masked is since there are scripts which can run on html5 and can bypass the proxy settings.

Let me know if you need more help.

Eliezer

----
http://ngtech.co.il/lmgtfy/
Linux System Administrator
Mobile: +972-5-28704261
Email: eliezer at ngtech.co.il


From: squid-users [mailto:squid-users-bounces at lists.squid-cache.org] On Behalf Of Alex Muir
Sent: Monday, March 6, 2017 1:31 PM
To: squid-users at lists.squid-cache.org
Subject: [squid-users] anonymous squid setup on digital ocean centos 6 but my IP still detected



Greetings,

I have implemented the following https://www.digitalocean.com/community/tutorials/how-to-install-squid-proxy-on-centos-6 from digital ocean which sets up squid with settings to not have my IP address forwarded.
I'm finding however that my local IP is still detected when I google what my local ip address is.  I've configured firefox browser to use the proxy and have confirmed that it is using the proxy. Additionally https goes through the proxy however http is blocked. I'd like to ensure that http is not blocked.
What changes do I need to make to get this working as desired?
Here is the squid config setup:
[root at CENTOSMASTER ~]# cat /etc/squid/squid.conf
#
# Recommended minimum configuration:
#
acl manager proto cache_object
acl localhost src http://127.0.0.1/32 ::1
acl to_localhost dst http://127.0.0.0/8 http://0.0.0.0/32 ::1

# Example rule allowing access from your local networks.
# Adapt to list your (internal) IP networks from where browsing
# should be allowed
acl localnet src http://10.0.0.0/8 # RFC1918 possible internal network
acl localnet src http://172.16.0.0/12  # RFC1918 possible internal network
acl localnet src http://192.168.0.0/16 # RFC1918 possible internal network
acl localnet src fc00::/7       # RFC 4193 local private network range
acl localnet src fe80::/10      # RFC 4291 link-local (directly plugged) machines

acl SSL_ports port 443
acl Safe_ports port 80      # http
acl Safe_ports port 21      # ftp
acl Safe_ports port 443     # https
acl Safe_ports port 70      # gopher
acl Safe_ports port 210     # wais
acl Safe_ports port 1025-65535  # unregistered ports
acl Safe_ports port 280     # http-mgmt
acl Safe_ports port 488     # gss-http
acl Safe_ports port 591     # filemaker
acl Safe_ports port 777     # multiling http
acl CONNECT method CONNECT

#
# Recommended minimum Access Permission configuration:
#
# Only allow cachemgr access from localhost
http_access allow manager localhost
http_access deny manager

# Deny requests to certain unsafe ports
http_access deny !Safe_ports

# Deny CONNECT to other than secure SSL ports
#http_access deny CONNECT !SSL_ports

# We strongly recommend the following be uncommented to protect innocent
# web applications running on the proxy server who think the only
# one who can access services on "localhost" is a local user
#http_access deny to_localhost

#
# INSERT YOUR OWN RULE(S) HERE TO ALLOW ACCESS FROM YOUR CLIENTS
#

# Example rule allowing access from your local networks.
# Adapt localnet in the ACL section to list your (internal) IP networks
# from where browsing should be allowed
http_access allow localnet
http_access allow localhost

# And finally deny all other access to this proxy
http_access deny all

# Squid normally listens to port 3128
http_port 3128

# Uncomment and adjust the following to add a disk cache directory.
#cache_dir ufs /var/spool/squid 100 16 256

# Leave coredumps in the first cache dir
coredump_dir /var/spool/squid

# Add any of your own refresh_pattern entries above these.
refresh_pattern ^ftp:       1440    20% 10080
refresh_pattern ^gopher:    1440    0%  1440
refresh_pattern -i (/cgi-bin/|\?) 0 0%  0
refresh_pattern .       0   20% 

via off
forwarded_for off

request_header_access Allow allow all 
request_header_access Authorization allow all 
request_header_access WWW-Authenticate allow all 
request_header_access Proxy-Authorization allow all 
request_header_access Proxy-Authenticate allow all 
request_header_access Cache-Control allow all 
request_header_access Content-Encoding allow all 
request_header_access Content-Length allow all 
request_header_access Content-Type allow all 
request_header_access Date allow all 
request_header_access Expires allow all 
request_header_access Host allow all 
request_header_access If-Modified-Since allow all 
request_header_access Last-Modified allow all 
request_header_access Location allow all 
request_header_access Pragma allow all 
request_header_access Accept allow all 
request_header_access Accept-Charset allow all 
request_header_access Accept-Encoding allow all 
request_header_access Accept-Language allow all 
request_header_access Content-Language allow all 
request_header_access Mime-Version allow all 
request_header_access Retry-After allow all 
request_header_access Title allow all 
request_header_access Connection allow all 
request_header_access Proxy-Connection allow all 
request_header_access User-Agent allow all 
request_header_access Cookie allow all 
request_header_access All deny all

I've posted the question on stackoverflow

http://serverfault.com/questions/836385/anonymous-squid-setup-on-digital-ocean-centos-6-but-my-ip-still-detected

Regards
Alex Muir
Chief Data Engineer/Architect
Jahtoe Technology
http://tech.jahtoe.com



From alex at tech.jahtoe.com  Mon Mar  6 18:13:04 2017
From: alex at tech.jahtoe.com (Alex Muir)
Date: Mon, 6 Mar 2017 18:13:04 +0000
Subject: [squid-users] anonymous squid setup on digital ocean centos 6
 but my IP still detected
In-Reply-To: <002701d2969f$4e9acce0$ebd066a0$@ngtech.co.il>
References: <CAFtPEJYVL7mmEbAGYAM_DpXyEr5gYm5EzYrKNN=8p8vMxtgPow@mail.gmail.com>
 <002701d2969f$4e9acce0$ebd066a0$@ngtech.co.il>
Message-ID: <CAFtPEJbipG66wKafd8EHu1JYjfhNb=FUcbp-kW9n8mW589xFWw@mail.gmail.com>

Eliezer
Thanks for your good advice..

Regards
Alex
tech.jahtoe.com
bafila.jahtoe.com

On 6 Mar 2017 17:30, "Eliezer Croitoru" <eliezer at ngtech.co.il> wrote:

> Hey Alex,
>
> First goes first: If you spin any version of CentOS these days I recommend
> to use CentOS 7 and not 6.
> This is also based on many use cases which proved(to me and many others)
> that the kernel and many other components improved performance.
> If you already spinoff a server to mask your IP I believe that the more
> "perfected" way to do so is using a VPN.
> Specifically OpenVPN and a CentOS 7 with pritunl on it as the management
> web interface for OpenVPN.
> The instructions on how to install it are at:
> https://docs.pritunl.com/docs/installation
>
> The client can be found at:
> https://openvpn.net/index.php/open-source/downloads.html
>
> And it would mask your IP address for all of your connections from the
> machine you are working on.
> Notice that some that the reason your IP is not masked is since there are
> scripts which can run on html5 and can bypass the proxy settings.
>
> Let me know if you need more help.
>
> Eliezer
>
> ----
> http://ngtech.co.il/lmgtfy/
> Linux System Administrator
> Mobile: +972-5-28704261
> Email: eliezer at ngtech.co.il
>
>
> From: squid-users [mailto:squid-users-bounces at lists.squid-cache.org] On
> Behalf Of Alex Muir
> Sent: Monday, March 6, 2017 1:31 PM
> To: squid-users at lists.squid-cache.org
> Subject: [squid-users] anonymous squid setup on digital ocean centos 6 but
> my IP still detected
>
>
>
> Greetings,
>
> I have implemented the following https://www.digitalocean.com/
> community/tutorials/how-to-install-squid-proxy-on-centos-6 from digital
> ocean which sets up squid with settings to not have my IP address forwarded.
> I'm finding however that my local IP is still detected when I google what
> my local ip address is.  I've configured firefox browser to use the proxy
> and have confirmed that it is using the proxy. Additionally https goes
> through the proxy however http is blocked. I'd like to ensure that http is
> not blocked.
> What changes do I need to make to get this working as desired?
> Here is the squid config setup:
> [root at CENTOSMASTER ~]# cat /etc/squid/squid.conf
> #
> # Recommended minimum configuration:
> #
> acl manager proto cache_object
> acl localhost src http://127.0.0.1/32 ::1
> acl to_localhost dst http://127.0.0.0/8 http://0.0.0.0/32 ::1
>
> # Example rule allowing access from your local networks.
> # Adapt to list your (internal) IP networks from where browsing
> # should be allowed
> acl localnet src http://10.0.0.0/8 # RFC1918 possible internal network
> acl localnet src http://172.16.0.0/12  # RFC1918 possible internal network
> acl localnet src http://192.168.0.0/16 # RFC1918 possible internal network
> acl localnet src fc00::/7       # RFC 4193 local private network range
> acl localnet src fe80::/10      # RFC 4291 link-local (directly plugged)
> machines
>
> acl SSL_ports port 443
> acl Safe_ports port 80      # http
> acl Safe_ports port 21      # ftp
> acl Safe_ports port 443     # https
> acl Safe_ports port 70      # gopher
> acl Safe_ports port 210     # wais
> acl Safe_ports port 1025-65535  # unregistered ports
> acl Safe_ports port 280     # http-mgmt
> acl Safe_ports port 488     # gss-http
> acl Safe_ports port 591     # filemaker
> acl Safe_ports port 777     # multiling http
> acl CONNECT method CONNECT
>
> #
> # Recommended minimum Access Permission configuration:
> #
> # Only allow cachemgr access from localhost
> http_access allow manager localhost
> http_access deny manager
>
> # Deny requests to certain unsafe ports
> http_access deny !Safe_ports
>
> # Deny CONNECT to other than secure SSL ports
> #http_access deny CONNECT !SSL_ports
>
> # We strongly recommend the following be uncommented to protect innocent
> # web applications running on the proxy server who think the only
> # one who can access services on "localhost" is a local user
> #http_access deny to_localhost
>
> #
> # INSERT YOUR OWN RULE(S) HERE TO ALLOW ACCESS FROM YOUR CLIENTS
> #
>
> # Example rule allowing access from your local networks.
> # Adapt localnet in the ACL section to list your (internal) IP networks
> # from where browsing should be allowed
> http_access allow localnet
> http_access allow localhost
>
> # And finally deny all other access to this proxy
> http_access deny all
>
> # Squid normally listens to port 3128
> http_port 3128
>
> # Uncomment and adjust the following to add a disk cache directory.
> #cache_dir ufs /var/spool/squid 100 16 256
>
> # Leave coredumps in the first cache dir
> coredump_dir /var/spool/squid
>
> # Add any of your own refresh_pattern entries above these.
> refresh_pattern ^ftp:       1440    20% 10080
> refresh_pattern ^gopher:    1440    0%  1440
> refresh_pattern -i (/cgi-bin/|\?) 0 0%  0
> refresh_pattern .       0   20%
>
> via off
> forwarded_for off
>
> request_header_access Allow allow all
> request_header_access Authorization allow all
> request_header_access WWW-Authenticate allow all
> request_header_access Proxy-Authorization allow all
> request_header_access Proxy-Authenticate allow all
> request_header_access Cache-Control allow all
> request_header_access Content-Encoding allow all
> request_header_access Content-Length allow all
> request_header_access Content-Type allow all
> request_header_access Date allow all
> request_header_access Expires allow all
> request_header_access Host allow all
> request_header_access If-Modified-Since allow all
> request_header_access Last-Modified allow all
> request_header_access Location allow all
> request_header_access Pragma allow all
> request_header_access Accept allow all
> request_header_access Accept-Charset allow all
> request_header_access Accept-Encoding allow all
> request_header_access Accept-Language allow all
> request_header_access Content-Language allow all
> request_header_access Mime-Version allow all
> request_header_access Retry-After allow all
> request_header_access Title allow all
> request_header_access Connection allow all
> request_header_access Proxy-Connection allow all
> request_header_access User-Agent allow all
> request_header_access Cookie allow all
> request_header_access All deny all
>
> I've posted the question on stackoverflow
>
> http://serverfault.com/questions/836385/anonymous-
> squid-setup-on-digital-ocean-centos-6-but-my-ip-still-detected
>
> Regards
> Alex Muir
> Chief Data Engineer/Architect
> Jahtoe Technology
> http://tech.jahtoe.com
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20170306/70050700/attachment.htm>

From vero.ovando at live.com  Tue Mar  7 13:35:41 2017
From: vero.ovando at live.com (=?gb2312?B?VmVyqK5uaWNhIE92YW5kbw==?=)
Date: Tue, 7 Mar 2017 13:35:41 +0000
Subject: [squid-users] ext_wbinfo_group_acl is not working
In-Reply-To: <CY1PR15MB090721EFB8012BD1933F26D69E2F0@CY1PR15MB0907.namprd15.prod.outlook.com>
References: <CY1PR15MB090721EFB8012BD1933F26D69E2F0@CY1PR15MB0907.namprd15.prod.outlook.com>
Message-ID: <CY1PR15MB0907F0E1B5FC0CE065483F299E2F0@CY1PR15MB0907.namprd15.prod.outlook.com>

Hi, everybody!


I have my Squid 3.4.8 running in Debian Jessie. It has been working with Active Directory authentication for more than a year without any kind of problem. But since a couple of weeks ago, suddenly, it stopped authenticate users, asking for credentials (username and pass) and they are not able to browse. I am getting this messages in /var/log/cache.log:


2017/03/04 12:04:25.806 kid1| WARNING: external ACL 'Grupos_AD' queue overload. Request rejected 'user1 it_group'.


After some research I found this thread http://www.squid-cache.org/mail-archive/squid-users/200902/0386.html and followed the suggestions posted by Amos. But nothing happened.

I tried rejoining the server to domain. Everything was fine in that way: wbinfo -u, wbinfo -g and wbinfo -P correctly returns all the users, groups and information of the domain.


After restart Squid service, I noticed that neither helper ext_wbinfo_group_acl nor pinger are started:


12:04:01 [root at server ]# systemctl status squid3.service -l
? squid3.service - LSB: Squid HTTP Proxy version 3.x
   Loaded: loaded (/etc/init.d/squid3)
   Active: active (running) since s?b 2017-03-04 12:04:01 ART; 3s ago
  Process: 4537 ExecStop=/etc/init.d/squid3 stop (code=exited, status=0/SUCCESS)
  Process: 4560 ExecStart=/etc/init.d/squid3 start (code=exited, status=0/SUCCESS)
   CGroup: /system.slice/squid3.service
           ??4593 /usr/sbin/squid3 -YC -f /etc/squid3/squid.conf
           ??4595 (squid-1) -YC -f /etc/squid3/squid.conf
           ??4596 (ntlm_auth) --helper-protocol=squid-2.5-ntlmssp --DOMAIN=MYDOMAIN
mar 04 12:04:01 server.mydomain.com squid3[4560]: Starting Squid HTTP Proxy 3.x: squid32017/03/04 12:04:01| WARNING: external_acl_type option children=N has been deprecated in favor of children-max=N and children-startup=N
mar 04 12:04:01 server.mydomain.com squid3[4593]: Squid Parent: will start 1 kids
mar 04 12:04:01 server.mydomain.com squid3[4593]: Squid Parent: (squid-1) process 4595 started
mar 04 12:04:01 server.mydomain.com squid3[4560]: .


12:04:30 [root at server ]# ps fax | grep ext_wbinfo_group_acl
 1418 pts/0    S+     0:00              \_ grep ext_wbinfo_group_acl

If I run echo "mydomain\user1 it_group" | /usr/lib/squid3/ext_wbinfo_group_acl -d, it returns

Debugging mode ON.
Got mydomain\user1 it_group from squid
User:  -mydomain\user1-
Group: -it_group-
SID:   -S-1-5-21-2290000000-711000000-3300000000-3949-
GID:   -10006-
Sending OK to squid
OK

What it's a good, because that user belongs to that group. If I change the group name, it returns an ERR.

Here is my squid.conf:

#===========================================================================
http_port 3128
visible_hostname proxy.squid
cache_mgr server at proxy.com<mailto:server at proxy.com>
cache_effective_user proxy
error_directory /usr/share/squid3/errors/es
err_page_stylesheet /etc/squid3/estilo.css

####################################################
#******************************Ports*************************************#
####################################################

#acl manager proto cache_object
#acl all src 0.0.0.0/0.0.0.0
#acl localhost src 127.0.0.1/32
acl SSL_ports port 443
acl Safe_ports port 80
acl Safe_ports port 21
acl Safe_ports port 443
acl Safe_ports port 70 #prot gopher
acl Safe_ports port 210 #whais
acl Safe_ports port 280 #http-mgmt
acl Safe_ports port 488 #gss-http
acl Safe_ports port 591 #filemaker
acl Safe_ports port 8080
acl Safe_ports port 2481
acl Safe_ports port 20010
acl Safe_ports port 777 #multi http
#acl purge method PURGE
acl CONNECT method CONNECT

acl_uses_indirect_client on
delay_pool_uses_indirect_client on
log_uses_indirect_client on


##############################################################
#*******************Active Directory HELPERS**************************#
##############################################################

auth_param ntlm program /usr/bin/ntlm_auth --helper-protocol=squid-2.5-ntlmssp --DOMAIN=MYDOMAIN
auth_param ntlm children 100
auth_param ntlm keep_alive off

auth_param basic program /usr/bin/ntlm_auth --helper-protocol=squid-2.5-basic
auth_param basic children 100
auth_param basic realm Servidor proxy-cache
auth_param basic credentialsttl 2 hours


#######################################################################
#****************************ACL******************************************#
###########################################################################

#---------------------------ACL Active Directory------------------------#
external_acl_type Grupos_AD ttl=10 negative_ttl=10 children=100 %LOGIN /usr/lib/squid3/ext_wbinfo_group_acl -d
acl it_group external Grupos_AD it_group

------------------Acceso s?lo a usuarios autenticados--------------------#
acl auth proxy_auth REQUIRED
http_access deny !auth

#-----------------------------Grupo *it_group*----------------------------#
http_access allow it_group allow

http_access allow manager localhost
http_access deny manager
#http_access allow purge localhost
#http_access deny purge
http_access deny !Safe_ports
http_access deny CONNECT !SSL_PORTS

http_access deny all

dead_peer_timeout 20 seconds
strip_query_terms on
debug_options ALL,1 33,2 28,9
coredump_dir /var/spool/squid3
ftp_passive on
ftp_sanitycheck off
ftp_telnet_protocol off
read_ahead_gap 1 MB
positive_dns_ttl 6 hours
forward_max_tries 25


############################################################################
#*************************Log********************************#
############################################################################

logformat squid %ts.%03tu %6tr %>a %Ss/%03>Hs %<st %rm %ru %un %Sh/%<A %mt
cache_access_log /var/log/squid3/access.log
cache_log /var/log/squid3/cache.log
logfile_rotate 0

############################################################################
#******************Cache and memory***************************#
############################################################################

cache_mem 1024 MB
maximum_object_size_in_memory 1024 KB
memory_cache_mode always
cache_dir aufs /var/spool/squid3 15000 16 256
maximum_object_size 96 MB
minimum_object_size 10 KB
#cache_replacement_policy heap LFUDA
cache_replacement_policy heap GDSF
memory_replacement_policy heap GDSF
#memory_replacement_policy lru
cache_store_log none
#log_fqdn off
log_icp_queries off
buffered_logs off
#emulate_httpd_log off
redirect_rewrites_host_header off
cache_swap_low 80
cache_swap_high 95

#===========================================================================

It is really weird, I really don't know how to solve this. I hope my explanation was clear.

For testing purposes, I have another Squid working with the same AD server, and it is going fine: the helper and pinger are executed as you can see here:

root at debian-test-server:/etc/squid3# systemctl status squid3.service
? squid3.service - LSB: Squid HTTP Proxy version 3.x
   Loaded: loaded (/etc/init.d/squid3)
   Active: active (running) since lun 2017-02-13 07:35:01 ART; 2 weeks 5 days ago
  Process: 570 ExecStart=/etc/init.d/squid3 start (code=exited, status=0/SUCCESS)
   CGroup: /system.slice/squid3.service
           ?? 1017 /usr/sbin/squid3 -YC -f /etc/squid3/squid.conf
           ?? 1020 (squid-1) -YC -f /etc/squid3/squid.conf
           ?? 1945 /usr/bin/perl -w /usr/lib/squid3/ext_wbinfo_group_acl -d
           ?? 1968 (ntlm_auth) --helper-protocol=squid-2.5-ntlmssp --DOMAIN=MYDOMAIN
           ?? 1969 (ntlm_auth) --helper-protocol=squid-2.5-ntlmssp --DOMAIN=MYDOMAIN
           ?? 1970 (ntlm_auth) --helper-protocol=squid-2.5-ntlmssp --DOMAIN=MYDOMAIN
           ?? 1971 (ntlm_auth) --helper-protocol=squid-2.5-ntlmssp --DOMAIN=MYDOMAIN
           ?? 1972 (ntlm_auth) --helper-protocol=squid-2.5-ntlmssp --DOMAIN=MYDOMAIN
           ?? 1973 (ntlm_auth) --helper-protocol=squid-2.5-ntlmssp --DOMAIN=MYDOMAIN
           ?? 1974 (ntlm_auth) --helper-protocol=squid-2.5-ntlmssp --DOMAIN=MYDOMAIN
           ?? 1993 /usr/bin/perl -w /usr/lib/squid3/ext_wbinfo_group_acl -d
           ?? 2029 /usr/bin/perl -w /usr/lib/squid3/ext_wbinfo_group_acl -d
           ??63477 (pinger)
           ??63478 (ntlm_auth) --helper-protocol=squid-2.5-ntlmssp --DOMAIN=MYDOMAIN
           ??63479 (ntlm_auth) --helper-protocol=squid-2.5-basic
           ??63480 /usr/bin/perl -w /usr/lib/squid3/ext_wbinfo_group_acl -d

I will appreciate your help!

Thanks!

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20170307/389a0ae8/attachment.htm>

From sothy.e98 at gmail.com  Tue Mar  7 14:05:52 2017
From: sothy.e98 at gmail.com (sothy shan)
Date: Tue, 7 Mar 2017 15:05:52 +0100
Subject: [squid-users] squid-4.0.18 error when running
Message-ID: <CAHcF_0Z+N7a=EtyL+VmRVdYX8KbYNzmXY3KC07+eh3NsJNWLaw@mail.gmail.com>

Hello,

I am trying to run <<sudo squid -d 2 -N -X >>, I got the followng error.

Why is this error ? Thanks for your help.


Best regards
Sothy

2017/03/07 15:02:48.221| 1,2| main.cc(1568) SquidMain: running
RegisteredRunner::finalizeConfig
2017/03/07 15:02:48.221| 1,2| main.cc(1569) SquidMain: running
RegisteredRunner::claimMemoryNeeds
2017/03/07 15:02:48.221| 1,2| main.cc(1570) SquidMain: running
RegisteredRunner::useConfig
2017/03/07 15:02:48.221| Not currently OK to rewrite swap log.
2017/03/07 15:02:48.221| storeDirWriteCleanLogs: Operation aborted.
FATAL: Ipc::Mem::Segment::create failed to
shm_open(/squid-cf__metadata.shm): (17) File exists
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20170307/a639262f/attachment.htm>

From mikes at surcouf.co.uk  Tue Mar  7 15:21:16 2017
From: mikes at surcouf.co.uk (Mike Surcouf)
Date: Tue, 7 Mar 2017 15:21:16 +0000
Subject: [squid-users] kerb auth groups KV note acl config
Message-ID: <2197768425D7F5479A0FFB3FEC212F7FF5D88ABD@aesmail.surcouf.local>

Outputting the groups as KV pairs in AD environments  on auth seems like a great performance enhancement and will allow me to ditch my ldap lookups.
Is there any docs on how to set this up?
Even looking at the source I can't seem to work it out.
I would like to test and potentially contribute to the DOCS although I am only a git user and bazaar would be new to me so I may just post my experience in this thread.

>From what I can see I need to setup a note acl but I am unsure of the key names etc.

A short example would be great.

Thanks

Mike


From harishmeetsu at yahoo.com  Tue Mar  7 22:16:15 2017
From: harishmeetsu at yahoo.com (Hareesh)
Date: Tue, 7 Mar 2017 14:16:15 -0800 (PST)
Subject: [squid-users] Setting Up Squid - my scenario
Message-ID: <1488924975216-4681702.post@n4.nabble.com>

Hi,

I have a requirement to setup Squid behind a corporate proxy. The corporate
proxy uses Kerberos for authentication and people will need to set their
systems/laptops/desktops to use a specified port and host name of that proxy
to connect to internet.

However to make our applications on various platforms connect to internet
seamlessly as needed, we decided to configure a proxy that forwards the
requests to corporate proxy with out taking any creds from its users. I am
looking for something similar to CNTLM but which can also support Kerberos
and be more stable. I am assuming that by using specific directives it is
possible to set a http proxy using squid that doesn't take user's details
but authenticates with parent proxy using its own creds and provide access
to internet.

Is this feasible to be implemented using Squid at the first place? If yes,
how can this be achieved? What are the directives that can be used as an
example? As a first time user, its highly confusing to understand how these
directives can be used in specific scenarios.

Thanks!




--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/Setting-Up-Squid-my-scenario-tp4681702.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From eliezer at ngtech.co.il  Wed Mar  8 01:19:56 2017
From: eliezer at ngtech.co.il (Eliezer  Croitoru)
Date: Wed, 8 Mar 2017 03:19:56 +0200
Subject: [squid-users] Getting lots of: "WARNING: HTTP: Invalid Response: No
	object data received for ..." in cache.log squid 3.5.24
Message-ID: <00db01d297aa$190b2140$4b2163c0$@ngtech.co.il>

Hey,

I am seeing lots of these lines in my cache.log:
2017/03/07 07:18:24 kid1| WARNING: HTTP: Invalid Response: No object data
received for
https://uk.zen.yandex.com/static/avatars/get-zen_doc/62191/db_52558842211898
87810/smart_crop_540x405 AKA
uk.zen.yandex.com/static/avatars/get-zen_doc/62191/db_5255884221189887810/sm
art_crop_540x405
2017/03/07 07:18:24 kid1| WARNING: HTTP: Invalid Response: No object data
received for
https://uk.zen.yandex.com/static/avatars/get-zen_doc/44972/db_-4158010171634
232351/smart_crop_540x405 AKA
uk.zen.yandex.com/static/avatars/get-zen_doc/44972/db_-4158010171634232351/s
mart_crop_540x405
2017/03/07 07:18:24 kid1| WARNING: HTTP: Invalid Response: No object data
received for
https://uk.zen.yandex.com/static/avatars/get-zen_doc/61795/db_-1328243198340
915340/smart_crop_540x405 AKA
uk.zen.yandex.com/static/avatars/get-zen_doc/61795/db_-1328243198340915340/s
mart_crop_540x405
2017/03/07 07:18:25 kid1| WARNING: HTTP: Invalid Response: No object data
received for
https://uk.zen.yandex.com/static/avatars/get-zen_doc/16074/rca_-664462727960
6228598/smart_crop_540x405 AKA
uk.zen.yandex.com/static/avatars/get-zen_doc/16074/rca_-6644627279606228598/
smart_crop_540x405
.

What does it mean?
What can I do to handle the issue?
It seems that if I will not turn these off my cache.log eventually will be
filled with them.

Thanks,
Eliezer

----
Eliezer Croitoru
Linux System Administrator
Mobile: +972-5-28704261
Email: eliezer at ngtech.co.il





From markus.rietzler at fv.nrw.de  Wed Mar  8 10:26:16 2017
From: markus.rietzler at fv.nrw.de (Rietzler, Markus (RZF, Aufg 324 / <RIETZLER_SOFTWARE>))
Date: Wed, 8 Mar 2017 10:26:16 +0000
Subject: [squid-users] microsoft edge and proxy auth not working
Message-ID: <1FCF9DA5B29068478ECF15896F19F08401B8AC0D2C@Y011008.bk.fin.local>

we have some windows 10 clients using microsoft edge browser.
access to internet is only allowed for authenticated users. we are using samba/winbind auth 

auth_param ntlm program /usr/bin/ntlm_auth --helper-protocol=squid-2.5-ntlmssp
auth_param ntlm children 64 startup=24 idle=12
auth_param ntlm keep_alive on
acl auth_user proxy_auth REQUIRED

on windows 10 clients with IE11 it is working (with ntlm automatic auth) on the same machine, with Microsoft edge I get TCP_Denied/407 message. seems I only get one single TCP_DENIED/407 line in accesslog and an auth dialog pops up. I have disabled basic auth via ntlm.
shouldn't there be 3 lines for proxy auth? with IE11 I see those three lines (2x TCP_DENIED/407 and 1x TCP_MISS/200), no popup at all.

winbind/samba itself seems to work, as I can do an user auth against apache with winbind/samba - even over some squid proxies with connection-auth allowed. but not for proxy-auth.
is there any option in squid.conf which prevents Edge to do a successful auth?


http://www.wuppertal-live.de/A0.gif
Mit freundlichen Gr??en

Markus Rietzler



From markus.rietzler at fv.nrw.de  Wed Mar  8 10:28:29 2017
From: markus.rietzler at fv.nrw.de (Rietzler, Markus (RZF, Aufg 324 / <RIETZLER_SOFTWARE>))
Date: Wed, 8 Mar 2017 10:28:29 +0000
Subject: [squid-users] microsoft edge and proxy auth not working
In-Reply-To: <1FCF9DA5B29068478ECF15896F19F08401B8AC0D2C@Y011008.bk.fin.local>
References: <1FCF9DA5B29068478ECF15896F19F08401B8AC0D2C@Y011008.bk.fin.local>
Message-ID: <1FCF9DA5B29068478ECF15896F19F08401B8AC0D3D@Y011008.bk.fin.local>

i should add that we are using squid 3.5.24.


> -----Urspr?ngliche Nachricht-----
> Von: squid-users [mailto:squid-users-bounces at lists.squid-cache.org] Im
> Auftrag von Rietzler, Markus (RZF, Aufg 324 / <RIETZLER_SOFTWARE>)
> Gesendet: Mittwoch, 8. M?rz 2017 11:26
> An: squid-users at lists.squid-cache.org
> Betreff: [squid-users] microsoft edge and proxy auth not working
> 
> we have some windows 10 clients using microsoft edge browser.
> access to internet is only allowed for authenticated users. we are using
> samba/winbind auth
> 
> auth_param ntlm program /usr/bin/ntlm_auth --helper-protocol=squid-2.5-
> ntlmssp
> auth_param ntlm children 64 startup=24 idle=12
> auth_param ntlm keep_alive on
> acl auth_user proxy_auth REQUIRED
> 
> on windows 10 clients with IE11 it is working (with ntlm automatic auth)
> on the same machine, with Microsoft edge I get TCP_Denied/407 message.
> seems I only get one single TCP_DENIED/407 line in accesslog and an auth
> dialog pops up. I have disabled basic auth via ntlm.
> shouldn't there be 3 lines for proxy auth? with IE11 I see those three
> lines (2x TCP_DENIED/407 and 1x TCP_MISS/200), no popup at all.
> 
> winbind/samba itself seems to work, as I can do an user auth against
> apache with winbind/samba - even over some squid proxies with
> connection-auth allowed. but not for proxy-auth.
> is there any option in squid.conf which prevents Edge to do a successful
> auth?
> 
> 
> http://www.wuppertal-live.de/A0.gif
> Mit freundlichen Gr??en
> 
> Markus Rietzler
> 
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users

From harishmeetsu at yahoo.com  Wed Mar  8 13:37:53 2017
From: harishmeetsu at yahoo.com (Hareesh)
Date: Wed, 8 Mar 2017 05:37:53 -0800 (PST)
Subject: [squid-users] Setting Up Squid - my scenario
In-Reply-To: <1488924975216-4681702.post@n4.nabble.com>
References: <1488924975216-4681702.post@n4.nabble.com>
Message-ID: <1488980273971-4681706.post@n4.nabble.com>

Any help?



--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/Setting-Up-Squid-my-scenario-tp4681702p4681706.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From squid3 at treenet.co.nz  Thu Mar  9 12:41:13 2017
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Fri, 10 Mar 2017 01:41:13 +1300
Subject: [squid-users] Fwd: reverse proxy HTTPS
In-Reply-To: <CAHcF_0a7Jpj-VXdtuLx6E8L7Psm_U959R=KAA338DVB8isuO4Q@mail.gmail.com>
References: <CAHcF_0b8PKGxJFobuE2NJd6r1eFGw9BXUf9nJX6_gCL3ma5OUA@mail.gmail.com>
 <CAHcF_0a7Jpj-VXdtuLx6E8L7Psm_U959R=KAA338DVB8isuO4Q@mail.gmail.com>
Message-ID: <801a89f8-5d1d-989a-386f-9985ea0cde47@treenet.co.nz>

On 6/03/2017 11:21 p.m., sothy shan wrote:
> Hi,
> 
> I can give precise what I am doing on this part.See the previous mail below
> for my exact requirement.
> 
> //create the keys.
> 
> $openssl req -new -keyout key.pem -nodes -x509 -days 365 -out cert.pem
> 
> Both keys(cert.pem and key.pem) are places in /etc/squid/.
> 
> Then, I make following in squid.
> +++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
> https_port 192.168.1.69:443 cert=/etc/squid/cert.pem key=/etc/squid/key.pem

The "accel" mode flag s missing.

It is that alone which makes squid a reverse-proxy. The rest of the
config details are 'agnostic' to the proxy type/mode.


> cache_peer X.Y.Z.Z parent 443 0 no-query originserver
> 
> 
> http_access allow all
> ++++++++++++++++++++++++++++++++++++++++++++++
> 
> When I type in browser like this https://192.168.1.69

Thats okay for a first test, but you should use a domain as soon as
possible so all the domain related validations have a chance to be tested.
 There are cert domain and SNI validations happening at the TLS/SSL
level, and there should also be dstdomain ACLs in squid.conf to ensure
only the wanted domains traffic gets handled by the proxy.

Amos



From sothy.e98 at gmail.com  Thu Mar  9 12:54:49 2017
From: sothy.e98 at gmail.com (sothy shan)
Date: Thu, 9 Mar 2017 13:54:49 +0100
Subject: [squid-users] Fwd: reverse proxy HTTPS
In-Reply-To: <801a89f8-5d1d-989a-386f-9985ea0cde47@treenet.co.nz>
References: <CAHcF_0b8PKGxJFobuE2NJd6r1eFGw9BXUf9nJX6_gCL3ma5OUA@mail.gmail.com>
 <CAHcF_0a7Jpj-VXdtuLx6E8L7Psm_U959R=KAA338DVB8isuO4Q@mail.gmail.com>
 <801a89f8-5d1d-989a-386f-9985ea0cde47@treenet.co.nz>
Message-ID: <CAHcF_0ZQt+70KiEox=G2oUw5ZQEKCZ+PZ3tSm==aCTZ5FAjsKQ@mail.gmail.com>

On Thu, Mar 9, 2017 at 1:41 PM, Amos Jeffries <squid3 at treenet.co.nz> wrote:

> On 6/03/2017 11:21 p.m., sothy shan wrote:
> > Hi,
> >
> > I can give precise what I am doing on this part.See the previous mail
> below
> > for my exact requirement.
> >
> > //create the keys.
> >
> > $openssl req -new -keyout key.pem -nodes -x509 -days 365 -out cert.pem
> >
> > Both keys(cert.pem and key.pem) are places in /etc/squid/.
> >
> > Then, I make following in squid.
> > +++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
> > https_port 192.168.1.69:443 cert=/etc/squid/cert.pem
> key=/etc/squid/key.pem
>
> The "accel" mode flag s missing.
>
> It is that alone which makes squid a reverse-proxy. The rest of the
> config details are 'agnostic' to the proxy type/mode.
>
Yes. I made it like that. It worked!

>
>
> > cache_peer X.Y.Z.Z parent 443 0 no-query originserver
> >
> >
> > http_access allow all
> > ++++++++++++++++++++++++++++++++++++++++++++++
> >
> > When I type in browser like this https://192.168.1.69
>
> Thats okay for a first test, but you should use a domain as soon as
> possible so all the domain related validations have a chance to be tested.
>  There are cert domain and SNI validations happening at the TLS/SSL
> level, and there should also be dstdomain ACLs in squid.conf to ensure
> only the wanted domains traffic gets handled by the proxy.
>
> Amos
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20170309/eeb89c95/attachment.htm>

From squid3 at treenet.co.nz  Thu Mar  9 13:25:02 2017
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Fri, 10 Mar 2017 02:25:02 +1300
Subject: [squid-users] squid 3.5.24 Host header forgery detected
In-Reply-To: <1488800622.869641941@f68.i.mail.ru>
References: <1488800622.869641941@f68.i.mail.ru>
Message-ID: <a2248aee-71e3-ef8e-99ea-70f8334913d4@treenet.co.nz>

On 7/03/2017 12:43 a.m., hellname hellname wrote:
> Hello. I have the squid 3.5.24 from source:
> configure options:  '--build=x86_64-linux-gnu' '--prefix=/usr' '--includedir=/include' '--mandir=/share/man' '--infodir=/share/info' '--sysconfdir=/etc' '--localstatedir=/var' '--libexecdir=/lib/squid' '--srcdir=.' '--disable-maintainer-mode' '--disable-dependency-tracking' '--disable-silent-rules' '--datadir=/usr/share/squid' '--sysconfdir=/etc/squid' '--mandir=/usr/share/man' '--enable-inline' '--disable-arch-native' '--enable-async-io=8' '--enable-storeio=ufs,aufs,diskd,rock' '--enable-removal-policies=lru,heap' '--enable-delay-pools' '--enable-cache-digests' '--enable-icap-client' '--enable-follow-x-forwarded-for' '--enable-auth-basic=DB,fake,getpwnam,LDAP,NCSA,NIS,PAM,POP3,RADIUS,SASL,SMB' '--enable-auth-digest=file,LDAP' '--enable-auth-negotiate=kerberos,wrapper' '--enable-auth-ntlm=fake,smb_lm' '--enable-external-acl-helpers=file_userip,kerberos_ldap_group,LDAP_group,session,SQL_session,unix_group,wbinfo_group' '--enable-url-rewrite-helpers=fake' '--enable-eui' '--enable-esi' '--enable-icmp' '--enable-zph-qos' '--enable-ecap' '--disable-translation' '--with-swapdir=/var/spool/squid' '--with-logdir=/var/log/squid' '--with-pidfile=/var/run/squid.pid' '--with-filedescriptors=65536' '--with-large-files' '--with-default-user=proxy' '--enable-ssl' '--enable-ssl-crtd' '--with-openssl' '--enable-linux-netfilter' 'CFLAGS=-g -O2 -fPIE -fstack-protector-strong -Wformat -Werror=format-security -Wall' 'LDFLAGS=-fPIE -pie -Wl,-z,relro -Wl,-z,now' 'CPPFLAGS=-D_FORTIFY_SOURCE=2' 'CXXFLAGS=-g -O2 -fPIE -fstack-protector-strong -Wformat -Werror=format-security' 'build_alias=x86_64-linux-gnu'
> 
> I also have this configuration:
> max_filedesc 35000
> acl localnet src 172.16.16.0/24
> acl localnet src 172.22.1.0/24
> acl SSL_ports port 443
> acl CONNECT method CONNECT
> acl AdminsIP src "/etc/squid/admips.txt"
> acl BlackList dstdomain "/etc/squid/BL.txt"
> acl BlockInetExlWhite src "/etc/squid/BI.txt"
> acl WhiteList dstdomain "/etc/squid/WL.txt"
> acl manager proto manager

"manager" ACL is now built-in. You can remove that incorrect definition.


> via off
> forwarded_for off
> follow_x_forwarded_for deny all

Remove the above line. It is defining a default, the only thing having
the above configured does is waste CPU calculating the header values.

> visible_hostname my.server.com
> hosts_file /etc/hosts

Above is also the default yes?

> dns_nameservers 172.16.16.11
> ipcache_size 10240
> negative_dns_ttl 5 minutes
> fqdncache_size 10240
> http_access deny !Safe_ports
> http_access deny CONNECT !SSL_ports
> http_access allow manager
> http_access deny manager

You already just "allow manager". The deny above will not work.

This looks like somebody got a bit confused about the current
best-practice when compared to old config files.

My 'best current practice' recommendation is for te top 4 http_access
lines to be:

 http_access deny !Safe_ports
 http_access deny CONNECT !SSL_ports
 http_access allow localhost
 http_access deny manager

.. naturally that only applies if an "allow localhost" is one of the
next http_access allow things done (see below).

If what you wanted was actually to allow *anybody* (literally anyone
including external to your network) to view your manager reports. Then
just remove that "deny manager" line.



> cachemgr_passwd password all
> http_access allow localhost
> http_access allow AdminsIP
> http_access deny BlockInetExlWhite !WhiteList
> http_access deny BlackList
> http_access allow localnet
> http_access deny all
> cache deny all
> http_port 0.0.0.0:3128 intercept
> http_port 0.0.0.0:3130
> https_port 0.0.0.0:3129 intercept ssl-bump generate-host-certificates=on dynamic_cert_mem_cache_size=4MB cert=/etc/squid/ssl/myCA.pem
> always_direct allow all

Remove that always_direct. It is a bug workaround which was only valid
for a whole 2 months back in 2012 or so for squid-3.1.

> sslproxy_cert_error allow all
> sslproxy_flags DONT_VERIFY_PEER

Please remove the above. I really cannot stress enough how bad it is. It
completely and utterly removes all security from HTTPS.

DONT_VERIFY_PEER is of almost zero benefit and allows several whole
classes of attacks to be performed (completely invisibly to you and your
client) by *other* people attacking your Internet connections. Then
"sslproxy_cert_error allow all" forbids Squid from informing anybody
(including you!) about anything suspicious Squid might still be able to
detect.

Yes errors will happen in TLS/SSL. You *need* to know what those are,
and for that you *need* Squid to be doing the peer/server verification.

_Some_ errors can be ignored when used by certain servers. That is what
sslproxy_cert_error exists for. Use it sparingly.


> acl blocked ssl::server_name "/etc/squid/BL.txt"
> acl step1 at_step SslBump1
> ssl_bump peek step1
> ssl_bump terminate blocked
> ssl_bump splice all
> sslcrtd_program /lib/squid/ssl_crtd -s /var/lib/ssl_db -M 4MB
> coredump_dir /var/spool/squid
> refresh_pattern ^ftp: 1440 20% 10080
> refresh_pattern ^gopher: 1440 0% 1440
> refresh_pattern -i (/cgi-bin/|\?) 0 0% 0
> refresh_pattern . 0 20% 4320
> maximum_object_size 61440 KB
> minimum_object_size 3 KB
> cache_swap_low 90
> cache_swap_high 95
> maximum_object_size_in_memory 512 KB
> memory_replacement_policy lru
> logfile_rotate 0
> cache_mgr  admin at my.server.com  
> 
> All users in my company have enabled auto-proxy-config in OS with wpad url configuration, below you can see wpad.dat:
> function FindProxyForURL(url, host) {
> var proxy_on = "PROXY 172.16.16.30:3130";
> var proxy_off = "DIRECT";
> var network = "172.16.16.0";
> var network1 = "192.168.100.0";
> var subnet = "255.255.255.0";
> 
> var proxy_bypass = new Array(
> "*.slack.com",
> "*.slack-edge.com",
> "*.slack-msgs.com",
> "*slack-files.com",
> "*slack-imgs.com",
> "*slack-edge.com",
> "*slack-core.com",
> "*slack-redir.net",
> "192.168.100.0",
> "127.0.0.1",
> "localhost"
> );
> if (isInNet(host, network, subnet)) {
> return proxy_off;
> }
> if (isInNet(host, network1, subnet)) {
> return proxy_off;
> }
> 
> for (var i = 0; i < proxy_bypass.length; i++) {
> if (shExpMatch(host, proxy_bypass[i])) {
> return DIRECT;

I spy a bug. " return proxy_off; " ? there is no local variable called
DIRECT in this PAC file.

> }
> }
> 
> if (shExpMatch(url, "http:*") ||
> shExpMatch(url, "https:*") ||
> shExpMatch(url, "ftp:*")) {
> return proxy_on;
> }
> 
> // Finally, send all other requests direct.
> return proxy_off;
> }
> 


> I see the problems when to try work with slack app or gmail apps (e.g.
calendar), in the squid log I see many entries:
> 2017/03/06 14:38:03| SECURITY ALERT: Host header forgery detected on local=54.230.99.128:443 remote=172.16.16.123:64857 FD 748 flags=33 (local IP does not match any domain IP)
> 2017/03/06 14:38:03| SECURITY ALERT: on URL: slack.com:443

When the client (172.16.16.123) fetches the URL "slack.com:443" the
server IP your NAT system tells Squid about (local=54.230.99.128:443) is
not listed in the DNS records for the domain slack.com.

This page explains the situation:
<http://wiki.squid-cache.org/KnowledgeBase/HostHeaderForgery>

Note that your PAC file sasy the proxy should not be used for that
domain. Yet still NAT'ing the traffic into ports 3128 and 3129. The
whole point of having a PAC file is to avoid NAT (and these types of
problem) in the first place.

Amos


From squid3 at treenet.co.nz  Thu Mar  9 13:33:17 2017
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Fri, 10 Mar 2017 02:33:17 +1300
Subject: [squid-users] Ssl bump tunneling connection by using Common Name
In-Reply-To: <002301d29698$7149c0e0$53dd42a0$@ngtech.co.il>
References: <CAHWcLuOGMmnPA=teUUSzycw_CFbfmMxQGp2XvjiaBHE52PyEBQ@mail.gmail.com>
 <002301d29698$7149c0e0$53dd42a0$@ngtech.co.il>
Message-ID: <9edf7e93-5472-444b-3575-a089eac80f0e@treenet.co.nz>

On 7/03/2017 5:41 a.m., Eliezer  Croitoru wrote:
> Hey,
> 
> There was something about it but I believe it's only on squid version 4.0.X.

FTR; Squid-4 brings the ability to tunnel Skype clients that were using
something that looked a bit like TLS but wasn't (along with the many
port 443 non-TLS uses). If the Skype clients are now actually using TLS
messages, that is no longer as useful.

Amos



From duanyao at ustc.edu  Thu Mar  9 14:00:03 2017
From: duanyao at ustc.edu (=?UTF-8?B?5q615Z6a?=)
Date: Thu, 9 Mar 2017 22:00:03 +0800
Subject: [squid-users] Can't start Squid with workers via systemd on CentOS 7
Message-ID: <1965c37d-0297-55a4-cbcc-09f9916237c5@ustc.edu>

I installed Squid 4.0.18 on CentOS 7 X86_84 according to 
http://wiki.squid-cache.org/KnowledgeBase/CentOS#Squid_Beta_release

I add a line "workers 2" to `/etc/squid/squid.conf` (just a copy of 
`squid.conf.default`)
and start squid via `systemctl start squid`, It seems squid is running, 
except that it
is not listening to the port 3128 (confirmed by netstat).

However, if I start squid in command line (`squid -sYC`), squid does 
listen to port 3128.

There are some differences in `/var/log/squid/cache.log` for these two 
cases:

* start via `systemctl start squid`:

   2017/03/09 21:12:54 kid3| commBind Cannot bind socket FD 12 to [::]: 
(98) Address already in use
   2017/03/09 21:12:54 kid2| commBind Cannot bind socket FD 21 to [::]: 
(98) Address already in use
   2017/03/09 21:12:54 kid1| commBind Cannot bind socket FD 21 to [::]: 
(98) Address already in use

* start via `squid -sYC`:

   2017/03/09 21:19:28 kid1| Accepting HTTP Socket connections at 
local=[::]:3128 remote=[::] FD 15 flags=1
   2017/03/09 21:19:28 kid2| Accepting HTTP Socket connections at 
local=[::]:3128 remote=[::] FD 15 flags=1

The full logs are attached. Please help.
-------------- next part --------------
2017/03/09 21:19:28| creating PID file: /var/run/squid.pid
2017/03/09 21:19:28 kid3| Set Current Directory to /var/spool/squid
2017/03/09 21:19:28 kid3| Starting Squid Cache version 4.0.18 for x86_64-redhat-linux-gnu...
2017/03/09 21:19:28 kid3| Service Name: squid
2017/03/09 21:19:28 kid3| Process ID 3082
2017/03/09 21:19:28 kid3| Process Roles: coordinator
2017/03/09 21:19:28 kid3| With 1024 file descriptors available
2017/03/09 21:19:28 kid3| Initializing IP Cache...
2017/03/09 21:19:28 kid1| Set Current Directory to /var/spool/squid
2017/03/09 21:19:28 kid1| Starting Squid Cache version 4.0.18 for x86_64-redhat-linux-gnu...
2017/03/09 21:19:28 kid1| Service Name: squid
2017/03/09 21:19:28 kid1| Process ID 3084
2017/03/09 21:19:28 kid1| Process Roles: worker
2017/03/09 21:19:28 kid1| With 1024 file descriptors available
2017/03/09 21:19:28 kid3| DNS Socket created at [::], FD 9
2017/03/09 21:19:28 kid1| Initializing IP Cache...
2017/03/09 21:19:28 kid1| DNS Socket created at [::], FD 12
2017/03/09 21:19:28 kid1| DNS Socket created at 0.0.0.0, FD 13
2017/03/09 21:19:28 kid1| Warning: Could not find any nameservers. Trying to use localhost
2017/03/09 21:19:28 kid2| Set Current Directory to /var/spool/squid
2017/03/09 21:19:28 kid3| DNS Socket created at 0.0.0.0, FD 10
2017/03/09 21:19:28 kid3| Warning: Could not find any nameservers. Trying to use localhost
2017/03/09 21:19:28 kid2| Starting Squid Cache version 4.0.18 for x86_64-redhat-linux-gnu...
2017/03/09 21:19:28 kid2| Service Name: squid
2017/03/09 21:19:28 kid3| Please check your /etc/resolv.conf file
2017/03/09 21:19:28 kid2| Process ID 3083
2017/03/09 21:19:28 kid3| or use the 'dns_nameservers' option in squid.conf.
2017/03/09 21:19:28 kid1| Please check your /etc/resolv.conf file
2017/03/09 21:19:28 kid2| Process Roles: worker
2017/03/09 21:19:28 kid1| or use the 'dns_nameservers' option in squid.conf.
2017/03/09 21:19:28 kid3| Logfile: opening log daemon:/var/log/squid/access.log
2017/03/09 21:19:28 kid2| With 1024 file descriptors available
2017/03/09 21:19:28 kid2| Initializing IP Cache...
2017/03/09 21:19:28 kid3| Logfile Daemon: opening log /var/log/squid/access.log
2017/03/09 21:19:28 kid2| DNS Socket created at [::], FD 12
2017/03/09 21:19:28 kid2| DNS Socket created at 0.0.0.0, FD 13
2017/03/09 21:19:28 kid2| Warning: Could not find any nameservers. Trying to use localhost
2017/03/09 21:19:28 kid1| Logfile: opening log daemon:/var/log/squid/access.log
2017/03/09 21:19:28 kid1| Logfile Daemon: opening log /var/log/squid/access.log
2017/03/09 21:19:28 kid2| Please check your /etc/resolv.conf file
2017/03/09 21:19:28 kid2| or use the 'dns_nameservers' option in squid.conf.
2017/03/09 21:19:28 kid3| Local cache digest enabled; rebuild/rewrite every 3600/3600 sec
2017/03/09 21:19:28 kid3| Store logging disabled
2017/03/09 21:19:28 kid3| Swap maxSize 0 + 262144 KB, estimated 20164 objects
2017/03/09 21:19:28 kid3| Target number of buckets: 1008
2017/03/09 21:19:28 kid3| Using 8192 Store buckets
2017/03/09 21:19:28 kid3| Max Mem  size: 262144 KB [shared]
2017/03/09 21:19:28 kid2| Logfile: opening log daemon:/var/log/squid/access.log
2017/03/09 21:19:28 kid3| Max Swap size: 0 KB
2017/03/09 21:19:28 kid3| Using Least Load store dir selection
2017/03/09 21:19:28 kid3| Set Current Directory to /var/spool/squid
2017/03/09 21:19:28 kid1| Local cache digest enabled; rebuild/rewrite every 3600/3600 sec
2017/03/09 21:19:28 kid1| Store logging disabled
2017/03/09 21:19:28 kid2| Logfile Daemon: opening log /var/log/squid/access.log
2017/03/09 21:19:28 kid1| Swap maxSize 0 + 262144 KB, estimated 20164 objects
2017/03/09 21:19:28 kid1| Target number of buckets: 1008
2017/03/09 21:19:28 kid1| Using 8192 Store buckets
2017/03/09 21:19:28 kid1| Max Mem  size: 262144 KB [shared]
2017/03/09 21:19:28 kid1| Max Swap size: 0 KB
2017/03/09 21:19:28 kid1| Using Least Load store dir selection
2017/03/09 21:19:28 kid2| Local cache digest enabled; rebuild/rewrite every 3600/3600 sec
2017/03/09 21:19:28 kid2| Store logging disabled
2017/03/09 21:19:28 kid2| Swap maxSize 0 + 262144 KB, estimated 20164 objects
2017/03/09 21:19:28 kid2| Target number of buckets: 1008
2017/03/09 21:19:28 kid2| Using 8192 Store buckets
2017/03/09 21:19:28 kid1| Set Current Directory to /var/spool/squid
2017/03/09 21:19:28 kid2| Max Mem  size: 262144 KB [shared]
2017/03/09 21:19:28 kid2| Max Swap size: 0 KB
2017/03/09 21:19:28 kid2| Using Least Load store dir selection
2017/03/09 21:19:28 kid2| Set Current Directory to /var/spool/squid
2017/03/09 21:19:28 kid3| Finished loading MIME types and icons.
2017/03/09 21:19:28 kid3| Squid plugin modules loaded: 0
2017/03/09 21:19:28 kid3| Adaptation support is off.
2017/03/09 21:19:28 kid1| Finished loading MIME types and icons.
2017/03/09 21:19:28 kid1| HTCP Disabled.
2017/03/09 21:19:28 kid1| Squid plugin modules loaded: 0
2017/03/09 21:19:28 kid1| Adaptation support is off.
2017/03/09 21:19:28 kid1| Accepting HTTP Socket connections at local=[::]:3128 remote=[::] FD 15 flags=1
2017/03/09 21:19:28 kid2| Finished loading MIME types and icons.
2017/03/09 21:19:28 kid2| HTCP Disabled.
2017/03/09 21:19:28 kid2| Squid plugin modules loaded: 0
2017/03/09 21:19:28 kid2| Adaptation support is off.
2017/03/09 21:19:28 kid2| Accepting HTTP Socket connections at local=[::]:3128 remote=[::] FD 15 flags=1
2017/03/09 21:19:29 kid3| storeLateRelease: released 0 objects
2017/03/09 21:19:29 kid1| storeLateRelease: released 0 objects
2017/03/09 21:19:29 kid2| storeLateRelease: released 0 objects
-------------- next part --------------
2017/03/09 21:12:54| creating PID file: /var/run/squid.pid
2017/03/09 21:12:54 kid3| Set Current Directory to /var/spool/squid
2017/03/09 21:12:54 kid2| Set Current Directory to /var/spool/squid
2017/03/09 21:12:54 kid1| Set Current Directory to /var/spool/squid
2017/03/09 21:12:54 kid1| Starting Squid Cache version 4.0.18 for x86_64-redhat-linux-gnu...
2017/03/09 21:12:54 kid3| Starting Squid Cache version 4.0.18 for x86_64-redhat-linux-gnu...
2017/03/09 21:12:54 kid2| Starting Squid Cache version 4.0.18 for x86_64-redhat-linux-gnu...
2017/03/09 21:12:54 kid1| Service Name: squid
2017/03/09 21:12:54 kid3| Service Name: squid
2017/03/09 21:12:54 kid2| Service Name: squid
2017/03/09 21:12:54 kid3| Process ID 2957
2017/03/09 21:12:54 kid1| Process ID 2959
2017/03/09 21:12:54 kid2| Process ID 2958
2017/03/09 21:12:54 kid3| Process Roles: coordinator
2017/03/09 21:12:54 kid1| Process Roles: worker
2017/03/09 21:12:54 kid2| Process Roles: worker
2017/03/09 21:12:54 kid3| With 16384 file descriptors available
2017/03/09 21:12:54 kid2| With 16384 file descriptors available
2017/03/09 21:12:54 kid1| With 16384 file descriptors available
2017/03/09 21:12:54 kid3| Initializing IP Cache...
2017/03/09 21:12:54 kid2| Initializing IP Cache...
2017/03/09 21:12:54 kid1| Initializing IP Cache...
2017/03/09 21:12:54 kid3| DNS Socket created at [::], FD 9
2017/03/09 21:12:54 kid3| DNS Socket created at 0.0.0.0, FD 10
2017/03/09 21:12:54 kid1| DNS Socket created at [::], FD 12
2017/03/09 21:12:54 kid1| DNS Socket created at 0.0.0.0, FD 13
2017/03/09 21:12:54 kid2| DNS Socket created at [::], FD 12
2017/03/09 21:12:54 kid2| DNS Socket created at 0.0.0.0, FD 13
2017/03/09 21:12:54 kid3| Warning: Could not find any nameservers. Trying to use localhost
2017/03/09 21:12:54 kid2| Warning: Could not find any nameservers. Trying to use localhost
2017/03/09 21:12:54 kid1| Warning: Could not find any nameservers. Trying to use localhost
2017/03/09 21:12:54 kid3| Please check your /etc/resolv.conf file
2017/03/09 21:12:54 kid2| Please check your /etc/resolv.conf file
2017/03/09 21:12:54 kid1| Please check your /etc/resolv.conf file
2017/03/09 21:12:54 kid3| or use the 'dns_nameservers' option in squid.conf.
2017/03/09 21:12:54 kid2| or use the 'dns_nameservers' option in squid.conf.
2017/03/09 21:12:54 kid1| or use the 'dns_nameservers' option in squid.conf.
2017/03/09 21:12:54 kid3| Logfile: opening log daemon:/var/log/squid/access.log
2017/03/09 21:12:54 kid3| Logfile Daemon: opening log /var/log/squid/access.log
2017/03/09 21:12:54 kid1| Logfile: opening log daemon:/var/log/squid/access.log
2017/03/09 21:12:54 kid1| Logfile Daemon: opening log /var/log/squid/access.log
2017/03/09 21:12:54 kid2| Logfile: opening log daemon:/var/log/squid/access.log
2017/03/09 21:12:54 kid2| Logfile Daemon: opening log /var/log/squid/access.log
2017/03/09 21:12:54 kid3| Local cache digest enabled; rebuild/rewrite every 3600/3600 sec
2017/03/09 21:12:54 kid3| Store logging disabled
2017/03/09 21:12:54 kid3| Swap maxSize 0 + 262144 KB, estimated 20164 objects
2017/03/09 21:12:54 kid3| Target number of buckets: 1008
2017/03/09 21:12:54 kid3| Using 8192 Store buckets
2017/03/09 21:12:54 kid3| Max Mem  size: 262144 KB [shared]
2017/03/09 21:12:54 kid3| Max Swap size: 0 KB
2017/03/09 21:12:54 kid3| Using Least Load store dir selection
2017/03/09 21:12:54 kid3| Set Current Directory to /var/spool/squid
2017/03/09 21:12:54 kid2| Local cache digest enabled; rebuild/rewrite every 3600/3600 sec
2017/03/09 21:12:54 kid2| Store logging disabled
2017/03/09 21:12:54 kid2| Swap maxSize 0 + 262144 KB, estimated 20164 objects
2017/03/09 21:12:54 kid2| Target number of buckets: 1008
2017/03/09 21:12:54 kid2| Using 8192 Store buckets
2017/03/09 21:12:54 kid2| Max Mem  size: 262144 KB [shared]
2017/03/09 21:12:54 kid2| Max Swap size: 0 KB
2017/03/09 21:12:54 kid2| Using Least Load store dir selection
2017/03/09 21:12:54 kid2| Set Current Directory to /var/spool/squid
2017/03/09 21:12:54 kid1| Local cache digest enabled; rebuild/rewrite every 3600/3600 sec
2017/03/09 21:12:54 kid1| Store logging disabled
2017/03/09 21:12:54 kid1| Swap maxSize 0 + 262144 KB, estimated 20164 objects
2017/03/09 21:12:54 kid1| Target number of buckets: 1008
2017/03/09 21:12:54 kid1| Using 8192 Store buckets
2017/03/09 21:12:54 kid1| Max Mem  size: 262144 KB [shared]
2017/03/09 21:12:54 kid1| Max Swap size: 0 KB
2017/03/09 21:12:54 kid1| Using Least Load store dir selection
2017/03/09 21:12:54 kid1| Set Current Directory to /var/spool/squid
2017/03/09 21:12:54 kid3| Finished loading MIME types and icons.
2017/03/09 21:12:54 kid3| Squid plugin modules loaded: 0
2017/03/09 21:12:54 kid3| Adaptation support is off.
2017/03/09 21:12:54 kid3| commBind Cannot bind socket FD 12 to [::]: (98) Address already in use
2017/03/09 21:12:54 kid2| Finished loading MIME types and icons.
2017/03/09 21:12:54 kid2| HTCP Disabled.
2017/03/09 21:12:54 kid2| Squid plugin modules loaded: 0
2017/03/09 21:12:54 kid2| Adaptation support is off.
2017/03/09 21:12:54 kid2| commBind Cannot bind socket FD 21 to [::]: (98) Address already in use
2017/03/09 21:12:54 kid1| Finished loading MIME types and icons.
2017/03/09 21:12:54 kid1| HTCP Disabled.
2017/03/09 21:12:54 kid1| Squid plugin modules loaded: 0
2017/03/09 21:12:54 kid1| Adaptation support is off.
2017/03/09 21:12:54 kid1| commBind Cannot bind socket FD 21 to [::]: (98) Address already in use
2017/03/09 21:12:55 kid3| storeLateRelease: released 0 objects
2017/03/09 21:12:55 kid2| storeLateRelease: released 0 objects
2017/03/09 21:12:55 kid1| storeLateRelease: released 0 objects

From squid3 at treenet.co.nz  Thu Mar  9 14:20:21 2017
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Fri, 10 Mar 2017 03:20:21 +1300
Subject: [squid-users] anonymous squid setup on digital ocean centos 6
 but my IP still detected
In-Reply-To: <CAFtPEJYVL7mmEbAGYAM_DpXyEr5gYm5EzYrKNN=8p8vMxtgPow@mail.gmail.com>
References: <CAFtPEJYVL7mmEbAGYAM_DpXyEr5gYm5EzYrKNN=8p8vMxtgPow@mail.gmail.com>
Message-ID: <d6bc7784-6799-f940-7ba0-970d8fb522f3@treenet.co.nz>

On 7/03/2017 12:31 a.m., Alex Muir wrote:
> Greetings,
> 
> I have implemented the following tutorial
> <https://www.digitalocean.com/community/tutorials/how-to-install-squid-proxy-on-centos-6>
> from
> digital ocean which sets up squid with settings to not have my IP address
> forwarded.
> 
> I'm finding however that my local IP is still detected when I google what
> my local ip address is.  I've configured firefox browser to use the proxy
> and have confirmed that it is using the proxy. Additionally https goes
> through the proxy however http is blocked. I'd like to ensure that http is
> not blocked.
> 
> What changes do I need to make to get this working as desired?

Firstly, you need to use an up-to-date version of Squid.

Secondly, configure with:
"
  via off
  forwarded_for transparent
"

Beyond those two lines there is nothing Squid can do to prevent active
scripts from reporting what your browser thinks its IP address is.
Eliezer already referred to html5 things. The browser itself is
betraying you just by running those scripts.

Note that the above config is *all* you need to do. The
request_header_access settings are not relevant. All they do is filter
custom headers the browser may be sending and Squid blindly relaying.
Which brings us to the HTTPS ...

When a browser is configured to use a proxy (like yours), HTTPS goes
through as a request to setup an opaque tunnel end-to-end. In other
words all the encrypted HTTP messages are direct between your browser
and the server.

So, the request_header_access filtering which you are having the proxy
do is useless and not doing a thing to those encrypted messages.
"working" there means the server has complete visibility into the client
browser but cannot see the proxy - except that the 'client IP' the
server sees is that of the proxy. Which is what the above two mentioned
config lines make happen for the HTTP traffic.


Amos



From uhlar at fantomas.sk  Thu Mar  9 14:21:34 2017
From: uhlar at fantomas.sk (Matus UHLAR - fantomas)
Date: Thu, 9 Mar 2017 15:21:34 +0100
Subject: [squid-users] squid workers question
Message-ID: <20170309142134.GA30976@fantomas.sk>

Hello,

I have installed squid 3.4.8 on linux 3.16/64bit (debian 8 / jessie version)

(I know it's old, but I prefer using distribution-provided SW unless it has
real problem distribution isn't able to fix)

- does this version have known memory leaks?
http://www.squid-cache.org/Versions/v3/3.5/ChangeLog.txt
shows some leaks fixed but they all seem to be related to something we don't
use (certificated, Surrogate capability), unless the:

"Fix memory leak of HttpRequest objects" 
that is fixed in 3.5.16 applies to 3.4 too.


I configured rock store (for smaller files) and (later) standard aufs for others:

cache_dir rock /var/spool/squid3/rock 1024 max-size=32768
#cache_dir aufs /var/spool/squid3 8192 16 256 min-size=32769

are those correct values? (bug 3411 says something about 256B metadata)


logs said this:

2017/03/02 18:32:15 kid1| /var/spool/squid3 exists
...
2017/03/02 18:32:18 kid3| Swap maxSize 0 + 262144 KB, estimated 20164 objects
2017/03/02 18:32:18 kid2| Swap maxSize 1048576 + 262144 KB, estimated 100824 objects

- do I get it right that kid1 is the Master, kid2 is the disker for rock
   store and kid3 is the single worker process?


After first start I noticed crash:

2017/03/02 18:32:18 kid3| Max Mem  size: 262144 KB [shared]
2017/03/02 18:32:18 kid2| Max Mem  size: 262144 KB [shared]
2017/03/02 18:32:18 kid3| Max Swap size: 0 KB
2017/03/02 18:32:18 kid1| WARNING: disk-cache maximum object size is too large for mem-cache: 16384.00 KB > 32.00 KB
2017/03/02 18:32:18 kid2| Max Swap size: 1048576 KB
2017/03/02 18:32:18 kid3| Using Least Load store dir selection
2017/03/02 18:32:18 kid3| Set Current Directory to /var/spool/squid3
FATAL: Ipc::Mem::Segment::open failed to shm_open(/squid-cache_mem.shm): (2) No such file or directory

Squid Cache (Version 3.4.8): Terminated abnormally.
FATAL: Ipc::Mem::Segment::open failed to
shm_open(/squid-var.spool.squid3.rock.shm): (2) No such file or directory

Squid Cache (Version 3.4.8): Terminated abnormally.


... this happened in http://bugs.squid-cache.org/show_bug.cgi?id=3411
however that 

- restart with "workers 1" worked, but isn't that the default?
   or was the creash caused by something else?
(will try to replicate)



-- 
Matus UHLAR - fantomas, uhlar at fantomas.sk ; http://www.fantomas.sk/
Warning: I wish NOT to receive e-mail advertising to this address.
Varovanie: na tuto adresu chcem NEDOSTAVAT akukolvek reklamnu postu.
Fucking windows! Bring Bill Gates! (Southpark the movie)


From harishmeetsu at yahoo.com  Thu Mar  9 14:23:27 2017
From: harishmeetsu at yahoo.com (Hareesh)
Date: Thu, 9 Mar 2017 06:23:27 -0800 (PST)
Subject: [squid-users] Login=PASS   --> Query
Message-ID: <1489069407227-4681714.post@n4.nabble.com>

Hi

I have a query related to the option Login=PASS in cache-peer. The
documentation mentions the following.

	login=PASS	Send login details received from client to this peer.
			Authentication is not required by this option.
			
			*If there are no client-provided authentication headers
			to pass on, but username and password are available
			from an external ACL user= and password= result tags
			they may be sent instead.*

I want to understand what do they exactly mean by the text given *bold*?
Where and how can the user and password be given as acls.

Can some one please shed some light with possible example?

Thanks





--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/Login-PASS-Query-tp4681714.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From uhlar at fantomas.sk  Thu Mar  9 14:32:24 2017
From: uhlar at fantomas.sk (Matus UHLAR - fantomas)
Date: Thu, 9 Mar 2017 15:32:24 +0100
Subject: [squid-users] cachemgr CGI version compatibility
Message-ID: <20170309143223.GB30976@fantomas.sk>

Hello,

will older cachemgr.cgi work well with newer squid?


-- 
Matus UHLAR - fantomas, uhlar at fantomas.sk ; http://www.fantomas.sk/
Warning: I wish NOT to receive e-mail advertising to this address.
Varovanie: na tuto adresu chcem NEDOSTAVAT akukolvek reklamnu postu.
There's a long-standing bug relating to the x86 architecture that
allows you to install Windows.   -- Matthew D. Fuller


From squid3 at treenet.co.nz  Thu Mar  9 14:46:00 2017
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Fri, 10 Mar 2017 03:46:00 +1300
Subject: [squid-users] ext_wbinfo_group_acl is not working
In-Reply-To: <CY1PR15MB0907F0E1B5FC0CE065483F299E2F0@CY1PR15MB0907.namprd15.prod.outlook.com>
References: <CY1PR15MB090721EFB8012BD1933F26D69E2F0@CY1PR15MB0907.namprd15.prod.outlook.com>
 <CY1PR15MB0907F0E1B5FC0CE065483F299E2F0@CY1PR15MB0907.namprd15.prod.outlook.com>
Message-ID: <4ef48a6a-df57-09b9-2a84-39c7b9ee6c0c@treenet.co.nz>

On 8/03/2017 2:35 a.m., Ver?nica Ovando wrote:
> Hi, everybody!
> 
> 
> I have my Squid 3.4.8 running in Debian Jessie. It has been working with Active Directory authentication for more than a year without any kind of problem. But since a couple of weeks ago, suddenly, it stopped authenticate users, asking for credentials (username and pass) and they are not able to browse. I am getting this messages in /var/log/cache.log:
> 
> 
> 2017/03/04 12:04:25.806 kid1| WARNING: external ACL 'Grupos_AD' queue overload. Request rejected 'user1 it_group'.
> 

This means that your AD is not keeping up with the traffic through your
proxy.
Since your Squid has children=100 it will queue up to 200 transactions
waiting for the helper before this message is shown.


Dis cache.log have anything else from the external helper? you have
debug mode enabled (-d) so it should be reporting if there are any
issues with AD other than simply slowness.


> 
> After some research I found this thread http://www.squid-cache.org/mail-archive/squid-users/200902/0386.html and followed the suggestions posted by Amos. But nothing happened.
> 
> I tried rejoining the server to domain. Everything was fine in that way: wbinfo -u, wbinfo -g and wbinfo -P correctly returns all the users, groups and information of the domain.
> 
> 
> After restart Squid service, I noticed that neither helper ext_wbinfo_group_acl nor pinger are started:
> 
> 
> 12:04:01 [root at server ]# systemctl status squid3.service -l

NOTE: do not trust systemd information about Squid-3. The two are not
compatible and systemd often says incorrect things because it makes
incorrect assumptions about the squid process(es). Especially if there
has been a process crash and auto-restart at any point during Squid
operation.


> ? squid3.service - LSB: Squid HTTP Proxy version 3.x
>    Loaded: loaded (/etc/init.d/squid3)
>    Active: active (running) since s?b 2017-03-04 12:04:01 ART; 3s ago
>   Process: 4537 ExecStop=/etc/init.d/squid3 stop (code=exited, status=0/SUCCESS)
>   Process: 4560 ExecStart=/etc/init.d/squid3 start (code=exited, status=0/SUCCESS)
>    CGroup: /system.slice/squid3.service
>            ??4593 /usr/sbin/squid3 -YC -f /etc/squid3/squid.conf
>            ??4595 (squid-1) -YC -f /etc/squid3/squid.conf
>            ??4596 (ntlm_auth) --helper-protocol=squid-2.5-ntlmssp --DOMAIN=MYDOMAIN
> mar 04 12:04:01 server.mydomain.com squid3[4560]: Starting Squid HTTP Proxy 3.x: squid3
> 2017/03/04 12:04:01| WARNING: external_acl_type option children=N has been deprecated in favor of children-max=N and children-startup=N

Please note the warning and update your config file.

> mar 04 12:04:01 server.mydomain.com squid3[4593]: Squid Parent: will start 1 kids
> mar 04 12:04:01 server.mydomain.com squid3[4593]: Squid Parent: (squid-1) process 4595 started
> mar 04 12:04:01 server.mydomain.com squid3[4560]: .
> 
> 
> 12:04:30 [root at server ]# ps fax | grep ext_wbinfo_group_acl
>  1418 pts/0    S+     0:00              \_ grep ext_wbinfo_group_acl
> 
> If I run echo "mydomain\user1 it_group" | /usr/lib/squid3/ext_wbinfo_group_acl -d, it returns
> 
> Debugging mode ON.
> Got mydomain\user1 it_group from squid
> User:  -mydomain\user1-
> Group: -it_group-
> SID:   -S-1-5-21-2290000000-711000000-3300000000-3949-
> GID:   -10006-
> Sending OK to squid
> OK
> 
> What it's a good, because that user belongs to that group. If I change the group name, it returns an ERR.
> 
> Here is my squid.conf:
> 
> #===========================================================================
> http_port 3128
> visible_hostname proxy.squid
> cache_mgr server at proxy.com
> cache_effective_user proxy
> error_directory /usr/share/squid3/errors/es
> err_page_stylesheet /etc/squid3/estilo.css
> 
> ####################################################
> #******************************Ports*************************************#
> ####################################################
> 
> #acl manager proto cache_object
> #acl all src 0.0.0.0/0.0.0.0
> #acl localhost src 127.0.0.1/32
> acl SSL_ports port 443
> acl Safe_ports port 80
> acl Safe_ports port 21
> acl Safe_ports port 443
> acl Safe_ports port 70 #prot gopher
> acl Safe_ports port 210 #whais
> acl Safe_ports port 280 #http-mgmt
> acl Safe_ports port 488 #gss-http
> acl Safe_ports port 591 #filemaker
> acl Safe_ports port 8080
> acl Safe_ports port 2481
> acl Safe_ports port 20010
> acl Safe_ports port 777 #multi http
> #acl purge method PURGE
> acl CONNECT method CONNECT
> 
> acl_uses_indirect_client on
> delay_pool_uses_indirect_client on
> log_uses_indirect_client on
> 
> 
> ##############################################################
> #*******************Active Directory HELPERS**************************#
> ##############################################################
> 
> auth_param ntlm program /usr/bin/ntlm_auth --helper-protocol=squid-2.5-ntlmssp --DOMAIN=MYDOMAIN
> auth_param ntlm children 100
> auth_param ntlm keep_alive off
> 
> auth_param basic program /usr/bin/ntlm_auth --helper-protocol=squid-2.5-basic
> auth_param basic children 100
> auth_param basic realm Servidor proxy-cache
> auth_param basic credentialsttl 2 hours
> 
> 
> #######################################################################
> #****************************ACL******************************************#
> ###########################################################################
> 
> #---------------------------ACL Active Directory------------------------#
> external_acl_type Grupos_AD ttl=10 negative_ttl=10 children=100 %LOGIN /usr/lib/squid3/ext_wbinfo_group_acl -d
> acl it_group external Grupos_AD it_group
> 
> ------------------Acceso s?lo a usuarios autenticados--------------------#
> acl auth proxy_auth REQUIRED
> http_access deny !auth
> 
> #-----------------------------Grupo *it_group*----------------------------#
> http_access allow it_group allow

What is this extra "allow" on the end of the line for?

I dont see any ACL named "allow" in the above config. So that may be
preventing Squid from restarting, which would confuse systemd.


> 
> http_access allow manager localhost
> http_access deny manager
> #http_access allow purge localhost
> #http_access deny purge

Please move the below two lines up to be the very first http_access
lines in your config. Part of their purpose is to protect against some
DoS conditions which can cause exactly this type of overload on headers.

> http_access deny !Safe_ports
> http_access deny CONNECT !SSL_PORTS
> 
> http_access deny all
> 
> dead_peer_timeout 20 seconds
> strip_query_terms on
> debug_options ALL,1 33,2 28,9
> coredump_dir /var/spool/squid3
> ftp_passive on
> ftp_sanitycheck off
> ftp_telnet_protocol off
> read_ahead_gap 1 MB
> positive_dns_ttl 6 hours
> forward_max_tries 25
> 
> 
> ############################################################################
> #*************************Log********************************#
> ############################################################################
> 
> logformat squid %ts.%03tu %6tr %>a %Ss/%03>Hs %<st %rm %ru %un %Sh/%<A %mt
> cache_access_log /var/log/squid3/access.log
> cache_log /var/log/squid3/cache.log
> logfile_rotate 0
> 
> ############################################################################
> #******************Cache and memory***************************#
> ############################################################################
> 
> cache_mem 1024 MB
> maximum_object_size_in_memory 1024 KB
> memory_cache_mode always
> cache_dir aufs /var/spool/squid3 15000 16 256
> maximum_object_size 96 MB
> minimum_object_size 10 KB
> #cache_replacement_policy heap LFUDA
> cache_replacement_policy heap GDSF
> memory_replacement_policy heap GDSF
> #memory_replacement_policy lru
> cache_store_log none
> #log_fqdn off
> log_icp_queries off
> buffered_logs off
> #emulate_httpd_log off
> redirect_rewrites_host_header off
> cache_swap_low 80
> cache_swap_high 95
> 
> #===========================================================================
> 
> It is really weird, I really don't know how to solve this. I hope my explanation was clear.
> 
> For testing purposes, I have another Squid working with the same AD server, and it is going fine: the helper and pinger are executed as you can see here:
> 
> root at debian-test-server:/etc/squid3# systemctl status squid3.service
> ? squid3.service - LSB: Squid HTTP Proxy version 3.x
>    Loaded: loaded (/etc/init.d/squid3)
>    Active: active (running) since lun 2017-02-13 07:35:01 ART; 2 weeks 5 days ago
>   Process: 570 ExecStart=/etc/init.d/squid3 start (code=exited, status=0/SUCCESS)
>    CGroup: /system.slice/squid3.service
>            ?? 1017 /usr/sbin/squid3 -YC -f /etc/squid3/squid.conf
>            ?? 1020 (squid-1) -YC -f /etc/squid3/squid.conf
>            ?? 1945 /usr/bin/perl -w /usr/lib/squid3/ext_wbinfo_group_acl -d
>            ?? 1968 (ntlm_auth) --helper-protocol=squid-2.5-ntlmssp --DOMAIN=MYDOMAIN
>            ?? 1969 (ntlm_auth) --helper-protocol=squid-2.5-ntlmssp --DOMAIN=MYDOMAIN
>            ?? 1970 (ntlm_auth) --helper-protocol=squid-2.5-ntlmssp --DOMAIN=MYDOMAIN
>            ?? 1971 (ntlm_auth) --helper-protocol=squid-2.5-ntlmssp --DOMAIN=MYDOMAIN
>            ?? 1972 (ntlm_auth) --helper-protocol=squid-2.5-ntlmssp --DOMAIN=MYDOMAIN
>            ?? 1973 (ntlm_auth) --helper-protocol=squid-2.5-ntlmssp --DOMAIN=MYDOMAIN
>            ?? 1974 (ntlm_auth) --helper-protocol=squid-2.5-ntlmssp --DOMAIN=MYDOMAIN
>            ?? 1993 /usr/bin/perl -w /usr/lib/squid3/ext_wbinfo_group_acl -d
>            ?? 2029 /usr/bin/perl -w /usr/lib/squid3/ext_wbinfo_group_acl -d
>            ??63477 (pinger)
>            ??63478 (ntlm_auth) --helper-protocol=squid-2.5-ntlmssp --DOMAIN=MYDOMAIN
>            ??63479 (ntlm_auth) --helper-protocol=squid-2.5-basic
>            ??63480 /usr/bin/perl -w /usr/lib/squid3/ext_wbinfo_group_acl -d
> 

As configured your Squid should be starting exactly 100 of each - no
more, no less. I suspect from both these traces that you dont actually
need 100 of each helper running, or systemd is confused already.

The current Squid versions can auto-start helpers as needed. See the
auth_param and external_acl_type documentation for the max=, startup=
and idle= options. That may help a little, or at least allow you to
configure higher max limits to cope with slow AD periods.



Another possibility is converting to the LDAP group lookup instead of
using the wbinfo tool to do lookups. I know that LDAP does not suffer
from wbind connection limits, which might be part of your issue.

Amos



From squid3 at treenet.co.nz  Thu Mar  9 14:56:35 2017
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Fri, 10 Mar 2017 03:56:35 +1300
Subject: [squid-users] squid-4.0.18 error when running
In-Reply-To: <CAHcF_0Z+N7a=EtyL+VmRVdYX8KbYNzmXY3KC07+eh3NsJNWLaw@mail.gmail.com>
References: <CAHcF_0Z+N7a=EtyL+VmRVdYX8KbYNzmXY3KC07+eh3NsJNWLaw@mail.gmail.com>
Message-ID: <167701a6-7d94-f7c3-153a-55c618e8cc40@treenet.co.nz>

On 8/03/2017 3:05 a.m., sothy shan wrote:
> Hello,
> 
> I am trying to run <<sudo squid -d 2 -N -X >>, I got the followng error.
> 
> Why is this error ? Thanks for your help.
> 
> 
> Best regards
> Sothy
> 
> 2017/03/07 15:02:48.221| 1,2| main.cc(1568) SquidMain: running
> RegisteredRunner::finalizeConfig
> 2017/03/07 15:02:48.221| 1,2| main.cc(1569) SquidMain: running
> RegisteredRunner::claimMemoryNeeds
> 2017/03/07 15:02:48.221| 1,2| main.cc(1570) SquidMain: running
> RegisteredRunner::useConfig
> 2017/03/07 15:02:48.221| Not currently OK to rewrite swap log.
> 2017/03/07 15:02:48.221| storeDirWriteCleanLogs: Operation aborted.
> FATAL: Ipc::Mem::Segment::create failed to
> shm_open(/squid-cf__metadata.shm): (17) File exists
>>

Several things:

1) The -N option is not compatible with SMP operations and auto-disables
these shared memory things in current Squid.

  - is your version a current 3.5.* ?


2) some other Squid process is either still running.

 - fix should be obvious. Stop the other Squid.,

 - or use -n option to run a sandboxed instance with a different process
namespace.


3) previously Squid crashed leaving the SHM sockets open.

 - simply deleting the /dev/shm/squid-*.shm should fix the issue.

 - And an upgrade to a current Squid-3.5 which can handle these
leftovers would be best.


Amos



From rousskov at measurement-factory.com  Thu Mar  9 14:57:55 2017
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Thu, 9 Mar 2017 07:57:55 -0700
Subject: [squid-users] Can't start Squid with workers via systemd on
 CentOS 7
In-Reply-To: <1965c37d-0297-55a4-cbcc-09f9916237c5@ustc.edu>
References: <1965c37d-0297-55a4-cbcc-09f9916237c5@ustc.edu>
Message-ID: <5b702ed3-fc97-3c98-4b01-0892f9a04f09@measurement-factory.com>

On 03/09/2017 07:00 AM, ?? wrote:
> I installed Squid 4.0.18 on CentOS 7 X86_84 according to
> http://wiki.squid-cache.org/KnowledgeBase/CentOS#Squid_Beta_release
> 
> I add a line "workers 2" to `/etc/squid/squid.conf` (just a copy of
> `squid.conf.default`)
> and start squid via `systemctl start squid`, It seems squid is running,
> except that it is not listening to the port 3128 (confirmed by netstat).
> 
> However, if I start squid in command line (`squid -sYC`), squid does
> listen to port 3128.
> 
> There are some differences in `/var/log/squid/cache.log` for these two
> cases:
> 
> * start via `systemctl start squid`:
> 
>   2017/03/09 21:12:54 kid3| commBind Cannot bind socket FD 12 to [::]: (98) Address already in use
>   2017/03/09 21:12:54 kid2| commBind Cannot bind socket FD 21 to [::]: (98) Address already in use
>   2017/03/09 21:12:54 kid1| commBind Cannot bind socket FD 21 to [::]: (98) Address already in use
> 
> * start via `squid -sYC`:
> 
>   2017/03/09 21:19:28 kid1| Accepting HTTP Socket connections at local=[::]:3128 remote=[::] FD 15 flags=1
>   2017/03/09 21:19:28 kid2| Accepting HTTP Socket connections at local=[::]:3128 remote=[::] FD 15 flags=1
> 
> The full logs are attached. Please help.

Most likely, your systemctl script for Squid is broken. I do not know
about CentOS 7 specifically, but many older scripts often start two
modern Squids at once due to various backward compatibility problems
with Squid startup interface and the way those scripts were written.
Study/log what that script does and you will probably find the answer to
your question.

HTH,

Alex.



From mikes at surcouf.co.uk  Thu Mar  9 15:06:31 2017
From: mikes at surcouf.co.uk (Mike Surcouf)
Date: Thu, 9 Mar 2017 15:06:31 +0000
Subject: [squid-users] kerb auth groups KV note acl config
In-Reply-To: <2197768425D7F5479A0FFB3FEC212F7FF5D88ABD@aesmail.surcouf.local>
References: <2197768425D7F5479A0FFB3FEC212F7FF5D88ABD@aesmail.surcouf.local>
Message-ID: <2197768425D7F5479A0FFB3FEC212F7FF5D8C9DE@aesmail.surcouf.local>

@Markus

I would really like to give this a go.
Good to get some people using this stuff

-----Original Message-----
From: squid-users [mailto:squid-users-bounces at lists.squid-cache.org] On Behalf Of Mike Surcouf
Sent: 07 March 2017 15:21
To: 'squid-users at lists.squid-cache.org'
Subject: [squid-users] kerb auth groups KV note acl config

Outputting the groups as KV pairs in AD environments  on auth seems like a great performance enhancement and will allow me to ditch my ldap lookups.
Is there any docs on how to set this up?
Even looking at the source I can't seem to work it out.
I would like to test and potentially contribute to the DOCS although I am only a git user and bazaar would be new to me so I may just post my experience in this thread.

From what I can see I need to setup a note acl but I am unsure of the key names etc.

A short example would be great.

Thanks

Mike
_______________________________________________
squid-users mailing list
squid-users at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-users

From squid3 at treenet.co.nz  Thu Mar  9 15:14:53 2017
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Fri, 10 Mar 2017 04:14:53 +1300
Subject: [squid-users] cachemgr CGI version compatibility
In-Reply-To: <20170309143223.GB30976@fantomas.sk>
References: <20170309143223.GB30976@fantomas.sk>
Message-ID: <e40602b9-8f31-acac-dabb-5c3181fdff9b@treenet.co.nz>

On 10/03/2017 3:32 a.m., Matus UHLAR - fantomas wrote:
> Hello,
> 
> will older cachemgr.cgi work well with newer squid?
> 

Yes they should. Likewise the newer cachemgr.cgi should work as well
with older Squid. The tool and Squid are explicitly being kept both
forward and backward compatible.


But be aware that cachemgr.cgi older than 3.5.17 may be vulnerable to
<http://www.squid-cache.org/Advisories/SQUID-2016_5.txt>- which means
they cannot safely handle some reports (as listed in the advisory).

And if you are talking *very* old CGI version maybe
<http://www.squid-cache.org/Advisories/SQUID-2012_1.txt> as well, which
is somewhat worse.

Amos



From squid3 at treenet.co.nz  Thu Mar  9 15:22:57 2017
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Fri, 10 Mar 2017 04:22:57 +1300
Subject: [squid-users] Can't start Squid with workers via systemd on
 CentOS 7
In-Reply-To: <5b702ed3-fc97-3c98-4b01-0892f9a04f09@measurement-factory.com>
References: <1965c37d-0297-55a4-cbcc-09f9916237c5@ustc.edu>
 <5b702ed3-fc97-3c98-4b01-0892f9a04f09@measurement-factory.com>
Message-ID: <5391c2bb-7539-ee9f-8658-286725c398fd@treenet.co.nz>

On 10/03/2017 3:57 a.m., Alex Rousskov wrote:
> On 03/09/2017 07:00 AM, ?? wrote:
>> I installed Squid 4.0.18 on CentOS 7 X86_84 according to
>> http://wiki.squid-cache.org/KnowledgeBase/CentOS#Squid_Beta_release
>>
>> I add a line "workers 2" to `/etc/squid/squid.conf` (just a copy of
>> `squid.conf.default`)
>> and start squid via `systemctl start squid`, It seems squid is running,
>> except that it is not listening to the port 3128 (confirmed by netstat).
>>
>> However, if I start squid in command line (`squid -sYC`), squid does
>> listen to port 3128.
>>
>> There are some differences in `/var/log/squid/cache.log` for these two
>> cases:
>>
>> * start via `systemctl start squid`:
>>
>>   2017/03/09 21:12:54 kid3| commBind Cannot bind socket FD 12 to [::]: (98) Address already in use
>>   2017/03/09 21:12:54 kid2| commBind Cannot bind socket FD 21 to [::]: (98) Address already in use
>>   2017/03/09 21:12:54 kid1| commBind Cannot bind socket FD 21 to [::]: (98) Address already in use
>>
>> * start via `squid -sYC`:
>>
>>   2017/03/09 21:19:28 kid1| Accepting HTTP Socket connections at local=[::]:3128 remote=[::] FD 15 flags=1
>>   2017/03/09 21:19:28 kid2| Accepting HTTP Socket connections at local=[::]:3128 remote=[::] FD 15 flags=1
>>
>> The full logs are attached. Please help.
> 
> Most likely, your systemctl script for Squid is broken. I do not know
> about CentOS 7 specifically, but many older scripts often start two
> modern Squids at once due to various backward compatibility problems
> with Squid startup interface and the way those scripts were written.
> Study/log what that script does and you will probably find the answer to
> your question.


Please check that you are using the squid.service file provided with
Squid-4 sources (under tools/systemd/squid.service). Remove the startup
scripts or any squid.service file created for Squid-3 versions, they are
usually wrong for Squid-4 with systemd.

After that you will need to carefully check that any previous attempts
to run Squid have been fully stopped and have not left any files (ie
.pid) or /dev/shm sockets behind before attempting to start Squid again.

HTH
Amos



From squid3 at treenet.co.nz  Thu Mar  9 16:00:23 2017
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Fri, 10 Mar 2017 05:00:23 +1300
Subject: [squid-users] squid workers question
In-Reply-To: <20170309142134.GA30976@fantomas.sk>
References: <20170309142134.GA30976@fantomas.sk>
Message-ID: <ce49fdac-98e7-3d9f-5710-76b083dc3829@treenet.co.nz>

On 10/03/2017 3:21 a.m., Matus UHLAR - fantomas wrote:
> Hello,
> 
> I have installed squid 3.4.8 on linux 3.16/64bit (debian 8 / jessie
> version)
> 
> (I know it's old, but I prefer using distribution-provided SW unless it has
> real problem distribution isn't able to fix)
> 
> - does this version have known memory leaks?
> http://www.squid-cache.org/Versions/v3/3.5/ChangeLog.txt
> shows some leaks fixed but they all seem to be related to something we
> don't
> use (certificated, Surrogate capability), unless the:
> 
> "Fix memory leak of HttpRequest objects" that is fixed in 3.5.16 applies
> to 3.4 too.
> 

IIRC that does, and there were some issues with CONNECT exceeding
configured limits.

The Bug 3553 issue
<http://www.squid-cache.org/Versions/v3/3.5/changesets/squid-3.5-13903.patch>
can also cause nasty issues on busy proxy as the cache disk overflows
from too-slow purging.


> 
> I configured rock store (for smaller files) and (later) standard aufs
> for others:
> 
> cache_dir rock /var/spool/squid3/rock 1024 max-size=32768
> #cache_dir aufs /var/spool/squid3 8192 16 256 min-size=32769
> 
> are those correct values? (bug 3411 says something about 256B metadata)
> 

Those 256 Byte will matter for Squid-3.4. It may be worthwhile adjusting
for anyway.

> 
> logs said this:
> 
> 2017/03/02 18:32:15 kid1| /var/spool/squid3 exists
> ...
> 2017/03/02 18:32:18 kid3| Swap maxSize 0 + 262144 KB, estimated 20164
> objects
> 2017/03/02 18:32:18 kid2| Swap maxSize 1048576 + 262144 KB, estimated
> 100824 objects
> 
> - do I get it right that kid1 is the Master, kid2 is the disker for rock
>   store and kid3 is the single worker process?
> 

Alex may corect me here but AFAIK; Master (the daemon manager) should
not have a number, the workers should be kid1 thru kid(N), the Disker
should be kid (N+1) thru kid (N+D), and the Coordinator should be kid(N+D+).

I suspect the coordinator changes its kid number during config parse as
things like workers and diskers are discovered if that matters. After
config the numbers are reliable.

There is also a bug that the FATAL messages do not indicate their
timestamp or what kid is applicable. So one has to guess somewhat based
on surrounding log info.

> 
> After first start I noticed crash:
> 
> 2017/03/02 18:32:18 kid3| Max Mem  size: 262144 KB [shared]
> 2017/03/02 18:32:18 kid2| Max Mem  size: 262144 KB [shared]
> 2017/03/02 18:32:18 kid3| Max Swap size: 0 KB
> 2017/03/02 18:32:18 kid1| WARNING: disk-cache maximum object size is too
> large for mem-cache: 16384.00 KB > 32.00 KB
> 2017/03/02 18:32:18 kid2| Max Swap size: 1048576 KB
> 2017/03/02 18:32:18 kid3| Using Least Load store dir selection
> 2017/03/02 18:32:18 kid3| Set Current Directory to /var/spool/squid3
> FATAL: Ipc::Mem::Segment::open failed to shm_open(/squid-cache_mem.shm):
> (2) No such file or directory
> 
> Squid Cache (Version 3.4.8): Terminated abnormally.
> FATAL: Ipc::Mem::Segment::open failed to
> shm_open(/squid-var.spool.squid3.rock.shm): (2) No such file or directory
> 
> Squid Cache (Version 3.4.8): Terminated abnormally.
> 
> 
> ... this happened in http://bugs.squid-cache.org/show_bug.cgi?id=3411
> however that
> - restart with "workers 1" worked, but isn't that the default?

Maybe. There is SMP and no SMP at all - both have 1 worker. It is
unclear to me which is the default and whether "workers 1" switches to
the other or not.


>   or was the creash caused by something else?
> (will try to replicate)

In my experience that "No such X" messages on the SHM usually means
/dev/shm is not mounted.

(For completeness it could also mean the device name/path is too long on
MacOS.)

Amos



From rousskov at measurement-factory.com  Thu Mar  9 16:07:19 2017
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Thu, 9 Mar 2017 09:07:19 -0700
Subject: [squid-users] squid workers question
In-Reply-To: <20170309142134.GA30976@fantomas.sk>
References: <20170309142134.GA30976@fantomas.sk>
Message-ID: <f547a991-3178-7b95-113d-fb4b533c3056@measurement-factory.com>

On 03/09/2017 07:21 AM, Matus UHLAR - fantomas wrote:

> I have installed squid 3.4.8 on linux 3.16/64bit (debian 8 / jessie
> version)
> 
> (I know it's old, but I prefer using distribution-provided SW unless it has
> real problem distribution isn't able to fix)

My answers are based on v5 code. (I know v5 is new, but I do not
remember v3.4 specifics and v5 answers will be valid for a longer time.)


> I configured rock store (for smaller files) and (later) standard aufs
> for others:
> 
> cache_dir rock /var/spool/squid3/rock 1024 max-size=32768
> #cache_dir aufs /var/spool/squid3 8192 16 256 min-size=32769
> 
> are those correct values? (bug 3411 says something about 256B metadata)

Both rock and AUFS stores support large objects so there is no
requirement to split storage based on object sizes. Each store has
various pros and cons, but lack of large object support is not one of
the distinguishing characteristics.


> - do I get it right that kid1 is the Master, kid2 is the disker for rock
>   store and kid3 is the single worker process?

In SMP mode (which, BTW, AUFS store does not support), Master is not a
kid (it is a parent of all kids), the first N kids are workers, the next
D kids are diskers, and the last kid is Coordinator. Please see the
following wiki section for more details.

   http://wiki.squid-cache.org/Features/SmpScale#Terminology

If possible, avoid relying on this specific numbering scheme because
mapping kid numbers to kid roles is not a part of a stable Squid
interface IMO.


> - restart with "workers 1" worked, but isn't that the default?

Yes, "1" is the default value for the workers directive.


HTH,

Alex.



From squid3 at treenet.co.nz  Thu Mar  9 16:12:04 2017
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Fri, 10 Mar 2017 05:12:04 +1300
Subject: [squid-users] microsoft edge and proxy auth not working
In-Reply-To: <1FCF9DA5B29068478ECF15896F19F08401B8AC0D3D@Y011008.bk.fin.local>
References: <1FCF9DA5B29068478ECF15896F19F08401B8AC0D2C@Y011008.bk.fin.local>
 <1FCF9DA5B29068478ECF15896F19F08401B8AC0D3D@Y011008.bk.fin.local>
Message-ID: <8b10adcd-f55a-65ba-cd37-28851575f6e0@treenet.co.nz>

On 8/03/2017 11:28 p.m., Rietzler, Markus (RZF, Aufg 324 /
<RIETZLER_SOFTWARE>) wrote:
> i should add that we are using squid 3.5.24.
> 

Try with "auth_param ntlm keep_alive off". Recently the browsers have
been needing that.

Though frankly I am surprised if Edge supports NTLM at all. It was
deprecated in April 2006 and MS announced removal was being actively
pushed in all thier software since Win7.

> 
>> -----Urspr?ngliche Nachricht-----
>> Von: Rietzler, Markus
>>
>> we have some windows 10 clients using microsoft edge browser.
>> access to internet is only allowed for authenticated users. we are using
>> samba/winbind auth
>>
>> auth_param ntlm program /usr/bin/ntlm_auth --helper-protocol=squid-2.5-
>> ntlmssp
>> auth_param ntlm children 64 startup=24 idle=12
>> auth_param ntlm keep_alive on
>> acl auth_user proxy_auth REQUIRED
>>
>> on windows 10 clients with IE11 it is working (with ntlm automatic auth)
>> on the same machine, with Microsoft edge I get TCP_Denied/407 message.
>> seems I only get one single TCP_DENIED/407 line in accesslog and an auth
>> dialog pops up. I have disabled basic auth via ntlm.
>> shouldn't there be 3 lines for proxy auth? with IE11 I see those three
>> lines (2x TCP_DENIED/407 and 1x TCP_MISS/200), no popup at all.

Not specifically. There should be 1+ for NTLM. Success with NTLM shows
2+. Failure shows 1 or 3 or infinite loop (hello Safari and Firefox 30-ish).


>>
>> winbind/samba itself seems to work, as I can do an user auth against
>> apache with winbind/samba - even over some squid proxies with
>> connection-auth allowed. but not for proxy-auth.
>> is there any option in squid.conf which prevents Edge to do a successful
>> auth?

If other software succeeds then the only thing that might be related is
the keep-alive option mentioned above. Otherwise the problem is in Edge
itself.

Amos



From uhlar at fantomas.sk  Thu Mar  9 16:14:15 2017
From: uhlar at fantomas.sk (Matus UHLAR - fantomas)
Date: Thu, 9 Mar 2017 17:14:15 +0100
Subject: [squid-users] squid workers question
In-Reply-To: <ce49fdac-98e7-3d9f-5710-76b083dc3829@treenet.co.nz>
References: <20170309142134.GA30976@fantomas.sk>
 <ce49fdac-98e7-3d9f-5710-76b083dc3829@treenet.co.nz>
Message-ID: <20170309161415.GA7401@fantomas.sk>

>On 10/03/2017 3:21 a.m., Matus UHLAR - fantomas wrote:
>> I have installed squid 3.4.8 on linux 3.16/64bit (debian 8 / jessie
>> version)

>> - does this version have known memory leaks?
>> http://www.squid-cache.org/Versions/v3/3.5/ChangeLog.txt
>> shows some leaks fixed but they all seem to be related to something we
>> don't
>> use (certificated, Surrogate capability), unless the:
>>
>> "Fix memory leak of HttpRequest objects" that is fixed in 3.5.16 applies
>> to 3.4 too.

On 10.03.17 05:00, Amos Jeffries wrote:
>IIRC that does, and there were some issues with CONNECT exceeding
>configured limits.
>
>The Bug 3553 issue
><http://www.squid-cache.org/Versions/v3/3.5/changesets/squid-3.5-13903.patch>
>can also cause nasty issues on busy proxy as the cache disk overflows
>from too-slow purging.

seems that my memory problem is somehow related to 4g of "2K Buffers"
whatever that means. This is cachrmgr output:


		(bytes)	KB/ch	obj/ch	(#)	used	free	part	%Frag	(#)	(KB)	high (KB)	high (hrs)	%Tot	(#)	(KB)	high (KB)	high (hrs)	%alloc	(#)	(KB)	high (KB)	(#)		%cnt	%vol	(#)/sec
2K Buffer	2048								1986398	3972796	3972796		0.00		89.763	1986390	3972780	3972796		0.00		100.000	8	16	198		10736355	4.914	19.208	0.009


>> cache_dir rock /var/spool/squid3/rock 1024 max-size=32768
>> #cache_dir aufs /var/spool/squid3 8192 16 256 min-size=32769
>>
>> are those correct values? (bug 3411 says something about 256B metadata)

>Those 256 Byte will matter for Squid-3.4.

doesn't it for later squid versions?

>It may be worthwhile adjusting for anyway.

changed to:

#cache_dir rock /var/spool/squid3/rock 1024 max-size=32512
#cache_dir aufs /var/spool/squid3 4096 16 256 min-size=32513


>> After first start I noticed crash:
>>
>> 2017/03/02 18:32:18 kid3| Max Mem  size: 262144 KB [shared]
>> 2017/03/02 18:32:18 kid2| Max Mem  size: 262144 KB [shared]
>> 2017/03/02 18:32:18 kid3| Max Swap size: 0 KB
>> 2017/03/02 18:32:18 kid1| WARNING: disk-cache maximum object size is too
>> large for mem-cache: 16384.00 KB > 32.00 KB
>> 2017/03/02 18:32:18 kid2| Max Swap size: 1048576 KB
>> 2017/03/02 18:32:18 kid3| Using Least Load store dir selection
>> 2017/03/02 18:32:18 kid3| Set Current Directory to /var/spool/squid3
>> FATAL: Ipc::Mem::Segment::open failed to shm_open(/squid-cache_mem.shm):
>> (2) No such file or directory
>>
>> Squid Cache (Version 3.4.8): Terminated abnormally.
>> FATAL: Ipc::Mem::Segment::open failed to
>> shm_open(/squid-var.spool.squid3.rock.shm): (2) No such file or directory
>>
>> Squid Cache (Version 3.4.8): Terminated abnormally.
>>
>>
>> ... this happened in http://bugs.squid-cache.org/show_bug.cgi?id=3411
>> however that
>> - restart with "workers 1" worked, but isn't that the default?
>
>Maybe. There is SMP and no SMP at all - both have 1 worker. It is
>unclear to me which is the default and whether "workers 1" switches to
>the other or not.
>
>
>>   or was the creash caused by something else?
>> (will try to replicate)
>
>In my experience that "No such X" messages on the SHM usually means
>/dev/shm is not mounted.

I believe it was all the time.

-- 
Matus UHLAR - fantomas, uhlar at fantomas.sk ; http://www.fantomas.sk/
Warning: I wish NOT to receive e-mail advertising to this address.
Varovanie: na tuto adresu chcem NEDOSTAVAT akukolvek reklamnu postu.
I don't have lysdexia. The Dog wouldn't allow that.


From uhlar at fantomas.sk  Thu Mar  9 16:19:40 2017
From: uhlar at fantomas.sk (Matus UHLAR - fantomas)
Date: Thu, 9 Mar 2017 17:19:40 +0100
Subject: [squid-users] squid workers question
In-Reply-To: <f547a991-3178-7b95-113d-fb4b533c3056@measurement-factory.com>
References: <20170309142134.GA30976@fantomas.sk>
 <f547a991-3178-7b95-113d-fb4b533c3056@measurement-factory.com>
Message-ID: <20170309161940.GB7401@fantomas.sk>

>On 03/09/2017 07:21 AM, Matus UHLAR - fantomas wrote:
>> I have installed squid 3.4.8 on linux 3.16/64bit (debian 8 / jessie
>> version)
>>
>> (I know it's old, but I prefer using distribution-provided SW unless it has
>> real problem distribution isn't able to fix)

On 09.03.17 09:07, Alex Rousskov wrote:
>My answers are based on v5 code. (I know v5 is new, but I do not
>remember v3.4 specifics and v5 answers will be valid for a longer time.)

OK

>> I configured rock store (for smaller files) and (later) standard aufs
>> for others:
>>
>> cache_dir rock /var/spool/squid3/rock 1024 max-size=32768
>> #cache_dir aufs /var/spool/squid3 8192 16 256 min-size=32769
>>
>> are those correct values? (bug 3411 says something about 256B metadata)

>Both rock and AUFS stores support large objects so there is no
>requirement to split storage based on object sizes. Each store has
>various pros and cons, but lack of large object support is not one of
>the distinguishing characteristics.

I thought the cons of *ufs/disks is ineffective storage of small files,
while rock is ineffective with big files...

>> - do I get it right that kid1 is the Master, kid2 is the disker for rock
>>   store and kid3 is the single worker process?
>
>In SMP mode (which, BTW, AUFS store does not support),

could it crash squid instead of complaining?

> Master is not a
>kid (it is a parent of all kids), the first N kids are workers, the next
>D kids are diskers, and the last kid is Coordinator. Please see the
>following wiki section for more details.
>
>   http://wiki.squid-cache.org/Features/SmpScale#Terminology

I read that prior to posting, buuw I was wondering where do those kids get.

>If possible, avoid relying on this specific numbering scheme because
>mapping kid numbers to kid roles is not a part of a stable Squid
>interface IMO.
>
>
>> - restart with "workers 1" worked, but isn't that the default?
>
>Yes, "1" is the default value for the workers directive.

and this is why I wonder we have three kids, both when "workers" commented out
and when set to 1.

-- 
Matus UHLAR - fantomas, uhlar at fantomas.sk ; http://www.fantomas.sk/
Warning: I wish NOT to receive e-mail advertising to this address.
Varovanie: na tuto adresu chcem NEDOSTAVAT akukolvek reklamnu postu.
     One OS to rule them all, One OS to find them, 
One OS to bring them all and into darkness bind them 


From harishmeetsu at yahoo.com  Thu Mar  9 16:19:45 2017
From: harishmeetsu at yahoo.com (S V Hareesh)
Date: Thu, 9 Mar 2017 16:19:45 +0000 (UTC)
Subject: [squid-users] Setting Up Squid - my scenario
References: <1190296055.2535835.1489076385522.ref@mail.yahoo.com>
Message-ID: <1190296055.2535835.1489076385522@mail.yahoo.com>

On top of the conf file from default setup on Windows, I added the following line in the conf. I added the dns servers and allowed localhost.

cache_peer <corporate_proxy> parent 80 0 default connection-auth=on proxy-only

never_direct allow all

When I point my browser to this proxy, it gives me 407, auth required.?

Also, configured squid service on windows to run with a service account that has access to Internet/corp proxy.
Sent from Yahoo Mail on Android
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20170309/3d4a2478/attachment.htm>

From squid3 at treenet.co.nz  Thu Mar  9 16:30:03 2017
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Fri, 10 Mar 2017 05:30:03 +1300
Subject: [squid-users] squid workers question
In-Reply-To: <20170309161415.GA7401@fantomas.sk>
References: <20170309142134.GA30976@fantomas.sk>
 <ce49fdac-98e7-3d9f-5710-76b083dc3829@treenet.co.nz>
 <20170309161415.GA7401@fantomas.sk>
Message-ID: <19dd204a-2f6b-914d-2858-5afdb521aaea@treenet.co.nz>

On 10/03/2017 5:14 a.m., Matus UHLAR - fantomas wrote:
>> On 10/03/2017 3:21 a.m., Matus UHLAR - fantomas wrote:
>>> I have installed squid 3.4.8 on linux 3.16/64bit (debian 8 / jessie
>>> version)
> 
>>> - does this version have known memory leaks?
>>> http://www.squid-cache.org/Versions/v3/3.5/ChangeLog.txt
>>> shows some leaks fixed but they all seem to be related to something we
>>> don't
>>> use (certificated, Surrogate capability), unless the:
>>>
>>> "Fix memory leak of HttpRequest objects" that is fixed in 3.5.16 applies
>>> to 3.4 too.
> 
> On 10.03.17 05:00, Amos Jeffries wrote:
>> IIRC that does, and there were some issues with CONNECT exceeding
>> configured limits.
>>
>> The Bug 3553 issue
>> <http://www.squid-cache.org/Versions/v3/3.5/changesets/squid-3.5-13903.patch>
>>
>> can also cause nasty issues on busy proxy as the cache disk overflows
>> from too-slow purging.
> 
> seems that my memory problem is somehow related to 4g of "2K Buffers"
> whatever that means. This is cachrmgr output:
> 
> 
>         (bytes)    KB/ch    obj/ch    (#)    used    free    part   
> %Frag    (#)    (KB)    high (KB)    high (hrs)    %Tot    (#)   
> (KB)    high (KB)    high (hrs)    %alloc    (#)    (KB)    high (KB)   
> (#)        %cnt    %vol    (#)/sec
> 2K Buffer    2048                                1986398    3972796   
> 3972796        0.00        89.763    1986390    3972780   
> 3972796        0.00        100.000    8    16    198        10736355   
> 4.914    19.208    0.009
> 

Ah. So anything that is using a generic 2KB of memory. Tricky to track
down :-(.

I would look at the next few entries to see if there is a good clue
about what system might be worth a closer look (largest amount of things
active, or anything else not being released). Mostly it is I/O using the
various Buffer's, but some other things do as well.


> 
>>> cache_dir rock /var/spool/squid3/rock 1024 max-size=32768
>>> #cache_dir aufs /var/spool/squid3 8192 16 256 min-size=32769
>>>
>>> are those correct values? (bug 3411 says something about 256B metadata)
> 
>> Those 256 Byte will matter for Squid-3.4.
> 
> doesn't it for later squid versions?

Nope :-). Squid-3.5 'large rock' feature adds slots as needed to fit the
extra meta bytes. So 32KB is no longer an absolute limit.

Amos



From duanyao at ustc.edu  Thu Mar  9 16:32:14 2017
From: duanyao at ustc.edu (=?UTF-8?B?5q615Z6a?=)
Date: Fri, 10 Mar 2017 00:32:14 +0800
Subject: [squid-users] Can't start Squid with workers via systemd on
 CentOS 7
In-Reply-To: <5391c2bb-7539-ee9f-8658-286725c398fd@treenet.co.nz>
References: <1965c37d-0297-55a4-cbcc-09f9916237c5@ustc.edu>
 <5b702ed3-fc97-3c98-4b01-0892f9a04f09@measurement-factory.com>
 <5391c2bb-7539-ee9f-8658-286725c398fd@treenet.co.nz>
Message-ID: <92a12d76-6378-2d73-a212-999c4dc989ed@ustc.edu>



? 2017/3/9 23:22, Amos Jeffries ??:
> On 10/03/2017 3:57 a.m., Alex Rousskov wrote:
>> On 03/09/2017 07:00 AM, ?? wrote:
>>> I installed Squid 4.0.18 on CentOS 7 X86_84 according to
>>> http://wiki.squid-cache.org/KnowledgeBase/CentOS#Squid_Beta_release
>>>
>>> I add a line "workers 2" to `/etc/squid/squid.conf` (just a copy of
>>> `squid.conf.default`)
>>> and start squid via `systemctl start squid`, It seems squid is running,
>>> except that it is not listening to the port 3128 (confirmed by netstat).
>>>
>>> However, if I start squid in command line (`squid -sYC`), squid does
>>> listen to port 3128.
>>>
>>> There are some differences in `/var/log/squid/cache.log` for these two
>>> cases:
>>>
>>> * start via `systemctl start squid`:
>>>
>>>    2017/03/09 21:12:54 kid3| commBind Cannot bind socket FD 12 to [::]: (98) Address already in use
>>>    2017/03/09 21:12:54 kid2| commBind Cannot bind socket FD 21 to [::]: (98) Address already in use
>>>    2017/03/09 21:12:54 kid1| commBind Cannot bind socket FD 21 to [::]: (98) Address already in use
>>>
>>> * start via `squid -sYC`:
>>>
>>>    2017/03/09 21:19:28 kid1| Accepting HTTP Socket connections at local=[::]:3128 remote=[::] FD 15 flags=1
>>>    2017/03/09 21:19:28 kid2| Accepting HTTP Socket connections at local=[::]:3128 remote=[::] FD 15 flags=1
>>>
>>> The full logs are attached. Please help.
>> Most likely, your systemctl script for Squid is broken. I do not know
>> about CentOS 7 specifically, but many older scripts often start two
>> modern Squids at once due to various backward compatibility problems
>> with Squid startup interface and the way those scripts were written.
>> Study/log what that script does and you will probably find the answer to
>> your question.
>
> Please check that you are using the squid.service file provided with
> Squid-4 sources (under tools/systemd/squid.service). Remove the startup
> scripts or any squid.service file created for Squid-3 versions, they are
> usually wrong for Squid-4 with systemd.
The `squid.service` I used was shipped with the prebuild 4.0.18 rpm from
http://www1.ngtech.co.il/repo/centos/$releasever/beta/$basearch/

I uninstalled squid-3.5.20 rpm before install 4.0.18.
The `squid.service` content:

[Unit]
Description=Squid Web Proxy Server
Documentation=man:squid(8)
After=network.target

[Service]
Type=forking
LimitNOFILE=16384
PIDFile=/var/run/squid.pid
ExecStartPre=/usr/bin/mkdir -p /var/run/squid
ExecStartPre=/usr/bin/chown squid.squid /var/run/squid
ExecStart=/usr/sbin/squid -sYC
ExecReload=/usr/sbin/squid -kreconf
ExecStop=/usr/sbin/squidshut.sh
TimeoutStopSec=36
KillMode=none

[Install]
WantedBy=multi-user.target


Even if I replace it with the one under `tools/systemd/squid.service` of 
squid 4.0.18 source,
still no luck:

[Unit]
Description=Squid Web Proxy Server
Documentation=man:squid(8)
After=network.target nss-lookup.target

[Service]
Type=forking
PIDFile=/var/run/squid.pid
ExecStartPre=/usr/sbin/squid --foreground -z
ExecStart=/usr/sbin/squid -sYC
ExecReload=/bin/kill -HUP $MAINPID
KillMode=mixed

[Install]
WantedBy=multi-user.target

>
> After that you will need to carefully check that any previous attempts
> to run Squid have been fully stopped and have not left any files (ie
> .pid) or /dev/shm sockets behind before attempting to start Squid again.

I checked that there are no `/run/squid.pid `, `/var/run/squid.pid ` and 
`/dev/shm/squid*` before starting squid.

There are still similar messages in `/var/log/squid/cache.log`:

2017/03/10 00:20:00 kid2| commBind Cannot bind socket FD 21 to [::]: (2) 
No such file or directory
2017/03/10 00:20:00 kid1| commBind Cannot bind socket FD 21 to [::]: (2) 
No such file or directory
2017/03/10 00:20:00 kid3| commBind Cannot bind socket FD 12 to [::]: (2) 
No such file or directory

>
> HTH
> Amos
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users




From squid3 at treenet.co.nz  Thu Mar  9 16:35:49 2017
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Fri, 10 Mar 2017 05:35:49 +1300
Subject: [squid-users] squid workers question
In-Reply-To: <20170309161940.GB7401@fantomas.sk>
References: <20170309142134.GA30976@fantomas.sk>
 <f547a991-3178-7b95-113d-fb4b533c3056@measurement-factory.com>
 <20170309161940.GB7401@fantomas.sk>
Message-ID: <5f406c4a-fe6f-8ff0-f097-0d5ab0cc031b@treenet.co.nz>

On 10/03/2017 5:19 a.m., Matus UHLAR - fantomas wrote:
>> On 03/09/2017 07:21 AM, Matus UHLAR - fantomas wrote:
>>> I have installed squid 3.4.8 on linux 3.16/64bit (debian 8 / jessie
>>> version)
>>>
>>> (I know it's old, but I prefer using distribution-provided SW unless
>>> it has
>>> real problem distribution isn't able to fix)
> 
> On 09.03.17 09:07, Alex Rousskov wrote:
>> My answers are based on v5 code. (I know v5 is new, but I do not
>> remember v3.4 specifics and v5 answers will be valid for a longer time.)
> 
> OK
> 
>>> I configured rock store (for smaller files) and (later) standard aufs
>>> for others:
>>>
>>> cache_dir rock /var/spool/squid3/rock 1024 max-size=32768
>>> #cache_dir aufs /var/spool/squid3 8192 16 256 min-size=32769
>>>
>>> are those correct values? (bug 3411 says something about 256B metadata)
> 
>> Both rock and AUFS stores support large objects so there is no
>> requirement to split storage based on object sizes. Each store has
>> various pros and cons, but lack of large object support is not one of
>> the distinguishing characteristics.
> 
> I thought the cons of *ufs/disks is ineffective storage of small files,
> while rock is ineffective with big files...
> 

Yes, *efficiency*, not lack of support.

[except in this case of your 3.4 where rock does explicitly lack the
support].


>>> - do I get it right that kid1 is the Master, kid2 is the disker for rock
>>>   store and kid3 is the single worker process?
>>
>> In SMP mode (which, BTW, AUFS store does not support),
> 
> could it crash squid instead of complaining?

Are you talking about when AUFS is used in SMP mode? it is not that
simple, there are ways to configure it to be used by a single worker
("if" directive, or $(process_number) macro) which is fine and work well
for certain version-specific situations.


Amos



From rousskov at measurement-factory.com  Thu Mar  9 16:42:45 2017
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Thu, 9 Mar 2017 09:42:45 -0700
Subject: [squid-users] squid workers question
In-Reply-To: <20170309161940.GB7401@fantomas.sk>
References: <20170309142134.GA30976@fantomas.sk>
 <f547a991-3178-7b95-113d-fb4b533c3056@measurement-factory.com>
 <20170309161940.GB7401@fantomas.sk>
Message-ID: <6ca274d9-948c-01c5-c3dd-13044d3cfe7b@measurement-factory.com>

On 03/09/2017 09:19 AM, Matus UHLAR - fantomas wrote:
> On 09.03.17 09:07, Alex Rousskov wrote:
>> Both rock and AUFS stores support large objects so there is no
>> requirement to split storage based on object sizes. Each store has
>> various pros and cons, but lack of large object support is not one of
>> the distinguishing characteristics.

> I thought the cons of *ufs/disks is ineffective storage of small files,
> while rock is ineffective with big files...

While this assertion is true in some environments for some definitions
of "ineffective", I would avoid such generalizations: If your goal is to
optimize overall Squid performance, then there are a lot more factors
that you should take into account before deciding whether it is is a
good idea to split cache_dirs based on object sizes.


>>> - do I get it right that kid1 is the Master, kid2 is the disker for rock
>>>   store and kid3 is the single worker process?
>>
>> In SMP mode (which, BTW, AUFS store does not support),
> 
> could it crash squid instead of complaining?

In SMP mode, SMP-unaware stores crash, corrupt, and/or duplicate
responses, depending on your luck and squid.conf. They do not complain.


>> Master is not a
>> kid (it is a parent of all kids), the first N kids are workers, the next
>> D kids are diskers, and the last kid is Coordinator. Please see the
>> following wiki section for more details.
>>   http://wiki.squid-cache.org/Features/SmpScale#Terminology



>> Yes, "1" is the default value for the workers directive.

> and this is why I wonder we have three kids, both when "workers"
> commented out and when set to 1.

In my earlier paragraph, N is the value of the workers directive. There
are other kids (D diskers and one Coordinator). D is the number of rock
cache_dirs. Both workers and diskers need shared memory and IPC.

If you want to turn off SMP while using rock store, start Squid with -N.
When started with -N, there will be a single process playing all four
roles (master, worker, disker, and Coordinator).


HTH,

Alex.



From squid3 at treenet.co.nz  Thu Mar  9 16:49:56 2017
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Fri, 10 Mar 2017 05:49:56 +1300
Subject: [squid-users] Setting Up Squid - my scenario
In-Reply-To: <1190296055.2535835.1489076385522@mail.yahoo.com>
References: <1190296055.2535835.1489076385522.ref@mail.yahoo.com>
 <1190296055.2535835.1489076385522@mail.yahoo.com>
Message-ID: <b4a65caf-0332-5a56-915a-dbffcbe07fc7@treenet.co.nz>

On 10/03/2017 5:19 a.m., S V Hareesh wrote:
> On top of the conf file from default setup on Windows, I added the following line in the conf. I added the dns servers and allowed localhost.
> 
> cache_peer <corporate_proxy> parent 80 0 default connection-auth=on proxy-only
> 
> never_direct allow all
> 
> When I point my browser to this proxy, it gives me 407, auth required. 
> 
> Also, configured squid service on windows to run with a service account that has access to Internet/corp proxy.

Squid cannot authenticate to a cache_peer using NTLM. It can only do
Nagotiate/Kerberos to the parent proxy, and only when "login=NEGOTIATE"
is added (with or without a named keytab file).

NOTE: 'connection-auth=on' is about allowing the browser to use NTLM or
Negotiate/Kerberos through the cache_peer. It needs to also have
"login=PASSTHRU" if that peer is a proxy (as opposed to a web or
Exchange server).

See the 'AUTHENTICATION OPTIONS' section of
<http://www.squid-cache.org/Doc/config/cache_peer/>

Amos



From rousskov at measurement-factory.com  Thu Mar  9 16:52:48 2017
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Thu, 9 Mar 2017 09:52:48 -0700
Subject: [squid-users] Can't start Squid with workers via systemd on
 CentOS 7
In-Reply-To: <92a12d76-6378-2d73-a212-999c4dc989ed@ustc.edu>
References: <1965c37d-0297-55a4-cbcc-09f9916237c5@ustc.edu>
 <5b702ed3-fc97-3c98-4b01-0892f9a04f09@measurement-factory.com>
 <5391c2bb-7539-ee9f-8658-286725c398fd@treenet.co.nz>
 <92a12d76-6378-2d73-a212-999c4dc989ed@ustc.edu>
Message-ID: <1b488eae-37ef-c4a5-9849-c5b26052533b@measurement-factory.com>

On 03/09/2017 09:32 AM, ?? wrote:
> commBind Cannot bind socket



From uhlar at fantomas.sk  Thu Mar  9 16:54:30 2017
From: uhlar at fantomas.sk (Matus UHLAR - fantomas)
Date: Thu, 9 Mar 2017 17:54:30 +0100
Subject: [squid-users] squid workers question
In-Reply-To: <6ca274d9-948c-01c5-c3dd-13044d3cfe7b@measurement-factory.com>
References: <20170309142134.GA30976@fantomas.sk>
 <f547a991-3178-7b95-113d-fb4b533c3056@measurement-factory.com>
 <20170309161940.GB7401@fantomas.sk>
 <6ca274d9-948c-01c5-c3dd-13044d3cfe7b@measurement-factory.com>
Message-ID: <20170309165429.GA8584@fantomas.sk>

>> On 09.03.17 09:07, Alex Rousskov wrote:
>>> Both rock and AUFS stores support large objects so there is no
>>> requirement to split storage based on object sizes. Each store has
>>> various pros and cons, but lack of large object support is not one of
>>> the distinguishing characteristics.
>
>> I thought the cons of *ufs/disks is ineffective storage of small files,
>> while rock is ineffective with big files...
>
>While this assertion is true in some environments for some definitions
>of "ineffective", I would avoid such generalizations: If your goal is to
>optimize overall Squid performance, then there are a lot more factors
>that you should take into account before deciding whether it is is a
>good idea to split cache_dirs based on object sizes.

aha, there's much to learn yet.
any idea where should I start?


>>> Master is not a
>>> kid (it is a parent of all kids), the first N kids are workers, the next
>>> D kids are diskers, and the last kid is Coordinator. Please see the
>>> following wiki section for more details.
>>>   http://wiki.squid-cache.org/Features/SmpScale#Terminology

>>> Yes, "1" is the default value for the workers directive.
>
>> and this is why I wonder we have three kids, both when "workers"
>> commented out and when set to 1.
>
>In my earlier paragraph, N is the value of the workers directive. There
>are other kids (D diskers and one Coordinator). D is the number of rock
>cache_dirs. Both workers and diskers need shared memory and IPC.
>
>If you want to turn off SMP while using rock store, start Squid with -N.
>When started with -N, there will be a single process playing all four
>roles (master, worker, disker, and Coordinator).

Will running with "workers 1" avoid this issue while using separate
processes for diskers?

-- 
Matus UHLAR - fantomas, uhlar at fantomas.sk ; http://www.fantomas.sk/
Warning: I wish NOT to receive e-mail advertising to this address.
Varovanie: na tuto adresu chcem NEDOSTAVAT akukolvek reklamnu postu.
- Have you got anything without Spam in it?
- Well, there's Spam egg sausage and Spam, that's not got much Spam in it.


From uhlar at fantomas.sk  Thu Mar  9 16:54:30 2017
From: uhlar at fantomas.sk (Matus UHLAR - fantomas)
Date: Thu, 9 Mar 2017 17:54:30 +0100
Subject: [squid-users] squid workers question
In-Reply-To: <6ca274d9-948c-01c5-c3dd-13044d3cfe7b@measurement-factory.com>
References: <20170309142134.GA30976@fantomas.sk>
 <f547a991-3178-7b95-113d-fb4b533c3056@measurement-factory.com>
 <20170309161940.GB7401@fantomas.sk>
 <6ca274d9-948c-01c5-c3dd-13044d3cfe7b@measurement-factory.com>
Message-ID: <20170309165429.GA8584@fantomas.sk>

>> On 09.03.17 09:07, Alex Rousskov wrote:
>>> Both rock and AUFS stores support large objects so there is no
>>> requirement to split storage based on object sizes. Each store has
>>> various pros and cons, but lack of large object support is not one of
>>> the distinguishing characteristics.
>
>> I thought the cons of *ufs/disks is ineffective storage of small files,
>> while rock is ineffective with big files...
>
>While this assertion is true in some environments for some definitions
>of "ineffective", I would avoid such generalizations: If your goal is to
>optimize overall Squid performance, then there are a lot more factors
>that you should take into account before deciding whether it is is a
>good idea to split cache_dirs based on object sizes.

aha, there's much to learn yet.
any idea where should I start?


>>> Master is not a
>>> kid (it is a parent of all kids), the first N kids are workers, the next
>>> D kids are diskers, and the last kid is Coordinator. Please see the
>>> following wiki section for more details.
>>>   http://wiki.squid-cache.org/Features/SmpScale#Terminology

>>> Yes, "1" is the default value for the workers directive.
>
>> and this is why I wonder we have three kids, both when "workers"
>> commented out and when set to 1.
>
>In my earlier paragraph, N is the value of the workers directive. There
>are other kids (D diskers and one Coordinator). D is the number of rock
>cache_dirs. Both workers and diskers need shared memory and IPC.
>
>If you want to turn off SMP while using rock store, start Squid with -N.
>When started with -N, there will be a single process playing all four
>roles (master, worker, disker, and Coordinator).

Will running with "workers 1" avoid this issue while using separate
processes for diskers?

-- 
Matus UHLAR - fantomas, uhlar at fantomas.sk ; http://www.fantomas.sk/
Warning: I wish NOT to receive e-mail advertising to this address.
Varovanie: na tuto adresu chcem NEDOSTAVAT akukolvek reklamnu postu.
- Have you got anything without Spam in it?
- Well, there's Spam egg sausage and Spam, that's not got much Spam in it.


From rousskov at measurement-factory.com  Thu Mar  9 16:56:21 2017
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Thu, 9 Mar 2017 09:56:21 -0700
Subject: [squid-users] Can't start Squid with workers via systemd on
 CentOS 7
In-Reply-To: <92a12d76-6378-2d73-a212-999c4dc989ed@ustc.edu>
References: <1965c37d-0297-55a4-cbcc-09f9916237c5@ustc.edu>
 <5b702ed3-fc97-3c98-4b01-0892f9a04f09@measurement-factory.com>
 <5391c2bb-7539-ee9f-8658-286725c398fd@treenet.co.nz>
 <92a12d76-6378-2d73-a212-999c4dc989ed@ustc.edu>
Message-ID: <4e1ff658-daa0-930a-20c0-566be54e6fae@measurement-factory.com>

On 03/09/2017 09:32 AM, ?? wrote:
> There are still similar messages in `/var/log/squid/cache.log`:
> 
> 2017/03/10 00:20:00 kid2| commBind Cannot bind socket FD 21 to [::]: (2)
> No such file or directory

This feels like an IPC problem (with an awful diagnostic). There are a
few hints at http://wiki.squid-cache.org/Features/SmpScale and lots of
squid-users posts that might help troubleshoot that.

Sorry for the earlier accidental empty response.

Alex.



From rafael.akchurin at diladele.com  Thu Mar  9 17:00:34 2017
From: rafael.akchurin at diladele.com (Rafael Akchurin)
Date: Thu, 9 Mar 2017 17:00:34 +0000
Subject: [squid-users] microsoft edge and proxy auth not working
In-Reply-To: <8b10adcd-f55a-65ba-cd37-28851575f6e0@treenet.co.nz>
References: <1FCF9DA5B29068478ECF15896F19F08401B8AC0D2C@Y011008.bk.fin.local>
 <1FCF9DA5B29068478ECF15896F19F08401B8AC0D3D@Y011008.bk.fin.local>
 <8b10adcd-f55a-65ba-cd37-28851575f6e0@treenet.co.nz>
Message-ID: <DB6PR0401MB2680D3C0EF7481BC9565DDEA8F210@DB6PR0401MB2680.eurprd04.prod.outlook.com>

Hello Amos, Markus, all,

Just as a side note - I also suffered  from this error sometime before with Edge and our custom NTLM relay to domain controllers (run as auth helper by Squid). The strange thing it went away after installing some (unknown) Windows update.

I do have the "auth_param ntlm keep_alive off" in the config though.

It all makes me quite suspicious the error was/is in Edge or in my curly hands.

Best regards,
Rafael Akchurin
Diladele B.V.

-----Original Message-----
From: squid-users [mailto:squid-users-bounces at lists.squid-cache.org] On Behalf Of Amos Jeffries
Sent: Thursday, March 9, 2017 5:12 PM
To: squid-users at lists.squid-cache.org
Subject: Re: [squid-users] microsoft edge and proxy auth not working

On 8/03/2017 11:28 p.m., Rietzler, Markus (RZF, Aufg 324 /
<RIETZLER_SOFTWARE>) wrote:
> i should add that we are using squid 3.5.24.
> 

Try with "auth_param ntlm keep_alive off". Recently the browsers have been needing that.

Though frankly I am surprised if Edge supports NTLM at all. It was deprecated in April 2006 and MS announced removal was being actively pushed in all thier software since Win7.

> 
>> -----Urspr?ngliche Nachricht-----
>> Von: Rietzler, Markus
>>
>> we have some windows 10 clients using microsoft edge browser.
>> access to internet is only allowed for authenticated users. we are 
>> using samba/winbind auth
>>
>> auth_param ntlm program /usr/bin/ntlm_auth 
>> --helper-protocol=squid-2.5- ntlmssp auth_param ntlm children 64 
>> startup=24 idle=12 auth_param ntlm keep_alive on acl auth_user 
>> proxy_auth REQUIRED
>>
>> on windows 10 clients with IE11 it is working (with ntlm automatic 
>> auth) on the same machine, with Microsoft edge I get TCP_Denied/407 message.
>> seems I only get one single TCP_DENIED/407 line in accesslog and an 
>> auth dialog pops up. I have disabled basic auth via ntlm.
>> shouldn't there be 3 lines for proxy auth? with IE11 I see those 
>> three lines (2x TCP_DENIED/407 and 1x TCP_MISS/200), no popup at all.

Not specifically. There should be 1+ for NTLM. Success with NTLM shows
2+. Failure shows 1 or 3 or infinite loop (hello Safari and Firefox 30-ish).


>>
>> winbind/samba itself seems to work, as I can do an user auth against 
>> apache with winbind/samba - even over some squid proxies with 
>> connection-auth allowed. but not for proxy-auth.
>> is there any option in squid.conf which prevents Edge to do a 
>> successful auth?

If other software succeeds then the only thing that might be related is the keep-alive option mentioned above. Otherwise the problem is in Edge itself.

Amos

_______________________________________________
squid-users mailing list
squid-users at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-users

From rousskov at measurement-factory.com  Thu Mar  9 17:01:51 2017
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Thu, 9 Mar 2017 10:01:51 -0700
Subject: [squid-users] squid workers question
In-Reply-To: <20170309165429.GA8584@fantomas.sk>
References: <20170309142134.GA30976@fantomas.sk>
 <f547a991-3178-7b95-113d-fb4b533c3056@measurement-factory.com>
 <20170309161940.GB7401@fantomas.sk>
 <6ca274d9-948c-01c5-c3dd-13044d3cfe7b@measurement-factory.com>
 <20170309165429.GA8584@fantomas.sk>
Message-ID: <8ef542a5-dba3-236f-224b-944990d4cd68@measurement-factory.com>

On 03/09/2017 09:54 AM, Matus UHLAR - fantomas wrote:
>>>> Master is not a
>>>> kid (it is a parent of all kids), the first N kids are workers, the
>>>> next D kids are diskers, and the last kid is Coordinator. Please see the
>>>> following wiki section for more details.
>>>>   http://wiki.squid-cache.org/Features/SmpScale#Terminology

>>>> Yes, "1" is the default value for the workers directive.

>>> and this is why I wonder we have three kids, both when "workers"
>>> commented out and when set to 1.

>> In my earlier paragraph, N is the value of the workers directive. There
>> are other kids (D diskers and one Coordinator). D is the number of rock
>> cache_dirs. Both workers and diskers need shared memory and IPC.
>>
>> If you want to turn off SMP while using rock store, start Squid with -N.
>> When started with -N, there will be a single process playing all four
>> roles (master, worker, disker, and Coordinator).

> Will running with "workers 1" avoid this issue while using separate
> processes for diskers?

No. Disker processes need shared memory to communicate with worker(s).

Alex.



From harishmeetsu at yahoo.com  Thu Mar  9 17:00:09 2017
From: harishmeetsu at yahoo.com (S V Hareesh)
Date: Thu, 9 Mar 2017 17:00:09 +0000 (UTC)
Subject: [squid-users] Setting Up Squid - my scenario
In-Reply-To: <b4a65caf-0332-5a56-915a-dbffcbe07fc7@treenet.co.nz>
References: <1190296055.2535835.1489076385522.ref@mail.yahoo.com>
 <1190296055.2535835.1489076385522@mail.yahoo.com>
 <b4a65caf-0332-5a56-915a-dbffcbe07fc7@treenet.co.nz>
Message-ID: <821797722.2515523.1489078809096@mail.yahoo.com>

Ok, I tried that but it didnt work. I can put a conf file here. As a start I am currently in a scenario trying to replace a simple CNTLM HTTP proxy with Squid. I want to configure one account which authenticates with parent proxy send the downstream requests with out taking any creds.
Sent from Yahoo Mail on Android 
 
  On Thu, Mar 9, 2017 at 4:50 PM, Amos Jeffries<squid3 at treenet.co.nz> wrote:   On 10/03/2017 5:19 a.m., S V Hareesh wrote:
> On top of the conf file from default setup on Windows, I added the following line in the conf. I added the dns servers and allowed localhost.
> 
> cache_peer <corporate_proxy> parent 80 0 default connection-auth=on proxy-only
> 
> never_direct allow all
> 
> When I point my browser to this proxy, it gives me 407, auth required. 
> 
> Also, configured squid service on windows to run with a service account that has access to Internet/corp proxy.

Squid cannot authenticate to a cache_peer using NTLM. It can only do
Nagotiate/Kerberos to the parent proxy, and only when "login=NEGOTIATE"
is added (with or without a named keytab file).

NOTE: 'connection-auth=on' is about allowing the browser to use NTLM or
Negotiate/Kerberos through the cache_peer. It needs to also have
"login=PASSTHRU" if that peer is a proxy (as opposed to a web or
Exchange server).

See the 'AUTHENTICATION OPTIONS' section of
<http://www.squid-cache.org/Doc/config/cache_peer/>

Amos

_______________________________________________
squid-users mailing list
squid-users at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-users
  
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20170309/0c722157/attachment.htm>

From squid3 at treenet.co.nz  Thu Mar  9 17:04:21 2017
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Fri, 10 Mar 2017 06:04:21 +1300
Subject: [squid-users] Login=PASS --> Query
In-Reply-To: <1489069407227-4681714.post@n4.nabble.com>
References: <1489069407227-4681714.post@n4.nabble.com>
Message-ID: <4aa63890-26e5-8718-43d3-124cffb5ce5c@treenet.co.nz>

On 10/03/2017 3:23 a.m., Hareesh wrote:
> Hi
> 
> I have a query related to the option Login=PASS in cache-peer. The
> documentation mentions the following.
> 
> 	login=PASS	Send login details received from client to this peer.
> 			Authentication is not required by this option.
> 			
> 			*If there are no client-provided authentication headers
> 			to pass on, but username and password are available
> 			from an external ACL user= and password= result tags
> 			they may be sent instead.*
> 
> I want to understand what do they exactly mean by the text given *bold*?
> Where and how can the user and password be given as acls.
> 
> Can some one please shed some light with possible example?
> 

The usual purpose of external-ACL helper (external_acl_type) is to do
authorization (allowed/denied) checks (*not* authentication!!).

But it can also do out-of-band processing on what it gets given (eg
Cookie header, or WWW-Auth* header with custom scheme type, or IP and
IDENT values) and send back a response like "OK user=blah password=hello".

If there is no authenticated HTTP-auth login credentials for the request
these external-ACL provided credentials may be used to fulfill the
login=PASS requirement of delivering a Basic authentication header to
the peer.

Amos



From uhlar at fantomas.sk  Thu Mar  9 17:18:18 2017
From: uhlar at fantomas.sk (Matus UHLAR - fantomas)
Date: Thu, 9 Mar 2017 18:18:18 +0100
Subject: [squid-users] squid workers question
In-Reply-To: <19dd204a-2f6b-914d-2858-5afdb521aaea@treenet.co.nz>
References: <20170309142134.GA30976@fantomas.sk>
 <ce49fdac-98e7-3d9f-5710-76b083dc3829@treenet.co.nz>
 <20170309161415.GA7401@fantomas.sk>
 <19dd204a-2f6b-914d-2858-5afdb521aaea@treenet.co.nz>
Message-ID: <20170309171818.GB8584@fantomas.sk>

On 10.03.17 05:30, Amos Jeffries wrote:
>> seems that my memory problem is somehow related to 4g of "2K Buffers"
>> whatever that means. This is cachrmgr output:
>>
>>
>>         (bytes)    KB/ch    obj/ch    (#)    used    free    part
>> %Frag    (#)    (KB)    high (KB)    high (hrs)    %Tot    (#)
>> (KB)    high (KB)    high (hrs)    %alloc    (#)    (KB)    high (KB)
>> (#)        %cnt    %vol    (#)/sec
>> 2K Buffer    2048                                1986398    3972796
>> 3972796        0.00        89.763    1986390    3972780
>> 3972796        0.00        100.000    8    16    198        10736355
>> 4.914    19.208    0.009
>>
>
>Ah. So anything that is using a generic 2KB of memory. Tricky to track
>down :-(.
>
>I would look at the next few entries to see if there is a good clue
>about what system might be worth a closer look (largest amount of things
>active, or anything else not being released). Mostly it is I/O using the
>various Buffer's, but some other things do as well.

will be glad if this helps...

			(bytes)	KB/ch	obj/ch	(#)	used	free	part	%Frag	(#)	(KB)	high (KB)	high (hrs)	%Tot	(#)	(KB)	high (KB)	high (hrs)	%alloc	(#)	(KB)	high (KB)	(#)		%cnt	%vol	(#)/sec
mem_node		4136								62447	252228	274188		2.37		5.699	62423	252131	274188		2.37		99.962	24	97	26545		6917600		3.166	24.994	0.005
cbdata MemBuf (10)	64								2015362	125961	125961		0.01		2.846	2015348	125960	125961		0.01		99.999	14	1	103		3800466		1.739	0.212	0.004
Short Strings		40								670389	26188	28203		2.51		0.592	670008	26173	28203		2.51		99.943	381	15	1423		113557901	51.974	3.968	0.078


>>>> cache_dir rock /var/spool/squid3/rock 1024 max-size=32768
>>>> #cache_dir aufs /var/spool/squid3 8192 16 256 min-size=32769
>>>>
>>>> are those correct values? (bug 3411 says something about 256B metadata)
>>
>>> Those 256 Byte will matter for Squid-3.4.
>>
>> doesn't it for later squid versions?
>
>Nope :-). Squid-3.5 'large rock' feature adds slots as needed to fit the
>extra meta bytes. So 32KB is no longer an absolute limit.

will it waste whole slot or does it already support smaller chunks?

-- 
Matus UHLAR - fantomas, uhlar at fantomas.sk ; http://www.fantomas.sk/
Warning: I wish NOT to receive e-mail advertising to this address.
Varovanie: na tuto adresu chcem NEDOSTAVAT akukolvek reklamnu postu.
Linux - It's now safe to turn on your computer.
Linux - Teraz mozete pocitac bez obav zapnut.


From squid3 at treenet.co.nz  Thu Mar  9 17:19:48 2017
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Fri, 10 Mar 2017 06:19:48 +1300
Subject: [squid-users] Setting Up Squid - my scenario
In-Reply-To: <821797722.2515523.1489078809096@mail.yahoo.com>
References: <1190296055.2535835.1489076385522.ref@mail.yahoo.com>
 <1190296055.2535835.1489076385522@mail.yahoo.com>
 <b4a65caf-0332-5a56-915a-dbffcbe07fc7@treenet.co.nz>
 <821797722.2515523.1489078809096@mail.yahoo.com>
Message-ID: <3b7d2a8e-c915-59a4-b6c5-759b80b9a289@treenet.co.nz>

On 10/03/2017 6:00 a.m., S V Hareesh wrote:
> Ok, I tried that but it didnt work. I can put a conf file here. As a
> start I am currently in a scenario trying to replace a simple CNTLM
> HTTP proxy with Squid. I want to configure one account which
> authenticates with parent proxy send the downstream requests with out
> taking any creds.

If your CNTLM was running on the Squid machine and using the credentials
for the service account you have setup Squid to use now - then the
cache_peer login=NEGOTIATE should make Squid operate as equivalent to
what CNTLM was doing.

The config file would be useful for anyone who follows up (not just me,
who will be out of time shortly for another few days).

Also, if you can track what HTTP messages are happening and whether the
Kerberos is working properly for the Squid->parent messages it would be
useful.

The current Squid can provide HTTP details in cache.log with
"debug_options 11,2". I'm not sure how you would test the Kerberos on a
Windows installation, but the Negotiate auth headers in those messages
might give a few clues anyway.

HTH
Amos


>   On Thu, Mar 9, 2017 at 4:50 PM, Amos Jeffrieswrote:
>>     On 10/03/2017 5:19 a.m., S V Hareesh wrote:
>> On top of the conf file from default setup on Windows, I added the following line in the conf. I added the dns servers and allowed localhost.
>>
>> cache_peer <corporate_proxy> parent 80 0 default connection-auth=on proxy-only
>>
>> never_direct allow all
>>
>> When I point my browser to this proxy, it gives me 407, auth required. 
>>
>> Also, configured squid service on windows to run with a service account that has access to Internet/corp proxy.
> 
> Squid cannot authenticate to a cache_peer using NTLM. It can only do
> Nagotiate/Kerberos to the parent proxy, and only when "login=NEGOTIATE"
> is added (with or without a named keytab file).
> 
> NOTE: 'connection-auth=on' is about allowing the browser to use NTLM or
> Negotiate/Kerberos through the cache_peer. It needs to also have
> "login=PASSTHRU" if that peer is a proxy (as opposed to a web or
> Exchange server).
> 
> See the 'AUTHENTICATION OPTIONS' section of
> <http://www.squid-cache.org/Doc/config/cache_peer/>
> 
> Amos
> 
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>   
> 



From uhlar at fantomas.sk  Thu Mar  9 17:24:36 2017
From: uhlar at fantomas.sk (Matus UHLAR - fantomas)
Date: Thu, 9 Mar 2017 18:24:36 +0100
Subject: [squid-users] squid workers question
In-Reply-To: <8ef542a5-dba3-236f-224b-944990d4cd68@measurement-factory.com>
References: <20170309142134.GA30976@fantomas.sk>
 <f547a991-3178-7b95-113d-fb4b533c3056@measurement-factory.com>
 <20170309161940.GB7401@fantomas.sk>
 <6ca274d9-948c-01c5-c3dd-13044d3cfe7b@measurement-factory.com>
 <20170309165429.GA8584@fantomas.sk>
 <8ef542a5-dba3-236f-224b-944990d4cd68@measurement-factory.com>
Message-ID: <20170309172435.GC8584@fantomas.sk>

>On 03/09/2017 09:54 AM, Matus UHLAR - fantomas wrote:
>>>>> Master is not a
>>>>> kid (it is a parent of all kids), the first N kids are workers, the
>>>>> next D kids are diskers, and the last kid is Coordinator. Please see the
>>>>> following wiki section for more details.
>>>>>   http://wiki.squid-cache.org/Features/SmpScale#Terminology
>
>>>>> Yes, "1" is the default value for the workers directive.
>
>>>> and this is why I wonder we have three kids, both when "workers"
>>>> commented out and when set to 1.
>
>>> In my earlier paragraph, N is the value of the workers directive. There
>>> are other kids (D diskers and one Coordinator). D is the number of rock
>>> cache_dirs. Both workers and diskers need shared memory and IPC.
>>>
>>> If you want to turn off SMP while using rock store, start Squid with -N.
>>> When started with -N, there will be a single process playing all four
>>> roles (master, worker, disker, and Coordinator).
>
>> Will running with "workers 1" avoid this issue while using separate
>> processes for diskers?


On 09.03.17 10:01, Alex Rousskov wrote:
>No. Disker processes need shared memory to communicate with worker(s).

I should rephrase my question:

since aufs is incompatible with SMP and rockstore runs separate diskers, 
is running aufs with rock store and safe, when not running with "-N"?

Thanks
-- 
Matus UHLAR - fantomas, uhlar at fantomas.sk ; http://www.fantomas.sk/
Warning: I wish NOT to receive e-mail advertising to this address.
Varovanie: na tuto adresu chcem NEDOSTAVAT akukolvek reklamnu postu.
M$ Win's are shit, do not use it !


From harishmeetsu at yahoo.com  Thu Mar  9 17:30:28 2017
From: harishmeetsu at yahoo.com (S V Hareesh)
Date: Thu, 9 Mar 2017 17:30:28 +0000 (UTC)
Subject: [squid-users] Setting Up Squid - my scenario
In-Reply-To: <3b7d2a8e-c915-59a4-b6c5-759b80b9a289@treenet.co.nz>
References: <1190296055.2535835.1489076385522.ref@mail.yahoo.com>
 <1190296055.2535835.1489076385522@mail.yahoo.com>
 <b4a65caf-0332-5a56-915a-dbffcbe07fc7@treenet.co.nz>
 <821797722.2515523.1489078809096@mail.yahoo.com>
 <3b7d2a8e-c915-59a4-b6c5-759b80b9a289@treenet.co.nz>
Message-ID: <209881710.2578283.1489080628327@mail.yahoo.com>

Here is the conf file.
http_port 3128cache_peer <corp_proxy_IP> parent <Parent_proxy_port> 0 no-query default proxy-only login=NEGOTIATE?
acl manager proto cache_objectacl localhost src 127.0.0.1/255.255.255.255acl all src 0.0.0.0/0.0.0.0http_access allow allnever_direct allow allicp_access deny all
cache_effective_user squid#cache_effective_group wheel
max_filedescriptors 3200?I am not sure what is it that I am missing. I have been wire sharking both the resultant packets when using CNTLM and when using squid, I can see the proxy auth header piece in the Squid packet missing.
CheersS.V.Hareeshhttp://svhareesh.blogspot.com?
 

    On Thursday, 9 March 2017 5:20 PM, Amos Jeffries <squid3 at treenet.co.nz> wrote:
 

 On 10/03/2017 6:00 a.m., S V Hareesh wrote:
> Ok, I tried that but it didnt work. I can put a conf file here. As a
> start I am currently in a scenario trying to replace a simple CNTLM
> HTTP proxy with Squid. I want to configure one account which
> authenticates with parent proxy send the downstream requests with out
> taking any creds.

If your CNTLM was running on the Squid machine and using the credentials
for the service account you have setup Squid to use now - then the
cache_peer login=NEGOTIATE should make Squid operate as equivalent to
what CNTLM was doing.

The config file would be useful for anyone who follows up (not just me,
who will be out of time shortly for another few days).

Also, if you can track what HTTP messages are happening and whether the
Kerberos is working properly for the Squid->parent messages it would be
useful.

The current Squid can provide HTTP details in cache.log with
"debug_options 11,2". I'm not sure how you would test the Kerberos on a
Windows installation, but the Negotiate auth headers in those messages
might give a few clues anyway.

HTH
Amos


>? On Thu, Mar 9, 2017 at 4:50 PM, Amos Jeffrieswrote:
>>? ? On 10/03/2017 5:19 a.m., S V Hareesh wrote:
>> On top of the conf file from default setup on Windows, I added the following line in the conf. I added the dns servers and allowed localhost.
>>
>> cache_peer <corporate_proxy> parent 80 0 default connection-auth=on proxy-only
>>
>> never_direct allow all
>>
>> When I point my browser to this proxy, it gives me 407, auth required. 
>>
>> Also, configured squid service on windows to run with a service account that has access to Internet/corp proxy.
> 
> Squid cannot authenticate to a cache_peer using NTLM. It can only do
> Nagotiate/Kerberos to the parent proxy, and only when "login=NEGOTIATE"
> is added (with or without a named keytab file).
> 
> NOTE: 'connection-auth=on' is about allowing the browser to use NTLM or
> Negotiate/Kerberos through the cache_peer. It needs to also have
> "login=PASSTHRU" if that peer is a proxy (as opposed to a web or
> Exchange server).
> 
> See the 'AUTHENTICATION OPTIONS' section of
> <http://www.squid-cache.org/Doc/config/cache_peer/>
> 
> Amos
> 
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>? 
> 



   
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20170309/305977ce/attachment.htm>

From vero.ovando at live.com  Thu Mar  9 17:33:08 2017
From: vero.ovando at live.com (=?gb2312?B?VmVyqK5uaWNhIE92YW5kbw==?=)
Date: Thu, 9 Mar 2017 17:33:08 +0000
Subject: [squid-users] ext_wbinfo_group_acl is not working
Message-ID: <CY1PR15MB0907AFB73B2F304BF587005E9E210@CY1PR15MB0907.namprd15.prod.outlook.com>

Hi, everybody!


I have my Squid 3.4.8 running in Debian Jessie. It has been working with Active Directory authentication for more than a year without any kind of problem. But since a couple of weeks ago, suddenly, it stopped authenticate users, asking for credentials (username and pass) and they are not able to browse. I am getting this messages in /var/log/cache.log:


2017/03/04 12:04:25.806 kid1| WARNING: external ACL 'Grupos_AD' queue overload. Request rejected 'user1 it_group'.


After some research I found this thread http://www.squid-cache.org/mail-archive/squid-users/200902/0386.html and followed the suggestions posted by Amos. But nothing happened.

I tried rejoining the server to domain. Everything was fine in that way: wbinfo -u, wbinfo -g and wbinfo -P correctly returns all the users, groups and information of the domain.


After restart Squid service, I noticed that neither helper ext_wbinfo_group_acl nor pinger are started:


12:04:01 [root at server ]# systemctl status squid3.service -l
? squid3.service - LSB: Squid HTTP Proxy version 3.x
   Loaded: loaded (/etc/init.d/squid3)
   Active: active (running) since s?b 2017-03-04 12:04:01 ART; 3s ago
  Process: 4537 ExecStop=/etc/init.d/squid3 stop (code=exited, status=0/SUCCESS)
  Process: 4560 ExecStart=/etc/init.d/squid3 start (code=exited, status=0/SUCCESS)
   CGroup: /system.slice/squid3.service
           ??4593 /usr/sbin/squid3 -YC -f /etc/squid3/squid.conf
           ??4595 (squid-1) -YC -f /etc/squid3/squid.conf
           ??4596 (ntlm_auth) --helper-protocol=squid-2.5-ntlmssp --DOMAIN=MYDOMAIN
mar 04 12:04:01 server.mydomain.com squid3[4560]: Starting Squid HTTP Proxy 3.x: squid32017/03/04 12:04:01| WARNING: external_acl_type option children=N has been deprecated in favor of children-max=N and children-startup=N
mar 04 12:04:01 server.mydomain.com squid3[4593]: Squid Parent: will start 1 kids
mar 04 12:04:01 server.mydomain.com squid3[4593]: Squid Parent: (squid-1) process 4595 started
mar 04 12:04:01 server.mydomain.com squid3[4560]: .


12:04:30 [root at server ]# ps fax | grep ext_wbinfo_group_acl
 1418 pts/0    S+     0:00              \_ grep ext_wbinfo_group_acl

If I run echo "mydomain\user1 it_group" | /usr/lib/squid3/ext_wbinfo_group_acl -d, it returns

Debugging mode ON.
Got mydomain\user1 it_group from squid
User:  -mydomain\user1-
Group: -it_group-
SID:   -S-1-5-21-2290000000-711000000-3300000000-3949-
GID:   -10006-
Sending OK to squid
OK

What it's a good, because that user belongs to that group. If I change the group name, it returns an ERR.

Here is my squid.conf:

#===========================================================================
http_port 3128
visible_hostname proxy.squid
cache_mgr server at proxy.com<mailto:server at proxy.com>
cache_effective_user proxy
error_directory /usr/share/squid3/errors/es
err_page_stylesheet /etc/squid3/estilo.css

####################################################
#******************************Ports*************************************#
####################################################

#acl manager proto cache_object
#acl all src 0.0.0.0/0.0.0.0
#acl localhost src 127.0.0.1/32
acl SSL_ports port 443
acl Safe_ports port 80
acl Safe_ports port 21
acl Safe_ports port 443
acl Safe_ports port 70 #prot gopher
acl Safe_ports port 210 #whais
acl Safe_ports port 280 #http-mgmt
acl Safe_ports port 488 #gss-http
acl Safe_ports port 591 #filemaker
acl Safe_ports port 8080
acl Safe_ports port 2481
acl Safe_ports port 20010
acl Safe_ports port 777 #multi http
#acl purge method PURGE
acl CONNECT method CONNECT

acl_uses_indirect_client on
delay_pool_uses_indirect_client on
log_uses_indirect_client on


##############################################################
#*******************Active Directory HELPERS**************************#
##############################################################

auth_param ntlm program /usr/bin/ntlm_auth --helper-protocol=squid-2.5-ntlmssp --DOMAIN=MYDOMAIN
auth_param ntlm children 100
auth_param ntlm keep_alive off

auth_param basic program /usr/bin/ntlm_auth --helper-protocol=squid-2.5-basic
auth_param basic children 100
auth_param basic realm Servidor proxy-cache
auth_param basic credentialsttl 2 hours


#######################################################################
#****************************ACL******************************************#
###########################################################################

#---------------------------ACL Active Directory------------------------#
external_acl_type Grupos_AD ttl=10 negative_ttl=10 children=100 %LOGIN /usr/lib/squid3/ext_wbinfo_group_acl -d
acl it_group external Grupos_AD it_group

------------------Acceso s?lo a usuarios autenticados--------------------#
acl auth proxy_auth REQUIRED
http_access deny !auth

#-----------------------------Grupo *it_group*----------------------------#
http_access allow it_group allow

http_access allow manager localhost
http_access deny manager
#http_access allow purge localhost
#http_access deny purge
http_access deny !Safe_ports
http_access deny CONNECT !SSL_PORTS

http_access deny all

dead_peer_timeout 20 seconds
strip_query_terms on
debug_options ALL,1 33,2 28,9
coredump_dir /var/spool/squid3
ftp_passive on
ftp_sanitycheck off
ftp_telnet_protocol off
read_ahead_gap 1 MB
positive_dns_ttl 6 hours
forward_max_tries 25


############################################################################
#*************************Log********************************#
############################################################################

logformat squid %ts.%03tu %6tr %>a %Ss/%03>Hs %<st %rm %ru %un %Sh/%<A %mt
cache_access_log /var/log/squid3/access.log
cache_log /var/log/squid3/cache.log
logfile_rotate 0

############################################################################
#******************Cache and memory***************************#
############################################################################

cache_mem 1024 MB
maximum_object_size_in_memory 1024 KB
memory_cache_mode always
cache_dir aufs /var/spool/squid3 15000 16 256
maximum_object_size 96 MB
minimum_object_size 10 KB
#cache_replacement_policy heap LFUDA
cache_replacement_policy heap GDSF
memory_replacement_policy heap GDSF
#memory_replacement_policy lru
cache_store_log none
#log_fqdn off
log_icp_queries off
buffered_logs off
#emulate_httpd_log off
redirect_rewrites_host_header off
cache_swap_low 80
cache_swap_high 95

#===========================================================================

It is really weird, I really don't know how to solve this. I hope my explanation was clear.

For testing purposes, I have another Squid working with the same AD server, and it is going fine: the helper and pinger are executed as you can see here:

root at debian-test-server:/etc/squid3# systemctl status squid3.service
? squid3.service - LSB: Squid HTTP Proxy version 3.x
   Loaded: loaded (/etc/init.d/squid3)
   Active: active (running) since lun 2017-02-13 07:35:01 ART; 2 weeks 5 days ago
  Process: 570 ExecStart=/etc/init.d/squid3 start (code=exited, status=0/SUCCESS)
   CGroup: /system.slice/squid3.service
           ?? 1017 /usr/sbin/squid3 -YC -f /etc/squid3/squid.conf
           ?? 1020 (squid-1) -YC -f /etc/squid3/squid.conf
           ?? 1945 /usr/bin/perl -w /usr/lib/squid3/ext_wbinfo_group_acl -d
           ?? 1968 (ntlm_auth) --helper-protocol=squid-2.5-ntlmssp --DOMAIN=MYDOMAIN
           ?? 1969 (ntlm_auth) --helper-protocol=squid-2.5-ntlmssp --DOMAIN=MYDOMAIN
           ?? 1970 (ntlm_auth) --helper-protocol=squid-2.5-ntlmssp --DOMAIN=MYDOMAIN
           ?? 1971 (ntlm_auth) --helper-protocol=squid-2.5-ntlmssp --DOMAIN=MYDOMAIN
           ?? 1972 (ntlm_auth) --helper-protocol=squid-2.5-ntlmssp --DOMAIN=MYDOMAIN
           ?? 1973 (ntlm_auth) --helper-protocol=squid-2.5-ntlmssp --DOMAIN=MYDOMAIN
           ?? 1974 (ntlm_auth) --helper-protocol=squid-2.5-ntlmssp --DOMAIN=MYDOMAIN
           ?? 1993 /usr/bin/perl -w /usr/lib/squid3/ext_wbinfo_group_acl -d
           ?? 2029 /usr/bin/perl -w /usr/lib/squid3/ext_wbinfo_group_acl -d
           ??63477 (pinger)
           ??63478 (ntlm_auth) --helper-protocol=squid-2.5-ntlmssp --DOMAIN=MYDOMAIN
           ??63479 (ntlm_auth) --helper-protocol=squid-2.5-basic
           ??63480 /usr/bin/perl -w /usr/lib/squid3/ext_wbinfo_group_acl -d

I also tried reinstalling Squid. Nothing happened.

I will appreciate your help!

Thanks!

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20170309/ee94f1fd/attachment.htm>

From uhlar at fantomas.sk  Thu Mar  9 17:36:29 2017
From: uhlar at fantomas.sk (Matus UHLAR - fantomas)
Date: Thu, 9 Mar 2017 18:36:29 +0100
Subject: [squid-users] pinger without peers
Message-ID: <20170309173629.GA9814@fantomas.sk>

Hello,

does it have sense to run pinger without having cache peers configured?

if I get the "Network DB Statistics:" output properly, it seems that 33% of
hosts is unreachable.

-- 
Matus UHLAR - fantomas, uhlar at fantomas.sk ; http://www.fantomas.sk/
Warning: I wish NOT to receive e-mail advertising to this address.
Varovanie: na tuto adresu chcem NEDOSTAVAT akukolvek reklamnu postu.
Linux IS user friendly, it's just selective who its friends are...


From rousskov at measurement-factory.com  Thu Mar  9 17:49:52 2017
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Thu, 9 Mar 2017 10:49:52 -0700
Subject: [squid-users] squid workers question
In-Reply-To: <20170309171818.GB8584@fantomas.sk>
References: <20170309142134.GA30976@fantomas.sk>
 <ce49fdac-98e7-3d9f-5710-76b083dc3829@treenet.co.nz>
 <20170309161415.GA7401@fantomas.sk>
 <19dd204a-2f6b-914d-2858-5afdb521aaea@treenet.co.nz>
 <20170309171818.GB8584@fantomas.sk>
Message-ID: <c76ff797-0487-99c1-d9bb-7d75f0476a06@measurement-factory.com>

On 03/09/2017 10:18 AM, Matus UHLAR - fantomas wrote:
>>>>> cache_dir rock /var/spool/squid3/rock 1024 max-size=32768
>>>>> #cache_dir aufs /var/spool/squid3 8192 16 256 min-size=32769
>>>>>
>>>>> are those correct values? (bug 3411 says something about 256B
>>>>> metadata)
>>>
>>>> Those 256 Byte will matter for Squid-3.4.
>>>
>>> doesn't it for later squid versions?
>>
>> Nope :-). Squid-3.5 'large rock' feature adds slots as needed to fit the
>> extra meta bytes. So 32KB is no longer an absolute limit.
> 
> will it waste whole slot or does it already support smaller chunks?

Just like a regular file system used by AUFS, Rock always uses the whole
slot, usually wasting some space. Please note that smaller
slots/pages/chunks also waste various resources (including space in some
cases!).

Alex.



From mikes at surcouf.co.uk  Thu Mar  9 17:57:37 2017
From: mikes at surcouf.co.uk (Mike Surcouf)
Date: Thu, 9 Mar 2017 17:57:37 +0000
Subject: [squid-users] microsoft edge and proxy auth not working
In-Reply-To: <DB6PR0401MB2680D3C0EF7481BC9565DDEA8F210@DB6PR0401MB2680.eurprd04.prod.outlook.com>
References: <1FCF9DA5B29068478ECF15896F19F08401B8AC0D2C@Y011008.bk.fin.local>
 <1FCF9DA5B29068478ECF15896F19F08401B8AC0D3D@Y011008.bk.fin.local>
 <8b10adcd-f55a-65ba-cd37-28851575f6e0@treenet.co.nz>
 <DB6PR0401MB2680D3C0EF7481BC9565DDEA8F210@DB6PR0401MB2680.eurprd04.prod.outlook.com>
Message-ID: <2197768425D7F5479A0FFB3FEC212F7FF5D8D1A3@aesmail.surcouf.local>

Hi Rafael

Is there any reason you can't use Kerberos.
Note you will need to create a keytab but the setup is not that hard and in the docs.
I use it very successfully on window AD network.

auth_param negotiate program /usr/lib64/squid/negotiate_kerberos_auth
auth_param negotiate children 20
auth_param negotiate keep_alive on

Thanks

Mike

-----Original Message-----
From: squid-users [mailto:squid-users-bounces at lists.squid-cache.org] On Behalf Of Rafael Akchurin
Sent: 09 March 2017 17:01
To: Amos Jeffries; squid-users at lists.squid-cache.org
Subject: Re: [squid-users] microsoft edge and proxy auth not working

Hello Amos, Markus, all,

Just as a side note - I also suffered  from this error sometime before with Edge and our custom NTLM relay to domain controllers (run as auth helper by Squid). The strange thing it went away after installing some (unknown) Windows update.

I do have the "auth_param ntlm keep_alive off" in the config though.

It all makes me quite suspicious the error was/is in Edge or in my curly hands.

Best regards,
Rafael Akchurin
Diladele B.V.

-----Original Message-----
From: squid-users [mailto:squid-users-bounces at lists.squid-cache.org] On Behalf Of Amos Jeffries
Sent: Thursday, March 9, 2017 5:12 PM
To: squid-users at lists.squid-cache.org
Subject: Re: [squid-users] microsoft edge and proxy auth not working

On 8/03/2017 11:28 p.m., Rietzler, Markus (RZF, Aufg 324 /
<RIETZLER_SOFTWARE>) wrote:
> i should add that we are using squid 3.5.24.
> 

Try with "auth_param ntlm keep_alive off". Recently the browsers have been needing that.

Though frankly I am surprised if Edge supports NTLM at all. It was deprecated in April 2006 and MS announced removal was being actively pushed in all thier software since Win7.

> 
>> -----Urspr?ngliche Nachricht-----
>> Von: Rietzler, Markus
>>
>> we have some windows 10 clients using microsoft edge browser.
>> access to internet is only allowed for authenticated users. we are 
>> using samba/winbind auth
>>
>> auth_param ntlm program /usr/bin/ntlm_auth
>> --helper-protocol=squid-2.5- ntlmssp auth_param ntlm children 64
>> startup=24 idle=12 auth_param ntlm keep_alive on acl auth_user 
>> proxy_auth REQUIRED
>>
>> on windows 10 clients with IE11 it is working (with ntlm automatic
>> auth) on the same machine, with Microsoft edge I get TCP_Denied/407 message.
>> seems I only get one single TCP_DENIED/407 line in accesslog and an 
>> auth dialog pops up. I have disabled basic auth via ntlm.
>> shouldn't there be 3 lines for proxy auth? with IE11 I see those 
>> three lines (2x TCP_DENIED/407 and 1x TCP_MISS/200), no popup at all.

Not specifically. There should be 1+ for NTLM. Success with NTLM shows
2+. Failure shows 1 or 3 or infinite loop (hello Safari and Firefox 30-ish).


>>
>> winbind/samba itself seems to work, as I can do an user auth against 
>> apache with winbind/samba - even over some squid proxies with 
>> connection-auth allowed. but not for proxy-auth.
>> is there any option in squid.conf which prevents Edge to do a 
>> successful auth?

If other software succeeds then the only thing that might be related is the keep-alive option mentioned above. Otherwise the problem is in Edge itself.

Amos

_______________________________________________
squid-users mailing list
squid-users at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-users
_______________________________________________
squid-users mailing list
squid-users at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-users

From rafael.akchurin at diladele.com  Thu Mar  9 18:01:24 2017
From: rafael.akchurin at diladele.com (Rafael Akchurin)
Date: Thu, 9 Mar 2017 18:01:24 +0000
Subject: [squid-users] microsoft edge and proxy auth not working
In-Reply-To: <2197768425D7F5479A0FFB3FEC212F7FF5D8D1A3@aesmail.surcouf.local>
References: <1FCF9DA5B29068478ECF15896F19F08401B8AC0D2C@Y011008.bk.fin.local>
 <1FCF9DA5B29068478ECF15896F19F08401B8AC0D3D@Y011008.bk.fin.local>
 <8b10adcd-f55a-65ba-cd37-28851575f6e0@treenet.co.nz>
 <DB6PR0401MB2680D3C0EF7481BC9565DDEA8F210@DB6PR0401MB2680.eurprd04.prod.outlook.com>,
 <2197768425D7F5479A0FFB3FEC212F7FF5D8D1A3@aesmail.surcouf.local>
Message-ID: <E3D787EC-2561-44C7-98C8-000FE40EDE75@diladele.com>

Hello Mike,

I specifically was debugging our NTLM implementation with Edge :)

Kerberos works just fine, you are correct.

Best regards,
Rafael Akchurin

> Op 9 mrt. 2017 om 18:57 heeft Mike Surcouf <mikes at surcouf.co.uk> het volgende geschreven:
> 
> Hi Rafael
> 
> Is there any reason you can't use Kerberos.
> Note you will need to create a keytab but the setup is not that hard and in the docs.
> I use it very successfully on window AD network.
> 
> auth_param negotiate program /usr/lib64/squid/negotiate_kerberos_auth
> auth_param negotiate children 20
> auth_param negotiate keep_alive on
> 
> Thanks
> 
> Mike
> 
> -----Original Message-----
> From: squid-users [mailto:squid-users-bounces at lists.squid-cache.org] On Behalf Of Rafael Akchurin
> Sent: 09 March 2017 17:01
> To: Amos Jeffries; squid-users at lists.squid-cache.org
> Subject: Re: [squid-users] microsoft edge and proxy auth not working
> 
> Hello Amos, Markus, all,
> 
> Just as a side note - I also suffered  from this error sometime before with Edge and our custom NTLM relay to domain controllers (run as auth helper by Squid). The strange thing it went away after installing some (unknown) Windows update.
> 
> I do have the "auth_param ntlm keep_alive off" in the config though.
> 
> It all makes me quite suspicious the error was/is in Edge or in my curly hands.
> 
> Best regards,
> Rafael Akchurin
> Diladele B.V.
> 
> -----Original Message-----
> From: squid-users [mailto:squid-users-bounces at lists.squid-cache.org] On Behalf Of Amos Jeffries
> Sent: Thursday, March 9, 2017 5:12 PM
> To: squid-users at lists.squid-cache.org
> Subject: Re: [squid-users] microsoft edge and proxy auth not working
> 
> On 8/03/2017 11:28 p.m., Rietzler, Markus (RZF, Aufg 324 /
> <RIETZLER_SOFTWARE>) wrote:
>> i should add that we are using squid 3.5.24.
>> 
> 
> Try with "auth_param ntlm keep_alive off". Recently the browsers have been needing that.
> 
> Though frankly I am surprised if Edge supports NTLM at all. It was deprecated in April 2006 and MS announced removal was being actively pushed in all thier software since Win7.
> 
>> 
>>> -----Urspr?ngliche Nachricht-----
>>> Von: Rietzler, Markus
>>> 
>>> we have some windows 10 clients using microsoft edge browser.
>>> access to internet is only allowed for authenticated users. we are 
>>> using samba/winbind auth
>>> 
>>> auth_param ntlm program /usr/bin/ntlm_auth
>>> --helper-protocol=squid-2.5- ntlmssp auth_param ntlm children 64
>>> startup=24 idle=12 auth_param ntlm keep_alive on acl auth_user 
>>> proxy_auth REQUIRED
>>> 
>>> on windows 10 clients with IE11 it is working (with ntlm automatic
>>> auth) on the same machine, with Microsoft edge I get TCP_Denied/407 message.
>>> seems I only get one single TCP_DENIED/407 line in accesslog and an 
>>> auth dialog pops up. I have disabled basic auth via ntlm.
>>> shouldn't there be 3 lines for proxy auth? with IE11 I see those 
>>> three lines (2x TCP_DENIED/407 and 1x TCP_MISS/200), no popup at all.
> 
> Not specifically. There should be 1+ for NTLM. Success with NTLM shows
> 2+. Failure shows 1 or 3 or infinite loop (hello Safari and Firefox 30-ish).
> 
> 
>>> 
>>> winbind/samba itself seems to work, as I can do an user auth against 
>>> apache with winbind/samba - even over some squid proxies with 
>>> connection-auth allowed. but not for proxy-auth.
>>> is there any option in squid.conf which prevents Edge to do a 
>>> successful auth?
> 
> If other software succeeds then the only thing that might be related is the keep-alive option mentioned above. Otherwise the problem is in Edge itself.
> 
> Amos
> 
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users

From rousskov at measurement-factory.com  Thu Mar  9 18:02:17 2017
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Thu, 9 Mar 2017 11:02:17 -0700
Subject: [squid-users] squid workers question
In-Reply-To: <20170309172435.GC8584@fantomas.sk>
References: <20170309142134.GA30976@fantomas.sk>
 <f547a991-3178-7b95-113d-fb4b533c3056@measurement-factory.com>
 <20170309161940.GB7401@fantomas.sk>
 <6ca274d9-948c-01c5-c3dd-13044d3cfe7b@measurement-factory.com>
 <20170309165429.GA8584@fantomas.sk>
 <8ef542a5-dba3-236f-224b-944990d4cd68@measurement-factory.com>
 <20170309172435.GC8584@fantomas.sk>
Message-ID: <7b386e00-97ed-054c-902f-0df367e8ec49@measurement-factory.com>

On 03/09/2017 10:24 AM, Matus UHLAR - fantomas wrote:
>> On 03/09/2017 09:54 AM, Matus UHLAR - fantomas wrote:
>>>>>> Master is not a
>>>>>> kid (it is a parent of all kids), the first N kids are workers, the
>>>>>> next D kids are diskers, and the last kid is Coordinator. Please
>>>>>> see the
>>>>>> following wiki section for more details.
>>>>>>   http://wiki.squid-cache.org/Features/SmpScale#Terminology
>>
>>>>>> Yes, "1" is the default value for the workers directive.
>>
>>>>> and this is why I wonder we have three kids, both when "workers"
>>>>> commented out and when set to 1.
>>
>>>> In my earlier paragraph, N is the value of the workers directive. There
>>>> are other kids (D diskers and one Coordinator). D is the number of rock
>>>> cache_dirs. Both workers and diskers need shared memory and IPC.
>>>>
>>>> If you want to turn off SMP while using rock store, start Squid with
>>>> -N.
>>>> When started with -N, there will be a single process playing all four
>>>> roles (master, worker, disker, and Coordinator).
>>
>>> Will running with "workers 1" avoid this issue while using separate
>>> processes for diskers?
> 
> 
> On 09.03.17 10:01, Alex Rousskov wrote:
>> No. Disker processes need shared memory to communicate with worker(s).
> 
> I should rephrase my question:
> 
> since aufs is incompatible with SMP and rockstore runs separate diskers,

To clarify: Rock store is supported with our without separate disker
processes. Rock is SMP-aware, SMP-capable, and nonSMP-capable. All other
stores are SMP-unaware and nonSMP-capable.


> is running aufs with rock store and safe, when not running with "-N"?

Running a combination of AUFS and Rock stores in non-SMP mode may be
safe but primary Rock store developers do not test this combination.

Running AUFS in SMP mode is unsafe by default but some admins use
configuration hacks to make it work for them. Primary Store developers
do not test such configurations (or AUFS in general).

Running a combination of AUFS and Rock stores in SMP mode is crazy.

Alex.



From mikes at surcouf.co.uk  Thu Mar  9 18:09:11 2017
From: mikes at surcouf.co.uk (Mike Surcouf)
Date: Thu, 9 Mar 2017 18:09:11 +0000
Subject: [squid-users] microsoft edge and proxy auth not working
In-Reply-To: <E3D787EC-2561-44C7-98C8-000FE40EDE75@diladele.com>
References: <1FCF9DA5B29068478ECF15896F19F08401B8AC0D2C@Y011008.bk.fin.local>
 <1FCF9DA5B29068478ECF15896F19F08401B8AC0D3D@Y011008.bk.fin.local>
 <8b10adcd-f55a-65ba-cd37-28851575f6e0@treenet.co.nz>
 <DB6PR0401MB2680D3C0EF7481BC9565DDEA8F210@DB6PR0401MB2680.eurprd04.prod.outlook.com>,
 <2197768425D7F5479A0FFB3FEC212F7FF5D8D1A3@aesmail.surcouf.local>
 <E3D787EC-2561-44C7-98C8-000FE40EDE75@diladele.com>
Message-ID: <2197768425D7F5479A0FFB3FEC212F7FF5D8D297@aesmail.surcouf.local>

Ah OK sorry
I am curious why you have a reason to use NTLM over Kerberos? :-)

-----Original Message-----
From: Rafael Akchurin [mailto:rafael.akchurin at diladele.com] 
Sent: 09 March 2017 18:01
To: Mike Surcouf
Cc: Amos Jeffries; squid-users at lists.squid-cache.org
Subject: Re: [squid-users] microsoft edge and proxy auth not working

Hello Mike,

I specifically was debugging our NTLM implementation with Edge :)

Kerberos works just fine, you are correct.

Best regards,
Rafael Akchurin

> Op 9 mrt. 2017 om 18:57 heeft Mike Surcouf <mikes at surcouf.co.uk> het volgende geschreven:
> 
> Hi Rafael
> 
> Is there any reason you can't use Kerberos.
> Note you will need to create a keytab but the setup is not that hard and in the docs.
> I use it very successfully on window AD network.
> 
> auth_param negotiate program /usr/lib64/squid/negotiate_kerberos_auth
> auth_param negotiate children 20
> auth_param negotiate keep_alive on
> 
> Thanks
> 
> Mike
> 
> -----Original Message-----
> From: squid-users [mailto:squid-users-bounces at lists.squid-cache.org] 
> On Behalf Of Rafael Akchurin
> Sent: 09 March 2017 17:01
> To: Amos Jeffries; squid-users at lists.squid-cache.org
> Subject: Re: [squid-users] microsoft edge and proxy auth not working
> 
> Hello Amos, Markus, all,
> 
> Just as a side note - I also suffered  from this error sometime before with Edge and our custom NTLM relay to domain controllers (run as auth helper by Squid). The strange thing it went away after installing some (unknown) Windows update.
> 
> I do have the "auth_param ntlm keep_alive off" in the config though.
> 
> It all makes me quite suspicious the error was/is in Edge or in my curly hands.
> 
> Best regards,
> Rafael Akchurin
> Diladele B.V.
> 
> -----Original Message-----
> From: squid-users [mailto:squid-users-bounces at lists.squid-cache.org] 
> On Behalf Of Amos Jeffries
> Sent: Thursday, March 9, 2017 5:12 PM
> To: squid-users at lists.squid-cache.org
> Subject: Re: [squid-users] microsoft edge and proxy auth not working
> 
> On 8/03/2017 11:28 p.m., Rietzler, Markus (RZF, Aufg 324 /
> <RIETZLER_SOFTWARE>) wrote:
>> i should add that we are using squid 3.5.24.
>> 
> 
> Try with "auth_param ntlm keep_alive off". Recently the browsers have been needing that.
> 
> Though frankly I am surprised if Edge supports NTLM at all. It was deprecated in April 2006 and MS announced removal was being actively pushed in all thier software since Win7.
> 
>> 
>>> -----Urspr?ngliche Nachricht-----
>>> Von: Rietzler, Markus
>>> 
>>> we have some windows 10 clients using microsoft edge browser.
>>> access to internet is only allowed for authenticated users. we are 
>>> using samba/winbind auth
>>> 
>>> auth_param ntlm program /usr/bin/ntlm_auth
>>> --helper-protocol=squid-2.5- ntlmssp auth_param ntlm children 64
>>> startup=24 idle=12 auth_param ntlm keep_alive on acl auth_user 
>>> proxy_auth REQUIRED
>>> 
>>> on windows 10 clients with IE11 it is working (with ntlm automatic
>>> auth) on the same machine, with Microsoft edge I get TCP_Denied/407 message.
>>> seems I only get one single TCP_DENIED/407 line in accesslog and an 
>>> auth dialog pops up. I have disabled basic auth via ntlm.
>>> shouldn't there be 3 lines for proxy auth? with IE11 I see those 
>>> three lines (2x TCP_DENIED/407 and 1x TCP_MISS/200), no popup at all.
> 
> Not specifically. There should be 1+ for NTLM. Success with NTLM shows
> 2+. Failure shows 1 or 3 or infinite loop (hello Safari and Firefox 30-ish).
> 
> 
>>> 
>>> winbind/samba itself seems to work, as I can do an user auth against 
>>> apache with winbind/samba - even over some squid proxies with 
>>> connection-auth allowed. but not for proxy-auth.
>>> is there any option in squid.conf which prevents Edge to do a 
>>> successful auth?
> 
> If other software succeeds then the only thing that might be related is the keep-alive option mentioned above. Otherwise the problem is in Edge itself.
> 
> Amos
> 
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users

From rafael.akchurin at diladele.com  Thu Mar  9 18:17:38 2017
From: rafael.akchurin at diladele.com (Rafael Akchurin)
Date: Thu, 9 Mar 2017 18:17:38 +0000
Subject: [squid-users] microsoft edge and proxy auth not working
In-Reply-To: <2197768425D7F5479A0FFB3FEC212F7FF5D8D297@aesmail.surcouf.local>
References: <1FCF9DA5B29068478ECF15896F19F08401B8AC0D2C@Y011008.bk.fin.local>
 <1FCF9DA5B29068478ECF15896F19F08401B8AC0D3D@Y011008.bk.fin.local>
 <8b10adcd-f55a-65ba-cd37-28851575f6e0@treenet.co.nz>
 <DB6PR0401MB2680D3C0EF7481BC9565DDEA8F210@DB6PR0401MB2680.eurprd04.prod.outlook.com>
 <2197768425D7F5479A0FFB3FEC212F7FF5D8D1A3@aesmail.surcouf.local>
 <E3D787EC-2561-44C7-98C8-000FE40EDE75@diladele.com>
 <2197768425D7F5479A0FFB3FEC212F7FF5D8D297@aesmail.surcouf.local>
Message-ID: <D7175D18-CAF6-4E2E-B807-A41DC9B32CA3@diladele.com>

The thing is, when you got some machines in your network which are not joined to the domain (think apple, linux) you still need NTLM support on proxy :(

And having full blown Samba just because of those few is too much of admin's hassle - so we had to write NTLM relay that would rebind to domain controller with LDAP protocol passing NTLM token back and forth.

Joining Squid proxy to the domain (which is required to authenticate using Samba/NTLM) also prevents from successful reverts from vm snapshots after 30 days and requires rejoin - thus preventing us from creating easily provisioned/thrown away scalable web filter / proxy instances (think docker).

Best regards,
Rafael Akchurin

> Op 9 mrt. 2017 om 19:09 heeft Mike Surcouf <mikes at surcouf.co.uk> het volgende geschreven:
> 
> Ah OK sorry
> I am curious why you have a reason to use NTLM over Kerberos? :-)
> 
> -----Original Message-----
> From: Rafael Akchurin [mailto:rafael.akchurin at diladele.com] 
> Sent: 09 March 2017 18:01
> To: Mike Surcouf
> Cc: Amos Jeffries; squid-users at lists.squid-cache.org
> Subject: Re: [squid-users] microsoft edge and proxy auth not working
> 
> Hello Mike,
> 
> I specifically was debugging our NTLM implementation with Edge :)
> 
> Kerberos works just fine, you are correct.
> 
> Best regards,
> Rafael Akchurin
> 
>> Op 9 mrt. 2017 om 18:57 heeft Mike Surcouf <mikes at surcouf.co.uk> het volgende geschreven:
>> 
>> Hi Rafael
>> 
>> Is there any reason you can't use Kerberos.
>> Note you will need to create a keytab but the setup is not that hard and in the docs.
>> I use it very successfully on window AD network.
>> 
>> auth_param negotiate program /usr/lib64/squid/negotiate_kerberos_auth
>> auth_param negotiate children 20
>> auth_param negotiate keep_alive on
>> 
>> Thanks
>> 
>> Mike
>> 
>> -----Original Message-----
>> From: squid-users [mailto:squid-users-bounces at lists.squid-cache.org] 
>> On Behalf Of Rafael Akchurin
>> Sent: 09 March 2017 17:01
>> To: Amos Jeffries; squid-users at lists.squid-cache.org
>> Subject: Re: [squid-users] microsoft edge and proxy auth not working
>> 
>> Hello Amos, Markus, all,
>> 
>> Just as a side note - I also suffered  from this error sometime before with Edge and our custom NTLM relay to domain controllers (run as auth helper by Squid). The strange thing it went away after installing some (unknown) Windows update.
>> 
>> I do have the "auth_param ntlm keep_alive off" in the config though.
>> 
>> It all makes me quite suspicious the error was/is in Edge or in my curly hands.
>> 
>> Best regards,
>> Rafael Akchurin
>> Diladele B.V.
>> 
>> -----Original Message-----
>> From: squid-users [mailto:squid-users-bounces at lists.squid-cache.org] 
>> On Behalf Of Amos Jeffries
>> Sent: Thursday, March 9, 2017 5:12 PM
>> To: squid-users at lists.squid-cache.org
>> Subject: Re: [squid-users] microsoft edge and proxy auth not working
>> 
>> On 8/03/2017 11:28 p.m., Rietzler, Markus (RZF, Aufg 324 /
>> <RIETZLER_SOFTWARE>) wrote:
>>> i should add that we are using squid 3.5.24.
>>> 
>> 
>> Try with "auth_param ntlm keep_alive off". Recently the browsers have been needing that.
>> 
>> Though frankly I am surprised if Edge supports NTLM at all. It was deprecated in April 2006 and MS announced removal was being actively pushed in all thier software since Win7.
>> 
>>> 
>>>> -----Urspr?ngliche Nachricht-----
>>>> Von: Rietzler, Markus
>>>> 
>>>> we have some windows 10 clients using microsoft edge browser.
>>>> access to internet is only allowed for authenticated users. we are 
>>>> using samba/winbind auth
>>>> 
>>>> auth_param ntlm program /usr/bin/ntlm_auth
>>>> --helper-protocol=squid-2.5- ntlmssp auth_param ntlm children 64
>>>> startup=24 idle=12 auth_param ntlm keep_alive on acl auth_user 
>>>> proxy_auth REQUIRED
>>>> 
>>>> on windows 10 clients with IE11 it is working (with ntlm automatic
>>>> auth) on the same machine, with Microsoft edge I get TCP_Denied/407 message.
>>>> seems I only get one single TCP_DENIED/407 line in accesslog and an 
>>>> auth dialog pops up. I have disabled basic auth via ntlm.
>>>> shouldn't there be 3 lines for proxy auth? with IE11 I see those 
>>>> three lines (2x TCP_DENIED/407 and 1x TCP_MISS/200), no popup at all.
>> 
>> Not specifically. There should be 1+ for NTLM. Success with NTLM shows
>> 2+. Failure shows 1 or 3 or infinite loop (hello Safari and Firefox 30-ish).
>> 
>> 
>>>> 
>>>> winbind/samba itself seems to work, as I can do an user auth against 
>>>> apache with winbind/samba - even over some squid proxies with 
>>>> connection-auth allowed. but not for proxy-auth.
>>>> is there any option in squid.conf which prevents Edge to do a 
>>>> successful auth?
>> 
>> If other software succeeds then the only thing that might be related is the keep-alive option mentioned above. Otherwise the problem is in Edge itself.
>> 
>> Amos
>> 
>> _______________________________________________
>> squid-users mailing list
>> squid-users at lists.squid-cache.org
>> http://lists.squid-cache.org/listinfo/squid-users
>> _______________________________________________
>> squid-users mailing list
>> squid-users at lists.squid-cache.org
>> http://lists.squid-cache.org/listinfo/squid-users

From bpk678 at gmail.com  Thu Mar  9 19:22:04 2017
From: bpk678 at gmail.com (Brendan Kearney)
Date: Thu, 9 Mar 2017 14:22:04 -0500
Subject: [squid-users] microsoft edge and proxy auth not working
In-Reply-To: <19BB337E-27F3-4A6D-977B-F63AD1A23093@diladele.com>
References: <1FCF9DA5B29068478ECF15896F19F08401B8AC0D2C@Y011008.bk.fin.local>
 <1FCF9DA5B29068478ECF15896F19F08401B8AC0D3D@Y011008.bk.fin.local>
 <8b10adcd-f55a-65ba-cd37-28851575f6e0@treenet.co.nz>
 <DB6PR0401MB2680D3C0EF7481BC9565DDEA8F210@DB6PR0401MB2680.eurprd04.prod.outlook.com>
 <2197768425D7F5479A0FFB3FEC212F7FF5D8D1A3@aesmail.surcouf.local>
 <E3D787EC-2561-44C7-98C8-000FE40EDE75@diladele.com>
 <2197768425D7F5479A0FFB3FEC212F7FF5D8D297@aesmail.surcouf.local>
 <D7175D18-CAF6-4E2E-B807-A41DC9B32CA3@diladele.com>
 <4500504a-e806-1d00-bc6f-32e8184f9695@gmail.com>
 <19BB337E-27F3-4A6D-977B-F63AD1A23093@diladele.com>
Message-ID: <2ee41650-b5bf-2bd4-33c7-43461da35d8f@gmail.com>

adding this back to the mailing list, for the benefit of those who 
search for it.

i do not have simple and easy to use instructions for mac os x and linux 
participation in AD.  it is not a simple task.  on linux, you will need 
to look into SSSD (Simple Security Services Daemon) and understand that 
process.

i have a mac for work, and it is a domain member object, so i know it 
can be done.  i dont know how it is done, and would think there are 
internet articles that you can search for on the subject.

On 03/09/2017 02:13 PM, Rafael Akchurin wrote:
> Hello Brendan,
>
> Yes by default we have NTLM disabled :)
>
> Unfortunately we must keep the proxy solution in parity with DC capabilities in AD which luckily still support NTLM authentication through LDAP.
>
> This allows us to relay the tokens without Samba as I described in previous mail.
>
> BTW if you could share the ready to use (simple) instructions to have Kerberous auth supported ftom Mac/iPhone/iPad and Linux (Ubuntu/CentOS) it would be beneficial to all.
>
> Best regards,
> Rafael Akchurin
>
>> Op 9 mrt. 2017 om 19:47 heeft Brendan Kearney <bpk678 at gmail.com> het volgende geschreven:
>>
>>> On 03/09/2017 01:17 PM, Rafael Akchurin wrote:
>>> The thing is, when you got some machines in your network which are not joined to the domain (think apple, linux) you still need NTLM support on proxy :(
>>>
>>> And having full blown Samba just because of those few is too much of admin's hassle - so we had to write NTLM relay that would rebind to domain controller with LDAP protocol passing NTLM token back and forth.
>>>
>>> Joining Squid proxy to the domain (which is required to authenticate using Samba/NTLM) also prevents from successful reverts from vm snapshots after 30 days and requires rejoin - thus preventing us from creating easily provisioned/thrown away scalable web filter / proxy instances (think docker).
>>>
>>> Best regards,
>>> Rafael Akchurin
>>>
>>>> Op 9 mrt. 2017 om 19:09 heeft Mike Surcouf <mikes at surcouf.co.uk> het volgende geschreven:
>>>>
>>>> Ah OK sorry
>>>> I am curious why you have a reason to use NTLM over Kerberos? :-)
>>>>
>>>> -----Original Message-----
>>>> From: Rafael Akchurin [mailto:rafael.akchurin at diladele.com]
>>>> Sent: 09 March 2017 18:01
>>>> To: Mike Surcouf
>>>> Cc: Amos Jeffries; squid-users at lists.squid-cache.org
>>>> Subject: Re: [squid-users] microsoft edge and proxy auth not working
>>>>
>>>> Hello Mike,
>>>>
>>>> I specifically was debugging our NTLM implementation with Edge :)
>>>>
>>>> Kerberos works just fine, you are correct.
>>>>
>>>> Best regards,
>>>> Rafael Akchurin
>>>>
>>>>> Op 9 mrt. 2017 om 18:57 heeft Mike Surcouf <mikes at surcouf.co.uk> het volgende geschreven:
>>>>>
>>>>> Hi Rafael
>>>>>
>>>>> Is there any reason you can't use Kerberos.
>>>>> Note you will need to create a keytab but the setup is not that hard and in the docs.
>>>>> I use it very successfully on window AD network.
>>>>>
>>>>> auth_param negotiate program /usr/lib64/squid/negotiate_kerberos_auth
>>>>> auth_param negotiate children 20
>>>>> auth_param negotiate keep_alive on
>>>>>
>>>>> Thanks
>>>>>
>>>>> Mike
>>>>>
>>>>> -----Original Message-----
>>>>> From: squid-users [mailto:squid-users-bounces at lists.squid-cache.org]
>>>>> On Behalf Of Rafael Akchurin
>>>>> Sent: 09 March 2017 17:01
>>>>> To: Amos Jeffries; squid-users at lists.squid-cache.org
>>>>> Subject: Re: [squid-users] microsoft edge and proxy auth not working
>>>>>
>>>>> Hello Amos, Markus, all,
>>>>>
>>>>> Just as a side note - I also suffered  from this error sometime before with Edge and our custom NTLM relay to domain controllers (run as auth helper by Squid). The strange thing it went away after installing some (unknown) Windows update.
>>>>>
>>>>> I do have the "auth_param ntlm keep_alive off" in the config though.
>>>>>
>>>>> It all makes me quite suspicious the error was/is in Edge or in my curly hands.
>>>>>
>>>>> Best regards,
>>>>> Rafael Akchurin
>>>>> Diladele B.V.
>>>>>
>>>>> -----Original Message-----
>>>>> From: squid-users [mailto:squid-users-bounces at lists.squid-cache.org]
>>>>> On Behalf Of Amos Jeffries
>>>>> Sent: Thursday, March 9, 2017 5:12 PM
>>>>> To: squid-users at lists.squid-cache.org
>>>>> Subject: Re: [squid-users] microsoft edge and proxy auth not working
>>>>>
>>>>> On 8/03/2017 11:28 p.m., Rietzler, Markus (RZF, Aufg 324 /
>>>>> <RIETZLER_SOFTWARE>) wrote:
>>>>>> i should add that we are using squid 3.5.24.
>>>>>>
>>>>> Try with "auth_param ntlm keep_alive off". Recently the browsers have been needing that.
>>>>>
>>>>> Though frankly I am surprised if Edge supports NTLM at all. It was deprecated in April 2006 and MS announced removal was being actively pushed in all thier software since Win7.
>>>>>
>>>>>>> -----Urspr?ngliche Nachricht-----
>>>>>>> Von: Rietzler, Markus
>>>>>>>
>>>>>>> we have some windows 10 clients using microsoft edge browser.
>>>>>>> access to internet is only allowed for authenticated users. we are
>>>>>>> using samba/winbind auth
>>>>>>>
>>>>>>> auth_param ntlm program /usr/bin/ntlm_auth
>>>>>>> --helper-protocol=squid-2.5- ntlmssp auth_param ntlm children 64
>>>>>>> startup=24 idle=12 auth_param ntlm keep_alive on acl auth_user
>>>>>>> proxy_auth REQUIRED
>>>>>>>
>>>>>>> on windows 10 clients with IE11 it is working (with ntlm automatic
>>>>>>> auth) on the same machine, with Microsoft edge I get TCP_Denied/407 message.
>>>>>>> seems I only get one single TCP_DENIED/407 line in accesslog and an
>>>>>>> auth dialog pops up. I have disabled basic auth via ntlm.
>>>>>>> shouldn't there be 3 lines for proxy auth? with IE11 I see those
>>>>>>> three lines (2x TCP_DENIED/407 and 1x TCP_MISS/200), no popup at all.
>>>>> Not specifically. There should be 1+ for NTLM. Success with NTLM shows
>>>>> 2+. Failure shows 1 or 3 or infinite loop (hello Safari and Firefox 30-ish).
>>>>>
>>>>>
>>>>>>> winbind/samba itself seems to work, as I can do an user auth against
>>>>>>> apache with winbind/samba - even over some squid proxies with
>>>>>>> connection-auth allowed. but not for proxy-auth.
>>>>>>> is there any option in squid.conf which prevents Edge to do a
>>>>>>> successful auth?
>>>>> If other software succeeds then the only thing that might be related is the keep-alive option mentioned above. Otherwise the problem is in Edge itself.
>>>>>
>>>>> Amos
>>>>>
>>>>> _______________________________________________
>>>>> squid-users mailing list
>>>>> squid-users at lists.squid-cache.org
>>>>> http://lists.squid-cache.org/listinfo/squid-users
>>>>> _______________________________________________
>>>>> squid-users mailing list
>>>>> squid-users at lists.squid-cache.org
>>>>> http://lists.squid-cache.org/listinfo/squid-users
>>> _______________________________________________
>>> squid-users mailing list
>>> squid-users at lists.squid-cache.org
>>> http://lists.squid-cache.org/listinfo/squid-users
>> mac os x and linux can be joined to the domain and use kerberos auth.  it sounds like you have a "dont want to" situation, instead of a "cannot" situation there.
>>
>> when both kerberos and ntlm are advertised as supported by the proxy, the client will negotiate which auth method is used.  when both the client and the proxy support kerberos, that will be used.  if the client is then not able to pull a ticket from the directory to satisfy the kerberos auth to the proxy, the auth fails.  the fall back to ntlm as the auth method will not occur in this situation.
>>
>> ntlm will only be used if one or both of the parties does not support kerberos, and therefore the negotiated auth method chosen is not kerberos.
>>
>> in Edge, disable the IWA (Integrated Windows Authentication) under Advanced settings, and close/relaunch any browser windows. this should get you using only ntlm.  effectively, you are forcing the client to not support kerberos and preventing that auth method from being negotiated for use.
>>
>> hth,
>>
>> brendan
>>


From squid3 at treenet.co.nz  Thu Mar  9 19:33:51 2017
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Fri, 10 Mar 2017 08:33:51 +1300
Subject: [squid-users] pinger without peers
In-Reply-To: <20170309173629.GA9814@fantomas.sk>
References: <20170309173629.GA9814@fantomas.sk>
Message-ID: <ea420b98-9886-173d-baf8-692f2969e467@treenet.co.nz>

On 10/03/2017 6:36 a.m., Matus UHLAR - fantomas wrote:
> Hello,
> 
> does it have sense to run pinger without having cache peers configured?
> 
> if I get the "Network DB Statistics:" output properly, it seems that 33% of
> hosts is unreachable.
> 

The code using it is called peer-selection, but that includes choosing
between origin servers (DIRECT) as well as cache_peer lines. So it can
affect traffic routing even if you dont have cache_peer. Though less
that RTT based cache_peer LB algorithms.

The sad results are due to admin ICMP echo blocking. Hopefully that is
all they are blocking though and not other mandatory ICMP codes like
path-MTU discovery.

Amos



From eliezer at ngtech.co.il  Fri Mar 10 04:13:54 2017
From: eliezer at ngtech.co.il (Eliezer  Croitoru)
Date: Fri, 10 Mar 2017 06:13:54 +0200
Subject: [squid-users] Can't start Squid with workers via systemd on
	CentOS 7
In-Reply-To: <1965c37d-0297-55a4-cbcc-09f9916237c5@ustc.edu>
References: <1965c37d-0297-55a4-cbcc-09f9916237c5@ustc.edu>
Message-ID: <01a201d29954$bb474b40$31d5e1c0$@ngtech.co.il>

I can test it locally but it will take me couple days to get there.

----
Eliezer Croitoru
Linux System Administrator
Mobile: +972-5-28704261
Email: eliezer at ngtech.co.il


-----Original Message-----
From: squid-users [mailto:squid-users-bounces at lists.squid-cache.org] On Behalf Of ??
Sent: Thursday, March 9, 2017 4:00 PM
To: squid-users at lists.squid-cache.org
Subject: [squid-users] Can't start Squid with workers via systemd on CentOS 7

I installed Squid 4.0.18 on CentOS 7 X86_84 according to http://wiki.squid-cache.org/KnowledgeBase/CentOS#Squid_Beta_release

I add a line "workers 2" to `/etc/squid/squid.conf` (just a copy of
`squid.conf.default`)
and start squid via `systemctl start squid`, It seems squid is running, except that it is not listening to the port 3128 (confirmed by netstat).

However, if I start squid in command line (`squid -sYC`), squid does listen to port 3128.

There are some differences in `/var/log/squid/cache.log` for these two
cases:

* start via `systemctl start squid`:

   2017/03/09 21:12:54 kid3| commBind Cannot bind socket FD 12 to [::]: 
(98) Address already in use
   2017/03/09 21:12:54 kid2| commBind Cannot bind socket FD 21 to [::]: 
(98) Address already in use
   2017/03/09 21:12:54 kid1| commBind Cannot bind socket FD 21 to [::]: 
(98) Address already in use

* start via `squid -sYC`:

   2017/03/09 21:19:28 kid1| Accepting HTTP Socket connections at
local=[::]:3128 remote=[::] FD 15 flags=1
   2017/03/09 21:19:28 kid2| Accepting HTTP Socket connections at
local=[::]:3128 remote=[::] FD 15 flags=1

The full logs are attached. Please help.



From uhlar at fantomas.sk  Fri Mar 10 09:28:41 2017
From: uhlar at fantomas.sk (Matus UHLAR - fantomas)
Date: Fri, 10 Mar 2017 10:28:41 +0100
Subject: [squid-users] pinger without peers
In-Reply-To: <ea420b98-9886-173d-baf8-692f2969e467@treenet.co.nz>
References: <20170309173629.GA9814@fantomas.sk>
 <ea420b98-9886-173d-baf8-692f2969e467@treenet.co.nz>
Message-ID: <20170310092841.GA31011@fantomas.sk>

>On 10/03/2017 6:36 a.m., Matus UHLAR - fantomas wrote:
>> does it have sense to run pinger without having cache peers configured?
>>
>> if I get the "Network DB Statistics:" output properly, it seems that 33% of
>> hosts is unreachable.

On 10.03.17 08:33, Amos Jeffries wrote:
>The code using it is called peer-selection, but that includes choosing
>between origin servers (DIRECT) as well as cache_peer lines. So it can
>affect traffic routing even if you dont have cache_peer. Though less
>that RTT based cache_peer LB algorithms.

does that mean pinger output is used for servers with multiple addresses?
does it only ping servers with multiple addresses?

>The sad results are due to admin ICMP echo blocking. Hopefully that is
>all they are blocking though and not other mandatory ICMP codes like
>path-MTU discovery.

of course. But since I'm trying to solve different problem, I'm thinking of
trurning off everything that can cause issues.

-- 
Matus UHLAR - fantomas, uhlar at fantomas.sk ; http://www.fantomas.sk/
Warning: I wish NOT to receive e-mail advertising to this address.
Varovanie: na tuto adresu chcem NEDOSTAVAT akukolvek reklamnu postu.
"To Boot or not to Boot, that's the question." [WD1270 Caviar]


From uhlar at fantomas.sk  Fri Mar 10 09:38:33 2017
From: uhlar at fantomas.sk (Matus UHLAR - fantomas)
Date: Fri, 10 Mar 2017 10:38:33 +0100
Subject: [squid-users] squid workers question
In-Reply-To: <7b386e00-97ed-054c-902f-0df367e8ec49@measurement-factory.com>
References: <20170309142134.GA30976@fantomas.sk>
 <f547a991-3178-7b95-113d-fb4b533c3056@measurement-factory.com>
 <20170309161940.GB7401@fantomas.sk>
 <6ca274d9-948c-01c5-c3dd-13044d3cfe7b@measurement-factory.com>
 <20170309165429.GA8584@fantomas.sk>
 <8ef542a5-dba3-236f-224b-944990d4cd68@measurement-factory.com>
 <20170309172435.GC8584@fantomas.sk>
 <7b386e00-97ed-054c-902f-0df367e8ec49@measurement-factory.com>
Message-ID: <20170310093832.GB31011@fantomas.sk>

>On 03/09/2017 10:24 AM, Matus UHLAR - fantomas wrote:
>> is running aufs with rock store and safe, when not running with "-N"?

On 09.03.17 11:02, Alex Rousskov wrote:
>Running AUFS in SMP mode is unsafe by default but some admins use
>configuration hacks to make it work for them. Primary Store developers
>do not test such configurations (or AUFS in general).

according to the "workers" docs, "workers 0" is the same as "squid -N" and
"workers 1" is non-SMP mode

so, with "workers 1", both aufs and rock should work properly, with rock
using separate process, correct?

>Running a combination of AUFS and Rock stores in SMP mode is crazy.

I get that as in SMP mode, ufs should be used?

-- 
Matus UHLAR - fantomas, uhlar at fantomas.sk ; http://www.fantomas.sk/
Warning: I wish NOT to receive e-mail advertising to this address.
Varovanie: na tuto adresu chcem NEDOSTAVAT akukolvek reklamnu postu.
10 GOTO 10 : REM (C) Bill Gates 1998, All Rights Reserved!


From markus.rietzler at fv.nrw.de  Fri Mar 10 09:50:02 2017
From: markus.rietzler at fv.nrw.de (Rietzler, Markus (RZF, Aufg 324 / <RIETZLER_SOFTWARE>))
Date: Fri, 10 Mar 2017 09:50:02 +0000
Subject: [squid-users] microsoft edge and proxy auth not working
In-Reply-To: <8b10adcd-f55a-65ba-cd37-28851575f6e0@treenet.co.nz>
References: <1FCF9DA5B29068478ECF15896F19F08401B8AC0D2C@Y011008.bk.fin.local>
 <1FCF9DA5B29068478ECF15896F19F08401B8AC0D3D@Y011008.bk.fin.local>
 <8b10adcd-f55a-65ba-cd37-28851575f6e0@treenet.co.nz>
Message-ID: <1FCF9DA5B29068478ECF15896F19F08401B8AC14FF@Y011008.bk.fin.local>

we have tried with "auth_param ntlm keep_alive off", but both with on/off it does not make a difference.
seems realy to be connected to patch level and installed patches on windows 10.


> -----Urspr?ngliche Nachricht-----
> Von: squid-users [mailto:squid-users-bounces at lists.squid-cache.org] Im
> Auftrag von Amos Jeffries
> Gesendet: Donnerstag, 9. M?rz 2017 17:12
> An: squid-users at lists.squid-cache.org
> Betreff: Re: [squid-users] microsoft edge and proxy auth not working
> 
> On 8/03/2017 11:28 p.m., Rietzler, Markus (RZF, Aufg 324 /
> <RIETZLER_SOFTWARE>) wrote:
> > i should add that we are using squid 3.5.24.
> >
> 
> Try with "auth_param ntlm keep_alive off". Recently the browsers have
> been needing that.
> 
> Though frankly I am surprised if Edge supports NTLM at all. It was
> deprecated in April 2006 and MS announced removal was being actively
> pushed in all thier software since Win7.
> 
> >
> >> -----Urspr?ngliche Nachricht-----
> >> Von: Rietzler, Markus
> >>
> >> we have some windows 10 clients using microsoft edge browser.
> >> access to internet is only allowed for authenticated users. we are
> using
> >> samba/winbind auth
> >>
> >> auth_param ntlm program /usr/bin/ntlm_auth --helper-protocol=squid-
> 2.5-
> >> ntlmssp
> >> auth_param ntlm children 64 startup=24 idle=12
> >> auth_param ntlm keep_alive on
> >> acl auth_user proxy_auth REQUIRED
> >>
> >> on windows 10 clients with IE11 it is working (with ntlm automatic
> auth)
> >> on the same machine, with Microsoft edge I get TCP_Denied/407
> message.
> >> seems I only get one single TCP_DENIED/407 line in accesslog and an
> auth
> >> dialog pops up. I have disabled basic auth via ntlm.
> >> shouldn't there be 3 lines for proxy auth? with IE11 I see those
> three
> >> lines (2x TCP_DENIED/407 and 1x TCP_MISS/200), no popup at all.
> 
> Not specifically. There should be 1+ for NTLM. Success with NTLM shows
> 2+. Failure shows 1 or 3 or infinite loop (hello Safari and Firefox 30-
> ish).
> 
> 
> >>
> >> winbind/samba itself seems to work, as I can do an user auth against
> >> apache with winbind/samba - even over some squid proxies with
> >> connection-auth allowed. but not for proxy-auth.
> >> is there any option in squid.conf which prevents Edge to do a
> successful
> >> auth?
> 
> If other software succeeds then the only thing that might be related is
> the keep-alive option mentioned above. Otherwise the problem is in Edge
> itself.
> 
> Amos
> 
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users

From markus.rietzler at fv.nrw.de  Fri Mar 10 09:52:32 2017
From: markus.rietzler at fv.nrw.de (Rietzler, Markus (RZF, Aufg 324 / <RIETZLER_SOFTWARE>))
Date: Fri, 10 Mar 2017 09:52:32 +0000
Subject: [squid-users] microsoft edge and proxy auth not working
In-Reply-To: <2197768425D7F5479A0FFB3FEC212F7FF5D8D1A3@aesmail.surcouf.local>
References: <1FCF9DA5B29068478ECF15896F19F08401B8AC0D2C@Y011008.bk.fin.local>
 <1FCF9DA5B29068478ECF15896F19F08401B8AC0D3D@Y011008.bk.fin.local>
 <8b10adcd-f55a-65ba-cd37-28851575f6e0@treenet.co.nz>
 <DB6PR0401MB2680D3C0EF7481BC9565DDEA8F210@DB6PR0401MB2680.eurprd04.prod.outlook.com>
 <2197768425D7F5479A0FFB3FEC212F7FF5D8D1A3@aesmail.surcouf.local>
Message-ID: <1FCF9DA5B29068478ECF15896F19F08401B8AC150F@Y011008.bk.fin.local>

Kerberos is on the wishlist for very long. 
one reason was: the setup is a bit complicated and we do have 150 proxies in our subsidiaries. so we need 150 different Kerberos setups with 150 trusts and tickets and certificates etc. so we work on this to have it someday replaced...

thanxs

> -----Urspr?ngliche Nachricht-----
> Von: squid-users [mailto:squid-users-bounces at lists.squid-cache.org] Im
> Auftrag von Mike Surcouf
> Gesendet: Donnerstag, 9. M?rz 2017 18:58
> An: 'Rafael Akchurin'; Amos Jeffries; squid-users at lists.squid-cache.org
> Betreff: Re: [squid-users] microsoft edge and proxy auth not working
> 
> Hi Rafael
> 
> Is there any reason you can't use Kerberos.
> Note you will need to create a keytab but the setup is not that hard and
> in the docs.
> I use it very successfully on window AD network.
> 
> auth_param negotiate program /usr/lib64/squid/negotiate_kerberos_auth
> auth_param negotiate children 20
> auth_param negotiate keep_alive on
> 
> Thanks
> 
> Mike
> 
> -----Original Message-----
> From: squid-users [mailto:squid-users-bounces at lists.squid-cache.org] On
> Behalf Of Rafael Akchurin
> Sent: 09 March 2017 17:01
> To: Amos Jeffries; squid-users at lists.squid-cache.org
> Subject: Re: [squid-users] microsoft edge and proxy auth not working
> 
> Hello Amos, Markus, all,
> 
> Just as a side note - I also suffered  from this error sometime before
> with Edge and our custom NTLM relay to domain controllers (run as auth
> helper by Squid). The strange thing it went away after installing some
> (unknown) Windows update.
> 
> I do have the "auth_param ntlm keep_alive off" in the config though.
> 
> It all makes me quite suspicious the error was/is in Edge or in my curly
> hands.
> 
> Best regards,
> Rafael Akchurin
> Diladele B.V.
> 
> -----Original Message-----
> From: squid-users [mailto:squid-users-bounces at lists.squid-cache.org] On
> Behalf Of Amos Jeffries
> Sent: Thursday, March 9, 2017 5:12 PM
> To: squid-users at lists.squid-cache.org
> Subject: Re: [squid-users] microsoft edge and proxy auth not working
> 
> On 8/03/2017 11:28 p.m., Rietzler, Markus (RZF, Aufg 324 /
> <RIETZLER_SOFTWARE>) wrote:
> > i should add that we are using squid 3.5.24.
> >
> 
> Try with "auth_param ntlm keep_alive off". Recently the browsers have
> been needing that.
> 
> Though frankly I am surprised if Edge supports NTLM at all. It was
> deprecated in April 2006 and MS announced removal was being actively
> pushed in all thier software since Win7.
> 
> >
> >> -----Urspr?ngliche Nachricht-----
> >> Von: Rietzler, Markus
> >>
> >> we have some windows 10 clients using microsoft edge browser.
> >> access to internet is only allowed for authenticated users. we are
> >> using samba/winbind auth
> >>
> >> auth_param ntlm program /usr/bin/ntlm_auth
> >> --helper-protocol=squid-2.5- ntlmssp auth_param ntlm children 64
> >> startup=24 idle=12 auth_param ntlm keep_alive on acl auth_user
> >> proxy_auth REQUIRED
> >>
> >> on windows 10 clients with IE11 it is working (with ntlm automatic
> >> auth) on the same machine, with Microsoft edge I get TCP_Denied/407
> message.
> >> seems I only get one single TCP_DENIED/407 line in accesslog and an
> >> auth dialog pops up. I have disabled basic auth via ntlm.
> >> shouldn't there be 3 lines for proxy auth? with IE11 I see those
> >> three lines (2x TCP_DENIED/407 and 1x TCP_MISS/200), no popup at all.
> 
> Not specifically. There should be 1+ for NTLM. Success with NTLM shows
> 2+. Failure shows 1 or 3 or infinite loop (hello Safari and Firefox 30-
> ish).
> 
> 
> >>
> >> winbind/samba itself seems to work, as I can do an user auth against
> >> apache with winbind/samba - even over some squid proxies with
> >> connection-auth allowed. but not for proxy-auth.
> >> is there any option in squid.conf which prevents Edge to do a
> >> successful auth?
> 
> If other software succeeds then the only thing that might be related is
> the keep-alive option mentioned above. Otherwise the problem is in Edge
> itself.
> 
> Amos
> 
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users

From mikes at surcouf.co.uk  Fri Mar 10 09:56:18 2017
From: mikes at surcouf.co.uk (Mike Surcouf)
Date: Fri, 10 Mar 2017 09:56:18 +0000
Subject: [squid-users] microsoft edge and proxy auth not working
In-Reply-To: <1FCF9DA5B29068478ECF15896F19F08401B8AC150F@Y011008.bk.fin.local>
References: <1FCF9DA5B29068478ECF15896F19F08401B8AC0D2C@Y011008.bk.fin.local>
 <1FCF9DA5B29068478ECF15896F19F08401B8AC0D3D@Y011008.bk.fin.local>
 <8b10adcd-f55a-65ba-cd37-28851575f6e0@treenet.co.nz>
 <DB6PR0401MB2680D3C0EF7481BC9565DDEA8F210@DB6PR0401MB2680.eurprd04.prod.outlook.com>
 <2197768425D7F5479A0FFB3FEC212F7FF5D8D1A3@aesmail.surcouf.local>
 <1FCF9DA5B29068478ECF15896F19F08401B8AC150F@Y011008.bk.fin.local>
Message-ID: <2197768425D7F5479A0FFB3FEC212F7FF5D8E9BF@aesmail.surcouf.local>

Are the browsing machines domain joined?
If so and you are just talking about joining the squid proxies to the domains for auth delegation to the dcs this is greatly simplified with realmd now.
Could probably be scripted quite easily.

-----Original Message-----
From: squid-users [mailto:squid-users-bounces at lists.squid-cache.org] On Behalf Of Rietzler, Markus (RZF, Aufg 324 / <RIETZLER_SOFTWARE>)
Sent: 10 March 2017 09:53
To: squid-users at lists.squid-cache.org
Subject: Re: [squid-users] microsoft edge and proxy auth not working

Kerberos is on the wishlist for very long. 
one reason was: the setup is a bit complicated and we do have 150 proxies in our subsidiaries. so we need 150 different Kerberos setups with 150 trusts and tickets and certificates etc. so we work on this to have it someday replaced...

thanxs

> -----Urspr?ngliche Nachricht-----
> Von: squid-users [mailto:squid-users-bounces at lists.squid-cache.org] Im 
> Auftrag von Mike Surcouf
> Gesendet: Donnerstag, 9. M?rz 2017 18:58
> An: 'Rafael Akchurin'; Amos Jeffries; 
> squid-users at lists.squid-cache.org
> Betreff: Re: [squid-users] microsoft edge and proxy auth not working
> 
> Hi Rafael
> 
> Is there any reason you can't use Kerberos.
> Note you will need to create a keytab but the setup is not that hard 
> and in the docs.
> I use it very successfully on window AD network.
> 
> auth_param negotiate program /usr/lib64/squid/negotiate_kerberos_auth
> auth_param negotiate children 20
> auth_param negotiate keep_alive on
> 
> Thanks
> 
> Mike
> 
> -----Original Message-----
> From: squid-users [mailto:squid-users-bounces at lists.squid-cache.org] 
> On Behalf Of Rafael Akchurin
> Sent: 09 March 2017 17:01
> To: Amos Jeffries; squid-users at lists.squid-cache.org
> Subject: Re: [squid-users] microsoft edge and proxy auth not working
> 
> Hello Amos, Markus, all,
> 
> Just as a side note - I also suffered  from this error sometime before 
> with Edge and our custom NTLM relay to domain controllers (run as auth 
> helper by Squid). The strange thing it went away after installing some
> (unknown) Windows update.
> 
> I do have the "auth_param ntlm keep_alive off" in the config though.
> 
> It all makes me quite suspicious the error was/is in Edge or in my 
> curly hands.
> 
> Best regards,
> Rafael Akchurin
> Diladele B.V.
> 
> -----Original Message-----
> From: squid-users [mailto:squid-users-bounces at lists.squid-cache.org] 
> On Behalf Of Amos Jeffries
> Sent: Thursday, March 9, 2017 5:12 PM
> To: squid-users at lists.squid-cache.org
> Subject: Re: [squid-users] microsoft edge and proxy auth not working
> 
> On 8/03/2017 11:28 p.m., Rietzler, Markus (RZF, Aufg 324 /
> <RIETZLER_SOFTWARE>) wrote:
> > i should add that we are using squid 3.5.24.
> >
> 
> Try with "auth_param ntlm keep_alive off". Recently the browsers have 
> been needing that.
> 
> Though frankly I am surprised if Edge supports NTLM at all. It was 
> deprecated in April 2006 and MS announced removal was being actively 
> pushed in all thier software since Win7.
> 
> >
> >> -----Urspr?ngliche Nachricht-----
> >> Von: Rietzler, Markus
> >>
> >> we have some windows 10 clients using microsoft edge browser.
> >> access to internet is only allowed for authenticated users. we are 
> >> using samba/winbind auth
> >>
> >> auth_param ntlm program /usr/bin/ntlm_auth
> >> --helper-protocol=squid-2.5- ntlmssp auth_param ntlm children 64
> >> startup=24 idle=12 auth_param ntlm keep_alive on acl auth_user 
> >> proxy_auth REQUIRED
> >>
> >> on windows 10 clients with IE11 it is working (with ntlm automatic
> >> auth) on the same machine, with Microsoft edge I get TCP_Denied/407
> message.
> >> seems I only get one single TCP_DENIED/407 line in accesslog and an 
> >> auth dialog pops up. I have disabled basic auth via ntlm.
> >> shouldn't there be 3 lines for proxy auth? with IE11 I see those 
> >> three lines (2x TCP_DENIED/407 and 1x TCP_MISS/200), no popup at all.
> 
> Not specifically. There should be 1+ for NTLM. Success with NTLM shows
> 2+. Failure shows 1 or 3 or infinite loop (hello Safari and Firefox 
> 2+30-
> ish).
> 
> 
> >>
> >> winbind/samba itself seems to work, as I can do an user auth 
> >> against apache with winbind/samba - even over some squid proxies 
> >> with connection-auth allowed. but not for proxy-auth.
> >> is there any option in squid.conf which prevents Edge to do a 
> >> successful auth?
> 
> If other software succeeds then the only thing that might be related 
> is the keep-alive option mentioned above. Otherwise the problem is in 
> Edge itself.
> 
> Amos
> 
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
_______________________________________________
squid-users mailing list
squid-users at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-users

From rafael.akchurin at diladele.com  Fri Mar 10 10:35:15 2017
From: rafael.akchurin at diladele.com (Rafael Akchurin)
Date: Fri, 10 Mar 2017 10:35:15 +0000
Subject: [squid-users] microsoft edge and proxy auth not working
In-Reply-To: <2197768425D7F5479A0FFB3FEC212F7FF5D8E9BF@aesmail.surcouf.local>
References: <1FCF9DA5B29068478ECF15896F19F08401B8AC0D2C@Y011008.bk.fin.local>
 <1FCF9DA5B29068478ECF15896F19F08401B8AC0D3D@Y011008.bk.fin.local>
 <8b10adcd-f55a-65ba-cd37-28851575f6e0@treenet.co.nz>
 <DB6PR0401MB2680D3C0EF7481BC9565DDEA8F210@DB6PR0401MB2680.eurprd04.prod.outlook.com>
 <2197768425D7F5479A0FFB3FEC212F7FF5D8D1A3@aesmail.surcouf.local>
 <1FCF9DA5B29068478ECF15896F19F08401B8AC150F@Y011008.bk.fin.local>
 <2197768425D7F5479A0FFB3FEC212F7FF5D8E9BF@aesmail.surcouf.local>
Message-ID: <VI1PR0401MB268541F69D8382ABC23ABB3E8F200@VI1PR0401MB2685.eurprd04.prod.outlook.com>

Hello all,

There is another way (not better but another) that does not require you do join squid machines to domain: Map proxy SPN to a designated user. I describe this at https://docs.diladele.com/administrator_guide_4_9/active_directory/create_user/index.html

Pros - have one user that can be used by farm of squid proxies without the need to join boxes to domain.
Cons - that one user needs to be managed separately from all other users - i.e. you do not want to set the password expiration policy for it - otherwise your exported keytab will be invalid.

My 2 cents.

Rafael Akchurin
Diladele B.V.


-----Original Message-----
From: squid-users [mailto:squid-users-bounces at lists.squid-cache.org] On Behalf Of Mike Surcouf
Sent: Friday, March 10, 2017 10:56 AM
To: 'Rietzler, Markus (RZF, Aufg 324 / <RIETZLER_SOFTWARE>)' <markus.rietzler at fv.nrw.de>; squid-users at lists.squid-cache.org
Subject: Re: [squid-users] microsoft edge and proxy auth not working

Are the browsing machines domain joined?
If so and you are just talking about joining the squid proxies to the domains for auth delegation to the dcs this is greatly simplified with realmd now.
Could probably be scripted quite easily.

-----Original Message-----
From: squid-users [mailto:squid-users-bounces at lists.squid-cache.org] On Behalf Of Rietzler, Markus (RZF, Aufg 324 / <RIETZLER_SOFTWARE>)
Sent: 10 March 2017 09:53
To: squid-users at lists.squid-cache.org
Subject: Re: [squid-users] microsoft edge and proxy auth not working

Kerberos is on the wishlist for very long. 
one reason was: the setup is a bit complicated and we do have 150 proxies in our subsidiaries. so we need 150 different Kerberos setups with 150 trusts and tickets and certificates etc. so we work on this to have it someday replaced...

thanxs

> -----Urspr?ngliche Nachricht-----
> Von: squid-users [mailto:squid-users-bounces at lists.squid-cache.org] Im 
> Auftrag von Mike Surcouf
> Gesendet: Donnerstag, 9. M?rz 2017 18:58
> An: 'Rafael Akchurin'; Amos Jeffries; 
> squid-users at lists.squid-cache.org
> Betreff: Re: [squid-users] microsoft edge and proxy auth not working
> 
> Hi Rafael
> 
> Is there any reason you can't use Kerberos.
> Note you will need to create a keytab but the setup is not that hard 
> and in the docs.
> I use it very successfully on window AD network.
> 
> auth_param negotiate program /usr/lib64/squid/negotiate_kerberos_auth
> auth_param negotiate children 20
> auth_param negotiate keep_alive on
> 
> Thanks
> 
> Mike
> 
> -----Original Message-----
> From: squid-users [mailto:squid-users-bounces at lists.squid-cache.org]
> On Behalf Of Rafael Akchurin
> Sent: 09 March 2017 17:01
> To: Amos Jeffries; squid-users at lists.squid-cache.org
> Subject: Re: [squid-users] microsoft edge and proxy auth not working
> 
> Hello Amos, Markus, all,
> 
> Just as a side note - I also suffered  from this error sometime before 
> with Edge and our custom NTLM relay to domain controllers (run as auth 
> helper by Squid). The strange thing it went away after installing some
> (unknown) Windows update.
> 
> I do have the "auth_param ntlm keep_alive off" in the config though.
> 
> It all makes me quite suspicious the error was/is in Edge or in my 
> curly hands.
> 
> Best regards,
> Rafael Akchurin
> Diladele B.V.
> 
> -----Original Message-----
> From: squid-users [mailto:squid-users-bounces at lists.squid-cache.org]
> On Behalf Of Amos Jeffries
> Sent: Thursday, March 9, 2017 5:12 PM
> To: squid-users at lists.squid-cache.org
> Subject: Re: [squid-users] microsoft edge and proxy auth not working
> 
> On 8/03/2017 11:28 p.m., Rietzler, Markus (RZF, Aufg 324 /
> <RIETZLER_SOFTWARE>) wrote:
> > i should add that we are using squid 3.5.24.
> >
> 
> Try with "auth_param ntlm keep_alive off". Recently the browsers have 
> been needing that.
> 
> Though frankly I am surprised if Edge supports NTLM at all. It was 
> deprecated in April 2006 and MS announced removal was being actively 
> pushed in all thier software since Win7.
> 
> >
> >> -----Urspr?ngliche Nachricht-----
> >> Von: Rietzler, Markus
> >>
> >> we have some windows 10 clients using microsoft edge browser.
> >> access to internet is only allowed for authenticated users. we are 
> >> using samba/winbind auth
> >>
> >> auth_param ntlm program /usr/bin/ntlm_auth
> >> --helper-protocol=squid-2.5- ntlmssp auth_param ntlm children 64
> >> startup=24 idle=12 auth_param ntlm keep_alive on acl auth_user 
> >> proxy_auth REQUIRED
> >>
> >> on windows 10 clients with IE11 it is working (with ntlm automatic
> >> auth) on the same machine, with Microsoft edge I get TCP_Denied/407
> message.
> >> seems I only get one single TCP_DENIED/407 line in accesslog and an 
> >> auth dialog pops up. I have disabled basic auth via ntlm.
> >> shouldn't there be 3 lines for proxy auth? with IE11 I see those 
> >> three lines (2x TCP_DENIED/407 and 1x TCP_MISS/200), no popup at all.
> 
> Not specifically. There should be 1+ for NTLM. Success with NTLM shows
> 2+. Failure shows 1 or 3 or infinite loop (hello Safari and Firefox
> 2+30-
> ish).
> 
> 
> >>
> >> winbind/samba itself seems to work, as I can do an user auth 
> >> against apache with winbind/samba - even over some squid proxies 
> >> with connection-auth allowed. but not for proxy-auth.
> >> is there any option in squid.conf which prevents Edge to do a 
> >> successful auth?
> 
> If other software succeeds then the only thing that might be related 
> is the keep-alive option mentioned above. Otherwise the problem is in 
> Edge itself.
> 
> Amos
> 
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
_______________________________________________
squid-users mailing list
squid-users at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-users
_______________________________________________
squid-users mailing list
squid-users at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-users

From eliezer at ngtech.co.il  Fri Mar 10 11:50:38 2017
From: eliezer at ngtech.co.il (Eliezer  Croitoru)
Date: Fri, 10 Mar 2017 13:50:38 +0200
Subject: [squid-users] squid workers question
In-Reply-To: <5f406c4a-fe6f-8ff0-f097-0d5ab0cc031b@treenet.co.nz>
References: <20170309142134.GA30976@fantomas.sk>
 <f547a991-3178-7b95-113d-fb4b533c3056@measurement-factory.com>
 <20170309161940.GB7401@fantomas.sk>
 <5f406c4a-fe6f-8ff0-f097-0d5ab0cc031b@treenet.co.nz>
Message-ID: <01a401d29994$8cf2af10$a6d80d30$@ngtech.co.il>

Just to add that one of my current test labs of squid is a combination of:
1 haproxy as balancer(or a custom LB I wrote)
2+ squid instances with the proxy protocol enabled and each has it's own ufs\aufs cache_dir

The idea was to verify if it would be possible to let different instances communicate via htcp\icp as the "signaling" or "communication" channel instead of shared memory.
The situation is that I cannot continue testing it since there is a bug with the sibling proxies communication.

I would be happy if this is will be resolved and then the test might be able to let couple squid instances that runs on the same machine to be able to utilize aufs\ufs better(taking into account that there is an overhead to this whole setup..) for some places.

Eliezer

----
Eliezer Croitoru
Linux System Administrator
Mobile: +972-5-28704261
Email: eliezer at ngtech.co.il


-----Original Message-----
From: squid-users [mailto:squid-users-bounces at lists.squid-cache.org] On Behalf Of Amos Jeffries
Sent: Thursday, March 9, 2017 6:36 PM
To: squid-users at lists.squid-cache.org
Subject: Re: [squid-users] squid workers question

On 10/03/2017 5:19 a.m., Matus UHLAR - fantomas wrote:
>> On 03/09/2017 07:21 AM, Matus UHLAR - fantomas wrote:
>>> I have installed squid 3.4.8 on linux 3.16/64bit (debian 8 / jessie
>>> version)
>>>
>>> (I know it's old, but I prefer using distribution-provided SW unless 
>>> it has real problem distribution isn't able to fix)
> 
> On 09.03.17 09:07, Alex Rousskov wrote:
>> My answers are based on v5 code. (I know v5 is new, but I do not 
>> remember v3.4 specifics and v5 answers will be valid for a longer 
>> time.)
> 
> OK
> 
>>> I configured rock store (for smaller files) and (later) standard 
>>> aufs for others:
>>>
>>> cache_dir rock /var/spool/squid3/rock 1024 max-size=32768 #cache_dir 
>>> aufs /var/spool/squid3 8192 16 256 min-size=32769
>>>
>>> are those correct values? (bug 3411 says something about 256B 
>>> metadata)
> 
>> Both rock and AUFS stores support large objects so there is no 
>> requirement to split storage based on object sizes. Each store has 
>> various pros and cons, but lack of large object support is not one of 
>> the distinguishing characteristics.
> 
> I thought the cons of *ufs/disks is ineffective storage of small 
> files, while rock is ineffective with big files...
> 

Yes, *efficiency*, not lack of support.

[except in this case of your 3.4 where rock does explicitly lack the support].


>>> - do I get it right that kid1 is the Master, kid2 is the disker for rock
>>>   store and kid3 is the single worker process?
>>
>> In SMP mode (which, BTW, AUFS store does not support),
> 
> could it crash squid instead of complaining?

Are you talking about when AUFS is used in SMP mode? it is not that simple, there are ways to configure it to be used by a single worker ("if" directive, or $(process_number) macro) which is fine and work well for certain version-specific situations.


Amos

_______________________________________________
squid-users mailing list
squid-users at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-users



From rousskov at measurement-factory.com  Fri Mar 10 15:52:18 2017
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Fri, 10 Mar 2017 08:52:18 -0700
Subject: [squid-users] squid workers question
In-Reply-To: <20170310093832.GB31011@fantomas.sk>
References: <20170309142134.GA30976@fantomas.sk>
 <f547a991-3178-7b95-113d-fb4b533c3056@measurement-factory.com>
 <20170309161940.GB7401@fantomas.sk>
 <6ca274d9-948c-01c5-c3dd-13044d3cfe7b@measurement-factory.com>
 <20170309165429.GA8584@fantomas.sk>
 <8ef542a5-dba3-236f-224b-944990d4cd68@measurement-factory.com>
 <20170309172435.GC8584@fantomas.sk>
 <7b386e00-97ed-054c-902f-0df367e8ec49@measurement-factory.com>
 <20170310093832.GB31011@fantomas.sk>
Message-ID: <ab1f1823-51ba-89e8-1b81-e6a270c2159e@measurement-factory.com>

On 03/10/2017 02:38 AM, Matus UHLAR - fantomas wrote:
>> On 03/09/2017 10:24 AM, Matus UHLAR - fantomas wrote:
>>> is running aufs with rock store and safe, when not running with "-N"?
> 
> On 09.03.17 11:02, Alex Rousskov wrote:
>> Running AUFS in SMP mode is unsafe by default but some admins use
>> configuration hacks to make it work for them. Primary Store developers
>> do not test such configurations (or AUFS in general).
> 
> according to the "workers" docs, "workers 0" is the same as "squid -N" and
> "workers 1" is non-SMP mode

Sorry, but that 2010 documentation is outdated. It was written before
Rock store, a 2011 feature that changed what "SMP mode" means. This is
my fault. Here is a replacement draft that I was working on until wiki
went down:

> NAME: workers
> DEFAULT: 1
> 	Number of main Squid processes or "workers" to fork and maintain.
> 
> 	In a typical setup, each worker listens on all http_port(s) and
> 	proxies requests without talking to other workers. Depending on
> 	configuration, other Squid processes (e.g., rock store "diskers")
> 	may also participate in request processing. All such Squid processes
> 	are collectively called "kids".
> 
> 	Setting workers to 0 disables kids creation and is similar to
> 	running "squid -N ...". A positive value starts that many workers.
> 
> 	When multiple concurrent kids are in use, Squid is said to work in
> 	"SMP mode". Some Squid features (e.g., ufs-based cache_dirs) are not
> 	SMP-aware and should not or cannot be used in SMP mode.
> 
> 	See http://wiki.squid-cache.org/Features/SmpScale for details.

The final version will probably move and extend the terminology-related
text to the SMP section preamble -- it is kind of wrong to talk about
diskers when documenting workers. Improvements and constructive
suggestions welcomed!


> so, with "workers 1", both aufs and rock should work properly, with rock
> using separate process, correct?

There are several ways to interpret your question, but the most likely
interpretation leads to the "incorrect" answer: Without -N, a
combination of "workers 1" and at least one "cache_dir rock" enables
SMP. Do not use ufs-based cache_dirs in SMP mode.


>> Running a combination of AUFS and Rock stores in SMP mode is crazy.

> I get that as in SMP mode, ufs should be used?

I do not know what you mean by "that" but ufs is not SMP-aware. Thus,
ufs (including all ufs-based stores such as aufs and diskd):

* should not be used in SMP mode at all and
* must not be used in SMP mode in combination with rock stores.


HTH,

Alex.



From uhlar at fantomas.sk  Fri Mar 10 18:00:06 2017
From: uhlar at fantomas.sk (Matus UHLAR - fantomas)
Date: Fri, 10 Mar 2017 19:00:06 +0100
Subject: [squid-users] debugging squid memory
Message-ID: <20170310180005.GA5196@fantomas.sk>

Hello,

as I have already noted in other thread, I seem to have memory leak in squid
3.4.8 (debian 8 jessie) version, only memory cache used now.

Looking at cacti graphs, seems that memory usage grows by half-hour or hour
jumps. Is there any job done periodically in squid?


and if there's nothing like that, and I'd like to debug squid memory usage
to see, if there's a leak:
are there any useful hints (including: don't do it)?

- which debug_options to use (13, 19, 59, 63)?

- can I find out anything by squidclient or cachemgr.cgi
   (looking at vm_objects didn't found anything useful).

- any hint what to search for in logs?

Thanks.

-- 
Matus UHLAR - fantomas, uhlar at fantomas.sk ; http://www.fantomas.sk/
Warning: I wish NOT to receive e-mail advertising to this address.
Varovanie: na tuto adresu chcem NEDOSTAVAT akukolvek reklamnu postu.
REALITY.SYS corrupted. Press any key to reboot Universe.


From ygreenfield at kewsystems.com  Fri Mar 10 19:14:36 2017
From: ygreenfield at kewsystems.com (Yosi Greenfield)
Date: Fri, 10 Mar 2017 14:14:36 -0500
Subject: [squid-users] Data usage reported in log files
Message-ID: <DB929284116348E1AED00305A358D1F3@OhrSomayach>

Hello all,

I'm analyzing my squid logs with sarg, and I see that the number of
bytes reported as used by any particular user are often nowhere
near the bytes reported by netflow and tcpdump.

I'm trying to trace my users' data usage by site, but I'm unable to
do so from the log files because of this.

Can someone please explain to me what I might be missing? Why does
squid log report one thing and netflow and tcpdump show something
else?

Thanks very much!

Yosi



From Antony.Stone at squid.open.source.it  Fri Mar 10 19:20:42 2017
From: Antony.Stone at squid.open.source.it (Antony Stone)
Date: Fri, 10 Mar 2017 20:20:42 +0100
Subject: [squid-users] Data usage reported in log files
In-Reply-To: <DB929284116348E1AED00305A358D1F3@OhrSomayach>
References: <DB929284116348E1AED00305A358D1F3@OhrSomayach>
Message-ID: <201703102020.42470.Antony.Stone@squid.open.source.it>

On Friday 10 March 2017 at 20:14:36, Yosi Greenfield wrote:

> Hello all,
> 
> I'm analyzing my squid logs with sarg, and I see that the number of
> bytes reported as used by any particular user are often nowhere
> near the bytes reported by netflow and tcpdump.

Which is larger?

> I'm trying to trace my users' data usage by site, but I'm unable to
> do so from the log files because of this.

Well, what is it you really want to know?

netflow / tcpdump will give you accurate numbers for the quantity of data on 
your Internet link - I assume this is what you're most interested in?

Squid will show you what quantity of data goes to/from the clients, but is 
that really important?

> Can someone please explain to me what I might be missing? Why does
> squid log report one thing and netflow and tcpdump show something
> else?

Data compression?

HTTP responses are often gzipped, so if tcpdump is showing you smaller numbers 
of bytes than Squid reports, that's what I'd look at first.


Antony.

-- 
This sentence contains exacly three erors.

                                                   Please reply to the list;
                                                         please *don't* CC me.


From ygreenfield at kewsystems.com  Fri Mar 10 19:27:34 2017
From: ygreenfield at kewsystems.com (Yosi Greenfield)
Date: Fri, 10 Mar 2017 14:27:34 -0500
Subject: [squid-users] Data usage reported in log files
In-Reply-To: <201703102020.42470.Antony.Stone@squid.open.source.it>
References: <DB929284116348E1AED00305A358D1F3@OhrSomayach>
 <201703102020.42470.Antony.Stone@squid.open.source.it>
Message-ID: <C0711A87F86D4230ACCB06CB219A1983@OhrSomayach>

Thanks!

Netflow is much larger.

I really want to know exactly what site is costing my users data. Many of
our users are on metered connections and are paying for overage, but I can't
tell where that overage is being used. Are they using youtube, webmail,
wetransfer? I see only a fraction of their actual proxy usage in my squid
logs.

Data compression would give the opposite result, so that's not what I'm
seeing.

Any other ideas?


-----Original Message-----
From: squid-users [mailto:squid-users-bounces at lists.squid-cache.org] On
Behalf Of Antony Stone
Sent: Friday, March 10, 2017 2:21 PM
To: squid-users at lists.squid-cache.org
Subject: Re: [squid-users] Data usage reported in log files

On Friday 10 March 2017 at 20:14:36, Yosi Greenfield wrote:

> Hello all,
> 
> I'm analyzing my squid logs with sarg, and I see that the number of 
> bytes reported as used by any particular user are often nowhere near 
> the bytes reported by netflow and tcpdump.

Which is larger?

> I'm trying to trace my users' data usage by site, but I'm unable to do 
> so from the log files because of this.

Well, what is it you really want to know?

netflow / tcpdump will give you accurate numbers for the quantity of data on
your Internet link - I assume this is what you're most interested in?

Squid will show you what quantity of data goes to/from the clients, but is
that really important?

> Can someone please explain to me what I might be missing? Why does 
> squid log report one thing and netflow and tcpdump show something 
> else?

Data compression?

HTTP responses are often gzipped, so if tcpdump is showing you smaller
numbers of bytes than Squid reports, that's what I'd look at first.


Antony.

-- 
This sentence contains exacly three erors.

                                                   Please reply to the list;
                                                         please *don't* CC
me.
_______________________________________________
squid-users mailing list
squid-users at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-users



From marcus.kool at urlfilterdb.com  Fri Mar 10 20:37:39 2017
From: marcus.kool at urlfilterdb.com (Marcus Kool)
Date: Fri, 10 Mar 2017 17:37:39 -0300
Subject: [squid-users] Data usage reported in log files
In-Reply-To: <C0711A87F86D4230ACCB06CB219A1983@OhrSomayach>
References: <DB929284116348E1AED00305A358D1F3@OhrSomayach>
 <201703102020.42470.Antony.Stone@squid.open.source.it>
 <C0711A87F86D4230ACCB06CB219A1983@OhrSomayach>
Message-ID: <6f600f2b-9f3b-2cf7-09bc-dd09ba2965a0@urlfilterdb.com>



On 10/03/17 16:27, Yosi Greenfield wrote:
> Thanks!
>
> Netflow is much larger.
>
> I really want to know exactly what site is costing my users data. Many of
> our users are on metered connections and are paying for overage, but I can't
> tell where that overage is being used. Are they using youtube, webmail,
> wetransfer? I see only a fraction of their actual proxy usage in my squid
> logs.
>
> Data compression would give the opposite result, so that's not what I'm
> seeing.
>
> Any other ideas?

Is there any traffic that is not directed to Squid?

Do you use ssl-bump in bump mode ?
If not, Squid has no idea how many bytes go through the (HTTPS) tunnels.

Marcus


> -----Original Message-----
> From: squid-users [mailto:squid-users-bounces at lists.squid-cache.org] On
> Behalf Of Antony Stone
> Sent: Friday, March 10, 2017 2:21 PM
> To: squid-users at lists.squid-cache.org
> Subject: Re: [squid-users] Data usage reported in log files
>
> On Friday 10 March 2017 at 20:14:36, Yosi Greenfield wrote:
>
>> Hello all,
>>
>> I'm analyzing my squid logs with sarg, and I see that the number of
>> bytes reported as used by any particular user are often nowhere near
>> the bytes reported by netflow and tcpdump.
>
> Which is larger?
>
>> I'm trying to trace my users' data usage by site, but I'm unable to do
>> so from the log files because of this.
>
> Well, what is it you really want to know?
>
> netflow / tcpdump will give you accurate numbers for the quantity of data on
> your Internet link - I assume this is what you're most interested in?
>
> Squid will show you what quantity of data goes to/from the clients, but is
> that really important?
>
>> Can someone please explain to me what I might be missing? Why does
>> squid log report one thing and netflow and tcpdump show something
>> else?
>
> Data compression?
>
> HTTP responses are often gzipped, so if tcpdump is showing you smaller
> numbers of bytes than Squid reports, that's what I'd look at first.
>
>
> Antony.
>


From ygreenfield at kewsystems.com  Fri Mar 10 20:44:59 2017
From: ygreenfield at kewsystems.com (Yosi Greenfield)
Date: Fri, 10 Mar 2017 15:44:59 -0500
Subject: [squid-users] Data usage reported in log files
In-Reply-To: <6f600f2b-9f3b-2cf7-09bc-dd09ba2965a0@urlfilterdb.com>
References: <DB929284116348E1AED00305A358D1F3@OhrSomayach>
 <201703102020.42470.Antony.Stone@squid.open.source.it>
 <C0711A87F86D4230ACCB06CB219A1983@OhrSomayach>
 <6f600f2b-9f3b-2cf7-09bc-dd09ba2965a0@urlfilterdb.com>
Message-ID: <9BEC434D6CCB4111B11C103B4BB6C279@OhrSomayach>

Aha! That could be it. I use sslbump, but not for all users. I'll
check that out, although I think that it's a problem even for bumped
users. Even for bumped users we don't bump all sites, so that really
could be it.

Thanks!


-----Original Message-----
From: squid-users [mailto:squid-users-bounces at lists.squid-cache.org] On
Behalf Of Marcus Kool
Sent: Friday, March 10, 2017 3:38 PM
To: squid-users at lists.squid-cache.org
Subject: Re: [squid-users] Data usage reported in log files



On 10/03/17 16:27, Yosi Greenfield wrote:
> Thanks!
>
> Netflow is much larger.
>
> I really want to know exactly what site is costing my users data. Many 
> of our users are on metered connections and are paying for overage, 
> but I can't tell where that overage is being used. Are they using 
> youtube, webmail, wetransfer? I see only a fraction of their actual 
> proxy usage in my squid logs.
>
> Data compression would give the opposite result, so that's not what 
> I'm seeing.
>
> Any other ideas?

Is there any traffic that is not directed to Squid?

Do you use ssl-bump in bump mode ?
If not, Squid has no idea how many bytes go through the (HTTPS) tunnels.

Marcus


> -----Original Message-----
> From: squid-users [mailto:squid-users-bounces at lists.squid-cache.org] 
> On Behalf Of Antony Stone
> Sent: Friday, March 10, 2017 2:21 PM
> To: squid-users at lists.squid-cache.org
> Subject: Re: [squid-users] Data usage reported in log files
>
> On Friday 10 March 2017 at 20:14:36, Yosi Greenfield wrote:
>
>> Hello all,
>>
>> I'm analyzing my squid logs with sarg, and I see that the number of 
>> bytes reported as used by any particular user are often nowhere near 
>> the bytes reported by netflow and tcpdump.
>
> Which is larger?
>
>> I'm trying to trace my users' data usage by site, but I'm unable to 
>> do so from the log files because of this.
>
> Well, what is it you really want to know?
>
> netflow / tcpdump will give you accurate numbers for the quantity of 
> data on your Internet link - I assume this is what you're most interested
in?
>
> Squid will show you what quantity of data goes to/from the clients, 
> but is that really important?
>
>> Can someone please explain to me what I might be missing? Why does 
>> squid log report one thing and netflow and tcpdump show something 
>> else?
>
> Data compression?
>
> HTTP responses are often gzipped, so if tcpdump is showing you smaller 
> numbers of bytes than Squid reports, that's what I'd look at first.
>
>
> Antony.
>
_______________________________________________
squid-users mailing list
squid-users at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-users



From yvoinov at gmail.com  Fri Mar 10 20:50:19 2017
From: yvoinov at gmail.com (Yuri Voinov)
Date: Sat, 11 Mar 2017 02:50:19 +0600
Subject: [squid-users] Data usage reported in log files
In-Reply-To: <9BEC434D6CCB4111B11C103B4BB6C279@OhrSomayach>
References: <DB929284116348E1AED00305A358D1F3@OhrSomayach>
 <201703102020.42470.Antony.Stone@squid.open.source.it>
 <C0711A87F86D4230ACCB06CB219A1983@OhrSomayach>
 <6f600f2b-9f3b-2cf7-09bc-dd09ba2965a0@urlfilterdb.com>
 <9BEC434D6CCB4111B11C103B4BB6C279@OhrSomayach>
Message-ID: <84a3effa-f50c-d574-c677-52a28125b941@gmail.com>

Gentlemen, and it never occurred to you that there are other types of
traffic besides HTTP / HTTPS, right?

DNS, ICMP, other protocols?


11.03.2017 2:44, Yosi Greenfield ?????:
> Aha! That could be it. I use sslbump, but not for all users. I'll
> check that out, although I think that it's a problem even for bumped
> users. Even for bumped users we don't bump all sites, so that really
> could be it.
>
> Thanks!
>
>
> -----Original Message-----
> From: squid-users [mailto:squid-users-bounces at lists.squid-cache.org] On
> Behalf Of Marcus Kool
> Sent: Friday, March 10, 2017 3:38 PM
> To: squid-users at lists.squid-cache.org
> Subject: Re: [squid-users] Data usage reported in log files
>
>
>
> On 10/03/17 16:27, Yosi Greenfield wrote:
>> Thanks!
>>
>> Netflow is much larger.
>>
>> I really want to know exactly what site is costing my users data. Many 
>> of our users are on metered connections and are paying for overage, 
>> but I can't tell where that overage is being used. Are they using 
>> youtube, webmail, wetransfer? I see only a fraction of their actual 
>> proxy usage in my squid logs.
>>
>> Data compression would give the opposite result, so that's not what 
>> I'm seeing.
>>
>> Any other ideas?
> Is there any traffic that is not directed to Squid?
>
> Do you use ssl-bump in bump mode ?
> If not, Squid has no idea how many bytes go through the (HTTPS) tunnels.
>
> Marcus
>
>
>> -----Original Message-----
>> From: squid-users [mailto:squid-users-bounces at lists.squid-cache.org] 
>> On Behalf Of Antony Stone
>> Sent: Friday, March 10, 2017 2:21 PM
>> To: squid-users at lists.squid-cache.org
>> Subject: Re: [squid-users] Data usage reported in log files
>>
>> On Friday 10 March 2017 at 20:14:36, Yosi Greenfield wrote:
>>
>>> Hello all,
>>>
>>> I'm analyzing my squid logs with sarg, and I see that the number of 
>>> bytes reported as used by any particular user are often nowhere near 
>>> the bytes reported by netflow and tcpdump.
>> Which is larger?
>>
>>> I'm trying to trace my users' data usage by site, but I'm unable to 
>>> do so from the log files because of this.
>> Well, what is it you really want to know?
>>
>> netflow / tcpdump will give you accurate numbers for the quantity of 
>> data on your Internet link - I assume this is what you're most interested
> in?
>> Squid will show you what quantity of data goes to/from the clients, 
>> but is that really important?
>>
>>> Can someone please explain to me what I might be missing? Why does 
>>> squid log report one thing and netflow and tcpdump show something 
>>> else?
>> Data compression?
>>
>> HTTP responses are often gzipped, so if tcpdump is showing you smaller 
>> numbers of bytes than Squid reports, that's what I'd look at first.
>>
>>
>> Antony.
>>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users

-- 
Bugs to the Future
-------------- next part --------------
A non-text attachment was scrubbed...
Name: 0x613DEC46.asc
Type: application/pgp-keys
Size: 2437 bytes
Desc: not available
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20170311/f672bb40/attachment.key>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: signature.asc
Type: application/pgp-signature
Size: 473 bytes
Desc: OpenPGP digital signature
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20170311/f672bb40/attachment.sig>

From rousskov at measurement-factory.com  Fri Mar 10 20:57:25 2017
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Fri, 10 Mar 2017 13:57:25 -0700
Subject: [squid-users] Data usage reported in log files
In-Reply-To: <6f600f2b-9f3b-2cf7-09bc-dd09ba2965a0@urlfilterdb.com>
References: <DB929284116348E1AED00305A358D1F3@OhrSomayach>
 <201703102020.42470.Antony.Stone@squid.open.source.it>
 <C0711A87F86D4230ACCB06CB219A1983@OhrSomayach>
 <6f600f2b-9f3b-2cf7-09bc-dd09ba2965a0@urlfilterdb.com>
Message-ID: <0bab97e6-e503-a2f9-0c8d-371047e67b52@measurement-factory.com>

On 03/10/2017 01:37 PM, Marcus Kool wrote:
> Squid has no idea how many bytes go through the (HTTPS) tunnels.

Actually, Squid knows the number of raw (encrypted) TCP payload bytes
inside a tunnel and should log that.

Squid also knows and logs the number of HTTP (decrypted) bytes if the
SSL tunnel is bumped. In that case, the logged number is often smaller
but could also be larger than the corresponding TCP payload, depending
on whether SSL uses compression.

In any case, Squid numbers do not contain TCP/IP/Ethernet headers and
control messages. They may also lack HTTP chunked encoding overheads.
Failed Squid-to-server connections are not logged if they were
successfully retried.

There are also logging/accounting bugs because there is currently no
automated system to detect them. For a recent example, see our fix at
http://bazaar.launchpad.net/~squid/squid/trunk/revision/14838

If you use the latest release and see a disparity (between Squid-logged
numbers and other sources of information) that cannot be explained by
known factors, consider reporting it.


Thank you,

Alex.



From Antony.Stone at squid.open.source.it  Fri Mar 10 20:57:54 2017
From: Antony.Stone at squid.open.source.it (Antony Stone)
Date: Fri, 10 Mar 2017 21:57:54 +0100
Subject: [squid-users] Data usage reported in log files
In-Reply-To: <84a3effa-f50c-d574-c677-52a28125b941@gmail.com>
References: <DB929284116348E1AED00305A358D1F3@OhrSomayach>
 <9BEC434D6CCB4111B11C103B4BB6C279@OhrSomayach>
 <84a3effa-f50c-d574-c677-52a28125b941@gmail.com>
Message-ID: <201703102157.55130.Antony.Stone@squid.open.source.it>

On Friday 10 March 2017 at 21:50:19, Yuri Voinov wrote:

> Gentlemen, and it never occurred to you that there are other types of
> traffic besides HTTP / HTTPS, right?
> 
> DNS, ICMP, other protocols?

I'm assuming Yosi has been measuring only TCP traffic, but even if he's been 
measuring everything, I don't think DNS, ICMP and other protocols would add 
more than 1% on top of HTTP/S, unless (as Marcus suggested) there is also 
totally-non-Squid traffic on the link being measured.


Antony.

> 11.03.2017 2:44, Yosi Greenfield ?????:
> > Aha! That could be it. I use sslbump, but not for all users. I'll
> > check that out, although I think that it's a problem even for bumped
> > users. Even for bumped users we don't bump all sites, so that really
> > could be it.
> > 
> > Thanks!
> > 
> > 
> > -----Original Message-----
> > From: squid-users [mailto:squid-users-bounces at lists.squid-cache.org] On
> > Behalf Of Marcus Kool
> > Sent: Friday, March 10, 2017 3:38 PM
> > To: squid-users at lists.squid-cache.org
> > Subject: Re: [squid-users] Data usage reported in log files
> > 
> > On 10/03/17 16:27, Yosi Greenfield wrote:
> >> Thanks!
> >> 
> >> Netflow is much larger.
> >> 
> >> I really want to know exactly what site is costing my users data. Many
> >> of our users are on metered connections and are paying for overage,
> >> but I can't tell where that overage is being used. Are they using
> >> youtube, webmail, wetransfer? I see only a fraction of their actual
> >> proxy usage in my squid logs.
> >> 
> >> Data compression would give the opposite result, so that's not what
> >> I'm seeing.
> >> 
> >> Any other ideas?
> > 
> > Is there any traffic that is not directed to Squid?
> > 
> > Do you use ssl-bump in bump mode ?
> > If not, Squid has no idea how many bytes go through the (HTTPS) tunnels.
> > 
> > Marcus
> > 
> >> -----Original Message-----
> >> From: squid-users [mailto:squid-users-bounces at lists.squid-cache.org]
> >> On Behalf Of Antony Stone
> >> Sent: Friday, March 10, 2017 2:21 PM
> >> To: squid-users at lists.squid-cache.org
> >> Subject: Re: [squid-users] Data usage reported in log files
> >> 
> >> On Friday 10 March 2017 at 20:14:36, Yosi Greenfield wrote:
> >>> Hello all,
> >>> 
> >>> I'm analyzing my squid logs with sarg, and I see that the number of
> >>> bytes reported as used by any particular user are often nowhere near
> >>> the bytes reported by netflow and tcpdump.
> >> 
> >> Which is larger?
> >> 
> >>> I'm trying to trace my users' data usage by site, but I'm unable to
> >>> do so from the log files because of this.
> >> 
> >> Well, what is it you really want to know?
> >> 
> >> netflow / tcpdump will give you accurate numbers for the quantity of
> >> data on your Internet link - I assume this is what you're most
> >> interested in?
> > 
> >> Squid will show you what quantity of data goes to/from the clients,
> >> but is that really important?
> >> 
> >>> Can someone please explain to me what I might be missing? Why does
> >>> squid log report one thing and netflow and tcpdump show something
> >>> else?
> >> 
> >> Data compression?
> >> 
> >> HTTP responses are often gzipped, so if tcpdump is showing you smaller
> >> numbers of bytes than Squid reports, that's what I'd look at first.
> >> 
> >> 
> >> Antony.

-- 
Normal people think "If it ain't broke, don't fix it".
Engineers think "If it ain't broke, it doesn't have enough features yet".

                                                   Please reply to the list;
                                                         please *don't* CC me.


From yvoinov at gmail.com  Fri Mar 10 21:19:30 2017
From: yvoinov at gmail.com (Yuri Voinov)
Date: Sat, 11 Mar 2017 03:19:30 +0600
Subject: [squid-users] Data usage reported in log files
In-Reply-To: <201703102157.55130.Antony.Stone@squid.open.source.it>
References: <DB929284116348E1AED00305A358D1F3@OhrSomayach>
 <9BEC434D6CCB4111B11C103B4BB6C279@OhrSomayach>
 <84a3effa-f50c-d574-c677-52a28125b941@gmail.com>
 <201703102157.55130.Antony.Stone@squid.open.source.it>
Message-ID: <9cc68282-773a-b25b-9679-90fc6f215d65@gmail.com>



11.03.2017 2:57, Antony Stone ?????:
> On Friday 10 March 2017 at 21:50:19, Yuri Voinov wrote:
>
>> Gentlemen, and it never occurred to you that there are other types of
>> traffic besides HTTP / HTTPS, right?
>>
>> DNS, ICMP, other protocols?
> I'm assuming Yosi has been measuring only TCP traffic, but even if he's been 
> measuring everything, I don't think DNS, ICMP and other protocols would add 
> more than 1% on top of HTTP/S, unless (as Marcus suggested) there is also 
> totally-non-Squid traffic on the link being measured.
Come on, sure? Even in L7? Really? Cool story, bro!
>
>
> Antony.
>
>> 11.03.2017 2:44, Yosi Greenfield ?????:
>>> Aha! That could be it. I use sslbump, but not for all users. I'll
>>> check that out, although I think that it's a problem even for bumped
>>> users. Even for bumped users we don't bump all sites, so that really
>>> could be it.
>>>
>>> Thanks!
>>>
>>>
>>> -----Original Message-----
>>> From: squid-users [mailto:squid-users-bounces at lists.squid-cache.org] On
>>> Behalf Of Marcus Kool
>>> Sent: Friday, March 10, 2017 3:38 PM
>>> To: squid-users at lists.squid-cache.org
>>> Subject: Re: [squid-users] Data usage reported in log files
>>>
>>> On 10/03/17 16:27, Yosi Greenfield wrote:
>>>> Thanks!
>>>>
>>>> Netflow is much larger.
>>>>
>>>> I really want to know exactly what site is costing my users data. Many
>>>> of our users are on metered connections and are paying for overage,
>>>> but I can't tell where that overage is being used. Are they using
>>>> youtube, webmail, wetransfer? I see only a fraction of their actual
>>>> proxy usage in my squid logs.
>>>>
>>>> Data compression would give the opposite result, so that's not what
>>>> I'm seeing.
>>>>
>>>> Any other ideas?
>>> Is there any traffic that is not directed to Squid?
>>>
>>> Do you use ssl-bump in bump mode ?
>>> If not, Squid has no idea how many bytes go through the (HTTPS) tunnels.
>>>
>>> Marcus
>>>
>>>> -----Original Message-----
>>>> From: squid-users [mailto:squid-users-bounces at lists.squid-cache.org]
>>>> On Behalf Of Antony Stone
>>>> Sent: Friday, March 10, 2017 2:21 PM
>>>> To: squid-users at lists.squid-cache.org
>>>> Subject: Re: [squid-users] Data usage reported in log files
>>>>
>>>> On Friday 10 March 2017 at 20:14:36, Yosi Greenfield wrote:
>>>>> Hello all,
>>>>>
>>>>> I'm analyzing my squid logs with sarg, and I see that the number of
>>>>> bytes reported as used by any particular user are often nowhere near
>>>>> the bytes reported by netflow and tcpdump.
>>>> Which is larger?
>>>>
>>>>> I'm trying to trace my users' data usage by site, but I'm unable to
>>>>> do so from the log files because of this.
>>>> Well, what is it you really want to know?
>>>>
>>>> netflow / tcpdump will give you accurate numbers for the quantity of
>>>> data on your Internet link - I assume this is what you're most
>>>> interested in?
>>>> Squid will show you what quantity of data goes to/from the clients,
>>>> but is that really important?
>>>>
>>>>> Can someone please explain to me what I might be missing? Why does
>>>>> squid log report one thing and netflow and tcpdump show something
>>>>> else?
>>>> Data compression?
>>>>
>>>> HTTP responses are often gzipped, so if tcpdump is showing you smaller
>>>> numbers of bytes than Squid reports, that's what I'd look at first.
>>>>
>>>>
>>>> Antony.

-- 
Bugs to the Future
-------------- next part --------------
A non-text attachment was scrubbed...
Name: 0x613DEC46.asc
Type: application/pgp-keys
Size: 2437 bytes
Desc: not available
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20170311/e3e05e85/attachment.key>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: signature.asc
Type: application/pgp-signature
Size: 473 bytes
Desc: OpenPGP digital signature
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20170311/e3e05e85/attachment.sig>

From yvoinov at gmail.com  Fri Mar 10 21:22:59 2017
From: yvoinov at gmail.com (Yuri Voinov)
Date: Sat, 11 Mar 2017 03:22:59 +0600
Subject: [squid-users] Data usage reported in log files
In-Reply-To: <9cc68282-773a-b25b-9679-90fc6f215d65@gmail.com>
References: <DB929284116348E1AED00305A358D1F3@OhrSomayach>
 <9BEC434D6CCB4111B11C103B4BB6C279@OhrSomayach>
 <84a3effa-f50c-d574-c677-52a28125b941@gmail.com>
 <201703102157.55130.Antony.Stone@squid.open.source.it>
 <9cc68282-773a-b25b-9679-90fc6f215d65@gmail.com>
Message-ID: <9f9d0e4a-1a0c-5a60-57b7-256777bf71bf@gmail.com>

Of course, there is no stream video from security cams, no voice IP, no
SIP, no torrents, no RDP, no other protocol. They simple does not exists
and we're all believe that's all not above over 1% of overall traffic.
Yes. Sure. Really.

Only web-surfing :) Sure :)


11.03.2017 3:19, Yuri Voinov ?????:
>
> 11.03.2017 2:57, Antony Stone ?????:
>> On Friday 10 March 2017 at 21:50:19, Yuri Voinov wrote:
>>
>>> Gentlemen, and it never occurred to you that there are other types of
>>> traffic besides HTTP / HTTPS, right?
>>>
>>> DNS, ICMP, other protocols?
>> I'm assuming Yosi has been measuring only TCP traffic, but even if he's been 
>> measuring everything, I don't think DNS, ICMP and other protocols would add 
>> more than 1% on top of HTTP/S, unless (as Marcus suggested) there is also 
>> totally-non-Squid traffic on the link being measured.
> Come on, sure? Even in L7? Really? Cool story, bro!
>>
>> Antony.
>>
>>> 11.03.2017 2:44, Yosi Greenfield ?????:
>>>> Aha! That could be it. I use sslbump, but not for all users. I'll
>>>> check that out, although I think that it's a problem even for bumped
>>>> users. Even for bumped users we don't bump all sites, so that really
>>>> could be it.
>>>>
>>>> Thanks!
>>>>
>>>>
>>>> -----Original Message-----
>>>> From: squid-users [mailto:squid-users-bounces at lists.squid-cache.org] On
>>>> Behalf Of Marcus Kool
>>>> Sent: Friday, March 10, 2017 3:38 PM
>>>> To: squid-users at lists.squid-cache.org
>>>> Subject: Re: [squid-users] Data usage reported in log files
>>>>
>>>> On 10/03/17 16:27, Yosi Greenfield wrote:
>>>>> Thanks!
>>>>>
>>>>> Netflow is much larger.
>>>>>
>>>>> I really want to know exactly what site is costing my users data. Many
>>>>> of our users are on metered connections and are paying for overage,
>>>>> but I can't tell where that overage is being used. Are they using
>>>>> youtube, webmail, wetransfer? I see only a fraction of their actual
>>>>> proxy usage in my squid logs.
>>>>>
>>>>> Data compression would give the opposite result, so that's not what
>>>>> I'm seeing.
>>>>>
>>>>> Any other ideas?
>>>> Is there any traffic that is not directed to Squid?
>>>>
>>>> Do you use ssl-bump in bump mode ?
>>>> If not, Squid has no idea how many bytes go through the (HTTPS) tunnels.
>>>>
>>>> Marcus
>>>>
>>>>> -----Original Message-----
>>>>> From: squid-users [mailto:squid-users-bounces at lists.squid-cache.org]
>>>>> On Behalf Of Antony Stone
>>>>> Sent: Friday, March 10, 2017 2:21 PM
>>>>> To: squid-users at lists.squid-cache.org
>>>>> Subject: Re: [squid-users] Data usage reported in log files
>>>>>
>>>>> On Friday 10 March 2017 at 20:14:36, Yosi Greenfield wrote:
>>>>>> Hello all,
>>>>>>
>>>>>> I'm analyzing my squid logs with sarg, and I see that the number of
>>>>>> bytes reported as used by any particular user are often nowhere near
>>>>>> the bytes reported by netflow and tcpdump.
>>>>> Which is larger?
>>>>>
>>>>>> I'm trying to trace my users' data usage by site, but I'm unable to
>>>>>> do so from the log files because of this.
>>>>> Well, what is it you really want to know?
>>>>>
>>>>> netflow / tcpdump will give you accurate numbers for the quantity of
>>>>> data on your Internet link - I assume this is what you're most
>>>>> interested in?
>>>>> Squid will show you what quantity of data goes to/from the clients,
>>>>> but is that really important?
>>>>>
>>>>>> Can someone please explain to me what I might be missing? Why does
>>>>>> squid log report one thing and netflow and tcpdump show something
>>>>>> else?
>>>>> Data compression?
>>>>>
>>>>> HTTP responses are often gzipped, so if tcpdump is showing you smaller
>>>>> numbers of bytes than Squid reports, that's what I'd look at first.
>>>>>
>>>>>
>>>>> Antony.

-- 
Bugs to the Future
-------------- next part --------------
A non-text attachment was scrubbed...
Name: 0x613DEC46.asc
Type: application/pgp-keys
Size: 2437 bytes
Desc: not available
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20170311/0fcf275d/attachment.key>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: signature.asc
Type: application/pgp-signature
Size: 473 bytes
Desc: OpenPGP digital signature
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20170311/0fcf275d/attachment.sig>

From yvoinov at gmail.com  Fri Mar 10 21:27:35 2017
From: yvoinov at gmail.com (Yuri Voinov)
Date: Sat, 11 Mar 2017 03:27:35 +0600
Subject: [squid-users] Data usage reported in log files
In-Reply-To: <9f9d0e4a-1a0c-5a60-57b7-256777bf71bf@gmail.com>
References: <DB929284116348E1AED00305A358D1F3@OhrSomayach>
 <9BEC434D6CCB4111B11C103B4BB6C279@OhrSomayach>
 <84a3effa-f50c-d574-c677-52a28125b941@gmail.com>
 <201703102157.55130.Antony.Stone@squid.open.source.it>
 <9cc68282-773a-b25b-9679-90fc6f215d65@gmail.com>
 <9f9d0e4a-1a0c-5a60-57b7-256777bf71bf@gmail.com>
Message-ID: <727aa7dc-e131-c6d1-c560-dffebc83c8c8@gmail.com>

Think of one simple thing. Squid does not see and can not see protocols
that do not support. What do you expect from it? Does it work on L1/L2?
No? Then what is the discussion about?


11.03.2017 3:22, Yuri Voinov ?????:
> Of course, there is no stream video from security cams, no voice IP, no
> SIP, no torrents, no RDP, no other protocol. They simple does not exists
> and we're all believe that's all not above over 1% of overall traffic.
> Yes. Sure. Really.
>
> Only web-surfing :) Sure :)
>
>
> 11.03.2017 3:19, Yuri Voinov ?????:
>> 11.03.2017 2:57, Antony Stone ?????:
>>> On Friday 10 March 2017 at 21:50:19, Yuri Voinov wrote:
>>>
>>>> Gentlemen, and it never occurred to you that there are other types of
>>>> traffic besides HTTP / HTTPS, right?
>>>>
>>>> DNS, ICMP, other protocols?
>>> I'm assuming Yosi has been measuring only TCP traffic, but even if he's been 
>>> measuring everything, I don't think DNS, ICMP and other protocols would add 
>>> more than 1% on top of HTTP/S, unless (as Marcus suggested) there is also 
>>> totally-non-Squid traffic on the link being measured.
>> Come on, sure? Even in L7? Really? Cool story, bro!
>>> Antony.
>>>
>>>> 11.03.2017 2:44, Yosi Greenfield ?????:
>>>>> Aha! That could be it. I use sslbump, but not for all users. I'll
>>>>> check that out, although I think that it's a problem even for bumped
>>>>> users. Even for bumped users we don't bump all sites, so that really
>>>>> could be it.
>>>>>
>>>>> Thanks!
>>>>>
>>>>>
>>>>> -----Original Message-----
>>>>> From: squid-users [mailto:squid-users-bounces at lists.squid-cache.org] On
>>>>> Behalf Of Marcus Kool
>>>>> Sent: Friday, March 10, 2017 3:38 PM
>>>>> To: squid-users at lists.squid-cache.org
>>>>> Subject: Re: [squid-users] Data usage reported in log files
>>>>>
>>>>> On 10/03/17 16:27, Yosi Greenfield wrote:
>>>>>> Thanks!
>>>>>>
>>>>>> Netflow is much larger.
>>>>>>
>>>>>> I really want to know exactly what site is costing my users data. Many
>>>>>> of our users are on metered connections and are paying for overage,
>>>>>> but I can't tell where that overage is being used. Are they using
>>>>>> youtube, webmail, wetransfer? I see only a fraction of their actual
>>>>>> proxy usage in my squid logs.
>>>>>>
>>>>>> Data compression would give the opposite result, so that's not what
>>>>>> I'm seeing.
>>>>>>
>>>>>> Any other ideas?
>>>>> Is there any traffic that is not directed to Squid?
>>>>>
>>>>> Do you use ssl-bump in bump mode ?
>>>>> If not, Squid has no idea how many bytes go through the (HTTPS) tunnels.
>>>>>
>>>>> Marcus
>>>>>
>>>>>> -----Original Message-----
>>>>>> From: squid-users [mailto:squid-users-bounces at lists.squid-cache.org]
>>>>>> On Behalf Of Antony Stone
>>>>>> Sent: Friday, March 10, 2017 2:21 PM
>>>>>> To: squid-users at lists.squid-cache.org
>>>>>> Subject: Re: [squid-users] Data usage reported in log files
>>>>>>
>>>>>> On Friday 10 March 2017 at 20:14:36, Yosi Greenfield wrote:
>>>>>>> Hello all,
>>>>>>>
>>>>>>> I'm analyzing my squid logs with sarg, and I see that the number of
>>>>>>> bytes reported as used by any particular user are often nowhere near
>>>>>>> the bytes reported by netflow and tcpdump.
>>>>>> Which is larger?
>>>>>>
>>>>>>> I'm trying to trace my users' data usage by site, but I'm unable to
>>>>>>> do so from the log files because of this.
>>>>>> Well, what is it you really want to know?
>>>>>>
>>>>>> netflow / tcpdump will give you accurate numbers for the quantity of
>>>>>> data on your Internet link - I assume this is what you're most
>>>>>> interested in?
>>>>>> Squid will show you what quantity of data goes to/from the clients,
>>>>>> but is that really important?
>>>>>>
>>>>>>> Can someone please explain to me what I might be missing? Why does
>>>>>>> squid log report one thing and netflow and tcpdump show something
>>>>>>> else?
>>>>>> Data compression?
>>>>>>
>>>>>> HTTP responses are often gzipped, so if tcpdump is showing you smaller
>>>>>> numbers of bytes than Squid reports, that's what I'd look at first.
>>>>>>
>>>>>>
>>>>>> Antony.

-- 
Bugs to the Future
-------------- next part --------------
A non-text attachment was scrubbed...
Name: 0x613DEC46.asc
Type: application/pgp-keys
Size: 2437 bytes
Desc: not available
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20170311/db8b1acc/attachment.key>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: signature.asc
Type: application/pgp-signature
Size: 473 bytes
Desc: OpenPGP digital signature
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20170311/db8b1acc/attachment.sig>

From Antony.Stone at squid.open.source.it  Fri Mar 10 21:31:11 2017
From: Antony.Stone at squid.open.source.it (Antony Stone)
Date: Fri, 10 Mar 2017 22:31:11 +0100
Subject: [squid-users] Data usage reported in log files
In-Reply-To: <9f9d0e4a-1a0c-5a60-57b7-256777bf71bf@gmail.com>
References: <DB929284116348E1AED00305A358D1F3@OhrSomayach>
 <9cc68282-773a-b25b-9679-90fc6f215d65@gmail.com>
 <9f9d0e4a-1a0c-5a60-57b7-256777bf71bf@gmail.com>
Message-ID: <201703102231.12089.Antony.Stone@squid.open.source.it>

On Friday 10 March 2017 at 22:22:59, Yuri Voinov wrote:

> Of course, there is no stream video from security cams, no voice IP, no
> SIP, no torrents, no RDP, no other protocol. They simple does not exists
> and we're all believe that's all not above over 1% of overall traffic.
> Yes. Sure. Really.
> 
> Only web-surfing :) Sure :)

Thanks for the standard sarcasm.

Has it occurred to you that Yosi might have been measuring traffic to & from the 
IP of the Squid server, so as to ignore everything else he knows is happening 
on his network, so he can compare like with like?

My "not more than 1%" was for the additional traffic to/from the Squid server, 
other than HTTP/S.


Antony.

> 11.03.2017 3:19, Yuri Voinov ?????:
> > 11.03.2017 2:57, Antony Stone ?????:
> >> On Friday 10 March 2017 at 21:50:19, Yuri Voinov wrote:
> >>> Gentlemen, and it never occurred to you that there are other types of
> >>> traffic besides HTTP / HTTPS, right?
> >>> 
> >>> DNS, ICMP, other protocols?
> >> 
> >> I'm assuming Yosi has been measuring only TCP traffic, but even if he's
> >> been measuring everything, I don't think DNS, ICMP and other protocols
> >> would add more than 1% on top of HTTP/S, unless (as Marcus suggested)
> >> there is also totally-non-Squid traffic on the link being measured.
> > 
> > Come on, sure? Even in L7? Really? Cool story, bro!
> > 
> >> Antony.
> >> 
> >>> 11.03.2017 2:44, Yosi Greenfield ?????:
> >>>> Aha! That could be it. I use sslbump, but not for all users. I'll
> >>>> check that out, although I think that it's a problem even for bumped
> >>>> users. Even for bumped users we don't bump all sites, so that really
> >>>> could be it.
> >>>> 
> >>>> Thanks!
> >>>> 
> >>>> 
> >>>> -----Original Message-----
> >>>> From: squid-users [mailto:squid-users-bounces at lists.squid-cache.org]
> >>>> On Behalf Of Marcus Kool
> >>>> Sent: Friday, March 10, 2017 3:38 PM
> >>>> To: squid-users at lists.squid-cache.org
> >>>> Subject: Re: [squid-users] Data usage reported in log files
> >>>> 
> >>>> On 10/03/17 16:27, Yosi Greenfield wrote:
> >>>>> Thanks!
> >>>>> 
> >>>>> Netflow is much larger.
> >>>>> 
> >>>>> I really want to know exactly what site is costing my users data.
> >>>>> Many of our users are on metered connections and are paying for
> >>>>> overage, but I can't tell where that overage is being used. Are they
> >>>>> using youtube, webmail, wetransfer? I see only a fraction of their
> >>>>> actual proxy usage in my squid logs.
> >>>>> 
> >>>>> Data compression would give the opposite result, so that's not what
> >>>>> I'm seeing.
> >>>>> 
> >>>>> Any other ideas?
> >>>> 
> >>>> Is there any traffic that is not directed to Squid?
> >>>> 
> >>>> Do you use ssl-bump in bump mode ?
> >>>> If not, Squid has no idea how many bytes go through the (HTTPS)
> >>>> tunnels.
> >>>> 
> >>>> Marcus
> >>>> 
> >>>>> -----Original Message-----
> >>>>> From: squid-users [mailto:squid-users-bounces at lists.squid-cache.org]
> >>>>> On Behalf Of Antony Stone
> >>>>> Sent: Friday, March 10, 2017 2:21 PM
> >>>>> To: squid-users at lists.squid-cache.org
> >>>>> Subject: Re: [squid-users] Data usage reported in log files
> >>>>> 
> >>>>> On Friday 10 March 2017 at 20:14:36, Yosi Greenfield wrote:
> >>>>>> Hello all,
> >>>>>> 
> >>>>>> I'm analyzing my squid logs with sarg, and I see that the number of
> >>>>>> bytes reported as used by any particular user are often nowhere near
> >>>>>> the bytes reported by netflow and tcpdump.
> >>>>> 
> >>>>> Which is larger?
> >>>>> 
> >>>>>> I'm trying to trace my users' data usage by site, but I'm unable to
> >>>>>> do so from the log files because of this.
> >>>>> 
> >>>>> Well, what is it you really want to know?
> >>>>> 
> >>>>> netflow / tcpdump will give you accurate numbers for the quantity of
> >>>>> data on your Internet link - I assume this is what you're most
> >>>>> interested in?
> >>>>> Squid will show you what quantity of data goes to/from the clients,
> >>>>> but is that really important?
> >>>>> 
> >>>>>> Can someone please explain to me what I might be missing? Why does
> >>>>>> squid log report one thing and netflow and tcpdump show something
> >>>>>> else?
> >>>>> 
> >>>>> Data compression?
> >>>>> 
> >>>>> HTTP responses are often gzipped, so if tcpdump is showing you
> >>>>> smaller numbers of bytes than Squid reports, that's what I'd look at
> >>>>> first.
> >>>>> 
> >>>>> 
> >>>>> Antony.

-- 
<flopsie> yes, but this is #lbw, we don't do normal

                                                   Please reply to the list;
                                                         please *don't* CC me.


From yvoinov at gmail.com  Fri Mar 10 21:33:44 2017
From: yvoinov at gmail.com (Yuri Voinov)
Date: Sat, 11 Mar 2017 03:33:44 +0600
Subject: [squid-users] Data usage reported in log files
In-Reply-To: <727aa7dc-e131-c6d1-c560-dffebc83c8c8@gmail.com>
References: <DB929284116348E1AED00305A358D1F3@OhrSomayach>
 <9BEC434D6CCB4111B11C103B4BB6C279@OhrSomayach>
 <84a3effa-f50c-d574-c677-52a28125b941@gmail.com>
 <201703102157.55130.Antony.Stone@squid.open.source.it>
 <9cc68282-773a-b25b-9679-90fc6f215d65@gmail.com>
 <9f9d0e4a-1a0c-5a60-57b7-256777bf71bf@gmail.com>
 <727aa7dc-e131-c6d1-c560-dffebc83c8c8@gmail.com>
Message-ID: <08958481-09ae-c065-ca17-6a0b2f413543@gmail.com>

According to the above, NetFlow will always show much more traffic than
the SQUID. This is obvious and there is nothing to discuss here. If this
is not clear to someone, put a collector that collects statistics at the
data link level and compare the counters. I'm not just talking about
TCP, Alex. There is also the UDP. And there are a lot of protocols that
squid can not see, including for the simple reason that these packets
are never routed to a SQUID.

We have not seen the network topology and the full configuration of
network devices - what are we arguing about and guessing about?


11.03.2017 3:27, Yuri Voinov ?????:
> Think of one simple thing. Squid does not see and can not see protocols
> that do not support. What do you expect from it? Does it work on L1/L2?
> No? Then what is the discussion about?
>
>
> 11.03.2017 3:22, Yuri Voinov ?????:
>> Of course, there is no stream video from security cams, no voice IP, no
>> SIP, no torrents, no RDP, no other protocol. They simple does not exists
>> and we're all believe that's all not above over 1% of overall traffic.
>> Yes. Sure. Really.
>>
>> Only web-surfing :) Sure :)
>>
>>
>> 11.03.2017 3:19, Yuri Voinov ?????:
>>> 11.03.2017 2:57, Antony Stone ?????:
>>>> On Friday 10 March 2017 at 21:50:19, Yuri Voinov wrote:
>>>>
>>>>> Gentlemen, and it never occurred to you that there are other types of
>>>>> traffic besides HTTP / HTTPS, right?
>>>>>
>>>>> DNS, ICMP, other protocols?
>>>> I'm assuming Yosi has been measuring only TCP traffic, but even if he's been 
>>>> measuring everything, I don't think DNS, ICMP and other protocols would add 
>>>> more than 1% on top of HTTP/S, unless (as Marcus suggested) there is also 
>>>> totally-non-Squid traffic on the link being measured.
>>> Come on, sure? Even in L7? Really? Cool story, bro!
>>>> Antony.
>>>>
>>>>> 11.03.2017 2:44, Yosi Greenfield ?????:
>>>>>> Aha! That could be it. I use sslbump, but not for all users. I'll
>>>>>> check that out, although I think that it's a problem even for bumped
>>>>>> users. Even for bumped users we don't bump all sites, so that really
>>>>>> could be it.
>>>>>>
>>>>>> Thanks!
>>>>>>
>>>>>>
>>>>>> -----Original Message-----
>>>>>> From: squid-users [mailto:squid-users-bounces at lists.squid-cache.org] On
>>>>>> Behalf Of Marcus Kool
>>>>>> Sent: Friday, March 10, 2017 3:38 PM
>>>>>> To: squid-users at lists.squid-cache.org
>>>>>> Subject: Re: [squid-users] Data usage reported in log files
>>>>>>
>>>>>> On 10/03/17 16:27, Yosi Greenfield wrote:
>>>>>>> Thanks!
>>>>>>>
>>>>>>> Netflow is much larger.
>>>>>>>
>>>>>>> I really want to know exactly what site is costing my users data. Many
>>>>>>> of our users are on metered connections and are paying for overage,
>>>>>>> but I can't tell where that overage is being used. Are they using
>>>>>>> youtube, webmail, wetransfer? I see only a fraction of their actual
>>>>>>> proxy usage in my squid logs.
>>>>>>>
>>>>>>> Data compression would give the opposite result, so that's not what
>>>>>>> I'm seeing.
>>>>>>>
>>>>>>> Any other ideas?
>>>>>> Is there any traffic that is not directed to Squid?
>>>>>>
>>>>>> Do you use ssl-bump in bump mode ?
>>>>>> If not, Squid has no idea how many bytes go through the (HTTPS) tunnels.
>>>>>>
>>>>>> Marcus
>>>>>>
>>>>>>> -----Original Message-----
>>>>>>> From: squid-users [mailto:squid-users-bounces at lists.squid-cache.org]
>>>>>>> On Behalf Of Antony Stone
>>>>>>> Sent: Friday, March 10, 2017 2:21 PM
>>>>>>> To: squid-users at lists.squid-cache.org
>>>>>>> Subject: Re: [squid-users] Data usage reported in log files
>>>>>>>
>>>>>>> On Friday 10 March 2017 at 20:14:36, Yosi Greenfield wrote:
>>>>>>>> Hello all,
>>>>>>>>
>>>>>>>> I'm analyzing my squid logs with sarg, and I see that the number of
>>>>>>>> bytes reported as used by any particular user are often nowhere near
>>>>>>>> the bytes reported by netflow and tcpdump.
>>>>>>> Which is larger?
>>>>>>>
>>>>>>>> I'm trying to trace my users' data usage by site, but I'm unable to
>>>>>>>> do so from the log files because of this.
>>>>>>> Well, what is it you really want to know?
>>>>>>>
>>>>>>> netflow / tcpdump will give you accurate numbers for the quantity of
>>>>>>> data on your Internet link - I assume this is what you're most
>>>>>>> interested in?
>>>>>>> Squid will show you what quantity of data goes to/from the clients,
>>>>>>> but is that really important?
>>>>>>>
>>>>>>>> Can someone please explain to me what I might be missing? Why does
>>>>>>>> squid log report one thing and netflow and tcpdump show something
>>>>>>>> else?
>>>>>>> Data compression?
>>>>>>>
>>>>>>> HTTP responses are often gzipped, so if tcpdump is showing you smaller
>>>>>>> numbers of bytes than Squid reports, that's what I'd look at first.
>>>>>>>
>>>>>>>
>>>>>>> Antony.

-- 
Bugs to the Future
-------------- next part --------------
A non-text attachment was scrubbed...
Name: 0x613DEC46.asc
Type: application/pgp-keys
Size: 2437 bytes
Desc: not available
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20170311/ef3b8360/attachment.key>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: signature.asc
Type: application/pgp-signature
Size: 473 bytes
Desc: OpenPGP digital signature
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20170311/ef3b8360/attachment.sig>

From Antony.Stone at squid.open.source.it  Fri Mar 10 21:43:39 2017
From: Antony.Stone at squid.open.source.it (Antony Stone)
Date: Fri, 10 Mar 2017 22:43:39 +0100
Subject: [squid-users] Data usage reported in log files
In-Reply-To: <08958481-09ae-c065-ca17-6a0b2f413543@gmail.com>
References: <DB929284116348E1AED00305A358D1F3@OhrSomayach>
 <727aa7dc-e131-c6d1-c560-dffebc83c8c8@gmail.com>
 <08958481-09ae-c065-ca17-6a0b2f413543@gmail.com>
Message-ID: <201703102243.40075.Antony.Stone@squid.open.source.it>

On Friday 10 March 2017 at 22:33:44, Yuri Voinov wrote:

> We have not seen the network topology and the full configuration of
> network devices - what are we arguing about and guessing about?

Nobody is arguing, and we are guessing so that we might be helpful to Yosi who 
asked the question.

Incidentally, please could you consider putting all of your comments (which 
are unrelated to further replies from other people) into a single posting, 
instead of sending, for example, four emails to the list, each replying only 
to your own previous comment?

That would make things far easier to follow in the conversation.


Thanks,


Antony.

-- 
I thought of going into banking, until I lost interest.

                                                   Please reply to the list;
                                                         please *don't* CC me.


From ygreenfield at kewsystems.com  Fri Mar 10 21:47:27 2017
From: ygreenfield at kewsystems.com (Yosi Greenfield)
Date: Fri, 10 Mar 2017 16:47:27 -0500
Subject: [squid-users] Data usage reported in log files
In-Reply-To: <201703102231.12089.Antony.Stone@squid.open.source.it>
References: <DB929284116348E1AED00305A358D1F3@OhrSomayach>
 <9cc68282-773a-b25b-9679-90fc6f215d65@gmail.com>
 <9f9d0e4a-1a0c-5a60-57b7-256777bf71bf@gmail.com>
 <201703102231.12089.Antony.Stone@squid.open.source.it>
Message-ID: <CC4787A1219F4434B617FC8D13E12A3E@OhrSomayach>

Gentlemen,

Thanks Antony. Yes, we are accounting for everything else. I'm
talking about port 3128 and 3129 only. 

Any other traffic is being tracked both by netflow and tcpdump and
they match. What does not match is 3128/9 and squid log.

I'll report back after the weekend if the discrepancy is all
sslbump traffic.

Thank you all,
Yosi


-----Original Message-----
From: squid-users [mailto:squid-users-bounces at lists.squid-cache.org] On
Behalf Of Antony Stone
Sent: Friday, March 10, 2017 4:31 PM
To: squid-users at lists.squid-cache.org
Subject: Re: [squid-users] Data usage reported in log files

On Friday 10 March 2017 at 22:22:59, Yuri Voinov wrote:

> Of course, there is no stream video from security cams, no voice IP, 
> no SIP, no torrents, no RDP, no other protocol. They simple does not 
> exists and we're all believe that's all not above over 1% of overall
traffic.
> Yes. Sure. Really.
> 
> Only web-surfing :) Sure :)

Thanks for the standard sarcasm.

Has it occurred to you that Yosi might have been measuring traffic to & from
the IP of the Squid server, so as to ignore everything else he knows is
happening on his network, so he can compare like with like?

My "not more than 1%" was for the additional traffic to/from the Squid
server, other than HTTP/S.


Antony.

> 11.03.2017 3:19, Yuri Voinov ?????:
> > 11.03.2017 2:57, Antony Stone ?????:
> >> On Friday 10 March 2017 at 21:50:19, Yuri Voinov wrote:
> >>> Gentlemen, and it never occurred to you that there are other types of
> >>> traffic besides HTTP / HTTPS, right?
> >>> 
> >>> DNS, ICMP, other protocols?
> >> 
> >> I'm assuming Yosi has been measuring only TCP traffic, but even if he's
> >> been measuring everything, I don't think DNS, ICMP and other protocols
> >> would add more than 1% on top of HTTP/S, unless (as Marcus suggested)
> >> there is also totally-non-Squid traffic on the link being measured.
> > 
> > Come on, sure? Even in L7? Really? Cool story, bro!
> > 
> >> Antony.
> >> 
> >>> 11.03.2017 2:44, Yosi Greenfield ?????:
> >>>> Aha! That could be it. I use sslbump, but not for all users. I'll
> >>>> check that out, although I think that it's a problem even for bumped
> >>>> users. Even for bumped users we don't bump all sites, so that really
> >>>> could be it.
> >>>> 
> >>>> Thanks!
> >>>> 
> >>>> 
> >>>> -----Original Message-----
> >>>> From: squid-users [mailto:squid-users-bounces at lists.squid-cache.org]
> >>>> On Behalf Of Marcus Kool
> >>>> Sent: Friday, March 10, 2017 3:38 PM
> >>>> To: squid-users at lists.squid-cache.org
> >>>> Subject: Re: [squid-users] Data usage reported in log files
> >>>> 
> >>>> On 10/03/17 16:27, Yosi Greenfield wrote:
> >>>>> Thanks!
> >>>>> 
> >>>>> Netflow is much larger.
> >>>>> 
> >>>>> I really want to know exactly what site is costing my users data.
> >>>>> Many of our users are on metered connections and are paying for
> >>>>> overage, but I can't tell where that overage is being used. Are they
> >>>>> using youtube, webmail, wetransfer? I see only a fraction of their
> >>>>> actual proxy usage in my squid logs.
> >>>>> 
> >>>>> Data compression would give the opposite result, so that's not what
> >>>>> I'm seeing.
> >>>>> 
> >>>>> Any other ideas?
> >>>> 
> >>>> Is there any traffic that is not directed to Squid?
> >>>> 
> >>>> Do you use ssl-bump in bump mode ?
> >>>> If not, Squid has no idea how many bytes go through the (HTTPS)
> >>>> tunnels.
> >>>> 
> >>>> Marcus
> >>>> 
> >>>>> -----Original Message-----
> >>>>> From: squid-users [mailto:squid-users-bounces at lists.squid-cache.org]
> >>>>> On Behalf Of Antony Stone
> >>>>> Sent: Friday, March 10, 2017 2:21 PM
> >>>>> To: squid-users at lists.squid-cache.org
> >>>>> Subject: Re: [squid-users] Data usage reported in log files
> >>>>> 
> >>>>> On Friday 10 March 2017 at 20:14:36, Yosi Greenfield wrote:
> >>>>>> Hello all,
> >>>>>> 
> >>>>>> I'm analyzing my squid logs with sarg, and I see that the number of
> >>>>>> bytes reported as used by any particular user are often nowhere
near
> >>>>>> the bytes reported by netflow and tcpdump.
> >>>>> 
> >>>>> Which is larger?
> >>>>> 
> >>>>>> I'm trying to trace my users' data usage by site, but I'm unable to
> >>>>>> do so from the log files because of this.
> >>>>> 
> >>>>> Well, what is it you really want to know?
> >>>>> 
> >>>>> netflow / tcpdump will give you accurate numbers for the quantity of
> >>>>> data on your Internet link - I assume this is what you're most
> >>>>> interested in?
> >>>>> Squid will show you what quantity of data goes to/from the clients,
> >>>>> but is that really important?
> >>>>> 
> >>>>>> Can someone please explain to me what I might be missing? Why does
> >>>>>> squid log report one thing and netflow and tcpdump show something
> >>>>>> else?
> >>>>> 
> >>>>> Data compression?
> >>>>> 
> >>>>> HTTP responses are often gzipped, so if tcpdump is showing you
> >>>>> smaller numbers of bytes than Squid reports, that's what I'd look at
> >>>>> first.
> >>>>> 
> >>>>> 
> >>>>> Antony.

-- 
<flopsie> yes, but this is #lbw, we don't do normal

                                                   Please reply to the list;
                                                         please *don't* CC
me.
_______________________________________________
squid-users mailing list
squid-users at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-users



From yvoinov at gmail.com  Fri Mar 10 21:49:09 2017
From: yvoinov at gmail.com (Yuri Voinov)
Date: Sat, 11 Mar 2017 03:49:09 +0600
Subject: [squid-users] Data usage reported in log files
In-Reply-To: <201703102243.40075.Antony.Stone@squid.open.source.it>
References: <DB929284116348E1AED00305A358D1F3@OhrSomayach>
 <727aa7dc-e131-c6d1-c560-dffebc83c8c8@gmail.com>
 <08958481-09ae-c065-ca17-6a0b2f413543@gmail.com>
 <201703102243.40075.Antony.Stone@squid.open.source.it>
Message-ID: <ffee70a1-113a-a1e5-c77e-81d5c54de4cd@gmail.com>



11.03.2017 3:43, Antony Stone ?????:
> On Friday 10 March 2017 at 22:33:44, Yuri Voinov wrote:
>
>> We have not seen the network topology and the full configuration of
>> network devices - what are we arguing about and guessing about?
> Nobody is arguing, and we are guessing so that we might be helpful to Yosi who 
> asked the question.
Guessing can be worse than a lack of response. As they take them away
from the true picture. Especially when you do not have any facts.
>
> Incidentally, please could you consider putting all of your comments (which 
> are unrelated to further replies from other people) into a single posting, 
> instead of sending, for example, four emails to the list, each replying only 
> to your own previous comment?
>
> That would make things far easier to follow in the conversation.
I'll think about it in the future. I usually do not get into the
discussion here, except for very rare cases.
>
>
> Thanks,
>
>
> Antony.
>

-- 
Bugs to the Future
-------------- next part --------------
A non-text attachment was scrubbed...
Name: 0x613DEC46.asc
Type: application/pgp-keys
Size: 2437 bytes
Desc: not available
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20170311/fc5c244d/attachment.key>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: signature.asc
Type: application/pgp-signature
Size: 473 bytes
Desc: OpenPGP digital signature
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20170311/fc5c244d/attachment.sig>

From yvoinov at gmail.com  Fri Mar 10 21:51:43 2017
From: yvoinov at gmail.com (Yuri Voinov)
Date: Sat, 11 Mar 2017 03:51:43 +0600
Subject: [squid-users] Data usage reported in log files
In-Reply-To: <CC4787A1219F4434B617FC8D13E12A3E@OhrSomayach>
References: <DB929284116348E1AED00305A358D1F3@OhrSomayach>
 <9cc68282-773a-b25b-9679-90fc6f215d65@gmail.com>
 <9f9d0e4a-1a0c-5a60-57b7-256777bf71bf@gmail.com>
 <201703102231.12089.Antony.Stone@squid.open.source.it>
 <CC4787A1219F4434B617FC8D13E12A3E@OhrSomayach>
Message-ID: <598ef201-44e9-bd17-c8a6-2bf41fa0fa65@gmail.com>



11.03.2017 3:47, Yosi Greenfield ?????:
> Gentlemen,
>
> Thanks Antony. Yes, we are accounting for everything else. I'm
> talking about port 3128 and 3129 only. 
>
> Any other traffic is being tracked both by netflow and tcpdump and
> they match. What does not match is 3128/9 and squid log.
It can be also because of tunneled traffic.
>
> I'll report back after the weekend if the discrepancy is all
> sslbump traffic.
>
> Thank you all,
> Yosi
>
>
> -----Original Message-----
> From: squid-users [mailto:squid-users-bounces at lists.squid-cache.org] On
> Behalf Of Antony Stone
> Sent: Friday, March 10, 2017 4:31 PM
> To: squid-users at lists.squid-cache.org
> Subject: Re: [squid-users] Data usage reported in log files
>
> On Friday 10 March 2017 at 22:22:59, Yuri Voinov wrote:
>
>> Of course, there is no stream video from security cams, no voice IP, 
>> no SIP, no torrents, no RDP, no other protocol. They simple does not 
>> exists and we're all believe that's all not above over 1% of overall
> traffic.
>> Yes. Sure. Really.
>>
>> Only web-surfing :) Sure :)
> Thanks for the standard sarcasm.
>
> Has it occurred to you that Yosi might have been measuring traffic to & from
> the IP of the Squid server, so as to ignore everything else he knows is
> happening on his network, so he can compare like with like?
>
> My "not more than 1%" was for the additional traffic to/from the Squid
> server, other than HTTP/S.
>
>
> Antony.
>
>> 11.03.2017 3:19, Yuri Voinov ?????:
>>> 11.03.2017 2:57, Antony Stone ?????:
>>>> On Friday 10 March 2017 at 21:50:19, Yuri Voinov wrote:
>>>>> Gentlemen, and it never occurred to you that there are other types of
>>>>> traffic besides HTTP / HTTPS, right?
>>>>>
>>>>> DNS, ICMP, other protocols?
>>>> I'm assuming Yosi has been measuring only TCP traffic, but even if he's
>>>> been measuring everything, I don't think DNS, ICMP and other protocols
>>>> would add more than 1% on top of HTTP/S, unless (as Marcus suggested)
>>>> there is also totally-non-Squid traffic on the link being measured.
>>> Come on, sure? Even in L7? Really? Cool story, bro!
>>>
>>>> Antony.
>>>>
>>>>> 11.03.2017 2:44, Yosi Greenfield ?????:
>>>>>> Aha! That could be it. I use sslbump, but not for all users. I'll
>>>>>> check that out, although I think that it's a problem even for bumped
>>>>>> users. Even for bumped users we don't bump all sites, so that really
>>>>>> could be it.
>>>>>>
>>>>>> Thanks!
>>>>>>
>>>>>>
>>>>>> -----Original Message-----
>>>>>> From: squid-users [mailto:squid-users-bounces at lists.squid-cache.org]
>>>>>> On Behalf Of Marcus Kool
>>>>>> Sent: Friday, March 10, 2017 3:38 PM
>>>>>> To: squid-users at lists.squid-cache.org
>>>>>> Subject: Re: [squid-users] Data usage reported in log files
>>>>>>
>>>>>> On 10/03/17 16:27, Yosi Greenfield wrote:
>>>>>>> Thanks!
>>>>>>>
>>>>>>> Netflow is much larger.
>>>>>>>
>>>>>>> I really want to know exactly what site is costing my users data.
>>>>>>> Many of our users are on metered connections and are paying for
>>>>>>> overage, but I can't tell where that overage is being used. Are they
>>>>>>> using youtube, webmail, wetransfer? I see only a fraction of their
>>>>>>> actual proxy usage in my squid logs.
>>>>>>>
>>>>>>> Data compression would give the opposite result, so that's not what
>>>>>>> I'm seeing.
>>>>>>>
>>>>>>> Any other ideas?
>>>>>> Is there any traffic that is not directed to Squid?
>>>>>>
>>>>>> Do you use ssl-bump in bump mode ?
>>>>>> If not, Squid has no idea how many bytes go through the (HTTPS)
>>>>>> tunnels.
>>>>>>
>>>>>> Marcus
>>>>>>
>>>>>>> -----Original Message-----
>>>>>>> From: squid-users [mailto:squid-users-bounces at lists.squid-cache.org]
>>>>>>> On Behalf Of Antony Stone
>>>>>>> Sent: Friday, March 10, 2017 2:21 PM
>>>>>>> To: squid-users at lists.squid-cache.org
>>>>>>> Subject: Re: [squid-users] Data usage reported in log files
>>>>>>>
>>>>>>> On Friday 10 March 2017 at 20:14:36, Yosi Greenfield wrote:
>>>>>>>> Hello all,
>>>>>>>>
>>>>>>>> I'm analyzing my squid logs with sarg, and I see that the number of
>>>>>>>> bytes reported as used by any particular user are often nowhere
> near
>>>>>>>> the bytes reported by netflow and tcpdump.
>>>>>>> Which is larger?
>>>>>>>
>>>>>>>> I'm trying to trace my users' data usage by site, but I'm unable to
>>>>>>>> do so from the log files because of this.
>>>>>>> Well, what is it you really want to know?
>>>>>>>
>>>>>>> netflow / tcpdump will give you accurate numbers for the quantity of
>>>>>>> data on your Internet link - I assume this is what you're most
>>>>>>> interested in?
>>>>>>> Squid will show you what quantity of data goes to/from the clients,
>>>>>>> but is that really important?
>>>>>>>
>>>>>>>> Can someone please explain to me what I might be missing? Why does
>>>>>>>> squid log report one thing and netflow and tcpdump show something
>>>>>>>> else?
>>>>>>> Data compression?
>>>>>>>
>>>>>>> HTTP responses are often gzipped, so if tcpdump is showing you
>>>>>>> smaller numbers of bytes than Squid reports, that's what I'd look at
>>>>>>> first.
>>>>>>>
>>>>>>>
>>>>>>> Antony.

-- 
Bugs to the Future
-------------- next part --------------
A non-text attachment was scrubbed...
Name: 0x613DEC46.asc
Type: application/pgp-keys
Size: 2437 bytes
Desc: not available
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20170311/f69fef7f/attachment.key>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: signature.asc
Type: application/pgp-signature
Size: 473 bytes
Desc: OpenPGP digital signature
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20170311/f69fef7f/attachment.sig>

From ahmed.zaeem at netstream.ps  Sat Mar 11 17:42:39 2017
From: ahmed.zaeem at netstream.ps (--Ahmad--)
Date: Sat, 11 Mar 2017 19:42:39 +0200
Subject: [squid-users] squid 3.5.2==> HTTPS FATAL: The ssl_crtd helpers
	are crashing too rapidly, need help!
In-Reply-To: <BF13D4F4-ABE5-4A60-87C6-11FB66A02686@gmail.com>
References: <592CB65D-536B-40F8-B887-12127542DCB5@netstream.ps>
 <38a07e38-7347-313a-b27f-08fd7eddfa3f@gmail.com>
 <97215197-78fc-e987-9a9d-51a697010e7b@gmail.com>
 <714528e5-a6d5-e72a-2bc7-9950a8eecb73@gmail.com>
 <7c1d01d293b5$9b22d2c0$d1687840$@ngtech.co.il>
 <a5a63820-0afd-563b-f627-3819733a5680@gmail.com>
 <4F85658B-A32E-4D74-98E6-E1CEE90168EA@netstream.ps>
 <7c6801d29420$9481bb00$bd853100$@ngtech.co.il>
 <b6eee0af-ea58-105b-f6f6-dd6f25c58277@measurement-factory.com>
 <96d301d2950d$e83beac0$b8b3c040$@ngtech.co.il>
 <BF13D4F4-ABE5-4A60-87C6-11FB66A02686@gmail.com>
Message-ID: <B7763EFA-39CF-45F1-8EC5-C6E6B4616B68@netstream.ps>

hello Guys i do confirm that the issue resolved on the version 3.5.24

thanks all 

> On Mar 6, 2017, at 4:33 PM, Guy Helmer <guy.helmer at gmail.com> wrote:
> 
> Hi, all,
> 
> A couple of years ago, I wrote a perl script that ran a specified number of ssl_crtd processes with simultaneous requests to expose the problem and test the resolution. I?ve attached it below in case it would help test/diagnose the situation. It has hard-coded paths at the top of the script that would need to be updated for any particular test environment, and its testing certificate directory would need to be prepared in advance just as for a regular instance of squid running ssl_crtd.
> 
> I initially investigated the problem and helped improve the database file locking problem by changing the locking protocol from lockf() to flock(). I haven?t seen the problem occur on FreeBSD since the flock() changes went in. However, when I last looked at the code, some operating systems were still configured to use lockf() locking which does not function as needed due to its unintuitive POSIX / X/Open semantics: the first close() on the locked file (even via a different file descriptor) releases the lock. MacOS?s man page notes "File locks are released on first close by the locking process of any file descriptor for the file?.  There are situations in the ssl_crtd code where the database file is open using multiple file descriptors simultaneously, and close() calls occur that would cause read/write hazards due to loss of the lockf() lock.
> 
> My $0.02,
> Guy
> 
> 
>> On Mar 4, 2017, at 11:36 AM, Eliezer Croitoru <eliezer at ngtech.co.il> wrote:
>> 
>> Hey Alex,
>> 
>> I still believe that an upgrade will improve since it's hard for me to imagine that only two admins are having trouble with it.
>> What will scare me is that If indeed many admins are having such issues and they do not ask or report about these.
>> I hope that with this thread we will be one step smarter and one step closer to a satisfying solution rather then the currently used options.
>> 
>> Eliezer
>> 
>> ----
>> Eliezer Croitoru
>> Linux System Administrator
>> Mobile: +972-5-28704261
>> Email: eliezer at ngtech.co.il
>> 
>> 
>> -----Original Message-----
>> From: Alex Rousskov [mailto:rousskov at measurement-factory.com] 
>> Sent: Friday, March 3, 2017 5:56 PM
>> To: squid-users at lists.squid-cache.org
>> Cc: Eliezer Croitoru <eliezer at ngtech.co.il>
>> Subject: Re: [squid-users] squid 3.5.2==> HTTPS FATAL: The ssl_crtd helpers are crashing too rapidly, need help!
>> 
>> On 03/03/2017 06:17 AM, Eliezer Croitoru wrote:
>> 
>>> one of the options is to fence the ssl_crtd with some kind of lock
>>> mechanism for the DB rebuild time.
>> 
>> ssl_crtd already has a lock mechanism. If that mechanism is buggy, it
>> needs to be fixed, but it does not make sense to add another one.
>> 
>> There is still a lot of room for improvements, of course. For example,
>> compared to a log-monitoring watchdog mentioned by Yuri:
>> 
>> * Teaching ssl_crtd to run a sysadmin-provided script on database
>> failures will allow the sysadmin to handle such failures almost
>> transparently to the users and may reduce configuration headaches
>> associated with ssl_crtd message text changes.
>> 
>> * Teaching Squid to run a sysadmin-provided script on helper crashes
>> will allow the sysadmin to handle such failures more transparently to
>> the users and may reduce configuration headaches associated with helper
>> message text changes.
>> 
>> 
>> Alex.
>> 
>> 
>> _______________________________________________
>> squid-users mailing list
>> squid-users at lists.squid-cache.org
>> http://lists.squid-cache.org/listinfo/squid-users
> 
> <stress-sslcrtd.perl>
> 
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users



From eliezer at ngtech.co.il  Sat Mar 11 20:39:10 2017
From: eliezer at ngtech.co.il (Eliezer  Croitoru)
Date: Sat, 11 Mar 2017 22:39:10 +0200
Subject: [squid-users] debugging squid memory
In-Reply-To: <20170310180005.GA5196@fantomas.sk>
References: <20170310180005.GA5196@fantomas.sk>
Message-ID: <024701d29aa7$89bbd510$9d337f30$@ngtech.co.il>

Hey,

Just asking loud, is there any chance you will give run an upgrade from 3.4.8 to 3.5.24?

Eliezer

----
Eliezer Croitoru
Linux System Administrator
Mobile: +972-5-28704261
Email: eliezer at ngtech.co.il


-----Original Message-----
From: squid-users [mailto:squid-users-bounces at lists.squid-cache.org] On Behalf Of Matus UHLAR - fantomas
Sent: Friday, March 10, 2017 8:00 PM
To: squid-users at squid-cache.org
Subject: [squid-users] debugging squid memory

Hello,

as I have already noted in other thread, I seem to have memory leak in squid
3.4.8 (debian 8 jessie) version, only memory cache used now.

Looking at cacti graphs, seems that memory usage grows by half-hour or hour
jumps. Is there any job done periodically in squid?


and if there's nothing like that, and I'd like to debug squid memory usage
to see, if there's a leak:
are there any useful hints (including: don't do it)?

- which debug_options to use (13, 19, 59, 63)?

- can I find out anything by squidclient or cachemgr.cgi
   (looking at vm_objects didn't found anything useful).

- any hint what to search for in logs?

Thanks.

-- 
Matus UHLAR - fantomas, uhlar at fantomas.sk ; http://www.fantomas.sk/
Warning: I wish NOT to receive e-mail advertising to this address.
Varovanie: na tuto adresu chcem NEDOSTAVAT akukolvek reklamnu postu.
REALITY.SYS corrupted. Press any key to reboot Universe.
_______________________________________________
squid-users mailing list
squid-users at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-users



From eliezer at ngtech.co.il  Sat Mar 11 20:48:19 2017
From: eliezer at ngtech.co.il (Eliezer  Croitoru)
Date: Sat, 11 Mar 2017 22:48:19 +0200
Subject: [squid-users] Data usage reported in log files
In-Reply-To: <CC4787A1219F4434B617FC8D13E12A3E@OhrSomayach>
References: <DB929284116348E1AED00305A358D1F3@OhrSomayach>
 <9cc68282-773a-b25b-9679-90fc6f215d65@gmail.com>
 <9f9d0e4a-1a0c-5a60-57b7-256777bf71bf@gmail.com>
 <201703102231.12089.Antony.Stone@squid.open.source.it>
 <CC4787A1219F4434B617FC8D13E12A3E@OhrSomayach>
Message-ID: <024901d29aa8$d0a36c80$71ea4580$@ngtech.co.il>

Hey Yosi,

Can you see if the differences is on the incoming or outgoing traffic?
Squid will only account for incoming and if you are using some kind of caching with the quick_abort and other partial content prefetch it would make sense that the actual consumption of the bits from the Internet to squid will not match from squid to clients.

If you can send me or share with others your squid.conf we might be able to understand if something there might cause such an issue.

Thanks,
Eliezer

* Feel free to contact me directly by skype or phone
----
Eliezer Croitoru
Linux System Administrator
Mobile: +972-5-28704261
Email: eliezer at ngtech.co.il


-----Original Message-----
From: squid-users [mailto:squid-users-bounces at lists.squid-cache.org] On Behalf Of Yosi Greenfield
Sent: Friday, March 10, 2017 11:47 PM
To: squid-users at lists.squid-cache.org
Subject: Re: [squid-users] Data usage reported in log files

Gentlemen,

Thanks Antony. Yes, we are accounting for everything else. I'm talking about port 3128 and 3129 only. 

Any other traffic is being tracked both by netflow and tcpdump and they match. What does not match is 3128/9 and squid log.

I'll report back after the weekend if the discrepancy is all sslbump traffic.

Thank you all,
Yosi


-----Original Message-----
From: squid-users [mailto:squid-users-bounces at lists.squid-cache.org] On Behalf Of Antony Stone
Sent: Friday, March 10, 2017 4:31 PM
To: squid-users at lists.squid-cache.org
Subject: Re: [squid-users] Data usage reported in log files

On Friday 10 March 2017 at 22:22:59, Yuri Voinov wrote:

> Of course, there is no stream video from security cams, no voice IP, 
> no SIP, no torrents, no RDP, no other protocol. They simple does not 
> exists and we're all believe that's all not above over 1% of overall
traffic.
> Yes. Sure. Really.
> 
> Only web-surfing :) Sure :)

Thanks for the standard sarcasm.

Has it occurred to you that Yosi might have been measuring traffic to & from the IP of the Squid server, so as to ignore everything else he knows is happening on his network, so he can compare like with like?

My "not more than 1%" was for the additional traffic to/from the Squid server, other than HTTP/S.


Antony.

> 11.03.2017 3:19, Yuri Voinov ?????:
> > 11.03.2017 2:57, Antony Stone ?????:
> >> On Friday 10 March 2017 at 21:50:19, Yuri Voinov wrote:
> >>> Gentlemen, and it never occurred to you that there are other types 
> >>> of traffic besides HTTP / HTTPS, right?
> >>> 
> >>> DNS, ICMP, other protocols?
> >> 
> >> I'm assuming Yosi has been measuring only TCP traffic, but even if 
> >> he's been measuring everything, I don't think DNS, ICMP and other 
> >> protocols would add more than 1% on top of HTTP/S, unless (as 
> >> Marcus suggested) there is also totally-non-Squid traffic on the link being measured.
> > 
> > Come on, sure? Even in L7? Really? Cool story, bro!
> > 
> >> Antony.
> >> 
> >>> 11.03.2017 2:44, Yosi Greenfield ?????:
> >>>> Aha! That could be it. I use sslbump, but not for all users. I'll 
> >>>> check that out, although I think that it's a problem even for 
> >>>> bumped users. Even for bumped users we don't bump all sites, so 
> >>>> that really could be it.
> >>>> 
> >>>> Thanks!
> >>>> 
> >>>> 
> >>>> -----Original Message-----
> >>>> From: squid-users 
> >>>> [mailto:squid-users-bounces at lists.squid-cache.org]
> >>>> On Behalf Of Marcus Kool
> >>>> Sent: Friday, March 10, 2017 3:38 PM
> >>>> To: squid-users at lists.squid-cache.org
> >>>> Subject: Re: [squid-users] Data usage reported in log files
> >>>> 
> >>>> On 10/03/17 16:27, Yosi Greenfield wrote:
> >>>>> Thanks!
> >>>>> 
> >>>>> Netflow is much larger.
> >>>>> 
> >>>>> I really want to know exactly what site is costing my users data.
> >>>>> Many of our users are on metered connections and are paying for 
> >>>>> overage, but I can't tell where that overage is being used. Are 
> >>>>> they using youtube, webmail, wetransfer? I see only a fraction 
> >>>>> of their actual proxy usage in my squid logs.
> >>>>> 
> >>>>> Data compression would give the opposite result, so that's not 
> >>>>> what I'm seeing.
> >>>>> 
> >>>>> Any other ideas?
> >>>> 
> >>>> Is there any traffic that is not directed to Squid?
> >>>> 
> >>>> Do you use ssl-bump in bump mode ?
> >>>> If not, Squid has no idea how many bytes go through the (HTTPS) 
> >>>> tunnels.
> >>>> 
> >>>> Marcus
> >>>> 
> >>>>> -----Original Message-----
> >>>>> From: squid-users 
> >>>>> [mailto:squid-users-bounces at lists.squid-cache.org]
> >>>>> On Behalf Of Antony Stone
> >>>>> Sent: Friday, March 10, 2017 2:21 PM
> >>>>> To: squid-users at lists.squid-cache.org
> >>>>> Subject: Re: [squid-users] Data usage reported in log files
> >>>>> 
> >>>>> On Friday 10 March 2017 at 20:14:36, Yosi Greenfield wrote:
> >>>>>> Hello all,
> >>>>>> 
> >>>>>> I'm analyzing my squid logs with sarg, and I see that the 
> >>>>>> number of bytes reported as used by any particular user are 
> >>>>>> often nowhere
near
> >>>>>> the bytes reported by netflow and tcpdump.
> >>>>> 
> >>>>> Which is larger?
> >>>>> 
> >>>>>> I'm trying to trace my users' data usage by site, but I'm 
> >>>>>> unable to do so from the log files because of this.
> >>>>> 
> >>>>> Well, what is it you really want to know?
> >>>>> 
> >>>>> netflow / tcpdump will give you accurate numbers for the 
> >>>>> quantity of data on your Internet link - I assume this is what 
> >>>>> you're most interested in?
> >>>>> Squid will show you what quantity of data goes to/from the 
> >>>>> clients, but is that really important?
> >>>>> 
> >>>>>> Can someone please explain to me what I might be missing? Why 
> >>>>>> does squid log report one thing and netflow and tcpdump show 
> >>>>>> something else?
> >>>>> 
> >>>>> Data compression?
> >>>>> 
> >>>>> HTTP responses are often gzipped, so if tcpdump is showing you 
> >>>>> smaller numbers of bytes than Squid reports, that's what I'd 
> >>>>> look at first.
> >>>>> 
> >>>>> 
> >>>>> Antony.

--
<flopsie> yes, but this is #lbw, we don't do normal

                                                   Please reply to the list;
                                                         please *don't* CC me.
_______________________________________________
squid-users mailing list
squid-users at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-users

_______________________________________________
squid-users mailing list
squid-users at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-users



From eliezer at ngtech.co.il  Sat Mar 11 20:53:43 2017
From: eliezer at ngtech.co.il (Eliezer  Croitoru)
Date: Sat, 11 Mar 2017 22:53:43 +0200
Subject: [squid-users] squid-4.0.18 error when running
In-Reply-To: <CAHcF_0Z+N7a=EtyL+VmRVdYX8KbYNzmXY3KC07+eh3NsJNWLaw@mail.gmail.com>
References: <CAHcF_0Z+N7a=EtyL+VmRVdYX8KbYNzmXY3KC07+eh3NsJNWLaw@mail.gmail.com>
Message-ID: <024b01d29aa9$9234f3a0$b69edae0$@ngtech.co.il>

What OS are you using?

----
http://ngtech.co.il/lmgtfy/
Linux System Administrator
Mobile: +972-5-28704261
Email: eliezer at ngtech.co.il


From: squid-users [mailto:squid-users-bounces at lists.squid-cache.org] On Behalf Of sothy shan
Sent: Tuesday, March 7, 2017 4:06 PM
To: squid-users at lists.squid-cache.org
Subject: [squid-users] squid-4.0.18 error when running

Hello,
I am trying to run <<sudo squid -d 2 -N -X >>, I got the followng error.

Why is this error ? Thanks for your help.


Best regards
Sothy

2017/03/07 15:02:48.221| 1,2| main.cc(1568) SquidMain: running RegisteredRunner::finalizeConfig
2017/03/07 15:02:48.221| 1,2| main.cc(1569) SquidMain: running RegisteredRunner::claimMemoryNeeds
2017/03/07 15:02:48.221| 1,2| main.cc(1570) SquidMain: running RegisteredRunner::useConfig
2017/03/07 15:02:48.221| Not currently OK to rewrite swap log.
2017/03/07 15:02:48.221| storeDirWriteCleanLogs: Operation aborted.
FATAL: Ipc::Mem::Segment::create failed to shm_open(/squid-cf__metadata.shm): (17) File exists
>




From eliezer at ngtech.co.il  Sat Mar 11 20:54:44 2017
From: eliezer at ngtech.co.il (Eliezer  Croitoru)
Date: Sat, 11 Mar 2017 22:54:44 +0200
Subject: [squid-users] cachemgr CGI version compatibility
In-Reply-To: <e40602b9-8f31-acac-dabb-5c3181fdff9b@treenet.co.nz>
References: <20170309143223.GB30976@fantomas.sk>
 <e40602b9-8f31-acac-dabb-5c3181fdff9b@treenet.co.nz>
Message-ID: <024d01d29aa9$b64d3770$22e7a650$@ngtech.co.il>

The title of the email was:
"squid-4.0.18 error when running"

Eliezer

----
Eliezer Croitoru
Linux System Administrator
Mobile: +972-5-28704261
Email: eliezer at ngtech.co.il


-----Original Message-----
From: squid-users [mailto:squid-users-bounces at lists.squid-cache.org] On Behalf Of Amos Jeffries
Sent: Thursday, March 9, 2017 5:15 PM
To: squid-users at lists.squid-cache.org
Subject: Re: [squid-users] cachemgr CGI version compatibility

On 10/03/2017 3:32 a.m., Matus UHLAR - fantomas wrote:
> Hello,
> 
> will older cachemgr.cgi work well with newer squid?
> 

Yes they should. Likewise the newer cachemgr.cgi should work as well with older Squid. The tool and Squid are explicitly being kept both forward and backward compatible.


But be aware that cachemgr.cgi older than 3.5.17 may be vulnerable to
<http://www.squid-cache.org/Advisories/SQUID-2016_5.txt>- which means they cannot safely handle some reports (as listed in the advisory).

And if you are talking *very* old CGI version maybe <http://www.squid-cache.org/Advisories/SQUID-2012_1.txt> as well, which is somewhat worse.

Amos

_______________________________________________
squid-users mailing list
squid-users at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-users



From mynixmail at gmail.com  Sun Mar 12 07:11:46 2017
From: mynixmail at gmail.com (Danny)
Date: Sun, 12 Mar 2017 09:11:46 +0200
Subject: [squid-users] reply_body_max_size question
Message-ID: <20170312071146.GA3794@fever.havannah.local>

Hi,

Just want someone to confirm my current reply_body_max_size setup. I have a
simple network at home i.e: Debian with a wireless card (wlan0) which is bridged
(br0) to an ethernet card (eth0). All devices comes through the wireless card
(wlan0) and then of to the router.

I want "localnet_sniper localnet_bridge localnet_fever localnet_44081 localnet_dannyS4" to have unlimited download capabilty but
"localnet_vS5mini localnet_anTab2 localnet_vTab3 localnet_samsungTV localnet_samsungDVD localnet_dhcp" must be limited to a
5MB download limit.

Here is my configuration:
######################################################################################################################################
acl localnet src 10.0.0.0/24	# RFC1918 possible internal network
acl localnet_sniper src 10.0.0.3        #(eth0)
acl localnet_bridge src 10.0.0.4        #(br0)
acl localnet_fever src 10.0.0.5         #(wlan0)
acl localnet_44081 src 10.0.0.11        #(RaspberryPi3)
acl localnet_dannyS4 src 10.0.0.54
acl localnet_vS5mini src 10.0.0.55
acl localnet_shotgun src 10.0.0.56
acl localnet_anTab2 src 10.0.0.71
acl localnet_vTab3 src 10.0.0.73
acl localnet_samsungTV src 10.0.0.80
acl localnet_samsungDVD src 10.0.0.81
acl localnet_dhcp src 10.0.0.201
acl localnet_dhcp src 10.0.0.202
acl localnet_dhcp src 10.0.0.203
acl localnet_dhcp src 10.0.0.204

http_access allow password
http_access allow localhost
http_access allow localnet
http_access allow localnet_sniper
http_access allow localnet_bridge
http_access allow localnet_fever
http_access allow localnet_44081
http_access allow localnet_dannyS4
http_access allow localnet_vS5mini
http_access allow localnet_anTab2
http_access allow localnet_vTab3
http_access allow localnet_samsungTV
http_access allow localnet_samsungDVD
http_access allow localnet_dhcp

reply_body_max_size 9999999999 MB localnet_sniper localnet_bridge localnet_fever localnet_44081 localnet_dannyS4
reply_body_max_size 5 MB localnet_vS5mini localnet_anTab2 localnet_vTab3 localnet_samsungTV localnet_samsungDVD localnet_dhcp

url_rewrite_program /usr/bin/squidGuard -c /etc/squidguard/squidGuard.conf
redirect_program /usr/bin/squidGuard -c /etc/squidguard/squidGuard.conf
#######################################################################################################################################

Any help will be greatly appreciated.

Thank you

Danny


From uhlar at fantomas.sk  Sun Mar 12 15:33:27 2017
From: uhlar at fantomas.sk (Matus UHLAR - fantomas)
Date: Sun, 12 Mar 2017 16:33:27 +0100
Subject: [squid-users] cachemgr CGI version compatibility
In-Reply-To: <024d01d29aa9$b64d3770$22e7a650$@ngtech.co.il>
References: <20170309143223.GB30976@fantomas.sk>
 <e40602b9-8f31-acac-dabb-5c3181fdff9b@treenet.co.nz>
 <024d01d29aa9$b64d3770$22e7a650$@ngtech.co.il>
Message-ID: <20170312153327.GB27151@fantomas.sk>

On 11.03.17 22:54, Eliezer  Croitoru wrote:
>The title of the email was:
>"squid-4.0.18 error when running"

no, it was not, you mistook my email for someone else's

>On 10/03/2017 3:32 a.m., Matus UHLAR - fantomas wrote:
>> will older cachemgr.cgi work well with newer squid?

>Yes they should. Likewise the newer cachemgr.cgi should work as well with
> older Squid.  The tool and Squid are explicitly being kept both forward
> and backward compatible.

I'm happy to know that.

>But be aware that cachemgr.cgi older than 3.5.17 may be vulnerable to
><http://www.squid-cache.org/Advisories/SQUID-2016_5.txt>- which means they
>cannot safely handle some reports (as listed in the advisory).

luckily debian people take care of that:

squid3 (3.1.20-2.2+deb7u6) wheezy-security; urgency=medium

   * squid31-CVE-2016-4051-cachemgr-MemBuf.patch: make cachemgr use MemBuf.

>And if you are talking *very* old CGI version maybe
> <http://www.squid-cache.org/Advisories/SQUID-2012_1.txt> as well, which is
> somewhat worse.

squid3 (3.1.20-2.2) unstable; urgency=low

   * Non-maintainer upload.
   * Add fix-701123-regression-in-cachemgr.patch patch.
     Fix missing bits in the fix for CVE-2012-5643 and CVE-2013-0189 causing


...those are good reasons to use distribution with security updates
thanks for warnings anyway

-- 
Matus UHLAR - fantomas, uhlar at fantomas.sk ; http://www.fantomas.sk/
Warning: I wish NOT to receive e-mail advertising to this address.
Varovanie: na tuto adresu chcem NEDOSTAVAT akukolvek reklamnu postu.
Christian Science Programming: "Let God Debug It!".


From uhlar at fantomas.sk  Sun Mar 12 15:35:01 2017
From: uhlar at fantomas.sk (Matus UHLAR - fantomas)
Date: Sun, 12 Mar 2017 16:35:01 +0100
Subject: [squid-users] debugging squid memory
In-Reply-To: <024701d29aa7$89bbd510$9d337f30$@ngtech.co.il>
References: <20170310180005.GA5196@fantomas.sk>
 <024701d29aa7$89bbd510$9d337f30$@ngtech.co.il>
Message-ID: <20170312153501.GC27151@fantomas.sk>

On 11.03.17 22:39, Eliezer  Croitoru wrote:
>Just asking loud, is there any chance you will give run an upgrade from 3.4.8 to 3.5.24?

yes, but I will have to take care of that - debian people take care of the
version in debian.

I prefer fixing things in debian and push fixes back to distribution.

>as I have already noted in other thread, I seem to have memory leak in squid
>3.4.8 (debian 8 jessie) version, only memory cache used now.

-- 
Matus UHLAR - fantomas, uhlar at fantomas.sk ; http://www.fantomas.sk/
Warning: I wish NOT to receive e-mail advertising to this address.
Varovanie: na tuto adresu chcem NEDOSTAVAT akukolvek reklamnu postu.
(R)etry, (A)bort, (C)ancer


From uhlar at fantomas.sk  Sun Mar 12 15:35:01 2017
From: uhlar at fantomas.sk (Matus UHLAR - fantomas)
Date: Sun, 12 Mar 2017 16:35:01 +0100
Subject: [squid-users] debugging squid memory
In-Reply-To: <024701d29aa7$89bbd510$9d337f30$@ngtech.co.il>
References: <20170310180005.GA5196@fantomas.sk>
 <024701d29aa7$89bbd510$9d337f30$@ngtech.co.il>
Message-ID: <20170312153501.GC27151@fantomas.sk>

On 11.03.17 22:39, Eliezer  Croitoru wrote:
>Just asking loud, is there any chance you will give run an upgrade from 3.4.8 to 3.5.24?

yes, but I will have to take care of that - debian people take care of the
version in debian.

I prefer fixing things in debian and push fixes back to distribution.

>as I have already noted in other thread, I seem to have memory leak in squid
>3.4.8 (debian 8 jessie) version, only memory cache used now.

-- 
Matus UHLAR - fantomas, uhlar at fantomas.sk ; http://www.fantomas.sk/
Warning: I wish NOT to receive e-mail advertising to this address.
Varovanie: na tuto adresu chcem NEDOSTAVAT akukolvek reklamnu postu.
(R)etry, (A)bort, (C)ancer


From harishmeetsu at yahoo.com  Sun Mar 12 17:05:59 2017
From: harishmeetsu at yahoo.com (S V Hareesh)
Date: Sun, 12 Mar 2017 17:05:59 +0000 (UTC)
Subject: [squid-users] Setting Up Squid - my scenario
In-Reply-To: <209881710.2578283.1489080628327@mail.yahoo.com>
References: <1190296055.2535835.1489076385522.ref@mail.yahoo.com>
 <1190296055.2535835.1489076385522@mail.yahoo.com>
 <b4a65caf-0332-5a56-915a-dbffcbe07fc7@treenet.co.nz>
 <821797722.2515523.1489078809096@mail.yahoo.com>
 <3b7d2a8e-c915-59a4-b6c5-759b80b9a289@treenet.co.nz>
 <209881710.2578283.1489080628327@mail.yahoo.com>
Message-ID: <94682844.4059124.1489338359749@mail.yahoo.com>

I have put login=PASS and configured Squid service to run with a service account that has internet account, it started working.?CheersS.V.Hareeshhttp://svhareesh.blogspot.com?Say no to plastic & pollution.... go environment friendly 

    On Thursday, 9 March 2017 5:30 PM, S V Hareesh <harishmeetsu at yahoo.com> wrote:
 

 Here is the conf file.
http_port 3128cache_peer <corp_proxy_IP> parent <Parent_proxy_port> 0 no-query default proxy-only login=NEGOTIATE?
acl manager proto cache_objectacl localhost src 127.0.0.1/255.255.255.255acl all src 0.0.0.0/0.0.0.0http_access allow allnever_direct allow allicp_access deny all
cache_effective_user squid#cache_effective_group wheel
max_filedescriptors 3200?I am not sure what is it that I am missing. I have been wire sharking both the resultant packets when using CNTLM and when using squid, I can see the proxy auth header piece in the Squid packet missing.
CheersS.V.Hareeshhttp://svhareesh.blogspot.com?
 

    On Thursday, 9 March 2017 5:20 PM, Amos Jeffries <squid3 at treenet.co.nz> wrote:
 

 On 10/03/2017 6:00 a.m., S V Hareesh wrote:
> Ok, I tried that but it didnt work. I can put a conf file here. As a
> start I am currently in a scenario trying to replace a simple CNTLM
> HTTP proxy with Squid. I want to configure one account which
> authenticates with parent proxy send the downstream requests with out
> taking any creds.

If your CNTLM was running on the Squid machine and using the credentials
for the service account you have setup Squid to use now - then the
cache_peer login=NEGOTIATE should make Squid operate as equivalent to
what CNTLM was doing.

The config file would be useful for anyone who follows up (not just me,
who will be out of time shortly for another few days).

Also, if you can track what HTTP messages are happening and whether the
Kerberos is working properly for the Squid->parent messages it would be
useful.

The current Squid can provide HTTP details in cache.log with
"debug_options 11,2". I'm not sure how you would test the Kerberos on a
Windows installation, but the Negotiate auth headers in those messages
might give a few clues anyway.

HTH
Amos


>? On Thu, Mar 9, 2017 at 4:50 PM, Amos Jeffrieswrote:
>>? ? On 10/03/2017 5:19 a.m., S V Hareesh wrote:
>> On top of the conf file from default setup on Windows, I added the following line in the conf. I added the dns servers and allowed localhost.
>>
>> cache_peer <corporate_proxy> parent 80 0 default connection-auth=on proxy-only
>>
>> never_direct allow all
>>
>> When I point my browser to this proxy, it gives me 407, auth required. 
>>
>> Also, configured squid service on windows to run with a service account that has access to Internet/corp proxy.
> 
> Squid cannot authenticate to a cache_peer using NTLM. It can only do
> Nagotiate/Kerberos to the parent proxy, and only when "login=NEGOTIATE"
> is added (with or without a named keytab file).
> 
> NOTE: 'connection-auth=on' is about allowing the browser to use NTLM or
> Negotiate/Kerberos through the cache_peer. It needs to also have
> "login=PASSTHRU" if that peer is a proxy (as opposed to a web or
> Exchange server).
> 
> See the 'AUTHENTICATION OPTIONS' section of
> <http://www.squid-cache.org/Doc/config/cache_peer/>
> 
> Amos
> 
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>? 
> 



   

   
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20170312/7e8b0831/attachment.htm>

From eliezer at ngtech.co.il  Mon Mar 13 07:56:12 2017
From: eliezer at ngtech.co.il (Eliezer  Croitoru)
Date: Mon, 13 Mar 2017 09:56:12 +0200
Subject: [squid-users] debugging squid memory
In-Reply-To: <20170312153501.GC27151@fantomas.sk>
References: <20170310180005.GA5196@fantomas.sk>
 <024701d29aa7$89bbd510$9d337f30$@ngtech.co.il>
 <20170312153501.GC27151@fantomas.sk>
Message-ID: <02cf01d29bcf$4c5944e0$e50bcea0$@ngtech.co.il>

Hey,

I am not maintain yet  Debian and Ubuntu Squid-Cache packaging but I started to do so as a testing step towards the next one.
My squid debian based packages are not including ecap support due to the overhead but I believe that diladele provides up-to-date Ubuntu and Maybe debian packages.

I understand that you prefer Debian to push fixes into their builds but I am not deep enough in the Debian world to tell you I understand it all goes there and what affects patches being pushed into the stable build.
Would you consider to use an external debian packge that follows squid stable releases?

Thanks,
Eliezer

----
Eliezer Croitoru
Linux System Administrator
Mobile: +972-5-28704261
Email: eliezer at ngtech.co.il


-----Original Message-----
From: squid-users [mailto:squid-users-bounces at lists.squid-cache.org] On Behalf Of Matus UHLAR - fantomas
Sent: Sunday, March 12, 2017 5:35 PM
To: squid-users at lists.squid-cache.org; squid-users at squid-cache.org
Subject: Re: [squid-users] debugging squid memory

On 11.03.17 22:39, Eliezer  Croitoru wrote:
>Just asking loud, is there any chance you will give run an upgrade from 3.4.8 to 3.5.24?

yes, but I will have to take care of that - debian people take care of the
version in debian.

I prefer fixing things in debian and push fixes back to distribution.

>as I have already noted in other thread, I seem to have memory leak in squid
>3.4.8 (debian 8 jessie) version, only memory cache used now.

-- 
Matus UHLAR - fantomas, uhlar at fantomas.sk ; http://www.fantomas.sk/
Warning: I wish NOT to receive e-mail advertising to this address.
Varovanie: na tuto adresu chcem NEDOSTAVAT akukolvek reklamnu postu.
(R)etry, (A)bort, (C)ancer
_______________________________________________
squid-users mailing list
squid-users at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-users



From eliezer at ngtech.co.il  Mon Mar 13 07:56:12 2017
From: eliezer at ngtech.co.il (Eliezer  Croitoru)
Date: Mon, 13 Mar 2017 09:56:12 +0200
Subject: [squid-users] debugging squid memory
In-Reply-To: <20170312153501.GC27151@fantomas.sk>
References: <20170310180005.GA5196@fantomas.sk>
 <024701d29aa7$89bbd510$9d337f30$@ngtech.co.il>
 <20170312153501.GC27151@fantomas.sk>
Message-ID: <02cf01d29bcf$4c5944e0$e50bcea0$@ngtech.co.il>

Hey,

I am not maintain yet  Debian and Ubuntu Squid-Cache packaging but I started to do so as a testing step towards the next one.
My squid debian based packages are not including ecap support due to the overhead but I believe that diladele provides up-to-date Ubuntu and Maybe debian packages.

I understand that you prefer Debian to push fixes into their builds but I am not deep enough in the Debian world to tell you I understand it all goes there and what affects patches being pushed into the stable build.
Would you consider to use an external debian packge that follows squid stable releases?

Thanks,
Eliezer

----
Eliezer Croitoru
Linux System Administrator
Mobile: +972-5-28704261
Email: eliezer at ngtech.co.il


-----Original Message-----
From: squid-users [mailto:squid-users-bounces at lists.squid-cache.org] On Behalf Of Matus UHLAR - fantomas
Sent: Sunday, March 12, 2017 5:35 PM
To: squid-users at lists.squid-cache.org; squid-users at squid-cache.org
Subject: Re: [squid-users] debugging squid memory

On 11.03.17 22:39, Eliezer  Croitoru wrote:
>Just asking loud, is there any chance you will give run an upgrade from 3.4.8 to 3.5.24?

yes, but I will have to take care of that - debian people take care of the
version in debian.

I prefer fixing things in debian and push fixes back to distribution.

>as I have already noted in other thread, I seem to have memory leak in squid
>3.4.8 (debian 8 jessie) version, only memory cache used now.

-- 
Matus UHLAR - fantomas, uhlar at fantomas.sk ; http://www.fantomas.sk/
Warning: I wish NOT to receive e-mail advertising to this address.
Varovanie: na tuto adresu chcem NEDOSTAVAT akukolvek reklamnu postu.
(R)etry, (A)bort, (C)ancer
_______________________________________________
squid-users mailing list
squid-users at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-users



From serhatkoroglu at outlook.com  Mon Mar 13 11:26:43 2017
From: serhatkoroglu at outlook.com (Serhat Koroglu)
Date: Mon, 13 Mar 2017 11:26:43 +0000
Subject: [squid-users] Squid wiki
In-Reply-To: <DB6PR0502MB3045473A4700B23E9D6366AEB4250@DB6PR0502MB3045.eurprd05.prod.outlook.com>
References: <DB6PR0502MB3045473A4700B23E9D6366AEB4250@DB6PR0502MB3045.eurprd05.prod.outlook.com>
Message-ID: <DB6PR0502MB3045BF6E3D4992A5075FF981B4250@DB6PR0502MB3045.eurprd05.prod.outlook.com>

Hello,

Squid wiki pages not working.  It seems there is a server error.

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20170313/16bce31a/attachment.htm>

From gkinkie at gmail.com  Mon Mar 13 11:30:32 2017
From: gkinkie at gmail.com (Francesco Chemolli)
Date: Mon, 13 Mar 2017 11:30:32 +0000
Subject: [squid-users] Squid wiki
In-Reply-To: <DB6PR0502MB3045BF6E3D4992A5075FF981B4250@DB6PR0502MB3045.eurprd05.prod.outlook.com>
References: <DB6PR0502MB3045473A4700B23E9D6366AEB4250@DB6PR0502MB3045.eurprd05.prod.outlook.com>
 <DB6PR0502MB3045BF6E3D4992A5075FF981B4250@DB6PR0502MB3045.eurprd05.prod.outlook.com>
Message-ID: <ADE01A54-59E4-4DDB-AD27-84A180641EEE@gmail.com>


> On 13 Mar 2017, at 11:26, Serhat Koroglu <serhatkoroglu at outlook.com> wrote:
> 
> Hello,
> Squid wiki pages not working.  It seems there is a server error. 


Hi,
   that's correct. The server suffered a broken hard disk.
The support team is working on fixing it, it'll take at least a day to fix it
No data has been lost, but sorry for the inconvenience.

	Francesco

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20170313/2ebe7f4a/attachment.htm>

From darvin at reduc.edu.cu  Mon Mar 13 17:33:06 2017
From: darvin at reduc.edu.cu (Darvin Rivera Aguilar)
Date: Mon, 13 Mar 2017 13:33:06 -0400
Subject: [squid-users] Squid in Private Network and Multiple WAN (Best Load
 Balance Solution).
Message-ID: <aa4a7ae2-44f0-70f7-fa1f-e69dcedd92e8@reduc.edu.cu>

Hi,
I have my public squid ip (1.1.1.1:3128) on my local network and 4 wan 
address. Two wan address (2.2.2.1 and 2.2.2.3) for Education Network 
(example: acl for .edu site); and other two network (3.3.3.1 and 
4.4.4.1) for General Porpuse (example: the rest of navegation... .com, 
.org...)
I need to balance the traffic for all request in 1.1.1.1:3128.
1- I need to install and configure HAproxy for load balance solution?
2- I need to install HAproxy for Education Network (2.2.2.1 and 2.2.2.3) 
and other HAproxy for General Porpuse (3.3.3.1 and 4.4.4.1)?
3- Use Parent Proxy?
4- Any other solution or the best.

Grettings
Darvin




------------------------------------------------------------------
---- Universidad de Camag?ey "Ignacio Agramonte Loynaz", Cuba ----
------------------------------------------------------------------
---- https://intranet.reduc.edu.cu/ -------------------------------
---- https://www.reduc.edu.cu/ ----------------------------------
------------------------------------------------------------------


From gkinkie at gmail.com  Mon Mar 13 23:10:37 2017
From: gkinkie at gmail.com (Kinkie)
Date: Mon, 13 Mar 2017 23:10:37 +0000
Subject: [squid-users] Wiki outage - solved
Message-ID: <CA+Y8hcMeSw+43NZOSv2bhWj1HznY8bPmL5gFUvu7+yCYHoj17A@mail.gmail.com>

Hi all,
   due to a hard drive failure on one of the servers we run, the wiki has
been unavailable in the past couple of days.
   The volunteers overseeing the project infrastructure have been able to
restore the service by moving it to a different hardware, and the wiki
should now become progressively available again as DNS records propagate.

   Please join me in thanking the volunteers who help run the squid
infrastructure for donating their time and expertise.

   It is a good momento to remind everyone that the Squid project and the
Squid Software Foundation rely on everyone's effort and on generous
donations by individuals, companies and organizations to continue
supporting squid and accompanying services.

The list of main sponsors is at
http://www.squid-cache.org/Support/sponsors.html

Please refer to http://www.squid-cache.org/Foundation/donate.html if you
wish to donate financial or material resources to the Squid project.

-- 
    Francesco Chemolli
    Vice President, Squid Software Foundation
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20170313/4d529966/attachment.htm>

From serhatkoroglu at outlook.com  Tue Mar 14 07:15:04 2017
From: serhatkoroglu at outlook.com (Serhat Koroglu)
Date: Tue, 14 Mar 2017 07:15:04 +0000
Subject: [squid-users] Squid Authentication with HTTP REST API
Message-ID: <DB6PR0502MB304543ACB94E65D8F5CA8516B4240@DB6PR0502MB3045.eurprd05.prod.outlook.com>

Hello,

Is there any possibilty implementing an authentication through a custom XML Web Service or HTTP REST API? What should I check?


-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20170314/1ac60a90/attachment.htm>

From eliezer at ngtech.co.il  Tue Mar 14 07:54:09 2017
From: eliezer at ngtech.co.il (Eliezer  Croitoru)
Date: Tue, 14 Mar 2017 09:54:09 +0200
Subject: [squid-users] Wiki outage - solved
In-Reply-To: <CA+Y8hcMeSw+43NZOSv2bhWj1HznY8bPmL5gFUvu7+yCYHoj17A@mail.gmail.com>
References: <CA+Y8hcMeSw+43NZOSv2bhWj1HznY8bPmL5gFUvu7+yCYHoj17A@mail.gmail.com>
Message-ID: <158901d29c98$29ddced0$7d996c70$@ngtech.co.il>

Thanks!
To all these individuals which helps daily and helped in this specific scenario.
I must admit that the wiki was missing to me and I started to understand a bit more how it fulfills the position of something like:
"The Squid-Cache Bible"

With hope that this will help us move forward and thrive from this towards a better future.

All The Bests,
Eliezer Croitoru

----
http://ngtech.co.il/lmgtfy/
Linux System Administrator
Mobile: +972-5-28704261
Email: eliezer at ngtech.co.il


From: squid-users [mailto:squid-users-bounces at lists.squid-cache.org] On Behalf Of Kinkie
Sent: Tuesday, March 14, 2017 1:11 AM
To: Squid Users <squid-users at lists.squid-cache.org>
Subject: [squid-users] Wiki outage - solved

Hi all,
   due to a hard drive failure on one of the servers we run, the wiki has been unavailable in the past couple of days.
   The volunteers overseeing the project infrastructure have been able to restore the service by moving it to a different hardware, and the wiki should now become progressively available again as DNS records propagate.

   Please join me in thanking the volunteers who help run the squid infrastructure for donating their time and expertise.

   It is a good momento to remind everyone that the Squid project and the Squid Software Foundation rely on everyone's effort and on generous donations by individuals, companies and organizations to continue supporting squid and accompanying services.

The list of main sponsors is at http://www.squid-cache.org/Support/sponsors.html

Please refer to http://www.squid-cache.org/Foundation/donate.html if you wish to donate financial or material resources to the Squid project.


-- 
    Francesco Chemolli
    Vice President, Squid Software Foundation



From evan.blackstone at oecu.org  Tue Mar 14 16:14:13 2017
From: evan.blackstone at oecu.org (Evan Blackstone)
Date: Tue, 14 Mar 2017 16:14:13 +0000
Subject: [squid-users] CentOS 7 -- Squid 3.5.20
Message-ID: <e92925ccf13e4f8097ddfa3380bb3d0d@oecu.org>

Good morning all,

I installed Squid from the main CentOS repos, and I noticed that ssl_crtd is missing from the package (making SSL inspection broken). I've found a thread on serverfault in which someone just recommend downloading and installing squid-helpers.rpm from a third party repo, but that is unfortunately not permissible in my circumstance.

Any suggestions? Am I missing something?

Evan Blackstone
Oklahoma Employees Credit Union
Information Security Officer
Phone: 405.606.6369
Mobile: 405.669.1638
Fax: 405.606.6345
evan.blackstone at oecu.org <mailto:evan.blackstone at oecu.org>
www.oecu.org <http://www.oecu.org>
[cid:image001.jpg at 01D29CB4.1C3075F0]<https://www.facebook.com/OKEmployeesCU/>[cid:image002.jpg at 01D29CB4.1C3075F0]<https://www.instagram.com/okemployeescu/>[cid:image003.jpg at 01D29CB4.1C3075F0]<https://www.linkedin.com/company/oklahoma-employees-credit-union>[cid:image004.jpg at 01D29CB4.1C3075F0]<https://twitter.com/oecu>[cid:image005.jpg at 01D29CB4.1C3075F0]<https://www.youtube.com/user/MyOECU>


________________________________

NOTICE:
This e-mail is intended solely for the use of the individual to whom it is addressed and may contain information that is privileged, confidential or otherwise exempt from disclosure. If the reader of this e-mail is not the intended recipient or the employee or agent responsible for delivering the message to the intended recipient, you are hereby notified that any dissemination, distribution, or copying of this communication is strictly prohibited. If you have received this communication in error, please immediately notify us by replying to the original message at the listed email address.

Thank you
Oklahoma Employees Credit Union
http://www.oecu.org
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20170314/63bd7afd/attachment.htm>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: image001.jpg
Type: image/jpeg
Size: 897 bytes
Desc: image001.jpg
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20170314/63bd7afd/attachment.jpg>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: image002.jpg
Type: image/jpeg
Size: 930 bytes
Desc: image002.jpg
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20170314/63bd7afd/attachment-0001.jpg>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: image003.jpg
Type: image/jpeg
Size: 842 bytes
Desc: image003.jpg
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20170314/63bd7afd/attachment-0002.jpg>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: image004.jpg
Type: image/jpeg
Size: 905 bytes
Desc: image004.jpg
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20170314/63bd7afd/attachment-0003.jpg>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: image005.jpg
Type: image/jpeg
Size: 911 bytes
Desc: image005.jpg
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20170314/63bd7afd/attachment-0004.jpg>

From Tommy.Craddock at bicgraphic.com  Tue Mar 14 16:18:25 2017
From: Tommy.Craddock at bicgraphic.com (Craddock, Tommy)
Date: Tue, 14 Mar 2017 16:18:25 +0000
Subject: [squid-users] CentOS 7 -- Squid 3.5.20
In-Reply-To: <e92925ccf13e4f8097ddfa3380bb3d0d@oecu.org>
References: <e92925ccf13e4f8097ddfa3380bb3d0d@oecu.org>
Message-ID: <CA86A9283AA07E478F6B0629521FFEE74AB38B@CLWSEXCMBX02.na.bicworld.com>

Hello,

Pull down the source, edit the rpm spec file to build what you need, and build it yourself.  This is the best way to maintain packages in an env like yours.


Tommy Craddock

From: squid-users [mailto:squid-users-bounces at lists.squid-cache.org] On Behalf Of Evan Blackstone
Sent: Tuesday, March 14, 2017 12:14 PM
To: squid-users at lists.squid-cache.org
Subject: [squid-users] CentOS 7 -- Squid 3.5.20

Good morning all,

I installed Squid from the main CentOS repos, and I noticed that ssl_crtd is missing from the package (making SSL inspection broken). I've found a thread on serverfault in which someone just recommend downloading and installing squid-helpers.rpm from a third party repo, but that is unfortunately not permissible in my circumstance.

Any suggestions? Am I missing something?

Evan Blackstone
Oklahoma Employees Credit Union
Information Security Officer
Phone: 405.606.6369
Mobile: 405.669.1638
Fax: 405.606.6345
evan.blackstone at oecu.org <mailto:evan.blackstone at oecu.org>
www.oecu.org <http://www.oecu.org>
[cid:image001.jpg at 01D29CBD.13ECE550]<https://www.facebook.com/OKEmployeesCU/>[cid:image002.jpg at 01D29CBD.13ECE550]<https://www.instagram.com/okemployeescu/>[cid:image003.jpg at 01D29CBD.13ECE550]<https://www.linkedin.com/company/oklahoma-employees-credit-union>[cid:image004.jpg at 01D29CBD.13ECE550]<https://twitter.com/oecu>[cid:image005.jpg at 01D29CBD.13ECE550]<https://www.youtube.com/user/MyOECU>

________________________________

NOTICE:
This e-mail is intended solely for the use of the individual to whom it is addressed and may contain information that is privileged, confidential or otherwise exempt from disclosure. If the reader of this e-mail is not the intended recipient or the employee or agent responsible for delivering the message to the intended recipient, you are hereby notified that any dissemination, distribution, or copying of this communication is strictly prohibited. If you have received this communication in error, please immediately notify us by replying to the original message at the listed email address.

Thank you
Oklahoma Employees Credit Union
http://www.oecu.org

______________________________________________________________________
This email has been scanned by the Symantec Email Security.cloud service.
For more information please visit http://www.symanteccloud.com
______________________________________________________________________

______________________________________________________________________
This email has been scanned by the Symantec Email Security.cloud service.
For more information please visit http://www.symanteccloud.com
______________________________________________________________________
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20170314/fbf60ec7/attachment.htm>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: image001.jpg
Type: image/jpeg
Size: 897 bytes
Desc: image001.jpg
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20170314/fbf60ec7/attachment.jpg>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: image002.jpg
Type: image/jpeg
Size: 930 bytes
Desc: image002.jpg
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20170314/fbf60ec7/attachment-0001.jpg>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: image003.jpg
Type: image/jpeg
Size: 842 bytes
Desc: image003.jpg
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20170314/fbf60ec7/attachment-0002.jpg>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: image004.jpg
Type: image/jpeg
Size: 905 bytes
Desc: image004.jpg
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20170314/fbf60ec7/attachment-0003.jpg>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: image005.jpg
Type: image/jpeg
Size: 911 bytes
Desc: image005.jpg
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20170314/fbf60ec7/attachment-0004.jpg>

From eliezer at ngtech.co.il  Tue Mar 14 20:47:36 2017
From: eliezer at ngtech.co.il (Eliezer  Croitoru)
Date: Tue, 14 Mar 2017 22:47:36 +0200
Subject: [squid-users] CentOS 7 -- Squid 3.5.20
In-Reply-To: <CA86A9283AA07E478F6B0629521FFEE74AB38B@CLWSEXCMBX02.na.bicworld.com>
References: <e92925ccf13e4f8097ddfa3380bb3d0d@oecu.org>
 <CA86A9283AA07E478F6B0629521FFEE74AB38B@CLWSEXCMBX02.na.bicworld.com>
Message-ID: <15e201d29d04$3651b480$a2f51d80$@ngtech.co.il>

As the maintainer of the *external*CentOS repository I believe that there is
some confusion.
You should read the note in the wiki about my repository:
http://wiki.squid-cache.org/KnowledgeBase/CentOS#Pre-Built_Binary_Packages

You cannot install squid-helpers rpm without the squid rpm from the repo
since your main squid binaries will be missing the core functionality of
bumping ssl connections.
The ssl_crtd is just a helper software which can be written by the squid
team or by someone else.
It happens so the Squid-Cache team found it unreasonable to release an
helper api without the helper itself.
Squid main function is a simple forward proxy without any ssl
bumping\interception and also since it's illegal in more than some places on
the globe to use ssl-bump, I decided package it as a helper
You can use the RPMS or SRPMS from my repository just get into:
http://ngtech.co.il/repo/centos/7/

and find your way in the folder.
If you need more help I am here for it.

All The Bests,
Eliezer

----
http://ngtech.co.il/lmgtfy/
Linux System Administrator
Mobile: +972-5-28704261
Email: eliezer at ngtech.co.il


From: squid-users [mailto:squid-users-bounces at lists.squid-cache.org] On
Behalf Of Craddock, Tommy
Sent: Tuesday, March 14, 2017 6:18 PM
To: squid-users at lists.squid-cache.org
Subject: Re: [squid-users] CentOS 7 -- Squid 3.5.20

Hello, 

Pull down the source, edit the rpm spec file to build what you need, and
build it yourself.? This is the best way to maintain packages in an env like
yours. 


Tommy Craddock

From: squid-users [mailto:squid-users-bounces at lists.squid-cache.org] On
Behalf Of Evan Blackstone
Sent: Tuesday, March 14, 2017 12:14 PM
To: mailto:squid-users at lists.squid-cache.org
Subject: [squid-users] CentOS 7 -- Squid 3.5.20

Good morning all,

I installed Squid from the main CentOS repos, and I noticed that ssl_crtd is
missing from the package (making SSL inspection broken). I?ve found a thread
on serverfault in which someone just recommend downloading and installing
squid-helpers.rpm from a third party repo, but that is unfortunately not
permissible in my circumstance. 

Any suggestions? Am I missing something? 

Evan Blackstone 
Oklahoma Employees Credit Union 
Information Security Officer 
Phone: 405.606.6369 
Mobile: 405.669.1638 
Fax: 405.606.6345 
mailto:evan.blackstone at oecu.org
http://www.oecu.org
https://www.facebook.com/OKEmployeesCU/https://www.instagram.com/okemployees
cu/https://www.linkedin.com/company/oklahoma-employees-credit-unionhttps://t
witter.com/oecuhttps://www.youtube.com/user/MyOECU

________________________________________

NOTICE:
This e-mail is intended solely for the use of the individual to whom it is
addressed and may contain information that is privileged, confidential or
otherwise exempt from disclosure. If the reader of this e-mail is not the
intended recipient or the employee or agent responsible for delivering the
message to the intended recipient, you are hereby notified that any
dissemination, distribution, or copying of this communication is strictly
prohibited. If you have received this communication in error, please
immediately notify us by replying to the original message at the listed
email address.

Thank you
Oklahoma Employees Credit Union
http://www.oecu.org

______________________________________________________________________
This email has been scanned by the Symantec Email Security.cloud service.
For more information please visit http://www.symanteccloud.com
______________________________________________________________________

______________________________________________________________________
This email has been scanned by the Symantec Email Security.cloud service.
For more information please visit http://www.symanteccloud.com
______________________________________________________________________



From evan.blackstone at oecu.org  Tue Mar 14 22:26:44 2017
From: evan.blackstone at oecu.org (Evan Blackstone)
Date: Tue, 14 Mar 2017 22:26:44 +0000
Subject: [squid-users] CentOS 7 -- Squid 3.5.20
In-Reply-To: <15e201d29d04$3651b480$a2f51d80$@ngtech.co.il>
References: <e92925ccf13e4f8097ddfa3380bb3d0d@oecu.org>
 <CA86A9283AA07E478F6B0629521FFEE74AB38B@CLWSEXCMBX02.na.bicworld.com>
 <15e201d29d04$3651b480$a2f51d80$@ngtech.co.il>
Message-ID: <63ae8bbe19ee498f8ddd79b1d2f10c04@oecu.org>

Thank you very much for the info. 

What's weird is that the "squid -v" output from the CentOS distribution has the "--with-openssl" and "--enable-ssl_crtd" flags enabled! I just assumed that it was capable of SSL stuff since I saw that.


I went ahead and just compiled it per Tommy's suggestion earlier in the day. (I even used the output of the "squid -v" command from the prebuilt CentOS version.) Had a little trouble figuring out the permissions/folder/Squid user stuff (since that stuff is done automatically when installing from repos), but it seems to be up and running now. 

Thank you guys for the help again!



-----Original Message-----
From: squid-users [mailto:squid-users-bounces at lists.squid-cache.org] On Behalf Of Eliezer Croitoru
Sent: Tuesday, March 14, 2017 3:48 PM
To: 'Craddock, Tommy' <Tommy.Craddock at bicgraphic.com>
Cc: squid-users at lists.squid-cache.org
Subject: Re: [squid-users] CentOS 7 -- Squid 3.5.20

As the maintainer of the *external*CentOS repository I believe that there is some confusion.
You should read the note in the wiki about my repository:
http://wiki.squid-cache.org/KnowledgeBase/CentOS#Pre-Built_Binary_Packages

You cannot install squid-helpers rpm without the squid rpm from the repo since your main squid binaries will be missing the core functionality of bumping ssl connections.
The ssl_crtd is just a helper software which can be written by the squid team or by someone else.
It happens so the Squid-Cache team found it unreasonable to release an helper api without the helper itself.
Squid main function is a simple forward proxy without any ssl bumping\interception and also since it's illegal in more than some places on the globe to use ssl-bump, I decided package it as a helper You can use the RPMS or SRPMS from my repository just get into:
http://ngtech.co.il/repo/centos/7/

and find your way in the folder.
If you need more help I am here for it.

All The Bests,
Eliezer

----
http://ngtech.co.il/lmgtfy/
Linux System Administrator
Mobile: +972-5-28704261
Email: eliezer at ngtech.co.il


From: squid-users [mailto:squid-users-bounces at lists.squid-cache.org] On Behalf Of Craddock, Tommy
Sent: Tuesday, March 14, 2017 6:18 PM
To: squid-users at lists.squid-cache.org
Subject: Re: [squid-users] CentOS 7 -- Squid 3.5.20

Hello, 

Pull down the source, edit the rpm spec file to build what you need, and build it yourself.? This is the best way to maintain packages in an env like yours. 


Tommy Craddock

From: squid-users [mailto:squid-users-bounces at lists.squid-cache.org] On Behalf Of Evan Blackstone
Sent: Tuesday, March 14, 2017 12:14 PM
To: mailto:squid-users at lists.squid-cache.org
Subject: [squid-users] CentOS 7 -- Squid 3.5.20

Good morning all,

I installed Squid from the main CentOS repos, and I noticed that ssl_crtd is missing from the package (making SSL inspection broken). I?ve found a thread on serverfault in which someone just recommend downloading and installing squid-helpers.rpm from a third party repo, but that is unfortunately not permissible in my circumstance. 

Any suggestions? Am I missing something? 

Evan Blackstone
Oklahoma Employees Credit Union
Information Security Officer
Phone: 405.606.6369
Mobile: 405.669.1638
Fax: 405.606.6345
mailto:evan.blackstone at oecu.org
http://www.oecu.org
https://www.facebook.com/OKEmployeesCU/https://www.instagram.com/okemployees
cu/https://www.linkedin.com/company/oklahoma-employees-credit-unionhttps://t
witter.com/oecuhttps://www.youtube.com/user/MyOECU

________________________________________

NOTICE:
This e-mail is intended solely for the use of the individual to whom it is addressed and may contain information that is privileged, confidential or otherwise exempt from disclosure. If the reader of this e-mail is not the intended recipient or the employee or agent responsible for delivering the message to the intended recipient, you are hereby notified that any dissemination, distribution, or copying of this communication is strictly prohibited. If you have received this communication in error, please immediately notify us by replying to the original message at the listed email address.

Thank you
Oklahoma Employees Credit Union
http://www.oecu.org

______________________________________________________________________
This email has been scanned by the Symantec Email Security.cloud service.
For more information please visit http://www.symanteccloud.com ______________________________________________________________________

______________________________________________________________________
This email has been scanned by the Symantec Email Security.cloud service.
For more information please visit http://www.symanteccloud.com ______________________________________________________________________

_______________________________________________
squid-users mailing list
squid-users at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-users

From marcel at guineanet.net  Tue Mar 14 23:30:16 2017
From: marcel at guineanet.net (Marcel Fossua)
Date: Wed, 15 Mar 2017 00:30:16 +0100
Subject: [squid-users] squid always dies
In-Reply-To: <63ae8bbe19ee498f8ddd79b1d2f10c04@oecu.org>
References: <e92925ccf13e4f8097ddfa3380bb3d0d@oecu.org>
 <CA86A9283AA07E478F6B0629521FFEE74AB38B@CLWSEXCMBX02.na.bicworld.com>
 <15e201d29d04$3651b480$a2f51d80$@ngtech.co.il>
 <63ae8bbe19ee498f8ddd79b1d2f10c04@oecu.org>
Message-ID: <0b71896e-826a-2f6d-d4f3-0ab91d635a78@guineanet.net>

Hi all
please cant someone give me a hand with this issue ?
my squid box randomly dies after running for some times and I always 
need to restart it
I get this output from cache.log

*Page faults with physical i/o: 0
CPU Usage: 2.636 seconds = 1.034 user + 1.602 sys
Maximum Resident Size: 541392 KB
Page faults with physical i/o: 0
CPU Usage: 2.728 seconds = 0.996 user + 1.732 sys
Maximum Resident Size: 552320 KB
Page faults with physical i/o: 0
CPU Usage: 2.999 seconds = 1.158 user + 1.841 sys
Maximum Resident Size: 548464 KB
Page faults with physical i/o: 0


thanks and Bests regards

*
-- 
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20170315/4b30bc4a/attachment.htm>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: fossua-vcard.jpg
Type: image/jpeg
Size: 17204 bytes
Desc: not available
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20170315/4b30bc4a/attachment.jpg>

From squid3 at treenet.co.nz  Wed Mar 15 02:03:42 2017
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Wed, 15 Mar 2017 15:03:42 +1300
Subject: [squid-users] Squid Authentication with HTTP REST API
In-Reply-To: <DB6PR0502MB304543ACB94E65D8F5CA8516B4240@DB6PR0502MB3045.eurprd05.prod.outlook.com>
References: <DB6PR0502MB304543ACB94E65D8F5CA8516B4240@DB6PR0502MB3045.eurprd05.prod.outlook.com>
Message-ID: <0323d49e-3a2b-4fed-e12c-609306c9b150@treenet.co.nz>

On 14/03/2017 8:15 p.m., Serhat Koroglu wrote:
> Hello,
> 
> Is there any possibilty implementing an authentication through a custom XML Web Service or HTTP REST API? What should I check?
> 

Squid supports the HTTP authentication framework (RFC 7235
<https://tools.ietf.org/html/rfc7235>). Squid is intentionally designed
not to touch the message payloads.

If the API uses custom headers then you can possibly do it with an
external_acl_type helper that takes those headers and returns
credentials to Squid.

But, if the API uses message payloads you will likely need something
like an ICAP service or eCAP module to do the payload processing.


Amos



From squid3 at treenet.co.nz  Wed Mar 15 02:49:04 2017
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Wed, 15 Mar 2017 15:49:04 +1300
Subject: [squid-users] reply_body_max_size question
In-Reply-To: <20170312071146.GA3794@fever.havannah.local>
References: <20170312071146.GA3794@fever.havannah.local>
Message-ID: <5d8e9a9a-e8a0-d51e-a10b-0e45fa6ba566@treenet.co.nz>

On 12/03/2017 8:11 p.m., Danny wrote:
> Hi,
> 
> Just want someone to confirm my current reply_body_max_size setup. I have a
> simple network at home i.e: Debian with a wireless card (wlan0) which is bridged
> (br0) to an ethernet card (eth0). All devices comes through the wireless card
> (wlan0) and then of to the router.
> 
> I want "localnet_sniper localnet_bridge localnet_fever localnet_44081 localnet_dannyS4" to have unlimited download capabilty but
> "localnet_vS5mini localnet_anTab2 localnet_vTab3 localnet_samsungTV localnet_samsungDVD localnet_dhcp" must be limited to a
> 5MB download limit.
> 
> Here is my configuration:
> ######################################################################################################################################
> acl localnet src 10.0.0.0/24	# RFC1918 possible internal network
> acl localnet_sniper src 10.0.0.3        #(eth0)
> acl localnet_bridge src 10.0.0.4        #(br0)
> acl localnet_fever src 10.0.0.5         #(wlan0)
> acl localnet_44081 src 10.0.0.11        #(RaspberryPi3)
> acl localnet_dannyS4 src 10.0.0.54
> acl localnet_vS5mini src 10.0.0.55
> acl localnet_shotgun src 10.0.0.56
> acl localnet_anTab2 src 10.0.0.71
> acl localnet_vTab3 src 10.0.0.73
> acl localnet_samsungTV src 10.0.0.80
> acl localnet_samsungDVD src 10.0.0.81
> acl localnet_dhcp src 10.0.0.201
> acl localnet_dhcp src 10.0.0.202
> acl localnet_dhcp src 10.0.0.203
> acl localnet_dhcp src 10.0.0.204
> 
> http_access allow password
> http_access allow localhost
> http_access allow localnet

The localnet ACL above matches and allows all requests from any IP in
the 10.*/24 to use the proxy.

So none of the below individual IP checks will ever be reached. They are
pointless anyway since they do the same as the more generic "allow
localnet".


> http_access allow localnet_sniper
> http_access allow localnet_bridge
> http_access allow localnet_fever
> http_access allow localnet_44081
> http_access allow localnet_dannyS4
> http_access allow localnet_vS5mini
> http_access allow localnet_anTab2
> http_access allow localnet_vTab3
> http_access allow localnet_samsungTV
> http_access allow localnet_samsungDVD
> http_access allow localnet_dhcp


The default security protections for Safe_ports, SSL_ports, CONNECT,
manager access, and final "deny all" are missing.

I hope you have just omited them from this mail, not removed them from
your config.

> 
> reply_body_max_size 9 999 999 999 MB localnet_sniper localnet_bridge localnet_fever localnet_44081 localnet_dannyS4

Squid understands the magic word "none" to mean no limit. The above is
setting a large, but not impossible limit of ~9.3 PB.


> reply_body_max_size 5 MB localnet_vS5mini localnet_anTab2 localnet_vTab3 localnet_samsungTV localnet_samsungDVD localnet_dhcp
> 

The ACLs on both these lines are defining an impossible situation.
See <http://wiki.squid-cache.org/SquidFaq/SquidAcl#Common_Mistakes> for
what is going wrong there and ways to fix it.

Transactions which do not have a limit applied, are of course unlimited.
So drop the ACL's explicitly listing what not to limit. You only need
ACL to match what does get limited, and only one is needed (you are only
matching on IP, nothing complex).

Like so:

 acl limit_5MB src 10.0.0.201-10.0.0.204 # dhcp
 acl limit_5MB src 10.0.0.80    # samsung TV
 acl limit_5MB src 10.0.0.81    # samsung DVD
 ...
 reply_body_max_size 5 MB limit_5MB

That is it.


> url_rewrite_program /usr/bin/squidGuard -c /etc/squidguard/squidGuard.conf
> redirect_program /usr/bin/squidGuard -c /etc/squidguard/squidGuard.conf


redirect_program is a deprecated alias for url_rewrite_program. You can
only have one configured for use. So, only the latter of the two
directives will do anything.

Amos



From squid3 at treenet.co.nz  Wed Mar 15 02:55:24 2017
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Wed, 15 Mar 2017 15:55:24 +1300
Subject: [squid-users] pinger without peers
In-Reply-To: <20170310092841.GA31011@fantomas.sk>
References: <20170309173629.GA9814@fantomas.sk>
 <ea420b98-9886-173d-baf8-692f2969e467@treenet.co.nz>
 <20170310092841.GA31011@fantomas.sk>
Message-ID: <9a507652-1dd4-df8b-d6d7-ad741ec2bd40@treenet.co.nz>

On 10/03/2017 10:28 p.m., Matus UHLAR - fantomas wrote:
>> On 10/03/2017 6:36 a.m., Matus UHLAR - fantomas wrote:
>>> does it have sense to run pinger without having cache peers configured?
>>>
>>> if I get the "Network DB Statistics:" output properly, it seems that
>>> 33% of
>>> hosts is unreachable.
> 
> On 10.03.17 08:33, Amos Jeffries wrote:
>> The code using it is called peer-selection, but that includes choosing
>> between origin servers (DIRECT) as well as cache_peer lines. So it can
>> affect traffic routing even if you dont have cache_peer. Though less
>> that RTT based cache_peer LB algorithms.
> 
> does that mean pinger output is used for servers with multiple addresses?
> does it only ping servers with multiple addresses?

All servers that are selected for use get pinged, but use of the ping
results is uncommon, usually the data comes back far too late to affect
the immediate transaction IME. It is probably of some use on persistent
connections.

> 
>> The sad results are due to admin ICMP echo blocking. Hopefully that is
>> all they are blocking though and not other mandatory ICMP codes like
>> path-MTU discovery.
> 
> of course. But since I'm trying to solve different problem, I'm thinking of
> trurning off everything that can cause issues.
> 

Nod. It is of minor relevance on most Gbit speed (or even Mbit speed)
networks today.

Amos



From squid3 at treenet.co.nz  Wed Mar 15 03:32:14 2017
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Wed, 15 Mar 2017 16:32:14 +1300
Subject: [squid-users] Squid in Private Network and Multiple WAN (Best
 Load Balance Solution).
In-Reply-To: <aa4a7ae2-44f0-70f7-fa1f-e69dcedd92e8@reduc.edu.cu>
References: <aa4a7ae2-44f0-70f7-fa1f-e69dcedd92e8@reduc.edu.cu>
Message-ID: <4b71f181-b20d-418d-6376-19f533f8630e@treenet.co.nz>

On 14/03/2017 6:33 a.m., Darvin Rivera Aguilar wrote:
> Hi,
> I have my public squid ip (1.1.1.1:3128) on my local network and 4 wan
> address. Two wan address (2.2.2.1 and 2.2.2.3) for Education Network
> (example: acl for .edu site); and other two network (3.3.3.1 and
> 4.4.4.1) for General Porpuse (example: the rest of navegation... .com,
> .org...)

Load Balancing in the proxy is the wrong solution for this. There is
nothing to balance. What you are looking for is routing.

Do you have 2 or 4 physical WAN uplinks being used for this?

If 2 uplinks;

Then having two IPs on each does not matter. Just pick one IP that Squid
will use for each traffic type and select it with tcp_outgoing_address
like so:

 acl Education_Network dstdomain .edu
 tcp_outgoing_address 2.2.2.1 Education_Network
 # other traffic
 tcp_outgoing_address 3.3.3.1



If 4 uplinks - i.e. by two IP's you actually mean there are two uplinks
for each type of traffic;

The best approach here is to leave the load balancing in the TCP stack,
but have the proxy doing traffic classification so that TCP stack knows
where each connection / flow is needing to go.

In squid.conf use tcp_outgoing_tos directive with a dstdomain ACL
matching the Education to classify the traffic types (Education vs
General). Like so:

 acl Education_Network dstdomain .edu
 tcp_outgoing_tos 0x10 Education_Network
 # other traffic
 tcp_outgoing_tos 0x20


Then you just need TCP networking rules to use the 0x10 or 0x20 to load
balance between the two uplinks for that type of traffic.

The exact outgoing IP address does not matter to the proxy, so long as
the kernel assigns a correct one for the uplink which is going to be used.


HTH
Amos



From squid3 at treenet.co.nz  Wed Mar 15 03:33:50 2017
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Wed, 15 Mar 2017 16:33:50 +1300
Subject: [squid-users] squid always dies
In-Reply-To: <0b71896e-826a-2f6d-d4f3-0ab91d635a78@guineanet.net>
References: <e92925ccf13e4f8097ddfa3380bb3d0d@oecu.org>
 <CA86A9283AA07E478F6B0629521FFEE74AB38B@CLWSEXCMBX02.na.bicworld.com>
 <15e201d29d04$3651b480$a2f51d80$@ngtech.co.il>
 <63ae8bbe19ee498f8ddd79b1d2f10c04@oecu.org>
 <0b71896e-826a-2f6d-d4f3-0ab91d635a78@guineanet.net>
Message-ID: <d54341b5-ca26-2fab-4b6b-88c7780c28fb@treenet.co.nz>

On 15/03/2017 12:30 p.m., Marcel Fossua wrote:
> Hi all
> please cant someone give me a hand with this issue ?
> my squid box randomly dies after running for some times and I always need to 
> restart it
> I get this output from cache.log
> 
> *Page faults with physical i/o: 0
> CPU Usage: 2.636 seconds = 1.034 user + 1.602 sys
> Maximum Resident Size: 541392 KB
> Page faults with physical i/o: 0
> CPU Usage: 2.728 seconds = 0.996 user + 1.732 sys
> Maximum Resident Size: 552320 KB
> Page faults with physical i/o: 0
> CPU Usage: 2.999 seconds = 1.158 user + 1.841 sys
> Maximum Resident Size: 548464 KB
> Page faults with physical i/o: 0
> 

1) Please dont hijack a thread about a completely different topic.

2) This info tells nothing about any problem.

Cheers
Amos



From wolle5050 at gmx.de  Wed Mar 15 06:06:53 2017
From: wolle5050 at gmx.de (Jens Offenbach)
Date: Wed, 15 Mar 2017 07:06:53 +0100
Subject: [squid-users] No failover when default parent proxy fails (Squid
	3.5.12)
Message-ID: <trinity-daf2229d-e256-478a-9fd8-a5005f819ef5-1489558013937@3capp-gmx-bs10>

Hi,
I have two parent proxies configured, but Squid seems to stick to the default proxy even when the proxy cannot be reached:
2017/03/15 06:40:13 kid1| TCP connection to proxy.mycompany.de/8080 failed
2017/03/15 06:40:13 kid1| Detected DEAD Parent: proxy.mycompany.de
2017/03/15 06:40:13 kid1| TCP connection to proxy.mycompany.de/8080 failed
2017/03/15 06:40:13 kid1| TCP connection to proxy.mycompany.de/8080 failed
2017/03/15 06:40:13 kid1| TCP connection to proxy.mycompany.de/8080 failed
2017/03/15 06:40:13 kid1| TCP connection to proxy.mycompany.de/8080 failed
2017/03/15 06:40:13 kid1| TCP connection to proxy.mycompany.de/8080 failed
2017/03/15 06:40:13 kid1| TCP connection to proxy.mycompany.de/8080 failed
2017/03/15 06:40:43 kid1| TCP connection to proxy.mycompany.de/8080 failed
2017/03/15 06:40:43 kid1| TCP connection to proxy.mycompany.de/8080 failed
2017/03/15 06:40:43 kid1| TCP connection to proxy.mycompany.de/8080 failed
2017/03/15 06:40:43 kid1| TCP connection to proxy.mycompany.de/8080 failed
2017/03/15 06:40:43 kid1| TCP connection to proxy.mycompany.de/8080 failed
2017/03/15 06:40:43 kid1| TCP connection to proxy.mycompany.de/8080 failed
2017/03/15 06:40:43 kid1| TCP connection to proxy.mycompany.de/8080 failed

No failover takes place... I must miss someting in my config. Can someone please help me. I am on Ubuntu 16.04.2:
$ squid -v
Squid Cache: Version 3.5.12
Service Name: squid
Ubuntu linux
configure options:  '--build=x86_64-linux-gnu' '--prefix=/usr' '--includedir=${prefix}/include' '--mandir=${prefix}/share/man' '--infodir=${prefix}/share/info' '--sysconfdir=/etc' '--localstatedir=/var' '--libexecdir=${prefix}/lib/squid3' '--srcdir=.' '--disable-maintainer-mode' '--disable-dependency-tracking' '--disable-silent-rules' 'BUILDCXXFLAGS=-g -O2 -fPIE -fstack-protector-strong -Wformat -Werror=format-security -Wl,-Bsymbolic-functions -fPIE -pie -Wl,-z,relro -Wl,-z,now' '--datadir=/usr/share/squid' '--sysconfdir=/etc/squid' '--libexecdir=/usr/lib/squid' '--mandir=/usr/share/man' '--enable-inline' '--disable-arch-native' '--enable-async-io=8' '--enable-storeio=ufs,aufs,diskd,rock' '--enable-removal-policies=lru,heap' '--enable-delay-pools' '--enable-cache-digests' '--enable-icap-client' '--enable-follow-x-forwarded-for' '--enable-auth-basic=DB,fake,getpwnam,LDAP,NCSA,NIS,PAM,POP3,RADIUS,SASL,SMB' '--enable-auth-digest=file,LDAP' '--enable-auth-negotiate=kerberos,wrapper' '--enable-auth-ntlm=fake,smb_lm' '--enable-external-acl-helpers=file_userip,kerberos_ldap_group,LDAP_group,session,SQL_session,unix_group,wbinfo_group' '--enable-url-rewrite-helpers=fake' '--enable-eui' '--enable-esi' '--enable-icmp' '--enable-zph-qos' '--enable-ecap' '--disable-translation' '--with-swapdir=/var/spool/squid' '--with-logdir=/var/log/squid' '--with-pidfile=/var/run/squid.pid' '--with-filedescriptors=65536' '--with-large-files' '--with-default-user=proxy' '--enable-build-info=Ubuntu linux' '--enable-linux-netfilter' 'build_alias=x86_64-linux-gnu' 'CFLAGS=-g -O2 -fPIE -fstack-protector-strong -Wformat -Werror=format-security -Wall' 'LDFLAGS=-Wl,-Bsymbolic-functions -fPIE -pie -Wl,-z,relro -Wl,-z,now' 'CPPFLAGS=-Wdate-time -D_FORTIFY_SOURCE=2' 'CXXFLAGS=-g -O2 -fPIE -fstack-protector-strong -Wformat -Werror=format-security'

This is my squid.conf
# ACCESS CONTROLS
# -----------------------------------------------------------------------------
  # OpenStack Networks
  acl localnet src 10.116.0.0/20
  acl localnet src 10.30.200.0/21
  acl localnet src 10.30.216.0/22

  # mycompany Networks
  acl to_matnet dst 139.2.0.0/16
  acl to_matnet dst 193.96.112.0/21
  acl to_matnet dst 192.109.216.0/24
  acl to_matnet dst 100.1.4.0/22
  acl to_matnet dst 10.0.0.0/8
  acl to_matnet dst 172.16.0.0/12
  acl to_matnet dst 192.168.0.0/16

  # SSL-Ports
  acl SSL_ports port 443 # https
  acl SSL_ports port 563 # snews
  acl SSL_ports port 873 # rsync

  # Safe-Ports
  acl Safe_ports port 80  # http
  acl Safe_ports port 21  # ftp
  acl Safe_ports port 443 # https
  acl Safe_ports port 70  # gopher
  acl Safe_ports port 210 # wais
  acl Safe_ports port 1025-65535 # unregistered ports
  acl Safe_ports port 280 # http-mgmt
  acl Safe_ports port 488 # gss-http
  acl Safe_ports port 591 # filemaker
  acl Safe_ports port 777 # multiling http
  acl Safe_ports port 631 # cups
  acl Safe_ports port 873 # rsync
  acl Safe_ports port 901 # SWAT

  # HTTPS
  acl CONNECT method CONNECT

  http_access deny  !Safe_ports
  http_access deny  CONNECT !SSL_ports
  http_access allow manager localhost
  http_access deny  manager
  http_access allow localnet
  http_access allow localhost
  http_access deny all

# NETWORK OPTIONS
# -----------------------------------------------------------------------------
  http_port 10.30.202.99:3128

# OPTIONS WHICH AFFECT THE NEIGHBOR SELECTION ALGORITHM
# -----------------------------------------------------------------------------
  cache_peer proxy.mycompany.de parent 8080 0 no-query no-digest default
  cache_peer  roxy.mycompany.de parent 8080 0 no-query no-digest

# MEMORY CACHE OPTIONS
# -----------------------------------------------------------------------------
  maximum_object_size_in_memory 8 MB
  memory_replacement_policy heap LFUDA
  cache_mem 256 MB

# DISK CACHE OPTIONS
# -----------------------------------------------------------------------------
  maximum_object_size 10 GB
  cache_replacement_policy heap GDSF
  cache_dir ufs /var/cache/squid 88894 16 256 max-size=10737418240

# LOGFILE OPTIONS
# -----------------------------------------------------------------------------
  access_log daemon:/var/log/squid/access.log squid

# OPTIONS FOR TROUBLESHOOTING
# -----------------------------------------------------------------------------
  cache_log /var/log/squid/cache.log
  coredump_dir /var/log/squid

# OPTIONS FOR TUNING THE CACHE
# -----------------------------------------------------------------------------
  max_stale 6 days
  shutdown_lifetime 5 seconds

# ADMINISTRATIVE PARAMETERS
# -----------------------------------------------------------------------------
  visible_hostname mos-proxy.mycompany.com

# OPTIONS INFLUENCING REQUEST FORWARDING 
# -----------------------------------------------------------------------------
  always_direct allow to_matnet
  never_direct  allow all

# DNS OPTIONS
# -----------------------------------------------------------------------------
  dns_nameservers 139.2.34.171
  dns_nameservers 139.2.34.37

# MISCELLANEOUS
# -----------------------------------------------------------------------------
  memory_pools off


From eliezer at ngtech.co.il  Wed Mar 15 09:01:15 2017
From: eliezer at ngtech.co.il (Eliezer  Croitoru)
Date: Wed, 15 Mar 2017 11:01:15 +0200
Subject: [squid-users] Squid Authentication with HTTP REST API
In-Reply-To: <0323d49e-3a2b-4fed-e12c-609306c9b150@treenet.co.nz>
References: <DB6PR0502MB304543ACB94E65D8F5CA8516B4240@DB6PR0502MB3045.eurprd05.prod.outlook.com>
 <0323d49e-3a2b-4fed-e12c-609306c9b150@treenet.co.nz>
Message-ID: <165d01d29d6a$b3a847d0$1af8d770$@ngtech.co.il>

Hey Serhat,(first name right?)

>From what I understand you have a specific case.
Today the squid project doesn't have an example on how to implement such a solution.
I am willing to write an example for such a use case.
If you are willing to give me some of the details privately I would be able to put up together an ICAP server as an example.

Eliezer

----
Eliezer Croitoru
Linux System Administrator
Mobile: +972-5-28704261
Email: eliezer at ngtech.co.il


-----Original Message-----
From: squid-users [mailto:squid-users-bounces at lists.squid-cache.org] On Behalf Of Amos Jeffries
Sent: Wednesday, March 15, 2017 4:04 AM
To: squid-users at lists.squid-cache.org
Subject: Re: [squid-users] Squid Authentication with HTTP REST API

On 14/03/2017 8:15 p.m., Serhat Koroglu wrote:
> Hello,
> 
> Is there any possibilty implementing an authentication through a custom XML Web Service or HTTP REST API? What should I check?
> 

Squid supports the HTTP authentication framework (RFC 7235
<https://tools.ietf.org/html/rfc7235>). Squid is intentionally designed
not to touch the message payloads.

If the API uses custom headers then you can possibly do it with an
external_acl_type helper that takes those headers and returns
credentials to Squid.

But, if the API uses message payloads you will likely need something
like an ICAP service or eCAP module to do the payload processing.


Amos

_______________________________________________
squid-users mailing list
squid-users at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-users



From mikes at surcouf.co.uk  Wed Mar 15 09:18:14 2017
From: mikes at surcouf.co.uk (Mike Surcouf)
Date: Wed, 15 Mar 2017 09:18:14 +0000
Subject: [squid-users] kerb auth groups KV note acl config
In-Reply-To: <2197768425D7F5479A0FFB3FEC212F7FF5D88ABD@aesmail.surcouf.local>
References: <2197768425D7F5479A0FFB3FEC212F7FF5D88ABD@aesmail.surcouf.local>
Message-ID: <2197768425D7F5479A0FFB3FEC212F7FF5D9D05D@aesmail.surcouf.local>

This is bulleted as a new feature for v4.
Yet there is no way to test this without a quick reply letting me know the basic usage.
Anyone  got a snippet on how this is setup 

-----Original Message-----
From: squid-users [mailto:squid-users-bounces at lists.squid-cache.org] On Behalf Of Mike Surcouf
Sent: 07 March 2017 15:21
To: 'squid-users at lists.squid-cache.org'
Subject: [squid-users] kerb auth groups KV note acl config

Outputting the groups as KV pairs in AD environments  on auth seems like a great performance enhancement and will allow me to ditch my ldap lookups.
Is there any docs on how to set this up?
Even looking at the source I can't seem to work it out.
I would like to test and potentially contribute to the DOCS although I am only a git user and bazaar would be new to me so I may just post my experience in this thread.

From what I can see I need to setup a note acl but I am unsure of the key names etc.

A short example would be great.

Thanks

Mike
_______________________________________________
squid-users mailing list
squid-users at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-users

From squid3 at treenet.co.nz  Wed Mar 15 14:38:39 2017
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 16 Mar 2017 03:38:39 +1300
Subject: [squid-users] No failover when default parent proxy fails
 (Squid 3.5.12)
In-Reply-To: <trinity-daf2229d-e256-478a-9fd8-a5005f819ef5-1489558013937@3capp-gmx-bs10>
References: <trinity-daf2229d-e256-478a-9fd8-a5005f819ef5-1489558013937@3capp-gmx-bs10>
Message-ID: <5ccce9cb-0eff-65e8-ea4a-b57ff976bab7@treenet.co.nz>

On 15/03/2017 7:06 p.m., Jens Offenbach wrote:
> Hi,
> I have two parent proxies configured, but Squid seems to stick to the default proxy even when the proxy cannot be reached:
> 2017/03/15 06:40:13 kid1| TCP connection to proxy.mycompany.de/8080 failed
> 2017/03/15 06:40:13 kid1| Detected DEAD Parent: proxy.mycompany.de
> 2017/03/15 06:40:13 kid1| TCP connection to proxy.mycompany.de/8080 failed
> 2017/03/15 06:40:13 kid1| TCP connection to proxy.mycompany.de/8080 failed
> 2017/03/15 06:40:13 kid1| TCP connection to proxy.mycompany.de/8080 failed
> 2017/03/15 06:40:13 kid1| TCP connection to proxy.mycompany.de/8080 failed
> 2017/03/15 06:40:13 kid1| TCP connection to proxy.mycompany.de/8080 failed
> 2017/03/15 06:40:13 kid1| TCP connection to proxy.mycompany.de/8080 failed
> 2017/03/15 06:40:43 kid1| TCP connection to proxy.mycompany.de/8080 failed
> 2017/03/15 06:40:43 kid1| TCP connection to proxy.mycompany.de/8080 failed
> 2017/03/15 06:40:43 kid1| TCP connection to proxy.mycompany.de/8080 failed
> 2017/03/15 06:40:43 kid1| TCP connection to proxy.mycompany.de/8080 failed
> 2017/03/15 06:40:43 kid1| TCP connection to proxy.mycompany.de/8080 failed
> 2017/03/15 06:40:43 kid1| TCP connection to proxy.mycompany.de/8080 failed
> 2017/03/15 06:40:43 kid1| TCP connection to proxy.mycompany.de/8080 failed


Appearances can be deceiving at times. This shows that several "a
connections were attempted but does not specify if that was the orider
they were initiated or not.
 It also does not indicate whether that was HTTP or probe for
ressurected peer.

You have disabled the probes though (no-query no-digest) so it is
unlikely Squid will ever detect a DEAD peer as coming alive again. This
makes me suspect that the "roxy" peer was also detected dead some time
earlier. If that has happened then the "FIRST_AVAILABLE" peer selection
algorithm will produce no possible routes and Squid falls back to the
DEFAULT peer ... which happens to be that one you see in the log and
will try that peer until it works again.

If you want to see the exact route selection results add this to your
squid.conf:
 debug_options 44,2


Amos



From wolle5050 at gmx.de  Wed Mar 15 15:24:03 2017
From: wolle5050 at gmx.de (Jens Offenbach)
Date: Wed, 15 Mar 2017 16:24:03 +0100
Subject: [squid-users] No failover when default parent proxy fails
 (Squid 3.5.12)
In-Reply-To: <5ccce9cb-0eff-65e8-ea4a-b57ff976bab7@treenet.co.nz>
References: <trinity-daf2229d-e256-478a-9fd8-a5005f819ef5-1489558013937@3capp-gmx-bs10>, 
 <5ccce9cb-0eff-65e8-ea4a-b57ff976bab7@treenet.co.nz>
Message-ID: <trinity-c99337bb-091e-4bab-b503-32a36fa23906-1489591443212@3capp-gmx-bs10>

@Amos:
Thanks a lot for your help.

I have modified my squid.conf:
? cache_peer proxy.mycompany.de parent 8080 0 connect-timeout=5 connect-fail-limit=3 default
? cache_peer? roxy.mycompany.de parent 8080 0 connect-timeout=5 connect-fail-limit=3
?
Do you think this will work? It it currently a little bit difficult for my to test the new configuration.

Regards,
Jens


Gesendet:?Mittwoch, 15. M?rz 2017 um 15:38 Uhr
Von:?"Amos Jeffries" <squid3 at treenet.co.nz>
An:?squid-users at lists.squid-cache.org
Betreff:?Re: [squid-users] No failover when default parent proxy fails (Squid 3.5.12)
On 15/03/2017 7:06 p.m., Jens Offenbach wrote:
> Hi,
> I have two parent proxies configured, but Squid seems to stick to the default proxy even when the proxy cannot be reached:
> 2017/03/15 06:40:13 kid1| TCP connection to proxy.mycompany.de/8080 failed
> 2017/03/15 06:40:13 kid1| Detected DEAD Parent: proxy.mycompany.de
> 2017/03/15 06:40:13 kid1| TCP connection to proxy.mycompany.de/8080 failed
> 2017/03/15 06:40:13 kid1| TCP connection to proxy.mycompany.de/8080 failed
> 2017/03/15 06:40:13 kid1| TCP connection to proxy.mycompany.de/8080 failed
> 2017/03/15 06:40:13 kid1| TCP connection to proxy.mycompany.de/8080 failed
> 2017/03/15 06:40:13 kid1| TCP connection to proxy.mycompany.de/8080 failed
> 2017/03/15 06:40:13 kid1| TCP connection to proxy.mycompany.de/8080 failed
> 2017/03/15 06:40:43 kid1| TCP connection to proxy.mycompany.de/8080 failed
> 2017/03/15 06:40:43 kid1| TCP connection to proxy.mycompany.de/8080 failed
> 2017/03/15 06:40:43 kid1| TCP connection to proxy.mycompany.de/8080 failed
> 2017/03/15 06:40:43 kid1| TCP connection to proxy.mycompany.de/8080 failed
> 2017/03/15 06:40:43 kid1| TCP connection to proxy.mycompany.de/8080 failed
> 2017/03/15 06:40:43 kid1| TCP connection to proxy.mycompany.de/8080 failed
> 2017/03/15 06:40:43 kid1| TCP connection to proxy.mycompany.de/8080 failed


Appearances can be deceiving at times. This shows that several "a
connections were attempted but does not specify if that was the orider
they were initiated or not.
It also does not indicate whether that was HTTP or probe for
ressurected peer.

You have disabled the probes though (no-query no-digest) so it is
unlikely Squid will ever detect a DEAD peer as coming alive again. This
makes me suspect that the "roxy" peer was also detected dead some time
earlier. If that has happened then the "FIRST_AVAILABLE" peer selection
algorithm will produce no possible routes and Squid falls back to the
DEFAULT peer ... which happens to be that one you see in the log and
will try that peer until it works again.

If you want to see the exact route selection results add this to your
squid.conf:
debug_options 44,2


Amos

_______________________________________________
squid-users mailing list
squid-users at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-users


From wolle5050 at gmx.de  Thu Mar 16 05:23:40 2017
From: wolle5050 at gmx.de (Jens Offenbach)
Date: Thu, 16 Mar 2017 06:23:40 +0100
Subject: [squid-users] No failover when default parent proxy fails
 (Squid 3.5.12)
In-Reply-To: <5ccce9cb-0eff-65e8-ea4a-b57ff976bab7@treenet.co.nz>
References: <trinity-daf2229d-e256-478a-9fd8-a5005f819ef5-1489558013937@3capp-gmx-bs10>, 
 <5ccce9cb-0eff-65e8-ea4a-b57ff976bab7@treenet.co.nz>
Message-ID: <trinity-cda683b1-f582-48d5-a5c0-73130adfcf15-1489641820307@3capp-gmx-bs34>

Failover does not seem to work properly in case of HTTPS. When the primary parent proxy fails, it takes minutes until the download starts and in some cases it never starts.

Is there anything that must be configured specifially in case of HTTPS and timeouts for failover?

This is my squid.conf:
# ACCESS CONTROLS
# -----------------------------------------------------------------------------
  # Local Networks
  acl localnet src 139.2.0.0/16
  acl localnet src 193.96.112.0/21
  acl localnet src 192.109.216.0/24
  acl localnet src 100.1.4.0/22
  acl localnet src 10.0.0.0/8
  acl localnet src 172.16.0.0/12
  acl localnet src 192.168.0.0/16

  # mycompany Networks
  acl to_matnet dst 139.2.0.0/16
  acl to_matnet dst 193.96.112.0/21
  acl to_matnet dst 192.109.216.0/24
  acl to_matnet dst 100.1.4.0/22
  acl to_matnet dst 10.0.0.0/8
  acl to_matnet dst 172.16.0.0/12
  acl to_matnet dst 192.168.0.0/16

  # SSL-Ports
  acl SSL_ports port 443 # https
  acl SSL_ports port 563 # snews
  acl SSL_ports port 873 # rsync

  # Safe-Ports
  acl Safe_ports port 80  # http
  acl Safe_ports port 21  # ftp
  acl Safe_ports port 443 # https
  acl Safe_ports port 70  # gopher
  acl Safe_ports port 210 # wais
  acl Safe_ports port 1025-65535 # unregistered ports
  acl Safe_ports port 280 # http-mgmt
  acl Safe_ports port 488 # gss-http
  acl Safe_ports port 591 # filemaker
  acl Safe_ports port 777 # multiling http
  acl Safe_ports port 631 # cups
  acl Safe_ports port 873 # rsync
  acl Safe_ports port 901 # SWAT

  # HTTPS
  acl CONNECT method CONNECT

  http_access deny !Safe_ports
  http_access deny CONNECT !SSL_ports
  http_access allow manager localhost
  http_access deny  manager
  http_access allow localnet
  http_access allow localhost
  http_access deny all

# NETWORK OPTIONS
# -----------------------------------------------------------------------------
  http_port 3128

# OPTIONS WHICH AFFECT THE NEIGHBOR SELECTION ALGORITHM
# -----------------------------------------------------------------------------
  cache_peer proxy.mycompany.de parent 8080 0 no-digest no-query connect-timeout=5 connect-fail-limit=3 default
  cache_peer  roxy.mycompany.de parent 8080 0 no-digest no-query connect-timeout=5 connect-fail-limit=3

# MEMORY CACHE OPTIONS
# -----------------------------------------------------------------------------
  maximum_object_size_in_memory 8 MB
  memory_replacement_policy heap LFUDA
  cache_mem 256 MB

# DISK CACHE OPTIONS
# -----------------------------------------------------------------------------
  maximum_object_size 10 GB
  cache_replacement_policy heap GDSF
  cache_dir ufs /var/cache/squid 88894 16 256 max-size=10737418240

# LOGFILE OPTIONS
# -----------------------------------------------------------------------------
  access_log daemon:/var/log/squid/access.log squid

# OPTIONS FOR TROUBLESHOOTING
# -----------------------------------------------------------------------------
  cache_log /var/log/squid/cache.log
  coredump_dir /var/log/squid

# OPTIONS FOR TUNING THE CACHE
# -----------------------------------------------------------------------------
  max_stale 6 days
  shutdown_lifetime 5 seconds

# ADMINISTRATIVE PARAMETERS
# -----------------------------------------------------------------------------
  visible_hostname proxy.mycompany.com

# OPTIONS INFLUENCING REQUEST FORWARDING 
# -----------------------------------------------------------------------------
  always_direct allow to_matnet
  never_direct  allow all

# DNS OPTIONS
# -----------------------------------------------------------------------------
  dns_nameservers 139.2.34.171
  dns_nameservers 139.2.34.37

# MISCELLANEOUS
# -----------------------------------------------------------------------------
  memory_pools off

Regards,
Jens
?

Gesendet:?Mittwoch, 15. M?rz 2017 um 15:38 Uhr
Von:?"Amos Jeffries" <squid3 at treenet.co.nz>
An:?squid-users at lists.squid-cache.org
Betreff:?Re: [squid-users] No failover when default parent proxy fails (Squid 3.5.12)
On 15/03/2017 7:06 p.m., Jens Offenbach wrote:
> Hi,
> I have two parent proxies configured, but Squid seems to stick to the default proxy even when the proxy cannot be reached:
> 2017/03/15 06:40:13 kid1| TCP connection to proxy.mycompany.de/8080 failed
> 2017/03/15 06:40:13 kid1| Detected DEAD Parent: proxy.mycompany.de
> 2017/03/15 06:40:13 kid1| TCP connection to proxy.mycompany.de/8080 failed
> 2017/03/15 06:40:13 kid1| TCP connection to proxy.mycompany.de/8080 failed
> 2017/03/15 06:40:13 kid1| TCP connection to proxy.mycompany.de/8080 failed
> 2017/03/15 06:40:13 kid1| TCP connection to proxy.mycompany.de/8080 failed
> 2017/03/15 06:40:13 kid1| TCP connection to proxy.mycompany.de/8080 failed
> 2017/03/15 06:40:13 kid1| TCP connection to proxy.mycompany.de/8080 failed
> 2017/03/15 06:40:43 kid1| TCP connection to proxy.mycompany.de/8080 failed
> 2017/03/15 06:40:43 kid1| TCP connection to proxy.mycompany.de/8080 failed
> 2017/03/15 06:40:43 kid1| TCP connection to proxy.mycompany.de/8080 failed
> 2017/03/15 06:40:43 kid1| TCP connection to proxy.mycompany.de/8080 failed
> 2017/03/15 06:40:43 kid1| TCP connection to proxy.mycompany.de/8080 failed
> 2017/03/15 06:40:43 kid1| TCP connection to proxy.mycompany.de/8080 failed
> 2017/03/15 06:40:43 kid1| TCP connection to proxy.mycompany.de/8080 failed


Appearances can be deceiving at times. This shows that several "a
connections were attempted but does not specify if that was the orider
they were initiated or not.
It also does not indicate whether that was HTTP or probe for
ressurected peer.

You have disabled the probes though (no-query no-digest) so it is
unlikely Squid will ever detect a DEAD peer as coming alive again. This
makes me suspect that the "roxy" peer was also detected dead some time
earlier. If that has happened then the "FIRST_AVAILABLE" peer selection
algorithm will produce no possible routes and Squid falls back to the
DEFAULT peer ... which happens to be that one you see in the log and
will try that peer until it works again.

If you want to see the exact route selection results add this to your
squid.conf:
debug_options 44,2


Amos

_______________________________________________
squid-users mailing list
squid-users at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-users


From rousskov at measurement-factory.com  Thu Mar 16 05:37:18 2017
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Wed, 15 Mar 2017 23:37:18 -0600
Subject: [squid-users] No failover when default parent proxy fails
 (Squid 3.5.12)
In-Reply-To: <trinity-cda683b1-f582-48d5-a5c0-73130adfcf15-1489641820307@3capp-gmx-bs34>
References: <trinity-daf2229d-e256-478a-9fd8-a5005f819ef5-1489558013937@3capp-gmx-bs10>
 <5ccce9cb-0eff-65e8-ea4a-b57ff976bab7@treenet.co.nz>
 <trinity-cda683b1-f582-48d5-a5c0-73130adfcf15-1489641820307@3capp-gmx-bs34>
Message-ID: <821f2fe9-112b-5ec2-24d4-c6494fd5b1e6@measurement-factory.com>

On 03/15/2017 11:23 PM, Jens Offenbach wrote:
> Failover does not seem to work properly in case of HTTPS. 

We have recently discovered and fixed two bugs in failure recovery code
code for tunneled (not bumped) HTTPS connections:
http://lists.squid-cache.org/pipermail/squid-dev/2017-March/008243.html

I do not know whether our (not yet officially reviewed) fixes will help
in your use case and whether you can try v4 or v5 to test them.


HTH,

Alex.



From wolle5050 at gmx.de  Thu Mar 16 06:05:17 2017
From: wolle5050 at gmx.de (Jens Offenbach)
Date: Thu, 16 Mar 2017 07:05:17 +0100
Subject: [squid-users] No failover when default parent proxy fails
 (Squid 3.5.12)
In-Reply-To: <821f2fe9-112b-5ec2-24d4-c6494fd5b1e6@measurement-factory.com>
References: <trinity-daf2229d-e256-478a-9fd8-a5005f819ef5-1489558013937@3capp-gmx-bs10>
 <5ccce9cb-0eff-65e8-ea4a-b57ff976bab7@treenet.co.nz>
 <trinity-cda683b1-f582-48d5-a5c0-73130adfcf15-1489641820307@3capp-gmx-bs34>,
 <821f2fe9-112b-5ec2-24d4-c6494fd5b1e6@measurement-factory.com>
Message-ID: <trinity-d692acfc-f31f-4362-a19b-dcb276a0d962-1489644317027@3capp-gmx-bs34>

Thanks for your quick response...

I have also configured, but the value seems not to be honored:
connect_timeout 30 seconds

The primary peer is down, but Squid does not print any "Dead parent" in the logs. Every HTTPS request is forwarded to the primary peer and it takes 1 minute until the secondary peer gets used, even with "connect_timeout 30 seconds". I think, I am facing the first issue that has been fixed by your patch.

Are there any plans to backport this fix to Xenial APT repositories or to create a new Debian package for Squid4/5?

Regards,
Jens?


Gesendet:?Donnerstag, 16. M?rz 2017 um 06:37 Uhr
Von:?"Alex Rousskov" <rousskov at measurement-factory.com>
An:?squid-users at lists.squid-cache.org
Cc:?"Jens Offenbach" <wolle5050 at gmx.de>
Betreff:?Re: [squid-users] No failover when default parent proxy fails (Squid 3.5.12)
On 03/15/2017 11:23 PM, Jens Offenbach wrote:
> Failover does not seem to work properly in case of HTTPS.

We have recently discovered and fixed two bugs in failure recovery code
code for tunneled (not bumped) HTTPS connections:
http://lists.squid-cache.org/pipermail/squid-dev/2017-March/008243.html

I do not know whether our (not yet officially reviewed) fixes will help
in your use case and whether you can try v4 or v5 to test them.


HTH,

Alex.
?


From squid3 at treenet.co.nz  Thu Mar 16 08:27:36 2017
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 16 Mar 2017 21:27:36 +1300
Subject: [squid-users] No failover when default parent proxy fails
 (Squid 3.5.12)
In-Reply-To: <trinity-d692acfc-f31f-4362-a19b-dcb276a0d962-1489644317027@3capp-gmx-bs34>
References: <trinity-daf2229d-e256-478a-9fd8-a5005f819ef5-1489558013937@3capp-gmx-bs10>
 <5ccce9cb-0eff-65e8-ea4a-b57ff976bab7@treenet.co.nz>
 <trinity-cda683b1-f582-48d5-a5c0-73130adfcf15-1489641820307@3capp-gmx-bs34>
 <821f2fe9-112b-5ec2-24d4-c6494fd5b1e6@measurement-factory.com>
 <trinity-d692acfc-f31f-4362-a19b-dcb276a0d962-1489644317027@3capp-gmx-bs34>
Message-ID: <046af0e9-0baa-2b1b-164e-9edb0de5fa69@treenet.co.nz>

On 16/03/2017 7:05 p.m., Jens Offenbach wrote:
> Thanks for your quick response...
> 
> I have also configured, but the value seems not to be honored: 
> connect_timeout 30 seconds
> 
> The primary peer is down, but Squid does not print any "Dead parent"
> in the logs. Every HTTPS request is forwarded to the primary peer and
> it takes 1 minute until the secondary peer gets used, even with
> "connect_timeout 30 seconds". I think, I am facing the first issue
> that has been fixed by your patch.
> 

The global config options being ignored completely is correct because
your peer have individual connect-timeout=5 settings.

So, those 5sec timeouts should be used instead now as before.

Though note that they apply only to how long a TCP connection (SYN,
SYN-ACK) is waited for. There is also a dns_timeout and peer selection
timeout that apply separately to the act of connecting. And a
forward_timeout global limit that all those operations have to fit
within, including retries.


Did you have a chance to try the debug setting I suggested at the
beginning? That will give you an immediate view about what Squid is
detecting as usable paths for each and every request and at what times
relative to the DEAD/LIVE notice.



> Are there any plans to backport this fix to Xenial APT repositories
> or to create a new Debian package for Squid4/5?

That is up to the Ubuntu server team, but I think it Unlikely. Zesty is
the current stable and things like this generally dont have enough
widespread impact to qualify for LTS backports.

Debian is now frozen to stabilize for the "Buster" release, that will
contain Squid-3.5.23 plus some few critical patches which are already
set. A Squid-4 package is ready and waiting for the release freeze to
end before it goes public in the Debian Unstable/Testing repos.

Amos



From squid3 at treenet.co.nz  Thu Mar 16 09:23:30 2017
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 16 Mar 2017 22:23:30 +1300
Subject: [squid-users] kerb auth groups KV note acl config
In-Reply-To: <2197768425D7F5479A0FFB3FEC212F7FF5D9D05D@aesmail.surcouf.local>
References: <2197768425D7F5479A0FFB3FEC212F7FF5D88ABD@aesmail.surcouf.local>
 <2197768425D7F5479A0FFB3FEC212F7FF5D9D05D@aesmail.surcouf.local>
Message-ID: <04277d89-6f12-31e0-f0e9-d01fe6134b0c@treenet.co.nz>

On 15/03/2017 10:18 p.m., Mike Surcouf wrote:
> This is bulleted as a new feature for v4.
> Yet there is no way to test this without a quick reply letting me know the basic usage.
> Anyone  got a snippet on how this is setup 
> 

[ For TL;DR skip to the end of this mail. All this is first block is
just describing how it works. ]


This should be doable with Squid-3.4+ or at least 3.5. It requires only
the note ACL in squid plus a helper that sends group= response annotations.

It is marked as v4 becasue that is where the first helper with such
support is bundled. You can run that helper with older Squid, for
example by downloading Markus lastest release and building your own helper.


An auth helper which supports it does not needs anything configured by
you. It will "just work" (or not if it lacks annotation support). That
part is just a matter of finding out / ensuring your auth helper
provides the group kv-pairs. The usual command-line tests can probably
show that.

The auth helper by Markus should be producing a set of group=X
annotations automatically, one for each group the user is a member of.
Where the X is what AD calls a "SID" value representing a unique ID for
each group.


After those are received by Squid the note ACL type can be used in
squid.conf to match any of them quickly without an external helper
lookup for the group details. That enables reliable group ACLs anywhere
in squid.conf where they were previously at the mercy of external helper
result timeouts.


In absence of that input from the auth helper, an external_acl_type
helper or *any* helper really :-) can also send the same annotations to
Squid - with the same note ACL config later.

In its current form this is obviously most useful if you know the SID
that group names map to and can configure the note ACL appropriately. I
am hopeful that other helpers may be able to produce named groups or
such. But the values are likely to be specific to whatever the auth
system can provide.


For group lookup and comparison by name (the 'old' way) you can still
use an external helper. As I understand it AD requires two lookups; one
to find the users SID memberships and one to find the group name->SID
mapping for the group(s) being checked - then compare. The first is not
needed if the SID (%note{group}) is passed to the helper instead of
username (%LOGIN).
 This part does require v4, and has not been much tested to see where
the %note format code works for external_acl_type helpers (and where
not). YMMV.

IIRC Markus was waiting on support for %note{group} format code on
external_acl_type config lines. But that happened a long while back now.



> -----Original Message-----
> From: Mike Surcouf
> 
> Outputting the groups as KV pairs in AD environments  on auth seems like a great performance enhancement and will allow me to ditch my ldap lookups.
> Is there any docs on how to set this up?
> Even looking at the source I can't seem to work it out.
> I would like to test and potentially contribute to the DOCS although I am only a git user and bazaar would be new to me so I may just post my experience in this thread.
> 
> From what I can see I need to setup a note acl but I am unsure of the key names etc.

Correct. The key name is "group" ;-)


> 
> A short example would be great.
> 

As far as I am aware it should look like this:

  acl blah note group SID-12345-762576257263
  request_max_size 1 MB blah

Maybe also the -m flag on the ACL definition if recent changes merged
the group notes into a list.

HTH
Amos



From wolle5050 at gmx.de  Thu Mar 16 09:39:55 2017
From: wolle5050 at gmx.de (Jens Offenbach)
Date: Thu, 16 Mar 2017 10:39:55 +0100
Subject: [squid-users] No failover when default parent proxy fails
 (Squid 3.5.12)
In-Reply-To: <046af0e9-0baa-2b1b-164e-9edb0de5fa69@treenet.co.nz>
References: <trinity-daf2229d-e256-478a-9fd8-a5005f819ef5-1489558013937@3capp-gmx-bs10>
 <5ccce9cb-0eff-65e8-ea4a-b57ff976bab7@treenet.co.nz>
 <trinity-cda683b1-f582-48d5-a5c0-73130adfcf15-1489641820307@3capp-gmx-bs34>
 <821f2fe9-112b-5ec2-24d4-c6494fd5b1e6@measurement-factory.com>
 <trinity-d692acfc-f31f-4362-a19b-dcb276a0d962-1489644317027@3capp-gmx-bs34>,
 <046af0e9-0baa-2b1b-164e-9edb0de5fa69@treenet.co.nz>
Message-ID: <trinity-b14145da-c806-416a-9ebb-7db200b0e4d8-1489657195890@3capp-gmx-bs34>

This is the sceanrio;

Squid 3.5.12 is installed on "squid-proxy.mycompany.com". The two parent proxies are:
- Primary: proxy.mycompany.de:8080 (139.2.1.3)
- Fallback: roxy.mycompany.de:8080 (139.2.1.4)

I have misunderstood the "default" option in "cache_peer". When I got it right, it has the meaning of a fallback, so I switched it to "roxy.mycompany.de". "proxy.mycompany.de" should always be used and "roxy.mycompany.de" only when "proxy.mycompany.de" fails.

squid.conf:

# ACCESS CONTROLS
# -----------------------------------------------------------------------------
  # Local Networks
  acl localnet src 139.2.0.0/16
  acl localnet src 193.96.112.0/21
  acl localnet src 192.109.216.0/24
  acl localnet src 100.1.4.0/22
  acl localnet src 10.0.0.0/8
  acl localnet src 172.16.0.0/12
  acl localnet src 192.168.0.0/16

  # Materna Networks
  acl to_matnet dst 139.2.0.0/16
  acl to_matnet dst 193.96.112.0/21
  acl to_matnet dst 192.109.216.0/24
  acl to_matnet dst 100.1.4.0/22
  acl to_matnet dst 10.0.0.0/8
  acl to_matnet dst 172.16.0.0/12
  acl to_matnet dst 192.168.0.0/16

  # SSL-Ports
  acl SSL_ports port 443 # https
  acl SSL_ports port 563 # snews
  acl SSL_ports port 873 # rsync

  # Safe-Ports
  acl Safe_ports port 80  # http
  acl Safe_ports port 21  # ftp
  acl Safe_ports port 443 # https
  acl Safe_ports port 70  # gopher
  acl Safe_ports port 210 # wais
  acl Safe_ports port 1025-65535 # unregistered ports
  acl Safe_ports port 280 # http-mgmt
  acl Safe_ports port 488 # gss-http
  acl Safe_ports port 591 # filemaker
  acl Safe_ports port 777 # multiling http
  acl Safe_ports port 631 # cups
  acl Safe_ports port 873 # rsync
  acl Safe_ports port 901 # SWAT

  # HTTPS
  acl CONNECT method CONNECT

  http_access deny !Safe_ports
  http_access deny CONNECT !SSL_ports
  http_access allow manager localhost
  http_access deny  manager
  http_access allow localnet
  http_access allow localhost
  http_access deny all

# NETWORK OPTIONS
# -----------------------------------------------------------------------------
  http_port 3128
  http_port 3129 intercept

# OPTIONS WHICH AFFECT THE NEIGHBOR SELECTION ALGORITHM
# -----------------------------------------------------------------------------
  cache_peer proxy.materna.de parent 8080 0 no-digest no-query connect-timeout=5 connect-fail-limit=2
  cache_peer  roxy.materna.de parent 8080 0 no-digest no-query connect-timeout=5 connect-fail-limit=2 default

# MEMORY CACHE OPTIONS
# -----------------------------------------------------------------------------
  maximum_object_size_in_memory 8 MB
  memory_replacement_policy heap LFUDA
  cache_mem 256 MB

# DISK CACHE OPTIONS
# -----------------------------------------------------------------------------
  maximum_object_size 10 GB
  cache_replacement_policy heap GDSF
  cache_dir ufs /var/cache/squid 88894 16 256 max-size=10737418240

# LOGFILE OPTIONS
# -----------------------------------------------------------------------------
  access_log daemon:/var/log/squid/access.log squid

# OPTIONS FOR TROUBLESHOOTING
# -----------------------------------------------------------------------------
  cache_log /var/log/squid/cache.log
  coredump_dir /var/log/squid
  debug_options 44,2

# OPTIONS FOR TUNING THE CACHE
# -----------------------------------------------------------------------------
  max_stale 6 days
  shutdown_lifetime 5 seconds

# ADMINISTRATIVE PARAMETERS
# -----------------------------------------------------------------------------
  visible_hostname proxy.materna.com

# OPTIONS INFLUENCING REQUEST FORWARDING 
# -----------------------------------------------------------------------------
  always_direct allow to_matnet
  never_direct  allow all

# DNS OPTIONS
# -----------------------------------------------------------------------------
  dns_nameservers 139.2.34.171
  dns_nameservers 139.2.34.37

# MISCELLANEOUS
# -----------------------------------------------------------------------------
  memory_pools off

Now, I block traffic on "squid-proxy.mycompany.com" to the primary proxy "proxy.mycompany.de" (139.2.1.3) using IPTables:
$ iptables -A OUTPUT -p icmp -d 139.2.1.3 -j DROP
$ iptables -A OUTPUT -p tcp -d 139.2.1.3 -j DROP
$ iptables -A OUTPUT -p udp -d 139.2.1.3 -j DROP

On the test machine, I use:
$ export http_proxy=http://squid-proxy.mycompany.com:3128/
$ export https_proxy=http://squid-proxy.mycompany.com:3128/
$ export HTTP_PROXY=http://squid-proxy.mycompany.com:3128/
$ export HTTPS_PROXY=http://squid-proxy.mycompany.com:3128/

Trying to download a resource:
$ wget https://repository.apache.org/content/groups/snapshots/org/apache/karaf/apache-karaf/4.1.1-SNAPSHOT/apache-karaf-4.1.1-20170315.084054-35.tar.gz

The download hangs for 2 minutes until it gets started. A retry shows the same results, the download starts after 2 minutes showing:
--2017-03-16 09:31:26--  https://repository.apache.org/content/groups/snapshots/org/apache/karaf/apache-karaf/4.1.1-SNAPSHOT/apache-karaf-4.1.1-20170314.154157-34.tar.gz
Resolving squid-proxy.mycompany.com (squid-proxy.mycompany.com)... 10.152.132.41
Connecting to squid-proxy.mycompany.com (squid-proxy.mycompany.com)|10.152.132.41|:3128... connected.

cache.log:

2017/03/16 10:17:47 kid1| Shutdown: NTLM authentication.
2017/03/16 10:17:47 kid1| Shutdown: Negotiate authentication.
2017/03/16 10:17:47 kid1| Shutdown: Digest authentication.
2017/03/16 10:17:47 kid1| Shutdown: Basic authentication.
CPU Usage: 0.084 seconds = 0.052 user + 0.032 sys
Maximum Resident Size: 113840 KB
Page faults with physical i/o: 0
2017/03/16 10:17:48 kid1| Starting Squid Cache version 3.5.12 for x86_64-pc-linux-gnu...
2017/03/16 10:17:48 kid1| Service Name: squid
2017/03/16 10:17:48| pinger: Initialising ICMP pinger ...
2017/03/16 10:18:09.579 kid1| 44,2| peer_select.cc(258) peerSelectDnsPaths: Find IP destination for: http://proxy.materna.de:8080/squid-internal-dynamic/netdb' via proxy.materna.de
2017/03/16 10:18:09.579 kid1| 44,2| peer_select.cc(280) peerSelectDnsPaths: Found sources for 'http://proxy.materna.de:8080/squid-internal-dynamic/netdb'
2017/03/16 10:18:09.579 kid1| 44,2| peer_select.cc(281) peerSelectDnsPaths:   always_direct = ALLOWED
2017/03/16 10:18:09.579 kid1| 44,2| peer_select.cc(282) peerSelectDnsPaths:    never_direct = DUNNO
2017/03/16 10:18:09.579 kid1| 44,2| peer_select.cc(286) peerSelectDnsPaths:          DIRECT = local=0.0.0.0 remote=139.2.1.3:8080 flags=1
2017/03/16 10:18:09.579 kid1| 44,2| peer_select.cc(295) peerSelectDnsPaths:        timedout = 0
2017/03/16 10:18:12.279 kid1| 44,2| peer_select.cc(258) peerSelectDnsPaths: Find IP destination for: http://roxy.materna.de:8080/squid-internal-dynamic/netdb' via roxy.materna.de
2017/03/16 10:18:12.279 kid1| 44,2| peer_select.cc(280) peerSelectDnsPaths: Found sources for 'http://roxy.materna.de:8080/squid-internal-dynamic/netdb'
2017/03/16 10:18:12.279 kid1| 44,2| peer_select.cc(281) peerSelectDnsPaths:   always_direct = ALLOWED
2017/03/16 10:18:12.279 kid1| 44,2| peer_select.cc(282) peerSelectDnsPaths:    never_direct = DUNNO
2017/03/16 10:18:12.279 kid1| 44,2| peer_select.cc(286) peerSelectDnsPaths:          DIRECT = local=0.0.0.0 remote=139.2.1.4:8080 flags=1
2017/03/16 10:18:12.279 kid1| 44,2| peer_select.cc(295) peerSelectDnsPaths:        timedout = 0
2017/03/16 10:18:37.951 kid1| 44,2| peer_select.cc(258) peerSelectDnsPaths: Find IP destination for: repository.apache.org:443' via proxy.materna.de
2017/03/16 10:18:37.951 kid1| 44,2| peer_select.cc(258) peerSelectDnsPaths: Find IP destination for: repository.apache.org:443' via proxy.materna.de
2017/03/16 10:18:37.951 kid1| 44,2| peer_select.cc(258) peerSelectDnsPaths: Find IP destination for: repository.apache.org:443' via roxy.materna.de
2017/03/16 10:18:37.951 kid1| 44,2| peer_select.cc(258) peerSelectDnsPaths: Find IP destination for: repository.apache.org:443' via roxy.materna.de
2017/03/16 10:18:37.951 kid1| 44,2| peer_select.cc(280) peerSelectDnsPaths: Found sources for 'repository.apache.org:443'
2017/03/16 10:18:37.951 kid1| 44,2| peer_select.cc(281) peerSelectDnsPaths:   always_direct = DENIED
2017/03/16 10:18:37.951 kid1| 44,2| peer_select.cc(282) peerSelectDnsPaths:    never_direct = ALLOWED
2017/03/16 10:18:37.951 kid1| 44,2| peer_select.cc(292) peerSelectDnsPaths:      cache_peer = local=0.0.0.0 remote=139.2.1.3:8080 flags=1
2017/03/16 10:18:37.951 kid1| 44,2| peer_select.cc(292) peerSelectDnsPaths:      cache_peer = local=0.0.0.0 remote=139.2.1.3:8080 flags=1
2017/03/16 10:18:37.951 kid1| 44,2| peer_select.cc(292) peerSelectDnsPaths:      cache_peer = local=0.0.0.0 remote=139.2.1.4:8080 flags=1
2017/03/16 10:18:37.951 kid1| 44,2| peer_select.cc(292) peerSelectDnsPaths:      cache_peer = local=0.0.0.0 remote=139.2.1.4:8080 flags=1
2017/03/16 10:18:37.951 kid1| 44,2| peer_select.cc(295) peerSelectDnsPaths:        timedout = 0

access.log

1489656077.628 159679 10.30.216.160 TCP_TUNNEL/200 26328966 CONNECT repository.apache.org:443 - ANY_OLD_PARENT/139.2.1.4 -


Any hints?

Jens
?

Gesendet:?Donnerstag, 16. M?rz 2017 um 09:27 Uhr
Von:?"Amos Jeffries" <squid3 at treenet.co.nz>
An:?squid-users at lists.squid-cache.org
Betreff:?Re: [squid-users] No failover when default parent proxy fails (Squid 3.5.12)
On 16/03/2017 7:05 p.m., Jens Offenbach wrote:
> Thanks for your quick response...
>
> I have also configured, but the value seems not to be honored:
> connect_timeout 30 seconds
>
> The primary peer is down, but Squid does not print any "Dead parent"
> in the logs. Every HTTPS request is forwarded to the primary peer and
> it takes 1 minute until the secondary peer gets used, even with
> "connect_timeout 30 seconds". I think, I am facing the first issue
> that has been fixed by your patch.
>

The global config options being ignored completely is correct because
your peer have individual connect-timeout=5 settings.

So, those 5sec timeouts should be used instead now as before.

Though note that they apply only to how long a TCP connection (SYN,
SYN-ACK) is waited for. There is also a dns_timeout and peer selection
timeout that apply separately to the act of connecting. And a
forward_timeout global limit that all those operations have to fit
within, including retries.


Did you have a chance to try the debug setting I suggested at the
beginning? That will give you an immediate view about what Squid is
detecting as usable paths for each and every request and at what times
relative to the DEAD/LIVE notice.



> Are there any plans to backport this fix to Xenial APT repositories
> or to create a new Debian package for Squid4/5?

That is up to the Ubuntu server team, but I think it Unlikely. Zesty is
the current stable and things like this generally dont have enough
widespread impact to qualify for LTS backports.

Debian is now frozen to stabilize for the "Buster" release, that will
contain Squid-3.5.23 plus some few critical patches which are already
set. A Squid-4 package is ready and waiting for the release freeze to
end before it goes public in the Debian Unstable/Testing repos.

Amos

_______________________________________________
squid-users mailing list
squid-users at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-users


From mikes at surcouf.co.uk  Thu Mar 16 10:12:42 2017
From: mikes at surcouf.co.uk (Mike Surcouf)
Date: Thu, 16 Mar 2017 10:12:42 +0000
Subject: [squid-users] kerb auth groups KV note acl config
In-Reply-To: <04277d89-6f12-31e0-f0e9-d01fe6134b0c@treenet.co.nz>
References: <2197768425D7F5479A0FFB3FEC212F7FF5D88ABD@aesmail.surcouf.local>
 <2197768425D7F5479A0FFB3FEC212F7FF5D9D05D@aesmail.surcouf.local>
 <04277d89-6f12-31e0-f0e9-d01fe6134b0c@treenet.co.nz>
Message-ID: <2197768425D7F5479A0FFB3FEC212F7FF5D9FA95@aesmail.surcouf.local>

@Amos

Thanks for this

so to recap if I currently have

auth_param negotiate program /usr/lib64/squid/negotiate_kerberos_auth
auth_param negotiate children 20
auth_param negotiate keep_alive on

external_acl_type InternetAccessBanking %LOGIN /usr/lib64/squid/ext_kerberos_ldap_group_acl -u ldaps://aesdc02.surcouf.local:636 -b cn=SSSUsers,dc=surcouf,dc=local  -g InternetAccessBanking

I could replace it by

auth_param negotiate program /usr/lib64/squid/negotiate_kerberos_auth
auth_param negotiate children 20
auth_param negotiate keep_alive

acl InternetAccessBanking note group S-1-5-21-123456789-123456789-123456789-1234


Note where S-1-5-21-123456789-123456789-123456789-1234 is the SID for the group InternetAccessBanking


-----Original Message-----
From: squid-users [mailto:squid-users-bounces at lists.squid-cache.org] On Behalf Of Amos Jeffries
Sent: 16 March 2017 09:24
To: squid-users at lists.squid-cache.org
Subject: Re: [squid-users] kerb auth groups KV note acl config

On 15/03/2017 10:18 p.m., Mike Surcouf wrote:
> This is bulleted as a new feature for v4.
> Yet there is no way to test this without a quick reply letting me know the basic usage.
> Anyone  got a snippet on how this is setup 
> 

[ For TL;DR skip to the end of this mail. All this is first block is
just describing how it works. ]


This should be doable with Squid-3.4+ or at least 3.5. It requires only
the note ACL in squid plus a helper that sends group= response annotations.

It is marked as v4 becasue that is where the first helper with such
support is bundled. You can run that helper with older Squid, for
example by downloading Markus lastest release and building your own helper.


An auth helper which supports it does not needs anything configured by
you. It will "just work" (or not if it lacks annotation support). That
part is just a matter of finding out / ensuring your auth helper
provides the group kv-pairs. The usual command-line tests can probably
show that.

The auth helper by Markus should be producing a set of group=X
annotations automatically, one for each group the user is a member of.
Where the X is what AD calls a "SID" value representing a unique ID for
each group.


After those are received by Squid the note ACL type can be used in
squid.conf to match any of them quickly without an external helper
lookup for the group details. That enables reliable group ACLs anywhere
in squid.conf where they were previously at the mercy of external helper
result timeouts.


In absence of that input from the auth helper, an external_acl_type
helper or *any* helper really :-) can also send the same annotations to
Squid - with the same note ACL config later.

In its current form this is obviously most useful if you know the SID
that group names map to and can configure the note ACL appropriately. I
am hopeful that other helpers may be able to produce named groups or
such. But the values are likely to be specific to whatever the auth
system can provide.


For group lookup and comparison by name (the 'old' way) you can still
use an external helper. As I understand it AD requires two lookups; one
to find the users SID memberships and one to find the group name->SID
mapping for the group(s) being checked - then compare. The first is not
needed if the SID (%note{group}) is passed to the helper instead of
username (%LOGIN).
 This part does require v4, and has not been much tested to see where
the %note format code works for external_acl_type helpers (and where
not). YMMV.

IIRC Markus was waiting on support for %note{group} format code on
external_acl_type config lines. But that happened a long while back now.



> -----Original Message-----
> From: Mike Surcouf
> 
> Outputting the groups as KV pairs in AD environments  on auth seems like a great performance enhancement and will allow me to ditch my ldap lookups.
> Is there any docs on how to set this up?
> Even looking at the source I can't seem to work it out.
> I would like to test and potentially contribute to the DOCS although I am only a git user and bazaar would be new to me so I may just post my experience in this thread.
> 
> From what I can see I need to setup a note acl but I am unsure of the key names etc.

Correct. The key name is "group" ;-)


> 
> A short example would be great.
> 

As far as I am aware it should look like this:

  acl blah note group SID-12345-762576257263
  request_max_size 1 MB blah

Maybe also the -m flag on the ACL definition if recent changes merged
the group notes into a list.

HTH
Amos

_______________________________________________
squid-users mailing list
squid-users at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-users

From squid3 at treenet.co.nz  Thu Mar 16 10:51:48 2017
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 16 Mar 2017 23:51:48 +1300
Subject: [squid-users] No failover when default parent proxy fails
 (Squid 3.5.12)
In-Reply-To: <trinity-b14145da-c806-416a-9ebb-7db200b0e4d8-1489657195890@3capp-gmx-bs34>
References: <trinity-daf2229d-e256-478a-9fd8-a5005f819ef5-1489558013937@3capp-gmx-bs10>
 <5ccce9cb-0eff-65e8-ea4a-b57ff976bab7@treenet.co.nz>
 <trinity-cda683b1-f582-48d5-a5c0-73130adfcf15-1489641820307@3capp-gmx-bs34>
 <821f2fe9-112b-5ec2-24d4-c6494fd5b1e6@measurement-factory.com>
 <trinity-d692acfc-f31f-4362-a19b-dcb276a0d962-1489644317027@3capp-gmx-bs34>
 <046af0e9-0baa-2b1b-164e-9edb0de5fa69@treenet.co.nz>
 <trinity-b14145da-c806-416a-9ebb-7db200b0e4d8-1489657195890@3capp-gmx-bs34>
Message-ID: <0796b97d-2fdf-411d-9fa8-82dcefcc956d@treenet.co.nz>

On 16/03/2017 10:39 p.m., Jens Offenbach wrote:
> This is the sceanrio;
> 
> Squid 3.5.12 is installed on "squid-proxy.mycompany.com". The two parent proxies are:
> - Primary: proxy.mycompany.de:8080 (139.2.1.3)
> - Fallback: roxy.mycompany.de:8080 (139.2.1.4)
> 
> I have misunderstood the "default" option in "cache_peer". When I got it right, it has the meaning of a fallback, so I switched it to "roxy.mycompany.de". "proxy.mycompany.de" should always be used and "roxy.mycompany.de" only when "proxy.mycompany.de" fails.
> 

Well, kind of. Unless that peer is selected by one of the other
algorithms (for that it has to be 'alive') it will be appended as the
last-resort peer to be used regardless of DEAD/alive status.


> squid.conf:
> 
...
> 
> # OPTIONS WHICH AFFECT THE NEIGHBOR SELECTION ALGORITHM
> # -----------------------------------------------------------------------------
>   cache_peer proxy.materna.de parent 8080 0 no-digest no-query connect-timeout=5 connect-fail-limit=2
>   cache_peer  roxy.materna.de parent 8080 0 no-digest no-query connect-timeout=5 connect-fail-limit=2 default
> 
...
> # OPTIONS INFLUENCING REQUEST FORWARDING 
> # -----------------------------------------------------------------------------
>   always_direct allow to_matnet
>   never_direct  allow all
> 
> # DNS OPTIONS
> # -----------------------------------------------------------------------------
>   dns_nameservers 139.2.34.171
>   dns_nameservers 139.2.34.37
> 
...
> 
> Now, I block traffic on "squid-proxy.mycompany.com" to the primary proxy "proxy.mycompany.de" (139.2.1.3) using IPTables:
> $ iptables -A OUTPUT -p icmp -d 139.2.1.3 -j DROP
> $ iptables -A OUTPUT -p tcp -d 139.2.1.3 -j DROP
> $ iptables -A OUTPUT -p udp -d 139.2.1.3 -j DROP
> 

Are you trying to test connection timeout issues or a host going offline?
These iptables rules will force a timeout but not emulate a host
disconnection. Particularly when ICMP is also dropped.

When a host disconnects Squid will receive active signals (maybe via
ICMP) that the TCP SYN packet cannot get through. That speeds failure
recovery things up enormously. If the peer software simply
crashes/exits, different signals happen but with the same super fast
effects.

REJECT rules would be a better emulation of a machine disconnecting, or
an only-TCP REJECT rule emulates a peer software crash, etc. That way
the ICMP signalling still happens similar to those types of failure.



> On the test machine, I use:
> $ export http_proxy=http://squid-proxy.mycompany.com:3128/
> $ export https_proxy=http://squid-proxy.mycompany.com:3128/
> $ export HTTP_PROXY=http://squid-proxy.mycompany.com:3128/
> $ export HTTPS_PROXY=http://squid-proxy.mycompany.com:3128/
> 
> Trying to download a resource:
> $ wget https://repository.apache.org/content/groups/snapshots/org/apache/karaf/apache-karaf/4.1.1-SNAPSHOT/apache-karaf-4.1.1-20170315.084054-35.tar.gz
> 
> The download hangs for 2 minutes until it gets started. A retry shows the same results, the download starts after 2 minutes showing:
> --2017-03-16 09:31:26--  https://repository.apache.org/content/groups/snapshots/org/apache/karaf/apache-karaf/4.1.1-SNAPSHOT/apache-karaf-4.1.1-20170314.154157-34.tar.gz
> Resolving squid-proxy.mycompany.com (squid-proxy.mycompany.com)... 10.152.132.41
> Connecting to squid-proxy.mycompany.com (squid-proxy.mycompany.com)|10.152.132.41|:3128... connected.
> 
> cache.log:
> 
...
> 2017/03/16 10:17:48 kid1| Starting Squid Cache version 3.5.12 for x86_64-pc-linux-gnu...
> 2017/03/16 10:17:48 kid1| Service Name: squid
> 2017/03/16 10:17:48| pinger: Initialising ICMP pinger ...
> 2017/03/16 10:18:09.579 kid1| 44,2| peer_select.cc(258) peerSelectDnsPaths: Find IP destination for: http://proxy.materna.de:8080/squid-internal-dynamic/netdb' via proxy.materna.de
> 2017/03/16 10:18:09.579 kid1| 44,2| peer_select.cc(280) peerSelectDnsPaths: Found sources for 'http://proxy.materna.de:8080/squid-internal-dynamic/netdb'

These can be avoided by adding no-netdb-exchange option to the
cache_peer config lines. But it is probably a good idea to keep them for
production use as they will be the way of detecting a peer recovery to
live status.

...
> 2017/03/16 10:18:37.951 kid1| 44,2| peer_select.cc(258) peerSelectDnsPaths: Find IP destination for: repository.apache.org:443' via proxy.materna.de
> 2017/03/16 10:18:37.951 kid1| 44,2| peer_select.cc(258) peerSelectDnsPaths: Find IP destination for: repository.apache.org:443' via proxy.materna.de
> 2017/03/16 10:18:37.951 kid1| 44,2| peer_select.cc(258) peerSelectDnsPaths: Find IP destination for: repository.apache.org:443' via roxy.materna.de
> 2017/03/16 10:18:37.951 kid1| 44,2| peer_select.cc(258) peerSelectDnsPaths: Find IP destination for: repository.apache.org:443' via roxy.materna.de
> 2017/03/16 10:18:37.951 kid1| 44,2| peer_select.cc(280) peerSelectDnsPaths: Found sources for 'repository.apache.org:443'
> 2017/03/16 10:18:37.951 kid1| 44,2| peer_select.cc(281) peerSelectDnsPaths:   always_direct = DENIED
> 2017/03/16 10:18:37.951 kid1| 44,2| peer_select.cc(282) peerSelectDnsPaths:    never_direct = ALLOWED
> 2017/03/16 10:18:37.951 kid1| 44,2| peer_select.cc(292) peerSelectDnsPaths:      cache_peer = local=0.0.0.0 remote=139.2.1.3:8080 flags=1
> 2017/03/16 10:18:37.951 kid1| 44,2| peer_select.cc(292) peerSelectDnsPaths:      cache_peer = local=0.0.0.0 remote=139.2.1.3:8080 flags=1
> 2017/03/16 10:18:37.951 kid1| 44,2| peer_select.cc(292) peerSelectDnsPaths:      cache_peer = local=0.0.0.0 remote=139.2.1.4:8080 flags=1
> 2017/03/16 10:18:37.951 kid1| 44,2| peer_select.cc(292) peerSelectDnsPaths:      cache_peer = local=0.0.0.0 remote=139.2.1.4:8080 flags=1
> 2017/03/16 10:18:37.951 kid1| 44,2| peer_select.cc(295) peerSelectDnsPaths:        timedout = 0
> 

Hmm. Something is going wrong with our logic to ensure unique IP:port
entries in the list of selected paths. It should not be affecting your
issue much though.

> access.log
> 
> 1489656077.628 159679 10.30.216.160 TCP_TUNNEL/200 26328966 CONNECT repository.apache.org:443 - ANY_OLD_PARENT/139.2.1.4 -
> 

Uhm. One thing to be very wary of is that transactions are not logged
until they are completed. So things like their full duration and bytes
can be recorded.

When CONNECT are involved some people who are not fully aware of the
meanings of that request method can be surprised by lack of log entries.
It is a tunnel and whole *weeks* worth of various traffic can happen
inside before it reaches that complete state for logging.
 You might see nothing actually happening except CONNECT lines being
logged with zero sizes, or huge amounts of https:// URLs being fetched
without a single access.log line occuring ... or any mix of behaviour in
between.

This connection had 26MB transferred over it. The 'connect' stage (TCP
SYN / SYN-ACK exchange) may have been successful within the first 11
seconds (5sec timeout on first two cache_peer in that cache.log list,
then immediate success on the third) and just nothing visibly happening
on it at the HTTP level for a bit while the TLS crypto did things.

If things are breaking or going slowly at the TLS layer or higher, then
there is nothing you can do in this Squid. As far as this Squid is
concerned the TCP tunnel was setup fine and working. What is inside it
is opaque.

I have just done a test of those two peers from here to see how the
setup goes, and there is an over 2min 10-12sec delay before my ISPs NAT
system cuts the connection. Something is very broken with those
particular peers or the network they reside in. That whole process
should have taken under 350ms and been terminated by their end.

Amos



From squid3 at treenet.co.nz  Thu Mar 16 10:53:53 2017
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 16 Mar 2017 23:53:53 +1300
Subject: [squid-users] kerb auth groups KV note acl config
In-Reply-To: <2197768425D7F5479A0FFB3FEC212F7FF5D9FA95@aesmail.surcouf.local>
References: <2197768425D7F5479A0FFB3FEC212F7FF5D88ABD@aesmail.surcouf.local>
 <2197768425D7F5479A0FFB3FEC212F7FF5D9D05D@aesmail.surcouf.local>
 <04277d89-6f12-31e0-f0e9-d01fe6134b0c@treenet.co.nz>
 <2197768425D7F5479A0FFB3FEC212F7FF5D9FA95@aesmail.surcouf.local>
Message-ID: <16b05258-d4f1-fd4f-fc68-51e4b3ac7ba8@treenet.co.nz>

On 16/03/2017 11:12 p.m., Mike Surcouf wrote:
> @Amos
> 
> Thanks for this
> 
> so to recap if I currently have
> 
> auth_param negotiate program /usr/lib64/squid/negotiate_kerberos_auth
> auth_param negotiate children 20
> auth_param negotiate keep_alive on
> 
> external_acl_type InternetAccessBanking %LOGIN /usr/lib64/squid/ext_kerberos_ldap_group_acl -u ldaps://aesdc02.surcouf.local:636 -b cn=SSSUsers,dc=surcouf,dc=local  -g InternetAccessBanking
> 
> I could replace it by
> 
> auth_param negotiate program /usr/lib64/squid/negotiate_kerberos_auth
> auth_param negotiate children 20
> auth_param negotiate keep_alive
> 
> acl InternetAccessBanking note group S-1-5-21-123456789-123456789-123456789-1234
> 
> 
> Note where S-1-5-21-123456789-123456789-123456789-1234 is the SID for the group InternetAccessBanking
> 
> 

Yes.

Amos



From wolle5050 at gmx.de  Thu Mar 16 11:36:59 2017
From: wolle5050 at gmx.de (Jens Offenbach)
Date: Thu, 16 Mar 2017 12:36:59 +0100
Subject: [squid-users] No failover when default parent proxy fails
 (Squid 3.5.12)
In-Reply-To: <0796b97d-2fdf-411d-9fa8-82dcefcc956d@treenet.co.nz>
References: <trinity-daf2229d-e256-478a-9fd8-a5005f819ef5-1489558013937@3capp-gmx-bs10>
 <5ccce9cb-0eff-65e8-ea4a-b57ff976bab7@treenet.co.nz>
 <trinity-cda683b1-f582-48d5-a5c0-73130adfcf15-1489641820307@3capp-gmx-bs34>
 <821f2fe9-112b-5ec2-24d4-c6494fd5b1e6@measurement-factory.com>
 <trinity-d692acfc-f31f-4362-a19b-dcb276a0d962-1489644317027@3capp-gmx-bs34>
 <046af0e9-0baa-2b1b-164e-9edb0de5fa69@treenet.co.nz>
 <trinity-b14145da-c806-416a-9ebb-7db200b0e4d8-1489657195890@3capp-gmx-bs34>,
 <0796b97d-2fdf-411d-9fa8-82dcefcc956d@treenet.co.nz>
Message-ID: <trinity-524b1c00-28cb-4eb6-983f-6fd3906ed8fd-1489664219879@3capp-gmx-bs34>

Thanks for your detailed explaination...

Well, the "default" option does not seem to be the right choice to achieve the expected behaviour. Hopefully, the following change will make all the traffic passing through the primary proxy when it is reachable:

# OPTIONS WHICH AFFECT THE NEIGHBOR SELECTION ALGORITHM
# -----------------------------------------------------------------------------
  cache_peer proxy.materna.de parent 8080 0 no-digest no-query connect-timeout=5 connect-fail-limit=2 weight=2
  cache_peer roxy.materna.de parent 8080 0 no-digest no-query connect-timeout=5 connect-fail-limit=2

I am sorry for the incorrect test setup... I just added:
$ iptables -A OUTPUT -p tcp -d 139.2.1.3 -j REJECT

and the failover takes place very fast:
# iptables -D OUTPUT -p tcp -d 139.2.1.3 -j REJECT (Simulate primary proxy is online)
1489663755.767   4063 10.30.216.160 TCP_TUNNEL/200 26329103 CONNECT repository.apache.org:443 - FIRSTUP_PARENT/139.2.1.3 -
# iptables -A OUTPUT -p tcp -d 139.2.1.3 -j REJECT (Simulate primary proxy offline)
1489663818.913  33845 10.30.216.160 TCP_TUNNEL/200 20569674 CONNECT repository.apache.org:443 - ANY_OLD_PARENT/139.2.1.4 -
# iptables -D OUTPUT -p tcp -d 139.2.1.3 -j REJECT (Simulate primary proxy back online)
1489663850.148   4521 10.30.216.160 TCP_TUNNEL/200 20809195 CONNECT repository.apache.org:443 - FIRSTUP_PARENT/139.2.1.3 -

Those two parent proxies are not under my control. I will talk with my IT guys what is wrong here.

Next Wednesday is the next planned downtime of the primary proxy. I will test the failover at this time again.

You really helped me a lot. Thank you every much!

Regards,
Jens

Gesendet:?Donnerstag, 16. M?rz 2017 um 11:51 Uhr
Von:?"Amos Jeffries" <squid3 at treenet.co.nz>
An:?squid-users at lists.squid-cache.org
Betreff:?Re: [squid-users] No failover when default parent proxy fails (Squid 3.5.12)
On 16/03/2017 10:39 p.m., Jens Offenbach wrote:
> This is the sceanrio;
>
> Squid 3.5.12 is installed on "squid-proxy.mycompany.com". The two parent proxies are:
> - Primary: proxy.mycompany.de:8080 (139.2.1.3)
> - Fallback: roxy.mycompany.de:8080 (139.2.1.4)
>
> I have misunderstood the "default" option in "cache_peer". When I got it right, it has the meaning of a fallback, so I switched it to "roxy.mycompany.de". "proxy.mycompany.de" should always be used and "roxy.mycompany.de" only when "proxy.mycompany.de" fails.
>

Well, kind of. Unless that peer is selected by one of the other
algorithms (for that it has to be 'alive') it will be appended as the
last-resort peer to be used regardless of DEAD/alive status.


> squid.conf:
>
...
>
> # OPTIONS WHICH AFFECT THE NEIGHBOR SELECTION ALGORITHM
> # -----------------------------------------------------------------------------
> cache_peer proxy.materna.de parent 8080 0 no-digest no-query connect-timeout=5 connect-fail-limit=2
> cache_peer roxy.materna.de parent 8080 0 no-digest no-query connect-timeout=5 connect-fail-limit=2 default
>
...
> # OPTIONS INFLUENCING REQUEST FORWARDING
> # -----------------------------------------------------------------------------
> always_direct allow to_matnet
> never_direct allow all
>
> # DNS OPTIONS
> # -----------------------------------------------------------------------------
> dns_nameservers 139.2.34.171
> dns_nameservers 139.2.34.37
>
...
>
> Now, I block traffic on "squid-proxy.mycompany.com" to the primary proxy "proxy.mycompany.de" (139.2.1.3) using IPTables:
> $ iptables -A OUTPUT -p icmp -d 139.2.1.3 -j DROP
> $ iptables -A OUTPUT -p tcp -d 139.2.1.3 -j DROP
> $ iptables -A OUTPUT -p udp -d 139.2.1.3 -j DROP
>

Are you trying to test connection timeout issues or a host going offline?
These iptables rules will force a timeout but not emulate a host
disconnection. Particularly when ICMP is also dropped.

When a host disconnects Squid will receive active signals (maybe via
ICMP) that the TCP SYN packet cannot get through. That speeds failure
recovery things up enormously. If the peer software simply
crashes/exits, different signals happen but with the same super fast
effects.

REJECT rules would be a better emulation of a machine disconnecting, or
an only-TCP REJECT rule emulates a peer software crash, etc. That way
the ICMP signalling still happens similar to those types of failure.



> On the test machine, I use:
> $ export http_proxy=http://squid-proxy.mycompany.com:3128/
> $ export https_proxy=http://squid-proxy.mycompany.com:3128/[http://squid-proxy.mycompany.com:3128/]
> $ export HTTP_PROXY=http://squid-proxy.mycompany.com:3128/[http://squid-proxy.mycompany.com:3128/]
> $ export HTTPS_PROXY=http://squid-proxy.mycompany.com:3128/[http://squid-proxy.mycompany.com:3128/]
>
> Trying to download a resource:
> $ wget https://repository.apache.org/content/groups/snapshots/org/apache/karaf/apache-karaf/4.1.1-SNAPSHOT/apache-karaf-4.1.1-20170315.084054-35.tar.gz[https://repository.apache.org/content/groups/snapshots/org/apache/karaf/apache-karaf/4.1.1-SNAPSHOT/apache-karaf-4.1.1-20170315.084054-35.tar.gz]
>
> The download hangs for 2 minutes until it gets started. A retry shows the same results, the download starts after 2 minutes showing:
> --2017-03-16 09:31:26-- https://repository.apache.org/content/groups/snapshots/org/apache/karaf/apache-karaf/4.1.1-SNAPSHOT/apache-karaf-4.1.1-20170314.154157-34.tar.gz[https://repository.apache.org/content/groups/snapshots/org/apache/karaf/apache-karaf/4.1.1-SNAPSHOT/apache-karaf-4.1.1-20170314.154157-34.tar.gz]
> Resolving squid-proxy.mycompany.com (squid-proxy.mycompany.com)... 10.152.132.41
> Connecting to squid-proxy.mycompany.com (squid-proxy.mycompany.com)|10.152.132.41|:3128... connected.
>
> cache.log:
>
...
> 2017/03/16 10:17:48 kid1| Starting Squid Cache version 3.5.12 for x86_64-pc-linux-gnu...
> 2017/03/16 10:17:48 kid1| Service Name: squid
> 2017/03/16 10:17:48| pinger: Initialising ICMP pinger ...
> 2017/03/16 10:18:09.579 kid1| 44,2| peer_select.cc(258) peerSelectDnsPaths: Find IP destination for: http://proxy.materna.de:8080/squid-internal-dynamic/netdb'[http://proxy.materna.de:8080/squid-internal-dynamic/netdb'] via proxy.materna.de
> 2017/03/16 10:18:09.579 kid1| 44,2| peer_select.cc(280) peerSelectDnsPaths: Found sources for 'http://proxy.materna.de:8080/squid-internal-dynamic/netdb'[http://proxy.materna.de:8080/squid-internal-dynamic/netdb']

These can be avoided by adding no-netdb-exchange option to the
cache_peer config lines. But it is probably a good idea to keep them for
production use as they will be the way of detecting a peer recovery to
live status.

...
> 2017/03/16 10:18:37.951 kid1| 44,2| peer_select.cc(258) peerSelectDnsPaths: Find IP destination for: repository.apache.org:443' via proxy.materna.de
> 2017/03/16 10:18:37.951 kid1| 44,2| peer_select.cc(258) peerSelectDnsPaths: Find IP destination for: repository.apache.org:443' via proxy.materna.de
> 2017/03/16 10:18:37.951 kid1| 44,2| peer_select.cc(258) peerSelectDnsPaths: Find IP destination for: repository.apache.org:443' via roxy.materna.de
> 2017/03/16 10:18:37.951 kid1| 44,2| peer_select.cc(258) peerSelectDnsPaths: Find IP destination for: repository.apache.org:443' via roxy.materna.de
> 2017/03/16 10:18:37.951 kid1| 44,2| peer_select.cc(280) peerSelectDnsPaths: Found sources for 'repository.apache.org:443'
> 2017/03/16 10:18:37.951 kid1| 44,2| peer_select.cc(281) peerSelectDnsPaths: always_direct = DENIED
> 2017/03/16 10:18:37.951 kid1| 44,2| peer_select.cc(282) peerSelectDnsPaths: never_direct = ALLOWED
> 2017/03/16 10:18:37.951 kid1| 44,2| peer_select.cc(292) peerSelectDnsPaths: cache_peer = local=0.0.0.0 remote=139.2.1.3:8080 flags=1
> 2017/03/16 10:18:37.951 kid1| 44,2| peer_select.cc(292) peerSelectDnsPaths: cache_peer = local=0.0.0.0 remote=139.2.1.3:8080 flags=1
> 2017/03/16 10:18:37.951 kid1| 44,2| peer_select.cc(292) peerSelectDnsPaths: cache_peer = local=0.0.0.0 remote=139.2.1.4:8080 flags=1
> 2017/03/16 10:18:37.951 kid1| 44,2| peer_select.cc(292) peerSelectDnsPaths: cache_peer = local=0.0.0.0 remote=139.2.1.4:8080 flags=1
> 2017/03/16 10:18:37.951 kid1| 44,2| peer_select.cc(295) peerSelectDnsPaths: timedout = 0
>

Hmm. Something is going wrong with our logic to ensure unique IP:port
entries in the list of selected paths. It should not be affecting your
issue much though.

> access.log
>
> 1489656077.628 159679 10.30.216.160 TCP_TUNNEL/200 26328966 CONNECT repository.apache.org:443 - ANY_OLD_PARENT/139.2.1.4 -
>

Uhm. One thing to be very wary of is that transactions are not logged
until they are completed. So things like their full duration and bytes
can be recorded.

When CONNECT are involved some people who are not fully aware of the
meanings of that request method can be surprised by lack of log entries.
It is a tunnel and whole *weeks* worth of various traffic can happen
inside before it reaches that complete state for logging.
You might see nothing actually happening except CONNECT lines being
logged with zero sizes, or huge amounts of https:// URLs being fetched
without a single access.log line occuring ... or any mix of behaviour in
between.

This connection had 26MB transferred over it. The 'connect' stage (TCP
SYN / SYN-ACK exchange) may have been successful within the first 11
seconds (5sec timeout on first two cache_peer in that cache.log list,
then immediate success on the third) and just nothing visibly happening
on it at the HTTP level for a bit while the TLS crypto did things.

If things are breaking or going slowly at the TLS layer or higher, then
there is nothing you can do in this Squid. As far as this Squid is
concerned the TCP tunnel was setup fine and working. What is inside it
is opaque.

I have just done a test of those two peers from here to see how the
setup goes, and there is an over 2min 10-12sec delay before my ISPs NAT
system cuts the connection. Something is very broken with those
particular peers or the network they reside in. That whole process
should have taken under 350ms and been terminated by their end.

Amos

_______________________________________________
squid-users mailing list
squid-users at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-users[http://lists.squid-cache.org/listinfo/squid-users]


From mikes at surcouf.co.uk  Thu Mar 16 11:43:17 2017
From: mikes at surcouf.co.uk (Mike Surcouf)
Date: Thu, 16 Mar 2017 11:43:17 +0000
Subject: [squid-users] kerb auth groups KV note acl config
In-Reply-To: <16b05258-d4f1-fd4f-fc68-51e4b3ac7ba8@treenet.co.nz>
References: <2197768425D7F5479A0FFB3FEC212F7FF5D88ABD@aesmail.surcouf.local>
 <2197768425D7F5479A0FFB3FEC212F7FF5D9D05D@aesmail.surcouf.local>
 <04277d89-6f12-31e0-f0e9-d01fe6134b0c@treenet.co.nz>
 <2197768425D7F5479A0FFB3FEC212F7FF5D9FA95@aesmail.surcouf.local>
 <16b05258-d4f1-fd4f-fc68-51e4b3ac7ba8@treenet.co.nz>
Message-ID: <2197768425D7F5479A0FFB3FEC212F7FF5D9FDA5@aesmail.surcouf.local>

Ok I see Markus code moved into the main package for 4.
Quick question his code in there seems almost identical to 3.5 (at least on github mirror)
Currently cache is on Centos v6 and I use Eliezer's excellent rpms.

Do you think this will work with squid and squid-helpers 3.5.23?

-----Original Message-----
From: Amos Jeffries [mailto:squid3 at treenet.co.nz] 
Sent: 16 March 2017 10:54
To: Mike Surcouf; squid-users at lists.squid-cache.org
Subject: Re: [squid-users] kerb auth groups KV note acl config

On 16/03/2017 11:12 p.m., Mike Surcouf wrote:
> @Amos
> 
> Thanks for this
> 
> so to recap if I currently have
> 
> auth_param negotiate program /usr/lib64/squid/negotiate_kerberos_auth
> auth_param negotiate children 20
> auth_param negotiate keep_alive on
> 
> external_acl_type InternetAccessBanking %LOGIN /usr/lib64/squid/ext_kerberos_ldap_group_acl -u ldaps://aesdc02.surcouf.local:636 -b cn=SSSUsers,dc=surcouf,dc=local  -g InternetAccessBanking
> 
> I could replace it by
> 
> auth_param negotiate program /usr/lib64/squid/negotiate_kerberos_auth
> auth_param negotiate children 20
> auth_param negotiate keep_alive
> 
> acl InternetAccessBanking note group S-1-5-21-123456789-123456789-123456789-1234
> 
> 
> Note where S-1-5-21-123456789-123456789-123456789-1234 is the SID for the group InternetAccessBanking
> 
> 

Yes.

Amos


From rousskov at measurement-factory.com  Thu Mar 16 17:01:26 2017
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Thu, 16 Mar 2017 11:01:26 -0600
Subject: [squid-users] No failover when default parent proxy fails
 (Squid 3.5.12)
In-Reply-To: <046af0e9-0baa-2b1b-164e-9edb0de5fa69@treenet.co.nz>
References: <trinity-daf2229d-e256-478a-9fd8-a5005f819ef5-1489558013937@3capp-gmx-bs10>
 <5ccce9cb-0eff-65e8-ea4a-b57ff976bab7@treenet.co.nz>
 <trinity-cda683b1-f582-48d5-a5c0-73130adfcf15-1489641820307@3capp-gmx-bs34>
 <821f2fe9-112b-5ec2-24d4-c6494fd5b1e6@measurement-factory.com>
 <trinity-d692acfc-f31f-4362-a19b-dcb276a0d962-1489644317027@3capp-gmx-bs34>
 <046af0e9-0baa-2b1b-164e-9edb0de5fa69@treenet.co.nz>
Message-ID: <01a19daf-1114-ca15-85bf-7e9f99f48f58@measurement-factory.com>

On 03/16/2017 02:27 AM, Amos Jeffries wrote:

> The global config options being ignored completely is correct because
> your peer have individual connect-timeout=5 settings.

Please note that [individual] peer connect-timeout settings are ignored
on the tunneling path in unpatched Squid v4 and v5 (possibly v3 too):
http://lists.squid-cache.org/pipermail/squid-dev/2017-March/008243.html

Alex.



From richard.qian at magicwifi.com.cn  Fri Mar 17 01:37:59 2017
From: richard.qian at magicwifi.com.cn (=?utf-8?B?6ZKx5Zu95q2j?=)
Date: Fri, 17 Mar 2017 09:37:59 +0800
Subject: [squid-users] on_unsupported_protocol not work?
Message-ID: <tencent_1404AA824F4A8797187CC7B4@qq.com>

Hi guys:


I config squid.conf with


```
acl subnet src 192.168.0.0/16
on_unsupported_protocol tunnel subnet

```


But I still got the error message as below, did I do something wrong ?


2017/03/16 17:36:46.496| 5,2| TcpAcceptor.cc(226) doAccept: New connection on FD 15
2017/03/16 17:36:46.496| 5,2| TcpAcceptor.cc(315) acceptNext: connection on local=[::]:3128 remote=[::] FD 15 flags=41
2017/03/16 17:36:46.499| 33,2| Http1Server.cc(101) buildHttpRequest: Invalid Request
2017/03/16 17:36:46.499| 4,2| errorpage.cc(1260) BuildContent: No existing error page language negotiated for ERR_PROTOCOL_UNKNOWN. Using default error file.
2017/03/16 17:36:46.499| 20,2| store.cc(990) checkCachable: StoreEntry::checkCachable: NO: not cachable
2017/03/16 17:36:46.499| 20,2| store.cc(990) checkCachable: StoreEntry::checkCachable: NO: not cachable
2017/03/16 17:36:46.499| 20,2| store.cc(990) checkCachable: StoreEntry::checkCachable: NO: not cachable
2017/03/16 17:36:46.499| 88,2| client_side_reply.cc(2084) processReplyAccessResult: The reply for NONE error:invalid-request is ALLOWED, because it matched (access_log /mnt/squid/access.log line)
2017/03/16 17:36:46.500| 11,2| Stream.cc(266) sendStartOfMessage: HTTP Client local=157.255.139.239:80 remote=192.168.56.167:55904 FD 14 flags=33
2017/03/16 17:36:46.500| 11,2| Stream.cc(267) sendStartOfMessage: HTTP Client REPLY:
---------
HTTP/1.1 400 Bad Request
Server: squid/4.0.18
Mime-Version: 1.0
Date: Thu, 16 Mar 2017 09:36:46 GMT
Content-Type: text/html;charset=utf-8
Content-Length: 53
X-Squid-Error: ERR_PROTOCOL_UNKNOWN 0
Content-Language: en
X-Cache: MISS from MagicWiFi
X-Cache-Lookup: NONE from MagicWiFi:3128
Connection: close




----------
2017/03/16 17:36:46.500| 20,2| store.cc(990) checkCachable: StoreEntry::checkCachable: NO: not cachable
2017/03/16 17:36:46.500| 33,2| client_side.cc(896) kick: local=157.255.139.239:80 remote=192.168.56.167:55904 flags=33 Connection was closed
2017/03/16 17:36:46.500| 33,2| client_side.cc(588) swanSong: local=157.255.139.239:80 remote=192.168.56.167:55904 flags=33
2017/03/16 17:36:53.314| 5,2| TcpAcceptor.cc(226) doAccept: New connection on FD 15
2017/03/16 17:36:53.314| 5,2| TcpAcceptor.cc(315) acceptNext: connection on local=[::]:3128 remote=[::] FD 15 flags=41
2017/03/16 17:36:53.320| 33,2| Http1Server.cc(101) buildHttpRequest: Invalid Request
2017/03/16 17:36:53.320| 4,2| errorpage.cc(1260) BuildContent: No existing error page language negotiated for ERR_PROTOCOL_UNKNOWN. Using default error file.
2017/03/16 17:36:53.320| 20,2| store.cc(990) checkCachable: StoreEntry::checkCachable: NO: not cachable
2017/03/16 17:36:53.320| 20,2| store.cc(990) checkCachable: StoreEntry::checkCachable: NO: not cachable
2017/03/16 17:36:53.321| 20,2| store.cc(990) checkCachable: StoreEntry::checkCachable: NO: not cachable
2017/03/16 17:36:53.321| 88,2| client_side_reply.cc(2084) processReplyAccessResult: The reply for NONE error:invalid-request is ALLOWED, because it matched (access_log /mnt/squid/access.log line)
2017/03/16 17:36:53.321| 11,2| Stream.cc(266) sendStartOfMessage: HTTP Client local=157.255.139.239:80 remote=192.168.56.167:55905 FD 14 flags=33
2017/03/16 17:36:53.321| 11,2| Stream.cc(267) sendStartOfMessage: HTTP Client REPLY:
---------
HTTP/1.1 400 Bad Request
Server: squid/4.0.18
Mime-Version: 1.0
Date: Thu, 16 Mar 2017 09:36:53 GMT
Content-Type: text/html;charset=utf-8
Content-Length: 53
X-Squid-Error: ERR_PROTOCOL_UNKNOWN 0
Content-Language: en
X-Cache: MISS from MagicWiFi
X-Cache-Lookup: NONE from MagicWiFi:3128
Connection: close




----------
2017/03/16 17:36:53.321| 20,2| store.cc(990) checkCachable: StoreEntry::checkCachable: NO: not cachable
2017/03/16 17:36:53.321| 33,2| client_side.cc(896) kick: local=157.255.139.239:80 remote=192.168.56.167:55905 flags=33 Connection was closed
2017/03/16 17:36:53.321| 33,2| client_side.cc(588) swanSong: local=157.255.139.239:80 remote=192.168.56.167:55905 flags=33
2017/03/16 17:36:58.005| 5,2| TcpAcceptor.cc(226) doAccept: New connection on FD 15
2017/03/16 17:36:58.005| 5,2| TcpAcceptor.cc(315) acceptNext: connection on local=[::]:3128 remote=[::] FD 15 flags=41
2017/03/16 17:36:58.013| 33,2| Http1Server.cc(101) buildHttpRequest: Invalid Request
2017/03/16 17:36:58.013| 4,2| errorpage.cc(1260) BuildContent: No existing error page language negotiated for ERR_PROTOCOL_UNKNOWN. Using default error file.
2017/03/16 17:36:58.013| 20,2| store.cc(990) checkCachable: StoreEntry::checkCachable: NO: not cachable
2017/03/16 17:36:58.013| 20,2| store.cc(990) checkCachable: StoreEntry::checkCachable: NO: not cachable
2017/03/16 17:36:58.013| 20,2| store.cc(990) checkCachable: StoreEntry::checkCachable: NO: not cachable
2017/03/16 17:36:58.013| 88,2| client_side_reply.cc(2084) processReplyAccessResult: The reply for NONE error:invalid-request is ALLOWED, because it matched (access_log /mnt/squid/access.log line)
2017/03/16 17:36:58.014| 11,2| Stream.cc(266) sendStartOfMessage: HTTP Client local=157.255.40.106:80 remote=192.168.56.167:55906 FD 14 flags=33
2017/03/16 17:36:58.014| 11,2| Stream.cc(267) sendStartOfMessage: HTTP Client REPLY:
---------
HTTP/1.1 400 Bad Request
Server: squid/4.0.18
Mime-Version: 1.0
Date: Thu, 16 Mar 2017 09:36:58 GMT
Content-Type: text/html;charset=utf-8
Content-Length: 53
X-Squid-Error: ERR_PROTOCOL_UNKNOWN 0
Content-Language: en
X-Cache: MISS from MagicWiFi
X-Cache-Lookup: NONE from MagicWiFi:3128
Connection: close




----------
2017/03/16 17:36:58.014| 20,2| store.cc(990) checkCachable: StoreEntry::checkCachable: NO: not cachable
2017/03/16 17:36:58.014| 33,2| client_side.cc(896) kick: local=157.255.40.106:80 remote=192.168.56.167:55906 flags=33 Connection was closed
2017/03/16 17:36:58.014| 33,2| client_side.cc(588) swanSong: local=157.255.40.106:80 remote=192.168.56.167:55906 flags=33
2017/03/16 17:36:58.595| 5,2| TcpAcceptor.cc(226) doAccept: New connection on FD 15
2017/03/16 17:36:58.595| 5,2| TcpAcceptor.cc(315) acceptNext: connection on local=[::]:3128 remote=[::] FD 15 flags=41
2017/03/16 17:36:58.599| 33,2| Http1Server.cc(101) buildHttpRequest: Invalid Request
2017/03/16 17:36:58.599| 4,2| errorpage.cc(1260) BuildContent: No existing error page language negotiated for ERR_PROTOCOL_UNKNOWN. Using default error file.
2017/03/16 17:36:58.599| 20,2| store.cc(990) checkCachable: StoreEntry::checkCachable: NO: not cachable
2017/03/16 17:36:58.599| 20,2| store.cc(990) checkCachable: StoreEntry::checkCachable: NO: not cachable
2017/03/16 17:36:58.599| 20,2| store.cc(990) checkCachable: StoreEntry::checkCachable: NO: not cachable
2017/03/16 17:36:58.599| 88,2| client_side_reply.cc(2084) processReplyAccessResult: The reply for NONE error:invalid-request is ALLOWED, because it matched (access_log /mnt/squid/access.log line)
2017/03/16 17:36:58.599| 11,2| Stream.cc(266) sendStartOfMessage: HTTP Client local=157.255.40.106:80 remote=192.168.56.167:55907 FD 14 flags=33
2017/03/16 17:36:58.599| 11,2| Stream.cc(267) sendStartOfMessage: HTTP Client REPLY:
---------
HTTP/1.1 400 Bad Request
Server: squid/4.0.18
Mime-Version: 1.0
Date: Thu, 16 Mar 2017 09:36:58 GMT
Content-Type: text/html;charset=utf-8
Content-Length: 53
X-Squid-Error: ERR_PROTOCOL_UNKNOWN 0
Content-Language: en
X-Cache: MISS from MagicWiFi
X-Cache-Lookup: NONE from MagicWiFi:3128
Connection: close












------------------

??? Richard

??????

???????????????

Shenzhen Lemon Network Technology CO.,Ltd




??/Mobile?13538273761

??/Tel?0755-86699030

??/Add????????????????????1710-1711




??????????????????????????????????????????????????????????????????????????

If you are not the intended recipient, please notify us immediately and you must not use, retain, disclose, copy, print, forword or disseminate this email or any attachments.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20170317/65f962ff/attachment.htm>

From rousskov at measurement-factory.com  Fri Mar 17 03:57:11 2017
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Thu, 16 Mar 2017 21:57:11 -0600
Subject: [squid-users] on_unsupported_protocol not work?
In-Reply-To: <tencent_1404AA824F4A8797187CC7B4@qq.com>
References: <tencent_1404AA824F4A8797187CC7B4@qq.com>
Message-ID: <0003487e-0cd1-b3a5-33c7-03beb186603a@measurement-factory.com>

On 03/16/2017 07:37 PM, ??? wrote:

> acl subnet src 192.168.0.0/16
> on_unsupported_protocol tunnel subnet

The on_unsupported_protocol directive does not (and cannot) work for
cases where Squid does not know where the client is trying to get to:

> Currently, this directive has effect on intercepted connections and
> bumped tunnels only. Other cases are not supported because Squid
> cannot know the intended destination of other traffic.

AFAICT from your logs, Squid was not bumping a CONNECT tunnel. Did Squid
intercept the connection with a non-HTTP request? What is your http_port
3128 configuration in squid.conf?

And if your Squid is not intercepting (i.e., it works as a regular HTTP
forward proxy), then why is it receiving non-HTTP requests and what
would you like Squid to do with them?

Alex.


> 2017/03/16 17:36:46.496| 5,2| TcpAcceptor.cc(315) acceptNext: connection on local=[::]:3128 remote=[::] FD 15 flags=41
> 2017/03/16 17:36:46.499| 33,2| Http1Server.cc(101) buildHttpRequest: Invalid Request
> 2017/03/16 17:36:46.499| 4,2| errorpage.cc(1260) BuildContent: No existing error page language negotiated for ERR_PROTOCOL_UNKNOWN.



From frio_cervesa at hotmail.com  Fri Mar 17 04:42:16 2017
From: frio_cervesa at hotmail.com (senor)
Date: Fri, 17 Mar 2017 04:42:16 +0000
Subject: [squid-users] SMP and AUFS
Message-ID: <BN6PR17MB11408ADA794A592AD0CA5B5AF7390@BN6PR17MB1140.namprd17.prod.outlook.com>

Hi All,
In a thread with subject "squid workers question" Alex Rousskov has
stated that AUFS is not compatible with SMP mode. I need this clarified.

I understand that AUFS is not SMP aware but if each worker has its own
AUFS cache is there any problem other than the inefficiencies of
duplicate cache?

I'm pretty sure that AUFS is used with squid running in SMP mode a lot.
The squid wiki even has a CARP configuration example for this combination.

I apologize if I am misunderstanding the statements.

Senor

From sothy.e98 at gmail.com  Fri Mar 17 14:27:38 2017
From: sothy.e98 at gmail.com (sothy shan)
Date: Fri, 17 Mar 2017 15:27:38 +0100
Subject: [squid-users] commBind Cannot bind socket FD 28 to [::]: (2) No
	such file or directory
Message-ID: <CAHcF_0b3aHnXu=Fnak=YoTS9qG_7JHuLM5cwY7nhtcYWfGxDfQ@mail.gmail.com>

Hello,

I am using squid 4.0.18 to understand SMP features. I configured 3
workers,  and made rock file system for cache_dir.
I didn shm mount also.
When I start squid server (like this sudo squid -d 2). I am getting the
following error.
Any help? Thanks



2017/03/17 10:22:14 kid2| commBind Cannot bind socket FD 28 to [::]: (2) No
such file or directory
2017/03/17 10:22:14 kid3| Finished loading MIME types and icons.
2017/03/17 10:22:14 kid3| HTCP Disabled.
2017/03/17 10:22:14 kid3| Squid plugin modules loaded: 0
2017/03/17 10:22:14 kid3| Adaptation support is off.
2017/03/17 10:22:14 kid3| commBind Cannot bind socket FD 28 to [::]: (2) No
such file or directory
2017/03/17 10:22:21 kid2| ERROR: /var/spool/squidR/rock communication
channel establishment timeout
2017/03/17 10:22:21 kid2| Closing HTTP(S) port [::]:3128
2017/03/17 10:22:21 kid2| Not currently OK to rewrite swap log.
2017/03/17 10:22:21 kid2| storeDirWriteCleanLogs: Operation aborted.
FATAL: Rock cache_dir at /var/spool/squidR/rock failed to open db file: (2)
No such file or directory
2017/03/17 10:22:21 kid3| ERROR: /var/spool/squidR/rock communication
channel establishment timeout
2017/03/17 10:22:21 kid3| Closing HTTP(S) port [::]:3128
2017/03/17 10:22:21 kid3| Not currently OK to rewrite swap log.
2017/03/17 10:22:21 kid3| storeDirWriteCleanLogs: Operation aborted.
FATAL: Rock cache_dir at /var/spool/squidR/rock failed to open db file: (2)
No such file or directory
2017/03/17 10:22:21 kid1| ERROR: /var/spool/squidR/rock communication
channel establishment timeout
2017/03/17 10:22:21 kid1| Closing HTTP(S) port [::]:3128
2017/03/17 10:22:21 kid1| Not currently OK to rewrite swap log.
2017/03/17 10:22:21 kid1| storeDirWriteCleanLogs: Operation aborted.
FATAL: Rock cache_dir at /var/spool/squidR/rock failed to open db file: (2)
No such file or directory
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20170317/0562b15d/attachment.htm>

From rousskov at measurement-factory.com  Fri Mar 17 14:45:28 2017
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Fri, 17 Mar 2017 08:45:28 -0600
Subject: [squid-users] SMP and AUFS
In-Reply-To: <BN6PR17MB11408ADA794A592AD0CA5B5AF7390@BN6PR17MB1140.namprd17.prod.outlook.com>
References: <BN6PR17MB11408ADA794A592AD0CA5B5AF7390@BN6PR17MB1140.namprd17.prod.outlook.com>
Message-ID: <ecd815a6-b464-d22d-a3a0-4e8b2fa18203@measurement-factory.com>

On 03/16/2017 10:42 PM, senor wrote:

> I understand that AUFS is not SMP aware but if each worker has its own
> AUFS cache is there any problem other than the inefficiencies of
> duplicate cache?

Yes. Clients may get stale cached entries, possibly breaking advanced
HTTP transactions that rely on a more-or-less compliant proxy cache.

Also, I do not know exactly how local and shared cache indexes interact
when SMP-unaware store updates its local index without updating the
shared one. Most likely, such partial updates lead to bugs. You may
reduce bugs probability by not mixing shared and ufs-based stores in SMP
mode, but I doubt you can eliminate all problems that way.


> I'm pretty sure that AUFS is used with squid running in SMP mode a lot.

I can think of many examples where a lot of people do things they should
not be doing and do not do things they should be doing. Just because
many use X to solve some problem, does not make using X a good idea and
certainly does not make it the best solution available.


> The squid wiki even has a CARP configuration example for this combination.

I hope there are no official examples advertising SMP AUFS
configurations. If there are, they should be removed IMO.

Alex.



From chip_pop at hotmail.com  Fri Mar 17 14:45:03 2017
From: chip_pop at hotmail.com (joseph)
Date: Fri, 17 Mar 2017 07:45:03 -0700 (PDT)
Subject: [squid-users] squid-cache.org
Message-ID: <1489761903891-4681831.post@n4.nabble.com>

i can not access from Lebanon  any reason ??



--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/squid-cache-org-tp4681831.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From rousskov at measurement-factory.com  Fri Mar 17 15:27:08 2017
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Fri, 17 Mar 2017 09:27:08 -0600
Subject: [squid-users] commBind Cannot bind socket FD 28 to [::]: (2) No
 such file or directory
In-Reply-To: <CAHcF_0b3aHnXu=Fnak=YoTS9qG_7JHuLM5cwY7nhtcYWfGxDfQ@mail.gmail.com>
References: <CAHcF_0b3aHnXu=Fnak=YoTS9qG_7JHuLM5cwY7nhtcYWfGxDfQ@mail.gmail.com>
Message-ID: <19f03aa3-ecd6-dee3-e30e-5d5350dc5631@measurement-factory.com>

On 03/17/2017 08:27 AM, sothy shan wrote:

> I am using squid 4.0.18 to understand SMP features. I configured 3
> workers,  and made rock file system for cache_dir.
> I didn shm mount also.
> When I start squid server (like this sudo squid -d 2). I am getting the
> following error.
> Any help?

I would suspect IPC socket creation problems and start with

http://wiki.squid-cache.org/Features/SmpScale#Cannot_bind_socket_FD_NN_to_.5B::.5D:_.2813.29_Permission_denied

Alex.

> 2017/03/17 10:22:14 kid2| commBind Cannot bind socket FD 28 to [::]: (2) No such file or directory
> 2017/03/17 10:22:14 kid3| commBind Cannot bind socket FD 28 to [::]: (2) No such file or directory
> 2017/03/17 10:22:21 kid2| ERROR: /var/spool/squidR/rock communication channel establishment timeout



From rousskov at measurement-factory.com  Fri Mar 17 15:30:03 2017
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Fri, 17 Mar 2017 09:30:03 -0600
Subject: [squid-users] squid-cache.org
In-Reply-To: <1489761903891-4681831.post@n4.nabble.com>
References: <1489761903891-4681831.post@n4.nabble.com>
Message-ID: <ffce1d68-05b1-dcd1-21c8-dbb770864d64@measurement-factory.com>

On 03/17/2017 08:45 AM, joseph wrote:
> i can not access from Lebanon  any reason ??

You are not the only one affected (around the world). Project DNS setup
appears to be broken -- some of the slave zones do not have the right
information (or do not have enough information) AFAICT. The Squid NOC
has been informed.

Alex.



From chip_pop at hotmail.com  Fri Mar 17 15:30:14 2017
From: chip_pop at hotmail.com (joseph)
Date: Fri, 17 Mar 2017 08:30:14 -0700 (PDT)
Subject: [squid-users] squid-cache.org
In-Reply-To: <ffce1d68-05b1-dcd1-21c8-dbb770864d64@measurement-factory.com>
References: <1489761903891-4681831.post@n4.nabble.com>
 <ffce1d68-05b1-dcd1-21c8-dbb770864d64@measurement-factory.com>
Message-ID: <1489764614322-4681834.post@n4.nabble.com>

tks



--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/squid-cache-org-tp4681831p4681834.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From marcus.kool at urlfilterdb.com  Fri Mar 17 16:28:26 2017
From: marcus.kool at urlfilterdb.com (Marcus Kool)
Date: Fri, 17 Mar 2017 13:28:26 -0300
Subject: [squid-users] ufdbGuard 1.33.1 is released
Message-ID: <b418aa19-0bfd-8a2f-02a6-ac39627244ba@urlfilterdb.com>

ufdbGuard, the free URL filter for Squid, has a new release.
The highlights of this release are:
+ full UTF8 support for URLs
+ IPv6 support for sources
+ performance improvement for large systems
+ all reported issues have been fixed.

ufdbGuard was forked from squidGuard in 2005 and is actively maintained,
uses less resources and has more features than squidGuard.

ufdbGuard can be downloaded from https://sourceforge.net and https://www.urlfilterdb.com

Marcus Kool
author of ufdbGuard


From mynixmail at gmail.com  Sat Mar 18 13:53:22 2017
From: mynixmail at gmail.com (Danny)
Date: Sat, 18 Mar 2017 15:53:22 +0200
Subject: [squid-users] reply_body_max_size question
In-Reply-To: <5d8e9a9a-e8a0-d51e-a10b-0e45fa6ba566@treenet.co.nz>
References: <20170312071146.GA3794@fever.havannah.local>
 <5d8e9a9a-e8a0-d51e-a10b-0e45fa6ba566@treenet.co.nz>
Message-ID: <20170318135322.GA9563@fever.havannah.local>

Thank you Amos for the detailed reply. Never too old to learn are we?

Have a nice day

Danny

On Mar 15 17, Amos Jeffries :
> To: squid-users at lists.squid-cache.org
> Date: Wed, 15 Mar 2017 15:49:04 +1300
> From: Amos Jeffries <squid3 at treenet.co.nz>
> Subject: Re: [squid-users] reply_body_max_size question
> User-Agent: Mozilla/5.0 (Windows NT 6.1; WOW64; rv:45.0) Gecko/20100101
>  Thunderbird/45.8.0
> X-BeenThere: squid-users at lists.squid-cache.org
> 
> On 12/03/2017 8:11 p.m., Danny wrote:
> > Hi,
> > 
> > Just want someone to confirm my current reply_body_max_size setup. I have a
> > simple network at home i.e: Debian with a wireless card (wlan0) which is bridged
> > (br0) to an ethernet card (eth0). All devices comes through the wireless card
> > (wlan0) and then of to the router.
> > 
> > I want "localnet_sniper localnet_bridge localnet_fever localnet_44081 localnet_dannyS4" to have unlimited download capabilty but
> > "localnet_vS5mini localnet_anTab2 localnet_vTab3 localnet_samsungTV localnet_samsungDVD localnet_dhcp" must be limited to a
> > 5MB download limit.
> > 
> > Here is my configuration:
> > ######################################################################################################################################
> > acl localnet src 10.0.0.0/24	# RFC1918 possible internal network
> > acl localnet_sniper src 10.0.0.3        #(eth0)
> > acl localnet_bridge src 10.0.0.4        #(br0)
> > acl localnet_fever src 10.0.0.5         #(wlan0)
> > acl localnet_44081 src 10.0.0.11        #(RaspberryPi3)
> > acl localnet_dannyS4 src 10.0.0.54
> > acl localnet_vS5mini src 10.0.0.55
> > acl localnet_shotgun src 10.0.0.56
> > acl localnet_anTab2 src 10.0.0.71
> > acl localnet_vTab3 src 10.0.0.73
> > acl localnet_samsungTV src 10.0.0.80
> > acl localnet_samsungDVD src 10.0.0.81
> > acl localnet_dhcp src 10.0.0.201
> > acl localnet_dhcp src 10.0.0.202
> > acl localnet_dhcp src 10.0.0.203
> > acl localnet_dhcp src 10.0.0.204
> > 
> > http_access allow password
> > http_access allow localhost
> > http_access allow localnet
> 
> The localnet ACL above matches and allows all requests from any IP in
> the 10.*/24 to use the proxy.
> 
> So none of the below individual IP checks will ever be reached. They are
> pointless anyway since they do the same as the more generic "allow
> localnet".
> 
> 
> > http_access allow localnet_sniper
> > http_access allow localnet_bridge
> > http_access allow localnet_fever
> > http_access allow localnet_44081
> > http_access allow localnet_dannyS4
> > http_access allow localnet_vS5mini
> > http_access allow localnet_anTab2
> > http_access allow localnet_vTab3
> > http_access allow localnet_samsungTV
> > http_access allow localnet_samsungDVD
> > http_access allow localnet_dhcp
> 
> 
> The default security protections for Safe_ports, SSL_ports, CONNECT,
> manager access, and final "deny all" are missing.
> 
> I hope you have just omited them from this mail, not removed them from
> your config.
> 
> > 
> > reply_body_max_size 9 999 999 999 MB localnet_sniper localnet_bridge localnet_fever localnet_44081 localnet_dannyS4
> 
> Squid understands the magic word "none" to mean no limit. The above is
> setting a large, but not impossible limit of ~9.3 PB.
> 
> 
> > reply_body_max_size 5 MB localnet_vS5mini localnet_anTab2 localnet_vTab3 localnet_samsungTV localnet_samsungDVD localnet_dhcp
> > 
> 
> The ACLs on both these lines are defining an impossible situation.
> See <http://wiki.squid-cache.org/SquidFaq/SquidAcl#Common_Mistakes> for
> what is going wrong there and ways to fix it.
> 
> Transactions which do not have a limit applied, are of course unlimited.
> So drop the ACL's explicitly listing what not to limit. You only need
> ACL to match what does get limited, and only one is needed (you are only
> matching on IP, nothing complex).
> 
> Like so:
> 
>  acl limit_5MB src 10.0.0.201-10.0.0.204 # dhcp
>  acl limit_5MB src 10.0.0.80    # samsung TV
>  acl limit_5MB src 10.0.0.81    # samsung DVD
>  ...
>  reply_body_max_size 5 MB limit_5MB
> 
> That is it.
> 
> 
> > url_rewrite_program /usr/bin/squidGuard -c /etc/squidguard/squidGuard.conf
> > redirect_program /usr/bin/squidGuard -c /etc/squidguard/squidGuard.conf
> 
> 
> redirect_program is a deprecated alias for url_rewrite_program. You can
> only have one configured for use. So, only the latter of the two
> directives will do anything.
> 
> Amos
> 
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users


From frio_cervesa at hotmail.com  Sun Mar 19 05:11:42 2017
From: frio_cervesa at hotmail.com (senor)
Date: Sun, 19 Mar 2017 05:11:42 +0000
Subject: [squid-users] SMP and AUFS
In-Reply-To: <ecd815a6-b464-d22d-a3a0-4e8b2fa18203@measurement-factory.com>
References: <BN6PR17MB11408ADA794A592AD0CA5B5AF7390@BN6PR17MB1140.namprd17.prod.outlook.com>
 <ecd815a6-b464-d22d-a3a0-4e8b2fa18203@measurement-factory.com>
Message-ID: <BN6PR17MB1140460613B0187B1CE191C6F73B0@BN6PR17MB1140.namprd17.prod.outlook.com>

On 3/17/2017 7:45, Alex Rousskov wrote:
> On 03/16/2017 10:42 PM, senor wrote:
> 
>> I understand that AUFS is not SMP aware but if each worker has its own
>> AUFS cache is there any problem other than the inefficiencies of
>> duplicate cache?
> 
> Yes. Clients may get stale cached entries, possibly breaking advanced
> HTTP transactions that rely on a more-or-less compliant proxy cache.
> 
> Also, I do not know exactly how local and shared cache indexes interact
> when SMP-unaware store updates its local index without updating the
> shared one. Most likely, such partial updates lead to bugs. You may
> reduce bugs probability by not mixing shared and ufs-based stores in SMP
> mode, but I doubt you can eliminate all problems that way.
> 
> 
>> I'm pretty sure that AUFS is used with squid running in SMP mode a lot.
> 
> I can think of many examples where a lot of people do things they should
> not be doing and do not do things they should be doing. Just because
> many use X to solve some problem, does not make using X a good idea and
> certainly does not make it the best solution available.
> 
> 
>> The squid wiki even has a CARP configuration example for this combination.
> 
> I hope there are no official examples advertising SMP AUFS
> configurations. If there are, they should be removed IMO.
> 
> Alex.
> 
There are many references in the squid wiki, FAQ and Knowlegebase about
SMP but I don't see any of them reflecting the concerns you have brought
up. My point in mentioning that there are a lot of installations using
SMP and AUFS is that something widely used but buggy tends to be brought
up on this email list and I haven't seen it.

I'm not trying to claim there are no problems. I'm just making sure my
expectations are realistic. Your comments were the first I became aware
anyone thought poorly about the combination of AUFS with SMP. Rock is of
course preferred but it comes with more baggage than AUFS. My own
experience has been pretty good. Maybe just lucky.

Senor

From eliezer at ngtech.co.il  Sun Mar 19 09:11:49 2017
From: eliezer at ngtech.co.il (Eliezer  Croitoru)
Date: Sun, 19 Mar 2017 11:11:49 +0200
Subject: [squid-users] SMP and AUFS
In-Reply-To: <BN6PR17MB1140460613B0187B1CE191C6F73B0@BN6PR17MB1140.namprd17.prod.outlook.com>
References: <BN6PR17MB11408ADA794A592AD0CA5B5AF7390@BN6PR17MB1140.namprd17.prod.outlook.com>
 <ecd815a6-b464-d22d-a3a0-4e8b2fa18203@measurement-factory.com>
 <BN6PR17MB1140460613B0187B1CE191C6F73B0@BN6PR17MB1140.namprd17.prod.outlook.com>
Message-ID: <075101d2a090$db0bbcc0$91233640$@ngtech.co.il>

I think that some warning message like "WARNING: be sure you know that UFS\AUFS doesn't support SMP\MultiWorkers" should be added to the stderr or cache.log.

Eliezer

----
Eliezer Croitoru
Linux System Administrator
Mobile: +972-5-28704261
Email: eliezer at ngtech.co.il


-----Original Message-----
From: squid-users [mailto:squid-users-bounces at lists.squid-cache.org] On Behalf Of senor
Sent: Sunday, March 19, 2017 7:12 AM
To: squid-users at lists.squid-cache.org
Subject: Re: [squid-users] SMP and AUFS

On 3/17/2017 7:45, Alex Rousskov wrote:
> On 03/16/2017 10:42 PM, senor wrote:
> 
>> I understand that AUFS is not SMP aware but if each worker has its own
>> AUFS cache is there any problem other than the inefficiencies of
>> duplicate cache?
> 
> Yes. Clients may get stale cached entries, possibly breaking advanced
> HTTP transactions that rely on a more-or-less compliant proxy cache.
> 
> Also, I do not know exactly how local and shared cache indexes interact
> when SMP-unaware store updates its local index without updating the
> shared one. Most likely, such partial updates lead to bugs. You may
> reduce bugs probability by not mixing shared and ufs-based stores in SMP
> mode, but I doubt you can eliminate all problems that way.
> 
> 
>> I'm pretty sure that AUFS is used with squid running in SMP mode a lot.
> 
> I can think of many examples where a lot of people do things they should
> not be doing and do not do things they should be doing. Just because
> many use X to solve some problem, does not make using X a good idea and
> certainly does not make it the best solution available.
> 
> 
>> The squid wiki even has a CARP configuration example for this combination.
> 
> I hope there are no official examples advertising SMP AUFS
> configurations. If there are, they should be removed IMO.
> 
> Alex.
> 
There are many references in the squid wiki, FAQ and Knowlegebase about
SMP but I don't see any of them reflecting the concerns you have brought
up. My point in mentioning that there are a lot of installations using
SMP and AUFS is that something widely used but buggy tends to be brought
up on this email list and I haven't seen it.

I'm not trying to claim there are no problems. I'm just making sure my
expectations are realistic. Your comments were the first I became aware
anyone thought poorly about the combination of AUFS with SMP. Rock is of
course preferred but it comes with more baggage than AUFS. My own
experience has been pretty good. Maybe just lucky.

Senor
_______________________________________________
squid-users mailing list
squid-users at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-users



From marcus.kool at urlfilterdb.com  Sun Mar 19 12:36:23 2017
From: marcus.kool at urlfilterdb.com (Marcus Kool)
Date: Sun, 19 Mar 2017 09:36:23 -0300
Subject: [squid-users] SMP and AUFS
In-Reply-To: <075101d2a090$db0bbcc0$91233640$@ngtech.co.il>
References: <BN6PR17MB11408ADA794A592AD0CA5B5AF7390@BN6PR17MB1140.namprd17.prod.outlook.com>
 <ecd815a6-b464-d22d-a3a0-4e8b2fa18203@measurement-factory.com>
 <BN6PR17MB1140460613B0187B1CE191C6F73B0@BN6PR17MB1140.namprd17.prod.outlook.com>
 <075101d2a090$db0bbcc0$91233640$@ngtech.co.il>
Message-ID: <9d26c7c2-bb55-9119-e279-f9ddffb5d45c@urlfilterdb.com>

The root cause of why admins configure SMP + [A]UFS is the lack of good documentation.
A few lines in the wiki and squid.conf.documented should be enough.

Marcus


On 19/03/17 06:11, Eliezer  Croitoru wrote:
> I think that some warning message like "WARNING: be sure you know that UFS\AUFS doesn't support SMP\MultiWorkers" should be added to the stderr or cache.log.
>
> Eliezer
>
> ----
> Eliezer Croitoru
> Linux System Administrator
> Mobile: +972-5-28704261
> Email: eliezer at ngtech.co.il
>
>
> -----Original Message-----
> From: squid-users [mailto:squid-users-bounces at lists.squid-cache.org] On Behalf Of senor
> Sent: Sunday, March 19, 2017 7:12 AM
> To: squid-users at lists.squid-cache.org
> Subject: Re: [squid-users] SMP and AUFS
>
> On 3/17/2017 7:45, Alex Rousskov wrote:
>> On 03/16/2017 10:42 PM, senor wrote:
>>
>>> I understand that AUFS is not SMP aware but if each worker has its own
>>> AUFS cache is there any problem other than the inefficiencies of
>>> duplicate cache?
>>
>> Yes. Clients may get stale cached entries, possibly breaking advanced
>> HTTP transactions that rely on a more-or-less compliant proxy cache.
>>
>> Also, I do not know exactly how local and shared cache indexes interact
>> when SMP-unaware store updates its local index without updating the
>> shared one. Most likely, such partial updates lead to bugs. You may
>> reduce bugs probability by not mixing shared and ufs-based stores in SMP
>> mode, but I doubt you can eliminate all problems that way.
>>
>>
>>> I'm pretty sure that AUFS is used with squid running in SMP mode a lot.
>>
>> I can think of many examples where a lot of people do things they should
>> not be doing and do not do things they should be doing. Just because
>> many use X to solve some problem, does not make using X a good idea and
>> certainly does not make it the best solution available.
>>
>>
>>> The squid wiki even has a CARP configuration example for this combination.
>>
>> I hope there are no official examples advertising SMP AUFS
>> configurations. If there are, they should be removed IMO.
>>
>> Alex.
>>
> There are many references in the squid wiki, FAQ and Knowlegebase about
> SMP but I don't see any of them reflecting the concerns you have brought
> up. My point in mentioning that there are a lot of installations using
> SMP and AUFS is that something widely used but buggy tends to be brought
> up on this email list and I haven't seen it.
>
> I'm not trying to claim there are no problems. I'm just making sure my
> expectations are realistic. Your comments were the first I became aware
> anyone thought poorly about the combination of AUFS with SMP. Rock is of
> course preferred but it comes with more baggage than AUFS. My own
> experience has been pretty good. Maybe just lucky.
>
> Senor
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>


From eliezer at ngtech.co.il  Sun Mar 19 13:28:38 2017
From: eliezer at ngtech.co.il (Eliezer  Croitoru)
Date: Sun, 19 Mar 2017 15:28:38 +0200
Subject: [squid-users] SMP and AUFS
In-Reply-To: <9d26c7c2-bb55-9119-e279-f9ddffb5d45c@urlfilterdb.com>
References: <BN6PR17MB11408ADA794A592AD0CA5B5AF7390@BN6PR17MB1140.namprd17.prod.outlook.com>
 <ecd815a6-b464-d22d-a3a0-4e8b2fa18203@measurement-factory.com>
 <BN6PR17MB1140460613B0187B1CE191C6F73B0@BN6PR17MB1140.namprd17.prod.outlook.com>
 <075101d2a090$db0bbcc0$91233640$@ngtech.co.il>
 <9d26c7c2-bb55-9119-e279-f9ddffb5d45c@urlfilterdb.com>
Message-ID: <078301d2a0b4$b7f8a990$27e9fcb0$@ngtech.co.il>

I believe that some admins do not have enough time to sit and find the documentation.
I have seen it more than once that an admin just don't want to read the docs since it's like the "ocean" or something similar.

Eliezer

----
Eliezer Croitoru
Linux System Administrator
Mobile: +972-5-28704261
Email: eliezer at ngtech.co.il


-----Original Message-----
From: squid-users [mailto:squid-users-bounces at lists.squid-cache.org] On Behalf Of Marcus Kool
Sent: Sunday, March 19, 2017 2:36 PM
To: squid-users at lists.squid-cache.org
Subject: Re: [squid-users] SMP and AUFS

The root cause of why admins configure SMP + [A]UFS is the lack of good documentation.
A few lines in the wiki and squid.conf.documented should be enough.

Marcus


On 19/03/17 06:11, Eliezer  Croitoru wrote:
> I think that some warning message like "WARNING: be sure you know that UFS\AUFS doesn't support SMP\MultiWorkers" should be added to the stderr or cache.log.
>
> Eliezer
>
> ----
> Eliezer Croitoru
> Linux System Administrator
> Mobile: +972-5-28704261
> Email: eliezer at ngtech.co.il
>
>
> -----Original Message-----
> From: squid-users [mailto:squid-users-bounces at lists.squid-cache.org] 
> On Behalf Of senor
> Sent: Sunday, March 19, 2017 7:12 AM
> To: squid-users at lists.squid-cache.org
> Subject: Re: [squid-users] SMP and AUFS
>
> On 3/17/2017 7:45, Alex Rousskov wrote:
>> On 03/16/2017 10:42 PM, senor wrote:
>>
>>> I understand that AUFS is not SMP aware but if each worker has its 
>>> own AUFS cache is there any problem other than the inefficiencies of 
>>> duplicate cache?
>>
>> Yes. Clients may get stale cached entries, possibly breaking advanced 
>> HTTP transactions that rely on a more-or-less compliant proxy cache.
>>
>> Also, I do not know exactly how local and shared cache indexes 
>> interact when SMP-unaware store updates its local index without 
>> updating the shared one. Most likely, such partial updates lead to 
>> bugs. You may reduce bugs probability by not mixing shared and 
>> ufs-based stores in SMP mode, but I doubt you can eliminate all problems that way.
>>
>>
>>> I'm pretty sure that AUFS is used with squid running in SMP mode a lot.
>>
>> I can think of many examples where a lot of people do things they 
>> should not be doing and do not do things they should be doing. Just 
>> because many use X to solve some problem, does not make using X a 
>> good idea and certainly does not make it the best solution available.
>>
>>
>>> The squid wiki even has a CARP configuration example for this combination.
>>
>> I hope there are no official examples advertising SMP AUFS 
>> configurations. If there are, they should be removed IMO.
>>
>> Alex.
>>
> There are many references in the squid wiki, FAQ and Knowlegebase 
> about SMP but I don't see any of them reflecting the concerns you have 
> brought up. My point in mentioning that there are a lot of 
> installations using SMP and AUFS is that something widely used but 
> buggy tends to be brought up on this email list and I haven't seen it.
>
> I'm not trying to claim there are no problems. I'm just making sure my 
> expectations are realistic. Your comments were the first I became 
> aware anyone thought poorly about the combination of AUFS with SMP. 
> Rock is of course preferred but it comes with more baggage than AUFS. 
> My own experience has been pretty good. Maybe just lucky.
>
> Senor
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>
_______________________________________________
squid-users mailing list
squid-users at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-users



From rousskov at measurement-factory.com  Sun Mar 19 17:08:58 2017
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Sun, 19 Mar 2017 11:08:58 -0600
Subject: [squid-users] SMP and AUFS
In-Reply-To: <BN6PR17MB1140460613B0187B1CE191C6F73B0@BN6PR17MB1140.namprd17.prod.outlook.com>
References: <BN6PR17MB11408ADA794A592AD0CA5B5AF7390@BN6PR17MB1140.namprd17.prod.outlook.com>
 <ecd815a6-b464-d22d-a3a0-4e8b2fa18203@measurement-factory.com>
 <BN6PR17MB1140460613B0187B1CE191C6F73B0@BN6PR17MB1140.namprd17.prod.outlook.com>
Message-ID: <579d3509-99c6-417e-ae1f-3f50402510a9@measurement-factory.com>

On 03/18/2017 11:11 PM, senor wrote:

> There are many references in the squid wiki, FAQ and Knowlegebase about
> SMP but I don't see any of them reflecting the concerns you have brought
> up.

There is a paragraph about these problems at [1] (search for "ufs") but
I agree that better documentation, including wiki and
squid.conf.documented changes/additions would be nice.

  [1] http://wiki.squid-cache.org/Features/SmpScale


> My point in mentioning that there are a lot of installations using
> SMP and AUFS is that something widely used but buggy tends to be brought
> up on this email list and I haven't seen it.

IIRC, it has been brought up several times on the mailing lists and in
Bugzilla. Once you dedicate each ufs-based store to each individual
worker, most of the problems become subtle, often "invisible" to an
admin because they "break" transactions, not Squid, especially if you do
not use a mixture of ufs-based and rock stores. Using mailing list as an
indicator that as subtle problem does _not_ exist is a risky strategy IMO.

Alex.



From lists.squid-users at jamie-thompson.co.uk  Mon Mar 20 01:58:19 2017
From: lists.squid-users at jamie-thompson.co.uk (mr_jrt)
Date: Sun, 19 Mar 2017 18:58:19 -0700 (PDT)
Subject: [squid-users] SSL Bump issues
Message-ID: <1489975099709-4681843.post@n4.nabble.com>

Hello all,

Brief version:
Can't get ssl_bump working to get an old XP system's schannel.dll (i.e.
built-in SSL) talking to a TLS 1.2 server, but works with Firefox (which has
it's own SSL stack).

Long version:
This afternoon's task was to try and solve the issue of an old internal
legacy XP system (and thus stuck on TLS 1.0) that can't be upgraded, but
needs to be able to speak to servers running TLS 1.2. I've tried several
approaches, but using squid with ssl_bump seemed to be the most appropriate
solution, but for the life of me, I've not been able to get it to work
properly, so was hoping for a few pointers.

The software that needs to run uses the built-in schannel dll, but it can
have a proxy specified, so things don't have to be transparent, ...but it
does get stuck with all the limitations of the ancient schannel dll. Does
however mean I can use the system's IE for testing.

First up, I'm running Debian on my squid server. That means the distro
packages don't have ssl support compiled in, so I had to compile my own
packages. The version is 3.5.23, and the relevant configure output is:



I had to compile against the older version of openssl due to the changes in
their locking API, so I installed
https://packages.debian.org/stretch/libssl1.0-dev, which enabled me to
compile successfully.

I've looked at countless examples, i.e.
http://wiki.squid-cache.org/ConfigExamples/Intercept/SslBumpExplicit

...but the only way I've got any successful SSL proxying is with:


...but as expected, that's clearly not doing any bumping from the logs:



When I put anything more in, i.e.


Then it turns on the mode:


...but then I just get errors about no ciphers:


I have a test site I'm using that I can fiddle with the ciphers on, and I
can access it fine from the legacy system directly when I enable the old
stuff (TLS 1.0, etc), but even then it seems to be squid's encryption (or
maybe, decryption from the client?) that isn't working as it still won't
connect regardless of what I try.

Even if I throw in an explicit list of ciphers, copied from the target
server (incidentally, the same host as squid, if that's relevant), still
nada.

Interestingly, ssl_bump seems to work perfectly fine from Firefox from the
same machine, even when crippled down to TLS 1.0 only with the server set to
restrict to TLS 1.2. So it seems to be doing what I want, just not for
schannel.dll? I'm suspecting that openssl as used by squid can't speak any
ciphers that schannel can, so it seems the issue isn't actually between
squid and the target server, but between squid and the old client...



--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/SSL-Bump-issues-tp4681843.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From serhatkoroglu at outlook.com  Mon Mar 20 06:49:17 2017
From: serhatkoroglu at outlook.com (Serhat Koroglu)
Date: Mon, 20 Mar 2017 06:49:17 +0000
Subject: [squid-users] Squid Authentication with HTTP REST API
In-Reply-To: <165d01d29d6a$b3a847d0$1af8d770$@ngtech.co.il>
References: <DB6PR0502MB304543ACB94E65D8F5CA8516B4240@DB6PR0502MB3045.eurprd05.prod.outlook.com>
 <0323d49e-3a2b-4fed-e12c-609306c9b150@treenet.co.nz>,
 <165d01d29d6a$b3a847d0$1af8d770$@ngtech.co.il>
Message-ID: <DB6PR0502MB3045CD1F88CE4D8E166A103FB43A0@DB6PR0502MB3045.eurprd05.prod.outlook.com>

Sorry for late reply.

I have find a suitable solution for validation through a http web service. Here tells developing custom helper even using php: http://freesoftwaremagazine.com/articles/authentication_with_squid/ That's nice.


Then I know there is  authentication with oauth2 for squid-server. But you may be know, in oauth2 authentication, you must authorize the app using user's credentials e.g. facebook username and password. When this oauth2 method is used, your app must redirect to the oauth2 service to authorize your app.


You may had used many web sites like that with facebook login. So my question is how may squid server do this redirect  and authorization process using a third party oauth2 service? Squid asks username and password with web browser popup. Is there any example to this?


Regards,

Serhat.

________________________________
From: Eliezer Croitoru <eliezer at ngtech.co.il>
Sent: Wednesday, March 15, 2017 12:01:15 PM
To: 'Serhat Koroglu'
Cc: squid-users at lists.squid-cache.org
Subject: RE: [squid-users] Squid Authentication with HTTP REST API

Hey Serhat,(first name right?)

>From what I understand you have a specific case.
Today the squid project doesn't have an example on how to implement such a solution.
I am willing to write an example for such a use case.
If you are willing to give me some of the details privately I would be able to put up together an ICAP server as an example.

Eliezer

----
Eliezer Croitoru
Linux System Administrator
Mobile: +972-5-28704261
Email: eliezer at ngtech.co.il


-----Original Message-----
From: squid-users [mailto:squid-users-bounces at lists.squid-cache.org] On Behalf Of Amos Jeffries
Sent: Wednesday, March 15, 2017 4:04 AM
To: squid-users at lists.squid-cache.org
Subject: Re: [squid-users] Squid Authentication with HTTP REST API

On 14/03/2017 8:15 p.m., Serhat Koroglu wrote:
> Hello,
>
> Is there any possibilty implementing an authentication through a custom XML Web Service or HTTP REST API? What should I check?
>

Squid supports the HTTP authentication framework (RFC 7235
<https://tools.ietf.org/html/rfc7235>). Squid is intentionally designed
not to touch the message payloads.

If the API uses custom headers then you can possibly do it with an
external_acl_type helper that takes those headers and returns
credentials to Squid.

But, if the API uses message payloads you will likely need something
like an ICAP service or eCAP module to do the payload processing.


Amos

_______________________________________________
squid-users mailing list
squid-users at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-users

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20170320/c8dea21c/attachment.htm>

From turgut at kalfaoglu.com  Mon Mar 20 06:58:22 2017
From: turgut at kalfaoglu.com (=?UTF-8?Q?turgut_kalfao=c4=9flu?=)
Date: Mon, 20 Mar 2017 09:58:22 +0300
Subject: [squid-users] SSL
Message-ID: <8a64a97e-5356-cdb3-7f52-69b3af529f06@kalfaoglu.com>

Good morning everyone,

Congratulations on improving the SSL interception in squid; I am now 
caching https traffic - especially the beast called facebook.
Likewise google https works well; and I have no issues on the destop.

The androids are giving a few headaches:
1) WhatsApp videos do not download,
2) Facebook messenger will not connect.

Are these known issues, or shall I collect debugging info on them?
If there are any iptables or squid commands to bypass caching these; I'd 
be very glad if someone can share them.
I just want to be able to speed up the traffic without hindering any 
communication.

Many thanks,
Turgut Kalfaoglu



From tin at new-life.org.au  Mon Mar 20 07:19:54 2017
From: tin at new-life.org.au (Tim Bates)
Date: Mon, 20 Mar 2017 18:19:54 +1100
Subject: [squid-users] SSL Bump issues
In-Reply-To: <1489975099709-4681843.post@n4.nabble.com>
References: <1489975099709-4681843.post@n4.nabble.com>
Message-ID: <e0891943-26f9-c99d-5941-9f6bbdbcb80b@new-life.org.au>

Ignoring the Squid part, is it TLS 1.2 that's the root problem, or the 
ciphers?
Are you aware XP schannel.dll has some ciphers and protocols disabled by 
default, even though they're supported?

See here: 
https://support.microsoft.com/en-au/help/245030/how-to-restrict-the-use-of-certain-cryptographic-algorithms-and-protocols-in-schannel.dll

TB


On 20/03/2017 12:58 PM, mr_jrt wrote:
> Hello all,
>
> Brief version:
> Can't get ssl_bump working to get an old XP system's schannel.dll (i.e.
> built-in SSL) talking to a TLS 1.2 server, but works with Firefox (which has
> it's own SSL stack).
>
> Long version:
> This afternoon's task was to try and solve the issue of an old internal
> legacy XP system (and thus stuck on TLS 1.0) that can't be upgraded, but
> needs to be able to speak to servers running TLS 1.2. I've tried several
> approaches, but using squid with ssl_bump seemed to be the most appropriate
> solution, but for the life of me, I've not been able to get it to work
> properly, so was hoping for a few pointers.
>
> The software that needs to run uses the built-in schannel dll, but it can
> have a proxy specified, so things don't have to be transparent, ...but it
> does get stuck with all the limitations of the ancient schannel dll. Does
> however mean I can use the system's IE for testing.
>
> First up, I'm running Debian on my squid server. That means the distro
> packages don't have ssl support compiled in, so I had to compile my own
> packages. The version is 3.5.23, and the relevant configure output is:
>
>
>
> I had to compile against the older version of openssl due to the changes in
> their locking API, so I installed
> https://packages.debian.org/stretch/libssl1.0-dev, which enabled me to
> compile successfully.
>
> I've looked at countless examples, i.e.
> http://wiki.squid-cache.org/ConfigExamples/Intercept/SslBumpExplicit
>
> ...but the only way I've got any successful SSL proxying is with:
>
>
> ...but as expected, that's clearly not doing any bumping from the logs:
>
>
>
> When I put anything more in, i.e.
>
>
> Then it turns on the mode:
>
>
> ...but then I just get errors about no ciphers:
>
>
> I have a test site I'm using that I can fiddle with the ciphers on, and I
> can access it fine from the legacy system directly when I enable the old
> stuff (TLS 1.0, etc), but even then it seems to be squid's encryption (or
> maybe, decryption from the client?) that isn't working as it still won't
> connect regardless of what I try.
>
> Even if I throw in an explicit list of ciphers, copied from the target
> server (incidentally, the same host as squid, if that's relevant), still
> nada.
>
> Interestingly, ssl_bump seems to work perfectly fine from Firefox from the
> same machine, even when crippled down to TLS 1.0 only with the server set to
> restrict to TLS 1.2. So it seems to be doing what I want, just not for
> schannel.dll? I'm suspecting that openssl as used by squid can't speak any
> ciphers that schannel can, so it seems the issue isn't actually between
> squid and the target server, but between squid and the old client...
>
>
>
> --
> View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/SSL-Bump-issues-tp4681843.html
> Sent from the Squid - Users mailing list archive at Nabble.com.
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>



From eliezer at ngtech.co.il  Mon Mar 20 08:27:39 2017
From: eliezer at ngtech.co.il (Eliezer  Croitoru)
Date: Mon, 20 Mar 2017 10:27:39 +0200
Subject: [squid-users] Squid Authentication with HTTP REST API
In-Reply-To: <DB6PR0502MB3045CD1F88CE4D8E166A103FB43A0@DB6PR0502MB3045.eurprd05.prod.outlook.com>
References: <DB6PR0502MB304543ACB94E65D8F5CA8516B4240@DB6PR0502MB3045.eurprd05.prod.outlook.com>
 <0323d49e-3a2b-4fed-e12c-609306c9b150@treenet.co.nz>,
 <165d01d29d6a$b3a847d0$1af8d770$@ngtech.co.il>
 <DB6PR0502MB3045CD1F88CE4D8E166A103FB43A0@DB6PR0502MB3045.eurprd05.prod.outlook.com>
Message-ID: <001101d2a153$d6bca350$8435e9f0$@ngtech.co.il>

Hey Serhat,

The right way to support OAUTH2 or any similar idea would be using an ICAP
service or ECAP module(to my knowledge).
There might be a way to do it using an external_acl helper but I do not know
how and if it would be possible.

To my understanding OAUTH2 will use some redirection when a cookie is not
present and if present and valid then it will let you pass.
Also it will has a special token "portal" api which the OAUTH2 will redirect
towards in or order to get the cookie from the origin service.
In the backend when the request from the client to the api with the key will
be done the client token will be revalidated in the background
against the facebook or google or another OAUTH2 provider using the
developer API key.

Eliezer

----
http://ngtech.co.il/lmgtfy/
Linux System Administrator
Mobile: +972-5-28704261
Email: eliezer at ngtech.co.il


From: Serhat Koroglu [mailto:serhatkoroglu at outlook.com] 
Sent: Monday, March 20, 2017 8:49 AM
To: Eliezer Croitoru <eliezer at ngtech.co.il>;
squid-users at lists.squid-cache.org
Subject: Re: [squid-users] Squid Authentication with HTTP REST API

Sorry for late reply.
I have find a suitable?solution for validation through?a http web service.
Here tells developing custom helper even using
php:?http://freesoftwaremagazine.com/articles/authentication_with_squid/?Tha
t's nice.?

Then I know there is ?authentication with oauth2 for squid-server. But you
may be know, in oauth2 authentication, you must authorize the app using
user's credentials e.g. facebook username and password. When this oauth2
method is used, your app must redirect to the oauth2 service to authorize
your app.?

You may had used many web sites like that with facebook login. So my
question is how may?squid server do this redirect ?and authorization process
using?a third party oauth2 service? Squid asks username and password with
web browser popup. Is there any example to this??

Regards,
Serhat.
________________________________________
From: Eliezer Croitoru <mailto:eliezer at ngtech.co.il>
Sent: Wednesday, March 15, 2017 12:01:15 PM
To: 'Serhat Koroglu'
Cc: mailto:squid-users at lists.squid-cache.org
Subject: RE: [squid-users] Squid Authentication with HTTP REST API 
?
Hey Serhat,(first name right?)

>From what I understand you have a specific case.
Today the squid project doesn't have an example on how to implement such a
solution.
I am willing to write an example for such a use case.
If you are willing to give me some of the details privately I would be able
to put up together an ICAP server as an example.

Eliezer

----
Eliezer Croitoru
Linux System Administrator
Mobile: +972-5-28704261
Email: mailto:eliezer at ngtech.co.il


-----Original Message-----
From: squid-users [mailto:squid-users-bounces at lists.squid-cache.org] On
Behalf Of Amos Jeffries
Sent: Wednesday, March 15, 2017 4:04 AM
To: mailto:squid-users at lists.squid-cache.org
Subject: Re: [squid-users] Squid Authentication with HTTP REST API

On 14/03/2017 8:15 p.m., Serhat Koroglu wrote:
> Hello,
> 
> Is there any possibilty implementing an authentication through a custom
XML Web Service or HTTP REST API? What should I check?
> 

Squid supports the HTTP authentication framework (RFC 7235
<https://tools.ietf.org/html/rfc7235>). Squid is intentionally designed
not to touch the message payloads.

If the API uses custom headers then you can possibly do it with an
external_acl_type helper that takes those headers and returns
credentials to Squid.

But, if the API uses message payloads you will likely need something
like an ICAP service or eCAP module to do the payload processing.


Amos

_______________________________________________
squid-users mailing list
mailto:squid-users at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-users



From eliezer at ngtech.co.il  Mon Mar 20 08:28:16 2017
From: eliezer at ngtech.co.il (Eliezer  Croitoru)
Date: Mon, 20 Mar 2017 10:28:16 +0200
Subject: [squid-users] SSL
In-Reply-To: <8a64a97e-5356-cdb3-7f52-69b3af529f06@kalfaoglu.com>
References: <8a64a97e-5356-cdb3-7f52-69b3af529f06@kalfaoglu.com>
Message-ID: <001301d2a153$ec4a1040$c4de30c0$@ngtech.co.il>

What mobile devices are you using?

Eliezer

----
Eliezer Croitoru
Linux System Administrator
Mobile: +972-5-28704261
Email: eliezer at ngtech.co.il


-----Original Message-----
From: squid-users [mailto:squid-users-bounces at lists.squid-cache.org] On Behalf Of turgut kalfao?lu
Sent: Monday, March 20, 2017 8:58 AM
To: squid-users at lists.squid-cache.org
Subject: [squid-users] SSL

Good morning everyone,

Congratulations on improving the SSL interception in squid; I am now 
caching https traffic - especially the beast called facebook.
Likewise google https works well; and I have no issues on the destop.

The androids are giving a few headaches:
1) WhatsApp videos do not download,
2) Facebook messenger will not connect.

Are these known issues, or shall I collect debugging info on them?
If there are any iptables or squid commands to bypass caching these; I'd 
be very glad if someone can share them.
I just want to be able to speed up the traffic without hindering any 
communication.

Many thanks,
Turgut Kalfaoglu

_______________________________________________
squid-users mailing list
squid-users at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-users



From turgut at kalfaoglu.com  Mon Mar 20 12:25:14 2017
From: turgut at kalfaoglu.com (=?UTF-8?Q?turgut_kalfao=c4=9flu?=)
Date: Mon, 20 Mar 2017 15:25:14 +0300
Subject: [squid-users] SSL
In-Reply-To: <001301d2a153$ec4a1040$c4de30c0$@ngtech.co.il>
References: <8a64a97e-5356-cdb3-7f52-69b3af529f06@kalfaoglu.com>
 <001301d2a153$ec4a1040$c4de30c0$@ngtech.co.il>
Message-ID: <1beba701-b5f1-bfaf-f9b5-8117320956e8@kalfaoglu.com>

On 03/20/2017 11:28 AM, Eliezer Croitoru wrote:
> What mobile devices are you using?

It's an android 7 phone..
-turgut

>
> Eliezer
>
> ----
> Eliezer Croitoru
> Linux System Administrator
> Mobile: +972-5-28704261
> Email: eliezer at ngtech.co.il
>
>
> -----Original Message-----
> From: squid-users [mailto:squid-users-bounces at lists.squid-cache.org] On Behalf Of turgut kalfao?lu
> Sent: Monday, March 20, 2017 8:58 AM
> To: squid-users at lists.squid-cache.org
> Subject: [squid-users] SSL
>
> Good morning everyone,
>
> Congratulations on improving the SSL interception in squid; I am now
> caching https traffic - especially the beast called facebook.
> Likewise google https works well; and I have no issues on the destop.
>
> The androids are giving a few headaches:
> 1) WhatsApp videos do not download,
> 2) Facebook messenger will not connect.
>
> Are these known issues, or shall I collect debugging info on them?
> If there are any iptables or squid commands to bypass caching these; I'd
> be very glad if someone can share them.
> I just want to be able to speed up the traffic without hindering any
> communication.
>
> Many thanks,
> Turgut Kalfaoglu
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>
>



From uhlar at fantomas.sk  Mon Mar 20 15:20:49 2017
From: uhlar at fantomas.sk (Matus UHLAR - fantomas)
Date: Mon, 20 Mar 2017 16:20:49 +0100
Subject: [squid-users] squid workers question
In-Reply-To: <ab1f1823-51ba-89e8-1b81-e6a270c2159e@measurement-factory.com>
References: <20170309142134.GA30976@fantomas.sk>
 <f547a991-3178-7b95-113d-fb4b533c3056@measurement-factory.com>
 <20170309161940.GB7401@fantomas.sk>
 <6ca274d9-948c-01c5-c3dd-13044d3cfe7b@measurement-factory.com>
 <20170309165429.GA8584@fantomas.sk>
 <8ef542a5-dba3-236f-224b-944990d4cd68@measurement-factory.com>
 <20170309172435.GC8584@fantomas.sk>
 <7b386e00-97ed-054c-902f-0df367e8ec49@measurement-factory.com>
 <20170310093832.GB31011@fantomas.sk>
 <ab1f1823-51ba-89e8-1b81-e6a270c2159e@measurement-factory.com>
Message-ID: <20170320152049.GA26154@fantomas.sk>

On 10.03.17 08:52, Alex Rousskov wrote:
>Sorry, but that 2010 documentation is outdated. It was written before
>Rock store, a 2011 feature that changed what "SMP mode" means. This is
>my fault. Here is a replacement draft that I was working on until wiki
>went down:
>
>> NAME: workers
>> DEFAULT: 1
>> 	Number of main Squid processes or "workers" to fork and maintain.
>>
>> 	In a typical setup, each worker listens on all http_port(s) and
>> 	proxies requests without talking to other workers. Depending on
>> 	configuration, other Squid processes (e.g., rock store "diskers")
>> 	may also participate in request processing. All such Squid processes
>> 	are collectively called "kids".
>>
>> 	Setting workers to 0 disables kids creation and is similar to
>> 	running "squid -N ...". A positive value starts that many workers.

The default of 1 (only) creates kids for each rock store configured.

>> 	When multiple concurrent kids are in use, Squid is said to work in
>> 	"SMP mode". Some Squid features (e.g., ufs-based cache_dirs) are not
>> 	SMP-aware and should not or cannot be used in SMP mode.
>>
>> 	See http://wiki.squid-cache.org/Features/SmpScale for details.

very nice, thanks. However this is not meant for the wiki, but for:
http://www.squid-cache.org/Doc/config/workers/

maybe that pages could be updated (all but 3.2 versions are the same).


>The final version will probably move and extend the terminology-related
>text to the SMP section preamble -- it is kind of wrong to talk about
>diskers when documenting workers. Improvements and constructive
>suggestions welcomed!

compared to current version I'd change it to:

	1: start one main Squid process daemon (default)
            "no SMP" when rock store is not used
            "SMP" when rock store in use

>> so, with "workers 1", both aufs and rock should work properly, with rock
>> using separate process, correct?
>
>There are several ways to interpret your question, but the most likely
>interpretation leads to the "incorrect" answer: Without -N, a
>combination of "workers 1" and at least one "cache_dir rock" enables
>SMP. Do not use ufs-based cache_dirs in SMP mode.

That explains it. thanks.


-- 
Matus UHLAR - fantomas, uhlar at fantomas.sk ; http://www.fantomas.sk/
Warning: I wish NOT to receive e-mail advertising to this address.
Varovanie: na tuto adresu chcem NEDOSTAVAT akukolvek reklamnu postu.
99 percent of lawyers give the rest a bad name. 


From bosscb.chrisbren at gmail.com  Mon Mar 20 15:26:40 2017
From: bosscb.chrisbren at gmail.com (christian brendan)
Date: Mon, 20 Mar 2017 16:26:40 +0100
Subject: [squid-users] Squid Transparent/intercept Issues
Message-ID: <CAHptoxr2PKJFdjuyBehXmr4Mdg+6vqp+fwp3AHCGvb0C6DY0NA@mail.gmail.com>

Hello Everyone,

Squid Cache: Version 3.5.20
OS: CentOS 7

I have used squid for quite some times non transparently and it works,
problem kicks in when: http_port 3128 transparent is enabled.
Access denied error page shows up when transparent is enabled
ERRORThe requested URL could not be retrieved
------------------------------

The following error was encountered while trying to retrieve the URL:
http://www. <http://www.bing.com/>bing.com

*Access Denied.*

Access control configuration prevents your request from being allowed at
this time. Please contact your service provider if you feel this is
incorrect.

Your cache administrator is root
<root?subject=CacheErrorInfo%20-%20ERR_ACCESS_DENIED&body=CacheHost%3A%20radman.nocnet.comternet.com%0D%0AErrPage%3A%20ERR_ACCESS_DENIED%0D%0AErr%3A%20%5Bnone%5D%0D%0ATimeStamp%3A%20Mon,%2020%20Mar%202017%2013%3A09%3A32%20GMT%0D%0A%0D%0AClientIP%3A%2010.24.7.101%0D%0A%0D%0AHTTP%20Request%3A%0D%0AGET%20%2F%20HTTP%2F1.1%0AUpgrade-Insecure-Requests%3A%201%0D%0AUser-Agent%3A%20Mozilla%2F5.0%20(Windows%20NT%206.3%3B%20Win64%3B%20x64)%20AppleWebKit%2F537.36%20(KHTML,%20like%20Gecko)%20Chrome%2F56.0.2924.87%20Safari%2F537.36%0D%0AAccept%3A%20text%2Fhtml,application%2Fxhtml+xml,application%2Fxml%3Bq%3D0.9,image%2Fwebp,*%2F*%3Bq%3D0.8%0D%0AAccept-Encoding%3A%20gzip,%20deflate,%20sdch%0D%0AAccept-Language%3A%20en-US,en%3Bq%3D0.8%0D%0AVia%3A%201.1%20radman.nocnet.comternet.com%20(squid%2F3.5.20)%0D%0AX-Forwarded-For%3A%2010.24.7.99%0D%0ACache-Control%3A%20max-age%3D0%0D%0AConnection%3A%20keep-alive%0D%0AHost%3A%20www.servermom.org%0D%0A%0D%0A%0D%0A>
.


Some forums says
transparent was deprecated and replaced with "intercept"
<http://www.squid-cache.org/Doc/config/http_port/>
while others says otherwise.
Most confusing is, when http_port is set to transparent or intercept it
gives the same result
The only thing that seems to work is: http_port 3128 accel vhost
allow-direct
but i'm not comfortable with this because i do not think it was meant for
transparent operations besides it blocks https sites on the squid host
system.

Please i need advice on transparent mode best practices.

Is it http_port 3128 transparent or intercept or accel vhost allow-direct

which one is supported by the current version of squid 3.5.20 ?
Best Regards
Thanks
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20170320/a16d4868/attachment.htm>

From yvoinov at gmail.com  Mon Mar 20 15:44:27 2017
From: yvoinov at gmail.com (Yuri Voinov)
Date: Mon, 20 Mar 2017 21:44:27 +0600
Subject: [squid-users] Squid Transparent/intercept Issues
In-Reply-To: <CAHptoxr2PKJFdjuyBehXmr4Mdg+6vqp+fwp3AHCGvb0C6DY0NA@mail.gmail.com>
References: <CAHptoxr2PKJFdjuyBehXmr4Mdg+6vqp+fwp3AHCGvb0C6DY0NA@mail.gmail.com>
Message-ID: <16567b0c-c4ce-5d64-5843-ba149a183a74@gmail.com>

Did you tried our wiki:

http://wiki.squid-cache.org/ConfigExamples/Intercept

?

20.03.2017 21:26, christian brendan ?????:
> Hello Everyone,
>
> Squid Cache: Version 3.5.20
> OS: CentOS 7
>
> I have used squid for quite some times non transparently and it works,
> problem kicks in when: http_port 3128 transparent is enabled. 
> Access denied error page shows up when transparent is enabled
>
>
>   ERROR
>
>
>     The requested URL could not be retrieved
>
> ------------------------------------------------------------------------
>
> The following error was encountered while trying to retrieve the
> URL: http://www. <http://www.bing.com/>bing.com <http://bing.com/>
>
>     *Access Denied.*
>
> Access control configuration prevents your request from being allowed
> at this time. Please contact your service provider if you feel this is
> incorrect.
>
> Your cache administrator is root
> <mailto:root?subject=CacheErrorInfo%20-%20ERR_ACCESS_DENIED&body=CacheHost%3A%20radman.nocnet.comternet.com%0D%0AErrPage%3A%20ERR_ACCESS_DENIED%0D%0AErr%3A%20%5Bnone%5D%0D%0ATimeStamp%3A%20Mon,%2020%20Mar%202017%2013%3A09%3A32%20GMT%0D%0A%0D%0AClientIP%3A%2010.24.7.101%0D%0A%0D%0AHTTP%20Request%3A%0D%0AGET%20%2F%20HTTP%2F1.1%0AUpgrade-Insecure-Requests%3A%201%0D%0AUser-Agent%3A%20Mozilla%2F5.0%20%28Windows%20NT%206.3%3B%20Win64%3B%20x64%29%20AppleWebKit%2F537.36%20%28KHTML,%20like%20Gecko%29%20Chrome%2F56.0.2924.87%20Safari%2F537.36%0D%0AAccept%3A%20text%2Fhtml,application%2Fxhtml+xml,application%2Fxml%3Bq%3D0.9,image%2Fwebp,*%2F*%3Bq%3D0.8%0D%0AAccept-Encoding%3A%20gzip,%20deflate,%20sdch%0D%0AAccept-Language%3A%20en-US,en%3Bq%3D0.8%0D%0AVia%3A%201.1%20radman.nocnet.comternet.com%20%28squid%2F3.5.20%29%0D%0AX-Forwarded-For%3A%2010.24.7.99%0D%0ACache-Control%3A%20max-age%3D0%0D%0AConnection%3A%20keep-alive%0D%0AHost%3A%20www.servermom.org%0D%0A%0D%0A%0D%0A>.
>
>
>
> Some forums says
>
>
>       transparent was deprecated and replaced with "intercept"
>       <http://www.squid-cache.org/Doc/config/http_port/>
>
> while others says otherwise.
> Most confusing is, when http_port is set to transparent or intercept
> it gives the same result
> The only thing that seems to work is: http_port 3128 accel vhost
> allow-direct
> but i'm not comfortable with this because i do not think it was meant
> for transparent operations besides it blocks https sites on the squid
> host system.
>
> Please i need advice on transparent mode best practices.
>
> Is it http_port 3128 transparent or intercept or accel vhost allow-direct
>
> which one is supported by the current version of squid 3.5.20 ?
> Best Regards
> Thanks
>
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users

-- 
Bugs to the Future
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20170320/180aeed7/attachment.htm>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: 0x613DEC46.asc
Type: application/pgp-keys
Size: 2437 bytes
Desc: not available
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20170320/180aeed7/attachment.key>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: signature.asc
Type: application/pgp-signature
Size: 473 bytes
Desc: OpenPGP digital signature
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20170320/180aeed7/attachment.sig>

From Antony.Stone at squid.open.source.it  Mon Mar 20 15:56:17 2017
From: Antony.Stone at squid.open.source.it (Antony Stone)
Date: Mon, 20 Mar 2017 16:56:17 +0100
Subject: [squid-users] Squid Transparent/intercept Issues
In-Reply-To: <CAHptoxr2PKJFdjuyBehXmr4Mdg+6vqp+fwp3AHCGvb0C6DY0NA@mail.gmail.com>
References: <CAHptoxr2PKJFdjuyBehXmr4Mdg+6vqp+fwp3AHCGvb0C6DY0NA@mail.gmail.com>
Message-ID: <201703201656.18291.Antony.Stone@squid.open.source.it>

On Monday 20 March 2017 at 16:26:40, christian brendan wrote:

> Hello Everyone,
> 
> Squid Cache: Version 3.5.20
> OS: CentOS 7
> 
> I have used squid for quite some times non transparently and it works,
> problem kicks in when: http_port 3128 transparent is enabled.
> Access denied error page shows up when transparent is enabled
> ERRORThe requested URL could not be retrieved

How are you getting the packets to the Squid server for interception?

Is the Squid server in the default route between your clients and the 
Internet, or are you redirecting the packets to the Squid server somehow?

Please give *details* of how you are intercepting and sending the packets to 
Squid (eg: iptables rules, and which machine/s the rules are running on).


Antony.

-- 
Anything that improbable is effectively impossible.

 - Murray Gell-Mann, Nobel Prizewinner in Physics

                                                   Please reply to the list;
                                                         please *don't* CC me.


From uhlar at fantomas.sk  Mon Mar 20 16:15:16 2017
From: uhlar at fantomas.sk (Matus UHLAR - fantomas)
Date: Mon, 20 Mar 2017 17:15:16 +0100
Subject: [squid-users] SMP and AUFS
In-Reply-To: <579d3509-99c6-417e-ae1f-3f50402510a9@measurement-factory.com>
References: <BN6PR17MB11408ADA794A592AD0CA5B5AF7390@BN6PR17MB1140.namprd17.prod.outlook.com>
 <ecd815a6-b464-d22d-a3a0-4e8b2fa18203@measurement-factory.com>
 <BN6PR17MB1140460613B0187B1CE191C6F73B0@BN6PR17MB1140.namprd17.prod.outlook.com>
 <579d3509-99c6-417e-ae1f-3f50402510a9@measurement-factory.com>
Message-ID: <20170320161516.GB26154@fantomas.sk>

On 19.03.17 11:08, Alex Rousskov wrote:
>On 03/18/2017 11:11 PM, senor wrote:
>
>> There are many references in the squid wiki, FAQ and Knowlegebase about
>> SMP but I don't see any of them reflecting the concerns you have brought
>> up.
>
>There is a paragraph about these problems at [1] (search for "ufs") but
>I agree that better documentation, including wiki and
>squid.conf.documented changes/additions would be nice.
>
>  [1] http://wiki.squid-cache.org/Features/SmpScale
>
>
>> My point in mentioning that there are a lot of installations using
>> SMP and AUFS is that something widely used but buggy tends to be brought
>> up on this email list and I haven't seen it.
>
>IIRC, it has been brought up several times on the mailing lists and in
>Bugzilla. Once you dedicate each ufs-based store to each individual
>worker, most of the problems become subtle, often "invisible" to an
>admin because they "break" transactions, not Squid, especially if you do
>not use a mixture of ufs-based and rock stores. Using mailing list as an
>indicator that as subtle problem does _not_ exist is a risky strategy IMO.

Well, I personally will still be curious how much does SMP affect the case of
one worker and one or more diskers...

do diskers only provide I/O to the requestor? 

-- 
Matus UHLAR - fantomas, uhlar at fantomas.sk ; http://www.fantomas.sk/
Warning: I wish NOT to receive e-mail advertising to this address.
Varovanie: na tuto adresu chcem NEDOSTAVAT akukolvek reklamnu postu.
Depression is merely anger without enthusiasm. 


From rousskov at measurement-factory.com  Mon Mar 20 18:19:58 2017
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Mon, 20 Mar 2017 12:19:58 -0600
Subject: [squid-users] SMP and AUFS
In-Reply-To: <20170320161516.GB26154@fantomas.sk>
References: <BN6PR17MB11408ADA794A592AD0CA5B5AF7390@BN6PR17MB1140.namprd17.prod.outlook.com>
 <ecd815a6-b464-d22d-a3a0-4e8b2fa18203@measurement-factory.com>
 <BN6PR17MB1140460613B0187B1CE191C6F73B0@BN6PR17MB1140.namprd17.prod.outlook.com>
 <579d3509-99c6-417e-ae1f-3f50402510a9@measurement-factory.com>
 <20170320161516.GB26154@fantomas.sk>
Message-ID: <cd47a96b-357d-8cfd-41e4-d4d376da10c1@measurement-factory.com>

On 03/20/2017 10:15 AM, Matus UHLAR - fantomas wrote:

> Well, I personally will still be curious how much does SMP affect the
> case of one worker and one or more diskers...

I do not understand why you are asking this question in AUFS context.
AUFS does not use diskers! Today, only Rock store uses diskers (in SMP
mode). Some other [ufs-based] cache stores use various helper threads
and processes for I/O as well, but those helper processes are not
diskers or even kids in SMP terminology.


> do diskers only provide I/O to the requestor?

Diskers primary function is low-level disk cache I/O. Like all kids,
diskers respond to cache manager requests and Squid management events
(e.g. shutdown and reconfiguration). IIRC, diskers also build in-RAM
cache_dir index.

    http://wiki.squid-cache.org/Features/SmpScale#Terminology

HTH,

Alex.



From rousskov at measurement-factory.com  Mon Mar 20 18:32:44 2017
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Mon, 20 Mar 2017 12:32:44 -0600
Subject: [squid-users] squid workers question
In-Reply-To: <20170320152049.GA26154@fantomas.sk>
References: <20170309142134.GA30976@fantomas.sk>
 <f547a991-3178-7b95-113d-fb4b533c3056@measurement-factory.com>
 <20170309161940.GB7401@fantomas.sk>
 <6ca274d9-948c-01c5-c3dd-13044d3cfe7b@measurement-factory.com>
 <20170309165429.GA8584@fantomas.sk>
 <8ef542a5-dba3-236f-224b-944990d4cd68@measurement-factory.com>
 <20170309172435.GC8584@fantomas.sk>
 <7b386e00-97ed-054c-902f-0df367e8ec49@measurement-factory.com>
 <20170310093832.GB31011@fantomas.sk>
 <ab1f1823-51ba-89e8-1b81-e6a270c2159e@measurement-factory.com>
 <20170320152049.GA26154@fantomas.sk>
Message-ID: <5c14decf-fd76-b6cb-a497-85b4e226b34c@measurement-factory.com>

On 03/20/2017 09:20 AM, Matus UHLAR - fantomas wrote:
> On 10.03.17 08:52, Alex Rousskov wrote:
>> Sorry, but that 2010 documentation is outdated. It was written before
>> Rock store, a 2011 feature that changed what "SMP mode" means. This is
>> my fault. Here is a replacement draft that I was working on until wiki
>> went down:
>>
>>> NAME: workers
>>> DEFAULT: 1
>>>     Number of main Squid processes or "workers" to fork and maintain.
>>>
>>>     In a typical setup, each worker listens on all http_port(s) and
>>>     proxies requests without talking to other workers. Depending on
>>>     configuration, other Squid processes (e.g., rock store "diskers")
>>>     may also participate in request processing. All such Squid processes
>>>     are collectively called "kids".
>>>
>>>     Setting workers to 0 disables kids creation and is similar to
>>>     running "squid -N ...". A positive value starts that many workers.

> The default of 1 (only) creates kids for each rock store configured.

What makes you think that? I believe "workers 1" in the presence of rock
cache_dirs should create one kid to handle HTTP transaction _plus_ one
kid for each rock cache_dir.


>>>     When multiple concurrent kids are in use, Squid is said to work in
>>>     "SMP mode". Some Squid features (e.g., ufs-based cache_dirs) are not
>>>     SMP-aware and should not or cannot be used in SMP mode.
>>>
>>>     See http://wiki.squid-cache.org/Features/SmpScale for details.

> very nice, thanks. However this is not meant for the wiki, but for:
> http://www.squid-cache.org/Doc/config/workers/

To be more precise, the text is meant for src/cf.data.pre, from which
squid.conf.documented (and Doc/Config pages) are generated from. Not
sure why you say "However" though.


> maybe that pages could be updated (all but 3.2 versions are the same).

Once the above worker documentation changes are polished and committed
to the Squid repository, the affected generated pages/files will be
updated automatically.

The documentation for earlier versions may never be updated though -- it
depends on whether the changes are going to be ported and committed to
the code branches corresponding to those earlier versions.


>> The final version will probably move and extend the terminology-related
>> text to the SMP section preamble -- it is kind of wrong to talk about
>> diskers when documenting workers. Improvements and constructive
>> suggestions welcomed!
> 
> compared to current version I'd change it to:
> 
>     1: start one main Squid process daemon (default)
>            "no SMP" when rock store is not used
>            "SMP" when rock store in use

I agree that we should add something like this as a common-case example
of general rules. Thank you.

Alex.



From uhlar at fantomas.sk  Mon Mar 20 19:49:06 2017
From: uhlar at fantomas.sk (Matus UHLAR - fantomas)
Date: Mon, 20 Mar 2017 20:49:06 +0100
Subject: [squid-users] squid workers question
In-Reply-To: <5c14decf-fd76-b6cb-a497-85b4e226b34c@measurement-factory.com>
References: <20170309161940.GB7401@fantomas.sk>
 <6ca274d9-948c-01c5-c3dd-13044d3cfe7b@measurement-factory.com>
 <20170309165429.GA8584@fantomas.sk>
 <8ef542a5-dba3-236f-224b-944990d4cd68@measurement-factory.com>
 <20170309172435.GC8584@fantomas.sk>
 <7b386e00-97ed-054c-902f-0df367e8ec49@measurement-factory.com>
 <20170310093832.GB31011@fantomas.sk>
 <ab1f1823-51ba-89e8-1b81-e6a270c2159e@measurement-factory.com>
 <20170320152049.GA26154@fantomas.sk>
 <5c14decf-fd76-b6cb-a497-85b4e226b34c@measurement-factory.com>
Message-ID: <20170320194906.GA30456@fantomas.sk>

>> On 10.03.17 08:52, Alex Rousskov wrote:
>>> Sorry, but that 2010 documentation is outdated. It was written before
>>> Rock store, a 2011 feature that changed what "SMP mode" means. This is
>>> my fault. Here is a replacement draft that I was working on until wiki
>>> went down:
>>>
>>>> NAME: workers
>>>> DEFAULT: 1
>>>>     Number of main Squid processes or "workers" to fork and maintain.
>>>>
>>>>     In a typical setup, each worker listens on all http_port(s) and
>>>>     proxies requests without talking to other workers. Depending on
>>>>     configuration, other Squid processes (e.g., rock store "diskers")
>>>>     may also participate in request processing. All such Squid processes
>>>>     are collectively called "kids".
>>>>
>>>>     Setting workers to 0 disables kids creation and is similar to
>>>>     running "squid -N ...". A positive value starts that many workers.

>On 03/20/2017 09:20 AM, Matus UHLAR - fantomas wrote:
>> The default of 1 (only) creates kids for each rock store configured.

On 20.03.17 12:32, Alex Rousskov wrote:
>What makes you think that? I believe "workers 1" in the presence of rock
>cache_dirs should create one kid to handle HTTP transaction _plus_ one
>kid for each rock cache_dir.

That's exactly what I meant, for inclusion to your paragraph.
Should I replace "kids" with "one extra kid"?
and should I replace (only) by "however"?

>>>>     When multiple concurrent kids are in use, Squid is said to work in
>>>>     "SMP mode". Some Squid features (e.g., ufs-based cache_dirs) are not
>>>>     SMP-aware and should not or cannot be used in SMP mode.
>>>>
>>>>     See http://wiki.squid-cache.org/Features/SmpScale for details.
>
>> very nice, thanks. However this is not meant for the wiki, but for:
>> http://www.squid-cache.org/Doc/config/workers/
>
>To be more precise, the text is meant for src/cf.data.pre, from which
>squid.conf.documented (and Doc/Config pages) are generated from. Not
>sure why you say "However" though.

You mentioned you were working on the draft until wiki went down.
I understood the paragraph as replacement for "workers" documentation, not
as something to be written to wiki...

>> maybe that pages could be updated (all but 3.2 versions are the same).
>
>Once the above worker documentation changes are polished and committed
>to the Squid repository, the affected generated pages/files will be
>updated automatically.
>
>The documentation for earlier versions may never be updated though -- it
>depends on whether the changes are going to be ported and committed to
>the code branches corresponding to those earlier versions.

it's up to the release team.
I would recommend update the docs on the web to avoid issues for people
using older squid versions, e.g. in enterprise environment

>>> The final version will probably move and extend the terminology-related
>>> text to the SMP section preamble -- it is kind of wrong to talk about
>>> diskers when documenting workers. Improvements and constructive
>>> suggestions welcomed!
>>
>> compared to current version I'd change it to:
>>
>>     1: start one main Squid process daemon (default)
>>            "no SMP" when rock store is not used
>>            "SMP" when rock store in use
>
>I agree that we should add something like this as a common-case example
>of general rules. Thank you.

if we replace the current paragraph with your proposed one, I have proposed
change at the top

-- 
Matus UHLAR - fantomas, uhlar at fantomas.sk ; http://www.fantomas.sk/
Warning: I wish NOT to receive e-mail advertising to this address.
Varovanie: na tuto adresu chcem NEDOSTAVAT akukolvek reklamnu postu.
Eagles may soar, but weasels don't get sucked into jet engines. 


From rousskov at measurement-factory.com  Mon Mar 20 20:08:48 2017
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Mon, 20 Mar 2017 14:08:48 -0600
Subject: [squid-users] SSL Bump issues
In-Reply-To: <1489975099709-4681843.post@n4.nabble.com>
References: <1489975099709-4681843.post@n4.nabble.com>
Message-ID: <d729abc8-9a3a-25e0-9185-d1cdbd2d91cc@measurement-factory.com>

On 03/19/2017 07:58 PM, mr_jrt wrote:

> ...but the only way I've got any successful SSL proxying is with:
> 
> 
> ...but as expected, that's clearly not doing any bumping from the logs:
> 
> 
> 
> When I put anything more in, i.e.
> 
> 
> Then it turns on the mode:
> 
> 
> ...but then I just get errors about no ciphers:
> 

Please note that your configuration and other details in the post did
not get through to the mailing list (probably due to some fancy quoting
provided by Nabble that does not get through to the actual squid-users
mailing list).

Alex.



From sohanwijetunga at gmail.com  Tue Mar 21 07:05:25 2017
From: sohanwijetunga at gmail.com (Sohan Wijetunga)
Date: Tue, 21 Mar 2017 12:35:25 +0530
Subject: [squid-users] blocking or allowing specific youtube videos
Message-ID: <CAOUuUH671PqQQF4sd9ykGarqFiVOp_TZ8HMs6GfEBh3QTVjkwA@mail.gmail.com>

Project subject is blocking or allowing specific youtube videos. For that
research I hope to add more features but currently I?m stuck to take full
urls from clients. According to my project, environment should be client
server environment. All the client?s youtube traffic should be manage
through the gateway. I currently following squid helper programs it seems
to be fulfil my requirement but those examples are not enough for testing.
Using of squid helper program is to do some development in my research
future. I really need to do that project using squid.



 I look forward to hearing from you soon.

Thank you.

Best Regards,

Sohan.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20170321/435d3a19/attachment.htm>

From rafael.akchurin at diladele.com  Tue Mar 21 08:02:08 2017
From: rafael.akchurin at diladele.com (Rafael Akchurin)
Date: Tue, 21 Mar 2017 08:02:08 +0000
Subject: [squid-users] blocking or allowing specific youtube videos
In-Reply-To: <CAOUuUH671PqQQF4sd9ykGarqFiVOp_TZ8HMs6GfEBh3QTVjkwA@mail.gmail.com>
References: <CAOUuUH671PqQQF4sd9ykGarqFiVOp_TZ8HMs6GfEBh3QTVjkwA@mail.gmail.com>
Message-ID: <DB6PR0401MB26804C158EE39A692C113D438F3D0@DB6PR0401MB2680.eurprd04.prod.outlook.com>

Hello Sohan,

I guess this may be helpful in your project https://docs.diladele.com/faq/filtering/youtube_allow_only_specific_video.html
This can easily be done within squid.conf (ICAP is not required).

Best regards,
Rafael Akchurin
Diladele B.V.
https://www.diladele.com

--
Please take a look at Web Safety - our ICAP based web filter server for Squid proxy.


From: squid-users [mailto:squid-users-bounces at lists.squid-cache.org] On Behalf Of Sohan Wijetunga
Sent: Tuesday, March 21, 2017 8:05 AM
To: squid-users at lists.squid-cache.org
Subject: [squid-users] blocking or allowing specific youtube videos

Project subject is blocking or allowing specific youtube videos. For that research I hope to add more features but currently I?m stuck to take full urls from clients. According to my project, environment should be client server environment. All the client?s youtube traffic should be manage through the gateway. I currently following squid helper programs it seems to be fulfil my requirement but those examples are not enough for testing. Using of squid helper program is to do some development in my research future. I really need to do that project using squid.

 I look forward to hearing from you soon.
Thank you.
Best Regards,
Sohan.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20170321/2154604a/attachment.htm>

From marcus.kool at urlfilterdb.com  Tue Mar 21 09:54:51 2017
From: marcus.kool at urlfilterdb.com (Marcus Kool)
Date: Tue, 21 Mar 2017 06:54:51 -0300
Subject: [squid-users] blocking or allowing specific youtube videos
In-Reply-To: <CAOUuUH671PqQQF4sd9ykGarqFiVOp_TZ8HMs6GfEBh3QTVjkwA@mail.gmail.com>
References: <CAOUuUH671PqQQF4sd9ykGarqFiVOp_TZ8HMs6GfEBh3QTVjkwA@mail.gmail.com>
Message-ID: <f9589aac-8638-06f6-1760-bc333304a6a3@urlfilterdb.com>

ufdbGuard is a URL filter which given the input
    www.youtube.com/watch?v=XXXXXXXXXX
blocks the following URLs:
    www.youtube.com/watch?v=XXXXXXXXXX
    www.youtube.com/embed/XXXXXXXXXX
    www.youtube.com/get_video_info?video_id=XXXXXXXXXX
    ytimg.googleusercontent.com/vi/XXXXXXXXXX/
    i.ytimg.com/vi/XXXXXXXXXX/
    ...
ufdbGuard also blocks users who try to circumvent the URL filter with URLs like
    www.youtube.com/watch?foo=1&v=XXXXXXXXXX&bar=2

The acls of ufdbGuard can block or allow any set of URLs.

Marcus


On 21/03/17 04:05, Sohan Wijetunga wrote:
> Project subject is blocking or allowing specific youtube videos. For that research I hope to add more features but currently I?m stuck to take full urls from clients. According to my project,
> environment should be client server environment. All the client?s youtube traffic should be manage through the gateway. I currently following squid helper programs it seems to be fulfil my requirement
> but those examples are not enough for testing. Using of squid helper program is to do some development in my research future. I really need to do that project using squid.
>
>
>
>  I look forward to hearing from you soon.
>
> Thank you.
>
> Best Regards,
>
> Sohan.
>
>
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>


From listener at wernig.net  Tue Mar 21 10:35:02 2017
From: listener at wernig.net (Markus Wernig)
Date: Tue, 21 Mar 2017 11:35:02 +0100
Subject: [squid-users] Squid blocking own OCSP/AIA requests
Message-ID: <c2bbc299-849b-063b-1ed4-6b9517ba987c@wernig.net>

Hi all

I have configured Squid 4.0.18 (CentOS) with sslbump and clamav as
ecap_service. This works well.

One thing I've noticed though, are constant log entries like this in
access.log:

2017-03-21 10:35:08.338 +0100 000137 - TCP_DENIED/403 3607 GET
http://apps.identrust.com/roots/dstrootcax3.p7c - HIER_NONE/-
text/html;charset=utf-8 -
2017-03-21 10:35:08.345 +0100 000161 10.254.254.2 NONE/200 0 CONNECT
letsencrypt.org:443 - HIER_DIRECT/letsencrypt.org - -

It appears that this is the OCSP URI for Letsencrypt certificates.

And in fact every time this is logged, a CONNECT to a https uri is
logged that is using a Letsencrypt certificate (like eg.
https://letsencrypt.org).

Given that there is no client IP logged, I assume that squid is blocking
its own outgoing OCSP request here (the browser is configured to NOT use
OCSP).

The same seems to happen when there's no OCSP URI, but a regular AIA URI
in the certificate:

2017-03-21 10:36:19.773 +0100 000000 - TCP_DENIED/403 3734 GET
http://swisssign.net/cgi-bin/authority/download/5B257B96A465517EB839F3C078665EE83AE7F0EE
- HIER_NONE/- text/html;charset=utf-8 -
2017-03-21 10:36:19.782 +0100 000038 10.254.254.2 NONE/200 0 CONNECT
swisssign.net:443 - HIER_DIRECT/swisssign.net - -

I do have "http_access allow localhost" in squid.conf, but since there's
no IP associated with the request, this does not seem to help.

Is there a way to allow these outgoing internal requests? I've looked
through the FAQ and wiki, but couldn't find anything on the topic.

Thanks & best

/markus


From bosscb.chrisbren at gmail.com  Tue Mar 21 11:00:05 2017
From: bosscb.chrisbren at gmail.com (christian brendan)
Date: Tue, 21 Mar 2017 12:00:05 +0100
Subject: [squid-users] Squid Transparent/intercept Issues
Message-ID: <CAHptoxo6qw2FJYhDY94BubFL85Uj1Yrye88zcA1KYaGU6hhwmQ@mail.gmail.com>

Re: Squid Transparent/intercept Issues

On Tue, Mar 21, 2017 at 8:05 AM, <squid-users-request at lists.squid-cache.org>
wrote:

> Send squid-users mailing list submissions to
>         squid-users at lists.squid-cache.org
>
> To subscribe or unsubscribe via the World Wide Web, visit
>         http://lists.squid-cache.org/listinfo/squid-users
> or, via email, send a message with subject or body 'help' to
>         squid-users-request at lists.squid-cache.org
>
> You can reach the person managing the list at
>         squid-users-owner at lists.squid-cache.org
>
> When replying, please edit your Subject line so it is more specific
> than "Re: Contents of squid-users digest..."
>
>
> Today's Topics:
>
>    1. Re: Squid Transparent/intercept Issues (Antony Stone)
>    2. Re: SMP and AUFS (Matus UHLAR - fantomas)
>    3. Re: SMP and AUFS (Alex Rousskov)
>    4. Re: squid workers question (Alex Rousskov)
>    5. Re: squid workers question (Matus UHLAR - fantomas)
>    6. Re: SSL Bump issues (Alex Rousskov)
>    7. blocking or allowing specific youtube videos (Sohan Wijetunga)
>
>
> ----------------------------------------------------------------------
>
> Message: 1
> Date: Mon, 20 Mar 2017 16:56:17 +0100
> From: Antony Stone <Antony.Stone at squid.open.source.it>
> To: squid-users at lists.squid-cache.org
> Subject: Re: [squid-users] Squid Transparent/intercept Issues
> Message-ID: <201703201656.18291.Antony.Stone at squid.open.source.it>
> Content-Type: Text/Plain;  charset="iso-8859-15"
>
> On Monday 20 March 2017 at 16:26:40, christian brendan wrote:
>
> > Hello Everyone,
> >
> > Squid Cache: Version 3.5.20
> > OS: CentOS 7
> >
> > I have used squid for quite some times non transparently and it works,
> > problem kicks in when: http_port 3128 transparent is enabled.
> > Access denied error page shows up when transparent is enabled
> > ERRORThe requested URL could not be retrieved
>
> How are you getting the packets to the Squid server for interception?
>
> Is the Squid server in the default route between your clients and the
> Internet, or are you redirecting the packets to the Squid server somehow?
>
> Please give *details* of how you are intercepting and sending the packets
> to
> Squid (eg: iptables rules, and which machine/s the rules are running on).
>
>
> Antony.
>
> --
> Anything that improbable is effectively impossible.
>
>  - Murray Gell-Mann, Nobel Prizewinner in Physics
>
>                                                    Please reply to the
> list;
>                                                          please *don't* CC
> me.
>
>
> ------------------------------
>
> Message: 2
> Date: Mon, 20 Mar 2017 17:15:16 +0100
> From: Matus UHLAR - fantomas <uhlar at fantomas.sk>
> To: squid-users at lists.squid-cache.org
> Subject: Re: [squid-users] SMP and AUFS
> Message-ID: <20170320161516.GB26154 at fantomas.sk>
> Content-Type: text/plain; charset=us-ascii; format=flowed
>
> On 19.03.17 11:08, Alex Rousskov wrote:
> >On 03/18/2017 11:11 PM, senor wrote:
> >
> >> There are many references in the squid wiki, FAQ and Knowlegebase about
> >> SMP but I don't see any of them reflecting the concerns you have brought
> >> up.
> >
> >There is a paragraph about these problems at [1] (search for "ufs") but
> >I agree that better documentation, including wiki and
> >squid.conf.documented changes/additions would be nice.
> >
> >  [1] http://wiki.squid-cache.org/Features/SmpScale
> >
> >
> >> My point in mentioning that there are a lot of installations using
> >> SMP and AUFS is that something widely used but buggy tends to be brought
> >> up on this email list and I haven't seen it.
> >
> >IIRC, it has been brought up several times on the mailing lists and in
> >Bugzilla. Once you dedicate each ufs-based store to each individual
> >worker, most of the problems become subtle, often "invisible" to an
> >admin because they "break" transactions, not Squid, especially if you do
> >not use a mixture of ufs-based and rock stores. Using mailing list as an
> >indicator that as subtle problem does _not_ exist is a risky strategy IMO.
>
> Well, I personally will still be curious how much does SMP affect the case
> of
> one worker and one or more diskers...
>
> do diskers only provide I/O to the requestor?
>
> --
> Matus UHLAR - fantomas, uhlar at fantomas.sk ; http://www.fantomas.sk/
> Warning: I wish NOT to receive e-mail advertising to this address.
> Varovanie: na tuto adresu chcem NEDOSTAVAT akukolvek reklamnu postu.
> Depression is merely anger without enthusiasm.
>
>
> ------------------------------
>
> Message: 3
> Date: Mon, 20 Mar 2017 12:19:58 -0600
> From: Alex Rousskov <rousskov at measurement-factory.com>
> To: squid-users at lists.squid-cache.org
> Subject: Re: [squid-users] SMP and AUFS
> Message-ID:
>         <cd47a96b-357d-8cfd-41e4-d4d376da10c1 at measurement-factory.com>
> Content-Type: text/plain; charset=utf-8
>
> On 03/20/2017 10:15 AM, Matus UHLAR - fantomas wrote:
>
> > Well, I personally will still be curious how much does SMP affect the
> > case of one worker and one or more diskers...
>
> I do not understand why you are asking this question in AUFS context.
> AUFS does not use diskers! Today, only Rock store uses diskers (in SMP
> mode). Some other [ufs-based] cache stores use various helper threads
> and processes for I/O as well, but those helper processes are not
> diskers or even kids in SMP terminology.
>
>
> > do diskers only provide I/O to the requestor?
>
> Diskers primary function is low-level disk cache I/O. Like all kids,
> diskers respond to cache manager requests and Squid management events
> (e.g. shutdown and reconfiguration). IIRC, diskers also build in-RAM
> cache_dir index.
>
>     http://wiki.squid-cache.org/Features/SmpScale#Terminology
>
> HTH,
>
> Alex.
>
>
>
> ------------------------------
>
> Message: 4
> Date: Mon, 20 Mar 2017 12:32:44 -0600
> From: Alex Rousskov <rousskov at measurement-factory.com>
> To: squid-users at lists.squid-cache.org
> Subject: Re: [squid-users] squid workers question
> Message-ID:
>         <5c14decf-fd76-b6cb-a497-85b4e226b34c at measurement-factory.com>
> Content-Type: text/plain; charset=utf-8
>
> On 03/20/2017 09:20 AM, Matus UHLAR - fantomas wrote:
> > On 10.03.17 08:52, Alex Rousskov wrote:
> >> Sorry, but that 2010 documentation is outdated. It was written before
> >> Rock store, a 2011 feature that changed what "SMP mode" means. This is
> >> my fault. Here is a replacement draft that I was working on until wiki
> >> went down:
> >>
> >>> NAME: workers
> >>> DEFAULT: 1
> >>>     Number of main Squid processes or "workers" to fork and maintain.
> >>>
> >>>     In a typical setup, each worker listens on all http_port(s) and
> >>>     proxies requests without talking to other workers. Depending on
> >>>     configuration, other Squid processes (e.g., rock store "diskers")
> >>>     may also participate in request processing. All such Squid
> processes
> >>>     are collectively called "kids".
> >>>
> >>>     Setting workers to 0 disables kids creation and is similar to
> >>>     running "squid -N ...". A positive value starts that many workers.
>
> > The default of 1 (only) creates kids for each rock store configured.
>
> What makes you think that? I believe "workers 1" in the presence of rock
> cache_dirs should create one kid to handle HTTP transaction _plus_ one
> kid for each rock cache_dir.
>
>
> >>>     When multiple concurrent kids are in use, Squid is said to work in
> >>>     "SMP mode". Some Squid features (e.g., ufs-based cache_dirs) are
> not
> >>>     SMP-aware and should not or cannot be used in SMP mode.
> >>>
> >>>     See http://wiki.squid-cache.org/Features/SmpScale for details.
>
> > very nice, thanks. However this is not meant for the wiki, but for:
> > http://www.squid-cache.org/Doc/config/workers/
>
> To be more precise, the text is meant for src/cf.data.pre, from which
> squid.conf.documented (and Doc/Config pages) are generated from. Not
> sure why you say "However" though.
>
>
> > maybe that pages could be updated (all but 3.2 versions are the same).
>
> Once the above worker documentation changes are polished and committed
> to the Squid repository, the affected generated pages/files will be
> updated automatically.
>
> The documentation for earlier versions may never be updated though -- it
> depends on whether the changes are going to be ported and committed to
> the code branches corresponding to those earlier versions.
>
>
> >> The final version will probably move and extend the terminology-related
> >> text to the SMP section preamble -- it is kind of wrong to talk about
> >> diskers when documenting workers. Improvements and constructive
> >> suggestions welcomed!
> >
> > compared to current version I'd change it to:
> >
> >     1: start one main Squid process daemon (default)
> >            "no SMP" when rock store is not used
> >            "SMP" when rock store in use
>
> I agree that we should add something like this as a common-case example
> of general rules. Thank you.
>
> Alex.
>
>
>
> ------------------------------
>
> Message: 5
> Date: Mon, 20 Mar 2017 20:49:06 +0100
> From: Matus UHLAR - fantomas <uhlar at fantomas.sk>
> To: squid-users at lists.squid-cache.org
> Subject: Re: [squid-users] squid workers question
> Message-ID: <20170320194906.GA30456 at fantomas.sk>
> Content-Type: text/plain; charset=us-ascii; format=flowed
>
> >> On 10.03.17 08:52, Alex Rousskov wrote:
> >>> Sorry, but that 2010 documentation is outdated. It was written before
> >>> Rock store, a 2011 feature that changed what "SMP mode" means. This is
> >>> my fault. Here is a replacement draft that I was working on until wiki
> >>> went down:
> >>>
> >>>> NAME: workers
> >>>> DEFAULT: 1
> >>>>     Number of main Squid processes or "workers" to fork and maintain.
> >>>>
> >>>>     In a typical setup, each worker listens on all http_port(s) and
> >>>>     proxies requests without talking to other workers. Depending on
> >>>>     configuration, other Squid processes (e.g., rock store "diskers")
> >>>>     may also participate in request processing. All such Squid
> processes
> >>>>     are collectively called "kids".
> >>>>
> >>>>     Setting workers to 0 disables kids creation and is similar to
> >>>>     running "squid -N ...". A positive value starts that many workers.
>
> >On 03/20/2017 09:20 AM, Matus UHLAR - fantomas wrote:
> >> The default of 1 (only) creates kids for each rock store configured.
>
> On 20.03.17 12:32, Alex Rousskov wrote:
> >What makes you think that? I believe "workers 1" in the presence of rock
> >cache_dirs should create one kid to handle HTTP transaction _plus_ one
> >kid for each rock cache_dir.
>
> That's exactly what I meant, for inclusion to your paragraph.
> Should I replace "kids" with "one extra kid"?
> and should I replace (only) by "however"?
>
> >>>>     When multiple concurrent kids are in use, Squid is said to work in
> >>>>     "SMP mode". Some Squid features (e.g., ufs-based cache_dirs) are
> not
> >>>>     SMP-aware and should not or cannot be used in SMP mode.
> >>>>
> >>>>     See http://wiki.squid-cache.org/Features/SmpScale for details.
> >
> >> very nice, thanks. However this is not meant for the wiki, but for:
> >> http://www.squid-cache.org/Doc/config/workers/
> >
> >To be more precise, the text is meant for src/cf.data.pre, from which
> >squid.conf.documented (and Doc/Config pages) are generated from. Not
> >sure why you say "However" though.
>
> You mentioned you were working on the draft until wiki went down.
> I understood the paragraph as replacement for "workers" documentation, not
> as something to be written to wiki...
>
> >> maybe that pages could be updated (all but 3.2 versions are the same).
> >
> >Once the above worker documentation changes are polished and committed
> >to the Squid repository, the affected generated pages/files will be
> >updated automatically.
> >
> >The documentation for earlier versions may never be updated though -- it
> >depends on whether the changes are going to be ported and committed to
> >the code branches corresponding to those earlier versions.
>
> it's up to the release team.
> I would recommend update the docs on the web to avoid issues for people
> using older squid versions, e.g. in enterprise environment
>
> >>> The final version will probably move and extend the terminology-related
> >>> text to the SMP section preamble -- it is kind of wrong to talk about
> >>> diskers when documenting workers. Improvements and constructive
> >>> suggestions welcomed!
> >>
> >> compared to current version I'd change it to:
> >>
> >>     1: start one main Squid process daemon (default)
> >>            "no SMP" when rock store is not used
> >>            "SMP" when rock store in use
> >
> >I agree that we should add something like this as a common-case example
> >of general rules. Thank you.
>
> if we replace the current paragraph with your proposed one, I have proposed
> change at the top
>
> --
> Matus UHLAR - fantomas, uhlar at fantomas.sk ; http://www.fantomas.sk/
> Warning: I wish NOT to receive e-mail advertising to this address.
> Varovanie: na tuto adresu chcem NEDOSTAVAT akukolvek reklamnu postu.
> Eagles may soar, but weasels don't get sucked into jet engines.
>
>
> ------------------------------
>
> Message: 6
> Date: Mon, 20 Mar 2017 14:08:48 -0600
> From: Alex Rousskov <rousskov at measurement-factory.com>
> To: squid-users at lists.squid-cache.org
> Subject: Re: [squid-users] SSL Bump issues
> Message-ID:
>         <d729abc8-9a3a-25e0-9185-d1cdbd2d91cc at measurement-factory.com>
> Content-Type: text/plain; charset=utf-8
>
> On 03/19/2017 07:58 PM, mr_jrt wrote:
>
> > ...but the only way I've got any successful SSL proxying is with:
> >
> >
> > ...but as expected, that's clearly not doing any bumping from the logs:
> >
> >
> >
> > When I put anything more in, i.e.
> >
> >
> > Then it turns on the mode:
> >
> >
> > ...but then I just get errors about no ciphers:
> >
>
> Please note that your configuration and other details in the post did
> not get through to the mailing list (probably due to some fancy quoting
> provided by Nabble that does not get through to the actual squid-users
> mailing list).
>
> Alex.
>
>
>
> ------------------------------
>
> Message: 7
> Date: Tue, 21 Mar 2017 12:35:25 +0530
> From: Sohan Wijetunga <sohanwijetunga at gmail.com>
> To: squid-users at lists.squid-cache.org
> Subject: [squid-users] blocking or allowing specific youtube videos
> Message-ID:
>         <CAOUuUH671PqQQF4sd9ykGarqFiVOp_TZ8HMs6GfEBh3QTVjkwA at mail.
> gmail.com>
> Content-Type: text/plain; charset="utf-8"
>
> Project subject is blocking or allowing specific youtube videos. For that
> research I hope to add more features but currently I?m stuck to take full
> urls from clients. According to my project, environment should be client
> server environment. All the client?s youtube traffic should be manage
> through the gateway. I currently following squid helper programs it seems
> to be fulfil my requirement but those examples are not enough for testing.
> Using of squid helper program is to do some development in my research
> future. I really need to do that project using squid.
>
>
>
>  I look forward to hearing from you soon.
>
> Thank you.
>
> Best Regards,
>
> Sohan.
> -------------- next part --------------
> An HTML attachment was scrubbed...
> URL: <http://lists.squid-cache.org/pipermail/squid-users/
> attachments/20170321/435d3a19/attachment.html>
>
> ------------------------------
>
> Subject: Digest Footer
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>
>
> ------------------------------
>
> End of squid-users Digest, Vol 31, Issue 59
> *******************************************
>



?@Antony.Stone
1. ?I am using mikrotik routerboard to redirect traffic, with this rule:
dd action=dst-nat chain=dstnat comment="Redirect port 80 to SquidProxy"
dst-port=80 protocol=tcp \ src-address=10.24.7.100 to-addresses=10.24.7.101
to-ports=3128

3.? It is not in default route, packets is been redirected.

?4. There is no iptable rules, firewall is disabled for this test.

Regards
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20170321/485abf31/attachment.htm>

From Antony.Stone at squid.open.source.it  Tue Mar 21 11:12:01 2017
From: Antony.Stone at squid.open.source.it (Antony Stone)
Date: Tue, 21 Mar 2017 12:12:01 +0100
Subject: [squid-users] Squid Transparent/intercept Issues
In-Reply-To: <CAHptoxo6qw2FJYhDY94BubFL85Uj1Yrye88zcA1KYaGU6hhwmQ@mail.gmail.com>
References: <CAHptoxo6qw2FJYhDY94BubFL85Uj1Yrye88zcA1KYaGU6hhwmQ@mail.gmail.com>
Message-ID: <201703211212.01346.Antony.Stone@squid.open.source.it>

On Tuesday 21 March 2017 at 12:00:05, christian brendan wrote:

> > Today's Topics:
> >    1. Re: Squid Transparent/intercept Issues (Antony Stone)
> >    2. Re: SMP and AUFS (Matus UHLAR - fantomas)
> >    3. Re: SMP and AUFS (Alex Rousskov)
> >    4. Re: squid workers question (Alex Rousskov)
> >    5. Re: squid workers question (Matus UHLAR - fantomas)
> >    6. Re: SSL Bump issues (Alex Rousskov)
> >    7. blocking or allowing specific youtube videos (Sohan Wijetunga)

Please edit your reply when responding to a digest email, deleting everything 
not specific to your question.

> > Date: Mon, 20 Mar 2017 16:56:17 +0100
> > From: Antony Stone
> > To: squid-users at lists.squid-cache.org
> > Subject: Re: [squid-users] Squid Transparent/intercept Issues
> > 
> > On Monday 20 March 2017 at 16:26:40, christian brendan wrote:
> > > Hello Everyone,
> > > 
> > > Squid Cache: Version 3.5.20
> > > OS: CentOS 7
> > > 
> > > I have used squid for quite some times non transparently and it works,
> > > problem kicks in when: http_port 3128 transparent is enabled.
> > > Access denied error page shows up when transparent is enabled
> > > ERRORThe requested URL could not be retrieved
> > 
> > How are you getting the packets to the Squid server for interception?
> > 
> > Is the Squid server in the default route between your clients and the
> > Internet, or are you redirecting the packets to the Squid server somehow?
> > 
> > Please give *details* of how you are intercepting and sending the packets
> > to Squid (eg: iptables rules, and which machine/s the rules are running
> > on).
> > 
> > 
> > Antony.

> ?@Antony.Stone
> 1. ?I am using mikrotik routerboard to redirect traffic, with this rule:
> dd action=dst-nat chain=dstnat comment="Redirect port 80 to SquidProxy"
> dst-port=80 protocol=tcp \ src-address=10.24.7.100 to-addresses=10.24.7.101
> to-ports=3128

Okay, so there's your problem, then.

You must not use DSTNAT on a separate router to send packets to Squid for 
intercept.

(This used to work in older versions of Squid, but does not work any more and 
is documented on the wiki, for example at
http://wiki.squid-cache.org/ConfigExamples/Intercept/LinuxDnat )

Note the wording: "NOTE: This configuration is given for use on the squid box."  
That means the NAT rules *must* be running on the Squid box itself and not (in 
your case) on the Mikrotik router.

> 3.? It is not in default route, packets is been redirected.

In that case you need to use policy routing to get the packets *unchanged* to 
the Squid box - see the above link, and also
http://wiki.squid-cache.org/ConfigExamples/Intercept/IptablesPolicyRoute

> ?4. There is no iptable rules, firewall is disabled for this test.

You have to have a REDIRECT rule on the machine running Squid to get it to see 
the packets (once they are no longer being DNATted).

Please try to follow the guidelines at
http://wiki.squid-cache.org/ConfigExamples/Intercept/LinuxDnat and 
http://wiki.squid-cache.org/ConfigExamples/Intercept/IptablesPolicyRoute and 
then come back to us with details of what you've tried, if there are still 
problems.


Regards,


Antony.

-- 
A user interface is like a joke.
If you have to explain it, it didn't work.

                                                   Please reply to the list;
                                                         please *don't* CC me.


From vero.ovando at live.com  Tue Mar 21 12:04:07 2017
From: vero.ovando at live.com (=?utf-8?B?VmVyw7NuaWNhIE92YW5kbw==?=)
Date: Tue, 21 Mar 2017 12:04:07 +0000
Subject: [squid-users] ext_wbinfo_group_acl is not working
In-Reply-To: <4ef48a6a-df57-09b9-2a84-39c7b9ee6c0c@treenet.co.nz>
References: <4ef48a6a-df57-09b9-2a84-39c7b9ee6c0c@treenet.co.nz>
Message-ID: <BY2PR15MB0901BD353F86AE7E20E7F9919E3D0@BY2PR15MB0901.namprd15.prod.outlook.com>

>> Hi, everybody!
>>
>>
>> I have my Squid 3.4.8 running in Debian Jessie. It has been working with Active Directory authentication for more than a year without any kind of problem. But since a couple of weeks ago, suddenly, it stopped authenticate users, asking for credentials (username and pass) and they are not able to browse. I am getting this messages in /var/log/cache.log:
>>
>>
>> 2017/03/04 12:04:25.806 kid1| WARNING: external ACL 'Grupos_AD' queue overload. Request rejected 'user1 it_group'.
>>
>
>This means that your AD is not keeping up with the traffic through your
>proxy.
>Since your Squid has children=100 it will queue up to 200 transactions
>waiting for the helper before this message is shown.
>
>
>Dis cache.log have anything else from the external helper? you have
>debug mode enabled (-d) so it should be reporting if there are any
>issues with AD other than simply slowness.
>
>

Yes. I found this:
14:53:48 [root at server squid3]# tail -f /var/log/squid3/cache.log | grep helper
2017/03/16 14:54:19.527 kid1| Acl.cc(62) AuthenticateAcl: returning 2 sending credentials to helper.
2017/03/16 14:54:19.532 kid1| Acl.cc(62) AuthenticateAcl: returning 2 sending credentials to helper.
2017/03/16 14:54:20.743 kid1| Acl.cc(62) AuthenticateAcl: returning 2 sending credentials to helper.

And this:
2017/03/16 14:53:47.887 kid1| Acl.cc(118) FindByName: ACL::FindByName 'it_group'
2017/03/16 14:53:47.887 kid1| Gadgets.cc(71) aclGetDenyInfoPage: got called for it_group
2017/03/16 14:53:48.028 kid1| Acl.cc(157) matches: checking it_group
2017/03/16 14:53:48.028 kid1| Acl.cc(177) matches: checked: it_group = -1
2017/03/16 14:53:48.028 kid1| Gadgets.cc(103) aclIsProxyAuth: aclIsProxyAuth: called for it_group
2017/03/16 14:53:48.028 kid1| Acl.cc(118) FindByName: ACL::FindByName 'it_group'

>>
>> After some research I found this thread http://www.squid-cache.org/mail-archive/squid-users/200902/0386.html and followed the suggestions posted by Amos. But nothing happened.
>>
>> I tried rejoining the server to domain. Everything was fine in that way: wbinfo -u, wbinfo -g and wbinfo -P correctly returns all the users, groups and information of the domain.
>>
>>
>> After restart Squid service, I noticed that neither helper ext_wbinfo_group_acl nor pinger are started:
>>
>>
>> 12:04:01 [root at server ]# systemctl status squid3.service -l
>
>NOTE: do not trust systemd information about Squid-3. The two are not
>compatible and systemd often says incorrect things because it makes
>incorrect assumptions about the squid process(es). Especially if there
>has been a process crash and auto-restart at any point during Squid
>operation.
>
>
>> ? squid3.service - LSB: Squid HTTP Proxy version 3.x
>>    Loaded: loaded (/etc/init.d/squid3)
>>    Active: active (running) since s?b 2017-03-04 12:04:01 ART; 3s ago
>>   Process: 4537 ExecStop=/etc/init.d/squid3 stop (code=exited, status=0/SUCCESS)
>>   Process: 4560 ExecStart=/etc/init.d/squid3 start (code=exited, status=0/SUCCESS)
>>    CGroup: /system.slice/squid3.service
>>            ??4593 /usr/sbin/squid3 -YC -f /etc/squid3/squid.conf
>>            ??4595 (squid-1) -YC -f /etc/squid3/squid.conf
>>            ??4596 (ntlm_auth) --helper-protocol=squid-2.5-ntlmssp --DOMAIN=MYDOMAIN
>> mar 04 12:04:01 server.mydomain.com squid3[4560]: Starting Squid HTTP Proxy 3.x: squid3
>> 2017/03/04 12:04:01| WARNING: external_acl_type option children=N has been deprecated in favor of children-max=N and children-startup=N
>
>Please note the warning and update your config file.
>
>> mar 04 12:04:01 server.mydomain.com squid3[4593]: Squid Parent: will start 1 kids
>> mar 04 12:04:01 server.mydomain.com squid3[4593]: Squid Parent: (squid-1) process 4595 started
>> mar 04 12:04:01 server.mydomain.com squid3[4560]: .
>>
>>
>> 12:04:30 [root at server ]# ps fax | grep ext_wbinfo_group_acl
>>  1418 pts/0    S+     0:00              \_ grep ext_wbinfo_group_acl
>>
>> If I run echo "mydomain\user1 it_group" | /usr/lib/squid3/ext_wbinfo_group_acl -d, it returns
>>
>> Debugging mode ON.
>> Got mydomain\user1 it_group from squid
>> User:  -mydomain\user1-
>> Group: -it_group-
>> SID:   -S-1-5-21-2290000000-711000000-3300000000-3949-
>> GID:   -10006-
>> Sending OK to squid
>> OK
>>
>> What it's a good, because that user belongs to that group. If I change the group name, it returns an ERR.
>>
>> Here is my squid.conf:
>>
>> #===========================================================================
>> http_port 3128
>> visible_hostname proxy.squid
>> cache_mgr server at proxy.com
>> cache_effective_user proxy
>> error_directory /usr/share/squid3/errors/es
>> err_page_stylesheet /etc/squid3/estilo.css
>>
>> ####################################################
>> #******************************Ports*************************************#
>> ####################################################
>>
>> #acl manager proto cache_object
>> #acl all src 0.0.0.0/0.0.0.0
>> #acl localhost src 127.0.0.1/32
>> acl SSL_ports port 443
>> acl Safe_ports port 80
>> acl Safe_ports port 21
>> acl Safe_ports port 443
>> acl Safe_ports port 70 #prot gopher
>> acl Safe_ports port 210 #whais
>> acl Safe_ports port 280 #http-mgmt
>> acl Safe_ports port 488 #gss-http
>> acl Safe_ports port 591 #filemaker
>> acl Safe_ports port 8080
>> acl Safe_ports port 2481
>> acl Safe_ports port 20010
>> acl Safe_ports port 777 #multi http
>> #acl purge method PURGE
>> acl CONNECT method CONNECT
>>
>> acl_uses_indirect_client on
>> delay_pool_uses_indirect_client on
>> log_uses_indirect_client on
>>
>>
>> ##############################################################
>> #*******************Active Directory HELPERS**************************#
>> ##############################################################
>>
>> auth_param ntlm program /usr/bin/ntlm_auth --helper-protocol=squid-2.5-ntlmssp --DOMAIN=MYDOMAIN
>> auth_param ntlm children 100
>> auth_param ntlm keep_alive off
>>
>> auth_param basic program /usr/bin/ntlm_auth --helper-protocol=squid-2.5-basic
>> auth_param basic children 100
>> auth_param basic realm Servidor proxy-cache
>> auth_param basic credentialsttl 2 hours
>>
>>
>> #######################################################################
>> #****************************ACL******************************************#
>> ###########################################################################
>>
>> #---------------------------ACL Active Directory------------------------#
>> external_acl_type Grupos_AD ttl=10 negative_ttl=10 children=100 %LOGIN /usr/lib/squid3/ext_wbinfo_group_acl -d
>> acl it_group external Grupos_AD it_group
>>
>> ------------------Acceso s?lo a usuarios autenticados--------------------#
>> acl auth proxy_auth REQUIRED
>> http_access deny !auth
>>
>> #-----------------------------Grupo *it_group*----------------------------#
>> http_access allow it_group allow
>
>What is this extra "allow" on the end of the line for?
>
>I dont see any ACL named "allow" in the above config. So that may be
>preventing Squid from restarting, which would confuse systemd.
>
>
This is only a mistake. ACL "allow" does not exist. So it should be:
#-----------------------------Grupo *it_group*----------------------------#
http_access allow it_group
>>
>> http_access allow manager localhost
>> http_access deny manager
>> #http_access allow purge localhost
>> #http_access deny purge
>
>Please move the below two lines up to be the very first http_access
>lines in your config. Part of their purpose is to protect against some
>DoS conditions which can cause exactly this type of overload on headers.
>
I'll move it.
>> http_access deny !Safe_ports
>> http_access deny CONNECT !SSL_PORTS
>>
>> http_access deny all
>>
>> dead_peer_timeout 20 seconds
>> strip_query_terms on
>> debug_options ALL,1 33,2 28,9
>> coredump_dir /var/spool/squid3
>> ftp_passive on
>> ftp_sanitycheck off
>> ftp_telnet_protocol off
>> read_ahead_gap 1 MB
>> positive_dns_ttl 6 hours
>> forward_max_tries 25
>>
>>
>> ############################################################################
>> #*************************Log********************************#
>> ############################################################################
>>
>> logformat squid %ts.%03tu %6tr %>a %Ss/%03>Hs %<st %rm %ru %un %Sh/%<A %mt
>> cache_access_log /var/log/squid3/access.log
>> cache_log /var/log/squid3/cache.log
>> logfile_rotate 0
>>
>> ############################################################################
>> #******************Cache and memory***************************#
>> ############################################################################
>>
>> cache_mem 1024 MB
>> maximum_object_size_in_memory 1024 KB
>> memory_cache_mode always
>> cache_dir aufs /var/spool/squid3 15000 16 256
>> maximum_object_size 96 MB
>> minimum_object_size 10 KB
>> #cache_replacement_policy heap LFUDA
>> cache_replacement_policy heap GDSF
>> memory_replacement_policy heap GDSF
>> #memory_replacement_policy lru
>> cache_store_log none
>> #log_fqdn off
>> log_icp_queries off
>> buffered_logs off
>> #emulate_httpd_log off
>> redirect_rewrites_host_header off
>> cache_swap_low 80
>> cache_swap_high 95
>>
>> #===========================================================================
>>
>> It is really weird, I really don't know how to solve this. I hope my explanation was clear.
>>
>> For testing purposes, I have another Squid working with the same AD server, and it is going fine: the helper and pinger are executed as you can see here:
>>
>> root at debian-test-server:/etc/squid3# systemctl status squid3.service
>> ? squid3.service - LSB: Squid HTTP Proxy version 3.x
>>    Loaded: loaded (/etc/init.d/squid3)
>>    Active: active (running) since lun 2017-02-13 07:35:01 ART; 2 weeks 5 days ago
>>   Process: 570 ExecStart=/etc/init.d/squid3 start (code=exited, status=0/SUCCESS)
>>    CGroup: /system.slice/squid3.service
>>            ?? 1017 /usr/sbin/squid3 -YC -f /etc/squid3/squid.conf
>>            ?? 1020 (squid-1) -YC -f /etc/squid3/squid.conf
>>            ?? 1945 /usr/bin/perl -w /usr/lib/squid3/ext_wbinfo_group_acl -d
>>            ?? 1968 (ntlm_auth) --helper-protocol=squid-2.5-ntlmssp --DOMAIN=MYDOMAIN
>>            ?? 1969 (ntlm_auth) --helper-protocol=squid-2.5-ntlmssp --DOMAIN=MYDOMAIN
>>            ?? 1970 (ntlm_auth) --helper-protocol=squid-2.5-ntlmssp --DOMAIN=MYDOMAIN
>>            ?? 1971 (ntlm_auth) --helper-protocol=squid-2.5-ntlmssp --DOMAIN=MYDOMAIN
>>            ?? 1972 (ntlm_auth) --helper-protocol=squid-2.5-ntlmssp --DOMAIN=MYDOMAIN
>>            ?? 1973 (ntlm_auth) --helper-protocol=squid-2.5-ntlmssp --DOMAIN=MYDOMAIN
>>            ?? 1974 (ntlm_auth) --helper-protocol=squid-2.5-ntlmssp --DOMAIN=MYDOMAIN
>>            ?? 1993 /usr/bin/perl -w /usr/lib/squid3/ext_wbinfo_group_acl -d
>>            ?? 2029 /usr/bin/perl -w /usr/lib/squid3/ext_wbinfo_group_acl -d
>>            ??63477 (pinger)
>>            ??63478 (ntlm_auth) --helper-protocol=squid-2.5-ntlmssp --DOMAIN=MYDOMAIN
>>            ??63479 (ntlm_auth) --helper-protocol=squid-2.5-basic
>>            ??63480 /usr/bin/perl -w /usr/lib/squid3/ext_wbinfo_group_acl -d
>>
>
>As configured your Squid should be starting exactly 100 of each - no
>more, no less. I suspect from both these traces that you dont actually
>need 100 of each helper running, or systemd is confused already.
>
>The current Squid versions can auto-start helpers as needed. See the
>auth_param and external_acl_type documentation for the max=, startup=
>and idle= options. That may help a little, or at least allow you to
>configure higher max limits to cope with slow AD periods.
>
>
>

I tried with those params but nothing happens. The helper doesn't  auto-start.
external_acl_type Grupos_AD ttl=10 children-max=10 children-startup=10 children-idle=10 %LOGIN /usr/lib/squid3/ext_wbinfo_group_acl -d

And as suggested in the Squid wiki http://www.squid-cache.org/Doc/config/auth_param/, I used this values for:
auth_param ntlm program /usr/bin/ntlm_auth --helper-protocol=squid-2.5-ntlmssp --DOMAIN=RENTAS
auth_param ntlm children 20 startup=0 idle=1
auth_param ntlm keep_alive off

auth_param basic program /usr/bin/ntlm_auth --helper-protocol=squid-2.5-basic
auth_param basic children 5 startup=5 idle=1
auth_param basic realm DPR-proxy
auth_param basic credentialsttl 2 hours

>Another possibility is converting to the LDAP group lookup instead of
>using the wbinfo tool to do lookups. I know that LDAP does not suffer
>from wbind connection limits, which might be part of your issue.
>
>Amos
>
Please Amos provide me further guidance. Cannot find a solution to this. Thanks!
--
Ver?nica Ovando
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20170321/041ffc4a/attachment.htm>

From harishmeetsu at yahoo.com  Tue Mar 21 12:30:37 2017
From: harishmeetsu at yahoo.com (S V Hareesh)
Date: Tue, 21 Mar 2017 12:30:37 +0000 (UTC)
Subject: [squid-users] Cache-Peer - Negotiate
References: <1372479245.745829.1490099437343.ref@mail.yahoo.com>
Message-ID: <1372479245.745829.1490099437343@mail.yahoo.com>

Hi,
I am trying to setup Squid as a local HTTP child proxy to a parent/corporate Cisco Ironport WSA proxy. I need help in setting up authentication(Negotiate) to be done automatically from any client who is trying to access internet through the child proxy. So here is what I did.
   
   - Installed Squid on Windows machine with the installable given by Diladele v 3.5.24. Configured the service to run with an account (domain\account1) that has admin rights to that machine.   

   - Got a keytab file for the account and host from our AD Admins. Here is the command run to get the keytab file.   

? ? ? ? ? ? ktpass /princ HTTP/server1.subdomain.domain.com at SUBDOMAIN.DOMAIN.COM/mapuser?domain\account1?/crypto all ?/pass <password_for_account1> /ptypeKRB5_NT_PRINCIPAL /out account.keytab
   
   - Copied that keytab file into etc\squid folder of my Windows installation of Squid.   

   -    

   - Set the following configuration in squid.conf.   


http_port 3128cache_peer <parent_proxy_Ip> parent 80 0 no-query default proxy-only login=NEGOTIATE
http_access allow allnever_direct allow allicp_access deny all
dns_nameservers <DNS_IP1> ?<DNS_IP2> 127.0.0.1
My objective is **any allowed client** irrespective of Unix/Windows/domain/non-domain users should be able to reach to internet. I will set up ACL to specify the IP addresses to use this proxy later. But for now, I am getting a 407 error from any machine trying to use this proxy. I am not sure what is going wrong. Please advise.
I was looking at this link as well.
Squid - Users - Parent proxy with authentication

  
|  
|   
|   
|   |    |

   |

  |
|  
|   |  
Squid - Users - Parent proxy with authentication
 Parent proxy with authentication. Hello, can someone please tell me, what my my cache_peer line must look like, ...  |   |

  |

  |

 

TIA
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20170321/411a1090/attachment.htm>

From harishmeetsu at yahoo.com  Tue Mar 21 12:26:15 2017
From: harishmeetsu at yahoo.com (Hareesh)
Date: Tue, 21 Mar 2017 05:26:15 -0700 (PDT)
Subject: [squid-users] Cache-Peer - Negotiate
Message-ID: <1372479245.745829.1490099437343@mail.yahoo.com>

Hi,
I am trying to setup Squid as a local HTTP child proxy to a parent/corporate Cisco Ironport WSA proxy. I need help in setting up authentication(Negotiate) to be done automatically from any client who is trying to access internet through the child proxy. So here is what I did.
   
   - Installed Squid on Windows machine with the installable given by Diladele v 3.5.24. Configured the service to run with an account (domain\account1) that has admin rights to that machine.   

   - Got a keytab file for the account and host from our AD Admins. Here is the command run to get the keytab file.   

? ? ? ? ? ? ktpass /princ HTTP/server1.subdomain.domain.com at SUBDOMAIN.DOMAIN.COM/mapuser?domain\account1?/crypto all ?/pass <password_for_account1> /ptypeKRB5_NT_PRINCIPAL /out account.keytab
   
   - Copied that keytab file into etc\squid folder of my Windows installation of Squid.   

   -    

   - Set the following configuration in squid.conf.   


http_port 3128cache_peer <parent_proxy_Ip> parent 80 0 no-query default proxy-only login=NEGOTIATE
http_access allow allnever_direct allow allicp_access deny all
dns_nameservers <DNS_IP1> ?<DNS_IP2> 127.0.0.1
My objective is **any allowed client** irrespective of Unix/Windows/domain/non-domain users should be able to reach to internet. I will set up ACL to specify the IP addresses to use this proxy later. But for now, I am getting a 407 error from any machine trying to use this proxy. I am not sure what is going wrong. Please advise.
I was looking at this link as well.
Squid - Users - Parent proxy with authentication

  
|  
|   
|   
|   |    |

   |

  |
|  
|   |  
Squid - Users - Parent proxy with authentication
 Parent proxy with authentication. Hello, can someone please tell me, what my my cache_peer line must look like, ...  |   |

  |

  |

 

TIA




--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/Cache-Peer-Negotiate-tp4681866.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From tarotapprentice at yahoo.com  Tue Mar 21 13:46:35 2017
From: tarotapprentice at yahoo.com (TarotApprentice)
Date: Wed, 22 Mar 2017 00:46:35 +1100
Subject: [squid-users] Load balance on two internet connections
Message-ID: <65A19584-E99E-46BC-894B-80AA08152CB6@yahoo.com>

If I have two internet connections is it possible to spread the traffic between them? I have a single squid 3.5 instance as an explicit proxy under Linux.

MarkJ


From uhlar at fantomas.sk  Tue Mar 21 14:26:10 2017
From: uhlar at fantomas.sk (Matus UHLAR - fantomas)
Date: Tue, 21 Mar 2017 15:26:10 +0100
Subject: [squid-users] Load balance on two internet connections
In-Reply-To: <65A19584-E99E-46BC-894B-80AA08152CB6@yahoo.com>
References: <65A19584-E99E-46BC-894B-80AA08152CB6@yahoo.com>
Message-ID: <20170321142610.GB23732@fantomas.sk>

On 22.03.17 00:46, TarotApprentice wrote:
>If I have two internet connections is it possible to spread the traffic between them? I have a single squid 3.5 instance as an explicit proxy under Linux.

you can use multiple outgoing IP Addresses in squid

http://wiki.squid-cache.org/ConfigExamples/Strange/RotatingIPs

however you need to tell your OS (and/or router) to send different source
IPs through different connections.
-- 
Matus UHLAR - fantomas, uhlar at fantomas.sk ; http://www.fantomas.sk/
Warning: I wish NOT to receive e-mail advertising to this address.
Varovanie: na tuto adresu chcem NEDOSTAVAT akukolvek reklamnu postu.
If Barbie is so popular, why do you have to buy her friends? 


From rousskov at measurement-factory.com  Tue Mar 21 14:51:04 2017
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Tue, 21 Mar 2017 08:51:04 -0600
Subject: [squid-users] Squid blocking own OCSP/AIA requests
In-Reply-To: <c2bbc299-849b-063b-1ed4-6b9517ba987c@wernig.net>
References: <c2bbc299-849b-063b-1ed4-6b9517ba987c@wernig.net>
Message-ID: <08efda5e-fd27-faf9-f341-1a067111a72a@measurement-factory.com>

On 03/21/2017 04:35 AM, Markus Wernig wrote:
> 
> 2017-03-21 10:35:08.338 +0100 000137 - TCP_DENIED/403 3607 GET http://apps.identrust.com/roots/dstrootcax3.p7c - HIER_NONE/- text/html;charset=utf-8 -
> 2017-03-21 10:35:08.345 +0100 000161 10.254.254.2 NONE/200 0 CONNECT letsencrypt.org:443 - HIER_DIRECT/letsencrypt.org - -
> 
> It appears that this is the OCSP URI for Letsencrypt certificates.
> 
> And in fact every time this is logged, a CONNECT to a https uri is
> logged that is using a Letsencrypt certificate (like eg.
> https://letsencrypt.org).
> 
> Given that there is no client IP logged, I assume that squid is blocking
> its own outgoing OCSP request here

You are correct, but I would rephrase that to sound less masochistic:
Your http_access rules block Squid-generated requests, including
certificate download requests.


> The same seems to happen when there's no OCSP URI, but a regular AIA URI
> in the certificate:
> 
> 2017-03-21 10:36:19.773 +0100 000000 - TCP_DENIED/403 3734 GET http://swisssign.net/cgi-bin/authority/download/5B257B96A465517EB839F3C078665EE83AE7F0EE - HIER_NONE/- text/html;charset=utf-8 -
> 2017-03-21 10:36:19.782 +0100 000038 10.254.254.2 NONE/200 0 CONNECT swisssign.net:443 - HIER_DIRECT/swisssign.net - -

I do not remember whether the new certificate downloader feature
supports both OCSP and AIA, but your triage implies that it does. Same
access rules apply to all downloader requests.


> I do have "http_access allow localhost" in squid.conf, but since there's
> no IP associated with the request, this does not seem to help.

Correct. Regular "src" ACLs and their equivalents do not match internal
requests because they have no client [IP addresses].


> Is there a way to allow these outgoing internal requests? I've looked
> through the FAQ and wiki, but couldn't find anything on the topic.

This has been discussed on squid-users, and Factory is working on a
long-term solution. Meanwhile, there is a short-term workaround that may
work for you. Search for generatedBySquid at the following URL but do
read the follow up emails for possible problems you might face:

http://lists.squid-cache.org/pipermail/squid-users/2017-January/014224.html


HTH,

Alex.



From vbvbrj at gmail.com  Tue Mar 21 14:56:39 2017
From: vbvbrj at gmail.com (Mimiko)
Date: Tue, 21 Mar 2017 16:56:39 +0200
Subject: [squid-users] Load balance on two internet connections
In-Reply-To: <65A19584-E99E-46BC-894B-80AA08152CB6@yahoo.com>
References: <65A19584-E99E-46BC-894B-80AA08152CB6@yahoo.com>
Message-ID: <3f92707c-d468-37f8-d3ca-0b4f77eb95a4@gmail.com>

On 21.03.2017 15:46, TarotApprentice wrote:
> If I have two internet connections is it possible to spread the traffic between them? I have a single squid 3.5 instance as an explicit proxy under Linux.

Hello.

I use output load balancing network traffic between 3 ISP using linux load-balancing ip rule. And squid automatically is balanced.
http://serverfault.com/questions/93678/load-balancing-nat-ing-multiple-isp-connections-on-linux



-- 
Mimiko desu.


From jason at tresgeek.net  Tue Mar 21 15:52:46 2017
From: jason at tresgeek.net (Jason B. Nance)
Date: Tue, 21 Mar 2017 10:52:46 -0500 (CDT)
Subject: [squid-users] URL list from a URL
Message-ID: <895324221.1881.1490111566009.JavaMail.zimbra@tresgeek.net>

Hello,

I'm using Squid 3.5.20 and wonder if it is possible to define an ACL which retrieves the list of URLs from another URL (similar to pointing to a file).  In this specific use case it is to allow a Foreman server to sync Yum content from the CentOS mirrors.  I tell Foreman to use the following URL:

http://mirrorlist.centos.org/?release=7&arch=x86_64&repo=updates

Which returns a list of URLs, such as:

http://repo1.dal.innoscale.net/centos/7.3.1611/updates/x86_64/
http://linux.mirrors.es.net/centos/7.3.1611/updates/x86_64/
http://reflector.westga.edu/repos/CentOS/7.3.1611/updates/x86_64/
http://mirror.jax.hugeserver.com/centos/7.3.1611/updates/x86_64/
http://ftp.linux.ncsu.edu/pub/CentOS/7.3.1611/updates/x86_64/
http://mirror.nexcess.net/CentOS/7.3.1611/updates/x86_64/
http://mirror.web-ster.com/centos/7.3.1611/updates/x86_64/
http://centos.host-engine.com/7.3.1611/updates/x86_64/
http://mirror.raystedman.net/centos/7.3.1611/updates/x86_64/
http://mirror.linux.duke.edu/pub/centos/7.3.1611/updates/x86_64/

Foreman then starts a new HTTP connection (not a redirect) to attempt to connect to those in turn until it works.

So I would like to configure Squid to allow the Foreman server access to any of those URLs (the list changes somewhat often).

I started to go down the external_acl_type but am wondering if I'm missing something obvious.

Regards,

j


From bosscb.chrisbren at gmail.com  Tue Mar 21 16:29:36 2017
From: bosscb.chrisbren at gmail.com (christian brendan)
Date: Tue, 21 Mar 2017 17:29:36 +0100
Subject: [squid-users] squid-users Digest, Vol 31, Issue 61
In-Reply-To: <mailman.3.1490097601.3695.squid-users@lists.squid-cache.org>
References: <mailman.3.1490097601.3695.squid-users@lists.squid-cache.org>
Message-ID: <CAHptoxpwTefxeu9MikOmhVqkEUoc48mxNah14j9m-j-vDd1pkA@mail.gmail.com>

Thanks a lot for the information.
I will try this and give feedback.
Best Regards

On Tue, Mar 21, 2017 at 1:00 PM, <squid-users-request at lists.squid-cache.org>
wrote:

> Send squid-users mailing list submissions to
>         squid-users at lists.squid-cache.org
>
> To subscribe or unsubscribe via the World Wide Web, visit
>         http://lists.squid-cache.org/listinfo/squid-users
> or, via email, send a message with subject or body 'help' to
>         squid-users-request at lists.squid-cache.org
>
> You can reach the person managing the list at
>         squid-users-owner at lists.squid-cache.org
>
> When replying, please edit your Subject line so it is more specific
> than "Re: Contents of squid-users digest..."
>
>
> Today's Topics:
>
>    1. Re: Squid Transparent/intercept Issues (Antony Stone)
>
>
> ----------------------------------------------------------------------
>
> Message: 1
> Date: Tue, 21 Mar 2017 12:12:01 +0100
> From: Antony Stone <Antony.Stone at squid.open.source.it>
> To: squid-users at lists.squid-cache.org
> Subject: Re: [squid-users] Squid Transparent/intercept Issues
> Message-ID: <201703211212.01346.Antony.Stone at squid.open.source.it>
> Content-Type: Text/Plain;  charset="utf-8"
>
> On Tuesday 21 March 2017 at 12:00:05, christian brendan wrote:
>
> > > Today's Topics:
> > >    1. Re: Squid Transparent/intercept Issues (Antony Stone)
> > >    2. Re: SMP and AUFS (Matus UHLAR - fantomas)
> > >    3. Re: SMP and AUFS (Alex Rousskov)
> > >    4. Re: squid workers question (Alex Rousskov)
> > >    5. Re: squid workers question (Matus UHLAR - fantomas)
> > >    6. Re: SSL Bump issues (Alex Rousskov)
> > >    7. blocking or allowing specific youtube videos (Sohan Wijetunga)
>
> Please edit your reply when responding to a digest email, deleting
> everything
> not specific to your question.
>
> > > Date: Mon, 20 Mar 2017 16:56:17 +0100
> > > From: Antony Stone
> > > To: squid-users at lists.squid-cache.org
> > > Subject: Re: [squid-users] Squid Transparent/intercept Issues
> > >
> > > On Monday 20 March 2017 at 16:26:40, christian brendan wrote:
> > > > Hello Everyone,
> > > >
> > > > Squid Cache: Version 3.5.20
> > > > OS: CentOS 7
> > > >
> > > > I have used squid for quite some times non transparently and it
> works,
> > > > problem kicks in when: http_port 3128 transparent is enabled.
> > > > Access denied error page shows up when transparent is enabled
> > > > ERRORThe requested URL could not be retrieved
> > >
> > > How are you getting the packets to the Squid server for interception?
> > >
> > > Is the Squid server in the default route between your clients and the
> > > Internet, or are you redirecting the packets to the Squid server
> somehow?
> > >
> > > Please give *details* of how you are intercepting and sending the
> packets
> > > to Squid (eg: iptables rules, and which machine/s the rules are running
> > > on).
> > >
> > >
> > > Antony.
>
> > ?@Antony.Stone
> > 1. ?I am using mikrotik routerboard to redirect traffic, with this rule:
> > dd action=dst-nat chain=dstnat comment="Redirect port 80 to SquidProxy"
> > dst-port=80 protocol=tcp \ src-address=10.24.7.100
> to-addresses=10.24.7.101
> > to-ports=3128
>
> Okay, so there's your problem, then.
>
> You must not use DSTNAT on a separate router to send packets to Squid for
> intercept.
>
> (This used to work in older versions of Squid, but does not work any more
> and
> is documented on the wiki, for example at
> http://wiki.squid-cache.org/ConfigExamples/Intercept/LinuxDnat )
>
> Note the wording: "NOTE: This configuration is given for use on the squid
> box."
> That means the NAT rules *must* be running on the Squid box itself and not
> (in
> your case) on the Mikrotik router.
>
> > 3.? It is not in default route, packets is been redirected.
>
> In that case you need to use policy routing to get the packets *unchanged*
> to
> the Squid box - see the above link, and also
> http://wiki.squid-cache.org/ConfigExamples/Intercept/IptablesPolicyRoute
>
> > ?4. There is no iptable rules, firewall is disabled for this test.
>
> You have to have a REDIRECT rule on the machine running Squid to get it to
> see
> the packets (once they are no longer being DNATted).
>
> Please try to follow the guidelines at
> http://wiki.squid-cache.org/ConfigExamples/Intercept/LinuxDnat and
> http://wiki.squid-cache.org/ConfigExamples/Intercept/IptablesPolicyRoute
> and
> then come back to us with details of what you've tried, if there are still
> problems.
>
>
> Regards,
>
>
> Antony.
>
> --
> A user interface is like a joke.
> If you have to explain it, it didn't work.
>
>                                                    Please reply to the
> list;
>                                                          please *don't* CC
> me.
>
>
> ------------------------------
>
> Subject: Digest Footer
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>
>
> ------------------------------
>
> End of squid-users Digest, Vol 31, Issue 61
> *******************************************
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20170321/4bdb3abf/attachment.htm>

From Antony.Stone at squid.open.source.it  Tue Mar 21 16:36:57 2017
From: Antony.Stone at squid.open.source.it (Antony Stone)
Date: Tue, 21 Mar 2017 17:36:57 +0100
Subject: [squid-users] squid-users Digest, Vol 31, Issue 61
In-Reply-To: <CAHptoxpwTefxeu9MikOmhVqkEUoc48mxNah14j9m-j-vDd1pkA@mail.gmail.com>
References: <mailman.3.1490097601.3695.squid-users@lists.squid-cache.org>
 <CAHptoxpwTefxeu9MikOmhVqkEUoc48mxNah14j9m-j-vDd1pkA@mail.gmail.com>
Message-ID: <201703211736.58205.Antony.Stone@squid.open.source.it>

On Tuesday 21 March 2017 at 17:29:36, christian brendan wrote:

> Thanks a lot for the information.
> I will try this and give feedback.
> Best Regards

Please note both of the following for when you post your feedback:

1. The request inserted into the email you replied to by the mailing list 
system:

	When replying, please edit your Subject line so it is more specific than
	"Re: Contents of squid-users digest..."

2. The request I made in my reply to you:


	Please edit your reply when responding to a digest email, deleting
	everything not specific to your question.


Thanks,


Antony.

-- 
"Measuring average network latency is about as useful as measuring the mean 
temperature of patients in a hospital."

 - St?phane Bortzmeyer

                                                   Please reply to the list;
                                                         please *don't* CC me.


From yvoinov at gmail.com  Tue Mar 21 18:19:43 2017
From: yvoinov at gmail.com (Yuri Voinov)
Date: Wed, 22 Mar 2017 00:19:43 +0600
Subject: [squid-users] URL list from a URL
In-Reply-To: <895324221.1881.1490111566009.JavaMail.zimbra@tresgeek.net>
References: <895324221.1881.1490111566009.JavaMail.zimbra@tresgeek.net>
Message-ID: <1f441f69-102e-7855-4ac9-758227a76682@gmail.com>

Yes.

Functionality you required is:

http://wiki.squid-cache.org/Features/StoreID


21.03.2017 21:52, Jason B. Nance ?????:
> Hello,
>
> I'm using Squid 3.5.20 and wonder if it is possible to define an ACL which retrieves the list of URLs from another URL (similar to pointing to a file).  In this specific use case it is to allow a Foreman server to sync Yum content from the CentOS mirrors.  I tell Foreman to use the following URL:
>
> http://mirrorlist.centos.org/?release=7&arch=x86_64&repo=updates
>
> Which returns a list of URLs, such as:
>
> http://repo1.dal.innoscale.net/centos/7.3.1611/updates/x86_64/
> http://linux.mirrors.es.net/centos/7.3.1611/updates/x86_64/
> http://reflector.westga.edu/repos/CentOS/7.3.1611/updates/x86_64/
> http://mirror.jax.hugeserver.com/centos/7.3.1611/updates/x86_64/
> http://ftp.linux.ncsu.edu/pub/CentOS/7.3.1611/updates/x86_64/
> http://mirror.nexcess.net/CentOS/7.3.1611/updates/x86_64/
> http://mirror.web-ster.com/centos/7.3.1611/updates/x86_64/
> http://centos.host-engine.com/7.3.1611/updates/x86_64/
> http://mirror.raystedman.net/centos/7.3.1611/updates/x86_64/
> http://mirror.linux.duke.edu/pub/centos/7.3.1611/updates/x86_64/
>
> Foreman then starts a new HTTP connection (not a redirect) to attempt to connect to those in turn until it works.
>
> So I would like to configure Squid to allow the Foreman server access to any of those URLs (the list changes somewhat often).
>
> I started to go down the external_acl_type but am wondering if I'm missing something obvious.
>
> Regards,
>
> j
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users

-- 
Bugs to the Future
-------------- next part --------------
A non-text attachment was scrubbed...
Name: 0x613DEC46.asc
Type: application/pgp-keys
Size: 2437 bytes
Desc: not available
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20170322/a2a17c66/attachment.key>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: signature.asc
Type: application/pgp-signature
Size: 473 bytes
Desc: OpenPGP digital signature
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20170322/a2a17c66/attachment.sig>

From cwaldon at otn.ca  Tue Mar 21 19:04:23 2017
From: cwaldon at otn.ca (Waldon, Cooper)
Date: Tue, 21 Mar 2017 19:04:23 +0000
Subject: [squid-users] Assistance with WCCPv2 Setup with Cisco Router
Message-ID: <YTOPR01MB04766B2AE6D78BBE5022DD0CDF3D0@YTOPR01MB0476.CANPRD01.PROD.OUTLOOK.COM>

Hello All,

I'm trying to set up a transparent proxy for http and https using Cisco Routers and Squid.  I have followed the configuration examples that are listed under the wccp2 overview section (http://wiki.squid-cache.org/Features/Wccp2) of the squid wiki but I'm still having some issues.

I have a little lab set up with a Cisco 7200 Router and a VM with CentOS running the proxy.

The "WAN" IP of the Router is 192.168.0.23.  The IP of the Squid Proxy is 192.168.0.24 and both have the default gateway of 192.168.0.1 which is the "ISP"

The Client is sitting on a LAN behind the Router in the 10.10.10.0/24 subnet and is also sitting behind nat.

I believe that the router and proxy are communicating properly based on the information in the show ip wccp command on the router as it shows clients and routers as well as showing that packets are being forwarded:

R3#show ip wccp
Global WCCP information:
    Router information:
        Router Identifier:                   192.168.0.23
        Configured source-interface:         GigabitEthernet5/0

    Service Identifier: web-cache
        Protocol Version:                    2.00
        Number of Service Group Clients:     1
        Number of Service Group Routers:     1
        Total Packets Redirected:            1079
          Process:                           0
          CEF:                               1079
        Service mode:                        Open
        Service Access-list:                 -none-
        Total Packets Dropped Closed:        0
        Redirect access-list:                100
        Total Packets Denied Redirect:       0
        Total Packets Unassigned:            0
        Group access-list:                   10
        Total Messages Denied to Group:      0
        Total Authentication failures:       0
        Total GRE Bypassed Packets Received: 0
          Process:                           0
          CEF:                               0
       GRE tunnel interface:                Tunnel1

    Service Identifier: 70
        Protocol Version:                    2.00
        Number of Service Group Clients:     1
        Number of Service Group Routers:     1
        Total Packets Redirected:            500
          Process:                           0
          CEF:                               500
        Service mode:                        Open
        Service Access-list:                 -none-
        Total Packets Dropped Closed:        0
        Redirect access-list:                100
        Total Packets Denied Redirect:       0
        Total Packets Unassigned:            0
        Group access-list:                   10
        Total Messages Denied to Group:      0
        Total Authentication failures:       0
        Total GRE Bypassed Packets Received: 0
          Process:                           0
          CEF:                               0
        GRE tunnel interface:                Tunnel0

Here is the relevant squid wccp configuration:

----Output removed----
# Squid normally listens to port 3128
http_port 3128
http_port 0.0.0.0:3129

# WCCPv2 Parameters
wccp2_router 192.168.0.23
wccp2_forwarding_method 1
wccp2_return_method 1
wccp2_assignment_method hash
wccp2_service standard 0
wccp2_service dynamic 70
wccp2_service_info 70 protocol=tcp flags=dst_ip_hash,src_ip_alt_hash,src_port_alt_hash priority=231 ports=443

---Output remove----

I think that the issue lies with the iptables configuration as I do not see any packets been processed in the nat table.  I have tried a few different methods such as:

iptables -t nat -A PREROUTING -i wccp0 -p tcp -dport 80 -j REDIRECT -to-port 3129
iptables -t nat -A PREROUTING -i wccp0 -p tcp -dport 443 -j REDIRECT -to-port 3129
iptables -t nat -A POSTROUTING -j MASQUERADE

or

iptables -t nat -A PREROUTING -p tcp -dport 80 -j DNAT -to-destination 192.168.0.24:3129
iptables -t nat -A PREROUTING -p tcp -dport 443 -j DNAT -to-destination 192.168.0.24:3129
iptables -t nat -A POSTROUTING -j MASQUERADE

I have also tried adding ACCEPT commands to the PREROUTING zone just in case the proxy is dropping the packets right away but that also doesn't work.

The proxy functions perfectly when the client is configured to use a proxy so there doesn't appear to be any issues with routing or anything like that, it's just the transparent proxying that isn't working.

If anyone has any suggestions of what I could try that would be greatly appreciated.  Let me know if anything is unclear or if you need further clarification.

Thank you,
Cooper Waldon


Cooper Waldon l Network Engineer l OTN l 416-446-4110 x 4473 l www.otn.ca<http://www.otn.ca/> | Service Desk 1-855-654-0888 x2

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20170321/d8f6bbb7/attachment.htm>

From yvoinov at gmail.com  Tue Mar 21 19:13:14 2017
From: yvoinov at gmail.com (Yuri Voinov)
Date: Wed, 22 Mar 2017 01:13:14 +0600
Subject: [squid-users] Assistance with WCCPv2 Setup with Cisco Router
In-Reply-To: <YTOPR01MB04766B2AE6D78BBE5022DD0CDF3D0@YTOPR01MB0476.CANPRD01.PROD.OUTLOOK.COM>
References: <YTOPR01MB04766B2AE6D78BBE5022DD0CDF3D0@YTOPR01MB0476.CANPRD01.PROD.OUTLOOK.COM>
Message-ID: <6742afc7-5262-2f33-7a3e-e43075648bfc@gmail.com>



22.03.2017 1:04, Waldon, Cooper ?????:
>
> Hello All,
>
>  
>
> I?m trying to set up a transparent proxy for http and https using
> Cisco Routers and Squid.  I have followed the configuration examples
> that are listed under the wccp2 overview section
> (http://wiki.squid-cache.org/Features/Wccp2) of the squid wiki but I?m
> still having some issues.
>
>  
>
> I have a little lab set up with a Cisco 7200 Router and a VM with
> CentOS running the proxy.
>
>  
>
> The ?WAN? IP of the Router is 192.168.0.23.  The IP of the Squid Proxy
> is 192.168.0.24 and both have the default gateway of 192.168.0.1 which
> is the ?ISP?
>
>  
>
> The Client is sitting on a LAN behind the Router in the 10.10.10.0/24
> subnet and is also sitting behind nat.
>
>  
>
> I believe that the router and proxy are communicating properly based
> on the information in the show ip wccp command on the router as it
> shows clients and routers as well as showing that packets are being
> forwarded:
>
>  
>
> R3#show ip wccp
>
> Global WCCP information:
>
>     Router information:
>
>         Router Identifier:                   192.168.0.23
>
>         Configured source-interface:         GigabitEthernet5/0
>
>  
>
>     Service Identifier: web-cache
>
>         Protocol Version:                    2.00
>
>         Number of Service Group Clients:     1
>
>         Number of Service Group Routers:     1
>
>         Total Packets Redirected:            1079
>
>           Process:                           0
>
>           CEF:                               1079
>
>         Service mode:                        Open
>
>         Service Access-list:                 -none-
>
>         Total Packets Dropped Closed:        0
>
>         Redirect access-list:                100
>
>         Total Packets Denied Redirect:       0
>
>         Total Packets Unassigned:            0
>
>         Group access-list:                   10
>
>         Total Messages Denied to Group:      0
>
>         Total Authentication failures:       0
>
>         Total GRE Bypassed Packets Received: 0
>
>           Process:                           0
>
>           CEF:                               0
>
>        GRE tunnel interface:                Tunnel1
>
>  
>
>     Service Identifier: 70
>
>         Protocol Version:                    2.00
>
>         Number of Service Group Clients:     1
>
>         Number of Service Group Routers:     1
>
>         Total Packets Redirected:            500
>
>           Process:                           0
>
>           CEF:                               500
>
>         Service mode:                        Open
>
>         Service Access-list:                 -none-
>
>         Total Packets Dropped Closed:        0
>
>         Redirect access-list:                100
>
>         Total Packets Denied Redirect:       0
>
>         Total Packets Unassigned:            0
>
>         Group access-list:                   10
>
>         Total Messages Denied to Group:      0
>
>         Total Authentication failures:       0
>
>         Total GRE Bypassed Packets Received: 0
>
>           Process:                           0
>
>           CEF:                               0
>
>         GRE tunnel interface:                Tunnel0
>
>  
>
> Here is the relevant squid wccp configuration:
>
>  
>
> ----Output removed----
>
> # Squid normally listens to port 3128
>
> http_port 3128
>
> http_port 0.0.0.0:3129
>
>  
>
> # WCCPv2 Parameters
>
> wccp2_router 192.168.0.23
>
> wccp2_forwarding_method 1
>
> wccp2_return_method 1
>
> wccp2_assignment_method hash
>
> wccp2_service standard 0
>
> wccp2_service dynamic 70
>
> wccp2_service_info 70 protocol=tcp
> flags=dst_ip_hash,src_ip_alt_hash,src_port_alt_hash priority=231 ports=443
>
>  
>
> ---Output remove----
>
>  
>
> I think that the issue lies with the iptables configuration as I do
> not see any packets been processed in the nat table.  I have tried a
> few different methods such as:
>
>  
>
> iptables -t nat -A PREROUTING -i wccp0 -p tcp ?dport 80 -j REDIRECT
> ?to-port 3129
>
> iptables -t nat -A PREROUTING -i wccp0 -p tcp ?dport 443 -j REDIRECT
> ?to-port 3129
>
> iptables -t nat -A POSTROUTING -j MASQUERADE
>
>  
>
> or
>
>  
>
> iptables -t nat -A PREROUTING -p tcp ?dport 80 -j DNAT ?to-destination
> 192.168.0.24:3129
>
> iptables -t nat -A PREROUTING -p tcp ?dport 443 -j DNAT
> ?to-destination 192.168.0.24:3129
>
> iptables -t nat -A POSTROUTING -j MASQUERADE
>
>  
>
> I have also tried adding ACCEPT commands to the PREROUTING zone just
> in case the proxy is dropping the packets right away but that also
> doesn?t work.
>
1.Ports, you using for redirection, in squid, should be defined as
'intercept':

http_port 3126 intercept

https_port 3127 intercept ssl-bump generate-host-certificates=on
dynamic_cert_mem_cache_size=4MB cert=/usr/local/squid/etc/rootCA2.crt
key=/usr/local/squid/etc/rootCA2.key
tls-cafile=/usr/local/squid/etc/rootCA12.crt
options=SINGLE_DH_USE:SINGLE_ECDH_USE
tls-dh=secp384r1:/usr/local/squid/etc/dhparam.pem
cipher=HIGH:MEDIUM:RC4:3DES:!aNULL:!eNULL:!LOW:!MD5:!EXP:!PSK:!SRP:!DSS
tls-no-npn sslflags=VERIFY_CRL_ALL

http_port 3128 ssl-bump generate-host-certificates=on
dynamic_cert_mem_cache_size=4MB cert=/usr/local/squid/etc/rootCA2.crt
key=/usr/local/squid/etc/rootCA2.key
tls-cafile=/usr/local/squid/etc/rootCA12.crt
options=SINGLE_DH_USE:SINGLE_ECDH_USE
tls-dh=secp384r1:/usr/local/squid/etc/dhparam.pem
cipher=HIGH:MEDIUM:RC4:3DES:!aNULL:!eNULL:!LOW:!MD5:!EXP:!PSK:!SRP:!DSS
tls-no-npn sslflags=VERIFY_CRL_ALL

(example from my config, DON'T copy-n-paste!)

2. HTTP and HTTPS ports should be different.
3. HTTPS port should be configured correctly.

http://wiki.squid-cache.org/ConfigExamples/Intercept/CiscoIOSv15Wccp2

Read carefully - here is explained almost all.

>  
>
> The proxy functions perfectly when the client is configured to use a
> proxy so there doesn?t appear to be any issues with routing or
> anything like that, it?s just the transparent proxying that isn?t working.
>
>  
>
> If anyone has any suggestions of what I could try that would be
> greatly appreciated.  Let me know if anything is unclear or if you
> need further clarification.
>
>  
>
> Thank you,
>
> Cooper Waldon
>
>  
>
>  
>
> *Cooper Waldon** **l **Network
> Engineer** **l****OTN****l****416-446-4110 x 4473 **l** **www.otn.ca*
> <http://www.otn.ca/>***|****Service Desk 1-855-654-0888 x2*
>
>  
>
>
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users

-- 
Bugs to the Future
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20170322/904df808/attachment.htm>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: 0x613DEC46.asc
Type: application/pgp-keys
Size: 2437 bytes
Desc: not available
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20170322/904df808/attachment.key>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: signature.asc
Type: application/pgp-signature
Size: 473 bytes
Desc: OpenPGP digital signature
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20170322/904df808/attachment.sig>

From yvoinov at gmail.com  Tue Mar 21 19:14:19 2017
From: yvoinov at gmail.com (Yuri Voinov)
Date: Wed, 22 Mar 2017 01:14:19 +0600
Subject: [squid-users] Assistance with WCCPv2 Setup with Cisco Router
In-Reply-To: <YTOPR01MB04766B2AE6D78BBE5022DD0CDF3D0@YTOPR01MB0476.CANPRD01.PROD.OUTLOOK.COM>
References: <YTOPR01MB04766B2AE6D78BBE5022DD0CDF3D0@YTOPR01MB0476.CANPRD01.PROD.OUTLOOK.COM>
Message-ID: <d33498f4-3dfc-4fe2-2a35-3a64f4a08d24@gmail.com>

Ah, forgot about this:

http://wiki.squid-cache.org/ConfigExamples/Intercept


22.03.2017 1:04, Waldon, Cooper ?????:
>
> Hello All,
>
>  
>
> I?m trying to set up a transparent proxy for http and https using
> Cisco Routers and Squid.  I have followed the configuration examples
> that are listed under the wccp2 overview section
> (http://wiki.squid-cache.org/Features/Wccp2) of the squid wiki but I?m
> still having some issues.
>
>  
>
> I have a little lab set up with a Cisco 7200 Router and a VM with
> CentOS running the proxy.
>
>  
>
> The ?WAN? IP of the Router is 192.168.0.23.  The IP of the Squid Proxy
> is 192.168.0.24 and both have the default gateway of 192.168.0.1 which
> is the ?ISP?
>
>  
>
> The Client is sitting on a LAN behind the Router in the 10.10.10.0/24
> subnet and is also sitting behind nat.
>
>  
>
> I believe that the router and proxy are communicating properly based
> on the information in the show ip wccp command on the router as it
> shows clients and routers as well as showing that packets are being
> forwarded:
>
>  
>
> R3#show ip wccp
>
> Global WCCP information:
>
>     Router information:
>
>         Router Identifier:                   192.168.0.23
>
>         Configured source-interface:         GigabitEthernet5/0
>
>  
>
>     Service Identifier: web-cache
>
>         Protocol Version:                    2.00
>
>         Number of Service Group Clients:     1
>
>         Number of Service Group Routers:     1
>
>         Total Packets Redirected:            1079
>
>           Process:                           0
>
>           CEF:                               1079
>
>         Service mode:                        Open
>
>         Service Access-list:                 -none-
>
>         Total Packets Dropped Closed:        0
>
>         Redirect access-list:                100
>
>         Total Packets Denied Redirect:       0
>
>         Total Packets Unassigned:            0
>
>         Group access-list:                   10
>
>         Total Messages Denied to Group:      0
>
>         Total Authentication failures:       0
>
>         Total GRE Bypassed Packets Received: 0
>
>           Process:                           0
>
>           CEF:                               0
>
>        GRE tunnel interface:                Tunnel1
>
>  
>
>     Service Identifier: 70
>
>         Protocol Version:                    2.00
>
>         Number of Service Group Clients:     1
>
>         Number of Service Group Routers:     1
>
>         Total Packets Redirected:            500
>
>           Process:                           0
>
>           CEF:                               500
>
>         Service mode:                        Open
>
>         Service Access-list:                 -none-
>
>         Total Packets Dropped Closed:        0
>
>         Redirect access-list:                100
>
>         Total Packets Denied Redirect:       0
>
>         Total Packets Unassigned:            0
>
>         Group access-list:                   10
>
>         Total Messages Denied to Group:      0
>
>         Total Authentication failures:       0
>
>         Total GRE Bypassed Packets Received: 0
>
>           Process:                           0
>
>           CEF:                               0
>
>         GRE tunnel interface:                Tunnel0
>
>  
>
> Here is the relevant squid wccp configuration:
>
>  
>
> ----Output removed----
>
> # Squid normally listens to port 3128
>
> http_port 3128
>
> http_port 0.0.0.0:3129
>
>  
>
> # WCCPv2 Parameters
>
> wccp2_router 192.168.0.23
>
> wccp2_forwarding_method 1
>
> wccp2_return_method 1
>
> wccp2_assignment_method hash
>
> wccp2_service standard 0
>
> wccp2_service dynamic 70
>
> wccp2_service_info 70 protocol=tcp
> flags=dst_ip_hash,src_ip_alt_hash,src_port_alt_hash priority=231 ports=443
>
>  
>
> ---Output remove----
>
>  
>
> I think that the issue lies with the iptables configuration as I do
> not see any packets been processed in the nat table.  I have tried a
> few different methods such as:
>
>  
>
> iptables -t nat -A PREROUTING -i wccp0 -p tcp ?dport 80 -j REDIRECT
> ?to-port 3129
>
> iptables -t nat -A PREROUTING -i wccp0 -p tcp ?dport 443 -j REDIRECT
> ?to-port 3129
>
> iptables -t nat -A POSTROUTING -j MASQUERADE
>
>  
>
> or
>
>  
>
> iptables -t nat -A PREROUTING -p tcp ?dport 80 -j DNAT ?to-destination
> 192.168.0.24:3129
>
> iptables -t nat -A PREROUTING -p tcp ?dport 443 -j DNAT
> ?to-destination 192.168.0.24:3129
>
> iptables -t nat -A POSTROUTING -j MASQUERADE
>
>  
>
> I have also tried adding ACCEPT commands to the PREROUTING zone just
> in case the proxy is dropping the packets right away but that also
> doesn?t work.
>
>  
>
> The proxy functions perfectly when the client is configured to use a
> proxy so there doesn?t appear to be any issues with routing or
> anything like that, it?s just the transparent proxying that isn?t working.
>
>  
>
> If anyone has any suggestions of what I could try that would be
> greatly appreciated.  Let me know if anything is unclear or if you
> need further clarification.
>
>  
>
> Thank you,
>
> Cooper Waldon
>
>  
>
>  
>
> *Cooper Waldon** **l **Network
> Engineer** **l****OTN****l****416-446-4110 x 4473 **l** **www.otn.ca*
> <http://www.otn.ca/>***|****Service Desk 1-855-654-0888 x2*
>
>  
>
>
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users

-- 
Bugs to the Future
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20170322/19763217/attachment.htm>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: 0x613DEC46.asc
Type: application/pgp-keys
Size: 2437 bytes
Desc: not available
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20170322/19763217/attachment.key>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: signature.asc
Type: application/pgp-signature
Size: 473 bytes
Desc: OpenPGP digital signature
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20170322/19763217/attachment.sig>

From yvoinov at gmail.com  Tue Mar 21 19:17:04 2017
From: yvoinov at gmail.com (Yuri Voinov)
Date: Wed, 22 Mar 2017 01:17:04 +0600
Subject: [squid-users] Assistance with WCCPv2 Setup with Cisco Router
In-Reply-To: <YTOPR01MB04766B2AE6D78BBE5022DD0CDF3D0@YTOPR01MB0476.CANPRD01.PROD.OUTLOOK.COM>
References: <YTOPR01MB04766B2AE6D78BBE5022DD0CDF3D0@YTOPR01MB0476.CANPRD01.PROD.OUTLOOK.COM>
Message-ID: <f0b72dd7-0071-4a07-9df7-7bb38c0eefd6@gmail.com>

PS. You configured GRE tunnel, as I can see. Check it defined on both
sides: on router and on your proxy box. Also note, GRE will process on
router CPU, instead of L2 redirection, which is runs on control plane
and hardware accelerated.


22.03.2017 1:04, Waldon, Cooper ?????:
>
> Hello All,
>
>  
>
> I?m trying to set up a transparent proxy for http and https using
> Cisco Routers and Squid.  I have followed the configuration examples
> that are listed under the wccp2 overview section
> (http://wiki.squid-cache.org/Features/Wccp2) of the squid wiki but I?m
> still having some issues.
>
>  
>
> I have a little lab set up with a Cisco 7200 Router and a VM with
> CentOS running the proxy.
>
>  
>
> The ?WAN? IP of the Router is 192.168.0.23.  The IP of the Squid Proxy
> is 192.168.0.24 and both have the default gateway of 192.168.0.1 which
> is the ?ISP?
>
>  
>
> The Client is sitting on a LAN behind the Router in the 10.10.10.0/24
> subnet and is also sitting behind nat.
>
>  
>
> I believe that the router and proxy are communicating properly based
> on the information in the show ip wccp command on the router as it
> shows clients and routers as well as showing that packets are being
> forwarded:
>
>  
>
> R3#show ip wccp
>
> Global WCCP information:
>
>     Router information:
>
>         Router Identifier:                   192.168.0.23
>
>         Configured source-interface:         GigabitEthernet5/0
>
>  
>
>     Service Identifier: web-cache
>
>         Protocol Version:                    2.00
>
>         Number of Service Group Clients:     1
>
>         Number of Service Group Routers:     1
>
>         Total Packets Redirected:            1079
>
>           Process:                           0
>
>           CEF:                               1079
>
>         Service mode:                        Open
>
>         Service Access-list:                 -none-
>
>         Total Packets Dropped Closed:        0
>
>         Redirect access-list:                100
>
>         Total Packets Denied Redirect:       0
>
>         Total Packets Unassigned:            0
>
>         Group access-list:                   10
>
>         Total Messages Denied to Group:      0
>
>         Total Authentication failures:       0
>
>         Total GRE Bypassed Packets Received: 0
>
>           Process:                           0
>
>           CEF:                               0
>
>        GRE tunnel interface:                Tunnel1
>
>  
>
>     Service Identifier: 70
>
>         Protocol Version:                    2.00
>
>         Number of Service Group Clients:     1
>
>         Number of Service Group Routers:     1
>
>         Total Packets Redirected:            500
>
>           Process:                           0
>
>           CEF:                               500
>
>         Service mode:                        Open
>
>         Service Access-list:                 -none-
>
>         Total Packets Dropped Closed:        0
>
>         Redirect access-list:                100
>
>         Total Packets Denied Redirect:       0
>
>         Total Packets Unassigned:            0
>
>         Group access-list:                   10
>
>         Total Messages Denied to Group:      0
>
>         Total Authentication failures:       0
>
>         Total GRE Bypassed Packets Received: 0
>
>           Process:                           0
>
>           CEF:                               0
>
>         GRE tunnel interface:                Tunnel0
>
>  
>
> Here is the relevant squid wccp configuration:
>
>  
>
> ----Output removed----
>
> # Squid normally listens to port 3128
>
> http_port 3128
>
> http_port 0.0.0.0:3129
>
>  
>
> # WCCPv2 Parameters
>
> wccp2_router 192.168.0.23
>
> wccp2_forwarding_method 1
>
> wccp2_return_method 1
>
> wccp2_assignment_method hash
>
> wccp2_service standard 0
>
> wccp2_service dynamic 70
>
> wccp2_service_info 70 protocol=tcp
> flags=dst_ip_hash,src_ip_alt_hash,src_port_alt_hash priority=231 ports=443
>
>  
>
> ---Output remove----
>
>  
>
> I think that the issue lies with the iptables configuration as I do
> not see any packets been processed in the nat table.  I have tried a
> few different methods such as:
>
>  
>
> iptables -t nat -A PREROUTING -i wccp0 -p tcp ?dport 80 -j REDIRECT
> ?to-port 3129
>
> iptables -t nat -A PREROUTING -i wccp0 -p tcp ?dport 443 -j REDIRECT
> ?to-port 3129
>
> iptables -t nat -A POSTROUTING -j MASQUERADE
>
>  
>
> or
>
>  
>
> iptables -t nat -A PREROUTING -p tcp ?dport 80 -j DNAT ?to-destination
> 192.168.0.24:3129
>
> iptables -t nat -A PREROUTING -p tcp ?dport 443 -j DNAT
> ?to-destination 192.168.0.24:3129
>
> iptables -t nat -A POSTROUTING -j MASQUERADE
>
>  
>
> I have also tried adding ACCEPT commands to the PREROUTING zone just
> in case the proxy is dropping the packets right away but that also
> doesn?t work.
>
>  
>
> The proxy functions perfectly when the client is configured to use a
> proxy so there doesn?t appear to be any issues with routing or
> anything like that, it?s just the transparent proxying that isn?t working.
>
>  
>
> If anyone has any suggestions of what I could try that would be
> greatly appreciated.  Let me know if anything is unclear or if you
> need further clarification.
>
>  
>
> Thank you,
>
> Cooper Waldon
>
>  
>
>  
>
> *Cooper Waldon** **l **Network
> Engineer** **l****OTN****l****416-446-4110 x 4473 **l** **www.otn.ca*
> <http://www.otn.ca/>***|****Service Desk 1-855-654-0888 x2*
>
>  
>
>
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users

-- 
Bugs to the Future
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20170322/5bb44caa/attachment.htm>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: 0x613DEC46.asc
Type: application/pgp-keys
Size: 2437 bytes
Desc: not available
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20170322/5bb44caa/attachment.key>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: signature.asc
Type: application/pgp-signature
Size: 473 bytes
Desc: OpenPGP digital signature
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20170322/5bb44caa/attachment.sig>

From jason at tresgeek.net  Tue Mar 21 20:30:16 2017
From: jason at tresgeek.net (Jason B. Nance)
Date: Tue, 21 Mar 2017 15:30:16 -0500 (CDT)
Subject: [squid-users] URL list from a URL
In-Reply-To: <1f441f69-102e-7855-4ac9-758227a76682@gmail.com>
References: <895324221.1881.1490111566009.JavaMail.zimbra@tresgeek.net>
 <1f441f69-102e-7855-4ac9-758227a76682@gmail.com>
Message-ID: <1138189488.3918.1490128216177.JavaMail.zimbra@tresgeek.net>

Hi Yuri,

I should have mentioned that I'm not caching, I'm only using Squid for whitelisting in this case.  Would you still say this is the right path?  It seems that there is a fair amount of hard coding in this method at least based on:




----- Original Message -----
From: "Yuri Voinov" <yvoinov at gmail.com>
To: squid-users at lists.squid-cache.org
Sent: Tuesday, March 21, 2017 1:19:43 PM
Subject: Re: [squid-users] URL list from a URL

Yes.

Functionality you required is:

http://wiki.squid-cache.org/Features/StoreID


21.03.2017 21:52, Jason B. Nance ?????:
> Hello,
>
> I'm using Squid 3.5.20 and wonder if it is possible to define an ACL which retrieves the list of URLs from another URL (similar to pointing to a file).  In this specific use case it is to allow a Foreman server to sync Yum content from the CentOS mirrors.  I tell Foreman to use the following URL:
>
> http://mirrorlist.centos.org/?release=7&arch=x86_64&repo=updates
>
> Which returns a list of URLs, such as:
>
> http://repo1.dal.innoscale.net/centos/7.3.1611/updates/x86_64/
> http://linux.mirrors.es.net/centos/7.3.1611/updates/x86_64/
> http://reflector.westga.edu/repos/CentOS/7.3.1611/updates/x86_64/
> http://mirror.jax.hugeserver.com/centos/7.3.1611/updates/x86_64/
> http://ftp.linux.ncsu.edu/pub/CentOS/7.3.1611/updates/x86_64/
> http://mirror.nexcess.net/CentOS/7.3.1611/updates/x86_64/
> http://mirror.web-ster.com/centos/7.3.1611/updates/x86_64/
> http://centos.host-engine.com/7.3.1611/updates/x86_64/
> http://mirror.raystedman.net/centos/7.3.1611/updates/x86_64/
> http://mirror.linux.duke.edu/pub/centos/7.3.1611/updates/x86_64/
>
> Foreman then starts a new HTTP connection (not a redirect) to attempt to connect to those in turn until it works.
>
> So I would like to configure Squid to allow the Foreman server access to any of those URLs (the list changes somewhat often).
>
> I started to go down the external_acl_type but am wondering if I'm missing something obvious.
>
> Regards,
>
> j
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users

-- 
Bugs to the Future

_______________________________________________
squid-users mailing list
squid-users at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-users


From jason at tresgeek.net  Tue Mar 21 20:32:11 2017
From: jason at tresgeek.net (Jason B. Nance)
Date: Tue, 21 Mar 2017 15:32:11 -0500 (CDT)
Subject: [squid-users] URL list from a URL
In-Reply-To: <1138189488.3918.1490128216177.JavaMail.zimbra@tresgeek.net>
References: <895324221.1881.1490111566009.JavaMail.zimbra@tresgeek.net>
 <1f441f69-102e-7855-4ac9-758227a76682@gmail.com>
 <1138189488.3918.1490128216177.JavaMail.zimbra@tresgeek.net>
Message-ID: <1430194535.3937.1490128331358.JavaMail.zimbra@tresgeek.net>

I'm sorry, this message was sent prematurely. :-\

Completed message follows.


Hi Yuri,

I should have mentioned that I'm not caching, I'm only using Squid for whitelisting in this case.  Would you still say this is the right path?  It seems that there is a fair amount of hard coding in this method at least based on:

http://wiki.squid-cache.org/Features/StoreID/DB

I guess a URL regex could also work given that all the URIs are similar.

Regards,

j


----- Original Message -----
From: "Yuri Voinov" <yvoinov at gmail.com>
To: squid-users at lists.squid-cache.org
Sent: Tuesday, March 21, 2017 1:19:43 PM
Subject: Re: [squid-users] URL list from a URL

Yes.

Functionality you required is:

http://wiki.squid-cache.org/Features/StoreID


21.03.2017 21:52, Jason B. Nance ?????:
> Hello,
>
> I'm using Squid 3.5.20 and wonder if it is possible to define an ACL which retrieves the list of URLs from another URL (similar to pointing to a file).  In this specific use case it is to allow a Foreman server to sync Yum content from the CentOS mirrors.  I tell Foreman to use the following URL:
>
> http://mirrorlist.centos.org/?release=7&arch=x86_64&repo=updates
>
> Which returns a list of URLs, such as:
>
> http://repo1.dal.innoscale.net/centos/7.3.1611/updates/x86_64/
> http://linux.mirrors.es.net/centos/7.3.1611/updates/x86_64/
> http://reflector.westga.edu/repos/CentOS/7.3.1611/updates/x86_64/
> http://mirror.jax.hugeserver.com/centos/7.3.1611/updates/x86_64/
> http://ftp.linux.ncsu.edu/pub/CentOS/7.3.1611/updates/x86_64/
> http://mirror.nexcess.net/CentOS/7.3.1611/updates/x86_64/
> http://mirror.web-ster.com/centos/7.3.1611/updates/x86_64/
> http://centos.host-engine.com/7.3.1611/updates/x86_64/
> http://mirror.raystedman.net/centos/7.3.1611/updates/x86_64/
> http://mirror.linux.duke.edu/pub/centos/7.3.1611/updates/x86_64/
>
> Foreman then starts a new HTTP connection (not a redirect) to attempt to connect to those in turn until it works.
>
> So I would like to configure Squid to allow the Foreman server access to any of those URLs (the list changes somewhat often).
>
> I started to go down the external_acl_type but am wondering if I'm missing something obvious.
>
> Regards,
>
> j
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users

-- 
Bugs to the Future

_______________________________________________
squid-users mailing list
squid-users at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-users


From rousskov at measurement-factory.com  Tue Mar 21 21:07:40 2017
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Tue, 21 Mar 2017 15:07:40 -0600
Subject: [squid-users] [RFC] Do not use idle dead peers
Message-ID: <d61cc3b2-96ab-03fb-d122-13d128a80515@measurement-factory.com>

Hello,

    This Request For Comments proposes to remove a subtle Squid
(mis)feature. If you happen to use the feature detailed below or know
somebody who does, please speak up to protect it! If nobody defends this
feature, we may remove it (to get rid of its bad side effects).


If you use cache_peers, you know that when a peer cannot be reached,
Squid tries a few times (see cache_peer connect-fail-limit; default is
10 times) and then declares the peer "dead". For example:

> 2017/03/21 10:11:46.380| TCP connection to 127.0.0.4/80 failed
> 2017/03/21 10:11:46.380| TCP connection to 127.0.0.4/80 failed
...
> 2017/03/21 10:11:46.394| TCP connection to 127.0.0.4/80 failed
> 2017/03/21 10:11:46.394| Detected DEAD Parent: peer4

Normally, Squid does not forward HTTP transactions to dead peers because
doing so is likely to cause timeouts and other problems. Squid has
mechanisms that detect revived (i.e., no longer dead) peers without
sending regular HTTP requests to peers considered dead. One such
mechanism is TCP probes that check whether opening a TCP connection to
the dead peer started to work.

There are several problems with dead peer handling, and we are working
on fixing some of them, but this RFC focuses on one specific feature:

* Squid may forward an HTTP request to an otherwise eligible but dead
peer that was idle[1] for some time[2].

This "use idle dead peer" feature was introduced as a small part of a
much bigger bug #14 fix. AFAICT, the stated goal of the feature was
speeding up failure recovery:

> revno: 6631
> timestamp: Sat 2004-04-03 21:07:38 +0000
> message:
>   Bug #14: connection setup may look like syn flood attack if server is
>   refusing connection
>   
>   If the contacted server refuses connection then the repeated attempts to
>   connect to the server may look like a syn flood attack. This patch makes
>   Squid behave a little friendler in such case and:
> ...
>    * Cleanup of peer TCP probing to correct timeout management etc and to
>   more promptly recover after a failure.


The "more promptly recover after a failure" phrase probably refers to
the elimination of a single TCP connect(2) peer usage delay or, to be
more precise, the delay between the following two events:

* Start:  An HTTP transaction initiates a background TCP connect probe
          (but is not sent to the dead idle peer).

* Finish: A successful result of a TCP probe initiated above
          (allowing future transactions to use the revived peer).

AFAICT, the feature justification/logic goes something like this: If
there were no failures for a while then perhaps the peer is not dead
anymore. Let's try using it for the current HTTP transaction and see
what happens. If we are lucky, we will start using the peer sooner!

Since the lack of failures does not imply success, the feature may lead
to regular HTTP client transactions being sent to a truly dead peer.
Such transactions may experience delays (at best) or client
disconnects/errors (at worst), depending on Squid and client
configurations/state.

IMO, Squid should not risk regular HTTP transactions this way, and the
actual benefits of such risks are slim in most environments. Thus, we
should remove this feature and simply let existing TCP probes to revive
dead peers. This feature removal does not increase the number of TCP
probes. This feature removal does not delay HTTP transactions as such
(it only delays the time when Squid can resume peer usage).

Does anybody need this "use idle dead peers" feature?


[1] Here, "idle" essentially means a peer that Squid did not probe or
otherwise contact for a while[2]. Peers become idle if they are not
selected by peering algorithms as potential forwarding destinations
(e.g., a dead round-robin parent with very low weight is likely to
become idle even if its "heavy" cousins remain very busy).

[2] The inactivity time associated with becoming idle is calculated as
ten times the peer_connect_timeout (or ten times cache_peer
connect-timeout when set). It defaults to 10*30 seconds or 5 minutes.


Thank you,

Alex.
P.S. Please resist the temptation to discuss other peering problems on
this thread, including other problems associated with detection and
revival of dead peers. Let's focus on this specific feature proposed for
removal.


From yvoinov at gmail.com  Tue Mar 21 21:25:08 2017
From: yvoinov at gmail.com (Yuri Voinov)
Date: Wed, 22 Mar 2017 03:25:08 +0600
Subject: [squid-users] URL list from a URL
In-Reply-To: <1430194535.3937.1490128331358.JavaMail.zimbra@tresgeek.net>
References: <895324221.1881.1490111566009.JavaMail.zimbra@tresgeek.net>
 <1f441f69-102e-7855-4ac9-758227a76682@gmail.com>
 <1138189488.3918.1490128216177.JavaMail.zimbra@tresgeek.net>
 <1430194535.3937.1490128331358.JavaMail.zimbra@tresgeek.net>
Message-ID: <2db1c853-838f-5ed5-31ec-70380f415fb0@gmail.com>



22.03.2017 2:32, Jason B. Nance ?????:
> I'm sorry, this message was sent prematurely. :-\
>
> Completed message follows.
>
>
> Hi Yuri,
>
> I should have mentioned that I'm not caching, I'm only using Squid for whitelisting in this case.  Would you still say this is the right path?  It seems that there is a fair amount of hard coding in this method at least based on:
>
> http://wiki.squid-cache.org/Features/StoreID/DB
>
> I guess a URL regex could also work given that all the URIs are similar.
Mmmmmmmm. May be. You can write common regex for all mirrors, yes.
> Regards,
>
> j
>
>
> ----- Original Message -----
> From: "Yuri Voinov" <yvoinov at gmail.com>
> To: squid-users at lists.squid-cache.org
> Sent: Tuesday, March 21, 2017 1:19:43 PM
> Subject: Re: [squid-users] URL list from a URL
>
> Yes.
>
> Functionality you required is:
>
> http://wiki.squid-cache.org/Features/StoreID
>
>
> 21.03.2017 21:52, Jason B. Nance ?????:
>> Hello,
>>
>> I'm using Squid 3.5.20 and wonder if it is possible to define an ACL which retrieves the list of URLs from another URL (similar to pointing to a file).  In this specific use case it is to allow a Foreman server to sync Yum content from the CentOS mirrors.  I tell Foreman to use the following URL:
>>
>> http://mirrorlist.centos.org/?release=7&arch=x86_64&repo=updates
>>
>> Which returns a list of URLs, such as:
>>
>> http://repo1.dal.innoscale.net/centos/7.3.1611/updates/x86_64/
>> http://linux.mirrors.es.net/centos/7.3.1611/updates/x86_64/
>> http://reflector.westga.edu/repos/CentOS/7.3.1611/updates/x86_64/
>> http://mirror.jax.hugeserver.com/centos/7.3.1611/updates/x86_64/
>> http://ftp.linux.ncsu.edu/pub/CentOS/7.3.1611/updates/x86_64/
>> http://mirror.nexcess.net/CentOS/7.3.1611/updates/x86_64/
>> http://mirror.web-ster.com/centos/7.3.1611/updates/x86_64/
>> http://centos.host-engine.com/7.3.1611/updates/x86_64/
>> http://mirror.raystedman.net/centos/7.3.1611/updates/x86_64/
>> http://mirror.linux.duke.edu/pub/centos/7.3.1611/updates/x86_64/
>>
>> Foreman then starts a new HTTP connection (not a redirect) to attempt to connect to those in turn until it works.
>>
>> So I would like to configure Squid to allow the Foreman server access to any of those URLs (the list changes somewhat often).
>>
>> I started to go down the external_acl_type but am wondering if I'm missing something obvious.
>>
>> Regards,
>>
>> j
>> _______________________________________________
>> squid-users mailing list
>> squid-users at lists.squid-cache.org
>> http://lists.squid-cache.org/listinfo/squid-users

-- 
Bugs to the Future
-------------- next part --------------
A non-text attachment was scrubbed...
Name: 0x613DEC46.asc
Type: application/pgp-keys
Size: 2437 bytes
Desc: not available
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20170322/189abac7/attachment.key>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: signature.asc
Type: application/pgp-signature
Size: 473 bytes
Desc: OpenPGP digital signature
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20170322/189abac7/attachment.sig>

From rousskov at measurement-factory.com  Tue Mar 21 21:42:33 2017
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Tue, 21 Mar 2017 15:42:33 -0600
Subject: [squid-users] URL list from a URL
In-Reply-To: <1138189488.3918.1490128216177.JavaMail.zimbra@tresgeek.net>
References: <895324221.1881.1490111566009.JavaMail.zimbra@tresgeek.net>
 <1f441f69-102e-7855-4ac9-758227a76682@gmail.com>
 <1138189488.3918.1490128216177.JavaMail.zimbra@tresgeek.net>
Message-ID: <705ad8c6-fa21-dd5f-7419-45d07c1284d9@measurement-factory.com>

On 03/21/2017 02:30 PM, Jason B. Nance wrote:

> I should have mentioned that I'm not caching, I'm only using Squid
> for whitelisting in this case.  Would you still say this is the right
> path? 

No. You probably have two better options:

1. Use a file with list of mirror URLs as an ACL parameter. Write a
script that updates that file and reconfigures Squid as needed. Please
keep in mind that Squid reconfiguration is currently a relatively
heavy/intrusive operation, even if there were not changes except for
that single ACL.

2. Write an external_acl helper that will consult the mirror list. This
will make each HTTP transaction a little slower (because it needs to go
to the helper) but eliminates reconfigurations. The helper itself or
some other script will still need to update the mirror list as needed,
of course.


HTH,

Alex.




>> Hello,
>>
>> I'm using Squid 3.5.20 and wonder if it is possible to define an ACL which retrieves the list of URLs from another URL (similar to pointing to a file).  In this specific use case it is to allow a Foreman server to sync Yum content from the CentOS mirrors.  I tell Foreman to use the following URL:
>>
>> http://mirrorlist.centos.org/?release=7&arch=x86_64&repo=updates
>>
>> Which returns a list of URLs, such as:
>>
>> http://repo1.dal.innoscale.net/centos/7.3.1611/updates/x86_64/
>> http://linux.mirrors.es.net/centos/7.3.1611/updates/x86_64/
>> http://reflector.westga.edu/repos/CentOS/7.3.1611/updates/x86_64/
>> http://mirror.jax.hugeserver.com/centos/7.3.1611/updates/x86_64/
>> http://ftp.linux.ncsu.edu/pub/CentOS/7.3.1611/updates/x86_64/
>> http://mirror.nexcess.net/CentOS/7.3.1611/updates/x86_64/
>> http://mirror.web-ster.com/centos/7.3.1611/updates/x86_64/
>> http://centos.host-engine.com/7.3.1611/updates/x86_64/
>> http://mirror.raystedman.net/centos/7.3.1611/updates/x86_64/
>> http://mirror.linux.duke.edu/pub/centos/7.3.1611/updates/x86_64/
>>
>> Foreman then starts a new HTTP connection (not a redirect) to attempt to connect to those in turn until it works.
>>
>> So I would like to configure Squid to allow the Foreman server access to any of those URLs (the list changes somewhat often).
>>
>> I started to go down the external_acl_type but am wondering if I'm missing something obvious.



From eliezer at ngtech.co.il  Wed Mar 22 00:17:24 2017
From: eliezer at ngtech.co.il (Eliezer  Croitoru)
Date: Wed, 22 Mar 2017 02:17:24 +0200
Subject: [squid-users] URL list from a URL
In-Reply-To: <705ad8c6-fa21-dd5f-7419-45d07c1284d9@measurement-factory.com>
References: <895324221.1881.1490111566009.JavaMail.zimbra@tresgeek.net>
 <1f441f69-102e-7855-4ac9-758227a76682@gmail.com>
 <1138189488.3918.1490128216177.JavaMail.zimbra@tresgeek.net>
 <705ad8c6-fa21-dd5f-7419-45d07c1284d9@measurement-factory.com>
Message-ID: <000d01d2a2a1$aedd9fc0$0c98df40$@ngtech.co.il>

The current StoreID helper can be converted pretty fast into what he needs.

Eliezer

----
Eliezer Croitoru
Linux System Administrator
Mobile: +972-5-28704261
Email: eliezer at ngtech.co.il


-----Original Message-----
From: squid-users [mailto:squid-users-bounces at lists.squid-cache.org] On Behalf Of Alex Rousskov
Sent: Tuesday, March 21, 2017 11:43 PM
To: squid-users at lists.squid-cache.org
Subject: Re: [squid-users] URL list from a URL

On 03/21/2017 02:30 PM, Jason B. Nance wrote:

> I should have mentioned that I'm not caching, I'm only using Squid
> for whitelisting in this case.  Would you still say this is the right
> path? 

No. You probably have two better options:

1. Use a file with list of mirror URLs as an ACL parameter. Write a
script that updates that file and reconfigures Squid as needed. Please
keep in mind that Squid reconfiguration is currently a relatively
heavy/intrusive operation, even if there were not changes except for
that single ACL.

2. Write an external_acl helper that will consult the mirror list. This
will make each HTTP transaction a little slower (because it needs to go
to the helper) but eliminates reconfigurations. The helper itself or
some other script will still need to update the mirror list as needed,
of course.


HTH,

Alex.




>> Hello,
>>
>> I'm using Squid 3.5.20 and wonder if it is possible to define an ACL which retrieves the list of URLs from another URL (similar to pointing to a file).  In this specific use case it is to allow a Foreman server to sync Yum content from the CentOS mirrors.  I tell Foreman to use the following URL:
>>
>> http://mirrorlist.centos.org/?release=7&arch=x86_64&repo=updates
>>
>> Which returns a list of URLs, such as:
>>
>> http://repo1.dal.innoscale.net/centos/7.3.1611/updates/x86_64/
>> http://linux.mirrors.es.net/centos/7.3.1611/updates/x86_64/
>> http://reflector.westga.edu/repos/CentOS/7.3.1611/updates/x86_64/
>> http://mirror.jax.hugeserver.com/centos/7.3.1611/updates/x86_64/
>> http://ftp.linux.ncsu.edu/pub/CentOS/7.3.1611/updates/x86_64/
>> http://mirror.nexcess.net/CentOS/7.3.1611/updates/x86_64/
>> http://mirror.web-ster.com/centos/7.3.1611/updates/x86_64/
>> http://centos.host-engine.com/7.3.1611/updates/x86_64/
>> http://mirror.raystedman.net/centos/7.3.1611/updates/x86_64/
>> http://mirror.linux.duke.edu/pub/centos/7.3.1611/updates/x86_64/
>>
>> Foreman then starts a new HTTP connection (not a redirect) to attempt to connect to those in turn until it works.
>>
>> So I would like to configure Squid to allow the Foreman server access to any of those URLs (the list changes somewhat often).
>>
>> I started to go down the external_acl_type but am wondering if I'm missing something obvious.

_______________________________________________
squid-users mailing list
squid-users at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-users



From squid3 at treenet.co.nz  Wed Mar 22 02:09:21 2017
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Wed, 22 Mar 2017 15:09:21 +1300
Subject: [squid-users] Squid Authentication with HTTP REST API
In-Reply-To: <001101d2a153$d6bca350$8435e9f0$@ngtech.co.il>
References: <DB6PR0502MB304543ACB94E65D8F5CA8516B4240@DB6PR0502MB3045.eurprd05.prod.outlook.com>
 <0323d49e-3a2b-4fed-e12c-609306c9b150@treenet.co.nz>
 <165d01d29d6a$b3a847d0$1af8d770$@ngtech.co.il>
 <DB6PR0502MB3045CD1F88CE4D8E166A103FB43A0@DB6PR0502MB3045.eurprd05.prod.outlook.com>
 <001101d2a153$d6bca350$8435e9f0$@ngtech.co.il>
Message-ID: <5a350ef7-b6c4-a767-0f97-aaddf5fa42bb@treenet.co.nz>

On 20/03/2017 9:27 p.m., Eliezer  Croitoru wrote:
> Hey Serhat,
> 
> The right way to support OAUTH2 or any similar idea would be using an ICAP
> service or ECAP module(to my knowledge).

Sigh. Another perfect example of how giving us incorrect information
results in bad answers.

OAuth2 is an actual standard authentication scheme with defined HTTP
features, not "a custom XML Web Service or HTTP REST API" which Serhat
was asking for earlier.

The proper way to implement OAuth2 is with the Bearer authentication
scheme. I did that implementation years ago right after Oauth2 Bearer
was standardized, but it did not get merged because nobody was using it
with proxies at the time and the final polish was going to be hard.

The patch (for an early 3.5) can be found at
<http://www.squid-cache.org/mail-archive/squid-dev/201407/0147.html> and
<http://wiki.squid-cache.org/Features/BearerAuthentication> the
documentation for using it in a patched squid. If you want to sponsor
the work Serhat I would be happy to update it to current releases.

HTH
Amos



From cwaldon at otn.ca  Wed Mar 22 02:11:29 2017
From: cwaldon at otn.ca (Waldon, Cooper)
Date: Wed, 22 Mar 2017 02:11:29 +0000
Subject: [squid-users] Assistance with WCCPv2 Setup with Cisco Router
	(Yuri Voinov)
In-Reply-To: <mailman.12205.1490123678.20516.squid-users@lists.squid-cache.org>
References: <mailman.12205.1490123678.20516.squid-users@lists.squid-cache.org>
Message-ID: <YTOPR01MB04762793993AB6F623D316BFDF3C0@YTOPR01MB0476.CANPRD01.PROD.OUTLOOK.COM>

Hello,


I'm not sure if I understand what you're asking me to check.


I've followed the steps in the configuration examples provided and it's still not working.


The Squid proxy and Cisco router are both configured to use GRE as you mentioned in your other message:


R3#show ip wccp summary
WCCP version 2 enabled, 2 services

Service     Clients   Routers   Assign      Redirect   Bypass
-------     -------   -------   ------      --------   ------
Default routing table (Router Id: 192.168.0.23):
web-cache   1         1         HASH        GRE        GRE
70          1         1         HASH        GRE        GRE

I did find this note listed on the page, http://wiki.squid-cache.org/ConfigExamples/Intercept/CiscoIOSv15Wccp2


[{i}] Note: As usual, it's expected your Squid already configured with HTTP and HTTPS ports.


But I'm not entirely sure what that means?


Thanks,

Cooper


Message: 1
Date: Wed, 22 Mar 2017 01:14:19 +0600
From: Yuri Voinov <yvoinov at gmail.com>
To: squid-users at lists.squid-cache.org
Subject: Re: [squid-users] Assistance with WCCPv2 Setup with Cisco
        Router
Message-ID: <d33498f4-3dfc-4fe2-2a35-3a64f4a08d24 at gmail.com>
Content-Type: text/plain; charset="utf-8"

Ah, forgot about this:

https://na01.safelinks.protection.outlook.com/?url=http%3A%2F%2Fwiki.squid-cache.org%2FConfigExamples%2FIntercept&data=01%7C01%7Ccwaldon%40otn.ca%7C719ecf3df906402c5bef08d4708e801f%7Cb211ab61e77f4bffabd5f70e4344653f%7C1&sdata=EPs3eDmARBmwyp8VES4Ret7aO8ZlIQ7H1LRZKC7lUQQ%3D&reserved=0



-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20170322/ec9e28d8/attachment.htm>

From squid3 at treenet.co.nz  Wed Mar 22 02:16:17 2017
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Wed, 22 Mar 2017 15:16:17 +1300
Subject: [squid-users] Squid Authentication with HTTP REST API
In-Reply-To: <DB6PR0502MB3045CD1F88CE4D8E166A103FB43A0@DB6PR0502MB3045.eurprd05.prod.outlook.com>
References: <DB6PR0502MB304543ACB94E65D8F5CA8516B4240@DB6PR0502MB3045.eurprd05.prod.outlook.com>
 <0323d49e-3a2b-4fed-e12c-609306c9b150@treenet.co.nz>
 <165d01d29d6a$b3a847d0$1af8d770$@ngtech.co.il>
 <DB6PR0502MB3045CD1F88CE4D8E166A103FB43A0@DB6PR0502MB3045.eurprd05.prod.outlook.com>
Message-ID: <582ca0d2-b9eb-1cba-efce-3ec400a8dea5@treenet.co.nz>

On 20/03/2017 7:49 p.m., Serhat Koroglu wrote:
> Sorry for late reply.
> 
> I have find a suitable solution for validation through a http web
> service. Here tells developing custom helper even using php:
> http://freesoftwaremagazine.com/articles/authentication_with_squid/
> That's nice.
> 
> 
> Then I know there is  authentication with oauth2 for squid-server.
> But you may be know, in oauth2 authentication, you must authorize the
> app using user's credentials e.g. facebook username and password.
> When this oauth2 method is used, your app must redirect to the oauth2
> service to authorize your app.
> 
> 
> You may had used many web sites like that with facebook login. So my
> question is how may squid server do this redirect  and authorization
> process using a third party oauth2 service? Squid asks username and
> password with web browser popup. Is there any example to this?

No this is abolutely *not* how Squid asks for authentication.

Squid "asks" telling the client that authentication credentials are
needed and listing the schemes that it will accept credentials for.
*Some* clients (eg Browsers) decide all on their own to use a popup to
annoy the user if they cannot supply any of those credential types
automatically. How they try to get them (or not) is way outside and
irrelevant to Squid.

For OAuth2 Squid just needs to support the Bearer authentication scheme,
and a helper to verify the credentials token which the client/browser
sends back.

Amos



From cwaldon at otn.ca  Wed Mar 22 02:28:31 2017
From: cwaldon at otn.ca (Waldon, Cooper)
Date: Wed, 22 Mar 2017 02:28:31 +0000
Subject: [squid-users]  Assistance with WCCPv2 Setup with Cisco Router
In-Reply-To: <mailman.12205.1490123678.20516.squid-users@lists.squid-cache.org>
References: <mailman.12205.1490123678.20516.squid-users@lists.squid-cache.org>
Message-ID: <YTOPR01MB04760DCA6CFB52FACAB08CDBDF3C0@YTOPR01MB0476.CANPRD01.PROD.OUTLOOK.COM>

Sorry, I didn't see your original reply.


I will look into these issues and troubleshoot further, thank you.


Cooper


________________________________
From: squid-users <squid-users-bounces at lists.squid-cache.org> on behalf of squid-users-request at lists.squid-cache.org <squid-users-request at lists.squid-cache.org>
Sent: Tuesday, March 21, 2017 3:14 PM
To: squid-users at lists.squid-cache.org
Subject: squid-users Digest, Vol 31, Issue 67

Send squid-users mailing list submissions to
        squid-users at lists.squid-cache.org

To subscribe or unsubscribe via the World Wide Web, visit
        https://na01.safelinks.protection.outlook.com/?url=http%3A%2F%2Flists.squid-cache.org%2Flistinfo%2Fsquid-users&data=01%7C01%7Ccwaldon%40otn.ca%7C719ecf3df906402c5bef08d4708e801f%7Cb211ab61e77f4bffabd5f70e4344653f%7C1&sdata=S%2BTxOG9DaQkq8MDxF5obmrM4R%2BtekaFg8S4fXUlynec%3D&reserved=0
or, via email, send a message with subject or body 'help' to
        squid-users-request at lists.squid-cache.org

You can reach the person managing the list at
        squid-users-owner at lists.squid-cache.org

When replying, please edit your Subject line so it is more specific
than "Re: Contents of squid-users digest..."


Today's Topics:

   1. Re: Assistance with WCCPv2 Setup with Cisco Router (Yuri Voinov)


----------------------------------------------------------------------

Message: 1
Date: Wed, 22 Mar 2017 01:14:19 +0600
From: Yuri Voinov <yvoinov at gmail.com>
To: squid-users at lists.squid-cache.org
Subject: Re: [squid-users] Assistance with WCCPv2 Setup with Cisco
        Router
Message-ID: <d33498f4-3dfc-4fe2-2a35-3a64f4a08d24 at gmail.com>
Content-Type: text/plain; charset="utf-8"

Ah, forgot about this:

https://na01.safelinks.protection.outlook.com/?url=http%3A%2F%2Fwiki.squid-cache.org%2FConfigExamples%2FIntercept&data=01%7C01%7Ccwaldon%40otn.ca%7C719ecf3df906402c5bef08d4708e801f%7Cb211ab61e77f4bffabd5f70e4344653f%7C1&sdata=EPs3eDmARBmwyp8VES4Ret7aO8ZlIQ7H1LRZKC7lUQQ%3D&reserved=0


22.03.2017 1:04, Waldon, Cooper ?????:
>
> Hello All,
>
>
>
> I?m trying to set up a transparent proxy for http and https using
> Cisco Routers and Squid.  I have followed the configuration examples
> that are listed under the wccp2 overview section
> (https://na01.safelinks.protection.outlook.com/?url=http%3A%2F%2Fwiki.squid-cache.org%2FFeatures%2FWccp2&data=01%7C01%7Ccwaldon%40otn.ca%7C719ecf3df906402c5bef08d4708e801f%7Cb211ab61e77f4bffabd5f70e4344653f%7C1&sdata=kEcy58RMI6q8cV0SzQacGAjm6q5NsSGO%2By8PRmvUf5w%3D&reserved=0) of the squid wiki but I?m
> still having some issues.
>
>
>
> I have a little lab set up with a Cisco 7200 Router and a VM with
> CentOS running the proxy.
>
>
>
> The ?WAN? IP of the Router is 192.168.0.23.  The IP of the Squid Proxy
> is 192.168.0.24 and both have the default gateway of 192.168.0.1 which
> is the ?ISP?
>
>
>
> The Client is sitting on a LAN behind the Router in the 10.10.10.0/24
> subnet and is also sitting behind nat.
>
>
>
> I believe that the router and proxy are communicating properly based
> on the information in the show ip wccp command on the router as it
> shows clients and routers as well as showing that packets are being
> forwarded:
>
>
>
> R3#show ip wccp
>
> Global WCCP information:
>
>     Router information:
>
>         Router Identifier:                   192.168.0.23
>
>         Configured source-interface:         GigabitEthernet5/0
>
>
>
>     Service Identifier: web-cache
>
>         Protocol Version:                    2.00
>
>         Number of Service Group Clients:     1
>
>         Number of Service Group Routers:     1
>
>         Total Packets Redirected:            1079
>
>           Process:                           0
>
>           CEF:                               1079
>
>         Service mode:                        Open
>
>         Service Access-list:                 -none-
>
>         Total Packets Dropped Closed:        0
>
>         Redirect access-list:                100
>
>         Total Packets Denied Redirect:       0
>
>         Total Packets Unassigned:            0
>
>         Group access-list:                   10
>
>         Total Messages Denied to Group:      0
>
>         Total Authentication failures:       0
>
>         Total GRE Bypassed Packets Received: 0
>
>           Process:                           0
>
>           CEF:                               0
>
>        GRE tunnel interface:                Tunnel1
>
>
>
>     Service Identifier: 70
>
>         Protocol Version:                    2.00
>
>         Number of Service Group Clients:     1
>
>         Number of Service Group Routers:     1
>
>         Total Packets Redirected:            500
>
>           Process:                           0
>
>           CEF:                               500
>
>         Service mode:                        Open
>
>         Service Access-list:                 -none-
>
>         Total Packets Dropped Closed:        0
>
>         Redirect access-list:                100
>
>         Total Packets Denied Redirect:       0
>
>         Total Packets Unassigned:            0
>
>         Group access-list:                   10
>
>         Total Messages Denied to Group:      0
>
>         Total Authentication failures:       0
>
>         Total GRE Bypassed Packets Received: 0
>
>           Process:                           0
>
>           CEF:                               0
>
>         GRE tunnel interface:                Tunnel0
>
>
>
> Here is the relevant squid wccp configuration:
>
>
>
> ----Output removed----
>
> # Squid normally listens to port 3128
>
> http_port 3128
>
> http_port 0.0.0.0:3129
>
>
>
> # WCCPv2 Parameters
>
> wccp2_router 192.168.0.23
>
> wccp2_forwarding_method 1
>
> wccp2_return_method 1
>
> wccp2_assignment_method hash
>
> wccp2_service standard 0
>
> wccp2_service dynamic 70
>
> wccp2_service_info 70 protocol=tcp
> flags=dst_ip_hash,src_ip_alt_hash,src_port_alt_hash priority=231 ports=443
>
>
>
> ---Output remove----
>
>
>
> I think that the issue lies with the iptables configuration as I do
> not see any packets been processed in the nat table.  I have tried a
> few different methods such as:
>
>
>
> iptables -t nat -A PREROUTING -i wccp0 -p tcp ?dport 80 -j REDIRECT
> ?to-port 3129
>
> iptables -t nat -A PREROUTING -i wccp0 -p tcp ?dport 443 -j REDIRECT
> ?to-port 3129
>
> iptables -t nat -A POSTROUTING -j MASQUERADE
>
>
>
> or
>
>
>
> iptables -t nat -A PREROUTING -p tcp ?dport 80 -j DNAT ?to-destination
> 192.168.0.24:3129
>
> iptables -t nat -A PREROUTING -p tcp ?dport 443 -j DNAT
> ?to-destination 192.168.0.24:3129
>
> iptables -t nat -A POSTROUTING -j MASQUERADE
>
>
>
> I have also tried adding ACCEPT commands to the PREROUTING zone just
> in case the proxy is dropping the packets right away but that also
> doesn?t work.
>
>
>
> The proxy functions perfectly when the client is configured to use a
> proxy so there doesn?t appear to be any issues with routing or
> anything like that, it?s just the transparent proxying that isn?t working.
>
>
>
> If anyone has any suggestions of what I could try that would be
> greatly appreciated.  Let me know if anything is unclear or if you
> need further clarification.
>
>
>
> Thank you,
>
> Cooper Waldon
>
>
>
>
>
> *Cooper Waldon** **l **Network
> Engineer** **l****OTN****l****416-446-4110 x 4473 **l** **www.otn.ca*
> <http://www.otn.ca/>***|****Service Desk 1-855-654-0888 x2*
>
>
>
>
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> https://na01.safelinks.protection.outlook.com/?url=http%3A%2F%2Flists.squid-cache.org%2Flistinfo%2Fsquid-users&data=01%7C01%7Ccwaldon%40otn.ca%7C719ecf3df906402c5bef08d4708e801f%7Cb211ab61e77f4bffabd5f70e4344653f%7C1&sdata=S%2BTxOG9DaQkq8MDxF5obmrM4R%2BtekaFg8S4fXUlynec%3D&reserved=0

--
Bugs to the Future
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <https://na01.safelinks.protection.outlook.com/?url=http%3A%2F%2Flists.squid-cache.org%2Fpipermail%2Fsquid-users%2Fattachments%2F20170322%2F19763217%2Fattachment.html&data=01%7C01%7Ccwaldon%40otn.ca%7C719ecf3df906402c5bef08d4708e801f%7Cb211ab61e77f4bffabd5f70e4344653f%7C1&sdata=%2BdRlah9JgnWfvUSTiulB%2BaTWQXY%2BNmyP%2BsAa4A%2FXL%2BU%3D&reserved=0>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: 0x613DEC46.asc
Type: application/pgp-keys
Size: 2437 bytes
Desc: not available
URL: <https://na01.safelinks.protection.outlook.com/?url=http%3A%2F%2Flists.squid-cache.org%2Fpipermail%2Fsquid-users%2Fattachments%2F20170322%2F19763217%2Fattachment.key&data=01%7C01%7Ccwaldon%40otn.ca%7C719ecf3df906402c5bef08d4708e801f%7Cb211ab61e77f4bffabd5f70e4344653f%7C1&sdata=CnmuhfgaBl5NFNuEp0C9VqbOuFAhlX32zIehczGNRl8%3D&reserved=0>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: signature.asc
Type: application/pgp-signature
Size: 473 bytes
Desc: OpenPGP digital signature
URL: <https://na01.safelinks.protection.outlook.com/?url=http%3A%2F%2Flists.squid-cache.org%2Fpipermail%2Fsquid-users%2Fattachments%2F20170322%2F19763217%2Fattachment.sig&data=01%7C01%7Ccwaldon%40otn.ca%7C719ecf3df906402c5bef08d4708e801f%7Cb211ab61e77f4bffabd5f70e4344653f%7C1&sdata=z1x4mb0FAeJqa0WPXZdnHuKgXc8BLAVL6INMTfquOaY%3D&reserved=0>

------------------------------

Subject: Digest Footer

_______________________________________________
squid-users mailing list
squid-users at lists.squid-cache.org
https://na01.safelinks.protection.outlook.com/?url=http%3A%2F%2Flists.squid-cache.org%2Flistinfo%2Fsquid-users&data=01%7C01%7Ccwaldon%40otn.ca%7C719ecf3df906402c5bef08d4708e801f%7Cb211ab61e77f4bffabd5f70e4344653f%7C1&sdata=S%2BTxOG9DaQkq8MDxF5obmrM4R%2BtekaFg8S4fXUlynec%3D&reserved=0


------------------------------

End of squid-users Digest, Vol 31, Issue 67
*******************************************
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20170322/18370430/attachment.htm>

From squid3 at treenet.co.nz  Wed Mar 22 03:23:21 2017
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Wed, 22 Mar 2017 16:23:21 +1300
Subject: [squid-users] ext_wbinfo_group_acl is not working
In-Reply-To: <BY2PR15MB0901BD353F86AE7E20E7F9919E3D0@BY2PR15MB0901.namprd15.prod.outlook.com>
References: <4ef48a6a-df57-09b9-2a84-39c7b9ee6c0c@treenet.co.nz>
 <BY2PR15MB0901BD353F86AE7E20E7F9919E3D0@BY2PR15MB0901.namprd15.prod.outlook.com>
Message-ID: <5910078c-9383-6a99-e878-57846bd77b7b@treenet.co.nz>

On 22/03/2017 1:04 a.m., Ver?nica Ovando wrote:
>>> Hi, everybody!
>>>
>>>
>>> I have my Squid 3.4.8 running in Debian Jessie. It has been working with Active Directory authentication for more than a year without any kind of problem. But since a couple of weeks ago, suddenly, it stopped authenticate users, asking for credentials (username and pass) and they are not able to browse. I am getting this messages in /var/log/cache.log:
>>>
>>>
>>> 2017/03/04 12:04:25.806 kid1| WARNING: external ACL 'Grupos_AD' queue overload. Request rejected 'user1 it_group'.
>>>
>>
>> This means that your AD is not keeping up with the traffic through your
>> proxy.
>> Since your Squid has children=100 it will queue up to 200 transactions
>> waiting for the helper before this message is shown.
>>
>>
>> Dis cache.log have anything else from the external helper? you have
>> debug mode enabled (-d) so it should be reporting if there are any
>> issues with AD other than simply slowness.
>>
>>
> 
> Yes. I found this:
> 14:53:48 [root at server squid3]# tail -f /var/log/squid3/cache.log | grep helper
> 2017/03/16 14:54:19.527 kid1| Acl.cc(62) AuthenticateAcl: returning 2 sending credentials to helper.
> 2017/03/16 14:54:19.532 kid1| Acl.cc(62) AuthenticateAcl: returning 2 sending credentials to helper.
> 2017/03/16 14:54:20.743 kid1| Acl.cc(62) AuthenticateAcl: returning 2 sending credentials to helper.
> 
> And this:
> 2017/03/16 14:53:47.887 kid1| Acl.cc(118) FindByName: ACL::FindByName 'it_group'
> 2017/03/16 14:53:47.887 kid1| Gadgets.cc(71) aclGetDenyInfoPage: got called for it_group
> 2017/03/16 14:53:48.028 kid1| Acl.cc(157) matches: checking it_group
> 2017/03/16 14:53:48.028 kid1| Acl.cc(177) matches: checked: it_group = -1
> 2017/03/16 14:53:48.028 kid1| Gadgets.cc(103) aclIsProxyAuth: aclIsProxyAuth: called for it_group
> 2017/03/16 14:53:48.028 kid1| Acl.cc(118) FindByName: ACL::FindByName 'it_group'
> 

Hmm. These lines are still from Squid itself. Note the "kid" portion of
the line entry is only produced by Squid.
I'm not sure about debug on the Samba helper, but debug lines from
helpers will usually show up either no timestamp, maybe with a different
timestamp format, and maybe the helpers binary name where the kid part is.

Anyhow, the above lines seem to indicate the group checking it being
done. Or at least started. Which matches what you said earlier, they are
starting but may be overloading.


<snip>
>>
>> The current Squid versions can auto-start helpers as needed. See the
>> auth_param and external_acl_type documentation for the max=, startup=
>> and idle= options. That may help a little, or at least allow you to
>> configure higher max limits to cope with slow AD periods.
>>
>>
>>
> 
> I tried with those params but nothing happens. The helper doesn't  auto-start.
> external_acl_type Grupos_AD ttl=10 children-max=10 children-startup=10 children-idle=10 %LOGIN /usr/lib/squid3/ext_wbinfo_group_acl -d
> 
> And as suggested in the Squid wiki http://www.squid-cache.org/Doc/config/auth_param/, I used this values for:
> auth_param ntlm program /usr/bin/ntlm_auth --helper-protocol=squid-2.5-ntlmssp --DOMAIN=RENTAS
> auth_param ntlm children 20 startup=0 idle=1
> auth_param ntlm keep_alive off
> 
> auth_param basic program /usr/bin/ntlm_auth --helper-protocol=squid-2.5-basic
> auth_param basic children 5 startup=5 idle=1
> auth_param basic realm DPR-proxy
> auth_param basic credentialsttl 2 hours
> 
>> Another possibility is converting to the LDAP group lookup instead of
>> using the wbinfo tool to do lookups. I know that LDAP does not suffer
>>from wbind connection limits, which might be part of your issue.
>>
>> Amos
>>
> Please Amos provide me further guidance. Cannot find a solution to this. Thanks!

I'm stuck myself now. The info you ave been able to provide does not
seem to contain any useful clues about what the problem might be other
than just the helper queue overloading. It has been a long time since I
went near the Windows stuff so memory has paged out sorry :-(.

Maybe raising the helpers max limit? with these 5 and 10 helper limits I
would expect Squid to only be able to handle 20-150 requests per second.
I usually advice using a limit of ~200 helpers for NTLM related things,
a bit more if you have a busy proxy. The point of the new
max/startup/idle feature is that you can set the startup value low and a
much higher maximum.


One other thing that might help is; with that starup=0 you should not
see any NTLM auth helpers immediately after starting Squid. The first
ntlm_auth helper is only started (from idle=1) when the first HTTP
request that needs credential checking is received. Your checks will
need to take that into consideration.
 (I suspect systemd may not be able to track helpers which are started
after the initial startup process is completed.)

Amos



From squid3 at treenet.co.nz  Wed Mar 22 03:53:28 2017
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Wed, 22 Mar 2017 16:53:28 +1300
Subject: [squid-users] Cache-Peer - Negotiate
In-Reply-To: <1372479245.745829.1490099437343@mail.yahoo.com>
References: <1372479245.745829.1490099437343@mail.yahoo.com>
Message-ID: <93be607c-16b9-2d0f-6844-818bc597610f@treenet.co.nz>

On 22/03/2017 1:26 a.m., Hareesh wrote:
> Hi,
> I am trying to setup Squid as a local HTTP child proxy to a parent/corporate Cisco Ironport WSA proxy. I need help in setting up authentication(Negotiate) to be done automatically from any client who is trying to access internet through the child proxy. So here is what I did.
>    
>    - Installed Squid on Windows machine with the installable given by Diladele v 3.5.24. Configured the service to run with an account (domain\account1) that has admin rights to that machine.   
> 
>    - Got a keytab file for the account and host from our AD Admins. Here is the command run to get the keytab file.   
> 
>             ktpass /princ HTTP/server1.subdomain.domain.com at SUBDOMAIN.DOMAIN.COM/mapuser domain\account1 /crypto all  /pass <password_for_account1> /ptypeKRB5_NT_PRINCIPAL /out account.keytab
>    
>    - Copied that keytab file into etc\squid folder of my Windows installation of Squid.   
> 
>    -    
> 
>    - Set the following configuration in squid.conf.   
> 
> 
> http_port 3128cache_peer <parent_proxy_Ip> parent 80 0 no-query default proxy-only login=NEGOTIATE
> http_access allow allnever_direct allow allicp_access deny all
> dns_nameservers <DNS_IP1>  <DNS_IP2> 127.0.0.1
> My objective is **any allowed client** irrespective of Unix/Windows/domain/non-domain users should be able to reach to internet.

The non-domain users will bite you. Guaranteed. Not being part of the
domain means no credentials, which means no authentication can be performed.

Apart from that either the Negotiate works or it does not. That is
entirely dependent on whether the client has working credentials - which
is outside of Squid control. The particular client OS may reflect
whether that OS has working Kerberos credentials available, but that is
as far as OS comes into the equation.


> I will set up ACL to specify the IP addresses to use this proxy later. But for now, I am getting a 407 error from any machine trying to use this proxy. I am not sure what is going wrong. Please advise.

407 is entirely expected with secure types of authentication. No sane
client will broacast its credentials aross the network without first
being informed that they are necessary - and what type/scheme they have
to be. That takes the form of a 407 or 401.

What is expected is that the client handles that silently and re-tries
immediately with the required credentials. No further 407 *on that TCP
connection*.

The per-TCP connection detail is important. If you have persistent
connections enabled in the proxy you can expect some 407's then abunch
of authenticated traffic without any.

Browsers can open 8 connections _per tab_ on startup each of which has
to be authenticated separately. Other software usually only 1, but can
open ~4. This can result in a burst of 407's on first contact between
any particualr user and the proxy. It should stabilize quickly though,
and all happen quietly without the user being involved.



> I was looking at this link as well.
> Squid - Users - Parent proxy with authentication
> 
>   
> |  
> |   
> |   
> |   |    |
> 
>    |
> 
>   |
> |  
> |   |  
> Squid - Users - Parent proxy with authentication
>  Parent proxy with authentication. Hello, can someone please tell me, what my my cache_peer line must look like, ...  |   |
> 
>   |
> 
>   |
> 

I think you have the sequence reversed. It should be:

 user/client -> Squid -> Cisco -> Interwebs


The config line you have:
  cache_peer <parent_proxy_Ip> parent 80 0 no-query default proxy-only
login=NEGOTIATE

... tells Squid to login *as itself* to the upstream proxy.

That is fine if you want the users to login to Squid (or not for the
non-domain ones), and everything going through the upstream proxy to be
authenticated and logged against the Squid account.
 If so, then add the auth_param lines to verify the credentials Squid is
gettign from users.

For passing through the Negotiate credentials client-to-Cisco, you
should use "login=PASSTHRU connection-auth=on". Squid itself does not
participate in the authentication, so does not need any keytabs etc
setup for those clients.

However, as I mentioned above the non-domain users will bite you. They
dont have credentials so cannot authenticate at all.

With a bit of config trickery, you could use that line with
login=NEGOTIATE to send the Squid proxy account keytab details to get
them through the upstream.

The config trick is how to detect whether the client is off-domain
(dedicated http_port with myportname ACL?) then use cache_peer_access to
split that off-domain traffic through the login=NEGOTIATE peer while
on-domain goes through the login=PASSTHRU peer.

PS. this is just theoretcical config so YMMV.


HTH
Amos



From bosscb.chrisbren at gmail.com  Wed Mar 22 10:59:14 2017
From: bosscb.chrisbren at gmail.com (christian brendan)
Date: Wed, 22 Mar 2017 11:59:14 +0100
Subject: [squid-users] Squid Transparent/intercept Issues
Message-ID: <CAHptoxqQ7ECbY2R=5YBcO8WWfPvFnjeM2dOQzEovXiU12On-cw@mail.gmail.com>

One more thing,
Does this implies using two NICs (Network Interface Cards)?
And the squid server has to be in-between clients and the internet?

Regards




On Tue, Mar 21, 2017 at 5:29 PM, christian brendan <
bosscb.chrisbren at gmail.com> wrote:

> Thanks a lot for the information.
> I will try this and give feedback.
> Best Regards
>
> On Tue, Mar 21, 2017 at 1:00 PM, <squid-users-request at lists.
> squid-cache.org> wrote:
>
>> Send squid-users mailing list submissions to
>>         squid-users at lists.squid-cache.org
>>
>> To subscribe or unsubscribe via the World Wide Web, visit
>>         http://lists.squid-cache.org/listinfo/squid-users
>> or, via email, send a message with subject or body 'help' to
>>         squid-users-request at lists.squid-cache.org
>>
>> You can reach the person managing the list at
>>         squid-users-owner at lists.squid-cache.org
>>
>> When replying, please edit your Subject line so it is more specific
>> than "Re: Contents of squid-users digest..."
>>
>>
>> Today's Topics:
>>
>>    1. Re: Squid Transparent/intercept Issues (Antony Stone)
>>
>>
>> ----------------------------------------------------------------------
>>
>> Message: 1
>> Date: Tue, 21 Mar 2017 12:12:01 +0100
>> From: Antony Stone <Antony.Stone at squid.open.source.it>
>> To: squid-users at lists.squid-cache.org
>> Subject: Re: [squid-users] Squid Transparent/intercept Issues
>> Message-ID: <201703211212.01346.Antony.Stone at squid.open.source.it>
>> Content-Type: Text/Plain;  charset="utf-8"
>>
>> On Tuesday 21 March 2017 at 12:00:05, christian brendan wrote:
>>
>> > > Today's Topics:
>> > >    1. Re: Squid Transparent/intercept Issues (Antony Stone)
>> > >    2. Re: SMP and AUFS (Matus UHLAR - fantomas)
>> > >    3. Re: SMP and AUFS (Alex Rousskov)
>> > >    4. Re: squid workers question (Alex Rousskov)
>> > >    5. Re: squid workers question (Matus UHLAR - fantomas)
>> > >    6. Re: SSL Bump issues (Alex Rousskov)
>> > >    7. blocking or allowing specific youtube videos (Sohan Wijetunga)
>>
>> Please edit your reply when responding to a digest email, deleting
>> everything
>> not specific to your question.
>>
>> > > Date: Mon, 20 Mar 2017 16:56:17 +0100
>> > > From: Antony Stone
>> > > To: squid-users at lists.squid-cache.org
>> > > Subject: Re: [squid-users] Squid Transparent/intercept Issues
>> > >
>> > > On Monday 20 March 2017 at 16:26:40, christian brendan wrote:
>> > > > Hello Everyone,
>> > > >
>> > > > Squid Cache: Version 3.5.20
>> > > > OS: CentOS 7
>> > > >
>> > > > I have used squid for quite some times non transparently and it
>> works,
>> > > > problem kicks in when: http_port 3128 transparent is enabled.
>> > > > Access denied error page shows up when transparent is enabled
>> > > > ERRORThe requested URL could not be retrieved
>> > >
>> > > How are you getting the packets to the Squid server for interception?
>> > >
>> > > Is the Squid server in the default route between your clients and the
>> > > Internet, or are you redirecting the packets to the Squid server
>> somehow?
>> > >
>> > > Please give *details* of how you are intercepting and sending the
>> packets
>> > > to Squid (eg: iptables rules, and which machine/s the rules are
>> running
>> > > on).
>> > >
>> > >
>> > > Antony.
>>
>> > ?@Antony.Stone
>> > 1. ?I am using mikrotik routerboard to redirect traffic, with this rule:
>> > dd action=dst-nat chain=dstnat comment="Redirect port 80 to SquidProxy"
>> > dst-port=80 protocol=tcp \ src-address=10.24.7.100
>> to-addresses=10.24.7.101
>> > to-ports=3128
>>
>> Okay, so there's your problem, then.
>>
>> You must not use DSTNAT on a separate router to send packets to Squid for
>> intercept.
>>
>> (This used to work in older versions of Squid, but does not work any more
>> and
>> is documented on the wiki, for example at
>> http://wiki.squid-cache.org/ConfigExamples/Intercept/LinuxDnat )
>>
>> Note the wording: "NOTE: This configuration is given for use on the squid
>> box."
>> That means the NAT rules *must* be running on the Squid box itself and
>> not (in
>> your case) on the Mikrotik router.
>>
>> > 3.? It is not in default route, packets is been redirected.
>>
>> In that case you need to use policy routing to get the packets
>> *unchanged* to
>> the Squid box - see the above link, and also
>> http://wiki.squid-cache.org/ConfigExamples/Intercept/IptablesPolicyRoute
>>
>> > ?4. There is no iptable rules, firewall is disabled for this test.
>>
>> You have to have a REDIRECT rule on the machine running Squid to get it
>> to see
>> the packets (once they are no longer being DNATted).
>>
>> Please try to follow the guidelines at
>> http://wiki.squid-cache.org/ConfigExamples/Intercept/LinuxDnat and
>> http://wiki.squid-cache.org/ConfigExamples/Intercept/IptablesPolicyRoute
>> and
>> then come back to us with details of what you've tried, if there are still
>> problems.
>>
>>
>> Regards,
>>
>>
>> Antony.
>>
>> --
>> A user interface is like a joke.
>> If you have to explain it, it didn't work.
>>
>>                                                    Please reply to the
>> list;
>>                                                          please *don't*
>> CC me.
>>
>>
>> ------------------------------
>>
>> Subject: Digest Footer
>>
>> _______________________________________________
>> squid-users mailing list
>> squid-users at lists.squid-cache.org
>> http://lists.squid-cache.org/listinfo/squid-users
>>
>>
>> ------------------------------
>>
>> End of squid-users Digest, Vol 31, Issue 61
>> *******************************************
>>
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20170322/97ae6f1b/attachment.htm>

From Antony.Stone at squid.open.source.it  Wed Mar 22 11:38:38 2017
From: Antony.Stone at squid.open.source.it (Antony Stone)
Date: Wed, 22 Mar 2017 12:38:38 +0100
Subject: [squid-users] Squid Transparent/intercept Issues
In-Reply-To: <CAHptoxqQ7ECbY2R=5YBcO8WWfPvFnjeM2dOQzEovXiU12On-cw@mail.gmail.com>
References: <CAHptoxqQ7ECbY2R=5YBcO8WWfPvFnjeM2dOQzEovXiU12On-cw@mail.gmail.com>
Message-ID: <201703221238.38894.Antony.Stone@squid.open.source.it>

On Wednesday 22 March 2017 at 11:59:14, christian brendan wrote:

> One more thing,
> Does this implies using two NICs (Network Interface Cards)?

No, this is not necessary.

> And the squid server has to be in-between clients and the internet?

That is the simpler way of doing it (in which case you would want two NICs, 
yes).

Basically your choices are:

1. Put the Squid server in the route between clients and the Internet (so, it 
has two NICs, each with an address on different networks), and an IPtables 
REDIRECT rule to send port 80 & 443 traffic to Squid.

2. Put your Squid server (with one NIC) wherever you like, having just a 
single IP address (and able to route to the Internet), and use policy routing 
on your Mikrotik router to send any packets from clients heading for port 80 & 
443 out on the Internet, to the Squid server instead (without doing DNAT and 
changing the destination address).  You still need the REDIRECT rule on the 
Squid server, and you must ensure that when Squid then makes its own request 
out to the Internet, that goes out, and does not get intercepted by the 
Mikrotik and sent back to Squid again :)


Antony.

-- 
Late in 1972 President Richard Nixon announced that the rate of increase of 
inflation was decreasing.   This was the first time a sitting president used a 
third derivative to advance his case for re-election.

 - Hugo Rossi, Notices of the American Mathematical Society

                                                   Please reply to the list;
                                                         please *don't* CC me.


From listener at wernig.net  Wed Mar 22 13:20:21 2017
From: listener at wernig.net (Markus Wernig)
Date: Wed, 22 Mar 2017 14:20:21 +0100
Subject: [squid-users] Squid blocking own OCSP/AIA requests
In-Reply-To: <c2bbc299-849b-063b-1ed4-6b9517ba987c@wernig.net>
References: <c2bbc299-849b-063b-1ed4-6b9517ba987c@wernig.net>
Message-ID: <9a72eb04-124c-3222-d7b5-56d9cd2406bf@wernig.net>

Small update:

- The URL http://apps.identrust.com/roots/dstrootcax3.p7c is not the
OCSP responder, but the AIA for the Root CA (DST Root CA X3) embedded in
the issuing CA's certificate's CA Issuers.
- Same for
http://swisssign.net/cgi-bin/authority/download/5B257B96A465517EB839F3C078665EE83AE7F0EE:
AIA for Root CA.

Since squid is sslbumping the connection, it must be doing the AIA
lookups (presumably for SSL verification). Does anybody have an idea why
it is blocking its own requests?

Best /markus

On 03/21/2017 11:35 AM, Markus Wernig wrote:
> Hi all
> 
> I have configured Squid 4.0.18 (CentOS) with sslbump and clamav as
> ecap_service. This works well.
> 
> One thing I've noticed though, are constant log entries like this in
> access.log:
> 
> 2017-03-21 10:35:08.338 +0100 000137 - TCP_DENIED/403 3607 GET
> http://apps.identrust.com/roots/dstrootcax3.p7c - HIER_NONE/-
> text/html;charset=utf-8 -
> 2017-03-21 10:35:08.345 +0100 000161 10.254.254.2 NONE/200 0 CONNECT
> letsencrypt.org:443 - HIER_DIRECT/letsencrypt.org - -
> 
> It appears that this is the OCSP URI for Letsencrypt certificates.
> 
> And in fact every time this is logged, a CONNECT to a https uri is
> logged that is using a Letsencrypt certificate (like eg.
> https://letsencrypt.org).
> 
> Given that there is no client IP logged, I assume that squid is blocking
> its own outgoing OCSP request here (the browser is configured to NOT use
> OCSP).
> 
> The same seems to happen when there's no OCSP URI, but a regular AIA URI
> in the certificate:
> 
> 2017-03-21 10:36:19.773 +0100 000000 - TCP_DENIED/403 3734 GET
> http://swisssign.net/cgi-bin/authority/download/5B257B96A465517EB839F3C078665EE83AE7F0EE
> - HIER_NONE/- text/html;charset=utf-8 -
> 2017-03-21 10:36:19.782 +0100 000038 10.254.254.2 NONE/200 0 CONNECT
> swisssign.net:443 - HIER_DIRECT/swisssign.net - -
> 
> I do have "http_access allow localhost" in squid.conf, but since there's
> no IP associated with the request, this does not seem to help.
> 
> Is there a way to allow these outgoing internal requests? I've looked
> through the FAQ and wiki, but couldn't find anything on the topic.
> 
> Thanks & best
> 
> /markus
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
> 


-- 
Markus Wernig
Unix/Network Security Engineer
PGP: D9203D2A4AD9FC3333DEEF9DF7ACC6208E82E4DC
SIP/XMPP: markus at wernig.net
Furch D25-SR Cut - Ovation CE C2078AX-5
-----------------------------------------
http://xfer.ch - http://markus.wernig.net
-----------------------------------------




From heiler.bemerguy at cinbesa.com.br  Wed Mar 22 13:44:07 2017
From: heiler.bemerguy at cinbesa.com.br (Heiler Bemerguy)
Date: Wed, 22 Mar 2017 10:44:07 -0300
Subject: [squid-users] Skype via squid
Message-ID: <adeeac2f-3234-6717-d25b-66f6f6c0dc70@cinbesa.com.br>


Hi dude,

I've noticed our users are being blocked by a rule which prevents 
CONNECTs to IP addresses instead of FQDN.

What puzzles me is WHY skype is trying to connect to IPs even after 
connecting to FQDNs.. ? Have anyone noticed this? Any workaround apart 
from whitelisting Microsoft IPs...... ???

LOG:

1490189501.442    879 10.32.3.102 TCP_TUNNEL/200 8797 CONNECT 
login.live.com:443 - HIER_DIRECT/131.253.61.96 -
1490189502.241    740 10.32.3.102 TCP_TUNNEL/200 6160 CONNECT 
login.live.com:443 - HIER_DIRECT/131.253.61.66 -
1490189503.017    741 10.32.3.102 TCP_TUNNEL/200 13808 CONNECT 
login.live.com:443 - HIER_DIRECT/131.253.61.66 -
1490189510.193    729 10.32.3.102 TCP_TUNNEL/200 8784 CONNECT 
login.live.com:443 - HIER_DIRECT/131.253.61.66 -
1490189511.068    779 10.32.3.102 TCP_TUNNEL/200 6160 CONNECT 
login.live.com:443 - HIER_DIRECT/131.253.61.66 -
1490189512.162    763 10.32.3.102 TCP_TUNNEL/200 7376 CONNECT 
login.live.com:443 - HIER_DIRECT/131.253.61.66 -
1490189512.216    816 10.32.3.102 TCP_TUNNEL/200 7376 CONNECT 
login.live.com:443 - HIER_DIRECT/131.253.61.68 -
1490189517.077      1 10.32.3.102 NONE/503 0 CONNECT 157.55.130.146:443 
- HIER_NONE/- -
1490189525.321      1 10.32.3.102 NONE/503 0 CONNECT 64.4.23.160:443 - 
HIER_NONE/- -
1490189526.333      1 10.32.3.102 NONE/503 0 CONNECT 111.221.77.165:443 
- HIER_NONE/- -
1490189527.345      1 10.32.3.102 NONE/503 0 CONNECT 157.55.130.144:443 
- HIER_NONE/- -
1490189528.354      1 10.32.3.102 NONE/503 0 CONNECT 111.221.77.149:443 
- HIER_NONE/- -
1490189529.368      1 10.32.3.102 NONE/503 0 CONNECT 157.55.235.164:443 
- HIER_NONE/- -
1490189531.375      1 10.32.3.102 NONE/503 0 CONNECT 157.56.52.20:443 - 
HIER_NONE/- -
1490189532.385      1 10.32.3.102 NONE/503 0 CONNECT 65.55.223.31:443 - 
HIER_NONE/- -
1490189533.393      1 10.32.3.102 NONE/503 0 CONNECT 157.56.52.21:443 - 
HIER_NONE/- -


-- 
Atenciosamente / Best Regards,

Heiler Bemerguy
Network Manager - CINBESA
55 91 98151-4894/3184-1751



From rousskov at measurement-factory.com  Wed Mar 22 13:51:06 2017
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Wed, 22 Mar 2017 07:51:06 -0600
Subject: [squid-users] URL list from a URL
In-Reply-To: <000d01d2a2a1$aedd9fc0$0c98df40$@ngtech.co.il>
References: <895324221.1881.1490111566009.JavaMail.zimbra@tresgeek.net>
 <1f441f69-102e-7855-4ac9-758227a76682@gmail.com>
 <1138189488.3918.1490128216177.JavaMail.zimbra@tresgeek.net>
 <705ad8c6-fa21-dd5f-7419-45d07c1284d9@measurement-factory.com>
 <000d01d2a2a1$aedd9fc0$0c98df40$@ngtech.co.il>
Message-ID: <36106225-78b6-03f3-28fd-31c2cd0b0211@measurement-factory.com>

On 03/21/2017 06:17 PM, Eliezer Croitoru wrote:
> The current StoreID helper can be converted pretty fast into what he needs.

Jason needs to block access. How can a [converted] StoreID helper block
access without becoming an external_acl helper?

Alex.

> -----Original Message-----
> From: squid-users [mailto:squid-users-bounces at lists.squid-cache.org] On Behalf Of Alex Rousskov
> Sent: Tuesday, March 21, 2017 11:43 PM
> To: squid-users at lists.squid-cache.org
> Subject: Re: [squid-users] URL list from a URL
> 
> On 03/21/2017 02:30 PM, Jason B. Nance wrote:
> 
>> I should have mentioned that I'm not caching, I'm only using Squid
>> for whitelisting in this case.  Would you still say this is the right
>> path? 
> 
> No. You probably have two better options:
> 
> 1. Use a file with list of mirror URLs as an ACL parameter. Write a
> script that updates that file and reconfigures Squid as needed. Please
> keep in mind that Squid reconfiguration is currently a relatively
> heavy/intrusive operation, even if there were not changes except for
> that single ACL.
> 
> 2. Write an external_acl helper that will consult the mirror list. This
> will make each HTTP transaction a little slower (because it needs to go
> to the helper) but eliminates reconfigurations. The helper itself or
> some other script will still need to update the mirror list as needed,
> of course.
> 
> 
> HTH,
> 
> Alex.
> 
> 
> 
> 
>>> Hello,
>>>
>>> I'm using Squid 3.5.20 and wonder if it is possible to define an ACL which retrieves the list of URLs from another URL (similar to pointing to a file).  In this specific use case it is to allow a Foreman server to sync Yum content from the CentOS mirrors.  I tell Foreman to use the following URL:
>>>
>>> http://mirrorlist.centos.org/?release=7&arch=x86_64&repo=updates
>>>
>>> Which returns a list of URLs, such as:
>>>
>>> http://repo1.dal.innoscale.net/centos/7.3.1611/updates/x86_64/
>>> http://linux.mirrors.es.net/centos/7.3.1611/updates/x86_64/
>>> http://reflector.westga.edu/repos/CentOS/7.3.1611/updates/x86_64/
>>> http://mirror.jax.hugeserver.com/centos/7.3.1611/updates/x86_64/
>>> http://ftp.linux.ncsu.edu/pub/CentOS/7.3.1611/updates/x86_64/
>>> http://mirror.nexcess.net/CentOS/7.3.1611/updates/x86_64/
>>> http://mirror.web-ster.com/centos/7.3.1611/updates/x86_64/
>>> http://centos.host-engine.com/7.3.1611/updates/x86_64/
>>> http://mirror.raystedman.net/centos/7.3.1611/updates/x86_64/
>>> http://mirror.linux.duke.edu/pub/centos/7.3.1611/updates/x86_64/
>>>
>>> Foreman then starts a new HTTP connection (not a redirect) to attempt to connect to those in turn until it works.
>>>
>>> So I would like to configure Squid to allow the Foreman server access to any of those URLs (the list changes somewhat often).
>>>
>>> I started to go down the external_acl_type but am wondering if I'm missing something obvious.
> 
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
> 



From eduardoocarneiro at gmail.com  Wed Mar 22 14:35:27 2017
From: eduardoocarneiro at gmail.com (Eduardo Carneiro)
Date: Wed, 22 Mar 2017 07:35:27 -0700 (PDT)
Subject: [squid-users] Free Squid helper for dynamic content caching
In-Reply-To: <1568304621.2140.1427826316687.JavaMail.root@hscbrasil.com.br>
References: <1568304621.2140.1427826316687.JavaMail.root@hscbrasil.com.br>
Message-ID: <1490193327083-4681896.post@n4.nabble.com>

I have been using this helper for a while. It works very well.
Congratulations!

But I noticed that after squid 3.5.19, this helper doesn't work anymore. Is
this a known problem? Is there any way to fix this?

Best regards.
Eduardo Carneiro



--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/Free-Squid-helper-for-dynamic-content-caching-tp4670617p4681896.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From jason at tresgeek.net  Wed Mar 22 14:43:45 2017
From: jason at tresgeek.net (Jason B. Nance)
Date: Wed, 22 Mar 2017 09:43:45 -0500 (CDT)
Subject: [squid-users] URL list from a URL
In-Reply-To: <705ad8c6-fa21-dd5f-7419-45d07c1284d9@measurement-factory.com>
References: <895324221.1881.1490111566009.JavaMail.zimbra@tresgeek.net>
 <1f441f69-102e-7855-4ac9-758227a76682@gmail.com>
 <1138189488.3918.1490128216177.JavaMail.zimbra@tresgeek.net>
 <705ad8c6-fa21-dd5f-7419-45d07c1284d9@measurement-factory.com>
Message-ID: <360847457.1266.1490193825375.JavaMail.zimbra@tresgeek.net>

Thank you.  In regards to the external ACL helper, would I basically say something like:

external_acl_type foo %URL /path/to/my/helper

And then have my helper compare the contents of %URL to the mirror list and spit out "OK" for pass or "ERR" for fail?

Thanks,

j


----- Original Message -----
From: "Alex Rousskov" <rousskov at measurement-factory.com>
To: squid-users at lists.squid-cache.org
Cc: "Jason Nance" <jason at tresgeek.net>
Sent: Tuesday, March 21, 2017 4:42:33 PM
Subject: Re: [squid-users] URL list from a URL

On 03/21/2017 02:30 PM, Jason B. Nance wrote:

> I should have mentioned that I'm not caching, I'm only using Squid
> for whitelisting in this case.  Would you still say this is the right
> path? 

No. You probably have two better options:

1. Use a file with list of mirror URLs as an ACL parameter. Write a
script that updates that file and reconfigures Squid as needed. Please
keep in mind that Squid reconfiguration is currently a relatively
heavy/intrusive operation, even if there were not changes except for
that single ACL.

2. Write an external_acl helper that will consult the mirror list. This
will make each HTTP transaction a little slower (because it needs to go
to the helper) but eliminates reconfigurations. The helper itself or
some other script will still need to update the mirror list as needed,
of course.


HTH,

Alex.




>> Hello,
>>
>> I'm using Squid 3.5.20 and wonder if it is possible to define an ACL which retrieves the list of URLs from another URL (similar to pointing to a file).  In this specific use case it is to allow a Foreman server to sync Yum content from the CentOS mirrors.  I tell Foreman to use the following URL:
>>
>> http://mirrorlist.centos.org/?release=7&arch=x86_64&repo=updates
>>
>> Which returns a list of URLs, such as:
>>
>> http://repo1.dal.innoscale.net/centos/7.3.1611/updates/x86_64/
>> http://linux.mirrors.es.net/centos/7.3.1611/updates/x86_64/
>> http://reflector.westga.edu/repos/CentOS/7.3.1611/updates/x86_64/
>> http://mirror.jax.hugeserver.com/centos/7.3.1611/updates/x86_64/
>> http://ftp.linux.ncsu.edu/pub/CentOS/7.3.1611/updates/x86_64/
>> http://mirror.nexcess.net/CentOS/7.3.1611/updates/x86_64/
>> http://mirror.web-ster.com/centos/7.3.1611/updates/x86_64/
>> http://centos.host-engine.com/7.3.1611/updates/x86_64/
>> http://mirror.raystedman.net/centos/7.3.1611/updates/x86_64/
>> http://mirror.linux.duke.edu/pub/centos/7.3.1611/updates/x86_64/
>>
>> Foreman then starts a new HTTP connection (not a redirect) to attempt to connect to those in turn until it works.
>>
>> So I would like to configure Squid to allow the Foreman server access to any of those URLs (the list changes somewhat often).
>>
>> I started to go down the external_acl_type but am wondering if I'm missing something obvious.


From yvoinov at gmail.com  Wed Mar 22 15:08:33 2017
From: yvoinov at gmail.com (Yuri Voinov)
Date: Wed, 22 Mar 2017 21:08:33 +0600
Subject: [squid-users] Free Squid helper for dynamic content caching
In-Reply-To: <1490193327083-4681896.post@n4.nabble.com>
References: <1568304621.2140.1427826316687.JavaMail.root@hscbrasil.com.br>
 <1490193327083-4681896.post@n4.nabble.com>
Message-ID: <83a66d29-e45f-367f-80e9-a384aec8d354@gmail.com>

Things are changed in the web on regular basis. Nothing permanent in the
world.

So, store ID rules lost relevance and no longer work.

You can fix it yourself, the code is open.


22.03.2017 20:35, Eduardo Carneiro ?????:
> I have been using this helper for a while. It works very well.
> Congratulations!
>
> But I noticed that after squid 3.5.19, this helper doesn't work anymore. Is
> this a known problem? Is there any way to fix this?
>
> Best regards.
> Eduardo Carneiro
>
>
>
> --
> View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/Free-Squid-helper-for-dynamic-content-caching-tp4670617p4681896.html
> Sent from the Squid - Users mailing list archive at Nabble.com.
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users

-- 
Bugs to the Future
-------------- next part --------------
A non-text attachment was scrubbed...
Name: 0x613DEC46.asc
Type: application/pgp-keys
Size: 2437 bytes
Desc: not available
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20170322/075a55c4/attachment.key>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: signature.asc
Type: application/pgp-signature
Size: 473 bytes
Desc: OpenPGP digital signature
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20170322/075a55c4/attachment.sig>

From rousskov at measurement-factory.com  Wed Mar 22 15:12:57 2017
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Wed, 22 Mar 2017 09:12:57 -0600
Subject: [squid-users] Squid blocking own OCSP/AIA requests
In-Reply-To: <9a72eb04-124c-3222-d7b5-56d9cd2406bf@wernig.net>
References: <c2bbc299-849b-063b-1ed4-6b9517ba987c@wernig.net>
 <9a72eb04-124c-3222-d7b5-56d9cd2406bf@wernig.net>
Message-ID: <60ec9956-4267-1a60-2f80-ae5924aaee8a@measurement-factory.com>

On 03/22/2017 07:20 AM, Markus Wernig wrote:
> Small update:
> 
> - The URL ... is the AIA for the Root CA
> 
> Since squid is sslbumping the connection, it must be doing the AIA
> lookups (presumably for SSL verification). Does anybody have an idea why
> it is blocking its own requests?

My answer has not changed:

http://lists.squid-cache.org/pipermail/squid-users/2017-March/014773.html

Did you expect your update to change that answer?

Alex.



From cheemeng at function.com  Wed Mar 22 15:17:32 2017
From: cheemeng at function.com (Chee M Gui)
Date: Wed, 22 Mar 2017 08:17:32 -0700
Subject: [squid-users] Squid stopped working after cache.log and access.log
	rotation
Message-ID: <CAMyAatDzxB=+zNcB-jvc8vb+hse44Ne80tiaNUHgT_yQgM1iPg@mail.gmail.com>

Hi All

We recently installed Squid 3.5.12-1ubuntu7.3 on Ubuntu 16.04.2 LTS.  It
ran fine at first but stopped working after a while.   telnet server 3128
still works, i.e., opens a blank window, but Squid is just not accepting
requests.    Then we realized that there is no new access.log file.   The
access.log file stopped rotated at 6:24AM on 3/17/2017.    It looks like
Squid wasn't able to create a new access.log?  We could not find any error
message in syslog or the cache.log.      We haven't rebooted the server
because we want to know what went wrong.   It isn't the firewall blocking
Squid because Squid was working fine all the while until recently.  Also
after it stopped working, we disabled the firewall to see if it would work
but it still didn't work.

root at paproxy:/var/log/squid# ls -alt
total 15536
drwxr-xr-x 2 proxy proxy      4096 Mar 21 06:25 .
-rw-r----- 1 proxy proxy        63 Mar 21 06:25 cache.log
drwxrwxr-x 9 root  syslog     4096 Mar 21 06:25 ..
-rw-r----- 1 proxy proxy        63 Mar 20 06:25 cache.log.1
-rw-r----- 1 proxy proxy        83 Mar 19 06:25 cache.log.2.gz
-rw-r----- 1 proxy proxy  15759111 Mar 17 06:24 access.log.1
-rw-r----- 1 proxy proxy    117223 Mar 17 05:52 netdb.state

Any ideas what went wrong?

Thank you very much in anticipation.

Gui
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20170322/f0c065e7/attachment.htm>

From Antony.Stone at squid.open.source.it  Wed Mar 22 15:21:00 2017
From: Antony.Stone at squid.open.source.it (Antony Stone)
Date: Wed, 22 Mar 2017 16:21:00 +0100
Subject: [squid-users] Squid stopped working after cache.log and
	access.log rotation
In-Reply-To: <CAMyAatDzxB=+zNcB-jvc8vb+hse44Ne80tiaNUHgT_yQgM1iPg@mail.gmail.com>
References: <CAMyAatDzxB=+zNcB-jvc8vb+hse44Ne80tiaNUHgT_yQgM1iPg@mail.gmail.com>
Message-ID: <201703221621.00502.Antony.Stone@squid.open.source.it>

On Wednesday 22 March 2017 at 16:17:32, Chee M Gui wrote:

> Hi All
> 
> We recently installed Squid 3.5.12-1ubuntu7.3 on Ubuntu 16.04.2 LTS.  It
> ran fine at first but stopped working after a while.   telnet server 3128
> still works, i.e., opens a blank window, but Squid is just not accepting
> requests.    Then we realized that there is no new access.log file.   The
> access.log file stopped rotated at 6:24AM on 3/17/2017.    It looks like
> Squid wasn't able to create a new access.log?  We could not find any error
> message in syslog or the cache.log.      We haven't rebooted the server
> because we want to know what went wrong.   It isn't the firewall blocking
> Squid because Squid was working fine all the while until recently.  Also
> after it stopped working, we disabled the firewall to see if it would work
> but it still didn't work.
> 
> root at paproxy:/var/log/squid# ls -alt
> total 15536
> drwxr-xr-x 2 proxy proxy      4096 Mar 21 06:25 .
> -rw-r----- 1 proxy proxy        63 Mar 21 06:25 cache.log
> drwxrwxr-x 9 root  syslog     4096 Mar 21 06:25 ..
> -rw-r----- 1 proxy proxy        63 Mar 20 06:25 cache.log.1
> -rw-r----- 1 proxy proxy        83 Mar 19 06:25 cache.log.2.gz
> -rw-r----- 1 proxy proxy  15759111 Mar 17 06:24 access.log.1
> -rw-r----- 1 proxy proxy    117223 Mar 17 05:52 netdb.state
> 
> Any ideas what went wrong?

Any chance you've run out of disk space?


Antony.

-- 
I bought a book about anti-gravity.  The reviews say you can't put it down.

                                                   Please reply to the list;
                                                         please *don't* CC me.


From jason at tresgeek.net  Wed Mar 22 15:23:09 2017
From: jason at tresgeek.net (Jason B. Nance)
Date: Wed, 22 Mar 2017 10:23:09 -0500 (CDT)
Subject: [squid-users] Squid stopped working after cache.log and
 access.log	rotation
In-Reply-To: <CAMyAatDzxB=+zNcB-jvc8vb+hse44Ne80tiaNUHgT_yQgM1iPg@mail.gmail.com>
References: <CAMyAatDzxB=+zNcB-jvc8vb+hse44Ne80tiaNUHgT_yQgM1iPg@mail.gmail.com>
Message-ID: <1548158039.1456.1490196189795.JavaMail.zimbra@tresgeek.net>

If you do "lsof /var/log | grep -i delete" does it show squid writing to a deleted access.log / cache.log? 

j 



From: "Chee M Gui" <cheemeng at function.com> 
To: squid-users at lists.squid-cache.org 
Sent: Wednesday, March 22, 2017 10:17:32 AM 
Subject: [squid-users] Squid stopped working after cache.log and access.log rotation 


Hi All 

We recently installed Squid 3.5.12-1ubuntu7.3 on Ubuntu 16.04.2 LTS. It ran fine at first but stopped working after a while. telnet server 3128 still works, i.e., opens a blank window, but Squid is just not accepting requests. Then we realized that there is no new access.log file. The access.log file stopped rotated at 6:24AM on 3/17/2017. It looks like Squid wasn't able to create a new access.log? We could not find any error message in syslog or the cache.log. We haven't rebooted the server because we want to know what went wrong. It isn't the firewall blocking Squid because Squid was working fine all the while until recently. Also after it stopped working, we disabled the firewall to see if it would work but it still didn't work. 

root at paproxy:/var/log/squid# ls -alt 
total 15536 
drwxr-xr-x 2 proxy proxy 4096 Mar 21 06:25 . 
-rw-r----- 1 proxy proxy 63 Mar 21 06:25 cache.log 
drwxrwxr-x 9 root syslog 4096 Mar 21 06:25 .. 
-rw-r----- 1 proxy proxy 63 Mar 20 06:25 cache.log.1 
-rw-r----- 1 proxy proxy 83 Mar 19 06:25 cache.log.2.gz 
-rw-r----- 1 proxy proxy 15759111 Mar 17 06:24 access.log.1 
-rw-r----- 1 proxy proxy 117223 Mar 17 05:52 netdb.state 

Any ideas what went wrong? 

Thank you very much in anticipation. 

Gui 

_______________________________________________ 
squid-users mailing list 
squid-users at lists.squid-cache.org 
http://lists.squid-cache.org/listinfo/squid-users 
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20170322/24cdd50f/attachment.htm>

From eliezer at ngtech.co.il  Wed Mar 22 15:25:36 2017
From: eliezer at ngtech.co.il (Eliezer  Croitoru)
Date: Wed, 22 Mar 2017 17:25:36 +0200
Subject: [squid-users] URL list from a URL
In-Reply-To: <36106225-78b6-03f3-28fd-31c2cd0b0211@measurement-factory.com>
References: <895324221.1881.1490111566009.JavaMail.zimbra@tresgeek.net>
 <1f441f69-102e-7855-4ac9-758227a76682@gmail.com>
 <1138189488.3918.1490128216177.JavaMail.zimbra@tresgeek.net>
 <705ad8c6-fa21-dd5f-7419-45d07c1284d9@measurement-factory.com>
 <000d01d2a2a1$aedd9fc0$0c98df40$@ngtech.co.il>
 <36106225-78b6-03f3-28fd-31c2cd0b0211@measurement-factory.com>
Message-ID: <00b001d2a320$8e289170$aa79b450$@ngtech.co.il>

Hey Alex,

I didn't meant by convert the StoreID helper to convert it into an external_acl helper....
It has both OK and ERR and a "checklist" which would be a match or not.

Eliezer

----
Eliezer Croitoru
Linux System Administrator
Mobile: +972-5-28704261
Email: eliezer at ngtech.co.il



-----Original Message-----
From: Alex Rousskov [mailto:rousskov at measurement-factory.com] 
Sent: Wednesday, March 22, 2017 3:51 PM
To: squid-users at lists.squid-cache.org
Cc: Eliezer Croitoru <eliezer at ngtech.co.il>
Subject: Re: [squid-users] URL list from a URL

On 03/21/2017 06:17 PM, Eliezer Croitoru wrote:
> The current StoreID helper can be converted pretty fast into what he needs.

Jason needs to block access. How can a [converted] StoreID helper block
access without becoming an external_acl helper?

Alex.

> -----Original Message-----
> From: squid-users [mailto:squid-users-bounces at lists.squid-cache.org] On Behalf Of Alex Rousskov
> Sent: Tuesday, March 21, 2017 11:43 PM
> To: squid-users at lists.squid-cache.org
> Subject: Re: [squid-users] URL list from a URL
> 
> On 03/21/2017 02:30 PM, Jason B. Nance wrote:
> 
>> I should have mentioned that I'm not caching, I'm only using Squid
>> for whitelisting in this case.  Would you still say this is the right
>> path? 
> 
> No. You probably have two better options:
> 
> 1. Use a file with list of mirror URLs as an ACL parameter. Write a
> script that updates that file and reconfigures Squid as needed. Please
> keep in mind that Squid reconfiguration is currently a relatively
> heavy/intrusive operation, even if there were not changes except for
> that single ACL.
> 
> 2. Write an external_acl helper that will consult the mirror list. This
> will make each HTTP transaction a little slower (because it needs to go
> to the helper) but eliminates reconfigurations. The helper itself or
> some other script will still need to update the mirror list as needed,
> of course.
> 
> 
> HTH,
> 
> Alex.
> 
> 
> 
> 
>>> Hello,
>>>
>>> I'm using Squid 3.5.20 and wonder if it is possible to define an ACL which retrieves the list of URLs from another URL (similar to pointing to a file).  In this specific use case it is to allow a Foreman server to sync Yum content from the CentOS mirrors.  I tell Foreman to use the following URL:
>>>
>>> http://mirrorlist.centos.org/?release=7&arch=x86_64&repo=updates
>>>
>>> Which returns a list of URLs, such as:
>>>
>>> http://repo1.dal.innoscale.net/centos/7.3.1611/updates/x86_64/
>>> http://linux.mirrors.es.net/centos/7.3.1611/updates/x86_64/
>>> http://reflector.westga.edu/repos/CentOS/7.3.1611/updates/x86_64/
>>> http://mirror.jax.hugeserver.com/centos/7.3.1611/updates/x86_64/
>>> http://ftp.linux.ncsu.edu/pub/CentOS/7.3.1611/updates/x86_64/
>>> http://mirror.nexcess.net/CentOS/7.3.1611/updates/x86_64/
>>> http://mirror.web-ster.com/centos/7.3.1611/updates/x86_64/
>>> http://centos.host-engine.com/7.3.1611/updates/x86_64/
>>> http://mirror.raystedman.net/centos/7.3.1611/updates/x86_64/
>>> http://mirror.linux.duke.edu/pub/centos/7.3.1611/updates/x86_64/
>>>
>>> Foreman then starts a new HTTP connection (not a redirect) to attempt to connect to those in turn until it works.
>>>
>>> So I would like to configure Squid to allow the Foreman server access to any of those URLs (the list changes somewhat often).
>>>
>>> I started to go down the external_acl_type but am wondering if I'm missing something obvious.
> 
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
> 




From squid3 at treenet.co.nz  Wed Mar 22 15:37:54 2017
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 23 Mar 2017 04:37:54 +1300
Subject: [squid-users] Skype via squid
In-Reply-To: <adeeac2f-3234-6717-d25b-66f6f6c0dc70@cinbesa.com.br>
References: <adeeac2f-3234-6717-d25b-66f6f6c0dc70@cinbesa.com.br>
Message-ID: <64b91f7d-592e-7df4-45f5-1b0af85acdf0@treenet.co.nz>

On 23/03/2017 2:44 a.m., Heiler Bemerguy wrote:
> 
> Hi dude,
> 
> I've noticed our users are being blocked by a rule which prevents
> CONNECTs to IP addresses instead of FQDN.
> 
> What puzzles me is WHY skype is trying to connect to IPs even after
> connecting to FQDNs.. ? Have anyone noticed this? Any workaround apart
> from whitelisting Microsoft IPs...... ???

This has always been the case. Skype was originally a P2P application,
since end users normally do not have custom reverse-DNS entries for
personal domain names (and Skype no easy way to reliably find out even
if they do) those usually auto-negotiates its data connections using
raw-IP to bust their way through NAT breakages, then uses the results.

The domain name part at the beginning is a much more recent addition by
MS to use their generic live.co autenticaion APIs, and not actually a
part unique to Skype itself.


That is why the wiki config example combines a regex ACL for matching
raw-IP values in the URL, and the User-Agent header detection.

Amos



From eduardoocarneiro at gmail.com  Wed Mar 22 15:38:40 2017
From: eduardoocarneiro at gmail.com (Eduardo Carneiro)
Date: Wed, 22 Mar 2017 08:38:40 -0700 (PDT)
Subject: [squid-users] Free Squid helper for dynamic content caching
In-Reply-To: <83a66d29-e45f-367f-80e9-a384aec8d354@gmail.com>
References: <1568304621.2140.1427826316687.JavaMail.root@hscbrasil.com.br>
 <1490193327083-4681896.post@n4.nabble.com>
 <83a66d29-e45f-367f-80e9-a384aec8d354@gmail.com>
Message-ID: <1490197120728-4681905.post@n4.nabble.com>

Hi Yuri.

The reason I came here is because I've already tried but I didn't succeed. I
really expected a more specific answer. Not just "You can fix it yourself,
the code is open."

Anyway, thanks. I'll Keep trying to fix this.



--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/Free-Squid-helper-for-dynamic-content-caching-tp4670617p4681905.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From yvoinov at gmail.com  Wed Mar 22 15:53:39 2017
From: yvoinov at gmail.com (Yuri Voinov)
Date: Wed, 22 Mar 2017 21:53:39 +0600
Subject: [squid-users] Free Squid helper for dynamic content caching
In-Reply-To: <1490197120728-4681905.post@n4.nabble.com>
References: <1568304621.2140.1427826316687.JavaMail.root@hscbrasil.com.br>
 <1490193327083-4681896.post@n4.nabble.com>
 <83a66d29-e45f-367f-80e9-a384aec8d354@gmail.com>
 <1490197120728-4681905.post@n4.nabble.com>
Message-ID: <2c43d869-4398-b512-9433-610ef135c5be@gmail.com>

I'm afraid that rewriting the rules is a big job. I strongly doubt that
someone will lay it out in public open access for free. Saving traffic
is money.

Here's what I want to say. There are no really effective helpers in the
public domain. As I know.

22.03.2017 21:38, Eduardo Carneiro ?????:
> Hi Yuri.
>
> The reason I came here is because I've already tried but I didn't succeed. I
> really expected a more specific answer. Not just "You can fix it yourself,
> the code is open."
>
> Anyway, thanks. I'll Keep trying to fix this.
>
>
>
> --
> View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/Free-Squid-helper-for-dynamic-content-caching-tp4670617p4681905.html
> Sent from the Squid - Users mailing list archive at Nabble.com.
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users

-- 
Bugs to the Future
-------------- next part --------------
A non-text attachment was scrubbed...
Name: 0x613DEC46.asc
Type: application/pgp-keys
Size: 2437 bytes
Desc: not available
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20170322/1109b599/attachment.key>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: signature.asc
Type: application/pgp-signature
Size: 473 bytes
Desc: OpenPGP digital signature
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20170322/1109b599/attachment.sig>

From rousskov at measurement-factory.com  Wed Mar 22 16:10:07 2017
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Wed, 22 Mar 2017 10:10:07 -0600
Subject: [squid-users] URL list from a URL
In-Reply-To: <00b001d2a320$8e289170$aa79b450$@ngtech.co.il>
References: <895324221.1881.1490111566009.JavaMail.zimbra@tresgeek.net>
 <1f441f69-102e-7855-4ac9-758227a76682@gmail.com>
 <1138189488.3918.1490128216177.JavaMail.zimbra@tresgeek.net>
 <705ad8c6-fa21-dd5f-7419-45d07c1284d9@measurement-factory.com>
 <000d01d2a2a1$aedd9fc0$0c98df40$@ngtech.co.il>
 <36106225-78b6-03f3-28fd-31c2cd0b0211@measurement-factory.com>
 <00b001d2a320$8e289170$aa79b450$@ngtech.co.il>
Message-ID: <2a04f166-c2a0-26eb-3093-a7a3e0b865f9@measurement-factory.com>

On 03/22/2017 09:25 AM, Eliezer Croitoru wrote:

> I didn't meant by convert the StoreID helper to convert it into an external_acl helper....
> It has both OK and ERR and a "checklist" which would be a match or not.

Sigh. The combination of your answers does not make any sense to me.
Squid does not use StoreID helpers to block access, regardless of what a
StoreID helper returns. It is certainly possible to take some StoreID
helper code and make an external_acl helper out of it, but that falls
under my option #2.

Perhaps what you meant to say is something like "Use StoreID helper X
available at Y to implement option #2 -- that X code has everything you
need!"?

Alex.


> -----Original Message-----
> From: Alex Rousskov [mailto:rousskov at measurement-factory.com] 
> Sent: Wednesday, March 22, 2017 3:51 PM
> To: squid-users at lists.squid-cache.org
> Cc: Eliezer Croitoru <eliezer at ngtech.co.il>
> Subject: Re: [squid-users] URL list from a URL
> 
> On 03/21/2017 06:17 PM, Eliezer Croitoru wrote:
>> The current StoreID helper can be converted pretty fast into what he needs.
> 
> Jason needs to block access. How can a [converted] StoreID helper block
> access without becoming an external_acl helper?
> 
> Alex.
> 
>> -----Original Message-----
>> From: squid-users [mailto:squid-users-bounces at lists.squid-cache.org] On Behalf Of Alex Rousskov
>> Sent: Tuesday, March 21, 2017 11:43 PM
>> To: squid-users at lists.squid-cache.org
>> Subject: Re: [squid-users] URL list from a URL
>>
>> On 03/21/2017 02:30 PM, Jason B. Nance wrote:
>>
>>> I should have mentioned that I'm not caching, I'm only using Squid
>>> for whitelisting in this case.  Would you still say this is the right
>>> path? 
>>
>> No. You probably have two better options:
>>
>> 1. Use a file with list of mirror URLs as an ACL parameter. Write a
>> script that updates that file and reconfigures Squid as needed. Please
>> keep in mind that Squid reconfiguration is currently a relatively
>> heavy/intrusive operation, even if there were not changes except for
>> that single ACL.
>>
>> 2. Write an external_acl helper that will consult the mirror list. This
>> will make each HTTP transaction a little slower (because it needs to go
>> to the helper) but eliminates reconfigurations. The helper itself or
>> some other script will still need to update the mirror list as needed,
>> of course.
>>
>>
>> HTH,
>>
>> Alex.
>>
>>
>>
>>
>>>> Hello,
>>>>
>>>> I'm using Squid 3.5.20 and wonder if it is possible to define an ACL which retrieves the list of URLs from another URL (similar to pointing to a file).  In this specific use case it is to allow a Foreman server to sync Yum content from the CentOS mirrors.  I tell Foreman to use the following URL:
>>>>
>>>> http://mirrorlist.centos.org/?release=7&arch=x86_64&repo=updates
>>>>
>>>> Which returns a list of URLs, such as:
>>>>
>>>> http://repo1.dal.innoscale.net/centos/7.3.1611/updates/x86_64/
>>>> http://linux.mirrors.es.net/centos/7.3.1611/updates/x86_64/
>>>> http://reflector.westga.edu/repos/CentOS/7.3.1611/updates/x86_64/
>>>> http://mirror.jax.hugeserver.com/centos/7.3.1611/updates/x86_64/
>>>> http://ftp.linux.ncsu.edu/pub/CentOS/7.3.1611/updates/x86_64/
>>>> http://mirror.nexcess.net/CentOS/7.3.1611/updates/x86_64/
>>>> http://mirror.web-ster.com/centos/7.3.1611/updates/x86_64/
>>>> http://centos.host-engine.com/7.3.1611/updates/x86_64/
>>>> http://mirror.raystedman.net/centos/7.3.1611/updates/x86_64/
>>>> http://mirror.linux.duke.edu/pub/centos/7.3.1611/updates/x86_64/
>>>>
>>>> Foreman then starts a new HTTP connection (not a redirect) to attempt to connect to those in turn until it works.
>>>>
>>>> So I would like to configure Squid to allow the Foreman server access to any of those URLs (the list changes somewhat often).
>>>>
>>>> I started to go down the external_acl_type but am wondering if I'm missing something obvious.
>>
>> _______________________________________________
>> squid-users mailing list
>> squid-users at lists.squid-cache.org
>> http://lists.squid-cache.org/listinfo/squid-users
>>
> 



From eliezer at ngtech.co.il  Wed Mar 22 18:20:14 2017
From: eliezer at ngtech.co.il (Eliezer  Croitoru)
Date: Wed, 22 Mar 2017 20:20:14 +0200
Subject: [squid-users] URL list from a URL
In-Reply-To: <2a04f166-c2a0-26eb-3093-a7a3e0b865f9@measurement-factory.com>
References: <895324221.1881.1490111566009.JavaMail.zimbra@tresgeek.net>
 <1f441f69-102e-7855-4ac9-758227a76682@gmail.com>
 <1138189488.3918.1490128216177.JavaMail.zimbra@tresgeek.net>
 <705ad8c6-fa21-dd5f-7419-45d07c1284d9@measurement-factory.com>
 <000d01d2a2a1$aedd9fc0$0c98df40$@ngtech.co.il>
 <36106225-78b6-03f3-28fd-31c2cd0b0211@measurement-factory.com>
 <00b001d2a320$8e289170$aa79b450$@ngtech.co.il>
 <2a04f166-c2a0-26eb-3093-a7a3e0b865f9@measurement-factory.com>
Message-ID: <08aa01d2a338$f3a7eb50$daf7c1f0$@ngtech.co.il>

Almost, What I was talking about and meant was to take the current official StoreID helper written in perl and remove the parts of the ID replacement while leaving the regex matching intact.
Then pass to the StoreID helper the url and the source IP and let it match the url to the regex from the list in the "DB file".
If it matches either return OK or ERR depends on the intention of the helper.
...The X code have almost everything needed so just convert it.
I am offering to convert it if there is some interest in it.

Eliezer

----
Eliezer Croitoru
Linux System Administrator
Mobile: +972-5-28704261
Email: eliezer at ngtech.co.il



-----Original Message-----
From: squid-users [mailto:squid-users-bounces at lists.squid-cache.org] On Behalf Of Alex Rousskov
Sent: Wednesday, March 22, 2017 6:10 PM
To: squid-users at lists.squid-cache.org
Subject: Re: [squid-users] URL list from a URL

On 03/22/2017 09:25 AM, Eliezer Croitoru wrote:

> I didn't meant by convert the StoreID helper to convert it into an external_acl helper....
> It has both OK and ERR and a "checklist" which would be a match or not.

Sigh. The combination of your answers does not make any sense to me.
Squid does not use StoreID helpers to block access, regardless of what a StoreID helper returns. It is certainly possible to take some StoreID helper code and make an external_acl helper out of it, but that falls under my option #2.

Perhaps what you meant to say is something like "Use StoreID helper X available at Y to implement option #2 -- that X code has everything you need!"?

Alex.


> -----Original Message-----
> From: Alex Rousskov [mailto:rousskov at measurement-factory.com]
> Sent: Wednesday, March 22, 2017 3:51 PM
> To: squid-users at lists.squid-cache.org
> Cc: Eliezer Croitoru <eliezer at ngtech.co.il>
> Subject: Re: [squid-users] URL list from a URL
> 
> On 03/21/2017 06:17 PM, Eliezer Croitoru wrote:
>> The current StoreID helper can be converted pretty fast into what he needs.
> 
> Jason needs to block access. How can a [converted] StoreID helper 
> block access without becoming an external_acl helper?
> 
> Alex.
> 
>> -----Original Message-----
>> From: squid-users [mailto:squid-users-bounces at lists.squid-cache.org] 
>> On Behalf Of Alex Rousskov
>> Sent: Tuesday, March 21, 2017 11:43 PM
>> To: squid-users at lists.squid-cache.org
>> Subject: Re: [squid-users] URL list from a URL
>>
>> On 03/21/2017 02:30 PM, Jason B. Nance wrote:
>>
>>> I should have mentioned that I'm not caching, I'm only using Squid 
>>> for whitelisting in this case.  Would you still say this is the 
>>> right path?
>>
>> No. You probably have two better options:
>>
>> 1. Use a file with list of mirror URLs as an ACL parameter. Write a 
>> script that updates that file and reconfigures Squid as needed. 
>> Please keep in mind that Squid reconfiguration is currently a 
>> relatively heavy/intrusive operation, even if there were not changes 
>> except for that single ACL.
>>
>> 2. Write an external_acl helper that will consult the mirror list. 
>> This will make each HTTP transaction a little slower (because it 
>> needs to go to the helper) but eliminates reconfigurations. The 
>> helper itself or some other script will still need to update the 
>> mirror list as needed, of course.
>>
>>
>> HTH,
>>
>> Alex.
>>
>>
>>
>>
>>>> Hello,
>>>>
>>>> I'm using Squid 3.5.20 and wonder if it is possible to define an ACL which retrieves the list of URLs from another URL (similar to pointing to a file).  In this specific use case it is to allow a Foreman server to sync Yum content from the CentOS mirrors.  I tell Foreman to use the following URL:
>>>>
>>>> http://mirrorlist.centos.org/?release=7&arch=x86_64&repo=updates
>>>>
>>>> Which returns a list of URLs, such as:
>>>>
>>>> http://repo1.dal.innoscale.net/centos/7.3.1611/updates/x86_64/
>>>> http://linux.mirrors.es.net/centos/7.3.1611/updates/x86_64/
>>>> http://reflector.westga.edu/repos/CentOS/7.3.1611/updates/x86_64/
>>>> http://mirror.jax.hugeserver.com/centos/7.3.1611/updates/x86_64/
>>>> http://ftp.linux.ncsu.edu/pub/CentOS/7.3.1611/updates/x86_64/
>>>> http://mirror.nexcess.net/CentOS/7.3.1611/updates/x86_64/
>>>> http://mirror.web-ster.com/centos/7.3.1611/updates/x86_64/
>>>> http://centos.host-engine.com/7.3.1611/updates/x86_64/
>>>> http://mirror.raystedman.net/centos/7.3.1611/updates/x86_64/
>>>> http://mirror.linux.duke.edu/pub/centos/7.3.1611/updates/x86_64/
>>>>
>>>> Foreman then starts a new HTTP connection (not a redirect) to attempt to connect to those in turn until it works.
>>>>
>>>> So I would like to configure Squid to allow the Foreman server access to any of those URLs (the list changes somewhat often).
>>>>
>>>> I started to go down the external_acl_type but am wondering if I'm missing something obvious.
>>
>> _______________________________________________
>> squid-users mailing list
>> squid-users at lists.squid-cache.org
>> http://lists.squid-cache.org/listinfo/squid-users
>>
> 

_______________________________________________
squid-users mailing list
squid-users at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-users



From cheemeng at function.com  Thu Mar 23 16:36:01 2017
From: cheemeng at function.com (Chee M Gui)
Date: Thu, 23 Mar 2017 09:36:01 -0700
Subject: [squid-users] Squid stopped working after cache.log and
 access.log rotation
In-Reply-To: <1548158039.1456.1490196189795.JavaMail.zimbra@tresgeek.net>
References: <CAMyAatDzxB=+zNcB-jvc8vb+hse44Ne80tiaNUHgT_yQgM1iPg@mail.gmail.com>
 <1548158039.1456.1490196189795.JavaMail.zimbra@tresgeek.net>
Message-ID: <CAMyAatABvrhXkY-JefdxK2L1c1MbDV8BoufRYnN9w2_5EQVkYA@mail.gmail.com>

Thank you Jason.

lsof /var/log | grep -i delete  does not show anything
Likewise for  lsof /var/log/squid | grep -i delete

Thank you Antony
No, we have a lot of disk space.



root at paproxy:/var/log# lsof /var/log/squid | grep -i delete
root at paproxy:/var/log# lsof /var/log/squid | more
root at paproxy:/var/log# ps -ax | grep squid
 1708 pts/0    S+     0:00 grep --color=auto squid
25482 ?        Ss     0:00 /usr/sbin/squid -YC -f /etc/squid/squid.conf
25484 ?        S      2:27 (squid-1) -YC -f /etc/squid/squid.conf
25485 ?        S      0:03 (logfile-daemon) /var/log/squid/access.log
root at paproxy:/var/log# lsof /var/log/squid | grep -i delete
root at paproxy:/var/log# cd squid
root at paproxy:/var/log/squid# ls -al
total 15540
drwxr-xr-x 2 proxy proxy      4096 Mar 23 06:25 .
drwxrwxr-x 9 root  syslog     4096 Mar 23 06:25 ..
-rw-r----- 1 proxy proxy  15759111 Mar 17 06:24 access.log.1
-rw-r----- 1 proxy proxy        63 Mar 23 06:25 cache.log
-rw-r----- 1 proxy proxy        63 Mar 22 06:25 cache.log.1
-rw-r----- 1 proxy proxy        63 Mar 19 06:25 cache.log.2
-rw-r----- 1 proxy proxy        83 Mar 21 06:25 cache.log.2.gz
-rw-r----- 1 proxy proxy    117223 Mar 17 05:52 netdb.state
root at paproxy:/var/log/squid#


On Wed, Mar 22, 2017 at 8:23 AM, Jason B. Nance <jason at tresgeek.net> wrote:

> If you do "lsof /var/log | grep -i delete" does it show squid writing to a
> deleted access.log / cache.log?
>
> j
>
>
> ------------------------------
> *From: *"Chee M Gui" <cheemeng at function.com>
> *To: *squid-users at lists.squid-cache.org
> *Sent: *Wednesday, March 22, 2017 10:17:32 AM
> *Subject: *[squid-users] Squid stopped working after cache.log and
> access.log        rotation
>
>
> Hi All
>
> We recently installed Squid 3.5.12-1ubuntu7.3 on Ubuntu 16.04.2 LTS.  It
> ran fine at first but stopped working after a while.   telnet server 3128
> still works, i.e., opens a blank window, but Squid is just not accepting
> requests.    Then we realized that there is no new access.log file.   The
> access.log file stopped rotated at 6:24AM on 3/17/2017.    It looks like
> Squid wasn't able to create a new access.log?  We could not find any error
> message in syslog or the cache.log.      We haven't rebooted the server
> because we want to know what went wrong.   It isn't the firewall blocking
> Squid because Squid was working fine all the while until recently.  Also
> after it stopped working, we disabled the firewall to see if it would work
> but it still didn't work.
>
> root at paproxy:/var/log/squid# ls -alt
> total 15536
> drwxr-xr-x 2 proxy proxy      4096 Mar 21 06:25 .
> -rw-r----- 1 proxy proxy        63 Mar 21 06:25 cache.log
> drwxrwxr-x 9 root  syslog     4096 Mar 21 06:25 ..
> -rw-r----- 1 proxy proxy        63 Mar 20 06:25 cache.log.1
> -rw-r----- 1 proxy proxy        83 Mar 19 06:25 cache.log.2.gz
> -rw-r----- 1 proxy proxy  15759111 Mar 17 06:24 access.log.1
> -rw-r----- 1 proxy proxy    117223 Mar 17 05:52 netdb.state
>
> Any ideas what went wrong?
>
> Thank you very much in anticipation.
>
> Gui
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20170323/b881154c/attachment.htm>

From Antony.Stone at squid.open.source.it  Thu Mar 23 18:49:48 2017
From: Antony.Stone at squid.open.source.it (Antony Stone)
Date: Thu, 23 Mar 2017 19:49:48 +0100
Subject: [squid-users] Squid stopped working after cache.log and
	access.log rotation
In-Reply-To: <CAMyAatABvrhXkY-JefdxK2L1c1MbDV8BoufRYnN9w2_5EQVkYA@mail.gmail.com>
References: <CAMyAatDzxB=+zNcB-jvc8vb+hse44Ne80tiaNUHgT_yQgM1iPg@mail.gmail.com>
 <1548158039.1456.1490196189795.JavaMail.zimbra@tresgeek.net>
 <CAMyAatABvrhXkY-JefdxK2L1c1MbDV8BoufRYnN9w2_5EQVkYA@mail.gmail.com>
Message-ID: <201703231949.48765.Antony.Stone@squid.open.source.it>

On Thursday 23 March 2017 at 17:36:01, Chee M Gui wrote:

> Thank you Jason.
> 
> lsof /var/log | grep -i delete  does not show anything
> Likewise for  lsof /var/log/squid | grep -i delete
> 
> Thank you Antony
> No, we have a lot of disk space.

What does your squid logfile rotation script contain?


Antony.

-- 
What makes you think I know what I'm talking about?
I just have more O'Reilly books than most people.

                                                   Please reply to the list;
                                                         please *don't* CC me.


From eliezer at ngtech.co.il  Thu Mar 23 20:59:21 2017
From: eliezer at ngtech.co.il (Eliezer  Croitoru)
Date: Thu, 23 Mar 2017 22:59:21 +0200
Subject: [squid-users] Squid stopped working after cache.log and
	access.log	rotation
In-Reply-To: <CAMyAatDzxB=+zNcB-jvc8vb+hse44Ne80tiaNUHgT_yQgM1iPg@mail.gmail.com>
References: <CAMyAatDzxB=+zNcB-jvc8vb+hse44Ne80tiaNUHgT_yQgM1iPg@mail.gmail.com>
Message-ID: <097b01d2a418$588f1c70$09ad5550$@ngtech.co.il>

There is another option!
The log rotate script is doing something nasty or the systemd service file start up squid in a weird way.
The output of:
$ systemctl status squid
$ top -n1 -b
$ ps aux
$ df -h
$ netstat -ntulp
$ lsof -n|egrep "proxy|squid"

How many clients this system has?
Is the system facing the Internet directly or behind some nat(aws or another provider)?

The above are the basic required data to understand the situation.

All The Bests,
Eliezer

----
http://ngtech.co.il/lmgtfy/
Linux System Administrator
Mobile: +972-5-28704261
Email: eliezer at ngtech.co.il


From: squid-users [mailto:squid-users-bounces at lists.squid-cache.org] On Behalf Of Chee M Gui
Sent: Wednesday, March 22, 2017 5:18 PM
To: squid-users at lists.squid-cache.org
Subject: [squid-users] Squid stopped working after cache.log and access.log rotation



Hi All

We recently installed Squid 3.5.12-1ubuntu7.3 on Ubuntu 16.04.2 LTS.  It ran fine at first but stopped working after a while.   telnet server 3128 still works, i.e., opens a blank window, but Squid is just not accepting requests.    Then we realized that there is no new access.log file.   The access.log file stopped rotated at 6:24AM on 3/17/2017.    It looks like Squid wasn't able to create a new access.log?  We could not find any error message in syslog or the cache.log.      We haven't rebooted the server because we want to know what went wrong.   It isn't the firewall blocking Squid because Squid was working fine all the while until recently.  Also after it stopped working, we disabled the firewall to see if it would work but it still didn't work.

root at paproxy:/var/log/squid# ls -alt
total 15536
drwxr-xr-x 2 proxy proxy      4096 Mar 21 06:25 .
-rw-r----- 1 proxy proxy        63 Mar 21 06:25 cache.log
drwxrwxr-x 9 root  syslog     4096 Mar 21 06:25 ..
-rw-r----- 1 proxy proxy        63 Mar 20 06:25 cache.log.1
-rw-r----- 1 proxy proxy        83 Mar 19 06:25 cache.log.2.gz
-rw-r----- 1 proxy proxy  15759111 Mar 17 06:24 access.log.1
-rw-r----- 1 proxy proxy    117223 Mar 17 05:52 netdb.state

Any ideas what went wrong?

Thank you very much in anticipation.

Gui



From cheemeng at function.com  Fri Mar 24 02:05:40 2017
From: cheemeng at function.com (Chee M Gui)
Date: Thu, 23 Mar 2017 19:05:40 -0700
Subject: [squid-users] Why do we have to set pinger_enable to off to get
	Squid to work
Message-ID: <CAMyAatANq9sPPLpfpo=ZV_q24qP1kzR_vBXg9K1i4ZKKcOQw5g@mail.gmail.com>

Hi All


We recently installed Squid 3.5.12-1ubuntu7.3 on Ubuntu 16.04.2 LTS on two
machines.     Squid works fine on the first machine.   But on the second
machine, Squid is able to start successfully but is not listening to any
port at all.   Not the default 3128 or any other port we configure in
squid.conf.   We disabled the iptables firewall and it didn't help.    Then
we read the link below and were able to get Squid to listen to 3218 by
setting pinger_enable to off in squid.conf.

However, we don't want to have to set pinger_enable to off (the default is
on).   And the link below doesn't explain how to get pinger to work.
 There isn't any information in the logs.   We are planning to reinstall
the OS and then try again.    But does anybody here knows how to get Squid
to work with pinger_enable set to on (default)?

http://squid-web-proxy-cache.1019090.n4.nabble.com/Squid-not-listening-on-any-port-td4667004.html

Thank you very much in anticipation

Gui
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20170323/88434834/attachment.htm>

From eliezer at ngtech.co.il  Fri Mar 24 11:09:08 2017
From: eliezer at ngtech.co.il (Eliezer  Croitoru)
Date: Fri, 24 Mar 2017 14:09:08 +0300
Subject: [squid-users] Why do we have to set pinger_enable to off to
	get	Squid to work
In-Reply-To: <CAMyAatANq9sPPLpfpo=ZV_q24qP1kzR_vBXg9K1i4ZKKcOQw5g@mail.gmail.com>
References: <CAMyAatANq9sPPLpfpo=ZV_q24qP1kzR_vBXg9K1i4ZKKcOQw5g@mail.gmail.com>
Message-ID: <0b6001d2a48f$0f587660$2e096320$@ngtech.co.il>

Hey Gui,

The right way to allow pinger to work is to use the setuid bit ie:
$ chwon root.root /pinger_path/pinger
$ chmod +s /pinger_path/pinger

This will allow pinger to start as a root use by any of the machine users and in this case squid.
There might be another restriction by the OS but I am not sure since I am using my own packge for Ubuntu for now.

All The Bests,
Eliezer

----
http://ngtech.co.il/lmgtfy/
Linux System Administrator
Mobile: +972-5-28704261
Email: eliezer at ngtech.co.il


From: squid-users [mailto:squid-users-bounces at lists.squid-cache.org] On Behalf Of Chee M Gui
Sent: Friday, March 24, 2017 5:06 AM
To: squid-users at lists.squid-cache.org
Subject: [squid-users] Why do we have to set pinger_enable to off to get Squid to work

Hi All



We recently installed Squid 3.5.12-1ubuntu7.3 on Ubuntu 16.04.2 LTS on two machines.     Squid works fine on the first machine.   But on the second machine, Squid is able to start successfully but is not listening to any port at all.   Not the default 3128 or any other port we configure in squid.conf.   We disabled the iptables firewall and it didn't help.    Then we read the link below and were able to get Squid to listen to 3218 by setting pinger_enable to off in squid.conf.

However, we don't want to have to set pinger_enable to off (the default is on).   And the link below doesn't explain how to get pinger to work.    There isn't any information in the logs.   We are planning to reinstall the OS and then try again.    But does anybody here knows how to get Squid to work with pinger_enable set to on (default)?

http://squid-web-proxy-cache.1019090.n4.nabble.com/Squid-not-listening-on-any-port-td4667004.html

Thank you very much in anticipation

Gui




From eliezer at ngtech.co.il  Fri Mar 24 12:50:11 2017
From: eliezer at ngtech.co.il (Eliezer  Croitoru)
Date: Fri, 24 Mar 2017 15:50:11 +0300
Subject: [squid-users] URL list from a URL
In-Reply-To: <895324221.1881.1490111566009.JavaMail.zimbra@tresgeek.net>
References: <895324221.1881.1490111566009.JavaMail.zimbra@tresgeek.net>
Message-ID: <0b7701d2a49d$30350ed0$909f2c70$@ngtech.co.il>

My suggestion is to run every minute a curl request into a tmp file and then compare to the existing one, then sort and uniq and replace the existing DB
In the external acl helper script use some kind of version testing by the last date updated.
You can use a simple comparison to do that.
So two scripts:
1 - update by a crontab
2 - external_acl helper

Will update later.

Eliezer

----
Eliezer Croitoru
Linux System Administrator
Mobile: +972-5-28704261
Email: eliezer at ngtech.co.il



-----Original Message-----
From: squid-users [mailto:squid-users-bounces at lists.squid-cache.org] On Behalf Of Jason B. Nance
Sent: Tuesday, March 21, 2017 5:53 PM
To: squid-users at lists.squid-cache.org
Subject: [squid-users] URL list from a URL

Hello,

I'm using Squid 3.5.20 and wonder if it is possible to define an ACL which retrieves the list of URLs from another URL (similar to pointing to a file).  In this specific use case it is to allow a Foreman server to sync Yum content from the CentOS mirrors.  I tell Foreman to use the following URL:

http://mirrorlist.centos.org/?release=7&arch=x86_64&repo=updates

Which returns a list of URLs, such as:

http://repo1.dal.innoscale.net/centos/7.3.1611/updates/x86_64/
http://linux.mirrors.es.net/centos/7.3.1611/updates/x86_64/
http://reflector.westga.edu/repos/CentOS/7.3.1611/updates/x86_64/
http://mirror.jax.hugeserver.com/centos/7.3.1611/updates/x86_64/
http://ftp.linux.ncsu.edu/pub/CentOS/7.3.1611/updates/x86_64/
http://mirror.nexcess.net/CentOS/7.3.1611/updates/x86_64/
http://mirror.web-ster.com/centos/7.3.1611/updates/x86_64/
http://centos.host-engine.com/7.3.1611/updates/x86_64/
http://mirror.raystedman.net/centos/7.3.1611/updates/x86_64/
http://mirror.linux.duke.edu/pub/centos/7.3.1611/updates/x86_64/

Foreman then starts a new HTTP connection (not a redirect) to attempt to connect to those in turn until it works.

So I would like to configure Squid to allow the Foreman server access to any of those URLs (the list changes somewhat often).

I started to go down the external_acl_type but am wondering if I'm missing something obvious.

Regards,

j
_______________________________________________
squid-users mailing list
squid-users at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-users



From eliezer at ngtech.co.il  Fri Mar 24 13:33:06 2017
From: eliezer at ngtech.co.il (Eliezer  Croitoru)
Date: Fri, 24 Mar 2017 16:33:06 +0300
Subject: [squid-users] URL list from a URL
In-Reply-To: <895324221.1881.1490111566009.JavaMail.zimbra@tresgeek.net>
References: <895324221.1881.1490111566009.JavaMail.zimbra@tresgeek.net>
Message-ID: <0b7b01d2a4a3$2bcb3d50$8361b7f0$@ngtech.co.il>

OK so I wrote these helpers that can help to allow the updates despite the changes in urls on gist:
https://gist.github.com/elico/dc1af72344231d6d49af4eacecfae8df

The squid.conf should be something like:
external_acl_type centos_mirror_check ipv4 concurrency=200 ttl=15 %URI %SRC %METHOD /opt/bin/centos-extacl.rb /etc/squid/centosrepos.txt
acl centos_mirror_check_acl external centos_mirror_check
htttp_access allow !CONNECT centos_mirror_check_acl


Let me know if this is not enough or there is a need for some tweaks.

Eliezer

----
Eliezer Croitoru
Linux System Administrator
Mobile: +972-5-28704261
Email: eliezer at ngtech.co.il



-----Original Message-----
From: squid-users [mailto:squid-users-bounces at lists.squid-cache.org] On Behalf Of Jason B. Nance
Sent: Tuesday, March 21, 2017 5:53 PM
To: squid-users at lists.squid-cache.org
Subject: [squid-users] URL list from a URL

Hello,

I'm using Squid 3.5.20 and wonder if it is possible to define an ACL which retrieves the list of URLs from another URL (similar to pointing to a file).  In this specific use case it is to allow a Foreman server to sync Yum content from the CentOS mirrors.  I tell Foreman to use the following URL:

http://mirrorlist.centos.org/?release=7&arch=x86_64&repo=updates

Which returns a list of URLs, such as:

http://repo1.dal.innoscale.net/centos/7.3.1611/updates/x86_64/
http://linux.mirrors.es.net/centos/7.3.1611/updates/x86_64/
http://reflector.westga.edu/repos/CentOS/7.3.1611/updates/x86_64/
http://mirror.jax.hugeserver.com/centos/7.3.1611/updates/x86_64/
http://ftp.linux.ncsu.edu/pub/CentOS/7.3.1611/updates/x86_64/
http://mirror.nexcess.net/CentOS/7.3.1611/updates/x86_64/
http://mirror.web-ster.com/centos/7.3.1611/updates/x86_64/
http://centos.host-engine.com/7.3.1611/updates/x86_64/
http://mirror.raystedman.net/centos/7.3.1611/updates/x86_64/
http://mirror.linux.duke.edu/pub/centos/7.3.1611/updates/x86_64/

Foreman then starts a new HTTP connection (not a redirect) to attempt to connect to those in turn until it works.

So I would like to configure Squid to allow the Foreman server access to any of those URLs (the list changes somewhat often).

I started to go down the external_acl_type but am wondering if I'm missing something obvious.

Regards,

j
_______________________________________________
squid-users mailing list
squid-users at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-users



From eliezer at ngtech.co.il  Fri Mar 24 14:59:16 2017
From: eliezer at ngtech.co.il (Eliezer  Croitoru)
Date: Fri, 24 Mar 2017 17:59:16 +0300
Subject: [squid-users] URL list from a URL
In-Reply-To: <0b7b01d2a4a3$2bcb3d50$8361b7f0$@ngtech.co.il>
References: <895324221.1881.1490111566009.JavaMail.zimbra@tresgeek.net>
 <0b7b01d2a4a3$2bcb3d50$8361b7f0$@ngtech.co.il>
Message-ID: <0ef101d2a4af$3559df00$a00d9d00$@ngtech.co.il>

And if you want the full list of mirrors being updated instead of using curl you can use the next script to parse the full mirror list page:
http://mirror-status.centos.org/

Using the script:
https://gist.github.com/elico/dc1af72344231d6d49af4eacecfae8df#file-update-mirrors-list-sh

Let me know if it fills the gaps.

Eliezer

----
Eliezer Croitoru
Linux System Administrator
Mobile: +972-5-28704261
Email: eliezer at ngtech.co.il



-----Original Message-----
From: squid-users [mailto:squid-users-bounces at lists.squid-cache.org] On Behalf Of Eliezer Croitoru
Sent: Friday, March 24, 2017 4:33 PM
To: 'Jason B. Nance' <jason at tresgeek.net>; squid-users at lists.squid-cache.org
Subject: Re: [squid-users] URL list from a URL

OK so I wrote these helpers that can help to allow the updates despite the changes in urls on gist:
https://gist.github.com/elico/dc1af72344231d6d49af4eacecfae8df

The squid.conf should be something like:
external_acl_type centos_mirror_check ipv4 concurrency=200 ttl=15 %URI %SRC %METHOD /opt/bin/centos-extacl.rb /etc/squid/centosrepos.txt
acl centos_mirror_check_acl external centos_mirror_check
htttp_access allow !CONNECT centos_mirror_check_acl


Let me know if this is not enough or there is a need for some tweaks.

Eliezer

----
Eliezer Croitoru
Linux System Administrator
Mobile: +972-5-28704261
Email: eliezer at ngtech.co.il



-----Original Message-----
From: squid-users [mailto:squid-users-bounces at lists.squid-cache.org] On Behalf Of Jason B. Nance
Sent: Tuesday, March 21, 2017 5:53 PM
To: squid-users at lists.squid-cache.org
Subject: [squid-users] URL list from a URL

Hello,

I'm using Squid 3.5.20 and wonder if it is possible to define an ACL which retrieves the list of URLs from another URL (similar to pointing to a file).  In this specific use case it is to allow a Foreman server to sync Yum content from the CentOS mirrors.  I tell Foreman to use the following URL:

http://mirrorlist.centos.org/?release=7&arch=x86_64&repo=updates

Which returns a list of URLs, such as:

http://repo1.dal.innoscale.net/centos/7.3.1611/updates/x86_64/
http://linux.mirrors.es.net/centos/7.3.1611/updates/x86_64/
http://reflector.westga.edu/repos/CentOS/7.3.1611/updates/x86_64/
http://mirror.jax.hugeserver.com/centos/7.3.1611/updates/x86_64/
http://ftp.linux.ncsu.edu/pub/CentOS/7.3.1611/updates/x86_64/
http://mirror.nexcess.net/CentOS/7.3.1611/updates/x86_64/
http://mirror.web-ster.com/centos/7.3.1611/updates/x86_64/
http://centos.host-engine.com/7.3.1611/updates/x86_64/
http://mirror.raystedman.net/centos/7.3.1611/updates/x86_64/
http://mirror.linux.duke.edu/pub/centos/7.3.1611/updates/x86_64/

Foreman then starts a new HTTP connection (not a redirect) to attempt to connect to those in turn until it works.

So I would like to configure Squid to allow the Foreman server access to any of those URLs (the list changes somewhat often).

I started to go down the external_acl_type but am wondering if I'm missing something obvious.

Regards,

j
_______________________________________________
squid-users mailing list
squid-users at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-users

_______________________________________________
squid-users mailing list
squid-users at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-users



From eliezer at ngtech.co.il  Fri Mar 24 22:01:33 2017
From: eliezer at ngtech.co.il (Eliezer  Croitoru)
Date: Sat, 25 Mar 2017 01:01:33 +0300
Subject: [squid-users] URL list from a URL
In-Reply-To: <0ef101d2a4af$3559df00$a00d9d00$@ngtech.co.il>
References: <895324221.1881.1490111566009.JavaMail.zimbra@tresgeek.net>
 <0b7b01d2a4a3$2bcb3d50$8361b7f0$@ngtech.co.il>
 <0ef101d2a4af$3559df00$a00d9d00$@ngtech.co.il>
Message-ID: <018e01d2a4ea$334d1d40$99e757c0$@ngtech.co.il>

Sorry a type with the link so:
And if you want the full list of mirrors being updated instead of using curl you can use the next script to parse the full mirror list page:
http://mirror-status.centos.org/

Using the script:
https://gist.github.com/elico/dc1af72344231d6d49af4eacecfae8df#file-get-full-mirrolist-rb

Eliezer


----
Eliezer Croitoru
Linux System Administrator
Mobile: +972-5-28704261
Email: eliezer at ngtech.co.il



-----Original Message-----
From: squid-users [mailto:squid-users-bounces at lists.squid-cache.org] On Behalf Of Eliezer Croitoru
Sent: Friday, March 24, 2017 5:59 PM
To: 'Jason B. Nance' <jason at tresgeek.net>; squid-users at lists.squid-cache.org
Subject: Re: [squid-users] URL list from a URL

And if you want the full list of mirrors being updated instead of using curl you can use the next script to parse the full mirror list page:
http://mirror-status.centos.org/

Using the script:
https://gist.github.com/elico/dc1af72344231d6d49af4eacecfae8df#file-update-mirrors-list-sh

Let me know if it fills the gaps.

Eliezer

----
Eliezer Croitoru
Linux System Administrator
Mobile: +972-5-28704261
Email: eliezer at ngtech.co.il



-----Original Message-----
From: squid-users [mailto:squid-users-bounces at lists.squid-cache.org] On Behalf Of Eliezer Croitoru
Sent: Friday, March 24, 2017 4:33 PM
To: 'Jason B. Nance' <jason at tresgeek.net>; squid-users at lists.squid-cache.org
Subject: Re: [squid-users] URL list from a URL

OK so I wrote these helpers that can help to allow the updates despite the changes in urls on gist:
https://gist.github.com/elico/dc1af72344231d6d49af4eacecfae8df

The squid.conf should be something like:
external_acl_type centos_mirror_check ipv4 concurrency=200 ttl=15 %URI %SRC %METHOD /opt/bin/centos-extacl.rb /etc/squid/centosrepos.txt acl centos_mirror_check_acl external centos_mirror_check htttp_access allow !CONNECT centos_mirror_check_acl


Let me know if this is not enough or there is a need for some tweaks.

Eliezer

----
Eliezer Croitoru
Linux System Administrator
Mobile: +972-5-28704261
Email: eliezer at ngtech.co.il



-----Original Message-----
From: squid-users [mailto:squid-users-bounces at lists.squid-cache.org] On Behalf Of Jason B. Nance
Sent: Tuesday, March 21, 2017 5:53 PM
To: squid-users at lists.squid-cache.org
Subject: [squid-users] URL list from a URL

Hello,

I'm using Squid 3.5.20 and wonder if it is possible to define an ACL which retrieves the list of URLs from another URL (similar to pointing to a file).  In this specific use case it is to allow a Foreman server to sync Yum content from the CentOS mirrors.  I tell Foreman to use the following URL:

http://mirrorlist.centos.org/?release=7&arch=x86_64&repo=updates

Which returns a list of URLs, such as:

http://repo1.dal.innoscale.net/centos/7.3.1611/updates/x86_64/
http://linux.mirrors.es.net/centos/7.3.1611/updates/x86_64/
http://reflector.westga.edu/repos/CentOS/7.3.1611/updates/x86_64/
http://mirror.jax.hugeserver.com/centos/7.3.1611/updates/x86_64/
http://ftp.linux.ncsu.edu/pub/CentOS/7.3.1611/updates/x86_64/
http://mirror.nexcess.net/CentOS/7.3.1611/updates/x86_64/
http://mirror.web-ster.com/centos/7.3.1611/updates/x86_64/
http://centos.host-engine.com/7.3.1611/updates/x86_64/
http://mirror.raystedman.net/centos/7.3.1611/updates/x86_64/
http://mirror.linux.duke.edu/pub/centos/7.3.1611/updates/x86_64/

Foreman then starts a new HTTP connection (not a redirect) to attempt to connect to those in turn until it works.

So I would like to configure Squid to allow the Foreman server access to any of those URLs (the list changes somewhat often).

I started to go down the external_acl_type but am wondering if I'm missing something obvious.

Regards,

j
_______________________________________________
squid-users mailing list
squid-users at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-users

_______________________________________________
squid-users mailing list
squid-users at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-users

_______________________________________________
squid-users mailing list
squid-users at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-users



From je at ktf.rtu.lv  Sun Mar 26 11:56:32 2017
From: je at ktf.rtu.lv (=?utf-8?b?SsSBbmlz?=)
Date: Sun, 26 Mar 2017 14:56:32 +0300
Subject: [squid-users] Communication fails between parent and child if using
	SSL/TLS
Message-ID: <20170326145632.Horde.eeHqwM3TWfbCwDLaApKkTqL@inbox.dv.lv>

Hi!

theoretically, I have configured two squids in a parent-child cache structure.

It works perfectly if it is just "plaintext" communications, but is i  
set the to use ssl (for non https traffic),
the following error occurs:
X-Squid-Error: ERR_CONNECT_FAIL 111

and

TCP connection to PARENT/PORT failed

pop: lookup for key {PARENT/PORT} failed

child's cache_peer config:

cache_peer PARENT parent PORT 0 proxy-only ssl \
            sslcert=/path/to/cert.pem \
            sslkey=/path/to/key.key \
            sslflags=DONT_VERIFY_PEER

parent's:

https_port PORT \
     cert=/path/to/parent/cert.pem \
     key=/path/to/parent/key.key \
     sslflags=NO_DEFAULT_CA

yes, and parent for some reason is not listening on PORT (according to  
netstat -l -n)

connection for child to parent - allowed (is stay the same either for  
non-ssl or ssl-enabled cfg.

squid's .configure:
   --prefix=/usr \
   --libdir=/usr/lib${LIBDIRSUFFIX} \
   --sysconfdir=/etc/squid \
   --localstatedir=/var/log/squid \
   --datadir=/usr/share/squid \
   --with-pidfile=/var/run/squid \
   --mandir=/usr/man \
   --with-logdir=/var/log/squid \
   --disable-devpoll \
   --enable-snmp \
   --enable-ssl \
   --enable-linux-netfilter \
   --enable-async-io \
   --disable-translation \
   --build=$ARCH-slackware-linux

What disappoints - with older version of squid it worked. The upgrade  
turned it down.


From je at ktf.rtu.lv  Sun Mar 26 12:01:43 2017
From: je at ktf.rtu.lv (=?utf-8?b?SsSBbmlz?=)
Date: Sun, 26 Mar 2017 15:01:43 +0300
Subject: [squid-users] Communication fails between parent and child if
 using SSL/TLS
In-Reply-To: <20170326145632.Horde.eeHqwM3TWfbCwDLaApKkTqL@inbox.dv.lv>
References: <20170326145632.Horde.eeHqwM3TWfbCwDLaApKkTqL@inbox.dv.lv>
Message-ID: <20170326150143.Horde.fDQFJquG5jdiXr4BYEXP7J_@inbox.dv.lv>


Cit?ts J?nis <je at ktf.rtu.lv>
Sun, 26 Mar 2017 14:56:32 +0300:

> Hi!
>
> theoretically, I have configured two squids in a parent-child cache  
> structure.
>
> It works perfectly if it is just "plaintext" communications, but is  
> i set the to use ssl (for non https traffic),
> the following error occurs:
> X-Squid-Error: ERR_CONNECT_FAIL 111
>
> and
>
> TCP connection to PARENT/PORT failed
>
> pop: lookup for key {PARENT/PORT} failed
>
> child's cache_peer config:
>
> cache_peer PARENT parent PORT 0 proxy-only ssl \
>            sslcert=/path/to/cert.pem \
>            sslkey=/path/to/key.key \
>            sslflags=DONT_VERIFY_PEER
>
> parent's:
>
> https_port PORT \
>     cert=/path/to/parent/cert.pem \
>     key=/path/to/parent/key.key \
>     sslflags=NO_DEFAULT_CA
>
> yes, and parent for some reason is not listening on PORT (according  
> to netstat -l -n)
>
> connection for child to parent - allowed (is stay the same either  
> for non-ssl or ssl-enabled cfg.
>
> squid's .configure:
>   --prefix=/usr \
>   --libdir=/usr/lib${LIBDIRSUFFIX} \
>   --sysconfdir=/etc/squid \
>   --localstatedir=/var/log/squid \
>   --datadir=/usr/share/squid \
>   --with-pidfile=/var/run/squid \
>   --mandir=/usr/man \
>   --with-logdir=/var/log/squid \
>   --disable-devpoll \
>   --enable-snmp \
>   --enable-ssl \
>   --enable-linux-netfilter \
>   --enable-async-io \
>   --disable-translation \
>   --build=$ARCH-slackware-linux
>
> What disappoints - with older version of squid it worked. The  
> upgrade turned it down.

bots ends use gnutls.



From squid3 at treenet.co.nz  Sun Mar 26 19:42:46 2017
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Mon, 27 Mar 2017 08:42:46 +1300
Subject: [squid-users] Communication fails between parent and child if
 using SSL/TLS
In-Reply-To: <20170326150143.Horde.fDQFJquG5jdiXr4BYEXP7J_@inbox.dv.lv>
References: <20170326145632.Horde.eeHqwM3TWfbCwDLaApKkTqL@inbox.dv.lv>
 <20170326150143.Horde.fDQFJquG5jdiXr4BYEXP7J_@inbox.dv.lv>
Message-ID: <ccafdc87-80e2-51a2-a510-fc6214523b9b@treenet.co.nz>

On 27/03/2017 1:01 a.m., J?nis wrote:
> 
> Cit?ts J?nis
> Sun, 26 Mar 2017 14:56:32 +0300:
> 
>> Hi!
>>
>> theoretically, I have configured two squids in a parent-child cache
>> structure.
>>
>> It works perfectly if it is just "plaintext" communications, but is i
>> set the to use ssl (for non https traffic),
>> the following error occurs:
>> X-Squid-Error: ERR_CONNECT_FAIL 111
>>
>> and
>>
>> TCP connection to PARENT/PORT failed
>>
>> pop: lookup for key {PARENT/PORT} failed
>>
>> child's cache_peer config:
>>
>> cache_peer PARENT parent PORT 0 proxy-only ssl \
>>            sslcert=/path/to/cert.pem \
>>            sslkey=/path/to/key.key \
>>            sslflags=DONT_VERIFY_PEER
>>
>> parent's:
>>
>> https_port PORT \
>>     cert=/path/to/parent/cert.pem \
>>     key=/path/to/parent/key.key \
>>     sslflags=NO_DEFAULT_CA
>>
>> yes, and parent for some reason is not listening on PORT (according to
>> netstat -l -n)
>>
>> connection for child to parent - allowed (is stay the same either for
>> non-ssl or ssl-enabled cfg.
>>
>> squid's .configure:
>>   --prefix=/usr \
>>   --libdir=/usr/lib${LIBDIRSUFFIX} \
>>   --sysconfdir=/etc/squid \
>>   --localstatedir=/var/log/squid \
>>   --datadir=/usr/share/squid \
>>   --with-pidfile=/var/run/squid \
>>   --mandir=/usr/man \
>>   --with-logdir=/var/log/squid \
>>   --disable-devpoll \
>>   --enable-snmp \
>>   --enable-ssl \
>>   --enable-linux-netfilter \
>>   --enable-async-io \
>>   --disable-translation \
>>   --build=$ARCH-slackware-linux
>>
>> What disappoints - with older version of squid it worked. The upgrade
>> turned it down.

By "the upgrade" you mean what version(s) changed?

> 
> bots ends use gnutls.
> 

GnuTLS support is not available for https_port yet. You need
build option  --with-openssl for at least that part. --enable-ssl is
deprecated.

Amos


From squid-user at tlinx.org  Mon Mar 27 19:26:08 2017
From: squid-user at tlinx.org (L A Walsh)
Date: Mon, 27 Mar 2017 12:26:08 -0700
Subject: [squid-users] hsc-dynamic-cache: relied on storeID rules? Removed
	in 3.5.20?
Message-ID: <58D96750.3010405@tlinx.org>

This caught my attention as my housemate tends to watch alot of
youtube videos, and caching some of them might speed up their
access, so was trying to understand what was meant in your post:

Yuri Voinov wrote:
> Things are changed in the web on regular basis. Nothing permanent in the
> world.
>
> So, store ID rules lost relevance and no longer work.
>   
----
    Is the problem that "Store ID rules lost relevance" caused by a
change from squid 3.5.19 -> 3.5.20?

    That doesn't sound so much like a change on the web, but a change
in squid.

    Were storeID rules removed in 3.5.20?  If that's the case,
what might have replaced them?

@Eduardo: am I to understand that this plugin worked in 3.5.19, but
not in 3.5.20 and above?  (trying to get the versions right)

Also, Eduardo -- what specific features above 3.5.19 were you hoping
to include by upgrading?  I.e. is 3.5.19 not working for you for some
reason?  It might be easier to cherry pick changes that you needed from
some later version back into 3.5.19 if some major feature
(like storeID rules?) was removed after that version...

Thanks Yuri!
Linda




From yvoinov at gmail.com  Mon Mar 27 19:42:06 2017
From: yvoinov at gmail.com (Yuri Voinov)
Date: Tue, 28 Mar 2017 01:42:06 +0600
Subject: [squid-users] hsc-dynamic-cache: relied on storeID rules?
	Removed in 3.5.20?
In-Reply-To: <58D96750.3010405@tlinx.org>
References: <58D96750.3010405@tlinx.org>
Message-ID: <1fa04ada-ed51-da4f-a55c-1a2e2f9f226a@gmail.com>



28.03.2017 1:26, L A Walsh ?????:
> This caught my attention as my housemate tends to watch alot of
> youtube videos, and caching some of them might speed up their
> access, so was trying to understand what was meant in your post:
>
> Yuri Voinov wrote:
>> Things are changed in the web on regular basis. Nothing permanent in the
>> world.
>>
>> So, store ID rules lost relevance and no longer work.
What word is not clear here?
>>   
> ----
>    Is the problem that "Store ID rules lost relevance" caused by a
> change from squid 3.5.19 -> 3.5.20?
Caused independently during "things changes time to time".
>
>    That doesn't sound so much like a change on the web, but a change
> in squid.
Heh? Really?
>
>    Were storeID rules removed in 3.5.20?  If that's the case,
> what might have replaced them?
Why did it happen? What does the documentation tell us?
>
> @Eduardo: am I to understand that this plugin worked in 3.5.19, but
> not in 3.5.20 and above?  (trying to get the versions right)
>
> Also, Eduardo -- what specific features above 3.5.19 were you hoping
> to include by upgrading?  I.e. is 3.5.19 not working for you for some
> reason?  It might be easier to cherry pick changes that you needed from
> some later version back into 3.5.19 if some major feature
> (like storeID rules?) was removed after that version...
Right now I'm running Squid v5.x. With store ID. So?
>
> Thanks Yuri!
> Linda
>
>

-- 
Bugs to the Future
-------------- next part --------------
A non-text attachment was scrubbed...
Name: 0x613DEC46.asc
Type: application/pgp-keys
Size: 2437 bytes
Desc: not available
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20170328/480be184/attachment.key>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: signature.asc
Type: application/pgp-signature
Size: 473 bytes
Desc: OpenPGP digital signature
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20170328/480be184/attachment.sig>

From eliezer at ngtech.co.il  Mon Mar 27 20:12:52 2017
From: eliezer at ngtech.co.il (Eliezer  Croitoru)
Date: Mon, 27 Mar 2017 23:12:52 +0300
Subject: [squid-users] hsc-dynamic-cache: relied on storeID rules?
	Removed	in 3.5.20?
In-Reply-To: <58D96750.3010405@tlinx.org>
References: <58D96750.3010405@tlinx.org>
Message-ID: <042c01d2a736$83db5df0$8b9219d0$@ngtech.co.il>

Hey Linda,

As the pathcer\author of StoreID I will try to clarify what might seems odd.
StoreID is a "static" rule which is one of the squid cache fundamentals.
The feature is the option to tweak this internal cache object ID.
This is a very static feature and will not be changed for a very long time from now on.
Most of the public helpers I have seen are very "simple" and rely on very simple things.
The main idea is that the admin can know and predict what would be the StoreID.
It's like "knowing" what the kid is going to say next from the past experience with him.
In a similar way StoreID allows an admin that can predict that a specific URL will contain the same content as another one that was previously cached.
In a more simple way, it gives that admin the option to run a "de-duplication" script.

But since many systems these days are aware of the option to predict what would be the next url(think about an exe or other binary that can be replaced on-the-fly\in-transit) the developers changed and change(like a Diffe Hellman) their way of transporting content to the end client.
Due to this "Diffe Hellman" feature that many added it makes the more simple scripts useless since the developers became much smarter.
And back to the analogy, the kid grows and talks in a certain language.
Now the kids is much more mature after couple years and he became smart enough so you would need to actually answer the question "where do kids come from" or some other questions.
Then the answer can be creative enough to leave the child with a satisfying and a good one. If you "miss" this opportunity and will answer a very non-realistic answer(compared to the kid's age) the next time he would demand from you a more demanding answer and eventually if you would answer too many bad responses he will eventually go and research by himself in other and maybe a more reliable sources.
(the above analogy is what gave squid a very bad reputation by many admins)

Today Squid-Cache is smart enough to at-least answer the right answer for a demanding client even if the admin does something wrong in his StoreID helper.
Indeed you will see TCP_MISS and it won't cache but this is only since the admin might have the illusion that an encrypted content can be predicted when a Diffie  Helman cryptography is creating the plain url's these days.

Hope It helped,
Eliezer

* For youtube to be cached the best way is to learn more about content adaptation and to have a look at some youtube video downloaders scripts.

----
Eliezer Croitoru
Linux System Administrator
Mobile: +972-5-28704261
Email: eliezer at ngtech.co.il



-----Original Message-----
From: squid-users [mailto:squid-users-bounces at lists.squid-cache.org] On Behalf Of L A Walsh
Sent: Monday, March 27, 2017 10:26 PM
To: Yuri Voinov <yvoinov at gmail.com>
Cc: squid-users at squid-cache.org
Subject: [squid-users] hsc-dynamic-cache: relied on storeID rules? Removed in 3.5.20?

This caught my attention as my housemate tends to watch alot of
youtube videos, and caching some of them might speed up their
access, so was trying to understand what was meant in your post:

Yuri Voinov wrote:
> Things are changed in the web on regular basis. Nothing permanent in the
> world.
>
> So, store ID rules lost relevance and no longer work.
>   
----
    Is the problem that "Store ID rules lost relevance" caused by a
change from squid 3.5.19 -> 3.5.20?

    That doesn't sound so much like a change on the web, but a change
in squid.

    Were storeID rules removed in 3.5.20?  If that's the case,
what might have replaced them?

@Eduardo: am I to understand that this plugin worked in 3.5.19, but
not in 3.5.20 and above?  (trying to get the versions right)

Also, Eduardo -- what specific features above 3.5.19 were you hoping
to include by upgrading?  I.e. is 3.5.19 not working for you for some
reason?  It might be easier to cherry pick changes that you needed from
some later version back into 3.5.19 if some major feature
(like storeID rules?) was removed after that version...

Thanks Yuri!
Linda


_______________________________________________
squid-users mailing list
squid-users at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-users



From eduardoocarneiro at gmail.com  Tue Mar 28 11:18:52 2017
From: eduardoocarneiro at gmail.com (Eduardo Carneiro)
Date: Tue, 28 Mar 2017 04:18:52 -0700 (PDT)
Subject: [squid-users] hsc-dynamic-cache: relied on storeID rules?
	Removed in 3.5.20?
In-Reply-To: <58D96750.3010405@tlinx.org>
References: <58D96750.3010405@tlinx.org>
Message-ID: <1490699932730-4681924.post@n4.nabble.com>

Hello everyone.

Answering Linda's questions:

- Yes, I tried to use the hsc helper in versions above 3.5.19. But I did not
succeed.
- There are no specific features in the versions above 3.5.19 that I need to
use. But, for security reasons, I'd rather use the newer versions.

I posted on this just because, as I said before, I'd rather use newer
versions. Anyway, when I have some free time, I'll try to fix it.

My knowledge on this is limited, but I also think this problem is something
wrong with Squid. Following the logic, today in 3.5.19 still works normally.
I can cache Youtube, Facebook, etc. However, when I use a version above
3.5.19 it simply stops working.

Finally, I repeat: My knowledge of this is limited. But I'll try to fix that
in the future.



--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/hsc-dynamic-cache-relied-on-storeID-rules-Removed-in-3-5-20-tp4681921p4681924.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From giesbert at exactt.de  Tue Mar 28 13:54:04 2017
From: giesbert at exactt.de (Max Giesbert)
Date: Tue, 28 Mar 2017 13:54:04 +0000
Subject: [squid-users] Problem with basic_ldap_auth
Message-ID: <CAJxj0KAcgig3bj85zkFDMpkMoGkdm1KLqtXHaQBtdmDCa0MKQA@mail.gmail.com>

Hi everyone,

I am running Squid from the repos on Ubuntu 16.04. I can login to Squid
using basic_nsca_auth without problems. When I replace the according line
using basic_ldap_auth I am unable to login. The browser keeps requesting
the user credentials over and over again.

If I run the command in a shell it works as expected. I type:

/usr/lib/squid/basic_ldap_auth -v 3 -b ou="MYOU",dc=DOMAINNAME,dc=TLD -D
ldap_squid at MYDOMAIN -w "THEPASSWORD" -f sAMAccountName=%s -h MYHOST

Then I type username and password separated by a space and it gives "OK" or
ERR if wrong credentials are provided.

Any hints what could be wrong? How can I debug further what actually is
happened behind the scenes?

Thx for your help.

Max
-- 
Dipl. Inf. (FH) Max Giesbert
Gesch?ftsf?hrer

exactt technology GmbH

Mobil: +49 17 75 07 53 44
Festnetz: +49 89 38 15 64 42 1
Fax: +49 89 38 15 64 42 5
Web: http://exactt.de

F?ustlestra?e 3
80339 M?nchen
Germany

Fernwartung:
https://get.teamviewer.com/y5kz3vc

Sitz der Gesellschaft: M?nchen
Registergericht: Amtsgericht M?nchen, HRB 213444
Gesch?ftsf?hrer: Max Giesbert, Roman Weiss
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20170328/4b88122e/attachment.htm>

From cwaldon at otn.ca  Tue Mar 28 16:50:31 2017
From: cwaldon at otn.ca (Waldon, Cooper)
Date: Tue, 28 Mar 2017 16:50:31 +0000
Subject: [squid-users] Custom Error Messages
Message-ID: <YTOPR01MB0476E160CCC5CB6E23C176C9DF320@YTOPR01MB0476.CANPRD01.PROD.OUTLOOK.COM>

Hi All,

I'm attempting to make a custom error message when a page is blocked.  I found the example here:
http://wiki.squid-cache.org/SquidFaq/SquidAcl#I_want_to_customize.2C_or_make_my_own_error_messages.

But it's not working for me.

My configuration looks like this:

acl test url_regex "/etc/squid/blacklists/test/domains"
deny_info block test
http_access deny test

The page is being blocked but I get an error message that says Internal Error: Missing Template block

I've tried changing the second line to be an absolute path but it doesn't work.  I'm not sure what the default directory for the error messages is.  I did find another thread that mentions it should be in the .../error/templates/ folder but when I create that folder (i.e. /etc/squid/error/templates/) it still doesn't work.

I installed squid from rpm in RedHat 7 so I'm not sure if the default paths are changed?

Thanks,

Cooper Waldon l Network Engineer l OTN l 416-446-4110 x 4473 l www.otn.ca<http://www.otn.ca/> | Service Desk 1-855-654-0888 x2

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20170328/dc2e9d84/attachment.htm>

From cwaldon at otn.ca  Tue Mar 28 17:28:03 2017
From: cwaldon at otn.ca (Waldon, Cooper)
Date: Tue, 28 Mar 2017 17:28:03 +0000
Subject: [squid-users] Custom Error Messages
In-Reply-To: <YTOPR01MB0476E160CCC5CB6E23C176C9DF320@YTOPR01MB0476.CANPRD01.PROD.OUTLOOK.COM>
References: <YTOPR01MB0476E160CCC5CB6E23C176C9DF320@YTOPR01MB0476.CANPRD01.PROD.OUTLOOK.COM>
Message-ID: <YTOPR01MB0476A869BB14BACC493511D1DF320@YTOPR01MB0476.CANPRD01.PROD.OUTLOOK.COM>

Nevermind, I found the path.

For anyone else who need it it's:

/usr/share/squid/

From: squid-users [mailto:squid-users-bounces at lists.squid-cache.org] On Behalf Of Waldon, Cooper
Sent: Tuesday, March 28, 2017 12:51 PM
To: squid-users at lists.squid-cache.org
Subject: [squid-users] Custom Error Messages


This sender failed our fraud detection checks and may not be who they appear to be. Learn about spoofing<http://aka.ms/LearnAboutSpoofing>

Feedback<http://aka.ms/SafetyTipsFeedback>

Hi All,

I'm attempting to make a custom error message when a page is blocked.  I found the example here:
http://wiki.squid-cache.org/SquidFaq/SquidAcl#I_want_to_customize.2C_or_make_my_own_error_messages<https://na01.safelinks.protection.outlook.com/?url=http%3A%2F%2Fwiki.squid-cache.org%2FSquidFaq%2FSquidAcl%23I_want_to_customize.2C_or_make_my_own_error_messages&data=01%7C01%7Ccwaldon%40otn.ca%7C565a14918aff4f5f203908d475fa8f9c%7Cb211ab61e77f4bffabd5f70e4344653f%7C1&sdata=0VgOhV2MNKBgmpPiDvJb3uweqFyNWM11OSY2Hr0cUMo%3D&reserved=0>.

But it's not working for me.

My configuration looks like this:

acl test url_regex "/etc/squid/blacklists/test/domains"
deny_info block test
http_access deny test

The page is being blocked but I get an error message that says Internal Error: Missing Template block

I've tried changing the second line to be an absolute path but it doesn't work.  I'm not sure what the default directory for the error messages is.  I did find another thread that mentions it should be in the .../error/templates/ folder but when I create that folder (i.e. /etc/squid/error/templates/) it still doesn't work.

I installed squid from rpm in RedHat 7 so I'm not sure if the default paths are changed?

Thanks,

Cooper Waldon l Network Engineer l OTN l 416-446-4110 x 4473 l www.otn.ca<http://www.otn.ca/> | Service Desk 1-855-654-0888 x2

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20170328/02efa123/attachment.htm>

From frio_cervesa at hotmail.com  Tue Mar 28 22:07:30 2017
From: frio_cervesa at hotmail.com (senor)
Date: Tue, 28 Mar 2017 22:07:30 +0000
Subject: [squid-users] https_port and capath
Message-ID: <BN6PR17MB114004712BDD997506282D99F7320@BN6PR17MB1140.namprd17.prod.outlook.com>

Previous questions on this list referred to using the capath= option to https_port directive to fill in certificates missing in the chain to the Root CA trusted by the clients. I can not seem to get that to work.

I see no error in parsing even with debug on (debug section 3,9). The directive is read and no error produced but also no hint that the file pointed to by capath is used for anything. The SSL negotiation is not changed. The same 2 certs are passed. Just the signing cert and the signed cert.

directive:
 https_port 192.168.12.10:8443 intercept ssl-bump cert=/etc/squid/mitm.crt key=/etc/squid/mitm.key cafile=/etc/squid/mitm_chain.crt generate-host-certificates=on dynamic_cert_mem_cache_size=32MB name=mitm

The RootCA.crt is trusted by clients.
The Root CA signed intermediate1
Intermediate1 signed intermediate2
cert=intermediate2
cafile=intermediate1

This command succeeds:
openssl verify -CAfile RootCA.crt -untrusted intermediate1.crt intermediateL2.crt
If the untrusted intermediate1 is added to client the MITM works.

I realize this wouldn't be used very often and I'd prefer not using it myself but it is necessary in this case. 
Any hints?
Thanks in advance,
Senor

From squid-user at tlinx.org  Tue Mar 28 23:55:02 2017
From: squid-user at tlinx.org (L A Walsh)
Date: Tue, 28 Mar 2017 16:55:02 -0700
Subject: [squid-users] hsc-dynamic-cache: relied on storeID rules?
	Removed	in 3.5.20?
In-Reply-To: <042c01d2a736$83db5df0$8b9219d0$@ngtech.co.il>
References: <58D96750.3010405@tlinx.org>
 <042c01d2a736$83db5df0$8b9219d0$@ngtech.co.il>
Message-ID: <58DAF7D6.3030607@tlinx.org>

Eliezer Croitoru wrote:
> Hey Linda,
> 
> As the pathcer\author of StoreID I will try to clarify what might seems odd.
> StoreID is a "static" rule which is one of the squid cache fundamentals.
> The feature is the option to tweak this internal cache object ID.
> This is a very static feature and will not be changed for a very long time from now on.
> Most of the public helpers I have seen are very "simple" and rely on very simple things.
----
	Makes sense, otherwise too prone to breakage.



> 
> But since many systems these days are aware of the option to predict what would be the next url(think about an exe or other binary that can be replaced on-the-fly\in-transit) the developers changed and change(like a Diffe Hellman) their way of transporting content to the end client.
> Due to this "Diffe Hellman" feature that many added it makes the more simple scripts useless since the developers became much smarter.
----
	yeah, my use case was fairly simple -- same
person w/same browser, watching same vid a 2nd time.

	They gave me many "kudos" and noticed that
youtube was noticeably faster to browse through when I
implemented the SSL interception on the squid proxy
that web traffic goes through.  In that case, it was mainly
the caching of the video-thumbs that noticeably sped up
moving through YT pages.

> Indeed you will see TCP_MISS and it won't cache but this is only since the admin might have the illusion that an encrypted content can be predicted when a Diffie  Helman cryptography is creating the plain url's these days.
----
	Oh yeah.  Have noted that there are an infinite number
of ways to access the same URL and have thought about ways 
I might collapse them to 1 URL, but it's just idle thinking as
other things on the plate.

	One good idea that didn't get updated was an
extension in FF, that tried to store some of the latest
Javascript libs that sites used so if they asked for the lib
from a common site (like jquery), it might return the result
from a local cache.  

It wouldn't help for those sites that merge
multiple JS files and minify them.

But many sites have 15-20 different websites that are 
"included" to get different elements (fonts, stylesheets,
JS libs, etc) from different sources.  They seem to
include URL's like a developer would use
#include files...(and often take forever to load).

multiple elements from different URLs like they would
use multiple header include files in a local compilation.


> Hope It helped,
> Eliezer

Thanks for the explanation, certainly more useful
than just telling someone:

 "the web broke it"... 
:-)







From squid3 at treenet.co.nz  Wed Mar 29 01:07:53 2017
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Wed, 29 Mar 2017 14:07:53 +1300
Subject: [squid-users] Problem with basic_ldap_auth
In-Reply-To: <CAJxj0KAcgig3bj85zkFDMpkMoGkdm1KLqtXHaQBtdmDCa0MKQA@mail.gmail.com>
References: <CAJxj0KAcgig3bj85zkFDMpkMoGkdm1KLqtXHaQBtdmDCa0MKQA@mail.gmail.com>
Message-ID: <85b32f70-deae-7aac-ae38-ec64c855e59c@treenet.co.nz>

On 29/03/2017 2:54 a.m., Max Giesbert wrote:
> Hi everyone,
> 
> I am running Squid from the repos on Ubuntu 16.04. I can login to Squid
> using basic_nsca_auth without problems. When I replace the according line
> using basic_ldap_auth I am unable to login. The browser keeps requesting
> the user credentials over and over again.
> 
> If I run the command in a shell it works as expected. I type:
> 
> /usr/lib/squid/basic_ldap_auth -v 3 -b ou="MYOU",dc=DOMAINNAME,dc=TLD -D
> ldap_squid at MYDOMAIN -w "THEPASSWORD" -f sAMAccountName=%s -h MYHOST
> 
> Then I type username and password separated by a space and it gives "OK" or
> ERR if wrong credentials are provided.
> 
> Any hints what could be wrong? How can I debug further what actually is
> happened behind the scenes?
> 

Why do you have quotes around the MYOU ?
 If that includes spaces or special characters you need quotes around
the whole "ou=...dc=TLD" parameter for Squid to pass it to the helper as
a single command line option.

Amos



From yvoinov at gmail.com  Wed Mar 29 19:28:59 2017
From: yvoinov at gmail.com (Yuri Voinov)
Date: Thu, 30 Mar 2017 01:28:59 +0600
Subject: [squid-users] hsc-dynamic-cache: relied on storeID rules?
 Removed in 3.5.20?
In-Reply-To: <58DAF7D6.3030607@tlinx.org>
References: <58D96750.3010405@tlinx.org>
 <042c01d2a736$83db5df0$8b9219d0$@ngtech.co.il> <58DAF7D6.3030607@tlinx.org>
Message-ID: <3ab15214-83e4-3637-ce1e-4cd9df53b8f8@gmail.com>



29.03.2017 5:55, L A Walsh ?????:
> Eliezer Croitoru wrote:
>> Hey Linda,
>>
>> As the pathcer\author of StoreID I will try to clarify what might
>> seems odd.
>> StoreID is a "static" rule which is one of the squid cache fundamentals.
>> The feature is the option to tweak this internal cache object ID.
>> This is a very static feature and will not be changed for a very long
>> time from now on.
>> Most of the public helpers I have seen are very "simple" and rely on
>> very simple things.
> ----
>     Makes sense, otherwise too prone to breakage.
Don't think so. More effective (and complex) solutions just not free
(and, of course, open-source). I don't think that C++ code is easy to
breakage. :)
>
>
>
>>
>> But since many systems these days are aware of the option to predict
>> what would be the next url(think about an exe or other binary that
>> can be replaced on-the-fly\in-transit) the developers changed and
>> change(like a Diffe Hellman) their way of transporting content to the
>> end client.
>> Due to this "Diffe Hellman" feature that many added it makes the more
>> simple scripts useless since the developers became much smarter.
> ----
>     yeah, my use case was fairly simple -- same
> person w/same browser, watching same vid a 2nd time.
>
>     They gave me many "kudos" and noticed that
> youtube was noticeably faster to browse through when I
> implemented the SSL interception on the squid proxy
> that web traffic goes through.  In that case, it was mainly
> the caching of the video-thumbs that noticeably sped up
> moving through YT pages.
YT now is very different thing. As I told much times, YT is actively
opposite caching (to force ISP uses Google Cache) by shuffling
underlying CDN URLs and/or encrypts parts of URL. So, it is very
difficult to make YT videos cacheable in runtime this time. Just
possible to cache static YT pages (and, so, not at all).
>
>> Indeed you will see TCP_MISS and it won't cache but this is only
>> since the admin might have the illusion that an encrypted content can
>> be predicted when a Diffie  Helman cryptography is creating the plain
>> url's these days.
> ----
>     Oh yeah.  Have noted that there are an infinite number
> of ways to access the same URL and have thought about ways I might
> collapse them to 1 URL, but it's just idle thinking as
> other things on the plate.
>
>     One good idea that didn't get updated was an
> extension in FF, that tried to store some of the latest
> Javascript libs that sites used so if they asked for the lib
> from a common site (like jquery), it might return the result
> from a local cache. 
> It wouldn't help for those sites that merge
> multiple JS files and minify them.
>
> But many sites have 15-20 different websites that are "included" to
> get different elements (fonts, stylesheets,
> JS libs, etc) from different sources.  They seem to
In this case Store ID is useful. You simple write regex which combines
this several URLs to one.
> include URL's like a developer would use
> #include files...(and often take forever to load).
>
> multiple elements from different URLs like they would
> use multiple header include files in a local compilation.
>
>
>> Hope It helped,
>> Eliezer
>
> Thanks for the explanation, certainly more useful
> than just telling someone:
>
> "the web broke it"... :-)
It is hardly an explanation that will help to solve a specific question.
It takes some effort, often very big. And - yes, the web actively
opposes caching, or at least does not worry about caching at all. Is it
any wonder that those who could solve this problem not hurry to spread
the results of hard works free of charge around the world? :)
>
>
>
>
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users

-- 
Bugs to the Future
-------------- next part --------------
A non-text attachment was scrubbed...
Name: 0x613DEC46.asc
Type: application/pgp-keys
Size: 2437 bytes
Desc: not available
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20170330/f0518477/attachment.key>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: signature.asc
Type: application/pgp-signature
Size: 473 bytes
Desc: OpenPGP digital signature
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20170330/f0518477/attachment.sig>

From ivan.kolesnikov.fkn at gmail.com  Thu Mar 30 08:37:28 2017
From: ivan.kolesnikov.fkn at gmail.com (Ivan Kolesnikov)
Date: Thu, 30 Mar 2017 11:37:28 +0300
Subject: [squid-users] ECAP: How to Add header to get request
Message-ID: <CAE+Q0hVTx_w6Z-VSNCh9oikGa2tt4sySDroFf5W2YDbLjxdfkA@mail.gmail.com>

Hi Everyone,

I need add a Cookie header for some get requests via SQUID.
I can add: 'request_header_add Cookie "My_cookie_value" all' in squid.conf,
but in this case Cookie header will be add for all requests and I can't
manipulate with my "My_cookie_value". I updated adapter_modifying.cc and
add the following code in Adapter::Xaction::start() function:
static const libecap::Name name_cookie("Cookie");
const libecap::Header::Value value_cookie =
libecap::Area::FromTempString("video_key=My_cookie_value");
        adapted->header().add(name_cookie, value_cookie);
In that case Cookie header was add in all responses.

Please see my squid.conf:
loadable_modules /usr/local/lib/ecap/ecap_adapter_modifying.so
ecap_enable on
ecap_service ecapModifier respmod_precache \
        uri=ecap://e-cap.org/ecap/services/sample/modifying \
        victim=awerewrewrewrwerefbfcvglkflds9349rdsgfdk9dfgkj95tfnvxcncbnbv
\
        replacement=$$$$$$$
adaptation_access ecapModifier allow all


Please advice how to correctly add header in get requests?


Best Regards,
Ivan Kolesnikov
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20170330/cdea44ba/attachment.htm>

From uhlar at fantomas.sk  Thu Mar 30 08:43:32 2017
From: uhlar at fantomas.sk (Matus UHLAR - fantomas)
Date: Thu, 30 Mar 2017 10:43:32 +0200
Subject: [squid-users] SMP and AUFS
In-Reply-To: <cd47a96b-357d-8cfd-41e4-d4d376da10c1@measurement-factory.com>
References: <BN6PR17MB11408ADA794A592AD0CA5B5AF7390@BN6PR17MB1140.namprd17.prod.outlook.com>
 <ecd815a6-b464-d22d-a3a0-4e8b2fa18203@measurement-factory.com>
 <BN6PR17MB1140460613B0187B1CE191C6F73B0@BN6PR17MB1140.namprd17.prod.outlook.com>
 <579d3509-99c6-417e-ae1f-3f50402510a9@measurement-factory.com>
 <20170320161516.GB26154@fantomas.sk>
 <cd47a96b-357d-8cfd-41e4-d4d376da10c1@measurement-factory.com>
Message-ID: <20170330084332.GA21951@fantomas.sk>

>On 03/20/2017 10:15 AM, Matus UHLAR - fantomas wrote:
>> Well, I personally will still be curious how much does SMP affect the
>> case of one worker and one or more diskers...

On 20.03.17 12:19, Alex Rousskov wrote:
>I do not understand why you are asking this question in AUFS context.
>AUFS does not use diskers!

because it's diskers (rock store) who switch to SMP mode when "workers 1" is
used, and this may cause troubles to *ufs.

> Today, only Rock store uses diskers (in SMP
>mode). Some other [ufs-based] cache stores use various helper threads
>and processes for I/O as well, but those helper processes are not
>diskers or even kids in SMP terminology.

this is something I'd like to understand better...

>> do diskers only provide I/O to the requestor?

>Diskers primary function is low-level disk cache I/O. Like all kids,
>diskers respond to cache manager requests and Squid management events
>(e.g. shutdown and reconfiguration). IIRC, diskers also build in-RAM
>cache_dir index.
>
>    http://wiki.squid-cache.org/Features/SmpScale#Terminology

their own or also the shared one?

-- 
Matus UHLAR - fantomas, uhlar at fantomas.sk ; http://www.fantomas.sk/
Warning: I wish NOT to receive e-mail advertising to this address.
Varovanie: na tuto adresu chcem NEDOSTAVAT akukolvek reklamnu postu.
- Holmes, what kind of school did you study to be a detective?
- Elementary, Watson.  -- Daffy Duck & Porky Pig


From squid3 at treenet.co.nz  Thu Mar 30 09:49:51 2017
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 30 Mar 2017 22:49:51 +1300
Subject: [squid-users] SMP and AUFS
In-Reply-To: <20170330084332.GA21951@fantomas.sk>
References: <BN6PR17MB11408ADA794A592AD0CA5B5AF7390@BN6PR17MB1140.namprd17.prod.outlook.com>
 <ecd815a6-b464-d22d-a3a0-4e8b2fa18203@measurement-factory.com>
 <BN6PR17MB1140460613B0187B1CE191C6F73B0@BN6PR17MB1140.namprd17.prod.outlook.com>
 <579d3509-99c6-417e-ae1f-3f50402510a9@measurement-factory.com>
 <20170320161516.GB26154@fantomas.sk>
 <cd47a96b-357d-8cfd-41e4-d4d376da10c1@measurement-factory.com>
 <20170330084332.GA21951@fantomas.sk>
Message-ID: <f3c3da0e-9cdd-fcd9-a33f-896fc12d3ea6@treenet.co.nz>

On 30/03/2017 9:43 p.m., Matus UHLAR - fantomas wrote:
>> On 03/20/2017 10:15 AM, Matus UHLAR - fantomas wrote:
>>> Well, I personally will still be curious how much does SMP affect the
>>> case of one worker and one or more diskers...
> 
> On 20.03.17 12:19, Alex Rousskov wrote:
>> I do not understand why you are asking this question in AUFS context.
>> AUFS does not use diskers!
> 
> because it's diskers (rock store) who switch to SMP mode when "workers
> 1" is
> used, and this may cause troubles to *ufs.
> 
>> Today, only Rock store uses diskers (in SMP
>> mode). Some other [ufs-based] cache stores use various helper threads
>> and processes for I/O as well, but those helper processes are not
>> diskers or even kids in SMP terminology.
> 
> this is something I'd like to understand better...

Alex is referring to;

a) "diskd" cache_dir type, which uses a helper named 'diskd' to offload
parts of the file manipulations to that helper. (I've not looked closely
to see which, sorry).

b) squid builds with 'unlinkd' feature enabled, which uses an helper
called 'unlinkd' to perform file deletion outside the squid process.

c) AUFS which starts threads to perform the read/write I/O and file
deletion stuff.

... and I'm still learning of other such odd things in weird rarely
touched corners. There is something thread-like in digest creation for
walking the store index quickly and swap log rebuilding for example.


> 
>>> do diskers only provide I/O to the requestor?
> 
>> Diskers primary function is low-level disk cache I/O. Like all kids,
>> diskers respond to cache manager requests and Squid management events
>> (e.g. shutdown and reconfiguration). IIRC, diskers also build in-RAM
>> cache_dir index.
>>
>>    http://wiki.squid-cache.org/Features/SmpScale#Terminology
> 
> their own or also the shared one?

Alex may correct me but AFAIK there is currently only the shared index.
Diskers just do better with it through generic parallel-CPU efficiencies
than a non-SMP Squid would spreading the index actions between other
transaction work.

Amos



From squid3 at treenet.co.nz  Thu Mar 30 09:56:40 2017
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 30 Mar 2017 22:56:40 +1300
Subject: [squid-users] ECAP: How to Add header to get request
In-Reply-To: <CAE+Q0hVTx_w6Z-VSNCh9oikGa2tt4sySDroFf5W2YDbLjxdfkA@mail.gmail.com>
References: <CAE+Q0hVTx_w6Z-VSNCh9oikGa2tt4sySDroFf5W2YDbLjxdfkA@mail.gmail.com>
Message-ID: <2577d80e-cd52-6be2-7830-69c4c6dfe13a@treenet.co.nz>

On 30/03/2017 9:37 p.m., Ivan Kolesnikov wrote:
> 
> Please advice how to correctly add header in get requests?

FYI: This is best directed towards the actual e-CAP support location(s)
mentioned in <http://e-cap.org/Support>.

Support from here is more topics about how to configure Squid to use
eCAP modules, and which ones are most useful for whatever the task is.
Code questions (even about Squid code) are technically off-topic.

HTH
Amos



From rousskov at measurement-factory.com  Thu Mar 30 14:28:19 2017
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Thu, 30 Mar 2017 08:28:19 -0600
Subject: [squid-users] ECAP: How to Add header to get request
In-Reply-To: <2577d80e-cd52-6be2-7830-69c4c6dfe13a@treenet.co.nz>
References: <CAE+Q0hVTx_w6Z-VSNCh9oikGa2tt4sySDroFf5W2YDbLjxdfkA@mail.gmail.com>
 <2577d80e-cd52-6be2-7830-69c4c6dfe13a@treenet.co.nz>
Message-ID: <56ed8d4d-c79f-f5e3-240c-440f385df9ca@measurement-factory.com>

On 03/30/2017 03:56 AM, Amos Jeffries wrote:
> On 30/03/2017 9:37 p.m., Ivan Kolesnikov wrote:
>>
>> Please advice how to correctly add header in get requests?
>
> FYI: This is best directed towards the actual e-CAP support location(s)
> mentioned in <http://e-cap.org/Support>.

Yes, general eCAP questions should not be discussed here. However, 
Ivan's problem (or at least one of the problems) is with Squid 
configuration, which is appropriate for this mailing list.

 >> ecap_service ecapModifier respmod_precache

To modify requests, use reqmod_precache vectoring point. If you need to 
modify requests and responses, then use both reqmod_precache and 
respmod_precache vectoring points.

Alex.


From giesbert at exactt.de  Thu Mar 30 14:46:31 2017
From: giesbert at exactt.de (Max Giesbert)
Date: Thu, 30 Mar 2017 14:46:31 +0000
Subject: [squid-users] Problem with basic_ldap_auth
In-Reply-To: <85b32f70-deae-7aac-ae38-ec64c855e59c@treenet.co.nz>
References: <CAJxj0KAcgig3bj85zkFDMpkMoGkdm1KLqtXHaQBtdmDCa0MKQA@mail.gmail.com>
 <85b32f70-deae-7aac-ae38-ec64c855e59c@treenet.co.nz>
Message-ID: <CAJxj0KB79WUP63AzQsXT6DTiKsxvQOmMjh+TsAUWBkGvAwJcNw@mail.gmail.com>

Thank you so much! That did the trick. You made my day :-D

Amos Jeffries <squid3 at treenet.co.nz> schrieb am Mi., 29. M?rz 2017 um
03:08 Uhr:

> On 29/03/2017 2:54 a.m., Max Giesbert wrote:
> > Hi everyone,
> >
> > I am running Squid from the repos on Ubuntu 16.04. I can login to Squid
> > using basic_nsca_auth without problems. When I replace the according line
> > using basic_ldap_auth I am unable to login. The browser keeps requesting
> > the user credentials over and over again.
> >
> > If I run the command in a shell it works as expected. I type:
> >
> > /usr/lib/squid/basic_ldap_auth -v 3 -b ou="MYOU",dc=DOMAINNAME,dc=TLD -D
> > ldap_squid at MYDOMAIN -w "THEPASSWORD" -f sAMAccountName=%s -h MYHOST
> >
> > Then I type username and password separated by a space and it gives "OK"
> or
> > ERR if wrong credentials are provided.
> >
> > Any hints what could be wrong? How can I debug further what actually is
> > happened behind the scenes?
> >
>
> Why do you have quotes around the MYOU ?
>  If that includes spaces or special characters you need quotes around
> the whole "ou=...dc=TLD" parameter for Squid to pass it to the helper as
> a single command line option.
>
> Amos
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>
-- 
Dipl. Inf. (FH) Max Giesbert
Gesch?ftsf?hrer

exactt technology GmbH

Mobil: +49 17 75 07 53 44
Festnetz: +49 89 38 15 64 42 1
Fax: +49 89 38 15 64 42 5
Web: http://exactt.de

F?ustlestra?e 3
80339 M?nchen
Germany

Fernwartung:
https://get.teamviewer.com/y5kz3vc

Sitz der Gesellschaft: M?nchen
Registergericht: Amtsgericht M?nchen, HRB 213444
Gesch?ftsf?hrer: Max Giesbert, Roman Weiss
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20170330/a0846065/attachment.htm>

From eric.veirasgalisson at gmail.com  Thu Mar 30 15:01:20 2017
From: eric.veirasgalisson at gmail.com (Eric Veiras Galisson)
Date: Thu, 30 Mar 2017 17:01:20 +0200
Subject: [squid-users] HTTPS reverse proxy: SSL Certficate verification
	failed
Message-ID: <CAJmUcmbfVqJGWpetdEqtFxfMpfbgVi7niRU+cr1_Fk9wNVAwaQ@mail.gmail.com>

Hello,

I want to setup Squid as a HTTPS reverse proxy for several of our websites,
but I have a certificate verification problem on Squid access.log.
Our upstream webservers are behind a VPN tunnel and only the Squid server
can access it. (*We actually use Nginx for the same purpose but want to
switch to Squid)*

                              HTTPS                           HTTPS
[client browser] -----------------------> [Squid]
--------------------------> [upstream server]


I run squid 3.4.8-6+deb8u4 recompiled with --enable-ssl
--with-open-ssl="/etc/ssl/openssl.cnf" on Debian Jessie.

The certificate presented to the client is the same as on the upstream
server, a wildcard one signed by GeoTrust (with intermediate CA). It
appears correctly in the browser.
The problem comes from squid verification of upstream certificate.

My basic squid.conf looks like

https_port <squid IP>:443 accel defaultsite=upstream1.domain.tld vhost
cert=<path to SSL cert>

cache_peer <upstream IP> parent 443 0 no-query originserver name=upstream1
ssl

acl upstream1 dstdomain upstream1.domain.tld
cache_peer_access upstream1 allow upstream1

And logs are full of

fwdNegotiateSSL: Error negotiating SSL connection on FD 14:
error:14090086:SSL routines:SSL3_GET_SERVER_CERTIFICATE:certificate verify
failed (1/-1/0)
TCP connection to <upstream IP> failed


If I verify with openssl the upstream server, I got an error but if I give
it the intermediary CA certificate (to be precise I give it the full chain
concatenated in one file), it's OK.

$ openssl s_client -showcerts -connect upstream.domain.tld:443 -CAfile
<path to full cert chain>.pem
CONNECTED(00000003)
depth=2 C = US, O = GeoTrust Inc., CN = GeoTrust Global CA
verify return:1
depth=1 C = US, O = GeoTrust Inc., CN = RapidSSL SHA256 CA
verify return:1
depth=0 CN = *.fraudbuster.mobi
verify return:1

...

    Timeout   : 300 (sec)
    Verify return code: 0 (ok)


In squid, I tried several options for cache_peer (sslcapath and
sslcafile...) but I keep having this error. Of course the
sslflags=DONT_VERIFY_PEER,DONT_VERIFY_DOMAIN options solve the problem, but
I don't want to use this solution (my certificate is legitimate and want to
validate it normally).

What am I doing wrong? and what should I do to make squid work in this
setup?

Thanks.

Eric.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20170330/8bd2b25f/attachment.htm>

From rousskov at measurement-factory.com  Thu Mar 30 15:13:26 2017
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Thu, 30 Mar 2017 09:13:26 -0600
Subject: [squid-users] SMP and AUFS
In-Reply-To: <20170330084332.GA21951@fantomas.sk>
References: <BN6PR17MB11408ADA794A592AD0CA5B5AF7390@BN6PR17MB1140.namprd17.prod.outlook.com>
 <ecd815a6-b464-d22d-a3a0-4e8b2fa18203@measurement-factory.com>
 <BN6PR17MB1140460613B0187B1CE191C6F73B0@BN6PR17MB1140.namprd17.prod.outlook.com>
 <579d3509-99c6-417e-ae1f-3f50402510a9@measurement-factory.com>
 <20170320161516.GB26154@fantomas.sk>
 <cd47a96b-357d-8cfd-41e4-d4d376da10c1@measurement-factory.com>
 <20170330084332.GA21951@fantomas.sk>
Message-ID: <62553c7f-75d4-baf1-4062-3e2d5f7edf41@measurement-factory.com>

On 03/30/2017 02:43 AM, Matus UHLAR - fantomas wrote:
>> On 03/20/2017 10:15 AM, Matus UHLAR - fantomas wrote:
>>> Well, I personally will still be curious how much does SMP affect the
>>> case of one worker and one or more diskers...
>
> On 20.03.17 12:19, Alex Rousskov wrote:
>> I do not understand why you are asking this question in AUFS context.
>> AUFS does not use diskers!

> because it's diskers (rock store) who switch to SMP mode when "workers
> 1" is used, and this may cause troubles to *ufs.

Diskers do not cause trouble -- they are doing exactly what they have 
been designed to do. It is ufs (being SMP-unaware) that causes trouble 
in SMP mode when admins enable ufs-based cache_dirs in SMP configurations.


>> Diskers primary function is low-level disk cache I/O. Like all kids,
>> diskers respond to cache manager requests and Squid management events
>> (e.g. shutdown and reconfiguration). IIRC, diskers also build in-RAM
>> cache_dir index.
>>
>>    http://wiki.squid-cache.org/Features/SmpScale#Terminology

> their own or also the shared one?

Their own, which is shared among all workers. In many contexts, one can 
view the set of all disk-specific shared indexes as one big shared disk 
index. Please note that my recollection that diskers build the in-RAM 
cache_dir index may be wrong -- this behavior may have changed, and I 
have not checked the code or commit logs. It would certainly make sense 
for diskers to build their indexes, but I do not remember whether we 
have implemented that optimization.


HTH,

Alex.


From juandebas at gmail.com  Thu Mar 30 16:55:09 2017
From: juandebas at gmail.com (Juande)
Date: Thu, 30 Mar 2017 09:55:09 -0700 (PDT)
Subject: [squid-users] Using client certificate for all connection
Message-ID: <1490892909667-4681942.post@n4.nabble.com>

Hi

I want to configure squid so every request through the proxy get client
certificate authenticated.

I need some automatic software audit tools to access to a server that uses
client certificates to access to its contents.

Any suggestions?

BR
Juan



--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/Using-client-certificate-for-all-connection-tp4681942.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From Antony.Stone at squid.open.source.it  Thu Mar 30 20:52:24 2017
From: Antony.Stone at squid.open.source.it (Antony Stone)
Date: Thu, 30 Mar 2017 22:52:24 +0200
Subject: [squid-users] Using client certificate for all connection
In-Reply-To: <1490892909667-4681942.post@n4.nabble.com>
References: <1490892909667-4681942.post@n4.nabble.com>
Message-ID: <201703302252.25018.Antony.Stone@squid.open.source.it>

On Thursday 30 March 2017 at 18:55:09, Juande wrote:

> Hi
> 
> I want to configure squid so every request through the proxy get client
> certificate authenticated.
> 
> I need some automatic software audit tools to access to a server that uses
> client certificates to access to its contents.

Are you saying that you want all client requests, to any server, to be 
authenticated by Squid (or a helper) for the client certificate?

Or are you saying that all requests to a specific server are required to be 
authenticated by client certificate, and Squid is supposed to supply this 
certificate (because the client itself cannot)?


Antony.

-- 
"The tofu battle I saw last weekend was quite brutal."

 - Marija Danute Brigita Kuncaitis

                                                   Please reply to the list;
                                                         please *don't* CC me.


From partyposse at mail.com  Fri Mar 31 01:24:05 2017
From: partyposse at mail.com (dakotamartinez)
Date: Thu, 30 Mar 2017 18:24:05 -0700 (PDT)
Subject: [squid-users] MAXHTTPORTS CentOS 6
Message-ID: <1490923445770-4681944.post@n4.nabble.com>

Hi,

I'm new the forum. But i'm a sneaker website proxy seller and I use /24
subnets with squid in order to connect to these websites. I configure these
servers with different ports as well as outgoing IP's. After applying all my
ports and everything I get this in my cache log:

"WARNING: You have too many 'http_port' lines.
         The limit is 128"

I can't seem to figure it out. Is there a way to change the max ports value
in the CentOS operating system? I'm sort of a beginner user with squid. I
don't know too much. If there is, could somebody guide me through it.

Thanks,

Dakota





--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/MAXHTTPORTS-CentOS-6-tp4681944.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From eliezer at ngtech.co.il  Fri Mar 31 01:55:05 2017
From: eliezer at ngtech.co.il (Eliezer  Croitoru)
Date: Fri, 31 Mar 2017 04:55:05 +0300
Subject: [squid-users] Using client certificate for all connection
In-Reply-To: <1490892909667-4681942.post@n4.nabble.com>
References: <1490892909667-4681942.post@n4.nabble.com>
Message-ID: <0b0901d2a9c1$d179fa20$746dee60$@ngtech.co.il>

As far my understanding goes squid doesn't have this function yet.
Maybe if you will put haproxy(not sure) infront of squid you might be able to achieve your goal.

Eliezer

----
Eliezer Croitoru
Linux System Administrator
Mobile: +972-5-28704261
Email: eliezer at ngtech.co.il



-----Original Message-----
From: squid-users [mailto:squid-users-bounces at lists.squid-cache.org] On Behalf Of Juande
Sent: Thursday, March 30, 2017 7:55 PM
To: squid-users at lists.squid-cache.org
Subject: [squid-users] Using client certificate for all connection

Hi

I want to configure squid so every request through the proxy get client certificate authenticated.

I need some automatic software audit tools to access to a server that uses client certificates to access to its contents.

Any suggestions?

BR
Juan



--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/Using-client-certificate-for-all-connection-tp4681942.html
Sent from the Squid - Users mailing list archive at Nabble.com.
_______________________________________________
squid-users mailing list
squid-users at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-users



From eliezer at ngtech.co.il  Fri Mar 31 01:55:37 2017
From: eliezer at ngtech.co.il (Eliezer  Croitoru)
Date: Fri, 31 Mar 2017 04:55:37 +0300
Subject: [squid-users] MAXHTTPORTS CentOS 6
In-Reply-To: <1490923445770-4681944.post@n4.nabble.com>
References: <1490923445770-4681944.post@n4.nabble.com>
Message-ID: <0b0b01d2a9c1$e4b64b20$ae22e160$@ngtech.co.il>

Only by compiling from source.

Eliezer

----
Eliezer Croitoru
Linux System Administrator
Mobile: +972-5-28704261
Email: eliezer at ngtech.co.il



-----Original Message-----
From: squid-users [mailto:squid-users-bounces at lists.squid-cache.org] On Behalf Of dakotamartinez
Sent: Friday, March 31, 2017 4:24 AM
To: squid-users at lists.squid-cache.org
Subject: [squid-users] MAXHTTPORTS CentOS 6

Hi,

I'm new the forum. But i'm a sneaker website proxy seller and I use /24
subnets with squid in order to connect to these websites. I configure these
servers with different ports as well as outgoing IP's. After applying all my
ports and everything I get this in my cache log:

"WARNING: You have too many 'http_port' lines.
         The limit is 128"

I can't seem to figure it out. Is there a way to change the max ports value
in the CentOS operating system? I'm sort of a beginner user with squid. I
don't know too much. If there is, could somebody guide me through it.

Thanks,

Dakota





--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/MAXHTTPORTS-CentOS-6-tp4681944.html
Sent from the Squid - Users mailing list archive at Nabble.com.
_______________________________________________
squid-users mailing list
squid-users at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-users



From partyposse at mail.com  Fri Mar 31 01:55:20 2017
From: partyposse at mail.com (dakotamartinez)
Date: Thu, 30 Mar 2017 18:55:20 -0700 (PDT)
Subject: [squid-users] MAXHTTPORTS CentOS 6
In-Reply-To: <0b0b01d2a9c1$e4b64b20$ae22e160$@ngtech.co.il>
References: <1490923445770-4681944.post@n4.nabble.com>
 <0b0b01d2a9c1$e4b64b20$ae22e160$@ngtech.co.il>
Message-ID: <1490925320544-4681947.post@n4.nabble.com>

Is there a way I could go about doing this? I've read some stuff in similar
forums but haven't seen a direct answer.

Dakota



--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/MAXHTTPORTS-CentOS-6-tp4681944p4681947.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From squid3 at treenet.co.nz  Fri Mar 31 02:44:28 2017
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Fri, 31 Mar 2017 15:44:28 +1300
Subject: [squid-users] HTTPS reverse proxy: SSL Certficate verification
 failed
In-Reply-To: <CAJmUcmbfVqJGWpetdEqtFxfMpfbgVi7niRU+cr1_Fk9wNVAwaQ@mail.gmail.com>
References: <CAJmUcmbfVqJGWpetdEqtFxfMpfbgVi7niRU+cr1_Fk9wNVAwaQ@mail.gmail.com>
Message-ID: <a9bc23dc-5638-7512-0d2f-738c954caef3@treenet.co.nz>

On 31/03/2017 4:01 a.m., Eric Veiras Galisson wrote:
> Hello,
> 
> I want to setup Squid as a HTTPS reverse proxy for several of our websites,
> but I have a certificate verification problem on Squid access.log.
> Our upstream webservers are behind a VPN tunnel and only the Squid server
> can access it. (*We actually use Nginx for the same purpose but want to
> switch to Squid)*
> 
>                               HTTPS                           HTTPS
> [client browser] -----------------------> [Squid]
> --------------------------> [upstream server]
> 
> 
> I run squid 3.4.8-6+deb8u4 recompiled with --enable-ssl
> --with-open-ssl="/etc/ssl/openssl.cnf" on Debian Jessie.
> 
> The certificate presented to the client is the same as on the upstream
> server, a wildcard one signed by GeoTrust (with intermediate CA). It
> appears correctly in the browser.
> The problem comes from squid verification of upstream certificate.
> 
...
> 
> What am I doing wrong? and what should I do to make squid work in this
> setup?

The server (and Squid) should be supplying the intermediate cert along
with its own cert for best validation behaviour.

If it cannot, use the cache_peer sslcafile= option to provide Squid with
a PEM file containing the public certs of the whole chain (excluding the
server cert itself). Order of those certs in the file is important. I've
forgotten which end of the chain to start with sorry, but it is one or
the other and definitely sequential.

When that is working you should use the sslflags=NO_DEFAULT_CA option to
prevent MITM on those connections altering the chain - and saves a huge
amount of memory :-).

Amos



From kevinmuehlparzer at hotmail.de  Fri Mar 31 08:22:11 2017
From: kevinmuehlparzer at hotmail.de (CrossfireAUT)
Date: Fri, 31 Mar 2017 01:22:11 -0700 (PDT)
Subject: [squid-users] Squid Authentication if URL is on a Blacklist from
	SquidGuard
Message-ID: <1490948531197-4681950.post@n4.nabble.com>

Hello Squid-Community!

I need your help with a rather non-standard config.
My aim is as following:
-> Users that use my proxy (will deploy it via group policy in AD) should be
able to use my proxy without authentication
-> if a user invokes SquidGuard (he wants to call up a URL on my
blacklists), he should get prompted for his username and password
-> only users of the AD-group webusers should be able to continue and go to
this site on the blacklist
I know, it isn't the best way to use SquidGuard, but a customer wants it
that way.

My current config is as following:
auth_param basic program /usr/lib/squid/basic_ldap_auth -R -b
"dc=xxxx,dc=local" -D testuser at xxxx.local -W /etc/squid/squid.secrets -f
sAMAccountName=%s -h 172.30.0.36
auth_param basic children 10
auth_param basic realm xxxx
auth_param basic credentialsttl 2 hours

external_acl_type webusers %LOGIN /usr/lib/squid/ext_ldap_group_acl -b
"dc=xxxx,dc=local" -D testuser at xxxx.local -W /etc/squid/squid.secrets -f
"(&(sAMAccountName=%v)(memberOf=cn=%a,cn=Users,dc=xxxx,dc=local))" -h
172.30.0.36

authenticate_ip_ttl 1 second




acl auth proxy_auth REQUIRED
acl no_webusers dstdomain .xxxx.at
acl ldapgroup_webusers external webusers webusers

acl SSL_ports port 443
acl Safe_ports port 80          # http
acl Safe_ports port 21          # ftp
acl Safe_ports port 443         # https
acl Safe_ports port 70          # gopher
acl Safe_ports port 210         # wais
acl Safe_ports port 1025-65535  # unregistered ports
acl Safe_ports port 280         # http-mgmt
acl Safe_ports port 488         # gss-http
acl Safe_ports port 591         # filemaker
acl Safe_ports port 777         # multiling http
acl CONNECT method CONNECT


http_access deny !Safe_ports
http_access deny CONNECT !SSL_ports
http_access allow localhost manager
http_access deny manager

http_access deny !auth
http_access allow no_webusers

http_access allow ldapgroup_webuser

http_access deny all

http_port 3128


url_rewrite_program /usr/bin/squidGuard -c /etc/squidguard/squidGuard.conf
url_rewrite_children 4




So my users get prompted for their username/passwords everytime they restart
their browser.
If they call up a domain on my blacklists, they get ACCESS DENIED.

Does anyone know how you can achieve this?
Until know, I tried really hard, thought it would be a good idea to ask the
user-list!

Regards,
Kevin



--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/Squid-Authentication-if-URL-is-on-a-Blacklist-from-SquidGuard-tp4681950.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From juandebas at gmail.com  Fri Mar 31 08:39:01 2017
From: juandebas at gmail.com (Juande)
Date: Fri, 31 Mar 2017 01:39:01 -0700 (PDT)
Subject: [squid-users] Using client certificate for all connection
In-Reply-To: <201703302252.25018.Antony.Stone@squid.open.source.it>
References: <1490892909667-4681942.post@n4.nabble.com>
 <201703302252.25018.Antony.Stone@squid.open.source.it>
Message-ID: <1490949541069-4681951.post@n4.nabble.com>

Half and half. I need a way to client certificate authorize the requests from
my analyzer software that does not support certificate authentication, but
does support using a proxy. 

So I need that squid provides the certificate for all requests to all
servers. We have testing certificates that work in many servers, so I can
use the same certificate to authenticate in all of them.

Im already doing that with Owasp ZAP proxy, but I need a command line only
proxy to keep it running on my testing machine, and I thought that a
versatile proxy like squid would have this option.



--
View this message in context: http://squid-web-proxy-cache.1019090.n4.nabble.com/Using-client-certificate-for-all-connection-tp4681942p4681951.html
Sent from the Squid - Users mailing list archive at Nabble.com.


From eric.veirasgalisson at gmail.com  Fri Mar 31 15:42:04 2017
From: eric.veirasgalisson at gmail.com (Eric Veiras Galisson)
Date: Fri, 31 Mar 2017 17:42:04 +0200
Subject: [squid-users] HTTPS reverse proxy: SSL Certficate verification
	failed
In-Reply-To: <a9bc23dc-5638-7512-0d2f-738c954caef3@treenet.co.nz>
References: <CAJmUcmbfVqJGWpetdEqtFxfMpfbgVi7niRU+cr1_Fk9wNVAwaQ@mail.gmail.com>
 <a9bc23dc-5638-7512-0d2f-738c954caef3@treenet.co.nz>
Message-ID: <CAJmUcmarAkwGX7f7Gw4iBPBTA-Yc-b-OAeiLMXgpg9cQZGPmYA@mail.gmail.com>

On Fri, Mar 31, 2017 at 4:44 AM, Amos Jeffries <squid3 at treenet.co.nz> wrote:

> On 31/03/2017 4:01 a.m., Eric Veiras Galisson wrote:
> > Hello,
> >
> > I want to setup Squid as a HTTPS reverse proxy for several of our
> websites,
> > but I have a certificate verification problem on Squid access.log.
> > Our upstream webservers are behind a VPN tunnel and only the Squid server
> > can access it. (*We actually use Nginx for the same purpose but want to
> > switch to Squid)*
> >
> >                               HTTPS                           HTTPS
> > [client browser] -----------------------> [Squid]
> > --------------------------> [upstream server]
> >
> >
> > I run squid 3.4.8-6+deb8u4 recompiled with --enable-ssl
> > --with-open-ssl="/etc/ssl/openssl.cnf" on Debian Jessie.
> >
> > The certificate presented to the client is the same as on the upstream
> > server, a wildcard one signed by GeoTrust (with intermediate CA). It
> > appears correctly in the browser.
> > The problem comes from squid verification of upstream certificate.
> >
> ...
> >
> > What am I doing wrong? and what should I do to make squid work in this
> > setup?
>
> The server (and Squid) should be supplying the intermediate cert along
> with its own cert for best validation behaviour.
>

Both the server and squid (https_port cert= option) are actually using the
same certificate: a single file with priv key, server certificate and
intermediary cert CA.



> If it cannot, use the cache_peer sslcafile= option to provide Squid with
> a PEM file containing the public certs of the whole chain (excluding the
> server cert itself). Order of those certs in the file is important. I've
> forgotten which end of the chain to start with sorry, but it is one or
> the other and definitely sequential.
>
>
I changed cache_peer directive to add sslcafile option to a PEM file
containing root and intermediary CA certificate, in one order or the other.

And when verifying with openssl s_client -showcerts -connect
upstream1.domain.tld directly (no squid) or via squid, it's OK [1]

$ openssl s_client -showcerts -connect upstream.domain.tld:443
CONNECTED(00000003)
depth=2 C = US, O = GeoTrust Inc., CN = GeoTrust Global CA
verify return:1
depth=1 C = US, O = GeoTrust Inc., CN = RapidSSL SHA256 CA
verify return:1
depth=0 CN = *.domain.tld
verify return:1
---
    Verify return code: 0 (ok)


But I still have the same error when connecting to the website with a
browser.




> When that is working you should use the sslflags=NO_DEFAULT_CA option to
> prevent MITM on those connections altering the chain - and saves a huge
> amount of memory :-).
>
> Amos
>


[1] for this server at least, which is Apache 2.4 on Debian Jessie, old
Apache 2.2 on Debian Wheezy don't work, but it's another problem.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20170331/bd099ee1/attachment.htm>

