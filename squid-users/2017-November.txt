From duanyao at ustc.edu  Wed Nov  1 02:03:43 2017
From: duanyao at ustc.edu (duanyao)
Date: Wed, 1 Nov 2017 10:03:43 +0800
Subject: [squid-users] With Rock storage,
 it seems an object can not be copied from disk cache to memory
 cache after a restart
Message-ID: <0da1b165-4a46-6bf3-aab5-e12e8c9ed81d@ustc.edu>

Hi,

I have an issue: if I restart Squid (with Rock storage), I can no longer 
get TCP_MEM_HIT for any object got cached before that restart -- I 
always get TCP_HIT instead.


Steps to reproduce:

1) Stop squid.

2) Remove & create a new Rock storage, start Squid.

3) Get a url via Squid, e.g.:

   curl -o /dev/null -v -x localhost:3128 
http://cdn.bootcss.com/font-linux/0.9/font-linux.woff

   Suppose Squid is running on localhost:3128.

   This should get a TCP_MISS.

4) Repeat step 3, and this should get a TCP_MEM_HIT.

5) Stop & start Squid.

6) Repeat step 3 for more than 2 times, and all the results are TCP_HIT, 
however I expect the 2nd and following rounds should be TCP_MEM_HIT.


So it seems an object can never be copied from disk cache to memory 
cache after a restart. Is this normal?

I tested 3.5.27 and master branch in github, the results are same. The 
configuration file is attached.


Thanks.

Duan, Yao

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20171101/b4273683/attachment.htm>
-------------- next part --------------
workers 2

shutdown_lifetime 3 second

acl localnet src 10.0.0.0/8	# RFC1918 possible internal network
acl localnet src 172.16.0.0/12	# RFC1918 possible internal network
acl localnet src 192.168.0.0/16	# RFC1918 possible internal network
acl localnet src fc00::/7       # RFC 4193 local private network range
acl localnet src fe80::/10      # RFC 4291 link-local (directly plugged) machines

acl SSL_ports port 443
acl Safe_ports port 80		# http
acl Safe_ports port 21		# ftp
acl Safe_ports port 443		# https
acl Safe_ports port 70		# gopher
acl Safe_ports port 210		# wais
acl Safe_ports port 1025-65535	# unregistered ports
acl Safe_ports port 280		# http-mgmt
acl Safe_ports port 488		# gss-http
acl Safe_ports port 591		# filemaker
acl Safe_ports port 777		# multiling http
acl CONNECT method CONNECT

http_access deny !Safe_ports

http_access deny CONNECT !SSL_ports

acl PURGE method PURGE
http_access allow PURGE localhost
http_access deny PURGE

http_access allow localhost manager
http_access deny manager

http_access allow localnet
http_access allow localhost

http_access deny all

http_port 3128

cache_dir rock /home/duanyao/project/proxy/squid-root/var/cache/squid/rock 10

cache_effective_user duanyao


From rousskov at measurement-factory.com  Wed Nov  1 03:27:37 2017
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Tue, 31 Oct 2017 21:27:37 -0600
Subject: [squid-users] With Rock storage,
 it seems an object can not be copied from disk cache to memory
 cache after a restart
In-Reply-To: <0da1b165-4a46-6bf3-aab5-e12e8c9ed81d@ustc.edu>
References: <0da1b165-4a46-6bf3-aab5-e12e8c9ed81d@ustc.edu>
Message-ID: <e1ccabb8-69b6-6784-353e-02fbaf2e48eb@measurement-factory.com>

On 10/31/2017 08:03 PM, duanyao wrote:

> it seems an object can never be copied from disk cache to memory
> cache after a restart. Is this normal?

No, I do not think this is "normal". I do not remember intentionally
disabling such functionality to work around some other problem.

However, my recollection may be faulty, some unknown bug may block this
copying, and/or SMP Squid may lack some code to enable this copying.


> I tested 3.5.27 and master branch in github, the results are same.

I appreciate you testing master! If you get no useful responses,
consider filing a bug report and/or fixing this problem. It is an
important performance problem that should be fixed.


Thank you,

Alex.


From duanyao at ustc.edu  Wed Nov  1 03:40:26 2017
From: duanyao at ustc.edu (duanyao)
Date: Wed, 1 Nov 2017 11:40:26 +0800
Subject: [squid-users] With Rock storage,
 it seems an object can not be copied from disk cache to memory
 cache after a restart
In-Reply-To: <e1ccabb8-69b6-6784-353e-02fbaf2e48eb@measurement-factory.com>
References: <0da1b165-4a46-6bf3-aab5-e12e8c9ed81d@ustc.edu>
 <e1ccabb8-69b6-6784-353e-02fbaf2e48eb@measurement-factory.com>
Message-ID: <17704ba2-1796-942b-65ab-e49812093d4d@ustc.edu>

? 2017/11/1 ??11:27, Alex Rousskov ??:
> On 10/31/2017 08:03 PM, duanyao wrote:
>
>> it seems an object can never be copied from disk cache to memory
>> cache after a restart. Is this normal?
> No, I do not think this is "normal". I do not remember intentionally
> disabling such functionality to work around some other problem.
>
> However, my recollection may be faulty, some unknown bug may block this
> copying, and/or SMP Squid may lack some code to enable this copying.
>
>
>> I tested 3.5.27 and master branch in github, the results are same.
> I appreciate you testing master! If you get no useful responses,
> consider filing a bug report and/or fixing this problem. It is an
> important performance problem that should be fixed.

Ok, I'll file a bug first.

>
>
> Thank you,
>
> Alex.





From pheriko.support at gmail.com  Wed Nov  1 05:14:31 2017
From: pheriko.support at gmail.com (Periko Support)
Date: Tue, 31 Oct 2017 22:14:31 -0700
Subject: [squid-users] WIndows Server AD+Squid Integration: Query User
	Specs
In-Reply-To: <CAK2yrTbqWoBLW_XjAxGYYaoNf-1Yc=xAs+b=otR+_dUPxxnDFg@mail.gmail.com>
References: <CAK2yrTaty2uFRugOHOiKLZXbraePx1DCvrLm-hhxSSOK393Bxw@mail.gmail.com>
 <c3fa4c52-6366-51df-5549-c0c0a0e2b6d0@gmail.com>
 <CAK2yrTa+yxkNjNWcVzC=O6DkuvaNqnqt6M1vCeDU6NX1rcojbA@mail.gmail.com>
 <29846fb0-e69d-d1fe-f629-bf0bb3f2db46@gmail.com>
 <CAK2yrTbqWoBLW_XjAxGYYaoNf-1Yc=xAs+b=otR+_dUPxxnDFg@mail.gmail.com>
Message-ID: <CAK2yrTbzi8NeRqUo+7jZkZYP9-rgPpeJg0-1398JfQxJyFOjcg@mail.gmail.com>

Done, is working.
Thanks Yuri.

On Tue, Oct 31, 2017 at 1:36 PM, Periko Support
<pheriko.support at gmail.com> wrote:
> Testing teacher, let u know, thanks and hi-5...
>
> On Tue, Oct 31, 2017 at 1:30 PM, Yuri <yvoinov at gmail.com> wrote:
>> (facepalm) Don't use superuser rights (or equivalent) - never - if you
>> really won't require it.
>>
>> Yes, any user who has permissions to read access to AD.
>>
>> 01.11.2017 2:28, Periko Support ?????:
>>> Don't hate me Yuri, this is why I ask first.
>>> Group Users, them I can use any user for my squid settings?
>>> Thanks...don't hate me please..
>>>
>>> On Tue, Oct 31, 2017 at 1:19 PM, Yuri <yvoinov at gmail.com> wrote:
>>>>
>>>> 01.11.2017 2:16, Periko Support ?????:
>>>>> HI Guys.
>>>>>
>>>>> I want to integrate my authtenticacion vs a Windows Server 2012 R2 AD,
>>>>> my doubt is related to the user I have to use from the AD.
>>>>>
>>>>> What type of user can I use for squid config to query our AD?
>>>>>
>>>>> Must be from the Admin group or must a specific role?
>>>> The principle of the least privilege. What kind of devil is a member of
>>>> the "Administrators" group?!
>>>>
>>>> At max member of group "Users".
>>>>> Squid 3.5.x, thanks guys for your time!!!
>>>>> _______________________________________________
>>>>> squid-users mailing list
>>>>> squid-users at lists.squid-cache.org
>>>>> http://lists.squid-cache.org/listinfo/squid-users
>>>> --
>>>> **************************
>>>> * C++: Bug to the future *
>>>> **************************
>>>>
>>>>
>>>> _______________________________________________
>>>> squid-users mailing list
>>>> squid-users at lists.squid-cache.org
>>>> http://lists.squid-cache.org/listinfo/squid-users
>>>>
>>> _______________________________________________
>>> squid-users mailing list
>>> squid-users at lists.squid-cache.org
>>> http://lists.squid-cache.org/listinfo/squid-users
>>
>> --
>> **************************
>> * C++: Bug to the future *
>> **************************
>>
>>
>> _______________________________________________
>> squid-users mailing list
>> squid-users at lists.squid-cache.org
>> http://lists.squid-cache.org/listinfo/squid-users
>>


From christof.gerber1 at gmail.com  Wed Nov  1 09:20:21 2017
From: christof.gerber1 at gmail.com (Christof Gerber)
Date: Wed, 1 Nov 2017 10:20:21 +0100
Subject: [squid-users] Squid 3.5 with nonblocking ecap adapter
Message-ID: <CAFyThpJ79PjfCJspusGzK1LeRL7-Qg5F7m_PrLLVYYRsWO-UAQ@mail.gmail.com>

Hi everyone

I am working on an ecap adapter which attaches to Squid 3.5. The
adapter is going to make an external socket call which results in an
REST API call (lookup)  which will cause a delay of some milliseconds
until the response is available. As ecap is blocking I assume Squid
will be blocked during this period of time which is not desirable from
a performance point of view. Is my assumption right that squid will be
blocked until the eCAP API call returns? Is there a way other than
programming the eCAP adapter in asynchronous mode?

Thanks for your help.
Best
Christof

-- 
Christof Gerber
Email: christof.gerber1 at gmail.com


From torsten.luettgert at thinkproject.com  Wed Nov  1 10:09:05 2017
From: torsten.luettgert at thinkproject.com (Torsten Luettgert)
Date: Wed, 1 Nov 2017 11:09:05 +0100
Subject: [squid-users] Using HTTP headers in a store ID helper -- possible?
Message-ID: <20171101110859.0b63ab70@dev03.berlin.thinkproject.com>

Hello,

we were wondering if it's possible to use HTTP headers in an external
store ID helper, not only the URL.

Any help/hints appreciated :-)

Thanks for your time,
Torsten


From squid3 at treenet.co.nz  Wed Nov  1 11:49:28 2017
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 2 Nov 2017 00:49:28 +1300
Subject: [squid-users] Using HTTP headers in a store ID helper --
 possible?
In-Reply-To: <20171101110859.0b63ab70@dev03.berlin.thinkproject.com>
References: <20171101110859.0b63ab70@dev03.berlin.thinkproject.com>
Message-ID: <937ba724-3d3f-328f-8df0-9c525643ac12@treenet.co.nz>

On 01/11/17 23:09, Torsten Luettgert wrote:
> Hello,
> 
> we were wondering if it's possible to use HTTP headers in an external
> store ID helper, not only the URL.
> 

Depends on the header.
* Response headers do not exist when Store-ID helper is called.
* Request headers (and maybe ICAP REQMOD headers) can be sent as extra 
parameters using <http://www.squid-cache.org/Doc/config/store_id_extras/>.

Amos


From squid3 at treenet.co.nz  Wed Nov  1 12:52:28 2017
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 2 Nov 2017 01:52:28 +1300
Subject: [squid-users] can't block streaming
In-Reply-To: <006901d352ef$0b1d7e60$21587b20$@skno.by>
References: <01df01d35227$6188bdf0$249a39d0$@skno.by>
 <0142c03e-f30b-f1a9-b331-2d1c7e9697a0@treenet.co.nz>
 <006901d352ef$0b1d7e60$21587b20$@skno.by>
Message-ID: <4c084243-8480-9d8b-6caf-ff6c9d503b3a@treenet.co.nz>

On 01/11/17 21:54, Vacheslav wrote:
> Thanks for your time,
> 
> -----Original Message-----
> From: Amos Jeffries
> Sent: Tuesday, October 31, 2017 5:45 PM
> 
> On 31/10/17 22:05, Vacheslav wrote:
>> Peace,
>>
>> I tired searching and debugging but I couldn?t find a solution,
>> whatever I do youtube keeps working.
>>
>> Here is my configuration:
> ...
>> # Media Streams
>>
>> ## MediaPlayer MMS Protocol
>>
>> acl media rep_mime_type mms
>>
>> acl mediapr url_regex dvrplayer mediastream ^mms://
>>
>> ## (Squid does not yet handle the URI as a known proto type.)
> 
>> Unsupported URI schemes should result in the client receiving an HTTP error page instead of Squid handling the traffic.
> 
>> Which also explains your problems: the Browser is either not using the proxy at all for this traffic, or sending the traffic through a CONNECT tunnel that is allowed to be created for other reasons.
> 
> Well I tried unchecking automatically detect proxy settings. There are 2 network cards on the squid, one with a gateway, the same  is used as the proxy ip port 3128 and youtube is not in the bypass proxylist. I tried using opera, the same result.

Things like YT do not have to be on any bypass list to avoid the proxy. 
It just has to have a URL scheme for some protocol the browser detects 
as not able to go through the HTTP-only proxy. eg "mms:"

Since mms:// means a non-HTTP protocol and it is not commonly supported 
by HTTP proxies, the browsers usually send it directly to the mms 
protocol port(s) AFAIK.


> What do you mean by a connect tunnel?

Things like this:

"
  CONNECT r1---sn-ntqe6n76.googlevideo.com:443 HTTP/1.1

  ... non-HTTP data stream.
"

Which tells Squid to open a TCP connection to the named server and port. 
That is how a YouTube video I'm watching right now is currently going 
through a test Squid. The browser of course shows it as a GET request 
for some https: URI, but the proxy only sees that CONNECT.

To see what is inside that particular port 443 tunnel one has to use 
SSL_Bump feature to decrypt the HTTPS protocol that is supposed to be on 
that port.


> ...
> 
>> # We strongly recommend the following be uncommented to protect
>> innocent
>>
>> # web applications running on the proxy server who think the only
>>
>> # one who can access services on "localhost" is a local user
>>
>> #http_access deny to_localhost
>>
>> # Deny all blocked extension
>>
>> error_directory /usr/share/squid/errors/en
>>
>> deny_info ERR_BLOCKED_FILES blockfiles
>>
>> http_access deny blockfiles
>>
>> #
>>
>> # INSERT YOUR OWN RULE(S) HERE TO ALLOW ACCESS FROM YOUR CLIENTS
>>
> 
>> Please read the above line, and consider all the custom rules you placed above it.
> I moved the below text to under
> # INSERT YOUR OWN RULE(S) HERE TO ALLOW ACCESS FROM YOUR CLIENTS
> 
> http_access deny mediapr
> http_access deny mediapr1
> http_access deny mediapr2
> http_access deny mediapr3
> http_reply_access deny media
> ...
>>
>> #url_rewrite_program /usr/sbin/squidGuard
>>
>> #url_rewrite_children 5
>>
>> #debug_options ALL,1 33,2 28,9
>>
>> And where must I place the before last 2 lines in order for squid
>> guard to work?
>>
> 
>> Right there where they are in your config will do.
> 
>> What do you expect SquidGuard to do?
> 
> At first, I thought squid guard is needed to block file extension, then I discovered that it blocks urls so it is not a bad idea to block porn sites and porn search terms.

Ah, I see. Well, if you are new to it I advise to try using squid.conf 
ACLs first. Sending things to helpers is quite I/O and memory intensive 
and most of what SG does can be done better by modern Squid.

Also, SquidGuard specifically is very outdated software and no longer 
maintained. If you have to do access control in a helper at all it is 
better to use the external_acl_type interface and other helpers that 
meet the more specific need.


> 
>> If Squid itself cannot identify any URLs with "mms://" scheme there is no hope of SG being passed the non-existent URLs.
> 
> This I didn't digest!
> 

See above with the CONNECT example. *If* the request is actually going 
through the proxy, the URI as far as Squid can see would be something 
like "r1---sn-ntqe6n76.googlevideo.com:443", or maybe just a raw-IP and 
port.

So what Squid can pass the URI helper is only that origin-form URI, not 
the encrypted (if HTTPS) or tunneled (if non-HTTP/HTTPS) absolute-URI 
stuff where the scheme is.

Amos


From squid3 at treenet.co.nz  Wed Nov  1 13:11:00 2017
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 2 Nov 2017 02:11:00 +1300
Subject: [squid-users] Squid doesn't reload webpage like other clients do
In-Reply-To: <6e9f85bc-5dc6-c377-1fb5-204619972ace@measurement-factory.com>
References: <f877aee786b8477e9c6191a9d6edc32a@ocgepvsw3101.ocr.priv>
 <6a0985c1-394d-c19b-f03d-a658e937ab42@measurement-factory.com>
 <98941ab1-fc1d-8d2a-b1bf-41ab834d7be9@treenet.co.nz>
 <6e9f85bc-5dc6-c377-1fb5-204619972ace@measurement-factory.com>
Message-ID: <71449549-7349-1f6d-98ef-45585de812a3@treenet.co.nz>

On 01/11/17 05:53, Alex Rousskov wrote:
> On 10/31/2017 02:51 AM, Amos Jeffries wrote:
>> That operational state (HTTP underway) also means RFC 7230 is the
>> relevant place to look for behaviour requirements. Section 6.3.1 says:
> 
>>  ? A proxy MUST NOT automatically retry non-idempotent requests.
> 
>> So it depends entirely on what type of HTTP request was being performed.
> 
> Unfortunately, it is more nuanced than that. The RFC section you are
> quoting lives inside the "6.3. Persistence" section about persistent
> connections. It also talks about retransmitting a "sequence of
> requests", further implying connection persistency scope. The old RFC
> 2616 even gave a very pconn-specific example as the rationale.
> 
> Squid implementation assumes that all those requirements apply to
> persistent connection race conditions and only to those conditions.
> Whether that is what RFC authors wanted is not clear to me. It would be
> nice to clarify that with the HTTP WG. Any volunteers?
> 
> Alex.
> 

As the first line of that section says:
"
    HTTP/1.1 defaults to the use of "persistent connections"
"

So IMO it is reasonable to assume that the HTTP/1.1 portion of the 
traffic is always scoped as 'persistent'. Most of the section(s) are 
about issues later in the connection traffic, but those specific 
statements might be applied before the first transaction is complete.

The question seems to be more about whether we scope the period between 
SYN+ACK and first data bytes arriving to be governed by TCP or HTTP/1.1 
protocol. The HTTP side has at least those statements I referenced for 
better interoperable behaviour than currently coded.

Amos


From rousskov at measurement-factory.com  Wed Nov  1 15:23:38 2017
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Wed, 1 Nov 2017 09:23:38 -0600
Subject: [squid-users] Squid 3.5 with nonblocking ecap adapter
In-Reply-To: <CAFyThpJ79PjfCJspusGzK1LeRL7-Qg5F7m_PrLLVYYRsWO-UAQ@mail.gmail.com>
References: <CAFyThpJ79PjfCJspusGzK1LeRL7-Qg5F7m_PrLLVYYRsWO-UAQ@mail.gmail.com>
Message-ID: <572c5876-1762-3284-9b0c-93d2188f237a@measurement-factory.com>

On 11/01/2017 03:20 AM, Christof Gerber wrote:

> [Will Squid] be blocked until the eCAP API call returns?

To answer the exact question above: Yes, the Squid worker making an eCAP
API call will block until that call returns. The same is true for all
other API calls, all system calls, and all internal calls. This is how
C/C++ works. I am stating the obvious for the record, in case somebody
with a different (or insufficient) programming languages background
stumbles upon this thread.

What you are really asking, I suspect, is whether Squid or the eCAP
library uses threads to automatically make eCAP adapter operations
asynchronous to the primary Squid operations. The answer to that
question is "no": The relevant Squid code does not use threads, and
there are no threads in the eCAP library code.

Also, there is no magical layer between Squid and 99% of eCAP calls --
Squid calls go directly to your eCAP adapter code and vice versa. IIRC,
the only (unimportant) exception to that "direct calls" observation is
the eCAP service registry API, where there is a thin eCAP layer
insulating the adapter from the host application. That layer is also
synchronous though.


> Is there a way other than
> programming the eCAP adapter in asynchronous mode?

I do not think there is a better alternative. AFAICT, you only have two
options:

  A) Change Squid to move eCAP calls to thread(s).
  B) Use threads inside the adapter to make its operations asynchronous.

As you know, the sample async adapter and the ClamAV adapter use (B).
That approach has its problems (because it currently does not require
the host application to be threads-aware), but it works reasonably well
for many use cases.


Cheers,

Alex.


From yvoinov at gmail.com  Wed Nov  1 15:26:39 2017
From: yvoinov at gmail.com (Yuri)
Date: Wed, 1 Nov 2017 21:26:39 +0600
Subject: [squid-users] Squid 3.5 with nonblocking ecap adapter
In-Reply-To: <572c5876-1762-3284-9b0c-93d2188f237a@measurement-factory.com>
References: <CAFyThpJ79PjfCJspusGzK1LeRL7-Qg5F7m_PrLLVYYRsWO-UAQ@mail.gmail.com>
 <572c5876-1762-3284-9b0c-93d2188f237a@measurement-factory.com>
Message-ID: <7dc34796-0908-ca0a-2fdd-2725bc1f68ea@gmail.com>



01.11.2017 21:23, Alex Rousskov ?????:
> On 11/01/2017 03:20 AM, Christof Gerber wrote:
>
>> [Will Squid] be blocked until the eCAP API call returns?
> To answer the exact question above: Yes, the Squid worker making an eCAP
> API call will block until that call returns. The same is true for all
> other API calls, all system calls, and all internal calls. This is how
> C/C++ works. I am stating the obvious for the record, in case somebody
> with a different (or insufficient) programming languages background
> stumbles upon this thread.
>
> What you are really asking, I suspect, is whether Squid or the eCAP
> library uses threads to automatically make eCAP adapter operations
> asynchronous to the primary Squid operations. The answer to that
> question is "no": The relevant Squid code does not use threads, and
> there are no threads in the eCAP library code.
>
> Also, there is no magical layer between Squid and 99% of eCAP calls --
> Squid calls go directly to your eCAP adapter code and vice versa. IIRC,
> the only (unimportant) exception to that "direct calls" observation is
> the eCAP service registry API, where there is a thin eCAP layer
> insulating the adapter from the host application. That layer is also
> synchronous though.
>
>
>> Is there a way other than
>> programming the eCAP adapter in asynchronous mode?
> I do not think there is a better alternative. AFAICT, you only have two
> options:
>
>   A) Change Squid to move eCAP calls to thread(s).
>   B) Use threads inside the adapter to make its operations asynchronous.
Alex, AFAIK B) is impossible. Because of I see no way to push body from
ecap library to adapter at whole, not chunk. Or I am stupid bastard? I
see no documented way to do this.
>
> As you know, the sample async adapter and the ClamAV adapter use (B).
> That approach has its problems (because it currently does not require
> the host application to be threads-aware), but it works reasonably well
> for many use cases.
>
>
> Cheers,
>
> Alex.
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users

-- 
**************************
* C++: Bug to the future *
**************************

-------------- next part --------------
A non-text attachment was scrubbed...
Name: 0x3E3743A7.asc
Type: application/pgp-keys
Size: 2887 bytes
Desc: not available
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20171101/9d57c470/attachment.key>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: signature.asc
Type: application/pgp-signature
Size: 659 bytes
Desc: OpenPGP digital signature
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20171101/9d57c470/attachment.sig>

From yvoinov at gmail.com  Wed Nov  1 15:28:02 2017
From: yvoinov at gmail.com (Yuri)
Date: Wed, 1 Nov 2017 21:28:02 +0600
Subject: [squid-users] Squid 3.5 with nonblocking ecap adapter
In-Reply-To: <572c5876-1762-3284-9b0c-93d2188f237a@measurement-factory.com>
References: <CAFyThpJ79PjfCJspusGzK1LeRL7-Qg5F7m_PrLLVYYRsWO-UAQ@mail.gmail.com>
 <572c5876-1762-3284-9b0c-93d2188f237a@measurement-factory.com>
Message-ID: <ad246d9c-136d-c851-b10f-d9795553913d@gmail.com>



01.11.2017 21:23, Alex Rousskov ?????:
> On 11/01/2017 03:20 AM, Christof Gerber wrote:
>
>> [Will Squid] be blocked until the eCAP API call returns?
> To answer the exact question above: Yes, the Squid worker making an eCAP
> API call will block until that call returns. The same is true for all
> other API calls, all system calls, and all internal calls. This is how
> C/C++ works. I am stating the obvious for the record, in case somebody
> with a different (or insufficient) programming languages background
> stumbles upon this thread.
>
> What you are really asking, I suspect, is whether Squid or the eCAP
> library uses threads to automatically make eCAP adapter operations
> asynchronous to the primary Squid operations. The answer to that
> question is "no": The relevant Squid code does not use threads, and
> there are no threads in the eCAP library code.
>
> Also, there is no magical layer between Squid and 99% of eCAP calls --
> Squid calls go directly to your eCAP adapter code and vice versa. IIRC,
> the only (unimportant) exception to that "direct calls" observation is
> the eCAP service registry API, where there is a thin eCAP layer
> insulating the adapter from the host application. That layer is also
> synchronous though.
>
>
>> Is there a way other than
>> programming the eCAP adapter in asynchronous mode?
> I do not think there is a better alternative. AFAICT, you only have two
> options:
>
>   A) Change Squid to move eCAP calls to thread(s).
>   B) Use threads inside the adapter to make its operations asynchronous.
Also I see no way to exchange between adapter and host with signals
related partial body processing in async manner.
>
> As you know, the sample async adapter and the ClamAV adapter use (B).
> That approach has its problems (because it currently does not require
> the host application to be threads-aware), but it works reasonably well
> for many use cases.
>
>
> Cheers,
>
> Alex.
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users

-- 
**************************
* C++: Bug to the future *
**************************

-------------- next part --------------
A non-text attachment was scrubbed...
Name: 0x3E3743A7.asc
Type: application/pgp-keys
Size: 2887 bytes
Desc: not available
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20171101/268d60a4/attachment.key>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: signature.asc
Type: application/pgp-signature
Size: 659 bytes
Desc: OpenPGP digital signature
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20171101/268d60a4/attachment.sig>

From yvoinov at gmail.com  Wed Nov  1 15:40:39 2017
From: yvoinov at gmail.com (Yuri)
Date: Wed, 1 Nov 2017 21:40:39 +0600
Subject: [squid-users] Squid 3.5 with nonblocking ecap adapter
In-Reply-To: <572c5876-1762-3284-9b0c-93d2188f237a@measurement-factory.com>
References: <CAFyThpJ79PjfCJspusGzK1LeRL7-Qg5F7m_PrLLVYYRsWO-UAQ@mail.gmail.com>
 <572c5876-1762-3284-9b0c-93d2188f237a@measurement-factory.com>
Message-ID: <1ce51ccb-5c48-18dc-d7f8-0b3cbf7ac91c@gmail.com>



01.11.2017 21:23, Alex Rousskov ?????:
> On 11/01/2017 03:20 AM, Christof Gerber wrote:
>
>> [Will Squid] be blocked until the eCAP API call returns?
> To answer the exact question above: Yes, the Squid worker making an eCAP
> API call will block until that call returns. The same is true for all
> other API calls, all system calls, and all internal calls. This is how
> C/C++ works. I am stating the obvious for the record, in case somebody
> with a different (or insufficient) programming languages background
> stumbles upon this thread.
>
> What you are really asking, I suspect, is whether Squid or the eCAP
> library uses threads to automatically make eCAP adapter operations
> asynchronous to the primary Squid operations. The answer to that
> question is "no": The relevant Squid code does not use threads, and
> there are no threads in the eCAP library code.
>
> Also, there is no magical layer between Squid and 99% of eCAP calls --
> Squid calls go directly to your eCAP adapter code and vice versa. IIRC,
> the only (unimportant) exception to that "direct calls" observation is
> the eCAP service registry API, where there is a thin eCAP layer
> insulating the adapter from the host application. That layer is also
> synchronous though.
>
>
>> Is there a way other than
>> programming the eCAP adapter in asynchronous mode?
> I do not think there is a better alternative. AFAICT, you only have two
> options:
>
>   A) Change Squid to move eCAP calls to thread(s).
This is absolutely not necessary. The interface of the store ID allows
you to write a threaded asynchronous (thread-aware) helper in
combination with a thread-unaware. This allows its interface. However,
the eCAP interface does not allow this because of the synchronous nature
of the ecap library and this restricts of its interaction with the host.
About this OP asks exactly.
>   B) Use threads inside the adapter to make its operations asynchronous.
>
> As you know, the sample async adapter and the ClamAV adapter use (B).
This sample useless on whole class of cases. For example, it will not
work with gzip compression. I've spent much time in attempts to make
gzip adapter threaded/async. Without any success.
> That approach has its problems (because it currently does not require
> the host application to be threads-aware), but it works reasonably well
> for many use cases.
And it does not work for a much larger number of cases due to library
limitations.
>
>
> Cheers,
>
> Alex.
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users

-- 
**************************
* C++: Bug to the future *
**************************

-------------- next part --------------
A non-text attachment was scrubbed...
Name: 0x3E3743A7.asc
Type: application/pgp-keys
Size: 2887 bytes
Desc: not available
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20171101/3a0b5d91/attachment.key>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: signature.asc
Type: application/pgp-signature
Size: 659 bytes
Desc: OpenPGP digital signature
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20171101/3a0b5d91/attachment.sig>

From rousskov at measurement-factory.com  Wed Nov  1 17:37:21 2017
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Wed, 1 Nov 2017 11:37:21 -0600
Subject: [squid-users] Squid 3.5 with nonblocking ecap adapter
In-Reply-To: <7dc34796-0908-ca0a-2fdd-2725bc1f68ea@gmail.com>
References: <CAFyThpJ79PjfCJspusGzK1LeRL7-Qg5F7m_PrLLVYYRsWO-UAQ@mail.gmail.com>
 <572c5876-1762-3284-9b0c-93d2188f237a@measurement-factory.com>
 <7dc34796-0908-ca0a-2fdd-2725bc1f68ea@gmail.com>
Message-ID: <474c8b74-bc63-52f4-e56a-7f9b25e45195@measurement-factory.com>

On 11/01/2017 09:26 AM, Yuri wrote:
>>> Is there a way other than
>>> programming the eCAP adapter in asynchronous mode?

>> I do not think there is a better alternative. AFAICT, you only have two
>> options:
>>
>>   A) Change Squid to move eCAP calls to thread(s).
>>   B) Use threads inside the adapter to make its operations asynchronous.

> AFAIK B) is impossible.

It is not only possible but implemented in sample and production
adapters (as I have said later in the same email).


> I see no way to push body from ecap library to adapter at whole, not
> chunk.

This part of your assertion does not compute for me:

* First of all, it seems completely unrelated to the OP question.

* Second, the body is not pushed "from the eCAP library". Body bytes are
"pushed from" ("made available by" would be a more accurate term) the
host application (e.g., Squid) to (for) the adapter and vice versa. The
amount and timing of body bytes availability are determined by the side
doing the "pushing" and, naturally, body data presence on that side.

Many adapters "stream" message content (processing each body piece on
the fly) and some adapters accumulate whole bodies before acting on
them. The best design depends on the adapter purpose and various
risk/resources/performance/complexity trade-offs.


> I see no documented way to do this.

Asynchronous adapter API is documented in libecap/doc/async.txt. There
are no detailed tutorials, but there is working code to study.

There is no documentation dedicated to body exchanges, but each
adapter::Xaction::ab*() and host::Xaction::vb*() method is briefly
documented and there are public working adapters that illustrate the use
of those methods.

If you have questions specific to eCAP, then this Squid mailing list is
not the right place to ask them, but see http://www.e-cap.org/support/


> And it does not work for a much larger number of cases due to library
> limitations.

AFAICT, you are conflating developer failures with library limitations.
The eCAP library is far from perfect, but it does not have the flaws you
allege.


HTH,

Alex.


>> As you know, the sample async adapter and the ClamAV adapter use (B).
>> That approach has its problems (because it currently does not require
>> the host application to be threads-aware), but it works reasonably well
>> for many use cases.


From yvoinov at gmail.com  Wed Nov  1 17:45:05 2017
From: yvoinov at gmail.com (Yuri)
Date: Wed, 1 Nov 2017 23:45:05 +0600
Subject: [squid-users] Squid 3.5 with nonblocking ecap adapter
In-Reply-To: <474c8b74-bc63-52f4-e56a-7f9b25e45195@measurement-factory.com>
References: <CAFyThpJ79PjfCJspusGzK1LeRL7-Qg5F7m_PrLLVYYRsWO-UAQ@mail.gmail.com>
 <572c5876-1762-3284-9b0c-93d2188f237a@measurement-factory.com>
 <7dc34796-0908-ca0a-2fdd-2725bc1f68ea@gmail.com>
 <474c8b74-bc63-52f4-e56a-7f9b25e45195@measurement-factory.com>
Message-ID: <a311b58f-214c-14f5-7dd1-a8e46bd82dc6@gmail.com>



01.11.2017 23:37, Alex Rousskov ?????:
> On 11/01/2017 09:26 AM, Yuri wrote:
>>>> Is there a way other than
>>>> programming the eCAP adapter in asynchronous mode?
>>> I do not think there is a better alternative. AFAICT, you only have two
>>> options:
>>>
>>>   A) Change Squid to move eCAP calls to thread(s).
>>>   B) Use threads inside the adapter to make its operations asynchronous.
>> AFAIK B) is impossible.
> It is not only possible but implemented in sample and production
> adapters (as I have said later in the same email).
"Stop talking - just show the code." :) (Ideally - in C++, not in C. And
not adapter sample - it's illustrates nothing because it is mixture of
C++03 and pure C pthreads).
>
>
>> I see no way to push body from ecap library to adapter at whole, not
>> chunk.
> This part of your assertion does not compute for me:
>
> * First of all, it seems completely unrelated to the OP question.
>
> * Second, the body is not pushed "from the eCAP library". Body bytes are
> "pushed from" ("made available by" would be a more accurate term) the
> host application (e.g., Squid) to (for) the adapter and vice versa. The
> amount and timing of body bytes availability are determined by the side
> doing the "pushing" and, naturally, body data presence on that side.
Do not cling to words. You understand what I mean. And, for that matter,
yes, the host gives the body - but through the library. If I understand
the design correctly. Otherwise it is not clear, what does the library
have to do with and why is it needed?
>
> Many adapters "stream" message content (processing each body piece on
> the fly) and some adapters accumulate whole bodies before acting on
> them. The best design depends on the adapter purpose and various
> risk/resources/performance/complexity trade-offs.
>
>
>> I see no documented way to do this.
> Asynchronous adapter API is documented in libecap/doc/async.txt. There
> are no detailed tutorials, but there is working code to study.
Seriously? Here is a text file with common words you call good
documentation?
>
> There is no documentation dedicated to body exchanges, but each
> adapter::Xaction::ab*() and host::Xaction::vb*() method is briefly
> documented and there are public working adapters that illustrate the use
> of those methods.
The key word is "briefly". How much public working async adapters do you
know? Which I can study?
>
> If you have questions specific to eCAP, then this Squid mailing list is
> not the right place to ask them, but see http://www.e-cap.org/support/
>
>
>> And it does not work for a much larger number of cases due to library
>> limitations.
> AFAICT, you are conflating developer failures with library limitations.
> The eCAP library is far from perfect, but it does not have the flaws you
> allege.
Ha. Ok, let's agree on it.
>
>
> HTH,
>
> Alex.
>
>
>>> As you know, the sample async adapter and the ClamAV adapter use (B).
>>> That approach has its problems (because it currently does not require
>>> the host application to be threads-aware), but it works reasonably well
>>> for many use cases.

-- 
**************************
* C++: Bug to the future *
**************************

-------------- next part --------------
A non-text attachment was scrubbed...
Name: 0x3E3743A7.asc
Type: application/pgp-keys
Size: 2887 bytes
Desc: not available
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20171101/9bac3adb/attachment.key>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: signature.asc
Type: application/pgp-signature
Size: 659 bytes
Desc: OpenPGP digital signature
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20171101/9bac3adb/attachment.sig>

From rousskov at measurement-factory.com  Wed Nov  1 20:40:43 2017
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Wed, 1 Nov 2017 14:40:43 -0600
Subject: [squid-users] Squid 3.5 with nonblocking ecap adapter
In-Reply-To: <a311b58f-214c-14f5-7dd1-a8e46bd82dc6@gmail.com>
References: <CAFyThpJ79PjfCJspusGzK1LeRL7-Qg5F7m_PrLLVYYRsWO-UAQ@mail.gmail.com>
 <572c5876-1762-3284-9b0c-93d2188f237a@measurement-factory.com>
 <7dc34796-0908-ca0a-2fdd-2725bc1f68ea@gmail.com>
 <474c8b74-bc63-52f4-e56a-7f9b25e45195@measurement-factory.com>
 <a311b58f-214c-14f5-7dd1-a8e46bd82dc6@gmail.com>
Message-ID: <91762f33-a47d-c8a0-200d-ce2c00546f6e@measurement-factory.com>

On 11/01/2017 11:45 AM, Yuri wrote:
> 01.11.2017 23:37, Alex Rousskov ?????:
>>>> B) Use threads inside the adapter to make its operations asynchronous.

>>> AFAIK B) is impossible.

>> It is not only possible but implemented in sample and production
>> adapters (as I have said later in the same email).

> "Stop talking - just show the code." :) 

I have already pointed to the relevant code: For example, the eCAP
ClamAV adapter implements (B) since version 2.


> (Ideally - in C++, not in C. And
> not adapter sample - it's illustrates nothing because it is mixture of
> C++03 and pure C pthreads).

All eCAP adapters use C++ because eCAP API requires C++. Some may also
use C libraries (because there are no better C++ alternatives and/or for
other reasons). Your disqualification of adapters using C libraries in
general and pthreads in particular makes no sense to me in this context.
I assume it is just a misunderstanding of how things actually work and
recommend avoiding making broad statements about C inapplicability.


> yes, the host gives the body - but through the library. If I understand
> the design correctly. Otherwise it is not clear, what does the library
> have to do with and why is it needed?

In this context, the eCAP library role is mostly limited to allowing the
dynamic linker to find the right class method where the host application
calls the adapter or where the adapter calls the host application. The
library itself is a thin shim that does not "do" much besides declaring
method names. The eCAP library does not store body pieces, for example.
This makes phrases like "body from ecap library" invalid/misleading. In
my earlier email, I just wanted to clarify that misunderstanding in case
somebody stumbles upon this thread while trying to understand how things
work.

I do not intend to continue this thread because I find your messages too
hostile and not Squid-specific enough. If you have eCAP-specific
questions, please do not post them here. Use eCAP support channels
instead: http://www.e-cap.org/support/


Thank you,

Alex.


From 747620227 at qq.com  Thu Nov  2 02:14:51 2017
From: 747620227 at qq.com (=?gb18030?B?R35Efkx1bmF0aWM=?=)
Date: Thu, 2 Nov 2017 10:14:51 +0800
Subject: [squid-users] how does Squid 3.5 parse https responce
Message-ID: <tencent_9188F5685739BE5DB83CBD54E289EC3E8D07@qq.com>

I want to know how the squid parse the body of certificate when they send HTTPS requests and get the server's certificate.
The operation of this part is in which class or function.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20171102/6ca44663/attachment.htm>

From squid3 at treenet.co.nz  Thu Nov  2 08:11:00 2017
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 2 Nov 2017 21:11:00 +1300
Subject: [squid-users] how does Squid 3.5 parse https responce
In-Reply-To: <tencent_9188F5685739BE5DB83CBD54E289EC3E8D07@qq.com>
References: <tencent_9188F5685739BE5DB83CBD54E289EC3E8D07@qq.com>
Message-ID: <63280d74-3bcd-d70b-3388-cb55d705e98e@treenet.co.nz>

On 02/11/17 15:14, G~D~Lunatic wrote:
> I want to know how the squid parse the body of certificate when they 
> send HTTPS requests and get the server's certificate.

HTTPS messages and TLS certificates are very different things.

> The operation of this part is in which class or function.
> 

This mailing list is for usage discussions. Code discussion happens on 
the squid-dev mailing list (Cc'd).

Amos


From rentorbuy at yahoo.com  Thu Nov  2 09:09:04 2017
From: rentorbuy at yahoo.com (Vieri)
Date: Thu, 2 Nov 2017 09:09:04 +0000 (UTC)
Subject: [squid-users] squid cpu usage
In-Reply-To: <1d42585b-d96f-f261-fa2b-61b54b1641ef@measurement-factory.com>
References: <742455879.1992418.1508848440399.ref@mail.yahoo.com>
 <742455879.1992418.1508848440399@mail.yahoo.com>
 <efb11c97-0368-a8ff-c1be-9781ac450026@measurement-factory.com>
 <790628251.3784841.1509016514244@mail.yahoo.com>
 <5f672c1b-ce28-35f3-df09-982e9c76c9cc@measurement-factory.com>
 <2087507399.4185171.1509055107307@mail.yahoo.com>
 <1d42585b-d96f-f261-fa2b-61b54b1641ef@measurement-factory.com>
Message-ID: <1237606686.975475.1509613744306@mail.yahoo.com>

Just in case someone else bumps into this, I've had the same issue trying to restart Squid from inittab. System-wide ulimit settings are not honored when running scripts from either crontab or inittab.


From rentorbuy at yahoo.com  Thu Nov  2 11:10:38 2017
From: rentorbuy at yahoo.com (Vieri)
Date: Thu, 2 Nov 2017 11:10:38 +0000 (UTC)
Subject: [squid-users] squid cpu usage
In-Reply-To: <1d42585b-d96f-f261-fa2b-61b54b1641ef@measurement-factory.com>
References: <742455879.1992418.1508848440399.ref@mail.yahoo.com>
 <742455879.1992418.1508848440399@mail.yahoo.com>
 <efb11c97-0368-a8ff-c1be-9781ac450026@measurement-factory.com>
 <790628251.3784841.1509016514244@mail.yahoo.com>
 <5f672c1b-ce28-35f3-df09-982e9c76c9cc@measurement-factory.com>
 <2087507399.4185171.1509055107307@mail.yahoo.com>
 <1d42585b-d96f-f261-fa2b-61b54b1641ef@measurement-factory.com>
Message-ID: <1981887728.1020983.1509621038790@mail.yahoo.com>

It's seems logical that the cron daemon needs to be restarted for ulimits to apply.
Init should also be restarted, but "init u" is not enough. A system reboot seems to be inevitable.


From chip_pop at hotmail.com  Thu Nov  2 14:10:28 2017
From: chip_pop at hotmail.com (joseph)
Date: Thu, 2 Nov 2017 07:10:28 -0700 (MST)
Subject: [squid-users] non existing  file to patch
Message-ID: <1509631828952-0.post@n4.nabble.com>

can't find file to patch at input line 951
Perhaps you used the wrong -p or --strip option?
The text leading up to this was:
--------------------------
|diff --git src/ssl/stub_libsslutil.cc src/ssl/stub_libsslutil.cc
|index 006be27..e0d3803 100644
|--- src/ssl/stub_libsslutil.cc
|+++ src/ssl/stub_libsslutil.cc

=========================================
diff --git src/ssl/stub_libsslutil.cc src/ssl/stub_libsslutil.cc
index 006be27..e0d3803 100644
--- src/ssl/stub_libsslutil.cc
+++ src/ssl/stub_libsslutil.cc
@@ -27,13 +27,13 @@ void Ssl::CrtdMessage::parseBody(BodyParams & map,
std::string & other_part) con
 void Ssl::CrtdMessage::composeBody(BodyParams const & map, std::string
const & other_part) STUB
 
 #include "ssl/gadgets.h"
-X509_REQ * Ssl::createNewX509Request(EVP_PKEY_Pointer const &, const char
*) STUB_RETVAL(NULL)
-bool Ssl::writeCertAndPrivateKeyToMemory(Security::CertPointer const &,
EVP_PKEY_Pointer const &, std::string &) STUB_RETVAL(false)
-bool Ssl::writeCertAndPrivateKeyToFile(Security::CertPointer const &,
EVP_PKEY_Pointer const &, char const *) STUB_RETVAL(false)
-bool Ssl::readCertAndPrivateKeyFromMemory(Security::CertPointer &,
EVP_PKEY_Pointer &, char const *) STUB_RETVAL(false)
-X509 * Ssl::signRequest(X509_REQ_Pointer const &, Security::CertPointer
const &, EVP_PKEY_Pointer const &, ASN1_TIME *, BIGNUM const *)
STUB_RETVAL(NULL)
-bool Ssl::generateSslCertificateAndPrivateKey(char const *,
Security::CertPointer const &, EVP_PKEY_Pointer const &,
Security::CertPointer &, EVP_PKEY_Pointer &, BIGNUM const *)
STUB_RETVAL(false)
-void Ssl::readCertAndPrivateKeyFromFiles(Security::CertPointer &,
EVP_PKEY_Pointer &, char const *, char const *) STUB
+X509_REQ * Ssl::createNewX509Request(Security::PrivateKeyPointer const &,
const char *) STUB_RETVAL(nullptr)
+bool Ssl::writeCertAndPrivateKeyToMemory(Security::CertPointer const &,
Security::PrivateKeyPointer const &, std::string &) STUB_RETVAL(false)
+bool Ssl::writeCertAndPrivateKeyToFile(Security::CertPointer const &,
Security::PrivateKeyPointer const &, char const *) STUB_RETVAL(false)
+bool Ssl::readCertAndPrivateKeyFromMemory(Security::CertPointer &,
Security::PrivateKeyPointer &, char const *) STUB_RETVAL(false)
+X509 * Ssl::signRequest(X509_REQ_Pointer const &, Security::CertPointer
const &, Security::PrivateKeyPointer const &, ASN1_TIME *, BIGNUM const *)
STUB_RETVAL(nullptr)
+bool Ssl::generateSslCertificateAndPrivateKey(char const *,
Security::CertPointer const &, Security::PrivateKeyPointer const &,
Security::CertPointer &, Security::PrivateKeyPointer &, BIGNUM const *)
STUB_RETVAL(false)
+void Ssl::readCertAndPrivateKeyFromFiles(Security::CertPointer &,
Security::PrivateKeyPointer &, char const *, char const *) STUB
 bool Ssl::sslDateIsInTheFuture(char const *) STUB_RETVAL(false)
 

also please guys remove         non existing  folder  wen posting patch
public thanks
a/src/ssl/stub_libsslutil.cc
b/src/ssl/stub_libsslutil.cc

a ? b?



--
Sent from: http://squid-web-proxy-cache.1019090.n4.nabble.com/Squid-Users-f1019091.html


From rousskov at measurement-factory.com  Thu Nov  2 14:27:23 2017
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Thu, 2 Nov 2017 08:27:23 -0600
Subject: [squid-users] non existing file to patch
In-Reply-To: <1509631828952-0.post@n4.nabble.com>
References: <1509631828952-0.post@n4.nabble.com>
Message-ID: <5b61fd6c-8f14-e13b-87f0-3ddb5c622495@measurement-factory.com>

On 11/02/2017 08:10 AM, joseph wrote:

> also please guys remove         non existing  folder  wen posting patch
> public thanks
> a/src/ssl/stub_libsslutil.cc
> b/src/ssl/stub_libsslutil.cc
> 
> a ? b?

This is how git generates patches. If you are not using git to apply
patches, then use "patch -p1 ..." to ignore first-level directories.

Alex.
P.S. Your primary (implied) question may lack sufficient context for
anybody to help you without digging through all the current patches.


From chip_pop at hotmail.com  Thu Nov  2 14:44:06 2017
From: chip_pop at hotmail.com (joseph)
Date: Thu, 2 Nov 2017 07:44:06 -0700 (MST)
Subject: [squid-users] non existing file to patch
In-Reply-To: <5b61fd6c-8f14-e13b-87f0-3ddb5c622495@measurement-factory.com>
References: <1509631828952-0.post@n4.nabble.com>
 <5b61fd6c-8f14-e13b-87f0-3ddb5c622495@measurement-factory.com>
Message-ID: <1509633846327-0.post@n4.nabble.com>

wat about missing file
src/ssl/stub_libsslutil.cc 




--
Sent from: http://squid-web-proxy-cache.1019090.n4.nabble.com/Squid-Users-f1019091.html


From rousskov at measurement-factory.com  Thu Nov  2 15:15:18 2017
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Thu, 2 Nov 2017 09:15:18 -0600
Subject: [squid-users] non existing file to patch
In-Reply-To: <1509633846327-0.post@n4.nabble.com>
References: <1509631828952-0.post@n4.nabble.com>
 <5b61fd6c-8f14-e13b-87f0-3ddb5c622495@measurement-factory.com>
 <1509633846327-0.post@n4.nabble.com>
Message-ID: <6037367c-3983-ae7f-cf23-0d2929718339@measurement-factory.com>

On 11/02/2017 08:44 AM, joseph wrote:
> wat about missing file
> src/ssl/stub_libsslutil.cc 

As I said in the last email, your question may lack sufficient context
for anybody to help you without digging through all the current patches.
Do not assume we know what patch you are using and what Squid snapshot
you are patching. If you are using a patch for one Squid snapshot to
patch another Squid snapshot, you may have to backport or upport changes.

Alex.


From logic4life at gmail.com  Thu Nov  2 16:29:51 2017
From: logic4life at gmail.com (Stephen Stark)
Date: Thu, 2 Nov 2017 12:29:51 -0400
Subject: [squid-users] Squid 3.5 ICAP Problems
Message-ID: <CAOKqvx5XPdoYZjMap1NyDO+ERraK4nxvxFBT2k=ysTiXM+MjOg@mail.gmail.com>

Hello everyone,

I am having problems using Squid with ICAP (C-ICAP and clamd). The thing
that is bugging me is I had this was working fine and now it cannot connect
to the local ICAP service.

Below is the debug section 93,3 to see what was going on when I restarted
Squid:

2017/11/02 12:06:34.546 kid1| 93,3| ServiceRep.cc(712) detach: detaching
ICAP service: icap://127.0.0.1:1344/virus_scan [down,!opt]
2017/11/02 12:06:34.546 kid1| 93,3| ServiceRep.cc(712) detach: detaching
ICAP service: icap://127.0.0.1:1344/srv_content_filtering [down,!opt]
2017/11/02 12:06:34.548 kid1| 93,3| Service.cc(19) Service: creating
adaptation service service_cfi_resp
2017/11/02 12:06:34.548 kid1| 93,3| Service.cc(19) Service: creating
adaptation service service_avi_resp
2017/11/02 12:06:34.548 kid1| 93,3| Config.cc(195) finalize: Created 2
adaptation services
2017/11/02 12:06:34.548 kid1| Adaptation support is on
2017/11/02 12:06:34.548 kid1| 93,2| Config.cc(211) FinalizeEach:
Initialized 2 message adaptation services
2017/11/02 12:06:34.548 kid1| 93,2| Config.cc(211) FinalizeEach:
Initialized 1 message adaptation service groups
2017/11/02 12:06:34.548 kid1| 93,2| Config.cc(211) FinalizeEach:
Initialized 3 message adaptation access rules
2017/11/02 12:06:51.415 kid1| 93,3| AccessCheck.cc(196) callBack: NULL
2017/11/02 12:06:51.415 kid1| 93,3| client_side_request.cc(1074)
noteAdaptationAclCheckDone: 0x10dd728 adaptationAclCheckDone called
2017/11/02 12:06:51.454 kid1| 93,3| AccessCheck.cc(196) callBack: 0xd45b80*2
2017/11/02 12:06:51.454 kid1| 93,3| Xaction.cc(60) Xaction:
Adaptation::Icap::ModXact constructed, this=0x124c4b8 [icapxjob146537]
2017/11/02 12:06:51.454 kid1| 93,3| Xaction.cc(60) Xaction:
Adaptation::Icap::OptXact constructed, this=0x120c818 [icapxjob146539]
2017/11/02 12:06:51.454 kid1| 93,3| ServiceRep.cc(122) getConnection: got
connection:
2017/11/02 12:06:51.454 kid1| 93,3| Xaction.cc(145) openConnection:
Adaptation::Icap::OptXact opens connection to 127.0.0.1:1344
2017/11/02 12:06:51.454 kid1| 93,3| AsyncCall.cc(26) AsyncCall: The
AsyncCall Adaptation::Icap::Xaction::noteCommConnected constructed,
this=0x10a1ed0 [call901778]
2017/11/02 12:06:51.454 kid1| 93,3| AsyncCall.cc(93) ScheduleCall:
ConnOpener.cc(137) will call
Adaptation::Icap::Xaction::noteCommConnected(local=[::] remote=
127.0.0.1:1344 flags=1, errno=101, flag=-8, data=0x120c818) [call901778]
2017/11/02 12:06:51.454 kid1| 93,3| AsyncCallQueue.cc(55) fireNext:
entering Adaptation::Icap::Xaction::noteCommConnected(local=[::] remote=
127.0.0.1:1344 flags=1, errno=101, flag=-8, data=0x120c818)
2017/11/02 12:06:51.454 kid1| 93,3| AsyncCall.cc(38) make: make call
Adaptation::Icap::Xaction::noteCommConnected [call901778]
2017/11/02 12:06:51.454 kid1| 93,3| AsyncJob.cc(123) callStart:
Adaptation::Icap::OptXact status in: [/ job146539]
2017/11/02 12:06:51.454 kid1| 93,2| Xaction.cc(272) dieOnConnectionFailure:
Adaptation::Icap::OptXact failed to connect to icap://
127.0.0.1:1344/virus_scan
2017/11/02 12:06:51.454 kid1| 93,3| ServiceRep.cc(161)
noteConnectionFailed: Connection failed: failure
2017/11/02 12:06:51.454 kid1| 93,3| ../../../src/base/AsyncJobCalls.h(177)
dial: Adaptation::Icap::Xaction::noteCommConnected threw exception: cannot
connect to the ICAP service
2017/11/02 12:06:51.454 kid1| 93,3| Xaction.cc(71) ~Xaction:
Adaptation::Icap::OptXact destructed, this=0x120c818 [icapxjob146539]
2017/11/02 12:06:51.454 kid1| 93,3| AsyncCallQueue.cc(57) fireNext: leaving
Adaptation::Icap::Xaction::noteCommConnected(local=[::] remote=
127.0.0.1:1344 flags=1, errno=101, flag=-8, data=0x120c818)
2017/11/02 12:06:51.454 kid1| 93,3| Launcher.cc(95) noteXactAbort: cannot
retry or repeat a failed transaction
2017/11/02 12:06:51.454 kid1| 93,3| ServiceRep.cc(534)
noteAdaptationAnswer: failed to fetch options [down,!opt,fail1]
2017/11/02 12:06:51.454 kid1| optional ICAP service is down after an
options fetch failure: icap://127.0.0.1:1344/virus_scan [down,!opt]

Looks like it load my rules and then tries to connect and fails. I read
almost every post I could find but do not seem to have the same problem.

I can use the c-icap-client and test each service. It looks fine.

># ./c-icap-client -s virus_scan
ICAP server:localhost, ip:127.0.0.1, port:1344

OPTIONS:
        Allow 204: Yes
        Preview: 1024
        Keep alive: Yes

ICAP HEADERS:
        ICAP/1.0 200 OK
        Methods: RESPMOD, REQMOD
        Service: C-ICAP/0.4.3 server - Antivirus service
        ISTag: CI0001-J8gT2j9ufFux2fjZGxq1qAAA
        Transfer-Preview: *
        Options-TTL: 3600
        Date: Thu, 02 Nov 2017 16:17:15 GMT
        Preview: 1024
        Allow: 204
        Encapsulated: null-body=0

># ./c-icap-client -s virus_scan -f /bin/ls
ICAP server:localhost, ip:127.0.0.1, port:1344

No modification needed (Allow 204 response)

I can post some of my squid.conf file below for icap options:

icap_enable on
adaptation_send_client_ip on
icap_persistent_connections on
icap_service_failure_limit -1

icap_send_client_ip on
icap_send_client_username on
icap_client_username_header X-Authenticated-User
icap_preview_enable on
icap_preview_size 1024


icap_service service_cfi_resp respmod_precache icap://
127.0.0.1:1344/srv_content_filtering routing=on bypass=on

icap_service service_avi_resp respmod_precache icap://
127.0.0.1:1344/virus_scan routing=on bypass=on

adaptation_service_chain check_services service_avi_resp service_cfi_resp

adaptation_access check_services allow Antivirus_users

adaptation_access service_avi_resp deny all
adaptation_access service_cfi_resp deny all



If you need more information I can provide it. I am stuck at why this does
not work anymore.

Note: this is basic linux box running Squid 3.5.22 with C-ICAP 0.4.3 and
ClamAV 0.99.2 also i am not using caching with squid.

Thanks
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20171102/9e6d68b7/attachment.htm>

From rousskov at measurement-factory.com  Thu Nov  2 17:21:33 2017
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Thu, 2 Nov 2017 11:21:33 -0600
Subject: [squid-users] Squid 3.5 ICAP Problems
In-Reply-To: <CAOKqvx5XPdoYZjMap1NyDO+ERraK4nxvxFBT2k=ysTiXM+MjOg@mail.gmail.com>
References: <CAOKqvx5XPdoYZjMap1NyDO+ERraK4nxvxFBT2k=ysTiXM+MjOg@mail.gmail.com>
Message-ID: <3d7f5b00-c1f7-4277-e1b9-dfeac355c617@measurement-factory.com>

On 11/02/2017 10:29 AM, Stephen Stark wrote:
> Adaptation::Icap::Xaction::noteCommConnected(local=[::]
> remote=127.0.0.1:1344 flags=1, errno=101, ...

The logs you have provided do not show where/why exactly the TCP
connection to the ICAP service fails, but error number 101 is probably
"network unreachable". This is unusual but not impossible for localhost
traffic. The next step depends on the failure cause. There are at least
two major cases to consider:

A) If Squid sends packets to 127.0.0.1:1344, then you can easily
reproduce the problem using something like "telnet" or "nc" on the Squid
box command line. Just make sure you use the right _source_ IP address
for the connection! It has to be the same source IP address that Squid
is using. It might not be 127.0.0.1. Running that command as Squid user
might also be important if the Squid box have some fancy user-specific
networking restrictions.

B) If Squid does not send packets to 127.0.0.1:1344, then one can figure
out what goes wrong by studying relevant ALL,9 logs. You may also want
to address other errors or warnings Squid logs to cache.log (if any).

You can determine whether Squid sends packets to 127.0.0.1:1344 by
collecting a packet trace (for all Squid box interfaces!) and/or running
strace (for the Squid worker process).


HTH,

Alex.


From dinoop at factweavers.com  Fri Nov  3 04:23:45 2017
From: dinoop at factweavers.com (dinoop)
Date: Thu, 2 Nov 2017 21:23:45 -0700 (MST)
Subject: [squid-users] How to use ipv6 address to send requests
Message-ID: <1509683025201-0.post@n4.nabble.com>

I have installed Squid in one digital ocean ubuntu machine.

What I want to do is: I will send a ipv4 address to this proxy server and I
need to pick up and use a iPv6 address configured in the squid.conf. I have
added 2 ipv6 addresses in the conf file.

If I try the following command by specifying address as ipv6, it works fine.

curl -H 'Cache-Control: no-cache' --proxy localhost:3128
[2400:xxx:x:xx::xxx:a001]
ie, it will pick a random ipv6 address from the conf file and send the
request through that ip address.

If i try the following command by specifying address as ipv4, its not
working

curl -H 'Cache-Control: no-cache' --proxy localhost:3128 34.xxx.xxx.148
ie, Its not picking the ipv6 address specified in the conf file. Instead its
using the server public ip .

My /etc/squid/squid.conf file content is something like this now.

/acl one_fourth_3 random 1/2
acl one_fourth_4 random 1/1
tcp_outgoing_address 2604:xxxx:xxx:xxx::xx:a001 one_fourth_3
tcp_outgoing_address 2604:xxxx:xxx:xxx::xx:a002 one_fourth_4

http_access deny !safe_ports
http_access deny CONNECT !SSL_ports
http_access allow localhost manager
http_access deny manager
http_access allow localhost
http_access allow all
http_access deny all
http_port 3128
/

My squid version is

Squid Cache: Version 3.5.12
Service Name: squid



--
Sent from: http://squid-web-proxy-cache.1019090.n4.nabble.com/Squid-Users-f1019091.html


From jeffmerkey at gmail.com  Fri Nov  3 06:45:22 2017
From: jeffmerkey at gmail.com (Jeffrey Merkey)
Date: Fri, 3 Nov 2017 00:45:22 -0600
Subject: [squid-users] Google Chrome reports "Too many redirects" on
 ssl-dumped connections with LA Times News Website
Message-ID: <CA+ekxPVtc9b2nDNvAsmT=ebEy6PRZbB91A-JVDoeeuxfGz7wOQ@mail.gmail.com>

This error is extremely hard to reproduce, and I found it can be
cleared by restarting squid, which seems to make it go away.   It
seems to take several hours of non-stop proxy use then once the error
occurs the we browser reports "too many redirects" and certificate
errors.

Doing a restart on Centos 7 clears it:

# systemctl restart squid

The log shows some sort of "refresh unmodified state before it happens:

1509690588.252    167 127.0.0.1 TAG_NONE/200 0 CONNECT
events.bouncex.net:443 - HIER_DIRECT/35.190.62.200 -
1509690588.272    210 127.0.0.1 TAG_NONE/200 0 CONNECT
analytics.twitter.com:443 - HIER_DIRECT/199.59.149.200 -
1509690588.280     62 127.0.0.1 TCP_REFRESH_UNMODIFIED/200 38412 GET
http://www.latimes.com/nation/la-na-vegas-shooting-sheriff-20171102-story.html
- HIER_DIRECT/104.120.143.198 text/html      <================== error
is here
1509690588.356    220 127.0.0.1 TCP_MISS/200 960 GET
https://partners.tremorhub.com/syncnoad? - HIER_DIRECT/34.228.123.38
text/xml
1509690588.366    304 127.0.0.1 TAG_NONE/200 0 CONNECT
geo.moatads.com:443 - HIER_DIRECT/52.21.172.68 -
1509690588.374    303 127.0.0.1 TAG_NONE/200 0 CONNECT
rtr.innovid.com:443 - HIER_DIRECT/13.58.208.14 -
1509690588.377     33 127.0.0.1 TCP_MISS/200 498 GET
https://tribpubdfp745347008913.s.moatpixel.com/pixel.gif? - HIER_

If there are particulars and I attempt to recreate this problem are
there any specific logging parms or settings that would help you
understand this particular error or shed some light on it that I could
set on my end.

Jeff


From jeffmerkey at gmail.com  Fri Nov  3 07:06:17 2017
From: jeffmerkey at gmail.com (Jeffrey Merkey)
Date: Fri, 3 Nov 2017 01:06:17 -0600
Subject: [squid-users] Google Chrome reports "Too many redirects" on
 ssl-dumped connections with LA Times News Website
In-Reply-To: <CA+ekxPVtc9b2nDNvAsmT=ebEy6PRZbB91A-JVDoeeuxfGz7wOQ@mail.gmail.com>
References: <CA+ekxPVtc9b2nDNvAsmT=ebEy6PRZbB91A-JVDoeeuxfGz7wOQ@mail.gmail.com>
Message-ID: <CA+ekxPVaLrJ25bNCVTaCJFj0+3u_iwmiHeB1_ExiJ8drPsEDew@mail.gmail.com>

On 11/3/17, Jeffrey Merkey <jeffmerkey at gmail.com> wrote:
> This error is extremely hard to reproduce, and I found it can be
> cleared by restarting squid, which seems to make it go away.   It
> seems to take several hours of non-stop proxy use then once the error
> occurs the we browser reports "too many redirects" and certificate
> errors.
>
> Doing a restart on Centos 7 clears it:
>
> # systemctl restart squid
>
> The log shows some sort of "refresh unmodified state before it happens:
>
> 1509690588.252    167 127.0.0.1 TAG_NONE/200 0 CONNECT
> events.bouncex.net:443 - HIER_DIRECT/35.190.62.200 -
> 1509690588.272    210 127.0.0.1 TAG_NONE/200 0 CONNECT
> analytics.twitter.com:443 - HIER_DIRECT/199.59.149.200 -
> 1509690588.280     62 127.0.0.1 TCP_REFRESH_UNMODIFIED/200 38412 GET
> http://www.latimes.com/nation/la-na-vegas-shooting-sheriff-20171102-story.html
> - HIER_DIRECT/104.120.143.198 text/html      <================== error
> is here
> 1509690588.356    220 127.0.0.1 TCP_MISS/200 960 GET
> https://partners.tremorhub.com/syncnoad? - HIER_DIRECT/34.228.123.38
> text/xml
> 1509690588.366    304 127.0.0.1 TAG_NONE/200 0 CONNECT
> geo.moatads.com:443 - HIER_DIRECT/52.21.172.68 -
> 1509690588.374    303 127.0.0.1 TAG_NONE/200 0 CONNECT
> rtr.innovid.com:443 - HIER_DIRECT/13.58.208.14 -
> 1509690588.377     33 127.0.0.1 TCP_MISS/200 498 GET
> https://tribpubdfp745347008913.s.moatpixel.com/pixel.gif? - HIER_
>
> If there are particulars and I attempt to recreate this problem are
> there any specific logging parms or settings that would help you
> understand this particular error or shed some light on it that I could
> set on my end.
>
> Jeff
>

One important thing I failed to mention is that other websites seem to
work fine at fetching pages, it seems to affect cached webpages that
seem to be the problem.  What specific trace functions can I enable to
help run down this issue and narrow it down to a root cause?

Jeff


From squid3 at treenet.co.nz  Fri Nov  3 07:07:51 2017
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Fri, 3 Nov 2017 20:07:51 +1300
Subject: [squid-users] How to use ipv6 address to send requests
In-Reply-To: <1509683025201-0.post@n4.nabble.com>
References: <1509683025201-0.post@n4.nabble.com>
Message-ID: <f2047031-4898-785a-3846-80d08392f9c2@treenet.co.nz>

On 03/11/17 17:23, dinoop wrote:
> I have installed Squid in one digital ocean ubuntu machine.
> 
> What I want to do is: I will send a ipv4 address to this proxy server and I
> need to pick up and use a iPv6 address configured in the squid.conf. I have
> added 2 ipv6 addresses in the conf file.
> 
> If I try the following command by specifying address as ipv6, it works fine.
> 
> curl -H 'Cache-Control: no-cache' --proxy localhost:3128
> [2400:xxx:x:xx::xxx:a001]
> ie, it will pick a random ipv6 address from the conf file and send the
> request through that ip address.
> 
> If i try the following command by specifying address as ipv4, its not
> working
> 
> curl -H 'Cache-Control: no-cache' --proxy localhost:3128 34.xxx.xxx.148
> ie, Its not picking the ipv6 address specified in the conf file. Instead its
> using the server public ip .

Of course. You cannot open an IPv6 connection to an IPv4 server address.

Amos


From squid3 at treenet.co.nz  Fri Nov  3 07:38:26 2017
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Fri, 3 Nov 2017 20:38:26 +1300
Subject: [squid-users] Google Chrome reports "Too many redirects" on
 ssl-dumped connections with LA Times News Website
In-Reply-To: <CA+ekxPVtc9b2nDNvAsmT=ebEy6PRZbB91A-JVDoeeuxfGz7wOQ@mail.gmail.com>
References: <CA+ekxPVtc9b2nDNvAsmT=ebEy6PRZbB91A-JVDoeeuxfGz7wOQ@mail.gmail.com>
Message-ID: <7f7335a6-d9a4-2415-fbd1-565424de7002@treenet.co.nz>

On 03/11/17 19:45, Jeffrey Merkey wrote:
> This error is extremely hard to reproduce, and I found it can be
> cleared by restarting squid, which seems to make it go away.   It
> seems to take several hours of non-stop proxy use then once the error
> occurs the we browser reports "too many redirects" and certificate
> errors.
> 
> Doing a restart on Centos 7 clears it:
> 
> # systemctl restart squid
> 
> The log shows some sort of "refresh unmodified state before it happens:
> 
> 1509690588.252    167 127.0.0.1 TAG_NONE/200 0 CONNECT
> events.bouncex.net:443 - HIER_DIRECT/35.190.62.200 -
> 1509690588.272    210 127.0.0.1 TAG_NONE/200 0 CONNECT
> analytics.twitter.com:443 - HIER_DIRECT/199.59.149.200 -
> 1509690588.280     62 127.0.0.1 TCP_REFRESH_UNMODIFIED/200 38412 GET
> http://www.latimes.com/nation/la-na-vegas-shooting-sheriff-20171102-story.html
> - HIER_DIRECT/104.120.143.198 text/html      <================== error
> is here

This is a 200 status response. So whatever "redirection" is occuring is 
not part of the HTTP for that transaction.

The refresh means that something was cached beforehand but was stale so 
the server had to be asked for permission to deliver it. UNMODIFIED 
means the server responded by indicating it was okay to use.

> 1509690588.356    220 127.0.0.1 TCP_MISS/200 960 GET
> https://partners.tremorhub.com/syncnoad? - HIER_DIRECT/34.228.123.38
> text/xml
> 1509690588.366    304 127.0.0.1 TAG_NONE/200 0 CONNECT
> geo.moatads.com:443 - HIER_DIRECT/52.21.172.68 -
> 1509690588.374    303 127.0.0.1 TAG_NONE/200 0 CONNECT
> rtr.innovid.com:443 - HIER_DIRECT/13.58.208.14 -
> 1509690588.377     33 127.0.0.1 TCP_MISS/200 498 GET
> https://tribpubdfp745347008913.s.moatpixel.com/pixel.gif? - HIER_
> 
> If there are particulars and I attempt to recreate this problem are
> there any specific logging parms or settings that would help you
> understand this particular error or shed some light on it that I could
> set on my end.

The tool at redbot.org shows the HTTP protocol and all the content at 
that refreshed URL is all relatively normal. Some Vary issues, but that 
should not be leading to redirect loops.


Since the error is showing up in the browser and not easily visible in 
the server traffic I think the best place to look would be to debug what 
the browser is doing exactly. It probably has something to do with how 
it handles those cert errors (ie TLS-Everywhere misfeatures always 
trying to do broken https:// when http:// works fine).


Also, which Squid version are you using may matter. You didn't say which.

Amos


From m_zouhairy at skno.by  Fri Nov  3 09:42:33 2017
From: m_zouhairy at skno.by (Vacheslav)
Date: Fri, 3 Nov 2017 12:42:33 +0300
Subject: [squid-users] can't block streaming
In-Reply-To: <4c084243-8480-9d8b-6caf-ff6c9d503b3a@treenet.co.nz>
References: <01df01d35227$6188bdf0$249a39d0$@skno.by>
 <0142c03e-f30b-f1a9-b331-2d1c7e9697a0@treenet.co.nz>
 <006901d352ef$0b1d7e60$21587b20$@skno.by>
 <4c084243-8480-9d8b-6caf-ff6c9d503b3a@treenet.co.nz>
Message-ID: <001201d35488$1315af30$39410d90$@skno.by>



-----Original Message-----
From: squid-users [mailto:squid-users-bounces at lists.squid-cache.org] On Behalf Of Amos Jeffries
Sent: Wednesday, November 1, 2017 3:52 PM
To: squid-users at lists.squid-cache.org
Subject: Re: [squid-users] can't block streaming

On 01/11/17 21:54, Vacheslav wrote:
> Thanks for your time,
>
> -----Original Message-----
> From: Amos Jeffries
> Sent: Tuesday, October 31, 2017 5:45 PM
>
> On 31/10/17 22:05, Vacheslav wrote:
>> Peace,
>>
>> I tired searching and debugging but I couldn?t find a solution, 
>> whatever I do youtube keeps working.
>>
>> Here is my configuration:
> ...
>> # Media Streams
>>
>> ## MediaPlayer MMS Protocol
>>
>> acl media rep_mime_type mms
>>
>> acl mediapr url_regex dvrplayer mediastream ^mms://
>>
>> ## (Squid does not yet handle the URI as a known proto type.)
>
>> Unsupported URI schemes should result in the client receiving an HTTP 
>> error page instead of Squid handling the traffic.
>
>> Which also explains your problems: the Browser is either not using 
>> the proxy at all for this traffic, or sending the traffic through a 
>> CONNECT tunnel that is allowed to be created for other reasons.
>
> Well I tried unchecking automatically detect proxy settings. There are 
> 2 network cards on the squid, one with a gateway, the same  is used as 
> the proxy ip port 3128 and youtube is not in the bypass proxylist. I 
> tried using opera, the same result.

>Things like YT do not have to be on any bypass list to avoid the proxy.
>It just has to have a URL scheme for some protocol the browser detects as not able to go through the HTTP-only proxy. eg "mms:"

>Since mms:// means a non-HTTP protocol and it is not commonly supported by HTTP proxies, the browsers usually send it directly >to the mms protocol port(s) AFAIK.

Well I tired switching the ip of the pc to one that can't do http and https at all without proxy. I tested it without proxy enabled and internet sites don't open, I switched the proxy back on and youtube works when it is forbidden.


> What do you mean by a connect tunnel?

>Things like this:

"
  >CONNECT r1---sn-ntqe6n76.googlevideo.com:443 HTTP/1.1

  >... non-HTTP data stream.
"

>Which tells Squid to open a TCP connection to the named server and port.
That is how a YouTube video I'm watching right now is currently going through a test Squid. The browser of course shows it as a GET request for some https: URI, but the proxy only sees that CONNECT.

To see what is inside that particular port 443 tunnel one has to use SSL_Bump feature to decrypt the HTTPS protocol that is supposed to be on that port.


> ...
>
>> # We strongly recommend the following be uncommented to protect 
>> innocent
>>
>> # web applications running on the proxy server who think the only
>>
>> # one who can access services on "localhost" is a local user
>>
>> #http_access deny to_localhost
>>
>> # Deny all blocked extension
>>
>> error_directory /usr/share/squid/errors/en
>>
>> deny_info ERR_BLOCKED_FILES blockfiles
>>
>> http_access deny blockfiles
>>
>> #
>>
>> # INSERT YOUR OWN RULE(S) HERE TO ALLOW ACCESS FROM YOUR CLIENTS
>>
>
>> Please read the above line, and consider all the custom rules you 
>> placed above it.
> I moved the below text to under
> # INSERT YOUR OWN RULE(S) HERE TO ALLOW ACCESS FROM YOUR CLIENTS
>
> http_access deny mediapr
> http_access deny mediapr1
> http_access deny mediapr2
> http_access deny mediapr3
> http_reply_access deny media
> ...
>>
>> #url_rewrite_program /usr/sbin/squidGuard
>>
>> #url_rewrite_children 5
>>
>> #debug_options ALL,1 33,2 28,9
>>
>> And where must I place the before last 2 lines in order for squid 
>> guard to work?
>>
>
>> Right there where they are in your config will do.
>
>> What do you expect SquidGuard to do?
>
> At first, I thought squid guard is needed to block file extension, 
> then I discovered that it blocks urls so it is not a bad idea to block 
> porn sites and porn search terms.

>Ah, I see. Well, if you are new to it I advise to try using squid.conf ACLs first. Sending things to helpers is quite I/O and memory intensive and most of what SG does can be done better by modern Squid.

Also, SquidGuard specifically is very outdated software and no longer maintained. If you have to do access control in a helper at all it is better to use the external_acl_type interface and other helpers that meet the more specific need.

Well then, I'll go with your advice and not use prehistoric software.

>
>> If Squid itself cannot identify any URLs with "mms://" scheme there 
>> is no hope of SG being passed the non-existent URLs.
>
> This I didn't digest!
>

>See above with the CONNECT example. *If* the request is actually going through the proxy, the URI as far as Squid can see would be something like "r1---sn-ntqe6n76.googlevideo.com:443", or maybe just a raw-IP and port.

So what Squid can pass the URI helper is only that origin-form URI, not the encrypted (if HTTPS) or tunneled (if non-HTTP/HTTPS) absolute-URI stuff where the scheme is.

Amos
_______________________________________________
squid-users mailing list
squid-users at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-users




From squid3 at treenet.co.nz  Fri Nov  3 10:30:03 2017
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Fri, 3 Nov 2017 23:30:03 +1300
Subject: [squid-users] can't block streaming
In-Reply-To: <001201d35488$1315af30$39410d90$@skno.by>
References: <01df01d35227$6188bdf0$249a39d0$@skno.by>
 <0142c03e-f30b-f1a9-b331-2d1c7e9697a0@treenet.co.nz>
 <006901d352ef$0b1d7e60$21587b20$@skno.by>
 <4c084243-8480-9d8b-6caf-ff6c9d503b3a@treenet.co.nz>
 <001201d35488$1315af30$39410d90$@skno.by>
Message-ID: <d4d3675b-2865-e8fc-4cb5-850af7621e5c@treenet.co.nz>

On 03/11/17 22:42, Vacheslav wrote:
> 
> 
> -----Original Message-----
> From: Amos Jeffries
> Sent: Wednesday, November 1, 2017 3:52 PM
> 
> On 01/11/17 21:54, Vacheslav wrote:
>> Thanks for your time,
>>
>> -----Original Message-----
>> From: Amos Jeffries
>> Sent: Tuesday, October 31, 2017 5:45 PM
>>
>> On 31/10/17 22:05, Vacheslav wrote:
>>> Peace,
>>>
>>> I tired searching and debugging but I couldn?t find a solution,
>>> whatever I do youtube keeps working.
>>>
>>> Here is my configuration:
>> ...
>>> # Media Streams
>>>
>>> ## MediaPlayer MMS Protocol
>>>
>>> acl media rep_mime_type mms
>>>
>>> acl mediapr url_regex dvrplayer mediastream ^mms://
>>>
>>> ## (Squid does not yet handle the URI as a known proto type.)
>>
>>> Unsupport> To: squid-users at lists.squid-cache.org
> Subject: Re: [squid-users] can't block streaming
ed URI schemes should result in the client receiving an HTTP
>>> error page instead of Squid handling the traffic.
>>
>>> Which also explains your problems: the Browser is either not using
>>> the proxy at all for this traffic, or sending the traffic through a
>>> CONNECT tunnel that is allowed to be created for other reasons.
>>
>> Well I tried unchecking automatically detect proxy settings. There are
>> 2 network cards on the squid, one with a gateway, the same  is used as
>> the proxy ip port 3128 and youtube is not in the bypass proxylist. I
>> tried using opera, the same result.
> 
>> Things like YT do not have to be on any bypass list to avoid the proxy.
>> It just has to have a URL scheme for some protocol the browser detects as not able to go through the HTTP-only proxy. eg "mms:"
> 
>> Since mms:// means a non-HTTP protocol and it is not commonly supported by HTTP proxies, the browsers usually send it directly >to the mms protocol port(s) AFAIK.
> 
> Well I tired switching the ip of the pc to one that can't do http and https at all without proxy. I tested it without proxy enabled and internet sites don't open, I switched the proxy back on and youtube works when it is forbidden.
> 

That test is not conclusive enough I'm afraid. YouTube system is very 
complex and requires about a dozen transactions to take place in order 
to find the video content. Any one of those HTTP(S) responses may 
reference a non-HTTP video type as being in use at the end of the chain 
so your ACLs dont block it specifically.

That complexity might make it _seem_ easier to 'block' YT videos by 
breaking the chain of transactions. But unfortunately you have to 
isolate and block the right ones for that to happen without letting the 
client do any failover behaviour it might be capable of. AND many of 
them are buried inside encrypted tunnels nowdays.

So blocking one of Google service usually means blocking large areas, or 
all, of their other services as well. Unless you want to go down the 
MITM road and decrypt the HTTPS at the proxy - with limited success even 
then thanks to the cert pinning Google does.

Amos


From marcus.kool at urlfilterdb.com  Fri Nov  3 10:43:51 2017
From: marcus.kool at urlfilterdb.com (Marcus Kool)
Date: Fri, 3 Nov 2017 08:43:51 -0200
Subject: [squid-users] can't block streaming
In-Reply-To: <001201d35488$1315af30$39410d90$@skno.by>
References: <01df01d35227$6188bdf0$249a39d0$@skno.by>
 <0142c03e-f30b-f1a9-b331-2d1c7e9697a0@treenet.co.nz>
 <006901d352ef$0b1d7e60$21587b20$@skno.by>
 <4c084243-8480-9d8b-6caf-ff6c9d503b3a@treenet.co.nz>
 <001201d35488$1315af30$39410d90$@skno.by>
Message-ID: <55e16909-538c-bf85-e188-a9938e2ca6a4@urlfilterdb.com>

It is not clear what exactly you want to achieve.
Block everything from youtube ?

Amos told you that squidGuard is not maintained for many years but forgot to mention that ufdbGuard does the same thing and has regular updates.
ufdbGuard has a feature to block a set of Youtube videos identified by the video ID and automagically block all related images too.

Marcus


On 03/11/17 07:42, Vacheslav wrote:
> 
> 
> -----Original Message-----
> From: squid-users [mailto:squid-users-bounces at lists.squid-cache.org] On Behalf Of Amos Jeffries
> Sent: Wednesday, November 1, 2017 3:52 PM
> To: squid-users at lists.squid-cache.org
> Subject: Re: [squid-users] can't block streaming
> 
> On 01/11/17 21:54, Vacheslav wrote:
>> Thanks for your time,
>>
>> -----Original Message-----
>> From: Amos Jeffries
>> Sent: Tuesday, October 31, 2017 5:45 PM
>>
>> On 31/10/17 22:05, Vacheslav wrote:
>>> Peace,
>>>
>>> I tired searching and debugging but I couldn?t find a solution,
>>> whatever I do youtube keeps working.
>>>
>>> Here is my configuration:
>> ...
>>> # Media Streams
>>>
>>> ## MediaPlayer MMS Protocol
>>>
>>> acl media rep_mime_type mms
>>>
>>> acl mediapr url_regex dvrplayer mediastream ^mms://
>>>
>>> ## (Squid does not yet handle the URI as a known proto type.)
>>
>>> Unsupported URI schemes should result in the client receiving an HTTP
>>> error page instead of Squid handling the traffic.
>>
>>> Which also explains your problems: the Browser is either not using
>>> the proxy at all for this traffic, or sending the traffic through a
>>> CONNECT tunnel that is allowed to be created for other reasons.
>>
>> Well I tried unchecking automatically detect proxy settings. There are
>> 2 network cards on the squid, one with a gateway, the same  is used as
>> the proxy ip port 3128 and youtube is not in the bypass proxylist. I
>> tried using opera, the same result.
> 
>> Things like YT do not have to be on any bypass list to avoid the proxy.
>> It just has to have a URL scheme for some protocol the browser detects as not able to go through the HTTP-only proxy. eg "mms:"
> 
>> Since mms:// means a non-HTTP protocol and it is not commonly supported by HTTP proxies, the browsers usually send it directly >to the mms protocol port(s) AFAIK.
> 
> Well I tired switching the ip of the pc to one that can't do http and https at all without proxy. I tested it without proxy enabled and internet sites don't open, I switched the proxy back on and youtube works when it is forbidden.
> 
> 
>> What do you mean by a connect tunnel?
> 
>> Things like this:
> 
> "
>    >CONNECT r1---sn-ntqe6n76.googlevideo.com:443 HTTP/1.1
> 
>    >... non-HTTP data stream.
> "
> 
>> Which tells Squid to open a TCP connection to the named server and port.
> That is how a YouTube video I'm watching right now is currently going through a test Squid. The browser of course shows it as a GET request for some https: URI, but the proxy only sees that CONNECT.
> 
> To see what is inside that particular port 443 tunnel one has to use SSL_Bump feature to decrypt the HTTPS protocol that is supposed to be on that port.
> 
> 
>> ...
>>
>>> # We strongly recommend the following be uncommented to protect
>>> innocent
>>>
>>> # web applications running on the proxy server who think the only
>>>
>>> # one who can access services on "localhost" is a local user
>>>
>>> #http_access deny to_localhost
>>>
>>> # Deny all blocked extension
>>>
>>> error_directory /usr/share/squid/errors/en
>>>
>>> deny_info ERR_BLOCKED_FILES blockfiles
>>>
>>> http_access deny blockfiles
>>>
>>> #
>>>
>>> # INSERT YOUR OWN RULE(S) HERE TO ALLOW ACCESS FROM YOUR CLIENTS
>>>
>>
>>> Please read the above line, and consider all the custom rules you
>>> placed above it.
>> I moved the below text to under
>> # INSERT YOUR OWN RULE(S) HERE TO ALLOW ACCESS FROM YOUR CLIENTS
>>
>> http_access deny mediapr
>> http_access deny mediapr1
>> http_access deny mediapr2
>> http_access deny mediapr3
>> http_reply_access deny media
>> ...
>>>
>>> #url_rewrite_program /usr/sbin/squidGuard
>>>
>>> #url_rewrite_children 5
>>>
>>> #debug_options ALL,1 33,2 28,9
>>>
>>> And where must I place the before last 2 lines in order for squid
>>> guard to work?
>>>
>>
>>> Right there where they are in your config will do.
>>
>>> What do you expect SquidGuard to do?
>>
>> At first, I thought squid guard is needed to block file extension,
>> then I discovered that it blocks urls so it is not a bad idea to block
>> porn sites and porn search terms.
> 
>> Ah, I see. Well, if you are new to it I advise to try using squid.conf ACLs first. Sending things to helpers is quite I/O and memory intensive and most of what SG does can be done better by modern Squid.
> 
> Also, SquidGuard specifically is very outdated software and no longer maintained. If you have to do access control in a helper at all it is better to use the external_acl_type interface and other helpers that meet the more specific need.
> 
> Well then, I'll go with your advice and not use prehistoric software.
> 
>>
>>> If Squid itself cannot identify any URLs with "mms://" scheme there
>>> is no hope of SG being passed the non-existent URLs.
>>
>> This I didn't digest!
>>
> 
>> See above with the CONNECT example. *If* the request is actually going through the proxy, the URI as far as Squid can see would be something like "r1---sn-ntqe6n76.googlevideo.com:443", or maybe just a raw-IP and port.
> 
> So what Squid can pass the URI helper is only that origin-form URI, not the encrypted (if HTTPS) or tunneled (if non-HTTP/HTTPS) absolute-URI stuff where the scheme is.
> 
> Amos
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
> 
> 
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
> 


From eliezer at ngtech.co.il  Fri Nov  3 11:54:02 2017
From: eliezer at ngtech.co.il (Eliezer Croitoru)
Date: Fri, 3 Nov 2017 13:54:02 +0200
Subject: [squid-users] Squid 3.5 ICAP Problems
In-Reply-To: <3d7f5b00-c1f7-4277-e1b9-dfeac355c617@measurement-factory.com>
References: <CAOKqvx5XPdoYZjMap1NyDO+ERraK4nxvxFBT2k=ysTiXM+MjOg@mail.gmail.com>
 <3d7f5b00-c1f7-4277-e1b9-dfeac355c617@measurement-factory.com>
Message-ID: <0a3b01d3549a$71120720$53361560$@ngtech.co.il>

I'm not sure but after testing with telnet  or\and nc you can try to verify the open files limits on the system which might cause this issue.
To identify how many connections are opened to the service you can use netstat or ss tools.
netstat -ntp

and a similar on ss.

Also it's a good practice to put an ICAP periodic(2-20 seconds apparat) with an OPTIONS request to make sure that the service is alive.
If you want my testing script let me know.

Eliezer

----
Eliezer Croitoru
Linux System Administrator
Mobile: +972-5-28704261
Email: eliezer at ngtech.co.il



-----Original Message-----
From: squid-users [mailto:squid-users-bounces at lists.squid-cache.org] On Behalf Of Alex Rousskov
Sent: Thursday, November 2, 2017 19:22
To: Stephen Stark <logic4life at gmail.com>; squid-users at lists.squid-cache.org
Subject: Re: [squid-users] Squid 3.5 ICAP Problems

On 11/02/2017 10:29 AM, Stephen Stark wrote:
> Adaptation::Icap::Xaction::noteCommConnected(local=[::]
> remote=127.0.0.1:1344 flags=1, errno=101, ...

The logs you have provided do not show where/why exactly the TCP connection to the ICAP service fails, but error number 101 is probably "network unreachable". This is unusual but not impossible for localhost traffic. The next step depends on the failure cause. There are at least two major cases to consider:

A) If Squid sends packets to 127.0.0.1:1344, then you can easily reproduce the problem using something like "telnet" or "nc" on the Squid box command line. Just make sure you use the right _source_ IP address for the connection! It has to be the same source IP address that Squid is using. It might not be 127.0.0.1. Running that command as Squid user might also be important if the Squid box have some fancy user-specific networking restrictions.

B) If Squid does not send packets to 127.0.0.1:1344, then one can figure out what goes wrong by studying relevant ALL,9 logs. You may also want to address other errors or warnings Squid logs to cache.log (if any).

You can determine whether Squid sends packets to 127.0.0.1:1344 by collecting a packet trace (for all Squid box interfaces!) and/or running strace (for the Squid worker process).


HTH,

Alex.
_______________________________________________
squid-users mailing list
squid-users at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-users



From flashdown at data-core.org  Fri Nov  3 14:16:11 2017
From: flashdown at data-core.org (Flashdown)
Date: Fri, 03 Nov 2017 15:16:11 +0100
Subject: [squid-users] Squid 3.5 ICAP Problems
In-Reply-To: <0a3b01d3549a$71120720$53361560$@ngtech.co.il>
References: <CAOKqvx5XPdoYZjMap1NyDO+ERraK4nxvxFBT2k=ysTiXM+MjOg@mail.gmail.com>
 <3d7f5b00-c1f7-4277-e1b9-dfeac355c617@measurement-factory.com>
 <0a3b01d3549a$71120720$53361560$@ngtech.co.il>
Message-ID: <daa7ff32f506df9a3945574b6acf5e8d@data-core.org>

Hey Guys,

I also have a long term issue with C-ICAP on many servers in different 
countries.

It looks like an ICAP Queue overload happening within squid. Since after 
taking a lot time in order to fine tune and debug the C-ICAP Service and 
the clamav-daemon in order to ensure that it can handle the high load 
and is configured properly, the ICAP suspended, down and UP messages 
dissappeared entirely in the cache.log, which I had after some days, 
then some weeks (after tuning the ICAP Server even more) occuring and 
after some messages the performance become steady slow until squid 
service is restarted. So now I have fixed the C-ICAP Server that is 
using squidclamav as C-ICAP Module for in-stream scanning. 
http://squidclamav.darold.net/, So I no longer can see any C-ICAP DOWN 
and UP Messages anymore. Which tells me that C-ICAP seems to always have 
enough workers to take the load.

So as I said now after finalizing my C-ICAP Server fine tuning, 
reconfiguration and debugging. I have no more ICAP Up and down messages 
and now after some weeks squid became slow again. When restarting the 
squid service all works fine again until some day in some weeks where it 
becomes slow again until service restart.

So I guess I need to set the right debug options for you guys, but that 
would create a very big log file since it takes days or weeks until this 
issue is happening. Any suggestions on what you think how I can do 
debugging in the best way?


BTW: I hope I do not mix up issues here, but it seems to be related to 
my issue that I am face since I was moving to a newer Version back in 
the old days then squid 3.5.12 <- somewhere there this story started for 
me. Unsure if it was exactliy starting with this version, some versions 
later or some earlier. But well it definitely was not happening on 
3.5.10 or earlier, so here I can cut it a  bit. So the issue was 
definitely introduced somewhere between 3.5.11 until 3.5.18. But as I 
said now I can at least fully outline the C-ICAP Server, which the cache 
log confirms by not showing any down, suspended and up messages any 
more.

I believe when I enforce a service restart once in the night, I will 
never see this issue of slow performance caused by the C-ICAP Module 
within squid anymore.

Best regards,
Enrico/Flashdown



Am 2017-11-03 12:54, schrieb Eliezer Croitoru:
> I'm not sure but after testing with telnet  or\and nc you can try to
> verify the open files limits on the system which might cause this
> issue.
> To identify how many connections are opened to the service you can use
> netstat or ss tools.
> netstat -ntp
> 
> and a similar on ss.
> 
> Also it's a good practice to put an ICAP periodic(2-20 seconds
> apparat) with an OPTIONS request to make sure that the service is
> alive.
> If you want my testing script let me know.
> 
> Eliezer
> 
> ----
> Eliezer Croitoru
> Linux System Administrator
> Mobile: +972-5-28704261
> Email: eliezer at ngtech.co.il
> 
> 
> 
> -----Original Message-----
> From: squid-users [mailto:squid-users-bounces at lists.squid-cache.org]
> On Behalf Of Alex Rousskov
> Sent: Thursday, November 2, 2017 19:22
> To: Stephen Stark <logic4life at gmail.com>; 
> squid-users at lists.squid-cache.org
> Subject: Re: [squid-users] Squid 3.5 ICAP Problems
> 
> On 11/02/2017 10:29 AM, Stephen Stark wrote:
>> Adaptation::Icap::Xaction::noteCommConnected(local=[::]
>> remote=127.0.0.1:1344 flags=1, errno=101, ...
> 
> The logs you have provided do not show where/why exactly the TCP
> connection to the ICAP service fails, but error number 101 is probably
> "network unreachable". This is unusual but not impossible for
> localhost traffic. The next step depends on the failure cause. There
> are at least two major cases to consider:
> 
> A) If Squid sends packets to 127.0.0.1:1344, then you can easily
> reproduce the problem using something like "telnet" or "nc" on the
> Squid box command line. Just make sure you use the right _source_ IP
> address for the connection! It has to be the same source IP address
> that Squid is using. It might not be 127.0.0.1. Running that command
> as Squid user might also be important if the Squid box have some fancy
> user-specific networking restrictions.
> 
> B) If Squid does not send packets to 127.0.0.1:1344, then one can
> figure out what goes wrong by studying relevant ALL,9 logs. You may
> also want to address other errors or warnings Squid logs to cache.log
> (if any).
> 
> You can determine whether Squid sends packets to 127.0.0.1:1344 by
> collecting a packet trace (for all Squid box interfaces!) and/or
> running strace (for the Squid worker process).
> 
> 
> HTH,
> 
> Alex.
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
> 
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users


From squid3 at treenet.co.nz  Fri Nov  3 15:17:46 2017
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sat, 4 Nov 2017 04:17:46 +1300
Subject: [squid-users] Squid 3.5 ICAP Problems
In-Reply-To: <daa7ff32f506df9a3945574b6acf5e8d@data-core.org>
References: <CAOKqvx5XPdoYZjMap1NyDO+ERraK4nxvxFBT2k=ysTiXM+MjOg@mail.gmail.com>
 <3d7f5b00-c1f7-4277-e1b9-dfeac355c617@measurement-factory.com>
 <0a3b01d3549a$71120720$53361560$@ngtech.co.il>
 <daa7ff32f506df9a3945574b6acf5e8d@data-core.org>
Message-ID: <fdcc9657-8fec-519d-17cf-6f0b9f9299e2@treenet.co.nz>

You seem to be mixing up terminology and software names quite a bit and 
its making your post quite confusing even for those of us who know the 
subject matter.

"C-ICAP" is the name of a piece of server software. There is no "C-ICAP 
module in Squid".

ICAP is a networking protocol. Both C-ICAP and Squid software support 
and use the ICAP protocol to exchange messages between each other.

AFAIK, there are no queues in Squid ICAP handling. Messages are 
delivered sequentially on ICAP connections with the response waited for 
before the next is sent. There _are_ multiple persistent connections 
running in parallel, but should not be message pipelining.

The UP/DOWN messages in cache.log occur when the ICAP protocol server 
(C-ICAP software) stops responding to the small OPTIONS messages.

Amos


From rousskov at measurement-factory.com  Fri Nov  3 15:35:38 2017
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Fri, 3 Nov 2017 09:35:38 -0600
Subject: [squid-users] Squid 3.5 ICAP Problems
In-Reply-To: <fdcc9657-8fec-519d-17cf-6f0b9f9299e2@treenet.co.nz>
References: <CAOKqvx5XPdoYZjMap1NyDO+ERraK4nxvxFBT2k=ysTiXM+MjOg@mail.gmail.com>
 <3d7f5b00-c1f7-4277-e1b9-dfeac355c617@measurement-factory.com>
 <0a3b01d3549a$71120720$53361560$@ngtech.co.il>
 <daa7ff32f506df9a3945574b6acf5e8d@data-core.org>
 <fdcc9657-8fec-519d-17cf-6f0b9f9299e2@treenet.co.nz>
Message-ID: <1aea0845-e4dc-6262-3f8e-58c59f7e673b@measurement-factory.com>

On 11/03/2017 09:17 AM, Amos Jeffries wrote:

> AFAIK, there are no queues in Squid ICAP handling. 

ICAP queues do exist. For example, Squid may queue ICAP requests (while
they are waiting for a connection slot) in order to obey Max-Connections
limits imposed by the ICAP service or squid.conf.

Alex.


From rousskov at measurement-factory.com  Fri Nov  3 15:40:52 2017
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Fri, 3 Nov 2017 09:40:52 -0600
Subject: [squid-users] Squid 3.5 ICAP Problems
In-Reply-To: <daa7ff32f506df9a3945574b6acf5e8d@data-core.org>
References: <CAOKqvx5XPdoYZjMap1NyDO+ERraK4nxvxFBT2k=ysTiXM+MjOg@mail.gmail.com>
 <3d7f5b00-c1f7-4277-e1b9-dfeac355c617@measurement-factory.com>
 <0a3b01d3549a$71120720$53361560$@ngtech.co.il>
 <daa7ff32f506df9a3945574b6acf5e8d@data-core.org>
Message-ID: <53944c02-a754-16ec-8025-9ee6b145f7d1@measurement-factory.com>

On 11/03/2017 08:16 AM, Flashdown wrote:

> So I guess I need to set the right debug options for you guys, but that
> would create a very big log file since it takes days or weeks until this
> issue is happening. Any suggestions on what you think how I can do
> debugging in the best way?

You can turn detailed debugging on when the problem re-appears. Search
Squid wiki for "-k debug" for a related discussion and advice.

However, for performance issues, it is best to try to isolate the
bottleneck _before_ slowing down Squid with detailed debugging. That is
difficult to do and there are no scripted actions to follow, but you can
start by logging (access.log and icap.log) various response times and
the number of open connections (/proc/ or netstat -n) to narrow down
suspects.


HTH,

Alex.


From jeffmerkey at gmail.com  Fri Nov  3 19:35:05 2017
From: jeffmerkey at gmail.com (Jeffrey Merkey)
Date: Fri, 3 Nov 2017 13:35:05 -0600
Subject: [squid-users] Google Chrome reports "Too many redirects" on
 ssl-dumped connections with LA Times News Website
In-Reply-To: <7f7335a6-d9a4-2415-fbd1-565424de7002@treenet.co.nz>
References: <CA+ekxPVtc9b2nDNvAsmT=ebEy6PRZbB91A-JVDoeeuxfGz7wOQ@mail.gmail.com>
 <7f7335a6-d9a4-2415-fbd1-565424de7002@treenet.co.nz>
Message-ID: <CA+ekxPV6Sg-9O8ZriS+HP8riAiTdyj4tfMxXvSfUpSgyTAC5fw@mail.gmail.com>

On 11/3/17, Amos Jeffries <squid3 at treenet.co.nz> wrote:
> On 03/11/17 19:45, Jeffrey Merkey wrote:
>> This error is extremely hard to reproduce, and I found it can be
>> cleared by restarting squid, which seems to make it go away.   It
>> seems to take several hours of non-stop proxy use then once the error
>> occurs the we browser reports "too many redirects" and certificate
>> errors.
>>
>> Doing a restart on Centos 7 clears it:
>>
>> # systemctl restart squid
>>
>> The log shows some sort of "refresh unmodified state before it happens:
>>
>> 1509690588.252    167 127.0.0.1 TAG_NONE/200 0 CONNECT
>> events.bouncex.net:443 - HIER_DIRECT/35.190.62.200 -
>> 1509690588.272    210 127.0.0.1 TAG_NONE/200 0 CONNECT
>> analytics.twitter.com:443 - HIER_DIRECT/199.59.149.200 -
>> 1509690588.280     62 127.0.0.1 TCP_REFRESH_UNMODIFIED/200 38412 GET
>> http://www.latimes.com/nation/la-na-vegas-shooting-sheriff-20171102-story.html
>> - HIER_DIRECT/104.120.143.198 text/html      <================== error
>> is here
>
> This is a 200 status response. So whatever "redirection" is occuring is
> not part of the HTTP for that transaction.
>
> The refresh means that something was cached beforehand but was stale so
> the server had to be asked for permission to deliver it. UNMODIFIED
> means the server responded by indicating it was okay to use.
>
>> 1509690588.356    220 127.0.0.1 TCP_MISS/200 960 GET
>> https://partners.tremorhub.com/syncnoad? - HIER_DIRECT/34.228.123.38
>> text/xml
>> 1509690588.366    304 127.0.0.1 TAG_NONE/200 0 CONNECT
>> geo.moatads.com:443 - HIER_DIRECT/52.21.172.68 -
>> 1509690588.374    303 127.0.0.1 TAG_NONE/200 0 CONNECT
>> rtr.innovid.com:443 - HIER_DIRECT/13.58.208.14 -
>> 1509690588.377     33 127.0.0.1 TCP_MISS/200 498 GET
>> https://tribpubdfp745347008913.s.moatpixel.com/pixel.gif? - HIER_
>>
>> If there are particulars and I attempt to recreate this problem are
>> there any specific logging parms or settings that would help you
>> understand this particular error or shed some light on it that I could
>> set on my end.
>
> The tool at redbot.org shows the HTTP protocol and all the content at
> that refreshed URL is all relatively normal. Some Vary issues, but that
> should not be leading to redirect loops.
>
>
> Since the error is showing up in the browser and not easily visible in
> the server traffic I think the best place to look would be to debug what
> the browser is doing exactly. It probably has something to do with how
> it handles those cert errors (ie TLS-Everywhere misfeatures always
> trying to do broken https:// when http:// works fine).
>
>
> Also, which Squid version are you using may matter. You didn't say which.
>
> Amos
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>

Hi Amos,

Thanks for responding, the squid version is:

Squid Cache: Version 3.5.27
Service Name: squid

This binary uses OpenSSL 1.0.1e-fips 11 Feb 2013. For legal
restrictions on distribution see
https://www.openssl.org/source/license.html

configure options:  '--with-openssl' '--enable-ssl'
'--enable-ssl-crtd' '--enable-http-violations'

I also wanted to let you know that I upgraded my Chrome browser about
a week ago and that's when the redirect errors started showing up.
This makes me lean towards the possibility that it's a bug of some
sort in the Chrome browser itself.   What makes me suspect another bug
in Squid is the fact that restarting the squid server clears the
browser error.  I will attempt to log the error better the next time I
see it and perhaps that will help run it down.  If the bug is in
Chrome then its clearly not a problem with Squid, but the fact that
reloading squid clears the bug gives me pause to review both.

The specific Chrome version I am seeing this error with is:

obtained from about:version

Google Chrome	60.0.3112.101 (Official Build) (64-bit)
Revision	1f3c0cf4b3083dfbe4da434af1726820cf384ce3-refs/branch-heads/3112@{#723}
OS	Linux
JavaScript	V8 6.0.286.54
Flash	27.0.0.183
/home/jmerkey/.config/google-chrome/PepperFlash/27.0.0.183/libpepflashplayer.so
User Agent	Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML,
like Gecko) Chrome/60.0.3112.101 Safari/537.36
Command Line	/usr/bin/google-chrome-stable --flag-switches-begin
--flag-switches-end
Executable Path	/opt/google/chrome/google-chrome
Profile Path	/home/jmerkey/.config/google-chrome/Profile 1
Variations	241fff6c-4eda1c57
3095aa95-3f4a17df
7c1bc906-f55a7974
47e5d3db-3d47f4f4
d43bf3e5-bd7cd813
ba3f87da-45bda656
5ca89f9-3f4a17df
f3499283-7711d854
9e201a2b-7e3ae057
5b3ed0a1-3f4a17df
68812885-4d2fac87
9bd94ed7-b1c9f6b0
b791c1b8-3f4a17df
9773d3bd-f23d1dea
2e109477-f3b42e62
99144bc3-3cc2175e
9e5c75f1-dadcfe94
f79cb77b-3d47f4f4
b7786474-d93a0620
27219e67-b2047178
23a898eb-e0e2610f
64224f74-5087fa4a
56302f8c-2f882e70
de03e059-e65e20f2
f56e0452-f23d1dea
1354da85-f34af386
494d8760-91c810ef
3ac60855-486e2a9c
f296190c-a0af34c0
4442aae2-75cb33fc
ed1d377-e1cc0f14
75f0f0a0-e1cc0f14
e2b18481-e1cc0f14
e7e71889-e1cc0f14
828a5926-9d7acf42
a88c475d-3d47f4f4

Jeff


From logic4life at gmail.com  Fri Nov  3 21:45:01 2017
From: logic4life at gmail.com (Stephen Stark)
Date: Fri, 3 Nov 2017 17:45:01 -0400
Subject: [squid-users] Squid 3.5 ICAP Problems
In-Reply-To: <CAOKqvx5VKX-xN+0Zu5xKx3uhwKWq4-pR_EcUTKg-Hi5UFp35Ag@mail.gmail.com>
References: <CAOKqvx5XPdoYZjMap1NyDO+ERraK4nxvxFBT2k=ysTiXM+MjOg@mail.gmail.com>
 <3d7f5b00-c1f7-4277-e1b9-dfeac355c617@measurement-factory.com>
 <0a3b01d3549a$71120720$53361560$@ngtech.co.il>
 <daa7ff32f506df9a3945574b6acf5e8d@data-core.org>
 <53944c02-a754-16ec-8025-9ee6b145f7d1@measurement-factory.com>
 <CAOKqvx5VKX-xN+0Zu5xKx3uhwKWq4-pR_EcUTKg-Hi5UFp35Ag@mail.gmail.com>
Message-ID: <CAOKqvx6m+ndDgbtQBT7OaaEsx_S1rdAEEkYZ+x1FRqEeKZxcwQ@mail.gmail.com>

Hello,
I found put my problem.

c-icap service was running fine.
I had a modification on FwdState.cc that was not handling any localhost
request. So it squid could not resolve host.

Thanks for the help! It helped me find where the problem was using debug
options ALL,9.


On Nov 3, 2017 11:41 AM, "Alex Rousskov" <rousskov at measurement-factory.com>
wrote:

On 11/03/2017 08:16 AM, Flashdown wrote:

> So I guess I need to set the right debug options for you guys, but that
> would create a very big log file since it takes days or weeks until this
> issue is happening. Any suggestions on what you think how I can do
> debugging in the best way?

You can turn detailed debugging on when the problem re-appears. Search
Squid wiki for "-k debug" for a related discussion and advice.

However, for performance issues, it is best to try to isolate the
bottleneck _before_ slowing down Squid with detailed debugging. That is
difficult to do and there are no scripted actions to follow, but you can
start by logging (access.log and icap.log) various response times and
the number of open connections (/proc/ or netstat -n) to narrow down
suspects.


HTH,

Alex.
_______________________________________________
squid-users mailing list
squid-users at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-users
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20171103/f6e05ee9/attachment.htm>

From nilesh.gavali at tcs.com  Mon Nov  6 12:22:11 2017
From: nilesh.gavali at tcs.com (Nilesh Gavali)
Date: Mon, 6 Nov 2017 12:22:11 +0000
Subject: [squid-users] window media player can not play .wax file via Squid
	proxy
Message-ID: <OF06172801.D41F2471-ON802581D0.00435D9F-802581D0.0043F244@tcs.com>

Hi Team.
We are having squid deployed for users which work well for browsing 
internet. Recently we received complaint from some users who access one of 
the vendor URL to listen audio recordings.
The procedure they follow as
login to Vendor portal URL via squid proxy
select recording option
select recording , it will get downloaded as .wax file
user open .wax file in media player after which they get error message as 
unable to play media.

I checked the .wax file which contains link to media file on their same 
portal URL. but window media player cant access it.

Not sure what is wrong here. I traced squid access logs and able to see 
the request going to vendor portal but still window media player cant play 
this file

Need urgent help if anyone work on this kind of requirement.


Thanks & Regards
Nilesh Suresh Gavali
=====-----=====-----=====
Notice: The information contained in this e-mail
message and/or attachments to it may contain 
confidential or privileged information. If you are 
not the intended recipient, any dissemination, use, 
review, distribution, printing or copying of the 
information contained in this e-mail message 
and/or attachments to it are strictly prohibited. If 
you have received this communication in error, 
please notify us by reply e-mail or telephone and 
immediately and permanently delete the message 
and any attachments. Thank you


-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20171106/44c0b16c/attachment.htm>

From Antony.Stone at squid.open.source.it  Mon Nov  6 12:36:07 2017
From: Antony.Stone at squid.open.source.it (Antony Stone)
Date: Mon, 6 Nov 2017 12:36:07 +0000
Subject: [squid-users] window media player can not play .wax file via
	Squid proxy
In-Reply-To: <OF06172801.D41F2471-ON802581D0.00435D9F-802581D0.0043F244@tcs.com>
References: <OF06172801.D41F2471-ON802581D0.00435D9F-802581D0.0043F244@tcs.com>
Message-ID: <201711061236.07563.Antony.Stone@squid.open.source.it>

On Monday 06 November 2017 at 12:22:11, Nilesh Gavali wrote:

> Hi Team.
> We are having squid deployed for users which work well for browsing
> internet. Recently we received complaint from some users who access one of
> the vendor URL to listen audio recordings.
> The procedure they follow as
> login to Vendor portal URL via squid proxy
> select recording option
> select recording , it will get downloaded as .wax file
> user open .wax file in media player after which they get error message as
> unable to play media.
> 
> I checked the .wax file which contains link to media file on their same
> portal URL. but window media player cant access it.

What type of URL is it?  HTTP?  HTTPS?  Something else?

> Not sure what is wrong here. I traced squid access logs and able to see
> the request going to vendor portal

Which request?

The browser request for the .wax file, or the media player request for the 
actual media file?

> but still window media player cant play this file

It basically sounds as though either:

 -  Windows media player is trying to access a URL directly without going 
through the proxy, or

 - Windows media player is requesting a URL type which Squid can't handle

Tell us what URLs you're seeing in the access log, and what URL is contained 
in the .wax file - that might give us more of a clue.


Antony.

-- 
Douglas was one of those writers who honourably failed to get anywhere with 
'weekending'.  It put a premium on people who could write things that lasted 
thirty seconds, and Douglas was incapable of writing a single sentence that 
lasted less than thirty seconds.

 - Geoffrey Perkins, about Douglas Adams

                                                   Please reply to the list;
                                                         please *don't* CC me.


From rentorbuy at yahoo.com  Tue Nov  7 11:42:11 2017
From: rentorbuy at yahoo.com (Vieri)
Date: Tue, 7 Nov 2017 11:42:11 +0000 (UTC)
Subject: [squid-users] Squid and CPU 100%
References: <348071111.3798275.1510054931204.ref@mail.yahoo.com>
Message-ID: <348071111.3798275.1510054931204@mail.yahoo.com>

Sorry to bring this back up, but every now and then (few days) I need to restart squid because its CPU usage goes up to 100% for a long time (irresponsive).

Right before restarting Squid:

# free [11/07/17 11:18:52]
total used free shared buff/cache available
Mem: 32865056 14811320 1374212 157028 16679524 17557656
Swap: 37036988 287320 36749668

# squidclient mgr:info [11/07/17 11:18:52]

# squidclient mgr:mem [11/07/17 11:19:02]

# squidclient mgr:storedir [11/07/17 11:19:22]

# squidclient mgr:filedescriptors [11/07/17 11:19:42]

# squidclient mgr:events [11/07/17 11:20:12]

In other words, squidclient does not get any info from squid (waited max 30 seconds - maybe I need to wait more).

I also dumped an strace here:
https://drive.google.com/file/d/1yed-sNWrc6KBQSOV7KPbrYOForwwtkCe/view?usp=sharing

You probably won't be able to draw any conclusions so I'll try to update to squid-3.5.27-20171101-re69e56c (I'm now running 3.5.27-20170916-re69e56c).

Note that stopping Squid with:

/usr/sbin/squid -k shutdown -f /etc/squid/squid.conf -n squid

takes a LONG time, but it eventually does stop (the process dies out).

I can then restart it cleanly.

I don't need to kill the pid (unless I'm told to hurry up).

After checking the log I can see these messages:

2017/11/07 11:16:59 kid1| WARNING! Your cache is running out of filedescriptors
2017/11/07 11:16:59 kid1| comm_open: socket failure: (24) Too many open files

Now, you can notice that the max_filedescriptors is quite high, and was set correctly before and after the issue I'm reporting:

# tail -n 100000 /var/log/squid/cache.log | grep -i descript
2017/11/06 06:36:14 kid1| With 32768 file descriptors available
2017/11/07 11:15:15 kid1| WARNING! Your cache is running out of filedescriptors
2017/11/07 11:15:31 kid1| WARNING! Your cache is running out of filedescriptors
2017/11/07 11:16:13 kid1| WARNING! Your cache is running out of filedescriptors
2017/11/07 11:16:59 kid1| WARNING! Your cache is running out of filedescriptors
2017/11/07 11:17:35 kid1| WARNING! Your cache is running out of filedescriptors
2017/11/07 11:17:51 kid1| WARNING! Your cache is running out of filedescriptors
2017/11/07 11:18:07 kid1| WARNING! Your cache is running out of filedescriptors
2017/11/07 11:18:25 kid1| WARNING! Your cache is running out of filedescriptors
2017/11/07 11:18:45 kid1| WARNING! Your cache is running out of filedescriptors
2017/11/07 11:19:03 kid1| WARNING! Your cache is running out of filedescriptors
2017/11/07 11:19:19 kid1| WARNING! Your cache is running out of filedescriptors
2017/11/07 11:19:35 kid1| WARNING! Your cache is running out of filedescriptors
2017/11/07 11:19:53 kid1| WARNING! Your cache is running out of filedescriptors
2017/11/07 11:20:09 kid1| WARNING! Your cache is running out of filedescriptors
2017/11/07 11:20:28 kid1| WARNING! Your cache is running out of filedescriptors
2017/11/07 11:20:44 kid1| WARNING! Your cache is running out of filedescriptors
2017/11/07 11:21:01 kid1| WARNING! Your cache is running out of filedescriptors
2017/11/07 11:23:05 kid1| With 32768 file descriptors available

So I'm worried that 32768 may not be enough.
Is this weird, or should I really increase this value?

Thanks,

Vieri


From rahmatellahmohammed at gmail.com  Tue Nov  7 14:34:36 2017
From: rahmatellahmohammed at gmail.com (rmohammed)
Date: Tue, 7 Nov 2017 07:34:36 -0700 (MST)
Subject: [squid-users] problem squid squidguard with outlook 2016
Message-ID: <1510065276421-0.post@n4.nabble.com>

I have a problem with outlook 2016 in my office,

When i activate squid and squidguard,

outlook stop to receive and send emails,

can anyone help me plz?

thanks



--
Sent from: http://squid-web-proxy-cache.1019090.n4.nabble.com/Squid-Users-f1019091.html


From rousskov at measurement-factory.com  Tue Nov  7 14:55:22 2017
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Tue, 7 Nov 2017 07:55:22 -0700
Subject: [squid-users] Squid and CPU 100%
In-Reply-To: <348071111.3798275.1510054931204@mail.yahoo.com>
References: <348071111.3798275.1510054931204.ref@mail.yahoo.com>
 <348071111.3798275.1510054931204@mail.yahoo.com>
Message-ID: <34c510cb-9ad4-ffab-7905-c0fd04ab6007@measurement-factory.com>

On 11/07/2017 04:42 AM, Vieri wrote:
> So I'm worried that 32768 may not be enough.
> Is this weird, or should I really increase this value?

Think about the underlying physics of what you are observing. It may
help reduce guessing and guide you towards a solution:

You can estimate the reasonable number of file descriptors using
expected maximum request rate and mean response time. Add, say 30%, to
account for long-lived persistent connections. Remember that Squid uses
one descriptor for the from-client connection and one descriptor for the
to-server connection. If that estimate is way below 32K, then the
current limit itself is not the problem. Otherwise, it probably is (and
then you probably need more Squid workers not more descriptors per worker).

* It is possible, perhaps even likely, that some some unknown problem
suddenly consumes almost all CPU cycles, drastically slowing Squid down,
and quickly consuming all file descriptors (because accepted connections
are not served fast enough).

* It is also possible, albeit less likely, that some unknown problem
slows Squid down over time and slowly leads to excessive file descriptor
use and even 100% CPU usage.

To distinguish the two cases, consider studying transaction response
times and the total number of connections logged every minute of every
hour. You should collect such stats for many other reasons anyway!

Alex.
P.S. I trust you have already checked all system logs for more clues and
found nothing of interest there.


From thesnable at gmail.com  Tue Nov  7 15:52:40 2017
From: thesnable at gmail.com (snable snable)
Date: Tue, 7 Nov 2017 16:52:40 +0100
Subject: [squid-users] 4.0.21 Ssl bump access denied
In-Reply-To: <CADYcWGS_hiYUN+DeA-pd2DeLseDF_53daBnkRBTArWqZXLQEVw@mail.gmail.com>
References: <CADYcWGSbu1AMvS6QCPBq8b6urLb1Ty7Cs2NgujpSex4Fp_wHTg@mail.gmail.com>
 <CADYcWGS_hiYUN+DeA-pd2DeLseDF_53daBnkRBTArWqZXLQEVw@mail.gmail.com>
Message-ID: <CADYcWGStxuT4aHpG3P13h-A9jnZ-PXWOzbZmV8Bkz6BeH+7uCQ@mail.gmail.com>

Hello

i forward from.my openwrt router the traffic for 443 and 80 to my squid box
to port 3129 and 3128

certificates gets created from squid

but i always get on every single page an access denied error from the proxy.

ssl_bump bump all

is configured

any idea?

thanka
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20171107/063ae173/attachment.htm>

From rentorbuy at yahoo.com  Tue Nov  7 16:28:56 2017
From: rentorbuy at yahoo.com (Vieri)
Date: Tue, 7 Nov 2017 16:28:56 +0000 (UTC)
Subject: [squid-users] ERR_ICAP_FAILURE unless squid reconfigure
References: <991998915.4009150.1510072136826.ref@mail.yahoo.com>
Message-ID: <991998915.4009150.1510072136826@mail.yahoo.com>

Hi,

I noticed that whenever I restart c-icap (http://c-icap.sourceforge.net/), the Squid cache HTTP clients display the ERR_ICAP_FAILURE page.
If I send the reconfigure action to the squid process then this "error" goes away.

How can I avoid having to "recofnigure" squid when c-icap is restarted (if possible)?

I have this in squid.conf:

icap_enable on
icap_send_client_ip on
icap_send_client_username on
icap_client_username_encode off
icap_client_username_header X-Authenticated-User
icap_preview_enable on
icap_preview_size 1024
icap_service squidclamav respmod_precache bypass=0 icap://127.0.0.1:1344/clamav
adaptation_access squidclamav allow all

Thanks,

Vieri


From flashdown at data-core.org  Tue Nov  7 16:41:13 2017
From: flashdown at data-core.org (Enrico Heine)
Date: Tue, 07 Nov 2017 17:41:13 +0100
Subject: [squid-users] ERR_ICAP_FAILURE unless squid reconfigure
In-Reply-To: <991998915.4009150.1510072136826@mail.yahoo.com>
References: <991998915.4009150.1510072136826.ref@mail.yahoo.com>
 <991998915.4009150.1510072136826@mail.yahoo.com>
Message-ID: <0E6EB9C9-6C9D-433A-B87A-755AFF752DBC@data-core.org>

Set bypass to 1

Am 7. November 2017 17:28:56 MEZ schrieb Vieri <rentorbuy at yahoo.com>:
>Hi,
>
>I noticed that whenever I restart c-icap
>(http://c-icap.sourceforge.net/), the Squid cache HTTP clients display
>the ERR_ICAP_FAILURE page.
>If I send the reconfigure action to the squid process then this "error"
>goes away.
>
>How can I avoid having to "recofnigure" squid when c-icap is restarted
>(if possible)?
>
>I have this in squid.conf:
>
>icap_enable on
>icap_send_client_ip on
>icap_send_client_username on
>icap_client_username_encode off
>icap_client_username_header X-Authenticated-User
>icap_preview_enable on
>icap_preview_size 1024
>icap_service squidclamav respmod_precache bypass=0
>icap://127.0.0.1:1344/clamav
>adaptation_access squidclamav allow all
>
>Thanks,
>
>Vieri
>_______________________________________________
>squid-users mailing list
>squid-users at lists.squid-cache.org
>http://lists.squid-cache.org/listinfo/squid-users

-- 
Diese Nachricht wurde von meinem Android-Ger?t mit K-9 Mail gesendet.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20171107/450b10db/attachment.htm>

From rousskov at measurement-factory.com  Tue Nov  7 16:48:36 2017
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Tue, 7 Nov 2017 09:48:36 -0700
Subject: [squid-users] ERR_ICAP_FAILURE unless squid reconfigure
In-Reply-To: <991998915.4009150.1510072136826@mail.yahoo.com>
References: <991998915.4009150.1510072136826.ref@mail.yahoo.com>
 <991998915.4009150.1510072136826@mail.yahoo.com>
Message-ID: <60227778-866a-b7aa-545c-5e25d3f1435c@measurement-factory.com>

On 11/07/2017 09:28 AM, Vieri wrote:

> whenever I restart c-icap, the Squid cache HTTP clients display the ERR_ICAP_FAILURE page.

Yes, when an essential ICAP service is unavailable, Squid will try
icap_service_failure_limit times and then will declare the service as
down, displaying the corresponding error to users.


> If I send the reconfigure action to the squid process then this "error" goes away.

FWIW, the error should also "go away" after icap_service_revival_delay
(provided the ICAP server is running again by that time).


> How can I avoid having to "recofnigure" squid when c-icap is restarted (if possible)?

You can increase icap_service_failure_limit or mark the ICAP service as
optional (i.e., use "bypass=on").


HTH,

Alex.


From rentorbuy at yahoo.com  Tue Nov  7 17:16:09 2017
From: rentorbuy at yahoo.com (Vieri)
Date: Tue, 7 Nov 2017 17:16:09 +0000 (UTC)
Subject: [squid-users] Squid and CPU 100%
In-Reply-To: <34c510cb-9ad4-ffab-7905-c0fd04ab6007@measurement-factory.com>
References: <348071111.3798275.1510054931204.ref@mail.yahoo.com>
 <348071111.3798275.1510054931204@mail.yahoo.com>
 <34c510cb-9ad4-ffab-7905-c0fd04ab6007@measurement-factory.com>
Message-ID: <1530147598.4027146.1510074969465@mail.yahoo.com>

A quick grep at access.log before the issue I reported shows that there were 1350 lines during a full minute. So I understand that would mean there were 1350 requests during that minute even though some of them were denied by squid.conf's policies. So I should estimate less than 2 * 1350. I would use that value anyway, and add 30% to that. I'd end up with 3510.

I grepped access.log for requests during other time ranges around the time of the reported issue, and I've come up with more or less the same values.

So, correct me if I'm wrong, but raising max_filedescriptors (currently at 32768) won't solve the root cause of the problem.

You mention that maybe another unknown process may suddenly consume almost all CPU cycles, drastically slowing Squid down, and quickly consuming all file descriptors.
That could be the case since I'm using c-icap + clamav. The clamd process reaches 100% peaks at times (for a very short while), but I would have to prove that this can be the cause. Wouldn't I need to monitor CPU usage by all processes at all times?

Also, wouldn't it be useful to check "squidclient mgr:filedescriptors" every 10 minutes or so? I have the feeling it's steadily growing over time, even when overall CPU usage is low. So the second less likely theory may also be a candidate.

Under which circumstances would "squidclient mgr:filedescriptors" show ever growing numbers even on very low CPU and network usage (squid seems to be very responsive)?

Vieri


From rousskov at measurement-factory.com  Tue Nov  7 17:37:54 2017
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Tue, 7 Nov 2017 10:37:54 -0700
Subject: [squid-users] Squid and CPU 100%
In-Reply-To: <1530147598.4027146.1510074969465@mail.yahoo.com>
References: <348071111.3798275.1510054931204.ref@mail.yahoo.com>
 <348071111.3798275.1510054931204@mail.yahoo.com>
 <34c510cb-9ad4-ffab-7905-c0fd04ab6007@measurement-factory.com>
 <1530147598.4027146.1510074969465@mail.yahoo.com>
Message-ID: <688ef975-60b5-f0e8-6acb-76b624bb2853@measurement-factory.com>

On 11/07/2017 10:16 AM, Vieri wrote:

> A quick grep at access.log before the issue I reported shows that
> there were 1350 lines during a full minute. So I understand that
> would mean there were 1350 requests during that minute even though
> some of them were denied by squid.conf's policies. So I should
> estimate less than 2 * 1350. I would use that value anyway, and add
> 30% to that. I'd end up with 3510.

You are talking about request rate. To get the number of connections,
you need to account for response time:

    number of concurrent connections = request rate * response time

And then add 30% overhead for idle persistent connections. That 30%
number is just a rough guess of mine. You can possibly do (much) better
if you study your actual Squid stats.


> So, correct me if I'm wrong, but raising max_filedescriptors
> (currently at 32768) won't solve the root cause of the problem.

Even though you have not computed the right kind of value, I do not
expect you to reach a different conclusion when you fix your formula
(unless your response times are huge). With 1350 requests/minute, your
response times would have to be in the order of minutes to get to 32K
open connections!

Please note that mgr:info and other cheap cache manager pages report
request rates (or equivalent). You do not need to count access log lines
to access and monitor that data.


> You mention that maybe another unknown process may suddenly consume
> almost all CPU cycles,

I did not. I was talking about Squid problems, not another process.


> Wouldn't I need to monitor CPU usage by all processes at all times?

This is not my area of expertise, but, ideally, one should monitor
server usage (using appropriate system administration/monitoring tools).


> Also, wouldn't it be useful to check "squidclient
> mgr:filedescriptors" every 10 minutes or so? I have the feeling it's
> steadily growing over time, even when overall CPU usage is low. So
> the second less likely theory may also be a candidate.

To monitor the number of file descriptors, use mgr:info. It is a much
cheaper action than mgr:filedescriptors, which has to list every
descriptor in every worker. Using mgr:filedescriptors is a good idea in
many cases, but using it every minute may have a noticeable negative
performance impact on concurrent user transactions.


> Under which circumstances would "squidclient mgr:filedescriptors"
> show ever growing numbers even on very low CPU and network usage
> (squid seems to be very responsive)?

A lot of factors, including idle connection pool warmup, bots, and bugs,
may lead to a steady increase in the number of idle and/or stuck
connections. Each idle or stuck connection does not consume much CPU or
network resources...


Alex.


From Antony.Stone at squid.open.source.it  Tue Nov  7 19:06:44 2017
From: Antony.Stone at squid.open.source.it (Antony Stone)
Date: Tue, 7 Nov 2017 19:06:44 +0000
Subject: [squid-users] problem squid squidguard with outlook 2016
In-Reply-To: <1510065276421-0.post@n4.nabble.com>
References: <1510065276421-0.post@n4.nabble.com>
Message-ID: <201711071906.44530.Antony.Stone@squid.open.source.it>

On Tuesday 07 November 2017 at 14:34:36, rmohammed wrote:

> I have a problem with outlook 2016 in my office,
> 
> When i activate squid and squidguard,

Is the behaviour the same if you use Squid without Squidguard?

> outlook stop to receive and send emails,
> 
> can anyone help me plz?

Tell us what you see in the access log when people attempt this.

Also, if there is any error message reported by Outlook 2016, please tell us 
what it says.


Antony.

-- 
It is also possible that putting the birds in a laboratory setting 
inadvertently renders them relatively incompetent.

 - Daniel C Dennett

                                                   Please reply to the list;
                                                         please *don't* CC me.


From nilesh.gavali at tcs.com  Tue Nov  7 19:33:52 2017
From: nilesh.gavali at tcs.com (Nilesh Gavali)
Date: Tue, 7 Nov 2017 19:33:52 +0000
Subject: [squid-users] squid-users Digest, Vol 39, Issue 10
In-Reply-To: <mailman.3.1510056001.19555.squid-users@lists.squid-cache.org>
References: <mailman.3.1510056001.19555.squid-users@lists.squid-cache.org>
Message-ID: <OF53FEDC32.BBF4751A-ON802581D1.006B20BD-802581D1.006B77B1@tcs.com>

Hi Antony,
I found where was the issue,
Actually access given to the site was based on LDAP group membership when 
user try to launch recording from site, the .wax file will be played from 
local machine IP , which is getting blocked.
created ACL to allow specific ip to access the Site and it worked.
Thank for point to direction to review log file

Thanks & Regards
Nilesh Suresh Gavali




From:   squid-users-request at lists.squid-cache.org
To:     squid-users at lists.squid-cache.org
Date:   07/11/2017 12:47
Subject:        squid-users Digest, Vol 39, Issue 10
Sent by:        "squid-users" <squid-users-bounces at lists.squid-cache.org>



Send squid-users mailing list submissions to
                 squid-users at lists.squid-cache.org

To subscribe or unsubscribe via the World Wide Web, visit
                 http://lists.squid-cache.org/listinfo/squid-users
or, via email, send a message with subject or body 'help' to
                 squid-users-request at lists.squid-cache.org

You can reach the person managing the list at
                 squid-users-owner at lists.squid-cache.org

When replying, please edit your Subject line so it is more specific
than "Re: Contents of squid-users digest..."


Today's Topics:

   1. window media player can not play .wax file via Squid proxy
      (Nilesh Gavali)
   2. Re: window media player can not play .wax file via Squid
      proxy (Antony Stone)
   3. Squid and CPU 100% (Vieri)


----------------------------------------------------------------------

Message: 1
Date: Mon, 6 Nov 2017 12:22:11 +0000
From: Nilesh Gavali <nilesh.gavali at tcs.com>
To: squid-users at lists.squid-cache.org
Subject: [squid-users] window media player can not play .wax file via
                 Squid           proxy
Message-ID:
 <OF06172801.D41F2471-ON802581D0.00435D9F-802581D0.0043F244 at tcs.com>
Content-Type: text/plain; charset="utf-8"

Hi Team.
We are having squid deployed for users which work well for browsing 
internet. Recently we received complaint from some users who access one of 

the vendor URL to listen audio recordings.
The procedure they follow as
login to Vendor portal URL via squid proxy
select recording option
select recording , it will get downloaded as .wax file
user open .wax file in media player after which they get error message as 
unable to play media.

I checked the .wax file which contains link to media file on their same 
portal URL. but window media player cant access it.

Not sure what is wrong here. I traced squid access logs and able to see 
the request going to vendor portal but still window media player cant play 

this file

Need urgent help if anyone work on this kind of requirement.


Thanks & Regards
Nilesh Suresh Gavali
=====-----=====-----=====
Notice: The information contained in this e-mail
message and/or attachments to it may contain 
confidential or privileged information. If you are 
not the intended recipient, any dissemination, use, 
review, distribution, printing or copying of the 
information contained in this e-mail message 
and/or attachments to it are strictly prohibited. If 
you have received this communication in error, 
please notify us by reply e-mail or telephone and 
immediately and permanently delete the message 
and any attachments. Thank you


-------------- next part --------------
An HTML attachment was scrubbed...
URL: <
http://lists.squid-cache.org/pipermail/squid-users/attachments/20171106/44c0b16c/attachment-0001.html
>

------------------------------

Message: 2
Date: Mon, 6 Nov 2017 12:36:07 +0000
From: Antony Stone <Antony.Stone at squid.open.source.it>
To: squid-users at lists.squid-cache.org
Subject: Re: [squid-users] window media player can not play .wax file
                 via             Squid proxy
Message-ID: <201711061236.07563.Antony.Stone at squid.open.source.it>
Content-Type: Text/Plain;  charset="iso-8859-15"

On Monday 06 November 2017 at 12:22:11, Nilesh Gavali wrote:

> Hi Team.
> We are having squid deployed for users which work well for browsing
> internet. Recently we received complaint from some users who access one 
of
> the vendor URL to listen audio recordings.
> The procedure they follow as
> login to Vendor portal URL via squid proxy
> select recording option
> select recording , it will get downloaded as .wax file
> user open .wax file in media player after which they get error message 
as
> unable to play media.
> 
> I checked the .wax file which contains link to media file on their same
> portal URL. but window media player cant access it.

What type of URL is it?  HTTP?  HTTPS?  Something else?

> Not sure what is wrong here. I traced squid access logs and able to see
> the request going to vendor portal

Which request?

The browser request for the .wax file, or the media player request for the 

actual media file?

> but still window media player cant play this file

It basically sounds as though either:

 -  Windows media player is trying to access a URL directly without going 
through the proxy, or

 - Windows media player is requesting a URL type which Squid can't handle

Tell us what URLs you're seeing in the access log, and what URL is 
contained 
in the .wax file - that might give us more of a clue.


Antony.

-- 
Douglas was one of those writers who honourably failed to get anywhere 
with 
'weekending'.  It put a premium on people who could write things that 
lasted 
thirty seconds, and Douglas was incapable of writing a single sentence 
that 
lasted less than thirty seconds.

 - Geoffrey Perkins, about Douglas Adams

                                                   Please reply to the 
list;
                                                         please *don't* CC 
me.


------------------------------

Message: 3
Date: Tue, 7 Nov 2017 11:42:11 +0000 (UTC)
From: Vieri <rentorbuy at yahoo.com>
To: "squid-users at lists.squid-cache.org"
                 <squid-users at lists.squid-cache.org>
Subject: [squid-users] Squid and CPU 100%
Message-ID: <348071111.3798275.1510054931204 at mail.yahoo.com>
Content-Type: text/plain; charset=UTF-8

Sorry to bring this back up, but every now and then (few days) I need to 
restart squid because its CPU usage goes up to 100% for a long time 
(irresponsive).

Right before restarting Squid:

# free [11/07/17 11:18:52]
total used free shared buff/cache available
Mem: 32865056 14811320 1374212 157028 16679524 17557656
Swap: 37036988 287320 36749668

# squidclient mgr:info [11/07/17 11:18:52]

# squidclient mgr:mem [11/07/17 11:19:02]

# squidclient mgr:storedir [11/07/17 11:19:22]

# squidclient mgr:filedescriptors [11/07/17 11:19:42]

# squidclient mgr:events [11/07/17 11:20:12]

In other words, squidclient does not get any info from squid (waited max 
30 seconds - maybe I need to wait more).

I also dumped an strace here:
https://drive.google.com/file/d/1yed-sNWrc6KBQSOV7KPbrYOForwwtkCe/view?usp=sharing


You probably won't be able to draw any conclusions so I'll try to update 
to squid-3.5.27-20171101-re69e56c (I'm now running 
3.5.27-20170916-re69e56c).

Note that stopping Squid with:

/usr/sbin/squid -k shutdown -f /etc/squid/squid.conf -n squid

takes a LONG time, but it eventually does stop (the process dies out).

I can then restart it cleanly.

I don't need to kill the pid (unless I'm told to hurry up).

After checking the log I can see these messages:

2017/11/07 11:16:59 kid1| WARNING! Your cache is running out of 
filedescriptors
2017/11/07 11:16:59 kid1| comm_open: socket failure: (24) Too many open 
files

Now, you can notice that the max_filedescriptors is quite high, and was 
set correctly before and after the issue I'm reporting:

# tail -n 100000 /var/log/squid/cache.log | grep -i descript
2017/11/06 06:36:14 kid1| With 32768 file descriptors available
2017/11/07 11:15:15 kid1| WARNING! Your cache is running out of 
filedescriptors
2017/11/07 11:15:31 kid1| WARNING! Your cache is running out of 
filedescriptors
2017/11/07 11:16:13 kid1| WARNING! Your cache is running out of 
filedescriptors
2017/11/07 11:16:59 kid1| WARNING! Your cache is running out of 
filedescriptors
2017/11/07 11:17:35 kid1| WARNING! Your cache is running out of 
filedescriptors
2017/11/07 11:17:51 kid1| WARNING! Your cache is running out of 
filedescriptors
2017/11/07 11:18:07 kid1| WARNING! Your cache is running out of 
filedescriptors
2017/11/07 11:18:25 kid1| WARNING! Your cache is running out of 
filedescriptors
2017/11/07 11:18:45 kid1| WARNING! Your cache is running out of 
filedescriptors
2017/11/07 11:19:03 kid1| WARNING! Your cache is running out of 
filedescriptors
2017/11/07 11:19:19 kid1| WARNING! Your cache is running out of 
filedescriptors
2017/11/07 11:19:35 kid1| WARNING! Your cache is running out of 
filedescriptors
2017/11/07 11:19:53 kid1| WARNING! Your cache is running out of 
filedescriptors
2017/11/07 11:20:09 kid1| WARNING! Your cache is running out of 
filedescriptors
2017/11/07 11:20:28 kid1| WARNING! Your cache is running out of 
filedescriptors
2017/11/07 11:20:44 kid1| WARNING! Your cache is running out of 
filedescriptors
2017/11/07 11:21:01 kid1| WARNING! Your cache is running out of 
filedescriptors
2017/11/07 11:23:05 kid1| With 32768 file descriptors available

So I'm worried that 32768 may not be enough.
Is this weird, or should I really increase this value?

Thanks,

Vieri


------------------------------

Subject: Digest Footer

_______________________________________________
squid-users mailing list
squid-users at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-users


------------------------------

End of squid-users Digest, Vol 39, Issue 10
*******************************************

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20171107/e5c6ebe0/attachment.htm>

From jeffmerkey at gmail.com  Tue Nov  7 23:02:14 2017
From: jeffmerkey at gmail.com (Jeffrey Merkey)
Date: Tue, 7 Nov 2017 16:02:14 -0700
Subject: [squid-users] Google Chrome reports "Too many redirects" on
 ssl-dumped connections with LA Times News Website
In-Reply-To: <CA+ekxPV6Sg-9O8ZriS+HP8riAiTdyj4tfMxXvSfUpSgyTAC5fw@mail.gmail.com>
References: <CA+ekxPVtc9b2nDNvAsmT=ebEy6PRZbB91A-JVDoeeuxfGz7wOQ@mail.gmail.com>
 <7f7335a6-d9a4-2415-fbd1-565424de7002@treenet.co.nz>
 <CA+ekxPV6Sg-9O8ZriS+HP8riAiTdyj4tfMxXvSfUpSgyTAC5fw@mail.gmail.com>
Message-ID: <CA+ekxPXZWOiEJr6jmhyd07+Twsx6OwCBeZeUOPXEpAGN76HbWw@mail.gmail.com>

I have done extensive testing and have been able to recreate this
error reliably on both Chrome and Firefox with or without squid loaded
or installed.    I have determined that it is not a bug in Squid, and
it also does not appear to be a bug in the browser but some sort of
problem with websites involved.

Most notable I have seen it not only on the LA Times Website but also
the Social Security website as well among others.   It appears to be a
side effect of some sort of spyware or adware on the websites
involved.

Since it can be recreated with or without squid and it occurs with
more than one browser it is clearly not a bug in Squid, c-icap, or the
browsers.

Jeff


From ash.benz at bk.ru  Tue Nov  7 23:18:13 2017
From: ash.benz at bk.ru (A. Benz)
Date: Wed, 8 Nov 2017 07:18:13 +0800
Subject: [squid-users] Non intrusive sslbump for whitelisting (asked many
	times but..)
Message-ID: <ae8fe086-e227-6b58-c6d8-f4e37d062eee@bk.ru>

Hi all,


## Intro

I read many blogs and emails on this list related to what I'm trying to 
do, but most go into bumping or do things that are not as simple as I'm 
trying to achieve.

I have an extremely slow line, with very high latency in a remote 
location. About 14 people are sharing this line. Nowadays with all the 
mobile apps trying to sync and such, the line stalls to unusable all the 
time.

I tried doing filters with firewall or dns level, but those are not 
effective. In the end I figured squid might be my best option.

## End intro


I have squid 3.5.27 running under LEDE (OpenWrt fork), ie its 
cross-compiled for a MIPS based SoC (mediatek mt7621). I mention this 
because you will see some options in the config file that won't make 
sense otherwise.

It works great, here's what I'm trying to achieve: Allow access only to 
a pre-defined list of websites (whitelist). http is straightforward, but 
if the connection is https all I need to know is domain, if its allowed, 
let it pass, otherwise terminate.

this setup is working as intended with the config attached below, 
however the issue I'm facing is that some servers are "loadbalanced", 
this would give me the forgery error, eg:

"SECURITY ALERT: Host header forgery detected on...."

Here's a specific example, there's a corporate domain for webmail 
access, and some loadbalance config makes use of different IPs, I think 
this is what triggers the error. My question is, can I just ignore this 
error somehow and allow the connection? From what I gather this 
connection is cut by squid before it reaches the client..

Also if there's anything else obviously wrong with my setup please let 
me know.

Many thanks.


Here's my config:


### squid.conf begin

acl localnet src 10.0.0.0/8
acl localnet src 172.16.0.0/12
acl localnet src 192.168.0.0/16

acl ssl_ports port 443

acl safe_ports port 80
acl safe_ports port 443
acl connect method connect
acl http_whitelist dstdomain "/etc/squid/whitelist.txt"
acl https_whitelist ssl::server_name "/etc/squid/whitelist.txt"
acl ips_whitelist dst "/etc/squid/ips.txt"

http_port 3128 intercept
http_port 3129

http_access deny !safe_ports
http_access deny connect !ssl_ports
http_access allow ssl_ports
http_access allow http_whitelist
http_access allow ips_whitelist
http_access deny all

https_port 3130 intercept ssl-bump \
	cert=/etc/squid/myCA.pem \
	generate-host-certificates=off dynamic_cert_mem_cache_size=4MB

acl step1 at_step SslBump1
acl step2 at_step SslBump2
acl step3 at_step SslBump3

ssl_bump peek step1 all
ssl_bump splice https_whitelist
ssl_bump splice ips_whitelist
ssl_bump terminate all


refresh_pattern ^ftp: 1440 20% 10080
refresh_pattern ^gopher: 1440 0% 1440
refresh_pattern -i (/cgi-bin/|\?) 0 0% 0
refresh_pattern . 0 20% 4320

cache deny all
access_log none
cache_log /dev/null
cache_store_log stdio:/dev/null
logfile_rotate 0

logfile_daemon /dev/null
coredump_dir /tmp/squid
visible_hostname main_Firewall
pinger_enable off
mime_table /tmp/squid/mime.conf
sslcrtd_program /usr/lib/squid/ssl_crtd -s /tmp/squid/ssldb -M 4MB

## config file end.




## whitelist.txt begin

.nokia.com

## whitelist.txt end.
-- 

Regards,
A. Benz


From augustus_meyer at gmx.net  Wed Nov  8 04:07:30 2017
From: augustus_meyer at gmx.net (reinerotto)
Date: Tue, 7 Nov 2017 21:07:30 -0700 (MST)
Subject: [squid-users] Non intrusive sslbump for whitelisting (asked
	many times but..)
In-Reply-To: <ae8fe086-e227-6b58-c6d8-f4e37d062eee@bk.ru>
References: <ae8fe086-e227-6b58-c6d8-f4e37d062eee@bk.ru>
Message-ID: <1510114050401-0.post@n4.nabble.com>

>I tried doing filters with firewall or dns level, but those are not
effective.<
(dnsmasq + ipset) + iptables should do it. You most likely need
(dnsmasq+ipset) to allow traffic to multi-IP sites like google, facebook
etc. 
Will work on openwrt/LEDE, too. As I am using it.



--
Sent from: http://squid-web-proxy-cache.1019090.n4.nabble.com/Squid-Users-f1019091.html


From frio_cervesa at hotmail.com  Wed Nov  8 04:15:02 2017
From: frio_cervesa at hotmail.com (senor)
Date: Wed, 8 Nov 2017 04:15:02 +0000
Subject: [squid-users] ALPN, HTTP/2 and sslbump
Message-ID: <BY2PR17MB018222156B827DDF84E7C6FCF7560@BY2PR17MB0182.namprd17.prod.outlook.com>

I am surprised that I didn't find this question asked and answered 
recently. Maybe this issue is newer than I realize.

I understand that support of HTTPS/2 is in development but I'd like to 
better understand what is and is not currently supported. I discovered 
the other day that an intercepted client https connection, which 
included both h2 and http/1.1 in the ALPN extension, was tunneled when 
the server responded with only h2. I'm assuming that was due to squid 
not fully supporting HTTP/2.

My initial need is to prevent the tunnel. Preferably by forcing http/1.1 
and bumping but just denying the connection is second best. I'm not 
aware of any squid built-in mechanisms to manage ALPN or HTTP/2 so I'm 
thinking the external_acl is the only way to go. I think the client ALPN 
data is available at bump step 2 but what options do I have at that point?

Help or corrections to my assumptions are appreciated.

Senor


From squid3 at treenet.co.nz  Wed Nov  8 04:23:23 2017
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Wed, 8 Nov 2017 17:23:23 +1300
Subject: [squid-users] Non intrusive sslbump for whitelisting (asked
 many times but..)
In-Reply-To: <ae8fe086-e227-6b58-c6d8-f4e37d062eee@bk.ru>
References: <ae8fe086-e227-6b58-c6d8-f4e37d062eee@bk.ru>
Message-ID: <96e5ae3d-6dc6-fca3-7c70-b038ecf16ddf@treenet.co.nz>

On 08/11/17 12:18, A. Benz wrote:
> Hi all,
> 
> 
> ## Intro
> 
> I read many blogs and emails on this list related to what I'm trying to 
> do, but most go into bumping or do things that are not as simple as I'm 
> trying to achieve.
> 
> I have an extremely slow line, with very high latency in a remote 
> location. About 14 people are sharing this line. Nowadays with all the 
> mobile apps trying to sync and such, the line stalls to unusable all the 
> time.
> 
> I tried doing filters with firewall or dns level, but those are not 
> effective. In the end I figured squid might be my best option.
> 
> ## End intro
> 
> 
> I have squid 3.5.27 running under LEDE (OpenWrt fork), ie its 
> cross-compiled for a MIPS based SoC (mediatek mt7621). I mention this 
> because you will see some options in the config file that won't make 
> sense otherwise.
> 

NP: That should not be making much difference to the squid.conf 
settings. The worst limitations such devices impose are things that 
should be solved by OS settings outside of squid.conf. eg the cache.log 
going to a pipe for remote logging instead of a filename, and 
system-level FD limits.


> It works great, here's what I'm trying to achieve: Allow access only to 
> a pre-defined list of websites (whitelist). http is straightforward, but 
> if the connection is https all I need to know is domain, if its allowed, 
> let it pass, otherwise terminate.
> 
> this setup is working as intended with the config attached below, 
> however the issue I'm facing is that some servers are "loadbalanced", 
> this would give me the forgery error, eg:
> 
> "SECURITY ALERT: Host header forgery detected on...."
> 

The workarounds and gotcha's listed at 
<https://wiki.squid-cache.org/KnowledgeBase/HostHeaderForgery> are the 
best you can hope for there. The most successful all-round solution is 
to increase EDNS0 capabilities.


> Here's a specific example, there's a corporate domain for webmail 
> access, and some loadbalance config makes use of different IPs, I think 
> this is what triggers the error. My question is, can I just ignore this 
> error somehow and allow the connection? From what I gather this 
> connection is cut by squid before it reaches the client..

Squid default behaviour is to allow the connection only to the same 
IP:port the client was connecting to. If that is not working your 
network configuration is screwed up. Specifically your routing or NAT.

NAT of the dst-IP:port *MUST NOT* happen on any device between the 
client machine and the proxy machine. Squid needs access directly to the 
kernel NAT records of the device doing that NAT operation. So it can 
only happen on the Squid device.
  You must *route* the packets unchanged to the Squid device (possibly 
over a tunnel if necessary).


> 
> Also if there's anything else obviously wrong with my setup please let 
> me know.
> 
> Many thanks.
> 
> 
> Here's my config:
> 
> 
> ### squid.conf begin
> 
> acl localnet src 10.0.0.0/8
> acl localnet src 172.16.0.0/12
> acl localnet src 192.168.0.0/16
> 
> acl ssl_ports port 443
> 
> acl safe_ports port 80
> acl safe_ports port 443
> acl connect method connect

NP: the default above ACL names are case-sensitive and some of them 
involve built-in default values which you are preventing having any 
effect by using custom lower-case ACL names.


> acl http_whitelist dstdomain "/etc/squid/whitelist.txt"
> acl https_whitelist ssl::server_name "/etc/squid/whitelist.txt"
> acl ips_whitelist dst "/etc/squid/ips.txt"
> 
> http_port 3128 intercept
> http_port 3129

Port 3128 is registered for forward-proxy traffic. Ideally you would 
have those lines reversed like so:

  http_port 3128
  http_port 3129 intercept

... with the corresponding NAT change for the intercept port.

Also, to have your SSL-Bump whitelists applied to forward-proxy CONNECT 
traffic you should have ssl-bump settings on that 3128 forward-proxy 
port matching those on the port 3130 line.


> 
> http_access deny !safe_ports
> http_access deny connect !ssl_ports

> http_access allow ssl_ports

Rather than allowing unlimited access to anyone on the Internet to use 
your limited bandwidth outbound connection for access to port 443 you 
should be using the localnet ACL that restricts use of the proxy to 
people on your LAN - those 14 clients you mentioned sharing the line.

[NP: It is not possible in this setup to determine what remote users are 
abusing your proxy. All traffic logs from your firewall etc will show 
Squid as the client, not the remote [ab]user. Squid access.log records 
you are sending to /dev/null is the *only* record of such activities.]


To make your whitelists have any effect replace the above "allow 
ssl_ports" line with a "deny !localnet" line.

If that change causes issues then your whitelists are incorrect / 
incomplete. You then need the (currently discarded) access.log and/or 
cache.log data to solve the issue properly.


> http_access allow http_whitelist
> http_access allow ips_whitelist
> http_access deny all
> 
> https_port 3130 intercept ssl-bump \
>  ????cert=/etc/squid/myCA.pem \
>  ????generate-host-certificates=off dynamic_cert_mem_cache_size=4MB
> 
> acl step1 at_step SslBump1
> acl step2 at_step SslBump2
> acl step3 at_step SslBump3
> 
> ssl_bump peek step1 all
> ssl_bump splice https_whitelist
> ssl_bump splice ips_whitelist
> ssl_bump terminate all
> 

That seems fine. The problem is not part of this _config_. If you are 
having any SSL-Bump issues please try a build of the latest Squid-4. It 
may be related to bugs in Squid-3 SSL-Bump or modern TLS things Squid-3 
cannot cope with - there is a growing list of those.

> 
> cache deny all

In the latest Squid-3 use "store_miss deny all" instead of the above.

> access_log none

The above is fine if you are certain of the squid.conf working 100% 
properly. But since you are debugging issues you may need those 
transaction details.

NP: access.log can be logged to syslog or a TCP pipe by Squid. To 
deliver the log content externally for normal audit purposes instead of 
using space on the device.

> cache_log /dev/null

You *need* the information logged here. By default only the most 
operationally critical errors are recorded.

NP: the cache.log can usually be a Unix-pipe delivering data to a remote 
server if the local machine is constrained.

> cache_store_log stdio:/dev/null

Above line is *actively* harmful. The Squid-3 default is not to waste 
cycles logging *unless* you enter something like the above in 
squid.conf. The above makes Squid allocate device resources to logging 
that data to /dev/null.

> logfile_rotate 0
> 
> logfile_daemon /dev/null

/dev/null is not a valid application filename.

Build your Squid with --disable-logfile-daemon.

> coredump_dir /tmp/squid
> visible_hostname main_Firewall

The *visible* hostname is the domain delivered to clients and denied 
parties in the URLs to fetch error message data and FTP icons from 
Squid. It needs to be a valid FQDN.

Amos


From squid3 at treenet.co.nz  Wed Nov  8 04:29:12 2017
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Wed, 8 Nov 2017 17:29:12 +1300
Subject: [squid-users] ALPN, HTTP/2 and sslbump
In-Reply-To: <BY2PR17MB018222156B827DDF84E7C6FCF7560@BY2PR17MB0182.namprd17.prod.outlook.com>
References: <BY2PR17MB018222156B827DDF84E7C6FCF7560@BY2PR17MB0182.namprd17.prod.outlook.com>
Message-ID: <177b9497-8b06-2049-c74b-02b77d151fe5@treenet.co.nz>

On 08/11/17 17:15, senor wrote:
> I am surprised that I didn't find this question asked and answered
> recently. Maybe this issue is newer than I realize.
> 
> I understand that support of HTTPS/2 is in development but I'd like to
> better understand what is and is not currently supported. I discovered
> the other day that an intercepted client https connection, which
> included both h2 and http/1.1 in the ALPN extension, was tunneled when
> the server responded with only h2. I'm assuming that was due to squid
> not fully supporting HTTP/2.

Hmm. If you are using SSL-Bump to bump the traffic the current Squid 
should be delivering an ALPN containing only HTTP/1.1 to the server. 
Sending h2 in the ALPN is only valid if the proxy supports h2 natively 
or intends up front to splice the transaction back to "tunneled".


> 
> My initial need is to prevent the tunnel. Preferably by forcing http/1.1
> and bumping but just denying the connection is second best. I'm not
> aware of any squid built-in mechanisms to manage ALPN or HTTP/2 so I'm
> thinking the external_acl is the only way to go. I think the client ALPN
> data is available at bump step 2 but what options do I have at that point?
> 
> Help or corrections to my assumptions are appreciated.
> 

Any info about your Squid version, and squid.conf contents - especially 
http_access and SSL-Bump related things would be useful. Random guesses 
about complex things like TLS are harmful to solving actual problems.

Amos


From squid3 at treenet.co.nz  Wed Nov  8 04:32:33 2017
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Wed, 8 Nov 2017 17:32:33 +1300
Subject: [squid-users] 4.0.21 Ssl bump access denied
In-Reply-To: <CADYcWGStxuT4aHpG3P13h-A9jnZ-PXWOzbZmV8Bkz6BeH+7uCQ@mail.gmail.com>
References: <CADYcWGSbu1AMvS6QCPBq8b6urLb1Ty7Cs2NgujpSex4Fp_wHTg@mail.gmail.com>
 <CADYcWGS_hiYUN+DeA-pd2DeLseDF_53daBnkRBTArWqZXLQEVw@mail.gmail.com>
 <CADYcWGStxuT4aHpG3P13h-A9jnZ-PXWOzbZmV8Bkz6BeH+7uCQ@mail.gmail.com>
Message-ID: <f159b55a-512d-0cbd-ab6a-ca53ceb8c9ad@treenet.co.nz>

On 08/11/17 04:52, snable snable wrote:
> Hello
> 
> i forward from.my openwrt router the traffic for 443 and 80 to my squid 
> box to port 3129 and 3128
> 

What do you mean by "forward" ?

Any dst-IP:port NAT operation *MUST* only happen on the Squid device 
itself or _later_ down the traffic path. Traffic must be *routed* to 
that Squid device.


Amos


From rentorbuy at yahoo.com  Wed Nov  8 09:21:22 2017
From: rentorbuy at yahoo.com (Vieri)
Date: Wed, 8 Nov 2017 09:21:22 +0000 (UTC)
Subject: [squid-users] url_rewrite_program and ACLs
References: <609790878.4516177.1510132882849.ref@mail.yahoo.com>
Message-ID: <609790878.4516177.1510132882849@mail.yahoo.com>

Hi,

I'm not sure I understand how url_rewrite_program works.

In the example below I'm trying to allow traffic from CLIENT_IP_ADDR to SERVER_DOMAIN_ADDR where CLIENT_IP_ADDR is in the allowed_ips ACL, and SERVER_DOMAIN_ADDR is in the allowed_domains ACL (I know it's redundant, but it's just an example).

http_access allow localnet !restricted_ips allowed_domains
http_access allow localnet !restricted_ips allowed_ips
http_reply_access allow localnet !restricted_ips allowed_ips
http_reply_access allow localnet !restricted_ips allowed_domains

[...]

url_rewrite_program /usr/bin/squidGuard
url_rewrite_children 80 startup=10 idle=3

http_access allow localnet

The problem is that the browser on the CLIENT_IP_ADDR displays the "redirect" page defined in squidGuard when trying to access SERVER_DOMAIN_ADDR.

I could configure the allowed_domains and allowed_ips ACLs within squidGuard itself, but shouldn't the Squid rule prevail anyway?
Is the redirection done regardless of what precedes in squid.conf?

Thanks,

Vieri


From dernikov1 at gmail.com  Wed Nov  8 09:26:11 2017
From: dernikov1 at gmail.com (Bike dernikov1)
Date: Wed, 8 Nov 2017 10:26:11 +0100
Subject: [squid-users] SQUID memory error after vm.swappines changed from 60
	to 10
Message-ID: <CAAPixKhsif5+ad1prsoFAZXVXcVAxpdqkw4RdvGbfK-UxhkzTQ@mail.gmail.com>

Hi, I hope that someone can explain what happened, why squid stopped working.
The problem is related to  memory/swap handling.

After we changed vm.swappiness parameter from 60 to 10 (tuning
attempt, to lower a disk usage, because we have only 4 disks in a
RAID10, so disk subsystem  is a weak link), we got a lot of errors in
cache.log.
The problems started after scheduled logrotate after  2AM.
Squid ran out of memory, auth helpers stopped working.
It's weird because we didn't disable swap, but behavior is like we did.
After an error, we increased parameter from 10 to 40.

The server has 24GB DDR3 memory,  disk swap set to 24GB, 12 CPU (24HT cores).
We have 2800 users, using  kerberos authentication, squidguard for
filtering, ldap authorization.
When problem appeared memory was still 3GB free (free column), ram
(caching) was filled to 15GB, so 21 GB ram filled, 3GB free.

Thanks for help,


errors from cache.log.

2017/11/08 02:55:27| Set Current Directory to /var/log/squid/
2017/11/08 02:55:27 kid1| storeDirWriteCleanLogs: Starting...
2017/11/08 02:55:27 kid1|   Finished.  Wrote 0 entries.
2017/11/08 02:55:27 kid1|   Took 0.00 seconds (  0.00 entries/sec).
2017/11/08 02:55:27 kid1| logfileRotate: daemon:/var/log/squid/access.log
2017/11/08 02:55:27 kid1| logfileRotate: daemon:/var/log/squid/access.log
2017/11/08 02:55:28 kid1| Pinger socket opened on FD 30
2017/11/08 02:55:28 kid1| helperOpenServers: Starting 1/1000
'squidGuard' processes
2017/11/08 02:55:28 kid1| ipcCreate: fork: (12) Cannot allocate memory
2017/11/08 02:55:28 kid1| WARNING: Cannot run '/usr/bin/squidGuard' process.
2017/11/08 02:55:28 kid1| helperOpenServers: Starting 300/3000
'negotiate_kerberos_auth' processes
2017/11/08 02:55:28 kid1| ipcCreate: fork: (12) Cannot allocate memory
2017/11/08 02:55:28 kid1| WARNING: Cannot run
'/usr/lib/squid/negotiate_kerberos_auth' process.
2017/11/08 02:55:28 kid1| ipcCreate: fork: (12) Cannot allocate memory
2017/11/08 02:55:28 kid1| WARNING: Cannot run
'/usr/lib/squid/negotiate_kerberos_auth' process.
2017/11/08 02:55:28 kid1| ipcCreate: fork: (12) Cannot allocate memory
2017/11/08 02:55:28 kid1| WARNING: Cannot run
'/usr/lib/squid/negotiate_kerberos_auth' process.

external ACL 'memberof' queue overload. Using stale result.


From marcus.kool at urlfilterdb.com  Wed Nov  8 09:53:53 2017
From: marcus.kool at urlfilterdb.com (Marcus Kool)
Date: Wed, 8 Nov 2017 07:53:53 -0200
Subject: [squid-users] SQUID memory error after vm.swappines changed
 from 60 to 10
In-Reply-To: <CAAPixKhsif5+ad1prsoFAZXVXcVAxpdqkw4RdvGbfK-UxhkzTQ@mail.gmail.com>
References: <CAAPixKhsif5+ad1prsoFAZXVXcVAxpdqkw4RdvGbfK-UxhkzTQ@mail.gmail.com>
Message-ID: <dab0b62b-57a5-5718-b0bd-70fb3de0c124@urlfilterdb.com>

There is definitely a problem with available memory because Squid cannot fork.
So start with looking at how much memory Squid and its helpers use.
Do do have other processes on this system that consume a lot of memory ?

Also note that ufdbGuard uses less memory that squidGuard.
If there are 30 helpers squidguard uses 300% more memory than ufdbGuard.

Look at the wiki for more information about memory usage:
https://wiki.squid-cache.org/SquidFaq/SquidMemory   (currently has an expired certificate but it is safe to go ahead)

Marcus


On 08/11/17 07:26, Bike dernikov1 wrote:
> Hi, I hope that someone can explain what happened, why squid stopped working.
> The problem is related to  memory/swap handling.
> 
> After we changed vm.swappiness parameter from 60 to 10 (tuning
> attempt, to lower a disk usage, because we have only 4 disks in a
> RAID10, so disk subsystem  is a weak link), we got a lot of errors in
> cache.log.
> The problems started after scheduled logrotate after  2AM.
> Squid ran out of memory, auth helpers stopped working.
> It's weird because we didn't disable swap, but behavior is like we did.
> After an error, we increased parameter from 10 to 40.
> 
> The server has 24GB DDR3 memory,  disk swap set to 24GB, 12 CPU (24HT cores).
> We have 2800 users, using  kerberos authentication, squidguard for
> filtering, ldap authorization.
> When problem appeared memory was still 3GB free (free column), ram
> (caching) was filled to 15GB, so 21 GB ram filled, 3GB free.
> 
> Thanks for help,
> 
> 
> errors from cache.log.
> 
> 2017/11/08 02:55:27| Set Current Directory to /var/log/squid/
> 2017/11/08 02:55:27 kid1| storeDirWriteCleanLogs: Starting...
> 2017/11/08 02:55:27 kid1|   Finished.  Wrote 0 entries.
> 2017/11/08 02:55:27 kid1|   Took 0.00 seconds (  0.00 entries/sec).
> 2017/11/08 02:55:27 kid1| logfileRotate: daemon:/var/log/squid/access.log
> 2017/11/08 02:55:27 kid1| logfileRotate: daemon:/var/log/squid/access.log
> 2017/11/08 02:55:28 kid1| Pinger socket opened on FD 30
> 2017/11/08 02:55:28 kid1| helperOpenServers: Starting 1/1000
> 'squidGuard' processes
> 2017/11/08 02:55:28 kid1| ipcCreate: fork: (12) Cannot allocate memory
> 2017/11/08 02:55:28 kid1| WARNING: Cannot run '/usr/bin/squidGuard' process.
> 2017/11/08 02:55:28 kid1| helperOpenServers: Starting 300/3000
> 'negotiate_kerberos_auth' processes
> 2017/11/08 02:55:28 kid1| ipcCreate: fork: (12) Cannot allocate memory
> 2017/11/08 02:55:28 kid1| WARNING: Cannot run
> '/usr/lib/squid/negotiate_kerberos_auth' process.
> 2017/11/08 02:55:28 kid1| ipcCreate: fork: (12) Cannot allocate memory
> 2017/11/08 02:55:28 kid1| WARNING: Cannot run
> '/usr/lib/squid/negotiate_kerberos_auth' process.
> 2017/11/08 02:55:28 kid1| ipcCreate: fork: (12) Cannot allocate memory
> 2017/11/08 02:55:28 kid1| WARNING: Cannot run
> '/usr/lib/squid/negotiate_kerberos_auth' process.
> 
> external ACL 'memberof' queue overload. Using stale result.
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
> 


From squid3 at treenet.co.nz  Wed Nov  8 10:43:44 2017
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Wed, 8 Nov 2017 23:43:44 +1300
Subject: [squid-users] url_rewrite_program and ACLs
In-Reply-To: <609790878.4516177.1510132882849@mail.yahoo.com>
References: <609790878.4516177.1510132882849.ref@mail.yahoo.com>
 <609790878.4516177.1510132882849@mail.yahoo.com>
Message-ID: <a2a261d8-a1bf-d811-6a50-928957fb9d93@treenet.co.nz>

On 08/11/17 22:21, Vieri wrote:
> Hi,
> 
> I'm not sure I understand how url_rewrite_program works.
> 

Squid takes the URI from an HTTP request it is servicing and delivers it 
to the helper. The helper delivers a new URI back to Squid (or not). 
Squid then generates an entirely new HTTP request to use for server 
contact instead of the client one. Squid delivers the server response to 
that other request as the answer to the clients original request.

At no point is Squid doing any denial based on the URL-rewrite program.


> In the example below I'm trying to allow traffic from CLIENT_IP_ADDR to SERVER_DOMAIN_ADDR where CLIENT_IP_ADDR is in the allowed_ips ACL, and SERVER_DOMAIN_ADDR is in the allowed_domains ACL (I know it's redundant, but it's just an example).
> 
> http_access allow localnet !restricted_ips allowed_domains
> http_access allow localnet !restricted_ips allowed_ips
> http_reply_access allow localnet !restricted_ips allowed_ips
> http_reply_access allow localnet !restricted_ips allowed_domains
> 
> [...]
> 
> url_rewrite_program /usr/bin/squidGuard
> url_rewrite_children 80 startup=10 idle=3
> 
> http_access allow localnet


The http_access lines are checked to determine whether a client request 
is allowed to be serviced. So the transaction can be allowed or denied here.

If allowed to process, eventually the transaction checks the 
url_rewrite_access lines to determine whether the helper is involved 
with adapting the transaction. Their default is "allow".

If the helper is used, the URL replacement may (or not) be given by SG 
at that point. Its decisions are determined by its own config, not Squid.

The http_reply_access determines whether the server response is allowed 
to be delivered. In this case the response is coming from your SG's 
localhost server.

> 
> The problem is that the browser on the CLIENT_IP_ADDR displays the "redirect" page defined in squidGuard when trying to access SERVER_DOMAIN_ADDR.
>  > I could configure the allowed_domains and allowed_ips ACLs within 
squidGuard itself, but shouldn't the Squid rule prevail anyway?
> Is the redirection done regardless of what precedes in squid.conf?

The directive purposes matter. squid.conf url_rewrite_access determines 
whether the helper is used. If those directives 'deny' the helper being 
used, OR if the helper returns a no-change result (empty line or ERR) 
there is no rewrite done.

PS. You do not need SG at all for access control and it is a lot simpler 
and faster to drop the helper part of things entirely. Especially in 
cases like this example.

Amos


From rentorbuy at yahoo.com  Wed Nov  8 13:59:47 2017
From: rentorbuy at yahoo.com (Vieri)
Date: Wed, 8 Nov 2017 13:59:47 +0000 (UTC)
Subject: [squid-users] ERR_ICAP_FAILURE unless squid reconfigure
In-Reply-To: <60227778-866a-b7aa-545c-5e25d3f1435c@measurement-factory.com>
References: <991998915.4009150.1510072136826.ref@mail.yahoo.com>
 <991998915.4009150.1510072136826@mail.yahoo.com>
 <60227778-866a-b7aa-545c-5e25d3f1435c@measurement-factory.com>
Message-ID: <534751279.4656537.1510149587602@mail.yahoo.com>

I set icap_service_failure_limit -1.

It seems to work OK now in my case.

Thanks!


From rentorbuy at yahoo.com  Wed Nov  8 14:12:16 2017
From: rentorbuy at yahoo.com (Vieri)
Date: Wed, 8 Nov 2017 14:12:16 +0000 (UTC)
Subject: [squid-users] url_rewrite_program and ACLs
References: <1990137371.4646637.1510150336187.ref@mail.yahoo.com>
Message-ID: <1990137371.4646637.1510150336187@mail.yahoo.com>

Thanks. I defined the following, and it worked as expected:

url_rewrite_access deny allowed_domains
url_rewrite_access deny allowed_ips
url_rewrite_program /usr/bin/squidGuard
url_rewrite_children 80 startup=10 idle=3


How can I rewrite a URL in squid without a helper such as SG?
ie. how can emulate SG's "rew" in squid.conf?

Thanks,

Vieri


From rentorbuy at yahoo.com  Wed Nov  8 14:23:25 2017
From: rentorbuy at yahoo.com (Vieri)
Date: Wed, 8 Nov 2017 14:23:25 +0000 (UTC)
Subject: [squid-users] squid and squidGuard redirect
References: <816059292.4665098.1510151005624.ref@mail.yahoo.com>
Message-ID: <816059292.4665098.1510151005624@mail.yahoo.com>

Hi,

I have this in my SG config:

acl {
default {
pass allowed !disallowed all
redirect http://squidserver/proxy-error/
}
}

>From a LAN client browser I can access and display the page at http://squidserver/proxy-error/ (direct access).

However, when SG is triggered and should send that redirect to the client browser, the client times out after a while, and displays Squid's ERR_CONNECT_FAIL with squidserver's IP address in the details.

I don't see anything useful in both Squid and SquidGuard's logs.

What could I try?

Thanks,

Vieri


From marcus.kool at urlfilterdb.com  Wed Nov  8 14:48:52 2017
From: marcus.kool at urlfilterdb.com (Marcus Kool)
Date: Wed, 8 Nov 2017 12:48:52 -0200
Subject: [squid-users] SQUID memory error after vm.swappines changed
 from 60 to 10
In-Reply-To: <CAAPixKjepL2BEePSH83XeBNB+UPy4wKoqjTn0_Axoa+n0THo7w@mail.gmail.com>
References: <CAAPixKhsif5+ad1prsoFAZXVXcVAxpdqkw4RdvGbfK-UxhkzTQ@mail.gmail.com>
 <dab0b62b-57a5-5718-b0bd-70fb3de0c124@urlfilterdb.com>
 <CAAPixKjepL2BEePSH83XeBNB+UPy4wKoqjTn0_Axoa+n0THo7w@mail.gmail.com>
Message-ID: <daf2a3ca-d5bd-f84c-0182-099f845c4e61@urlfilterdb.com>


On 08/11/17 11:36, Bike dernikov1 wrote:
> Hi,
> 
> We stumbled on ufdbGuard, but licence/price was problem, we didn't
> read documentation carefully.
yes, ufdbguard is free.

> We will definitely try ufdbGuard, but we are now in process of moving
> squid/squidguard to production, so we can't test on production (angry
> users, Internet must work :)).
> 
> Memory compsumption:squid use largest part of memory  (12GB now,
> second proces use 300MB memory), 14GB used by all process. So squid
> use over 80% of total used memory.
> So no there are not any problematic process. But we changed swappiness
> settings.
Did you monitor Squid for growth (it can start with 12 GB and grow slowly) ?

Squid cannot fork and higher swappiness increases the amount of memory that the OS can use to copy processes.
It makes me think that you have the memory overcommit set to 2 (no overcommit).
What is the output of the following command ?
    sysctl  -a | grep overcommit

> Advice for some settings:
> We have absolute max peak of  2500 users which user squid (of 2800),
> what are recomended settings for:
> negotiate_kerberos_children start/idle
> squidguard helpers.

I have little experience with kerberos, but most likely this is not the issue.
When Squid cannot fork the helpers, helper settings do not matter much.

For 2500 users you probably need 32-64 squidguard helpers.

Marcus

> Thanks for help,
> 
> On Wed, Nov 8, 2017 at 10:53 AM, Marcus Kool
> <marcus.kool at urlfilterdb.com> wrote:
>> There is definitely a problem with available memory because Squid cannot
>> fork.
>> So start with looking at how much memory Squid and its helpers use.
>> Do do have other processes on this system that consume a lot of memory ?
>>
>> Also note that ufdbGuard uses less memory that squidGuard.
>> If there are 30 helpers squidguard uses 300% more memory than ufdbGuard.
>>
>> Look at the wiki for more information about memory usage:
>> https://wiki.squid-cache.org/SquidFaq/SquidMemory   (currently has an
>> expired certificate but it is safe to go ahead)
>>
>> Marcus
>>
>>
>>
>> On 08/11/17 07:26, Bike dernikov1 wrote:
>>>
>>> Hi, I hope that someone can explain what happened, why squid stopped
>>> working.
>>> The problem is related to  memory/swap handling.
>>>
>>> After we changed vm.swappiness parameter from 60 to 10 (tuning
>>> attempt, to lower a disk usage, because we have only 4 disks in a
>>> RAID10, so disk subsystem  is a weak link), we got a lot of errors in
>>> cache.log.
>>> The problems started after scheduled logrotate after  2AM.
>>> Squid ran out of memory, auth helpers stopped working.
>>> It's weird because we didn't disable swap, but behavior is like we did.
>>> After an error, we increased parameter from 10 to 40.
>>>
>>> The server has 24GB DDR3 memory,  disk swap set to 24GB, 12 CPU (24HT
>>> cores).
>>> We have 2800 users, using  kerberos authentication, squidguard for
>>> filtering, ldap authorization.
>>> When problem appeared memory was still 3GB free (free column), ram
>>> (caching) was filled to 15GB, so 21 GB ram filled, 3GB free.
>>>
>>> Thanks for help,
>>>
>>>
>>> errors from cache.log.
>>>
>>> 2017/11/08 02:55:27| Set Current Directory to /var/log/squid/
>>> 2017/11/08 02:55:27 kid1| storeDirWriteCleanLogs: Starting...
>>> 2017/11/08 02:55:27 kid1|   Finished.  Wrote 0 entries.
>>> 2017/11/08 02:55:27 kid1|   Took 0.00 seconds (  0.00 entries/sec).
>>> 2017/11/08 02:55:27 kid1| logfileRotate: daemon:/var/log/squid/access.log
>>> 2017/11/08 02:55:27 kid1| logfileRotate: daemon:/var/log/squid/access.log
>>> 2017/11/08 02:55:28 kid1| Pinger socket opened on FD 30
>>> 2017/11/08 02:55:28 kid1| helperOpenServers: Starting 1/1000
>>> 'squidGuard' processes
>>> 2017/11/08 02:55:28 kid1| ipcCreate: fork: (12) Cannot allocate memory
>>> 2017/11/08 02:55:28 kid1| WARNING: Cannot run '/usr/bin/squidGuard'
>>> process.
>>> 2017/11/08 02:55:28 kid1| helperOpenServers: Starting 300/3000
>>> 'negotiate_kerberos_auth' processes
>>> 2017/11/08 02:55:28 kid1| ipcCreate: fork: (12) Cannot allocate memory
>>> 2017/11/08 02:55:28 kid1| WARNING: Cannot run
>>> '/usr/lib/squid/negotiate_kerberos_auth' process.
>>> 2017/11/08 02:55:28 kid1| ipcCreate: fork: (12) Cannot allocate memory
>>> 2017/11/08 02:55:28 kid1| WARNING: Cannot run
>>> '/usr/lib/squid/negotiate_kerberos_auth' process.
>>> 2017/11/08 02:55:28 kid1| ipcCreate: fork: (12) Cannot allocate memory
>>> 2017/11/08 02:55:28 kid1| WARNING: Cannot run
>>> '/usr/lib/squid/negotiate_kerberos_auth' process.
>>>
>>> external ACL 'memberof' queue overload. Using stale result.
>>> _______________________________________________
>>> squid-users mailing list
>>> squid-users at lists.squid-cache.org
>>> http://lists.squid-cache.org/listinfo/squid-users
>>>
>> _______________________________________________
>> squid-users mailing list
>> squid-users at lists.squid-cache.org
>> http://lists.squid-cache.org/listinfo/squid-users
> 
> 


From marcus.kool at urlfilterdb.com  Wed Nov  8 14:53:51 2017
From: marcus.kool at urlfilterdb.com (Marcus Kool)
Date: Wed, 8 Nov 2017 12:53:51 -0200
Subject: [squid-users] squid and squidGuard redirect
In-Reply-To: <816059292.4665098.1510151005624@mail.yahoo.com>
References: <816059292.4665098.1510151005624.ref@mail.yahoo.com>
 <816059292.4665098.1510151005624@mail.yahoo.com>
Message-ID: <515b8071-a215-e413-789b-f11e17a166e3@urlfilterdb.com>

Hi Vieri,

I suggest to replace squidGuard with ufdbGuard.
Then you can set
    ufdb-debug-filter 1
or
    ufdb-debug-filter 2  # very verbose
in ufdbGuard.conf and see exactly what happens.

Note that squidguard has no maintenance for over 5 years and ufdbGuard has regular maintenance.

Marcus


On 08/11/17 12:23, Vieri wrote:
> Hi,
> 
> I have this in my SG config:
> 
> acl {
> default {
> pass allowed !disallowed all
> redirect http://squidserver/proxy-error/
> }
> }
> 
>  From a LAN client browser I can access and display the page at http://squidserver/proxy-error/ (direct access).
> 
> However, when SG is triggered and should send that redirect to the client browser, the client times out after a while, and displays Squid's ERR_CONNECT_FAIL with squidserver's IP address in the details.
> 
> I don't see anything useful in both Squid and SquidGuard's logs.
> 
> What could I try?
> 
> Thanks,
> 
> Vieri
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
> 


From rousskov at measurement-factory.com  Wed Nov  8 15:08:52 2017
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Wed, 8 Nov 2017 08:08:52 -0700
Subject: [squid-users] SQUID memory error after vm.swappines changed
 from 60 to 10
In-Reply-To: <CAAPixKhsif5+ad1prsoFAZXVXcVAxpdqkw4RdvGbfK-UxhkzTQ@mail.gmail.com>
References: <CAAPixKhsif5+ad1prsoFAZXVXcVAxpdqkw4RdvGbfK-UxhkzTQ@mail.gmail.com>
Message-ID: <86188b87-0cc3-343c-7b56-7f01354a3aa8@measurement-factory.com>

On 11/08/2017 02:26 AM, Bike dernikov1 wrote:

> I hope that someone can explain what happened, why squid stopped working.

I can suggest a working theory: You did not have enough RAM before
vm.swappiness changes and the same insufficient RAM problem led to
failed system calls after you told the OS to free RAM (into swap) less
aggressively. The changes effectively decreased the amount of
immediately available RAM.

If Squid is the primary service on your server, then consider:

1. Removing RAID (for Squid disk cache).

2. Removing swap (or at least not counting on it and treating
   any non-trivial swap usage as a system misconfiguration).

3. Identify the primary RAM consumers and
   the factors that influence their memory usage.

The above actions may not solve your "insufficient RAM" problem (unless
RAID was the primary culprit) but they will give you a better starting
point for finding the solution. Others on the list can help you get
there, especially if you have specific questions.


Good luck,

Alex.

> The server has 24GB DDR3 memory,  disk swap set to 24GB, 12 CPU (24HT cores).
> We have 2800 users, using  kerberos authentication, squidguard for
> filtering, ldap authorization.


From squid3 at treenet.co.nz  Wed Nov  8 16:21:38 2017
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 9 Nov 2017 05:21:38 +1300
Subject: [squid-users] url_rewrite_program and ACLs
In-Reply-To: <1990137371.4646637.1510150336187@mail.yahoo.com>
References: <1990137371.4646637.1510150336187.ref@mail.yahoo.com>
 <1990137371.4646637.1510150336187@mail.yahoo.com>
Message-ID: <921fbc5d-fce5-fd6f-b62e-8afd617994c7@treenet.co.nz>

On 09/11/17 03:12, Vieri wrote:
> Thanks. I defined the following, and it worked as expected:
> 
> url_rewrite_access deny allowed_domains
> url_rewrite_access deny allowed_ips
> url_rewrite_program /usr/bin/squidGuard
> url_rewrite_children 80 startup=10 idle=3
> 
> 
> How can I rewrite a URL in squid without a helper such as SG?
> ie. how can emulate SG's "rew" in squid.conf?

That depends on the rew(rite) substitutions being made, and more 
specifically what your intended end-goal behind having it was.

Usually what it is used for is emulate the denial of a request, or to 
redirect it somewhere specific.


* Denial is better done by "http_access deny ...". No rewrite/redirect 
necessary at all.

* Redirect is better done by using deny_info to change what action 
'deny' means for a particular ACL. Like so:

  acl foo ...
  http_access deny foo
  deny_info 302:http://example.com/ foo

In Squid-3.2+ the deny_info URL portion can use logformat macros for 
dynamic redirection - like the "rew" substitutions only changing 
portions of the URL.

Time constraints are added by using a time ACL on the original *_access 
line to limit when the foo ACL gets checked (aka. takes effect).


NP: The SG documented example for use of "rew" (diverting traffic to a 
local server during work hours) is better performed by a cache_peer 
directing traffic to a local server, and cache_peer_access ACLs 
determining what and when traffic gets delivered there. No denials, 
redirects, or rewrites necessary.


Amos


From Antony.Stone at squid.open.source.it  Wed Nov  8 16:56:15 2017
From: Antony.Stone at squid.open.source.it (Antony Stone)
Date: Wed, 8 Nov 2017 16:56:15 +0000
Subject: [squid-users] problem squid squidguard with outlook 2016
In-Reply-To: <CAMgVhhG+2q_nzskf=R9U+a+NC4+U7JozGs1F_bAL7FF2YbxiFw@mail.gmail.com>
References: <1510065276421-0.post@n4.nabble.com>
 <201711071906.44530.Antony.Stone@squid.open.source.it>
 <CAMgVhhG+2q_nzskf=R9U+a+NC4+U7JozGs1F_bAL7FF2YbxiFw@mail.gmail.com>
Message-ID: <201711081656.15326.Antony.Stone@squid.open.source.it>

On Wednesday 08 November 2017 at 13:55:44, Mohammed Rahmatellah wrote:

> hello guys,
> 
> this is the access log for squid, and the bloc file (squidguard)

No, don't just send us the entire log files and expect us to work out which 
requests were related to the problem.

Pick a specific machine in your internal network, note its IP address, get 
Outlook to do its thing at a known time, and then look at the log files for 
that IP address at that time to see what happened.

If you can't work things out for yourself based on this guidance, by all means 
post those specific entries from your log files here and ask for help.

Oh, and by the way, please answer the other two parts of my reply below:

 - is the behaviour the same if you use Squid without Squidguard?

 - what is the error message reported by Outlook, if any?

> 2017-11-07 20:06 GMT+01:00 Antony Stone:
> > On Tuesday 07 November 2017 at 14:34:36, rmohammed wrote:
> > > I have a problem with outlook 2016 in my office,
> > > 
> > > When i activate squid and squidguard,
> > 
> > Is the behaviour the same if you use Squid without Squidguard?
> > 
> > > outlook stop to receive and send emails,
> > > 
> > > can anyone help me plz?
> > 
> > Tell us what you see in the access log when people attempt this.
> > 
> > Also, if there is any error message reported by Outlook 2016, please tell
> > us what it says.

-- 
"In fact I wanted to be John Cleese and it took me some time to realise that 
the job was already taken."

 - Douglas Adams

                                                   Please reply to the list;
                                                         please *don't* CC me.


From frio_cervesa at hotmail.com  Wed Nov  8 18:22:50 2017
From: frio_cervesa at hotmail.com (senor)
Date: Wed, 8 Nov 2017 18:22:50 +0000
Subject: [squid-users] ALPN, HTTP/2 and sslbump
In-Reply-To: <177b9497-8b06-2049-c74b-02b77d151fe5@treenet.co.nz>
References: <BY2PR17MB018222156B827DDF84E7C6FCF7560@BY2PR17MB0182.namprd17.prod.outlook.com>
 <177b9497-8b06-2049-c74b-02b77d151fe5@treenet.co.nz>
Message-ID: <SN1PR17MB01916129CD15F8D14D231211F7560@SN1PR17MB0191.namprd17.prod.outlook.com>

Thanks Amos. I guess I was assuming that squid was just copying the ALPN 
extension info from Client Hello without regard to capabilities (squid 
3.5.26). I'll take another stab at the debug info and post more details 
if that doesn't pop something up.


Senor


On 11/7/2017 20:29, Amos Jeffries wrote:
> On 08/11/17 17:15, senor wrote:
>> I am surprised that I didn't find this question asked and answered
>> recently. Maybe this issue is newer than I realize.
>>
>> I understand that support of HTTPS/2 is in development but I'd like to
>> better understand what is and is not currently supported. I discovered
>> the other day that an intercepted client https connection, which
>> included both h2 and http/1.1 in the ALPN extension, was tunneled when
>> the server responded with only h2. I'm assuming that was due to squid
>> not fully supporting HTTP/2.
>
> Hmm. If you are using SSL-Bump to bump the traffic the current Squid 
> should be delivering an ALPN containing only HTTP/1.1 to the server. 
> Sending h2 in the ALPN is only valid if the proxy supports h2 natively 
> or intends up front to splice the transaction back to "tunneled".
>
>
>>
>> My initial need is to prevent the tunnel. Preferably by forcing http/1.1
>> and bumping but just denying the connection is second best. I'm not
>> aware of any squid built-in mechanisms to manage ALPN or HTTP/2 so I'm
>> thinking the external_acl is the only way to go. I think the client ALPN
>> data is available at bump step 2 but what options do I have at that 
>> point?
>>
>> Help or corrections to my assumptions are appreciated.
>>
>
> Any info about your Squid version, and squid.conf contents - 
> especially http_access and SSL-Bump related things would be useful. 
> Random guesses about complex things like TLS are harmful to solving 
> actual problems.
>
> Amos
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users


From ash.benz at bk.ru  Wed Nov  8 22:34:01 2017
From: ash.benz at bk.ru (A. Benz)
Date: Thu, 9 Nov 2017 06:34:01 +0800
Subject: [squid-users] Non intrusive sslbump for whitelisting (asked
 many times but..)
In-Reply-To: <96e5ae3d-6dc6-fca3-7c70-b038ecf16ddf@treenet.co.nz>
References: <ae8fe086-e227-6b58-c6d8-f4e37d062eee@bk.ru>
 <96e5ae3d-6dc6-fca3-7c70-b038ecf16ddf@treenet.co.nz>
Message-ID: <75c1764d-1923-f6c0-1896-2c64d1190779@bk.ru>

Hi Amos,

Many thanks for your detailed reply.

I have modified the config following your comments, before you see the 
new config (attached below), pls let me know your thoughts on the following:

1.

 > The workarounds and gotcha's listed at
 > <https://wiki.squid-cache.org/KnowledgeBase/HostHeaderForgery> are the
 > best you can hope for there. The most successful all-round solution is
 > to increase EDNS0 capabilities.

My particular case is a single server only, a corporate email server. 
This server is publicly accessible from internet (and has a valid signed 
SSL cert), now, on the remote location, there's a VPN setup that 
redirects access to the mail server to a private IP, eg 10.x.x.x (and 
this differs depending on loadbalance decision).

Without squid, I can connect to webmail, but with squid I get the 
forgery error. Does the EDNS0 fix this? See its almost working exactly 
as I need now, except for access to this single domain.. so if there's a 
workaround (even if it requires a recompile) to ignore this single 
domain do let me know.


2.

 > NAT of the dst-IP:port *MUST NOT* happen on any device between the
 > client machine and the proxy machine. Squid needs access directly to the
 > kernel NAT records of the device doing that NAT operation. So it can
 > only happen on the Squid device.
 >   You must *route* the packets unchanged to the Squid device (possibly
 > over a tunnel if necessary).

It happens on the same device (LEDE/OpenWrt router where squid is 
running), so the router is configured to intercept http (80) and https 
(443) traffic and redirect it to squid's ports:
80 ---> 3129
443 --> 3130


3.

 > Rather than allowing unlimited access to anyone on the Internet to use
 > your limited bandwidth outbound connection for access to port 443 you
 > should be using the localnet ACL that restricts use of the proxy to
 > people on your LAN - those 14 clients you mentioned sharing the line.
 >
 > [NP: It is not possible in this setup to determine what remote users are
 > abusing your proxy. All traffic logs from your firewall etc will show
 > Squid as the client, not the remote [ab]user. Squid access.log records
 > you are sending to /dev/null is the *only* record of such activities.]
 >
 >

I think I didn't word my earlier email properly, apologies for not being 
clear. No one from the internet has access to squid, the listening ports 
are not open to public, only accessible from LAN.

With abuse I meant the 14 users.. you know nowadays with mobiles/tablets 
and all the apps and syncing, I only allow ports 443 and 80 (and those 
are intercepted and forwarded to squid). All other ports are blocked.
The bandwidth available is extremely scarce and hence why I'm setting 
this up.


4.

 > To make your whitelists have any effect replace the above "allow
 > ssl_ports" line with a "deny !localnet" line.

When I do this, it doesn't work anymore. I get "Your connection is not 
secure" from firefox, and since google has HSTS, I can't "ignore and 
proceed". The squid access log shows (not .google.com is in whitelist.txt):

1510180110.096      0 192.168.1.178 TCP_DENIED/200 0 CONNECT 
108.177.14.103:443 - HIER_NONE/- -

Once I switch back to "allow SSL_ports" I can connect (squid splices' 
the connection, no complaints from firefox).


5.

I followed your comments about the config changes: changed acls to match 
original config in upper case. Swapped  the port numbers, but about 
having my ssl-bump match on 3129, pls check my new one see if I did it 
right.


## begin squid.conf


acl localnet src 10.0.0.0/8
acl localnet src 172.16.0.0/12
acl localnet src 192.168.0.0/16

acl SSL_ports port 443

acl Safe_ports port 80
acl Safe_ports port 443
acl CONNECT method CONNECT
acl http_whitelist dstdomain "/etc/squid/whitelist.txt"
acl https_whitelist ssl::server_name "/etc/squid/whitelist.txt"
acl ips_whitelist dst "/etc/squid/ips.txt"


http_access deny !Safe_ports
http_access deny CONNECT !SSL_ports
http_access allow SSL_ports
# http_access deny !localnet
http_access allow http_whitelist
http_access allow ips_whitelist
http_access deny all

http_port 3128 ssl-bump \
	cert=/etc/squid/myCA.pem \
	generate-host-certificates=off dynamic_cert_mem_cache_size=4MB

https_port 3130 intercept ssl-bump \
	cert=/etc/squid/myCA.pem \
	generate-host-certificates=off dynamic_cert_mem_cache_size=4MB

acl step1 at_step SslBump1
acl step2 at_step SslBump2
acl step3 at_step SslBump3

ssl_bump peek step1 all
ssl_bump splice https_whitelist
ssl_bump splice ips_whitelist
ssl_bump terminate all


refresh_pattern ^ftp: 1440 20% 10080
refresh_pattern ^gopher: 1440 0% 1440
refresh_pattern -i (/cgi-bin/|\?) 0 0% 0
refresh_pattern . 0 20% 4320

store_miss deny all
cache_log /tmp/squid/squid.log
access_log /tmp/squid/access.log
logfile_rotate 0

logfile_daemon /usr/bin/logger
http_port 3129 intercept
coredump_dir /tmp/squid
visible_hostname LEDE.lan
pinger_enable off
mime_table /tmp/squid/mime.conf
sslcrtd_program /usr/lib/squid/ssl_crtd -s /tmp/squid/ssldb -M 4MB

## end config

Many thanks!


Regards,
A. Benz

On 11/08/17 12:23, Amos Jeffries wrote:
> On 08/11/17 12:18, A. Benz wrote:
>> Hi all,
>>
>>
>> ## Intro
>>
>> I read many blogs and emails on this list related to what I'm trying 
>> to do, but most go into bumping or do things that are not as simple as 
>> I'm trying to achieve.
>>
>> I have an extremely slow line, with very high latency in a remote 
>> location. About 14 people are sharing this line. Nowadays with all the 
>> mobile apps trying to sync and such, the line stalls to unusable all 
>> the time.
>>
>> I tried doing filters with firewall or dns level, but those are not 
>> effective. In the end I figured squid might be my best option.
>>
>> ## End intro
>>
>>
>> I have squid 3.5.27 running under LEDE (OpenWrt fork), ie its 
>> cross-compiled for a MIPS based SoC (mediatek mt7621). I mention this 
>> because you will see some options in the config file that won't make 
>> sense otherwise.
>>
> 
> NP: That should not be making much difference to the squid.conf 
> settings. The worst limitations such devices impose are things that 
> should be solved by OS settings outside of squid.conf. eg the cache.log 
> going to a pipe for remote logging instead of a filename, and 
> system-level FD limits.
> 
> 
>> It works great, here's what I'm trying to achieve: Allow access only 
>> to a pre-defined list of websites (whitelist). http is 
>> straightforward, but if the connection is https all I need to know is 
>> domain, if its allowed, let it pass, otherwise terminate.
>>
>> this setup is working as intended with the config attached below, 
>> however the issue I'm facing is that some servers are "loadbalanced", 
>> this would give me the forgery error, eg:
>>
>> "SECURITY ALERT: Host header forgery detected on...."
>>
> 
> The workarounds and gotcha's listed at 
> <https://wiki.squid-cache.org/KnowledgeBase/HostHeaderForgery> are the 
> best you can hope for there. The most successful all-round solution is 
> to increase EDNS0 capabilities.
> 
> 
>> Here's a specific example, there's a corporate domain for webmail 
>> access, and some loadbalance config makes use of different IPs, I 
>> think this is what triggers the error. My question is, can I just 
>> ignore this error somehow and allow the connection? From what I gather 
>> this connection is cut by squid before it reaches the client..
> 
> Squid default behaviour is to allow the connection only to the same 
> IP:port the client was connecting to. If that is not working your 
> network configuration is screwed up. Specifically your routing or NAT.
> 
> NAT of the dst-IP:port *MUST NOT* happen on any device between the 
> client machine and the proxy machine. Squid needs access directly to the 
> kernel NAT records of the device doing that NAT operation. So it can 
> only happen on the Squid device.
>  ?You must *route* the packets unchanged to the Squid device (possibly 
> over a tunnel if necessary).
> 
> 
>>
>> Also if there's anything else obviously wrong with my setup please let 
>> me know.
>>
>> Many thanks.
>>
>>
>> Here's my config:
>>
>>
>> ### squid.conf begin
>>
>> acl localnet src 10.0.0.0/8
>> acl localnet src 172.16.0.0/12
>> acl localnet src 192.168.0.0/16
>>
>> acl ssl_ports port 443
>>
>> acl safe_ports port 80
>> acl safe_ports port 443
>> acl connect method connect
> 
> NP: the default above ACL names are case-sensitive and some of them 
> involve built-in default values which you are preventing having any 
> effect by using custom lower-case ACL names.
> 
> 
>> acl http_whitelist dstdomain "/etc/squid/whitelist.txt"
>> acl https_whitelist ssl::server_name "/etc/squid/whitelist.txt"
>> acl ips_whitelist dst "/etc/squid/ips.txt"
>>
>> http_port 3128 intercept
>> http_port 3129
> 
> Port 3128 is registered for forward-proxy traffic. Ideally you would 
> have those lines reversed like so:
> 
>  ?http_port 3128
>  ?http_port 3129 intercept
> 
> ... with the corresponding NAT change for the intercept port.
> 
> Also, to have your SSL-Bump whitelists applied to forward-proxy CONNECT 
> traffic you should have ssl-bump settings on that 3128 forward-proxy 
> port matching those on the port 3130 line.
> 
> 
>>
>> http_access deny !safe_ports
>> http_access deny connect !ssl_ports
> 
>> http_access allow ssl_ports
> 
> Rather than allowing unlimited access to anyone on the Internet to use 
> your limited bandwidth outbound connection for access to port 443 you 
> should be using the localnet ACL that restricts use of the proxy to 
> people on your LAN - those 14 clients you mentioned sharing the line.
> 
> [NP: It is not possible in this setup to determine what remote users are 
> abusing your proxy. All traffic logs from your firewall etc will show 
> Squid as the client, not the remote [ab]user. Squid access.log records 
> you are sending to /dev/null is the *only* record of such activities.]
> 
> 
> To make your whitelists have any effect replace the above "allow 
> ssl_ports" line with a "deny !localnet" line.
> 
> If that change causes issues then your whitelists are incorrect / 
> incomplete. You then need the (currently discarded) access.log and/or 
> cache.log data to solve the issue properly.
> 
> 
>> http_access allow http_whitelist
>> http_access allow ips_whitelist
>> http_access deny all
>>
>> https_port 3130 intercept ssl-bump \
>> ?????cert=/etc/squid/myCA.pem \
>> ?????generate-host-certificates=off dynamic_cert_mem_cache_size=4MB
>>
>> acl step1 at_step SslBump1
>> acl step2 at_step SslBump2
>> acl step3 at_step SslBump3
>>
>> ssl_bump peek step1 all
>> ssl_bump splice https_whitelist
>> ssl_bump splice ips_whitelist
>> ssl_bump terminate all
>>
> 
> That seems fine. The problem is not part of this _config_. If you are 
> having any SSL-Bump issues please try a build of the latest Squid-4. It 
> may be related to bugs in Squid-3 SSL-Bump or modern TLS things Squid-3 
> cannot cope with - there is a growing list of those.
> 
>>
>> cache deny all
> 
> In the latest Squid-3 use "store_miss deny all" instead of the above.
> 
>> access_log none
> 
> The above is fine if you are certain of the squid.conf working 100% 
> properly. But since you are debugging issues you may need those 
> transaction details.
> 
> NP: access.log can be logged to syslog or a TCP pipe by Squid. To 
> deliver the log content externally for normal audit purposes instead of 
> using space on the device.
> 
>> cache_log /dev/null
> 
> You *need* the information logged here. By default only the most 
> operationally critical errors are recorded.
> 
> NP: the cache.log can usually be a Unix-pipe delivering data to a remote 
> server if the local machine is constrained.
> 
>> cache_store_log stdio:/dev/null
> 
> Above line is *actively* harmful. The Squid-3 default is not to waste 
> cycles logging *unless* you enter something like the above in 
> squid.conf. The above makes Squid allocate device resources to logging 
> that data to /dev/null.
> 
>> logfile_rotate 0
>>
>> logfile_daemon /dev/null
> 
> /dev/null is not a valid application filename.
> 
> Build your Squid with --disable-logfile-daemon.
> 
>> coredump_dir /tmp/squid
>> visible_hostname main_Firewall
> 
> The *visible* hostname is the domain delivered to clients and denied 
> parties in the URLs to fetch error message data and FTP icons from 
> Squid. It needs to be a valid FQDN.
> 
> Amos
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users


From jeffrey.a.dunham at gmail.com  Wed Nov  8 23:38:34 2017
From: jeffrey.a.dunham at gmail.com (Jeffrey Dunham)
Date: Wed, 8 Nov 2017 15:38:34 -0800
Subject: [squid-users] Request/Response sizes for traffic over TCP_TUNNEL
Message-ID: <CAC_c4eoWk-RM4AK_kwS8g=dn4G=3Jo4TLRT=NLWq+7N=_+802w@mail.gmail.com>

I am using squid as a proxy and one of the tasks using the proxy is upload
some large files to S3 in AWS.
I'm trying to later on aggregate how much data was going through to S3 by
looking through the squid access log files.  The reality is, any way I can
tell how much traffic is going through to squid to S3 would be great, I was
going after the access file as it seemed most obvious.  However my logfiles
don't seem to show this information. I'm wondering if it's possible to show
this.  I thought that <st and >st would get me there.  I tried a few
different config settings for logging here's what it's at now:

logformat squid %ts.%03tu %6tr %>a %>A %Ss/%03>Hs %<st %>st %rm %ru %un
%Sh/%<A %mt

And my logs for a 200MB upload look like (this is broken into a multipart
upload):

1510181731.211  53953 <ip> <vpn> TCP_TUNNEL/200 3521 41 CONNECT
s3.amazonaws.com:443 - HIER_DIRECT/s3.amazonaws.com -
1510181731.991  54721 <ip> <vpn> TCP_TUNNEL/200 3865 41 CONNECT
s3.amazonaws.com:443 - HIER_DIRECT/s3.amazonaws.com -
1510181733.884  56646 <ip> <vpn> TCP_TUNNEL/200 3865 41 CONNECT
s3.amazonaws.com:443 - HIER_DIRECT/s3.amazonaws.com -
1510181737.049  59783 <ip> <vpn> TCP_TUNNEL/200 4209 41 CONNECT
s3.amazonaws.com:443 - HIER_DIRECT/s3.amazonaws.com -
1510181737.794  61246 <ip> <vpn> TCP_TUNNEL/200 5592 41 CONNECT
s3.amazonaws.com:443 - HIER_DIRECT/s3.amazonaws.com -
1510181737.796  60537 <ip> <vpn> TCP_TUNNEL/200 4178 41 CONNECT
s3.amazonaws.com:443 - HIER_DIRECT/s3.amazonaws.com -
1510181737.799  60529 <ip> <vpn> TCP_TUNNEL/200 3834 41 CONNECT
s3.amazonaws.com:443 - HIER_DIRECT/s3.amazonaws.com -
1510181737.800  60535 <ip> <vpn> TCP_TUNNEL/200 4178 41 CONNECT
s3.amazonaws.com:443 - HIER_DIRECT/s3.amazonaws.com -
1510181737.801  60531 <ip> <vpn> TCP_TUNNEL/200 4178 41 CONNECT
s3.amazonaws.com:443 - HIER_DIRECT/s3.amazonaws.com -
1510181737.807  60542 <ip> <vpn> TCP_TUNNEL/200 4209 41 CONNECT
s3.amazonaws.com:443 - HIER_DIRECT/s3.amazonaws.com -

Obviously there's nothing indicating the real size of the file that I can
tell, besides the fact that the connection is open awhile.

Any help would be appreciated!
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20171108/b83928d4/attachment.htm>

From rousskov at measurement-factory.com  Thu Nov  9 00:08:01 2017
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Wed, 8 Nov 2017 17:08:01 -0700
Subject: [squid-users] Request/Response sizes for traffic over TCP_TUNNEL
In-Reply-To: <CAC_c4eoWk-RM4AK_kwS8g=dn4G=3Jo4TLRT=NLWq+7N=_+802w@mail.gmail.com>
References: <CAC_c4eoWk-RM4AK_kwS8g=dn4G=3Jo4TLRT=NLWq+7N=_+802w@mail.gmail.com>
Message-ID: <8dd69fad-fdaf-ee94-49be-e1951723420a@measurement-factory.com>

On 11/08/2017 04:38 PM, Jeffrey Dunham wrote:

> logformat squid %ts.%03tu %6tr %>a %>A %Ss/%03>Hs %<st %>st %rm %ru %un
> %Sh/%<A %mt
> 
> And my logs for a 200MB upload look like (this is broken into a
> multipart upload):
> 
> 1510181731.211? 53953 <ip> <vpn>?TCP_TUNNEL/200 3521 41 CONNECT?s3.amazonaws.com:443...
> 1510181731.991? 54721 <ip> <vpn>?TCP_TUNNEL/200 3865 41 CONNECT?s3.amazonaws.com:443...
> 1510181733.884? 56646 <ip> <vpn>?TCP_TUNNEL/200 3865 41 CONNECT?s3.amazonaws.com:443...
> 1510181737.049? 59783 <ip> <vpn> TCP_TUNNEL/200 4209 41 CONNECT?s3.amazonaws.com:443...
> 1510181737.794? 61246 <ip> <vpn>?TCP_TUNNEL/200 5592 41 CONNECT?s3.amazonaws.com:443...

Sounds like the recently fix bug #4653 (%st lies about tunneled traffic
volumes): https://bugs.squid-cache.org/show_bug.cgi?id=4653

Alex.


From jun357572957zhao at hotmail.com  Thu Nov  9 02:33:23 2017
From: jun357572957zhao at hotmail.com (=?gb2312?B?1dQgv6E=?=)
Date: Thu, 9 Nov 2017 02:33:23 +0000
Subject: [squid-users] How to onfiguration https?
Message-ID: <DM5PR2201MB141996FA3C91862960BE3C6F98570@DM5PR2201MB1419.namprd22.prod.outlook.com>

I want to know squid4.0.21 version of the configuration https changes 
and how to configuration https in squid.conf?

Thank you !


From squid3 at treenet.co.nz  Thu Nov  9 04:57:10 2017
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 9 Nov 2017 17:57:10 +1300
Subject: [squid-users] How to onfiguration https?
In-Reply-To: <DM5PR2201MB141996FA3C91862960BE3C6F98570@DM5PR2201MB1419.namprd22.prod.outlook.com>
References: <DM5PR2201MB141996FA3C91862960BE3C6F98570@DM5PR2201MB1419.namprd22.prod.outlook.com>
Message-ID: <bee0a8b7-422c-808e-3636-149e0060235b@treenet.co.nz>

On 09/11/17 15:33, ? ? wrote:
> I want to know squid4.0.21 version of the configuration https changes
> and how to configuration https in squid.conf?
> 

Almost the same as in Squid-3.5. "squid -k parse" after the upgrade 
should tell you what changes need to be made for renaming of things you 
are using.

As always the set of changes to squid.conf are listed in the Squid 
Release Notes 
<http://www.squid-cache.org/Versions/v4/squid-4.0.21-RELEASENOTES.html#s3>

FYI: The Squid-4 notes should be correct and _almost_ complete. The 
version is beta still, so the full audit that takes place for stable 
release has not been done on them yet. If you have any doubts consult 
the squid.conf.documented texts which should be completely up to date 
(<http://www.squid-cache.org/Doc/config/>).


Cheers
Amos


From rentorbuy at yahoo.com  Thu Nov  9 07:51:07 2017
From: rentorbuy at yahoo.com (Vieri)
Date: Thu, 9 Nov 2017 07:51:07 +0000 (UTC)
Subject: [squid-users] url_rewrite_program and ACLs
In-Reply-To: <921fbc5d-fce5-fd6f-b62e-8afd617994c7@treenet.co.nz>
References: <1990137371.4646637.1510150336187.ref@mail.yahoo.com>
 <1990137371.4646637.1510150336187@mail.yahoo.com>
 <921fbc5d-fce5-fd6f-b62e-8afd617994c7@treenet.co.nz>
Message-ID: <131538788.5259356.1510213867661@mail.yahoo.com>


________________________________
From: Amos Jeffries <squid3 at treenet.co.nz>
>
> acl foo ...
>  http_access deny foo
>  deny_info 302:http://example.com/ foo
>
> In Squid-3.2+ the deny_info URL portion can use logformat macros for 
> dynamic redirection - like the "rew" substitutions only changing 
> portions of the URL.


I was already using deny_info like this:

deny_info http://squidserver/proxy-error/?a=%a&B=%B&e=%e&E=%E&H=%H&i=%i&M=%M&o=%o&R=%R&T=%T&U=%U&u=%u&w=%w&x=%x&acl=denied_domains denied_domains

I was wondering how to do an immediate redirect without doing it from my custom php script. Now I see.

I guess I'll have trouble redirecting https sites though... (TLS/SSL trust issues)

I don't know if I can "cleanly" redirect from an HTTPS to an HTTP site (ie. so the user's browser doesn't show a "can't open page" message of some sort...).

You mention that I can avoid using SG, ufdbGuard, or any other redirector/helper for access control. The problem I see when trying to use huge plain text blacklists within Squid directly is that it takes a LOT of time for the proxy cache to start up as it populates the ACLs.
I can't afford to wait for Squid to do that before serving client requests. I'd rather "allow everything" until the ACLs are populated than have users wait for so long.
Am I missing something? Is there a way to tell Squid to process the ACLs "in the background", but start handling requests immediately. If so, is it also possible to tell squid in which order to process the ACLs, ie. first process the allowed_domains ACL, and then the denied_domains ACLs? (is ordering in squid.conf enough?)

I'm saying this because I've sometimes had the need to restart Squid during the worst time of the day (peak working hours). That usually happens when I have an issue with too many open file descriptors (working on it). Stopping it cleanly takes up to 2-3 minutes. If I had to wait several more minutes for Squid to start again because it re-populates huge blacklist ACLs then I think they'd hang me for it.

Thanks,

Vieri


From ibrahim.ercan at hotmail.com  Thu Nov  9 08:43:19 2017
From: ibrahim.ercan at hotmail.com (Ibrahim Ercan)
Date: Thu, 9 Nov 2017 08:43:19 +0000
Subject: [squid-users] url_rewrite_extras can't send source port on tproxy
	mode
Message-ID: <HE1PR06MB30506B383E84671D373B3BBCEE570@HE1PR06MB3050.eurprd06.prod.outlook.com>

Hi.
I have a problem regarding tproxy.

I configured squid with tproxy by below tutorial.
https://wiki.squid-cache.org/Features/Tproxy4

Then I configured a url rewrite program as follows

url_rewrite_program /path/to/foo.py
url_rewrite_extras "%>a %>p"

On nat configuration it is working well, but on tproxy configuration
my script gets source port (>p) as 0.

Do you know any idea why, and how to solve it?

Thanks


--
Ibrahim Ercan

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20171109/e770d0a7/attachment.htm>

From simon at baladia.gov.kw  Thu Nov  9 10:03:17 2017
From: simon at baladia.gov.kw (Simon Dcunha)
Date: Thu, 9 Nov 2017 13:03:17 +0300 (AST)
Subject: [squid-users] https access only for few users
In-Reply-To: <1206568009.133991.1510214321125.JavaMail.zimbra@baladia.gov.kw>
References: <1206568009.133991.1510214321125.JavaMail.zimbra@baladia.gov.kw>
Message-ID: <118182430.134188.1510221797652.JavaMail.zimbra@baladia.gov.kw>


Dear All,

I have squid running with no issues for a long time and recently i have the below task.
User access to internet is based on physical machine IP address acl so only user ips listed in the conf file have access to internet

i need to allow access to only one site for some users and deny everything else. its a https site
the site is https://mof-sc-site.custhelp.com/

the users whos ips are listed in below acl should  access the above site only and nothing else.

so i have a access list as follows 

----------------------------------------------------
acl onesite src 172.16.52.23 172.16.6.121
acl allowed_site url_regex "/etc/squid/site"
http_access allow onesite allowed_site
http_access deny onesite
------------------------------------------------------

in /etc/squid/site i have
------------------
.mof*

Now when I try to access the above site it says page cannot be displayed and in the squid access.log i see the below
--------------
1510224319.009      0 172.16.6.121 TCP_DENIED/403 4201 CONNECT mof-sc-site.custhelp.com:443 - HIER_NONE/- text/html


but if I try to access http://www.mof.gov.kw the home page is displayed and works fine

appreciate your advice and help


regards

simon





-- 
---------
Network Administrator
Kuwait Municipality!!!
-- 
---------
Network Administrator
Kuwait Municipality!!!

-- 
This message has been scanned for viruses and
dangerous content by MailScanner, and is
believed to be clean.



From squid3 at treenet.co.nz  Thu Nov  9 10:09:40 2017
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 9 Nov 2017 23:09:40 +1300
Subject: [squid-users] url_rewrite_program and ACLs
In-Reply-To: <131538788.5259356.1510213867661@mail.yahoo.com>
References: <1990137371.4646637.1510150336187.ref@mail.yahoo.com>
 <1990137371.4646637.1510150336187@mail.yahoo.com>
 <921fbc5d-fce5-fd6f-b62e-8afd617994c7@treenet.co.nz>
 <131538788.5259356.1510213867661@mail.yahoo.com>
Message-ID: <e89c27ac-b147-426e-e220-2c566b5e7586@treenet.co.nz>

On 09/11/17 20:51, Vieri wrote:
> 
> ________________________________
> From: Amos Jeffries
>>
>> acl foo ...
>>   http_access deny foo
>>   deny_info 302:http://example.com/ foo
>>
>> In Squid-3.2+ the deny_info URL portion can use logformat macros for
>> dynamic redirection - like the "rew" substitutions only changing
>> portions of the URL.
> 
> 
> I was already using deny_info like this:
> 
> deny_info http://squidserver/proxy-error/?a=%a&B=%B&e=%e&E=%E&H=%H&i=%i&M=%M&o=%o&R=%R&T=%T&U=%U&u=%u&w=%w&x=%x&acl=denied_domains denied_domains
> 
> I was wondering how to do an immediate redirect without doing it from my custom php script. Now I see.
> 
> I guess I'll have trouble redirecting https sites though... (TLS/SSL trust issues)

The proper HTTP *redirect* with 3xx statsu code has no problems there. 
Since the redirect is simply telling the client to connect elsewhere.

It is re-writing the URL which encounters cert problems because the 
server gets secretly changed from what the client "knows" it is 
connected to mid-way along the connection.

If you are redirecting https:// URLs to a http:// location they may 
still warn about insecure content. But that is not a cert issue, just 
the browser scare tactics for pushing its "TLS-Everywhere" policy on 
your users.


> 
> I don't know if I can "cleanly" redirect from an HTTPS to an HTTP site (ie. so the user's browser doesn't show a "can't open page" message of some sort...).
> 
> You mention that I can avoid using SG, ufdbGuard, or any other redirector/helper for access control. The problem I see when trying to use huge plain text blacklists within Squid directly is that it takes a LOT of time for the proxy cache to start up as it populates the ACLs.
> I can't afford to wait for Squid to do that before serving client requests. I'd rather "allow everything" until the ACLs are populated than have users wait for so long.
> Am I missing something? Is there a way to tell Squid to process the ACLs "in the background", but start handling requests immediately. If so, is it also possible to tell squid in which order to process the ACLs, ie. first process the allowed_domains ACL, and then the denied_domains ACLs? (is ordering in squid.conf enough?)

Darn. You have the one case that calls for keeping the helper :-(

You can still move the ACLs that load in a reasonable times into 
squid.conf and leave the others in SG/ufdbguard. Using 
url_rewrite_access to restrict which transactions the helper gets 
involved with. That will reduce its latency impact on lie traffic, but 
still cause much the same memory related (non-)issues as now.

> 
> I'm saying this because I've sometimes had the need to restart Squid during the worst time of the day (peak working hours). That usually happens when I have an issue with too many open file descriptors (working on it). Stopping it cleanly takes up to 2-3 minutes. If I had to wait several more minutes for Squid to start again because it re-populates huge blacklist ACLs then I think they'd hang me for it.
> 

One thing that may also help you there while you figure out what those 
FD issues are caused by:

If you run "squid -k shutdown" once it signals a slow graceful shutdown 
of Squid. Which will take a *minimum* of the time configured in 
shutdown_lifetime directive (default 30sec) to wait for existing clients 
to finish up and disconnect.

Running "squid -k shutdown" a _second_ time sends the running proxy a 
signal to immediately skip to the processing as if the shutdown_lifetime 
had already been reached.

The shutdown time with two sequential calls should be only the time 
needed to close all the open FD connections, shutdown helpers and 
release however much memory Squid is using. All that happens in just a 
few seconds normally, but I'm not sure if "normal" matches your FD 
overload case. Will definitely be faster than waiting for the graceful 
shutdown anyhow.


Amos


From squid3 at treenet.co.nz  Thu Nov  9 10:31:47 2017
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 9 Nov 2017 23:31:47 +1300
Subject: [squid-users] https access only for few users
In-Reply-To: <118182430.134188.1510221797652.JavaMail.zimbra@baladia.gov.kw>
References: <1206568009.133991.1510214321125.JavaMail.zimbra@baladia.gov.kw>
 <118182430.134188.1510221797652.JavaMail.zimbra@baladia.gov.kw>
Message-ID: <a8e3b1b9-de4e-d4c6-12b7-9f9bb87f4428@treenet.co.nz>

On 09/11/17 23:03, Simon Dcunha wrote:
> 
> Dear All,
> 
> I have squid running with no issues for a long time and recently i have the below task.
> User access to internet is based on physical machine IP address acl so only user ips listed in the conf file have access to internet
> 
> i need to allow access to only one site for some users and deny everything else. its a https site
> the site is https://mof-sc-site.custhelp.com/
> 
> the users whos ips are listed in below acl should  access the above site only and nothing else.
> 
> so i have a access list as follows
> 
> ----------------------------------------------------
> acl onesite src 172.16.52.23 172.16.6.121
> acl allowed_site url_regex "/etc/squid/site"
> http_access allow onesite allowed_site
> http_access deny onesite
> ------------------------------------------------------
> 
> in /etc/squid/site i have
> ------------------
> .mof*
> 
> Now when I try to access the above site it says page cannot be displayed and in the squid access.log i see the below
> --------------
> 1510224319.009      0 172.16.6.121 TCP_DENIED/403 4201 CONNECT mof-sc-site.custhelp.com:443 - HIER_NONE/- text/html
> 
> 
> but if I try to access http://www.mof.gov.kw the home page is displayed and works fine
> 
> appreciate your advice and help
> 

You are a) using the wrong tool [regex] for the job of matching a single 
*domain*, and b) using regex VERY VERY badly.

Your regex says any URL in existence that contains _any_ single 
character followed by 'm' then 'o' is a match for the ACL - thus is 
allowed to the "onesite" client(s). The 'f' being optional (the *) and 
at the end of the pattern means it does not matter at all for the 
matching and may as well not exist.



What you should be doing is using an ACL type that matches domain names 
and telling it the domain that you want to match:

   acl allowed_site dstdomain mof-sc-site.custhelp.com

The rest of your config snippet was correct for what you want to do.

Amos


From rentorbuy at yahoo.com  Thu Nov  9 11:39:04 2017
From: rentorbuy at yahoo.com (Vieri)
Date: Thu, 9 Nov 2017 11:39:04 +0000 (UTC)
Subject: [squid-users] url_rewrite_program and ACLs
In-Reply-To: <e89c27ac-b147-426e-e220-2c566b5e7586@treenet.co.nz>
References: <1990137371.4646637.1510150336187.ref@mail.yahoo.com>
 <1990137371.4646637.1510150336187@mail.yahoo.com>
 <921fbc5d-fce5-fd6f-b62e-8afd617994c7@treenet.co.nz>
 <131538788.5259356.1510213867661@mail.yahoo.com>
 <e89c27ac-b147-426e-e220-2c566b5e7586@treenet.co.nz>
Message-ID: <2014705679.5324553.1510227544567@mail.yahoo.com>


________________________________
From: Amos Jeffries <squid3 at treenet.co.nz>
>
> Darn. You have the one case that calls for keeping the helper :-(
>
> You can still move the ACLs that load in a reasonable times into 
> squid.conf and leave the others in SG/ufdbguard. Using 
> url_rewrite_access to restrict which transactions the helper gets 
> involved with. That will reduce its latency impact on lie traffic, but 
> still cause much the same memory related (non-)issues as now.


That's exactly what I'm doing right now...
Thanks.

> Running "squid -k shutdown" a _second_ time sends the running proxy a 
> signal to immediately skip to the processing as if the shutdown_lifetime 
> had already been reached.


Thanks for that double-shutdown signal trick. I'll have to try that asap.

I'm making progress (sort of) on the FD (non-)issues I'm having.

I'll try to post back to Alex asap.

I have a custom perl script that does MySQL lookups for blacklisted sites (lots of them - so I can't use ACLs within squid.conf). I define that helper with external_acl_type.

Yesterday I changed my squid.conf by disabling this helper, and used squidGuard instead.
I noticed a huge improvement.

I took this snapshot yesterday:

15:25 08/11/2017:

File descriptor usage for squid:
Maximum number of file descriptors: 65536
Largest file desc currently in use: 2730
Number of file desc currently in use: 1838
Files queued for open: 0
Available number of file descriptors: 63698
Reserved number of file descriptors: 100
Store Disk files open: 0

Today I took another peak and found:

Thu Nov 9 12:19:05 CET 2017:

File descriptor usage for squid:
Maximum number of file descriptors: 65536
Largest file desc currently in use: 6980
Number of file desc currently in use: 6627
Files queued for open: 0
Available number of file descriptors: 58909
Reserved number of file descriptors: 100
Store Disk files open: 0

The FDs are still increasing steadily, but a LOT less.

On the other hand, the "free" RAM went from 2GB yesterday to just 275MB today:

# free --mega
total used free shared buff/cache available
Mem: 32865 8685 275 157 23904 23683
Swap: 37036 286 36750

Used swap is still low enough (unchanged actually), so I guess I don't need to worry about it.

However, I'm bound to have issues when the "free" mem reaches 0... and I bet it will eventually.
That's when the double-shutdown trick will kick in.

I'll review the perl helper code, or maybe just switch to ufdbGuard.

Thanks,

Vieri


From dernikov1 at gmail.com  Thu Nov  9 13:04:41 2017
From: dernikov1 at gmail.com (Bike dernikov1)
Date: Thu, 9 Nov 2017 14:04:41 +0100
Subject: [squid-users] SQUID memory error after vm.swappines changed
 from 60 to 10
In-Reply-To: <daf2a3ca-d5bd-f84c-0182-099f845c4e61@urlfilterdb.com>
References: <CAAPixKhsif5+ad1prsoFAZXVXcVAxpdqkw4RdvGbfK-UxhkzTQ@mail.gmail.com>
 <dab0b62b-57a5-5718-b0bd-70fb3de0c124@urlfilterdb.com>
 <CAAPixKjepL2BEePSH83XeBNB+UPy4wKoqjTn0_Axoa+n0THo7w@mail.gmail.com>
 <daf2a3ca-d5bd-f84c-0182-099f845c4e61@urlfilterdb.com>
Message-ID: <CAAPixKhCKhySLEZiUi1sA-hBwQ74KxM1XDN-jKEFL5B3=hfoeQ@mail.gmail.com>

On Wed, Nov 8, 2017 at 3:48 PM, Marcus Kool <marcus.kool at urlfilterdb.com> wrote:
>
> On 08/11/17 11:36, Bike dernikov1 wrote:
>>
>> Hi,
>>
>> We stumbled on ufdbGuard, but licence/price was problem, we didn't
>> read documentation carefully.
>
> yes, ufdbguard is free.
>
>> We will definitely try ufdbGuard, but we are now in process of moving
>> squid/squidguard to production, so we can't test on production (angry
>> users, Internet must work :)).
>>
>> Memory compsumption:squid use largest part of memory  (12GB now,
>> second proces use 300MB memory), 14GB used by all process. So squid
>> use over 80% of total used memory.
>> So no there are not any problematic process. But we changed swappiness
>> settings.
>
> Did you monitor Squid for growth (it can start with 12 GB and grow slowly) ?

Yes we are monitoring continuosly.
Now:
Output from free -m.

           total       used    free   shared  buff/cache  available
Mem:  24101     20507  256    146      3337         3034
Swap: 24561      5040   19521

vm.swappiness=40

Memory by process:
squid  Virt       RES   SHR  MEM%
           22,9G  18.7   8164   79,6
squidguard two process  300MB boths,.

CPU 0.33 0.37 0.43

> Squid cannot fork and higher swappiness increases the amount of memory that
> the OS can use to copy processes.
> It makes me think that you have the memory overcommit set to 2 (no
> overcommit).
> What is the output of the following command ?
>    sysctl  -a | grep overcommit

Command output:

vm.nr_overcommit_hugepages = 0
vm.overcommit_kbytes = 0
vm.overcommit_memory = 0
vm.overcommit_ratio = 50

cat /proc/sys/vm/overcommit_memory
0


>> Advice for some settings:
>> We have absolute max peak of  2500 users which user squid (of 2800),
>> what are recomended settings for:
>> negotiate_kerberos_children start/idle
>> squidguard helpers.
>
>
> I have little experience with kerberos, but most likely this is not the
> issue.
> When Squid cannot fork the helpers, helper settings do not matter much.

> For 2500 users you probably need 32-64 squidguard helpers.

Can you confirm: For 2500 users:

url_rewrite children X (squidguard)  32-64 will be ok ? We have set
much larger number.

For  helper:
negotitate_kerberos_auth

auth_param negotiate children X startup Y idle Z. What X, Y, Z are
best for our user number ?

We disabled kerberos replay cache because of disk performance (4 SAS
DISK  15K, RAID 10) (iowait jumped high, and CPU load jumped to min
40 max 200).
We don't use disk caching.

Thanks for help,

> Marcus
>
>
>> Thanks for help,
>>
>> On Wed, Nov 8, 2017 at 10:53 AM, Marcus Kool
>> <marcus.kool at urlfilterdb.com> wrote:
>>>
>>> There is definitely a problem with available memory because Squid cannot
>>> fork.
>>> So start with looking at how much memory Squid and its helpers use.
>>> Do do have other processes on this system that consume a lot of memory ?
>>>
>>> Also note that ufdbGuard uses less memory that squidGuard.
>>> If there are 30 helpers squidguard uses 300% more memory than ufdbGuard.
>>>
>>> Look at the wiki for more information about memory usage:
>>> https://wiki.squid-cache.org/SquidFaq/SquidMemory   (currently has an
>>> expired certificate but it is safe to go ahead)
>>>
>>> Marcus
>>>
>>>
>>>
>>> On 08/11/17 07:26, Bike dernikov1 wrote:
>>>>
>>>>
>>>> Hi, I hope that someone can explain what happened, why squid stopped
>>>> working.
>>>> The problem is related to  memory/swap handling.
>>>>
>>>> After we changed vm.swappiness parameter from 60 to 10 (tuning
>>>> attempt, to lower a disk usage, because we have only 4 disks in a
>>>> RAID10, so disk subsystem  is a weak link), we got a lot of errors in
>>>> cache.log.
>>>> The problems started after scheduled logrotate after  2AM.
>>>> Squid ran out of memory, auth helpers stopped working.
>>>> It's weird because we didn't disable swap, but behavior is like we did.
>>>> After an error, we increased parameter from 10 to 40.
>>>>
>>>> The server has 24GB DDR3 memory,  disk swap set to 24GB, 12 CPU (24HT
>>>> cores).
>>>> We have 2800 users, using  kerberos authentication, squidguard for
>>>> filtering, ldap authorization.
>>>> When problem appeared memory was still 3GB free (free column), ram
>>>> (caching) was filled to 15GB, so 21 GB ram filled, 3GB free.
>>>>
>>>> Thanks for help,
>>>>
>>>>
>>>> errors from cache.log.
>>>>
>>>> 2017/11/08 02:55:27| Set Current Directory to /var/log/squid/
>>>> 2017/11/08 02:55:27 kid1| storeDirWriteCleanLogs: Starting...
>>>> 2017/11/08 02:55:27 kid1|   Finished.  Wrote 0 entries.
>>>> 2017/11/08 02:55:27 kid1|   Took 0.00 seconds (  0.00 entries/sec).
>>>> 2017/11/08 02:55:27 kid1| logfileRotate:
>>>> daemon:/var/log/squid/access.log
>>>> 2017/11/08 02:55:27 kid1| logfileRotate:
>>>> daemon:/var/log/squid/access.log
>>>> 2017/11/08 02:55:28 kid1| Pinger socket opened on FD 30
>>>> 2017/11/08 02:55:28 kid1| helperOpenServers: Starting 1/1000
>>>> 'squidGuard' processes
>>>> 2017/11/08 02:55:28 kid1| ipcCreate: fork: (12) Cannot allocate memory
>>>> 2017/11/08 02:55:28 kid1| WARNING: Cannot run '/usr/bin/squidGuard'
>>>> process.
>>>> 2017/11/08 02:55:28 kid1| helperOpenServers: Starting 300/3000
>>>> 'negotiate_kerberos_auth' processes
>>>> 2017/11/08 02:55:28 kid1| ipcCreate: fork: (12) Cannot allocate memory
>>>> 2017/11/08 02:55:28 kid1| WARNING: Cannot run
>>>> '/usr/lib/squid/negotiate_kerberos_auth' process.
>>>> 2017/11/08 02:55:28 kid1| ipcCreate: fork: (12) Cannot allocate memory
>>>> 2017/11/08 02:55:28 kid1| WARNING: Cannot run
>>>> '/usr/lib/squid/negotiate_kerberos_auth' process.
>>>> 2017/11/08 02:55:28 kid1| ipcCreate: fork: (12) Cannot allocate memory
>>>> 2017/11/08 02:55:28 kid1| WARNING: Cannot run
>>>> '/usr/lib/squid/negotiate_kerberos_auth' process.
>>>>
>>>> external ACL 'memberof' queue overload. Using stale result.
>>>> _______________________________________________
>>>> squid-users mailing list
>>>> squid-users at lists.squid-cache.org
>>>> http://lists.squid-cache.org/listinfo/squid-users
>>>>
>>> _______________________________________________
>>> squid-users mailing list
>>> squid-users at lists.squid-cache.org
>>> http://lists.squid-cache.org/listinfo/squid-users
>>
>>
>>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users


From rentorbuy at yahoo.com  Thu Nov  9 13:05:53 2017
From: rentorbuy at yahoo.com (Vieri)
Date: Thu, 9 Nov 2017 13:05:53 +0000 (UTC)
Subject: [squid-users] url_rewrite_program and ACLs
In-Reply-To: <e89c27ac-b147-426e-e220-2c566b5e7586@treenet.co.nz>
References: <1990137371.4646637.1510150336187.ref@mail.yahoo.com>
 <1990137371.4646637.1510150336187@mail.yahoo.com>
 <921fbc5d-fce5-fd6f-b62e-8afd617994c7@treenet.co.nz>
 <131538788.5259356.1510213867661@mail.yahoo.com>
 <e89c27ac-b147-426e-e220-2c566b5e7586@treenet.co.nz>
Message-ID: <1403738783.5328471.1510232753673@mail.yahoo.com>


________________________________
From: Amos Jeffries <squid3 at treenet.co.nz>
>
> The shutdown time with two sequential calls should be only the time 
> needed to close all the open FD connections, shutdown helpers and 
> release however much memory Squid is using. All that happens in just a 

> few seconds normally

I updated to version 3.5.27-20171101-re69e56c, and restarted squid with the double-shutdown trick (I didn't NEED to do it - I just couldn't wait to try it out ;-) ).

The result was acceptable. It didn't take a "few seconds", but almost 30 seconds. Still, it's a LOT better than before when I had to wait 2 or 3 minutes.

It's fair to say that there weren't as many open FDs as in my previous cases when shutting down Squid was very slow. Now, open FDs were around 7000.
A lot of mem was freed up after the restart.

Making progress.

Thanks,

Vieri


From dernikov1 at gmail.com  Thu Nov  9 13:49:39 2017
From: dernikov1 at gmail.com (Bike dernikov1)
Date: Thu, 9 Nov 2017 14:49:39 +0100
Subject: [squid-users] SQUID memory error after vm.swappines changed
 from 60 to 10
In-Reply-To: <86188b87-0cc3-343c-7b56-7f01354a3aa8@measurement-factory.com>
References: <CAAPixKhsif5+ad1prsoFAZXVXcVAxpdqkw4RdvGbfK-UxhkzTQ@mail.gmail.com>
 <86188b87-0cc3-343c-7b56-7f01354a3aa8@measurement-factory.com>
Message-ID: <CAAPixKiTszJDHUDK__PFgQ_+g0kR0RX4KZUNykFANKnTqzydsg@mail.gmail.com>

>>On 11/08/2017 02:26 AM, Bike dernikov1 wrote:

>> I hope that someone can explain what happened, why squid stopped working.

>I can suggest a working theory: You did not have enough RAM before
>vm.swappiness changes and the same insufficient RAM problem led to
>failed system calls after you told the OS to free RAM (into swap) less
>aggressively. The changes effectively decreased the amount of
>immediately available RAM.

>From "free -m output" (available column)  i thought that we have
enough memory, because  as I can remember, available never dropped
under 2GB.
As I understood  that system with  vm.swappines set to 10, will start
draining ram to swap after 90% memory used.
So why didn't swap hop in ? That we set vm.swappines to 0 i would
understand error.
Because errors started with  logrotate, i suspect that  problem
exploded because of logrotate and swap started to write to a disk at
same time,
and because of slow disks (small IOPS) it could not drain memory fast
enough, so available memory vanished and hell lose from there.


>If Squid is the primary service on your server, then consider:

It isn't only service but use over 90-95 percent memory resources.

>1. Removing RAID (for Squid disk cache).

We don't use disk cache because of slow disks, compared to  "high"
bandwith (we have 500Mbit/s)

>2. Removing swap (or at least not counting on it and treating
>  any non-trivial swap usage as a system misconfiguration).

What to search ? Key words, vm.swappines got us in problem, as we
tried to tune  ?

>3. Identify the primary RAM consumers and
>  the factors that influence their memory usage.

Squid, we suspect on kerberos negotiator, squid helpers, we will tune
up down setting over two weeks.
We have problems with squid.conf reconfiguration, because users lose
connection (they got squid error screen for moment) so it is nightmare
if we changing configuration in work time.

>The above actions may not solve your "insufficient RAM" problem (unless
>RAID was the primary culprit) but they will give you a better starting
>point for finding the solution. Others on the list can help you get
>there, especially if you have specific questions.
>
>
>Good luck,
>
>Alex.

Thanks for theory, and help.


On Wed, Nov 8, 2017 at 4:08 PM, Alex Rousskov
<rousskov at measurement-factory.com> wrote:
> On 11/08/2017 02:26 AM, Bike dernikov1 wrote:
>
>> I hope that someone can explain what happened, why squid stopped working.
>
> I can suggest a working theory: You did not have enough RAM before
> vm.swappiness changes and the same insufficient RAM problem led to
> failed system calls after you told the OS to free RAM (into swap) less
> aggressively. The changes effectively decreased the amount of
> immediately available RAM.
>
> If Squid is the primary service on your server, then consider:
>
> 1. Removing RAID (for Squid disk cache).
>
> 2. Removing swap (or at least not counting on it and treating
>    any non-trivial swap usage as a system misconfiguration).
>
> 3. Identify the primary RAM consumers and
>    the factors that influence their memory usage.
>
> The above actions may not solve your "insufficient RAM" problem (unless
> RAID was the primary culprit) but they will give you a better starting
> point for finding the solution. Others on the list can help you get
> there, especially if you have specific questions.
>
>
> Good luck,
>
> Alex.
>
>> The server has 24GB DDR3 memory,  disk swap set to 24GB, 12 CPU (24HT cores).
>> We have 2800 users, using  kerberos authentication, squidguard for
>> filtering, ldap authorization.


From guarez at gmail.com  Thu Nov  9 13:58:13 2017
From: guarez at gmail.com (=?UTF-8?B?Tmljb2zDoXMgSGVydsOp?=)
Date: Thu, 9 Nov 2017 10:58:13 -0300
Subject: [squid-users] Problem with squid and acces to localhost ipv6
Message-ID: <CAMtDEa3g-tZHp+zS30xd3qQYLccBXz4ZH658HBSZjgjViVpCKg@mail.gmail.com>

Hi, I have a problem with some users who use pgadmin v4, sometimes a
"connection to :: 1 failed" error appears when they enter http: //
localhost: 55501 / the system returns (111) connection refused. Why can
this be happening? and how can I solve it?
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20171109/f5c15304/attachment.htm>

From rousskov at measurement-factory.com  Thu Nov  9 15:03:22 2017
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Thu, 9 Nov 2017 08:03:22 -0700
Subject: [squid-users] SQUID memory error after vm.swappines changed
 from 60 to 10
In-Reply-To: <CAAPixKiTszJDHUDK__PFgQ_+g0kR0RX4KZUNykFANKnTqzydsg@mail.gmail.com>
References: <CAAPixKhsif5+ad1prsoFAZXVXcVAxpdqkw4RdvGbfK-UxhkzTQ@mail.gmail.com>
 <86188b87-0cc3-343c-7b56-7f01354a3aa8@measurement-factory.com>
 <CAAPixKiTszJDHUDK__PFgQ_+g0kR0RX4KZUNykFANKnTqzydsg@mail.gmail.com>
Message-ID: <bfb6f0fc-357b-2156-7cec-e43e9718d521@measurement-factory.com>

On 11/09/2017 06:49 AM, Bike dernikov1 wrote:
>>> On 11/08/2017 02:26 AM, Bike dernikov1 wrote:

> So why didn't swap hop in?

As I implied earlier, I do not recommend researching swap-related
details because I do not recommend using (or relying on) swap.


>> 2. Removing swap (or at least not counting on it and treating
>>  any non-trivial swap usage as a system misconfiguration).

> What to search ? Key words, vm.swappines got us in problem, as we
> tried to tune  ?

I do not understand the question. I am sure you know how to turn swap
off (i.e., completely disable it).


> We have problems with squid.conf reconfiguration, because users lose
> connection (they got squid error screen for moment) so it is nightmare
> if we changing configuration in work time.

Yes, this is a common complaint and a known Squid problem.

http://wiki.squid-cache.org/SquidFaq/AboutSquid#How_to_add_a_new_Squid_feature.2C_enhance.2C_of_fix_something.3F

Alex.


From marcus.kool at urlfilterdb.com  Thu Nov  9 16:13:11 2017
From: marcus.kool at urlfilterdb.com (Marcus Kool)
Date: Thu, 9 Nov 2017 14:13:11 -0200
Subject: [squid-users] SQUID memory error after vm.swappines changed
 from 60 to 10
In-Reply-To: <CAAPixKhCKhySLEZiUi1sA-hBwQ74KxM1XDN-jKEFL5B3=hfoeQ@mail.gmail.com>
References: <CAAPixKhsif5+ad1prsoFAZXVXcVAxpdqkw4RdvGbfK-UxhkzTQ@mail.gmail.com>
 <dab0b62b-57a5-5718-b0bd-70fb3de0c124@urlfilterdb.com>
 <CAAPixKjepL2BEePSH83XeBNB+UPy4wKoqjTn0_Axoa+n0THo7w@mail.gmail.com>
 <daf2a3ca-d5bd-f84c-0182-099f845c4e61@urlfilterdb.com>
 <CAAPixKhCKhySLEZiUi1sA-hBwQ74KxM1XDN-jKEFL5B3=hfoeQ@mail.gmail.com>
Message-ID: <a7c29c43-131c-8f0b-fa66-88db16d0f8e5@urlfilterdb.com>



On 09/11/17 11:04, Bike dernikov1 wrote:
[snip]
>>> Memory compsumption:squid use largest part of memory  (12GB now,
>>> second proces use 300MB memory), 14GB used by all process. So squid
>>> use over 80% of total used memory.
>>> So no there are not any problematic process. But we changed swappiness
>>> settings.
>>
>> Did you monitor Squid for growth (it can start with 12 GB and grow slowly) ?
> 
> Yes we are monitoring continuosly.
> Now:
> Output from free -m.
> 
>             total       used    free   shared  buff/cache  available
> Mem:  24101     20507  256    146      3337         3034
> Swap: 24561      5040   19521
> 
> vm.swappiness=40
> 
> Memory by process:
> squid  Virt       RES   SHR  MEM%
>             22,9G  18.7   8164   79,6

Hmm. Squid grew from 12 GB to 18.7 GB (23 GB virtual).

With vm.swappiness=40 Linux starts to page out parts of processes when they occupy more than 60% of the memory.
This is a potential bottleneck and I would have also decreased vm.swappiness to 10 as you did.

My guess is that Squid starts too many helpers in a short time frame and that because of paging there are too many forks in progress simultaneously which causes the memory exhaustion.

I suggest to reduce the memory cache of Squid by 50% and set vm.swappiness to 20.
And then observe:
- total memory use
- total swap usage (should be lower than the 5 GB that you have now)
- number of helper processes that are started in short time frames
And then in small steps increase the memory cache and maybe further reduce vm.swappiness to 10.

> squidguard two process  300MB boths,.
> 
> CPU 0.33 0.37 0.43
> 
>> Squid cannot fork and higher swappiness increases the amount of memory that
>> the OS can use to copy processes.
>> It makes me think that you have the memory overcommit set to 2 (no
>> overcommit).
>> What is the output of the following command ?
>>     sysctl  -a | grep overcommit
> 
> Command output:
> 
> vm.nr_overcommit_hugepages = 0
> vm.overcommit_kbytes = 0
> vm.overcommit_memory = 0
> vm.overcommit_ratio = 50
> 
> cat /proc/sys/vm/overcommit_memory
> 0

The overcommit settings look fine.

> 
>>> Advice for some settings:
>>> We have absolute max peak of  2500 users which user squid (of 2800),
>>> what are recomended settings for:
>>> negotiate_kerberos_children start/idle
>>> squidguard helpers.
>>
>>
>> I have little experience with kerberos, but most likely this is not the
>> issue.
>> When Squid cannot fork the helpers, helper settings do not matter much.
> 
>> For 2500 users you probably need 32-64 squidguard helpers.
> 
> Can you confirm: For 2500 users:
> 
> url_rewrite children X (squidguard)  32-64 will be ok ? We have set
> much larger number.

Did I understand it correctly that earlier in this reply you said that there are two squidguard processes (300 MB each).
ufdbGuard is faster than squidGuard and has multithreaded helpers.  ufdbGuard needs less helpers than squidGuard.

If you have a much larger number than 64 url rewrite helpers than I suggest to switch to ufdbGuard as soon as possible since the memory usage is then at least 600% less.

> For  helper:
> negotitate_kerberos_auth
> 
> auth_param negotiate children X startup Y idle Z. What X, Y, Z are
> best for our user number ?
> 
> We disabled kerberos replay cache because of disk performance (4 SAS
> DISK  15K, RAID 10) (iowait jumped high, and CPU load jumped to min
> 40 max 200).
> We don't use disk caching.
> 
> Thanks for help,
> 
>> Marcus
>>
>>
>>> Thanks for help,
>>>
>>> On Wed, Nov 8, 2017 at 10:53 AM, Marcus Kool
>>> <marcus.kool at urlfilterdb.com> wrote:
>>>>
>>>> There is definitely a problem with available memory because Squid cannot
>>>> fork.
>>>> So start with looking at how much memory Squid and its helpers use.
>>>> Do do have other processes on this system that consume a lot of memory ?
>>>>
>>>> Also note that ufdbGuard uses less memory that squidGuard.
>>>> If there are 30 helpers squidguard uses 300% more memory than ufdbGuard.
>>>>
>>>> Look at the wiki for more information about memory usage:
>>>> https://wiki.squid-cache.org/SquidFaq/SquidMemory   (currently has an
>>>> expired certificate but it is safe to go ahead)
>>>>
>>>> Marcus
>>>>
>>>>
>>>>
>>>> On 08/11/17 07:26, Bike dernikov1 wrote:
>>>>>
>>>>>
>>>>> Hi, I hope that someone can explain what happened, why squid stopped
>>>>> working.
>>>>> The problem is related to  memory/swap handling.
>>>>>
>>>>> After we changed vm.swappiness parameter from 60 to 10 (tuning
>>>>> attempt, to lower a disk usage, because we have only 4 disks in a
>>>>> RAID10, so disk subsystem  is a weak link), we got a lot of errors in
>>>>> cache.log.
>>>>> The problems started after scheduled logrotate after  2AM.
>>>>> Squid ran out of memory, auth helpers stopped working.
>>>>> It's weird because we didn't disable swap, but behavior is like we did.
>>>>> After an error, we increased parameter from 10 to 40.
>>>>>
>>>>> The server has 24GB DDR3 memory,  disk swap set to 24GB, 12 CPU (24HT
>>>>> cores).
>>>>> We have 2800 users, using  kerberos authentication, squidguard for
>>>>> filtering, ldap authorization.
>>>>> When problem appeared memory was still 3GB free (free column), ram
>>>>> (caching) was filled to 15GB, so 21 GB ram filled, 3GB free.
>>>>>
>>>>> Thanks for help,
>>>>>
>>>>>
>>>>> errors from cache.log.
>>>>>
>>>>> 2017/11/08 02:55:27| Set Current Directory to /var/log/squid/
>>>>> 2017/11/08 02:55:27 kid1| storeDirWriteCleanLogs: Starting...
>>>>> 2017/11/08 02:55:27 kid1|   Finished.  Wrote 0 entries.
>>>>> 2017/11/08 02:55:27 kid1|   Took 0.00 seconds (  0.00 entries/sec).
>>>>> 2017/11/08 02:55:27 kid1| logfileRotate:
>>>>> daemon:/var/log/squid/access.log
>>>>> 2017/11/08 02:55:27 kid1| logfileRotate:
>>>>> daemon:/var/log/squid/access.log
>>>>> 2017/11/08 02:55:28 kid1| Pinger socket opened on FD 30
>>>>> 2017/11/08 02:55:28 kid1| helperOpenServers: Starting 1/1000
>>>>> 'squidGuard' processes
>>>>> 2017/11/08 02:55:28 kid1| ipcCreate: fork: (12) Cannot allocate memory
>>>>> 2017/11/08 02:55:28 kid1| WARNING: Cannot run '/usr/bin/squidGuard'
>>>>> process.
>>>>> 2017/11/08 02:55:28 kid1| helperOpenServers: Starting 300/3000
>>>>> 'negotiate_kerberos_auth' processes
>>>>> 2017/11/08 02:55:28 kid1| ipcCreate: fork: (12) Cannot allocate memory
>>>>> 2017/11/08 02:55:28 kid1| WARNING: Cannot run
>>>>> '/usr/lib/squid/negotiate_kerberos_auth' process.
>>>>> 2017/11/08 02:55:28 kid1| ipcCreate: fork: (12) Cannot allocate memory
>>>>> 2017/11/08 02:55:28 kid1| WARNING: Cannot run
>>>>> '/usr/lib/squid/negotiate_kerberos_auth' process.
>>>>> 2017/11/08 02:55:28 kid1| ipcCreate: fork: (12) Cannot allocate memory
>>>>> 2017/11/08 02:55:28 kid1| WARNING: Cannot run
>>>>> '/usr/lib/squid/negotiate_kerberos_auth' process.
>>>>>
>>>>> external ACL 'memberof' queue overload. Using stale result.
>>>>> _______________________________________________
>>>>> squid-users mailing list
>>>>> squid-users at lists.squid-cache.org
>>>>> http://lists.squid-cache.org/listinfo/squid-users
>>>>>
>>>> _______________________________________________
>>>> squid-users mailing list
>>>> squid-users at lists.squid-cache.org
>>>> http://lists.squid-cache.org/listinfo/squid-users
>>>
>>>
>>>
>> _______________________________________________
>> squid-users mailing list
>> squid-users at lists.squid-cache.org
>> http://lists.squid-cache.org/listinfo/squid-users
> 
> 


From squid3 at treenet.co.nz  Thu Nov  9 19:30:39 2017
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Fri, 10 Nov 2017 08:30:39 +1300
Subject: [squid-users] Non intrusive sslbump for whitelisting (asked
 many times but..)
In-Reply-To: <75c1764d-1923-f6c0-1896-2c64d1190779@bk.ru>
References: <ae8fe086-e227-6b58-c6d8-f4e37d062eee@bk.ru>
 <96e5ae3d-6dc6-fca3-7c70-b038ecf16ddf@treenet.co.nz>
 <75c1764d-1923-f6c0-1896-2c64d1190779@bk.ru>
Message-ID: <7a55ca37-23d0-a0f3-6397-2a772f91745f@treenet.co.nz>

On 09/11/17 11:34, A. Benz wrote:
> Hi Amos,
> 
> Many thanks for your detailed reply.
> 
> I have modified the config following your comments, before you see the 
> new config (attached below), pls let me know your thoughts on the 
> following:
> 
> 1.
> 
>  > The workarounds and gotcha's listed at
>  > <https://wiki.squid-cache.org/KnowledgeBase/HostHeaderForgery> are the
>  > best you can hope for there. The most successful all-round solution is
>  > to increase EDNS0 capabilities.
> 
> My particular case is a single server only, a corporate email server. 
> This server is publicly accessible from internet (and has a valid signed 
> SSL cert), now, on the remote location, there's a VPN setup that 
> redirects access to the mail server to a private IP, eg 10.x.x.x (and 
> this differs depending on loadbalance decision).

Do you mean the VPN exit point has that 10/8 IP address? or that the 
traffic from the client is altered to be going to that IP before it 
reaches Squid?

The latter is broken because it destroys the original dst-IP values on 
the TCP connection. Which Squid needs to setup the server connection.


> 
> Without squid, I can connect to webmail, but with squid I get the 
> forgery error. Does the EDNS0 fix this? See its almost working exactly 
> as I need now, except for access to this single domain.. so if there's a 
> workaround (even if it requires a recompile) to ignore this single 
> domain do let me know.
> 

EDNS0 fixes problems with services that load balance by rotating the IP 
addresses delivered in response to A/AAAA queries, possibly omitting 
some records if the final few don't fit. That results in Squid getting 
different IPs occasionally than the one the client is using. EDNS0 
extends the available DNS response packet size to fit all the records so 
Squid can see them all even when a large set is rotating.

There are a few major hosting providers that have that behaviour in 
their DNS. If you have not hit it yet you are very lucky.


> 
> 2.
> 
>  > NAT of the dst-IP:port *MUST NOT* happen on any device between the
>  > client machine and the proxy machine. Squid needs access directly to the
>  > kernel NAT records of the device doing that NAT operation. So it can
>  > only happen on the Squid device.
>  >?? You must *route* the packets unchanged to the Squid device (possibly
>  > over a tunnel if necessary).
> 
> It happens on the same device (LEDE/OpenWrt router where squid is 
> running), so the router is configured to intercept http (80) and https 
> (443) traffic and redirect it to squid's ports:
> 80 ---> 3129
> 443 --> 3130
> 
> 
> 3.
> 
>  > Rather than allowing unlimited access to anyone on the Internet to use
>  > your limited bandwidth outbound connection for access to port 443 you
>  > should be using the localnet ACL that restricts use of the proxy to
>  > people on your LAN - those 14 clients you mentioned sharing the line.
>  >
>  > [NP: It is not possible in this setup to determine what remote users are
>  > abusing your proxy. All traffic logs from your firewall etc will show
>  > Squid as the client, not the remote [ab]user. Squid access.log records
>  > you are sending to /dev/null is the *only* record of such activities.]
>  >
>  >
> 
> I think I didn't word my earlier email properly, apologies for not being 
> clear. No one from the internet has access to squid, the listening ports 
> are not open to public, only accessible from LAN.

If for any reason those firewall rules change in unexpected ways or 
don't block something you expect to be blocked this may leave a security 
hole open. It does not seem to be necessary really, so best to close.


> 
> With abuse I meant the 14 users.. you know nowadays with mobiles/tablets 
> and all the apps and syncing, I only allow ports 443 and 80 (and those 
> are intercepted and forwarded to squid). All other ports are blocked.
> The bandwidth available is extremely scarce and hence why I'm setting 
> this up.

The point I was trying to emphasize is that your Squid is accepting 
*anything* in those port 443 connections.


> 
> 4.
> 
>  > To make your whitelists have any effect replace the above "allow
>  > ssl_ports" line with a "deny !localnet" line.
> 
> When I do this, it doesn't work anymore. I get "Your connection is not 
> secure" from firefox, and since google has HSTS, I can't "ignore and 
> proceed". The squid access log shows (not .google.com is in whitelist.txt):

HSTS requires a header "Strict-Transport-Security" to be delivered from 
servers before it takes effect. You can erase that header from replies 
going through Squid with the reply_header_access directive. Current 
Squid should be doing it automatically.
There may be issues with HSTS if the header is received before the 
device connects to your network, or if it arrives over an uncontrolled 
CONNECT tunnel. But there is not much to be done about those cases.


> 
> 1510180110.096????? 0 192.168.1.178 TCP_DENIED/200 0 CONNECT 
> 108.177.14.103:443 - HIER_NONE/- -
> 
> Once I switch back to "allow SSL_ports" I can connect (squid splices' 
> the connection, no complaints from firefox).
> 

That means the server that HTTPS connection is attempting to reach is 
not on your whitelist. It is therefore one of the things you wanted to 
be blocked according to your stated policy.

  - as I mentioned earlier, if that causes any issues your whitelist is 
incomplete.


> 
> 5.
> 
> I followed your comments about the config changes: changed acls to match 
> original config in upper case. Swapped? the port numbers, but about 
> having my ssl-bump match on 3129, pls check my new one see if I did it 
> right.
> 
> 
> ## begin squid.conf
> 
> 
> acl localnet src 10.0.0.0/8
> acl localnet src 172.16.0.0/12
> acl localnet src 192.168.0.0/16
> 
> acl SSL_ports port 443
> 
> acl Safe_ports port 80
> acl Safe_ports port 443
> acl CONNECT method CONNECT
> acl http_whitelist dstdomain "/etc/squid/whitelist.txt"
> acl https_whitelist ssl::server_name "/etc/squid/whitelist.txt"
> acl ips_whitelist dst "/etc/squid/ips.txt"
> 
> 
> http_access deny !Safe_ports
> http_access deny CONNECT !SSL_ports
> http_access allow SSL_ports
> # http_access deny !localnet
> http_access allow http_whitelist
> http_access allow ips_whitelist
> http_access deny all
> 
> http_port 3128 ssl-bump \
>  ????cert=/etc/squid/myCA.pem \
>  ????generate-host-certificates=off dynamic_cert_mem_cache_size=4MB
> 
> https_port 3130 intercept ssl-bump \
>  ????cert=/etc/squid/myCA.pem \
>  ????generate-host-certificates=off dynamic_cert_mem_cache_size=4MB
> 
> acl step1 at_step SslBump1
> acl step2 at_step SslBump2
> acl step3 at_step SslBump3
> 
> ssl_bump peek step1 all
> ssl_bump splice https_whitelist
> ssl_bump splice ips_whitelist
> ssl_bump terminate all
> 
> 
> refresh_pattern ^ftp: 1440 20% 10080
> refresh_pattern ^gopher: 1440 0% 1440
> refresh_pattern -i (/cgi-bin/|\?) 0 0% 0
> refresh_pattern . 0 20% 4320
> 
> store_miss deny all
> cache_log /tmp/squid/squid.log
> access_log /tmp/squid/access.log
> logfile_rotate 0
> 
> logfile_daemon /usr/bin/logger
> http_port 3129 intercept
> coredump_dir /tmp/squid
> visible_hostname LEDE.lan
> pinger_enable off
> mime_table /tmp/squid/mime.conf
> sslcrtd_program /usr/lib/squid/ssl_crtd -s /tmp/squid/ssldb -M 4MB
> 
> ## end config
> 
> Many thanks!
> 
> 
> Regards,
> A. Benz
> 
> On 11/08/17 12:23, Amos Jeffries wrote:
>> On 08/11/17 12:18, A. Benz wrote:
>>> Hi all,
>>>
>>>
>>> ## Intro
>>>
>>> I read many blogs and emails on this list related to what I'm trying 
>>> to do, but most go into bumping or do things that are not as simple 
>>> as I'm trying to achieve.
>>>
>>> I have an extremely slow line, with very high latency in a remote 
>>> location. About 14 people are sharing this line. Nowadays with all 
>>> the mobile apps trying to sync and such, the line stalls to unusable 
>>> all the time.
>>>
>>> I tried doing filters with firewall or dns level, but those are not 
>>> effective. In the end I figured squid might be my best option.
>>>
>>> ## End intro
>>>
>>>
>>> I have squid 3.5.27 running under LEDE (OpenWrt fork), ie its 
>>> cross-compiled for a MIPS based SoC (mediatek mt7621). I mention this 
>>> because you will see some options in the config file that won't make 
>>> sense otherwise.
>>>
>>
>> NP: That should not be making much difference to the squid.conf 
>> settings. The worst limitations such devices impose are things that 
>> should be solved by OS settings outside of squid.conf. eg the 
>> cache.log going to a pipe for remote logging instead of a filename, 
>> and system-level FD limits.
>>
>>
>>> It works great, here's what I'm trying to achieve: Allow access only 
>>> to a pre-defined list of websites (whitelist). http is 
>>> straightforward, but if the connection is https all I need to know is 
>>> domain, if its allowed, let it pass, otherwise terminate.
>>>
>>> this setup is working as intended with the config attached below, 
>>> however the issue I'm facing is that some servers are "loadbalanced", 
>>> this would give me the forgery error, eg:
>>>
>>> "SECURITY ALERT: Host header forgery detected on...."
>>>
>>
>> The workarounds and gotcha's listed at 
>> <https://wiki.squid-cache.org/KnowledgeBase/HostHeaderForgery> are the 
>> best you can hope for there. The most successful all-round solution is 
>> to increase EDNS0 capabilities.
>>
>>
>>> Here's a specific example, there's a corporate domain for webmail 
>>> access, and some loadbalance config makes use of different IPs, I 
>>> think this is what triggers the error. My question is, can I just 
>>> ignore this error somehow and allow the connection? From what I 
>>> gather this connection is cut by squid before it reaches the client..
>>
>> Squid default behaviour is to allow the connection only to the same 
>> IP:port the client was connecting to. If that is not working your 
>> network configuration is screwed up. Specifically your routing or NAT.
>>
>> NAT of the dst-IP:port *MUST NOT* happen on any device between the 
>> client machine and the proxy machine. Squid needs access directly to 
>> the kernel NAT records of the device doing that NAT operation. So it 
>> can only happen on the Squid device.
>> ??You must *route* the packets unchanged to the Squid device (possibly 
>> over a tunnel if necessary).
>>
>>
>>>
>>> Also if there's anything else obviously wrong with my setup please 
>>> let me know.
>>>
>>> Many thanks.
>>>
>>>
>>> Here's my config:
>>>
>>>
>>> ### squid.conf begin
>>>
>>> acl localnet src 10.0.0.0/8
>>> acl localnet src 172.16.0.0/12
>>> acl localnet src 192.168.0.0/16
>>>
>>> acl ssl_ports port 443
>>>
>>> acl safe_ports port 80
>>> acl safe_ports port 443
>>> acl connect method connect
>>
>> NP: the default above ACL names are case-sensitive and some of them 
>> involve built-in default values which you are preventing having any 
>> effect by using custom lower-case ACL names.
>>
>>
>>> acl http_whitelist dstdomain "/etc/squid/whitelist.txt"
>>> acl https_whitelist ssl::server_name "/etc/squid/whitelist.txt"
>>> acl ips_whitelist dst "/etc/squid/ips.txt"
>>>
>>> http_port 3128 intercept
>>> http_port 3129
>>
>> Port 3128 is registered for forward-proxy traffic. Ideally you would 
>> have those lines reversed like so:
>>
>> ??http_port 3128
>> ??http_port 3129 intercept
>>
>> ... with the corresponding NAT change for the intercept port.
>>
>> Also, to have your SSL-Bump whitelists applied to forward-proxy 
>> CONNECT traffic you should have ssl-bump settings on that 3128 
>> forward-proxy port matching those on the port 3130 line.
>>
>>
>>>
>>> http_access deny !safe_ports
>>> http_access deny connect !ssl_ports
>>
>>> http_access allow ssl_ports
>>
>> Rather than allowing unlimited access to anyone on the Internet to use 
>> your limited bandwidth outbound connection for access to port 443 you 
>> should be using the localnet ACL that restricts use of the proxy to 
>> people on your LAN - those 14 clients you mentioned sharing the line.
>>
>> [NP: It is not possible in this setup to determine what remote users 
>> are abusing your proxy. All traffic logs from your firewall etc will 
>> show Squid as the client, not the remote [ab]user. Squid access.log 
>> records you are sending to /dev/null is the *only* record of such 
>> activities.]
>>
>>
>> To make your whitelists have any effect replace the above "allow 
>> ssl_ports" line with a "deny !localnet" line.
>>
>> If that change causes issues then your whitelists are incorrect / 
>> incomplete. You then need the (currently discarded) access.log and/or 
>> cache.log data to solve the issue properly.
>>
>>
>>> http_access allow http_whitelist
>>> http_access allow ips_whitelist
>>> http_access deny all
>>>
>>> https_port 3130 intercept ssl-bump \
>>> ?????cert=/etc/squid/myCA.pem \
>>> ?????generate-host-certificates=off dynamic_cert_mem_cache_size=4MB
>>>
>>> acl step1 at_step SslBump1
>>> acl step2 at_step SslBump2
>>> acl step3 at_step SslBump3
>>>
>>> ssl_bump peek step1 all
>>> ssl_bump splice https_whitelist
>>> ssl_bump splice ips_whitelist
>>> ssl_bump terminate all
>>>
>>
>> That seems fine. The problem is not part of this _config_. If you are 
>> having any SSL-Bump issues please try a build of the latest Squid-4. 
>> It may be related to bugs in Squid-3 SSL-Bump or modern TLS things 
>> Squid-3 cannot cope with - there is a growing list of those.
>>
>>>
>>> cache deny all
>>
>> In the latest Squid-3 use "store_miss deny all" instead of the above.
>>
>>> access_log none
>>
>> The above is fine if you are certain of the squid.conf working 100% 
>> properly. But since you are debugging issues you may need those 
>> transaction details.
>>
>> NP: access.log can be logged to syslog or a TCP pipe by Squid. To 
>> deliver the log content externally for normal audit purposes instead 
>> of using space on the device.
>>
>>> cache_log /dev/null
>>
>> You *need* the information logged here. By default only the most 
>> operationally critical errors are recorded.
>>
>> NP: the cache.log can usually be a Unix-pipe delivering data to a 
>> remote server if the local machine is constrained.
>>
>>> cache_store_log stdio:/dev/null
>>
>> Above line is *actively* harmful. The Squid-3 default is not to waste 
>> cycles logging *unless* you enter something like the above in 
>> squid.conf. The above makes Squid allocate device resources to logging 
>> that data to /dev/null.
>>
>>> logfile_rotate 0
>>>
>>> logfile_daemon /dev/null
>>
>> /dev/null is not a valid application filename.
>>
>> Build your Squid with --disable-logfile-daemon.
>>
>>> coredump_dir /tmp/squid
>>> visible_hostname main_Firewall
>>
>> The *visible* hostname is the domain delivered to clients and denied 
>> parties in the URLs to fetch error message data and FTP icons from 
>> Squid. It needs to be a valid FQDN.
>>
>> Amos
>> _______________________________________________
>> squid-users mailing list
>> squid-users at lists.squid-cache.org
>> http://lists.squid-cache.org/listinfo/squid-users
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users


From simon at baladia.gov.kw  Thu Nov  9 19:36:11 2017
From: simon at baladia.gov.kw (Simon Dcunha)
Date: Thu, 9 Nov 2017 22:36:11 +0300 (AST)
Subject: [squid-users] https access only for few users
In-Reply-To: <1011261745.134948.1510256142660.JavaMail.zimbra@baladia.gov.kw>
References: <1206568009.133991.1510214321125.JavaMail.zimbra@baladia.gov.kw>
 <118182430.134188.1510221797652.JavaMail.zimbra@baladia.gov.kw>
 <a8e3b1b9-de4e-d4c6-12b7-9f9bb87f4428@treenet.co.nz>
Message-ID: <725999985.134949.1510256171458.JavaMail.zimbra@baladia.gov.kw>

Dear Amos,

Million thanks for the immediate reply.

i will check it out and let you know

so sorry for being so silly

regards

simon

----- Original Message -----
From: "Amos Jeffries" <squid3 at treenet.co.nz>
To: squid-users at lists.squid-cache.org
Sent: Thursday, November 9, 2017 1:31:47 PM
Subject: Re: [squid-users] https access only for few users

On 09/11/17 23:03, Simon Dcunha wrote:
> 
> Dear All,
> 
> I have squid running with no issues for a long time and recently i have the below task.
> User access to internet is based on physical machine IP address acl so only user ips listed in the conf file have access to internet
> 
> i need to allow access to only one site for some users and deny everything else. its a https site
> the site is https://mof-sc-site.custhelp.com/
> 
> the users whos ips are listed in below acl should  access the above site only and nothing else.
> 
> so i have a access list as follows
> 
> ----------------------------------------------------
> acl onesite src 172.16.52.23 172.16.6.121
> acl allowed_site url_regex "/etc/squid/site"
> http_access allow onesite allowed_site
> http_access deny onesite
> ------------------------------------------------------
> 
> in /etc/squid/site i have
> ------------------
> .mof*
> 
> Now when I try to access the above site it says page cannot be displayed and in the squid access.log i see the below
> --------------
> 1510224319.009      0 172.16.6.121 TCP_DENIED/403 4201 CONNECT mof-sc-site.custhelp.com:443 - HIER_NONE/- text/html
> 
> 
> but if I try to access http://www.mof.gov.kw the home page is displayed and works fine
> 
> appreciate your advice and help
> 

You are a) using the wrong tool [regex] for the job of matching a single 
*domain*, and b) using regex VERY VERY badly.

Your regex says any URL in existence that contains _any_ single 
character followed by 'm' then 'o' is a match for the ACL - thus is 
allowed to the "onesite" client(s). The 'f' being optional (the *) and 
at the end of the pattern means it does not matter at all for the 
matching and may as well not exist.



What you should be doing is using an ACL type that matches domain names 
and telling it the domain that you want to match:

   acl allowed_site dstdomain mof-sc-site.custhelp.com

The rest of your config snippet was correct for what you want to do.

Amos
_______________________________________________
squid-users mailing list
squid-users at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-users

-- 
This message has been scanned for viruses and
dangerous content by MailScanner, and is
believed to be clean.
-- 
---------
Network Administrator
Kuwait Municipality!!!

-- 
This message has been scanned for viruses and
dangerous content by MailScanner, and is
believed to be clean.



From squid3 at treenet.co.nz  Thu Nov  9 23:30:56 2017
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Fri, 10 Nov 2017 12:30:56 +1300
Subject: [squid-users] Problem with squid and acces to localhost ipv6
In-Reply-To: <CAMtDEa3g-tZHp+zS30xd3qQYLccBXz4ZH658HBSZjgjViVpCKg@mail.gmail.com>
References: <CAMtDEa3g-tZHp+zS30xd3qQYLccBXz4ZH658HBSZjgjViVpCKg@mail.gmail.com>
Message-ID: <f8b85d41-7f3d-918f-2405-98c8873cb127@treenet.co.nz>

On 10/11/17 02:58, Nicol?s Herv? wrote:
> Hi, I have a problem with some users who use pgadmin v4, sometimes a 
> "connection to :: 1 failed" error appears when they enter http: // 
> localhost: 55501 / the system returns (111) connection refused. Why can 
> this be happening? and how can I solve it?


The details you have provide do not contain any useful clues about the 
'why' question.  You will have to figure that out for yourself.

Check what IP addresses the pgadmin service is listening on. Does it 
actually listen on *all* localhost IP addresses? or is it perhape only 
listening on 127.0.0.1 - which despite 'common knowledge' is not the 
only localhost address even in IPv4.

Amos


From squid3 at treenet.co.nz  Fri Nov 10 00:17:18 2017
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Fri, 10 Nov 2017 13:17:18 +1300
Subject: [squid-users] url_rewrite_program and ACLs
In-Reply-To: <2014705679.5324553.1510227544567@mail.yahoo.com>
References: <1990137371.4646637.1510150336187.ref@mail.yahoo.com>
 <1990137371.4646637.1510150336187@mail.yahoo.com>
 <921fbc5d-fce5-fd6f-b62e-8afd617994c7@treenet.co.nz>
 <131538788.5259356.1510213867661@mail.yahoo.com>
 <e89c27ac-b147-426e-e220-2c566b5e7586@treenet.co.nz>
 <2014705679.5324553.1510227544567@mail.yahoo.com>
Message-ID: <b0bd560b-081d-ec7b-261b-d92c1d479f98@treenet.co.nz>

On 10/11/17 00:39, Vieri wrote:
> 
> ________________________________
> From: Amos Jeffries <squid3 at treenet.co.nz>
>>
>> Darn. You have the one case that calls for keeping the helper :-(
>>
>> You can still move the ACLs that load in a reasonable times into
>> squid.conf and leave the others in SG/ufdbguard. Using
>> url_rewrite_access to restrict which transactions the helper gets
>> involved with. That will reduce its latency impact on lie traffic, but
>> still cause much the same memory related (non-)issues as now.
> 
> 
> That's exactly what I'm doing right now...
> Thanks.
> 
>> Running "squid -k shutdown" a _second_ time sends the running proxy a
>> signal to immediately skip to the processing as if the shutdown_lifetime
>> had already been reached.
> 
> 
> Thanks for that double-shutdown signal trick. I'll have to try that asap.
> 
> I'm making progress (sort of) on the FD (non-)issues I'm having.
> 
> I'll try to post back to Alex asap.
> 
> I have a custom perl script that does MySQL lookups for blacklisted sites (lots of them - so I can't use ACLs within squid.conf). I define that helper with external_acl_type.
> 
> Yesterday I changed my squid.conf by disabling this helper, and used squidGuard instead.
> I noticed a huge improvement.

Hmm that is suspicious. AFAIK SquidGuards' one remaining useful feature 
is how it loads large lists into memory in big chunks then processes 
after its technically already processing traffic from Squid - whereas 
Squid loads the files line by line, which is slower initially. Once 
loaded there is no difference in the lookup algorithms, and the SQL DB 
storage should be no different to how SG does it.

I would compare your custom script to the ext_sql_session_acl.pl.in 
script we bundle with current Squid.
  If yours lacks concurrency channel-ID I highly recommend adding that 
behaviour.
  If the DB is designed to store the protocol scheme, domain[:port] and 
path?query portions of URLs in separate columns it will be more 
efficient to pass those parameters as separate (%PROTO %DST %PORT %PATH) 
to the helper instead of just %URI.

The overheads in Squid of using external_acl_type helper interface 
should be slightly less than the url_rewrite_program for SG. The SQL DB 
data loading is about the same or better than what SG does AFAIK.


> 
> I took this snapshot yesterday:
> 
> 15:25 08/11/2017:
> 
> File descriptor usage for squid:
> Maximum number of file descriptors: 65536
> Largest file desc currently in use: 2730
> Number of file desc currently in use: 1838
> Files queued for open: 0
> Available number of file descriptors: 63698
> Reserved number of file descriptors: 100
> Store Disk files open: 0
> 
> Today I took another peak and found:
> 
> Thu Nov 9 12:19:05 CET 2017:
> 
> File descriptor usage for squid:
> Maximum number of file descriptors: 65536
> Largest file desc currently in use: 6980
> Number of file desc currently in use: 6627
> Files queued for open: 0
> Available number of file descriptors: 58909
> Reserved number of file descriptors: 100
> Store Disk files open: 0
> 
> The FDs are still increasing steadily, but a LOT less.
> 
> On the other hand, the "free" RAM went from 2GB yesterday to just 275MB today:
> 

Ouch, but kind of expected with those FD number increase. Each client 
connection will use about 3 FD, and two of them will use ~70KB each and 
the third will use some multiple of your avg object size.

Which reminds me ... Are you using SSL-Bump? if so ensure that you have 
configured "sslflags=NO_DEFAULT_CA" on the port lines. The default 
Trusted CA set can add a huge amount of useless memory to each client 
connection, which can add up to many GB quite quickly.


> # free --mega
> total used free shared buff/cache available
> Mem: 32865 8685 275 157 23904 23683
> Swap: 37036 286 36750
> 
> Used swap is still low enough (unchanged actually), so I guess I don't need to worry about it.

Nod, until the RAM runs out entirely, then problems are definitely to be 
expected and that sounds like it is your problem now.

FYI: The first side-effect of RAM swapping is that Squid starts slowing 
down on completed transactions - memory cache hits slow by 3-6 orders of 
magnitude when swapping in/out from disk, and any I/O buffers swapped 
out get 1+ speed penalties for the swap I/O time.
  That all leads to more active client connections overlapping their 
active periods (thus more FD usage total), and also clients start 
opening more connections to get better service from parallel fetches. So 
FD usage gets growth from two directions simultaneously.
  Which is all in a feedback loop since that extra memory pressure from 
more FD slows all transactions even further ... until the machine goes 
crunch. Maybe a literal crunch - I actually physically blew up a test 
machine (fire and smoke puring out the back!) measuring the effects of 
RAID on a overloaded proxy about a decade ago.


> 
> However, I'm bound to have issues when the "free" mem reaches 0... and I bet it will eventually.
> That's when the double-shutdown trick will kick in.
> 
> I'll review the perl helper code, or maybe just switch to ufdbGuard.
> 
> Thanks,
> 
> Vieri
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
> 


From squid3 at treenet.co.nz  Fri Nov 10 00:35:30 2017
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Fri, 10 Nov 2017 13:35:30 +1300
Subject: [squid-users] https access only for few users
In-Reply-To: <725999985.134949.1510256171458.JavaMail.zimbra@baladia.gov.kw>
References: <1206568009.133991.1510214321125.JavaMail.zimbra@baladia.gov.kw>
 <118182430.134188.1510221797652.JavaMail.zimbra@baladia.gov.kw>
 <a8e3b1b9-de4e-d4c6-12b7-9f9bb87f4428@treenet.co.nz>
 <725999985.134949.1510256171458.JavaMail.zimbra@baladia.gov.kw>
Message-ID: <44ef3017-5668-74d2-79be-d54a26ef7f69@treenet.co.nz>

On 10/11/17 08:36, Simon Dcunha wrote:
> Dear Amos,
> 
> Million thanks for the immediate reply.
> 
> i will check it out and let you know
> 
> so sorry for being so silly
> 
> regards
> 
> simon

You're Welcome, and no worries. Helping people learn to do things better 
is most of what this list is about.

Amos


From rentorbuy at yahoo.com  Fri Nov 10 12:01:13 2017
From: rentorbuy at yahoo.com (Vieri)
Date: Fri, 10 Nov 2017 12:01:13 +0000 (UTC)
Subject: [squid-users] url_rewrite_program and ACLs
In-Reply-To: <b0bd560b-081d-ec7b-261b-d92c1d479f98@treenet.co.nz>
References: <1990137371.4646637.1510150336187.ref@mail.yahoo.com>
 <1990137371.4646637.1510150336187@mail.yahoo.com>
 <921fbc5d-fce5-fd6f-b62e-8afd617994c7@treenet.co.nz>
 <131538788.5259356.1510213867661@mail.yahoo.com>
 <e89c27ac-b147-426e-e220-2c566b5e7586@treenet.co.nz>
 <2014705679.5324553.1510227544567@mail.yahoo.com>
 <b0bd560b-081d-ec7b-261b-d92c1d479f98@treenet.co.nz>
Message-ID: <122984048.190246.1510315273119@mail.yahoo.com>

BTW the docs say:

#         %PATH         Requested URL path
#         %METHOD       Request method
#         %MYADDR       Squid interface address
#         %MYPORT       Squid http_port number
#         %PATH         Requested URL-path (including query-string if any)


There's no such thing as %QUERY, right?
Squid cannot send "just the path" in one variable, and "just the query after ?" in another variable, right?
%PATH is the only thing I can read from to get both path and query (eg. "/some/path/?a=1&b=2")?

squid 3.5.27

Thanks,

Vieri


From ash.benz at bk.ru  Fri Nov 10 12:05:03 2017
From: ash.benz at bk.ru (A. Benz)
Date: Fri, 10 Nov 2017 20:05:03 +0800
Subject: [squid-users] Non intrusive sslbump for whitelisting (asked
 many times but..)
In-Reply-To: <7a55ca37-23d0-a0f3-6397-2a772f91745f@treenet.co.nz>
References: <ae8fe086-e227-6b58-c6d8-f4e37d062eee@bk.ru>
 <96e5ae3d-6dc6-fca3-7c70-b038ecf16ddf@treenet.co.nz>
 <75c1764d-1923-f6c0-1896-2c64d1190779@bk.ru>
 <7a55ca37-23d0-a0f3-6397-2a772f91745f@treenet.co.nz>
Message-ID: <4563ade7-9bdf-0237-bf8e-2ac6842bdc25@bk.ru>

Hi Amos,

Thanks for your continued support.

1.

> Do you mean the VPN exit point has that 10/8 IP address? or that the 
> traffic from the client is altered to be going to that IP before it 
> reaches Squid?
>
> The latter is broken because it destroys the original dst-IP values on 
> the TCP connection. Which Squid needs to setup the server connection. 

Let me put it as an example:

 From the normal internet: mail.amosprivateserver.org > publicly 
accessible IP.

 From my place: mail.amosprivateserver.org > 10.x.x.x (corporate 
network, accessible only from within the place).

Anyways no worries about this! I decided to make an exception in the 
redirect rule, so that if the outgoing traffic matches the IP 10.x.x.x 
then the firewall will not redirect the traffic to squid and instead 
establish a connection directly.

This is not ideal, but it works.


2.

> If for any reason those firewall rules change in unexpected ways or 
> don't block something you expect to be blocked this may leave a 
> security hole open. It does not seem to be necessary really, so best 
> to close. 

But if I put the lines you told me:

# http_access allow SSL_ports (commented out as you advised)
http_access deny !localnet

Then I can't get any https work per my whitelist (google for example 
complains of HSTS..etc) There must be another thing messed up in my 
config then?


3.

> The point I was trying to emphasize is that your Squid is accepting 
> *anything* in those port 443 connections. 
>
> HSTS requires a header "Strict-Transport-Security" to be delivered 
> from servers before it takes effect. You can erase that header from 
> replies going through Squid with the reply_header_access directive. 
> Current Squid should be doing it automatically.
> There may be issues with HSTS if the header is received before the 
> device connects to your network, or if it arrives over an uncontrolled 
> CONNECT tunnel. But there is not much to be done about those cases. 
I'm not sure I follow (please bare with me!), see I'm simply trying to 
prevent abuse by only allowing connections to whats in whitelist, is my 
approach in my current config doing that? If there's a better way please 
tell me. This is going over a satellite connection with extremely poor 
latency and bandwidth, so Squid's job is to stop the connections from 
reaching out, and only allowing whats in whitelist.


4.

> That means the server that HTTPS connection is attempting to reach is 
> not on your whitelist. It is therefore one of the things you wanted to 
> be blocked according to your stated policy.
>
> ?- as I mentioned earlier, if that causes any issues your whitelist is 
> incomplete. 
But it is, eg whitelist.txt has .google.com, shouldn't google.com be 
accessible then (without complaining about HSTS, since I'm splicing?). 
Note if I reverse the commented out lines, as below, then it works:

http_access allow SSL_ports
# http_access deny !localnet (now it works).


As I mentioned no worries about the forgery error as I'm simply adding 
an exception for that to circumvent squid, now I simply need to polish 
up my squid.conf to only allow whats in whitelist (while not consuming 
any unnecessary bandwidth).


Latest version below:

### begin squid.conf

acl localnet src 10.0.0.0/8
acl localnet src 172.16.0.0/12
acl localnet src 192.168.0.0/16

acl SSL_ports port 443

acl Safe_ports port 80
acl Safe_ports port 443
acl CONNECT method CONNECT
acl http_whitelist dstdomain "/etc/squid/whitelist.txt"
acl https_whitelist ssl::server_name "/etc/squid/whitelist.txt"
acl ips_whitelist dst "/etc/squid/ips.txt"


http_access deny !Safe_ports
http_access deny CONNECT !SSL_ports
http_access allow SSL_ports
# http_access deny !localnet
http_access allow http_whitelist
http_access allow ips_whitelist
http_access deny all

http_port 3128 ssl-bump \
 ????cert=/etc/squid/myCA.pem \
 ????generate-host-certificates=off dynamic_cert_mem_cache_size=4MB

https_port 3130 intercept ssl-bump \
 ????cert=/etc/squid/myCA.pem \
 ????generate-host-certificates=off dynamic_cert_mem_cache_size=4MB

acl step1 at_step SslBump1
acl step2 at_step SslBump2
acl step3 at_step SslBump3

ssl_bump peek step1 all
ssl_bump splice https_whitelist
ssl_bump splice ips_whitelist
ssl_bump terminate all


refresh_pattern ^ftp: 1440 20% 10080
refresh_pattern ^gopher: 1440 0% 1440
refresh_pattern -i (/cgi-bin/|\?) 0 0% 0
refresh_pattern . 0 20% 4320

store_miss deny all
cache_log /tmp/squid/squid.log
access_log /tmp/squid/access.log
logfile_rotate 0

logfile_daemon /usr/bin/logger
http_port 3129 intercept
coredump_dir /tmp/squid
visible_hostname LEDE.lan
pinger_enable off
mime_table /tmp/squid/mime.conf
sslcrtd_program /usr/lib/squid/ssl_crtd -s /tmp/squid/ssldb -M 4MB

## end config

Regards,
A. Benz

On 11/10/17 03:30, Amos Jeffries wrote:
> On 09/11/17 11:34, A. Benz wrote:
>> Hi Amos,
>>
>> Many thanks for your detailed reply.
>>
>> I have modified the config following your comments, before you see 
>> the new config (attached below), pls let me know your thoughts on the 
>> following:
>>
>> 1.
>>
>> ?> The workarounds and gotcha's listed at
>> ?> <https://wiki.squid-cache.org/KnowledgeBase/HostHeaderForgery> are 
>> the
>> ?> best you can hope for there. The most successful all-round 
>> solution is
>> ?> to increase EDNS0 capabilities.
>>
>> My particular case is a single server only, a corporate email server. 
>> This server is publicly accessible from internet (and has a valid 
>> signed SSL cert), now, on the remote location, there's a VPN setup 
>> that redirects access to the mail server to a private IP, eg 10.x.x.x 
>> (and this differs depending on loadbalance decision).
>
> Do you mean the VPN exit point has that 10/8 IP address? or that the 
> traffic from the client is altered to be going to that IP before it 
> reaches Squid?
>
> The latter is broken because it destroys the original dst-IP values on 
> the TCP connection. Which Squid needs to setup the server connection.
>
>
>>
>> Without squid, I can connect to webmail, but with squid I get the 
>> forgery error. Does the EDNS0 fix this? See its almost working 
>> exactly as I need now, except for access to this single domain.. so 
>> if there's a workaround (even if it requires a recompile) to ignore 
>> this single domain do let me know.
>>
>
> EDNS0 fixes problems with services that load balance by rotating the 
> IP addresses delivered in response to A/AAAA queries, possibly 
> omitting some records if the final few don't fit. That results in 
> Squid getting different IPs occasionally than the one the client is 
> using. EDNS0 extends the available DNS response packet size to fit all 
> the records so Squid can see them all even when a large set is rotating.
>
> There are a few major hosting providers that have that behaviour in 
> their DNS. If you have not hit it yet you are very lucky.
>
>
>>
>> 2.
>>
>> ?> NAT of the dst-IP:port *MUST NOT* happen on any device between the
>> ?> client machine and the proxy machine. Squid needs access directly 
>> to the
>> ?> kernel NAT records of the device doing that NAT operation. So it can
>> ?> only happen on the Squid device.
>> ?>?? You must *route* the packets unchanged to the Squid device 
>> (possibly
>> ?> over a tunnel if necessary).
>>
>> It happens on the same device (LEDE/OpenWrt router where squid is 
>> running), so the router is configured to intercept http (80) and 
>> https (443) traffic and redirect it to squid's ports:
>> 80 ---> 3129
>> 443 --> 3130
>>
>>
>> 3.
>>
>> ?> Rather than allowing unlimited access to anyone on the Internet to 
>> use
>> ?> your limited bandwidth outbound connection for access to port 443 you
>> ?> should be using the localnet ACL that restricts use of the proxy to
>> ?> people on your LAN - those 14 clients you mentioned sharing the line.
>> ?>
>> ?> [NP: It is not possible in this setup to determine what remote 
>> users are
>> ?> abusing your proxy. All traffic logs from your firewall etc will show
>> ?> Squid as the client, not the remote [ab]user. Squid access.log 
>> records
>> ?> you are sending to /dev/null is the *only* record of such 
>> activities.]
>> ?>
>> ?>
>>
>> I think I didn't word my earlier email properly, apologies for not 
>> being clear. No one from the internet has access to squid, the 
>> listening ports are not open to public, only accessible from LAN.
>
> If for any reason those firewall rules change in unexpected ways or 
> don't block something you expect to be blocked this may leave a 
> security hole open. It does not seem to be necessary really, so best 
> to close.
>
>
>>
>> With abuse I meant the 14 users.. you know nowadays with 
>> mobiles/tablets and all the apps and syncing, I only allow ports 443 
>> and 80 (and those are intercepted and forwarded to squid). All other 
>> ports are blocked.
>> The bandwidth available is extremely scarce and hence why I'm setting 
>> this up.
>
> The point I was trying to emphasize is that your Squid is accepting 
> *anything* in those port 443 connections.
>
>
>>
>> 4.
>>
>> ?> To make your whitelists have any effect replace the above "allow
>> ?> ssl_ports" line with a "deny !localnet" line.
>>
>> When I do this, it doesn't work anymore. I get "Your connection is 
>> not secure" from firefox, and since google has HSTS, I can't "ignore 
>> and proceed". The squid access log shows (not .google.com is in 
>> whitelist.txt):
>
> HSTS requires a header "Strict-Transport-Security" to be delivered 
> from servers before it takes effect. You can erase that header from 
> replies going through Squid with the reply_header_access directive. 
> Current Squid should be doing it automatically.
> There may be issues with HSTS if the header is received before the 
> device connects to your network, or if it arrives over an uncontrolled 
> CONNECT tunnel. But there is not much to be done about those cases.
>
>
>>
>> 1510180110.096????? 0 192.168.1.178 TCP_DENIED/200 0 CONNECT 
>> 108.177.14.103:443 - HIER_NONE/- -
>>
>> Once I switch back to "allow SSL_ports" I can connect (squid splices' 
>> the connection, no complaints from firefox).
>>
>
> That means the server that HTTPS connection is attempting to reach is 
> not on your whitelist. It is therefore one of the things you wanted to 
> be blocked according to your stated policy.
>
> ?- as I mentioned earlier, if that causes any issues your whitelist is 
> incomplete.
>
>
>>
>> 5.
>>
>> I followed your comments about the config changes: changed acls to 
>> match original config in upper case. Swapped? the port numbers, but 
>> about having my ssl-bump match on 3129, pls check my new one see if I 
>> did it right.
>>
>>
>> ## begin squid.conf
>>
>>
>> acl localnet src 10.0.0.0/8
>> acl localnet src 172.16.0.0/12
>> acl localnet src 192.168.0.0/16
>>
>> acl SSL_ports port 443
>>
>> acl Safe_ports port 80
>> acl Safe_ports port 443
>> acl CONNECT method CONNECT
>> acl http_whitelist dstdomain "/etc/squid/whitelist.txt"
>> acl https_whitelist ssl::server_name "/etc/squid/whitelist.txt"
>> acl ips_whitelist dst "/etc/squid/ips.txt"
>>
>>
>> http_access deny !Safe_ports
>> http_access deny CONNECT !SSL_ports
>> http_access allow SSL_ports
>> # http_access deny !localnet
>> http_access allow http_whitelist
>> http_access allow ips_whitelist
>> http_access deny all
>>
>> http_port 3128 ssl-bump \
>> ?????cert=/etc/squid/myCA.pem \
>> ?????generate-host-certificates=off dynamic_cert_mem_cache_size=4MB
>>
>> https_port 3130 intercept ssl-bump \
>> ?????cert=/etc/squid/myCA.pem \
>> ?????generate-host-certificates=off dynamic_cert_mem_cache_size=4MB
>>
>> acl step1 at_step SslBump1
>> acl step2 at_step SslBump2
>> acl step3 at_step SslBump3
>>
>> ssl_bump peek step1 all
>> ssl_bump splice https_whitelist
>> ssl_bump splice ips_whitelist
>> ssl_bump terminate all
>>
>>
>> refresh_pattern ^ftp: 1440 20% 10080
>> refresh_pattern ^gopher: 1440 0% 1440
>> refresh_pattern -i (/cgi-bin/|\?) 0 0% 0
>> refresh_pattern . 0 20% 4320
>>
>> store_miss deny all
>> cache_log /tmp/squid/squid.log
>> access_log /tmp/squid/access.log
>> logfile_rotate 0
>>
>> logfile_daemon /usr/bin/logger
>> http_port 3129 intercept
>> coredump_dir /tmp/squid
>> visible_hostname LEDE.lan
>> pinger_enable off
>> mime_table /tmp/squid/mime.conf
>> sslcrtd_program /usr/lib/squid/ssl_crtd -s /tmp/squid/ssldb -M 4MB
>>
>> ## end config
>>
>> Many thanks!
>>
>>
>> Regards,
>> A. Benz
>>
>> On 11/08/17 12:23, Amos Jeffries wrote:
>>> On 08/11/17 12:18, A. Benz wrote:
>>>> Hi all,
>>>>
>>>>
>>>> ## Intro
>>>>
>>>> I read many blogs and emails on this list related to what I'm 
>>>> trying to do, but most go into bumping or do things that are not as 
>>>> simple as I'm trying to achieve.
>>>>
>>>> I have an extremely slow line, with very high latency in a remote 
>>>> location. About 14 people are sharing this line. Nowadays with all 
>>>> the mobile apps trying to sync and such, the line stalls to 
>>>> unusable all the time.
>>>>
>>>> I tried doing filters with firewall or dns level, but those are not 
>>>> effective. In the end I figured squid might be my best option.
>>>>
>>>> ## End intro
>>>>
>>>>
>>>> I have squid 3.5.27 running under LEDE (OpenWrt fork), ie its 
>>>> cross-compiled for a MIPS based SoC (mediatek mt7621). I mention 
>>>> this because you will see some options in the config file that 
>>>> won't make sense otherwise.
>>>>
>>>
>>> NP: That should not be making much difference to the squid.conf 
>>> settings. The worst limitations such devices impose are things that 
>>> should be solved by OS settings outside of squid.conf. eg the 
>>> cache.log going to a pipe for remote logging instead of a filename, 
>>> and system-level FD limits.
>>>
>>>
>>>> It works great, here's what I'm trying to achieve: Allow access 
>>>> only to a pre-defined list of websites (whitelist). http is 
>>>> straightforward, but if the connection is https all I need to know 
>>>> is domain, if its allowed, let it pass, otherwise terminate.
>>>>
>>>> this setup is working as intended with the config attached below, 
>>>> however the issue I'm facing is that some servers are 
>>>> "loadbalanced", this would give me the forgery error, eg:
>>>>
>>>> "SECURITY ALERT: Host header forgery detected on...."
>>>>
>>>
>>> The workarounds and gotcha's listed at 
>>> <https://wiki.squid-cache.org/KnowledgeBase/HostHeaderForgery> are 
>>> the best you can hope for there. The most successful all-round 
>>> solution is to increase EDNS0 capabilities.
>>>
>>>
>>>> Here's a specific example, there's a corporate domain for webmail 
>>>> access, and some loadbalance config makes use of different IPs, I 
>>>> think this is what triggers the error. My question is, can I just 
>>>> ignore this error somehow and allow the connection? From what I 
>>>> gather this connection is cut by squid before it reaches the client..
>>>
>>> Squid default behaviour is to allow the connection only to the same 
>>> IP:port the client was connecting to. If that is not working your 
>>> network configuration is screwed up. Specifically your routing or NAT.
>>>
>>> NAT of the dst-IP:port *MUST NOT* happen on any device between the 
>>> client machine and the proxy machine. Squid needs access directly to 
>>> the kernel NAT records of the device doing that NAT operation. So it 
>>> can only happen on the Squid device.
>>> ??You must *route* the packets unchanged to the Squid device 
>>> (possibly over a tunnel if necessary).
>>>
>>>
>>>>
>>>> Also if there's anything else obviously wrong with my setup please 
>>>> let me know.
>>>>
>>>> Many thanks.
>>>>
>>>>
>>>> Here's my config:
>>>>
>>>>
>>>> ### squid.conf begin
>>>>
>>>> acl localnet src 10.0.0.0/8
>>>> acl localnet src 172.16.0.0/12
>>>> acl localnet src 192.168.0.0/16
>>>>
>>>> acl ssl_ports port 443
>>>>
>>>> acl safe_ports port 80
>>>> acl safe_ports port 443
>>>> acl connect method connect
>>>
>>> NP: the default above ACL names are case-sensitive and some of them 
>>> involve built-in default values which you are preventing having any 
>>> effect by using custom lower-case ACL names.
>>>
>>>
>>>> acl http_whitelist dstdomain "/etc/squid/whitelist.txt"
>>>> acl https_whitelist ssl::server_name "/etc/squid/whitelist.txt"
>>>> acl ips_whitelist dst "/etc/squid/ips.txt"
>>>>
>>>> http_port 3128 intercept
>>>> http_port 3129
>>>
>>> Port 3128 is registered for forward-proxy traffic. Ideally you would 
>>> have those lines reversed like so:
>>>
>>> ??http_port 3128
>>> ??http_port 3129 intercept
>>>
>>> ... with the corresponding NAT change for the intercept port.
>>>
>>> Also, to have your SSL-Bump whitelists applied to forward-proxy 
>>> CONNECT traffic you should have ssl-bump settings on that 3128 
>>> forward-proxy port matching those on the port 3130 line.
>>>
>>>
>>>>
>>>> http_access deny !safe_ports
>>>> http_access deny connect !ssl_ports
>>>
>>>> http_access allow ssl_ports
>>>
>>> Rather than allowing unlimited access to anyone on the Internet to 
>>> use your limited bandwidth outbound connection for access to port 
>>> 443 you should be using the localnet ACL that restricts use of the 
>>> proxy to people on your LAN - those 14 clients you mentioned sharing 
>>> the line.
>>>
>>> [NP: It is not possible in this setup to determine what remote users 
>>> are abusing your proxy. All traffic logs from your firewall etc will 
>>> show Squid as the client, not the remote [ab]user. Squid access.log 
>>> records you are sending to /dev/null is the *only* record of such 
>>> activities.]
>>>
>>>
>>> To make your whitelists have any effect replace the above "allow 
>>> ssl_ports" line with a "deny !localnet" line.
>>>
>>> If that change causes issues then your whitelists are incorrect / 
>>> incomplete. You then need the (currently discarded) access.log 
>>> and/or cache.log data to solve the issue properly.
>>>
>>>
>>>> http_access allow http_whitelist
>>>> http_access allow ips_whitelist
>>>> http_access deny all
>>>>
>>>> https_port 3130 intercept ssl-bump \
>>>> ?????cert=/etc/squid/myCA.pem \
>>>> ?????generate-host-certificates=off dynamic_cert_mem_cache_size=4MB
>>>>
>>>> acl step1 at_step SslBump1
>>>> acl step2 at_step SslBump2
>>>> acl step3 at_step SslBump3
>>>>
>>>> ssl_bump peek step1 all
>>>> ssl_bump splice https_whitelist
>>>> ssl_bump splice ips_whitelist
>>>> ssl_bump terminate all
>>>>
>>>
>>> That seems fine. The problem is not part of this _config_. If you 
>>> are having any SSL-Bump issues please try a build of the latest 
>>> Squid-4. It may be related to bugs in Squid-3 SSL-Bump or modern TLS 
>>> things Squid-3 cannot cope with - there is a growing list of those.
>>>
>>>>
>>>> cache deny all
>>>
>>> In the latest Squid-3 use "store_miss deny all" instead of the above.
>>>
>>>> access_log none
>>>
>>> The above is fine if you are certain of the squid.conf working 100% 
>>> properly. But since you are debugging issues you may need those 
>>> transaction details.
>>>
>>> NP: access.log can be logged to syslog or a TCP pipe by Squid. To 
>>> deliver the log content externally for normal audit purposes instead 
>>> of using space on the device.
>>>
>>>> cache_log /dev/null
>>>
>>> You *need* the information logged here. By default only the most 
>>> operationally critical errors are recorded.
>>>
>>> NP: the cache.log can usually be a Unix-pipe delivering data to a 
>>> remote server if the local machine is constrained.
>>>
>>>> cache_store_log stdio:/dev/null
>>>
>>> Above line is *actively* harmful. The Squid-3 default is not to 
>>> waste cycles logging *unless* you enter something like the above in 
>>> squid.conf. The above makes Squid allocate device resources to 
>>> logging that data to /dev/null.
>>>
>>>> logfile_rotate 0
>>>>
>>>> logfile_daemon /dev/null
>>>
>>> /dev/null is not a valid application filename.
>>>
>>> Build your Squid with --disable-logfile-daemon.
>>>
>>>> coredump_dir /tmp/squid
>>>> visible_hostname main_Firewall
>>>
>>> The *visible* hostname is the domain delivered to clients and denied 
>>> parties in the URLs to fetch error message data and FTP icons from 
>>> Squid. It needs to be a valid FQDN.
>>>
>>> Amos
>>> _______________________________________________
>>> squid-users mailing list
>>> squid-users at lists.squid-cache.org
>>> http://lists.squid-cache.org/listinfo/squid-users
>> _______________________________________________
>> squid-users mailing list
>> squid-users at lists.squid-cache.org
>> http://lists.squid-cache.org/listinfo/squid-users
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20171110/4573beb0/attachment.htm>

From dernikov1 at gmail.com  Fri Nov 10 13:16:04 2017
From: dernikov1 at gmail.com (Bike dernikov1)
Date: Fri, 10 Nov 2017 14:16:04 +0100
Subject: [squid-users] SQUID memory error after vm.swappines changed
 from 60 to 10
In-Reply-To: <bfb6f0fc-357b-2156-7cec-e43e9718d521@measurement-factory.com>
References: <CAAPixKhsif5+ad1prsoFAZXVXcVAxpdqkw4RdvGbfK-UxhkzTQ@mail.gmail.com>
 <86188b87-0cc3-343c-7b56-7f01354a3aa8@measurement-factory.com>
 <CAAPixKiTszJDHUDK__PFgQ_+g0kR0RX4KZUNykFANKnTqzydsg@mail.gmail.com>
 <bfb6f0fc-357b-2156-7cec-e43e9718d521@measurement-factory.com>
Message-ID: <CAAPixKhNB-1zkFP5X=b7f8__XEY0-9XtEGi8N0EkXTGB6kbygw@mail.gmail.com>

On Thu, Nov 9, 2017 at 4:03 PM, Alex Rousskov
<rousskov at measurement-factory.com> wrote:
> On 11/09/2017 06:49 AM, Bike dernikov1 wrote:
>>>> On 11/08/2017 02:26 AM, Bike dernikov1 wrote:
>
>> So why didn't swap hop in?
>
> As I implied earlier, I do not recommend researching swap-related
> details because I do not recommend using (or relying on) swap.
>
>>> 2. Removing swap (or at least not counting on it and treating
>>>  any non-trivial swap usage as a system misconfiguration).
>
>> What to search ? Key words, vm.swappines got us in problem, as we
>> tried to tune  ?

> I do not understand the question. I am sure you know how to turn swap
> off (i.e., completely disable it).

Question referenced at text inside  ( ).
So you suggest that we totally disable disk  swap  (or just for
debuging) ? That setting on production can be disaster.
We will try, we reduced number of auth helper, prolong ldap caching.
Little steps but at last every bit counts.

>> We have problems with squid.conf reconfiguration, because users lose
>> connection (they got squid error screen for moment) so it is nightmare
>> if we changing configuration in work time.
>
> Yes, this is a common complaint and a known Squid problem.
>
> http://wiki.squid-cache.org/SquidFaq/AboutSquid#How_to_add_a_new_Squid_feature.2C_enhance.2C_of_fix_something.3F
>
> Alex.

For now we collect requests, and push changes after hour work, so no
problems but we have delay in request, good enough.
Thanks for help.


From dernikov1 at gmail.com  Fri Nov 10 14:11:27 2017
From: dernikov1 at gmail.com (Bike dernikov1)
Date: Fri, 10 Nov 2017 15:11:27 +0100
Subject: [squid-users] SQUID memory error after vm.swappines changed
 from 60 to 10
In-Reply-To: <a7c29c43-131c-8f0b-fa66-88db16d0f8e5@urlfilterdb.com>
References: <CAAPixKhsif5+ad1prsoFAZXVXcVAxpdqkw4RdvGbfK-UxhkzTQ@mail.gmail.com>
 <dab0b62b-57a5-5718-b0bd-70fb3de0c124@urlfilterdb.com>
 <CAAPixKjepL2BEePSH83XeBNB+UPy4wKoqjTn0_Axoa+n0THo7w@mail.gmail.com>
 <daf2a3ca-d5bd-f84c-0182-099f845c4e61@urlfilterdb.com>
 <CAAPixKhCKhySLEZiUi1sA-hBwQ74KxM1XDN-jKEFL5B3=hfoeQ@mail.gmail.com>
 <a7c29c43-131c-8f0b-fa66-88db16d0f8e5@urlfilterdb.com>
Message-ID: <CAAPixKjhfEEXeoY7GHRCo=2Pmc3s0HqQkGPWTEH5LOYCfD3YCQ@mail.gmail.com>

On Thu, Nov 9, 2017 at 5:13 PM, Marcus Kool <marcus.kool at urlfilterdb.com> wrote:
>
>
> On 09/11/17 11:04, Bike dernikov1 wrote:
> [snip]
>>>>
>>>> Memory compsumption:squid use largest part of memory  (12GB now,
>>>> second proces use 300MB memory), 14GB used by all process. So squid
>>>> use over 80% of total used memory.
>>>> So no there are not any problematic process. But we changed swappiness
>>>> settings.
>>>
>>>
>>> Did you monitor Squid for growth (it can start with 12 GB and grow
>>> slowly) ?
>>
>>
>> Yes we are monitoring continuosly.
>> Now:
>> Output from free -m.
>>
>>             total       used    free   shared  buff/cache  available
>> Mem:  24101     20507  256    146      3337         3034
>> Swap: 24561      5040   19521
>>
>> vm.swappiness=40
>>
>> Memory by process:
>> squid  Virt       RES   SHR  MEM%
>>             22,9G  18.7   8164   79,6
>
>
> Hmm. Squid grew from 12 GB to 18.7 GB (23 GB virtual).

Today problem appeared again after logrotate at 2.56AM.
Used memory was at peek 23,7GB.
Before logrorate started, cached was at 2GB, buffer at 1,5GB.
After logrorate started cache jumped to 3.7GB and buffer unchanged at 1,5GB.

Fork errors stopped after 1 minute. At 2:57.
cache memory dropped by 500MB  to 3.2GB and continued at same level
till morning, buffer  same at 1.5GB.

After 4 at 3:00 minutes new WARNING appeared. external ACL queue
overload. Using stale results.

We have night shift and they told us that Internet worked ok.

After restart at around 7.00AM used memory dropped from 22 GB to 7GB,
cache and buffer remain at same levels.

> With vm.swappiness=40 Linux starts to page out parts of processes when they
> occupy more than 60% of the memory.
> This is a potential bottleneck and I would have also decreased vm.swappiness
> to 10 as you did.
>
> My guess is that Squid starts too many helpers in a short time frame and
> that because of paging there are too many forks in progress simultaneously
> which causes the memory exhaustion.

We are now testing with 100 helpers for negotiate_kerberos_auth.
vm.swappiness returned to 60.

> I suggest to reduce the memory cache of Squid by 50% and set vm.swappiness
> to 20.

Squid cache memory is set at 14GB reduced from 16GB from 20GB  in two turns.

> And then observe:
> - total memory use
> - total swap usage (should be lower than the 5 GB that you have now)
> - number of helper processes that are started in short time frames
> And then in small steps increase the memory cache and maybe further reduce
> vm.swappiness to 10.

If we survive with actual setup, we will continue with reducing as you suggest.
Last extreme will be swap disable swappof but just for test with 6
eyes on monitoring :)

>> squidguard two process  300MB boths,.
>>
>> CPU 0.33 0.37 0.43
>>
>>> Squid cannot fork and higher swappiness increases the amount of memory
>>> that
>>> the OS can use to copy processes.
>>> It makes me think that you have the memory overcommit set to 2 (no
>>> overcommit).
>>> What is the output of the following command ?
>>>     sysctl  -a | grep overcommit
>>
>>
>> Command output:
>>
>> vm.nr_overcommit_hugepages = 0
>> vm.overcommit_kbytes = 0
>> vm.overcommit_memory = 0
>> vm.overcommit_ratio = 50
>>
>> cat /proc/sys/vm/overcommit_memory
>> 0
>
>
> The overcommit settings look fine.

At least something right :)

>>
>>>> Advice for some settings:
>>>> We have absolute max peak of  2500 users which user squid (of 2800),
>>>> what are recomended settings for:
>>>> negotiate_kerberos_children start/idle
>>>> squidguard helpers.
>>>
>>>
>>>
>>> I have little experience with kerberos, but most likely this is not the
>>> issue.
>>> When Squid cannot fork the helpers, helper settings do not matter much.
>>
>>
>>> For 2500 users you probably need 32-64 squidguard helpers.
>>
>>
>> Can you confirm: For 2500 users:
>>
>> url_rewrite children X (squidguard)  32-64 will be ok ? We have set
>> much larger number.

Squidguard url_rewrite children was set to 64.

> Did I understand it correctly that earlier in this reply you said that there
> are two squidguard processes (300 MB each).

Yes (first two process in htop, two rewrite childrens) others was on 0.0%.

> ufdbGuard is faster than squidGuard and has multithreaded helpers.
> ufdbGuard needs less helpers than squidGuard.
> If you have a much larger number than 64 url rewrite helpers than I suggest
> to switch to ufdbGuard as soon as possible since the memory usage is then at
> least 600% less.

UfdbGuard have few strong features. Development, kerberos,
concurency/multitreading.
As i wrote, if we read documentation slower we wouldn't
Do ufdbGuard supoort ldap secure auth ? We tried ldap secure with
squidguard without success.

>> For  helper:
>> negotitate_kerberos_auth
>>
>> auth_param negotiate children X startup Y idle Z. What X, Y, Z are
>> best for our user number ?
>>
>> We disabled kerberos replay cache because of disk performance (4 SAS
>> DISK  15K, RAID 10) (iowait jumped high, and CPU load jumped to min
>> 40 max 200).
>> We don't use disk caching.
>>
>> Thanks for help,
>>
>>> Marcus
>>>
>>>
>>>> Thanks for help,
>>>>
>>>> On Wed, Nov 8, 2017 at 10:53 AM, Marcus Kool
>>>> <marcus.kool at urlfilterdb.com> wrote:
>>>>>
>>>>>
>>>>> There is definitely a problem with available memory because Squid
>>>>> cannot
>>>>> fork.
>>>>> So start with looking at how much memory Squid and its helpers use.
>>>>> Do do have other processes on this system that consume a lot of memory
>>>>> ?
>>>>>
>>>>> Also note that ufdbGuard uses less memory that squidGuard.
>>>>> If there are 30 helpers squidguard uses 300% more memory than
>>>>> ufdbGuard.
>>>>>
>>>>> Look at the wiki for more information about memory usage:
>>>>> https://wiki.squid-cache.org/SquidFaq/SquidMemory   (currently has an
>>>>> expired certificate but it is safe to go ahead)
>>>>>
>>>>> Marcus
>>>>>
>>>>>
>>>>>
>>>>> On 08/11/17 07:26, Bike dernikov1 wrote:
>>>>>>
>>>>>>
>>>>>>
>>>>>> Hi, I hope that someone can explain what happened, why squid stopped
>>>>>> working.
>>>>>> The problem is related to  memory/swap handling.
>>>>>>
>>>>>> After we changed vm.swappiness parameter from 60 to 10 (tuning
>>>>>> attempt, to lower a disk usage, because we have only 4 disks in a
>>>>>> RAID10, so disk subsystem  is a weak link), we got a lot of errors in
>>>>>> cache.log.
>>>>>> The problems started after scheduled logrotate after  2AM.
>>>>>> Squid ran out of memory, auth helpers stopped working.
>>>>>> It's weird because we didn't disable swap, but behavior is like we
>>>>>> did.
>>>>>> After an error, we increased parameter from 10 to 40.
>>>>>>
>>>>>> The server has 24GB DDR3 memory,  disk swap set to 24GB, 12 CPU (24HT
>>>>>> cores).
>>>>>> We have 2800 users, using  kerberos authentication, squidguard for
>>>>>> filtering, ldap authorization.
>>>>>> When problem appeared memory was still 3GB free (free column), ram
>>>>>> (caching) was filled to 15GB, so 21 GB ram filled, 3GB free.
>>>>>>
>>>>>> Thanks for help,
>>>>>>
>>>>>>
>>>>>> errors from cache.log.
>>>>>>
>>>>>> 2017/11/08 02:55:27| Set Current Directory to /var/log/squid/
>>>>>> 2017/11/08 02:55:27 kid1| storeDirWriteCleanLogs: Starting...
>>>>>> 2017/11/08 02:55:27 kid1|   Finished.  Wrote 0 entries.
>>>>>> 2017/11/08 02:55:27 kid1|   Took 0.00 seconds (  0.00 entries/sec).
>>>>>> 2017/11/08 02:55:27 kid1| logfileRotate:
>>>>>> daemon:/var/log/squid/access.log
>>>>>> 2017/11/08 02:55:27 kid1| logfileRotate:
>>>>>> daemon:/var/log/squid/access.log
>>>>>> 2017/11/08 02:55:28 kid1| Pinger socket opened on FD 30
>>>>>> 2017/11/08 02:55:28 kid1| helperOpenServers: Starting 1/1000
>>>>>> 'squidGuard' processes
>>>>>> 2017/11/08 02:55:28 kid1| ipcCreate: fork: (12) Cannot allocate memory
>>>>>> 2017/11/08 02:55:28 kid1| WARNING: Cannot run '/usr/bin/squidGuard'
>>>>>> process.
>>>>>> 2017/11/08 02:55:28 kid1| helperOpenServers: Starting 300/3000
>>>>>> 'negotiate_kerberos_auth' processes
>>>>>> 2017/11/08 02:55:28 kid1| ipcCreate: fork: (12) Cannot allocate memory
>>>>>> 2017/11/08 02:55:28 kid1| WARNING: Cannot run
>>>>>> '/usr/lib/squid/negotiate_kerberos_auth' process.
>>>>>> 2017/11/08 02:55:28 kid1| ipcCreate: fork: (12) Cannot allocate memory
>>>>>> 2017/11/08 02:55:28 kid1| WARNING: Cannot run
>>>>>> '/usr/lib/squid/negotiate_kerberos_auth' process.
>>>>>> 2017/11/08 02:55:28 kid1| ipcCreate: fork: (12) Cannot allocate memory
>>>>>> 2017/11/08 02:55:28 kid1| WARNING: Cannot run
>>>>>> '/usr/lib/squid/negotiate_kerberos_auth' process.
>>>>>>
>>>>>> external ACL 'memberof' queue overload. Using stale result.
>>>>>> _______________________________________________
>>>>>> squid-users mailing list
>>>>>> squid-users at lists.squid-cache.org
>>>>>> http://lists.squid-cache.org/listinfo/squid-users
>>>>>>
>>>>> _______________________________________________
>>>>> squid-users mailing list
>>>>> squid-users at lists.squid-cache.org
>>>>> http://lists.squid-cache.org/listinfo/squid-users
>>>>
>>>>
>>>>
>>>>
>>> _______________________________________________
>>> squid-users mailing list
>>> squid-users at lists.squid-cache.org
>>> http://lists.squid-cache.org/listinfo/squid-users
>>
>>
>>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users


From rousskov at measurement-factory.com  Fri Nov 10 15:43:08 2017
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Fri, 10 Nov 2017 08:43:08 -0700
Subject: [squid-users] SQUID memory error after vm.swappines changed
 from 60 to 10
In-Reply-To: <CAAPixKhNB-1zkFP5X=b7f8__XEY0-9XtEGi8N0EkXTGB6kbygw@mail.gmail.com>
References: <CAAPixKhsif5+ad1prsoFAZXVXcVAxpdqkw4RdvGbfK-UxhkzTQ@mail.gmail.com>
 <86188b87-0cc3-343c-7b56-7f01354a3aa8@measurement-factory.com>
 <CAAPixKiTszJDHUDK__PFgQ_+g0kR0RX4KZUNykFANKnTqzydsg@mail.gmail.com>
 <bfb6f0fc-357b-2156-7cec-e43e9718d521@measurement-factory.com>
 <CAAPixKhNB-1zkFP5X=b7f8__XEY0-9XtEGi8N0EkXTGB6kbygw@mail.gmail.com>
Message-ID: <bd5c2853-4480-4808-d1d4-9d8b1a8f6b0d@measurement-factory.com>

On 11/10/2017 06:16 AM, Bike dernikov1 wrote:

> So you suggest that we totally disable disk swap (or just for
> debuging) ? 

I would aim for totally disabling disk swap, but getting to that
configuration is easier if you keep swap enabled and consider any
non-trivial swap use as a problem that you need to fix. After all
non-trivial swap uses are eliminated, it does not really matter much
whether you keep swap enabled or not!

Please note that this suggestion is specific to performance-sensitive
Squid servers -- many other servers have very legitimate reasons to use
swap. YMMV.


> That setting on production can be disaster.

Squid swapping in production is an arguably worse disaster, as you have
learned. In many cases, it is better to deal with a lack of swap than to
rely on swap's magical effects that most humans poorly understand. YMMV.


> We will try, we reduced number of auth helper, prolong ldap caching.
> Little steps but at last every bit counts.

... unless those bits are actually hurting. You may need to understand
your server memory usage better to reduce the time spent on trying
random changes. For example, if auth helpers are close to being
overloaded, reducing their number may make things worse.

Alex.


From marcus.kool at urlfilterdb.com  Fri Nov 10 22:11:47 2017
From: marcus.kool at urlfilterdb.com (Marcus Kool)
Date: Fri, 10 Nov 2017 20:11:47 -0200
Subject: [squid-users] SQUID memory error after vm.swappines changed
 from 60 to 10
In-Reply-To: <CAAPixKjhfEEXeoY7GHRCo=2Pmc3s0HqQkGPWTEH5LOYCfD3YCQ@mail.gmail.com>
References: <CAAPixKhsif5+ad1prsoFAZXVXcVAxpdqkw4RdvGbfK-UxhkzTQ@mail.gmail.com>
 <dab0b62b-57a5-5718-b0bd-70fb3de0c124@urlfilterdb.com>
 <CAAPixKjepL2BEePSH83XeBNB+UPy4wKoqjTn0_Axoa+n0THo7w@mail.gmail.com>
 <daf2a3ca-d5bd-f84c-0182-099f845c4e61@urlfilterdb.com>
 <CAAPixKhCKhySLEZiUi1sA-hBwQ74KxM1XDN-jKEFL5B3=hfoeQ@mail.gmail.com>
 <a7c29c43-131c-8f0b-fa66-88db16d0f8e5@urlfilterdb.com>
 <CAAPixKjhfEEXeoY7GHRCo=2Pmc3s0HqQkGPWTEH5LOYCfD3YCQ@mail.gmail.com>
Message-ID: <6c7dbab4-fe9c-c543-7afa-9cea677c1de7@urlfilterdb.com>



On 10/11/17 12:11, Bike dernikov1 wrote:
> On Thu, Nov 9, 2017 at 5:13 PM, Marcus Kool <marcus.kool at urlfilterdb.com> wrote:
>>
>>
>> On 09/11/17 11:04, Bike dernikov1 wrote:
>> [snip]
>>>>>
>>>>> Memory compsumption:squid use largest part of memory  (12GB now,
>>>>> second proces use 300MB memory), 14GB used by all process. So squid
>>>>> use over 80% of total used memory.
>>>>> So no there are not any problematic process. But we changed swappiness
>>>>> settings.
>>>>
>>>>
>>>> Did you monitor Squid for growth (it can start with 12 GB and grow
>>>> slowly) ?
>>>
>>>
>>> Yes we are monitoring continuosly.
>>> Now:
>>> Output from free -m.
>>>
>>>              total       used    free   shared  buff/cache  available
>>> Mem:  24101     20507  256    146      3337         3034
>>> Swap: 24561      5040   19521
>>>
>>> vm.swappiness=40
>>>
>>> Memory by process:
>>> squid  Virt       RES   SHR  MEM%
>>>              22,9G  18.7   8164   79,6
>>
>>
>> Hmm. Squid grew from 12 GB to 18.7 GB (23 GB virtual).
> 
> Today problem appeared again after logrotate at 2.56AM.
> Used memory was at peek 23,7GB.

ok. it is clear that Squid grows too much.
On a 24GB system with many helpers and a URL filter I think the maximum size should be 14GB.

> Before logrorate started, cached was at 2GB, buffer at 1,5GB.
> After logrorate started cache jumped to 3.7GB and buffer unchanged at 1,5GB.
> 
> Fork errors stopped after 1 minute. At 2:57.
> cache memory dropped by 500MB  to 3.2GB and continued at same level
> till morning, buffer  same at 1.5GB.
> 
> After 4 at 3:00 minutes new WARNING appeared. external ACL queue
> overload. Using stale results.
> 
> We have night shift and they told us that Internet worked ok.
> 
> After restart at around 7.00AM used memory dropped from 22 GB to 7GB,
> cache and buffer remain at same levels.

How come Squid uses 7 GB at startup when there is no disk cache ?

>> With vm.swappiness=40 Linux starts to page out parts of processes when they
>> occupy more than 60% of the memory.
>> This is a potential bottleneck and I would have also decreased vm.swappiness
>> to 10 as you did.
>>
>> My guess is that Squid starts too many helpers in a short time frame and
>> that because of paging there are too many forks in progress simultaneously
>> which causes the memory exhaustion.
> 
> We are now testing with 100 helpers for negotiate_kerberos_auth.
> vm.swappiness returned to 60.
> 
>> I suggest to reduce the memory cache of Squid by 50% and set vm.swappiness
>> to 20.
> 
> Squid cache memory is set at 14GB reduced from 16GB from 20GB  in two turns.

are you saying that you have
    cache_mem 14G
If yes, you should read the memory FAQ and reduce this.
'cache_mem 14G' explains that Squid starts 'small' and grows over time.

>> And then observe:
>> - total memory use
>> - total swap usage (should be lower than the 5 GB that you have now)
>> - number of helper processes that are started in short time frames
>> And then in small steps increase the memory cache and maybe further reduce
>> vm.swappiness to 10.
> 
> If we survive with actual setup, we will continue with reducing as you suggest.
> Last extreme will be swap disable swappof but just for test with 6
> eyes on monitoring :)
> 
>>> squidguard two process  300MB boths,.
>>>
>>> CPU 0.33 0.37 0.43
>>>
>>>> Squid cannot fork and higher swappiness increases the amount of memory
>>>> that
>>>> the OS can use to copy processes.
>>>> It makes me think that you have the memory overcommit set to 2 (no
>>>> overcommit).
>>>> What is the output of the following command ?
>>>>      sysctl  -a | grep overcommit
>>>
>>>
>>> Command output:
>>>
>>> vm.nr_overcommit_hugepages = 0
>>> vm.overcommit_kbytes = 0
>>> vm.overcommit_memory = 0
>>> vm.overcommit_ratio = 50
>>>
>>> cat /proc/sys/vm/overcommit_memory
>>> 0
>>
>>
>> The overcommit settings look fine.
> 
> At least something right :)
> 
>>>
>>>>> Advice for some settings:
>>>>> We have absolute max peak of  2500 users which user squid (of 2800),
>>>>> what are recomended settings for:
>>>>> negotiate_kerberos_children start/idle
>>>>> squidguard helpers.
>>>>
>>>>
>>>>
>>>> I have little experience with kerberos, but most likely this is not the
>>>> issue.
>>>> When Squid cannot fork the helpers, helper settings do not matter much.
>>>
>>>
>>>> For 2500 users you probably need 32-64 squidguard helpers.
>>>
>>>
>>> Can you confirm: For 2500 users:
>>>
>>> url_rewrite children X (squidguard)  32-64 will be ok ? We have set
>>> much larger number.
> 
> Squidguard url_rewrite children was set to 64.
> 
>> Did I understand it correctly that earlier in this reply you said that there
>> are two squidguard processes (300 MB each).
> 
> Yes (first two process in htop, two rewrite childrens) others was on 0.0%.
> 
>> ufdbGuard is faster than squidGuard and has multithreaded helpers.
>> ufdbGuard needs less helpers than squidGuard.
>> If you have a much larger number than 64 url rewrite helpers than I suggest
>> to switch to ufdbGuard as soon as possible since the memory usage is then at
>> least 600% less.
> 
> UfdbGuard have few strong features. Development, kerberos,
> concurency/multitreading.
> As i wrote, if we read documentation slower we wouldn't
> Do ufdbGuard supoort ldap secure auth ? We tried ldap secure with
> squidguard without success.

ufdbGuard supports any user database with the "execuserlist" feature.
See the Reference Manual for details.

>>> For  helper:
>>> negotitate_kerberos_auth
>>>
>>> auth_param negotiate children X startup Y idle Z. What X, Y, Z are
>>> best for our user number ?
>>>
>>> We disabled kerberos replay cache because of disk performance (4 SAS
>>> DISK  15K, RAID 10) (iowait jumped high, and CPU load jumped to min
>>> 40 max 200).
>>> We don't use disk caching.
>>>
>>> Thanks for help,
>>>
>>>> Marcus
>>>>
>>>>
>>>>> Thanks for help,
>>>>>
>>>>> On Wed, Nov 8, 2017 at 10:53 AM, Marcus Kool
>>>>> <marcus.kool at urlfilterdb.com> wrote:
>>>>>>
>>>>>>
>>>>>> There is definitely a problem with available memory because Squid
>>>>>> cannot
>>>>>> fork.
>>>>>> So start with looking at how much memory Squid and its helpers use.
>>>>>> Do do have other processes on this system that consume a lot of memory
>>>>>> ?
>>>>>>
>>>>>> Also note that ufdbGuard uses less memory that squidGuard.
>>>>>> If there are 30 helpers squidguard uses 300% more memory than
>>>>>> ufdbGuard.
>>>>>>
>>>>>> Look at the wiki for more information about memory usage:
>>>>>> https://wiki.squid-cache.org/SquidFaq/SquidMemory   (currently has an
>>>>>> expired certificate but it is safe to go ahead)
>>>>>>
>>>>>> Marcus
>>>>>>
>>>>>>
>>>>>>
>>>>>> On 08/11/17 07:26, Bike dernikov1 wrote:
>>>>>>>
>>>>>>>
>>>>>>>
>>>>>>> Hi, I hope that someone can explain what happened, why squid stopped
>>>>>>> working.
>>>>>>> The problem is related to  memory/swap handling.
>>>>>>>
>>>>>>> After we changed vm.swappiness parameter from 60 to 10 (tuning
>>>>>>> attempt, to lower a disk usage, because we have only 4 disks in a
>>>>>>> RAID10, so disk subsystem  is a weak link), we got a lot of errors in
>>>>>>> cache.log.
>>>>>>> The problems started after scheduled logrotate after  2AM.
>>>>>>> Squid ran out of memory, auth helpers stopped working.
>>>>>>> It's weird because we didn't disable swap, but behavior is like we
>>>>>>> did.
>>>>>>> After an error, we increased parameter from 10 to 40.
>>>>>>>
>>>>>>> The server has 24GB DDR3 memory,  disk swap set to 24GB, 12 CPU (24HT
>>>>>>> cores).
>>>>>>> We have 2800 users, using  kerberos authentication, squidguard for
>>>>>>> filtering, ldap authorization.
>>>>>>> When problem appeared memory was still 3GB free (free column), ram
>>>>>>> (caching) was filled to 15GB, so 21 GB ram filled, 3GB free.
>>>>>>>
>>>>>>> Thanks for help,
>>>>>>>
>>>>>>>
>>>>>>> errors from cache.log.
>>>>>>>
>>>>>>> 2017/11/08 02:55:27| Set Current Directory to /var/log/squid/
>>>>>>> 2017/11/08 02:55:27 kid1| storeDirWriteCleanLogs: Starting...
>>>>>>> 2017/11/08 02:55:27 kid1|   Finished.  Wrote 0 entries.
>>>>>>> 2017/11/08 02:55:27 kid1|   Took 0.00 seconds (  0.00 entries/sec).
>>>>>>> 2017/11/08 02:55:27 kid1| logfileRotate:
>>>>>>> daemon:/var/log/squid/access.log
>>>>>>> 2017/11/08 02:55:27 kid1| logfileRotate:
>>>>>>> daemon:/var/log/squid/access.log
>>>>>>> 2017/11/08 02:55:28 kid1| Pinger socket opened on FD 30
>>>>>>> 2017/11/08 02:55:28 kid1| helperOpenServers: Starting 1/1000
>>>>>>> 'squidGuard' processes
>>>>>>> 2017/11/08 02:55:28 kid1| ipcCreate: fork: (12) Cannot allocate memory
>>>>>>> 2017/11/08 02:55:28 kid1| WARNING: Cannot run '/usr/bin/squidGuard'
>>>>>>> process.
>>>>>>> 2017/11/08 02:55:28 kid1| helperOpenServers: Starting 300/3000
>>>>>>> 'negotiate_kerberos_auth' processes
>>>>>>> 2017/11/08 02:55:28 kid1| ipcCreate: fork: (12) Cannot allocate memory
>>>>>>> 2017/11/08 02:55:28 kid1| WARNING: Cannot run
>>>>>>> '/usr/lib/squid/negotiate_kerberos_auth' process.
>>>>>>> 2017/11/08 02:55:28 kid1| ipcCreate: fork: (12) Cannot allocate memory
>>>>>>> 2017/11/08 02:55:28 kid1| WARNING: Cannot run
>>>>>>> '/usr/lib/squid/negotiate_kerberos_auth' process.
>>>>>>> 2017/11/08 02:55:28 kid1| ipcCreate: fork: (12) Cannot allocate memory
>>>>>>> 2017/11/08 02:55:28 kid1| WARNING: Cannot run
>>>>>>> '/usr/lib/squid/negotiate_kerberos_auth' process.
>>>>>>>
>>>>>>> external ACL 'memberof' queue overload. Using stale result.
>>>>>>> _______________________________________________
>>>>>>> squid-users mailing list
>>>>>>> squid-users at lists.squid-cache.org
>>>>>>> http://lists.squid-cache.org/listinfo/squid-users
>>>>>>>
>>>>>> _______________________________________________
>>>>>> squid-users mailing list
>>>>>> squid-users at lists.squid-cache.org
>>>>>> http://lists.squid-cache.org/listinfo/squid-users
>>>>>
>>>>>
>>>>>
>>>>>
>>>> _______________________________________________
>>>> squid-users mailing list
>>>> squid-users at lists.squid-cache.org
>>>> http://lists.squid-cache.org/listinfo/squid-users
>>>
>>>
>>>
>> _______________________________________________
>> squid-users mailing list
>> squid-users at lists.squid-cache.org
>> http://lists.squid-cache.org/listinfo/squid-users
> 
> 


From squid3 at treenet.co.nz  Sat Nov 11 00:20:56 2017
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sat, 11 Nov 2017 13:20:56 +1300
Subject: [squid-users] url_rewrite_program and ACLs
In-Reply-To: <122984048.190246.1510315273119@mail.yahoo.com>
References: <1990137371.4646637.1510150336187.ref@mail.yahoo.com>
 <1990137371.4646637.1510150336187@mail.yahoo.com>
 <921fbc5d-fce5-fd6f-b62e-8afd617994c7@treenet.co.nz>
 <131538788.5259356.1510213867661@mail.yahoo.com>
 <e89c27ac-b147-426e-e220-2c566b5e7586@treenet.co.nz>
 <2014705679.5324553.1510227544567@mail.yahoo.com>
 <b0bd560b-081d-ec7b-261b-d92c1d479f98@treenet.co.nz>
 <122984048.190246.1510315273119@mail.yahoo.com>
Message-ID: <6402db8c-511c-9015-4a31-1f56610adcc3@treenet.co.nz>

On 11/11/17 01:01, Vieri wrote:
> BTW the docs say:
> 
> #         %PATH         Requested URL path
> #         %METHOD       Request method
> #         %MYADDR       Squid interface address
> #         %MYPORT       Squid http_port number
> #         %PATH         Requested URL-path (including query-string if any)
> 
> 
> There's no such thing as %QUERY, right?
> Squid cannot send "just the path" in one variable, and "just the query after ?" in another variable, right?
> %PATH is the only thing I can read from to get both path and query (eg. "/some/path/?a=1&b=2")?
> 

Correct.

Amos


From squid3 at treenet.co.nz  Sat Nov 11 01:03:27 2017
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sat, 11 Nov 2017 14:03:27 +1300
Subject: [squid-users] Non intrusive sslbump for whitelisting (asked
 many times but..)
In-Reply-To: <4563ade7-9bdf-0237-bf8e-2ac6842bdc25@bk.ru>
References: <ae8fe086-e227-6b58-c6d8-f4e37d062eee@bk.ru>
 <96e5ae3d-6dc6-fca3-7c70-b038ecf16ddf@treenet.co.nz>
 <75c1764d-1923-f6c0-1896-2c64d1190779@bk.ru>
 <7a55ca37-23d0-a0f3-6397-2a772f91745f@treenet.co.nz>
 <4563ade7-9bdf-0237-bf8e-2ac6842bdc25@bk.ru>
Message-ID: <0de1efc3-3587-1584-0d32-735974f39600@treenet.co.nz>

On 11/11/17 01:05, A. Benz wrote:
> Hi Amos,
> 
> Thanks for your continued support.
> 
> 1.
> 
>> Do you mean the VPN exit point has that 10/8 IP address? or that the 
>> traffic from the client is altered to be going to that IP before it 
>> reaches Squid?
>>
>> The latter is broken because it destroys the original dst-IP values on 
>> the TCP connection. Which Squid needs to setup the server connection. 
> 
> Let me put it as an example:
> 
>  From the normal internet: mail.amosprivateserver.org > publicly 
> accessible IP.
> 
>  From my place: mail.amosprivateserver.org > 10.x.x.x (corporate 
> network, accessible only from within the place).
> 
> Anyways no worries about this! I decided to make an exception in the 
> redirect rule, so that if the outgoing traffic matches the IP 10.x.x.x 
> then the firewall will not redirect the traffic to squid and instead 
> establish a connection directly.
> 
> This is not ideal, but it works.
> 

Or have Squid relay everything through the same server(s) and
the server do the distinguishing between traffic and just relay 
everythign to the same


> 
> 2.
> 
>> If for any reason those firewall rules change in unexpected ways or 
>> don't block something you expect to be blocked this may leave a 
>> security hole open. It does not seem to be necessary really, so best 
>> to close. 
> 
> But if I put the lines you told me:
> 
> # http_access allow SSL_ports (commented out as you advised)
> http_access deny !localnet
> 
> Then I can't get any https work per my whitelist (google for example 
> complains of HSTS..etc) There must be another thing messed up in my 
> config then?
> 

Seems so.

> 
> 3.
> 
>> The point I was trying to emphasize is that your Squid is accepting 
>> *anything* in those port 443 connections. 
>>
>> HSTS requires a header "Strict-Transport-Security" to be delivered 
>> from servers before it takes effect. You can erase that header from 
>> replies going through Squid with the reply_header_access directive. 
>> Current Squid should be doing it automatically.
>> There may be issues with HSTS if the header is received before the 
>> device connects to your network, or if it arrives over an uncontrolled 
>> CONNECT tunnel. But there is not much to be done about those cases. 
> I'm not sure I follow (please bare with me!), see I'm simply trying to 
> prevent abuse by only allowing connections to whats in whitelist, is my 
> approach in my current config doing that? If there's a better way please 
> tell me. This is going over a satellite connection with extremely poor 
> latency and bandwidth, so Squid's job is to stop the connections from 
> reaching out, and only allowing whats in whitelist.
> 

HSTS is enabled on if the client receives a header called 
"Strict-Transport-Security". And only for a time indicated by the header 
itself.

So removing that header prevents HSTS being turned on in the client.

However the client can receive that header via any one of several 
different protocols (eg. HTTP/2, SPDY, QUIC, and HTTPS if you are 
splicing any of it). So preventing HSTS breaking things randomly 
requires they be blocked and only the filtered traffic allowed.

> 
> 4.
> 
>> That means the server that HTTPS connection is attempting to reach is 
>> not on your whitelist. It is therefore one of the things you wanted to 
>> be blocked according to your stated policy.
>>
>> ?- as I mentioned earlier, if that causes any issues your whitelist is 
>> incomplete. 
> But it is, eg whitelist.txt has .google.com, shouldn't google.com be 
> accessible then (without complaining about HSTS, since I'm splicing?). 


Splicing allows traffic to go through without the HSTS header removal. 
So the above (3) issues from HSTS maybe still happening.


As a general rule of thumb, if you are splicing a domain never bump it 
and if you are wanting to bump it never splice it nor allow any other 
ways to reach it except through the bumping proxy.

It is technically possible to transition a domain from being 
spliced/relayed to bumped, but it can be a painful process involving a 
lot of unavoidable TLS related "errors" for clients during the 
transition timeout dictated by the HSTS header and/or similar things.


As to the whitelist 'not working'. Earlier you posted a access.log entry 
that contained:

  1510180110.096      0 192.168.1.178 TCP_DENIED/200 0 CONNECT
    108.177.14.103:443 - HIER_NONE/- -

Note how that IP address is *not* a domain in *.google.com.
Even its reverse-DNS does not match *.google.com.

Googles rDNS server names are in *.1e100.net domain, not *.google.com. 
They also have a lot of other service name crossovers like "YouTube" 
actually being *.googlevideos.com, and gmail actually being 
docs.google.com sometimes (heh!).


To access any service with SSL-Bump your whitelist needs to allow its 
public domain name(s), reverse-DNS server name(s), and maybe also raw-IP 
address(s). That goes for any service, not just Google's.


And ... that rule of thumb I mention above applies to the entire bunch 
as a collective service. That is where major corps like Google cause 
headaches simply by having so many inter-related pieces with crossover 
like gmail and youtube domains vs the 1e100.net reverse-DNS.


Amos


From squid3 at treenet.co.nz  Sat Nov 11 01:54:20 2017
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sat, 11 Nov 2017 14:54:20 +1300
Subject: [squid-users] Non intrusive sslbump for whitelisting (asked
 many times but..)
In-Reply-To: <0de1efc3-3587-1584-0d32-735974f39600@treenet.co.nz>
References: <ae8fe086-e227-6b58-c6d8-f4e37d062eee@bk.ru>
 <96e5ae3d-6dc6-fca3-7c70-b038ecf16ddf@treenet.co.nz>
 <75c1764d-1923-f6c0-1896-2c64d1190779@bk.ru>
 <7a55ca37-23d0-a0f3-6397-2a772f91745f@treenet.co.nz>
 <4563ade7-9bdf-0237-bf8e-2ac6842bdc25@bk.ru>
 <0de1efc3-3587-1584-0d32-735974f39600@treenet.co.nz>
Message-ID: <2b9b21dd-1ef1-deaf-ce40-3ad4775c1ec2@treenet.co.nz>

On 11/11/17 14:03, Amos Jeffries wrote:
> On 11/11/17 01:05, A. Benz wrote:
>> Hi Amos,
>>
>> Thanks for your continued support.
>>
>> 1.
>>
>>> Do you mean the VPN exit point has that 10/8 IP address? or that the 
>>> traffic from the client is altered to be going to that IP before it 
>>> reaches Squid?
>>>
>>> The latter is broken because it destroys the original dst-IP values 
>>> on the TCP connection. Which Squid needs to setup the server connection. 
>>
>> Let me put it as an example:
>>
>> ?From the normal internet: mail.amosprivateserver.org > publicly 
>> accessible IP.
>>
>> ?From my place: mail.amosprivateserver.org > 10.x.x.x (corporate 
>> network, accessible only from within the place).
>>
>> Anyways no worries about this! I decided to make an exception in the 
>> redirect rule, so that if the outgoing traffic matches the IP 10.x.x.x 
>> then the firewall will not redirect the traffic to squid and instead 
>> establish a connection directly.
>>
>> This is not ideal, but it works.
>>
> 
> Or have Squid relay everything through the same server(s) and
> the server do the distinguishing between traffic and just relay 
> everythign to the same
> 

Damn that sounds daft.

What I meant to write was:

Or have Squid relay everything through the same server(s) and
the server do the distinguishing between traffic .

Or setup a cache_peer and have the traffic with src IP of the internal 
clients going to that domain sent there.

Amos


From tarotapprentice at yahoo.com  Sat Nov 11 07:28:22 2017
From: tarotapprentice at yahoo.com (TarotApprentice)
Date: Sat, 11 Nov 2017 07:28:22 +0000 (UTC)
Subject: [squid-users] APT/Raspbian caching
References: <240817737.77506.1510385302847.ref@mail.yahoo.com>
Message-ID: <240817737.77506.1510385302847@mail.yahoo.com>

One of the deployments I have is a cluster of Raspberry Pis. I am trying to cache apt content. The cluster has one node running (only) squid 3.5.23 with an external HDD. All the nodes have an entry in apt.conf.d pointing to the squid node:


Acquire::http::Proxy "http://localhost:3128";  (this is on the squid machine)


I find that the fetching of the release and index files is painfully slow. Squid seems to be caching the .deb and .udeb files and providing them once they've been downloaded but the apt update command seems particularly slow. All the Pi's are running the stretch release with some also using (Debian's) stretch-backports


Thoughts that spring to mind:

1. The Raspberry Pi foundation machines/network are overloaded.

2. IPv6 timing out and then trying IPv4 (both are enabled on the Pi)

3. Maybe APT is using HTTPS and I need to define a port for it to listen on.


Has anyone on the list tried using apt-cacher-ng with squid?


Below is my squid.conf and some output from apt taken from the Pi3 running squid.


MarkJ



sudo apt update

Get:1 http://archive.raspberrypi.org/debian stretch InRelease [25.3 kB]

Get:2 http://mirrordirector.raspbian.org/raspbian stretch InRelease [15.0 kB]

Fetched 40.2 kB in 1min 2s (639 B/s)

Reading package lists... Done

Building dependency tree

Reading state information... Done

All packages are up to date.


ping archive.raspberrypi.org

PING lb.raspberrypi.org (93.93.130.39) 56(84) bytes of data.

64 bytes from 93.93.130.39 (93.93.130.39): icmp_seq=1 ttl=52 time=301 ms

64 bytes from 93.93.130.39 (93.93.130.39): icmp_seq=2 ttl=52 time=301 ms


PING mirrordirector.raspbian.org (93.93.128.193) 56(84) bytes of data.

64 bytes from 93.93.128.193 (93.93.128.193): icmp_seq=1 ttl=52 time=300 ms

64 bytes from 93.93.128.193 (93.93.128.193): icmp_seq=2 ttl=52 time=301 ms


Squid.conf

acl localnet src 192.168.1.0/24 # internal network

acl localnet src fc00::/7       # RFC 4193 local private network range

acl localnet src fe80::/10      # RFC 4291 link-local (directly plugged) machines

acl aptop src 192.168.*.**  # lappie (redacted)

acl SSL_ports port 443

acl Safe_ports port 80          # http

acl Safe_ports port 21          # ftp

acl Safe_ports port 443         # https

acl Safe_ports port 70          # gopher

acl Safe_ports port 210         # wais

acl Safe_ports port 1025-65535  # unregistered ports

acl Safe_ports port 280         # http-mgmt

acl Safe_ports port 488         # gss-http

acl Safe_ports port 591         # filemaker

acl Safe_ports port 777         # multiling http

acl CONNECT method CONNECT

acl ads dstdomain .doubleclick.net

acl phishing dstdomain .flashtalking.com

#

# Recommended minimum Access Permission configuration:

#

# Deny requests to certain unsafe ports

http_access deny !Safe_ports

# Deny CONNECT to other than secure SSL ports

http_access deny CONNECT !SSL_ports

# Deny adverts and phishing

http_access deny ads

http_access deny phishing

# Only allow cachemgr access from laptop

http_access allow laptop manager

http_access deny manager

#

# INSERT YOUR OWN RULE(S) HERE TO ALLOW ACCESS FROM YOUR CLIENTS

#

# Example rule allowing access from your local networks.

# Adapt localnet in the ACL section to list your (internal) IP networks

# from where browsing should be allowed

http_access allow localnet

http_access allow localhost

# And finally deny all other access to this proxy

http_access deny all

# Squid normally listens to port 3128

http_port 3128

# Memory to use (default 8Mb)

cache_mem 500 MB

# Max object to cache (default 4Mb)

maximum_object_size 256 MB

# replacement_policy (default lru)

memory_replacement_policy lru

cache_replacement_policy heap LFUDA

# disk cache aufs, directory, 18Gb, 32x256 directories

cache_dir aufs /var/spool/squid 18432 32 256

# Dont abort downloads cache them

quick_abort_min -1 KB

# client_request_buffer_max_size (default 512 KB) used for uploads

client_request_buffer_max_size 128 KB

# Coredumps

coredump_dir /var/spool/squid

# Access logging

access_log /var/log/squid/access.log squid

# Cache log

cache_log /var/log/squid/cache.log

# number of logfiles to keep

logfile_rotate 5

# NetDB log disabled

netdb_filename none

# debian refresh pattern

refresh_pattern (\.deb|\.udeb)$ 1440 80% 10080

#

# Add any of your own refresh_pattern entries above these.

#

refresh_pattern ^ftp:           1440    20%     10080

refresh_pattern ^gopher:        1440    0%      1440

refresh_pattern -i (/cgi-bin/|\?) 0     0%      0

refresh_pattern .               0       20%     4320

# iinet DNS name servers

dns_nameservers 203.0.178.191 203.215.29.191

# verify dest servers (default off)

host_verify_strict on

# max number of open files

max_filedescriptors 1024

# try IPv4 before IPv6

dns_v4_first on

# disable pinger (default on)

pinger_enable off

# shutdown delay (default 30 secs)

shutdown_lifetime 5 seconds


From rentorbuy at yahoo.com  Sat Nov 11 17:48:39 2017
From: rentorbuy at yahoo.com (Vieri)
Date: Sat, 11 Nov 2017 17:48:39 +0000 (UTC)
Subject: [squid-users] url_rewrite_program and ACLs
In-Reply-To: <b0bd560b-081d-ec7b-261b-d92c1d479f98@treenet.co.nz>
References: <1990137371.4646637.1510150336187.ref@mail.yahoo.com>
 <1990137371.4646637.1510150336187@mail.yahoo.com>
 <921fbc5d-fce5-fd6f-b62e-8afd617994c7@treenet.co.nz>
 <131538788.5259356.1510213867661@mail.yahoo.com>
 <e89c27ac-b147-426e-e220-2c566b5e7586@treenet.co.nz>
 <2014705679.5324553.1510227544567@mail.yahoo.com>
 <b0bd560b-081d-ec7b-261b-d92c1d479f98@treenet.co.nz>
Message-ID: <1062451071.212201.1510422519167@mail.yahoo.com>


________________________________
From: Amos Jeffries <squid3 at treenet.co.nz>
>
> I would compare your custom script to the ext_sql_session_acl.pl.in 
> script we bundle with current Squid.
>  If yours lacks concurrency channel-ID I highly recommend adding that 
> behaviour.
>  If the DB is designed to store the protocol scheme, domain[:port] and 
> path?query portions of URLs in separate columns it will be more 
> efficient to pass those parameters as separate (%PROTO %DST %PORT %PATH) 
> to the helper instead of just %URI.
>
> The overheads in Squid of using external_acl_type helper interface 
> should be slightly less than the url_rewrite_program for SG. The SQL DB 
> data loading is about the same or better than what SG does AFAIK.


Thanks for the info. The script I was running was actually using concurrency with channel IDs. I also think it was correctly closing all file handles, DB connections, etc. However, I'm now merging your script which looks tidier into the one I was using. I'll see how it behaves over a few days.


> Ouch, but kind of expected with those FD number increase.

I'll have to find out why I'm still seeing this number rise albeit slower.
> Which reminds me ... Are you using SSL-Bump? if so ensure that you have 
> configured "sslflags=NO_DEFAULT_CA" on the port lines. The default 
> Trusted CA set can add a huge amount of useless memory to each client 
> connection, which can add up to many GB quite quickly.


Many thanks. Applied. I also noticed that Squid 4 defaults to tls_default_ca=off. Will keep that in mind when migrating to v.4 (actually, I'll just need to remove sslflags=NO_DEFAULT_CA).

> Nod, until the RAM runs out entirely, then problems are definitely to be 
> expected and that sounds like it is your problem now.


I really don't know how Linux manages memory, but despite my open FDs growing steadily and my "free" RAM slowly decreasing, at some point I noticed that the FDs kept growing slowly while the free mem suddenly went back up a bit (not a whole lot, but significantly -- around 0.5-0.7GB sudden increase).
> I actually physically blew up a test 
> machine (fire and smoke puring out the back!) measuring the effects of 
> RAID on a overloaded proxy about a decade ago.


I don't need this kind of horror stories... :-) Not yet at least. Let me first get a grip on my installation.

Thanks,

Vieri


From thesnable at gmail.com  Sun Nov 12 11:46:45 2017
From: thesnable at gmail.com (snable snable)
Date: Sun, 12 Nov 2017 12:46:45 +0100
Subject: [squid-users] 4.0.21 Ssl bump access denied
In-Reply-To: <CADYcWGTMoSEGsxm3C13T+NVakKCqo716V41wVn+6+PZ9RWMexA@mail.gmail.com>
References: <CADYcWGSbu1AMvS6QCPBq8b6urLb1Ty7Cs2NgujpSex4Fp_wHTg@mail.gmail.com>
 <CADYcWGS_hiYUN+DeA-pd2DeLseDF_53daBnkRBTArWqZXLQEVw@mail.gmail.com>
 <CADYcWGStxuT4aHpG3P13h-A9jnZ-PXWOzbZmV8Bkz6BeH+7uCQ@mail.gmail.com>
 <f159b55a-512d-0cbd-ab6a-ca53ceb8c9ad@treenet.co.nz>
 <CADYcWGRU=9dMdLoru_TtpZcR2gizkCgQvnGo=5WQn5atdmD8FA@mail.gmail.com>
 <CADYcWGTMoSEGsxm3C13T+NVakKCqo716V41wVn+6+PZ9RWMexA@mail.gmail.com>
Message-ID: <CADYcWGQ0AiCa0L5KmKYF_ERZF1y648bXSUUJpQKgQnFpDNUu5Q@mail.gmail.com>

hey

thanks:

i post in detail

i have an openwrt box. clients are attached there to the 192.168.2.0/24
network via nat. i attached the router as a wan device on my 192.168.1.0/24
with 192.168.1.254 as my internet gateway.

i have a squidbox  with squid 4 running on ports 3128 and 3129 and 3130.
 i forward the traffic from the openwrt via:


iptables -t mangle -A PREROUTING -j ACCEPT -p tcp
 --dport 80 -s 192.168.1.222
iptables -t mangle -A PREROUTING -j MARK --set-ma
rk 3 -p tcp --dport 80
iptables -t mangle -A PREROUTING -j ACCEPT -p tcp
 --dport 443 -s 192.168.1.222
iptables -t mangle -A PREROUTING -j MARK --set-ma
rk 3 -p tcp --dport 443
ip rule add fwmark 3 table 2
ip route add default via 192.168.1.222 dev eth0.2
 table 2

on the squid box redirected it via

iptables -A PREROUTING -t nat -i eth0 -p tcp --dp
ort 443 -j REDIRECT --to-port 3129

iptables -A PREROUTING -t nat -i eth0 -p tcp --dp
ort 80 -j REDIRECT --to-port 3128


http works fine


https brings:

ERRORThe requested URL could not be retrieved
------------------------------

The following error was encountered while trying to retrieve the URL:
https://192.168.1.222/*

*Connection to 192.168.1.222 failed.*

The system returned: *(111) Connection refused*

The remote host or network may be down. Please try the request again.

Your cache administrator is webmaster
<webmaster?subject=CacheErrorInfo%20-%20ERR_CONNECT_FAIL&body=CacheHost%3A%20raspberrypi%0D%0AErrPage%3A%20ERR_CONNECT_FAIL%0D%0AErr%3A%20%28111%29%20Connection%20refused%0D%0ATimeStamp%3A%20Sun,%2012%20Nov%202017%2011%3A44%3A04%20GMT%0D%0A%0D%0AClientIP%3A%20192.168.1.200%0D%0AServerIP%3A%20192.168.1.222%0D%0A%0D%0AHTTP%20Request%3A%0D%0ACONNECT%20%2F%20HTTP%2F1.1%0AHost%3A%20192.168.1.222%0D%0A%0D%0A%0D%0A>
.




i had this working a while ago but i forget how.


Am 08.11.2017 05:32 schrieb "Amos Jeffries" <squid3 at treenet.co.nz>:

> On 08/11/17 04:52, snable snable wrote:
>
>> Hello
>>
>> i forward from.my openwrt router the traffic for 443 and 80 to my squid
>> box to port 3129 and 3128
>>
>>
> What do you mean by "forward" ?
>
> Any dst-IP:port NAT operation *MUST* only happen on the Squid device
> itself or _later_ down the traffic path. Traffic must be *routed* to that
> Squid device.
>
>
> Amos
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20171112/e70757eb/attachment.htm>

From thesnable at gmail.com  Sun Nov 12 12:25:00 2017
From: thesnable at gmail.com (snable snable)
Date: Sun, 12 Nov 2017 13:25:00 +0100
Subject: [squid-users] 4.0.21 Ssl bump access denied
In-Reply-To: <CADYcWGR=xRtPKjn2f769JNWk50Fi5aGBmEQVhoCHA77xYTPAOQ@mail.gmail.com>
References: <CADYcWGSbu1AMvS6QCPBq8b6urLb1Ty7Cs2NgujpSex4Fp_wHTg@mail.gmail.com>
 <CADYcWGS_hiYUN+DeA-pd2DeLseDF_53daBnkRBTArWqZXLQEVw@mail.gmail.com>
 <CADYcWGStxuT4aHpG3P13h-A9jnZ-PXWOzbZmV8Bkz6BeH+7uCQ@mail.gmail.com>
 <f159b55a-512d-0cbd-ab6a-ca53ceb8c9ad@treenet.co.nz>
 <CADYcWGRU=9dMdLoru_TtpZcR2gizkCgQvnGo=5WQn5atdmD8FA@mail.gmail.com>
 <CADYcWGTMoSEGsxm3C13T+NVakKCqo716V41wVn+6+PZ9RWMexA@mail.gmail.com>
 <CADYcWGQ0AiCa0L5KmKYF_ERZF1y648bXSUUJpQKgQnFpDNUu5Q@mail.gmail.com>
 <CADYcWGR=xRtPKjn2f769JNWk50Fi5aGBmEQVhoCHA77xYTPAOQ@mail.gmail.com>
Message-ID: <CADYcWGT6Aq7zzZo4CGxymCBo31jzJA_Jwh1d=RzG3UhgGAZ0KQ@mail.gmail.com>

Access.log brings for www.heise.de on https

NECT 192.168.1.222:443 - HIER_NONE/- -
1510489280.731      2 192.168.1.200 NONE/200 0 CO
NNECT 192.168.1.222:443 - HIER_NONE/- -
1510489280.836      1 192.168.1.200 TCP_MISS/503
4691 GET https://www.heise.de/ - ORIGINAL_DST/192
.168.1.222 text/html
1510489280.892      1 192.168.1.200 TCP_MISS/503
4703 GET https://www.heise.de/favicon.ico - ORIGI
NAL_DST/192.168.1.222 text/html
1510489283.136      2 192.168.1.200 NONE/200 0 CO
NNECT 192.168.1.222:443 - HIER_NONE/- -
1510489283.224      1 192.168.1.200 TCP_MISS/503


Am 12.11.2017 12:46 schrieb "snable snable" <thesnable at gmail.com>:



hey

thanks:

i post in detail

i have an openwrt box. clients are attached there to the 192.168.2.0/24
network via nat. i attached the router as a wan device on my 192.168.1.0/24
with 192.168.1.254 as my internet gateway.

i have a squidbox  with squid 4 running on ports 3128 and 3129 and 3130.
 i forward the traffic from the openwrt via:


iptables -t mangle -A PREROUTING -j ACCEPT -p tcp
 --dport 80 -s 192.168.1.222
iptables -t mangle -A PREROUTING -j MARK --set-ma
rk 3 -p tcp --dport 80
iptables -t mangle -A PREROUTING -j ACCEPT -p tcp
 --dport 443 -s 192.168.1.222
iptables -t mangle -A PREROUTING -j MARK --set-ma
rk 3 -p tcp --dport 443
ip rule add fwmark 3 table 2
ip route add default via 192.168.1.222 dev eth0.2
 table 2

on the squid box redirected it via

iptables -A PREROUTING -t nat -i eth0 -p tcp --dp
ort 443 -j REDIRECT --to-port 3129

iptables -A PREROUTING -t nat -i eth0 -p tcp --dp
ort 80 -j REDIRECT --to-port 3128


http works fine


https brings:

ERRORThe requested URL could not be retrieved
------------------------------

The following error was encountered while trying to retrieve the URL:
https://192.168.1.222/*

*Connection to 192.168.1.222 failed.*

The system returned: *(111) Connection refused*

The remote host or network may be down. Please try the request again.

Your cache administrator is webmaster
<webmaster?subject=CacheErrorInfo%20-%20ERR_CONNECT_FAIL&body=CacheHost%3A%20raspberrypi%0D%0AErrPage%3A%20ERR_CONNECT_FAIL%0D%0AErr%3A%20%28111%29%20Connection%20refused%0D%0ATimeStamp%3A%20Sun,%2012%20Nov%202017%2011%3A44%3A04%20GMT%0D%0A%0D%0AClientIP%3A%20192.168.1.200%0D%0AServerIP%3A%20192.168.1.222%0D%0A%0D%0AHTTP%20Request%3A%0D%0ACONNECT%20%2F%20HTTP%2F1.1%0AHost%3A%20192.168.1.222%0D%0A%0D%0A%0D%0A>
.




i had this working a while ago but i forget how.


Am 08.11.2017 05:32 schrieb "Amos Jeffries" <squid3 at treenet.co.nz>:

> On 08/11/17 04:52, snable snable wrote:
>
>> Hello
>>
>> i forward from.my openwrt router the traffic for 443 and 80 to my squid
>> box to port 3129 and 3128
>>
>>
> What do you mean by "forward" ?
>
> Any dst-IP:port NAT operation *MUST* only happen on the Squid device
> itself or _later_ down the traffic path. Traffic must be *routed* to that
> Squid device.
>
>
> Amos
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20171112/6a23c335/attachment.htm>

From squid3 at treenet.co.nz  Mon Nov 13 04:04:49 2017
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Mon, 13 Nov 2017 17:04:49 +1300
Subject: [squid-users] 4.0.21 Ssl bump access denied
In-Reply-To: <CADYcWGT6Aq7zzZo4CGxymCBo31jzJA_Jwh1d=RzG3UhgGAZ0KQ@mail.gmail.com>
References: <CADYcWGSbu1AMvS6QCPBq8b6urLb1Ty7Cs2NgujpSex4Fp_wHTg@mail.gmail.com>
 <CADYcWGS_hiYUN+DeA-pd2DeLseDF_53daBnkRBTArWqZXLQEVw@mail.gmail.com>
 <CADYcWGStxuT4aHpG3P13h-A9jnZ-PXWOzbZmV8Bkz6BeH+7uCQ@mail.gmail.com>
 <f159b55a-512d-0cbd-ab6a-ca53ceb8c9ad@treenet.co.nz>
 <CADYcWGRU=9dMdLoru_TtpZcR2gizkCgQvnGo=5WQn5atdmD8FA@mail.gmail.com>
 <CADYcWGTMoSEGsxm3C13T+NVakKCqo716V41wVn+6+PZ9RWMexA@mail.gmail.com>
 <CADYcWGQ0AiCa0L5KmKYF_ERZF1y648bXSUUJpQKgQnFpDNUu5Q@mail.gmail.com>
 <CADYcWGR=xRtPKjn2f769JNWk50Fi5aGBmEQVhoCHA77xYTPAOQ@mail.gmail.com>
 <CADYcWGT6Aq7zzZo4CGxymCBo31jzJA_Jwh1d=RzG3UhgGAZ0KQ@mail.gmail.com>
Message-ID: <189607aa-58c1-83ba-2653-6b052fbe8efc@treenet.co.nz>

On 13/11/17 01:25, snable snable wrote:
> Access.log brings for www.heise.de on https
> 
> NECT 192.168.1.222:443 <http://192.168.1.222:443> - HIER_NONE/- -
> 1510489280.731? ? ? 2 192.168.1.200 NONE/200 0 CO
> NNECT 192.168.1.222:443 <http://192.168.1.222:443> - HIER_NONE/- -
> 1510489280.836? ? ? 1 192.168.1.200 TCP_MISS/503
> 4691 GET https://www.heise.de/ - ORIGINAL_DST/192
> .168.1.222 text/html


ORIGINAL_DST is the server IP your system NAT tables say the client is 
connecting to.

So the above means the NAT system is intercepting the client at 
192.168.1.200 connecting to the webserver at 192.168.1.222:443.


> 
> Am 12.11.2017 12:46 schrieb "snable snable" wrote:
> 
>         hey
> 
>         thanks:
> 
>         i post in detail
> 
>         i have an openwrt box. clients are attached there to the
>         192.168.2.0/24 <http://192.168.2.0/24> network via nat. i
>         attached the router as a wan device on my 192.168.1.0/24
>         <http://192.168.1.0/24> with 192.168.1.254 as my internet gateway.
> 
>         i have a squidbox? with squid 4 running on ports 3128 and 3129
>         and 3130.
>          ?i forward the traffic from the openwrt via:
> 
>         iptables -t mangle -A PREROUTING -j ACCEPT -p tcp
>          ?--dport 80 -s 192.168.1.222
>         iptables -t mangle -A PREROUTING -j MARK --set-ma
>         rk 3 -p tcp --dport 80
>         iptables -t mangle -A PREROUTING -j ACCEPT -p tcp
>          ?--dport 443 -s 192.168.1.222
>         iptables -t mangle -A PREROUTING -j MARK --set-ma
>         rk 3 -p tcp --dport 443
>         ip rule add fwmark 3 table 2
>         ip route add default via 192.168.1.222 dev eth0.2
>          ?table 2
> 
>         on the squid box redirected it via
> 
>         iptables -A PREROUTING -t nat -i eth0 -p tcp --dp
>         ort 443 -j REDIRECT --to-port 3129
>         iptables -A PREROUTING -t nat -i eth0 -p tcp --dp
>         ort 80 -j REDIRECT --to-port 3128
> 

There are no rules above preventing the NAT system intercepting the 
Squid outbound traffic.

Please see the iptables rules documented at: 
<https://wiki.squid-cache.org/ConfigExamples/Intercept/LinuxRedirect>.

-j ACCEPT in the *mangle* table only means iptables does not do your 
MARKing. It has no effect on these NAT table operations.

Amos


From rentorbuy at yahoo.com  Mon Nov 13 08:33:23 2017
From: rentorbuy at yahoo.com (Vieri)
Date: Mon, 13 Nov 2017 08:33:23 +0000 (UTC)
Subject: [squid-users] url_rewrite_program and ACLs
In-Reply-To: <b0bd560b-081d-ec7b-261b-d92c1d479f98@treenet.co.nz>
References: <1990137371.4646637.1510150336187.ref@mail.yahoo.com>
 <1990137371.4646637.1510150336187@mail.yahoo.com>
 <921fbc5d-fce5-fd6f-b62e-8afd617994c7@treenet.co.nz>
 <131538788.5259356.1510213867661@mail.yahoo.com>
 <e89c27ac-b147-426e-e220-2c566b5e7586@treenet.co.nz>
 <2014705679.5324553.1510227544567@mail.yahoo.com>
 <b0bd560b-081d-ec7b-261b-d92c1d479f98@treenet.co.nz>
Message-ID: <270284647.836282.1510562003626@mail.yahoo.com>


________________________________
From: Amos Jeffries <squid3 at treenet.co.nz>
>>
>> File descriptor usage for squid:
>> Maximum number of file descriptors: 65536
>> Largest file desc currently in use: 6980
>> Number of file desc currently in use: 6627
>> Files queued for open: 0
>> Available number of file descriptors: 58909
>> Reserved number of file descriptors: 100
>> Store Disk files open: 0
>> 
>> The FDs are still increasing steadily, but a LOT less.
>> 
>> On the other hand, the "free" RAM went from 2GB yesterday to just 275MB today:
>
> Which reminds me ... Are you using SSL-Bump? if so ensure that you have 

> configured "sslflags=NO_DEFAULT_CA"

FYI, this looks promising... I set "sslflags=NO_DEFAULT_CA". Today I had these readings:

File descriptor usage for squid:
Maximum number of file descriptors:   65536
Largest file desc currently in use:   7712
Number of file desc currently in use: 7220
Files queued for open:                   0
Available number of file descriptors: 58316
Reserved number of file descriptors:   100
Store Disk files open:                   0

# free --mega
total        used        free      shared  buff/cache   available
Mem:          32865        8235        3409         157       21221       24133
Swap:         37036         247       36789


So I'm having more open FDs than last time, but now I have more "free RAM" (3409MB instead of 275MB).


Thanks!

Vieri


From dernikov1 at gmail.com  Mon Nov 13 09:34:32 2017
From: dernikov1 at gmail.com (Bike dernikov1)
Date: Mon, 13 Nov 2017 10:34:32 +0100
Subject: [squid-users] SQUID memory error after vm.swappines changed
 from 60 to 10
In-Reply-To: <bd5c2853-4480-4808-d1d4-9d8b1a8f6b0d@measurement-factory.com>
References: <CAAPixKhsif5+ad1prsoFAZXVXcVAxpdqkw4RdvGbfK-UxhkzTQ@mail.gmail.com>
 <86188b87-0cc3-343c-7b56-7f01354a3aa8@measurement-factory.com>
 <CAAPixKiTszJDHUDK__PFgQ_+g0kR0RX4KZUNykFANKnTqzydsg@mail.gmail.com>
 <bfb6f0fc-357b-2156-7cec-e43e9718d521@measurement-factory.com>
 <CAAPixKhNB-1zkFP5X=b7f8__XEY0-9XtEGi8N0EkXTGB6kbygw@mail.gmail.com>
 <bd5c2853-4480-4808-d1d4-9d8b1a8f6b0d@measurement-factory.com>
Message-ID: <CAAPixKgzxz_hCWxW+T5TBuBFqG4z6b3KFy0-9qE31Vq33MWsmw@mail.gmail.com>

On Fri, Nov 10, 2017 at 4:43 PM, Alex Rousskov
<rousskov at measurement-factory.com> wrote:
> On 11/10/2017 06:16 AM, Bike dernikov1 wrote:
>
>> So you suggest that we totally disable disk swap (or just for
>> debuging) ?
>
> I would aim for totally disabling disk swap, but getting to that
> configuration is easier if you keep swap enabled and consider any
> non-trivial swap use as a problem that you need to fix. After all
> non-trivial swap uses are eliminated, it does not really matter much
> whether you keep swap enabled or not!

That is also my goal. I have other scenarios in mind.
We have free older servers  IBM X3550 (8 cores), which can be upgrade
to 48GB ram, (from other old servers)
On new ones,we have only 24GB on primary, and 16GB ram on  second server.
For disabling swap, X3550  servers would be better, but they have only 8 cores.
That could be problem, but for now we have only 0.5 load on 12/HT 24  cores.
I know that core per core new cpu-s can process  4x-8x more, so more testing :)

> Please note that this suggestion is specific to performance-sensitive
> Squid servers -- many other servers have very legitimate reasons to use
> swap. YMMV.
>
Oracle can be good example.
Yes, we have one Oracle server, swap killed him, users was wild, load
average over 10 continuosly (8 core), 16GB mem.
We upgraded ram to 48GB. Now, max load is at 4-5 in peeks, 1-2  during
day. Swap enabled :) it use 9GB, but use all ram :).
Users don't call after upgrade.

>
>> That setting on production can be disaster.
>
> Squid swapping in production is an arguably worse disaster, as you have
> learned. In many cases, it is better to deal with a lack of swap than to
> rely on swap's magical effects that most humans poorly understand. YMMV.

In this scenario, swap is backup cache (as I understand) ? As TIER  in  SAN-s.
Swap could be used  to translate back data to mem if used, but it
stays on disk and purge after some time if not used ?
Or I am in delusion ?

>> We will try, we reduced number of auth helper, prolong ldap caching.
>> Little steps but at last every bit counts.

We reduced kerberos helpers to 100 on Friday, 20 of those 100
currently active, 13 of those have requests/replies different than
zero.
Now reduced to 50. No problem appear over weekend.
cache_mem 14G
swappines = 60
ldap cache set to 24h

Mem stats:

             total        used        free      shared  buff/cache   available
Mem:          24101       16032        1488         156        6580        7497
Swap:         24561        1907       22654

> ... unless those bits are actually hurting. You may need to understand
> your server memory usage better to reduce the time spent on trying
> random changes. For example, if auth helpers are close to being
> overloaded, reducing their number may make things worse.

No overload after changes. When we saw that we don't use so much
helpers it was logicaly to reduce that  number in little steps.
I agree, random testing can be painfully, on production even more.
Can you recommend best way to analyze memory, (except free -m, top/htop).
Squid-internal-mgr/mem have nice detail, i will start there with. Do
you have better way ?

> Alex.

Thanks for help,


From dernikov1 at gmail.com  Mon Nov 13 09:46:44 2017
From: dernikov1 at gmail.com (Bike dernikov1)
Date: Mon, 13 Nov 2017 10:46:44 +0100
Subject: [squid-users] SQUID memory error after vm.swappines changed
 from 60 to 10
In-Reply-To: <6c7dbab4-fe9c-c543-7afa-9cea677c1de7@urlfilterdb.com>
References: <CAAPixKhsif5+ad1prsoFAZXVXcVAxpdqkw4RdvGbfK-UxhkzTQ@mail.gmail.com>
 <dab0b62b-57a5-5718-b0bd-70fb3de0c124@urlfilterdb.com>
 <CAAPixKjepL2BEePSH83XeBNB+UPy4wKoqjTn0_Axoa+n0THo7w@mail.gmail.com>
 <daf2a3ca-d5bd-f84c-0182-099f845c4e61@urlfilterdb.com>
 <CAAPixKhCKhySLEZiUi1sA-hBwQ74KxM1XDN-jKEFL5B3=hfoeQ@mail.gmail.com>
 <a7c29c43-131c-8f0b-fa66-88db16d0f8e5@urlfilterdb.com>
 <CAAPixKjhfEEXeoY7GHRCo=2Pmc3s0HqQkGPWTEH5LOYCfD3YCQ@mail.gmail.com>
 <6c7dbab4-fe9c-c543-7afa-9cea677c1de7@urlfilterdb.com>
Message-ID: <CAAPixKizv3srCOXaLv7Cn41-f9quoP0TSNZPiscKgWH77aBP1Q@mail.gmail.com>

On Fri, Nov 10, 2017 at 11:11 PM, Marcus Kool
<marcus.kool at urlfilterdb.com> wrote:
>
>
> On 10/11/17 12:11, Bike dernikov1 wrote:
>>
>> On Thu, Nov 9, 2017 at 5:13 PM, Marcus Kool <marcus.kool at urlfilterdb.com>
>> wrote:
>>>
>>>
>>>
>>> On 09/11/17 11:04, Bike dernikov1 wrote:
>>> [snip]
>>>>>>
>>>>>>
>>>>>> Memory compsumption:squid use largest part of memory  (12GB now,
>>>>>> second proces use 300MB memory), 14GB used by all process. So squid
>>>>>> use over 80% of total used memory.
>>>>>> So no there are not any problematic process. But we changed swappiness
>>>>>> settings.
>>>>>
>>>>>
>>>>>
>>>>> Did you monitor Squid for growth (it can start with 12 GB and grow
>>>>> slowly) ?
>>>>
>>>>
>>>>
>>>> Yes we are monitoring continuosly.
>>>> Now:
>>>> Output from free -m.
>>>>
>>>>              total       used    free   shared  buff/cache  available
>>>> Mem:  24101     20507  256    146      3337         3034
>>>> Swap: 24561      5040   19521
>>>>
>>>> vm.swappiness=40
>>>>
>>>> Memory by process:
>>>> squid  Virt       RES   SHR  MEM%
>>>>              22,9G  18.7   8164   79,6
>>>
>>>
>>>
>>> Hmm. Squid grew from 12 GB to 18.7 GB (23 GB virtual).
>>
>>
>> Today problem appeared again after logrotate at 2.56AM.
>> Used memory was at peek 23,7GB.
>
>
> ok. it is clear that Squid grows too much.
> On a 24GB system with many helpers and a URL filter I think the maximum size
> should be 14GB.

We set cache_mem to 14GB. For now no problems appeared.
We failed with helpers totally. We had problems with keytab cache, so
we thought that increase number will help.
At first we setup 700 kerberos helpers (insane from now  50 / 13 Active)
Until we disabled cache we couldn't stabilize server.
Disk was too slow, IO wait exploded, CPU load was at one time over
200. Users weren't happy.

>> Before logrorate started, cached was at 2GB, buffer at 1,5GB.
>> After logrorate started cache jumped to 3.7GB and buffer unchanged at
>> 1,5GB.
>>
>> Fork errors stopped after 1 minute. At 2:57.
>> cache memory dropped by 500MB  to 3.2GB and continued at same level
>> till morning, buffer  same at 1.5GB.
>>
>> After 4 at 3:00 minutes new WARNING appeared. external ACL queue
>> overload. Using stale results.
>>
>> We have night shift and they told us that Internet worked ok.
>>
>> After restart at around 7.00AM used memory dropped from 22 GB to 7GB,
>> cache and buffer remain at same levels.
>
>
> How come Squid uses 7 GB at startup when there is no disk cache ?
>
>>> With vm.swappiness=40 Linux starts to page out parts of processes when
>>> they
>>> occupy more than 60% of the memory.
>>> This is a potential bottleneck and I would have also decreased
>>> vm.swappiness
>>> to 10 as you did.
>>>
>>> My guess is that Squid starts too many helpers in a short time frame and
>>> that because of paging there are too many forks in progress
>>> simultaneously
>>> which causes the memory exhaustion.
>>
>>
>> We are now testing with 100 helpers for negotiate_kerberos_auth.
>> vm.swappiness returned to 60.
>>
>>> I suggest to reduce the memory cache of Squid by 50% and set
>>> vm.swappiness
>>> to 20.
>>
>>
>> Squid cache memory is set at 14GB reduced from 16GB from 20GB  in two
>> turns.
>
>
> are you saying that you have
>    cache_mem 14G
> If yes, you should read the memory FAQ and reduce this.
> 'cache_mem 14G' explains that Squid starts 'small' and grows over time.

For our case, what do you recomend.  10GB or even lower ?
Plan reading today, i hope that I will have peace, to concentrate.

>>> And then observe:
>>> - total memory use
>>> - total swap usage (should be lower than the 5 GB that you have now)
>>> - number of helper processes that are started in short time frames
>>> And then in small steps increase the memory cache and maybe further
>>> reduce
>>> vm.swappiness to 10.
>>
>>
>> If we survive with actual setup, we will continue with reducing as you
>> suggest.
>> Last extreme will be swap disable swappof but just for test with 6
>> eyes on monitoring :)
>>
>>>> squidguard two process  300MB boths,.
>>>>
>>>> CPU 0.33 0.37 0.43
>>>>
>>>>> Squid cannot fork and higher swappiness increases the amount of memory
>>>>> that
>>>>> the OS can use to copy processes.
>>>>> It makes me think that you have the memory overcommit set to 2 (no
>>>>> overcommit).
>>>>> What is the output of the following command ?
>>>>>      sysctl  -a | grep overcommit
>>>>
>>>>
>>>>
>>>> Command output:
>>>>
>>>> vm.nr_overcommit_hugepages = 0
>>>> vm.overcommit_kbytes = 0
>>>> vm.overcommit_memory = 0
>>>> vm.overcommit_ratio = 50
>>>>
>>>> cat /proc/sys/vm/overcommit_memory
>>>> 0
>>>
>>>
>>>
>>> The overcommit settings look fine.
>>
>>
>> At least something right :)
>>
>>>>
>>>>>> Advice for some settings:
>>>>>> We have absolute max peak of  2500 users which user squid (of 2800),
>>>>>> what are recomended settings for:
>>>>>> negotiate_kerberos_children start/idle
>>>>>> squidguard helpers.
>>>>>
>>>>>
>>>>>
>>>>>
>>>>> I have little experience with kerberos, but most likely this is not the
>>>>> issue.
>>>>> When Squid cannot fork the helpers, helper settings do not matter much.
>>>>
>>>>
>>>>
>>>>> For 2500 users you probably need 32-64 squidguard helpers.
>>>>
>>>>
>>>>
>>>> Can you confirm: For 2500 users:
>>>>
>>>> url_rewrite children X (squidguard)  32-64 will be ok ? We have set
>>>> much larger number.
>>
>>
>> Squidguard url_rewrite children was set to 64.
>>
>>> Did I understand it correctly that earlier in this reply you said that
>>> there
>>> are two squidguard processes (300 MB each).
>>
>>
>> Yes (first two process in htop, two rewrite childrens) others was on 0.0%.
>>
>>> ufdbGuard is faster than squidGuard and has multithreaded helpers.
>>> ufdbGuard needs less helpers than squidGuard.
>>> If you have a much larger number than 64 url rewrite helpers than I
>>> suggest
>>> to switch to ufdbGuard as soon as possible since the memory usage is then
>>> at
>>> least 600% less.
>>
>>
>> UfdbGuard have few strong features. Development, kerberos,
>> concurency/multitreading.
>> As i wrote, if we read documentation slower we wouldn't
>> Do ufdbGuard supoort ldap secure auth ? We tried ldap secure with
>> squidguard without success.
>
>
> ufdbGuard supports any user database with the "execuserlist" feature.
> See the Reference Manual for details.

As I can tell we will have much work in front of us.
But no price to high to get rid of TMG  :)
Thanks for help.

>>>> For  helper:
>>>> negotitate_kerberos_auth
>>>>
>>>> auth_param negotiate children X startup Y idle Z. What X, Y, Z are
>>>> best for our user number ?
>>>>
>>>> We disabled kerberos replay cache because of disk performance (4 SAS
>>>> DISK  15K, RAID 10) (iowait jumped high, and CPU load jumped to min
>>>> 40 max 200).
>>>> We don't use disk caching.
>>>>
>>>> Thanks for help,
>>>>
>>>>> Marcus
>>>>>
>>>>>
>>>>>> Thanks for help,
>>>>>>
>>>>>> On Wed, Nov 8, 2017 at 10:53 AM, Marcus Kool
>>>>>> <marcus.kool at urlfilterdb.com> wrote:
>>>>>>>
>>>>>>>
>>>>>>>
>>>>>>> There is definitely a problem with available memory because Squid
>>>>>>> cannot
>>>>>>> fork.
>>>>>>> So start with looking at how much memory Squid and its helpers use.
>>>>>>> Do do have other processes on this system that consume a lot of
>>>>>>> memory
>>>>>>> ?
>>>>>>>
>>>>>>> Also note that ufdbGuard uses less memory that squidGuard.
>>>>>>> If there are 30 helpers squidguard uses 300% more memory than
>>>>>>> ufdbGuard.
>>>>>>>
>>>>>>> Look at the wiki for more information about memory usage:
>>>>>>> https://wiki.squid-cache.org/SquidFaq/SquidMemory   (currently has an
>>>>>>> expired certificate but it is safe to go ahead)
>>>>>>>
>>>>>>> Marcus
>>>>>>>
>>>>>>>
>>>>>>>
>>>>>>> On 08/11/17 07:26, Bike dernikov1 wrote:
>>>>>>>>
>>>>>>>>
>>>>>>>>
>>>>>>>>
>>>>>>>> Hi, I hope that someone can explain what happened, why squid stopped
>>>>>>>> working.
>>>>>>>> The problem is related to  memory/swap handling.
>>>>>>>>
>>>>>>>> After we changed vm.swappiness parameter from 60 to 10 (tuning
>>>>>>>> attempt, to lower a disk usage, because we have only 4 disks in a
>>>>>>>> RAID10, so disk subsystem  is a weak link), we got a lot of errors
>>>>>>>> in
>>>>>>>> cache.log.
>>>>>>>> The problems started after scheduled logrotate after  2AM.
>>>>>>>> Squid ran out of memory, auth helpers stopped working.
>>>>>>>> It's weird because we didn't disable swap, but behavior is like we
>>>>>>>> did.
>>>>>>>> After an error, we increased parameter from 10 to 40.
>>>>>>>>
>>>>>>>> The server has 24GB DDR3 memory,  disk swap set to 24GB, 12 CPU
>>>>>>>> (24HT
>>>>>>>> cores).
>>>>>>>> We have 2800 users, using  kerberos authentication, squidguard for
>>>>>>>> filtering, ldap authorization.
>>>>>>>> When problem appeared memory was still 3GB free (free column), ram
>>>>>>>> (caching) was filled to 15GB, so 21 GB ram filled, 3GB free.
>>>>>>>>
>>>>>>>> Thanks for help,
>>>>>>>>
>>>>>>>>
>>>>>>>> errors from cache.log.
>>>>>>>>
>>>>>>>> 2017/11/08 02:55:27| Set Current Directory to /var/log/squid/
>>>>>>>> 2017/11/08 02:55:27 kid1| storeDirWriteCleanLogs: Starting...
>>>>>>>> 2017/11/08 02:55:27 kid1|   Finished.  Wrote 0 entries.
>>>>>>>> 2017/11/08 02:55:27 kid1|   Took 0.00 seconds (  0.00 entries/sec).
>>>>>>>> 2017/11/08 02:55:27 kid1| logfileRotate:
>>>>>>>> daemon:/var/log/squid/access.log
>>>>>>>> 2017/11/08 02:55:27 kid1| logfileRotate:
>>>>>>>> daemon:/var/log/squid/access.log
>>>>>>>> 2017/11/08 02:55:28 kid1| Pinger socket opened on FD 30
>>>>>>>> 2017/11/08 02:55:28 kid1| helperOpenServers: Starting 1/1000
>>>>>>>> 'squidGuard' processes
>>>>>>>> 2017/11/08 02:55:28 kid1| ipcCreate: fork: (12) Cannot allocate
>>>>>>>> memory
>>>>>>>> 2017/11/08 02:55:28 kid1| WARNING: Cannot run '/usr/bin/squidGuard'
>>>>>>>> process.
>>>>>>>> 2017/11/08 02:55:28 kid1| helperOpenServers: Starting 300/3000
>>>>>>>> 'negotiate_kerberos_auth' processes
>>>>>>>> 2017/11/08 02:55:28 kid1| ipcCreate: fork: (12) Cannot allocate
>>>>>>>> memory
>>>>>>>> 2017/11/08 02:55:28 kid1| WARNING: Cannot run
>>>>>>>> '/usr/lib/squid/negotiate_kerberos_auth' process.
>>>>>>>> 2017/11/08 02:55:28 kid1| ipcCreate: fork: (12) Cannot allocate
>>>>>>>> memory
>>>>>>>> 2017/11/08 02:55:28 kid1| WARNING: Cannot run
>>>>>>>> '/usr/lib/squid/negotiate_kerberos_auth' process.
>>>>>>>> 2017/11/08 02:55:28 kid1| ipcCreate: fork: (12) Cannot allocate
>>>>>>>> memory
>>>>>>>> 2017/11/08 02:55:28 kid1| WARNING: Cannot run
>>>>>>>> '/usr/lib/squid/negotiate_kerberos_auth' process.
>>>>>>>>
>>>>>>>> external ACL 'memberof' queue overload. Using stale result.
>>>>>>>> _______________________________________________
>>>>>>>> squid-users mailing list
>>>>>>>> squid-users at lists.squid-cache.org
>>>>>>>> http://lists.squid-cache.org/listinfo/squid-users
>>>>>>>>
>>>>>>> _______________________________________________
>>>>>>> squid-users mailing list
>>>>>>> squid-users at lists.squid-cache.org
>>>>>>> http://lists.squid-cache.org/listinfo/squid-users
>>>>>>
>>>>>>
>>>>>>
>>>>>>
>>>>>>
>>>>> _______________________________________________
>>>>> squid-users mailing list
>>>>> squid-users at lists.squid-cache.org
>>>>> http://lists.squid-cache.org/listinfo/squid-users
>>>>
>>>>
>>>>
>>>>
>>> _______________________________________________
>>> squid-users mailing list
>>> squid-users at lists.squid-cache.org
>>> http://lists.squid-cache.org/listinfo/squid-users
>>
>>
>>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users


From niklas.bachmaier at googlemail.com  Mon Nov 13 10:21:46 2017
From: niklas.bachmaier at googlemail.com (Niklas Bachmaier)
Date: Mon, 13 Nov 2017 11:21:46 +0100
Subject: [squid-users] OCSP stapling and must-staple
Message-ID: <CAFjr+MpgKJ-tSRaJ6J1L3NCH7J-Mg8z5hXmsEsTXJWxKJ5O=7A@mail.gmail.com>

Hello everyone

The last post I found on OCSP with Squid is from 2015 where it says
that Squid does not support OCSP by any means.

For certificate revocation checking we would like to make use of the
OCSP must-staple feature (defined in RFC 7633). We are asking
ourselves if OCSP stapling and especially must-staple is now supported
by Squid and, if it is, if there is any special configuration needed
to activate it.

We are currently using Squid 3.5 with OpenSSL version 1.0.2m from 2 Nov 2017.

Thanks already for any input on this!

Niklas


From marcus.kool at urlfilterdb.com  Mon Nov 13 11:15:09 2017
From: marcus.kool at urlfilterdb.com (Marcus Kool)
Date: Mon, 13 Nov 2017 09:15:09 -0200
Subject: [squid-users] SQUID memory error after vm.swappines changed
 from 60 to 10
In-Reply-To: <CAAPixKizv3srCOXaLv7Cn41-f9quoP0TSNZPiscKgWH77aBP1Q@mail.gmail.com>
References: <CAAPixKhsif5+ad1prsoFAZXVXcVAxpdqkw4RdvGbfK-UxhkzTQ@mail.gmail.com>
 <dab0b62b-57a5-5718-b0bd-70fb3de0c124@urlfilterdb.com>
 <CAAPixKjepL2BEePSH83XeBNB+UPy4wKoqjTn0_Axoa+n0THo7w@mail.gmail.com>
 <daf2a3ca-d5bd-f84c-0182-099f845c4e61@urlfilterdb.com>
 <CAAPixKhCKhySLEZiUi1sA-hBwQ74KxM1XDN-jKEFL5B3=hfoeQ@mail.gmail.com>
 <a7c29c43-131c-8f0b-fa66-88db16d0f8e5@urlfilterdb.com>
 <CAAPixKjhfEEXeoY7GHRCo=2Pmc3s0HqQkGPWTEH5LOYCfD3YCQ@mail.gmail.com>
 <6c7dbab4-fe9c-c543-7afa-9cea677c1de7@urlfilterdb.com>
 <CAAPixKizv3srCOXaLv7Cn41-f9quoP0TSNZPiscKgWH77aBP1Q@mail.gmail.com>
Message-ID: <a0dd0ba2-3995-735e-d3b7-8ce66330c951@urlfilterdb.com>



On 13/11/17 07:46, Bike dernikov1 wrote:

>> are you saying that you have
>>     cache_mem 14G
>> If yes, you should read the memory FAQ and reduce this.
>> 'cache_mem 14G' explains that Squid starts 'small' and grows over time.
> 
> For our case, what do you recomend.  10GB or even lower ?
> Plan reading today, i hope that I will have peace, to concentrate.

cache_mem does NOT define the total memory use of Squid.
The FAQ explains it.
On a 24G system you can start with 7 GB and only after 3 days of running without issues and verifying that the cache is 100% utilised (if not, Squid can grow) and there is sufficient free memory, you 
can increase it.



From dernikov1 at gmail.com  Mon Nov 13 12:42:38 2017
From: dernikov1 at gmail.com (Bike dernikov1)
Date: Mon, 13 Nov 2017 13:42:38 +0100
Subject: [squid-users] SQUID memory error after vm.swappines changed
 from 60 to 10
In-Reply-To: <a0dd0ba2-3995-735e-d3b7-8ce66330c951@urlfilterdb.com>
References: <CAAPixKhsif5+ad1prsoFAZXVXcVAxpdqkw4RdvGbfK-UxhkzTQ@mail.gmail.com>
 <dab0b62b-57a5-5718-b0bd-70fb3de0c124@urlfilterdb.com>
 <CAAPixKjepL2BEePSH83XeBNB+UPy4wKoqjTn0_Axoa+n0THo7w@mail.gmail.com>
 <daf2a3ca-d5bd-f84c-0182-099f845c4e61@urlfilterdb.com>
 <CAAPixKhCKhySLEZiUi1sA-hBwQ74KxM1XDN-jKEFL5B3=hfoeQ@mail.gmail.com>
 <a7c29c43-131c-8f0b-fa66-88db16d0f8e5@urlfilterdb.com>
 <CAAPixKjhfEEXeoY7GHRCo=2Pmc3s0HqQkGPWTEH5LOYCfD3YCQ@mail.gmail.com>
 <6c7dbab4-fe9c-c543-7afa-9cea677c1de7@urlfilterdb.com>
 <CAAPixKizv3srCOXaLv7Cn41-f9quoP0TSNZPiscKgWH77aBP1Q@mail.gmail.com>
 <a0dd0ba2-3995-735e-d3b7-8ce66330c951@urlfilterdb.com>
Message-ID: <CAAPixKh2S=Z6C6_Rz8E8J2UbK_e6QZj3obF=GOyf5tsr1BWT+A@mail.gmail.com>

On Mon, Nov 13, 2017 at 12:15 PM, Marcus Kool
<marcus.kool at urlfilterdb.com> wrote:
>
>
> On 13/11/17 07:46, Bike dernikov1 wrote:
>
>>> are you saying that you have
>>>     cache_mem 14G
>>> If yes, you should read the memory FAQ and reduce this.
>>> 'cache_mem 14G' explains that Squid starts 'small' and grows over time.
>>
>>
>> For our case, what do you recomend.  10GB or even lower ?
>> Plan reading today, i hope that I will have peace, to concentrate.
>
>
> cache_mem does NOT define the total memory use of Squid.
> The FAQ explains it.
> On a 24G system you can start with 7 GB and only after 3 days of running
> without issues and verifying that the cache is 100% utilised (if not, Squid
> can grow) and there is sufficient free memory, you can increase it.

Read FAQ, i am now trying to pass trough squid-internal-mgr/ reports/statistics.
For now we will stay at cache_mem 14GB, because we are modifying too
many settings at same time.


Cache information for squid:
Hits as % of all requests: 5min: 5.8%, 60min: 6.4%
Hits as % of bytes sent: 5min: 16.8%, 60min: 18.7%
Memory hits as % of hit requests: 5min: 64.1%, 60min: 64.9%
Disk hits as % of hit requests: 5min: 0.0%, 60min: 0.1%
Storage Swap size: 0 KB
Storage Swap capacity: 0.0% used,  0.0% free

Mean Object Size: 0.00 KB
Requests given to unlinkd: 0




> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users


From dernikov1 at gmail.com  Mon Nov 13 12:46:07 2017
From: dernikov1 at gmail.com (Bike dernikov1)
Date: Mon, 13 Nov 2017 13:46:07 +0100
Subject: [squid-users] SQUID memory error after vm.swappines changed
 from 60 to 10
In-Reply-To: <a0dd0ba2-3995-735e-d3b7-8ce66330c951@urlfilterdb.com>
References: <CAAPixKhsif5+ad1prsoFAZXVXcVAxpdqkw4RdvGbfK-UxhkzTQ@mail.gmail.com>
 <dab0b62b-57a5-5718-b0bd-70fb3de0c124@urlfilterdb.com>
 <CAAPixKjepL2BEePSH83XeBNB+UPy4wKoqjTn0_Axoa+n0THo7w@mail.gmail.com>
 <daf2a3ca-d5bd-f84c-0182-099f845c4e61@urlfilterdb.com>
 <CAAPixKhCKhySLEZiUi1sA-hBwQ74KxM1XDN-jKEFL5B3=hfoeQ@mail.gmail.com>
 <a7c29c43-131c-8f0b-fa66-88db16d0f8e5@urlfilterdb.com>
 <CAAPixKjhfEEXeoY7GHRCo=2Pmc3s0HqQkGPWTEH5LOYCfD3YCQ@mail.gmail.com>
 <6c7dbab4-fe9c-c543-7afa-9cea677c1de7@urlfilterdb.com>
 <CAAPixKizv3srCOXaLv7Cn41-f9quoP0TSNZPiscKgWH77aBP1Q@mail.gmail.com>
 <a0dd0ba2-3995-735e-d3b7-8ce66330c951@urlfilterdb.com>
Message-ID: <CAAPixKiLTGrCGt3Tb6eHYB0kH75YB8J1ZGaRKtHXzJ=n5BoF=w@mail.gmail.com>

On Mon, Nov 13, 2017 at 12:15 PM, Marcus Kool
<marcus.kool at urlfilterdb.com> wrote:
>
>
> On 13/11/17 07:46, Bike dernikov1 wrote:
>
>>> are you saying that you have
>>>     cache_mem 14G
>>> If yes, you should read the memory FAQ and reduce this.
>>> 'cache_mem 14G' explains that Squid starts 'small' and grows over time.
>>
>>
>> For our case, what do you recomend.  10GB or even lower ?
>> Plan reading today, i hope that I will have peace, to concentrate.
>
>
> cache_mem does NOT define the total memory use of Squid.
> The FAQ explains it.
> On a 24G system you can start with 7 GB and only after 3 days of running
> without issues and verifying that the cache is 100% utilised (if not, Squid
> can grow) and there is sufficient free memory, you can increase it.
>

Read FAQ.
Now  trying to pass trough squid-internal-mgr/ reports/statistics.
For now we will stay at cache_mem 14GB, because we are modifying too
many settings at same time. Now at 99% used at 14GB if i read correctly.
Thanks for sugestions and help.


squid-internal-mgr/info output:
Cache information for squid:
Hits as % of all requests: 5min: 5.8%, 60min: 6.4%
Hits as % of bytes sent: 5min: 16.8%, 60min: 18.7%
Memory hits as % of hit requests: 5min: 64.1%, 60min: 64.9%
Disk hits as % of hit requests: 5min: 0.0%, 60min: 0.1%
Storage Swap size: 0 KB
Storage Swap capacity: 0.0% used,  0.0% free
Storage Mem size: 14195856 KB
Storage Mem capacity: 99.0% used,  1.0% free
Mean Object Size: 0.00 KB
Requests given to unlinkd: 0


> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users


From marcus.kool at urlfilterdb.com  Mon Nov 13 13:32:54 2017
From: marcus.kool at urlfilterdb.com (Marcus Kool)
Date: Mon, 13 Nov 2017 11:32:54 -0200
Subject: [squid-users] SQUID memory error after vm.swappines changed
 from 60 to 10
In-Reply-To: <CAAPixKiLTGrCGt3Tb6eHYB0kH75YB8J1ZGaRKtHXzJ=n5BoF=w@mail.gmail.com>
References: <CAAPixKhsif5+ad1prsoFAZXVXcVAxpdqkw4RdvGbfK-UxhkzTQ@mail.gmail.com>
 <dab0b62b-57a5-5718-b0bd-70fb3de0c124@urlfilterdb.com>
 <CAAPixKjepL2BEePSH83XeBNB+UPy4wKoqjTn0_Axoa+n0THo7w@mail.gmail.com>
 <daf2a3ca-d5bd-f84c-0182-099f845c4e61@urlfilterdb.com>
 <CAAPixKhCKhySLEZiUi1sA-hBwQ74KxM1XDN-jKEFL5B3=hfoeQ@mail.gmail.com>
 <a7c29c43-131c-8f0b-fa66-88db16d0f8e5@urlfilterdb.com>
 <CAAPixKjhfEEXeoY7GHRCo=2Pmc3s0HqQkGPWTEH5LOYCfD3YCQ@mail.gmail.com>
 <6c7dbab4-fe9c-c543-7afa-9cea677c1de7@urlfilterdb.com>
 <CAAPixKizv3srCOXaLv7Cn41-f9quoP0TSNZPiscKgWH77aBP1Q@mail.gmail.com>
 <a0dd0ba2-3995-735e-d3b7-8ce66330c951@urlfilterdb.com>
 <CAAPixKiLTGrCGt3Tb6eHYB0kH75YB8J1ZGaRKtHXzJ=n5BoF=w@mail.gmail.com>
Message-ID: <733e9e3e-c863-aee0-8d42-9cc84c8e7799@urlfilterdb.com>



On 13/11/17 10:46, Bike dernikov1 wrote:
> On Mon, Nov 13, 2017 at 12:15 PM, Marcus Kool
> <marcus.kool at urlfilterdb.com> wrote:
>>
>>
>> On 13/11/17 07:46, Bike dernikov1 wrote:
>>
>>>> are you saying that you have
>>>>      cache_mem 14G
>>>> If yes, you should read the memory FAQ and reduce this.
>>>> 'cache_mem 14G' explains that Squid starts 'small' and grows over time.
>>>
>>>
>>> For our case, what do you recomend.  10GB or even lower ?
>>> Plan reading today, i hope that I will have peace, to concentrate.
>>
>>
>> cache_mem does NOT define the total memory use of Squid.
>> The FAQ explains it.
>> On a 24G system you can start with 7 GB and only after 3 days of running
>> without issues and verifying that the cache is 100% utilised (if not, Squid
>> can grow) and there is sufficient free memory, you can increase it.
>>
> 
> Read FAQ.
> Now  trying to pass trough squid-internal-mgr/ reports/statistics.
> For now we will stay at cache_mem 14GB, because we are modifying too
> many settings at same time. Now at 99% used at 14GB if i read correctly.
> Thanks for sugestions and help.
> 
> 
> squid-internal-mgr/info output:
> Cache information for squid:
> Hits as % of all requests: 5min: 5.8%, 60min: 6.4%
> Hits as % of bytes sent: 5min: 16.8%, 60min: 18.7%
> Memory hits as % of hit requests: 5min: 64.1%, 60min: 64.9%
> Disk hits as % of hit requests: 5min: 0.0%, 60min: 0.1%
> Storage Swap size: 0 KB
> Storage Swap capacity: 0.0% used,  0.0% free
> Storage Mem size: 14195856 KB
> Storage Mem capacity: 99.0% used,  1.0% free
> Mean Object Size: 0.00 KB
> Requests given to unlinkd: 0

Beware that he storage mem size is not the same as total memory used.


From rousskov at measurement-factory.com  Mon Nov 13 15:36:02 2017
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Mon, 13 Nov 2017 08:36:02 -0700
Subject: [squid-users] SQUID memory error after vm.swappines changed
 from 60 to 10
In-Reply-To: <CAAPixKgzxz_hCWxW+T5TBuBFqG4z6b3KFy0-9qE31Vq33MWsmw@mail.gmail.com>
References: <CAAPixKhsif5+ad1prsoFAZXVXcVAxpdqkw4RdvGbfK-UxhkzTQ@mail.gmail.com>
 <86188b87-0cc3-343c-7b56-7f01354a3aa8@measurement-factory.com>
 <CAAPixKiTszJDHUDK__PFgQ_+g0kR0RX4KZUNykFANKnTqzydsg@mail.gmail.com>
 <bfb6f0fc-357b-2156-7cec-e43e9718d521@measurement-factory.com>
 <CAAPixKhNB-1zkFP5X=b7f8__XEY0-9XtEGi8N0EkXTGB6kbygw@mail.gmail.com>
 <bd5c2853-4480-4808-d1d4-9d8b1a8f6b0d@measurement-factory.com>
 <CAAPixKgzxz_hCWxW+T5TBuBFqG4z6b3KFy0-9qE31Vq33MWsmw@mail.gmail.com>
Message-ID: <f8132fc7-58b0-e427-d7ae-60204da73e3e@measurement-factory.com>

On 11/13/2017 02:34 AM, Bike dernikov1 wrote:
> On Fri, Nov 10, 2017 at 4:43 PM, Alex Rousskov wrote:
>> Squid swapping in production is an arguably worse disaster, as you have
>> learned. In many cases, it is better to deal with a lack of swap than to
>> rely on swap's magical effects that most humans poorly understand. YMMV.

> In this scenario, swap is backup cache (as I understand)?

In this scenario, swap is not a cache! In fact it is pretty much the
opposite:

* A cache is, by definition, an optional unreliable "fast" storage meant
to reduce the need to go to some "slow" storage.

* When in active use, swap is required reliable slow storage meant to
extend fast storage (RAM) capacity.

Do you see how almost every adjective in the first bullet is replaced
with an antonym in the second one?

Some services, including many databases, overallocate RAM to store
rarely used (computed and/or preloaded) data. When that data is swapped
out, the service often continues to operate normally because the data is
rarely accessed (and/or because swapping it in is still cheaper than
computing it from scratch).

With Squid, it is very difficult for the OS to correctly identify the
rarely used RAM areas to swap out. When the OS swaps out the wrong area,
Squid slows down (to access that area), which only increases the number
of concurrent transactions and, hence, the amount of RAM Squid needs to
operate, which triggers more wrong swap outs, creating a vicious cycle.


> Swap could be used  to translate back data to mem if used, but it
> stays on disk and purge after some time if not used ?

The purging bit is wrong. Think of swap as very very very slow RAM.

Alex.


From rousskov at measurement-factory.com  Mon Nov 13 16:07:46 2017
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Mon, 13 Nov 2017 09:07:46 -0700
Subject: [squid-users] OCSP stapling and must-staple
In-Reply-To: <CAFjr+MpgKJ-tSRaJ6J1L3NCH7J-Mg8z5hXmsEsTXJWxKJ5O=7A@mail.gmail.com>
References: <CAFjr+MpgKJ-tSRaJ6J1L3NCH7J-Mg8z5hXmsEsTXJWxKJ5O=7A@mail.gmail.com>
Message-ID: <f21607bb-9bd4-03a7-ed79-6e6e0d3f5692@measurement-factory.com>

On 11/13/2017 03:21 AM, Niklas Bachmaier wrote:

> The last post I found on OCSP with Squid is from 2015 where it says
> that Squid does not support OCSP by any means.

For the record, here is that 2015 thread:
http://lists.squid-cache.org/pipermail/squid-users/2015-October/005831.html


> For certificate revocation checking we would like to make use of the
> OCSP must-staple feature (defined in RFC 7633). We are asking
> ourselves if OCSP stapling and especially must-staple is now supported
> by Squid and, if it is, if there is any special configuration needed
> to activate it.

AFAIK, OpenSSL does not automatically validate OCSP-related parts of the
server Hello. Squid does not do that either (yet?). As I said in 2015,
it may be possible to do the required validation using an external
certificate validator (sslcrtvalidator_program). If not already possible
"as is", it is probably not difficult to add the missing bits to Squid
to enable such external OCSP validation.


HTH,

Alex.


From akash.kumar.patel at appdirect.com  Mon Nov 13 22:21:15 2017
From: akash.kumar.patel at appdirect.com (tappdint)
Date: Mon, 13 Nov 2017 15:21:15 -0700 (MST)
Subject: [squid-users] Proxy does not send response for internal host
Message-ID: <1510611675802-0.post@n4.nabble.com>

I'm quite new to proxies/networking so I'll try and be as verbose as I can. I
have a VM that will request access to an internal host (dev.abc.com). This
host container and other containers which are dependencies are created
locally on my mac (using docker for mac). So I also run a docker squid
container  (datadog/squid) on my mac to allow that VM to use the same
network/access same hosts. From inside my VM when I use the proxy to curl
sites like Google/Facebook/Yahoo i get proper responses. But when I curl
this host I get an ERR_DNS_FAIL result.

<div id="content">
<p>The following error was encountered while trying to retrieve the URL: 
http://dev.abc.com/ <http://dev.abc.com/>  </p>

<blockquote id="error">
<p>*Unable to determine IP address from host name <q>dev.abc.com</q>*</p>
</blockquote>

<p>The DNS server returned:</p>
<blockquote id="data">
<pre>Name Error: The domain name does not exist.</pre>
</blockquote>

<p>This means that the cache was not able to resolve the hostname presented
in the URL. Check if the address is correct.</p>

<p>Your cache administrator is  webmaster
<mailto:webmaster?subject=CacheErrorInfo%20-%20ERR_DNS_FAIL&amp;body=CacheHost%3A%20538a0f427a73%0D%0AErrPage%3A%20ERR_DNS_FAIL%0D%0AErr%3A%20%5Bnone%5D%0D%0ADNS%20ErrMsg%3A%20Name%20Error%3A%20The%20domain%20name%20does%20not%20exist.%0D%0ATimeStamp%3A%20Tue,%2007%20Nov%202017%2022%3A18%3A35%20GMT%0D%0A%0D%0AClientIP%3A%20192.168.99.1%0D%0A%0D%0AHTTP%20Request%3A%0D%0AGET%20%2F%20HTTP%2F1.1%0AUser-Agent%3A%20curl%2F7.43.0%0D%0AAccept%3A%20*%2F*%0D%0AProxy-Connection%3A%20Keep-Alive%0D%0AHost%3A%x.yz.com%0D%0A%0D%0A%0D%0A> 
.</p>
<br>
</div>

<hr>
<div id="footer">
<p>Generated Tue, 07 Nov 2017 22:18:35 GMT by 538a0f427a73
(squid/3.5.12)</p>

</div>

I've searched online for posts with similar errors but I'm still confused. I
*think* that the issue lies in my squid config but I'm not sure what exactly
I need to modify/add. I basically took the huge config file provided by the
image and removed all the unnecessary comments/options. The site said that
/Please note that the stock configuration available with the container is
set for local access 
/. So thats currently what my config does. Here is what is looks like:

---------------------------------------------------
# Adapt to list your (internal) IP networks from where browsing should be
allowed
acl localnet src 10.0.0.0/8     # RFC1918 possible internal network
acl localnet src 172.16.0.0/12  # RFC1918 possible internal network
acl localnet src 192.168.0.0/16 # RFC1918 possible internal network
acl localnet src fc00::/7       # RFC 4193 local private network range
acl localnet src fe80::/10      # RFC 4291 link-local (directly plugged)
machines

acl SSL_ports port 443
acl Safe_ports port 80          # http
acl Safe_ports port 21          # ftp
acl Safe_ports port 443         # https
acl Safe_ports port 70          # gopher
acl Safe_ports port 210         # wais
acl Safe_ports port 1025-65535  # unregistered ports
acl Safe_ports port 280         # http-mgmt
acl Safe_ports port 488         # gss-http
acl Safe_ports port 591         # filemaker
acl Safe_ports port 777         # multiling http
acl CONNECT method CONNECT

# Recommended minimum Access Permission configuration:
# Deny requests to certain unsafe ports
http_access deny !Safe_ports

# Deny CONNECT to other than secure SSL ports
http_access deny CONNECT !SSL_ports

# Only allow cachemgr access from localhost
http_access allow localhost manager
http_access deny manager

http_access deny to_localhost

http_access allow localnet
http_access allow localhost

# And finally deny all other access to this proxy
http_access deny all


# Squid normally listens to port 3128
http_port 3128

debug_options rotate=1 ALL,2
---------------------------------------------------

What modifications/config options should i be looking into so that my
internal host resolves? Thanks!



--
Sent from: http://squid-web-proxy-cache.1019090.n4.nabble.com/Squid-Users-f1019091.html


From squid3 at treenet.co.nz  Tue Nov 14 02:36:56 2017
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Tue, 14 Nov 2017 15:36:56 +1300
Subject: [squid-users] Proxy does not send response for internal host
In-Reply-To: <1510611675802-0.post@n4.nabble.com>
References: <1510611675802-0.post@n4.nabble.com>
Message-ID: <7e0807cf-74e3-31b1-db2a-f0ec619220ce@treenet.co.nz>

On 14/11/17 11:21, tappdint wrote:
> I'm quite new to proxies/networking so I'll try and be as verbose as I can. I
> have a VM that will request access to an internal host (dev.abc.com). This
> host container and other containers which are dependencies are created
> locally on my mac (using docker for mac). So I also run a docker squid
> container  (datadog/squid) on my mac to allow that VM to use the same
> network/access same hosts. From inside my VM when I use the proxy to curl
> sites like Google/Facebook/Yahoo i get proper responses. But when I curl
> this host I get an ERR_DNS_FAIL result.
> 
> <div id="content">
> <p>The following error was encountered while trying to retrieve the URL:
> http://dev.abc.com/ <http://dev.abc.com/>  </p>
> 
> <blockquote id="error">
> <p>*Unable to determine IP address from host name <q>dev.abc.com</q>*</p>
> </blockquote>
> 
> <p>The DNS server returned:</p>
> <blockquote id="data">
> <pre>Name Error: The domain name does not exist.</pre>
> </blockquote>
> 
> <p>This means that the cache was not able to resolve the hostname presented
> in the URL. Check if the address is correct.</p>
> 

There should be nothing mysterious here. The above error message text is 
literal and clear about the problem as can be. If you do not understand 
them please lookup the terms is uses.

"the cache" here is a less common term meaning your Squid. The others 
are all common networking names for the pieces and things happening.


> 
> I've searched online for posts with similar errors but I'm still confused. I
> *think* that the issue lies in my squid config but I'm not sure what exactly
> I need to modify/add. I basically took the huge config file provided by the
> image and removed all the unnecessary comments/options. The site said that
> /Please note that the stock configuration available with the container is
> set for local access
> /. So thats currently what my config does. Here is what is looks like:
> 
... <snip default Squid-3 config documentation file>


> 
> What modifications/config options should i be looking into so that my
> internal host resolves? Thanks!
> 

You need to either;

  * configure the VM running Squid to use a DNS resolver that knows what 
the internal domains are, or

  * configure the DNS resolver it is using to know those internal 
hostnames, or

  * configure your network using the standardized .local namespace 
intended for local hostnames and all the VMs to use m-DNS.

Amos


From rentorbuy at yahoo.com  Tue Nov 14 09:46:51 2017
From: rentorbuy at yahoo.com (Vieri)
Date: Tue, 14 Nov 2017 09:46:51 +0000 (UTC)
Subject: [squid-users] deny_info
References: <1727217364.194806.1510652811222.ref@mail.yahoo.com>
Message-ID: <1727217364.194806.1510652811222@mail.yahoo.com>

Hi,

I'm trying to figure out how to correctly handle ERROR pages (or deny pages) in one particular case.

An HTTP client is trying to access a website as https://example.org/.

I'm getting the following info in cache.log:

2017/11/14 09:11:11.481 kid1| 85,2| client_side_request.cc(745) clientAccessCheckDone: The request GET https://example.org/ is ALLOWED; last ACL checked: bl_lookup
2017/11/14 09:11:11.481 kid1| 85,2| client_side_request.cc(721) clientAccessCheck2: No adapted_http_access configuration. default: ALLOW
2017/11/14 09:11:11.481 kid1| 85,2| client_side_request.cc(745) clientAccessCheckDone: The request GET https://example.org/ is ALLOWED; last ACL checked: bl_lookup
2017/11/14 09:11:11.591 kid1| 88,2| client_side_reply.cc(2073) processReplyAccessResult: The reply for GET https://example.org/ is DENIED, because it matched denied_restricted1_mimetypes_rep
2017/11/14 09:11:11.591 kid1| 88,2| client_side_reply.cc(2073) processReplyAccessResult: The reply for GET https://example.org/ is ALLOWED, because it matched denied_restricted1_mimetypes_rep

This is what I have in squid.conf (part of it):

external_acl_type bllookup ttl=86400 negative_ttl=86400 children-max=80 children-startup=10 children-idle=3 concurrency=8 %PROTO %DST %PORT %PATH /opt/custom/scripts/ext_sql_blwl_acl.pl --table=shallalist_bl --categories=adv,aggressive,alcohol,anonvpn,automobile_bikes,automobile_boats,automobie_cars,automobile_planes,chat,costtraps,dating,drugs,dynamic,finance_insurance,finance_moneylending,finance_other,finance_realestate,finance_trading,fortunetlling,forum,gamble,hacking,hobby_cooking,hobby_games-misc,hobby_games-online,hobby_gardening,hobby_pets,homestyle,imagehosting,isp,jobsearch,military,models,ovies,music,podcasts,politics,porn,radiotv,recreation_humor,recreation_martialarts,recreation_restaurants,recreation_sports,recreation_travel,recreation_welless,redirector,religion,remotecontrol,ringtones,science_astronomy,science_chemistry,sex_education,sex_lingerie,shopping,socialnet,spyware,tracker,updatesitesurlshortener,violence,warez,weapons,webphone,webradio,webtv
acl allowed_ips src "/opt/custom/proxy-settings/allowed.ips"
acl allowed_extra1_ips src "/opt/custom/proxy-settings/allowed.extra1.ips"
acl allowed_groups external nt_group "/opt/custom/proxy-settings/allowed.groups"
acl allowed_domains dstdomain "/opt/custom/proxy-settings/allowed.domains"
acl allowed_domains_filetypes dstdomain "/opt/custom/proxy-settings/allowed.domains.filetypes"
acl allowed_domains_mimetypes dstdomain "/opt/custom/proxy-settings/allowed.domains.mimetypes"
acl denied_domains dstdomain -i "/opt/custom/proxy-settings/denied.domains"
acl denied_extra1_domains dstdomain -i "/opt/custom/proxy-settings/denied.extra1.domains"
acl denied_ads url_regex "/opt/custom/proxy-settings/denied.ads"
acl denied_filetypes urlpath_regex -i "/opt/custom/proxy-settings/denied.filetypes"
acl denied_mimetypes_req req_mime_type -i "/opt/custom/proxy-settings/denied.mimetypes"
acl denied_extra1_mimetypes_req req_mime_type -i "/opt/custom/proxy-settings/denied.extra1.mimetypes"
acl denied_mimetypes_rep rep_mime_type -i "/opt/custom/proxy-settings/denied.mimetypes"
acl denied_extra1_mimetypes_rep rep_mime_type -i "/opt/custom/proxy-settings/denied.extra1.mimetypes"
acl denied_restricted1_mimetypes_req req_mime_type -i "/opt/custom/proxy-settings/denied.restricted1.mimetypes"
acl denied_restricted1_mimetypes_rep rep_mime_type -i "/opt/custom/proxy-settings/denied.restricted1.mimetypes"
acl allowed_restricted1_domains dstdomain -i "/opt/custom/proxy-settings/allowed.restricted1.domains"
acl allowed_restricted1_ips dst "/opt/custom/proxy-settings/allowed.restricted1.ips"
acl restricted_ips src "/opt/custom/proxy-settings/restricted.ips"
acl restricted_groups external nt_group "/opt/custom/proxy-settings/restricted.groups"
acl restricted_domains dstdomain "/opt/custom/proxy-settings/restricted.domains"
acl bl_lookup external bllookup
acl denied_urlshorteners dstdomain -i "/opt/custom/proxy-settings/db/HMANshallalist/urlshortener/domains"

http_access deny explicit !ORG_all
http_access deny explicit SSL_ports
http_access deny intercepted !localnet
http_access deny interceptedssl !localnet

http_access allow localnet !restricted_ips allowed_domains
http_access allow localnet !restricted_ips allowed_ips
http_reply_access allow localnet !restricted_ips allowed_ips
http_reply_access allow localnet !restricted_ips allowed_domains
http_access allow restricted_ips restricted_domains
http_access deny restricted_ips

http_access deny !allowed_ips denied_urlshorteners
http_access deny CONNECT !allowed_ips denied_urlshorteners
deny_info http://proxy-server1/proxy-error/?a=%a&B=%B&e=%e&E=%E&H=%H&i=%i&M=%M&o=%o&R=%R&T=%T&U=%U&u=%u&w=%w&x=%x&acl=denied_urlshorteners denied_urlshorteners

http_access allow denied_restricted1_mimetypes_req allowed_restricted1_domains
http_access allow denied_restricted1_mimetypes_req allowed_restricted1_ips
http_reply_access allow denied_restricted1_mimetypes_rep allowed_restricted1_domains
http_reply_access allow denied_restricted1_mimetypes_rep allowed_restricted1_ips

http_access allow denied_extra1_mimetypes_req allowed_extra1_ips denied_extra1_domains
http_reply_access allow denied_extra1_mimetypes_rep allowed_extra1_ips denied_extra1_domains

http_access deny denied_restricted1_mimetypes_req
http_reply_access deny denied_restricted1_mimetypes_rep

http_access deny denied_extra1_mimetypes_req
http_reply_access deny denied_extra1_mimetypes_rep

http_access deny !allowed_ips denied_domains
http_access deny CONNECT !allowed_ips denied_domains
deny_info http://proxy-server1/proxy-error/?a=%a&B=%B&e=%e&E=%E&H=%H&i=%i&M=%M&o=%o&R=%R&T=%T&U=%U&u=%u&w=%w&x=%x&acl=denied_domains denied_domains

http_access allow allowed_extra1_ips denied_extra1_domains
http_access deny denied_extra1_domains
deny_info http://proxy-server1/proxy-error/?a=%a&B=%B&e=%e&E=%E&H=%H&i=%i&M=%M&o=%o&R=%R&T=%T&U=%U&u=%u&w=%w&x=%x&acl=denied_extra1_domains denied_extra1_domains

http_access deny denied_filetypes !allowed_domains_filetypes
http_reply_access deny denied_filetypes !allowed_domains_filetypes
deny_info http://proxy-server1/proxy-error/?a=%a&B=%B&e=%e&E=%E&H=%H&i=%i&M=%M&o=%o&R=%R&T=%T&U=%U&u=%u&w=%w&x=%x&acl=denied_filetypes denied_filetypes

http_access deny denied_mimetypes_req !allowed_domains_mimetypes
http_reply_access deny denied_mimetypes_rep !allowed_domains_mimetypes
deny_info http://proxy-server1/proxy-error/?a=%a&B=%B&e=%e&E=%E&H=%H&i=%i&M=%M&o=%o&R=%R&T=%T&U=%U&u=%u&w=%w&x=%x&acl=denied_mimetypes denied_mimetypes_req
deny_info http://proxy-server1/proxy-error/?a=%a&B=%B&e=%e&E=%E&H=%H&i=%i&M=%M&o=%o&R=%R&T=%T&U=%U&u=%u&w=%w&x=%x&acl=denied_mimetypes denied_mimetypes_rep

http_access allow localnet bl_lookup

----

I understand Squid accepts the REQUEST, but not the REPLY as it matches denied_restricted1_mimetypes_rep. However, I don't understand why the client browser doesn't display the deny_info page at http://proxy-server1/proxy-error/?a=%a&B=%B&e=%e&E=%E&H=%H&i=%i&M=%M&o=%o&R=%R&T=%T&U=%U&u=%u&w=%w&x=%x&acl=denied_mimetypes. Instead, it shows ERR_ACCESS_DENIED.

Thanks,

Vieri


From dernikov1 at gmail.com  Tue Nov 14 15:11:03 2017
From: dernikov1 at gmail.com (Bike dernikov1)
Date: Tue, 14 Nov 2017 16:11:03 +0100
Subject: [squid-users] SQUID memory error after vm.swappines changed
 from 60 to 10
In-Reply-To: <733e9e3e-c863-aee0-8d42-9cc84c8e7799@urlfilterdb.com>
References: <CAAPixKhsif5+ad1prsoFAZXVXcVAxpdqkw4RdvGbfK-UxhkzTQ@mail.gmail.com>
 <dab0b62b-57a5-5718-b0bd-70fb3de0c124@urlfilterdb.com>
 <CAAPixKjepL2BEePSH83XeBNB+UPy4wKoqjTn0_Axoa+n0THo7w@mail.gmail.com>
 <daf2a3ca-d5bd-f84c-0182-099f845c4e61@urlfilterdb.com>
 <CAAPixKhCKhySLEZiUi1sA-hBwQ74KxM1XDN-jKEFL5B3=hfoeQ@mail.gmail.com>
 <a7c29c43-131c-8f0b-fa66-88db16d0f8e5@urlfilterdb.com>
 <CAAPixKjhfEEXeoY7GHRCo=2Pmc3s0HqQkGPWTEH5LOYCfD3YCQ@mail.gmail.com>
 <6c7dbab4-fe9c-c543-7afa-9cea677c1de7@urlfilterdb.com>
 <CAAPixKizv3srCOXaLv7Cn41-f9quoP0TSNZPiscKgWH77aBP1Q@mail.gmail.com>
 <a0dd0ba2-3995-735e-d3b7-8ce66330c951@urlfilterdb.com>
 <CAAPixKiLTGrCGt3Tb6eHYB0kH75YB8J1ZGaRKtHXzJ=n5BoF=w@mail.gmail.com>
 <733e9e3e-c863-aee0-8d42-9cc84c8e7799@urlfilterdb.com>
Message-ID: <CAAPixKg1LYYK-t7hg_mY7CBAYLSuLOqD9tDHn_-mzzQPKEC7BQ@mail.gmail.com>

On Mon, Nov 13, 2017 at 2:32 PM, Marcus Kool
<marcus.kool at urlfilterdb.com> wrote:
>
>
> On 13/11/17 10:46, Bike dernikov1 wrote:
>>
>> On Mon, Nov 13, 2017 at 12:15 PM, Marcus Kool
>> <marcus.kool at urlfilterdb.com> wrote:
>>>
>>>
>>>
>>> On 13/11/17 07:46, Bike dernikov1 wrote:
>>>
>>>>> are you saying that you have
>>>>>      cache_mem 14G
>>>>> If yes, you should read the memory FAQ and reduce this.
>>>>> 'cache_mem 14G' explains that Squid starts 'small' and grows over time.
>>>>
>>>>
>>>>
>>>> For our case, what do you recomend.  10GB or even lower ?
>>>> Plan reading today, i hope that I will have peace, to concentrate.
>>>
>>>
>>>
>>> cache_mem does NOT define the total memory use of Squid.
>>> The FAQ explains it.
>>> On a 24G system you can start with 7 GB and only after 3 days of running
>>> without issues and verifying that the cache is 100% utilised (if not,
>>> Squid
>>> can grow) and there is sufficient free memory, you can increase it.
>>>
>>
>> Read FAQ.
>> Now  trying to pass trough squid-internal-mgr/ reports/statistics.
>> For now we will stay at cache_mem 14GB, because we are modifying too
>> many settings at same time. Now at 99% used at 14GB if i read correctly.
>> Thanks for sugestions and help.
>>
>>
>> squid-internal-mgr/info output:
>> Cache information for squid:
>> Hits as % of all requests: 5min: 5.8%, 60min: 6.4%
>> Hits as % of bytes sent: 5min: 16.8%, 60min: 18.7%
>> Memory hits as % of hit requests: 5min: 64.1%, 60min: 64.9%
>> Disk hits as % of hit requests: 5min: 0.0%, 60min: 0.1%
>> Storage Swap size: 0 KB
>> Storage Swap capacity: 0.0% used,  0.0% free
>> Storage Mem size: 14195856 KB
>> Storage Mem capacity: 99.0% used,  1.0% free
>> Mean Object Size: 0.00 KB
>> Requests given to unlinkd: 0
>
>
> Beware that he storage mem size is not the same as total memory used.

It is not same but not too much difference. No problems so far today :)

              total        used        free      shared  buff/cache   available
Mem:          24101       17008         695         134        6397        6543
Swap:         24561         342       24219



> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users


From akash.kumar.patel at appdirect.com  Tue Nov 14 15:19:22 2017
From: akash.kumar.patel at appdirect.com (tappdint)
Date: Tue, 14 Nov 2017 08:19:22 -0700 (MST)
Subject: [squid-users] Proxy does not send response for internal host
In-Reply-To: <7e0807cf-74e3-31b1-db2a-f0ec619220ce@treenet.co.nz>
References: <1510611675802-0.post@n4.nabble.com>
 <7e0807cf-74e3-31b1-db2a-f0ec619220ce@treenet.co.nz>
Message-ID: <1510672762332-0.post@n4.nabble.com>

Amos Jeffries wrote
> You need to either;
> 
>   * configure the VM running Squid to use a DNS resolver that knows what 
> the internal domains are, or
> 
>   * configure the DNS resolver it is using to know those internal 
> hostnames, or
> 
>   * configure your network using the standardized .local namespace 
> intended for local hostnames and all the VMs to use m-DNS.

Just wanted to clarify/confirm. My squid and host app containers are running
locally on my mac (no VM) and the application that requests said host is the
one running in the VM. Are the 3 options you provided still valid? 

For the first option I was thinking of figuring out what DNS Resolvers my
Mac uses to resolve hosts and add those to my squid.conf using the
*dns_nameservers* config (along with the existing ones already in Squid's
resolv.conf). Would that be the correct approach?




--
Sent from: http://squid-web-proxy-cache.1019090.n4.nabble.com/Squid-Users-f1019091.html


From dernikov1 at gmail.com  Tue Nov 14 15:41:04 2017
From: dernikov1 at gmail.com (Bike dernikov1)
Date: Tue, 14 Nov 2017 16:41:04 +0100
Subject: [squid-users] SQUID memory error after vm.swappines changed
 from 60 to 10
In-Reply-To: <f8132fc7-58b0-e427-d7ae-60204da73e3e@measurement-factory.com>
References: <CAAPixKhsif5+ad1prsoFAZXVXcVAxpdqkw4RdvGbfK-UxhkzTQ@mail.gmail.com>
 <86188b87-0cc3-343c-7b56-7f01354a3aa8@measurement-factory.com>
 <CAAPixKiTszJDHUDK__PFgQ_+g0kR0RX4KZUNykFANKnTqzydsg@mail.gmail.com>
 <bfb6f0fc-357b-2156-7cec-e43e9718d521@measurement-factory.com>
 <CAAPixKhNB-1zkFP5X=b7f8__XEY0-9XtEGi8N0EkXTGB6kbygw@mail.gmail.com>
 <bd5c2853-4480-4808-d1d4-9d8b1a8f6b0d@measurement-factory.com>
 <CAAPixKgzxz_hCWxW+T5TBuBFqG4z6b3KFy0-9qE31Vq33MWsmw@mail.gmail.com>
 <f8132fc7-58b0-e427-d7ae-60204da73e3e@measurement-factory.com>
Message-ID: <CAAPixKh2Prf7PgQvbqzEiUKgdGLW-jQvDBzdNJNoqJPuxUYwRA@mail.gmail.com>

On Mon, Nov 13, 2017 at 4:36 PM, Alex Rousskov
<rousskov at measurement-factory.com> wrote:
> On 11/13/2017 02:34 AM, Bike dernikov1 wrote:
>> On Fri, Nov 10, 2017 at 4:43 PM, Alex Rousskov wrote:
>>> Squid swapping in production is an arguably worse disaster, as you have
>>> learned. In many cases, it is better to deal with a lack of swap than to
>>> rely on swap's magical effects that most humans poorly understand. YMMV.
>
>> In this scenario, swap is backup cache (as I understand)?
>
> In this scenario, swap is not a cache! In fact it is pretty much the
> opposite:
>
> * A cache is, by definition, an optional unreliable "fast" storage meant
> to reduce the need to go to some "slow" storage.
>
> * When in active use, swap is required reliable slow storage meant to
> extend fast storage (RAM) capacity.
>
> Do you see how almost every adjective in the first bullet is replaced
> with an antonym in the second one?

Yes, definitely mixed up, I meant right but wrote wrong.

> Some services, including many databases, over allocate RAM to store
> rarely used (computed and/or preloaded) data. When that data is swapped
> out, the service often continues to operate normally because the data is
> rarely accessed (and/or because swapping it in is still cheaper than
> computing it from scratch).
>
> With Squid, it is very difficult for the OS to correctly identify the
> rarely used RAM areas to swap out. When the OS swaps out the wrong area,
> Squid slows down (to access that area), which only increases the number
> of concurrent transactions and, hence, the amount of RAM Squid needs to
> operate, which triggers more wrong swap outs, creating a vicious cycle.

That is why best solution would be swap of. Definitely testing swapoff
next week, if we won't have new problems.

>> Swap could be used  to translate back data to mem if used, but it
>> stays on disk and purge after some time if not used ?
>
> The purging bit is wrong. Think of swap as very very very slow RAM.
>
> Alex.

So, when squid need something from swap, it will load that data back to ram.
For purge, data then  stay in swap forever ?

Swap very slow ram, best explanation. Use that metaphor during
mentoring younger coworkers :).
Thanks for explaining swap memory problems.


From rousskov at measurement-factory.com  Tue Nov 14 17:12:36 2017
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Tue, 14 Nov 2017 10:12:36 -0700
Subject: [squid-users] SQUID memory error after vm.swappines changed
 from 60 to 10
In-Reply-To: <CAAPixKh2Prf7PgQvbqzEiUKgdGLW-jQvDBzdNJNoqJPuxUYwRA@mail.gmail.com>
References: <CAAPixKhsif5+ad1prsoFAZXVXcVAxpdqkw4RdvGbfK-UxhkzTQ@mail.gmail.com>
 <86188b87-0cc3-343c-7b56-7f01354a3aa8@measurement-factory.com>
 <CAAPixKiTszJDHUDK__PFgQ_+g0kR0RX4KZUNykFANKnTqzydsg@mail.gmail.com>
 <bfb6f0fc-357b-2156-7cec-e43e9718d521@measurement-factory.com>
 <CAAPixKhNB-1zkFP5X=b7f8__XEY0-9XtEGi8N0EkXTGB6kbygw@mail.gmail.com>
 <bd5c2853-4480-4808-d1d4-9d8b1a8f6b0d@measurement-factory.com>
 <CAAPixKgzxz_hCWxW+T5TBuBFqG4z6b3KFy0-9qE31Vq33MWsmw@mail.gmail.com>
 <f8132fc7-58b0-e427-d7ae-60204da73e3e@measurement-factory.com>
 <CAAPixKh2Prf7PgQvbqzEiUKgdGLW-jQvDBzdNJNoqJPuxUYwRA@mail.gmail.com>
Message-ID: <cf19e140-7d8b-d216-d2c1-b8d4cdd8eb6b@measurement-factory.com>

On 11/14/2017 08:41 AM, Bike dernikov1 wrote:
> On Mon, Nov 13, 2017 at 4:36 PM, Alex Rousskov wrote:
>> On 11/13/2017 02:34 AM, Bike dernikov1 wrote:
>>> Swap could be used  to translate back data to mem if used, but it
>>> stays on disk and purge after some time if not used ?

>> The purging bit is wrong. Think of swap as very very very slow RAM.

> So, when squid need something from swap, it will load that data back to ram.

In this context, swap is an OS-level concept. Squid does not know that
the OS memory manager has swapped some of Squid data from RAM to disk.
OS does not know what swapped out data means to Squid. When Squid tries
to access data at a swapped out address, the OS blocks the Squid process
and loads the missing data from disk into RAM (usually after swapping
out some RAM bytes to free RAM space for those requested bytes).

Here, "data" essentially means any sequence of bytes allocated by Squid.
For example, some of those swapped out bytes may have nothing to do with
Squid memory cache. Swapped out bytes can even be Squid binary code.


> For purge, data then  stay in swap forever ?

Swapped out process data stays swapped out until it is either accessed
by the process (and is swapped in by the OS) or the process terminates
(without accessing those swapped out bytes). The latter is unlikely for
Squid data unless the Squid process dies prematurely (i.e., without
doing internal cleanup).


HTH,

Alex.


From ash.benz at bk.ru  Wed Nov 15 06:51:58 2017
From: ash.benz at bk.ru (A. Benz)
Date: Wed, 15 Nov 2017 14:51:58 +0800
Subject: [squid-users] Non intrusive sslbump for whitelisting (asked
 many times but..)
In-Reply-To: <2b9b21dd-1ef1-deaf-ce40-3ad4775c1ec2@treenet.co.nz>
References: <ae8fe086-e227-6b58-c6d8-f4e37d062eee@bk.ru>
 <96e5ae3d-6dc6-fca3-7c70-b038ecf16ddf@treenet.co.nz>
 <75c1764d-1923-f6c0-1896-2c64d1190779@bk.ru>
 <7a55ca37-23d0-a0f3-6397-2a772f91745f@treenet.co.nz>
 <4563ade7-9bdf-0237-bf8e-2ac6842bdc25@bk.ru>
 <0de1efc3-3587-1584-0d32-735974f39600@treenet.co.nz>
 <2b9b21dd-1ef1-deaf-ce40-3ad4775c1ec2@treenet.co.nz>
Message-ID: <c21364b8-7736-1ab1-3e81-7c2b3e26bd32@bk.ru>

Hi Amos,

Just wanted to follow up on this saying thanks for taking the time to reply.

Cheers.

Regards,
A. Benz

On 11/11/17 09:54, Amos Jeffries wrote:
> On 11/11/17 14:03, Amos Jeffries wrote:
>> On 11/11/17 01:05, A. Benz wrote:
>>> Hi Amos,
>>>
>>> Thanks for your continued support.
>>>
>>> 1.
>>>
>>>> Do you mean the VPN exit point has that 10/8 IP address? or that 
>>>> the traffic from the client is altered to be going to that IP 
>>>> before it reaches Squid?
>>>>
>>>> The latter is broken because it destroys the original dst-IP values 
>>>> on the TCP connection. Which Squid needs to setup the server 
>>>> connection. 
>>>
>>> Let me put it as an example:
>>>
>>> ?From the normal internet: mail.amosprivateserver.org > publicly 
>>> accessible IP.
>>>
>>> ?From my place: mail.amosprivateserver.org > 10.x.x.x (corporate 
>>> network, accessible only from within the place).
>>>
>>> Anyways no worries about this! I decided to make an exception in the 
>>> redirect rule, so that if the outgoing traffic matches the IP 
>>> 10.x.x.x then the firewall will not redirect the traffic to squid 
>>> and instead establish a connection directly.
>>>
>>> This is not ideal, but it works.
>>>
>>
>> Or have Squid relay everything through the same server(s) and
>> the server do the distinguishing between traffic and just relay 
>> everythign to the same
>>
>
> Damn that sounds daft.
>
> What I meant to write was:
>
> Or have Squid relay everything through the same server(s) and
> the server do the distinguishing between traffic .
>
> Or setup a cache_peer and have the traffic with src IP of the internal 
> clients going to that domain sent there.
>
> Amos
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users



From rentorbuy at yahoo.com  Wed Nov 15 11:18:30 2017
From: rentorbuy at yahoo.com (Vieri)
Date: Wed, 15 Nov 2017 11:18:30 +0000 (UTC)
Subject: [squid-users] block user agent
References: <160200155.402986.1510744710954.ref@mail.yahoo.com>
Message-ID: <160200155.402986.1510744710954@mail.yahoo.com>

Hi,

I'm trying to block some user agents (I know it's easy to fake, but most users won't try to fake that header value).

The following works:

acl denied_useragent browser Chrome
acl denied_useragent browser MSIE
acl denied_useragent browser Opera
acl denied_useragent browser Trident
[...]
http_access deny denied_useragent
http_reply_access deny denied_useragent
deny_info http://proxy-server1/proxy-error/?a=%a&B=%B&e=%e&E=%E&H=%H&i=%i&M=%M&o=%o&R=%R&T=%T&U=%U&u=%u&w=%w&x=%x&acl=denied_useragent denied_useragent

The following works for HTTP sites, but not for HTTPS sites in an ssl-bumped setup:

acl allowed_useragent browser Firefox/
[...]
http_access deny !allowed_useragent
deny_info http://proxy-server1/proxy-error/?a=%a&B=%B&e=%e&E=%E&H=%H&i=%i&M=%M&o=%o&R=%R&T=%T&U=%U&u=%u&w=%w&x=%x&acl=allowed_useragent allowed_useragent


What could I try?

Thanks,

Vieri


From dernikov1 at gmail.com  Wed Nov 15 12:32:21 2017
From: dernikov1 at gmail.com (Bike dernikov1)
Date: Wed, 15 Nov 2017 13:32:21 +0100
Subject: [squid-users] SQUID memory error after vm.swappines changed
 from 60 to 10
In-Reply-To: <cf19e140-7d8b-d216-d2c1-b8d4cdd8eb6b@measurement-factory.com>
References: <CAAPixKhsif5+ad1prsoFAZXVXcVAxpdqkw4RdvGbfK-UxhkzTQ@mail.gmail.com>
 <86188b87-0cc3-343c-7b56-7f01354a3aa8@measurement-factory.com>
 <CAAPixKiTszJDHUDK__PFgQ_+g0kR0RX4KZUNykFANKnTqzydsg@mail.gmail.com>
 <bfb6f0fc-357b-2156-7cec-e43e9718d521@measurement-factory.com>
 <CAAPixKhNB-1zkFP5X=b7f8__XEY0-9XtEGi8N0EkXTGB6kbygw@mail.gmail.com>
 <bd5c2853-4480-4808-d1d4-9d8b1a8f6b0d@measurement-factory.com>
 <CAAPixKgzxz_hCWxW+T5TBuBFqG4z6b3KFy0-9qE31Vq33MWsmw@mail.gmail.com>
 <f8132fc7-58b0-e427-d7ae-60204da73e3e@measurement-factory.com>
 <CAAPixKh2Prf7PgQvbqzEiUKgdGLW-jQvDBzdNJNoqJPuxUYwRA@mail.gmail.com>
 <cf19e140-7d8b-d216-d2c1-b8d4cdd8eb6b@measurement-factory.com>
Message-ID: <CAAPixKjyVQmEuHhnq5hMdv4q6tKdVFkcUJw_H8Cj7GbZbgHJ+w@mail.gmail.com>

On Tue, Nov 14, 2017 at 6:12 PM, Alex Rousskov
<rousskov at measurement-factory.com> wrote:
> On 11/14/2017 08:41 AM, Bike dernikov1 wrote:
>> On Mon, Nov 13, 2017 at 4:36 PM, Alex Rousskov wrote:
>>> On 11/13/2017 02:34 AM, Bike dernikov1 wrote:
>>>> Swap could be used  to translate back data to mem if used, but it
>>>> stays on disk and purge after some time if not used ?
>
>>> The purging bit is wrong. Think of swap as very very very slow RAM.
>
>> So, when squid need something from swap, it will load that data back to ram.
>
> In this context, swap is an OS-level concept. Squid does not know that
> the OS memory manager has swapped some of Squid data from RAM to disk.
> OS does not know what swapped out data means to Squid. When Squid tries
> to access data at a swapped out address, the OS blocks the Squid process
> and loads the missing data from disk into RAM (usually after swapping
> out some RAM bytes to free RAM space for those requested bytes).
>
> Here, "data" essentially means any sequence of bytes allocated by Squid.
> For example, some of those swapped out bytes may have nothing to do with
> Squid memory cache. Swapped out bytes can even be Squid binary code.
>
>
>> For purge, data then  stay in swap forever ?
>
> Swapped out process data stays swapped out until it is either accessed
> by the process (and is swapped in by the OS) or the process terminates
> (without accessing those swapped out bytes). The latter is unlikely for
> Squid data unless the Squid process dies prematurely (i.e., without
> doing internal cleanup).
>
> HTH,
>
> Alex.

Superb explanation. I think that now I understand process much better.

If i can ask under same title:
Yesterday we had error in logs: syslog, cache.log, dmesg,access.log

segfault at 8 ip ....... sp ..... error 4 is squid
process pid exited due to signal 11 with status 0

Squid restarted,  that was at the end of work, and i didn't  notice
change while surfing.
I noticed change in used memory, after i went trough logs, and found segfault.

Can you point me, how to analyze what happened.
Can that be problem with kernel ?

Thanks for help.


From dernikov1 at gmail.com  Wed Nov 15 12:43:41 2017
From: dernikov1 at gmail.com (Bike dernikov1)
Date: Wed, 15 Nov 2017 13:43:41 +0100
Subject: [squid-users] Is it possible to apply squid delay pools on
	users/groups from AD ?
Message-ID: <CAAPixKjCmVuYH05zUGqkpON6znrozmjW2s1XRF5Y05+pOx=aDw@mail.gmail.com>

Hi,
this is my second topic, i wouldn't wan to mix with first. I hope that is ok.
i hope that someone succeeded  to apply delay pools on users/groups from AD.
We are now using  delay pool  on whole 10.0.0.0/8, but that is a
problem as different users have different requirements.   We have 30
locations, and we can set different rules by ip, but than we would
need one rule for one location, we would need to use static ip,
network reconfiguration, but that solution would be nightmare for
administration, and we would like to avoid static ip-s for users.
Thanks for help.


From joe.e.foster at googlemail.com  Wed Nov 15 13:32:15 2017
From: joe.e.foster at googlemail.com (Joe Foster)
Date: Wed, 15 Nov 2017 13:32:15 +0000
Subject: [squid-users] SSL Bump for regex URL comparison
Message-ID: <1510752735.2344.0.camel@gmail.com>

Good afternoon,

I have a small router onto which I have installed Squid.

I am trying to filter HTTPS urls for bad words on a blocked list. 

It will require the client on the safe side of the router to install the
certificate, this isn't an issue as it's an open process and not an
illigal MITM attack. 

Below is my squid.conf

As you will see I have been playing around with where to put the code
and what code to put in. 

I only have a small amount of flash drive so I have put the auto-gen
cert directory in /tmp/. I am aware this is volatile memory but until I
have a better solution I will be doing this. 

I have put a firewall rule in to forward 443 to 3128.

https://wiki.squid-cache.org/Features/SslBump
https://wiki.squid-cache.org/SquidFaq/SquidAcl

I also don't want to cache due to flash drive issues. Is this possible?

Its the same cert in /root/ and /certs/ before anyone points it out. 

Nothing has been appearing in the log files either but this is no
surprise.

Been up till 1am last few nights on this so you assistance is very
appreciated. 

Thank you very much,

Joe

acl localnet src 10.0.0.0/8 
acl localnet src 172.16.0.0/12 
acl localnet src 192.168.1.0/16 
acl localnet src fc00::/7 
acl localnet src fe80::/10 
 
acl ssl_ports port 443 
 
acl safe_ports port 80 
acl safe_ports port 21 
acl safe_ports port 443 
acl safe_ports port 70 
acl safe_ports port 210 
acl safe_ports port 1025-65535 
acl safe_ports port 280 
acl safe_ports port 488 
acl safe_ports port 591 
acl safe_ports port 777 
acl connect method connect 
 
#acl safe_ports port 3128 
http_port 3128 ssl-bump generate-host-certificates=on
dynamic_cert_mem_cache_size=1MB cert=/root/myCA.pem 
 
http_access deny !safe_ports 
http_access deny connect !ssl_ports 
 
http_access allow localhost manager 
http_access deny manager 
 
http_access deny to_localhost 
 
http_access allow localnet 
http_access allow localhost 
 
#http_port 3128 intercept 

acl BadWords url_regex "/etc/badwords"
http_access deny Badwords
 
cache deny all 
 
#ssl_bump bump all 
#http_port 3128 ssl-bump generate-host-certificates=on
dynamic_cert_mem_cache_size=1MB cert=/root/myCA.pem 
 
http_access deny all 
 
refresh_pattern ^ftp: 1440 20% 10080 
refresh_pattern ^gopher: 1440 0% 1440 
refresh_pattern -i (/cgi-bin/|\?) 0 0% 0 
refresh_pattern . 0 20% 4320 
 
access_log /squid.log 
cache_log /squid1.log 
cache_store_log stdio:/squid2.log 
logfile_rotate 0 
 
logfile_daemon /dev/null 
 
#http_port 3128 intercept 
 
#cache deny all 
 
#ssl_bump bump all 
#http_port 3128 ssl-bump generate-host-certificates=on
dynamic_cert_mem_cache_size=1MB cert=/root/myCA.pem 




From rentorbuy at yahoo.com  Thu Nov 16 07:26:50 2017
From: rentorbuy at yahoo.com (Vieri)
Date: Thu, 16 Nov 2017 07:26:50 +0000 (UTC)
Subject: [squid-users] cannot set pid_filename in an include
References: <524676651.1015231.1510817210794.ref@mail.yahoo.com>
Message-ID: <524676651.1015231.1510817210794@mail.yahoo.com>

Hi,

Correct me if I'm wrong, but this may be a parsing bug:

# /etc/init.d/squid.test start
* /etc/squid/squid.test.conf must set pid_filename to
*    /run/squid.test.pid


However, I have:

# grep include /etc/squid/squid.test.conf
include /etc/squid/squid.custom.test
include /etc/squid/squid.custom.rules.test


# grep pid_filename /etc/squid/squid.custom.test
pid_filename /run/squid.test.pid

Squid Object Cache: Version 3.5.27-20171101-re69e56c

Thanks,

Vieri


From squid3 at treenet.co.nz  Thu Nov 16 07:40:28 2017
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 16 Nov 2017 20:40:28 +1300
Subject: [squid-users] deny_info
In-Reply-To: <1727217364.194806.1510652811222@mail.yahoo.com>
References: <1727217364.194806.1510652811222.ref@mail.yahoo.com>
 <1727217364.194806.1510652811222@mail.yahoo.com>
Message-ID: <38d4ef6a-6311-6cf6-4f01-6074651c74c0@treenet.co.nz>

On 14/11/17 22:46, Vieri wrote:
> Hi,
> 
> I'm trying to figure out how to correctly handle ERROR pages (or deny pages) in one particular case.
> 
> An HTTP client is trying to access a website as https://example.org/.
> 
> I'm getting the following info in cache.log:
> 
> 2017/11/14 09:11:11.481 kid1| 85,2| client_side_request.cc(745) clientAccessCheckDone: The request GET https://example.org/ is ALLOWED; last ACL checked: bl_lookup
> 2017/11/14 09:11:11.481 kid1| 85,2| client_side_request.cc(721) clientAccessCheck2: No adapted_http_access configuration. default: ALLOW
> 2017/11/14 09:11:11.481 kid1| 85,2| client_side_request.cc(745) clientAccessCheckDone: The request GET https://example.org/ is ALLOWED; last ACL checked: bl_lookup
> 2017/11/14 09:11:11.591 kid1| 88,2| client_side_reply.cc(2073) processReplyAccessResult: The reply for GET https://example.org/ is DENIED, because it matched denied_restricted1_mimetypes_rep
> 2017/11/14 09:11:11.591 kid1| 88,2| client_side_reply.cc(2073) processReplyAccessResult: The reply for GET https://example.org/ is ALLOWED, because it matched denied_restricted1_mimetypes_rep
> 
> This is what I have in squid.conf (part of it):
> 
...
> acl denied_restricted1_mimetypes_rep rep_mime_type -i "/opt/custom/proxy-settings/denied.restricted1.mimetypes"
...
> http_reply_access allow denied_restricted1_mimetypes_rep allowed_restricted1_domains
> http_reply_access allow denied_restricted1_mimetypes_rep allowed_restricted1_ips
> 
...
> http_reply_access deny denied_restricted1_mimetypes_rep
> 
...
> 
> ----
> 
> I understand Squid accepts the REQUEST, but not the REPLY as it matches denied_restricted1_mimetypes_rep. However, I don't understand why the client browser doesn't display the deny_info page at http://proxy-server1/proxy-error/?a=%a&B=%B&e=%e&E=%E&H=%H&i=%i&M=%M&o=%o&R=%R&T=%T&U=%U&u=%u&w=%w&x=%x&acl=denied_mimetypes. Instead, it shows ERR_ACCESS_DENIED.


Because there are actually no custom deny_info attached to that 
"denied_restricted1_mimetypes_rep" ACL.


Amos


From squid3 at treenet.co.nz  Thu Nov 16 07:49:50 2017
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 16 Nov 2017 20:49:50 +1300
Subject: [squid-users] Proxy does not send response for internal host
In-Reply-To: <1510672762332-0.post@n4.nabble.com>
References: <1510611675802-0.post@n4.nabble.com>
 <7e0807cf-74e3-31b1-db2a-f0ec619220ce@treenet.co.nz>
 <1510672762332-0.post@n4.nabble.com>
Message-ID: <2ea66000-2d64-bf85-93fd-51876ff512c1@treenet.co.nz>

On 15/11/17 04:19, tappdint wrote:
> Amos Jeffries wrote
>> You need to either;
>>
>>    * configure the VM running Squid to use a DNS resolver that knows what
>> the internal domains are, or
>>
>>    * configure the DNS resolver it is using to know those internal
>> hostnames, or
>>
>>    * configure your network using the standardized .local namespace
>> intended for local hostnames and all the VMs to use m-DNS.
> 
> Just wanted to clarify/confirm. My squid and host app containers are running
> locally on my mac (no VM) and the application that requests said host is the
> one running in the VM. Are the 3 options you provided still valid?

Yes. Though there may be complications depending on whether the 
containers have their own DNS server settings or use the hosts.


> 
> For the first option I was thinking of figuring out what DNS Resolvers my
> Mac uses to resolve hosts and add those to my squid.conf using the
> *dns_nameservers* config (along with the existing ones already in Squid's
> resolv.conf). Would that be the correct approach?
> 

Squid should be loading the /etc/hosts and /etc/resolv.conf settings by 
default. You only need the dns_nameservers if you want to override the 
machines normal ones.

Amos


From rentorbuy at yahoo.com  Thu Nov 16 07:52:29 2017
From: rentorbuy at yahoo.com (Vieri)
Date: Thu, 16 Nov 2017 07:52:29 +0000 (UTC)
Subject: [squid-users] deny_info
In-Reply-To: <38d4ef6a-6311-6cf6-4f01-6074651c74c0@treenet.co.nz>
References: <1727217364.194806.1510652811222.ref@mail.yahoo.com>
 <1727217364.194806.1510652811222@mail.yahoo.com>
 <38d4ef6a-6311-6cf6-4f01-6074651c74c0@treenet.co.nz>
Message-ID: <1919900086.1067058.1510818749446@mail.yahoo.com>

________________________________
From: Amos Jeffries <squid3 at treenet.co.nz>
>
> Because there are actually no custom deny_info attached to that 
> "denied_restricted1_mimetypes_rep" ACL.


Right. I don't know how I missed that. Sorry.

Thanks again.

Vieri


From squid3 at treenet.co.nz  Thu Nov 16 07:55:11 2017
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 16 Nov 2017 20:55:11 +1300
Subject: [squid-users] block user agent
In-Reply-To: <160200155.402986.1510744710954@mail.yahoo.com>
References: <160200155.402986.1510744710954.ref@mail.yahoo.com>
 <160200155.402986.1510744710954@mail.yahoo.com>
Message-ID: <61ce5b3f-bb5e-235f-351c-f3b4ae339fd5@treenet.co.nz>

On 16/11/17 00:18, Vieri wrote:
> Hi,
> 
> I'm trying to block some user agents (I know it's easy to fake, but most users won't try to fake that header value).
> 
> The following works:
> 
> acl denied_useragent browser Chrome
> acl denied_useragent browser MSIE
> acl denied_useragent browser Opera
> acl denied_useragent browser Trident
> [...]
> http_access deny denied_useragent
> http_reply_access deny denied_useragent
> deny_info http://proxy-server1/proxy-error/?a=%a&B=%B&e=%e&E=%E&H=%H&i=%i&M=%M&o=%o&R=%R&T=%T&U=%U&u=%u&w=%w&x=%x&acl=denied_useragent denied_useragent
> 
> The following works for HTTP sites, but not for HTTPS sites in an ssl-bumped setup:
> 
> acl allowed_useragent browser Firefox/
> [...]
> http_access deny !allowed_useragent
> deny_info http://proxy-server1/proxy-error/?a=%a&B=%B&e=%e&E=%E&H=%H&i=%i&M=%M&o=%o&R=%R&T=%T&U=%U&u=%u&w=%w&x=%x&acl=allowed_useragent allowed_useragent
> 
> 
> What could I try?
> 

The User-Agent along with all HTTP layer details in HTTPS are hidden 
behind the encryption layer. TO do anything with them you must decrypt 
the traffic first. If you can decrypt it turns into regular HTTP traffic 
- the normal access controls should then work as-is.


Amos


From squid3 at treenet.co.nz  Thu Nov 16 07:58:35 2017
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 16 Nov 2017 20:58:35 +1300
Subject: [squid-users] SQUID memory error after vm.swappines changed
 from 60 to 10
In-Reply-To: <CAAPixKjyVQmEuHhnq5hMdv4q6tKdVFkcUJw_H8Cj7GbZbgHJ+w@mail.gmail.com>
References: <CAAPixKhsif5+ad1prsoFAZXVXcVAxpdqkw4RdvGbfK-UxhkzTQ@mail.gmail.com>
 <86188b87-0cc3-343c-7b56-7f01354a3aa8@measurement-factory.com>
 <CAAPixKiTszJDHUDK__PFgQ_+g0kR0RX4KZUNykFANKnTqzydsg@mail.gmail.com>
 <bfb6f0fc-357b-2156-7cec-e43e9718d521@measurement-factory.com>
 <CAAPixKhNB-1zkFP5X=b7f8__XEY0-9XtEGi8N0EkXTGB6kbygw@mail.gmail.com>
 <bd5c2853-4480-4808-d1d4-9d8b1a8f6b0d@measurement-factory.com>
 <CAAPixKgzxz_hCWxW+T5TBuBFqG4z6b3KFy0-9qE31Vq33MWsmw@mail.gmail.com>
 <f8132fc7-58b0-e427-d7ae-60204da73e3e@measurement-factory.com>
 <CAAPixKh2Prf7PgQvbqzEiUKgdGLW-jQvDBzdNJNoqJPuxUYwRA@mail.gmail.com>
 <cf19e140-7d8b-d216-d2c1-b8d4cdd8eb6b@measurement-factory.com>
 <CAAPixKjyVQmEuHhnq5hMdv4q6tKdVFkcUJw_H8Cj7GbZbgHJ+w@mail.gmail.com>
Message-ID: <ca66b2db-a288-db6b-c77e-8205c2541224@treenet.co.nz>

On 16/11/17 01:32, Bike dernikov1 wrote:
> 
> If i can ask under same title:
> Yesterday we had error in logs: syslog, cache.log, dmesg,access.log
> 
> segfault at 8 ip ....... sp ..... error 4 is squid
> process pid exited due to signal 11 with status 0
> 
> Squid restarted,  that was at the end of work, and i didn't  notice
> change while surfing.
> I noticed change in used memory, after i went trough logs, and found segfault.
> 
> Can you point me, how to analyze what happened.
> Can that be problem with kernel ?
> 

How to retrieve info about these type of things is detailed at 
<https://wiki.squid-cache.org/SquidFaq/BugReporting>.

NP: If you do not have core files enabled, then the data from that 
segfault is probably gone irretrievably. You may need to use the script 
to capture segfault details from a running proxy (the 'minimal downtime' 
section).


Amos


From squid3 at treenet.co.nz  Thu Nov 16 08:02:35 2017
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 16 Nov 2017 21:02:35 +1300
Subject: [squid-users] Is it possible to apply squid delay pools on
 users/groups from AD ?
In-Reply-To: <CAAPixKjCmVuYH05zUGqkpON6znrozmjW2s1XRF5Y05+pOx=aDw@mail.gmail.com>
References: <CAAPixKjCmVuYH05zUGqkpON6znrozmjW2s1XRF5Y05+pOx=aDw@mail.gmail.com>
Message-ID: <7810701a-8fb1-7930-d513-498f7cd09564@treenet.co.nz>

On 16/11/17 01:43, Bike dernikov1 wrote:
> Hi,
> this is my second topic, i wouldn't wan to mix with first. I hope that is ok.
> i hope that someone succeeded  to apply delay pools on users/groups from AD.
> We are now using  delay pool  on whole 10.0.0.0/8, but that is a
> problem as different users have different requirements.   We have 30
> locations, and we can set different rules by ip, but than we would
> need one rule for one location, we would need to use static ip,
> network reconfiguration, but that solution would be nightmare for
> administration, and we would like to avoid static ip-s for users.

It depends on your Squid version.

The latest Squid with annotation support are capable of receiving 
user/group names from the auth and external ACL helpers. These get 
attached to the transaction and can be matched with the 'note' type ACL 
in any later 'fast-category' access controls like delay_pools.

If your Squid is too old to use note ACL, or your helper(s) not 
providing the relevant details to Squid (in Squid-3.4+ helper syntax). 
Then no, sorry.

Amos


From squid3 at treenet.co.nz  Thu Nov 16 08:15:02 2017
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 16 Nov 2017 21:15:02 +1300
Subject: [squid-users] SSL Bump for regex URL comparison
In-Reply-To: <1510752735.2344.0.camel@gmail.com>
References: <1510752735.2344.0.camel@gmail.com>
Message-ID: <cb402607-bdd4-d157-48a8-9ad4e4aa5c30@treenet.co.nz>

On 16/11/17 02:32, Joe Foster wrote:
> Good afternoon,
> 
> I have a small router onto which I have installed Squid.
> 
> I am trying to filter HTTPS urls for bad words on a blocked list.
> 
> It will require the client on the safe side of the router to install the
> certificate, this isn't an issue as it's an open process and not an
> illigal MITM attack.
> 
> Below is my squid.conf
> 
> As you will see I have been playing around with where to put the code
> and what code to put in.
> 
> I only have a small amount of flash drive so I have put the auto-gen
> cert directory in /tmp/. I am aware this is volatile memory but until I
> have a better solution I will be doing this.

Since /tmp is subject to random deletion of content you will need to 
make sure you always shutdown Squid and re-run the ssl_crtd (etc.) 
create command to re-generate the cert DB structures whenever the device 
erases its /tmp content. Otherwise your proxy will crash and/or client 
connections will start being terminated with strange looking errors.


IMO you would probably be better off setting the cert DB to a very small 
size suitable for your limited space - or disabling it entirely [more on 
that below].

> 
> I have put a firewall rule in to forward 443 to 3128.
> 
> https://wiki.squid-cache.org/Features/SslBump
> https://wiki.squid-cache.org/SquidFaq/SquidAcl
> 
> I also don't want to cache due to flash drive issues. Is this possible?
> 

 From the documentation of the SSL-Bump settings:
  <http://www.squid-cache.org/Doc/config/http_port/>
"
   dynamic_cert_mem_cache_size=SIZE
     Approximate total RAM size spent on cached generated
     certificates. If set to zero, caching is disabled. The
     default value is 4MB.
"

> Its the same cert in /root/ and /certs/ before anyone points it out.
> 
> Nothing has been appearing in the log files either but this is no
> surprise.
> 
> Been up till 1am last few nights on this so you assistance is very
> appreciated.

That sounds like you are having a problem. But I don't see any mention 
of what that is exactly.

Amos


From joe.e.foster at googlemail.com  Thu Nov 16 08:21:42 2017
From: joe.e.foster at googlemail.com (Joe Foster)
Date: Thu, 16 Nov 2017 08:21:42 +0000
Subject: [squid-users] SSL Bump for regex URL comparison
In-Reply-To: <cb402607-bdd4-d157-48a8-9ad4e4aa5c30@treenet.co.nz>
References: <1510752735.2344.0.camel@gmail.com>
 <cb402607-bdd4-d157-48a8-9ad4e4aa5c30@treenet.co.nz>
Message-ID: <CAPzmcFuuZ92PtZrURV5MpuLtCBefPuASV61paUTe4DBHjXz8PQ@mail.gmail.com>

Hello Amos,

The problem is the connections are not getting through. It just acts like
there is no WiFi connection.

Adding the cert db every start up isn?t an issue.

I was thinking of having a small cert cache locally instead thinking about
it since.

The connections just aren?t being made. No ssl warning.

Thank you

Joe


On Thu, 16 Nov 2017 at 08:15, Amos Jeffries <squid3 at treenet.co.nz> wrote:

> On 16/11/17 02:32, Joe Foster wrote:
> > Good afternoon,
> >
> > I have a small router onto which I have installed Squid.
> >
> > I am trying to filter HTTPS urls for bad words on a blocked list.
> >
> > It will require the client on the safe side of the router to install the
> > certificate, this isn't an issue as it's an open process and not an
> > illigal MITM attack.
> >
> > Below is my squid.conf
> >
> > As you will see I have been playing around with where to put the code
> > and what code to put in.
> >
> > I only have a small amount of flash drive so I have put the auto-gen
> > cert directory in /tmp/. I am aware this is volatile memory but until I
> > have a better solution I will be doing this.
>
> Since /tmp is subject to random deletion of content you will need to
> make sure you always shutdown Squid and re-run the ssl_crtd (etc.)
> create command to re-generate the cert DB structures whenever the device
> erases its /tmp content. Otherwise your proxy will crash and/or client
> connections will start being terminated with strange looking errors.
>
>
> IMO you would probably be better off setting the cert DB to a very small
> size suitable for your limited space - or disabling it entirely [more on
> that below].
>
> >
> > I have put a firewall rule in to forward 443 to 3128.
> >
> > https://wiki.squid-cache.org/Features/SslBump
> > https://wiki.squid-cache.org/SquidFaq/SquidAcl
> >
> > I also don't want to cache due to flash drive issues. Is this possible?
> >
>
>  From the documentation of the SSL-Bump settings:
>   <http://www.squid-cache.org/Doc/config/http_port/>
> "
>    dynamic_cert_mem_cache_size=SIZE
>      Approximate total RAM size spent on cached generated
>      certificates. If set to zero, caching is disabled. The
>      default value is 4MB.
> "
>
> > Its the same cert in /root/ and /certs/ before anyone points it out.
> >
> > Nothing has been appearing in the log files either but this is no
> > surprise.
> >
> > Been up till 1am last few nights on this so you assistance is very
> > appreciated.
>
> That sounds like you are having a problem. But I don't see any mention
> of what that is exactly.
>
> Amos
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20171116/87b8a33c/attachment.htm>

From squid3 at treenet.co.nz  Thu Nov 16 08:23:16 2017
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 16 Nov 2017 21:23:16 +1300
Subject: [squid-users] cannot set pid_filename in an include
In-Reply-To: <524676651.1015231.1510817210794@mail.yahoo.com>
References: <524676651.1015231.1510817210794.ref@mail.yahoo.com>
 <524676651.1015231.1510817210794@mail.yahoo.com>
Message-ID: <b1efcdbd-b18a-653d-e344-f46c2a82eb3e@treenet.co.nz>

On 16/11/17 20:26, Vieri wrote:
> Hi,
> 
> Correct me if I'm wrong, but this may be a parsing bug:
> 
> # /etc/init.d/squid.test start
> * /etc/squid/squid.test.conf must set pid_filename to
> *    /run/squid.test.pid
> 
> 
> However, I have:
> 
> # grep include /etc/squid/squid.test.conf
> include /etc/squid/squid.custom.test
> include /etc/squid/squid.custom.rules.test
> 
> 
> # grep pid_filename /etc/squid/squid.custom.test
> pid_filename /run/squid.test.pid
> 
> Squid Object Cache: Version 3.5.27-20171101-re69e56c
> 

Works for me:

/squid/sbin/squid-3.5 -k parse -f /squid/test_pidfinc.conf
2017/11/16 21:18:47| Startup: Initializing Authentication Schemes ...
2017/11/16 21:18:47| Startup: Initialized Authentication Scheme 'basic'
2017/11/16 21:18:47| Startup: Initialized Authentication Scheme 'digest'
2017/11/16 21:18:47| Startup: Initialized Authentication Scheme 'negotiate'
2017/11/16 21:18:47| Startup: Initialized Authentication Scheme 'ntlm'
2017/11/16 21:18:47| Startup: Initialized Authentication.
2017/11/16 21:18:47| Processing Configuration File: 
/squid/test_pidfinc.conf (depth 0)
2017/11/16 21:18:47| Processing: include /squid/foo_pidf_nc.conf
2017/11/16 21:18:47| Processing Configuration File: 
/squid/foo_pidf_nc.conf (depth 1)
2017/11/16 21:18:47| Processing: pid_filename /squid/foo_custom.id


Note how the complaint is coming from your init script, not Squid.

Amos


From rentorbuy at yahoo.com  Thu Nov 16 08:29:08 2017
From: rentorbuy at yahoo.com (Vieri)
Date: Thu, 16 Nov 2017 08:29:08 +0000 (UTC)
Subject: [squid-users] block user agent
In-Reply-To: <61ce5b3f-bb5e-235f-351c-f3b4ae339fd5@treenet.co.nz>
References: <160200155.402986.1510744710954.ref@mail.yahoo.com>
 <160200155.402986.1510744710954@mail.yahoo.com>
 <61ce5b3f-bb5e-235f-351c-f3b4ae339fd5@treenet.co.nz>
Message-ID: <360621682.1064547.1510820948893@mail.yahoo.com>


________________________________
From: Amos Jeffries <squid3 at treenet.co.nz>
>
>> The following works:
>> 
>> acl denied_useragent browser Chrome
>> acl denied_useragent browser MSIE
>> acl denied_useragent browser Opera
>> acl denied_useragent browser Trident
>> [...]
>> http_access deny denied_useragent
>> http_reply_access deny denied_useragent
>> deny_info http://proxy-server1/proxy-error/?a=%a&B=%B&e=%e&E=%E&H=%H&i=%i&M=%M&o=%o&R=%R&T=%T&U=%U&u=%u&w=%w&x=%x&acl=denied_useragent denied_useragent
>> 
>> The following works for HTTP sites, but not for HTTPS sites in an ssl-bumped setup:
>> 
>> acl allowed_useragent browser Firefox/
>> [...]
>> http_access deny !allowed_useragent
>> deny_info http://proxy-server1/proxy-error/?a=%a&B=%B&e=%e&E=%E&H=%H&i=%i&M=%M&o=%o&R=%R&T=%T&U=%U&u=%u&w=%w&x=%x&acl=allowed_useragent allowed_useragent
>>
> The User-Agent along with all HTTP layer details in HTTPS are hidden 
> behind the encryption layer. TO do anything with them you must decrypt 
> the traffic first. If you can decrypt it turns into regular HTTP traffic 
> - the normal access controls should then work as-is.


So why does my first example actually work even for https sites?

acl denied_useragent browser Chrome
acl denied_useragent browser MSIE
acl denied_useragent browser Opera
acl denied_useragent browser Trident
[...]
http_access deny denied_useragent
http_reply_access deny denied_useragent
deny_info http://proxy-server1/proxy-error/?a=%a&B=%B&e=%e&E=%E&H=%H&i=%i&M=%M&o=%o&R=%R&T=%T&U=%U&u=%u&w=%w&x=%x&acl=denied_useragent denied_useragent

If the above "works" then another way would be to use a negated regular expression such as:
acl denied_useragent browser (?!Firefox)
but I don't think it's allowed.

Vieri


From rentorbuy at yahoo.com  Thu Nov 16 08:32:56 2017
From: rentorbuy at yahoo.com (Vieri)
Date: Thu, 16 Nov 2017 08:32:56 +0000 (UTC)
Subject: [squid-users] cannot set pid_filename in an include
In-Reply-To: <b1efcdbd-b18a-653d-e344-f46c2a82eb3e@treenet.co.nz>
References: <524676651.1015231.1510817210794.ref@mail.yahoo.com>
 <524676651.1015231.1510817210794@mail.yahoo.com>
 <b1efcdbd-b18a-653d-e344-f46c2a82eb3e@treenet.co.nz>
Message-ID: <261165997.1035865.1510821176029@mail.yahoo.com>

________________________________
From: Amos Jeffries <squid3 at treenet.co.nz>
>
> Note how the complaint is coming from your init script, not Squid.


{Thanks,Sorry} again.

Vieri


From rentorbuy at yahoo.com  Thu Nov 16 08:44:57 2017
From: rentorbuy at yahoo.com (Vieri)
Date: Thu, 16 Nov 2017 08:44:57 +0000 (UTC)
Subject: [squid-users] block user agent
In-Reply-To: <61ce5b3f-bb5e-235f-351c-f3b4ae339fd5@treenet.co.nz>
References: <160200155.402986.1510744710954.ref@mail.yahoo.com>
 <160200155.402986.1510744710954@mail.yahoo.com>
 <61ce5b3f-bb5e-235f-351c-f3b4ae339fd5@treenet.co.nz>
Message-ID: <1140434830.1059565.1510821897156@mail.yahoo.com>

Let me rephrase my previous question "So why does my first example actually work even for https sites?" to "So why does my first example actually work even for https sites in an ssl-bumped setup (the same as in example 2)?"


From squid3 at treenet.co.nz  Thu Nov 16 09:33:34 2017
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 16 Nov 2017 22:33:34 +1300
Subject: [squid-users] block user agent
In-Reply-To: <360621682.1064547.1510820948893@mail.yahoo.com>
References: <160200155.402986.1510744710954.ref@mail.yahoo.com>
 <160200155.402986.1510744710954@mail.yahoo.com>
 <61ce5b3f-bb5e-235f-351c-f3b4ae339fd5@treenet.co.nz>
 <360621682.1064547.1510820948893@mail.yahoo.com>
Message-ID: <b1f2869e-c094-6421-af7b-fd7ea8dfe3ff@treenet.co.nz>

On 16/11/17 21:29, Vieri wrote:
> 
> ________________________________
> From: Amos Jeffries <squid3 at treenet.co.nz>
>>
>>> The following works:
>>>
>>> acl denied_useragent browser Chrome
>>> acl denied_useragent browser MSIE
>>> acl denied_useragent browser Opera
>>> acl denied_useragent browser Trident
>>> [...]
>>> http_access deny denied_useragent
>>> http_reply_access deny denied_useragent
>>> deny_info http://proxy-server1/proxy-error/?a=%a&B=%B&e=%e&E=%E&H=%H&i=%i&M=%M&o=%o&R=%R&T=%T&U=%U&u=%u&w=%w&x=%x&acl=denied_useragent denied_useragent
>>>
>>> The following works for HTTP sites, but not for HTTPS sites in an ssl-bumped setup:
>>>
>>> acl allowed_useragent browser Firefox/
>>> [...]
>>> http_access deny !allowed_useragent
>>> deny_info http://proxy-server1/proxy-error/?a=%a&B=%B&e=%e&E=%E&H=%H&i=%i&M=%M&o=%o&R=%R&T=%T&U=%U&u=%u&w=%w&x=%x&acl=allowed_useragent allowed_useragent
>>>
>> The User-Agent along with all HTTP layer details in HTTPS are hidden
>> behind the encryption layer. TO do anything with them you must decrypt
>> the traffic first. If you can decrypt it turns into regular HTTP traffic
>> - the normal access controls should then work as-is.
> 
> 
> So why does my first example actually work even for https sites?

If you are decrypting the traffic, then it works as I said exactly the 
same as for HTTP messages.

If you are not decrypting the traffic, but receiving forward-proxy 
traffic then you are probably blocking the CONNECT messages that setup 
tunnels for HTTPS - it has a User-Agent header *if* it was generated by 
a UA instead of an intermediary like Squid.

> 
> acl denied_useragent browser Chrome
> acl denied_useragent browser MSIE
> acl denied_useragent browser Opera
> acl denied_useragent browser Trident
> [...]
> http_access deny denied_useragent
> http_reply_access deny denied_useragent
> deny_info http://proxy-server1/proxy-error/?a=%a&B=%B&e=%e&E=%E&H=%H&i=%i&M=%M&o=%o&R=%R&T=%T&U=%U&u=%u&w=%w&x=%x&acl=denied_useragent denied_useragent
> 
> If the above "works" then another way would be to use a negated regular expression such as:
> acl denied_useragent browser (?!Firefox)
> but I don't think it's allowed.

AFAIK that feature is part of a different regex grammar than the one 
Squid uses.

PS. you do know the UA strings of modern browsers all reference each 
other right?  "Chrome like-Gecko like Firefox" etc.

Amos


From uhlar at fantomas.sk  Thu Nov 16 09:38:11 2017
From: uhlar at fantomas.sk (Matus UHLAR - fantomas)
Date: Thu, 16 Nov 2017 10:38:11 +0100
Subject: [squid-users] SSL Bump for regex URL comparison
In-Reply-To: <CAPzmcFuuZ92PtZrURV5MpuLtCBefPuASV61paUTe4DBHjXz8PQ@mail.gmail.com>
References: <1510752735.2344.0.camel@gmail.com>
 <cb402607-bdd4-d157-48a8-9ad4e4aa5c30@treenet.co.nz>
 <CAPzmcFuuZ92PtZrURV5MpuLtCBefPuASV61paUTe4DBHjXz8PQ@mail.gmail.com>
Message-ID: <20171116093811.GA27977@fantomas.sk>

On 16.11.17 08:21, Joe Foster wrote:
>The problem is the connections are not getting through. It just acts like
>there is no WiFi connection.

what exactly is the error? Does squid receive those connections?
does squid reject them?

-- 
Matus UHLAR - fantomas, uhlar at fantomas.sk ; http://www.fantomas.sk/
Warning: I wish NOT to receive e-mail advertising to this address.
Varovanie: na tuto adresu chcem NEDOSTAVAT akukolvek reklamnu postu.
We are but packets in the Internet of life (userfriendly.org)


From vedavyas.vayalpadu at accenture.com  Thu Nov 16 09:42:28 2017
From: vedavyas.vayalpadu at accenture.com (Vayalpadu, Vedavyas)
Date: Thu, 16 Nov 2017 09:42:28 +0000
Subject: [squid-users] Need help
Message-ID: <MWHP114MB01604AD7D727E0CE94BB76C28F2E0@MWHP114MB0160.NAMP114.PROD.OUTLOOK.COM>

Hi All,

Iam getting this error in /var/log/messages.

Nov 16 10:17:20 dkbavlpxpxy01 squid[91497]:
Failed to select source for 'https://dkbavwpato02.global.internal.carlsberggroup.com/SES/services/masterdata/administratorServices-1.0.wsdl'

And customer is not able to connect to the application.

External App <-> Proxy <-> Internal application

Can anyone help ??

Best regards,

Vyas  (vedavyas vayalpadu )
IBM-AIX-UNIX Support
vedavyas.vayalpadu at accenture.com<mailto:mogens.kjaer at carlsberg.com>

[cid:image001.jpg at 01D311DA.F66233D0]
Accenture BDC-10B
Bagmane World Technology Center, 6,
Service Road, Chinappa Layout, Mahadevapura,
Bengaluru, Karnataka 560048 - INDIA


________________________________

This message is for the designated recipient only and may contain privileged, proprietary, or otherwise confidential information. If you have received it in error, please notify the sender immediately and delete the original. Any other use of the e-mail by you is prohibited. Where allowed by local law, electronic communications with Accenture and its affiliates, including e-mail and instant messaging (including content), may be scanned by our systems for the purposes of information security and assessment of internal compliance with Accenture policy.
______________________________________________________________________________________

www.accenture.com
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20171116/889315e4/attachment.htm>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: image001.jpg
Type: image/jpeg
Size: 2229 bytes
Desc: image001.jpg
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20171116/889315e4/attachment.jpg>

From uhlar at fantomas.sk  Thu Nov 16 10:39:05 2017
From: uhlar at fantomas.sk (Matus UHLAR - fantomas)
Date: Thu, 16 Nov 2017 11:39:05 +0100
Subject: [squid-users] Need help
In-Reply-To: <MWHP114MB01604AD7D727E0CE94BB76C28F2E0@MWHP114MB0160.NAMP114.PROD.OUTLOOK.COM>
References: <MWHP114MB01604AD7D727E0CE94BB76C28F2E0@MWHP114MB0160.NAMP114.PROD.OUTLOOK.COM>
Message-ID: <20171116103905.GA30455@fantomas.sk>

On 16.11.17 09:42, Vayalpadu, Vedavyas wrote:
>Nov 16 10:17:20 dkbavlpxpxy01 squid[91497]:
>Failed to select source for 'https://dkbavwpato02.global.internal.carlsberggroup.com/SES/services/masterdata/administratorServices-1.0.wsdl'
>
>And customer is not able to connect to the application.
>
>External App <-> Proxy <-> Internal application

have you played with always_direct and never_direct?

-- 
Matus UHLAR - fantomas, uhlar at fantomas.sk ; http://www.fantomas.sk/
Warning: I wish NOT to receive e-mail advertising to this address.
Varovanie: na tuto adresu chcem NEDOSTAVAT akukolvek reklamnu postu.
I'm not interested in your website anymore.
If you need cookies, bake them yourself.


From rentorbuy at yahoo.com  Thu Nov 16 10:53:25 2017
From: rentorbuy at yahoo.com (Vieri)
Date: Thu, 16 Nov 2017 10:53:25 +0000 (UTC)
Subject: [squid-users] block user agent
In-Reply-To: <b1f2869e-c094-6421-af7b-fd7ea8dfe3ff@treenet.co.nz>
References: <160200155.402986.1510744710954.ref@mail.yahoo.com>
 <160200155.402986.1510744710954@mail.yahoo.com>
 <61ce5b3f-bb5e-235f-351c-f3b4ae339fd5@treenet.co.nz>
 <360621682.1064547.1510820948893@mail.yahoo.com>
 <b1f2869e-c094-6421-af7b-fd7ea8dfe3ff@treenet.co.nz>
Message-ID: <1640984436.1076600.1510829605569@mail.yahoo.com>


________________________________
From: Amos Jeffries <squid3 at treenet.co.nz>
>
> If you are decrypting the traffic, then it works as I said exactly the 
> same as for HTTP messages.
>
> If you are not decrypting the traffic, but receiving forward-proxy 
> traffic then you are probably blocking the CONNECT messages that setup 
> tunnels for HTTPS - it has a User-Agent header *if* it was generated by 
> a UA instead of an intermediary like Squid.


So I would need to allow CONNECT messages.
Something like:
http_access allow CONNECT allowed_useragent

Anyway, I'm not sure what "decrypting the traffic" implies. If I want an ssl-bumped setup to fully handle all HTTPS connections, and be able to detect the user-agent on https connections, how should I configure Squid? Should I allow all CONNECT messages?

> AFAIK that feature is part of a different regex grammar than the one 
> Squid uses.


I think I read something about Squid being built with a user-defined regex grammar/lib. Anyway, I take it it's not feasible for now.
> PS. you do know the UA strings of modern browsers all reference each 
> other right?  "Chrome like-Gecko like Firefox" etc.


Yes, but... We require IE for some Intranet apps, and Firefox for other Extranet apps.
We can set a custom user agent string for the Firefox browser. We also have other http user agents with customized UA strings. So we're 99% sure that all browser clients going through Squid will be tagged correctly. That's the reason why I would prefer to "deny all user agents" except one ("my custom UA string"). Most users will not try to tamper with this.
I do not want to "allow all except a list of substrings" because it would be a nightmare.

Vieri


From vedavyas.vayalpadu at accenture.com  Thu Nov 16 13:08:11 2017
From: vedavyas.vayalpadu at accenture.com (Vayalpadu, Vedavyas)
Date: Thu, 16 Nov 2017 13:08:11 +0000
Subject: [squid-users] [External] Re:  Need help
In-Reply-To: <20171116103905.GA30455@fantomas.sk>
References: <MWHP114MB01604AD7D727E0CE94BB76C28F2E0@MWHP114MB0160.NAMP114.PROD.OUTLOOK.COM>
 <20171116103905.GA30455@fantomas.sk>
Message-ID: <MWHP114MB01607B586795E57F9EA75E328F2E0@MWHP114MB0160.NAMP114.PROD.OUTLOOK.COM>

Hello uhlar,

No , I am bit new to squid proxy server, we have taken a TCP dump from the system and we see that.

1. From external application to proxy server the traffic is flowing, but from Proxy server to the internal application server traffic is not flowing.
2. But from Proxy server to the internal application, trace route and telnet is happening.


Regards
Vyas


-----Original Message-----
From: squid-users [mailto:squid-users-bounces at lists.squid-cache.org] On Behalf Of Matus UHLAR - fantomas
Sent: Thursday, November 16, 2017 4:09 PM
To: squid-users at lists.squid-cache.org
Subject: [External] Re: [squid-users] Need help

On 16.11.17 09:42, Vayalpadu, Vedavyas wrote:
>Nov 16 10:17:20 dkbavlpxpxy01 squid[91497]:
>Failed to select source for 'https://urldefense.proofpoint.com/v2/url?u=https-3A__dkbavwpato02.global.internal.carlsberggroup.com_SES_services_masterdata_administratorServices-2D1.0.wsdl&d=DwIGaQ&c=eIGjsITfXP_y-DLLX0uEHXJvU8nOHrUK8IrwNKOtkVU&r=tFxAuERmcRdMDY2ODYAvl6bEao1jdCMqbJq7uebMlVg&m=ZaZkTWTnQj0-Uep5Zf0yP_MVIEN1gfxZ_I9NX9Xk4cc&s=7SR_061NskZJRLA7JY-3UKcudzhlUJ8zM2jKpZcZzJo&e='
>
>And customer is not able to connect to the application.
>
>External App <-> Proxy <-> Internal application

have you played with always_direct and never_direct?

--
Matus UHLAR - fantomas, uhlar at fantomas.sk ; https://urldefense.proofpoint.com/v2/url?u=http-3A__www.fantomas.sk_&d=DwIGaQ&c=eIGjsITfXP_y-DLLX0uEHXJvU8nOHrUK8IrwNKOtkVU&r=tFxAuERmcRdMDY2ODYAvl6bEao1jdCMqbJq7uebMlVg&m=ZaZkTWTnQj0-Uep5Zf0yP_MVIEN1gfxZ_I9NX9Xk4cc&s=vku4_zNZHIZRL1GK-bXjDxTx1e-4Ra2rrNxS6AkBBgA&e=
Warning: I wish NOT to receive e-mail advertising to this address.
Varovanie: na tuto adresu chcem NEDOSTAVAT akukolvek reklamnu postu.
I'm not interested in your website anymore.
If you need cookies, bake them yourself.
_______________________________________________
squid-users mailing list
squid-users at lists.squid-cache.org
https://urldefense.proofpoint.com/v2/url?u=http-3A__lists.squid-2Dcache.org_listinfo_squid-2Dusers&d=DwIGaQ&c=eIGjsITfXP_y-DLLX0uEHXJvU8nOHrUK8IrwNKOtkVU&r=tFxAuERmcRdMDY2ODYAvl6bEao1jdCMqbJq7uebMlVg&m=ZaZkTWTnQj0-Uep5Zf0yP_MVIEN1gfxZ_I9NX9Xk4cc&s=0G7PG9nHGau1SEPMMLPgeP2yZFUSm7lxEoZhWaPyHt0&e=

________________________________

This message is for the designated recipient only and may contain privileged, proprietary, or otherwise confidential information. If you have received it in error, please notify the sender immediately and delete the original. Any other use of the e-mail by you is prohibited. Where allowed by local law, electronic communications with Accenture and its affiliates, including e-mail and instant messaging (including content), may be scanned by our systems for the purposes of information security and assessment of internal compliance with Accenture policy.
______________________________________________________________________________________

www.accenture.com

From dernikov1 at gmail.com  Thu Nov 16 14:40:01 2017
From: dernikov1 at gmail.com (Bike dernikov1)
Date: Thu, 16 Nov 2017 15:40:01 +0100
Subject: [squid-users] Is it possible to apply squid delay pools on
 users/groups from AD ?
In-Reply-To: <7810701a-8fb1-7930-d513-498f7cd09564@treenet.co.nz>
References: <CAAPixKjCmVuYH05zUGqkpON6znrozmjW2s1XRF5Y05+pOx=aDw@mail.gmail.com>
 <7810701a-8fb1-7930-d513-498f7cd09564@treenet.co.nz>
Message-ID: <CAAPixKgjJQgtYR0xC-8YUr4VtipKCkn-RTu_S+z8f1DRyWbWCA@mail.gmail.com>

Thanks for info, we searched for solution  but found that is not
possible to combine delay polls, and forum is our last hope, so far we
solved almost everything :)
We have: Squid Object Cache: Version 3.5.23, so it could  work.
Can you give us example, how to use it.  Colleague searched for
example but couldn't find it.
Thanks for help.

On Thu, Nov 16, 2017 at 9:02 AM, Amos Jeffries <squid3 at treenet.co.nz> wrote:
> On 16/11/17 01:43, Bike dernikov1 wrote:
>>
>> Hi,
>> this is my second topic, i wouldn't wan to mix with first. I hope that is
>> ok.
>> i hope that someone succeeded  to apply delay pools on users/groups from
>> AD.
>> We are now using  delay pool  on whole 10.0.0.0/8, but that is a
>> problem as different users have different requirements.   We have 30
>> locations, and we can set different rules by ip, but than we would
>> need one rule for one location, we would need to use static ip,
>> network reconfiguration, but that solution would be nightmare for
>> administration, and we would like to avoid static ip-s for users.
>
>
> It depends on your Squid version.
>
> The latest Squid with annotation support are capable of receiving user/group
> names from the auth and external ACL helpers. These get attached to the
> transaction and can be matched with the 'note' type ACL in any later
> 'fast-category' access controls like delay_pools.
>
> If your Squid is too old to use note ACL, or your helper(s) not providing
> the relevant details to Squid (in Squid-3.4+ helper syntax). Then no, sorry.
>
> Amos
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users


From dernikov1 at gmail.com  Thu Nov 16 14:49:33 2017
From: dernikov1 at gmail.com (Bike dernikov1)
Date: Thu, 16 Nov 2017 15:49:33 +0100
Subject: [squid-users] SQUID memory error after vm.swappines changed
 from 60 to 10
In-Reply-To: <ca66b2db-a288-db6b-c77e-8205c2541224@treenet.co.nz>
References: <CAAPixKhsif5+ad1prsoFAZXVXcVAxpdqkw4RdvGbfK-UxhkzTQ@mail.gmail.com>
 <86188b87-0cc3-343c-7b56-7f01354a3aa8@measurement-factory.com>
 <CAAPixKiTszJDHUDK__PFgQ_+g0kR0RX4KZUNykFANKnTqzydsg@mail.gmail.com>
 <bfb6f0fc-357b-2156-7cec-e43e9718d521@measurement-factory.com>
 <CAAPixKhNB-1zkFP5X=b7f8__XEY0-9XtEGi8N0EkXTGB6kbygw@mail.gmail.com>
 <bd5c2853-4480-4808-d1d4-9d8b1a8f6b0d@measurement-factory.com>
 <CAAPixKgzxz_hCWxW+T5TBuBFqG4z6b3KFy0-9qE31Vq33MWsmw@mail.gmail.com>
 <f8132fc7-58b0-e427-d7ae-60204da73e3e@measurement-factory.com>
 <CAAPixKh2Prf7PgQvbqzEiUKgdGLW-jQvDBzdNJNoqJPuxUYwRA@mail.gmail.com>
 <cf19e140-7d8b-d216-d2c1-b8d4cdd8eb6b@measurement-factory.com>
 <CAAPixKjyVQmEuHhnq5hMdv4q6tKdVFkcUJw_H8Cj7GbZbgHJ+w@mail.gmail.com>
 <ca66b2db-a288-db6b-c77e-8205c2541224@treenet.co.nz>
Message-ID: <CAAPixKhekp_0ndvhVLio8hqjgwMO=p3McWvnauPPiFqEmg0K4w@mail.gmail.com>

On Thu, Nov 16, 2017 at 8:58 AM, Amos Jeffries <squid3 at treenet.co.nz> wrote:
> On 16/11/17 01:32, Bike dernikov1 wrote:
>>
>>
>> If i can ask under same title:
>> Yesterday we had error in logs: syslog, cache.log, dmesg,access.log
>>
>> segfault at 8 ip ....... sp ..... error 4 is squid
>> process pid exited due to signal 11 with status 0
>>
>> Squid restarted,  that was at the end of work, and i didn't  notice
>> change while surfing.
>> I noticed change in used memory, after i went trough logs, and found
>> segfault.
>>
>> Can you point me, how to analyze what happened.
>> Can that be problem with kernel ?
>>
>
> How to retrieve info about these type of things is detailed at
> <https://wiki.squid-cache.org/SquidFaq/BugReporting>.

I wasn't sure it is bug, so i didn't want to post it that is a  bug.
As you now confirm that it can be bug i will prepare for retriving
infos.
I just hope that bug won't  happen at high  load in middle of working day.


> NP: If you do not have core files enabled, then the data from that segfault
> is probably gone irretrievably. You may need to use the script to capture
> segfault details from a running proxy (the 'minimal downtime' section).

I am sure that i didn't enabled it.

>
> Amos
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users

Thanks for help.


From akash.kumar.patel at appdirect.com  Thu Nov 16 14:57:03 2017
From: akash.kumar.patel at appdirect.com (tappdint)
Date: Thu, 16 Nov 2017 07:57:03 -0700 (MST)
Subject: [squid-users] Proxy does not send response for internal host
In-Reply-To: <2ea66000-2d64-bf85-93fd-51876ff512c1@treenet.co.nz>
References: <1510611675802-0.post@n4.nabble.com>
 <7e0807cf-74e3-31b1-db2a-f0ec619220ce@treenet.co.nz>
 <1510672762332-0.post@n4.nabble.com>
 <2ea66000-2d64-bf85-93fd-51876ff512c1@treenet.co.nz>
Message-ID: <1510844223960-0.post@n4.nabble.com>

I was able to get the proxy to work properly with the original settings I
posted. The issue was with the docker network. There were multiple networks
and the squid container ran on a separate network rather than the network
where all the containers were operating. To fix the issue I simply ran squid
with an extra flag (--network) and everything seems to be working fine now.
Thanks!



--
Sent from: http://squid-web-proxy-cache.1019090.n4.nabble.com/Squid-Users-f1019091.html


From rousskov at measurement-factory.com  Thu Nov 16 17:47:52 2017
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Thu, 16 Nov 2017 10:47:52 -0700
Subject: [squid-users] block user agent
In-Reply-To: <1140434830.1059565.1510821897156@mail.yahoo.com>
References: <160200155.402986.1510744710954.ref@mail.yahoo.com>
 <160200155.402986.1510744710954@mail.yahoo.com>
 <61ce5b3f-bb5e-235f-351c-f3b4ae339fd5@treenet.co.nz>
 <1140434830.1059565.1510821897156@mail.yahoo.com>
Message-ID: <4f2bc2b8-f307-e62c-5225-de3054e9e556@measurement-factory.com>

On 11/16/2017 01:44 AM, Vieri wrote:
> Let me rephrase my previous question "So why does my first example
> actually work even for https sites?" to "So why does my first example
> actually work even for https sites in an ssl-bumped setup (the same
> as in example 2)?"

AFAICT, there is not enough information to answer that or the original
question. Going forward, I recommend two steps:

1. Your "works" and "does not work" setups currently differ in at least
three variables: user agent name, slash after the user agent name, and
acl negation in http_access. Find out which single variable is
responsible for the breakage by eliminating all other differences.

2. Post two ALL,2 cache.logs, each containing a single transaction, one
for the "works" case and one for the "does not work" case polished as
discussed in #1.


HTH,

Alex.


From rousskov at measurement-factory.com  Thu Nov 16 17:54:15 2017
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Thu, 16 Nov 2017 10:54:15 -0700
Subject: [squid-users] deny_info
In-Reply-To: <1919900086.1067058.1510818749446@mail.yahoo.com>
References: <1727217364.194806.1510652811222.ref@mail.yahoo.com>
 <1727217364.194806.1510652811222@mail.yahoo.com>
 <38d4ef6a-6311-6cf6-4f01-6074651c74c0@treenet.co.nz>
 <1919900086.1067058.1510818749446@mail.yahoo.com>
Message-ID: <2ea7df40-ddc9-4e5e-d0ed-3034797aac3f@measurement-factory.com>

On 11/16/2017 12:52 AM, Vieri wrote:
> From: Amos Jeffries <squid3 at treenet.co.nz>
>> Because there are actually no custom deny_info attached to that 
>> "denied_restricted1_mimetypes_rep" ACL.


> Right. I don't know how I missed that. Sorry.


FWIW, I recommend avoiding "denied", "allowed", and similar prefixes in
ACL names because these prefixes clash with directive actions. ACLs
(names should) characterize transactions, not actions that Squid should
apply to those transactions. Polishing your names may simplify your
configuration, which may help avoid misconfiguration and/or confusion like

    http_access allow denied_foo

Alex.


From evan at pierce.co.za  Thu Nov 16 19:18:15 2017
From: evan at pierce.co.za (Evan Pierce)
Date: Thu, 16 Nov 2017 21:18:15 +0200
Subject: [squid-users] Slow speedtest results
Message-ID: <a8cfe13f-2544-cec9-1f9c-bb37b489152a@pierce.co.za>

Hi all

Any idea why when using www.speedtest.net on my squid proxy ( squid 
3.5.27 on Centos 6.9) gives consistently false/bad speeds while doing a 
speed test. The actual speed when downloading a file from a actual web 
server like say the microsoft website is consistently good (30Mb/s fiber 
- download speed 3.4MB/s) but a speed test done at the same time sits at 
around 3 to 4Mb/s. I have tried turning caching off and various other 
"tuning" settings on squid but nothing has fundamentally altered the 
speed. Running command line speedtest gives a correct speedtest from the 
squid host. Test machine was machine running firefox and chrome with the 
proxy statically configured and wasn't under any load. A similarly 
configured squid on smaller hardware and the same service provider runs 
consistently gives an accurate speedtest (same centos and squid 
versions). Any one have any ideas?

thanks

Evan



From jonathanthomascho22 at gmail.com  Thu Nov 16 19:40:30 2017
From: jonathanthomascho22 at gmail.com (Jonathan thomas Cho)
Date: Thu, 16 Nov 2017 14:40:30 -0500
Subject: [squid-users] Deny ports to users
Message-ID: <CAEF7nDqOqxZMB6GptQpY_ESMvXozCnMN97p55sEQsf35hxDB7A@mail.gmail.com>

Hello, I was curious how to restrict users from accessing ports .

I have 4 workers and need them to have their own ports and not able to use
the other 3.

I currently use :

http_port 3128 name=ip2
http_port 3129 name=ip3
http_port 3130 name=ip4

acl ip2 myip x.x.x.2
acl ip3 myip x.x.x.3
acl ip4 myip x.x.x.4
tcp_outgoing_address x.x.x.2 ip2
tcp_outgoing_address x.x.x.3 ip3
tcp_outgoing_address x.x.x.4 ip4

However 3129 still work on all 4 ports.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20171116/e501e4ce/attachment.htm>

From yvoinov at gmail.com  Thu Nov 16 19:42:58 2017
From: yvoinov at gmail.com (Yuri)
Date: Fri, 17 Nov 2017 01:42:58 +0600
Subject: [squid-users] Deny ports to users
In-Reply-To: <CAEF7nDqOqxZMB6GptQpY_ESMvXozCnMN97p55sEQsf35hxDB7A@mail.gmail.com>
References: <CAEF7nDqOqxZMB6GptQpY_ESMvXozCnMN97p55sEQsf35hxDB7A@mail.gmail.com>
Message-ID: <4c7b8b56-8bec-66c7-9aef-5cc356502646@gmail.com>

You choose not appropriate tool for you task.

Squid is a proxy, not a firewall.


17.11.2017 1:40, Jonathan thomas Cho ?????:
> Hello, I was curious how to restrict users from accessing ports .?
>
> I have 4 workers and need them to have their own ports and not able to
> use the other 3.??
>
> I currently use :
>
> http_port 3128 name=ip2
> http_port 3129 name=ip3
> http_port 3130 name=ip4
>
> acl ip2 myip x.x.x.2
> acl ip3 myip x.x.x.3
> acl ip4 myip x.x.x.4
> tcp_outgoing_address x.x.x.2 ip2
> tcp_outgoing_address x.x.x.3 ip3
> tcp_outgoing_address x.x.x.4 ip4
>
> However 3129 still work on all 4 ports.
>
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users

-- 
**************************
* C++: Bug to the future *
**************************

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20171117/36439100/attachment.htm>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: signature.asc
Type: application/pgp-signature
Size: 648 bytes
Desc: OpenPGP digital signature
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20171117/36439100/attachment.sig>

From Antony.Stone at squid.open.source.it  Thu Nov 16 20:43:26 2017
From: Antony.Stone at squid.open.source.it (Antony Stone)
Date: Thu, 16 Nov 2017 20:43:26 +0000
Subject: [squid-users] Slow speedtest results
In-Reply-To: <a8cfe13f-2544-cec9-1f9c-bb37b489152a@pierce.co.za>
References: <a8cfe13f-2544-cec9-1f9c-bb37b489152a@pierce.co.za>
Message-ID: <201711162043.27251.Antony.Stone@squid.open.source.it>

On Thursday 16 November 2017 at 19:18:15, Evan Pierce wrote:

> Hi all
> 
> Any idea why when using www.speedtest.net on my squid proxy ( squid
> 3.5.27 on Centos 6.9) gives consistently false/bad speeds while doing a
> speed test.

> A similarly configured squid on smaller hardware and the same service
> provider runs consistently gives an accurate speedtest (same centos and
> squid versions).

Please explain in more detail what the difference is between these two:

 - what hardware?

 - what does "similarly configured" mean - what are the differences?

 - what are the differences in reported results?

Oh, and just to be sure - are these both on the same connection, or on two 
different connections to the same provider?


Antony.

-- 
I just got a new mobile phone, and I called it Titanic.  It's already syncing.

                                                   Please reply to the list;
                                                         please *don't* CC me.


From rousskov at measurement-factory.com  Thu Nov 16 20:55:38 2017
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Thu, 16 Nov 2017 13:55:38 -0700
Subject: [squid-users] Slow speedtest results
In-Reply-To: <a8cfe13f-2544-cec9-1f9c-bb37b489152a@pierce.co.za>
References: <a8cfe13f-2544-cec9-1f9c-bb37b489152a@pierce.co.za>
Message-ID: <5d308d1a-8a40-1fe9-e56b-73d378708083@measurement-factory.com>

On 11/16/2017 12:18 PM, Evan Pierce wrote:

> Any idea why when using www.speedtest.net on my squid proxy ( squid
> 3.5.27 on Centos 6.9) gives consistently false/bad speeds while doing a
> speed test. The actual speed when downloading a file from a actual web
> server like say the microsoft website is consistently good (30Mb/s fiber
> - download speed 3.4MB/s) but a speed test done at the same time sits at
> around 3 to 4Mb/s. I have tried turning caching off and various other
> "tuning" settings on squid but nothing has fundamentally altered the
> speed. Running command line speedtest gives a correct speedtest from the
> squid host. Test machine was machine running firefox and chrome with the
> proxy statically configured and wasn't under any load. A similarly
> configured squid on smaller hardware and the same service provider runs
> consistently gives an accurate speedtest (same centos and squid
> versions). Any one have any ideas?

I trust you have checked cache.log, system log, and network interface
statistics for warnings, errors, and red flags unique to the non-working
use case.

Make sure that browser-proxy path is about the same in all tests you
compare. The problem might be related to browser-Squid communication.

Since you have a "working" case (on "smaller hardware"), I would try the
following using identical Squid versions:

1. Use the default Squid configuration with Squid memory caching
disabled on both boxes. Is one setup still a lot "slower" than the other?

2. Compare access.logs and mgr:info output of the two tests (one test
performed after a clean Squid start). Any unexpected differences?

3. If you have not already, test a Squid configuration identical to that
"working" case (you can rename directories/hostnames if really needed,
of course, but do not change anything you do not have to change). Is one
setup still a lot "slower" than the other?

4. Comparing cache.logs of virtually identically configured Squids with
debug_options set to ALL,3 or higher may expose the critical difference.
Debugging will slow Squid down a lot, of course, but perhaps you will
see that one of the Squids is doing something that the other one does
not do.


HTH,

Alex.


From evan at pierce.co.za  Thu Nov 16 21:53:19 2017
From: evan at pierce.co.za (Evan Pierce)
Date: Thu, 16 Nov 2017 23:53:19 +0200
Subject: [squid-users] Slow speedtest results
In-Reply-To: <5d308d1a-8a40-1fe9-e56b-73d378708083@measurement-factory.com>
References: <a8cfe13f-2544-cec9-1f9c-bb37b489152a@pierce.co.za>
 <5d308d1a-8a40-1fe9-e56b-73d378708083@measurement-factory.com>
Message-ID: <843f1b0c-0bcd-5b7d-4072-686be514ae3c@pierce.co.za>

On 2017/11/16 10:55 PM, Alex Rousskov wrote:
> On 11/16/2017 12:18 PM, Evan Pierce wrote:
>
>> Any idea why when using www.speedtest.net on my squid proxy ( squid
>> 3.5.27 on Centos 6.9) gives consistently false/bad speeds while doing a
>> speed test. The actual speed when downloading a file from a actual web
>> server like say the microsoft website is consistently good (30Mb/s fiber
>> - download speed 3.4MB/s) but a speed test done at the same time sits at
>> around 3 to 4Mb/s. I have tried turning caching off and various other
>> "tuning" settings on squid but nothing has fundamentally altered the
>> speed. Running command line speedtest gives a correct speedtest from the
>> squid host. Test machine was machine running firefox and chrome with the
>> proxy statically configured and wasn't under any load. A similarly
>> configured squid on smaller hardware and the same service provider runs
>> consistently gives an accurate speedtest (same centos and squid
>> versions). Any one have any ideas?
> I trust you have checked cache.log, system log, and network interface
> statistics for warnings, errors, and red flags unique to the non-working
> use case.
Yes ... no obvious "red flags"
> Make sure that browser-proxy path is about the same in all tests you
> compare. The problem might be related to browser-Squid communication.
In both cases the test browser machines are physically cabled in to the 
same gigabit switch as the squid proxy/firewall machine

> Since you have a "working" case (on "smaller hardware"), I would try the
> following using identical Squid versions:
>
> 1. Use the default Squid configuration with Squid memory caching
> disabled on both boxes. Is one setup still a lot "slower" than the other?

Yes, however the "bigger" site has more vlans so it has slightly more 
https access lines and acls.
>
> 2. Compare access.logs and mgr:info output of the two tests (one test
> performed after a clean Squid start). Any unexpected differences?

Nothing jumps out at me.
>
> 3. If you have not already, test a Squid configuration identical to that
> "working" case (you can rename directories/hostnames if really needed,
> of course, but do not change anything you do not have to change). Is one
> setup still a lot "slower" than the other?
Yes one is slower.
> 4. Comparing cache.logs of virtually identically configured Squids with
> debug_options set to ALL,3 or higher may expose the critical difference.
> Debugging will slow Squid down a lot, of course, but perhaps you will
> see that one of the Squids is doing something that the other one does
> not do.
>
I can't see anything but both are in production and being used while I 
was testing so generated a lot of data

> HTH,
>
> Alex.




From rousskov at measurement-factory.com  Fri Nov 17 00:05:27 2017
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Thu, 16 Nov 2017 17:05:27 -0700
Subject: [squid-users] Slow speedtest results
In-Reply-To: <843f1b0c-0bcd-5b7d-4072-686be514ae3c@pierce.co.za>
References: <a8cfe13f-2544-cec9-1f9c-bb37b489152a@pierce.co.za>
 <5d308d1a-8a40-1fe9-e56b-73d378708083@measurement-factory.com>
 <843f1b0c-0bcd-5b7d-4072-686be514ae3c@pierce.co.za>
Message-ID: <b04c98ea-0a29-738a-6902-5caa26ca6dfd@measurement-factory.com>

On 11/16/2017 02:53 PM, Evan Pierce wrote:
> I can't see anything but both are in production and being used while I
> was testing so generated a lot of data

Sorry, I did not realize you are using live Squids for these tests!
Combining real and test traffic makes triage a lot harder and pretty
much all of the tests I mentioned are nearly pointless on live Squids,
especially if those Squids handle substantially different traffic
streams. If there is no way to take these Squids offline for a test then

a) Consider starting a separate Squid instance (on each box) using the
otherwise default config with no memory cache and a dedicated http_port.

b) Consider fixing your overall architecture so that it becomes possible
to take any single Squid instance offline when needed without serious
effects on users.

Alex.


From richardpeeters45 at gmail.com  Fri Nov 17 02:09:32 2017
From: richardpeeters45 at gmail.com (Richard Peeters)
Date: Thu, 16 Nov 2017 18:09:32 -0800
Subject: [squid-users] CONNECT + custom data
Message-ID: <CABfb+p2oumUOHwVeqr-CB0u-zg6=93Ea0_jibrfODK5oHR2FHw@mail.gmail.com>

Hi All,

I have a requirement to forward proxy an opaque stream of data. One of
the servers (acting as a client -A- to SQUID ) will use the CONNECT
method to connect to SQUID (on server B) and squid will then proxy
this data for A.

My question is I want to pass metadata from A to B which B will strip
out before proxying the data outbound, and I cannot find a way to do
that.

If this was an HTTP stream, headers could have been added by A and B
could have stripped them, but with my case I dont think even content
adaptation will help.

Can someone please advise on what feature of SQUID I should be looking
at to achieve this ot whether it is possible or not.

I have been reading documentation for less than 24 hours, please
pardon my ignorance.

Thanks,
Rich


From 747620227 at qq.com  Fri Nov 17 02:32:54 2017
From: 747620227 at qq.com (=?gb18030?B?R35Efkx1bmF0aWM=?=)
Date: Fri, 17 Nov 2017 10:32:54 +0800
Subject: [squid-users] squid 3.5.27 . https website
Message-ID: <tencent_F2EB85AEC07C5370527AEDC4F8D9721AAF08@qq.com>

i use squid 3.5.27 as a transparent proxy. With the proxy , i access some https websites like www.hupu.com. But the webpage does not show correctly.  There are some websizes similar such as https://www.zhihu.com , https://www.jd.com/ . So i want to know where problem is or how to deal with it.

The webpage remind like"   s1.hdslb.com used an invalid security certificate. This  certificate is valid for the following domain names only: *  .zhaopin.com, * .zhaopin.cn, * .dpfile.com, * .cdn.myqcloud.com, *  .sogoucdn. SSL error code: SSL_ERROR_BAD_CERT_DOMAIN  "


how can i send a screenshot to explain?

Here is my configure
# Recommended minimum configuration:
#

# Example rule allowing access from your local networks.
# Adapt to list your (internal) IP networks from where browsing
# should be allowed
acl localnet src 10.0.0.0/8     # RFC1918 possible internal network
acl localnet src 172.16.0.0/12  # RFC1918 possible internal network
acl localnet src 192.168.0.0/16 # RFC1918 possible internal network
acl localnet src fc00::/7       # RFC 4193 local private network range
acl localnet src fe80::/10      # RFC 4291 link-local (directly plugged) machines

acl SSL_ports port 443
acl Safe_ports port 80          # http
acl Safe_ports port 21          # ftp
acl Safe_ports port 443         # https
acl Safe_ports port 70          # gopher
acl Safe_ports port 210         # wais
acl Safe_ports port 1025-65535  # unregistered ports
acl Safe_ports port 280         # http-mgmt
acl Safe_ports port 488         # gss-http
acl Safe_ports port 591         # filemaker
acl Safe_ports port 777         # multiling http
acl CONNECT method CONNECT

#
# Recommended minimum Access Permission configuration:
#
# Deny requests to certain unsafe ports
http_access deny !Safe_ports

# Deny CONNECT to other than secure SSL ports
http_access deny CONNECT !SSL_ports
# Only allow cachemgr access from localhost
http_access allow localhost manager
http_access deny manager
http_access allow all

# We strongly recommend the following be uncommented to protect innocent
# web applications running on the proxy server who think the only
# one who can access services on "localhost" is a local user
#http_access deny to_localhost

#
# INSERT YOUR OWN RULE(S) HERE TO ALLOW ACCESS FROM YOUR CLIENTS
#

# Example rule allowing access from your local networks.
# Adapt localnet in the ACL section to list your (internal) IP networks
# from where browsing should be allowed
http_access allow localnet
http_access allow localhost
acl NCACHE method GET
no_cache deny NCACHE

# And finally deny all other access to this proxy
request_header_access Via deny all #hide squid header
request_header_access X-Forwarded-For deny all #hide squid header
#request_timeout 2 minutes #client request timeout

# Squid normally listens to port 3128
http_port 3120

http_port 3128 intercept

https_port 192.168.51.115:3129 intercept ssl-bump connection-auth=off generate-host-certificates=on dynamic_cert_mem_cache_size=4MB cert=/usr/local/squid/ssl_cert/myCA.pem key=/usr/local/squid/ssl_cert/myCA.pem
always_direct allow all
ssl_bump server-first all
acl ssl_step1 at_step SslBump1
acl ssl_step2 at_step SslBump2
acl ssl_step3 at_step SslBump3
ssl_bump peek ssl_step1
ssl_bump splice all

sslproxy_version 0
sslproxy_cert_error allow all
sslproxy_flags DONT_VERIFY_PEER

sslcrtd_program /usr/local/squid/libexec/ssl_crtd -s /usr/local/squid/lib/ssl_db -M 4MB
sslcrtd_children 8 startup=1 idle=1

#Uncomment and adjust the following to add a disk cache directory.
cache_dir ufs /usr/local/squid/var/cache/squid 4096 16 256
minimum_object_size 0 KB
maximum_object_size 4096 KB
ipcache_size 1024 MB
ipcache_low 70
ipcache_high 95
fqdncache_size 1024 MB
cache_mem 1024 MB
cache_swap_low 90
cache_swap_high 95


# Leave coredumps in the first cache dir
coredump_dir /usr/local/squid/var/cache/squid
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20171117/7cc1d0b3/attachment.htm>

From squid3 at treenet.co.nz  Fri Nov 17 02:39:12 2017
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Fri, 17 Nov 2017 15:39:12 +1300
Subject: [squid-users] Is it possible to apply squid delay pools on
 users/groups from AD ?
In-Reply-To: <CAAPixKgjJQgtYR0xC-8YUr4VtipKCkn-RTu_S+z8f1DRyWbWCA@mail.gmail.com>
References: <CAAPixKjCmVuYH05zUGqkpON6znrozmjW2s1XRF5Y05+pOx=aDw@mail.gmail.com>
 <7810701a-8fb1-7930-d513-498f7cd09564@treenet.co.nz>
 <CAAPixKgjJQgtYR0xC-8YUr4VtipKCkn-RTu_S+z8f1DRyWbWCA@mail.gmail.com>
Message-ID: <c494d8d0-ec48-7707-6620-b124655b77bb@treenet.co.nz>

On 17/11/17 03:40, Bike dernikov1 wrote:
> Thanks for info, we searched for solution  but found that is not
> possible to combine delay polls, and forum is our last hope, so far we
> solved almost everything :)
> We have: Squid Object Cache: Version 3.5.23, so it could  work.
> Can you give us example, how to use it.  Colleague searched for
> example but couldn't find it.
> Thanks for help.
> 

An example for username would be:

  auth_param ...
  acl login proxy_auth REQUIRED
  http_access deny !login

  delay_pools 1
  delay_class 1 ...
  delay_parameters 1 ...

  acl slow note user Fred Bob
  delay_access 1 allow slow


For groups, the latest Kerberos auth helpers from Marcus Moeller are 
sending the SID and group details back to Squid for this. The other 
helpers bundled by Squid are not yet sending group names back.

(I was hoping to have that ready for Squid-4, but have not had the time. 
Patches or github PR welcome if anyone wants to contribute).

Amos


From squid3 at treenet.co.nz  Fri Nov 17 02:49:13 2017
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Fri, 17 Nov 2017 15:49:13 +1300
Subject: [squid-users] [External] Re: Need help
In-Reply-To: <MWHP114MB01607B586795E57F9EA75E328F2E0@MWHP114MB0160.NAMP114.PROD.OUTLOOK.COM>
References: <MWHP114MB01604AD7D727E0CE94BB76C28F2E0@MWHP114MB0160.NAMP114.PROD.OUTLOOK.COM>
 <20171116103905.GA30455@fantomas.sk>
 <MWHP114MB01607B586795E57F9EA75E328F2E0@MWHP114MB0160.NAMP114.PROD.OUTLOOK.COM>
Message-ID: <b9cde1c3-3ac0-38be-43ef-c8df270bbdfb@treenet.co.nz>

On 17/11/17 02:08, Vayalpadu, Vedavyas wrote:
> Hello uhlar,
> 
> No , I am bit new to squid proxy server, we have taken a TCP dump from the system and we see that.
> 
> 1. From external application to proxy server the traffic is flowing, but from Proxy server to the internal application server traffic is not flowing.
> 2. But from Proxy server to the internal application, trace route and telnet is happening.
> 

That log entry means that Squid was not able to locate any server IP 
addresses that will work for that transaction.

Either DNS has no results, client original dst-IP is not available, or 
the servers Squid does find are a) forbidden for use or b) currently 
DOWN according to the dynamic availability checks (ICMP echo, ICP query 
for cache_peer, and past TCP connection attempts).

The cache.log lines immediately following the one you quoted tell the 
results from DNS and each of those access controls so you can see what 
the reason for the failure was.

Amos


From squid3 at treenet.co.nz  Fri Nov 17 02:53:40 2017
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Fri, 17 Nov 2017 15:53:40 +1300
Subject: [squid-users] SQUID memory error after vm.swappines changed
 from 60 to 10
In-Reply-To: <CAAPixKhekp_0ndvhVLio8hqjgwMO=p3McWvnauPPiFqEmg0K4w@mail.gmail.com>
References: <CAAPixKhsif5+ad1prsoFAZXVXcVAxpdqkw4RdvGbfK-UxhkzTQ@mail.gmail.com>
 <86188b87-0cc3-343c-7b56-7f01354a3aa8@measurement-factory.com>
 <CAAPixKiTszJDHUDK__PFgQ_+g0kR0RX4KZUNykFANKnTqzydsg@mail.gmail.com>
 <bfb6f0fc-357b-2156-7cec-e43e9718d521@measurement-factory.com>
 <CAAPixKhNB-1zkFP5X=b7f8__XEY0-9XtEGi8N0EkXTGB6kbygw@mail.gmail.com>
 <bd5c2853-4480-4808-d1d4-9d8b1a8f6b0d@measurement-factory.com>
 <CAAPixKgzxz_hCWxW+T5TBuBFqG4z6b3KFy0-9qE31Vq33MWsmw@mail.gmail.com>
 <f8132fc7-58b0-e427-d7ae-60204da73e3e@measurement-factory.com>
 <CAAPixKh2Prf7PgQvbqzEiUKgdGLW-jQvDBzdNJNoqJPuxUYwRA@mail.gmail.com>
 <cf19e140-7d8b-d216-d2c1-b8d4cdd8eb6b@measurement-factory.com>
 <CAAPixKjyVQmEuHhnq5hMdv4q6tKdVFkcUJw_H8Cj7GbZbgHJ+w@mail.gmail.com>
 <ca66b2db-a288-db6b-c77e-8205c2541224@treenet.co.nz>
 <CAAPixKhekp_0ndvhVLio8hqjgwMO=p3McWvnauPPiFqEmg0K4w@mail.gmail.com>
Message-ID: <ac3150fb-0659-fd69-0d1e-fba9e596abfd@treenet.co.nz>

On 17/11/17 03:49, Bike dernikov1 wrote:
> On Thu, Nov 16, 2017 at 8:58 AM, Amos Jeffries wrote:
>> On 16/11/17 01:32, Bike dernikov1 wrote:
>>>
>>>
>>> If i can ask under same title:
>>> Yesterday we had error in logs: syslog, cache.log, dmesg,access.log
>>>
>>> segfault at 8 ip ....... sp ..... error 4 is squid
>>> process pid exited due to signal 11 with status 0
>>>
>>> Squid restarted,  that was at the end of work, and i didn't  notice
>>> change while surfing.
>>> I noticed change in used memory, after i went trough logs, and found
>>> segfault.
>>>
>>> Can you point me, how to analyze what happened.
>>> Can that be problem with kernel ?
>>>
>>
>> How to retrieve info about these type of things is detailed at
>> <https://wiki.squid-cache.org/SquidFaq/BugReporting>.
> 
> I wasn't sure it is bug, so i didn't want to post it that is a  bug.
> As you now confirm that it can be bug i will prepare for retriving
> infos.
> I just hope that bug won't  happen at high  load in middle of working day.
> 

The how-to are just on that page because if you are reporting that kind 
of bug those details are mandatory. You dont have to be reporting a bug 
to use the techniques.

That said, segfault is almost always a bug. Though it could be a bug in 
the system environment or hardware rather than Squid. The details you 
get from looking at the traces should indicate whether those are actual 
or not.


> 
>> NP: If you do not have core files enabled, then the data from that segfault
>> is probably gone irretrievably. You may need to use the script to capture
>> segfault details from a running proxy (the 'minimal downtime' section).
> 
> I am sure that i didn't enabled it.
> 

Okay, then you will need to for further diagnosis.

Amos


From squid3 at treenet.co.nz  Fri Nov 17 02:58:08 2017
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Fri, 17 Nov 2017 15:58:08 +1300
Subject: [squid-users] Proxy does not send response for internal host
In-Reply-To: <1510844223960-0.post@n4.nabble.com>
References: <1510611675802-0.post@n4.nabble.com>
 <7e0807cf-74e3-31b1-db2a-f0ec619220ce@treenet.co.nz>
 <1510672762332-0.post@n4.nabble.com>
 <2ea66000-2d64-bf85-93fd-51876ff512c1@treenet.co.nz>
 <1510844223960-0.post@n4.nabble.com>
Message-ID: <1f38e536-ce59-1c2f-5d9d-98864fa1135e@treenet.co.nz>

On 17/11/17 03:57, tappdint wrote:
> I was able to get the proxy to work properly with the original settings I
> posted. The issue was with the docker network. There were multiple networks
> and the squid container ran on a separate network rather than the network
> where all the containers were operating. To fix the issue I simply ran squid
> with an extra flag (--network) and everything seems to be working fine now.
> Thanks!
> 

Cool. Sounds like you have a very interesting use-case there.

Would you be able to write up the design and configuration settings for 
a page in our wiki?

eg. <https://wiki.squid-cache.org/ConfigExample/ContainerNetworks>


Amos


From squid3 at treenet.co.nz  Fri Nov 17 03:29:44 2017
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Fri, 17 Nov 2017 16:29:44 +1300
Subject: [squid-users] Deny ports to users
In-Reply-To: <4c7b8b56-8bec-66c7-9aef-5cc356502646@gmail.com>
References: <CAEF7nDqOqxZMB6GptQpY_ESMvXozCnMN97p55sEQsf35hxDB7A@mail.gmail.com>
 <4c7b8b56-8bec-66c7-9aef-5cc356502646@gmail.com>
Message-ID: <72c307d0-8910-ac1d-acfd-3d84cde04ad6@treenet.co.nz>


On 17/11/17 08:42, Yuri wrote:
> You choose not appropriate tool for you task.
> 
> Squid is a proxy, not a firewall.
> 

Indeed.


> 
> 17.11.2017 1:40, Jonathan thomas Cho ?????:
>> Hello, I was curious how to restrict users from accessing ports .
>>
>> I have 4 workers and need them to have their own ports and not able to 
>> use the other 3.
>>
>> I currently use :
>>
>> http_port 3128 name=ip2
>> http_port 3129 name=ip3
>> http_port 3130 name=ip4

The above are directives for the *listening* ports receiving 
client<->Squid connections.

You have here configured this Squid *process* (all workers of it) to use 
port 3128 on all IP addresses the machine has been assigned. Same for 
port 3129 and 3130.

Squid cannot control which port a client decides to connect to. It can 
only listen (or not).

I assume you mean you want each worker to use different listening ports. 
That can be done by using the ${process_number} config macro in the port 
number itself eg. http_port 313${Process_number}.
  However, be aware that will lead to issues with the coordinator 
process not being able to manage SMP port functionality and worker 
automatic restart after crashes will have issues since the process 
number changes there too. And you thus cannot reliably use the port 
name/number for other things like you seem to be wanting.


>> >> acl ip2 myip x.x.x.2
>> acl ip3 myip x.x.x.3
>> acl ip4 myip x.x.x.4

"myip" is deprecated, it does not work at all well. Use "myportname" 
instead.

Your Squid should complain about this when you run '-k parse' to check 
your config validity. If your Squid does not support that new ACL type 
you definitely need to upgrade.


>> tcp_outgoing_address x.x.x.2 ip2
>> tcp_outgoing_address x.x.x.3 ip3
>> tcp_outgoing_address x.x.x.4 ip4
>>

These are for Squid<->server connections. Has nothing to do with 
client<->Squid connections.

The OS selects which ports are use here. Not Squid.


>> However 3129 still work on all 4 ports.
>>

3129 is a port number. Singular. It does not *listen* on other values.

The traffic arriving on connections *to* there is independent of the 
outgoing connection port numbers - which are not controllable as 
mentioned above. So it is not clear what you are trying to say by that.


Amos


From squid3 at treenet.co.nz  Fri Nov 17 03:39:50 2017
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Fri, 17 Nov 2017 16:39:50 +1300
Subject: [squid-users] CONNECT + custom data
In-Reply-To: <CABfb+p2oumUOHwVeqr-CB0u-zg6=93Ea0_jibrfODK5oHR2FHw@mail.gmail.com>
References: <CABfb+p2oumUOHwVeqr-CB0u-zg6=93Ea0_jibrfODK5oHR2FHw@mail.gmail.com>
Message-ID: <1cddc1ca-3b04-ab8c-98e1-0611cfa8ff95@treenet.co.nz>

On 17/11/17 15:09, Richard Peeters wrote:
> Hi All,
> 
> I have a requirement to forward proxy an opaque stream of data. One of
> the servers (acting as a client -A- to SQUID ) will use the CONNECT
> method to connect to SQUID (on server B) and squid will then proxy
> this data for A.
> 
> My question is I want to pass metadata from A to B which B will strip
> out before proxying the data outbound, and I cannot find a way to do
> that.

"metadata" in HTTP just means headers.

For custom hop-by-hop headers your client application needs to use 
Connection: header to control their removal by the recieving next-hop 
HTTP agent. See <https://tools.ietf.org/html/rfc7230#section-6.1>.
  The custom header field-values can be accessed using the various 
request/reply header regex ACL types, same as any header.

Squid does not touch any of the 'payload' section following a CONNECT 
message. It always gets relayed as-is or rejected completely.
  Except when SSL-Bump is configured to decrypt tunnelled TLS traffic. 
Custom payload formats are not possible there, only TLS syntax.

Amos


From squid3 at treenet.co.nz  Fri Nov 17 04:13:40 2017
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Fri, 17 Nov 2017 17:13:40 +1300
Subject: [squid-users] squid 3.5.27 . https website
In-Reply-To: <tencent_F2EB85AEC07C5370527AEDC4F8D9721AAF08@qq.com>
References: <tencent_F2EB85AEC07C5370527AEDC4F8D9721AAF08@qq.com>
Message-ID: <f2c05d8f-2184-0622-83a0-5c481c13a064@treenet.co.nz>

On 17/11/17 15:32, G~D~Lunatic wrote:
> i use squid 3.5.27 as a transparent proxy.

Small correction: You have configured NAT interception proxy with 
SSL-Bump'ing. Not truly transparent.
  There are some vital differences. Most specific to your case is that 
interception proxies do alter the traffic in significant ways (not 
transparently relay as-is).


> With the proxy , i access 
> some https websites like www.hupu.com. But the 
> webpage does not show correctly.? There are some websizes similar such 
> as https://www.zhihu.com, https://www.jd.com/. So i want to know where problem is or how to 
> deal with it.
> 
> The webpage remind like" ? s1.hdslb.com used an invalid security 
> certificate. This certificate is valid for the following domain names 
> only: * .zhaopin.com, * .zhaopin.cn, * .dpfile.com, * .cdn.myqcloud.com, 
> * .sogoucdn. SSL error code: SSL_ERROR_BAD_CERT_DOMAIN? "
> 
> how can i send a screenshot to explain?
> Here is my configure
> # Recommended minimum configuration:
> #
> 
> # Example rule allowing access from your local networks.
> # Adapt to list your (internal) IP networks from where browsing
> # should be allowed
> acl localnet src 10.0.0.0/8???? # RFC1918 possible internal network
> acl localnet src 172.16.0.0/12? # RFC1918 possible internal network
> acl localnet src 192.168.0.0/16 # RFC1918 possible internal network
> acl localnet src fc00::/7?????? # RFC 4193 local private network range
> acl localnet src fe80::/10????? # RFC 4291 link-local (directly plugged) 
> machines
> 
> acl SSL_ports port 443
> acl Safe_ports port 80????????? # http
> acl Safe_ports port 21????????? # ftp
> acl Safe_ports port 443???????? # https
> acl Safe_ports port 70????????? # gopher
> acl Safe_ports port 210???????? # wais
> acl Safe_ports port 1025-65535? # unregistered ports
> acl Safe_ports port 280???????? # http-mgmt
> acl Safe_ports port 488???????? # gss-http
> acl Safe_ports port 591???????? # filemaker
> acl Safe_ports port 777???????? # multiling http
> acl CONNECT method CONNECT
> 
> #
> # Recommended minimum Access Permission configuration:
> #
> # Deny requests to certain unsafe ports
> http_access deny !Safe_ports
> 
> # Deny CONNECT to other than secure SSL ports
> http_access deny CONNECT !SSL_ports
> # Only allow cachemgr access from localhost
> http_access allow localhost manager
> http_access deny manager
> http_access allow all

*Extremely* unsafe configuration. This proxy is now an "open proxy". 
Anybody can abuse it for any use whatsoever.

Combined with how you have disabled below recording of all TLS traffic 
problems (and thus hacking attempts) and do server-first bumping of 
clients what you end up with is a remarkably dangerous piece of software 
whose most useful property is being a way to attack your network. :-(



> 
> # We strongly recommend the following be uncommented to protect innocent
> # web applications running on the proxy server who think the only
> # one who can access services on "localhost" is a local user
> #http_access deny to_localhost
> 
> #
> # INSERT YOUR OWN RULE(S) HERE TO ALLOW ACCESS FROM YOUR CLIENTS
> #
> 
> # Example rule allowing access from your local networks.
> # Adapt localnet in the ACL section to list your (internal) IP networks
> # from where browsing should be allowed
> http_access allow localnet
> http_access allow localhost
> acl NCACHE method GET
> no_cache deny NCACHE

"no_cache" is an deprecated directive. It was removed because it 
confused people. Delete the "no_" prefix.


Also, most other methods are not cacheable. So why not do it the simple way?

  cache deny all
or
  store_miss deny all


> 
> # And finally deny all other access to this proxy
> request_header_access Via deny all #hide squid header
> request_header_access X-Forwarded-For deny all #hide squid header
> #request_timeout 2 minutes #client request timeout
> 

The above is a very slow and nasty way to perform:

  via off
  forwarded_for delete


Though if you want to be transparent, use these instead:
  via off
  forwarded_for transparent


> # Squid normally listens to port 3128
> http_port 3120
> 
> http_port 3128 intercept
> 
> https_port 192.168.51.115:3129 intercept ssl-bump connection-auth=off 
> generate-host-certificates=on dynamic_cert_mem_cache_size=4MB 
> cert=/usr/local/squid/ssl_cert/myCA.pem 
> key=/usr/local/squid/ssl_cert/myCA.pem
> always_direct allow all

The use of "always_direct allow all" is a now useless workaround for a 
long ago fixed bug. No version of Squid available in any distro today 
needs it.


> ssl_bump server-first all
> acl ssl_step1 at_step SslBump1
> acl ssl_step2 at_step SslBump2
> acl ssl_step3 at_step SslBump3
> ssl_bump peek ssl_step1
> ssl_bump splice all

You are mixing up rules from multiple different versions of the SSL-Bump 
feature.

"server-first" is equivalent to:

  ssl_bump peek ssl_step1
  ssl_bump bump all

It overrides all the ssl_bump lines following it.


> 
> sslproxy_version 0
> sslproxy_cert_error allow all
> sslproxy_flags DONT_VERIFY_PEER

Remove all three of the above lines. You may then be able to see what is 
going on if the errors are in the TLS layer.

All these lines do is hide errors and network abuse from *you*, the 
admin. Not your clients or users - they will still get errors.


I think your problem is that the bumping done by "server-first" is 
clashing with several modern TLS features that sites use. You will not 
be able to see which problem it is though until you re-enable recording 
and display of TLS issues.


Amos


From 1linuxengineer at gmail.com  Fri Nov 17 07:33:45 2017
From: 1linuxengineer at gmail.com (=?UTF-8?Q?Bernhard_D=C3=BCbi?=)
Date: Fri, 17 Nov 2017 08:33:45 +0100
Subject: [squid-users] forward proxy to reverse proxy to app
Message-ID: <CACxnGeRJqUFXxukLhBArj9SMn5=c9uqpMfUuseoDRz2mXPL6=g@mail.gmail.com>

Hi,

I try to configure squid for a very special usecase but can't get it
to work. So, if you could give me some hints on how to do it right,
that would be great

Here's what I try to achieve:

the browser has proxy:8080 configured as manual proxy
from the browser I access some websites
when the request is plain http then the reply must be a redirect to https
when the request is https then the ssl connection must be termintaed
on the proxy and the request must be forwarded as http to the
application server

I know, I could just forget about ssl an go directly the app server
with http bt the customer insists on that particular setup

we use several domains like app1.doma.com, app2.domb.biz, app3.domc.org
in order to return the correct certificate for each request, I need a
dedicated ip:port combination for each certificate

I came up with the following setup

browser -> proxy:8080 -> squid    for http://app1.doma.com ->
127.0.0.1:10081 -> haproxy -> redirect
                for https://app1.doma.com -> 127.0.0.1:10401 ->
haproxy -> terminate ssl -> app1.local.net:8123
                for http://app2.doma.com -> 127.0.0.1:10082 -> haproxy
-> redirect
                for https://app2.doma.com -> 127.0.0.1:10402 ->
haproxy -> terminate ssl -> app2.local.net:8765
                for http://app3.doma.com -> 127.0.0.1:10083 -> haproxy
-> redirect
                for https://app3.doma.com -> 127.0.0.1:10403 ->
haproxy -> terminate ssl -> app3.local.net:8888

here's the configuration I created so far

http_port 8080

# User networks
acl Users src 10.11.12.0/22

acl SSL_ports port 443
acl Safe_ports port 80          # http
acl Safe_ports port 21          # ftp
acl Safe_ports port 443         # https
acl Safe_ports port 70          # gopher
acl Safe_ports port 210         # wais
acl Safe_ports port 1025-65535  # unregistered ports
acl Safe_ports port 280         # http-mgmt
acl Safe_ports port 488         # gss-http
acl Safe_ports port 591         # filemaker
acl Safe_ports port 777         # multiling http
acl CONNECT method CONNECT
http_access deny !Safe_ports
http_access deny CONNECT !SSL_ports
http_access allow localhost manager
http_access deny manager
http_access allow localhost
http_access allow Users
http_access deny all
coredump_dir /var/spool/squid
cache deny all
never_direct allow all

acl to_domA dstdomain .doma.com
acl to_domB dstdomain .domb.biz
acl to_domC dstdomain .domc.org

cache_peer 127.0.0.1 parent 10081 0 name=domA_redirect no-query originserver
cache_peer_access domA_redirect allow !CONNECT to_domA
cache_peer 127.0.0.1 parent 10401 0 name=domA_ssl no-query originserver
cache_peer_access domA_ssl allow CONNECT to_domA

cache_peer 127.0.0.1 parent 10082 0 name=domB_redirect no-query originserver
cache_peer_access domB_redirect allow !CONNECT to_domB
cache_peer 127.0.0.1 parent 10402 0 name=domB_ssl no-query originserver
cache_peer_access domB_ssl allow CONNECT to_domB

cache_peer 127.0.0.1 parent 10083 0 name=domC_redirect no-query originserver
cache_peer_access domC_redirect allow !CONNECT to_domC
cache_peer 127.0.0.1 parent 10403 0 name=domC_ssl no-query originserver
cache_peer_access domC_ssl allow CONNECT to_domC



the plain http part works, squid selects the correct peer and haproxy
reponds with the redirect

ssl respectifely the CONNECT call is the problem

2017/11/17 07:56:21.429 kid1| 28,3| Checklist.cc(63) markFinished:
0x55d69a951b68 answer ALLOWED for match
2017/11/17 07:56:21.429 kid1| 28,3| Checklist.cc(163) checkCallback:
ACLChecklist::checkCallback: 0x55d69a951b68 answer=ALLOWED
2017/11/17 07:56:21.429 kid1| 44,3| peer_select.cc(171)
peerCheckNeverDirectDone: peerCheckNeverDirectDone: ALLOWED
2017/11/17 07:56:21.429 kid1| 44,3| peer_select.cc(177)
peerCheckNeverDirectDone: direct = DIRECT_NO (never_direct allow)
2017/11/17 07:56:21.429 kid1| 44,3| peer_select.cc(441) peerSelectFoo:
CONNECT app1.doma.com
2017/11/17 07:56:21.429 kid1| 44,3| peer_select.cc(685)
peerGetSomeParent: CONNECT app1.doma.com
2017/11/17 07:56:21.429 kid1| 44,2| peer_select.cc(280)
peerSelectDnsPaths: Failed to select source for 'app1.doma.com:443'
2017/11/17 07:56:21.429 kid1| 44,2| peer_select.cc(281)
peerSelectDnsPaths:   always_direct = DENIED
2017/11/17 07:56:21.429 kid1| 44,2| peer_select.cc(282)
peerSelectDnsPaths:    never_direct = ALLOWED
2017/11/17 07:56:21.429 kid1| 44,2| peer_select.cc(295)
peerSelectDnsPaths:        timedout = 0
2017/11/17 07:56:21.429 kid1| 26,3| tunnel.cc(1156)
tunnelPeerSelectComplete: No paths found. Aborting CONNECT
2017/11/17 07:56:21.429 kid1| 4,3| errorpage.cc(633) errorSend:
local=10.1.2.3:8080 remote=10.11.12.13:61110 FD 12 flags=1,
err=0x55d69a511528
2017/11/17 07:56:21.429 kid1| 4,2| errorpage.cc(1262) BuildContent: No
existing error page language negotiated for ERR_CANNOT_FORWARD. Using
default error file.


if it makes any difference here some details about os and squid:

root at proj-proxy:~# dpkg -l | grep squid
ii  squid                             3.5.12-1ubuntu7.4
          amd64        Full featured Web Proxy cache (HTTP proxy)
ii  squid-common                      3.5.12-1ubuntu7.4
          all          Full featured Web Proxy cache (HTTP proxy) -
common files
ii  squid-langpack                    20150704-1
          all          Localized error pages for Squid

root at proj-proxy:~# uname -a
Linux proj-proxy 4.4.0-98-generic #121-Ubuntu SMP Tue Oct 10 14:24:03
UTC 2017 x86_64 x86_64 x86_64 GNU/Linux

root at proj-proxy:~# cat /etc/os-release
NAME="Ubuntu"
VERSION="16.04.3 LTS (Xenial Xerus)"
ID=ubuntu
ID_LIKE=debian
PRETTY_NAME="Ubuntu 16.04.3 LTS"
VERSION_ID="16.04"
HOME_URL="http://www.ubuntu.com/"
SUPPORT_URL="http://help.ubuntu.com/"
BUG_REPORT_URL="http://bugs.launchpad.net/ubuntu/"
VERSION_CODENAME=xenial
UBUNTU_CODENAME=xenial



Many thanks for your help
Bernhard


From vedavyas.vayalpadu at accenture.com  Fri Nov 17 07:49:44 2017
From: vedavyas.vayalpadu at accenture.com (Vayalpadu, Vedavyas)
Date: Fri, 17 Nov 2017 07:49:44 +0000
Subject: [squid-users] [External] Re: Need help
In-Reply-To: <b9cde1c3-3ac0-38be-43ef-c8df270bbdfb@treenet.co.nz>
References: <MWHP114MB01604AD7D727E0CE94BB76C28F2E0@MWHP114MB0160.NAMP114.PROD.OUTLOOK.COM>
 <20171116103905.GA30455@fantomas.sk>
 <MWHP114MB01607B586795E57F9EA75E328F2E0@MWHP114MB0160.NAMP114.PROD.OUTLOOK.COM>
 <b9cde1c3-3ac0-38be-43ef-c8df270bbdfb@treenet.co.nz>
Message-ID: <MWHP114MB0160061C53CDC1F4AAE1CDFD8F2F0@MWHP114MB0160.NAMP114.PROD.OUTLOOK.COM>

Hello All,

Thanks for your help, we have resolved the issue once replaced the Old IP with the New IP under "cache_peer" in squid.conf file.


Regards
Vyas


-----Original Message-----
From: squid-users [mailto:squid-users-bounces at lists.squid-cache.org] On Behalf Of Amos Jeffries
Sent: Friday, November 17, 2017 8:19 AM
To: squid-users at lists.squid-cache.org
Subject: Re: [squid-users] [External] Re: Need help

On 17/11/17 02:08, Vayalpadu, Vedavyas wrote:
> Hello uhlar,
>
> No , I am bit new to squid proxy server, we have taken a TCP dump from the system and we see that.
>
> 1. From external application to proxy server the traffic is flowing, but from Proxy server to the internal application server traffic is not flowing.
> 2. But from Proxy server to the internal application, trace route and telnet is happening.
>

That log entry means that Squid was not able to locate any server IP addresses that will work for that transaction.

Either DNS has no results, client original dst-IP is not available, or the servers Squid does find are a) forbidden for use or b) currently DOWN according to the dynamic availability checks (ICMP echo, ICP query for cache_peer, and past TCP connection attempts).

The cache.log lines immediately following the one you quoted tell the results from DNS and each of those access controls so you can see what the reason for the failure was.

Amos
_______________________________________________
squid-users mailing list
squid-users at lists.squid-cache.org
https://urldefense.proofpoint.com/v2/url?u=http-3A__lists.squid-2Dcache.org_listinfo_squid-2Dusers&d=DwIGaQ&c=eIGjsITfXP_y-DLLX0uEHXJvU8nOHrUK8IrwNKOtkVU&r=tFxAuERmcRdMDY2ODYAvl6bEao1jdCMqbJq7uebMlVg&m=UsclePwFVCW_nTLv9f9aWlqPevA0YWHfBgMhHkW3UAU&s=xYvC3w5aHxuzbApxX_RUJEXBaJVXcrgBbTaTfXf95wg&e=

________________________________

This message is for the designated recipient only and may contain privileged, proprietary, or otherwise confidential information. If you have received it in error, please notify the sender immediately and delete the original. Any other use of the e-mail by you is prohibited. Where allowed by local law, electronic communications with Accenture and its affiliates, including e-mail and instant messaging (including content), may be scanned by our systems for the purposes of information security and assessment of internal compliance with Accenture policy.
______________________________________________________________________________________

www.accenture.com

From squid3 at treenet.co.nz  Fri Nov 17 07:59:15 2017
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Fri, 17 Nov 2017 20:59:15 +1300
Subject: [squid-users] [External] Re: Need help
In-Reply-To: <MWHP114MB0160061C53CDC1F4AAE1CDFD8F2F0@MWHP114MB0160.NAMP114.PROD.OUTLOOK.COM>
References: <MWHP114MB01604AD7D727E0CE94BB76C28F2E0@MWHP114MB0160.NAMP114.PROD.OUTLOOK.COM>
 <20171116103905.GA30455@fantomas.sk>
 <MWHP114MB01607B586795E57F9EA75E328F2E0@MWHP114MB0160.NAMP114.PROD.OUTLOOK.COM>
 <b9cde1c3-3ac0-38be-43ef-c8df270bbdfb@treenet.co.nz>
 <MWHP114MB0160061C53CDC1F4AAE1CDFD8F2F0@MWHP114MB0160.NAMP114.PROD.OUTLOOK.COM>
Message-ID: <c880952b-b965-415d-c9b7-7f2860f482e8@treenet.co.nz>

On 17/11/17 20:49, Vayalpadu, Vedavyas wrote:
> Hello All,
> 
> Thanks for your help, we have resolved the issue once replaced the Old IP with the New IP under "cache_peer" in squid.conf file.
> 

You know that you can place a hostname there right? no need to manually 
configure the IP address.

Amos


From squid3 at treenet.co.nz  Fri Nov 17 09:58:10 2017
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Fri, 17 Nov 2017 22:58:10 +1300
Subject: [squid-users] forward proxy to reverse proxy to app
In-Reply-To: <CACxnGeRJqUFXxukLhBArj9SMn5=c9uqpMfUuseoDRz2mXPL6=g@mail.gmail.com>
References: <CACxnGeRJqUFXxukLhBArj9SMn5=c9uqpMfUuseoDRz2mXPL6=g@mail.gmail.com>
Message-ID: <68bada7c-aebd-66cb-d6d8-1c480f508550@treenet.co.nz>

On 17/11/17 20:33, Bernhard D?bi wrote:
> Hi,
> 
> I try to configure squid for a very special usecase but can't get it
> to work. So, if you could give me some hints on how to do it right,
> that would be great
> 
> Here's what I try to achieve:
> 
> the browser has proxy:8080 configured as manual proxy
> from the browser I access some websites
> when the request is plain http then the reply must be a redirect to https
> when the request is https then the ssl connection must be termintaed
> on the proxy and the request must be forwarded as http to the
> application server


A forward/explicit proxy like yours is required to ensure that the 
security level of traffic remains unchanged across both client and 
server connections. Never downgraded without explicit knowledge by both 
endpoints. Bad problems ensue if you downgrade with either endpoint 
thinking it is secure end-to-end.


> 
> I know, I could just forget about ssl an go directly the app server
> with http bt the customer insists on that particular setup
> 
> we use several domains like app1.doma.com, app2.domb.biz, app3.domc.org
> in order to return the correct certificate for each request, I need a
> dedicated ip:port combination for each certificate

That is only relevant for *reverse-proxy*, not a forward/explicit proxy 
like yours.

If you have a explicit TLS connection between the clients and Squid 
forward/explicit you only need a certificate confirming Squid's hostname 
to the client.

If you are using SSL-Bump to decrypt the HTTPS traffic Squid can 
auto-generate certificates on the client connection based on the 
upstream server cert details.


Amos


From Walter.H at mathemainzel.info  Fri Nov 17 12:39:09 2017
From: Walter.H at mathemainzel.info (Walter H.)
Date: Fri, 17 Nov 2017 13:39:09 +0100
Subject: [squid-users] https://wiki.squid-cache.org provides invalid
	certificate chain ...
Message-ID: <5A0ED86D.3080208@mathemainzel.info>

for more information see
https://www.ssllabs.com/ssltest/analyze.html?d=wiki.squid-cache.org

- missing intermediate certificate
- ssl3 active, poodle vulnerable ...

Greetings,
Walter


-------------- next part --------------
A non-text attachment was scrubbed...
Name: smime.p7s
Type: application/pkcs7-signature
Size: 3491 bytes
Desc: S/MIME Cryptographic Signature
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20171117/cf9dfe7e/attachment.bin>

From joe.e.foster at googlemail.com  Fri Nov 17 12:45:48 2017
From: joe.e.foster at googlemail.com (Joe Foster)
Date: Fri, 17 Nov 2017 12:45:48 +0000
Subject: [squid-users] [Fwd: Re:  SSL Bump for regex URL comparison]
Message-ID: <1510922748.3771.8.camel@gmail.com>

Good morning,

I have tried the attached but I still receive the same result.

I have attached a screen shot to show what happens, its like there is no
connection. 

I have tried it with and without listing 3128 as a safe ssl port. I
imagine its not needed as its generated from Squid. 

HTTPS isn't connecting, HTTP is though that's no surprise, I'm only
diverting port 443 to port 3128. 

There are no logs being generated so I cant find out more. 

I can't for the life of me see what I'm doing wrong. 

Your advise if greatly received.

Thank you

Joe
 

I have the below rule added to my firewall for the redirect:
connection config redirect
        option proto 'tcp'
        option src 'lan'
        option src_ip '!192.168.1.101'
        option src_dport '443'
        option dest 'lan'
        option dest_ip '192.168.1.101'
        option dest_port '3128'
        option target 'DNAT'



On Thu, 2017-11-16 at 10:38 +0100, Matus UHLAR - fantomas wrote:
> On 16.11.17 08:21, Joe Foster wrote:
> >The problem is the connections are not getting through. It just acts
like
> >there is no WiFi connection.
> 
> what exactly is the error? Does squid receive those connections?
> does squid reject them?
> 

-------------- next part --------------
A non-text attachment was scrubbed...
Name: Screenshot from 2017-11-17 09-36-42.png
Type: image/png
Size: 61597 bytes
Desc: not available
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20171117/5a5888b1/attachment.png>
-------------- next part --------------
acl localnet src 10.0.0.0/8
acl localnet src 172.16.0.0/12
acl localnet src 192.168.1.0/16
acl localnet src fc00::/7
acl localnet src fe80::/10

acl ssl_ports port 443
acl ssl_ports port 3128

acl safe_ports port 80
acl safe_ports port 21
acl safe_ports port 443
acl safe_ports port 70
acl safe_ports port 210
acl safe_ports port 1025-65535
acl safe_ports port 280
acl safe_ports port 488
acl safe_ports port 591
acl safe_ports port 777
acl safe_ports port 3128
acl connect method connect

#acl ssl_ports port 3128
#http_port 3128 ssl-bump cert=/certs/myCA.pem

http_access deny !safe_ports
http_access deny connect !ssl_ports

http_access allow localhost manager
http_access deny manager

http_access deny to_localhost

acl bad_domain url_regex -i "/badwords"
acl good_domain url_regex -i "/safewords"

http_access deny bad_domain !good_domain
http_access allow good_domain

http_access allow localnet
http_access allow localhost

cache deny all

http_access deny all

# Squid normally listens to port 3128
http_port 3128 ssl-bump generate-host-certificates=on dynamic_cert_mem_cache_size=50KB cert=/certs/myCA.pem

ssl_bump server-first all

sslproxy_cert_error allow all
sslproxy_flags DONT_VERIFY_PEER

refresh_pattern ^ftp: 1440 20% 10080
refresh_pattern ^gopher: 1440 0% 1440
refresh_pattern -i (/cgi-bin/|\?) 0 0% 0
refresh_pattern . 0 20% 4320

access_log /squid.log
cache_log /squid1.log
cache_store_log stdio:/squid2.log
logfile_rotate 0

logfile_daemon /dev/null

From rousskov at measurement-factory.com  Fri Nov 17 15:19:49 2017
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Fri, 17 Nov 2017 08:19:49 -0700
Subject: [squid-users] CONNECT + custom data
In-Reply-To: <1cddc1ca-3b04-ab8c-98e1-0611cfa8ff95@treenet.co.nz>
References: <CABfb+p2oumUOHwVeqr-CB0u-zg6=93Ea0_jibrfODK5oHR2FHw@mail.gmail.com>
 <1cddc1ca-3b04-ab8c-98e1-0611cfa8ff95@treenet.co.nz>
Message-ID: <c5111c14-8508-06bc-dbd1-071f02b29e91@measurement-factory.com>

On 11/16/2017 08:39 PM, Amos Jeffries wrote:
> On 17/11/17 15:09, Richard Peeters wrote:
>> I have a requirement to forward proxy an opaque stream of data. One of
>> the servers (acting as a client -A- to SQUID ) will use the CONNECT
>> method to connect to SQUID (on server B) and squid will then proxy
>> this data for A.
>>
>> My question is I want to pass metadata from A to B which B will strip
>> out before proxying the data outbound, and I cannot find a way to do
>> that.


> "metadata" in HTTP just means headers.
> 
> For custom hop-by-hop headers your client application needs to use
> Connection: header to control their removal by the recieving next-hop
> HTTP agent. See <https://tools.ietf.org/html/rfc7230#section-6.1>.
> The custom header field-values can be accessed using the various
> request/reply header regex ACL types, same as any header.

Also, if Squid (B) does not talk to another proxy and instead connects
to the origin server directly, then no Connection listing is required
for the CONNECT request headers. In this case, the CONNECT request
received by B is the only HTTP request; it is not forwarded anywhere.

Alex.


From rentorbuy at yahoo.com  Fri Nov 17 15:27:51 2017
From: rentorbuy at yahoo.com (Vieri)
Date: Fri, 17 Nov 2017 15:27:51 +0000 (UTC)
Subject: [squid-users] block user agent
In-Reply-To: <4f2bc2b8-f307-e62c-5225-de3054e9e556@measurement-factory.com>
References: <160200155.402986.1510744710954.ref@mail.yahoo.com>
 <160200155.402986.1510744710954@mail.yahoo.com>
 <61ce5b3f-bb5e-235f-351c-f3b4ae339fd5@treenet.co.nz>
 <1140434830.1059565.1510821897156@mail.yahoo.com>
 <4f2bc2b8-f307-e62c-5225-de3054e9e556@measurement-factory.com>
Message-ID: <107610513.275578.1510932471911@mail.yahoo.com>

________________________________
From: Alex Rousskov <rousskov at measurement-factory.com>
> 1. Your "works" and "does not work" setups currently differ in at least
> three variables: user agent name, slash after the user agent name, and
> acl negation in http_access. Find out which single variable is
> responsible for the breakage by eliminating all other differences.
> 
> 2. Post two ALL,2 cache.logs, each containing a single transaction, one
> for the "works" case and one for the "does not work" case polished as
> discussed in #1.



I can't really do anything about #1 except maybe leave out the forward slash.
That's because my 2 examples are trying to achieve the opposite.
Let me just rephrase everything so it's crystal clear.

My goal is to deny all client traffic from browsers that DO NOT have a specific user-agent string. So this is a negated statement. One of the things I can't do in Squid is define an ACL with a negated lookahead such as (?!useragentname).

So I set up two examples.

Common to both:

acl allowed_useragent browser MyAllowedUAstring
acl denied_useragent browser MyDeniedUAstring

# example 1:
http_access deny denied_useragent
http_reply_access deny denied_useragent
deny_info http://proxy-server1/proxy-error/?a=%a&B=%B&e=%e&E=%E&H=%H&i=%i&M=%M&o=%o&R=%R&T=%T&U=%U&u=%u&w=%w&x=%x&acl=denied_useragent denied_useragent

I then run this from my test client:

# curl --insecure --user-agent MyAllowedUAstring https://www.gentoo.org
-> works as expected (I see the web site). I guess you don't need to see cache.log here.

Now I run this:

# curl --insecure --user-agent MyDeniedUAstring https://www.gentoo.org
-> works as expected (I'm denied access and I see Squid's error page).
I guess there's no need for the full log here either. It boils down to this anyway:
2017/11/17 13:24:26.937 kid1| 28,2| RegexData.cc(73) match: aclRegexData::match: match '(MyDeniedUAstring)' found in 'MyDeniedUAstring'
2017/11/17 13:24:26.937 kid1| 85,2| client_side_request.cc(745) clientAccessCheckDone: The request GET https://www.gentoo.org/ is DENIED; last ACL checked: denied_useragent

I'm done with example 1. That's because I cannot make a consistent list of all user agents I want to actively block. Instead, I want to "deny everyone except one or two".

Also, since negative lookaheads are not supported in regular expressions, I change my example 1 to:

# example 2:
http_access deny !allowed_useragent
http_reply_access deny !allowed_useragent
deny_info http://proxy-server1/proxy-error/?a=%a&B=%B&e=%e&E=%E&H=%H&i=%i&M=%M&o=%o&R=%R&T=%T&U=%U&u=%u&w=%w&x=%x&acl=denied_useragent allowed_useragent

Then I run this from the client:

# curl --insecure --user-agent MyAllowedUAstring https://www.gentoo.org
-> I was expecting to be allowed access since Squid denies "everything that's not" MyAllowedUAstring. Well, at least I should have passed the "deny" line in example 2.
However, I'm being blocked right there. This is the full log:

2017/11/17 13:30:42.216 kid1| 5,2| TcpAcceptor.cc(220) doAccept: New connection on FD 88
2017/11/17 13:30:42.216 kid1| 5,2| TcpAcceptor.cc(295) acceptNext: connection on local=[::]:3229 remote=[::] FD 88 flags=25
2017/11/17 13:30:42.216 kid1| 33,2| client_side.cc(3943) httpsSslBumpAccessCheckDone: sslBump needed for local=89.16.167.134:443 remote=10.215.144.48 FD 8 flags=17 method 4
2017/11/17 13:30:42.216 kid1| 11,2| client_side.cc(2372) parseHttpRequest: HTTP Client local=89.16.167.134:443 remote=10.215.144.48 FD 8 flags=17
2017/11/17 13:30:42.216 kid1| 11,2| client_side.cc(2373) parseHttpRequest: HTTP Client REQUEST:
---------
CONNECT 89.16.167.134:443 HTTP/1.1
Host: 89.16.167.134:443


----------
2017/11/17 13:30:42.216 kid1| 85,2| client_side_request.cc(745) clientAccessCheckDone: The request CONNECT 89.16.167.134:443 is DENIED; last ACL checked: allowed_useragent
2017/11/17 13:30:42.216 kid1| 20,2| store.cc(996) checkCachable: StoreEntry::checkCachable: NO: not cachable
2017/11/17 13:30:42.216 kid1| 20,2| store.cc(996) checkCachable: StoreEntry::checkCachable: NO: not cachable
2017/11/17 13:30:42.216 kid1| 20,2| store.cc(996) checkCachable: StoreEntry::checkCachable: NO: not cachable
2017/11/17 13:30:42.226 kid1| 83,2| client_side.cc(3843) clientNegotiateSSL: clientNegotiateSSL: New session 0x125e030 on FD 8 (10.215.144.48:65262)
2017/11/17 13:30:42.226 kid1| 11,2| client_side.cc(2372) parseHttpRequest: HTTP Client local=89.16.167.134:443 remote=10.215.144.48 FD 8 flags=17
2017/11/17 13:30:42.226 kid1| 11,2| client_side.cc(2373) parseHttpRequest: HTTP Client REQUEST:
---------
GET / HTTP/1.1
Host: www.gentoo.org
User-Agent: MyAllowedUAstring
Accept: */*


----------
2017/11/17 13:30:42.227 kid1| 28,2| RegexData.cc(73) match: aclRegexData::match: match '(MyAllowedUAstring)' found in 'MyAllowedUAstring'
2017/11/17 13:30:42.227 kid1| 88,2| client_side_reply.cc(2073) processReplyAccessResult: The reply for GET https://www.gentoo.org/ is ALLOWED, because it matched denied_mimetypes_rep
2017/11/17 13:30:42.227 kid1| 11,2| client_side.cc(1409) sendStartOfMessage: HTTP Client local=89.16.167.134:443 remote=10.215.144.48 FD 8 flags=17
2017/11/17 13:30:42.227 kid1| 11,2| client_side.cc(1410) sendStartOfMessage: HTTP Client REPLY:
---------
HTTP/1.1 307 Temporary Redirect
Server: squid
Mime-Version: 1.0
Date: Fri, 17 Nov 2017 12:30:42 GMT
Content-Type: text/html;charset=utf-8
Content-Length: 0
Location: http://proxy-server1/proxy-error/?a=-&B=&e=0&E=%5BNo%20Error%5D&H=89.16.167.134&i=10.215.144.48&M=CONNECT&o=&R=/&T=Fri,%2017%20Nov%202017%2012%3A30%3A42%20GMT&U=https%3A%2F%2F89.16.167.134%2F*&u=89.16.167.134%3A443&w=IT%40mydomain.org&x=&acl=denied_useragent
X-Squid-Error: 403 Access Denied
X-Cache: MISS from proxy-server1
X-Cache-Lookup: NONE from proxy-server1:3227
Connection: close

Note that I have these defaults in my squid conf file:

acl CONNECT method CONNECT
http_access deny CONNECT !SSL_ports

Let's try another one:

# curl --insecure --user-agent MyDeniedUAstring https://www.gentoo.org
-> This is as expected, I guess.

Full log:

2017/11/17 13:30:10.365 kid1| 5,2| TcpAcceptor.cc(220) doAccept: New connection on FD 88
2017/11/17 13:30:10.365 kid1| 5,2| TcpAcceptor.cc(295) acceptNext: connection on local=[::]:3229 remote=[::] FD 88 flags=25
2017/11/17 13:30:10.365 kid1| 33,2| client_side.cc(3943) httpsSslBumpAccessCheckDone: sslBump needed for local=89.16.167.134:443 remote=10.215.144.48 FD 8 flags=17 method 4
2017/11/17 13:30:10.365 kid1| 11,2| client_side.cc(2372) parseHttpRequest: HTTP Client local=89.16.167.134:443 remote=10.215.144.48 FD 8 flags=17
2017/11/17 13:30:10.365 kid1| 11,2| client_side.cc(2373) parseHttpRequest: HTTP Client REQUEST:
---------
CONNECT 89.16.167.134:443 HTTP/1.1
Host: 89.16.167.134:443


----------
2017/11/17 13:30:10.365 kid1| 85,2| client_side_request.cc(745) clientAccessCheckDone: The request CONNECT 89.16.167.134:443 is DENIED; last ACL checked: allowed_useragent
2017/11/17 13:30:10.365 kid1| 20,2| store.cc(996) checkCachable: StoreEntry::checkCachable: NO: not cachable
2017/11/17 13:30:10.365 kid1| 20,2| store.cc(996) checkCachable: StoreEntry::checkCachable: NO: not cachable
2017/11/17 13:30:10.365 kid1| 20,2| store.cc(996) checkCachable: StoreEntry::checkCachable: NO: not cachable
2017/11/17 13:30:10.385 kid1| 83,2| client_side.cc(3843) clientNegotiateSSL: clientNegotiateSSL: New session 0xdbdc70 on FD 8 (10.215.144.48:65237)
2017/11/17 13:30:10.386 kid1| 11,2| client_side.cc(2372) parseHttpRequest: HTTP Client local=89.16.167.134:443 remote=10.215.144.48 FD 8 flags=17
2017/11/17 13:30:10.386 kid1| 11,2| client_side.cc(2373) parseHttpRequest: HTTP Client REQUEST:
---------
GET / HTTP/1.1
Host: www.gentoo.org
User-Agent: MyDeniedUAstring
Accept: */*


----------
2017/11/17 13:30:10.386 kid1| 88,2| client_side_reply.cc(2073) processReplyAccessResult: The reply for GET https://www.gentoo.org/ is DENIED, because it matched allowed_useragent
2017/11/17 13:30:10.386 kid1| 20,2| store.cc(996) checkCachable: StoreEntry::checkCachable: NO: not cachable
2017/11/17 13:30:10.386 kid1| 20,2| store.cc(996) checkCachable: StoreEntry::checkCachable: NO: not cachable
2017/11/17 13:30:10.386 kid1| 20,2| store.cc(996) checkCachable: StoreEntry::checkCachable: NO: not cachable
2017/11/17 13:30:10.386 kid1| 88,2| client_side_reply.cc(2073) processReplyAccessResult: The reply for GET https://www.gentoo.org/ is ALLOWED, because it matched allowed_useragent
2017/11/17 13:30:10.386 kid1| 11,2| client_side.cc(1409) sendStartOfMessage: HTTP Client local=89.16.167.134:443 remote=10.215.144.48 FD 8 flags=17
2017/11/17 13:30:10.386 kid1| 11,2| client_side.cc(1410) sendStartOfMessage: HTTP Client REPLY:
---------
HTTP/1.1 302 Found
Server: squid
Mime-Version: 1.0
Date: Fri, 17 Nov 2017 12:30:10 GMT
Content-Type: text/html;charset=utf-8
Content-Length: 0
Location: http://proxy-server1/proxy-error/?a=-&B=&e=0&E=%5BNo%20Error%5D&H=www.gentoo.org&i=10.215.144.48&M=GET&o=&R=/&T=Fri,%2017%20Nov%202017%2012%3A30%3A10%20GMT&U=https%3A%2F%2Fwww.gentoo.org%2F&u=https%3A%2F%2Fwww.gentoo.org%2F&w=IT%40mydomain.org&x=&acl=denied_useragent
X-Squid-Error: 403 Access Denied
X-Cache: MISS from proxy-server1
X-Cache-Lookup: NONE from proxy-server1:3227
Connection: close

Now for plain HTTP with example 2.

# curl --user-agent MyDeniedUAstring http://www.fltk.org/index.php
-> As expected. It blocks access.

Full log:

2017/11/17 15:56:52.648 kid1| 5,2| TcpAcceptor.cc(220) doAccept: New connection on FD 85
2017/11/17 15:56:52.648 kid1| 5,2| TcpAcceptor.cc(295) acceptNext: connection on local=[::]:3228 remote=[::] FD 85 flags=25
2017/11/17 15:56:52.648 kid1| 11,2| client_side.cc(2372) parseHttpRequest: HTTP Client local=66.39.46.122:80 remote=10.215.144.48 FD 8 flags=17
2017/11/17 15:56:52.648 kid1| 11,2| client_side.cc(2373) parseHttpRequest: HTTP Client REQUEST:
---------
GET /index.php HTTP/1.1
Host: www.fltk.org
User-Agent: MyDeniedUAstring
Accept: */*


----------
2017/11/17 15:56:52.648 kid1| 85,2| client_side_request.cc(745) clientAccessCheckDone: The request GET http://www.fltk.org/index.php is DENIED; last ACL checked: allowed_useragent
2017/11/17 15:56:52.648 kid1| 20,2| store.cc(996) checkCachable: StoreEntry::checkCachable: NO: not cachable
2017/11/17 15:56:52.648 kid1| 20,2| store.cc(996) checkCachable: StoreEntry::checkCachable: NO: not cachable
2017/11/17 15:56:52.648 kid1| 20,2| store.cc(996) checkCachable: StoreEntry::checkCachable: NO: not cachable
2017/11/17 15:56:52.648 kid1| 88,2| client_side_reply.cc(2073) processReplyAccessResult: The reply for GET http://www.fltk.org/index.php is ALLOWED, because it matched allowed_useragent
2017/11/17 15:56:52.648 kid1| 11,2| client_side.cc(1409) sendStartOfMessage: HTTP Client local=66.39.46.122:80 remote=10.215.144.48 FD 8 flags=17
2017/11/17 15:56:52.648 kid1| 11,2| client_side.cc(1410) sendStartOfMessage: HTTP Client REPLY:
---------
HTTP/1.1 302 Found
Server: squid
Mime-Version: 1.0
Date: Fri, 17 Nov 2017 14:56:52 GMT
Content-Type: text/html;charset=utf-8
Content-Length: 0
Location: http://proxy-server1/proxy-error/?a=-&B=&e=0&E=%5BNo%20Error%5D&H=www.fltk.org&i=10.215.144.48&M=GET&o=&R=/index.php&T=Fri,%2017%20Nov%202017%2014%3A56%3A52%20GMT&U=http%3A%2F%2Fwww.fltk.org%2Findex.php&u=http%3A%2F%2Fwww.fltk.org%2Findex.php&w=IT%40mydomain.org&x=&acl=denied_useragent
X-Squid-Error: 403 Access Denied
X-Cache: MISS from proxy-server1
X-Cache-Lookup: NONE from proxy-server1:3227
Connection: keep-alive

However, now comes the interesting part.

# curl --user-agent MyAllowedUAstring http://www.fltk.org/index.php
-> works as expected (I see the web site). 

Full log:

2017/11/17 15:55:23.550 kid1| 5,2| TcpAcceptor.cc(220) doAccept: New connection on FD 85
2017/11/17 15:55:23.550 kid1| 5,2| TcpAcceptor.cc(295) acceptNext: connection on local=[::]:3228 remote=[::] FD 85 flags=25
2017/11/17 15:55:23.551 kid1| 11,2| client_side.cc(2372) parseHttpRequest: HTTP Client local=66.39.46.122:80 remote=10.215.144.48 FD 8 flags=17
2017/11/17 15:55:23.551 kid1| 11,2| client_side.cc(2373) parseHttpRequest: HTTP Client REQUEST:
---------
GET /index.php HTTP/1.1
Host: www.fltk.org
User-Agent: MyAllowedUAstring
Accept: */*


----------
2017/11/17 15:55:23.551 kid1| 28,2| RegexData.cc(73) match: aclRegexData::match: match '(MyAllowedUAstring)' found in 'MyAllowedUAstring'
2017/11/17 15:55:23.551 kid1| 82,2| external_acl.cc(805) aclMatchExternal: bllookup("http www.fltk.org 80 /index.php") = lookup needed
2017/11/17 15:55:23.551 kid1| 82,2| external_acl.cc(808) aclMatchExternal: "http www.fltk.org 80 /index.php": queueing a call.
2017/11/17 15:55:23.551 kid1| 82,2| external_acl.cc(1444) Start: fg lookup in 'bllookup' for 'http www.fltk.org 80 /index.php'
2017/11/17 15:55:23.551 kid1| 82,2| external_acl.cc(811) aclMatchExternal: "http www.fltk.org 80 /index.php": return -1.
2017/11/17 15:55:23.553 kid1| 82,2| external_acl.cc(1372) externalAclHandleReply: reply={result=OK, notes={message: www.fltk.org site not found in blacklist; }}
2017/11/17 15:55:23.553 kid1| 82,2| external_acl.cc(1288) external_acl_cache_add: external_acl_cache_add: Adding 'http www.fltk.org 80 /index.php' = ALLOWED
2017/11/17 15:55:23.553 kid1| 82,2| external_acl.cc(841) aclMatchExternal: bllookup = ALLOWED
2017/11/17 15:55:23.553 kid1| 85,2| client_side_request.cc(745) clientAccessCheckDone: The request GET http://www.fltk.org/index.php is ALLOWED; last ACL checked: bl_lookup
2017/11/17 15:55:23.553 kid1| 85,2| client_side_request.cc(721) clientAccessCheck2: No adapted_http_access configuration. default: ALLOW
2017/11/17 15:55:23.553 kid1| 85,2| client_side_request.cc(745) clientAccessCheckDone: The request GET http://www.fltk.org/index.php is ALLOWED; last ACL checked: bl_lookup
2017/11/17 15:55:23.554 kid1| 88,2| client_side_reply.cc(593) cacheHit: clientProcessHit: Vary detected!
2017/11/17 15:55:23.554 kid1| 17,2| FwdState.cc(133) FwdState: Forwarding client request local=66.39.46.122:80 remote=10.215.144.48 FD 8 flags=17, url=http://www.fltk.org/index.php
2017/11/17 15:55:23.554 kid1| 44,2| peer_select.cc(280) peerSelectDnsPaths: Found sources for 'http://www.fltk.org/index.php'
2017/11/17 15:55:23.554 kid1| 44,2| peer_select.cc(281) peerSelectDnsPaths:   always_direct = DENIED
2017/11/17 15:55:23.554 kid1| 44,2| peer_select.cc(282) peerSelectDnsPaths:    never_direct = DENIED
2017/11/17 15:55:23.554 kid1| 44,2| peer_select.cc(288) peerSelectDnsPaths:    ORIGINAL_DST = local=10.215.144.48 remote=66.39.46.122:80 flags=25
2017/11/17 15:55:23.554 kid1| 44,2| peer_select.cc(295) peerSelectDnsPaths:        timedout = 0
2017/11/17 15:55:23.708 kid1| 11,2| http.cc(2229) sendRequest: HTTP Server local=10.215.144.48:35373 remote=66.39.46.122:80 FD 13 flags=25
2017/11/17 15:55:23.708 kid1| 11,2| http.cc(2230) sendRequest: HTTP Server REQUEST:
---------
GET /index.php HTTP/1.1
User-Agent: MyAllowedUAstring
Accept: */*
Host: www.fltk.org
Cache-Control: max-age=259200
Connection: keep-alive


----------
2017/11/17 15:55:23.884 kid1| ctx: enter level  0: 'http://www.fltk.org/index.php'
2017/11/17 15:55:23.884 kid1| 11,2| http.cc(719) processReplyHeader: HTTP Server local=10.215.144.48:35373 remote=66.39.46.122:80 FD 13 flags=25
2017/11/17 15:55:23.884 kid1| 11,2| http.cc(720) processReplyHeader: HTTP Server REPLY:
---------
HTTP/1.1 200 OK
Date: Fri, 17 Nov 2017 14:55:23 GMT
Server: Apache/2.4.29
Cache-Control: no-cache
Vary: Accept-Encoding
Keep-Alive: timeout=5, max=100
Connection: Keep-Alive
Transfer-Encoding: chunked
Content-Type: text/html

3a02
<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN" "http://www.w3.org/TR/html4/loose.dtd">
<html>
<head>
<title>Fast Light Toolkit - Fast Light Toolkit (FLTK)</title>
<meta http-equiv='Pragma' content='no-cache'>
<meta http-equiv='Content-Type' content='text/html; charset=utf-8'>
<link rel='stylesheet' type='text/css' href='fltk.css'>
<link rel='alternate' title='FLTK RSS' type='application/rss+xml' href='index.rss'>
<link rel='shortcut icon' href='favicon.ico' type='image/x-icon'>
<meta name='keywords' content='gui toolkit,c++,linux,unix,macos x,x11,windows'>
</head>
<body>
<table width='100%' border='0' cellspacing='0' cellpadding='0' summary='Page'>
<tr class='header'><td valign='top' width='15' rowspan='2'><a href='index.php'><img src='images/top-left.gif' width='15' height='70' border='0' alt=''></a></td><td valign='top' width='224' rowspan='2'><a href='index.php'><img src='images/top-middle.gif' width='224' height='70' border='0' alt=''></a></td><td width='100%' height='40'><h1>Fast Light Toolkit</h1> </td><td align='right' nowrap>
<table cellpadding=0 cellspacing=0 border=0><tr><td valign=top nowrap>
<a href=fltk-rss.xml><img src=images/rss-fee
----------
2017/11/17 15:55:23.885 kid1| ctx: exit level  0
2017/11/17 15:55:23.885 kid1| 23,2| url.cc(407) urlParse: urlParse: URI has whitespace: {icap://127.0.0.1:1344/clamav ICAP/1.0
}
2017/11/17 15:55:24.038 kid1| 28,2| RegexData.cc(73) match: aclRegexData::match: match '(MyAllowedUAstring)' found in 'MyAllowedUAstring'
2017/11/17 15:55:24.038 kid1| 88,2| client_side_reply.cc(2073) processReplyAccessResult: The reply for GET http://www.fltk.org/index.php is ALLOWED, because it matched denied_mimetypes_rep
2017/11/17 15:55:24.038 kid1| 11,2| client_side.cc(1409) sendStartOfMessage: HTTP Client local=66.39.46.122:80 remote=10.215.144.48 FD 8 flags=17
2017/11/17 15:55:24.038 kid1| 11,2| client_side.cc(1410) sendStartOfMessage: HTTP Client REPLY:
---------
HTTP/1.1 200 OK
Date: Fri, 17 Nov 2017 14:55:23 GMT
Server: Apache/2.4.29
Cache-Control: no-cache
Vary: Accept-Encoding
Content-Type: text/html
Via: ICAP/1.0 proxy-server1.hospitalmanacor.org (C-ICAP/0.5.2 SquidClamav/Antivirus service )
X-Cache: MISS from proxy-server1
X-Cache-Lookup: MISS from proxy-server1:3227
Transfer-Encoding: chunked
Connection: keep-alive

How can I modify my example 2 settings so this access control works the same way with both http and https in an ssl-bumped environment.

Thanks,

Vieri


From yvoinov at gmail.com  Fri Nov 17 15:29:54 2017
From: yvoinov at gmail.com (Yuri)
Date: Fri, 17 Nov 2017 21:29:54 +0600
Subject: [squid-users] block user agent
In-Reply-To: <107610513.275578.1510932471911@mail.yahoo.com>
References: <160200155.402986.1510744710954.ref@mail.yahoo.com>
 <160200155.402986.1510744710954@mail.yahoo.com>
 <61ce5b3f-bb5e-235f-351c-f3b4ae339fd5@treenet.co.nz>
 <1140434830.1059565.1510821897156@mail.yahoo.com>
 <4f2bc2b8-f307-e62c-5225-de3054e9e556@measurement-factory.com>
 <107610513.275578.1510932471911@mail.yahoo.com>
Message-ID: <b9db7cbf-2591-c04c-7fb0-220da8db1509@gmail.com>



17.11.2017 21:27, Vieri ?????:
> ________________________________
> From: Alex Rousskov <rousskov at measurement-factory.com>
>> 1. Your "works" and "does not work" setups currently differ in at least
>> three variables: user agent name, slash after the user agent name, and
>> acl negation in http_access. Find out which single variable is
>> responsible for the breakage by eliminating all other differences.
>>
>> 2. Post two ALL,2 cache.logs, each containing a single transaction, one
>> for the "works" case and one for the "does not work" case polished as
>> discussed in #1.
>
>
> I can't really do anything about #1 except maybe leave out the forward slash.
> That's because my 2 examples are trying to achieve the opposite.
> Let me just rephrase everything so it's crystal clear.
>
> My goal is to deny all client traffic from browsers that DO NOT have a specific user-agent string. So this is a negated statement. One of the things I can't do in Squid is define an ACL with a negated lookahead such as (?!useragentname).
I hope you listen about browser extensions for UA spoofing?
>
> So I set up two examples.
>
> Common to both:
>
> acl allowed_useragent browser MyAllowedUAstring
> acl denied_useragent browser MyDeniedUAstring
>
> # example 1:
> http_access deny denied_useragent
> http_reply_access deny denied_useragent
> deny_info http://proxy-server1/proxy-error/?a=%a&B=%B&e=%e&E=%E&H=%H&i=%i&M=%M&o=%o&R=%R&T=%T&U=%U&u=%u&w=%w&x=%x&acl=denied_useragent denied_useragent
>
> I then run this from my test client:
>
> # curl --insecure --user-agent MyAllowedUAstring https://www.gentoo.org
> -> works as expected (I see the web site). I guess you don't need to see cache.log here.
>
> Now I run this:
>
> # curl --insecure --user-agent MyDeniedUAstring https://www.gentoo.org
> -> works as expected (I'm denied access and I see Squid's error page).
> I guess there's no need for the full log here either. It boils down to this anyway:
> 2017/11/17 13:24:26.937 kid1| 28,2| RegexData.cc(73) match: aclRegexData::match: match '(MyDeniedUAstring)' found in 'MyDeniedUAstring'
> 2017/11/17 13:24:26.937 kid1| 85,2| client_side_request.cc(745) clientAccessCheckDone: The request GET https://www.gentoo.org/ is DENIED; last ACL checked: denied_useragent
>
> I'm done with example 1. That's because I cannot make a consistent list of all user agents I want to actively block. Instead, I want to "deny everyone except one or two".
>
> Also, since negative lookaheads are not supported in regular expressions, I change my example 1 to:
>
> # example 2:
> http_access deny !allowed_useragent
> http_reply_access deny !allowed_useragent
> deny_info http://proxy-server1/proxy-error/?a=%a&B=%B&e=%e&E=%E&H=%H&i=%i&M=%M&o=%o&R=%R&T=%T&U=%U&u=%u&w=%w&x=%x&acl=denied_useragent allowed_useragent
>
> Then I run this from the client:
>
> # curl --insecure --user-agent MyAllowedUAstring https://www.gentoo.org
> -> I was expecting to be allowed access since Squid denies "everything that's not" MyAllowedUAstring. Well, at least I should have passed the "deny" line in example 2.
> However, I'm being blocked right there. This is the full log:
>
> 2017/11/17 13:30:42.216 kid1| 5,2| TcpAcceptor.cc(220) doAccept: New connection on FD 88
> 2017/11/17 13:30:42.216 kid1| 5,2| TcpAcceptor.cc(295) acceptNext: connection on local=[::]:3229 remote=[::] FD 88 flags=25
> 2017/11/17 13:30:42.216 kid1| 33,2| client_side.cc(3943) httpsSslBumpAccessCheckDone: sslBump needed for local=89.16.167.134:443 remote=10.215.144.48 FD 8 flags=17 method 4
> 2017/11/17 13:30:42.216 kid1| 11,2| client_side.cc(2372) parseHttpRequest: HTTP Client local=89.16.167.134:443 remote=10.215.144.48 FD 8 flags=17
> 2017/11/17 13:30:42.216 kid1| 11,2| client_side.cc(2373) parseHttpRequest: HTTP Client REQUEST:
> ---------
> CONNECT 89.16.167.134:443 HTTP/1.1
> Host: 89.16.167.134:443
>
>
> ----------
> 2017/11/17 13:30:42.216 kid1| 85,2| client_side_request.cc(745) clientAccessCheckDone: The request CONNECT 89.16.167.134:443 is DENIED; last ACL checked: allowed_useragent
> 2017/11/17 13:30:42.216 kid1| 20,2| store.cc(996) checkCachable: StoreEntry::checkCachable: NO: not cachable
> 2017/11/17 13:30:42.216 kid1| 20,2| store.cc(996) checkCachable: StoreEntry::checkCachable: NO: not cachable
> 2017/11/17 13:30:42.216 kid1| 20,2| store.cc(996) checkCachable: StoreEntry::checkCachable: NO: not cachable
> 2017/11/17 13:30:42.226 kid1| 83,2| client_side.cc(3843) clientNegotiateSSL: clientNegotiateSSL: New session 0x125e030 on FD 8 (10.215.144.48:65262)
> 2017/11/17 13:30:42.226 kid1| 11,2| client_side.cc(2372) parseHttpRequest: HTTP Client local=89.16.167.134:443 remote=10.215.144.48 FD 8 flags=17
> 2017/11/17 13:30:42.226 kid1| 11,2| client_side.cc(2373) parseHttpRequest: HTTP Client REQUEST:
> ---------
> GET / HTTP/1.1
> Host: www.gentoo.org
> User-Agent: MyAllowedUAstring
> Accept: */*
>
>
> ----------
> 2017/11/17 13:30:42.227 kid1| 28,2| RegexData.cc(73) match: aclRegexData::match: match '(MyAllowedUAstring)' found in 'MyAllowedUAstring'
> 2017/11/17 13:30:42.227 kid1| 88,2| client_side_reply.cc(2073) processReplyAccessResult: The reply for GET https://www.gentoo.org/ is ALLOWED, because it matched denied_mimetypes_rep
> 2017/11/17 13:30:42.227 kid1| 11,2| client_side.cc(1409) sendStartOfMessage: HTTP Client local=89.16.167.134:443 remote=10.215.144.48 FD 8 flags=17
> 2017/11/17 13:30:42.227 kid1| 11,2| client_side.cc(1410) sendStartOfMessage: HTTP Client REPLY:
> ---------
> HTTP/1.1 307 Temporary Redirect
> Server: squid
> Mime-Version: 1.0
> Date: Fri, 17 Nov 2017 12:30:42 GMT
> Content-Type: text/html;charset=utf-8
> Content-Length: 0
> Location: http://proxy-server1/proxy-error/?a=-&B=&e=0&E=%5BNo%20Error%5D&H=89.16.167.134&i=10.215.144.48&M=CONNECT&o=&R=/&T=Fri,%2017%20Nov%202017%2012%3A30%3A42%20GMT&U=https%3A%2F%2F89.16.167.134%2F*&u=89.16.167.134%3A443&w=IT%40mydomain.org&x=&acl=denied_useragent
> X-Squid-Error: 403 Access Denied
> X-Cache: MISS from proxy-server1
> X-Cache-Lookup: NONE from proxy-server1:3227
> Connection: close
>
> Note that I have these defaults in my squid conf file:
>
> acl CONNECT method CONNECT
> http_access deny CONNECT !SSL_ports
>
> Let's try another one:
>
> # curl --insecure --user-agent MyDeniedUAstring https://www.gentoo.org
> -> This is as expected, I guess.
>
> Full log:
>
> 2017/11/17 13:30:10.365 kid1| 5,2| TcpAcceptor.cc(220) doAccept: New connection on FD 88
> 2017/11/17 13:30:10.365 kid1| 5,2| TcpAcceptor.cc(295) acceptNext: connection on local=[::]:3229 remote=[::] FD 88 flags=25
> 2017/11/17 13:30:10.365 kid1| 33,2| client_side.cc(3943) httpsSslBumpAccessCheckDone: sslBump needed for local=89.16.167.134:443 remote=10.215.144.48 FD 8 flags=17 method 4
> 2017/11/17 13:30:10.365 kid1| 11,2| client_side.cc(2372) parseHttpRequest: HTTP Client local=89.16.167.134:443 remote=10.215.144.48 FD 8 flags=17
> 2017/11/17 13:30:10.365 kid1| 11,2| client_side.cc(2373) parseHttpRequest: HTTP Client REQUEST:
> ---------
> CONNECT 89.16.167.134:443 HTTP/1.1
> Host: 89.16.167.134:443
>
>
> ----------
> 2017/11/17 13:30:10.365 kid1| 85,2| client_side_request.cc(745) clientAccessCheckDone: The request CONNECT 89.16.167.134:443 is DENIED; last ACL checked: allowed_useragent
> 2017/11/17 13:30:10.365 kid1| 20,2| store.cc(996) checkCachable: StoreEntry::checkCachable: NO: not cachable
> 2017/11/17 13:30:10.365 kid1| 20,2| store.cc(996) checkCachable: StoreEntry::checkCachable: NO: not cachable
> 2017/11/17 13:30:10.365 kid1| 20,2| store.cc(996) checkCachable: StoreEntry::checkCachable: NO: not cachable
> 2017/11/17 13:30:10.385 kid1| 83,2| client_side.cc(3843) clientNegotiateSSL: clientNegotiateSSL: New session 0xdbdc70 on FD 8 (10.215.144.48:65237)
> 2017/11/17 13:30:10.386 kid1| 11,2| client_side.cc(2372) parseHttpRequest: HTTP Client local=89.16.167.134:443 remote=10.215.144.48 FD 8 flags=17
> 2017/11/17 13:30:10.386 kid1| 11,2| client_side.cc(2373) parseHttpRequest: HTTP Client REQUEST:
> ---------
> GET / HTTP/1.1
> Host: www.gentoo.org
> User-Agent: MyDeniedUAstring
> Accept: */*
>
>
> ----------
> 2017/11/17 13:30:10.386 kid1| 88,2| client_side_reply.cc(2073) processReplyAccessResult: The reply for GET https://www.gentoo.org/ is DENIED, because it matched allowed_useragent
> 2017/11/17 13:30:10.386 kid1| 20,2| store.cc(996) checkCachable: StoreEntry::checkCachable: NO: not cachable
> 2017/11/17 13:30:10.386 kid1| 20,2| store.cc(996) checkCachable: StoreEntry::checkCachable: NO: not cachable
> 2017/11/17 13:30:10.386 kid1| 20,2| store.cc(996) checkCachable: StoreEntry::checkCachable: NO: not cachable
> 2017/11/17 13:30:10.386 kid1| 88,2| client_side_reply.cc(2073) processReplyAccessResult: The reply for GET https://www.gentoo.org/ is ALLOWED, because it matched allowed_useragent
> 2017/11/17 13:30:10.386 kid1| 11,2| client_side.cc(1409) sendStartOfMessage: HTTP Client local=89.16.167.134:443 remote=10.215.144.48 FD 8 flags=17
> 2017/11/17 13:30:10.386 kid1| 11,2| client_side.cc(1410) sendStartOfMessage: HTTP Client REPLY:
> ---------
> HTTP/1.1 302 Found
> Server: squid
> Mime-Version: 1.0
> Date: Fri, 17 Nov 2017 12:30:10 GMT
> Content-Type: text/html;charset=utf-8
> Content-Length: 0
> Location: http://proxy-server1/proxy-error/?a=-&B=&e=0&E=%5BNo%20Error%5D&H=www.gentoo.org&i=10.215.144.48&M=GET&o=&R=/&T=Fri,%2017%20Nov%202017%2012%3A30%3A10%20GMT&U=https%3A%2F%2Fwww.gentoo.org%2F&u=https%3A%2F%2Fwww.gentoo.org%2F&w=IT%40mydomain.org&x=&acl=denied_useragent
> X-Squid-Error: 403 Access Denied
> X-Cache: MISS from proxy-server1
> X-Cache-Lookup: NONE from proxy-server1:3227
> Connection: close
>
> Now for plain HTTP with example 2.
>
> # curl --user-agent MyDeniedUAstring http://www.fltk.org/index.php
> -> As expected. It blocks access.
>
> Full log:
>
> 2017/11/17 15:56:52.648 kid1| 5,2| TcpAcceptor.cc(220) doAccept: New connection on FD 85
> 2017/11/17 15:56:52.648 kid1| 5,2| TcpAcceptor.cc(295) acceptNext: connection on local=[::]:3228 remote=[::] FD 85 flags=25
> 2017/11/17 15:56:52.648 kid1| 11,2| client_side.cc(2372) parseHttpRequest: HTTP Client local=66.39.46.122:80 remote=10.215.144.48 FD 8 flags=17
> 2017/11/17 15:56:52.648 kid1| 11,2| client_side.cc(2373) parseHttpRequest: HTTP Client REQUEST:
> ---------
> GET /index.php HTTP/1.1
> Host: www.fltk.org
> User-Agent: MyDeniedUAstring
> Accept: */*
>
>
> ----------
> 2017/11/17 15:56:52.648 kid1| 85,2| client_side_request.cc(745) clientAccessCheckDone: The request GET http://www.fltk.org/index.php is DENIED; last ACL checked: allowed_useragent
> 2017/11/17 15:56:52.648 kid1| 20,2| store.cc(996) checkCachable: StoreEntry::checkCachable: NO: not cachable
> 2017/11/17 15:56:52.648 kid1| 20,2| store.cc(996) checkCachable: StoreEntry::checkCachable: NO: not cachable
> 2017/11/17 15:56:52.648 kid1| 20,2| store.cc(996) checkCachable: StoreEntry::checkCachable: NO: not cachable
> 2017/11/17 15:56:52.648 kid1| 88,2| client_side_reply.cc(2073) processReplyAccessResult: The reply for GET http://www.fltk.org/index.php is ALLOWED, because it matched allowed_useragent
> 2017/11/17 15:56:52.648 kid1| 11,2| client_side.cc(1409) sendStartOfMessage: HTTP Client local=66.39.46.122:80 remote=10.215.144.48 FD 8 flags=17
> 2017/11/17 15:56:52.648 kid1| 11,2| client_side.cc(1410) sendStartOfMessage: HTTP Client REPLY:
> ---------
> HTTP/1.1 302 Found
> Server: squid
> Mime-Version: 1.0
> Date: Fri, 17 Nov 2017 14:56:52 GMT
> Content-Type: text/html;charset=utf-8
> Content-Length: 0
> Location: http://proxy-server1/proxy-error/?a=-&B=&e=0&E=%5BNo%20Error%5D&H=www.fltk.org&i=10.215.144.48&M=GET&o=&R=/index.php&T=Fri,%2017%20Nov%202017%2014%3A56%3A52%20GMT&U=http%3A%2F%2Fwww.fltk.org%2Findex.php&u=http%3A%2F%2Fwww.fltk.org%2Findex.php&w=IT%40mydomain.org&x=&acl=denied_useragent
> X-Squid-Error: 403 Access Denied
> X-Cache: MISS from proxy-server1
> X-Cache-Lookup: NONE from proxy-server1:3227
> Connection: keep-alive
>
> However, now comes the interesting part.
>
> # curl --user-agent MyAllowedUAstring http://www.fltk.org/index.php
> -> works as expected (I see the web site). 
>
> Full log:
>
> 2017/11/17 15:55:23.550 kid1| 5,2| TcpAcceptor.cc(220) doAccept: New connection on FD 85
> 2017/11/17 15:55:23.550 kid1| 5,2| TcpAcceptor.cc(295) acceptNext: connection on local=[::]:3228 remote=[::] FD 85 flags=25
> 2017/11/17 15:55:23.551 kid1| 11,2| client_side.cc(2372) parseHttpRequest: HTTP Client local=66.39.46.122:80 remote=10.215.144.48 FD 8 flags=17
> 2017/11/17 15:55:23.551 kid1| 11,2| client_side.cc(2373) parseHttpRequest: HTTP Client REQUEST:
> ---------
> GET /index.php HTTP/1.1
> Host: www.fltk.org
> User-Agent: MyAllowedUAstring
> Accept: */*
>
>
> ----------
> 2017/11/17 15:55:23.551 kid1| 28,2| RegexData.cc(73) match: aclRegexData::match: match '(MyAllowedUAstring)' found in 'MyAllowedUAstring'
> 2017/11/17 15:55:23.551 kid1| 82,2| external_acl.cc(805) aclMatchExternal: bllookup("http www.fltk.org 80 /index.php") = lookup needed
> 2017/11/17 15:55:23.551 kid1| 82,2| external_acl.cc(808) aclMatchExternal: "http www.fltk.org 80 /index.php": queueing a call.
> 2017/11/17 15:55:23.551 kid1| 82,2| external_acl.cc(1444) Start: fg lookup in 'bllookup' for 'http www.fltk.org 80 /index.php'
> 2017/11/17 15:55:23.551 kid1| 82,2| external_acl.cc(811) aclMatchExternal: "http www.fltk.org 80 /index.php": return -1.
> 2017/11/17 15:55:23.553 kid1| 82,2| external_acl.cc(1372) externalAclHandleReply: reply={result=OK, notes={message: www.fltk.org site not found in blacklist; }}
> 2017/11/17 15:55:23.553 kid1| 82,2| external_acl.cc(1288) external_acl_cache_add: external_acl_cache_add: Adding 'http www.fltk.org 80 /index.php' = ALLOWED
> 2017/11/17 15:55:23.553 kid1| 82,2| external_acl.cc(841) aclMatchExternal: bllookup = ALLOWED
> 2017/11/17 15:55:23.553 kid1| 85,2| client_side_request.cc(745) clientAccessCheckDone: The request GET http://www.fltk.org/index.php is ALLOWED; last ACL checked: bl_lookup
> 2017/11/17 15:55:23.553 kid1| 85,2| client_side_request.cc(721) clientAccessCheck2: No adapted_http_access configuration. default: ALLOW
> 2017/11/17 15:55:23.553 kid1| 85,2| client_side_request.cc(745) clientAccessCheckDone: The request GET http://www.fltk.org/index.php is ALLOWED; last ACL checked: bl_lookup
> 2017/11/17 15:55:23.554 kid1| 88,2| client_side_reply.cc(593) cacheHit: clientProcessHit: Vary detected!
> 2017/11/17 15:55:23.554 kid1| 17,2| FwdState.cc(133) FwdState: Forwarding client request local=66.39.46.122:80 remote=10.215.144.48 FD 8 flags=17, url=http://www.fltk.org/index.php
> 2017/11/17 15:55:23.554 kid1| 44,2| peer_select.cc(280) peerSelectDnsPaths: Found sources for 'http://www.fltk.org/index.php'
> 2017/11/17 15:55:23.554 kid1| 44,2| peer_select.cc(281) peerSelectDnsPaths:   always_direct = DENIED
> 2017/11/17 15:55:23.554 kid1| 44,2| peer_select.cc(282) peerSelectDnsPaths:    never_direct = DENIED
> 2017/11/17 15:55:23.554 kid1| 44,2| peer_select.cc(288) peerSelectDnsPaths:    ORIGINAL_DST = local=10.215.144.48 remote=66.39.46.122:80 flags=25
> 2017/11/17 15:55:23.554 kid1| 44,2| peer_select.cc(295) peerSelectDnsPaths:        timedout = 0
> 2017/11/17 15:55:23.708 kid1| 11,2| http.cc(2229) sendRequest: HTTP Server local=10.215.144.48:35373 remote=66.39.46.122:80 FD 13 flags=25
> 2017/11/17 15:55:23.708 kid1| 11,2| http.cc(2230) sendRequest: HTTP Server REQUEST:
> ---------
> GET /index.php HTTP/1.1
> User-Agent: MyAllowedUAstring
> Accept: */*
> Host: www.fltk.org
> Cache-Control: max-age=259200
> Connection: keep-alive
>
>
> ----------
> 2017/11/17 15:55:23.884 kid1| ctx: enter level  0: 'http://www.fltk.org/index.php'
> 2017/11/17 15:55:23.884 kid1| 11,2| http.cc(719) processReplyHeader: HTTP Server local=10.215.144.48:35373 remote=66.39.46.122:80 FD 13 flags=25
> 2017/11/17 15:55:23.884 kid1| 11,2| http.cc(720) processReplyHeader: HTTP Server REPLY:
> ---------
> HTTP/1.1 200 OK
> Date: Fri, 17 Nov 2017 14:55:23 GMT
> Server: Apache/2.4.29
> Cache-Control: no-cache
> Vary: Accept-Encoding
> Keep-Alive: timeout=5, max=100
> Connection: Keep-Alive
> Transfer-Encoding: chunked
> Content-Type: text/html
>
> 3a02
> <!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN" "http://www.w3.org/TR/html4/loose.dtd">
> <html>
> <head>
> <title>Fast Light Toolkit - Fast Light Toolkit (FLTK)</title>
> <meta http-equiv='Pragma' content='no-cache'>
> <meta http-equiv='Content-Type' content='text/html; charset=utf-8'>
> <link rel='stylesheet' type='text/css' href='fltk.css'>
> <link rel='alternate' title='FLTK RSS' type='application/rss+xml' href='index.rss'>
> <link rel='shortcut icon' href='favicon.ico' type='image/x-icon'>
> <meta name='keywords' content='gui toolkit,c++,linux,unix,macos x,x11,windows'>
> </head>
> <body>
> <table width='100%' border='0' cellspacing='0' cellpadding='0' summary='Page'>
> <tr class='header'><td valign='top' width='15' rowspan='2'><a href='index.php'><img src='images/top-left.gif' width='15' height='70' border='0' alt=''></a></td><td valign='top' width='224' rowspan='2'><a href='index.php'><img src='images/top-middle.gif' width='224' height='70' border='0' alt=''></a></td><td width='100%' height='40'><h1>Fast Light Toolkit</h1> </td><td align='right' nowrap>
> <table cellpadding=0 cellspacing=0 border=0><tr><td valign=top nowrap>
> <a href=fltk-rss.xml><img src=images/rss-fee
> ----------
> 2017/11/17 15:55:23.885 kid1| ctx: exit level  0
> 2017/11/17 15:55:23.885 kid1| 23,2| url.cc(407) urlParse: urlParse: URI has whitespace: {icap://127.0.0.1:1344/clamav ICAP/1.0
> }
> 2017/11/17 15:55:24.038 kid1| 28,2| RegexData.cc(73) match: aclRegexData::match: match '(MyAllowedUAstring)' found in 'MyAllowedUAstring'
> 2017/11/17 15:55:24.038 kid1| 88,2| client_side_reply.cc(2073) processReplyAccessResult: The reply for GET http://www.fltk.org/index.php is ALLOWED, because it matched denied_mimetypes_rep
> 2017/11/17 15:55:24.038 kid1| 11,2| client_side.cc(1409) sendStartOfMessage: HTTP Client local=66.39.46.122:80 remote=10.215.144.48 FD 8 flags=17
> 2017/11/17 15:55:24.038 kid1| 11,2| client_side.cc(1410) sendStartOfMessage: HTTP Client REPLY:
> ---------
> HTTP/1.1 200 OK
> Date: Fri, 17 Nov 2017 14:55:23 GMT
> Server: Apache/2.4.29
> Cache-Control: no-cache
> Vary: Accept-Encoding
> Content-Type: text/html
> Via: ICAP/1.0 proxy-server1.hospitalmanacor.org (C-ICAP/0.5.2 SquidClamav/Antivirus service )
> X-Cache: MISS from proxy-server1
> X-Cache-Lookup: MISS from proxy-server1:3227
> Transfer-Encoding: chunked
> Connection: keep-alive
>
> How can I modify my example 2 settings so this access control works the same way with both http and https in an ssl-bumped environment.
>
> Thanks,
>
> Vieri
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users

-- 
**************************
* C++: Bug to the future *
**************************


-------------- next part --------------
A non-text attachment was scrubbed...
Name: signature.asc
Type: application/pgp-signature
Size: 648 bytes
Desc: OpenPGP digital signature
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20171117/e436875c/attachment.sig>

From rousskov at measurement-factory.com  Fri Nov 17 16:25:20 2017
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Fri, 17 Nov 2017 09:25:20 -0700
Subject: [squid-users] block user agent
In-Reply-To: <107610513.275578.1510932471911@mail.yahoo.com>
References: <160200155.402986.1510744710954.ref@mail.yahoo.com>
 <160200155.402986.1510744710954@mail.yahoo.com>
 <61ce5b3f-bb5e-235f-351c-f3b4ae339fd5@treenet.co.nz>
 <1140434830.1059565.1510821897156@mail.yahoo.com>
 <4f2bc2b8-f307-e62c-5225-de3054e9e556@measurement-factory.com>
 <107610513.275578.1510932471911@mail.yahoo.com>
Message-ID: <8edbcf42-bc37-a63b-8be0-daf088770108@measurement-factory.com>

On 11/17/2017 08:27 AM, Vieri wrote:
> From: Alex Rousskov <rousskov at measurement-factory.com>
>> 1. Your "works" and "does not work" setups currently differ in at least
>> three variables: user agent name, slash after the user agent name, and
>> acl negation in http_access. Find out which single variable is
>> responsible for the breakage by eliminating all other differences.
>>
>> 2. Post two ALL,2 cache.logs, each containing a single transaction, one
>> for the "works" case and one for the "does not work" case polished as
>> discussed in #1.

> I can't really do anything about #1 except maybe leave out the forward slash.
> That's because my 2 examples are trying to achieve the opposite.

You may be conflating two very different goals:

  A) Understanding why Squid does X.
  B) Configuring Squid to do what you want.

My response was focused on the former. Once you understand, you can
probably accomplish the latter on your own.

To understand why two similar setups act differently, I would reduce the
number of different variables until you find the variable that explains
the difference. Yes, none of the tested reduced setups may do what you
want your production Squid to do, but that should not matter for now.
You are after understanding.

The usual alternative to the above approach is trying random
configurations until you think Squid works the way you want it to work.
Usually, Squid still does not do what you think it does, but your test
cases do not expose the difference.


> My goal is to deny all client traffic from browsers that DO NOT have
> a specific user-agent string. So this is a negated statement.

There is no need to use negation for that. If the goodAgents ACL matches
requests with "specific user-agent string", then you can do this:

  http_access allow goodAgents
  http_access deny all

As you can see, there is no ACL negation or negative ACLs.


> Common to both:
> 
> acl allowed_useragent browser MyAllowedUAstring
> acl denied_useragent browser MyDeniedUAstring
> 
> # example 1:
> http_access deny denied_useragent
> http_reply_access deny denied_useragent

If you want to block access to the origin server, do not use
http_reply_access. Use http_access.


> # example 2:
> http_access deny !allowed_useragent
> http_reply_access deny !allowed_useragent
> 
> Then I run this from the client:
> 
> # curl --insecure --user-agent MyAllowedUAstring https://www.gentoo.org
> -> I was expecting to be allowed access since Squid denies "everything that's not" MyAllowedUAstring. Well, at least I should have passed the "deny" line in example 2.
> However, I'm being blocked right there.

> ---------
> CONNECT 89.16.167.134:443 HTTP/1.1
> Host: 89.16.167.134:443
> ----------
> clientAccessCheckDone: The request CONNECT 89.16.167.134:443 is DENIED; last ACL checked: allowed_useragent


As you can see, your CONNECT request was denied (because it lacks the
User-Agent header). The rest does not matter much (for now), but Squid
bumps the connection to serve the error page in response to the first
bumped HTTP request (regardless of what that first bumped HTTP request
looks like).

Alex.


From squid3 at treenet.co.nz  Fri Nov 17 16:30:39 2017
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sat, 18 Nov 2017 05:30:39 +1300
Subject: [squid-users] block user agent
In-Reply-To: <107610513.275578.1510932471911@mail.yahoo.com>
References: <160200155.402986.1510744710954.ref@mail.yahoo.com>
 <160200155.402986.1510744710954@mail.yahoo.com>
 <61ce5b3f-bb5e-235f-351c-f3b4ae339fd5@treenet.co.nz>
 <1140434830.1059565.1510821897156@mail.yahoo.com>
 <4f2bc2b8-f307-e62c-5225-de3054e9e556@measurement-factory.com>
 <107610513.275578.1510932471911@mail.yahoo.com>
Message-ID: <90faeb83-b69a-3f8c-9eb4-b5010a57c0ee@treenet.co.nz>


On 18/11/17 04:27, Vieri wrote:
> ________________________________
> From: Alex Rousskov <rousskov at measurement-factory.com>
>> 1. Your "works" and "does not work" setups currently differ in at least
>> three variables: user agent name, slash after the user agent name, and
>> acl negation in http_access. Find out which single variable is
>> responsible for the breakage by eliminating all other differences.
>>
>> 2. Post two ALL,2 cache.logs, each containing a single transaction, one
>> for the "works" case and one for the "does not work" case polished as
>> discussed in #1.
> 
> 
> 
> I can't really do anything about #1 except maybe leave out the forward slash.
> That's because my 2 examples are trying to achieve the opposite.
> Let me just rephrase everything so it's crystal clear.
> 
> My goal is to deny all client traffic from browsers that DO NOT have a specific user-agent string. So this is a negated statement. One of the things I can't do in Squid is define an ACL with a negated lookahead such as (?!useragentname).
> 
> So I set up two examples.
> 
> Common to both:
> 
> acl allowed_useragent browser MyAllowedUAstring
> acl denied_useragent browser MyDeniedUAstring
> 
> # example 1:
> http_access deny denied_useragent
> http_reply_access deny denied_useragent
> deny_info http://proxy-server1/proxy-error/?a=%a&B=%B&e=%e&E=%E&H=%H&i=%i&M=%M&o=%o&R=%R&T=%T&U=%U&u=%u&w=%w&x=%x&acl=denied_useragent denied_useragent
> 
> I then run this from my test client:
> 
> # curl --insecure --user-agent MyAllowedUAstring https://www.gentoo.org
> -> works as expected (I see the web site). I guess you don't need to see cache.log here.
> 
> Now I run this:
> 
> # curl --insecure --user-agent MyDeniedUAstring https://www.gentoo.org
> -> works as expected (I'm denied access and I see Squid's error page).
> I guess there's no need for the full log here either. It boils down to this anyway:
> 2017/11/17 13:24:26.937 kid1| 28,2| RegexData.cc(73) match: aclRegexData::match: match '(MyDeniedUAstring)' found in 'MyDeniedUAstring'
> 2017/11/17 13:24:26.937 kid1| 85,2| client_side_request.cc(745) clientAccessCheckDone: The request GET https://www.gentoo.org/ is DENIED; last ACL checked: denied_useragent
> 
> I'm done with example 1. That's because I cannot make a consistent list of all user agents I want to actively block. Instead, I want to "deny everyone except one or two".
> 
> Also, since negative lookaheads are not supported in regular expressions, I change my example 1 to:
> 
> # example 2:
> http_access deny !allowed_useragent
> http_reply_access deny !allowed_useragent
> deny_info http://proxy-server1/proxy-error/?a=%a&B=%B&e=%e&E=%E&H=%H&i=%i&M=%M&o=%o&R=%R&T=%T&U=%U&u=%u&w=%w&x=%x&acl=denied_useragent allowed_useragent
> 
> Then I run this from the client:
> 
> # curl --insecure --user-agent MyAllowedUAstring https://www.gentoo.org
> -> I was expecting to be allowed access since Squid denies "everything that's not" MyAllowedUAstring. Well, at least I should have passed the "deny" line in example 2.
> However, I'm being blocked right there. This is the full log:
> 
> 2017/11/17 13:30:42.216 kid1| 5,2| TcpAcceptor.cc(220) doAccept: New connection on FD 88
> 2017/11/17 13:30:42.216 kid1| 5,2| TcpAcceptor.cc(295) acceptNext: connection on local=[::]:3229 remote=[::] FD 88 flags=25
> 2017/11/17 13:30:42.216 kid1| 33,2| client_side.cc(3943) httpsSslBumpAccessCheckDone: sslBump needed for local=89.16.167.134:443 remote=10.215.144.48 FD 8 flags=17 method 4
> 2017/11/17 13:30:42.216 kid1| 11,2| client_side.cc(2372) parseHttpRequest: HTTP Client local=89.16.167.134:443 remote=10.215.144.48 FD 8 flags=17
> 2017/11/17 13:30:42.216 kid1| 11,2| client_side.cc(2373) parseHttpRequest: HTTP Client REQUEST:
> ---------
> CONNECT 89.16.167.134:443 HTTP/1.1
> Host: 89.16.167.134:443
> 

This is the CONNECT request generated internally by Squid for the 
bumping process.


> 
> ----------
> 2017/11/17 13:30:42.216 kid1| 85,2| client_side_request.cc(745) clientAccessCheckDone: The request CONNECT 89.16.167.134:443 is DENIED; last ACL checked: allowed_useragent
> 2017/11/17 13:30:42.216 kid1| 20,2| store.cc(996) checkCachable: StoreEntry::checkCachable: NO: not cachable
> 2017/11/17 13:30:42.216 kid1| 20,2| store.cc(996) checkCachable: StoreEntry::checkCachable: NO: not cachable
> 2017/11/17 13:30:42.216 kid1| 20,2| store.cc(996) checkCachable: StoreEntry::checkCachable: NO: not cachable
> 2017/11/17 13:30:42.226 kid1| 83,2| client_side.cc(3843) clientNegotiateSSL: clientNegotiateSSL: New session 0x125e030 on FD 8 (10.215.144.48:65262)
> 2017/11/17 13:30:42.226 kid1| 11,2| client_side.cc(2372) parseHttpRequest: HTTP Client local=89.16.167.134:443 remote=10.215.144.48 FD 8 flags=17
> 2017/11/17 13:30:42.226 kid1| 11,2| client_side.cc(2373) parseHttpRequest: HTTP Client REQUEST:
> ---------
> GET / HTTP/1.1
> Host: www.gentoo.org
> User-Agent: MyAllowedUAstring
> Accept: */*
> 
> 
> ----------
> 2017/11/17 13:30:42.227 kid1| 28,2| RegexData.cc(73) match: aclRegexData::match: match '(MyAllowedUAstring)' found in 'MyAllowedUAstring'
> 2017/11/17 13:30:42.227 kid1| 88,2| client_side_reply.cc(2073) processReplyAccessResult: The reply for GET https://www.gentoo.org/ is ALLOWED, because it matched denied_mimetypes_rep

Please notice the above text and what ACL it is talking about.

Hint: it is NOT the one you are talking about testing.


> 2017/11/17 13:30:42.227 kid1| 11,2| client_side.cc(1409) sendStartOfMessage: HTTP Client local=89.16.167.134:443 remote=10.215.144.48 FD 8 flags=17
> 2017/11/17 13:30:42.227 kid1| 11,2| client_side.cc(1410) sendStartOfMessage: HTTP Client REPLY:
> ---------
> HTTP/1.1 307 Temporary Redirect
> Server: squid
> Mime-Version: 1.0
> Date: Fri, 17 Nov 2017 12:30:42 GMT
> Content-Type: text/html;charset=utf-8
> Content-Length: 0
> Location: http://proxy-server1/proxy-error/?a=-&B=&e=0&E=%5BNo%20Error%5D&H=89.16.167.134&i=10.215.144.48&M=CONNECT&o=&R=/&T=Fri,%2017%20Nov%202017%2012%3A30%3A42%20GMT&U=https%3A%2F%2F89.16.167.134%2F*&u=89.16.167.134%3A443&w=IT%40mydomain.org&x=&acl=denied_useragent
> X-Squid-Error: 403 Access Denied
> X-Cache: MISS from proxy-server1
> X-Cache-Lookup: NONE from proxy-server1:3227
> Connection: close

This is the denial "error" response generated by Squid.

...
> 
> How can I modify my example 2 settings so this access control works the same way with both http and https in an ssl-bumped environment.

It already does. The environment is what is different.

You are looking at Squid generated messages and trying to get them 
replaced with other Squid generated messages simply because they are 
generated by Squid not some arbitrary UA.


If you could replace that Squid generated message with another Squid 
generated message, and replace that Squid generated message with another 
Squid generated message, and replace that Squid generated message with 
another Squid generated message, and replace that Squid generated 
message with another Squid generated message, .... until the machine 
crashes or client gives up waiting and closes the connection.

Amos


From squid3 at treenet.co.nz  Fri Nov 17 16:52:23 2017
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sat, 18 Nov 2017 05:52:23 +1300
Subject: [squid-users] https://wiki.squid-cache.org provides invalid
 certificate chain ...
In-Reply-To: <5A0ED86D.3080208@mathemainzel.info>
References: <5A0ED86D.3080208@mathemainzel.info>
Message-ID: <ba4e9aba-cbc5-9b96-2f40-0e8838361c27@treenet.co.nz>

On 18/11/17 01:39, Walter H. wrote:
> for more information see
> https://www.ssllabs.com/ssltest/analyze.html?d=wiki.squid-cache.org
> 
> - missing intermediate certificate
> - ssl3 active, poodle vulnerable ...
> 

None of those issues appear in the test results I get from that URL you 
referenced. SSLv3 is definitely not even supported by our wiki server.

The tester appears to be broken in regards to the chain test. There is 
*no* chain. Our cert is directly signed by the LetsEncrypt CA.

Amos


From squid3 at treenet.co.nz  Fri Nov 17 17:01:18 2017
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sat, 18 Nov 2017 06:01:18 +1300
Subject: [squid-users] [Fwd: Re: SSL Bump for regex URL comparison]
In-Reply-To: <1510922748.3771.8.camel@gmail.com>
References: <1510922748.3771.8.camel@gmail.com>
Message-ID: <9f7f51c2-bda4-8d57-89a7-30e3063fe6bb@treenet.co.nz>

On 18/11/17 01:45, Joe Foster wrote:
> Good morning,
> 
> I have tried the attached but I still receive the same result.
> 
> I have attached a screen shot to show what happens, its like there is no
> connection.
> 

There isn't ...

> I have tried it with and without listing 3128 as a safe ssl port. I
> imagine its not needed as its generated from Squid.
> 
> HTTPS isn't connecting, HTTP is though that's no surprise, I'm only
> diverting port 443 to port 3128.

Your port 3128 is configured to only accept plaintext HTTP traffic. It 
cannot handle the TLS on port 443 traffic.

FWIW the "ssl-bump" option does not make an http_port capable of 
receiving TLS. It just makes Squid attempt to decrypt the data tunneled 
inside plain-text CONNECT requests (if any), in accordance with the 
ssl_bump rules actions.

> 
> There are no logs being generated so I cant find out more.
> 

Most currently distributed Squid versions do not log connections that 
fail with no HTTP activity happening on them. Except when debugging the 
underlying TCP I/O activity.



> I can't for the life of me see what I'm doing wrong.
> 
> Your advise if greatly received.
> 
> Thank you
> 
> Joe
>   
> 
> I have the below rule added to my firewall for the redirect:
> connection config redirect
>          option proto 'tcp'
>          option src 'lan'
>          option src_ip '!192.168.1.101'
>          option src_dport '443'
>          option dest 'lan'
>          option dest_ip '192.168.1.101'
>          option dest_port '3128'
>          option target 'DNAT'
> 

NAT can only happen on the Squid machine itself. You must *route* the 
packets without any type of DNAT prior to their arrival at the Squid device.

Amos


From gkinkie at gmail.com  Fri Nov 17 17:47:02 2017
From: gkinkie at gmail.com (Kinkie)
Date: Fri, 17 Nov 2017 17:47:02 +0000
Subject: [squid-users] https://wiki.squid-cache.org provides invalid
 certificate chain ...
In-Reply-To: <ba4e9aba-cbc5-9b96-2f40-0e8838361c27@treenet.co.nz>
References: <5A0ED86D.3080208@mathemainzel.info>
 <ba4e9aba-cbc5-9b96-2f40-0e8838361c27@treenet.co.nz>
Message-ID: <CA+Y8hcNNp2VqvqKBYXftvm10_PWzcNGsth3qkuk7C-ndxLwxLA@mail.gmail.com>

I have already acted on it but couldn?t communicate in time, sorry. Thanks
for notifying and for looking into it.


On Fri, 17 Nov 2017 at 17:52, Amos Jeffries <squid3 at treenet.co.nz> wrote:

> On 18/11/17 01:39, Walter H. wrote:
> > for more information see
> > https://www.ssllabs.com/ssltest/analyze.html?d=wiki.squid-cache.org
> >
> > - missing intermediate certificate
> > - ssl3 active, poodle vulnerable ...
> >
>
> None of those issues appear in the test results I get from that URL you
> referenced. SSLv3 is definitely not even supported by our wiki server.
>
> The tester appears to be broken in regards to the chain test. There is
> *no* chain. Our cert is directly signed by the LetsEncrypt CA.
>
> Amos
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>
-- 
@mobile
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20171117/1f9a433f/attachment.htm>

From dernikov1 at gmail.com  Fri Nov 17 18:34:09 2017
From: dernikov1 at gmail.com (Bike dernikov1)
Date: Fri, 17 Nov 2017 19:34:09 +0100
Subject: [squid-users] Is it possible to apply squid delay pools on
 users/groups from AD ?
In-Reply-To: <c494d8d0-ec48-7707-6620-b124655b77bb@treenet.co.nz>
References: <CAAPixKjCmVuYH05zUGqkpON6znrozmjW2s1XRF5Y05+pOx=aDw@mail.gmail.com>
 <7810701a-8fb1-7930-d513-498f7cd09564@treenet.co.nz>
 <CAAPixKgjJQgtYR0xC-8YUr4VtipKCkn-RTu_S+z8f1DRyWbWCA@mail.gmail.com>
 <c494d8d0-ec48-7707-6620-b124655b77bb@treenet.co.nz>
Message-ID: <CAAPixKiCHFq053TRoht6O8d=p4paBizo12UF2or3TgYZgbOBaQ@mail.gmail.com>

On Fri, Nov 17, 2017 at 3:39 AM, Amos Jeffries <squid3 at treenet.co.nz> wrote:
> On 17/11/17 03:40, Bike dernikov1 wrote:
>>
>> Thanks for info, we searched for solution  but found that is not
>> possible to combine delay polls, and forum is our last hope, so far we
>> solved almost everything :)
>> We have: Squid Object Cache: Version 3.5.23, so it could  work.
>> Can you give us example, how to use it.  Colleague searched for
>> example but couldn't find it.
>> Thanks for help.
>>
>
> An example for username would be:
>
>  auth_param ...
>  acl login proxy_auth REQUIRED
>  http_access deny !login
>
>  delay_pools 1
>  delay_class 1 ...
>  delay_parameters 1 ...
>
>  acl slow note user Fred Bob
>  delay_access 1 allow slow
>
>
> For groups, the latest Kerberos auth helpers from Marcus Moeller are sending
> the SID and group details back to Squid for this. The other helpers bundled
> by Squid are not yet sending group names back.

We have 3.04sq version (when started with debug -d option i got that
line in log)

From:
http://squid-web-proxy-cache.1019090.n4.nabble.com/Simple-ACL-help-for-Kerberos-authenticated-sessions-td4683281.html,
can be found that only Squid 4 have this option, if i understand correctly ?

> (I was hoping to have that ready for Squid-4, but have not had the time.
> Patches or github PR welcome if anyone wants to contribute).

I would help, but I am not even a p from programmer, nearly average or
under average admin, as can be seen from questions.
Thank for info. We would spent days before or if we would find info.

> Amos
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users


From dernikov1 at gmail.com  Fri Nov 17 19:03:55 2017
From: dernikov1 at gmail.com (Bike dernikov1)
Date: Fri, 17 Nov 2017 20:03:55 +0100
Subject: [squid-users] SQUID memory error after vm.swappines changed
 from 60 to 10
In-Reply-To: <ac3150fb-0659-fd69-0d1e-fba9e596abfd@treenet.co.nz>
References: <CAAPixKhsif5+ad1prsoFAZXVXcVAxpdqkw4RdvGbfK-UxhkzTQ@mail.gmail.com>
 <86188b87-0cc3-343c-7b56-7f01354a3aa8@measurement-factory.com>
 <CAAPixKiTszJDHUDK__PFgQ_+g0kR0RX4KZUNykFANKnTqzydsg@mail.gmail.com>
 <bfb6f0fc-357b-2156-7cec-e43e9718d521@measurement-factory.com>
 <CAAPixKhNB-1zkFP5X=b7f8__XEY0-9XtEGi8N0EkXTGB6kbygw@mail.gmail.com>
 <bd5c2853-4480-4808-d1d4-9d8b1a8f6b0d@measurement-factory.com>
 <CAAPixKgzxz_hCWxW+T5TBuBFqG4z6b3KFy0-9qE31Vq33MWsmw@mail.gmail.com>
 <f8132fc7-58b0-e427-d7ae-60204da73e3e@measurement-factory.com>
 <CAAPixKh2Prf7PgQvbqzEiUKgdGLW-jQvDBzdNJNoqJPuxUYwRA@mail.gmail.com>
 <cf19e140-7d8b-d216-d2c1-b8d4cdd8eb6b@measurement-factory.com>
 <CAAPixKjyVQmEuHhnq5hMdv4q6tKdVFkcUJw_H8Cj7GbZbgHJ+w@mail.gmail.com>
 <ca66b2db-a288-db6b-c77e-8205c2541224@treenet.co.nz>
 <CAAPixKhekp_0ndvhVLio8hqjgwMO=p3McWvnauPPiFqEmg0K4w@mail.gmail.com>
 <ac3150fb-0659-fd69-0d1e-fba9e596abfd@treenet.co.nz>
Message-ID: <CAAPixKjVmPO53HvCkfXVFLbueiytUpU4=-jYv_c+to++TdB0bg@mail.gmail.com>

On Fri, Nov 17, 2017 at 3:53 AM, Amos Jeffries <squid3 at treenet.co.nz> wrote:
> On 17/11/17 03:49, Bike dernikov1 wrote:
>>
>> On Thu, Nov 16, 2017 at 8:58 AM, Amos Jeffries wrote:
>>>
>>> On 16/11/17 01:32, Bike dernikov1 wrote:
>>>>
>>>>
>>>>
>>>> If i can ask under same title:
>>>> Yesterday we had error in logs: syslog, cache.log, dmesg,access.log
>>>>
>>>> segfault at 8 ip ....... sp ..... error 4 is squid
>>>> process pid exited due to signal 11 with status 0
>>>>
>>>> Squid restarted,  that was at the end of work, and i didn't  notice
>>>> change while surfing.
>>>> I noticed change in used memory, after i went trough logs, and found
>>>> segfault.
>>>>
>>>> Can you point me, how to analyze what happened.
>>>> Can that be problem with kernel ?
>>>>
>>>
>>> How to retrieve info about these type of things is detailed at
>>> <https://wiki.squid-cache.org/SquidFaq/BugReporting>.
>>
>>
>> I wasn't sure it is bug, so i didn't want to post it that is a  bug.
>> As you now confirm that it can be bug i will prepare for retriving
>> infos.
>> I just hope that bug won't  happen at high  load in middle of working day.
>>
>
> The how-to are just on that page because if you are reporting that kind of
> bug those details are mandatory. You dont have to be reporting a bug to use
> the techniques.
>
> That said, segfault is almost always a bug. Though it could be a bug in the
> system environment or hardware rather than Squid. The details you get from
> looking at the traces should indicate whether those are actual or not.

In the begining, we had many crashes, and we thought that we have hardware bug.
We had two different servers, Fujitsu RX600 and X3550M3.  We was
testing Squid  on Centos and Debian.
Debian won because of new squidguard version on  which work
authorization with ldap.
First upgrade to Debian 9 (stable) crashed installation on Fujitsu. It
couldn't boot with new kernel.
Same Debian worked on IBM X3550M3. So it was a nightmare for testing.
We returned to stable kernel, and problems disappeared until now.
Although only one segfault so far in 3 days.

>>
>>> NP: If you do not have core files enabled, then the data from that
>>> segfault
>>> is probably gone irretrievably. You may need to use the script to capture
>>> segfault details from a running proxy (the 'minimal downtime' section).
>>
>>
>> I am sure that i didn't enabled it.
>>
>
> Okay, then you will need to for further diagnosis.

>From  Monday we will start with reconfiguration. Each day new problem.
Migration slowed to stop :(

Today we had different problem (with exhausted inodes). Logs exploded,
with no space on disk errors (disk on 60% free)
Luckily, we found   what  was problem (sarg and scripted  generated
reports) under 5 minutes.
We lost half  day for rewriting scripts. I hope that we solve that
problem  for good :).

> Amos

We couldn't done it without you help.
Thanks a lot.


From Walter.H at mathemainzel.info  Sat Nov 18 12:51:05 2017
From: Walter.H at mathemainzel.info (Walter H.)
Date: Sat, 18 Nov 2017 13:51:05 +0100
Subject: [squid-users] https://wiki.squid-cache.org provides invalid
 certificate chain ...
In-Reply-To: <5A0ED86D.3080208@mathemainzel.info>
References: <5A0ED86D.3080208@mathemainzel.info>
Message-ID: <5A102CB9.9080602@mathemainzel.info>

Hello,

still certificate issues: missing intermediate certificate

Greetings,
Walter

On 17.11.2017 13:39, Walter H. wrote:
> for more information see
> https://www.ssllabs.com/ssltest/analyze.html?d=wiki.squid-cache.org
>
> - missing intermediate certificate
> - ssl3 active, poodle vulnerable ...
>
> Greetings,
> Walter


-------------- next part --------------
A non-text attachment was scrubbed...
Name: smime.p7s
Type: application/pkcs7-signature
Size: 3491 bytes
Desc: S/MIME Cryptographic Signature
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20171118/9e7ad0a2/attachment.bin>

From Walter.H at mathemainzel.info  Sat Nov 18 12:56:11 2017
From: Walter.H at mathemainzel.info (Walter H.)
Date: Sat, 18 Nov 2017 13:56:11 +0100
Subject: [squid-users] https://wiki.squid-cache.org provides invalid
 certificate chain ...
In-Reply-To: <5A102CB9.9080602@mathemainzel.info>
References: <5A0ED86D.3080208@mathemainzel.info>
 <5A102CB9.9080602@mathemainzel.info>
Message-ID: <5A102DEB.6040608@mathemainzel.info>

On 18.11.2017 13:51, Walter H. wrote:
> Hello,
>
> still certificate issues: missing intermediate certificate
>
> Greetings,
> Walter
@Amos:

>  There is
>  *no* chain. Our cert is directly signed by the LetsEncrypt CA.
>  Amos

that's wrong;  LetsEncrypt is only an intermediate, and MUST be given by the server,
as it isn't in any Trust Store by default.



-------------- next part --------------
A non-text attachment was scrubbed...
Name: smime.p7s
Type: application/pkcs7-signature
Size: 3491 bytes
Desc: S/MIME Cryptographic Signature
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20171118/a08bcd39/attachment.bin>

From acrow at integrafin.co.uk  Sat Nov 18 15:10:38 2017
From: acrow at integrafin.co.uk (Alex Crow)
Date: Sat, 18 Nov 2017 15:10:38 +0000
Subject: [squid-users] https://wiki.squid-cache.org provides invalid
 certificate chain ...
In-Reply-To: <5A102DEB.6040608@mathemainzel.info>
References: <5A0ED86D.3080208@mathemainzel.info>
 <5A102CB9.9080602@mathemainzel.info> <5A102DEB.6040608@mathemainzel.info>
Message-ID: <e3649c3e-5b45-114b-1bdc-9b270ec3303a@integrafin.co.uk>



On 18/11/17 12:56, Walter H. wrote:
> On 18.11.2017 13:51, Walter H. wrote:
>> Hello,
>>
>> still certificate issues: missing intermediate certificate
>>
>> Greetings,
>> Walter
> @Amos:
>
>> ?There is
>> ?*no* chain. Our cert is directly signed by the LetsEncrypt CA.
>> ?Amos
>
> that's wrong;? LetsEncrypt is only an intermediate, and MUST be given 
> by the server,
> as it isn't in any Trust Store by default.
>
>

Yep, I use LE and an it has a root CA and an intermediate - mine has:

DSA Root CA X3 -> Let's Encrypt Authority X3 -> <my Domain>.

Cheers

Alex


--
This message is intended only for the addressee and may contain
confidential information. Unless you are that person, you may not
disclose its contents or use it in any way and are requested to delete
the message along with any attachments and notify us immediately.
This email is not intended to, nor should it be taken to, constitute advice.
The information provided is correct to our knowledge & belief and must not
be used as a substitute for obtaining tax, regulatory, investment, legal or
any other appropriate advice.

"Transact" is operated by Integrated Financial Arrangements Ltd.
29 Clement's Lane, London EC4N 7AE. Tel: (020) 7608 4900 Fax: (020) 7608 5300.
(Registered office: as above; Registered in England and Wales under
number: 3727592). Authorised and regulated by the Financial Conduct
Authority (entered on the Financial Services Register; no. 190856).


From kevin at coretechx.com  Sat Nov 18 21:21:38 2017
From: kevin at coretechx.com (Kevin Wong)
Date: Sat, 18 Nov 2017 13:21:38 -0800
Subject: [squid-users] Squid Behavior to Ping Destination on Registered Ports
Message-ID: <CANPLj3CPUz5=eXC-fqk9p1Y_++STe9QbQqaHq6uOikuGaTR4Lg@mail.gmail.com>

My firewall (Juniper SRX) caught outbound ICMP flows using vulnerable ports
before initiating outbound HTTP traffic.  I am running an updated Squid
Proxy on Ubuntu 16.04.  Can anybody explain or confirm the Squid behavior?

Oct 15 03:53:37  firewall RT_FLOW: RT_FLOW_SESSION_DENY: session
denied 10.1.1.1/1024->91.189.91.23/42518 0x0 icmp 1(8) deny vlan1
uplink UNKNOWN UNKNOWN N/A(N/A) irb.420 UNKNOWN policy deny

Oct 15 08:06:20  firewall RT_FLOW: RT_FLOW_SESSION_DENY: session
denied 10.1.1.1/1280->91.189.91.26/42518 0x0 icmp 1(8) deny vlan1
uplink UNKNOWN UNKNOWN N/A(N/A) irb.420 UNKNOWN policy deny

Oct 15 10:46:47  firewall RT_FLOW: RT_FLOW_SESSION_DENY: session
denied 10.1.1.1/1536->91.189.91.26/42518 0x0 icmp 1(8) deny vlan1
uplink UNKNOWN UNKNOWN N/A(N/A) irb.420 UNKNOWN policy deny


For more details and flow examples, I posted on serverfault:

https://serverfault.com/questions/879394/squid-proxy-using-vulnerable-ports
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20171118/72bfe332/attachment.htm>

From Antony.Stone at squid.open.source.it  Sat Nov 18 22:06:31 2017
From: Antony.Stone at squid.open.source.it (Antony Stone)
Date: Sat, 18 Nov 2017 22:06:31 +0000
Subject: [squid-users] Squid Behavior to Ping Destination on Registered
	Ports
In-Reply-To: <CANPLj3CPUz5=eXC-fqk9p1Y_++STe9QbQqaHq6uOikuGaTR4Lg@mail.gmail.com>
References: <CANPLj3CPUz5=eXC-fqk9p1Y_++STe9QbQqaHq6uOikuGaTR4Lg@mail.gmail.com>
Message-ID: <201711182206.31894.Antony.Stone@squid.open.source.it>

On Saturday 18 November 2017 at 21:21:38, Kevin Wong wrote:

> My firewall (Juniper SRX) caught outbound ICMP flows using vulnerable ports

That makes no sense.  ICMP doesn't use port numbers.

> before initiating outbound HTTP traffic.  I am running an updated Squid
> Proxy on Ubuntu 16.04.  Can anybody explain or confirm the Squid behavior?

What ICMP traffic are you blocking and why?


Antony.

-- 
I bought a book about anti-gravity.  The reviews say you can't put it down.

                                                   Please reply to the list;
                                                         please *don't* CC me.


From kevin at coretechx.com  Sat Nov 18 22:37:20 2017
From: kevin at coretechx.com (Kevin Wong)
Date: Sat, 18 Nov 2017 14:37:20 -0800
Subject: [squid-users] Squid Behavior to Ping Destination on Registered
	Ports
In-Reply-To: <CANPLj3CPUz5=eXC-fqk9p1Y_++STe9QbQqaHq6uOikuGaTR4Lg@mail.gmail.com>
References: <CANPLj3CPUz5=eXC-fqk9p1Y_++STe9QbQqaHq6uOikuGaTR4Lg@mail.gmail.com>
Message-ID: <CANPLj3B3j8iTcxeZ7mWMdRxdnmDjiJ=Hx1ZVjBddD9w3R6bbNQ@mail.gmail.com>

> Date: Sat, 18 Nov 2017 22:06:31 +0000
> From: Antony Stone <Antony.Stone at squid.open.source.it>
> To: squid-users at lists.squid-cache.org
> Subject: Re: [squid-users] Squid Behavior to Ping Destination on
>         Registered      Ports
> Message-ID: <201711182206.31894.Antony.Stone at squid.open.source.it>
> Content-Type: Text/Plain;  charset="iso-8859-15"
>
> On Saturday 18 November 2017 at 21:21:38, Kevin Wong wrote:
>
> > My firewall (Juniper SRX) caught outbound ICMP flows using vulnerable
> ports
>
> That makes no sense.  ICMP doesn't use port numbers.
>
>
That is why I asked the list and was a follow up question if somebody
replied it is "normal traffic to find the path to the destination or
proxies in between".


> > before initiating outbound HTTP traffic.  I am running an updated Squid
> > Proxy on Ubuntu 16.04.  Can anybody explain or confirm the Squid
> behavior?
>
> What ICMP traffic are you blocking and why?
>
>
Besides some basic IDS rules, I'm not blocking ICMP traffic.  What's being
blocked are all ports that are not explicitly allowed outbound.  In this
case, ports 1024, 1280, and 1536 were blocked and 80/tcp, 53/udp are
allowed outbound.


>
> Antony.
>
> --
> I bought a book about anti-gravity.  The reviews say you can't put it down.
>
>                                                    Please reply to the
> list;
>                                                          please *don't* CC
> me.
>
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20171118/b02075a9/attachment.htm>

From Antony.Stone at squid.open.source.it  Sat Nov 18 22:46:41 2017
From: Antony.Stone at squid.open.source.it (Antony Stone)
Date: Sat, 18 Nov 2017 22:46:41 +0000
Subject: [squid-users] Squid Behavior to Ping Destination on Registered
	Ports
In-Reply-To: <CANPLj3B3j8iTcxeZ7mWMdRxdnmDjiJ=Hx1ZVjBddD9w3R6bbNQ@mail.gmail.com>
References: <CANPLj3CPUz5=eXC-fqk9p1Y_++STe9QbQqaHq6uOikuGaTR4Lg@mail.gmail.com>
 <CANPLj3B3j8iTcxeZ7mWMdRxdnmDjiJ=Hx1ZVjBddD9w3R6bbNQ@mail.gmail.com>
Message-ID: <201711182246.41562.Antony.Stone@squid.open.source.it>

On Saturday 18 November 2017 at 22:37:20, Kevin Wong wrote:

> > Date: Sat, 18 Nov 2017 22:06:31 +0000
> > From: Antony Stone <Antony.Stone at squid.open.source.it>
> > To: squid-users at lists.squid-cache.org
> > Subject: Re: [squid-users] Squid Behavior to Ping Destination on
> > 
> >         Registered      Ports
> > 
> > Message-ID: <201711182206.31894.Antony.Stone at squid.open.source.it>
> > Content-Type: Text/Plain;  charset="iso-8859-15"
> > 
> > On Saturday 18 November 2017 at 21:21:38, Kevin Wong wrote:
> > > My firewall (Juniper SRX) caught outbound ICMP flows using vulnerable
> > > ports
> > 
> > That makes no sense.  ICMP doesn't use port numbers.
> 
> That is why I asked the list and was a follow up question if somebody
> replied it is "normal traffic to find the path to the destination or
> proxies in between".

So what does your firewall mean by catching "outbound ICMP flows using 
vulnerable ports"?

What exactly is it catching and complaining about?

> > > before initiating outbound HTTP traffic.  I am running an updated Squid
> > > Proxy on Ubuntu 16.04.  Can anybody explain or confirm the Squid
> > > behavior?
> > 
> > What ICMP traffic are you blocking and why?
> 
> Besides some basic IDS rules, I'm not blocking ICMP traffic.

Well:

Oct 15 10:46:47  firewall RT_FLOW: RT_FLOW_SESSION_DENY: session
denied 10.1.1.1/1536->91.189.91.26/42518 0x0 icmp 1(8) deny vlan1
uplink UNKNOWN UNKNOWN N/A(N/A) irb.420 UNKNOWN policy deny

certainly looks like blocked ICMP traffic to me.

> What's being blocked are all ports

So, that means UDP and TCP (but not ICMP)

> that are not explicitly allowed outbound.  In this case, ports 1024, 1280,
> and 1536 were blocked and 80/tcp, 53/udp are allowed outbound.

Where are those blocked port numbers in your firewall logs?


Antony.

-- 
Pavlov is in the pub enjoying a pint.
The barman rings for last orders, and Pavlov jumps up exclaiming "Damn!  I 
forgot to feed the dog!"

                                                   Please reply to the list;
                                                         please *don't* CC me.


From squid3 at treenet.co.nz  Sun Nov 19 02:19:00 2017
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sun, 19 Nov 2017 15:19:00 +1300
Subject: [squid-users] Squid Behavior to Ping Destination on Registered
 Ports
In-Reply-To: <CANPLj3B3j8iTcxeZ7mWMdRxdnmDjiJ=Hx1ZVjBddD9w3R6bbNQ@mail.gmail.com>
References: <CANPLj3CPUz5=eXC-fqk9p1Y_++STe9QbQqaHq6uOikuGaTR4Lg@mail.gmail.com>
 <CANPLj3B3j8iTcxeZ7mWMdRxdnmDjiJ=Hx1ZVjBddD9w3R6bbNQ@mail.gmail.com>
Message-ID: <5f9c051b-132f-c411-b676-d5c1d9a25405@treenet.co.nz>

On 19/11/17 11:37, Kevin Wong wrote:
> 
> From: Antony Stone
> 
>     On Saturday 18 November 2017 at 21:21:38, Kevin Wong wrote:
> 
>      > My firewall (Juniper SRX) caught outbound ICMP flows using
>     vulnerable ports
> 
>     That makes no sense.? ICMP doesn't use port numbers.
> 
> 
> That is why I asked the list and was a follow up question if somebody 
> replied it is "normal traffic to find the path to the destination or 
> proxies in between".
> 

Squid does use ICMP echo to determine RTT to peers and servers to select 
the fastest route. But it does not use ports, even sets the port in the 
payload to 0 so DPI should not mistake it.



>      > before initiating outbound HTTP traffic.? I am running an updated
>     Squid
>      > Proxy on Ubuntu 16.04.? Can anybody explain or confirm the Squid
>     behavior?
> 
>     What ICMP traffic are you blocking and why?
> 
> 
> Besides some basic IDS rules, I'm not blocking ICMP traffic.? What's 
> being blocked are all ports that are not explicitly allowed outbound.  
> In this case, ports 1024, 1280, and 1536 were blocked and 80/tcp, 53/udp 
> are allowed outbound.
> 

Amos


From 747620227 at qq.com  Mon Nov 20 08:06:08 2017
From: 747620227 at qq.com (=?gb18030?B?R35Efkx1bmF0aWM=?=)
Date: Mon, 20 Nov 2017 16:06:08 +0800
Subject: [squid-users] squid 3.5.27 .https website show
	SEC_ERROR_UNKNOWN_ISSUER
Message-ID: <tencent_23D6055D07F89CE04AF4E688DFB3E1340C09@qq.com>

with your help. i changed my configure. and now the https problem is that SEC_ERROR_UNKNOWN_ISSUER.
i use squid 3.5.27 as a transparent proxy and a icap client .With the proxy , i access most of https websites like www.amazon.com. but failed  . So i want to know where problem is or how to deal with it.

The webpage remind like"   www.amazon.com used an invalid security certificate. The certificate is not trusted because of its self-signature. This certificate is invalid for the name www.amazon.com. Error code: SEC_ERROR_UNKNOWN_ISSUER "




Here is my configure

# Squid normally listens to port 3128
http_port 3120

http_port 3128 intercept

https_port 192.168.51.200:3129 intercept ssl-bump connection-auth=off generate-host-certificates=on dynamic_cert_mem_cache_size=4MB cert=/usr/local/squid/ssl_cert/myCA.pem key=/usr/local/squid/ssl_cert/myCA.pem 

#acl ssl_step1 at_step SslBump1
#acl ssl_step2 at_step SslBump2
#acl ssl_step3 at_step SslBump3
#ssl_bump peek ssl_step1
#ssl_bump splice all

sslcrtd_program /usr/local/squid/libexec/ssl_crtd -s /usr/local/squid/lib/ssl_db -M 4MB
sslcrtd_children 8 startup=1 idle=1


#icap
icap_enable on
icap_preview_enable on
icap_preview_size 1024
icap_send_client_ip on
adaptation_meta X-Client-Port "%>p"
icap_206_enable on
icap_persistent_connections off

icap_service service_req reqmod_precache 0 icap://192.168.51.200:1344/echo
icap_service service_res respmod_precache 1 icap://192.168.51.200:1344/echo
adaptation_access service_res allow all
adaptation_access service_req allow all
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20171120/0db5dc11/attachment.htm>

From rentorbuy at yahoo.com  Mon Nov 20 08:45:40 2017
From: rentorbuy at yahoo.com (Vieri)
Date: Mon, 20 Nov 2017 08:45:40 +0000 (UTC)
Subject: [squid-users] block user agent
In-Reply-To: <8edbcf42-bc37-a63b-8be0-daf088770108@measurement-factory.com>
References: <160200155.402986.1510744710954.ref@mail.yahoo.com>
 <160200155.402986.1510744710954@mail.yahoo.com>
 <61ce5b3f-bb5e-235f-351c-f3b4ae339fd5@treenet.co.nz>
 <1140434830.1059565.1510821897156@mail.yahoo.com>
 <4f2bc2b8-f307-e62c-5225-de3054e9e556@measurement-factory.com>
 <107610513.275578.1510932471911@mail.yahoo.com>
 <8edbcf42-bc37-a63b-8be0-daf088770108@measurement-factory.com>
Message-ID: <752783637.1436443.1511167540536@mail.yahoo.com>

________________________________
From: Alex Rousskov <rousskov at measurement-factory.com>
>
> You may be conflating two very different goals:
>
>   A) Understanding why Squid does X.
>   B) Configuring Squid to do what you want.
>
> My response was focused on the former. Once you understand, you can
> probably accomplish the latter on your own.


You are absolutely right. I'd like to uderstand how Squid *access rules work.


To put it bluntly, http_access and http_reply_access rules are processed one after another as they appear in squid.conf. It "exits" the sequence (ie. stops going through each http_*access rule) as soon as it hits a match.


The http_*access rules take on ACLs which can be AND'ed if the conditions are in one line, or OR'ed if they are on seperate lines.
eg.
http_access allow goodAgents !baddomains (AND)
#--
http_access allow goodAgents
http_access deny baddomains (OR)

>> My goal is to deny all client traffic from browsers that DO NOT have
>> a specific user-agent string. So this is a negated statement.
>
> There is no need to use negation for that. If the goodAgents ACL matches
> requests with "specific user-agent string", then you can do this:
>
>   http_access allow goodAgents
>   http_access deny all
> 
> As you can see, there is no ACL negation or negative ACLs.


I understand your example, but unfortunately, I was looking for something else. It's my mistake because I started this thread with basic, stripped-down examples without giving details on what I need to achieve. I wasn't doing ACL negation just for kicks. It's because I need to integrate it into a broader setup.

Your example "works", but Squid will match "goodAgent" in your first line, and exit without going on. I require to apply other rules afterwards. In other words, my intention was to first filter based on the UA string, and block all except eg. MyAllowedUAstring. From then on, I need to apply the rest of my rules.

>> clientAccessCheckDone: The request CONNECT 89.16.167.134:443 is DENIED; last ACL checked: allowed_useragent
> 
> As you can see, your CONNECT request was denied (because it lacks the
> User-Agent header). The rest does not matter much (for now), but Squid
> bumps the connection to serve the error page in response to the first
> bumped HTTP request (regardless of what that first bumped HTTP request
> looks like).

So... What is the security implication of allowing all CONNECT messages to port 443?

The following acl + access rules I set up actually "work" as in my previous "example 2". I simply allowed the CONNECT messages. Here's most of my squid.conf file:


acl SSL_ports port 443
acl Safe_ports port 80 # http
acl Safe_ports port 443 # https
acl CONNECT method CONNECT
http_access deny !Safe_ports

http_access deny CONNECT !SSL_ports
http_access allow localhost manager
http_access deny manager

acl explicit myportname 3128
acl intercepted myportname 3129
acl interceptedssl myportname 3130
http_port 3128
http_port 3129 tproxy
https_port 3130 tproxy ssl-bump generate-host-certificates=on dynamic_cert_mem_cache_size=16MB cert=/etc/ssl/squid/proxyserver.pem sslflags=NO_DEFAULT_CA
sslcrtd_program /usr/libexec/squid/ssl_crtd -s /var/lib/squid/ssl_db -M 16MB
sslcrtd_children 40 startup=20 idle=10
cache_dir diskd /var/cache/squid 32 16 256

external_acl_type nt_group ttl=0 children-max=50 %LOGIN /usr/libexec/squid/ext_wbinfo_group_acl -K

auth_param negotiate program /usr/libexec/squid/negotiate_kerberos_auth -s HTTP/proxy-server1.mydomain.org at mydomain.org
auth_param negotiate children 60
auth_param negotiate keep_alive on

acl localnet src 10.0.0.0/8
acl localnet src 192.168.0.0/16

acl ORG_all proxy_auth REQUIRED

external_acl_type bllookup ttl=86400 negative_ttl=86400 children-max=80 children-startup=10 children-idle=3 concurrency=8 %PROTO %DST %PORT %PATH /opt/custom/scripts/run/scripts/firewall/ext_sql_blwl_acl.pl --table=shallalist_bl --categories=adv,aggressive,alcohol,anonvpn,automobile_bikes,automobile_boats,automobile_cars,automobile_planes,chat,costtraps,dating,drugs,dynamic,finance_insurance,finance_moneylending,finance_other,finance_realestate,finance_trading,fortunetelling,forum,gamble,hacking,hobby_cooking,hobby_games-misc,hobby_games-online,hobby_gardening,hobby_pets,homestyle,imagehosting,isp,jobsearch,military,models,movies,music,podcasts,politics,porn,radiotv,recreation_humor,recreation_martialarts,recreation_restaurants,recreation_sports,recreation_travel,recreation_wellness,redirector,religion,remotecontrol,ringtones,science_astronomy,science_chemistry,sex_education,sex_lingerie,shopping,socialnet,spyware,tracker,updatesites,urlshortener,violence,warez,weapons,webphone,webradio,webtv
acl allowed_ips src "/opt/custom/proxy-settings/allowed.ips"
acl allowed_extra1_ips src "/opt/custom/proxy-settings/allowed.extra1.ips"
acl allowed_groups external nt_group "/opt/custom/proxy-settings/allowed.groups"
acl allowed_domains dstdomain "/opt/custom/proxy-settings/allowed.domains"
acl allowed_domains_filetypes dstdomain "/opt/custom/proxy-settings/allowed.domains.filetypes"
acl allowed_domains_mimetypes dstdomain "/opt/custom/proxy-settings/allowed.domains.mimetypes"
acl denied_domains dstdomain -i "/opt/custom/proxy-settings/denied.domains"
acl denied_extra1_domains dstdomain -i "/opt/custom/proxy-settings/denied.extra1.domains"
acl denied_ads url_regex "/opt/custom/proxy-settings/denied.ads"
acl denied_filetypes urlpath_regex -i "/opt/custom/proxy-settings/denied.filetypes"
acl denied_mimetypes_req req_mime_type -i "/opt/custom/proxy-settings/denied.mimetypes"
acl denied_extra1_mimetypes_req req_mime_type -i "/opt/custom/proxy-settings/denied.extra1.mimetypes"
acl denied_mimetypes_rep rep_mime_type -i "/opt/custom/proxy-settings/denied.mimetypes"
acl denied_extra1_mimetypes_rep rep_mime_type -i "/opt/custom/proxy-settings/denied.extra1.mimetypes"
acl denied_restricted1_mimetypes_req req_mime_type -i "/opt/custom/proxy-settings/denied.restricted1.mimetypes"
acl denied_restricted1_mimetypes_rep rep_mime_type -i "/opt/custom/proxy-settings/denied.restricted1.mimetypes"
acl allowed_restricted1_domains dstdomain -i "/opt/custom/proxy-settings/allowed.restricted1.domains"
acl allowed_restricted1_ips dst "/opt/custom/proxy-settings/allowed.restricted1.ips"
acl restricted_ips src "/opt/custom/proxy-settings/restricted.ips"
acl restricted_groups external nt_group "/opt/custom/proxy-settings/restricted.groups"
acl restricted_domains dstdomain "/opt/custom/proxy-settings/restricted.domains"
acl bl_lookup external bllookup
acl denied_urlshorteners dstdomain -i "/etc/squidGuard/db/HMANshallalist/urlshortener/domains"

acl allowed_useragent browser MyAllowedUAstring

http_access deny explicit !ORG_all
http_access deny explicit SSL_ports
http_access deny intercepted !localnet
http_access deny interceptedssl !localnet

http_access allow CONNECT SSL_ports
http_access deny !allowed_useragent
deny_info http://proxy-server1/proxy-error/?a=%a&B=%B&e=%e&E=%E&H=%H&i=%i&M=%M&o=%o&R=%R&T=%T&U=%U&u=%u&w=%w&x=%x&acl=denied_useragent allowed_useragent

http_access allow localnet !restricted_ips allowed_domains
http_access allow localnet !restricted_ips allowed_ips
http_reply_access allow localnet !restricted_ips allowed_ips
http_reply_access allow localnet !restricted_ips allowed_domains
http_access allow restricted_ips restricted_domains
http_access deny restricted_ips

http_access deny !allowed_ips denied_urlshorteners
deny_info http://proxy-server1/proxy-error/?a=%a&B=%B&e=%e&E=%E&H=%H&i=%i&M=%M&o=%o&R=%R&T=%T&U=%U&u=%u&w=%w&x=%x&acl=denied_urlshorteners denied_urlshorteners

http_access allow denied_restricted1_mimetypes_req allowed_restricted1_domains
http_access allow denied_restricted1_mimetypes_req allowed_restricted1_ips
http_reply_access allow denied_restricted1_mimetypes_rep allowed_restricted1_domains
http_reply_access allow denied_restricted1_mimetypes_rep allowed_restricted1_ips

http_access allow denied_extra1_mimetypes_req allowed_extra1_ips denied_extra1_domains
http_reply_access allow denied_extra1_mimetypes_rep allowed_extra1_ips denied_extra1_domains

http_access deny denied_restricted1_mimetypes_req
http_reply_access deny denied_restricted1_mimetypes_rep
deny_info http://proxy-server1/proxy-error/?a=%a&B=%B&e=%e&E=%E&H=%H&i=%i&M=%M&o=%o&R=%R&T=%T&U=%U&u=%u&w=%w&x=%x&acl=denied_mimetypes denied_restricted1_mimetypes_rep
deny_info http://proxy-server1/proxy-error/?a=%a&B=%B&e=%e&E=%E&H=%H&i=%i&M=%M&o=%o&R=%R&T=%T&U=%U&u=%u&w=%w&x=%x&acl=denied_mimetypes denied_restricted1_mimetypes_req

http_access deny denied_extra1_mimetypes_req
http_reply_access deny denied_extra1_mimetypes_rep
deny_info http://proxy-server1/proxy-error/?a=%a&B=%B&e=%e&E=%E&H=%H&i=%i&M=%M&o=%o&R=%R&T=%T&U=%U&u=%u&w=%w&x=%x&acl=denied_mimetypes denied_extra1_mimetypes_req
deny_info http://proxy-server1/proxy-error/?a=%a&B=%B&e=%e&E=%E&H=%H&i=%i&M=%M&o=%o&R=%R&T=%T&U=%U&u=%u&w=%w&x=%x&acl=denied_mimetypes denied_extra1_mimetypes_rep

http_access deny !allowed_ips denied_domains
deny_info http://proxy-server1/proxy-error/?a=%a&B=%B&e=%e&E=%E&H=%H&i=%i&M=%M&o=%o&R=%R&T=%T&U=%U&u=%u&w=%w&x=%x&acl=denied_domains denied_domains

http_access allow allowed_extra1_ips denied_extra1_domains
http_access deny denied_extra1_domains
deny_info http://proxy-server1/proxy-error/?a=%a&B=%B&e=%e&E=%E&H=%H&i=%i&M=%M&o=%o&R=%R&T=%T&U=%U&u=%u&w=%w&x=%x&acl=denied_extra1_domains denied_extra1_domains

http_access deny denied_filetypes !allowed_domains_filetypes
http_reply_access deny denied_filetypes !allowed_domains_filetypes
deny_info http://proxy-server1/proxy-error/?a=%a&B=%B&e=%e&E=%E&H=%H&i=%i&M=%M&o=%o&R=%R&T=%T&U=%U&u=%u&w=%w&x=%x&acl=denied_filetypes denied_filetypes

http_access deny denied_mimetypes_req !allowed_domains_mimetypes
http_reply_access deny denied_mimetypes_rep !allowed_domains_mimetypes
deny_info http://proxy-server1/proxy-error/?a=%a&B=%B&e=%e&E=%E&H=%H&i=%i&M=%M&o=%o&R=%R&T=%T&U=%U&u=%u&w=%w&x=%x&acl=denied_mimetypes denied_mimetypes_req
deny_info http://proxy-server1/proxy-error/?a=%a&B=%B&e=%e&E=%E&H=%H&i=%i&M=%M&o=%o&R=%R&T=%T&U=%U&u=%u&w=%w&x=%x&acl=denied_mimetypes denied_mimetypes_rep

http_access allow localnet bl_lookup
http_access allow localhost

http_access deny all

I'd greatly appreciate your input on this.

Hoping to understand Squid logic someday.

Thanks,

Vieri


From rentorbuy at yahoo.com  Mon Nov 20 09:15:00 2017
From: rentorbuy at yahoo.com (Vieri)
Date: Mon, 20 Nov 2017 09:15:00 +0000 (UTC)
Subject: [squid-users] url_rewrite_program and ACLs
In-Reply-To: <b0bd560b-081d-ec7b-261b-d92c1d479f98@treenet.co.nz>
References: <1990137371.4646637.1510150336187.ref@mail.yahoo.com>
 <1990137371.4646637.1510150336187@mail.yahoo.com>
 <921fbc5d-fce5-fd6f-b62e-8afd617994c7@treenet.co.nz>
 <131538788.5259356.1510213867661@mail.yahoo.com>
 <e89c27ac-b147-426e-e220-2c566b5e7586@treenet.co.nz>
 <2014705679.5324553.1510227544567@mail.yahoo.com>
 <b0bd560b-081d-ec7b-261b-d92c1d479f98@treenet.co.nz>
Message-ID: <1915184707.1464260.1511169300258@mail.yahoo.com>

________________________________
From: Amos Jeffries <squid3 at treenet.co.nz>
>
> I would compare your custom script to the ext_sql_session_acl.pl.in 
> script we bundle with current Squid.


I've rewritten my perl script, and have been running it for a full week now without any issues. Free RAM drops down to alarming values, but then rises back up again. In any case, "used swap" is always the same. The only thing that keeps be edgy is the fact that the open FDs keep growing (slowly but steadily). After a few days the value is around 6000, but after a week (today) it's:

Squid Object Cache: Version 3.5.27-20171101-re69e56c
Build Info:
Service Name: squid
Start Time:     Mon, 13 Nov 2017 11:06:36 GMT
Current Time:   Mon, 20 Nov 2017 08:48:00 GMT
Connection information for squid:
Number of clients accessing cache:      582
Number of HTTP requests received:       6435251
Number of ICP messages received:        0
Number of ICP messages sent:    0
Number of queued ICP replies:   0
Number of HTCP messages received:       0
Number of HTCP messages sent:   0
Request failure ratio:   0.00
Average HTTP requests per minute since start:   647.3
Average ICP messages per minute since start:    0.0
Select loop called: 246503925 times, 2.420 ms avg
Cache information for squid:
Hits as % of all requests:      5min: 4.4%, 60min: 4.3%
Hits as % of bytes sent:        5min: -0.7%, 60min: -6.0%
Memory hits as % of hit requests:       5min: 75.4%, 60min: 67.9%
Disk hits as % of hit requests: 5min: 0.0%, 60min: 0.1%
Storage Swap size:      29848 KB
Storage Swap capacity:  91.1% used,  8.9% free
Storage Mem size:       29120 KB
Storage Mem capacity:   88.9% used, 11.1% free
Mean Object Size:       13.19 KB
Requests given to unlinkd:      97921
Median Service Times (seconds)  5 min    60 min:
HTTP Requests (All):   0.18699  0.19742
Cache Misses:          0.19742  0.20843
Cache Hits:            0.00000  0.00000
Near Hits:             0.00000  0.27332
Not-Modified Replies:  0.00000  0.00000
DNS Lookups:           0.08334  0.07618
ICP Queries:           0.00000  0.00000
Resource usage for squid:
UP Time:        596484.490 seconds
CPU Time:       15823.550 seconds
CPU Usage:      2.65%
CPU Usage, 5 minute avg:        4.38%
CPU Usage, 60 minute avg:       4.86%
Maximum Resident Size: 14493888 KB
Page faults with physical i/o: 0
Memory accounted for:
Total accounted:       -862888 KB
memPoolAlloc calls: 2199430697
memPoolFree calls:  2241183896
File descriptor usage for squid:
Maximum number of file descriptors:   65536
Largest file desc currently in use:   12714
Number of file desc currently in use: 11998
Files queued for open:                   0
Available number of file descriptors: 53538
Reserved number of file descriptors:   100
Store Disk files open:                   0
Internal Data Structures:
2520 StoreEntries
2519 StoreEntries with MemObjects
2314 Hot Object Cache Items
2263 on-disk objects


mgr:filedescriptors shows a great deal of these:

Remote Address        Description
--------------------- ------------------------------
127.0.0.1:1344        127.0.0.1


# squidclient mgr:filedescriptors | grep -c "127.0.0.1:1344"
11578


Port 1344 is where the c-icap daemon listens on.
This is the relevant part in squid.conf:

icap_enable on
icap_send_client_ip on
icap_send_client_username on
icap_client_username_encode off
icap_client_username_header X-Authenticated-User
icap_preview_enable on
icap_preview_size 1024
icap_service squidclamav respmod_precache bypass=0 icap://127.0.0.1:1344/clamav
adaptation_access squidclamav allow all
icap_service_failure_limit -1


The number of connections to this port fluctuates over time (it also decreases), but overall it clearly increases day by day.
I could have an issue with either c-icap itself or one of its modules.
I'll keep an eye on it.

Thanks,

Vieri


From squid at ephemeric.online  Mon Nov 20 13:24:45 2017
From: squid at ephemeric.online (Robert Gabriel)
Date: Mon, 20 Nov 2017 15:24:45 +0200
Subject: [squid-users] SSL Accel Connection Reset
Message-ID: <20171120132445.GC26545@mail.ephemeric.online>

Hi,

I've tried to setup SSL accel AKA reverse proxy. The HTTP accel part works but no joy
for the SSL side. Maybe I've misunderstood or misconfigured something?

Thank you for the help.

[root at node03 tmp]# export https_proxy="https://192.168.122.130:443"

[root at node03 tmp]# curl -v --insecure https://data.ephemeric.local/test.tgz
* About to connect() to proxy 192.168.122.130 port 443 (#0)
*   Trying 192.168.122.130...
* Connected to 192.168.122.130 (192.168.122.130) port 443 (#0)
* Establish HTTP proxy tunnel to data.ephemeric.local:443
> CONNECT data.ephemeric.local:443 HTTP/1.1
> Host: data.ephemeric.local:443
> User-Agent: curl/7.29.0
> Proxy-Connection: Keep-Alive
> 
* Recv failure: Connection reset by peer
* Received HTTP code 0 from proxy after CONNECT
* Connection #0 to host 192.168.122.130 left intact
curl: (56) Recv failure: Connection reset by peer

I have run "squid -NX" and nothing in stdout. I have enabled debug log too.
I have tailed access and cache logs, nothing.
This is so frustrating as the connection is reset and no logs to help.

I followed this example and generated the certs etc:
https://wiki.squid-cache.org/ConfigExamples/Reverse/SslWithWildcardCertifiate

debug_options rotate=1 ALL,9
prefer_direct on
forwarded_for on

acl localnet src 192.168.122.0/24
acl localnet src fc00::/7       
acl localnet src fe80::/10     
acl SSL_ports port 443
acl Safe_ports port 80	
acl Safe_ports port 21
acl Safe_ports port 443
acl Safe_ports port 70
acl Safe_ports port 210
acl Safe_ports port 1025-65535
acl Safe_ports port 280	
acl Safe_ports port 488
acl Safe_ports port 591
acl Safe_ports port 777	
acl CONNECT method CONNECT

acl our_sites dstdomain download.fedoraproject.org centos mirror.centos.org artifacts.elastic.co data.ephemeric.local dl.google.com dl-ssl.google.com

http_access deny !Safe_ports
http_access deny CONNECT !SSL_ports
http_access allow localhost manager
http_access deny manager
http_access allow localnet
http_access allow localhost
http_access allow our_sites
http_access deny all

cache_peer 127.0.0.1 parent 80 0 no-query originserver name=myAccel

cache_peer_access myAccel allow our_sites
cache_peer_access myAccel deny all

http_port 8000
http_port 3128 accel defaultsite=data.ephemeric.local
http_port 8080 ssl-bump generate-host-certificates=on dynamic_cert_mem_cache_size=4MB cert=/etc/squid/squid.crt key=/etc/squid/squid.key

https_port 443 accel defaultsite=data.ephemeric.local cert=/etc/squid/myCA.pem

maximum_object_size 4096 MB
cache_dir ufs /media/data/var/spool/squid 10000 16 256
coredump_dir /media/data/var/spool/squid
cache_replacement_policy heap LFUDA

[root at data squid]# cat /etc/redhat-release 
CentOS Linux release 7.4.1708 (Core) 

[root at data squid]# uname -r
3.10.0-693.5.2.el7.x86_64

[root at data squid]# squid -v
Squid Cache: Version 3.5.20
Service Name: squid
configure options:  '--build=x86_64-redhat-linux-gnu' '--host=x86_64-redhat-linux-gnu' '--program-prefix=' '--prefix=/usr' '--exec-prefix=/usr' '--bindir=/usr/bin' '--sbindir=/usr/sbin' '--sysconfdir=/etc' '--datadir=/usr/share' '--includedir=/usr/include' '--libdir=/usr/lib64' '--libexecdir=/usr/libexec' '--sharedstatedir=/var/lib' '--mandir=/usr/share/man' '--infodir=/usr/share/info' '--disable-strict-error-checking' '--exec_prefix=/usr' '--libexecdir=/usr/lib64/squid' '--localstatedir=/var' '--datadir=/usr/share/squid' '--sysconfdir=/etc/squid' '--with-logdir=$(localstatedir)/log/squid' '--with-pidfile=$(localstatedir)/run/squid.pid' '--disable-dependency-tracking' '--enable-eui' '--enable-follow-x-forwarded-for' '--enable-auth' '--enable-auth-basic=DB,LDAP,MSNT-multi-domain,NCSA,NIS,PAM,POP3,RADIUS,SASL,SMB,SMB_LM,getpwnam' '--enable-auth-ntlm=smb_lm,fake' '--enable-auth-digest=file,LDAP,eDirectory' '--enable-auth-negotiate=kerberos' '--enable-external-acl-helpers=file_userip,LDAP_group,time_quota,session,unix_group,wbinfo_group' '--enable-cache-digests' '--enable-cachemgr-hostname=localhost' '--enable-delay-pools' '--enable-epoll' '--enable-ident-lookups' '--enable-linux-netfilter' '--enable-removal-policies=heap,lru' '--enable-snmp' '--enable-ssl-crtd' '--enable-storeio=aufs,diskd,rock,ufs' '--enable-wccpv2' '--enable-esi' '--enable-ecap' '--with-aio' '--with-default-user=squid' '--with-dl' '--with-openssl' '--with-pthreads' '--disable-arch-native' 'build_alias=x86_64-redhat-linux-gnu' 'host_alias=x86_64-redhat-linux-gnu' 'CFLAGS=-O2 -g -pipe -Wall -Wp,-D_FORTIFY_SOURCE=2 -fexceptions -fstack-protector-strong --param=ssp-buffer-size=4 -grecord-gcc-switches   -m64 -mtune=generic -fpie' 'LDFLAGS=-Wl,-z,relro  -pie -Wl,-z,relro -Wl,-z,now' 'CXXFLAGS=-O2 -g -pipe -Wall -Wp,-D_FORTIFY_SOURCE=2 -fexceptions -fstack-protector-strong --param=ssp-buffer-size=4 -grecord-gcc-switches   -m64 -mtune=generic -fpie' 'PKG_CONFIG_PATH=:/usr/lib64/pkgconfig:/usr/share/pkgconfig


From squid3 at treenet.co.nz  Mon Nov 20 13:51:44 2017
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Tue, 21 Nov 2017 02:51:44 +1300
Subject: [squid-users] block user agent
In-Reply-To: <752783637.1436443.1511167540536@mail.yahoo.com>
References: <160200155.402986.1510744710954.ref@mail.yahoo.com>
 <160200155.402986.1510744710954@mail.yahoo.com>
 <61ce5b3f-bb5e-235f-351c-f3b4ae339fd5@treenet.co.nz>
 <1140434830.1059565.1510821897156@mail.yahoo.com>
 <4f2bc2b8-f307-e62c-5225-de3054e9e556@measurement-factory.com>
 <107610513.275578.1510932471911@mail.yahoo.com>
 <8edbcf42-bc37-a63b-8be0-daf088770108@measurement-factory.com>
 <752783637.1436443.1511167540536@mail.yahoo.com>
Message-ID: <45c9ef41-5134-4df0-b46b-25dc0035eff1@treenet.co.nz>

On 20/11/17 21:45, Vieri wrote:
> ________________________________
> From: Alex Rousskov
>>
>> You may be conflating two very different goals:
>>
>>    A) Understanding why Squid does X.
>>    B) Configuring Squid to do what you want.
>>
>> My response was focused on the former. Once you understand, you can
>> probably accomplish the latter on your own.
> 
> 
> You are absolutely right. I'd like to uderstand how Squid *access rules work.
> 
> 
> To put it bluntly, http_access and http_reply_access rules are processed one after another as they appear in squid.conf. It "exits" the sequence (ie. stops going through each http_*access rule) as soon as it hits a match.
> 

Not quite. The lines which start with the same directive name are 
executed that way. Each directive has a different timing within the 
transaction lifetime.


  http_access allow foo
  http_reply_access deny foo
  http_access allow bar

Is the same as

  http_access allow foo
  http_access allow bar

  http_reply_access deny foo


http_access lines are checked on a client HTTP request arriving. 
http_reply_access on a server HTTP reply arriving.


> 
> The http_*access rules take on ACLs which can be AND'ed if the conditions are in one line, or OR'ed if they are on seperate lines.

That is binary, access control lines are trinary logic with short-cuts.

> eg.
> http_access allow goodAgents !baddomains (AND)
> #--
> http_access allow goodAgents
> http_access deny baddomains (OR)

You also have to take action into account.

For this:

   http_access allow goodAgents !baddomains (AND)

  If the first line matches the allow happens.
  otherwise deny happens

ie. goodAgents are only allowed to non-baddomains. All non-goodAgents 
are denied to everything.


For this:

   http_access allow goodAgents
   http_access deny baddomains (OR)

  If the first line matches the allow happens,
  If the second matches deny happens,
  otherwise allow happens.

ie. goodAgents are allowed to do anything. All non-goodAgents are denied 
only to baddomains.


> 
>>> My goal is to deny all client traffic from browsers that DO NOT have
>>> a specific user-agent string. So this is a negated statement.
>>
>> There is no need to use negation for that. If the goodAgents ACL matches
>> requests with "specific user-agent string", then you can do this:
>>
>>    http_access allow goodAgents
>>    http_access deny all
>>
>> As you can see, there is no ACL negation or negative ACLs.
> 
> 
> I understand your example, but unfortunately, I was looking for something else. It's my mistake because I started this thread with basic, stripped-down examples without giving details on what I need to achieve. I wasn't doing ACL negation just for kicks. It's because I need to integrate it into a broader setup.
> 
> Your example "works", but Squid will match "goodAgent" in your first line, and exit without going on. I require to apply other rules afterwards. In other words, my intention was to first filter based on the UA string, and block all except eg. MyAllowedUAstring. From then on, I need to apply the rest of my rules.
> 
>>> clientAccessCheckDone: The request CONNECT 89.16.167.134:443 is DENIED; last ACL checked: allowed_useragent
>>
>> As you can see, your CONNECT request was denied (because it lacks the
>> User-Agent header). The rest does not matter much (for now), but Squid
>> bumps the connection to serve the error page in response to the first
>> bumped HTTP request (regardless of what that first bumped HTTP request
>> looks like).
> 
> So... What is the security implication of allowing all CONNECT messages to port 443?
> 

Allowing them all the way through Squid is bad. But that is not what is 
needed here. ssl_bump rules get applied after the CONNECT is accepted 
*in* for proxy processing and they decide what happens to the tunneled 
data based on what is found there.
  If bumping is decided the TLS gets removed and the messages inside 
individually go through the http_access process.


...
> I'd greatly appreciate your input on this.
> 
> Hoping to understand Squid logic someday.

To speed that up enable debug_options 28,3.

I second Alex's recommendation about removing the "denied_" and 
"allowed_" bits of your ACL names. It will make what is going on a LOT 
clearer to see and understand.


Amos


From squid3 at treenet.co.nz  Mon Nov 20 14:49:11 2017
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Tue, 21 Nov 2017 03:49:11 +1300
Subject: [squid-users] squid 3.5.27 .https website show
 SEC_ERROR_UNKNOWN_ISSUER
In-Reply-To: <tencent_23D6055D07F89CE04AF4E688DFB3E1340C09@qq.com>
References: <tencent_23D6055D07F89CE04AF4E688DFB3E1340C09@qq.com>
Message-ID: <75a754db-33f7-32db-3cb7-5151d1750ded@treenet.co.nz>

On 20/11/17 21:06, G~D~Lunatic wrote:
> with your help. i changed my configure. and now the https problem is 
> that SEC_ERROR_UNKNOWN_ISSUER.
> i use squid 3.5.27 as a transparent proxy and a icap client .With the 
> proxy , i access most of https websites like www.amazon.com 
> http://www.hupu.com. but failed? . So i want to know where problem is 
> or how to deal with it.
> 

The config you presented has one major problem - you have configured 
ssl-bump option on the https_port but do not have any ssl_bump 
directives telling Squid what bumping actions are to be done.

What Squid does under that circumstance is bump the TLS using an invalid 
server certificate and deliver an error page to the client in hopes that 
either the invalid cert will throw up an error, or the error page might 
be displayed.

Amos


From squid3 at treenet.co.nz  Mon Nov 20 14:51:21 2017
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Tue, 21 Nov 2017 03:51:21 +1300
Subject: [squid-users] url_rewrite_program and ACLs
In-Reply-To: <1915184707.1464260.1511169300258@mail.yahoo.com>
References: <1990137371.4646637.1510150336187.ref@mail.yahoo.com>
 <1990137371.4646637.1510150336187@mail.yahoo.com>
 <921fbc5d-fce5-fd6f-b62e-8afd617994c7@treenet.co.nz>
 <131538788.5259356.1510213867661@mail.yahoo.com>
 <e89c27ac-b147-426e-e220-2c566b5e7586@treenet.co.nz>
 <2014705679.5324553.1510227544567@mail.yahoo.com>
 <b0bd560b-081d-ec7b-261b-d92c1d479f98@treenet.co.nz>
 <1915184707.1464260.1511169300258@mail.yahoo.com>
Message-ID: <133e0298-3080-d4fd-5a18-342b1b8b55c7@treenet.co.nz>

On 20/11/17 22:15, Vieri wrote:
> ________________________________
> From: Amos Jeffries <squid3 at treenet.co.nz>
>>
>> I would compare your custom script to the ext_sql_session_acl.pl.in
>> script we bundle with current Squid.
> 
> 
> I've rewritten my perl script, and have been running it for a full week now without any issues. Free RAM drops down to alarming values, but then rises back up again. In any case, "used swap" is always the same. The only thing that keeps be edgy is the fact that the open FDs keep growing (slowly but steadily). After a few days the value is around 6000, but after a week (today) it's:
> 


> Squid Object Cache: Version 3.5.27-20171101-re69e56c
> Build Info:
> Service Name: squid
> Start Time:     Mon, 13 Nov 2017 11:06:36 GMT
> Current Time:   Mon, 20 Nov 2017 08:48:00 GMT

> Average HTTP requests per minute since start:   647.3
...
> File descriptor usage for squid:
> Maximum number of file descriptors:   65536
> Largest file desc currently in use:   12714
> Number of file desc currently in use: 11998

So ~100 RPS, I would expect the open FDs used by ICAP to be around 
299-300. Not the 11K+ you are seeing.

If we assume that each request opens a new connection and they are not 
closed until TCP times out on the socket we do get numbers much more 
like that 11K+ you are seeing.

That implies that ICAP transactions are probably not finishing 
completely. Is the ICAP service finishing the ICAP reply messages 
properly? (eg with a 0-size chunk)

Or maybe the error that led to you deciding to configure 
"icap_service_failure_limit -1" was actually a real problem.


> 
> mgr:filedescriptors shows a great deal of these:
> 
> Remote Address        Description
> --------------------- ------------------------------
> 127.0.0.1:1344        127.0.0.1
> 
> 
> # squidclient mgr:filedescriptors | grep -c "127.0.0.1:1344"
> 11578
> 
> 
> Port 1344 is where the c-icap daemon listens on.
> This is the relevant part in squid.conf:
> 
> icap_enable on
> icap_send_client_ip on
> icap_send_client_username on
> icap_client_username_encode off
> icap_client_username_header X-Authenticated-User
> icap_preview_enable on
> icap_preview_size 1024
> icap_service squidclamav respmod_precache bypass=0 icap://127.0.0.1:1344/clamav
> adaptation_access squidclamav allow all
> icap_service_failure_limit -1
> 
> 
> The number of connections to this port fluctuates over time (it also decreases), but overall it clearly increases day by day.
> I could have an issue with either c-icap itself or one of its modules.
> I'll keep an eye on it.

I think those 11K open FDs is sufficient evidence to say something is 
wrong with the ICAP transactions. If you can get a packet trace of some 
of some ICAP connections it might give better clues to what is happening 
there.

Amos


From phackmann at gmail.com  Mon Nov 20 16:02:30 2017
From: phackmann at gmail.com (Paul Hackmann)
Date: Mon, 20 Nov 2017 10:02:30 -0600
Subject: [squid-users] different authentication for different ports
Message-ID: <CADQL5rOxV-YHTkR0Ft9hC40o1948FjPrxvY+NWwtQ758wer3FQ@mail.gmail.com>

Hi all.  I've got a fairly basic squid config set up on linux.  I have
basic authentication set up on it to the default 3128 port, and it works
just fine.  I would like to keep this configuration.  However, I would like
to set up another port that only allows a certain whitelist of websites
that doesn't require or ask for authentication.  I want to set this up for
certain apps that don't have proxy settings built into them.  I want
windows to be able to connect to some sites, but not everything and if it
can't reach the site, I don't want it to ask for credentials.  With my
current configuration, it asks for credentials for any app that is trying
to connect to a non-whitelisted website.  Is this configuration possible
and do you have an example?  Sorry if this has been answered before, I am
very green to squid yet.

Thanks,
PH
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20171120/9ee993bd/attachment.htm>

From squid3 at treenet.co.nz  Mon Nov 20 16:31:36 2017
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Tue, 21 Nov 2017 05:31:36 +1300
Subject: [squid-users] SSL Accel Connection Reset
In-Reply-To: <20171120132445.GC26545@mail.ephemeric.online>
References: <20171120132445.GC26545@mail.ephemeric.online>
Message-ID: <d9d30f5c-59b7-7462-a47b-3e310f0f8ef5@treenet.co.nz>

On 21/11/17 02:24, Robert Gabriel wrote:
> Hi,
> 
> I've tried to setup SSL accel AKA reverse proxy. The HTTP accel part works but no joy
> for the SSL side. Maybe I've misunderstood or misconfigured something?
> 
> Thank you for the help.
> 
> [root at node03 tmp]# export https_proxy="https://192.168.122.130:443"
> 
> [root at node03 tmp]# curl -v --insecure https://data.ephemeric.local/test.tgz

As you can see from the below curl is using a CONNECT tunnel, which are 
only valid to a forward-proxy.

The https_port in Squid is expecting the TCP connection to immediately 
start with TLS traffic. Not a plain-text CONNECT message.


> * About to connect() to proxy 192.168.122.130 port 443 (#0)
> *   Trying 192.168.122.130...
> * Connected to 192.168.122.130 (192.168.122.130) port 443 (#0)
> * Establish HTTP proxy tunnel to data.ephemeric.local:443
>> CONNECT data.ephemeric.local:443 HTTP/1.1
>> Host: data.ephemeric.local:443
>> User-Agent: curl/7.29.0
>> Proxy-Connection: Keep-Alive
>>


Amos



From squid3 at treenet.co.nz  Mon Nov 20 17:38:08 2017
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Tue, 21 Nov 2017 06:38:08 +1300
Subject: [squid-users] different authentication for different ports
In-Reply-To: <CADQL5rOxV-YHTkR0Ft9hC40o1948FjPrxvY+NWwtQ758wer3FQ@mail.gmail.com>
References: <CADQL5rOxV-YHTkR0Ft9hC40o1948FjPrxvY+NWwtQ758wer3FQ@mail.gmail.com>
Message-ID: <f5548575-c296-b934-24a7-63dc388f7148@treenet.co.nz>

On 21/11/17 05:02, Paul Hackmann wrote:
> Hi all.? I've got a fairly basic squid config set up on linux.? I have 
> basic authentication set up on it to the default 3128 port, and it works 
> just fine.? I would like to keep this configuration.? However, I would 
> like to set up another port that only allows a certain whitelist of 
> websites that doesn't require or ask for authentication.? I want to set 
> this up for certain apps that don't have proxy settings built into 
> them.? I want windows to be able to connect to some sites, but not 
> everything and if it can't reach the site, I don't want it to ask for 
> credentials.? With my current configuration, it asks for credentials for 
> any app that is trying to connect to a non-whitelisted website.? Is this 
> configuration possible and do you have an example?? Sorry if this has 
> been answered before, I am very green to squid yet.

Simply place the http_access rules for handling that traffic above the 
first line which requires authentication.

   http_access ... lines that dont require auth.

   acl login proxy_auth REQUIRED
   http_access deny !login

   http_access ... rules for authenticated users.


Amos


From phackmann at gmail.com  Mon Nov 20 17:56:33 2017
From: phackmann at gmail.com (Paul Hackmann)
Date: Mon, 20 Nov 2017 11:56:33 -0600
Subject: [squid-users] different authentication for different ports
In-Reply-To: <f5548575-c296-b934-24a7-63dc388f7148@treenet.co.nz>
References: <CADQL5rOxV-YHTkR0Ft9hC40o1948FjPrxvY+NWwtQ758wer3FQ@mail.gmail.com>
 <f5548575-c296-b934-24a7-63dc388f7148@treenet.co.nz>
Message-ID: <CADQL5rMn_XJUyoygitWRjNOXv0QRtHa0jcUW+-L2c2MRNmbheA@mail.gmail.com>

Amos,

If the website that is being asked for is not in the whitelist, won't it
fall through and ask for authentication?  That is how it seems to work to
me.  That's why I am thinking I need 2 different ports or something to do
what I want.

PH

On Mon, Nov 20, 2017 at 11:38 AM, Amos Jeffries <squid3 at treenet.co.nz>
wrote:

> On 21/11/17 05:02, Paul Hackmann wrote:
>
>> Hi all.  I've got a fairly basic squid config set up on linux.  I have
>> basic authentication set up on it to the default 3128 port, and it works
>> just fine.  I would like to keep this configuration.  However, I would like
>> to set up another port that only allows a certain whitelist of websites
>> that doesn't require or ask for authentication.  I want to set this up for
>> certain apps that don't have proxy settings built into them.  I want
>> windows to be able to connect to some sites, but not everything and if it
>> can't reach the site, I don't want it to ask for credentials.  With my
>> current configuration, it asks for credentials for any app that is trying
>> to connect to a non-whitelisted website.  Is this configuration possible
>> and do you have an example?  Sorry if this has been answered before, I am
>> very green to squid yet.
>>
>
> Simply place the http_access rules for handling that traffic above the
> first line which requires authentication.
>
>   http_access ... lines that dont require auth.
>
>   acl login proxy_auth REQUIRED
>   http_access deny !login
>
>   http_access ... rules for authenticated users.
>
>
> Amos
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>



-- 
Paul Hackmann
Sims TV/Haven Electronics
121 N. Vine St.
West Union, IA. 52175
563-422-5751 <(563)%20422-5751>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20171120/2082c94e/attachment.htm>

From nachofw at adinet.com.uy  Tue Nov 21 01:09:49 2017
From: nachofw at adinet.com.uy (Ignacio Freyre)
Date: Mon, 20 Nov 2017 22:09:49 -0300 (UYT)
Subject: [squid-users] parent peer timeout
In-Reply-To: <1563892658.35447760.1511226076962.JavaMail.zimbra@vera.com.uy>
References: <1563892658.35447760.1511226076962.JavaMail.zimbra@vera.com.uy>
Message-ID: <1893873433.35455499.1511226589061.JavaMail.zimbra@vera.com.uy>

Hi guys, i have a simple configuration that i'm testing with 2 parent proxys for a specific domain, if parent proxy 192.168.1.1 fails, failover to 192.168.1.2 proxy.
I have a couple of questions:
1)Having configured "connect-timeout=3" and "connect-fail-limit=2", failover takes about 2 minutes, how can I reduce failover time?
2)If I enable cache_peer_access statements, failover never happens because the peers dont get detected as dead

#CONFIGURATION START
#hostname
visible_hostname testing

#parent proxy's
cache_peer 192.168.1.1 parent 3128 0 no-query no-digest connect-timeout=3 connect-fail-limit=2
cache_peer 192.168.1.2 parent 3128 0 no-query no-digest connect-timeout=3 connect-fail-limit=2

#send traffic to peers
acl foo_url url_regex site\.domain\.com
never_direct allow foo_url

#peer access
cache_peer_access 192.168.1.1 deny !foo_url
cache_peer_access 192.168.1.2 deny !foo_url

#allow all for testing purposes
http_access allow all

# Squid normally listens to port 3128
http_port 3128

# Leave coredumps in the first cache dir
coredump_dir /var/spool/squid

# Add any of your own refresh_pattern entries above these.
refresh_pattern ^ftp:           1440    20%     10080
refresh_pattern ^gopher:        1440    0%      1440
refresh_pattern -i (/cgi-bin/|\?) 0     0%      0
refresh_pattern .               0       20%     4320
#CONFIGURATION END

LOGS that I see when peer is detected as dead
2017/11/20 22:55:02| Ready to serve requests.
2017/11/20 22:55:03| storeLateRelease: released 0 objects
2017/11/20 22:56:55| TCP connection to 192.168.1.1/3128 failed
2017/11/20 22:56:55| TCP connection to 192.168.1.1/3128 failed
2017/11/20 22:56:55| Detected DEAD Parent: 192.168.1.1



From jun357572957zhao at hotmail.com  Tue Nov 21 01:31:19 2017
From: jun357572957zhao at hotmail.com (=?gb2312?B?1dQgv6E=?=)
Date: Tue, 21 Nov 2017 01:31:19 +0000
Subject: [squid-users] How to configure https_port ssl-bump ?
Message-ID: <DM5PR2201MB1419A6E3BB847336E5A9EB8398230@DM5PR2201MB1419.namprd22.prod.outlook.com>

Hello?I configured  https_port in squid.conf . However  when I access https websites using Firefox?it shows   "Your connection is not secure" .

The error like this:
www.amazon.com<http://www.amazon.com> used an invalid security certificate.
This certificate is valid for 23.13.186.212 only.
Error code: SSL_ERROR_BAD_CERT_DOMAIN

My CA produced by openssl is a Self-signed certificate.

Here is my squid.conf:

https_port 192.168.51.200:3129 intercept ssl-bump connection-auth=off generate-host-certificates=on dynamic_cert_mem_cache_size=4MB cert=/usr/local/squid/ssl_cert/myCA.pem key=/usr/local/squid/ssl_cert/myCA.pem

acl ssl_step1 at_step SslBump1
acl ssl_step2 at_step SslBump2
acl ssl_step3 at_step SslBump3

ssl_bump bump all

sslcrtd_program /usr/local/squid/libexec/ssl_crtd -s /usr/local/squid/lib/ssl_db -M 4MB
sslcrtd_children 8 startup=1 idle=1

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20171121/be17f28b/attachment.htm>

From squid3 at treenet.co.nz  Tue Nov 21 03:13:30 2017
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Tue, 21 Nov 2017 16:13:30 +1300
Subject: [squid-users] different authentication for different ports
In-Reply-To: <CADQL5rMn_XJUyoygitWRjNOXv0QRtHa0jcUW+-L2c2MRNmbheA@mail.gmail.com>
References: <CADQL5rOxV-YHTkR0Ft9hC40o1948FjPrxvY+NWwtQ758wer3FQ@mail.gmail.com>
 <f5548575-c296-b934-24a7-63dc388f7148@treenet.co.nz>
 <CADQL5rMn_XJUyoygitWRjNOXv0QRtHa0jcUW+-L2c2MRNmbheA@mail.gmail.com>
Message-ID: <c165ff4c-35a2-b6c5-3854-c30ff90a5df1@treenet.co.nz>

On 21/11/17 06:56, Paul Hackmann wrote:
> Amos,
> 
> If the website that is being asked for is not in the whitelist, won't it 
> fall through and ask for authentication?? That is how it seems to work 
> to me.? That's why I am thinking I need 2 different ports or something 
> to do what I want.

You do need two different ports regardless of the http_access rules. One 
for the forward/explicit proxy traffic and one for the intercept/tproxy 
traffic. The TCP IP:port details for each of those "modes" is given in 
completely different ways and the HTTP message syntax is also different 
so the *cannot* be delivered to the same ports.


A whitelist generally is formed from two lines, one allowing and one 
denying everything else.

If 'everything else' is defined as just the stuff arriving in one 
specific port you get this:

  http_port 3128
  http_port 3129 intercept

  acl portX myportname 3129

  http_access allow portX whitelist
  http_access deny portX

  http_access deny !login
  ...

Amos


> 
> PH
> 
> On Mon, Nov 20, 2017 at 11:38 AM, Amos Jeffries <squid3 at treenet.co.nz 
> <mailto:squid3 at treenet.co.nz>> wrote:
> 
>     On 21/11/17 05:02, Paul Hackmann wrote:
> 
>         Hi all.? I've got a fairly basic squid config set up on linux. 
>         I have basic authentication set up on it to the default 3128
>         port, and it works just fine.? I would like to keep this
>         configuration.? However, I would like to set up another port
>         that only allows a certain whitelist of websites that doesn't
>         require or ask for authentication.? I want to set this up for
>         certain apps that don't have proxy settings built into them.? I
>         want windows to be able to connect to some sites, but not
>         everything and if it can't reach the site, I don't want it to
>         ask for credentials.? With my current configuration, it asks for
>         credentials for any app that is trying to connect to a
>         non-whitelisted website.? Is this configuration possible and do
>         you have an example?? Sorry if this has been answered before, I
>         am very green to squid yet.
> 
> 
>     Simply place the http_access rules for handling that traffic above
>     the first line which requires authentication.
> 
>      ? http_access ... lines that dont require auth.
> 
>      ? acl login proxy_auth REQUIRED
>      ? http_access deny !login
> 
>      ? http_access ... rules for authenticated users.
> 
> 
>     Amos
>     _______________________________________________
>     squid-users mailing list
>     squid-users at lists.squid-cache.org
>     <mailto:squid-users at lists.squid-cache.org>
>     http://lists.squid-cache.org/listinfo/squid-users
>     <http://lists.squid-cache.org/listinfo/squid-users>
> 
> 
> 
> 
> -- 
> Paul Hackmann
> Sims TV/Haven Electronics
> 121 N. Vine St.
> West Union, IA. 52175
> 563-422-5751 <tel:(563)%20422-5751>
> 
> 
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
> 


From squid3 at treenet.co.nz  Tue Nov 21 03:25:26 2017
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Tue, 21 Nov 2017 16:25:26 +1300
Subject: [squid-users] How to configure https_port ssl-bump ?
In-Reply-To: <DM5PR2201MB1419A6E3BB847336E5A9EB8398230@DM5PR2201MB1419.namprd22.prod.outlook.com>
References: <DM5PR2201MB1419A6E3BB847336E5A9EB8398230@DM5PR2201MB1419.namprd22.prod.outlook.com>
Message-ID: <68f0504b-d661-3ff8-13b8-bbd89f3c3d20@treenet.co.nz>

On 21/11/17 14:31, ? ? wrote:
> Hello?I configured? https_port in squid.conf . However? when I access 
> https websites?using Firefox?it shows? ?"Your connection is not secure" .
> 
> The?error?like?this:
> www.amazon.com <http://www.amazon.com> used an invalid security 
> certificate.
> This certificate is valid for 23.13.186.212 only.
> Error code: SSL_ERROR_BAD_CERT_DOMAIN
> 
> My CA produced by openssl is a Self-signed certificate.
> 

There are two poitns of brokenness here.

Firstly, has your Firefox been configured with the DER certificate of 
that CA so that it trusts your CA?

Without that Firefox will not trust *any* certs generated by your Squid.


> Here is my squid.conf:
> 
> https_port 192.168.51.200:3129 intercept ssl-bump connection-auth=off 
> generate-host-certificates=on dynamic_cert_mem_cache_size=4MB 
> cert=/usr/local/squid/ssl_cert/myCA.pem 
> key=/usr/local/squid/ssl_cert/myCA.pem
> 
> acl ssl_step1 at_step SslBump1
> acl ssl_step2 at_step SslBump2
> acl ssl_step3 at_step SslBump3
> 
> ssl_bump bump all
> 

Secondly, the above configures Squid to auto-generate the server 
certificate with zero details from the real server AND zero details from 
the client TLS handshake.

All Squid has to work with at that point is the raw-IP which the client 
used to connect the TCP connection underneath everything. So of course 
that is what the cert will be bound to.


Have you read this page to learn the bare basics about what is going on?
<https://wiki.squid-cache.org/Features/SslPeekAndSplice>
I know the page looks long, but that is the minimal details you need to 
know about TLS and the Squid SSL-Bump features.


Amos


From squid3 at treenet.co.nz  Tue Nov 21 04:09:17 2017
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Tue, 21 Nov 2017 17:09:17 +1300
Subject: [squid-users] parent peer timeout
In-Reply-To: <1893873433.35455499.1511226589061.JavaMail.zimbra@vera.com.uy>
References: <1563892658.35447760.1511226076962.JavaMail.zimbra@vera.com.uy>
 <1893873433.35455499.1511226589061.JavaMail.zimbra@vera.com.uy>
Message-ID: <4e5788da-03da-0835-d52c-a204c139871c@treenet.co.nz>

On 21/11/17 14:09, Ignacio Freyre wrote:
> Hi guys, i have a simple configuration that i'm testing with 2 parent proxys for a specific domain, if parent proxy 192.168.1.1 fails, failover to 192.168.1.2 proxy.
> I have a couple of questions:
> 1)Having configured "connect-timeout=3" and "connect-fail-limit=2", failover takes about 2 minutes, how can I reduce failover time?

Are you actually terminating the peer, or just simulating it some other way?

The behaviour you are seeing is what will happen for the particular 
error you cause to happen. I suspect you are only simulating a firewall 
rule table overload (ie firewall suddenly stops allowing *new* 
connections) instead of actual peer machine disconnect or shutdown.

The connect-timeout=3 is to make *new* TCP connections signal failure if 
the SYN+ACK takes more than 3 seconds to return. Otherwise it is a 
successful connect.

Added to that Squid is HTTP/1.1 software these days. Which means it uses 
multiplexing and pipeline features to reduce new TCP connections being 
needed at all. So that type of network failure may have zero effect on 
the proxy<->peer communications. Exactly as intended by the HTTP/1.1 design.


> 2)If I enable cache_peer_access statements, failover never happens because the peers dont get detected as dead


You disabled the features used as primary methods of detecting dead 
peers (no-query no-digest).

Additionally restricting traffic with cache_peer_access removes 
additional hints from HTTP and TCP traffic.


It is hard to say how those two things are impacting your proxies peer 
selection logic, since it is also complicated by the things mentioned 
above about #1.


> 
> #CONFIGURATION START
> #hostname
> visible_hostname testing
> 
> #parent proxy's
> cache_peer 192.168.1.1 parent 3128 0 no-query no-digest connect-timeout=3 connect-fail-limit=2
> cache_peer 192.168.1.2 parent 3128 0 no-query no-digest connect-timeout=3 connect-fail-limit=2
> 
> #send traffic to peers
> acl foo_url url_regex site\.domain\.com
> never_direct allow foo_url

regex is the second slowest ACL type around, generally to match domain 
use dstdomain ACL type.


> 
> #peer access
> cache_peer_access 192.168.1.1 deny !foo_url
> cache_peer_access 192.168.1.2 deny !foo_url
> 
> #allow all for testing purposes
> http_access allow all
> 

Not a good idea even for testing purposes. If there is a problem with 
your intended http_access rules that needs solving before anything else 
can be properly investigated since what is allowed to be handled by the 
proxy impacts on what can happen for outbound attempts.


> # Squid normally listens to port 3128
> http_port 3128
> 
> # Leave coredumps in the first cache dir
> coredump_dir /var/spool/squid
> 
> # Add any of your own refresh_pattern entries above these.
> refresh_pattern ^ftp:           1440    20%     10080
> refresh_pattern ^gopher:        1440    0%      1440
> refresh_pattern -i (/cgi-bin/|\?) 0     0%      0
> refresh_pattern .               0       20%     4320
> #CONFIGURATION END
> 
> LOGS that I see when peer is detected as dead
> 2017/11/20 22:55:02| Ready to serve requests.
> 2017/11/20 22:55:03| storeLateRelease: released 0 objects
> 2017/11/20 22:56:55| TCP connection to 192.168.1.1/3128 failed
> 2017/11/20 22:56:55| TCP connection to 192.168.1.1/3128 failed
> 2017/11/20 22:56:55| Detected DEAD Parent: 192.168.1.1
> 

Configure "debug_options 28,3" to see the peer selection results.


Amos


From jonathanthomascho22 at gmail.com  Tue Nov 21 05:53:58 2017
From: jonathanthomascho22 at gmail.com (Jonathan thomas Cho)
Date: Tue, 21 Nov 2017 00:53:58 -0500
Subject: [squid-users] open 256 port
Message-ID: <5a13bf78.974f370a.7615c.2823@mx.google.com>

Do I need to use command ?./configure CXXFLAGS="-DMAXTCPLISTENPORTS=256"? to open more than 128 ports still on 3.5 squid? Also where do I enter this command to open more ports? Thank you.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20171121/4ea36077/attachment.htm>

From squid3 at treenet.co.nz  Tue Nov 21 06:20:57 2017
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Tue, 21 Nov 2017 19:20:57 +1300
Subject: [squid-users] open 256 port
In-Reply-To: <5a13bf78.974f370a.7615c.2823@mx.google.com>
References: <5a13bf78.974f370a.7615c.2823@mx.google.com>
Message-ID: <9a450fd5-e281-063f-6304-690c1f32d22c@treenet.co.nz>

On 21/11/17 18:53, Jonathan thomas Cho wrote:
> Do I need to use command ?./configure 
> CXXFLAGS="-DMAXTCPLISTENPORTS=256"? to open more than 128 ports still on 
> 3.5 squid? Also where do I enter this command to open more ports? Thank you.
> 

It is a compile option required to use more than 128 *_port lines in 
squid.conf, yes. It has nothing to do with the number of TCP ports 
opened by those *_port lines though.

See the details on how to compile for your OS 
<https://wiki.squid-cache.org/SquidFaq/BinaryPackages>

Amos


From rentorbuy at yahoo.com  Tue Nov 21 10:06:38 2017
From: rentorbuy at yahoo.com (Vieri)
Date: Tue, 21 Nov 2017 10:06:38 +0000 (UTC)
Subject: [squid-users] block user agent
In-Reply-To: <45c9ef41-5134-4df0-b46b-25dc0035eff1@treenet.co.nz>
References: <160200155.402986.1510744710954.ref@mail.yahoo.com>
 <160200155.402986.1510744710954@mail.yahoo.com>
 <61ce5b3f-bb5e-235f-351c-f3b4ae339fd5@treenet.co.nz>
 <1140434830.1059565.1510821897156@mail.yahoo.com>
 <4f2bc2b8-f307-e62c-5225-de3054e9e556@measurement-factory.com>
 <107610513.275578.1510932471911@mail.yahoo.com>
 <8edbcf42-bc37-a63b-8be0-daf088770108@measurement-factory.com>
 <752783637.1436443.1511167540536@mail.yahoo.com>
 <45c9ef41-5134-4df0-b46b-25dc0035eff1@treenet.co.nz>
Message-ID: <85837667.184253.1511258798085@mail.yahoo.com>


________________________________
From: Amos Jeffries <squid3 at treenet.co.nz>
>
>   http_access allow goodAgents !baddomains (AND)
>
>  If the first line matches the allow happens.
>  otherwise deny happens
>
> ie. goodAgents are only allowed to non-baddomains. All non-goodAgents 
> are denied to everything.


>From this I deduce that in my case I cannot use "http_access allow goodAgents", but I need to go for "http_access deny !goodAgents" so I can continue on evaluating the rest of my http_access rules.
> Allowing them all the way through Squid is bad. But that is not what is 
> needed here. ssl_bump rules get applied after the CONNECT is accepted 
> *in* for proxy processing and they decide what happens to the tunneled 
> data based on what is found there.
>   If bumping is decided the TLS gets removed and the messages inside 
> individually go through the http_access process.


You lost me there. Here's what I did today.

I took your advice (and Alex's), and renamed my ACL labels. Unfortunately, I'm still a little confused :-(.

Here's part of the new Squid config (I took away the "allow all connect messages"):

# grep -v ^# /etc/squid/squid.test.include.rules | grep -v ^$
external_acl_type nt_group ttl=0 children-max=50 %LOGIN /usr/libexec/squid/ext_wbinfo_group_acl -K
auth_param negotiate program /usr/libexec/squid/negotiate_kerberos_auth -s HTTP/proxy-server1.mydomain.org at mydomain.org
auth_param negotiate children 60
auth_param negotiate keep_alive on
acl localnet src 10.0.0.0/8
acl localnet src 192.168.0.0/16
acl ORG_all proxy_auth REQUIRED
external_acl_type bllookup ttl=86400 negative_ttl=86400 children-max=80 children-startup=10 children-idle=3 concurrency=8 %PROTO %DST %PORT %PATH /opt/custom/scripts/run/scripts/firewall/ext_sql_blwl_acl.pl --table=shallalist_bl --categories=adv,aggressive,alcohol,anonvpn,automobile_bikes,automobile_boats,automobile_cars,automobile_planes,chat,costtraps,dating,drugs,dynamic,finance_insurance,finance_moneylending,finance_other,finance_realestate,finance_trading,fortunetelling,forum,gamble,hacking,hobby_cooking,hobby_games-misc,hobby_games-online,hobby_gardening,hobby_pets,homestyle,imagehosting,isp,jobsearch,military,models,movies,music,podcasts,politics,porn,radiotv,recreation_humor,recreation_martialarts,recreation_restaurants,recreation_sports,recreation_travel,recreation_wellness,redirector,religion,remotecontrol,ringtones,science_astronomy,science_chemistry,sex_education,sex_lingerie,shopping,socialnet,spyware,tracker,updatesites,urlshortener,violence,warez,weapons,webphone,webradio,webtv
acl privileged_src_ips src "/opt/custom/proxy-settings/allowed.ips"
acl privileged_extra1_src_ips src "/opt/custom/proxy-settings/allowed.extra1.ips"
acl privileged_user_groups external nt_group "/opt/custom/proxy-settings/allowed.groups"
acl good_dst_domains dstdomain "/opt/custom/proxy-settings/allowed.domains"
acl good_dst_domains_with_any_filetype dstdomain "/opt/custom/proxy-settings/allowed.domains.filetypes"
acl good_dst_domains_with_any_mimetype dstdomain "/opt/custom/proxy-settings/allowed.domains.mimetypes"
acl bad_dst_domains dstdomain -i "/opt/custom/proxy-settings/denied.domains"
acl limited_dst_domains_1 dstdomain -i "/opt/custom/proxy-settings/denied.extra1.domains"
acl bad_ads url_regex "/opt/custom/proxy-settings/denied.ads"
acl bad_filetypes urlpath_regex -i "/opt/custom/proxy-settings/denied.filetypes"
acl bad_requested_mimetypes req_mime_type -i "/opt/custom/proxy-settings/denied.mimetypes"
acl limited_requested_mimetypes_1 req_mime_type -i "/opt/custom/proxy-settings/denied.extra1.mimetypes"
acl bad_replied_mimetypes rep_mime_type -i "/opt/custom/proxy-settings/denied.mimetypes"
acl limited_replied_mimetypes_1 rep_mime_type -i "/opt/custom/proxy-settings/denied.extra1.mimetypes"
acl restricted_requested_mimetypes_1 req_mime_type -i "/opt/custom/proxy-settings/denied.restricted1.mimetypes"
acl restricted_replied_mimetypes_1 rep_mime_type -i "/opt/custom/proxy-settings/denied.restricted1.mimetypes"
acl restricted_good_dst_domains_1 dstdomain -i "/opt/custom/proxy-settings/allowed.restricted1.domains"
acl restricted_src_ips_1 dst "/opt/custom/proxy-settings/allowed.restricted1.ips"
acl explicit_only_src_ips src "/opt/custom/proxy-settings/restricted.ips"
acl explicit_only_user_groups external nt_group "/opt/custom/proxy-settings/restricted.groups"
acl explicit_only_dst_domains dstdomain "/opt/custom/proxy-settings/restricted.domains"
acl bl_lookup external bllookup
acl bad_urlshorteners dstdomain -i "/etc/squidGuard/db/HMANshallalist/urlshortener/domains"
acl redirected_domain_1 dstdomain .somedomain.com
acl good_useragents browser Firefox/
acl good_useragents browser Edge/
acl src_ips_with_any_useragent src "/opt/custom/proxy-settings/allowed.useragents.ips"
http_access deny explicit !ORG_all
http_access deny explicit SSL_ports
http_access deny intercepted !localnet
http_access deny interceptedssl !localnet
http_access deny !good_useragents !src_ips_with_any_useragent
deny_info http://proxy-server1/proxy-error/?a=%a&B=%B&e=%e&E=%E&H=%H&i=%i&M=%M&o=%o&R=%R&T=%T&U=%U&u=%u&w=%w&x=%x&acl=bad_useragents good_useragents
deny_info http://proxy-server1/proxy-error/?a=%a&B=%B&e=%e&E=%E&H=%H&i=%i&M=%M&o=%o&R=%R&T=%T&U=%U&u=%u&w=%w&x=%x&acl=bad_useragents src_ips_with_any_useragent
http_access allow localnet !explicit_only_src_ips good_dst_domains
http_access allow localnet !explicit_only_src_ips privileged_src_ips
http_reply_access allow localnet !explicit_only_src_ips privileged_src_ips
http_reply_access allow localnet !explicit_only_src_ips good_dst_domains
http_access allow explicit_only_src_ips explicit_only_dst_domains
http_access deny explicit_only_src_ips
http_access deny redirected_domain_1
deny_info 302:http://www.google.es redirected_domain_1
http_access deny !privileged_src_ips bad_urlshorteners
deny_info http://proxy-server1/proxy-error/?a=%a&B=%B&e=%e&E=%E&H=%H&i=%i&M=%M&o=%o&R=%R&T=%T&U=%U&u=%u&w=%w&x=%x&acl=bad_urlshorteners bad_urlshorteners
http_access allow restricted_requested_mimetypes_1 restricted_good_dst_domains_1
http_access allow restricted_requested_mimetypes_1 restricted_src_ips_1
http_reply_access allow restricted_replied_mimetypes_1 restricted_good_dst_domains_1
http_reply_access allow restricted_replied_mimetypes_1 restricted_src_ips_1
http_access allow limited_requested_mimetypes_1 privileged_extra1_src_ips limited_dst_domains_1
http_reply_access allow limited_replied_mimetypes_1 privileged_extra1_src_ips limited_dst_domains_1
http_access deny restricted_requested_mimetypes_1
http_reply_access deny restricted_replied_mimetypes_1
deny_info http://proxy-server1/proxy-error/?a=%a&B=%B&e=%e&E=%E&H=%H&i=%i&M=%M&o=%o&R=%R&T=%T&U=%U&u=%u&w=%w&x=%x&acl=bad_mimetypes restricted_replied_mimetypes_1
deny_info http://proxy-server1/proxy-error/?a=%a&B=%B&e=%e&E=%E&H=%H&i=%i&M=%M&o=%o&R=%R&T=%T&U=%U&u=%u&w=%w&x=%x&acl=bad_mimetypes restricted_requested_mimetypes_1
http_access deny limited_requested_mimetypes_1
http_reply_access deny limited_replied_mimetypes_1
deny_info http://proxy-server1/proxy-error/?a=%a&B=%B&e=%e&E=%E&H=%H&i=%i&M=%M&o=%o&R=%R&T=%T&U=%U&u=%u&w=%w&x=%x&acl=bad_mimetypes limited_requested_mimetypes_1
deny_info http://proxy-server1/proxy-error/?a=%a&B=%B&e=%e&E=%E&H=%H&i=%i&M=%M&o=%o&R=%R&T=%T&U=%U&u=%u&w=%w&x=%x&acl=bad_mimetypes limited_replied_mimetypes_1
http_access deny !privileged_src_ips bad_dst_domains
deny_info http://proxy-server1/proxy-error/?a=%a&B=%B&e=%e&E=%E&H=%H&i=%i&M=%M&o=%o&R=%R&T=%T&U=%U&u=%u&w=%w&x=%x&acl=bad_dst_domains bad_dst_domains
http_access allow privileged_extra1_src_ips limited_dst_domains_1
http_access deny limited_dst_domains_1
deny_info http://proxy-server1/proxy-error/?a=%a&B=%B&e=%e&E=%E&H=%H&i=%i&M=%M&o=%o&R=%R&T=%T&U=%U&u=%u&w=%w&x=%x&acl=limited_dst_domains_1 limited_dst_domains_1
http_access deny bad_filetypes !good_dst_domains_with_any_filetype
http_reply_access deny bad_filetypes !good_dst_domains_with_any_filetype
deny_info http://proxy-server1/proxy-error/?a=%a&B=%B&e=%e&E=%E&H=%H&i=%i&M=%M&o=%o&R=%R&T=%T&U=%U&u=%u&w=%w&x=%x&acl=bad_filetypes bad_filetypes
http_access deny bad_requested_mimetypes !good_dst_domains_with_any_mimetype
http_reply_access deny bad_replied_mimetypes !good_dst_domains_with_any_mimetype
deny_info http://proxy-server1/proxy-error/?a=%a&B=%B&e=%e&E=%E&H=%H&i=%i&M=%M&o=%o&R=%R&T=%T&U=%U&u=%u&w=%w&x=%x&acl=bad_mimetypes bad_requested_mimetypes
deny_info http://proxy-server1/proxy-error/?a=%a&B=%B&e=%e&E=%E&H=%H&i=%i&M=%M&o=%o&R=%R&T=%T&U=%U&u=%u&w=%w&x=%x&acl=bad_mimetypes bad_replied_mimetypes
http_access allow localnet bl_lookup
#debug_options rotate=1 28,3
debug_options rotate=1 ALL,2
append_domain .mydomain.org
reply_header_access Alternate-Protocol deny all
ssl_bump stare all
ssl_bump bump all
icap_enable on
icap_send_client_ip on
icap_send_client_username on
icap_client_username_encode off
icap_client_username_header X-Authenticated-User
icap_preview_enable on
icap_preview_size 1024
icap_service squidclamav respmod_precache bypass=0 icap://127.0.0.1:1344/clamav
adaptation_access squidclamav allow all
include /etc/squid/squid.include.common
include /etc/squid/squid.include.hide
cache_mem 32 MB
max_filedescriptors 65536
icap_service_failure_limit -1

Here's what I do from a client:

curl --insecure --user-agent Firefox/57 https://www.gentoo.org/

Here's what I get with 28,3 debug options:

2017/11/21 10:02:24.278 kid1| 28,3| Checklist.cc(70) preCheck: 0xeb47c8 checking slow rules
2017/11/21 10:02:24.278 kid1| 28,3| Ip.cc(539) match: aclIpMatchIp: '10.215.144.48' found
2017/11/21 10:02:24.278 kid1| 28,3| Acl.cc(158) matches: checked: all = 1
2017/11/21 10:02:24.278 kid1| 28,3| Acl.cc(158) matches: checked: (ssl_bump rule) = 1
2017/11/21 10:02:24.278 kid1| 28,3| Acl.cc(158) matches: checked: (ssl_bump rules) = 1
2017/11/21 10:02:24.278 kid1| 28,3| Checklist.cc(63) markFinished: 0xeb47c8 answer ALLOWED for match
2017/11/21 10:02:24.278 kid1| 28,3| Checklist.cc(163) checkCallback: ACLChecklist::checkCallback: 0xeb47c8 answer=ALLOWED
2017/11/21 10:02:24.278 kid1| 28,3| Checklist.cc(70) preCheck: 0x13450c8 checking slow rules
2017/11/21 10:02:24.278 kid1| 28,3| Acl.cc(158) matches: checked: Safe_ports = 1
2017/11/21 10:02:24.278 kid1| 28,3| Acl.cc(158) matches: checked: !Safe_ports = 0
2017/11/21 10:02:24.278 kid1| 28,3| Acl.cc(158) matches: checked: http_access#1 = 0
2017/11/21 10:02:24.278 kid1| 28,3| Acl.cc(158) matches: checked: CONNECT = 1
2017/11/21 10:02:24.278 kid1| 28,3| Acl.cc(158) matches: checked: SSL_ports = 1
2017/11/21 10:02:24.278 kid1| 28,3| Acl.cc(158) matches: checked: !SSL_ports = 0
2017/11/21 10:02:24.278 kid1| 28,3| Acl.cc(158) matches: checked: http_access#2 = 0
2017/11/21 10:02:24.278 kid1| 28,3| Ip.cc(539) match: aclIpMatchIp: '10.215.144.48' NOT found
2017/11/21 10:02:24.278 kid1| 28,3| Acl.cc(158) matches: checked: localhost = 0
2017/11/21 10:02:24.278 kid1| 28,3| Acl.cc(158) matches: checked: http_access#3 = 0
2017/11/21 10:02:24.278 kid1| 28,3| RegexData.cc(51) match: aclRegexData::match: checking '89.16.167.134:443'
2017/11/21 10:02:24.278 kid1| 28,3| RegexData.cc(62) match: aclRegexData::match: looking for '(^cache_object://)'
2017/11/21 10:02:24.278 kid1| 28,3| RegexData.cc(62) match: aclRegexData::match: looking for '(^https?://[^/]+/squid-internal-mgr/)'
2017/11/21 10:02:24.278 kid1| 28,3| Acl.cc(158) matches: checked: manager = 0
2017/11/21 10:02:24.278 kid1| 28,3| Acl.cc(158) matches: checked: http_access#4 = 0
2017/11/21 10:02:24.278 kid1| 28,3| StringData.cc(34) match: aclMatchStringList: checking '3229'
2017/11/21 10:02:24.278 kid1| 28,3| StringData.cc(37) match: aclMatchStringList: '3229' NOT found
2017/11/21 10:02:24.278 kid1| 28,3| Acl.cc(158) matches: checked: explicit = 0
2017/11/21 10:02:24.278 kid1| 28,3| Acl.cc(158) matches: checked: http_access#5 = 0
2017/11/21 10:02:24.278 kid1| 28,3| StringData.cc(34) match: aclMatchStringList: checking '3229'
2017/11/21 10:02:24.278 kid1| 28,3| StringData.cc(37) match: aclMatchStringList: '3229' NOT found
2017/11/21 10:02:24.278 kid1| 28,3| Acl.cc(158) matches: checked: explicit = 0
2017/11/21 10:02:24.278 kid1| 28,3| Acl.cc(158) matches: checked: http_access#6 = 0
2017/11/21 10:02:24.278 kid1| 28,3| StringData.cc(34) match: aclMatchStringList: checking '3229'
2017/11/21 10:02:24.278 kid1| 28,3| StringData.cc(37) match: aclMatchStringList: '3229' NOT found
2017/11/21 10:02:24.278 kid1| 28,3| Acl.cc(158) matches: checked: intercepted = 0
2017/11/21 10:02:24.278 kid1| 28,3| Acl.cc(158) matches: checked: http_access#7 = 0
2017/11/21 10:02:24.278 kid1| 28,3| StringData.cc(34) match: aclMatchStringList: checking '3229'
2017/11/21 10:02:24.278 kid1| 28,3| StringData.cc(37) match: aclMatchStringList: '3229' found
2017/11/21 10:02:24.278 kid1| 28,3| Acl.cc(158) matches: checked: interceptedssl = 1
2017/11/21 10:02:24.278 kid1| 28,3| Ip.cc(539) match: aclIpMatchIp: '10.215.144.48' found
2017/11/21 10:02:24.278 kid1| 28,3| Acl.cc(158) matches: checked: localnet = 1
2017/11/21 10:02:24.278 kid1| 28,3| Acl.cc(158) matches: checked: !localnet = 0
2017/11/21 10:02:24.278 kid1| 28,3| Acl.cc(158) matches: checked: http_access#8 = 0
2017/11/21 10:02:24.278 kid1| 28,3| Acl.cc(158) matches: checked: good_useragents = 0
2017/11/21 10:02:24.278 kid1| 28,3| Acl.cc(158) matches: checked: !good_useragents = 1
2017/11/21 10:02:24.278 kid1| 28,3| Ip.cc(539) match: aclIpMatchIp: '10.215.144.48' NOT found
2017/11/21 10:02:24.278 kid1| 28,3| Acl.cc(158) matches: checked: src_ips_with_any_useragent = 0
2017/11/21 10:02:24.278 kid1| 28,3| Acl.cc(158) matches: checked: !src_ips_with_any_useragent = 1
2017/11/21 10:02:24.278 kid1| 28,3| Acl.cc(158) matches: checked: http_access#9 = 1
2017/11/21 10:02:24.278 kid1| 28,3| Acl.cc(158) matches: checked: http_access = 1
2017/11/21 10:02:24.278 kid1| 28,3| Checklist.cc(63) markFinished: 0x13450c8 answer DENIED for match
2017/11/21 10:02:24.278 kid1| 28,3| Checklist.cc(163) checkCallback: ACLChecklist::checkCallback: 0x13450c8 answer=DENIED
2017/11/21 10:02:24.278 kid1| 28,3| Checklist.cc(70) preCheck: 0x7ffd4e3e2530 checking fast ACLs
2017/11/21 10:02:24.278 kid1| 28,3| Acl.cc(158) matches: checked: (access_log daemon:/var/log/squid/access.test.log line) = 1
2017/11/21 10:02:24.278 kid1| 28,3| Acl.cc(158) matches: checked: access_log daemon:/var/log/squid/access.test.log = 1
2017/11/21 10:02:24.278 kid1| 28,3| Checklist.cc(63) markFinished: 0x7ffd4e3e2530 answer ALLOWED for match
2017/11/21 10:02:24.288 kid1| 28,3| Checklist.cc(70) preCheck: 0xeb47c8 checking slow rules
2017/11/21 10:02:24.288 kid1| 28,3| Ip.cc(539) match: aclIpMatchIp: '10.215.144.48' found
2017/11/21 10:02:24.288 kid1| 28,3| Acl.cc(158) matches: checked: localnet = 1
2017/11/21 10:02:24.288 kid1| 28,3| Ip.cc(539) match: aclIpMatchIp: '10.215.144.48' NOT found
2017/11/21 10:02:24.288 kid1| 28,3| Acl.cc(158) matches: checked: explicit_only_src_ips = 0
2017/11/21 10:02:24.288 kid1| 28,3| Acl.cc(158) matches: checked: !explicit_only_src_ips = 1
2017/11/21 10:02:24.288 kid1| 28,3| Ip.cc(539) match: aclIpMatchIp: '10.215.144.48' NOT found
2017/11/21 10:02:24.288 kid1| 28,3| Acl.cc(158) matches: checked: privileged_src_ips = 0
2017/11/21 10:02:24.288 kid1| 28,3| Acl.cc(158) matches: checked: http_reply_access#1 = 0
2017/11/21 10:02:24.288 kid1| 28,3| Ip.cc(539) match: aclIpMatchIp: '10.215.144.48' found
2017/11/21 10:02:24.288 kid1| 28,3| Acl.cc(158) matches: checked: localnet = 1
2017/11/21 10:02:24.288 kid1| 28,3| Ip.cc(539) match: aclIpMatchIp: '10.215.144.48' NOT found
2017/11/21 10:02:24.288 kid1| 28,3| Acl.cc(158) matches: checked: explicit_only_src_ips = 0
2017/11/21 10:02:24.288 kid1| 28,3| Acl.cc(158) matches: checked: !explicit_only_src_ips = 1
2017/11/21 10:02:24.288 kid1| 28,3| DomainData.cc(108) match: aclMatchDomainList: checking 'www.gentoo.org'
2017/11/21 10:02:24.288 kid1| 28,3| DomainData.cc(113) match: aclMatchDomainList: 'www.gentoo.org' NOT found
2017/11/21 10:02:24.288 kid1| 28,3| Acl.cc(158) matches: checked: good_dst_domains = 0
2017/11/21 10:02:24.288 kid1| 28,3| Acl.cc(158) matches: checked: http_reply_access#2 = 0
2017/11/21 10:02:24.288 kid1| 28,3| RegexData.cc(51) match: aclRegexData::match: checking 'text/html;charset=utf-8'
2017/11/21 10:02:24.288 kid1| 28,3| RegexData.cc(62) match: aclRegexData::match: looking for '(^application/octet-stream$)'
2017/11/21 10:02:24.288 kid1| 28,3| Acl.cc(158) matches: checked: restricted_replied_mimetypes_1 = 0
2017/11/21 10:02:24.288 kid1| 28,3| Acl.cc(158) matches: checked: http_reply_access#3 = 0
2017/11/21 10:02:24.288 kid1| 28,3| RegexData.cc(51) match: aclRegexData::match: checking 'text/html;charset=utf-8'
2017/11/21 10:02:24.288 kid1| 28,3| RegexData.cc(62) match: aclRegexData::match: looking for '(^application/octet-stream$)'
2017/11/21 10:02:24.288 kid1| 28,3| Acl.cc(158) matches: checked: restricted_replied_mimetypes_1 = 0
2017/11/21 10:02:24.288 kid1| 28,3| Acl.cc(158) matches: checked: http_reply_access#4 = 0
2017/11/21 10:02:24.288 kid1| 28,3| RegexData.cc(51) match: aclRegexData::match: checking 'text/html;charset=utf-8'
2017/11/21 10:02:24.289 kid1| 28,3| RegexData.cc(62) match: aclRegexData::match: looking for '(^application/mp21$)|(^application/mp4$)|(^application/vnd.rn-realmedia$)|(^application/vnd.tmobile-livetv$)|(^audio/)|(^video/)'
2017/11/21 10:02:24.289 kid1| 28,3| Acl.cc(158) matches: checked: limited_replied_mimetypes_1 = 0
2017/11/21 10:02:24.289 kid1| 28,3| Acl.cc(158) matches: checked: http_reply_access#5 = 0
2017/11/21 10:02:24.289 kid1| 28,3| RegexData.cc(51) match: aclRegexData::match: checking 'text/html;charset=utf-8'
2017/11/21 10:02:24.289 kid1| 28,3| RegexData.cc(62) match: aclRegexData::match: looking for '(^application/octet-stream$)'
2017/11/21 10:02:24.289 kid1| 28,3| Acl.cc(158) matches: checked: restricted_replied_mimetypes_1 = 0
2017/11/21 10:02:24.289 kid1| 28,3| Acl.cc(158) matches: checked: http_reply_access#6 = 0
2017/11/21 10:02:24.289 kid1| 28,3| RegexData.cc(51) match: aclRegexData::match: checking 'text/html;charset=utf-8'
2017/11/21 10:02:24.289 kid1| 28,3| RegexData.cc(62) match: aclRegexData::match: looking for '(^application/mp21$)|(^application/mp4$)|(^application/vnd.rn-realmedia$)|(^application/vnd.tmobile-livetv$)|(^audio/)|(^video/)'
2017/11/21 10:02:24.289 kid1| 28,3| Acl.cc(158) matches: checked: limited_replied_mimetypes_1 = 0
2017/11/21 10:02:24.289 kid1| 28,3| Acl.cc(158) matches: checked: http_reply_access#7 = 0
2017/11/21 10:02:24.289 kid1| 28,3| RegexData.cc(51) match: aclRegexData::match: checking '/'
2017/11/21 10:02:24.289 kid1| 28,3| RegexData.cc(62) match: aclRegexData::match: looking for '(\.ade(\?.*)?$)|(\.adp(\?.*)?$)|(\.app(\?.*)?$)|(\.asd(\?.*)?$)|(\.asf(\?.*)?$)|(\.asx(\?.*)?$)|(\.avi(\?.*)?$)|(\.bas(\?.*)?$)|(\.bat(\?.*)?$)|(\.cab(\?.*)?$)|(\.chm(\?.*)?$)|(\.cmd(\?.*)?$)|(\.cpl(\?.*)?$)|(\.dll$)|(\.exe(\?.*)?$)|(\.fxp(\?.*)?$)|(\.hlp(\?.*)?$)|(\.hta(\?.*)?$)|(\.hto(\?.*)?$)|(\.inf(\?.*)?$)|(\.ini(\?.*)?$)|(\.ins(\?.*)?$)|(\.iso(\?.*)?$)|(\.isp(\?.*)?$)|(\.jse(.?)(\?.*)?$)|(\.jse(\?.*)?$)|(\.lib(\?.*)?$)|(\.lnk(\?.*)?$)|(\.mar(\?.*)?$)|(\.mdb(\?.*)?$)|(\.mde(\?.*)?$)|(\.mp3(\?.*)?$)|(\.mpeg(\?.*)?$)|(\.mpg(\?.*)?$)|(\.msc(\?.*)?$)|(\.msi(\?.*)?$)|(\.msp(\?.*)?$)|(\.mst(\?.*)?$)|(\.ocx(\?.*)?$)|(\.pcd(\?.*)?$)|(\.pif(\?.*)?$)|(\.prg(\?.*)?$)|(\.reg(\?.*)?$)|(\.scr(\?.*)?$)|(\.sct(\?.*)?$)|(\.sh(\?.*)?$)|(\.shb(\?.*)?$)|(\.shs(\?.*)?$)|(\.sys(\?.*)?$)|(\.url(\?.*)?$)|(\.vb(\?.*)?$)|(\.vbe(\?.*)?$)|(\.vbs(\?.*)?$)|(\.vcs(\?.*)?$)|(\.vxd(\?.*)?$)|(\.wmd(\?.*)?$)|(\.wms(\?.*)?$)|(\.wmv(\?.*)?$)|(\.wmz(\?.*)?$)|(\.wsc(\?.*)?$)|(\.wsf(\?.*)?$)|(\.wsh(\?.*)?$)'
2017/11/21 10:02:24.289 kid1| 28,3| Acl.cc(158) matches: checked: bad_filetypes = 0
2017/11/21 10:02:24.289 kid1| 28,3| Acl.cc(158) matches: checked: http_reply_access#8 = 0
2017/11/21 10:02:24.289 kid1| 28,3| RegexData.cc(51) match: aclRegexData::match: checking 'text/html;charset=utf-8'
2017/11/21 10:02:24.289 kid1| 28,3| RegexData.cc(62) match: aclRegexData::match: looking for '(^application/ecmascript$)|(^application/mp21$)|(^application/mp4$)|(^application/oebps-package+xml$)|(^application/vnd.amazon.ebook$)|(^application/vnd.android.package-archive$)|(^application/vnd.gmx$)|(^application/vnd.google-earth.kml+xml$)|(^application/vnd.google-earth.kmz$)|(^application/vnd.ms-cab-compressed$)|(^application/vnd.ms-excel.addin.macroenabled.12$)|(^application/vnd.ms-excel.sheet.binary.macroenabled.12$)|(^application/vnd.ms-excel.sheet.macroenabled.12$)|(^application/vnd.ms-excel.template.macroenabled.12$)|(^application/vnd.ms-powerpoint.addin.macroenabled.12$)|(^application/vnd.ms-powerpoint.presentation.macroenabled.12$)|(^application/vnd.ms-powerpoint.slide.macroenabled.12$)|(^application/vnd.ms-powerpoint.slideshow.macroenabled.12$)|(^application/vnd.ms-powerpoint.template.macroenabled.12$)|(^application/vnd.ms-wpl$)|(^application/vnd.ms.wms-hdr.asfv1$)|(^application/vnd.realvnc.bed$)|(^application/vnd.rn-realmedia$)|(^application/vnd.tmobile-livetv$)|(^application/x-authorware-bin$)|(^application/x-cab$)|(^application/x-iso9660-image$)|(^application/x-mms-framed$)|(^application/x-ms-wm$)|(^application/x-msdos-program$)|(^application/x-msdownload$)|(^application/x-shar$)|(^application/x-vbs$)|(^audio/)|(^text/vbs$)|(^text/vbscript$)|(^video/)'
2017/11/21 10:02:24.289 kid1| 28,3| Acl.cc(158) matches: checked: bad_replied_mimetypes = 0
2017/11/21 10:02:24.289 kid1| 28,3| Acl.cc(158) matches: checked: http_reply_access#9 = 0
2017/11/21 10:02:24.289 kid1| 28,3| Acl.cc(158) matches: checked: http_reply_access = 0
2017/11/21 10:02:24.289 kid1| 28,3| Checklist.cc(386) calcImplicitAnswer: 0xeb47c8 NO match found, last action DENIED so returning ALLOWED
2017/11/21 10:02:24.289 kid1| 28,3| Checklist.cc(63) markFinished: 0xeb47c8 answer ALLOWED for implicit rule won
2017/11/21 10:02:24.289 kid1| 28,3| Checklist.cc(163) checkCallback: ACLChecklist::checkCallback: 0xeb47c8 answer=ALLOWED
2017/11/21 10:02:24.289 kid1| 28,3| Checklist.cc(70) preCheck: 0x7ffd4e3e2660 checking fast ACLs
2017/11/21 10:02:24.289 kid1| 28,3| Acl.cc(158) matches: checked: (access_log daemon:/var/log/squid/access.test.log line) = 1
2017/11/21 10:02:24.289 kid1| 28,3| Acl.cc(158) matches: checked: access_log daemon:/var/log/squid/access.test.log = 1
2017/11/21 10:02:24.289 kid1| 28,3| Checklist.cc(63) markFinished: 0x7ffd4e3e2660 answer ALLOWED for match

It seems that Squid decides to ALLOW, right?

Now, here's the log with ALL,2:

2017/11/21 10:07:01.079 kid1| 5,2| TcpAcceptor.cc(220) doAccept: New connection on FD 93
2017/11/21 10:07:01.079 kid1| 5,2| TcpAcceptor.cc(295) acceptNext: connection on local=[::]:3229 remote=[::] FD 93 flags=25
2017/11/21 10:07:01.079 kid1| 33,2| client_side.cc(3943) httpsSslBumpAccessCheckDone: sslBump needed for local=89.16.167.134:443 remote=10.215.144.48 FD 13 flags=17 method 4
2017/11/21 10:07:01.079 kid1| 11,2| client_side.cc(2372) parseHttpRequest: HTTP Client local=89.16.167.134:443 remote=10.215.144.48 FD 13 flags=17
2017/11/21 10:07:01.079 kid1| 11,2| client_side.cc(2373) parseHttpRequest: HTTP Client REQUEST:
---------
CONNECT 89.16.167.134:443 HTTP/1.1
Host: 89.16.167.134:443


----------
2017/11/21 10:07:01.079 kid1| 85,2| client_side_request.cc(745) clientAccessCheckDone: The request CONNECT 89.16.167.134:443 is DENIED; last ACL checked: src_ips_with_any_useragent
2017/11/21 10:07:01.079 kid1| 20,2| store.cc(996) checkCachable: StoreEntry::checkCachable: NO: not cachable
2017/11/21 10:07:01.079 kid1| 20,2| store.cc(996) checkCachable: StoreEntry::checkCachable: NO: not cachable
2017/11/21 10:07:01.079 kid1| 20,2| store.cc(996) checkCachable: StoreEntry::checkCachable: NO: not cachable
2017/11/21 10:07:01.089 kid1| 83,2| client_side.cc(3843) clientNegotiateSSL: clientNegotiateSSL: New session 0x13c6250 on FD 13 (10.215.144.48:42279)
2017/11/21 10:07:01.090 kid1| 11,2| client_side.cc(2372) parseHttpRequest: HTTP Client local=89.16.167.134:443 remote=10.215.144.48 FD 13 flags=17
2017/11/21 10:07:01.090 kid1| 11,2| client_side.cc(2373) parseHttpRequest: HTTP Client REQUEST:
---------
GET / HTTP/1.1
Host: www.gentoo.org
User-Agent: Firefox/57
Accept: */*


----------
2017/11/21 10:07:01.090 kid1| 88,2| client_side_reply.cc(2073) processReplyAccessResult: The reply for GET https://www.gentoo.org/ is ALLOWED, because it matched bad_replied_mimetypes
2017/11/21 10:07:01.090 kid1| 11,2| client_side.cc(1409) sendStartOfMessage: HTTP Client local=89.16.167.134:443 remote=10.215.144.48 FD 13 flags=17
2017/11/21 10:07:01.090 kid1| 11,2| client_side.cc(1410) sendStartOfMessage: HTTP Client REPLY:
---------
HTTP/1.1 307 Temporary Redirect
Server: squid
Mime-Version: 1.0
Date: Tue, 21 Nov 2017 09:07:01 GMT
Content-Type: text/html;charset=utf-8
Content-Length: 0
Location: http://proxy-server1/proxy-error/?a=-&B=&e=0&E=%5BNo%20Error%5D&H=89.16.167.134&i=10.215.144.48&M=CONNECT&o=&R=/&T=Tue,%2021%20Nov%202017%2009%3A07%3A01%20GMT&U=https%3A%2F%2F89.16.167.134%2F*&u=89.16.167.134%3A443&w=IT%40mydomain.org&x=&acl=bad_useragents
X-Squid-Error: 403 Access Denied
X-Cache: MISS from proxy-server1
X-Cache-Lookup: NONE from proxy-server1:3227
Connection: close


----------
2017/11/21 10:07:01.090 kid1| 33,2| client_side.cc(832) swanSong: local=89.16.167.134:443 remote=10.215.144.48 flags=17
2017/11/21 10:07:01.090 kid1| 20,2| store.cc(996) checkCachable: StoreEntry::checkCachable: NO: not cachable
2017/11/21 10:07:01.090 kid1| 20,2| store.cc(996) checkCachable: StoreEntry::checkCachable: NO: not cachable

Isn't the message "The request CONNECT 89.16.167.134:443 is DENIED" what I should be concentrating on?
Isn't that the root cause?
In another message, you mentioned that I should notice that Squid reports another ACL name (in this case, after the name change, it's "bad_replied_mimetypes").
In any case, the message "The reply for GET https://www.gentoo.org/ is ALLOWED" means that Squid should ALLOW, right?
However, why do I get a 307 redirect to a deny_info page (where incidentally the URL refers to bad_useragents, not bad_replied_mimetypes)?

I can't seem to clear this out and make it work without adding "http_access allow CONNECT SSL_ports" right before checking for the useragent.

Help greatly appreciated.

Vieri


From squid at ephemeric.online  Tue Nov 21 10:36:53 2017
From: squid at ephemeric.online (Robert Gabriel)
Date: Tue, 21 Nov 2017 12:36:53 +0200
Subject: [squid-users] SSL Accel Connection Reset
In-Reply-To: <d9d30f5c-59b7-7462-a47b-3e310f0f8ef5@treenet.co.nz>
References: <20171120132445.GC26545@mail.ephemeric.online>
 <d9d30f5c-59b7-7462-a47b-3e310f0f8ef5@treenet.co.nz>
Message-ID: <20171121103653.GA27366@mail.ephemeric.online>

Hi Amos,

Oh man, I feel so stupid. Thank you for pointing that out.
I apologise for my ignorance to all on the list.

It's all working now, cheers!

On Tue 21 Nov, 05:31, Amos Jeffries wrote:
> On 21/11/17 02:24, Robert Gabriel wrote:
> >Hi,
> >
> >I've tried to setup SSL accel AKA reverse proxy. The HTTP accel part works but no joy
> >for the SSL side. Maybe I've misunderstood or misconfigured something?
> >
> >Thank you for the help.
> >
> >[root at node03 tmp]# export https_proxy="https://192.168.122.130:443"
> >
> >[root at node03 tmp]# curl -v --insecure https://data.ephemeric.local/test.tgz
> 
> As you can see from the below curl is using a CONNECT tunnel, which are only
> valid to a forward-proxy.
> 
> The https_port in Squid is expecting the TCP connection to immediately start
> with TLS traffic. Not a plain-text CONNECT message.
> 
> 
> >* About to connect() to proxy 192.168.122.130 port 443 (#0)
> >*   Trying 192.168.122.130...
> >* Connected to 192.168.122.130 (192.168.122.130) port 443 (#0)
> >* Establish HTTP proxy tunnel to data.ephemeric.local:443
> >>CONNECT data.ephemeric.local:443 HTTP/1.1
> >>Host: data.ephemeric.local:443
> >>User-Agent: curl/7.29.0
> >>Proxy-Connection: Keep-Alive
> >>
> 
> 
> Amos
> 
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users


From nachofw at adinet.com.uy  Tue Nov 21 16:00:17 2017
From: nachofw at adinet.com.uy (Ignacio Freyre)
Date: Tue, 21 Nov 2017 13:00:17 -0300 (UYT)
Subject: [squid-users] parent peer timeout (Amos Jeffries)
In-Reply-To: <mailman.1327.1511258816.2879.squid-users@lists.squid-cache.org>
References: <mailman.1327.1511258816.2879.squid-users@lists.squid-cache.org>
Message-ID: <1979722988.35930395.1511280017168.JavaMail.zimbra@vera.com.uy>

Hi Amos, thanks for taking the time to analize this.

>Are you actually terminating the peer, or just simulating it some other way?
My method of testing is shutting down the service on the parent "192.168.1.1" with "/etc/init.d/squid stop", whith this in place there are no remaining active connections, and no new ones are being established, all I see is tcp RST responses.
It seems there is a TCP timer that is not configurable, because of the time it takes to notice the dead peer:
> 2017/11/20 22:55:02| Ready to serve requests.
> 2017/11/20 22:55:03| storeLateRelease: released 0 objects
> 2017/11/20 22:56:55| TCP connection to 192.168.1.1/3128 failed
> 2017/11/20 22:56:55| TCP connection to 192.168.1.1/3128 failed
> 2017/11/20 22:56:55| Detected DEAD Parent: 192.168.1.1
My objective is to configure dead peer detection based only in TCP connection, can this be achieved?

Do I need to allow a specific type of traffic with "cache_peer_access" statements so dead peer detection happens?, if I comment those lines, dead peer detection works, but I need to enable it so i can filter what traffic those parent peers accept.


regards,
ignacio



On 21/11/17 14:09, Ignacio Freyre wrote:
> Hi guys, i have a simple configuration that i'm testing with 2 parent proxys for a specific domain, if parent proxy 192.168.1.1 fails, failover to 192.168.1.2 proxy.
> I have a couple of questions:
> 1)Having configured "connect-timeout=3" and "connect-fail-limit=2", failover takes about 2 minutes, how can I reduce failover time?

Are you actually terminating the peer, or just simulating it some other way?

The behaviour you are seeing is what will happen for the particular 
error you cause to happen. I suspect you are only simulating a firewall 
rule table overload (ie firewall suddenly stops allowing *new* 
connections) instead of actual peer machine disconnect or shutdown.

The connect-timeout=3 is to make *new* TCP connections signal failure if 
the SYN+ACK takes more than 3 seconds to return. Otherwise it is a 
successful connect.

Added to that Squid is HTTP/1.1 software these days. Which means it uses 
multiplexing and pipeline features to reduce new TCP connections being 
needed at all. So that type of network failure may have zero effect on 
the proxy<->peer communications. Exactly as intended by the HTTP/1.1 design.


> 2)If I enable cache_peer_access statements, failover never happens because the peers dont get detected as dead


You disabled the features used as primary methods of detecting dead 
peers (no-query no-digest).

Additionally restricting traffic with cache_peer_access removes 
additional hints from HTTP and TCP traffic.


It is hard to say how those two things are impacting your proxies peer 
selection logic, since it is also complicated by the things mentioned 
above about #1.


> 
> #CONFIGURATION START
> #hostname
> visible_hostname testing
> 
> #parent proxy's
> cache_peer 192.168.1.1 parent 3128 0 no-query no-digest connect-timeout=3 connect-fail-limit=2
> cache_peer 192.168.1.2 parent 3128 0 no-query no-digest connect-timeout=3 connect-fail-limit=2
> 
> #send traffic to peers
> acl foo_url url_regex site\.domain\.com
> never_direct allow foo_url

regex is the second slowest ACL type around, generally to match domain 
use dstdomain ACL type.


> 
> #peer access
> cache_peer_access 192.168.1.1 deny !foo_url
> cache_peer_access 192.168.1.2 deny !foo_url
> 
> #allow all for testing purposes
> http_access allow all
> 

Not a good idea even for testing purposes. If there is a problem with 
your intended http_access rules that needs solving before anything else 
can be properly investigated since what is allowed to be handled by the 
proxy impacts on what can happen for outbound attempts.


> # Squid normally listens to port 3128
> http_port 3128
> 
> # Leave coredumps in the first cache dir
> coredump_dir /var/spool/squid
> 
> # Add any of your own refresh_pattern entries above these.
> refresh_pattern ^ftp:           1440    20%     10080
> refresh_pattern ^gopher:        1440    0%      1440
> refresh_pattern -i (/cgi-bin/|\?) 0     0%      0
> refresh_pattern .               0       20%     4320
> #CONFIGURATION END
> 
> LOGS that I see when peer is detected as dead
> 2017/11/20 22:55:02| Ready to serve requests.
> 2017/11/20 22:55:03| storeLateRelease: released 0 objects
> 2017/11/20 22:56:55| TCP connection to 192.168.1.1/3128 failed
> 2017/11/20 22:56:55| TCP connection to 192.168.1.1/3128 failed
> 2017/11/20 22:56:55| Detected DEAD Parent: 192.168.1.1
> 

Configure "debug_options 28,3" to see the peer selection results.


Amos



From squid3 at treenet.co.nz  Tue Nov 21 16:54:09 2017
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Wed, 22 Nov 2017 05:54:09 +1300
Subject: [squid-users] block user agent
In-Reply-To: <85837667.184253.1511258798085@mail.yahoo.com>
References: <160200155.402986.1510744710954.ref@mail.yahoo.com>
 <160200155.402986.1510744710954@mail.yahoo.com>
 <61ce5b3f-bb5e-235f-351c-f3b4ae339fd5@treenet.co.nz>
 <1140434830.1059565.1510821897156@mail.yahoo.com>
 <4f2bc2b8-f307-e62c-5225-de3054e9e556@measurement-factory.com>
 <107610513.275578.1510932471911@mail.yahoo.com>
 <8edbcf42-bc37-a63b-8be0-daf088770108@measurement-factory.com>
 <752783637.1436443.1511167540536@mail.yahoo.com>
 <45c9ef41-5134-4df0-b46b-25dc0035eff1@treenet.co.nz>
 <85837667.184253.1511258798085@mail.yahoo.com>
Message-ID: <84a81f25-baf9-0255-6465-391248c556c5@treenet.co.nz>

On 21/11/17 23:06, Vieri wrote:
> 
> ________________________________
> From: Amos Jeffries
>>
>>    http_access allow goodAgents !baddomains (AND)
>>
>>   If the first line matches the allow happens.
>>   otherwise deny happens
>>
>> ie. goodAgents are only allowed to non-baddomains. All non-goodAgents
>> are denied to everything.
> 
> 
>  From this I deduce that in my case I cannot use "http_access allow goodAgents", but I need to go for "http_access deny !goodAgents" so I can continue on evaluating the rest of my http_access rules.
>> Allowing them all the way through Squid is bad. But that is not what is
>> needed here. ssl_bump rules get applied after the CONNECT is accepted
>> *in* for proxy processing and they decide what happens to the tunneled
>> data based on what is found there.
>>    If bumping is decided the TLS gets removed and the messages inside
>> individually go through the http_access process.
> 
> 
> You lost me there. Here's what I did today.
> 
> I took your advice (and Alex's), and renamed my ACL labels. Unfortunately, I'm still a little confused :-(.
> 

[ snipping the log traces to keep the mail relatively small ]
> 
> It seems that Squid decides to ALLOW, right?

Look at the "markFinished" lines for the outcome of each set of access 
controls outcomes. The lines leading up to those details which ACL tests 
are performed and which *_access lines they were on.


Ignoring the first few quoted lines which are for some earlier 
ssl-bumped transaction I see:

* http_access line #9 DENIED an HTTP request.

* access_log is ALLOWED to record something. Probably unrelated traffic.

* http_reply_access line #9 ALLOWED a reply to be delivered, then

* access_log is ALLOWED to log something.


> 
> Isn't the message "The request CONNECT 89.16.167.134:443 is DENIED" what I should be concentrating on?
> Isn't that the root cause?

Yes, that line is the outcome. The cause of the denial is what ACL 
check(s) led to it.

Specifically in these log lines:

   matches: checked: interceptedssl = 1
   match: aclIpMatchIp: '10.215.144.48' found
   matches: checked: localnet = 1
   matches: checked: !localnet = 0
   matches: checked: http_access#8 = 0

   matches: checked: good_useragents = 0
   matches: checked: !good_useragents = 1
   match: aclIpMatchIp: '10.215.144.48' NOT found
   matches: checked: src_ips_with_any_useragent = 0
   matches: checked: !src_ips_with_any_useragent = 1
   matches: checked: http_access#9 = 1
   matches: checked: http_access = 1

Which strangely do not seem to match your squid.conf details. These are 
the 8th and 9th http_access lines in the squid.conf which is used by the 
running proxy.
But in your quoted squid.conf they are lines #4 and #5.


  http_access deny interceptedssl !localnet
   - it was interceptedssl from localnet
   - so not a match due to !localnet

  http_access deny !good_useragents !src_ips_with_any_useragent
   - it has no UA, and
   - it is not listed n that IP whitelist.
   - the NOT condition (!) on both of those make the whole line a match.


> In another message, you mentioned that I should notice that Squid reports another ACL name (in this case, after the name change, it's "bad_replied_mimetypes").
> In any case, the message "The reply for GET https://www.gentoo.org/ is ALLOWED" means that Squid should ALLOW, right?

No, it means that *reply* is allowed.

A reply *might* be from a server, or from cache, or a 403 denial error 
page generated by Squid, or one of the deny_info redirects you have 
configured to happen - like that one in the "HTTP Client REPLY" in the 
second log trace.



> However, why do I get a 307 redirect to a deny_info page (where incidentally the URL refers to bad_useragents, not bad_replied_mimetypes)?

Because the CONNECT _request_ was denied and that redirect _reply_ is 
what a deny_info with a URL generates when its associated ACL causes a 
denial.

bad_useragents was the ACL checking the *request* which triggered that 
redirect to happen.

bad_replied_mimetypes was just the last ACL tested to see if that 
redirect was allowed to be delivered to the client.


If we assume that the two log traces actually line up then 
bad_replied_mimetypes is actually irrelevant because the 
http_reply_access result is actually "NO match found, last action DENIED 
so returning ALLOWED"


> 
> I can't seem to clear this out and make it work without adding "http_access allow CONNECT SSL_ports" right before checking for the useragent.


If you place that after the default "deny CONNECT !SSL_ports", and 
before your UA checks, AND if you are using ssl_bump on the allowed 
tunnels then you can relatively safely use "allow CONNECT".

Just be careful that the CONNECT allowed by that are always handled 
safely by the ssl_bump rules you have.
  Meaning that you either bump or terminate traffic you are not sure is 
okay, splice if you are reasonably sure, etc. it is a balancing effort 
between "splice as much as possible" and "terminate if unsure of the 
traffic" advice.


Just FYI you would be a huge amount better off dropping the UA 
fingerprinting. It's a _really_ simplistic idea about the HTTP world, 
and it is partly because of that overly-simplistic nature and depending 
on unreliable values that you are having so much more trouble than 
normal admin face.

Amos


From phackmann at gmail.com  Tue Nov 21 17:08:35 2017
From: phackmann at gmail.com (Paul Hackmann)
Date: Tue, 21 Nov 2017 11:08:35 -0600
Subject: [squid-users] different authentication for different ports
In-Reply-To: <c165ff4c-35a2-b6c5-3854-c30ff90a5df1@treenet.co.nz>
References: <CADQL5rOxV-YHTkR0Ft9hC40o1948FjPrxvY+NWwtQ758wer3FQ@mail.gmail.com>
 <f5548575-c296-b934-24a7-63dc388f7148@treenet.co.nz>
 <CADQL5rMn_XJUyoygitWRjNOXv0QRtHa0jcUW+-L2c2MRNmbheA@mail.gmail.com>
 <c165ff4c-35a2-b6c5-3854-c30ff90a5df1@treenet.co.nz>
Message-ID: <CADQL5rNY+_n0nN5kDbRc9N86Y8-yFPH3Ds2TitJpLhnF=KxTEQ@mail.gmail.com>

Amos,

That was exactly what I was looking for.  I tried it and it seems to work
just like I wanted.  My other alternative would have been to run 2 copies
of squid, but this is much cleaner from my perspective.  Thank you very
much!

PH

On Mon, Nov 20, 2017 at 9:13 PM, Amos Jeffries <squid3 at treenet.co.nz> wrote:

> On 21/11/17 06:56, Paul Hackmann wrote:
>
>> Amos,
>>
>> If the website that is being asked for is not in the whitelist, won't it
>> fall through and ask for authentication?  That is how it seems to work to
>> me.  That's why I am thinking I need 2 different ports or something to do
>> what I want.
>>
>
> You do need two different ports regardless of the http_access rules. One
> for the forward/explicit proxy traffic and one for the intercept/tproxy
> traffic. The TCP IP:port details for each of those "modes" is given in
> completely different ways and the HTTP message syntax is also different so
> the *cannot* be delivered to the same ports.
>
>
> A whitelist generally is formed from two lines, one allowing and one
> denying everything else.
>
> If 'everything else' is defined as just the stuff arriving in one specific
> port you get this:
>
>  http_port 3128
>  http_port 3129 intercept
>
>  acl portX myportname 3129
>
>  http_access allow portX whitelist
>  http_access deny portX
>
>  http_access deny !login
>  ...
>
> Amos
>
>
>
>> PH
>>
>>
>> On Mon, Nov 20, 2017 at 11:38 AM, Amos Jeffries <squid3 at treenet.co.nz
>> <mailto:squid3 at treenet.co.nz>> wrote:
>>
>>     On 21/11/17 05:02, Paul Hackmann wrote:
>>
>>         Hi all.  I've got a fairly basic squid config set up on linux.
>>      I have basic authentication set up on it to the default 3128
>>         port, and it works just fine.  I would like to keep this
>>         configuration.  However, I would like to set up another port
>>         that only allows a certain whitelist of websites that doesn't
>>         require or ask for authentication.  I want to set this up for
>>         certain apps that don't have proxy settings built into them.  I
>>         want windows to be able to connect to some sites, but not
>>         everything and if it can't reach the site, I don't want it to
>>         ask for credentials.  With my current configuration, it asks for
>>         credentials for any app that is trying to connect to a
>>         non-whitelisted website.  Is this configuration possible and do
>>         you have an example?  Sorry if this has been answered before, I
>>         am very green to squid yet.
>>
>>
>>     Simply place the http_access rules for handling that traffic above
>>     the first line which requires authentication.
>>
>>        http_access ... lines that dont require auth.
>>
>>        acl login proxy_auth REQUIRED
>>        http_access deny !login
>>
>>        http_access ... rules for authenticated users.
>>
>>
>>     Amos
>>     _______________________________________________
>>     squid-users mailing list
>>     squid-users at lists.squid-cache.org
>>     <mailto:squid-users at lists.squid-cache.org>
>>     http://lists.squid-cache.org/listinfo/squid-users
>>     <http://lists.squid-cache.org/listinfo/squid-users>
>>
>>
>>
>>
>> --
>> Paul Hackmann
>> Sims TV/Haven Electronics
>> 121 N. Vine St.
>> West Union, IA. 52175
>> 563-422-5751 <tel:(563)%20422-5751>
>>
>>
>> _______________________________________________
>> squid-users mailing list
>> squid-users at lists.squid-cache.org
>> http://lists.squid-cache.org/listinfo/squid-users
>>
>> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>



-- 
Paul Hackmann
Sims TV/Haven Electronics
121 N. Vine St.
West Union, IA. 52175
563-422-5751
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20171121/252c9f0e/attachment.htm>

From squid3 at treenet.co.nz  Tue Nov 21 17:15:49 2017
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Wed, 22 Nov 2017 06:15:49 +1300
Subject: [squid-users] parent peer timeout (Amos Jeffries)
In-Reply-To: <1979722988.35930395.1511280017168.JavaMail.zimbra@vera.com.uy>
References: <mailman.1327.1511258816.2879.squid-users@lists.squid-cache.org>
 <1979722988.35930395.1511280017168.JavaMail.zimbra@vera.com.uy>
Message-ID: <dbc9fea5-36ed-df06-90ad-7ad4ce951b0e@treenet.co.nz>

On 22/11/17 05:00, Ignacio Freyre wrote:
> Hi Amos, thanks for taking the time to analize this.
> 
>> Are you actually terminating the peer, or just simulating it some other way?
> My method of testing is shutting down the service on the parent "192.168.1.1" with "/etc/init.d/squid stop", whith this in place there are no remaining active connections, and no new ones are being established, all I see is tcp RST responses.

Ah, add to your tests a check to see when that process actually stops. 
It is quite likely that a long portion of those 2 minutes is the peer 
doing its slow graceful shutdown procedure - during which time it will 
stay LIVE and not DEAD.

You may also want to monitor the TCP state of the connections from Squid 
to the peer. Termination by the endpoint may not immediately trigger 
full connection closure all the way into Squid. So there is a bit of 
delay there as well until Squid picks up on the change.

The best way to shutdown Squid is with the "squid -k shutdown" command. 
Use it twice in a row for quick shutdown. First use initiates shutdown, 
second one skips the process to the end of the graceful delay.


> It seems there is a TCP timer that is not configurable, because of the time it takes to notice the dead peer:
>> 2017/11/20 22:55:02| Ready to serve requests.
>> 2017/11/20 22:55:03| storeLateRelease: released 0 objects
>> 2017/11/20 22:56:55| TCP connection to 192.168.1.1/3128 failed
>> 2017/11/20 22:56:55| TCP connection to 192.168.1.1/3128 failed
>> 2017/11/20 22:56:55| Detected DEAD Parent: 192.168.1.1
> My objective is to configure dead peer detection based only in TCP connection, can this be achieved?

Yes, by the means you already configured.

Also ICMP is not optional. Ensure you have it working in your network. 
TCP connect errors are sent using ICMP from the network router(s) to 
Squid in just nanoseconds instead of whole seconds of waiting times. 
That should make the connect-timeout= setting mostly irrelevant.


> 
> Do I need to allow a specific type of traffic with "cache_peer_access" statements so dead peer detection happens?, if I comment those lines, dead peer detection works, but I need to enable it so i can filter what traffic those parent peers accept.
> 

What you configured should have been fine.

The issue is just that by relying only on the TCP/HTTP traffic for 
detection, reducing traffic sent to the peer also reduces its chances to 
detect failures. YMMV as to whether that is a good thing or not.

Amos


From Ralf.Hildebrandt at charite.de  Wed Nov 22 10:33:59 2017
From: Ralf.Hildebrandt at charite.de (Ralf Hildebrandt)
Date: Wed, 22 Nov 2017 11:33:59 +0100
Subject: [squid-users] Lots of "BUG 3279: HTTP reply without Date:" after
 update to squid-5.0.0-20171117-r4d27d0a
Message-ID: <20171122103359.tixi7ewbqtgua5uh@charite.de>

I was updating from squid-5.0.0-20171103-re3bf07f
to                  squid-5.0.0-20171117-rd816577 (and I was also testing squid-5.0.0-20171117-r4d27d0a)

today and immediately found lots of "BUG 3279: HTTP reply without
Date:" messages in my log (cache_dir had been cleared upon start,
meaning I'm starting with a clean slate!):

2017/11/22 11:16:02| BUG 3279: HTTP reply without Date:
2017/11/22 11:16:02| StoreEntry->key: 2CD6020000000000B515000000000000
2017/11/22 11:16:02| StoreEntry->next: 0x405f50e8
2017/11/22 11:16:02| StoreEntry->mem_obj: 0x35969af0
2017/11/22 11:16:02| StoreEntry->timestamp: 0
2017/11/22 11:16:02| StoreEntry->lastref: 1511345762
2017/11/22 11:16:02| StoreEntry->expires: 1511345762
2017/11/22 11:16:02| StoreEntry->lastModified_: -1
2017/11/22 11:16:02| StoreEntry->swap_file_sz: 0
2017/11/22 11:16:02| StoreEntry->refcount: 1
2017/11/22 11:16:02| StoreEntry->flags: REVALIDATE_ALWAYS,RELEASE_REQUEST,DISPATCHED,PRIVATE,VALIDATED
2017/11/22 11:16:02| StoreEntry->swap_dirn: -1
2017/11/22 11:16:02| StoreEntry->swap_filen: -1
2017/11/22 11:16:02| StoreEntry->lock_count: 3
2017/11/22 11:16:02| StoreEntry->mem_status: 0
2017/11/22 11:16:02| StoreEntry->ping_status: 2
2017/11/22 11:16:02| StoreEntry->store_status: 1
2017/11/22 11:16:02| StoreEntry->swap_status: 0
2017/11/22 11:16:03| BUG 3279: HTTP reply without Date:
2017/11/22 11:16:03| StoreEntry->key: 13D6020000000000B515000000000000
2017/11/22 11:16:03| StoreEntry->next: 0
2017/11/22 11:16:03| StoreEntry->mem_obj: 0x57a85d0
2017/11/22 11:16:03| StoreEntry->timestamp: 0
2017/11/22 11:16:03| StoreEntry->lastref: 1511345762
2017/11/22 11:16:03| StoreEntry->expires: 1511345763
2017/11/22 11:16:03| StoreEntry->lastModified_: -1
2017/11/22 11:16:03| StoreEntry->swap_file_sz: 0
2017/11/22 11:16:03| StoreEntry->refcount: 1
2017/11/22 11:16:03| StoreEntry->flags: RELEASE_REQUEST,DISPATCHED,PRIVATE,VALIDATED
2017/11/22 11:16:03| StoreEntry->swap_dirn: -1
2017/11/22 11:16:03| StoreEntry->swap_filen: -1
2017/11/22 11:16:03| StoreEntry->lock_count: 3
2017/11/22 11:16:03| StoreEntry->mem_status: 0
2017/11/22 11:16:03| StoreEntry->ping_status: 2
2017/11/22 11:16:03| StoreEntry->store_status: 1
2017/11/22 11:16:03| StoreEntry->swap_status: 0
2017/11/22 11:16:03| BUG 3279: HTTP reply without Date:
2017/11/22 11:16:03| StoreEntry->key: 6AD6020000000000B515000000000000
2017/11/22 11:16:03| StoreEntry->next: 0
2017/11/22 11:16:03| StoreEntry->mem_obj: 0x2fe3e2b0
2017/11/22 11:16:03| StoreEntry->timestamp: 0
2017/11/22 11:16:03| StoreEntry->lastref: 1511345763
2017/11/22 11:16:03| StoreEntry->expires: 1511345763
2017/11/22 11:16:03| StoreEntry->lastModified_: -1
2017/11/22 11:16:03| StoreEntry->swap_file_sz: 0
2017/11/22 11:16:03| StoreEntry->refcount: 1
2017/11/22 11:16:03| StoreEntry->flags: REVALIDATE_ALWAYS,RELEASE_REQUEST,DISPATCHED,PRIVATE,VALIDATED
2017/11/22 11:16:03| StoreEntry->swap_dirn: -1
2017/11/22 11:16:03| StoreEntry->swap_filen: -1
2017/11/22 11:16:03| StoreEntry->lock_count: 3
2017/11/22 11:16:03| StoreEntry->mem_status: 0
2017/11/22 11:16:03| StoreEntry->ping_status: 2
2017/11/22 11:16:03| StoreEntry->store_status: 1
2017/11/22 11:16:03| StoreEntry->swap_status: 0

is this a known bug with the recent snapshot?


-- 
Ralf Hildebrandt                   Charite Universit?tsmedizin Berlin
ralf.hildebrandt at charite.de        Campus Benjamin Franklin
https://www.charite.de             Hindenburgdamm 30, 12203 Berlin
Gesch?ftsbereich IT, Abt. Netzwerk fon: +49-30-450.570.155


From rentorbuy at yahoo.com  Wed Nov 22 10:48:48 2017
From: rentorbuy at yahoo.com (Vieri)
Date: Wed, 22 Nov 2017 10:48:48 +0000 (UTC)
Subject: [squid-users] block user agent
In-Reply-To: <84a81f25-baf9-0255-6465-391248c556c5@treenet.co.nz>
References: <160200155.402986.1510744710954.ref@mail.yahoo.com>
 <160200155.402986.1510744710954@mail.yahoo.com>
 <61ce5b3f-bb5e-235f-351c-f3b4ae339fd5@treenet.co.nz>
 <1140434830.1059565.1510821897156@mail.yahoo.com>
 <4f2bc2b8-f307-e62c-5225-de3054e9e556@measurement-factory.com>
 <107610513.275578.1510932471911@mail.yahoo.com>
 <8edbcf42-bc37-a63b-8be0-daf088770108@measurement-factory.com>
 <752783637.1436443.1511167540536@mail.yahoo.com>
 <45c9ef41-5134-4df0-b46b-25dc0035eff1@treenet.co.nz>
 <85837667.184253.1511258798085@mail.yahoo.com>
 <84a81f25-baf9-0255-6465-391248c556c5@treenet.co.nz>
Message-ID: <1355260125.127820.1511347728876@mail.yahoo.com>

________________________________
From: Amos Jeffries <squid3 at treenet.co.nz>
>
> If you place that after the default "deny CONNECT !SSL_ports", and 
> before your UA checks, AND if you are using ssl_bump on the allowed 
> tunnels then you can relatively safely use "allow CONNECT".
> 
> Just be careful that the CONNECT allowed by that are always handled 
> safely by the ssl_bump rules you have.
>   Meaning that you either bump or terminate traffic you are not sure is 
> okay, splice if you are reasonably sure, etc. it is a balancing effort 
> between "splice as much as possible" and "terminate if unsure of the 
> traffic" advice.


As you say, I placed "allow CONNECT" after the default "deny CONNECT !SSL_ports", and before my UA checks. I'm also using:
ssl_bump stare all
ssl_bump bump all


Considering the following (taken from previous e-mail):

http_access deny intercepted !localnet
http_access deny interceptedssl !localnet
http_access deny explicit !ORG_all
http_access deny explicit SSL_ports

Would it be "safer" or "indifferent" to use the following right before the UA checks?

http_access allow CONNECT interceptedssl SSL_ports


> Just FYI you would be a huge amount better off dropping the UA 
> fingerprinting. It's a _really_ simplistic idea about the HTTP world, 
> and it is partly because of that overly-simplistic nature and depending 
> on unreliable values that you are having so much more trouble than 
> normal admin face.


I'm aware that UA checks are not fully reliable, but in a big corporate environment it can reveal a lot of interested information.

I also know that some HTTP clients mimic others' user-agent strings or substrings. They can even sometimes dynamically change them.

However, in my particular case I could define a custom UA for our corporate browser allowed to go through Squid. For instance, Firefox can easily do that. Other browsers such as Edge seem not to.
In any case, it is not my intention to do so long-term. In short-term I found out that:

1) Squid logic *can* be understood :-)

2) some hosts may have HTTP clients that should be blocked even though the rest of the Squid rules were not programmed for that (so I couldn't know about it). A simple example: we may allow traffic to all microsoft sites, but some software may not necessarily be well installed/configured. I found that Microsoft Office may connect to an MS site to download or update software with a utility/service called 
OfficeClickToRun. Of course, generic rules in Squid.conf already blocked unauthorized downloads according to mimetypes or filetypes. However, some clients could be whitelisted and allowed to download (eg. from all MS sites). In this case, I would not necessarily want OfficeClickToRun to update. That could be done by identifying the dst domains, but they could change in time, and in any case would require more digging into. 


Adobe has similar http client behavior.


Anyway, it's informative to say the least, and can be used to improve the rest of the "standard" squid acl access rules.

I was also thinking of using custom HTTP headers such as X-MyCustomHeader: Whatever instead of UA strings. Custom headers can easily be added in Firefox, and other browsers such as Edge also seem to support that.

Anyway, I had a great time fiddling with Squid.
Thank you for your assistance.

Vieri


From chip_pop at hotmail.com  Wed Nov 22 12:25:14 2017
From: chip_pop at hotmail.com (joseph)
Date: Wed, 22 Nov 2017 05:25:14 -0700 (MST)
Subject: [squid-users] Lots of "BUG 3279: HTTP reply without Date:"
 after update to squid-5.0.0-20171117-r4d27d0a
In-Reply-To: <20171122103359.tixi7ewbqtgua5uh@charite.de>
References: <20171122103359.tixi7ewbqtgua5uh@charite.de>
Message-ID: <1511353514783-0.post@n4.nabble.com>

after investigating  bug was exist but hidden all update ar correct
https://bugs.squid-cache.org/show_bug.cgi?id=3279
bug 3279  not yet fixed in v5
hint if they adapt the v3 patch will solve the problem :)



-----
************************** 
***** Crash to the future  ****
**************************
--
Sent from: http://squid-web-proxy-cache.1019090.n4.nabble.com/Squid-Users-f1019091.html


From squid3 at treenet.co.nz  Wed Nov 22 12:41:30 2017
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 23 Nov 2017 01:41:30 +1300
Subject: [squid-users] block user agent
In-Reply-To: <1355260125.127820.1511347728876@mail.yahoo.com>
References: <160200155.402986.1510744710954.ref@mail.yahoo.com>
 <160200155.402986.1510744710954@mail.yahoo.com>
 <61ce5b3f-bb5e-235f-351c-f3b4ae339fd5@treenet.co.nz>
 <1140434830.1059565.1510821897156@mail.yahoo.com>
 <4f2bc2b8-f307-e62c-5225-de3054e9e556@measurement-factory.com>
 <107610513.275578.1510932471911@mail.yahoo.com>
 <8edbcf42-bc37-a63b-8be0-daf088770108@measurement-factory.com>
 <752783637.1436443.1511167540536@mail.yahoo.com>
 <45c9ef41-5134-4df0-b46b-25dc0035eff1@treenet.co.nz>
 <85837667.184253.1511258798085@mail.yahoo.com>
 <84a81f25-baf9-0255-6465-391248c556c5@treenet.co.nz>
 <1355260125.127820.1511347728876@mail.yahoo.com>
Message-ID: <c56942ab-fe7d-782b-930c-0a969151d92f@treenet.co.nz>

On 22/11/17 23:48, Vieri wrote:
> ________________________________
> From: Amos Jeffries <squid3 at treenet.co.nz>
>>
>> If you place that after the default "deny CONNECT !SSL_ports", and
>> before your UA checks, AND if you are using ssl_bump on the allowed
>> tunnels then you can relatively safely use "allow CONNECT".
>>
>> Just be careful that the CONNECT allowed by that are always handled
>> safely by the ssl_bump rules you have.
>>    Meaning that you either bump or terminate traffic you are not sure is
>> okay, splice if you are reasonably sure, etc. it is a balancing effort
>> between "splice as much as possible" and "terminate if unsure of the
>> traffic" advice.
> 
> 
> As you say, I placed "allow CONNECT" after the default "deny CONNECT !SSL_ports", and before my UA checks. I'm also using:
> ssl_bump stare all
> ssl_bump bump all
> 
> 
> Considering the following (taken from previous e-mail):
> 
> http_access deny intercepted !localnet
> http_access deny interceptedssl !localnet
> http_access deny explicit !ORG_all
> http_access deny explicit SSL_ports
> 
> Would it be "safer" or "indifferent" to use the following right before the UA checks?
> 
> http_access allow CONNECT interceptedssl SSL_ports
> 

All CONNECT transactions that get past that earlier line with !SSL_Ports 
will match SSL_Ports. So that part of the line is redundant.

The "CONNECT interceptedssl" is more restricted than just "CONNECT" - so 
is safer due to that yes. But also leaves some traffic open to the same 
denial problem you had earlier if non-UA CONNECT happen other ways. Up 
to you whether that is wanted or acceptible.


Amos


From rentorbuy at yahoo.com  Wed Nov 22 14:33:50 2017
From: rentorbuy at yahoo.com (Vieri)
Date: Wed, 22 Nov 2017 14:33:50 +0000 (UTC)
Subject: [squid-users] url_rewrite_program and ACLs
In-Reply-To: <133e0298-3080-d4fd-5a18-342b1b8b55c7@treenet.co.nz>
References: <1990137371.4646637.1510150336187.ref@mail.yahoo.com>
 <1990137371.4646637.1510150336187@mail.yahoo.com>
 <921fbc5d-fce5-fd6f-b62e-8afd617994c7@treenet.co.nz>
 <131538788.5259356.1510213867661@mail.yahoo.com>
 <e89c27ac-b147-426e-e220-2c566b5e7586@treenet.co.nz>
 <2014705679.5324553.1510227544567@mail.yahoo.com>
 <b0bd560b-081d-ec7b-261b-d92c1d479f98@treenet.co.nz>
 <1915184707.1464260.1511169300258@mail.yahoo.com>
 <133e0298-3080-d4fd-5a18-342b1b8b55c7@treenet.co.nz>
Message-ID: <1170730150.225886.1511361230924@mail.yahoo.com>


________________________________
From: Amos Jeffries <squid3 at treenet.co.nz>
>
> If we assume that each request opens a new connection and they are not 
> closed until TCP times out on the socket we do get numbers much more 
> like that 11K+ you are seeing.
> 
> That implies that ICAP transactions are probably not finishing 

> completely.

I'll have to look into this asap. Quick question: if I restart c-icap shouldn't I see a drop in open FD numbers if it were c-icap's "fault"?

I restarted c-icap (stop+start), but the open FDs are the same.

Thanks,

Vieri


From squid3 at treenet.co.nz  Wed Nov 22 15:23:54 2017
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 23 Nov 2017 04:23:54 +1300
Subject: [squid-users] url_rewrite_program and ACLs
In-Reply-To: <1170730150.225886.1511361230924@mail.yahoo.com>
References: <1990137371.4646637.1510150336187.ref@mail.yahoo.com>
 <1990137371.4646637.1510150336187@mail.yahoo.com>
 <921fbc5d-fce5-fd6f-b62e-8afd617994c7@treenet.co.nz>
 <131538788.5259356.1510213867661@mail.yahoo.com>
 <e89c27ac-b147-426e-e220-2c566b5e7586@treenet.co.nz>
 <2014705679.5324553.1510227544567@mail.yahoo.com>
 <b0bd560b-081d-ec7b-261b-d92c1d479f98@treenet.co.nz>
 <1915184707.1464260.1511169300258@mail.yahoo.com>
 <133e0298-3080-d4fd-5a18-342b1b8b55c7@treenet.co.nz>
 <1170730150.225886.1511361230924@mail.yahoo.com>
Message-ID: <a98d0d87-3b4b-7387-f207-b3068d3424a5@treenet.co.nz>

On 23/11/17 03:33, Vieri wrote:
> 
> ________________________________
> From: Amos Jeffries <squid3 at treenet.co.nz>
>>
>> If we assume that each request opens a new connection and they are not
>> closed until TCP times out on the socket we do get numbers much more
>> like that 11K+ you are seeing.
>>
>> That implies that ICAP transactions are probably not finishing
> 
>> completely.
> 
> I'll have to look into this asap. Quick question: if I restart c-icap shouldn't I see a drop in open FD numbers if it were c-icap's "fault"?
> 
> I restarted c-icap (stop+start), but the open FDs are the same.
> 

Maybe, but there have been a few bugs where Squid being tricked into 
waiting on something arriving that was never coming left sockets in the 
TCP half-open CLOSE_WAIT state indefinitely.

Amos


From xeron.oskom at gmail.com  Thu Nov 23 01:20:52 2017
From: xeron.oskom at gmail.com (Ivan Larionov)
Date: Wed, 22 Nov 2017 17:20:52 -0800
Subject: [squid-users] forward_max_tries 1 has no effect
Message-ID: <CAHvB88zR7wxUjMGEdrqELaMW3+pQq21hAkJR__4pWXendnmhzQ@mail.gmail.com>

Hello.

We have an issue with squid when it tries to re-forward / retry failed
request even when forward_max_tries is set to 1. The situation when it
happens is when there's no response, parent just closes the connection.

Relevant parts of configuration so you understand the architecture:

cache_peer 127.0.0.1 parent 18070 0 no-query no-digest no-netdb-exchange
name=proxy
never_direct allow all
negative_ttl 0 seconds
forward_max_tries 1
retry_on_error off

The traffic flow from tcpdump is like this:

squid to parent
GET http://HOST/
parent to squid
ACK

waiting (no response for ~40 seconds)

parent to squid
FIN, ACK
squid to parent
FIN, ACK
parent to squid
FIN

Immediately after that:

squid to parent
GET http://HOST/ (again)

Debug logs from ALL,2:

?
http.cc(2229) sendRequest: HTTP Server local=127.0.0.2:46867 remote=
127.0.0.1:18070 FD 26 flags=1
http.cc(2230) sendRequest: HTTP Server REQUEST:
---------
GET http://HOST:12345/ HTTP/1.1
?
http.cc(1299) continueAfterParsingHeader: WARNING: HTTP: Invalid Response:
No object data received for http://HOST:12345/ AKA HOST/
FwdState.cc(655) handleUnregisteredServerEnd: self=0x430a438*2
err=0x445fcf8 http://HOST:12345/
http.cc(2229) sendRequest: HTTP Server local=127.0.0.2:34417 remote=
127.0.0.1:18070 FD 26 flags=1
http.cc(2230) sendRequest: HTTP Server REQUEST:
---------
GET http://HOST:12345/ HTTP/1.1
?

It doesn't happen 100% times. Sometimes squid returns 502 after the 1st
try, sometimes it retries once. Also I haven't seen more than 1 retry.

Could it be a bug? We'd really like to disable these retries.

Squid Cache: Version 3.5.27
Service Name: squid
configure options:  '--program-prefix=' '--prefix=/usr'
'--exec-prefix=/usr' '--bindir=/usr/sbin' '--sbindir=/usr/sbin'
'--sysconfdir=/etc/squid' '--libdir=/usr/lib' '--libexecdir=/usr/lib/squid'
'--includedir=/usr/include' '--datadir=/usr/share'
'--sharedstatedir=/usr/com' '--localstatedir=/var'
'--mandir=/usr/share/man' '--infodir=/usr/share/info' '--enable-epoll'
'--enable-removal-policies=heap,lru' '--enable-storeio=aufs,rock'
'--enable-delay-pools' '--with-pthreads' '--enable-cache-digests'
'--with-large-files' '--with-maxfd=16384' '--enable-htcp'

-- 
With best regards, Ivan Larionov.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20171122/c445001d/attachment.htm>

From squid3 at treenet.co.nz  Thu Nov 23 08:32:39 2017
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 23 Nov 2017 21:32:39 +1300
Subject: [squid-users] forward_max_tries 1 has no effect
In-Reply-To: <CAHvB88zR7wxUjMGEdrqELaMW3+pQq21hAkJR__4pWXendnmhzQ@mail.gmail.com>
References: <CAHvB88zR7wxUjMGEdrqELaMW3+pQq21hAkJR__4pWXendnmhzQ@mail.gmail.com>
Message-ID: <fe6c5513-20c9-4b41-467a-9d199e38f20b@treenet.co.nz>

On 23/11/17 14:20, Ivan Larionov wrote:
> Hello.
> 
> We have an issue with squid when it tries to re-forward / retry failed 
> request even when?forward_max_tries is set to 1. The situation when it 
> happens is when there's no response, parent just closes the connection.
> 
...
> 
> It doesn't happen 100% times. Sometimes squid returns 502 after the 1st 
> try, sometimes it retries once. Also I haven't seen more than 1 retry.
> 

Please enable debug_options 44,2 to see what destinations your Squid is 
actually finding.

max_forward_tries is just a rough cap on the number of server names 
which can be found when generating that list. The actual destinations 
count can exceed it if one or more of the servers happens to have 
multiple IPs to try.

The overall transaction can involve retries if one of the other layers 
(TCP or HTTP) contains retry semantics to a single server.



> Could it be a bug? We'd really like to disable these retries.
> 

Why are trying to break HTTP?
What is the actual problem you are trying to resolve here?


Amos


From squid3 at treenet.co.nz  Thu Nov 23 08:56:30 2017
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 23 Nov 2017 21:56:30 +1300
Subject: [squid-users] Lots of "BUG 3279: HTTP reply without Date:"
 after update to squid-5.0.0-20171117-r4d27d0a
In-Reply-To: <20171122103359.tixi7ewbqtgua5uh@charite.de>
References: <20171122103359.tixi7ewbqtgua5uh@charite.de>
Message-ID: <3267f287-4362-048e-338c-d226098ed38e@treenet.co.nz>

On 22/11/17 23:33, Ralf Hildebrandt wrote:
> I was updating from squid-5.0.0-20171103-re3bf07f
> to                  squid-5.0.0-20171117-rd816577 (and I was also testing squid-5.0.0-20171117-r4d27d0a)
> 
> today and immediately found lots of "BUG 3279: HTTP reply without
> Date:" messages in my log (cache_dir had been cleared upon start,
> meaning I'm starting with a clean slate!):
> 
> 2017/11/22 11:16:02| BUG 3279: HTTP reply without Date:
...
 >
> is this a known bug with the recent snapshot?
> 

Yes, it still happens when a server delivers responses without a Date 
header. Though IIRC it is mostly seen in the cases where an IMS/INM 
conditional response is returning without a Date header (a message state 
which is forbidden in HTTP).
  Taking a wild guess I suspect they are probably occuring when a client 
revalidating its client-side cached content from one of the broken 
servers. Or slightly less likely one of the early responses in cache_mem 
right after the restart has the problem.


The patch joseph referred to synthesizes a fake Date header some time 
far in the past. Thus forcing the content to become 'stale' in the 
cache. That can either led to further refresh conditionals which 
perpetuate the problem, or to purging the object from cache - thus 
causing extra long fetch times for all traffic to that server.

Current Squid should only be logging that message at a relatively high 
debug level (not important anymore) and synthesizing a Date header only 
when the 2xx status is delivered to the client - the object cannot be 
used for revalidation due to the missing header.


Amos


From squid3 at treenet.co.nz  Thu Nov 23 09:03:33 2017
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 23 Nov 2017 22:03:33 +1300
Subject: [squid-users] Lots of "BUG 3279: HTTP reply without Date:"
 after update to squid-5.0.0-20171117-r4d27d0a
In-Reply-To: <1511353514783-0.post@n4.nabble.com>
References: <20171122103359.tixi7ewbqtgua5uh@charite.de>
 <1511353514783-0.post@n4.nabble.com>
Message-ID: <f061a23d-6267-f89f-cc90-520a41ae61a9@treenet.co.nz>

On 23/11/17 01:25, joseph wrote:
> after investigating  bug was exist but hidden all update ar correct
> https://bugs.squid-cache.org/show_bug.cgi?id=3279
> bug 3279  not yet fixed in v5
> hint if they adapt the v3 patch will solve the problem :)
> 
> 

So according to the latest info you posted to that bug report a patch 
which merely alters how logging displays a text string suddenly makes 
inbound traffic to your proxy contain no Date headers.

Something is seriously wrong with your system.

Amos


From chip_pop at hotmail.com  Thu Nov 23 12:44:42 2017
From: chip_pop at hotmail.com (joseph)
Date: Thu, 23 Nov 2017 05:44:42 -0700 (MST)
Subject: [squid-users] Lots of "BUG 3279: HTTP reply without Date:"
 after update to squid-5.0.0-20171117-r4d27d0a
In-Reply-To: <f061a23d-6267-f89f-cc90-520a41ae61a9@treenet.co.nz>
References: <20171122103359.tixi7ewbqtgua5uh@charite.de>
 <1511353514783-0.post@n4.nabble.com>
 <f061a23d-6267-f89f-cc90-520a41ae61a9@treenet.co.nz>
Message-ID: <1511441082949-0.post@n4.nabble.com>

Amos Jeffries wrote
> On 23/11/17 01:25, joseph wrote:
>> after investigating  bug was exist but hidden all update ar correct
>> https://bugs.squid-cache.org/show_bug.cgi?id=3279
>> bug 3279  not yet fixed in v5
>> hint if they adapt the v3 patch will solve the problem :)
>> 
>> 
> 
>>>So according to the latest info you posted to that bug report a patch 
>>>which merely alters how logging displays a text string suddenly makes 
>>>inbound traffic to your proxy contain no Date headers.
> yes 
>>>Something is seriously wrong with your system.
> not only me re read this post  the time i post in bug section i have my
> cache.log  large enough to make me wonder
> and  more ppl i know using latest snap... got those date bug  header in
> log 
> Amos
> _______________________________________________
> squid-users mailing list

> squid-users at .squid-cache

> http://lists.squid-cache.org/listinfo/squid-users





-----
************************** 
***** Crash to the future  ****
**************************
--
Sent from: http://squid-web-proxy-cache.1019090.n4.nabble.com/Squid-Users-f1019091.html


From xeron.oskom at gmail.com  Thu Nov 23 21:03:33 2017
From: xeron.oskom at gmail.com (Ivan Larionov)
Date: Thu, 23 Nov 2017 13:03:33 -0800
Subject: [squid-users] forward_max_tries 1 has no effect
In-Reply-To: <fe6c5513-20c9-4b41-467a-9d199e38f20b@treenet.co.nz>
References: <CAHvB88zR7wxUjMGEdrqELaMW3+pQq21hAkJR__4pWXendnmhzQ@mail.gmail.com>
 <fe6c5513-20c9-4b41-467a-9d199e38f20b@treenet.co.nz>
Message-ID: <7E5CA2C7-96B6-4535-88D2-0176B0D10EC4@gmail.com>

> 
> On Nov 23, 2017, at 12:32 AM, Amos Jeffries <squid3 at treenet.co.nz> wrote:
> 
> On 23/11/17 14:20, Ivan Larionov wrote:
>> Hello.
>> We have an issue with squid when it tries to re-forward / retry failed request even when forward_max_tries is set to 1. The situation when it happens is when there's no response, parent just closes the connection.
> ...
>> It doesn't happen 100% times. Sometimes squid returns 502 after the 1st try, sometimes it retries once. Also I haven't seen more than 1 retry.
> 
> Please enable debug_options 44,2 to see what destinations your Squid is actually finding.

I'll check this on Monday.

> 
> max_forward_tries is just a rough cap on the number of server names which can be found when generating that list. The actual destinations count can exceed it if one or more of the servers happens to have multiple IPs to try.
> 
> The overall transaction can involve retries if one of the other layers (TCP or HTTP) contains retry semantics to a single server.
> 
> 
> 
>> Could it be a bug? We'd really like to disable these retries.
> 
> Why are trying to break HTTP?
> What is the actual problem you are trying to resolve here?
> 

Why do you think I'm trying to break HTTP?

squid forwards the request to parent but parent misbehaves and just closes the connection after 40 seconds. I'm trying to prevent retry of request in such situation. Why squid retries if I never asked him to do it and specifically said "forward_max_tries 1".

And this is not a connection failure, squid successfully establishes the connection and sends the request, parent ACKs it, just never responses back and proactively closes the connection.

We're already fixing parent behavior, but still want to disable retries on squid side.

> 
> Amos
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users



From squid3 at treenet.co.nz  Fri Nov 24 07:43:39 2017
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Fri, 24 Nov 2017 20:43:39 +1300
Subject: [squid-users] forward_max_tries 1 has no effect
In-Reply-To: <7E5CA2C7-96B6-4535-88D2-0176B0D10EC4@gmail.com>
References: <CAHvB88zR7wxUjMGEdrqELaMW3+pQq21hAkJR__4pWXendnmhzQ@mail.gmail.com>
 <fe6c5513-20c9-4b41-467a-9d199e38f20b@treenet.co.nz>
 <7E5CA2C7-96B6-4535-88D2-0176B0D10EC4@gmail.com>
Message-ID: <c88e0e11-bc43-5c21-20e7-bf95dad0b212@treenet.co.nz>


On 24/11/17 10:03, Ivan Larionov wrote:
>>
>> On Nov 23, 2017, at 12:32 AM, Amos Jeffries <squid3 at treenet.co.nz> wrote:
>>
>> On 23/11/17 14:20, Ivan Larionov wrote:
>>> Hello.
>>> We have an issue with squid when it tries to re-forward / retry failed request even when forward_max_tries is set to 1. The situation when it happens is when there's no response, parent just closes the connection.
>> ...
>>> It doesn't happen 100% times. Sometimes squid returns 502 after the 1st try, sometimes it retries once. Also I haven't seen more than 1 retry.
>>
>> Please enable debug_options 44,2 to see what destinations your Squid is actually finding.
> 
> I'll check this on Monday.
> 
>>
>> max_forward_tries is just a rough cap on the number of server names which can be found when generating that list. The actual destinations count can exceed it if one or more of the servers happens to have multiple IPs to try.
>>
>> The overall transaction can involve retries if one of the other layers (TCP or HTTP) contains retry semantics to a single server.
>>
>>
>>
>>> Could it be a bug? We'd really like to disable these retries.
>>
>> Why are trying to break HTTP?
>> What is the actual problem you are trying to resolve here?
>>
> 
> Why do you think I'm trying to break HTTP?
> 
> squid forwards the request to parent but parent misbehaves and just closes the connection after 40 seconds. I'm trying to prevent retry of request in such situation. Why squid retries if I never asked him to do it and specifically said "forward_max_tries 1".
> 
> And this is not a connection failure, squid successfully establishes the connection and sends the request, parent ACKs it, just never responses back and proactively closes the connection.
> 

This is not misbehaviour on the part of either Squid nor the parent.
<https://tools.ietf.org/html/rfc7230#section-6.3.1>
"Connections can be closed at any time, with or without intention."


As has been discussed in other threads recently there are servers out 
there starting to greylist TCP connections, closing the first one some 
time *after* SYN+ACK regardless of what the proxy sends and accepting 
any followup connection attempts.

NP: That can result in exactly the behaviour you describe from the peer 
as Squid does not wait for a FIN to arrive before sending its upstream 
HTTP request - Squid will "randomly" get a FIN or a RST depending on 
whether the FIN or the DATA packet wins the race into the Squid machines 
TCP stack. FIN and RST have different retry properties which might 
explain your "sometimes retries" behaviour.


Also, TCP connections fail quite often for many other reasons anyway. 
Anything from power fluctuations at a router to BGP switching the packet 
route dropping packets. They are most often a short-term situation which 
is resolved by the time the repeat is attempted.

What you are trying to do will result in Squid being unable to cope with 
any of these transitory restrictions from the TCP environment and force 
the client to receive a terminal error page.
  That will greatly slow down detection and recovery from the slightly 
longer-lived TCP issues in Squid itself and may result in N other 
clients also unnecessarily receiving the same error response as bad 
connection attempts gets spread between many clients (all getting 
errors) instead of isolated to the one/few who hit it when the issue 
initially occurs.

Expanding the retries to large numbers (ie the recent default change to 
25), or to low numbers (eg the old default of 5) are reasonable things 
to do depending on the network stability to your upstreams. But going 
all the way to 0 retries is guaranteed to lead to more client visible 
problems than necessary.


All that asside I phrased it as a question because you might have had a 
good reason for increasing the visible failure rates.


> We're already fixing parent behavior, but still want to disable retries on squid side.
> 


Since you describe this as peer misbehaviour, then treating it to Squids 
normal TCP failure recovery is the best behaviour. Retry is the intended 
correct behaviour for a proxy to perform on any non-idempotent requests. 
In your case up to a value of 1 retry before declaring non-temporary 
route failure.

NP: idempotent vs non-idempotent may be another reason behind the 
observed behaviour of retry happening only sometimes.


If you are doing this due to overall latency/delay on the affected 
client traffic you would be better off reducing the timeouts involved 
(cache_peer connect-timeout= parameter AFAICS) than aiming at a retry 
count of 0. Perhapse also requiring a few standby=N persistent 
connections to be maintained if the peer is HTTP/1.1 capable.

Amos


From hoangminhung at gmail.com  Fri Nov 24 13:04:13 2017
From: hoangminhung at gmail.com (=?UTF-8?B?bWluaCBoxrBuZyDEkeG7lyBob8Ogbmc=?=)
Date: Fri, 24 Nov 2017 20:04:13 +0700
Subject: [squid-users] Fwd: [Squid-3.5.20]Squid transparent proxy http/https
 without client site config
In-Reply-To: <CAO0fBEpZvu=GFP0q=XzdzQMxBq629YA901MZFGEQbbLXmB9ZHg@mail.gmail.com>
References: <CAO0fBEpZvu=GFP0q=XzdzQMxBq629YA901MZFGEQbbLXmB9ZHg@mail.gmail.com>
Message-ID: <CAO0fBEp1ni6_RYnX-jnwggnr7StHvB+5-X8a9c3O6Fms-3S0uQ@mail.gmail.com>

Dear Squid-users,
I want to setup a Squid proxy in transparent mode http/https traffic
without any config in Client site.

I use Squid 3.5.20 on Centos7.I just install squid with default feature as *yum
install squid.*

I just do that , but i have some problem with my output logging in
access.log .
Specifically, my access.log only show ip_address_server:443 instead domain
name of destination server like that :


*1511525732.912    206 172.18.18.15 TAG_NONE/200 0
CONNECT 172.217.24.35:443 <http://172.217.24.35:443/> -
ORIGINAL_DST/172.217.24.35 <http://172.217.24.35/> -*

I know that i take some mistake in my squid.conf . But i can't find out how
to fix it. Could you please show me how to improve my squid.conf .

Here is my squid.conf file in attact file

Waiting for your reply.
Thanks alot !

-- 
Thanks & Best Regards,
--------------
?? Ho?ng Minh H?ng
Gmail : hoangminhung at gmail.com
S?T : 01234454115



-- 
Thanks & Best Regards,
--------------
?? Ho?ng Minh H?ng
Gmail : hoangminhung at gmail.com
S?T : 01234454115
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20171124/a549ae40/attachment.htm>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: squid.conf
Type: application/octet-stream
Size: 798 bytes
Desc: not available
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20171124/a549ae40/attachment.obj>

From squid3 at treenet.co.nz  Fri Nov 24 13:27:26 2017
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sat, 25 Nov 2017 02:27:26 +1300
Subject: [squid-users] Fwd: [Squid-3.5.20]Squid transparent proxy
 http/https without client site config
In-Reply-To: <CAO0fBEp1ni6_RYnX-jnwggnr7StHvB+5-X8a9c3O6Fms-3S0uQ@mail.gmail.com>
References: <CAO0fBEpZvu=GFP0q=XzdzQMxBq629YA901MZFGEQbbLXmB9ZHg@mail.gmail.com>
 <CAO0fBEp1ni6_RYnX-jnwggnr7StHvB+5-X8a9c3O6Fms-3S0uQ@mail.gmail.com>
Message-ID: <67f77ee1-3266-4171-b669-39665f9cb72a@treenet.co.nz>

On 25/11/17 02:04, minh h?ng ?? ho?ng wrote:
> 
> 
> Dear Squid-users,
> I want to setup a Squid proxy in transparent mode http/https traffic 
> without any config in Client site.
> 
> I use Squid 3.5.20 on Centos7.I just install squid with default feature 
> as *yum install squid.*
> *
> *
> I just do that , but i have some problem with my output logging in 
> access.log .
> Specifically, my access.log only show ip_address_server:443 instead 
> domain name of destination server like that :
> 
> 
> *1511525732.912? ? 206 172.18.18.15 TAG_NONE/200 0 CONNECT 
> 172.217.24.35:443 - ORIGINAL_DST/172.217.24.35 -*
> *
> *
> I know that i take some mistake in my squid.conf . But i can't find out 
> how to fix it. Could you please show me how to?improve my squid.conf .
> 

You configured "ssl_bump none all".

<https://wiki.squid-cache.org/Features/SslPeekAndSplice#Actions>
"do not use these with Squid-3.5 and newer"


Use this instead:

  acl step1 at_step SslBump1
  ssl_bump peek step1
  ssl_bump splice all


There should be two log entries per HTTPS connection. One before peek 
happens with raw-IP:port details. And a second one after peek which may 
have a _server_ name (*not* domain name) if and only if the client sends 
TLS SNI extension data.

Amos


From alex at dvm.esines.cu  Fri Nov 24 16:08:13 2017
From: alex at dvm.esines.cu (=?UTF-8?Q?Alex_Guti=c3=a9rrez_Mart=c3=adnez?=)
Date: Fri, 24 Nov 2017 11:08:13 -0500
Subject: [squid-users] sqstat log
Message-ID: <02c937a4-7264-447b-808a-82507f466f4f@dvm.esines.cu>

Hello Squid comunity

Is there a way to remove sqstat log from the main log?. The sqstat log 
drives crazy the program used on my company to review squid log. And my 
superiors hate sarg and calamaris.

This is my sqstat config

acl webserver src 172.16.1.7/27
http_access allow manager webserver
http_access allow localhost manager
http_access deny manager

this is a log sample

1511538926.154????? 1 127.0.0.1 TCP_MISS/200 10846 GET 
cache_object://localhost/active_requests - HIER_NONE/- text/plain


Thanks in advance.

-- 
Saludos Cordiales

Lic. Alex Guti?rrez Mart?nez

Tel. +53 7 2710327





From squid3 at treenet.co.nz  Fri Nov 24 16:27:00 2017
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sat, 25 Nov 2017 05:27:00 +1300
Subject: [squid-users] sqstat log
In-Reply-To: <02c937a4-7264-447b-808a-82507f466f4f@dvm.esines.cu>
References: <02c937a4-7264-447b-808a-82507f466f4f@dvm.esines.cu>
Message-ID: <6ed13dbf-429d-554e-4b09-166f2557fb74@treenet.co.nz>

On 25/11/17 05:08, Alex Guti?rrez Mart?nez wrote:
> Hello Squid comunity
> 
> Is there a way to remove sqstat log from the main log?. The sqstat log 
> drives crazy the program used on my company to review squid log. And my 
> superiors hate sarg and calamaris.

What do you mean by "drives crazy" ?
If a log processor cannot handle real log contents it is broken.


> 
> This is my sqstat config
> 
> acl webserver src 172.16.1.7/27
> http_access allow manager webserver
> http_access allow localhost manager
> http_access deny manager
> 

Add this above your other access_log lines:

  access_log none localhost manager


Amos


From jlay at slave-tothe-box.net  Fri Nov 24 19:30:41 2017
From: jlay at slave-tothe-box.net (James Lay)
Date: Fri, 24 Nov 2017 12:30:41 -0700
Subject: [squid-users] Working peek/splice no longer functioning on some
	sites
Message-ID: <1511551841.2392.5.camel@slave-tothe-box.net>

Topic says it...this setup has been working well for a long time, but
now there are some sites that are failing the TLS handshake. ?Here's my
setup:

acl localnet src 192.168.1.0/24
acl SSL_ports port 443
acl Safe_ports port 80
acl Safe_ports port 443
acl CONNECT method CONNECT
acl allowed_http_sites url_regex "/opt/etc/squid/http_url.txt"

http_access deny !Safe_ports
http_access deny CONNECT !SSL_Ports
http_access allow SSL_ports
http_access allow allowed_http_sites
http_access deny all


ssl_bump peek all
acl allowed_https_sites ssl::server_name_regex
"/opt/etc/squid/http_url.txt"
ssl_bump splice allowed_https_sites
ssl_bump terminate all

sslproxy_cert_error allow all
sslproxy_capath /etc/ssl/certs
sslproxy_flags DONT_VERIFY_PEER?
#sslproxy_options ALL

sslcrtd_program /opt/libexec/ssl_crtd -s /opt/var/ssl_db -M 4MB
sslcrtd_children 5

http_port 3128 intercept
https_port 3129 intercept ssl-bump
cert=/opt/etc/squid/certs/sslsplit_ca_cert.pem
cafile=/opt/etc/squid/certs/sslsplit_ca_cert.pem
key=/opt/etc/squid/certs/sslsplit_ca_key.pem??generate-host-
certificates=on dynamic_cert_mem_cache_size=4MB
sslflags=NO_SESSION_REUSE


logformat mine %>a %[ui %[un [%tl] "%rm %ru HTTP/%rv" %ssl::>sni
%ssl::>cert_subject %>Hs %<st %Ss:%Sh?

access_log syslog:daemon.info mine?

refresh_pattern -i (cgi-bin|\?)	0	0%	0
refresh_pattern .		0	20%	4320

coredump_dir /opt/var?

For example, the file http_url.txt contains:

account\.elderscrollsonline\.com
\.elderscrollsonline\.com
elderscrollsonline\.com


After doing some reading it looks like this is http2 traffic: ?https://
wiki.squid-cache.org/Features/HTTP2.

Is there anything I can do to continue using squid with more and more
sites using http2? ?Pcap enclosed..thank you.

James
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20171124/661bddfc/attachment.htm>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: error.pcap
Type: application/vnd.tcpdump.pcap
Size: 1095 bytes
Desc: not available
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20171124/661bddfc/attachment.pcap>

From jlay at slave-tothe-box.net  Fri Nov 24 19:32:21 2017
From: jlay at slave-tothe-box.net (James Lay)
Date: Fri, 24 Nov 2017 12:32:21 -0700
Subject: [squid-users] Working peek/splice no longer functioning on some
 sites
In-Reply-To: <1511551841.2392.5.camel@slave-tothe-box.net>
References: <1511551841.2392.5.camel@slave-tothe-box.net>
Message-ID: <1511551941.2392.6.camel@slave-tothe-box.net>

I should add this is squid-3.5.27. ?Thank you.
On Fri, 2017-11-24 at 12:30 -0700, James wrote:
> Topic says it...this setup has been working well for a long time, but
> now there are some sites that are failing the TLS handshake. ?Here's
> my setup:
> 
> acl localnet src 192.168.1.0/24
> acl SSL_ports port 443
> acl Safe_ports port 80
> acl Safe_ports port 443
> acl CONNECT method CONNECT
> acl allowed_http_sites url_regex "/opt/etc/squid/http_url.txt"
> 
> http_access deny !Safe_ports
> http_access deny CONNECT !SSL_Ports
> http_access allow SSL_ports
> http_access allow allowed_http_sites
> http_access deny all
> 
> 
> ssl_bump peek all
> acl allowed_https_sites ssl::server_name_regex
> "/opt/etc/squid/http_url.txt"
> ssl_bump splice allowed_https_sites
> ssl_bump terminate all
> 
> sslproxy_cert_error allow all
> sslproxy_capath /etc/ssl/certs
> sslproxy_flags DONT_VERIFY_PEER?
> #sslproxy_options ALL
> 
> sslcrtd_program /opt/libexec/ssl_crtd -s /opt/var/ssl_db -M 4MB
> sslcrtd_children 5
> 
> http_port 3128 intercept
> https_port 3129 intercept ssl-bump
> cert=/opt/etc/squid/certs/sslsplit_ca_cert.pem
> cafile=/opt/etc/squid/certs/sslsplit_ca_cert.pem
> key=/opt/etc/squid/certs/sslsplit_ca_key.pem??generate-host-
> certificates=on dynamic_cert_mem_cache_size=4MB
> sslflags=NO_SESSION_REUSE
> 
> 
> logformat mine %>a %[ui %[un [%tl] "%rm %ru HTTP/%rv" %ssl::>sni
> %ssl::>cert_subject %>Hs %
> 
> access_log syslog:daemon.info mine?
> 
> refresh_pattern -i (cgi-bin|\?)	0	0%	0
> refresh_pattern .		0	20%	4320
> 
> coredump_dir /opt/var?
> 
> For example, the file http_url.txt contains:
> 
> account\.elderscrollsonline\.com
> \.elderscrollsonline\.com
> elderscrollsonline\.com
> 
> 
> After doing some reading it looks like this is http2 traffic: ?https:
> //wiki.squid-cache.org/Features/HTTP2.
> 
> Is there anything I can do to continue using squid with more and more
> sites using http2? ?Pcap enclosed..thank you.
> 
> James
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20171124/b1b7facb/attachment.htm>

From hoangminhung at gmail.com  Sat Nov 25 06:40:25 2017
From: hoangminhung at gmail.com (=?UTF-8?B?bWluaCBoxrBuZyDEkeG7lyBob8Ogbmc=?=)
Date: Sat, 25 Nov 2017 13:40:25 +0700
Subject: [squid-users] Fwd: [Squid-3.5.20]Squid transparent proxy
 http/https without client site config
In-Reply-To: <67f77ee1-3266-4171-b669-39665f9cb72a@treenet.co.nz>
References: <CAO0fBEpZvu=GFP0q=XzdzQMxBq629YA901MZFGEQbbLXmB9ZHg@mail.gmail.com>
 <CAO0fBEp1ni6_RYnX-jnwggnr7StHvB+5-X8a9c3O6Fms-3S0uQ@mail.gmail.com>
 <67f77ee1-3266-4171-b669-39665f9cb72a@treenet.co.nz>
Message-ID: <CAO0fBEr+8=Ehdgb6nzXS2Rzrm-fADbwh-Z+yE7qirdgU-8A8eQ@mail.gmail.com>

Dear Amos, thank you so much for your quickly reply .
I have tried to replace my SSL config with your suggestion. But my squid
get a error like this in cache.log:

2017/11/25 13:21:49 kid1| SECURITY ALERT: Host header forgery detected on
local=216.58.199.110:443 remote=172.18.18.15:55704 FD 13 flags=33 (local IP
does not match any domain IP)
2017/11/25 13:21:49 kid1| SECURITY ALERT: on URL: apis.google.com:443
2017/11/25 13:21:49 kid1| SECURITY ALERT: Host header forgery detected on
local=172.217.25.3:443 remote=172.18.18.15:55705 FD 17 flags=33 (local IP
does not match any domain IP)
2017/11/25 13:21:49 kid1| SECURITY ALERT: on URL: www.google.com.vn:443
2017/11/25 13:21:53 kid1| SECURITY ALERT: Host header forgery detected on
local=157.240.13.35:443 remote=172.18.18.15:55720 FD 22 flags=33 (local IP
does not match any domain IP)
2017/11/25 13:21:53 kid1| SECURITY ALERT: on URL: www.facebook.com:443
2017/11/25 13:21:54 kid1| SECURITY ALERT: Host header forgery detected on
local=157.240.13.35:443 remote=172.18.18.15:55724 FD 22 flags=33 (local IP
does not match any domain IP)
2017/11/25 13:21:54 kid1| SECURITY ALERT: on URL: www.facebook.com:443

So i can't access www.facebook.com. It's error on my browser :
*ERR_SSL_PROTOCOL_ERROR*

I find out the same issue in this discussion :
http://lists.squid-cache.org/pipermail/squid-users/2016-June/011014.html

And then i try to make my squid becomes a cache DNS itself using Unbound.
But look like it does'nt work . I get same error before install cache DNS.
Here is my DNS test on my Squid:

[root at localhost ~]# nslookup
> google.com
Server: 127.0.0.1
Address: 127.0.0.1#53

Non-authoritative answer:
Name: google.com
Address: 216.58.203.46

And this is my dns config in squid.config :

# --------- DNS AND IP CACHES [4341]

dns_nameservers 127.0.0.1
dns_v4_first on
#original_dst off
client_dst_passthru off
host_verify_strict off
ignore_unknown_nameservers off
dns_timeout 120 seconds
ipcache_size 1024
ipcache_low 90
ipcache_high 95
fqdncache_size 1024
positive_dns_ttl 6 hours
negative_dns_ttl 300 seconds

Could you help me please :(

2017-11-24 20:27 GMT+07:00 Amos Jeffries <squid3 at treenet.co.nz>:

> On 25/11/17 02:04, minh h?ng ?? ho?ng wrote:
>
>>
>>
>> Dear Squid-users,
>> I want to setup a Squid proxy in transparent mode http/https traffic
>> without any config in Client site.
>>
>> I use Squid 3.5.20 on Centos7.I just install squid with default feature
>> as *yum install squid.*
>> *
>> *
>> I just do that , but i have some problem with my output logging in
>> access.log .
>> Specifically, my access.log only show ip_address_server:443 instead
>> domain name of destination server like that :
>>
>>
>> *1511525732.912    206 172.18.18.15 TAG_NONE/200 0 CONNECT
>> 172.217.24.35:443 - ORIGINAL_DST/172.217.24.35 -*
>> *
>> *
>> I know that i take some mistake in my squid.conf . But i can't find out
>> how to fix it. Could you please show me how to improve my squid.conf .
>>
>>
> You configured "ssl_bump none all".
>
> <https://wiki.squid-cache.org/Features/SslPeekAndSplice#Actions>
> "do not use these with Squid-3.5 and newer"
>
>
> Use this instead:
>
>  acl step1 at_step SslBump1
>  ssl_bump peek step1
>  ssl_bump splice all
>
>
> There should be two log entries per HTTPS connection. One before peek
> happens with raw-IP:port details. And a second one after peek which may
> have a _server_ name (*not* domain name) if and only if the client sends
> TLS SNI extension data.
>
> Amos
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>



-- 
Thanks & Best Regards,
--------------
?? Ho?ng Minh H?ng
Gmail : hoangminhung at gmail.com
S?T : 01234454115
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20171125/9d08651a/attachment.htm>

From squid3 at treenet.co.nz  Sat Nov 25 10:48:59 2017
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sat, 25 Nov 2017 23:48:59 +1300
Subject: [squid-users] Working peek/splice no longer functioning on some
 sites
In-Reply-To: <1511551841.2392.5.camel@slave-tothe-box.net>
References: <1511551841.2392.5.camel@slave-tothe-box.net>
Message-ID: <2ea83011-92bc-f134-e662-2f1fd34fce89@treenet.co.nz>

On 25/11/17 08:30, James Lay wrote:
> Topic says it...this setup has been working well for a long time, but 
> now there are some sites that are failing the TLS handshake. ?Here's my 
> setup:
> 
> acl localnet src 192.168.1.0/24
> acl SSL_ports port 443
> acl Safe_ports port 80
> acl Safe_ports port 443
> acl CONNECT method CONNECT
> acl allowed_http_sites url_regex "/opt/etc/squid/http_url.txt"
> 
> http_access deny !Safe_ports
> http_access deny CONNECT !SSL_Ports
> http_access allow SSL_ports
> http_access allow allowed_http_sites
> http_access deny all
> 
> 
> ssl_bump peek all
> acl allowed_https_sites ssl::server_name_regex "/opt/etc/squid/http_url.txt"
> ssl_bump splice allowed_https_sites
> ssl_bump terminate all


Because you have "peek all" being performed the transaction MUST pass 
your regex patterns with both TLS SNI from the client *and* the server 
certificate SubjectName values. Either one not matching will perform 
that "terminate all" on the TLS handshake.


> 
> sslproxy_cert_error allow all
> sslproxy_capath /etc/ssl/certs
> sslproxy_flags DONT_VERIFY_PEER
> #sslproxy_options ALL


Also, please remove these "*_error allow all" and DONT_VERIFY_PEER lines 
from your config. They are actively harmful.

> 
> sslcrtd_program /opt/libexec/ssl_crtd -s /opt/var/ssl_db -M 4MB
> sslcrtd_children 5
> 
> http_port 3128 intercept
> https_port 3129 intercept ssl-bump 
> cert=/opt/etc/squid/certs/sslsplit_ca_cert.pem 
> cafile=/opt/etc/squid/certs/sslsplit_ca_cert.pem 
> key=/opt/etc/squid/certs/sslsplit_ca_key.pem 

NP: when cert= and key= are in the same file you do not need to specify 
key=.

> generate-host-certificates=on 
> dynamic_cert_mem_cache_size=4MB sslflags=NO_SESSION_REUSE
> 

It is also best to add "sslflags=NO_DEFAULT_CA" to these ports for 
Squid-3. That will save a lot of useless memory overheads.


> 
> logformat mine %>a %[ui %[un [%tl] "%rm %ru HTTP/%rv" %ssl::>sni 
> %ssl::>cert_subject %>Hs %<st %Ss:%Sh
> 
...
> For example, the file http_url.txt contains:
> 
> account\.elderscrollsonline\.com
> \.elderscrollsonline\.com
> elderscrollsonline\.com
> 
> 
> After doing some reading it looks like this is http2 traffic: 
> https://wiki.squid-cache.org/Features/HTTP2.
> 

There is no sign of HTTP/2 in that PCAP trace. There is SPDY/3 and 
HTTP/1.1 being offered by the client.


If that is from the client to Squid, then please check the matching 
Squid->server for what is going on there.



If the problem remains please try Squid-4. It has more advanced TLS 
capabilities than Squid-3.

Amos


From squid3 at treenet.co.nz  Sat Nov 25 11:17:24 2017
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sun, 26 Nov 2017 00:17:24 +1300
Subject: [squid-users] Fwd: [Squid-3.5.20]Squid transparent proxy
 http/https without client site config
In-Reply-To: <CAO0fBEr+8=Ehdgb6nzXS2Rzrm-fADbwh-Z+yE7qirdgU-8A8eQ@mail.gmail.com>
References: <CAO0fBEpZvu=GFP0q=XzdzQMxBq629YA901MZFGEQbbLXmB9ZHg@mail.gmail.com>
 <CAO0fBEp1ni6_RYnX-jnwggnr7StHvB+5-X8a9c3O6Fms-3S0uQ@mail.gmail.com>
 <67f77ee1-3266-4171-b669-39665f9cb72a@treenet.co.nz>
 <CAO0fBEr+8=Ehdgb6nzXS2Rzrm-fADbwh-Z+yE7qirdgU-8A8eQ@mail.gmail.com>
Message-ID: <ead14327-92a2-c89e-1782-6c1b489216d2@treenet.co.nz>

On 25/11/17 19:40, minh h?ng ?? ho?ng wrote:
> Dear Amos, thank you so much for your quickly reply .
> I have tried to replace my SSL config with your suggestion. But my squid 
> get a error like this in cache.log:
> 
> 2017/11/25 13:21:49 kid1| SECURITY ALERT: Host header forgery detected 
> on local=216.58.199.110:443 
> remote=172.18.18.15:55704 FD 13 flags=33 
> (local IP does not match any domain IP)

...
> 
> So i can't access www.facebook.com. It's error 
> on my browser : *ERR_SSL_PROTOCOL_ERROR*
> *


> *
> I find out the same issue in this discussion : 
> http://lists.squid-cache.org/pipermail/squid-users/2016-June/011014.html
> 

The more complete info about that problem, the things to avoid, and the 
workarounds that help reduce it can be found at 
<https://wiki.squid-cache.org/KnowledgeBase/HostHeaderForgery>

Be aware that there is no full solution yet. The latest Squid-4 and 
Squid-5 functionality is getting closer to coping with these services, 
but still not complete.


> And then i try to make my squid becomes a cache DNS itself using 
> Unbound. But look like it does'nt work . I get same error before install 
> cache DNS.

Not just the Squid machine but *all* the clients going through your 
Squid also have to be using the same DNS resolver for that workaround. 
Any of them using other resolvers (eg 8.8.8.8 or similar services) 
*will* hit these errors.


> Here is my DNS test on my Squid:
> 
> [root at localhost ~]# nslookup google.com 
> Server:127.0.0.1
> Address:127.0.0.1#53
> 
> Non-authoritative answer:
> Name:google.com
> Address: 216.58.203.46
> 

"google.com" is not your problem. The domain names in the log are:

  apis.google.com    != 216.58.199.110
  www.google.com.vn  != 172.217.25.3
  www.facebook.com   != 157.240.13.35

Also, be aware that the problem is extremely temporary. It can change 
between failed and working in any random millisecond. So testing even a 
few seconds later often shows different results.


> And this is my dns config in squid.config :
> 
> # --------- DNS AND IP CACHES [4341]
> 
> dns_nameservers 127.0.0.1
> dns_v4_first on
> #original_dst off
> client_dst_passthru off

The above setting is rejecting clients when the host verify fails.
TO let traffic through the proxy when host-verify fails set it back to 
the default "client_dst_passthru on".

The Host verify failure is most dangerous when cached - so that is 
always prohibited. But upstream routing is difficult for Squid to 
determine - thus that config option. It is left up to you whether you 
risk your clients getting infected by that mechanism - Squid just 
minimizes the damage and risk by limiting it to the one client making 
the suspicious request.

Amos


From jlay at slave-tothe-box.net  Sat Nov 25 11:52:29 2017
From: jlay at slave-tothe-box.net (James Lay)
Date: Sat, 25 Nov 2017 04:52:29 -0700
Subject: [squid-users] Working peek/splice no longer functioning on some
 sites
In-Reply-To: <2ea83011-92bc-f134-e662-2f1fd34fce89@treenet.co.nz>
References: <1511551841.2392.5.camel@slave-tothe-box.net>
 <2ea83011-92bc-f134-e662-2f1fd34fce89@treenet.co.nz>
Message-ID: <1511610749.2338.1.camel@slave-tothe-box.net>

On Sat, 2017-11-25 at 23:48 +1300, Amos Jeffries wrote:
> On 25/11/17 08:30, James Lay wrote:
> > 
> > Topic says it...this setup has been working well for a long time,
> > but?
> > now there are some sites that are failing the TLS handshake.
> > ?Here's my?
> > setup:
> > 
> > acl localnet src 192.168.1.0/24
> > acl SSL_ports port 443
> > acl Safe_ports port 80
> > acl Safe_ports port 443
> > acl CONNECT method CONNECT
> > acl allowed_http_sites url_regex "/opt/etc/squid/http_url.txt"
> > 
> > http_access deny !Safe_ports
> > http_access deny CONNECT !SSL_Ports
> > http_access allow SSL_ports
> > http_access allow allowed_http_sites
> > http_access deny all
> > 
> > 
> > ssl_bump peek all
> > acl allowed_https_sites ssl::server_name_regex
> > "/opt/etc/squid/http_url.txt"
> > ssl_bump splice allowed_https_sites
> > ssl_bump terminate all
> 
> Because you have "peek all" being performed the transaction MUST
> pass?
> your regex patterns with both TLS SNI from the client *and* the
> server?
> certificate SubjectName values. Either one not matching will perform?
> that "terminate all" on the TLS handshake.
> 
Thanks Amos...do you have a suggestion for changing this to match one
or the other instead of both?
James
> > 
> > 
> > sslproxy_cert_error allow all
> > sslproxy_capath /etc/ssl/certs
> > sslproxy_flags DONT_VERIFY_PEER
> > #sslproxy_options ALL
> > 

> 
> 
> 
> Also, please remove these "*_error allow all" and DONT_VERIFY_PEER lines 
> from your config. They are actively harmful.
> 
> 
> > 
> > 
> > sslcrtd_program /opt/libexec/ssl_crtd -s /opt/var/ssl_db -M 4MB
> > sslcrtd_children 5
> > 
> > http_port 3128 intercept
> > https_port 3129 intercept ssl-bump 
> > cert=/opt/etc/squid/certs/sslsplit_ca_cert.pem 
> > cafile=/opt/etc/squid/certs/sslsplit_ca_cert.pem 
> > key=/opt/etc/squid/certs/sslsplit_ca_key.pem 
> > 

> 
> 
> NP: when cert= and key= are in the same file you do not need to specify 
> key=.
> 
> 
> > 
> > generate-host-certificates=on 
> > dynamic_cert_mem_cache_size=4MB sslflags=NO_SESSION_REUSE
> > 
> > 

> 
> 
> It is also best to add "sslflags=NO_DEFAULT_CA" to these ports for 
> Squid-3. That will save a lot of useless memory overheads.
> 
> 
> 
> > 
> > 
> > logformat mine %>a %[ui %[un [%tl] "%rm %ru HTTP/%rv" %ssl::>sni 
> > %ssl::>cert_subject %>Hs %<st %Ss:%Sh
> > 
> > 

> 
> ...
> 
> > 
> > For example, the file http_url.txt contains:
> > 
> > account\.elderscrollsonline\.com
> > \.elderscrollsonline\.com
> > elderscrollsonline\.com
> > 
> > 
> > After doing some reading it looks like this is http2 traffic: 
> > 
https://wiki.squid-cache.org/Features/HTTP2.
> > .
> > 
> > 

> 
> 
> There is no sign of HTTP/2 in that PCAP trace. There is SPDY/3 and 
> HTTP/1.1 being offered by the client.
> 
> 
> If that is from the client to Squid, then please check the matching 
> Squid->server for what is going on there.
> 
> 
> 
> If the problem remains please try Squid-4. It has more advanced TLS 
> capabilities than Squid-3.
> 
> Amos
> _______________________________________________
> squid-users mailing list
> 
squid-users at lists.squid-cache.org> 
http://lists.squid-cache.org/listinfo/squid-users> 
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20171125/68cd331e/attachment.htm>

From squid3 at treenet.co.nz  Sat Nov 25 12:33:36 2017
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sun, 26 Nov 2017 01:33:36 +1300
Subject: [squid-users] Working peek/splice no longer functioning on some
 sites
In-Reply-To: <1511610749.2338.1.camel@slave-tothe-box.net>
References: <1511551841.2392.5.camel@slave-tothe-box.net>
 <2ea83011-92bc-f134-e662-2f1fd34fce89@treenet.co.nz>
 <1511610749.2338.1.camel@slave-tothe-box.net>
Message-ID: <57c8c167-dfae-6579-8f75-c65ae3160c0e@treenet.co.nz>

On 26/11/17 00:52, James Lay wrote:
> On Sat, 2017-11-25 at 23:48 +1300, Amos Jeffries wrote:
>> On 25/11/17 08:30, James Lay wrote:
>>> Topic says it...this setup has been working well for a long time, but 
>>> now there are some sites that are failing the TLS handshake. ?Here's 
>>> my setup: acl localnet src 192.168.1.0/24 acl SSL_ports port 443 acl 
>>> Safe_ports port 80 acl Safe_ports port 443 acl CONNECT method CONNECT 
>>> acl allowed_http_sites url_regex "/opt/etc/squid/http_url.txt" 
>>> http_access deny !Safe_ports http_access deny CONNECT !SSL_Ports 
>>> http_access allow SSL_ports http_access allow allowed_http_sites 
>>> http_access deny all ssl_bump peek all acl allowed_https_sites 
>>> ssl::server_name_regex "/opt/etc/squid/http_url.txt" ssl_bump splice 
>>> allowed_https_sites ssl_bump terminate all 
>>
>>
>>
>> Because you have "peek all" being performed the transaction MUST pass
>> your regex patterns with both TLS SNI from the client *and* the server
>> certificate SubjectName values. Either one not matching will perform
>> that "terminate all" on the TLS handshake.
>>
> 
> Thanks Amos...do you have a suggestion for changing this to match one or 
> the other instead of both?

Doing the splice check before the peek should do that. First one of the 
server_names data sources to match will then splice and non-matches fall 
through to either peek or terminate if no more peeking possible.

Amos


From jlay at slave-tothe-box.net  Sat Nov 25 12:45:59 2017
From: jlay at slave-tothe-box.net (James Lay)
Date: Sat, 25 Nov 2017 05:45:59 -0700
Subject: [squid-users] Working peek/splice no longer functioning on some
 sites
In-Reply-To: <57c8c167-dfae-6579-8f75-c65ae3160c0e@treenet.co.nz>
References: <1511551841.2392.5.camel@slave-tothe-box.net>
 <2ea83011-92bc-f134-e662-2f1fd34fce89@treenet.co.nz>
 <1511610749.2338.1.camel@slave-tothe-box.net>
 <57c8c167-dfae-6579-8f75-c65ae3160c0e@treenet.co.nz>
Message-ID: <1511613959.2338.6.camel@slave-tothe-box.net>

On Sun, 2017-11-26 at 01:33 +1300, Amos Jeffries wrote:
> On 26/11/17 00:52, James Lay wrote:
> > 
> > On Sat, 2017-11-25 at 23:48 +1300, Amos Jeffries wrote:
> > > 
> > > On 25/11/17 08:30, James Lay wrote:
> > > > 
> > > > Topic says it...this setup has been working well for a long
> > > > time, but?
> > > > now there are some sites that are failing the TLS handshake.
> > > > ?Here's?
> > > > my setup: acl localnet src 192.168.1.0/24 acl SSL_ports port
> > > > 443 acl?
> > > > Safe_ports port 80 acl Safe_ports port 443 acl CONNECT method
> > > > CONNECT?
> > > > acl allowed_http_sites url_regex "/opt/etc/squid/http_url.txt"?
> > > > http_access deny !Safe_ports http_access deny CONNECT
> > > > !SSL_Ports?
> > > > http_access allow SSL_ports http_access allow
> > > > allowed_http_sites?
> > > > http_access deny all ssl_bump peek all acl allowed_https_sites?
> > > > ssl::server_name_regex "/opt/etc/squid/http_url.txt" ssl_bump
> > > > splice?
> > > > allowed_https_sites ssl_bump terminate all?
> > > 
> > > 
> > > Because you have "peek all" being performed the transaction MUST
> > > pass
> > > your regex patterns with both TLS SNI from the client *and* the
> > > server
> > > certificate SubjectName values. Either one not matching will
> > > perform
> > > that "terminate all" on the TLS handshake.
> > > 
> > Thanks Amos...do you have a suggestion for changing this to match
> > one or?
> > the other instead of both?
> Doing the splice check before the peek should do that. First one of
> the?
> server_names data sources to match will then splice and non-matches
> fall?
> through to either peek or terminate if no more peeking possible.
> 
> Amos
Perfect..I've modded my lines with:
acl broken_https_sites ssl::server_name_regex
"/opt/etc/squid/broken_url.txt"
ssl_bump splice broken_https_sites
ssl_bump peek all
acl allowed_https_sites ssl::server_name_regex
"/opt/etc/squid/http_url.txt"
ssl_bump splice allowed_https_sites
ssl_bump terminate all

Hopefully that fixes these up. ?Another site besides the the one this
thread is fbcdn.net. ?Again, these DID work, but something within the
last month has changed...guessing Facebook and Elder Scrolls Online
have added additional TLS security. ?Thanks as always Amos.
James
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20171125/a9650c38/attachment.htm>

From rightkicktech at gmail.com  Sun Nov 26 07:50:23 2017
From: rightkicktech at gmail.com (Alex K)
Date: Sun, 26 Nov 2017 09:50:23 +0200
Subject: [squid-users] Working peek/splice no longer functioning on some
	sites
In-Reply-To: <1511613959.2338.6.camel@slave-tothe-box.net>
References: <1511551841.2392.5.camel@slave-tothe-box.net>
 <2ea83011-92bc-f134-e662-2f1fd34fce89@treenet.co.nz>
 <1511610749.2338.1.camel@slave-tothe-box.net>
 <57c8c167-dfae-6579-8f75-c65ae3160c0e@treenet.co.nz>
 <1511613959.2338.6.camel@slave-tothe-box.net>
Message-ID: <CABMULtJZUh+i+QSNOYP6tPhS6Y57d+WVQPidCMyNQ7b6GR-UzQ@mail.gmail.com>

Perhaps an alternative is to peek only on step1:

acl step1 at_step SslBump1

ssl_bump peek step1
acl allowed_https_sites ssl::server_name_regex "/opt/etc/squid/http_url.txt"
ssl_bump splice allowed_https_sites
ssl_bump terminate all

On Nov 25, 2017 14:46, "James Lay" <jlay at slave-tothe-box.net> wrote:

> On Sun, 2017-11-26 at 01:33 +1300, Amos Jeffries wrote:
>
> On 26/11/17 00:52, James Lay wrote:
>
>
> On Sat, 2017-11-25 at 23:48 +1300, Amos Jeffries wrote:
>
>
> On 25/11/17 08:30, James Lay wrote:
>
>
> Topic says it...this setup has been working well for a long time, but
> now there are some sites that are failing the TLS handshake.  Here's
> my setup: acl localnet src 192.168.1.0/24 acl SSL_ports port 443 acl
> Safe_ports port 80 acl Safe_ports port 443 acl CONNECT method CONNECT
> acl allowed_http_sites url_regex "/opt/etc/squid/http_url.txt"
> http_access deny !Safe_ports http_access deny CONNECT !SSL_Ports
> http_access allow SSL_ports http_access allow allowed_http_sites
> http_access deny all ssl_bump peek all acl allowed_https_sites
> ssl::server_name_regex "/opt/etc/squid/http_url.txt" ssl_bump splice
> allowed_https_sites ssl_bump terminate all
>
>
>
>
>
> Because you have "peek all" being performed the transaction MUST pass
> your regex patterns with both TLS SNI from the client *and* the server
> certificate SubjectName values. Either one not matching will perform
> that "terminate all" on the TLS handshake.
>
>
>
>
> Thanks Amos...do you have a suggestion for changing this to match one or
> the other instead of both?
>
>
>
> Doing the splice check before the peek should do that. First one of the
> server_names data sources to match will then splice and non-matches fall
> through to either peek or terminate if no more peeking possible.
>
> Amos
>
>
> Perfect..I've modded my lines with:
>
> acl broken_https_sites ssl::server_name_regex "/opt/etc/squid/broken_url.
> txt"
> ssl_bump splice broken_https_sites
> ssl_bump peek all
> acl allowed_https_sites ssl::server_name_regex
> "/opt/etc/squid/http_url.txt"
> ssl_bump splice allowed_https_sites
> ssl_bump terminate all
>
> Hopefully that fixes these up.  Another site besides the the one this
> thread is fbcdn.net.  Again, these DID work, but something within the
> last month has changed...guessing Facebook and Elder Scrolls Online have
> added additional TLS security.  Thanks as always Amos.
>
> James
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20171126/5b3e42a1/attachment.htm>

From hoangminhung at gmail.com  Mon Nov 27 02:11:46 2017
From: hoangminhung at gmail.com (=?UTF-8?B?bWluaCBoxrBuZyDEkeG7lyBob8Ogbmc=?=)
Date: Mon, 27 Nov 2017 09:11:46 +0700
Subject: [squid-users] Fwd: [Squid-3.5.20]Squid transparent proxy
 http/https without client site config
In-Reply-To: <ead14327-92a2-c89e-1782-6c1b489216d2@treenet.co.nz>
References: <CAO0fBEpZvu=GFP0q=XzdzQMxBq629YA901MZFGEQbbLXmB9ZHg@mail.gmail.com>
 <CAO0fBEp1ni6_RYnX-jnwggnr7StHvB+5-X8a9c3O6Fms-3S0uQ@mail.gmail.com>
 <67f77ee1-3266-4171-b669-39665f9cb72a@treenet.co.nz>
 <CAO0fBEr+8=Ehdgb6nzXS2Rzrm-fADbwh-Z+yE7qirdgU-8A8eQ@mail.gmail.com>
 <ead14327-92a2-c89e-1782-6c1b489216d2@treenet.co.nz>
Message-ID: <CAO0fBEpUoORHnYBkHh8jcAOByfpT8vpw628=TW2t94xAWTDMKA@mail.gmail.com>

>
> Not just the Squid machine but *all* the clients going through your Squid
>> also have to be using the same DNS resolver for that workaround. Any of
>> them using other resolvers (eg 8.8.8.8 or similar services) *will* hit
>> these errors.
>>
>
>
> And this is my dns config in squid.config :
>>
>> # --------- DNS AND IP CACHES [4341]
>>
>> dns_nameservers 127.0.0.1
>> dns_v4_first on
>> #original_dst off
>> client_dst_passthru off
>>
>
> The above setting is rejecting clients when the host verify fails.
> TO let traffic through the proxy when host-verify fails set it back to the
> default "client_dst_passthru on".
>
> The Host verify failure is most dangerous when cached - so that is always
> prohibited. But upstream routing is difficult for Squid to determine - thus
> that config option. It is left up to you whether you risk your clients
> getting infected by that mechanism - Squid just minimizes the damage and
> risk by limiting it to the one client making the suspicious request.
>
>
Thanks alot for your suggestion, i thought that i made some mistake in my
DNS. I will try to find out and show you the result.
-- 
Thanks & Best Regards,
--------------
?? Ho?ng Minh H?ng
Gmail : hoangminhung at gmail.com
S?T : 01234454115
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20171127/dd397839/attachment.htm>

From Martin.Stegner at coburg.de  Mon Nov 27 08:20:22 2017
From: Martin.Stegner at coburg.de (Stegner, Martin)
Date: Mon, 27 Nov 2017 08:20:22 +0000
Subject: [squid-users] filtering HTTPS sites with transparent child Squid
Message-ID: <f85b7fcf2fc74d279689804993b41ee3@coburg.de>

Hi everyone,

I've set up a Squid as a transparent child-proxy. Every request is redirected to another Squid with the content filtering add-on e2guardian. I encounter the problem that the transparent child Squid only forwards IP-Addresses to the e2guardian when HTTPS is used and so e2guardian cant filter anything because it can only filter by URL.

Here are some parts of the config:

http_port 3130
http_port 3128 intercept
https_port 3129 intercept ssl-bump cert=/etc/squid/cert/squid.pem

ssl_bump splice all          (if I use any other option than splice nothing works for some reason)

cache_peer 172.16.0.252 parent 8080 0 default no-query no-digest

Is there any possibility that the transparent child Squid forwards the URL tot he main Squid proxy?

Thanks everyone
Martin

_______________________________________________

Stadt Coburg
Amt f?r Informations- und Kommunikationstechnik
Abteilungsleiter Systemadministration Schulen
Uferstra?e 7, 96450 Coburg
Tel. 09561-89 1166
Fax 09561-89 61166
E-Mail: Martin.Stegner at coburg.de<mailto:Martin.Stegner at coburg.de>
http://www.coburg.de<http://www.coburg.de/>
http://schulen.coburg.de<http://schulen.coburg.de/>

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20171127/acd2838c/attachment.htm>

From squid3 at treenet.co.nz  Mon Nov 27 10:30:08 2017
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Mon, 27 Nov 2017 23:30:08 +1300
Subject: [squid-users] filtering HTTPS sites with transparent child Squid
In-Reply-To: <f85b7fcf2fc74d279689804993b41ee3@coburg.de>
References: <f85b7fcf2fc74d279689804993b41ee3@coburg.de>
Message-ID: <21809ecd-81f6-9765-a2c6-79a347f3a394@treenet.co.nz>

On 27/11/17 21:20, Stegner, Martin wrote:
> Hi everyone,
> 
> I?ve set up a Squid as a transparent child-proxy. Every request is 
> redirected to another Squid with the content filtering add-on 
> e2guardian. I encounter the problem that the transparent child Squid 
> only forwards IP-Addresses to the e2guardian when HTTPS is used and so 
> e2guardian cant filter anything because it can only filter by URL.
> 

A good demonstration of why calling a URL-rewrite helper a "content 
filter" is completely wrong.

Real content filters receive the actual content and can filter it. ICAP 
and eCAP exist for that and get passed the decrypted HTTPS messages (if 
any).



> Here are some parts of the config:
> 
> http_port 3130
> 
> http_port 3128 intercept
> 
> https_port 3129 intercept ssl-bump cert=/etc/squid/cert/squid.pem
> 
> ssl_bump splice all ???????? (if I use any other option than splice 
> nothing works for some reason)

Splice tells Squid to not decrypt. Thus no content access on those 
transactions.


> 
> cache_peer 172.16.0.252 parent 8080 0 default no-query no-digest
> 
> Is there any possibility that the transparent child Squid forwards the 
> URL tot he main Squid proxy?

It already is passing what it has. "The" URI of the message being 
processed happens to be an authority-form URI. see 
<https://tools.ietf.org/html/rfc7230#section-5.3.3>.


.. and also;

* Squid requires a secure server connection to deliver decrypted content 
to. So the cache_peer needs to have the 'ssl' option and be accepting 
TLS proxy connections to receive anything other than the spliced traffic.

* The CONNECT message has to complete and the TLS inside it decrypted 
before any URL with "https://" scheme is known. When bumping to do the 
decrypt the above criteria applies.

* HTTP/1.1 connections contain many pipelined requests. So there are 
potentially many https:// URLs involved inside the crypto - it is not 
possible to know in advance of decryption what those might be.


Amos


From jlay at slave-tothe-box.net  Mon Nov 27 14:50:11 2017
From: jlay at slave-tothe-box.net (James Lay)
Date: Mon, 27 Nov 2017 07:50:11 -0700
Subject: [squid-users] Working peek/splice no longer functioning on some
 sites
In-Reply-To: <CABMULtJZUh+i+QSNOYP6tPhS6Y57d+WVQPidCMyNQ7b6GR-UzQ@mail.gmail.com>
References: <1511551841.2392.5.camel@slave-tothe-box.net>
 <2ea83011-92bc-f134-e662-2f1fd34fce89@treenet.co.nz>
 <1511610749.2338.1.camel@slave-tothe-box.net>
 <57c8c167-dfae-6579-8f75-c65ae3160c0e@treenet.co.nz>
 <1511613959.2338.6.camel@slave-tothe-box.net>
 <CABMULtJZUh+i+QSNOYP6tPhS6Y57d+WVQPidCMyNQ7b6GR-UzQ@mail.gmail.com>
Message-ID: <1511794211.2147.4.camel@slave-tothe-box.net>

On Sun, 2017-11-26 at 09:50 +0200, Alex K wrote:
> Perhaps an alternative is to peek only on step1:
> 
> acl step1 at_step SslBump1
> 
> ssl_bump peek step1
> acl allowed_https_sites ssl::server_name_regex
> "/opt/etc/squid/http_url.txt"
> ssl_bump splice allowed_https_sites
> ssl_bump terminate all
Hrmm...wouldn't that negate the ability to read the cert on step2?
In layman's terms I'm thinking:
"peek at step1"
"splice acl allow matched sni's"
"peek at step2"
"splice acl allow'd matched certs"
"terminate the rest"
Would that work Amos?

> On Nov 25, 2017 14:46,
>  "James Lay" <jlay at slave-tothe-box.net> wrote:
> > On Sun, 2017-11-26 at 01:33 +1300, Amos Jeffries wrote:
> > > On 26/11/17 00:52, James Lay wrote:
> > > 
> > > > 
> > > > On Sat, 2017-11-25 at 23:48 +1300, Amos Jeffries wrote:
> > > > 
> > > > > 
> > > > > On 25/11/17 08:30, James Lay wrote:
> > > > > 
> > > > > > 
> > > > > > Topic says it...this setup has been working well for a long time, but 
> > > > > > now there are some sites that are failing the TLS handshake.? Here's 
> > > > > > my setup: acl localnet src 192.168.1.0/24 acl SSL_ports port 443 acl 
> > > > > >  acl SSL_ports port 443 acl 
> > > > > > Safe_ports port 80 acl Safe_ports port 443 acl CONNECT method CONNECT 
> > > > > > acl allowed_http_sites url_regex "/opt/etc/squid/http_url.txt" 
> > > > > > http_access deny !Safe_ports http_access deny CONNECT !SSL_Ports 
> > > > > > http_access allow SSL_ports http_access allow allowed_http_sites 
> > > > > > http_access deny all ssl_bump peek all acl allowed_https_sites 
> > > > > > ssl::server_name_regex "/opt/etc/squid/http_url.txt" ssl_bump splice 
> > > > > > allowed_https_sites ssl_bump terminate all 
> > > > > > 

> > > > > 
> > > > > 
> > > > > 
> > > > > 
> > > > > Because you have "peek all" being performed the transaction MUST pass
> > > > > your regex patterns with both TLS SNI from the client *and* the server
> > > > > certificate SubjectName values. Either one not matching will perform
> > > > > that "terminate all" on the TLS handshake.
> > > > > 
> > > > > 

> > > > 
> > > > 
> > > > Thanks Amos...do you have a suggestion for changing this to match one or 
> > > > the other instead of both?
> > > > 

> > > 
> > > 
> > > Doing the splice check before the peek should do that. First one of the 
> > > server_names data sources to match will then splice and non-matches fall 
> > > through to either peek or terminate if no more peeking possible.
> > > 
> > > Amos
> > > 

> > > > Perfect..I've modded my lines with:
> > > > acl broken_https_sites ssl::server_name_regex "/opt/etc/squid/broken_url.> > txt"
> > ssl_bump splice broken_https_sites
> > ssl_bump peek all
> > acl allowed_https_sites ssl::server_name_regex "/opt/etc/squid/http_url.txt"
> > ssl_bump splice allowed_https_sites
> > ssl_bump terminate all

> > Hopefully that fixes these up.? Another site besides the the one this thread is fbcdn.net.? Again, these DID work, but something within the last month has changed...guessing Facebook and Elder Scrolls Online have added additional TLS security.? Thanks as always Amos.
> > > > James

> > ______________________________> > _________________
> > 
> > squid-users mailing list
> > 
squid-users at lists.squid-cache.org
> > 
http://lists.squid-cache.org/listinfo/squid-users
> > 

> 
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20171127/855f2ede/attachment.htm>

From zlyun1979 at gmail.com  Mon Nov 27 19:24:43 2017
From: zlyun1979 at gmail.com (LINGYUN ZHAO)
Date: Mon, 27 Nov 2017 11:24:43 -0800
Subject: [squid-users] Transparent Squid
Message-ID: <CAFY5H86BkncuZwj=_eJc=XK4eThfM+yAipxp19qr19Ok-qJgkw@mail.gmail.com>

Dear Squid team,


I need Squid as a real 'transparent' proxy on Fedora without changing 5
tuples. Is it possible?


The setup is simple as Client ---------- Fedora --------Server

The Squid version is 3.5.20.The key configuration on Squid as below:

   http_port 0.0.0.0:3128 transparent

   acl localnet src 10.0.0.0/24

   http_access allow localnet

And I configured a NAT on Fedora.

   iptables -t nat -A PREROUTING -i eth1 -p tcp --dport 80 -j DNAT --to
10.0.0.1:3128


When I run curl on Client to server. I found the server receives the
traffic with Fedora's IP address and different source port, instead of
Client IP address and original source port.


Thanks a lot
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20171127/78af08d8/attachment.htm>

From xeron.oskom at gmail.com  Tue Nov 28 00:19:52 2017
From: xeron.oskom at gmail.com (Ivan Larionov)
Date: Mon, 27 Nov 2017 16:19:52 -0800
Subject: [squid-users] forward_max_tries 1 has no effect
In-Reply-To: <c88e0e11-bc43-5c21-20e7-bf95dad0b212@treenet.co.nz>
References: <CAHvB88zR7wxUjMGEdrqELaMW3+pQq21hAkJR__4pWXendnmhzQ@mail.gmail.com>
 <fe6c5513-20c9-4b41-467a-9d199e38f20b@treenet.co.nz>
 <7E5CA2C7-96B6-4535-88D2-0176B0D10EC4@gmail.com>
 <c88e0e11-bc43-5c21-20e7-bf95dad0b212@treenet.co.nz>
Message-ID: <CAHvB88wx=yjJyKrrQojEAF4Cz1RsR3pdrvYZCB2mWMDv58tttw@mail.gmail.com>

I think I found part of the config which triggers the retry.

======

# Check for newproxy request header
acl newproxy_acl req_header x-use-newproxy -i true

# proxy
cache_peer 127.0.0.1 parent 18070 0 no-query no-digest no-netdb-exchange
name=proxy
cache_peer_access proxy deny newproxy_acl

# newproxy
cache_peer 127.0.0.1 parent 18079 0 no-query no-digest no-netdb-exchange
name=newproxy
cache_peer_access newproxy allow newproxy_acl
cache_peer_access newproxy deny all

never_direct allow all

======

I see retries only when squid config has 2 parents. If I comment out
everything related to "newproxy" I can't reproduce this behavior anymore.

BUT

My test request could only be forwarded to "proxy" since it doesn't have
"x-use-newproxy" header.

Which results in the following:

squid has 2 parents
request could only be forwarded to the 1st parent due to ACL
parent doesn't respond and closes the TCP connection
squid retries with the same parent ignoring "forward_max_tries 1"

I also want to clarify some facts which make me think that it could be a
bug.

1. There're no issues with TCP connection. Squid successfully connects to
parent and sends an HTTP request.
2. Parent ACKs HTTP request and then correctly closes the connection with
FIN,ACK after 40 seconds. There're no TCP timeouts/reconnects involved. The
only issue here is that parent doesn't send any HTTP response.
3. forward_max_tries is set to 1 to make sure squid won't retry. Parent
handles the retry so we don't want squid to make any additional retries.

Also see
https://wiki.squid-cache.org/SquidFaq/InnerWorkings#When_does_Squid_re-forward_a_client_request.3F

> Squid does not try to re-forward a request if at least one of the
following conditions is true:
> ?
> The number of forwarding attempts exceeded forward_max_tries. For
example, if you set forward_max_tries to 1 (one), then no requests will be
re-forwarded.
> ?
> Squid has no alternative destinations to try. Please note that
alternative destinations may include multiple next hop IP addresses and
multiple peers.
> ?

Another part of config which may or may not be related (this is to increase
the amount of local ports to use):

# 33% of traffic per local IP
acl third random 1/3
acl half random 1/2
tcp_outgoing_address 127.0.0.2 third
tcp_outgoing_address 127.0.0.3 half
tcp_outgoing_address 127.0.0.4

Logs:

ALL,2 (includes 44,2):

2017/11/27 15:53:40.542| 5,2| TcpAcceptor.cc(220) doAccept: New connection
on FD 15
2017/11/27 15:53:40.542| 5,2| TcpAcceptor.cc(295) acceptNext: connection on
local=0.0.0.0:3128 remote=[::] FD 15 flags=9
2017/11/27 15:53:40.543| 11,2| client_side.cc(2372) parseHttpRequest: HTTP
Client local=127.0.0.1:3128 remote=127.0.0.1:53798 FD 45 flags=1
2017/11/27 15:53:40.543| 11,2| client_side.cc(2373) parseHttpRequest: HTTP
Client REQUEST:
---------
GET http://HOST:12345/ HTTP/1.1
Host: HOST:12345
User-Agent: curl/7.51.0
Accept: */*
Proxy-Connection: Keep-Alive


----------
2017/11/27 15:53:40.543| 85,2| client_side_request.cc(745)
clientAccessCheckDone: The request GET http://HOST:12345/ is ALLOWED; last
ACL checked: localhost
2017/11/27 15:53:40.543| 85,2| client_side_request.cc(721)
clientAccessCheck2: No adapted_http_access configuration. default: ALLOW
2017/11/27 15:53:40.543| 85,2| client_side_request.cc(745)
clientAccessCheckDone: The request GET http://HOST:12345/ is ALLOWED; last
ACL checked: localhost
2017/11/27 15:53:40.543| 17,2| FwdState.cc(133) FwdState: Forwarding client
request local=127.0.0.1:3128 remote=127.0.0.1:53798 FD 45 flags=1, url=
http://HOST:12345/
2017/11/27 15:53:40.543| 44,2| peer_select.cc(258) peerSelectDnsPaths: Find
IP destination for: http://HOST:12345/' via 127.0.0.1
2017/11/27 15:53:40.543| 44,2| peer_select.cc(280) peerSelectDnsPaths:
Found sources for 'http://HOST:12345/'
2017/11/27 15:53:40.543| 44,2| peer_select.cc(281) peerSelectDnsPaths:
 always_direct = DENIED
2017/11/27 15:53:40.543| 44,2| peer_select.cc(282) peerSelectDnsPaths:
never_direct = ALLOWED
2017/11/27 15:53:40.543| 44,2| peer_select.cc(292) peerSelectDnsPaths:
cache_peer = local=127.0.0.3 remote=127.0.0.1:18070 flags=1
2017/11/27 15:53:40.543| 44,2| peer_select.cc(295) peerSelectDnsPaths:
  timedout = 0
2017/11/27 15:53:40.543| 11,2| http.cc(2229) sendRequest: HTTP Server local=
127.0.0.3:57091 remote=127.0.0.1:18070 FD 40 flags=1
2017/11/27 15:53:40.543| 11,2| http.cc(2230) sendRequest: HTTP Server
REQUEST:
---------
GET http://HOST:12345/ HTTP/1.1
User-Agent: curl/7.51.0
Accept: */*
Host: HOST:12345
Cache-Control: max-age=259200
Connection: keep-alive


----------

[SKIPPED 40 seconds until parent closes TCP connection with FIN,ACK]

2017/11/27 15:54:20.627| 11,2| http.cc(1299) continueAfterParsingHeader:
WARNING: HTTP: Invalid Response: No object data received for
http://HOST:12345/ AKA HOST/
2017/11/27 15:54:20.627| 17,2| FwdState.cc(655)
handleUnregisteredServerEnd: self=0x3e31838*2 err=0x409b338
http://HOST:12345/
2017/11/27 15:54:20.627| 11,2| http.cc(2229) sendRequest: HTTP Server local=
127.0.0.3:41355 remote=127.0.0.1:18070 FD 40 flags=1
2017/11/27 15:54:20.627| 11,2| http.cc(2230) sendRequest: HTTP Server
REQUEST:
---------
GET http://HOST:12345/ HTTP/1.1
User-Agent: curl/7.51.0
Accept: */*
Host: HOST:12345
Cache-Control: max-age=259200
Connection: keep-alive


----------

[SKIPPED 40 seconds again until parent closes TCP connection with FIN,ACK]

2017/11/27 15:55:00.728| ctx: enter level  0: 'http://HOST:12345/'
2017/11/27 15:55:00.728| 11,2| http.cc(719) processReplyHeader: HTTP Server
local=127.0.0.3:41355 remote=127.0.0.1:18070 FD 40 flags=1
2017/11/27 15:55:00.728| 11,2| http.cc(720) processReplyHeader: HTTP Server
REPLY:
---------
HTTP/1.0 502 Bad Gateway
Cache-Control: no-cache
Connection: close
Content-Type: text/html

<html><body><h1>502 Bad Gateway</h1>
The server returned an invalid or incomplete response.
</body></html>

----------
2017/11/27 15:55:00.728| ctx: exit level  0
2017/11/27 15:55:00.728| 20,2| store.cc(996) checkCachable:
StoreEntry::checkCachable: NO: not cachable
2017/11/27 15:55:00.728| 20,2| store.cc(996) checkCachable:
StoreEntry::checkCachable: NO: not cachable
2017/11/27 15:55:00.728| 88,2| client_side_reply.cc(2073)
processReplyAccessResult: The reply for GET http://HOST:12345/ is ALLOWED,
because it matched (access_log stdio:/var/log/squid/access.log line)
2017/11/27 15:55:00.728| 11,2| client_side.cc(1409) sendStartOfMessage:
HTTP Client local=127.0.0.1:3128 remote=127.0.0.1:53798 FD 45 flags=1
2017/11/27 15:55:00.728| 11,2| client_side.cc(1410) sendStartOfMessage:
HTTP Client REPLY:
---------
HTTP/1.1 502 Bad Gateway
Date: Mon, 27 Nov 2017 23:54:20 GMT
Cache-Control: no-cache
Content-Type: text/html
X-Cache: MISS from ip-172-23-18-130
X-Cache-Lookup: MISS from ip-172-23-18-130:3128
Transfer-Encoding: chunked
Connection: keep-alive


----------


On Thu, Nov 23, 2017 at 11:43 PM, Amos Jeffries <squid3 at treenet.co.nz>
wrote:

>
> On 24/11/17 10:03, Ivan Larionov wrote:
>
>>
>>> On Nov 23, 2017, at 12:32 AM, Amos Jeffries <squid3 at treenet.co.nz>
>>> wrote:
>>>
>>> On 23/11/17 14:20, Ivan Larionov wrote:
>>>
>>>> Hello.
>>>> We have an issue with squid when it tries to re-forward / retry failed
>>>> request even when forward_max_tries is set to 1. The situation when it
>>>> happens is when there's no response, parent just closes the connection.
>>>>
>>> ...
>>>
>>>> It doesn't happen 100% times. Sometimes squid returns 502 after the 1st
>>>> try, sometimes it retries once. Also I haven't seen more than 1 retry.
>>>>
>>>
>>> Please enable debug_options 44,2 to see what destinations your Squid is
>>> actually finding.
>>>
>>
>> I'll check this on Monday.
>>
>>
>>> max_forward_tries is just a rough cap on the number of server names
>>> which can be found when generating that list. The actual destinations count
>>> can exceed it if one or more of the servers happens to have multiple IPs to
>>> try.
>>>
>>> The overall transaction can involve retries if one of the other layers
>>> (TCP or HTTP) contains retry semantics to a single server.
>>>
>>>
>>>
>>> Could it be a bug? We'd really like to disable these retries.
>>>>
>>>
>>> Why are trying to break HTTP?
>>> What is the actual problem you are trying to resolve here?
>>>
>>>
>> Why do you think I'm trying to break HTTP?
>>
>> squid forwards the request to parent but parent misbehaves and just
>> closes the connection after 40 seconds. I'm trying to prevent retry of
>> request in such situation. Why squid retries if I never asked him to do it
>> and specifically said "forward_max_tries 1".
>>
>> And this is not a connection failure, squid successfully establishes the
>> connection and sends the request, parent ACKs it, just never responses back
>> and proactively closes the connection.
>>
>>
> This is not misbehaviour on the part of either Squid nor the parent.
> <https://tools.ietf.org/html/rfc7230#section-6.3.1>
> "Connections can be closed at any time, with or without intention."
>
>
> As has been discussed in other threads recently there are servers out
> there starting to greylist TCP connections, closing the first one some time
> *after* SYN+ACK regardless of what the proxy sends and accepting any
> followup connection attempts.
>
> NP: That can result in exactly the behaviour you describe from the peer as
> Squid does not wait for a FIN to arrive before sending its upstream HTTP
> request - Squid will "randomly" get a FIN or a RST depending on whether the
> FIN or the DATA packet wins the race into the Squid machines TCP stack. FIN
> and RST have different retry properties which might explain your "sometimes
> retries" behaviour.
>
>
> Also, TCP connections fail quite often for many other reasons anyway.
> Anything from power fluctuations at a router to BGP switching the packet
> route dropping packets. They are most often a short-term situation which is
> resolved by the time the repeat is attempted.
>
> What you are trying to do will result in Squid being unable to cope with
> any of these transitory restrictions from the TCP environment and force the
> client to receive a terminal error page.
>  That will greatly slow down detection and recovery from the slightly
> longer-lived TCP issues in Squid itself and may result in N other clients
> also unnecessarily receiving the same error response as bad connection
> attempts gets spread between many clients (all getting errors) instead of
> isolated to the one/few who hit it when the issue initially occurs.
>
> Expanding the retries to large numbers (ie the recent default change to
> 25), or to low numbers (eg the old default of 5) are reasonable things to
> do depending on the network stability to your upstreams. But going all the
> way to 0 retries is guaranteed to lead to more client visible problems than
> necessary.
>
>
> All that asside I phrased it as a question because you might have had a
> good reason for increasing the visible failure rates.
>
>
> We're already fixing parent behavior, but still want to disable retries on
>> squid side.
>>
>>
>
> Since you describe this as peer misbehaviour, then treating it to Squids
> normal TCP failure recovery is the best behaviour. Retry is the intended
> correct behaviour for a proxy to perform on any non-idempotent requests. In
> your case up to a value of 1 retry before declaring non-temporary route
> failure.
>
> NP: idempotent vs non-idempotent may be another reason behind the observed
> behaviour of retry happening only sometimes.
>
>
> If you are doing this due to overall latency/delay on the affected client
> traffic you would be better off reducing the timeouts involved (cache_peer
> connect-timeout= parameter AFAICS) than aiming at a retry count of 0.
> Perhapse also requiring a few standby=N persistent connections to be
> maintained if the peer is HTTP/1.1 capable.
>
> Amos
>



-- 
With best regards, Ivan Larionov.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20171127/889e8d27/attachment.htm>

From hoangminhung at gmail.com  Tue Nov 28 10:32:06 2017
From: hoangminhung at gmail.com (=?UTF-8?B?bWluaCBoxrBuZyDEkeG7lyBob8Ogbmc=?=)
Date: Tue, 28 Nov 2017 17:32:06 +0700
Subject: [squid-users] Fwd: [Squid-3.5.20]Squid transparent proxy
 http/https without client site config
In-Reply-To: <CAO0fBEpUoORHnYBkHh8jcAOByfpT8vpw628=TW2t94xAWTDMKA@mail.gmail.com>
References: <CAO0fBEpZvu=GFP0q=XzdzQMxBq629YA901MZFGEQbbLXmB9ZHg@mail.gmail.com>
 <CAO0fBEp1ni6_RYnX-jnwggnr7StHvB+5-X8a9c3O6Fms-3S0uQ@mail.gmail.com>
 <67f77ee1-3266-4171-b669-39665f9cb72a@treenet.co.nz>
 <CAO0fBEr+8=Ehdgb6nzXS2Rzrm-fADbwh-Z+yE7qirdgU-8A8eQ@mail.gmail.com>
 <ead14327-92a2-c89e-1782-6c1b489216d2@treenet.co.nz>
 <CAO0fBEpUoORHnYBkHh8jcAOByfpT8vpw628=TW2t94xAWTDMKA@mail.gmail.com>
Message-ID: <CAO0fBEp7gg0KpDCC4TxqK9u4eSKr2hee1DoWEVsnVRd19LUjPQ@mail.gmail.com>

Dear Amos,
I solved my problem by following this :
1 - I used my Mikrotik router as a cache DNS
2 - Both Squid proxy and my client use Mikrotik' DNS

=> It no more take alert form cache.log

Thanks alot :)
-- 
Thanks & Best Regards,
--------------
?? Ho?ng Minh H?ng
Gmail : hoangminhung at gmail.com
S?T : 01234454115
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20171128/cbc1755f/attachment.htm>

From uhlar at fantomas.sk  Tue Nov 28 13:03:43 2017
From: uhlar at fantomas.sk (Matus UHLAR - fantomas)
Date: Tue, 28 Nov 2017 14:03:43 +0100
Subject: [squid-users] Transparent Squid
In-Reply-To: <CAFY5H86BkncuZwj=_eJc=XK4eThfM+yAipxp19qr19Ok-qJgkw@mail.gmail.com>
References: <CAFY5H86BkncuZwj=_eJc=XK4eThfM+yAipxp19qr19Ok-qJgkw@mail.gmail.com>
Message-ID: <20171128130343.GA8443@fantomas.sk>

On 27.11.17 11:24, LINGYUN ZHAO wrote:
>I need Squid as a real 'transparent' proxy on Fedora without changing 5
>tuples. Is it possible?

tuples?

>The setup is simple as Client ---------- Fedora --------Server

is fedora NAT device and also running squid?

>The Squid version is 3.5.20.The key configuration on Squid as below:
>
>   http_port 0.0.0.0:3128 transparent
>
>   acl localnet src 10.0.0.0/24
>
>   http_access allow localnet
>
>And I configured a NAT on Fedora.
>
>   iptables -t nat -A PREROUTING -i eth1 -p tcp --dport 80 -j DNAT --to
>10.0.0.1:3128

this only works if "fedora" does the NAT and runs squid.
(just for sure)

>When I run curl on Client to server. I found the server receives the
>traffic with Fedora's IP address and different source port, instead of
>Client IP address and original source port.

when you redirect traffic tyo the squid and squid connect to the server,
it's logical that server sees squid's IP.

if you want to keep source client's IP, you need tproxy:
https://wiki.squid-cache.org/Features/Tproxy4


-- 
Matus UHLAR - fantomas, uhlar at fantomas.sk ; http://www.fantomas.sk/
Warning: I wish NOT to receive e-mail advertising to this address.
Varovanie: na tuto adresu chcem NEDOSTAVAT akukolvek reklamnu postu.
Due to unexpected conditions Windows 2000 will be released
in first quarter of year 1901


From rousskov at measurement-factory.com  Tue Nov 28 15:32:53 2017
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Tue, 28 Nov 2017 08:32:53 -0700
Subject: [squid-users] forward_max_tries 1 has no effect
In-Reply-To: <CAHvB88wx=yjJyKrrQojEAF4Cz1RsR3pdrvYZCB2mWMDv58tttw@mail.gmail.com>
References: <CAHvB88zR7wxUjMGEdrqELaMW3+pQq21hAkJR__4pWXendnmhzQ@mail.gmail.com>
 <fe6c5513-20c9-4b41-467a-9d199e38f20b@treenet.co.nz>
 <7E5CA2C7-96B6-4535-88D2-0176B0D10EC4@gmail.com>
 <c88e0e11-bc43-5c21-20e7-bf95dad0b212@treenet.co.nz>
 <CAHvB88wx=yjJyKrrQojEAF4Cz1RsR3pdrvYZCB2mWMDv58tttw@mail.gmail.com>
Message-ID: <4e0b3811-2965-32d7-fe70-a6987ced93f8@measurement-factory.com>

On 11/27/2017 05:19 PM, Ivan Larionov wrote:

> I see retries only when squid config has 2 parents. If I comment out
> everything related to "newproxy" I can't reproduce this behavior anymore.

The posted logs are not detailed enough to confirm or deny that IMO, but
I suspect that you are dealing with at least one bug.


> https://wiki.squid-cache.org/SquidFaq/InnerWorkings#When_does_Squid_re-forward_a_client_request.3F
> 
>> Squid does not try to re-forward a request if at least one of the following conditions is true:
>>
>> The number of forwarding attempts exceeded forward_max_tries. For
>> example, if you set forward_max_tries to 1 (one), then no requests
>> will be re-forwarded.


AFAICT, there is an off-by-one bug in Squid that violates the above:

>     if (n_tries > Config.forward_max_tries)
>         return false;

The n_tries counter is incremented before Squid makes a request
forwarding attempt. With n_tries and Config.forward_max_tries both set
to 1, the quoted FwdState::checkRetry() code will not prevent
re-forwarding. There is a similar problem in FwdState::reforward(). This
reasoning needs confirmation/testing.

Please note that simply changing the ">" operator to ">=" may break
other things in a difficult-to-detect-by-simple-tests ways. The correct
fix may be more complex than it looks and may involve making policy
decisions regarding forward_max_tries meaning. The best fix would remove
checkRetry() and reforward() duplication. This code is difficult to work
with; many related code names are misleading.


>> Squid has no alternative destinations to try. Please note that
>> alternative destinations may include multiple next hop IP addresses
>> and multiple peers.

The fact that Squid sends two requests to the same peer with only one
peer address selected suggests that Squid is retrying a failed
persistent connection rather than re-forwarding after receiving a bad
response. Again, the logs are not detailed enough to distinguish the two
cases. I can only see that a single peer/destination address was
selected (not two), which is correct/expected behavior. I cannot see
what happened next with sufficient detail.

Going forward, you have several options, including:

A. Post a link to compressed ALL,7+ logs to confirm bug(s).
B. Fix the broken condition(s) in FwdState. See above.

HTH,

Alex.


> 2017/11/27 15:53:40.542| 5,2| TcpAcceptor.cc(220) doAccept: New connection on FD 15
> 2017/11/27 15:53:40.542| 5,2| TcpAcceptor.cc(295) acceptNext: connection
> on local=0.0.0.0:3128 <http://0.0.0.0:3128> remote=[::] FD 15 flags=9
> 2017/11/27 15:53:40.543| 11,2| client_side.cc(2372) parseHttpRequest:
> HTTP Client local=127.0.0.1:3128 <http://127.0.0.1:3128>
> remote=127.0.0.1:53798 <http://127.0.0.1:53798> FD 45 flags=1
> 2017/11/27 15:53:40.543| 11,2| client_side.cc(2373) parseHttpRequest:
> HTTP Client REQUEST:
> ---------
> GET http://HOST:12345/ HTTP/1.1
> Host: HOST:12345
> User-Agent: curl/7.51.0
> Accept: */*
> Proxy-Connection: Keep-Alive
> 
> 
> ----------
> 2017/11/27 15:53:40.543| 85,2| client_side_request.cc(745)
> clientAccessCheckDone: The request GET http://HOST:12345/ is ALLOWED;
> last ACL checked: localhost
> 2017/11/27 15:53:40.543| 85,2| client_side_request.cc(721)
> clientAccessCheck2: No adapted_http_access configuration. default: ALLOW
> 2017/11/27 15:53:40.543| 85,2| client_side_request.cc(745)
> clientAccessCheckDone: The request GET http://HOST:12345/ is ALLOWED;
> last ACL checked: localhost
> 2017/11/27 15:53:40.543| 17,2| FwdState.cc(133) FwdState: Forwarding
> client request local=127.0.0.1:3128 <http://127.0.0.1:3128>
> remote=127.0.0.1:53798 <http://127.0.0.1:53798> FD 45 flags=1,
> url=http://HOST:12345/
> 2017/11/27 15:53:40.543| 44,2| peer_select.cc(258) peerSelectDnsPaths:
> Find IP destination for: http://HOST:12345/' via 127.0.0.1
> 2017/11/27 15:53:40.543| 44,2| peer_select.cc(280) peerSelectDnsPaths:
> Found sources for 'http://HOST:12345/'
> 2017/11/27 15:53:40.543| 44,2| peer_select.cc(281) peerSelectDnsPaths:?
> ?always_direct = DENIED
> 2017/11/27 15:53:40.543| 44,2| peer_select.cc(282) peerSelectDnsPaths:?
> ? never_direct = ALLOWED
> 2017/11/27 15:53:40.543| 44,2| peer_select.cc(292) peerSelectDnsPaths:?
> ? ? cache_peer = local=127.0.0.3 remote=127.0.0.1:18070
> <http://127.0.0.1:18070> flags=1
> 2017/11/27 15:53:40.543| 44,2| peer_select.cc(295) peerSelectDnsPaths:?
> ? ? ? timedout = 0
> 2017/11/27 15:53:40.543| 11,2| http.cc(2229) sendRequest: HTTP Server
> local=127.0.0.3:57091 <http://127.0.0.3:57091> remote=127.0.0.1:18070
> <http://127.0.0.1:18070> FD 40 flags=1
> 2017/11/27 15:53:40.543| 11,2| http.cc(2230) sendRequest: HTTP Server
> REQUEST:
> ---------
> GET http://HOST:12345/ HTTP/1.1
> User-Agent: curl/7.51.0
> Accept: */*
> Host: HOST:12345
> Cache-Control: max-age=259200
> Connection: keep-alive
> 
> 
> ----------
> 
> [SKIPPED 40 seconds until parent closes TCP connection with FIN,ACK]
> 
> 2017/11/27 15:54:20.627| 11,2| http.cc(1299) continueAfterParsingHeader:
> WARNING: HTTP: Invalid Response: No object data received for
> http://HOST:12345/ AKA HOST/
> 2017/11/27 15:54:20.627| 17,2| FwdState.cc(655)
> handleUnregisteredServerEnd: self=0x3e31838*2 err=0x409b338
> http://HOST:12345/
> 2017/11/27 15:54:20.627| 11,2| http.cc(2229) sendRequest: HTTP Server
> local=127.0.0.3:41355 <http://127.0.0.3:41355> remote=127.0.0.1:18070
> <http://127.0.0.1:18070> FD 40 flags=1
> 2017/11/27 15:54:20.627| 11,2| http.cc(2230) sendRequest: HTTP Server
> REQUEST:
> ---------
> GET http://HOST:12345/ HTTP/1.1
> User-Agent: curl/7.51.0
> Accept: */*
> Host: HOST:12345
> Cache-Control: max-age=259200
> Connection: keep-alive
> 
> 
> ----------
> 
> [SKIPPED 40 seconds again until parent closes TCP connection with FIN,ACK]
> 
> 2017/11/27 15:55:00.728| ctx: enter level? 0: 'http://HOST:12345/'
> 2017/11/27 15:55:00.728| 11,2| http.cc(719) processReplyHeader: HTTP
> Server local=127.0.0.3:41355 <http://127.0.0.3:41355>
> remote=127.0.0.1:18070 <http://127.0.0.1:18070> FD 40 flags=1
> 2017/11/27 15:55:00.728| 11,2| http.cc(720) processReplyHeader: HTTP
> Server REPLY:
> ---------
> HTTP/1.0 502 Bad Gateway
> Cache-Control: no-cache
> Connection: close
> Content-Type: text/html
> 
> <html><body><h1>502 Bad Gateway</h1>
> The server returned an invalid or incomplete response.
> </body></html>
> 
> ----------
> 2017/11/27 15:55:00.728| ctx: exit level? 0
> 2017/11/27 15:55:00.728| 20,2| store.cc(996) checkCachable:
> StoreEntry::checkCachable: NO: not cachable
> 2017/11/27 15:55:00.728| 20,2| store.cc(996) checkCachable:
> StoreEntry::checkCachable: NO: not cachable
> 2017/11/27 15:55:00.728| 88,2| client_side_reply.cc(2073)
> processReplyAccessResult: The reply for GET http://HOST:12345/ is
> ALLOWED, because it matched (access_log stdio:/var/log/squid/access.log
> line)
> 2017/11/27 15:55:00.728| 11,2| client_side.cc(1409) sendStartOfMessage:
> HTTP Client local=127.0.0.1:3128 <http://127.0.0.1:3128>
> remote=127.0.0.1:53798 <http://127.0.0.1:53798> FD 45 flags=1
> 2017/11/27 15:55:00.728| 11,2| client_side.cc(1410) sendStartOfMessage:
> HTTP Client REPLY:
> ---------
> HTTP/1.1 502 Bad Gateway
> Date: Mon, 27 Nov 2017 23:54:20 GMT
> Cache-Control: no-cache
> Content-Type: text/html
> X-Cache: MISS from ip-172-23-18-130
> X-Cache-Lookup: MISS from ip-172-23-18-130:3128
> Transfer-Encoding: chunked
> Connection: keep-alive
> 
> 
> ----------


From xeron.oskom at gmail.com  Tue Nov 28 21:27:44 2017
From: xeron.oskom at gmail.com (Ivan Larionov)
Date: Tue, 28 Nov 2017 13:27:44 -0800
Subject: [squid-users] forward_max_tries 1 has no effect
In-Reply-To: <4e0b3811-2965-32d7-fe70-a6987ced93f8@measurement-factory.com>
References: <CAHvB88zR7wxUjMGEdrqELaMW3+pQq21hAkJR__4pWXendnmhzQ@mail.gmail.com>
 <fe6c5513-20c9-4b41-467a-9d199e38f20b@treenet.co.nz>
 <7E5CA2C7-96B6-4535-88D2-0176B0D10EC4@gmail.com>
 <c88e0e11-bc43-5c21-20e7-bf95dad0b212@treenet.co.nz>
 <CAHvB88wx=yjJyKrrQojEAF4Cz1RsR3pdrvYZCB2mWMDv58tttw@mail.gmail.com>
 <4e0b3811-2965-32d7-fe70-a6987ced93f8@measurement-factory.com>
Message-ID: <CAHvB88yEgcfYNrLYDENCXm2me=jguqgj-9FB9KdtiJV9catORQ@mail.gmail.com>

Thanks Alex, this is very helpful.

Another interesting fact is that I can't reproduce this issue if squid has
no other traffic except my testing requests. But it's easy to reproduce
when server has other traffic.

The problem is that with other traffic I can't provide the whole log file
with debug ALL,7 enabled because it has other requests.

So I tried to select only parts related to my test request (this is ALL,7):

https://www.dropbox.com/s/udzeipeerf5o38t/squid_retry_logs.tgz?dl=1


On Tue, Nov 28, 2017 at 7:32 AM, Alex Rousskov <
rousskov at measurement-factory.com> wrote:

> On 11/27/2017 05:19 PM, Ivan Larionov wrote:
>
> > I see retries only when squid config has 2 parents. If I comment out
> > everything related to "newproxy" I can't reproduce this behavior anymore.
>
> The posted logs are not detailed enough to confirm or deny that IMO, but
> I suspect that you are dealing with at least one bug.
>
>
> > https://wiki.squid-cache.org/SquidFaq/InnerWorkings#When_
> does_Squid_re-forward_a_client_request.3F
> >
> >> Squid does not try to re-forward a request if at least one of the
> following conditions is true:
> >>
> >> The number of forwarding attempts exceeded forward_max_tries. For
> >> example, if you set forward_max_tries to 1 (one), then no requests
> >> will be re-forwarded.
>
>
> AFAICT, there is an off-by-one bug in Squid that violates the above:
>
> >     if (n_tries > Config.forward_max_tries)
> >         return false;
>
> The n_tries counter is incremented before Squid makes a request
> forwarding attempt. With n_tries and Config.forward_max_tries both set
> to 1, the quoted FwdState::checkRetry() code will not prevent
> re-forwarding. There is a similar problem in FwdState::reforward(). This
> reasoning needs confirmation/testing.
>
> Please note that simply changing the ">" operator to ">=" may break
> other things in a difficult-to-detect-by-simple-tests ways. The correct
> fix may be more complex than it looks and may involve making policy
> decisions regarding forward_max_tries meaning. The best fix would remove
> checkRetry() and reforward() duplication. This code is difficult to work
> with; many related code names are misleading.
>
>
> >> Squid has no alternative destinations to try. Please note that
> >> alternative destinations may include multiple next hop IP addresses
> >> and multiple peers.
>
> The fact that Squid sends two requests to the same peer with only one
> peer address selected suggests that Squid is retrying a failed
> persistent connection rather than re-forwarding after receiving a bad
> response. Again, the logs are not detailed enough to distinguish the two
> cases. I can only see that a single peer/destination address was
> selected (not two), which is correct/expected behavior. I cannot see
> what happened next with sufficient detail.
>
> Going forward, you have several options, including:
>
> A. Post a link to compressed ALL,7+ logs to confirm bug(s).
> B. Fix the broken condition(s) in FwdState. See above.
>
> HTH,
>
> Alex.
>
>
> > 2017/11/27 15:53:40.542| 5,2| TcpAcceptor.cc(220) doAccept: New
> connection on FD 15
> > 2017/11/27 15:53:40.542| 5,2| TcpAcceptor.cc(295) acceptNext: connection
> > on local=0.0.0.0:3128 <http://0.0.0.0:3128> remote=[::] FD 15 flags=9
> > 2017/11/27 15:53:40.543| 11,2| client_side.cc(2372) parseHttpRequest:
> > HTTP Client local=127.0.0.1:3128 <http://127.0.0.1:3128>
> > remote=127.0.0.1:53798 <http://127.0.0.1:53798> FD 45 flags=1
> > 2017/11/27 15:53:40.543| 11,2| client_side.cc(2373) parseHttpRequest:
> > HTTP Client REQUEST:
> > ---------
> > GET http://HOST:12345/ HTTP/1.1
> > Host: HOST:12345
> > User-Agent: curl/7.51.0
> > Accept: */*
> > Proxy-Connection: Keep-Alive
> >
> >
> > ----------
> > 2017/11/27 15:53:40.543| 85,2| client_side_request.cc(745)
> > clientAccessCheckDone: The request GET http://HOST:12345/ is ALLOWED;
> > last ACL checked: localhost
> > 2017/11/27 15:53:40.543| 85,2| client_side_request.cc(721)
> > clientAccessCheck2: No adapted_http_access configuration. default: ALLOW
> > 2017/11/27 15:53:40.543| 85,2| client_side_request.cc(745)
> > clientAccessCheckDone: The request GET http://HOST:12345/ is ALLOWED;
> > last ACL checked: localhost
> > 2017/11/27 15:53:40.543| 17,2| FwdState.cc(133) FwdState: Forwarding
> > client request local=127.0.0.1:3128 <http://127.0.0.1:3128>
> > remote=127.0.0.1:53798 <http://127.0.0.1:53798> FD 45 flags=1,
> > url=http://HOST:12345/
> > 2017/11/27 15:53:40.543| 44,2| peer_select.cc(258) peerSelectDnsPaths:
> > Find IP destination for: http://HOST:12345/' via 127.0.0.1
> > 2017/11/27 15:53:40.543| 44,2| peer_select.cc(280) peerSelectDnsPaths:
> > Found sources for 'http://HOST:12345/'
> > 2017/11/27 15:53:40.543| 44,2| peer_select.cc(281) peerSelectDnsPaths:
> >  always_direct = DENIED
> > 2017/11/27 15:53:40.543| 44,2| peer_select.cc(282) peerSelectDnsPaths:
> >   never_direct = ALLOWED
> > 2017/11/27 15:53:40.543| 44,2| peer_select.cc(292) peerSelectDnsPaths:
> >     cache_peer = local=127.0.0.3 remote=127.0.0.1:18070
> > <http://127.0.0.1:18070> flags=1
> > 2017/11/27 15:53:40.543| 44,2| peer_select.cc(295) peerSelectDnsPaths:
> >       timedout = 0
> > 2017/11/27 15:53:40.543| 11,2| http.cc(2229) sendRequest: HTTP Server
> > local=127.0.0.3:57091 <http://127.0.0.3:57091> remote=127.0.0.1:18070
> > <http://127.0.0.1:18070> FD 40 flags=1
> > 2017/11/27 15:53:40.543| 11,2| http.cc(2230) sendRequest: HTTP Server
> > REQUEST:
> > ---------
> > GET http://HOST:12345/ HTTP/1.1
> > User-Agent: curl/7.51.0
> > Accept: */*
> > Host: HOST:12345
> > Cache-Control: max-age=259200
> > Connection: keep-alive
> >
> >
> > ----------
> >
> > [SKIPPED 40 seconds until parent closes TCP connection with FIN,ACK]
> >
> > 2017/11/27 15:54:20.627| 11,2| http.cc(1299) continueAfterParsingHeader:
> > WARNING: HTTP: Invalid Response: No object data received for
> > http://HOST:12345/ AKA HOST/
> > 2017/11/27 15:54:20.627| 17,2| FwdState.cc(655)
> > handleUnregisteredServerEnd: self=0x3e31838*2 err=0x409b338
> > http://HOST:12345/
> > 2017/11/27 15:54:20.627| 11,2| http.cc(2229) sendRequest: HTTP Server
> > local=127.0.0.3:41355 <http://127.0.0.3:41355> remote=127.0.0.1:18070
> > <http://127.0.0.1:18070> FD 40 flags=1
> > 2017/11/27 15:54:20.627| 11,2| http.cc(2230) sendRequest: HTTP Server
> > REQUEST:
> > ---------
> > GET http://HOST:12345/ HTTP/1.1
> > User-Agent: curl/7.51.0
> > Accept: */*
> > Host: HOST:12345
> > Cache-Control: max-age=259200
> > Connection: keep-alive
> >
> >
> > ----------
> >
> > [SKIPPED 40 seconds again until parent closes TCP connection with
> FIN,ACK]
> >
> > 2017/11/27 15:55:00.728| ctx: enter level  0: 'http://HOST:12345/'
> > 2017/11/27 15:55:00.728| 11,2| http.cc(719) processReplyHeader: HTTP
> > Server local=127.0.0.3:41355 <http://127.0.0.3:41355>
> > remote=127.0.0.1:18070 <http://127.0.0.1:18070> FD 40 flags=1
> > 2017/11/27 15:55:00.728| 11,2| http.cc(720) processReplyHeader: HTTP
> > Server REPLY:
> > ---------
> > HTTP/1.0 502 Bad Gateway
> > Cache-Control: no-cache
> > Connection: close
> > Content-Type: text/html
> >
> > <html><body><h1>502 Bad Gateway</h1>
> > The server returned an invalid or incomplete response.
> > </body></html>
> >
> > ----------
> > 2017/11/27 15:55:00.728| ctx: exit level  0
> > 2017/11/27 15:55:00.728| 20,2| store.cc(996) checkCachable:
> > StoreEntry::checkCachable: NO: not cachable
> > 2017/11/27 15:55:00.728| 20,2| store.cc(996) checkCachable:
> > StoreEntry::checkCachable: NO: not cachable
> > 2017/11/27 15:55:00.728| 88,2| client_side_reply.cc(2073)
> > processReplyAccessResult: The reply for GET http://HOST:12345/ is
> > ALLOWED, because it matched (access_log stdio:/var/log/squid/access.log
> > line)
> > 2017/11/27 15:55:00.728| 11,2| client_side.cc(1409) sendStartOfMessage:
> > HTTP Client local=127.0.0.1:3128 <http://127.0.0.1:3128>
> > remote=127.0.0.1:53798 <http://127.0.0.1:53798> FD 45 flags=1
> > 2017/11/27 15:55:00.728| 11,2| client_side.cc(1410) sendStartOfMessage:
> > HTTP Client REPLY:
> > ---------
> > HTTP/1.1 502 Bad Gateway
> > Date: Mon, 27 Nov 2017 23:54:20 GMT
> > Cache-Control: no-cache
> > Content-Type: text/html
> > X-Cache: MISS from ip-172-23-18-130
> > X-Cache-Lookup: MISS from ip-172-23-18-130:3128
> > Transfer-Encoding: chunked
> > Connection: keep-alive
> >
> >
> > ----------
>



-- 
With best regards, Ivan Larionov.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20171128/f98dfef6/attachment.htm>

From rousskov at measurement-factory.com  Wed Nov 29 03:12:40 2017
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Tue, 28 Nov 2017 20:12:40 -0700
Subject: [squid-users] forward_max_tries 1 has no effect
In-Reply-To: <CAHvB88yEgcfYNrLYDENCXm2me=jguqgj-9FB9KdtiJV9catORQ@mail.gmail.com>
References: <CAHvB88zR7wxUjMGEdrqELaMW3+pQq21hAkJR__4pWXendnmhzQ@mail.gmail.com>
 <fe6c5513-20c9-4b41-467a-9d199e38f20b@treenet.co.nz>
 <7E5CA2C7-96B6-4535-88D2-0176B0D10EC4@gmail.com>
 <c88e0e11-bc43-5c21-20e7-bf95dad0b212@treenet.co.nz>
 <CAHvB88wx=yjJyKrrQojEAF4Cz1RsR3pdrvYZCB2mWMDv58tttw@mail.gmail.com>
 <4e0b3811-2965-32d7-fe70-a6987ced93f8@measurement-factory.com>
 <CAHvB88yEgcfYNrLYDENCXm2me=jguqgj-9FB9KdtiJV9catORQ@mail.gmail.com>
Message-ID: <20b1fce2-6677-fd06-2bf1-4e74f05e214c@measurement-factory.com>

On 11/28/2017 02:27 PM, Ivan Larionov wrote:

> Another interesting fact is that I can't reproduce this issue if squid
> has no other traffic except my testing requests. But it's easy to
> reproduce when server has other traffic.

I did not check your logs carefully, but I believe that (when things do
not work the way you expect) your Squid is retrying a failed persistent
connection (rather than re-forwarding the request after receiving a bad
response). See the "pconn race happened" line below:

> 1_retry.log:2017/11/28 12:55:12.731| 17,3| FwdState.cc(416) fail: ERR_ZERO_SIZE_OBJECT "Bad Gateway"
> 1_retry.log:2017/11/28 12:55:12.731| 17,5| FwdState.cc(430) fail: pconn race happened
> 1_retry.log:2017/11/28 12:55:12.731| 93,5| AsyncJob.cc(84) mustStop: HttpStateData will stop, reason: HttpStateData::continueAfterParsingHeader
> 1_retry.log:2017/11/28 12:55:12.731| 17,3| FwdState.cc(618) retryOrBail: re-forwarding (1 tries, 40 secs)
> 1_retry.log:2017/11/28 12:55:12.731| 17,4| FwdState.cc(621) retryOrBail: retrying the same destination


Assuming you tested with forward_max_tries set to 1, the retryOrBail
lines above confirm the off-by-one problem I was describing in my
previous response.

AFAICT, compensating by setting forward_max_tries to zero will _not_
work (for reasons unrelated to the problem at hand).


FWIW, your current options include those outlined at
http://wiki.squid-cache.org/SquidFaq/AboutSquid#How_to_add_a_new_Squid_feature.2C_enhance.2C_of_fix_something.3F


Alex.


> On Tue, Nov 28, 2017 at 7:32 AM, Alex Rousskov wrote:
> 
>     On 11/27/2017 05:19 PM, Ivan Larionov wrote:
> 
>     > I see retries only when squid config has 2 parents. If I comment out
>     > everything related to "newproxy" I can't reproduce this behavior anymore.
> 
>     The posted logs are not detailed enough to confirm or deny that IMO, but
>     I suspect that you are dealing with at least one bug.
> 
> 
>     > https://wiki.squid-cache.org/SquidFaq/InnerWorkings#When_does_Squid_re-forward_a_client_request.3F
>     <https://wiki.squid-cache.org/SquidFaq/InnerWorkings#When_does_Squid_re-forward_a_client_request.3F>
>     >
>     >> Squid does not try to re-forward a request if at least one of the following conditions is true:
>     >>
>     >> The number of forwarding attempts exceeded forward_max_tries. For
>     >> example, if you set forward_max_tries to 1 (one), then no requests
>     >> will be re-forwarded.
> 
> 
>     AFAICT, there is an off-by-one bug in Squid that violates the above:
> 
>     >? ? ?if (n_tries > Config.forward_max_tries)
>     >? ? ? ? ?return false;
> 
>     The n_tries counter is incremented before Squid makes a request
>     forwarding attempt. With n_tries and Config.forward_max_tries both set
>     to 1, the quoted FwdState::checkRetry() code will not prevent
>     re-forwarding. There is a similar problem in FwdState::reforward(). This
>     reasoning needs confirmation/testing.
> 
>     Please note that simply changing the ">" operator to ">=" may break
>     other things in a difficult-to-detect-by-simple-tests ways. The correct
>     fix may be more complex than it looks and may involve making policy
>     decisions regarding forward_max_tries meaning. The best fix would remove
>     checkRetry() and reforward() duplication. This code is difficult to work
>     with; many related code names are misleading.
> 
> 
>     >> Squid has no alternative destinations to try. Please note that
>     >> alternative destinations may include multiple next hop IP addresses
>     >> and multiple peers.
> 
>     The fact that Squid sends two requests to the same peer with only one
>     peer address selected suggests that Squid is retrying a failed
>     persistent connection rather than re-forwarding after receiving a bad
>     response. Again, the logs are not detailed enough to distinguish the two
>     cases. I can only see that a single peer/destination address was
>     selected (not two), which is correct/expected behavior. I cannot see
>     what happened next with sufficient detail.
> 
>     Going forward, you have several options, including:
> 
>     A. Post a link to compressed ALL,7+ logs to confirm bug(s).
>     B. Fix the broken condition(s) in FwdState. See above.
> 
>     HTH,
> 
>     Alex.
> 
> 
>     > 2017/11/27 15:53:40.542| 5,2| TcpAcceptor.cc(220) doAccept: New connection on FD 15
>     > 2017/11/27 15:53:40.542| 5,2| TcpAcceptor.cc(295) acceptNext: connection
>     > on local=0.0.0.0:3128 <http://0.0.0.0:3128> <http://0.0.0.0:3128>
>     remote=[::] FD 15 flags=9
>     > 2017/11/27 15:53:40.543| 11,2| client_side.cc(2372) parseHttpRequest:
>     > HTTP Client local=127.0.0.1:3128 <http://127.0.0.1:3128>
>     <http://127.0.0.1:3128>
>     > remote=127.0.0.1:53798 <http://127.0.0.1:53798>
>     <http://127.0.0.1:53798> FD 45 flags=1
>     > 2017/11/27 15:53:40.543| 11,2| client_side.cc(2373) parseHttpRequest:
>     > HTTP Client REQUEST:
>     > ---------
>     > GET http://HOST:12345/ HTTP/1.1
>     > Host: HOST:12345
>     > User-Agent: curl/7.51.0
>     > Accept: */*
>     > Proxy-Connection: Keep-Alive
>     >
>     >
>     > ----------
>     > 2017/11/27 15:53:40.543| 85,2| client_side_request.cc(745)
>     > clientAccessCheckDone: The request GET http://HOST:12345/ is ALLOWED;
>     > last ACL checked: localhost
>     > 2017/11/27 15:53:40.543| 85,2| client_side_request.cc(721)
>     > clientAccessCheck2: No adapted_http_access configuration. default: ALLOW
>     > 2017/11/27 15:53:40.543| 85,2| client_side_request.cc(745)
>     > clientAccessCheckDone: The request GET http://HOST:12345/ is ALLOWED;
>     > last ACL checked: localhost
>     > 2017/11/27 15:53:40.543| 17,2| FwdState.cc(133) FwdState: Forwarding
>     > client request local=127.0.0.1:3128 <http://127.0.0.1:3128>
>     <http://127.0.0.1:3128>
>     > remote=127.0.0.1:53798 <http://127.0.0.1:53798>
>     <http://127.0.0.1:53798> FD 45 flags=1,
>     > url=http://HOST:12345/
>     > 2017/11/27 15:53:40.543| 44,2| peer_select.cc(258) peerSelectDnsPaths:
>     > Find IP destination for: http://HOST:12345/' via 127.0.0.1
>     > 2017/11/27 15:53:40.543| 44,2| peer_select.cc(280) peerSelectDnsPaths:
>     > Found sources for 'http://HOST:12345/'
>     > 2017/11/27 15:53:40.543| 44,2| peer_select.cc(281) peerSelectDnsPaths:?
>     > ?always_direct = DENIED
>     > 2017/11/27 15:53:40.543| 44,2| peer_select.cc(282) peerSelectDnsPaths:?
>     > ? never_direct = ALLOWED
>     > 2017/11/27 15:53:40.543| 44,2| peer_select.cc(292) peerSelectDnsPaths:?
>     > ? ? cache_peer = local=127.0.0.3 remote=127.0.0.1:18070 <http://127.0.0.1:18070>
>     > <http://127.0.0.1:18070> flags=1
>     > 2017/11/27 15:53:40.543| 44,2| peer_select.cc(295) peerSelectDnsPaths:?
>     > ? ? ? timedout = 0
>     > 2017/11/27 15:53:40.543| 11,2| http.cc(2229) sendRequest: HTTP Server
>     > local=127.0.0.3:57091 <http://127.0.0.3:57091>
>     <http://127.0.0.3:57091> remote=127.0.0.1:18070 <http://127.0.0.1:18070>
>     > <http://127.0.0.1:18070> FD 40 flags=1
>     > 2017/11/27 15:53:40.543| 11,2| http.cc(2230) sendRequest: HTTP Server
>     > REQUEST:
>     > ---------
>     > GET http://HOST:12345/ HTTP/1.1
>     > User-Agent: curl/7.51.0
>     > Accept: */*
>     > Host: HOST:12345
>     > Cache-Control: max-age=259200
>     > Connection: keep-alive
>     >
>     >
>     > ----------
>     >
>     > [SKIPPED 40 seconds until parent closes TCP connection with FIN,ACK]
>     >
>     > 2017/11/27 15:54:20.627| 11,2| http.cc(1299) continueAfterParsingHeader:
>     > WARNING: HTTP: Invalid Response: No object data received for
>     > http://HOST:12345/ AKA HOST/
>     > 2017/11/27 15:54:20.627| 17,2| FwdState.cc(655)
>     > handleUnregisteredServerEnd: self=0x3e31838*2 err=0x409b338
>     > http://HOST:12345/
>     > 2017/11/27 15:54:20.627| 11,2| http.cc(2229) sendRequest: HTTP Server
>     > local=127.0.0.3:41355 <http://127.0.0.3:41355>
>     <http://127.0.0.3:41355> remote=127.0.0.1:18070 <http://127.0.0.1:18070>
>     > <http://127.0.0.1:18070> FD 40 flags=1
>     > 2017/11/27 15:54:20.627| 11,2| http.cc(2230) sendRequest: HTTP Server
>     > REQUEST:
>     > ---------
>     > GET http://HOST:12345/ HTTP/1.1
>     > User-Agent: curl/7.51.0
>     > Accept: */*
>     > Host: HOST:12345
>     > Cache-Control: max-age=259200
>     > Connection: keep-alive
>     >
>     >
>     > ----------
>     >
>     > [SKIPPED 40 seconds again until parent closes TCP connection with FIN,ACK]
>     >
>     > 2017/11/27 15:55:00.728| ctx: enter level? 0: 'http://HOST:12345/'
>     > 2017/11/27 15:55:00.728| 11,2| http.cc(719) processReplyHeader: HTTP
>     > Server local=127.0.0.3:41355 <http://127.0.0.3:41355>
>     <http://127.0.0.3:41355>
>     > remote=127.0.0.1:18070 <http://127.0.0.1:18070>
>     <http://127.0.0.1:18070> FD 40 flags=1
>     > 2017/11/27 15:55:00.728| 11,2| http.cc(720) processReplyHeader: HTTP
>     > Server REPLY:
>     > ---------
>     > HTTP/1.0 502 Bad Gateway
>     > Cache-Control: no-cache
>     > Connection: close
>     > Content-Type: text/html
>     >
>     > <html><body><h1>502 Bad Gateway</h1>
>     > The server returned an invalid or incomplete response.
>     > </body></html>
>     >
>     > ----------
>     > 2017/11/27 15:55:00.728| ctx: exit level? 0
>     > 2017/11/27 15:55:00.728| 20,2| store.cc(996) checkCachable:
>     > StoreEntry::checkCachable: NO: not cachable
>     > 2017/11/27 15:55:00.728| 20,2| store.cc(996) checkCachable:
>     > StoreEntry::checkCachable: NO: not cachable
>     > 2017/11/27 15:55:00.728| 88,2| client_side_reply.cc(2073)
>     > processReplyAccessResult: The reply for GET http://HOST:12345/ is
>     > ALLOWED, because it matched (access_log stdio:/var/log/squid/access.log
>     > line)
>     > 2017/11/27 15:55:00.728| 11,2| client_side.cc(1409) sendStartOfMessage:
>     > HTTP Client local=127.0.0.1:3128 <http://127.0.0.1:3128>
>     <http://127.0.0.1:3128>
>     > remote=127.0.0.1:53798 <http://127.0.0.1:53798>
>     <http://127.0.0.1:53798> FD 45 flags=1
>     > 2017/11/27 15:55:00.728| 11,2| client_side.cc(1410)
>     sendStartOfMessage:
>     > HTTP Client REPLY:
>     > ---------
>     > HTTP/1.1 502 Bad Gateway
>     > Date: Mon, 27 Nov 2017 23:54:20 GMT
>     > Cache-Control: no-cache
>     > Content-Type: text/html
>     > X-Cache: MISS from ip-172-23-18-130
>     > X-Cache-Lookup: MISS from ip-172-23-18-130:3128
>     > Transfer-Encoding: chunked
>     > Connection: keep-alive
>     >
>     >
>     > ----------
> 
> 
> 
> 
> -- 
> With best regards, Ivan Larionov.



From hoangminhung at gmail.com  Wed Nov 29 12:34:38 2017
From: hoangminhung at gmail.com (=?UTF-8?B?bWluaCBoxrBuZyDEkeG7lyBob8Ogbmc=?=)
Date: Wed, 29 Nov 2017 19:34:38 +0700
Subject: [squid-users] Fwd: [Squid-3.5.20]Squid transparent proxy
 http/https without client site config
In-Reply-To: <CAO0fBEp7gg0KpDCC4TxqK9u4eSKr2hee1DoWEVsnVRd19LUjPQ@mail.gmail.com>
References: <CAO0fBEpZvu=GFP0q=XzdzQMxBq629YA901MZFGEQbbLXmB9ZHg@mail.gmail.com>
 <CAO0fBEp1ni6_RYnX-jnwggnr7StHvB+5-X8a9c3O6Fms-3S0uQ@mail.gmail.com>
 <67f77ee1-3266-4171-b669-39665f9cb72a@treenet.co.nz>
 <CAO0fBEr+8=Ehdgb6nzXS2Rzrm-fADbwh-Z+yE7qirdgU-8A8eQ@mail.gmail.com>
 <ead14327-92a2-c89e-1782-6c1b489216d2@treenet.co.nz>
 <CAO0fBEpUoORHnYBkHh8jcAOByfpT8vpw628=TW2t94xAWTDMKA@mail.gmail.com>
 <CAO0fBEp7gg0KpDCC4TxqK9u4eSKr2hee1DoWEVsnVRd19LUjPQ@mail.gmail.com>
Message-ID: <CAO0fBEpRN+R2zxBQQb4iK9Q5iusdcYFAtxd5SLyHVngJj-dmbg@mail.gmail.com>

Dear Amos,
Sorry for concluded hurriedly.
When i do a test with 1 user, it's seem ok, no more Aler from cache.log.
But when i test with more users, the Alert log from cache.log happen again.
And so i can't access some https page as chatwork.com , facebook.com

2017/11/29 18:06:41 kid1| SECURITY ALERT: Host header forgery detected on
local=54.238.137.130:443 remote=172.16.255.10:61831 FD 131 flags=33 (local
IP does not match any domain IP)
2017/11/29 18:06:41 kid1| SECURITY ALERT: on URL: www.chatwork.com:443
2017/11/29 18:06:48 kid1| SECURITY ALERT: Host header forgery detected on
local=31.13.95.8:443 remote=172.16.255.51:54984 FD 173 flags=33 (local IP
does not match any domain IP)
2017/11/29 18:06:48 kid1| SECURITY ALERT: on URL: api.facebook.com:443
2017/11/29 18:08:07 kid1| SECURITY ALERT: Host header forgery detected on
local=31.13.95.12:443 remote=172.16.255.51:54990 FD 51 flags=33 (local IP
does not match any domain IP)
2017/11/29 18:08:07 kid1| SECURITY ALERT: on URL: static.xx.fbcdn.net:443
2017/11/29 18:08:50 kid1| SECURITY ALERT: Host header forgery detected on
local=172.217.24.197:443 remote=172.16.255.10:61866 FD 34 flags=33 (local
IP does not match any domain IP)
2017/11/29 18:08:50 kid1| SECURITY ALERT: on URL: mail.google.com:443
2017/11/29 18:09:43 kid1| SECURITY ALERT: Host header forgery detected on
local=13.113.80.172:443 remote=172.16.255.10:61890 FD 124 flags=33 (local
IP does not match any domain IP)
2017/11/29 18:09:43 kid1| SECURITY ALERT: on URL: ws-chatwork.pusher.com:443
2017/11/29 18:10:59 kid1| WARNING: 1 swapin MD5 mismatches
2017/11/29 18:11:00 kid1| SECURITY ALERT: Host header forgery detected on
local=157.240.15.22:443 remote=172.16.255.51:55032 FD 93 flags=33 (local IP
does not match any domain IP)
2017/11/29 18:11:00 kid1| SECURITY ALERT: on URL: connect.facebook.net:443
2017/11/29 18:13:15 kid1| SECURITY ALERT: Host header forgery detected on
local=31.13.95.36:443 remote=172.16.255.12:33158 FD 25 flags=33 (local IP
does not match any domain IP)
2017/11/29 18:13:15 kid1| SECURITY ALERT: on URL: www.facebook.com:443
2017/11/29 18:14:00 kid1| SECURITY ALERT: Host header forgery detected on
local=31.13.95.34:443 remote=172.16.255.59:39526 FD 74 flags=33 (local IP
does not match any domain IP)
2017/11/29 18:14:00 kid1| SECURITY ALERT: on URL: mqtt-mini.facebook.com:443


I have a Mikrotik router (172.16.1.1), and some Lan Local. With every Lan,
my DHCP allocates DNS, gateway to my LAN. Ext : 172.16.255.0/24 with
gateway : 172.16.255.254 and DNS 172.16.255.254
- Mikrotik config with Cache DNS from 8.8.8.8
- Squid use DNS 172.16.1.1 ( Mikrotik DNS)
- Squid config DNS to 172.16.1.1
- Client use DNS allocated by DHCP (but there is still Mikrotik router)

Here is my full squid.conf :

#Allollow LAN Network

# Allow Network ACL Allow/Deny Section#
acl SSL_ports port 443
acl Safe_ports port 80
acl Safe_ports port 443
acl Safe_ports port 1025-65535

acl CONNECT method CONNECT
acl fb dstdomain .facebook.com

#http_access deny CONNECT fb

http_access allow localhost
http_access allow all


# Transparent Proxy Parameters
http_port 3130
http_port 3128 intercept
https_port 3129 intercept ssl-bump generate-host-certificates=off
cert=/etc/squid/ssl_cert/squid-3.5.27.pem

### SSL config ###
#-Start-#
#ssl_bump none all
 acl step1 at_step SslBump1
 ssl_bump peek step1
 ssl_bump splice all
#-End-#

# --------- Add X-Forwarded-for in headers [0]?
#-Start-#
forwarded_for transparent
#-End-#

debug_options ALL,1

log_fqdn on
emulate_httpd_log on
icap_enable on

global_internal_static on
short_icon_urls on
log_uses_indirect_client         on


# --------- DNS AND IP CACHES [4341]

dns_nameservers 172.16.1.1
dns_v4_first on
host_verify_strict off
ignore_unknown_nameservers off
dns_timeout 120 seconds
ipcache_size 1024
ipcache_low 90
ipcache_high 95
fqdncache_size 1024
positive_dns_ttl 6 hours
negative_dns_ttl 300 seconds
---------------------------------------------------------

Could you please help me . Thanks & Best Regards,

2017-11-28 17:32 GMT+07:00 minh h?ng ?? ho?ng <hoangminhung at gmail.com>:

> Dear Amos,
> I solved my problem by following this :
> 1 - I used my Mikrotik router as a cache DNS
> 2 - Both Squid proxy and my client use Mikrotik' DNS
>
> => It no more take alert from cache.log
>
> Thanks alot :)
> --
> Thanks & Best Regards,
> --------------
> ?? Ho?ng Minh H?ng
> Gmail : hoangminhung at gmail.com
> S?T : 01234454115
>



-- 
Thanks & Best Regards,
--------------
?? Ho?ng Minh H?ng
Gmail : hoangminhung at gmail.com
S?T : 01234454115
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20171129/45614bc1/attachment.htm>

From squid3 at treenet.co.nz  Wed Nov 29 13:29:59 2017
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 30 Nov 2017 02:29:59 +1300
Subject: [squid-users] Fwd: [Squid-3.5.20]Squid transparent proxy
 http/https without client site config
In-Reply-To: <CAO0fBEpRN+R2zxBQQb4iK9Q5iusdcYFAtxd5SLyHVngJj-dmbg@mail.gmail.com>
References: <CAO0fBEpZvu=GFP0q=XzdzQMxBq629YA901MZFGEQbbLXmB9ZHg@mail.gmail.com>
 <CAO0fBEp1ni6_RYnX-jnwggnr7StHvB+5-X8a9c3O6Fms-3S0uQ@mail.gmail.com>
 <67f77ee1-3266-4171-b669-39665f9cb72a@treenet.co.nz>
 <CAO0fBEr+8=Ehdgb6nzXS2Rzrm-fADbwh-Z+yE7qirdgU-8A8eQ@mail.gmail.com>
 <ead14327-92a2-c89e-1782-6c1b489216d2@treenet.co.nz>
 <CAO0fBEpUoORHnYBkHh8jcAOByfpT8vpw628=TW2t94xAWTDMKA@mail.gmail.com>
 <CAO0fBEp7gg0KpDCC4TxqK9u4eSKr2hee1DoWEVsnVRd19LUjPQ@mail.gmail.com>
 <CAO0fBEpRN+R2zxBQQb4iK9Q5iusdcYFAtxd5SLyHVngJj-dmbg@mail.gmail.com>
Message-ID: <b96f1945-e7cd-72b9-0d93-5f4644d24555@treenet.co.nz>

On 30/11/17 01:34, minh h?ng ?? ho?ng wrote:
> Dear Amos,
> Sorry for concluded hurriedly.
> When i do a test with 1 user, it's seem ok, no more Aler from cache.log. 
> But when i test with more users, the Alert log from cache.log happen 
> again. And so i can't access some https page as chatwork.com , facebook.com.


You are understanding that this is a log entry that cannot be completely 
removed right? the problem can only be reduced in how much damage is 
done, not fixed.

Also be aware that the cache.log records every security event. Even when 
the user does not see anything unusual because Squid sends them 
transparently to the server they were trying to contact as if the proxy 
was not there (real transparency).

You seem to be doing everything that can be done about the connectivity 
issues related to that log message.


I suspect that any remaining issues you are now having with those HTTPS 
sites is a separate problem with the Squid-3 SSL-Bump code or TLS 
protocol itself. You need to take a closer look at the exact 
transactions that are going on with those remaining problem sites.

If the problem turns out to be anything in the TLS protocol messages the 
'splice' action that your Squid is currently doing means that type of 
problem has nothing to do with Squid. It is the client and server 
endpoints having the issue between themselves.

You could also try out Squid 3.5.27 or Squid-4 code for a more up to 
date SSL-Bump implementation. There are a few changes to how the 
connection management works that might show up as weird problems in 
Squid-3 despite the splice. Even the 7 months between your 3.5.20 and 
3.5.27 has a few of those.

Amos


From squid3 at treenet.co.nz  Wed Nov 29 14:29:26 2017
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 30 Nov 2017 03:29:26 +1300
Subject: [squid-users] Working peek/splice no longer functioning on some
 sites
In-Reply-To: <1511794211.2147.4.camel@slave-tothe-box.net>
References: <1511551841.2392.5.camel@slave-tothe-box.net>
 <2ea83011-92bc-f134-e662-2f1fd34fce89@treenet.co.nz>
 <1511610749.2338.1.camel@slave-tothe-box.net>
 <57c8c167-dfae-6579-8f75-c65ae3160c0e@treenet.co.nz>
 <1511613959.2338.6.camel@slave-tothe-box.net>
 <CABMULtJZUh+i+QSNOYP6tPhS6Y57d+WVQPidCMyNQ7b6GR-UzQ@mail.gmail.com>
 <1511794211.2147.4.camel@slave-tothe-box.net>
Message-ID: <7e3e81c9-0949-9f16-f1f8-7efc052dae78@treenet.co.nz>

On 28/11/17 03:50, James Lay wrote:
> On Sun, 2017-11-26 at 09:50 +0200, Alex K wrote:
>> Perhaps an alternative is to peek only on step1:
>>
>> acl step1 at_step SslBump1
>>
>> ssl_bump peek step1
>> acl allowed_https_sites ssl::server_name_regex 
>> "/opt/etc/squid/http_url.txt"
>> ssl_bump splice allowed_https_sites
>> ssl_bump terminate all
> 
> Hrmm...wouldn't that negate the ability to read the cert on step2?
> 

Yes it would.

> In layman's terms I'm thinking:
> "peek at step1"
> "splice acl allow matched sni's"
> "peek at step2"
> "splice acl allow'd matched certs"
> "terminate the rest"
> 
> Would that work Amos?
> 

This is essentially what I suggested at the beginning.

Placing splice action and your ACLs on the first ssl_bump line ensures 
that at each step if enough details are known to splice it will happen.

The second line being "peek all" make peek happen at every step for 
which it is possible (step 1 and step 2 - not step 3).

"terminate all" being last makes it happen for "all the rest", aka step 
3 if Squid gets that far without splicing.


The only difference is that my suggested way would also allow splicing 
the CONNECT if it happens to be presented with a host name in the 
authority-URI. Which cannot happen on your proxy unless your port 3128 
happens to be intercepting traffic between clients and another proxy.


BTW please do not use port 3128 for intercept. It is officially 
registered for HTTP proxy traffic and so qualifies as "well known".

Amos


From squid3 at treenet.co.nz  Wed Nov 29 14:30:50 2017
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 30 Nov 2017 03:30:50 +1300
Subject: [squid-users] Transparent Squid
In-Reply-To: <20171128130343.GA8443@fantomas.sk>
References: <CAFY5H86BkncuZwj=_eJc=XK4eThfM+yAipxp19qr19Ok-qJgkw@mail.gmail.com>
 <20171128130343.GA8443@fantomas.sk>
Message-ID: <b88b816e-04da-b94a-20de-0d9022a01263@treenet.co.nz>

On 29/11/17 02:03, Matus UHLAR - fantomas wrote:
> On 27.11.17 11:24, LINGYUN ZHAO wrote:
>> I need Squid as a real 'transparent' proxy on Fedora without changing 5
>> tuples. Is it possible?
> 
> tuples?
> 

The technical name for an entry in the system NAT table is a 3-tuple or 
4-tuple.

Though what a "5 tuples" means interests me. There are not 5 IP and port 
details in a TCP message. Only two of each.



>> The setup is simple as Client ---------- Fedora --------Server
> 
> is fedora NAT device and also running squid?
> 
>> The Squid version is 3.5.20.The key configuration on Squid as below:
>>
>> ? http_port 0.0.0.0:3128 transparent
>>
>> ? acl localnet src 10.0.0.0/24
>>
>> ? http_access allow localnet
>>
>> And I configured a NAT on Fedora.
>>
>> ? iptables -t nat -A PREROUTING -i eth1 -p tcp --dport 80 -j DNAT --to
>> 10.0.0.1:3128
> 
> this only works if "fedora" does the NAT and runs squid.
> (just for sure)
> 
>> When I run curl on Client to server. I found the server receives the
>> traffic with Fedora's IP address and different source port, instead of
>> Client IP address and original source port.
> 
> when you redirect traffic tyo the squid and squid connect to the server,
> it's logical that server sees squid's IP.

Not just logical. That is how NAT works and why "transparent" is such a 
wrong way to describe NAT interception.


> 
> if you want to keep source client's IP, you need tproxy:
> https://wiki.squid-cache.org/Features/Tproxy4
> 

Indeed.

  ... and to answer what appears to be the original question directly:

   No you cannot make Squid use both src-IP *and* src-port from the 
client details. They are already being used by the client TCP connection.


All you can do is pick between one of them or neither.

For "neither" use NAT. NAT interception is *not* transparent - the Squid 
machines IP:port always used on outgoing traffic to servers. Otherwise 
you break the routing system or the NAT system.


For "one" use TPROXY. T(ransparent)PROXY uses only the client src-IP. 
The difference in src-port is the only way to distinguish the packets 
arriving into the Squid machine from server.

Amos


From xeron.oskom at gmail.com  Wed Nov 29 21:15:33 2017
From: xeron.oskom at gmail.com (Ivan Larionov)
Date: Wed, 29 Nov 2017 13:15:33 -0800
Subject: [squid-users] forward_max_tries 1 has no effect
In-Reply-To: <20b1fce2-6677-fd06-2bf1-4e74f05e214c@measurement-factory.com>
References: <CAHvB88zR7wxUjMGEdrqELaMW3+pQq21hAkJR__4pWXendnmhzQ@mail.gmail.com>
 <fe6c5513-20c9-4b41-467a-9d199e38f20b@treenet.co.nz>
 <7E5CA2C7-96B6-4535-88D2-0176B0D10EC4@gmail.com>
 <c88e0e11-bc43-5c21-20e7-bf95dad0b212@treenet.co.nz>
 <CAHvB88wx=yjJyKrrQojEAF4Cz1RsR3pdrvYZCB2mWMDv58tttw@mail.gmail.com>
 <4e0b3811-2965-32d7-fe70-a6987ced93f8@measurement-factory.com>
 <CAHvB88yEgcfYNrLYDENCXm2me=jguqgj-9FB9KdtiJV9catORQ@mail.gmail.com>
 <20b1fce2-6677-fd06-2bf1-4e74f05e214c@measurement-factory.com>
Message-ID: <CAHvB88wh+sy7WNSbBcpDuX82YXtUtOTUqdS11WdK1DNpm-LSzw@mail.gmail.com>

Thanks Alex.

Unfortunately I don't have enough C/C++ skills to fix it.

I've created a bug report ?
https://bugs.squid-cache.org/show_bug.cgi?id=4788

We've also changed parent behavior so it will not silently close the
connection but will return 502 in this exact situation and seems like it
fixes unexpected squid re-forward.

On Tue, Nov 28, 2017 at 7:12 PM, Alex Rousskov <
rousskov at measurement-factory.com> wrote:

> On 11/28/2017 02:27 PM, Ivan Larionov wrote:
>
> > Another interesting fact is that I can't reproduce this issue if squid
> > has no other traffic except my testing requests. But it's easy to
> > reproduce when server has other traffic.
>
> I did not check your logs carefully, but I believe that (when things do
> not work the way you expect) your Squid is retrying a failed persistent
> connection (rather than re-forwarding the request after receiving a bad
> response). See the "pconn race happened" line below:
>
> > 1_retry.log:2017/11/28 12:55:12.731| 17,3| FwdState.cc(416) fail:
> ERR_ZERO_SIZE_OBJECT "Bad Gateway"
> > 1_retry.log:2017/11/28 12:55:12.731| 17,5| FwdState.cc(430) fail: pconn
> race happened
> > 1_retry.log:2017/11/28 12:55:12.731| 93,5| AsyncJob.cc(84) mustStop:
> HttpStateData will stop, reason: HttpStateData::continueAfterParsingHeader
> > 1_retry.log:2017/11/28 12:55:12.731| 17,3| FwdState.cc(618) retryOrBail:
> re-forwarding (1 tries, 40 secs)
> > 1_retry.log:2017/11/28 12:55:12.731| 17,4| FwdState.cc(621) retryOrBail:
> retrying the same destination
>
>
> Assuming you tested with forward_max_tries set to 1, the retryOrBail
> lines above confirm the off-by-one problem I was describing in my
> previous response.
>
> AFAICT, compensating by setting forward_max_tries to zero will _not_
> work (for reasons unrelated to the problem at hand).
>
>
> FWIW, your current options include those outlined at
> http://wiki.squid-cache.org/SquidFaq/AboutSquid#How_to_
> add_a_new_Squid_feature.2C_enhance.2C_of_fix_something.3F
>
>
> Alex.
>
>
> > On Tue, Nov 28, 2017 at 7:32 AM, Alex Rousskov wrote:
> >
> >     On 11/27/2017 05:19 PM, Ivan Larionov wrote:
> >
> >     > I see retries only when squid config has 2 parents. If I comment
> out
> >     > everything related to "newproxy" I can't reproduce this behavior
> anymore.
> >
> >     The posted logs are not detailed enough to confirm or deny that IMO,
> but
> >     I suspect that you are dealing with at least one bug.
> >
> >
> >     > https://wiki.squid-cache.org/SquidFaq/InnerWorkings#When_
> does_Squid_re-forward_a_client_request.3F
> >     <https://wiki.squid-cache.org/SquidFaq/InnerWorkings#When_
> does_Squid_re-forward_a_client_request.3F>
> >     >
> >     >> Squid does not try to re-forward a request if at least one of the
> following conditions is true:
> >     >>
> >     >> The number of forwarding attempts exceeded forward_max_tries. For
> >     >> example, if you set forward_max_tries to 1 (one), then no requests
> >     >> will be re-forwarded.
> >
> >
> >     AFAICT, there is an off-by-one bug in Squid that violates the above:
> >
> >     >     if (n_tries > Config.forward_max_tries)
> >     >         return false;
> >
> >     The n_tries counter is incremented before Squid makes a request
> >     forwarding attempt. With n_tries and Config.forward_max_tries both
> set
> >     to 1, the quoted FwdState::checkRetry() code will not prevent
> >     re-forwarding. There is a similar problem in FwdState::reforward().
> This
> >     reasoning needs confirmation/testing.
> >
> >     Please note that simply changing the ">" operator to ">=" may break
> >     other things in a difficult-to-detect-by-simple-tests ways. The
> correct
> >     fix may be more complex than it looks and may involve making policy
> >     decisions regarding forward_max_tries meaning. The best fix would
> remove
> >     checkRetry() and reforward() duplication. This code is difficult to
> work
> >     with; many related code names are misleading.
> >
> >
> >     >> Squid has no alternative destinations to try. Please note that
> >     >> alternative destinations may include multiple next hop IP
> addresses
> >     >> and multiple peers.
> >
> >     The fact that Squid sends two requests to the same peer with only one
> >     peer address selected suggests that Squid is retrying a failed
> >     persistent connection rather than re-forwarding after receiving a bad
> >     response. Again, the logs are not detailed enough to distinguish the
> two
> >     cases. I can only see that a single peer/destination address was
> >     selected (not two), which is correct/expected behavior. I cannot see
> >     what happened next with sufficient detail.
> >
> >     Going forward, you have several options, including:
> >
> >     A. Post a link to compressed ALL,7+ logs to confirm bug(s).
> >     B. Fix the broken condition(s) in FwdState. See above.
> >
> >     HTH,
> >
> >     Alex.
> >
> >
> >     > 2017/11/27 15:53:40.542| 5,2| TcpAcceptor.cc(220) doAccept: New
> connection on FD 15
> >     > 2017/11/27 15:53:40.542| 5,2| TcpAcceptor.cc(295) acceptNext:
> connection
> >     > on local=0.0.0.0:3128 <http://0.0.0.0:3128> <http://0.0.0.0:3128>
> >     remote=[::] FD 15 flags=9
> >     > 2017/11/27 15:53:40.543| 11,2| client_side.cc(2372)
> parseHttpRequest:
> >     > HTTP Client local=127.0.0.1:3128 <http://127.0.0.1:3128>
> >     <http://127.0.0.1:3128>
> >     > remote=127.0.0.1:53798 <http://127.0.0.1:53798>
> >     <http://127.0.0.1:53798> FD 45 flags=1
> >     > 2017/11/27 15:53:40.543| 11,2| client_side.cc(2373)
> parseHttpRequest:
> >     > HTTP Client REQUEST:
> >     > ---------
> >     > GET http://HOST:12345/ HTTP/1.1
> >     > Host: HOST:12345
> >     > User-Agent: curl/7.51.0
> >     > Accept: */*
> >     > Proxy-Connection: Keep-Alive
> >     >
> >     >
> >     > ----------
> >     > 2017/11/27 15:53:40.543| 85,2| client_side_request.cc(745)
> >     > clientAccessCheckDone: The request GET http://HOST:12345/ is
> ALLOWED;
> >     > last ACL checked: localhost
> >     > 2017/11/27 15:53:40.543| 85,2| client_side_request.cc(721)
> >     > clientAccessCheck2: No adapted_http_access configuration. default:
> ALLOW
> >     > 2017/11/27 15:53:40.543| 85,2| client_side_request.cc(745)
> >     > clientAccessCheckDone: The request GET http://HOST:12345/ is
> ALLOWED;
> >     > last ACL checked: localhost
> >     > 2017/11/27 15:53:40.543| 17,2| FwdState.cc(133) FwdState:
> Forwarding
> >     > client request local=127.0.0.1:3128 <http://127.0.0.1:3128>
> >     <http://127.0.0.1:3128>
> >     > remote=127.0.0.1:53798 <http://127.0.0.1:53798>
> >     <http://127.0.0.1:53798> FD 45 flags=1,
> >     > url=http://HOST:12345/
> >     > 2017/11/27 15:53:40.543| 44,2| peer_select.cc(258)
> peerSelectDnsPaths:
> >     > Find IP destination for: http://HOST:12345/' via 127.0.0.1
> >     > 2017/11/27 15:53:40.543| 44,2| peer_select.cc(280)
> peerSelectDnsPaths:
> >     > Found sources for 'http://HOST:12345/'
> >     > 2017/11/27 15:53:40.543| 44,2| peer_select.cc(281)
> peerSelectDnsPaths:
> >     >  always_direct = DENIED
> >     > 2017/11/27 15:53:40.543| 44,2| peer_select.cc(282)
> peerSelectDnsPaths:
> >     >   never_direct = ALLOWED
> >     > 2017/11/27 15:53:40.543| 44,2| peer_select.cc(292)
> peerSelectDnsPaths:
> >     >     cache_peer = local=127.0.0.3 remote=127.0.0.1:18070 <
> http://127.0.0.1:18070>
> >     > <http://127.0.0.1:18070> flags=1
> >     > 2017/11/27 15:53:40.543| 44,2| peer_select.cc(295)
> peerSelectDnsPaths:
> >     >       timedout = 0
> >     > 2017/11/27 15:53:40.543| 11,2| http.cc(2229) sendRequest: HTTP
> Server
> >     > local=127.0.0.3:57091 <http://127.0.0.3:57091>
> >     <http://127.0.0.3:57091> remote=127.0.0.1:18070 <
> http://127.0.0.1:18070>
> >     > <http://127.0.0.1:18070> FD 40 flags=1
> >     > 2017/11/27 15:53:40.543| 11,2| http.cc(2230) sendRequest: HTTP
> Server
> >     > REQUEST:
> >     > ---------
> >     > GET http://HOST:12345/ HTTP/1.1
> >     > User-Agent: curl/7.51.0
> >     > Accept: */*
> >     > Host: HOST:12345
> >     > Cache-Control: max-age=259200
> >     > Connection: keep-alive
> >     >
> >     >
> >     > ----------
> >     >
> >     > [SKIPPED 40 seconds until parent closes TCP connection with
> FIN,ACK]
> >     >
> >     > 2017/11/27 15:54:20.627| 11,2| http.cc(1299)
> continueAfterParsingHeader:
> >     > WARNING: HTTP: Invalid Response: No object data received for
> >     > http://HOST:12345/ AKA HOST/
> >     > 2017/11/27 15:54:20.627| 17,2| FwdState.cc(655)
> >     > handleUnregisteredServerEnd: self=0x3e31838*2 err=0x409b338
> >     > http://HOST:12345/
> >     > 2017/11/27 15:54:20.627| 11,2| http.cc(2229) sendRequest: HTTP
> Server
> >     > local=127.0.0.3:41355 <http://127.0.0.3:41355>
> >     <http://127.0.0.3:41355> remote=127.0.0.1:18070 <
> http://127.0.0.1:18070>
> >     > <http://127.0.0.1:18070> FD 40 flags=1
> >     > 2017/11/27 15:54:20.627| 11,2| http.cc(2230) sendRequest: HTTP
> Server
> >     > REQUEST:
> >     > ---------
> >     > GET http://HOST:12345/ HTTP/1.1
> >     > User-Agent: curl/7.51.0
> >     > Accept: */*
> >     > Host: HOST:12345
> >     > Cache-Control: max-age=259200
> >     > Connection: keep-alive
> >     >
> >     >
> >     > ----------
> >     >
> >     > [SKIPPED 40 seconds again until parent closes TCP connection with
> FIN,ACK]
> >     >
> >     > 2017/11/27 15:55:00.728| ctx: enter level  0: 'http://HOST:12345/'
> >     > 2017/11/27 15:55:00.728| 11,2| http.cc(719) processReplyHeader:
> HTTP
> >     > Server local=127.0.0.3:41355 <http://127.0.0.3:41355>
> >     <http://127.0.0.3:41355>
> >     > remote=127.0.0.1:18070 <http://127.0.0.1:18070>
> >     <http://127.0.0.1:18070> FD 40 flags=1
> >     > 2017/11/27 15:55:00.728| 11,2| http.cc(720) processReplyHeader:
> HTTP
> >     > Server REPLY:
> >     > ---------
> >     > HTTP/1.0 502 Bad Gateway
> >     > Cache-Control: no-cache
> >     > Connection: close
> >     > Content-Type: text/html
> >     >
> >     > <html><body><h1>502 Bad Gateway</h1>
> >     > The server returned an invalid or incomplete response.
> >     > </body></html>
> >     >
> >     > ----------
> >     > 2017/11/27 15:55:00.728| ctx: exit level  0
> >     > 2017/11/27 15:55:00.728| 20,2| store.cc(996) checkCachable:
> >     > StoreEntry::checkCachable: NO: not cachable
> >     > 2017/11/27 15:55:00.728| 20,2| store.cc(996) checkCachable:
> >     > StoreEntry::checkCachable: NO: not cachable
> >     > 2017/11/27 15:55:00.728| 88,2| client_side_reply.cc(2073)
> >     > processReplyAccessResult: The reply for GET http://HOST:12345/ is
> >     > ALLOWED, because it matched (access_log
> stdio:/var/log/squid/access.log
> >     > line)
> >     > 2017/11/27 15:55:00.728| 11,2| client_side.cc(1409)
> sendStartOfMessage:
> >     > HTTP Client local=127.0.0.1:3128 <http://127.0.0.1:3128>
> >     <http://127.0.0.1:3128>
> >     > remote=127.0.0.1:53798 <http://127.0.0.1:53798>
> >     <http://127.0.0.1:53798> FD 45 flags=1
> >     > 2017/11/27 15:55:00.728| 11,2| client_side.cc(1410)
> >     sendStartOfMessage:
> >     > HTTP Client REPLY:
> >     > ---------
> >     > HTTP/1.1 502 Bad Gateway
> >     > Date: Mon, 27 Nov 2017 23:54:20 GMT
> >     > Cache-Control: no-cache
> >     > Content-Type: text/html
> >     > X-Cache: MISS from ip-172-23-18-130
> >     > X-Cache-Lookup: MISS from ip-172-23-18-130:3128
> >     > Transfer-Encoding: chunked
> >     > Connection: keep-alive
> >     >
> >     >
> >     > ----------
> >
> >
> >
> >
> > --
> > With best regards, Ivan Larionov.
>
>


-- 
With best regards, Ivan Larionov.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20171129/91eb60c6/attachment.htm>

From carlosd2002 at hotmail.com  Thu Nov 30 18:32:21 2017
From: carlosd2002 at hotmail.com (carlos)
Date: Thu, 30 Nov 2017 11:32:21 -0700 (MST)
Subject: [squid-users] squid - kid registration timed out
Message-ID: <1512066741917-0.post@n4.nabble.com>

Hi all,

I m new to squid and i m installing it i na RHEL 7.4, squid version 3.5.20.

I have a setup with 3 squid instances listening in 3 diferent ports (but
other than they are basically the same), each instance have 4 workers. 

The problem is when i restart the squid ou after some random time i get :

in cache log:
FATAL: kid3 registration timed out
Squid Cache (Version 3.5.20): Terminated abnormally.
CPU Usage: 0.030 seconds = 0.020 user + 0.010 sys
Maximum Resident Size: 67648 KB
Page faults with physical i/o: 0

in messages log:
 (squid-3): kid3 registration timed out
squid[32613]: Squid Parent: (squid-3) process 32697 exited with status 1
Squid Parent: (squid-3) process 32697 will not be restarted due to repeated,
frequent failures

The squid other than that is working as aspected.

I read in some post that they pointed to:

https://wiki.squid-cache.org/Features/SmpScale
net.local.dgram.recvspace: 262144
net.local.dgram.maxdgram: 16384 

These parameters seems to be FreeBSD, and do not exists in RHEL, anyone had
this problem in RHEL? How to solve it?

the most near/relevant parameters that i found are:
        cat /proc/sys/net/core/rmem_max
        cat /proc/sys/net/core/wmem_max
?    cat /proc/sys/net/unix/max_dgram_qlen

Any idea if they will solve? or any recommended value?

NOTE: not using cache (deny cache all)

Chears,
Carlos



--
Sent from: http://squid-web-proxy-cache.1019090.n4.nabble.com/Squid-Users-f1019091.html


