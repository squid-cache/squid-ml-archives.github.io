From squid3 at treenet.co.nz  Fri Sep  1 02:36:13 2017
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Fri, 1 Sep 2017 14:36:13 +1200
Subject: [squid-users] acl problem (Amos Jeffries)
In-Reply-To: <c0d1291f-d842-140f-02e6-50951ea96bb7@dvm.esines.cu>
References: <c0d1291f-d842-140f-02e6-50951ea96bb7@dvm.esines.cu>
Message-ID: <d1b2f054-e37b-934a-4e82-f37cfa2c4039@treenet.co.nz>

On 01/09/17 00:44, Alex Guti?rrez Mart?nez wrote:
> Thanks for answering Mr. Jeffries, I just applied his recommendations, I 
> changed the "allow basic_ldap_auth" rule to "deny! Basic_ldap_auth",

Good.

> I 
> also left the acl names denied and removed their respective "acl deny 
> rule" and the rule "http_access deny I left it on the last line.

Hmm. I assume you are referring to the commenting out of the needless 
denies I mentioned. That looks okay now.

> Although I did not give problems the "squid3 -k parse". But the link to 
> the ldap suddenly stopped working, searching at 
> "http://www.squid-cache.org/Doc/config/" I saw that I had to change the 
> parameter "external_acl_type Group" to "external_acl_type ldap_group" .

No, you can use any name you like for that parameter.

The first parameter of the external_acl_type directive is just a custom 
name / label to refer to that particular external helper in the acl 
lines later.

For example:

  external_acl_type foo ...

  acl ... external foo ...


> The Ldap user password has not change and there are other applications 
> that are using the ldap correctly at this time, any sugestions?
> 

I see you also changed the rules giving permission for 'full' group to 
access the proxy. That change broke a few things.


> Here is a copy of my current configuration file
> 
> 
> #Escondemos la version del squid
> httpd_suppress_version_string on
> #nombre que queremos que muestre el squid como nuestro host
> visible_hostname Hermes
> #no permitimos que nada pase por nuestro proxy
> via off
> forwarded_for off
> follow_x_forwarded_for deny all
> #puertos que permitiremos
> acl SSL_ports port 443
> acl Safe_ports port 80 # http
> acl Safe_ports port 21 # ftp
> acl Safe_ports port 443 # https
> acl Safe_ports port 70 # gopher
> acl Safe_ports port 210 # wais
> acl Safe_ports port 1025-65535 # unregistered ports
> acl Safe_ports port 280 # http-mgmt
> acl Safe_ports port 488 # gss-http
> acl Safe_ports port 591 # filemaker
> acl Safe_ports port 777 # multiling http
> acl CONNECT method CONNECT
> http_access allow localhost manager
> http_access deny manager

You have another set of rules at the bottom of the config for manager 
access. These rules let sqstat etc through without logging in, the ones 
at the bottom require login.

If you need sqstat etc to login, then remove these manger lines.

If you need sqstat etc to get through without login. Then:
  * remove the above lines, and
  * move the sqstat rules from the bottom of the config up to just below 
the CONNECT rule below here.

> # Permitimos los puertos inseguros
> http_access allow !Safe_ports
> http_access allow CONNECT !SSL_ports

The above rules are supposed to be _preventing_ hacking attacks through 
your proxy. The default lines were very carefully designed to add that 
protection without overriding your local policies.
  The change to make the above use "allow" lets anybody through the 
proxy without any control (ouch).

Please return that to the default:
  http_access deny !Safe_ports
  http_access deny CONNECT !SSL_ports


Your rules for 'manger' ACL should go somewhere after these rules. (That 
Best Practice has changed recently, so the 3.3 default config does not 
do it right.)


> debug_options ALL,9
> ########################################################
> #auth ldap#
> ########################################################
> auth_param basic program /usr/lib/squid3/basic_ldap_auth -P? -R -b 
> "dc=empresa,dc=cuba,dc=cu" -D cn=ldap,ou=squid,dc=empresa,dc=cuba,dc=cu 
> -W /etc/squid3/clave.txt -f sAMAccountName=%s -v 3 -s sub -h 172.16.4.10
> external_acl_type Group %LOGIN /usr/lib/squid3/ext_ldap_group_acl -R -b 
> "dc=empresa,dc=cuba,dc=cu" -D 
> cn=cn=ldap,ou=squid,dc=empresa,dc=cuba,dc=cu -W /etc/squid3/clave.txt -f 
> "(&(objectclass=user)(sAMAccountName=%u) 
> (memberof=cn=%g,dc=empresa,dc=cuba,dc=cu))" -h 172.16.4.10

Is there actually a space in the middle of that -f parameter string?
I'm not very familiar with LDAP syntax, but the other configs I have 
seen using it do not have a space there.

NP: If it helps Squid understands line wrapping in squid.conf. Just add 
a '\' as the last character and some whitespace at the beginning of the 
next line. That can help avoid email wrap problems.


> #######################################################
> #auth que no funcionan y deben arreglarse
> ##########################################################
> auth_param basic children 10
> auth_param basic realm hermes.empresa.cuba.cu
> auth_param basic credentialsttl 2 hour
> acl basic_ldap_auth proxy_auth REQUIRED

> http_access deny !basic_ldap_auth
> #http_access deny all
> ########################################################
> #restricciones selectivas#
> ########################################################
> acl dmz src 172.16.4.0/27
> acl navegacion src 192.168.9.0/24
> acl full external Group InternetFull
> acl limitado external Group InternetLimitado
> acl sociales dstdomain -n "/etc/squid3/bloqueo/sociales"
> acl extensiones urlpath_regex -i "/etc/squid3/bloqueo/listaextensiones"
> http_access deny !full sociales
> http_access deny !full !limitado navegacion
> http_access deny !full dmz


These extra changes are adding some new problems.

Earlier you had some allow lines to let the 'full' group use the proxy. 
They were okay [assuming that was what you wanted], only the way they 
interacted with the login ACL was broken.

You do need some allow lines to tell Squid what to allow for logged in 
users. The order you need for best use of authentication is this:

  # rules for things that do not require authentication
  http_access allow/deny ...

  # require authentication to happen
  http_access deny !login

  # rules for authenticated users
  http_access allow/deny ...

  # prevent any other / unexpected access of the proxy
  http_access deny all


It may help if you write out your policy in human language statements. 
Being as simple as you can. Each statement will then usually be an 
http_access line and you can shuffle the order around until the config 
file 'reads' correctly to both you/us and Squid.

Note: if you find yourself writing 'except' or 'unless' that means there 
are probably going to be multiple http_access lines to match your policy 
statement, with the exception ones being ordered first.


For example reading your current rules:

 > http_access deny !full sociales

* "everyone not in group full are denied access to sociales domains"

 > http_access deny !full !limitado navegacion

* "everyone not in group full and not in group limitado and on a 
navegacion machine are denied"

  -> see how this is very clumsy to write in human language. That 
probably means a mistake and things could be simpler.

 > http_access deny !full dmz

* "everyone not in group full and coming from dmz are denied"


It is usually better to design in a way that avoids so many '!' / not 
statements. That is both easier for us humans to read and understand, 
and usually faster for Squid to process - especially when it has to 
pause the transaction and wait for a helper response on each ACL test.

eg. from what you have mentioned so far I think you want to end up with 
something like this:

  # ... some rules for anything 'full' group are denied ?

  # otherwise, 'full' group are allowed though unrestricted
  http_access allow full

  # ... things denied to everyone outside the 'full' group
  http_access deny dmz
  http_access deny sociales

  # ... navegacion are allowed if their user is in 'limitado' group
  #     (except to 'sociales' domains)
  http_access allow navegacion limitado

  # no more things are allowed
  http_access deny all



> ########################################################
> #restricciones obligadas#
> ########################################################
> #acl blacklist url_regex -i "/etc/squid3/listanegra"
> #http_access deny blacklist
> acl bl7 dstdomain -n "/etc/squid3/bloqueo/correos"
> #http_access allow full !limitado bl7
> acl bl1 url_regex -i "/etc/squid3/bloqueo/porno"
> #http_access deny bl1
> acl bl2 url_regex -i "/etc/squid3/bloqueo/android"
> #http_access deny bl2
> acl bl3 url_regex -i "/etc/squid3/bloqueo/prox1"
> #http_access deny bl3
> acl bl4 url_regex -i "/etc/squid3/bloqueo/prox2"
> #http_access deny bl4
> acl bl5 url_regex -i "/etc/squid3/bloqueo/prox3"
> #http_access deny bl5
> acl bl6 url_regex -i "/etc/squid3/bloqueo/prox4"
> #http_access deny bl6
> #acl ladmin src "/etc/squid3/ladmin"


> #########################################################################
> #proxy_padre #
> #########################################################################
> cache_peer 172.16.1.24 parent 8000 0
> #nunca permitimos conexiones directas, siempre a traves del proxy
> never_direct allow all
> #######################################################################
> # puerto en que el proxy nos escuchara
> http_port 3128
> ###############################################################################
> maximum_object_size 100 MB
> cache_dir aufs /var/cache/squid3 1024000 16 256
> cache_mem 128 MB
> cache_store_log /var/cache/squid3/cache_store.log
> coredump_dir /var/cache/squid3/dump
> #minimum_expiry_time 600 seconds
> ############################
> client_db off
> offline_mode off
> cache_swap_low 5
> cache_swap_high 10
> cache_replacement_policy heap GDSF
> maximum_object_size_in_memory 256 KB
> chunked_request_body_max_size 4096 KB
> half_closed_clients off
> quick_abort_min 2 KB
> ############################
> # establecemos los archivos de volcado en /var/cache/squid3/
> coredump_dir /var/cache/squid3/
> ###############################################################################
> #Establecemos los patrones de refrescamiento de la cache #
> #patron de refrescamiento -- tipo de archivo -- tiempo del objeto -- %de 
> refrescamiento -- tiempo #
> #1440 minutos equivalen a 24 horas #
> ###############################################################################
> refresh_pattern ^ftp: 1440 20% 10080
> refresh_pattern ^gopher: 1440 0% 1440
> refresh_pattern -i .(gif|png|jpg|jpeg|ico)$ 10080 20% 43200 
> override-expire ignore-no-store ignore-private
> refresh_pattern -i .(iso|avi|wav|mp3|mp4|mpeg|swf|flv|x-flv)$ 43200 20% 
> 432000 override-expire ignore-no-store ignore-private
> #refresh_pattern -i (/cgi-bin/|?) 0 0% 0
> refresh_pattern . 0 20% 4320
> max_filedescriptors 3200
> ##cuanto el squid intenta cachear en mi nombre
> read_ahead_gap 256 KB
> #################
> #sqstat
> #################
> #acl manager proto cache_object
> # replace 10.0.0.1 with your webserver IP
> acl webserver src 172.16.4.25/27
> http_access allow manager webserver
> http_access allow localhost manager
> http_access deny manager
> ###############################################################################
> #Delay#
> ###############################################################################
> client_delay_initial_bucket_level 60
> delay_initial_bucket_level 75
> delay_pools 2
> memory_pools off
> 
> #Canal 1 extensiones.
> delay_class 1 2
> delay_parameters 1 16384/32768 8192/16384
> delay_access 1 allow sociales extensiones
> delay_access 1 deny all
> 
> #Canal 2 para usuarios.
> delay_class 2 2
> delay_parameters 2 65536/65536 32768/32768
> delay_access 2 allow navegacion
> delay_access 2 deny all
> http_access deny all
> #end of line
> ####################################################################################
> 
> 
> 
> 
> PD: Please forgive my english, it's no my native language.
> 
> -- 
> Saludos Cordiales
> 
> Lic. Alex Guti?rrez Mart?nez
> 


From dave.mehler at gmail.com  Fri Sep  1 07:36:02 2017
From: dave.mehler at gmail.com (David Mehler)
Date: Fri, 1 Sep 2017 03:36:02 -0400
Subject: [squid-users] Recompiling Squid3 for Mips hardware,
	enabling captive portal?
Message-ID: <CAPORhP6RwnxJv0hHTYTCSmntSkBVSZ+-mZL+0MVc6Rf7sRcuKQ@mail.gmail.com>

Hello,

I used Squid 2 a while back, when my networks were different.

I'm now wanting to implement squid 3.x. I've got an Asus Rt-N66U
router, which I believe has a Mips processor on it. I am wondering how
to recompile Squid for Mips, or obtain the compilation script for the
entware version of squid and recompile that?

My goal is to compile in helpers for squid as well as captive portal
functionality.

Are there any helper libraries for squid3 to act as a captive portal?
I'm wanting to run a captive portal on my router for wireless clients,
looked in to chillispot but was wanting something recent and that has
documentation, so wanting to use squid.

Thanks.
Dave.


From eliezer at ngtech.co.il  Fri Sep  1 08:46:15 2017
From: eliezer at ngtech.co.il (Eliezer Croitoru)
Date: Fri, 1 Sep 2017 11:46:15 +0300
Subject: [squid-users] Your cache is running out of filedescriptors
In-Reply-To: <29da01d322b2$f0617c00$d1247400$@ngtech.co.il>
References: <1380559112.780474.1504129692448.ref@mail.yahoo.com>
 <1380559112.780474.1504129692448@mail.yahoo.com>
 <254d01d321de$bf12e820$3d38b860$@ngtech.co.il>
 <1476092454.241584.1504164413296@mail.yahoo.com>
 <29da01d322b2$f0617c00$d1247400$@ngtech.co.il>
Message-ID: <2a8001d322fe$c5a09770$50e1c650$@ngtech.co.il>

Sorry a typo..

You will need to use:
ulimit -Hn 65535

first and after this apply the lower limit:
ulimit -n 16384

Eliezer

----
Eliezer Croitoru
Linux System Administrator
Mobile: +972-5-28704261
Email: eliezer at ngtech.co.il



-----Original Message-----
From: squid-users [mailto:squid-users-bounces at lists.squid-cache.org] On Behalf Of Eliezer Croitoru
Sent: Friday, September 1, 2017 02:43
To: 'Vieri' <rentorbuy at yahoo.com>; squid-users at lists.squid-cache.org
Subject: Re: [squid-users] Your cache is running out of filedescriptors

Hey Vieri,

The hard and soft limit are designed to administratively allow a specific service or user have a "space" between the expected to the unexpected.
I assume it's meant also for other things like giving a specific user or service a basic(soft) limit and a higher limit that will not cripple the whole machine(hard..).
But I don't remember the exact idea behind it.

For a sysadmin it's only a matter of restrictions to not wear out the hardware or to prevent resources abuse.

As Amos suggested, since squid almost 100% requires root privileges then you can add to the openrc or system startup service\script the specific limit you want to apply in the scope of any start\restart of the service(squid).
You will need to use:
ulimit -Hn 65535

first and after this apply the lower limit:
ulimit -Hn 16384

just notice that depends on the hardware and the network you should monitor the server and the services to make sure squid will not be used to abuse your connection.

I hope the above will help you.

Eliezer

----
Eliezer Croitoru
Linux System Administrator
Mobile: +972-5-28704261
Email: eliezer at ngtech.co.il



-----Original Message-----
From: squid-users [mailto:squid-users-bounces at lists.squid-cache.org] On Behalf Of Vieri
Sent: Thursday, August 31, 2017 10:27
To: squid-users at lists.squid-cache.org
Subject: Re: [squid-users] Your cache is running out of filedescriptors

I'd like to add a note to my previous message.

I set the following values, and I'll see what happens:

* hard nofile 65535
* soft nofile 16384


("hard" being a top limit a non-root process cannot exceed)

So I take it that Squid will start with a default of 16384, but will be able to increase up to 65535 if it needs to.


By the way, restarting squid from the same shell (ssh) does not apply the new values.
I had to re-log into the system.

There's probably a ulimit command line option to apply the values without logging out.

Anyway, the squid log confirms the new value.


Also, I guess it would be preferable to reboot the server if I wanted the same limits to apply to all running processes (or restart each and every service/daemon one by one).


I also set the following directives.
For local caching proxy:

client_lifetime 480 minutes


For reverse proxies:

client_lifetime 60 minutes


I left the other options alone: 
read_timeout, request_timeout, persistent_request_timeout and quick_abort

Vieri
_______________________________________________
squid-users mailing list
squid-users at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-users

_______________________________________________
squid-users mailing list
squid-users at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-users



From squid3 at treenet.co.nz  Fri Sep  1 08:51:26 2017
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Fri, 1 Sep 2017 20:51:26 +1200
Subject: [squid-users] Recompiling Squid3 for Mips hardware,
 enabling captive portal?
In-Reply-To: <CAPORhP6RwnxJv0hHTYTCSmntSkBVSZ+-mZL+0MVc6Rf7sRcuKQ@mail.gmail.com>
References: <CAPORhP6RwnxJv0hHTYTCSmntSkBVSZ+-mZL+0MVc6Rf7sRcuKQ@mail.gmail.com>
Message-ID: <7b4df18f-1275-1bf6-fd7e-e454e278db4c@treenet.co.nz>

On 01/09/17 19:36, David Mehler wrote:
> Hello,
> 
> I used Squid 2 a while back, when my networks were different.
> 
> I'm now wanting to implement squid 3.x. I've got an Asus Rt-N66U
> router, which I believe has a Mips processor on it.

That is something you will need to be sure of if you are unable to build 
Squid natively on the device. Squid can be cross-built from another 
machine, but you need to have used the right CPU-arch tuplets in the 
--host and --build cross-build options to run the resulting binaries.

If you don't already know what I mean by cross-build and tuplets you 
have some research to do before starting. :-)
OpenWRT has some fairly up to date info 
<https://forum.openwrt.org/viewtopic.php?id=52968>, not sure about Entware.


> I am wondering how
> to recompile Squid for Mips, or obtain the compilation script for the
> entware version of squid and recompile that?

CPU arch should only matter to your build tools so they can produce 
binaries that run on the device. That is a general code building thing.

The issues specifically with building Squid will be whether you have 
support in that devices OS for the Squid features you want to use.

Run 'squid -v' to retrieve the build options from an existing Squid if 
you have or can find one for the device. Then figure out which ones you 
want in your build.

> 
> My goal is to compile in helpers for squid as well as captive portal
> functionality.
> 
> Are there any helper libraries for squid3 to act as a captive portal?

Just the OS NAT interface. Sometimes that is a library (eg Linux 
netfilter), sometimes just the getsockopt() kernel API (eg OpenBSD).


Amos


From gummeah at gmail.com  Fri Sep  1 12:18:37 2017
From: gummeah at gmail.com (Alexander Lazarev)
Date: Fri, 1 Sep 2017 15:18:37 +0300
Subject: [squid-users] Squid reverse-proxy. How it decides when to
	refresh?
In-Reply-To: <CACG7tM8GxTL2wnePzdbnP=7YfHstv1e5ye-UDr4z=uj6df7q2w@mail.gmail.com>
References: <CACG7tM9JNbo4Uzf4ECDU=qL_psuW-XJONNZSt_tGfYJpfXVsVg@mail.gmail.com>
 <8ee439ff-c20d-d4cd-f3fd-ace81c896cdb@treenet.co.nz>
 <CACG7tM8GxTL2wnePzdbnP=7YfHstv1e5ye-UDr4z=uj6df7q2w@mail.gmail.com>
Message-ID: <CACG7tM8iAdFKU6tyOzYCnKfLD0WCsMw77zzqZKwk14Dq+FM1Aw@mail.gmail.com>

Well. looks like squid using heuristics after all:
2017/09/01 14:49:12.296 kid2| 22,3| refresh.cc(291) refreshCheck: checking
freshness of 'http://mydomain.zone/1.txt'
2017/09/01 14:49:12.296 kid2| 22,3| refresh.cc(312) refreshCheck: Matched
'<none> 0 20%% 259200'
2017/09/01 14:49:12.296 kid2| 22,3| refresh.cc(314) refreshCheck:
age:    65955
2017/09/01 14:49:12.296 kid2| 22,3| refresh.cc(316) refreshCheck:
check_time:     Fri, 01 Sep 2017 11:49:12 GMT
2017/09/01 14:49:12.296 kid2| 22,3| refresh.cc(318) refreshCheck:
entry->timestamp:       Thu, 31 Aug 2017 17:29:57 GMT
2017/09/01 14:49:12.296 kid2| 22,3| refresh.cc(179) refreshStaleness: No
explicit expiry given, using heuristics to determine freshness
2017/09/01 14:49:12.296 kid2| 22,3| refresh.cc(198) refreshStaleness: Last
modified 5524975 sec before we cached it, L-M factor 20.00% = 1104995 sec
freshness lifetime
2017/09/01 14:49:12.296 kid2| 22,3| refresh.cc(205) refreshStaleness:
FRESH: age 65955 <= stale_age 1104995
2017/09/01 14:49:12.296 kid2| 22,3| refresh.cc(338) refreshCheck: Staleness
= -1
2017/09/01 14:49:12.296 kid2| 22,3| refresh.cc(461) refreshCheck: Object
isn't stale..
2017/09/01 14:49:12.296 kid2| 22,3| refresh.cc(470) refreshCheck: returning
FRESH_LMFACTOR_RULE

It's a shame there's no warning header, like "
https://tools.ietf.org/html/rfc7234#section-5.5.4" suggests.
Guess, I need to set refresh_pattern's max option to minimal value.

On Thu, Aug 31, 2017 at 8:26 PM, Alexander Lazarev <gummeah at gmail.com>
wrote:

> Thank you for reply!
> I still don't understand what's happening.
> I create file 1.txt with a little bit of text data. Request it with curl.
> Web-server returns it without any cache related headers to squid, squid
> returns it to me. Getting it with curl one more time, squid serves it
> straight from cache without validation(no entries in log on origin server).
> I create one more file 2.txt with some data. Do same things, same headers
> in response. Second response from squid is from cache but validated from
> origin server(i see 304 in origin server logs).
> What could be wrong?
> I have thought maybe squid applying heuristic freshness, but i didn't see
> any warnings in headers.
> Maybe some sort of a bug?
>
> On Fri, Aug 25, 2017 at 6:18 PM, Amos Jeffries <squid3 at treenet.co.nz>
> wrote:
>
>> On 26/08/17 00:37, Alexander Lazarev wrote:
>>
>>> Hello guys!
>>> I'm using squid as a reverse-proxy. And I can't understand how squid
>>> decides when to check for fresh version of file from origin server.
>>> It looks like for some documents it sends 'If-Modified-Since' or similar
>>> headers and if it gets 304, it serves file from cache. And for some
>>> documents it doesn't check for fresh version and always serves from cache.
>>> > I was testing that with curl without any additional headers.
>>> Can some explain how that works or where I can read about that in detail?
>>>
>>
>> The HTTP specification RFC 723x series was re-written to be a lot more
>> easily understood, so those are probably the best place to read up about it.
>>
>> The features you are asking about are covered in:
>>
>> Hypertext Transfer Protocol (HTTP/1.1): Conditional Requests
>>  <https://tools.ietf.org/html/rfc7232>
>>
>> Hypertext Transfer Protocol (HTTP/1.1): Caching
>>  <https://tools.ietf.org/html/rfc7234>
>>
>>
>> And is it possible to make squid always check for fresh version before
>>> serving from cache?
>>>
>>
>> It does when needed. The situation may be clearer after reading the above.
>>
>> Amos
>> _______________________________________________
>> squid-users mailing list
>> squid-users at lists.squid-cache.org
>> http://lists.squid-cache.org/listinfo/squid-users
>>
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20170901/17c88551/attachment.htm>

From squid3 at treenet.co.nz  Fri Sep  1 13:46:31 2017
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sat, 2 Sep 2017 01:46:31 +1200
Subject: [squid-users] Squid reverse-proxy. How it decides when to
 refresh?
In-Reply-To: <CACG7tM8iAdFKU6tyOzYCnKfLD0WCsMw77zzqZKwk14Dq+FM1Aw@mail.gmail.com>
References: <CACG7tM9JNbo4Uzf4ECDU=qL_psuW-XJONNZSt_tGfYJpfXVsVg@mail.gmail.com>
 <8ee439ff-c20d-d4cd-f3fd-ace81c896cdb@treenet.co.nz>
 <CACG7tM8GxTL2wnePzdbnP=7YfHstv1e5ye-UDr4z=uj6df7q2w@mail.gmail.com>
 <CACG7tM8iAdFKU6tyOzYCnKfLD0WCsMw77zzqZKwk14Dq+FM1Aw@mail.gmail.com>
Message-ID: <e46d7402-10f2-dc8a-2528-f6eb97d8c8fd@treenet.co.nz>

On 02/09/17 00:18, Alexander Lazarev wrote:
> Well. looks like squid using heuristics after all:
> 2017/09/01 14:49:12.296 kid2| 22,3| refresh.cc(291) refreshCheck: 
> checking freshness of 'http://mydomain.zone/1.txt'
> 2017/09/01 14:49:12.296 kid2| 22,3| refresh.cc(312) refreshCheck: 
> Matched '<none> 0 20%% 259200'
> 2017/09/01 14:49:12.296 kid2| 22,3| refresh.cc(314) refreshCheck:       
> age: ? ?65955
> 2017/09/01 14:49:12.296 kid2| 22,3| refresh.cc(316) refreshCheck:       
> check_time: ? ? Fri, 01 Sep 2017 11:49:12 GMT
> 2017/09/01 14:49:12.296 kid2| 22,3| refresh.cc(318) refreshCheck:       
> entry->timestamp: ? ? ? Thu, 31 Aug 2017 17:29:57 GMT
> 2017/09/01 14:49:12.296 kid2| 22,3| refresh.cc(179) refreshStaleness: No 
> explicit expiry given, using heuristics to determine freshness
> 2017/09/01 14:49:12.296 kid2| 22,3| refresh.cc(198) refreshStaleness: 
> Last modified 5524975 sec before we cached it, L-M factor 20.00% = 
> 1104995 sec freshness lifetime
> 2017/09/01 14:49:12.296 kid2| 22,3| refresh.cc(205) refreshStaleness: 
> FRESH: age 65955 <= stale_age 1104995
> 2017/09/01 14:49:12.296 kid2| 22,3| refresh.cc(338) refreshCheck: 
> Staleness = -1
> 2017/09/01 14:49:12.296 kid2| 22,3| refresh.cc(461) refreshCheck: Object 
> isn't stale..
> 2017/09/01 14:49:12.296 kid2| 22,3| refresh.cc(470) refreshCheck: 
> returning FRESH_LMFACTOR_RULE
> 
> It's a shame there's no warning header, like 
> "https://tools.ietf.org/html/rfc7234#section-5.5.4" suggests.

There should be when that cached response becomes 24 hrs old. That log 
says Squid only received the object ~18 hrs ago, so the cached 
*response* has not been around for 24hrs yet even though the *content* 
it refers to on the server is older.

Content on a server being old is no particular cause for alarm if it 
gets cached a few seconds/hrs/mins on a proxy.

Though note that Warning headers about heuristics being used are an 
OPTIONAL, so it is also not a problem if they are absent.

> Guess, I need to set refresh_pattern's max option to minimal value.
> 

Any particular reason you are worried about all this?

Heuristic freshness is normal and usually perfectly fine. A proxy making 
heuristic decisions is only a problem if it is ignoring server or client 
instructions about the content cacheability. Also, a reverse-proxy as an 
agent of the server effectively has permission to ignore things the 
client wants - though it is usually a good idea to do a background 
revalidation if the client insists strongly on new content (eg. 
reload-into-ims option), because that tends to mean there is some 
problem with what it got earlier [maybe whats in the cache].


> On Thu, Aug 31, 2017 at 8:26 PM, Alexander Lazarev wrote:
> 
>     Thank you for reply!
>     I still don't understand what's happening.
>     I create file 1.txt with a little bit of text data. Request it with
>     curl. Web-server returns it without any cache related headers to
>     squid, squid returns it to me. Getting it with curl one more time,
>     squid serves it straight from cache without validation(no entries in
>     log on origin server).
>     I create one more file 2.txt with some data. Do same things, same
>     headers in response. Second response from squid is from cache but
>     validated from origin server(i see 304 in origin server logs).
>     What could be wrong?

Nothing wrong. Both sequences are valid and normal. The difference could 
just be a timing variation as small as a nanosecond in what operations 
are performed relative to each other - with heuristics based on 0.2 of a 
recently created objects age HTTP's 1 second in timing resolution is 
both very course and very sensitive to rounding limits.

Amos


From gummeah at gmail.com  Fri Sep  1 14:56:38 2017
From: gummeah at gmail.com (Alexander Lazarev)
Date: Fri, 1 Sep 2017 17:56:38 +0300
Subject: [squid-users] Squid reverse-proxy. How it decides when to
	refresh?
In-Reply-To: <e46d7402-10f2-dc8a-2528-f6eb97d8c8fd@treenet.co.nz>
References: <CACG7tM9JNbo4Uzf4ECDU=qL_psuW-XJONNZSt_tGfYJpfXVsVg@mail.gmail.com>
 <8ee439ff-c20d-d4cd-f3fd-ace81c896cdb@treenet.co.nz>
 <CACG7tM8GxTL2wnePzdbnP=7YfHstv1e5ye-UDr4z=uj6df7q2w@mail.gmail.com>
 <CACG7tM8iAdFKU6tyOzYCnKfLD0WCsMw77zzqZKwk14Dq+FM1Aw@mail.gmail.com>
 <e46d7402-10f2-dc8a-2528-f6eb97d8c8fd@treenet.co.nz>
Message-ID: <CACG7tM8FTu+y3k0VUT5Q_VJgmVNK_UraKP-N3zPXkNTpRqW8=A@mail.gmail.com>

It's all pretty clear to me now after I read RFC and found relationship
between that and refresh_pattern usage.
Thank you.

On Fri, Sep 1, 2017 at 4:46 PM, Amos Jeffries <squid3 at treenet.co.nz> wrote:

> On 02/09/17 00:18, Alexander Lazarev wrote:
>
>> Well. looks like squid using heuristics after all:
>> 2017/09/01 14:49:12.296 kid2| 22,3| refresh.cc(291) refreshCheck:
>> checking freshness of 'http://mydomain.zone/1.txt'
>> 2017/09/01 14:49:12.296 kid2| 22,3| refresh.cc(312) refreshCheck: Matched
>> '<none> 0 20%% 259200'
>> 2017/09/01 14:49:12.296 kid2| 22,3| refresh.cc(314) refreshCheck:
>>  age:    65955
>> 2017/09/01 14:49:12.296 kid2| 22,3| refresh.cc(316) refreshCheck:
>>  check_time:     Fri, 01 Sep 2017 11:49:12 GMT
>> 2017/09/01 14:49:12.296 kid2| 22,3| refresh.cc(318) refreshCheck:
>>  entry->timestamp:       Thu, 31 Aug 2017 17:29:57 GMT
>> 2017/09/01 14:49:12.296 kid2| 22,3| refresh.cc(179) refreshStaleness: No
>> explicit expiry given, using heuristics to determine freshness
>> 2017/09/01 14:49:12.296 kid2| 22,3| refresh.cc(198) refreshStaleness:
>> Last modified 5524975 sec before we cached it, L-M factor 20.00% = 1104995
>> sec freshness lifetime
>> 2017/09/01 14:49:12.296 kid2| 22,3| refresh.cc(205) refreshStaleness:
>> FRESH: age 65955 <= stale_age 1104995
>> 2017/09/01 14:49:12.296 kid2| 22,3| refresh.cc(338) refreshCheck:
>> Staleness = -1
>> 2017/09/01 14:49:12.296 kid2| 22,3| refresh.cc(461) refreshCheck: Object
>> isn't stale..
>> 2017/09/01 14:49:12.296 kid2| 22,3| refresh.cc(470) refreshCheck:
>> returning FRESH_LMFACTOR_RULE
>>
>> It's a shame there's no warning header, like "
>> https://tools.ietf.org/html/rfc7234#section-5.5.4" suggests.
>>
>
> There should be when that cached response becomes 24 hrs old. That log
> says Squid only received the object ~18 hrs ago, so the cached *response*
> has not been around for 24hrs yet even though the *content* it refers to on
> the server is older.
>
> Content on a server being old is no particular cause for alarm if it gets
> cached a few seconds/hrs/mins on a proxy.
>
> Though note that Warning headers about heuristics being used are an
> OPTIONAL, so it is also not a problem if they are absent.
>
> Guess, I need to set refresh_pattern's max option to minimal value.
>>
>>
> Any particular reason you are worried about all this?
>
> Heuristic freshness is normal and usually perfectly fine. A proxy making
> heuristic decisions is only a problem if it is ignoring server or client
> instructions about the content cacheability. Also, a reverse-proxy as an
> agent of the server effectively has permission to ignore things the client
> wants - though it is usually a good idea to do a background revalidation if
> the client insists strongly on new content (eg. reload-into-ims option),
> because that tends to mean there is some problem with what it got earlier
> [maybe whats in the cache].
>
>
> On Thu, Aug 31, 2017 at 8:26 PM, Alexander Lazarev wrote:
>>
>>     Thank you for reply!
>>     I still don't understand what's happening.
>>     I create file 1.txt with a little bit of text data. Request it with
>>     curl. Web-server returns it without any cache related headers to
>>     squid, squid returns it to me. Getting it with curl one more time,
>>     squid serves it straight from cache without validation(no entries in
>>     log on origin server).
>>     I create one more file 2.txt with some data. Do same things, same
>>     headers in response. Second response from squid is from cache but
>>     validated from origin server(i see 304 in origin server logs).
>>     What could be wrong?
>>
>
> Nothing wrong. Both sequences are valid and normal. The difference could
> just be a timing variation as small as a nanosecond in what operations are
> performed relative to each other - with heuristics based on 0.2 of a
> recently created objects age HTTP's 1 second in timing resolution is both
> very course and very sensitive to rounding limits.
>
>
> Amos
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20170901/46485140/attachment.htm>

From rentorbuy at yahoo.com  Fri Sep  1 15:20:44 2017
From: rentorbuy at yahoo.com (Vieri)
Date: Fri, 1 Sep 2017 15:20:44 +0000 (UTC)
Subject: [squid-users] Your cache is running out of filedescriptors
In-Reply-To: <2a8001d322fe$c5a09770$50e1c650$@ngtech.co.il>
References: <1380559112.780474.1504129692448.ref@mail.yahoo.com>
 <1380559112.780474.1504129692448@mail.yahoo.com>
 <254d01d321de$bf12e820$3d38b860$@ngtech.co.il>
 <1476092454.241584.1504164413296@mail.yahoo.com>
 <29da01d322b2$f0617c00$d1247400$@ngtech.co.il>
 <2a8001d322fe$c5a09770$50e1c650$@ngtech.co.il>
Message-ID: <626595763.1223953.1504279244325@mail.yahoo.com>

________________________________
From: Eliezer Croitoru <eliezer at ngtech.co.il>

>
> You will need to use:> ulimit -Hn 65535
> 
> first and after this apply the lower limit:
> ulimit -n 16384
>

> As Amos suggested, since squid almost 100% requires root privileges then you can add to the openrc or system startup 

> service\script the specific limit you want to apply in the scope of any start\restart of the service(squid).

Many thanks to both of you.

I created 01_squid.conf in /etc/security/limits.d/ with:
* hard nofile 65535
* soft nofile 16384

I then restarted squid, and haven't had any issues for the last 24+ hours.

I was hoping to change that file to:
squid hard nofile 65535
squid soft nofile 16384


However, correct me if I'm wrong, but it seems to me that you're saying that Squid adjusts the limit as "root" user, not as the squid user.

I have these main processes:

root      5690  0.0  0.0  87444  5676 ?        Ss   Aug31   0:00 /usr/sbin/squid -YC -f /etc/squid/squid.conf -n squid
squid     5694  2.9  3.3 1188628 1109564 ?     S    Aug31  55:06 (squid-1) -YC -f /etc/squid/squid.conf -n squid


So, is it preferable to use the squid user name in limits.conf's "domain" field, or should I use your method by modifying my openrc init script?

BTW my system is Gentoo, and here's what I can read in the default openrc init script:

# Maximum file descriptors squid can open is determined by:
# a basic default of N=1024
#  ... altered by ./configure --with-filedescriptors=N
#  ... overridden on production by squid.conf max_filedescriptors (if,
#  and only if, setrlimit() RLIMIT_NOFILE is able to be built+used).
# Since we do not configure hard coded # of filedescriptors anymore,
# there is no need for ulimit calls in the init script.
# Use max_filedescriptors in squid.conf instead.


... and here's the start function:

start() {
checkconfig || return 1
checkpath -d -q -m 0750 -o squid:squid /run/${SVCNAME}
ebegin "Starting ${SVCNAME} (service name ${SVCNAME//[^[:alnum:]]/})"
KRB5_KTNAME="${SQUID_KEYTAB}" /usr/sbin/squid ${SQUID_OPTS} -f /etc/squid/${SVCNAME}.conf -n ${SVCNAME//[^[:alnum:]]/}
eend $? && sleep 1
}


The thing is that if Gentoo's default hard ulimit is x then I can't just set max_filedescriptors to a value >x in squid.conf. It simply won't work. Or will it?
When squid starts up as root, can it increase via setrlimit() to whatever value is in max_filedescriptors even if ulimit -Ha shows a lower value for nofiles? 


These are the defaults on my system:

# ulimit -a
core file size          (blocks, -c) 0
data seg size           (kbytes, -d) unlimited
scheduling priority             (-e) 0
file size               (blocks, -f) unlimited
pending signals                 (-i) 127512
max locked memory       (kbytes, -l) 64
max memory size         (kbytes, -m) unlimited
open files                      (-n) 1024
pipe size            (512 bytes, -p) 8
POSIX message queues     (bytes, -q) 819200
real-time priority              (-r) 0
stack size              (kbytes, -s) 8192
cpu time               (seconds, -t) unlimited
max user processes              (-u) 127512
virtual memory          (kbytes, -v) unlimited
file locks                      (-x) unlimited


# ulimit -Ha
core file size          (blocks, -c) unlimited
data seg size           (kbytes, -d) unlimited
scheduling priority             (-e) 0
file size               (blocks, -f) unlimited
pending signals                 (-i) 127512
max locked memory       (kbytes, -l) 64
max memory size         (kbytes, -m) unlimited
open files                      (-n) 4096
pipe size            (512 bytes, -p) 8
POSIX message queues     (bytes, -q) 819200
real-time priority              (-r) 0
stack size              (kbytes, -s) unlimited
cpu time               (seconds, -t) unlimited
max user processes              (-u) 127512
virtual memory          (kbytes, -v) unlimited
file locks                      (-x) unlimited


So, if I were to use your method I guess I would need to modify the init script's start() function like this:

start() {
[...]
ulimit -Hn 65535
ulimit -n 16384
ebegin "Starting ${SVCNAME} (service name ${SVCNAME//[^[:alnum:]]/})"
KRB5_KTNAME="${SQUID_KEYTAB}" /usr/sbin/squid ${SQUID_OPTS} -f /etc/squid/${SVCNAME}.conf -n ${SVCNAME//[^[:alnum:]]/}

[...]

Vieri


From eliezer at ngtech.co.il  Sat Sep  2 20:47:14 2017
From: eliezer at ngtech.co.il (Eliezer Croitoru)
Date: Sat, 2 Sep 2017 23:47:14 +0300
Subject: [squid-users] Your cache is running out of filedescriptors
In-Reply-To: <626595763.1223953.1504279244325@mail.yahoo.com>
References: <1380559112.780474.1504129692448.ref@mail.yahoo.com>
 <1380559112.780474.1504129692448@mail.yahoo.com>
 <254d01d321de$bf12e820$3d38b860$@ngtech.co.il>
 <1476092454.241584.1504164413296@mail.yahoo.com>
 <29da01d322b2$f0617c00$d1247400$@ngtech.co.il>
 <2a8001d322fe$c5a09770$50e1c650$@ngtech.co.il>
 <626595763.1223953.1504279244325@mail.yahoo.com>
Message-ID: <2ea101d3242c$a8a8ebc0$f9fac340$@ngtech.co.il>

Squid uses the root limit and also the current environment limit.
Then current environment limit can be changed only by the root user..
So your openrc script change should apply the best fix instead of allowing root have a basic high limit.
But if someone has root privilges on the machine it doesn't matter anyway so..
Choose how you want to upper the limit.
Using the basic limits way or the openrc one.

Eliezer

----
Eliezer Croitoru
Linux System Administrator
Mobile: +972-5-28704261
Email: eliezer at ngtech.co.il



-----Original Message-----
From: squid-users [mailto:squid-users-bounces at lists.squid-cache.org] On Behalf Of Vieri
Sent: Friday, September 1, 2017 18:21
To: squid-users at lists.squid-cache.org
Subject: Re: [squid-users] Your cache is running out of filedescriptors

________________________________
From: Eliezer Croitoru <eliezer at ngtech.co.il>

>
> You will need to use:> ulimit -Hn 65535
> 
> first and after this apply the lower limit:
> ulimit -n 16384
>

> As Amos suggested, since squid almost 100% requires root privileges then you can add to the openrc or system startup 

> service\script the specific limit you want to apply in the scope of any start\restart of the service(squid).

Many thanks to both of you.

I created 01_squid.conf in /etc/security/limits.d/ with:
* hard nofile 65535
* soft nofile 16384

I then restarted squid, and haven't had any issues for the last 24+ hours.

I was hoping to change that file to:
squid hard nofile 65535
squid soft nofile 16384


However, correct me if I'm wrong, but it seems to me that you're saying that Squid adjusts the limit as "root" user, not as the squid user.

I have these main processes:

root      5690  0.0  0.0  87444  5676 ?        Ss   Aug31   0:00 /usr/sbin/squid -YC -f /etc/squid/squid.conf -n squid
squid     5694  2.9  3.3 1188628 1109564 ?     S    Aug31  55:06 (squid-1) -YC -f /etc/squid/squid.conf -n squid


So, is it preferable to use the squid user name in limits.conf's "domain" field, or should I use your method by modifying my openrc init script?

BTW my system is Gentoo, and here's what I can read in the default openrc init script:

# Maximum file descriptors squid can open is determined by:
# a basic default of N=1024
#  ... altered by ./configure --with-filedescriptors=N
#  ... overridden on production by squid.conf max_filedescriptors (if,
#  and only if, setrlimit() RLIMIT_NOFILE is able to be built+used).
# Since we do not configure hard coded # of filedescriptors anymore,
# there is no need for ulimit calls in the init script.
# Use max_filedescriptors in squid.conf instead.


... and here's the start function:

start() {
checkconfig || return 1
checkpath -d -q -m 0750 -o squid:squid /run/${SVCNAME}
ebegin "Starting ${SVCNAME} (service name ${SVCNAME//[^[:alnum:]]/})"
KRB5_KTNAME="${SQUID_KEYTAB}" /usr/sbin/squid ${SQUID_OPTS} -f /etc/squid/${SVCNAME}.conf -n ${SVCNAME//[^[:alnum:]]/}
eend $? && sleep 1
}


The thing is that if Gentoo's default hard ulimit is x then I can't just set max_filedescriptors to a value >x in squid.conf. It simply won't work. Or will it?
When squid starts up as root, can it increase via setrlimit() to whatever value is in max_filedescriptors even if ulimit -Ha shows a lower value for nofiles? 


These are the defaults on my system:

# ulimit -a
core file size          (blocks, -c) 0
data seg size           (kbytes, -d) unlimited
scheduling priority             (-e) 0
file size               (blocks, -f) unlimited
pending signals                 (-i) 127512
max locked memory       (kbytes, -l) 64
max memory size         (kbytes, -m) unlimited
open files                      (-n) 1024
pipe size            (512 bytes, -p) 8
POSIX message queues     (bytes, -q) 819200
real-time priority              (-r) 0
stack size              (kbytes, -s) 8192
cpu time               (seconds, -t) unlimited
max user processes              (-u) 127512
virtual memory          (kbytes, -v) unlimited
file locks                      (-x) unlimited


# ulimit -Ha
core file size          (blocks, -c) unlimited
data seg size           (kbytes, -d) unlimited
scheduling priority             (-e) 0
file size               (blocks, -f) unlimited
pending signals                 (-i) 127512
max locked memory       (kbytes, -l) 64
max memory size         (kbytes, -m) unlimited
open files                      (-n) 4096
pipe size            (512 bytes, -p) 8
POSIX message queues     (bytes, -q) 819200
real-time priority              (-r) 0
stack size              (kbytes, -s) unlimited
cpu time               (seconds, -t) unlimited
max user processes              (-u) 127512
virtual memory          (kbytes, -v) unlimited
file locks                      (-x) unlimited


So, if I were to use your method I guess I would need to modify the init script's start() function like this:

start() {
[...]
ulimit -Hn 65535
ulimit -n 16384
ebegin "Starting ${SVCNAME} (service name ${SVCNAME//[^[:alnum:]]/})"
KRB5_KTNAME="${SQUID_KEYTAB}" /usr/sbin/squid ${SQUID_OPTS} -f /etc/squid/${SVCNAME}.conf -n ${SVCNAME//[^[:alnum:]]/}

[...]

Vieri
_______________________________________________
squid-users mailing list
squid-users at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-users



From jonathanthomascho22 at gmail.com  Sun Sep  3 05:37:34 2017
From: jonathanthomascho22 at gmail.com (Jonathan thomas Cho)
Date: Sun, 3 Sep 2017 01:37:34 -0400
Subject: [squid-users] Squid Cache_peer
Message-ID: <59ab951e.06a2370a.a3d4f.9e33@mx.google.com>


Hello, I seem to have a issue with cache peer. I have 2 parent IPS, however, I want port 3128 to go to 1 parent while 3129 go to the second parent.  Here is my current config, I hope you can adjust it for me. Thank you

Http_port 3128
http_port 3129

coredump_dir /var/spool/squid3
refresh_pattern ^ftp:       1440    20% 10080
refresh_pattern ^gopher:    1440    0%  1440
refresh_pattern -i (/cgi-bin/|\?) 0 0%  0
refresh_pattern (Release|Packages(.gz)*)$      0       20%     2880
refresh_pattern .       0   20% 4320

cache_peer 1xx.xxx.xxx.xxx parent 3128 0 proxy-only no-query default login=username:password name=user1
cache_peer 1xx.xxx.xxx.xxx parent 3128 0 proxy-only no-query default login=username:password name=user2

never_direct allow all


as you can see, I?m not entirely sure on what to do to route each parent ip to specific port so users cant use more ports than needed. 
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20170903/763490da/attachment.htm>

From squid3 at treenet.co.nz  Sun Sep  3 08:20:58 2017
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sun, 3 Sep 2017 20:20:58 +1200
Subject: [squid-users] Squid Cache_peer
In-Reply-To: <59ab951e.06a2370a.a3d4f.9e33@mx.google.com>
References: <59ab951e.06a2370a.a3d4f.9e33@mx.google.com>
Message-ID: <4a2a09e6-f9fd-12cc-ed0a-8f295afe138d@treenet.co.nz>

On 03/09/17 17:37, Jonathan thomas Cho wrote:
> Hello, I seem to have a issue with cache peer. I have 2 parent IPS, 
> however, I want port 3128 to go to 1 parent while 3129 go to the second 
> parent. ?Here is my current config, I hope you can adjust it for me. 

Not without understanding what it is you are actually trying to achieve. 
What you ask for above is satisfied by adding cache_peer_access rules.

BUT your explanation below of _why_ you want it indicates you may 
actually need something entirely different to what you are asking about.


> Thank you
> 
> Http_port 3128
> 
> http_port 3129
> 
> coredump_dir /var/spool/squid3
> 
> refresh_pattern ^ftp:?????? 1440??? 20% 10080
> 
> refresh_pattern ^gopher:??? 1440??? 0%? 1440
> 
> refresh_pattern -i (/cgi-bin/|\?) 0 0%? 0
> 
> refresh_pattern (Release|Packages(.gz)*)$????? 0?????? 20%???? 2880
> 
> refresh_pattern .?????? 0?? 20% 4320
> 
> cache_peer 1xx.xxx.xxx.xxx parent 3128 0 proxy-only no-query default 
> login=username:password name=user1
> 
> cache_peer 1xx.xxx.xxx.xxx parent 3128 0 proxy-only no-query default 
> login=username:password name=user2
> 
> never_direct allow all
> 
> as you can see, I?m not entirely sure on what to do to route each parent 
> ip to specific port so users cant use more ports than needed.
> 

That sentence does not compute.

a) there is no sign of any "user" in your config.

Squid does have credentials that is sends to each peer - but that has 
nothing to do with any human / user. It is a pair of *machine* 
credentials for Basic auth representing Squid itself.


b) You have not configured the standby= parameters which force Squid to 
open more connections than it needs. So your Squid already does require 
every single port it opens to those peers.
  ie It is not possible for this Squid to be encountering the problem 
you say your are trying to avoid.


Also, Squid uses the multiplexing and pipeline mechanisms in HTTP. So 
any port opens to the peer *will* be used for multiple messages until 
one requires it to be closed.
  By manually configuring specific routing you are add limits to those 
mechanisms which are likely to result in *more* ports being used to each 
peer. Since Squid can no longer pipeline traffic contradictory to your 
rules even if peer #1 has no open ports and peer #2 has an open and 
available port waiting for messages such as the one your rules send to 
peer #1 - and vice versa.

So, you are apparently trying to work around a problem that this Squid 
cannot encounter by adding complexity that will cause it to happen.


Can you more clearly describe what exactly you are trying to achieve here?
  and what problem you have encountered (or think you might) that is 
behind your request?


Amos


From ahmed.zaeem at netstream.ps  Sun Sep  3 19:49:39 2017
From: ahmed.zaeem at netstream.ps (--Ahmad--)
Date: Sun, 3 Sep 2017 22:49:39 +0300
Subject: [squid-users] squid with quota limit using external helper problem !
Message-ID: <47F841AF-71FC-4162-BFA9-A1CABFA6FC50@netstream.ps>

Hello squid folks .

I?m trying to use squid external helper to get quote to ips or users.

I?m following the wiki :

http://www.mikealeonetti.com/wiki/index.php?title=Squid_Arms_and_Tentacles:_Bandwidth_quotas

i have done everything my side on squid .

i have tested the connection :
root at localhost:~# /usr/local/bin/bandwidth_calculate /etc/squid/bandwidth_rules
root at localhost:~# 

no errors above !

#######################################

the issue I?m not sure if I?m using squid config file integration correctly .

here is my squid.conf file :

dns_v4_first on
acl localnet src all
auth_param basic program /lib/squid/basic_ncsa_auth  /etc/squid/squid_user
acl ncsa_users proxy_auth REQUIRED
auth_param basic children 1000
external_acl_type bandwidth_check ttl=60 %SRC /usr/local/bin/bandwidth_check
acl bandwidth_auth external bandwidth_check
http_access allow localnet bandwidth_auth
http_access deny  localnet !bandwidth_auth
###################################################
cache_effective_user squid
cache_effective_group squid
###########################################
http_access allow ncsa_users
############################
acl SSL_ports port 443
acl Safe_ports port 80          # http
acl Safe_ports port 21          # ftp
acl Safe_ports port 443         # https
acl Safe_ports port 70          # gopher
acl Safe_ports port 210         # wais
acl Safe_ports port 1025-65535  # unregistered ports
acl Safe_ports port 280         # http-mgmt
acl Safe_ports port 488         # gss-http
acl Safe_ports port 591         # filemaker
acl Safe_ports port 777         # multiling http
acl CONNECT method CONNECT
http_access deny !Safe_ports
http_access deny CONNECT !SSL_ports
http_access allow localhost manager
http_access deny manager
http_access allow localhost
http_access deny all
http_port 3128
coredump_dir /var/spool/squid
refresh_pattern ^ftp:           1440    20%     10080
refresh_pattern ^gopher:        1440    0%      1440
refresh_pattern -i (/cgi-bin/|\?) 0     0%      0
refresh_pattern (Release|Packages(.gz)*)$      0       20%     2880
refresh_pattern . 

















here is errors i get :


2017/09/03 19:32:38 kid1| WARNING: external ACL 'bandwidth_check' queue overload. Request rejected '11.13.209.12'.
2017/09/03 19:38:31 kid1| WARNING: external ACL 'bandwidth_check' queue overload. Request rejected '11.13.209.12'.
2017/09/03 19:44:46 kid1| WARNING: external ACL 'bandwidth_check' queue overload. Request rejected '148.161.111.42'.
2017/09/03 19:44:47 kid1| WARNING: external ACL 'bandwidth_check' queue overload. Request rejected '148.161.111.42?.



but I?m sure 100 % that the ips above not blacklisted bec i check them over the helper :



root at localhost:~/squid-3.5.22# /usr/local/bin/bandwidth_check
11.13.209.12
OK
11.13.209.12
OK





root at localhost:~# cat /etc/squid/bandwidth_rules 
# A subnet
192.168.1.0/24        100mb/d 500mb/w    10gb/m
# A range
10.0.0.100-200        200mb/m
# A single IP
192.168.2.105        1gb/w 20gb/m
# A username
mike                 5gb/w
as1  10mb/d
hola    10mb/d
11.13.209.12           10mb/d





here is squid when it run :

root at localhost:~# tailf /var/log/squid/cache.log
2017/09/03 19:32:33 kid1| ERROR: Failed to create helper child read FD: TCP [::1]
2017/09/03 19:32:33 kid1| WARNING: Cannot run '/usr/local/bin/bandwidth_check' process.
2017/09/03 19:32:33 kid1| HTCP Disabled.
2017/09/03 19:32:33 kid1| Finished loading MIME types and icons.
2017/09/03 19:32:33 kid1| Accepting HTTP Socket connections at local=44.33.95.148:10001 remote=[::] FD 36 flags=9
2017/09/03 19:32:33 kid1| Accepting HTTP Socket connections at local=44.33.95.148:10002 remote=[::] FD 37 flags=9
2017/09/03 19:32:38 kid1| WARNING: external ACL 'bandwidth_check' queue overload. Request rejected '11.13.209.12'.
2017/09/03 19:38:31 kid1| WARNING: external ACL 'bandwidth_check' queue overload. Request rejected '11.13.209.12'.
2017/09/03 19:44:46 kid1| WARNING: external ACL 'bandwidth_check' queue overload. Request rejected '148.161.111.42'.
2017/09/03 19:44:47 kid1| WARNING: external ACL 'bandwidth_check' queue overload. Request rejected '148.161.111.42'.
2017/09/03 19:46:14 kid1| WARNING: external ACL 'bandwidth_check' queue overload. Request rejected '11.13.209.12?.




Guys i know this is not squid 100 % question .

plz don?t put me down and just guide me where to troubleshoot to figure out this issue .


many thanks !







-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20170903/a46eee10/attachment.htm>

From squid3 at treenet.co.nz  Mon Sep  4 05:10:23 2017
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Mon, 4 Sep 2017 17:10:23 +1200
Subject: [squid-users] squid with quota limit using external helper
 problem !
In-Reply-To: <47F841AF-71FC-4162-BFA9-A1CABFA6FC50@netstream.ps>
References: <47F841AF-71FC-4162-BFA9-A1CABFA6FC50@netstream.ps>
Message-ID: <47a5c10e-ae64-e366-d6bc-f494836da3a4@treenet.co.nz>

On 04/09/17 07:49, --Ahmad-- wrote:
> Hello squid folks .
> 
> I?m trying to use squid external helper to get quote to ips or users.
> 
> I?m following the wiki :
> 
> http://www.mikealeonetti.com/wiki/index.php?title=Squid_Arms_and_Tentacles:_Bandwidth_quotas
> 
> i have done everything my side on squid .
> 
> i have tested the connection :
> root at localhost:~# /usr/local/bin/bandwidth_calculate 
> /etc/squid/bandwidth_rules
> root at localhost:~#
> 
> no errors above !
> 
> #######################################
> 
> the issue I?m not sure if I?m using squid config file integration 
> correctly .
> 
> here is my squid.conf file :
> 
> dns_v4_first on
> acl localnet src all

You have defined your LAN to be the entire Internet. Don't do that.

Define localnet to be your actual network ranges.

Use the provided 'all' ACL to refer to things that are allowed/denied to 
everyone online. Most of the time 'all' is unnecessary.

If you expect clients from the general web to access your proxy and some 
access control to apply to them, then simply do not limit those access 
controls with the 'localnet' ACL.


> auth_param basic program /lib/squid/basic_ncsa_auth ?/etc/squid/squid_user
> acl ncsa_users proxy_auth REQUIRED
> auth_param basic children 1000

How many users do expect exactly?

Squid de-duplicated overlapping Basic auth logins so one user can login 
multiple times at once (ie login bursts when a Browser starts up) with 
only one query sent to the auth helper. NCSA is also extremely fast lookups.

If you bumped that up because of the WARNING logged, then please change 
your practices to fix ERRORs before WARNINGs.
* WARNINGs are logged for things Squid can workaround but needs help to 
fix properly,
* ERRORs are things Squid cannot do anything about and need your attention,
* FATALs are things that are absolutely critical to fix if you are going 
to use Squid at all.


> external_acl_type bandwidth_check ttl=60 %SRC /usr/local/bin/bandwidth_check

The ttl= parameter needs to be 0 for accurate bandwidth results. With 
the above the helper is only checked once per minute, not on every request.
Keep in mind that you are only controlling whether new requests can 
start, and once started they will complete. So regular re-checking is 
required to minimize overages.

NP: negative_ttl= control how often Squid re-checks results from the 
helper once users go over their quota. This is the option that you will 
want to tune with non-0 values to reduce helper load, but also keep it 
low enough not to block clients for too long after their quota renews.


> acl bandwidth_auth external bandwidth_check
> http_access allow localnet bandwidth_auth
> http_access deny ?localnet !bandwidth_auth

The wiki is documenting the above two rules as *alternatives*. I suggest 
you go back and read their descriptions, then pick the one that does 
what you need.


> ###################################################
> cache_effective_user squid
> cache_effective_group squid
> ###########################################
> http_access allow ncsa_users

This will only login users that broadcast their credentials. It will not 
require credentials from clients, and none of your below rules require 
login to have happened.

Best practice for authentication is to place the rules applying to 
non-authenticate clients first, then have:

   http_access deny !ncsa_users

... then to follow that with any rules applying to authenticated clients.


> ############################
> acl SSL_ports port 443
> acl Safe_ports port 80 ? ? ? ? ?# http
> acl Safe_ports port 21 ? ? ? ? ?# ftp
> acl Safe_ports port 443 ? ? ? ? # https
> acl Safe_ports port 70 ? ? ? ? ?# gopher
> acl Safe_ports port 210 ? ? ? ? # wais
> acl Safe_ports port 1025-65535 ?# unregistered ports
> acl Safe_ports port 280 ? ? ? ? # http-mgmt
> acl Safe_ports port 488 ? ? ? ? # gss-http
> acl Safe_ports port 591 ? ? ? ? # filemaker
> acl Safe_ports port 777 ? ? ? ? # multiling http
> acl CONNECT method CONNECT
> http_access deny !Safe_ports
> http_access deny CONNECT !SSL_ports

These Safe_ports and CONNECT rule need to be *above* all of your custom 
rules. Otherwise they will have zero ability to protect your proxy 
against the DoS and hijacking attacks they are supposed to prevent.

<snip>
> 
> here is errors i get :
> 
> 
> 2017/09/03 19:32:38 kid1| WARNING: external ACL 'bandwidth_check' queue 
> overload. Request rejected '11.13.209.12'.
> 2017/09/03 19:38:31 kid1| WARNING: external ACL 'bandwidth_check' queue 
> overload. Request rejected '11.13.209.12'.
> 2017/09/03 19:44:46 kid1| WARNING: external ACL 'bandwidth_check' queue 
> overload. Request rejected '148.161.111.42'.
> 2017/09/03 19:44:47 kid1| WARNING: external ACL 'bandwidth_check' queue 
> overload. Request rejected '148.161.111.42?.
> 
> 
> 
> but I?m sure 100 % that the ips above not blacklisted bec i check them 
> over the helper :

Please re-read the WARNING message.

IPs are *not* being rejected because they are listed. They are being 
rejected because the helper lookup queue is overloaded and no OK is 
received.

> 
> here is squid when it run :
> 
> root at localhost:~# tailf /var/log/squid/cache.log
> 2017/09/03 19:32:33 kid1| ERROR: Failed to create helper child read FD: 
> TCP [::1]

Fix that ERROR. The WARNING's about the helper and ACL checking are all 
side effects of there not actually being a helper running.

There are several ways to do that:

1) fix the helpers IPv6 support. It seems not to have any, or if it does 
is somehow still only using the IPv4-only address of localhost. Squid is 
trying to contact it over an IPv6-v4-mapped address for localhost.


2) add the 'ipv4' option to your external_acl_type, to make Squid 
temporarily be IPv4-only when talking to this helper.

While (2) is very tempting and easy, you will probably find that an 
IPv4-only helper like this has errors when it gets told the IP address 
of an IPv6 client. So (1) is the better option and I see the wiki page 
author goes on about being happy to fix problem with their helper - just 
get in touch.


Amos


From wehategrey at gmail.com  Mon Sep  4 06:53:41 2017
From: wehategrey at gmail.com (Grey)
Date: Sun, 3 Sep 2017 23:53:41 -0700 (MST)
Subject: [squid-users] Kerberos access denied and reauthentication
In-Reply-To: <1501568689783-4683244.post@n4.nabble.com>
References: <1501144027034-4683224.post@n4.nabble.com>
 <8da798ed-0038-0bba-9a70-b213bb43bc69@gmail.com>
 <1501231571078-4683232.post@n4.nabble.com>
 <a0b261a5-7506-06ee-39cd-82b07c20d6de@gmail.com>
 <1501568689783-4683244.post@n4.nabble.com>
Message-ID: <1504508021326-0.post@n4.nabble.com>

Looks like since posting the log the problem has disappeared for all 5 of my
test users; since nothing has been changed on the network, could it have
been caused by a Firefox and Chrome bug that has been recently fixed (I
don't recall ever seeing the problem on IE)? Does anyone know of the
existence of bugs compatible with what I've experienced?
Thanks.



--
Sent from: http://squid-web-proxy-cache.1019090.n4.nabble.com/Squid-Users-f1019091.html


From rentorbuy at yahoo.com  Mon Sep  4 07:13:37 2017
From: rentorbuy at yahoo.com (Vieri)
Date: Mon, 4 Sep 2017 07:13:37 +0000 (UTC)
Subject: [squid-users] external ACL queue overload
References: <1269966217.2425378.1504509217679.ref@mail.yahoo.com>
Message-ID: <1269966217.2425378.1504509217679@mail.yahoo.com>

Hi,

I sometimes get messages such as this one:

WARNING: external ACL 'bllookup' queue overload. Using stale result.

My squid.conf has this defined:

external_acl_type bllookup ttl=60 %URI /opt/custom/scripts/run/scripts/firewall/squid_url_lookup.pl -tbl_name=shallalist_bl adv,aggressive,...

Does it make sense to increase the following values:

ttl=86400 children-max=50 children-startup=2 children-idle=5

I'd like to know what "children-startup" means exactly.

Thanks,

Vieri


From chiasa.men at web.de  Mon Sep  4 08:36:41 2017
From: chiasa.men at web.de (chiasa.men)
Date: Mon, 04 Sep 2017 10:36:41 +0200
Subject: [squid-users] RC4-MD5 cipher is always enabled?
Message-ID: <2119528.JO9MT1vNor@march>

"RC4-MD5" seems to be always enabled. Is there a way to prohibit RC4-MD5?



squid.conf:
https_port 3128 accel defaultsite=www.example.com cert=/example/cert.pem key=/
example/key.pem
sslproxy_version 6
sslproxy_options NO_SSLv2,NO_SSLv3,NO_TLSv1,NO_TLSv1_1,NO_TICKET
sslproxy_cipher ECDHE-ECDSA-AES256-GCM-SHA384:!RC4:!MD5


squid -f /tmp/s.conf -N -d debug


SSLScan reports RC4-MD5 is accepted:

sslscan --no-failed localhost:3128
    Accepted  TLSv1  256 bits  AES256-SHA
    Accepted  TLSv1  256 bits  CAMELLIA256-SHA
    Accepted  TLSv1  128 bits  AES128-SHA
    Accepted  TLSv1  128 bits  SEED-SHA
    Accepted  TLSv1  128 bits  CAMELLIA128-SHA
    Accepted  TLSv1  128 bits  RC4-SHA
    Accepted  TLSv1  128 bits  RC4-MD5
    Accepted  TLSv1  112 bits  DES-CBC3-SHA


Connection with RC4-MD5 is successful:
openssl s_client -connect localhost:3128 -cipher RC4-MD5
New, TLSv1/SSLv3, Cipher is RC4-MD5
    Cipher    : RC4-MD5


Connection with rejected ciphers is not successful:

openssl s_client -connect localhost:3128 -cipher ECDHE-RSA-NULL-SHA
140016624731800:error:14077410:SSL routines:SSL23_GET_SERVER_HELLO:sslv3 alert 
handshake failure:s23_clnt.c:769:

New, (NONE), Cipher is (NONE)
    Cipher    : 0000




From ahmed.zaeem at netstream.ps  Mon Sep  4 09:38:19 2017
From: ahmed.zaeem at netstream.ps (--Ahmad--)
Date: Mon, 4 Sep 2017 12:38:19 +0300
Subject: [squid-users] squid with quota limit using external helper
	problem !
In-Reply-To: <47a5c10e-ae64-e366-d6bc-f494836da3a4@treenet.co.nz>
References: <47F841AF-71FC-4162-BFA9-A1CABFA6FC50@netstream.ps>
 <47a5c10e-ae64-e366-d6bc-f494836da3a4@treenet.co.nz>
Message-ID: <AED32C3A-FE19-48C6-B0D9-6803D88DF0EA@netstream.ps>

Hi amos , thanks for the kind response .

i denied to rebuild squid without IPV6 support and seems now no error in helper .


i just curious to know about the auth directors in squid how should i arrange it :

acl localnet src all

auth_param basic program /lib/squid/basic_ncsa_auth  /etc/squid/squid_user
acl ncsa_users proxy_auth REQUIRED
auth_param basic children 1000

external_acl_type bandwidth_check ttl=0 %SRC /usr/local/bin/bandwidth_check
acl bandwidth_auth external bandwidth_check
http_access allow localnet bandwidth_auth
http_access deny  localnet !bandwidth_auth
###################################################
http_access allow ncsa_users


is above correct sequence to block any user exceeded quota ?
also should i use  
external_acl_type bandwidth_check ttl=0 %SRC /usr/local/bin/bandwidth_check

or

external_acl_type bandwidth_check ttl=0 %SRC %LOGIN /usr/local/bin/bandwidth_check

or 

external_acl_type bandwidth_check ttl=0  %EXT_USER /usr/local/bin/bandwidth_check


thanks amos in advance 
> On Sep 4, 2017, at 8:10 AM, Amos Jeffries <squid3 at treenet.co.nz> wrote:
> 
> On 04/09/17 07:49, --Ahmad-- wrote:
>> Hello squid folks .
>> I?m trying to use squid external helper to get quote to ips or users.
>> I?m following the wiki :
>> http://www.mikealeonetti.com/wiki/index.php?title=Squid_Arms_and_Tentacles:_Bandwidth_quotas
>> i have done everything my side on squid .
>> i have tested the connection :
>> root at localhost:~# /usr/local/bin/bandwidth_calculate /etc/squid/bandwidth_rules
>> root at localhost:~#
>> no errors above !
>> #######################################
>> the issue I?m not sure if I?m using squid config file integration correctly .
>> here is my squid.conf file :
>> dns_v4_first on
>> acl localnet src all
> 
> You have defined your LAN to be the entire Internet. Don't do that.
> 
> Define localnet to be your actual network ranges.
> 
> Use the provided 'all' ACL to refer to things that are allowed/denied to everyone online. Most of the time 'all' is unnecessary.
> 
> If you expect clients from the general web to access your proxy and some access control to apply to them, then simply do not limit those access controls with the 'localnet' ACL.
> 
> 
>> auth_param basic program /lib/squid/basic_ncsa_auth  /etc/squid/squid_user
>> acl ncsa_users proxy_auth REQUIRED
>> auth_param basic children 1000
> 
> How many users do expect exactly?
> 
> Squid de-duplicated overlapping Basic auth logins so one user can login multiple times at once (ie login bursts when a Browser starts up) with only one query sent to the auth helper. NCSA is also extremely fast lookups.
> 
> If you bumped that up because of the WARNING logged, then please change your practices to fix ERRORs before WARNINGs.
> * WARNINGs are logged for things Squid can workaround but needs help to fix properly,
> * ERRORs are things Squid cannot do anything about and need your attention,
> * FATALs are things that are absolutely critical to fix if you are going to use Squid at all.
> 
> 
>> external_acl_type bandwidth_check ttl=60 %SRC /usr/local/bin/bandwidth_check
> 
> The ttl= parameter needs to be 0 for accurate bandwidth results. With the above the helper is only checked once per minute, not on every request.
> Keep in mind that you are only controlling whether new requests can start, and once started they will complete. So regular re-checking is required to minimize overages.
> 
> NP: negative_ttl= control how often Squid re-checks results from the helper once users go over their quota. This is the option that you will want to tune with non-0 values to reduce helper load, but also keep it low enough not to block clients for too long after their quota renews.
> 
> 
>> acl bandwidth_auth external bandwidth_check
>> http_access allow localnet bandwidth_auth
>> http_access deny  localnet !bandwidth_auth
> 
> The wiki is documenting the above two rules as *alternatives*. I suggest you go back and read their descriptions, then pick the one that does what you need.
> 
> 
>> ###################################################
>> cache_effective_user squid
>> cache_effective_group squid
>> ###########################################
>> http_access allow ncsa_users
> 
> This will only login users that broadcast their credentials. It will not require credentials from clients, and none of your below rules require login to have happened.
> 
> Best practice for authentication is to place the rules applying to non-authenticate clients first, then have:
> 
>  http_access deny !ncsa_users
> 
> ... then to follow that with any rules applying to authenticated clients.
> 
> 
>> ############################
>> acl SSL_ports port 443
>> acl Safe_ports port 80          # http
>> acl Safe_ports port 21          # ftp
>> acl Safe_ports port 443         # https
>> acl Safe_ports port 70          # gopher
>> acl Safe_ports port 210         # wais
>> acl Safe_ports port 1025-65535  # unregistered ports
>> acl Safe_ports port 280         # http-mgmt
>> acl Safe_ports port 488         # gss-http
>> acl Safe_ports port 591         # filemaker
>> acl Safe_ports port 777         # multiling http
>> acl CONNECT method CONNECT
>> http_access deny !Safe_ports
>> http_access deny CONNECT !SSL_ports
> 
> These Safe_ports and CONNECT rule need to be *above* all of your custom rules. Otherwise they will have zero ability to protect your proxy against the DoS and hijacking attacks they are supposed to prevent.
> 
> <snip>
>> here is errors i get :
>> 2017/09/03 19:32:38 kid1| WARNING: external ACL 'bandwidth_check' queue overload. Request rejected '11.13.209.12'.
>> 2017/09/03 19:38:31 kid1| WARNING: external ACL 'bandwidth_check' queue overload. Request rejected '11.13.209.12'.
>> 2017/09/03 19:44:46 kid1| WARNING: external ACL 'bandwidth_check' queue overload. Request rejected '148.161.111.42'.
>> 2017/09/03 19:44:47 kid1| WARNING: external ACL 'bandwidth_check' queue overload. Request rejected '148.161.111.42?.
>> but I?m sure 100 % that the ips above not blacklisted bec i check them over the helper :
> 
> Please re-read the WARNING message.
> 
> IPs are *not* being rejected because they are listed. They are being rejected because the helper lookup queue is overloaded and no OK is received.
> 
>> here is squid when it run :
>> root at localhost:~# tailf /var/log/squid/cache.log
>> 2017/09/03 19:32:33 kid1| ERROR: Failed to create helper child read FD: TCP [::1]
> 
> Fix that ERROR. The WARNING's about the helper and ACL checking are all side effects of there not actually being a helper running.
> 
> There are several ways to do that:
> 
> 1) fix the helpers IPv6 support. It seems not to have any, or if it does is somehow still only using the IPv4-only address of localhost. Squid is trying to contact it over an IPv6-v4-mapped address for localhost.
> 
> 
> 2) add the 'ipv4' option to your external_acl_type, to make Squid temporarily be IPv4-only when talking to this helper.
> 
> While (2) is very tempting and easy, you will probably find that an IPv4-only helper like this has errors when it gets told the IP address of an IPv6 client. So (1) is the better option and I see the wiki page author goes on about being happy to fix problem with their helper - just get in touch.
> 
> 
> Amos
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20170904/c521352d/attachment.htm>

From rafael.akchurin at diladele.com  Mon Sep  4 10:25:37 2017
From: rafael.akchurin at diladele.com (Rafael Akchurin)
Date: Mon, 4 Sep 2017 10:25:37 +0000
Subject: [squid-users] [icap] Web Safety 5.1 ICAP web filter plugin for
	Squid is Ready
Message-ID: <DB6PR0401MB26801FA307CC7B2F5A26A6618F910@DB6PR0401MB2680.eurprd04.prod.outlook.com>

Greetings everyone,

New version of Web Safety (ICAP web filter plugin for Squid 3.5) is ready for production. Build number 5.1.0.493A generated on August 9, 2017.

* This version contains the ability to bypass the blocked page (using a bypass token). Upon being presented with a blocked page user can click on "Proceed Anyway" button and the blocked domain is then added to a temporary white-list. Bypass time and policies to allow bypassing can be customized by the administrator.

* We have changed a lot in the Admin UI internally, splitting the monstrous Django code files into small and manageable classes. In future this will allow us to provide free Admin UI for all Squid users easily detachable from the commercial Web Safety plugin.

Application is packed as virtual appliance to be run in VMWare ESXi (vSphere) or Microsoft Hyper-V and is available from https://www.diladele.com/virtual_appliance.html. Installation scripts for real hardware are hosted on our GitHub repository at https://github.com/diladele/websafety-virtual-appliance.

Please direct all support questions to support at diladele.com or submit through GitHub issue tracker (https://github.com/diladele/websafety-issues/milestones). In the next version we are planning to add antivirus engine ClamAV (and possibly another commercial antivirus) as another ICAP service chain to Admin UI.

Best regards,
Rafael Akchurin
Diladele B.V. 
https://www.diladele.com 

--
P.S. Squid 3.5.27 for Windows is on the way.


From squid3 at treenet.co.nz  Mon Sep  4 12:07:54 2017
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Tue, 5 Sep 2017 00:07:54 +1200
Subject: [squid-users] RC4-MD5 cipher is always enabled?
In-Reply-To: <2119528.JO9MT1vNor@march>
References: <2119528.JO9MT1vNor@march>
Message-ID: <89071cf2-f799-a3e4-9da4-df1e2f240b0e@treenet.co.nz>

On 04/09/17 20:36, chiasa.men wrote:
> "RC4-MD5" seems to be always enabled. Is there a way to prohibit RC4-MD5?
> 
> 
> 
> squid.conf:
> https_port 3128 accel defaultsite=www.example.com cert=/example/cert.pem key=/
> example/key.pem

Above line configures the what Squid listening port parameters are. 
There are no cipher restrictions listed, so any cipher the library 
configuration allows is accepted on client->Squid connections.



> sslproxy_version 6
> sslproxy_options NO_SSLv2,NO_SSLv3,NO_TLSv1,NO_TLSv1_1,NO_TICKET
> sslproxy_cipher ECDHE-ECDSA-AES256-GCM-SHA384:!RC4:!MD5
> 

These lines configure what Squid uses on its outbound server 
connections. Those connections (only) are restricted by !RC4:!MD5.


Is the problem obvious now?


To make the Squid listening port reject RC4 or MD5 you need to add an 
ssloptions= or sslcipher= parameter to the port line. Its syntax is the 
same as the values on the sslproxy_* lines.


PS;
  To make other services on the machine gain these same TLS protections 
you should find and alter the library config file instead. OpenSSL's 
libssl is a bit unusual, despite being a library it has its own 
system-wide config file just like applications.

The squid.conf should only contain things which are different from your 
machines basic security profile.


HTH
Amos


From squid3 at treenet.co.nz  Mon Sep  4 14:55:42 2017
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Tue, 5 Sep 2017 02:55:42 +1200
Subject: [squid-users] external ACL queue overload
In-Reply-To: <1269966217.2425378.1504509217679@mail.yahoo.com>
References: <1269966217.2425378.1504509217679.ref@mail.yahoo.com>
 <1269966217.2425378.1504509217679@mail.yahoo.com>
Message-ID: <0d241494-ab65-4df9-2ddc-ecbc79976a1a@treenet.co.nz>

On 04/09/17 19:13, Vieri wrote:
> Hi,
> 
> I sometimes get messages such as this one:
> 
> WARNING: external ACL 'bllookup' queue overload. Using stale result.
> 
> My squid.conf has this defined:
> 
> external_acl_type bllookup ttl=60 %URI /opt/custom/scripts/run/scripts/firewall/squid_url_lookup.pl -tbl_name=shallalist_bl adv,aggressive,...
> 
> Does it make sense to increase the following values:
> 
> ttl=86400 children-max=50 children-startup=2 children-idle=5
> 

Maybe. The ttl= value is entirely up to you, it should be long enough 
not to send too many queries to the helper, and short enough that 
changes to the lists which result in OK responses do not lag overly 
long. Use negative_ttl= to tune the equivalent TTL for ERR results.



> I'd like to know what "children-startup" means exactly.

startup is the number of helpers Squid will start immediately on 
starting or reconfiguring. If used it should be set to about the number 
necessary for handling your baseline traffic load.

idle is the number of new helpers Squid will start in a batch together 
if it cannot send a lookup to the already running ones. This may take 
some seconds so the value needs to be large enough to pick up the minor 
peak of traffic accumulated during that delay - but otherwise is arbitrary.

max is the limit beyond which Squid will not start more helpers, even if 
idle says more are needed.


Amos


From squid3 at treenet.co.nz  Mon Sep  4 15:23:14 2017
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Tue, 5 Sep 2017 03:23:14 +1200
Subject: [squid-users] squid with quota limit using external helper
 problem !
In-Reply-To: <3A8BBF8D-4FC5-40F0-A81D-EEA9768257C6@gmail.com>
References: <47F841AF-71FC-4162-BFA9-A1CABFA6FC50@netstream.ps>
 <47a5c10e-ae64-e366-d6bc-f494836da3a4@treenet.co.nz>
 <3A8BBF8D-4FC5-40F0-A81D-EEA9768257C6@gmail.com>
Message-ID: <fb3778d6-4398-7d83-51aa-1be7fed0350f@treenet.co.nz>

On 04/09/17 21:09, Ahmed Alzaeem wrote:
> Hi amos , thanks for the kind response .
> 
> i denied to rebuild squid without IPV6 support and seems now no error in 
> helper .
> 
> 
> i just curious to know about the auth directors in squid how should i 
> arrange it :
> 
> acl localnet src all
> 
> auth_param basic program /lib/squid/basic_ncsa_auth ?/etc/squid/squid_user
> acl ncsa_users proxy_auth REQUIRED
> auth_param basic children 1000
> 
> external_acl_type bandwidth_check ttl=0 %SRC /usr/local/bin/bandwidth_check
> acl bandwidth_auth external bandwidth_check
> http_access allow localnet bandwidth_auth
> http_access deny ?localnet !bandwidth_auth
> ###################################################
> http_access allow ncsa_users
> 
> 
> is above correct sequence to block any user exceeded quota ?

I put comments under each problematic line in my last post about the 
problems in that http_access sequence. The config has not changed, so 
they are all still occuring.


> also should i use
> external_acl_type bandwidth_check ttl=0 %SRC /usr/local/bin/bandwidth_check
> 
> or
> 
> external_acl_type bandwidth_check ttl=0 %SRC %LOGIN 
> /usr/local/bin/bandwidth_check
> 
> or
> 
> external_acl_type bandwidth_check ttl=0 
> *%EXT_USER*?/usr/local/bin/bandwidth_check
> 

That is up to you, and depends on what you want the helper to be checking.

%LOGIN supplies the HTTP authentication login. It will trigger a full 
authentication sequence if there are no credentials, so place all uses 
of ACLs involving this after your ncsa_users login check.

%EXT_USER supplies the user= value some earlier external_acl_type helper 
produced. You do not seem to have any other external ACL helpers - so 
this is probably not for you.

If you have a mix of authentication methods happening you might want the 
%un code.

Amos


From erdosain9 at gmail.com  Mon Sep  4 16:20:36 2017
From: erdosain9 at gmail.com (erdosain9)
Date: Mon, 4 Sep 2017 09:20:36 -0700 (MST)
Subject: [squid-users] SSL3_GET_SERVER_CERTIFICATE:certificate verify failed
Message-ID: <1504542036808-0.post@n4.nabble.com>

Hi.
Im having a lot of this in cache.log... is this normal?? The https is access
is working fine... but i have those error.

2017/09/04 13:10:58 kid1| Error negotiating SSL on FD 467:
error:14090086:SSL routines:SSL3_GET_SERVER_CERTIFICATE:certificate verify
failed (
1/-1/0)
2017/09/04 13:10:58 kid1| Error negotiating SSL on FD 58: error:14090086:SSL
routines:SSL3_GET_SERVER_CERTIFICATE:certificate verify failed (1
/-1/0)
2017/09/04 13:10:59 kid1| Error negotiating SSL on FD 640:
error:14090086:SSL routines:SSL3_GET_SERVER_CERTIFICATE:certificate verify
failed (
1/-1/0)
2017/09/04 13:11:01 kid1| Error negotiating SSL on FD 640:
error:14090086:SSL routines:SSL3_GET_SERVER_CERTIFICATE:certificate verify
failed (
1/-1/0)
2017/09/04 13:11:01 kid1| Error negotiating SSL on FD 794:
error:14090086:SSL routines:SSL3_GET_SERVER_CERTIFICATE:certificate verify
failed (
1/-1/0)
2017/09/04 13:11:02 kid1| Error negotiating SSL on FD 314:
error:14090086:SSL routines:SSL3_GET_SERVER_CERTIFICATE:certificate verify
failed (
1/-1/0)
2017/09/04 13:11:28 kid1| Error negotiating SSL on FD 299:
error:14090086:SSL routines:SSL3_GET_SERVER_CERTIFICATE:certificate verify
failed (
1/-1/0)
2017/09/04 13:11:29 kid1| Error negotiating SSL on FD 299:
error:14090086:SSL routines:SSL3_GET_SERVER_CERTIFICATE:certificate verify
failed (
1/-1/0)
2017/09/04 13:11:31 kid1| Error negotiating SSL on FD 620:
error:14090086:SSL routines:SSL3_GET_SERVER_CERTIFICATE:certificate verify
failed (
1/-1/0)
2017/09/04 13:11:31 kid1| Error negotiating SSL on FD 105:
error:14090086:SSL routines:SSL3_GET_SERVER_CERTIFICATE:certificate verify
failed (
1/-1/0)
2017/09/04 13:11:31 kid1| Error negotiating SSL on FD 495:
error:14090086:SSL routines:SSL3_GET_SERVER_CERTIFICATE:certificate verify
failed (
1/-1/0)
2017/09/04 13:11:32 kid1| Error negotiating SSL on FD 495:
error:14090086:SSL routines:SSL3_GET_SERVER_CERTIFICATE:certificate verify
failed (
1/-1/0)
2017/09/04 13:11:39 kid1| Error negotiating SSL on FD 457:
error:14090086:SSL routines:SSL3_GET_SERVER_CERTIFICATE:certificate verify
failed (
1/-1/0)
2017/09/04 13:11:40 kid1| Error negotiating SSL on FD 457:
error:14090086:SSL routines:SSL3_GET_SERVER_CERTIFICATE:certificate verify
failed (
1/-1/0)
2017/09/04 13:11:40 kid1| Error negotiating SSL on FD 452:
error:14090086:SSL routines:SSL3_GET_SERVER_CERTIFICATE:certificate verify
failed (1/-1/0)
2017/09/04 13:11:41 kid1| Error negotiating SSL on FD 452:
error:14090086:SSL routines:SSL3_GET_SERVER_CERTIFICATE:certificate verify
failed (1/-1/0)
2017/09/04 13:11:41 kid1| Error negotiating SSL on FD 210:
error:14090086:SSL routines:SSL3_GET_SERVER_CERTIFICATE:certificate verify
failed (1/-1/0)
2017/09/04 13:11:42 kid1| Error negotiating SSL on FD 210:
error:14090086:SSL routines:SSL3_GET_SERVER_CERTIFICATE:certificate verify
failed (1/-1/0)
2017/09/04 13:11:58 kid1| Error negotiating SSL on FD 197:
error:14090086:SSL routines:SSL3_GET_SERVER_CERTIFICATE:certificate verify
failed (1/-1/0)
2017/09/04 13:11:58 kid1| Error negotiating SSL on FD 197:
error:14090086:SSL routines:SSL3_GET_SERVER_CERTIFICATE:certificate verify
failed (1/-1/0)
2017/09/04 13:11:59 kid1| Error negotiating SSL on FD 472:
error:14090086:SSL routines:SSL3_GET_SERVER_CERTIFICATE:certificate verify
failed (:




--
Sent from: http://squid-web-proxy-cache.1019090.n4.nabble.com/Squid-Users-f1019091.html


From erdosain9 at gmail.com  Mon Sep  4 16:49:08 2017
From: erdosain9 at gmail.com (erdosain9)
Date: Mon, 4 Sep 2017 09:49:08 -0700 (MST)
Subject: [squid-users] DNS Server Failure
Message-ID: <1504543748040-0.post@n4.nabble.com>

Hi.
There is a way to know what can be happend with this failure?
Thanks to all.

Internal DNS Statistics:

The Queue:
                       DELAY SINCE
  ID   SIZE SENDS FIRST SEND LAST SEND M FQDN
------ ---- ----- ---------- --------- - ----

DNS jumbo-grams: not working

Nameservers:
IP ADDRESS                                     # QUERIES # REPLIES Type
---------------------------------------------- --------- --------- --------
192.168.1.107                                     27862     27862 recurse
192.168.1.222                                       425       411 recurse

Rcode Matrix:
RCODE ATTEMPT1 ATTEMPT2 ATTEMPT3 PROBLEM
    0   590210      205       41 : Success
    1        0        0        0 : Packet Format Error
    2     7165     6950     6909 : DNS Server Failure
    3    21827       10        0 : Non-Existent Domain
    4        0        0        0 : Not Implemented
    5        0        0        0 : Query Refused
    6        0        0        0 : Name Exists when it should not
    7        0        0        0 : RR Set Exists when it should not
    8        0        0        0 : RR Set that should exist does not
    9        0        0        0 : Server Not Authoritative for zone
   10        0        0        0 : Name not contained in zone
   16        0        0        0 : Bad OPT Version or TSIG Signature Failure




--
Sent from: http://squid-web-proxy-cache.1019090.n4.nabble.com/Squid-Users-f1019091.html


From Antony.Stone at squid.open.source.it  Mon Sep  4 16:54:24 2017
From: Antony.Stone at squid.open.source.it (Antony Stone)
Date: Mon, 4 Sep 2017 18:54:24 +0200
Subject: [squid-users] DNS Server Failure
In-Reply-To: <1504543748040-0.post@n4.nabble.com>
References: <1504543748040-0.post@n4.nabble.com>
Message-ID: <201709041854.24525.Antony.Stone@squid.open.source.it>

On Monday 04 September 2017 at 18:49:08, erdosain9 wrote:

> Hi.
> There is a way to know what can be happend with this failure?
> Thanks to all.

Given that the DNS servers are on private IPs, I assume they're internal to 
your network, so what does the logging on the DNS servers themselves say?

> Internal DNS Statistics:
> 
> The Queue:
>                        DELAY SINCE
>   ID   SIZE SENDS FIRST SEND LAST SEND M FQDN
> ------ ---- ----- ---------- --------- - ----
> 
> DNS jumbo-grams: not working
> 
> Nameservers:
> IP ADDRESS                                     # QUERIES # REPLIES Type
> ---------------------------------------------- --------- --------- --------
> 192.168.1.107                                     27862     27862 recurse
> 192.168.1.222                                       425       411 recurse
> 
> Rcode Matrix:
> RCODE ATTEMPT1 ATTEMPT2 ATTEMPT3 PROBLEM
>     0   590210      205       41 : Success
>     1        0        0        0 : Packet Format Error
>     2     7165     6950     6909 : DNS Server Failure
>     3    21827       10        0 : Non-Existent Domain
>     4        0        0        0 : Not Implemented
>     5        0        0        0 : Query Refused
>     6        0        0        0 : Name Exists when it should not
>     7        0        0        0 : RR Set Exists when it should not
>     8        0        0        0 : RR Set that should exist does not
>     9        0        0        0 : Server Not Authoritative for zone
>    10        0        0        0 : Name not contained in zone
>    16        0        0        0 : Bad OPT Version or TSIG Signature
> Failure

-- 
Angela Merkel arrives at Paris airport.
"Nationality?" asks the immigration officer.
"German," she replies.
"Occupation?"
"No, just here for a summit conference."

                                                   Please reply to the list;
                                                         please *don't* CC me.


From ahmed.zaeem at netstream.ps  Mon Sep  4 19:17:44 2017
From: ahmed.zaeem at netstream.ps (--Ahmad--)
Date: Mon, 4 Sep 2017 22:17:44 +0300
Subject: [squid-users] squid cache peer not rotating over round robin !
Message-ID: <478FA637-6576-47EC-A193-2185B865626D@netstream.ps>

hello folks 

I?m trying to rotate squid request over several peers 
my config are below but they are only stuck with 1 peer .

acl custNet dstdomain  .trustly.com  .ing.nl  .adyen.com .rabobank.nl .abn.nl .iplocation.net .abnamro.com .abnamro.nl .abnamro.nl
cache_peer 66.78.18.1 parent 41311 0 no-query round-robin no-digest no-tproxy proxy-only
cache_peer 66.78.18.2 parent 41311 0 no-query round-robin no-digest no-tproxy proxy-only
cache_peer 66.78.18.3 parent 41311 0 0 no-query round-robin no-digest no-tproxy proxy-only
cache_peer 66.78.18.4 parent 41311 0 0 no-query round-robin no-digest no-tproxy proxy-only
cache_peer 66.78.18.5 parent 41311 0 no-query round-robin no-digest no-tproxy proxy-only
cache_peer 66.78.18.6 parent 41311 0 no-query round-robin no-digest no-tproxy proxy-only
cache_peer 66.78.18.7 parent 41311 0 no-query round-robin no-digest no-tproxy proxy-only
cache_peer 66.78.18.8 parent 41311 0 no-query round-robin no-digest no-tproxy proxy-only
cache_peer 66.78.18.9 parent 41311 0 no-query round-robin no-digest no-tproxy proxy-only
#####################
never_direct allow custNet
http_access allow custNet


any idea what wrong with round robin above ?


cheers



From chiasa.men at web.de  Tue Sep  5 08:55:56 2017
From: chiasa.men at web.de (chiasa.men)
Date: Tue, 05 Sep 2017 10:55:56 +0200
Subject: [squid-users] RC4-MD5 cipher is always enabled?
In-Reply-To: <89071cf2-f799-a3e4-9da4-df1e2f240b0e@treenet.co.nz>
References: <2119528.JO9MT1vNor@march>
 <89071cf2-f799-a3e4-9da4-df1e2f240b0e@treenet.co.nz>
Message-ID: <2066590.CTfiIqpzsz@march>

Am Montag, 4. September 2017, 14:07:54 CEST schrieb Amos Jeffries:
> On 04/09/17 20:36, chiasa.men wrote:
> > "RC4-MD5" seems to be always enabled. Is there a way to prohibit RC4-MD5?
> > 
> > 
> > 
> > squid.conf:
> > https_port 3128 accel defaultsite=www.example.com cert=/example/cert.pem
> > key=/ example/key.pem
> 
> Above line configures the what Squid listening port parameters are.
> There are no cipher restrictions listed, so any cipher the library
> configuration allows is accepted on client->Squid connections.
> 
> > sslproxy_version 6
> > sslproxy_options NO_SSLv2,NO_SSLv3,NO_TLSv1,NO_TLSv1_1,NO_TICKET
> > sslproxy_cipher ECDHE-ECDSA-AES256-GCM-SHA384:!RC4:!MD5
> 
> These lines configure what Squid uses on its outbound server
> connections. Those connections (only) are restricted by !RC4:!MD5.
> 
> 
> Is the problem obvious now?
> 
> 
> To make the Squid listening port reject RC4 or MD5 you need to add an
> ssloptions= or sslcipher= parameter to the port line. Its syntax is the
> same as the values on the sslproxy_* lines.
> 
> 
> PS;
>   To make other services on the machine gain these same TLS protections
> you should find and alter the library config file instead. OpenSSL's
> libssl is a bit unusual, despite being a library it has its own
> system-wide config file just like applications.
> 
> The squid.conf should only contain things which are different from your
> machines basic security profile.
> 
> 
> HTH
> Amos
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users

Thanks, that was easy... but:

That does not work:

https_port 3128 accel defaultsite=www.example.com cert=/example/cert.pem key=/
example/key.pem cipher=ECDHE-ECDSA-AES256-GCM-SHA384:!RC4:!MD5

openssl s_client -connect localhost:3128
140048907216536:error:14077410:SSL routines:SSL23_GET_SERVER_HELLO:sslv3 alert 
handshake failure:s23_clnt.c:769:


Allowing RC4 and MD5 works:

https_port 3128 accel defaultsite=www.example.com cert=/example/cert.pem key=/
example/key.pem cipher=ECDHE-ECDSA-AES256-GCM-SHA384:RC4:MD5

openssl s_client -connect localhost:3128
    Cipher    : ECDH-ECDSA-RC4-SHA


But openssl works without allowing RC4 and MD5:

openssl s_server -cert /example/cert.pem -key /example/key.pem -cipher 'ECDHE-
ECDSA-AES256-GCM-SHA384:!RC4:!MD5'

openssl s_client -connect localhost:4433 
    Cipher    : ECDHE-ECDSA-AES256-GCM-SHA384


So I guess the certificate and the openssl part should work. 
Maybe you could give another advice?

btw, the used squid version:
Squid Cache: Version 3.5.12
Service Name: squid
Ubuntu linux
configure options:  '--build=x86_64-linux-gnu' '--prefix=/usr' '--includedir=$
{prefix}/include' '--mandir=${prefix}/share/man' '--infodir=${prefix}/share/
info' '--sysconfdir=/etc' '--localstatedir=/var' '--libexecdir=${prefix}/lib/
squid3' '--srcdir=.' '--disable-maintainer-mode' '--disable-dependency-
tracking' '--disable-silent-rules' 'BUILDCXXFLAGS=-g -O2 -fPIE -fstack-
protector-strong -Wformat -Werror=format-security -Wl,-Bsymbolic-functions -
fPIE -pie -Wl,-z,relro -Wl,-z,now' '--datadir=/usr/share/squid' '--
sysconfdir=/etc/squid' '--libexecdir=/usr/lib/squid' '--mandir=/usr/share/man' 
'--enable-inline' '--disable-arch-native' '--enable-async-io=8' '--enable-
storeio=ufs,aufs,diskd,rock' '--enable-removal-policies=lru,heap' '--enable-
delay-pools' '--enable-cache-digests' '--enable-icap-client' '--enable-follow-
x-forwarded-for' '--enable-auth-
basic=DB,fake,getpwnam,LDAP,NCSA,NIS,PAM,POP3,RADIUS,SASL,SMB' '--enable-auth-
digest=file,LDAP' '--enable-auth-negotiate=kerberos,wrapper' '--enable-auth-
ntlm=fake,smb_lm' '--enable-external-acl-
helpers=file_userip,kerberos_ldap_group,LDAP_group,session,SQL_session,unix_group,wbinfo_group' 
'--enable-url-rewrite-helpers=fake' '--enable-eui' '--with-openssl' '--enable-
ssl-crtd' '--enable-esi' '--enable-icmp' '--enable-zph-qos' '--enable-ecap' 
'--disable-translation' '--with-swapdir=/var/spool/squid' '--with-logdir=/var/
log/squid' '--with-pidfile=/var/run/squid.pid' '--with-filedescriptors=65536' 
'--with-large-files' '--with-default-user=proxy' '--enable-build-info=Ubuntu 
linux' '--enable-linux-netfilter' 'build_alias=x86_64-linux-gnu' 'CFLAGS=-g -
O2 -fPIE -fstack-protector-strong -Wformat -Werror=format-security -Wall' 
'LDFLAGS=-Wl,-Bsymbolic-functions -fPIE -pie -Wl,-z,relro -Wl,-z,now' 
'CPPFLAGS=-Wdate-time -D_FORTIFY_SOURCE=2' 'CXXFLAGS=-g -O2 -fPIE -fstack-
protector-strong -Wformat -Werror=format-security'



From rentorbuy at yahoo.com  Tue Sep  5 09:31:47 2017
From: rentorbuy at yahoo.com (Vieri)
Date: Tue, 5 Sep 2017 09:31:47 +0000 (UTC)
Subject: [squid-users] gateway failure
References: <134856292.3128555.1504603907833.ref@mail.yahoo.com>
Message-ID: <134856292.3128555.1504603907833@mail.yahoo.com>

Hi,

I'm sometimes getting hit by ERR_GATEWAY_FAILURE. I'd like to know what could be causing this issue.
When this happens on a production server, I don't have much time to investigate.
I usually only have enough time to ssh into the squid server, test internet access via command line, and before I know it, the issue's gone.

Nothing much in cache.log. I have debug_options rotate=1 ALL,1. I'd rather not set ALL,9 on a production system for something that happens maybe only once every 2 or 3 days.
I'm not sure however which sections and levels to  set so I can get an idea as to why I'm getting ERR_GATEWAY_FAILURE.

https://wiki.squid-cache.org/KnowledgeBase/DebugSections

Any suggestions?

Thanks,

Vieri


From rentorbuy at yahoo.com  Tue Sep  5 09:36:55 2017
From: rentorbuy at yahoo.com (Vieri)
Date: Tue, 5 Sep 2017 09:36:55 +0000 (UTC)
Subject: [squid-users] external ACL queue overload
In-Reply-To: <0d241494-ab65-4df9-2ddc-ecbc79976a1a@treenet.co.nz>
References: <1269966217.2425378.1504509217679.ref@mail.yahoo.com>
 <1269966217.2425378.1504509217679@mail.yahoo.com>
 <0d241494-ab65-4df9-2ddc-ecbc79976a1a@treenet.co.nz>
Message-ID: <1800813293.3124413.1504604215018@mail.yahoo.com>

Thanks for clearing that up.
I haven't seen queue overloads since. Hope this keeps up.

Vieri


From squid3 at treenet.co.nz  Tue Sep  5 09:57:06 2017
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Tue, 5 Sep 2017 21:57:06 +1200
Subject: [squid-users] RC4-MD5 cipher is always enabled?
In-Reply-To: <2066590.CTfiIqpzsz@march>
References: <2119528.JO9MT1vNor@march>
 <89071cf2-f799-a3e4-9da4-df1e2f240b0e@treenet.co.nz>
 <2066590.CTfiIqpzsz@march>
Message-ID: <46c7124d-58bc-5300-8e41-81716d67747e@treenet.co.nz>

On 05/09/17 20:55, chiasa.men wrote> Thanks, that was easy... but:
> 
> That does not work:
> 
> https_port 3128 accel defaultsite=www.example.com cert=/example/cert.pem key=/
> example/key.pem cipher=ECDHE-ECDSA-AES256-GCM-SHA384:!RC4:!MD5
> 
> openssl s_client -connect localhost:3128
> 140048907216536:error:14077410:SSL routines:SSL23_GET_SERVER_HELLO:sslv3 alert
> handshake failure:s23_clnt.c:769:
> 
> 
> Allowing RC4 and MD5 works:
> 
> https_port 3128 accel defaultsite=www.example.com cert=/example/cert.pem key=/
> example/key.pem cipher=ECDHE-ECDSA-AES256-GCM-SHA384:RC4:MD5
> 
> openssl s_client -connect localhost:3128
>      Cipher    : ECDH-ECDSA-RC4-SHA
> 
> 
> But openssl works without allowing RC4 and MD5:
> 
> openssl s_server -cert /example/cert.pem -key /example/key.pem -cipher 'ECDHE-
> ECDSA-AES256-GCM-SHA384:!RC4:!MD5'
> 
> openssl s_client -connect localhost:4433
>      Cipher    : ECDHE-ECDSA-AES256-GCM-SHA384
> 
> 
> So I guess the certificate and the openssl part should work.
> Maybe you could give another advice?

"
cipher=
	Colon separated list of supported ciphers.
	NOTE: some ciphers such as EDH ciphers depend on
	additional settings. If those settings are
	omitted the ciphers may be silently ignored
	by the OpenSSL library."
"

For the ECDHE-* ciphers to work the server end needs to be configured 
with curve parameters. That is done the tls-dh= option with a curve name 
and

"
tls-dh=[curve:]file
	File containing DH parameters for temporary/ephemeral DH key
	exchanges, optionally prefixed by a curve for ephemeral ECDH
	key exchanges.
	See OpenSSL documentation for details on how to create the
	DH parameter file. Supported curves for ECDH can be listed
	using the "openssl ecparam -list_curves" command.

	WARNING: EDH and EECDH ciphers will be silently disabled if
	this option is not set.
"


> 
> btw, the used squid version:
> Squid Cache: Version 3.5.12
> Service Name: squid
> Ubuntu linux


Please upgrade. Somewhat urgently.

* TLS/SSL has had a *lot* of progress in the past few years. There are 
many security related issues resolved in the latest releases which exist 
in the older ones.

* ECDHE is a good example of the change. It is not supported *at all* by 
that old version of Squid.

When using TLS/SSL support Squid-3.5.24 is currently the oldest 
acceptable Squid release as it contains extra mitigation for TLS DoS 
vulnerabilities. The current 3.5.27 would be best from the 3.5 series.

If you are not already aware there is no official security 
support/tracking from Debian and Ubuntu for TLS/SSL vulnerabilities as 
their packages do not ship with OpenSSL support. So following their 
stable/security package version is of no benefit for TLS/SSL issues, you 
need to track upstream releases yourself when custom building software 
(that goes for anything, not just Squid).

Amos


From squid3 at treenet.co.nz  Tue Sep  5 10:22:01 2017
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Tue, 5 Sep 2017 22:22:01 +1200
Subject: [squid-users] gateway failure
In-Reply-To: <134856292.3128555.1504603907833@mail.yahoo.com>
References: <134856292.3128555.1504603907833.ref@mail.yahoo.com>
 <134856292.3128555.1504603907833@mail.yahoo.com>
Message-ID: <667cd486-558c-9c00-a743-b4e3447c659e@treenet.co.nz>

On 05/09/17 21:31, Vieri wrote:
> Hi,
> 
> I'm sometimes getting hit by ERR_GATEWAY_FAILURE. I'd like to know what could be causing this issue.
> When this happens on a production server, I don't have much time to investigate.
> I usually only have enough time to ssh into the squid server, test internet access via command line, and before I know it, the issue's gone.
> 

Squid generates GATEWAY_FAILURE when URL-redirector/rewriter is not 
responding or TLS handshakes fail.

If it is the crypo issues that is exactly the kind of things which your 
SSH connection will not be able to get through either so of course is 
already gone when that TCP + encryption succeeds.


> Nothing much in cache.log. I have debug_options rotate=1 ALL,1. I'd rather not set ALL,9 on a production system for something that happens maybe only once every 2 or 3 days.
> I'm not sure however which sections and levels to  set so I can get an idea as to why I'm getting ERR_GATEWAY_FAILURE.
> 
> https://wiki.squid-cache.org/KnowledgeBase/DebugSections
> 
> Any suggestions?

In absence of ALL,9 (or ALL,6) you will have to work your way through 
the list of components involved with upstream server connections and 
then any components you are using that can slow Squid down in general or 
periodically.

DNS, Comm, and TLS levels - and also things like Digest creation, store 
rebuild, and cache replacement policy actions. Unfortunately most of 
those are major components used all the time, so not much better than 
ALL,6 in terms of log output.


Main focus obviously is on the domain/server(s) whose URL hit the issue, 
but anything else could be impacting the transaction latency so it is by 
no means certain to be that server.


I would start with DNS to see if the results are coming back fast 
enough. With the latest Squid you will also have to check all the 
permutations of DNS response ordering and timing since the "Happy 
Eyeballs" algorithms can mens Squid is only working with partial DNS 
results and failing when the incomplete IP set are all broken servers.


Then check for ICMPv4/v6 issues on the route(s) between Squid and all 
the servers IPs. A lot of networks still have disabled ICMP fully or 
partially in ways which can break route recovery. Lack of ICMP is how 
temporary router power spikes etc halfway across the Internet can kill 
traffic on your network for brief times.
  On the one hand these ICMP issues are not temporary (though Squid may 
only hit them if trying certain IPs), on the other it is not something 
that can be tested or logged from inside Squid. You will need to setup 
some sort of monitor to watch servers Squid connects to - maybe a 
trigger to automatically check anything that results in the gateway 
error being logged before you can manually login.


Next on the line would be TLS handshake behaviours with all IPs of the 
problem server(s). That is easier to test after the fact, but don't take 
success as a guarantee. It could still be a temporary failure in the 
handshake.

 From there it is pot-luck and hope there are some clues lying about to 
hint at good directions.

Amos


From squid3 at treenet.co.nz  Tue Sep  5 10:39:19 2017
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Tue, 5 Sep 2017 22:39:19 +1200
Subject: [squid-users] squid cache peer not rotating over round robin !
In-Reply-To: <478FA637-6576-47EC-A193-2185B865626D@netstream.ps>
References: <478FA637-6576-47EC-A193-2185B865626D@netstream.ps>
Message-ID: <9d353983-6cc1-f239-9616-2d50c89d3b5b@treenet.co.nz>

On 05/09/17 07:17, --Ahmad-- wrote:
> hello folks
> 
> I?m trying to rotate squid request over several peers
> my config are below but they are only stuck with 1 peer .
> 
> acl custNet dstdomain  .trustly.com  .ing.nl  .adyen.com .rabobank.nl .abn.nl .iplocation.net .abnamro.com .abnamro.nl .abnamro.nl
> cache_peer 66.78.18.1 parent 41311 0 no-query round-robin no-digest no-tproxy proxy-only
> cache_peer 66.78.18.2 parent 41311 0 no-query round-robin no-digest no-tproxy proxy-only
> cache_peer 66.78.18.3 parent 41311 0 0 no-query round-robin no-digest no-tproxy proxy-only
> cache_peer 66.78.18.4 parent 41311 0 0 no-query round-robin no-digest no-tproxy proxy-only
> cache_peer 66.78.18.5 parent 41311 0 no-query round-robin no-digest no-tproxy proxy-only
> cache_peer 66.78.18.6 parent 41311 0 no-query round-robin no-digest no-tproxy proxy-only
> cache_peer 66.78.18.7 parent 41311 0 no-query round-robin no-digest no-tproxy proxy-only
> cache_peer 66.78.18.8 parent 41311 0 no-query round-robin no-digest no-tproxy proxy-only
> cache_peer 66.78.18.9 parent 41311 0 no-query round-robin no-digest no-tproxy proxy-only
> #####################
> never_direct allow custNet
> http_access allow custNet
> 
> 
> any idea what wrong with round robin above ?
> 

The lines for peer *.3 and *.4 contain an extra '0' in the options area 
after ICP-port which may be preventing Squid from loading this config.

Other than that nothing is visible from the details you mention.


Some random possibilities:

* Check that all peers are detected as ALIVE by Squid. Peers that are 
marked as DEAD (10 consecutive failures occured) are skipped in the load 
balancing algorithms. That 'no-query' disables the UDP checks used to 
detect peers going from DEAD to LIVE status, so error recovery will be 
quite slow.


* Check that the "requests" you are referring to are not all inside a 
single CONNECT tunnel. Tunnels count as a single request in plain-text 
HTTP no matter how much 'crypted traffic they contain.


I assume this is the same 3.5.22 you mentioned using in threads from a 
few days ago.

Amos


From squid3 at treenet.co.nz  Tue Sep  5 11:01:59 2017
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Tue, 5 Sep 2017 23:01:59 +1200
Subject: [squid-users] SSL3_GET_SERVER_CERTIFICATE:certificate verify
 failed
In-Reply-To: <1504542036808-0.post@n4.nabble.com>
References: <1504542036808-0.post@n4.nabble.com>
Message-ID: <02ed068f-0aba-5e16-4f2f-30d36ee9d9f9@treenet.co.nz>

On 05/09/17 04:20, erdosain9 wrote:
> Hi.
> Im having a lot of this in cache.log... is this normal?? The https is access
> is working fine... but i have those error.
> 
 > 2017/09/04 13:10:58 kid1| Error negotiating SSL on FD 467:
 > error:14090086:SSL routines:SSL3_GET_SERVER_CERTIFICATE:certificate 
verify
 > failed (
 > 1/-1/0)


Yes and no. "Normal" is relative to why it is happening.

eg if your network is under attack it is "normal" to see signs like 
this, but hardly desirable.

On the other hand if the CA certificate being verified has expired or 
revoked it is both normal and desirable to see these instead of letting 
the traffic though. Opinions on that differ a lot though.



* Check that your Squid machines ca-certificates are up to date with the 
latest ones available. That can make your proxy unable to deal with CA 
changes unless you stay up to date. Regular updates are on the order of 
weeks, but can happen with no notice if any CA is breached or goes rogue.

* Check that your crypto library is also the latest available. Some 
types of change in TLS extensions can lead to cert errors if the library 
does not understand what fields in the server cert mean. This also helps 
prevent many cipher related errors.

* Take a closer look at the HTTP(S) transaction using the mentioned FD 
number. That may need a section 11,2 trace to see the URL and server 
names and/or IP. See if the openssl command line tools can tell you what 
is non-verifiable about the server cert.

* If it turns out to be an intermediary cert not known by Squid, check 
carefully whether you actually want to trust it. If so you can use 
sslproxy_foreign_intermediate_certs to load it explicitly (or Squid-4 
should auto-download as needed).
<http://www.squid-cache.org/Doc/config/sslproxy_foreign_intermediate_certs/>


It is rarely any other type of occurance that can be solved by Squid. 
The above should provide some clues to further debugging if necessary.

Amos


From eliezer at ngtech.co.il  Tue Sep  5 16:58:48 2017
From: eliezer at ngtech.co.il (Eliezer Croitoru)
Date: Tue, 5 Sep 2017 19:58:48 +0300
Subject: [squid-users] gateway failure
In-Reply-To: <134856292.3128555.1504603907833@mail.yahoo.com>
References: <134856292.3128555.1504603907833.ref@mail.yahoo.com>
 <134856292.3128555.1504603907833@mail.yahoo.com>
Message-ID: <375001d32668$3e82f360$bb88da20$@ngtech.co.il>

Hey Vieri,

You can run a crontab job(s) that will run periodic tests against public dns and http(s) servers.
Also try to enable path mtu discovery which might help in some cases.
You can also try to use iptables clamp-mss  \ set-mss to either set a static or by the path mtu. Take a peek at:
http://lartc.org/howto/lartc.cookbook.mtu-mss.html

You can try first to use:
iptables -I FORWARD -p tcp --tcp-flags SYN,RST SYN -j TCPMSS --set-mss 1450

or:
iptables -I FORWARD -p tcp --tcp-flags SYN,RST SYN -j TCPMSS  --clamp-mss-to-pmtu

Also try to actually discover the path mtu using:
# tracepath -n www.squid-cache.org

I don't know what OS you are running there but it's nice to have either munin or nagios on the squid server to debug such issues.(after you verify it's not or is mtu issue).

Let me know if you need any help setting up a couple testing scripts.

Eliezer

----
Eliezer Croitoru
Linux System Administrator
Mobile: +972-5-28704261
Email: eliezer at ngtech.co.il



-----Original Message-----
From: squid-users [mailto:squid-users-bounces at lists.squid-cache.org] On Behalf Of Vieri
Sent: Tuesday, September 5, 2017 12:32
To: squid-users at lists.squid-cache.org
Subject: [squid-users] gateway failure

Hi,

I'm sometimes getting hit by ERR_GATEWAY_FAILURE. I'd like to know what could be causing this issue.
When this happens on a production server, I don't have much time to investigate.
I usually only have enough time to ssh into the squid server, test internet access via command line, and before I know it, the issue's gone.

Nothing much in cache.log. I have debug_options rotate=1 ALL,1. I'd rather not set ALL,9 on a production system for something that happens maybe only once every 2 or 3 days.
I'm not sure however which sections and levels to  set so I can get an idea as to why I'm getting ERR_GATEWAY_FAILURE.

https://wiki.squid-cache.org/KnowledgeBase/DebugSections

Any suggestions?

Thanks,

Vieri
_______________________________________________
squid-users mailing list
squid-users at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-users



From shradhashukla90 at gmail.com  Tue Sep  5 20:19:03 2017
From: shradhashukla90 at gmail.com (SShukla)
Date: Tue, 5 Sep 2017 13:19:03 -0700 (MST)
Subject: [squid-users] How to setup squid as reverse proxy to intercept
	Office365 traffic
Message-ID: <1504642743764-0.post@n4.nabble.com>

Hi,

 I am trying to setup squid as reverse proxy to intercept office365 traffic.
 The configuration we need to do in squid.conf file, I need help with that.
 Could anyone provide me sample config file that would help me set this
 up? or any directions will be much appreciated.

 Thanks



--
Sent from: http://squid-web-proxy-cache.1019090.n4.nabble.com/Squid-Users-f1019091.html


From Antony.Stone at squid.open.source.it  Tue Sep  5 22:08:55 2017
From: Antony.Stone at squid.open.source.it (Antony Stone)
Date: Wed, 6 Sep 2017 00:08:55 +0200
Subject: [squid-users] How to setup squid as reverse proxy to intercept
	Office365 traffic
In-Reply-To: <1504642743764-0.post@n4.nabble.com>
References: <1504642743764-0.post@n4.nabble.com>
Message-ID: <201709060008.56111.Antony.Stone@squid.open.source.it>

On Tuesday 05 September 2017 at 22:19:03, SShukla wrote:

> Hi,
> 
>  I am trying to setup squid as reverse proxy to intercept office365
> traffic.

Why do you want to set up a *reverse* proxy for Office 365 traffic?

Are you running Office 365 servers and want some sort of front-end for them?

I thought Microsoft ran all the Office 365 servers, and everyone else was just 
clients - in which surely what you need is a *forward* proxy.

> The configuration we need to do in squid.conf file, I need help with that.
> Could anyone provide me sample config file that would help me set this up? or
> any directions will be much appreciated.

Please explain what you are trying to achieve - what problem you are trying to 
solve - what you want to happen as a result of inserting this "magic Squid 
box" into your network.

Once we know that, we might be able to help you constructively.


Regards,


Antony.

-- 
The Magic Words are Squeamish Ossifrage.

                                                   Please reply to the list;
                                                         please *don't* CC me.


From alex at dvm.esines.cu  Wed Sep  6 13:26:00 2017
From: alex at dvm.esines.cu (=?UTF-8?Q?Alex_Guti=c3=a9rrez_Mart=c3=adnez?=)
Date: Wed, 6 Sep 2017 09:26:00 -0400
Subject: [squid-users] cache config
Message-ID: <0ecbbcfd-4ada-5f41-6f88-acd73807ecd6@dvm.esines.cu>

Hi everyone, i have 100 GB on my cache partition, but squid only use 1.5 
GB. My internet connection its incredibly slow, any advice on how 
optimize my connection will be appreciated.

This is the configuration of my cache.

maximum_object_size 300 MB
cache_dir aufs /var/cache/squid3 1024000 16 256
cache_mem 256 MB
cache_store_log /var/cache/squid3/cache_store.log
coredump_dir /var/cache/squid3/dump
minimum_expiry_time 600 seconds
cache_swap_low 87
cache_swap_high 90
############################
client_db off
offline_mode off
cache_swap_low 87
cache_swap_high 90
cache_replacement_policy heap GDSF
maximum_object_size_in_memory 128 KB
chunked_request_body_max_size 4096 KB
half_closed_clients off
############################
# establecemos los archivos de volcado en /var/cache/squid3/
coredump_dir /var/cache/squid3/
refresh_pattern ^ftp: 1440 20% 10080
refresh_pattern ^gopher: 1440 0% 1440
refresh_pattern -i \.(gif|png|jpg|jpeg|ico)$ 10080 90% 43200 
override-expire ignore-no-store ignore-private
refresh_pattern -i \.(iso|avi|wav|mp3|mp4|mpeg|swf|flv|x-flv)$ 43200 90% 
432000 override-expire ignore-no-store ignore-private
refresh_pattern -i 
\.(deb|rpm|exe|zip|tar|tgz|ram|rar|bin|ppt|doc|tiff|pdf)$ 10080 90% 
43200 override-expire ignore-no-store ignore-private
refresh_pattern -i \.index.(html|htm)$ 0 40% 10080
refresh_pattern -i \.(html|htm|css|js)$ 1440 40% 40320
refresh_pattern . 0 40% 40320

-- 
Saludos Cordiales

Lic. Alex Guti?rrez Mart?nez





From chiasa.men at web.de  Wed Sep  6 15:33:57 2017
From: chiasa.men at web.de (chiasa.men)
Date: Wed, 06 Sep 2017 17:33:57 +0200
Subject: [squid-users] RC4-MD5 cipher is always enabled?
In-Reply-To: <46c7124d-58bc-5300-8e41-81716d67747e@treenet.co.nz>
References: <2119528.JO9MT1vNor@march> <2066590.CTfiIqpzsz@march>
 <46c7124d-58bc-5300-8e41-81716d67747e@treenet.co.nz>
Message-ID: <7850015.rIaCz1vyRX@march>

Am Dienstag, 5. September 2017, 11:57:06 CEST schrieb Amos Jeffries:
> On 05/09/17 20:55, chiasa.men wrote> Thanks, that was easy... but:
> > That does not work:
> > 
> > https_port 3128 accel defaultsite=www.example.com cert=/example/cert.pem
> > key=/ example/key.pem cipher=ECDHE-ECDSA-AES256-GCM-SHA384:!RC4:!MD5
> > 
> > openssl s_client -connect localhost:3128
> > 140048907216536:error:14077410:SSL routines:SSL23_GET_SERVER_HELLO:sslv3
> > alert handshake failure:s23_clnt.c:769:
> > 
> > 
> > Allowing RC4 and MD5 works:
> > 
> > https_port 3128 accel defaultsite=www.example.com cert=/example/cert.pem
> > key=/ example/key.pem cipher=ECDHE-ECDSA-AES256-GCM-SHA384:RC4:MD5
> > 
> > openssl s_client -connect localhost:3128
> > 
> >      Cipher    : ECDH-ECDSA-RC4-SHA
> > 
> > But openssl works without allowing RC4 and MD5:
> > 
> > openssl s_server -cert /example/cert.pem -key /example/key.pem -cipher
> > 'ECDHE- ECDSA-AES256-GCM-SHA384:!RC4:!MD5'
> > 
> > openssl s_client -connect localhost:4433
> > 
> >      Cipher    : ECDHE-ECDSA-AES256-GCM-SHA384
> > 
> > So I guess the certificate and the openssl part should work.
> > Maybe you could give another advice?
> 
> "
> cipher=
> 	Colon separated list of supported ciphers.
> 	NOTE: some ciphers such as EDH ciphers depend on
> 	additional settings. If those settings are
> 	omitted the ciphers may be silently ignored
> 	by the OpenSSL library."
> "
> 
> For the ECDHE-* ciphers to work the server end needs to be configured
> with curve parameters. That is done the tls-dh= option with a curve name
> and
> 
> "
> tls-dh=[curve:]file
> 	File containing DH parameters for temporary/ephemeral DH key
> 	exchanges, optionally prefixed by a curve for ephemeral ECDH
> 	key exchanges.
> 	See OpenSSL documentation for details on how to create the
> 	DH parameter file. Supported curves for ECDH can be listed
> 	using the "openssl ecparam -list_curves" command.
> 
> 	WARNING: EDH and EECDH ciphers will be silently disabled if
> 	this option is not set.
> "
> 
> > btw, the used squid version:
> > Squid Cache: Version 3.5.12
> > Service Name: squid
> > Ubuntu linux
> 
> Please upgrade. Somewhat urgently.
> 
> * TLS/SSL has had a *lot* of progress in the past few years. There are
> many security related issues resolved in the latest releases which exist
> in the older ones.
> 
> * ECDHE is a good example of the change. It is not supported *at all* by
> that old version of Squid.
> 
> When using TLS/SSL support Squid-3.5.24 is currently the oldest
> acceptable Squid release as it contains extra mitigation for TLS DoS
> vulnerabilities. The current 3.5.27 would be best from the 3.5 series.
> 
> If you are not already aware there is no official security
> support/tracking from Debian and Ubuntu for TLS/SSL vulnerabilities as
> their packages do not ship with OpenSSL support. So following their
> stable/security package version is of no benefit for TLS/SSL issues, you
> need to track upstream releases yourself when custom building software
> (that goes for anything, not just Squid).
> 
> Amos
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users

Thanks - rtfm often helps. Sorry for that!

Furthermore my certificates were not corresponding to the ecc so I had to 
regenerate them via "openssl ecparam" (not openssl rsa). Kind of obvious but I 
just forgot about them.

The version was simply compiled via apt source on Ubuntu. I'm using the 
current version now (un/fortunately Ubuntu is not bleeding edge)



From shradhashukla90 at gmail.com  Wed Sep  6 17:34:15 2017
From: shradhashukla90 at gmail.com (SShukla)
Date: Wed, 6 Sep 2017 10:34:15 -0700 (MST)
Subject: [squid-users] How to setup squid as reverse proxy to intercept
 Office365 traffic
In-Reply-To: <201709060008.56111.Antony.Stone@squid.open.source.it>
References: <1504642743764-0.post@n4.nabble.com>
 <201709060008.56111.Antony.Stone@squid.open.source.it>
Message-ID: <1504719255120-0.post@n4.nabble.com>

* Why do you want to set up a *reverse* proxy for Office 365 traffic? *

We need to use reverse proxy to direct the traffic going to Office 365
through an ICAP Server. 

While trying to configure Squid to do so, we have encountered a few issues,
for example: since Office 365 has an IP range, one of the issue is to figure
out the IP used by Office 365 which can then be specified in squid
configuration 







--
Sent from: http://squid-web-proxy-cache.1019090.n4.nabble.com/Squid-Users-f1019091.html


From eliezer at ngtech.co.il  Wed Sep  6 17:44:45 2017
From: eliezer at ngtech.co.il (Eliezer Croitoru)
Date: Wed, 6 Sep 2017 20:44:45 +0300
Subject: [squid-users] RC4-MD5 cipher is always enabled?
In-Reply-To: <7850015.rIaCz1vyRX@march>
References: <2119528.JO9MT1vNor@march> <2066590.CTfiIqpzsz@march>
 <46c7124d-58bc-5300-8e41-81716d67747e@treenet.co.nz>
 <7850015.rIaCz1vyRX@march>
Message-ID: <3a9101d32737$d429f380$7c7dda80$@ngtech.co.il>

What version of Ubuntu are you using?
I am releasing deb packages for the LTS versions at:
http://ngtech.co.il/repo/ubuntu/

I have yet to update the repo db but the latest version of 3.5 is there for 14.04 and 16.04.(with ssl-bump)

Eliezer

----
Eliezer Croitoru
Linux System Administrator
Mobile: +972-5-28704261
Email: eliezer at ngtech.co.il



-----Original Message-----
From: squid-users [mailto:squid-users-bounces at lists.squid-cache.org] On Behalf Of chiasa.men
Sent: Wednesday, September 6, 2017 18:34
To: squid-users at lists.squid-cache.org
Subject: Re: [squid-users] RC4-MD5 cipher is always enabled?

Am Dienstag, 5. September 2017, 11:57:06 CEST schrieb Amos Jeffries:
> On 05/09/17 20:55, chiasa.men wrote> Thanks, that was easy... but:
> > That does not work:
> > 
> > https_port 3128 accel defaultsite=www.example.com cert=/example/cert.pem
> > key=/ example/key.pem cipher=ECDHE-ECDSA-AES256-GCM-SHA384:!RC4:!MD5
> > 
> > openssl s_client -connect localhost:3128
> > 140048907216536:error:14077410:SSL routines:SSL23_GET_SERVER_HELLO:sslv3
> > alert handshake failure:s23_clnt.c:769:
> > 
> > 
> > Allowing RC4 and MD5 works:
> > 
> > https_port 3128 accel defaultsite=www.example.com cert=/example/cert.pem
> > key=/ example/key.pem cipher=ECDHE-ECDSA-AES256-GCM-SHA384:RC4:MD5
> > 
> > openssl s_client -connect localhost:3128
> > 
> >      Cipher    : ECDH-ECDSA-RC4-SHA
> > 
> > But openssl works without allowing RC4 and MD5:
> > 
> > openssl s_server -cert /example/cert.pem -key /example/key.pem -cipher
> > 'ECDHE- ECDSA-AES256-GCM-SHA384:!RC4:!MD5'
> > 
> > openssl s_client -connect localhost:4433
> > 
> >      Cipher    : ECDHE-ECDSA-AES256-GCM-SHA384
> > 
> > So I guess the certificate and the openssl part should work.
> > Maybe you could give another advice?
> 
> "
> cipher=
> 	Colon separated list of supported ciphers.
> 	NOTE: some ciphers such as EDH ciphers depend on
> 	additional settings. If those settings are
> 	omitted the ciphers may be silently ignored
> 	by the OpenSSL library."
> "
> 
> For the ECDHE-* ciphers to work the server end needs to be configured
> with curve parameters. That is done the tls-dh= option with a curve name
> and
> 
> "
> tls-dh=[curve:]file
> 	File containing DH parameters for temporary/ephemeral DH key
> 	exchanges, optionally prefixed by a curve for ephemeral ECDH
> 	key exchanges.
> 	See OpenSSL documentation for details on how to create the
> 	DH parameter file. Supported curves for ECDH can be listed
> 	using the "openssl ecparam -list_curves" command.
> 
> 	WARNING: EDH and EECDH ciphers will be silently disabled if
> 	this option is not set.
> "
> 
> > btw, the used squid version:
> > Squid Cache: Version 3.5.12
> > Service Name: squid
> > Ubuntu linux
> 
> Please upgrade. Somewhat urgently.
> 
> * TLS/SSL has had a *lot* of progress in the past few years. There are
> many security related issues resolved in the latest releases which exist
> in the older ones.
> 
> * ECDHE is a good example of the change. It is not supported *at all* by
> that old version of Squid.
> 
> When using TLS/SSL support Squid-3.5.24 is currently the oldest
> acceptable Squid release as it contains extra mitigation for TLS DoS
> vulnerabilities. The current 3.5.27 would be best from the 3.5 series.
> 
> If you are not already aware there is no official security
> support/tracking from Debian and Ubuntu for TLS/SSL vulnerabilities as
> their packages do not ship with OpenSSL support. So following their
> stable/security package version is of no benefit for TLS/SSL issues, you
> need to track upstream releases yourself when custom building software
> (that goes for anything, not just Squid).
> 
> Amos
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users

Thanks - rtfm often helps. Sorry for that!

Furthermore my certificates were not corresponding to the ecc so I had to 
regenerate them via "openssl ecparam" (not openssl rsa). Kind of obvious but I 
just forgot about them.

The version was simply compiled via apt source on Ubuntu. I'm using the 
current version now (un/fortunately Ubuntu is not bleeding edge)

_______________________________________________
squid-users mailing list
squid-users at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-users



From Antony.Stone at squid.open.source.it  Thu Sep  7 10:21:22 2017
From: Antony.Stone at squid.open.source.it (Antony Stone)
Date: Thu, 7 Sep 2017 12:21:22 +0200
Subject: [squid-users] How to setup squid as reverse proxy to intercept
	Office365 traffic
In-Reply-To: <1504719255120-0.post@n4.nabble.com>
References: <1504642743764-0.post@n4.nabble.com>
 <201709060008.56111.Antony.Stone@squid.open.source.it>
 <1504719255120-0.post@n4.nabble.com>
Message-ID: <201709071221.23007.Antony.Stone@squid.open.source.it>

On Wednesday 06 September 2017 at 19:34:15, SShukla wrote:

> * Why do you want to set up a *reverse* proxy for Office 365 traffic? *
> 
> We need to use reverse proxy to direct the traffic going to Office 365
> through an ICAP Server.

I still don't understand why you think this needs to be a reverse proxy rather 
than a standard forwarding proxy.

> While trying to configure Squid to do so, we have encountered a few issues,
> for example: since Office 365 has an IP range, one of the issue is to
> figure out the IP used by Office 365 which can then be specified in squid
> configuration

Well, quite.

A forwarding proxy wouldn't need to be configured with this information.


Antony.

-- 
I wasn't sure about having a beard at first, but then it grew on me.

                                                   Please reply to the list;
                                                         please *don't* CC me.


From squid3 at treenet.co.nz  Thu Sep  7 11:01:15 2017
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 7 Sep 2017 23:01:15 +1200
Subject: [squid-users] cache config
In-Reply-To: <0ecbbcfd-4ada-5f41-6f88-acd73807ecd6@dvm.esines.cu>
References: <0ecbbcfd-4ada-5f41-6f88-acd73807ecd6@dvm.esines.cu>
Message-ID: <2a19c613-5cce-b7e4-90e1-9c1b8da25cfe@treenet.co.nz>

On 07/09/17 01:26, Alex Guti?rrez Mart?nez wrote:
> Hi everyone, i have 100 GB on my cache partition, but squid only use 1.5 
> GB. My internet connection its incredibly slow, any advice on how 
> optimize my connection will be appreciated.
> 

You are missing details of;
* what Squid version you are using (squid -v output), and
* how much traffic is going through the proxy, and
* roughly how many users this cache is servicing, and
* what HIT rates you are currently achieving, and
* what (the 'info' cache manager report - "squidclient mgr:info" or 
cachemgr.cgi page or http://example.local:3128/squid-internal-mgr/info)


Some things to keep in mind (in no particular order):

* A proxy cache is best for large numbers of clients. The fewer users 
exist the more likely the data is being cached on the client machine 
itself (eg in Browser caches) - an aggregating proxy cache will not 
store much of it unless their traffic is significantly different AND the 
proxy cache is larger than the client caches. As user count grows the 
per-user differences build up and proxy shows more caching benefits.

* Some traffic is simply not cacheable. Cacheability is determined by 
the type of domains and sites being contacted, what they do etc. 
Sometimes traffic is simply not cacheable.

* Caches take time to fill up, the fill rate decreases exponentially. 
Each new object added requires that it has not already been used before. 
It is very likely that your users only visited a small number of sites 
and thus only a small amount of content actually is being used. Again 
the more clients use the proxy the more data differences grow the cache.
  Have you given it enough time for more than a few GB of HTTP traffic 
to go through the proxy?

* HTTPS is not cacheable in its encrypted form. As the Internet drive 
towards HTTPS grows increasingly less content is cacheable without 
performing an MITM on the traffic.


* a 64-bit build of Squid is needed to operate well with more than a few 
GB of data. 'Large file' support does not help much as the size of 
individual files is not he problem, size counters for cache management 
need to be 64-bit.
  1.5GB looks suspiciously like the 32-bit numerical wrap happening.


> This is the configuration of my cache.
> 
> maximum_object_size 300 MB
> cache_dir aufs /var/cache/squid3 1024000 16 256

The above cache uses just under 1 TB of disk space, not 100 GB.

Try 97280 for a 100GB disk. That is 97% of the drive of cache and 3% bit 
for OS use and temporary oversize object storage.


> cache_mem 256 MB
> cache_store_log /var/cache/squid3/cache_store.log
> coredump_dir /var/cache/squid3/dump
> minimum_expiry_time 600 seconds

This maybe part of your problem. The larger this value is the more 
likely that dynamic content will *not* be cached.

It is checking whether objects are fresh or stale 600sec in the future 
and only caching the ones that will be fresh at that time. Which is very 
unlikely to be true for any dynamic content.

My recommendation is to remove this from your config file or configure 
it a bit smaller than the default 60sec - but not too much smaller.


> cache_swap_low 87
> cache_swap_high 90

Raise these back to the default 90-95% thresholds for data purging.
You can do that by removing the directives entirely.

NP: the closer these are to 100% the more cache will be able to be 
filled during normal use. But also the more work Squid will do when 
purging to make space for new stuff - which can slow down all 
transactions underway is one of the transactions need a lot of space.
  It is a slow job tuning these properly and requires the cache to be 
relatively full first.

Remove the duplicate ones below anyway.

> ############################
> client_db off
> offline_mode off
> cache_swap_low 87
> cache_swap_high 90
> cache_replacement_policy heap GDSF
> maximum_object_size_in_memory 128 KB
> chunked_request_body_max_size 4096 KB
> half_closed_clients off
> ############################
> # establecemos los archivos de volcado en /var/cache/squid3/
> coredump_dir /var/cache/squid3/
> refresh_pattern ^ftp: 1440 20% 10080
> refresh_pattern ^gopher: 1440 0% 1440
> refresh_pattern -i \.(gif|png|jpg|jpeg|ico)$ 10080 90% 43200 
> override-expire ignore-no-store ignore-private
> refresh_pattern -i \.(iso|avi|wav|mp3|mp4|mpeg|swf|flv|x-flv)$ 43200 90% 
> 432000 override-expire ignore-no-store ignore-private
> refresh_pattern -i 
> \.(deb|rpm|exe|zip|tar|tgz|ram|rar|bin|ppt|doc|tiff|pdf)$ 10080 90% 
> 43200 override-expire ignore-no-store ignore-private
> refresh_pattern -i \.index.(html|htm)$ 0 40% 10080
> refresh_pattern -i \.(html|htm|css|js)$ 1440 40% 40320
> refresh_pattern . 0 40% 40320
> 


From shradhashukla90 at gmail.com  Thu Sep  7 14:34:02 2017
From: shradhashukla90 at gmail.com (SShukla)
Date: Thu, 7 Sep 2017 07:34:02 -0700 (MST)
Subject: [squid-users] How to setup squid as reverse proxy to intercept
 Office365 traffic
In-Reply-To: <201709071221.23007.Antony.Stone@squid.open.source.it>
References: <1504642743764-0.post@n4.nabble.com>
 <201709060008.56111.Antony.Stone@squid.open.source.it>
 <1504719255120-0.post@n4.nabble.com>
 <201709071221.23007.Antony.Stone@squid.open.source.it>
Message-ID: <1504794842430-0.post@n4.nabble.com>

Thanks for replying Antony 

So one requirement for our solution is that, a user in a group using our
setup would have their traffic always pass through our proxy(Squid proxy +
ICAP), whether they are in their office on the company network, or at home
on their own internet, or anywhere else using their mobile data connection. 
This was one of the key reasons we settled on using a reverse proxy.  

In our current deployment, however, we do not have the unique circumstances
that Office 365 presents.  It?s entirely possible that a forward proxy is
the ONLY way to accomplish a similar end result in this environment, but if
a reverse proxy is possible ? , it would be our first choice.





--
Sent from: http://squid-web-proxy-cache.1019090.n4.nabble.com/Squid-Users-f1019091.html


From Antony.Stone at squid.open.source.it  Thu Sep  7 14:38:40 2017
From: Antony.Stone at squid.open.source.it (Antony Stone)
Date: Thu, 7 Sep 2017 16:38:40 +0200
Subject: [squid-users] How to setup squid as reverse proxy to intercept
	Office365 traffic
In-Reply-To: <1504794842430-0.post@n4.nabble.com>
References: <1504642743764-0.post@n4.nabble.com>
 <201709071221.23007.Antony.Stone@squid.open.source.it>
 <1504794842430-0.post@n4.nabble.com>
Message-ID: <201709071638.40716.Antony.Stone@squid.open.source.it>

On Thursday 07 September 2017 at 16:34:02, SShukla wrote:

> Thanks for replying Antony
> 
> So one requirement for our solution is that, a user in a group using our
> setup would have their traffic always pass through our proxy(Squid proxy +
> ICAP), whether they are in their office on the company network, or at home
> on their own internet, or anywhere else using their mobile data connection.
> This was one of the key reasons we settled on using a reverse proxy.

For me, this still does not compute.

How is a user at home or on a mobile data connection going to find your reverse 
proxy?

How is this easier than having their equipment configured to use a forwarding 
proxy?

What is your understanding of the purpose of a reverse proxy?

> In our current deployment, however, we do not have the unique circumstances
> that Office 365 presents.  It?s entirely possible that a forward proxy is
> the ONLY way to accomplish a similar end result in this environment, but if
> a reverse proxy is possible ? , it would be our first choice.

Give me an example - what DNS name would a user connect to, and what would 
that DNS name resolve to, in order to end up on your reverse proxy?


Antony.

-- 
You can tell that the day just isn't going right when you find yourself using 
the telephone before the toilet.

                                                   Please reply to the list;
                                                         please *don't* CC me.


From erdosain9 at gmail.com  Thu Sep  7 14:44:53 2017
From: erdosain9 at gmail.com (erdosain9)
Date: Thu, 7 Sep 2017 07:44:53 -0700 (MST)
Subject: [squid-users] ipcCreate: fork: (12) Cannot allocate memory
Message-ID: <1504795493107-0.post@n4.nabble.com>

Hi to all.
all was working fine.. but today Im having this issue


2017/09/07 11:34:49 kid1| Starting new negotiateauthenticator helpers...
2017/09/07 11:34:49 kid1| helperOpenServers: Starting 1/35
'negotiate_kerberos_auth' processes
2017/09/07 11:34:50 kid1| Starting new negotiateauthenticator helpers...
2017/09/07 11:34:50 kid1| helperOpenServers: Starting 1/35
'negotiate_kerberos_auth' processes
2017/09/07 11:34:50 kid1| ipcCreate: fork: (12) Cannot allocate memory
2017/09/07 11:34:50 kid1| WARNING: Cannot run
'/lib64/squid/negotiate_kerberos_auth' process.
2017/09/07 11:34:50 kid1| Starting new negotiateauthenticator helpers...
2017/09/07 11:34:50 kid1| helperOpenServers: Starting 1/35
'negotiate_kerberos_auth' processes
2017/09/07 11:34:50 kid1| ipcCreate: fork: (12) Cannot allocate memory
2017/09/07 11:34:50 kid1| WARNING: Cannot run
'/lib64/squid/negotiate_kerberos_auth' process.
2017/09/07 11:34:50 kid1| Starting new negotiateauthenticator helpers...
2017/09/07 11:34:50 kid1| helperOpenServers: Starting 1/35
'negotiate_kerberos_auth' processes
2017/09/07 11:34:50 kid1| ipcCreate: fork: (12) Cannot allocate memory
2017/09/07 11:34:50 kid1| WARNING: Cannot run
'/lib64/squid/negotiate_kerberos_auth' process.
2017/09/07 11:34:50 kid1| Starting new negotiateauthenticator helpers...
2017/09/07 11:34:50 kid1| helperOpenServers: Starting 1/35
'negotiate_kerberos_auth' processes
2017/09/07 11:34:50 kid1| ipcCreate: fork: (12) Cannot allocate memory
2017/09/07 11:34:50 kid1| WARNING: Cannot run
'/lib64/squid/negotiate_kerberos_auth' process.
2017/09/07 11:34:50 kid1| Starting new negotiateauthenticator helpers...
2017/09/07 11:34:50 kid1| helperOpenServers: Starting 1/35
'negotiate_kerberos_auth' processes
2017/09/07 11:34:50 kid1| ipcCreate: fork: (12) Cannot allocate memory
2017/09/07 11:34:50 kid1| WARNING: Cannot run
'/lib64/squid/negotiate_kerberos_auth' process.
2017/09/07 11:34:50 kid1| Starting new ssl_crtd helpers...
2017/09/07 11:34:50 kid1| helperOpenServers: Starting 1/32 'ssl_crtd'
processes
2017/09/07 11:34:51 kid1| Starting new ssl_crtd helpers...
2017/09/07 11:34:51 kid1| helperOpenServers: Starting 1/32 'ssl_crtd'
processes
2017/09/07 11:34:51 kid1| ipcCreate: fork: (12) Cannot allocate memory
2017/09/07 11:34:51 kid1| WARNING: Cannot run '/usr/lib64/squid/ssl_crtd'
process.


Can somebody give me a hand??
Thanks to all.



--
Sent from: http://squid-web-proxy-cache.1019090.n4.nabble.com/Squid-Users-f1019091.html


From erdosain9 at gmail.com  Thu Sep  7 14:48:53 2017
From: erdosain9 at gmail.com (erdosain9)
Date: Thu, 7 Sep 2017 07:48:53 -0700 (MST)
Subject: [squid-users] ipcCreate: fork: (12) Cannot allocate memory
In-Reply-To: <1504795493107-0.post@n4.nabble.com>
References: <1504795493107-0.post@n4.nabble.com>
Message-ID: <1504795733032-0.post@n4.nabble.com>

By the way,

              total        used        free      shared  buff/cache  
available
Mem:           3,7G        3,0G        122M         13M        554M       
422M
Swap:          2,0G        160M        1,8G




--
Sent from: http://squid-web-proxy-cache.1019090.n4.nabble.com/Squid-Users-f1019091.html


From squid3 at treenet.co.nz  Thu Sep  7 14:54:44 2017
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Fri, 8 Sep 2017 02:54:44 +1200
Subject: [squid-users] ipcCreate: fork: (12) Cannot allocate memory
In-Reply-To: <1504795493107-0.post@n4.nabble.com>
References: <1504795493107-0.post@n4.nabble.com>
Message-ID: <113811a4-8974-0009-7f72-a21d4f8ae7f4@treenet.co.nz>

On 08/09/17 02:44, erdosain9 wrote:
> Hi to all.
> all was working fine.. but today Im having this issue
> 
> 
> 2017/09/07 11:34:49 kid1| Starting new negotiateauthenticator helpers...
> 2017/09/07 11:34:49 kid1| helperOpenServers: Starting 1/35
> 'negotiate_kerberos_auth' processes
> 2017/09/07 11:34:50 kid1| Starting new negotiateauthenticator helpers...
> 2017/09/07 11:34:50 kid1| helperOpenServers: Starting 1/35
> 'negotiate_kerberos_auth' processes
> 2017/09/07 11:34:50 kid1| ipcCreate: fork: (12) Cannot allocate memory
> 2017/09/07 11:34:50 kid1| WARNING: Cannot run
> '/lib64/squid/negotiate_kerberos_auth' process.


How much RAM does this machine have?

What are your cache_mem and cache_dir settings?

How much RAM is Squid using when these fork's happen?

How much RAM is Squid normally using?

Does the machine have memory 'swap' enabled?

Any per-process limits on RAM consumption?


Amos


From squid3 at treenet.co.nz  Thu Sep  7 15:06:37 2017
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Fri, 8 Sep 2017 03:06:37 +1200
Subject: [squid-users] ipcCreate: fork: (12) Cannot allocate memory
In-Reply-To: <1504795733032-0.post@n4.nabble.com>
References: <1504795493107-0.post@n4.nabble.com>
 <1504795733032-0.post@n4.nabble.com>
Message-ID: <7609a420-c260-cf57-1e57-e26058ff4b90@treenet.co.nz>

On 08/09/17 02:48, erdosain9 wrote:
> By the way,
> 
>                total        used        free      shared  buff/cache
> available
> Mem:           3,7G        3,0G        122M         13M        554M
> 422M
> Swap:          2,0G        160M        1,8G
> 

How much of that 3GB of RAM is Squid using?
  Your swap need to be at least twice that number.

The fork() starting each helper *doubles* the amount of memory the 
kernel counts as being Squid's (once for Squid, once for helper) BUT 
this extra helper memory is virtual and thus almost all placed inside 
the 'swap' area.

So you need a lot of swap space for the kernel to (pretend to) use with 
Squid helpers. The helper itself should use a much smaller amount of 
real RAM so should be no problem there if the fork() can do its thing.

Amos


From sodhia.rohit at gmail.com  Thu Sep  7 19:28:16 2017
From: sodhia.rohit at gmail.com (Rohit Sodhia)
Date: Thu, 7 Sep 2017 15:28:16 -0400
Subject: [squid-users] Looking for assistance with setting up a TLS proxy
Message-ID: <8a732acc-3657-2c8d-0b6f-65a025d683de@gmail.com>

Greetings,

I'm a backend dev who's been suddenly assigned a task to create a squid 
proxy to intercept cURL/wget requests. We've got old servers that don't 
support TLS 1.2 and some of the services we use will be requiring it 
soon, so the decision was made to route cURL and wget requests through a 
pair of squid servers. Unfortunately, I'm not a sysop (or even really 
knowledgeable in this area) and am having some trouble, hoping someone 
wouldn't mind helping me out.

I've been through the squid documentation and been playing around with 
the examples on the squid site, including finding one for creating an 
interception proxy. However, if I'm correct, for squid to be able to 
upgrade the TLS requests from their current 1.0 to 1.2, squid would need 
to decrypt the incoming request, then reencrypt it? I'm hoping someone 
out there may be willing to help point me in the right direction; I've 
been given a tight deadline, and both learning about the technologies 
and find an effective solution is straining.

Thank you,

-- 
Rohit Sodhia



From squid-user at tlinx.org  Thu Sep  7 21:14:40 2017
From: squid-user at tlinx.org (L A Walsh)
Date: Thu, 07 Sep 2017 14:14:40 -0700
Subject: [squid-users] TLS: 1st time w/intermediate cert: not working;
 ideas on what I'm doing wrong?
Message-ID: <59B1B6C0.9090605@tlinx.org>

Got an error message from squid where I'm doing https-bumping:

--------------------------
The following error was encountered while trying to retrieve the URL: 
https://help.ea.com/

    *Failed to establish a secure connection to 52.0.220.87*

The system returned:

    (71) Protocol error (TLS code: X509_V_ERR_UNABLE_TO_GET_ISSUER_CERT_LOCALLY)

    SSL Certficate error: certificate issuer (CA) not known:
    /C=US/O=Symantec Corporation/OU=Symantec Trust Network/CN=Symantec
    Class 3 Secure Server CA - G4

This proxy and the remote host failed to negotiate a mutually acceptable 
security settings for handling your request. It is possible that the 
remote host does not support secure connections, or the proxy is not 
satisfied with the host security credentials.

--------------------------------

Googling found:
http://squid-web-proxy-cache.1019090.n4.nabble.com/Howto-fix-X509-V-ERR-UNABLE-TO-GET-ISSUER-CERT-LOCALLY-Squid-error-td4682015.html

Used openssl.com to get the intermediate certs (2 hosts are referenced
in parallel chains).  The two certs looked like:

-----BEGIN CERTIFICATE-----
...hexstuff==
-----END CERTIFICATE-----


Added the certs to a file and that filename to my squid.conf on a line:

sslproxy_foreign_intermediate_certs /etc/squid/ssl_intermediates/cert.pem

restarted squid, but am still getting same error.

Am I missing some obvious step?

Looking for a clue... ;-)

Thanks!
-l








From yvoinov at gmail.com  Thu Sep  7 21:19:51 2017
From: yvoinov at gmail.com (Yuri)
Date: Fri, 8 Sep 2017 03:19:51 +0600
Subject: [squid-users] TLS: 1st time w/intermediate cert: not working;
 ideas on what I'm doing wrong?
In-Reply-To: <59B1B6C0.9090605@tlinx.org>
References: <59B1B6C0.9090605@tlinx.org>
Message-ID: <758f4bba-ee2e-852d-f45b-f787659b77b2@gmail.com>



08.09.2017 3:14, L A Walsh ?????:
> Got an error message from squid where I'm doing https-bumping:
>
> --------------------------
> The following error was encountered while trying to retrieve the URL:
> https://help.ea.com/
>
> ?? *Failed to establish a secure connection to 52.0.220.87*
>
> The system returned:
>
> ?? (71) Protocol error (TLS code:
> X509_V_ERR_UNABLE_TO_GET_ISSUER_CERT_LOCALLY)
>
> ?? SSL Certficate error: certificate issuer (CA) not known:
> ?? /C=US/O=Symantec Corporation/OU=Symantec Trust Network/CN=Symantec
> ?? Class 3 Secure Server CA - G4
>
> This proxy and the remote host failed to negotiate a mutually
> acceptable security settings for handling your request. It is possible
> that the remote host does not support secure connections, or the proxy
> is not satisfied with the host security credentials.
>
> --------------------------------
>
> Googling found:
> http://squid-web-proxy-cache.1019090.n4.nabble.com/Howto-fix-X509-V-ERR-UNABLE-TO-GET-ISSUER-CERT-LOCALLY-Squid-error-td4682015.html
>
>
> Used openssl.com to get the intermediate certs (2 hosts are referenced
> in parallel chains).? The two certs looked like:
>
> -----BEGIN CERTIFICATE-----
> ...hexstuff==
> -----END CERTIFICATE-----
>
>
> Added the certs to a file and that filename to my squid.conf on a line:
>
> sslproxy_foreign_intermediate_certs /etc/squid/ssl_intermediates/cert.pem
>
> restarted squid, but am still getting same error.
>
> Am I missing some obvious step?
Yup :)

#? TAG: sslproxy_foreign_intermediate_certs
#??? Many origin servers fail to send their full server certificate
#??? chain for verification, assuming the client already has or can
#??? easily locate any missing intermediate certificates.
#
#??? Squid uses the certificates from the specified file to fill in
#??? these missing chains when trying to validate origin server
#??? certificate chains.
#
#??? The file is expected to contain zero or more PEM-encoded
#??? intermediate certificates. These certificates are not treated
#??? as trusted root certificates, and any self-signed certificate in
#??? this file will be ignored.
#Default:
# none

>
> Looking for a clue... ;-)
https://wiki.squid-cache.org/ConfigExamples/Intercept/SslBumpExplicit?highlight=%28Ssl%29%7C%28Bump%29%7C%28explicit%29#Missing_intermediate_certificates
>
> Thanks!
> -l
>
>
>
>
>
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users


-------------- next part --------------
A non-text attachment was scrubbed...
Name: signature.asc
Type: application/pgp-signature
Size: 473 bytes
Desc: OpenPGP digital signature
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20170908/10ccf9f7/attachment.sig>

From yvoinov at gmail.com  Thu Sep  7 21:24:25 2017
From: yvoinov at gmail.com (Yuri)
Date: Fri, 8 Sep 2017 03:24:25 +0600
Subject: [squid-users] TLS: 1st time w/intermediate cert: not working;
 ideas on what I'm doing wrong?
In-Reply-To: <59B1B6C0.9090605@tlinx.org>
References: <59B1B6C0.9090605@tlinx.org>
Message-ID: <a92e2a88-e5d8-ea23-5e69-cf7c08ab7cbd@gmail.com>

Ooooops,

miss end of message :)

Check all CA's chain. It is possible your root CA's bundle not complete.

I usually use root CA's from Mozilla (added to squid.conf as one file)
and own self-supported intermediate CA's list (file).

But in addition I'm using Squid 5.x with working cert's downloader ;)


08.09.2017 3:14, L A Walsh ?????:
> Got an error message from squid where I'm doing https-bumping:
>
> --------------------------
> The following error was encountered while trying to retrieve the URL:
> https://help.ea.com/
>
> ?? *Failed to establish a secure connection to 52.0.220.87*
>
> The system returned:
>
> ?? (71) Protocol error (TLS code:
> X509_V_ERR_UNABLE_TO_GET_ISSUER_CERT_LOCALLY)
>
> ?? SSL Certficate error: certificate issuer (CA) not known:
> ?? /C=US/O=Symantec Corporation/OU=Symantec Trust Network/CN=Symantec
> ?? Class 3 Secure Server CA - G4
>
> This proxy and the remote host failed to negotiate a mutually
> acceptable security settings for handling your request. It is possible
> that the remote host does not support secure connections, or the proxy
> is not satisfied with the host security credentials.
>
> --------------------------------
>
> Googling found:
> http://squid-web-proxy-cache.1019090.n4.nabble.com/Howto-fix-X509-V-ERR-UNABLE-TO-GET-ISSUER-CERT-LOCALLY-Squid-error-td4682015.html
>
>
> Used openssl.com to get the intermediate certs (2 hosts are referenced
> in parallel chains).? The two certs looked like:
>
> -----BEGIN CERTIFICATE-----
> ...hexstuff==
> -----END CERTIFICATE-----
>
>
> Added the certs to a file and that filename to my squid.conf on a line:
>
> sslproxy_foreign_intermediate_certs /etc/squid/ssl_intermediates/cert.pem
>
> restarted squid, but am still getting same error.
>
> Am I missing some obvious step?
>
> Looking for a clue... ;-)
>
> Thanks!
> -l
>
>
>
>
>
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users


-------------- next part --------------
A non-text attachment was scrubbed...
Name: signature.asc
Type: application/pgp-signature
Size: 473 bytes
Desc: OpenPGP digital signature
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20170908/5c840f6d/attachment.sig>

From yvoinov at gmail.com  Thu Sep  7 21:26:27 2017
From: yvoinov at gmail.com (Yuri)
Date: Fri, 8 Sep 2017 03:26:27 +0600
Subject: [squid-users] TLS: 1st time w/intermediate cert: not working;
 ideas on what I'm doing wrong?
In-Reply-To: <a92e2a88-e5d8-ea23-5e69-cf7c08ab7cbd@gmail.com>
References: <59B1B6C0.9090605@tlinx.org>
 <a92e2a88-e5d8-ea23-5e69-cf7c08ab7cbd@gmail.com>
Message-ID: <1222937e-bb7f-a67d-d19f-c92bb2e99b74@gmail.com>

Also. Symantec's root's can be already removed from most bundles (you
should hear about it, is it?).

So. May be can be required to add Symantec's root(s) manually to proxy
root CA bundle.


08.09.2017 3:24, Yuri ?????:
> Ooooops,
>
> miss end of message :)
>
> Check all CA's chain. It is possible your root CA's bundle not complete.
>
> I usually use root CA's from Mozilla (added to squid.conf as one file)
> and own self-supported intermediate CA's list (file).
>
> But in addition I'm using Squid 5.x with working cert's downloader ;)
>
>
> 08.09.2017 3:14, L A Walsh ?????:
>> Got an error message from squid where I'm doing https-bumping:
>>
>> --------------------------
>> The following error was encountered while trying to retrieve the URL:
>> https://help.ea.com/
>>
>> ?? *Failed to establish a secure connection to 52.0.220.87*
>>
>> The system returned:
>>
>> ?? (71) Protocol error (TLS code:
>> X509_V_ERR_UNABLE_TO_GET_ISSUER_CERT_LOCALLY)
>>
>> ?? SSL Certficate error: certificate issuer (CA) not known:
>> ?? /C=US/O=Symantec Corporation/OU=Symantec Trust Network/CN=Symantec
>> ?? Class 3 Secure Server CA - G4
>>
>> This proxy and the remote host failed to negotiate a mutually
>> acceptable security settings for handling your request. It is possible
>> that the remote host does not support secure connections, or the proxy
>> is not satisfied with the host security credentials.
>>
>> --------------------------------
>>
>> Googling found:
>> http://squid-web-proxy-cache.1019090.n4.nabble.com/Howto-fix-X509-V-ERR-UNABLE-TO-GET-ISSUER-CERT-LOCALLY-Squid-error-td4682015.html
>>
>>
>> Used openssl.com to get the intermediate certs (2 hosts are referenced
>> in parallel chains).? The two certs looked like:
>>
>> -----BEGIN CERTIFICATE-----
>> ...hexstuff==
>> -----END CERTIFICATE-----
>>
>>
>> Added the certs to a file and that filename to my squid.conf on a line:
>>
>> sslproxy_foreign_intermediate_certs /etc/squid/ssl_intermediates/cert.pem
>>
>> restarted squid, but am still getting same error.
>>
>> Am I missing some obvious step?
>>
>> Looking for a clue... ;-)
>>
>> Thanks!
>> -l
>>
>>
>>
>>
>>
>>
>> _______________________________________________
>> squid-users mailing list
>> squid-users at lists.squid-cache.org
>> http://lists.squid-cache.org/listinfo/squid-users
>


-------------- next part --------------
A non-text attachment was scrubbed...
Name: signature.asc
Type: application/pgp-signature
Size: 473 bytes
Desc: OpenPGP digital signature
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20170908/3e08c025/attachment.sig>

From rafael.akchurin at diladele.com  Thu Sep  7 21:42:47 2017
From: rafael.akchurin at diladele.com (Rafael Akchurin)
Date: Thu, 7 Sep 2017 21:42:47 +0000
Subject: [squid-users] TLS: 1st time w/intermediate cert: not working;
 ideas on what I'm doing wrong?
In-Reply-To: <59B1B6C0.9090605@tlinx.org>
References: <59B1B6C0.9090605@tlinx.org>
Message-ID: <DB6PR0401MB26803D83F5B9EFB1FD19B9E88F940@DB6PR0401MB2680.eurprd04.prod.outlook.com>

Hello LA, Yuri,

The server analysis at https://www.ssllabs.com/ssltest/analyze.html?d=help.ea.com&s=52.0.220.87&latest shows the certificate chain presented by the remote server is indeed incomplete, specifically the following certificate is not presented:

---
Symantec Class 3 Secure Server CA - G4
Fingerprint SHA256: eae72eb454bf6c3977ebd289e970b2f5282949190093d0d26f98d0f0d6a9cf17
Pin SHA256: 9n0izTnSRF+W4W4JTq51avSXkWhQB8duS2bxVLfzXsY=
RSA 2048 bits (e 65537) / SHA256withRSA
---

Adding it to the intermediate certificate file as indicated on https://docs.diladele.com/faq/squid/fix_unable_to_get_issuer_cert_locally.html#way-1-add-missing-certificate-to-squid-web-safety-5-1-recommended and reloading Squid 3.5.23 allows to successfully see and bump the site.

Our UI generates exactly the same config setting as you have tried:
sslproxy_foreign_intermediate_certs /opt/websafety/etc/squid/foreign_intermediate_certs.pem

So it must be working :)

Best regards,
Rafael Akchurin
Diladele B.V.



-----Original Message-----
From: squid-users [mailto:squid-users-bounces at lists.squid-cache.org] On Behalf Of L A Walsh
Sent: Thursday, September 7, 2017 11:15 PM
To: squid-users at squid-cache.org
Subject: [squid-users] TLS: 1st time w/intermediate cert: not working; ideas on what I'm doing wrong?

Got an error message from squid where I'm doing https-bumping:

--------------------------
The following error was encountered while trying to retrieve the URL: 
https://help.ea.com/

    *Failed to establish a secure connection to 52.0.220.87*

The system returned:

    (71) Protocol error (TLS code: X509_V_ERR_UNABLE_TO_GET_ISSUER_CERT_LOCALLY)

    SSL Certficate error: certificate issuer (CA) not known:
    /C=US/O=Symantec Corporation/OU=Symantec Trust Network/CN=Symantec
    Class 3 Secure Server CA - G4

This proxy and the remote host failed to negotiate a mutually acceptable security settings for handling your request. It is possible that the remote host does not support secure connections, or the proxy is not satisfied with the host security credentials.

--------------------------------

Googling found:
http://squid-web-proxy-cache.1019090.n4.nabble.com/Howto-fix-X509-V-ERR-UNABLE-TO-GET-ISSUER-CERT-LOCALLY-Squid-error-td4682015.html

Used openssl.com to get the intermediate certs (2 hosts are referenced in parallel chains).  The two certs looked like:

-----BEGIN CERTIFICATE-----
...hexstuff==
-----END CERTIFICATE-----


Added the certs to a file and that filename to my squid.conf on a line:

sslproxy_foreign_intermediate_certs /etc/squid/ssl_intermediates/cert.pem

restarted squid, but am still getting same error.

Am I missing some obvious step?

Looking for a clue... ;-)

Thanks!
-l






_______________________________________________
squid-users mailing list
squid-users at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-users

From yvoinov at gmail.com  Thu Sep  7 21:44:40 2017
From: yvoinov at gmail.com (Yuri)
Date: Fri, 8 Sep 2017 03:44:40 +0600
Subject: [squid-users] TLS: 1st time w/intermediate cert: not working;
 ideas on what I'm doing wrong?
In-Reply-To: <DB6PR0401MB26803D83F5B9EFB1FD19B9E88F940@DB6PR0401MB2680.eurprd04.prod.outlook.com>
References: <59B1B6C0.9090605@tlinx.org>
 <DB6PR0401MB26803D83F5B9EFB1FD19B9E88F940@DB6PR0401MB2680.eurprd04.prod.outlook.com>
Message-ID: <11116601-1553-37c9-2632-ac4cdbec1201@gmail.com>

Hi, Raf. Just checking on two my servers - works like charm without any
movings :) I'm already have good intermediate CA's bundle :)


08.09.2017 3:42, Rafael Akchurin ?????:
> Hello LA, Yuri,
>
> The server analysis at https://www.ssllabs.com/ssltest/analyze.html?d=help.ea.com&s=52.0.220.87&latest shows the certificate chain presented by the remote server is indeed incomplete, specifically the following certificate is not presented:
>
> ---
> Symantec Class 3 Secure Server CA - G4
> Fingerprint SHA256: eae72eb454bf6c3977ebd289e970b2f5282949190093d0d26f98d0f0d6a9cf17
> Pin SHA256: 9n0izTnSRF+W4W4JTq51avSXkWhQB8duS2bxVLfzXsY=
> RSA 2048 bits (e 65537) / SHA256withRSA
> ---
>
> Adding it to the intermediate certificate file as indicated on https://docs.diladele.com/faq/squid/fix_unable_to_get_issuer_cert_locally.html#way-1-add-missing-certificate-to-squid-web-safety-5-1-recommended and reloading Squid 3.5.23 allows to successfully see and bump the site.
>
> Our UI generates exactly the same config setting as you have tried:
> sslproxy_foreign_intermediate_certs /opt/websafety/etc/squid/foreign_intermediate_certs.pem
>
> So it must be working :)
>
> Best regards,
> Rafael Akchurin
> Diladele B.V.
>
>
>
> -----Original Message-----
> From: squid-users [mailto:squid-users-bounces at lists.squid-cache.org] On Behalf Of L A Walsh
> Sent: Thursday, September 7, 2017 11:15 PM
> To: squid-users at squid-cache.org
> Subject: [squid-users] TLS: 1st time w/intermediate cert: not working; ideas on what I'm doing wrong?
>
> Got an error message from squid where I'm doing https-bumping:
>
> --------------------------
> The following error was encountered while trying to retrieve the URL: 
> https://help.ea.com/
>
>     *Failed to establish a secure connection to 52.0.220.87*
>
> The system returned:
>
>     (71) Protocol error (TLS code: X509_V_ERR_UNABLE_TO_GET_ISSUER_CERT_LOCALLY)
>
>     SSL Certficate error: certificate issuer (CA) not known:
>     /C=US/O=Symantec Corporation/OU=Symantec Trust Network/CN=Symantec
>     Class 3 Secure Server CA - G4
>
> This proxy and the remote host failed to negotiate a mutually acceptable security settings for handling your request. It is possible that the remote host does not support secure connections, or the proxy is not satisfied with the host security credentials.
>
> --------------------------------
>
> Googling found:
> http://squid-web-proxy-cache.1019090.n4.nabble.com/Howto-fix-X509-V-ERR-UNABLE-TO-GET-ISSUER-CERT-LOCALLY-Squid-error-td4682015.html
>
> Used openssl.com to get the intermediate certs (2 hosts are referenced in parallel chains).  The two certs looked like:
>
> -----BEGIN CERTIFICATE-----
> ...hexstuff==
> -----END CERTIFICATE-----
>
>
> Added the certs to a file and that filename to my squid.conf on a line:
>
> sslproxy_foreign_intermediate_certs /etc/squid/ssl_intermediates/cert.pem
>
> restarted squid, but am still getting same error.
>
> Am I missing some obvious step?
>
> Looking for a clue... ;-)
>
> Thanks!
> -l
>
>
>
>
>
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users


-------------- next part --------------
A non-text attachment was scrubbed...
Name: signature.asc
Type: application/pgp-signature
Size: 473 bytes
Desc: OpenPGP digital signature
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20170908/f2590d02/attachment.sig>

From squid-user at tlinx.org  Thu Sep  7 21:46:22 2017
From: squid-user at tlinx.org (L A Walsh)
Date: Thu, 07 Sep 2017 14:46:22 -0700
Subject: [squid-users] TLS: 1st time w/intermediate cert: not working;
 ideas on what I'm doing wrong?
In-Reply-To: <a92e2a88-e5d8-ea23-5e69-cf7c08ab7cbd@gmail.com>
References: <59B1B6C0.9090605@tlinx.org>
 <a92e2a88-e5d8-ea23-5e69-cf7c08ab7cbd@gmail.com>
Message-ID: <59B1BE2E.8000106@tlinx.org>

Yuri wrote:
> Ooooops,
>
> miss end of message :)
>   
---
    I did search first! ;^)



> Check all CA's chain. It is possible your root CA's bundle not complete.
>   
---
    Likely problem...


> I usually use root CA's from Mozilla (added to squid.conf as one file)
> and own self-supported intermediate CA's list (file).
>   
----
How often do they update?  I.e. should I set up a cron job to download
and concatenate the CA's?  Is there a preferred D/L URL?





> But in addition I'm using Squid 5.x with working cert's downloader ;)
>   
----
:^/  --- hmmm.... and I'm not even running 4.x... *ouch*...

Is that going to be backported to 3.x?  Isn't 4.x the beta/devel version,
or is it 4.x=beta and 5.x=devel?


Tnx!
-l





From squid3 at treenet.co.nz  Thu Sep  7 21:47:42 2017
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Fri, 8 Sep 2017 09:47:42 +1200
Subject: [squid-users] Looking for assistance with setting up a TLS proxy
In-Reply-To: <8a732acc-3657-2c8d-0b6f-65a025d683de@gmail.com>
References: <8a732acc-3657-2c8d-0b6f-65a025d683de@gmail.com>
Message-ID: <53f77aba-d083-546e-c3a1-7543a7dd018f@treenet.co.nz>

On 08/09/17 07:28, Rohit Sodhia wrote:
> Greetings,
> 
> I'm a backend dev who's been suddenly assigned a task to create a squid 
> proxy to intercept cURL/wget requests. We've got old servers that don't 
> support TLS 1.2 and some of the services we use will be requiring it 
> soon, so the decision was made to route cURL and wget requests through a 
> pair of squid servers. Unfortunately, I'm not a sysop (or even really 
> knowledgeable in this area) and am having some trouble, hoping someone 
> wouldn't mind helping me out.
> 
> I've been through the squid documentation and been playing around with 
> the examples on the squid site, including finding one for creating an 
> interception proxy. However, if I'm correct, for squid to be able to 
> upgrade the TLS requests from their current 1.0 to 1.2, squid would need 
> to decrypt the incoming request, then reencrypt it?

Yes. The TLS messaging needs replacing to negotiate TLS/1.0 variants of 
things, and often the server cert itself needs replacing entirely due to 
TLS/1.1+ extension bits inside it.


> I'm hoping someone 
> out there may be willing to help point me in the right direction; I've 
> been given a tight deadline, and both learning about the technologies 
> and find an effective solution is straining.
> 
> Thank you,
> 

The feature details for Squid TLS interception capabilities is 
<https://wiki.squid-cache.org/Features/SslPeekAndSplice>

The more you know about TLS messaging the easier it is to grasp what 
Squid is doing. But the basics as covered on that pages' second section 
should be sufficient to use the feature.

Some things that might trip you up:

* the "stare" and "splice" actions we normally advise using cannot be 
used when translating TLS versions. They deliver the client TLS version 
(at least) on messages to the server.

* bump at step1 (maybe step2) will do exactly what you need. This 
emulates the client-first bumping action which is documented as "causes 
a lot of problems" mostly in terms of adding major TLS vulnerabilities 
to the whole system - so minimize use as much as possible.

* TLS SNI and similar extensions are generally not sent by TLS/1.0 
clients. Which makes it difficult to tell what service is being 
requested, and thus to do that above minimization.


HTH
Amos


From yvoinov at gmail.com  Thu Sep  7 21:49:37 2017
From: yvoinov at gmail.com (Yuri)
Date: Fri, 8 Sep 2017 03:49:37 +0600
Subject: [squid-users] TLS: 1st time w/intermediate cert: not working;
 ideas on what I'm doing wrong?
In-Reply-To: <59B1BE2E.8000106@tlinx.org>
References: <59B1B6C0.9090605@tlinx.org>
 <a92e2a88-e5d8-ea23-5e69-cf7c08ab7cbd@gmail.com> <59B1BE2E.8000106@tlinx.org>
Message-ID: <0cd82881-137e-8508-3d20-e51f91ab89c6@gmail.com>



08.09.2017 3:46, L A Walsh ?????:
> Yuri wrote:
>> Ooooops,
>>
>> miss end of message :)
>> ? 
> ---
> ?? I did search first! ;^)
>
>
>
>> Check all CA's chain. It is possible your root CA's bundle not complete.
>> ? 
> ---
> ?? Likely problem...
>
>
>> I usually use root CA's from Mozilla (added to squid.conf as one file)
>> and own self-supported intermediate CA's list (file).
>> ? 
> ----
> How often do they update?? I.e. should I set up a cron job to download
> and concatenate the CA's?? Is there a preferred D/L URL?
I added to cron once per month update. Script (specific to my setups) to
update and reconfigure squid.
I use this URL:
https://raw.githubusercontent.com/bagder/ca-bundle/master/ca-bundle.crt

>
>
>
>
>
>> But in addition I'm using Squid 5.x with working cert's downloader ;)
>> ? 
> ----
> :^/? --- hmmm.... and I'm not even running 4.x... *ouch*...
>
> Is that going to be backported to 3.x?? Isn't 4.x the beta/devel version,
> or is it 4.x=beta and 5.x=devel?
>
>
> Tnx!
> -l
>
>
>


-------------- next part --------------
A non-text attachment was scrubbed...
Name: signature.asc
Type: application/pgp-signature
Size: 473 bytes
Desc: OpenPGP digital signature
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20170908/fdb571b2/attachment.sig>

From yvoinov at gmail.com  Thu Sep  7 21:52:40 2017
From: yvoinov at gmail.com (Yuri)
Date: Fri, 8 Sep 2017 03:52:40 +0600
Subject: [squid-users] TLS: 1st time w/intermediate cert: not working;
 ideas on what I'm doing wrong?
In-Reply-To: <0cd82881-137e-8508-3d20-e51f91ab89c6@gmail.com>
References: <59B1B6C0.9090605@tlinx.org>
 <a92e2a88-e5d8-ea23-5e69-cf7c08ab7cbd@gmail.com> <59B1BE2E.8000106@tlinx.org>
 <0cd82881-137e-8508-3d20-e51f91ab89c6@gmail.com>
Message-ID: <f0bc98d5-66a9-86df-bd60-e3b653a11bed@gmail.com>



08.09.2017 3:49, Yuri ?????:
>
> 08.09.2017 3:46, L A Walsh ?????:
>> Yuri wrote:
>>> Ooooops,
>>>
>>> miss end of message :)
>>> ? 
>> ---
>> ?? I did search first! ;^)
>>
>>
>>
>>> Check all CA's chain. It is possible your root CA's bundle not complete.
>>> ? 
>> ---
>> ?? Likely problem...
>>
>>
>>> I usually use root CA's from Mozilla (added to squid.conf as one file)
>>> and own self-supported intermediate CA's list (file).
>>> ? 
>> ----
>> How often do they update?? I.e. should I set up a cron job to download
>> and concatenate the CA's?? Is there a preferred D/L URL?
> I added to cron once per month update. Script (specific to my setups) to
> update and reconfigure squid.
> I use this URL:
> https://raw.githubusercontent.com/bagder/ca-bundle/master/ca-bundle.crt
>
>>
>>
>>
>>
>>> But in addition I'm using Squid 5.x with working cert's downloader ;)
>>> ? 
>> ----
>> :^/? --- hmmm.... and I'm not even running 4.x... *ouch*...
3.5.26 (last known) works with relatively complete intermediates and
with some manually added root CA's.
>>
>> Is that going to be backported to 3.x?? Isn't 4.x the beta/devel version,
>> or is it 4.x=beta and 5.x=devel?
AFAIK it's not planning to backport it to 3.x, can't say about current
4.x. A bit long time migrated to development 5.x. Due to required features.
>>
>>
>> Tnx!
>> -l
>>
>>
>>
>


-------------- next part --------------
A non-text attachment was scrubbed...
Name: signature.asc
Type: application/pgp-signature
Size: 473 bytes
Desc: OpenPGP digital signature
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20170908/95f4e1bb/attachment.sig>

From squid3 at treenet.co.nz  Thu Sep  7 22:05:34 2017
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Fri, 8 Sep 2017 10:05:34 +1200
Subject: [squid-users] TLS: 1st time w/intermediate cert: not working;
 ideas on what I'm doing wrong?
In-Reply-To: <f0bc98d5-66a9-86df-bd60-e3b653a11bed@gmail.com>
References: <59B1B6C0.9090605@tlinx.org>
 <a92e2a88-e5d8-ea23-5e69-cf7c08ab7cbd@gmail.com> <59B1BE2E.8000106@tlinx.org>
 <0cd82881-137e-8508-3d20-e51f91ab89c6@gmail.com>
 <f0bc98d5-66a9-86df-bd60-e3b653a11bed@gmail.com>
Message-ID: <fd4d6a4f-73c9-8c54-66a8-6e50bc062d0e@treenet.co.nz>

On 08/09/17 09:52, Yuri wrote:
> 
> 
> 08.09.2017 3:49, Yuri ?????:
>>
>> 08.09.2017 3:46, L A Walsh ?????:
>>> Yuri wrote:
>>>
>>>
>>>> But in addition I'm using Squid 5.x with working cert's downloader ;)
>>>>    
>>> ----
>>> :^/? --- hmmm.... and I'm not even running 4.x... *ouch*...
> 3.5.26 (last known) works with relatively complete intermediates and
> with some manually added root CA's.
>>>
>>> Is that going to be backported to 3.x?? Isn't 4.x the beta/devel version,
>>> or is it 4.x=beta and 5.x=devel?
> AFAIK it's not planning to backport it to 3.x, can't say about current
> 4.x. A bit long time migrated to development 5.x. Due to required features.

Should be working in v4 (beta) now.

And yes, no plans for backport to v3.5 - it is big code change.


Amos


From squid-user at tlinx.org  Thu Sep  7 23:25:57 2017
From: squid-user at tlinx.org (L A Walsh)
Date: Thu, 07 Sep 2017 16:25:57 -0700
Subject: [squid-users] TLS: 1st time w/intermediate cert: not working;
 ideas on what I'm doing wrong?
In-Reply-To: <0cd82881-137e-8508-3d20-e51f91ab89c6@gmail.com>
References: <59B1B6C0.9090605@tlinx.org>
 <a92e2a88-e5d8-ea23-5e69-cf7c08ab7cbd@gmail.com> <59B1BE2E.8000106@tlinx.org>
 <0cd82881-137e-8508-3d20-e51f91ab89c6@gmail.com>
Message-ID: <59B1D585.3060605@tlinx.org>

Yuri wrote:

>>> Check all CA's chain. It is possible your root CA's bundle not complete.
>>>   
>> ---
>>    Likely problem...


Fixed as per URL:


> I use this URL:
> https://raw.githubusercontent.com/bagder/ca-bundle/master/ca-bundle.crt

and working now...

Thanks!
Linda


From yvoinov at gmail.com  Thu Sep  7 23:37:45 2017
From: yvoinov at gmail.com (Yuri)
Date: Fri, 8 Sep 2017 05:37:45 +0600
Subject: [squid-users] TLS: 1st time w/intermediate cert: not working;
 ideas on what I'm doing wrong?
In-Reply-To: <59B1D585.3060605@tlinx.org>
References: <59B1B6C0.9090605@tlinx.org>
 <a92e2a88-e5d8-ea23-5e69-cf7c08ab7cbd@gmail.com> <59B1BE2E.8000106@tlinx.org>
 <0cd82881-137e-8508-3d20-e51f91ab89c6@gmail.com> <59B1D585.3060605@tlinx.org>
Message-ID: <016f1e8d-9f59-a070-7768-3235e1fdbc1c@gmail.com>

You r welcome ;)


08.09.2017 5:25, L A Walsh ?????:
> Yuri wrote:
>
>>>> Check all CA's chain. It is possible your root CA's bundle not
>>>> complete.
>>>> ? 
>>> ---
>>> ?? Likely problem...
>
>
> Fixed as per URL:
>
>
>> I use this URL:
>> https://raw.githubusercontent.com/bagder/ca-bundle/master/ca-bundle.crt
>
> and working now...
>
> Thanks!
> Linda


-------------- next part --------------
A non-text attachment was scrubbed...
Name: signature.asc
Type: application/pgp-signature
Size: 473 bytes
Desc: OpenPGP digital signature
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20170908/81f8640a/attachment.sig>

From steve at opendium.com  Fri Sep  8 16:37:26 2017
From: steve at opendium.com (Steve Hill)
Date: Fri, 8 Sep 2017 17:37:26 +0100
Subject: [squid-users] High memory usage associated with ssl_bump and broken
	clients
Message-ID: <8727f26a-165a-80e8-ac9b-71f013ced2ce@opendium.com>


I've identified a problem with Squid 3.5.26 using a lot of memory when 
some broken clients are on the network.  Strictly speaking this isn't 
really Squid's fault, but it is a denial of service mechanism so I 
wonder if Squid can help mitigate it.

The situation is this:

Squid is set up as a transparent proxy performing SSL bumping.
A client makes an HTTPS connection, which Squid intercepts.  The client 
sends a TLS client handshake and squid responds with a handshake and the 
bumped certificate.  The client doesn't like the bumped certificate, but 
rather than cleanly aborting the TLS session and then sending a TCP FIN, 
it just tears down the connection with a TCP RST packet.

Ordinarily, Squid's side of the connection would be torn down in 
response to the RST, so there would be no problem.  But unfortunately, 
under high network loads the RST packet sometimes gets dropped and as 
far as Squid is concerned the connection never gets closed.

The busted clients I'm seeing the most problems with retry the 
connection immediately rather than waiting for a retry timer.


Problems:
1. A connection that hasn't completed the TLS handshake doesn't appear 
to ever time out (in this case, the server handshake and certificate 
exchange has been completed, but the key exchange never starts).

2. If the client sends an RST and the RST is lost, the client won't send 
another RST until Squid sends some data to it on the aborted connection. 
  In this case, Squid is waiting for data from the client, which will 
never come, and will not send any new data to the client.  Squid will 
never know that the client aborted the connection.

3. There is a lot of memory associated with each connection - my tests 
suggest around 1MB.  In normal operation these kinds of dead connections 
can gradually stack up, leading to a slow but significant memory "leak"; 
when a really badly behaved client is on the network it can open tens of 
thousands of connections per minute and the memory consumption brings 
down the server.

4. We can expect similar problems with devices on flakey network 
connections, even when the clients are well behaved.


My thoughts:
Connections should have a reasonably short timeout during the TLS 
handshake - if a client hasn't completed the handshake and made an HTTP 
request over the encrypted connection within a few seconds, something is 
broken and Squid should tear down the connection.  These connections 
certainly shouldn't be able to persist forever with neither side sending 
any data.


Testing:
I wrote a Python script that makes 1000 concurrent connections as 
quickly as it can and send a TLS client handshake over them.  Once all 
of the connections are open, it then waits for responses from Squid 
(which would contain the server handshake and certificate) and quits, 
tearing down all of the the connections with an RST.

It seems that the RST packets for around 300 of those connections were 
dropped - this sounds surprising, but since all 1000 connections were 
aborted simultaneously, there would be a flood of RST packets and its 
probably reasonable to expect a significant number to be dropped.  The 
end result was that netstat showed Squid still had about 300 established 
connections, which would never go away.

-- 
  - Steve Hill
    Technical Director
    Opendium    Online Safety / Web Filtering    http://www.opendium.com

    Enquiries                 Support
    ---------                 -------
    sales at opendium.com        support at opendium.com
    +44-1792-824568           +44-1792-825748


From rentorbuy at yahoo.com  Fri Sep  8 17:39:00 2017
From: rentorbuy at yahoo.com (Vieri)
Date: Fri, 8 Sep 2017 17:39:00 +0000 (UTC)
Subject: [squid-users] squid cache takes a break
References: <1234530834.188010.1504892340901.ref@mail.yahoo.com>
Message-ID: <1234530834.188010.1504892340901@mail.yahoo.com>

Hi,

Sorry for the title, but I really don't know how to describe what just happened today. It's really odd.

I previously posted a few similar issues which were all fixed if I increased certain parameters (ulimits, children-{max,startup,idle}, TTL, etc.).

This time however, after several days trouble-free I got another show-stopper. The local squid cache stopped serving for almost half an hour. After that, it all started working again magically. I had the chance to log into the server with ssh and try a few things:

- In the cache log I could see these messages:
Starting new bllookup helpers...
helperOpenServers: Starting 10/80 'squid_url_lookup.pl' processes
WARNING: Cannot run '/opt/custom/scripts/run/scripts/firewall/squid_url_lookup.pl' process.
WARNING: Cannot run '/opt/custom/scripts/run/scripts/firewall/squid_url_lookup.pl' process.
WARNING: Cannot run '/opt/custom/scripts/run/scripts/firewall/squid_url_lookup.pl' process.

It doesn't say much as to why it "cannot run" the external program.

This is how the program is defined in squid.conf:
external_acl_type bllookup ttl=86400 negative_ttl=86400 children-max=80 children-startup=40 children-idle=10 %URI /opt/custom/scripts/run/scripts/firewall/squid_url_lookup.pl [...]

Other than that, the log is pretty quiet.

The HTTP clients do not get served at all. They keep waiting for a reply.

# ps aux | grep squid
root      3043  0.0  0.0  84856  1728 ?        Ss   Aug31   0:00 /usr/sbin/squid -YC -f /etc/squid/squid.http.conf -n squidhttp
squid     3046  0.0  0.0 128232 31052 ?        S    Aug31   0:35 (squid-1) -YC -f /etc/squid/squid.http.conf -n squidhttp
root      3538  0.0  0.0  86912  1740 ?        Ss   Aug31   0:00 /usr/sbin/squid -YC -f /etc/squid/squid.https.conf -n squidhttps
squid     3540  0.0  0.1 134616 35608 ?        S    Aug31   1:09 (squid-1) -YC -f /etc/squid/squid.https.conf -n squidhttps
root      5690  0.0  0.0  87444  1736 ?        Ss   Aug31   0:00 /usr/sbin/squid -YC -f /etc/squid/squid.conf -n squid
squid     5694  2.4  6.5 3769624 2136968 ?     S    Aug31 293:24 (squid-1) -YC -f /etc/squid/squid.conf -n squid
squid     5727  0.0  0.0   4008   524 ?        S    Aug31   0:01 (unlinkd)
squid     5728  0.0  0.0  13904  1576 ?        S    Aug31   2:09 diskd 5830660 5830661 5830662
squid    11927  0.0  0.0   4156   644 ?        S    Sep07   0:36 (logfile-daemon) /var/log/squid/access.log
squid    11937  1.7  0.0  41792  6232 ?        S    Sep07  31:08 (ssl_crtd) -s /var/lib/squid/ssl_db -M 16MB
squid    11939  0.1  0.0  41776  6288 ?        S    Sep07   3:09 (ssl_crtd) -s /var/lib/squid/ssl_db -M 16MB
squid    11940  0.0  0.0  41784  6356 ?        S    Sep07   0:28 (ssl_crtd) -s /var/lib/squid/ssl_db -M 16MB
squid    11941  0.0  0.0  41800  6308 ?        S    Sep07   0:07 (ssl_crtd) -s /var/lib/squid/ssl_db -M 16MB
squid    11942  0.0  0.0  41800  6308 ?        S    Sep07   0:02 (ssl_crtd) -s /var/lib/squid/ssl_db -M 16MB
squid    11943  0.0  0.0  41784  6320 ?        S    Sep07   0:00 (ssl_crtd) -s /var/lib/squid/ssl_db -M 16MB
squid    11944  0.0  0.0  41784  6068 ?        S    Sep07   0:00 (ssl_crtd) -s /var/lib/squid/ssl_db -M 16MB
squid    11945  0.0  0.0  41780  6372 ?        S    Sep07   0:00 (ssl_crtd) -s /var/lib/squid/ssl_db -M 16MB
squid    11946  0.0  0.0  41800  6852 ?        S    Sep07   0:00 (ssl_crtd) -s /var/lib/squid/ssl_db -M 16MB
squid    11947  0.0  0.0  41784  6756 ?        S    Sep07   0:00 (ssl_crtd) -s /var/lib/squid/ssl_db -M 16MB
squid    11948  0.0  0.0  41792  6784 ?        S    Sep07   0:00 (ssl_crtd) -s /var/lib/squid/ssl_db -M 16MB
squid    11949  0.0  0.0  41780  6672 ?        S    Sep07   0:00 (ssl_crtd) -s /var/lib/squid/ssl_db -M 16MB
squid    11950  0.0  0.0  41780  6660 ?        S    Sep07   0:00 (ssl_crtd) -s /var/lib/squid/ssl_db -M 16MB
squid    11951  0.0  0.0  41760  6308 ?        S    Sep07   0:00 (ssl_crtd) -s /var/lib/squid/ssl_db -M 16MB
squid    11952  0.0  0.0  41772  6336 ?        S    Sep07   0:00 (ssl_crtd) -s /var/lib/squid/ssl_db -M 16MB
squid    11953  0.0  0.0  41772  6284 ?        S    Sep07   0:00 (ssl_crtd) -s /var/lib/squid/ssl_db -M 16MB
squid    11954  0.0  0.0  41776  6956 ?        S    Sep07   0:00 (ssl_crtd) -s /var/lib/squid/ssl_db -M 16MB
squid    11955  0.0  0.0  41772  6524 ?        S    Sep07   0:00 (ssl_crtd) -s /var/lib/squid/ssl_db -M 16MB
squid    11956  0.0  0.0  41772  6664 ?        S    Sep07   0:00 (ssl_crtd) -s /var/lib/squid/ssl_db -M 16MB
squid    11958  0.0  0.0  41772  6284 ?        S    Sep07   0:00 (ssl_crtd) -s /var/lib/squid/ssl_db -M 16MB
squid    11959  0.0  0.0  40444  3368 ?        S    Sep07   0:00 (ssl_crtd) -s /var/lib/squid/ssl_db -M 16MB
squid    11960  0.0  0.0  40444  3368 ?        S    Sep07   0:00 (ssl_crtd) -s /var/lib/squid/ssl_db -M 16MB
squid    11968  0.0  0.0  40444  3368 ?        S    Sep07   0:00 (ssl_crtd) -s /var/lib/squid/ssl_db -M 16MB
squid    11969  0.0  0.0  40444  3368 ?        S    Sep07   0:00 (ssl_crtd) -s /var/lib/squid/ssl_db -M 16MB
squid    11970  0.0  0.0  40444  3368 ?        S    Sep07   0:00 (ssl_crtd) -s /var/lib/squid/ssl_db -M 16MB
squid    11971  0.0  0.0  40444  3368 ?        S    Sep07   0:00 (ssl_crtd) -s /var/lib/squid/ssl_db -M 16MB
squid    11972  0.0  0.0  40444  3368 ?        S    Sep07   0:00 (ssl_crtd) -s /var/lib/squid/ssl_db -M 16MB
squid    11973  0.0  0.0  40444  3528 ?        S    Sep07   0:00 (ssl_crtd) -s /var/lib/squid/ssl_db -M 16MB
squid    11974  0.0  0.0  40444  3364 ?        S    Sep07   0:00 (ssl_crtd) -s /var/lib/squid/ssl_db -M 16MB
squid    11977  0.0  0.0  40444  3528 ?        S    Sep07   0:00 (ssl_crtd) -s /var/lib/squid/ssl_db -M 16MB
squid    11979  0.0  0.0  40444  3528 ?        S    Sep07   0:00 (ssl_crtd) -s /var/lib/squid/ssl_db -M 16MB
squid    11980  0.0  0.0  40444  3528 ?        S    Sep07   0:00 (ssl_crtd) -s /var/lib/squid/ssl_db -M 16MB
squid    11981  0.0  0.0  40444  3532 ?        S    Sep07   0:00 (ssl_crtd) -s /var/lib/squid/ssl_db -M 16MB
squid    11982  0.0  0.0  40444  3528 ?        S    Sep07   0:00 (ssl_crtd) -s /var/lib/squid/ssl_db -M 16MB
squid    11983  0.0  0.0  40444  3524 ?        S    Sep07   0:00 (ssl_crtd) -s /var/lib/squid/ssl_db -M 16MB
squid    11984  0.0  0.0  40444  3528 ?        S    Sep07   0:00 (ssl_crtd) -s /var/lib/squid/ssl_db -M 16MB
squid    11986  0.0  0.0  40444  3364 ?        S    Sep07   0:00 (ssl_crtd) -s /var/lib/squid/ssl_db -M 16MB
squid    11987  0.0  0.0  40444  3368 ?        S    Sep07   0:00 (ssl_crtd) -s /var/lib/squid/ssl_db -M 16MB
squid    11988  0.0  0.0  40444  3336 ?        S    Sep07   0:00 (ssl_crtd) -s /var/lib/squid/ssl_db -M 16MB
squid    11989  0.0  0.0  40444  3368 ?        S    Sep07   0:00 (ssl_crtd) -s /var/lib/squid/ssl_db -M 16MB
squid    11990  0.0  0.0  27204  8000 ?        S    Sep07   0:00 /usr/bin/perl -w /usr/libexec/squid/ext_wbinfo_group_acl -K
squid    11991  0.0  0.0  27204  7744 ?        S    Sep07   0:00 /usr/bin/perl -w /usr/libexec/squid/ext_wbinfo_group_acl -K
squid    11992  0.0  0.0  27140  7816 ?        S    Sep07   0:00 /usr/bin/perl -w /usr/libexec/squid/ext_wbinfo_group_acl -K
squid    11993  0.0  0.0  27140  7824 ?        S    Sep07   0:00 /usr/bin/perl -w /usr/libexec/squid/ext_wbinfo_group_acl -K
squid    11994  0.0  0.0  27140  7756 ?        S    Sep07   0:00 /usr/bin/perl -w /usr/libexec/squid/ext_wbinfo_group_acl -K
squid    11995  0.1  0.0  69048 12904 ?        S    Sep07   3:32 /usr/bin/perl -w -s /opt/custom/scripts/run/scripts/firewall/squid_url_lookup.pl -tbl_name=shallalist_bl adv,aggressive,alcohol,anonvpn,automobile_bikes,automobile_boats,automobile_cars,automobile_planes,chat,costtraps,dating,drugs,dynamic,finance_insurance,finance_moneylending,finance_other,finance_realestate,finance_trading,fortunetelling,forum,gamble,hacking,hobby_cooking,hobby_games-misc,hobby_games-online,hobby_gardening,hobby_pets,homestyle,imagehosting,isp,jobsearch,military,models,movies,music,podcasts,politics,porn,radiotv,recreation_humor,recreation_martialarts,recreation_restaurants,recreation_sports,recreation_travel,recreation_wellness,redirector,religion,remotecontrol,ringtones,science_astronomy,science_chemistry,sex_education,sex_lingerie,shopping,socialnet,spyware,tracker,updatesites,urlshortener,violence,warez,weapons,webphone,webradio,webtv
squid    11996  0.0  0.0  69052 12764 ?        S    Sep07   0:22 /usr/bin/perl -w -s /opt/custom/scripts/run/scripts/firewall/squid_url_lookup.pl -tbl_name=shallalist_bl adv,aggressive,alcohol,anonvpn,automobile_bikes,automobile_boats,automobile_cars,automobile_planes,chat,costtraps,dating,drugs,dynamic,finance_insurance,finance_moneylending,finance_other,finance_realestate,finance_trading,fortunetelling,forum,gamble,hacking,hobby_cooking,hobby_games-misc,hobby_games-online,hobby_gardening,hobby_pets,homestyle,imagehosting,isp,jobsearch,military,models,movies,music,podcasts,politics,porn,radiotv,recreation_humor,recreation_martialarts,recreation_restaurants,recreation_sports,recreation_travel,recreation_wellness,redirector,religion,remotecontrol,ringtones,science_astronomy,science_chemistry,sex_education,sex_lingerie,shopping,socialnet,spyware,tracker,updatesites,urlshortener,violence,warez,weapons,webphone,webradio,webtv
squid    11997  0.0  0.0  69064 12796 ?        S    Sep07   0:07 /usr/bin/perl -w -s /opt/custom/scripts/run/scripts/firewall/squid_url_lookup.pl -tbl_name=shallalist_bl adv,aggressive,alcohol,anonvpn,automobile_bikes,automobile_boats,automobile_cars,automobile_planes,chat,costtraps,dating,drugs,dynamic,finance_insurance,finance_moneylending,finance_other,finance_realestate,finance_trading,fortunetelling,forum,gamble,hacking,hobby_cooking,hobby_games-misc,hobby_games-online,hobby_gardening,hobby_pets,homestyle,imagehosting,isp,jobsearch,military,models,movies,music,podcasts,politics,porn,radiotv,recreation_humor,recreation_martialarts,recreation_restaurants,recreation_sports,recreation_travel,recreation_wellness,redirector,religion,remotecontrol,ringtones,science_astronomy,science_chemistry,sex_education,sex_lingerie,shopping,socialnet,spyware,tracker,updatesites,urlshortener,violence,warez,weapons,webphone,webradio,webtv
squid    11998  0.0  0.0  69056 13232 ?        S    Sep07   0:04 /usr/bin/perl -w -s /opt/custom/scripts/run/scripts/firewall/squid_url_lookup.pl -tbl_name=shallalist_bl adv,aggressive,alcohol,anonvpn,automobile_bikes,automobile_boats,automobile_cars,automobile_planes,chat,costtraps,dating,drugs,dynamic,finance_insurance,finance_moneylending,finance_other,finance_realestate,finance_trading,fortunetelling,forum,gamble,hacking,hobby_cooking,hobby_games-misc,hobby_games-online,hobby_gardening,hobby_pets,homestyle,imagehosting,isp,jobsearch,military,models,movies,music,podcasts,politics,porn,radiotv,recreation_humor,recreation_martialarts,recreation_restaurants,recreation_sports,recreation_travel,recreation_wellness,redirector,religion,remotecontrol,ringtones,science_astronomy,science_chemistry,sex_education,sex_lingerie,shopping,socialnet,spyware,tracker,updatesites,urlshortener,violence,warez,weapons,webphone,webradio,webtv
squid    11999  0.0  0.0  69036 13108 ?        S    Sep07   0:03 /usr/bin/perl -w -s /opt/custom/scripts/run/scripts/firewall/squid_url_lookup.pl -tbl_name=shallalist_bl adv,aggressive,alcohol,anonvpn,automobile_bikes,automobile_boats,automobile_cars,automobile_planes,chat,costtraps,dating,drugs,dynamic,finance_insurance,finance_moneylending,finance_other,finance_realestate,finance_trading,fortunetelling,forum,gamble,hacking,hobby_cooking,hobby_games-misc,hobby_games-online,hobby_gardening,hobby_pets,homestyle,imagehosting,isp,jobsearch,military,models,movies,music,podcasts,politics,porn,radiotv,recreation_humor,recreation_martialarts,recreation_restaurants,recreation_sports,recreation_travel,recreation_wellness,redirector,religion,remotecontrol,ringtones,science_astronomy,science_chemistry,sex_education,sex_lingerie,shopping,socialnet,spyware,tracker,updatesites,urlshortener,violence,warez,weapons,webphone,webradio,webtv
squid    12000  0.0  0.0  69028 12944 ?        S    Sep07   0:02 /usr/bin/perl -w -s /opt/custom/scripts/run/scripts/firewall/squid_url_lookup.pl -tbl_name=shallalist_bl adv,aggressive,alcohol,anonvpn,automobile_bikes,automobile_boats,automobile_cars,automobile_planes,chat,costtraps,dating,drugs,dynamic,finance_insurance,finance_moneylending,finance_other,finance_realestate,finance_trading,fortunetelling,forum,gamble,hacking,hobby_cooking,hobby_games-misc,hobby_games-online,hobby_gardening,hobby_pets,homestyle,imagehosting,isp,jobsearch,military,models,movies,music,podcasts,politics,porn,radiotv,recreation_humor,recreation_martialarts,recreation_restaurants,recreation_sports,recreation_travel,recreation_wellness,redirector,religion,remotecontrol,ringtones,science_astronomy,science_chemistry,sex_education,sex_lingerie,shopping,socialnet,spyware,tracker,updatesites,urlshortener,violence,warez,weapons,webphone,webradio,webtv
squid    12001  0.0  0.0  69044 12824 ?        S    Sep07   0:00 /usr/bin/perl -w -s /opt/custom/scripts/run/scripts/firewall/squid_url_lookup.pl -tbl_name=shallalist_bl adv,aggressive,alcohol,anonvpn,automobile_bikes,automobile_boats,automobile_cars,automobile_planes,chat,costtraps,dating,drugs,dynamic,finance_insurance,finance_moneylending,finance_other,finance_realestate,finance_trading,fortunetelling,forum,gamble,hacking,hobby_cooking,hobby_games-misc,hobby_games-online,hobby_gardening,hobby_pets,homestyle,imagehosting,isp,jobsearch,military,models,movies,music,podcasts,politics,porn,radiotv,recreation_humor,recreation_martialarts,recreation_restaurants,recreation_sports,recreation_travel,recreation_wellness,redirector,religion,remotecontrol,ringtones,science_astronomy,science_chemistry,sex_education,sex_lingerie,shopping,socialnet,spyware,tracker,updatesites,urlshortener,violence,warez,weapons,webphone,webradio,webtv
squid    12003  0.0  0.0  68920 12716 ?        S    Sep07   0:00 /usr/bin/perl -w -s /opt/custom/scripts/run/scripts/firewall/squid_url_lookup.pl -tbl_name=shallalist_bl adv,aggressive,alcohol,anonvpn,automobile_bikes,automobile_boats,automobile_cars,automobile_planes,chat,costtraps,dating,drugs,dynamic,finance_insurance,finance_moneylending,finance_other,finance_realestate,finance_trading,fortunetelling,forum,gamble,hacking,hobby_cooking,hobby_games-misc,hobby_games-online,hobby_gardening,hobby_pets,homestyle,imagehosting,isp,jobsearch,military,models,movies,music,podcasts,politics,porn,radiotv,recreation_humor,recreation_martialarts,recreation_restaurants,recreation_sports,recreation_travel,recreation_wellness,redirector,religion,remotecontrol,ringtones,science_astronomy,science_chemistry,sex_education,sex_lingerie,shopping,socialnet,spyware,tracker,updatesites,urlshortener,violence,warez,weapons,webphone,webradio,webtv
squid    12004  0.0  0.0  68792 12596 ?        S    Sep07   0:00 /usr/bin/perl -w -s /opt/custom/scripts/run/scripts/firewall/squid_url_lookup.pl -tbl_name=shallalist_bl adv,aggressive,alcohol,anonvpn,automobile_bikes,automobile_boats,automobile_cars,automobile_planes,chat,costtraps,dating,drugs,dynamic,finance_insurance,finance_moneylending,finance_other,finance_realestate,finance_trading,fortunetelling,forum,gamble,hacking,hobby_cooking,hobby_games-misc,hobby_games-online,hobby_gardening,hobby_pets,homestyle,imagehosting,isp,jobsearch,military,models,movies,music,podcasts,politics,porn,radiotv,recreation_humor,recreation_martialarts,recreation_restaurants,recreation_sports,recreation_travel,recreation_wellness,redirector,religion,remotecontrol,ringtones,science_astronomy,science_chemistry,sex_education,sex_lingerie,shopping,socialnet,spyware,tracker,updatesites,urlshortener,violence,warez,weapons,webphone,webradio,webtv
squid    12005  0.0  0.0  68792 12408 ?        S    Sep07   0:00 /usr/bin/perl -w -s /opt/custom/scripts/run/scripts/firewall/squid_url_lookup.pl -tbl_name=shallalist_bl adv,aggressive,alcohol,anonvpn,automobile_bikes,automobile_boats,automobile_cars,automobile_planes,chat,costtraps,dating,drugs,dynamic,finance_insurance,finance_moneylending,finance_other,finance_realestate,finance_trading,fortunetelling,forum,gamble,hacking,hobby_cooking,hobby_games-misc,hobby_games-online,hobby_gardening,hobby_pets,homestyle,imagehosting,isp,jobsearch,military,models,movies,music,podcasts,politics,porn,radiotv,recreation_humor,recreation_martialarts,recreation_restaurants,recreation_sports,recreation_travel,recreation_wellness,redirector,religion,remotecontrol,ringtones,science_astronomy,science_chemistry,sex_education,sex_lingerie,shopping,socialnet,spyware,tracker,updatesites,urlshortener,violence,warez,weapons,webphone,webradio,webtv
squid    12006  0.0  0.0  68792 12712 ?        S    Sep07   0:00 /usr/bin/perl -w -s /opt/custom/scripts/run/scripts/firewall/squid_url_lookup.pl -tbl_name=shallalist_bl adv,aggressive,alcohol,anonvpn,automobile_bikes,automobile_boats,automobile_cars,automobile_planes,chat,costtraps,dating,drugs,dynamic,finance_insurance,finance_moneylending,finance_other,finance_realestate,finance_trading,fortunetelling,forum,gamble,hacking,hobby_cooking,hobby_games-misc,hobby_games-online,hobby_gardening,hobby_pets,homestyle,imagehosting,isp,jobsearch,military,models,movies,music,podcasts,politics,porn,radiotv,recreation_humor,recreation_martialarts,recreation_restaurants,recreation_sports,recreation_travel,recreation_wellness,redirector,religion,remotecontrol,ringtones,science_astronomy,science_chemistry,sex_education,sex_lingerie,shopping,socialnet,spyware,tracker,updatesites,urlshortener,violence,warez,weapons,webphone,webradio,webtv
squid    12007  0.0  0.0  68792 12716 ?        S    Sep07   0:00 /usr/bin/perl -w -s /opt/custom/scripts/run/scripts/firewall/squid_url_lookup.pl -tbl_name=shallalist_bl adv,aggressive,alcohol,anonvpn,automobile_bikes,automobile_boats,automobile_cars,automobile_planes,chat,costtraps,dating,drugs,dynamic,finance_insurance,finance_moneylending,finance_other,finance_realestate,finance_trading,fortunetelling,forum,gamble,hacking,hobby_cooking,hobby_games-misc,hobby_games-online,hobby_gardening,hobby_pets,homestyle,imagehosting,isp,jobsearch,military,models,movies,music,podcasts,politics,porn,radiotv,recreation_humor,recreation_martialarts,recreation_restaurants,recreation_sports,recreation_travel,recreation_wellness,redirector,religion,remotecontrol,ringtones,science_astronomy,science_chemistry,sex_education,sex_lingerie,shopping,socialnet,spyware,tracker,updatesites,urlshortener,violence,warez,weapons,webphone,webradio,webtv
squid    12008  0.0  0.0  68792 12532 ?        S    Sep07   0:00 /usr/bin/perl -w -s /opt/custom/scripts/run/scripts/firewall/squid_url_lookup.pl -tbl_name=shallalist_bl adv,aggressive,alcohol,anonvpn,automobile_bikes,automobile_boats,automobile_cars,automobile_planes,chat,costtraps,dating,drugs,dynamic,finance_insurance,finance_moneylending,finance_other,finance_realestate,finance_trading,fortunetelling,forum,gamble,hacking,hobby_cooking,hobby_games-misc,hobby_games-online,hobby_gardening,hobby_pets,homestyle,imagehosting,isp,jobsearch,military,models,movies,music,podcasts,politics,porn,radiotv,recreation_humor,recreation_martialarts,recreation_restaurants,recreation_sports,recreation_travel,recreation_wellness,redirector,religion,remotecontrol,ringtones,science_astronomy,science_chemistry,sex_education,sex_lingerie,shopping,socialnet,spyware,tracker,updatesites,urlshortener,violence,warez,weapons,webphone,webradio,webtv
squid    12009  0.0  0.0  68792 12664 ?        S    Sep07   0:00 /usr/bin/perl -w -s /opt/custom/scripts/run/scripts/firewall/squid_url_lookup.pl -tbl_name=shallalist_bl adv,aggressive,alcohol,anonvpn,automobile_bikes,automobile_boats,automobile_cars,automobile_planes,chat,costtraps,dating,drugs,dynamic,finance_insurance,finance_moneylending,finance_other,finance_realestate,finance_trading,fortunetelling,forum,gamble,hacking,hobby_cooking,hobby_games-misc,hobby_games-online,hobby_gardening,hobby_pets,homestyle,imagehosting,isp,jobsearch,military,models,movies,music,podcasts,politics,porn,radiotv,recreation_humor,recreation_martialarts,recreation_restaurants,recreation_sports,recreation_travel,recreation_wellness,redirector,religion,remotecontrol,ringtones,science_astronomy,science_chemistry,sex_education,sex_lingerie,shopping,socialnet,spyware,tracker,updatesites,urlshortener,violence,warez,weapons,webphone,webradio,webtv
squid    12010  0.0  0.0  68792 12504 ?        S    Sep07   0:00 /usr/bin/perl -w -s /opt/custom/scripts/run/scripts/firewall/squid_url_lookup.pl -tbl_name=shallalist_bl adv,aggressive,alcohol,anonvpn,automobile_bikes,automobile_boats,automobile_cars,automobile_planes,chat,costtraps,dating,drugs,dynamic,finance_insurance,finance_moneylending,finance_other,finance_realestate,finance_trading,fortunetelling,forum,gamble,hacking,hobby_cooking,hobby_games-misc,hobby_games-online,hobby_gardening,hobby_pets,homestyle,imagehosting,isp,jobsearch,military,models,movies,music,podcasts,politics,porn,radiotv,recreation_humor,recreation_martialarts,recreation_restaurants,recreation_sports,recreation_travel,recreation_wellness,redirector,religion,remotecontrol,ringtones,science_astronomy,science_chemistry,sex_education,sex_lingerie,shopping,socialnet,spyware,tracker,updatesites,urlshortener,violence,warez,weapons,webphone,webradio,webtv
squid    12011  0.0  0.0  68792 12660 ?        S    Sep07   0:00 /usr/bin/perl -w -s /opt/custom/scripts/run/scripts/firewall/squid_url_lookup.pl -tbl_name=shallalist_bl adv,aggressive,alcohol,anonvpn,automobile_bikes,automobile_boats,automobile_cars,automobile_planes,chat,costtraps,dating,drugs,dynamic,finance_insurance,finance_moneylending,finance_other,finance_realestate,finance_trading,fortunetelling,forum,gamble,hacking,hobby_cooking,hobby_games-misc,hobby_games-online,hobby_gardening,hobby_pets,homestyle,imagehosting,isp,jobsearch,military,models,movies,music,podcasts,politics,porn,radiotv,recreation_humor,recreation_martialarts,recreation_restaurants,recreation_sports,recreation_travel,recreation_wellness,redirector,religion,remotecontrol,ringtones,science_astronomy,science_chemistry,sex_education,sex_lingerie,shopping,socialnet,spyware,tracker,updatesites,urlshortener,violence,warez,weapons,webphone,webradio,webtv
squid    12012  0.0  0.0  68792 12556 ?        S    Sep07   0:00 /usr/bin/perl -w -s /opt/custom/scripts/run/scripts/firewall/squid_url_lookup.pl -tbl_name=shallalist_bl adv,aggressive,alcohol,anonvpn,automobile_bikes,automobile_boats,automobile_cars,automobile_planes,chat,costtraps,dating,drugs,dynamic,finance_insurance,finance_moneylending,finance_other,finance_realestate,finance_trading,fortunetelling,forum,gamble,hacking,hobby_cooking,hobby_games-misc,hobby_games-online,hobby_gardening,hobby_pets,homestyle,imagehosting,isp,jobsearch,military,models,movies,music,podcasts,politics,porn,radiotv,recreation_humor,recreation_martialarts,recreation_restaurants,recreation_sports,recreation_travel,recreation_wellness,redirector,religion,remotecontrol,ringtones,science_astronomy,science_chemistry,sex_education,sex_lingerie,shopping,socialnet,spyware,tracker,updatesites,urlshortener,violence,warez,weapons,webphone,webradio,webtv
squid    12013  0.0  0.0  68792 13816 ?        S    Sep07   0:00 /usr/bin/perl -w -s /opt/custom/scripts/run/scripts/firewall/squid_url_lookup.pl -tbl_name=shallalist_bl adv,aggressive,alcohol,anonvpn,automobile_bikes,automobile_boats,automobile_cars,automobile_planes,chat,costtraps,dating,drugs,dynamic,finance_insurance,finance_moneylending,finance_other,finance_realestate,finance_trading,fortunetelling,forum,gamble,hacking,hobby_cooking,hobby_games-misc,hobby_games-online,hobby_gardening,hobby_pets,homestyle,imagehosting,isp,jobsearch,military,models,movies,music,podcasts,politics,porn,radiotv,recreation_humor,recreation_martialarts,recreation_restaurants,recreation_sports,recreation_travel,recreation_wellness,redirector,religion,remotecontrol,ringtones,science_astronomy,science_chemistry,sex_education,sex_lingerie,shopping,socialnet,spyware,tracker,updatesites,urlshortener,violence,warez,weapons,webphone,webradio,webtv
squid    12014  0.0  0.0  68792 13740 ?        S    Sep07   0:00 /usr/bin/perl -w -s /opt/custom/scripts/run/scripts/firewall/squid_url_lookup.pl -tbl_name=shallalist_bl adv,aggressive,alcohol,anonvpn,automobile_bikes,automobile_boats,automobile_cars,automobile_planes,chat,costtraps,dating,drugs,dynamic,finance_insurance,finance_moneylending,finance_other,finance_realestate,finance_trading,fortunetelling,forum,gamble,hacking,hobby_cooking,hobby_games-misc,hobby_games-online,hobby_gardening,hobby_pets,homestyle,imagehosting,isp,jobsearch,military,models,movies,music,podcasts,politics,porn,radiotv,recreation_humor,recreation_martialarts,recreation_restaurants,recreation_sports,recreation_travel,recreation_wellness,redirector,religion,remotecontrol,ringtones,science_astronomy,science_chemistry,sex_education,sex_lingerie,shopping,socialnet,spyware,tracker,updatesites,urlshortener,violence,warez,weapons,webphone,webradio,webtv
squid    12015  0.0  0.0  68792 13756 ?        S    Sep07   0:00 /usr/bin/perl -w -s /opt/custom/scripts/run/scripts/firewall/squid_url_lookup.pl -tbl_name=shallalist_bl adv,aggressive,alcohol,anonvpn,automobile_bikes,automobile_boats,automobile_cars,automobile_planes,chat,costtraps,dating,drugs,dynamic,finance_insurance,finance_moneylending,finance_other,finance_realestate,finance_trading,fortunetelling,forum,gamble,hacking,hobby_cooking,hobby_games-misc,hobby_games-online,hobby_gardening,hobby_pets,homestyle,imagehosting,isp,jobsearch,military,models,movies,music,podcasts,politics,porn,radiotv,recreation_humor,recreation_martialarts,recreation_restaurants,recreation_sports,recreation_travel,recreation_wellness,redirector,religion,remotecontrol,ringtones,science_astronomy,science_chemistry,sex_education,sex_lingerie,shopping,socialnet,spyware,tracker,updatesites,urlshortener,violence,warez,weapons,webphone,webradio,webtv
squid    12016  0.0  0.0  68792 13820 ?        S    Sep07   0:00 /usr/bin/perl -w -s /opt/custom/scripts/run/scripts/firewall/squid_url_lookup.pl -tbl_name=shallalist_bl adv,aggressive,alcohol,anonvpn,automobile_bikes,automobile_boats,automobile_cars,automobile_planes,chat,costtraps,dating,drugs,dynamic,finance_insurance,finance_moneylending,finance_other,finance_realestate,finance_trading,fortunetelling,forum,gamble,hacking,hobby_cooking,hobby_games-misc,hobby_games-online,hobby_gardening,hobby_pets,homestyle,imagehosting,isp,jobsearch,military,models,movies,music,podcasts,politics,porn,radiotv,recreation_humor,recreation_martialarts,recreation_restaurants,recreation_sports,recreation_travel,recreation_wellness,redirector,religion,remotecontrol,ringtones,science_astronomy,science_chemistry,sex_education,sex_lingerie,shopping,socialnet,spyware,tracker,updatesites,urlshortener,violence,warez,weapons,webphone,webradio,webtv
squid    12017  0.0  0.0  68792 13812 ?        S    Sep07   0:00 /usr/bin/perl -w -s /opt/custom/scripts/run/scripts/firewall/squid_url_lookup.pl -tbl_name=shallalist_bl adv,aggressive,alcohol,anonvpn,automobile_bikes,automobile_boats,automobile_cars,automobile_planes,chat,costtraps,dating,drugs,dynamic,finance_insurance,finance_moneylending,finance_other,finance_realestate,finance_trading,fortunetelling,forum,gamble,hacking,hobby_cooking,hobby_games-misc,hobby_games-online,hobby_gardening,hobby_pets,homestyle,imagehosting,isp,jobsearch,military,models,movies,music,podcasts,politics,porn,radiotv,recreation_humor,recreation_martialarts,recreation_restaurants,recreation_sports,recreation_travel,recreation_wellness,redirector,religion,remotecontrol,ringtones,science_astronomy,science_chemistry,sex_education,sex_lingerie,shopping,socialnet,spyware,tracker,updatesites,urlshortener,violence,warez,weapons,webphone,webradio,webtv
squid    12018  0.0  0.0  68792 13820 ?        S    Sep07   0:00 /usr/bin/perl -w -s /opt/custom/scripts/run/scripts/firewall/squid_url_lookup.pl -tbl_name=shallalist_bl adv,aggressive,alcohol,anonvpn,automobile_bikes,automobile_boats,automobile_cars,automobile_planes,chat,costtraps,dating,drugs,dynamic,finance_insurance,finance_moneylending,finance_other,finance_realestate,finance_trading,fortunetelling,forum,gamble,hacking,hobby_cooking,hobby_games-misc,hobby_games-online,hobby_gardening,hobby_pets,homestyle,imagehosting,isp,jobsearch,military,models,movies,music,podcasts,politics,porn,radiotv,recreation_humor,recreation_martialarts,recreation_restaurants,recreation_sports,recreation_travel,recreation_wellness,redirector,religion,remotecontrol,ringtones,science_astronomy,science_chemistry,sex_education,sex_lingerie,shopping,socialnet,spyware,tracker,updatesites,urlshortener,violence,warez,weapons,webphone,webradio,webtv
squid    12019  0.0  0.0  68792 13792 ?        S    Sep07   0:00 /usr/bin/perl -w -s /opt/custom/scripts/run/scripts/firewall/squid_url_lookup.pl -tbl_name=shallalist_bl adv,aggressive,alcohol,anonvpn,automobile_bikes,automobile_boats,automobile_cars,automobile_planes,chat,costtraps,dating,drugs,dynamic,finance_insurance,finance_moneylending,finance_other,finance_realestate,finance_trading,fortunetelling,forum,gamble,hacking,hobby_cooking,hobby_games-misc,hobby_games-online,hobby_gardening,hobby_pets,homestyle,imagehosting,isp,jobsearch,military,models,movies,music,podcasts,politics,porn,radiotv,recreation_humor,recreation_martialarts,recreation_restaurants,recreation_sports,recreation_travel,recreation_wellness,redirector,religion,remotecontrol,ringtones,science_astronomy,science_chemistry,sex_education,sex_lingerie,shopping,socialnet,spyware,tracker,updatesites,urlshortener,violence,warez,weapons,webphone,webradio,webtv
squid    12020  0.0  0.0  32932  9372 ?        S    Sep07   0:00 /usr/bin/perl -w -s /opt/custom/scripts/run/scripts/firewall/squid_url_lookup.pl -tbl_name=shallalist_bl adv,aggressive,alcohol,anonvpn,automobile_bikes,automobile_boats,automobile_cars,automobile_planes,chat,costtraps,dating,drugs,dynamic,finance_insurance,finance_moneylending,finance_other,finance_realestate,finance_trading,fortunetelling,forum,gamble,hacking,hobby_cooking,hobby_games-misc,hobby_games-online,hobby_gardening,hobby_pets,homestyle,imagehosting,isp,jobsearch,military,models,movies,music,podcasts,politics,porn,radiotv,recreation_humor,recreation_martialarts,recreation_restaurants,recreation_sports,recreation_travel,recreation_wellness,redirector,religion,remotecontrol,ringtones,science_astronomy,science_chemistry,sex_education,sex_lingerie,shopping,socialnet,spyware,tracker,updatesites,urlshortener,violence,warez,weapons,webphone,webradio,webtv
squid    12021  0.0  0.0  32932  9396 ?        S    Sep07   0:00 /usr/bin/perl -w -s /opt/custom/scripts/run/scripts/firewall/squid_url_lookup.pl -tbl_name=shallalist_bl adv,aggressive,alcohol,anonvpn,automobile_bikes,automobile_boats,automobile_cars,automobile_planes,chat,costtraps,dating,drugs,dynamic,finance_insurance,finance_moneylending,finance_other,finance_realestate,finance_trading,fortunetelling,forum,gamble,hacking,hobby_cooking,hobby_games-misc,hobby_games-online,hobby_gardening,hobby_pets,homestyle,imagehosting,isp,jobsearch,military,models,movies,music,podcasts,politics,porn,radiotv,recreation_humor,recreation_martialarts,recreation_restaurants,recreation_sports,recreation_travel,recreation_wellness,redirector,religion,remotecontrol,ringtones,science_astronomy,science_chemistry,sex_education,sex_lingerie,shopping,socialnet,spyware,tracker,updatesites,urlshortener,violence,warez,weapons,webphone,webradio,webtv
squid    12022  0.0  0.0  32932  9364 ?        S    Sep07   0:00 /usr/bin/perl -w -s /opt/custom/scripts/run/scripts/firewall/squid_url_lookup.pl -tbl_name=shallalist_bl adv,aggressive,alcohol,anonvpn,automobile_bikes,automobile_boats,automobile_cars,automobile_planes,chat,costtraps,dating,drugs,dynamic,finance_insurance,finance_moneylending,finance_other,finance_realestate,finance_trading,fortunetelling,forum,gamble,hacking,hobby_cooking,hobby_games-misc,hobby_games-online,hobby_gardening,hobby_pets,homestyle,imagehosting,isp,jobsearch,military,models,movies,music,podcasts,politics,porn,radiotv,recreation_humor,recreation_martialarts,recreation_restaurants,recreation_sports,recreation_travel,recreation_wellness,redirector,religion,remotecontrol,ringtones,science_astronomy,science_chemistry,sex_education,sex_lingerie,shopping,socialnet,spyware,tracker,updatesites,urlshortener,violence,warez,weapons,webphone,webradio,webtv
squid    12023  0.0  0.0  32932  9400 ?        S    Sep07   0:00 /usr/bin/perl -w -s /opt/custom/scripts/run/scripts/firewall/squid_url_lookup.pl -tbl_name=shallalist_bl adv,aggressive,alcohol,anonvpn,automobile_bikes,automobile_boats,automobile_cars,automobile_planes,chat,costtraps,dating,drugs,dynamic,finance_insurance,finance_moneylending,finance_other,finance_realestate,finance_trading,fortunetelling,forum,gamble,hacking,hobby_cooking,hobby_games-misc,hobby_games-online,hobby_gardening,hobby_pets,homestyle,imagehosting,isp,jobsearch,military,models,movies,music,podcasts,politics,porn,radiotv,recreation_humor,recreation_martialarts,recreation_restaurants,recreation_sports,recreation_travel,recreation_wellness,redirector,religion,remotecontrol,ringtones,science_astronomy,science_chemistry,sex_education,sex_lingerie,shopping,socialnet,spyware,tracker,updatesites,urlshortener,violence,warez,weapons,webphone,webradio,webtv
squid    12025  0.0  0.0  32932  9396 ?        S    Sep07   0:00 /usr/bin/perl -w -s /opt/custom/scripts/run/scripts/firewall/squid_url_lookup.pl -tbl_name=shallalist_bl adv,aggressive,alcohol,anonvpn,automobile_bikes,automobile_boats,automobile_cars,automobile_planes,chat,costtraps,dating,drugs,dynamic,finance_insurance,finance_moneylending,finance_other,finance_realestate,finance_trading,fortunetelling,forum,gamble,hacking,hobby_cooking,hobby_games-misc,hobby_games-online,hobby_gardening,hobby_pets,homestyle,imagehosting,isp,jobsearch,military,models,movies,music,podcasts,politics,porn,radiotv,recreation_humor,recreation_martialarts,recreation_restaurants,recreation_sports,recreation_travel,recreation_wellness,redirector,religion,remotecontrol,ringtones,science_astronomy,science_chemistry,sex_education,sex_lingerie,shopping,socialnet,spyware,tracker,updatesites,urlshortener,violence,warez,weapons,webphone,webradio,webtv
squid    12026  0.0  0.0  32932  9396 ?        S    Sep07   0:00 /usr/bin/perl -w -s /opt/custom/scripts/run/scripts/firewall/squid_url_lookup.pl -tbl_name=shallalist_bl adv,aggressive,alcohol,anonvpn,automobile_bikes,automobile_boats,automobile_cars,automobile_planes,chat,costtraps,dating,drugs,dynamic,finance_insurance,finance_moneylending,finance_other,finance_realestate,finance_trading,fortunetelling,forum,gamble,hacking,hobby_cooking,hobby_games-misc,hobby_games-online,hobby_gardening,hobby_pets,homestyle,imagehosting,isp,jobsearch,military,models,movies,music,podcasts,politics,porn,radiotv,recreation_humor,recreation_martialarts,recreation_restaurants,recreation_sports,recreation_travel,recreation_wellness,redirector,religion,remotecontrol,ringtones,science_astronomy,science_chemistry,sex_education,sex_lingerie,shopping,socialnet,spyware,tracker,updatesites,urlshortener,violence,warez,weapons,webphone,webradio,webtv
squid    12027  0.0  0.0  32932  9372 ?        S    Sep07   0:00 /usr/bin/perl -w -s /opt/custom/scripts/run/scripts/firewall/squid_url_lookup.pl -tbl_name=shallalist_bl adv,aggressive,alcohol,anonvpn,automobile_bikes,automobile_boats,automobile_cars,automobile_planes,chat,costtraps,dating,drugs,dynamic,finance_insurance,finance_moneylending,finance_other,finance_realestate,finance_trading,fortunetelling,forum,gamble,hacking,hobby_cooking,hobby_games-misc,hobby_games-online,hobby_gardening,hobby_pets,homestyle,imagehosting,isp,jobsearch,military,models,movies,music,podcasts,politics,porn,radiotv,recreation_humor,recreation_martialarts,recreation_restaurants,recreation_sports,recreation_travel,recreation_wellness,redirector,religion,remotecontrol,ringtones,science_astronomy,science_chemistry,sex_education,sex_lingerie,shopping,socialnet,spyware,tracker,updatesites,urlshortener,violence,warez,weapons,webphone,webradio,webtv
squid    12028  0.0  0.0  32932  9368 ?        S    Sep07   0:00 /usr/bin/perl -w -s /opt/custom/scripts/run/scripts/firewall/squid_url_lookup.pl -tbl_name=shallalist_bl adv,aggressive,alcohol,anonvpn,automobile_bikes,automobile_boats,automobile_cars,automobile_planes,chat,costtraps,dating,drugs,dynamic,finance_insurance,finance_moneylending,finance_other,finance_realestate,finance_trading,fortunetelling,forum,gamble,hacking,hobby_cooking,hobby_games-misc,hobby_games-online,hobby_gardening,hobby_pets,homestyle,imagehosting,isp,jobsearch,military,models,movies,music,podcasts,politics,porn,radiotv,recreation_humor,recreation_martialarts,recreation_restaurants,recreation_sports,recreation_travel,recreation_wellness,redirector,religion,remotecontrol,ringtones,science_astronomy,science_chemistry,sex_education,sex_lingerie,shopping,socialnet,spyware,tracker,updatesites,urlshortener,violence,warez,weapons,webphone,webradio,webtv
squid    12029  0.0  0.0  32932  9372 ?        S    Sep07   0:00 /usr/bin/perl -w -s /opt/custom/scripts/run/scripts/firewall/squid_url_lookup.pl -tbl_name=shallalist_bl adv,aggressive,alcohol,anonvpn,automobile_bikes,automobile_boats,automobile_cars,automobile_planes,chat,costtraps,dating,drugs,dynamic,finance_insurance,finance_moneylending,finance_other,finance_realestate,finance_trading,fortunetelling,forum,gamble,hacking,hobby_cooking,hobby_games-misc,hobby_games-online,hobby_gardening,hobby_pets,homestyle,imagehosting,isp,jobsearch,military,models,movies,music,podcasts,politics,porn,radiotv,recreation_humor,recreation_martialarts,recreation_restaurants,recreation_sports,recreation_travel,recreation_wellness,redirector,religion,remotecontrol,ringtones,science_astronomy,science_chemistry,sex_education,sex_lingerie,shopping,socialnet,spyware,tracker,updatesites,urlshortener,violence,warez,weapons,webphone,webradio,webtv
squid    12032  0.0  0.0  32932  9372 ?        S    Sep07   0:00 /usr/bin/perl -w -s /opt/custom/scripts/run/scripts/firewall/squid_url_lookup.pl -tbl_name=shallalist_bl adv,aggressive,alcohol,anonvpn,automobile_bikes,automobile_boats,automobile_cars,automobile_planes,chat,costtraps,dating,drugs,dynamic,finance_insurance,finance_moneylending,finance_other,finance_realestate,finance_trading,fortunetelling,forum,gamble,hacking,hobby_cooking,hobby_games-misc,hobby_games-online,hobby_gardening,hobby_pets,homestyle,imagehosting,isp,jobsearch,military,models,movies,music,podcasts,politics,porn,radiotv,recreation_humor,recreation_martialarts,recreation_restaurants,recreation_sports,recreation_travel,recreation_wellness,redirector,religion,remotecontrol,ringtones,science_astronomy,science_chemistry,sex_education,sex_lingerie,shopping,socialnet,spyware,tracker,updatesites,urlshortener,violence,warez,weapons,webphone,webradio,webtv
squid    12033  0.0  0.0  32932  9368 ?        S    Sep07   0:00 /usr/bin/perl -w -s /opt/custom/scripts/run/scripts/firewall/squid_url_lookup.pl -tbl_name=shallalist_bl adv,aggressive,alcohol,anonvpn,automobile_bikes,automobile_boats,automobile_cars,automobile_planes,chat,costtraps,dating,drugs,dynamic,finance_insurance,finance_moneylending,finance_other,finance_realestate,finance_trading,fortunetelling,forum,gamble,hacking,hobby_cooking,hobby_games-misc,hobby_games-online,hobby_gardening,hobby_pets,homestyle,imagehosting,isp,jobsearch,military,models,movies,music,podcasts,politics,porn,radiotv,recreation_humor,recreation_martialarts,recreation_restaurants,recreation_sports,recreation_travel,recreation_wellness,redirector,religion,remotecontrol,ringtones,science_astronomy,science_chemistry,sex_education,sex_lingerie,shopping,socialnet,spyware,tracker,updatesites,urlshortener,violence,warez,weapons,webphone,webradio,webtv
squid    12034  0.0  0.0  32932  9392 ?        S    Sep07   0:00 /usr/bin/perl -w -s /opt/custom/scripts/run/scripts/firewall/squid_url_lookup.pl -tbl_name=shallalist_bl adv,aggressive,alcohol,anonvpn,automobile_bikes,automobile_boats,automobile_cars,automobile_planes,chat,costtraps,dating,drugs,dynamic,finance_insurance,finance_moneylending,finance_other,finance_realestate,finance_trading,fortunetelling,forum,gamble,hacking,hobby_cooking,hobby_games-misc,hobby_games-online,hobby_gardening,hobby_pets,homestyle,imagehosting,isp,jobsearch,military,models,movies,music,podcasts,politics,porn,radiotv,recreation_humor,recreation_martialarts,recreation_restaurants,recreation_sports,recreation_travel,recreation_wellness,redirector,religion,remotecontrol,ringtones,science_astronomy,science_chemistry,sex_education,sex_lingerie,shopping,socialnet,spyware,tracker,updatesites,urlshortener,violence,warez,weapons,webphone,webradio,webtv
squid    12036  0.0  0.0  32932  9404 ?        S    Sep07   0:00 /usr/bin/perl -w -s /opt/custom/scripts/run/scripts/firewall/squid_url_lookup.pl -tbl_name=shallalist_bl adv,aggressive,alcohol,anonvpn,automobile_bikes,automobile_boats,automobile_cars,automobile_planes,chat,costtraps,dating,drugs,dynamic,finance_insurance,finance_moneylending,finance_other,finance_realestate,finance_trading,fortunetelling,forum,gamble,hacking,hobby_cooking,hobby_games-misc,hobby_games-online,hobby_gardening,hobby_pets,homestyle,imagehosting,isp,jobsearch,military,models,movies,music,podcasts,politics,porn,radiotv,recreation_humor,recreation_martialarts,recreation_restaurants,recreation_sports,recreation_travel,recreation_wellness,redirector,religion,remotecontrol,ringtones,science_astronomy,science_chemistry,sex_education,sex_lingerie,shopping,socialnet,spyware,tracker,updatesites,urlshortener,violence,warez,weapons,webphone,webradio,webtv
squid    12037  0.0  0.0  32932  9400 ?        S    Sep07   0:00 /usr/bin/perl -w -s /opt/custom/scripts/run/scripts/firewall/squid_url_lookup.pl -tbl_name=shallalist_bl adv,aggressive,alcohol,anonvpn,automobile_bikes,automobile_boats,automobile_cars,automobile_planes,chat,costtraps,dating,drugs,dynamic,finance_insurance,finance_moneylending,finance_other,finance_realestate,finance_trading,fortunetelling,forum,gamble,hacking,hobby_cooking,hobby_games-misc,hobby_games-online,hobby_gardening,hobby_pets,homestyle,imagehosting,isp,jobsearch,military,models,movies,music,podcasts,politics,porn,radiotv,recreation_humor,recreation_martialarts,recreation_restaurants,recreation_sports,recreation_travel,recreation_wellness,redirector,religion,remotecontrol,ringtones,science_astronomy,science_chemistry,sex_education,sex_lingerie,shopping,socialnet,spyware,tracker,updatesites,urlshortener,violence,warez,weapons,webphone,webradio,webtv
squid    12038  0.0  0.0  32932  9368 ?        S    Sep07   0:00 /usr/bin/perl -w -s /opt/custom/scripts/run/scripts/firewall/squid_url_lookup.pl -tbl_name=shallalist_bl adv,aggressive,alcohol,anonvpn,automobile_bikes,automobile_boats,automobile_cars,automobile_planes,chat,costtraps,dating,drugs,dynamic,finance_insurance,finance_moneylending,finance_other,finance_realestate,finance_trading,fortunetelling,forum,gamble,hacking,hobby_cooking,hobby_games-misc,hobby_games-online,hobby_gardening,hobby_pets,homestyle,imagehosting,isp,jobsearch,military,models,movies,music,podcasts,politics,porn,radiotv,recreation_humor,recreation_martialarts,recreation_restaurants,recreation_sports,recreation_travel,recreation_wellness,redirector,religion,remotecontrol,ringtones,science_astronomy,science_chemistry,sex_education,sex_lingerie,shopping,socialnet,spyware,tracker,updatesites,urlshortener,violence,warez,weapons,webphone,webradio,webtv
squid    12039  0.0  0.0  32932  9368 ?        S    Sep07   0:00 /usr/bin/perl -w -s /opt/custom/scripts/run/scripts/firewall/squid_url_lookup.pl -tbl_name=shallalist_bl adv,aggressive,alcohol,anonvpn,automobile_bikes,automobile_boats,automobile_cars,automobile_planes,chat,costtraps,dating,drugs,dynamic,finance_insurance,finance_moneylending,finance_other,finance_realestate,finance_trading,fortunetelling,forum,gamble,hacking,hobby_cooking,hobby_games-misc,hobby_games-online,hobby_gardening,hobby_pets,homestyle,imagehosting,isp,jobsearch,military,models,movies,music,podcasts,politics,porn,radiotv,recreation_humor,recreation_martialarts,recreation_restaurants,recreation_sports,recreation_travel,recreation_wellness,redirector,religion,remotecontrol,ringtones,science_astronomy,science_chemistry,sex_education,sex_lingerie,shopping,socialnet,spyware,tracker,updatesites,urlshortener,violence,warez,weapons,webphone,webradio,webtv
squid    14108  0.0  0.0  24404  1392 ?        S    02:13   0:00 (negotiate_kerberos_auth) -s HTTP/inf-fw2.mydomain.org at MYDOMAIN.ORG
root     28492  0.0  0.0  86908  4220 ?        Ss   Sep05   0:00 /usr/sbin/squid -YC -f /etc/squid/squid.owa.conf -n squidowa
squid    28495  0.0  0.0 103616 18364 ?        S    Sep05   0:12 (squid-1) -YC -f /etc/squid/squid.owa.conf -n squidowa
root     29120  0.0  0.0  86908  4220 ?        Ss   Sep05   0:00 /usr/sbin/squid -YC -f /etc/squid/squid.owa2.conf -n squidowa2
squid    29123  0.2  0.6 293460 206776 ?       S    Sep05  10:30 (squid-1) -YC -f /etc/squid/squid.owa2.conf -n squidowa2
squid    30291  0.0  0.0  24404  2392 ?        S    Sep07   0:00 (negotiate_kerberos_auth) -s HTTP/inf-fw2.mydomain.org at MYDOMAIN.ORG
squid    30330  0.0  6.4 3769624 2127928 ?     S    13:42   0:00 (squid-1) -YC -f /etc/squid/squid.conf -n squid
squid    30866  0.0  6.4 3769624 2127928 ?     S    13:44   0:00 (squid-1) -YC -f /etc/squid/squid.conf -n squid


top - 13:52:38 up 9 days,  6:19,  2 users,  load average: 2.04, 1.82, 1.65
Tasks: 405 total,   1 running, 404 sleeping,   0 stopped,   0 zombie
%Cpu0  :  0.0 us,  0.0 sy,  0.0 ni, 97.4 id,  0.0 wa,  0.0 hi,  2.6 si,  0.0 st
%Cpu1  :  0.0 us,  0.3 sy,  0.0 ni, 97.0 id,  0.0 wa,  0.0 hi,  2.6 si,  0.0 st
%Cpu2  :  0.3 us,  0.3 sy,  0.0 ni, 98.7 id,  0.0 wa,  0.0 hi,  0.7 si,  0.0 st
%Cpu3  :  0.0 us,  0.0 sy,  0.0 ni, 99.7 id,  0.0 wa,  0.0 hi,  0.3 si,  0.0 st
%Cpu4  :  0.0 us,  0.0 sy,  0.0 ni, 96.7 id,  0.0 wa,  0.0 hi,  3.3 si,  0.0 st
%Cpu5  :  0.0 us,  0.3 sy,  0.0 ni, 99.0 id,  0.0 wa,  0.0 hi,  0.7 si,  0.0 st
%Cpu6  :  0.3 us,  0.0 sy,  0.0 ni, 99.0 id,  0.0 wa,  0.0 hi,  0.7 si,  0.0 st
%Cpu7  :  0.3 us,  0.0 sy,  0.0 ni, 95.7 id,  0.0 wa,  0.0 hi,  4.0 si,  0.0 st
KiB Mem : 32865056 total, 12324092 free, 16396808 used,  4144156 buff/cache
KiB Swap: 37036988 total, 35197252 free,  1839736 used. 15977208 avail Mem

PID USER      PR  NI    VIRT    RES    SHR S  %CPU %MEM     TIME+ COMMAND
29123 squid     20   0  293460 206776   8480 S   0.7  0.6  10:31.16 squid
427 root      20   0   20280   2912   2168 R   0.3  0.0   0:00.05 top
7902 named     20   0  821424 111644   5720 S   0.3  0.3  95:30.99 named
16766 suricata  20   0 2127364 358940   7356 S   0.3  1.1   3:12.03 Suricata-Main
1 root      20   0    4176   1512   1464 S   0.0  0.0   0:06.34 init

I then issued:
# strace -o squid.trace /usr/sbin/squid -YC -f /etc/squid/squid.conf -n squid
I didn't get much out of it, but I can post it if someone would like to see it.

I tried to stop Squid with my openrc script and failed.
So I issued the following manually several times:
# squid -n squid -k shutdown
The first result was:
# squid -n squid -k shutdown
2017/09/08 13:56:01.125| 24,8| SBuf.cc(124) ~SBuf: SBuf59 destructed
2017/09/08 13:56:01.125| 24,9| MemBlob.cc(83) ~MemBlob: destructed, this=0xd74290 id=blob170 capacity=40 size=16
2017/09/08 13:56:01.125| 24,8| SBuf.cc(124) ~SBuf: SBuf48 destructed
2017/09/08 13:56:01.125| 24,9| MemBlob.cc(83) ~MemBlob: destructed, this=0xd709a0 id=blob62 capacity=40 size=30
2017/09/08 13:56:01.125| 24,8| SBuf.cc(124) ~SBuf: SBuf44 destructed
2017/09/08 13:56:01.125| 24,9| MemBlob.cc(83) ~MemBlob: destructed, this=0xcdc6e0 id=blob51 capacity=40 size=12
2017/09/08 13:56:01.125| 28,3| Acl.cc(384) ~ACL: freeing ACL adaptation_access
2017/09/08 13:56:01.125| 45,9| cbdata.cc(321) cbdataInternalFree: 0xdf8298
2017/09/08 13:56:01.125| 45,9| cbdata.cc(338) cbdataInternalFree: Freeing 0xdf8298
2017/09/08 13:56:01.125| 24,8| SBuf.cc(124) ~SBuf: SBuf38 destructed
2017/09/08 13:56:01.125| 24,9| MemBlob.cc(83) ~MemBlob: destructed, this=0xd60be0 id=blob37 capacity=16388 size=8
2017/09/08 13:56:01.125| 24,8| SBuf.cc(124) ~SBuf: SBuf37 destructed
2017/09/08 13:56:01.125| 24,9| MemBlob.cc(83) ~MemBlob: destructed, this=0xd5cb90 id=blob36 capacity=16388 size=5
[...etc...]
Subsequent calls yielded:
squid: ERROR: Could not send signal 15 to process 5694: (3) No such process
2017/09/08 13:56:09.993| 24,8| SBuf.cc(124) ~SBuf: SBuf59 destructed
2017/09/08 13:56:09.993| 24,9| MemBlob.cc(83) ~MemBlob: destructed, this=0xd74290 id=blob170 capacity=40 size=16
2017/09/08 13:56:09.993| 24,8| SBuf.cc(124) ~SBuf: SBuf48 destructed
2017/09/08 13:56:09.993| 24,9| MemBlob.cc(83) ~MemBlob: destructed, this=0xd709a0 id=blob62 capacity=40 size=30
2017/09/08 13:56:09.993| 24,8| SBuf.cc(124) ~SBuf: SBuf44 destructed
2017/09/08 13:56:09.993| 24,9| MemBlob.cc(83) ~MemBlob: destructed, this=0xcdc6e0 id=blob51 capacity=40 size=12
2017/09/08 13:56:09.993| 28,3| Acl.cc(384) ~ACL: freeing ACL adaptation_access
2017/09/08 13:56:09.993| 45,9| cbdata.cc(321) cbdataInternalFree: 0xdf8298
2017/09/08 13:56:09.993| 45,9| cbdata.cc(338) cbdataInternalFree: Freeing 0xdf8298
2017/09/08 13:56:09.993| 24,8| SBuf.cc(124) ~SBuf: SBuf38 destructed
2017/09/08 13:56:09.993| 24,9| MemBlob.cc(83) ~MemBlob: destructed, this=0xd60be0 id=blob37 capacity=16388 size=8
2017/09/08 13:56:09.993| 24,8| SBuf.cc(124) ~SBuf: SBuf37 destructed
2017/09/08 13:56:09.994| 24,9| MemBlob.cc(83) ~MemBlob: destructed, this=0xd5cb90 id=blob36 capacity=16388 size=5
[...etc...]
# ps aux | grep squid | grep '\-n'
squid      805  0.0  6.4 3769624 2127988 ?     S    13:53   0:00 (squid-1) -YC -f /etc/squid/squid.conf -n squid
squid     1452  0.0  6.4 3769624 2127988 ?     S    13:55   0:00 (squid-1) -YC -f /etc/squid/squid.conf -n squid
root      3043  0.0  0.0  84856  1728 ?        Ss   Aug31   0:00 /usr/sbin/squid -YC -f /etc/squid/squid.http.conf -n squidhttp
squid     3046  0.0  0.0 128232 31052 ?        S    Aug31   0:35 (squid-1) -YC -f /etc/squid/squid.http.conf -n squidhttp
root      3538  0.0  0.0  86912  1740 ?        Ss   Aug31   0:00 /usr/sbin/squid -YC -f /etc/squid/squid.https.conf -n squidhttps
squid     3540  0.0  0.1 134940 36244 ?        S    Aug31   1:09 (squid-1) -YC -f /etc/squid/squid.https.conf -n squidhttps
root     28492  0.0  0.0  86908  4220 ?        Ss   Sep05   0:00 /usr/sbin/squid -YC -f /etc/squid/squid.owa.conf -n squidowa
squid    28495  0.0  0.0 103616 18364 ?        S    Sep05   0:12 (squid-1) -YC -f /etc/squid/squid.owa.conf -n squidowa
root     29120  0.0  0.0  86908  4220 ?        Ss   Sep05   0:00 /usr/sbin/squid -YC -f /etc/squid/squid.owa2.conf -n squidowa2
squid    29123  0.2  0.6 293460 206776 ?       S    Sep05  10:31 (squid-1) -YC -f /etc/squid/squid.owa2.conf -n squidowa2
squid    30330  0.0  6.4 3769624 2127928 ?     S    13:42   0:00 (squid-1) -YC -f /etc/squid/squid.conf -n squid
squid    30866  0.0  6.4 3769624 2127928 ?     S    13:44   0:00 (squid-1) -YC -f /etc/squid/squid.conf -n squid
squid    31507  0.0  6.4 3769624 2127928 ?     S    13:47   0:00 (squid-1) -YC -f /etc/squid/squid.conf -n squid
squid    32055  0.0  6.4 3769624 2127928 ?     S    13:49   0:00 (squid-1) -YC -f /etc/squid/squid.conf -n squid
squid    32659  0.0  6.4 3769624 2127928 ?     S    13:51   0:00 (squid-1) -YC -f /etc/squid/squid.conf -n squid

I changed the debug_options to ALL,9 and tried to restart or "reconfigure" Squid.
I had trouble with that and decided to:
- stop all squid instances (the other reverse proxies, eg. squidhttp, squidhttps, squidowa, squidowa2, were working fine)
- manually kill all squid processes related to the failing local cache (including ssl_crtd)
- made sure there were no more squid processes and that debug_options was ALL,9
- started the local squid cache only (only 1 daemon - no reverse proxies)

Surprisingly, it still didn't work...
Clients could not browse. The squid log only had this set of messages once in a while (very quiet log):

2017/09/08 14:18:10.322 kid1| 54,3| ipc.cc(180) ipcCreate: ipcCreate: prfd FD 94
2017/09/08 14:18:10.322 kid1| 54,3| ipc.cc(181) ipcCreate: ipcCreate: pwfd FD 94
2017/09/08 14:18:10.322 kid1| 54,3| ipc.cc(182) ipcCreate: ipcCreate: crfd FD 93
2017/09/08 14:18:10.322 kid1| 54,3| ipc.cc(183) ipcCreate: ipcCreate: cwfd FD 93
2017/09/08 14:18:10.322 kid1| 54,3| ipc.cc(196) ipcCreate: ipcCreate: FD 94 sockaddr [::1]:40905
2017/09/08 14:18:10.322 kid1| 54,3| ipc.cc(212) ipcCreate: ipcCreate: FD 93 sockaddr [::1]:50647
2017/09/08 14:18:10.322 kid1| 54,3| ipc.cc(222) ipcCreate: ipcCreate: FD 93 listening...
2017/09/08 14:18:10.322 kid1| 5,3| comm.cc(868) _comm_close: comm_close: start closing FD 93
2017/09/08 14:18:10.322 kid1| 5,3| comm.cc(540) commUnsetFdTimeout: Remove timeout for FD 93
2017/09/08 14:18:10.322 kid1| 5,5| comm.cc(721) commCallCloseHandlers: commCallCloseHandlers: FD 93
2017/09/08 14:18:10.322 kid1| 5,4| AsyncCall.cc(26) AsyncCall: The AsyncCall comm_close_complete constructed, this=0xeb7c10 [call123]
2017/09/08 14:18:10.322 kid1| 5,4| AsyncCall.cc(93) ScheduleCall: comm.cc(941) will call comm_close_complete(FD 93) [call123]
2017/09/08 14:18:10.322 kid1| 5,9| comm.cc(602) comm_connect_addr: connecting socket FD 94 to [::1]:50647 (want family: 10)
2017/09/08 14:18:10.322 kid1| 21,3| tools.cc(543) leave_suid: leave_suid: PID 8303 called
2017/09/08 14:18:10.323 kid1| 21,3| tools.cc(636) no_suid: no_suid: PID 8303 giving up root priveleges forever
2017/09/08 14:18:10.323 kid1| 54,3| ipc.cc(304) ipcCreate: ipcCreate: calling accept on FD 93
[...]
2017/09/08 14:24:49.682 kid1| 5,5| comm.cc(644) comm_connect_addr: sock=98, addrinfo( flags=4, family=10, socktype=1, protocol=6, &addr=0xeb79a0, addrlen=28 )
2017/09/08 14:24:49.682 kid1| 5,9| comm.cc(645) comm_connect_addr: connect FD 98: (-1) (110) Connection timed out
2017/09/08 14:24:49.682 kid1| 14,9| comm.cc(646) comm_connect_addr: connecting to: [::1]:60557
2017/09/08 14:24:49.682 kid1| 5,3| comm.cc(868) _comm_close: comm_close: start closing FD 98
2017/09/08 14:24:49.682 kid1| 5,3| comm.cc(540) commUnsetFdTimeout: Remove timeout for FD 98
2017/09/08 14:24:49.682 kid1| 5,5| comm.cc(721) commCallCloseHandlers: commCallCloseHandlers: FD 98
2017/09/08 14:24:49.682 kid1| 5,4| AsyncCall.cc(26) AsyncCall: The AsyncCall comm_close_complete constructed, this=0xeb7e90 [call128]
2017/09/08 14:24:49.682 kid1| 5,4| AsyncCall.cc(93) ScheduleCall: comm.cc(941) will call comm_close_complete(FD 98) [call128]
2017/09/08 14:24:49.682 kid1| WARNING: Cannot run '/usr/libexec/squid/ext_wbinfo_group_acl' process.
2017/09/08 14:24:49.682 kid1| 50,3| comm.cc(347) comm_openex: comm_openex: Attempt open socket for: [::1]
2017/09/08 14:24:49.682 kid1| 50,3| comm.cc(388) comm_openex: comm_openex: Opened socket local=[::1] remote=[::] FD 99 flags=1 : family=10, type=1, protocol=0
2017/09/08 14:24:49.682 kid1| 5,5| comm.cc(420) comm_init_opened: local=[::1] remote=[::] FD 99 flags=1 is a new socket
2017/09/08 14:24:49.682 kid1| 51,3| fd.cc(198) fd_open: fd_open() FD 99 ext_wbinfo_group_acl
2017/09/08 14:24:49.682 kid1| 50,6| comm.cc(209) commBind: commBind: bind socket FD 99 to [::1]
2017/09/08 14:24:49.682 kid1| 50,3| comm.cc(347) comm_openex: comm_openex: Attempt open socket for: [::1]
2017/09/08 14:24:49.682 kid1| 50,3| comm.cc(388) comm_openex: comm_openex: Opened socket local=[::1] remote=[::] FD 100 flags=1 : family=10, type=1, protocol=0
2017/09/08 14:24:49.682 kid1| 5,5| comm.cc(420) comm_init_opened: local=[::1] remote=[::] FD 100 flags=1 is a new socket
2017/09/08 14:24:49.682 kid1| 51,3| fd.cc(198) fd_open: fd_open() FD 100 ext_wbinfo_group_acl
2017/09/08 14:24:49.682 kid1| 50,6| comm.cc(209) commBind: commBind: bind socket FD 100 to [::1]
2017/09/08 14:24:49.682 kid1| 54,3| ipc.cc(180) ipcCreate: ipcCreate: prfd FD 100
2017/09/08 14:24:49.682 kid1| 54,3| ipc.cc(181) ipcCreate: ipcCreate: pwfd FD 100
2017/09/08 14:24:49.682 kid1| 54,3| ipc.cc(182) ipcCreate: ipcCreate: crfd FD 99
2017/09/08 14:24:49.682 kid1| 54,3| ipc.cc(183) ipcCreate: ipcCreate: cwfd FD 99
2017/09/08 14:24:49.682 kid1| 54,3| ipc.cc(196) ipcCreate: ipcCreate: FD 100 sockaddr [::1]:56367
2017/09/08 14:24:49.682 kid1| 54,3| ipc.cc(212) ipcCreate: ipcCreate: FD 99 sockaddr [::1]:35643
2017/09/08 14:24:49.682 kid1| 54,3| ipc.cc(222) ipcCreate: ipcCreate: FD 99 listening...
2017/09/08 14:24:49.683 kid1| 5,3| comm.cc(868) _comm_close: comm_close: start closing FD 99
2017/09/08 14:24:49.683 kid1| 5,3| comm.cc(540) commUnsetFdTimeout: Remove timeout for FD 99
2017/09/08 14:24:49.683 kid1| 5,5| comm.cc(721) commCallCloseHandlers: commCallCloseHandlers: FD 99
2017/09/08 14:24:49.683 kid1| 5,4| AsyncCall.cc(26) AsyncCall: The AsyncCall comm_close_complete constructed, this=0xeb7f10 [call129]
2017/09/08 14:24:49.683 kid1| 5,4| AsyncCall.cc(93) ScheduleCall: comm.cc(941) will call comm_close_complete(FD 99) [call129]
2017/09/08 14:24:49.683 kid1| 5,9| comm.cc(602) comm_connect_addr: connecting socket FD 100 to [::1]:35643 (want family: 10)
2017/09/08 14:24:49.683 kid1| 21,3| tools.cc(543) leave_suid: leave_suid: PID 10064 called
2017/09/08 14:24:49.683 kid1| 21,3| tools.cc(636) no_suid: no_suid: PID 10064 giving up root priveleges forever
2017/09/08 14:24:49.683 kid1| 54,3| ipc.cc(304) ipcCreate: ipcCreate: calling accept on FD 99

I also tried restarting Winbind, c-icap server, clamd, but nothing changed. They all seemed to be working properly anyway.

Finally, at some point (after half an hour) right after killing all squid processes yet again, I restarted the squid services one last time (reverted to debug_options ALL,1) before giving up and deciding to let users bypass the proxy. Well, it all started working again...

So now I'd really like to know what I can do the next time it stops working like this.
I'm considering setting debug_options to ALL,6 while I still can, and wait to see if it fails again. When it does, I might have more information.

Any suggestions?

Vieri


From squid3 at treenet.co.nz  Sat Sep  9 12:10:33 2017
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sun, 10 Sep 2017 00:10:33 +1200
Subject: [squid-users] squid cache takes a break
In-Reply-To: <1234530834.188010.1504892340901@mail.yahoo.com>
References: <1234530834.188010.1504892340901.ref@mail.yahoo.com>
 <1234530834.188010.1504892340901@mail.yahoo.com>
Message-ID: <ef119e7a-6b6f-0a22-f903-a257ecb0af83@treenet.co.nz>

On 09/09/17 05:39, Vieri wrote:
> Hi,
> 
> Sorry for the title, but I really don't know how to describe what just happened today. It's really odd.
> 
> I previously posted a few similar issues which were all fixed if I increased certain parameters (ulimits, children-{max,startup,idle}, TTL, etc.).
> 
> This time however, after several days trouble-free I got another show-stopper. The local squid cache stopped serving for almost half an hour. After that, it all started working again magically. I had the chance to log into the server with ssh and try a few things:
> 
> - In the cache log I could see these messages:
> Starting new bllookup helpers...
> helperOpenServers: Starting 10/80 'squid_url_lookup.pl' processes
> WARNING: Cannot run '/opt/custom/scripts/run/scripts/firewall/squid_url_lookup.pl' process.
> WARNING: Cannot run '/opt/custom/scripts/run/scripts/firewall/squid_url_lookup.pl' process.
> WARNING: Cannot run '/opt/custom/scripts/run/scripts/firewall/squid_url_lookup.pl' process.
> 
> It doesn't say much as to why it "cannot run" the external program.
> 

Looking at the code that message only seems to get logged if there is a 
TCP/UDP connection involved and it is having packet errors (many reasons 
for that).



> This is how the program is defined in squid.conf:
> external_acl_type bllookup ttl=86400 negative_ttl=86400 children-max=80 children-startup=40 children-idle=10 %URI /opt/custom/scripts/run/scripts/firewall/squid_url_lookup.pl [...]
> 

... so far matching, external ACL use a private/localhost TCP connection 
between each helper and Squid.

> Other than that, the log is pretty quiet.
> 
> The HTTP clients do not get served at all. They keep waiting for a reply.
> 

Squid is waiting for a reply from a helper about whether the request is 
allowed or not.


> 
> top - 13:52:38 up 9 days,  6:19,  2 users,  load average: 2.04, 1.82, 1.65
> Tasks: 405 total,   1 running, 404 sleeping,   0 stopped,   0 zombie
> %Cpu0  :  0.0 us,  0.0 sy,  0.0 ni, 97.4 id,  0.0 wa,  0.0 hi,  2.6 si,  0.0 st
> %Cpu1  :  0.0 us,  0.3 sy,  0.0 ni, 97.0 id,  0.0 wa,  0.0 hi,  2.6 si,  0.0 st
> %Cpu2  :  0.3 us,  0.3 sy,  0.0 ni, 98.7 id,  0.0 wa,  0.0 hi,  0.7 si,  0.0 st
> %Cpu3  :  0.0 us,  0.0 sy,  0.0 ni, 99.7 id,  0.0 wa,  0.0 hi,  0.3 si,  0.0 st
> %Cpu4  :  0.0 us,  0.0 sy,  0.0 ni, 96.7 id,  0.0 wa,  0.0 hi,  3.3 si,  0.0 st
> %Cpu5  :  0.0 us,  0.3 sy,  0.0 ni, 99.0 id,  0.0 wa,  0.0 hi,  0.7 si,  0.0 st
> %Cpu6  :  0.3 us,  0.0 sy,  0.0 ni, 99.0 id,  0.0 wa,  0.0 hi,  0.7 si,  0.0 st
> %Cpu7  :  0.3 us,  0.0 sy,  0.0 ni, 95.7 id,  0.0 wa,  0.0 hi,  4.0 si,  0.0 st
> KiB Mem : 32865056 total, 12324092 free, 16396808 used,  4144156 buff/cache
> KiB Swap: 37036988 total, 35197252 free,  1839736 used. 15977208 avail Mem
> 
> PID USER      PR  NI    VIRT    RES    SHR S  %CPU %MEM     TIME+ COMMAND
> 29123 squid     20   0  293460 206776   8480 S   0.7  0.6  10:31.16 squid

Starting each helper requires 206766 MB of RAM to be allocated in 
swap/virtual. Copying 10x that data (+10 helpers per occurance) may take 
a while.

We workaround that normally by using concurrency. Helpers that support 
high levels of concurrency can handle a lot more than blocking /

Note that concurrency is just a way of pipelining requests to a helper, 
it does not require multi-threading though helpers using MT are 
naturally even better with concurrency.


> # ps aux | grep squid | grep '\-n'
> squid      805  0.0  6.4 3769624 2127988 ?     S    13:53   0:00 (squid-1) -YC -f /etc/squid/squid.conf -n squid
> squid     1452  0.0  6.4 3769624 2127988 ?     S    13:55   0:00 (squid-1) -YC -f /etc/squid/squid.conf -n squid
> root      3043  0.0  0.0  84856  1728 ?        Ss   Aug31   0:00 /usr/sbin/squid -YC -f /etc/squid/squid.http.conf -n squidhttp
> squid     3046  0.0  0.0 128232 31052 ?        S    Aug31   0:35 (squid-1) -YC -f /etc/squid/squid.http.conf -n squidhttp
> root      3538  0.0  0.0  86912  1740 ?        Ss   Aug31   0:00 /usr/sbin/squid -YC -f /etc/squid/squid.https.conf -n squidhttps
> squid     3540  0.0  0.1 134940 36244 ?        S    Aug31   1:09 (squid-1) -YC -f /etc/squid/squid.https.conf -n squidhttps
> root     28492  0.0  0.0  86908  4220 ?        Ss   Sep05   0:00 /usr/sbin/squid -YC -f /etc/squid/squid.owa.conf -n squidowa
> squid    28495  0.0  0.0 103616 18364 ?        S    Sep05   0:12 (squid-1) -YC -f /etc/squid/squid.owa.conf -n squidowa
> root     29120  0.0  0.0  86908  4220 ?        Ss   Sep05   0:00 /usr/sbin/squid -YC -f /etc/squid/squid.owa2.conf -n squidowa2
> squid    29123  0.2  0.6 293460 206776 ?       S    Sep05  10:31 (squid-1) -YC -f /etc/squid/squid.owa2.conf -n squidowa2
> squid    30330  0.0  6.4 3769624 2127928 ?     S    13:42   0:00 (squid-1) -YC -f /etc/squid/squid.conf -n squid
> squid    30866  0.0  6.4 3769624 2127928 ?     S    13:44   0:00 (squid-1) -YC -f /etc/squid/squid.conf -n squid
> squid    31507  0.0  6.4 3769624 2127928 ?     S    13:47   0:00 (squid-1) -YC -f /etc/squid/squid.conf -n squid
> squid    32055  0.0  6.4 3769624 2127928 ?     S    13:49   0:00 (squid-1) -YC -f /etc/squid/squid.conf -n squid
> squid    32659  0.0  6.4 3769624 2127928 ?     S    13:51   0:00 (squid-1) -YC -f /etc/squid/squid.conf -n squid
> 
> I changed the debug_options to ALL,9 and tried to restart or "reconfigure" Squid.
> I had trouble with that and decided to:
> - stop all squid instances (the other reverse proxies, eg. squidhttp, squidhttps, squidowa, squidowa2, were working fine)
> - manually kill all squid processes related to the failing local cache (including ssl_crtd)
> - made sure there were no more squid processes and that debug_options was ALL,9
> - started the local squid cache only (only 1 daemon - no reverse proxies)
> 
> Surprisingly, it still didn't work...
> Clients could not browse. The squid log only had this set of messages once in a while (very quiet log):
> 
> 2017/09/08 14:18:10.322 kid1| 54,3| ipc.cc(180) ipcCreate: ipcCreate: prfd FD 94
> 2017/09/08 14:18:10.322 kid1| 54,3| ipc.cc(181) ipcCreate: ipcCreate: pwfd FD 94
> 2017/09/08 14:18:10.322 kid1| 54,3| ipc.cc(182) ipcCreate: ipcCreate: crfd FD 93
> 2017/09/08 14:18:10.322 kid1| 54,3| ipc.cc(183) ipcCreate: ipcCreate: cwfd FD 93
> 2017/09/08 14:18:10.322 kid1| 54,3| ipc.cc(196) ipcCreate: ipcCreate: FD 94 sockaddr [::1]:40905
> 2017/09/08 14:18:10.322 kid1| 54,3| ipc.cc(212) ipcCreate: ipcCreate: FD 93 sockaddr [::1]:50647
> 2017/09/08 14:18:10.322 kid1| 54,3| ipc.cc(222) ipcCreate: ipcCreate: FD 93 listening...
> 2017/09/08 14:18:10.322 kid1| 5,3| comm.cc(868) _comm_close: comm_close: start closing FD 93
> 2017/09/08 14:18:10.322 kid1| 5,3| comm.cc(540) commUnsetFdTimeout: Remove timeout for FD 93
> 2017/09/08 14:18:10.322 kid1| 5,5| comm.cc(721) commCallCloseHandlers: commCallCloseHandlers: FD 93
> 2017/09/08 14:18:10.322 kid1| 5,4| AsyncCall.cc(26) AsyncCall: The AsyncCall comm_close_complete constructed, this=0xeb7c10 [call123]
> 2017/09/08 14:18:10.322 kid1| 5,4| AsyncCall.cc(93) ScheduleCall: comm.cc(941) will call comm_close_complete(FD 93) [call123]
> 2017/09/08 14:18:10.322 kid1| 5,9| comm.cc(602) comm_connect_addr: connecting socket FD 94 to [::1]:50647 (want family: 10)
> 2017/09/08 14:18:10.322 kid1| 21,3| tools.cc(543) leave_suid: leave_suid: PID 8303 called
> 2017/09/08 14:18:10.323 kid1| 21,3| tools.cc(636) no_suid: no_suid: PID 8303 giving up root priveleges forever
> 2017/09/08 14:18:10.323 kid1| 54,3| ipc.cc(304) ipcCreate: ipcCreate: calling accept on FD 93

Whatever this helper being started on FD 93/94 was, it is successful.
We know it is a helper because the FD are used by ipc.cc (helper I/O 
channels) for something, but that is all.


> [...]
> 2017/09/08 14:24:49.682 kid1| 5,5| comm.cc(644) comm_connect_addr: sock=98, addrinfo( flags=4, family=10, socktype=1, protocol=6, &addr=0xeb79a0, addrlen=28 )
> 2017/09/08 14:24:49.682 kid1| 5,9| comm.cc(645) comm_connect_addr: connect FD 98: (-1) (110) Connection timed out
> 2017/09/08 14:24:49.682 kid1| 14,9| comm.cc(646) comm_connect_addr: connecting to: [::1]:60557
> 2017/09/08 14:24:49.682 kid1| 5,3| comm.cc(868) _comm_close: comm_close: start closing FD 98
> 2017/09/08 14:24:49.682 kid1| 5,3| comm.cc(540) commUnsetFdTimeout: Remove timeout for FD 98
> 2017/09/08 14:24:49.682 kid1| 5,5| comm.cc(721) commCallCloseHandlers: commCallCloseHandlers: FD 98
> 2017/09/08 14:24:49.682 kid1| 5,4| AsyncCall.cc(26) AsyncCall: The AsyncCall comm_close_complete constructed, this=0xeb7e90 [call128]
> 2017/09/08 14:24:49.682 kid1| 5,4| AsyncCall.cc(93) ScheduleCall: comm.cc(941) will call comm_close_complete(FD 98) [call128]
> 2017/09/08 14:24:49.682 kid1| WARNING: Cannot run '/usr/libexec/squid/ext_wbinfo_group_acl' process.

The comm TCP timeout is for FD 98. It may or may not be related to the 
wbinfo helper. The trace shown does not include any ipc.cc or fd.cc 
output indicating what that FD is used for. see the below trace.


This trace is more complete:

> 2017/09/08 14:24:49.682 kid1| 50,3| comm.cc(347) comm_openex: comm_openex: Attempt open socket for: [::1]
> 2017/09/08 14:24:49.682 kid1| 50,3| comm.cc(388) comm_openex: comm_openex: Opened socket local=[::1] remote=[::] FD 99 flags=1 : family=10, type=1, protocol=0
> 2017/09/08 14:24:49.682 kid1| 5,5| comm.cc(420) comm_init_opened: local=[::1] remote=[::] FD 99 flags=1 is a new socket
> 2017/09/08 14:24:49.682 kid1| 51,3| fd.cc(198) fd_open: fd_open() FD 99 ext_wbinfo_group_acl
> 2017/09/08 14:24:49.682 kid1| 50,6| comm.cc(209) commBind: commBind: bind socket FD 99 to [::1]
> 2017/09/08 14:24:49.682 kid1| 50,3| comm.cc(347) comm_openex: comm_openex: Attempt open socket for: [::1]
> 2017/09/08 14:24:49.682 kid1| 50,3| comm.cc(388) comm_openex: comm_openex: Opened socket local=[::1] remote=[::] FD 100 flags=1 : family=10, type=1, protocol=0
> 2017/09/08 14:24:49.682 kid1| 5,5| comm.cc(420) comm_init_opened: local=[::1] remote=[::] FD 100 flags=1 is a new socket
> 2017/09/08 14:24:49.682 kid1| 51,3| fd.cc(198) fd_open: fd_open() FD 100 ext_wbinfo_group_acl
> 2017/09/08 14:24:49.682 kid1| 50,6| comm.cc(209) commBind: commBind: bind socket FD 100 to [::1]
> 2017/09/08 14:24:49.682 kid1| 54,3| ipc.cc(180) ipcCreate: ipcCreate: prfd FD 100
> 2017/09/08 14:24:49.682 kid1| 54,3| ipc.cc(181) ipcCreate: ipcCreate: pwfd FD 100
> 2017/09/08 14:24:49.682 kid1| 54,3| ipc.cc(182) ipcCreate: ipcCreate: crfd FD 99
> 2017/09/08 14:24:49.682 kid1| 54,3| ipc.cc(183) ipcCreate: ipcCreate: cwfd FD 99
> 2017/09/08 14:24:49.682 kid1| 54,3| ipc.cc(196) ipcCreate: ipcCreate: FD 100 sockaddr [::1]:56367
> 2017/09/08 14:24:49.682 kid1| 54,3| ipc.cc(212) ipcCreate: ipcCreate: FD 99 sockaddr [::1]:35643
> 2017/09/08 14:24:49.682 kid1| 54,3| ipc.cc(222) ipcCreate: ipcCreate: FD 99 listening...
> 2017/09/08 14:24:49.683 kid1| 5,3| comm.cc(868) _comm_close: comm_close: start closing FD 99
> 2017/09/08 14:24:49.683 kid1| 5,3| comm.cc(540) commUnsetFdTimeout: Remove timeout for FD 99
> 2017/09/08 14:24:49.683 kid1| 5,5| comm.cc(721) commCallCloseHandlers: commCallCloseHandlers: FD 99
> 2017/09/08 14:24:49.683 kid1| 5,4| AsyncCall.cc(26) AsyncCall: The AsyncCall comm_close_complete constructed, this=0xeb7f10 [call129]
> 2017/09/08 14:24:49.683 kid1| 5,4| AsyncCall.cc(93) ScheduleCall: comm.cc(941) will call comm_close_complete(FD 99) [call129]
> 2017/09/08 14:24:49.683 kid1| 5,9| comm.cc(602) comm_connect_addr: connecting socket FD 100 to [::1]:35643 (want family: 10)
> 2017/09/08 14:24:49.683 kid1| 21,3| tools.cc(543) leave_suid: leave_suid: PID 10064 called
> 2017/09/08 14:24:49.683 kid1| 21,3| tools.cc(636) no_suid: no_suid: PID 10064 giving up root priveleges forever
> 2017/09/08 14:24:49.683 kid1| 54,3| ipc.cc(304) ipcCreate: ipcCreate: calling accept on FD 99

This ext_wbinfo_group_acl helper on FD 99/100 was started successfully 
AFAIK.


> 
> I also tried restarting Winbind, c-icap server, clamd, but nothing changed. They all seemed to be working properly anyway.
> 
> Finally, at some point (after half an hour) right after killing all squid processes yet again, I restarted the squid services one last time (reverted to debug_options ALL,1) before giving up and deciding to let users bypass the proxy. Well, it all started working again...
> 
> So now I'd really like to know what I can do the next time it stops working like this.

Couple of workarounds.

a) start fewer helpers at a time.

b) reduce cache_mem.

c) add concurrency support to the helpers.

All of the above are aimed at reducing the amount of background 
administrative work Squid has to do for helpers, or the amount of memory 
consumed. And thus reducing the duration and effects of these pauses, 
even if (a) causes them to be slightly more frequent.


> I'm considering setting debug_options to ALL,6 while I still can, and wait to see if it fails again. When it does, I might have more information.
> 

The log lines above indicate sections "5,9 50,6 51,3 54,9" are logging 
the helper startup details. You can reduce debug_options to those for 
smaller logs to see if the TCP connect timeout is affecting helper startup.


Amos


From squid3 at treenet.co.nz  Sat Sep  9 12:35:53 2017
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sun, 10 Sep 2017 00:35:53 +1200
Subject: [squid-users] High memory usage associated with ssl_bump and
 broken clients
In-Reply-To: <8727f26a-165a-80e8-ac9b-71f013ced2ce@opendium.com>
References: <8727f26a-165a-80e8-ac9b-71f013ced2ce@opendium.com>
Message-ID: <e0c2d11b-5544-7f27-7691-4ac9f69a0486@treenet.co.nz>

On 09/09/17 04:37, Steve Hill wrote:
> 
> I've identified a problem with Squid 3.5.26 using a lot of memory when 
> some broken clients are on the network.? Strictly speaking this isn't 
> really Squid's fault, but it is a denial of service mechanism so I 
> wonder if Squid can help mitigate it.
> 

AFAIK every connection opened or accepted by Squid does have a timeout, 
though some of them are long. The mitigation is probably to reduce 
request_timeout (v2+) or better the request_start_timeout (v4+).


Please bring up your research on squid-dev mailing list so the guys 
working on TLS/SSL and QA can all see it.


You may also need to update the networks congestion control algorithms 
to ones that better handle RST packets.

Amos


From olivier.marchetta at outlook.com  Sat Sep  9 19:20:03 2017
From: olivier.marchetta at outlook.com (Olivier MARCHETTA)
Date: Sat, 9 Sep 2017 19:20:03 +0000
Subject: [squid-users] Http write cache
Message-ID: <AM5PR0901MB0898042F9B554A7AB8D948F3E16A0@AM5PR0901MB0898.eurprd09.prod.outlook.com>

Hello,

I recently set up a squid reverse proxy cache for Sharepoint Online with the help of Amos.

It is now accelerating all reads by caching objects in Squid.

Now I?m facing a more tricky problem : writing objects to the parent server.

In this case it?s a direct connection and so it is slow.

I am not familiar with all the options and capabilities of the http protocol, but do you know if it is possible to have an asynchronous write back to the parent server to accelerate the writes ?

Thank you.

Olivier Marchetta

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20170909/5e68f15c/attachment.htm>

From squid3 at treenet.co.nz  Sun Sep 10 03:31:45 2017
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sun, 10 Sep 2017 15:31:45 +1200
Subject: [squid-users] Http write cache
In-Reply-To: <AM5PR0901MB0898042F9B554A7AB8D948F3E16A0@AM5PR0901MB0898.eurprd09.prod.outlook.com>
References: <AM5PR0901MB0898042F9B554A7AB8D948F3E16A0@AM5PR0901MB0898.eurprd09.prod.outlook.com>
Message-ID: <3fef73d3-58e7-f30f-a453-6d75124534e9@treenet.co.nz>

On 10/09/17 07:20, Olivier MARCHETTA wrote:
> Hello,
> 
> I recently set up a squid reverse proxy cache for Sharepoint Online with 
> the help of Amos.
> 
> It is now accelerating all reads by caching objects in Squid.
> 
> Now I?m facing a more tricky problem : writing objects to the parent server.
> 
> In this case it?s a direct connection and so it is slow.
> 
> I am not familiar with all the options and capabilities of the http 
> protocol, but do you know if it is possible to have an asynchronous 
> write back to the parent server to accelerate the writes ?

No, HTTP is message oriented. The equivalent of a write is a request 
message with a payload (usually PUT or POST).

Origin servers can sometimes respond to requests with payload 
("uploads") before the request has fully arrived, but any subsequent 
network issues are guaranteed to result in data loss - so the practice 
is discouraged. It is definitely not safe for a proxy to do so 
independent of the origin server.

Amos


From olivier.marchetta at outlook.com  Sun Sep 10 09:14:17 2017
From: olivier.marchetta at outlook.com (Olivier MARCHETTA)
Date: Sun, 10 Sep 2017 09:14:17 +0000
Subject: [squid-users] Http write cache
In-Reply-To: <3fef73d3-58e7-f30f-a453-6d75124534e9@treenet.co.nz>
References: <AM5PR0901MB0898042F9B554A7AB8D948F3E16A0@AM5PR0901MB0898.eurprd09.prod.outlook.com>
 <3fef73d3-58e7-f30f-a453-6d75124534e9@treenet.co.nz>
Message-ID: <AM5PR0901MB0898486A6C4A6ABD3F5B8A44E16B0@AM5PR0901MB0898.eurprd09.prod.outlook.com>

Hello,

>Origin servers can sometimes respond to requests with payload ("uploads") before the request has fully arrived, but any subsequent network issues are guaranteed to result in data loss - so the practice is discouraged.

If I understand, when it's a download (GET), Squid will replace the payload with the object in cache, if fresh.
But the HTTP control messages are still coming from the Origin server.
In case of an upload (PUT), it won't accelerate to use the Squid cache,
because the client has to wait for the Origin server's response of the payload transfer (or request).

The only option to make uploads faster is if the Origin server is aware that the client is using a reverse proxy cache and respond to the upload request before the full payload transfer. 

Tell me if I'm wrong, but I think that I understand now.
Meaning that if I want to "bufferize" the writes it has to happen with another protocol before the WebDAV connection to Sharepoint Online.

Thank you.
Olivier MARCHETTA




-----Original Message-----
From: squid-users [mailto:squid-users-bounces at lists.squid-cache.org] On Behalf Of Amos Jeffries
Sent: Sunday, September 10, 2017 4:32 AM
To: squid-users at lists.squid-cache.org
Subject: Re: [squid-users] Http write cache

On 10/09/17 07:20, Olivier MARCHETTA wrote:
> Hello,
> 
> I recently set up a squid reverse proxy cache for Sharepoint Online 
> with the help of Amos.
> 
> It is now accelerating all reads by caching objects in Squid.
> 
> Now I?m facing a more tricky problem : writing objects to the parent server.
> 
> In this case it?s a direct connection and so it is slow.
> 
> I am not familiar with all the options and capabilities of the http 
> protocol, but do you know if it is possible to have an asynchronous 
> write back to the parent server to accelerate the writes ?

No, HTTP is message oriented. The equivalent of a write is a request message with a payload (usually PUT or POST).

Origin servers can sometimes respond to requests with payload
("uploads") before the request has fully arrived, but any subsequent network issues are guaranteed to result in data loss - so the practice is discouraged. It is definitely not safe for a proxy to do so independent of the origin server.

Amos
_______________________________________________
squid-users mailing list
squid-users at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-users

From squid3 at treenet.co.nz  Sun Sep 10 17:25:18 2017
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Mon, 11 Sep 2017 05:25:18 +1200
Subject: [squid-users] Http write cache
In-Reply-To: <AM5PR0901MB0898486A6C4A6ABD3F5B8A44E16B0@AM5PR0901MB0898.eurprd09.prod.outlook.com>
References: <AM5PR0901MB0898042F9B554A7AB8D948F3E16A0@AM5PR0901MB0898.eurprd09.prod.outlook.com>
 <3fef73d3-58e7-f30f-a453-6d75124534e9@treenet.co.nz>
 <AM5PR0901MB0898486A6C4A6ABD3F5B8A44E16B0@AM5PR0901MB0898.eurprd09.prod.outlook.com>
Message-ID: <567dd5f9-f932-b8c7-5a2c-f6d86861cfb9@treenet.co.nz>

On 10/09/17 21:14, Olivier MARCHETTA wrote:
> Hello,
> 
>> Origin servers can sometimes respond to requests with payload ("uploads") before the request has fully arrived, but any subsequent network issues are guaranteed to result in data loss - so the practice is discouraged.
> 
> If I understand, when it's a download (GET), Squid will replace the payload with the object in cache, if fresh.

Nod. This is possible because two identical requests

> But the HTTP control messages are still coming from the Origin server.

Not necessarily. There are no "control messages" as such in HTTP. The 
cache controls are delivered along with the cached payload to indicate 
what can be done with it. Synchronous server contact (aka revalidation) 
to deliver responses is only required if those controls say so.


> In case of an upload (PUT), it won't accelerate to use the Squid cache,
> because the client has to wait for the Origin server's response of the payload transfer (or request).

Yes. Squid has never seen the request before, so has no idea what 
response will appear as a result.

> 
> The only option to make uploads faster is if the Origin server is aware that the client is using a reverse proxy cache and respond to the upload request before the full payload transfer.
> 

Close, bit not quite. The server does not need to know about the proxy, 
it just has to know the upload payload is "pointless waste of bandwidth" 
  (where data loss don't matter) and deliver its response early.

For example; this is usually seen with NTLM authentication, where 
uploads without credentials are denied early. Because the upload has to 
be repeated in full with the right credentials and all the bytes from 
the first attempt can be dropped in-transit by the proxy.


> Tell me if I'm wrong, but I think that I understand now.
> Meaning that if I want to "bufferize" the writes it has to happen with another protocol before the WebDAV connection to Sharepoint Online.
> 

The "other protocol" is WebDAV as far as I know. HTTP is just about 
delivery of some request and its corresponding response. How WebDAV 
transfers use HTTP messaging, and which parts of HTTP and WebDAV the 
client and server implement may or may not support the behaviour you want.


You are then colliding with the definition differences between "cache" 
and "buffer". Caches store *past* data for the purpose of reducing 
current/future server work, buffers store *current* data awaiting delivery.
  An upload is normally not something seen previously, so not cacheable.

Proxies and the network itself *do* buffer data along the way. But that 
in no way adds any asynchronous properties to HTTP. The client still has 
to wait for the HTTP response to be delivered back to it before it can 
consider the HTTP part of that transaction over - the "transaction" in 
this context may or may not be the full WebDAV upload+processing on the 
server.

HTTP has some mechanisms that can help improve upload behaviour and 
avoid pointless bandwidth delivery. Notably the Expect:100-continue and 
Range features and 201/202 status codes. WebDAV extensions to HTTP add 
various other things I'm not very familiar with.
  Between them they can signal to the client a server is a) contactable 
before data gets delivered, b) deliver it in small chunks to minimize 
loss, and c) that any given part has completed arrival and awaiting some 
state (ie full object arrival) and/or some async processing.


BUT, as should be obvious these are all application-logic level things 
(ie WebDAV) and require explicit support by both the endpoint 
applications on server and client for that logic to take place. The 
async properties arise from how things are done *between* HTTP 
transactions. The interactions are separate synchronous request+response 
message pairs as far as Squid and any HTTP infrastructure is concerned.

Amos


From rentorbuy at yahoo.com  Mon Sep 11 08:49:01 2017
From: rentorbuy at yahoo.com (Vieri)
Date: Mon, 11 Sep 2017 08:49:01 +0000 (UTC)
Subject: [squid-users] squid cache takes a break
In-Reply-To: <ef119e7a-6b6f-0a22-f903-a257ecb0af83@treenet.co.nz>
References: <1234530834.188010.1504892340901.ref@mail.yahoo.com>
 <1234530834.188010.1504892340901@mail.yahoo.com>
 <ef119e7a-6b6f-0a22-f903-a257ecb0af83@treenet.co.nz>
Message-ID: <500349185.1012564.1505119741831@mail.yahoo.com>


________________________________
From: Amos Jeffries <squid3 at treenet.co.nz>
>
> a) start fewer helpers at a time.
> 
> b) reduce cache_mem.
> 
> c) add concurrency support to the helpers.


So I decreased the startup, idle, cache_mem values:

# egrep 'startup=|idle=' squid.conf
external_acl_type bllookup ttl=86400 negative_ttl=86400 children-max=80 children-startup=10 children-idle=3 %URI /opt/custom/scripts/run/scripts/firewall/squid_url_lookup.pl [...]
sslcrtd_children 128 startup=10 idle=3

# grep cache_mem squid.conf
cache_mem 64 MB

I also set debug_options to "ALL,1 5,9 50,6 51,3 54,9".

As far as concurrency is concerned, I never programmed a helper to support this feature.
If it were to be done in Perl, do you know by any chance if it would require Perl6 "promises" with await/start function calls?

Currently, my "bllookup" helper is a simple Perl5 script which reads from standard input like so:

while( <STDIN> )
{
[...lookup URI in a MySQL database and reply accordingly to Squid...]
}

It does not handle the channel-ID field.

I haven't found many Squid concurrency-enabled helper examples out there.

By the way, I see that Squid defaults to IPv6 for helper communications. I suppose it wouldn't make any real difference if I tried "ipv4" with "external_acl_type".
If I don't get any new info next time Squid slows down to a crawl, I'll probably try ipv4 just for kicks.

What I still don't get is how long it takes for Squid to get back to work after I do a complete restart (after thoroughly killing all related processes, including helpers). I'm talking more than 5 minutes here...
If I ever get the same issue again, I understand that I can:

- stop squid & eventually kill all apparently stalled processes

- modify squid.conf, and decrease or comment out all *startup= and *idle= options

- start squid

At this point, I should expect Squid to be up and serving within a reasonable amount of time, even if I may get squid warnings later on asking me to increase those values.
Or maybe not, because the Linux kernel might be busy cleaning up the swap space anyway?

One last thing. I'm running squid 3.5.26. I'll try to upgrade to 3.5.27 asap.

Thanks,

Vieri


From olivier.marchetta at outlook.com  Mon Sep 11 12:50:08 2017
From: olivier.marchetta at outlook.com (Olivier MARCHETTA)
Date: Mon, 11 Sep 2017 12:50:08 +0000
Subject: [squid-users] Http write cache
In-Reply-To: <567dd5f9-f932-b8c7-5a2c-f6d86861cfb9@treenet.co.nz>
References: <AM5PR0901MB0898042F9B554A7AB8D948F3E16A0@AM5PR0901MB0898.eurprd09.prod.outlook.com>
 <3fef73d3-58e7-f30f-a453-6d75124534e9@treenet.co.nz>
 <AM5PR0901MB0898486A6C4A6ABD3F5B8A44E16B0@AM5PR0901MB0898.eurprd09.prod.outlook.com>
 <567dd5f9-f932-b8c7-5a2c-f6d86861cfb9@treenet.co.nz>
Message-ID: <AM5PR0901MB08985F32EA7A50667C617C48E1680@AM5PR0901MB0898.eurprd09.prod.outlook.com>

Thank you Amos for this enlightenment.
I really do appreciate your help.
I will stay with the reverse proxy configuration for our POC.
We need more to cache the libraries data reads than the writes at the moment.
And the next version of OneDrive client should help with the asynchronous writes.
Still, it will download from the Cloud so Squid is necessary in all cases.

Thank you.
Regards,
Olivier MARCHETTA


-----Original Message-----
From: squid-users [mailto:squid-users-bounces at lists.squid-cache.org] On Behalf Of Amos Jeffries
Sent: Sunday, September 10, 2017 6:25 PM
To: squid-users at lists.squid-cache.org
Subject: Re: [squid-users] Http write cache

On 10/09/17 21:14, Olivier MARCHETTA wrote:
> Hello,
> 
>> Origin servers can sometimes respond to requests with payload ("uploads") before the request has fully arrived, but any subsequent network issues are guaranteed to result in data loss - so the practice is discouraged.
> 
> If I understand, when it's a download (GET), Squid will replace the payload with the object in cache, if fresh.

Nod. This is possible because two identical requests

> But the HTTP control messages are still coming from the Origin server.

Not necessarily. There are no "control messages" as such in HTTP. The cache controls are delivered along with the cached payload to indicate what can be done with it. Synchronous server contact (aka revalidation) to deliver responses is only required if those controls say so.


> In case of an upload (PUT), it won't accelerate to use the Squid 
> cache, because the client has to wait for the Origin server's response of the payload transfer (or request).

Yes. Squid has never seen the request before, so has no idea what response will appear as a result.

> 
> The only option to make uploads faster is if the Origin server is aware that the client is using a reverse proxy cache and respond to the upload request before the full payload transfer.
> 

Close, bit not quite. The server does not need to know about the proxy, it just has to know the upload payload is "pointless waste of bandwidth" 
  (where data loss don't matter) and deliver its response early.

For example; this is usually seen with NTLM authentication, where uploads without credentials are denied early. Because the upload has to be repeated in full with the right credentials and all the bytes from the first attempt can be dropped in-transit by the proxy.


> Tell me if I'm wrong, but I think that I understand now.
> Meaning that if I want to "bufferize" the writes it has to happen with another protocol before the WebDAV connection to Sharepoint Online.
> 

The "other protocol" is WebDAV as far as I know. HTTP is just about delivery of some request and its corresponding response. How WebDAV transfers use HTTP messaging, and which parts of HTTP and WebDAV the client and server implement may or may not support the behaviour you want.


You are then colliding with the definition differences between "cache" 
and "buffer". Caches store *past* data for the purpose of reducing current/future server work, buffers store *current* data awaiting delivery.
  An upload is normally not something seen previously, so not cacheable.

Proxies and the network itself *do* buffer data along the way. But that in no way adds any asynchronous properties to HTTP. The client still has to wait for the HTTP response to be delivered back to it before it can consider the HTTP part of that transaction over - the "transaction" in this context may or may not be the full WebDAV upload+processing on the server.

HTTP has some mechanisms that can help improve upload behaviour and avoid pointless bandwidth delivery. Notably the Expect:100-continue and Range features and 201/202 status codes. WebDAV extensions to HTTP add various other things I'm not very familiar with.
  Between them they can signal to the client a server is a) contactable before data gets delivered, b) deliver it in small chunks to minimize loss, and c) that any given part has completed arrival and awaiting some state (ie full object arrival) and/or some async processing.


BUT, as should be obvious these are all application-logic level things (ie WebDAV) and require explicit support by both the endpoint applications on server and client for that logic to take place. The async properties arise from how things are done *between* HTTP transactions. The interactions are separate synchronous request+response message pairs as far as Squid and any HTTP infrastructure is concerned.

Amos
_______________________________________________
squid-users mailing list
squid-users at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-users

From squid3 at treenet.co.nz  Mon Sep 11 13:34:19 2017
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Tue, 12 Sep 2017 01:34:19 +1200
Subject: [squid-users] squid cache takes a break
In-Reply-To: <500349185.1012564.1505119741831@mail.yahoo.com>
References: <1234530834.188010.1504892340901.ref@mail.yahoo.com>
 <1234530834.188010.1504892340901@mail.yahoo.com>
 <ef119e7a-6b6f-0a22-f903-a257ecb0af83@treenet.co.nz>
 <500349185.1012564.1505119741831@mail.yahoo.com>
Message-ID: <1349ed09-a160-3dce-9a13-cb55e162b2a0@treenet.co.nz>

On 11/09/17 20:49, Vieri wrote:
> 
> ________________________________
> From: Amos Jeffries
>>
>> a) start fewer helpers at a time.
>>
>> b) reduce cache_mem.
>>
>> c) add concurrency support to the helpers.
> 
> 
> So I decreased the startup, idle, cache_mem values:
> 
> # egrep 'startup=|idle=' squid.conf
> external_acl_type bllookup ttl=86400 negative_ttl=86400 children-max=80 children-startup=10 children-idle=3 %URI /opt/custom/scripts/run/scripts/firewall/squid_url_lookup.pl [...]
> sslcrtd_children 128 startup=10 idle=3
> 
> # grep cache_mem squid.conf
> cache_mem 64 MB
> 
> I also set debug_options to "ALL,1 5,9 50,6 51,3 54,9".
> 
> As far as concurrency is concerned, I never programmed a helper to support this feature.
> If it were to be done in Perl, do you know by any chance if it would require Perl6 "promises" with await/start function calls?
> 

Don't know the answer to that one sorry. But ...

> Currently, my "bllookup" helper is a simple Perl5 script which reads from standard input like so:
> 
> while( <STDIN> )
> {
> [...lookup URI in a MySQL database and reply accordingly to Squid...]
> }
> 
> It does not handle the channel-ID field.

That is all it needs to do to begin with; parse off the numeric value 
from the input line and send it back as prefix on the output line. The 
helper does not need threading or anything particularly special for the 
minimal support.

> 
> I haven't found many Squid concurrency-enabled helper examples out there.
> 

Nod.

> By the way, I see that Squid defaults to IPv6 for helper communications. I suppose it wouldn't make any real difference if I tried "ipv4" with "external_acl_type".

If the helper is running at all without it, then no.

> If I don't get any new info next time Squid slows down to a crawl, I'll probably try ipv4 just for kicks.
> 
> What I still don't get is how long it takes for Squid to get back to work after I do a complete restart (after thoroughly killing all related processes, including helpers). I'm talking more than 5 minutes here...
> If I ever get the same issue again, I understand that I can:
> 
> - stop squid & eventually kill all apparently stalled processes
> 
> - modify squid.conf, and decrease or comment out all *startup= and *idle= options
> 
> - start squid
> 
> At this point, I should expect Squid to be up and serving within a reasonable amount of time, even if I may get squid warnings later on asking me to increase those values.
> Or maybe not, because the Linux kernel might be busy cleaning up the swap space anyway?

Something along those lines, though the disk cache related things can 
sometimes take a surprisingly long time to complete. It's hard to tell 
these possibilities apart without a trace of some kind to provide clues 
about what is going on during the pause.

> 
> One last thing. I'm running squid 3.5.26. I'll try to upgrade to 3.5.27 asap.
> 

Nod.

Amos


From sodhia.rohit at gmail.com  Mon Sep 11 17:50:53 2017
From: sodhia.rohit at gmail.com (Rohit Sodhia)
Date: Mon, 11 Sep 2017 13:50:53 -0400
Subject: [squid-users] Need assistance debugging Squid error: ssl_ctrd
	helpers crashing too quickly
Message-ID: <CAN1w9teczQdmfM8s-xe8H5kLgMPwyh6+cgRvJqV4owPSjRHcNw@mail.gmail.com>

I've been trying to setup a Squid box to bump SSL requests via the tutorial
on the Squid site and
https://stackoverflow.com/questions/34398484/can-i-use-squid-to-upgrade-client-tls-connections

Unfortunately, when I run it, I get the following errors in my squid logs:

Squid Cache (Version 3.5.20): Terminated abnormally.
CPU Usage: 0.031 seconds = 0.026 user + 0.005 sys
Maximum Resident Size: 71792 KB
Page faults with physical i/o: 0
2017/09/11 12:42:19 kid1| Current Directory is /
2017/09/11 12:42:19 kid1| Starting Squid Cache version 3.5.20 for
x86_64-redhat-linux-gnu...
2017/09/11 12:42:19 kid1| Service Name: squid
2017/09/11 12:42:19 kid1| Process ID 1711
2017/09/11 12:42:19 kid1| Process Roles: worker
2017/09/11 12:42:19 kid1| With 16384 file descriptors available
2017/09/11 12:42:19 kid1| Initializing IP Cache...
2017/09/11 12:42:19 kid1| DNS Socket created at [::], FD 6
2017/09/11 12:42:19 kid1| DNS Socket created at 0.0.0.0, FD 7
2017/09/11 12:42:19 kid1| Adding domain marvel.nyc.ent from /etc/resolv.conf
2017/09/11 12:42:19 kid1| Adding nameserver 172.21.20.200 from
/etc/resolv.conf
2017/09/11 12:42:19 kid1| Adding nameserver 172.21.20.201 from
/etc/resolv.conf
2017/09/11 12:42:19 kid1| Adding nameserver 172.20.102.201 from
/etc/resolv.conf
2017/09/11 12:42:19 kid1| helperOpenServers: Starting 5/32 'ssl_crtd'
processes
(ssl_crtd): Uninitialized SSL certificate database directory:
/var/lib/ssl_db. To initialize, run "ssl_crtd -c -s /var/lib/ssl_db".
(ssl_crtd): Uninitialized SSL certificate database directory:
/var/lib/ssl_db. To initialize, run "ssl_crtd -c -s /var/lib/ssl_db".
(ssl_crtd): Uninitialized SSL certificate database directory:
/var/lib/ssl_db. To initialize, run "ssl_crtd -c -s /var/lib/ssl_db".
(ssl_crtd): Uninitialized SSL certificate database directory:
/var/lib/ssl_db. To initialize, run "ssl_crtd -c -s /var/lib/ssl_db".
(ssl_crtd): Uninitialized SSL certificate database directory:
/var/lib/ssl_db. To initialize, run "ssl_crtd -c -s /var/lib/ssl_db".
2017/09/11 12:42:19 kid1| Logfile: opening log
stdio:/var/log/squid/access.log
2017/09/11 12:42:19 kid1| Local cache digest enabled; rebuild/rewrite every
3600/3600 sec
2017/09/11 12:42:19 kid1| Store logging disabled
2017/09/11 12:42:19 kid1| Swap maxSize 0 + 262144 KB, estimated 20164
objects
2017/09/11 12:42:19 kid1| Target number of buckets: 1008
2017/09/11 12:42:19 kid1| Using 8192 Store buckets
2017/09/11 12:42:19 kid1| Max Mem  size: 262144 KB
2017/09/11 12:42:19 kid1| Max Swap size: 0 KB
2017/09/11 12:42:19 kid1| Using Least Load store dir selection
2017/09/11 12:42:19 kid1| Current Directory is /
2017/09/11 12:42:19 kid1| Finished loading MIME types and icons.
2017/09/11 12:42:19 kid1| HTCP Disabled.
2017/09/11 12:42:19 kid1| Squid plugin modules loaded: 0
2017/09/11 12:42:19 kid1| Adaptation support is off.
2017/09/11 12:42:19 kid1| Accepting SSL bumped HTTP Socket connections at
local=[::]:3128 remote=[::] FD 21 flags=9
2017/09/11 12:42:19 kid1| WARNING: ssl_crtd #Hlpr1 exited
2017/09/11 12:42:19 kid1| Too few ssl_crtd processes are running (need 1/32)
2017/09/11 12:42:19 kid1| Closing HTTP port [::]:3128
2017/09/11 12:42:19 kid1| storeDirWriteCleanLogs: Starting...
2017/09/11 12:42:19 kid1|   Finished.  Wrote 0 entries.
2017/09/11 12:42:19 kid1|   Took 0.00 seconds (  0.00 entries/sec).
FATAL: The ssl_crtd helpers are crashing too rapidly, need help!

I ran the ssl_crtd command, though that didn't help. From google, it seems
other people have had this error, but I can't find a solution and hope
someone may be able to advise me.

Thank you for any assistance.
Rohit Sodhia
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20170911/099f754c/attachment.htm>

From yvoinov at gmail.com  Mon Sep 11 18:17:24 2017
From: yvoinov at gmail.com (Yuri)
Date: Tue, 12 Sep 2017 00:17:24 +0600
Subject: [squid-users] Need assistance debugging Squid error: ssl_ctrd
 helpers crashing too quickly
In-Reply-To: <CAN1w9teczQdmfM8s-xe8H5kLgMPwyh6+cgRvJqV4owPSjRHcNw@mail.gmail.com>
References: <CAN1w9teczQdmfM8s-xe8H5kLgMPwyh6+cgRvJqV4owPSjRHcNw@mail.gmail.com>
Message-ID: <465ed3a2-58b6-4ae1-b017-62c3f48b32b4@gmail.com>

It tells you what's happens.


11.09.2017 23:50, Rohit Sodhia ?????:
> (ssl_crtd): Uninitialized SSL certificate database directory:
> /var/lib/ssl_db. To initialize, run "ssl_crtd -c -s /var/lib/ssl_db".


-------------- next part --------------
A non-text attachment was scrubbed...
Name: signature.asc
Type: application/pgp-signature
Size: 473 bytes
Desc: OpenPGP digital signature
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20170912/c0027acd/attachment.sig>

From sodhia.rohit at gmail.com  Mon Sep 11 18:21:04 2017
From: sodhia.rohit at gmail.com (Rohit Sodhia)
Date: Mon, 11 Sep 2017 14:21:04 -0400
Subject: [squid-users] Need assistance debugging Squid error: ssl_ctrd
 helpers crashing too quickly
In-Reply-To: <465ed3a2-58b6-4ae1-b017-62c3f48b32b4@gmail.com>
References: <CAN1w9teczQdmfM8s-xe8H5kLgMPwyh6+cgRvJqV4owPSjRHcNw@mail.gmail.com>
 <465ed3a2-58b6-4ae1-b017-62c3f48b32b4@gmail.com>
Message-ID: <CAN1w9tfN34wFxavuAcXGOVKcAArj8jzAmXsvn-Rd1=f3_KpE4g@mail.gmail.com>

Yes, but telling me it's crashing unfortunately doesn't help me figure out
why or how to fix it. I've run the command it suggests but it doesn't help.
I'm unfortunately not an ops guy familiar with this kind of stuff; I don't
see anything on how to figure out what to do about it.

On Mon, Sep 11, 2017 at 2:17 PM, Yuri <yvoinov at gmail.com> wrote:

> It tells you what's happens.
>
>
> 11.09.2017 23:50, Rohit Sodhia ?????:
> > (ssl_crtd): Uninitialized SSL certificate database directory:
> > /var/lib/ssl_db. To initialize, run "ssl_crtd -c -s /var/lib/ssl_db".
>
>
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20170911/095623fd/attachment.htm>

From yvoinov at gmail.com  Mon Sep 11 18:22:26 2017
From: yvoinov at gmail.com (Yuri)
Date: Tue, 12 Sep 2017 00:22:26 +0600
Subject: [squid-users] Need assistance debugging Squid error: ssl_ctrd
 helpers crashing too quickly
In-Reply-To: <CAN1w9tfN34wFxavuAcXGOVKcAArj8jzAmXsvn-Rd1=f3_KpE4g@mail.gmail.com>
References: <CAN1w9teczQdmfM8s-xe8H5kLgMPwyh6+cgRvJqV4owPSjRHcNw@mail.gmail.com>
 <465ed3a2-58b6-4ae1-b017-62c3f48b32b4@gmail.com>
 <CAN1w9tfN34wFxavuAcXGOVKcAArj8jzAmXsvn-Rd1=f3_KpE4g@mail.gmail.com>
Message-ID: <3e79dfa3-d2b6-b742-cc9f-d72a6ae94d8b@gmail.com>

Show output of

ls -al /var/lib/ssl_db


12.09.2017 0:21, Rohit Sodhia ?????:
> Yes, but telling me it's crashing unfortunately doesn't help me figure
> out why or how to fix it. I've run the command it suggests but it
> doesn't help. I'm unfortunately not an ops guy familiar with this kind
> of stuff; I don't see anything on how to figure out what to do about it.
>
> On Mon, Sep 11, 2017 at 2:17 PM, Yuri <yvoinov at gmail.com
> <mailto:yvoinov at gmail.com>> wrote:
>
>     It tells you what's happens.
>
>
>     11.09.2017 23:50, Rohit Sodhia ?????:
>     > (ssl_crtd): Uninitialized SSL certificate database directory:
>     > /var/lib/ssl_db. To initialize, run "ssl_crtd -c -s
>     /var/lib/ssl_db".
>
>
>
>     _______________________________________________
>     squid-users mailing list
>     squid-users at lists.squid-cache.org
>     <mailto:squid-users at lists.squid-cache.org>
>     http://lists.squid-cache.org/listinfo/squid-users
>     <http://lists.squid-cache.org/listinfo/squid-users>
>
>

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20170912/59c69b69/attachment.htm>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: signature.asc
Type: application/pgp-signature
Size: 473 bytes
Desc: OpenPGP digital signature
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20170912/59c69b69/attachment.sig>

From sodhia.rohit at gmail.com  Mon Sep 11 18:23:29 2017
From: sodhia.rohit at gmail.com (Rohit Sodhia)
Date: Mon, 11 Sep 2017 14:23:29 -0400
Subject: [squid-users] Need assistance debugging Squid error: ssl_ctrd
 helpers crashing too quickly
In-Reply-To: <3e79dfa3-d2b6-b742-cc9f-d72a6ae94d8b@gmail.com>
References: <CAN1w9teczQdmfM8s-xe8H5kLgMPwyh6+cgRvJqV4owPSjRHcNw@mail.gmail.com>
 <465ed3a2-58b6-4ae1-b017-62c3f48b32b4@gmail.com>
 <CAN1w9tfN34wFxavuAcXGOVKcAArj8jzAmXsvn-Rd1=f3_KpE4g@mail.gmail.com>
 <3e79dfa3-d2b6-b742-cc9f-d72a6ae94d8b@gmail.com>
Message-ID: <CAN1w9tchdgXXRNrb7nvf9_3i4_pjgw3oUQmDMHxsRWYA8hLLPA@mail.gmail.com>

total 8
drwxr-xr-x.  3 root root   48 Sep 11 12:42 .
drwxr-xr-x. 32 root root 4096 Sep 11 12:42 ..
drwxr-xr-x.  2 root root    6 Sep 11 12:42 certs
-rw-r--r--.  1 root root    0 Sep 11 12:42 index.txt
-rw-r--r--.  1 root root    1 Sep 11 12:42 size


On Mon, Sep 11, 2017 at 2:22 PM, Yuri <yvoinov at gmail.com> wrote:

> Show output of
>
> ls -al /var/lib/ssl_db
>
> 12.09.2017 0:21, Rohit Sodhia ?????:
>
> Yes, but telling me it's crashing unfortunately doesn't help me figure out
> why or how to fix it. I've run the command it suggests but it doesn't help.
> I'm unfortunately not an ops guy familiar with this kind of stuff; I don't
> see anything on how to figure out what to do about it.
>
> On Mon, Sep 11, 2017 at 2:17 PM, Yuri <yvoinov at gmail.com> wrote:
>
>> It tells you what's happens.
>>
>>
>> 11.09.2017 23:50, Rohit Sodhia ?????:
>> > (ssl_crtd): Uninitialized SSL certificate database directory:
>> > /var/lib/ssl_db. To initialize, run "ssl_crtd -c -s /var/lib/ssl_db".
>>
>>
>>
>> _______________________________________________
>> squid-users mailing list
>> squid-users at lists.squid-cache.org
>> http://lists.squid-cache.org/listinfo/squid-users
>>
>>
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20170911/b046172c/attachment.htm>

From yvoinov at gmail.com  Mon Sep 11 18:25:27 2017
From: yvoinov at gmail.com (Yuri)
Date: Tue, 12 Sep 2017 00:25:27 +0600
Subject: [squid-users] Need assistance debugging Squid error: ssl_ctrd
 helpers crashing too quickly
In-Reply-To: <CAN1w9tchdgXXRNrb7nvf9_3i4_pjgw3oUQmDMHxsRWYA8hLLPA@mail.gmail.com>
References: <CAN1w9teczQdmfM8s-xe8H5kLgMPwyh6+cgRvJqV4owPSjRHcNw@mail.gmail.com>
 <465ed3a2-58b6-4ae1-b017-62c3f48b32b4@gmail.com>
 <CAN1w9tfN34wFxavuAcXGOVKcAArj8jzAmXsvn-Rd1=f3_KpE4g@mail.gmail.com>
 <3e79dfa3-d2b6-b742-cc9f-d72a6ae94d8b@gmail.com>
 <CAN1w9tchdgXXRNrb7nvf9_3i4_pjgw3oUQmDMHxsRWYA8hLLPA@mail.gmail.com>
Message-ID: <0b3ad24a-1d30-d0ca-4f16-fbde772c5e21@gmail.com>

Here you root of problem.

Should be (on my setups):

# ls -al /var/lib/ssl_db
total 326
drwxr-xr-x 3 squid squid????? 5 Sep? 5 00:53 .
drwxr-xr-x 8 root? other????? 8 Sep? 5 00:53 ..
drwxr-xr-x 2 squid squid??? 454 Sep 11 23:37 certs
-rw-r--r-- 1 squid squid 280575 Sep 11 23:37 index.txt
-rw-r--r-- 1 squid squid????? 7 Sep 11 23:37 size

I.e. Squid has no access to SSL cache dir structures.


12.09.2017 0:23, Rohit Sodhia ?????:
> total 8
> drwxr-xr-x.? 3 root root?? 48 Sep 11 12:42 .
> drwxr-xr-x. 32 root root 4096 Sep 11 12:42 ..
> drwxr-xr-x.? 2 root root??? 6 Sep 11 12:42 certs
> -rw-r--r--.? 1 root root??? 0 Sep 11 12:42 index.txt
> -rw-r--r--.? 1 root root??? 1 Sep 11 12:42 size
>
>
> On Mon, Sep 11, 2017 at 2:22 PM, Yuri <yvoinov at gmail.com
> <mailto:yvoinov at gmail.com>> wrote:
>
>     Show output of
>
>     ls -al /var/lib/ssl_db
>
>
>     12.09.2017 0:21, Rohit Sodhia ?????:
>>     Yes, but telling me it's crashing unfortunately doesn't help me
>>     figure out why or how to fix it. I've run the command it suggests
>>     but it doesn't help. I'm unfortunately not an ops guy familiar
>>     with this kind of stuff; I don't see anything on how to figure
>>     out what to do about it.
>>
>>     On Mon, Sep 11, 2017 at 2:17 PM, Yuri <yvoinov at gmail.com
>>     <mailto:yvoinov at gmail.com>> wrote:
>>
>>         It tells you what's happens.
>>
>>
>>         11.09.2017 23:50, Rohit Sodhia ?????:
>>         > (ssl_crtd): Uninitialized SSL certificate database directory:
>>         > /var/lib/ssl_db. To initialize, run "ssl_crtd -c -s
>>         /var/lib/ssl_db".
>>
>>
>>
>>         _______________________________________________
>>         squid-users mailing list
>>         squid-users at lists.squid-cache.org
>>         <mailto:squid-users at lists.squid-cache.org>
>>         http://lists.squid-cache.org/listinfo/squid-users
>>         <http://lists.squid-cache.org/listinfo/squid-users>
>>
>>
>
>

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20170912/eac52107/attachment.htm>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: signature.asc
Type: application/pgp-signature
Size: 473 bytes
Desc: OpenPGP digital signature
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20170912/eac52107/attachment.sig>

From sodhia.rohit at gmail.com  Mon Sep 11 18:30:08 2017
From: sodhia.rohit at gmail.com (Rohit Sodhia)
Date: Mon, 11 Sep 2017 14:30:08 -0400
Subject: [squid-users] Need assistance debugging Squid error: ssl_ctrd
 helpers crashing too quickly
In-Reply-To: <0b3ad24a-1d30-d0ca-4f16-fbde772c5e21@gmail.com>
References: <CAN1w9teczQdmfM8s-xe8H5kLgMPwyh6+cgRvJqV4owPSjRHcNw@mail.gmail.com>
 <465ed3a2-58b6-4ae1-b017-62c3f48b32b4@gmail.com>
 <CAN1w9tfN34wFxavuAcXGOVKcAArj8jzAmXsvn-Rd1=f3_KpE4g@mail.gmail.com>
 <3e79dfa3-d2b6-b742-cc9f-d72a6ae94d8b@gmail.com>
 <CAN1w9tchdgXXRNrb7nvf9_3i4_pjgw3oUQmDMHxsRWYA8hLLPA@mail.gmail.com>
 <0b3ad24a-1d30-d0ca-4f16-fbde772c5e21@gmail.com>
Message-ID: <CAN1w9tf=HzUHEBcpwtvEeoNc4DV5iq=aMFoZKubXjwxTZq1wfQ@mail.gmail.com>

Thanks for the feedback! I just used yum (it's a CentOS 7 VB) and it set it
up like that. I changed the owner and group to squid:squid and tried
restarting squid, but still get the same errors. I thought to run the
command again, but this time it says

/usr/lib64/squid/ssl_crtd: Cannot create /var/lib/ssl_db

If this folder has incorrect permissions are there possibly other
permission issues?

On Mon, Sep 11, 2017 at 2:25 PM, Yuri <yvoinov at gmail.com> wrote:

> Here you root of problem.
>
> Should be (on my setups):
>
> # ls -al /var/lib/ssl_db
> total 326
> drwxr-xr-x 3 squid squid      5 Sep  5 00:53 .
> drwxr-xr-x 8 root  other      8 Sep  5 00:53 ..
> drwxr-xr-x 2 squid squid    454 Sep 11 23:37 certs
> -rw-r--r-- 1 squid squid 280575 Sep 11 23:37 index.txt
> -rw-r--r-- 1 squid squid      7 Sep 11 23:37 size
>
> I.e. Squid has no access to SSL cache dir structures.
>
> 12.09.2017 0:23, Rohit Sodhia ?????:
>
> total 8
> drwxr-xr-x.  3 root root   48 Sep 11 12:42 .
> drwxr-xr-x. 32 root root 4096 Sep 11 12:42 ..
> drwxr-xr-x.  2 root root    6 Sep 11 12:42 certs
> -rw-r--r--.  1 root root    0 Sep 11 12:42 index.txt
> -rw-r--r--.  1 root root    1 Sep 11 12:42 size
>
>
> On Mon, Sep 11, 2017 at 2:22 PM, Yuri <yvoinov at gmail.com> wrote:
>
>> Show output of
>>
>> ls -al /var/lib/ssl_db
>>
>> 12.09.2017 0:21, Rohit Sodhia ?????:
>>
>> Yes, but telling me it's crashing unfortunately doesn't help me figure
>> out why or how to fix it. I've run the command it suggests but it doesn't
>> help. I'm unfortunately not an ops guy familiar with this kind of stuff; I
>> don't see anything on how to figure out what to do about it.
>>
>> On Mon, Sep 11, 2017 at 2:17 PM, Yuri <yvoinov at gmail.com> wrote:
>>
>>> It tells you what's happens.
>>>
>>>
>>> 11.09.2017 23:50, Rohit Sodhia ?????:
>>> > (ssl_crtd): Uninitialized SSL certificate database directory:
>>> > /var/lib/ssl_db. To initialize, run "ssl_crtd -c -s /var/lib/ssl_db".
>>>
>>>
>>>
>>> _______________________________________________
>>> squid-users mailing list
>>> squid-users at lists.squid-cache.org
>>> http://lists.squid-cache.org/listinfo/squid-users
>>>
>>>
>>
>>
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20170911/a2552d96/attachment.htm>

From yvoinov at gmail.com  Mon Sep 11 18:33:26 2017
From: yvoinov at gmail.com (Yuri)
Date: Tue, 12 Sep 2017 00:33:26 +0600
Subject: [squid-users] Need assistance debugging Squid error: ssl_ctrd
 helpers crashing too quickly
In-Reply-To: <CAN1w9tf=HzUHEBcpwtvEeoNc4DV5iq=aMFoZKubXjwxTZq1wfQ@mail.gmail.com>
References: <CAN1w9teczQdmfM8s-xe8H5kLgMPwyh6+cgRvJqV4owPSjRHcNw@mail.gmail.com>
 <465ed3a2-58b6-4ae1-b017-62c3f48b32b4@gmail.com>
 <CAN1w9tfN34wFxavuAcXGOVKcAArj8jzAmXsvn-Rd1=f3_KpE4g@mail.gmail.com>
 <3e79dfa3-d2b6-b742-cc9f-d72a6ae94d8b@gmail.com>
 <CAN1w9tchdgXXRNrb7nvf9_3i4_pjgw3oUQmDMHxsRWYA8hLLPA@mail.gmail.com>
 <0b3ad24a-1d30-d0ca-4f16-fbde772c5e21@gmail.com>
 <CAN1w9tf=HzUHEBcpwtvEeoNc4DV5iq=aMFoZKubXjwxTZq1wfQ@mail.gmail.com>
Message-ID: <3a026389-5c8b-ca43-f909-a8e46a5b1faa@gmail.com>

Most probably you squid runs as another user than squid.

Check your squid.conf for cache_effective_user and cache_effective_group
values.

Then change SSL cache permissions to this values. Should work.


12.09.2017 0:30, Rohit Sodhia ?????:
> Thanks for the feedback! I just used yum (it's a CentOS 7 VB) and it
> set it up like that. I changed the owner and group to squid:squid and
> tried restarting squid, but still get the same errors. I thought to
> run the command again, but this time it says
>
> /usr/lib64/squid/ssl_crtd: Cannot create /var/lib/ssl_db
>
> If this folder has incorrect permissions are there possibly other
> permission issues?
>
> On Mon, Sep 11, 2017 at 2:25 PM, Yuri <yvoinov at gmail.com
> <mailto:yvoinov at gmail.com>> wrote:
>
>     Here you root of problem.
>
>     Should be (on my setups):
>
>     # ls -al /var/lib/ssl_db
>     total 326
>     drwxr-xr-x 3 squid squid????? 5 Sep? 5 00:53 .
>     drwxr-xr-x 8 root? other????? 8 Sep? 5 00:53 ..
>     drwxr-xr-x 2 squid squid??? 454 Sep 11 23:37 certs
>     -rw-r--r-- 1 squid squid 280575 Sep 11 23:37 index.txt
>     -rw-r--r-- 1 squid squid????? 7 Sep 11 23:37 size
>
>     I.e. Squid has no access to SSL cache dir structures.
>
>
>     12.09.2017 0:23, Rohit Sodhia ?????:
>>     total 8
>>     drwxr-xr-x.? 3 root root?? 48 Sep 11 12:42 .
>>     drwxr-xr-x. 32 root root 4096 Sep 11 12:42 ..
>>     drwxr-xr-x.? 2 root root??? 6 Sep 11 12:42 certs
>>     -rw-r--r--.? 1 root root??? 0 Sep 11 12:42 index.txt
>>     -rw-r--r--.? 1 root root??? 1 Sep 11 12:42 size
>>
>>
>>     On Mon, Sep 11, 2017 at 2:22 PM, Yuri <yvoinov at gmail.com
>>     <mailto:yvoinov at gmail.com>> wrote:
>>
>>         Show output of
>>
>>         ls -al /var/lib/ssl_db
>>
>>
>>         12.09.2017 0:21, Rohit Sodhia ?????:
>>>         Yes, but telling me it's crashing unfortunately doesn't help
>>>         me figure out why or how to fix it. I've run the command it
>>>         suggests but it doesn't help. I'm unfortunately not an ops
>>>         guy familiar with this kind of stuff; I don't see anything
>>>         on how to figure out what to do about it.
>>>
>>>         On Mon, Sep 11, 2017 at 2:17 PM, Yuri <yvoinov at gmail.com
>>>         <mailto:yvoinov at gmail.com>> wrote:
>>>
>>>             It tells you what's happens.
>>>
>>>
>>>             11.09.2017 23:50, Rohit Sodhia ?????:
>>>             > (ssl_crtd): Uninitialized SSL certificate database
>>>             directory:
>>>             > /var/lib/ssl_db. To initialize, run "ssl_crtd -c -s
>>>             /var/lib/ssl_db".
>>>
>>>
>>>
>>>             _______________________________________________
>>>             squid-users mailing list
>>>             squid-users at lists.squid-cache.org
>>>             <mailto:squid-users at lists.squid-cache.org>
>>>             http://lists.squid-cache.org/listinfo/squid-users
>>>             <http://lists.squid-cache.org/listinfo/squid-users>
>>>
>>>
>>
>>
>
>

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20170912/7d98c51a/attachment.htm>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: signature.asc
Type: application/pgp-signature
Size: 473 bytes
Desc: OpenPGP digital signature
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20170912/7d98c51a/attachment.sig>

From sodhia.rohit at gmail.com  Mon Sep 11 18:36:02 2017
From: sodhia.rohit at gmail.com (Rohit Sodhia)
Date: Mon, 11 Sep 2017 14:36:02 -0400
Subject: [squid-users] Need assistance debugging Squid error: ssl_ctrd
 helpers crashing too quickly
In-Reply-To: <3a026389-5c8b-ca43-f909-a8e46a5b1faa@gmail.com>
References: <CAN1w9teczQdmfM8s-xe8H5kLgMPwyh6+cgRvJqV4owPSjRHcNw@mail.gmail.com>
 <465ed3a2-58b6-4ae1-b017-62c3f48b32b4@gmail.com>
 <CAN1w9tfN34wFxavuAcXGOVKcAArj8jzAmXsvn-Rd1=f3_KpE4g@mail.gmail.com>
 <3e79dfa3-d2b6-b742-cc9f-d72a6ae94d8b@gmail.com>
 <CAN1w9tchdgXXRNrb7nvf9_3i4_pjgw3oUQmDMHxsRWYA8hLLPA@mail.gmail.com>
 <0b3ad24a-1d30-d0ca-4f16-fbde772c5e21@gmail.com>
 <CAN1w9tf=HzUHEBcpwtvEeoNc4DV5iq=aMFoZKubXjwxTZq1wfQ@mail.gmail.com>
 <3a026389-5c8b-ca43-f909-a8e46a5b1faa@gmail.com>
Message-ID: <CAN1w9tf0+h6W_T4kn49_70rexuQ6=Wy9hgTbE2mTPANE-hh3oQ@mail.gmail.com>

Neither of those values are set in my config. Even though I'm not using
squid for caching, I need those values? They aren't set in the default
configs either.

On Mon, Sep 11, 2017 at 2:33 PM, Yuri <yvoinov at gmail.com> wrote:

> Most probably you squid runs as another user than squid.
>
> Check your squid.conf for cache_effective_user and cache_effective_group
> values.
>
> Then change SSL cache permissions to this values. Should work.
>
> 12.09.2017 0:30, Rohit Sodhia ?????:
>
> Thanks for the feedback! I just used yum (it's a CentOS 7 VB) and it set
> it up like that. I changed the owner and group to squid:squid and tried
> restarting squid, but still get the same errors. I thought to run the
> command again, but this time it says
>
> /usr/lib64/squid/ssl_crtd: Cannot create /var/lib/ssl_db
>
> If this folder has incorrect permissions are there possibly other
> permission issues?
>
> On Mon, Sep 11, 2017 at 2:25 PM, Yuri <yvoinov at gmail.com> wrote:
>
>> Here you root of problem.
>>
>> Should be (on my setups):
>>
>> # ls -al /var/lib/ssl_db
>> total 326
>> drwxr-xr-x 3 squid squid      5 Sep  5 00:53 .
>> drwxr-xr-x 8 root  other      8 Sep  5 00:53 ..
>> drwxr-xr-x 2 squid squid    454 Sep 11 23:37 certs
>> -rw-r--r-- 1 squid squid 280575 Sep 11 23:37 index.txt
>> -rw-r--r-- 1 squid squid      7 Sep 11 23:37 size
>>
>> I.e. Squid has no access to SSL cache dir structures.
>>
>> 12.09.2017 0:23, Rohit Sodhia ?????:
>>
>> total 8
>> drwxr-xr-x.  3 root root   48 Sep 11 12:42 .
>> drwxr-xr-x. 32 root root 4096 Sep 11 12:42 ..
>> drwxr-xr-x.  2 root root    6 Sep 11 12:42 certs
>> -rw-r--r--.  1 root root    0 Sep 11 12:42 index.txt
>> -rw-r--r--.  1 root root    1 Sep 11 12:42 size
>>
>>
>> On Mon, Sep 11, 2017 at 2:22 PM, Yuri <yvoinov at gmail.com> wrote:
>>
>>> Show output of
>>>
>>> ls -al /var/lib/ssl_db
>>>
>>> 12.09.2017 0:21, Rohit Sodhia ?????:
>>>
>>> Yes, but telling me it's crashing unfortunately doesn't help me figure
>>> out why or how to fix it. I've run the command it suggests but it doesn't
>>> help. I'm unfortunately not an ops guy familiar with this kind of stuff; I
>>> don't see anything on how to figure out what to do about it.
>>>
>>> On Mon, Sep 11, 2017 at 2:17 PM, Yuri <yvoinov at gmail.com> wrote:
>>>
>>>> It tells you what's happens.
>>>>
>>>>
>>>> 11.09.2017 23:50, Rohit Sodhia ?????:
>>>> > (ssl_crtd): Uninitialized SSL certificate database directory:
>>>> > /var/lib/ssl_db. To initialize, run "ssl_crtd -c -s /var/lib/ssl_db".
>>>>
>>>>
>>>>
>>>> _______________________________________________
>>>> squid-users mailing list
>>>> squid-users at lists.squid-cache.org
>>>> http://lists.squid-cache.org/listinfo/squid-users
>>>>
>>>>
>>>
>>>
>>
>>
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20170911/6dabf25a/attachment.htm>

From yvoinov at gmail.com  Mon Sep 11 18:39:21 2017
From: yvoinov at gmail.com (Yuri)
Date: Tue, 12 Sep 2017 00:39:21 +0600
Subject: [squid-users] Need assistance debugging Squid error: ssl_ctrd
 helpers crashing too quickly
In-Reply-To: <CAN1w9tf0+h6W_T4kn49_70rexuQ6=Wy9hgTbE2mTPANE-hh3oQ@mail.gmail.com>
References: <CAN1w9teczQdmfM8s-xe8H5kLgMPwyh6+cgRvJqV4owPSjRHcNw@mail.gmail.com>
 <465ed3a2-58b6-4ae1-b017-62c3f48b32b4@gmail.com>
 <CAN1w9tfN34wFxavuAcXGOVKcAArj8jzAmXsvn-Rd1=f3_KpE4g@mail.gmail.com>
 <3e79dfa3-d2b6-b742-cc9f-d72a6ae94d8b@gmail.com>
 <CAN1w9tchdgXXRNrb7nvf9_3i4_pjgw3oUQmDMHxsRWYA8hLLPA@mail.gmail.com>
 <0b3ad24a-1d30-d0ca-4f16-fbde772c5e21@gmail.com>
 <CAN1w9tf=HzUHEBcpwtvEeoNc4DV5iq=aMFoZKubXjwxTZq1wfQ@mail.gmail.com>
 <3a026389-5c8b-ca43-f909-a8e46a5b1faa@gmail.com>
 <CAN1w9tf0+h6W_T4kn49_70rexuQ6=Wy9hgTbE2mTPANE-hh3oQ@mail.gmail.com>
Message-ID: <ea4740ba-1706-df39-32c8-120a9740181e@gmail.com>

I'm not Linux fanboy, but modern squid never runs as root. So, most
probably it runs as nobody user.

Ah, yes:

#? TAG: cache_effective_user
#??? If you start Squid as root, it will change its effective/real
#??? UID/GID to the user specified below.? The default is to change
#??? to UID of nobody.
#??? see also; cache_effective_group
#Default:
# cache_effective_user nobody

#? TAG: cache_effective_group
#??? Squid sets the GID to the effective user's default group ID
#??? (taken from the password file) and supplementary group list
#??? from the groups membership.
#
#??? If you want Squid to run with a specific GID regardless of
#??? the group memberships of the effective user then set this
#??? to the group (or GID) you want Squid to run as. When set
#??? all other group privileges of the effective user are ignored
#??? and only this GID is effective. If Squid is not started as
#??? root the user starting Squid MUST be member of the specified
#??? group.
#
#??? This option is not recommended by the Squid Team.
#??? Our preference is for administrators to configure a secure
#??? user account for squid with UID/GID matching system policies.
#Default:
# Use system group memberships of the cache_effective_user account

As documented. :)

AFAIK best solution is create non-privileged group & user (like
squid/squid) and set both this parameters explicity.

Then change owner recursively on SSL cache to this user.


12.09.2017 0:36, Rohit Sodhia ?????:
> Neither of those values are set in my config. Even though I'm not
> using squid for caching, I need those values? They aren't set in the
> default configs either.
>
> On Mon, Sep 11, 2017 at 2:33 PM, Yuri <yvoinov at gmail.com
> <mailto:yvoinov at gmail.com>> wrote:
>
>     Most probably you squid runs as another user than squid.
>
>     Check your squid.conf for cache_effective_user and
>     cache_effective_group values.
>
>     Then change SSL cache permissions to this values. Should work.
>
>
>     12.09.2017 0:30, Rohit Sodhia ?????:
>>     Thanks for the feedback! I just used yum (it's a CentOS 7 VB) and
>>     it set it up like that. I changed the owner and group to
>>     squid:squid and tried restarting squid, but still get the same
>>     errors. I thought to run the command again, but this time it says
>>
>>     /usr/lib64/squid/ssl_crtd: Cannot create /var/lib/ssl_db
>>
>>     If this folder has incorrect permissions are there possibly other
>>     permission issues?
>>
>>     On Mon, Sep 11, 2017 at 2:25 PM, Yuri <yvoinov at gmail.com
>>     <mailto:yvoinov at gmail.com>> wrote:
>>
>>         Here you root of problem.
>>
>>         Should be (on my setups):
>>
>>         # ls -al /var/lib/ssl_db
>>         total 326
>>         drwxr-xr-x 3 squid squid????? 5 Sep? 5 00:53 .
>>         drwxr-xr-x 8 root? other????? 8 Sep? 5 00:53 ..
>>         drwxr-xr-x 2 squid squid??? 454 Sep 11 23:37 certs
>>         -rw-r--r-- 1 squid squid 280575 Sep 11 23:37 index.txt
>>         -rw-r--r-- 1 squid squid????? 7 Sep 11 23:37 size
>>
>>         I.e. Squid has no access to SSL cache dir structures.
>>
>>
>>         12.09.2017 0:23, Rohit Sodhia ?????:
>>>         total 8
>>>         drwxr-xr-x.? 3 root root?? 48 Sep 11 12:42 .
>>>         drwxr-xr-x. 32 root root 4096 Sep 11 12:42 ..
>>>         drwxr-xr-x.? 2 root root??? 6 Sep 11 12:42 certs
>>>         -rw-r--r--.? 1 root root??? 0 Sep 11 12:42 index.txt
>>>         -rw-r--r--.? 1 root root??? 1 Sep 11 12:42 size
>>>
>>>
>>>         On Mon, Sep 11, 2017 at 2:22 PM, Yuri <yvoinov at gmail.com
>>>         <mailto:yvoinov at gmail.com>> wrote:
>>>
>>>             Show output of
>>>
>>>             ls -al /var/lib/ssl_db
>>>
>>>
>>>             12.09.2017 0:21, Rohit Sodhia ?????:
>>>>             Yes, but telling me it's crashing unfortunately doesn't
>>>>             help me figure out why or how to fix it. I've run the
>>>>             command it suggests but it doesn't help. I'm
>>>>             unfortunately not an ops guy familiar with this kind of
>>>>             stuff; I don't see anything on how to figure out what
>>>>             to do about it.
>>>>
>>>>             On Mon, Sep 11, 2017 at 2:17 PM, Yuri
>>>>             <yvoinov at gmail.com <mailto:yvoinov at gmail.com>> wrote:
>>>>
>>>>                 It tells you what's happens.
>>>>
>>>>
>>>>                 11.09.2017 23:50, Rohit Sodhia ?????:
>>>>                 > (ssl_crtd): Uninitialized SSL certificate
>>>>                 database directory:
>>>>                 > /var/lib/ssl_db. To initialize, run "ssl_crtd -c
>>>>                 -s /var/lib/ssl_db".
>>>>
>>>>
>>>>
>>>>                 _______________________________________________
>>>>                 squid-users mailing list
>>>>                 squid-users at lists.squid-cache.org
>>>>                 <mailto:squid-users at lists.squid-cache.org>
>>>>                 http://lists.squid-cache.org/listinfo/squid-users
>>>>                 <http://lists.squid-cache.org/listinfo/squid-users>
>>>>
>>>>
>>>
>>>
>>
>>
>
>

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20170912/fb170f19/attachment.htm>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: signature.asc
Type: application/pgp-signature
Size: 473 bytes
Desc: OpenPGP digital signature
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20170912/fb170f19/attachment.sig>

From sodhia.rohit at gmail.com  Mon Sep 11 18:42:18 2017
From: sodhia.rohit at gmail.com (Rohit Sodhia)
Date: Mon, 11 Sep 2017 14:42:18 -0400
Subject: [squid-users] Need assistance debugging Squid error: ssl_ctrd
 helpers crashing too quickly
In-Reply-To: <ea4740ba-1706-df39-32c8-120a9740181e@gmail.com>
References: <CAN1w9teczQdmfM8s-xe8H5kLgMPwyh6+cgRvJqV4owPSjRHcNw@mail.gmail.com>
 <465ed3a2-58b6-4ae1-b017-62c3f48b32b4@gmail.com>
 <CAN1w9tfN34wFxavuAcXGOVKcAArj8jzAmXsvn-Rd1=f3_KpE4g@mail.gmail.com>
 <3e79dfa3-d2b6-b742-cc9f-d72a6ae94d8b@gmail.com>
 <CAN1w9tchdgXXRNrb7nvf9_3i4_pjgw3oUQmDMHxsRWYA8hLLPA@mail.gmail.com>
 <0b3ad24a-1d30-d0ca-4f16-fbde772c5e21@gmail.com>
 <CAN1w9tf=HzUHEBcpwtvEeoNc4DV5iq=aMFoZKubXjwxTZq1wfQ@mail.gmail.com>
 <3a026389-5c8b-ca43-f909-a8e46a5b1faa@gmail.com>
 <CAN1w9tf0+h6W_T4kn49_70rexuQ6=Wy9hgTbE2mTPANE-hh3oQ@mail.gmail.com>
 <ea4740ba-1706-df39-32c8-120a9740181e@gmail.com>
Message-ID: <CAN1w9teM35_=1X7CcvnMb9LtXRKW0okDBBN9aqO5-UKMdcGwaA@mail.gmail.com>

I'll try that immediately, thanks! I appreciate all your advice; hopefully
I won't have to reach out again :p

On Mon, Sep 11, 2017 at 2:39 PM, Yuri <yvoinov at gmail.com> wrote:

> I'm not Linux fanboy, but modern squid never runs as root. So, most
> probably it runs as nobody user.
>
> Ah, yes:
>
> #  TAG: cache_effective_user
> #    If you start Squid as root, it will change its effective/real
> #    UID/GID to the user specified below.  The default is to change
> #    to UID of nobody.
> #    see also; cache_effective_group
> #Default:
> # cache_effective_user nobody
>
> #  TAG: cache_effective_group
> #    Squid sets the GID to the effective user's default group ID
> #    (taken from the password file) and supplementary group list
> #    from the groups membership.
> #
> #    If you want Squid to run with a specific GID regardless of
> #    the group memberships of the effective user then set this
> #    to the group (or GID) you want Squid to run as. When set
> #    all other group privileges of the effective user are ignored
> #    and only this GID is effective. If Squid is not started as
> #    root the user starting Squid MUST be member of the specified
> #    group.
> #
> #    This option is not recommended by the Squid Team.
> #    Our preference is for administrators to configure a secure
> #    user account for squid with UID/GID matching system policies.
> #Default:
> # Use system group memberships of the cache_effective_user account
>
> As documented. :)
>
> AFAIK best solution is create non-privileged group & user (like
> squid/squid) and set both this parameters explicity.
>
> Then change owner recursively on SSL cache to this user.
>
> 12.09.2017 0:36, Rohit Sodhia ?????:
>
> Neither of those values are set in my config. Even though I'm not using
> squid for caching, I need those values? They aren't set in the default
> configs either.
>
> On Mon, Sep 11, 2017 at 2:33 PM, Yuri <yvoinov at gmail.com> wrote:
>
>> Most probably you squid runs as another user than squid.
>>
>> Check your squid.conf for cache_effective_user and cache_effective_group
>> values.
>>
>> Then change SSL cache permissions to this values. Should work.
>>
>> 12.09.2017 0:30, Rohit Sodhia ?????:
>>
>> Thanks for the feedback! I just used yum (it's a CentOS 7 VB) and it set
>> it up like that. I changed the owner and group to squid:squid and tried
>> restarting squid, but still get the same errors. I thought to run the
>> command again, but this time it says
>>
>> /usr/lib64/squid/ssl_crtd: Cannot create /var/lib/ssl_db
>>
>> If this folder has incorrect permissions are there possibly other
>> permission issues?
>>
>> On Mon, Sep 11, 2017 at 2:25 PM, Yuri <yvoinov at gmail.com> wrote:
>>
>>> Here you root of problem.
>>>
>>> Should be (on my setups):
>>>
>>> # ls -al /var/lib/ssl_db
>>> total 326
>>> drwxr-xr-x 3 squid squid      5 Sep  5 00:53 .
>>> drwxr-xr-x 8 root  other      8 Sep  5 00:53 ..
>>> drwxr-xr-x 2 squid squid    454 Sep 11 23:37 certs
>>> -rw-r--r-- 1 squid squid 280575 Sep 11 23:37 index.txt
>>> -rw-r--r-- 1 squid squid      7 Sep 11 23:37 size
>>>
>>> I.e. Squid has no access to SSL cache dir structures.
>>>
>>> 12.09.2017 0:23, Rohit Sodhia ?????:
>>>
>>> total 8
>>> drwxr-xr-x.  3 root root   48 Sep 11 12:42 .
>>> drwxr-xr-x. 32 root root 4096 Sep 11 12:42 ..
>>> drwxr-xr-x.  2 root root    6 Sep 11 12:42 certs
>>> -rw-r--r--.  1 root root    0 Sep 11 12:42 index.txt
>>> -rw-r--r--.  1 root root    1 Sep 11 12:42 size
>>>
>>>
>>> On Mon, Sep 11, 2017 at 2:22 PM, Yuri <yvoinov at gmail.com> wrote:
>>>
>>>> Show output of
>>>>
>>>> ls -al /var/lib/ssl_db
>>>>
>>>> 12.09.2017 0:21, Rohit Sodhia ?????:
>>>>
>>>> Yes, but telling me it's crashing unfortunately doesn't help me figure
>>>> out why or how to fix it. I've run the command it suggests but it doesn't
>>>> help. I'm unfortunately not an ops guy familiar with this kind of stuff; I
>>>> don't see anything on how to figure out what to do about it.
>>>>
>>>> On Mon, Sep 11, 2017 at 2:17 PM, Yuri <yvoinov at gmail.com> wrote:
>>>>
>>>>> It tells you what's happens.
>>>>>
>>>>>
>>>>> 11.09.2017 23:50, Rohit Sodhia ?????:
>>>>> > (ssl_crtd): Uninitialized SSL certificate database directory:
>>>>> > /var/lib/ssl_db. To initialize, run "ssl_crtd -c -s /var/lib/ssl_db".
>>>>>
>>>>>
>>>>>
>>>>> _______________________________________________
>>>>> squid-users mailing list
>>>>> squid-users at lists.squid-cache.org
>>>>> http://lists.squid-cache.org/listinfo/squid-users
>>>>>
>>>>>
>>>>
>>>>
>>>
>>>
>>
>>
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20170911/298602e2/attachment.htm>

From sodhia.rohit at gmail.com  Mon Sep 11 19:23:42 2017
From: sodhia.rohit at gmail.com (Rohit Sodhia)
Date: Mon, 11 Sep 2017 15:23:42 -0400
Subject: [squid-users] Need assistance debugging Squid error: ssl_ctrd
 helpers crashing too quickly
In-Reply-To: <CAN1w9teM35_=1X7CcvnMb9LtXRKW0okDBBN9aqO5-UKMdcGwaA@mail.gmail.com>
References: <CAN1w9teczQdmfM8s-xe8H5kLgMPwyh6+cgRvJqV4owPSjRHcNw@mail.gmail.com>
 <465ed3a2-58b6-4ae1-b017-62c3f48b32b4@gmail.com>
 <CAN1w9tfN34wFxavuAcXGOVKcAArj8jzAmXsvn-Rd1=f3_KpE4g@mail.gmail.com>
 <3e79dfa3-d2b6-b742-cc9f-d72a6ae94d8b@gmail.com>
 <CAN1w9tchdgXXRNrb7nvf9_3i4_pjgw3oUQmDMHxsRWYA8hLLPA@mail.gmail.com>
 <0b3ad24a-1d30-d0ca-4f16-fbde772c5e21@gmail.com>
 <CAN1w9tf=HzUHEBcpwtvEeoNc4DV5iq=aMFoZKubXjwxTZq1wfQ@mail.gmail.com>
 <3a026389-5c8b-ca43-f909-a8e46a5b1faa@gmail.com>
 <CAN1w9tf0+h6W_T4kn49_70rexuQ6=Wy9hgTbE2mTPANE-hh3oQ@mail.gmail.com>
 <ea4740ba-1706-df39-32c8-120a9740181e@gmail.com>
 <CAN1w9teM35_=1X7CcvnMb9LtXRKW0okDBBN9aqO5-UKMdcGwaA@mail.gmail.com>
Message-ID: <CAN1w9tcuVmZnQV+4aj=ZXD=rwBeOUUaHq7xOJkoGurGeq-=nwQ@mail.gmail.com>

Unfortunately, no luck yet. Thank you again for your help before.

I found that the user squid and group squid existed already, so I added

cache_effective_user squid
cache_effective_group squid

to my config (first two lines), made sure /var/lib/ssl_db and it's contents
were set to squid:squid and restarted the service, but I'm still getting
the same error :(

On Mon, Sep 11, 2017 at 2:42 PM, Rohit Sodhia <sodhia.rohit at gmail.com>
wrote:

> I'll try that immediately, thanks! I appreciate all your advice; hopefully
> I won't have to reach out again :p
>
> On Mon, Sep 11, 2017 at 2:39 PM, Yuri <yvoinov at gmail.com> wrote:
>
>> I'm not Linux fanboy, but modern squid never runs as root. So, most
>> probably it runs as nobody user.
>>
>> Ah, yes:
>>
>> #  TAG: cache_effective_user
>> #    If you start Squid as root, it will change its effective/real
>> #    UID/GID to the user specified below.  The default is to change
>> #    to UID of nobody.
>> #    see also; cache_effective_group
>> #Default:
>> # cache_effective_user nobody
>>
>> #  TAG: cache_effective_group
>> #    Squid sets the GID to the effective user's default group ID
>> #    (taken from the password file) and supplementary group list
>> #    from the groups membership.
>> #
>> #    If you want Squid to run with a specific GID regardless of
>> #    the group memberships of the effective user then set this
>> #    to the group (or GID) you want Squid to run as. When set
>> #    all other group privileges of the effective user are ignored
>> #    and only this GID is effective. If Squid is not started as
>> #    root the user starting Squid MUST be member of the specified
>> #    group.
>> #
>> #    This option is not recommended by the Squid Team.
>> #    Our preference is for administrators to configure a secure
>> #    user account for squid with UID/GID matching system policies.
>> #Default:
>> # Use system group memberships of the cache_effective_user account
>>
>> As documented. :)
>>
>> AFAIK best solution is create non-privileged group & user (like
>> squid/squid) and set both this parameters explicity.
>>
>> Then change owner recursively on SSL cache to this user.
>>
>> 12.09.2017 0:36, Rohit Sodhia ?????:
>>
>> Neither of those values are set in my config. Even though I'm not using
>> squid for caching, I need those values? They aren't set in the default
>> configs either.
>>
>> On Mon, Sep 11, 2017 at 2:33 PM, Yuri <yvoinov at gmail.com> wrote:
>>
>>> Most probably you squid runs as another user than squid.
>>>
>>> Check your squid.conf for cache_effective_user and cache_effective_group
>>> values.
>>>
>>> Then change SSL cache permissions to this values. Should work.
>>>
>>> 12.09.2017 0:30, Rohit Sodhia ?????:
>>>
>>> Thanks for the feedback! I just used yum (it's a CentOS 7 VB) and it set
>>> it up like that. I changed the owner and group to squid:squid and tried
>>> restarting squid, but still get the same errors. I thought to run the
>>> command again, but this time it says
>>>
>>> /usr/lib64/squid/ssl_crtd: Cannot create /var/lib/ssl_db
>>>
>>> If this folder has incorrect permissions are there possibly other
>>> permission issues?
>>>
>>> On Mon, Sep 11, 2017 at 2:25 PM, Yuri <yvoinov at gmail.com> wrote:
>>>
>>>> Here you root of problem.
>>>>
>>>> Should be (on my setups):
>>>>
>>>> # ls -al /var/lib/ssl_db
>>>> total 326
>>>> drwxr-xr-x 3 squid squid      5 Sep  5 00:53 .
>>>> drwxr-xr-x 8 root  other      8 Sep  5 00:53 ..
>>>> drwxr-xr-x 2 squid squid    454 Sep 11 23:37 certs
>>>> -rw-r--r-- 1 squid squid 280575 Sep 11 23:37 index.txt
>>>> -rw-r--r-- 1 squid squid      7 Sep 11 23:37 size
>>>>
>>>> I.e. Squid has no access to SSL cache dir structures.
>>>>
>>>> 12.09.2017 0:23, Rohit Sodhia ?????:
>>>>
>>>> total 8
>>>> drwxr-xr-x.  3 root root   48 Sep 11 12:42 .
>>>> drwxr-xr-x. 32 root root 4096 Sep 11 12:42 ..
>>>> drwxr-xr-x.  2 root root    6 Sep 11 12:42 certs
>>>> -rw-r--r--.  1 root root    0 Sep 11 12:42 index.txt
>>>> -rw-r--r--.  1 root root    1 Sep 11 12:42 size
>>>>
>>>>
>>>> On Mon, Sep 11, 2017 at 2:22 PM, Yuri <yvoinov at gmail.com> wrote:
>>>>
>>>>> Show output of
>>>>>
>>>>> ls -al /var/lib/ssl_db
>>>>>
>>>>> 12.09.2017 0:21, Rohit Sodhia ?????:
>>>>>
>>>>> Yes, but telling me it's crashing unfortunately doesn't help me figure
>>>>> out why or how to fix it. I've run the command it suggests but it doesn't
>>>>> help. I'm unfortunately not an ops guy familiar with this kind of stuff; I
>>>>> don't see anything on how to figure out what to do about it.
>>>>>
>>>>> On Mon, Sep 11, 2017 at 2:17 PM, Yuri <yvoinov at gmail.com> wrote:
>>>>>
>>>>>> It tells you what's happens.
>>>>>>
>>>>>>
>>>>>> 11.09.2017 23:50, Rohit Sodhia ?????:
>>>>>> > (ssl_crtd): Uninitialized SSL certificate database directory:
>>>>>> > /var/lib/ssl_db. To initialize, run "ssl_crtd -c -s
>>>>>> /var/lib/ssl_db".
>>>>>>
>>>>>>
>>>>>>
>>>>>> _______________________________________________
>>>>>> squid-users mailing list
>>>>>> squid-users at lists.squid-cache.org
>>>>>> http://lists.squid-cache.org/listinfo/squid-users
>>>>>>
>>>>>>
>>>>>
>>>>>
>>>>
>>>>
>>>
>>>
>>
>>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20170911/e227a97f/attachment.htm>

From yvoinov at gmail.com  Mon Sep 11 19:41:32 2017
From: yvoinov at gmail.com (Yuri)
Date: Tue, 12 Sep 2017 01:41:32 +0600
Subject: [squid-users] Need assistance debugging Squid error: ssl_ctrd
 helpers crashing too quickly
In-Reply-To: <CAN1w9tcuVmZnQV+4aj=ZXD=rwBeOUUaHq7xOJkoGurGeq-=nwQ@mail.gmail.com>
References: <CAN1w9teczQdmfM8s-xe8H5kLgMPwyh6+cgRvJqV4owPSjRHcNw@mail.gmail.com>
 <465ed3a2-58b6-4ae1-b017-62c3f48b32b4@gmail.com>
 <CAN1w9tfN34wFxavuAcXGOVKcAArj8jzAmXsvn-Rd1=f3_KpE4g@mail.gmail.com>
 <3e79dfa3-d2b6-b742-cc9f-d72a6ae94d8b@gmail.com>
 <CAN1w9tchdgXXRNrb7nvf9_3i4_pjgw3oUQmDMHxsRWYA8hLLPA@mail.gmail.com>
 <0b3ad24a-1d30-d0ca-4f16-fbde772c5e21@gmail.com>
 <CAN1w9tf=HzUHEBcpwtvEeoNc4DV5iq=aMFoZKubXjwxTZq1wfQ@mail.gmail.com>
 <3a026389-5c8b-ca43-f909-a8e46a5b1faa@gmail.com>
 <CAN1w9tf0+h6W_T4kn49_70rexuQ6=Wy9hgTbE2mTPANE-hh3oQ@mail.gmail.com>
 <ea4740ba-1706-df39-32c8-120a9740181e@gmail.com>
 <CAN1w9teM35_=1X7CcvnMb9LtXRKW0okDBBN9aqO5-UKMdcGwaA@mail.gmail.com>
 <CAN1w9tcuVmZnQV+4aj=ZXD=rwBeOUUaHq7xOJkoGurGeq-=nwQ@mail.gmail.com>
Message-ID: <75a12def-048c-7519-5b9a-fc767579285d@gmail.com>

Well. Let's check more deep.

Show me parameter sslcrtd_program in your squid.conf


12.09.2017 1:23, Rohit Sodhia ?????:
> Unfortunately, no luck yet. Thank you again for your help before.
>
> I found that the user squid and group squid existed already, so I added
>
> cache_effective_user squid
> cache_effective_group squid
>
> to my config (first two lines), made sure /var/lib/ssl_db and it's
> contents were set to squid:squid and restarted the service, but I'm
> still getting the same error :(
>
> On Mon, Sep 11, 2017 at 2:42 PM, Rohit Sodhia <sodhia.rohit at gmail.com
> <mailto:sodhia.rohit at gmail.com>> wrote:
>
>     I'll try that immediately, thanks! I appreciate all your advice;
>     hopefully I won't have to reach out again :p
>
>     On Mon, Sep 11, 2017 at 2:39 PM, Yuri <yvoinov at gmail.com
>     <mailto:yvoinov at gmail.com>> wrote:
>
>         I'm not Linux fanboy, but modern squid never runs as root. So,
>         most probably it runs as nobody user.
>
>         Ah, yes:
>
>         #? TAG: cache_effective_user
>         #??? If you start Squid as root, it will change its effective/real
>         #??? UID/GID to the user specified below.? The default is to
>         change
>         #??? to UID of nobody.
>         #??? see also; cache_effective_group
>         #Default:
>         # cache_effective_user nobody
>
>         #? TAG: cache_effective_group
>         #??? Squid sets the GID to the effective user's default group ID
>         #??? (taken from the password file) and supplementary group list
>         #??? from the groups membership.
>         #
>         #??? If you want Squid to run with a specific GID regardless of
>         #??? the group memberships of the effective user then set this
>         #??? to the group (or GID) you want Squid to run as. When set
>         #??? all other group privileges of the effective user are ignored
>         #??? and only this GID is effective. If Squid is not started as
>         #??? root the user starting Squid MUST be member of the specified
>         #??? group.
>         #
>         #??? This option is not recommended by the Squid Team.
>         #??? Our preference is for administrators to configure a secure
>         #??? user account for squid with UID/GID matching system policies.
>         #Default:
>         # Use system group memberships of the cache_effective_user account
>
>         As documented. :)
>
>         AFAIK best solution is create non-privileged group & user
>         (like squid/squid) and set both this parameters explicity.
>
>         Then change owner recursively on SSL cache to this user.
>
>
>         12.09.2017 0:36, Rohit Sodhia ?????:
>>         Neither of those values are set in my config. Even though I'm
>>         not using squid for caching, I need those values? They aren't
>>         set in the default configs either.
>>
>>         On Mon, Sep 11, 2017 at 2:33 PM, Yuri <yvoinov at gmail.com
>>         <mailto:yvoinov at gmail.com>> wrote:
>>
>>             Most probably you squid runs as another user than squid.
>>
>>             Check your squid.conf for cache_effective_user and
>>             cache_effective_group values.
>>
>>             Then change SSL cache permissions to this values. Should
>>             work.
>>
>>
>>             12.09.2017 0:30, Rohit Sodhia ?????:
>>>             Thanks for the feedback! I just used yum (it's a CentOS
>>>             7 VB) and it set it up like that. I changed the owner
>>>             and group to squid:squid and tried restarting squid, but
>>>             still get the same errors. I thought to run the command
>>>             again, but this time it says
>>>
>>>             /usr/lib64/squid/ssl_crtd: Cannot create /var/lib/ssl_db
>>>
>>>             If this folder has incorrect permissions are there
>>>             possibly other permission issues?
>>>
>>>             On Mon, Sep 11, 2017 at 2:25 PM, Yuri <yvoinov at gmail.com
>>>             <mailto:yvoinov at gmail.com>> wrote:
>>>
>>>                 Here you root of problem.
>>>
>>>                 Should be (on my setups):
>>>
>>>                 # ls -al /var/lib/ssl_db
>>>                 total 326
>>>                 drwxr-xr-x 3 squid squid????? 5 Sep? 5 00:53 .
>>>                 drwxr-xr-x 8 root? other????? 8 Sep? 5 00:53 ..
>>>                 drwxr-xr-x 2 squid squid??? 454 Sep 11 23:37 certs
>>>                 -rw-r--r-- 1 squid squid 280575 Sep 11 23:37 index.txt
>>>                 -rw-r--r-- 1 squid squid????? 7 Sep 11 23:37 size
>>>
>>>                 I.e. Squid has no access to SSL cache dir structures.
>>>
>>>
>>>                 12.09.2017 0:23, Rohit Sodhia ?????:
>>>>                 total 8
>>>>                 drwxr-xr-x.? 3 root root?? 48 Sep 11 12:42 .
>>>>                 drwxr-xr-x. 32 root root 4096 Sep 11 12:42 ..
>>>>                 drwxr-xr-x.? 2 root root??? 6 Sep 11 12:42 certs
>>>>                 -rw-r--r--.? 1 root root??? 0 Sep 11 12:42 index.txt
>>>>                 -rw-r--r--.? 1 root root??? 1 Sep 11 12:42 size
>>>>
>>>>
>>>>                 On Mon, Sep 11, 2017 at 2:22 PM, Yuri
>>>>                 <yvoinov at gmail.com <mailto:yvoinov at gmail.com>> wrote:
>>>>
>>>>                     Show output of
>>>>
>>>>                     ls -al /var/lib/ssl_db
>>>>
>>>>
>>>>                     12.09.2017 0:21, Rohit Sodhia ?????:
>>>>>                     Yes, but telling me it's crashing
>>>>>                     unfortunately doesn't help me figure out why
>>>>>                     or how to fix it. I've run the command it
>>>>>                     suggests but it doesn't help. I'm
>>>>>                     unfortunately not an ops guy familiar with
>>>>>                     this kind of stuff; I don't see anything on
>>>>>                     how to figure out what to do about it.
>>>>>
>>>>>                     On Mon, Sep 11, 2017 at 2:17 PM, Yuri
>>>>>                     <yvoinov at gmail.com <mailto:yvoinov at gmail.com>>
>>>>>                     wrote:
>>>>>
>>>>>                         It tells you what's happens.
>>>>>
>>>>>
>>>>>                         11.09.2017 23:50, Rohit Sodhia ?????:
>>>>>                         > (ssl_crtd): Uninitialized SSL
>>>>>                         certificate database directory:
>>>>>                         > /var/lib/ssl_db. To initialize, run
>>>>>                         "ssl_crtd -c -s /var/lib/ssl_db".
>>>>>
>>>>>
>>>>>
>>>>>                         _______________________________________________
>>>>>                         squid-users mailing list
>>>>>                         squid-users at lists.squid-cache.org
>>>>>                         <mailto:squid-users at lists.squid-cache.org>
>>>>>                         http://lists.squid-cache.org/listinfo/squid-users
>>>>>                         <http://lists.squid-cache.org/listinfo/squid-users>
>>>>>
>>>>>
>>>>
>>>>
>>>
>>>
>>
>>
>
>
>

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20170912/bb380392/attachment.htm>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: signature.asc
Type: application/pgp-signature
Size: 473 bytes
Desc: OpenPGP digital signature
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20170912/bb380392/attachment.sig>

From sodhia.rohit at gmail.com  Mon Sep 11 19:58:12 2017
From: sodhia.rohit at gmail.com (Rohit Sodhia)
Date: Mon, 11 Sep 2017 15:58:12 -0400
Subject: [squid-users] Need assistance debugging Squid error: ssl_ctrd
 helpers crashing too quickly
In-Reply-To: <75a12def-048c-7519-5b9a-fc767579285d@gmail.com>
References: <CAN1w9teczQdmfM8s-xe8H5kLgMPwyh6+cgRvJqV4owPSjRHcNw@mail.gmail.com>
 <465ed3a2-58b6-4ae1-b017-62c3f48b32b4@gmail.com>
 <CAN1w9tfN34wFxavuAcXGOVKcAArj8jzAmXsvn-Rd1=f3_KpE4g@mail.gmail.com>
 <3e79dfa3-d2b6-b742-cc9f-d72a6ae94d8b@gmail.com>
 <CAN1w9tchdgXXRNrb7nvf9_3i4_pjgw3oUQmDMHxsRWYA8hLLPA@mail.gmail.com>
 <0b3ad24a-1d30-d0ca-4f16-fbde772c5e21@gmail.com>
 <CAN1w9tf=HzUHEBcpwtvEeoNc4DV5iq=aMFoZKubXjwxTZq1wfQ@mail.gmail.com>
 <3a026389-5c8b-ca43-f909-a8e46a5b1faa@gmail.com>
 <CAN1w9tf0+h6W_T4kn49_70rexuQ6=Wy9hgTbE2mTPANE-hh3oQ@mail.gmail.com>
 <ea4740ba-1706-df39-32c8-120a9740181e@gmail.com>
 <CAN1w9teM35_=1X7CcvnMb9LtXRKW0okDBBN9aqO5-UKMdcGwaA@mail.gmail.com>
 <CAN1w9tcuVmZnQV+4aj=ZXD=rwBeOUUaHq7xOJkoGurGeq-=nwQ@mail.gmail.com>
 <75a12def-048c-7519-5b9a-fc767579285d@gmail.com>
Message-ID: <CAN1w9tfm8UCSW-bVhL-fOCFZD9DzxdJzW0cXyNczbwddN681jw@mail.gmail.com>

sslcrtd_program /usr/lib64/squid/ssl_crtd -s /var/lib/ssl_db -M 4MB

I used the line from the Stack Overflow question I linked earlier.

On Mon, Sep 11, 2017 at 3:41 PM, Yuri <yvoinov at gmail.com> wrote:

> Well. Let's check more deep.
>
> Show me parameter sslcrtd_program in your squid.conf
>
> 12.09.2017 1:23, Rohit Sodhia ?????:
>
> Unfortunately, no luck yet. Thank you again for your help before.
>
> I found that the user squid and group squid existed already, so I added
>
> cache_effective_user squid
> cache_effective_group squid
>
> to my config (first two lines), made sure /var/lib/ssl_db and it's
> contents were set to squid:squid and restarted the service, but I'm still
> getting the same error :(
>
> On Mon, Sep 11, 2017 at 2:42 PM, Rohit Sodhia <sodhia.rohit at gmail.com>
> wrote:
>
>> I'll try that immediately, thanks! I appreciate all your advice;
>> hopefully I won't have to reach out again :p
>>
>> On Mon, Sep 11, 2017 at 2:39 PM, Yuri <yvoinov at gmail.com> wrote:
>>
>>> I'm not Linux fanboy, but modern squid never runs as root. So, most
>>> probably it runs as nobody user.
>>>
>>> Ah, yes:
>>>
>>> #  TAG: cache_effective_user
>>> #    If you start Squid as root, it will change its effective/real
>>> #    UID/GID to the user specified below.  The default is to change
>>> #    to UID of nobody.
>>> #    see also; cache_effective_group
>>> #Default:
>>> # cache_effective_user nobody
>>>
>>> #  TAG: cache_effective_group
>>> #    Squid sets the GID to the effective user's default group ID
>>> #    (taken from the password file) and supplementary group list
>>> #    from the groups membership.
>>> #
>>> #    If you want Squid to run with a specific GID regardless of
>>> #    the group memberships of the effective user then set this
>>> #    to the group (or GID) you want Squid to run as. When set
>>> #    all other group privileges of the effective user are ignored
>>> #    and only this GID is effective. If Squid is not started as
>>> #    root the user starting Squid MUST be member of the specified
>>> #    group.
>>> #
>>> #    This option is not recommended by the Squid Team.
>>> #    Our preference is for administrators to configure a secure
>>> #    user account for squid with UID/GID matching system policies.
>>> #Default:
>>> # Use system group memberships of the cache_effective_user account
>>>
>>> As documented. :)
>>>
>>> AFAIK best solution is create non-privileged group & user (like
>>> squid/squid) and set both this parameters explicity.
>>>
>>> Then change owner recursively on SSL cache to this user.
>>>
>>> 12.09.2017 0:36, Rohit Sodhia ?????:
>>>
>>> Neither of those values are set in my config. Even though I'm not using
>>> squid for caching, I need those values? They aren't set in the default
>>> configs either.
>>>
>>> On Mon, Sep 11, 2017 at 2:33 PM, Yuri <yvoinov at gmail.com> wrote:
>>>
>>>> Most probably you squid runs as another user than squid.
>>>>
>>>> Check your squid.conf for cache_effective_user and
>>>> cache_effective_group values.
>>>>
>>>> Then change SSL cache permissions to this values. Should work.
>>>>
>>>> 12.09.2017 0:30, Rohit Sodhia ?????:
>>>>
>>>> Thanks for the feedback! I just used yum (it's a CentOS 7 VB) and it
>>>> set it up like that. I changed the owner and group to squid:squid and tried
>>>> restarting squid, but still get the same errors. I thought to run the
>>>> command again, but this time it says
>>>>
>>>> /usr/lib64/squid/ssl_crtd: Cannot create /var/lib/ssl_db
>>>>
>>>> If this folder has incorrect permissions are there possibly other
>>>> permission issues?
>>>>
>>>> On Mon, Sep 11, 2017 at 2:25 PM, Yuri <yvoinov at gmail.com> wrote:
>>>>
>>>>> Here you root of problem.
>>>>>
>>>>> Should be (on my setups):
>>>>>
>>>>> # ls -al /var/lib/ssl_db
>>>>> total 326
>>>>> drwxr-xr-x 3 squid squid      5 Sep  5 00:53 .
>>>>> drwxr-xr-x 8 root  other      8 Sep  5 00:53 ..
>>>>> drwxr-xr-x 2 squid squid    454 Sep 11 23:37 certs
>>>>> -rw-r--r-- 1 squid squid 280575 Sep 11 23:37 index.txt
>>>>> -rw-r--r-- 1 squid squid      7 Sep 11 23:37 size
>>>>>
>>>>> I.e. Squid has no access to SSL cache dir structures.
>>>>>
>>>>> 12.09.2017 0:23, Rohit Sodhia ?????:
>>>>>
>>>>> total 8
>>>>> drwxr-xr-x.  3 root root   48 Sep 11 12:42 .
>>>>> drwxr-xr-x. 32 root root 4096 Sep 11 12:42 ..
>>>>> drwxr-xr-x.  2 root root    6 Sep 11 12:42 certs
>>>>> -rw-r--r--.  1 root root    0 Sep 11 12:42 index.txt
>>>>> -rw-r--r--.  1 root root    1 Sep 11 12:42 size
>>>>>
>>>>>
>>>>> On Mon, Sep 11, 2017 at 2:22 PM, Yuri <yvoinov at gmail.com> wrote:
>>>>>
>>>>>> Show output of
>>>>>>
>>>>>> ls -al /var/lib/ssl_db
>>>>>>
>>>>>> 12.09.2017 0:21, Rohit Sodhia ?????:
>>>>>>
>>>>>> Yes, but telling me it's crashing unfortunately doesn't help me
>>>>>> figure out why or how to fix it. I've run the command it suggests but it
>>>>>> doesn't help. I'm unfortunately not an ops guy familiar with this kind of
>>>>>> stuff; I don't see anything on how to figure out what to do about it.
>>>>>>
>>>>>> On Mon, Sep 11, 2017 at 2:17 PM, Yuri <yvoinov at gmail.com> wrote:
>>>>>>
>>>>>>> It tells you what's happens.
>>>>>>>
>>>>>>>
>>>>>>> 11.09.2017 23:50, Rohit Sodhia ?????:
>>>>>>> > (ssl_crtd): Uninitialized SSL certificate database directory:
>>>>>>> > /var/lib/ssl_db. To initialize, run "ssl_crtd -c -s
>>>>>>> /var/lib/ssl_db".
>>>>>>>
>>>>>>>
>>>>>>>
>>>>>>> _______________________________________________
>>>>>>> squid-users mailing list
>>>>>>> squid-users at lists.squid-cache.org
>>>>>>> http://lists.squid-cache.org/listinfo/squid-users
>>>>>>>
>>>>>>>
>>>>>>
>>>>>>
>>>>>
>>>>>
>>>>
>>>>
>>>
>>>
>>
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20170911/2ad68959/attachment.htm>

From yvoinov at gmail.com  Mon Sep 11 20:02:39 2017
From: yvoinov at gmail.com (Yuri)
Date: Tue, 12 Sep 2017 02:02:39 +0600
Subject: [squid-users] Need assistance debugging Squid error: ssl_ctrd
 helpers crashing too quickly
In-Reply-To: <CAN1w9tfm8UCSW-bVhL-fOCFZD9DzxdJzW0cXyNczbwddN681jw@mail.gmail.com>
References: <CAN1w9teczQdmfM8s-xe8H5kLgMPwyh6+cgRvJqV4owPSjRHcNw@mail.gmail.com>
 <465ed3a2-58b6-4ae1-b017-62c3f48b32b4@gmail.com>
 <CAN1w9tfN34wFxavuAcXGOVKcAArj8jzAmXsvn-Rd1=f3_KpE4g@mail.gmail.com>
 <3e79dfa3-d2b6-b742-cc9f-d72a6ae94d8b@gmail.com>
 <CAN1w9tchdgXXRNrb7nvf9_3i4_pjgw3oUQmDMHxsRWYA8hLLPA@mail.gmail.com>
 <0b3ad24a-1d30-d0ca-4f16-fbde772c5e21@gmail.com>
 <CAN1w9tf=HzUHEBcpwtvEeoNc4DV5iq=aMFoZKubXjwxTZq1wfQ@mail.gmail.com>
 <3a026389-5c8b-ca43-f909-a8e46a5b1faa@gmail.com>
 <CAN1w9tf0+h6W_T4kn49_70rexuQ6=Wy9hgTbE2mTPANE-hh3oQ@mail.gmail.com>
 <ea4740ba-1706-df39-32c8-120a9740181e@gmail.com>
 <CAN1w9teM35_=1X7CcvnMb9LtXRKW0okDBBN9aqO5-UKMdcGwaA@mail.gmail.com>
 <CAN1w9tcuVmZnQV+4aj=ZXD=rwBeOUUaHq7xOJkoGurGeq-=nwQ@mail.gmail.com>
 <75a12def-048c-7519-5b9a-fc767579285d@gmail.com>
 <CAN1w9tfm8UCSW-bVhL-fOCFZD9DzxdJzW0cXyNczbwddN681jw@mail.gmail.com>
Message-ID: <b55ec078-e25a-4c83-d5b2-6ea624699996@gmail.com>

Wait. Squid 3.5.20? So ancient?


12.09.2017 1:58, Rohit Sodhia ?????:
> sslcrtd_program /usr/lib64/squid/ssl_crtd -s /var/lib/ssl_db -M 4MB
>
> I used the line from the Stack Overflow question I linked earlier.
>
> On Mon, Sep 11, 2017 at 3:41 PM, Yuri <yvoinov at gmail.com
> <mailto:yvoinov at gmail.com>> wrote:
>
>     Well. Let's check more deep.
>
>     Show me parameter sslcrtd_program in your squid.conf
>
>
>     12.09.2017 1:23, Rohit Sodhia ?????:
>>     Unfortunately, no luck yet. Thank you again for your help before.
>>
>>     I found that the user squid and group squid existed already, so I
>>     added
>>
>>     cache_effective_user squid
>>     cache_effective_group squid
>>
>>     to my config (first two lines), made sure /var/lib/ssl_db and
>>     it's contents were set to squid:squid and restarted the service,
>>     but I'm still getting the same error :(
>>
>>     On Mon, Sep 11, 2017 at 2:42 PM, Rohit Sodhia
>>     <sodhia.rohit at gmail.com <mailto:sodhia.rohit at gmail.com>> wrote:
>>
>>         I'll try that immediately, thanks! I appreciate all your
>>         advice; hopefully I won't have to reach out again :p
>>
>>         On Mon, Sep 11, 2017 at 2:39 PM, Yuri <yvoinov at gmail.com
>>         <mailto:yvoinov at gmail.com>> wrote:
>>
>>             I'm not Linux fanboy, but modern squid never runs as
>>             root. So, most probably it runs as nobody user.
>>
>>             Ah, yes:
>>
>>             #? TAG: cache_effective_user
>>             #??? If you start Squid as root, it will change its
>>             effective/real
>>             #??? UID/GID to the user specified below.? The default is
>>             to change
>>             #??? to UID of nobody.
>>             #??? see also; cache_effective_group
>>             #Default:
>>             # cache_effective_user nobody
>>
>>             #? TAG: cache_effective_group
>>             #??? Squid sets the GID to the effective user's default
>>             group ID
>>             #??? (taken from the password file) and supplementary
>>             group list
>>             #??? from the groups membership.
>>             #
>>             #??? If you want Squid to run with a specific GID
>>             regardless of
>>             #??? the group memberships of the effective user then set
>>             this
>>             #??? to the group (or GID) you want Squid to run as. When set
>>             #??? all other group privileges of the effective user are
>>             ignored
>>             #??? and only this GID is effective. If Squid is not
>>             started as
>>             #??? root the user starting Squid MUST be member of the
>>             specified
>>             #??? group.
>>             #
>>             #??? This option is not recommended by the Squid Team.
>>             #??? Our preference is for administrators to configure a
>>             secure
>>             #??? user account for squid with UID/GID matching system
>>             policies.
>>             #Default:
>>             # Use system group memberships of the
>>             cache_effective_user account
>>
>>             As documented. :)
>>
>>             AFAIK best solution is create non-privileged group & user
>>             (like squid/squid) and set both this parameters explicity.
>>
>>             Then change owner recursively on SSL cache to this user.
>>
>>
>>             12.09.2017 0:36, Rohit Sodhia ?????:
>>>             Neither of those values are set in my config. Even
>>>             though I'm not using squid for caching, I need those
>>>             values? They aren't set in the default configs either.
>>>
>>>             On Mon, Sep 11, 2017 at 2:33 PM, Yuri <yvoinov at gmail.com
>>>             <mailto:yvoinov at gmail.com>> wrote:
>>>
>>>                 Most probably you squid runs as another user than squid.
>>>
>>>                 Check your squid.conf for cache_effective_user and
>>>                 cache_effective_group values.
>>>
>>>                 Then change SSL cache permissions to this values.
>>>                 Should work.
>>>
>>>
>>>                 12.09.2017 0:30, Rohit Sodhia ?????:
>>>>                 Thanks for the feedback! I just used yum (it's a
>>>>                 CentOS 7 VB) and it set it up like that. I changed
>>>>                 the owner and group to squid:squid and tried
>>>>                 restarting squid, but still get the same errors. I
>>>>                 thought to run the command again, but this time it says
>>>>
>>>>                 /usr/lib64/squid/ssl_crtd: Cannot create
>>>>                 /var/lib/ssl_db
>>>>
>>>>                 If this folder has incorrect permissions are there
>>>>                 possibly other permission issues?
>>>>
>>>>                 On Mon, Sep 11, 2017 at 2:25 PM, Yuri
>>>>                 <yvoinov at gmail.com <mailto:yvoinov at gmail.com>> wrote:
>>>>
>>>>                     Here you root of problem.
>>>>
>>>>                     Should be (on my setups):
>>>>
>>>>                     # ls -al /var/lib/ssl_db
>>>>                     total 326
>>>>                     drwxr-xr-x 3 squid squid????? 5 Sep? 5 00:53 .
>>>>                     drwxr-xr-x 8 root? other????? 8 Sep? 5 00:53 ..
>>>>                     drwxr-xr-x 2 squid squid??? 454 Sep 11 23:37 certs
>>>>                     -rw-r--r-- 1 squid squid 280575 Sep 11 23:37
>>>>                     index.txt
>>>>                     -rw-r--r-- 1 squid squid????? 7 Sep 11 23:37 size
>>>>
>>>>                     I.e. Squid has no access to SSL cache dir
>>>>                     structures.
>>>>
>>>>
>>>>                     12.09.2017 0:23, Rohit Sodhia ?????:
>>>>>                     total 8
>>>>>                     drwxr-xr-x.? 3 root root?? 48 Sep 11 12:42 .
>>>>>                     drwxr-xr-x. 32 root root 4096 Sep 11 12:42 ..
>>>>>                     drwxr-xr-x.? 2 root root??? 6 Sep 11 12:42 certs
>>>>>                     -rw-r--r--.? 1 root root??? 0 Sep 11 12:42
>>>>>                     index.txt
>>>>>                     -rw-r--r--.? 1 root root??? 1 Sep 11 12:42 size
>>>>>
>>>>>
>>>>>                     On Mon, Sep 11, 2017 at 2:22 PM, Yuri
>>>>>                     <yvoinov at gmail.com <mailto:yvoinov at gmail.com>>
>>>>>                     wrote:
>>>>>
>>>>>                         Show output of
>>>>>
>>>>>                         ls -al /var/lib/ssl_db
>>>>>
>>>>>
>>>>>                         12.09.2017 0:21, Rohit Sodhia ?????:
>>>>>>                         Yes, but telling me it's crashing
>>>>>>                         unfortunately doesn't help me figure out
>>>>>>                         why or how to fix it. I've run the
>>>>>>                         command it suggests but it doesn't help.
>>>>>>                         I'm unfortunately not an ops guy familiar
>>>>>>                         with this kind of stuff; I don't see
>>>>>>                         anything on how to figure out what to do
>>>>>>                         about it.
>>>>>>
>>>>>>                         On Mon, Sep 11, 2017 at 2:17 PM, Yuri
>>>>>>                         <yvoinov at gmail.com
>>>>>>                         <mailto:yvoinov at gmail.com>> wrote:
>>>>>>
>>>>>>                             It tells you what's happens.
>>>>>>
>>>>>>
>>>>>>                             11.09.2017 23:50, Rohit Sodhia ?????:
>>>>>>                             > (ssl_crtd): Uninitialized SSL
>>>>>>                             certificate database directory:
>>>>>>                             > /var/lib/ssl_db. To initialize, run
>>>>>>                             "ssl_crtd -c -s /var/lib/ssl_db".
>>>>>>
>>>>>>
>>>>>>
>>>>>>                             _______________________________________________
>>>>>>                             squid-users mailing list
>>>>>>                             squid-users at lists.squid-cache.org
>>>>>>                             <mailto:squid-users at lists.squid-cache.org>
>>>>>>                             http://lists.squid-cache.org/listinfo/squid-users
>>>>>>                             <http://lists.squid-cache.org/listinfo/squid-users>
>>>>>>
>>>>>>
>>>>>
>>>>>
>>>>
>>>>
>>>
>>>
>>
>>
>>
>
>

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20170912/4434d5ab/attachment.htm>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: signature.asc
Type: application/pgp-signature
Size: 473 bytes
Desc: OpenPGP digital signature
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20170912/4434d5ab/attachment.sig>

From sodhia.rohit at gmail.com  Mon Sep 11 20:05:03 2017
From: sodhia.rohit at gmail.com (Rohit Sodhia)
Date: Mon, 11 Sep 2017 16:05:03 -0400
Subject: [squid-users] Need assistance debugging Squid error: ssl_ctrd
 helpers crashing too quickly
In-Reply-To: <b55ec078-e25a-4c83-d5b2-6ea624699996@gmail.com>
References: <CAN1w9teczQdmfM8s-xe8H5kLgMPwyh6+cgRvJqV4owPSjRHcNw@mail.gmail.com>
 <465ed3a2-58b6-4ae1-b017-62c3f48b32b4@gmail.com>
 <CAN1w9tfN34wFxavuAcXGOVKcAArj8jzAmXsvn-Rd1=f3_KpE4g@mail.gmail.com>
 <3e79dfa3-d2b6-b742-cc9f-d72a6ae94d8b@gmail.com>
 <CAN1w9tchdgXXRNrb7nvf9_3i4_pjgw3oUQmDMHxsRWYA8hLLPA@mail.gmail.com>
 <0b3ad24a-1d30-d0ca-4f16-fbde772c5e21@gmail.com>
 <CAN1w9tf=HzUHEBcpwtvEeoNc4DV5iq=aMFoZKubXjwxTZq1wfQ@mail.gmail.com>
 <3a026389-5c8b-ca43-f909-a8e46a5b1faa@gmail.com>
 <CAN1w9tf0+h6W_T4kn49_70rexuQ6=Wy9hgTbE2mTPANE-hh3oQ@mail.gmail.com>
 <ea4740ba-1706-df39-32c8-120a9740181e@gmail.com>
 <CAN1w9teM35_=1X7CcvnMb9LtXRKW0okDBBN9aqO5-UKMdcGwaA@mail.gmail.com>
 <CAN1w9tcuVmZnQV+4aj=ZXD=rwBeOUUaHq7xOJkoGurGeq-=nwQ@mail.gmail.com>
 <75a12def-048c-7519-5b9a-fc767579285d@gmail.com>
 <CAN1w9tfm8UCSW-bVhL-fOCFZD9DzxdJzW0cXyNczbwddN681jw@mail.gmail.com>
 <b55ec078-e25a-4c83-d5b2-6ea624699996@gmail.com>
Message-ID: <CAN1w9tcg+gysMsCWVK_OC46Uf=6tN2uVtXL61uVjvYALe_CuSA@mail.gmail.com>

I'll try to find it, but I read a few articles/SO questions that suggested
there were bugs in 4 relating to SSL bumping? If they were wrong, I'd be
glad to go forward. Should I be removing the yum squid package and compile
my own? Is 3.5 problematic besides being old?

On Mon, Sep 11, 2017 at 4:02 PM, Yuri <yvoinov at gmail.com> wrote:

> Wait. Squid 3.5.20? So ancient?
>
> 12.09.2017 1:58, Rohit Sodhia ?????:
>
> sslcrtd_program /usr/lib64/squid/ssl_crtd -s /var/lib/ssl_db -M 4MB
>
> I used the line from the Stack Overflow question I linked earlier.
>
> On Mon, Sep 11, 2017 at 3:41 PM, Yuri <yvoinov at gmail.com> wrote:
>
>> Well. Let's check more deep.
>>
>> Show me parameter sslcrtd_program in your squid.conf
>>
>> 12.09.2017 1:23, Rohit Sodhia ?????:
>>
>> Unfortunately, no luck yet. Thank you again for your help before.
>>
>> I found that the user squid and group squid existed already, so I added
>>
>> cache_effective_user squid
>> cache_effective_group squid
>>
>> to my config (first two lines), made sure /var/lib/ssl_db and it's
>> contents were set to squid:squid and restarted the service, but I'm still
>> getting the same error :(
>>
>> On Mon, Sep 11, 2017 at 2:42 PM, Rohit Sodhia <sodhia.rohit at gmail.com>
>> wrote:
>>
>>> I'll try that immediately, thanks! I appreciate all your advice;
>>> hopefully I won't have to reach out again :p
>>>
>>> On Mon, Sep 11, 2017 at 2:39 PM, Yuri <yvoinov at gmail.com> wrote:
>>>
>>>> I'm not Linux fanboy, but modern squid never runs as root. So, most
>>>> probably it runs as nobody user.
>>>>
>>>> Ah, yes:
>>>>
>>>> #  TAG: cache_effective_user
>>>> #    If you start Squid as root, it will change its effective/real
>>>> #    UID/GID to the user specified below.  The default is to change
>>>> #    to UID of nobody.
>>>> #    see also; cache_effective_group
>>>> #Default:
>>>> # cache_effective_user nobody
>>>>
>>>> #  TAG: cache_effective_group
>>>> #    Squid sets the GID to the effective user's default group ID
>>>> #    (taken from the password file) and supplementary group list
>>>> #    from the groups membership.
>>>> #
>>>> #    If you want Squid to run with a specific GID regardless of
>>>> #    the group memberships of the effective user then set this
>>>> #    to the group (or GID) you want Squid to run as. When set
>>>> #    all other group privileges of the effective user are ignored
>>>> #    and only this GID is effective. If Squid is not started as
>>>> #    root the user starting Squid MUST be member of the specified
>>>> #    group.
>>>> #
>>>> #    This option is not recommended by the Squid Team.
>>>> #    Our preference is for administrators to configure a secure
>>>> #    user account for squid with UID/GID matching system policies.
>>>> #Default:
>>>> # Use system group memberships of the cache_effective_user account
>>>>
>>>> As documented. :)
>>>>
>>>> AFAIK best solution is create non-privileged group & user (like
>>>> squid/squid) and set both this parameters explicity.
>>>>
>>>> Then change owner recursively on SSL cache to this user.
>>>>
>>>> 12.09.2017 0:36, Rohit Sodhia ?????:
>>>>
>>>> Neither of those values are set in my config. Even though I'm not using
>>>> squid for caching, I need those values? They aren't set in the default
>>>> configs either.
>>>>
>>>> On Mon, Sep 11, 2017 at 2:33 PM, Yuri <yvoinov at gmail.com> wrote:
>>>>
>>>>> Most probably you squid runs as another user than squid.
>>>>>
>>>>> Check your squid.conf for cache_effective_user and
>>>>> cache_effective_group values.
>>>>>
>>>>> Then change SSL cache permissions to this values. Should work.
>>>>>
>>>>> 12.09.2017 0:30, Rohit Sodhia ?????:
>>>>>
>>>>> Thanks for the feedback! I just used yum (it's a CentOS 7 VB) and it
>>>>> set it up like that. I changed the owner and group to squid:squid and tried
>>>>> restarting squid, but still get the same errors. I thought to run the
>>>>> command again, but this time it says
>>>>>
>>>>> /usr/lib64/squid/ssl_crtd: Cannot create /var/lib/ssl_db
>>>>>
>>>>> If this folder has incorrect permissions are there possibly other
>>>>> permission issues?
>>>>>
>>>>> On Mon, Sep 11, 2017 at 2:25 PM, Yuri <yvoinov at gmail.com> wrote:
>>>>>
>>>>>> Here you root of problem.
>>>>>>
>>>>>> Should be (on my setups):
>>>>>>
>>>>>> # ls -al /var/lib/ssl_db
>>>>>> total 326
>>>>>> drwxr-xr-x 3 squid squid      5 Sep  5 00:53 .
>>>>>> drwxr-xr-x 8 root  other      8 Sep  5 00:53 ..
>>>>>> drwxr-xr-x 2 squid squid    454 Sep 11 23:37 certs
>>>>>> -rw-r--r-- 1 squid squid 280575 Sep 11 23:37 index.txt
>>>>>> -rw-r--r-- 1 squid squid      7 Sep 11 23:37 size
>>>>>>
>>>>>> I.e. Squid has no access to SSL cache dir structures.
>>>>>>
>>>>>> 12.09.2017 0:23, Rohit Sodhia ?????:
>>>>>>
>>>>>> total 8
>>>>>> drwxr-xr-x.  3 root root   48 Sep 11 12:42 .
>>>>>> drwxr-xr-x. 32 root root 4096 Sep 11 12:42 ..
>>>>>> drwxr-xr-x.  2 root root    6 Sep 11 12:42 certs
>>>>>> -rw-r--r--.  1 root root    0 Sep 11 12:42 index.txt
>>>>>> -rw-r--r--.  1 root root    1 Sep 11 12:42 size
>>>>>>
>>>>>>
>>>>>> On Mon, Sep 11, 2017 at 2:22 PM, Yuri <yvoinov at gmail.com> wrote:
>>>>>>
>>>>>>> Show output of
>>>>>>>
>>>>>>> ls -al /var/lib/ssl_db
>>>>>>>
>>>>>>> 12.09.2017 0:21, Rohit Sodhia ?????:
>>>>>>>
>>>>>>> Yes, but telling me it's crashing unfortunately doesn't help me
>>>>>>> figure out why or how to fix it. I've run the command it suggests but it
>>>>>>> doesn't help. I'm unfortunately not an ops guy familiar with this kind of
>>>>>>> stuff; I don't see anything on how to figure out what to do about it.
>>>>>>>
>>>>>>> On Mon, Sep 11, 2017 at 2:17 PM, Yuri <yvoinov at gmail.com> wrote:
>>>>>>>
>>>>>>>> It tells you what's happens.
>>>>>>>>
>>>>>>>>
>>>>>>>> 11.09.2017 23:50, Rohit Sodhia ?????:
>>>>>>>> > (ssl_crtd): Uninitialized SSL certificate database directory:
>>>>>>>> > /var/lib/ssl_db. To initialize, run "ssl_crtd -c -s
>>>>>>>> /var/lib/ssl_db".
>>>>>>>>
>>>>>>>>
>>>>>>>>
>>>>>>>> _______________________________________________
>>>>>>>> squid-users mailing list
>>>>>>>> squid-users at lists.squid-cache.org
>>>>>>>> http://lists.squid-cache.org/listinfo/squid-users
>>>>>>>>
>>>>>>>>
>>>>>>>
>>>>>>>
>>>>>>
>>>>>>
>>>>>
>>>>>
>>>>
>>>>
>>>
>>
>>
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20170911/6081e6f1/attachment.htm>

From yvoinov at gmail.com  Mon Sep 11 20:07:44 2017
From: yvoinov at gmail.com (Yuri)
Date: Tue, 12 Sep 2017 02:07:44 +0600
Subject: [squid-users] Need assistance debugging Squid error: ssl_ctrd
 helpers crashing too quickly
In-Reply-To: <CAN1w9tcg+gysMsCWVK_OC46Uf=6tN2uVtXL61uVjvYALe_CuSA@mail.gmail.com>
References: <CAN1w9teczQdmfM8s-xe8H5kLgMPwyh6+cgRvJqV4owPSjRHcNw@mail.gmail.com>
 <465ed3a2-58b6-4ae1-b017-62c3f48b32b4@gmail.com>
 <CAN1w9tfN34wFxavuAcXGOVKcAArj8jzAmXsvn-Rd1=f3_KpE4g@mail.gmail.com>
 <3e79dfa3-d2b6-b742-cc9f-d72a6ae94d8b@gmail.com>
 <CAN1w9tchdgXXRNrb7nvf9_3i4_pjgw3oUQmDMHxsRWYA8hLLPA@mail.gmail.com>
 <0b3ad24a-1d30-d0ca-4f16-fbde772c5e21@gmail.com>
 <CAN1w9tf=HzUHEBcpwtvEeoNc4DV5iq=aMFoZKubXjwxTZq1wfQ@mail.gmail.com>
 <3a026389-5c8b-ca43-f909-a8e46a5b1faa@gmail.com>
 <CAN1w9tf0+h6W_T4kn49_70rexuQ6=Wy9hgTbE2mTPANE-hh3oQ@mail.gmail.com>
 <ea4740ba-1706-df39-32c8-120a9740181e@gmail.com>
 <CAN1w9teM35_=1X7CcvnMb9LtXRKW0okDBBN9aqO5-UKMdcGwaA@mail.gmail.com>
 <CAN1w9tcuVmZnQV+4aj=ZXD=rwBeOUUaHq7xOJkoGurGeq-=nwQ@mail.gmail.com>
 <75a12def-048c-7519-5b9a-fc767579285d@gmail.com>
 <CAN1w9tfm8UCSW-bVhL-fOCFZD9DzxdJzW0cXyNczbwddN681jw@mail.gmail.com>
 <b55ec078-e25a-4c83-d5b2-6ea624699996@gmail.com>
 <CAN1w9tcg+gysMsCWVK_OC46Uf=6tN2uVtXL61uVjvYALe_CuSA@mail.gmail.com>
Message-ID: <5fc76932-c22f-b423-3b4d-64c2416e8b7e@gmail.com>

Seems latest 4.0.21 is good enough. Most critical SSL-related bugs
almost closed or closed.

At least latest 3.5.27 is released. AFAIK this is minimum to
problem-free running.

Repositories software sometimes has strange quirks, or sometimes rancid.

12.09.2017 2:05, Rohit Sodhia ?????:
> I'll try to find it, but I read a few articles/SO questions that
> suggested there were bugs in 4 relating to SSL bumping? If they were
> wrong, I'd be glad to go forward. Should I be removing the yum squid
> package and compile my own? Is 3.5 problematic besides being old?
>
> On Mon, Sep 11, 2017 at 4:02 PM, Yuri <yvoinov at gmail.com
> <mailto:yvoinov at gmail.com>> wrote:
>
>     Wait. Squid 3.5.20? So ancient?
>
>
>     12.09.2017 1:58, Rohit Sodhia ?????:
>>     sslcrtd_program /usr/lib64/squid/ssl_crtd -s /var/lib/ssl_db -M 4MB
>>
>>     I used the line from the Stack Overflow question I linked earlier.
>>
>>     On Mon, Sep 11, 2017 at 3:41 PM, Yuri <yvoinov at gmail.com
>>     <mailto:yvoinov at gmail.com>> wrote:
>>
>>         Well. Let's check more deep.
>>
>>         Show me parameter sslcrtd_program in your squid.conf
>>
>>
>>         12.09.2017 1:23, Rohit Sodhia ?????:
>>>         Unfortunately, no luck yet. Thank you again for your help
>>>         before.
>>>
>>>         I found that the user squid and group squid existed already,
>>>         so I added
>>>
>>>         cache_effective_user squid
>>>         cache_effective_group squid
>>>
>>>         to my config (first two lines), made sure /var/lib/ssl_db
>>>         and it's contents were set to squid:squid and restarted the
>>>         service, but I'm still getting the same error :(
>>>
>>>         On Mon, Sep 11, 2017 at 2:42 PM, Rohit Sodhia
>>>         <sodhia.rohit at gmail.com <mailto:sodhia.rohit at gmail.com>> wrote:
>>>
>>>             I'll try that immediately, thanks! I appreciate all your
>>>             advice; hopefully I won't have to reach out again :p
>>>
>>>             On Mon, Sep 11, 2017 at 2:39 PM, Yuri <yvoinov at gmail.com
>>>             <mailto:yvoinov at gmail.com>> wrote:
>>>
>>>                 I'm not Linux fanboy, but modern squid never runs as
>>>                 root. So, most probably it runs as nobody user.
>>>
>>>                 Ah, yes:
>>>
>>>                 #? TAG: cache_effective_user
>>>                 #??? If you start Squid as root, it will change its
>>>                 effective/real
>>>                 #??? UID/GID to the user specified below.? The
>>>                 default is to change
>>>                 #??? to UID of nobody.
>>>                 #??? see also; cache_effective_group
>>>                 #Default:
>>>                 # cache_effective_user nobody
>>>
>>>                 #? TAG: cache_effective_group
>>>                 #??? Squid sets the GID to the effective user's
>>>                 default group ID
>>>                 #??? (taken from the password file) and
>>>                 supplementary group list
>>>                 #??? from the groups membership.
>>>                 #
>>>                 #??? If you want Squid to run with a specific GID
>>>                 regardless of
>>>                 #??? the group memberships of the effective user
>>>                 then set this
>>>                 #??? to the group (or GID) you want Squid to run as.
>>>                 When set
>>>                 #??? all other group privileges of the effective
>>>                 user are ignored
>>>                 #??? and only this GID is effective. If Squid is not
>>>                 started as
>>>                 #??? root the user starting Squid MUST be member of
>>>                 the specified
>>>                 #??? group.
>>>                 #
>>>                 #??? This option is not recommended by the Squid Team.
>>>                 #??? Our preference is for administrators to
>>>                 configure a secure
>>>                 #??? user account for squid with UID/GID matching
>>>                 system policies.
>>>                 #Default:
>>>                 # Use system group memberships of the
>>>                 cache_effective_user account
>>>
>>>                 As documented. :)
>>>
>>>                 AFAIK best solution is create non-privileged group &
>>>                 user (like squid/squid) and set both this parameters
>>>                 explicity.
>>>
>>>                 Then change owner recursively on SSL cache to this user.
>>>
>>>
>>>                 12.09.2017 0:36, Rohit Sodhia ?????:
>>>>                 Neither of those values are set in my config. Even
>>>>                 though I'm not using squid for caching, I need
>>>>                 those values? They aren't set in the default
>>>>                 configs either.
>>>>
>>>>                 On Mon, Sep 11, 2017 at 2:33 PM, Yuri
>>>>                 <yvoinov at gmail.com <mailto:yvoinov at gmail.com>> wrote:
>>>>
>>>>                     Most probably you squid runs as another user
>>>>                     than squid.
>>>>
>>>>                     Check your squid.conf for cache_effective_user
>>>>                     and cache_effective_group values.
>>>>
>>>>                     Then change SSL cache permissions to this
>>>>                     values. Should work.
>>>>
>>>>
>>>>                     12.09.2017 0:30, Rohit Sodhia ?????:
>>>>>                     Thanks for the feedback! I just used yum (it's
>>>>>                     a CentOS 7 VB) and it set it up like that. I
>>>>>                     changed the owner and group to squid:squid and
>>>>>                     tried restarting squid, but still get the same
>>>>>                     errors. I thought to run the command again,
>>>>>                     but this time it says
>>>>>
>>>>>                     /usr/lib64/squid/ssl_crtd: Cannot create
>>>>>                     /var/lib/ssl_db
>>>>>
>>>>>                     If this folder has incorrect permissions are
>>>>>                     there possibly other permission issues?
>>>>>
>>>>>                     On Mon, Sep 11, 2017 at 2:25 PM, Yuri
>>>>>                     <yvoinov at gmail.com <mailto:yvoinov at gmail.com>>
>>>>>                     wrote:
>>>>>
>>>>>                         Here you root of problem.
>>>>>
>>>>>                         Should be (on my setups):
>>>>>
>>>>>                         # ls -al /var/lib/ssl_db
>>>>>                         total 326
>>>>>                         drwxr-xr-x 3 squid squid????? 5 Sep? 5 00:53 .
>>>>>                         drwxr-xr-x 8 root? other????? 8 Sep? 5
>>>>>                         00:53 ..
>>>>>                         drwxr-xr-x 2 squid squid??? 454 Sep 11
>>>>>                         23:37 certs
>>>>>                         -rw-r--r-- 1 squid squid 280575 Sep 11
>>>>>                         23:37 index.txt
>>>>>                         -rw-r--r-- 1 squid squid????? 7 Sep 11
>>>>>                         23:37 size
>>>>>
>>>>>                         I.e. Squid has no access to SSL cache dir
>>>>>                         structures.
>>>>>
>>>>>
>>>>>                         12.09.2017 0:23, Rohit Sodhia ?????:
>>>>>>                         total 8
>>>>>>                         drwxr-xr-x.? 3 root root?? 48 Sep 11 12:42 .
>>>>>>                         drwxr-xr-x. 32 root root 4096 Sep 11 12:42 ..
>>>>>>                         drwxr-xr-x.? 2 root root??? 6 Sep 11
>>>>>>                         12:42 certs
>>>>>>                         -rw-r--r--.? 1 root root??? 0 Sep 11
>>>>>>                         12:42 index.txt
>>>>>>                         -rw-r--r--.? 1 root root??? 1 Sep 11
>>>>>>                         12:42 size
>>>>>>
>>>>>>
>>>>>>                         On Mon, Sep 11, 2017 at 2:22 PM, Yuri
>>>>>>                         <yvoinov at gmail.com
>>>>>>                         <mailto:yvoinov at gmail.com>> wrote:
>>>>>>
>>>>>>                             Show output of
>>>>>>
>>>>>>                             ls -al /var/lib/ssl_db
>>>>>>
>>>>>>
>>>>>>                             12.09.2017 0:21, Rohit Sodhia ?????:
>>>>>>>                             Yes, but telling me it's crashing
>>>>>>>                             unfortunately doesn't help me figure
>>>>>>>                             out why or how to fix it. I've run
>>>>>>>                             the command it suggests but it
>>>>>>>                             doesn't help. I'm unfortunately not
>>>>>>>                             an ops guy familiar with this kind
>>>>>>>                             of stuff; I don't see anything on
>>>>>>>                             how to figure out what to do about it.
>>>>>>>
>>>>>>>                             On Mon, Sep 11, 2017 at 2:17 PM,
>>>>>>>                             Yuri <yvoinov at gmail.com
>>>>>>>                             <mailto:yvoinov at gmail.com>> wrote:
>>>>>>>
>>>>>>>                                 It tells you what's happens.
>>>>>>>
>>>>>>>
>>>>>>>                                 11.09.2017 23:50, Rohit Sodhia
>>>>>>>                                 ?????:
>>>>>>>                                 > (ssl_crtd): Uninitialized SSL
>>>>>>>                                 certificate database directory:
>>>>>>>                                 > /var/lib/ssl_db. To
>>>>>>>                                 initialize, run "ssl_crtd -c -s
>>>>>>>                                 /var/lib/ssl_db".
>>>>>>>
>>>>>>>
>>>>>>>
>>>>>>>                                 _______________________________________________
>>>>>>>                                 squid-users mailing list
>>>>>>>                                 squid-users at lists.squid-cache.org
>>>>>>>                                 <mailto:squid-users at lists.squid-cache.org>
>>>>>>>                                 http://lists.squid-cache.org/listinfo/squid-users
>>>>>>>                                 <http://lists.squid-cache.org/listinfo/squid-users>
>>>>>>>
>>>>>>>
>>>>>>
>>>>>>
>>>>>
>>>>>
>>>>
>>>>
>>>
>>>
>>>
>>
>>
>
>

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20170912/869bc632/attachment.htm>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: signature.asc
Type: application/pgp-signature
Size: 473 bytes
Desc: OpenPGP digital signature
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20170912/869bc632/attachment.sig>

From sodhia.rohit at gmail.com  Mon Sep 11 20:15:23 2017
From: sodhia.rohit at gmail.com (Rohit Sodhia)
Date: Mon, 11 Sep 2017 16:15:23 -0400
Subject: [squid-users] Need assistance debugging Squid error: ssl_ctrd
 helpers crashing too quickly
In-Reply-To: <5fc76932-c22f-b423-3b4d-64c2416e8b7e@gmail.com>
References: <CAN1w9teczQdmfM8s-xe8H5kLgMPwyh6+cgRvJqV4owPSjRHcNw@mail.gmail.com>
 <465ed3a2-58b6-4ae1-b017-62c3f48b32b4@gmail.com>
 <CAN1w9tfN34wFxavuAcXGOVKcAArj8jzAmXsvn-Rd1=f3_KpE4g@mail.gmail.com>
 <3e79dfa3-d2b6-b742-cc9f-d72a6ae94d8b@gmail.com>
 <CAN1w9tchdgXXRNrb7nvf9_3i4_pjgw3oUQmDMHxsRWYA8hLLPA@mail.gmail.com>
 <0b3ad24a-1d30-d0ca-4f16-fbde772c5e21@gmail.com>
 <CAN1w9tf=HzUHEBcpwtvEeoNc4DV5iq=aMFoZKubXjwxTZq1wfQ@mail.gmail.com>
 <3a026389-5c8b-ca43-f909-a8e46a5b1faa@gmail.com>
 <CAN1w9tf0+h6W_T4kn49_70rexuQ6=Wy9hgTbE2mTPANE-hh3oQ@mail.gmail.com>
 <ea4740ba-1706-df39-32c8-120a9740181e@gmail.com>
 <CAN1w9teM35_=1X7CcvnMb9LtXRKW0okDBBN9aqO5-UKMdcGwaA@mail.gmail.com>
 <CAN1w9tcuVmZnQV+4aj=ZXD=rwBeOUUaHq7xOJkoGurGeq-=nwQ@mail.gmail.com>
 <75a12def-048c-7519-5b9a-fc767579285d@gmail.com>
 <CAN1w9tfm8UCSW-bVhL-fOCFZD9DzxdJzW0cXyNczbwddN681jw@mail.gmail.com>
 <b55ec078-e25a-4c83-d5b2-6ea624699996@gmail.com>
 <CAN1w9tcg+gysMsCWVK_OC46Uf=6tN2uVtXL61uVjvYALe_CuSA@mail.gmail.com>
 <5fc76932-c22f-b423-3b4d-64c2416e8b7e@gmail.com>
Message-ID: <CAN1w9teamDf9LS0z8T2pf3P07oikqea6qA0PWD_bHa1_n03GsA@mail.gmail.com>

Ah. I'm on 3.5.20; not sure how far back that is. Is that the core of the
problem?

On Mon, Sep 11, 2017 at 4:07 PM, Yuri <yvoinov at gmail.com> wrote:

> Seems latest 4.0.21 is good enough. Most critical SSL-related bugs almost
> closed or closed.
>
> At least latest 3.5.27 is released. AFAIK this is minimum to problem-free
> running.
>
> Repositories software sometimes has strange quirks, or sometimes rancid.
> 12.09.2017 2:05, Rohit Sodhia ?????:
>
> I'll try to find it, but I read a few articles/SO questions that suggested
> there were bugs in 4 relating to SSL bumping? If they were wrong, I'd be
> glad to go forward. Should I be removing the yum squid package and compile
> my own? Is 3.5 problematic besides being old?
>
> On Mon, Sep 11, 2017 at 4:02 PM, Yuri <yvoinov at gmail.com> wrote:
>
>> Wait. Squid 3.5.20? So ancient?
>>
>> 12.09.2017 1:58, Rohit Sodhia ?????:
>>
>> sslcrtd_program /usr/lib64/squid/ssl_crtd -s /var/lib/ssl_db -M 4MB
>>
>> I used the line from the Stack Overflow question I linked earlier.
>>
>> On Mon, Sep 11, 2017 at 3:41 PM, Yuri <yvoinov at gmail.com> wrote:
>>
>>> Well. Let's check more deep.
>>>
>>> Show me parameter sslcrtd_program in your squid.conf
>>>
>>> 12.09.2017 1:23, Rohit Sodhia ?????:
>>>
>>> Unfortunately, no luck yet. Thank you again for your help before.
>>>
>>> I found that the user squid and group squid existed already, so I added
>>>
>>> cache_effective_user squid
>>> cache_effective_group squid
>>>
>>> to my config (first two lines), made sure /var/lib/ssl_db and it's
>>> contents were set to squid:squid and restarted the service, but I'm still
>>> getting the same error :(
>>>
>>> On Mon, Sep 11, 2017 at 2:42 PM, Rohit Sodhia <sodhia.rohit at gmail.com>
>>> wrote:
>>>
>>>> I'll try that immediately, thanks! I appreciate all your advice;
>>>> hopefully I won't have to reach out again :p
>>>>
>>>> On Mon, Sep 11, 2017 at 2:39 PM, Yuri <yvoinov at gmail.com> wrote:
>>>>
>>>>> I'm not Linux fanboy, but modern squid never runs as root. So, most
>>>>> probably it runs as nobody user.
>>>>>
>>>>> Ah, yes:
>>>>>
>>>>> #  TAG: cache_effective_user
>>>>> #    If you start Squid as root, it will change its effective/real
>>>>> #    UID/GID to the user specified below.  The default is to change
>>>>> #    to UID of nobody.
>>>>> #    see also; cache_effective_group
>>>>> #Default:
>>>>> # cache_effective_user nobody
>>>>>
>>>>> #  TAG: cache_effective_group
>>>>> #    Squid sets the GID to the effective user's default group ID
>>>>> #    (taken from the password file) and supplementary group list
>>>>> #    from the groups membership.
>>>>> #
>>>>> #    If you want Squid to run with a specific GID regardless of
>>>>> #    the group memberships of the effective user then set this
>>>>> #    to the group (or GID) you want Squid to run as. When set
>>>>> #    all other group privileges of the effective user are ignored
>>>>> #    and only this GID is effective. If Squid is not started as
>>>>> #    root the user starting Squid MUST be member of the specified
>>>>> #    group.
>>>>> #
>>>>> #    This option is not recommended by the Squid Team.
>>>>> #    Our preference is for administrators to configure a secure
>>>>> #    user account for squid with UID/GID matching system policies.
>>>>> #Default:
>>>>> # Use system group memberships of the cache_effective_user account
>>>>>
>>>>> As documented. :)
>>>>>
>>>>> AFAIK best solution is create non-privileged group & user (like
>>>>> squid/squid) and set both this parameters explicity.
>>>>>
>>>>> Then change owner recursively on SSL cache to this user.
>>>>>
>>>>> 12.09.2017 0:36, Rohit Sodhia ?????:
>>>>>
>>>>> Neither of those values are set in my config. Even though I'm not
>>>>> using squid for caching, I need those values? They aren't set in the
>>>>> default configs either.
>>>>>
>>>>> On Mon, Sep 11, 2017 at 2:33 PM, Yuri <yvoinov at gmail.com> wrote:
>>>>>
>>>>>> Most probably you squid runs as another user than squid.
>>>>>>
>>>>>> Check your squid.conf for cache_effective_user and
>>>>>> cache_effective_group values.
>>>>>>
>>>>>> Then change SSL cache permissions to this values. Should work.
>>>>>>
>>>>>> 12.09.2017 0:30, Rohit Sodhia ?????:
>>>>>>
>>>>>> Thanks for the feedback! I just used yum (it's a CentOS 7 VB) and it
>>>>>> set it up like that. I changed the owner and group to squid:squid and tried
>>>>>> restarting squid, but still get the same errors. I thought to run the
>>>>>> command again, but this time it says
>>>>>>
>>>>>> /usr/lib64/squid/ssl_crtd: Cannot create /var/lib/ssl_db
>>>>>>
>>>>>> If this folder has incorrect permissions are there possibly other
>>>>>> permission issues?
>>>>>>
>>>>>> On Mon, Sep 11, 2017 at 2:25 PM, Yuri <yvoinov at gmail.com> wrote:
>>>>>>
>>>>>>> Here you root of problem.
>>>>>>>
>>>>>>> Should be (on my setups):
>>>>>>>
>>>>>>> # ls -al /var/lib/ssl_db
>>>>>>> total 326
>>>>>>> drwxr-xr-x 3 squid squid      5 Sep  5 00:53 .
>>>>>>> drwxr-xr-x 8 root  other      8 Sep  5 00:53 ..
>>>>>>> drwxr-xr-x 2 squid squid    454 Sep 11 23:37 certs
>>>>>>> -rw-r--r-- 1 squid squid 280575 Sep 11 23:37 index.txt
>>>>>>> -rw-r--r-- 1 squid squid      7 Sep 11 23:37 size
>>>>>>>
>>>>>>> I.e. Squid has no access to SSL cache dir structures.
>>>>>>>
>>>>>>> 12.09.2017 0:23, Rohit Sodhia ?????:
>>>>>>>
>>>>>>> total 8
>>>>>>> drwxr-xr-x.  3 root root   48 Sep 11 12:42 .
>>>>>>> drwxr-xr-x. 32 root root 4096 Sep 11 12:42 ..
>>>>>>> drwxr-xr-x.  2 root root    6 Sep 11 12:42 certs
>>>>>>> -rw-r--r--.  1 root root    0 Sep 11 12:42 index.txt
>>>>>>> -rw-r--r--.  1 root root    1 Sep 11 12:42 size
>>>>>>>
>>>>>>>
>>>>>>> On Mon, Sep 11, 2017 at 2:22 PM, Yuri <yvoinov at gmail.com> wrote:
>>>>>>>
>>>>>>>> Show output of
>>>>>>>>
>>>>>>>> ls -al /var/lib/ssl_db
>>>>>>>>
>>>>>>>> 12.09.2017 0:21, Rohit Sodhia ?????:
>>>>>>>>
>>>>>>>> Yes, but telling me it's crashing unfortunately doesn't help me
>>>>>>>> figure out why or how to fix it. I've run the command it suggests but it
>>>>>>>> doesn't help. I'm unfortunately not an ops guy familiar with this kind of
>>>>>>>> stuff; I don't see anything on how to figure out what to do about it.
>>>>>>>>
>>>>>>>> On Mon, Sep 11, 2017 at 2:17 PM, Yuri <yvoinov at gmail.com> wrote:
>>>>>>>>
>>>>>>>>> It tells you what's happens.
>>>>>>>>>
>>>>>>>>>
>>>>>>>>> 11.09.2017 23:50, Rohit Sodhia ?????:
>>>>>>>>> > (ssl_crtd): Uninitialized SSL certificate database directory:
>>>>>>>>> > /var/lib/ssl_db. To initialize, run "ssl_crtd -c -s
>>>>>>>>> /var/lib/ssl_db".
>>>>>>>>>
>>>>>>>>>
>>>>>>>>>
>>>>>>>>> _______________________________________________
>>>>>>>>> squid-users mailing list
>>>>>>>>> squid-users at lists.squid-cache.org
>>>>>>>>> http://lists.squid-cache.org/listinfo/squid-users
>>>>>>>>>
>>>>>>>>>
>>>>>>>>
>>>>>>>>
>>>>>>>
>>>>>>>
>>>>>>
>>>>>>
>>>>>
>>>>>
>>>>
>>>
>>>
>>
>>
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20170911/649a494f/attachment.htm>

From yvoinov at gmail.com  Mon Sep 11 20:17:38 2017
From: yvoinov at gmail.com (Yuri)
Date: Tue, 12 Sep 2017 02:17:38 +0600
Subject: [squid-users] Need assistance debugging Squid error: ssl_ctrd
 helpers crashing too quickly
In-Reply-To: <CAN1w9teamDf9LS0z8T2pf3P07oikqea6qA0PWD_bHa1_n03GsA@mail.gmail.com>
References: <CAN1w9teczQdmfM8s-xe8H5kLgMPwyh6+cgRvJqV4owPSjRHcNw@mail.gmail.com>
 <3e79dfa3-d2b6-b742-cc9f-d72a6ae94d8b@gmail.com>
 <CAN1w9tchdgXXRNrb7nvf9_3i4_pjgw3oUQmDMHxsRWYA8hLLPA@mail.gmail.com>
 <0b3ad24a-1d30-d0ca-4f16-fbde772c5e21@gmail.com>
 <CAN1w9tf=HzUHEBcpwtvEeoNc4DV5iq=aMFoZKubXjwxTZq1wfQ@mail.gmail.com>
 <3a026389-5c8b-ca43-f909-a8e46a5b1faa@gmail.com>
 <CAN1w9tf0+h6W_T4kn49_70rexuQ6=Wy9hgTbE2mTPANE-hh3oQ@mail.gmail.com>
 <ea4740ba-1706-df39-32c8-120a9740181e@gmail.com>
 <CAN1w9teM35_=1X7CcvnMb9LtXRKW0okDBBN9aqO5-UKMdcGwaA@mail.gmail.com>
 <CAN1w9tcuVmZnQV+4aj=ZXD=rwBeOUUaHq7xOJkoGurGeq-=nwQ@mail.gmail.com>
 <75a12def-048c-7519-5b9a-fc767579285d@gmail.com>
 <CAN1w9tfm8UCSW-bVhL-fOCFZD9DzxdJzW0cXyNczbwddN681jw@mail.gmail.com>
 <b55ec078-e25a-4c83-d5b2-6ea624699996@gmail.com>
 <CAN1w9tcg+gysMsCWVK_OC46Uf=6tN2uVtXL61uVjvYALe_CuSA@mail.gmail.com>
 <5fc76932-c22f-b423-3b4d-64c2416e8b7e@gmail.com>
 <CAN1w9teamDf9LS0z8T2pf3P07oikqea6qA0PWD_bHa1_n03GsA@mail.gmail.com>
Message-ID: <c9bd31f1-f877-2584-35ce-fe570e3fe098@gmail.com>

Hardly,

most probably something in repo's package. However, upgrade is always
recommended, especially with modern functionality. It changes fast enough.

12.09.2017 2:15, Rohit Sodhia ?????:
> Ah. I'm on 3.5.20; not sure how far back that is. Is that the core of
> the problem?
>
> On Mon, Sep 11, 2017 at 4:07 PM, Yuri <yvoinov at gmail.com
> <mailto:yvoinov at gmail.com>> wrote:
>
>     Seems latest 4.0.21 is good enough. Most critical SSL-related bugs
>     almost closed or closed.
>
>     At least latest 3.5.27 is released. AFAIK this is minimum to
>     problem-free running.
>
>     Repositories software sometimes has strange quirks, or sometimes
>     rancid.
>
>     12.09.2017 2:05, Rohit Sodhia ?????:
>
>>     I'll try to find it, but I read a few articles/SO questions that
>>     suggested there were bugs in 4 relating to SSL bumping? If they
>>     were wrong, I'd be glad to go forward. Should I be removing the
>>     yum squid package and compile my own? Is 3.5 problematic besides
>>     being old?
>>
>>     On Mon, Sep 11, 2017 at 4:02 PM, Yuri <yvoinov at gmail.com
>>     <mailto:yvoinov at gmail.com>> wrote:
>>
>>         Wait. Squid 3.5.20? So ancient?
>>
>>
>>         12.09.2017 1:58, Rohit Sodhia ?????:
>>>         sslcrtd_program /usr/lib64/squid/ssl_crtd -s /var/lib/ssl_db
>>>         -M 4MB
>>>
>>>         I used the line from the Stack Overflow question I linked
>>>         earlier.
>>>
>>>         On Mon, Sep 11, 2017 at 3:41 PM, Yuri <yvoinov at gmail.com
>>>         <mailto:yvoinov at gmail.com>> wrote:
>>>
>>>             Well. Let's check more deep.
>>>
>>>             Show me parameter sslcrtd_program in your squid.conf
>>>
>>>
>>>             12.09.2017 1:23, Rohit Sodhia ?????:
>>>>             Unfortunately, no luck yet. Thank you again for your
>>>>             help before.
>>>>
>>>>             I found that the user squid and group squid existed
>>>>             already, so I added
>>>>
>>>>             cache_effective_user squid
>>>>             cache_effective_group squid
>>>>
>>>>             to my config (first two lines), made sure
>>>>             /var/lib/ssl_db and it's contents were set to
>>>>             squid:squid and restarted the service, but I'm still
>>>>             getting the same error :(
>>>>
>>>>             On Mon, Sep 11, 2017 at 2:42 PM, Rohit Sodhia
>>>>             <sodhia.rohit at gmail.com
>>>>             <mailto:sodhia.rohit at gmail.com>> wrote:
>>>>
>>>>                 I'll try that immediately, thanks! I appreciate all
>>>>                 your advice; hopefully I won't have to reach out
>>>>                 again :p
>>>>
>>>>                 On Mon, Sep 11, 2017 at 2:39 PM, Yuri
>>>>                 <yvoinov at gmail.com <mailto:yvoinov at gmail.com>> wrote:
>>>>
>>>>                     I'm not Linux fanboy, but modern squid never
>>>>                     runs as root. So, most probably it runs as
>>>>                     nobody user.
>>>>
>>>>                     Ah, yes:
>>>>
>>>>                     #? TAG: cache_effective_user
>>>>                     #??? If you start Squid as root, it will change
>>>>                     its effective/real
>>>>                     #??? UID/GID to the user specified below.? The
>>>>                     default is to change
>>>>                     #??? to UID of nobody.
>>>>                     #??? see also; cache_effective_group
>>>>                     #Default:
>>>>                     # cache_effective_user nobody
>>>>
>>>>                     #? TAG: cache_effective_group
>>>>                     #??? Squid sets the GID to the effective user's
>>>>                     default group ID
>>>>                     #??? (taken from the password file) and
>>>>                     supplementary group list
>>>>                     #??? from the groups membership.
>>>>                     #
>>>>                     #??? If you want Squid to run with a specific
>>>>                     GID regardless of
>>>>                     #??? the group memberships of the effective
>>>>                     user then set this
>>>>                     #??? to the group (or GID) you want Squid to
>>>>                     run as. When set
>>>>                     #??? all other group privileges of the
>>>>                     effective user are ignored
>>>>                     #??? and only this GID is effective. If Squid
>>>>                     is not started as
>>>>                     #??? root the user starting Squid MUST be
>>>>                     member of the specified
>>>>                     #??? group.
>>>>                     #
>>>>                     #??? This option is not recommended by the
>>>>                     Squid Team.
>>>>                     #??? Our preference is for administrators to
>>>>                     configure a secure
>>>>                     #??? user account for squid with UID/GID
>>>>                     matching system policies.
>>>>                     #Default:
>>>>                     # Use system group memberships of the
>>>>                     cache_effective_user account
>>>>
>>>>                     As documented. :)
>>>>
>>>>                     AFAIK best solution is create non-privileged
>>>>                     group & user (like squid/squid) and set both
>>>>                     this parameters explicity.
>>>>
>>>>                     Then change owner recursively on SSL cache to
>>>>                     this user.
>>>>
>>>>
>>>>                     12.09.2017 0:36, Rohit Sodhia ?????:
>>>>>                     Neither of those values are set in my config.
>>>>>                     Even though I'm not using squid for caching, I
>>>>>                     need those values? They aren't set in the
>>>>>                     default configs either.
>>>>>
>>>>>                     On Mon, Sep 11, 2017 at 2:33 PM, Yuri
>>>>>                     <yvoinov at gmail.com <mailto:yvoinov at gmail.com>>
>>>>>                     wrote:
>>>>>
>>>>>                         Most probably you squid runs as another
>>>>>                         user than squid.
>>>>>
>>>>>                         Check your squid.conf for
>>>>>                         cache_effective_user and
>>>>>                         cache_effective_group values.
>>>>>
>>>>>                         Then change SSL cache permissions to this
>>>>>                         values. Should work.
>>>>>
>>>>>
>>>>>                         12.09.2017 0:30, Rohit Sodhia ?????:
>>>>>>                         Thanks for the feedback! I just used yum
>>>>>>                         (it's a CentOS 7 VB) and it set it up
>>>>>>                         like that. I changed the owner and group
>>>>>>                         to squid:squid and tried restarting
>>>>>>                         squid, but still get the same errors. I
>>>>>>                         thought to run the command again, but
>>>>>>                         this time it says
>>>>>>
>>>>>>                         /usr/lib64/squid/ssl_crtd: Cannot create
>>>>>>                         /var/lib/ssl_db
>>>>>>
>>>>>>                         If this folder has incorrect permissions
>>>>>>                         are there possibly other permission issues?
>>>>>>
>>>>>>                         On Mon, Sep 11, 2017 at 2:25 PM, Yuri
>>>>>>                         <yvoinov at gmail.com
>>>>>>                         <mailto:yvoinov at gmail.com>> wrote:
>>>>>>
>>>>>>                             Here you root of problem.
>>>>>>
>>>>>>                             Should be (on my setups):
>>>>>>
>>>>>>                             # ls -al /var/lib/ssl_db
>>>>>>                             total 326
>>>>>>                             drwxr-xr-x 3 squid squid????? 5 Sep?
>>>>>>                             5 00:53 .
>>>>>>                             drwxr-xr-x 8 root? other????? 8 Sep?
>>>>>>                             5 00:53 ..
>>>>>>                             drwxr-xr-x 2 squid squid??? 454 Sep
>>>>>>                             11 23:37 certs
>>>>>>                             -rw-r--r-- 1 squid squid 280575 Sep
>>>>>>                             11 23:37 index.txt
>>>>>>                             -rw-r--r-- 1 squid squid????? 7 Sep
>>>>>>                             11 23:37 size
>>>>>>
>>>>>>                             I.e. Squid has no access to SSL cache
>>>>>>                             dir structures.
>>>>>>
>>>>>>
>>>>>>                             12.09.2017 0:23, Rohit Sodhia ?????:
>>>>>>>                             total 8
>>>>>>>                             drwxr-xr-x.? 3 root root?? 48 Sep 11
>>>>>>>                             12:42 .
>>>>>>>                             drwxr-xr-x. 32 root root 4096 Sep 11
>>>>>>>                             12:42 ..
>>>>>>>                             drwxr-xr-x.? 2 root root??? 6 Sep 11
>>>>>>>                             12:42 certs
>>>>>>>                             -rw-r--r--.? 1 root root??? 0 Sep 11
>>>>>>>                             12:42 index.txt
>>>>>>>                             -rw-r--r--.? 1 root root??? 1 Sep 11
>>>>>>>                             12:42 size
>>>>>>>
>>>>>>>
>>>>>>>                             On Mon, Sep 11, 2017 at 2:22 PM,
>>>>>>>                             Yuri <yvoinov at gmail.com
>>>>>>>                             <mailto:yvoinov at gmail.com>> wrote:
>>>>>>>
>>>>>>>                                 Show output of
>>>>>>>
>>>>>>>                                 ls -al /var/lib/ssl_db
>>>>>>>
>>>>>>>
>>>>>>>                                 12.09.2017 0:21, Rohit Sodhia ?????:
>>>>>>>>                                 Yes, but telling me it's
>>>>>>>>                                 crashing unfortunately doesn't
>>>>>>>>                                 help me figure out why or how
>>>>>>>>                                 to fix it. I've run the command
>>>>>>>>                                 it suggests but it doesn't
>>>>>>>>                                 help. I'm unfortunately not an
>>>>>>>>                                 ops guy familiar with this kind
>>>>>>>>                                 of stuff; I don't see anything
>>>>>>>>                                 on how to figure out what to do
>>>>>>>>                                 about it.
>>>>>>>>
>>>>>>>>                                 On Mon, Sep 11, 2017 at 2:17
>>>>>>>>                                 PM, Yuri <yvoinov at gmail.com
>>>>>>>>                                 <mailto:yvoinov at gmail.com>> wrote:
>>>>>>>>
>>>>>>>>                                     It tells you what's happens.
>>>>>>>>
>>>>>>>>
>>>>>>>>                                     11.09.2017 23:50, Rohit
>>>>>>>>                                     Sodhia ?????:
>>>>>>>>                                     > (ssl_crtd): Uninitialized
>>>>>>>>                                     SSL certificate database
>>>>>>>>                                     directory:
>>>>>>>>                                     > /var/lib/ssl_db. To
>>>>>>>>                                     initialize, run "ssl_crtd
>>>>>>>>                                     -c -s /var/lib/ssl_db".
>>>>>>>>
>>>>>>>>
>>>>>>>>
>>>>>>>>                                     _______________________________________________
>>>>>>>>                                     squid-users mailing list
>>>>>>>>                                     squid-users at lists.squid-cache.org
>>>>>>>>                                     <mailto:squid-users at lists.squid-cache.org>
>>>>>>>>                                     http://lists.squid-cache.org/listinfo/squid-users
>>>>>>>>                                     <http://lists.squid-cache.org/listinfo/squid-users>
>>>>>>>>
>>>>>>>>
>>>>>>>
>>>>>>>
>>>>>>
>>>>>>
>>>>>
>>>>>
>>>>
>>>>
>>>>
>>>
>>>
>>
>>
>
>

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20170912/47907ddd/attachment.htm>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: signature.asc
Type: application/pgp-signature
Size: 473 bytes
Desc: OpenPGP digital signature
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20170912/47907ddd/attachment.sig>

From sodhia.rohit at gmail.com  Mon Sep 11 20:18:39 2017
From: sodhia.rohit at gmail.com (Rohit Sodhia)
Date: Mon, 11 Sep 2017 16:18:39 -0400
Subject: [squid-users] Need assistance debugging Squid error: ssl_ctrd
 helpers crashing too quickly
In-Reply-To: <c9bd31f1-f877-2584-35ce-fe570e3fe098@gmail.com>
References: <CAN1w9teczQdmfM8s-xe8H5kLgMPwyh6+cgRvJqV4owPSjRHcNw@mail.gmail.com>
 <3e79dfa3-d2b6-b742-cc9f-d72a6ae94d8b@gmail.com>
 <CAN1w9tchdgXXRNrb7nvf9_3i4_pjgw3oUQmDMHxsRWYA8hLLPA@mail.gmail.com>
 <0b3ad24a-1d30-d0ca-4f16-fbde772c5e21@gmail.com>
 <CAN1w9tf=HzUHEBcpwtvEeoNc4DV5iq=aMFoZKubXjwxTZq1wfQ@mail.gmail.com>
 <3a026389-5c8b-ca43-f909-a8e46a5b1faa@gmail.com>
 <CAN1w9tf0+h6W_T4kn49_70rexuQ6=Wy9hgTbE2mTPANE-hh3oQ@mail.gmail.com>
 <ea4740ba-1706-df39-32c8-120a9740181e@gmail.com>
 <CAN1w9teM35_=1X7CcvnMb9LtXRKW0okDBBN9aqO5-UKMdcGwaA@mail.gmail.com>
 <CAN1w9tcuVmZnQV+4aj=ZXD=rwBeOUUaHq7xOJkoGurGeq-=nwQ@mail.gmail.com>
 <75a12def-048c-7519-5b9a-fc767579285d@gmail.com>
 <CAN1w9tfm8UCSW-bVhL-fOCFZD9DzxdJzW0cXyNczbwddN681jw@mail.gmail.com>
 <b55ec078-e25a-4c83-d5b2-6ea624699996@gmail.com>
 <CAN1w9tcg+gysMsCWVK_OC46Uf=6tN2uVtXL61uVjvYALe_CuSA@mail.gmail.com>
 <5fc76932-c22f-b423-3b4d-64c2416e8b7e@gmail.com>
 <CAN1w9teamDf9LS0z8T2pf3P07oikqea6qA0PWD_bHa1_n03GsA@mail.gmail.com>
 <c9bd31f1-f877-2584-35ce-fe570e3fe098@gmail.com>
Message-ID: <CAN1w9tfQt3Mivwpyo+u3Qp0agQ8pOgz2MGo2Wvb5AdGU3zbkjw@mail.gmail.com>

Ok. Looks like 3.5.20 is the latest on the yum repo I'm using, so guess
I'll have to learn how to compile it myself; never compiled a package
before.

On Mon, Sep 11, 2017 at 4:17 PM, Yuri <yvoinov at gmail.com> wrote:

> Hardly,
>
> most probably something in repo's package. However, upgrade is always
> recommended, especially with modern functionality. It changes fast enough.
>
> 12.09.2017 2:15, Rohit Sodhia ?????:
>
> Ah. I'm on 3.5.20; not sure how far back that is. Is that the core of the
> problem?
>
> On Mon, Sep 11, 2017 at 4:07 PM, Yuri <yvoinov at gmail.com> wrote:
>
>> Seems latest 4.0.21 is good enough. Most critical SSL-related bugs almost
>> closed or closed.
>>
>> At least latest 3.5.27 is released. AFAIK this is minimum to problem-free
>> running.
>>
>> Repositories software sometimes has strange quirks, or sometimes rancid.
>> 12.09.2017 2:05, Rohit Sodhia ?????:
>>
>> I'll try to find it, but I read a few articles/SO questions that
>> suggested there were bugs in 4 relating to SSL bumping? If they were wrong,
>> I'd be glad to go forward. Should I be removing the yum squid package and
>> compile my own? Is 3.5 problematic besides being old?
>>
>> On Mon, Sep 11, 2017 at 4:02 PM, Yuri <yvoinov at gmail.com> wrote:
>>
>>> Wait. Squid 3.5.20? So ancient?
>>>
>>> 12.09.2017 1:58, Rohit Sodhia ?????:
>>>
>>> sslcrtd_program /usr/lib64/squid/ssl_crtd -s /var/lib/ssl_db -M 4MB
>>>
>>> I used the line from the Stack Overflow question I linked earlier.
>>>
>>> On Mon, Sep 11, 2017 at 3:41 PM, Yuri <yvoinov at gmail.com> wrote:
>>>
>>>> Well. Let's check more deep.
>>>>
>>>> Show me parameter sslcrtd_program in your squid.conf
>>>>
>>>> 12.09.2017 1:23, Rohit Sodhia ?????:
>>>>
>>>> Unfortunately, no luck yet. Thank you again for your help before.
>>>>
>>>> I found that the user squid and group squid existed already, so I added
>>>>
>>>> cache_effective_user squid
>>>> cache_effective_group squid
>>>>
>>>> to my config (first two lines), made sure /var/lib/ssl_db and it's
>>>> contents were set to squid:squid and restarted the service, but I'm still
>>>> getting the same error :(
>>>>
>>>> On Mon, Sep 11, 2017 at 2:42 PM, Rohit Sodhia <sodhia.rohit at gmail.com>
>>>> wrote:
>>>>
>>>>> I'll try that immediately, thanks! I appreciate all your advice;
>>>>> hopefully I won't have to reach out again :p
>>>>>
>>>>> On Mon, Sep 11, 2017 at 2:39 PM, Yuri <yvoinov at gmail.com> wrote:
>>>>>
>>>>>> I'm not Linux fanboy, but modern squid never runs as root. So, most
>>>>>> probably it runs as nobody user.
>>>>>>
>>>>>> Ah, yes:
>>>>>>
>>>>>> #  TAG: cache_effective_user
>>>>>> #    If you start Squid as root, it will change its effective/real
>>>>>> #    UID/GID to the user specified below.  The default is to change
>>>>>> #    to UID of nobody.
>>>>>> #    see also; cache_effective_group
>>>>>> #Default:
>>>>>> # cache_effective_user nobody
>>>>>>
>>>>>> #  TAG: cache_effective_group
>>>>>> #    Squid sets the GID to the effective user's default group ID
>>>>>> #    (taken from the password file) and supplementary group list
>>>>>> #    from the groups membership.
>>>>>> #
>>>>>> #    If you want Squid to run with a specific GID regardless of
>>>>>> #    the group memberships of the effective user then set this
>>>>>> #    to the group (or GID) you want Squid to run as. When set
>>>>>> #    all other group privileges of the effective user are ignored
>>>>>> #    and only this GID is effective. If Squid is not started as
>>>>>> #    root the user starting Squid MUST be member of the specified
>>>>>> #    group.
>>>>>> #
>>>>>> #    This option is not recommended by the Squid Team.
>>>>>> #    Our preference is for administrators to configure a secure
>>>>>> #    user account for squid with UID/GID matching system policies.
>>>>>> #Default:
>>>>>> # Use system group memberships of the cache_effective_user account
>>>>>>
>>>>>> As documented. :)
>>>>>>
>>>>>> AFAIK best solution is create non-privileged group & user (like
>>>>>> squid/squid) and set both this parameters explicity.
>>>>>>
>>>>>> Then change owner recursively on SSL cache to this user.
>>>>>>
>>>>>> 12.09.2017 0:36, Rohit Sodhia ?????:
>>>>>>
>>>>>> Neither of those values are set in my config. Even though I'm not
>>>>>> using squid for caching, I need those values? They aren't set in the
>>>>>> default configs either.
>>>>>>
>>>>>> On Mon, Sep 11, 2017 at 2:33 PM, Yuri <yvoinov at gmail.com> wrote:
>>>>>>
>>>>>>> Most probably you squid runs as another user than squid.
>>>>>>>
>>>>>>> Check your squid.conf for cache_effective_user and
>>>>>>> cache_effective_group values.
>>>>>>>
>>>>>>> Then change SSL cache permissions to this values. Should work.
>>>>>>>
>>>>>>> 12.09.2017 0:30, Rohit Sodhia ?????:
>>>>>>>
>>>>>>> Thanks for the feedback! I just used yum (it's a CentOS 7 VB) and it
>>>>>>> set it up like that. I changed the owner and group to squid:squid and tried
>>>>>>> restarting squid, but still get the same errors. I thought to run the
>>>>>>> command again, but this time it says
>>>>>>>
>>>>>>> /usr/lib64/squid/ssl_crtd: Cannot create /var/lib/ssl_db
>>>>>>>
>>>>>>> If this folder has incorrect permissions are there possibly other
>>>>>>> permission issues?
>>>>>>>
>>>>>>> On Mon, Sep 11, 2017 at 2:25 PM, Yuri <yvoinov at gmail.com> wrote:
>>>>>>>
>>>>>>>> Here you root of problem.
>>>>>>>>
>>>>>>>> Should be (on my setups):
>>>>>>>>
>>>>>>>> # ls -al /var/lib/ssl_db
>>>>>>>> total 326
>>>>>>>> drwxr-xr-x 3 squid squid      5 Sep  5 00:53 .
>>>>>>>> drwxr-xr-x 8 root  other      8 Sep  5 00:53 ..
>>>>>>>> drwxr-xr-x 2 squid squid    454 Sep 11 23:37 certs
>>>>>>>> -rw-r--r-- 1 squid squid 280575 Sep 11 23:37 index.txt
>>>>>>>> -rw-r--r-- 1 squid squid      7 Sep 11 23:37 size
>>>>>>>>
>>>>>>>> I.e. Squid has no access to SSL cache dir structures.
>>>>>>>>
>>>>>>>> 12.09.2017 0:23, Rohit Sodhia ?????:
>>>>>>>>
>>>>>>>> total 8
>>>>>>>> drwxr-xr-x.  3 root root   48 Sep 11 12:42 .
>>>>>>>> drwxr-xr-x. 32 root root 4096 Sep 11 12:42 ..
>>>>>>>> drwxr-xr-x.  2 root root    6 Sep 11 12:42 certs
>>>>>>>> -rw-r--r--.  1 root root    0 Sep 11 12:42 index.txt
>>>>>>>> -rw-r--r--.  1 root root    1 Sep 11 12:42 size
>>>>>>>>
>>>>>>>>
>>>>>>>> On Mon, Sep 11, 2017 at 2:22 PM, Yuri <yvoinov at gmail.com> wrote:
>>>>>>>>
>>>>>>>>> Show output of
>>>>>>>>>
>>>>>>>>> ls -al /var/lib/ssl_db
>>>>>>>>>
>>>>>>>>> 12.09.2017 0:21, Rohit Sodhia ?????:
>>>>>>>>>
>>>>>>>>> Yes, but telling me it's crashing unfortunately doesn't help me
>>>>>>>>> figure out why or how to fix it. I've run the command it suggests but it
>>>>>>>>> doesn't help. I'm unfortunately not an ops guy familiar with this kind of
>>>>>>>>> stuff; I don't see anything on how to figure out what to do about it.
>>>>>>>>>
>>>>>>>>> On Mon, Sep 11, 2017 at 2:17 PM, Yuri <yvoinov at gmail.com> wrote:
>>>>>>>>>
>>>>>>>>>> It tells you what's happens.
>>>>>>>>>>
>>>>>>>>>>
>>>>>>>>>> 11.09.2017 23:50, Rohit Sodhia ?????:
>>>>>>>>>> > (ssl_crtd): Uninitialized SSL certificate database directory:
>>>>>>>>>> > /var/lib/ssl_db. To initialize, run "ssl_crtd -c -s
>>>>>>>>>> /var/lib/ssl_db".
>>>>>>>>>>
>>>>>>>>>>
>>>>>>>>>>
>>>>>>>>>> _______________________________________________
>>>>>>>>>> squid-users mailing list
>>>>>>>>>> squid-users at lists.squid-cache.org
>>>>>>>>>> http://lists.squid-cache.org/listinfo/squid-users
>>>>>>>>>>
>>>>>>>>>>
>>>>>>>>>
>>>>>>>>>
>>>>>>>>
>>>>>>>>
>>>>>>>
>>>>>>>
>>>>>>
>>>>>>
>>>>>
>>>>
>>>>
>>>
>>>
>>
>>
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20170911/2c3ab1ef/attachment.htm>

From yvoinov at gmail.com  Mon Sep 11 20:19:50 2017
From: yvoinov at gmail.com (Yuri)
Date: Tue, 12 Sep 2017 02:19:50 +0600
Subject: [squid-users] Need assistance debugging Squid error: ssl_ctrd
 helpers crashing too quickly
In-Reply-To: <CAN1w9tfQt3Mivwpyo+u3Qp0agQ8pOgz2MGo2Wvb5AdGU3zbkjw@mail.gmail.com>
References: <CAN1w9teczQdmfM8s-xe8H5kLgMPwyh6+cgRvJqV4owPSjRHcNw@mail.gmail.com>
 <0b3ad24a-1d30-d0ca-4f16-fbde772c5e21@gmail.com>
 <CAN1w9tf=HzUHEBcpwtvEeoNc4DV5iq=aMFoZKubXjwxTZq1wfQ@mail.gmail.com>
 <3a026389-5c8b-ca43-f909-a8e46a5b1faa@gmail.com>
 <CAN1w9tf0+h6W_T4kn49_70rexuQ6=Wy9hgTbE2mTPANE-hh3oQ@mail.gmail.com>
 <ea4740ba-1706-df39-32c8-120a9740181e@gmail.com>
 <CAN1w9teM35_=1X7CcvnMb9LtXRKW0okDBBN9aqO5-UKMdcGwaA@mail.gmail.com>
 <CAN1w9tcuVmZnQV+4aj=ZXD=rwBeOUUaHq7xOJkoGurGeq-=nwQ@mail.gmail.com>
 <75a12def-048c-7519-5b9a-fc767579285d@gmail.com>
 <CAN1w9tfm8UCSW-bVhL-fOCFZD9DzxdJzW0cXyNczbwddN681jw@mail.gmail.com>
 <b55ec078-e25a-4c83-d5b2-6ea624699996@gmail.com>
 <CAN1w9tcg+gysMsCWVK_OC46Uf=6tN2uVtXL61uVjvYALe_CuSA@mail.gmail.com>
 <5fc76932-c22f-b423-3b4d-64c2416e8b7e@gmail.com>
 <CAN1w9teamDf9LS0z8T2pf3P07oikqea6qA0PWD_bHa1_n03GsA@mail.gmail.com>
 <c9bd31f1-f877-2584-35ce-fe570e3fe098@gmail.com>
 <CAN1w9tfQt3Mivwpyo+u3Qp0agQ8pOgz2MGo2Wvb5AdGU3zbkjw@mail.gmail.com>
Message-ID: <f0d313ab-2c69-0237-4207-6342358b8ff0@gmail.com>

Everything happens once for the first time;)


12.09.2017 2:18, Rohit Sodhia ?????:
> Ok. Looks like 3.5.20 is the latest on the yum repo I'm using, so
> guess I'll have to learn how to compile it myself; never compiled a
> package before.
>
> On Mon, Sep 11, 2017 at 4:17 PM, Yuri <yvoinov at gmail.com
> <mailto:yvoinov at gmail.com>> wrote:
>
>     Hardly,
>
>     most probably something in repo's package. However, upgrade is
>     always recommended, especially with modern functionality. It
>     changes fast enough.
>
>     12.09.2017 2:15, Rohit Sodhia ?????:
>>     Ah. I'm on 3.5.20; not sure how far back that is. Is that the
>>     core of the problem?
>>
>>     On Mon, Sep 11, 2017 at 4:07 PM, Yuri <yvoinov at gmail.com
>>     <mailto:yvoinov at gmail.com>> wrote:
>>
>>         Seems latest 4.0.21 is good enough. Most critical SSL-related
>>         bugs almost closed or closed.
>>
>>         At least latest 3.5.27 is released. AFAIK this is minimum to
>>         problem-free running.
>>
>>         Repositories software sometimes has strange quirks, or
>>         sometimes rancid.
>>
>>         12.09.2017 2:05, Rohit Sodhia ?????:
>>
>>>         I'll try to find it, but I read a few articles/SO questions
>>>         that suggested there were bugs in 4 relating to SSL bumping?
>>>         If they were wrong, I'd be glad to go forward. Should I be
>>>         removing the yum squid package and compile my own? Is 3.5
>>>         problematic besides being old?
>>>
>>>         On Mon, Sep 11, 2017 at 4:02 PM, Yuri <yvoinov at gmail.com
>>>         <mailto:yvoinov at gmail.com>> wrote:
>>>
>>>             Wait. Squid 3.5.20? So ancient?
>>>
>>>
>>>             12.09.2017 1:58, Rohit Sodhia ?????:
>>>>             sslcrtd_program /usr/lib64/squid/ssl_crtd -s
>>>>             /var/lib/ssl_db -M 4MB
>>>>
>>>>             I used the line from the Stack Overflow question I
>>>>             linked earlier.
>>>>
>>>>             On Mon, Sep 11, 2017 at 3:41 PM, Yuri
>>>>             <yvoinov at gmail.com <mailto:yvoinov at gmail.com>> wrote:
>>>>
>>>>                 Well. Let's check more deep.
>>>>
>>>>                 Show me parameter sslcrtd_program in your squid.conf
>>>>
>>>>
>>>>                 12.09.2017 1:23, Rohit Sodhia ?????:
>>>>>                 Unfortunately, no luck yet. Thank you again for
>>>>>                 your help before.
>>>>>
>>>>>                 I found that the user squid and group squid
>>>>>                 existed already, so I added
>>>>>
>>>>>                 cache_effective_user squid
>>>>>                 cache_effective_group squid
>>>>>
>>>>>                 to my config (first two lines), made sure
>>>>>                 /var/lib/ssl_db and it's contents were set to
>>>>>                 squid:squid and restarted the service, but I'm
>>>>>                 still getting the same error :(
>>>>>
>>>>>                 On Mon, Sep 11, 2017 at 2:42 PM, Rohit Sodhia
>>>>>                 <sodhia.rohit at gmail.com
>>>>>                 <mailto:sodhia.rohit at gmail.com>> wrote:
>>>>>
>>>>>                     I'll try that immediately, thanks! I
>>>>>                     appreciate all your advice; hopefully I won't
>>>>>                     have to reach out again :p
>>>>>
>>>>>                     On Mon, Sep 11, 2017 at 2:39 PM, Yuri
>>>>>                     <yvoinov at gmail.com <mailto:yvoinov at gmail.com>>
>>>>>                     wrote:
>>>>>
>>>>>                         I'm not Linux fanboy, but modern squid
>>>>>                         never runs as root. So, most probably it
>>>>>                         runs as nobody user.
>>>>>
>>>>>                         Ah, yes:
>>>>>
>>>>>                         #? TAG: cache_effective_user
>>>>>                         #??? If you start Squid as root, it will
>>>>>                         change its effective/real
>>>>>                         #??? UID/GID to the user specified below.?
>>>>>                         The default is to change
>>>>>                         #??? to UID of nobody.
>>>>>                         #??? see also; cache_effective_group
>>>>>                         #Default:
>>>>>                         # cache_effective_user nobody
>>>>>
>>>>>                         #? TAG: cache_effective_group
>>>>>                         #??? Squid sets the GID to the effective
>>>>>                         user's default group ID
>>>>>                         #??? (taken from the password file) and
>>>>>                         supplementary group list
>>>>>                         #??? from the groups membership.
>>>>>                         #
>>>>>                         #??? If you want Squid to run with a
>>>>>                         specific GID regardless of
>>>>>                         #??? the group memberships of the
>>>>>                         effective user then set this
>>>>>                         #??? to the group (or GID) you want Squid
>>>>>                         to run as. When set
>>>>>                         #??? all other group privileges of the
>>>>>                         effective user are ignored
>>>>>                         #??? and only this GID is effective. If
>>>>>                         Squid is not started as
>>>>>                         #??? root the user starting Squid MUST be
>>>>>                         member of the specified
>>>>>                         #??? group.
>>>>>                         #
>>>>>                         #??? This option is not recommended by the
>>>>>                         Squid Team.
>>>>>                         #??? Our preference is for administrators
>>>>>                         to configure a secure
>>>>>                         #??? user account for squid with UID/GID
>>>>>                         matching system policies.
>>>>>                         #Default:
>>>>>                         # Use system group memberships of the
>>>>>                         cache_effective_user account
>>>>>
>>>>>                         As documented. :)
>>>>>
>>>>>                         AFAIK best solution is create
>>>>>                         non-privileged group & user (like
>>>>>                         squid/squid) and set both this parameters
>>>>>                         explicity.
>>>>>
>>>>>                         Then change owner recursively on SSL cache
>>>>>                         to this user.
>>>>>
>>>>>
>>>>>                         12.09.2017 0:36, Rohit Sodhia ?????:
>>>>>>                         Neither of those values are set in my
>>>>>>                         config. Even though I'm not using squid
>>>>>>                         for caching, I need those values? They
>>>>>>                         aren't set in the default configs either.
>>>>>>
>>>>>>                         On Mon, Sep 11, 2017 at 2:33 PM, Yuri
>>>>>>                         <yvoinov at gmail.com
>>>>>>                         <mailto:yvoinov at gmail.com>> wrote:
>>>>>>
>>>>>>                             Most probably you squid runs as
>>>>>>                             another user than squid.
>>>>>>
>>>>>>                             Check your squid.conf for
>>>>>>                             cache_effective_user and
>>>>>>                             cache_effective_group values.
>>>>>>
>>>>>>                             Then change SSL cache permissions to
>>>>>>                             this values. Should work.
>>>>>>
>>>>>>
>>>>>>                             12.09.2017 0:30, Rohit Sodhia ?????:
>>>>>>>                             Thanks for the feedback! I just used
>>>>>>>                             yum (it's a CentOS 7 VB) and it set
>>>>>>>                             it up like that. I changed the owner
>>>>>>>                             and group to squid:squid and tried
>>>>>>>                             restarting squid, but still get the
>>>>>>>                             same errors. I thought to run the
>>>>>>>                             command again, but this time it says
>>>>>>>
>>>>>>>                             /usr/lib64/squid/ssl_crtd: Cannot
>>>>>>>                             create /var/lib/ssl_db
>>>>>>>
>>>>>>>                             If this folder has incorrect
>>>>>>>                             permissions are there possibly other
>>>>>>>                             permission issues?
>>>>>>>
>>>>>>>                             On Mon, Sep 11, 2017 at 2:25 PM,
>>>>>>>                             Yuri <yvoinov at gmail.com
>>>>>>>                             <mailto:yvoinov at gmail.com>> wrote:
>>>>>>>
>>>>>>>                                 Here you root of problem.
>>>>>>>
>>>>>>>                                 Should be (on my setups):
>>>>>>>
>>>>>>>                                 # ls -al /var/lib/ssl_db
>>>>>>>                                 total 326
>>>>>>>                                 drwxr-xr-x 3 squid squid????? 5
>>>>>>>                                 Sep? 5 00:53 .
>>>>>>>                                 drwxr-xr-x 8 root? other????? 8
>>>>>>>                                 Sep? 5 00:53 ..
>>>>>>>                                 drwxr-xr-x 2 squid squid??? 454
>>>>>>>                                 Sep 11 23:37 certs
>>>>>>>                                 -rw-r--r-- 1 squid squid 280575
>>>>>>>                                 Sep 11 23:37 index.txt
>>>>>>>                                 -rw-r--r-- 1 squid squid????? 7
>>>>>>>                                 Sep 11 23:37 size
>>>>>>>
>>>>>>>                                 I.e. Squid has no access to SSL
>>>>>>>                                 cache dir structures.
>>>>>>>
>>>>>>>
>>>>>>>                                 12.09.2017 0:23, Rohit Sodhia ?????:
>>>>>>>>                                 total 8
>>>>>>>>                                 drwxr-xr-x.? 3 root root?? 48
>>>>>>>>                                 Sep 11 12:42 .
>>>>>>>>                                 drwxr-xr-x. 32 root root 4096
>>>>>>>>                                 Sep 11 12:42 ..
>>>>>>>>                                 drwxr-xr-x.? 2 root root??? 6
>>>>>>>>                                 Sep 11 12:42 certs
>>>>>>>>                                 -rw-r--r--.? 1 root root??? 0
>>>>>>>>                                 Sep 11 12:42 index.txt
>>>>>>>>                                 -rw-r--r--.? 1 root root??? 1
>>>>>>>>                                 Sep 11 12:42 size
>>>>>>>>
>>>>>>>>
>>>>>>>>                                 On Mon, Sep 11, 2017 at 2:22
>>>>>>>>                                 PM, Yuri <yvoinov at gmail.com
>>>>>>>>                                 <mailto:yvoinov at gmail.com>> wrote:
>>>>>>>>
>>>>>>>>                                     Show output of
>>>>>>>>
>>>>>>>>                                     ls -al /var/lib/ssl_db
>>>>>>>>
>>>>>>>>
>>>>>>>>                                     12.09.2017 0:21, Rohit
>>>>>>>>                                     Sodhia ?????:
>>>>>>>>>                                     Yes, but telling me it's
>>>>>>>>>                                     crashing unfortunately
>>>>>>>>>                                     doesn't help me figure out
>>>>>>>>>                                     why or how to fix it. I've
>>>>>>>>>                                     run the command it
>>>>>>>>>                                     suggests but it doesn't
>>>>>>>>>                                     help. I'm unfortunately
>>>>>>>>>                                     not an ops guy familiar
>>>>>>>>>                                     with this kind of stuff; I
>>>>>>>>>                                     don't see anything on how
>>>>>>>>>                                     to figure out what to do
>>>>>>>>>                                     about it.
>>>>>>>>>
>>>>>>>>>                                     On Mon, Sep 11, 2017 at
>>>>>>>>>                                     2:17 PM, Yuri
>>>>>>>>>                                     <yvoinov at gmail.com
>>>>>>>>>                                     <mailto:yvoinov at gmail.com>>
>>>>>>>>>                                     wrote:
>>>>>>>>>
>>>>>>>>>                                         It tells you what's
>>>>>>>>>                                         happens.
>>>>>>>>>
>>>>>>>>>
>>>>>>>>>                                         11.09.2017 23:50,
>>>>>>>>>                                         Rohit Sodhia ?????:
>>>>>>>>>                                         > (ssl_crtd):
>>>>>>>>>                                         Uninitialized SSL
>>>>>>>>>                                         certificate database
>>>>>>>>>                                         directory:
>>>>>>>>>                                         > /var/lib/ssl_db. To
>>>>>>>>>                                         initialize, run
>>>>>>>>>                                         "ssl_crtd -c -s
>>>>>>>>>                                         /var/lib/ssl_db".
>>>>>>>>>
>>>>>>>>>
>>>>>>>>>
>>>>>>>>>                                         _______________________________________________
>>>>>>>>>                                         squid-users mailing list
>>>>>>>>>                                         squid-users at lists.squid-cache.org
>>>>>>>>>                                         <mailto:squid-users at lists.squid-cache.org>
>>>>>>>>>                                         http://lists.squid-cache.org/listinfo/squid-users
>>>>>>>>>                                         <http://lists.squid-cache.org/listinfo/squid-users>
>>>>>>>>>
>>>>>>>>>
>>>>>>>>
>>>>>>>>
>>>>>>>
>>>>>>>
>>>>>>
>>>>>>
>>>>>
>>>>>
>>>>>
>>>>
>>>>
>>>
>>>
>>
>>
>
>

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20170912/163b6f75/attachment.htm>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: signature.asc
Type: application/pgp-signature
Size: 473 bytes
Desc: OpenPGP digital signature
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20170912/163b6f75/attachment.sig>

From adrian.m.miller at gmail.com  Mon Sep 11 20:31:27 2017
From: adrian.m.miller at gmail.com (Adrian Miller)
Date: Tue, 12 Sep 2017 06:31:27 +1000
Subject: [squid-users] squid-users Digest, Vol 37, Issue 30
In-Reply-To: <CAOLOQx0tdoymMbORG22mULbXPqa=K7k+e=rNVSk2CLsxUGbN3A@mail.gmail.com>
References: <mailman.1274.1505161142.3209.squid-users@lists.squid-cache.org>
 <CAOLOQx0Uj0sdRU-h2Jep0qxV6C4Dxm+j=Z3Zt3kkcAZj_15RPA@mail.gmail.com>
 <CAOLOQx0tdoymMbORG22mULbXPqa=K7k+e=rNVSk2CLsxUGbN3A@mail.gmail.com>
Message-ID: <CAOLOQx1OGKY55C6J_VWqKod-YXQ6S8Ps6qmucq5y-Xz8CgC_gA@mail.gmail.com>

Jesus, never seen so many messages that could have been answered by reading
the basic squid docs.

Tempted to unsub....sheesh

On 12 Sep. 2017 6:19 am, <squid-users-request at lists.squid-cache.org> wrote:

Send squid-users mailing list submissions to
        squid-users at lists.squid-cache.org

To subscribe or unsubscribe via the World Wide Web, visit
        http://lists.squid-cache.org/listinfo/squid-users
or, via email, send a message with subject or body 'help' to
        squid-users-request at lists.squid-cache.org

You can reach the person managing the list at
        squid-users-owner at lists.squid-cache.org

When replying, please edit your Subject line so it is more specific
than "Re: Contents of squid-users digest..."


Today's Topics:

   1. Re: Need assistance debugging Squid error: ssl_ctrd helpers
      crashing too quickly (Rohit Sodhia)


----------------------------------------------------------------------

Message: 1
Date: Mon, 11 Sep 2017 16:18:39 -0400
From: Rohit Sodhia <sodhia.rohit at gmail.com>
To: Yuri <yvoinov at gmail.com>
Cc: squid-users at lists.squid-cache.org
Subject: Re: [squid-users] Need assistance debugging Squid error:
        ssl_ctrd helpers crashing too quickly
Message-ID:
        <CAN1w9tfQt3Mivwpyo+u3Qp0agQ8pOgz2MGo2Wvb5AdGU3zbkjw at mail.gmail.com>
Content-Type: text/plain; charset="utf-8"

Ok. Looks like 3.5.20 is the latest on the yum repo I'm using, so guess
I'll have to learn how to compile it myself; never compiled a package
before.

On Mon, Sep 11, 2017 at 4:17 PM, Yuri <yvoinov at gmail.com> wrote:

> Hardly,
>
> most probably something in repo's package. However, upgrade is always
> recommended, especially with modern functionality. It changes fast enough.
>
> 12.09.2017 2:15, Rohit Sodhia ?????:
>
> Ah. I'm on 3.5.20; not sure how far back that is. Is that the core of the
> problem?
>
> On Mon, Sep 11, 2017 at 4:07 PM, Yuri <yvoinov at gmail.com> wrote:
>
>> Seems latest 4.0.21 is good enough. Most critical SSL-related bugs almost
>> closed or closed.
>>
>> At least latest 3.5.27 is released. AFAIK this is minimum to problem-free
>> running.
>>
>> Repositories software sometimes has strange quirks, or sometimes rancid.
>> 12.09.2017 2:05, Rohit Sodhia ?????:
>>
>> I'll try to find it, but I read a few articles/SO questions that
>> suggested there were bugs in 4 relating to SSL bumping? If they were
wrong,
>> I'd be glad to go forward. Should I be removing the yum squid package and
>> compile my own? Is 3.5 problematic besides being old?
>>
>> On Mon, Sep 11, 2017 at 4:02 PM, Yuri <yvoinov at gmail.com> wrote:
>>
>>> Wait. Squid 3.5.20? So ancient?
>>>
>>> 12.09.2017 1:58, Rohit Sodhia ?????:
>>>
>>> sslcrtd_program /usr/lib64/squid/ssl_crtd -s /var/lib/ssl_db -M 4MB
>>>
>>> I used the line from the Stack Overflow question I linked earlier.
>>>
>>> On Mon, Sep 11, 2017 at 3:41 PM, Yuri <yvoinov at gmail.com> wrote:
>>>
>>>> Well. Let's check more deep.
>>>>
>>>> Show me parameter sslcrtd_program in your squid.conf
>>>>
>>>> 12.09.2017 1:23, Rohit Sodhia ?????:
>>>>
>>>> Unfortunately, no luck yet. Thank you again for your help before.
>>>>
>>>> I found that the user squid and group squid existed already, so I added
>>>>
>>>> cache_effective_user squid
>>>> cache_effective_group squid
>>>>
>>>> to my config (first two lines), made sure /var/lib/ssl_db and it's
>>>> contents were set to squid:squid and restarted the service, but I'm
still
>>>> getting the same error :(
>>>>
>>>> On Mon, Sep 11, 2017 at 2:42 PM, Rohit Sodhia <sodhia.rohit at gmail.com>
>>>> wrote:
>>>>
>>>>> I'll try that immediately, thanks! I appreciate all your advice;
>>>>> hopefully I won't have to reach out again :p
>>>>>
>>>>> On Mon, Sep 11, 2017 at 2:39 PM, Yuri <yvoinov at gmail.com> wrote:
>>>>>
>>>>>> I'm not Linux fanboy, but modern squid never runs as root. So, most
>>>>>> probably it runs as nobody user.
>>>>>>
>>>>>> Ah, yes:
>>>>>>
>>>>>> #  TAG: cache_effective_user
>>>>>> #    If you start Squid as root, it will change its effective/real
>>>>>> #    UID/GID to the user specified below.  The default is to change
>>>>>> #    to UID of nobody.
>>>>>> #    see also; cache_effective_group
>>>>>> #Default:
>>>>>> # cache_effective_user nobody
>>>>>>
>>>>>> #  TAG: cache_effective_group
>>>>>> #    Squid sets the GID to the effective user's default group ID
>>>>>> #    (taken from the password file) and supplementary group list
>>>>>> #    from the groups membership.
>>>>>> #
>>>>>> #    If you want Squid to run with a specific GID regardless of
>>>>>> #    the group memberships of the effective user then set this
>>>>>> #    to the group (or GID) you want Squid to run as. When set
>>>>>> #    all other group privileges of the effective user are ignored
>>>>>> #    and only this GID is effective. If Squid is not started as
>>>>>> #    root the user starting Squid MUST be member of the specified
>>>>>> #    group.
>>>>>> #
>>>>>> #    This option is not recommended by the Squid Team.
>>>>>> #    Our preference is for administrators to configure a secure
>>>>>> #    user account for squid with UID/GID matching system policies.
>>>>>> #Default:
>>>>>> # Use system group memberships of the cache_effective_user account
>>>>>>
>>>>>> As documented. :)
>>>>>>
>>>>>> AFAIK best solution is create non-privileged group & user (like
>>>>>> squid/squid) and set both this parameters explicity.
>>>>>>
>>>>>> Then change owner recursively on SSL cache to this user.
>>>>>>
>>>>>> 12.09.2017 0:36, Rohit Sodhia ?????:
>>>>>>
>>>>>> Neither of those values are set in my config. Even though I'm not
>>>>>> using squid for caching, I need those values? They aren't set in the
>>>>>> default configs either.
>>>>>>
>>>>>> On Mon, Sep 11, 2017 at 2:33 PM, Yuri <yvoinov at gmail.com> wrote:
>>>>>>
>>>>>>> Most probably you squid runs as another user than squid.
>>>>>>>
>>>>>>> Check your squid.conf for cache_effective_user and
>>>>>>> cache_effective_group values.
>>>>>>>
>>>>>>> Then change SSL cache permissions to this values. Should work.
>>>>>>>
>>>>>>> 12.09.2017 0:30, Rohit Sodhia ?????:
>>>>>>>
>>>>>>> Thanks for the feedback! I just used yum (it's a CentOS 7 VB) and it
>>>>>>> set it up like that. I changed the owner and group to squid:squid
and tried
>>>>>>> restarting squid, but still get the same errors. I thought to run
the
>>>>>>> command again, but this time it says
>>>>>>>
>>>>>>> /usr/lib64/squid/ssl_crtd: Cannot create /var/lib/ssl_db
>>>>>>>
>>>>>>> If this folder has incorrect permissions are there possibly other
>>>>>>> permission issues?
>>>>>>>
>>>>>>> On Mon, Sep 11, 2017 at 2:25 PM, Yuri <yvoinov at gmail.com> wrote:
>>>>>>>
>>>>>>>> Here you root of problem.
>>>>>>>>
>>>>>>>> Should be (on my setups):
>>>>>>>>
>>>>>>>> # ls -al /var/lib/ssl_db
>>>>>>>> total 326
>>>>>>>> drwxr-xr-x 3 squid squid      5 Sep  5 00:53 .
>>>>>>>> drwxr-xr-x 8 root  other      8 Sep  5 00:53 ..
>>>>>>>> drwxr-xr-x 2 squid squid    454 Sep 11 23:37 certs
>>>>>>>> -rw-r--r-- 1 squid squid 280575 Sep 11 23:37 index.txt
>>>>>>>> -rw-r--r-- 1 squid squid      7 Sep 11 23:37 size
>>>>>>>>
>>>>>>>> I.e. Squid has no access to SSL cache dir structures.
>>>>>>>>
>>>>>>>> 12.09.2017 0:23, Rohit Sodhia ?????:
>>>>>>>>
>>>>>>>> total 8
>>>>>>>> drwxr-xr-x.  3 root root   48 Sep 11 12:42 .
>>>>>>>> drwxr-xr-x. 32 root root 4096 Sep 11 12:42 ..
>>>>>>>> drwxr-xr-x.  2 root root    6 Sep 11 12:42 certs
>>>>>>>> -rw-r--r--.  1 root root    0 Sep 11 12:42 index.txt
>>>>>>>> -rw-r--r--.  1 root root    1 Sep 11 12:42 size
>>>>>>>>
>>>>>>>>
>>>>>>>> On Mon, Sep 11, 2017 at 2:22 PM, Yuri <yvoinov at gmail.com> wrote:
>>>>>>>>
>>>>>>>>> Show output of
>>>>>>>>>
>>>>>>>>> ls -al /var/lib/ssl_db
>>>>>>>>>
>>>>>>>>> 12.09.2017 0:21, Rohit Sodhia ?????:
>>>>>>>>>
>>>>>>>>> Yes, but telling me it's crashing unfortunately doesn't help me
>>>>>>>>> figure out why or how to fix it. I've run the command it suggests
but it
>>>>>>>>> doesn't help. I'm unfortunately not an ops guy familiar with this
kind of
>>>>>>>>> stuff; I don't see anything on how to figure out what to do about
it.
>>>>>>>>>
>>>>>>>>> On Mon, Sep 11, 2017 at 2:17 PM, Yuri <yvoinov at gmail.com> wrote:
>>>>>>>>>
>>>>>>>>>> It tells you what's happens.
>>>>>>>>>>
>>>>>>>>>>
>>>>>>>>>> 11.09.2017 23:50, Rohit Sodhia ?????:
>>>>>>>>>> > (ssl_crtd): Uninitialized SSL certificate database directory:
>>>>>>>>>> > /var/lib/ssl_db. To initialize, run "ssl_crtd -c -s
>>>>>>>>>> /var/lib/ssl_db".
>>>>>>>>>>
>>>>>>>>>>
>>>>>>>>>>
>>>>>>>>>> _______________________________________________
>>>>>>>>>> squid-users mailing list
>>>>>>>>>> squid-users at lists.squid-cache.org
>>>>>>>>>> http://lists.squid-cache.org/listinfo/squid-users
>>>>>>>>>>
>>>>>>>>>>
>>>>>>>>>
>>>>>>>>>
>>>>>>>>
>>>>>>>>
>>>>>>>
>>>>>>>
>>>>>>
>>>>>>
>>>>>
>>>>
>>>>
>>>
>>>
>>
>>
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/
attachments/20170911/2c3ab1ef/attachment.html>

------------------------------

Subject: Digest Footer

_______________________________________________
squid-users mailing list
squid-users at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-users


------------------------------

End of squid-users Digest, Vol 37, Issue 30
*******************************************
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20170912/16506679/attachment.htm>

From erdosain9 at gmail.com  Mon Sep 11 20:45:44 2017
From: erdosain9 at gmail.com (erdosain9)
Date: Mon, 11 Sep 2017 13:45:44 -0700 (MST)
Subject: [squid-users] ipcCreate: fork: (12) Cannot allocate memory
In-Reply-To: <7609a420-c260-cf57-1e57-e26058ff4b90@treenet.co.nz>
References: <1504795493107-0.post@n4.nabble.com>
 <1504795733032-0.post@n4.nabble.com>
 <7609a420-c260-cf57-1e57-e26058ff4b90@treenet.co.nz>
Message-ID: <1505162744886-0.post@n4.nabble.com>

Ok, thanks

i grow the swap

[root at squid /]# free -h
              total        used        free      shared  buff/cache  
available
Mem:           3,7G        1,0G        117M         29M        2,6G       
2,4G
Swap:          6,0G        124M        5,9G


related to swappiness what would be a good value??

i have this

 cat /proc/sys/vm/swappiness
30

cat /proc/sys/vm/vfs_cache_pressure

100

Thanks!



--
Sent from: http://squid-web-proxy-cache.1019090.n4.nabble.com/Squid-Users-f1019091.html


From squid3 at treenet.co.nz  Tue Sep 12 04:08:49 2017
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Tue, 12 Sep 2017 16:08:49 +1200
Subject: [squid-users] Need assistance debugging Squid error: ssl_ctrd
 helpers crashing too quickly
In-Reply-To: <f0d313ab-2c69-0237-4207-6342358b8ff0@gmail.com>
References: <CAN1w9teczQdmfM8s-xe8H5kLgMPwyh6+cgRvJqV4owPSjRHcNw@mail.gmail.com>
 <CAN1w9tf=HzUHEBcpwtvEeoNc4DV5iq=aMFoZKubXjwxTZq1wfQ@mail.gmail.com>
 <3a026389-5c8b-ca43-f909-a8e46a5b1faa@gmail.com>
 <CAN1w9tf0+h6W_T4kn49_70rexuQ6=Wy9hgTbE2mTPANE-hh3oQ@mail.gmail.com>
 <ea4740ba-1706-df39-32c8-120a9740181e@gmail.com>
 <CAN1w9teM35_=1X7CcvnMb9LtXRKW0okDBBN9aqO5-UKMdcGwaA@mail.gmail.com>
 <CAN1w9tcuVmZnQV+4aj=ZXD=rwBeOUUaHq7xOJkoGurGeq-=nwQ@mail.gmail.com>
 <75a12def-048c-7519-5b9a-fc767579285d@gmail.com>
 <CAN1w9tfm8UCSW-bVhL-fOCFZD9DzxdJzW0cXyNczbwddN681jw@mail.gmail.com>
 <b55ec078-e25a-4c83-d5b2-6ea624699996@gmail.com>
 <CAN1w9tcg+gysMsCWVK_OC46Uf=6tN2uVtXL61uVjvYALe_CuSA@mail.gmail.com>
 <5fc76932-c22f-b423-3b4d-64c2416e8b7e@gmail.com>
 <CAN1w9teamDf9LS0z8T2pf3P07oikqea6qA0PWD_bHa1_n03GsA@mail.gmail.com>
 <c9bd31f1-f877-2584-35ce-fe570e3fe098@gmail.com>
 <CAN1w9tfQt3Mivwpyo+u3Qp0agQ8pOgz2MGo2Wvb5AdGU3zbkjw@mail.gmail.com>
 <f0d313ab-2c69-0237-4207-6342358b8ff0@gmail.com>
Message-ID: <e76b21e7-72d9-98b4-75bd-c321ddded06a@treenet.co.nz>

Hi guys,

  You got so close but not quite.

Rohit;

* check your running Squid to see what user account it is using. You 
should not need to configure the effective user explicitly (unless it is 
that 'nobody' account - best prevent that account from playing with cert 
creation).

* Remove the ssl_db directory you have that was not working and create 
one fresh with write permissions to the Squid user *and* group. Note 
that is the top level ssl_db directory only.

* run restorecon on the new directory. This is needed for the create to 
work properly when SELinux is present.

* then run the ssl_crtd command _as the Squid user account_ ("su squid" 
or "sudo -i -u squid").

* run restorecon *again* on the formatted directory structure. This is 
needed for the normal Squid uses to work properly when SELinux is present.


That should be all that is needed to use this helper.


As for upgrades, yes it would be a good idea regardless of this issue. 
3.5.20 was July 2016 release[1] and its best not to be more than a month 
or two behind with ssl-bump things. Eliezers packages[2] should be okay 
if you want to avoid compiling.

[1] <http://www.squid-cache.org/Versions/v3/3.5/>
[2] <https://wiki.squid-cache.org/KnowledgeBase/RedHat> this page was 
badly out of date sorry, now updated.

Amos


From rentorbuy at yahoo.com  Tue Sep 12 10:59:26 2017
From: rentorbuy at yahoo.com (Vieri)
Date: Tue, 12 Sep 2017 10:59:26 +0000 (UTC)
Subject: [squid-users] squid cache takes a break
In-Reply-To: <1349ed09-a160-3dce-9a13-cb55e162b2a0@treenet.co.nz>
References: <1234530834.188010.1504892340901.ref@mail.yahoo.com>
 <1234530834.188010.1504892340901@mail.yahoo.com>
 <ef119e7a-6b6f-0a22-f903-a257ecb0af83@treenet.co.nz>
 <500349185.1012564.1505119741831@mail.yahoo.com>
 <1349ed09-a160-3dce-9a13-cb55e162b2a0@treenet.co.nz>
Message-ID: <1647206411.239445.1505213966573@mail.yahoo.com>


________________________________
From: Amos Jeffries <squid3 at treenet.co.nz>
>
> That is all it needs to do to begin with; parse off the numeric value 
> from the input line and send it back as prefix on the output line. The 

> helper does not need threading or anything particularly special for the > minimal support.


I thought it had to be asynchronous.
The docs say "Only used with helpers capable of processing more than one query at a time."

Example:
Squid sends "1 URI1" (or whatever) to the helper.
It does not wait for an immediate response.
In fact, Squid can send "2 URI2" before getting the reply to ID 1, right?
In my case, the helper is synchronous, non-MT. I don't think it will improve the time responses per-se.

In any case, my helper won't be able to process more than one query AT A TIME.

I tried it anyway. So here's the relevant code:

while( <STDIN> )
{
s/^\s*//;
s/\s*$//;
my @squidin = split;
my $squidn = scalar @squidin;
undef $url;
undef $channelid;
if ( ($squidn == 2 ) && (defined $squidin[0]) && ($squidin[0] =~ /^\d+?$/) ) {
$channelid = $squidin[0];
$url = $squidin[1] if (defined $squidin[1]);
} else {
$url = $squidin[0] if (defined $squidin[0]);
}

[...]
logtofile("Channel-ID: ".$channelid."\n") if ((defined $channelid) && ($debug >= 1));

[...do DB lookups, reply accordingly...]

if (defined $channelid) {
print( $channelid." OK\n" );
logtofile( $channelid." OK\n" ) if ($debug >= 1);
} else {
print( "OK\n" );
logtofile( "OK\n" ) if ($debug >= 1);
}
[...similar responses for ERR messages...]
}

Here's the relevant squid.conf line:
external_acl_type bllookup ttl=86400 negative_ttl=86400 children-max=80 children-startup=10 children-idle=3 concurrency=8 %URI /opt/custom/scripts/run/scripts/firewall/squid_url_lookup.pl [...]

How can I check in the Squid log that concurrency is "working"?

If the helper logs to a text file as in the trimmed code above, I notice that the channel ID is always 0. I get messages such as:
Channel-ID: 0
0 OK
Channel-ID: 0
0 ERR ...

Is this expected?

Despite this, I can see that the number of helper processes does not increase over time for now, and that HTTP/S client browsing is responsive enough.
# ps aux | grep -c squid_url_lookup.pl
11

One last thing. I'm using:
cache_dir diskd /var/cache/squid 100 16 256
I may want to try to comment out this directive for improved I/O performance.

Thanks,

Vieri


From Joseph.Garbacik at netapp.com  Tue Sep 12 12:01:54 2017
From: Joseph.Garbacik at netapp.com (Garbacik, Joe)
Date: Tue, 12 Sep 2017 12:01:54 +0000
Subject: [squid-users] Squid -k parse -f alternateconfig file
Message-ID: <3F70ECD6-F5BF-4BFF-8848-93B3EA914C99@contoso.com>

I am using an alternate filename for my squid configuration and am trying to validate the file below going live with it. Should I be able to use both the ?k parse and the ?f filename options at the same time? It looks like ?-k parse? is defaulting to only use only /etc/squid/squid.conf.
# squid ?f /etc/squid/squid-20170912a.conf -k parse
2017/09/12 07:56:05| Startup: Initializing Authentication Schemes ...
2017/09/12 07:56:05| Startup: Initialized Authentication Scheme 'basic'
2017/09/12 07:56:05| Startup: Initialized Authentication Scheme 'digest'
2017/09/12 07:56:05| Startup: Initialized Authentication Scheme 'negotiate'
2017/09/12 07:56:05| Startup: Initialized Authentication Scheme 'ntlm'
2017/09/12 07:56:05| Startup: Initialized Authentication.
2017/09/12 07:56:05| Processing Configuration File: /etc/squid/squid.conf (depth 0)
FATAL: Unable to open configuration file: /etc/squid/squid.conf: (2) No such file or directory
Squid Cache (Version 3.5.20): Terminated abnormally.
CPU Usage: 0.014 seconds = 0.006 user + 0.008 sys
Maximum Resident Size: 29344 KB
Page faults with physical i/o: 0

Thanks.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20170912/47e8ce72/attachment.htm>

From squid3 at treenet.co.nz  Tue Sep 12 13:07:32 2017
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Wed, 13 Sep 2017 01:07:32 +1200
Subject: [squid-users] squid cache takes a break
In-Reply-To: <1647206411.239445.1505213966573@mail.yahoo.com>
References: <1234530834.188010.1504892340901.ref@mail.yahoo.com>
 <1234530834.188010.1504892340901@mail.yahoo.com>
 <ef119e7a-6b6f-0a22-f903-a257ecb0af83@treenet.co.nz>
 <500349185.1012564.1505119741831@mail.yahoo.com>
 <1349ed09-a160-3dce-9a13-cb55e162b2a0@treenet.co.nz>
 <1647206411.239445.1505213966573@mail.yahoo.com>
Message-ID: <0c8dab7f-7db1-5f58-24d7-3b7e066e2f7c@treenet.co.nz>

On 12/09/17 22:59, Vieri wrote:
> 
> ________________________________
> From: Amos Jeffries <squid3 at treenet.co.nz>
>>
>> That is all it needs to do to begin with; parse off the numeric value
>> from the input line and send it back as prefix on the output line. The
> 
>> helper does not need threading or anything particularly special for the > minimal support.
> 
> 
> I thought it had to be asynchronous.
> The docs say "Only used with helpers capable of processing more than one query at a time."
> 
> Example:
> Squid sends "1 URI1" (or whatever) to the helper.
> It does not wait for an immediate response.
> In fact, Squid can send "2 URI2" before getting the reply to ID 1, right?

Yes.


> In my case, the helper is synchronous, non-MT. I don't think it will improve the time responses per-se.
> 
> In any case, my helper won't be able to process more than one query AT A TIME.
> 
> I tried it anyway. So here's the relevant code:
> 
> while( <STDIN> )
> {
> s/^\s*//;
> s/\s*$//;
> my @squidin = split;
> my $squidn = scalar @squidin;
> undef $url;
> undef $channelid;
> if ( ($squidn == 2 ) && (defined $squidin[0]) && ($squidin[0] =~ /^\d+?$/) ) {
> $channelid = $squidin[0];
> $url = $squidin[1] if (defined $squidin[1]);
> } else {
> $url = $squidin[0] if (defined $squidin[0]);
> }
> 
> [...]
> logtofile("Channel-ID: ".$channelid."\n") if ((defined $channelid) && ($debug >= 1));
> 
> [...do DB lookups, reply accordingly...]
> 
> if (defined $channelid) {
> print( $channelid." OK\n" );
> logtofile( $channelid." OK\n" ) if ($debug >= 1);
> } else {
> print( "OK\n" );
> logtofile( "OK\n" ) if ($debug >= 1);
> }
> [...similar responses for ERR messages...]
> }
> 
> Here's the relevant squid.conf line:
> external_acl_type bllookup ttl=86400 negative_ttl=86400 children-max=80 children-startup=10 children-idle=3 concurrency=8 %URI /opt/custom/scripts/run/scripts/firewall/squid_url_lookup.pl [...]
> 
> How can I check in the Squid log that concurrency is "working"?

Section 82, level 2 or 4 should log the queries.

Better info is in the cachemgr/squidclient "external_acl" report. Each 
helper is listed with its total and summary stats for each helper child.

> 
> If the helper logs to a text file as in the trimmed code above, I notice that the channel ID is always 0. I get messages such as:
> Channel-ID: 0
> 0 OK
> Channel-ID: 0
> 0 ERR ...
> 
> Is this expected?

Maybe.

If you make the helper pause a bit and throw a large number of different 
URLs at Squid you should see it grow a bit higher than 0.

> 
> Despite this, I can see that the number of helper processes does not increase over time for now, and that HTTP/S client browsing is responsive enough.
> # ps aux | grep -c squid_url_lookup.pl
> 11
> 

Yay.

> One last thing. I'm using:
> cache_dir diskd /var/cache/squid 100 16 256
> I may want to try to comment out this directive for improved I/O performance.
> 
> Thanks,
> 
> Vieri

Cheers
Amos


From squid3 at treenet.co.nz  Tue Sep 12 13:11:17 2017
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Wed, 13 Sep 2017 01:11:17 +1200
Subject: [squid-users] Squid -k parse -f alternateconfig file
In-Reply-To: <3F70ECD6-F5BF-4BFF-8848-93B3EA914C99@contoso.com>
References: <3F70ECD6-F5BF-4BFF-8848-93B3EA914C99@contoso.com>
Message-ID: <64ffff82-7bcc-49a2-e49e-89fc96ed7b6e@treenet.co.nz>

On 13/09/17 00:01, Garbacik, Joe wrote:
> I am using an alternate filename for my squid configuration and am 
> trying to validate the file below going live with it. Should I be able 
> to use both the ?k parse and the ?f filename options at the same time? 

Yes.

> It looks like ?-k parse? is defaulting to only use only 
> /etc/squid/squid.conf.
> 
> # squid ?f /etc/squid/squid-20170912a.conf -k parse

If that is a cut-n-paste the '-' on the 'f' is not a hyphen.

Otherwise, I'm not sure whats going on here. We have config regression 
tests and other things explicitly using the -f and -k in exactly the 
manner you are trying to use, so it is definitely working and checked 
regularly.

Amos


From eliezer at ngtech.co.il  Tue Sep 12 20:51:36 2017
From: eliezer at ngtech.co.il (Eliezer Croitoru)
Date: Tue, 12 Sep 2017 23:51:36 +0300
Subject: [squid-users] squid cache takes a break
In-Reply-To: <0c8dab7f-7db1-5f58-24d7-3b7e066e2f7c@treenet.co.nz>
References: <1234530834.188010.1504892340901.ref@mail.yahoo.com>
 <1234530834.188010.1504892340901@mail.yahoo.com>
 <ef119e7a-6b6f-0a22-f903-a257ecb0af83@treenet.co.nz>
 <500349185.1012564.1505119741831@mail.yahoo.com>
 <1349ed09-a160-3dce-9a13-cb55e162b2a0@treenet.co.nz>
 <1647206411.239445.1505213966573@mail.yahoo.com>
 <0c8dab7f-7db1-5f58-24d7-3b7e066e2f7c@treenet.co.nz>
Message-ID: <042e01d32c08$ec7ea680$c57bf380$@ngtech.co.il>

I just must add that if you understand how TCP works(which the helpers use to communicate with squid) then it makes sense that it is possible that...
The sender (ie squid) sent 100 lines but the client software yet to process them since it's in the OS or other software\hardware related buffer.

For me it was hard to understand at first since it's an STDIN\STDOUT interface so it would block after every write but it's not...

There is a possibility that if the helper can process every incoming request with threading or other method of concurrency then the performance of the helper and by that squid will be better but if only using the basic buffer works fine for you then great.

Eliezer

----
Eliezer Croitoru
Linux System Administrator
Mobile: +972-5-28704261
Email: eliezer at ngtech.co.il



-----Original Message-----
From: squid-users [mailto:squid-users-bounces at lists.squid-cache.org] On Behalf Of Amos Jeffries
Sent: Tuesday, September 12, 2017 16:08
To: squid-users at lists.squid-cache.org
Subject: Re: [squid-users] squid cache takes a break

On 12/09/17 22:59, Vieri wrote:
> 
> ________________________________
> From: Amos Jeffries <squid3 at treenet.co.nz>
>>
>> That is all it needs to do to begin with; parse off the numeric value
>> from the input line and send it back as prefix on the output line. The
> 
>> helper does not need threading or anything particularly special for the > minimal support.
> 
> 
> I thought it had to be asynchronous.
> The docs say "Only used with helpers capable of processing more than one query at a time."
> 
> Example:
> Squid sends "1 URI1" (or whatever) to the helper.
> It does not wait for an immediate response.
> In fact, Squid can send "2 URI2" before getting the reply to ID 1, right?

Yes.


> In my case, the helper is synchronous, non-MT. I don't think it will improve the time responses per-se.
> 
> In any case, my helper won't be able to process more than one query AT A TIME.
> 
> I tried it anyway. So here's the relevant code:
> 
> while( <STDIN> )
> {
> s/^\s*//;
> s/\s*$//;
> my @squidin = split;
> my $squidn = scalar @squidin;
> undef $url;
> undef $channelid;
> if ( ($squidn == 2 ) && (defined $squidin[0]) && ($squidin[0] =~ /^\d+?$/) ) {
> $channelid = $squidin[0];
> $url = $squidin[1] if (defined $squidin[1]);
> } else {
> $url = $squidin[0] if (defined $squidin[0]);
> }
> 
> [...]
> logtofile("Channel-ID: ".$channelid."\n") if ((defined $channelid) && ($debug >= 1));
> 
> [...do DB lookups, reply accordingly...]
> 
> if (defined $channelid) {
> print( $channelid." OK\n" );
> logtofile( $channelid." OK\n" ) if ($debug >= 1);
> } else {
> print( "OK\n" );
> logtofile( "OK\n" ) if ($debug >= 1);
> }
> [...similar responses for ERR messages...]
> }
> 
> Here's the relevant squid.conf line:
> external_acl_type bllookup ttl=86400 negative_ttl=86400 children-max=80 children-startup=10 children-idle=3 concurrency=8 %URI /opt/custom/scripts/run/scripts/firewall/squid_url_lookup.pl [...]
> 
> How can I check in the Squid log that concurrency is "working"?

Section 82, level 2 or 4 should log the queries.

Better info is in the cachemgr/squidclient "external_acl" report. Each 
helper is listed with its total and summary stats for each helper child.

> 
> If the helper logs to a text file as in the trimmed code above, I notice that the channel ID is always 0. I get messages such as:
> Channel-ID: 0
> 0 OK
> Channel-ID: 0
> 0 ERR ...
> 
> Is this expected?

Maybe.

If you make the helper pause a bit and throw a large number of different 
URLs at Squid you should see it grow a bit higher than 0.

> 
> Despite this, I can see that the number of helper processes does not increase over time for now, and that HTTP/S client browsing is responsive enough.
> # ps aux | grep -c squid_url_lookup.pl
> 11
> 

Yay.

> One last thing. I'm using:
> cache_dir diskd /var/cache/squid 100 16 256
> I may want to try to comment out this directive for improved I/O performance.
> 
> Thanks,
> 
> Vieri

Cheers
Amos
_______________________________________________
squid-users mailing list
squid-users at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-users



From yvoinov at gmail.com  Tue Sep 12 20:54:13 2017
From: yvoinov at gmail.com (Yuri)
Date: Wed, 13 Sep 2017 02:54:13 +0600
Subject: [squid-users] squid cache takes a break
In-Reply-To: <042e01d32c08$ec7ea680$c57bf380$@ngtech.co.il>
References: <1234530834.188010.1504892340901.ref@mail.yahoo.com>
 <1234530834.188010.1504892340901@mail.yahoo.com>
 <ef119e7a-6b6f-0a22-f903-a257ecb0af83@treenet.co.nz>
 <500349185.1012564.1505119741831@mail.yahoo.com>
 <1349ed09-a160-3dce-9a13-cb55e162b2a0@treenet.co.nz>
 <1647206411.239445.1505213966573@mail.yahoo.com>
 <0c8dab7f-7db1-5f58-24d7-3b7e066e2f7c@treenet.co.nz>
 <042e01d32c08$ec7ea680$c57bf380$@ngtech.co.il>
Message-ID: <88225e47-4f29-fa65-ec5e-b9b1c85446cd@gmail.com>

It is just enough not to reinvent the wheel. What needs op - already
exists and is called ufdbguard. And it's works perfectly with shallalist :)


13.09.2017 2:51, Eliezer Croitoru ?????:
> I just must add that if you understand how TCP works(which the helpers use to communicate with squid) then it makes sense that it is possible that...
> The sender (ie squid) sent 100 lines but the client software yet to process them since it's in the OS or other software\hardware related buffer.
>
> For me it was hard to understand at first since it's an STDIN\STDOUT interface so it would block after every write but it's not...
>
> There is a possibility that if the helper can process every incoming request with threading or other method of concurrency then the performance of the helper and by that squid will be better but if only using the basic buffer works fine for you then great.
>
> Eliezer
>
> ----
> Eliezer Croitoru
> Linux System Administrator
> Mobile: +972-5-28704261
> Email: eliezer at ngtech.co.il
>
>
>
> -----Original Message-----
> From: squid-users [mailto:squid-users-bounces at lists.squid-cache.org] On Behalf Of Amos Jeffries
> Sent: Tuesday, September 12, 2017 16:08
> To: squid-users at lists.squid-cache.org
> Subject: Re: [squid-users] squid cache takes a break
>
> On 12/09/17 22:59, Vieri wrote:
>> ________________________________
>> From: Amos Jeffries <squid3 at treenet.co.nz>
>>> That is all it needs to do to begin with; parse off the numeric value
>>> from the input line and send it back as prefix on the output line. The
>>> helper does not need threading or anything particularly special for the > minimal support.
>>
>> I thought it had to be asynchronous.
>> The docs say "Only used with helpers capable of processing more than one query at a time."
>>
>> Example:
>> Squid sends "1 URI1" (or whatever) to the helper.
>> It does not wait for an immediate response.
>> In fact, Squid can send "2 URI2" before getting the reply to ID 1, right?
> Yes.
>
>
>> In my case, the helper is synchronous, non-MT. I don't think it will improve the time responses per-se.
>>
>> In any case, my helper won't be able to process more than one query AT A TIME.
>>
>> I tried it anyway. So here's the relevant code:
>>
>> while( <STDIN> )
>> {
>> s/^\s*//;
>> s/\s*$//;
>> my @squidin = split;
>> my $squidn = scalar @squidin;
>> undef $url;
>> undef $channelid;
>> if ( ($squidn == 2 ) && (defined $squidin[0]) && ($squidin[0] =~ /^\d+?$/) ) {
>> $channelid = $squidin[0];
>> $url = $squidin[1] if (defined $squidin[1]);
>> } else {
>> $url = $squidin[0] if (defined $squidin[0]);
>> }
>>
>> [...]
>> logtofile("Channel-ID: ".$channelid."\n") if ((defined $channelid) && ($debug >= 1));
>>
>> [...do DB lookups, reply accordingly...]
>>
>> if (defined $channelid) {
>> print( $channelid." OK\n" );
>> logtofile( $channelid." OK\n" ) if ($debug >= 1);
>> } else {
>> print( "OK\n" );
>> logtofile( "OK\n" ) if ($debug >= 1);
>> }
>> [...similar responses for ERR messages...]
>> }
>>
>> Here's the relevant squid.conf line:
>> external_acl_type bllookup ttl=86400 negative_ttl=86400 children-max=80 children-startup=10 children-idle=3 concurrency=8 %URI /opt/custom/scripts/run/scripts/firewall/squid_url_lookup.pl [...]
>>
>> How can I check in the Squid log that concurrency is "working"?
> Section 82, level 2 or 4 should log the queries.
>
> Better info is in the cachemgr/squidclient "external_acl" report. Each 
> helper is listed with its total and summary stats for each helper child.
>
>> If the helper logs to a text file as in the trimmed code above, I notice that the channel ID is always 0. I get messages such as:
>> Channel-ID: 0
>> 0 OK
>> Channel-ID: 0
>> 0 ERR ...
>>
>> Is this expected?
> Maybe.
>
> If you make the helper pause a bit and throw a large number of different 
> URLs at Squid you should see it grow a bit higher than 0.
>
>> Despite this, I can see that the number of helper processes does not increase over time for now, and that HTTP/S client browsing is responsive enough.
>> # ps aux | grep -c squid_url_lookup.pl
>> 11
>>
> Yay.
>
>> One last thing. I'm using:
>> cache_dir diskd /var/cache/squid 100 16 256
>> I may want to try to comment out this directive for improved I/O performance.
>>
>> Thanks,
>>
>> Vieri
> Cheers
> Amos
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users


-------------- next part --------------
A non-text attachment was scrubbed...
Name: signature.asc
Type: application/pgp-signature
Size: 473 bytes
Desc: OpenPGP digital signature
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20170913/dc10aca7/attachment.sig>

From eliezer at ngtech.co.il  Tue Sep 12 23:33:48 2017
From: eliezer at ngtech.co.il (Eliezer Croitoru)
Date: Wed, 13 Sep 2017 02:33:48 +0300
Subject: [squid-users] squid cache takes a break
In-Reply-To: <88225e47-4f29-fa65-ec5e-b9b1c85446cd@gmail.com>
References: <1234530834.188010.1504892340901.ref@mail.yahoo.com>
 <1234530834.188010.1504892340901@mail.yahoo.com>
 <ef119e7a-6b6f-0a22-f903-a257ecb0af83@treenet.co.nz>
 <500349185.1012564.1505119741831@mail.yahoo.com>
 <1349ed09-a160-3dce-9a13-cb55e162b2a0@treenet.co.nz>
 <1647206411.239445.1505213966573@mail.yahoo.com>
 <0c8dab7f-7db1-5f58-24d7-3b7e066e2f7c@treenet.co.nz>
 <042e01d32c08$ec7ea680$c57bf380$@ngtech.co.il>
 <88225e47-4f29-fa65-ec5e-b9b1c85446cd@gmail.com>
Message-ID: <04a501d32c1f$9528e550$bf7aaff0$@ngtech.co.il>

Well, the ready to use products are not always what you need or want.
Even squid is not good enough for many scenarios...

If it works with shallalist it's nice but not the real deal for most cases.
Vieri might or might not clarify his scenario, but the issue here is not other then working with squid and a helper.

Eliezer

----
Eliezer Croitoru
Linux System Administrator
Mobile: +972-5-28704261
Email: eliezer at ngtech.co.il



-----Original Message-----
From: squid-users [mailto:squid-users-bounces at lists.squid-cache.org] On Behalf Of Yuri
Sent: Tuesday, September 12, 2017 23:54
To: squid-users at lists.squid-cache.org
Subject: Re: [squid-users] squid cache takes a break

It is just enough not to reinvent the wheel. What needs op - already
exists and is called ufdbguard. And it's works perfectly with shallalist :)


13.09.2017 2:51, Eliezer Croitoru ?????:
> I just must add that if you understand how TCP works(which the helpers use to communicate with squid) then it makes sense that it is possible that...
> The sender (ie squid) sent 100 lines but the client software yet to process them since it's in the OS or other software\hardware related buffer.
>
> For me it was hard to understand at first since it's an STDIN\STDOUT interface so it would block after every write but it's not...
>
> There is a possibility that if the helper can process every incoming request with threading or other method of concurrency then the performance of the helper and by that squid will be better but if only using the basic buffer works fine for you then great.
>
> Eliezer
>
> ----
> Eliezer Croitoru
> Linux System Administrator
> Mobile: +972-5-28704261
> Email: eliezer at ngtech.co.il
>
>
>
> -----Original Message-----
> From: squid-users [mailto:squid-users-bounces at lists.squid-cache.org] On Behalf Of Amos Jeffries
> Sent: Tuesday, September 12, 2017 16:08
> To: squid-users at lists.squid-cache.org
> Subject: Re: [squid-users] squid cache takes a break
>
> On 12/09/17 22:59, Vieri wrote:
>> ________________________________
>> From: Amos Jeffries <squid3 at treenet.co.nz>
>>> That is all it needs to do to begin with; parse off the numeric value
>>> from the input line and send it back as prefix on the output line. The
>>> helper does not need threading or anything particularly special for the > minimal support.
>>
>> I thought it had to be asynchronous.
>> The docs say "Only used with helpers capable of processing more than one query at a time."
>>
>> Example:
>> Squid sends "1 URI1" (or whatever) to the helper.
>> It does not wait for an immediate response.
>> In fact, Squid can send "2 URI2" before getting the reply to ID 1, right?
> Yes.
>
>
>> In my case, the helper is synchronous, non-MT. I don't think it will improve the time responses per-se.
>>
>> In any case, my helper won't be able to process more than one query AT A TIME.
>>
>> I tried it anyway. So here's the relevant code:
>>
>> while( <STDIN> )
>> {
>> s/^\s*//;
>> s/\s*$//;
>> my @squidin = split;
>> my $squidn = scalar @squidin;
>> undef $url;
>> undef $channelid;
>> if ( ($squidn == 2 ) && (defined $squidin[0]) && ($squidin[0] =~ /^\d+?$/) ) {
>> $channelid = $squidin[0];
>> $url = $squidin[1] if (defined $squidin[1]);
>> } else {
>> $url = $squidin[0] if (defined $squidin[0]);
>> }
>>
>> [...]
>> logtofile("Channel-ID: ".$channelid."\n") if ((defined $channelid) && ($debug >= 1));
>>
>> [...do DB lookups, reply accordingly...]
>>
>> if (defined $channelid) {
>> print( $channelid." OK\n" );
>> logtofile( $channelid." OK\n" ) if ($debug >= 1);
>> } else {
>> print( "OK\n" );
>> logtofile( "OK\n" ) if ($debug >= 1);
>> }
>> [...similar responses for ERR messages...]
>> }
>>
>> Here's the relevant squid.conf line:
>> external_acl_type bllookup ttl=86400 negative_ttl=86400 children-max=80 children-startup=10 children-idle=3 concurrency=8 %URI /opt/custom/scripts/run/scripts/firewall/squid_url_lookup.pl [...]
>>
>> How can I check in the Squid log that concurrency is "working"?
> Section 82, level 2 or 4 should log the queries.
>
> Better info is in the cachemgr/squidclient "external_acl" report. Each 
> helper is listed with its total and summary stats for each helper child.
>
>> If the helper logs to a text file as in the trimmed code above, I notice that the channel ID is always 0. I get messages such as:
>> Channel-ID: 0
>> 0 OK
>> Channel-ID: 0
>> 0 ERR ...
>>
>> Is this expected?
> Maybe.
>
> If you make the helper pause a bit and throw a large number of different 
> URLs at Squid you should see it grow a bit higher than 0.
>
>> Despite this, I can see that the number of helper processes does not increase over time for now, and that HTTP/S client browsing is responsive enough.
>> # ps aux | grep -c squid_url_lookup.pl
>> 11
>>
> Yay.
>
>> One last thing. I'm using:
>> cache_dir diskd /var/cache/squid 100 16 256
>> I may want to try to comment out this directive for improved I/O performance.
>>
>> Thanks,
>>
>> Vieri
> Cheers
> Amos
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users





From rentorbuy at yahoo.com  Wed Sep 13 08:21:43 2017
From: rentorbuy at yahoo.com (Vieri)
Date: Wed, 13 Sep 2017 08:21:43 +0000 (UTC)
Subject: [squid-users] squid cache takes a break
In-Reply-To: <04a501d32c1f$9528e550$bf7aaff0$@ngtech.co.il>
References: <1234530834.188010.1504892340901.ref@mail.yahoo.com>
 <1234530834.188010.1504892340901@mail.yahoo.com>
 <ef119e7a-6b6f-0a22-f903-a257ecb0af83@treenet.co.nz>
 <500349185.1012564.1505119741831@mail.yahoo.com>
 <1349ed09-a160-3dce-9a13-cb55e162b2a0@treenet.co.nz>
 <1647206411.239445.1505213966573@mail.yahoo.com>
 <0c8dab7f-7db1-5f58-24d7-3b7e066e2f7c@treenet.co.nz>
 <042e01d32c08$ec7ea680$c57bf380$@ngtech.co.il>
 <88225e47-4f29-fa65-ec5e-b9b1c85446cd@gmail.com>
 <04a501d32c1f$9528e550$bf7aaff0$@ngtech.co.il>
Message-ID: <1622418574.1060572.1505290903862@mail.yahoo.com>

Thanks for the suggestion. I'm sure ufdbguard works great even though it's not maintained/updated on my distro (Gentoo).

I use ready-made helpers/redirectors like squidGuard on other systems.
However, on this system I wanted to avoid depending on extra software. I also wanted to make my own helper so I could then combine Squid ACLs and do things such as:
- block access to blacklisted URLs on a Squid setup with transparent ssl_bump (no proxy auth)

- show custom deny web page with optional auth form to bypass this restriction
- authenticate via LDAP using a custom web form, and insert the user's client IP address into a database with a timeout
- auto-redirect the request to the restricted web site so the user on a particular client host can access the site for a given time frame

- use a squid ACL to look up the user's host IP address in the DB, and decide to allow or not


In any case, I've been experiencing lots of issues with Squid during the past 2 weeks. I can finally say that I've fine-tuned my setup thanks to the great help I found on this ML. One of the things that were nagging me was the helper part. Knowing how helpers work, and how they can be optimized on heavy traffic loads is "a good thing". For starters, I did not know how to use the concurrency option and how the use of it could benefit overall performance.


Thanks,

Vieri


From rafael.akchurin at diladele.com  Wed Sep 13 15:32:24 2017
From: rafael.akchurin at diladele.com (Rafael Akchurin)
Date: Wed, 13 Sep 2017 15:32:24 +0000
Subject: [squid-users] [squid for windows] article on how to enable sslbump
Message-ID: <DB6PR0401MB2680DF93FA517CD8CA10E2B48F6E0@DB6PR0401MB2680.eurprd04.prod.outlook.com>

Greetings everyone,



For all those using Squid version for Microsoft Windows - here is the article explaining how to enable HTTPS decryption (sslbump) on Windows platforms.

Please see https://docs.diladele.com/faq/squid/sslbump_squid_windows.html



If you find any errors please tell us at support at diladele.com<mailto:support at diladele.com>



--

Best regards,

Rafael Akchurin

Diladele B.V.

https://www.diladele.com


P.S. Build of Squid 3.5.27 for Microsoft Windows is still on the way :( ...
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20170913/26d10b1b/attachment.htm>

From yvoinov at gmail.com  Wed Sep 13 16:07:19 2017
From: yvoinov at gmail.com (Yuri)
Date: Wed, 13 Sep 2017 22:07:19 +0600
Subject: [squid-users] [squid for windows] article on how to enable
 sslbump
In-Reply-To: <DB6PR0401MB2680DF93FA517CD8CA10E2B48F6E0@DB6PR0401MB2680.eurprd04.prod.outlook.com>
References: <DB6PR0401MB2680DF93FA517CD8CA10E2B48F6E0@DB6PR0401MB2680.eurprd04.prod.outlook.com>
Message-ID: <4f78a429-cbb4-3a42-24ae-9cc5285eb080@gmail.com>



13.09.2017 21:32, Rafael Akchurin ?????:
>
> Greetings everyone,
>
> ?
>
> For all those using Squid version for Microsoft Windows ? here is the
> article explaining how to enable HTTPS decryption (sslbump) on Windows
> platforms.
>
> Please see https://docs.diladele.com/faq/squid/sslbump_squid_windows.html
>
> ?
>
> If you find any errors please tell us at support at diladele.com
> <mailto:support at diladele.com>
>
> ?
>
> --
>
> Best regards,
>
> Rafael Akchurin
>
> Diladele B.V.
>
> https://www.diladele.com
>
> ?
>
> P.S. Build of Squid 3.5.27 for Microsoft Windows is still on the way :( ?
>
BTW, Raf. Why not to build 4.0.21 already? Now 2017, 3.5.x is so
ancient, ever on Win64. :) I would like to see cert downloader also on
my laptop ;)
>
>
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20170913/d5908e15/attachment.htm>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: signature.asc
Type: application/pgp-signature
Size: 473 bytes
Desc: OpenPGP digital signature
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20170913/d5908e15/attachment.sig>

From rafael.akchurin at diladele.com  Wed Sep 13 16:17:52 2017
From: rafael.akchurin at diladele.com (Rafael Akchurin)
Date: Wed, 13 Sep 2017 16:17:52 +0000
Subject: [squid-users] [squid for windows] article on how to enable
 sslbump
In-Reply-To: <4f78a429-cbb4-3a42-24ae-9cc5285eb080@gmail.com>
References: <DB6PR0401MB2680DF93FA517CD8CA10E2B48F6E0@DB6PR0401MB2680.eurprd04.prod.outlook.com>,
 <4f78a429-cbb4-3a42-24ae-9cc5285eb080@gmail.com>
Message-ID: <3210B8D9-BCDC-4409-B9CB-0AC57B6FFECE@diladele.com>

Hello Yuri,

We tried building it several times, but it was not  clear why it failed.. so we keep postponing :(

Best regards,
Rafael Akchurin


Op 13 sep. 2017 om 18:07 heeft Yuri <yvoinov at gmail.com<mailto:yvoinov at gmail.com>> het volgende geschreven:



13.09.2017 21:32, Rafael Akchurin ?????:

Greetings everyone,



For all those using Squid version for Microsoft Windows ? here is the article explaining how to enable HTTPS decryption (sslbump) on Windows platforms.

Please see https://docs.diladele.com/faq/squid/sslbump_squid_windows.html



If you find any errors please tell us at support at diladele.com<mailto:support at diladele.com>



--

Best regards,

Rafael Akchurin

Diladele B.V.

https://www.diladele.com


P.S. Build of Squid 3.5.27 for Microsoft Windows is still on the way :( ?
BTW, Raf. Why not to build 4.0.21 already? Now 2017, 3.5.x is so ancient, ever on Win64. :) I would like to see cert downloader also on my laptop ;)



_______________________________________________
squid-users mailing list
squid-users at lists.squid-cache.org<mailto:squid-users at lists.squid-cache.org>
http://lists.squid-cache.org/listinfo/squid-users


_______________________________________________
squid-users mailing list
squid-users at lists.squid-cache.org<mailto:squid-users at lists.squid-cache.org>
http://lists.squid-cache.org/listinfo/squid-users
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20170913/b0fd9e39/attachment.htm>

From eliezer at ngtech.co.il  Wed Sep 13 18:02:41 2017
From: eliezer at ngtech.co.il (Eliezer Croitoru)
Date: Wed, 13 Sep 2017 21:02:41 +0300
Subject: [squid-users] squid-users Digest, Vol 37, Issue 30
In-Reply-To: <CAOLOQx1OGKY55C6J_VWqKod-YXQ6S8Ps6qmucq5y-Xz8CgC_gA@mail.gmail.com>
References: <mailman.1274.1505161142.3209.squid-users@lists.squid-cache.org>
 <CAOLOQx0Uj0sdRU-h2Jep0qxV6C4Dxm+j=Z3Zt3kkcAZj_15RPA@mail.gmail.com>
 <CAOLOQx0tdoymMbORG22mULbXPqa=K7k+e=rNVSk2CLsxUGbN3A@mail.gmail.com>
 <CAOLOQx1OGKY55C6J_VWqKod-YXQ6S8Ps6qmucq5y-Xz8CgC_gA@mail.gmail.com>
Message-ID: <081501d32cba$7e0ae9e0$7a20bda0$@ngtech.co.il>

I do not care if someone asks even if the docs are answering.
The docs of squid-cache are not something anyone should be able to remember by heart or even browse and just "find" a solution or a direction.
We(at least me) are here to try and help even for the cases which the docs already cover.

All The Bests,
Eliezer

----
http://ngtech.co.il/lmgtfy/
Linux System Administrator
Mobile: +972-5-28704261
Email: eliezer at ngtech.co.il


From: squid-users [mailto:squid-users-bounces at lists.squid-cache.org] On Behalf Of Adrian Miller
Sent: Monday, September 11, 2017 23:31
To: squid-users at lists.squid-cache.org
Subject: Re: [squid-users] squid-users Digest, Vol 37, Issue 30

Jesus, never seen so many messages that could have been answered by reading the basic squid docs.

Tempted to unsub....sheesh

On 12 Sep. 2017 6:19 am, <mailto:squid-users-request at lists.squid-cache.org> wrote:
Send squid-users mailing list submissions to
        mailto:squid-users at lists.squid-cache.org

To subscribe or unsubscribe via the World Wide Web, visit
        http://lists.squid-cache.org/listinfo/squid-users
or, via email, send a message with subject or body 'help' to
        mailto:squid-users-request at lists.squid-cache.org

You can reach the person managing the list at
        mailto:squid-users-owner at lists.squid-cache.org

When replying, please edit your Subject line so it is more specific
than "Re: Contents of squid-users digest..."


Today's Topics:

   1. Re: Need assistance debugging Squid error: ssl_ctrd helpers
      crashing too quickly (Rohit Sodhia)


----------------------------------------------------------------------

Message: 1
Date: Mon, 11 Sep 2017 16:18:39 -0400
From: Rohit Sodhia <mailto:sodhia.rohit at gmail.com>
To: Yuri <mailto:yvoinov at gmail.com>
Cc: mailto:squid-users at lists.squid-cache.org
Subject: Re: [squid-users] Need assistance debugging Squid error:
        ssl_ctrd helpers crashing too quickly
Message-ID:
        <mailto:CAN1w9tfQt3Mivwpyo%2Bu3Qp0agQ8pOgz2MGo2Wvb5AdGU3zbkjw at mail.gmail.com>
Content-Type: text/plain; charset="utf-8"

Ok. Looks like 3.5.20 is the latest on the yum repo I'm using, so guess
I'll have to learn how to compile it myself; never compiled a package
before.

On Mon, Sep 11, 2017 at 4:17 PM, Yuri <mailto:yvoinov at gmail.com> wrote:

> Hardly,
>
> most probably something in repo's package. However, upgrade is always
> recommended, especially with modern functionality. It changes fast enough.
>
> 12.09.2017 2:15, Rohit Sodhia ?????:
>
> Ah. I'm on 3.5.20; not sure how far back that is. Is that the core of the
> problem?
>
> On Mon, Sep 11, 2017 at 4:07 PM, Yuri <mailto:yvoinov at gmail.com> wrote:
>
>> Seems latest 4.0.21 is good enough. Most critical SSL-related bugs almost
>> closed or closed.
>>
>> At least latest 3.5.27 is released. AFAIK this is minimum to problem-free
>> running.
>>
>> Repositories software sometimes has strange quirks, or sometimes rancid.
>> 12.09.2017 2:05, Rohit Sodhia ?????:
>>
>> I'll try to find it, but I read a few articles/SO questions that
>> suggested there were bugs in 4 relating to SSL bumping? If they were wrong,
>> I'd be glad to go forward. Should I be removing the yum squid package and
>> compile my own? Is 3.5 problematic besides being old?
>>
>> On Mon, Sep 11, 2017 at 4:02 PM, Yuri <mailto:yvoinov at gmail.com> wrote:
>>
>>> Wait. Squid 3.5.20? So ancient?
>>>
>>> 12.09.2017 1:58, Rohit Sodhia ?????:
>>>
>>> sslcrtd_program /usr/lib64/squid/ssl_crtd -s /var/lib/ssl_db -M 4MB
>>>
>>> I used the line from the Stack Overflow question I linked earlier.
>>>
>>> On Mon, Sep 11, 2017 at 3:41 PM, Yuri <mailto:yvoinov at gmail.com> wrote:
>>>
>>>> Well. Let's check more deep.
>>>>
>>>> Show me parameter sslcrtd_program in your squid.conf
>>>>
>>>> 12.09.2017 1:23, Rohit Sodhia ?????:
>>>>
>>>> Unfortunately, no luck yet. Thank you again for your help before.
>>>>
>>>> I found that the user squid and group squid existed already, so I added
>>>>
>>>> cache_effective_user squid
>>>> cache_effective_group squid
>>>>
>>>> to my config (first two lines), made sure /var/lib/ssl_db and it's
>>>> contents were set to squid:squid and restarted the service, but I'm still
>>>> getting the same error :(
>>>>
>>>> On Mon, Sep 11, 2017 at 2:42 PM, Rohit Sodhia <mailto:sodhia.rohit at gmail.com>
>>>> wrote:
>>>>
>>>>> I'll try that immediately, thanks! I appreciate all your advice;
>>>>> hopefully I won't have to reach out again :p
>>>>>
>>>>> On Mon, Sep 11, 2017 at 2:39 PM, Yuri <mailto:yvoinov at gmail.com> wrote:
>>>>>
>>>>>> I'm not Linux fanboy, but modern squid never runs as root. So, most
>>>>>> probably it runs as nobody user.
>>>>>>
>>>>>> Ah, yes:
>>>>>>
>>>>>> #  TAG: cache_effective_user
>>>>>> #    If you start Squid as root, it will change its effective/real
>>>>>> #    UID/GID to the user specified below.  The default is to change
>>>>>> #    to UID of nobody.
>>>>>> #    see also; cache_effective_group
>>>>>> #Default:
>>>>>> # cache_effective_user nobody
>>>>>>
>>>>>> #  TAG: cache_effective_group
>>>>>> #    Squid sets the GID to the effective user's default group ID
>>>>>> #    (taken from the password file) and supplementary group list
>>>>>> #    from the groups membership.
>>>>>> #
>>>>>> #    If you want Squid to run with a specific GID regardless of
>>>>>> #    the group memberships of the effective user then set this
>>>>>> #    to the group (or GID) you want Squid to run as. When set
>>>>>> #    all other group privileges of the effective user are ignored
>>>>>> #    and only this GID is effective. If Squid is not started as
>>>>>> #    root the user starting Squid MUST be member of the specified
>>>>>> #    group.
>>>>>> #
>>>>>> #    This option is not recommended by the Squid Team.
>>>>>> #    Our preference is for administrators to configure a secure
>>>>>> #    user account for squid with UID/GID matching system policies.
>>>>>> #Default:
>>>>>> # Use system group memberships of the cache_effective_user account
>>>>>>
>>>>>> As documented. :)
>>>>>>
>>>>>> AFAIK best solution is create non-privileged group & user (like
>>>>>> squid/squid) and set both this parameters explicity.
>>>>>>
>>>>>> Then change owner recursively on SSL cache to this user.
>>>>>>
>>>>>> 12.09.2017 0:36, Rohit Sodhia ?????:
>>>>>>
>>>>>> Neither of those values are set in my config. Even though I'm not
>>>>>> using squid for caching, I need those values? They aren't set in the
>>>>>> default configs either.
>>>>>>
>>>>>> On Mon, Sep 11, 2017 at 2:33 PM, Yuri <mailto:yvoinov at gmail.com> wrote:
>>>>>>
>>>>>>> Most probably you squid runs as another user than squid.
>>>>>>>
>>>>>>> Check your squid.conf for cache_effective_user and
>>>>>>> cache_effective_group values.
>>>>>>>
>>>>>>> Then change SSL cache permissions to this values. Should work.
>>>>>>>
>>>>>>> 12.09.2017 0:30, Rohit Sodhia ?????:
>>>>>>>
>>>>>>> Thanks for the feedback! I just used yum (it's a CentOS 7 VB) and it
>>>>>>> set it up like that. I changed the owner and group to squid:squid and tried
>>>>>>> restarting squid, but still get the same errors. I thought to run the
>>>>>>> command again, but this time it says
>>>>>>>
>>>>>>> /usr/lib64/squid/ssl_crtd: Cannot create /var/lib/ssl_db
>>>>>>>
>>>>>>> If this folder has incorrect permissions are there possibly other
>>>>>>> permission issues?
>>>>>>>
>>>>>>> On Mon, Sep 11, 2017 at 2:25 PM, Yuri <mailto:yvoinov at gmail.com> wrote:
>>>>>>>
>>>>>>>> Here you root of problem.
>>>>>>>>
>>>>>>>> Should be (on my setups):
>>>>>>>>
>>>>>>>> # ls -al /var/lib/ssl_db
>>>>>>>> total 326
>>>>>>>> drwxr-xr-x 3 squid squid      5 Sep  5 00:53 .
>>>>>>>> drwxr-xr-x 8 root  other      8 Sep  5 00:53 ..
>>>>>>>> drwxr-xr-x 2 squid squid    454 Sep 11 23:37 certs
>>>>>>>> -rw-r--r-- 1 squid squid 280575 Sep 11 23:37 index.txt
>>>>>>>> -rw-r--r-- 1 squid squid      7 Sep 11 23:37 size
>>>>>>>>
>>>>>>>> I.e. Squid has no access to SSL cache dir structures.
>>>>>>>>
>>>>>>>> 12.09.2017 0:23, Rohit Sodhia ?????:
>>>>>>>>
>>>>>>>> total 8
>>>>>>>> drwxr-xr-x.  3 root root   48 Sep 11 12:42 .
>>>>>>>> drwxr-xr-x. 32 root root 4096 Sep 11 12:42 ..
>>>>>>>> drwxr-xr-x.  2 root root    6 Sep 11 12:42 certs
>>>>>>>> -rw-r--r--.  1 root root    0 Sep 11 12:42 index.txt
>>>>>>>> -rw-r--r--.  1 root root    1 Sep 11 12:42 size
>>>>>>>>
>>>>>>>>
>>>>>>>> On Mon, Sep 11, 2017 at 2:22 PM, Yuri <mailto:yvoinov at gmail.com> wrote:
>>>>>>>>
>>>>>>>>> Show output of
>>>>>>>>>
>>>>>>>>> ls -al /var/lib/ssl_db
>>>>>>>>>
>>>>>>>>> 12.09.2017 0:21, Rohit Sodhia ?????:
>>>>>>>>>
>>>>>>>>> Yes, but telling me it's crashing unfortunately doesn't help me
>>>>>>>>> figure out why or how to fix it. I've run the command it suggests but it
>>>>>>>>> doesn't help. I'm unfortunately not an ops guy familiar with this kind of
>>>>>>>>> stuff; I don't see anything on how to figure out what to do about it.
>>>>>>>>>
>>>>>>>>> On Mon, Sep 11, 2017 at 2:17 PM, Yuri <mailto:yvoinov at gmail.com> wrote:
>>>>>>>>>
>>>>>>>>>> It tells you what's happens.
>>>>>>>>>>
>>>>>>>>>>
>>>>>>>>>> 11.09.2017 23:50, Rohit Sodhia ?????:
>>>>>>>>>> > (ssl_crtd): Uninitialized SSL certificate database directory:
>>>>>>>>>> > /var/lib/ssl_db. To initialize, run "ssl_crtd -c -s
>>>>>>>>>> /var/lib/ssl_db".
>>>>>>>>>>
>>>>>>>>>>
>>>>>>>>>>
>>>>>>>>>> _______________________________________________
>>>>>>>>>> squid-users mailing list
>>>>>>>>>> mailto:squid-users at lists.squid-cache.org
>>>>>>>>>> http://lists.squid-cache.org/listinfo/squid-users
>>>>>>>>>>
>>>>>>>>>>
>>>>>>>>>
>>>>>>>>>
>>>>>>>>
>>>>>>>>
>>>>>>>
>>>>>>>
>>>>>>
>>>>>>
>>>>>
>>>>
>>>>
>>>
>>>
>>
>>
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20170911/2c3ab1ef/attachment.html>

------------------------------

Subject: Digest Footer

_______________________________________________
squid-users mailing list
mailto:squid-users at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-users


------------------------------

End of squid-users Digest, Vol 37, Issue 30
*******************************************




From yvoinov at gmail.com  Wed Sep 13 20:03:30 2017
From: yvoinov at gmail.com (Yuri)
Date: Thu, 14 Sep 2017 02:03:30 +0600
Subject: [squid-users] squid-users Digest, Vol 37, Issue 30
In-Reply-To: <081501d32cba$7e0ae9e0$7a20bda0$@ngtech.co.il>
References: <mailman.1274.1505161142.3209.squid-users@lists.squid-cache.org>
 <CAOLOQx0Uj0sdRU-h2Jep0qxV6C4Dxm+j=Z3Zt3kkcAZj_15RPA@mail.gmail.com>
 <CAOLOQx0tdoymMbORG22mULbXPqa=K7k+e=rNVSk2CLsxUGbN3A@mail.gmail.com>
 <CAOLOQx1OGKY55C6J_VWqKod-YXQ6S8Ps6qmucq5y-Xz8CgC_gA@mail.gmail.com>
 <081501d32cba$7e0ae9e0$7a20bda0$@ngtech.co.il>
Message-ID: <1f3baacf-07e5-878b-dd13-ca3e32abefba@gmail.com>

For a change, I agree with Eliezer. And about the documentation of
OpenSource is best mournfully silent.


14.09.2017 0:02, Eliezer Croitoru ?????:
> I do not care if someone asks even if the docs are answering.
> The docs of squid-cache are not something anyone should be able to remember by heart or even browse and just "find" a solution or a direction.
> We(at least me) are here to try and help even for the cases which the docs already cover.
>
> All The Bests,
> Eliezer
>
> ----
> http://ngtech.co.il/lmgtfy/
> Linux System Administrator
> Mobile: +972-5-28704261
> Email: eliezer at ngtech.co.il
>
>
> From: squid-users [mailto:squid-users-bounces at lists.squid-cache.org] On Behalf Of Adrian Miller
> Sent: Monday, September 11, 2017 23:31
> To: squid-users at lists.squid-cache.org
> Subject: Re: [squid-users] squid-users Digest, Vol 37, Issue 30
>
> Jesus, never seen so many messages that could have been answered by reading the basic squid docs.
>
> Tempted to unsub....sheesh
>
> On 12 Sep. 2017 6:19 am, <mailto:squid-users-request at lists.squid-cache.org> wrote:
> Send squid-users mailing list submissions to
>         mailto:squid-users at lists.squid-cache.org
>
> To subscribe or unsubscribe via the World Wide Web, visit
>         http://lists.squid-cache.org/listinfo/squid-users
> or, via email, send a message with subject or body 'help' to
>         mailto:squid-users-request at lists.squid-cache.org
>
> You can reach the person managing the list at
>         mailto:squid-users-owner at lists.squid-cache.org
>
> When replying, please edit your Subject line so it is more specific
> than "Re: Contents of squid-users digest..."
>
>
> Today's Topics:
>
>    1. Re: Need assistance debugging Squid error: ssl_ctrd helpers
>       crashing too quickly (Rohit Sodhia)
>
>
> ----------------------------------------------------------------------
>
> Message: 1
> Date: Mon, 11 Sep 2017 16:18:39 -0400
> From: Rohit Sodhia <mailto:sodhia.rohit at gmail.com>
> To: Yuri <mailto:yvoinov at gmail.com>
> Cc: mailto:squid-users at lists.squid-cache.org
> Subject: Re: [squid-users] Need assistance debugging Squid error:
>         ssl_ctrd helpers crashing too quickly
> Message-ID:
>         <mailto:CAN1w9tfQt3Mivwpyo%2Bu3Qp0agQ8pOgz2MGo2Wvb5AdGU3zbkjw at mail.gmail.com>
> Content-Type: text/plain; charset="utf-8"
>
> Ok. Looks like 3.5.20 is the latest on the yum repo I'm using, so guess
> I'll have to learn how to compile it myself; never compiled a package
> before.
>
> On Mon, Sep 11, 2017 at 4:17 PM, Yuri <mailto:yvoinov at gmail.com> wrote:
>
>> Hardly,
>>
>> most probably something in repo's package. However, upgrade is always
>> recommended, especially with modern functionality. It changes fast enough.
>>
>> 12.09.2017 2:15, Rohit Sodhia ?????:
>>
>> Ah. I'm on 3.5.20; not sure how far back that is. Is that the core of the
>> problem?
>>
>> On Mon, Sep 11, 2017 at 4:07 PM, Yuri <mailto:yvoinov at gmail.com> wrote:
>>
>>> Seems latest 4.0.21 is good enough. Most critical SSL-related bugs almost
>>> closed or closed.
>>>
>>> At least latest 3.5.27 is released. AFAIK this is minimum to problem-free
>>> running.
>>>
>>> Repositories software sometimes has strange quirks, or sometimes rancid.
>>> 12.09.2017 2:05, Rohit Sodhia ?????:
>>>
>>> I'll try to find it, but I read a few articles/SO questions that
>>> suggested there were bugs in 4 relating to SSL bumping? If they were wrong,
>>> I'd be glad to go forward. Should I be removing the yum squid package and
>>> compile my own? Is 3.5 problematic besides being old?
>>>
>>> On Mon, Sep 11, 2017 at 4:02 PM, Yuri <mailto:yvoinov at gmail.com> wrote:
>>>
>>>> Wait. Squid 3.5.20? So ancient?
>>>>
>>>> 12.09.2017 1:58, Rohit Sodhia ?????:
>>>>
>>>> sslcrtd_program /usr/lib64/squid/ssl_crtd -s /var/lib/ssl_db -M 4MB
>>>>
>>>> I used the line from the Stack Overflow question I linked earlier.
>>>>
>>>> On Mon, Sep 11, 2017 at 3:41 PM, Yuri <mailto:yvoinov at gmail.com> wrote:
>>>>
>>>>> Well. Let's check more deep.
>>>>>
>>>>> Show me parameter sslcrtd_program in your squid.conf
>>>>>
>>>>> 12.09.2017 1:23, Rohit Sodhia ?????:
>>>>>
>>>>> Unfortunately, no luck yet. Thank you again for your help before.
>>>>>
>>>>> I found that the user squid and group squid existed already, so I added
>>>>>
>>>>> cache_effective_user squid
>>>>> cache_effective_group squid
>>>>>
>>>>> to my config (first two lines), made sure /var/lib/ssl_db and it's
>>>>> contents were set to squid:squid and restarted the service, but I'm still
>>>>> getting the same error :(
>>>>>
>>>>> On Mon, Sep 11, 2017 at 2:42 PM, Rohit Sodhia <mailto:sodhia.rohit at gmail.com>
>>>>> wrote:
>>>>>
>>>>>> I'll try that immediately, thanks! I appreciate all your advice;
>>>>>> hopefully I won't have to reach out again :p
>>>>>>
>>>>>> On Mon, Sep 11, 2017 at 2:39 PM, Yuri <mailto:yvoinov at gmail.com> wrote:
>>>>>>
>>>>>>> I'm not Linux fanboy, but modern squid never runs as root. So, most
>>>>>>> probably it runs as nobody user.
>>>>>>>
>>>>>>> Ah, yes:
>>>>>>>
>>>>>>> #  TAG: cache_effective_user
>>>>>>> #    If you start Squid as root, it will change its effective/real
>>>>>>> #    UID/GID to the user specified below.  The default is to change
>>>>>>> #    to UID of nobody.
>>>>>>> #    see also; cache_effective_group
>>>>>>> #Default:
>>>>>>> # cache_effective_user nobody
>>>>>>>
>>>>>>> #  TAG: cache_effective_group
>>>>>>> #    Squid sets the GID to the effective user's default group ID
>>>>>>> #    (taken from the password file) and supplementary group list
>>>>>>> #    from the groups membership.
>>>>>>> #
>>>>>>> #    If you want Squid to run with a specific GID regardless of
>>>>>>> #    the group memberships of the effective user then set this
>>>>>>> #    to the group (or GID) you want Squid to run as. When set
>>>>>>> #    all other group privileges of the effective user are ignored
>>>>>>> #    and only this GID is effective. If Squid is not started as
>>>>>>> #    root the user starting Squid MUST be member of the specified
>>>>>>> #    group.
>>>>>>> #
>>>>>>> #    This option is not recommended by the Squid Team.
>>>>>>> #    Our preference is for administrators to configure a secure
>>>>>>> #    user account for squid with UID/GID matching system policies.
>>>>>>> #Default:
>>>>>>> # Use system group memberships of the cache_effective_user account
>>>>>>>
>>>>>>> As documented. :)
>>>>>>>
>>>>>>> AFAIK best solution is create non-privileged group & user (like
>>>>>>> squid/squid) and set both this parameters explicity.
>>>>>>>
>>>>>>> Then change owner recursively on SSL cache to this user.
>>>>>>>
>>>>>>> 12.09.2017 0:36, Rohit Sodhia ?????:
>>>>>>>
>>>>>>> Neither of those values are set in my config. Even though I'm not
>>>>>>> using squid for caching, I need those values? They aren't set in the
>>>>>>> default configs either.
>>>>>>>
>>>>>>> On Mon, Sep 11, 2017 at 2:33 PM, Yuri <mailto:yvoinov at gmail.com> wrote:
>>>>>>>
>>>>>>>> Most probably you squid runs as another user than squid.
>>>>>>>>
>>>>>>>> Check your squid.conf for cache_effective_user and
>>>>>>>> cache_effective_group values.
>>>>>>>>
>>>>>>>> Then change SSL cache permissions to this values. Should work.
>>>>>>>>
>>>>>>>> 12.09.2017 0:30, Rohit Sodhia ?????:
>>>>>>>>
>>>>>>>> Thanks for the feedback! I just used yum (it's a CentOS 7 VB) and it
>>>>>>>> set it up like that. I changed the owner and group to squid:squid and tried
>>>>>>>> restarting squid, but still get the same errors. I thought to run the
>>>>>>>> command again, but this time it says
>>>>>>>>
>>>>>>>> /usr/lib64/squid/ssl_crtd: Cannot create /var/lib/ssl_db
>>>>>>>>
>>>>>>>> If this folder has incorrect permissions are there possibly other
>>>>>>>> permission issues?
>>>>>>>>
>>>>>>>> On Mon, Sep 11, 2017 at 2:25 PM, Yuri <mailto:yvoinov at gmail.com> wrote:
>>>>>>>>
>>>>>>>>> Here you root of problem.
>>>>>>>>>
>>>>>>>>> Should be (on my setups):
>>>>>>>>>
>>>>>>>>> # ls -al /var/lib/ssl_db
>>>>>>>>> total 326
>>>>>>>>> drwxr-xr-x 3 squid squid      5 Sep  5 00:53 .
>>>>>>>>> drwxr-xr-x 8 root  other      8 Sep  5 00:53 ..
>>>>>>>>> drwxr-xr-x 2 squid squid    454 Sep 11 23:37 certs
>>>>>>>>> -rw-r--r-- 1 squid squid 280575 Sep 11 23:37 index.txt
>>>>>>>>> -rw-r--r-- 1 squid squid      7 Sep 11 23:37 size
>>>>>>>>>
>>>>>>>>> I.e. Squid has no access to SSL cache dir structures.
>>>>>>>>>
>>>>>>>>> 12.09.2017 0:23, Rohit Sodhia ?????:
>>>>>>>>>
>>>>>>>>> total 8
>>>>>>>>> drwxr-xr-x.  3 root root   48 Sep 11 12:42 .
>>>>>>>>> drwxr-xr-x. 32 root root 4096 Sep 11 12:42 ..
>>>>>>>>> drwxr-xr-x.  2 root root    6 Sep 11 12:42 certs
>>>>>>>>> -rw-r--r--.  1 root root    0 Sep 11 12:42 index.txt
>>>>>>>>> -rw-r--r--.  1 root root    1 Sep 11 12:42 size
>>>>>>>>>
>>>>>>>>>
>>>>>>>>> On Mon, Sep 11, 2017 at 2:22 PM, Yuri <mailto:yvoinov at gmail.com> wrote:
>>>>>>>>>
>>>>>>>>>> Show output of
>>>>>>>>>>
>>>>>>>>>> ls -al /var/lib/ssl_db
>>>>>>>>>>
>>>>>>>>>> 12.09.2017 0:21, Rohit Sodhia ?????:
>>>>>>>>>>
>>>>>>>>>> Yes, but telling me it's crashing unfortunately doesn't help me
>>>>>>>>>> figure out why or how to fix it. I've run the command it suggests but it
>>>>>>>>>> doesn't help. I'm unfortunately not an ops guy familiar with this kind of
>>>>>>>>>> stuff; I don't see anything on how to figure out what to do about it.
>>>>>>>>>>
>>>>>>>>>> On Mon, Sep 11, 2017 at 2:17 PM, Yuri <mailto:yvoinov at gmail.com> wrote:
>>>>>>>>>>
>>>>>>>>>>> It tells you what's happens.
>>>>>>>>>>>
>>>>>>>>>>>
>>>>>>>>>>> 11.09.2017 23:50, Rohit Sodhia ?????:
>>>>>>>>>>>> (ssl_crtd): Uninitialized SSL certificate database directory:
>>>>>>>>>>>> /var/lib/ssl_db. To initialize, run "ssl_crtd -c -s
>>>>>>>>>>> /var/lib/ssl_db".
>>>>>>>>>>>
>>>>>>>>>>>
>>>>>>>>>>>
>>>>>>>>>>> _______________________________________________
>>>>>>>>>>> squid-users mailing list
>>>>>>>>>>> mailto:squid-users at lists.squid-cache.org
>>>>>>>>>>> http://lists.squid-cache.org/listinfo/squid-users
>>>>>>>>>>>
>>>>>>>>>>>
>>>>>>>>>>
>>>>>>>>>
>>>>>>>>
>>>>>>>
>>>>>
>>>>
>>>
>>
> -------------- next part --------------
> An HTML attachment was scrubbed...
> URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20170911/2c3ab1ef/attachment.html>
>
> ------------------------------
>
> Subject: Digest Footer
>
> _______________________________________________
> squid-users mailing list
> mailto:squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>
>
> ------------------------------
>
> End of squid-users Digest, Vol 37, Issue 30
> *******************************************
>
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users


-------------- next part --------------
A non-text attachment was scrubbed...
Name: signature.asc
Type: application/pgp-signature
Size: 473 bytes
Desc: OpenPGP digital signature
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20170914/2560e115/attachment.sig>

From squid at bloms.de  Thu Sep 14 06:08:26 2017
From: squid at bloms.de (Dieter Bloms)
Date: Thu, 14 Sep 2017 08:08:26 +0200
Subject: [squid-users] get many logentries "ACL is used in context without
 an ALE state. Assuming mismatch" after upgrade from 3.5 to 4.0.21 when
 using external helper
Message-ID: <20170914060824.vl4hl2t7fg4xngrf@bloms.de>

Hello,

I used external helper with squid 3.5.xx several years without any
problem.
Now I tried to upgrade to squid 4.0.21 and squid seems to work fine, but
I get many logentries like:

--snip--
2017/09/14 07:43:12 kid3| WARNING: blockhostsdomain ACL is used in context without an ALE state. Assuming mismatch.
2017/09/14 07:43:12 kid3| WARNING: blockhostsip ACL is used in context without an ALE state. Assuming mismatch.
2017/09/14 07:44:12 kid4| WARNING: blockhostsdomain ACL is used in context without an ALE state. Assuming mismatch.
2017/09/14 07:44:12 kid4| WARNING: blockhostsip ACL is used in context without an ALE state. Assuming mismatch.
--snip--

when I switched the acls to a file list, the warnings are gone.

my acls for external helpers look like:

external_acl_type blockhostiptype ttl=3600 negative_ttl=3600 grace=50 children-max=10 children-startup=2 %DST /usr/bin/dnsbl-ip.pl bl
acl blockhostsip external blockhostiptype
external_acl_type blockhostdomaintype ttl=3600 negative_ttl=3600 grace=50 children-max=10 children-startup=2 %DST /usr/bin/dnsbl.pl dbl
acl blockhostsdomain external blockhostdomaintype

when I replaced to above lines with this two, the warnings are gone:

acl blockhostsip dst "/etc/squid/blockhosts.ips"
acl blockhostsdomain dstdomain "/etc/squid/blockhosts.domains"

but I want to use the external helpers, because the lists were updated
many times a day and a reconfigure of squid has an impact of 2-3 seconds.

As I said before, squid works fine and checks the acls, but I get many
warnings in the cache.log and don't know the cause of it.


-- 
Regards

  Dieter

--
I do not get viruses because I do not use MS software.
If you use Outlook then please do not put my email address in your
address-book so that WHEN you get a virus it won't use my address in the
>From field.


From squid3 at treenet.co.nz  Thu Sep 14 09:09:48 2017
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 14 Sep 2017 21:09:48 +1200
Subject: [squid-users] get many logentries "ACL is used in context
 without an ALE state. Assuming mismatch" after upgrade from 3.5 to 4.0.21
 when using external helper
In-Reply-To: <20170914060824.vl4hl2t7fg4xngrf@bloms.de>
References: <20170914060824.vl4hl2t7fg4xngrf@bloms.de>
Message-ID: <abaecc72-ef7a-a36a-074e-c8294bc36f88@treenet.co.nz>

On 14/09/17 18:08, Dieter Bloms wrote:
> 
> As I said before, squid works fine and checks the acls, but I get many
> warnings in the cache.log and don't know the cause of it.

The cause of it is a change to how external ACL locate their state data 
in Squid-4, so they can use logformat codes.

What access control(s) are you using this helper with?

Amos


From ronaldod32 at yahoo.com  Thu Sep 14 13:22:05 2017
From: ronaldod32 at yahoo.com (Ronald)
Date: Thu, 14 Sep 2017 13:22:05 +0000 (UTC)
Subject: [squid-users] Squid basic_nsca_auth stopped working after update.
References: <1444937124.1914390.1505395325252.ref@mail.yahoo.com>
Message-ID: <1444937124.1914390.1505395325252@mail.yahoo.com>

This morning the server was updated.I have? Centos 7 every moning it pushes the updates with yum.
This morning i got squid updated to :
Name??????? : squid
Arch??????? : x86_64
Epoch?????? : 7
Version???? : 3.5.20
Release???? : 10.el7

The side effect is :2017/09/14 15:02:31 kid1| helperOpenServers: Starting 0/5 'basic_ncsa_auth' processes
2017/09/14 15:02:31 kid1| helperOpenServers: No 'basic_ncsa_auth' processes needed.
Config file :
auth_param basic program /usr/lib64/squid/basic_ncsa_auth /etc/squid/passwd
auth_param basic children 5
auth_param basic realm Squid proxy-caching web server
auth_param basic credentialsttl 2 hours
auth_param basic casesensitive off
acl ncsa_users proxy_auth REQUIRED
http_access allow ncsa_users
http_access deny !ncsa_users


It worked month before and i tested the config line again. Still responded like it should. Did an account test and got an OK back from the tool.But squid says nothing installed..
Anyone an idea how to fix this problem ?
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20170914/731423ca/attachment.htm>

From squid at bloms.de  Thu Sep 14 14:25:21 2017
From: squid at bloms.de (Dieter Bloms)
Date: Thu, 14 Sep 2017 16:25:21 +0200
Subject: [squid-users] get many logentries "ACL is used in context
 without an ALE state. Assuming mismatch" after upgrade from 3.5 to 4.0.21
 when using external helper
In-Reply-To: <abaecc72-ef7a-a36a-074e-c8294bc36f88@treenet.co.nz>
References: <20170914060824.vl4hl2t7fg4xngrf@bloms.de>
 <abaecc72-ef7a-a36a-074e-c8294bc36f88@treenet.co.nz>
Message-ID: <20170914142521.37otwsrpvhs23ac6@bloms.de>

Hello Amos,

thank you for your answer!

On Thu, Sep 14, Amos Jeffries wrote:

> On 14/09/17 18:08, Dieter Bloms wrote:
> > 
> > As I said before, squid works fine and checks the acls, but I get many
> > warnings in the cache.log and don't know the cause of it.
> 
> The cause of it is a change to how external ACL locate their state data in
> Squid-4, so they can use logformat codes.
> 
> What access control(s) are you using this helper with?

http_access deny blockhostsip
http_access deny blockhostsdomain

logformat blockhosts %ts.%03tu;%>a;%Ss/%03>Hs;%rm;%ru
access_log daemon:/var/log/squid/blockhosts-domains.log blockhosts
blockhostsdomain
access_log daemon:/var/log/squid/blockhosts-ip.log blockhosts
blockhostsip

deny_info ERR_CUSTOM_BLOCKHOSTS blockhostsip
deny_info ERR_CUSTOM_BLOCKHOSTS blockhostsdomain

In the ERR_CUSTOM_BLOCKHOSTS we use one variable %U, which will be
filled by squid.


-- 
Regards

  Dieter

--
I do not get viruses because I do not use MS software.
If you use Outlook then please do not put my email address in your
address-book so that WHEN you get a virus it won't use my address in the
>From field.


From squid3 at treenet.co.nz  Thu Sep 14 22:31:58 2017
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Fri, 15 Sep 2017 10:31:58 +1200
Subject: [squid-users] Squid basic_nsca_auth stopped working after
	update.
In-Reply-To: <1444937124.1914390.1505395325252@mail.yahoo.com>
References: <1444937124.1914390.1505395325252.ref@mail.yahoo.com>
 <1444937124.1914390.1505395325252@mail.yahoo.com>
Message-ID: <ccc60c84-2179-ce5e-5258-e64b1fec462e@treenet.co.nz>

On 15/09/17 01:22, Ronald wrote:
> This morning the server was updated.
> I have? Centos 7 every moning it pushes the updates with yum.
> 
> This morning i got squid updated to :
> Name??????? : squid
> Arch??????? : x86_64
> Epoch?????? : 7
> Version???? : 3.5.20
> Release???? : 10.el7
> 
> The side effect is :
> 2017/09/14 15:02:31 kid1| helperOpenServers: Starting 0/5 
> 'basic_ncsa_auth' processes
> 2017/09/14 15:02:31 kid1| helperOpenServers: No 'basic_ncsa_auth' 
> processes needed.

So Squid thinks 5 helpers are already running and no new ones are 
needed. Is that wrong?

Amos


From eliezer at ngtech.co.il  Fri Sep 15 00:48:30 2017
From: eliezer at ngtech.co.il (Eliezer Croitoru)
Date: Fri, 15 Sep 2017 03:48:30 +0300
Subject: [squid-users] Squid basic_nsca_auth stopped working after
	update.
In-Reply-To: <1444937124.1914390.1505395325252@mail.yahoo.com>
References: <1444937124.1914390.1505395325252.ref@mail.yahoo.com>
 <1444937124.1914390.1505395325252@mail.yahoo.com>
Message-ID: <!&!AAAAAAAAAAAuAAAAAAAAAGDVRrtF13VDjYYZR9MgvgYBAMO2jhD3dRHOtM0AqgC7tuYAAAAAAA4AABAAAADb4RPbq4v/SanN5VZsC77yAQAAAAA=@ngtech.co.il>

Can you try changing the line into:

auth_param basic children 5 startup=5 idle=1

 

and then see what happens?

 

----

Eliezer Croitoru <http://ngtech.co.il/lmgtfy/> 
Linux System Administrator
Mobile: +972-5-28704261
Email: eliezer at ngtech.co.il



 

From: squid-users [mailto:squid-users-bounces at lists.squid-cache.org] On Behalf Of Ronald
Sent: Thursday, September 14, 2017 16:22
To: squid-users at lists.squid-cache.org
Subject: [squid-users] Squid basic_nsca_auth stopped working after update.

 

This morning the server was updated.

I have  Centos 7 every moning it pushes the updates with yum.

 

This morning i got squid updated to :

Name        : squid
Arch        : x86_64
Epoch       : 7
Version     : 3.5.20
Release     : 10.el7

 

The side effect is :

2017/09/14 15:02:31 kid1| helperOpenServers: Starting 0/5 'basic_ncsa_auth' processes
2017/09/14 15:02:31 kid1| helperOpenServers: No 'basic_ncsa_auth' processes needed.

 

Config file :

 

auth_param basic program /usr/lib64/squid/basic_ncsa_auth /etc/squid/passwd
auth_param basic children 5
auth_param basic realm Squid proxy-caching web server
auth_param basic credentialsttl 2 hours
auth_param basic casesensitive off
acl ncsa_users proxy_auth REQUIRED
http_access allow ncsa_users
http_access deny !ncsa_users

 

It worked month before and i tested the config line again. Still responded like it should. Did an account test and got an OK back from the tool.

But squid says nothing installed..

 

Anyone an idea how to fix this problem ?

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20170915/6a7de246/attachment.htm>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: image003.png
Type: image/png
Size: 11295 bytes
Desc: not available
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20170915/6a7de246/attachment.png>

From p.schaefer at creapptive.de  Fri Sep 15 00:53:17 2017
From: p.schaefer at creapptive.de (=?UTF-8?Q?Pascal_Sch=c3=a4fer?=)
Date: Fri, 15 Sep 2017 02:53:17 +0200
Subject: [squid-users] Squid radius Authentication
Message-ID: <add1e454-7e0a-250b-6ae3-f99cb6923ce5@creapptive.de>

Dear Ladies and Gentlemen,

I have a question about the authentication with a radius server.
I use Squid as a reverse proxy.
It is possible to use two radius server for different pages or
subdomains with squid_radius_auth?
I think about a maybe special configuration.
I try to use radius server A for the  website A and to use the radius
server B for the website B. Maybe it is good to know that the website A
is on web server A and Website B is on web server B.
I would like to use one Squid server instead of two Squid server (and
two port fowardings).

A Example of my configuration:

https://A.domain.com/... -> authentication over Radius Server A
https://B.domain.com/... -> authentication over Radius Server B

When I search on Google I don't found an acceptable answer for my question.
Should I program such function on my own or know someone a configuration
that work for my project?

With best regards


From matheusvf at gmail.com  Fri Sep 15 12:58:49 2017
From: matheusvf at gmail.com (Matheus Fernandes)
Date: Fri, 15 Sep 2017 09:58:49 -0300
Subject: [squid-users] Website pointed to 127.0.0.1
Message-ID: <CAA-wg2_h=7rcEi85V2J-wDVK3hPpVwnW7ia1v-KH9K=NDURRyA@mail.gmail.com>

Hello!
I have a fqdn that points to 127.0.0.1, when I try to access it through
squid, I get an error. I need to make it process on the same machine that
made the request, and not on squid server. I tried using always_direct
directive, but squid always tries to process at the server side.

This issue is the same presented at
http://lists.squid-cache.org/pipermail/squid-users/2015-May/003477.html
except that in my case I have hundreds of computers running squid, making
it a lot difficult to put an exception on every single browser.

Is there any way around this?

Thanks
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20170915/0b2c619a/attachment.htm>

From uhlar at fantomas.sk  Fri Sep 15 13:17:31 2017
From: uhlar at fantomas.sk (Matus UHLAR - fantomas)
Date: Fri, 15 Sep 2017 15:17:31 +0200
Subject: [squid-users] Website pointed to 127.0.0.1
In-Reply-To: <CAA-wg2_h=7rcEi85V2J-wDVK3hPpVwnW7ia1v-KH9K=NDURRyA@mail.gmail.com>
References: <CAA-wg2_h=7rcEi85V2J-wDVK3hPpVwnW7ia1v-KH9K=NDURRyA@mail.gmail.com>
Message-ID: <20170915131731.GA8755@fantomas.sk>

On 15.09.17 09:58, Matheus Fernandes wrote:
>I have a fqdn that points to 127.0.0.1, when I try to access it through
>squid, I get an error.

It's forbidden with default rules. 

> I need to make it process on the same machine that
>made the request, and not on squid server. I tried using always_direct
>directive, but squid always tries to process at the server side.

1. if you want your browser to connect directly, you must configure the
browser to connect directly, e.g. put localhost into "no proxy for" list.

you can NOT do that with squid.  When browser connects to squid, it's
already too late to do anything, since the connection was already created.

2. it is not possible for any other machine to connect to your local
address. That it how local address is defined.
Therefore, squid can not connect to your local interface.

-- 
Matus UHLAR - fantomas, uhlar at fantomas.sk ; http://www.fantomas.sk/
Warning: I wish NOT to receive e-mail advertising to this address.
Varovanie: na tuto adresu chcem NEDOSTAVAT akukolvek reklamnu postu.
Depression is merely anger without enthusiasm. 


From acrow at integrafin.co.uk  Fri Sep 15 13:19:09 2017
From: acrow at integrafin.co.uk (Alex Crow)
Date: Fri, 15 Sep 2017 14:19:09 +0100
Subject: [squid-users] Website pointed to 127.0.0.1
In-Reply-To: <CAA-wg2_h=7rcEi85V2J-wDVK3hPpVwnW7ia1v-KH9K=NDURRyA@mail.gmail.com>
References: <CAA-wg2_h=7rcEi85V2J-wDVK3hPpVwnW7ia1v-KH9K=NDURRyA@mail.gmail.com>
Message-ID: <1cb7f7df-6084-4a94-eea4-57f03eaeea1a@integrafin.co.uk>


On 15/09/17 13:58, Matheus Fernandes wrote:
> Hello!
> I have a fqdn that points to 127.0.0.1, when I try to access it 
> through squid, I get an error. I need to make it process on the same 
> machine that made the request, and not on squid server. I tried using 
> always_direct directive, but squid always tries to process at the 
> server side.
>
> This issue is the same presented at 
> http://lists.squid-cache.org/pipermail/squid-users/2015-May/003477.html
> except that in my case I have hundreds of computers running squid, 
> making it a lot difficult to put an exception on every single browser.
>
> Is there any way around this?
>
> Thanks
>
WPAD/PAC auto-config for your browser?

http://findproxyforurl.com/wpad-introduction/ 
<https://forum.palemoon.org/index.php>

--
This message is intended only for the addressee and may contain
confidential information. Unless you are that person, you may not
disclose its contents or use it in any way and are requested to delete
the message along with any attachments and notify us immediately.
This email is not intended to, nor should it be taken to, constitute advice.
The information provided is correct to our knowledge & belief and must not
be used as a substitute for obtaining tax, regulatory, investment, legal or
any other appropriate advice.

"Transact" is operated by Integrated Financial Arrangements Ltd.
29 Clement's Lane, London EC4N 7AE. Tel: (020) 7608 4900 Fax: (020) 7608 5300.
(Registered office: as above; Registered in England and Wales under
number: 3727592). Authorised and regulated by the Financial Conduct
Authority (entered on the Financial Services Register; no. 190856).
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20170915/205db042/attachment.htm>

From squid3 at treenet.co.nz  Fri Sep 15 13:49:24 2017
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sat, 16 Sep 2017 01:49:24 +1200
Subject: [squid-users] Squid radius Authentication
In-Reply-To: <add1e454-7e0a-250b-6ae3-f99cb6923ce5@creapptive.de>
References: <add1e454-7e0a-250b-6ae3-f99cb6923ce5@creapptive.de>
Message-ID: <803561cf-b8f8-f265-4b46-5335019d1380@treenet.co.nz>

On 15/09/17 12:53, Pascal Sch?fer wrote:
> Dear Ladies and Gentlemen,
> 
> I have a question about the authentication with a radius server.
> I use Squid as a reverse proxy.
> It is possible to use two radius server for different pages or
> subdomains with squid_radius_auth?

HTTP has no concept of "page" - so for that; no.

For sub-domains (OR specific URLs); maybe. Because the helper you are 
asking about does not use the key_extras feature provided by latest 
Squid versions.

You need to write your own helper that does what you want. That could be 
in the form of a wrapper script that starts multiple radius helper with 
the necessary parameters, and uses key_extra parameters to decide which 
one will handle any given auth lookup.

Since you are calling it the long obsolete name "squid_radius_auth", you 
probably do not have a current Squid version which supplies the 
key_extras feature. At the very least you will have to upgrade to at 
least Squid-3.5.

Amos


From p.schaefer at creapptive.de  Fri Sep 15 14:31:11 2017
From: p.schaefer at creapptive.de (=?UTF-8?Q?Pascal_Sch=c3=a4fer?=)
Date: Fri, 15 Sep 2017 16:31:11 +0200
Subject: [squid-users] Squid radius Authentication
In-Reply-To: <803561cf-b8f8-f265-4b46-5335019d1380@treenet.co.nz>
References: <add1e454-7e0a-250b-6ae3-f99cb6923ce5@creapptive.de>
 <803561cf-b8f8-f265-4b46-5335019d1380@treenet.co.nz>
Message-ID: <336c2b18-16a7-efb4-35ba-8979ba0442c2@creapptive.de>

Dear Amos,

Thank you for your reply!

>>
>> I have a question about the authentication with a radius server.
>> I use Squid as a reverse proxy.
>> It is possible to use two radius server for different pages or
>> subdomains with squid_radius_auth?
> 
> HTTP has no concept of "page" - so for that; no.
> 
> For sub-domains (OR specific URLs); maybe. Because the helper you are
> asking about does not use the key_extras feature provided by latest
> Squid version

Ok. Thank you. Exist another helper who did an authentication with a
radius server?

> 
> You need to write your own helper that does what you want. That could be
> in the form of a wrapper script that starts multiple radius helper with
> the necessary parameters, and uses key_extra parameters to decide which
> one will handle any given auth lookup.

Is this https://wiki.squid-cache.org/Features/AddonHelpers#Authenticator
the right wiki, where I have to lookup?
Make it sense that behind the radius server is a Windows NPS Server to
authenticate the Users?
So when I write the wrapper helper, I only need to decide which helper I
would like to start and with which parameters, like a Bash command?

> 
> Since you are calling it the long obsolete name "squid_radius_auth", you
> probably do not have a current Squid version which supplies the
> key_extras feature. At the very least you will have to upgrade to at
> least Squid-3.5.

I have a Squid-3.5, self compiled.
I think about to upgrade there on Squid-4 or to compile it and install
them fresh on the system. Is the name of them another in the newer versions?

best regards,

Pascal

> 
> Amos
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users


From squid3 at treenet.co.nz  Fri Sep 15 15:26:54 2017
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sat, 16 Sep 2017 03:26:54 +1200
Subject: [squid-users] Squid radius Authentication
In-Reply-To: <336c2b18-16a7-efb4-35ba-8979ba0442c2@creapptive.de>
References: <add1e454-7e0a-250b-6ae3-f99cb6923ce5@creapptive.de>
 <803561cf-b8f8-f265-4b46-5335019d1380@treenet.co.nz>
 <336c2b18-16a7-efb4-35ba-8979ba0442c2@creapptive.de>
Message-ID: <660c72f0-4806-8f02-7443-d735ea15fcbd@treenet.co.nz>

On 16/09/17 02:31, Pascal Sch?fer wrote:
> Dear Amos,
> 
> Thank you for your reply!
> 
>>>
>>> I have a question about the authentication with a radius server.
>>> I use Squid as a reverse proxy.
>>> It is possible to use two radius server for different pages or
>>> subdomains with squid_radius_auth?
>>
>> HTTP has no concept of "page" - so for that; no.
>>
>> For sub-domains (OR specific URLs); maybe. Because the helper you are
>> asking about does not use the key_extras feature provided by latest
>> Squid version
> 
> Ok. Thank you. Exist another helper who did an authentication with a
> radius server?
> 

I am aware of some proprietary ones existing. But that is not useful for 
you.

>>
>> You need to write your own helper that does what you want. That could be
>> in the form of a wrapper script that starts multiple radius helper with
>> the necessary parameters, and uses key_extra parameters to decide which
>> one will handle any given auth lookup.
> 
> Is this https://wiki.squid-cache.org/Features/AddonHelpers#Authenticator
> the right wiki, where I have to lookup?

That page describes the protocol Squid will be talking to your script 
with; and what is expected to arrive back.

> Make it sense that behind the radius server is a Windows NPS Server to
> authenticate the Users?

That does not matter unless you are writing the RADIUS parts yourself. 
In which case I cannot help, not knowing much about RADIUS protocol.


> So when I write the wrapper helper, I only need to decide which helper I
> would like to start and with which parameters, like a Bash command?
> 

Yes. Though helpers are required to run until Squid stops them. So best 
to start the child radius helpers at the beginning then just relay query 
and response lines appropriately when they arrive.


>>
>> Since you are calling it the long obsolete name "squid_radius_auth", you
>> probably do not have a current Squid version which supplies the
>> key_extras feature. At the very least you will have to upgrade to at
>> least Squid-3.5.
> 
> I have a Squid-3.5, self compiled.
> I think about to upgrade there on Squid-4 or to compile it and install
> them fresh on the system. Is the name of them another in the newer versions?

Then you should be fine, except "basic_radius_auth" is the helper binary 
name since Squid-3.2.


Amos


From eliezer at ngtech.co.il  Sun Sep 17 03:57:20 2017
From: eliezer at ngtech.co.il (Eliezer Croitoru)
Date: Sun, 17 Sep 2017 06:57:20 +0300
Subject: [squid-users] Squid radius Authentication
In-Reply-To: <add1e454-7e0a-250b-6ae3-f99cb6923ce5@creapptive.de>
References: <add1e454-7e0a-250b-6ae3-f99cb6923ce5@creapptive.de>
Message-ID: <12c401d32f69$0f9aa160$2ecfe420$@ngtech.co.il>

Hey,

What kind of authentication do you want\need? Basic?
Depends on your needs there might be a helper that you can use.
If you have only two domains\subdomains it's one thing but if you have more then these then the program would be different.

If I will have more details I might be able to answer your question and I maybe even have a radius authentication helper written somewhere which I can pull.

Eliezer

----
Eliezer Croitoru
Linux System Administrator
Mobile: +972-5-28704261
Email: eliezer at ngtech.co.il



-----Original Message-----
From: squid-users [mailto:squid-users-bounces at lists.squid-cache.org] On Behalf Of Pascal Sch?fer
Sent: Friday, September 15, 2017 03:53
To: squid-users at lists.squid-cache.org
Subject: [squid-users] Squid radius Authentication

Dear Ladies and Gentlemen,

I have a question about the authentication with a radius server.
I use Squid as a reverse proxy.
It is possible to use two radius server for different pages or
subdomains with squid_radius_auth?
I think about a maybe special configuration.
I try to use radius server A for the  website A and to use the radius
server B for the website B. Maybe it is good to know that the website A
is on web server A and Website B is on web server B.
I would like to use one Squid server instead of two Squid server (and
two port fowardings).

A Example of my configuration:

https://A.domain.com/... -> authentication over Radius Server A
https://B.domain.com/... -> authentication over Radius Server B

When I search on Google I don't found an acceptable answer for my question.
Should I program such function on my own or know someone a configuration
that work for my project?

With best regards
_______________________________________________
squid-users mailing list
squid-users at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-users



From rentorbuy at yahoo.com  Mon Sep 18 08:43:12 2017
From: rentorbuy at yahoo.com (Vieri)
Date: Mon, 18 Sep 2017 08:43:12 +0000 (UTC)
Subject: [squid-users] urlpath_regex negative assertions
References: <1388669959.4086755.1505724192971.ref@mail.yahoo.com>
Message-ID: <1388669959.4086755.1505724192971@mail.yahoo.com>

Hi,

I'd like to block access to URLs ending in *.dll except for those ending in mriweb.dll.

acl denied_filetypes urlpath_regex -i denied.filetypes

where denied.filetypes contains a list of expressions of which:

(\?!mriweb\.dll$).*\.dll$

This doesn't seem to work if I try to deny access.
eg. an http client can access http://whatever/mriweb_test.dll when it shouldn't.

Where's my mistake?

Thanks,

Vieri


From Antony.Stone at squid.open.source.it  Mon Sep 18 09:04:29 2017
From: Antony.Stone at squid.open.source.it (Antony Stone)
Date: Mon, 18 Sep 2017 10:04:29 +0100
Subject: [squid-users] urlpath_regex negative assertions
In-Reply-To: <1388669959.4086755.1505724192971@mail.yahoo.com>
References: <1388669959.4086755.1505724192971.ref@mail.yahoo.com>
 <1388669959.4086755.1505724192971@mail.yahoo.com>
Message-ID: <201709181004.29886.Antony.Stone@squid.open.source.it>

On Monday 18 September 2017 at 09:43:12, Vieri wrote:

> Hi,
> 
> I'd like to block access to URLs ending in *.dll except for those ending in
> mriweb.dll.
> 
> acl denied_filetypes urlpath_regex -i denied.filetypes
> 
> where denied.filetypes contains a list of expressions

Are the others working?

> of which:
> 
> (\?!mriweb\.dll$).*\.dll$

You have that regex all on one line?

> This doesn't seem to work if I try to deny access.
> eg. an http client can access http://whatever/mriweb_test.dll when it
> shouldn't.
> 
> Where's my mistake?

You need to have each possible matching regex on a separate line in the file.


Antony.

-- 
How I want a drink, alcoholic of course, after the heavy chapters involving 
quantum mechanics.

 - mnemonic for 3.14159265358979

                                                   Please reply to the list;
                                                         please *don't* CC me.


From squid3 at treenet.co.nz  Mon Sep 18 09:40:21 2017
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Mon, 18 Sep 2017 21:40:21 +1200
Subject: [squid-users] urlpath_regex negative assertions
In-Reply-To: <201709181004.29886.Antony.Stone@squid.open.source.it>
References: <1388669959.4086755.1505724192971.ref@mail.yahoo.com>
 <1388669959.4086755.1505724192971@mail.yahoo.com>
 <201709181004.29886.Antony.Stone@squid.open.source.it>
Message-ID: <ea2500d2-0e46-04cf-95cf-72558342aa24@treenet.co.nz>

On 18/09/17 21:04, Antony Stone wrote:
> On Monday 18 September 2017 at 09:43:12, Vieri wrote:
> 
>> Hi,
>>
>> I'd like to block access to URLs ending in *.dll except for those ending in
>> mriweb.dll.
>>
>> acl denied_filetypes urlpath_regex -i denied.filetypes
>>
>> where denied.filetypes contains a list of expressions
> 
> Are the others working?
> 
>> of which:
>>
>> (\?!mriweb\.dll$).*\.dll$
> 
> You have that regex all on one line?
> 
>> This doesn't seem to work if I try to deny access.
>> eg. an http client can access http://whatever/mriweb_test.dll when it
>> shouldn't.
>>
>> Where's my mistake?
> 
> You need to have each possible matching regex on a separate line in the file.
> 

Also the '\' in '\?' makes it mean exact character match - any special 
meaning (like doing a lookahead) is prevented.

Amos


From rentorbuy at yahoo.com  Mon Sep 18 11:54:10 2017
From: rentorbuy at yahoo.com (Vieri)
Date: Mon, 18 Sep 2017 11:54:10 +0000 (UTC)
Subject: [squid-users] urlpath_regex negative assertions
In-Reply-To: <ea2500d2-0e46-04cf-95cf-72558342aa24@treenet.co.nz>
References: <1388669959.4086755.1505724192971.ref@mail.yahoo.com>
 <1388669959.4086755.1505724192971@mail.yahoo.com>
 <201709181004.29886.Antony.Stone@squid.open.source.it>
 <ea2500d2-0e46-04cf-95cf-72558342aa24@treenet.co.nz>
Message-ID: <1493799289.4150921.1505735650310@mail.yahoo.com>


________________________________
From: Amos Jeffries <squid3 at treenet.co.nz>
> any special meaning (like doing a lookahead) is prevented.


OK, so I'll do an acl for deny and another for allow.

Thanks


From squid at mail.verwaiser.de  Mon Sep 18 13:45:48 2017
From: squid at mail.verwaiser.de (Verwaiser)
Date: Mon, 18 Sep 2017 06:45:48 -0700 (MST)
Subject: [squid-users] disable access.log logging on a specific entrys
Message-ID: <1505742348570-0.post@n4.nabble.com>

Hello,our access.log is filled up to 95% with useless entrys:
192.168.12.84 - - [18/Sep/2017:15:22:40 +0200] "POST
/SpamResolverNG/SpamResolverNG.dll?DoNewRequest HTTP/1.1" 400 3990
TAG_NONE:HIER_NONE
192.168.12.117 - - [18/Sep/2017:15:22:40 +0200] "POST
/SpamResolverNG/SpamResolverNG.dll?DoNewRequest HTTP/1.1" 400 3991
TAG_NONE:HIER_NONE
192.168.12.84 - - [18/Sep/2017:15:22:41 +0200] "POST
/SpamResolverNG/SpamResolverNG.dll?DoNewRequest HTTP/1.1" 400 3990
TAG_NONE:HIER_NONE
192.168.12.118 - - [18/Sep/2017:15:22:41 +0200] "POST
/SpamResolverNG/SpamResolverNG.dll?DoNewRequest HTTP/1.1" 400 3991
TAG_NONE:HIER_NONE
192.168.12.121 - - [18/Sep/2017:15:22:41 +0200] "POST
/SpamResolverNG/SpamResolverNG.dll?DoNewRequest HTTP/1.1" 400 3991
TAG_NONE:HIER_NONE
192.168.13.60 - - [18/Sep/2017:15:22:41 +0200] "POST
/SpamResolverNG/SpamResolverNG.dll?DoNewRequest HTTP/1.1" 400 3990
TAG_NONE:HIER_NONE
192.168.12.89 - - [18/Sep/2017:15:22:41 +0200] "POST
/SpamResolverNG/SpamResolverNG.dll?DoNewRequest HTTP/1.1" 400 3990
TAG_NONE:HIER_NONE
...
I think, the antivirus on several workstations will produce these entrys.
How can I get rid of them?
What I've tried:
1.)acl logNoSpamresolver url_regex -i SpamResolverNG
access_log /var/log/squid/access.log common !logNoSpamresolver

This will not filter the entrys from access.log


2.)
acl logNoSpamresolver url_regex -i http
access_log /var/log/squid/access.log common logNoSpamresolver

Now only urls with "http" will be loggeg, https or other are ignored too


Does anybody know a solution for this problem?

Holger




--
Sent from: http://squid-web-proxy-cache.1019090.n4.nabble.com/Squid-Users-f1019091.html


From rentorbuy at yahoo.com  Mon Sep 18 17:01:42 2017
From: rentorbuy at yahoo.com (Vieri)
Date: Mon, 18 Sep 2017 17:01:42 +0000 (UTC)
Subject: [squid-users] TCP: out of memory -- consider tuning tcp_mem
References: <604484031.4405155.1505754102804.ref@mail.yahoo.com>
Message-ID: <604484031.4405155.1505754102804@mail.yahoo.com>

Hi again,

I'm suddenly getting these errors in the log:

2017/09/18 18:13:48 kid1| Error negotiating SSL on FD 11010: error:1409F07F:SSL routines:ssl3_write_pending:bad write retry (1/-1/0)
2017/09/18 18:13:57 kid1| Error negotiating SSL on FD 11124: error:1409F07F:SSL routines:ssl3_write_pending:bad write retry (1/-1/0)
2017/09/18 18:13:57 kid1| Error negotiating SSL on FD 11124: error:1409F07F:SSL routines:ssl3_write_pending:bad write retry (1/-1/0)
2017/09/18 18:14:00 kid1| Error negotiating SSL connection on FD 11064: error:1408A0C1:SSL routines:ssl3_get_client_hello:no shared cipher (1/-1)
2017/09/18 18:14:00 kid1| Error negotiating SSL connection on FD 11064: error:1408A0C1:SSL routines:ssl3_get_client_hello:no shared cipher (1/-1)
2017/09/18 18:14:03 kid1| Error negotiating SSL connection on FD 10857: error:1408A0C1:SSL routines:ssl3_get_client_hello:no shared cipher (1/-1)
2017/09/18 18:14:04 kid1| Error negotiating SSL connection on FD 10857: error:1408A0C1:SSL routines:ssl3_get_client_hello:no shared cipher (1/-1)

This must be a kernel issue because I'm getting lots of these in /var/log/messages:

kernel: TCP: out of memory -- consider tuning tcp_mem

Here are my values:

# sysctl net.ipv4.tcp_mem
net.ipv4.tcp_mem = 384027       512036  768054
# sysctl net.ipv4.tcp_rmem
net.ipv4.tcp_rmem = 4096        87380   6291456
# sysctl net.ipv4.tcp_wmem
net.ipv4.tcp_wmem = 4096        16384   4194304
# sysctl net.core.rmem_max
net.core.rmem_max = 212992
# sysctl net.core.wmem_max
net.core.wmem_max = 212992

# uname -a
Linux inf-fw2 4.9.34-gentoo #1 SMP Mon Jul 10 11:05:23 CEST 2017 x86_64 AMD FX(tm)-8320 Eight-Core Processor AuthenticAMD GNU/Linux

# top
top - 17:51:33 up 19 days, 10:18,  2 users,  load average: 1.38, 1.49, 1.42
Tasks: 344 total,   1 running, 343 sleeping,   0 stopped,   0 zombie
%Cpu0  :  2.2 us,  0.5 sy,  0.0 ni, 93.0 id,  0.0 wa,  0.0 hi,  4.3 si,  0.0 st
%Cpu1  :  0.5 us,  0.0 sy,  0.0 ni, 97.9 id,  0.0 wa,  0.0 hi,  1.6 si,  0.0 st
%Cpu2  :  1.1 us,  0.0 sy,  0.5 ni, 95.2 id,  0.0 wa,  0.0 hi,  3.2 si,  0.0 st
%Cpu3  :  1.1 us,  0.5 sy,  0.0 ni, 96.3 id,  0.0 wa,  0.0 hi,  2.1 si,  0.0 st
%Cpu4  :  2.1 us,  0.0 sy,  0.0 ni, 96.3 id,  0.0 wa,  0.0 hi,  1.6 si,  0.0 st
%Cpu5  :  0.5 us,  0.0 sy,  0.0 ni, 98.9 id,  0.0 wa,  0.0 hi,  0.5 si,  0.0 st
%Cpu6  :  0.5 us,  1.1 sy,  0.0 ni, 96.8 id,  0.0 wa,  0.0 hi,  1.6 si,  0.0 st
%Cpu7  :  1.6 us,  0.0 sy,  0.0 ni, 90.9 id,  0.0 wa,  0.0 hi,  7.5 si,  0.0 st
KiB Mem : 32865056 total,   820664 free, 20358972 used, 11685420 buff/cache
KiB Swap: 37036988 total, 34924984 free,  2112004 used. 12014564 avail Mem

# cat /proc/net/sockstat
sockets: used 13121
TCP: inuse 10010 orphan 11 tw 246 alloc 12597 mem 772909
UDP: inuse 92 mem 59
UDPLITE: inuse 0
RAW: inuse 7
FRAG: inuse 0 memory 0

# cat /proc/net/sockstat6
TCP6: inuse 282
UDP6: inuse 40
UDPLITE6: inuse 0
RAW6: inuse 5
FRAG6: inuse 0 memory 0

#  sysctl -a |grep tcp
fs.nfs.nfs_callback_tcpport = 0
fs.nfs.nlm_tcpport = 0
net.ipv4.tcp_abort_on_overflow = 0
net.ipv4.tcp_adv_win_scale = 1
net.ipv4.tcp_allowed_congestion_control = cubic reno
net.ipv4.tcp_app_win = 31
net.ipv4.tcp_autocorking = 1
net.ipv4.tcp_available_congestion_control = cubic reno
net.ipv4.tcp_base_mss = 1024
net.ipv4.tcp_challenge_ack_limit = 1000
sysctl: net.ipv4.tcp_congestion_control = cubic
reading key "net.ipv6.conf.all.stable_secret"net.ipv4.tcp_dsack = 1

net.ipv4.tcp_early_retrans = 3
net.ipv4.tcp_ecn = 2
net.ipv4.tcp_ecn_fallback = 1
net.ipv4.tcp_fack = 1
net.ipv4.tcp_fastopen = 1
net.ipv4.tcp_fastopen_key = 6707aeac-2dd079df-0dee3da3-befd1107
net.ipv4.tcp_fin_timeout = 60
net.ipv4.tcp_frto = 2
net.ipv4.tcp_fwmark_accept = 0
net.ipv4.tcp_invalid_ratelimit = 500
net.ipv4.tcp_keepalive_intvl = 75
net.ipv4.tcp_keepalive_probes = 9
net.ipv4.tcp_keepalive_time = 7200
net.ipv4.tcp_limit_output_bytes = 262144
net.ipv4.tcp_low_latency = 0
net.ipv4.tcp_max_orphans = 131072
net.ipv4.tcp_max_reordering = 300
net.ipv4.tcp_max_syn_backlog = 1024
net.ipv4.tcp_max_tw_buckets = 131072
net.ipv4.tcp_mem = 384027       512036  768054
net.ipv4.tcp_min_rtt_wlen = 300
net.ipv4.tcp_min_tso_segs = 2
net.ipv4.tcp_moderate_rcvbuf = 1
net.ipv4.tcp_mtu_probing = 0
net.ipv4.tcp_no_metrics_save = 0
net.ipv4.tcp_notsent_lowat = -1
net.ipv4.tcp_orphan_retries = 0
net.ipv4.tcp_pacing_ca_ratio = 120
net.ipv4.tcp_pacing_ss_ratio = 200
net.ipv4.tcp_probe_interval = 600
net.ipv4.tcp_probe_threshold = 8
net.ipv4.tcp_recovery = 1
net.ipv4.tcp_reordering = 3
net.ipv4.tcp_retrans_collapse = 1
net.ipv4.tcp_retries1 = 3
net.ipv4.tcp_retries2 = 15
net.ipv4.tcp_rfc1337 = 0
net.ipv4.tcp_rmem = 4096        87380   6291456
net.ipv4.tcp_sack = 1
net.ipv4.tcp_slow_start_after_idle = 1
net.ipv4.tcp_stdurg = 0
net.ipv4.tcp_syn_retries = 6
net.ipv4.tcp_synack_retries = 5
net.ipv4.tcp_syncookies = 1
net.ipv4.tcp_thin_dupack = 0
net.ipv4.tcp_thin_linear_timeouts = 0
net.ipv4.tcp_timestamps = 1
net.ipv4.tcp_tso_win_divisor = 3
net.ipv4.tcp_tw_recycle = 0
net.ipv4.tcp_tw_reuse = 0
net.ipv4.tcp_window_scaling = 1
net.ipv4.tcp_wmem = 4096        16384   4194304
net.ipv4.tcp_workaround_signed_windows = 0
sysctl: reading key "net.ipv6.conf.default.stable_secret"
sysctl: reading key "net.ipv6.conf.enp10s0.stable_secret"
sysctl: reading key "net.ipv6.conf.enp5s0.stable_secret"
sysctl: reading key "net.ipv6.conf.enp6s0.stable_secret"
sysctl: reading key "net.ipv6.conf.enp7s0f0.stable_secret"
sysctl: reading key "net.ipv6.conf.enp7s0f1.stable_secret"
sysctl: reading key "net.ipv6.conf.enp7s0f2.stable_secret"
sysctl: reading key "net.ipv6.conf.enp7s0f3.stable_secret"
sysctl: reading key "net.ipv6.conf.enp8s5.stable_secret"
sysctl: reading key "net.ipv6.conf.lo.stable_secret"
net.netfilter.nf_conntrack_tcp_be_liberal = 0
net.netfilter.nf_conntrack_tcp_loose = 1
net.netfilter.nf_conntrack_tcp_max_retrans = 3
net.netfilter.nf_conntrack_tcp_timeout_close = 10
net.netfilter.nf_conntrack_tcp_timeout_close_wait = 60
net.netfilter.nf_conntrack_tcp_timeout_established = 432000
net.netfilter.nf_conntrack_tcp_timeout_fin_wait = 120
net.netfilter.nf_conntrack_tcp_timeout_last_ack = 30
net.netfilter.nf_conntrack_tcp_timeout_max_retrans = 300
net.netfilter.nf_conntrack_tcp_timeout_syn_recv = 60
net.netfilter.nf_conntrack_tcp_timeout_syn_sent = 120
net.netfilter.nf_conntrack_tcp_timeout_time_wait = 120
net.netfilter.nf_conntrack_tcp_timeout_unacknowledged = 300

Obviously, I'm having connection issues now.


Some suggest to increase tcp_mem, others say not to, but increase the other values such as:

sysctl -w net.core.rmem_max=8738000
sysctl -w net.core.wmem_max=6553600
sysctl -w net.ipv4.tcp_rmem=8192 873800 8738000
sysctl -w net.ipv4.tcp_wmem=4096 655360 6553600


Others suggest to also increase net.ipv4.tcp_max_orphans.


Can anyone please advise?
Why aren't the kernel defaults enough?
In any case, how should I calculate my optimum values given my RAM?

Also, if the kernel defaults are sensible then how can I find out if there's a memory leak?

If I stop/start squid 3.5.26 then the issue is solved, at least for some time.

Thanks,

Vieri


From rentorbuy at yahoo.com  Mon Sep 18 21:27:32 2017
From: rentorbuy at yahoo.com (Vieri)
Date: Mon, 18 Sep 2017 21:27:32 +0000 (UTC)
Subject: [squid-users] TCP: out of memory -- consider tuning tcp_mem
References: <1738890086.3864900.1505770052846.ref@mail.yahoo.com>
Message-ID: <1738890086.3864900.1505770052846@mail.yahoo.com>

Regarding my previous post, here's some more info.

On a 32 GB RAM system, RAM usage grew up to 20GB when I had those "out of memory" messages.

After restarting or even shutting down all 5 Squid instances I still get high RAM usage albeit lower than before.

# top

top - 22:42:23 up 19 days, 15:08,  2 users,  load average: 1.30, 1.48, 1.50
Tasks: 342 total,   1 running, 341 sleeping,   0 stopped,   0 zombie
%Cpu0  :  1.5 us,  1.5 sy,  0.0 ni, 95.5 id,  0.0 wa,  0.0 hi,  1.5 si,  0.0 st
%Cpu1  :  0.0 us,  0.0 sy,  0.0 ni, 98.5 id,  0.0 wa,  0.0 hi,  1.5 si,  0.0 st
%Cpu2  :  0.0 us,  0.0 sy,  0.0 ni, 98.5 id,  0.0 wa,  0.0 hi,  1.5 si,  0.0 st
%Cpu3  :  0.0 us,  0.0 sy,  0.0 ni, 97.0 id,  0.0 wa,  0.0 hi,  3.0 si,  0.0 st
%Cpu4  :  0.0 us,  0.0 sy,  0.0 ni, 97.0 id,  0.0 wa,  0.0 hi,  3.0 si,  0.0 st
%Cpu5  :  0.0 us,  0.0 sy,  0.0 ni,100.0 id,  0.0 wa,  0.0 hi,  0.0 si,  0.0 st
%Cpu6  :  0.0 us,  0.0 sy,  0.0 ni,100.0 id,  0.0 wa,  0.0 hi,  0.0 si,  0.0 st
%Cpu7  :  0.0 us,  0.0 sy,  0.0 ni, 95.5 id,  0.0 wa,  0.0 hi,  4.5 si,  0.0 st
KiB Mem : 32865056 total,  3679916 free, 17272208 used, 11912932 buff/cache
KiB Swap: 37036988 total, 35700916 free,  1336072 used. 15100248 avail Mem 

A "ps aux --sort -rss" show c-icap in the lead, so I restarted it.

I then got this reading:

# top

top - 22:55:15 up 19 days, 15:21,  2 users,  load average: 0.91, 1.06, 1.32
Tasks: 276 total,   1 running, 275 sleeping,   0 stopped,   0 zombie
%Cpu0  :  0.0 us,  0.0 sy,  0.0 ni, 98.1 id,  0.0 wa,  0.0 hi,  1.9 si,  0.0 st
%Cpu1  :  0.0 us,  0.0 sy,  0.0 ni, 97.2 id,  0.0 wa,  0.0 hi,  2.8 si,  0.0 st
%Cpu2  :  0.0 us,  0.0 sy,  0.0 ni, 99.1 id,  0.0 wa,  0.0 hi,  0.9 si,  0.0 st
%Cpu3  :  0.0 us,  0.9 sy,  0.0 ni, 98.1 id,  0.0 wa,  0.0 hi,  0.9 si,  0.0 st
%Cpu4  :  0.0 us,  0.0 sy,  0.0 ni, 98.1 id,  0.0 wa,  0.0 hi,  1.9 si,  0.0 st
%Cpu5  :  0.0 us,  0.0 sy,  0.0 ni, 99.1 id,  0.0 wa,  0.0 hi,  0.9 si,  0.0 st
%Cpu6  :  0.0 us,  0.0 sy,  0.0 ni, 97.2 id,  0.0 wa,  0.0 hi,  2.8 si,  0.0 st
%Cpu7  :  0.0 us,  0.0 sy,  0.0 ni, 92.5 id,  0.0 wa,  0.0 hi,  7.5 si,  0.0 st
KiB Mem : 32865056 total, 19145456 free,  1809168 used, 11910432 buff/cache
KiB Swap: 37036988 total, 36221980 free,   815008 used. 30568180 avail Mem 

Starting c-icap again, along with all 5 Squid instances yields this:

# top

top - 22:59:20 up 19 days, 15:25,  2 users,  load average: 1.25, 1.06, 1.24
Tasks: 292 total,   1 running, 291 sleeping,   0 stopped,   0 zombie
%Cpu0  :  0.0 us,  0.0 sy,  0.0 ni, 98.3 id,  0.0 wa,  0.0 hi,  1.7 si,  0.0 st
%Cpu1  :  0.0 us,  0.0 sy,  0.0 ni, 97.7 id,  0.0 wa,  0.0 hi,  2.3 si,  0.0 st
%Cpu2  :  0.3 us,  0.0 sy,  0.0 ni, 98.0 id,  0.0 wa,  0.0 hi,  1.7 si,  0.0 st
%Cpu3  :  0.0 us,  0.0 sy,  0.0 ni, 99.0 id,  0.0 wa,  0.0 hi,  1.0 si,  0.0 st
%Cpu4  :  0.3 us,  0.0 sy,  0.0 ni, 98.3 id,  0.0 wa,  0.0 hi,  1.3 si,  0.0 st
%Cpu5  :  0.0 us,  0.0 sy,  0.0 ni, 99.7 id,  0.0 wa,  0.0 hi,  0.3 si,  0.0 st
%Cpu6  :  0.0 us,  0.0 sy,  0.0 ni, 97.3 id,  0.0 wa,  0.0 hi,  2.7 si,  0.0 st
%Cpu7  :  0.0 us,  0.0 sy,  0.0 ni, 93.4 id,  0.0 wa,  0.0 hi,  6.6 si,  0.0 st
KiB Mem : 32865056 total, 19103744 free,  1843460 used, 11917852 buff/cache
KiB Swap: 37036988 total, 36221980 free,   815008 used. 30527568 avail Mem

So, it seems c-icap and/or squidclamav (that's what I'm using for content scanning) are responsible for this.

Nothing apparently relevant in the c-icap log though...

Has anyone experienced issue such as memory leaks with c-icap and/or squidclamav?
I'm using c-icap-0.5.2.

Thanks,

Vieri


From squid3 at treenet.co.nz  Mon Sep 18 22:48:25 2017
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Tue, 19 Sep 2017 10:48:25 +1200
Subject: [squid-users] disable access.log logging on a specific entrys
In-Reply-To: <1505742348570-0.post@n4.nabble.com>
References: <1505742348570-0.post@n4.nabble.com>
Message-ID: <6b90a1c3-e64e-eac5-21f5-ac5d74346655@treenet.co.nz>

On 19/09/17 01:45, Verwaiser wrote:
> 
> Does anybody know a solution for this problem?
> 

What Squid version?

Amos


From duanyao at ustc.edu  Tue Sep 19 06:25:44 2017
From: duanyao at ustc.edu (duanyao)
Date: Tue, 19 Sep 2017 14:25:44 +0800
Subject: [squid-users] Is it a good idea to use Linux swap partition/file
	with rock storage?
Message-ID: <82d9495a-b0c7-8cc9-845e-8dfbcfb950eb@ustc.edu>

Hi,

I notice that squid's rock storage uses large (and fixed) amount of 
shared memory even if it is not accessed. It's estimated as 
110byte/slot, so for a 256GB rock storage with 16KB slot, the memory 
requirement is about 1.7GB, which is quite large.

So my questions are:

1. Is there a way to reduce memory usage of rock storage?

2. On Linux squid puts its shared memory in /dev/shm, which can be 
backed by swap partition/file. Is it a good idea to use swap 
partition/file with rock storage to save some physical memory?

3. For rock storage, are /dev/shm/squid* frequently and randomly 
written? If the Linux swap is on SSD, will this causes 
performance/lifetime issues?

Regards,

Duan Yao



From rafael.akchurin at diladele.com  Tue Sep 19 08:10:45 2017
From: rafael.akchurin at diladele.com (Rafael Akchurin)
Date: Tue, 19 Sep 2017 08:10:45 +0000
Subject: [squid-users] Squid 3.5.27 for Microsoft Windows 64-bit is available
Message-ID: <DB6PR0401MB26804F7667F6DF2AE0D627CD8F600@DB6PR0401MB2680.eurprd04.prod.outlook.com>

Greetings everyone,

Sorry with a huge delay we would like to announce the availability of the CygWin based build of Squid proxy
for Microsoft Windows version 3.5.27 (amd64 only!).

* Original release notes are at http://www.squid-cache.org/Versions/v3/3.5/squid-3.5.27-RELEASENOTES.html .
* Ready to use MSI package can be downloaded from http://squid.diladele.com .
* List of open issues for the installer - https://github.com/diladele/squid-windows/issues

Thanks a lot for Squid developers for making this great software!

Please join our humble efforts to provide ready to run MSI installer for Squid on Microsoft Windows with all required dependencies at GitHub -
https://github.com/diladele/squid-windows . Report all issues/bugs/feature requests at GitHub project.
Issues about the *MSI installer only* can also be reported to support at diladele.com<mailto:support at diladele.com> .

Best regards,
Rafael Akchurin
Diladele B.V.
https://www.diladele.com


----
Cloud Guard URL re-writer for Squid proxy

We would also like to introduce our new research project - cloud based URL rewriter for Squid proxy. In short it
is an URL rewriter that gets integrated with Squid. The rewriter calls into
guard.diladele.com/api/* to process URL rewrite requests.

For now it works in Windows only. We plan to add support for Linux (amd64, MIPS, ARM based),
FreeBSD and pfSense if there is be enough interest for that. The project is in the beta stage now so
please use it as much as possible but on non production systems. Please direct your issues
to support at diladele.com.

Signup/Login is available at https://guard.diladele.com/login/ . Please note, due to early stage
of the project it is only possible to sign up from DE, FR, NL and UK. If you'd like to be notified
of when the project is available in your country, please join our community forum
(https://groups.google.com/d/forum/web-safety) or MailChimp hosted news
letter (http://eepurl.com/vXDPH ).
----

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20170919/27c7efe6/attachment.htm>

From squid3 at treenet.co.nz  Tue Sep 19 09:07:01 2017
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Tue, 19 Sep 2017 21:07:01 +1200
Subject: [squid-users] disable access.log logging on a specific entrys
In-Reply-To: <6b90a1c3-e64e-eac5-21f5-ac5d74346655@treenet.co.nz>
References: <1505742348570-0.post@n4.nabble.com>
 <6b90a1c3-e64e-eac5-21f5-ac5d74346655@treenet.co.nz>
Message-ID: <4861f665-44a4-9bac-657c-f5e5e750548b@treenet.co.nz>

 > -------- Urspr?ngliche Nachricht --------
 > Von: Amos Jeffries
 >
 > On 19/09/17 01:45, Verwaiser wrote:
 >  >
 >  > Does anybody know a solution for this problem?
 >  >
 >
 > What Squid version?
 >
 > Amos

On 19/09/17 20:56, admin wrote:
 > Sorry, I forgot...
 >
 > Squid version 3.5.21
 >

Please try an upgrade. The latest version works fine for me with the 
same config.

Amos


From squid3 at treenet.co.nz  Tue Sep 19 09:43:47 2017
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Tue, 19 Sep 2017 21:43:47 +1200
Subject: [squid-users] Is it a good idea to use Linux swap
 partition/file with rock storage?
In-Reply-To: <82d9495a-b0c7-8cc9-845e-8dfbcfb950eb@ustc.edu>
References: <82d9495a-b0c7-8cc9-845e-8dfbcfb950eb@ustc.edu>
Message-ID: <c29e1690-bc49-c8be-e332-cc07db10b770@treenet.co.nz>

On 19/09/17 18:25, duanyao wrote:
> Hi,
> 
> I notice that squid's rock storage uses large (and fixed) amount of 
> shared memory even if it is not accessed. It's estimated as 
> 110byte/slot, so for a 256GB rock storage with 16KB slot, the memory 
> requirement is about 1.7GB, which is quite large.
> 
> So my questions are:
> 
> 1. Is there a way to reduce memory usage of rock storage?
> 

Reducing the cache size is the only thing that will do that.

For the entire time your Squid is running it is adding to the cache 
contents. The rate of growth decreases over time, but will only ever 
stop growing if the cache reaches 100% full.

So going out of your way to make it use less memory during that warm-up 
phaze is pointless long-term. The memory *is* needed and not having it 
available for use with zero advance notice will lead to serious 
performance problems, up to and including DoS vulnerability in your proxy.

For general memory reduction see the FAQ:
<https://wiki.squid-cache.org/SquidFaq/SquidMemory#What_can_I_do_to_reduce_Squid.27s_memory_usage.3F>


> 2. On Linux squid puts its shared memory in /dev/shm, which can be 
> backed by swap partition/file. Is it a good idea to use swap 
> partition/file with rock storage to save some physical memory?
> 

Definitely No. The cache index has an extremely high rate of churn and a 
large number of random location reads per transaction. If any of it ever 
gets pushed out to a swap disk/file the proxy operational speed 
undergoes a performance reduction of 3-4 orders of magnitude. eg. 50GBps 
-> 2MBps.


> 3. For rock storage, are /dev/shm/squid* frequently and randomly 
> written? If the Linux swap is on SSD, will this causes 
> performance/lifetime issues?
> 

see the answer to (2).

Squid stresses disks in ways vastly different to what manufacturers 
optimize the hardware to handle. The HTTP caches have a very high 
write-to-read ratio. No disk actually survives more than a fraction of 
its manufacturer advertised lifetime. This problem is less visible with 
HDD due to their naturally long lifetimes.

Specific to your question, due to the churn mentioned in (2) using a 
disk as storage location for the cache index faces it with the worst of 
both worlds - very high read throughput and even higher write 
throughput. SSD avoid (some of) the speed problem, but at cost of 
shorter lifetimes. So the churn is much more relevant and perhapse 
costly in hardware replacements.

YMMV depending on the specific SSD model and how it is designed to cope 
with dead sectors - but it is guaranteed to wear out much faster than 
advertised.

Amos


From zeutech at gmail.com  Tue Sep 19 10:18:34 2017
From: zeutech at gmail.com (Iraj Norouzi)
Date: Tue, 19 Sep 2017 14:48:34 +0430
Subject: [squid-users] very slow squid response
Message-ID: <CANqMsrAxXCnhMrHM6qTQt=5yNTbUjCz_qpGC39WZuYifxnH34Q@mail.gmail.com>

hi everybody
i setup squid on ubuntu and centos with tproxy and wccp for 6 gb/s traffic
but when i try to test squid with 40 mb/s traffic it response very slow
while when i use direct browsing i can browse websites very fast, i used
tcpdump for tracing connections arrive time and there was no problem, also
i used watch -d for tracing packets match by iptables rules and it was ok,
i also used iptables trace command for tracing matching iptables rules,
there was no problem except i had latency on arriving packets on iptables
rule while tcpdump captured packets fast, it happened when my browsing was
so slow, at some times that my browsing was fast there was no latency on
iptables trace log.
i also used tcp and linux enhancement configurations, but nothing happened.
wccp send packets very well and tcpdump show capturing packets too but
browsing with squid is very slow.
please help me.

*Regards,Iraj Norouzi*
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20170919/81620bab/attachment.htm>

From Antony.Stone at squid.open.source.it  Tue Sep 19 10:34:37 2017
From: Antony.Stone at squid.open.source.it (Antony Stone)
Date: Tue, 19 Sep 2017 11:34:37 +0100
Subject: [squid-users] very slow squid response
In-Reply-To: <CANqMsrAxXCnhMrHM6qTQt=5yNTbUjCz_qpGC39WZuYifxnH34Q@mail.gmail.com>
References: <CANqMsrAxXCnhMrHM6qTQt=5yNTbUjCz_qpGC39WZuYifxnH34Q@mail.gmail.com>
Message-ID: <201709191134.37834.Antony.Stone@squid.open.source.it>

On Tuesday 19 September 2017 at 11:18:34, Iraj Norouzi wrote:

> hi everybody
> i setup squid on ubuntu and centos

Why both?

> with tproxy and wccp for 6 gb/s traffic

What hardware are you using for that sort of traffic flow?

> but when i try to test squid with 40 mb/s traffic

How are you generating "40 mb/s traffic"?  I'm assuming that your Internet 
connection is 6Gbps as stated above, so how are you restricting this down to 
40Mbps for testing?

> it response very slow

Numbers please.

> while when i use direct browsing i can browse websites very fast

Is the direct traffic still being routed through the Squid server (you say 
you're using tproxy, so I assume this is an intercept machine with the traffic 
going through it between client and server)?

> i used tcpdump for tracing connections arrive time and there was no problem,

Arrival time where?  From the origin server to Squid?  From Squid to the 
client?  What are you actually measuring?

> i used watch -d for tracing packets match by iptables rules and it was ok,

Please be more specific - what did you measure and what does "OK" mean?

Did you compare with and without Squid in place to see what differs?

> i also used iptables trace command for tracing matching iptables rules,
> there was no problem except i had latency on arriving packets on iptables
> rule while tcpdump captured packets fast, it happened when my browsing was
> so slow, at some times that my browsing was fast there was no latency on
> iptables trace log.

That description is too vague to know exactly what you were measuring and what 
results you got.

> i also used tcp and linux enhancement configurations

Details?

> but nothing happened.
> wccp send packets very well and tcpdump show capturing packets too but
> browsing with squid is very slow.

Firstly, please define "slow" - do you mean it takes a long time for new web 
pages / images / etc to appear (but once they start, they arrive quickly), or 
do you mean that a continuous stream of data (a "download") arrives more 
slowly when going through Squid than going direct (and if so, what are the 
different speeds)?

Secondly, what are you trying to achieve with Squid - what is its purpose in 
your network?

> please help me.

Please help us - give us more details about the hardware you're running this 
on, the version of Squid you're using, what WCCP routing / filtering you're 
doing, the measurements you've made and the results you got.


Regards,


Antony.

-- 
We all get the same amount of time - twenty-four hours per day.
How you use it is up to you.

                                                   Please reply to the list;
                                                         please *don't* CC me.


From Antony.Stone at squid.open.source.it  Tue Sep 19 11:02:45 2017
From: Antony.Stone at squid.open.source.it (Antony Stone)
Date: Tue, 19 Sep 2017 12:02:45 +0100
Subject: [squid-users] very slow squid response
In-Reply-To: <201709191134.37834.Antony.Stone@squid.open.source.it>
References: <CANqMsrAxXCnhMrHM6qTQt=5yNTbUjCz_qpGC39WZuYifxnH34Q@mail.gmail.com>
 <201709191134.37834.Antony.Stone@squid.open.source.it>
Message-ID: <201709191202.45898.Antony.Stone@squid.open.source.it>

On Tuesday 19 September 2017 at 11:34:37, Antony Stone wrote:

> Is the direct traffic still being routed through the Squid server (you say
> you're using tproxy, so I assume this is an intercept machine with the
> traffic going through it between client and server)?

Apologies - with WCCP this is not true.  It would be good to know more about 
your hardware / network / WCCP setup, though.

(As well as the answers to the questions in my previous email.)



Regards,


Antony.

-- 
Because it messes up the order in which people normally read text.
> Why is top-posting such a bad thing?
> > Top-posting.
> > > What is the most annoying way of replying to e-mail?

                                                   Please reply to the list;
                                                         please *don't* CC me.


From Antony.Stone at squid.open.source.it  Tue Sep 19 11:41:35 2017
From: Antony.Stone at squid.open.source.it (Antony Stone)
Date: Tue, 19 Sep 2017 12:41:35 +0100
Subject: [squid-users] Fwd: Re:  very slow squid response
Message-ID: <201709191241.35576.Antony.Stone@squid.open.source.it>

Hi.

Forwarding private reply back to the list in case it helps anyone reply with 
suggestions.

Iraj - please reply to the list in future.

Antony.

----------  Forwarded Message Starts  ----------

Subject: Re: [squid-users] very slow squid response
Date: Tuesday 19 September 2017 12:34:47
From: Iraj Norouzi <zeutech at gmail.com>
To: Antony Stone <Antony Stone <Antony.Stone at squid.open.source.it>>

hi Antony
thanks for you reply
> i setup squid on ubuntu and centos

Why both?
because of test and because i not get the result

> with tproxy and wccp for 6 gb/s traffic

What hardware are you using for that sort of traffic flow?
i use hp DL360 with 2 6 core processor with 3 GHZ and 64 GB RAM and 1 TB HDD

> but when i try to test squid with 40 mb/s traffic

How are you generating "40 mb/s traffic"?  I'm assuming that your Internet
connection is 6Gbps as stated above, so how are you restricting this down to
40Mbps for testing?
i redirect one class of ip address with 40 mb/s traffic for test of squid
and i am going decide to redirect whole of traffic to squid after getting
fast browsing

> it response very slow

Numbers please.
websites load at 2 or 1 second by direct browsing and load at 10 second or
not load by squid

> while when i use direct browsing i can browse websites very fast

Is the direct traffic still being routed through the Squid server (you say
you're using tproxy, so I assume this is an intercept machine with the
traffic
going through it between client and server)?
no, HTTP traffic redirect to squid by wccp and access-list config on Cisco
ip wccp 80 redirect-list wccp
ip wccp 90 redirect-list wccp_to_inside

ip access-list extended wccp
 remark Permit http access from clients
 permit tcp x.x.x.x 0.0.0.255 any eq www
 deny   ip any any
ip access-list extended wccp_to_inside
 permit tcp any eq www x.x.x.x 0.0.0.255
 deny   ip any any

> i used tcpdump for tracing connections arrive time and there was no
problem,

Arrival time where?  From the origin server to Squid?  From Squid to the
client?  What are you actually measuring?
yes, arriving source packets from clients to squid interface,by time that i
push enter on browser address bar and getting packets on tcpdump immediately

> i used watch -d for tracing packets match by iptables rules and it was ok,

Please be more specific - what did you measure and what does "OK" mean?
i add rule to iptables for tracing one website packets and i saw them on
kern.log that matched with the rule, so if you need please tell me to send
commands that i used.

ip rule add fwmark 1 lookup 100ip route add local 0.0.0.0/0 dev
enp3s0f0 table 100

iptables -t mangle -A PREROUTING -d $LOCALIP -j ACCEPTiptables -t
mangle -N DIVERTiptables -t mangle -A DIVERT -j MARK --set-mark
1iptables -t mangle -A DIVERT -j ACCEPTiptables -t mangle -A
PREROUTING -p tcp -m socket -j DIVERTiptables -t mangle -A PREROUTING
-p tcp --dport 80 -j TPROXY --tproxy-mark 0x1/0x1 --on-port 3129

watch -d iptables -t mangle -vnL

Did you compare with and without Squid in place to see what differs?
no, as i told when i browsing directly it is well and packets not coming to
squid and exit from cisco to internet and arrive to clients from cisco and
because of traffic on cisco i can't enable debugging on it but when i
browse from squid i get latency so i suppose the problem is squid or server
that squid running on it

> i also used iptables trace command for tracing matching iptables rules,
> there was no problem except i had latency on arriving packets on iptables
> rule while tcpdump captured packets fast, it happened when my browsing was
> so slow, at some times that my browsing was fast there was no latency on
> iptables trace log.

That description is too vague to know exactly what you were measuring and
what
results you got.

iptables -t raw -A PREROUTING -s x.x.x.x -j TRACE

iptables -t raw -A OUTPUT -s x.x.x.x -j TRACE

tailf /var/log/kern.log

tcpdump -e -i enp3s0f0 -d x.x.x.x dst port 80

tcpdump -e -i enp3s0f0 -s x.x.x.x src port 80



> i also used tcp and linux enhancement configurations

Details?
net.core.wmem_default=524288
net.core.wmem_max=16777216
net.core.rmem_default=524288
net.core.rmem_max=16777216
net.ipv4.tcp_window_scaling = 1
net.ipv4.tcp_rmem = 66560 524288 16777216
net.ipv4.tcp_wmem = 66560 524288 16777216
net.core.somaxconn=4000
net.ipv4.tcp_timestamps=0
net.ipv4.tcp_sack=0
net.ipv4.tcp_fin_timeout=20
net.ipv4.ip_local_port_range=10240 65000
net.ipv4.tcp_keepalive_time = 900
net.ipv4.tcp_keepalive_intvl = 900
net.ipv4.tcp_keepalive_probes = 9
net.core.somaxconn = 5000
net.core.netdev_max_backlog = 8000
net.ipv4.tcp_max_syn_backlog = 8096
net.ipv4.tcp_slow_start_after_idle = 0
net.ipv4.tcp_tw_reuse = 1

> but nothing happened.
> wccp send packets very well and tcpdump show capturing packets too but
> browsing with squid is very slow.

Firstly, please define "slow" - do you mean it takes a long time for new web
pages / images / etc to appear (but once they start, they arrive quickly)
browse in 10 second or not browsing, webpages that i browse for first time
or i browse for multiple times
, or do you mean that a continuous stream of data (a "download") arrives
more
slowly when going through Squid than going direct (and if so, what are the
different speeds)?
no just browsing,

Secondly, what are you trying to achieve with Squid - what is its purpose in
your network?
caching pages and their objects for better internet browsing by clients and
save bandwith.

squid.conf
dns_v4_first on
acl fanava_net src 89.221.82.161/32 # Fanava First clients range to cache
acl Safe_ports port 80 # http
acl Safe_ports port 280 # http-mgmt
acl Safe_ports port 488 # gss-http
acl Safe_ports port 777 # multiling http
acl CONNECT method CONNECT
http_access allow localhost manager
http_access deny manager
http_access allow localhost
http_access allow fanava_net
http_access deny all
http_port 0.0.0.0:3128
http_port 0.0.0.0:3129 tproxy
cache_mem 50 GB
maximum_object_size_in_memory 100 MB
minimum_object_size 2 KB
maximum_object_size 6 GB
#cache_dir ufs /var/spool/squid 1024000 256 512
cache_swap_low 90
cache_swap_high 92
coredump_dir /var/spool/squid
# Image files
refresh_pattern -i \.(png|gif|jpg|jpeg|bmp|tif|tiff)$ 10080 90% 43200
# Compressed files
refresh_pattern -i \.(zip|rar|tar|gz|tgz|z|arj|lha|lzh|iso|deb|rpm)$ 10080
90% 43200
# Binary files
refresh_pattern -i \.(exe|msi)$ 10080 90% 43200
# Multimedia files
refresh_pattern -i \.(mp3|wav|mid|midi|ram|avi|wmv|mpg|mpeg|mp4|swf|flv|)$
10080 90% 43200
# Document files
refresh_pattern -i \.(pdf|ps|doc|ppt|xls|pps)$ 10080 90% 43200
# HTML patterns
refresh_pattern -i \.index.(html|htm)$ 0 40% 10080
refresh_pattern -i \.default.(html|htm)$ 0 40% 10080
refresh_pattern -i \.(html|htm|css|js)$ 1440 40% 40320
# Default patterns
refresh_pattern ^ftp: 1440 20% 10080
refresh_pattern ^gopher: 1440 0% 1440
refresh_pattern -i (/cgi-bin/|\?) 0 0% 0
refresh_pattern . 0 20% 4320
refresh_pattern (Release|Packages(.gz)*)$ 0 20% 2880
cache_mgr zeutech at gmail.com
wccp2_router x.x.x.x
wccp_version 2
wccp2_rebuild_wait on
wccp2_forwarding_method 2
wccp2_return_method 2
wccp2_service dynamic 80
wccp2_service_info 80 protocol=tcp flags=src_ip_hash priority=240 ports=80
wccp2_service dynamic 90
wccp2_service_info 90 protocol=tcp flags=dst_ip_hash,ports_source
priority=240 ports=80


*Regards,Iraj Norouzi*
*+989122494558*

On Tue, Sep 19, 2017 at 3:04 PM, Antony Stone <
Antony.Stone at squid.open.source.it> wrote:

> On Tuesday 19 September 2017 at 11:18:34, Iraj Norouzi wrote:
>
> > hi everybody
> > i setup squid on ubuntu and centos
>
> Why both?
>
> > with tproxy and wccp for 6 gb/s traffic
>
> What hardware are you using for that sort of traffic flow?
>
> > but when i try to test squid with 40 mb/s traffic
>
> How are you generating "40 mb/s traffic"?  I'm assuming that your Internet
> connection is 6Gbps as stated above, so how are you restricting this down
> to
> 40Mbps for testing?
>
> > it response very slow
>
> Numbers please.
>
> > while when i use direct browsing i can browse websites very fast
>
> Is the direct traffic still being routed through the Squid server (you say
> you're using tproxy, so I assume this is an intercept machine with the
> traffic
> going through it between client and server)?
>
> > i used tcpdump for tracing connections arrive time and there was no
> problem,
>
> Arrival time where?  From the origin server to Squid?  From Squid to the
> client?  What are you actually measuring?
>
> > i used watch -d for tracing packets match by iptables rules and it was
> ok,
>
> Please be more specific - what did you measure and what does "OK" mean?
>
> Did you compare with and without Squid in place to see what differs?
>
> > i also used iptables trace command for tracing matching iptables rules,
> > there was no problem except i had latency on arriving packets on iptables
> > rule while tcpdump captured packets fast, it happened when my browsing
> was
> > so slow, at some times that my browsing was fast there was no latency on
> > iptables trace log.
>
> That description is too vague to know exactly what you were measuring and
> what
> results you got.
>
> > i also used tcp and linux enhancement configurations
>
> Details?
>
> > but nothing happened.
> > wccp send packets very well and tcpdump show capturing packets too but
> > browsing with squid is very slow.
>
> Firstly, please define "slow" - do you mean it takes a long time for new
> web
> pages / images / etc to appear (but once they start, they arrive quickly),
> or
> do you mean that a continuous stream of data (a "download") arrives more
> slowly when going through Squid than going direct (and if so, what are the
> different speeds)?
>
> Secondly, what are you trying to achieve with Squid - what is its purpose
> in
> your network?
>
> > please help me.
>
> Please help us - give us more details about the hardware you're running
> this
> on, the version of Squid you're using, what WCCP routing / filtering you're
> doing, the measurements you've made and the results you got.
>
>
> Regards,
>
>
> Antony.
>
> --
> We all get the same amount of time - twenty-four hours per day.
> How you use it is up to you.
>
>                                                    Please reply to the
> list;
>                                                          please *don't* CC
> me.
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>

----------  Forwarded Message Ends  ----------

-- 
Most people are aware that the Universe is big.

 - Paul Davies, Professor of Theoretical Physics

                                                   Please reply to the list;
                                                         please *don't* CC me.


From ahmed.zaeem at netstream.ps  Tue Sep 19 11:47:47 2017
From: ahmed.zaeem at netstream.ps (--Ahmad--)
Date: Tue, 19 Sep 2017 14:47:47 +0300
Subject: [squid-users] squid and hosts_file on ipv6 not working fine
Message-ID: <931410CF-5ADB-4CB9-9AD5-68BDB92CA8EE@netstream.ps>

hello folks 


sometimes i need to change domains for cetian ipv6 website 

i use the directive 

hosts_file /etc/hosts 
and in side it i have :

[root at server ~]# cat /etc/hosts
127.0.0.1   localhost localhost.localdomain localhost4 localhost4.localdomain4
::1         localhost localhost.localdomain localhost6 localhost6.localdomain6
2607:f8b0:4006:810::200e google.com
2607:f8b0:4006:810::200e www.google.com
2607:f8b0:4006:810::2003 google.de
2607:f8b0:4006:810::2003 www.google.de 




but squid still don?t take google.com from that host file and take it from outside 


any directive for IVP6 ? may be ?


cheers 




From zeutech at gmail.com  Tue Sep 19 11:54:54 2017
From: zeutech at gmail.com (Iraj Norouzi)
Date: Tue, 19 Sep 2017 16:24:54 +0430
Subject: [squid-users] very slow squid response
In-Reply-To: <201709191134.37834.Antony.Stone@squid.open.source.it>
References: <CANqMsrAxXCnhMrHM6qTQt=5yNTbUjCz_qpGC39WZuYifxnH34Q@mail.gmail.com>
 <201709191134.37834.Antony.Stone@squid.open.source.it>
Message-ID: <CANqMsrBp00APixTLDMZUg7-1s+6-1WOzM9tw8WUrOXRWYSjW3w@mail.gmail.com>

hi Antony
thanks for you reply
> i setup squid on ubuntu and centos

Why both?
because of test and because i not get the result

> with tproxy and wccp for 6 gb/s traffic

What hardware are you using for that sort of traffic flow?
i use hp DL360 with 2 6 core processor with 3 GHZ and 64 GB RAM and 1 TB HDD

> but when i try to test squid with 40 mb/s traffic

How are you generating "40 mb/s traffic"?  I'm assuming that your Internet
connection is 6Gbps as stated above, so how are you restricting this down to
40Mbps for testing?
i redirect one class of ip address with 40 mb/s traffic for test of squid
and i am going decide to redirect whole of traffic to squid after getting
fast browsing

> it response very slow

Numbers please.
websites load at 2 or 1 second by direct browsing and load at 10 second or
not load by squid

> while when i use direct browsing i can browse websites very fast

Is the direct traffic still being routed through the Squid server (you say
you're using tproxy, so I assume this is an intercept machine with the
traffic
going through it between client and server)?
no, HTTP traffic redirect to squid by wccp and access-list config on Cisco
ip wccp 80 redirect-list wccp
ip wccp 90 redirect-list wccp_to_inside

ip access-list extended wccp
 remark Permit http access from clients
 permit tcp x.x.x.x 0.0.0.255 any eq www
 deny   ip any any
ip access-list extended wccp_to_inside
 permit tcp any eq www x.x.x.x 0.0.0.255
 deny   ip any any

> i used tcpdump for tracing connections arrive time and there was no
problem,

Arrival time where?  From the origin server to Squid?  From Squid to the
client?  What are you actually measuring?
yes, arriving source packets from clients to squid interface,by time that i
push enter on browser address bar and getting packets on tcpdump immediately

> i used watch -d for tracing packets match by iptables rules and it was ok,

Please be more specific - what did you measure and what does "OK" mean?
i add rule to iptables for tracing one website packets and i saw them on
kern.log that matched with the rule, so if you need please tell me to send
commands that i used.

ip rule add fwmark 1 lookup 100ip route add local 0.0.0.0/0 dev
enp3s0f0 table 100

iptables -t mangle -A PREROUTING -d $LOCALIP -j ACCEPTiptables -t
mangle -N DIVERTiptables -t mangle -A DIVERT -j MARK --set-mark
1iptables -t mangle -A DIVERT -j ACCEPTiptables -t mangle -A
PREROUTING -p tcp -m socket -j DIVERTiptables -t mangle -A PREROUTING
-p tcp --dport 80 -j TPROXY --tproxy-mark 0x1/0x1 --on-port 3129

watch -d iptables -t mangle -vnL

Did you compare with and without Squid in place to see what differs?
no, as i told when i browsing directly it is well and packets not coming to
squid and exit from cisco to internet and arrive to clients from cisco and
because of traffic on cisco i can't enable debugging on it but when i
browse from squid i get latency so i suppose the problem is squid or server
that squid running on it

> i also used iptables trace command for tracing matching iptables rules,
> there was no problem except i had latency on arriving packets on iptables
> rule while tcpdump captured packets fast, it happened when my browsing was
> so slow, at some times that my browsing was fast there was no latency on
> iptables trace log.

That description is too vague to know exactly what you were measuring and
what
results you got.

iptables -t raw -A PREROUTING -s x.x.x.x -j TRACE

iptables -t raw -A OUTPUT -s x.x.x.x -j TRACE

tailf /var/log/kern.log

tcpdump -e -i enp3s0f0 -d x.x.x.x dst port 80

tcpdump -e -i enp3s0f0 -s x.x.x.x src port 80



> i also used tcp and linux enhancement configurations

Details?
net.core.wmem_default=524288
net.core.wmem_max=16777216
net.core.rmem_default=524288
net.core.rmem_max=16777216
net.ipv4.tcp_window_scaling = 1
net.ipv4.tcp_rmem = 66560 524288 16777216
net.ipv4.tcp_wmem = 66560 524288 16777216
net.core.somaxconn=4000
net.ipv4.tcp_timestamps=0
net.ipv4.tcp_sack=0
net.ipv4.tcp_fin_timeout=20
net.ipv4.ip_local_port_range=10240 65000
net.ipv4.tcp_keepalive_time = 900
net.ipv4.tcp_keepalive_intvl = 900
net.ipv4.tcp_keepalive_probes = 9
net.core.somaxconn = 5000
net.core.netdev_max_backlog = 8000
net.ipv4.tcp_max_syn_backlog = 8096
net.ipv4.tcp_slow_start_after_idle = 0
net.ipv4.tcp_tw_reuse = 1

> but nothing happened.
> wccp send packets very well and tcpdump show capturing packets too but
> browsing with squid is very slow.

Firstly, please define "slow" - do you mean it takes a long time for new web
pages / images / etc to appear (but once they start, they arrive quickly)
browse in 10 second or not browsing, webpages that i browse for first time
or i browse for multiple times
, or do you mean that a continuous stream of data (a "download") arrives
more
slowly when going through Squid than going direct (and if so, what are the
different speeds)?
no just browsing,

Secondly, what are you trying to achieve with Squid - what is its purpose in
your network?
caching pages and their objects for better internet browsing by clients and
save bandwith.

squid.conf
dns_v4_first on
acl fanava_net src x.x.x.x/32 <http://89.221.82.161/32> # Fanava First
clients range to cache
acl Safe_ports port 80 # http
acl Safe_ports port 280 # http-mgmt
acl Safe_ports port 488 # gss-http
acl Safe_ports port 777 # multiling http
acl CONNECT method CONNECT
http_access allow localhost manager
http_access deny manager
http_access allow localhost
http_access allow fanava_net
http_access deny all
http_port 0.0.0.0:3128
http_port 0.0.0.0:3129 tproxy
cache_mem 50 GB
maximum_object_size_in_memory 100 MB
minimum_object_size 2 KB
maximum_object_size 6 GB
#cache_dir ufs /var/spool/squid 1024000 256 512
cache_swap_low 90
cache_swap_high 92
coredump_dir /var/spool/squid
# Image files
refresh_pattern -i \.(png|gif|jpg|jpeg|bmp|tif|tiff)$ 10080 90% 43200
# Compressed files
refresh_pattern -i \.(zip|rar|tar|gz|tgz|z|arj|lha|lzh|iso|deb|rpm)$ 10080
90% 43200
# Binary files
refresh_pattern -i \.(exe|msi)$ 10080 90% 43200
# Multimedia files
refresh_pattern -i \.(mp3|wav|mid|midi|ram|avi|wmv|mpg|mpeg|mp4|swf|flv|)$
10080 90% 43200
# Document files
refresh_pattern -i \.(pdf|ps|doc|ppt|xls|pps)$ 10080 90% 43200
# HTML patterns
refresh_pattern -i \.index.(html|htm)$ 0 40% 10080
refresh_pattern -i \.default.(html|htm)$ 0 40% 10080
refresh_pattern -i \.(html|htm|css|js)$ 1440 40% 40320
# Default patterns
refresh_pattern ^ftp: 1440 20% 10080
refresh_pattern ^gopher: 1440 0% 1440
refresh_pattern -i (/cgi-bin/|\?) 0 0% 0
refresh_pattern . 0 20% 4320
refresh_pattern (Release|Packages(.gz)*)$ 0 20% 2880
cache_mgr zeutech at gmail.com
wccp2_router x.x.x.x
wccp_version 2
wccp2_rebuild_wait on
wccp2_forwarding_method 2
wccp2_return_method 2
wccp2_service dynamic 80
wccp2_service_info 80 protocol=tcp flags=src_ip_hash priority=240 ports=80
wccp2_service dynamic 90
wccp2_service_info 90 protocol=tcp flags=dst_ip_hash,ports_source
priority=240 ports=80


*Regards,Iraj Norouzi*


On Tue, Sep 19, 2017 at 3:04 PM, Antony Stone <
Antony.Stone at squid.open.source.it> wrote:

> On Tuesday 19 September 2017 at 11:18:34, Iraj Norouzi wrote:
>
> > hi everybody
> > i setup squid on ubuntu and centos
>
> Why both?
>
> > with tproxy and wccp for 6 gb/s traffic
>
> What hardware are you using for that sort of traffic flow?
>
> > but when i try to test squid with 40 mb/s traffic
>
> How are you generating "40 mb/s traffic"?  I'm assuming that your Internet
> connection is 6Gbps as stated above, so how are you restricting this down
> to
> 40Mbps for testing?
>
> > it response very slow
>
> Numbers please.
>
> > while when i use direct browsing i can browse websites very fast
>
> Is the direct traffic still being routed through the Squid server (you say
> you're using tproxy, so I assume this is an intercept machine with the
> traffic
> going through it between client and server)?
>
> > i used tcpdump for tracing connections arrive time and there was no
> problem,
>
> Arrival time where?  From the origin server to Squid?  From Squid to the
> client?  What are you actually measuring?
>
> > i used watch -d for tracing packets match by iptables rules and it was
> ok,
>
> Please be more specific - what did you measure and what does "OK" mean?
>
> Did you compare with and without Squid in place to see what differs?
>
> > i also used iptables trace command for tracing matching iptables rules,
> > there was no problem except i had latency on arriving packets on iptables
> > rule while tcpdump captured packets fast, it happened when my browsing
> was
> > so slow, at some times that my browsing was fast there was no latency on
> > iptables trace log.
>
> That description is too vague to know exactly what you were measuring and
> what
> results you got.
>
> > i also used tcp and linux enhancement configurations
>
> Details?
>
> > but nothing happened.
> > wccp send packets very well and tcpdump show capturing packets too but
> > browsing with squid is very slow.
>
> Firstly, please define "slow" - do you mean it takes a long time for new
> web
> pages / images / etc to appear (but once they start, they arrive quickly),
> or
> do you mean that a continuous stream of data (a "download") arrives more
> slowly when going through Squid than going direct (and if so, what are the
> different speeds)?
>
> Secondly, what are you trying to achieve with Squid - what is its purpose
> in
> your network?
>
> > please help me.
>
> Please help us - give us more details about the hardware you're running
> this
> on, the version of Squid you're using, what WCCP routing / filtering you're
> doing, the measurements you've made and the results you got.
>
>
> Regards,
>
>
> Antony.
>
> --
> We all get the same amount of time - twenty-four hours per day.
> How you use it is up to you.
>
>                                                    Please reply to the
> list;
>                                                          please *don't* CC
> me.
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20170919/33a30762/attachment.htm>

From p.schaefer at creapptive.de  Tue Sep 19 12:20:17 2017
From: p.schaefer at creapptive.de (=?UTF-8?Q?Pascal_Sch=c3=a4fer?=)
Date: Tue, 19 Sep 2017 14:20:17 +0200
Subject: [squid-users] Squid radius Authentication
In-Reply-To: <12c401d32f69$0f9aa160$2ecfe420$@ngtech.co.il>
References: <add1e454-7e0a-250b-6ae3-f99cb6923ce5@creapptive.de>
 <12c401d32f69$0f9aa160$2ecfe420$@ngtech.co.il>
Message-ID: <657fbb33-79cc-fddf-42d8-41ac66b51ded@creapptive.de>

Hey,

thank you for your reply.
Yes it would be Basic.
I think I will write my own helper as a generic solution, not only for 2
domains/subdomains. Do you had the same problem in the past?

The answer mails from Amos helped me a lot to know how I can program the
wrapper helper.

Pascal

Am 17.09.2017 um 05:57 schrieb Eliezer Croitoru:
> Hey,
> 
> What kind of authentication do you want\need? Basic?
> Depends on your needs there might be a helper that you can use.
> If you have only two domains\subdomains it's one thing but if you have more then these then the program would be different.
> 
> If I will have more details I might be able to answer your question and I maybe even have a radius authentication helper written somewhere which I can pull.
> 
> Eliezer
> 
> ----
> Eliezer Croitoru
> Linux System Administrator
> Mobile: +972-5-28704261
> Email: eliezer at ngtech.co.il
> 
> 
> 
> -----Original Message-----
> From: squid-users [mailto:squid-users-bounces at lists.squid-cache.org] On Behalf Of Pascal Sch?fer
> Sent: Friday, September 15, 2017 03:53
> To: squid-users at lists.squid-cache.org
> Subject: [squid-users] Squid radius Authentication
> 
> Dear Ladies and Gentlemen,
> 
> I have a question about the authentication with a radius server.
> I use Squid as a reverse proxy.
> It is possible to use two radius server for different pages or
> subdomains with squid_radius_auth?
> I think about a maybe special configuration.
> I try to use radius server A for the  website A and to use the radius
> server B for the website B. Maybe it is good to know that the website A
> is on web server A and Website B is on web server B.
> I would like to use one Squid server instead of two Squid server (and
> two port fowardings).
> 
> A Example of my configuration:
> 
> https://A.domain.com/... -> authentication over Radius Server A
> https://B.domain.com/... -> authentication over Radius Server B
> 
> When I search on Google I don't found an acceptable answer for my question.
> Should I program such function on my own or know someone a configuration
> that work for my project?
> 
> With best regards
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
> 


From ziegleka at gmail.com  Tue Sep 19 14:00:25 2017
From: ziegleka at gmail.com (kAja Ziegler)
Date: Tue, 19 Sep 2017 16:00:25 +0200
Subject: [squid-users] When the redirect [301, 302, 307] is cached by Squid?
Message-ID: <CAMuNeAsTYYSs7c-J9_yqZwJRZOsgcc9-ZJh0OVS1djVYBym=YA@mail.gmail.com>

Hi all,

  I want to ask why my Squid does not cache redirects 301, 302 and 307. See
anomised example below. Even if I call the URL more times or I open it in
the browser, I always get MISS independently of the return code 301, 302 or
307.

$ curl -v test.example.com/img307.jpg

> GET /img307.jpg HTTP/1.1
> Host: test.example.com
> User-Agent: curl/7.50.1
> Accept: */*
>
< HTTP/1.1 307 Temporary Redirect
< Date: Tue, 19 Sep 2017 12:27:50 GMT
< Server: Apache
< Location: http://test.example.com/img.svg
< Content-Length: 249
< Content-Type: text/html; charset=iso-8859-1
< *X-Cache: MISS* from <squid-proxy>
< *X-Cache-Lookup: MISS* from <squid-proxy>:3128
< Connection: keep-alive
<
<!DOCTYPE HTML PUBLIC "-//IETF//DTD HTML 2.0//EN">
<html><head>
<title>307 Temporary Redirect</title>
</head><body>
<h1>Temporary Redirect</h1>
<p>The document has moved <a href="http://test.example.com
/img.svg">here</a>.</p>
</body></html>


My anomised squid.conf is attached.

Thanks in advance for clarification

  zigi
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20170919/8c878dcb/attachment.htm>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: squid.conf
Type: application/octet-stream
Size: 4105 bytes
Desc: not available
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20170919/8c878dcb/attachment.obj>

From ncherukuri at partycity.com  Tue Sep 19 14:02:03 2017
From: ncherukuri at partycity.com (Cherukuri, Naresh)
Date: Tue, 19 Sep 2017 14:02:03 +0000
Subject: [squid-users] SSL_DB/certs are recahing 4MB on squid
Message-ID: <89638057A560FB458C01C197F81C7F5D190EFB1A@PACERS.amscan.corp>

Hello,

My squid ssl_db/certs memory is reaching to 4MB. What happens when it reaches 4MB? Is squid recycle by itself or  do I have to clear files on certs by removing all *.pem files. Please advise?
sslcrtd_program /usr/lib64/squid/ssl_crtd -s /var/lib/ssl_db -M 4MB sslcrtd_children 8 startup=1 idle=1

[root@****** ssl_db]# ls -ltr
total 200
-rw-r--r--. 1 squid squid      7 Sep 19 09:30 size
drwxr-xr-x. 2 squid squid  69632 Sep 19 09:30 certs
-rw-r--r--. 1 squid squid 123463 Sep 19 09:30 index.txt

[root@**** ssl_db]# du -sh certs
3.6M    certs

Thanks,
Naresh
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20170919/09cd0d21/attachment.htm>

From rousskov at measurement-factory.com  Tue Sep 19 14:10:08 2017
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Tue, 19 Sep 2017 08:10:08 -0600
Subject: [squid-users] SSL_DB/certs are recahing 4MB on squid
In-Reply-To: <89638057A560FB458C01C197F81C7F5D190EFB1A@PACERS.amscan.corp>
References: <89638057A560FB458C01C197F81C7F5D190EFB1A@PACERS.amscan.corp>
Message-ID: <4eb155f6-c7e3-a54c-86eb-24fd379234a5@measurement-factory.com>

On 09/19/2017 08:02 AM, Cherukuri, Naresh wrote:

> My squid ssl_db/certs memory is reaching to 4MB. What happens when it
> reaches 4MB? Is squid recycle by itself

Yes, Squid should evict old certificates in order to cache the new ones
while maintaining the total cache size at or below the configured 4MB level.


> [root@****** ssl_db]# ls -ltr
> [root@**** ssl_db]# du -sh certs

The above numbers do not matter to Squid. They should correlate with
Squid estimate of the database size, but to know how many bytes Squid
actually thinks the certificate database is using, do this instead:

  $ cat size


HTH,

Alex.


From ncherukuri at partycity.com  Tue Sep 19 14:21:45 2017
From: ncherukuri at partycity.com (Cherukuri, Naresh)
Date: Tue, 19 Sep 2017 14:21:45 +0000
Subject: [squid-users] SSL_DB/certs are recahing 4MB on squid
In-Reply-To: <4eb155f6-c7e3-a54c-86eb-24fd379234a5@measurement-factory.com>
References: <89638057A560FB458C01C197F81C7F5D190EFB1A@PACERS.amscan.corp>
 <4eb155f6-c7e3-a54c-86eb-24fd379234a5@measurement-factory.com>
Message-ID: <89638057A560FB458C01C197F81C7F5D190EFBF3@PACERS.amscan.corp>

Hello Alex,

Thank you for quick turnover.  Here is the screenshot of size output.

[root@******* ssl_db]# cat size
3620864

Thanks,
Naresh

-----Original Message-----
From: Alex Rousskov [mailto:rousskov at measurement-factory.com] 
Sent: Tuesday, September 19, 2017 10:10 AM
To: Cherukuri, Naresh; squid-users at lists.squid-cache.org
Subject: Re: [squid-users] SSL_DB/certs are recahing 4MB on squid

On 09/19/2017 08:02 AM, Cherukuri, Naresh wrote:

> My squid ssl_db/certs memory is reaching to 4MB. What happens when it 
> reaches 4MB? Is squid recycle by itself

Yes, Squid should evict old certificates in order to cache the new ones while maintaining the total cache size at or below the configured 4MB level.


> [root@****** ssl_db]# ls -ltr
> [root@**** ssl_db]# du -sh certs

The above numbers do not matter to Squid. They should correlate with Squid estimate of the database size, but to know how many bytes Squid actually thinks the certificate database is using, do this instead:

  $ cat size


HTH,

Alex.


From eliezer at ngtech.co.il  Tue Sep 19 14:50:45 2017
From: eliezer at ngtech.co.il (Eliezer Croitoru)
Date: Tue, 19 Sep 2017 17:50:45 +0300
Subject: [squid-users] Squid radius Authentication
In-Reply-To: <657fbb33-79cc-fddf-42d8-41ac66b51ded@creapptive.de>
References: <add1e454-7e0a-250b-6ae3-f99cb6923ce5@creapptive.de>
 <12c401d32f69$0f9aa160$2ecfe420$@ngtech.co.il>
 <657fbb33-79cc-fddf-42d8-41ac66b51ded@creapptive.de>
Message-ID: <1b7501d33156$ac731cd0$05595670$@ngtech.co.il>

Hey Pascal,

I have some experience with wrapper scripts but I must admit that it has couple things which led me to not use it.
One of the issues was excessive CPU usage since I was using a bash script as a wrapper.
I remember that long ago a sysadmin used something else then basic auth.
They had a WIFI system on the premise and every user could login to the WIFI network using it's username and password.
Then they pulled from the radius DB periodically the user=> ip mapping and applied acl's based on the client IP which is unique per username.

If I will write a helper I would probably use GoLang or ruby.
I was thinking about some way to make an helper generic enough but if you have an idea\sketch I might take it and will actually write the helper.
I have seen but have not used the next library:
https://github.com/layeh/radius

Which might be very helpful.

Eliezer 
----
Eliezer Croitoru
Linux System Administrator
Mobile: +972-5-28704261
Email: eliezer at ngtech.co.il



-----Original Message-----
From: Pascal Sch?fer [mailto:p.schaefer at creapptive.de] 
Sent: Tuesday, September 19, 2017 15:20
To: Eliezer Croitoru <eliezer at ngtech.co.il>; squid-users at lists.squid-cache.org
Subject: Re: [squid-users] Squid radius Authentication

Hey,

thank you for your reply.
Yes it would be Basic.
I think I will write my own helper as a generic solution, not only for 2
domains/subdomains. Do you had the same problem in the past?

The answer mails from Amos helped me a lot to know how I can program the
wrapper helper.

Pascal

Am 17.09.2017 um 05:57 schrieb Eliezer Croitoru:
> Hey,
> 
> What kind of authentication do you want\need? Basic?
> Depends on your needs there might be a helper that you can use.
> If you have only two domains\subdomains it's one thing but if you have more then these then the program would be different.
> 
> If I will have more details I might be able to answer your question and I maybe even have a radius authentication helper written somewhere which I can pull.
> 
> Eliezer
> 
> ----
> Eliezer Croitoru
> Linux System Administrator
> Mobile: +972-5-28704261
> Email: eliezer at ngtech.co.il
> 
> 
> 
> -----Original Message-----
> From: squid-users [mailto:squid-users-bounces at lists.squid-cache.org] On Behalf Of Pascal Sch?fer
> Sent: Friday, September 15, 2017 03:53
> To: squid-users at lists.squid-cache.org
> Subject: [squid-users] Squid radius Authentication
> 
> Dear Ladies and Gentlemen,
> 
> I have a question about the authentication with a radius server.
> I use Squid as a reverse proxy.
> It is possible to use two radius server for different pages or
> subdomains with squid_radius_auth?
> I think about a maybe special configuration.
> I try to use radius server A for the  website A and to use the radius
> server B for the website B. Maybe it is good to know that the website A
> is on web server A and Website B is on web server B.
> I would like to use one Squid server instead of two Squid server (and
> two port fowardings).
> 
> A Example of my configuration:
> 
> https://A.domain.com/... -> authentication over Radius Server A
> https://B.domain.com/... -> authentication over Radius Server B
> 
> When I search on Google I don't found an acceptable answer for my question.
> Should I program such function on my own or know someone a configuration
> that work for my project?
> 
> With best regards
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
> 



From eliezer at ngtech.co.il  Tue Sep 19 14:54:14 2017
From: eliezer at ngtech.co.il (Eliezer Croitoru)
Date: Tue, 19 Sep 2017 17:54:14 +0300
Subject: [squid-users] When the redirect [301, 302,
	307] is cached by Squid?
In-Reply-To: <CAMuNeAsTYYSs7c-J9_yqZwJRZOsgcc9-ZJh0OVS1djVYBym=YA@mail.gmail.com>
References: <CAMuNeAsTYYSs7c-J9_yqZwJRZOsgcc9-ZJh0OVS1djVYBym=YA@mail.gmail.com>
Message-ID: <1b7701d33157$28efb430$7acf1c90$@ngtech.co.il>

As you can see in the response headers there are no rules for caching:
< HTTP/1.1 307 Temporary Redirect
< Date: Tue, 19 Sep 2017 12:27:50 GMT
< Server: Apache
< Location: http://http://test.example.com/img.svg
< Content-Length: 249
< Content-Type: text/html; charset=iso-8859-1
< X-Cache: MISS from <squid-proxy>
< X-Cache-Lookup: MISS from <squid-proxy>:3128
< Connection: keep-alive

If you have a specific service try to use redbot to analyze the response:
https://redbot.org/

It might give you what you need.

All The Bests,
Eliezer

* I do not know if it should be this way or not since I am missing couple things from the setup such as squid.conf..

----
http://ngtech.co.il/lmgtfy/
Linux System Administrator
Mobile: +972-5-28704261
Email: eliezer at ngtech.co.il


From: squid-users [mailto:squid-users-bounces at lists.squid-cache.org] On Behalf Of kAja Ziegler
Sent: Tuesday, September 19, 2017 17:00
To: squid-users at lists.squid-cache.org
Subject: [squid-users] When the redirect [301, 302, 307] is cached by Squid?

Hi all,

  I want to ask why my Squid does not cache redirects 301, 302 and 307. See anomised example below. Even if I call the URL more times or I open it in the browser, I always get MISS independently of the return code 301, 302 or 307.

$ curl -v http://test.example.com/img307.jpg

> GET /img307.jpg HTTP/1.1
> Host: http://test.example.com
> User-Agent: curl/7.50.1
> Accept: */*
>
< HTTP/1.1 307 Temporary Redirect
< Date: Tue, 19 Sep 2017 12:27:50 GMT
< Server: Apache
< Location: http://http://test.example.com/img.svg
< Content-Length: 249
< Content-Type: text/html; charset=iso-8859-1
< X-Cache: MISS from <squid-proxy>
< X-Cache-Lookup: MISS from <squid-proxy>:3128
< Connection: keep-alive
<
<!DOCTYPE HTML PUBLIC "-//IETF//DTD HTML 2.0//EN">
<html><head>
<title>307 Temporary Redirect</title>
</head><body>
<h1>Temporary Redirect</h1>
<p>The document has moved <a href="http://http://test.example.com/img.svg">here</a>.</p>
</body></html>

My anomised squid.conf is attached.
Thanks in advance for clarification

  zigi






From squid3 at treenet.co.nz  Tue Sep 19 15:02:23 2017
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Wed, 20 Sep 2017 03:02:23 +1200
Subject: [squid-users] very slow squid response
In-Reply-To: <CANqMsrBp00APixTLDMZUg7-1s+6-1WOzM9tw8WUrOXRWYSjW3w@mail.gmail.com>
References: <CANqMsrAxXCnhMrHM6qTQt=5yNTbUjCz_qpGC39WZuYifxnH34Q@mail.gmail.com>
 <201709191134.37834.Antony.Stone@squid.open.source.it>
 <CANqMsrBp00APixTLDMZUg7-1s+6-1WOzM9tw8WUrOXRWYSjW3w@mail.gmail.com>
Message-ID: <8e688a15-cd29-4fdc-c1c0-335d5887f24e@treenet.co.nz>

On 19/09/17 23:54, Iraj Norouzi wrote:
> hi Antony
> thanks for you reply
>> i setup squid on ubuntu and centos
> 
> Why both?
> because of test and because i not get the result
> 
>> with tproxy and wccp for 6 gb/s traffic
> 
> What hardware are you using for that sort of traffic flow?
> i use hp DL360 with 2 6 core processor with 3 GHZ and 64 GB RAM and 1 TB HDD
> 
>> but when i try to test squid with 40 mb/s traffic
> 
> How are you generating "40 mb/s traffic"?? I'm assuming that your Internet
> connection is 6Gbps as stated above, so how are you restricting this down to
> 40Mbps for testing?
> i redirect one class of ip address with 40 mb/s traffic for test of 
> squid and i am going decide to redirect whole of traffic to squid after 
> getting fast browsing
> 

Squid is designed to optimize and reduce *bandwidth*. "fast browsing" is 
just a nice side effect of caching. It is a mistake to think that Squid 
will always produce faster browsing.

This is especially true during the initial cache warm-up period where 
DNS and HTTP objects are being fetched for the first time. Speed can 
only come from future fetches being reduced by the cache.

So when you are testing for speed with real traffic make sure there has 
been at least a few hrs for the caches to warm up.

Since you are testing with RAM-only caching right now be aware that 
every time you restart Squid *all* its caches (for all data types) get 
erased back to "cold"/empty.


>> it response very slow
> 
> Numbers please.
> websites load at 2 or 1 second by direct browsing and load at 10 second 
> or not load by squid
> 
>> while when i use direct browsing i can browse websites very fast
> 
> Is the direct traffic still being routed through the Squid server (you say
> you're using tproxy, so I assume this is an intercept machine with the 
> traffic
> going through it between client and server)?
> no, HTTP traffic redirect to squid by wccp and access-list config on Cisco

Cisco is tunneling the packets with WCCP to the Squid machine, which is 
intercepting the traffic with TPROXY.

So actually "yes", if not something would be terribly broken in your 
WCCP and TPROXY setup.


> ip wccp 80 redirect-list wccp
> ip wccp 90 redirect-list wccp_to_inside
> 
> ip access-list extended wccp
>  ?remark Permit http access from clients
>  ?permit tcp x.x.x.x 0.0.0.255 any eq www
>  ?deny ? ip any any
> ip access-list extended wccp_to_inside
>  ?permit tcp any eq www x.x.x.x 0.0.0.255
>  ?deny ? ip any any
> 
>> i used tcpdump for tracing connections arrive time and there was no problem,
> 
> Arrival time where?? From the origin server to Squid?? From Squid to the
> client?? What are you actually measuring?
> yes, arriving source packets from clients to squid interface,by time 
> that i push enter on browser address bar and getting packets on tcpdump 
> immediately

Packets arriving at Squid from the client is only the first ~1% of 
things that are going on. It would be a big problem if they took 
anything more than a few ms to arrive.

Once the packets arrive there are DNS lookups to do. Delay in DNS 
lookups is the most common cause of overall delays.

Then there is HTTP processing to find a source for the response. If the 
request has never been seen before that means a fair amount of logic to 
select potential upstream servers and attempt connections to them (maybe 
several or even all of them).

Then the server request has to be generated, and wait for a response.
Only when that server response comes back can stuff for the client 
response start to happen.

When the cache is involved the total time could be under 1ms, or 
somewhat around 50ms. If there are lots of server things to do the time 
can be hundreds of ms.


> 
>> i used watch -d for tracing packets match by iptables rules and it was ok,
> 
> Please be more specific - what did you measure and what does "OK" mean?
> i add rule to iptables for tracing one website packets and i saw them on 
> kern.log that matched with the rule, so if you need please tell me to 
> send commands that i used.

Which packets. As I detailed above, there are a minimum of 2 TCP 
connections involved with delivering a MISS object (client->Squid, and 
Squid->server) potentially many more if there are network issues 
connecting to server(s).


> 
> ip rule add fwmark 1 lookup 100 ip route add local 0.0.0.0/0 
> <http://0.0.0.0/0> dev enp3s0f0 table 100
> 
> iptables -t mangle -A PREROUTING -d $LOCALIP -j ACCEPT

You also need a rule before the above one which blocks traffic to
   "-d $LOCALIP -p 3129 -j REJECT"

> iptables -t 
> mangle -N DIVERT iptables -t mangle -A DIVERT -j MARK --set-mark 1 
> iptables -t mangle -A DIVERT -j ACCEPT iptables -t mangle -A PREROUTING 
> -p tcp -m socket -j DIVERT iptables -t mangle -A PREROUTING -p tcp 
> --dport 80 -j TPROXY --tproxy-mark 0x1/0x1 --on-port 3129
> 
> watch -d iptables -t mangle -vnL
> 
> Did you compare with and without Squid in place to see what differs?
> no, as i told when i browsing directly it is well and packets not coming 
> to squid and exit from cisco to internet and arrive to clients from 
> cisco and because of traffic on cisco i can't enable debugging on it but 
> when i browse from squid i get latency so i suppose the problem is squid 
> or server that squid running on it

Your test is incomplete.

To eliminate WCCP and routing issues being the cause of problems you 
need to compare:

A) normal browsing without WCCP  or TPROXY.

B) normal browsing with WCCP tunneling traffic to the Squid machine (no 
Squid running and no TPROXY).

C) normal browsing with WCCP tunneling traffic to the Squid machine 
where TPROXY diverts the traffic into Squid.

If things work any slower at test (B) than test (A) your problem is the 
WCCP + Squid machine setup, not Squid itself.


> 
>> i also used iptables trace command for tracing matching iptables rules,
>> there was no problem except i had latency on arriving packets on iptables
>> rule while tcpdump captured packets fast, it happened when my browsing was
>> so slow, at some times that my browsing was fast there was no latency on
>> iptables trace log.
> 
> That description is too vague to know exactly what you were measuring 
> and what
> results you got.
> 
> iptables -t raw -A PREROUTING -s x.x.x.x -j TRACE
> 
> iptables -t raw -A OUTPUT -s x.x.x.x -j TRACE
> 
> tailf /var/log/kern.log
> 
> tcpdump -e -i enp3s0f0 -d x.x.x.x dst port 80
> 
> tcpdump -e -i enp3s0f0 -s x.x.x.x src port 80
> 

"and what results you got" ?


>> i also used tcp and linux enhancement configurations
> 
> Details?
> net.core.wmem_default=524288
> net.core.wmem_max=16777216
> net.core.rmem_default=524288
> net.core.rmem_max=16777216
> net.ipv4.tcp_window_scaling = 1
> net.ipv4.tcp_rmem = 66560 524288 16777216
> net.ipv4.tcp_wmem = 66560 524288 16777216
> net.core.somaxconn=4000
> net.ipv4.tcp_timestamps=0
> net.ipv4.tcp_sack=0
> net.ipv4.tcp_fin_timeout=20
> net.ipv4.ip_local_port_range=10240 65000
> net.ipv4.tcp_keepalive_time = 900
> net.ipv4.tcp_keepalive_intvl = 900
> net.ipv4.tcp_keepalive_probes = 9
> net.core.somaxconn = 5000
> net.core.netdev_max_backlog = 8000
> net.ipv4.tcp_max_syn_backlog = 8096
> net.ipv4.tcp_slow_start_after_idle = 0
> net.ipv4.tcp_tw_reuse = 1
> 


Okay, so no improvement with those means they were not useful, please 
ensure they are back at defaults so as not to add problems to other 
experimental changes.


>> but nothing happened.
>> wccp send packets very well and tcpdump show capturing packets too but
>> browsing with squid is very slow.
> 
> Firstly, please define "slow" - do you mean it takes a long time for new web
> pages / images / etc to appear (but once they start, they arrive quickly)

 >> Iraj:
> browse in 10 second or not browsing, webpages that i browse for first 
> time or i browse for multiple times

For that sort of delay:

* check the DNS response times for lookups made by the Squid machine. 
The details of Squid's lookups can be found in the "idns" cache manager 
report.

*


 >> Antony:
> , or do you mean that a continuous stream of data (a "download") arrives 
> more
> slowly when going through Squid than going direct (and if so, what are the
> different speeds)?

 >> Iraj:
> no just browsing,
> 

 >> Antony:
> Secondly, what are you trying to achieve with Squid - what is its purpose in
> your network?

 >> Iraj:
> caching pages and their objects for better internet browsing by clients 
> and save bandwith.
> 
> squid.conf
> dns_v4_first on
> acl fanava_net src x.x.x.x/32
> acl Safe_ports port 80# http
> acl Safe_ports port 280# http-mgmt
> acl Safe_ports port 488# gss-http
> acl Safe_ports port 777# multiling http
> acl CONNECT method CONNECT
> http_access allow localhost manager
> http_access deny manager
> http_access allow localhost
> http_access allow fanava_net
> http_access deny all
> http_port 0.0.0.0:3128
> http_port 0.0.0.0:3129 tproxy
> cache_mem 50 GB

Does this machine actually have at least 52 GB of free RAM for Squid to use?

> maximum_object_size_in_memory 100 MB
> minimum_object_size 2 KB
> maximum_object_size 6 GB
> #cache_dir ufs /var/spool/squid 1024000 256 512
> cache_swap_low 90
> cache_swap_high 92
> coredump_dir /var/spool/squid
> # Image files
> refresh_pattern -i \.(png|gif|jpg|jpeg|bmp|tif|tiff)$ 10080 90% 43200
> # Compressed files
> refresh_pattern -i \.(zip|rar|tar|gz|tgz|z|arj|lha|lzh|iso|deb|rpm)$ 
> 10080 90% 43200
> # Binary files
> refresh_pattern -i \.(exe|msi)$ 10080 90% 43200
> # Multimedia files
> refresh_pattern -i 
> \.(mp3|wav|mid|midi|ram|avi|wmv|mpg|mpeg|mp4|swf|flv|)$ 10080 90% 43200
> # Document files
> refresh_pattern -i \.(pdf|ps|doc|ppt|xls|pps)$ 10080 90% 43200
> # HTML patterns
> refresh_pattern -i \.index.(html|htm)$ 0 40% 10080
> refresh_pattern -i \.default.(html|htm)$ 0 40% 10080
> refresh_pattern -i \.(html|htm|css|js)$ 1440 40% 40320
> # Default patterns
> refresh_pattern ^ftp: 1440 20% 10080
> refresh_pattern ^gopher: 1440 0% 1440
> refresh_pattern -i (/cgi-bin/|\?) 0 0% 0
> refresh_pattern . 0 20% 4320  # Fanava First 
> clients range to cache
> refresh_pattern (Release|Packages(.gz)*)$ 0 20% 2880
> cache_mgr ******
> wccp2_router x.x.x.x
> wccp_version 2
> wccp2_rebuild_wait on
> wccp2_forwarding_method 2
> wccp2_return_method 2
> wccp2_service dynamic 80
> wccp2_service_info 80 protocol=tcp flags=src_ip_hash priority=240 ports=80
> wccp2_service dynamic 90
> wccp2_service_info 90 protocol=tcp flags=dst_ip_hash,ports_source 
> priority=240 ports=80
> 
> *Regards,
> Iraj Norouzi*
> 

Amos


From squid3 at treenet.co.nz  Tue Sep 19 15:17:34 2017
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Wed, 20 Sep 2017 03:17:34 +1200
Subject: [squid-users] When the redirect [301, 302,
 307] is cached by Squid?
In-Reply-To: <CAMuNeAsTYYSs7c-J9_yqZwJRZOsgcc9-ZJh0OVS1djVYBym=YA@mail.gmail.com>
References: <CAMuNeAsTYYSs7c-J9_yqZwJRZOsgcc9-ZJh0OVS1djVYBym=YA@mail.gmail.com>
Message-ID: <16a27a18-d2a5-c1a8-628f-2f14685e931b@treenet.co.nz>

On 20/09/17 02:00, kAja Ziegler wrote:
> Hi all,
> 
>  ? I want to ask why my Squid does not cache redirects 301, 302 and 307. 
> See anomised example below. Even if I call the URL more times or I open 
> it in the browser, I always get MISS independently of the return code 
> 301, 302 or 307.

302 and 307 are not because as their status description indicates they 
are *temporary* results. They can only be cached if there are explicit 
details from the server indicating for how long.

301 should be cached unless the object would need revalidation immediately.


Amos


From squid3 at treenet.co.nz  Tue Sep 19 15:30:33 2017
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Wed, 20 Sep 2017 03:30:33 +1200
Subject: [squid-users] squid and hosts_file on ipv6 not working fine
In-Reply-To: <931410CF-5ADB-4CB9-9AD5-68BDB92CA8EE@netstream.ps>
References: <931410CF-5ADB-4CB9-9AD5-68BDB92CA8EE@netstream.ps>
Message-ID: <ed5805dd-b585-ba39-9ae1-431fdee75abc@treenet.co.nz>

On 19/09/17 23:47, --Ahmad-- wrote:
> hello folks
> 
> 
> sometimes i need to change domains for cetian ipv6 website
> 
> i use the directive
> 
> hosts_file /etc/hosts
> and in side it i have :
> 
> [root at server ~]# cat /etc/hosts
> 127.0.0.1   localhost localhost.localdomain localhost4 localhost4.localdomain4
> ::1         localhost localhost.localdomain localhost6 localhost6.localdomain6
> 2607:f8b0:4006:810::200e google.com
> 2607:f8b0:4006:810::200e www.google.com
> 2607:f8b0:4006:810::2003 google.de
> 2607:f8b0:4006:810::2003 www.google.de
> 
> 
> 
> 
> but squid still don?t take google.com from that host file and take it from outside
> 

How are you determining that?

and what does your access.log say for one of the requests that are doing it?


Amos


From squid3 at treenet.co.nz  Wed Sep 20 04:12:00 2017
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Wed, 20 Sep 2017 16:12:00 +1200
Subject: [squid-users] squid and hosts_file on ipv6 not working fine
In-Reply-To: <7D42C866-9AA9-4CA2-B2CE-50FA084DF865@gmail.com>
References: <931410CF-5ADB-4CB9-9AD5-68BDB92CA8EE@netstream.ps>
 <ed5805dd-b585-ba39-9ae1-431fdee75abc@treenet.co.nz>
 <7D42C866-9AA9-4CA2-B2CE-50FA084DF865@gmail.com>
Message-ID: <14c31a43-b183-1b1f-35b4-49af3ea3d79c@treenet.co.nz>

On 20/09/17 03:54, Ahmed Alzaeem wrote:
> access .log point to other address :
> 
> 1505824835.364 690000 12.13.207.211 TCP_TUNNEL/200 78573 CONNECT 
> www.google.com:443 - 
> HIER_DIRECT/2404:6800:4009:802::2004 -
> 
> from linux terminal i can reach goole from :
>>> 2607:f8b0:4006:810::200e google.com
> 
> 
> but in squid itself ??. ?no it dont ?and it reach goole ?using the 
> address2404:6800:4009:802::2004 ?not ?2607:f8b0:4006:810
> 
> so I?m sure the hosts_file directive works for IPV4 not for iPV6 address
> 

FYI: "google.com" is not the same domain as "www.google.com". So your 
hosts file line for "www.google.com" is the one that should be used, not 
the "google.com" line.


Did you restart or reconfigure Squid after making the hosts file changes?

And, if so does "squid -k parse" show an issues?

And, does the cache manager ipcache report show all these google entries 
with a 'H' flag?

Amos


From squid3 at treenet.co.nz  Wed Sep 20 14:55:07 2017
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 21 Sep 2017 02:55:07 +1200
Subject: [squid-users] disable access.log logging on a specific entrys
In-Reply-To: <4861f665-44a4-9bac-657c-f5e5e750548b@treenet.co.nz>
References: <1505742348570-0.post@n4.nabble.com>
 <6b90a1c3-e64e-eac5-21f5-ac5d74346655@treenet.co.nz>
 <4861f665-44a4-9bac-657c-f5e5e750548b@treenet.co.nz>
Message-ID: <ab21043f-023d-39ab-94c4-074a0eda1d52@treenet.co.nz>

On 21/09/17 01:42, Holger Wybranietz wrote:
 > Hello Amos,
 >
 > Yast doesn't show any newer version then 3.5.21 (you meant 3.5.27 is
 > working fine?).
 >

Yes. I tested with 3.5.27, 4.0.21, and latest v5 code. All hide the log 
entries when !logNoSpamresolver is used.


 > By the way:
 > The entrys I want to get filtered are similar to:
 >
 > 192.168.12.84 - - [18/Sep/2017:15:22:40 +0200] "POST
 > /SpamResolverNG/SpamResolverNG.dll?DoNewRequest HTTP/1.1" 400 3990
 > TAG_NONE:HIER_NONE
 >
 > I think, that this is not a "normal" url, "/SpamResolverNG/Spa..." seems
 > to be a directory path?

It's called an origin-form URI and is the true form of URLs delivered to 
web servers on port 80 and 443.

I suspect there is no Host header delivered by the client to allow Squid 
to convert it into an absolute-form URL for proxy consumption. Which 
would also explain the 400 status and *_NONE server details.


 > Is there another way to treat this kind of entries?
 >

That depends on your definition of "treat". They are all actual traffic 
consuming resources on the proxy, so it is a little odd to hide them 
from view. On the other hand you are using a web server log format in a 
proxy, which is very lossy anyway.


The config mentioned earlier was correct for what you tried to do. Its 
odd that it was not working.

Maybe something wrong with the regex. I'm thinking unicode characters 
etc not quite matching what the eyes seem to indicate - in either the 
URL itself or the config regex.


It might be a good idea to try and resolve the problem in the client 
software if you can;

- if the AV software is configured to use the proxy (including with 
auto-config methods, WPAD/PAC etc) then it is a bug to be sending that 
URL form to a proxy. The vendor may want to know and fix it since other 
customers will be having the same issue and this type of bug is 
security vulnerability for AV.

- if you are intercepting the traffic from port 80 or 443 somehow, then 
your interception would appear to be broken. Squid should always be able 
to determine the ORIGINAL_DST for intercepted traffic and transparently 
deliver it there when Host is missing or invalid.

Amos


From ziegleka at gmail.com  Thu Sep 21 08:11:48 2017
From: ziegleka at gmail.com (kAja Ziegler)
Date: Thu, 21 Sep 2017 10:11:48 +0200
Subject: [squid-users] When the redirect [301, 302,
	307] is cached by Squid?
In-Reply-To: <1b7701d33157$28efb430$7acf1c90$@ngtech.co.il>
References: <CAMuNeAsTYYSs7c-J9_yqZwJRZOsgcc9-ZJh0OVS1djVYBym=YA@mail.gmail.com>
 <1b7701d33157$28efb430$7acf1c90$@ngtech.co.il>
Message-ID: <CAMuNeAucO89J3ek5eJ=QUpWjyc+A+GW0kKdtia5_kz1GhEc7GQ@mail.gmail.com>

Thank you to pointing me on such nice tool.

zigi

On Tue, Sep 19, 2017 at 4:54 PM, Eliezer Croitoru <eliezer at ngtech.co.il>
wrote:

> As you can see in the response headers there are no rules for caching:
> < HTTP/1.1 307 Temporary Redirect
> < Date: Tue, 19 Sep 2017 12:27:50 GMT
> < Server: Apache
> < Location: http://http://test.example.com/img.svg
> < Content-Length: 249
> < Content-Type: text/html; charset=iso-8859-1
> < X-Cache: MISS from <squid-proxy>
> < X-Cache-Lookup: MISS from <squid-proxy>:3128
> < Connection: keep-alive
>
> If you have a specific service try to use redbot to analyze the response:
> https://redbot.org/
>
> It might give you what you need.
>
> All The Bests,
> Eliezer
>
> * I do not know if it should be this way or not since I am missing couple
> things from the setup such as squid.conf..
>
> ----
> http://ngtech.co.il/lmgtfy/
> Linux System Administrator
> Mobile: +972-5-28704261
> Email: eliezer at ngtech.co.il
>
>
> From: squid-users [mailto:squid-users-bounces at lists.squid-cache.org] On
> Behalf Of kAja Ziegler
> Sent: Tuesday, September 19, 2017 17:00
> To: squid-users at lists.squid-cache.org
> Subject: [squid-users] When the redirect [301, 302, 307] is cached by
> Squid?
>
> Hi all,
>
>   I want to ask why my Squid does not cache redirects 301, 302 and 307.
> See anomised example below. Even if I call the URL more times or I open it
> in the browser, I always get MISS independently of the return code 301, 302
> or 307.
>
> $ curl -v http://test.example.com/img307.jpg
>
> > GET /img307.jpg HTTP/1.1
> > Host: http://test.example.com
> > User-Agent: curl/7.50.1
> > Accept: */*
> >
> < HTTP/1.1 307 Temporary Redirect
> < Date: Tue, 19 Sep 2017 12:27:50 GMT
> < Server: Apache
> < Location: http://http://test.example.com/img.svg
> < Content-Length: 249
> < Content-Type: text/html; charset=iso-8859-1
> < X-Cache: MISS from <squid-proxy>
> < X-Cache-Lookup: MISS from <squid-proxy>:3128
> < Connection: keep-alive
> <
> <!DOCTYPE HTML PUBLIC "-//IETF//DTD HTML 2.0//EN">
> <html><head>
> <title>307 Temporary Redirect</title>
> </head><body>
> <h1>Temporary Redirect</h1>
> <p>The document has moved <a href="http://http://test.example.com/img.svg
> ">here</a>.</p>
> </body></html>
>
> My anomised squid.conf is attached.
> Thanks in advance for clarification
>
>   zigi
>
>
>
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20170921/27f76785/attachment.htm>

From ziegleka at gmail.com  Thu Sep 21 08:36:08 2017
From: ziegleka at gmail.com (kAja Ziegler)
Date: Thu, 21 Sep 2017 10:36:08 +0200
Subject: [squid-users] When the redirect [301, 302,
	307] is cached by Squid?
In-Reply-To: <16a27a18-d2a5-c1a8-628f-2f14685e931b@treenet.co.nz>
References: <CAMuNeAsTYYSs7c-J9_yqZwJRZOsgcc9-ZJh0OVS1djVYBym=YA@mail.gmail.com>
 <16a27a18-d2a5-c1a8-628f-2f14685e931b@treenet.co.nz>
Message-ID: <CAMuNeAv56nT4HBxjFQTQ_RifSOQrmajSKTh8Qo_gFgKtRvVsmA@mail.gmail.com>

Hi Amos,

302 and 307 are not because as their status description indicates they are
> *temporary* results. They can only be cached if there are explicit details
> from the server indicating for how long.
>

You were right. After I added header Cache-Control "max-age=60,
must-revalidate" to the redirects, then the response was cached. Thank you
for clarification.

301 should be cached unless the object would need revalidation immediately.


But for 301 I always get MISS - with and without a cache-control header:

$ curl -v http:/test.example.com/img301.jpg

> GET /img301.jpg HTTP/1.1
> Host: test.example.com
> User-Agent: curl/7.50.1
> Accept: */*
>
< HTTP/1.1 301 Moved Permanently
< Date: Thu, 21 Sep 2017 06:44:41 GMT
< Server: Apache
< Cache-Control: max-age=60, must-revalidate
< Location: http://test.example.com/img.svg
< Content-Length: 247
< Content-Type: text/html; charset=iso-8859-1
< X-Cache: MISS from <squid-proxy>
< X-Cache-Lookup: MISS from <squid-proxy>:3128

I can't find any information on Squid wiki or via Google if the object need
revalidation immediately.


I use Squid 3.4.14 from CentOS 6 package squid34-3.4.14-15.el6.x86_64.

With best regards,

zigi

On Tue, Sep 19, 2017 at 5:17 PM, Amos Jeffries <squid3 at treenet.co.nz> wrote:

> On 20/09/17 02:00, kAja Ziegler wrote:
>
>> Hi all,
>>
>>    I want to ask why my Squid does not cache redirects 301, 302 and 307.
>> See anomised example below. Even if I call the URL more times or I open it
>> in the browser, I always get MISS independently of the return code 301, 302
>> or 307.
>>
>
> 302 and 307 are not because as their status description indicates they are
> *temporary* results. They can only be cached if there are explicit details
> from the server indicating for how long.
>
> 301 should be cached unless the object would need revalidation immediately.
>
>
> Amos
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20170921/d19ba236/attachment.htm>

From squid3 at treenet.co.nz  Thu Sep 21 09:11:31 2017
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 21 Sep 2017 21:11:31 +1200
Subject: [squid-users] When the redirect [301, 302,
 307] is cached by Squid?
In-Reply-To: <CAMuNeAv56nT4HBxjFQTQ_RifSOQrmajSKTh8Qo_gFgKtRvVsmA@mail.gmail.com>
References: <CAMuNeAsTYYSs7c-J9_yqZwJRZOsgcc9-ZJh0OVS1djVYBym=YA@mail.gmail.com>
 <16a27a18-d2a5-c1a8-628f-2f14685e931b@treenet.co.nz>
 <CAMuNeAv56nT4HBxjFQTQ_RifSOQrmajSKTh8Qo_gFgKtRvVsmA@mail.gmail.com>
Message-ID: <85226f4c-1f2e-c0a3-eec3-97ca5e15603b@treenet.co.nz>

On 21/09/17 20:36, kAja Ziegler wrote:
> Hi Amos,
> 
>     302 and 307 are not because as their status description indicates
>     they are *temporary* results. They can only be cached if there are
>     explicit details from the server indicating for how long.
> 
> 
> You were right. After I added header Cache-Control "max-age=60, 
> must-revalidate" to the redirects, then the response was cached. Thank 
> you for clarification.
> 
>     301 should be cached unless the object would need revalidation
>     immediately.
> 
> 
> But for 301 I always get MISS - with and without a cache-control header:
> 
> $ curl -v http:/test.example.com/img301.jpg 
> <http://test.example.com/img301.jpg>
> 
>> GET /img301.jpg HTTP/1.1
>> Host: test.example.com <http://test.example.com>
>> User-Agent: curl/7.50.1
>> Accept: */*
>>
> < HTTP/1.1 301 Moved Permanently
> < Date: Thu, 21 Sep 2017 06:44:41 GMT
> < Server: Apache
> < Cache-Control: max-age=60, must-revalidate
> < Location: http://test.example.com <http://test.example.com>/img.svg
> < Content-Length: 247
> < Content-Type: text/html; charset=iso-8859-1
> < X-Cache: MISS from <squid-proxy>
> < X-Cache-Lookup: MISS from <squid-proxy>:3128
> 
> I can't find any information on Squid wiki or via Google if the object 
> need revalidation immediately.

Thats what the "must-revalidate" means. It should work better with just 
max-age or Expires header - and with a longer value than 60 sec since 
this is supposed to be a *permanent* situation.


Amos


From ziegleka at gmail.com  Thu Sep 21 10:16:46 2017
From: ziegleka at gmail.com (kAja Ziegler)
Date: Thu, 21 Sep 2017 12:16:46 +0200
Subject: [squid-users] When the redirect [301, 302,
	307] is cached by Squid?
In-Reply-To: <85226f4c-1f2e-c0a3-eec3-97ca5e15603b@treenet.co.nz>
References: <CAMuNeAsTYYSs7c-J9_yqZwJRZOsgcc9-ZJh0OVS1djVYBym=YA@mail.gmail.com>
 <16a27a18-d2a5-c1a8-628f-2f14685e931b@treenet.co.nz>
 <CAMuNeAv56nT4HBxjFQTQ_RifSOQrmajSKTh8Qo_gFgKtRvVsmA@mail.gmail.com>
 <85226f4c-1f2e-c0a3-eec3-97ca5e15603b@treenet.co.nz>
Message-ID: <CAMuNeAuXSgp+44CbhierUGHCcB2K6ELNr+YtXDC9LW8CVe-=yQ@mail.gmail.com>

>
> Thats what the "must-revalidate" means. It should work better with just
> max-age or Expires header - and with a longer value than 60 sec since this
> is supposed to be a *permanent* situation.


As I know "must-revalidate" mean "refuse to return stale responses to the
user even if they say that stale responses are acceptable" - cached object
must be revalidated.

Max-age=60 was only used for testing.

The results of my testing:

- redirect 302 or 307 - to be cached needs Cache-Control max-age > 0 or
Expires "access plus 1 seconds"
- redirect 301 - to be cached needs Cache-Control max-age > 60 or Expires
"access plus 61 seconds"

This is strange because I thought that 301 is always cached without
Cache-Control or Expires headers. And I can't find any information in the
documentation which describes such behaviour.

zigi

On Thu, Sep 21, 2017 at 11:11 AM, Amos Jeffries <squid3 at treenet.co.nz>
wrote:

> On 21/09/17 20:36, kAja Ziegler wrote:
>
>> Hi Amos,
>>
>>     302 and 307 are not because as their status description indicates
>>     they are *temporary* results. They can only be cached if there are
>>     explicit details from the server indicating for how long.
>>
>>
>> You were right. After I added header Cache-Control "max-age=60,
>> must-revalidate" to the redirects, then the response was cached. Thank you
>> for clarification.
>>
>>     301 should be cached unless the object would need revalidation
>>     immediately.
>>
>>
>> But for 301 I always get MISS - with and without a cache-control header:
>>
>> $ curl -v http:/test.example.com/img301.jpg <
>> http://test.example.com/img301.jpg>
>>
>> GET /img301.jpg HTTP/1.1
>>> Host: test.example.com <http://test.example.com>
>>> User-Agent: curl/7.50.1
>>> Accept: */*
>>>
>>> < HTTP/1.1 301 Moved Permanently
>> < Date: Thu, 21 Sep 2017 06:44:41 GMT
>> < Server: Apache
>> < Cache-Control: max-age=60, must-revalidate
>> < Location: http://test.example.com <http://test.example.com>/img.svg
>> < Content-Length: 247
>> < Content-Type: text/html; charset=iso-8859-1
>> < X-Cache: MISS from <squid-proxy>
>> < X-Cache-Lookup: MISS from <squid-proxy>:3128
>>
>> I can't find any information on Squid wiki or via Google if the object
>> need revalidation immediately.
>>
>
> Thats what the "must-revalidate" means. It should work better with just
> max-age or Expires header - and with a longer value than 60 sec since this
> is supposed to be a *permanent* situation.
>
>
> Amos
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20170921/f50002a3/attachment.htm>

From squid3 at treenet.co.nz  Thu Sep 21 12:24:11 2017
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Fri, 22 Sep 2017 00:24:11 +1200
Subject: [squid-users] When the redirect [301, 302,
 307] is cached by Squid?
In-Reply-To: <CAMuNeAuXSgp+44CbhierUGHCcB2K6ELNr+YtXDC9LW8CVe-=yQ@mail.gmail.com>
References: <CAMuNeAsTYYSs7c-J9_yqZwJRZOsgcc9-ZJh0OVS1djVYBym=YA@mail.gmail.com>
 <16a27a18-d2a5-c1a8-628f-2f14685e931b@treenet.co.nz>
 <CAMuNeAv56nT4HBxjFQTQ_RifSOQrmajSKTh8Qo_gFgKtRvVsmA@mail.gmail.com>
 <85226f4c-1f2e-c0a3-eec3-97ca5e15603b@treenet.co.nz>
 <CAMuNeAuXSgp+44CbhierUGHCcB2K6ELNr+YtXDC9LW8CVe-=yQ@mail.gmail.com>
Message-ID: <5d9406a2-0192-de83-a2fc-2ca577b6c07b@treenet.co.nz>

On 21/09/17 22:16, kAja Ziegler wrote:
>     Thats what the "must-revalidate" means. It should work better with
>     just max-age or Expires header - and with a longer value than 60 sec
>     since this is supposed to be a *permanent* situation.
> 
> 
> As I know "must-revalidate" mean "refuse to return stale responses to 
> the user even if they say that stale responses are acceptable" - cached 
> object must be revalidated.
> 
> Max-age=60 was only used for testing.
> 
> The results of my testing:
> 
> - redirect 302 or 307 - to be cached needs Cache-Control max-age > 0 or 
> Expires "access plus 1 seconds"
> - redirect 301 - to be cached needs Cache-Control max-age > 60 or 
> Expires "access plus 61 seconds"
> 

There is no >60 need on the 301. Just me suggesting that 60sec is too 
short caching time for a _permanent_ thing.

> This is strange because I thought that 301 is always cached without 
> Cache-Control or Expires headers. And I can't find any information in 
> the documentation which describes such behaviour.

It should be, so long as it is fresh so that means it does depend on 
refresh_pattern saying it is fresh when no controls are present.

Amos


From alex at dvm.esines.cu  Thu Sep 21 12:50:57 2017
From: alex at dvm.esines.cu (=?UTF-8?Q?Alex_Guti=c3=a9rrez_Mart=c3=adnez?=)
Date: Thu, 21 Sep 2017 08:50:57 -0400
Subject: [squid-users] time error problem
Message-ID: <5a4045a0-b19f-3fd0-677d-fa6f2bdb8ba8@dvm.esines.cu>

Hello everyone, i have an ubuntu 14.04 configured for time zone "Havana" 
on meridian -5. But when i get an error page on my squid, for whatever 
reason, it puts the time zone as if it were in meridian 0. Any idea why?


Thanks in advance

-- 
Saludos Cordiales

Lic. Alex Guti?rrez Mart?nez

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20170921/b2e55dac/attachment.htm>

From ziegleka at gmail.com  Thu Sep 21 13:16:04 2017
From: ziegleka at gmail.com (kAja Ziegler)
Date: Thu, 21 Sep 2017 15:16:04 +0200
Subject: [squid-users] When the redirect [301, 302,
	307] is cached by Squid?
In-Reply-To: <5d9406a2-0192-de83-a2fc-2ca577b6c07b@treenet.co.nz>
References: <CAMuNeAsTYYSs7c-J9_yqZwJRZOsgcc9-ZJh0OVS1djVYBym=YA@mail.gmail.com>
 <16a27a18-d2a5-c1a8-628f-2f14685e931b@treenet.co.nz>
 <CAMuNeAv56nT4HBxjFQTQ_RifSOQrmajSKTh8Qo_gFgKtRvVsmA@mail.gmail.com>
 <85226f4c-1f2e-c0a3-eec3-97ca5e15603b@treenet.co.nz>
 <CAMuNeAuXSgp+44CbhierUGHCcB2K6ELNr+YtXDC9LW8CVe-=yQ@mail.gmail.com>
 <5d9406a2-0192-de83-a2fc-2ca577b6c07b@treenet.co.nz>
Message-ID: <CAMuNeAuvGAPRg5cqzGjRVwtt3y03j4MXhu9ofhhOiYEs4op3ug@mail.gmail.com>

>
> There is no >60 need on the 301. Just me suggesting that 60sec is too
> short caching time for a _permanent_ thing.


There is - may be some misconfiguration in my squid.conf.

If I set max-age to values from interval <1;60> only 302 and 307 redirects
were cached (HIT) and no 301. When I increased the max-age value to 61 then
the 301 redirect was cached too.


But you are right, that the cause of no-caching the 301 redirect is the min
value of:

refresh_pattern .		0	20%	4320

But for redirects 302 and 307 the above-mentioned refresh_pattern is ignored.


My source server is Apache and the corresponding configuration is:

RewriteRule /img301.jpg /img.svg [R=301,L,E=rcache:1]

RewriteRule /img302.jpg /img.svg [R=302,L,E=rcache:1]
RewriteRule /img307.jpg /img.svg [R=307,L,E=rcache:1]

Header always set Cache-Control "max-age=3" env=rcache

zigi

On Thu, Sep 21, 2017 at 2:24 PM, Amos Jeffries <squid3 at treenet.co.nz> wrote:

> On 21/09/17 22:16, kAja Ziegler wrote:
>
>>     Thats what the "must-revalidate" means. It should work better with
>>     just max-age or Expires header - and with a longer value than 60 sec
>>     since this is supposed to be a *permanent* situation.
>>
>>
>> As I know "must-revalidate" mean "refuse to return stale responses to the
>> user even if they say that stale responses are acceptable" - cached object
>> must be revalidated.
>>
>> Max-age=60 was only used for testing.
>>
>> The results of my testing:
>>
>> - redirect 302 or 307 - to be cached needs Cache-Control max-age > 0 or
>> Expires "access plus 1 seconds"
>> - redirect 301 - to be cached needs Cache-Control max-age > 60 or Expires
>> "access plus 61 seconds"
>>
>>
> There is no >60 need on the 301. Just me suggesting that 60sec is too
> short caching time for a _permanent_ thing.
>
> This is strange because I thought that 301 is always cached without
>> Cache-Control or Expires headers. And I can't find any information in the
>> documentation which describes such behaviour.
>>
>
> It should be, so long as it is fresh so that means it does depend on
> refresh_pattern saying it is fresh when no controls are present.
>
> Amos
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20170921/0d759f92/attachment.htm>

From alex at dvm.esines.cu  Thu Sep 21 21:07:28 2017
From: alex at dvm.esines.cu (=?UTF-8?Q?Alex_Guti=c3=a9rrez_Mart=c3=adnez?=)
Date: Thu, 21 Sep 2017 17:07:28 -0400
Subject: [squid-users] delay pool not workin
Message-ID: <0e306c67-faed-9b4d-56e3-14f546e626a8@dvm.esines.cu>

Could someone be so kind? to explain to me why my rules do not work on 
my delays pools?


i got this acl "lento", in spanish means slow

acl lento url_regex -i "/etc/squid3/bloqueo/lento"

his format is the next:


.youtube.com

.facebook.com


My delay config is the next:


###############################################################################
#Delay#
###############################################################################
delay_pools 3

#Canal 1 extensiones.
delay_class 1 2
delay_parameters 1 32768/32768 32768/32768
delay_access 1 deny !sociales lento navegacion !extensiones
#delay_access 1 deny all

#Canal 2 para usuarios.
delay_class 2 2
delay_parameters 2 65536/65536 32768/32768
delay_access 2 deny !navegacion extensiones lento sociales
#delay_access 2 deny all

#Canal 2 para usuarios.
delay_class 3 1
delay_parameters 3 16384/16384
delay_access 3 deny extensiones navegacion sociales !lento
#delay_access 2 deny all



my problem is simple, on my sqstat show the url's of "lento" with 0 on 
delay parameter, i do not understand why it happens. the program should 
show 3


thanks in advance

-- 
Saludos Cordiales

Lic. Alex Guti?rrez Mart?nez

Tel. +53 7 2710327

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20170921/02ee6868/attachment.htm>

From squid3 at treenet.co.nz  Fri Sep 22 00:59:12 2017
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Fri, 22 Sep 2017 12:59:12 +1200
Subject: [squid-users] time error problem
In-Reply-To: <5a4045a0-b19f-3fd0-677d-fa6f2bdb8ba8@dvm.esines.cu>
References: <5a4045a0-b19f-3fd0-677d-fa6f2bdb8ba8@dvm.esines.cu>
Message-ID: <5a5eb457-9dd4-1fef-1c34-8442179c6ed4@treenet.co.nz>

On 22/09/17 00:50, Alex Guti?rrez Mart?nez wrote:
> Hello everyone, i have an ubuntu 14.04 configured for time zone "Havana" 
> on meridian -5. But when i get an error page on my squid, for whatever 
> reason, it puts the time zone as if it were in meridian 0. Any idea why?
> 

Because the Internet runs on UTC not your local timezone.
Error pages are sent to anyone trying to access your proxy regardless of 
location (eg denying external access through it).

Amos


From squid3 at treenet.co.nz  Fri Sep 22 01:03:49 2017
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Fri, 22 Sep 2017 13:03:49 +1200
Subject: [squid-users] delay pool not workin
In-Reply-To: <0e306c67-faed-9b4d-56e3-14f546e626a8@dvm.esines.cu>
References: <0e306c67-faed-9b4d-56e3-14f546e626a8@dvm.esines.cu>
Message-ID: <7b5d9b7d-7280-1cd6-4d8c-3257c267c942@treenet.co.nz>

On 22/09/17 09:07, Alex Guti?rrez Mart?nez wrote:
> Could someone be so kind? to explain to me why my rules do not work on 
> my delays pools?
> 
> 
> i got this acl "lento", in spanish means slow
> 
> acl lento url_regex -i "/etc/squid3/bloqueo/lento"
> 
> his format is the next:
> 
> 
> .youtube.com
> 
> .facebook.com
> 

First problem: you are putting domains in dstdomain format into a 
full-URL regex ACL.

Use dstdomain ACL type for these. Much faster.


> 
> My delay config is the next:
> 
> 
> ###############################################################################
> #Delay#
> ###############################################################################
> delay_pools 3
> 
> #Canal 1 extensiones.
> delay_class 1 2
> delay_parameters 1 32768/32768 32768/32768
> delay_access 1 deny !sociales lento navegacion !extensiones
> #delay_access 1 deny all
> 
> #Canal 2 para usuarios.
> delay_class 2 2
> delay_parameters 2 65536/65536 32768/32768
> delay_access 2 deny !navegacion extensiones lento sociales
> #delay_access 2 deny all
> 
> #Canal 2 para usuarios.
> delay_class 3 1
> delay_parameters 3 16384/16384
> delay_access 3 deny extensiones navegacion sociales !lento
> #delay_access 2 deny all
> 
> 

Second problem: deny, deny all. Nothing allowed to use these pools.


Amos


From sukhbaatar_t at yahoo.com  Fri Sep 22 02:37:23 2017
From: sukhbaatar_t at yahoo.com (Sukhbaatar T)
Date: Fri, 22 Sep 2017 02:37:23 +0000 (UTC)
Subject: [squid-users] squid-users Digest, Vol 37, Issue 50
In-Reply-To: <mailman.2211.1506042232.3209.squid-users@lists.squid-cache.org>
References: <mailman.2211.1506042232.3209.squid-users@lists.squid-cache.org>
Message-ID: <1026131530.72470.1506047843278@mail.yahoo.com>

Hello. Yesterday virus attack my squid proxy server. Lost my all config. Can you give me congfig file normal for Windows. 50gb size on hdd for cash, youtube, fb cashing, 8 gb ram. Forgot many command line for quick access. Forgot link for example.

Sent from Yahoo Mail for iPhone


On Friday, September 22, 2017, 9:03 AM, squid-users-request at lists.squid-cache.org wrote:

Send squid-users mailing list submissions to
??? squid-users at lists.squid-cache.org

To subscribe or unsubscribe via the World Wide Web, visit
??? http://lists.squid-cache.org/listinfo/squid-users
or, via email, send a message with subject or body 'help' to
??? squid-users-request at lists.squid-cache.org

You can reach the person managing the list at
??? squid-users-owner at lists.squid-cache.org

When replying, please edit your Subject line so it is more specific
than "Re: Contents of squid-users digest..."


Today's Topics:

? 1. Re: When the redirect [301, 302, 307] is cached by Squid?
? ? ? (Amos Jeffries)
? 2. time error problem (Alex Guti?rrez Mart?nez)
? 3. Re: When the redirect [301, 302,??? 307] is cached by Squid?
? ? ? (kAja Ziegler)
? 4. delay pool not workin (Alex Guti?rrez Mart?nez)
? 5. Re: time error problem (Amos Jeffries)
? 6. Re: delay pool not workin (Amos Jeffries)


----------------------------------------------------------------------

Message: 1
Date: Fri, 22 Sep 2017 00:24:11 +1200
From: Amos Jeffries <squid3 at treenet.co.nz>
To: squid-users at lists.squid-cache.org
Subject: Re: [squid-users] When the redirect [301, 302, 307] is cached
??? by Squid?
Message-ID: <5d9406a2-0192-de83-a2fc-2ca577b6c07b at treenet.co.nz>
Content-Type: text/plain; charset=utf-8; format=flowed

On 21/09/17 22:16, kAja Ziegler wrote:
>? ? Thats what the "must-revalidate" means. It should work better with
>? ? just max-age or Expires header - and with a longer value than 60 sec
>? ? since this is supposed to be a *permanent* situation.
> 
> 
> As I know "must-revalidate" mean "refuse to return stale responses to 
> the user even if they say that stale responses are acceptable" - cached 
> object must be revalidated.
> 
> Max-age=60 was only used for testing.
> 
> The results of my testing:
> 
> - redirect 302 or 307 - to be cached needs Cache-Control max-age > 0 or 
> Expires "access plus 1 seconds"
> - redirect 301 - to be cached needs Cache-Control max-age > 60 or 
> Expires "access plus 61 seconds"
> 

There is no >60 need on the 301. Just me suggesting that 60sec is too 
short caching time for a _permanent_ thing.

> This is strange because I thought that 301 is always cached without 
> Cache-Control or Expires headers. And I can't find any information in 
> the documentation which describes such behaviour.

It should be, so long as it is fresh so that means it does depend on 
refresh_pattern saying it is fresh when no controls are present.

Amos


------------------------------

Message: 2
Date: Thu, 21 Sep 2017 08:50:57 -0400
From: Alex Guti?rrez Mart?nez <alex at dvm.esines.cu>
To: squid-users at lists.squid-cache.org
Subject: [squid-users] time error problem
Message-ID: <5a4045a0-b19f-3fd0-677d-fa6f2bdb8ba8 at dvm.esines.cu>
Content-Type: text/plain; charset="utf-8"; Format="flowed"

Hello everyone, i have an ubuntu 14.04 configured for time zone "Havana" 
on meridian -5. But when i get an error page on my squid, for whatever 
reason, it puts the time zone as if it were in meridian 0. Any idea why?


Thanks in advance

-- 
Saludos Cordiales

Lic. Alex Guti?rrez Mart?nez

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20170921/b2e55dac/attachment-0001.html>

------------------------------

Message: 3
Date: Thu, 21 Sep 2017 15:16:04 +0200
From: kAja Ziegler <ziegleka at gmail.com>
To: squid-users at lists.squid-cache.org
Subject: Re: [squid-users] When the redirect [301, 302,??? 307] is cached
??? by Squid?
Message-ID:
??? <CAMuNeAuvGAPRg5cqzGjRVwtt3y03j4MXhu9ofhhOiYEs4op3ug at mail.gmail.com>
Content-Type: text/plain; charset="utf-8"

>
> There is no >60 need on the 301. Just me suggesting that 60sec is too
> short caching time for a _permanent_ thing.


There is - may be some misconfiguration in my squid.conf.

If I set max-age to values from interval <1;60> only 302 and 307 redirects
were cached (HIT) and no 301. When I increased the max-age value to 61 then
the 301 redirect was cached too.


But you are right, that the cause of no-caching the 301 redirect is the min
value of:

refresh_pattern .??? ??? 0??? 20%??? 4320

But for redirects 302 and 307 the above-mentioned refresh_pattern is ignored.


My source server is Apache and the corresponding configuration is:

RewriteRule /img301.jpg /img.svg [R=301,L,E=rcache:1]

RewriteRule /img302.jpg /img.svg [R=302,L,E=rcache:1]
RewriteRule /img307.jpg /img.svg [R=307,L,E=rcache:1]

Header always set Cache-Control "max-age=3" env=rcache

zigi

On Thu, Sep 21, 2017 at 2:24 PM, Amos Jeffries <squid3 at treenet.co.nz> wrote:

> On 21/09/17 22:16, kAja Ziegler wrote:
>
>>? ? Thats what the "must-revalidate" means. It should work better with
>>? ? just max-age or Expires header - and with a longer value than 60 sec
>>? ? since this is supposed to be a *permanent* situation.
>>
>>
>> As I know "must-revalidate" mean "refuse to return stale responses to the
>> user even if they say that stale responses are acceptable" - cached object
>> must be revalidated.
>>
>> Max-age=60 was only used for testing.
>>
>> The results of my testing:
>>
>> - redirect 302 or 307 - to be cached needs Cache-Control max-age > 0 or
>> Expires "access plus 1 seconds"
>> - redirect 301 - to be cached needs Cache-Control max-age > 60 or Expires
>> "access plus 61 seconds"
>>
>>
> There is no >60 need on the 301. Just me suggesting that 60sec is too
> short caching time for a _permanent_ thing.
>
> This is strange because I thought that 301 is always cached without
>> Cache-Control or Expires headers. And I can't find any information in the
>> documentation which describes such behaviour.
>>
>
> It should be, so long as it is fresh so that means it does depend on
> refresh_pattern saying it is fresh when no controls are present.
>
> Amos
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20170921/0d759f92/attachment-0001.html>

------------------------------

Message: 4
Date: Thu, 21 Sep 2017 17:07:28 -0400
From: Alex Guti?rrez Mart?nez <alex at dvm.esines.cu>
To: squid-users at lists.squid-cache.org
Subject: [squid-users] delay pool not workin
Message-ID: <0e306c67-faed-9b4d-56e3-14f546e626a8 at dvm.esines.cu>
Content-Type: text/plain; charset="utf-8"; Format="flowed"

Could someone be so kind? to explain to me why my rules do not work on 
my delays pools?


i got this acl "lento", in spanish means slow

acl lento url_regex -i "/etc/squid3/bloqueo/lento"

his format is the next:


.youtube.com

.facebook.com


My delay config is the next:


###############################################################################
#Delay#
###############################################################################
delay_pools 3

#Canal 1 extensiones.
delay_class 1 2
delay_parameters 1 32768/32768 32768/32768
delay_access 1 deny !sociales lento navegacion !extensiones
#delay_access 1 deny all

#Canal 2 para usuarios.
delay_class 2 2
delay_parameters 2 65536/65536 32768/32768
delay_access 2 deny !navegacion extensiones lento sociales
#delay_access 2 deny all

#Canal 2 para usuarios.
delay_class 3 1
delay_parameters 3 16384/16384
delay_access 3 deny extensiones navegacion sociales !lento
#delay_access 2 deny all



my problem is simple, on my sqstat show the url's of "lento" with 0 on 
delay parameter, i do not understand why it happens. the program should 
show 3


thanks in advance

-- 
Saludos Cordiales

Lic. Alex Guti?rrez Mart?nez

Tel. +53 7 2710327

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20170921/02ee6868/attachment-0001.html>

------------------------------

Message: 5
Date: Fri, 22 Sep 2017 12:59:12 +1200
From: Amos Jeffries <squid3 at treenet.co.nz>
To: squid-users at lists.squid-cache.org
Subject: Re: [squid-users] time error problem
Message-ID: <5a5eb457-9dd4-1fef-1c34-8442179c6ed4 at treenet.co.nz>
Content-Type: text/plain; charset=utf-8; format=flowed

On 22/09/17 00:50, Alex Guti?rrez Mart?nez wrote:
> Hello everyone, i have an ubuntu 14.04 configured for time zone "Havana" 
> on meridian -5. But when i get an error page on my squid, for whatever 
> reason, it puts the time zone as if it were in meridian 0. Any idea why?
> 

Because the Internet runs on UTC not your local timezone.
Error pages are sent to anyone trying to access your proxy regardless of 
location (eg denying external access through it).

Amos


------------------------------

Message: 6
Date: Fri, 22 Sep 2017 13:03:49 +1200
From: Amos Jeffries <squid3 at treenet.co.nz>
To: squid-users at lists.squid-cache.org
Subject: Re: [squid-users] delay pool not workin
Message-ID: <7b5d9b7d-7280-1cd6-4d8c-3257c267c942 at treenet.co.nz>
Content-Type: text/plain; charset=utf-8; format=flowed

On 22/09/17 09:07, Alex Guti?rrez Mart?nez wrote:
> Could someone be so kind? to explain to me why my rules do not work on 
> my delays pools?
> 
> 
> i got this acl "lento", in spanish means slow
> 
> acl lento url_regex -i "/etc/squid3/bloqueo/lento"
> 
> his format is the next:
> 
> 
> .youtube.com
> 
> .facebook.com
> 

First problem: you are putting domains in dstdomain format into a 
full-URL regex ACL.

Use dstdomain ACL type for these. Much faster.


> 
> My delay config is the next:
> 
> 
> ###############################################################################
> #Delay#
> ###############################################################################
> delay_pools 3
> 
> #Canal 1 extensiones.
> delay_class 1 2
> delay_parameters 1 32768/32768 32768/32768
> delay_access 1 deny !sociales lento navegacion !extensiones
> #delay_access 1 deny all
> 
> #Canal 2 para usuarios.
> delay_class 2 2
> delay_parameters 2 65536/65536 32768/32768
> delay_access 2 deny !navegacion extensiones lento sociales
> #delay_access 2 deny all
> 
> #Canal 2 para usuarios.
> delay_class 3 1
> delay_parameters 3 16384/16384
> delay_access 3 deny extensiones navegacion sociales !lento
> #delay_access 2 deny all
> 
> 

Second problem: deny, deny all. Nothing allowed to use these pools.


Amos


------------------------------

Subject: Digest Footer

_______________________________________________
squid-users mailing list
squid-users at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-users


------------------------------

End of squid-users Digest, Vol 37, Issue 50
*******************************************



-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20170922/1ce2c83d/attachment.htm>

From squid3 at treenet.co.nz  Fri Sep 22 06:40:30 2017
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Fri, 22 Sep 2017 18:40:30 +1200
Subject: [squid-users] squid-users Digest, Vol 37, Issue 50
In-Reply-To: <1026131530.72470.1506047843278@mail.yahoo.com>
References: <mailman.2211.1506042232.3209.squid-users@lists.squid-cache.org>
 <1026131530.72470.1506047843278@mail.yahoo.com>
Message-ID: <f69dc2dd-140d-fecd-1a00-aa45d0e42bc8@treenet.co.nz>

On 22/09/17 14:37, Sukhbaatar T wrote:
> Hello. Yesterday virus attack my squid proxy server. Lost my all config. 
> Can you give me congfig file normal for Windows. 50gb size on hdd for 
> cash, youtube, fb cashing, 8 gb ram. Forgot many command line for quick 
> access. Forgot link for example.
> 

Windows specific details can be found at:
  <https://wiki.squid-cache.org/KnowledgeBase/Windows>

For the MITM settings required to cache Facebook, info can be found here:
  <https://wiki.squid-cache.org/Features/SslPeekAndSplice>

YouTube is more difficult, the devs there are actively preventing 
caching. You will have to find out whatever your previous solution was 
and redo it.

The rest is general Squid use you should be able to find with a quick 
web search, if not checkout the FAQ in the wiki linked above.

Amos


From alex at dvm.esines.cu  Fri Sep 22 14:31:45 2017
From: alex at dvm.esines.cu (=?UTF-8?Q?Alex_Guti=c3=a9rrez_Mart=c3=adnez?=)
Date: Fri, 22 Sep 2017 10:31:45 -0400
Subject: [squid-users] delay pool not workin
Message-ID: <cfd8aaa5-730a-f824-89a8-96e59f87fe9f@dvm.esines.cu>

Could someone be so kind? to explain to me why my rules do not work on
my delays pools?


i got this acl "lento", in spanish means slow

acl lento url_regex -i "/etc/squid3/bloqueo/lento"

his format is the next:


.youtube.com

.facebook.com

First problem: you are putting domains in dstdomain format into a
full-URL regex ACL.

Use dstdomain ACL type for these. Much faster.


> My delay config is the next:
>
>
> ###############################################################################
> #Delay#
> ###############################################################################
> delay_pools 3
>
> #Canal 1 extensiones.
> delay_class 1 2
> delay_parameters 1 32768/32768 32768/32768
> delay_access 1 deny !sociales lento navegacion !extensiones
> #delay_access 1 deny all
>
> #Canal 2 para usuarios.
> delay_class 2 2
> delay_parameters 2 65536/65536 32768/32768
> delay_access 2 deny !navegacion extensiones lento sociales
> #delay_access 2 deny all
>
> #Canal 2 para usuarios.
> delay_class 3 1
> delay_parameters 3 16384/16384
> delay_access 3 deny extensiones navegacion sociales !lento
> #delay_access 2 deny all
>
>
Second problem: deny, deny all. Nothing allowed to use these pools.


Amos

############################################################################################################################

Thanks again Mr. Jeffries, i change my delay to:

acl navegaci?n src 192.168.9.0/24

acl lento dstdomain "/etc/squid3/bloqueo/lento"?? --> .youtube.com

acl sociales dstdomain "/etc/squid3/bloqueo/sociales"? --> .linkedin.com

acl correos dstdomain "/etc/squid3/bloqueo/correos" -->.mail.yahoo.com

acl extensiones urlpath_regex -i "/etc/squid3/bloqueo/listaextensiones" 
--> \.mkv$

delay_pools 3

#Canal 1 extensiones.
delay_class 1 1
delay_parameters 1 32768/32768
delay_access 1 allow extensiones !navegacion !lento !sociales !correos
delay_access 1 deny all

#Canal 2 para usuarios.
delay_class 2 1
delay_parameters 2 65536/65536
delay_access 2 allow navegacion !lento !sociales !correos !extensiones
delay_access 2 deny all

#Canal 3 para cosas lentas.
delay_class 3 1
delay_parameters 3 8192/16384
delay_access 3 allow lento sociales correos !navegacion !extensiones
delay_access 3 deny all

But my sqstat shows the use of delay pool # 2, # 1 and # 3 are disable. 
On youtube shows delay_pool=0.

I put the following configuration but I was unable to make it work. 
Again only delay pool # 2 was the only who worked this time.


delay_pools 3
Processing: delay_class 1 1
delay_parameters 1 32768/32768
delay_access 1 allow extensiones !navegacion !lento !sociales !correos
delay_class 2 1
delay_parameters 2 65536/65536
delay_access 2 allow navegacion !lento !sociales !correos !extensiones
delay_class 3 1
delay_parameters 3 8192/16384
delay_access 3 allow lento sociales correos !navegacion !extensiones

Im using squid 3.3.8 on ubuntu 14.04.

-- 
Saludos Cordiales

Lic. Alex Guti?rrez Mart?nez

Tel. +53 7 2710327

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20170922/23404c9d/attachment.htm>

From erdosain9 at gmail.com  Fri Sep 22 14:37:17 2017
From: erdosain9 at gmail.com (erdosain9)
Date: Fri, 22 Sep 2017 07:37:17 -0700 (MST)
Subject: [squid-users] Negotiate Authenticator and DNS
Message-ID: <1506091037147-0.post@n4.nabble.com>

Hi.
Im traying to improve the dns response because im having this times:

Negotiate Authenticator Statistics:
program: /lib64/squid/negotiate_kerberos_auth
number active: 32 of 32 (0 shutting down)
requests sent: 72241
replies received: 72241
queue length: 0
avg service time: 56 msec

   ID #	     FD	    PID	 # Requests	  # Replies	 Flags	   Time	 Offset
Request
     16	     30	  22242	      38896	      38896	     	  0.368	      0	(none)
     17	     32	  22243	      13404	      13404	     	  0.388	      0	(none)
     18	     38	  22244	       6962	       6962	     	  0.126	      0	(none)
     19	     61	  22245	       3895	       3895	     	  0.344	      0	(none)
     20	     65	  22246	       2636	       2636	     	  0.369	      0	(none)
     21	     74	  22247	       1879	       1879	     	  0.124	      0	(none)
     22	     76	  22248	       1177	       1177	     	  0.340	      0	(none)
     23	     78	  22249	        809	        809	     	  0.307	      0	(none)
     24	     79	  22250	        592	        592	     	  0.364	      0	(none)
     25	     81	  22251	        436	        436	     	  0.265	      0	(none)
     26	     94	  22252	        320	        320	     	  0.244	      0	(none)
     27	     96	  22253	        243	        243	     	  0.243	      0	(none)
     28	     98	  22254	        184	        184	     	  0.299	      0	(none)
     29	    109	  22255	        142	        142	     	  0.285	      0	(none)
     30	    111	  22256	        112	        112	     	  0.308	      0	(none)
     31	    113	  22257	         85	         85	     	  0.308	      0	(none)
     45	    473	  22285	         69	         69	     	  0.789	      0	(none)
     46	    475	  22286	         60	         60	     	  0.756	      0	(none)
     47	    480	  22287	         52	         52	     	  1.504	      0	(none)
     48	    495	  22288	         48	         48	     	  1.611	      0	(none)
     49	    499	  22289	         44	         44	     	  1.611	      0	(none)
     50	    580	  22291	         36	         36	     	  1.598	      0	(none)
     51	    596	  22292	         31	         31	     	  1.099	      0	(none)
     52	    593	  22293	         26	         26	     	  0.916	      0	(none)
     53	    547	  22308	         20	         20	     	  0.916	      0	(none)
     54	    550	  22309	         18	         18	     	  0.602	      0	(none)
     55	    551	  22310	         14	         14	     	  0.397	      0	(none)
     56	    553	  22311	         12	         12	     	  0.567	      0	(none)
     57	    552	  22312	         12	         12	     	  0.567	      0	(none)
     58	    397	  22313	         11	         11	     	  0.567	      0	(none)
     59	    407	  22314	         10	         10	     	  0.584	      0	(none)
     67	    436	  22355	          6	          6	     	  1.035	      0	(none)

Sometimes much more time, sometimes go to avg service time: 560 msec...

Sorry for my ignorance...
This Negotiate Authenticator is for users??? i mean this is related to, for
example, go to google.com, or is just the time that the user (client pc)
wait for be authenticate??

I think, that is related to go to a web (now i have my doubts). so i make a
dns with bind. and put that dns in squid config, and let the dns from the AD
in second place... but, when i restart this happend:

support_resolv.cc(289): pid=24587 :2017/09/22 11:16:35| kerberos_ldap_group:
ERROR: Error while resolving service record _ldap._tcp.DOMAIN.LAN with r
es_search
support_resolv.cc(71): pid=24587 :2017/09/22 11:16:35| kerberos_ldap_group:
ERROR: res_search: Unknown service record: _ldap._tcp.DOMAIN.LAN
support_resolv.cc(183): pid=24587 :2017/09/22 11:16:35| kerberos_ldap_group:
ERROR: Error while resolving hostname with getaddrinfo: Name or service 
not known
support_sasl.cc(276): pid=24587 :2017/09/22 11:16:35| kerberos_ldap_group:
ERROR: ldap_sasl_interactive_bind_s error: Can't contact LDAP server
support_ldap.cc(957): pid=24587 :2017/09/22 11:16:35| kerberos_ldap_group:
ERROR: Error while binding to ldap server with SASL/GSSAPI: Can't contact 
LDAP server


So, this post is for two question. 
1- The thing about Negotiate Authenticator (that value what represent?)
2- Can i improve making my own dns (apart from the the dns from the domain)?
(i prefer make other dns, than fix the dns from the domain, because i dont
manage that).

Thanks to all, and sorry for the ignorance, and my bad writing (i dont speak
english)



--
Sent from: http://squid-web-proxy-cache.1019090.n4.nabble.com/Squid-Users-f1019091.html


From squid3 at treenet.co.nz  Fri Sep 22 15:12:34 2017
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sat, 23 Sep 2017 03:12:34 +1200
Subject: [squid-users] delay pool not workin
In-Reply-To: <cfd8aaa5-730a-f824-89a8-96e59f87fe9f@dvm.esines.cu>
References: <cfd8aaa5-730a-f824-89a8-96e59f87fe9f@dvm.esines.cu>
Message-ID: <33e2fd6e-8548-5ba9-0b37-936a38a40fcf@treenet.co.nz>

On 23/09/17 02:31, Alex Guti?rrez Mart?nez wrote:
> Could someone be so kind? to explain to me why my rules do not work on
> my delays pools?
> 
...
> 
> Thanks again Mr. Jeffries, i change my delay to:
> 
> acl navegaci?n src 192.168.9.0/24
> 
> acl lento dstdomain "/etc/squid3/bloqueo/lento"?? --> .youtube.com
> 
> acl sociales dstdomain "/etc/squid3/bloqueo/sociales"? --> .linkedin.com
> 
> acl correos dstdomain "/etc/squid3/bloqueo/correos" -->.mail.yahoo.com
> 
> acl extensiones urlpath_regex -i "/etc/squid3/bloqueo/listaextensiones" 
> --> \.mkv$
> 
> delay_pools 3
> 
> #Canal 1 extensiones.
> delay_class 1 1
> delay_parameters 1 32768/32768
> delay_access 1 allow extensiones !navegacion !lento !sociales !correos
> delay_access 1 deny all
> 
> #Canal 2 para usuarios.
> delay_class 2 1
> delay_parameters 2 65536/65536
> delay_access 2 allow navegacion !lento !sociales !correos !extensiones
> delay_access 2 deny all
> 
> #Canal 3 para cosas lentas.
> delay_class 3 1
> delay_parameters 3 8192/16384
> delay_access 3 allow lento sociales correos !navegacion !extensiones
> delay_access 3 deny all
> 
> But my sqstat shows the use of delay pool # 2, # 1 and # 3 are disable. 
> On youtube shows delay_pool=0.
> 
> I put the following configuration but I was unable to make it work. 
> Again only delay pool # 2 was the only who worked this time.


Pool #3 requires the domain name of a single transaction to 
simultaneously be *mail.yahoo.com AND *.linkedin.com AND *.youtube.com
  Obviously that is impossible, so nothing can match the line that allows.

Pool #1 should match a few things. But probably not what you are testing 
with.

I suggest you try to re-write your ACLs in a simpler way with less '!' 
(not) modifiers. The way you are compressing lots of things into each 
line is no faster than multiple lines, but much harder to understand 
what is going on.


Amos


From EricL at daveramsey.com  Fri Sep 22 15:27:18 2017
From: EricL at daveramsey.com (Eric Lackey)
Date: Fri, 22 Sep 2017 15:27:18 +0000
Subject: [squid-users] Peek and Splice - Termination Log
Message-ID: <4626D608-CC19-486E-BA71-259A55FCEEE3@daveramsey.com>

Hello, we?re beginning to enable the Peek and Splice feature on Squid 3.5. Our ssl_bump configuration looks like below where we?re validating the request matches a domain in our allowed_sites file and then terminating the SSL connection if it does not. 

This is all working well except for the fact that we don?t have a good way to determine what is being blocked. In the configuration below, the only log we get is when Squid connects to the external server to get the SSL certificate and that is usually a 200 response. If the domain does not match our allowed list the connection is then terminated and no additional log is written. 

I know that we can see this in cache.log by enabling debugging (debug_options 28,4), but that?s a large amount of log data to try to process and report on and the structure of the log is not something that we can easily ingest into our logging platform. It would be great if we could get it into a JSON format similar to how we can with access_log. 

Does anyone else have a solution for this and if not, is this something that has been requested as a feature in the past?

Thanks in advance for any help.

======

# Define allowed sites
acl allowed_https_sites ssl::server_name_regex "/etc/squid/allowed_sites"

ssl_bump peek all
ssl_bump splice allowed_https_sites
ssl_bump terminate step3 all


From heiler.bemerguy at cinbesa.com.br  Fri Sep 22 15:48:53 2017
From: heiler.bemerguy at cinbesa.com.br (Heiler Bemerguy)
Date: Fri, 22 Sep 2017 12:48:53 -0300
Subject: [squid-users] delay pool not workin
In-Reply-To: <33e2fd6e-8548-5ba9-0b37-936a38a40fcf@treenet.co.nz>
References: <cfd8aaa5-730a-f824-89a8-96e59f87fe9f@dvm.esines.cu>
 <33e2fd6e-8548-5ba9-0b37-936a38a40fcf@treenet.co.nz>
Message-ID: <75b9a49f-bfea-1c87-5698-8134ea1c011b@cinbesa.com.br>

Amos, talking about delay pools, I have a question: does it work if the 
content being served is on a cache peer?

I think it only "shapes" traffic from a SERVER to squid, right? not from 
a peer cache to squid.. :/

I'm having problems because we use a huge Microsoft Updates repository 
as a cache peer and whenever a client on a 512kbit/s link (!!!!!!!!!) 
starts his box, all the link is flooded with updates from us to it.

htcp_access allow localnet
acl wu dstdom_regex \.download\.windowsupdate\.com$
acl wu-rejects dstdom_regex stats
acl GET method GET
cache_peer 10.1.10.10 parent 8081 0 proxy-only no-tproxy no-digest 
no-query no-netdb-exchange name=ms1
cache_peer_access ms1 allow GET wu !wu-rejects
cache_peer_access ms1 deny all
never_direct allow GET wu !wu-rejects
never_direct deny all
cache deny wu
cache allow all

prefer_direct off

acl srcdaico src 10.71.0.0/16
delay_pools 1
delay_class 1 3
delay_access 1 allow srcdaico !dstlocal
delay_access 1 deny all
delay_parameters 1 -1/-1 -1/-1 16000/16000


-- 
Atenciosamente / Best Regards,

Heiler Bemerguy
Network Manager - CINBESA
55 91 98151-4894/3184-1751


Em 22/09/2017 12:12, Amos Jeffries escreveu:
> On 23/09/17 02:31, Alex Guti?rrez Mart?nez wrote:
>> Could someone be so kind? to explain to me why my rules do not work on
>> my delays pools?
>>
> ...
>>
>> Thanks again Mr. Jeffries, i change my delay to:
>>
>> acl navegaci?n src 192.168.9.0/24
>>
>> acl lento dstdomain "/etc/squid3/bloqueo/lento"?? --> .youtube.com
>>
>> acl sociales dstdomain "/etc/squid3/bloqueo/sociales"? --> .linkedin.com
>>
>> acl correos dstdomain "/etc/squid3/bloqueo/correos" -->.mail.yahoo.com
>>
>> acl extensiones urlpath_regex -i 
>> "/etc/squid3/bloqueo/listaextensiones" --> \.mkv$
>>
>> delay_pools 3
>>
>> #Canal 1 extensiones.
>> delay_class 1 1
>> delay_parameters 1 32768/32768
>> delay_access 1 allow extensiones !navegacion !lento !sociales !correos
>> delay_access 1 deny all
>>
>> #Canal 2 para usuarios.
>> delay_class 2 1
>> delay_parameters 2 65536/65536
>> delay_access 2 allow navegacion !lento !sociales !correos !extensiones
>> delay_access 2 deny all
>>
>> #Canal 3 para cosas lentas.
>> delay_class 3 1
>> delay_parameters 3 8192/16384
>> delay_access 3 allow lento sociales correos !navegacion !extensiones
>> delay_access 3 deny all
>>
>> But my sqstat shows the use of delay pool # 2, # 1 and # 3 are 
>> disable. On youtube shows delay_pool=0.
>>
>> I put the following configuration but I was unable to make it work. 
>> Again only delay pool # 2 was the only who worked this time.
>
>
> Pool #3 requires the domain name of a single transaction to 
> simultaneously be *mail.yahoo.com AND *.linkedin.com AND *.youtube.com
> ?Obviously that is impossible, so nothing can match the line that allows.
>
> Pool #1 should match a few things. But probably not what you are 
> testing with.
>
> I suggest you try to re-write your ACLs in a simpler way with less '!' 
> (not) modifiers. The way you are compressing lots of things into each 
> line is no faster than multiple lines, but much harder to understand 
> what is going on.
>
>
> Amos
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users



From alex at dvm.esines.cu  Fri Sep 22 16:30:07 2017
From: alex at dvm.esines.cu (=?UTF-8?Q?Alex_Guti=c3=a9rrez_Mart=c3=adnez?=)
Date: Fri, 22 Sep 2017 12:30:07 -0400
Subject: [squid-users] delay pool not workin
Message-ID: <e44b476e-e2c7-047c-a0a1-7cf329f36c58@dvm.esines.cu>

Pool #3 requires the domain name of a single transaction to
simultaneously be *mail.yahoo.com AND *.linkedin.com AND *.youtube.com
 ? Obviously that is impossible, so nothing can match the line that allows.

Pool #1 should match a few things. But probably not what you are testing
with.

I suggest you try to re-write your ACLs in a simpler way with less '!'
(not) modifiers. The way you are compressing lots of things into each
line is no faster than multiple lines, but much harder to understand
what is going on.

#######################################################################
#######################################################################
#######################################################################
#######################################################################

Mr. Jeffries, I rewrite my acl?s on this ways:

# 1

#######################################################################

delay_pools 5

#Canal 1 extensiones.
delay_class 1 2
delay_access 1 allow extensiones
delay_access 1 deny navegacion lento sociales correos
delay_access 1 deny all
delay_parameters 1 16384/32768 32768/32768

#Canal 2 para usuarios.
delay_class 2 1
delay_access 2 allow navegacion
delay_access 2 deny lento sociales correos extensiones
delay_access 2 deny all
delay_parameters 2 65536/65536

#Canal 3 para cosas lentas.
delay_class 3 2
delay_access 3 allow lento
delay_access 3 deny navegacion extensiones sociales correos
delay_access 3 deny all
delay_parameters 3 4096/8192 8192/16384

#Canal 4 Sociales
delay_class 4 2
delay_access 4 allow sociales
delay_access 4 deny navegacion extensiones lento correos
delay_access 4 deny all
delay_parameters 4 4096/8192 8192/16384

#Canal 5 Correos
delay_class 5 2
delay_access 5 allow correos
delay_access 5 deny navegacion extensiones lento sociales
delay_access 5 deny all
delay_parameters 5 4096/8192 8192/16384

##############################################

#2

###################################################

delay_pools 5

#Canal 1 extensiones.
delay_class 1 2
delay_access 1 allow extensiones !navegacion !lento !sociales !correos
delay_access 1 deny all
delay_parameters 1 16384/32768 32768/32768

#Canal 2 para usuarios.
delay_class 2 1
delay_access 2 allow navegacion !lento !sociales !correos !extensiones
delay_access 2 deny all
delay_parameters 2 65536/65536

#Canal 3 para cosas lentas.
delay_class 3 2
delay_access 3 allow lento !navegacion !extensiones !sociales !correos
delay_access 3 deny all
delay_parameters 3 4096/8192 8192/16384

#Canal 4 Sociales
delay_class 4 2
delay_access 4 allow sociales !navegacion !extensiones !lento !correos
delay_access 4 deny all
delay_parameters 4 4096/8192 8192/16384

#Canal 5 Correos
delay_class 5 2
delay_access 5 allow correos !navegacion !extensiones !lento !sociales
delay_access 5 deny all
delay_parameters 5 4096/8192 8192/16384

#####################################################

#3

#######################################################

delay_pools 5

#Canal 1 extensiones.
delay_class 1 2
delay_access 1 allow extensiones
delay_access 1 deny all
delay_parameters 1 16384/32768 32768/32768

#Canal 2 para usuarios.
delay_class 2 1
delay_access 2 allow navegacion
delay_access 2 deny all
delay_parameters 2 65536/65536

#Canal 3 para cosas lentas.
delay_class 3 2
delay_access 3 allow lento
delay_access 3 deny all
delay_parameters 3 4096/8192 8192/16384

#Canal 4 Sociales
delay_class 4 2
delay_access 4 allow sociales
delay_access 4 deny all
delay_parameters 4 4096/8192 8192/16384

#Canal 5 Correos
delay_class 5 2
delay_access 5 allow correos
delay_access 5 deny all
delay_parameters 5 4096/8192 8192/16384
####################################################


Every request fails, only delay pool 2 is on use, execpt for example # 
2, in that case every request was transfer to delay pool # 1.

Any suggestions?

-- 
Saludos Cordiales

Lic. Alex Guti?rrez Mart?nez

Tel. +53 7 2710327





From synfinatic at gmail.com  Fri Sep 22 23:18:44 2017
From: synfinatic at gmail.com (Aaron Turner)
Date: Fri, 22 Sep 2017 16:18:44 -0700
Subject: [squid-users] Bug: Missing MemObject::storeId value
Message-ID: <CANAZdzXddzer-Bqf4jU0J2GbdRoixp8BSmoKvBFD-30kt9aWcw@mail.gmail.com>

Version: 3.5.26 on CentOS 7.3 on AWS EC2 m3.xlarge and 2x 100GB EBS
volumes for rock cache.

Doing some basic system tests and we're seeing a bunch of errors like:

2017/09/22 22:43:15 kid1| Bug: Missing MemObject::storeId value
2017/09/22 22:43:15 kid1| mem_hdr: 0x7f169d0a2a70 nodes.start() 0x7f169c6cc9d0
2017/09/22 22:43:15 kid1| mem_hdr: 0x7f169d0a2a70 nodes.finish() 0x7f169dae4e40
2017/09/22 22:43:15 kid1| MemObject->start_ping: 0.000000
2017/09/22 22:43:15 kid1| MemObject->inmem_hi: 20209
2017/09/22 22:43:15 kid1| MemObject->inmem_lo: 0
2017/09/22 22:43:15 kid1| MemObject->nclients: 0
2017/09/22 22:43:15 kid1| MemObject->reply: 0x7f167ee60db0
2017/09/22 22:43:15 kid1| MemObject->request: 0
2017/09/22 22:43:15 kid1| MemObject->logUri:
2017/09/22 22:43:15 kid1| MemObject->storeId:
2017/09/22 22:43:15 kid1| Bug: Missing MemObject::storeId value
2017/09/22 22:43:15 kid1| mem_hdr: 0x7f16a0388760 nodes.start() 0x7f16a6a4a500
2017/09/22 22:43:15 kid1| mem_hdr: 0x7f16a0388760 nodes.finish() 0x7f16a6a4a4d0
2017/09/22 22:43:15 kid1| MemObject->start_ping: 0.000000
2017/09/22 22:43:15 kid1| MemObject->inmem_hi: 50265
2017/09/22 22:43:15 kid1| MemObject->inmem_lo: 0
2017/09/22 22:43:15 kid1| MemObject->nclients: 0
2017/09/22 22:43:15 kid1| MemObject->reply: 0x7f169f83d7d0
2017/09/22 22:43:15 kid1| MemObject->request: 0
2017/09/22 22:43:15 kid1| MemObject->logUri:
2017/09/22 22:43:15 kid1| MemObject->storeId:

I did some googling and seems like a lot of comments about this with
Rock (we're using) and ICP/HTCP (not using).  Curious if this the same
bug or something new?  Are there config changes we can make to prevent
this (perhaps switching away from rock cache??)

We have a bunch of clients behind haproxy which is load balancing to
4x Squid.  Config of the squids is as:

http_access allow localhost manager
http_access deny manager

external_acl_type client_ip_map_0 %>ha{Our-Client}
/usr/lib64/squid/user_loadbalance.py 0 4
external_acl_type client_ip_map_1 %>ha{Our-Client}
/usr/lib64/squid/user_loadbalance.py 1 4
external_acl_type client_ip_map_2 %>ha{Our-Client}
/usr/lib64/squid/user_loadbalance.py 2 4
external_acl_type client_ip_map_3 %>ha{Our-Client}
/usr/lib64/squid/user_loadbalance.py 3 4

acl client_group_0 external client_ip_map_0
acl client_group_1 external client_ip_map_1
acl client_group_2 external client_ip_map_2
acl client_group_3 external client_ip_map_3

http_access allow client_group_0
http_access allow client_group_1
http_access allow client_group_2
http_access allow client_group_3
http_access deny all

tcp_outgoing_address 10.93.2.41 client_group_0
tcp_outgoing_address 10.93.2.76 client_group_1
tcp_outgoing_address 10.93.2.198 client_group_2
tcp_outgoing_address 10.93.3.178 client_group_3

cache_dir rock /var/lib/squid/cache1 51200
cache_dir rock /var/lib/squid/cache2 51200
coredump_dir /var/spool/squid
maximum_object_size_in_memory 8 MB
maximum_object_size 8 MB

cache_mem 6 GB
memory_cache_shared on
workers 4

refresh_pattern . 0 100% 30

http_port squid0001:3128 ssl-bump generate-host-certificates=on
dynamic_cert_mem_cache_size=400MB cert=/etc/squid/ssl_cert/myCA.pem
http_port localhost:3128
ssl_bump bump all

request_header_access Our-Client deny all
request_header_access Via deny all
forwarded_for delete

visible_hostname squid0001.lab.company.com
logformat adttest %tg %6tr %>a %Ss/%03>Hs %<st %rm %>ru %[un %Sh/%<a %mt %ea
access_log daemon:/var/log/squid/access.${process_number}.log adttest
icon_directory /usr/share/squid/icons

sslcrtd_program /usr/lib64/squid/ssl_crtd -s /var/lib/squid/ssl_db -M 4MB
sslcrtd_children 32 startup=2 idle=2
sslproxy_session_cache_size 100 MB
sslproxy_cert_error allow all
sslproxy_flags DONT_VERIFY_PEER


--
Aaron Turner
https://synfin.net/         Twitter: @synfinatic
My father once told me that respect for the truth comes close to being
the basis for all morality.  "Something cannot emerge from nothing,"
he said.  This is profound thinking if you understand how unstable
"the truth" can be.  -- Frank Herbert, Dune


From p.schaefer at creapptive.de  Sat Sep 23 16:05:41 2017
From: p.schaefer at creapptive.de (=?UTF-8?Q?Pascal_Sch=c3=a4fer?=)
Date: Sat, 23 Sep 2017 18:05:41 +0200
Subject: [squid-users] Squid radius Authentication
In-Reply-To: <660c72f0-4806-8f02-7443-d735ea15fcbd@treenet.co.nz>
References: <add1e454-7e0a-250b-6ae3-f99cb6923ce5@creapptive.de>
 <803561cf-b8f8-f265-4b46-5335019d1380@treenet.co.nz>
 <336c2b18-16a7-efb4-35ba-8979ba0442c2@creapptive.de>
 <660c72f0-4806-8f02-7443-d735ea15fcbd@treenet.co.nz>
Message-ID: <a6cb1d14-af0c-af4a-2474-fd5697c93a8f@creapptive.de>

Dear Amos,

I have another question to the key_extras for auth_param basic key_extras.
It is possible to give the helper more than one key_extras argument?
Maybe like this:

auth_param basic key_extras %macro
auth_param basic key_extras $macro

or

auth_param basic key_extras %macro %macro


And I tried some key_extras but the only usefull key_extras was %rp,
where I get /site/ from the URL: https://subdomain.domain.com/site/.
And when I try to use %rq my squid tell me an Error that he can't parse
the config file.
I wish I could get the whole URL from the squid.

Maybe do you know why that happens?
Or it isn't it the right key_extras %macro?
The most of the other %macros gives me the "-", which means that the
information is not available in this moment, where the authentication
helper get the username and password.

My squid version is squid 3.5.23-5 compiled from the sources of a debian
distribution (apt-get sources ... ).
I used these references for squid:

http://www.squid-cache.org/Doc/config/auth_param/
http://devel.squid-cache.org/customlog/logformat.html

I hope you can help me.

with best regards,

Pascal

Am 15.09.2017 um 17:26 schrieb Amos Jeffries:
> On 16/09/17 02:31, Pascal Sch?fer wrote:
>> Dear Amos,
>>
>> Thank you for your reply!
>>
>>>>
>>>> I have a question about the authentication with a radius server.
>>>> I use Squid as a reverse proxy.
>>>> It is possible to use two radius server for different pages or
>>>> subdomains with squid_radius_auth?
>>>
>>> HTTP has no concept of "page" - so for that; no.
>>>
>>> For sub-domains (OR specific URLs); maybe. Because the helper you are
>>> asking about does not use the key_extras feature provided by latest
>>> Squid version
>>
>> Ok. Thank you. Exist another helper who did an authentication with a
>> radius server?
>>
> 
> I am aware of some proprietary ones existing. But that is not useful for
> you.
> 
>>>
>>> You need to write your own helper that does what you want. That could be
>>> in the form of a wrapper script that starts multiple radius helper with
>>> the necessary parameters, and uses key_extra parameters to decide which
>>> one will handle any given auth lookup.
>>
>> Is this https://wiki.squid-cache.org/Features/AddonHelpers#Authenticator
>> the right wiki, where I have to lookup?
> 
> That page describes the protocol Squid will be talking to your script
> with; and what is expected to arrive back.
> 
>> Make it sense that behind the radius server is a Windows NPS Server to
>> authenticate the Users?
> 
> That does not matter unless you are writing the RADIUS parts yourself.
> In which case I cannot help, not knowing much about RADIUS protocol.
> 
> 
>> So when I write the wrapper helper, I only need to decide which helper I
>> would like to start and with which parameters, like a Bash command?
>>
> 
> Yes. Though helpers are required to run until Squid stops them. So best
> to start the child radius helpers at the beginning then just relay query
> and response lines appropriately when they arrive.
> 
> 
>>>
>>> Since you are calling it the long obsolete name "squid_radius_auth", you
>>> probably do not have a current Squid version which supplies the
>>> key_extras feature. At the very least you will have to upgrade to at
>>> least Squid-3.5.
>>
>> I have a Squid-3.5, self compiled.
>> I think about to upgrade there on Squid-4 or to compile it and install
>> them fresh on the system. Is the name of them another in the newer
>> versions?
> 
> Then you should be fine, except "basic_radius_auth" is the helper binary
> name since Squid-3.2.
> 
> 
> Amos
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users


From rousskov at measurement-factory.com  Mon Sep 25 14:11:57 2017
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Mon, 25 Sep 2017 08:11:57 -0600
Subject: [squid-users] Peek and Splice - Termination Log
In-Reply-To: <4626D608-CC19-486E-BA71-259A55FCEEE3@daveramsey.com>
References: <4626D608-CC19-486E-BA71-259A55FCEEE3@daveramsey.com>
Message-ID: <09695229-d572-d775-bbe0-0c23263bb6d5@measurement-factory.com>

On 09/22/2017 09:27 AM, Eric Lackey wrote:

> This is all working well except for the fact that we don?t have a
> good way to determine what is being blocked.

All transactions, including blocked ones, must be logged to access.log.
Squid had several bugs in this area. All known bugs (within this
discussion scope) should be fixed in the latest v5. I am not sure about
the latest v3, but I do see at least some of the fixes in v4. For
example:
https://github.com/squid-cache/squid/commit/da6dbcd110f7603f6d4cd9b3eef749311293fe77

Going forward:

* If something is not logged in the latest v3, then please consider
upgrading to v4. Filing a bug report in Bugzilla (see below) for v3
might motivate somebody to backport the fixes, but if the bug is fixed
in v4, then upgrading may be an overall better option, especially if you
use SslBump.

* If something is not logged in the latest v4 or v5, then please
consider filing a bug report in Bugzilla. Attaching an ALL,9 cache.log
while reproducing the issue using a single transaction on an otherwise
idle Squid will help developers triage your bug report.


Thank you,

Alex.
P.S. You do not need the "step3" ACL in the configuration below.

> acl allowed_https_sites ssl::server_name_regex "/etc/squid/allowed_sites"
> 
> ssl_bump peek all
> ssl_bump splice allowed_https_sites
> ssl_bump terminate step3 all


From heiler.bemerguy at cinbesa.com.br  Mon Sep 25 16:21:26 2017
From: heiler.bemerguy at cinbesa.com.br (Heiler Bemerguy)
Date: Mon, 25 Sep 2017 13:21:26 -0300
Subject: [squid-users] Bug: Missing MemObject::storeId value
In-Reply-To: <CANAZdzXddzer-Bqf4jU0J2GbdRoixp8BSmoKvBFD-30kt9aWcw@mail.gmail.com>
References: <CANAZdzXddzer-Bqf4jU0J2GbdRoixp8BSmoKvBFD-30kt9aWcw@mail.gmail.com>
Message-ID: <84c68989-0d1a-cdcf-c3eb-3488291fbf17@cinbesa.com.br>


I have this since forever.. 3.5.27 with one cache_peer and 4 rockstores

2017/09/21 11:19:45 kid1| Bug: Missing MemObject::storeId value
2017/09/21 11:19:45 kid1| mem_hdr: 0x1902d240 nodes.start() 0x552baa0
2017/09/21 11:19:45 kid1| mem_hdr: 0x1902d240 nodes.finish() 0x552baa0
2017/09/21 11:19:45 kid1| MemObject->start_ping: 0.000000
2017/09/21 11:19:45 kid1| MemObject->inmem_hi: 3335
2017/09/21 11:19:45 kid1| MemObject->inmem_lo: 0
2017/09/21 11:19:45 kid1| MemObject->nclients: 0
2017/09/21 11:19:45 kid1| MemObject->reply: 0xae4da80
2017/09/21 11:19:45 kid1| MemObject->request: 0
2017/09/21 11:19:45 kid1| MemObject->logUri:
2017/09/21 11:19:45 kid1| MemObject->storeId:

2017/09/21 11:19:46 kid1| Bug: Missing MemObject::storeId value
2017/09/21 11:19:46 kid1| mem_hdr: 0x6ce75d0 nodes.start() 0x54585b0
2017/09/21 11:19:46 kid1| mem_hdr: 0x6ce75d0 nodes.finish() 0xb237550
2017/09/21 11:19:46 kid1| MemObject->start_ping: 0.000000
2017/09/21 11:19:46 kid1| MemObject->inmem_hi: 14892
2017/09/21 11:19:46 kid1| MemObject->inmem_lo: 0
2017/09/21 11:19:46 kid1| MemObject->nclients: 0
2017/09/21 11:19:46 kid1| MemObject->reply: 0x9f08d10
2017/09/21 11:19:46 kid1| MemObject->request: 0
2017/09/21 11:19:46 kid1| MemObject->logUri:
2017/09/21 11:19:46 kid1| MemObject->storeId:


Em 22/09/2017 20:18, Aaron Turner escreveu:
> Version: 3.5.26 on CentOS 7.3 on AWS EC2 m3.xlarge and 2x 100GB EBS
> volumes for rock cache.
>
> Doing some basic system tests and we're seeing a bunch of errors like:
>
> 2017/09/22 22:43:15 kid1| Bug: Missing MemObject::storeId value
> 2017/09/22 22:43:15 kid1| mem_hdr: 0x7f169d0a2a70 nodes.start() 0x7f169c6cc9d0
> 2017/09/22 22:43:15 kid1| mem_hdr: 0x7f169d0a2a70 nodes.finish() 0x7f169dae4e40
> 2017/09/22 22:43:15 kid1| MemObject->start_ping: 0.000000
> 2017/09/22 22:43:15 kid1| MemObject->inmem_hi: 20209
> 2017/09/22 22:43:15 kid1| MemObject->inmem_lo: 0
> 2017/09/22 22:43:15 kid1| MemObject->nclients: 0
> 2017/09/22 22:43:15 kid1| MemObject->reply: 0x7f167ee60db0
> 2017/09/22 22:43:15 kid1| MemObject->request: 0
> 2017/09/22 22:43:15 kid1| MemObject->logUri:
> 2017/09/22 22:43:15 kid1| MemObject->storeId:
> 2017/09/22 22:43:15 kid1| Bug: Missing MemObject::storeId value
> 2017/09/22 22:43:15 kid1| mem_hdr: 0x7f16a0388760 nodes.start() 0x7f16a6a4a500
> 2017/09/22 22:43:15 kid1| mem_hdr: 0x7f16a0388760 nodes.finish() 0x7f16a6a4a4d0
> 2017/09/22 22:43:15 kid1| MemObject->start_ping: 0.000000
> 2017/09/22 22:43:15 kid1| MemObject->inmem_hi: 50265
> 2017/09/22 22:43:15 kid1| MemObject->inmem_lo: 0
> 2017/09/22 22:43:15 kid1| MemObject->nclients: 0
> 2017/09/22 22:43:15 kid1| MemObject->reply: 0x7f169f83d7d0
> 2017/09/22 22:43:15 kid1| MemObject->request: 0
> 2017/09/22 22:43:15 kid1| MemObject->logUri:
> 2017/09/22 22:43:15 kid1| MemObject->storeId:
>
> I did some googling and seems like a lot of comments about this with
> Rock (we're using) and ICP/HTCP (not using).  Curious if this the same
> bug or something new?  Are there config changes we can make to prevent
> this (perhaps switching away from rock cache??)
>
> We have a bunch of clients behind haproxy which is load balancing to
> 4x Squid.  Config of the squids is as:
>
> http_access allow localhost manager
> http_access deny manager
>
> external_acl_type client_ip_map_0 %>ha{Our-Client}
> /usr/lib64/squid/user_loadbalance.py 0 4
> external_acl_type client_ip_map_1 %>ha{Our-Client}
> /usr/lib64/squid/user_loadbalance.py 1 4
> external_acl_type client_ip_map_2 %>ha{Our-Client}
> /usr/lib64/squid/user_loadbalance.py 2 4
> external_acl_type client_ip_map_3 %>ha{Our-Client}
> /usr/lib64/squid/user_loadbalance.py 3 4
>
> acl client_group_0 external client_ip_map_0
> acl client_group_1 external client_ip_map_1
> acl client_group_2 external client_ip_map_2
> acl client_group_3 external client_ip_map_3
>
> http_access allow client_group_0
> http_access allow client_group_1
> http_access allow client_group_2
> http_access allow client_group_3
> http_access deny all
>
> tcp_outgoing_address 10.93.2.41 client_group_0
> tcp_outgoing_address 10.93.2.76 client_group_1
> tcp_outgoing_address 10.93.2.198 client_group_2
> tcp_outgoing_address 10.93.3.178 client_group_3
>
> cache_dir rock /var/lib/squid/cache1 51200
> cache_dir rock /var/lib/squid/cache2 51200
> coredump_dir /var/spool/squid
> maximum_object_size_in_memory 8 MB
> maximum_object_size 8 MB
>
> cache_mem 6 GB
> memory_cache_shared on
> workers 4
>
> refresh_pattern . 0 100% 30
>
> http_port squid0001:3128 ssl-bump generate-host-certificates=on
> dynamic_cert_mem_cache_size=400MB cert=/etc/squid/ssl_cert/myCA.pem
> http_port localhost:3128
> ssl_bump bump all
>
> request_header_access Our-Client deny all
> request_header_access Via deny all
> forwarded_for delete
>
> visible_hostname squid0001.lab.company.com
> logformat adttest %tg %6tr %>a %Ss/%03>Hs %<st %rm %>ru %[un %Sh/%<a %mt %ea
> access_log daemon:/var/log/squid/access.${process_number}.log adttest
> icon_directory /usr/share/squid/icons
>
> sslcrtd_program /usr/lib64/squid/ssl_crtd -s /var/lib/squid/ssl_db -M 4MB
> sslcrtd_children 32 startup=2 idle=2
> sslproxy_session_cache_size 100 MB
> sslproxy_cert_error allow all
> sslproxy_flags DONT_VERIFY_PEER
>
>
> --
> Aaron Turner
> https://synfin.net/         Twitter: @synfinatic
> My father once told me that respect for the truth comes close to being
> the basis for all morality.  "Something cannot emerge from nothing,"
> he said.  This is profound thinking if you understand how unstable
> "the truth" can be.  -- Frank Herbert, Dune
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users

-- 
Atenciosamente / Best Regards,

Heiler Bemerguy
Network Manager - CINBESA
55 91 98151-4894/3184-1751



From ppmartell at unah.edu.cu  Mon Sep 25 18:42:01 2017
From: ppmartell at unah.edu.cu (ppmartell at unah.edu.cu)
Date: Mon, 25 Sep 2017 14:42:01 -0400 (CDT)
Subject: [squid-users] acl url_regex on squid3 is not working using an
 online tested regular expression
In-Reply-To: <654311427.570222.1506364850126.JavaMail.root@unah.edu.cu>
Message-ID: <877952927.570355.1506364921562.JavaMail.root@unah.edu.cu>

I was asked to block Facebook access from 8:00am to 3:00pm for almost all users but them are using **alternative Facebook URLs** to access the social network anyway. This is consuming a lot of our low bandwidth and we can't even work. I decided to design a **regular expression (regex) to parse these URLs and block them**. I don't want to block all facebook URLs but only alternatives. An alternative Facebook URLs mostly contains the words **prod** or **iphone**. The next ones are alternative Facebook URLs registered by our proxy server: 

m.iphone.touch.prod.facebook.com 
m.iphone.haid.prod.facebook.com:443 
m.ct.prod.facebook.com 
m.vi-vn.prod.facebook.com 

The designed regex: `/((?=.*\biphone\b)|(?=.*\bprod\b)).*\.facebook\.com(\:|\d|)/` 

I tested this regex on https://regex101.com/ and https://www.regextester.com. The regex is **matching** for: 

m.iphone.touch.prod.facebook.com 
m.iphone.haid.prod.facebook.com:443 
m.ct.prod.facebook.com 
m.vi-vn.prod.facebook.com 

And is **not matching** for: 

www.facebook.com 
m.facebook.com 
mqtt.facebook.com (for purple-facebook) 
graph.facebook.com 
connect.facebook.com 
3-edge-chat.facebook.com 

So far this is what I wanted, alternative URLs blocked and regular Facebook URLs allowed. **My regex looks good to be used in squid**. 

Next step is to modify the file /etc/squid3/squid.conf by adding a new acl pointing the file that contains the regex: 

acl facebook dstdom_regex "/etc/squid3/acl/facebook" //The file contains the regex 
http_access deny pass facebook 

When I run **squid3 -k parse** for check the configuration file I am getting the errors: 

2017/09/22 11:12:26| Processing: acl facebook dstdom_regex "/etc/squid3/acl/facebook" 
2017/09/22 11:12:26| squid.conf line 78: acl facebook dstdom_regex "/etc/squid3/acl/facebook" 
2017/09/22 11:12:26| aclParseRegexList: Invalid regular expression '((?=.*\biphone\b)|(?=.*\bprod\b)).*\.facebook\.com(\:|\d|)': Invalid preceding regular expression 
2017/09/22 12:39:33| Warning: empty ACL: acl facebook dstdom_regex "/etc/squid3/acl/facebook" 

Obviously, the squid3 parser is tagging my acl as **wrong**, but I already tested online and it was good to use. Also it says the acl is empty. What does this mean? The acl was declared with the name **facebook**. I am very confused at this. 

-- 
Ing. Pedro Pablo Delgado Martell 

Participe en el Congreso Internacional de las Ciencias Agropecuarias (AGROCIENCIAS 2017) http://www.agrocienciascuba.com/
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20170925/32790062/attachment.htm>

From eliezer at ngtech.co.il  Mon Sep 25 18:45:48 2017
From: eliezer at ngtech.co.il (Eliezer Croitoru)
Date: Mon, 25 Sep 2017 21:45:48 +0300
Subject: [squid-users] Bug: Missing MemObject::storeId value
In-Reply-To: <CANAZdzXddzer-Bqf4jU0J2GbdRoixp8BSmoKvBFD-30kt9aWcw@mail.gmail.com>
References: <CANAZdzXddzer-Bqf4jU0J2GbdRoixp8BSmoKvBFD-30kt9aWcw@mail.gmail.com>
Message-ID: <0ccf01d3362e$8141e050$83c5a0f0$@ngtech.co.il>

Hey Aaron,

Just to clear out the doubt's, what happen when you use squid-cache without rock cache_dir? Is the problem appearing again?
Also, there is a possibility of a bug which is related to squid ssl-bump termination code on 3.5.X.
Testing 4.0.21 would be the best to understand if the issue is 3.5 local or if it was fixed in 4.X+ but, from my memory I think you will need to adapt your squid.conf ssl_bump configurations.
You can get the latest beta and stable binaries from my repo and the beta repo details are at:
https://wiki.squid-cache.org/action/edit/KnowledgeBase/CentOS#Squid_Beta_release

Also, since you are using haproxy in front of squid I would suggest you to use the proxy protocol(v1) which is the best way to pass the source ip addresses to the proxy.
I have tested squid to work with the proxy protocol v1 but yet to test v2.

All The Bests,
Eliezer

----
Eliezer Croitoru
Linux System Administrator
Mobile: +972-5-28704261
Email: eliezer at ngtech.co.il



-----Original Message-----
From: squid-users [mailto:squid-users-bounces at lists.squid-cache.org] On Behalf Of Aaron Turner
Sent: Saturday, September 23, 2017 02:19
To: squid-users at lists.squid-cache.org
Subject: [squid-users] Bug: Missing MemObject::storeId value

Version: 3.5.26 on CentOS 7.3 on AWS EC2 m3.xlarge and 2x 100GB EBS
volumes for rock cache.

Doing some basic system tests and we're seeing a bunch of errors like:

2017/09/22 22:43:15 kid1| Bug: Missing MemObject::storeId value
2017/09/22 22:43:15 kid1| mem_hdr: 0x7f169d0a2a70 nodes.start() 0x7f169c6cc9d0
2017/09/22 22:43:15 kid1| mem_hdr: 0x7f169d0a2a70 nodes.finish() 0x7f169dae4e40
2017/09/22 22:43:15 kid1| MemObject->start_ping: 0.000000
2017/09/22 22:43:15 kid1| MemObject->inmem_hi: 20209
2017/09/22 22:43:15 kid1| MemObject->inmem_lo: 0
2017/09/22 22:43:15 kid1| MemObject->nclients: 0
2017/09/22 22:43:15 kid1| MemObject->reply: 0x7f167ee60db0
2017/09/22 22:43:15 kid1| MemObject->request: 0
2017/09/22 22:43:15 kid1| MemObject->logUri:
2017/09/22 22:43:15 kid1| MemObject->storeId:
2017/09/22 22:43:15 kid1| Bug: Missing MemObject::storeId value
2017/09/22 22:43:15 kid1| mem_hdr: 0x7f16a0388760 nodes.start() 0x7f16a6a4a500
2017/09/22 22:43:15 kid1| mem_hdr: 0x7f16a0388760 nodes.finish() 0x7f16a6a4a4d0
2017/09/22 22:43:15 kid1| MemObject->start_ping: 0.000000
2017/09/22 22:43:15 kid1| MemObject->inmem_hi: 50265
2017/09/22 22:43:15 kid1| MemObject->inmem_lo: 0
2017/09/22 22:43:15 kid1| MemObject->nclients: 0
2017/09/22 22:43:15 kid1| MemObject->reply: 0x7f169f83d7d0
2017/09/22 22:43:15 kid1| MemObject->request: 0
2017/09/22 22:43:15 kid1| MemObject->logUri:
2017/09/22 22:43:15 kid1| MemObject->storeId:

I did some googling and seems like a lot of comments about this with
Rock (we're using) and ICP/HTCP (not using).  Curious if this the same
bug or something new?  Are there config changes we can make to prevent
this (perhaps switching away from rock cache??)

We have a bunch of clients behind haproxy which is load balancing to
4x Squid.  Config of the squids is as:

http_access allow localhost manager
http_access deny manager

external_acl_type client_ip_map_0 %>ha{Our-Client}
/usr/lib64/squid/user_loadbalance.py 0 4
external_acl_type client_ip_map_1 %>ha{Our-Client}
/usr/lib64/squid/user_loadbalance.py 1 4
external_acl_type client_ip_map_2 %>ha{Our-Client}
/usr/lib64/squid/user_loadbalance.py 2 4
external_acl_type client_ip_map_3 %>ha{Our-Client}
/usr/lib64/squid/user_loadbalance.py 3 4

acl client_group_0 external client_ip_map_0
acl client_group_1 external client_ip_map_1
acl client_group_2 external client_ip_map_2
acl client_group_3 external client_ip_map_3

http_access allow client_group_0
http_access allow client_group_1
http_access allow client_group_2
http_access allow client_group_3
http_access deny all

tcp_outgoing_address 10.93.2.41 client_group_0
tcp_outgoing_address 10.93.2.76 client_group_1
tcp_outgoing_address 10.93.2.198 client_group_2
tcp_outgoing_address 10.93.3.178 client_group_3

cache_dir rock /var/lib/squid/cache1 51200
cache_dir rock /var/lib/squid/cache2 51200
coredump_dir /var/spool/squid
maximum_object_size_in_memory 8 MB
maximum_object_size 8 MB

cache_mem 6 GB
memory_cache_shared on
workers 4

refresh_pattern . 0 100% 30

http_port squid0001:3128 ssl-bump generate-host-certificates=on
dynamic_cert_mem_cache_size=400MB cert=/etc/squid/ssl_cert/myCA.pem
http_port localhost:3128
ssl_bump bump all

request_header_access Our-Client deny all
request_header_access Via deny all
forwarded_for delete

visible_hostname squid0001.lab.company.com
logformat adttest %tg %6tr %>a %Ss/%03>Hs %<st %rm %>ru %[un %Sh/%<a %mt %ea
access_log daemon:/var/log/squid/access.${process_number}.log adttest
icon_directory /usr/share/squid/icons

sslcrtd_program /usr/lib64/squid/ssl_crtd -s /var/lib/squid/ssl_db -M 4MB
sslcrtd_children 32 startup=2 idle=2
sslproxy_session_cache_size 100 MB
sslproxy_cert_error allow all
sslproxy_flags DONT_VERIFY_PEER


--
Aaron Turner
https://synfin.net/         Twitter: @synfinatic
My father once told me that respect for the truth comes close to being
the basis for all morality.  "Something cannot emerge from nothing,"
he said.  This is profound thinking if you understand how unstable
"the truth" can be.  -- Frank Herbert, Dune
_______________________________________________
squid-users mailing list
squid-users at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-users



From synfinatic at gmail.com  Mon Sep 25 19:56:56 2017
From: synfinatic at gmail.com (Aaron Turner)
Date: Mon, 25 Sep 2017 12:56:56 -0700
Subject: [squid-users] Bug: Missing MemObject::storeId value
In-Reply-To: <0ccf01d3362e$8141e050$83c5a0f0$@ngtech.co.il>
References: <CANAZdzXddzer-Bqf4jU0J2GbdRoixp8BSmoKvBFD-30kt9aWcw@mail.gmail.com>
 <0ccf01d3362e$8141e050$83c5a0f0$@ngtech.co.il>
Message-ID: <CANAZdzVHj_hbmv=8ChDUSH_-UOHFPNc3VUurmiVROeJOMAtwYg@mail.gmail.com>

So is v4 stable?  I was the impression it was beta?  That said, if v4
has better memory tuning options then I'm all ears.  Right now I'm
fighting OOM errors (and the kernel OOM reaper) under sustained load.
I've come to realize 6GB is way way too much for my 14GB RAM systems,
but finding even 1GB is too much since each squid process is exceeding
4GB.  About to try 500MB now.

I can disable rock cache, but I need some disk cache- is there a better option?

As for haproxy, I actually don't care about the client IP... I'm
running haproxy locally on the servers where the clients reside.
Mostly I'm using it for squid failover and cache affinity so I don't
have to make all my caches peers of each other.


--
Aaron Turner
https://synfin.net/         Twitter: @synfinatic
My father once told me that respect for the truth comes close to being
the basis for all morality.  "Something cannot emerge from nothing,"
he said.  This is profound thinking if you understand how unstable
"the truth" can be.  -- Frank Herbert, Dune


On Mon, Sep 25, 2017 at 11:45 AM, Eliezer Croitoru <eliezer at ngtech.co.il> wrote:
> Hey Aaron,
>
> Just to clear out the doubt's, what happen when you use squid-cache without rock cache_dir? Is the problem appearing again?
> Also, there is a possibility of a bug which is related to squid ssl-bump termination code on 3.5.X.
> Testing 4.0.21 would be the best to understand if the issue is 3.5 local or if it was fixed in 4.X+ but, from my memory I think you will need to adapt your squid.conf ssl_bump configurations.
> You can get the latest beta and stable binaries from my repo and the beta repo details are at:
> https://wiki.squid-cache.org/action/edit/KnowledgeBase/CentOS#Squid_Beta_release
>
> Also, since you are using haproxy in front of squid I would suggest you to use the proxy protocol(v1) which is the best way to pass the source ip addresses to the proxy.
> I have tested squid to work with the proxy protocol v1 but yet to test v2.
>
> All The Bests,
> Eliezer
>
> ----
> Eliezer Croitoru
> Linux System Administrator
> Mobile: +972-5-28704261
> Email: eliezer at ngtech.co.il
>
>
>
> -----Original Message-----
> From: squid-users [mailto:squid-users-bounces at lists.squid-cache.org] On Behalf Of Aaron Turner
> Sent: Saturday, September 23, 2017 02:19
> To: squid-users at lists.squid-cache.org
> Subject: [squid-users] Bug: Missing MemObject::storeId value
>
> Version: 3.5.26 on CentOS 7.3 on AWS EC2 m3.xlarge and 2x 100GB EBS
> volumes for rock cache.
>
> Doing some basic system tests and we're seeing a bunch of errors like:
>
> 2017/09/22 22:43:15 kid1| Bug: Missing MemObject::storeId value
> 2017/09/22 22:43:15 kid1| mem_hdr: 0x7f169d0a2a70 nodes.start() 0x7f169c6cc9d0
> 2017/09/22 22:43:15 kid1| mem_hdr: 0x7f169d0a2a70 nodes.finish() 0x7f169dae4e40
> 2017/09/22 22:43:15 kid1| MemObject->start_ping: 0.000000
> 2017/09/22 22:43:15 kid1| MemObject->inmem_hi: 20209
> 2017/09/22 22:43:15 kid1| MemObject->inmem_lo: 0
> 2017/09/22 22:43:15 kid1| MemObject->nclients: 0
> 2017/09/22 22:43:15 kid1| MemObject->reply: 0x7f167ee60db0
> 2017/09/22 22:43:15 kid1| MemObject->request: 0
> 2017/09/22 22:43:15 kid1| MemObject->logUri:
> 2017/09/22 22:43:15 kid1| MemObject->storeId:
> 2017/09/22 22:43:15 kid1| Bug: Missing MemObject::storeId value
> 2017/09/22 22:43:15 kid1| mem_hdr: 0x7f16a0388760 nodes.start() 0x7f16a6a4a500
> 2017/09/22 22:43:15 kid1| mem_hdr: 0x7f16a0388760 nodes.finish() 0x7f16a6a4a4d0
> 2017/09/22 22:43:15 kid1| MemObject->start_ping: 0.000000
> 2017/09/22 22:43:15 kid1| MemObject->inmem_hi: 50265
> 2017/09/22 22:43:15 kid1| MemObject->inmem_lo: 0
> 2017/09/22 22:43:15 kid1| MemObject->nclients: 0
> 2017/09/22 22:43:15 kid1| MemObject->reply: 0x7f169f83d7d0
> 2017/09/22 22:43:15 kid1| MemObject->request: 0
> 2017/09/22 22:43:15 kid1| MemObject->logUri:
> 2017/09/22 22:43:15 kid1| MemObject->storeId:
>
> I did some googling and seems like a lot of comments about this with
> Rock (we're using) and ICP/HTCP (not using).  Curious if this the same
> bug or something new?  Are there config changes we can make to prevent
> this (perhaps switching away from rock cache??)
>
> We have a bunch of clients behind haproxy which is load balancing to
> 4x Squid.  Config of the squids is as:
>
> http_access allow localhost manager
> http_access deny manager
>
> external_acl_type client_ip_map_0 %>ha{Our-Client}
> /usr/lib64/squid/user_loadbalance.py 0 4
> external_acl_type client_ip_map_1 %>ha{Our-Client}
> /usr/lib64/squid/user_loadbalance.py 1 4
> external_acl_type client_ip_map_2 %>ha{Our-Client}
> /usr/lib64/squid/user_loadbalance.py 2 4
> external_acl_type client_ip_map_3 %>ha{Our-Client}
> /usr/lib64/squid/user_loadbalance.py 3 4
>
> acl client_group_0 external client_ip_map_0
> acl client_group_1 external client_ip_map_1
> acl client_group_2 external client_ip_map_2
> acl client_group_3 external client_ip_map_3
>
> http_access allow client_group_0
> http_access allow client_group_1
> http_access allow client_group_2
> http_access allow client_group_3
> http_access deny all
>
> tcp_outgoing_address 10.93.2.41 client_group_0
> tcp_outgoing_address 10.93.2.76 client_group_1
> tcp_outgoing_address 10.93.2.198 client_group_2
> tcp_outgoing_address 10.93.3.178 client_group_3
>
> cache_dir rock /var/lib/squid/cache1 51200
> cache_dir rock /var/lib/squid/cache2 51200
> coredump_dir /var/spool/squid
> maximum_object_size_in_memory 8 MB
> maximum_object_size 8 MB
>
> cache_mem 6 GB
> memory_cache_shared on
> workers 4
>
> refresh_pattern . 0 100% 30
>
> http_port squid0001:3128 ssl-bump generate-host-certificates=on
> dynamic_cert_mem_cache_size=400MB cert=/etc/squid/ssl_cert/myCA.pem
> http_port localhost:3128
> ssl_bump bump all
>
> request_header_access Our-Client deny all
> request_header_access Via deny all
> forwarded_for delete
>
> visible_hostname squid0001.lab.company.com
> logformat adttest %tg %6tr %>a %Ss/%03>Hs %<st %rm %>ru %[un %Sh/%<a %mt %ea
> access_log daemon:/var/log/squid/access.${process_number}.log adttest
> icon_directory /usr/share/squid/icons
>
> sslcrtd_program /usr/lib64/squid/ssl_crtd -s /var/lib/squid/ssl_db -M 4MB
> sslcrtd_children 32 startup=2 idle=2
> sslproxy_session_cache_size 100 MB
> sslproxy_cert_error allow all
> sslproxy_flags DONT_VERIFY_PEER
>
>
> --
> Aaron Turner
> https://synfin.net/         Twitter: @synfinatic
> My father once told me that respect for the truth comes close to being
> the basis for all morality.  "Something cannot emerge from nothing,"
> he said.  This is profound thinking if you understand how unstable
> "the truth" can be.  -- Frank Herbert, Dune
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>


From rousskov at measurement-factory.com  Mon Sep 25 20:45:27 2017
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Mon, 25 Sep 2017 14:45:27 -0600
Subject: [squid-users] acl url_regex on squid3 is not working using an
 online tested regular expression
In-Reply-To: <877952927.570355.1506364921562.JavaMail.root@unah.edu.cu>
References: <877952927.570355.1506364921562.JavaMail.root@unah.edu.cu>
Message-ID: <fe5b3a14-c3e3-1a94-9764-0b612cfe57b3@measurement-factory.com>

On 09/25/2017 12:42 PM, ppmartell at unah.edu.cu wrote:
> The designed regex:
> /((?=.*\biphone\b)|(?=.*\bprod\b)).*\.facebook\.com(\:|\d|)/

AFAICT, for the basic purpose of matching strings, the above mind
boggling regular expression can be simplified to:

  /\b(iphone|prod)\b.*\.facebook\.com/

Please note that I am _not_ saying that the expression works correctly
for your use case. I am only saying that its true meaning is much
simpler than the original version looks.


> aclParseRegexList: Invalid regular expression
> '((?=.*\biphone\b)|(?=.*\bprod\b)).*\.facebook\.com(\:|\d|)': Invalid
> preceding regular expression*

FWIW, the simplified expression above does not use the (?=...)
assertions, so it should not produce the above error.


> *??? 2017/09/22 12:39:33| Warning: empty ACL: acl facebook dstdom_regex
> "/etc/squid3/acl/facebook"*

> Obviously, the squid3 parser is tagging my acl as **wrong**, but I
> already tested online and it was good to use.

You did not test with the regex library used by your Squid. Different
libraries have different capabilities. The advanced feature (i.e., a
positive lookahead assertion) that your regex is using is apparently not
supported by the library that your Squid is using. Fortunately, that
feature is completely unnecessary for your use case.


> Also it says the acl is empty. What does this mean?

It probably means that after removing bad regexes, there were no regexes
left on the "acl ..." line. You can ignore this warning until you have
no warnings about invalid regular expressions.


HTH,

Alex.


From synfinatic at gmail.com  Mon Sep 25 23:23:14 2017
From: synfinatic at gmail.com (Aaron Turner)
Date: Mon, 25 Sep 2017 16:23:14 -0700
Subject: [squid-users] tuning squid memory (aka avoiding the reaper)
Message-ID: <CANAZdzXS7jFH_n44BgdYALzE_r3D7t78HRamz9qdKsbtfOVyqg@mail.gmail.com>

So I'm testing squid 3.5.26 on an m3.xlarge w/ 14GB of RAM.  Squid is
the only "real" service running (sshd and the like).  I'm running 4
workers, and 2 rock cache.  The workers seem to be growing unbounded
and given ~30min or so will cause the kernel to start killing off
processes until memory is freed.  Yes, my clients (32 of them) are
hitting this at about 250 URL's/min which doesn't seem that crazy, but
?\_(?)_/?

cache_mem 1 GB resulted in workers exceeding 4GB resident.  So I tried
500 MB, same problem.  Now I'm down to 250 MB and I'm still seeing
workers using 3-4GB of RAM after a few minutes and still growing which
is surprising since the docs indicate I should expect total memory to
be roughly 3x cache_mem.

mgr:info reports:

Squid Object Cache: Version 3.5.26
Build Info:
Service Name: squid
Start Time: Mon, 25 Sep 2017 22:53:22 GMT
Current Time: Mon, 25 Sep 2017 23:15:21 GMT
Connection information for squid:
Number of clients accessing cache: 12
Number of HTTP requests received: 568290
Number of ICP messages received: 0
Number of ICP messages sent: 0
Number of queued ICP replies: 0
Number of HTCP messages received: 0
Number of HTCP messages sent: 0
Request failure ratio: 0.00
Average HTTP requests per minute since start: 25851.6
Average ICP messages per minute since start: 0.0
Select loop called: 10686802 times, 2.009 ms avg
Cache information for squid:
Hits as % of all requests: 5min: 7.5%, 60min: 9.8%
Hits as % of bytes sent: 5min: 12.3%, 60min: 17.3%
Memory hits as % of hit requests: 5min: 36.2%, 60min: 40.0%
Disk hits as % of hit requests: 5min: 27.7%, 60min: 25.5%
Storage Swap size: 4481632 KB
Storage Swap capacity: 4.3% used, 95.7% free
Storage Mem size: 254656 KB
Storage Mem capacity: 99.5% used,  0.5% free
Mean Object Size: 26.30 KB
Requests given to unlinkd: 0
Median Service Times (seconds)  5 min    60 min:
HTTP Requests (All):   0.00800  0.00569
Cache Misses:          0.03766  0.04489
Cache Hits:            0.00030  0.00000
Near Hits:             0.05364  0.07135
Not-Modified Replies:  0.00236  0.00168
DNS Lookups:           0.04438  0.04540
ICP Queries:           0.00000  0.00000
Resource usage for squid:
UP Time: 1319.019 seconds
CPU Time: 1617.476 seconds
CPU Usage: 122.63%
CPU Usage, 5 minute avg: 107.50%
CPU Usage, 60 minute avg: 124.63%
Maximum Resident Size: 60715904 KB
Page faults with physical i/o: 8
Memory accounted for:
Total accounted:        44968 KB
memPoolAlloc calls: 158302074
memPoolFree calls:  159053228
File descriptor usage for squid:
Maximum number of file descriptors:   98304
Largest file desc currently in use:   1891
Number of file desc currently in use: 3423
Files queued for open:                   0
Available number of file descriptors: 94881
Reserved number of file descriptors:   600
Store Disk files open:                   8
Internal Data Structures:
 1702 StoreEntries
  454 StoreEntries with MemObjects
 4475 Hot Object Cache Items
170394 on-disk objects

top when things go bad:
  PID USER      PR  NI    VIRT    RES    SHR S  %CPU %MEM     TIME+ COMMAND
21450 squid     20   0 5379004 3.984g 619260 D  28.5 27.9   8:54.37
(squid-4) -f /etc/squid/squid.conf
21451 squid     20   0 5253164 3.850g 601248 D  26.6 27.0   7:46.71
(squid-3) -f /etc/squid/squid.conf
21453 squid     20   0 4750376 3.299g 504192 D  17.3 23.1   5:36.19
(squid-1) -f /etc/squid/squid.conf
21452 squid     20   0 4448280 2.994g 482136 D  16.6 21.0   5:43.58
(squid-2) -f /etc/squid/squid.conf
21449 squid     20   0 1292376 358544 346416 D   1.4  2.4   0:45.83
(squid-disk-5) -f /etc/squid/squid.conf
21448 squid     20   0 1292376 356924 344788 D   1.2  2.4   0:46.51
(squid-disk-6) -f /etc/squid/squid.conf
21447 squid     20   0  943584  11624     20 S   0.0  0.1   0:00.19
(squid-coord-7) -f /etc/squid/squid.conf


I'm trying to figure out why and how to fix.  One thing I've read
about the cache_mem knob is:

"If circumstances require, this limit will be exceeded.

Specifically, if your incoming request rate requires more than
'cache_mem' of memory to hold in-transit objects, Squid will
exceed this limit to satisfy the new requests.  When the load
decreases, blocks will be freed until the high-water mark is
reached.  Thereafter, blocks will be used to store hot
objects."

Not sure if this is the cause of my problem?  Maybe something else?
The FAQ says try a different malloc, so tried recompiling with
--enable-dlmalloc, but that had no impact.


--
Aaron Turner
https://synfin.net/         Twitter: @synfinatic
My father once told me that respect for the truth comes close to being
the basis for all morality.  "Something cannot emerge from nothing,"
he said.  This is profound thinking if you understand how unstable
"the truth" can be.  -- Frank Herbert, Dune


From squid3 at treenet.co.nz  Tue Sep 26 02:19:29 2017
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Tue, 26 Sep 2017 15:19:29 +1300
Subject: [squid-users] Bug: Missing MemObject::storeId value
In-Reply-To: <CANAZdzVHj_hbmv=8ChDUSH_-UOHFPNc3VUurmiVROeJOMAtwYg@mail.gmail.com>
References: <CANAZdzXddzer-Bqf4jU0J2GbdRoixp8BSmoKvBFD-30kt9aWcw@mail.gmail.com>
 <0ccf01d3362e$8141e050$83c5a0f0$@ngtech.co.il>
 <CANAZdzVHj_hbmv=8ChDUSH_-UOHFPNc3VUurmiVROeJOMAtwYg@mail.gmail.com>
Message-ID: <21a45d2b-758d-4d8c-bacd-cd0a8ecc92d6@treenet.co.nz>

On 26/09/17 08:56, Aaron Turner wrote:
> So is v4 stable?  I was the impression it was beta?  That said, if v4
> has better memory tuning options then I'm all ears.

Yes it is beta. Some bugs still to work out in the ssl-bump code, but 
that is all.

Overall the v4 ssl-bump code is far better behaved and capable than the 
3.5 series - so it is probably worth using despite the remaining bugs 
*if* your current issues are not showing up there.


First thing I would do though is adding sslflags=NO_DEFAULT_CA to the 
http_port line(s). It reduces the memory needs a lot when bumping in v3.5.


>  Right now I'm
> fighting OOM errors (and the kernel OOM reaper) under sustained load.
> I've come to realize 6GB is way way too much for my 14GB RAM systems,
> but finding even 1GB is too much since each squid process is exceeding
> 4GB.  About to try 500MB now.
> 
> I can disable rock cache, but I need some disk cache- is there a better option?

Possibly a smaller rock cache, and a UFS/AUFS/diskd cache - rock can 
share disk with another cache, its just the UFS/* caches that do not 
share well with each other.


Amos


From rousskov at measurement-factory.com  Tue Sep 26 03:26:21 2017
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Mon, 25 Sep 2017 21:26:21 -0600
Subject: [squid-users] tuning squid memory (aka avoiding the reaper)
In-Reply-To: <CANAZdzXS7jFH_n44BgdYALzE_r3D7t78HRamz9qdKsbtfOVyqg@mail.gmail.com>
References: <CANAZdzXS7jFH_n44BgdYALzE_r3D7t78HRamz9qdKsbtfOVyqg@mail.gmail.com>
Message-ID: <6fd4bb1b-8ec5-4883-2c57-8debb3890d9a@measurement-factory.com>

On 09/25/2017 05:23 PM, Aaron Turner wrote:
> So I'm testing squid 3.5.26 on an m3.xlarge w/ 14GB of RAM.  Squid is
> the only "real" service running (sshd and the like).  I'm running 4
> workers, and 2 rock cache.  The workers seem to be growing unbounded
> and given ~30min or so will cause the kernel to start killing off
> processes until memory is freed.  Yes, my clients (32 of them) are
> hitting this at about 250 URL's/min which doesn't seem that crazy, but
> ?\_(?)_/?
> 
> cache_mem 1 GB resulted in workers exceeding 4GB resident.  So I tried
> 500 MB, same problem.  Now I'm down to 250 MB and I'm still seeing
> workers using 3-4GB of RAM after a few minutes and still growing 

It is not the Squid memory cache that consumes your RAM, apparently.


> the docs indicate I should expect total memory to be roughly 3x cache_mem.

... which is an absurd formula for those using disk caches: Roughly
speaking, most large busy Squids spend most of their RAM on

* memory cache,
* disk cache indexes,
* SSL-related caches, and
* in-flight transactions.

Only one of those 4 components is proportional to cache_mem, with a
coefficient closer to 1 than to 3.


> mgr:info reports:

Thank you for posting this useful info. When you are using disk caching,
please also include the mgr:storedir report.


> I'm trying to figure out why and how to fix.

I recommend disabling all caching (memory and disk) and SslBump (if any)
to establish a baseline first. If everything looks stable and peachy for
a few hours, record/store the baseline measurements, and add one new
memory consumer (e.g., the memory cache). Ideally, this testing should
be done in a lab rather than on real users, but YMMV.


> One thing I've read about the cache_mem knob is:
> 
> "If circumstances require, this limit will be exceeded.
> 
> Specifically, if your incoming request rate requires more than
> 'cache_mem' of memory to hold in-transit objects, Squid will
> exceed this limit to satisfy the new requests.  When the load
> decreases, blocks will be freed until the high-water mark is
> reached.  Thereafter, blocks will be used to store hot
> objects."

The above is more-or-less accurate, but please note that in-transit
objects do not usually eat memory cache RAM in SMP mode. It is usually
best to think of in-flight transactions as a distinct SMP memory
consumer IMO.


> Not sure if this is the cause of my problem?

It could be -- it is difficult for me to say by looking at one random
mgr:info snapshot. If I have to guess based on that snapshot alone, then
my answer would be "no" because you have less than 4K concurrent
transactions and transaction response times are low. Hopefully somebody
else on the list can tell you more.



> The FAQ says try a different malloc, so tried recompiling with
> --enable-dlmalloc, but that had no impact.

Do not bother unless your deployment environment is very unusual. This
hint was helpful 20 years ago, but is rarely relevant these days AFAIK.
See above for a different attack plan.


HTH,

Alex.


From rousskov at measurement-factory.com  Tue Sep 26 03:36:18 2017
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Mon, 25 Sep 2017 21:36:18 -0600
Subject: [squid-users] Bug: Missing MemObject::storeId value
In-Reply-To: <21a45d2b-758d-4d8c-bacd-cd0a8ecc92d6@treenet.co.nz>
References: <CANAZdzXddzer-Bqf4jU0J2GbdRoixp8BSmoKvBFD-30kt9aWcw@mail.gmail.com>
 <0ccf01d3362e$8141e050$83c5a0f0$@ngtech.co.il>
 <CANAZdzVHj_hbmv=8ChDUSH_-UOHFPNc3VUurmiVROeJOMAtwYg@mail.gmail.com>
 <21a45d2b-758d-4d8c-bacd-cd0a8ecc92d6@treenet.co.nz>
Message-ID: <6c345cb6-fa7c-1a2d-d123-6401dcdb808d@measurement-factory.com>

On 09/25/2017 08:19 PM, Amos Jeffries wrote:
> On 26/09/17 08:56, Aaron Turner wrote:
>> I can disable rock cache, but I need some disk cache- is there a
>> better option?

> Possibly a smaller rock cache, and a UFS/AUFS/diskd cache - rock can
> share disk with another cache, its just the UFS/* caches that do not
> share well with each other.


In SMP mode, please:

* Do _not_ use rock cache with any other disk cache. The caching code is
not designed to mix SMP-unaware and SMP-aware caches in SMP mode.

* Avoid using non-rock disk caches in SMP mode because non-rock disk
caches are not SMP-aware. If you have to use SMP-unaware disk caches in
SMP mode, then confine each cache to one worker using SMP macros and do
not expect much help when things go wrong (e.g., Squid crashes and/or
HTTP violations). You should probably disable shared memory cache as
well in this case, for the reasons mentioned in the first bullet.


HTH,

Alex.


From eliezer at ngtech.co.il  Tue Sep 26 04:21:53 2017
From: eliezer at ngtech.co.il (Eliezer Croitoru)
Date: Tue, 26 Sep 2017 07:21:53 +0300
Subject: [squid-users] Bug: Missing MemObject::storeId value
In-Reply-To: <CANAZdzVHj_hbmv=8ChDUSH_-UOHFPNc3VUurmiVROeJOMAtwYg@mail.gmail.com>
References: <CANAZdzXddzer-Bqf4jU0J2GbdRoixp8BSmoKvBFD-30kt9aWcw@mail.gmail.com>
 <0ccf01d3362e$8141e050$83c5a0f0$@ngtech.co.il>
 <CANAZdzVHj_hbmv=8ChDUSH_-UOHFPNc3VUurmiVROeJOMAtwYg@mail.gmail.com>
Message-ID: <0dfa01d3367e$fb8a9960$f29fcc20$@ngtech.co.il>

Hey Aaron,

Consider the comments from Amos and Alex first before moving forward.
And again we need to clear out the current doubt's for both you and us.
We don't know if the issue is related to rock cache_dir or to squid-cache in general.
Currently for SMP aware caches the best disk cache is rock but you need to understand that the situation is that disk cache is a second level of caching and not the main goal.
You first need to make sure that squid works for you and then to make sure rock works good enough for you.
Also take into account that you actually "all in" for disk caching and it's not clear if you even need all this cache.
Before you decide that the disk cache is for you and that you really need it start low and aim higher, then in small steps move forward.
Start with a simple squid with ssl-bump without caching at all, then when you see it's stable enough from basic memory perspective for a period of 24 to 72 hours.
Then and only then when you see it's stable enough for you and the machine can take the load try to see if adding memory cache into the picture makes sense.
Check squid with it's default settings of cache_object sizes and try to analyze the cache logs to verify what are the most hot sites and objects that in use of your cache.
Only when you will have a clear view what is the demand from your cache proxy service you should consider moving forward to start investigating the usage of disk cache(with default cache object sizes).
Take into account that there is a possibility that squid will write object to the disk cache but will not use then and this is a very good reason to first test and analyze before going all in or out with squid.
Also start with a small disk cache(10GB max) and only after verifying that indeed the setup is working good enough try to find the right memory and disk cache utilization for your setup.

The above is my recommended recipe for a good and smooth start with squid in production environment.
You are not the first and probably not the last to receive this recommendation and I believe that some articles and resources that can be fetched from the Internet can miss-lead a Linux system administrator expectation from squid-cache or any cache.

Please test Squid-Cache one step at a time and do not get tempted to try to "cache all" since it's practically not possible.
Update us as you move forward with your tests.

Eliezer

----
Eliezer Croitoru
Linux System Administrator
Mobile: +972-5-28704261
Email: eliezer at ngtech.co.il



-----Original Message-----
From: Aaron Turner [mailto:synfinatic at gmail.com] 
Sent: Monday, September 25, 2017 22:57
To: Eliezer Croitoru <eliezer at ngtech.co.il>
Cc: squid-users at lists.squid-cache.org
Subject: Re: [squid-users] Bug: Missing MemObject::storeId value

So is v4 stable?  I was the impression it was beta?  That said, if v4
has better memory tuning options then I'm all ears.  Right now I'm
fighting OOM errors (and the kernel OOM reaper) under sustained load.
I've come to realize 6GB is way way too much for my 14GB RAM systems,
but finding even 1GB is too much since each squid process is exceeding
4GB.  About to try 500MB now.

I can disable rock cache, but I need some disk cache- is there a better option?

As for haproxy, I actually don't care about the client IP... I'm
running haproxy locally on the servers where the clients reside.
Mostly I'm using it for squid failover and cache affinity so I don't
have to make all my caches peers of each other.


--
Aaron Turner
https://synfin.net/         Twitter: @synfinatic
My father once told me that respect for the truth comes close to being
the basis for all morality.  "Something cannot emerge from nothing,"
he said.  This is profound thinking if you understand how unstable
"the truth" can be.  -- Frank Herbert, Dune


On Mon, Sep 25, 2017 at 11:45 AM, Eliezer Croitoru <eliezer at ngtech.co.il> wrote:
> Hey Aaron,
>
> Just to clear out the doubt's, what happen when you use squid-cache without rock cache_dir? Is the problem appearing again?
> Also, there is a possibility of a bug which is related to squid ssl-bump termination code on 3.5.X.
> Testing 4.0.21 would be the best to understand if the issue is 3.5 local or if it was fixed in 4.X+ but, from my memory I think you will need to adapt your squid.conf ssl_bump configurations.
> You can get the latest beta and stable binaries from my repo and the beta repo details are at:
> https://wiki.squid-cache.org/action/edit/KnowledgeBase/CentOS#Squid_Beta_release
>
> Also, since you are using haproxy in front of squid I would suggest you to use the proxy protocol(v1) which is the best way to pass the source ip addresses to the proxy.
> I have tested squid to work with the proxy protocol v1 but yet to test v2.
>
> All The Bests,
> Eliezer
>
> ----
> Eliezer Croitoru
> Linux System Administrator
> Mobile: +972-5-28704261
> Email: eliezer at ngtech.co.il
>
>
>
> -----Original Message-----
> From: squid-users [mailto:squid-users-bounces at lists.squid-cache.org] On Behalf Of Aaron Turner
> Sent: Saturday, September 23, 2017 02:19
> To: squid-users at lists.squid-cache.org
> Subject: [squid-users] Bug: Missing MemObject::storeId value
>
> Version: 3.5.26 on CentOS 7.3 on AWS EC2 m3.xlarge and 2x 100GB EBS
> volumes for rock cache.
>
> Doing some basic system tests and we're seeing a bunch of errors like:
>
> 2017/09/22 22:43:15 kid1| Bug: Missing MemObject::storeId value
> 2017/09/22 22:43:15 kid1| mem_hdr: 0x7f169d0a2a70 nodes.start() 0x7f169c6cc9d0
> 2017/09/22 22:43:15 kid1| mem_hdr: 0x7f169d0a2a70 nodes.finish() 0x7f169dae4e40
> 2017/09/22 22:43:15 kid1| MemObject->start_ping: 0.000000
> 2017/09/22 22:43:15 kid1| MemObject->inmem_hi: 20209
> 2017/09/22 22:43:15 kid1| MemObject->inmem_lo: 0
> 2017/09/22 22:43:15 kid1| MemObject->nclients: 0
> 2017/09/22 22:43:15 kid1| MemObject->reply: 0x7f167ee60db0
> 2017/09/22 22:43:15 kid1| MemObject->request: 0
> 2017/09/22 22:43:15 kid1| MemObject->logUri:
> 2017/09/22 22:43:15 kid1| MemObject->storeId:
> 2017/09/22 22:43:15 kid1| Bug: Missing MemObject::storeId value
> 2017/09/22 22:43:15 kid1| mem_hdr: 0x7f16a0388760 nodes.start() 0x7f16a6a4a500
> 2017/09/22 22:43:15 kid1| mem_hdr: 0x7f16a0388760 nodes.finish() 0x7f16a6a4a4d0
> 2017/09/22 22:43:15 kid1| MemObject->start_ping: 0.000000
> 2017/09/22 22:43:15 kid1| MemObject->inmem_hi: 50265
> 2017/09/22 22:43:15 kid1| MemObject->inmem_lo: 0
> 2017/09/22 22:43:15 kid1| MemObject->nclients: 0
> 2017/09/22 22:43:15 kid1| MemObject->reply: 0x7f169f83d7d0
> 2017/09/22 22:43:15 kid1| MemObject->request: 0
> 2017/09/22 22:43:15 kid1| MemObject->logUri:
> 2017/09/22 22:43:15 kid1| MemObject->storeId:
>
> I did some googling and seems like a lot of comments about this with
> Rock (we're using) and ICP/HTCP (not using).  Curious if this the same
> bug or something new?  Are there config changes we can make to prevent
> this (perhaps switching away from rock cache??)
>
> We have a bunch of clients behind haproxy which is load balancing to
> 4x Squid.  Config of the squids is as:
>
> http_access allow localhost manager
> http_access deny manager
>
> external_acl_type client_ip_map_0 %>ha{Our-Client}
> /usr/lib64/squid/user_loadbalance.py 0 4
> external_acl_type client_ip_map_1 %>ha{Our-Client}
> /usr/lib64/squid/user_loadbalance.py 1 4
> external_acl_type client_ip_map_2 %>ha{Our-Client}
> /usr/lib64/squid/user_loadbalance.py 2 4
> external_acl_type client_ip_map_3 %>ha{Our-Client}
> /usr/lib64/squid/user_loadbalance.py 3 4
>
> acl client_group_0 external client_ip_map_0
> acl client_group_1 external client_ip_map_1
> acl client_group_2 external client_ip_map_2
> acl client_group_3 external client_ip_map_3
>
> http_access allow client_group_0
> http_access allow client_group_1
> http_access allow client_group_2
> http_access allow client_group_3
> http_access deny all
>
> tcp_outgoing_address 10.93.2.41 client_group_0
> tcp_outgoing_address 10.93.2.76 client_group_1
> tcp_outgoing_address 10.93.2.198 client_group_2
> tcp_outgoing_address 10.93.3.178 client_group_3
>
> cache_dir rock /var/lib/squid/cache1 51200
> cache_dir rock /var/lib/squid/cache2 51200
> coredump_dir /var/spool/squid
> maximum_object_size_in_memory 8 MB
> maximum_object_size 8 MB
>
> cache_mem 6 GB
> memory_cache_shared on
> workers 4
>
> refresh_pattern . 0 100% 30
>
> http_port squid0001:3128 ssl-bump generate-host-certificates=on
> dynamic_cert_mem_cache_size=400MB cert=/etc/squid/ssl_cert/myCA.pem
> http_port localhost:3128
> ssl_bump bump all
>
> request_header_access Our-Client deny all
> request_header_access Via deny all
> forwarded_for delete
>
> visible_hostname squid0001.lab.company.com
> logformat adttest %tg %6tr %>a %Ss/%03>Hs %<st %rm %>ru %[un %Sh/%<a %mt %ea
> access_log daemon:/var/log/squid/access.${process_number}.log adttest
> icon_directory /usr/share/squid/icons
>
> sslcrtd_program /usr/lib64/squid/ssl_crtd -s /var/lib/squid/ssl_db -M 4MB
> sslcrtd_children 32 startup=2 idle=2
> sslproxy_session_cache_size 100 MB
> sslproxy_cert_error allow all
> sslproxy_flags DONT_VERIFY_PEER
>
>
> --
> Aaron Turner
> https://synfin.net/         Twitter: @synfinatic
> My father once told me that respect for the truth comes close to being
> the basis for all morality.  "Something cannot emerge from nothing,"
> he said.  This is profound thinking if you understand how unstable
> "the truth" can be.  -- Frank Herbert, Dune
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>



From synfinatic at gmail.com  Tue Sep 26 04:57:11 2017
From: synfinatic at gmail.com (Aaron Turner)
Date: Mon, 25 Sep 2017 21:57:11 -0700
Subject: [squid-users] Bug: Missing MemObject::storeId value
In-Reply-To: <0dfa01d3367e$fb8a9960$f29fcc20$@ngtech.co.il>
References: <CANAZdzXddzer-Bqf4jU0J2GbdRoixp8BSmoKvBFD-30kt9aWcw@mail.gmail.com>
 <0ccf01d3362e$8141e050$83c5a0f0$@ngtech.co.il>
 <CANAZdzVHj_hbmv=8ChDUSH_-UOHFPNc3VUurmiVROeJOMAtwYg@mail.gmail.com>
 <0dfa01d3367e$fb8a9960$f29fcc20$@ngtech.co.il>
Message-ID: <CANAZdzWzHOqBCP8Put405Xv6i2R1incmv7q6yykY=8cqJm4Niw@mail.gmail.com>

Yeah, sounds like I need to prove that ssl-bump is not eating memory
before I start worrying about caching.    Then slowly add features
until I find the smoking gun and focus on that.

I'm curious, does anyone have a suggestion of what modern high traffic
volume squid deployments look like? Seems like lots of the suggestions
are a bit out dated.  I'm trying to go with the KISS principle and not
do any fancy ICP/etc or multi-layer proxy config since that seems much
more difficult to deploy and benchmark.  Instead we're using haproxy
to have cache affinity across systems.  Obviously this may result in
some hot spotting, but it seems like we'll need enough servers that
hopefully the pain will be distributed.

The reason I'm looking at squid is that I've got a small server farm
of ~850 web clients which will be making ~10M page requests/day.
Right now I'm estimating about 50% of my traffic is SSL so bumping SSL
connections is pretty important.

--
Aaron Turner
https://synfin.net/         Twitter: @synfinatic
My father once told me that respect for the truth comes close to being
the basis for all morality.  "Something cannot emerge from nothing,"
he said.  This is profound thinking if you understand how unstable
"the truth" can be.  -- Frank Herbert, Dune


On Mon, Sep 25, 2017 at 9:21 PM, Eliezer Croitoru <eliezer at ngtech.co.il> wrote:
> Hey Aaron,
>
> Consider the comments from Amos and Alex first before moving forward.
> And again we need to clear out the current doubt's for both you and us.
> We don't know if the issue is related to rock cache_dir or to squid-cache in general.
> Currently for SMP aware caches the best disk cache is rock but you need to understand that the situation is that disk cache is a second level of caching and not the main goal.
> You first need to make sure that squid works for you and then to make sure rock works good enough for you.
> Also take into account that you actually "all in" for disk caching and it's not clear if you even need all this cache.
> Before you decide that the disk cache is for you and that you really need it start low and aim higher, then in small steps move forward.
> Start with a simple squid with ssl-bump without caching at all, then when you see it's stable enough from basic memory perspective for a period of 24 to 72 hours.
> Then and only then when you see it's stable enough for you and the machine can take the load try to see if adding memory cache into the picture makes sense.
> Check squid with it's default settings of cache_object sizes and try to analyze the cache logs to verify what are the most hot sites and objects that in use of your cache.
> Only when you will have a clear view what is the demand from your cache proxy service you should consider moving forward to start investigating the usage of disk cache(with default cache object sizes).
> Take into account that there is a possibility that squid will write object to the disk cache but will not use then and this is a very good reason to first test and analyze before going all in or out with squid.
> Also start with a small disk cache(10GB max) and only after verifying that indeed the setup is working good enough try to find the right memory and disk cache utilization for your setup.
>
> The above is my recommended recipe for a good and smooth start with squid in production environment.
> You are not the first and probably not the last to receive this recommendation and I believe that some articles and resources that can be fetched from the Internet can miss-lead a Linux system administrator expectation from squid-cache or any cache.
>
> Please test Squid-Cache one step at a time and do not get tempted to try to "cache all" since it's practically not possible.
> Update us as you move forward with your tests.
>
> Eliezer
>
> ----
> Eliezer Croitoru
> Linux System Administrator
> Mobile: +972-5-28704261
> Email: eliezer at ngtech.co.il
>
>
>
> -----Original Message-----
> From: Aaron Turner [mailto:synfinatic at gmail.com]
> Sent: Monday, September 25, 2017 22:57
> To: Eliezer Croitoru <eliezer at ngtech.co.il>
> Cc: squid-users at lists.squid-cache.org
> Subject: Re: [squid-users] Bug: Missing MemObject::storeId value
>
> So is v4 stable?  I was the impression it was beta?  That said, if v4
> has better memory tuning options then I'm all ears.  Right now I'm
> fighting OOM errors (and the kernel OOM reaper) under sustained load.
> I've come to realize 6GB is way way too much for my 14GB RAM systems,
> but finding even 1GB is too much since each squid process is exceeding
> 4GB.  About to try 500MB now.
>
> I can disable rock cache, but I need some disk cache- is there a better option?
>
> As for haproxy, I actually don't care about the client IP... I'm
> running haproxy locally on the servers where the clients reside.
> Mostly I'm using it for squid failover and cache affinity so I don't
> have to make all my caches peers of each other.
>
>
> --
> Aaron Turner
> https://synfin.net/         Twitter: @synfinatic
> My father once told me that respect for the truth comes close to being
> the basis for all morality.  "Something cannot emerge from nothing,"
> he said.  This is profound thinking if you understand how unstable
> "the truth" can be.  -- Frank Herbert, Dune
>
>
> On Mon, Sep 25, 2017 at 11:45 AM, Eliezer Croitoru <eliezer at ngtech.co.il> wrote:
>> Hey Aaron,
>>
>> Just to clear out the doubt's, what happen when you use squid-cache without rock cache_dir? Is the problem appearing again?
>> Also, there is a possibility of a bug which is related to squid ssl-bump termination code on 3.5.X.
>> Testing 4.0.21 would be the best to understand if the issue is 3.5 local or if it was fixed in 4.X+ but, from my memory I think you will need to adapt your squid.conf ssl_bump configurations.
>> You can get the latest beta and stable binaries from my repo and the beta repo details are at:
>> https://wiki.squid-cache.org/action/edit/KnowledgeBase/CentOS#Squid_Beta_release
>>
>> Also, since you are using haproxy in front of squid I would suggest you to use the proxy protocol(v1) which is the best way to pass the source ip addresses to the proxy.
>> I have tested squid to work with the proxy protocol v1 but yet to test v2.
>>
>> All The Bests,
>> Eliezer
>>
>> ----
>> Eliezer Croitoru
>> Linux System Administrator
>> Mobile: +972-5-28704261
>> Email: eliezer at ngtech.co.il
>>
>>
>>
>> -----Original Message-----
>> From: squid-users [mailto:squid-users-bounces at lists.squid-cache.org] On Behalf Of Aaron Turner
>> Sent: Saturday, September 23, 2017 02:19
>> To: squid-users at lists.squid-cache.org
>> Subject: [squid-users] Bug: Missing MemObject::storeId value
>>
>> Version: 3.5.26 on CentOS 7.3 on AWS EC2 m3.xlarge and 2x 100GB EBS
>> volumes for rock cache.
>>
>> Doing some basic system tests and we're seeing a bunch of errors like:
>>
>> 2017/09/22 22:43:15 kid1| Bug: Missing MemObject::storeId value
>> 2017/09/22 22:43:15 kid1| mem_hdr: 0x7f169d0a2a70 nodes.start() 0x7f169c6cc9d0
>> 2017/09/22 22:43:15 kid1| mem_hdr: 0x7f169d0a2a70 nodes.finish() 0x7f169dae4e40
>> 2017/09/22 22:43:15 kid1| MemObject->start_ping: 0.000000
>> 2017/09/22 22:43:15 kid1| MemObject->inmem_hi: 20209
>> 2017/09/22 22:43:15 kid1| MemObject->inmem_lo: 0
>> 2017/09/22 22:43:15 kid1| MemObject->nclients: 0
>> 2017/09/22 22:43:15 kid1| MemObject->reply: 0x7f167ee60db0
>> 2017/09/22 22:43:15 kid1| MemObject->request: 0
>> 2017/09/22 22:43:15 kid1| MemObject->logUri:
>> 2017/09/22 22:43:15 kid1| MemObject->storeId:
>> 2017/09/22 22:43:15 kid1| Bug: Missing MemObject::storeId value
>> 2017/09/22 22:43:15 kid1| mem_hdr: 0x7f16a0388760 nodes.start() 0x7f16a6a4a500
>> 2017/09/22 22:43:15 kid1| mem_hdr: 0x7f16a0388760 nodes.finish() 0x7f16a6a4a4d0
>> 2017/09/22 22:43:15 kid1| MemObject->start_ping: 0.000000
>> 2017/09/22 22:43:15 kid1| MemObject->inmem_hi: 50265
>> 2017/09/22 22:43:15 kid1| MemObject->inmem_lo: 0
>> 2017/09/22 22:43:15 kid1| MemObject->nclients: 0
>> 2017/09/22 22:43:15 kid1| MemObject->reply: 0x7f169f83d7d0
>> 2017/09/22 22:43:15 kid1| MemObject->request: 0
>> 2017/09/22 22:43:15 kid1| MemObject->logUri:
>> 2017/09/22 22:43:15 kid1| MemObject->storeId:
>>
>> I did some googling and seems like a lot of comments about this with
>> Rock (we're using) and ICP/HTCP (not using).  Curious if this the same
>> bug or something new?  Are there config changes we can make to prevent
>> this (perhaps switching away from rock cache??)
>>
>> We have a bunch of clients behind haproxy which is load balancing to
>> 4x Squid.  Config of the squids is as:
>>
>> http_access allow localhost manager
>> http_access deny manager
>>
>> external_acl_type client_ip_map_0 %>ha{Our-Client}
>> /usr/lib64/squid/user_loadbalance.py 0 4
>> external_acl_type client_ip_map_1 %>ha{Our-Client}
>> /usr/lib64/squid/user_loadbalance.py 1 4
>> external_acl_type client_ip_map_2 %>ha{Our-Client}
>> /usr/lib64/squid/user_loadbalance.py 2 4
>> external_acl_type client_ip_map_3 %>ha{Our-Client}
>> /usr/lib64/squid/user_loadbalance.py 3 4
>>
>> acl client_group_0 external client_ip_map_0
>> acl client_group_1 external client_ip_map_1
>> acl client_group_2 external client_ip_map_2
>> acl client_group_3 external client_ip_map_3
>>
>> http_access allow client_group_0
>> http_access allow client_group_1
>> http_access allow client_group_2
>> http_access allow client_group_3
>> http_access deny all
>>
>> tcp_outgoing_address 10.93.2.41 client_group_0
>> tcp_outgoing_address 10.93.2.76 client_group_1
>> tcp_outgoing_address 10.93.2.198 client_group_2
>> tcp_outgoing_address 10.93.3.178 client_group_3
>>
>> cache_dir rock /var/lib/squid/cache1 51200
>> cache_dir rock /var/lib/squid/cache2 51200
>> coredump_dir /var/spool/squid
>> maximum_object_size_in_memory 8 MB
>> maximum_object_size 8 MB
>>
>> cache_mem 6 GB
>> memory_cache_shared on
>> workers 4
>>
>> refresh_pattern . 0 100% 30
>>
>> http_port squid0001:3128 ssl-bump generate-host-certificates=on
>> dynamic_cert_mem_cache_size=400MB cert=/etc/squid/ssl_cert/myCA.pem
>> http_port localhost:3128
>> ssl_bump bump all
>>
>> request_header_access Our-Client deny all
>> request_header_access Via deny all
>> forwarded_for delete
>>
>> visible_hostname squid0001.lab.company.com
>> logformat adttest %tg %6tr %>a %Ss/%03>Hs %<st %rm %>ru %[un %Sh/%<a %mt %ea
>> access_log daemon:/var/log/squid/access.${process_number}.log adttest
>> icon_directory /usr/share/squid/icons
>>
>> sslcrtd_program /usr/lib64/squid/ssl_crtd -s /var/lib/squid/ssl_db -M 4MB
>> sslcrtd_children 32 startup=2 idle=2
>> sslproxy_session_cache_size 100 MB
>> sslproxy_cert_error allow all
>> sslproxy_flags DONT_VERIFY_PEER
>>
>>
>> --
>> Aaron Turner
>> https://synfin.net/         Twitter: @synfinatic
>> My father once told me that respect for the truth comes close to being
>> the basis for all morality.  "Something cannot emerge from nothing,"
>> he said.  This is profound thinking if you understand how unstable
>> "the truth" can be.  -- Frank Herbert, Dune
>> _______________________________________________
>> squid-users mailing list
>> squid-users at lists.squid-cache.org
>> http://lists.squid-cache.org/listinfo/squid-users
>>
>


From eliezer at ngtech.co.il  Tue Sep 26 04:59:00 2017
From: eliezer at ngtech.co.il (Eliezer Croitoru)
Date: Tue, 26 Sep 2017 07:59:00 +0300
Subject: [squid-users] Negotiate Authenticator and DNS
In-Reply-To: <1506091037147-0.post@n4.nabble.com>
References: <1506091037147-0.post@n4.nabble.com>
Message-ID: <0e0501d33684$2ad80720$80881560$@ngtech.co.il>

Hey,

How about using a local bind\unbound DNS server that has a forwarding zone defined only for the local domains?
For me it's a bit hard to understand the root cause for the issue but this is the best solution I can think about.
If you need some help about with bind\unbound DNS configurations just send me an email and I will try to help you with that.

All The Bests,
Eliezer

----
Eliezer Croitoru
Linux System Administrator
Mobile: +972-5-28704261
Email: eliezer at ngtech.co.il



-----Original Message-----
From: squid-users [mailto:squid-users-bounces at lists.squid-cache.org] On Behalf Of erdosain9
Sent: Friday, September 22, 2017 17:37
To: squid-users at lists.squid-cache.org
Subject: [squid-users] Negotiate Authenticator and DNS

Hi.
Im traying to improve the dns response because im having this times:

Negotiate Authenticator Statistics:
program: /lib64/squid/negotiate_kerberos_auth
number active: 32 of 32 (0 shutting down)
requests sent: 72241
replies received: 72241
queue length: 0
avg service time: 56 msec

   ID #	     FD	    PID	 # Requests	  # Replies	 Flags	   Time	 Offset
Request
     16	     30	  22242	      38896	      38896	     	  0.368	      0	(none)
     17	     32	  22243	      13404	      13404	     	  0.388	      0	(none)
     18	     38	  22244	       6962	       6962	     	  0.126	      0	(none)
     19	     61	  22245	       3895	       3895	     	  0.344	      0	(none)
     20	     65	  22246	       2636	       2636	     	  0.369	      0	(none)
     21	     74	  22247	       1879	       1879	     	  0.124	      0	(none)
     22	     76	  22248	       1177	       1177	     	  0.340	      0	(none)
     23	     78	  22249	        809	        809	     	  0.307	      0	(none)
     24	     79	  22250	        592	        592	     	  0.364	      0	(none)
     25	     81	  22251	        436	        436	     	  0.265	      0	(none)
     26	     94	  22252	        320	        320	     	  0.244	      0	(none)
     27	     96	  22253	        243	        243	     	  0.243	      0	(none)
     28	     98	  22254	        184	        184	     	  0.299	      0	(none)
     29	    109	  22255	        142	        142	     	  0.285	      0	(none)
     30	    111	  22256	        112	        112	     	  0.308	      0	(none)
     31	    113	  22257	         85	         85	     	  0.308	      0	(none)
     45	    473	  22285	         69	         69	     	  0.789	      0	(none)
     46	    475	  22286	         60	         60	     	  0.756	      0	(none)
     47	    480	  22287	         52	         52	     	  1.504	      0	(none)
     48	    495	  22288	         48	         48	     	  1.611	      0	(none)
     49	    499	  22289	         44	         44	     	  1.611	      0	(none)
     50	    580	  22291	         36	         36	     	  1.598	      0	(none)
     51	    596	  22292	         31	         31	     	  1.099	      0	(none)
     52	    593	  22293	         26	         26	     	  0.916	      0	(none)
     53	    547	  22308	         20	         20	     	  0.916	      0	(none)
     54	    550	  22309	         18	         18	     	  0.602	      0	(none)
     55	    551	  22310	         14	         14	     	  0.397	      0	(none)
     56	    553	  22311	         12	         12	     	  0.567	      0	(none)
     57	    552	  22312	         12	         12	     	  0.567	      0	(none)
     58	    397	  22313	         11	         11	     	  0.567	      0	(none)
     59	    407	  22314	         10	         10	     	  0.584	      0	(none)
     67	    436	  22355	          6	          6	     	  1.035	      0	(none)

Sometimes much more time, sometimes go to avg service time: 560 msec...

Sorry for my ignorance...
This Negotiate Authenticator is for users??? i mean this is related to, for
example, go to google.com, or is just the time that the user (client pc)
wait for be authenticate??

I think, that is related to go to a web (now i have my doubts). so i make a
dns with bind. and put that dns in squid config, and let the dns from the AD
in second place... but, when i restart this happend:

support_resolv.cc(289): pid=24587 :2017/09/22 11:16:35| kerberos_ldap_group:
ERROR: Error while resolving service record _ldap._tcp.DOMAIN.LAN with r
es_search
support_resolv.cc(71): pid=24587 :2017/09/22 11:16:35| kerberos_ldap_group:
ERROR: res_search: Unknown service record: _ldap._tcp.DOMAIN.LAN
support_resolv.cc(183): pid=24587 :2017/09/22 11:16:35| kerberos_ldap_group:
ERROR: Error while resolving hostname with getaddrinfo: Name or service 
not known
support_sasl.cc(276): pid=24587 :2017/09/22 11:16:35| kerberos_ldap_group:
ERROR: ldap_sasl_interactive_bind_s error: Can't contact LDAP server
support_ldap.cc(957): pid=24587 :2017/09/22 11:16:35| kerberos_ldap_group:
ERROR: Error while binding to ldap server with SASL/GSSAPI: Can't contact 
LDAP server


So, this post is for two question. 
1- The thing about Negotiate Authenticator (that value what represent?)
2- Can i improve making my own dns (apart from the the dns from the domain)?
(i prefer make other dns, than fix the dns from the domain, because i dont
manage that).

Thanks to all, and sorry for the ignorance, and my bad writing (i dont speak
english)



--
Sent from: http://squid-web-proxy-cache.1019090.n4.nabble.com/Squid-Users-f1019091.html
_______________________________________________
squid-users mailing list
squid-users at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-users



From eliezer at ngtech.co.il  Tue Sep 26 05:09:28 2017
From: eliezer at ngtech.co.il (Eliezer Croitoru)
Date: Tue, 26 Sep 2017 08:09:28 +0300
Subject: [squid-users] disable access.log logging on a specific entrys
In-Reply-To: <ab21043f-023d-39ab-94c4-074a0eda1d52@treenet.co.nz>
References: <1505742348570-0.post@n4.nabble.com>
 <6b90a1c3-e64e-eac5-21f5-ac5d74346655@treenet.co.nz>
 <4861f665-44a4-9bac-657c-f5e5e750548b@treenet.co.nz>
 <ab21043f-023d-39ab-94c4-074a0eda1d52@treenet.co.nz>
Message-ID: <0e0801d33685$a15663a0$e4032ae0$@ngtech.co.il>

If you trust the software which creates these requests you can bypass the proxy for the ip addresses of this system.
If you do not trust this software then it's better left passed to the proxy.
This URL should have Host header and if not then it's probably something that should be blocked.. or fixed by the vendor of the antispam solution.

All The Bests,
Eliezer

----
Eliezer Croitoru
Linux System Administrator
Mobile: +972-5-28704261
Email: eliezer at ngtech.co.il



-----Original Message-----
From: squid-users [mailto:squid-users-bounces at lists.squid-cache.org] On Behalf Of Amos Jeffries
Sent: Wednesday, September 20, 2017 17:55
To: squid-users at lists.squid-cache.org
Subject: Re: [squid-users] disable access.log logging on a specific entrys

On 21/09/17 01:42, Holger Wybranietz wrote:
 > Hello Amos,
 >
 > Yast doesn't show any newer version then 3.5.21 (you meant 3.5.27 is
 > working fine?).
 >

Yes. I tested with 3.5.27, 4.0.21, and latest v5 code. All hide the log 
entries when !logNoSpamresolver is used.


 > By the way:
 > The entrys I want to get filtered are similar to:
 >
 > 192.168.12.84 - - [18/Sep/2017:15:22:40 +0200] "POST
 > /SpamResolverNG/SpamResolverNG.dll?DoNewRequest HTTP/1.1" 400 3990
 > TAG_NONE:HIER_NONE
 >
 > I think, that this is not a "normal" url, "/SpamResolverNG/Spa..." seems
 > to be a directory path?

It's called an origin-form URI and is the true form of URLs delivered to 
web servers on port 80 and 443.

I suspect there is no Host header delivered by the client to allow Squid 
to convert it into an absolute-form URL for proxy consumption. Which 
would also explain the 400 status and *_NONE server details.


 > Is there another way to treat this kind of entries?
 >

That depends on your definition of "treat". They are all actual traffic 
consuming resources on the proxy, so it is a little odd to hide them 
from view. On the other hand you are using a web server log format in a 
proxy, which is very lossy anyway.


The config mentioned earlier was correct for what you tried to do. Its 
odd that it was not working.

Maybe something wrong with the regex. I'm thinking unicode characters 
etc not quite matching what the eyes seem to indicate - in either the 
URL itself or the config regex.


It might be a good idea to try and resolve the problem in the client 
software if you can;

- if the AV software is configured to use the proxy (including with 
auto-config methods, WPAD/PAC etc) then it is a bug to be sending that 
URL form to a proxy. The vendor may want to know and fix it since other 
customers will be having the same issue and this type of bug is 
security vulnerability for AV.

- if you are intercepting the traffic from port 80 or 443 somehow, then 
your interception would appear to be broken. Squid should always be able 
to determine the ORIGINAL_DST for intercepted traffic and transparently 
deliver it there when Host is missing or invalid.

Amos
_______________________________________________
squid-users mailing list
squid-users at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-users



From sukhbaatar_t at yahoo.com  Tue Sep 26 08:15:43 2017
From: sukhbaatar_t at yahoo.com (Sukhbaatar T)
Date: Tue, 26 Sep 2017 08:15:43 +0000 (UTC)
Subject: [squid-users] =?utf-8?q?=28no_subject=29?=
References: <1110594435.2155046.1506413743184.ref@mail.yahoo.com>
Message-ID: <1110594435.2155046.1506413743184@mail.yahoo.com>

Hello. I'm a teacher. My computer's OS is Windows 7. Installed SQUID 3.5.27. We have fifteen computers in our cabinet. The internet bandwidth is 4mb. We need to cache youtube and fb data of our kids. The youtube cache needs to be saved for a month, and the facebook to remain for a day. Our configuration is this small. Can you do the SQUID options for this (squid.conf file)? Thank you
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20170926/bebf6024/attachment.htm>

From mc8647 at mclink.it  Tue Sep 26 10:42:48 2017
From: mc8647 at mclink.it (Travel Factory S.r.l.)
Date: Tue, 26 Sep 2017 12:42:48 +0200
Subject: [squid-users] Still at 3.3.11... time to upgrade?
Message-ID: <web-20496547@mailbackend4.mclink.it>

I have to say that squid 3.3.11 worked flawlessy since january 2014...

... but I think it is time to upgrade.

My server is dated but it has 16 cores and 32 GB of ram, with less 
than 3000 users. Workload is split between 2 identical servers thanks 
to a proxy.pac.
I have spinning disks now but I also have a couple of SSD in the 
drawer, just in case they are now fully used.
O.S. is linux Red Hat 6.4.

Can you please suggest some docs to read about the migration path, the 
changes needed, the new features of 3.5?

Thanks


From squid3 at treenet.co.nz  Tue Sep 26 12:57:00 2017
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Wed, 27 Sep 2017 01:57:00 +1300
Subject: [squid-users] Negotiate Authenticator and DNS
In-Reply-To: <0e0501d33684$2ad80720$80881560$@ngtech.co.il>
References: <1506091037147-0.post@n4.nabble.com>
 <0e0501d33684$2ad80720$80881560$@ngtech.co.il>
Message-ID: <fde626bd-6766-9f26-aae8-a2fbef381280@treenet.co.nz>

On 26/09/17 17:59, Eliezer Croitoru wrote:
> Hey,
> 
> How about using a local bind\unbound DNS server that has a forwarding zone defined only for the local domains?
> For me it's a bit hard to understand the root cause for the issue but this is the best solution I can think about.
> If you need some help about with bind\unbound DNS configurations just send me an email and I will try to help you with that.


> -----Original Message-----
> From: erdosain9
> 
> Hi.
> Im traying to improve the dns response because im having this times:
> 
> Negotiate Authenticator Statistics:
> program: /lib64/squid/negotiate_kerberos_auth

Notice the name of the program above.

> 
> Sometimes much more time, sometimes go to avg service time: 560 msec...
> 

Thats not good, DNS should be much faster. But not related to the errors 
below.


> Sorry for my ignorance...
> This Negotiate Authenticator is for users??? i mean this is related to, for
> example, go to google.com, or is just the time that the user (client pc)
> wait for be authenticate??

The report you quoted was for Negotiate authentication helpers. Only. 
The times there relate to how long it takes to login.


> 
> I think, that is related to go to a web (now i have my doubts). so i make a
> dns with bind. and put that dns in squid config, and let the dns from the AD
> in second place... but, when i restart this happend:
> 
> support_resolv.cc(289): pid=24587 :2017/09/22 11:16:35| kerberos_ldap_group:

Notice the name (above) of the program reporting these errors.


> ERROR: Error while resolving service record _ldap._tcp.DOMAIN.LAN with r
> es_search
> support_resolv.cc(71): pid=24587 :2017/09/22 11:16:35| kerberos_ldap_group:
> ERROR: res_search: Unknown service record: _ldap._tcp.DOMAIN.LAN
> support_resolv.cc(183): pid=24587 :2017/09/22 11:16:35| kerberos_ldap_group:
> ERROR: Error while resolving hostname with getaddrinfo: Name or service
> not known
> support_sasl.cc(276): pid=24587 :2017/09/22 11:16:35| kerberos_ldap_group:
> ERROR: ldap_sasl_interactive_bind_s error: Can't contact LDAP server
> support_ldap.cc(957): pid=24587 :2017/09/22 11:16:35| kerberos_ldap_group:
> ERROR: Error while binding to ldap server with SASL/GSSAPI: Can't contact
> LDAP server
> 
> 
> So, this post is for two question.
> 1- The thing about Negotiate Authenticator (that value what represent?)
> 2- Can i improve making my own dns (apart from the the dns from the domain)?
> (i prefer make other dns, than fix the dns from the domain, because i dont
> manage that).

These errors are missing records and servers not running (or not 
existing?). Different DNS server would only help with lag.

> 
> Thanks to all, and sorry for the ignorance, and my bad writing (i dont speak
> english)
> 

Amos


From erdosain9 at gmail.com  Tue Sep 26 12:57:36 2017
From: erdosain9 at gmail.com (erdosain9)
Date: Tue, 26 Sep 2017 05:57:36 -0700 (MST)
Subject: [squid-users] Negotiate Authenticator and DNS
In-Reply-To: <0e0501d33684$2ad80720$80881560$@ngtech.co.il>
References: <1506091037147-0.post@n4.nabble.com>
 <0e0501d33684$2ad80720$80881560$@ngtech.co.il>
Message-ID: <1506430656710-0.post@n4.nabble.com>

Hi.
Thanks.
But there is some Time to live, for config in the squid, so the service is
not asking every time for authenticate??
Thanks!



--
Sent from: http://squid-web-proxy-cache.1019090.n4.nabble.com/Squid-Users-f1019091.html


From squid3 at treenet.co.nz  Tue Sep 26 13:13:10 2017
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Wed, 27 Sep 2017 02:13:10 +1300
Subject: [squid-users] Negotiate Authenticator and DNS
In-Reply-To: <1506430656710-0.post@n4.nabble.com>
References: <1506091037147-0.post@n4.nabble.com>
 <0e0501d33684$2ad80720$80881560$@ngtech.co.il>
 <1506430656710-0.post@n4.nabble.com>
Message-ID: <5872da48-88b6-dc4e-6757-0b9261460f7a@treenet.co.nz>

On 27/09/17 01:57, erdosain9 wrote:
> Hi.
> Thanks.
> But there is some Time to live, for config in the squid, so the service is
> not asking every time for authenticate??

For Negotiate and NTLM the credentials are supposed to be unique per 
connection, so each TCP connection requires separate lookup. But 
followup pipelined requests on a connection should not need auth helper 
lookups as they share the already authenticated credentials.

*group* lookups are different (and cached normally), but they are not 
authentication.

Amos


From squid3 at treenet.co.nz  Tue Sep 26 13:44:16 2017
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Wed, 27 Sep 2017 02:44:16 +1300
Subject: [squid-users] Still at 3.3.11... time to upgrade?
In-Reply-To: <web-20496547@mailbackend4.mclink.it>
References: <web-20496547@mailbackend4.mclink.it>
Message-ID: <4b5d7101-8f2a-5326-ae70-80d3d65e0a01@treenet.co.nz>

On 26/09/17 23:42, Travel Factory S.r.l. wrote:
> I have to say that squid 3.3.11 worked flawlessy since january 2014...
> 
> ... but I think it is time to upgrade.
> 
...
> 
> Can you please suggest some docs to read about the migration path, the 
> changes needed, the new features of 3.5?


Migration path is same for all Squid upgrades (even minor ones):

* install new Squid version

* run 'squid -k parse'

* fix anything it mentions as FATAL and ERROR. If possible the WARNING too.


Other details should be linked from:
   <https://wiki.squid-cache.org/Squid-3.4>
   <https://wiki.squid-cache.org/Squid-3.5>


Amos


From squid3 at treenet.co.nz  Tue Sep 26 13:47:42 2017
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Wed, 27 Sep 2017 02:47:42 +1300
Subject: [squid-users] (no subject)
In-Reply-To: <1110594435.2155046.1506413743184@mail.yahoo.com>
References: <1110594435.2155046.1506413743184.ref@mail.yahoo.com>
 <1110594435.2155046.1506413743184@mail.yahoo.com>
Message-ID: <4d0eab8f-9b38-69c9-a20e-3f3348d5e69d@treenet.co.nz>

On 26/09/17 21:15, Sukhbaatar T wrote:
> Hello. I'm a teacher. My computer's OS is Windows 7. Installed SQUID 
> 3.5.27. We have fifteen computers in our cabinet. The internet bandwidth 
> is 4mb. We need to cache youtube and fb data of our kids. The youtube 
> cache needs to be saved for a month, and the facebook to remain for a 
> day. Our configuration is this small. Can you do the SQUID options for 
> this (squid.conf file)? Thank you

It would be best if you learned to do it yourself. Then you would not 
have to ask us.

Did you see the response to your last post where I pasted the links to 
what you need?

Amos


From erdosain9 at gmail.com  Tue Sep 26 13:59:19 2017
From: erdosain9 at gmail.com (erdosain9)
Date: Tue, 26 Sep 2017 06:59:19 -0700 (MST)
Subject: [squid-users] Negotiate Authenticator and DNS
In-Reply-To: <5872da48-88b6-dc4e-6757-0b9261460f7a@treenet.co.nz>
References: <1506091037147-0.post@n4.nabble.com>
 <0e0501d33684$2ad80720$80881560$@ngtech.co.il>
 <1506430656710-0.post@n4.nabble.com>
 <5872da48-88b6-dc4e-6757-0b9261460f7a@treenet.co.nz>
Message-ID: <1506434359992-0.post@n4.nabble.com>

but, why so slow then???

"
For Negotiate and NTLM the credentials are supposed to be unique per
connection, so each TCP connection requires separate lookup. But
followup pipelined requests on a connection should not need auth helper
lookups as they share the already authenticated credentials.

*group* lookups are different (and cached normally), but they are not
authentication.

"

thanks



--
Sent from: http://squid-web-proxy-cache.1019090.n4.nabble.com/Squid-Users-f1019091.html


From squid3 at treenet.co.nz  Tue Sep 26 14:06:56 2017
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Wed, 27 Sep 2017 03:06:56 +1300
Subject: [squid-users] Negotiate Authenticator and DNS
In-Reply-To: <1506434359992-0.post@n4.nabble.com>
References: <1506091037147-0.post@n4.nabble.com>
 <0e0501d33684$2ad80720$80881560$@ngtech.co.il>
 <1506430656710-0.post@n4.nabble.com>
 <5872da48-88b6-dc4e-6757-0b9261460f7a@treenet.co.nz>
 <1506434359992-0.post@n4.nabble.com>
Message-ID: <284f1504-f4bd-ad38-b624-40e13324e954@treenet.co.nz>

On 27/09/17 02:59, erdosain9 wrote:
> but, why so slow then???
> 

What is so slow *exactly*?

The report you posted only tells about the initial lookups. Not the 
cached or pipelined results.


Amos


From erdosain9 at gmail.com  Tue Sep 26 14:34:40 2017
From: erdosain9 at gmail.com (erdosain9)
Date: Tue, 26 Sep 2017 07:34:40 -0700 (MST)
Subject: [squid-users] Negotiate Authenticator and DNS
In-Reply-To: <5872da48-88b6-dc4e-6757-0b9261460f7a@treenet.co.nz>
References: <1506091037147-0.post@n4.nabble.com>
 <0e0501d33684$2ad80720$80881560$@ngtech.co.il>
 <1506430656710-0.post@n4.nabble.com>
 <5872da48-88b6-dc4e-6757-0b9261460f7a@treenet.co.nz>
Message-ID: <1506436480080-0.post@n4.nabble.com>

Sorry, this is part of my config

###Kerberos Auth with ActiveDirectory###
auth_param negotiate program /lib64/squid/negotiate_kerberos_auth -s
HTTP/squid.domain.lan at DOMAIN.LAN
auth_param negotiate children 45 startup=0 idle=1
auth_param negotiate keep_alive on


external_acl_type i-full %LOGIN /usr/lib64/squid/ext_kerberos_ldap_group_acl
-g i-full at DOMAIN.LAN
external_acl_type i-limitado %LOGIN
/usr/lib64/squid/ext_kerberos_ldap_group_acl -g i-limitado at DOMAIN.LAN


#GRUPOS
acl i-full external i-full
acl i-limitado external i-limitado




--
Sent from: http://squid-web-proxy-cache.1019090.n4.nabble.com/Squid-Users-f1019091.html


From xpro6000 at gmail.com  Tue Sep 26 17:43:28 2017
From: xpro6000 at gmail.com (xpro6000)
Date: Tue, 26 Sep 2017 13:43:28 -0400
Subject: [squid-users] Reading gzip encodec content failed
Message-ID: <CAFoK1azKvArWwPF1AC2MhLhftcJ5iif1JigGqxb4qq+6mEKgCA@mail.gmail.com>

When I use my squid proxy server within my Java program I get the following
error

"Reading gzip encodec content failed.
java.util.zip.ZipException: Not in GZIP format"

This does not happen when I use other proxies or if I don't use a proxy at
all. Why is this happening? how can I fix the issue? I'm using Squid 3.5.23
installed on Debian 9
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20170926/dccaf867/attachment.htm>

From Antony.Stone at squid.open.source.it  Tue Sep 26 17:57:55 2017
From: Antony.Stone at squid.open.source.it (Antony Stone)
Date: Tue, 26 Sep 2017 19:57:55 +0200
Subject: [squid-users] Reading gzip encodec content failed
In-Reply-To: <CAFoK1azKvArWwPF1AC2MhLhftcJ5iif1JigGqxb4qq+6mEKgCA@mail.gmail.com>
References: <CAFoK1azKvArWwPF1AC2MhLhftcJ5iif1JigGqxb4qq+6mEKgCA@mail.gmail.com>
Message-ID: <201709261957.55889.Antony.Stone@squid.open.source.it>

On Tuesday 26 September 2017 at 19:43:28, xpro6000 wrote:

> When I use my squid proxy server within my Java program I get the following
> error
> 
> "Reading gzip encodec content failed.
> java.util.zip.ZipException: Not in GZIP format"
> 
> This does not happen when I use other proxies or if I don't use a proxy at
> all. Why is this happening? how can I fix the issue? I'm using Squid 3.5.23
> installed on Debian 9

We'd need to know quite a bit more about what "use my squid proxy server 
within my Java program" means.  Obviously (presumably) you mean you're 
performing HTTP transactions, with or without proxy support, but without any 
details about quite how you're doing this, we have no information to go on.

Here's a suggestion:

Perform one of these transactions from your Java program and show us what 
appears in the squid access log.

Then perform the same transaction using a browser, wget, curl or whatever can 
do the same thing, and show us what appears in the squid access log.

Whatever you do, just give us something we can work with so we have some ideas 
about what it is you're doing and what isn't working.

At the very least tell us what HTTP transaction you're trying to pass through 
Squid.


Antony.

-- 
#define SIX 1+5
#define NINE 8+1

int main() {
    printf("%d\n", SIX * NINE);
}
	- thanks to ECB for bringing this to my attention

                                                   Please reply to the list;
                                                         please *don't* CC me.


From ppmartell at unah.edu.cu  Tue Sep 26 19:27:10 2017
From: ppmartell at unah.edu.cu (ppmartell at unah.edu.cu)
Date: Tue, 26 Sep 2017 15:27:10 -0400 (CDT)
Subject: [squid-users] acl url_regex on squid3 is not working using an
 online tested regular expression
In-Reply-To: <877952927.570355.1506364921562.JavaMail.root@unah.edu.cu>
References: <877952927.570355.1506364921562.JavaMail.root@unah.edu.cu>
Message-ID: <649546619.667635.1506454030958.JavaMail.root@unah.edu.cu>

As Mr. Alex Rousskov suggested, the problem was the regex itself. He provided me a modified regex (more simple) and now the filter is working. 

My regex: ((?=.*\biphone\b)|(?=.*\bprod\b)).*\.facebook\.com(\:|\d|) 

Alex's regex: \b(iphone|prod)\b.*\.facebook\.com 

Using https://regex101.com/ both work, but squid only accepts the second one. After running squid3 -k parse I got no errors. Thanks Alex. 

Participe en el Congreso Internacional de las Ciencias Agropecuarias (AGROCIENCIAS 2017) http://www.agrocienciascuba.com/
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20170926/40a15f65/attachment.htm>

From synfinatic at gmail.com  Tue Sep 26 21:30:49 2017
From: synfinatic at gmail.com (Aaron Turner)
Date: Tue, 26 Sep 2017 14:30:49 -0700
Subject: [squid-users] Bug: Missing MemObject::storeId value
In-Reply-To: <CANAZdzWzHOqBCP8Put405Xv6i2R1incmv7q6yykY=8cqJm4Niw@mail.gmail.com>
References: <CANAZdzXddzer-Bqf4jU0J2GbdRoixp8BSmoKvBFD-30kt9aWcw@mail.gmail.com>
 <0ccf01d3362e$8141e050$83c5a0f0$@ngtech.co.il>
 <CANAZdzVHj_hbmv=8ChDUSH_-UOHFPNc3VUurmiVROeJOMAtwYg@mail.gmail.com>
 <0dfa01d3367e$fb8a9960$f29fcc20$@ngtech.co.il>
 <CANAZdzWzHOqBCP8Put405Xv6i2R1incmv7q6yykY=8cqJm4Niw@mail.gmail.com>
Message-ID: <CANAZdzU7w_TNS7pkUXQp6o7_=+c6RGFNQ2j-Lng4nP+7DYpF8Q@mail.gmail.com>

Just a followup.  Thanks to Amos which suggested setting
sslflags=NO_DEFAULT_CA on the http_port(s).  That seems to have fixed
the memory (leak??) problem.  Probably should run this for a few days
to be sure, but at least now I can run squid for a few hours and the
memory is much more stable vs. before where I'd start having problems
after about ~30min.

On a side note, the MemObject bug I referred to at the start of this
thread seems definitely related to enabling Rock cache.  I wasn't
seeing the error when I was running with and without the memcache, but
seems to have come back once I enabled the rock.  I'm still working on
tuning my squid caching preferences to match our needs, so I may have
more info later.

--
Aaron Turner
https://synfin.net/         Twitter: @synfinatic
My father once told me that respect for the truth comes close to being
the basis for all morality.  "Something cannot emerge from nothing,"
he said.  This is profound thinking if you understand how unstable
"the truth" can be.  -- Frank Herbert, Dune


On Mon, Sep 25, 2017 at 9:57 PM, Aaron Turner <synfinatic at gmail.com> wrote:
> Yeah, sounds like I need to prove that ssl-bump is not eating memory
> before I start worrying about caching.    Then slowly add features
> until I find the smoking gun and focus on that.
>
> I'm curious, does anyone have a suggestion of what modern high traffic
> volume squid deployments look like? Seems like lots of the suggestions
> are a bit out dated.  I'm trying to go with the KISS principle and not
> do any fancy ICP/etc or multi-layer proxy config since that seems much
> more difficult to deploy and benchmark.  Instead we're using haproxy
> to have cache affinity across systems.  Obviously this may result in
> some hot spotting, but it seems like we'll need enough servers that
> hopefully the pain will be distributed.
>
> The reason I'm looking at squid is that I've got a small server farm
> of ~850 web clients which will be making ~10M page requests/day.
> Right now I'm estimating about 50% of my traffic is SSL so bumping SSL
> connections is pretty important.
>
> --
> Aaron Turner
> https://synfin.net/         Twitter: @synfinatic
> My father once told me that respect for the truth comes close to being
> the basis for all morality.  "Something cannot emerge from nothing,"
> he said.  This is profound thinking if you understand how unstable
> "the truth" can be.  -- Frank Herbert, Dune
>
>
> On Mon, Sep 25, 2017 at 9:21 PM, Eliezer Croitoru <eliezer at ngtech.co.il> wrote:
>> Hey Aaron,
>>
>> Consider the comments from Amos and Alex first before moving forward.
>> And again we need to clear out the current doubt's for both you and us.
>> We don't know if the issue is related to rock cache_dir or to squid-cache in general.
>> Currently for SMP aware caches the best disk cache is rock but you need to understand that the situation is that disk cache is a second level of caching and not the main goal.
>> You first need to make sure that squid works for you and then to make sure rock works good enough for you.
>> Also take into account that you actually "all in" for disk caching and it's not clear if you even need all this cache.
>> Before you decide that the disk cache is for you and that you really need it start low and aim higher, then in small steps move forward.
>> Start with a simple squid with ssl-bump without caching at all, then when you see it's stable enough from basic memory perspective for a period of 24 to 72 hours.
>> Then and only then when you see it's stable enough for you and the machine can take the load try to see if adding memory cache into the picture makes sense.
>> Check squid with it's default settings of cache_object sizes and try to analyze the cache logs to verify what are the most hot sites and objects that in use of your cache.
>> Only when you will have a clear view what is the demand from your cache proxy service you should consider moving forward to start investigating the usage of disk cache(with default cache object sizes).
>> Take into account that there is a possibility that squid will write object to the disk cache but will not use then and this is a very good reason to first test and analyze before going all in or out with squid.
>> Also start with a small disk cache(10GB max) and only after verifying that indeed the setup is working good enough try to find the right memory and disk cache utilization for your setup.
>>
>> The above is my recommended recipe for a good and smooth start with squid in production environment.
>> You are not the first and probably not the last to receive this recommendation and I believe that some articles and resources that can be fetched from the Internet can miss-lead a Linux system administrator expectation from squid-cache or any cache.
>>
>> Please test Squid-Cache one step at a time and do not get tempted to try to "cache all" since it's practically not possible.
>> Update us as you move forward with your tests.
>>
>> Eliezer
>>
>> ----
>> Eliezer Croitoru
>> Linux System Administrator
>> Mobile: +972-5-28704261
>> Email: eliezer at ngtech.co.il
>>
>>
>>
>> -----Original Message-----
>> From: Aaron Turner [mailto:synfinatic at gmail.com]
>> Sent: Monday, September 25, 2017 22:57
>> To: Eliezer Croitoru <eliezer at ngtech.co.il>
>> Cc: squid-users at lists.squid-cache.org
>> Subject: Re: [squid-users] Bug: Missing MemObject::storeId value
>>
>> So is v4 stable?  I was the impression it was beta?  That said, if v4
>> has better memory tuning options then I'm all ears.  Right now I'm
>> fighting OOM errors (and the kernel OOM reaper) under sustained load.
>> I've come to realize 6GB is way way too much for my 14GB RAM systems,
>> but finding even 1GB is too much since each squid process is exceeding
>> 4GB.  About to try 500MB now.
>>
>> I can disable rock cache, but I need some disk cache- is there a better option?
>>
>> As for haproxy, I actually don't care about the client IP... I'm
>> running haproxy locally on the servers where the clients reside.
>> Mostly I'm using it for squid failover and cache affinity so I don't
>> have to make all my caches peers of each other.
>>
>>
>> --
>> Aaron Turner
>> https://synfin.net/         Twitter: @synfinatic
>> My father once told me that respect for the truth comes close to being
>> the basis for all morality.  "Something cannot emerge from nothing,"
>> he said.  This is profound thinking if you understand how unstable
>> "the truth" can be.  -- Frank Herbert, Dune
>>
>>
>> On Mon, Sep 25, 2017 at 11:45 AM, Eliezer Croitoru <eliezer at ngtech.co.il> wrote:
>>> Hey Aaron,
>>>
>>> Just to clear out the doubt's, what happen when you use squid-cache without rock cache_dir? Is the problem appearing again?
>>> Also, there is a possibility of a bug which is related to squid ssl-bump termination code on 3.5.X.
>>> Testing 4.0.21 would be the best to understand if the issue is 3.5 local or if it was fixed in 4.X+ but, from my memory I think you will need to adapt your squid.conf ssl_bump configurations.
>>> You can get the latest beta and stable binaries from my repo and the beta repo details are at:
>>> https://wiki.squid-cache.org/action/edit/KnowledgeBase/CentOS#Squid_Beta_release
>>>
>>> Also, since you are using haproxy in front of squid I would suggest you to use the proxy protocol(v1) which is the best way to pass the source ip addresses to the proxy.
>>> I have tested squid to work with the proxy protocol v1 but yet to test v2.
>>>
>>> All The Bests,
>>> Eliezer
>>>
>>> ----
>>> Eliezer Croitoru
>>> Linux System Administrator
>>> Mobile: +972-5-28704261
>>> Email: eliezer at ngtech.co.il
>>>
>>>
>>>
>>> -----Original Message-----
>>> From: squid-users [mailto:squid-users-bounces at lists.squid-cache.org] On Behalf Of Aaron Turner
>>> Sent: Saturday, September 23, 2017 02:19
>>> To: squid-users at lists.squid-cache.org
>>> Subject: [squid-users] Bug: Missing MemObject::storeId value
>>>
>>> Version: 3.5.26 on CentOS 7.3 on AWS EC2 m3.xlarge and 2x 100GB EBS
>>> volumes for rock cache.
>>>
>>> Doing some basic system tests and we're seeing a bunch of errors like:
>>>
>>> 2017/09/22 22:43:15 kid1| Bug: Missing MemObject::storeId value
>>> 2017/09/22 22:43:15 kid1| mem_hdr: 0x7f169d0a2a70 nodes.start() 0x7f169c6cc9d0
>>> 2017/09/22 22:43:15 kid1| mem_hdr: 0x7f169d0a2a70 nodes.finish() 0x7f169dae4e40
>>> 2017/09/22 22:43:15 kid1| MemObject->start_ping: 0.000000
>>> 2017/09/22 22:43:15 kid1| MemObject->inmem_hi: 20209
>>> 2017/09/22 22:43:15 kid1| MemObject->inmem_lo: 0
>>> 2017/09/22 22:43:15 kid1| MemObject->nclients: 0
>>> 2017/09/22 22:43:15 kid1| MemObject->reply: 0x7f167ee60db0
>>> 2017/09/22 22:43:15 kid1| MemObject->request: 0
>>> 2017/09/22 22:43:15 kid1| MemObject->logUri:
>>> 2017/09/22 22:43:15 kid1| MemObject->storeId:
>>> 2017/09/22 22:43:15 kid1| Bug: Missing MemObject::storeId value
>>> 2017/09/22 22:43:15 kid1| mem_hdr: 0x7f16a0388760 nodes.start() 0x7f16a6a4a500
>>> 2017/09/22 22:43:15 kid1| mem_hdr: 0x7f16a0388760 nodes.finish() 0x7f16a6a4a4d0
>>> 2017/09/22 22:43:15 kid1| MemObject->start_ping: 0.000000
>>> 2017/09/22 22:43:15 kid1| MemObject->inmem_hi: 50265
>>> 2017/09/22 22:43:15 kid1| MemObject->inmem_lo: 0
>>> 2017/09/22 22:43:15 kid1| MemObject->nclients: 0
>>> 2017/09/22 22:43:15 kid1| MemObject->reply: 0x7f169f83d7d0
>>> 2017/09/22 22:43:15 kid1| MemObject->request: 0
>>> 2017/09/22 22:43:15 kid1| MemObject->logUri:
>>> 2017/09/22 22:43:15 kid1| MemObject->storeId:
>>>
>>> I did some googling and seems like a lot of comments about this with
>>> Rock (we're using) and ICP/HTCP (not using).  Curious if this the same
>>> bug or something new?  Are there config changes we can make to prevent
>>> this (perhaps switching away from rock cache??)
>>>
>>> We have a bunch of clients behind haproxy which is load balancing to
>>> 4x Squid.  Config of the squids is as:
>>>
>>> http_access allow localhost manager
>>> http_access deny manager
>>>
>>> external_acl_type client_ip_map_0 %>ha{Our-Client}
>>> /usr/lib64/squid/user_loadbalance.py 0 4
>>> external_acl_type client_ip_map_1 %>ha{Our-Client}
>>> /usr/lib64/squid/user_loadbalance.py 1 4
>>> external_acl_type client_ip_map_2 %>ha{Our-Client}
>>> /usr/lib64/squid/user_loadbalance.py 2 4
>>> external_acl_type client_ip_map_3 %>ha{Our-Client}
>>> /usr/lib64/squid/user_loadbalance.py 3 4
>>>
>>> acl client_group_0 external client_ip_map_0
>>> acl client_group_1 external client_ip_map_1
>>> acl client_group_2 external client_ip_map_2
>>> acl client_group_3 external client_ip_map_3
>>>
>>> http_access allow client_group_0
>>> http_access allow client_group_1
>>> http_access allow client_group_2
>>> http_access allow client_group_3
>>> http_access deny all
>>>
>>> tcp_outgoing_address 10.93.2.41 client_group_0
>>> tcp_outgoing_address 10.93.2.76 client_group_1
>>> tcp_outgoing_address 10.93.2.198 client_group_2
>>> tcp_outgoing_address 10.93.3.178 client_group_3
>>>
>>> cache_dir rock /var/lib/squid/cache1 51200
>>> cache_dir rock /var/lib/squid/cache2 51200
>>> coredump_dir /var/spool/squid
>>> maximum_object_size_in_memory 8 MB
>>> maximum_object_size 8 MB
>>>
>>> cache_mem 6 GB
>>> memory_cache_shared on
>>> workers 4
>>>
>>> refresh_pattern . 0 100% 30
>>>
>>> http_port squid0001:3128 ssl-bump generate-host-certificates=on
>>> dynamic_cert_mem_cache_size=400MB cert=/etc/squid/ssl_cert/myCA.pem
>>> http_port localhost:3128
>>> ssl_bump bump all
>>>
>>> request_header_access Our-Client deny all
>>> request_header_access Via deny all
>>> forwarded_for delete
>>>
>>> visible_hostname squid0001.lab.company.com
>>> logformat adttest %tg %6tr %>a %Ss/%03>Hs %<st %rm %>ru %[un %Sh/%<a %mt %ea
>>> access_log daemon:/var/log/squid/access.${process_number}.log adttest
>>> icon_directory /usr/share/squid/icons
>>>
>>> sslcrtd_program /usr/lib64/squid/ssl_crtd -s /var/lib/squid/ssl_db -M 4MB
>>> sslcrtd_children 32 startup=2 idle=2
>>> sslproxy_session_cache_size 100 MB
>>> sslproxy_cert_error allow all
>>> sslproxy_flags DONT_VERIFY_PEER
>>>
>>>
>>> --
>>> Aaron Turner
>>> https://synfin.net/         Twitter: @synfinatic
>>> My father once told me that respect for the truth comes close to being
>>> the basis for all morality.  "Something cannot emerge from nothing,"
>>> he said.  This is profound thinking if you understand how unstable
>>> "the truth" can be.  -- Frank Herbert, Dune
>>> _______________________________________________
>>> squid-users mailing list
>>> squid-users at lists.squid-cache.org
>>> http://lists.squid-cache.org/listinfo/squid-users
>>>
>>


From eliezer at ngtech.co.il  Tue Sep 26 21:35:55 2017
From: eliezer at ngtech.co.il (Eliezer Croitoru)
Date: Wed, 27 Sep 2017 00:35:55 +0300
Subject: [squid-users] SSL-BUMP blackhole instance configuration
Message-ID: <!&!AAAAAAAAAAAuAAAAAAAAAGDVRrtF13VDjYYZR9MgvgYBAMO2jhD3dRHOtM0AqgC7tuYAAAAAAA4AABAAAACy4sFGxE7TRqRz+g8r7MbVAQAAAAA=@ngtech.co.il>

Hey All,

I have been working on couple tools which are using my drbl-peer library.
- external acl helper
- dns blacklist server
- and couple others..

I took a dns proxy server named grimd and upgraded it since the developer
didn't responded fast enough.
This dns proxy has a nice feature that allows it to "blackhole" A and AAAA
queries for blacklisted domains.
I can define the IPv4 and IPv6 host which will be the "blackhole" and it's
all playing well with plain HTTP(port 80).
But with HTTPS I want to be able to intercept all traffic and pass it into
the http cache-peer.
I am not sure what would be the best way to do it with squid but I was
thinking about something like:
- peek client SNI
- bump client first(compared to server first)

And I can use the same Root CA key+certificate that exists on the main squid
on interception instance.
I am not sure what version of Squid-Cache to use for this test (3.5.27 or
4.0.21).
The main thing I am not sure about such a setup is that the target ip:443
would be the "blackhole" squid instance itself and not the original server
ip address.
Would it matter at all if the destination ip is the Squid instance on port
443?
(I will try to use iptables nat REDIRECT from port 443 to 23129 which would
be an intercept port)

Thanks,
Eliezer

----
Eliezer Croitoru
Linux System Administrator
Mobile: +972-5-28704261
Email: eliezer at ngtech.co.il






From squid3 at treenet.co.nz  Tue Sep 26 23:28:12 2017
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Wed, 27 Sep 2017 12:28:12 +1300
Subject: [squid-users] delay pool not workin
In-Reply-To: <75b9a49f-bfea-1c87-5698-8134ea1c011b@cinbesa.com.br>
References: <cfd8aaa5-730a-f824-89a8-96e59f87fe9f@dvm.esines.cu>
 <33e2fd6e-8548-5ba9-0b37-936a38a40fcf@treenet.co.nz>
 <75b9a49f-bfea-1c87-5698-8134ea1c011b@cinbesa.com.br>
Message-ID: <16b388fe-b486-f3a6-2522-2144ec5c422d@treenet.co.nz>

On 23/09/17 03:48, Heiler Bemerguy wrote:
> Amos, talking about delay pools, I have a question: does it work if the 
> content being served is on a cache peer?
> 

It should, yes. Peers are no different than any other server in terms of 
I/O bytes.

The only thing I'm aware of in current Squid is maybe bugs with CONNECT 
tunnels. Older Squid the pools were a bit broken - I've not had any 
recent feedback about those bugs, some are still open but may have been 
fixed as side effects of other changes.


> I think it only "shapes" traffic from a SERVER to squid, right? not from 
> a peer cache to squid.. :/

Peer is just a server with some statically configured parameters - 
traffic format, routing ACLs, etc.

> 
> I'm having problems because we use a huge Microsoft Updates repository 
> as a cache peer and whenever a client on a 512kbit/s link (!!!!!!!!!) 
> starts his box, all the link is flooded with updates from us to it.

WU .cab files should be cacheable objects. Delay pools do not apply to 
HIT traffic, and REFRESH traffic is intentionally *much* smaller in 
terms of bytes to the server. So you can end up with delay pool shaping 
1-2 KB of Squid<->server data and the client receiving GB sized files.




> htcp_access allow localnet
> acl wu dstdom_regex \.download\.windowsupdate\.com$

Sigh. The above is a complex and CPU intensive way to write:

  acl wu dstdomain .download.windowsupdate.com


Rule of thumb: when there is an alternative - avoid regex.


> acl wu-rejects dstdom_regex stats
> acl GET method GET
> cache_peer 10.1.10.10 parent 8081 0 proxy-only no-tproxy no-digest 
> no-query no-netdb-exchange name=ms1
> cache_peer_access ms1 allow GET wu !wu-rejects
> cache_peer_access ms1 deny all
> never_direct allow GET wu !wu-rejects
> never_direct deny all
> cache deny wu
> cache allow all
> 
> prefer_direct off
> 
> acl srcdaico src 10.71.0.0/16
> delay_pools 1
> delay_class 1 3
> delay_access 1 allow srcdaico !dstlocal

I suspect this dstlocal is the reason for the peer not being delayed. 
Check that the peer IP is not in any of its ranges.

Amos


From squid3 at treenet.co.nz  Tue Sep 26 23:31:19 2017
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Wed, 27 Sep 2017 12:31:19 +1300
Subject: [squid-users] delay pool not workin
In-Reply-To: <e44b476e-e2c7-047c-a0a1-7cf329f36c58@dvm.esines.cu>
References: <e44b476e-e2c7-047c-a0a1-7cf329f36c58@dvm.esines.cu>
Message-ID: <7ced1a30-d01b-be60-9b79-d87da7072d95@treenet.co.nz>

On 23/09/17 04:30, Alex Guti?rrez Mart?nez wrote:
> Pool #3 requires the domain name of a single transaction to
> simultaneously be *mail.yahoo.com AND *.linkedin.com AND *.youtube.com
>  ? Obviously that is impossible, so nothing can match the line that allows.
> 
> Pool #1 should match a few things. But probably not what you are testing
> with.
> 
> I suggest you try to re-write your ACLs in a simpler way with less '!'
> (not) modifiers. The way you are compressing lots of things into each
> line is no faster than multiple lines, but much harder to understand
> what is going on.
> 
> #######################################################################
> #######################################################################
> #######################################################################
> #######################################################################
> 
> Mr. Jeffries, I rewrite my acl?s on this ways:
> 
...
> 
> Every request fails, only delay pool 2 is on use, execpt for example # 
> 2, in that case every request was transfer to delay pool # 1.
> 
> Any suggestions?
> 

Lets go back to the beginning;

Write out your policy rules in human words for me please, then we can 
simplify that version of them before converting to Squid ACLs.

Amos


From eliezer at ngtech.co.il  Tue Sep 26 23:33:00 2017
From: eliezer at ngtech.co.il (Eliezer Croitoru)
Date: Wed, 27 Sep 2017 02:33:00 +0300
Subject: [squid-users] (no subject)
In-Reply-To: <1110594435.2155046.1506413743184@mail.yahoo.com>
References: <1110594435.2155046.1506413743184.ref@mail.yahoo.com>
 <1110594435.2155046.1506413743184@mail.yahoo.com>
Message-ID: <119801d3371f$ca69c6c0$5f3d5440$@ngtech.co.il>

Hey,

My recommendation about YouTube Caching is to use a special server that will store the YouTube videos locally.
I have created such a service which run's in on a Linux box and you can see the details at:
http://gogs.ngtech.co.il/elicro/youtube-store

I have not completed every tool that I wanted to publish it with since I need time and funds to make it all work together.

The idea is that squid will allow YouTube every YouTube action such as search and browsing but not YouTube videos playback directly from the Internet.
When a video have not been downloaded yet, the user will receive a notification\splash page that allows him to request a download of the video or will automatically queue the download.
(maybe with username and password protection..)
And in the case that the user will want to get back into the original YouTube page he would be able to access the page but not the video using a special link that will be displayed in the "YouTube video splash page".
In the case that the video exists in the local service\server the user\client would be able to watch the video from the local store web-server.

I am using it here locally with a tiny NAS solution and it works great.

Caching YouTube images should be pretty easy\simple using StoreID.

Let me know if you have interest in such a solution.

Eliezer

----
http://ngtech.co.il/lmgtfy/
Linux System Administrator
Mobile: +972-5-28704261
Email: eliezer at ngtech.co.il


From: squid-users [mailto:squid-users-bounces at lists.squid-cache.org] On Behalf Of Sukhbaatar T
Sent: Tuesday, September 26, 2017 11:16
To: squid-users at lists.squid-cache.org
Subject: [squid-users] (no subject)

Hello.
I'm a teacher.
My computer's OS is Windows 7.
Installed SQUID 3.5.27.
We have fifteen computers in our cabinet.
The internet bandwidth is 4mb. We need to cache youtube and fb data of our kids.
The youtube cache needs to be saved for a month, and the facebook to remain for a day.
Our configuration is this small.
Can you do the SQUID options for this (squid.conf file)?

Thank you



From squid3 at treenet.co.nz  Tue Sep 26 23:46:14 2017
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Wed, 27 Sep 2017 12:46:14 +1300
Subject: [squid-users] Bug: Missing MemObject::storeId value
In-Reply-To: <CANAZdzU7w_TNS7pkUXQp6o7_=+c6RGFNQ2j-Lng4nP+7DYpF8Q@mail.gmail.com>
References: <CANAZdzXddzer-Bqf4jU0J2GbdRoixp8BSmoKvBFD-30kt9aWcw@mail.gmail.com>
 <0ccf01d3362e$8141e050$83c5a0f0$@ngtech.co.il>
 <CANAZdzVHj_hbmv=8ChDUSH_-UOHFPNc3VUurmiVROeJOMAtwYg@mail.gmail.com>
 <0dfa01d3367e$fb8a9960$f29fcc20$@ngtech.co.il>
 <CANAZdzWzHOqBCP8Put405Xv6i2R1incmv7q6yykY=8cqJm4Niw@mail.gmail.com>
 <CANAZdzU7w_TNS7pkUXQp6o7_=+c6RGFNQ2j-Lng4nP+7DYpF8Q@mail.gmail.com>
Message-ID: <a3c5952c-8871-4664-5116-57e9228a0b31@treenet.co.nz>

On 27/09/17 10:30, Aaron Turner wrote:
> Doing some basic system tests and we're seeing a bunch of errors like:
> 
> 2017/09/22 22:43:15 kid1| Bug: Missing MemObject::storeId value
> 2017/09/22 22:43:15 kid1| mem_hdr: 0x7f169d0a2a70 nodes.start() 0x7f169c6cc9d0
> 2017/09/22 22:43:15 kid1| mem_hdr: 0x7f169d0a2a70 nodes.finish() 0x7f169dae4e40
> 2017/09/22 22:43:15 kid1| MemObject->start_ping: 0.000000
> 2017/09/22 22:43:15 kid1| MemObject->inmem_hi: 20209
> 2017/09/22 22:43:15 kid1| MemObject->inmem_lo: 0
> 2017/09/22 22:43:15 kid1| MemObject->nclients: 0
> 2017/09/22 22:43:15 kid1| MemObject->reply: 0x7f167ee60db0
> 2017/09/22 22:43:15 kid1| MemObject->request: 0
> 2017/09/22 22:43:15 kid1| MemObject->logUri:
> 2017/09/22 22:43:15 kid1| MemObject->storeId:
> 2017/09/22 22:43:15 kid1| Bug: Missing MemObject::storeId value
> 2017/09/22 22:43:15 kid1| mem_hdr: 0x7f16a0388760 nodes.start() 0x7f16a6a4a500
> 2017/09/22 22:43:15 kid1| mem_hdr: 0x7f16a0388760 nodes.finish() 0x7f16a6a4a4d0
> 2017/09/22 22:43:15 kid1| MemObject->start_ping: 0.000000
> 2017/09/22 22:43:15 kid1| MemObject->inmem_hi: 50265
> 2017/09/22 22:43:15 kid1| MemObject->inmem_lo: 0
> 2017/09/22 22:43:15 kid1| MemObject->nclients: 0
> 2017/09/22 22:43:15 kid1| MemObject->reply: 0x7f169f83d7d0
> 2017/09/22 22:43:15 kid1| MemObject->request: 0
> 2017/09/22 22:43:15 kid1| MemObject->logUri:
> 2017/09/22 22:43:15 kid1| MemObject->storeId:


This is <http://bugs.squid-cache.org/show_bug.cgi?id=4527>

Amos


From synfinatic at gmail.com  Tue Sep 26 23:55:27 2017
From: synfinatic at gmail.com (Aaron Turner)
Date: Tue, 26 Sep 2017 16:55:27 -0700
Subject: [squid-users] Bug: Missing MemObject::storeId value
In-Reply-To: <a3c5952c-8871-4664-5116-57e9228a0b31@treenet.co.nz>
References: <CANAZdzXddzer-Bqf4jU0J2GbdRoixp8BSmoKvBFD-30kt9aWcw@mail.gmail.com>
 <0ccf01d3362e$8141e050$83c5a0f0$@ngtech.co.il>
 <CANAZdzVHj_hbmv=8ChDUSH_-UOHFPNc3VUurmiVROeJOMAtwYg@mail.gmail.com>
 <0dfa01d3367e$fb8a9960$f29fcc20$@ngtech.co.il>
 <CANAZdzWzHOqBCP8Put405Xv6i2R1incmv7q6yykY=8cqJm4Niw@mail.gmail.com>
 <CANAZdzU7w_TNS7pkUXQp6o7_=+c6RGFNQ2j-Lng4nP+7DYpF8Q@mail.gmail.com>
 <a3c5952c-8871-4664-5116-57e9228a0b31@treenet.co.nz>
Message-ID: <CANAZdzUsewLTFrTxEy-Xrkaa2UiXbzeEGr3Tof3+4nHNuT07sw@mail.gmail.com>

So reading the bug comments it doesn't sound like there's any config
changes I can make (other then not use rock, which in smp doesn't
sound like a good idea).   I might be able to run ALL,9 and collect
the output... would need to sanitize the URL's due to privacy/security
concerns.  Anything else I can/should do/consider?

Honestly, I'm not sure what the impact of this bug really is?  Is it
just a cache miss or???
--
Aaron Turner
https://synfin.net/         Twitter: @synfinatic
My father once told me that respect for the truth comes close to being
the basis for all morality.  "Something cannot emerge from nothing,"
he said.  This is profound thinking if you understand how unstable
"the truth" can be.  -- Frank Herbert, Dune


On Tue, Sep 26, 2017 at 4:46 PM, Amos Jeffries <squid3 at treenet.co.nz> wrote:
> On 27/09/17 10:30, Aaron Turner wrote:
>>
>> Doing some basic system tests and we're seeing a bunch of errors like:
>>
>> 2017/09/22 22:43:15 kid1| Bug: Missing MemObject::storeId value
>> 2017/09/22 22:43:15 kid1| mem_hdr: 0x7f169d0a2a70 nodes.start()
>> 0x7f169c6cc9d0
>> 2017/09/22 22:43:15 kid1| mem_hdr: 0x7f169d0a2a70 nodes.finish()
>> 0x7f169dae4e40
>> 2017/09/22 22:43:15 kid1| MemObject->start_ping: 0.000000
>> 2017/09/22 22:43:15 kid1| MemObject->inmem_hi: 20209
>> 2017/09/22 22:43:15 kid1| MemObject->inmem_lo: 0
>> 2017/09/22 22:43:15 kid1| MemObject->nclients: 0
>> 2017/09/22 22:43:15 kid1| MemObject->reply: 0x7f167ee60db0
>> 2017/09/22 22:43:15 kid1| MemObject->request: 0
>> 2017/09/22 22:43:15 kid1| MemObject->logUri:
>> 2017/09/22 22:43:15 kid1| MemObject->storeId:
>> 2017/09/22 22:43:15 kid1| Bug: Missing MemObject::storeId value
>> 2017/09/22 22:43:15 kid1| mem_hdr: 0x7f16a0388760 nodes.start()
>> 0x7f16a6a4a500
>> 2017/09/22 22:43:15 kid1| mem_hdr: 0x7f16a0388760 nodes.finish()
>> 0x7f16a6a4a4d0
>> 2017/09/22 22:43:15 kid1| MemObject->start_ping: 0.000000
>> 2017/09/22 22:43:15 kid1| MemObject->inmem_hi: 50265
>> 2017/09/22 22:43:15 kid1| MemObject->inmem_lo: 0
>> 2017/09/22 22:43:15 kid1| MemObject->nclients: 0
>> 2017/09/22 22:43:15 kid1| MemObject->reply: 0x7f169f83d7d0
>> 2017/09/22 22:43:15 kid1| MemObject->request: 0
>> 2017/09/22 22:43:15 kid1| MemObject->logUri:
>> 2017/09/22 22:43:15 kid1| MemObject->storeId:
>
>
>
> This is <http://bugs.squid-cache.org/show_bug.cgi?id=4527>
>
> Amos
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users


From squid3 at treenet.co.nz  Wed Sep 27 02:10:45 2017
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Wed, 27 Sep 2017 15:10:45 +1300
Subject: [squid-users] Bug: Missing MemObject::storeId value
In-Reply-To: <CANAZdzUsewLTFrTxEy-Xrkaa2UiXbzeEGr3Tof3+4nHNuT07sw@mail.gmail.com>
References: <CANAZdzXddzer-Bqf4jU0J2GbdRoixp8BSmoKvBFD-30kt9aWcw@mail.gmail.com>
 <0ccf01d3362e$8141e050$83c5a0f0$@ngtech.co.il>
 <CANAZdzVHj_hbmv=8ChDUSH_-UOHFPNc3VUurmiVROeJOMAtwYg@mail.gmail.com>
 <0dfa01d3367e$fb8a9960$f29fcc20$@ngtech.co.il>
 <CANAZdzWzHOqBCP8Put405Xv6i2R1incmv7q6yykY=8cqJm4Niw@mail.gmail.com>
 <CANAZdzU7w_TNS7pkUXQp6o7_=+c6RGFNQ2j-Lng4nP+7DYpF8Q@mail.gmail.com>
 <a3c5952c-8871-4664-5116-57e9228a0b31@treenet.co.nz>
 <CANAZdzUsewLTFrTxEy-Xrkaa2UiXbzeEGr3Tof3+4nHNuT07sw@mail.gmail.com>
Message-ID: <f74d83d3-3dc3-7f52-d91b-1fb9637ed9d4@treenet.co.nz>


On 27/09/17 12:55, Aaron Turner wrote:
> So reading the bug comments it doesn't sound like there's any config
> changes I can make (other then not use rock, which in smp doesn't
> sound like a good idea).   I might be able to run ALL,9 and collect
> the output... would need to sanitize the URL's due to privacy/security
> concerns.  Anything else I can/should do/consider?
> 
> Honestly, I'm not sure what the impact of this bug really is?  Is it
> just a cache miss or???

As far as I know yes.

Amos


From veiko at linux.ee  Wed Sep 27 09:46:44 2017
From: veiko at linux.ee (Veiko Kukk)
Date: Wed, 27 Sep 2017 12:46:44 +0300
Subject: [squid-users] Cache digest vs ICP
Message-ID: <CAG8vQeNHe1TNT4=b1JcLDu7hmCWvBTwkgUkZLKUezmQBnonQJQ@mail.gmail.com>

Hi,

We have cluster of squids in reverse proxy mode. Each one of those is
sibling to others and they all have same originservers as parents. Siblings
are configured with no-proxy keyword to achieve that they don't cache what
other siblings already have in their cache. This is to minimize data usage
costs from origin servers. What is in our cluster should never be fetched
again from origin because it never changes. It's not typical web cache,
it's CDN system for content that we create and control and squid is just
one of the internal parts and not exposed directly to the clients.

So far digest_generation has been set to off and only ICP has been used
between siblings. Mostly because digest stats had shown many rejects (not
containing 100% of cache objects) and documentation about digests is
confusing up to statements that while rebuilding digest, squid will stop
serving requests.

Since we need to have more siblings and more far away from each other, ICP
overhead becomes an issue (time spent on query). Having proper digest with
all of the objects in cache_dir included in digest could be better solution
due to not having delay of initial ICP request. Digest documentation states
that it's including based on refresh_pattern. It's a problem because to get
squid working as we want, we had to use offline_mode on.

Questions:
* What is the relationship between cache digests and ICP? If they are
active together, how are they used together?
* How are objects added to digest when rebuilding? Does this include lot of
disk i/o like scanning all cache_dir files or is it based on swap.state
contents?
* How can i see which objects are listed in cache digest?
* Why does sibling false positive result in sending client 504 and not
trying next sibling or parent? CD_SIBLING_HIT/192.168.1.52  TCP_MISS/504.
How to achieve proceeding with next cache_peer?

Best regards,
Veiko
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20170927/438bdfce/attachment.htm>

From rentorbuy at yahoo.com  Wed Sep 27 09:51:59 2017
From: rentorbuy at yahoo.com (Vieri)
Date: Wed, 27 Sep 2017 09:51:59 +0000 (UTC)
Subject: [squid-users] squid release number and patches
References: <2125956312.10189025.1506505919529.ref@mail.yahoo.com>
Message-ID: <2125956312.10189025.1506505919529@mail.yahoo.com>

Hi,

I use Squid on Gentoo.

Gentoo's package manager (ebuilds) installs only a few Squid releases. It does not support 3.5 patch releases or Squid 4 beta.

I customized Gentoo's Squid "ebuild" to support the latter. It was working fine while www.squid-cache.org named the archives with integers in the patch "release" version.
eg. I could install custom versions with these ebuilds:
squid-3.5.26_p2017070214182.ebuild -> grabbed Squid 3.5.26, patch 20170702 r14182 (assuming the latter always starts with 'r')

squid-4.0.17_beta_p2017012214968.ebuild -> grabbed Squid 4 beta, patch 20170122 r14968


However, the naming has changed upstream so ebuilds such as:

squid-3.5.27_p20170916e69e56c.ebuild

fail to install squid-3.5.27-20170916-re69e56c.tar.gz because the _p part is required to be a number.

Of course, I could change the Gentoo ebuild to include the patch version numbers within, but that would require more maintenance than a simple file name renaming.

Is it possible to revert to using http://www.squid-cache.org/Versions/v3/3.5/squid-3.5.x-<DATE>-r<INTEGER>.tar.gz file names?

Thanks,

Vieri


From chip_pop at hotmail.com  Wed Sep 27 13:57:34 2017
From: chip_pop at hotmail.com (joseph)
Date: Wed, 27 Sep 2017 06:57:34 -0700 (MST)
Subject: [squid-users] squid5 build error
Message-ID: <1506520654560-0.post@n4.nabble.com>

latest debian and latest gcc in repo
squid-5.0.0-20170919-r478fb99.tar.gz
gcc version 6.3.0 20170516 (Debian 6.3.0-18) 

any idea  or  its  some code need to be converted to work with latest gcc ?? 

gadgets.cc: In function ???const ASN1_BIT_STRING*
Ssl::X509_get_signature(const CertPointer&)???:
gadgets.cc:960:25: error: invalid conversion from ???ASN1_BIT_STRING** {aka
asn1_string_st**}??? to ???const ASN1_BIT_STRING** {aka const
asn1_string_st**}??? [-fpermissive]
     X509_get0_signature(&sig, &sig_alg, cert.get());
                         ^~~~
In file included from /usr/include/openssl/ssl.h:50:0,
                 from ../../src/security/Context.h:16,
                 from ../../src/security/forward.h:13,
                 from ../../src/ssl/gadgets.h:13,
                 from gadgets.cc:10:
/usr/include/openssl/x509.h:552:6: note:   initializing argument 1 of
???void X509_get0_signature(const ASN1_BIT_STRING**, const X509_ALGOR**,
const X509*)???
 void X509_get0_signature(const ASN1_BIT_STRING **psig,
      ^~~~~~~~~~~~~~~~~~~
gadgets.cc:960:31: error: invalid conversion from ???X509_ALGOR** {aka
X509_algor_st**}??? to ???const X509_ALGOR** {aka const X509_algor_st**}???
[-fpermissive]
     X509_get0_signature(&sig, &sig_alg, cert.get());
                               ^~~~~~~~
In file included from /usr/include/openssl/ssl.h:50:0,
                 from ../../src/security/Context.h:16,
                 from ../../src/security/forward.h:13,
                 from ../../src/ssl/gadgets.h:13,
                 from gadgets.cc:10:
/usr/include/openssl/x509.h:552:6: note:   initializing argument 2 of
???void X509_get0_signature(const ASN1_BIT_STRING**, const X509_ALGOR**,
const X509*)???
 void X509_get0_signature(const ASN1_BIT_STRING **psig,
      ^~~~~~~~~~~~~~~~~~~




--
Sent from: http://squid-web-proxy-cache.1019090.n4.nabble.com/Squid-Users-f1019091.html


From rousskov at measurement-factory.com  Wed Sep 27 15:06:07 2017
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Wed, 27 Sep 2017 09:06:07 -0600
Subject: [squid-users] Cache digest vs ICP
In-Reply-To: <CAG8vQeNHe1TNT4=b1JcLDu7hmCWvBTwkgUkZLKUezmQBnonQJQ@mail.gmail.com>
References: <CAG8vQeNHe1TNT4=b1JcLDu7hmCWvBTwkgUkZLKUezmQBnonQJQ@mail.gmail.com>
Message-ID: <5f025eaa-364e-e742-7532-c25a9575eed6@measurement-factory.com>

On 09/27/2017 03:46 AM, Veiko Kukk wrote:

> Siblings are configured with no-proxy keyword to achieve that they don't
> cache what other siblings already have in their cache. 

I assume that by "no-proxy" you meant "proxy-only".


> This is to minimize data usage costs from origin servers. 

The proxy-only option does not minimize the amount of data transmitted
between a proxy and the origin server. It reduces cache duplication
among cache peers.


> So far digest_generation has been set to off and only ICP has been used
> between siblings. Mostly because digest stats had shown many rejects
> (not containing 100% of cache objects) and documentation about digests
> is confusing up to statements that while rebuilding digest, squid will
> stop serving requests.

Please point me to the location of that statement. IMHO, it is not
confusing but incorrect. Non-SMP Squid stops servicing requests while
rebuilding a cache digest _chunk_, not the entire digest (unless the
digest is configured to have only one chunk, of course). The size if the
chunk is controlled by digest_rebuild_chunk_percentage.

Please note that non-SMP Squid stops servicing other requests when doing
virtually anything -- Squid is not threaded. The reason cache digests
are somewhat "special" in this context is because rebuilding the entire
digest may take a long time for large caches. Squid combats that by
splitting the digest rebuild process into chunks (a misleading term!),
digesting at most digest_rebuild_chunk_percentage of cached objects at a
time.

Cache Digests are not SMP aware (but should be). You may be able to work
around that limitation using SMP macros, but I have not tested that. I
do not remember whether a worker that is not configured to generate a
digest will still look it up in the cache when a peer asks for it.
Hopefully, the worker will do that lookup.


> Digest
> documentation states that it's including based on refresh_pattern. It's
> a problem because to get squid working as we want, we had to use
> offline_mode on.

If Cache Digests do not honor offline_mode, it is a (staleness
estimation code) bug that should be reported and fixed.

Meanwhile, does refresh_pattern stop working when offline_mode is on? If
not, then can you use refresh_pattern to emulate offline_mode effects
while still using offline_mode?


> * What is the relationship between cache digests and ICP?

IIRC, none, except the former is checked before the latter.


> If they are active together, how are they used together?

I have not tested this, but Cache Digests ought to be checked first, and
if they miss, then Squid should proceed to ICP/HTCP/etc. AFAICT, a Cache
Digest miss has no effect on other peer selection algorithms.


> * How are objects added to digest when rebuilding? Does this include lot
> of disk i/o like scanning all cache_dir files or is it based on
> swap.state contents?

Objects are digested based on the in-RAM cache index. There is no disk
I/O involved until the built digest needs to be stored on disk.


> * How can i see which objects are listed in cache digest?

A Cache Digest does not list/store object URLs -- it cannot produce a
list of previously digested objects. The only way to find out whether
object X was digested (with some degree of certainty) is to query the
digest for that object X.

I am not aware of any command-line interface for interrogating digests,
but it is certainly possible to build one. Please note that Squid
includes both the URL and the method into the object cache key (which is
what ends up being hashed by the Cache Digests code).


> * Why does sibling false positive result in sending client 504 and not
> trying next sibling or parent??CD_SIBLING_HIT/192.168.1.52
> TCP_MISS/504. How to achieve proceeding with next cache_peer?

Sounds like bug #4223 to me:
http://bugs.squid-cache.org/show_bug.cgi?id=4223


HTH,

Alex.


From rousskov at measurement-factory.com  Wed Sep 27 15:14:19 2017
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Wed, 27 Sep 2017 09:14:19 -0600
Subject: [squid-users] squid5 build error
In-Reply-To: <1506520654560-0.post@n4.nabble.com>
References: <1506520654560-0.post@n4.nabble.com>
Message-ID: <82a619a6-2a42-d278-049a-7aeaa6a142b5@measurement-factory.com>

On 09/27/2017 07:57 AM, joseph wrote:
> latest debian and latest gcc in repo
> squid-5.0.0-20170919-r478fb99.tar.gz
> gcc version 6.3.0 20170516 (Debian 6.3.0-18) 

Looks like an OpenSSL compatibility problem to me. Amos has fixed a
similar one recently but evidently that fix is not enough:
https://github.com/squid-cache/squid/commit/70cfe22f6f44316ad516135af38fb7b130034bd6

What is your OpenSSL (or equivalent) version?

Alex.


> gadgets.cc: In function ???const ASN1_BIT_STRING*
> Ssl::X509_get_signature(const CertPointer&)???:
> gadgets.cc:960:25: error: invalid conversion from ???ASN1_BIT_STRING** {aka
> asn1_string_st**}??? to ???const ASN1_BIT_STRING** {aka const
> asn1_string_st**}??? [-fpermissive]
>      X509_get0_signature(&sig, &sig_alg, cert.get());
>                          ^~~~
> In file included from /usr/include/openssl/ssl.h:50:0,
>                  from ../../src/security/Context.h:16,
>                  from ../../src/security/forward.h:13,
>                  from ../../src/ssl/gadgets.h:13,
>                  from gadgets.cc:10:
> /usr/include/openssl/x509.h:552:6: note:   initializing argument 1 of
> ???void X509_get0_signature(const ASN1_BIT_STRING**, const X509_ALGOR**,
> const X509*)???
>  void X509_get0_signature(const ASN1_BIT_STRING **psig,
>       ^~~~~~~~~~~~~~~~~~~
> gadgets.cc:960:31: error: invalid conversion from ???X509_ALGOR** {aka
> X509_algor_st**}??? to ???const X509_ALGOR** {aka const X509_algor_st**}???
> [-fpermissive]
>      X509_get0_signature(&sig, &sig_alg, cert.get());
>                                ^~~~~~~~
> In file included from /usr/include/openssl/ssl.h:50:0,
>                  from ../../src/security/Context.h:16,
>                  from ../../src/security/forward.h:13,
>                  from ../../src/ssl/gadgets.h:13,
>                  from gadgets.cc:10:
> /usr/include/openssl/x509.h:552:6: note:   initializing argument 2 of
> ???void X509_get0_signature(const ASN1_BIT_STRING**, const X509_ALGOR**,
> const X509*)???
>  void X509_get0_signature(const ASN1_BIT_STRING **psig,
>       ^~~~~~~~~~~~~~~~~~~


From hirsch098 at gmail.com  Wed Sep 27 17:46:46 2017
From: hirsch098 at gmail.com (stern0m1)
Date: Wed, 27 Sep 2017 10:46:46 -0700 (MST)
Subject: [squid-users] proof of concept squid injecting js even with pinned
	ca
Message-ID: <1506534406914-0.post@n4.nabble.com>

Hi, 
I am new to squid, please pardon me if this has already been discussed. 

Is there a proof of concept somewhere to successfully use squid as a
transparent proxy for  a mitm attack to inject js for sites/applications
with pinned a pinned certificate? 

Thanks 




--
Sent from: http://squid-web-proxy-cache.1019090.n4.nabble.com/Squid-Users-f1019091.html


From ivanlmj at gmail.com  Wed Sep 27 20:01:49 2017
From: ivanlmj at gmail.com (ivanleoncz)
Date: Wed, 27 Sep 2017 13:01:49 -0700 (MST)
Subject: [squid-users] Blocking HTTPS On Transparent/Interception Proxy
	Configuration
Message-ID: <1506542509063-0.post@n4.nabble.com>

Hello, Squid Users.

I'm not an experienced user for advanced configurations on Squid, so I need
some advice or help, which will be much appreciated.

As I was watching some of the logs from my Proxy, I noticed that there are
requests that are made first via HTTP, and the remote Web Server responds
with a 302 redirect to a HTTPS site.

I can use Facebook as an example:

++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
1505162176.649    102 192.168.0.108 TCP_MISS/204 257 GET
http://b-www.facebook.com/mobile/status.php - ORIGINAL_DST/31.13.66.37
text/plain
1505233881.293    176 192.168.0.149 TCP_MISS/302 387 GET
http://www.facebook.com/ - ORIGINAL_DST/31.13.66.36 text/html
1505240198.118    162 192.168.0.149 TCP_MISS/302 387 GET
http://www.facebook.com/ - ORIGINAL_DST/31.13.66.36 text/html
1505241490.335    203 192.168.0.149 TCP_MISS/302 387 GET
http://www.facebook.com/ - ORIGINAL_DST/157.240.3.35 text/html
1505248976.884    173 192.168.0.54 TCP_MISS/302 562 GET
http://www.facebook.com/plugins/like.php? - ORIGINAL_DST/31.13.66.36
text/html
1505303537.048    144 192.168.0.152 TCP_MISS/302 382 GET
http://www.facebook.com/ - ORIGINAL_DST/31.13.66.36 text/html
1505331296.129    181 192.168.0.108 TCP_MISS/302 635 GET
http://www.facebook.com/plugins/like.php? - ORIGINAL_DST/31.13.66.36
text/html
1505389662.830    144 192.168.0.152 TCP_MISS/302 382 GET
http://www.facebook.com/ - ORIGINAL_DST/157.240.17.35 text/html
1505393796.724    187 192.168.0.165 TCP_MISS/302 387 GET
http://www.facebook.com/ - ORIGINAL_DST/31.13.66.36 text/html
1505481730.533    145 192.168.0.74 TCP_MISS/302 484 GET
http://www.facebook.com/plugins/fan.php? - ORIGINAL_DST/157.240.17.35
text/html
1505756711.632    221 192.168.0.76 TCP_MISS/302 671 GET
http://www.facebook.com/plugins/likebox.php? - ORIGINAL_DST/31.13.66.36
text/html
1505849677.484    190 192.168.0.56 TCP_MISS/302 532 GET
http://www.facebook.com/plugins/like.php? - ORIGINAL_DST/31.13.66.36
text/html
1505913883.386    166 192.168.0.152 TCP_MISS/302 382 GET
http://www.facebook.com/ - ORIGINAL_DST/157.240.17.35 text/html
1505926185.493    146 192.168.0.56 TCP_MISS/302 532 GET
http://www.facebook.com/plugins/like.php? - ORIGINAL_DST/31.13.66.36
text/html
1506089311.489    152 192.168.0.62 TCP_MISS/302 587 GET
http://www.facebook.com/plugins/likebox.php? - ORIGINAL_DST/157.240.17.35
text/html
1506102859.349    171 192.168.0.41 TCP_MISS/302 528 GET
http://www.facebook.com/plugins/follow.php? - ORIGINAL_DST/157.240.3.35
text/html
1506449027.644    126 192.168.0.72 TCP_MISS/302 567 GET
http://www.facebook.com/plugins/like.php? - ORIGINAL_DST/157.240.17.35
text/html
1506458858.890    244 192.168.0.54 TCP_MISS/302 562 GET
http://www.facebook.com/plugins/like.php? - ORIGINAL_DST/157.240.3.35
text/html
1506531664.419    137 192.168.0.152 TCP_MISS/302 382 GET
http://www.facebook.com/ - ORIGINAL_DST/31.13.66.36 text/html
++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
  
With these logs, I can understand that a first request is made via HTTP and
a redirect is going to be performed. Am I right?

Seems like the same applies for other sites like YouTube, for example:

++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
1506454619.784    129 192.168.0.68 TCP_MISS/302 908 GET
http://www.youtube.com/ - ORIGINAL_DST/172.217.7.46 text/html
1506454859.606    127 192.168.0.68 TCP_MISS/302 908 GET
http://www.youtube.com/ - ORIGINAL_DST/172.217.7.46 text/html
1506455555.686    189 192.168.0.68 TCP_MISS/302 908 GET
http://www.youtube.com/ - ORIGINAL_DST/172.217.5.174 text/html
1506455678.559    181 192.168.0.68 TCP_MISS/302 908 GET
http://www.youtube.com/ - ORIGINAL_DST/172.217.7.46 text/html
1506455887.214    158 192.168.0.68 TCP_MISS/302 908 GET
http://www.youtube.com/ - ORIGINAL_DST/216.58.193.14 text/html
1506456578.142    127 192.168.0.68 TCP_MISS/302 908 GET
http://www.youtube.com/ - ORIGINAL_DST/172.217.5.174 text/html
1506457019.837    123 192.168.0.68 TCP_MISS/302 908 GET
http://www.youtube.com/ - ORIGINAL_DST/172.217.7.46 text/html
1506457532.332    110 192.168.0.68 TCP_MISS/302 908 GET
http://www.youtube.com/ - ORIGINAL_DST/216.58.193.46 text/html
1506457735.088    108 192.168.0.68 TCP_MISS/302 908 GET
http://www.youtube.com/ - ORIGINAL_DST/216.58.193.46 text/html
++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

Since that the first request is via HTTP, I was wondering:
 
    /- Why I cannot just deny the access for a site like "www.facebook.com",
"facebook.com", "youtube.com", etc.?/

If I cannot perform something like this, I'd like to know: 

    /- Is there any way or mechanism that can be used on Squid for blocking
HTTPS sites, that were originally accessed via 302 redirect?/

I know that there are tons of blogs, forums, etc., that they recommend
theusage of SSLBump, but I also know that MITM is not a good choice, since
that it's (or it could be) illegal, to eavesdrop a secure connection. So I
believe that SSL Bump is not an option.

Thank you all for the attention.

Best Regards,
@ivanleoncz




--
Sent from: http://squid-web-proxy-cache.1019090.n4.nabble.com/Squid-Users-f1019091.html


From eliezer at ngtech.co.il  Wed Sep 27 20:17:10 2017
From: eliezer at ngtech.co.il (Eliezer Croitoru)
Date: Wed, 27 Sep 2017 23:17:10 +0300
Subject: [squid-users] Blocking HTTPS On Transparent/Interception
	Proxy	Configuration
In-Reply-To: <1506542509063-0.post@n4.nabble.com>
References: <1506542509063-0.post@n4.nabble.com>
Message-ID: <151301d337cd$996da740$cc48f5c0$@ngtech.co.il>

Hey,

Can you clarify what do you want to achieve eventually?
If you want to block youtube or facebook I can recommend you on other solutions then in the application level.
The next repository:
https://github.com/vel21ripn/nDPI

Implements some level of deep packet inspection without the existence of a full fledged proxy and does the filtering in the kernel level.
Depends on the OS you are using you would be able to either compile or acquire the module and libraries that will allow you to block youtube and\or facebook.
Take a peek at the wiki of the module at:
https://github.com/vel21ripn/nDPI/wiki

I have published a package for CentOS 7 named "kmod-xt_ndpi" at:
http://ngtech.co.il/repo/centos/7/x86_64/kmod-xt_ndpi-2.0.1-2.el7.centos.x86_64.rpm

And just notice that the 2.0.1 is the 1.7 stable nDPI module but the version number is for the package and not the module version.

Another solution would be to maintain an iptables+ipset setup that detects access to facebook or youtube and block these.

If you will give more details on the scenario we might be able to offer a more efficient solution.
Also to block facebook and youtube traffic using ssl-bump you don't need to bump and run full MITM for all traffic but just for youtube or facebook requests.

All The Bests,
Eliezer

* let me know if you need more help. 

----
Eliezer Croitoru
Linux System Administrator
Mobile: +972-5-28704261
Email: eliezer at ngtech.co.il



-----Original Message-----
From: squid-users [mailto:squid-users-bounces at lists.squid-cache.org] On Behalf Of ivanleoncz
Sent: Wednesday, September 27, 2017 23:02
To: squid-users at lists.squid-cache.org
Subject: [squid-users] Blocking HTTPS On Transparent/Interception Proxy Configuration

Hello, Squid Users.

I'm not an experienced user for advanced configurations on Squid, so I need
some advice or help, which will be much appreciated.

As I was watching some of the logs from my Proxy, I noticed that there are
requests that are made first via HTTP, and the remote Web Server responds
with a 302 redirect to a HTTPS site.

I can use Facebook as an example:

++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
1505162176.649    102 192.168.0.108 TCP_MISS/204 257 GET
http://b-www.facebook.com/mobile/status.php - ORIGINAL_DST/31.13.66.37
text/plain
1505233881.293    176 192.168.0.149 TCP_MISS/302 387 GET
http://www.facebook.com/ - ORIGINAL_DST/31.13.66.36 text/html
1505240198.118    162 192.168.0.149 TCP_MISS/302 387 GET
http://www.facebook.com/ - ORIGINAL_DST/31.13.66.36 text/html
1505241490.335    203 192.168.0.149 TCP_MISS/302 387 GET
http://www.facebook.com/ - ORIGINAL_DST/157.240.3.35 text/html
1505248976.884    173 192.168.0.54 TCP_MISS/302 562 GET
http://www.facebook.com/plugins/like.php? - ORIGINAL_DST/31.13.66.36
text/html
1505303537.048    144 192.168.0.152 TCP_MISS/302 382 GET
http://www.facebook.com/ - ORIGINAL_DST/31.13.66.36 text/html
1505331296.129    181 192.168.0.108 TCP_MISS/302 635 GET
http://www.facebook.com/plugins/like.php? - ORIGINAL_DST/31.13.66.36
text/html
1505389662.830    144 192.168.0.152 TCP_MISS/302 382 GET
http://www.facebook.com/ - ORIGINAL_DST/157.240.17.35 text/html
1505393796.724    187 192.168.0.165 TCP_MISS/302 387 GET
http://www.facebook.com/ - ORIGINAL_DST/31.13.66.36 text/html
1505481730.533    145 192.168.0.74 TCP_MISS/302 484 GET
http://www.facebook.com/plugins/fan.php? - ORIGINAL_DST/157.240.17.35
text/html
1505756711.632    221 192.168.0.76 TCP_MISS/302 671 GET
http://www.facebook.com/plugins/likebox.php? - ORIGINAL_DST/31.13.66.36
text/html
1505849677.484    190 192.168.0.56 TCP_MISS/302 532 GET
http://www.facebook.com/plugins/like.php? - ORIGINAL_DST/31.13.66.36
text/html
1505913883.386    166 192.168.0.152 TCP_MISS/302 382 GET
http://www.facebook.com/ - ORIGINAL_DST/157.240.17.35 text/html
1505926185.493    146 192.168.0.56 TCP_MISS/302 532 GET
http://www.facebook.com/plugins/like.php? - ORIGINAL_DST/31.13.66.36
text/html
1506089311.489    152 192.168.0.62 TCP_MISS/302 587 GET
http://www.facebook.com/plugins/likebox.php? - ORIGINAL_DST/157.240.17.35
text/html
1506102859.349    171 192.168.0.41 TCP_MISS/302 528 GET
http://www.facebook.com/plugins/follow.php? - ORIGINAL_DST/157.240.3.35
text/html
1506449027.644    126 192.168.0.72 TCP_MISS/302 567 GET
http://www.facebook.com/plugins/like.php? - ORIGINAL_DST/157.240.17.35
text/html
1506458858.890    244 192.168.0.54 TCP_MISS/302 562 GET
http://www.facebook.com/plugins/like.php? - ORIGINAL_DST/157.240.3.35
text/html
1506531664.419    137 192.168.0.152 TCP_MISS/302 382 GET
http://www.facebook.com/ - ORIGINAL_DST/31.13.66.36 text/html
++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
  
With these logs, I can understand that a first request is made via HTTP and
a redirect is going to be performed. Am I right?

Seems like the same applies for other sites like YouTube, for example:

++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
1506454619.784    129 192.168.0.68 TCP_MISS/302 908 GET
http://www.youtube.com/ - ORIGINAL_DST/172.217.7.46 text/html
1506454859.606    127 192.168.0.68 TCP_MISS/302 908 GET
http://www.youtube.com/ - ORIGINAL_DST/172.217.7.46 text/html
1506455555.686    189 192.168.0.68 TCP_MISS/302 908 GET
http://www.youtube.com/ - ORIGINAL_DST/172.217.5.174 text/html
1506455678.559    181 192.168.0.68 TCP_MISS/302 908 GET
http://www.youtube.com/ - ORIGINAL_DST/172.217.7.46 text/html
1506455887.214    158 192.168.0.68 TCP_MISS/302 908 GET
http://www.youtube.com/ - ORIGINAL_DST/216.58.193.14 text/html
1506456578.142    127 192.168.0.68 TCP_MISS/302 908 GET
http://www.youtube.com/ - ORIGINAL_DST/172.217.5.174 text/html
1506457019.837    123 192.168.0.68 TCP_MISS/302 908 GET
http://www.youtube.com/ - ORIGINAL_DST/172.217.7.46 text/html
1506457532.332    110 192.168.0.68 TCP_MISS/302 908 GET
http://www.youtube.com/ - ORIGINAL_DST/216.58.193.46 text/html
1506457735.088    108 192.168.0.68 TCP_MISS/302 908 GET
http://www.youtube.com/ - ORIGINAL_DST/216.58.193.46 text/html
++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

Since that the first request is via HTTP, I was wondering:
 
    /- Why I cannot just deny the access for a site like "www.facebook.com",
"facebook.com", "youtube.com", etc.?/

If I cannot perform something like this, I'd like to know: 

    /- Is there any way or mechanism that can be used on Squid for blocking
HTTPS sites, that were originally accessed via 302 redirect?/

I know that there are tons of blogs, forums, etc., that they recommend
theusage of SSLBump, but I also know that MITM is not a good choice, since
that it's (or it could be) illegal, to eavesdrop a secure connection. So I
believe that SSL Bump is not an option.

Thank you all for the attention.

Best Regards,
@ivanleoncz




--
Sent from: http://squid-web-proxy-cache.1019090.n4.nabble.com/Squid-Users-f1019091.html
_______________________________________________
squid-users mailing list
squid-users at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-users



From xpro6000 at gmail.com  Wed Sep 27 23:19:13 2017
From: xpro6000 at gmail.com (xpro6000)
Date: Wed, 27 Sep 2017 19:19:13 -0400
Subject: [squid-users] make large amount of IPv6 IPs for use?
Message-ID: <CAFoK1ayppzhoULQ-LSy0ZV=W8h4HaFW5kob_LjK5K4Q9CkA1Mg@mail.gmail.com>

Right now I have a config file that enables Squid to have an incoming
connection based on an IPv4 address and based on the connection port a IPv6
address is used. Below is the relevant part of the config file.


## designate acl based on inbound connection name
acl user1 myportname 3100
acl user2 myportname 3101
acl user3 myportname 3102

## define outgoing IPv6 per user
tcp_outgoing_address 2001:19f0:9014:073e:0000:0000:0000:0000 user1
tcp_outgoing_address 2001:19f0:9014:073e:0000:0000:0000:0001 user2
tcp_outgoing_address 2001:19f0:9014:073e:0000:0000:0000:0002 user3



I want to add thousands of IPs like this and it would take too much time
and it would be error prone. Is there any better way of achieving this? By
giving ranges maybe?
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20170927/43630a7b/attachment.htm>

From squid3 at treenet.co.nz  Thu Sep 28 00:19:36 2017
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 28 Sep 2017 13:19:36 +1300
Subject: [squid-users] proof of concept squid injecting js even with
 pinned ca
In-Reply-To: <1506534406914-0.post@n4.nabble.com>
References: <1506534406914-0.post@n4.nabble.com>
Message-ID: <6a04d85d-9fbb-9a39-2d71-65307cd3cb75@treenet.co.nz>

On 28/09/17 06:46, stern0m1 wrote:
> Hi,
> I am new to squid, please pardon me if this has already been discussed.
> 
> Is there a proof of concept somewhere to successfully use squid as a
> transparent proxy for  a mitm attack to inject js for sites/applications
> with pinned a pinned certificate?

No, MITM is not possible for those. That is the point of pinning.

Amos


From chip_pop at hotmail.com  Thu Sep 28 00:46:24 2017
From: chip_pop at hotmail.com (joseph)
Date: Wed, 27 Sep 2017 17:46:24 -0700 (MST)
Subject: [squid-users] squid5 build error
In-Reply-To: <82a619a6-2a42-d278-049a-7aeaa6a142b5@measurement-factory.com>
References: <1506520654560-0.post@n4.nabble.com>
 <82a619a6-2a42-d278-049a-7aeaa6a142b5@measurement-factory.com>
Message-ID: <1506559584420-0.post@n4.nabble.com>

OpenSSL 1.1.0f  25 May 2017



--
Sent from: http://squid-web-proxy-cache.1019090.n4.nabble.com/Squid-Users-f1019091.html


From synfinatic at gmail.com  Thu Sep 28 01:12:22 2017
From: synfinatic at gmail.com (Aaron Turner)
Date: Wed, 27 Sep 2017 18:12:22 -0700
Subject: [squid-users] make large amount of IPv6 IPs for use?
In-Reply-To: <CAFoK1ayppzhoULQ-LSy0ZV=W8h4HaFW5kob_LjK5K4Q9CkA1Mg@mail.gmail.com>
References: <CAFoK1ayppzhoULQ-LSy0ZV=W8h4HaFW5kob_LjK5K4Q9CkA1Mg@mail.gmail.com>
Message-ID: <CANAZdzWbcKZm4bQrwfphcY9TrbVC=HrvirXT62PCN_4E+ry9Lw@mail.gmail.com>

Write a small shell script to generate it for you?  I don't think
squid supports ranges or mapping of this sort.
--
Aaron Turner
https://synfin.net/         Twitter: @synfinatic
My father once told me that respect for the truth comes close to being
the basis for all morality.  "Something cannot emerge from nothing,"
he said.  This is profound thinking if you understand how unstable
"the truth" can be.  -- Frank Herbert, Dune


On Wed, Sep 27, 2017 at 4:19 PM, xpro6000 <xpro6000 at gmail.com> wrote:
> Right now I have a config file that enables Squid to have an incoming
> connection based on an IPv4 address and based on the connection port a IPv6
> address is used. Below is the relevant part of the config file.
>
>
> ## designate acl based on inbound connection name
> acl user1 myportname 3100
> acl user2 myportname 3101
> acl user3 myportname 3102
>
> ## define outgoing IPv6 per user
> tcp_outgoing_address 2001:19f0:9014:073e:0000:0000:0000:0000 user1
> tcp_outgoing_address 2001:19f0:9014:073e:0000:0000:0000:0001 user2
> tcp_outgoing_address 2001:19f0:9014:073e:0000:0000:0000:0002 user3
>
>
>
> I want to add thousands of IPs like this and it would take too much time and
> it would be error prone. Is there any better way of achieving this? By
> giving ranges maybe?
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>


From squid3 at treenet.co.nz  Thu Sep 28 01:47:02 2017
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 28 Sep 2017 14:47:02 +1300
Subject: [squid-users] make large amount of IPv6 IPs for use?
In-Reply-To: <CAFoK1ayppzhoULQ-LSy0ZV=W8h4HaFW5kob_LjK5K4Q9CkA1Mg@mail.gmail.com>
References: <CAFoK1ayppzhoULQ-LSy0ZV=W8h4HaFW5kob_LjK5K4Q9CkA1Mg@mail.gmail.com>
Message-ID: <3bc6da4a-339a-3784-e0a7-2a1932f2dac3@treenet.co.nz>

On 28/09/17 12:19, xpro6000 wrote:
> Right now I have a config file that enables Squid to have an incoming 
> connection based on an IPv4 address and based on the connection port a 
> IPv6 address is used. Below is the relevant part of the config file.
> 
> 
> ## designate acl based on inbound connection name
> acl user1 myportname 3100
> acl user2 myportname 3101
> acl user3 myportname 3102
> 
> ## define outgoing IPv6 per user
> tcp_outgoing_address 2001:19f0:9014:073e:0000:0000:0000:0000 user1
> tcp_outgoing_address 2001:19f0:9014:073e:0000:0000:0000:0001 user2
> tcp_outgoing_address 2001:19f0:9014:073e:0000:0000:0000:0002 user3
> 
> 
> 
> I want to add thousands of IPs like this and it would take too much time 
> and it would be error prone. Is there any better way of achieving this? 
> By giving ranges maybe?

No, this was a bad config hack to begin with and all you are doing by 
trying to scale it up is making the problems worse. Squid is an HTTP 
proxy, not a NAT implementation.

*why* are you trying so hard to break how the HTTP side of the Internet 
works?


Amos


From christof.gerber1 at gmail.com  Thu Sep 28 05:52:00 2017
From: christof.gerber1 at gmail.com (Christof Gerber)
Date: Thu, 28 Sep 2017 07:52:00 +0200
Subject: [squid-users] Display eCAP meta-information on Squid error-page
Message-ID: <CAFyThp+u6Dfvs4mR55+rgWy8PqiXV7dzzMQ9-jF7kqx0HwYQBg@mail.gmail.com>

Hi Squid users

I have a question concerning eCAP implementation in Squid 3.5.

My goal is to display data which is provided by an eCAP adapter on a Squid
error-page. My primary goal is to achieve this with the eCAP transaction
meta-information which is provided by the eCAP adapter with
Adapter::Xaction::visitEachOption().

I know that with "adapt::<last_h" (1) this meta-information (options) can
be logged to access.log. Is it somehow possible that Squid access these
key-value pairs and hands it over to the Error page? Similar to "deny_info"
tags (2) which ensures the availability of the URL (%U) etc. in the html
code of the error-page.

The approach that the eCAP adapter would create the error-page and send it
back to squid as response body instead of calling blockVirgin() (3) is
known by me but this would only be the second choice if the approach in
question is not possible.

References:
(1) http://www.squid-cache.org/Doc/config/logformat/
(2) http://www.squid-cache.org/Doc/config/deny_info/
(3) https://answers.launchpad.net/ecap/+faq/2516

Thanks in advance for your help.
Best

Christof

-- 
Christof Gerber
Email: christof.gerber1 at gmail.com
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20170928/a255abd8/attachment.htm>

From eliezer at ngtech.co.il  Thu Sep 28 07:09:30 2017
From: eliezer at ngtech.co.il (Eliezer Croitoru)
Date: Thu, 28 Sep 2017 10:09:30 +0300
Subject: [squid-users] make large amount of IPv6 IPs for use?
In-Reply-To: <CAFoK1ayppzhoULQ-LSy0ZV=W8h4HaFW5kob_LjK5K4Q9CkA1Mg@mail.gmail.com>
References: <CAFoK1ayppzhoULQ-LSy0ZV=W8h4HaFW5kob_LjK5K4Q9CkA1Mg@mail.gmail.com>
Message-ID: <!&!AAAAAAAAAAAuAAAAAAAAAGDVRrtF13VDjYYZR9MgvgYBAMO2jhD3dRHOtM0AqgC7tuYAAAAAAA4AABAAAACDy3opc97ISaph7BpU+PQwAQAAAAA=@ngtech.co.il>

Do you really need squid for that?
What is your ultimate goal of using squid?
- Content Caching
- Content Filtering
- Applying traffic ACL's
- NAT
- Other??

If you don't need caching in the picture then you are using the wrong tool..
When we will have an answer to what you want to achieve we might be able to help you towards the right solution.

All The Bests,
Eliezer

----
http://ngtech.co.il/lmgtfy/
Linux System Administrator
Mobile: +972-5-28704261
Email: eliezer at ngtech.co.il


From: squid-users [mailto:squid-users-bounces at lists.squid-cache.org] On Behalf Of xpro6000
Sent: Thursday, September 28, 2017 02:19
To: squid-users at lists.squid-cache.org
Subject: [squid-users] make large amount of IPv6 IPs for use?

Right now I have a config file that enables Squid to have an incoming connection based on an IPv4 address and based on the connection port a IPv6 address is used. Below is the relevant part of the config file.


## designate acl based on inbound connection name
acl user1 myportname 3100
acl user2 myportname 3101
acl user3 myportname 3102

## define outgoing IPv6 per user
tcp_outgoing_address 2001:19f0:9014:073e:0000:0000:0000:0000 user1
tcp_outgoing_address 2001:19f0:9014:073e:0000:0000:0000:0001 user2
tcp_outgoing_address 2001:19f0:9014:073e:0000:0000:0000:0002 user3



I want to add thousands of IPs like this and it would take too much time and it would be error prone. Is there any better way of achieving this? By giving ranges maybe?



From rentorbuy at yahoo.com  Thu Sep 28 09:31:41 2017
From: rentorbuy at yahoo.com (Vieri)
Date: Thu, 28 Sep 2017 09:31:41 +0000 (UTC)
Subject: [squid-users] leakfinder
References: <382383813.10167665.1506591101613.ref@mail.yahoo.com>
Message-ID: <382383813.10167665.1506591101613@mail.yahoo.com>

Hi,

I enabled leakfinder in squid's configure script. However, the following does not show anything:
# squidclient mgr: | grep leaks

Also, the config script says:
--enable-leakfinder     Enable Leak Finding code. Enabling this alone does
nothing; you also have to modify the source code to
use the leak finding functions. Probably Useful for
hackers only.


Is there an "easy" way or a quick guide to enable these leak finding functions?

Thanks,

Vieri


From squid3 at treenet.co.nz  Thu Sep 28 10:12:51 2017
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 28 Sep 2017 23:12:51 +1300
Subject: [squid-users] leakfinder
In-Reply-To: <382383813.10167665.1506591101613@mail.yahoo.com>
References: <382383813.10167665.1506591101613.ref@mail.yahoo.com>
 <382383813.10167665.1506591101613@mail.yahoo.com>
Message-ID: <e044b202-d5e4-05ed-e146-b4a247835ddc@treenet.co.nz>

On 28/09/17 22:31, Vieri wrote:
> Hi,
> 
> I enabled leakfinder in squid's configure script. However, the following does not show anything:
> # squidclient mgr: | grep leaks
> 
> Also, the config script says:
> --enable-leakfinder     Enable Leak Finding code. Enabling this alone does
> nothing; you also have to modify the source code to
> use the leak finding functions. Probably Useful for
> hackers only.
> 
> 
> Is there an "easy" way or a quick guide to enable these leak finding functions?

As the docs you found say, you need to know how to modify the code build 
to use it. If you can't read the code to find and do the necessary 
additional changes, then this feature is not for you.

The current Squid code has no leaks AFAIK.

Amos


From anwesh_tiwari at yahoo.com  Thu Sep 28 16:10:02 2017
From: anwesh_tiwari at yahoo.com (anwesh tiwari)
Date: Thu, 28 Sep 2017 16:10:02 +0000 (UTC)
Subject: [squid-users] ipv6 acl access not working properly
References: <1715375839.11166961.1506615002160.ref@mail.yahoo.com>
Message-ID: <1715375839.11166961.1506615002160@mail.yahoo.com>

Ipv6 acl is not working as expected, if the ipv6 address of domain is unrouteable and it fallbacks to ipv4 even when its denied.

Details :
What I am trying to achieve :  I want to disable all IPv4 domain access from proxy and disable all ipv4 connections.

Here is my directives just before http_access deny all line in default squid conf.

dns_v4_first off
acl to_ipv6 dst ipv6
http_access deny !to_ipv6
http_access allow to_ipv6 
When I browse this site using proxy
http://whatismyipv6.com

This site has ipv6 AAAA record but thats is not routed when I check. 

Here is the log 
1506526125.315    327 <publicIP> TCP_MISS/200 2486 GET http://www.whatismyipv6.com/ - HIER_DIRECT/216.64.158.90 text/html
1506526126.259    632 <publicIP> TCP_MISS/200 31738 GET http://www.whatismyipv6.com/World-IPv6-Day.jpg - HIER_DIRECT/216.64.158.90 image/jpeg

The log shows that squid is able to browse the site which is explicitly denied by http_access directive.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20170928/b6f7d970/attachment.htm>

From rousskov at measurement-factory.com  Thu Sep 28 16:28:59 2017
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Thu, 28 Sep 2017 10:28:59 -0600
Subject: [squid-users] ipv6 acl access not working properly
In-Reply-To: <1715375839.11166961.1506615002160@mail.yahoo.com>
References: <1715375839.11166961.1506615002160.ref@mail.yahoo.com>
 <1715375839.11166961.1506615002160@mail.yahoo.com>
Message-ID: <9e9b98d7-96d3-5256-592e-8d9ea4433cbc@measurement-factory.com>

On 09/28/2017 10:10 AM, anwesh tiwari wrote:
> Ipv6 acl is not working as expected, if the ipv6 address of domain is unrouteable and it fallbacks to ipv4 even when its denied.
> 
> Details :
> What I am trying to achieve :  I want to disable all IPv4 domain access from proxy and disable all ipv4 connections.
> 
> Here is my directives just before http_access deny all line in default squid conf.
> 
> dns_v4_first off
> acl to_ipv6 dst ipv6
> http_access deny !to_ipv6
> http_access allow to_ipv6
> 
>  
> When I browse this site using proxy
> http://whatismyipv6.com <http://whatismyipv6.com/>
> 
> This site has ipv6 AAAA record but thats is not routed when I check. 
> 
> Here is the log 
> 1506526125.315    327 <publicIP> TCP_MISS/200 2486 GET http://www.whatismyipv6.com/ - HIER_DIRECT/216.64.158.90 text/html
> 1506526126.259    632 <publicIP> TCP_MISS/200 31738 GET http://www.whatismyipv6.com/World-IPv6-Day.jpg - HIER_DIRECT/216.64.158.90 image/jpeg
> 
> The log shows that squid is able to browse the site which is explicitly denied by http_access directive.


I will rephrase the above question in hope that other folks on this list
can help Anwesh Tiwari to solve his actual problem rather than tell him
yet again[1] that there is nothing wrong with the ipv6 ACL:

"I expected that using a dst ipv6 ACL with http_access would block IPv4
connections originating from Squid. I now understand that my
expectations were wrong. Please help me refine my goals and configure
Squid to achieve them. Thank you."

  [1] http://bugs.squid-cache.org/show_bug.cgi?id=4777


HTH,

Alex.


From edouardmoran at yahoo.fr  Thu Sep 28 16:40:44 2017
From: edouardmoran at yahoo.fr (EdouardM)
Date: Thu, 28 Sep 2017 09:40:44 -0700 (MST)
Subject: [squid-users] TCP_REFRESH_IGNORED/200 ???
Message-ID: <1506616844110-0.post@n4.nabble.com>

Hi All,

what's this Squid code "TCP_REFRESH_IGNORED" ?
we checked the page https://wiki.squid-cache.org/SquidFaq/SquidLogs and it's
not mentioned.
any idea ?

best regards.




--
Sent from: http://squid-web-proxy-cache.1019090.n4.nabble.com/Squid-Users-f1019091.html


From rousskov at measurement-factory.com  Thu Sep 28 17:51:53 2017
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Thu, 28 Sep 2017 11:51:53 -0600
Subject: [squid-users] TCP_REFRESH_IGNORED/200 ???
In-Reply-To: <1506616844110-0.post@n4.nabble.com>
References: <1506616844110-0.post@n4.nabble.com>
Message-ID: <c12281d1-0d02-53e7-cabb-d1b067b5cdc2@measurement-factory.com>

On 09/28/2017 10:40 AM, EdouardM wrote:

> what's this Squid code "TCP_REFRESH_IGNORED" ?

This recently added result code indicates that Squid tried to refresh
the previously cached response A, but got a response B that was older
than A (as determined by the Date header field). Squid ignored response
B. Squid probably served the response A instead.

This "ignore older responses" logic complies with RFC 7234 section 4
requirement: a cache MUST use the most recent response (as determined by
the Date header field).

Frequent TCP_REFRESH_IGNORED entries may warrant an investigation. You
may be dealing with broken origin servers, unusual race conditions (that
Squid might be mishandling), and/or Squid bugs.


> we checked the page https://wiki.squid-cache.org/SquidFaq/SquidLogs and it's
> not mentioned.

It is now.


HTH,

Alex.


From synfinatic at gmail.com  Thu Sep 28 20:19:54 2017
From: synfinatic at gmail.com (Aaron Turner)
Date: Thu, 28 Sep 2017 13:19:54 -0700
Subject: [squid-users] tuning squid memory (aka avoiding the reaper)
In-Reply-To: <6fd4bb1b-8ec5-4883-2c57-8debb3890d9a@measurement-factory.com>
References: <CANAZdzXS7jFH_n44BgdYALzE_r3D7t78HRamz9qdKsbtfOVyqg@mail.gmail.com>
 <6fd4bb1b-8ec5-4883-2c57-8debb3890d9a@measurement-factory.com>
Message-ID: <CANAZdzUDViPoOxXVUyd0=Krz+xdyn_R0viJXxLNEMmmGtFs5jA@mail.gmail.com>

Ok, so did some research and what I'm finding is that:

If I set sslflags=NO_DEFAULT_CA for http_port and disable both mem and
disk cache then memory is very stable.  It goes up for a little bit
and then pretty much stabilizes (it actually goes up and down a
little, but doesn't seem to be growing or trending up).

I then enabled memory cache (10GB worth) and ran that for a while.  As
the cache filled, memory usage obviously went up.  Once the cache
filled, memory usage continued to increase, but at a slower rate.
Unlike before, it doesn't seem to stabilize.  I'm seeing memory usage
increase in top (virtual, resident & shared) as well as in mgr:info's
"Total accounted" line.  It's not growing as fast before when I didn't
have the sslflags option, but it is growing.

What other information would be useful to debug this?

--
Aaron Turner
https://synfin.net/         Twitter: @synfinatic
My father once told me that respect for the truth comes close to being
the basis for all morality.  "Something cannot emerge from nothing,"
he said.  This is profound thinking if you understand how unstable
"the truth" can be.  -- Frank Herbert, Dune


On Mon, Sep 25, 2017 at 8:26 PM, Alex Rousskov
<rousskov at measurement-factory.com> wrote:
> On 09/25/2017 05:23 PM, Aaron Turner wrote:
>> So I'm testing squid 3.5.26 on an m3.xlarge w/ 14GB of RAM.  Squid is
>> the only "real" service running (sshd and the like).  I'm running 4
>> workers, and 2 rock cache.  The workers seem to be growing unbounded
>> and given ~30min or so will cause the kernel to start killing off
>> processes until memory is freed.  Yes, my clients (32 of them) are
>> hitting this at about 250 URL's/min which doesn't seem that crazy, but
>> ?\_(?)_/?
>>
>> cache_mem 1 GB resulted in workers exceeding 4GB resident.  So I tried
>> 500 MB, same problem.  Now I'm down to 250 MB and I'm still seeing
>> workers using 3-4GB of RAM after a few minutes and still growing
>
> It is not the Squid memory cache that consumes your RAM, apparently.
>
>
>> the docs indicate I should expect total memory to be roughly 3x cache_mem.
>
> ... which is an absurd formula for those using disk caches: Roughly
> speaking, most large busy Squids spend most of their RAM on
>
> * memory cache,
> * disk cache indexes,
> * SSL-related caches, and
> * in-flight transactions.
>
> Only one of those 4 components is proportional to cache_mem, with a
> coefficient closer to 1 than to 3.
>
>
>> mgr:info reports:
>
> Thank you for posting this useful info. When you are using disk caching,
> please also include the mgr:storedir report.
>
>
>> I'm trying to figure out why and how to fix.
>
> I recommend disabling all caching (memory and disk) and SslBump (if any)
> to establish a baseline first. If everything looks stable and peachy for
> a few hours, record/store the baseline measurements, and add one new
> memory consumer (e.g., the memory cache). Ideally, this testing should
> be done in a lab rather than on real users, but YMMV.
>
>
>> One thing I've read about the cache_mem knob is:
>>
>> "If circumstances require, this limit will be exceeded.
>>
>> Specifically, if your incoming request rate requires more than
>> 'cache_mem' of memory to hold in-transit objects, Squid will
>> exceed this limit to satisfy the new requests.  When the load
>> decreases, blocks will be freed until the high-water mark is
>> reached.  Thereafter, blocks will be used to store hot
>> objects."
>
> The above is more-or-less accurate, but please note that in-transit
> objects do not usually eat memory cache RAM in SMP mode. It is usually
> best to think of in-flight transactions as a distinct SMP memory
> consumer IMO.
>
>
>> Not sure if this is the cause of my problem?
>
> It could be -- it is difficult for me to say by looking at one random
> mgr:info snapshot. If I have to guess based on that snapshot alone, then
> my answer would be "no" because you have less than 4K concurrent
> transactions and transaction response times are low. Hopefully somebody
> else on the list can tell you more.
>
>
>
>> The FAQ says try a different malloc, so tried recompiling with
>> --enable-dlmalloc, but that had no impact.
>
> Do not bother unless your deployment environment is very unusual. This
> hint was helpful 20 years ago, but is rarely relevant these days AFAIK.
> See above for a different attack plan.
>
>
> HTH,
>
> Alex.


From synfinatic at gmail.com  Thu Sep 28 22:29:49 2017
From: synfinatic at gmail.com (Aaron Turner)
Date: Thu, 28 Sep 2017 15:29:49 -0700
Subject: [squid-users] cache hit rate isn't what I'd expect
Message-ID: <CANAZdzW5vN8QhzwA2LoYONKkHy0E60rAp26ivJf-y-Gw4PVm8A@mail.gmail.com>

So this grep through my access logs for this single URL does a good
job illustrating a rather interesting problem:

$ grep -h 'https://static.licdn.com/sc/h/ddzuq7qeny6qn0ysh3hj6pzmr
text/css ip_index=0,client=m0078269' access.*.log | sort


26/Sep/2017:20:10:27 TCP_HIT/200
https://static.licdn.com/sc/h/ddzuq7qeny6qn0ysh3hj6pzmr text/css
ip_index=0,client=m0078269_9094
26/Sep/2017:20:10:33 TCP_MISS/200
https://static.licdn.com/sc/h/ddzuq7qeny6qn0ysh3hj6pzmr text/css
ip_index=0,client=m0078269_9094
26/Sep/2017:20:10:42 TCP_MISS/200
https://static.licdn.com/sc/h/ddzuq7qeny6qn0ysh3hj6pzmr text/css
ip_index=0,client=m0078269_9094
26/Sep/2017:20:10:47 TCP_MISS/200
https://static.licdn.com/sc/h/ddzuq7qeny6qn0ysh3hj6pzmr text/css
ip_index=0,client=m0078269_9094
26/Sep/2017:20:10:52 TCP_MISS/200
https://static.licdn.com/sc/h/ddzuq7qeny6qn0ysh3hj6pzmr text/css
ip_index=0,client=m0078269_9094
26/Sep/2017:20:10:56 TCP_HIT/200
https://static.licdn.com/sc/h/ddzuq7qeny6qn0ysh3hj6pzmr text/css
ip_index=0,client=m0078269_9094
26/Sep/2017:20:11:11 TCP_MISS/200
https://static.licdn.com/sc/h/ddzuq7qeny6qn0ysh3hj6pzmr text/css
ip_index=0,client=m0078269_9094
26/Sep/2017:20:11:15 TCP_MISS/200
https://static.licdn.com/sc/h/ddzuq7qeny6qn0ysh3hj6pzmr text/css
ip_index=0,client=m0078269_9094
26/Sep/2017:20:11:19 TCP_MISS/200
https://static.licdn.com/sc/h/ddzuq7qeny6qn0ysh3hj6pzmr text/css
ip_index=0,client=m0078269_9094
26/Sep/2017:20:11:24 TCP_MISS/200
https://static.licdn.com/sc/h/ddzuq7qeny6qn0ysh3hj6pzmr text/css
ip_index=0,client=m0078269_9094
26/Sep/2017:20:11:28 TCP_MISS/200
https://static.licdn.com/sc/h/ddzuq7qeny6qn0ysh3hj6pzmr text/css
ip_index=0,client=m0078269_9094
26/Sep/2017:20:11:32 TCP_MISS/200
https://static.licdn.com/sc/h/ddzuq7qeny6qn0ysh3hj6pzmr text/css
ip_index=0,client=m0078269_9094
26/Sep/2017:20:11:37 TCP_MISS/200
https://static.licdn.com/sc/h/ddzuq7qeny6qn0ysh3hj6pzmr text/css
ip_index=0,client=m0078269_9094
26/Sep/2017:20:11:41 TCP_MISS/200
https://static.licdn.com/sc/h/ddzuq7qeny6qn0ysh3hj6pzmr text/css
ip_index=0,client=m0078269_9094
26/Sep/2017:20:11:48 TCP_MISS/200
https://static.licdn.com/sc/h/ddzuq7qeny6qn0ysh3hj6pzmr text/css
ip_index=0,client=m0078269_9094
26/Sep/2017:20:11:53 TCP_MISS/200
https://static.licdn.com/sc/h/ddzuq7qeny6qn0ysh3hj6pzmr text/css
ip_index=0,client=m0078269_9094
26/Sep/2017:20:11:57 TCP_MISS/200
https://static.licdn.com/sc/h/ddzuq7qeny6qn0ysh3hj6pzmr text/css
ip_index=0,client=m0078269_9094
26/Sep/2017:20:12:01 TCP_MISS/200
https://static.licdn.com/sc/h/ddzuq7qeny6qn0ysh3hj6pzmr text/css
ip_index=0,client=m0078269_9094
26/Sep/2017:20:12:06 TCP_MISS/200
https://static.licdn.com/sc/h/ddzuq7qeny6qn0ysh3hj6pzmr text/css
ip_index=0,client=m0078269_9094
26/Sep/2017:20:12:10 TCP_MISS/200
https://static.licdn.com/sc/h/ddzuq7qeny6qn0ysh3hj6pzmr text/css
ip_index=0,client=m0078269_9094
26/Sep/2017:20:12:14 TCP_MISS/200
https://static.licdn.com/sc/h/ddzuq7qeny6qn0ysh3hj6pzmr text/css
ip_index=0,client=m0078269_9094
26/Sep/2017:20:12:19 TCP_MISS/200
https://static.licdn.com/sc/h/ddzuq7qeny6qn0ysh3hj6pzmr text/css
ip_index=0,client=m0078269_9094
26/Sep/2017:20:12:23 TCP_MISS/200
https://static.licdn.com/sc/h/ddzuq7qeny6qn0ysh3hj6pzmr text/css
ip_index=0,client=m0078269_9094
26/Sep/2017:20:12:28 TCP_MISS/200
https://static.licdn.com/sc/h/ddzuq7qeny6qn0ysh3hj6pzmr text/css
ip_index=0,client=m0078269_9094
26/Sep/2017:20:12:32 TCP_MISS/200
https://static.licdn.com/sc/h/ddzuq7qeny6qn0ysh3hj6pzmr text/css
ip_index=0,client=m0078269_9094
26/Sep/2017:20:12:37 TCP_HIT/200
https://static.licdn.com/sc/h/ddzuq7qeny6qn0ysh3hj6pzmr text/css
ip_index=0,client=m0078269_9094
26/Sep/2017:20:12:41 TCP_MISS/200
https://static.licdn.com/sc/h/ddzuq7qeny6qn0ysh3hj6pzmr text/css
ip_index=0,client=m0078269_9094
26/Sep/2017:20:12:49 TCP_HIT/200
https://static.licdn.com/sc/h/ddzuq7qeny6qn0ysh3hj6pzmr text/css
ip_index=0,client=m0078269_9094
26/Sep/2017:20:12:59 TCP_HIT/200
https://static.licdn.com/sc/h/ddzuq7qeny6qn0ysh3hj6pzmr text/css
ip_index=0,client=m0078269_9094
26/Sep/2017:20:13:03 TCP_MISS/200
https://static.licdn.com/sc/h/ddzuq7qeny6qn0ysh3hj6pzmr text/css
ip_index=0,client=m0078269_9094
26/Sep/2017:20:13:08 TCP_HIT/200
https://static.licdn.com/sc/h/ddzuq7qeny6qn0ysh3hj6pzmr text/css
ip_index=0,client=m0078269_9094
26/Sep/2017:20:13:13 TCP_MISS/200
https://static.licdn.com/sc/h/ddzuq7qeny6qn0ysh3hj6pzmr text/css
ip_index=0,client=m0078269_9094
26/Sep/2017:20:13:27 TCP_MISS/200
https://static.licdn.com/sc/h/ddzuq7qeny6qn0ysh3hj6pzmr text/css
ip_index=0,client=m0078269_9094
26/Sep/2017:20:13:33 TCP_MISS/200
https://static.licdn.com/sc/h/ddzuq7qeny6qn0ysh3hj6pzmr text/css
ip_index=0,client=m0078269_9094
26/Sep/2017:20:13:37 TCP_MISS/200
https://static.licdn.com/sc/h/ddzuq7qeny6qn0ysh3hj6pzmr text/css
ip_index=0,client=m0078269_9094
26/Sep/2017:20:13:42 TCP_MISS/200
https://static.licdn.com/sc/h/ddzuq7qeny6qn0ysh3hj6pzmr text/css
ip_index=0,client=m0078269_9094
26/Sep/2017:20:13:47 TCP_HIT/200
https://static.licdn.com/sc/h/ddzuq7qeny6qn0ysh3hj6pzmr text/css
ip_index=0,client=m0078269_9094
26/Sep/2017:20:13:52 TCP_MISS/200
https://static.licdn.com/sc/h/ddzuq7qeny6qn0ysh3hj6pzmr text/css
ip_index=0,client=m0078269_9094
26/Sep/2017:20:13:56 TCP_MISS/200
https://static.licdn.com/sc/h/ddzuq7qeny6qn0ysh3hj6pzmr text/css
ip_index=0,client=m0078269_9094
26/Sep/2017:20:14:00 TCP_MISS/200
https://static.licdn.com/sc/h/ddzuq7qeny6qn0ysh3hj6pzmr text/css
ip_index=0,client=m0078269_9094
26/Sep/2017:20:14:06 TCP_HIT/200
https://static.licdn.com/sc/h/ddzuq7qeny6qn0ysh3hj6pzmr text/css
ip_index=0,client=m0078269_9094
26/Sep/2017:20:14:36 TCP_MISS/200
https://static.licdn.com/sc/h/ddzuq7qeny6qn0ysh3hj6pzmr text/css
ip_index=0,client=m0078269_9094
26/Sep/2017:20:14:41 TCP_MISS/200
https://static.licdn.com/sc/h/ddzuq7qeny6qn0ysh3hj6pzmr text/css
ip_index=0,client=m0078269_9094
26/Sep/2017:20:14:45 TCP_MISS/200
https://static.licdn.com/sc/h/ddzuq7qeny6qn0ysh3hj6pzmr text/css
ip_index=0,client=m0078269_9094
26/Sep/2017:20:14:54 TCP_MISS/200
https://static.licdn.com/sc/h/ddzuq7qeny6qn0ysh3hj6pzmr text/css
ip_index=0,client=m0078269_9094

At first I thought this was because the because I have a bunch of
clients, each of which behaves exactly the same except for one thing:
the client includes a unique request header that squid strips off
before forwarding to the server (you can see it logged as
client=mXXXXX_XXXX).  But in this case I've controlled for that and
only grep'd for a single client's request.  I've even tried setting
"vary_ignore_expire on", but that doesn't seem to be a complete fix.

I can't for the life of me understand why the low hit rate though.

--
Aaron Turner
https://synfin.net/         Twitter: @synfinatic
My father once told me that respect for the truth comes close to being
the basis for all morality.  "Something cannot emerge from nothing,"
he said.  This is profound thinking if you understand how unstable
"the truth" can be.  -- Frank Herbert, Dune


From squid3 at treenet.co.nz  Thu Sep 28 22:32:31 2017
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Fri, 29 Sep 2017 11:32:31 +1300
Subject: [squid-users] tuning squid memory (aka avoiding the reaper)
In-Reply-To: <CANAZdzUDViPoOxXVUyd0=Krz+xdyn_R0viJXxLNEMmmGtFs5jA@mail.gmail.com>
References: <CANAZdzXS7jFH_n44BgdYALzE_r3D7t78HRamz9qdKsbtfOVyqg@mail.gmail.com>
 <6fd4bb1b-8ec5-4883-2c57-8debb3890d9a@measurement-factory.com>
 <CANAZdzUDViPoOxXVUyd0=Krz+xdyn_R0viJXxLNEMmmGtFs5jA@mail.gmail.com>
Message-ID: <54372253-c45a-7ca2-37f8-8ecbdbd3ba27@treenet.co.nz>

On 29/09/17 09:19, Aaron Turner wrote:
> Ok, so did some research and what I'm finding is that:
> 
> If I set sslflags=NO_DEFAULT_CA for http_port and disable both mem and
> disk cache then memory is very stable.  It goes up for a little bit
> and then pretty much stabilizes (it actually goes up and down a
> little, but doesn't seem to be growing or trending up).
> 
> I then enabled memory cache (10GB worth) and ran that for a while.  As
> the cache filled, memory usage obviously went up.  Once the cache
> filled, memory usage continued to increase, but at a slower rate.
> Unlike before, it doesn't seem to stabilize.  I'm seeing memory usage
> increase in top (virtual, resident & shared) as well as in mgr:info's
> "Total accounted" line.  It's not growing as fast before when I didn't
> have the sslflags option, but it is growing.
> 
> What other information would be useful to debug this?
> 

Since the accounted is growing the mgr:mem report should contain some 
clues. It is a TSV spreadsheet of memory allocations, you may need a few 
snapshots of it to see trends.

Amos


From squid3 at treenet.co.nz  Thu Sep 28 22:47:29 2017
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Fri, 29 Sep 2017 11:47:29 +1300
Subject: [squid-users] cache hit rate isn't what I'd expect
In-Reply-To: <CANAZdzW5vN8QhzwA2LoYONKkHy0E60rAp26ivJf-y-Gw4PVm8A@mail.gmail.com>
References: <CANAZdzW5vN8QhzwA2LoYONKkHy0E60rAp26ivJf-y-Gw4PVm8A@mail.gmail.com>
Message-ID: <a5a92f62-e187-e7c0-f667-9d7773041b2f@treenet.co.nz>

On 29/09/17 11:29, Aaron Turner wrote:
> So this grep through my access logs for this single URL does a good
> job illustrating a rather interesting problem:
> 
> $ grep -h 'https://static.licdn.com/sc/h/ddzuq7qeny6qn0ysh3hj6pzmr
> text/css ip_index=0,client=m0078269' access.*.log | sort
> 
> 
...
 >
> At first I thought this was because the because I have a bunch of
> clients, each of which behaves exactly the same except for one thing:
> the client includes a unique request header that squid strips off
> before forwarding to the server (you can see it logged as
> client=mXXXXX_XXXX).  But in this case I've controlled for that and
> only grep'd for a single client's request.  I've even tried setting
> "vary_ignore_expire on", but that doesn't seem to be a complete fix.
> 
> I can't for the life of me understand why the low hit rate though.
> 

The duration and size fields are quite useful for detecting reasons for 
HIT/MISS.

Request headers should not affect the response caching, unless they are 
listed in the servers Vary header.

In this case the server is delivering broken Vary responses. redbot.org 
says it is using Vary:Accept-Encoding sometimes, so both the Vary and 
Accept-Encoding would be useful info to log.

I expect it is the usual problem of clients fighting over whose variant 
gets cached when this type of server breakage happens - when the Vary 
header changes or disappears, old variants become unfindable until it 
changes back.

Amos


From synfinatic at gmail.com  Thu Sep 28 23:35:58 2017
From: synfinatic at gmail.com (Aaron Turner)
Date: Thu, 28 Sep 2017 16:35:58 -0700
Subject: [squid-users] tuning squid memory (aka avoiding the reaper)
In-Reply-To: <54372253-c45a-7ca2-37f8-8ecbdbd3ba27@treenet.co.nz>
References: <CANAZdzXS7jFH_n44BgdYALzE_r3D7t78HRamz9qdKsbtfOVyqg@mail.gmail.com>
 <6fd4bb1b-8ec5-4883-2c57-8debb3890d9a@measurement-factory.com>
 <CANAZdzUDViPoOxXVUyd0=Krz+xdyn_R0viJXxLNEMmmGtFs5jA@mail.gmail.com>
 <54372253-c45a-7ca2-37f8-8ecbdbd3ba27@treenet.co.nz>
Message-ID: <CANAZdzXK4D14OaNpBLCm5ZPcgau_2+B38sXM2mTDe_bB8QwFvg@mail.gmail.com>

Ok, i'll work on that.  One other thing, is that if I let it run long
enough, squid will crash with errors like the following:

FATAL: Received Bus Error...dying.
2017/09/28 23:28:09 kid4| Closing HTTP port 10.93.3.4:3128
2017/09/28 23:28:09 kid4| Closing HTTP port 127.0.0.1:3128
2017/09/28 23:28:09 kid4| storeDirWriteCleanLogs: Starting...
2017/09/28 23:28:09 kid4|   Finished.  Wrote 0 entries.
2017/09/28 23:28:09 kid4|   Took 0.00 seconds (  0.00 entries/sec).
CPU Usage: 0.541 seconds = 0.466 user + 0.075 sys
Maximum Resident Size: 121440 KB
Page faults with physical i/o: 0

At first I thought the bus error was hardware, but it's happened on
two different EC2 instances now.

--
Aaron Turner
https://synfin.net/         Twitter: @synfinatic
My father once told me that respect for the truth comes close to being
the basis for all morality.  "Something cannot emerge from nothing,"
he said.  This is profound thinking if you understand how unstable
"the truth" can be.  -- Frank Herbert, Dune


On Thu, Sep 28, 2017 at 3:32 PM, Amos Jeffries <squid3 at treenet.co.nz> wrote:
> On 29/09/17 09:19, Aaron Turner wrote:
>>
>> Ok, so did some research and what I'm finding is that:
>>
>> If I set sslflags=NO_DEFAULT_CA for http_port and disable both mem and
>> disk cache then memory is very stable.  It goes up for a little bit
>> and then pretty much stabilizes (it actually goes up and down a
>> little, but doesn't seem to be growing or trending up).
>>
>> I then enabled memory cache (10GB worth) and ran that for a while.  As
>> the cache filled, memory usage obviously went up.  Once the cache
>> filled, memory usage continued to increase, but at a slower rate.
>> Unlike before, it doesn't seem to stabilize.  I'm seeing memory usage
>> increase in top (virtual, resident & shared) as well as in mgr:info's
>> "Total accounted" line.  It's not growing as fast before when I didn't
>> have the sslflags option, but it is growing.
>>
>> What other information would be useful to debug this?
>>
>
> Since the accounted is growing the mgr:mem report should contain some clues.
> It is a TSV spreadsheet of memory allocations, you may need a few snapshots
> of it to see trends.
>
> Amos
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users


From synfinatic at gmail.com  Thu Sep 28 23:54:09 2017
From: synfinatic at gmail.com (Aaron Turner)
Date: Thu, 28 Sep 2017 16:54:09 -0700
Subject: [squid-users] cache hit rate isn't what I'd expect
In-Reply-To: <a5a92f62-e187-e7c0-f667-9d7773041b2f@treenet.co.nz>
References: <CANAZdzW5vN8QhzwA2LoYONKkHy0E60rAp26ivJf-y-Gw4PVm8A@mail.gmail.com>
 <a5a92f62-e187-e7c0-f667-9d7773041b2f@treenet.co.nz>
Message-ID: <CANAZdzWOwAxCSZziruXjXysHow-oRG6vxb0Ci1JeB6CT1P2O9w@mail.gmail.com>

Here ya go

26/Sep/2017:20:10:27    137 10.93.3.47 TCP_HIT/200 11265 GET
https://static.licdn.com/sc/h/ddzuq7qeny6qn0ysh3hj6pzmr - HIER_NONE/-
26/Sep/2017:20:10:33     46 10.93.3.47 TCP_MISS/200 11259 GET
https://static.licdn.com/sc/h/ddzuq7qeny6qn0ysh3hj6pzmr -
HIER_DIRECT/192.229.163.180
26/Sep/2017:20:10:42      3 10.93.3.47 TCP_MISS/200 11259 GET
https://static.licdn.com/sc/h/ddzuq7qeny6qn0ysh3hj6pzmr -
HIER_DIRECT/192.229.163.180
26/Sep/2017:20:10:47      2 10.93.3.47 TCP_MISS/200 11259 GET
https://static.licdn.com/sc/h/ddzuq7qeny6qn0ysh3hj6pzmr -
HIER_DIRECT/192.229.163.180
26/Sep/2017:20:10:52      5 10.93.3.47 TCP_MISS/200 11259 GET
https://static.licdn.com/sc/h/ddzuq7qeny6qn0ysh3hj6pzmr -
HIER_DIRECT/192.229.163.180
26/Sep/2017:20:10:56    234 10.93.3.47 TCP_HIT/200 11265 GET
https://static.licdn.com/sc/h/ddzuq7qeny6qn0ysh3hj6pzmr - HIER_NONE/-
26/Sep/2017:20:11:11      3 10.93.3.47 TCP_MISS/200 11259 GET
https://static.licdn.com/sc/h/ddzuq7qeny6qn0ysh3hj6pzmr -
HIER_DIRECT/192.229.163.180
26/Sep/2017:20:11:15      3 10.93.3.47 TCP_MISS/200 11259 GET
https://static.licdn.com/sc/h/ddzuq7qeny6qn0ysh3hj6pzmr -
HIER_DIRECT/192.229.163.180
26/Sep/2017:20:11:19      6 10.93.3.47 TCP_MISS/200 11259 GET
https://static.licdn.com/sc/h/ddzuq7qeny6qn0ysh3hj6pzmr -
HIER_DIRECT/192.229.163.180
26/Sep/2017:20:11:24      5 10.93.3.47 TCP_MISS/200 11259 GET
https://static.licdn.com/sc/h/ddzuq7qeny6qn0ysh3hj6pzmr -
HIER_DIRECT/192.229.163.180
26/Sep/2017:20:11:28      3 10.93.3.47 TCP_MISS/200 11259 GET
https://static.licdn.com/sc/h/ddzuq7qeny6qn0ysh3hj6pzmr -
HIER_DIRECT/192.229.163.180
26/Sep/2017:20:11:32      1 10.93.3.47 TCP_MISS/200 11259 GET
https://static.licdn.com/sc/h/ddzuq7qeny6qn0ysh3hj6pzmr -
HIER_DIRECT/192.229.163.180
26/Sep/2017:20:11:37      2 10.93.3.47 TCP_MISS/200 11259 GET
https://static.licdn.com/sc/h/ddzuq7qeny6qn0ysh3hj6pzmr -
HIER_DIRECT/192.229.163.180
26/Sep/2017:20:11:41      2 10.93.3.47 TCP_MISS/200 11259 GET
https://static.licdn.com/sc/h/ddzuq7qeny6qn0ysh3hj6pzmr -
HIER_DIRECT/192.229.163.180
26/Sep/2017:20:11:48      3 10.93.3.47 TCP_MISS/200 11259 GET
https://static.licdn.com/sc/h/ddzuq7qeny6qn0ysh3hj6pzmr -
HIER_DIRECT/192.229.163.180
26/Sep/2017:20:11:53      4 10.93.3.47 TCP_MISS/200 11259 GET
https://static.licdn.com/sc/h/ddzuq7qeny6qn0ysh3hj6pzmr -
HIER_DIRECT/192.229.163.180
26/Sep/2017:20:11:57      6 10.93.3.47 TCP_MISS/200 11259 GET
https://static.licdn.com/sc/h/ddzuq7qeny6qn0ysh3hj6pzmr -
HIER_DIRECT/192.229.163.180
26/Sep/2017:20:12:01      7 10.93.3.47 TCP_MISS/200 11259 GET
https://static.licdn.com/sc/h/ddzuq7qeny6qn0ysh3hj6pzmr -
HIER_DIRECT/192.229.163.180
26/Sep/2017:20:12:06      5 10.93.3.47 TCP_MISS/200 11259 GET
https://static.licdn.com/sc/h/ddzuq7qeny6qn0ysh3hj6pzmr -
HIER_DIRECT/192.229.163.180
26/Sep/2017:20:12:10      4 10.93.3.47 TCP_MISS/200 11259 GET
https://static.licdn.com/sc/h/ddzuq7qeny6qn0ysh3hj6pzmr -
HIER_DIRECT/192.229.163.180
26/Sep/2017:20:12:14     11 10.93.3.47 TCP_MISS/200 11259 GET
https://static.licdn.com/sc/h/ddzuq7qeny6qn0ysh3hj6pzmr -
HIER_DIRECT/192.229.163.180
26/Sep/2017:20:12:19      3 10.93.3.47 TCP_MISS/200 11259 GET
https://static.licdn.com/sc/h/ddzuq7qeny6qn0ysh3hj6pzmr -
HIER_DIRECT/192.229.163.180
26/Sep/2017:20:12:23      6 10.93.3.47 TCP_MISS/200 11259 GET
https://static.licdn.com/sc/h/ddzuq7qeny6qn0ysh3hj6pzmr -
HIER_DIRECT/192.229.163.180
26/Sep/2017:20:12:28      4 10.93.3.47 TCP_MISS/200 11259 GET
https://static.licdn.com/sc/h/ddzuq7qeny6qn0ysh3hj6pzmr -
HIER_DIRECT/192.229.163.180
26/Sep/2017:20:12:32      6 10.93.3.47 TCP_MISS/200 11259 GET
https://static.licdn.com/sc/h/ddzuq7qeny6qn0ysh3hj6pzmr -
HIER_DIRECT/192.229.163.180
26/Sep/2017:20:12:37     96 10.93.3.47 TCP_HIT/200 11265 GET
https://static.licdn.com/sc/h/ddzuq7qeny6qn0ysh3hj6pzmr - HIER_NONE/-
26/Sep/2017:20:12:41      2 10.93.3.47 TCP_MISS/200 11259 GET
https://static.licdn.com/sc/h/ddzuq7qeny6qn0ysh3hj6pzmr -
HIER_DIRECT/192.229.163.180
26/Sep/2017:20:12:49    225 10.93.3.47 TCP_HIT/200 11266 GET
https://static.licdn.com/sc/h/ddzuq7qeny6qn0ysh3hj6pzmr - HIER_NONE/-
26/Sep/2017:20:12:59      0 10.93.3.47 TCP_HIT/200 11265 GET
https://static.licdn.com/sc/h/ddzuq7qeny6qn0ysh3hj6pzmr - HIER_NONE/-
26/Sep/2017:20:13:03      2 10.93.3.47 TCP_MISS/200 11259 GET
https://static.licdn.com/sc/h/ddzuq7qeny6qn0ysh3hj6pzmr -
HIER_DIRECT/192.229.163.180
26/Sep/2017:20:13:08      0 10.93.3.47 TCP_HIT/200 11265 GET
https://static.licdn.com/sc/h/ddzuq7qeny6qn0ysh3hj6pzmr - HIER_NONE/-
26/Sep/2017:20:13:13      2 10.93.3.47 TCP_MISS/200 11259 GET
https://static.licdn.com/sc/h/ddzuq7qeny6qn0ysh3hj6pzmr -
HIER_DIRECT/192.229.163.180
26/Sep/2017:20:13:27      3 10.93.3.47 TCP_MISS/200 11259 GET
https://static.licdn.com/sc/h/ddzuq7qeny6qn0ysh3hj6pzmr -
HIER_DIRECT/192.229.163.180
26/Sep/2017:20:13:33      2 10.93.3.47 TCP_MISS/200 11259 GET
https://static.licdn.com/sc/h/ddzuq7qeny6qn0ysh3hj6pzmr -
HIER_DIRECT/192.229.163.180
26/Sep/2017:20:13:37      4 10.93.3.47 TCP_MISS/200 11259 GET
https://static.licdn.com/sc/h/ddzuq7qeny6qn0ysh3hj6pzmr -
HIER_DIRECT/192.229.163.180
26/Sep/2017:20:13:42    185 10.93.3.47 TCP_MISS/200 11259 GET
https://static.licdn.com/sc/h/ddzuq7qeny6qn0ysh3hj6pzmr -
HIER_DIRECT/192.229.163.180
26/Sep/2017:20:13:47    349 10.93.3.47 TCP_HIT/200 11265 GET
https://static.licdn.com/sc/h/ddzuq7qeny6qn0ysh3hj6pzmr - HIER_NONE/-
26/Sep/2017:20:13:52     21 10.93.3.47 TCP_MISS/200 11259 GET
https://static.licdn.com/sc/h/ddzuq7qeny6qn0ysh3hj6pzmr -
HIER_DIRECT/192.229.163.180
26/Sep/2017:20:13:56      2 10.93.3.47 TCP_MISS/200 11259 GET
https://static.licdn.com/sc/h/ddzuq7qeny6qn0ysh3hj6pzmr -
HIER_DIRECT/192.229.163.180
26/Sep/2017:20:14:00      7 10.93.3.47 TCP_MISS/200 11259 GET
https://static.licdn.com/sc/h/ddzuq7qeny6qn0ysh3hj6pzmr -
HIER_DIRECT/192.229.163.180
26/Sep/2017:20:14:06     72 10.93.3.47 TCP_HIT/200 11266 GET
https://static.licdn.com/sc/h/ddzuq7qeny6qn0ysh3hj6pzmr - HIER_NONE/-
26/Sep/2017:20:14:36      2 10.93.3.47 TCP_MISS/200 11259 GET
https://static.licdn.com/sc/h/ddzuq7qeny6qn0ysh3hj6pzmr -
HIER_DIRECT/192.229.163.180
26/Sep/2017:20:14:41      3 10.93.3.47 TCP_MISS/200 11259 GET
https://static.licdn.com/sc/h/ddzuq7qeny6qn0ysh3hj6pzmr -
HIER_DIRECT/192.229.163.180
26/Sep/2017:20:14:45      2 10.93.3.47 TCP_MISS/200 11259 GET
https://static.licdn.com/sc/h/ddzuq7qeny6qn0ysh3hj6pzmr -
HIER_DIRECT/192.229.163.180
26/Sep/2017:20:14:54      3 10.93.3.47 TCP_MISS/200 11259 GET
https://static.licdn.com/sc/h/ddzuq7qeny6qn0ysh3hj6pzmr -
HIER_DIRECT/192.229.163.180
--
Aaron Turner
https://synfin.net/         Twitter: @synfinatic
My father once told me that respect for the truth comes close to being
the basis for all morality.  "Something cannot emerge from nothing,"
he said.  This is profound thinking if you understand how unstable
"the truth" can be.  -- Frank Herbert, Dune


On Thu, Sep 28, 2017 at 3:47 PM, Amos Jeffries <squid3 at treenet.co.nz> wrote:
> On 29/09/17 11:29, Aaron Turner wrote:
>>
>> So this grep through my access logs for this single URL does a good
>> job illustrating a rather interesting problem:
>>
>> $ grep -h 'https://static.licdn.com/sc/h/ddzuq7qeny6qn0ysh3hj6pzmr
>> text/css ip_index=0,client=m0078269' access.*.log | sort
>>
>>
> ...
>>
>>
>> At first I thought this was because the because I have a bunch of
>> clients, each of which behaves exactly the same except for one thing:
>> the client includes a unique request header that squid strips off
>> before forwarding to the server (you can see it logged as
>> client=mXXXXX_XXXX).  But in this case I've controlled for that and
>> only grep'd for a single client's request.  I've even tried setting
>> "vary_ignore_expire on", but that doesn't seem to be a complete fix.
>>
>> I can't for the life of me understand why the low hit rate though.
>>
>
> The duration and size fields are quite useful for detecting reasons for
> HIT/MISS.
>
> Request headers should not affect the response caching, unless they are
> listed in the servers Vary header.
>
> In this case the server is delivering broken Vary responses. redbot.org says
> it is using Vary:Accept-Encoding sometimes, so both the Vary and
> Accept-Encoding would be useful info to log.
>
> I expect it is the usual problem of clients fighting over whose variant gets
> cached when this type of server breakage happens - when the Vary header
> changes or disappears, old variants become unfindable until it changes back.
>
> Amos
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users


From xpro6000 at gmail.com  Fri Sep 29 02:46:25 2017
From: xpro6000 at gmail.com (xpro6000)
Date: Thu, 28 Sep 2017 22:46:25 -0400
Subject: [squid-users] Squid on Linux being detected as proxy,
	but not on Windows
Message-ID: <CAFoK1axMre5zBvQfPJ0O9iHEuk7yO-n+S9vep2nS6KSe4Kcr4w@mail.gmail.com>

I have also posted about this on stackexchange but never got a working
answer. I think here is a better place as it's dedicated to Squid

https://unix.stackexchange.com/questions/392849/squid-on-linux-being-detected-as-proxy-but-not-on-windows

Here is the problem I am facing

I have added the following to my squid.conf

via off
forwarded_for off
follow_x_forwarded_for deny all
request_header_access X-Forwarded-For deny all

but when I go to whoer.net it can detect that I am going through a
proxy, and the way it detects it, is by seeing that port 3128 is cached
even though I am using a different port. This does not happen with Squid on
Windows. How can I fix this problem? I'm running Squid on Debian 9

[image: enter image description here] <https://i.stack.imgur.com/OlVVT.png>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20170928/97aa9a5c/attachment.htm>

From eliezer at ngtech.co.il  Fri Sep 29 03:49:09 2017
From: eliezer at ngtech.co.il (Eliezer Croitoru)
Date: Fri, 29 Sep 2017 06:49:09 +0300
Subject: [squid-users] Squid on Linux being detected as proxy,
	but not on Windows
In-Reply-To: <CAFoK1axMre5zBvQfPJ0O9iHEuk7yO-n+S9vep2nS6KSe4Kcr4w@mail.gmail.com>
References: <CAFoK1axMre5zBvQfPJ0O9iHEuk7yO-n+S9vep2nS6KSe4Kcr4w@mail.gmail.com>
Message-ID: <1b0701d338d5$e848f000$b8dad000$@ngtech.co.il>

Hey,

Try to fetch the next url:
https://whoer.net/ports

and see what are the results.
If it states some port as "open" you will need to block access in the tcp level ie send a RESET or DROP packets to the ports which you don't want the world to see as open.
Else then this try to peek at the next article:
https://blog.ipvanish.com/webrtc-security-hole-leaks-real-ip-addresses/

It has a very nice tweak for FireFox that helps to cripple webrtc.

All The Bests,
Eliezer

----
http://ngtech.co.il/lmgtfy/
Linux System Administrator
Mobile: +972-5-28704261
Email: eliezer at ngtech.co.il


From: squid-users [mailto:squid-users-bounces at lists.squid-cache.org] On Behalf Of xpro6000
Sent: Friday, September 29, 2017 05:46
To: squid-users at lists.squid-cache.org
Subject: [squid-users] Squid on Linux being detected as proxy, but not on Windows

I have also posted about this on stackexchange but never got a working answer. I think here is a better place as it's dedicated to Squid

https://unix.stackexchange.com/questions/392849/squid-on-linux-being-detected-as-proxy-but-not-on-windows

Here is the problem I am facing

I have added the following to my squid.conf
via off
forwarded_for off
follow_x_forwarded_for deny all
request_header_access X-Forwarded-For deny all
but when I go to http://whoer.net it can detect that I am going through a proxy, and the way it detects it, is by seeing that port 3128 is cached even though I am using a different port. This does not happen with Squid on Windows. How can I fix this problem? I'm running Squid on Debian 9
https://i.stack.imgur.com/OlVVT.png





From squid3 at treenet.co.nz  Fri Sep 29 05:05:05 2017
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Fri, 29 Sep 2017 18:05:05 +1300
Subject: [squid-users] tuning squid memory (aka avoiding the reaper)
In-Reply-To: <CANAZdzXK4D14OaNpBLCm5ZPcgau_2+B38sXM2mTDe_bB8QwFvg@mail.gmail.com>
References: <CANAZdzXS7jFH_n44BgdYALzE_r3D7t78HRamz9qdKsbtfOVyqg@mail.gmail.com>
 <6fd4bb1b-8ec5-4883-2c57-8debb3890d9a@measurement-factory.com>
 <CANAZdzUDViPoOxXVUyd0=Krz+xdyn_R0viJXxLNEMmmGtFs5jA@mail.gmail.com>
 <54372253-c45a-7ca2-37f8-8ecbdbd3ba27@treenet.co.nz>
 <CANAZdzXK4D14OaNpBLCm5ZPcgau_2+B38sXM2mTDe_bB8QwFvg@mail.gmail.com>
Message-ID: <763e182c-d966-fd6a-ed2b-0c20d17c3274@treenet.co.nz>

On 29/09/17 12:35, Aaron Turner wrote:
> Ok, i'll work on that.  One other thing, is that if I let it run long
> enough, squid will crash with errors like the following:
> 
> FATAL: Received Bus Error...dying.
> 2017/09/28 23:28:09 kid4| Closing HTTP port 10.93.3.4:3128
> 2017/09/28 23:28:09 kid4| Closing HTTP port 127.0.0.1:3128
> 2017/09/28 23:28:09 kid4| storeDirWriteCleanLogs: Starting...
> 2017/09/28 23:28:09 kid4|   Finished.  Wrote 0 entries.
> 2017/09/28 23:28:09 kid4|   Took 0.00 seconds (  0.00 entries/sec).
> CPU Usage: 0.541 seconds = 0.466 user + 0.075 sys
> Maximum Resident Size: 121440 KB
> Page faults with physical i/o: 0
> 
> At first I thought the bus error was hardware, but it's happened on
> two different EC2 instances now.
> 

Yes "Bus Error" is definitely hardware. The OS kernel had an error 
loading some data from RAM into the CPU or something along those lines.

The only things we can do about it is check that your Squid is up to 
date with the system environment - eg fairly recent Squid version built 
with the OS latest compiler version against its current libc or whatever 
the equivalents of those are for your system. If there are binary level 
issues with the libc interfaces weird things can happen.

Amos


From edouardmoran at yahoo.fr  Fri Sep 29 09:23:02 2017
From: edouardmoran at yahoo.fr (EdouardM)
Date: Fri, 29 Sep 2017 02:23:02 -0700 (MST)
Subject: [squid-users] TCP_REFRESH_IGNORED/200 ???
In-Reply-To: <c12281d1-0d02-53e7-cabb-d1b067b5cdc2@measurement-factory.com>
References: <1506616844110-0.post@n4.nabble.com>
 <c12281d1-0d02-53e7-cabb-d1b067b5cdc2@measurement-factory.com>
Message-ID: <1506676982266-0.post@n4.nabble.com>

Hi Alex,
ok, thanks for the clear answer.

best regards



--
Sent from: http://squid-web-proxy-cache.1019090.n4.nabble.com/Squid-Users-f1019091.html


From rentorbuy at yahoo.com  Fri Sep 29 15:11:13 2017
From: rentorbuy at yahoo.com (Vieri)
Date: Fri, 29 Sep 2017 15:11:13 +0000 (UTC)
Subject: [squid-users] squid and http2
References: <1185116159.544480.1506697873988.ref@mail.yahoo.com>
Message-ID: <1185116159.544480.1506697873988@mail.yahoo.com>

Hi,

I gather Squid does not yet support http/2.0?
https://wiki.squid-cache.org/Features/HTTP2

Do you think it can make it into Squid 5?

Thanks,

Vieri


From chiasa.men at web.de  Fri Sep 29 15:32:04 2017
From: chiasa.men at web.de (chiasa.men)
Date: Fri, 29 Sep 2017 17:32:04 +0200
Subject: [squid-users] WARNING: DNS lookup for 'example.com' failed!
Message-ID: <2336927.eIq5oWLnTu@march>

I have to restart squid after each reboot to get it working. I think that is 
because squid starts before systemd has started the network and so the dns 
lookups fail:

journalctl says:
"squid.service: Unit cannot be reloaded because it is inactive."

cache.log contains:
"WARNING: DNS lookup for 'example.com' failed!"


The problems seems to be the squid.service file which I took from ubuntus 
squid package:

# Automatically generated by systemd-sysv-generator

[Unit]
Documentation=man:systemd-sysv-generator(8)
SourcePath=/etc/init.d/squid
Description=LSB: Squid HTTP Proxy version 3.x
Before=multi-user.target
Before=multi-user.target
Before=multi-user.target
Before=graphical.target
Before=shutdown.target
After=network-online.target
After=remote-fs.target
After=systemd-journald-dev-log.socket
After=nss-lookup.target
Wants=network-online.target
Conflicts=shutdown.target

[Service]
Type=forking
Restart=no
TimeoutSec=5min
IgnoreSIGPIPE=no
KillMode=process
GuessMainPID=no
RemainAfterExit=yes
ExecStart=/etc/init.d/squid start
ExecStop=/etc/init.d/squid stop
ExecReload=/etc/init.d/squid reload

Did anybody have the same problem and a nicer solution than adding each 
hostname to /etc/hosts?



From squid3 at treenet.co.nz  Fri Sep 29 15:43:10 2017
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sat, 30 Sep 2017 04:43:10 +1300
Subject: [squid-users] squid and http2
In-Reply-To: <1185116159.544480.1506697873988@mail.yahoo.com>
References: <1185116159.544480.1506697873988.ref@mail.yahoo.com>
 <1185116159.544480.1506697873988@mail.yahoo.com>
Message-ID: <113fddaf-fb5a-e923-5226-37912797f37a@treenet.co.nz>

On 30/09/17 04:11, Vieri wrote:
> Hi,
> 
> I gather Squid does not yet support http/2.0?
> https://wiki.squid-cache.org/Features/HTTP2
> 

Correct.


> Do you think it can make it into Squid 5?
> 

I have been trying for that, but things are going very slowly.


Amos


From squid3 at treenet.co.nz  Fri Sep 29 15:56:59 2017
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sat, 30 Sep 2017 04:56:59 +1300
Subject: [squid-users] WARNING: DNS lookup for 'example.com' failed!
In-Reply-To: <2336927.eIq5oWLnTu@march>
References: <2336927.eIq5oWLnTu@march>
Message-ID: <0ad4b9c3-53ba-f40e-1d44-cefaae5030e1@treenet.co.nz>

On 30/09/17 04:32, chiasa.men wrote:
> I have to restart squid after each reboot to get it working. I think that is
> because squid starts before systemd has started the network and so the dns
> lookups fail:
> 
> journalctl says:
> "squid.service: Unit cannot be reloaded because it is inactive."
> 

So systemd thinks Squid is disabled.


> cache.log contains:
> "WARNING: DNS lookup for 'example.com' failed!"
> 
> 
> The problems seems to be the squid.service file which I took from ubuntus
> squid package:

Nod, quite probably. Squid-3 does not support systemd.

The gory details of that are outlined in:
<https://bugs.debian.org/cgi-bin/bugreport.cgi?bug=855268#26>


Amos


From synfinatic at gmail.com  Fri Sep 29 16:45:19 2017
From: synfinatic at gmail.com (Aaron Turner)
Date: Fri, 29 Sep 2017 09:45:19 -0700
Subject: [squid-users] tuning squid memory (aka avoiding the reaper)
In-Reply-To: <763e182c-d966-fd6a-ed2b-0c20d17c3274@treenet.co.nz>
References: <CANAZdzXS7jFH_n44BgdYALzE_r3D7t78HRamz9qdKsbtfOVyqg@mail.gmail.com>
 <6fd4bb1b-8ec5-4883-2c57-8debb3890d9a@measurement-factory.com>
 <CANAZdzUDViPoOxXVUyd0=Krz+xdyn_R0viJXxLNEMmmGtFs5jA@mail.gmail.com>
 <54372253-c45a-7ca2-37f8-8ecbdbd3ba27@treenet.co.nz>
 <CANAZdzXK4D14OaNpBLCm5ZPcgau_2+B38sXM2mTDe_bB8QwFvg@mail.gmail.com>
 <763e182c-d966-fd6a-ed2b-0c20d17c3274@treenet.co.nz>
Message-ID: <CANAZdzV293R30V4Eex7B99vCJz_5JQ=4+SiaGD-65GivsQzALw@mail.gmail.com>

So this is smelling like a mem leak to me.  First after running for a few hours:

  PID USER      PR  NI    VIRT    RES    SHR S  %CPU %MEM     TIME+ COMMAND
 3188 squid     20   0 3586264 3.175g 1.007g R  63.4 22.2 162:59.38 squid
 3187 squid     20   0 2941332 2.585g 1.005g S  45.5 18.1 129:36.40 squid
 3190 squid     20   0 2641828 2.304g 1.001g R  41.4 16.1 109:49.08 squid
 3189 squid     20   0 2524892 2.182g 0.987g S  42.1 15.3 110:30.96 squid

I configured squid w/ 1GB mem cache, no disk cache, ssl bumping and 4
workers.  Looks like they've all pretty much mapped the 1GB which is
what I'd expect.  However, resident memory is clearly quite high
considering my config.

While this was running I was capturing the output of mgr:mem and
started looking at the numbers.  Now I'm not 100% I understand the
meanings of all the columns, but I also don't see any indication of
what is using all that resident memory.

I've grabbed a few of the mgr:mem output spanning the test and
uploaded them here since I hate sending attachments to lists:

https://synfin.net/misc/watch_share.tar.gz
--
Aaron Turner
https://synfin.net/         Twitter: @synfinatic
My father once told me that respect for the truth comes close to being
the basis for all morality.  "Something cannot emerge from nothing,"
he said.  This is profound thinking if you understand how unstable
"the truth" can be.  -- Frank Herbert, Dune


On Thu, Sep 28, 2017 at 10:05 PM, Amos Jeffries <squid3 at treenet.co.nz> wrote:
> On 29/09/17 12:35, Aaron Turner wrote:
>>
>> Ok, i'll work on that.  One other thing, is that if I let it run long
>> enough, squid will crash with errors like the following:
>>
>> FATAL: Received Bus Error...dying.
>> 2017/09/28 23:28:09 kid4| Closing HTTP port 10.93.3.4:3128
>> 2017/09/28 23:28:09 kid4| Closing HTTP port 127.0.0.1:3128
>> 2017/09/28 23:28:09 kid4| storeDirWriteCleanLogs: Starting...
>> 2017/09/28 23:28:09 kid4|   Finished.  Wrote 0 entries.
>> 2017/09/28 23:28:09 kid4|   Took 0.00 seconds (  0.00 entries/sec).
>> CPU Usage: 0.541 seconds = 0.466 user + 0.075 sys
>> Maximum Resident Size: 121440 KB
>> Page faults with physical i/o: 0
>>
>> At first I thought the bus error was hardware, but it's happened on
>> two different EC2 instances now.
>>
>
> Yes "Bus Error" is definitely hardware. The OS kernel had an error loading
> some data from RAM into the CPU or something along those lines.
>
> The only things we can do about it is check that your Squid is up to date
> with the system environment - eg fairly recent Squid version built with the
> OS latest compiler version against its current libc or whatever the
> equivalents of those are for your system. If there are binary level issues
> with the libc interfaces weird things can happen.
>
> Amos


From xpro6000 at gmail.com  Fri Sep 29 21:01:47 2017
From: xpro6000 at gmail.com (xpro6000)
Date: Fri, 29 Sep 2017 17:01:47 -0400
Subject: [squid-users] Make all IPv6 ips on system to be used as a proxy
Message-ID: <CAFoK1azxn=vp7g_uwcXDrWVp-PoUFqMcC7oiAtXOkv-ge=M0jg@mail.gmail.com>

I have multiple IPs on my Linux system and I need one of my programs to be
able to choose which IP to use. One way to achieve this is to use a proxy
server. But with Squid's default config file, the IP I connect to, is not
the IP used for the outgoing connection.

Is there anything I can do to enable Squid to use the same incoming IP for
the outgoing IP?
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20170929/c89c3ae2/attachment.htm>

From ttelford.groups at gmail.com  Fri Sep 29 21:56:00 2017
From: ttelford.groups at gmail.com (Troy Telford)
Date: Fri, 29 Sep 2017 15:56:00 -0600
Subject: [squid-users] IPv6 TPROXY and ICMP Messages
Message-ID: <9C973B00-679A-4996-A8A8-1A01CB6EA17F@gmail.com>

I've been slowly trying to get this fixed for a few years now... I had my system setup to use Squid + TPROXY using IPv6, and it was working great.

However, a couple of years ago, it simply stopped working, and I?ve been trying to figure out why ever since.

When I try to use IPv6+TPROXY+Squid, most sites simply ?hang? and never load. (TPROXY+IPv4 works fine)

I'm running Debian Sid, Shorewall6 5.0.15.6, and Squid 3.5.23. My ISP provides native IPv6 (Comcast).

I have Squid configured to accept TPROXY on port 3129, and configured clients on port 3128.

The best description (and command to reproduce the error) comes from test-IPv6.com (They suggest a curl command at http://test-ipv6.com/faq_pmtud.html')

Non-TPROXY connections work fine: Disabling TPROXY, or manually configuring the host to use a proxy @ proxy-hostname:3128 are both fine.

When I use TPROXY, there are issues with path MTU detection from the internet to my clients.

When I try the test URL to test-ipv6.com from a client inside the network, and check the packet dump using the following:

$ sudo tcpdump '(ip6 and icmp6 and ip6[40] = 2) or (ip6 and tcp port 80)' 

I see messages along the lines of:

<timestamp> IP6 {remote addr} > {my IPv6 addr}: ICMP6, packet too big, MTU 1280, length 1240

Otherwise, the connection is silent - the curl command doesn?t succeed. (It has no problems succeeding if I set http_proxy, or disable TPROXY).

Is it an issue with my firewall, is there an issue in Linux TPROXY support, is it Squid? I?m not sure.

?shorewall6 show | grep -i icmp? shows the expected allow for ICMP (I?m showing only the type2 ?packet too big? ? but there are the rest suggested in RFC4890)

    0     0 ACCEPT     icmpv6    *      *       ::/0                 ::/0                 ipv6-icmptype 2 /* Needed ICMP types (RFC4890) */

I?m fairly sure that the firewall is configured to pass the ICMPv6 messages from any interface to any interface - Clients inside the network are definitely seeing ?packet too big? messages.

So is there something in Squid which could be causing my path MTU issues? Is there anything i can do to eliminate Squid as a source of error?

THanks.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20170929/2ac5b349/attachment.htm>

From synfinatic at gmail.com  Fri Sep 29 22:37:44 2017
From: synfinatic at gmail.com (Aaron Turner)
Date: Fri, 29 Sep 2017 15:37:44 -0700
Subject: [squid-users] Make all IPv6 ips on system to be used as a proxy
In-Reply-To: <CAFoK1azxn=vp7g_uwcXDrWVp-PoUFqMcC7oiAtXOkv-ge=M0jg@mail.gmail.com>
References: <CAFoK1azxn=vp7g_uwcXDrWVp-PoUFqMcC7oiAtXOkv-ge=M0jg@mail.gmail.com>
Message-ID: <CANAZdzU7q3qfPO7XQWZe+j6ti66UKut97BCHQZi3ZW1HtvjyXg@mail.gmail.com>

If you don't need a proxy server for other reasons, there are better
ways.  Example, per-process routing:

http://www.evolware.org/?p=369

Or if you have control over the source code of the software,
setsockopt() will do it for you as well.
--
Aaron Turner
https://synfin.net/         Twitter: @synfinatic
My father once told me that respect for the truth comes close to being
the basis for all morality.  "Something cannot emerge from nothing,"
he said.  This is profound thinking if you understand how unstable
"the truth" can be.  -- Frank Herbert, Dune


On Fri, Sep 29, 2017 at 2:01 PM, xpro6000 <xpro6000 at gmail.com> wrote:
> I have multiple IPs on my Linux system and I need one of my programs to be
> able to choose which IP to use. One way to achieve this is to use a proxy
> server. But with Squid's default config file, the IP I connect to, is not
> the IP used for the outgoing connection.
>
> Is there anything I can do to enable Squid to use the same incoming IP for
> the outgoing IP?
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>


From synfinatic at gmail.com  Fri Sep 29 23:57:48 2017
From: synfinatic at gmail.com (Aaron Turner)
Date: Fri, 29 Sep 2017 16:57:48 -0700
Subject: [squid-users] tuning squid memory (aka avoiding the reaper)
In-Reply-To: <CANAZdzV293R30V4Eex7B99vCJz_5JQ=4+SiaGD-65GivsQzALw@mail.gmail.com>
References: <CANAZdzXS7jFH_n44BgdYALzE_r3D7t78HRamz9qdKsbtfOVyqg@mail.gmail.com>
 <6fd4bb1b-8ec5-4883-2c57-8debb3890d9a@measurement-factory.com>
 <CANAZdzUDViPoOxXVUyd0=Krz+xdyn_R0viJXxLNEMmmGtFs5jA@mail.gmail.com>
 <54372253-c45a-7ca2-37f8-8ecbdbd3ba27@treenet.co.nz>
 <CANAZdzXK4D14OaNpBLCm5ZPcgau_2+B38sXM2mTDe_bB8QwFvg@mail.gmail.com>
 <763e182c-d966-fd6a-ed2b-0c20d17c3274@treenet.co.nz>
 <CANAZdzV293R30V4Eex7B99vCJz_5JQ=4+SiaGD-65GivsQzALw@mail.gmail.com>
Message-ID: <CANAZdzVsEGcPhCgC0tWAwumoGS4kRUJTXgY8ftP3H9L9HOYq4w@mail.gmail.com>

One more update before I restart squid:

  PID USER      PR  NI    VIRT    RES    SHR S  %CPU %MEM     TIME+ COMMAND
 3188 squid     20   0 4821844 4.337g 1.008g R  53.0 30.4 349:31.24 squid
 3187 squid     20   0 3539696 3.153g 1.008g R  31.9 22.1 259:15.31 squid
 3190 squid     20   0 3198228 2.834g 1.008g S  29.2 19.8 230:23.30 squid
 3189 squid     20   0 3033460 2.680g 1.008g R  27.0 18.8 226:17.63 squid

https://synfin.net/misc/mgr_mem_1000.txt

--
Aaron Turner
https://synfin.net/         Twitter: @synfinatic
My father once told me that respect for the truth comes close to being
the basis for all morality.  "Something cannot emerge from nothing,"
he said.  This is profound thinking if you understand how unstable
"the truth" can be.  -- Frank Herbert, Dune


On Fri, Sep 29, 2017 at 9:45 AM, Aaron Turner <synfinatic at gmail.com> wrote:
> So this is smelling like a mem leak to me.  First after running for a few hours:
>
>   PID USER      PR  NI    VIRT    RES    SHR S  %CPU %MEM     TIME+ COMMAND
>  3188 squid     20   0 3586264 3.175g 1.007g R  63.4 22.2 162:59.38 squid
>  3187 squid     20   0 2941332 2.585g 1.005g S  45.5 18.1 129:36.40 squid
>  3190 squid     20   0 2641828 2.304g 1.001g R  41.4 16.1 109:49.08 squid
>  3189 squid     20   0 2524892 2.182g 0.987g S  42.1 15.3 110:30.96 squid
>
> I configured squid w/ 1GB mem cache, no disk cache, ssl bumping and 4
> workers.  Looks like they've all pretty much mapped the 1GB which is
> what I'd expect.  However, resident memory is clearly quite high
> considering my config.
>
> While this was running I was capturing the output of mgr:mem and
> started looking at the numbers.  Now I'm not 100% I understand the
> meanings of all the columns, but I also don't see any indication of
> what is using all that resident memory.
>
> I've grabbed a few of the mgr:mem output spanning the test and
> uploaded them here since I hate sending attachments to lists:
>
> https://synfin.net/misc/watch_share.tar.gz
> --
> Aaron Turner
> https://synfin.net/         Twitter: @synfinatic
> My father once told me that respect for the truth comes close to being
> the basis for all morality.  "Something cannot emerge from nothing,"
> he said.  This is profound thinking if you understand how unstable
> "the truth" can be.  -- Frank Herbert, Dune
>
>
> On Thu, Sep 28, 2017 at 10:05 PM, Amos Jeffries <squid3 at treenet.co.nz> wrote:
>> On 29/09/17 12:35, Aaron Turner wrote:
>>>
>>> Ok, i'll work on that.  One other thing, is that if I let it run long
>>> enough, squid will crash with errors like the following:
>>>
>>> FATAL: Received Bus Error...dying.
>>> 2017/09/28 23:28:09 kid4| Closing HTTP port 10.93.3.4:3128
>>> 2017/09/28 23:28:09 kid4| Closing HTTP port 127.0.0.1:3128
>>> 2017/09/28 23:28:09 kid4| storeDirWriteCleanLogs: Starting...
>>> 2017/09/28 23:28:09 kid4|   Finished.  Wrote 0 entries.
>>> 2017/09/28 23:28:09 kid4|   Took 0.00 seconds (  0.00 entries/sec).
>>> CPU Usage: 0.541 seconds = 0.466 user + 0.075 sys
>>> Maximum Resident Size: 121440 KB
>>> Page faults with physical i/o: 0
>>>
>>> At first I thought the bus error was hardware, but it's happened on
>>> two different EC2 instances now.
>>>
>>
>> Yes "Bus Error" is definitely hardware. The OS kernel had an error loading
>> some data from RAM into the CPU or something along those lines.
>>
>> The only things we can do about it is check that your Squid is up to date
>> with the system environment - eg fairly recent Squid version built with the
>> OS latest compiler version against its current libc or whatever the
>> equivalents of those are for your system. If there are binary level issues
>> with the libc interfaces weird things can happen.
>>
>> Amos


From xpro6000 at gmail.com  Sat Sep 30 00:13:37 2017
From: xpro6000 at gmail.com (xpro6000)
Date: Fri, 29 Sep 2017 20:13:37 -0400
Subject: [squid-users] Make all IPv6 ips on system to be used as a proxy
In-Reply-To: <CANAZdzU7q3qfPO7XQWZe+j6ti66UKut97BCHQZi3ZW1HtvjyXg@mail.gmail.com>
References: <CAFoK1azxn=vp7g_uwcXDrWVp-PoUFqMcC7oiAtXOkv-ge=M0jg@mail.gmail.com>
 <CANAZdzU7q3qfPO7XQWZe+j6ti66UKut97BCHQZi3ZW1HtvjyXg@mail.gmail.com>
Message-ID: <CAFoK1ayf551UHpNGmdObTu+EVkuFcJhCjuajRhMOj-y57hKrfg@mail.gmail.com>

The program is multithreaded so the process based routing won't work. There
is no simple way to make Squid use the connecting IP for the outgoing IP?

On Fri, Sep 29, 2017 at 6:37 PM, Aaron Turner <synfinatic at gmail.com> wrote:

> If you don't need a proxy server for other reasons, there are better
> ways.  Example, per-process routing:
>
> http://www.evolware.org/?p=369
>
> Or if you have control over the source code of the software,
> setsockopt() will do it for you as well.
> --
> Aaron Turner
> https://synfin.net/         Twitter: @synfinatic
> My father once told me that respect for the truth comes close to being
> the basis for all morality.  "Something cannot emerge from nothing,"
> he said.  This is profound thinking if you understand how unstable
> "the truth" can be.  -- Frank Herbert, Dune
>
>
> On Fri, Sep 29, 2017 at 2:01 PM, xpro6000 <xpro6000 at gmail.com> wrote:
> > I have multiple IPs on my Linux system and I need one of my programs to
> be
> > able to choose which IP to use. One way to achieve this is to use a proxy
> > server. But with Squid's default config file, the IP I connect to, is not
> > the IP used for the outgoing connection.
> >
> > Is there anything I can do to enable Squid to use the same incoming IP
> for
> > the outgoing IP?
> >
> > _______________________________________________
> > squid-users mailing list
> > squid-users at lists.squid-cache.org
> > http://lists.squid-cache.org/listinfo/squid-users
> >
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20170929/16f3adc7/attachment.htm>

From synfinatic at gmail.com  Sat Sep 30 00:42:14 2017
From: synfinatic at gmail.com (Aaron Turner)
Date: Fri, 29 Sep 2017 17:42:14 -0700
Subject: [squid-users] Make all IPv6 ips on system to be used as a proxy
In-Reply-To: <CAFoK1ayf551UHpNGmdObTu+EVkuFcJhCjuajRhMOj-y57hKrfg@mail.gmail.com>
References: <CAFoK1azxn=vp7g_uwcXDrWVp-PoUFqMcC7oiAtXOkv-ge=M0jg@mail.gmail.com>
 <CANAZdzU7q3qfPO7XQWZe+j6ti66UKut97BCHQZi3ZW1HtvjyXg@mail.gmail.com>
 <CAFoK1ayf551UHpNGmdObTu+EVkuFcJhCjuajRhMOj-y57hKrfg@mail.gmail.com>
Message-ID: <CANAZdzWdE-PPOsXcTr3TjiV1zzh1ZNk7s3eoPYbQq8gGzNR7iA@mail.gmail.com>

Run multiple instances of squid, one per IP address?  I'm not aware of
some magic config option to do what you want.

Seriously though, using a proxy to control your outgoing IP address is
weird.    Use setsockopt(SO_BINDTODEVICE) in your code.
--
Aaron Turner
https://synfin.net/         Twitter: @synfinatic
My father once told me that respect for the truth comes close to being
the basis for all morality.  "Something cannot emerge from nothing,"
he said.  This is profound thinking if you understand how unstable
"the truth" can be.  -- Frank Herbert, Dune


On Fri, Sep 29, 2017 at 5:13 PM, xpro6000 <xpro6000 at gmail.com> wrote:
> The program is multithreaded so the process based routing won't work. There
> is no simple way to make Squid use the connecting IP for the outgoing IP?
>
> On Fri, Sep 29, 2017 at 6:37 PM, Aaron Turner <synfinatic at gmail.com> wrote:
>>
>> If you don't need a proxy server for other reasons, there are better
>> ways.  Example, per-process routing:
>>
>> http://www.evolware.org/?p=369
>>
>> Or if you have control over the source code of the software,
>> setsockopt() will do it for you as well.
>> --
>> Aaron Turner
>> https://synfin.net/         Twitter: @synfinatic
>> My father once told me that respect for the truth comes close to being
>> the basis for all morality.  "Something cannot emerge from nothing,"
>> he said.  This is profound thinking if you understand how unstable
>> "the truth" can be.  -- Frank Herbert, Dune
>>
>>
>> On Fri, Sep 29, 2017 at 2:01 PM, xpro6000 <xpro6000 at gmail.com> wrote:
>> > I have multiple IPs on my Linux system and I need one of my programs to
>> > be
>> > able to choose which IP to use. One way to achieve this is to use a
>> > proxy
>> > server. But with Squid's default config file, the IP I connect to, is
>> > not
>> > the IP used for the outgoing connection.
>> >
>> > Is there anything I can do to enable Squid to use the same incoming IP
>> > for
>> > the outgoing IP?
>> >
>> > _______________________________________________
>> > squid-users mailing list
>> > squid-users at lists.squid-cache.org
>> > http://lists.squid-cache.org/listinfo/squid-users
>> >
>
>


From squid3 at treenet.co.nz  Sat Sep 30 02:37:47 2017
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sat, 30 Sep 2017 15:37:47 +1300
Subject: [squid-users] IPv6 TPROXY and ICMP Messages
In-Reply-To: <9C973B00-679A-4996-A8A8-1A01CB6EA17F@gmail.com>
References: <9C973B00-679A-4996-A8A8-1A01CB6EA17F@gmail.com>
Message-ID: <ce8a04f9-1210-0ee5-1088-8ce7342513fb@treenet.co.nz>

On 30/09/17 10:56, Troy Telford wrote:
> I?m fairly sure that the firewall is configured to pass the ICMPv6 
> messages from any interface to any interface - Clients inside the 
> network are definitely seeing ?packet too big? messages.
> 
> 
> So is there something in Squid which could be causing my path MTU 
> issues? Is there anything i can do to eliminate Squid as a source of error?
> 

Squid just schedules data for TCP to deliver and lets the OS handle the 
details. Sounds to me like your TCP stack is not handling those ICMP MTU 
responses correctly.

IIRC the kernel versions 2.6.32-2.6.36 were found to have issues with 
their ICMP. But those got resolved.

Amos


From jeffmerkey at gmail.com  Sat Sep 30 20:27:46 2017
From: jeffmerkey at gmail.com (Jeffrey Merkey)
Date: Sat, 30 Sep 2017 14:27:46 -0600
Subject: [squid-users] SSL Bump Failures with Google and Wikipedia
Message-ID: <CA+ekxPVs_Nho43T=jtF+pvzs5jNn9j63mWLxpDJda=VkVhkX6w@mail.gmail.com>

Hello All,

I have been working with the squid server and icap and I have been
running into problems with content cached from google and wikipedia.
Some sites using https, such as Centos.org work perfectly with ssl
bumping and I get the decrypted content as html and it's readable.
Other sites, such as google and wikipedia return what looks like
encrypted traffic, or perhaps mime encoded data, I am not sure which.

Are there cases where squid will default to direct mode and not
decrypt the traffic?  I am using the latest squid server 3.5.27.  I
really would like to get this working with google and wikipedia.  I
reviewed the page source code from the browser viewer and it looks
nothing like the data I am getting via the icap server.

Any assistance would be greatly appreciated.

The config I am using is:

#
# Recommended minimum configuration:
#

# Example rule allowing access from your local networks.
# Adapt to list your (internal) IP networks from where browsing
# should be allowed

acl localnet src 127.0.0.1
acl localnet src 10.0.0.0/8     # RFC1918 possible internal network
acl localnet src 172.16.0.0/12  # RFC1918 possible internal network
acl localnet src 192.168.0.0/16 # RFC1918 possible internal network
acl localnet src fc00::/7       # RFC 4193 local private network range
acl localnet src fe80::/10      # RFC 4291 link-local (directly
plugged) machines

acl SSL_ports port 443
acl Safe_ports port 80          # http
acl Safe_ports port 21          # ftp
acl Safe_ports port 443         # https
acl Safe_ports port 70          # gopher
acl Safe_ports port 210         # wais
acl Safe_ports port 1025-65535  # unregistered ports
acl Safe_ports port 280         # http-mgmt
acl Safe_ports port 488         # gss-http
acl Safe_ports port 591         # filemaker
acl Safe_ports port 777         # multiling http
acl CONNECT method CONNECT

#
# Recommended minimum Access Permission configuration:
#
# Deny requests to certain unsafe ports
http_access deny !Safe_ports

# Deny CONNECT to other than secure SSL ports
http_access deny CONNECT !SSL_ports

# Only allow cachemgr access from localhost
http_access allow localhost manager
http_access deny manager

# We strongly recommend the following be uncommented to protect innocent
# web applications running on the proxy server who think the only
# one who can access services on "localhost" is a local user
#http_access deny to_localhost

#
# INSERT YOUR OWN RULE(S) HERE TO ALLOW ACCESS FROM YOUR CLIENTS
#

# Example rule allowing access from your local networks.
# Adapt localnet in the ACL section to list your (internal) IP networks
# from where browsing should be allowed
http_access allow localnet
http_access allow localhost

# And finally deny all other access to this proxy
http_access deny all

# Squid normally listens to port 3128
#http_port 3128

# Uncomment and adjust the following to add a disk cache directory.
#cache_dir ufs /usr/local/squid/var/cache/squid 100 16 256

# Leave coredumps in the first cache dir
coredump_dir /usr/local/squid/var/cache/squid

#
# Add any of your own refresh_pattern entries above these.
#
refresh_pattern ^ftp:           1440    20%     10080
refresh_pattern ^gopher:        1440    0%      1440
refresh_pattern -i (/cgi-bin/|\?) 0     0%      0
refresh_pattern .               0       20%     4320

http_port 3128 ssl-bump generate-host-certificates=on
dynamic_cert_mem_cache_size=4MB cert=/etc/squid/ssl_cert/myCA.pem
http_port 3129

# SSL Bump Config
always_direct allow all
ssl_bump server-first all
sslproxy_cert_error deny all
sslproxy_flags DONT_VERIFY_PEER
sslcrtd_program /usr/local/squid/libexec/ssl_crtd -s /var/lib/ssl_db
-M 4MB sslcrtd_children 8 startup=1 idle=1

# For squid 3.5.x
#sslcrtd_program /usr/local/squid/libexec/ssl_crtd -s /var/lib/ssl_db -M 4MB

# For squid 4.x
# sslcrtd_program /usr/local/squid/libexec/security_file_certgen -s
/var/lib/ssl_db -M 4MB

icap_enable on
icap_send_client_ip on
icap_send_client_username on
icap_client_username_header X-Authenticated-User
icap_preview_enable on
icap_preview_size 1024
icap_service service_avi_req reqmod_precache
icap://127.0.0.1:1344/request bypass=1
adaptation_access service_avi_req allow all
icap_service service_avi_resp respmod_precache
icap://127.0.0.1:1344/cherokee bypass=0
adaptation_access service_avi_resp allow all

Jeff


From eliezer at ngtech.co.il  Sat Sep 30 21:18:39 2017
From: eliezer at ngtech.co.il (Eliezer Croitoru)
Date: Sun, 1 Oct 2017 00:18:39 +0300
Subject: [squid-users] SSL Bump Failures with Google and Wikipedia
In-Reply-To: <CA+ekxPVs_Nho43T=jtF+pvzs5jNn9j63mWLxpDJda=VkVhkX6w@mail.gmail.com>
References: <CA+ekxPVs_Nho43T=jtF+pvzs5jNn9j63mWLxpDJda=VkVhkX6w@mail.gmail.com>
Message-ID: <01c001d33a31$af7c16f0$0e7444d0$@ngtech.co.il>

Hey Jeffrey,

What happens when you disable the next icap service this way:
icap_service service_avi_resp respmod_precache icap://127.0.0.1:1344/cherokee bypass=0
adaptation_access service_avi_resp deny all

Is it still the same?
What I suspect is that the requests are defined to accept gzip compressed objects and the icap service is not "gnuzip" them which results in what you see.

To make sure that squid is not at fault here try to disable both icap services and then add then one at a time and see which of this triangle is giving you trouble.
I enhanced an ICAP library which is written in GoLang at:
https://github.com/elico/icap

And I have couple examples on how to work with http requests and responses at:
https://github.com/andybalholm/redwood/
https://github.com/andybalholm/redwood/search?utf8=%E2%9C%93&q=gzip&type=

Let me know if you need help finding out the issue.

All The Bests,
Eliezer

----
Eliezer Croitoru
Linux System Administrator
Mobile: +972-5-28704261
Email: eliezer at ngtech.co.il



-----Original Message-----
From: squid-users [mailto:squid-users-bounces at lists.squid-cache.org] On Behalf Of Jeffrey Merkey
Sent: Saturday, September 30, 2017 23:28
To: squid-users <squid-users at lists.squid-cache.org>
Subject: [squid-users] SSL Bump Failures with Google and Wikipedia

Hello All,

I have been working with the squid server and icap and I have been
running into problems with content cached from google and wikipedia.
Some sites using https, such as Centos.org work perfectly with ssl
bumping and I get the decrypted content as html and it's readable.
Other sites, such as google and wikipedia return what looks like
encrypted traffic, or perhaps mime encoded data, I am not sure which.

Are there cases where squid will default to direct mode and not
decrypt the traffic?  I am using the latest squid server 3.5.27.  I
really would like to get this working with google and wikipedia.  I
reviewed the page source code from the browser viewer and it looks
nothing like the data I am getting via the icap server.

Any assistance would be greatly appreciated.

The config I am using is:

#
# Recommended minimum configuration:
#

# Example rule allowing access from your local networks.
# Adapt to list your (internal) IP networks from where browsing
# should be allowed

acl localnet src 127.0.0.1
acl localnet src 10.0.0.0/8     # RFC1918 possible internal network
acl localnet src 172.16.0.0/12  # RFC1918 possible internal network
acl localnet src 192.168.0.0/16 # RFC1918 possible internal network
acl localnet src fc00::/7       # RFC 4193 local private network range
acl localnet src fe80::/10      # RFC 4291 link-local (directly
plugged) machines

acl SSL_ports port 443
acl Safe_ports port 80          # http
acl Safe_ports port 21          # ftp
acl Safe_ports port 443         # https
acl Safe_ports port 70          # gopher
acl Safe_ports port 210         # wais
acl Safe_ports port 1025-65535  # unregistered ports
acl Safe_ports port 280         # http-mgmt
acl Safe_ports port 488         # gss-http
acl Safe_ports port 591         # filemaker
acl Safe_ports port 777         # multiling http
acl CONNECT method CONNECT

#
# Recommended minimum Access Permission configuration:
#
# Deny requests to certain unsafe ports
http_access deny !Safe_ports

# Deny CONNECT to other than secure SSL ports
http_access deny CONNECT !SSL_ports

# Only allow cachemgr access from localhost
http_access allow localhost manager
http_access deny manager

# We strongly recommend the following be uncommented to protect innocent
# web applications running on the proxy server who think the only
# one who can access services on "localhost" is a local user
#http_access deny to_localhost

#
# INSERT YOUR OWN RULE(S) HERE TO ALLOW ACCESS FROM YOUR CLIENTS
#

# Example rule allowing access from your local networks.
# Adapt localnet in the ACL section to list your (internal) IP networks
# from where browsing should be allowed
http_access allow localnet
http_access allow localhost

# And finally deny all other access to this proxy
http_access deny all

# Squid normally listens to port 3128
#http_port 3128

# Uncomment and adjust the following to add a disk cache directory.
#cache_dir ufs /usr/local/squid/var/cache/squid 100 16 256

# Leave coredumps in the first cache dir
coredump_dir /usr/local/squid/var/cache/squid

#
# Add any of your own refresh_pattern entries above these.
#
refresh_pattern ^ftp:           1440    20%     10080
refresh_pattern ^gopher:        1440    0%      1440
refresh_pattern -i (/cgi-bin/|\?) 0     0%      0
refresh_pattern .               0       20%     4320

http_port 3128 ssl-bump generate-host-certificates=on
dynamic_cert_mem_cache_size=4MB cert=/etc/squid/ssl_cert/myCA.pem
http_port 3129

# SSL Bump Config
always_direct allow all
ssl_bump server-first all
sslproxy_cert_error deny all
sslproxy_flags DONT_VERIFY_PEER
sslcrtd_program /usr/local/squid/libexec/ssl_crtd -s /var/lib/ssl_db
-M 4MB sslcrtd_children 8 startup=1 idle=1

# For squid 3.5.x
#sslcrtd_program /usr/local/squid/libexec/ssl_crtd -s /var/lib/ssl_db -M 4MB

# For squid 4.x
# sslcrtd_program /usr/local/squid/libexec/security_file_certgen -s
/var/lib/ssl_db -M 4MB

icap_enable on
icap_send_client_ip on
icap_send_client_username on
icap_client_username_header X-Authenticated-User
icap_preview_enable on
icap_preview_size 1024
icap_service service_avi_req reqmod_precache icap://127.0.0.1:1344/request bypass=1
adaptation_access service_avi_req allow all

icap_service service_avi_resp respmod_precache icap://127.0.0.1:1344/cherokee bypass=0
adaptation_access service_avi_resp allow all

Jeff
_______________________________________________
squid-users mailing list
squid-users at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-users



From eliezer at ngtech.co.il  Sat Sep 30 21:21:48 2017
From: eliezer at ngtech.co.il (Eliezer Croitoru)
Date: Sun, 1 Oct 2017 00:21:48 +0300
Subject: [squid-users] Make all IPv6 ips on system to be used as a proxy
In-Reply-To: <CAFoK1azxn=vp7g_uwcXDrWVp-PoUFqMcC7oiAtXOkv-ge=M0jg@mail.gmail.com>
References: <CAFoK1azxn=vp7g_uwcXDrWVp-PoUFqMcC7oiAtXOkv-ge=M0jg@mail.gmail.com>
Message-ID: <!&!AAAAAAAAAAAuAAAAAAAAAGDVRrtF13VDjYYZR9MgvgYBAMO2jhD3dRHOtM0AqgC7tuYAAAAAAA4AABAAAACs9EXxHh6PRok3B4eK0ibsAQAAAAA=@ngtech.co.il>

Anyone mentioned tproxy?
It works for me...

Eliezer

----
http://ngtech.co.il/lmgtfy/
Linux System Administrator
Mobile: +972-5-28704261
Email: eliezer at ngtech.co.il


From: squid-users [mailto:squid-users-bounces at lists.squid-cache.org] On Behalf Of xpro6000
Sent: Saturday, September 30, 2017 00:02
To: squid-users at lists.squid-cache.org
Subject: [squid-users] Make all IPv6 ips on system to be used as a proxy

I have multiple IPs on my Linux system and I need one of my programs to be able to choose which IP to use. One way to achieve this is to use a proxy server. But with Squid's default config file, the IP I connect to, is not the IP used for the outgoing connection. 

Is there anything I can do to enable Squid to use the same incoming IP for the outgoing IP?



From jeffmerkey at gmail.com  Sat Sep 30 21:49:26 2017
From: jeffmerkey at gmail.com (Jeffrey Merkey)
Date: Sat, 30 Sep 2017 15:49:26 -0600
Subject: [squid-users] SSL Bump Failures with Google and Wikipedia
In-Reply-To: <01c001d33a31$af7c16f0$0e7444d0$@ngtech.co.il>
References: <CA+ekxPVs_Nho43T=jtF+pvzs5jNn9j63mWLxpDJda=VkVhkX6w@mail.gmail.com>
 <01c001d33a31$af7c16f0$0e7444d0$@ngtech.co.il>
Message-ID: <CA+ekxPUSMGNS1tr7nztMM2NaJVQgZyOLK8S+QZKhsoWBaR8tUA@mail.gmail.com>

On 9/30/17, Eliezer Croitoru <eliezer at ngtech.co.il> wrote:
> Hey Jeffrey,
>
> What happens when you disable the next icap service this way:
> icap_service service_avi_resp respmod_precache
> icap://127.0.0.1:1344/cherokee bypass=0
> adaptation_access service_avi_resp deny all
>
> Is it still the same?
> What I suspect is that the requests are defined to accept gzip compressed
> objects and the icap service is not "gnuzip" them which results in what you
> see.
>
> To make sure that squid is not at fault here try to disable both icap
> services and then add then one at a time and see which of this triangle is
> giving you trouble.
> I enhanced an ICAP library which is written in GoLang at:
> https://github.com/elico/icap
>
> And I have couple examples on how to work with http requests and responses
> at:
> https://github.com/andybalholm/redwood/
> https://github.com/andybalholm/redwood/search?utf8=%E2%9C%93&q=gzip&type=
>
> Let me know if you need help finding out the issue.
>
> All The Bests,
> Eliezer
>
> ----
> Eliezer Croitoru
> Linux System Administrator
> Mobile: +972-5-28704261
> Email: eliezer at ngtech.co.il
>
>
>
> -----Original Message-----
> From: squid-users [mailto:squid-users-bounces at lists.squid-cache.org] On
> Behalf Of Jeffrey Merkey
> Sent: Saturday, September 30, 2017 23:28
> To: squid-users <squid-users at lists.squid-cache.org>
> Subject: [squid-users] SSL Bump Failures with Google and Wikipedia
>
> Hello All,
>
> I have been working with the squid server and icap and I have been
> running into problems with content cached from google and wikipedia.
> Some sites using https, such as Centos.org work perfectly with ssl
> bumping and I get the decrypted content as html and it's readable.
> Other sites, such as google and wikipedia return what looks like
> encrypted traffic, or perhaps mime encoded data, I am not sure which.
>
> Are there cases where squid will default to direct mode and not
> decrypt the traffic?  I am using the latest squid server 3.5.27.  I
> really would like to get this working with google and wikipedia.  I
> reviewed the page source code from the browser viewer and it looks
> nothing like the data I am getting via the icap server.
>
> Any assistance would be greatly appreciated.
>
> The config I am using is:
>
> #
> # Recommended minimum configuration:
> #
>
> # Example rule allowing access from your local networks.
> # Adapt to list your (internal) IP networks from where browsing
> # should be allowed
>
> acl localnet src 127.0.0.1
> acl localnet src 10.0.0.0/8     # RFC1918 possible internal network
> acl localnet src 172.16.0.0/12  # RFC1918 possible internal network
> acl localnet src 192.168.0.0/16 # RFC1918 possible internal network
> acl localnet src fc00::/7       # RFC 4193 local private network range
> acl localnet src fe80::/10      # RFC 4291 link-local (directly
> plugged) machines
>
> acl SSL_ports port 443
> acl Safe_ports port 80          # http
> acl Safe_ports port 21          # ftp
> acl Safe_ports port 443         # https
> acl Safe_ports port 70          # gopher
> acl Safe_ports port 210         # wais
> acl Safe_ports port 1025-65535  # unregistered ports
> acl Safe_ports port 280         # http-mgmt
> acl Safe_ports port 488         # gss-http
> acl Safe_ports port 591         # filemaker
> acl Safe_ports port 777         # multiling http
> acl CONNECT method CONNECT
>
> #
> # Recommended minimum Access Permission configuration:
> #
> # Deny requests to certain unsafe ports
> http_access deny !Safe_ports
>
> # Deny CONNECT to other than secure SSL ports
> http_access deny CONNECT !SSL_ports
>
> # Only allow cachemgr access from localhost
> http_access allow localhost manager
> http_access deny manager
>
> # We strongly recommend the following be uncommented to protect innocent
> # web applications running on the proxy server who think the only
> # one who can access services on "localhost" is a local user
> #http_access deny to_localhost
>
> #
> # INSERT YOUR OWN RULE(S) HERE TO ALLOW ACCESS FROM YOUR CLIENTS
> #
>
> # Example rule allowing access from your local networks.
> # Adapt localnet in the ACL section to list your (internal) IP networks
> # from where browsing should be allowed
> http_access allow localnet
> http_access allow localhost
>
> # And finally deny all other access to this proxy
> http_access deny all
>
> # Squid normally listens to port 3128
> #http_port 3128
>
> # Uncomment and adjust the following to add a disk cache directory.
> #cache_dir ufs /usr/local/squid/var/cache/squid 100 16 256
>
> # Leave coredumps in the first cache dir
> coredump_dir /usr/local/squid/var/cache/squid
>
> #
> # Add any of your own refresh_pattern entries above these.
> #
> refresh_pattern ^ftp:           1440    20%     10080
> refresh_pattern ^gopher:        1440    0%      1440
> refresh_pattern -i (/cgi-bin/|\?) 0     0%      0
> refresh_pattern .               0       20%     4320
>
> http_port 3128 ssl-bump generate-host-certificates=on
> dynamic_cert_mem_cache_size=4MB cert=/etc/squid/ssl_cert/myCA.pem
> http_port 3129
>
> # SSL Bump Config
> always_direct allow all
> ssl_bump server-first all
> sslproxy_cert_error deny all
> sslproxy_flags DONT_VERIFY_PEER
> sslcrtd_program /usr/local/squid/libexec/ssl_crtd -s /var/lib/ssl_db
> -M 4MB sslcrtd_children 8 startup=1 idle=1
>
> # For squid 3.5.x
> #sslcrtd_program /usr/local/squid/libexec/ssl_crtd -s /var/lib/ssl_db -M
> 4MB
>
> # For squid 4.x
> # sslcrtd_program /usr/local/squid/libexec/security_file_certgen -s
> /var/lib/ssl_db -M 4MB
>
> icap_enable on
> icap_send_client_ip on
> icap_send_client_username on
> icap_client_username_header X-Authenticated-User
> icap_preview_enable on
> icap_preview_size 1024
> icap_service service_avi_req reqmod_precache icap://127.0.0.1:1344/request
> bypass=1
> adaptation_access service_avi_req allow all
>
> icap_service service_avi_resp respmod_precache
> icap://127.0.0.1:1344/cherokee bypass=0
> adaptation_access service_avi_resp allow all
>
> Jeff
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>
>

Eliezer,

Well, you certainly hit the nail on the head.  I added the following
code to check the content being sent to the icap server from squid,
and here is what I found when I check the headers being sent from the
remote web server:

Code to check for content type and encoding received by the icap
server added to c-icap:

    hdrs = ci_http_response_headers(req);
    content_type = ci_headers_value(hdrs, "Content-Type");
    if (content_type)
       ci_debug_printf(1,"srv_cherokee:  content-type: %s\n",
                       content_type);

    content_encoding = ci_headers_value(hdrs, "Content-Encoding");
    if (content_encoding)
       ci_debug_printf(1,"srv_cherokee:  content-encoding: %s\n",
                       content_encoding);

And the output from scanned pages sent over from squid:

srv_cherokee:  init request 0x7f3dbc008eb0
pool hits:1 allocations: 1
Allocating from objects pool object 5
pool hits:1 allocations: 1
Geting buffer from pool 4096:1
Requested service: cherokee
Read preview data if there are and process request
srv_cherokee:  content-type: text/html; charset=utf-8
srv_cherokee:  content-encoding: gzip         <-- As you stated, I am
getting gzipped data
srv_cherokee:  we expect to read :-1 body data
Allow 204...
Preview handler return allow 204 response
srv_cherokee:  release request 0x7f3dbc008eb0
Store buffer to long pool 4096:1
Storing to objects pool object 5
Log request to access log file /var/log/i-cap_access.log


Wikipedia  at https://en.wikipedia.org/wiki/HTTP_compression describes
the process as:

" ...
   Compression scheme negotiation[edit]
   In most cases, excluding the SDCH, the negotiation is done in two
steps, described in
   RFC 2616:

   1. The web client advertises which compression schemes it supports
by including a list
   of tokens in the HTTP request. For Content-Encoding, the list in a
field called Accept -
   Encoding; for Transfer-Encoding, the field is called TE.

   GET /encrypted-area HTTP/1.1
   Host: www.example.com
   Accept-Encoding: gzip, deflate

   2. If the server supports one or more compression schemes, the
outgoing data may be
   compressed by one or more methods supported by both parties. If
this is the case, the
   server will add a Content-Encoding or Transfer-Encoding field in
the HTTP response with
   the used schemes, separated by commas.

   HTTP/1.1 200 OK
   Date: mon, 26 June 2016 22:38:34 GMT
   Server: Apache/1.3.3.7 (Unix)  (Red-Hat/Linux)
   Last-Modified: Wed, 08 Jan 2003 23:11:55 GMT
   Accept-Ranges: bytes
   Content-Length: 438
   Connection: close
   Content-Type: text/html; charset=UTF-8
   Content-Encoding: gzip

   The web server is by no means obligated to use any compression method ? this
   depends on the internal settings of the web server and also may
depend on the internal
   architecture of the website in question.

   In case of SDCH a dictionary negotiation is also required, which
may involve additional
   steps, like downloading a proper dictionary from .
.."


So, it looks like it is a feature of the browser.  So, is it possible
to have squid gunzip the data or configure the browser not to send the
header  to remove "Accept-Encoding: gzip, deflate" from the request
sent to the remote server telling it to gzip the data?

Thanks

Jeff


From jeffmerkey at gmail.com  Sat Sep 30 22:10:26 2017
From: jeffmerkey at gmail.com (Jeffrey Merkey)
Date: Sat, 30 Sep 2017 16:10:26 -0600
Subject: [squid-users] SSL Bump Failures with Google and Wikipedia
In-Reply-To: <CA+ekxPUSMGNS1tr7nztMM2NaJVQgZyOLK8S+QZKhsoWBaR8tUA@mail.gmail.com>
References: <CA+ekxPVs_Nho43T=jtF+pvzs5jNn9j63mWLxpDJda=VkVhkX6w@mail.gmail.com>
 <01c001d33a31$af7c16f0$0e7444d0$@ngtech.co.il>
 <CA+ekxPUSMGNS1tr7nztMM2NaJVQgZyOLK8S+QZKhsoWBaR8tUA@mail.gmail.com>
Message-ID: <CA+ekxPULKT4u71ey652UbheHvBfN79_Lji8imvCHRmObvt35Gw@mail.gmail.com>

On 9/30/17, Jeffrey Merkey <jeffmerkey at gmail.com> wrote:
> On 9/30/17, Eliezer Croitoru <eliezer at ngtech.co.il> wrote:
>> Hey Jeffrey,
>>
>> What happens when you disable the next icap service this way:
>> icap_service service_avi_resp respmod_precache
>> icap://127.0.0.1:1344/cherokee bypass=0
>> adaptation_access service_avi_resp deny all
>>
>> Is it still the same?
>> What I suspect is that the requests are defined to accept gzip compressed
>> objects and the icap service is not "gnuzip" them which results in what
>> you
>> see.
>>
>> To make sure that squid is not at fault here try to disable both icap
>> services and then add then one at a time and see which of this triangle
>> is
>> giving you trouble.
>> I enhanced an ICAP library which is written in GoLang at:
>> https://github.com/elico/icap
>>
>> And I have couple examples on how to work with http requests and
>> responses
>> at:
>> https://github.com/andybalholm/redwood/
>> https://github.com/andybalholm/redwood/search?utf8=%E2%9C%93&q=gzip&type=
>>
>> Let me know if you need help finding out the issue.
>>
>> All The Bests,
>> Eliezer
>>
>> ----
>> Eliezer Croitoru
>> Linux System Administrator
>> Mobile: +972-5-28704261
>> Email: eliezer at ngtech.co.il
>>
>>
>>
>> -----Original Message-----
>> From: squid-users [mailto:squid-users-bounces at lists.squid-cache.org] On
>> Behalf Of Jeffrey Merkey
>> Sent: Saturday, September 30, 2017 23:28
>> To: squid-users <squid-users at lists.squid-cache.org>
>> Subject: [squid-users] SSL Bump Failures with Google and Wikipedia
>>
>> Hello All,
>>
>> I have been working with the squid server and icap and I have been
>> running into problems with content cached from google and wikipedia.
>> Some sites using https, such as Centos.org work perfectly with ssl
>> bumping and I get the decrypted content as html and it's readable.
>> Other sites, such as google and wikipedia return what looks like
>> encrypted traffic, or perhaps mime encoded data, I am not sure which.
>>
>> Are there cases where squid will default to direct mode and not
>> decrypt the traffic?  I am using the latest squid server 3.5.27.  I
>> really would like to get this working with google and wikipedia.  I
>> reviewed the page source code from the browser viewer and it looks
>> nothing like the data I am getting via the icap server.
>>
>> Any assistance would be greatly appreciated.
>>
>> The config I am using is:
>>
>> #
>> # Recommended minimum configuration:
>> #
>>
>> # Example rule allowing access from your local networks.
>> # Adapt to list your (internal) IP networks from where browsing
>> # should be allowed
>>
>> acl localnet src 127.0.0.1
>> acl localnet src 10.0.0.0/8     # RFC1918 possible internal network
>> acl localnet src 172.16.0.0/12  # RFC1918 possible internal network
>> acl localnet src 192.168.0.0/16 # RFC1918 possible internal network
>> acl localnet src fc00::/7       # RFC 4193 local private network range
>> acl localnet src fe80::/10      # RFC 4291 link-local (directly
>> plugged) machines
>>
>> acl SSL_ports port 443
>> acl Safe_ports port 80          # http
>> acl Safe_ports port 21          # ftp
>> acl Safe_ports port 443         # https
>> acl Safe_ports port 70          # gopher
>> acl Safe_ports port 210         # wais
>> acl Safe_ports port 1025-65535  # unregistered ports
>> acl Safe_ports port 280         # http-mgmt
>> acl Safe_ports port 488         # gss-http
>> acl Safe_ports port 591         # filemaker
>> acl Safe_ports port 777         # multiling http
>> acl CONNECT method CONNECT
>>
>> #
>> # Recommended minimum Access Permission configuration:
>> #
>> # Deny requests to certain unsafe ports
>> http_access deny !Safe_ports
>>
>> # Deny CONNECT to other than secure SSL ports
>> http_access deny CONNECT !SSL_ports
>>
>> # Only allow cachemgr access from localhost
>> http_access allow localhost manager
>> http_access deny manager
>>
>> # We strongly recommend the following be uncommented to protect innocent
>> # web applications running on the proxy server who think the only
>> # one who can access services on "localhost" is a local user
>> #http_access deny to_localhost
>>
>> #
>> # INSERT YOUR OWN RULE(S) HERE TO ALLOW ACCESS FROM YOUR CLIENTS
>> #
>>
>> # Example rule allowing access from your local networks.
>> # Adapt localnet in the ACL section to list your (internal) IP networks
>> # from where browsing should be allowed
>> http_access allow localnet
>> http_access allow localhost
>>
>> # And finally deny all other access to this proxy
>> http_access deny all
>>
>> # Squid normally listens to port 3128
>> #http_port 3128
>>
>> # Uncomment and adjust the following to add a disk cache directory.
>> #cache_dir ufs /usr/local/squid/var/cache/squid 100 16 256
>>
>> # Leave coredumps in the first cache dir
>> coredump_dir /usr/local/squid/var/cache/squid
>>
>> #
>> # Add any of your own refresh_pattern entries above these.
>> #
>> refresh_pattern ^ftp:           1440    20%     10080
>> refresh_pattern ^gopher:        1440    0%      1440
>> refresh_pattern -i (/cgi-bin/|\?) 0     0%      0
>> refresh_pattern .               0       20%     4320
>>
>> http_port 3128 ssl-bump generate-host-certificates=on
>> dynamic_cert_mem_cache_size=4MB cert=/etc/squid/ssl_cert/myCA.pem
>> http_port 3129
>>
>> # SSL Bump Config
>> always_direct allow all
>> ssl_bump server-first all
>> sslproxy_cert_error deny all
>> sslproxy_flags DONT_VERIFY_PEER
>> sslcrtd_program /usr/local/squid/libexec/ssl_crtd -s /var/lib/ssl_db
>> -M 4MB sslcrtd_children 8 startup=1 idle=1
>>
>> # For squid 3.5.x
>> #sslcrtd_program /usr/local/squid/libexec/ssl_crtd -s /var/lib/ssl_db -M
>> 4MB
>>
>> # For squid 4.x
>> # sslcrtd_program /usr/local/squid/libexec/security_file_certgen -s
>> /var/lib/ssl_db -M 4MB
>>
>> icap_enable on
>> icap_send_client_ip on
>> icap_send_client_username on
>> icap_client_username_header X-Authenticated-User
>> icap_preview_enable on
>> icap_preview_size 1024
>> icap_service service_avi_req reqmod_precache
>> icap://127.0.0.1:1344/request
>> bypass=1
>> adaptation_access service_avi_req allow all
>>
>> icap_service service_avi_resp respmod_precache
>> icap://127.0.0.1:1344/cherokee bypass=0
>> adaptation_access service_avi_resp allow all
>>
>> Jeff
>> _______________________________________________
>> squid-users mailing list
>> squid-users at lists.squid-cache.org
>> http://lists.squid-cache.org/listinfo/squid-users
>>
>>
>
> Eliezer,
>
> Well, you certainly hit the nail on the head.  I added the following
> code to check the content being sent to the icap server from squid,
> and here is what I found when I check the headers being sent from the
> remote web server:
>
> Code to check for content type and encoding received by the icap
> server added to c-icap:
>
>     hdrs = ci_http_response_headers(req);
>     content_type = ci_headers_value(hdrs, "Content-Type");
>     if (content_type)
>        ci_debug_printf(1,"srv_cherokee:  content-type: %s\n",
>                        content_type);
>
>     content_encoding = ci_headers_value(hdrs, "Content-Encoding");
>     if (content_encoding)
>        ci_debug_printf(1,"srv_cherokee:  content-encoding: %s\n",
>                        content_encoding);
>
> And the output from scanned pages sent over from squid:
>
> srv_cherokee:  init request 0x7f3dbc008eb0
> pool hits:1 allocations: 1
> Allocating from objects pool object 5
> pool hits:1 allocations: 1
> Geting buffer from pool 4096:1
> Requested service: cherokee
> Read preview data if there are and process request
> srv_cherokee:  content-type: text/html; charset=utf-8
> srv_cherokee:  content-encoding: gzip         <-- As you stated, I am
> getting gzipped data
> srv_cherokee:  we expect to read :-1 body data
> Allow 204...
> Preview handler return allow 204 response
> srv_cherokee:  release request 0x7f3dbc008eb0
> Store buffer to long pool 4096:1
> Storing to objects pool object 5
> Log request to access log file /var/log/i-cap_access.log
>
>
> Wikipedia  at https://en.wikipedia.org/wiki/HTTP_compression describes
> the process as:
>
> " ...
>    Compression scheme negotiation[edit]
>    In most cases, excluding the SDCH, the negotiation is done in two
> steps, described in
>    RFC 2616:
>
>    1. The web client advertises which compression schemes it supports
> by including a list
>    of tokens in the HTTP request. For Content-Encoding, the list in a
> field called Accept -
>    Encoding; for Transfer-Encoding, the field is called TE.
>
>    GET /encrypted-area HTTP/1.1
>    Host: www.example.com
>    Accept-Encoding: gzip, deflate
>
>    2. If the server supports one or more compression schemes, the
> outgoing data may be
>    compressed by one or more methods supported by both parties. If
> this is the case, the
>    server will add a Content-Encoding or Transfer-Encoding field in
> the HTTP response with
>    the used schemes, separated by commas.
>
>    HTTP/1.1 200 OK
>    Date: mon, 26 June 2016 22:38:34 GMT
>    Server: Apache/1.3.3.7 (Unix)  (Red-Hat/Linux)
>    Last-Modified: Wed, 08 Jan 2003 23:11:55 GMT
>    Accept-Ranges: bytes
>    Content-Length: 438
>    Connection: close
>    Content-Type: text/html; charset=UTF-8
>    Content-Encoding: gzip
>
>    The web server is by no means obligated to use any compression method ?
> this
>    depends on the internal settings of the web server and also may
> depend on the internal
>    architecture of the website in question.
>
>    In case of SDCH a dictionary negotiation is also required, which
> may involve additional
>    steps, like downloading a proper dictionary from .
> .."
>
>
> So, it looks like it is a feature of the browser.  So, is it possible
> to have squid gunzip the data or configure the browser not to send the
> header  to remove "Accept-Encoding: gzip, deflate" from the request
> sent to the remote server telling it to gzip the data?
>
> Thanks
>
> Jeff
>

I located this online for disabling gzip encoding on the browser side
for firefox:

http://forgetmenotes.blogspot.com/2009/05/how-to-disable-gzip-compression-in.html

Jeff


From rafael.akchurin at diladele.com  Sat Sep 30 22:16:07 2017
From: rafael.akchurin at diladele.com (Rafael Akchurin)
Date: Sat, 30 Sep 2017 22:16:07 +0000
Subject: [squid-users] SSL Bump Failures with Google and Wikipedia
In-Reply-To: <CA+ekxPUSMGNS1tr7nztMM2NaJVQgZyOLK8S+QZKhsoWBaR8tUA@mail.gmail.com>
References: <CA+ekxPVs_Nho43T=jtF+pvzs5jNn9j63mWLxpDJda=VkVhkX6w@mail.gmail.com>
 <01c001d33a31$af7c16f0$0e7444d0$@ngtech.co.il>,
 <CA+ekxPUSMGNS1tr7nztMM2NaJVQgZyOLK8S+QZKhsoWBaR8tUA@mail.gmail.com>
Message-ID: <BBEC09F5-1AB6-481E-92C4-F250C51B01D6@diladele.com>

Hello Jeff,

Do not forget Google and YouTube are now using brotli encoding extensively, not only gzip.

Best regards,
Rafael Akchurin

> Op 30 sep. 2017 om 23:49 heeft Jeffrey Merkey <jeffmerkey at gmail.com> het volgende geschreven:
> 
>> On 9/30/17, Eliezer Croitoru <eliezer at ngtech.co.il> wrote:
>> Hey Jeffrey,
>> 
>> What happens when you disable the next icap service this way:
>> icap_service service_avi_resp respmod_precache
>> icap://127.0.0.1:1344/cherokee bypass=0
>> adaptation_access service_avi_resp deny all
>> 
>> Is it still the same?
>> What I suspect is that the requests are defined to accept gzip compressed
>> objects and the icap service is not "gnuzip" them which results in what you
>> see.
>> 
>> To make sure that squid is not at fault here try to disable both icap
>> services and then add then one at a time and see which of this triangle is
>> giving you trouble.
>> I enhanced an ICAP library which is written in GoLang at:
>> https://github.com/elico/icap
>> 
>> And I have couple examples on how to work with http requests and responses
>> at:
>> https://github.com/andybalholm/redwood/
>> https://github.com/andybalholm/redwood/search?utf8=%E2%9C%93&q=gzip&type=
>> 
>> Let me know if you need help finding out the issue.
>> 
>> All The Bests,
>> Eliezer
>> 
>> ----
>> Eliezer Croitoru
>> Linux System Administrator
>> Mobile: +972-5-28704261
>> Email: eliezer at ngtech.co.il
>> 
>> 
>> 
>> -----Original Message-----
>> From: squid-users [mailto:squid-users-bounces at lists.squid-cache.org] On
>> Behalf Of Jeffrey Merkey
>> Sent: Saturday, September 30, 2017 23:28
>> To: squid-users <squid-users at lists.squid-cache.org>
>> Subject: [squid-users] SSL Bump Failures with Google and Wikipedia
>> 
>> Hello All,
>> 
>> I have been working with the squid server and icap and I have been
>> running into problems with content cached from google and wikipedia.
>> Some sites using https, such as Centos.org work perfectly with ssl
>> bumping and I get the decrypted content as html and it's readable.
>> Other sites, such as google and wikipedia return what looks like
>> encrypted traffic, or perhaps mime encoded data, I am not sure which.
>> 
>> Are there cases where squid will default to direct mode and not
>> decrypt the traffic?  I am using the latest squid server 3.5.27.  I
>> really would like to get this working with google and wikipedia.  I
>> reviewed the page source code from the browser viewer and it looks
>> nothing like the data I am getting via the icap server.
>> 
>> Any assistance would be greatly appreciated.
>> 
>> The config I am using is:
>> 
>> #
>> # Recommended minimum configuration:
>> #
>> 
>> # Example rule allowing access from your local networks.
>> # Adapt to list your (internal) IP networks from where browsing
>> # should be allowed
>> 
>> acl localnet src 127.0.0.1
>> acl localnet src 10.0.0.0/8     # RFC1918 possible internal network
>> acl localnet src 172.16.0.0/12  # RFC1918 possible internal network
>> acl localnet src 192.168.0.0/16 # RFC1918 possible internal network
>> acl localnet src fc00::/7       # RFC 4193 local private network range
>> acl localnet src fe80::/10      # RFC 4291 link-local (directly
>> plugged) machines
>> 
>> acl SSL_ports port 443
>> acl Safe_ports port 80          # http
>> acl Safe_ports port 21          # ftp
>> acl Safe_ports port 443         # https
>> acl Safe_ports port 70          # gopher
>> acl Safe_ports port 210         # wais
>> acl Safe_ports port 1025-65535  # unregistered ports
>> acl Safe_ports port 280         # http-mgmt
>> acl Safe_ports port 488         # gss-http
>> acl Safe_ports port 591         # filemaker
>> acl Safe_ports port 777         # multiling http
>> acl CONNECT method CONNECT
>> 
>> #
>> # Recommended minimum Access Permission configuration:
>> #
>> # Deny requests to certain unsafe ports
>> http_access deny !Safe_ports
>> 
>> # Deny CONNECT to other than secure SSL ports
>> http_access deny CONNECT !SSL_ports
>> 
>> # Only allow cachemgr access from localhost
>> http_access allow localhost manager
>> http_access deny manager
>> 
>> # We strongly recommend the following be uncommented to protect innocent
>> # web applications running on the proxy server who think the only
>> # one who can access services on "localhost" is a local user
>> #http_access deny to_localhost
>> 
>> #
>> # INSERT YOUR OWN RULE(S) HERE TO ALLOW ACCESS FROM YOUR CLIENTS
>> #
>> 
>> # Example rule allowing access from your local networks.
>> # Adapt localnet in the ACL section to list your (internal) IP networks
>> # from where browsing should be allowed
>> http_access allow localnet
>> http_access allow localhost
>> 
>> # And finally deny all other access to this proxy
>> http_access deny all
>> 
>> # Squid normally listens to port 3128
>> #http_port 3128
>> 
>> # Uncomment and adjust the following to add a disk cache directory.
>> #cache_dir ufs /usr/local/squid/var/cache/squid 100 16 256
>> 
>> # Leave coredumps in the first cache dir
>> coredump_dir /usr/local/squid/var/cache/squid
>> 
>> #
>> # Add any of your own refresh_pattern entries above these.
>> #
>> refresh_pattern ^ftp:           1440    20%     10080
>> refresh_pattern ^gopher:        1440    0%      1440
>> refresh_pattern -i (/cgi-bin/|\?) 0     0%      0
>> refresh_pattern .               0       20%     4320
>> 
>> http_port 3128 ssl-bump generate-host-certificates=on
>> dynamic_cert_mem_cache_size=4MB cert=/etc/squid/ssl_cert/myCA.pem
>> http_port 3129
>> 
>> # SSL Bump Config
>> always_direct allow all
>> ssl_bump server-first all
>> sslproxy_cert_error deny all
>> sslproxy_flags DONT_VERIFY_PEER
>> sslcrtd_program /usr/local/squid/libexec/ssl_crtd -s /var/lib/ssl_db
>> -M 4MB sslcrtd_children 8 startup=1 idle=1
>> 
>> # For squid 3.5.x
>> #sslcrtd_program /usr/local/squid/libexec/ssl_crtd -s /var/lib/ssl_db -M
>> 4MB
>> 
>> # For squid 4.x
>> # sslcrtd_program /usr/local/squid/libexec/security_file_certgen -s
>> /var/lib/ssl_db -M 4MB
>> 
>> icap_enable on
>> icap_send_client_ip on
>> icap_send_client_username on
>> icap_client_username_header X-Authenticated-User
>> icap_preview_enable on
>> icap_preview_size 1024
>> icap_service service_avi_req reqmod_precache icap://127.0.0.1:1344/request
>> bypass=1
>> adaptation_access service_avi_req allow all
>> 
>> icap_service service_avi_resp respmod_precache
>> icap://127.0.0.1:1344/cherokee bypass=0
>> adaptation_access service_avi_resp allow all
>> 
>> Jeff
>> _______________________________________________
>> squid-users mailing list
>> squid-users at lists.squid-cache.org
>> http://lists.squid-cache.org/listinfo/squid-users
>> 
>> 
> 
> Eliezer,
> 
> Well, you certainly hit the nail on the head.  I added the following
> code to check the content being sent to the icap server from squid,
> and here is what I found when I check the headers being sent from the
> remote web server:
> 
> Code to check for content type and encoding received by the icap
> server added to c-icap:
> 
>    hdrs = ci_http_response_headers(req);
>    content_type = ci_headers_value(hdrs, "Content-Type");
>    if (content_type)
>       ci_debug_printf(1,"srv_cherokee:  content-type: %s\n",
>                       content_type);
> 
>    content_encoding = ci_headers_value(hdrs, "Content-Encoding");
>    if (content_encoding)
>       ci_debug_printf(1,"srv_cherokee:  content-encoding: %s\n",
>                       content_encoding);
> 
> And the output from scanned pages sent over from squid:
> 
> srv_cherokee:  init request 0x7f3dbc008eb0
> pool hits:1 allocations: 1
> Allocating from objects pool object 5
> pool hits:1 allocations: 1
> Geting buffer from pool 4096:1
> Requested service: cherokee
> Read preview data if there are and process request
> srv_cherokee:  content-type: text/html; charset=utf-8
> srv_cherokee:  content-encoding: gzip         <-- As you stated, I am
> getting gzipped data
> srv_cherokee:  we expect to read :-1 body data
> Allow 204...
> Preview handler return allow 204 response
> srv_cherokee:  release request 0x7f3dbc008eb0
> Store buffer to long pool 4096:1
> Storing to objects pool object 5
> Log request to access log file /var/log/i-cap_access.log
> 
> 
> Wikipedia  at https://en.wikipedia.org/wiki/HTTP_compression describes
> the process as:
> 
> " ...
>   Compression scheme negotiation[edit]
>   In most cases, excluding the SDCH, the negotiation is done in two
> steps, described in
>   RFC 2616:
> 
>   1. The web client advertises which compression schemes it supports
> by including a list
>   of tokens in the HTTP request. For Content-Encoding, the list in a
> field called Accept -
>   Encoding; for Transfer-Encoding, the field is called TE.
> 
>   GET /encrypted-area HTTP/1.1
>   Host: www.example.com
>   Accept-Encoding: gzip, deflate
> 
>   2. If the server supports one or more compression schemes, the
> outgoing data may be
>   compressed by one or more methods supported by both parties. If
> this is the case, the
>   server will add a Content-Encoding or Transfer-Encoding field in
> the HTTP response with
>   the used schemes, separated by commas.
> 
>   HTTP/1.1 200 OK
>   Date: mon, 26 June 2016 22:38:34 GMT
>   Server: Apache/1.3.3.7 (Unix)  (Red-Hat/Linux)
>   Last-Modified: Wed, 08 Jan 2003 23:11:55 GMT
>   Accept-Ranges: bytes
>   Content-Length: 438
>   Connection: close
>   Content-Type: text/html; charset=UTF-8
>   Content-Encoding: gzip
> 
>   The web server is by no means obligated to use any compression method ? this
>   depends on the internal settings of the web server and also may
> depend on the internal
>   architecture of the website in question.
> 
>   In case of SDCH a dictionary negotiation is also required, which
> may involve additional
>   steps, like downloading a proper dictionary from .
> .."
> 
> 
> So, it looks like it is a feature of the browser.  So, is it possible
> to have squid gunzip the data or configure the browser not to send the
> header  to remove "Accept-Encoding: gzip, deflate" from the request
> sent to the remote server telling it to gzip the data?
> 
> Thanks
> 
> Jeff
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users


From jeffmerkey at gmail.com  Sat Sep 30 22:22:09 2017
From: jeffmerkey at gmail.com (Jeffrey Merkey)
Date: Sat, 30 Sep 2017 16:22:09 -0600
Subject: [squid-users] SSL Bump Failures with Google and Wikipedia
In-Reply-To: <BBEC09F5-1AB6-481E-92C4-F250C51B01D6@diladele.com>
References: <CA+ekxPVs_Nho43T=jtF+pvzs5jNn9j63mWLxpDJda=VkVhkX6w@mail.gmail.com>
 <01c001d33a31$af7c16f0$0e7444d0$@ngtech.co.il>
 <CA+ekxPUSMGNS1tr7nztMM2NaJVQgZyOLK8S+QZKhsoWBaR8tUA@mail.gmail.com>
 <BBEC09F5-1AB6-481E-92C4-F250C51B01D6@diladele.com>
Message-ID: <CA+ekxPX0tunSfanoB627eKcn9=MRU6WfunZ5RbQ15-3_c4z4Xw@mail.gmail.com>

On 9/30/17, Rafael Akchurin <rafael.akchurin at diladele.com> wrote:
> Hello Jeff,
>
> Do not forget Google and YouTube are now using brotli encoding extensively,
> not only gzip.
>
> Best regards,
> Rafael Akchurin
>
>> Op 30 sep. 2017 om 23:49 heeft Jeffrey Merkey <jeffmerkey at gmail.com> het
>> volgende geschreven:
>>
>>> On 9/30/17, Eliezer Croitoru <eliezer at ngtech.co.il> wrote:
>>> Hey Jeffrey,
>>>
>>> What happens when you disable the next icap service this way:
>>> icap_service service_avi_resp respmod_precache
>>> icap://127.0.0.1:1344/cherokee bypass=0
>>> adaptation_access service_avi_resp deny all
>>>
>>> Is it still the same?
>>> What I suspect is that the requests are defined to accept gzip
>>> compressed
>>> objects and the icap service is not "gnuzip" them which results in what
>>> you
>>> see.
>>>
>>> To make sure that squid is not at fault here try to disable both icap
>>> services and then add then one at a time and see which of this triangle
>>> is
>>> giving you trouble.
>>> I enhanced an ICAP library which is written in GoLang at:
>>> https://github.com/elico/icap
>>>
>>> And I have couple examples on how to work with http requests and
>>> responses
>>> at:
>>> https://github.com/andybalholm/redwood/
>>> https://github.com/andybalholm/redwood/search?utf8=%E2%9C%93&q=gzip&type=
>>>
>>> Let me know if you need help finding out the issue.
>>>
>>> All The Bests,
>>> Eliezer
>>>
>>> ----
>>> Eliezer Croitoru
>>> Linux System Administrator
>>> Mobile: +972-5-28704261
>>> Email: eliezer at ngtech.co.il
>>>
>>>
>>>
>>> -----Original Message-----
>>> From: squid-users [mailto:squid-users-bounces at lists.squid-cache.org] On
>>> Behalf Of Jeffrey Merkey
>>> Sent: Saturday, September 30, 2017 23:28
>>> To: squid-users <squid-users at lists.squid-cache.org>
>>> Subject: [squid-users] SSL Bump Failures with Google and Wikipedia
>>>
>>> Hello All,
>>>
>>> I have been working with the squid server and icap and I have been
>>> running into problems with content cached from google and wikipedia.
>>> Some sites using https, such as Centos.org work perfectly with ssl
>>> bumping and I get the decrypted content as html and it's readable.
>>> Other sites, such as google and wikipedia return what looks like
>>> encrypted traffic, or perhaps mime encoded data, I am not sure which.
>>>
>>> Are there cases where squid will default to direct mode and not
>>> decrypt the traffic?  I am using the latest squid server 3.5.27.  I
>>> really would like to get this working with google and wikipedia.  I
>>> reviewed the page source code from the browser viewer and it looks
>>> nothing like the data I am getting via the icap server.
>>>
>>> Any assistance would be greatly appreciated.
>>>
>>> The config I am using is:
>>>
>>> #
>>> # Recommended minimum configuration:
>>> #
>>>
>>> # Example rule allowing access from your local networks.
>>> # Adapt to list your (internal) IP networks from where browsing
>>> # should be allowed
>>>
>>> acl localnet src 127.0.0.1
>>> acl localnet src 10.0.0.0/8     # RFC1918 possible internal network
>>> acl localnet src 172.16.0.0/12  # RFC1918 possible internal network
>>> acl localnet src 192.168.0.0/16 # RFC1918 possible internal network
>>> acl localnet src fc00::/7       # RFC 4193 local private network range
>>> acl localnet src fe80::/10      # RFC 4291 link-local (directly
>>> plugged) machines
>>>
>>> acl SSL_ports port 443
>>> acl Safe_ports port 80          # http
>>> acl Safe_ports port 21          # ftp
>>> acl Safe_ports port 443         # https
>>> acl Safe_ports port 70          # gopher
>>> acl Safe_ports port 210         # wais
>>> acl Safe_ports port 1025-65535  # unregistered ports
>>> acl Safe_ports port 280         # http-mgmt
>>> acl Safe_ports port 488         # gss-http
>>> acl Safe_ports port 591         # filemaker
>>> acl Safe_ports port 777         # multiling http
>>> acl CONNECT method CONNECT
>>>
>>> #
>>> # Recommended minimum Access Permission configuration:
>>> #
>>> # Deny requests to certain unsafe ports
>>> http_access deny !Safe_ports
>>>
>>> # Deny CONNECT to other than secure SSL ports
>>> http_access deny CONNECT !SSL_ports
>>>
>>> # Only allow cachemgr access from localhost
>>> http_access allow localhost manager
>>> http_access deny manager
>>>
>>> # We strongly recommend the following be uncommented to protect innocent
>>> # web applications running on the proxy server who think the only
>>> # one who can access services on "localhost" is a local user
>>> #http_access deny to_localhost
>>>
>>> #
>>> # INSERT YOUR OWN RULE(S) HERE TO ALLOW ACCESS FROM YOUR CLIENTS
>>> #
>>>
>>> # Example rule allowing access from your local networks.
>>> # Adapt localnet in the ACL section to list your (internal) IP networks
>>> # from where browsing should be allowed
>>> http_access allow localnet
>>> http_access allow localhost
>>>
>>> # And finally deny all other access to this proxy
>>> http_access deny all
>>>
>>> # Squid normally listens to port 3128
>>> #http_port 3128
>>>
>>> # Uncomment and adjust the following to add a disk cache directory.
>>> #cache_dir ufs /usr/local/squid/var/cache/squid 100 16 256
>>>
>>> # Leave coredumps in the first cache dir
>>> coredump_dir /usr/local/squid/var/cache/squid
>>>
>>> #
>>> # Add any of your own refresh_pattern entries above these.
>>> #
>>> refresh_pattern ^ftp:           1440    20%     10080
>>> refresh_pattern ^gopher:        1440    0%      1440
>>> refresh_pattern -i (/cgi-bin/|\?) 0     0%      0
>>> refresh_pattern .               0       20%     4320
>>>
>>> http_port 3128 ssl-bump generate-host-certificates=on
>>> dynamic_cert_mem_cache_size=4MB cert=/etc/squid/ssl_cert/myCA.pem
>>> http_port 3129
>>>
>>> # SSL Bump Config
>>> always_direct allow all
>>> ssl_bump server-first all
>>> sslproxy_cert_error deny all
>>> sslproxy_flags DONT_VERIFY_PEER
>>> sslcrtd_program /usr/local/squid/libexec/ssl_crtd -s /var/lib/ssl_db
>>> -M 4MB sslcrtd_children 8 startup=1 idle=1
>>>
>>> # For squid 3.5.x
>>> #sslcrtd_program /usr/local/squid/libexec/ssl_crtd -s /var/lib/ssl_db -M
>>> 4MB
>>>
>>> # For squid 4.x
>>> # sslcrtd_program /usr/local/squid/libexec/security_file_certgen -s
>>> /var/lib/ssl_db -M 4MB
>>>
>>> icap_enable on
>>> icap_send_client_ip on
>>> icap_send_client_username on
>>> icap_client_username_header X-Authenticated-User
>>> icap_preview_enable on
>>> icap_preview_size 1024
>>> icap_service service_avi_req reqmod_precache
>>> icap://127.0.0.1:1344/request
>>> bypass=1
>>> adaptation_access service_avi_req allow all
>>>
>>> icap_service service_avi_resp respmod_precache
>>> icap://127.0.0.1:1344/cherokee bypass=0
>>> adaptation_access service_avi_resp allow all
>>>
>>> Jeff
>>> _______________________________________________
>>> squid-users mailing list
>>> squid-users at lists.squid-cache.org
>>> http://lists.squid-cache.org/listinfo/squid-users
>>>
>>>
>>
>> Eliezer,
>>
>> Well, you certainly hit the nail on the head.  I added the following
>> code to check the content being sent to the icap server from squid,
>> and here is what I found when I check the headers being sent from the
>> remote web server:
>>
>> Code to check for content type and encoding received by the icap
>> server added to c-icap:
>>
>>    hdrs = ci_http_response_headers(req);
>>    content_type = ci_headers_value(hdrs, "Content-Type");
>>    if (content_type)
>>       ci_debug_printf(1,"srv_cherokee:  content-type: %s\n",
>>                       content_type);
>>
>>    content_encoding = ci_headers_value(hdrs, "Content-Encoding");
>>    if (content_encoding)
>>       ci_debug_printf(1,"srv_cherokee:  content-encoding: %s\n",
>>                       content_encoding);
>>
>> And the output from scanned pages sent over from squid:
>>
>> srv_cherokee:  init request 0x7f3dbc008eb0
>> pool hits:1 allocations: 1
>> Allocating from objects pool object 5
>> pool hits:1 allocations: 1
>> Geting buffer from pool 4096:1
>> Requested service: cherokee
>> Read preview data if there are and process request
>> srv_cherokee:  content-type: text/html; charset=utf-8
>> srv_cherokee:  content-encoding: gzip         <-- As you stated, I am
>> getting gzipped data
>> srv_cherokee:  we expect to read :-1 body data
>> Allow 204...
>> Preview handler return allow 204 response
>> srv_cherokee:  release request 0x7f3dbc008eb0
>> Store buffer to long pool 4096:1
>> Storing to objects pool object 5
>> Log request to access log file /var/log/i-cap_access.log
>>
>>
>> Wikipedia  at https://en.wikipedia.org/wiki/HTTP_compression describes
>> the process as:
>>
>> " ...
>>   Compression scheme negotiation[edit]
>>   In most cases, excluding the SDCH, the negotiation is done in two
>> steps, described in
>>   RFC 2616:
>>
>>   1. The web client advertises which compression schemes it supports
>> by including a list
>>   of tokens in the HTTP request. For Content-Encoding, the list in a
>> field called Accept -
>>   Encoding; for Transfer-Encoding, the field is called TE.
>>
>>   GET /encrypted-area HTTP/1.1
>>   Host: www.example.com
>>   Accept-Encoding: gzip, deflate
>>
>>   2. If the server supports one or more compression schemes, the
>> outgoing data may be
>>   compressed by one or more methods supported by both parties. If
>> this is the case, the
>>   server will add a Content-Encoding or Transfer-Encoding field in
>> the HTTP response with
>>   the used schemes, separated by commas.
>>
>>   HTTP/1.1 200 OK
>>   Date: mon, 26 June 2016 22:38:34 GMT
>>   Server: Apache/1.3.3.7 (Unix)  (Red-Hat/Linux)
>>   Last-Modified: Wed, 08 Jan 2003 23:11:55 GMT
>>   Accept-Ranges: bytes
>>   Content-Length: 438
>>   Connection: close
>>   Content-Type: text/html; charset=UTF-8
>>   Content-Encoding: gzip
>>
>>   The web server is by no means obligated to use any compression method ?
>> this
>>   depends on the internal settings of the web server and also may
>> depend on the internal
>>   architecture of the website in question.
>>
>>   In case of SDCH a dictionary negotiation is also required, which
>> may involve additional
>>   steps, like downloading a proper dictionary from .
>> .."
>>
>>
>> So, it looks like it is a feature of the browser.  So, is it possible
>> to have squid gunzip the data or configure the browser not to send the
>> header  to remove "Accept-Encoding: gzip, deflate" from the request
>> sent to the remote server telling it to gzip the data?
>>
>> Thanks
>>
>> Jeff
>> _______________________________________________
>> squid-users mailing list
>> squid-users at lists.squid-cache.org
>> http://lists.squid-cache.org/listinfo/squid-users
>


Resetting the browser settings to disable gzip worked.  I can see the
pages now.  The best way to implement this is to intercept the
accept-encoding header and either remove it or modify it from icap
before being sent to the remote server.

Jeff


