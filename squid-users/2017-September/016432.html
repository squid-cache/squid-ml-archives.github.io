<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<HTML>
 <HEAD>
   <TITLE> [squid-users] tuning squid memory (aka avoiding the reaper)
   </TITLE>
   <LINK REL="Index" HREF="index.html" >
   <LINK REL="made" HREF="mailto:squid-users%40lists.squid-cache.org?Subject=Re%3A%20%5Bsquid-users%5D%20tuning%20squid%20memory%20%28aka%20avoiding%20the%20reaper%29&In-Reply-To=%3C6fd4bb1b-8ec5-4883-2c57-8debb3890d9a%40measurement-factory.com%3E">
   <META NAME="robots" CONTENT="index,nofollow">
   <style type="text/css">
       pre {
           white-space: pre-wrap;       /* css-2.1, curent FF, Opera, Safari */
           }
   </style>
   <META http-equiv="Content-Type" content="text/html; charset=us-ascii">
   <LINK REL="Previous"  HREF="016430.html">
   <LINK REL="Next"  HREF="016480.html">
 </HEAD>
 <BODY BGCOLOR="#ffffff">
   <H1>[squid-users] tuning squid memory (aka avoiding the reaper)</H1>
    <B>Alex Rousskov</B> 
    <A HREF="mailto:squid-users%40lists.squid-cache.org?Subject=Re%3A%20%5Bsquid-users%5D%20tuning%20squid%20memory%20%28aka%20avoiding%20the%20reaper%29&In-Reply-To=%3C6fd4bb1b-8ec5-4883-2c57-8debb3890d9a%40measurement-factory.com%3E"
       TITLE="[squid-users] tuning squid memory (aka avoiding the reaper)">rousskov at measurement-factory.com
       </A><BR>
    <I>Tue Sep 26 03:26:21 UTC 2017</I>
    <P><UL>
        <LI>Previous message (by thread): <A HREF="016430.html">[squid-users] tuning squid memory (aka avoiding the reaper)
</A></li>
        <LI>Next message (by thread): <A HREF="016480.html">[squid-users] tuning squid memory (aka avoiding the reaper)
</A></li>
         <LI> <B>Messages sorted by:</B> 
              <a href="date.html#16432">[ date ]</a>
              <a href="thread.html#16432">[ thread ]</a>
              <a href="subject.html#16432">[ subject ]</a>
              <a href="author.html#16432">[ author ]</a>
         </LI>
       </UL>
    <HR>  
<!--beginarticle-->
<PRE>On 09/25/2017 05:23 PM, Aaron Turner wrote:
&gt;<i> So I'm testing squid 3.5.26 on an m3.xlarge w/ 14GB of RAM.  Squid is
</I>&gt;<i> the only &quot;real&quot; service running (sshd and the like).  I'm running 4
</I>&gt;<i> workers, and 2 rock cache.  The workers seem to be growing unbounded
</I>&gt;<i> and given ~30min or so will cause the kernel to start killing off
</I>&gt;<i> processes until memory is freed.  Yes, my clients (32 of them) are
</I>&gt;<i> hitting this at about 250 URL's/min which doesn't seem that crazy, but
</I>&gt;<i> &#175;\_(&#12484;)_/&#175;
</I>&gt;<i> 
</I>&gt;<i> cache_mem 1 GB resulted in workers exceeding 4GB resident.  So I tried
</I>&gt;<i> 500 MB, same problem.  Now I'm down to 250 MB and I'm still seeing
</I>&gt;<i> workers using 3-4GB of RAM after a few minutes and still growing 
</I>
It is not the Squid memory cache that consumes your RAM, apparently.


&gt;<i> the docs indicate I should expect total memory to be roughly 3x cache_mem.
</I>
... which is an absurd formula for those using disk caches: Roughly
speaking, most large busy Squids spend most of their RAM on

* memory cache,
* disk cache indexes,
* SSL-related caches, and
* in-flight transactions.

Only one of those 4 components is proportional to cache_mem, with a
coefficient closer to 1 than to 3.


&gt;<i> mgr:info reports:
</I>
Thank you for posting this useful info. When you are using disk caching,
please also include the mgr:storedir report.


&gt;<i> I'm trying to figure out why and how to fix.
</I>
I recommend disabling all caching (memory and disk) and SslBump (if any)
to establish a baseline first. If everything looks stable and peachy for
a few hours, record/store the baseline measurements, and add one new
memory consumer (e.g., the memory cache). Ideally, this testing should
be done in a lab rather than on real users, but YMMV.


&gt;<i> One thing I've read about the cache_mem knob is:
</I>&gt;<i> 
</I>&gt;<i> &quot;If circumstances require, this limit will be exceeded.
</I>&gt;<i> 
</I>&gt;<i> Specifically, if your incoming request rate requires more than
</I>&gt;<i> 'cache_mem' of memory to hold in-transit objects, Squid will
</I>&gt;<i> exceed this limit to satisfy the new requests.  When the load
</I>&gt;<i> decreases, blocks will be freed until the high-water mark is
</I>&gt;<i> reached.  Thereafter, blocks will be used to store hot
</I>&gt;<i> objects.&quot;
</I>
The above is more-or-less accurate, but please note that in-transit
objects do not usually eat memory cache RAM in SMP mode. It is usually
best to think of in-flight transactions as a distinct SMP memory
consumer IMO.


&gt;<i> Not sure if this is the cause of my problem?
</I>
It could be -- it is difficult for me to say by looking at one random
mgr:info snapshot. If I have to guess based on that snapshot alone, then
my answer would be &quot;no&quot; because you have less than 4K concurrent
transactions and transaction response times are low. Hopefully somebody
else on the list can tell you more.



&gt;<i> The FAQ says try a different malloc, so tried recompiling with
</I>&gt;<i> --enable-dlmalloc, but that had no impact.
</I>
Do not bother unless your deployment environment is very unusual. This
hint was helpful 20 years ago, but is rarely relevant these days AFAIK.
See above for a different attack plan.


HTH,

Alex.

</PRE>

<!--endarticle-->
    <HR>
    <P><UL>
        <!--threads-->
	<LI>Previous message (by thread): <A HREF="016430.html">[squid-users] tuning squid memory (aka avoiding the reaper)
</A></li>
	<LI>Next message (by thread): <A HREF="016480.html">[squid-users] tuning squid memory (aka avoiding the reaper)
</A></li>
         <LI> <B>Messages sorted by:</B> 
              <a href="date.html#16432">[ date ]</a>
              <a href="thread.html#16432">[ thread ]</a>
              <a href="subject.html#16432">[ subject ]</a>
              <a href="author.html#16432">[ author ]</a>
         </LI>
       </UL>

<hr>
<a href="https://lists.squid-cache.org/listinfo/squid-users">More information about the squid-users
mailing list</a><br>
</body></html>
