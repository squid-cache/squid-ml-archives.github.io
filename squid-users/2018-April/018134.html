<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<HTML>
 <HEAD>
   <TITLE> [squid-users] Squid returns 400 to GET / HTTP/1.1 with Host Header
   </TITLE>
   <LINK REL="Index" HREF="index.html" >
   <LINK REL="made" HREF="mailto:squid-users%40lists.squid-cache.org?Subject=Re%3A%20%5Bsquid-users%5D%20Squid%20returns%20400%20to%20GET%20/%20HTTP/1.1%20with%20Host%0A%20Header&In-Reply-To=%3C944d6f58-23c8-416c-6860-05936c9a7929%40treenet.co.nz%3E">
   <META NAME="robots" CONTENT="index,nofollow">
   <style type="text/css">
       pre {
           white-space: pre-wrap;       /* css-2.1, curent FF, Opera, Safari */
           }
   </style>
   <META http-equiv="Content-Type" content="text/html; charset=us-ascii">
   <LINK REL="Previous"  HREF="018133.html">
   <LINK REL="Next"  HREF="018135.html">
 </HEAD>
 <BODY BGCOLOR="#ffffff">
   <H1>[squid-users] Squid returns 400 to GET / HTTP/1.1 with Host Header</H1>
    <B>Amos Jeffries</B> 
    <A HREF="mailto:squid-users%40lists.squid-cache.org?Subject=Re%3A%20%5Bsquid-users%5D%20Squid%20returns%20400%20to%20GET%20/%20HTTP/1.1%20with%20Host%0A%20Header&In-Reply-To=%3C944d6f58-23c8-416c-6860-05936c9a7929%40treenet.co.nz%3E"
       TITLE="[squid-users] Squid returns 400 to GET / HTTP/1.1 with Host Header">squid3 at treenet.co.nz
       </A><BR>
    <I>Mon Apr 23 15:31:44 UTC 2018</I>
    <P><UL>
        <LI>Previous message (by thread): <A HREF="018133.html">[squid-users] Squid returns 400 to GET / HTTP/1.1 with Host Header
</A></li>
        <LI>Next message (by thread): <A HREF="018135.html">[squid-users] Squid returns 400 to GET / HTTP/1.1 with Host	Header
</A></li>
         <LI> <B>Messages sorted by:</B> 
              <a href="date.html#18134">[ date ]</a>
              <a href="thread.html#18134">[ thread ]</a>
              <a href="subject.html#18134">[ subject ]</a>
              <a href="author.html#18134">[ author ]</a>
         </LI>
       </UL>
    <HR>  
<!--beginarticle-->
<PRE>On 24/04/18 03:15, Stephen Nelson-Smith wrote:
&gt;<i> Hello,
</I>&gt;<i> 
</I>&gt;<i> I need to demonstrate and test a Squid setup, which should blacklist
</I>&gt;<i> by default, and allow requests only to whitelisted URLs from known
</I>&gt;<i> networks. This is currently running in my staging environment, and is
</I>&gt;<i> working as expected, but I want to test and demo it on demand, with
</I>&gt;<i> nicer feedback than with curl.
</I>&gt;<i> 
</I>&gt;<i> I've deployed Redbot (<A HREF="https://github.com/mnot/redbot">https://github.com/mnot/redbot</A>), which I've set
</I>&gt;<i> up to send all HTTP requests via the Squid proxy
</I>&gt;<i> 
</I>&gt;<i> Using curl -x from the Redbot machine, all my tests pass, but using
</I>&gt;<i> the application, Squid returns a 400 whatever happens. All requests go
</I>&gt;<i> to Squid, and I see every request, but instead of returning a 403 and
</I>&gt;<i> X-Squid-Error: ERR_ACCESS_DENIED 0, or allowing the request, every
</I>&gt;<i> request gets a 400, and X-Squid-Error: ERR_INVALID_URL 0.
</I>&gt;<i> 
</I>
ERR_INVALID_URL --&gt; the URL is the invalid part, not the Host header.

&gt;<i> Digging into it - logs and tcpdump - the key difference I see is that
</I>&gt;<i> Redbot sends a request of the form:
</I>&gt;<i> 
</I>&gt;<i> GET / HTTP/1.1
</I>&gt;<i> Host: chess.com
</I>&gt;<i> 
</I>&gt;<i> Curl sends:
</I>&gt;<i> 
</I>&gt;<i> GET <A HREF="http://chess.com/">http://chess.com/</A> HTTP/1.1
</I>&gt;<i> Host: chess.com
</I>&gt;<i> 
</I>&gt;<i> From the RFC it seems like Redbot's request is perfectly valid, and so
</I>&gt;<i> I feel like Squid should do the right thing and deduce from the host
</I>&gt;<i> header what Redbot wants, and go through its ACLs. However, it just
</I>&gt;<i> errors with:
</I>
You missed the part where it says which type of recipient the various
URL forms are valid.

The redbot example is a origin-form URL - valid only when sent to origin
servers (or reverse-proxy). The curl one is an absolute-form URL - valid
when sent to proxies and gateways.

...
&gt;<i> 
</I>&gt;<i> Does this seem like a Squid config issue? Or do I need to make Redbot
</I>&gt;<i> make a request like Curl does?
</I>
Redbot is designed primarily for debugging HTTP problems with origin
servers to check why their output is not caching in a proxy or browser
properly. If you can find an option to inform it that it is operating
through a proxy, turn that on.

Amos

</PRE>

<!--endarticle-->
    <HR>
    <P><UL>
        <!--threads-->
	<LI>Previous message (by thread): <A HREF="018133.html">[squid-users] Squid returns 400 to GET / HTTP/1.1 with Host Header
</A></li>
	<LI>Next message (by thread): <A HREF="018135.html">[squid-users] Squid returns 400 to GET / HTTP/1.1 with Host	Header
</A></li>
         <LI> <B>Messages sorted by:</B> 
              <a href="date.html#18134">[ date ]</a>
              <a href="thread.html#18134">[ thread ]</a>
              <a href="subject.html#18134">[ subject ]</a>
              <a href="author.html#18134">[ author ]</a>
         </LI>
       </UL>

<hr>
<a href="https://lists.squid-cache.org/listinfo/squid-users">More information about the squid-users
mailing list</a><br>
</body></html>
