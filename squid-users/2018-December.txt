From johnrefwe at mail.com  Sat Dec  1 02:26:32 2018
From: johnrefwe at mail.com (John Refwe)
Date: Sat, 1 Dec 2018 03:26:32 +0100
Subject: [squid-users] Squid SSL-bump error Change Cipher Spec
Message-ID: <trinity-c385be18-fee5-46c6-8510-befbc0b221ba-1543631192182@3c-app-mailcom-lxa02>

An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20181201/77d11500/attachment.htm>

From anon.amish at gmail.com  Sat Dec  1 05:32:33 2018
From: anon.amish at gmail.com (Amish)
Date: Sat, 1 Dec 2018 11:02:33 +0530
Subject: [squid-users] What happens when duplicate external_acl_type are
	mentioned
Message-ID: <a3465f75-7194-5c4d-a04c-fdb4da8f6fd4@gmail.com>

Hello,

PREFACE:
---------

I have a squid.conf file which includes 2 files.

include pre.conf
include main.conf

main.conf will never be changed and contains most of the config and an 
external acl helper with roughly following directives:

external_acl_type ipuser queue-size=40 ttl=120 children-max=1 
children-startup=1 concurrency=20 %>a /usr/lib/squid/ip_to_user
acl proxyuser external ipuser
http_access deny !proxyuser

This helper simply reads IP address and returns username (OK user=xyz) 
or ERR.

Squid blocks the access if no user was found.


SPECIAL CASE HACK:
-------------------

Sometimes we switch to basic proxy authentication via database and do 
not require IP to user conversion.

So I add following lines to pre.conf

auth_param basic program /usr/lib/squid/basic_db_auth <arguments>
external_acl_type ipuser queue-size=40 ttl=120 children-max=1 
children-startup=1 concurrency=20 %>a %ul /usr/lib/squid/ip_to_user

%ul - forces squid to ask for authentication and same is passed to 
ip_to_user.

ip_to_user detects that user is already known and hence returns same 
user back without processing IP address.

This works as expected but I have a question / doubt.


QUESTION:
----------

Effectively squid.conf now has two external_acl_type lines with same 
name. (ipuser)

First one has %ul and other one does not.

 From my tests - first one gets the priority and second one is ignored 
by squid.

So my questions are:
1) Can I assume this to be always true?
2) Can there be a case where second gets called instead of first?
3) Can I expect this assumption to remain the same in future too?

Please guide.

Thank you in advance.

Regards,

Amish.


From squid3 at treenet.co.nz  Sat Dec  1 09:59:30 2018
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sat, 1 Dec 2018 22:59:30 +1300
Subject: [squid-users] Squid SSL-bump error Change Cipher Spec
In-Reply-To: <trinity-c385be18-fee5-46c6-8510-befbc0b221ba-1543631192182@3c-app-mailcom-lxa02>
References: <trinity-c385be18-fee5-46c6-8510-befbc0b221ba-1543631192182@3c-app-mailcom-lxa02>
Message-ID: <54f1988f-f5aa-b059-5d09-f6a4985e4df8@treenet.co.nz>


On 1/12/18 3:26 pm, John Refwe wrote:
> Hi,
> ?
> I have an error when going to a site that is set to be ssl-bumped in squid.
> ?
> I have modified my squid config so that I have not specified any ciphers
> (I read in another forum post this would be the way to make it closest
> to the standard openssl).
> ?


What are your squid.conf settings now?

Amos


From squid3 at treenet.co.nz  Sat Dec  1 10:11:00 2018
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sat, 1 Dec 2018 23:11:00 +1300
Subject: [squid-users] What happens when duplicate external_acl_type are
 mentioned
In-Reply-To: <a3465f75-7194-5c4d-a04c-fdb4da8f6fd4@gmail.com>
References: <a3465f75-7194-5c4d-a04c-fdb4da8f6fd4@gmail.com>
Message-ID: <dd82e71d-2ab2-9328-608b-61d9021ece06@treenet.co.nz>

On 1/12/18 6:32 pm, Amish wrote:
> QUESTION:
> ----------
> 
> Effectively squid.conf now has two external_acl_type lines with same
> name. (ipuser)
> 
> First one has %ul and other one does not.
> 
> From my tests - first one gets the priority and second one is ignored by
> squid.
> 
> So my questions are:
> 1) Can I assume this to be always true?

Now that you have found the lack of error message on startup one will be
added. It has not been a serious problem, so we are unlikely to make it
more than an ERROR message and explicitly ignore the second (time will
tell tough).

> 2) Can there be a case where second gets called instead of first?

Yes. They are looked up by name when processing the config ACLs on
startup/reconfigure. The one found may change on any restart,
reconfigure, or after changes to any lines referencing their shared name.

> 3) Can I expect this assumption to remain the same in future too?

No. It is undefined behaviour of the config parser, any change to the
ACL parts of that code might change it unexpectedly.

Amos


From squid3 at treenet.co.nz  Sat Dec  1 11:11:53 2018
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sun, 2 Dec 2018 00:11:53 +1300
Subject: [squid-users] Why does Squid4 do socket(AF_NETLINK, SOCK_RAW,
 NETLINK_NETFILTER) = -1 EACCES (Permission denied) ?
In-Reply-To: <366f7648d49d44b0af51e3f2767be764@mbxpsc3.winmail.deshaw.com>
References: <366f7648d49d44b0af51e3f2767be764@mbxpsc3.winmail.deshaw.com>
Message-ID: <c0ce406a-4894-46a9-8bc9-0977457f7225@treenet.co.nz>

On 1/12/18 3:43 am, Ahmad, Sarfaraz wrote:
> I think almost every time squid opens a TCP connection, It also tried to
> open a raw socket of type AF_NETLINK. Syscall pasted below.
> 
...>
> Any thoughts ?
> 


* To receive NAT intercepted connections Squid needs access to the
system NAT table to identify what origin server the client was actually
trying to get to before it was diverted into Squid.

* To send traffic with TPROXY interception Squid must setup the socket
for sending the spoofed IP addresses.

* To perform Netfilter MARK operations (both fetch and set) Squid uses
Netfilter Conntrack APIs.

* To fetch EUI information about connections received or sent after they
are open via POSIX getsockopt() or BSD ioctl() APIs. This is optional
and on by default (eui_lookup to configure)

Any of those may be defined by your system Netfilter libraries in terms
of AF_NETLINK traffic in the background. If they are doing things like
that then the ICMP sockets and (less likely) UDS sockets may also be
affected.

If the behaviour is as repeatable as you say you can use a ALL,9 level
cache .log trace to see what exactly Squid is trying to do at the time
it happens.

Amos


From anon.amish at gmail.com  Sat Dec  1 11:15:03 2018
From: anon.amish at gmail.com (Amish)
Date: Sat, 1 Dec 2018 16:45:03 +0530
Subject: [squid-users] What happens when duplicate external_acl_type are
 mentioned
In-Reply-To: <dd82e71d-2ab2-9328-608b-61d9021ece06@treenet.co.nz>
References: <a3465f75-7194-5c4d-a04c-fdb4da8f6fd4@gmail.com>
 <dd82e71d-2ab2-9328-608b-61d9021ece06@treenet.co.nz>
Message-ID: <7ee1e96e-cc0e-756a-da75-a8db7f53d96e@gmail.com>



On 01/12/18 3:41 pm, Amos Jeffries wrote:
> On 1/12/18 6:32 pm, Amish wrote:
>> QUESTION:
>> ----------
>>
>> Effectively squid.conf now has two external_acl_type lines with same
>> name. (ipuser)
>>
>> First one has %ul and other one does not.
>>
>>  From my tests - first one gets the priority and second one is ignored by
>> squid.
>>
>> So my questions are:
>> 1) Can I assume this to be always true?
> Now that you have found the lack of error message on startup one will be
> added. It has not been a serious problem, so we are unlikely to make it
> more than an ERROR message and explicitly ignore the second (time will
> tell tough).

Thank you for your quick response.

So if I pass %ul to external_acl_type, but dont use any auth_param, 
squid dies with an error.

"Can't use proxy auth because no authentication schemes are fully 
configured"

Is it possible for squid to not to die but instead warn and then just 
pass "-" (dash) for %ul?

Passing "-" is what squid normally does when macro value is unknown.

Or can we have additional macro %uL (capital L)

%ul will die with error (existing behavior)
%uL will pass username if available else pass "-" (dash)

This way my external_acl_type will work with OR without proxy auth

All I need to change my external_acl_type in main.conf is to add %uL to it.

If this proposal is acceptable, I will try to create a PR.

Regards,

Amish


From squid3 at treenet.co.nz  Sat Dec  1 11:54:13 2018
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sun, 2 Dec 2018 00:54:13 +1300
Subject: [squid-users] What happens when duplicate external_acl_type are
 mentioned
In-Reply-To: <7ee1e96e-cc0e-756a-da75-a8db7f53d96e@gmail.com>
References: <a3465f75-7194-5c4d-a04c-fdb4da8f6fd4@gmail.com>
 <dd82e71d-2ab2-9328-608b-61d9021ece06@treenet.co.nz>
 <7ee1e96e-cc0e-756a-da75-a8db7f53d96e@gmail.com>
Message-ID: <4003165f-9514-e342-6a17-0ef656c74907@treenet.co.nz>

On 2/12/18 12:15 am, Amish wrote:
> 
> 
> On 01/12/18 3:41 pm, Amos Jeffries wrote:
>> On 1/12/18 6:32 pm, Amish wrote:
>>> QUESTION:
>>> ----------
>>>
>>> Effectively squid.conf now has two external_acl_type lines with same
>>> name. (ipuser)
>>>
>>> First one has %ul and other one does not.
>>>
>>> ?From my tests - first one gets the priority and second one is
>>> ignored by
>>> squid.
>>>
>>> So my questions are:
>>> 1) Can I assume this to be always true?
>> Now that you have found the lack of error message on startup one will be
>> added. It has not been a serious problem, so we are unlikely to make it
>> more than an ERROR message and explicitly ignore the second (time will
>> tell tough).
> 
> Thank you for your quick response.
> 
> So if I pass %ul to external_acl_type, but dont use any auth_param,
> squid dies with an error.
> 
> "Can't use proxy auth because no authentication schemes are fully
> configured"
> 
> Is it possible for squid to not to die but instead warn and then just
> pass "-" (dash) for %ul?

The %ul code will generate an auth challenge exchange if no username is
available. So the auth system must be setup with parameters to use in
that challenge.

Use %un for when username is optional.

Amos


From anon.amish at gmail.com  Sat Dec  1 14:17:28 2018
From: anon.amish at gmail.com (Amish)
Date: Sat, 1 Dec 2018 19:47:28 +0530
Subject: [squid-users] What happens when duplicate external_acl_type are
 mentioned
In-Reply-To: <4003165f-9514-e342-6a17-0ef656c74907@treenet.co.nz>
References: <a3465f75-7194-5c4d-a04c-fdb4da8f6fd4@gmail.com>
 <dd82e71d-2ab2-9328-608b-61d9021ece06@treenet.co.nz>
 <7ee1e96e-cc0e-756a-da75-a8db7f53d96e@gmail.com>
 <4003165f-9514-e342-6a17-0ef656c74907@treenet.co.nz>
Message-ID: <cee6f40d-2769-f1cf-8837-4c0e5d507e9b@gmail.com>



On 01/12/18 5:24 pm, Amos Jeffries wrote:
> On 2/12/18 12:15 am, Amish wrote:
>>
>> Thank you for your quick response.
>>
>> So if I pass %ul to external_acl_type, but dont use any auth_param,
>> squid dies with an error.
>>
>> "Can't use proxy auth because no authentication schemes are fully
>> configured"
>>
>> Is it possible for squid to not to die but instead warn and then just
>> pass "-" (dash) for %ul?
> The %ul code will generate an auth challenge exchange if no username is
> available. So the auth system must be setup with parameters to use in
> that challenge.
>
> Use %un for when username is optional.
>

With %un I have a problem.

I have referenced to external acl twice in my squid.conf.

Simplified setup:

external_acl_type ipuser queue-size=40 ttl=120 children-max=1 
children-startup=1 concurrency=20 %>a %un /usr/lib/squid/ip_to_user
acl proxyuser external ipuser
http_access allow proxyuser restrictedports
http_access allow proxyuser restrictedsites

where some ports and some sites are allowed only for some users.

so when I try %un (with no auth param set), external acl helper gets 
request two times.

First time with "-" and then again with username that external acl 
helper itself replied with.

Squid sends: 1 127.0.0.1 - -
Helper reply: 1 OK user=local
Squid sends: 2 127.0.0.1 local -

(Dash at end is due to automatic addition of %DATA macro by squid)

1 was triggered by first http_access line and
2 was triggered by second http_access because %un is either %ul or %ue 
(which is now known due to 1)

In my case, it becomes completely unnecessary and an additional processing.

That is why I was thinking of additional macro %uL (capital L)

Regards,

Amish.


From drjoms at gmail.com  Sat Dec  1 21:14:58 2018
From: drjoms at gmail.com (Dmitri Seletski)
Date: Sat, 1 Dec 2018 21:14:58 +0000
Subject: [squid-users] squid disable ipv6 outbound traffic
Message-ID: <dd284fba-b837-3418-75d2-532f3cf7469f@gmail.com>

Hello Dear Squidies,

Situation:

I have,

IPv4 only tunnel for security.

IPv6 enabled ISP.

VM with Squid in it, that works over bridge.(so it has both NAT IPv4 IP 
an IPv6 IP)


Problem:

When i go to some sites,? Squid instead of pulling traffic over tunnel 
provider, does it over IPv6 enabled ISP of mine, which defeats purpose 
of VPN provider.

So i need to know how to kill IPv4, at least outbound traffic from Squid 
to rest of Internetz pages. (and no, preference to IPv4 DNS is not an 
option, as some pages are not available in IPv4, so i'd rather not see 
them at all)

Thanks in advance!

Dmitri



From rousskov at measurement-factory.com  Sun Dec  2 04:03:29 2018
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Sat, 1 Dec 2018 21:03:29 -0700
Subject: [squid-users] What happens when duplicate external_acl_type are
 mentioned
In-Reply-To: <dd82e71d-2ab2-9328-608b-61d9021ece06@treenet.co.nz>
References: <a3465f75-7194-5c4d-a04c-fdb4da8f6fd4@gmail.com>
 <dd82e71d-2ab2-9328-608b-61d9021ece06@treenet.co.nz>
Message-ID: <cb8f898c-85b7-aeb3-3e6f-cd2e90fddef7@measurement-factory.com>

On 12/1/18 3:11 AM, Amos Jeffries wrote:
> On 1/12/18 6:32 pm, Amish wrote:
>
>> Effectively squid.conf now has two external_acl_type lines with same
>> name. (ipuser)
>>
>> First one has %ul and other one does not.
>>
>> From my tests - first one gets the priority and second one is ignored by
>> squid.


>> 1) Can I assume this to be always true?

> Now that you have found the lack of error message on startup one will be
> added.

I agree that adding an ERROR message is the right first step.


> It has not been a serious problem,

We cannot know that. It is quite likely that this has been a serious
problem for somebody that we did not hear from (or do not remember
hearing from). It may even be a serious problem right now for somebody
who does not know it yet!

To reduce long-term headaches, I think we should be strict and deprecate
(and then prohibit) ignoring duplicated external_acl_type declarations.

I do not see any good reasons for ignoring this configuration error
forever. FWIW, the use case discussed in this thread is not a good
reason IMO because Squid configuration in question can and should be
easily generated (probably from a stable template) to correctly
accommodate the needs of the current authentication method.


Cheers,

Alex.


From anon.amish at gmail.com  Sun Dec  2 04:31:28 2018
From: anon.amish at gmail.com (Amish)
Date: Sun, 2 Dec 2018 10:01:28 +0530
Subject: [squid-users] Define and use a variable in squid.conf (Was: What
 happens when duplicate external_acl_type are mentioned)
In-Reply-To: <cb8f898c-85b7-aeb3-3e6f-cd2e90fddef7@measurement-factory.com>
References: <a3465f75-7194-5c4d-a04c-fdb4da8f6fd4@gmail.com>
 <dd82e71d-2ab2-9328-608b-61d9021ece06@treenet.co.nz>
 <cb8f898c-85b7-aeb3-3e6f-cd2e90fddef7@measurement-factory.com>
Message-ID: <1f4df70e-81e6-067c-61fd-60c583ff122e@gmail.com>


On 02/12/18 9:33 am, Alex Rousskov wrote:
>
> To reduce long-term headaches, I think we should be strict and deprecate
> (and then prohibit) ignoring duplicated external_acl_type declarations.
>
> I do not see any good reasons for ignoring this configuration error
> forever. FWIW, the use case discussed in this thread is not a good
> reason IMO because Squid configuration in question can and should be
> easily generated (probably from a stable template) to correctly
> accommodate the needs of the current authentication method.
>

Thank you for your clarification.

Now I am looking for alternate ways I can resolve my issue.

Does squid allow defining a variable and using it as argument or macro?

For example:

File: pre.conf
----------------
define proxyauth 0
#auth_param basic program /usr/lib/squid/basic_db_auth <arguments>


File: main.conf:
------------------
include pre.conf
external_acl_type ipuser queue-size=40 ttl=120 children-max=1 
children-startup=1 concurrency=20 %>a %variable{proxyauth} 
/usr/lib/squid/ip_to_user
# OR alternate form
# external_acl_type ipuser queue-size=40 ttl=120 children-max=1 
children-startup=1 concurrency=20 %>a /usr/lib/squid/ip_to_user 
--proxyauth %variable{proxyauth}


So if I want to use proxyauth I can uncomment auth_param line and change 
proxyauth to 1. ip_to_user will be smart enough to then act accordingly.

The reason I cant change main.conf directly is because its a 
standardized packaged file and gets overwritten every time package is 
updated.

This "define" feature can also have several other use in future.

Thank you,

Amish.


From squid3 at treenet.co.nz  Sun Dec  2 09:42:25 2018
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sun, 2 Dec 2018 22:42:25 +1300
Subject: [squid-users] squid disable ipv6 outbound traffic
In-Reply-To: <dd284fba-b837-3418-75d2-532f3cf7469f@gmail.com>
References: <dd284fba-b837-3418-75d2-532f3cf7469f@gmail.com>
Message-ID: <6dc679d9-1e52-8f75-de59-349d38853139@treenet.co.nz>

On 2/12/18 10:14 am, Dmitri Seletski wrote:
> Hello Dear Squidies,
> 
> Situation:
> 
> I have,
> 
> IPv4 only tunnel for security.
> 
> IPv6 enabled ISP.
> 
> VM with Squid in it, that works over bridge.(so it has both NAT IPv4 IP
> an IPv6 IP)
> 

FYI: Modern Internet connected software is required to prefer IPv6 over
the outdated and deprecated IPv4. Squid will not be the only software
with this behaviour so you need to do this properly (see below) not just
for Squid.

> 
> Problem:
> 
> When i go to some sites,? Squid instead of pulling traffic over tunnel
> provider, does it over IPv6 enabled ISP of mine, which defeats purpose
> of VPN provider.

Is that VPN provider running your traffic through some specialized
security checking software?

If not then Squid is providing *better* security just by existing in the
traffic path. Even for that IPv6 traffic.


> 
> So i need to know how to kill IPv4, at least outbound traffic from Squid
> to rest of Internetz pages. (and no, preference to IPv4 DNS is not an
> option, as some pages are not available in IPv4, so i'd rather not see
> them at all)

It is your OS which decides whether or not the VPN or the IPv6 is used
for any given connection.

So the proper way to do what you are asking is to set your VM's firewall
to only allow access through the VPN for connections made by Squid.
Connections to the IPv6 network should be rejected with an ICMPv6
"Network Unavailable" packet which makes Squid move on to the IPv4 attempts.

Amos


From squid3 at treenet.co.nz  Sun Dec  2 09:50:05 2018
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sun, 2 Dec 2018 22:50:05 +1300
Subject: [squid-users] Define and use a variable in squid.conf (Was:
 What happens when duplicate external_acl_type are mentioned)
In-Reply-To: <1f4df70e-81e6-067c-61fd-60c583ff122e@gmail.com>
References: <a3465f75-7194-5c4d-a04c-fdb4da8f6fd4@gmail.com>
 <dd82e71d-2ab2-9328-608b-61d9021ece06@treenet.co.nz>
 <cb8f898c-85b7-aeb3-3e6f-cd2e90fddef7@measurement-factory.com>
 <1f4df70e-81e6-067c-61fd-60c583ff122e@gmail.com>
Message-ID: <e99536d6-86f9-dd4d-1e3f-6fd67cbcd4b3@treenet.co.nz>

On 2/12/18 5:31 pm, Amish wrote:
> 
> On 02/12/18 9:33 am, Alex Rousskov wrote:
>>
>> To reduce long-term headaches, I think we should be strict and deprecate
>> (and then prohibit) ignoring duplicated external_acl_type declarations.
>>
>> I do not see any good reasons for ignoring this configuration error
>> forever. FWIW, the use case discussed in this thread is not a good
>> reason IMO because Squid configuration in question can and should be
>> easily generated (probably from a stable template) to correctly
>> accommodate the needs of the current authentication method.
>>
> 
> Thank you for your clarification.
> 
> Now I am looking for alternate ways I can resolve my issue.
> 

What is wrong with %un that makes it unusable?

It will contain username when Squid has been told a username and '-'
when none is known.

Amos


From anon.amish at gmail.com  Sun Dec  2 11:41:52 2018
From: anon.amish at gmail.com (Amish)
Date: Sun, 2 Dec 2018 17:11:52 +0530
Subject: [squid-users] Define and use a variable in squid.conf (Was:
 What happens when duplicate external_acl_type are mentioned)
In-Reply-To: <e99536d6-86f9-dd4d-1e3f-6fd67cbcd4b3@treenet.co.nz>
References: <a3465f75-7194-5c4d-a04c-fdb4da8f6fd4@gmail.com>
 <dd82e71d-2ab2-9328-608b-61d9021ece06@treenet.co.nz>
 <cb8f898c-85b7-aeb3-3e6f-cd2e90fddef7@measurement-factory.com>
 <1f4df70e-81e6-067c-61fd-60c583ff122e@gmail.com>
 <e99536d6-86f9-dd4d-1e3f-6fd67cbcd4b3@treenet.co.nz>
Message-ID: <e732bde5-79b2-46fb-7a25-6b95e4156aab@gmail.com>



On 02/12/18 3:20 pm, Amos Jeffries wrote:
> On 2/12/18 5:31 pm, Amish wrote:
>> On 02/12/18 9:33 am, Alex Rousskov wrote:
>>> To reduce long-term headaches, I think we should be strict and deprecate
>>> (and then prohibit) ignoring duplicated external_acl_type declarations.
>>>
>>> I do not see any good reasons for ignoring this configuration error
>>> forever. FWIW, the use case discussed in this thread is not a good
>>> reason IMO because Squid configuration in question can and should be
>>> easily generated (probably from a stable template) to correctly
>>> accommodate the needs of the current authentication method.
>>>
>> Thank you for your clarification.
>>
>> Now I am looking for alternate ways I can resolve my issue.
>>
> What is wrong with %un that makes it unusable?
>
> It will contain username when Squid has been told a username and '-'
> when none is known.

I believe you missed my reply. Here is the archive link to it:

http://lists.squid-cache.org/pipermail/squid-users/2018-December/019759.html

Amish.


From squid3 at treenet.co.nz  Sun Dec  2 13:15:42 2018
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Mon, 3 Dec 2018 02:15:42 +1300
Subject: [squid-users] Define and use a variable in squid.conf (Was:
 What happens when duplicate external_acl_type are mentioned)
In-Reply-To: <e732bde5-79b2-46fb-7a25-6b95e4156aab@gmail.com>
References: <a3465f75-7194-5c4d-a04c-fdb4da8f6fd4@gmail.com>
 <dd82e71d-2ab2-9328-608b-61d9021ece06@treenet.co.nz>
 <cb8f898c-85b7-aeb3-3e6f-cd2e90fddef7@measurement-factory.com>
 <1f4df70e-81e6-067c-61fd-60c583ff122e@gmail.com>
 <e99536d6-86f9-dd4d-1e3f-6fd67cbcd4b3@treenet.co.nz>
 <e732bde5-79b2-46fb-7a25-6b95e4156aab@gmail.com>
Message-ID: <be219af2-b737-32f6-57fb-02f2bf663052@treenet.co.nz>

On 3/12/18 12:41 am, Amish wrote:
> 
> 
> On 02/12/18 3:20 pm, Amos Jeffries wrote:
>> On 2/12/18 5:31 pm, Amish wrote:
>>> On 02/12/18 9:33 am, Alex Rousskov wrote:
>>>> To reduce long-term headaches, I think we should be strict and
>>>> deprecate
>>>> (and then prohibit) ignoring duplicated external_acl_type declarations.
>>>>
>>>> I do not see any good reasons for ignoring this configuration error
>>>> forever. FWIW, the use case discussed in this thread is not a good
>>>> reason IMO because Squid configuration in question can and should be
>>>> easily generated (probably from a stable template) to correctly
>>>> accommodate the needs of the current authentication method.
>>>>
>>> Thank you for your clarification.
>>>
>>> Now I am looking for alternate ways I can resolve my issue.
>>>
>> What is wrong with %un that makes it unusable?
>>
>> It will contain username when Squid has been told a username and '-'
>> when none is known.
> 
> I believe you missed my reply. Here is the archive link to it:
> 
> http://lists.squid-cache.org/pipermail/squid-users/2018-December/019759.html
> 

Ah, yes it has not arrived here for some reason.


There are actually _up to four_ helper checks being done when %ul is
used. Performance optimizations in Squid were/are preventing them being
very visible for Basic auth type and external ACL. But the helper state
is still being checked and if any of the cache TTLs end the check may
fall through to do a full helper query.
 * Each test of the proxyuser ACL involves a check of the external
helper cache.
  - If there was no cached result with that exact pattern a fully query
is sent.
 * Each test of the cache for an external helper using %ul (aka. %LOGIN)
requires a check of the auth_param helper cache (if any).
  - If there was no cached result with that exact pattern OR if the auth
scheme does not cache results, a fully query is sent to the auth_param
helper.

With your config and %ul:

 - (1) the auth_param helper is asked to login the client and provide a
username

 then:
  - (2A) the external ACL helper is asked if "user=X" username is okay
 OR:
  - (2B) the external ACL helper is asked if "-" username is okay

 then:
  - (3) the auth_param helper is asked to login the client and provide a
username

 then:
  - (4A) the external ACL helper is asked if "user=X" username is okay
 OR:
  - (4B) the external ACL helper is asked if "-" username is okay



With your config and %un:

 - (1) the external ACL helper is asked if "-" username is okay,

 then:
  - (2A) the external ACL helper is asked if "user=X" username is okay
  OR:
  - (2B) the external ACL helper is asked if "-" username is okay


For optimal performance (under either setup) you need to restructure
these lines:
  http_access allow proxyuser restrictedports
  http_access allow proxyuser restrictedsites

such that the helper is not being used multiple times:

  http_access deny !proxyuser
  http_access allow restrictedports
  http_access allow restrictedsites

Or,
  acl restrictedPlaces anyof restrictedports restrictedsites
  http_access allow proxyuser restrictedPlaces

Amos


From rousskov at measurement-factory.com  Sun Dec  2 17:44:06 2018
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Sun, 2 Dec 2018 10:44:06 -0700
Subject: [squid-users] Define and use a variable in squid.conf (Was:
 What happens when duplicate external_acl_type are mentioned)
In-Reply-To: <1f4df70e-81e6-067c-61fd-60c583ff122e@gmail.com>
References: <a3465f75-7194-5c4d-a04c-fdb4da8f6fd4@gmail.com>
 <dd82e71d-2ab2-9328-608b-61d9021ece06@treenet.co.nz>
 <cb8f898c-85b7-aeb3-3e6f-cd2e90fddef7@measurement-factory.com>
 <1f4df70e-81e6-067c-61fd-60c583ff122e@gmail.com>
Message-ID: <bc297342-7442-520d-5c8f-a5910c52c922@measurement-factory.com>

On 12/1/18 9:31 PM, Amish wrote:

> Now I am looking for alternate ways I can resolve my issue.

There are probably many ways to do this. There are ready-to-use
templating tools that may be a better solution (more on that further
below). However, I can offer a simple template-free approach that should
work:

   # main.conf
   ...
   include authentication.conf
   ...

and then do either

   cp -p foo.conf authentication.conf

or

   cp -p bar.conf authentication.conf

depending on which authentication mechanism you want to use with a
particular Squid instance.


> Does squid allow defining a variable

No, Squid does not. However, there must be ready-to-use templating tools
that can substitute a "variable" in squid.conf.template to generate
squid.conf without variables.


> and using it as argument or macro?

Squid has native support for simple preprocessor conditionals. Combined
with template variables (as discussed above), that support gives you
another way to accomplish the same outcome. The following sketch uses
${Amish::var} as a custom squid.conf.template variable:

    if ${Amish::ForceFoo} = 1
        ... configuration using approach foo ...
    else
        ... configuration using approach bar ...
    endif


> The reason I cant change main.conf directly is because its a
> standardized packaged file and gets overwritten every time package is
> updated.


That is not a valid argument IMO. You do not need to modify main.conf to
generate a configuration file that is actually used. My earlier
authentication.conf sketch is one example. And even something as simple
as the following sed-based script may work in many cases:

  sed s/AmishForceFoo/true/ main.conf > squid.conf


> This "define" feature can also have several other use in future.

True. I had considered adding it in the past. The primary reasons it has
not been added yet are:

1. squid.conf terrible syntax makes it difficult to introduce new
syntax-related changes/features safely. For variables to be useful, they
should be usable in many contexts while not being expanded in some other
contexts. As we learned the hard way when general quoted strings were
introduced, getting this right is tricky and sometimes nearly impossible.

2. Templates and such offer a ready-to-use variable support that can
avoid the general problems of #1 in specific deployment environments.
See examples above.

3. Nobody has guided the required changes from an RFC to implementation.


HTH,

Alex.


From anon.amish at gmail.com  Mon Dec  3 03:59:27 2018
From: anon.amish at gmail.com (Amish)
Date: Mon, 3 Dec 2018 09:29:27 +0530
Subject: [squid-users] Define and use a variable in squid.conf (Was:
 What happens when duplicate external_acl_type are mentioned)
In-Reply-To: <bc297342-7442-520d-5c8f-a5910c52c922@measurement-factory.com>
References: <a3465f75-7194-5c4d-a04c-fdb4da8f6fd4@gmail.com>
 <dd82e71d-2ab2-9328-608b-61d9021ece06@treenet.co.nz>
 <cb8f898c-85b7-aeb3-3e6f-cd2e90fddef7@measurement-factory.com>
 <1f4df70e-81e6-067c-61fd-60c583ff122e@gmail.com>
 <bc297342-7442-520d-5c8f-a5910c52c922@measurement-factory.com>
Message-ID: <b2512183-7a07-531b-5615-ab81f1a85732@gmail.com>



On 02/12/18 11:14 pm, Alex Rousskov wrote:
> On 12/1/18 9:31 PM, Amish wrote:
>
>> Now I am looking for alternate ways I can resolve my issue.
> There are probably many ways to do this. There are ready-to-use
> templating tools that may be a better solution (more on that further
> below). However, I can offer a simple template-free approach that should
> work:
>
>     # main.conf
>     ...
>     include authentication.conf
>     ...
>
> and then do either
>
>     cp -p foo.conf authentication.conf
>
> or
>
>     cp -p bar.conf authentication.conf
>
> depending on which authentication mechanism you want to use with a
> particular Squid instance.

This is what I decided on eventually.

Slightly different way but more or less on the same lines.

# main.conf
 ?? ...
 ?? include authentication.conf
 ?? ...

# authentication.conf
 ?? #auth_param ...
 ?? #external_acl_type ... %ul
 ?? external_acl_type ...

And comment / uncomment whichever method I want to use.

>
>
>> Does squid allow defining a variable
> No, Squid does not. However, there must be ready-to-use templating tools
> that can substitute a "variable" in squid.conf.template to generate
> squid.conf without variables.
>
>
>> and using it as argument or macro?
> Squid has native support for simple preprocessor conditionals. Combined
> with template variables (as discussed above), that support gives you
> another way to accomplish the same outcome. The following sketch uses
> ${Amish::var} as a custom squid.conf.template variable:
>
>      if ${Amish::ForceFoo} = 1
>          ... configuration using approach foo ...
>      else
>          ... configuration using approach bar ...
>      endif
>
>
>> The reason I cant change main.conf directly is because its a
>> standardized packaged file and gets overwritten every time package is
>> updated.
>
> That is not a valid argument IMO. You do not need to modify main.conf to
> generate a configuration file that is actually used. My earlier
> authentication.conf sketch is one example. And even something as simple
> as the following sed-based script may work in many cases:
>
>    sed s/AmishForceFoo/true/ main.conf > squid.conf

Actual squid.conf is:
include pre.conf
include main.conf

So above would not not work. And I do not want to modify main.conf

>
>> This "define" feature can also have several other use in future.
> True. I had considered adding it in the past. The primary reasons it has
> not been added yet are:
>
> 1. squid.conf terrible syntax makes it difficult to introduce new
> syntax-related changes/features safely. For variables to be useful, they
> should be usable in many contexts while not being expanded in some other
> contexts. As we learned the hard way when general quoted strings were
> introduced, getting this right is tricky and sometimes nearly impossible.

May be atleast 'if' statement can have define variable support. Because 
it already has similar support for ${process_number} etc.

So template variable substitution above is not required.

pre.conf
-----------
define proxyauth 1


main.conf
-------------
if ${proxyauth} = 1
 ?? # external acl with %ul
else
 ?? # external acl without %ul
endif


This will also allow squid packagers to allow administrators to activate 
/ deactive a feature just by simple define variable.

Sample squid.conf

# on top of squid.conf
define transparent 1

# somewhere inside
if ${transparent} = 1
 ?? http_port ... intercept ...
else
 ?? http_port ...
endif

This way newbie administrator does not need to know fine details about 
http_port.

> 2. Templates and such offer a ready-to-use variable support that can
> avoid the general problems of #1 in specific deployment environments.
> See examples above.
>
> 3. Nobody has guided the required changes from an RFC to implementation.
>
>

Thank you for spending your time for a detailed reply.

Regards

Amish


From rousskov at measurement-factory.com  Mon Dec  3 04:21:15 2018
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Sun, 2 Dec 2018 21:21:15 -0700
Subject: [squid-users] Define and use a variable in squid.conf (Was:
 What happens when duplicate external_acl_type are mentioned)
In-Reply-To: <b2512183-7a07-531b-5615-ab81f1a85732@gmail.com>
References: <a3465f75-7194-5c4d-a04c-fdb4da8f6fd4@gmail.com>
 <dd82e71d-2ab2-9328-608b-61d9021ece06@treenet.co.nz>
 <cb8f898c-85b7-aeb3-3e6f-cd2e90fddef7@measurement-factory.com>
 <1f4df70e-81e6-067c-61fd-60c583ff122e@gmail.com>
 <bc297342-7442-520d-5c8f-a5910c52c922@measurement-factory.com>
 <b2512183-7a07-531b-5615-ab81f1a85732@gmail.com>
Message-ID: <747fe24f-2049-999f-16dd-b4d28531dd5f@measurement-factory.com>

On 12/2/18 8:59 PM, Amish wrote:
> On 02/12/18 11:14 pm, Alex Rousskov wrote:
>> You do not need to modify main.conf to
>> generate a configuration file that is actually used. My earlier
>> authentication.conf sketch is one example. And even something as simple
>> as the following sed-based script may work in many cases:
>>
>> ?? sed s/AmishForceFoo/true/ main.conf > squid.conf

> Actual squid.conf is:
> include pre.conf
> include main.conf

... but it does not have to be. In the sed-based approach, there is no
pre.conf at all, and squid.conf is generated from main.conf.


> And I do not want to modify main.conf

The variable/sed-based approach requires a single main.conf modification
(once). Just like the authentication.conf approach requires a single
main.conf modification (that you have already performed).


> May be atleast 'if' statement can have define variable support.

The "I will just add this little thing in this little corner" is how we
ended up with terrible syntax in the first place...


> Because it already has similar support for ${process_number} etc.

Yes, and preprocessor macros are supported throughout the configuration
file, not just in if statements. However, they are preprocessor macros,
not configuration variables. We can add support for custom preprocessor
macros (in addition to the existing built-in ones), but, again, doing so
correctly requires some serious work. Meanwhile, template substitutions
are available as a local ready-to-use alternative.

Alex.


From anon.amish at gmail.com  Mon Dec  3 04:56:00 2018
From: anon.amish at gmail.com (Amish)
Date: Mon, 3 Dec 2018 10:26:00 +0530
Subject: [squid-users] Define and use a variable in squid.conf (Was:
 What happens when duplicate external_acl_type are mentioned)
In-Reply-To: <747fe24f-2049-999f-16dd-b4d28531dd5f@measurement-factory.com>
References: <a3465f75-7194-5c4d-a04c-fdb4da8f6fd4@gmail.com>
 <dd82e71d-2ab2-9328-608b-61d9021ece06@treenet.co.nz>
 <cb8f898c-85b7-aeb3-3e6f-cd2e90fddef7@measurement-factory.com>
 <1f4df70e-81e6-067c-61fd-60c583ff122e@gmail.com>
 <bc297342-7442-520d-5c8f-a5910c52c922@measurement-factory.com>
 <b2512183-7a07-531b-5615-ab81f1a85732@gmail.com>
 <747fe24f-2049-999f-16dd-b4d28531dd5f@measurement-factory.com>
Message-ID: <837ef57d-92a8-69f3-5663-0661f2ed9bb5@gmail.com>



On 03/12/18 9:51 am, Alex Rousskov wrote:
>
>> May be atleast 'if' statement can have define variable support.
> The "I will just add this little thing in this little corner" is how we
> ended up with terrible syntax in the first place...

Current syntax isn't outright bad. Its decent enough.

Only reason its bit terrible is because backward compatibility is given 
a priority and attempt is made to NOT to break old squid.conf.

So adding a new or better config and better defaults is bit tough.

May be squid.conf should have versioning processor at top.

$ cat squid.conf
cfgversion 4.0
# new syntax follows

If no version is mentioned existing syntax is assumed.

This way we do not have to worry much about backward compatibility.

Ofcourse this is a different subject altogether so I will end here on this.

>> Because it already has similar support for ${process_number} etc.
> Yes, and preprocessor macros are supported throughout the configuration
> file, not just in if statements. However, they are preprocessor macros,
> not configuration variables. We can add support for custom preprocessor
> macros (in addition to the existing built-in ones), but, again, doing so
> correctly requires some serious work. Meanwhile, template substitutions
> are available as a local ready-to-use alternative.

Thanks a lot again for your inputs.

Regards

Amish.


From pacolopezvelasco at gmail.com  Mon Dec  3 15:42:57 2018
From: pacolopezvelasco at gmail.com (pacolo)
Date: Mon, 3 Dec 2018 09:42:57 -0600 (CST)
Subject: [squid-users] Performance issue /cache_dir / cache_mem / SMP
	workers
In-Reply-To: <ef100110-3528-b0dd-7a49-8efce15a979b@treenet.co.nz>
References: <1541691984135-0.post@n4.nabble.com>
 <b2449f05-cb00-d598-f752-dcabddb11c82@measurement-factory.com>
 <1542891168393-0.post@n4.nabble.com> <1542982882368-0.post@n4.nabble.com>
 <ef100110-3528-b0dd-7a49-8efce15a979b@treenet.co.nz>
Message-ID: <1543851777570-0.post@n4.nabble.com>

Hello Amos,


>If you have a mix of "/cache${process_number}" and "/cache1" in your
>config files you may still be mixing SMP-aware and SMP-disabled access
>to the "/cache1" path.

That's exactly what happened to us, as I mentioned, we are new in the
Squid's world. I have several years of experience in Blue Coat proxies, but
these settings were preconfigured in their system.

>By your mention of "backend.conf" I assume you are trying to use
>something based on our example SMP CARP cluster configuration.
>If that is correct please compare what you have to the current example
>config &lt;https://wiki.squid-cache.org/ConfigExamples/SmpCarpCluster&gt;.

>It has had a few changes since initially written, and people
>copy-pasting it into tutorials without linking back to our info have got
>various bugs in their texts. Sometimes because they copied old versions
>that no longer work, or because they made arbitrary changes without
>properly understanding the consequences.

We followed that example, but it was last edited at 2013-03-23 04:15, so it
has a mixture of AUFS and Rock caches, that's why I missunderstood the
/cache${process_number} with the cache modes.

As you say, there's a lot of info into the tutorials released on the web,
but it's difficult to find the information updated with the most updated
version of Squid.

Maybe it will be worth an update of this example, it would save you some
time not having to reply to people that missunderstood that info.


>So the fix is to:
>
>1) stop Squid.
>
>2) make sure it is fully shutdown with no residual instances or
>processes running.
>
>3) make sure the SMP /dev/shm sockets opened by Squid are fully gone.
>Delete manually if necessary.
>
>4) make sure the PID file is fully gone. Delete manually if necessary.
>
>5) erase everything in the /cache1 directory.
>
>5a) optionally: erase any other caches you may have.
>  This will speed up the -z process, but only the cache showing errors
>actually needs to be fully clean to fix this error message.
>
>6) run "squid -z" manually and wait until it completes.
>
>7) start Squid.


The procedure worked perfectly.

But unfortunately, we faced other issues...

kid1| TCP connection to localhost/4003 failed
kid1| Detected DEAD Parent: backend-kid3
kid1| temporary disabling (Service Unavailable) digest from localhost

So, we changed the cache_peer definition from localhost to 127.0.0.1, as
suggested in https://wiki.squid-cache.org/Features/IPv6.
And that was all to make Squid running.

Your help is much appreciated.

Thanks!
Paco.



--
Sent from: http://squid-web-proxy-cache.1019090.n4.nabble.com/Squid-Users-f1019091.html


From rousskov at measurement-factory.com  Mon Dec  3 16:14:18 2018
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Mon, 3 Dec 2018 09:14:18 -0700
Subject: [squid-users] Define and use a variable in squid.conf (Was:
 What happens when duplicate external_acl_type are mentioned)
In-Reply-To: <837ef57d-92a8-69f3-5663-0661f2ed9bb5@gmail.com>
References: <a3465f75-7194-5c4d-a04c-fdb4da8f6fd4@gmail.com>
 <dd82e71d-2ab2-9328-608b-61d9021ece06@treenet.co.nz>
 <cb8f898c-85b7-aeb3-3e6f-cd2e90fddef7@measurement-factory.com>
 <1f4df70e-81e6-067c-61fd-60c583ff122e@gmail.com>
 <bc297342-7442-520d-5c8f-a5910c52c922@measurement-factory.com>
 <b2512183-7a07-531b-5615-ab81f1a85732@gmail.com>
 <747fe24f-2049-999f-16dd-b4d28531dd5f@measurement-factory.com>
 <837ef57d-92a8-69f3-5663-0661f2ed9bb5@gmail.com>
Message-ID: <e82289e3-4eb7-51c1-7ec9-6059c656b8af@measurement-factory.com>

On 12/2/18 9:56 PM, Amish wrote:

> Current syntax isn't outright bad. Its decent enough.

I disagree (and can back my opinion up with fact-based reasoning), but I
see no reason to argue about that right now.


> May be squid.conf should have versioning processor at top.

IIRC, feature flags are considered a better general solution than
versions when it comes to language evolution. FWIW, we used a "feature
flag" approach with configuration_includes_quoted_values.


> Ofcourse this is a different subject altogether so I will end here on this.

Indeed.

Thank you,

Alex.


From pacolopezvelasco at gmail.com  Mon Dec  3 16:23:20 2018
From: pacolopezvelasco at gmail.com (pacolo)
Date: Mon, 3 Dec 2018 10:23:20 -0600 (CST)
Subject: [squid-users] cache manager / webmin in SMP mode
Message-ID: <1543854200033-0.post@n4.nabble.com>

Hello there,

I am coming to you after having solved my first problems with Squid . Thanks
again!

Anyway, I am facing another issue with Cache Manager.

I had configured Webmin with the non-SMP mode, so I deployed SMP but Webmin
doesn't detect the cache dirs configured in squid.conf.

In non-SMP mode, the cache_dir was at /cache.
In SMP-mode, there are 4 Rock cache_dirs at /cache1 /cache2 /cache3 /cache4.

Then I changed the reference in /etc/webmin/squid.conf, from
cache_dir=/cache to cache_dir=/cache1.

But it still doesn't detect it.

I tried to access the cache manager through webmin GUI or squidclient
mgr:info -W password, but it returns access denied.

I have checked several posts but I haven't found relevant info for Webmin to
detect the cache_dir changes in SMP mode with Rock cache_dirs.

Any hint would be much appreciated.

I have attached the config files in case it helps.
backend.conf
<http://squid-web-proxy-cache.1019090.n4.nabble.com/file/t377599/backend.conf>  
frontend.conf
<http://squid-web-proxy-cache.1019090.n4.nabble.com/file/t377599/frontend.conf>  
squid.conf
<http://squid-web-proxy-cache.1019090.n4.nabble.com/file/t377599/squid.conf>  
etc_webmin_squid_config.txt
<http://squid-web-proxy-cache.1019090.n4.nabble.com/file/t377599/etc_webmin_squid_config.txt>  
tail_-n_100_var_log_squid_frontend.log
<http://squid-web-proxy-cache.1019090.n4.nabble.com/file/t377599/tail_-n_100_var_log_squid_frontend.log>  

Best regards!
Paco.




--
Sent from: http://squid-web-proxy-cache.1019090.n4.nabble.com/Squid-Users-f1019091.html


From pacolopezvelasco at gmail.com  Mon Dec  3 16:26:49 2018
From: pacolopezvelasco at gmail.com (pacolo)
Date: Mon, 3 Dec 2018 10:26:49 -0600 (CST)
Subject: [squid-users] cache manager / webmin in SMP mode
In-Reply-To: <1543854200033-0.post@n4.nabble.com>
References: <1543854200033-0.post@n4.nabble.com>
Message-ID: <1543854409029-0.post@n4.nabble.com>

Hi again,

I forgot to mention the software versions.

CentOS Linux release 7.5.1804 (Core)
Squid 3.5.20-12.el7
Webmin 1.900

Regards!
Paco.




--
Sent from: http://squid-web-proxy-cache.1019090.n4.nabble.com/Squid-Users-f1019091.html


From squid3 at treenet.co.nz  Mon Dec  3 21:18:36 2018
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Tue, 4 Dec 2018 10:18:36 +1300
Subject: [squid-users] cache manager / webmin in SMP mode
In-Reply-To: <1543854200033-0.post@n4.nabble.com>
References: <1543854200033-0.post@n4.nabble.com>
Message-ID: <d2404c86-1f36-6418-f11f-d0610f4cbf4a@treenet.co.nz>

On 4/12/18 5:23 am, pacolo wrote:
> Hello there,
> 
> I am coming to you after having solved my first problems with Squid . Thanks
> again!
> 
> Anyway, I am facing another issue with Cache Manager.
> 
> I had configured Webmin with the non-SMP mode, so I deployed SMP but Webmin
> doesn't detect the cache dirs configured in squid.conf.
> 

This sounds like a webmin issue. Webmin is not part of Squid. So you
would probably be best served asking this through the webmin help
channels. Though someone else here using webmin with SMP caches may have
an idea.

Amos


From johnrefwe at mail.com  Mon Dec  3 23:18:04 2018
From: johnrefwe at mail.com (johnr)
Date: Mon, 3 Dec 2018 17:18:04 -0600 (CST)
Subject: [squid-users] Squid SSL-bump error Change Cipher Spec
In-Reply-To: <54f1988f-f5aa-b059-5d09-f6a4985e4df8@treenet.co.nz>
References: <trinity-c385be18-fee5-46c6-8510-befbc0b221ba-1543631192182@3c-app-mailcom-lxa02>
 <54f1988f-f5aa-b059-5d09-f6a4985e4df8@treenet.co.nz>
Message-ID: <1543879084677-0.post@n4.nabble.com>

>What are your squid.conf settings now?

http_port 3128 ssl-bump
tls_outgoing_options NO_TICKET,ALL,No_SSLv3 min-version=1.0



--
Sent from: http://squid-web-proxy-cache.1019090.n4.nabble.com/Squid-Users-f1019091.html


From squid3 at treenet.co.nz  Tue Dec  4 07:25:06 2018
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Tue, 4 Dec 2018 20:25:06 +1300
Subject: [squid-users] Squid SSL-bump error Change Cipher Spec
In-Reply-To: <1543879084677-0.post@n4.nabble.com>
References: <trinity-c385be18-fee5-46c6-8510-befbc0b221ba-1543631192182@3c-app-mailcom-lxa02>
 <54f1988f-f5aa-b059-5d09-f6a4985e4df8@treenet.co.nz>
 <1543879084677-0.post@n4.nabble.com>
Message-ID: <760e6a9a-2f9f-9bee-864c-34e0f93dcd1c@treenet.co.nz>

On 4/12/18 12:18 pm, johnr wrote:
>> What are your squid.conf settings now?
> 
> http_port 3128 ssl-bump

You are missing a CA certificate for the bumping process to use for the
certificates it sends the clients.

Also you do not have any ssl_bump lines here. They are required to tell
Squid which of the TLS/SSL traffic to consider for handling. Without
those lines the ssl-bump on the port does nothing.


> tls_outgoing_options NO_TICKET,ALL,No_SSLv3 min-version=1.0
> 

This should be:
 tls_outgoing_options options=NO_TICKET,ALL,No_SSLv3 min-version=1.0

That use of "ALL" there is a bit obscure. What it actually does is
*enable* all sorts of unsafe security features the library would
normally disable by default. Such as 8-bit hashes and very insecure RSA
keys.

The min-version is only required if the library defaults to actively
rejecting TLS/1.0 or such.

To let the library use its defaults you simply do not configure Squid to
require anything at all (ie remove the tls_outgoing_options directive
entirely).

Amos


From info at schroeffu.ch  Tue Dec  4 16:10:38 2018
From: info at schroeffu.ch (info at schroeffu.ch)
Date: Tue, 04 Dec 2018 16:10:38 +0000
Subject: [squid-users] Squid 4.4 + SSL bump: Squid is crashing completely
 opening https://www.drcleaner.com/de/dr-cleaner/
Message-ID: <e1b32195676be381c29a7a50d1179086@schroeffu.ch>

Hi all,

my Squid 4.4 with SSL bump is crashing while trying to open this website: https://www.drcleaner.com/de/dr-cleaner/ (https://www.drcleaner.com/de/dr-cleaner/)
So, after trying open this site with SSL bump enabled, no Squid process is running anymore. Just. Dead.

What can I do for debug that properly better to report properly an issue?

SSL bump config:

http_port proxy02bs:8080 ssl-bump generate-host-certificates=on dynamic_cert_mem_cache_size=4MB cert=/etc/squid/certs/xx.pem key=/etc/squid/certs/xx.pem
sslcrtd_program /usr/lib/squid/security_file_certgen -s /var/lib/ssl_db -M 4MB
always_direct allow all
acl step1 at_step SslBump1
ssl_bump peek step1
ssl_bump bump all !domains_dont_sslbump
Latest words 'till the dead:

Dec 4 16:47:19 proxy02bs squid[1001]: assertion failed: http.cc:1530: "!Comm::MonitorsRead(serverConnection->fd)"
Dec 4 16:47:19 proxy02bs squid[604]: Squid Parent: squid-1 process 1001 exited due to signal 6 with status 0
Dec 4 16:47:19 proxy02bs squid[604]: Squid Parent: squid-1 process 1001 will not be restarted for 3600 seconds due to repeated, frequent failures
Dec 4 16:47:19 proxy02bs squid[604]: Exiting due to repeated, frequent failures
Dec 4 16:47:19 proxy02bs squid[604]: Removing PID file (/var/run/squid.pid)
Dec 4 16:47:19 proxy02bs systemd[1]: squid.service: Main process exited, code=exited, status=1/FAILURE
Dec 4 16:47:19 proxy02bs systemd[1]: squid.service: Killing process 666 (pinger) with signal SIGKILL.
Dec 4 16:47:19 proxy02bs systemd[1]: squid.service: Killing process 786 (pinger) with signal SIGKILL.
Dec 4 16:47:19 proxy02bs systemd[1]: squid.service: Killing process 855 (pinger) with signal SIGKILL.
Dec 4 16:47:19 proxy02bs systemd[1]: squid.service: Killing process 923 (pinger) with signal SIGKILL.
Dec 4 16:47:19 proxy02bs systemd[1]: squid.service: Killing process 995 (pinger) with signal SIGKILL.
Dec 4 16:47:19 proxy02bs systemd[1]: squid.service: Killing process 1004 (security_file_c) with signal SIGKILL.
Dec 4 16:47:19 proxy02bs systemd[1]: squid.service: Killing process 1007 (ufdbgclient) with signal SIGKILL.
Dec 4 16:47:19 proxy02bs systemd[1]: squid.service: Killing process 1008 (ufdbgclient) with signal SIGKILL.
Dec 4 16:47:19 proxy02bs systemd[1]: squid.service: Killing process 1065 (pinger) with signal SIGKILL.
Dec 4 16:47:19 proxy02bs systemd[1]: squid.service: Failed with result 'exit-code'.
Full syslog: https://pastebin.com/i9itZcZa (https://pastebin.com/i9itZcZa)
Full access.log: https://pastebin.com/Vc0A5sSG (https://pastebin.com/Vc0A5sSG)
Full cache.log: https://pastebin.com/xdi3RHqs (https://pastebin.com/xdi3RHqs)

Thanks for any help in advance
Schroeffu
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20181204/d777125b/attachment.htm>

From eliezer at ngtech.co.il  Tue Dec  4 19:57:26 2018
From: eliezer at ngtech.co.il (eliezer at ngtech.co.il)
Date: Tue, 4 Dec 2018 21:57:26 +0200
Subject: [squid-users] how to go from connect/tunnel in squid4 ->GET
In-Reply-To: <5C00254F.4040109@tlinx.org>
References: <5C00254F.4040109@tlinx.org>
Message-ID: <020401d48c0b$9541bfb0$bfc53f10$@ngtech.co.il>

Hey,

I'm not sure I understand the scenario and the issue.
>From the wiki page you quoted:
- https://wiki.squid-cache.org/ConfigExamples/Intercept/SslBumpExplicit

I understand you are trying to intercept ssl connections but it's not clear if any traffic is being intercepted or not.
If possible provide the:
- OS and distribution
- "squid -v" output
- some of the access.log that might provide more details on if the traffic is passing or not thru the proxy
- if linux then iptables rules
- if possible the whole squid.conf (remove or obscure any private details)

Eliezer

----
Eliezer Croitoru
Linux System Administrator
Mobile: +972-5-28704261
Email: eliezer at ngtech.co.il


-----Original Message-----
From: squid-users <squid-users-bounces at lists.squid-cache.org> On Behalf Of L A Walsh
Sent: Thursday, November 29, 2018 19:44
To: squid-users at squid-cache.org
Subject: [squid-users] how to go from connect/tunnel in squid4 ->GET

I had a version of this working in squid3.x, but it didn't work
for some sites and didn't work well with a newer Opera, but did
ok with an older FF-clone.

I bumped to squid4 a few months ago, but stil haven't gotten to the point
where I can see and cache individual requests and following config examples
@ https://wiki.squid-cache.org/ConfigExamples/Intercept/SslBumpExplicit,
I'm feeling rather clueless as to what I'm missing.

If someone could throw a few hints/clueballs my way I'd really appreciate
knowing what I'm doing wrong.

My port line looks like (it's all 1 line).
http_port ishtar.sc.tlinx.org:8118 ssl-bump generate-host-certificates=on dynamic_cert_mem_cache_size=64MB tls-cert=/etc/squid/ssl_cert/myCA.pem options=SINGLE_DH_USE,SINGLE_ECDH_USE  tls-dh=secp521r1,/etc/squid/ssl_cert/dhparam-4096.pem

myCA.pem contains both private+public sigs.  I generated a separate
dhparam file, but don't know if I was supposed to include the curve
type in the generation command or if it only uses that later.

I pre-generated the cert dir and it seems to be running, but I don't
see any certs appearing in the dir


Looking at squid w/ps, I see:
root     56805     1  0 04:28 ?        00:00:00 /usr/sbin/squid
squid    56807 56805 42 04:28 ?        00:00:03 (squid-1) --kid squid-1
squid    56809 56807  0 04:28 ?        00:00:00 (security_file_certgen) 
-s /var/cache/squid/lib/ssl_db -M 64MB
squid    56810 56807  0 04:28 ?        00:00:00 (security_file_certgen) 
-s /var/cache/squid/lib/ssl_db -M 64MB
squid    56811 56807  0 04:28 ?        00:00:00 (security_file_certgen) 
-s /var/cache/squid/lib/ssl_db -M 64MB
squid    56812 56807  0 04:28 ?        00:00:00 (security_file_certgen) 
-s /var/cache/squid/lib/ssl_db -M 64MB
squid    56813 56807  0 04:28 ?        00:00:00 (security_file_certgen) 
-s /var/cache/squid/lib/ssl_db -M 64MB
squid    56814 56807  0 04:28 ?        00:00:00 (logfile-daemon) 
/var/log/squid/access.log
squid    56815 56807  0 04:28 ?        00:00:00 (pinger)

Any ideas where I might be missing things?  I can decomment and
send the active lines from the config file if that would help.

Thanks for any pointers...





_______________________________________________
squid-users mailing list
squid-users at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-users



From ziprasidone146939277 at gmail.com  Wed Dec  5 00:31:39 2018
From: ziprasidone146939277 at gmail.com (ziprasidone146939277 at gmail.com)
Date: Tue, 4 Dec 2018 21:31:39 -0300
Subject: [squid-users] Squid 4.4 + SSL bump: Squid is crashing
	completely opening https://www.drcleaner.com/de/dr-cleaner/
In-Reply-To: <e1b32195676be381c29a7a50d1179086@schroeffu.ch>
References: <e1b32195676be381c29a7a50d1179086@schroeffu.ch>
Message-ID: <002401d48c31$e4fc1980$aef44c80$@gmail.com>

Hi,

 

Works ?well? on my squid v 4.4 (patched) ? debian 9.

 

Although the site does not load well, squid does not die:

 

(?)

 

TCP_MISS/502 1609 GET https://cache.drcleaner.com/extend/home/js/jquery-2.0.0.min.js - ORIGINAL_DST/99.84.27.102 text/html

TCP_MISS/403 684 GET https://s3-us-west-2.amazonaws.com/trustedsite-public/host/drcleaner.com/client.js - ORIGINAL_DST/52.218.200.72 application/xml

TCP_MISS/502 1609 GET https://cache.drcleaner.com/extend/home/css/index.css - ORIGINAL_DST/99.84.27.102 text/html

TCP_MISS/502 1609 GET https://cache.drcleaner.com/extend/home/css/bootstrap.min.css - ORIGINAL_DST/99.84.27.102 text/html

TCP_MISS/502 1609 GET https://cache.drcleaner.com/extend/home/js/jquery-2.0.0.min.js - ORIGINAL_DST/99.84.27.102 text/html

TCP_MISS/502 1609 GET https://cache.drcleaner.com/extend/home/js/jquery.screw.js - ORIGINAL_DST/99.84.27.102 text/html

TCP_MISS/502 1609 GET https://cache.drcleaner.com/extend/home/js/bg_pro.js - ORIGINAL_DST/99.84.27.102 text/html

TCP_MISS/502 1609 GET https://cache.drcleaner.com/extend/home/js/mobile.js - ORIGINAL_DST/99.84.27.102 text/html

TCP_MISS/502 1609 GET https://cache.drcleaner.com/wp-content/plugins/contact-form-7/includes/js/scripts.js? - ORIGINAL_DST/99.84.27.102 text/html

TCP_MISS/502 1609 GET https://cache.drcleaner.com/wp-includes/js/comment-reply.min.js? - ORIGINAL_DST/99.84.27.102 text/html

 

And over..

 

Please, see https://bugs.squid-cache.org/show_bug.cgi?id=4896

 

If your case is similar, there is a patch as a workaround.

 

HTH

 

 

De: squid-users <squid-users-bounces at lists.squid-cache.org> En nombre de info at schroeffu.ch
Enviado el: martes, 4 de diciembre de 2018 13:11
Para: squid-users at lists.squid-cache.org
Asunto: [squid-users] Squid 4.4 + SSL bump: Squid is crashing completely opening https://www.drcleaner.com/de/dr-cleaner/

 

Hi all,

my Squid 4.4 with SSL bump is crashing while trying to open this website: https://www.drcleaner.com/de/dr-cleaner/
So, after trying open this site with SSL bump enabled, no Squid process is running anymore. Just. Dead.

What can I do for debug that properly better to report properly an issue?

SSL bump config:

http_port proxy02bs:8080 ssl-bump generate-host-certificates=on dynamic_cert_mem_cache_size=4MB cert=/etc/squid/certs/xx.pem key=/etc/squid/certs/xx.pem
sslcrtd_program /usr/lib/squid/security_file_certgen -s /var/lib/ssl_db -M 4MB
always_direct allow all
acl step1 at_step SslBump1
ssl_bump peek step1
ssl_bump bump all !domains_dont_sslbump


Latest words 'till the dead:

Dec 4 16:47:19 proxy02bs squid[1001]: assertion failed: http.cc:1530: "!Comm::MonitorsRead(serverConnection->fd)"
Dec 4 16:47:19 proxy02bs squid[604]: Squid Parent: squid-1 process 1001 exited due to signal 6 with status 0
Dec 4 16:47:19 proxy02bs squid[604]: Squid Parent: squid-1 process 1001 will not be restarted for 3600 seconds due to repeated, frequent failures
Dec 4 16:47:19 proxy02bs squid[604]: Exiting due to repeated, frequent failures
Dec 4 16:47:19 proxy02bs squid[604]: Removing PID file (/var/run/squid.pid)
Dec 4 16:47:19 proxy02bs systemd[1]: squid.service: Main process exited, code=exited, status=1/FAILURE
Dec 4 16:47:19 proxy02bs systemd[1]: squid.service: Killing process 666 (pinger) with signal SIGKILL.
Dec 4 16:47:19 proxy02bs systemd[1]: squid.service: Killing process 786 (pinger) with signal SIGKILL.
Dec 4 16:47:19 proxy02bs systemd[1]: squid.service: Killing process 855 (pinger) with signal SIGKILL.
Dec 4 16:47:19 proxy02bs systemd[1]: squid.service: Killing process 923 (pinger) with signal SIGKILL.
Dec 4 16:47:19 proxy02bs systemd[1]: squid.service: Killing process 995 (pinger) with signal SIGKILL.
Dec 4 16:47:19 proxy02bs systemd[1]: squid.service: Killing process 1004 (security_file_c) with signal SIGKILL.
Dec 4 16:47:19 proxy02bs systemd[1]: squid.service: Killing process 1007 (ufdbgclient) with signal SIGKILL.
Dec 4 16:47:19 proxy02bs systemd[1]: squid.service: Killing process 1008 (ufdbgclient) with signal SIGKILL.
Dec 4 16:47:19 proxy02bs systemd[1]: squid.service: Killing process 1065 (pinger) with signal SIGKILL.
Dec 4 16:47:19 proxy02bs systemd[1]: squid.service: Failed with result 'exit-code'.


Full syslog: https://pastebin.com/i9itZcZa
Full access.log: https://pastebin.com/Vc0A5sSG
Full cache.log: https://pastebin.com/xdi3RHqs

Thanks for any help in advance
Schroeffu 

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20181204/0fac5673/attachment.htm>

From a.moehrlein at me.com  Wed Dec  5 07:10:31 2018
From: a.moehrlein at me.com (Andreas Moehrlein)
Date: Wed, 5 Dec 2018 08:10:31 +0100
Subject: [squid-users] squid4.4 - ssl_bump - deny_info / how to present a
 blocked message for unwanted https traffic
Message-ID: <F50D98EA-C701-475D-8CE2-0A0BD9F5DA12@me.com>

I use squid as a proxy to enable some http and https sites and redirect all other traffic to a captive portal.
Everything works fine, except for a redirect/deny_info for the not allowed https traffic.

Is there a mechanism, I can use to show a error message for https ?

All http + https traffic is redirected to squid via iptables.
Deny_info works perfect for http.

/etc/squid/squid.conf

acl step1 at_step SslBump1
acl https_whitelist ssl::server_name "/etc/squid/acl/general.list"

ssl_bump peek step1 all
ssl_bump splice https_whitelist
ssl_bump terminate all

deny_info http://10.10.12.1:81/captureme?redirect=%u all




Thanks in advance,
Andreas

From info at schroeffu.ch  Wed Dec  5 09:26:27 2018
From: info at schroeffu.ch (info at schroeffu.ch)
Date: Wed, 05 Dec 2018 09:26:27 +0000
Subject: [squid-users] Squid 4.4 + SSL bump: Squid is crashing
 completely opening https://www.drcleaner.com/de/dr-cleaner/
In-Reply-To: <002401d48c31$e4fc1980$aef44c80$@gmail.com>
References: <002401d48c31$e4fc1980$aef44c80$@gmail.com>
 <e1b32195676be381c29a7a50d1179086@schroeffu.ch>
Message-ID: <9ae9eb8c7d6b1a883bcdad822ca9fe47@schroeffu.ch>

> Hi,
> 
> Works ?well? on my squid v 4.4 (patched) ? debian 9.
> 
> Although the site does not load well, squid does not die:
> 
> (?)
> 
> TCP_MISS/502 1609 GET https://cache.drcleaner.com/extend/home/js/jquery-2.0.0.min.js -
> ORIGINAL_DST/99.84.27.102 text/html
> 
> TCP_MISS/403 684 GET
> https://s3-us-west-2.amazonaws.com/trustedsite-public/host/drcleaner.com/client.js -
> ORIGINAL_DST/52.218.200.72 application/xml
> 
> TCP_MISS/502 1609 GET https://cache.drcleaner.com/extend/home/css/index.css -
> ORIGINAL_DST/99.84.27.102 text/html
> 
> TCP_MISS/502 1609 GET https://cache.drcleaner.com/extend/home/css/bootstrap.min.css -
> ORIGINAL_DST/99.84.27.102 text/html
> 
> TCP_MISS/502 1609 GET https://cache.drcleaner.com/extend/home/js/jquery-2.0.0.min.js -
> ORIGINAL_DST/99.84.27.102 text/html
> 
> TCP_MISS/502 1609 GET https://cache.drcleaner.com/extend/home/js/jquery.screw.js -
> ORIGINAL_DST/99.84.27.102 text/html
> 
> TCP_MISS/502 1609 GET https://cache.drcleaner.com/extend/home/js/bg_pro.js -
> ORIGINAL_DST/99.84.27.102 text/html
> 
> TCP_MISS/502 1609 GET https://cache.drcleaner.com/extend/home/js/mobile.js -
> ORIGINAL_DST/99.84.27.102 text/html
> 
> TCP_MISS/502 1609 GET
> https://cache.drcleaner.com/wp-content/plugins/contact-form-7/includes/js/scripts.js? -
> ORIGINAL_DST/99.84.27.102 text/html
> 
> TCP_MISS/502 1609 GET https://cache.drcleaner.com/wp-includes/js/comment-reply.min.js? -
> ORIGINAL_DST/99.84.27.102 text/html
> 
> And over..
> 
> Please, see https://bugs.squid-cache.org/show_bug.cgi?id=4896
> 
> If your case is similar, there is a patch as a workaround.
> 
> HTH
> 

Your Squid 4.4 is patched with https://bugs.squid-cache.org/show_bug.cgi?id=4896 > SQUID-385-Comm_MonitorsRead-assertion-t3.patch ?
It seems exactly the issue I experienced.

I did recompile a testenvironment Squid with that patch, now the mentioned site is not killing my Squid anymore with SSL bump enabled. I am going to rollout the patched version this evening for our 20+ testusers on a pre-prod proxy. If there is any further issue, I'll comment the bugreport directly.

thanks 
Schroeffu


From christof.gerber1 at gmail.com  Wed Dec  5 12:03:14 2018
From: christof.gerber1 at gmail.com (Christof Gerber)
Date: Wed, 5 Dec 2018 13:03:14 +0100
Subject: [squid-users] Proxy Chaining with ssl_bump
Message-ID: <CAFyThpJkSLGFb1Y=_0x4vDmHCiHoMAeduEBt7SnC8hX+k=4N8g@mail.gmail.com>

I have a squid 3.5 as forward proxy that does ssl_bump by default.
Some traffic I need to forward in addition to a second proxy by proxy
chaining. The following configuration works for HTTP traffic but not
with HTTPS. I found out through
https://www.spinics.net/lists/squid/msg84767.html that this is because
Squid 3.5 is not capable of doing ssl_bump + proxy chaining because
the first proxy in the chain won't send a CONNECT after ssl_bump was
performed. My question is:

1. Is this finding still up-to-date , saying that Squid 3.5 does not
support ssl_bump + proxy chaining. How is it for Squid 4?

squid.conf snippet doing proxy chaining:

ssl_bump bump group_default
acl forward_group dstdomain .dropbox.com
cache_peer forward.domain.com parent 8080 0 no-query default
cache_peer_access forward.domain.com allow forward_group
never_direct allow forward_group
never_direct deny all


-- 
Christof Gerber
Email: christof.gerber1 at gmail.com


From squid3 at treenet.co.nz  Wed Dec  5 12:42:24 2018
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 6 Dec 2018 01:42:24 +1300
Subject: [squid-users] Proxy Chaining with ssl_bump
In-Reply-To: <CAFyThpJkSLGFb1Y=_0x4vDmHCiHoMAeduEBt7SnC8hX+k=4N8g@mail.gmail.com>
References: <CAFyThpJkSLGFb1Y=_0x4vDmHCiHoMAeduEBt7SnC8hX+k=4N8g@mail.gmail.com>
Message-ID: <1ca23f19-1a60-be29-dca8-24c8bc882a73@treenet.co.nz>

On 6/12/18 1:03 am, Christof Gerber wrote:
> I have a squid 3.5 as forward proxy that does ssl_bump by default.
> Some traffic I need to forward in addition to a second proxy by proxy
> chaining. The following configuration works for HTTP traffic but not
> with HTTPS. I found out through
> https://www.spinics.net/lists/squid/msg84767.html that this is because
> Squid 3.5 is not capable of doing ssl_bump + proxy chaining because
> the first proxy in the chain won't send a CONNECT after ssl_bump was
> performed. My question is:
> 
> 1. Is this finding still up-to-date , saying that Squid 3.5 does not
> support ssl_bump + proxy chaining. How is it for Squid 4?

The situation is better and constantly being improved. But the official
releases are still not doing CONNECT to upstream peers in the case where
traffic is fully decrypted by the first proxy. Only the cases where
decryption is avoided with splice or on_unsupported_protocol tunnel's.

IIRC Measurement Factory had an experimental git branch to add CONNECT
over non-TLS/SSL peers. I'm not sure what the status is on that now, it
has not been submitted for merge auditing yet.

Amos


From squid3 at treenet.co.nz  Wed Dec  5 13:29:36 2018
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 6 Dec 2018 02:29:36 +1300
Subject: [squid-users] squid4.4 - ssl_bump - deny_info / how to present
 a blocked message for unwanted https traffic
In-Reply-To: <F50D98EA-C701-475D-8CE2-0A0BD9F5DA12@me.com>
References: <F50D98EA-C701-475D-8CE2-0A0BD9F5DA12@me.com>
Message-ID: <c358d326-7a35-faf0-5d60-218ee93412d0@treenet.co.nz>

On 5/12/18 8:10 pm, Andreas Moehrlein wrote:
> I use squid as a proxy to enable some http and https sites and redirect all other traffic to a captive portal.
> Everything works fine, except for a redirect/deny_info for the not allowed https traffic.
> 
> Is there a mechanism, I can use to show a error message for https ?
> 

The TL;DR short answer:

  No, sorry, it is effectively impossible.


The longer version:

Browsers refuse to display any messages provided by proxies in response
to a CONNECT attempt. They replace them with their own in-house
messages. Usually blaming the proxy for 'connection failure', regardless
of what actually happened.

The only way around that is for the proxy to MITM and decrypt the TLS
traffic and forge an HTTP error response to the first HTTP request it
finds in there.

Which in the scenario of a Captive Portal effectively means that it is
impossible until Browsers change their behaviour. The Browser does not
have prior setup to trust the proxy CA in order for the handshake to
complete and error message to be delivered.

If you have any ability to configure the browser for that trust, then by
definition it is not a Captive Portal situation. Use it as a regular TLS
Proxy instead and your error and/or redirection should work _relatively_
okay, most of the time, sort of.



> All http + https traffic is redirected to squid via iptables.
> Deny_info works perfect for http.
> 
> /etc/squid/squid.conf
> 
> acl step1 at_step SslBump1
> acl https_whitelist ssl::server_name "/etc/squid/acl/general.list"
> 
> ssl_bump peek step1 all
> ssl_bump splice https_whitelist
> ssl_bump terminate all
> 
> deny_info http://10.10.12.1:81/captureme?redirect=%u all
> 

Please be aware that deny_info is the parameters on how to generate an
*HTTP* protocol response message when a "deny" action is initiated for
an HTTP transaction. The above line connects your parameters to the ACL
named "all" so denial by HTTP access control lines ending with " all"
(eg "http_access deny all") will send a redirect to that URL.

ssl_bump terminate is different in several important ways:

 * Firstly it is not access control for an HTTP transaction - it
flow-controls the stages of a TLS protocol handshake. Thus no "error
message" exists in the sense you are looking for.

* Secondly it uses a "terminate" action not a "deny". The entire
semantics and binary-layer of what is going on are critically different.
  The "terminate" takes the form of a TLS Alert frame of slightly more
than 4 bytes, usually followed with or embedded in a TCP FIN packet. No
HTTP at all and certainly nothing even resembling redirect possible (so
we cannot even map your intention into the TLS format).


Overall with this setup no there is no chance at all of the proxy error
message even existing let alone being sent to any User.


Amos


From rousskov at measurement-factory.com  Wed Dec  5 15:51:33 2018
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Wed, 5 Dec 2018 08:51:33 -0700
Subject: [squid-users] Proxy Chaining with ssl_bump
In-Reply-To: <CAFyThpJkSLGFb1Y=_0x4vDmHCiHoMAeduEBt7SnC8hX+k=4N8g@mail.gmail.com>
References: <CAFyThpJkSLGFb1Y=_0x4vDmHCiHoMAeduEBt7SnC8hX+k=4N8g@mail.gmail.com>
Message-ID: <6081981a-5eb4-e822-0f55-8d0d275d1e82@measurement-factory.com>

On 12/5/18 5:03 AM, Christof Gerber wrote:
> I found out through
> https://www.spinics.net/lists/squid/msg84767.html that this is because
> Squid 3.5 is not capable of doing ssl_bump + proxy chaining

> 1. Is this finding still up-to-date , saying that Squid 3.5 does not
> support ssl_bump + proxy chaining. How is it for Squid 4?

Forwarding most SslBump-related connections to peers is still
unsupported in official Squids, including Squid v3 and v4. Unofficial
Factory code that implements this feature is available for testing at

https://github.com/measurement-factory/squid/tree/SQUID-360-peering-for-SslBump

Alex.


From eliezer at ngtech.co.il  Thu Dec  6 11:27:41 2018
From: eliezer at ngtech.co.il (eliezer at ngtech.co.il)
Date: Thu, 6 Dec 2018 13:27:41 +0200
Subject: [squid-users] Why does Squid4 do socket(AF_NETLINK, SOCK_RAW,
	NETLINK_NETFILTER) = -1 EACCES (Permission denied) ?
In-Reply-To: <c0ce406a-4894-46a9-8bc9-0977457f7225@treenet.co.nz>
References: <366f7648d49d44b0af51e3f2767be764@mbxpsc3.winmail.deshaw.com>
 <c0ce406a-4894-46a9-8bc9-0977457f7225@treenet.co.nz>
Message-ID: <!&!AAAAAAAAAAAuAAAAAAAAANdDuvHk9MdNkXRSPO6m5z0BAMO2jhD3dRHOtM0AqgC7tuYAAAAAAA4AABAAAAAugdSj9GUcQ4MIva3f2gpwAQAAAAA=@ngtech.co.il>

I have seen this with selinux also.
I can trace the issue down but just to clear out my doubts and before delving into DEBUG all,9:
On a default squid 4.4 with one worker no cache with default squid.conf, should we expect it or maybe it is a side effect in the code?
(Technically speaking if I do not trust Squid in general then I should probably not entrust these netfilter socket to Squid)

Thanks,
Eliezer

----
Eliezer Croitoru
Linux System Administrator
Mobile: +972-5-28704261
Email: eliezer at ngtech.co.il


-----Original Message-----
From: squid-users <squid-users-bounces at lists.squid-cache.org> On Behalf Of Amos Jeffries
Sent: Saturday, December 1, 2018 13:12
To: squid-users at lists.squid-cache.org
Subject: Re: [squid-users] Why does Squid4 do socket(AF_NETLINK, SOCK_RAW, NETLINK_NETFILTER) = -1 EACCES (Permission denied) ?

On 1/12/18 3:43 am, Ahmad, Sarfaraz wrote:
> I think almost every time squid opens a TCP connection, It also tried to
> open a raw socket of type AF_NETLINK. Syscall pasted below.
> 
...>
> Any thoughts ?
> 


* To receive NAT intercepted connections Squid needs access to the
system NAT table to identify what origin server the client was actually
trying to get to before it was diverted into Squid.

* To send traffic with TPROXY interception Squid must setup the socket
for sending the spoofed IP addresses.

* To perform Netfilter MARK operations (both fetch and set) Squid uses
Netfilter Conntrack APIs.

* To fetch EUI information about connections received or sent after they
are open via POSIX getsockopt() or BSD ioctl() APIs. This is optional
and on by default (eui_lookup to configure)

Any of those may be defined by your system Netfilter libraries in terms
of AF_NETLINK traffic in the background. If they are doing things like
that then the ICMP sockets and (less likely) UDS sockets may also be
affected.

If the behaviour is as repeatable as you say you can use a ALL,9 level
cache .log trace to see what exactly Squid is trying to do at the time
it happens.

Amos
_______________________________________________
squid-users mailing list
squid-users at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-users



From eliezer at ngtech.co.il  Sat Dec  8 05:08:19 2018
From: eliezer at ngtech.co.il (eliezer at ngtech.co.il)
Date: Sat, 8 Dec 2018 07:08:19 +0200
Subject: [squid-users] NgTech CentOS 7 repo migrated
Message-ID: <0ae101d48eb4$093f2e20$1bbd8a60$@ngtech.co.il>

I got couple complains in the last month and it seems that my ISP at  the
time capped the traffic with QOS.

After adding the faster and the faster domains I noticed that the limits are
ridicules.

I managed to make sure that instead of 4kbps QOS enforcement a 1Mbps ie
100kbps will be enforced.

It's not the fastest connection ever but it's fast enough to mirror or
download from the service without any major issues.

 

All The Bests,

Eliezer 

 

----

 <http://ngtech.co.il/lmgtfy/> Eliezer Croitoru
Linux System Administrator
Mobile: +972-5-28704261
Email:  <mailto:eliezer at ngtech.co.il> eliezer at ngtech.co.il



 

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20181208/c171c72b/attachment.htm>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: image001.png
Type: image/png
Size: 11295 bytes
Desc: not available
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20181208/c171c72b/attachment.png>

From praveen.apalya at gmail.com  Mon Dec 10 07:56:30 2018
From: praveen.apalya at gmail.com (Praveen Valaboju)
Date: Mon, 10 Dec 2018 13:26:30 +0530
Subject: [squid-users] =?utf-8?q?=28no_subject=29?=
Message-ID: <CAMddHTfXA2asj3wNuQcbQzrK5sWyp1VYGxSCX3Y4wU4v9NSk2g@mail.gmail.com>

Hi,
we have configured squid3 with ssl,
cache is working fine with http and https for hls without any query
parameters in url

below two cases not working, could you please help us on this
1) Dash content is not getting cached - do we need to make any changes ?
2) if we append any query parameter like token, expiration time for url,
the cache is not working

Please find the squid config file
thanks in advance

Regards,
Praveen

ftp://artfiles.org/squid-cache.org/pub/archive/3.1/squid-3.1.19.tar.gz
openssl
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20181210/2474f0ac/attachment.htm>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: squid.conf
Type: application/octet-stream
Size: 16564 bytes
Desc: not available
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20181210/2474f0ac/attachment.obj>

From praveen.apalya at gmail.com  Mon Dec 10 08:01:51 2018
From: praveen.apalya at gmail.com (Praveen Valaboju)
Date: Mon, 10 Dec 2018 13:31:51 +0530
Subject: [squid-users] query parameters filtering
Message-ID: <CAMddHTcAMZ9oP2omYD8DCgF_2_wZs00AvB1MLyMfMaQgvizjZg@mail.gmail.com>

Hi,
we have configured squid3 with ssl,
cache is working fine with http and https for hls without any query
parameters in url

below two cases not working, could you please help us on this
1) Dash content is not getting cached - do we need to make any changes ?
2) if we append any query parameter like token, expiration time for url,
the cache is not working

Please find the squid config file
thanks in advance

Regards,
Praveen

ftp://artfiles.org/squid-cache.org/pub/archive/3.1/squid-3.1.19.tar.gz
openssl
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20181210/d94565e2/attachment.htm>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: squid.conf
Type: application/octet-stream
Size: 16564 bytes
Desc: not available
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20181210/d94565e2/attachment.obj>

From squid3 at treenet.co.nz  Tue Dec 11 00:52:31 2018
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Tue, 11 Dec 2018 13:52:31 +1300
Subject: [squid-users] query parameters filtering
In-Reply-To: <CAMddHTcAMZ9oP2omYD8DCgF_2_wZs00AvB1MLyMfMaQgvizjZg@mail.gmail.com>
References: <CAMddHTcAMZ9oP2omYD8DCgF_2_wZs00AvB1MLyMfMaQgvizjZg@mail.gmail.com>
Message-ID: <f01bb370-941b-2102-3d5b-544207d42d4b@treenet.co.nz>

On 10/12/18 9:01 pm, Praveen Valaboju wrote:
> 
> Hi,
> we have configured squid3 with ssl,?
> cache is working fine with http and https for hls without any query
> parameters in url
> 
> below two cases not working, could you please help us on this
> 1) Dash content is not getting cached - do we need to make any changes ?

Depends on what those Dash messages do, and what HTTP header mechanisms
they are using.

FYI: Your config is also causing a lot of HTTP violations to take place.
The only guarantee you have is that thing will *not* work perfectly.


> 2) if we append any query parameter like token, expiration time for url,
> the cache is not working
> 

Cache is based on a hash of the full URL. Any single-character change
anywhere, including the query section makes the transaction a different
URL. This is just how HTTP caching works.

Details such as expiration times or non-URL content identifiers are
supposed to be sent in the HTTP headers which exist for such things
(e.g. Expires, Cache-Control:max-age, or Last-Modified).


We have a Store-ID feature in Squid-3.4 or later to workaround sites
that duplicate *exactly* identical content at many URLs. This is a
dangerous feature though, so use with extreme caution.


> Please find the squid config file

FWIW; This config file shows a Squid version which is extremely out
dated. Squid-3.1 is not capable of several highly important and popular
TLS features, nor of proper HTTP/1.1 caching (still has HTTP/1.0 caching).

Our current production (aka. "stable") release is Squid-4.4. Or for less
recent OS versions the previous Squid-3.5.28 should be available and can
serve modern HTTP caching needs far better.

Amos


From mike.quentel.rbc at gmail.com  Tue Dec 11 15:41:56 2018
From: mike.quentel.rbc at gmail.com (Mike Quentel)
Date: Tue, 11 Dec 2018 10:41:56 -0500
Subject: [squid-users] SslBump Peek and Splice using Squid-4.1-5 in Amazon1
	Linux with Squid Helpers
Message-ID: <CAE53ZLD9oqt0cUKsrwHSDo8nDEo2Midty0SFMFo842-3u9ovAw@mail.gmail.com>

Hi, I have been unsuccessfully trying to get Squid-4.1-5 in AWS
(Amazon 1 Linux) to allow transparent proxy of certain domains, as
well as IPs associated with those domains, whilst rejecting everything
else.

I have been referencing documentation at
https://wiki.squid-cache.org/Features/SslPeekAndSplice

Version of Squid: 4.1-5 for Amazon 1 Linux available at
http://faster.ngtech.co.il/repo/amzn/1/beta/x86_64/ (many thanks to
@elico for these packages) specifically, the following:

1) http://faster.ngtech.co.il/repo/amzn/1/beta/x86_64/squid-4.1-5.amzn1.x86_64.rpm
2) http://faster.ngtech.co.il/repo/amzn/1/beta/x86_64/squid-helpers-4.1-5.amzn1.x86_64.rpm

Example of tests that I am running:

1) curl -kv https://service.us2.sumologic.com (EXPECTED: successfully
accessed; OBSERVED: successfully accessed)
2) curl -kv https://54.149.155.70 (EXPECTED: successfully accessed
because it resolves to service.us2.sumologic.com; OBSERVED:
"Certificate does not match domainname"  [No Error] (TLS code:
SQUID_X509_V_ERR_DOMAIN_MISMATCH))
3) curl -kv https://www.google.com (EXPECTED: failed to access;
OBSERVED: failed to access)
4) curl -kv https://172.217.13.164 (EXPECTED: failed to access;
OBSERVED: "Certificate does not match domainname"  [No Error] (TLS
code: SQUID_X509_V_ERR_DOMAIN_MISMATCH))

Below is the latest version of the squid.conf being used. Apologies
for any obvious errors--new to Squid here. I have been grappling with
this for weeks, with many iterations of squid.conf so any advice is
greatly appreciated; many thanks in advance.

---

visible_hostname squid

host_verify_strict off

# Handling HTTP requests
http_port 3128
http_port 3129 intercept

sslcrtd_children 10

acl CONNECT method CONNECT

# AWS services domain
acl allowed_http_sites dstdomain .amazonaws.com
# docker hub registry
acl allowed_http_sites dstdomain .docker.io
acl allowed_http_sites dstdomain .docker.com
acl allowed_http_sites dstdomain www.congiu.net

# Handling HTTPS requests
# https_port 3130 intercept ssl-bump generate-host-certificates=on
dynamic_cert_mem_cache_size=100MB cert=/etc/squid/squid.pem
https_port 3130 intercept ssl-bump dynamic_cert_mem_cache_size=100MB
cert=/etc/squid/squid.pem
acl SSL_port port 443

# AWS services domain
acl allowed_https_sites ssl::server_name .amazonaws.com
# docker hub registry
acl allowed_https_sites ssl::server_name .docker.io
acl allowed_https_sites ssl::server_name .docker.com

# project specific
acl allowed_https_sites ssl::server_name www.congiu.net
acl allowed_https_sites ssl::server_name mirrors.fedoraproject.org
acl allowed_https_sites ssl::server_name mirror.csclub.uwaterloo.ca

# nslookup resolved IPs for collectors.sumologic.com
# workaround solution to support sumologic collector
acl allowed_https_sites ssl::server_name .sumologic.com
# THE FOLLOWING TWO LINES DO NOT SEEM TO WORK AS EXPECTED
# acl allowed_https_sites ssl::server_name --server-provided
service.sumologic.com sslflags=DONT_VERIFY_PEER
# acl allowed_https_sites ssl::server_name --server-provided
service.us2.sumologic.com sslflags=DONT_VERIFY_PEER

acl step1 at_step SslBump1
acl step2 at_step SslBump2
acl step3 at_step SslBump3

ssl_bump peek step1 all
ssl_bump peek step2 allowed_https_sites
# http://lists.squid-cache.org/pipermail/squid-users/2018-September/019150.html
ssl_bump bump
ssl_bump splice step3 allowed_https_sites
ssl_bump bump
ssl_bump terminate step2 all

http_access allow CONNECT

# http_access allow SSL_port

http_access deny CONNECT !allowed_https_sites
http_access deny CONNECT !allowed_http_sites
http_access allow allowed_https_sites
http_access allow allowed_http_sites
http_access deny all

cache deny all

debug_options "ALL,9"


From flashdown at data-core.org  Tue Dec 11 17:53:23 2018
From: flashdown at data-core.org (Enrico Heine)
Date: Tue, 11 Dec 2018 18:53:23 +0100
Subject: [squid-users] SslBump Peek and Splice using Squid-4.1-5 in
	Amazon1	Linux with Squid Helpers
In-Reply-To: <CAE53ZLD9oqt0cUKsrwHSDo8nDEo2Midty0SFMFo842-3u9ovAw@mail.gmail.com>
References: <CAE53ZLD9oqt0cUKsrwHSDo8nDEo2Midty0SFMFo842-3u9ovAw@mail.gmail.com>
Message-ID: <33156AE7-7EFA-4B8D-8B75-059534D73251@data-core.org>

Dear Mike, 

Please checkout the following and let us know if you need further help. 

http://www.squid-cache.org/Doc/config/sslproxy_cert_error/

Best regards,

Flashdown

Am 11. Dezember 2018 16:41:56 MEZ schrieb Mike Quentel <mike.quentel.rbc at gmail.com>:
>Hi, I have been unsuccessfully trying to get Squid-4.1-5 in AWS
>(Amazon 1 Linux) to allow transparent proxy of certain domains, as
>well as IPs associated with those domains, whilst rejecting everything
>else.
>
>I have been referencing documentation at
>https://wiki.squid-cache.org/Features/SslPeekAndSplice
>
>Version of Squid: 4.1-5 for Amazon 1 Linux available at
>http://faster.ngtech.co.il/repo/amzn/1/beta/x86_64/ (many thanks to
>@elico for these packages) specifically, the following:
>
>1)
>http://faster.ngtech.co.il/repo/amzn/1/beta/x86_64/squid-4.1-5.amzn1.x86_64.rpm
>2)
>http://faster.ngtech.co.il/repo/amzn/1/beta/x86_64/squid-helpers-4.1-5.amzn1.x86_64.rpm
>
>Example of tests that I am running:
>
>1) curl -kv https://service.us2.sumologic.com (EXPECTED: successfully
>accessed; OBSERVED: successfully accessed)
>2) curl -kv https://54.149.155.70 (EXPECTED: successfully accessed
>because it resolves to service.us2.sumologic.com; OBSERVED:
>"Certificate does not match domainname"  [No Error] (TLS code:
>SQUID_X509_V_ERR_DOMAIN_MISMATCH))
>3) curl -kv https://www.google.com (EXPECTED: failed to access;
>OBSERVED: failed to access)
>4) curl -kv https://172.217.13.164 (EXPECTED: failed to access;
>OBSERVED: "Certificate does not match domainname"  [No Error] (TLS
>code: SQUID_X509_V_ERR_DOMAIN_MISMATCH))
>
>Below is the latest version of the squid.conf being used. Apologies
>for any obvious errors--new to Squid here. I have been grappling with
>this for weeks, with many iterations of squid.conf so any advice is
>greatly appreciated; many thanks in advance.
>
>---
>
>visible_hostname squid
>
>host_verify_strict off
>
># Handling HTTP requests
>http_port 3128
>http_port 3129 intercept
>
>sslcrtd_children 10
>
>acl CONNECT method CONNECT
>
># AWS services domain
>acl allowed_http_sites dstdomain .amazonaws.com
># docker hub registry
>acl allowed_http_sites dstdomain .docker.io
>acl allowed_http_sites dstdomain .docker.com
>acl allowed_http_sites dstdomain www.congiu.net
>
># Handling HTTPS requests
># https_port 3130 intercept ssl-bump generate-host-certificates=on
>dynamic_cert_mem_cache_size=100MB cert=/etc/squid/squid.pem
>https_port 3130 intercept ssl-bump dynamic_cert_mem_cache_size=100MB
>cert=/etc/squid/squid.pem
>acl SSL_port port 443
>
># AWS services domain
>acl allowed_https_sites ssl::server_name .amazonaws.com
># docker hub registry
>acl allowed_https_sites ssl::server_name .docker.io
>acl allowed_https_sites ssl::server_name .docker.com
>
># project specific
>acl allowed_https_sites ssl::server_name www.congiu.net
>acl allowed_https_sites ssl::server_name mirrors.fedoraproject.org
>acl allowed_https_sites ssl::server_name mirror.csclub.uwaterloo.ca
>
># nslookup resolved IPs for collectors.sumologic.com
># workaround solution to support sumologic collector
>acl allowed_https_sites ssl::server_name .sumologic.com
># THE FOLLOWING TWO LINES DO NOT SEEM TO WORK AS EXPECTED
># acl allowed_https_sites ssl::server_name --server-provided
>service.sumologic.com sslflags=DONT_VERIFY_PEER
># acl allowed_https_sites ssl::server_name --server-provided
>service.us2.sumologic.com sslflags=DONT_VERIFY_PEER
>
>acl step1 at_step SslBump1
>acl step2 at_step SslBump2
>acl step3 at_step SslBump3
>
>ssl_bump peek step1 all
>ssl_bump peek step2 allowed_https_sites
>#
>http://lists.squid-cache.org/pipermail/squid-users/2018-September/019150.html
>ssl_bump bump
>ssl_bump splice step3 allowed_https_sites
>ssl_bump bump
>ssl_bump terminate step2 all
>
>http_access allow CONNECT
>
># http_access allow SSL_port
>
>http_access deny CONNECT !allowed_https_sites
>http_access deny CONNECT !allowed_http_sites
>http_access allow allowed_https_sites
>http_access allow allowed_http_sites
>http_access deny all
>
>cache deny all
>
>debug_options "ALL,9"
>_______________________________________________
>squid-users mailing list
>squid-users at lists.squid-cache.org
>http://lists.squid-cache.org/listinfo/squid-users

-- 
Diese Nachricht wurde von meinem Android-Ger?t mit K-9 Mail gesendet.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20181211/f46270b6/attachment.htm>

From squid3 at treenet.co.nz  Tue Dec 11 23:07:49 2018
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Wed, 12 Dec 2018 12:07:49 +1300
Subject: [squid-users] SslBump Peek and Splice using Squid-4.1-5 in
 Amazon1 Linux with Squid Helpers
In-Reply-To: <33156AE7-7EFA-4B8D-8B75-059534D73251@data-core.org>
References: <CAE53ZLD9oqt0cUKsrwHSDo8nDEo2Midty0SFMFo842-3u9ovAw@mail.gmail.com>
 <33156AE7-7EFA-4B8D-8B75-059534D73251@data-core.org>
Message-ID: <15dc44bc-ef9e-02a6-f4bb-0d05087f5a3b@treenet.co.nz>

On 12/12/18 6:53 am, Enrico Heine wrote:
> Dear Mike,
> 
> Please checkout the following and let us know if you need further help.
> 
> http://www.squid-cache.org/Doc/config/sslproxy_cert_error/
> 

Before you use it though, please consider what the words "Certificate
does not match domainname" actually *mean*.

This Squid is configured to deliver a single specific custom-built
certificate to all clients who contact the proxy. Yet the proxy is being
used to receive TLS traffic for any domain and the admin is passing test
traffic for multiple different domains and raw-IP addresses.



> Best regards,
> 
> Flashdown
> 
> Am 11. Dezember 2018 16:41:56 MEZ schrieb Mike Quentel:
> 
>     Hi, I have been unsuccessfully trying to get Squid-4.1-5 in AWS
>     (Amazon 1 Linux) to allow transparent proxy of certain domains, as
>     well as IPs associated with those domains, whilst rejecting everything
>     else.
> 
>     I have been referencing documentation at
>     https://wiki.squid-cache.org/Features/SslPeekAndSplice
> 
>     Version of Squid: 4.1-5 for Amazon 1 Linux available at
>     http://faster.ngtech.co.il/repo/amzn/1/beta/x86_64/ (many thanks to
>     @elico for these packages) specifically, the following:
> 
>     1) http://faster.ngtech.co.il/repo/amzn/1/beta/x86_64/squid-4.1-5.amzn1.x86_64.rpm
>     2) http://faster.ngtech.co.il/repo/amzn/1/beta/x86_64/squid-helpers-4.1-5.amzn1.x86_64.rpm
> 
>     Example of tests that I am running:
> 
>     1) curl -kv https://service.us2.sumologic.com (EXPECTED: successfully
>     accessed; OBSERVED: successfully accessed)

The TLS SNI contains "service.us2.sumologic.com", and
 - the server produced an X.509 certificate for that domain, and
 - your server_name ACL matches it as a sub-domain of ".sumologic.com"


Note that the -k parameter for curl only disables security on the
curl<->Squid TSL connection. It has nothing to do with the
Squid<->origin connections.

You should really be using "curl --cacert /etc/squid/squid.pem" or
connections without the -k to test what actually happens for clients
when their traffic goes through your system.


>     2) curl -kv https://54.149.155.70 (EXPECTED: successfully accessed
>     because it resolves to service.us2.sumologic.com; OBSERVED:
>     "Certificate does not match domainname"  [No Error] (TLS code:
>     SQUID_X509_V_ERR_DOMAIN_MISMATCH))

IMO the expectation is what is wrong here.

The TLS SNI does not exist, and
 - being intercepted traffic the CONNECT authority is
"54.149.155.70:443", and
 - the server produced an X.509 certificate with SubjectName of either
"54.149.155.70" or something else not matching your server_name ACL entries.

FYI: server_name is a text-string matching ACL. I expect you will find
there is no reverse-DNS being performed during the ssl_bump testing,
only later after contact the server has already been decided to allow.
You can confirm that with the debug log your test produced. Look for the
lines saying what each ACL is checking for and against.


>     3) curl -kv https://www.google.com (EXPECTED: failed to access;
>     OBSERVED: failed to access)

"failed to access" is a gross over-simplification. This transaction is
both allowed and not-allowed at the same time.

If you look into the log I expect you will see this sequence happening:

 * the http_access rules *allow* the CONNECT tunnel, then

 * the ssl_bump rules select do "bump" action at Step-2 (aka. using only
the TLS clientHello details), then

 * curl -k ignores the small problem that you are not presenting the
X.509 keys belonging to Google.

 * the decrypted GET request inside the tunnel gets rejected because:
 - the "allowed_https_sites" ACL has no X.509 server details to test
against, so does not match.
  - the "allowed_http_sites" ACL does not match either
  - the "http_access deny all" matches everything reaching it.


>     4) curl -kv https://172.217.13.164 (EXPECTED: failed to access;
>     OBSERVED: "Certificate does not match domainname"  [No Error] (TLS
>     code: SQUID_X509_V_ERR_DOMAIN_MISMATCH))

Same thing going on as for test (2).


> 
>     Below is the latest version of the squid.conf being used. Apologies
>     for any obvious errors--new to Squid here. I have been grappling with
>     this for weeks, with many iterations of squid.conf so any advice is
>     greatly appreciated; many thanks in advance.
>     ------------------------------------------------------------------------
>     visible_hostname squid

You have connected this proxy to the Internet. The above is required by
Internet RFCs to be a FQDN (fully qualified domain name).

Even if you do not want to follow that requirements it MUST be a unique
name.  If any of your HTTP traffic ever goes through another proxy
sharing this *very common* config mistake you will encounter forwarding
loop errors.


> 
>     host_verify_strict off

This is the default. No need to configure it.

Also, if you added that because the errors you mentioned are talking
about domain verification - be aware that HTTP "Host:" header
verification is quite a different thing from TLS certificate verification.


> 
>     # Handling HTTP requests
>     http_port 3128
>     http_port 3129 intercept
> 
>     sslcrtd_children 10
> 
>     acl CONNECT method CONNECT
> 
>     # AWS services domain
>     acl allowed_http_sites dstdomain .amazonaws.com
>     # docker hub registry
>     acl allowed_http_sites dstdomain .docker.io
>     acl allowed_http_sites dstdomain .docker.com
>     acl allowed_http_sites dstdomain www.congiu.net
> 
>     # Handling HTTPS requests
>     # https_port 3130 intercept ssl-bump generate-host-certificates=on
>     dynamic_cert_mem_cache_size=100MB cert=/etc/squid/squid.pem
>     https_port 3130 intercept ssl-bump dynamic_cert_mem_cache_size=100MB
>     cert=/etc/squid/squid.pem

FYI: both the lines above behave identical because the generate-*
setting you removed was being set to its default value anyway.


>     acl SSL_port port 443
> 
>     # AWS services domain
>     acl allowed_https_sites ssl::server_name .amazonaws.com
>     # docker hub registry
>     acl allowed_https_sites ssl::server_name .docker.io
>     acl allowed_https_sites ssl::server_name .docker.com
> 
>     # project specific
>     acl allowed_https_sites ssl::server_name www.congiu.net
>     acl allowed_https_sites ssl::server_name mirrors.fedoraproject.org
>     acl allowed_https_sites ssl::server_name mirror.csclub.uwaterloo.ca
> 
>     # nslookup resolved IPs for collectors.sumologic.com
>     # workaround solution to support sumologic collector
>     acl allowed_https_sites ssl::server_name .sumologic.com
>     # THE FOLLOWING TWO LINES DO NOT SEEM TO WORK AS EXPECTED

The expectation is wrong here.

The string "sslflags=DONT_VERIFY_PEER" is not a valid domain nor server
hostname. So highly unlikely that the X.509 certificate SubjectName or
AltSubjectName from the origin server will contain that string.

Also, the flag sets the ACL matching algorithm. When that is set the ACL
cannot match during a ssl_bump "peek step1" cycle.

So one should expect this ACL to stop working when these lines are
added. Not expect that it would do anything useful.


FYI: The string "sslflags=DONT_VERIFY_PEER" is the name and value of a
option other directives elsewhere in squid.conf can use. But it is a
very, very, very bad idea to do so - even 'just for testing'.

 The flag DONT_VERIFY_PEER disables all of TLS security checks - meaning
the connection actively becomes *less* safe than regular/plain-text TCP
connections, while simultaneously hiding all resulting issues from
*your* admin view. Users still have problems, you just cannot see any
hint of them.
 So please purge that setting from any configs and documents you come
across. Investigate and fix any TLS problems that appear, don't just
hide the error messages and pretend everything works.
 Same reason not to be using the equivalent "curl -k" option for testing
TLS validation/verification problems.



>     # acl allowed_https_sites ssl::server_name --server-provided
>     service.sumologic.com sslflags=DONT_VERIFY_PEER
>     # acl allowed_https_sites ssl::server_name --server-provided
>     service.us2.sumologic.com sslflags=DONT_VERIFY_PEER
> 
>     acl step1 at_step SslBump1
>     acl step2 at_step SslBump2
>     acl step3 at_step SslBump3
> 
>     ssl_bump peek step1 all

The "all" here is useless and only adds confusion to anyone who thinks
it has any meaning.


>     ssl_bump peek step2 allowed_https_sites
>     # http://lists.squid-cache.org/pipermail/squid-users/2018-September/019150.html

The author of that did not understand how the ssl-bump processing was
working. That entire message thread is them attempting to learn and IMO
still not quite understanding the ideas in the end.

Blindly copying into your config from experiments by someone who does
not understand what they are doing is not a good idea. Use an actually
known-working config example (the Squid wiki has several), or try to
design your own based on your own understanding. At the very least we
can see from your self-designed attempt what you may be thinking and
hopefully teach you where any mistakes are visible.


>     ssl_bump bump

Please be aware that when this line when reached at step2 it performs
"client-first" bumping of the TLS.

That means bumping and performing TLS handshake without any real X.509
server details for your allowed_https_sites ACL to use. Only
client-provided claims about what server they are contacting (which may
be outright lies). This has side-effects on what your ssl::server_name
vs dstdomain ACL do in the later http_access checks.

Specifically when the server_name and the URL domain are different
things a simple as testing one first can change permissions for the
client in the other.


>     ssl_bump splice step3 allowed_https_sites

So some of your traffic will splice from the above line - but only
because of the peek (step2) then bump (step3) combination being impossible.


>     ssl_bump bump

There is already an unrestricted "bump" action earlier. This line does
nothing even if it were possibly reached.


>     ssl_bump terminate step2 all

There is a peek action specified earlier for step2, with an unrestricted
bump action as a fallback when allowed_https_sites fails to match. This
line is never reachable.

> 
>     http_access allow CONNECT

Ouch. Really do not do the above. The default config file shipped with
Squid starts with these lines for very good reasons:

"
  http_access deny !Safe_ports
  http_access deny CONNECT !SSL_ports
  http_access allow localhost manager
  http_access deny manager

  #
  # INSERT YOUR OWN RULE(S) HERE TO ALLOW ACCESS FROM YOUR CLIENTS
  #
"

Those reasons (DoS and proxy relay security vulnerabilities) are still
very much relevant in your setup, with no reason to remove them. So
please add them back before you continue testing things, with your
custom http_access rules *underneath* that comment line.

Also, the default SSL_ports is already setup in a way that meets your
requirements. You can adjust Safe_ports to be only the same port, or use
the default set of safe-for-HTTP ports.

FYI: the config you have right now allows any malicious origin server
receiving trafic on port 443 to present a X.509 certificate claiming to be.


> 
>     # http_access allow SSL_port
> 
>     http_access deny CONNECT !allowed_https_sites
>     http_access deny CONNECT !allowed_http_sites

The above two lines do nothing in your current config. CONNECT requests
are *always* allowed by the line you had earlier.

Once you move back to the default security checks these will start to do
things. It would probably be best to remove the two lines above to
prevent the unexpected new behaviour from confusing you further.


>     http_access allow allowed_https_sites
>     http_access allow allowed_http_sites

These ACLs are very badly named.

* The one called "allowed_https_sites":
 - will *not* match against HTTPS traffic arriving on port 3128 unless
the CONNECT authority-uri names a domain in your list.

- *will* match against HTTP traffic on port 3128 and port 3129 with
"https://" URLs.

* The one called "allowed_http_sites" *will* match against HTTPS traffic
arriving on any port.

 --> meaning that for both of them the "https" and "http" word in their
name is deceptive.

* Both these ACLs are used to *deny* traffic.
 --> meaning the "allowed" word in their name is deceptive.

What you are left with is just "sites" which is so vague as to be
meaningless.


It would be a lot clearer if you renamed them:
 - "allowed_https_sites" to "tls_servers"
 - "allowed_http_sites" to "url_domains"


>     http_access deny all
> 
>     cache deny all
> 
>     debug_options "ALL,9"

Cheers
Amos


From rejaine at bhz.jamef.com.br  Wed Dec 12 12:58:16 2018
From: rejaine at bhz.jamef.com.br (Rejaine Silveira Monteiro)
Date: Wed, 12 Dec 2018 10:58:16 -0200
Subject: [squid-users] TCP_MISS_ABORTED/000 FATAL: Received Segment Violation
Message-ID: <CAMTrDfX=PT+6yHRq7qYTT=EBMP=DQ7MU_RaF3FU22JF_uNWH2w@mail.gmail.com>

Hi, all

I'm trying to download a certain file from an FTP server. I can make
the access directly, without proxy, but when I use Squid the following
error occurs:

 TCP_MISS_ABORTED/000 0 GET
ftp://ftp.ibge.gov.br/Precos_Indices_de_Precos_ao_Consumidor/IPCA/Serie_Historica/ipca_SerieHist.zip
- HIER_DIRECT

I've tried several configurations, including "dns_v4_first on", but
nothing worked

This strange error also occurs in the cache.log

 tail /var/log/squid/cache.log -f
2018/12/12 10:56:09 kid1| storeLateRelease: released 0 objects
(squid-1)(_Z5deathi+0x5c)[0x7f7a8ca0f3ac]
/lib64/libpthread.so.0(+0xf870)[0x7f7a8c321870]
(squid-1)(_ZNK2Ip7Address4portEv+0x0)[0x7f7a8caa0940]
(squid-1)(+0x1e7950)[0x7f7a8c939950]
(squid-1)(+0x1e8ce8)[0x7f7a8c93ace8]
(squid-1)(_ZN12FtpStateData18handleControlReplyEv+0x151)[0x7f7a8c935511]
(squid-1)(_ZN9JobDialerI12FtpStateDataE4dialER9AsyncCall+0x35)[0x7f7a8c93f445]
(squid-1)(_ZN9AsyncCall4makeEv+0x319)[0x7f7a8ca8dbd9]
(squid-1)(_ZN14AsyncCallQueue8fireNextEv+0x1d5)[0x7f7a8ca91675]
(squid-1)(_ZN14AsyncCallQueue4fireEv+0x20)[0x7f7a8ca91a20]
(squid-1)(_ZN9EventLoop7runOnceEv+0xec)[0x7f7a8c91d33c]
(squid-1)(_ZN9EventLoop3runEv+0x18)[0x7f7a8c91d4d8]
(squid-1)(_Z9SquidMainiPPc+0x14ef)[0x7f7a8c99c69f]
(squid-1)(main+0x9)[0x7f7a8c88d129]
/lib64/libc.so.6(__libc_start_main+0xf5)[0x7f7a89e15b05]
(squid-1)(+0x140d95)[0x7f7a8c892d95]
FATAL: Received Segment Violation...dying.
2018/12/12 10:56:24 kid1| Closing HTTP port [::]:3128
2018/12/12 10:56:24 kid1| storeDirWriteCleanLogs: Starting...
2018/12/12 10:56:24 kid1|   Finished.  Wrote 23403 entries.
2018/12/12 10:56:24 kid1|   Took 0.01 seconds (2839136.24 entries/sec).

Here's more details about my version of Squid.

Squid Cache: Version 3.4.4
configure options:  '--host=x86_64-suse-linux-gnu'
'--build=x86_64-suse-linux-gnu' '--program-prefix=' '--prefix=/usr'
'--exec-prefix=/usr' '--bindir=/usr/bin' '--sbindir=/usr
/sbin' '--sysconfdir=/etc' '--datadir=/usr/share'
'--includedir=/usr/include' '--libdir=/usr/lib64'
'--libexecdir=/usr/lib' '--localstatedir=/var'
'--sharedstatedir=/usr/com'
'--mandir=/usr/share/man' '--infodir=/usr/share/info'
'--disable-dependency-tracking' '--disable-strict-error-checking'
'--sysconfdir=/etc/squid' '--libexecdir=/usr/sbin' '--d
atadir=/usr/share/squid' '--sharedstatedir=/var/squid'
'--with-logdir=/var/log/squid' '--with-pidfile=/run/squid.pid'
'--with-dl' '--enable-disk-io' '--enable-storeio' '--enab
le-removal-policies=heap,lru' '--enable-icmp' '--enable-delay-pools'
'--enable-esi' '--enable-icap-client' '--enable-useragent-log'
'--enable-referer-log' '--enable-kill-paren
t-hack' '--enable-arp-acl' '--enable-ssl' '--enable-ssl-crtd'
'--enable-forw-via-db' '--enable-cache-digests'
'--enable-linux-netfilter' '--with-large-files' '--enable-undersc
ores' '--enable-auth' '--enable-auth-basic' '--enable-auth-ntlm'
'--enable-auth-negotiate' '--enable-auth-digest'
'--enable-external-acl-helpers=LDAP_group,eDirectory_userip,f
ile_userip,kerberos_ldap_group,session,unix_group,wbinfo_group'
'--enable-ntlm-fail-open' '--enable-stacktraces'
'--enable-x-accelerator-vary' '--with-default-user=squid' '--d
isable-ident-lookups' '--enable-follow-x-forwarded-for'
'--disable-arch-native' 'build_alias=x86_64-suse-linux-gnu'
'host_alias=x86_64-suse-linux-gnu' 'CFLAGS=-fmessage-length
=0 -grecord-gcc-switches -O2 -Wall -D_FORTIFY_SOURCE=2
-fstack-protector -funwind-tables -fasynchronous-unwind-tables -g
-fPIE -fPIC -DOPENSSL_LOAD_CONF' 'LDFLAGS=-Wl,-z,relro
,-z,now -pie' 'CXXFLAGS=-fmessage-length=0 -grecord-gcc-switches -O2
-Wall -D_FORTIFY_SOURCE=2 -fstack-protector -funwind-tables
-fasynchronous-unwind-tables -g -fPIE -fPIC -D
OPENSSL_LOAD_CONF' 'PKG_CONFIG_PATH=:/usr/lib64/pkgconfig:/usr/share/pkgconfig

-- 
*Esta mensagem pode conter informa??es confidenciais ou privilegiadas, 
sendo seu sigilo protegido por lei. Se voc? n?o for o destinat?rio ou a 
pessoa autorizada a receber esta mensagem, n?o pode usar, copiar ou 
divulgar as informa??es nela contidas ou tomar qualquer a??o baseada nessas 
informa??es. Se voc? recebeu esta mensagem por engano, por favor avise 
imediatamente ao remetente, respondendo o e-mail e em seguida apague-o. 
Agradecemos sua coopera??o.*


From squid3 at treenet.co.nz  Wed Dec 12 13:41:46 2018
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 13 Dec 2018 02:41:46 +1300
Subject: [squid-users] TCP_MISS_ABORTED/000 FATAL: Received Segment
 Violation
In-Reply-To: <CAMTrDfX=PT+6yHRq7qYTT=EBMP=DQ7MU_RaF3FU22JF_uNWH2w@mail.gmail.com>
References: <CAMTrDfX=PT+6yHRq7qYTT=EBMP=DQ7MU_RaF3FU22JF_uNWH2w@mail.gmail.com>
Message-ID: <14e6e361-f62c-c571-8df6-31652e8cdd8a@treenet.co.nz>

On 13/12/18 1:58 am, Rejaine Silveira Monteiro wrote:
> Hi, all
> 
> I'm trying to download a certain file from an FTP server. I can make
> the access directly, without proxy, but when I use Squid the following
> error occurs:
> 
>  TCP_MISS_ABORTED/000 0 GET
> ftp://ftp.ibge.gov.br/Precos_Indices_de_Precos_ao_Consumidor/IPCA/Serie_Historica/ipca_SerieHist.zip
> - HIER_DIRECT
> 
> I've tried several configurations, including "dns_v4_first on", but
> nothing worked
> 
> This strange error also occurs in the cache.log
> 
>  tail /var/log/squid/cache.log -f
> 2018/12/12 10:56:09 kid1| storeLateRelease: released 0 objects
> (squid-1)(_Z5deathi+0x5c)[0x7f7a8ca0f3ac]
> /lib64/libpthread.so.0(+0xf870)[0x7f7a8c321870]
> (squid-1)(_ZNK2Ip7Address4portEv+0x0)[0x7f7a8caa0940]
> (squid-1)(+0x1e7950)[0x7f7a8c939950]
> (squid-1)(+0x1e8ce8)[0x7f7a8c93ace8]
> (squid-1)(_ZN12FtpStateData18handleControlReplyEv+0x151)[0x7f7a8c935511]
> (squid-1)(_ZN9JobDialerI12FtpStateDataE4dialER9AsyncCall+0x35)[0x7f7a8c93f445]


Can you replicate this in any of the current Squid versions (3.5.28 or
later)? There have been quite a few changes to the FTP related code
since Squid-3.4.

Amos


From subhish.pillai at broadcom.com  Wed Dec 12 19:58:15 2018
From: subhish.pillai at broadcom.com (subhish.pillai)
Date: Wed, 12 Dec 2018 13:58:15 -0600 (CST)
Subject: [squid-users] HTTPS proxy setup questions
Message-ID: <1544644695402-0.post@n4.nabble.com>

Hi All,

I have a client application that sends periodic usage data to an external
application server over HTTPS using REST API calls. I want to tunnel this
connection through an HTTPS proxy at the client location. I am trying to
setup Squid v4.4 on Centos 7 server for doing this. 

The clients are explicitly configured to connect through the proxy server
and has the CA certificate from the application server installed.

Being new to squid and proxy servers in general, I have a few basic
questions that I want to clarify --
1. What is the difference between SSL bumping and SSL interception? 
2. What is the difference between "http_port 3128 intercept" and "http_port
3128 transparent"? Do i need to setup the http_port as either of these?
3. Do I need to create self-signed certs on the proxy server and distribute
it to the client and application server?

I have tried setting up ssl-bump with self-signed certs on the proxy server,
but I get a "http/1.1 400 bad request" when the client attempts a connection
and also see this in the squid access log "NONE/400 3820 NONE
error:invalid-request - HIER_NONE/- text/html"

At this point I am not sure if this is a client connection problem or a
squid config issue. I would really appreciate if somebody could validate my
squid configuration below or can point me in the right direction.

Thanks,
Subhish

Here is my squid.conf config --
acl localnet src 10.0.0.0/8     # RFC1918 possible internal network
acl localnet src 172.16.0.0/12  # RFC1918 possible internal network
acl localnet src 192.168.0.0/16 # RFC1918 possible internal network
acl localnet src fc00::/7       # RFC 4193 local private network range
acl localnet src fe80::/10      # RFC 4291 link-local (directly plugged)
machines

acl SSL_ports port 443
acl SSL_ports port 8449
acl Safe_ports port 80          # http
acl Safe_ports port 21          # ftp
acl Safe_ports port 443         # https
acl Safe_ports port 70          # gopher
acl Safe_ports port 210         # wais
acl Safe_ports port 1025-65535  # unregistered ports
acl Safe_ports port 280         # http-mgmt
acl Safe_ports port 488         # gss-http
acl Safe_ports port 591         # filemaker
acl Safe_ports port 777         # multiling http
acl CONNECT method CONNECT

# Deny requests to certain unsafe ports
http_access deny !Safe_ports

# Deny CONNECT to other than secure SSL ports
#http_access deny CONNECT !SSL_ports
http_access allow CONNECT !SSL_ports

# Only allow cachemgr access from localhost
http_access allow localhost manager
http_access deny manager

# We strongly recommend the following be uncommented to protect innocent
# web applications running on the proxy server who think the only
# one who can access services on "localhost" is a local user
#http_access deny to_localhost

#
# INSERT YOUR OWN RULE(S) HERE TO ALLOW ACCESS FROM YOUR CLIENTS
#

# Example rule allowing access from your local networks.
# Adapt localnet in the ACL section to list your (internal) IP networks
# from where browsing should be allowed
http_access allow localnet
http_access allow localhost

# And finally deny all other access to this proxy
http_access allow all

# Squid normally listens to port 3128
#http_port 3128
http_port 3128 ssl-bump cert=/etc/squid/ssl_cert/myCA.pem
key=/etc/squid/ssl_cert/myCA.pem generate-host-certificates=on
dynamic_cert_mem_cache_size=4MB options=NO_SSLv2
#http_port 3128 ssl-bump cert=/etc/squid/ssl_cert/ca-bundle.pem
key=/etc/squid/ssl_cert/ca-bundle.pem generate-host-certificates=on
dynamic_cert_mem_cache_size=4MB options=NO_SSLv2

#dns_v4_first on
sslcrtd_program /usr/lib64/squid/security_file_certgen -s /var/lib/ssl_db -M
4MB

acl step1 at_step SslBump1

ssl_bump peek step1
ssl_bump bump all

#tls_outgoing_options cafile=/etc/squid/ssl_cert/ca-bundle.pem

## Allow server side certificate errors such as untrusted certificates,
otherwise the connection is closed for such errors
sslproxy_cert_error allow all

## Accept certificates that fail verification (should only be needed if
using 'sslproxy_cert_error allow all')
sslproxy_flags DONT_VERIFY_PEER

# Uncomment and adjust the following to add a disk cache directory.
#cache_dir ufs /var/cache/squid 100 16 256

# Leave coredumps in the first cache dir
coredump_dir /var/cache/squid
shutdown_lifetime 5 seconds
#
# Add any of your own refresh_pattern entries above these.
#
refresh_pattern ^ftp:           1440    20%     10080
refresh_pattern ^gopher:        1440    0%      1440
refresh_pattern -i (/cgi-bin/|\?) 0     0%      0
refresh_pattern .               0       20%     4320





--
Sent from: http://squid-web-proxy-cache.1019090.n4.nabble.com/Squid-Users-f1019091.html


From rousskov at measurement-factory.com  Wed Dec 12 22:49:09 2018
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Wed, 12 Dec 2018 15:49:09 -0700
Subject: [squid-users] HTTPS proxy setup questions
In-Reply-To: <1544644695402-0.post@n4.nabble.com>
References: <1544644695402-0.post@n4.nabble.com>
Message-ID: <508b070f-f479-b62f-af45-6ff5a303555b@measurement-factory.com>

On 12/12/18 12:58 PM, subhish.pillai wrote:


> 1. What is the difference between SSL bumping and SSL interception? 

These concepts describe activities at different layers:

* SSL bumping is, in Squid context, inspection of SSL traffic that often
also involves impersonating the origin server and decrypting encrypted
HTTP traffic (i.e. a MitM attack on the client-server HTTPS communication).

* SSL interception is, in this context, directing (TCP/IP traffic that
presumably carries) SSL traffic off its "natural" TCP/IP path so that it
gets to Squid. Interception itself works at protocol layers below SSL
and HTTP. What happens when the SSL traffic gets to Squid is outside
"SSL interception" scope.

Usually, folks intercept SSL traffic to bump it, but YMMV. It is
possible, for example, to simply log TCP-level information about the
intercepted traffic without any MitM attacks on SSL.


> 2. What is the difference between "http_port 3128 intercept" and "http_port
> 3128 transparent"? Do i need to setup the http_port as either of these?

The difference is in whether Squid impersonates the IP client, but you
need neither because your "clients are explicitly configured to connect
through the proxy server". You do not need to divert traffic from its
natural TCP/IP path to proxy it because that natural TCP/IP path already
goes through your proxy.


> 3. Do I need to create self-signed certs on the proxy server and distribute
> it to the client and application server?

* Yes if you want to inspect encrypted HTTP traffic of your client
application (i.e. get to the HTTP stuff inside the SSL layer).

* Yes if you want client to be able to read Squid-generated error pages.

* No otherwise. In this case, Squid will be just a blind TCP tunnel.


What do you want to use Squid for? The answer to that question has a
significant effect on your Squid configuration.


> # And finally deny all other access to this proxy
> http_access allow all

FWIW, your rule does not match the comment and creates an open proxy.
Both are bad.


HTH,

Alex.


From sam at myriadworks.org  Wed Dec 12 23:15:16 2018
From: sam at myriadworks.org (sam.handley)
Date: Wed, 12 Dec 2018 17:15:16 -0600 (CST)
Subject: [squid-users] SSL Bump with HTTP Cache Peer Parent
Message-ID: <1544656516869-0.post@n4.nabble.com>

I am not 100% confident what I am asking is possible but I'd love it to be
confirmed.

Here is what our setup would look like, I?ve explained a bit below:

DEVICE ---> PRX3 (HTTPS CACHE) ---> PRX2 ---> PRX1 ---> INTERNET

Our current environment is a bit behind the times and inflexible. We have a
local squid proxy/cache (PRX2) that we do not fully control that only caches
HTTP content. This proxy is downstream from another proxy which is also HTTP
(PRX1). Both just TUNNEL HTTPS. PRX1 is the only way out of our WAN to the
internet.

We would like to start caching HTTPS (PRX3) because these other proxies are
not and it is costing us bandwidth. With the config below and a direct
internet connection I can successfully connect and cache HTTP/S content.
However, this won?t work in our environment. We must go through a cache peer
either PRX1 or PRX2, adding either upstream proxy as a cache peer parent
results in either SSL errors or the request not being forwarded to the peer. 

I think what I need to do is TUNNEL the bumped request to PRX2 over HTTP. I
thought squid 4 could do this but can?t find any docs for it so it may have
been wishful thinking.

*--- SSL Error ---*
(71) Protocol error (TLS code: SQUID_ERR_SSL_HANDSHAKE)
Handshake with SSL server failed: error:1408F10B:SSL
routines:ssl3_get_record:wrong version number
*--- SSL Error ---*

*--- squid.conf ---*
## Proxy

# Only allow addresss in our subnet
acl LAN src 10.141.28.0/22
http_access allow LAN
http_access deny all

cache_mem 500 MB
maximum_object_size 5000 MB
range_offset_limit 5000 MB

# Set proxy port enable ssl bump, set root cert
http_port 3128 ssl-bump tls-cert=/etc/squid/CA.pem
generate-host-certificates=on dynamic_cert_mem_cache_size=4MB
options=NO_SSLv3,NO_TLSv1,NO_TLSv1_1,SINGLE_DH_USE,SINGLE_ECDH_USE
ssl_bump bump all

# Set cache directory and settings [type] [dir] [MB] [L1 = number of first
level subdirs] [L2 = number of second level subdirs] [[options]]
cache_dir diskd /srv/cache 10000 64 72

never_direct allow all
cache_peer 10.141.28.19 parent 800 0 no-query no-digest 

refresh_pattern ^ftp:           1440    20%     10080
refresh_pattern ^gopher:        1440    0%      1440
refresh_pattern -i (/cgi-bin/|\?) 0     0%      0
refresh_pattern .               0       20%     4320
*--- squid.conf ---*

*--- squid version info --- *
Squid Cache: Version 4.4
Service Name: squid

This binary uses OpenSSL 1.1.1a  20 Nov 2018. For legal restrictions on
distribution see https://www.openssl.org/source/license.html

configure options:  '--prefix=/usr' '--sbindir=/usr/bin'
'--datadir=/usr/share/squid' '--sysconfdir=/etc/squid'
'--libexecdir=/usr/lib/squid' '--localstatedir=/var'
'--with-logdir=/var/log/squid' '--with-pidfile=/run/squid.pid'
'--enable-auth' '--enable-auth-basic' '--enable-auth-ntlm'
'--enable-auth-digest' '--enable-auth-negotiate'
'--enable-removal-policies=lru,heap' '--enable-storeio=aufs,ufs,diskd,rock'
'--enable-delay-pools' '--with-openssl' '--enable-snmp'
'--enable-linux-netfilter' '--enable-ident-lookups' '--enable-useragent-log'
'--enable-cache-digests' '--enable-referer-log' '--enable-htcp'
'--enable-carp' '--enable-epoll' '--with-large-files' '--enable-arp-acl'
'--with-default-user=proxy' '--enable-async-io' '--enable-truncate'
'--enable-icap-client' '--enable-ssl-crtd' '--disable-arch-native'
'--disable-strict-error-checking' '--enable-wccpv2' 'CFLAGS=-march=x86-64
-mtune=generic -O2 -pipe -fstack-protector-strong -fno-plt'
'LDFLAGS=-Wl,-O1,--sort-common,--as-needed,-z,relro,-z,now'
'CPPFLAGS=-D_FORTIFY_SOURCE=2' 'CXXFLAGS=-march=x86-64 -mtune=generic -O2
-pipe -fstack-protector-strong -fno-plt'
*--- squid version info --- *
         



--
Sent from: http://squid-web-proxy-cache.1019090.n4.nabble.com/Squid-Users-f1019091.html


From subhish.pillai at broadcom.com  Wed Dec 12 23:50:55 2018
From: subhish.pillai at broadcom.com (Subhish Pillai)
Date: Wed, 12 Dec 2018 16:50:55 -0700
Subject: [squid-users] HTTPS proxy setup questions
In-Reply-To: <508b070f-f479-b62f-af45-6ff5a303555b@measurement-factory.com>
References: <1544644695402-0.post@n4.nabble.com>
 <508b070f-f479-b62f-af45-6ff5a303555b@measurement-factory.com>
Message-ID: <CAFODohhZnpb1rROaY5xW5HDDC91wJgfhM_=my7yvBi05v0BJrw@mail.gmail.com>

Thanks Alex, that was very helpful.

Based on your explanation, I just want to use squid as a blind TCP tunnel
carrying the HTTPS connection from client to app server.

In that case, I don't need to use ssl_bump feature and the ssl_crtd program
for certificate management, is that correct?

Would this config file work to setup the TCP tunnel --

acl localnet src 10.0.0.0/8     # RFC1918 possible internal network
acl SSL_ports port 443
acl Safe_ports port 80          # http
acl Safe_ports port 21          # ftp
acl Safe_ports port 443         # https
acl Safe_ports port 70          # gopher
acl Safe_ports port 210         # wais
acl Safe_ports port 1025-65535  # unregistered ports
acl Safe_ports port 280         # http-mgmt
acl Safe_ports port 488         # gss-http
acl Safe_ports port 591         # filemaker
acl Safe_ports port 777         # multiling http
acl CONNECT method CONNECT
# Deny requests to certain unsafe ports
http_access deny !Safe_ports
# Deny CONNECT to other than secure SSL ports
http_access deny CONNECT !SSL_ports
# Only allow cachemgr access from localhost
http_access allow localhost manager
http_access deny manager
http_access allow localnet
http_access allow localhost
# And finally deny all other access to this proxy
http_access deny all

# Squid normally listens to port 3128
http_port 3128

dns_v4_first on

## Allow server side certificate errors such as untrusted certificates,
otherwise the connection is closed for such errors
sslproxy_cert_error allow all

## Accept certificates that fail verification (should only be needed if
using 'sslproxy_cert_error allow all')
sslproxy_flags DONT_VERIFY_PEER

On Wed, Dec 12, 2018 at 3:49 PM Alex Rousskov <
rousskov at measurement-factory.com> wrote:

> On 12/12/18 12:58 PM, subhish.pillai wrote:
>
>
> > 1. What is the difference between SSL bumping and SSL interception?
>
> These concepts describe activities at different layers:
>
> * SSL bumping is, in Squid context, inspection of SSL traffic that often
> also involves impersonating the origin server and decrypting encrypted
> HTTP traffic (i.e. a MitM attack on the client-server HTTPS communication).
>
> * SSL interception is, in this context, directing (TCP/IP traffic that
> presumably carries) SSL traffic off its "natural" TCP/IP path so that it
> gets to Squid. Interception itself works at protocol layers below SSL
> and HTTP. What happens when the SSL traffic gets to Squid is outside
> "SSL interception" scope.
>
> Usually, folks intercept SSL traffic to bump it, but YMMV. It is
> possible, for example, to simply log TCP-level information about the
> intercepted traffic without any MitM attacks on SSL.
>
>
> > 2. What is the difference between "http_port 3128 intercept" and
> "http_port
> > 3128 transparent"? Do i need to setup the http_port as either of these?
>
> The difference is in whether Squid impersonates the IP client, but you
> need neither because your "clients are explicitly configured to connect
> through the proxy server". You do not need to divert traffic from its
> natural TCP/IP path to proxy it because that natural TCP/IP path already
> goes through your proxy.
>
>
> > 3. Do I need to create self-signed certs on the proxy server and
> distribute
> > it to the client and application server?
>
> * Yes if you want to inspect encrypted HTTP traffic of your client
> application (i.e. get to the HTTP stuff inside the SSL layer).
>
> * Yes if you want client to be able to read Squid-generated error pages.
>
> * No otherwise. In this case, Squid will be just a blind TCP tunnel.
>
>
> What do you want to use Squid for? The answer to that question has a
> significant effect on your Squid configuration.
>
>
> > # And finally deny all other access to this proxy
> > http_access allow all
>
> FWIW, your rule does not match the comment and creates an open proxy.
> Both are bad.
>
>
> HTH,
>
> Alex.
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>


-- 

*Subhish Pillai*

R&D Software Quality Engineer

Broadcom | Brocade Storage Networking

T (720) 462-2900
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20181212/e3006a19/attachment.htm>

From squid3 at treenet.co.nz  Thu Dec 13 01:00:27 2018
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 13 Dec 2018 14:00:27 +1300
Subject: [squid-users] SSL Bump with HTTP Cache Peer Parent
In-Reply-To: <1544656516869-0.post@n4.nabble.com>
References: <1544656516869-0.post@n4.nabble.com>
Message-ID: <76ea091a-afbb-758b-06e5-082c05761597@treenet.co.nz>

On 13/12/18 12:15 pm, sam.handley wrote:
> I am not 100% confident what I am asking is possible but I'd love it to be
> confirmed.
> 
> Here is what our setup would look like, I?ve explained a bit below:
> 
> DEVICE ---> PRX3 (HTTPS CACHE) ---> PRX2 ---> PRX1 ---> INTERNET
> 
> Our current environment is a bit behind the times and inflexible. We have a
> local squid proxy/cache (PRX2) that we do not fully control that only caches
> HTTP content. This proxy is downstream from another proxy which is also HTTP
> (PRX1). Both just TUNNEL HTTPS. PRX1 is the only way out of our WAN to the
> internet.
> 
> We would like to start caching HTTPS (PRX3) because these other proxies are
> not and it is costing us bandwidth. With the config below and a direct
> internet connection I can successfully connect and cache HTTP/S content.
> However, this won?t work in our environment. We must go through a cache peer
> either PRX1 or PRX2, adding either upstream proxy as a cache peer parent
> results in either SSL errors or the request not being forwarded to the peer. 
> 
> I think what I need to do is TUNNEL the bumped request to PRX2 over HTTP. I
> thought squid 4 could do this but can?t find any docs for it so it may have
> been wishful thinking.


No official version of Squid can do that directly for decrypted
requests. Only for HTTPS which is being spliced or non-HTTP protocols
that happen to get intercepted on port 443.

There are several possibilities though:

First is unofficial Factory code that implements this feature. Available
for testing at
<https://github.com/measurement-factory/squid/tree/SQUID-360-peering-for-SslBump>.


Second is that if PRX1 or PRX2 can be configured as a regular TLS proxy
- receiving proxy traffic to a https_port (or any non-Squid software'
equivalent). Then it can be used as a cache_peer with 'ssl' option(s).
This option restricts you to the dangerous client-first style bumping,
but you are already using that.


Third is that the DEVICE is set to pass https:// URLs to PRX2 using
regular TCP connections and HTTP rather than TLS. That leaves the PRX3
able to relay the non-encrypted requests to a peer as-is. Whichever
upstream PRX1 or PRX2 is used will need to do the encryption part before
the traffic hits the Internet.
 Despite the use of TCP connections this is slightly better than the
client-first bumping. At least each agent in the chain is clear and
truthful about the security properties of its connections.


Fourth is a workaround using NAT interception of port 443 and two
proxies. If you setup a PRX-A doing the bump but acting as if it had a
direct network connection. The PRX-B using a https_port with "intercept
ssl-bump" options and doing splice on all traffic and passing the
resulting CONNECT tunnels through to the PRX1 or PRX2.


All these have actually been used by some installations at one point or
another. So there is precedent, though YMMV which works for you.
 I expect #4 can also be done as a variant with SMP workers in PRX3
instead of separate proxies - that I am not aware of anyone doing though.


Amos


From squid3 at treenet.co.nz  Thu Dec 13 01:22:32 2018
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 13 Dec 2018 14:22:32 +1300
Subject: [squid-users] HTTPS proxy setup questions
In-Reply-To: <508b070f-f479-b62f-af45-6ff5a303555b@measurement-factory.com>
References: <1544644695402-0.post@n4.nabble.com>
 <508b070f-f479-b62f-af45-6ff5a303555b@measurement-factory.com>
Message-ID: <6efd26bf-c937-8295-57c1-73de8238af5b@treenet.co.nz>

On 13/12/18 11:49 am, Alex Rousskov wrote:
> On 12/12/18 12:58 PM, subhish.pillai wrote:
> 
>> 2. What is the difference between "http_port 3128 intercept" and "http_port
>> 3128 transparent"? Do i need to setup the http_port as either of these?
> 
> The difference is in whether Squid impersonates the IP client,

No. There is no difference in Squid between the http(s)_port
"transparent" and "intercept" options behaviour.

"transparent" is a deprecated option. Do not use it at all in any
current or recent Squid versions.

Amos


From squid3 at treenet.co.nz  Thu Dec 13 01:53:04 2018
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 13 Dec 2018 14:53:04 +1300
Subject: [squid-users] HTTPS proxy setup questions
In-Reply-To: <CAFODohhZnpb1rROaY5xW5HDDC91wJgfhM_=my7yvBi05v0BJrw@mail.gmail.com>
References: <1544644695402-0.post@n4.nabble.com>
 <508b070f-f479-b62f-af45-6ff5a303555b@measurement-factory.com>
 <CAFODohhZnpb1rROaY5xW5HDDC91wJgfhM_=my7yvBi05v0BJrw@mail.gmail.com>
Message-ID: <44ad4ea0-1864-3567-c0fa-d22aca9f2492@treenet.co.nz>

On 13/12/18 12:50 pm, Subhish Pillai wrote:
> Thanks Alex, that was very helpful.
> 
> Based on your explanation, I just want to use squid as a blind TCP
> tunnel carrying the HTTPS connection from client to app server.?
> 
> In that case, I don't need to use ssl_bump feature and the ssl_crtd
> program for certificate management, is that correct?
> 

Going by the description you gave of the client configuration, it should be.


> Would this config file work to setup the TCP tunnel --

...
> ## Allow server side certificate errors such as untrusted certificates,
> otherwise the connection is closed for such errors
> sslproxy_cert_error allow all
> 
> ## Accept certificates that fail verification (should only be needed if
> using 'sslproxy_cert_error allow all')
> sslproxy_flags DONT_VERIFY_PEER
> 

These sslproxy_* options only apply when Squid is actively performing
TLS to upstream servers. They have no place in the "blind tunnel" situation.
(They also are deprecated in Squid-4, replaced by the
tls_outgoing_options directive
<http://www.squid-cache.org/Doc/config/tls_outgoing_options/>).

If the client software is sending CONNECT requests containing the HTTPS
traffic, then there is absolutely nothing your config needs to do than
let them send those requests to the proxy (as the default config does).

You do not even need Squid to be built with TLS/SSL support. That is the
meaning of "blind" in this setup.

Amos


From squid3 at treenet.co.nz  Thu Dec 13 03:12:00 2018
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 13 Dec 2018 16:12:00 +1300
Subject: [squid-users] SSL Bump with HTTP Cache Peer Parent
In-Reply-To: <9e94ab69-a4d9-455f-790f-9315060abe3e@myriadworks.org>
References: <1544656516869-0.post@n4.nabble.com>
 <76ea091a-afbb-758b-06e5-082c05761597@treenet.co.nz>
 <9e94ab69-a4d9-455f-790f-9315060abe3e@myriadworks.org>
Message-ID: <410c342c-9f6d-51fc-26e4-bf0412b77531@treenet.co.nz>

[ please keep the traffic on-list. If you want private assistance I do
consult for a small fee. ]


On 13/12/18 2:51 pm, Sam Handley wrote:
> On 13/12/18 12:00 pm, Amos Jeffries wrote:
> 
> Thank you for your reply, it seems adding in an extra step could solve it, even if not ideal.
> Just a few questions about your suggestions./Second is that if PRX1 or PRX2 can be configured as a regular TLS proxy
> - receiving proxy traffic to a https_port (or any non-Squid software'
> equivalent). Then it can be used as a cache_peer with 'ssl' option(s). /Would both PRX1 and PRX2 require https_port to be configured?


It would yes. That config setting on the receiving proxy is what makes
it a "TLS proxy" instead of just a proxy.


> We have a limited ability to edit PRX2 and it does not have https_port enabled. /This option restricts you to the dangerous client-first style bumping,
> but you are already using that. /My experience so far is that performing any kind of bump action after step1 results in the connection being TUNNELLED and not cached by PRX1.

The only thing which would result in https:// traffic being cached in
PRX1 is for PRX1 to be doing the decrypt SSL-Bump itself, or acting at a
TLS proxy.

Traffic which has https:// URL scheme should only ever leave Squid over
an encrypted connection. Especially when "bump" is happening.


> For example, 
> 1.
> ssl_bump bump step2 all #results in the connection TUNNELLING and no caching.
> 

Your description earlier was the SSL-Bump and cache happening in PRX3.

PRX1 and PRX2 being regular proxies will see tunnels ... or nothing at all.


> 2.
> ssl_bump peek step2 all
> ssl_bump bump step3 all #results in the connection TUNNELLING and no caching.

IIRC, your lack of a Step-1 action implies tunneling.

The "peek" at Step-2 prohibits "bump" being used at Step-3. So even if
my recollection was incorrect above you get a tunnel from this step-3.


To decrypt and cache the traffic needs to be going through one of these
SSL-Bump sequences:

  bump, terminate, terminate
  peek, bump, terminate
  stare, bump, terminate
  peek, stare, bump
  stare, stare, bump

The terminate are just there as a catch-all for traffic which bump fails
for any reason. You could try splice instead of you want, but that will
also fail sometimes - terminate is the reliable one.


> 
> It would be preferred to use server-first if it did indeed cache the content, can you provide a more ideal bump configuration that will still allow us to cache HTTPS.
> 

For that set of requirements you are limited to the possibility #1, #3
or #4 from the set I wrote up earlier.

To always bump, and with server-first style you need to start with these
ssl_bump rules in PRX3:

  ssl_bump peek step1
  ssl_bump stare
  ssl_bump bump

... adding terminate or splice actions as needed for non-caching of
traffic which has issues with the bumping or you are prohibited from
decrypting.

Amos


From sam at myriadworks.org  Thu Dec 13 05:03:16 2018
From: sam at myriadworks.org (Sam Handley)
Date: Thu, 13 Dec 2018 16:03:16 +1100
Subject: [squid-users] SSL Bump with HTTP Cache Peer Parent
In-Reply-To: <410c342c-9f6d-51fc-26e4-bf0412b77531@treenet.co.nz>
References: <1544656516869-0.post@n4.nabble.com>
 <76ea091a-afbb-758b-06e5-082c05761597@treenet.co.nz>
 <9e94ab69-a4d9-455f-790f-9315060abe3e@myriadworks.org>
 <410c342c-9f6d-51fc-26e4-bf0412b77531@treenet.co.nz>
Message-ID: <9d0b5e7a-17fb-dc61-8bb5-074a5b5f30f9@myriadworks.org>


On 13/12/18 2:12 pm, Amos Jeffries wrote:
> [ please keep the traffic on-list. If you want private assistance I do
> consult for a small fee. ]
>
>
> On 13/12/18 2:51 pm, Sam Handley wrote:
>> On 13/12/18 12:00 pm, Amos Jeffries wrote:
>>
>> Thank you for your reply, it seems adding in an extra step could solve it, even if not ideal.
>> Just a few questions about your suggestions./Second is that if PRX1 or PRX2 can be configured as a regular TLS proxy
>> - receiving proxy traffic to a https_port (or any non-Squid software'
>> equivalent). Then it can be used as a cache_peer with 'ssl' option(s). /Would both PRX1 and PRX2 require https_port to be configured?
>
> It would yes. That config setting on the receiving proxy is what makes
> it a "TLS proxy" instead of just a proxy.
>
>
>> We have a limited ability to edit PRX2 and it does not have https_port enabled. /This option restricts you to the dangerous client-first style bumping,
>> but you are already using that. /My experience so far is that performing any kind of bump action after step1 results in the connection being TUNNELLED and not cached by PRX1.
> The only thing which would result in https:// traffic being cached in
> PRX1 is for PRX1 to be doing the decrypt SSL-Bump itself, or acting at a
> TLS proxy.
>
> Traffic which has https:// URL scheme should only ever leave Squid over
> an encrypted connection. Especially when "bump" is happening.
>
>
>> For example,
>> 1.
>> ssl_bump bump step2 all #results in the connection TUNNELLING and no caching.
>>
> Your description earlier was the SSL-Bump and cache happening in PRX3.
>
> PRX1 and PRX2 being regular proxies will see tunnels ... or nothing at all.
>
>
>> 2.
>> ssl_bump peek step2 all
>> ssl_bump bump step3 all #results in the connection TUNNELLING and no caching.
> IIRC, your lack of a Step-1 action implies tunneling.
>
> The "peek" at Step-2 prohibits "bump" being used at Step-3. So even if
> my recollection was incorrect above you get a tunnel from this step-3.
>
>
> To decrypt and cache the traffic needs to be going through one of these
> SSL-Bump sequences:
>
>    bump, terminate, terminate
>    peek, bump, terminate
>    stare, bump, terminate
>    peek, stare, bump
>    stare, stare, bump
>
> The terminate are just there as a catch-all for traffic which bump fails
> for any reason. You could try splice instead of you want, but that will
> also fail sometimes - terminate is the reliable one.
>
>
>> It would be preferred to use server-first if it did indeed cache the content, can you provide a more ideal bump configuration that will still allow us to cache HTTPS.
>>
> For that set of requirements you are limited to the possibility #1, #3
> or #4 from the set I wrote up earlier.
>
> To always bump, and with server-first style you need to start with these
> ssl_bump rules in PRX3:
>
>    ssl_bump peek step1
>    ssl_bump stare
>    ssl_bump bump
>
> ... adding terminate or splice actions as needed for non-caching of
> traffic which has issues with the bumping or you are prohibited from
> decrypting.
>
> Amos
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users


Sorry, still figuring out how to use the mailing list. I will contact you directly about consultation.

I also may have mixed up the order of my proxies. You are absolutely right.

This seems like it might be the best approach.

   ssl_bump peek step1
   ssl_bump stare step2
   ssl_bump bump step3
   ssl_bump splice spliceACL

Re-reading your options 4 sounds like it may work the best for us. It is certainly the one I understand the most.




From 726mediallc at gmail.com  Thu Dec 13 21:01:38 2018
From: 726mediallc at gmail.com (726mediallc)
Date: Thu, 13 Dec 2018 15:01:38 -0600 (CST)
Subject: [squid-users] a bit off topic. New user question
In-Reply-To: <CAEO0YGAUdewLw0EqEZoPfmFS0PUcZUyYQJKV74z_1+t5eDs-SA@mail.gmail.com>
References: <CAEO0YGAUdewLw0EqEZoPfmFS0PUcZUyYQJKV74z_1+t5eDs-SA@mail.gmail.com>
Message-ID: <1544734898911-0.post@n4.nabble.com>

I have a similar question for my site  https://bestelectricwaterboilers.com
<https://bestelectricwaterboilers.com>   



-----
owner of https://bestelectricwaterboilers.com
--
Sent from: http://squid-web-proxy-cache.1019090.n4.nabble.com/Squid-Users-f1019091.html


From 726mediallc at gmail.com  Thu Dec 13 21:05:57 2018
From: 726mediallc at gmail.com (726mediallc)
Date: Thu, 13 Dec 2018 15:05:57 -0600 (CST)
Subject: [squid-users] a bit off topic. New user question
In-Reply-To: <CAEO0YGAUdewLw0EqEZoPfmFS0PUcZUyYQJKV74z_1+t5eDs-SA@mail.gmail.com>
References: <CAEO0YGAUdewLw0EqEZoPfmFS0PUcZUyYQJKV74z_1+t5eDs-SA@mail.gmail.com>
Message-ID: <1544735157055-0.post@n4.nabble.com>

I have a similar question for my site  https://bestelectricwaterboilers.com
<https://bestelectricwaterboilers.com>   



-----
owner of https://bestelectricwaterboilers.com
--
Sent from: http://squid-web-proxy-cache.1019090.n4.nabble.com/Squid-Users-f1019091.html


From 726mediallc at gmail.com  Thu Dec 13 21:04:16 2018
From: 726mediallc at gmail.com (726mediallc)
Date: Thu, 13 Dec 2018 16:04:16 -0500
Subject: [squid-users] Caching for my site
Message-ID: <5c12ca53.1c69fb81.12768.e5ae@mx.google.com>


Is there an alternative to using WP plugins to speed up caching of my site https://bestelectricwaterboilers.com??

Sent via the Samsung Galaxy S8, an AT&T 4G LTE smartphone
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20181213/bc32e468/attachment.htm>

From squid3 at treenet.co.nz  Thu Dec 13 23:21:31 2018
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Fri, 14 Dec 2018 12:21:31 +1300
Subject: [squid-users] Caching for my site
In-Reply-To: <5c12ca53.1c69fb81.12768.e5ae@mx.google.com>
References: <5c12ca53.1c69fb81.12768.e5ae@mx.google.com>
Message-ID: <90e94b3f-918a-06a7-d4a2-1e8ecf7bab92@treenet.co.nz>

On 14/12/18 10:04 am, 726mediallc wrote:
> 
> Is there an alternative to using WP plugins to speed up caching of my
> site??
> 

Caching is not something that can be "sped up". It either happens or it
does not.

For wordpress sites you can of course write your own plugins or code to
send HTTP caching  headers and respond to. It is far easier and often
better to use something already existing though, if you can find a
plugin that works well.

Use the redbot.org tool to test your site behaviour with any options you
choose. Right now it is reporting that your site is already caching but
not producing correct Vary headers.

Amos


From johnrefwe at mail.com  Fri Dec 14 04:39:48 2018
From: johnrefwe at mail.com (John Refwe)
Date: Fri, 14 Dec 2018 05:39:48 +0100
Subject: [squid-users] HTTPS Settings
Message-ID: <trinity-6b22a897-f5a2-4bc3-84fe-3f7e8713749b-1544762388717@3c-app-mailcom-lxa10>

Hi,

I am writing about assistance with my SSL bump settings.

My squid conf (this is a simple version I'm using to test this issue) looks as follows:
# Leave coredumps in the first cache dir
coredump_dir /usr/local/squid/var/cache/squid

#
# Add any of your own refresh_pattern entries above these.
#
refresh_pattern ^ftp:           1440    20%     10080
refresh_pattern ^gopher:        1440    0%      1440
refresh_pattern -i (/cgi-bin/|\?) 0     0%      0
refresh_pattern .               0       20%     4320

http_access allow all
sslcrtd_children 2 startup=2 idle=1
http_port 3129 ssl-bump generate-host-certificates=on cert=/home/Guyfer/ssl_bump.pem options=NO_SSL_v2 

acl step1 at_step SslBump1
ssl_bump peek step1
ssl_bump bump all


There are a few websites, one of which is https://opts.ssa.gov where I get an error I'm having trouble understanding in the logs.

My browser shows a screen that reads: "Failed to establish a secure connection to 96.43.153.48. The system returned: (71) Protocol error (TLS code: SQUID_ERR_SSL_HANDSHAKE) Handshake with SSL server failed: error:1408E0F4:SSL routines:SSL3_GET_MESSAGE:unexpected message"... The cache logs contains the error "kid1| ERROR: negotiating TLS on FD 14: error:1408E0F4:SSL routines:SSL3_GET_MESSAGE:unexpected message (1/-1/0)"

Now, if I were to modify the ssl bump settings to just be ssl_bump bump all (no peek), things seem to function fine. Am I running into a known limitation of server-first bumping? I have tried this on Squid 4.4 and Squid 4.3.

Thank you for any help, it is much appreciated.

All the best,
John




From squid3 at treenet.co.nz  Fri Dec 14 05:08:25 2018
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Fri, 14 Dec 2018 18:08:25 +1300
Subject: [squid-users] HTTPS Settings
In-Reply-To: <trinity-6b22a897-f5a2-4bc3-84fe-3f7e8713749b-1544762388717@3c-app-mailcom-lxa10>
References: <trinity-6b22a897-f5a2-4bc3-84fe-3f7e8713749b-1544762388717@3c-app-mailcom-lxa10>
Message-ID: <a3955e3d-57e8-6bd2-19e5-083e02750c84@treenet.co.nz>

On 14/12/18 5:39 pm, John Refwe wrote:
> Hi,
> 
> I am writing about assistance with my SSL bump settings.
> 
> My squid conf (this is a simple version I'm using to test this issue) looks as follows:
> # Leave coredumps in the first cache dir
> coredump_dir /usr/local/squid/var/cache/squid
> 
> #
> # Add any of your own refresh_pattern entries above these.
> #
> refresh_pattern ^ftp:           1440    20%     10080
> refresh_pattern ^gopher:        1440    0%      1440
> refresh_pattern -i (/cgi-bin/|\?) 0     0%      0
> refresh_pattern .               0       20%     4320
> 
> http_access allow all
> sslcrtd_children 2 startup=2 idle=1
> http_port 3129 ssl-bump generate-host-certificates=on cert=/home/Guyfer/ssl_bump.pem options=NO_SSL_v2 
> 

FYI: SSLv2 support have been removed completely from Squid-4. That
includes things like "NO_SSL_v2".


> acl step1 at_step SslBump1
> ssl_bump peek step1
> ssl_bump bump all
> 
> 
> There are a few websites, one of which is https://opts.ssa.gov where I get an error I'm having trouble understanding in the logs.
> 
> My browser shows a screen that reads: "Failed to establish a secure connection to 96.43.153.48. The system returned: (71) Protocol error (TLS code: SQUID_ERR_SSL_HANDSHAKE) Handshake with SSL server failed: error:1408E0F4:SSL routines:SSL3_GET_MESSAGE:unexpected message"... The cache logs contains the error "kid1| ERROR: negotiating TLS on FD 14: error:1408E0F4:SSL routines:SSL3_GET_MESSAGE:unexpected message (1/-1/0)"
> 

The weird message is from your OpenSSL library. Apparently the server
being contacted for this transaction is not responding with TLS.


> Now, if I were to modify the ssl bump settings to just be ssl_bump bump all (no peek), things seem to function fine. Am I running into a known limitation of server-first bumping? I have tried this on Squid 4.4 and Squid 4.3.
> 

server-first is more equivalent to bumping at step3. You should use a
"stare" at step2 before bumping for more reliable behaviour. That may
not fix your issue though.


The best way to debug this further is to perform a packet capture of a
test transaction which is failing and look at what the server is sending
to Squid.


Amos


From 726mediallc at gmail.com  Fri Dec 14 05:55:08 2018
From: 726mediallc at gmail.com (726mediallc)
Date: Fri, 14 Dec 2018 00:55:08 -0500
Subject: [squid-users] Caching for my site
In-Reply-To: <90e94b3f-918a-06a7-d4a2-1e8ecf7bab92@treenet.co.nz>
Message-ID: <5c1345c0.1c69fb81.64a2a.b26a@mx.google.com>

Thank you for the quick reply, but what are "Vary headers?"


Sent via the Samsung Galaxy S8, an AT&T 4G LTE smartphone
-------- Original message --------From: Amos Jeffries <squid3 at treenet.co.nz> Date: 12/13/18  6:21 PM  (GMT-05:00) To: squid-users at lists.squid-cache.org Subject: Re: [squid-users] Caching for my site 
On 14/12/18 10:04 am, 726mediallc wrote:
> 
> Is there an alternative to using WP plugins to speed up caching of my
> site??
> 

Caching is not something that can be "sped up". It either happens or it
does not.

For wordpress sites you can of course write your own plugins or code to
send HTTP caching? headers and respond to. It is far easier and often
better to use something already existing though, if you can find a
plugin that works well.

Use the redbot.org tool to test your site behaviour with any options you
choose. Right now it is reporting that your site is already caching but
not producing correct Vary headers.

Amos
_______________________________________________
squid-users mailing list
squid-users at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-users
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20181214/a93cdb0e/attachment.htm>

From squid3 at treenet.co.nz  Fri Dec 14 06:37:05 2018
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Fri, 14 Dec 2018 19:37:05 +1300
Subject: [squid-users] Caching for my site
In-Reply-To: <5c1345c0.1c69fb81.64a2a.b26a@mx.google.com>
References: <5c1345c0.1c69fb81.64a2a.b26a@mx.google.com>
Message-ID: <2fe2e4c6-ce90-9490-e885-b7e13b307099@treenet.co.nz>

On 14/12/18 6:55 pm, 726mediallc wrote:
> Thank you for the quick reply, but what are "Vary headers?"
> 

"Vary" is an HTTP header presented by servers for use by caches. It
tells the cache which parts of a client request message to use when
looking for possible cache entries.

Without it only the URL is used and that prevents HTTP features like
compressed responses, per-user customization of page contents, login vs
non-login views of pages, and user language translations from working
properly.

Technical details at <https://tools.ietf.org/html/rfc7231#section-7.1.4>


PS. since you seem not to already know HTTP the wordpress plugins might
be the best way to start from. See what they do to HTTP messages vs the
HTTP documentation (RFCs 7230 to 7235), with the redbot tool as a way to
get a quick explanation about caching related features they enable.

Amos


From rousskov at measurement-factory.com  Fri Dec 14 16:04:49 2018
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Fri, 14 Dec 2018 09:04:49 -0700
Subject: [squid-users] HTTPS Settings
In-Reply-To: <trinity-6b22a897-f5a2-4bc3-84fe-3f7e8713749b-1544762388717@3c-app-mailcom-lxa10>
References: <trinity-6b22a897-f5a2-4bc3-84fe-3f7e8713749b-1544762388717@3c-app-mailcom-lxa10>
Message-ID: <76802047-75ad-8cb2-48c2-423977aec569@measurement-factory.com>

On 12/13/18 9:39 PM, John Refwe wrote:

> acl step1 at_step SslBump1
> ssl_bump peek step1
> ssl_bump bump all

> There are a few websites, one of which is https://opts.ssa.gov where
> I get an error I'm having trouble understanding in the logs.

Does an OpenSSL s_client test work for that site, from your Squid box?
It works for me, but your environment may be different:

$ openssl s_client --servername opts.ssa.gov --connect opts.ssa.gov:443
GET /


> Am I running into a known limitation of server-first bumping?

Why do you say "server-first bumping"? The Squid configuration you
posted does not use server-first bumping. It uses step2 bumping, which
is a completely different animal.

Collecting a packet sample from the broken transaction (client-Squid and
Squid-server packets, in all four directions), like Amos has suggested,
is a good next step, especially if you cannot reproduce with s_client.

Alex.


From anon.amish at gmail.com  Fri Dec 14 16:26:29 2018
From: anon.amish at gmail.com (Amish)
Date: Fri, 14 Dec 2018 21:56:29 +0530
Subject: [squid-users] note macro - %{policy_}note passes old values along
	with new value
Message-ID: <39130abd-4d18-e284-2c80-80219786576b@gmail.com>

Hello,

I have setup auth_param as follows:

auth_param basic program /usr/lib/squid/user_auth

Where user_auth authenticates user from a database and replies with a 
kv-pair as follows.

OK policy_=onlysomesites

Where policy_ (with underscore) is user policy picked from database.

This policy is passed on to url_rewrite_program via url_rewrite_extras 
as above.

url_rewrite_program /usr/lib/squid/url_rewrite
url_rewrite_extras "policy=%{policy_}note"

url_rewrite reads URL and the policy supplied (policy=onlysomesites) and 
allows or denies URL based on the policy.

Now if I change the policy in database. (to say allsites). Then for new 
requests user_auth returns with:
OK policy_=allsites

Now I was expecting that %{policy_}note will pass on only latest value 
to url_rewrite but somehow it passes old value + new value.

i.e. it passes "policy=onlysomesites,allsites"

How to pass only the latest value with note macro?

Please guide

Thank you

Amish.


From rousskov at measurement-factory.com  Fri Dec 14 17:07:55 2018
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Fri, 14 Dec 2018 10:07:55 -0700
Subject: [squid-users] note macro - %{policy_}note passes old values
 along with new value
In-Reply-To: <39130abd-4d18-e284-2c80-80219786576b@gmail.com>
References: <39130abd-4d18-e284-2c80-80219786576b@gmail.com>
Message-ID: <efed6d2b-d9dd-eb60-6ca8-433a3555a2cd@measurement-factory.com>

On 12/14/18 9:26 AM, Amish wrote:

> url_rewrite_program /usr/lib/squid/url_rewrite
> url_rewrite_extras "policy=%{policy_}note"

> I was expecting that %{policy_}note will pass on only latest value
> to url_rewrite but somehow it passes old value + new value.

Modern[1] Squid versions should remove old same-name annotations when
adding new ones. If you are running a supported version, and your Squid
merges/appends annotations, consider filing a bug report.


> How to pass only the latest value with note macro?

With modern Squids, you should not do anything special to accomplish
that. Only the latest annotation value should be preserved. If that is
not happening in your tests, consider filing a bug report, especially if
you can reproduce with Squid v4+.

[1]
https://github.com/squid-cache/squid/commit/457857fe7cf51037cd9e54e86c0985391d7ea594

Alex.


From subhish.pillai at broadcom.com  Fri Dec 14 19:03:17 2018
From: subhish.pillai at broadcom.com (Subhish Pillai)
Date: Fri, 14 Dec 2018 12:03:17 -0700
Subject: [squid-users] HTTPS proxy setup questions
In-Reply-To: <44ad4ea0-1864-3567-c0fa-d22aca9f2492@treenet.co.nz>
References: <1544644695402-0.post@n4.nabble.com>
 <508b070f-f479-b62f-af45-6ff5a303555b@measurement-factory.com>
 <CAFODohhZnpb1rROaY5xW5HDDC91wJgfhM_=my7yvBi05v0BJrw@mail.gmail.com>
 <44ad4ea0-1864-3567-c0fa-d22aca9f2492@treenet.co.nz>
Message-ID: <CAFODohi9M3pp1iGLmYZp00wrEMA5qomGHsJxrLEyTqmY0ZQeJQ@mail.gmail.com>

I was able to get https working over the http connect tunnel, but I was
still having issues with my client application connecting over the proxy.
After some research it so happens that we have implemented the HTTPS proxy
on the client side with libcurl. (implementing this - "
https://daniel.haxx.se/blog/2016/11/26/https-proxy-with-curl/").

So now my use case for the squid proxy is to be able to accept a
HTTPS_proxy request from the client and tunnel it forward to the
destination server. For that I copied over the CA bundle from the client
into the proxy server and pointed the "tls-cert" option to that file --

https_port 3128 tls-cert=/etc/squid/ssl_cert/ca-bundle.crt

I get the following error when starting the squid server --
*Dec 14 10:03:42 squid[131539]: FATAL: No valid signing certificate
configured for HTTPS_port [::]:3128*

How do I get this to work without having to create self-signed certs on the
proxy server and importing that into the client ca-bundle.

Am I missing any config steps in the squid.conf file? I think I have
misunderstood the function of the "tls-cert" and "tls-cafile" options in
the https_port line.

Thanks again for all the help!

Subhish


On Wed, Dec 12, 2018 at 6:53 PM Amos Jeffries <squid3 at treenet.co.nz> wrote:

> On 13/12/18 12:50 pm, Subhish Pillai wrote:
> > Thanks Alex, that was very helpful.
> >
> > Based on your explanation, I just want to use squid as a blind TCP
> > tunnel carrying the HTTPS connection from client to app server.
> >
> > In that case, I don't need to use ssl_bump feature and the ssl_crtd
> > program for certificate management, is that correct?
> >
>
> Going by the description you gave of the client configuration, it should
> be.
>
>
> > Would this config file work to setup the TCP tunnel --
>
> ...
> > ## Allow server side certificate errors such as untrusted certificates,
> > otherwise the connection is closed for such errors
> > sslproxy_cert_error allow all
> >
> > ## Accept certificates that fail verification (should only be needed if
> > using 'sslproxy_cert_error allow all')
> > sslproxy_flags DONT_VERIFY_PEER
> >
>
> These sslproxy_* options only apply when Squid is actively performing
> TLS to upstream servers. They have no place in the "blind tunnel"
> situation.
> (They also are deprecated in Squid-4, replaced by the
> tls_outgoing_options directive
> <http://www.squid-cache.org/Doc/config/tls_outgoing_options/>).
>
> If the client software is sending CONNECT requests containing the HTTPS
> traffic, then there is absolutely nothing your config needs to do than
> let them send those requests to the proxy (as the default config does).
>
> You do not even need Squid to be built with TLS/SSL support. That is the
> meaning of "blind" in this setup.
>
> Amos
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>


-- 

*Subhish Pillai*

R&D Software Quality Engineer

Broadcom | Brocade Storage Networking

T (720) 462-2900
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20181214/d10f353b/attachment.htm>

From russel_mcdonald at swbell.net  Fri Dec 14 19:52:07 2018
From: russel_mcdonald at swbell.net (Russel McDonald)
Date: Fri, 14 Dec 2018 19:52:07 +0000 (UTC)
Subject: [squid-users] Errors Compiling Squid Proxy on Windows using Cygwin
References: <2078690432.432558.1544817127712.ref@mail.yahoo.com>
Message-ID: <2078690432.432558.1544817127712@mail.yahoo.com>


Hi,

?I?ve followed the small set of steps for building squidproxy using Cygwin on Windows 7 but am getting compile time errors.Specifically the below. Sorry for emailing this question if this has beenanswered. I searched past emails and the most recent hit on errors building onWindows was from 2010.

?

Cygwin version 2.11.2 from Nov 8/2018 and installed ALLfeatures including devel.

Downloaded squid source, tar ?xvf ?squid-3.5.11.tar.gz (I tried version 3.5.28 as well)

./configure --bindir=/bin/squid --sbindir=/usr/sbin/squid--sysconfdir=/etc/squid??? --datadir=/usr/share/squid--libexecdir=/usr/lib/squid --disable-strict-error-checking???--with-logdir=/var/log/squid --with-swapdir=/var/cache/squid

And tried with --enable-win32-service as well.

?

Then make produces the error below. What am I doing incorrectly?

??.

?.

In file included from /usr/include/w32api/lm.h:18:0,

????????????????from ext_lm_group_acl.cc:96:

/usr/include/w32api/lmserver.h:30:33: error:`SERVICE_STATUS_HANDLE' was not dec

lared in this scope

?? WINBOOL WINAPISetServiceBits(SERVICE_STATUS_HANDLE hServiceStatus,DWORD dwSe

rviceBits,WINBOOL bSetBitsOn,WINBOOL bUpdateImmediately);

????????????????????????????????^~~~~~~~~~~~~~~~~~~~~

/usr/include/w32api/lmserver.h:30:33: note: suggestedalternative: `SERVICE_PAUS

ABLE'

?? WINBOOL WINAPISetServiceBits(SERVICE_STATUS_HANDLE hServiceStatus,DWORD dwSe

rviceBits,WINBOOL bSetBitsOn,WINBOOL bUpdateImmediately);

????????????????????????????????^~~~~~~~~~~~~~~~~~~~~

????????????????????????????????SERVICE_PAUSABLE

/usr/include/w32api/lmserver.h:30:76: error: expectedprimary-expression before

`dwServiceBits'

?? WINBOOL WINAPISetServiceBits(SERVICE_STATUS_HANDLE hServiceStatus,DWORD dwSe

rviceBits,WINBOOL bSetBitsOn,WINBOOL bUpdateImmediately);

???????????????????????????????????????????????????????????????????????????^~~~

~~~~~~~~~

/usr/include/w32api/lmserver.h:30:98: error: expectedprimary-expression before

`bSetBitsOn'

?? WINBOOL WINAPISetServiceBits(SERVICE_STATUS_HANDLE hServiceStatus,DWORD dwSe

rviceBits,WINBOOL bSetBitsOn,WINBOOL bUpdateImmediately);

?

?????????????????^~~~~~~~~~

/usr/include/w32api/lmserver.h:30:117: error: expectedprimary-expression before

`bUpdateImmediately'

?? WINBOOL WINAPISetServiceBits(SERVICE_STATUS_HANDLE hServiceStatus,DWORD dwSe

rviceBits,WINBOOL bSetBitsOn,WINBOOL bUpdateImmediately);

?

????????????????????????????????????^~~~~~~~~~~~~~~~~~

/usr/include/w32api/lmserver.h:30:135: error: expressionlist treated as compoun

d expression in initializer [-fpermissive]

?? WINBOOL WINAPISetServiceBits(SERVICE_STATUS_HANDLE hServiceStatus,DWORD dwSe
rviceBits,WINBOOL bSetBitsOn,WINBOOL bUpdateImmediately);
Thanks!
Russel McDonald
?

?
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20181214/635f2dac/attachment.htm>

From rousskov at measurement-factory.com  Fri Dec 14 21:33:37 2018
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Fri, 14 Dec 2018 14:33:37 -0700
Subject: [squid-users] HTTPS proxy setup questions
In-Reply-To: <CAFODohi9M3pp1iGLmYZp00wrEMA5qomGHsJxrLEyTqmY0ZQeJQ@mail.gmail.com>
References: <1544644695402-0.post@n4.nabble.com>
 <508b070f-f479-b62f-af45-6ff5a303555b@measurement-factory.com>
 <CAFODohhZnpb1rROaY5xW5HDDC91wJgfhM_=my7yvBi05v0BJrw@mail.gmail.com>
 <44ad4ea0-1864-3567-c0fa-d22aca9f2492@treenet.co.nz>
 <CAFODohi9M3pp1iGLmYZp00wrEMA5qomGHsJxrLEyTqmY0ZQeJQ@mail.gmail.com>
Message-ID: <d15c0a01-0b8c-4d01-7c92-bdb1b45967a9@measurement-factory.com>

On 12/14/18 12:03 PM, Subhish Pillai wrote:

> my use case for the squid proxy is to be able to accept a
> HTTPS_proxy request from the client and tunnel it forward to the
> destination server. 

> How do I get this to work without having to create self-signed certs on
> the proxy server and importing that into the client ca-bundle. 

Get a server certificate from a CA authority that the client trusts,
issued for the Squid proxy domain. Give Squid that certificate. For
example, you may be able to use a free letsencrypt.org CA.

An HTTPS proxy needs a certificate it can sign its traffic with. That
certificate must be issued by a client-trusted CA. Whether that is a
fake CA that you operate (what you may have referred to as a
"self-signed cert" above) or a real CA trusted by millions of other
clients (e.g., letsencrypt), is your choice.


> For that I copied over the CA bundle from the client
> into the proxy server and pointed the "tls-cert" option to that file

Why? Please suggest specific documentation changes that would remove the
implication that doing the above has something to do with your goals.
That option is for specifying the signing certificate (i.e. the
certificate the proxy is going to sign traffic with).


> Am I missing any config steps in the squid.conf file?

You are missing a clientca or tls-cafile option that triggers client
certificate request (from Squid to client) and gives Squid CA
certificates to trust when validating the client-supplied certificate.
This is unrelated to the Squid signing certificate discussed above.

Alex.


> On Wed, Dec 12, 2018 at 6:53 PM Amos Jeffries <squid3 at treenet.co.nz
> <mailto:squid3 at treenet.co.nz>> wrote:
> 
>     On 13/12/18 12:50 pm, Subhish Pillai wrote:
>     > Thanks Alex, that was very helpful.
>     >
>     > Based on your explanation, I just want to use squid as a blind TCP
>     > tunnel carrying the HTTPS connection from client to app server.?
>     >
>     > In that case, I don't need to use ssl_bump feature and the ssl_crtd
>     > program for certificate management, is that correct?
>     >
> 
>     Going by the description you gave of the client configuration, it
>     should be.
> 
> 
>     > Would this config file work to setup the TCP tunnel --
> 
>     ...
>     > ## Allow server side certificate errors such as untrusted
>     certificates,
>     > otherwise the connection is closed for such errors
>     > sslproxy_cert_error allow all
>     >
>     > ## Accept certificates that fail verification (should only be
>     needed if
>     > using 'sslproxy_cert_error allow all')
>     > sslproxy_flags DONT_VERIFY_PEER
>     >
> 
>     These sslproxy_* options only apply when Squid is actively performing
>     TLS to upstream servers. They have no place in the "blind tunnel"
>     situation.
>     (They also are deprecated in Squid-4, replaced by the
>     tls_outgoing_options directive
>     <http://www.squid-cache.org/Doc/config/tls_outgoing_options/>).
> 
>     If the client software is sending CONNECT requests containing the HTTPS
>     traffic, then there is absolutely nothing your config needs to do than
>     let them send those requests to the proxy (as the default config does).
> 
>     You do not even need Squid to be built with TLS/SSL support. That is the
>     meaning of "blind" in this setup.
> 
>     Amos
>     _______________________________________________
>     squid-users mailing list
>     squid-users at lists.squid-cache.org
>     <mailto:squid-users at lists.squid-cache.org>
>     http://lists.squid-cache.org/listinfo/squid-users
> 
> 
> 
> -- 
> 
> *Subhish Pillai*
> 
> R&D Software Quality Engineer
> 
> Broadcom | Brocade Storage Networking
> 
> T?(720) 462-2900
> 
> 
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
> 



From anon.amish at gmail.com  Fri Dec 14 23:26:32 2018
From: anon.amish at gmail.com (Amish)
Date: Sat, 15 Dec 2018 04:56:32 +0530
Subject: [squid-users] note macro - %{policy_}note passes old values
 along with new value
In-Reply-To: <efed6d2b-d9dd-eb60-6ca8-433a3555a2cd@measurement-factory.com>
References: <39130abd-4d18-e284-2c80-80219786576b@gmail.com>
 <efed6d2b-d9dd-eb60-6ca8-433a3555a2cd@measurement-factory.com>
Message-ID: <a2612076-9b59-b7de-0b94-d7f2011af222@gmail.com>



On 14/12/18 10:37 pm, Alex Rousskov wrote:
> On 12/14/18 9:26 AM, Amish wrote:
>
>> url_rewrite_program /usr/lib/squid/url_rewrite
>> url_rewrite_extras "policy=%{policy_}note"
>> I was expecting that %{policy_}note will pass on only latest value
>> to url_rewrite but somehow it passes old value + new value.
> Modern[1] Squid versions should remove old same-name annotations when
> adding new ones. If you are running a supported version, and your Squid
> merges/appends annotations, consider filing a bug report.
>

Sorry that I forgot to mention squid version.

But I am using squid 4.4 (Arch Linux)

>> How to pass only the latest value with note macro?
> With modern Squids, you should not do anything special to accomplish
> that. Only the latest annotation value should be preserved. If that is
> not happening in your tests, consider filing a bug report, especially if
> you can reproduce with Squid v4+.
>
> [1]
> https://github.com/squid-cache/squid/commit/457857fe7cf51037cd9e54e86c0985391d7ea594

I have not set any clt_conn_tag=TAG.

Does this feature of latest note value work only if I set clt_conn_tag=TAG?

If yes, do you mean I need to set clt_conn_tag=policy_?

Will file bug report if answer is no.

Thanks

Amish.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20181215/3613e719/attachment.htm>

From rousskov at measurement-factory.com  Fri Dec 14 23:57:38 2018
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Fri, 14 Dec 2018 16:57:38 -0700
Subject: [squid-users] note macro - %{policy_}note passes old values
 along with new value
In-Reply-To: <a2612076-9b59-b7de-0b94-d7f2011af222@gmail.com>
References: <39130abd-4d18-e284-2c80-80219786576b@gmail.com>
 <efed6d2b-d9dd-eb60-6ca8-433a3555a2cd@measurement-factory.com>
 <a2612076-9b59-b7de-0b94-d7f2011af222@gmail.com>
Message-ID: <696c2b06-6ca0-b0e6-5d5b-82a86703640c@measurement-factory.com>

On 12/14/18 4:26 PM, Amish wrote:
> On 14/12/18 10:37 pm, Alex Rousskov wrote:
>> On 12/14/18 9:26 AM, Amish wrote:

>>> How to pass only the latest value with note macro?

>> With modern Squids, you should not do anything special to accomplish
>> that. Only the latest annotation value should be preserved. If that is
>> not happening in your tests, consider filing a bug report, especially if
>> you can reproduce with Squid v4+.

>> https://github.com/squid-cache/squid/commit/457857fe7cf51037cd9e54e86c0985391d7ea594

> I have not set any clt_conn_tag=TAG.
> 
> Does this feature of latest note value work only if I set clt_conn_tag=TAG?

No, the two are pretty much unrelated. The always-erase-the-old-value
fix (not really a feature!) in the above-referenced commit should apply
to all annotations, not just client connection annotations. IIRC, our
connection annotation work simply exposed the fact that we screwed up
with annotation updates earlier, and we fixed that bug in the same project.

Alex.


From anon.amish at gmail.com  Sat Dec 15 01:03:43 2018
From: anon.amish at gmail.com (Amish)
Date: Sat, 15 Dec 2018 06:33:43 +0530
Subject: [squid-users] note macro - %{policy_}note passes old values
 along with new value
In-Reply-To: <696c2b06-6ca0-b0e6-5d5b-82a86703640c@measurement-factory.com>
References: <39130abd-4d18-e284-2c80-80219786576b@gmail.com>
 <efed6d2b-d9dd-eb60-6ca8-433a3555a2cd@measurement-factory.com>
 <a2612076-9b59-b7de-0b94-d7f2011af222@gmail.com>
 <696c2b06-6ca0-b0e6-5d5b-82a86703640c@measurement-factory.com>
Message-ID: <27dde56c-da71-fecf-65fb-476adbe3ab99@gmail.com>



On 15/12/18 5:27 am, Alex Rousskov wrote:
>>> With modern Squids, you should not do anything special to accomplish
>>> that. Only the latest annotation value should be preserved. If that is
>>> not happening in your tests, consider filing a bug report, especially if
>>> you can reproduce with Squid v4+.
>>> https://github.com/squid-cache/squid/commit/457857fe7cf51037cd9e54e86c0985391d7ea594
> No, the two are pretty much unrelated. The always-erase-the-old-value
> fix (not really a feature!) in the above-referenced commit should apply
> to all annotations, not just client connection annotations. IIRC, our
> connection annotation work simply exposed the fact that we screwed up
> with annotation updates earlier, and we fixed that bug in the same project.
>
> Alex.

Bug report with steps to reproduce filed:

https://bugs.squid-cache.org/show_bug.cgi?id=4912

Amish


From squid3 at treenet.co.nz  Mon Dec 17 01:36:39 2018
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Mon, 17 Dec 2018 14:36:39 +1300
Subject: [squid-users] Errors Compiling Squid Proxy on Windows using
 Cygwin
In-Reply-To: <2078690432.432558.1544817127712@mail.yahoo.com>
References: <2078690432.432558.1544817127712.ref@mail.yahoo.com>
 <2078690432.432558.1544817127712@mail.yahoo.com>
Message-ID: <f5e09bef-df4f-567b-e4ae-f564dc29e409@treenet.co.nz>

On 15/12/18 8:52 am, Russel McDonald wrote:
> Hi,
> 
> ?I?ve followed the small set of steps for building squid proxy using
> Cygwin on Windows 7 but am getting compile time errors. Specifically the
> below. Sorry for emailing this question if this has been answered. I
> searched past emails and the most recent hit on errors building on
> Windows was from 2010.
> 
> ?
> 
> Cygwin version 2.11.2 from Nov 8/2018 and installed ALL features
> including devel.
> 
> Downloaded squid source, tar ?xvf ?squid-3.5.11.tar.gz (I tried version
> 3.5.28 as well)
> 
> ./configure --bindir=/bin/squid --sbindir=/usr/sbin/squid
> --sysconfdir=/etc/squid??? --datadir=/usr/share/squid
> --libexecdir=/usr/lib/squid --disable-strict-error-checking???
> --with-logdir=/var/log/squid --with-swapdir=/var/cache/squid
> 
> And tried with --enable-win32-service as well.
> 
> ?
> 
> Then make produces the error below. What am I doing incorrectly?
> 

Your Squid is detecting the existence of Windows OS headers and trying
to build a helper which has bugs in your environment.

This type of problem is why the Diladele Cygwin instructions (and other
OS cross-building) list the helpers to build explicitly whereas native
builds can just use the auto-detect by ./configure. Doing the same for
your build will both avoid this issue and reduce the size of your Squid
build.


Please open a bug report in our bugzilla about this so it does not get
forgotten. I am not aware of anyone working on Windows bugs right now so
no ETA on when it might get fixed.

Amos


From rafael.akchurin at diladele.com  Mon Dec 17 07:03:16 2018
From: rafael.akchurin at diladele.com (Rafael Akchurin)
Date: Mon, 17 Dec 2018 07:03:16 +0000
Subject: [squid-users] Errors Compiling Squid Proxy on Windows using
 Cygwin
In-Reply-To: <f5e09bef-df4f-567b-e4ae-f564dc29e409@treenet.co.nz>
References: <2078690432.432558.1544817127712.ref@mail.yahoo.com>
 <2078690432.432558.1544817127712@mail.yahoo.com>
 <f5e09bef-df4f-567b-e4ae-f564dc29e409@treenet.co.nz>
Message-ID: <AM0PR04MB47537140AABB640D8CBAC0778FBC0@AM0PR04MB4753.eurprd04.prod.outlook.com>

Hello Russel, Amos, all list members,

The 3.5 compilation instructions are at https://docs.diladele.com/howtos/build_squid_windows/index.html
And we are still planning to allocate task forces to compile Squid 4 for Cygwin after initial failures ?

@Russel - see our project at - fork it if you have patches/improvements! At https://github.com/diladele/squid-windows

Best regards,
Rafael Akchurin
Diladele B.V.

-----Original Message-----
From: squid-users <squid-users-bounces at lists.squid-cache.org> On Behalf Of Amos Jeffries
Sent: Monday, 17 December 2018 02:37
To: squid-users at lists.squid-cache.org
Subject: Re: [squid-users] Errors Compiling Squid Proxy on Windows using Cygwin

On 15/12/18 8:52 am, Russel McDonald wrote:
> Hi,
> 
> ?I?ve followed the small set of steps for building squid proxy using 
> Cygwin on Windows 7 but am getting compile time errors. Specifically 
> the below. Sorry for emailing this question if this has been answered. 
> I searched past emails and the most recent hit on errors building on 
> Windows was from 2010.
> 
> ?
> 
> Cygwin version 2.11.2 from Nov 8/2018 and installed ALL features 
> including devel.
> 
> Downloaded squid source, tar ?xvf ?squid-3.5.11.tar.gz (I tried 
> version
> 3.5.28 as well)
> 
> ./configure --bindir=/bin/squid --sbindir=/usr/sbin/squid 
> --sysconfdir=/etc/squid??? --datadir=/usr/share/squid 
> --libexecdir=/usr/lib/squid --disable-strict-error-checking 
> --with-logdir=/var/log/squid --with-swapdir=/var/cache/squid
> 
> And tried with --enable-win32-service as well.
> 
> ?
> 
> Then make produces the error below. What am I doing incorrectly?
> 

Your Squid is detecting the existence of Windows OS headers and trying to build a helper which has bugs in your environment.

This type of problem is why the Diladele Cygwin instructions (and other OS cross-building) list the helpers to build explicitly whereas native builds can just use the auto-detect by ./configure. Doing the same for your build will both avoid this issue and reduce the size of your Squid build.


Please open a bug report in our bugzilla about this so it does not get forgotten. I am not aware of anyone working on Windows bugs right now so no ETA on when it might get fixed.

Amos
_______________________________________________
squid-users mailing list
squid-users at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-users

From rafael.akchurin at diladele.com  Mon Dec 17 12:48:37 2018
From: rafael.akchurin at diladele.com (Rafael Akchurin)
Date: Mon, 17 Dec 2018 12:48:37 +0000
Subject: [squid-users] [icap] Release Candidate of Web Safety 7.0 web filter
	for Squid proxy
Message-ID: <AM0PR04MB4753F2ECB66162B8EA9E69908FBC0@AM0PR04MB4753.eurprd04.prod.outlook.com>

Hello everyone,

The release candidate build of Web Safety ICAP web filter for Squid proxy (version 7.0.0.8768 built on December 14, 2018) is now available for download.
This version contains the following breaking changes, fixes and improvements:


  *   Base platform moved to Ubuntu 18 LTS
  *   Required Python version is now 3 instead of 2.
  *   Required Django version is now 2.1.2 instead of 1.11.
  *   Squid proxy is now version 4.4.
  *   FreeBSD 11, pfSense 2.4 and Raspbian 9 builds are available (status is still experimental)

Pre-configured virtual appliances for VMware ESXi/vSphere and Microsoft Hyper-V are available from https://www.diladele.com/download_next_version.html.
GitHub repo with automation scripts we used to build this virtual appliance from stock Ubuntu 18 LTS image is at https://github.com/diladele/websafety-virtual-appliance/tree/release-7.0.0/scripts.ubuntu18

Direct links to virtual appliances:


  *   http://packages.diladele.com/websafety/7.0.0.8768/va/ubuntu18/websafety.zip
  *   http://packages.diladele.com/websafety/7.0.0.8768/va/ubuntu18/websafety-hyperv.zip

Virtual appliance for Microsoft Azure is available from Azure Marketplace at https://azuremarketplace.microsoft.com/en-us/marketplace/apps/diladele.websafety?tab=Overview
Corresponding deployment guide is at https://docs.diladele.com/tutorials/proxy_in_microsoft_azure/index.html.

Virtual appliance for Amazon AWS is published at https://aws.amazon.com/marketplace/pp/B07KJHLHKC?qid=1542298277826&sr=0-1&ref_=srh_res_product_title with corresponding deployment guide at https://docs.diladele.com/tutorials/web_filter_amazon_aws/index.html

Your questions/issues/bugs are welcome at support at diladele.com

The final release is planned at the end of January 2019.
Thanks to all of you for making this possible!

Best regards,
Rafael Akchurin
Diladele B.V.

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20181217/d0f28e03/attachment.htm>

From darvin at reduc.edu.cu  Mon Dec 17 17:24:25 2018
From: darvin at reduc.edu.cu (Darvin Rivera Aguilar)
Date: Mon, 17 Dec 2018 12:24:25 -0500
Subject: [squid-users] Redirect youtube
Message-ID: <af105103-445c-cc01-eaac-0ca502395a0e@reduc.edu.cu>

Hello
I have Debian 9 and squid from oficial debian repo (Squid Cache: Version 
3.5.23)
It's posible to redirect https://www.youtube.com to 
https://m.youtube.com for all users in squid?
---"Convencion Internacional Cientifica y Tecnologica de la Universidad de Camaguey Ignacio Agramonte Loynaz" del 21 al 24 de mayo del 2019 Sede: Cayo Santa Maria, Cuba  http://cict.reduc.edu.cu/

---Universidad de Camaguey "Ignacio Agramonte Loynaz", Cuba http://www.reduc.edu.cu/


From squid3 at treenet.co.nz  Tue Dec 18 05:13:18 2018
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Tue, 18 Dec 2018 18:13:18 +1300
Subject: [squid-users] Redirect youtube
In-Reply-To: <af105103-445c-cc01-eaac-0ca502395a0e@reduc.edu.cu>
References: <af105103-445c-cc01-eaac-0ca502395a0e@reduc.edu.cu>
Message-ID: <27e031f3-0bad-bf99-78e1-0116771ec0c7@treenet.co.nz>

On 18/12/18 6:24 am, Darvin Rivera Aguilar wrote:
> Hello
> I have Debian 9 and squid from oficial debian repo (Squid Cache: Version
> 3.5.23)
> It's posible to redirect https://www.youtube.com to
> https://m.youtube.com for all users in squid?


Yes. However the Debian official packages do not provide support for
SSL-Bump features. So not with that particular Squid package you
currently have installed.


If you are needing to intercept that HTTPS traffic you will have to
rebuild Squid with SSL-Bump features. Instructions for re-building the
Debian package with OpenSSL are at
<https://wiki.squid-cache.org/KnowledgeBase/Debian>.
(Please use v4 instead of v3.5 for any new installations. Since you are
custom building it is a good idea to build the latest version to get the
most fixes and longest support times.)


If you are using Squid just as a TLS proxy the Squid v4 packages from
Debian Testing/Buster should be fine. There are some issues with TLS
fine tuning still being worked out, but that is separate from this
use-case of yours. Worst-case you can rebuild with OpenSSL as above.


Provided you actually use a proper *redirect* rather than a URL-rewrite
it should work fine:

 acl www_youtube dstdomain www.youtube.com
 deny_info 302:https://m.youtube.com%R www_youtube
 http_access deny www_youtube


Amos


From m.hoffmann.bs at gmail.com  Tue Dec 18 14:44:40 2018
From: m.hoffmann.bs at gmail.com (Martin Hoffmann)
Date: Tue, 18 Dec 2018 15:44:40 +0100
Subject: [squid-users] Squid4 with GnuTLS - specify ciphers or disable
	protocols
In-Reply-To: <84bfd973-055d-cdcc-bb1b-9059f0d4112f@treenet.co.nz>
References: <mailman.3.1541851201.4312.squid-users@lists.squid-cache.org>
 <CADpL81bKoNSQqsXyCkgWKCyL0kOpqgYGkVea-zxNWG2Aoe6BsQ@mail.gmail.com>
 <a9578d84-5b0f-b953-8616-e5651a7433ea@treenet.co.nz>
 <CADpL81as3hyEnOHWfaYTDKj+GPA9s+P-9ky9OV2CxaTXK-i5_g@mail.gmail.com>
 <84bfd973-055d-cdcc-bb1b-9059f0d4112f@treenet.co.nz>
Message-ID: <CADpL81ZCtgLMc6S+W-bpCGvZj2wA_dzZjQkQpJiDBRY1=wuCQA@mail.gmail.com>

Thanks that would be fine.
However meanwhile I have recompiled squid 4.4 with OpenSSL support
(added --enable-ssl
and --with-open-ssl=xxx  and removed --with-gnutls to debian/rules) just to
end with the same problems - I cannot seem to find how to disable certain
protocols or ciphers with squid 4.4.
With squid 3.3 / 3.5 it worked without problems with "https_port ...
cipher=ALL:!xxx options=NO_TLSv1,....". However despite of the docs
<http://www.squid-cache.org/Doc/config/http_port/> saying these options
should still work Squid4.4 just exits with Error:

FATAL: Unknown https_port option 'cipher=
FATAL: Unknown https_port option 'options=

This seems to be the case regardless if I compile it with OpenSSL support
or GnuTLS Support or both. Btw. How does Squid "know" which library to
chose if it's compiled with both libraries?

So what exactly am I missing here? Is the docs simply wrong? Or outdated?
Which exact keyword should set the OpenSSL ciphers? Which one should set
the GnuTLS priority strings? Is it the same keyword with different values??

I have then experimented with e.g. *tls-options=NO_TLSv1* setting in
Squid4.4 with OpenSSL but without any luck:

FATAL: Unknown TLS option 'NO_TLSv1'

So please could anyone provide a proved working example for disabling TLS
v1 or any Cipher in Squid 4.4? Either OpenSSL or GnuTLS would suffice to
bring me back on the right track.

Thanks in advance,

Martin

Am Di., 18. Dez. 2018 um 07:46 Uhr schrieb Amos Jeffries <
squid3 at treenet.co.nz>:

> On 18/12/18 3:57 am, Martin Hoffmann wrote:
> > Sorry for my late response, but I have been very busy the last weeks.
> > So I could finally find the time to patch my Squid 4.4 with your Patch
> > https://github.com/squid-cache/squid/pull/330
> >
>
> No worries, similar situation here.
>
> > However running patched squid with the following config still does
> > ignore all TLS specific settings (tls-options and tls-min-version):
> >
> > https_port 1.2.3.4:443 <http://1.2.3.4:443> tls-cert=/path/cert.crt
> > tls-key=/path/cert.key tls-dh=/path/dhparams.pem tls-min-version=1.2
> > accel defaultsite=some.domain.de <http://some.domain.de>
> >
> >
> > All attempts to disable certain ciphers or TLS version via
> > 'tls-options=SECURE128:+SECURE192:-VERS-ALL:+VERS-TLS1.2' also fails -
> > no change at all. It is as if squid totally ignores all GnuTLS specific
> > settings...? Is there still another bug regarding config?
> >
>
> Just the unhelpful "Hmm, thats odd". I intend to re-test all this in the
> next month or so to be able to give a better indication of what to
> expect working and see if any other regressions show up.
>
> Sorry,
> Amos
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20181218/eb9c0cb0/attachment.htm>

From mike.quentel.rbc at gmail.com  Tue Dec 18 15:40:38 2018
From: mike.quentel.rbc at gmail.com (Mike Quentel)
Date: Tue, 18 Dec 2018 10:40:38 -0500
Subject: [squid-users] squid-users Digest, Vol 52, Issue 13
In-Reply-To: <mailman.4981.1544569699.3019.squid-users@lists.squid-cache.org>
References: <mailman.4981.1544569699.3019.squid-users@lists.squid-cache.org>
Message-ID: <CAE53ZLAo70jTRqfVfbUSMUp1Y6AwbMPJbdUgWzBoAAhsBN3opg@mail.gmail.com>

Many thanks Enrico and Amos for the advice you each shared. I have
incorporated the suggested changes into squid.conf and almost have the
desired results in Squid.

Squid is successfully blocking access to TLS sites by an IP address,
but not the same sites using the domain names.

For example, Squid blocking the IP address (taken from nslookup) of
www.google.com successfully works (ERR_ACCESS_DENIED):
https://172.217.1.4

But, attempting to access https://www.google.com will still download
the page (200).

How can I force Squid to block the TLS FQDN versions of web sites that
are not in the white list?

This is the updated squid.conf:

---

visible_hostname squid

http_port 3129 intercept

sslcrtd_children 10

acl CONNECT method CONNECT

acl url_domains dstdomain .amazonaws.com
acl url_domains dstdomain .docker.io
acl url_domains dstdomain .docker.com
acl url_domains dstdomain .congiu.com

https_port 3130 ssl-bump intercept generate-host-certificates=on
dynamic_cert_mem_cache_size=100MB cert=/etc/squid/squid.pem
acl SSL_ports port 443
http_access allow SSL_ports

acl tls_servers ssl::server_name .amazonaws.com
acl tls_servers ssl::server_name .docker.io
acl tls_servers ssl::server_name .docker.com
acl tls_servers ssl::server_name .congiu.com
acl tls_servers ssl::server_name .fedoraproject.org
acl tls_servers ssl::server_name mirror.csclub.uwaterloo.ca
acl tls_servers ssl::server_name .sumologic.com

acl step1 at_step SslBump1
acl step2 at_step SslBump2
acl step3 at_step SslBump3

always_direct allow url_domains
sslproxy_cert_error allow all

http_access deny CONNECT !SSL_Ports
http_access allow url_domains
http_access allow tls_servers
http_access deny all

cache deny all

ssl_bump peek step1 all
ssl_bump peek step2 tls_servers
ssl_bump splice step3 tls_servers
ssl_bump stare step2
ssl_bump bump step3
ssl_bump terminate step2 all

# debug_options ALL,1 80,5
debug_options ALL,1 33,4

---

Thanks, Mike Quentel

On Tue, 11 Dec 2018 at 18:08, <squid-users-request at lists.squid-cache.org> wrote:
>
> Send squid-users mailing list submissions to
>         squid-users at lists.squid-cache.org
>
> To subscribe or unsubscribe via the World Wide Web, visit
>         http://lists.squid-cache.org/listinfo/squid-users
> or, via email, send a message with subject or body 'help' to
>         squid-users-request at lists.squid-cache.org
>
> You can reach the person managing the list at
>         squid-users-owner at lists.squid-cache.org
>
> When replying, please edit your Subject line so it is more specific
> than "Re: Contents of squid-users digest..."
>
>
> Today's Topics:
>
>    1. SslBump Peek and Splice using Squid-4.1-5 in Amazon1      Linux
>       with Squid Helpers (Mike Quentel)
>    2. Re: SslBump Peek and Splice using Squid-4.1-5 in  Amazon1
>       Linux with Squid Helpers (Enrico Heine)
>    3. Re: SslBump Peek and Splice using Squid-4.1-5 in Amazon1
>       Linux with Squid Helpers (Amos Jeffries)
>
>
> ----------------------------------------------------------------------
>
> Message: 1
> Date: Tue, 11 Dec 2018 10:41:56 -0500
> From: Mike Quentel <mike.quentel.rbc at gmail.com>
> To: squid-users at lists.squid-cache.org
> Subject: [squid-users] SslBump Peek and Splice using Squid-4.1-5 in
>         Amazon1 Linux with Squid Helpers
> Message-ID:
>         <CAE53ZLD9oqt0cUKsrwHSDo8nDEo2Midty0SFMFo842-3u9ovAw at mail.gmail.com>
> Content-Type: text/plain; charset="UTF-8"
>
> Hi, I have been unsuccessfully trying to get Squid-4.1-5 in AWS
> (Amazon 1 Linux) to allow transparent proxy of certain domains, as
> well as IPs associated with those domains, whilst rejecting everything
> else.
>
> I have been referencing documentation at
> https://wiki.squid-cache.org/Features/SslPeekAndSplice
>
> Version of Squid: 4.1-5 for Amazon 1 Linux available at
> http://faster.ngtech.co.il/repo/amzn/1/beta/x86_64/ (many thanks to
> @elico for these packages) specifically, the following:
>
> 1) http://faster.ngtech.co.il/repo/amzn/1/beta/x86_64/squid-4.1-5.amzn1.x86_64.rpm
> 2) http://faster.ngtech.co.il/repo/amzn/1/beta/x86_64/squid-helpers-4.1-5.amzn1.x86_64.rpm
>
> Example of tests that I am running:
>
> 1) curl -kv https://service.us2.sumologic.com (EXPECTED: successfully
> accessed; OBSERVED: successfully accessed)
> 2) curl -kv https://54.149.155.70 (EXPECTED: successfully accessed
> because it resolves to service.us2.sumologic.com; OBSERVED:
> "Certificate does not match domainname"  [No Error] (TLS code:
> SQUID_X509_V_ERR_DOMAIN_MISMATCH))
> 3) curl -kv https://www.google.com (EXPECTED: failed to access;
> OBSERVED: failed to access)
> 4) curl -kv https://172.217.13.164 (EXPECTED: failed to access;
> OBSERVED: "Certificate does not match domainname"  [No Error] (TLS
> code: SQUID_X509_V_ERR_DOMAIN_MISMATCH))
>
> Below is the latest version of the squid.conf being used. Apologies
> for any obvious errors--new to Squid here. I have been grappling with
> this for weeks, with many iterations of squid.conf so any advice is
> greatly appreciated; many thanks in advance.
>
> ---
>
> visible_hostname squid
>
> host_verify_strict off
>
> # Handling HTTP requests
> http_port 3128
> http_port 3129 intercept
>
> sslcrtd_children 10
>
> acl CONNECT method CONNECT
>
> # AWS services domain
> acl allowed_http_sites dstdomain .amazonaws.com
> # docker hub registry
> acl allowed_http_sites dstdomain .docker.io
> acl allowed_http_sites dstdomain .docker.com
> acl allowed_http_sites dstdomain www.congiu.net
>
> # Handling HTTPS requests
> # https_port 3130 intercept ssl-bump generate-host-certificates=on
> dynamic_cert_mem_cache_size=100MB cert=/etc/squid/squid.pem
> https_port 3130 intercept ssl-bump dynamic_cert_mem_cache_size=100MB
> cert=/etc/squid/squid.pem
> acl SSL_port port 443
>
> # AWS services domain
> acl allowed_https_sites ssl::server_name .amazonaws.com
> # docker hub registry
> acl allowed_https_sites ssl::server_name .docker.io
> acl allowed_https_sites ssl::server_name .docker.com
>
> # project specific
> acl allowed_https_sites ssl::server_name www.congiu.net
> acl allowed_https_sites ssl::server_name mirrors.fedoraproject.org
> acl allowed_https_sites ssl::server_name mirror.csclub.uwaterloo.ca
>
> # nslookup resolved IPs for collectors.sumologic.com
> # workaround solution to support sumologic collector
> acl allowed_https_sites ssl::server_name .sumologic.com
> # THE FOLLOWING TWO LINES DO NOT SEEM TO WORK AS EXPECTED
> # acl allowed_https_sites ssl::server_name --server-provided
> service.sumologic.com sslflags=DONT_VERIFY_PEER
> # acl allowed_https_sites ssl::server_name --server-provided
> service.us2.sumologic.com sslflags=DONT_VERIFY_PEER
>
> acl step1 at_step SslBump1
> acl step2 at_step SslBump2
> acl step3 at_step SslBump3
>
> ssl_bump peek step1 all
> ssl_bump peek step2 allowed_https_sites
> # http://lists.squid-cache.org/pipermail/squid-users/2018-September/019150.html
> ssl_bump bump
> ssl_bump splice step3 allowed_https_sites
> ssl_bump bump
> ssl_bump terminate step2 all
>
> http_access allow CONNECT
>
> # http_access allow SSL_port
>
> http_access deny CONNECT !allowed_https_sites
> http_access deny CONNECT !allowed_http_sites
> http_access allow allowed_https_sites
> http_access allow allowed_http_sites
> http_access deny all
>
> cache deny all
>
> debug_options "ALL,9"
>
>
> ------------------------------
>
> Message: 2
> Date: Tue, 11 Dec 2018 18:53:23 +0100
> From: Enrico Heine <flashdown at data-core.org>
> To: squid-users at lists.squid-cache.org, Mike Quentel
>         <mike.quentel.rbc at gmail.com>
> Subject: Re: [squid-users] SslBump Peek and Splice using Squid-4.1-5
>         in      Amazon1 Linux with Squid Helpers
> Message-ID: <33156AE7-7EFA-4B8D-8B75-059534D73251 at data-core.org>
> Content-Type: text/plain; charset="utf-8"
>
> Dear Mike,
>
> Please checkout the following and let us know if you need further help.
>
> http://www.squid-cache.org/Doc/config/sslproxy_cert_error/
>
> Best regards,
>
> Flashdown
>
> Am 11. Dezember 2018 16:41:56 MEZ schrieb Mike Quentel <mike.quentel.rbc at gmail.com>:
> >Hi, I have been unsuccessfully trying to get Squid-4.1-5 in AWS
> >(Amazon 1 Linux) to allow transparent proxy of certain domains, as
> >well as IPs associated with those domains, whilst rejecting everything
> >else.
> >
> >I have been referencing documentation at
> >https://wiki.squid-cache.org/Features/SslPeekAndSplice
> >
> >Version of Squid: 4.1-5 for Amazon 1 Linux available at
> >http://faster.ngtech.co.il/repo/amzn/1/beta/x86_64/ (many thanks to
> >@elico for these packages) specifically, the following:
> >
> >1)
> >http://faster.ngtech.co.il/repo/amzn/1/beta/x86_64/squid-4.1-5.amzn1.x86_64.rpm
> >2)
> >http://faster.ngtech.co.il/repo/amzn/1/beta/x86_64/squid-helpers-4.1-5.amzn1.x86_64.rpm
> >
> >Example of tests that I am running:
> >
> >1) curl -kv https://service.us2.sumologic.com (EXPECTED: successfully
> >accessed; OBSERVED: successfully accessed)
> >2) curl -kv https://54.149.155.70 (EXPECTED: successfully accessed
> >because it resolves to service.us2.sumologic.com; OBSERVED:
> >"Certificate does not match domainname"  [No Error] (TLS code:
> >SQUID_X509_V_ERR_DOMAIN_MISMATCH))
> >3) curl -kv https://www.google.com (EXPECTED: failed to access;
> >OBSERVED: failed to access)
> >4) curl -kv https://172.217.13.164 (EXPECTED: failed to access;
> >OBSERVED: "Certificate does not match domainname"  [No Error] (TLS
> >code: SQUID_X509_V_ERR_DOMAIN_MISMATCH))
> >
> >Below is the latest version of the squid.conf being used. Apologies
> >for any obvious errors--new to Squid here. I have been grappling with
> >this for weeks, with many iterations of squid.conf so any advice is
> >greatly appreciated; many thanks in advance.
> >
> >---
> >
> >visible_hostname squid
> >
> >host_verify_strict off
> >
> ># Handling HTTP requests
> >http_port 3128
> >http_port 3129 intercept
> >
> >sslcrtd_children 10
> >
> >acl CONNECT method CONNECT
> >
> ># AWS services domain
> >acl allowed_http_sites dstdomain .amazonaws.com
> ># docker hub registry
> >acl allowed_http_sites dstdomain .docker.io
> >acl allowed_http_sites dstdomain .docker.com
> >acl allowed_http_sites dstdomain www.congiu.net
> >
> ># Handling HTTPS requests
> ># https_port 3130 intercept ssl-bump generate-host-certificates=on
> >dynamic_cert_mem_cache_size=100MB cert=/etc/squid/squid.pem
> >https_port 3130 intercept ssl-bump dynamic_cert_mem_cache_size=100MB
> >cert=/etc/squid/squid.pem
> >acl SSL_port port 443
> >
> ># AWS services domain
> >acl allowed_https_sites ssl::server_name .amazonaws.com
> ># docker hub registry
> >acl allowed_https_sites ssl::server_name .docker.io
> >acl allowed_https_sites ssl::server_name .docker.com
> >
> ># project specific
> >acl allowed_https_sites ssl::server_name www.congiu.net
> >acl allowed_https_sites ssl::server_name mirrors.fedoraproject.org
> >acl allowed_https_sites ssl::server_name mirror.csclub.uwaterloo.ca
> >
> ># nslookup resolved IPs for collectors.sumologic.com
> ># workaround solution to support sumologic collector
> >acl allowed_https_sites ssl::server_name .sumologic.com
> ># THE FOLLOWING TWO LINES DO NOT SEEM TO WORK AS EXPECTED
> ># acl allowed_https_sites ssl::server_name --server-provided
> >service.sumologic.com sslflags=DONT_VERIFY_PEER
> ># acl allowed_https_sites ssl::server_name --server-provided
> >service.us2.sumologic.com sslflags=DONT_VERIFY_PEER
> >
> >acl step1 at_step SslBump1
> >acl step2 at_step SslBump2
> >acl step3 at_step SslBump3
> >
> >ssl_bump peek step1 all
> >ssl_bump peek step2 allowed_https_sites
> >#
> >http://lists.squid-cache.org/pipermail/squid-users/2018-September/019150.html
> >ssl_bump bump
> >ssl_bump splice step3 allowed_https_sites
> >ssl_bump bump
> >ssl_bump terminate step2 all
> >
> >http_access allow CONNECT
> >
> ># http_access allow SSL_port
> >
> >http_access deny CONNECT !allowed_https_sites
> >http_access deny CONNECT !allowed_http_sites
> >http_access allow allowed_https_sites
> >http_access allow allowed_http_sites
> >http_access deny all
> >
> >cache deny all
> >
> >debug_options "ALL,9"
> >_______________________________________________
> >squid-users mailing list
> >squid-users at lists.squid-cache.org
> >http://lists.squid-cache.org/listinfo/squid-users
>
> --
> Diese Nachricht wurde von meinem Android-Ger?t mit K-9 Mail gesendet.
> -------------- next part --------------
> An HTML attachment was scrubbed...
> URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20181211/f46270b6/attachment-0001.html>
>
> ------------------------------
>
> Message: 3
> Date: Wed, 12 Dec 2018 12:07:49 +1300
> From: Amos Jeffries <squid3 at treenet.co.nz>
> To: squid-users at lists.squid-cache.org
> Subject: Re: [squid-users] SslBump Peek and Splice using Squid-4.1-5
>         in Amazon1 Linux with Squid Helpers
> Message-ID: <15dc44bc-ef9e-02a6-f4bb-0d05087f5a3b at treenet.co.nz>
> Content-Type: text/plain; charset=utf-8
>
> On 12/12/18 6:53 am, Enrico Heine wrote:
> > Dear Mike,
> >
> > Please checkout the following and let us know if you need further help.
> >
> > http://www.squid-cache.org/Doc/config/sslproxy_cert_error/
> >
>
> Before you use it though, please consider what the words "Certificate
> does not match domainname" actually *mean*.
>
> This Squid is configured to deliver a single specific custom-built
> certificate to all clients who contact the proxy. Yet the proxy is being
> used to receive TLS traffic for any domain and the admin is passing test
> traffic for multiple different domains and raw-IP addresses.
>
>
>
> > Best regards,
> >
> > Flashdown
> >
> > Am 11. Dezember 2018 16:41:56 MEZ schrieb Mike Quentel:
> >
> >     Hi, I have been unsuccessfully trying to get Squid-4.1-5 in AWS
> >     (Amazon 1 Linux) to allow transparent proxy of certain domains, as
> >     well as IPs associated with those domains, whilst rejecting everything
> >     else.
> >
> >     I have been referencing documentation at
> >     https://wiki.squid-cache.org/Features/SslPeekAndSplice
> >
> >     Version of Squid: 4.1-5 for Amazon 1 Linux available at
> >     http://faster.ngtech.co.il/repo/amzn/1/beta/x86_64/ (many thanks to
> >     @elico for these packages) specifically, the following:
> >
> >     1) http://faster.ngtech.co.il/repo/amzn/1/beta/x86_64/squid-4.1-5.amzn1.x86_64.rpm
> >     2) http://faster.ngtech.co.il/repo/amzn/1/beta/x86_64/squid-helpers-4.1-5.amzn1.x86_64.rpm
> >
> >     Example of tests that I am running:
> >
> >     1) curl -kv https://service.us2.sumologic.com (EXPECTED: successfully
> >     accessed; OBSERVED: successfully accessed)
>
> The TLS SNI contains "service.us2.sumologic.com", and
>  - the server produced an X.509 certificate for that domain, and
>  - your server_name ACL matches it as a sub-domain of ".sumologic.com"
>
>
> Note that the -k parameter for curl only disables security on the
> curl<->Squid TSL connection. It has nothing to do with the
> Squid<->origin connections.
>
> You should really be using "curl --cacert /etc/squid/squid.pem" or
> connections without the -k to test what actually happens for clients
> when their traffic goes through your system.
>
>
> >     2) curl -kv https://54.149.155.70 (EXPECTED: successfully accessed
> >     because it resolves to service.us2.sumologic.com; OBSERVED:
> >     "Certificate does not match domainname"  [No Error] (TLS code:
> >     SQUID_X509_V_ERR_DOMAIN_MISMATCH))
>
> IMO the expectation is what is wrong here.
>
> The TLS SNI does not exist, and
>  - being intercepted traffic the CONNECT authority is
> "54.149.155.70:443", and
>  - the server produced an X.509 certificate with SubjectName of either
> "54.149.155.70" or something else not matching your server_name ACL entries.
>
> FYI: server_name is a text-string matching ACL. I expect you will find
> there is no reverse-DNS being performed during the ssl_bump testing,
> only later after contact the server has already been decided to allow.
> You can confirm that with the debug log your test produced. Look for the
> lines saying what each ACL is checking for and against.
>
>
> >     3) curl -kv https://www.google.com (EXPECTED: failed to access;
> >     OBSERVED: failed to access)
>
> "failed to access" is a gross over-simplification. This transaction is
> both allowed and not-allowed at the same time.
>
> If you look into the log I expect you will see this sequence happening:
>
>  * the http_access rules *allow* the CONNECT tunnel, then
>
>  * the ssl_bump rules select do "bump" action at Step-2 (aka. using only
> the TLS clientHello details), then
>
>  * curl -k ignores the small problem that you are not presenting the
> X.509 keys belonging to Google.
>
>  * the decrypted GET request inside the tunnel gets rejected because:
>  - the "allowed_https_sites" ACL has no X.509 server details to test
> against, so does not match.
>   - the "allowed_http_sites" ACL does not match either
>   - the "http_access deny all" matches everything reaching it.
>
>
> >     4) curl -kv https://172.217.13.164 (EXPECTED: failed to access;
> >     OBSERVED: "Certificate does not match domainname"  [No Error] (TLS
> >     code: SQUID_X509_V_ERR_DOMAIN_MISMATCH))
>
> Same thing going on as for test (2).
>
>
> >
> >     Below is the latest version of the squid.conf being used. Apologies
> >     for any obvious errors--new to Squid here. I have been grappling with
> >     this for weeks, with many iterations of squid.conf so any advice is
> >     greatly appreciated; many thanks in advance.
> >     ------------------------------------------------------------------------
> >     visible_hostname squid
>
> You have connected this proxy to the Internet. The above is required by
> Internet RFCs to be a FQDN (fully qualified domain name).
>
> Even if you do not want to follow that requirements it MUST be a unique
> name.  If any of your HTTP traffic ever goes through another proxy
> sharing this *very common* config mistake you will encounter forwarding
> loop errors.
>
>
> >
> >     host_verify_strict off
>
> This is the default. No need to configure it.
>
> Also, if you added that because the errors you mentioned are talking
> about domain verification - be aware that HTTP "Host:" header
> verification is quite a different thing from TLS certificate verification.
>
>
> >
> >     # Handling HTTP requests
> >     http_port 3128
> >     http_port 3129 intercept
> >
> >     sslcrtd_children 10
> >
> >     acl CONNECT method CONNECT
> >
> >     # AWS services domain
> >     acl allowed_http_sites dstdomain .amazonaws.com
> >     # docker hub registry
> >     acl allowed_http_sites dstdomain .docker.io
> >     acl allowed_http_sites dstdomain .docker.com
> >     acl allowed_http_sites dstdomain www.congiu.net
> >
> >     # Handling HTTPS requests
> >     # https_port 3130 intercept ssl-bump generate-host-certificates=on
> >     dynamic_cert_mem_cache_size=100MB cert=/etc/squid/squid.pem
> >     https_port 3130 intercept ssl-bump dynamic_cert_mem_cache_size=100MB
> >     cert=/etc/squid/squid.pem
>
> FYI: both the lines above behave identical because the generate-*
> setting you removed was being set to its default value anyway.
>
>
> >     acl SSL_port port 443
> >
> >     # AWS services domain
> >     acl allowed_https_sites ssl::server_name .amazonaws.com
> >     # docker hub registry
> >     acl allowed_https_sites ssl::server_name .docker.io
> >     acl allowed_https_sites ssl::server_name .docker.com
> >
> >     # project specific
> >     acl allowed_https_sites ssl::server_name www.congiu.net
> >     acl allowed_https_sites ssl::server_name mirrors.fedoraproject.org
> >     acl allowed_https_sites ssl::server_name mirror.csclub.uwaterloo.ca
> >
> >     # nslookup resolved IPs for collectors.sumologic.com
> >     # workaround solution to support sumologic collector
> >     acl allowed_https_sites ssl::server_name .sumologic.com
> >     # THE FOLLOWING TWO LINES DO NOT SEEM TO WORK AS EXPECTED
>
> The expectation is wrong here.
>
> The string "sslflags=DONT_VERIFY_PEER" is not a valid domain nor server
> hostname. So highly unlikely that the X.509 certificate SubjectName or
> AltSubjectName from the origin server will contain that string.
>
> Also, the flag sets the ACL matching algorithm. When that is set the ACL
> cannot match during a ssl_bump "peek step1" cycle.
>
> So one should expect this ACL to stop working when these lines are
> added. Not expect that it would do anything useful.
>
>
> FYI: The string "sslflags=DONT_VERIFY_PEER" is the name and value of a
> option other directives elsewhere in squid.conf can use. But it is a
> very, very, very bad idea to do so - even 'just for testing'.
>
>  The flag DONT_VERIFY_PEER disables all of TLS security checks - meaning
> the connection actively becomes *less* safe than regular/plain-text TCP
> connections, while simultaneously hiding all resulting issues from
> *your* admin view. Users still have problems, you just cannot see any
> hint of them.
>  So please purge that setting from any configs and documents you come
> across. Investigate and fix any TLS problems that appear, don't just
> hide the error messages and pretend everything works.
>  Same reason not to be using the equivalent "curl -k" option for testing
> TLS validation/verification problems.
>
>
>
> >     # acl allowed_https_sites ssl::server_name --server-provided
> >     service.sumologic.com sslflags=DONT_VERIFY_PEER
> >     # acl allowed_https_sites ssl::server_name --server-provided
> >     service.us2.sumologic.com sslflags=DONT_VERIFY_PEER
> >
> >     acl step1 at_step SslBump1
> >     acl step2 at_step SslBump2
> >     acl step3 at_step SslBump3
> >
> >     ssl_bump peek step1 all
>
> The "all" here is useless and only adds confusion to anyone who thinks
> it has any meaning.
>
>
> >     ssl_bump peek step2 allowed_https_sites
> >     # http://lists.squid-cache.org/pipermail/squid-users/2018-September/019150.html
>
> The author of that did not understand how the ssl-bump processing was
> working. That entire message thread is them attempting to learn and IMO
> still not quite understanding the ideas in the end.
>
> Blindly copying into your config from experiments by someone who does
> not understand what they are doing is not a good idea. Use an actually
> known-working config example (the Squid wiki has several), or try to
> design your own based on your own understanding. At the very least we
> can see from your self-designed attempt what you may be thinking and
> hopefully teach you where any mistakes are visible.
>
>
> >     ssl_bump bump
>
> Please be aware that when this line when reached at step2 it performs
> "client-first" bumping of the TLS.
>
> That means bumping and performing TLS handshake without any real X.509
> server details for your allowed_https_sites ACL to use. Only
> client-provided claims about what server they are contacting (which may
> be outright lies). This has side-effects on what your ssl::server_name
> vs dstdomain ACL do in the later http_access checks.
>
> Specifically when the server_name and the URL domain are different
> things a simple as testing one first can change permissions for the
> client in the other.
>
>
> >     ssl_bump splice step3 allowed_https_sites
>
> So some of your traffic will splice from the above line - but only
> because of the peek (step2) then bump (step3) combination being impossible.
>
>
> >     ssl_bump bump
>
> There is already an unrestricted "bump" action earlier. This line does
> nothing even if it were possibly reached.
>
>
> >     ssl_bump terminate step2 all
>
> There is a peek action specified earlier for step2, with an unrestricted
> bump action as a fallback when allowed_https_sites fails to match. This
> line is never reachable.
>
> >
> >     http_access allow CONNECT
>
> Ouch. Really do not do the above. The default config file shipped with
> Squid starts with these lines for very good reasons:
>
> "
>   http_access deny !Safe_ports
>   http_access deny CONNECT !SSL_ports
>   http_access allow localhost manager
>   http_access deny manager
>
>   #
>   # INSERT YOUR OWN RULE(S) HERE TO ALLOW ACCESS FROM YOUR CLIENTS
>   #
> "
>
> Those reasons (DoS and proxy relay security vulnerabilities) are still
> very much relevant in your setup, with no reason to remove them. So
> please add them back before you continue testing things, with your
> custom http_access rules *underneath* that comment line.
>
> Also, the default SSL_ports is already setup in a way that meets your
> requirements. You can adjust Safe_ports to be only the same port, or use
> the default set of safe-for-HTTP ports.
>
> FYI: the config you have right now allows any malicious origin server
> receiving trafic on port 443 to present a X.509 certificate claiming to be.
>
>
> >
> >     # http_access allow SSL_port
> >
> >     http_access deny CONNECT !allowed_https_sites
> >     http_access deny CONNECT !allowed_http_sites
>
> The above two lines do nothing in your current config. CONNECT requests
> are *always* allowed by the line you had earlier.
>
> Once you move back to the default security checks these will start to do
> things. It would probably be best to remove the two lines above to
> prevent the unexpected new behaviour from confusing you further.
>
>
> >     http_access allow allowed_https_sites
> >     http_access allow allowed_http_sites
>
> These ACLs are very badly named.
>
> * The one called "allowed_https_sites":
>  - will *not* match against HTTPS traffic arriving on port 3128 unless
> the CONNECT authority-uri names a domain in your list.
>
> - *will* match against HTTP traffic on port 3128 and port 3129 with
> "https://" URLs.
>
> * The one called "allowed_http_sites" *will* match against HTTPS traffic
> arriving on any port.
>
>  --> meaning that for both of them the "https" and "http" word in their
> name is deceptive.
>
> * Both these ACLs are used to *deny* traffic.
>  --> meaning the "allowed" word in their name is deceptive.
>
> What you are left with is just "sites" which is so vague as to be
> meaningless.
>
>
> It would be a lot clearer if you renamed them:
>  - "allowed_https_sites" to "tls_servers"
>  - "allowed_http_sites" to "url_domains"
>
>
> >     http_access deny all
> >
> >     cache deny all
> >
> >     debug_options "ALL,9"
>
> Cheers
> Amos
>
>
> ------------------------------
>
> Subject: Digest Footer
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>
>
> ------------------------------
>
> End of squid-users Digest, Vol 52, Issue 13
> *******************************************


From rousskov at measurement-factory.com  Tue Dec 18 16:01:58 2018
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Tue, 18 Dec 2018 09:01:58 -0700
Subject: [squid-users] squid-users Digest, Vol 52, Issue 13
In-Reply-To: <CAE53ZLAo70jTRqfVfbUSMUp1Y6AwbMPJbdUgWzBoAAhsBN3opg@mail.gmail.com>
References: <mailman.4981.1544569699.3019.squid-users@lists.squid-cache.org>
 <CAE53ZLAo70jTRqfVfbUSMUp1Y6AwbMPJbdUgWzBoAAhsBN3opg@mail.gmail.com>
Message-ID: <d0e6d2e9-8b34-6f52-9cd9-a8f52aeb013a@measurement-factory.com>

On 12/18/18 8:40 AM, Mike Quentel wrote:

> ssl_bump peek step1 all
> ssl_bump peek step2 tls_servers
> ssl_bump splice step3 tls_servers
> ssl_bump stare step2
> ssl_bump bump step3
> ssl_bump terminate step2 all

Just FYI: The above should be equivalent to the following simpler, less
misleading, and a bit faster configuration:

  ssl_bump peek step1

  ssl_bump peek tls_servers
  ssl_bump stare all

  ssl_bump splice all
  ssl_bump bump all


> attempting to access https://www.google.com will still download
> the page (200).


You have told Squid to allow that (and nearly every other) access:

> http_access allow SSL_ports


What you probably want is something like this:

  http_access deny CONNECT !SSL_ports
  http_access allow url_domains
  http_access allow tls_servers
  http_access deny all

but if you add http_port (or do not want to trust Squid with enforcing
connection pinning), then you will be better off with the default rules
that also prohibit access to !Safe_ports.


HTH,

Alex.


From squid3 at treenet.co.nz  Tue Dec 18 18:44:08 2018
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Wed, 19 Dec 2018 07:44:08 +1300
Subject: [squid-users] Squid4 with GnuTLS - specify ciphers or disable
 protocols
In-Reply-To: <CADpL81ZCtgLMc6S+W-bpCGvZj2wA_dzZjQkQpJiDBRY1=wuCQA@mail.gmail.com>
References: <mailman.3.1541851201.4312.squid-users@lists.squid-cache.org>
 <CADpL81bKoNSQqsXyCkgWKCyL0kOpqgYGkVea-zxNWG2Aoe6BsQ@mail.gmail.com>
 <a9578d84-5b0f-b953-8616-e5651a7433ea@treenet.co.nz>
 <CADpL81as3hyEnOHWfaYTDKj+GPA9s+P-9ky9OV2CxaTXK-i5_g@mail.gmail.com>
 <84bfd973-055d-cdcc-bb1b-9059f0d4112f@treenet.co.nz>
 <CADpL81ZCtgLMc6S+W-bpCGvZj2wA_dzZjQkQpJiDBRY1=wuCQA@mail.gmail.com>
Message-ID: <6a479c4f-9d53-2fea-759b-73f138434230@treenet.co.nz>

On 19/12/18 3:44 am, Martin Hoffmann wrote:
> Thanks that would be fine.
> However meanwhile I have recompiled squid 4.4 with OpenSSL support
> (added --enable-ssl 

Which does not exist any longer.

> and --with-open-ssl=xxx 

Which never existed at all.

The ./configure option name is " --with-openssl ". Add that to the
debian/rules file and rebuild.


> and removed --with-gnutls
> to debian/rules) just to end with the same problems 

Nod. That is because the options you have to enable OpenSSL do not exist.


# see './configure --help'
"

  --enable-ssl-crtd       Prevent Squid from directly generating TLS/SSL
                          private key and certificate. Instead enables
                          the certificate generator processes.

  --without-gnutls        Do not use GnuTLS for SSL.
                          Default: auto-detect

  --with-openssl[=PATH]   Compile with the OpenSSL libraries. The path
                          to the OpenSSL development libraries and
                          headers installation can be specified if
                          outside of the system standard directories

"

HTH
Amos



From subhish.pillai at broadcom.com  Tue Dec 18 19:52:48 2018
From: subhish.pillai at broadcom.com (Subhish Pillai)
Date: Tue, 18 Dec 2018 12:52:48 -0700
Subject: [squid-users] HTTPS proxy setup questions
In-Reply-To: <d15c0a01-0b8c-4d01-7c92-bdb1b45967a9@measurement-factory.com>
References: <1544644695402-0.post@n4.nabble.com>
 <508b070f-f479-b62f-af45-6ff5a303555b@measurement-factory.com>
 <CAFODohhZnpb1rROaY5xW5HDDC91wJgfhM_=my7yvBi05v0BJrw@mail.gmail.com>
 <44ad4ea0-1864-3567-c0fa-d22aca9f2492@treenet.co.nz>
 <CAFODohi9M3pp1iGLmYZp00wrEMA5qomGHsJxrLEyTqmY0ZQeJQ@mail.gmail.com>
 <d15c0a01-0b8c-4d01-7c92-bdb1b45967a9@measurement-factory.com>
Message-ID: <CAFODohhSg=pFki2Dx9CvPPBiib7pbzi-XUC4LfdFjZNgA_KtxA@mail.gmail.com>

Thank you for the directions, I have the https proxy working now.

I got a signed CA cert and installed it on the squid server and after
importing the intermediate cert into the client, it is working as expected.

*https_port 3128 tls-cert=/etc/squid/ssl_cert/ssl_certificate.cer
tls-key=/etc/squid/ssl_cert/proxy.key*

Thanks for all the help and the responsiveness.


Subhish


On Fri, Dec 14, 2018 at 2:33 PM Alex Rousskov <
rousskov at measurement-factory.com> wrote:

> On 12/14/18 12:03 PM, Subhish Pillai wrote:
>
> > my use case for the squid proxy is to be able to accept a
> > HTTPS_proxy request from the client and tunnel it forward to the
> > destination server.
>
> > How do I get this to work without having to create self-signed certs on
> > the proxy server and importing that into the client ca-bundle.
>
> Get a server certificate from a CA authority that the client trusts,
> issued for the Squid proxy domain. Give Squid that certificate. For
> example, you may be able to use a free letsencrypt.org CA.
>
> An HTTPS proxy needs a certificate it can sign its traffic with. That
> certificate must be issued by a client-trusted CA. Whether that is a
> fake CA that you operate (what you may have referred to as a
> "self-signed cert" above) or a real CA trusted by millions of other
> clients (e.g., letsencrypt), is your choice.
>
>
> > For that I copied over the CA bundle from the client
> > into the proxy server and pointed the "tls-cert" option to that file
>
> Why? Please suggest specific documentation changes that would remove the
> implication that doing the above has something to do with your goals.
> That option is for specifying the signing certificate (i.e. the
> certificate the proxy is going to sign traffic with).
>
>
> > Am I missing any config steps in the squid.conf file?
>
> You are missing a clientca or tls-cafile option that triggers client
> certificate request (from Squid to client) and gives Squid CA
> certificates to trust when validating the client-supplied certificate.
> This is unrelated to the Squid signing certificate discussed above.
>
> Alex.
>
>
> > On Wed, Dec 12, 2018 at 6:53 PM Amos Jeffries <squid3 at treenet.co.nz
> > <mailto:squid3 at treenet.co.nz>> wrote:
> >
> >     On 13/12/18 12:50 pm, Subhish Pillai wrote:
> >     > Thanks Alex, that was very helpful.
> >     >
> >     > Based on your explanation, I just want to use squid as a blind TCP
> >     > tunnel carrying the HTTPS connection from client to app server.
> >     >
> >     > In that case, I don't need to use ssl_bump feature and the ssl_crtd
> >     > program for certificate management, is that correct?
> >     >
> >
> >     Going by the description you gave of the client configuration, it
> >     should be.
> >
> >
> >     > Would this config file work to setup the TCP tunnel --
> >
> >     ...
> >     > ## Allow server side certificate errors such as untrusted
> >     certificates,
> >     > otherwise the connection is closed for such errors
> >     > sslproxy_cert_error allow all
> >     >
> >     > ## Accept certificates that fail verification (should only be
> >     needed if
> >     > using 'sslproxy_cert_error allow all')
> >     > sslproxy_flags DONT_VERIFY_PEER
> >     >
> >
> >     These sslproxy_* options only apply when Squid is actively performing
> >     TLS to upstream servers. They have no place in the "blind tunnel"
> >     situation.
> >     (They also are deprecated in Squid-4, replaced by the
> >     tls_outgoing_options directive
> >     <http://www.squid-cache.org/Doc/config/tls_outgoing_options/>).
> >
> >     If the client software is sending CONNECT requests containing the
> HTTPS
> >     traffic, then there is absolutely nothing your config needs to do
> than
> >     let them send those requests to the proxy (as the default config
> does).
> >
> >     You do not even need Squid to be built with TLS/SSL support. That is
> the
> >     meaning of "blind" in this setup.
> >
> >     Amos
> >     _______________________________________________
> >     squid-users mailing list
> >     squid-users at lists.squid-cache.org
> >     <mailto:squid-users at lists.squid-cache.org>
> >     http://lists.squid-cache.org/listinfo/squid-users
> >
> >
> >
> > --
> >
> > *Subhish Pillai*
> >
> > R&D Software Quality Engineer
> >
> > Broadcom | Brocade Storage Networking
> >
> > T (720) 462-2900
> >
> >
> > _______________________________________________
> > squid-users mailing list
> > squid-users at lists.squid-cache.org
> > http://lists.squid-cache.org/listinfo/squid-users
> >
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>


-- 

*Subhish Pillai*

R&D Software Quality Engineer

Broadcom | Brocade Storage Networking

T (720) 462-2900
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20181218/4bbaa754/attachment.htm>

From mike.quentel.rbc at gmail.com  Tue Dec 18 20:31:34 2018
From: mike.quentel.rbc at gmail.com (Mike Quentel)
Date: Tue, 18 Dec 2018 15:31:34 -0500
Subject: [squid-users] SslBump Peek and Splice using Squid-4.1-5 in
 Amazon1 Linux with Squid Helpers
Message-ID: <CAE53ZLC1m-qPk_t4pnaNS95bwkNJouuVafAnKSF9CLqtR1mpjg@mail.gmail.com>

Resending this message using corrected subject line...

Many thanks Enrico and Amos for the advice you each shared. I have
incorporated the suggested changes into squid.conf and almost have the
desired results in Squid.

Squid is successfully blocking access to TLS sites by an IP address,
but not the same sites using the domain names.

For example, Squid blocking the IP address (taken from nslookup) of
www.google.com successfully works (ERR_ACCESS_DENIED):
https://172.217.1.4

But, attempting to access https://www.google.com will still download
the page (200).

How can I force Squid to block the TLS FQDN versions of web sites that
are not in the white list?

This is the updated squid.conf:

---

visible_hostname squid

http_port 3129 intercept

sslcrtd_children 10

acl CONNECT method CONNECT

acl url_domains dstdomain .amazonaws.com
acl url_domains dstdomain .docker.io
acl url_domains dstdomain .docker.com
acl url_domains dstdomain .congiu.com

https_port 3130 ssl-bump intercept generate-host-certificates=3Don
dynamic_cert_mem_cache_size=3D100MB cert=3D/etc/squid/squid.pem
acl SSL_ports port 443
http_access allow SSL_ports

acl tls_servers ssl::server_name .amazonaws.com
acl tls_servers ssl::server_name .docker.io
acl tls_servers ssl::server_name .docker.com
acl tls_servers ssl::server_name .congiu.com
acl tls_servers ssl::server_name .fedoraproject.org
acl tls_servers ssl::server_name mirror.csclub.uwaterloo.ca
acl tls_servers ssl::server_name .sumologic.com

acl step1 at_step SslBump1
acl step2 at_step SslBump2
acl step3 at_step SslBump3

always_direct allow url_domains
sslproxy_cert_error allow all

http_access deny CONNECT !SSL_Ports
http_access allow url_domains
http_access allow tls_servers
http_access deny all

cache deny all

ssl_bump peek step1 all
ssl_bump peek step2 tls_servers
ssl_bump splice step3 tls_servers
ssl_bump stare step2
ssl_bump bump step3
ssl_bump terminate step2 all

# debug_options ALL,1 80,5
debug_options ALL,1 33,4

---

Thanks, Mike Quentel

On Tue, 11 Dec 2018 at 18:08, <squid-users-request at lists.squid-cache.org> w=
rote:
>
> Send squid-users mailing list submissions to
>         squid-users at lists.squid-cache.org
>
> To subscribe or unsubscribe via the World Wide Web, visit
>         http://lists.squid-cache.org/listinfo/squid-users
> or, via email, send a message with subject or body 'help' to
>         squid-users-request at lists.squid-cache.org
>
> You can reach the person managing the list at
>         squid-users-owner at lists.squid-cache.org
>
> When replying, please edit your Subject line so it is more specific
> than "Re: Contents of squid-users digest..."
>
>
> Today's Topics:
>
>    1. SslBump Peek and Splice using Squid-4.1-5 in Amazon1      Linux
>       with Squid Helpers (Mike Quentel)
>    2. Re: SslBump Peek and Splice using Squid-4.1-5 in  Amazon1
>       Linux with Squid Helpers (Enrico Heine)
>    3. Re: SslBump Peek and Splice using Squid-4.1-5 in Amazon1
>       Linux with Squid Helpers (Amos Jeffries)
>
>
> ----------------------------------------------------------------------
>
> Message: 1
> Date: Tue, 11 Dec 2018 10:41:56 -0500
> From: Mike Quentel <mike.quentel.rbc at gmail.com>
> To: squid-users at lists.squid-cache.org
> Subject: [squid-users] SslBump Peek and Splice using Squid-4.1-5 in
>         Amazon1 Linux with Squid Helpers
> Message-ID:
>         <CAE53ZLD9oqt0cUKsrwHSDo8nDEo2Midty0SFMFo842-3u9ovAw at mail.gmail.c=
om>
> Content-Type: text/plain; charset=3D"UTF-8"
>
> Hi, I have been unsuccessfully trying to get Squid-4.1-5 in AWS
> (Amazon 1 Linux) to allow transparent proxy of certain domains, as
> well as IPs associated with those domains, whilst rejecting everything
> else.
>
> I have been referencing documentation at
> https://wiki.squid-cache.org/Features/SslPeekAndSplice
>
> Version of Squid: 4.1-5 for Amazon 1 Linux available at
> http://faster.ngtech.co.il/repo/amzn/1/beta/x86_64/ (many thanks to
> @elico for these packages) specifically, the following:
>
> 1) http://faster.ngtech.co.il/repo/amzn/1/beta/x86_64/squid-4.1-5.amzn1.x=
86_64.rpm
> 2) http://faster.ngtech.co.il/repo/amzn/1/beta/x86_64/squid-helpers-4.1-5=
.amzn1.x86_64.rpm
>
> Example of tests that I am running:
>
> 1) curl -kv https://service.us2.sumologic.com (EXPECTED: successfully
> accessed; OBSERVED: successfully accessed)
> 2) curl -kv https://54.149.155.70 (EXPECTED: successfully accessed
> because it resolves to service.us2.sumologic.com; OBSERVED:
> "Certificate does not match domainname"  [No Error] (TLS code:
> SQUID_X509_V_ERR_DOMAIN_MISMATCH))
> 3) curl -kv https://www.google.com (EXPECTED: failed to access;
> OBSERVED: failed to access)
> 4) curl -kv https://172.217.13.164 (EXPECTED: failed to access;
> OBSERVED: "Certificate does not match domainname"  [No Error] (TLS
> code: SQUID_X509_V_ERR_DOMAIN_MISMATCH))
>
> Below is the latest version of the squid.conf being used. Apologies
> for any obvious errors--new to Squid here. I have been grappling with
> this for weeks, with many iterations of squid.conf so any advice is
> greatly appreciated; many thanks in advance.
>
> ---
>
> visible_hostname squid
>
> host_verify_strict off
>
> # Handling HTTP requests
> http_port 3128
> http_port 3129 intercept
>
> sslcrtd_children 10
>
> acl CONNECT method CONNECT
>
> # AWS services domain
> acl allowed_http_sites dstdomain .amazonaws.com
> # docker hub registry
> acl allowed_http_sites dstdomain .docker.io
> acl allowed_http_sites dstdomain .docker.com
> acl allowed_http_sites dstdomain www.congiu.net
>
> # Handling HTTPS requests
> # https_port 3130 intercept ssl-bump generate-host-certificates=3Don
> dynamic_cert_mem_cache_size=3D100MB cert=3D/etc/squid/squid.pem
> https_port 3130 intercept ssl-bump dynamic_cert_mem_cache_size=3D100MB
> cert=3D/etc/squid/squid.pem
> acl SSL_port port 443
>
> # AWS services domain
> acl allowed_https_sites ssl::server_name .amazonaws.com
> # docker hub registry
> acl allowed_https_sites ssl::server_name .docker.io
> acl allowed_https_sites ssl::server_name .docker.com
>
> # project specific
> acl allowed_https_sites ssl::server_name www.congiu.net
> acl allowed_https_sites ssl::server_name mirrors.fedoraproject.org
> acl allowed_https_sites ssl::server_name mirror.csclub.uwaterloo.ca
>
> # nslookup resolved IPs for collectors.sumologic.com
> # workaround solution to support sumologic collector
> acl allowed_https_sites ssl::server_name .sumologic.com
> # THE FOLLOWING TWO LINES DO NOT SEEM TO WORK AS EXPECTED
> # acl allowed_https_sites ssl::server_name --server-provided
> service.sumologic.com sslflags=3DDONT_VERIFY_PEER
> # acl allowed_https_sites ssl::server_name --server-provided
> service.us2.sumologic.com sslflags=3DDONT_VERIFY_PEER
>
> acl step1 at_step SslBump1
> acl step2 at_step SslBump2
> acl step3 at_step SslBump3
>
> ssl_bump peek step1 all
> ssl_bump peek step2 allowed_https_sites
> # http://lists.squid-cache.org/pipermail/squid-users/2018-September/01915=
0.html
> ssl_bump bump
> ssl_bump splice step3 allowed_https_sites
> ssl_bump bump
> ssl_bump terminate step2 all
>
> http_access allow CONNECT
>
> # http_access allow SSL_port
>
> http_access deny CONNECT !allowed_https_sites
> http_access deny CONNECT !allowed_http_sites
> http_access allow allowed_https_sites
> http_access allow allowed_http_sites
> http_access deny all
>
> cache deny all
>
> debug_options "ALL,9"
>
>
> ------------------------------
>
> Message: 2
> Date: Tue, 11 Dec 2018 18:53:23 +0100
> From: Enrico Heine <flashdown at data-core.org>
> To: squid-users at lists.squid-cache.org, Mike Quentel
>         <mike.quentel.rbc at gmail.com>
> Subject: Re: [squid-users] SslBump Peek and Splice using Squid-4.1-5
>         in      Amazon1 Linux with Squid Helpers
> Message-ID: <33156AE7-7EFA-4B8D-8B75-059534D73251 at data-core.org>
> Content-Type: text/plain; charset=3D"utf-8"
>
> Dear Mike,
>
> Please checkout the following and let us know if you need further help.
>
> http://www.squid-cache.org/Doc/config/sslproxy_cert_error/
>
> Best regards,
>
> Flashdown
>
> Am 11. Dezember 2018 16:41:56 MEZ schrieb Mike Quentel <mike.quentel.rbc@=
gmail.com>:
> >Hi, I have been unsuccessfully trying to get Squid-4.1-5 in AWS
> >(Amazon 1 Linux) to allow transparent proxy of certain domains, as
> >well as IPs associated with those domains, whilst rejecting everything
> >else.
> >
> >I have been referencing documentation at
> >https://wiki.squid-cache.org/Features/SslPeekAndSplice
> >
> >Version of Squid: 4.1-5 for Amazon 1 Linux available at
> >http://faster.ngtech.co.il/repo/amzn/1/beta/x86_64/ (many thanks to
> >@elico for these packages) specifically, the following:
> >
> >1)
> >http://faster.ngtech.co.il/repo/amzn/1/beta/x86_64/squid-4.1-5.amzn1.x86=
_64.rpm
> >2)
> >http://faster.ngtech.co.il/repo/amzn/1/beta/x86_64/squid-helpers-4.1-5.a=
mzn1.x86_64.rpm
> >
> >Example of tests that I am running:
> >
> >1) curl -kv https://service.us2.sumologic.com (EXPECTED: successfully
> >accessed; OBSERVED: successfully accessed)
> >2) curl -kv https://54.149.155.70 (EXPECTED: successfully accessed
> >because it resolves to service.us2.sumologic.com; OBSERVED:
> >"Certificate does not match domainname"  [No Error] (TLS code:
> >SQUID_X509_V_ERR_DOMAIN_MISMATCH))
> >3) curl -kv https://www.google.com (EXPECTED: failed to access;
> >OBSERVED: failed to access)
> >4) curl -kv https://172.217.13.164 (EXPECTED: failed to access;
> >OBSERVED: "Certificate does not match domainname"  [No Error] (TLS
> >code: SQUID_X509_V_ERR_DOMAIN_MISMATCH))
> >
> >Below is the latest version of the squid.conf being used. Apologies
> >for any obvious errors--new to Squid here. I have been grappling with
> >this for weeks, with many iterations of squid.conf so any advice is
> >greatly appreciated; many thanks in advance.
> >
> >---
> >
> >visible_hostname squid
> >
> >host_verify_strict off
> >
> ># Handling HTTP requests
> >http_port 3128
> >http_port 3129 intercept
> >
> >sslcrtd_children 10
> >
> >acl CONNECT method CONNECT
> >
> ># AWS services domain
> >acl allowed_http_sites dstdomain .amazonaws.com
> ># docker hub registry
> >acl allowed_http_sites dstdomain .docker.io
> >acl allowed_http_sites dstdomain .docker.com
> >acl allowed_http_sites dstdomain www.congiu.net
> >
> ># Handling HTTPS requests
> ># https_port 3130 intercept ssl-bump generate-host-certificates=3Don
> >dynamic_cert_mem_cache_size=3D100MB cert=3D/etc/squid/squid.pem
> >https_port 3130 intercept ssl-bump dynamic_cert_mem_cache_size=3D100MB
> >cert=3D/etc/squid/squid.pem
> >acl SSL_port port 443
> >
> ># AWS services domain
> >acl allowed_https_sites ssl::server_name .amazonaws.com
> ># docker hub registry
> >acl allowed_https_sites ssl::server_name .docker.io
> >acl allowed_https_sites ssl::server_name .docker.com
> >
> ># project specific
> >acl allowed_https_sites ssl::server_name www.congiu.net
> >acl allowed_https_sites ssl::server_name mirrors.fedoraproject.org
> >acl allowed_https_sites ssl::server_name mirror.csclub.uwaterloo.ca
> >
> ># nslookup resolved IPs for collectors.sumologic.com
> ># workaround solution to support sumologic collector
> >acl allowed_https_sites ssl::server_name .sumologic.com
> ># THE FOLLOWING TWO LINES DO NOT SEEM TO WORK AS EXPECTED
> ># acl allowed_https_sites ssl::server_name --server-provided
> >service.sumologic.com sslflags=3DDONT_VERIFY_PEER
> ># acl allowed_https_sites ssl::server_name --server-provided
> >service.us2.sumologic.com sslflags=3DDONT_VERIFY_PEER
> >
> >acl step1 at_step SslBump1
> >acl step2 at_step SslBump2
> >acl step3 at_step SslBump3
> >
> >ssl_bump peek step1 all
> >ssl_bump peek step2 allowed_https_sites
> >#
> >http://lists.squid-cache.org/pipermail/squid-users/2018-September/019150=
.html
> >ssl_bump bump
> >ssl_bump splice step3 allowed_https_sites
> >ssl_bump bump
> >ssl_bump terminate step2 all
> >
> >http_access allow CONNECT
> >
> ># http_access allow SSL_port
> >
> >http_access deny CONNECT !allowed_https_sites
> >http_access deny CONNECT !allowed_http_sites
> >http_access allow allowed_https_sites
> >http_access allow allowed_http_sites
> >http_access deny all
> >
> >cache deny all
> >
> >debug_options "ALL,9"
> >_______________________________________________
> >squid-users mailing list
> >squid-users at lists.squid-cache.org
> >http://lists.squid-cache.org/listinfo/squid-users
>
> --
> Diese Nachricht wurde von meinem Android-Ger=C3=A4t mit K-9 Mail gesendet=
.
> -------------- next part --------------
> An HTML attachment was scrubbed...
> URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/2018=
1211/f46270b6/attachment-0001.html>
>
> ------------------------------
>
> Message: 3
> Date: Wed, 12 Dec 2018 12:07:49 +1300
> From: Amos Jeffries <squid3 at treenet.co.nz>
> To: squid-users at lists.squid-cache.org
> Subject: Re: [squid-users] SslBump Peek and Splice using Squid-4.1-5
>         in Amazon1 Linux with Squid Helpers
> Message-ID: <15dc44bc-ef9e-02a6-f4bb-0d05087f5a3b at treenet.co.nz>
> Content-Type: text/plain; charset=3Dutf-8
>
> On 12/12/18 6:53 am, Enrico Heine wrote:
> > Dear Mike,
> >
> > Please checkout the following and let us know if you need further help.
> >
> > http://www.squid-cache.org/Doc/config/sslproxy_cert_error/
> >
>
> Before you use it though, please consider what the words "Certificate
> does not match domainname" actually *mean*.
>
> This Squid is configured to deliver a single specific custom-built
> certificate to all clients who contact the proxy. Yet the proxy is being
> used to receive TLS traffic for any domain and the admin is passing test
> traffic for multiple different domains and raw-IP addresses.
>
>
>
> > Best regards,
> >
> > Flashdown
> >
> > Am 11. Dezember 2018 16:41:56 MEZ schrieb Mike Quentel:
> >
> >     Hi, I have been unsuccessfully trying to get Squid-4.1-5 in AWS
> >     (Amazon 1 Linux) to allow transparent proxy of certain domains, as
> >     well as IPs associated with those domains, whilst rejecting everyth=
ing
> >     else.
> >
> >     I have been referencing documentation at
> >     https://wiki.squid-cache.org/Features/SslPeekAndSplice
> >
> >     Version of Squid: 4.1-5 for Amazon 1 Linux available at
> >     http://faster.ngtech.co.il/repo/amzn/1/beta/x86_64/ (many thanks to
> >     @elico for these packages) specifically, the following:
> >
> >     1) http://faster.ngtech.co.il/repo/amzn/1/beta/x86_64/squid-4.1-5.a=
mzn1.x86_64.rpm
> >     2) http://faster.ngtech.co.il/repo/amzn/1/beta/x86_64/squid-helpers=
-4.1-5.amzn1.x86_64.rpm
> >
> >     Example of tests that I am running:
> >
> >     1) curl -kv https://service.us2.sumologic.com (EXPECTED: successful=
ly
> >     accessed; OBSERVED: successfully accessed)
>
> The TLS SNI contains "service.us2.sumologic.com", and
>  - the server produced an X.509 certificate for that domain, and
>  - your server_name ACL matches it as a sub-domain of ".sumologic.com"
>
>
> Note that the -k parameter for curl only disables security on the
> curl<->Squid TSL connection. It has nothing to do with the
> Squid<->origin connections.
>
> You should really be using "curl --cacert /etc/squid/squid.pem" or
> connections without the -k to test what actually happens for clients
> when their traffic goes through your system.
>
>
> >     2) curl -kv https://54.149.155.70 (EXPECTED: successfully accessed
> >     because it resolves to service.us2.sumologic.com; OBSERVED:
> >     "Certificate does not match domainname"  [No Error] (TLS code:
> >     SQUID_X509_V_ERR_DOMAIN_MISMATCH))
>
> IMO the expectation is what is wrong here.
>
> The TLS SNI does not exist, and
>  - being intercepted traffic the CONNECT authority is
> "54.149.155.70:443", and
>  - the server produced an X.509 certificate with SubjectName of either
> "54.149.155.70" or something else not matching your server_name ACL entri=
es.
>
> FYI: server_name is a text-string matching ACL. I expect you will find
> there is no reverse-DNS being performed during the ssl_bump testing,
> only later after contact the server has already been decided to allow.
> You can confirm that with the debug log your test produced. Look for the
> lines saying what each ACL is checking for and against.
>
>
> >     3) curl -kv https://www.google.com (EXPECTED: failed to access;
> >     OBSERVED: failed to access)
>
> "failed to access" is a gross over-simplification. This transaction is
> both allowed and not-allowed at the same time.
>
> If you look into the log I expect you will see this sequence happening:
>
>  * the http_access rules *allow* the CONNECT tunnel, then
>
>  * the ssl_bump rules select do "bump" action at Step-2 (aka. using only
> the TLS clientHello details), then
>
>  * curl -k ignores the small problem that you are not presenting the
> X.509 keys belonging to Google.
>
>  * the decrypted GET request inside the tunnel gets rejected because:
>  - the "allowed_https_sites" ACL has no X.509 server details to test
> against, so does not match.
>   - the "allowed_http_sites" ACL does not match either
>   - the "http_access deny all" matches everything reaching it.
>
>
> >     4) curl -kv https://172.217.13.164 (EXPECTED: failed to access;
> >     OBSERVED: "Certificate does not match domainname"  [No Error] (TLS
> >     code: SQUID_X509_V_ERR_DOMAIN_MISMATCH))
>
> Same thing going on as for test (2).
>
>
> >
> >     Below is the latest version of the squid.conf being used. Apologies
> >     for any obvious errors--new to Squid here. I have been grappling wi=
th
> >     this for weeks, with many iterations of squid.conf so any advice is
> >     greatly appreciated; many thanks in advance.
> >     -------------------------------------------------------------------=
-----
> >     visible_hostname squid
>
> You have connected this proxy to the Internet. The above is required by
> Internet RFCs to be a FQDN (fully qualified domain name).
>
> Even if you do not want to follow that requirements it MUST be a unique
> name.  If any of your HTTP traffic ever goes through another proxy
> sharing this *very common* config mistake you will encounter forwarding
> loop errors.
>
>
> >
> >     host_verify_strict off
>
> This is the default. No need to configure it.
>
> Also, if you added that because the errors you mentioned are talking
> about domain verification - be aware that HTTP "Host:" header
> verification is quite a different thing from TLS certificate verification=
.
>
>
> >
> >     # Handling HTTP requests
> >     http_port 3128
> >     http_port 3129 intercept
> >
> >     sslcrtd_children 10
> >
> >     acl CONNECT method CONNECT
> >
> >     # AWS services domain
> >     acl allowed_http_sites dstdomain .amazonaws.com
> >     # docker hub registry
> >     acl allowed_http_sites dstdomain .docker.io
> >     acl allowed_http_sites dstdomain .docker.com
> >     acl allowed_http_sites dstdomain www.congiu.net
> >
> >     # Handling HTTPS requests
> >     # https_port 3130 intercept ssl-bump generate-host-certificates=3Do=
n
> >     dynamic_cert_mem_cache_size=3D100MB cert=3D/etc/squid/squid.pem
> >     https_port 3130 intercept ssl-bump dynamic_cert_mem_cache_size=3D10=
0MB
> >     cert=3D/etc/squid/squid.pem
>
> FYI: both the lines above behave identical because the generate-*
> setting you removed was being set to its default value anyway.
>
>
> >     acl SSL_port port 443
> >
> >     # AWS services domain
> >     acl allowed_https_sites ssl::server_name .amazonaws.com
> >     # docker hub registry
> >     acl allowed_https_sites ssl::server_name .docker.io
> >     acl allowed_https_sites ssl::server_name .docker.com
> >
> >     # project specific
> >     acl allowed_https_sites ssl::server_name www.congiu.net
> >     acl allowed_https_sites ssl::server_name mirrors.fedoraproject.org
> >     acl allowed_https_sites ssl::server_name mirror.csclub.uwaterloo.ca
> >
> >     # nslookup resolved IPs for collectors.sumologic.com
> >     # workaround solution to support sumologic collector
> >     acl allowed_https_sites ssl::server_name .sumologic.com
> >     # THE FOLLOWING TWO LINES DO NOT SEEM TO WORK AS EXPECTED
>
> The expectation is wrong here.
>
> The string "sslflags=3DDONT_VERIFY_PEER" is not a valid domain nor server
> hostname. So highly unlikely that the X.509 certificate SubjectName or
> AltSubjectName from the origin server will contain that string.
>
> Also, the flag sets the ACL matching algorithm. When that is set the ACL
> cannot match during a ssl_bump "peek step1" cycle.
>
> So one should expect this ACL to stop working when these lines are
> added. Not expect that it would do anything useful.
>
>
> FYI: The string "sslflags=3DDONT_VERIFY_PEER" is the name and value of a
> option other directives elsewhere in squid.conf can use. But it is a
> very, very, very bad idea to do so - even 'just for testing'.
>
>  The flag DONT_VERIFY_PEER disables all of TLS security checks - meaning
> the connection actively becomes *less* safe than regular/plain-text TCP
> connections, while simultaneously hiding all resulting issues from
> *your* admin view. Users still have problems, you just cannot see any
> hint of them.
>  So please purge that setting from any configs and documents you come
> across. Investigate and fix any TLS problems that appear, don't just
> hide the error messages and pretend everything works.
>  Same reason not to be using the equivalent "curl -k" option for testing
> TLS validation/verification problems.
>
>
>
> >     # acl allowed_https_sites ssl::server_name --server-provided
> >     service.sumologic.com sslflags=3DDONT_VERIFY_PEER
> >     # acl allowed_https_sites ssl::server_name --server-provided
> >     service.us2.sumologic.com sslflags=3DDONT_VERIFY_PEER
> >
> >     acl step1 at_step SslBump1
> >     acl step2 at_step SslBump2
> >     acl step3 at_step SslBump3
> >
> >     ssl_bump peek step1 all
>
> The "all" here is useless and only adds confusion to anyone who thinks
> it has any meaning.
>
>
> >     ssl_bump peek step2 allowed_https_sites
> >     # http://lists.squid-cache.org/pipermail/squid-users/2018-September=
/019150.html
>
> The author of that did not understand how the ssl-bump processing was
> working. That entire message thread is them attempting to learn and IMO
> still not quite understanding the ideas in the end.
>
> Blindly copying into your config from experiments by someone who does
> not understand what they are doing is not a good idea. Use an actually
> known-working config example (the Squid wiki has several), or try to
> design your own based on your own understanding. At the very least we
> can see from your self-designed attempt what you may be thinking and
> hopefully teach you where any mistakes are visible.
>
>
> >     ssl_bump bump
>
> Please be aware that when this line when reached at step2 it performs
> "client-first" bumping of the TLS.
>
> That means bumping and performing TLS handshake without any real X.509
> server details for your allowed_https_sites ACL to use. Only
> client-provided claims about what server they are contacting (which may
> be outright lies). This has side-effects on what your ssl::server_name
> vs dstdomain ACL do in the later http_access checks.
>
> Specifically when the server_name and the URL domain are different
> things a simple as testing one first can change permissions for the
> client in the other.
>
>
> >     ssl_bump splice step3 allowed_https_sites
>
> So some of your traffic will splice from the above line - but only
> because of the peek (step2) then bump (step3) combination being impossibl=
e.
>
>
> >     ssl_bump bump
>
> There is already an unrestricted "bump" action earlier. This line does
> nothing even if it were possibly reached.
>
>
> >     ssl_bump terminate step2 all
>
> There is a peek action specified earlier for step2, with an unrestricted
> bump action as a fallback when allowed_https_sites fails to match. This
> line is never reachable.
>
> >
> >     http_access allow CONNECT
>
> Ouch. Really do not do the above. The default config file shipped with
> Squid starts with these lines for very good reasons:
>
> "
>   http_access deny !Safe_ports
>   http_access deny CONNECT !SSL_ports
>   http_access allow localhost manager
>   http_access deny manager
>
>   #
>   # INSERT YOUR OWN RULE(S) HERE TO ALLOW ACCESS FROM YOUR CLIENTS
>   #
> "
>
> Those reasons (DoS and proxy relay security vulnerabilities) are still
> very much relevant in your setup, with no reason to remove them. So
> please add them back before you continue testing things, with your
> custom http_access rules *underneath* that comment line.
>
> Also, the default SSL_ports is already setup in a way that meets your
> requirements. You can adjust Safe_ports to be only the same port, or use
> the default set of safe-for-HTTP ports.
>
> FYI: the config you have right now allows any malicious origin server
> receiving trafic on port 443 to present a X.509 certificate claiming to b=
e.
>
>
> >
> >     # http_access allow SSL_port
> >
> >     http_access deny CONNECT !allowed_https_sites
> >     http_access deny CONNECT !allowed_http_sites
>
> The above two lines do nothing in your current config. CONNECT requests
> are *always* allowed by the line you had earlier.
>
> Once you move back to the default security checks these will start to do
> things. It would probably be best to remove the two lines above to
> prevent the unexpected new behaviour from confusing you further.
>
>
> >     http_access allow allowed_https_sites
> >     http_access allow allowed_http_sites
>
> These ACLs are very badly named.
>
> * The one called "allowed_https_sites":
>  - will *not* match against HTTPS traffic arriving on port 3128 unless
> the CONNECT authority-uri names a domain in your list.
>
> - *will* match against HTTP traffic on port 3128 and port 3129 with
> "https://" URLs.
>
> * The one called "allowed_http_sites" *will* match against HTTPS traffic
> arriving on any port.
>
>  --> meaning that for both of them the "https" and "http" word in their
> name is deceptive.
>
> * Both these ACLs are used to *deny* traffic.
>  --> meaning the "allowed" word in their name is deceptive.
>
> What you are left with is just "sites" which is so vague as to be
> meaningless.
>
>
> It would be a lot clearer if you renamed them:
>  - "allowed_https_sites" to "tls_servers"
>  - "allowed_http_sites" to "url_domains"
>
>
> >     http_access deny all
> >
> >     cache deny all
> >
> >     debug_options "ALL,9"
>
> Cheers
> Amos
>
>
> ------------------------------
>
> Subject: Digest Footer
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>
>
> ------------------------------
>
> End of squid-users Digest, Vol 52, Issue 13
> *******************************************


From oagvozd at gmail.com  Wed Dec 19 12:13:43 2018
From: oagvozd at gmail.com (Meridoff)
Date: Wed, 19 Dec 2018 15:13:43 +0300
Subject: [squid-users] squid https: using non-self-signed cert
Message-ID: <CAFfuDwztgw3yw7GAV9r3=jO5NzDzVXU_Dx52b-3FeaqHGbRsFg@mail.gmail.com>

Hello, when proxying https traffic squid needs self-signed cert.

But what if I use not self-signed cert ?  I need to use cert of my company
which is not self-signed. Is it possible ? May be I can use capath= option
for this..
Now squid complains: FATAL: No valid signing SSL certificate configured for
HTTPS_port 192.168.1.1:3128

Thanks.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20181219/0fccc768/attachment.htm>

From squidusers at daltontech.co.uk  Wed Dec 19 12:22:57 2018
From: squidusers at daltontech.co.uk (Squid users)
Date: Wed, 19 Dec 2018 12:22:57 +0000
Subject: [squid-users] Advice - Squid Proxy
References: <AM6PR08MB3784A7A46CE95BF58F9C679C8DBE0@AM6PR08MB3784.eurprd08.prod.outlook.com>
 <AM6PR08MB3784839C5A2C221A5EA340FC8DBE0@AM6PR08MB3784.eurprd08.prod.outlook.com>
Message-ID: <AM6PR08MB378448C5ADA66E0ED91EA33E8DBE0@AM6PR08MB3784.eurprd08.prod.outlook.com>

The attached configuration is currently in use on my computer. 

My aim is to use my laptop while I'm out and about (libraries, work etc) and when I'm at home have my TV and Phone connect into the proxy server.  This would allow caching by any device to my laptop so I'm minimising my connections outbound.

I also want it to record use by other people so I can monitor my internet use at home. 

As you can see I run bitdefender parental control on my computer. Would it be possible for someone to manipulate the proxy server to bypass this? Could the proxy server be used to hide / obscure actual sites visited?

Can anyone point out any flaws or issues.....

Thanks
-------------- next part --------------
A non-text attachment was scrubbed...
Name: layout2.jpg
Type: image/jpeg
Size: 58138 bytes
Desc: layout2.jpg
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20181219/991ec5c9/attachment.jpg>

From squid3 at treenet.co.nz  Wed Dec 19 13:15:02 2018
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 20 Dec 2018 02:15:02 +1300
Subject: [squid-users] squid https: using non-self-signed cert
In-Reply-To: <CAFfuDwztgw3yw7GAV9r3=jO5NzDzVXU_Dx52b-3FeaqHGbRsFg@mail.gmail.com>
References: <CAFfuDwztgw3yw7GAV9r3=jO5NzDzVXU_Dx52b-3FeaqHGbRsFg@mail.gmail.com>
Message-ID: <735c7850-b46d-06a5-42df-816d5748bc0d@treenet.co.nz>

On 20/12/18 1:13 am, Meridoff wrote:
> Hello, when proxying https traffic squid needs self-signed cert.
> 

No, Squid needs a certificate with properties compatible with the
particular "proxying https" which your proxy is configured to do.


Some of those uses require *a CA* certificate and key. Self-signed is
the simplest type of CA certificate - anybody can create and use one for
whatever they want.

There are other types of CA certificate and any of them are are also
usable in the situations where Squid simply needs a CA cert.



> But what if I use not self-signed cert ?

Depends on what type of certificate properties it *does* have.


>? I need to use cert of my
> company which is not self-signed.

Is it a CA certificate? probably not.

Do you actually need a CA for the feature(s) you are trying to use?
 probably yes, maybe no.

Please provide details of the config you are trying to setup so we can
answer more accurately. Right now anybody saying yes, no or giving
specific advice will have to be guessing about what you mean.


> Is it possible ? May be I can use
> capath= option for this..

No. The capath= option is for loading *multiple* CA certificates in
OpenSSL. It does not change the type of certificates loaded.


> Now squid complains:?FATAL: No valid signing SSL certificate configured
> for HTTPS_port 192.168.1.1:3128 <http://192.168.1.1:3128>
> 

That message from Squid simply says the cert you are loading is not
meeting the minimum requirements for the features you have configured in
Squid.

Yes that typically means one of the SSL-Bump features is being used and
the cert is not a CA. But there are also other situations that message
comes up, so please supply details about what you are actually trying to do.

Amos


From Antony.Stone at squid.open.source.it  Wed Dec 19 13:19:24 2018
From: Antony.Stone at squid.open.source.it (Antony Stone)
Date: Wed, 19 Dec 2018 14:19:24 +0100
Subject: [squid-users] Advice - Squid Proxy
In-Reply-To: <AM6PR08MB378448C5ADA66E0ED91EA33E8DBE0@AM6PR08MB3784.eurprd08.prod.outlook.com>
References: <AM6PR08MB3784A7A46CE95BF58F9C679C8DBE0@AM6PR08MB3784.eurprd08.prod.outlook.com>
 <AM6PR08MB3784839C5A2C221A5EA340FC8DBE0@AM6PR08MB3784.eurprd08.prod.outlook.com>
 <AM6PR08MB378448C5ADA66E0ED91EA33E8DBE0@AM6PR08MB3784.eurprd08.prod.outlook.com>
Message-ID: <201812191419.24315.Antony.Stone@squid.open.source.it>

On Wednesday 19 December 2018 at 13:22:57, Squid users wrote:

> The attached configuration is currently in use on my computer.

It isn't a network diagram; I'm not quite sure what to describe it as, but I 
don't even see where Squid is on there.

> My aim is to use my laptop while I'm out and about (libraries, work etc)
> and when I'm at home have my TV and Phone connect into the proxy server. 
> This would allow caching by any device to my laptop so I'm minimising my
> connections outbound.

So, Squid runs on your laptop?

What are the phone and TV supposed to do when the laptop isn't there?

> I also want it to record use by other people so I can monitor my internet
> use at home.

Define "use".  What level of detail do you want to record?

> As you can see I run bitdefender parental control on my computer. Would it
> be possible for someone to manipulate the proxy server to bypass this?
> Could the proxy server be used to hide / obscure actual sites visited?

Show us a rather more conventional network diagram, which shows how packets 
get to & from the Internet, and what filters / firewalls are in place between 
different bits of equipment, and we might be able to asnwer this.


Antony.

-- 
"Can you keep a secret?"
"Well, I shouldn't really tell you this, but... no."


                                                   Please reply to the list;
                                                         please *don't* CC me.


From eliezer at ngtech.co.il  Wed Dec 19 14:36:51 2018
From: eliezer at ngtech.co.il (Eliezer Croitoru)
Date: Wed, 19 Dec 2018 16:36:51 +0200
Subject: [squid-users] SslBump Peek and Splice using Squid-4.1-5 in
	Amazon1 Linux with Squid Helpers
In-Reply-To: <CAE53ZLC1m-qPk_t4pnaNS95bwkNJouuVafAnKSF9CLqtR1mpjg@mail.gmail.com>
References: <CAE53ZLC1m-qPk_t4pnaNS95bwkNJouuVafAnKSF9CLqtR1mpjg@mail.gmail.com>
Message-ID: <76c801d497a8$481c9680$d855c380$@ngtech.co.il>

Remove the line:
http_access allow SSL_ports

It bypass all the next ACL's....

Eliezer

----
Eliezer Croitoru
Linux System Administrator
Mobile: +972-5-28704261
Email: eliezer at ngtech.co.il



-----Original Message-----
From: squid-users [mailto:squid-users-bounces at lists.squid-cache.org] On Behalf Of Mike Quentel
Sent: Tuesday, December 18, 2018 22:32
To: squid-users at lists.squid-cache.org; squid3 at treenet.co.nz; asdf asdf <flashdown at data-core.org>
Subject: Re: [squid-users] SslBump Peek and Splice using Squid-4.1-5 in Amazon1 Linux with Squid Helpers

Resending this message using corrected subject line...

Many thanks Enrico and Amos for the advice you each shared. I have
incorporated the suggested changes into squid.conf and almost have the
desired results in Squid.

Squid is successfully blocking access to TLS sites by an IP address,
but not the same sites using the domain names.

For example, Squid blocking the IP address (taken from nslookup) of
www.google.com successfully works (ERR_ACCESS_DENIED):
https://172.217.1.4

But, attempting to access https://www.google.com will still download
the page (200).

How can I force Squid to block the TLS FQDN versions of web sites that
are not in the white list?

This is the updated squid.conf:

---

visible_hostname squid

http_port 3129 intercept

sslcrtd_children 10

acl CONNECT method CONNECT

acl url_domains dstdomain .amazonaws.com
acl url_domains dstdomain .docker.io
acl url_domains dstdomain .docker.com
acl url_domains dstdomain .congiu.com

https_port 3130 ssl-bump intercept generate-host-certificates=3Don
dynamic_cert_mem_cache_size=3D100MB cert=3D/etc/squid/squid.pem
acl SSL_ports port 443
http_access allow SSL_ports

acl tls_servers ssl::server_name .amazonaws.com
acl tls_servers ssl::server_name .docker.io
acl tls_servers ssl::server_name .docker.com
acl tls_servers ssl::server_name .congiu.com
acl tls_servers ssl::server_name .fedoraproject.org
acl tls_servers ssl::server_name mirror.csclub.uwaterloo.ca
acl tls_servers ssl::server_name .sumologic.com

acl step1 at_step SslBump1
acl step2 at_step SslBump2
acl step3 at_step SslBump3

always_direct allow url_domains
sslproxy_cert_error allow all

http_access deny CONNECT !SSL_Ports
http_access allow url_domains
http_access allow tls_servers
http_access deny all

cache deny all

ssl_bump peek step1 all
ssl_bump peek step2 tls_servers
ssl_bump splice step3 tls_servers
ssl_bump stare step2
ssl_bump bump step3
ssl_bump terminate step2 all

# debug_options ALL,1 80,5
debug_options ALL,1 33,4

---

Thanks, Mike Quentel

On Tue, 11 Dec 2018 at 18:08, <squid-users-request at lists.squid-cache.org> w=
rote:
>
> Send squid-users mailing list submissions to
>         squid-users at lists.squid-cache.org
>
> To subscribe or unsubscribe via the World Wide Web, visit
>         http://lists.squid-cache.org/listinfo/squid-users
> or, via email, send a message with subject or body 'help' to
>         squid-users-request at lists.squid-cache.org
>
> You can reach the person managing the list at
>         squid-users-owner at lists.squid-cache.org
>
> When replying, please edit your Subject line so it is more specific
> than "Re: Contents of squid-users digest..."
>
>
> Today's Topics:
>
>    1. SslBump Peek and Splice using Squid-4.1-5 in Amazon1      Linux
>       with Squid Helpers (Mike Quentel)
>    2. Re: SslBump Peek and Splice using Squid-4.1-5 in  Amazon1
>       Linux with Squid Helpers (Enrico Heine)
>    3. Re: SslBump Peek and Splice using Squid-4.1-5 in Amazon1
>       Linux with Squid Helpers (Amos Jeffries)
>
>
> ----------------------------------------------------------------------
>
> Message: 1
> Date: Tue, 11 Dec 2018 10:41:56 -0500
> From: Mike Quentel <mike.quentel.rbc at gmail.com>
> To: squid-users at lists.squid-cache.org
> Subject: [squid-users] SslBump Peek and Splice using Squid-4.1-5 in
>         Amazon1 Linux with Squid Helpers
> Message-ID:
>         <CAE53ZLD9oqt0cUKsrwHSDo8nDEo2Midty0SFMFo842-3u9ovAw at mail.gmail.c=
om>
> Content-Type: text/plain; charset=3D"UTF-8"
>
> Hi, I have been unsuccessfully trying to get Squid-4.1-5 in AWS
> (Amazon 1 Linux) to allow transparent proxy of certain domains, as
> well as IPs associated with those domains, whilst rejecting everything
> else.
>
> I have been referencing documentation at
> https://wiki.squid-cache.org/Features/SslPeekAndSplice
>
> Version of Squid: 4.1-5 for Amazon 1 Linux available at
> http://faster.ngtech.co.il/repo/amzn/1/beta/x86_64/ (many thanks to
> @elico for these packages) specifically, the following:
>
> 1) http://faster.ngtech.co.il/repo/amzn/1/beta/x86_64/squid-4.1-5.amzn1.x=
86_64.rpm
> 2) http://faster.ngtech.co.il/repo/amzn/1/beta/x86_64/squid-helpers-4.1-5=
.amzn1.x86_64.rpm
>
> Example of tests that I am running:
>
> 1) curl -kv https://service.us2.sumologic.com (EXPECTED: successfully
> accessed; OBSERVED: successfully accessed)
> 2) curl -kv https://54.149.155.70 (EXPECTED: successfully accessed
> because it resolves to service.us2.sumologic.com; OBSERVED:
> "Certificate does not match domainname"  [No Error] (TLS code:
> SQUID_X509_V_ERR_DOMAIN_MISMATCH))
> 3) curl -kv https://www.google.com (EXPECTED: failed to access;
> OBSERVED: failed to access)
> 4) curl -kv https://172.217.13.164 (EXPECTED: failed to access;
> OBSERVED: "Certificate does not match domainname"  [No Error] (TLS
> code: SQUID_X509_V_ERR_DOMAIN_MISMATCH))
>
> Below is the latest version of the squid.conf being used. Apologies
> for any obvious errors--new to Squid here. I have been grappling with
> this for weeks, with many iterations of squid.conf so any advice is
> greatly appreciated; many thanks in advance.
>
> ---
>
> visible_hostname squid
>
> host_verify_strict off
>
> # Handling HTTP requests
> http_port 3128
> http_port 3129 intercept
>
> sslcrtd_children 10
>
> acl CONNECT method CONNECT
>
> # AWS services domain
> acl allowed_http_sites dstdomain .amazonaws.com
> # docker hub registry
> acl allowed_http_sites dstdomain .docker.io
> acl allowed_http_sites dstdomain .docker.com
> acl allowed_http_sites dstdomain www.congiu.net
>
> # Handling HTTPS requests
> # https_port 3130 intercept ssl-bump generate-host-certificates=3Don
> dynamic_cert_mem_cache_size=3D100MB cert=3D/etc/squid/squid.pem
> https_port 3130 intercept ssl-bump dynamic_cert_mem_cache_size=3D100MB
> cert=3D/etc/squid/squid.pem
> acl SSL_port port 443
>
> # AWS services domain
> acl allowed_https_sites ssl::server_name .amazonaws.com
> # docker hub registry
> acl allowed_https_sites ssl::server_name .docker.io
> acl allowed_https_sites ssl::server_name .docker.com
>
> # project specific
> acl allowed_https_sites ssl::server_name www.congiu.net
> acl allowed_https_sites ssl::server_name mirrors.fedoraproject.org
> acl allowed_https_sites ssl::server_name mirror.csclub.uwaterloo.ca
>
> # nslookup resolved IPs for collectors.sumologic.com
> # workaround solution to support sumologic collector
> acl allowed_https_sites ssl::server_name .sumologic.com
> # THE FOLLOWING TWO LINES DO NOT SEEM TO WORK AS EXPECTED
> # acl allowed_https_sites ssl::server_name --server-provided
> service.sumologic.com sslflags=3DDONT_VERIFY_PEER
> # acl allowed_https_sites ssl::server_name --server-provided
> service.us2.sumologic.com sslflags=3DDONT_VERIFY_PEER
>
> acl step1 at_step SslBump1
> acl step2 at_step SslBump2
> acl step3 at_step SslBump3
>
> ssl_bump peek step1 all
> ssl_bump peek step2 allowed_https_sites
> # http://lists.squid-cache.org/pipermail/squid-users/2018-September/01915=
0.html
> ssl_bump bump
> ssl_bump splice step3 allowed_https_sites
> ssl_bump bump
> ssl_bump terminate step2 all
>
> http_access allow CONNECT
>
> # http_access allow SSL_port
>
> http_access deny CONNECT !allowed_https_sites
> http_access deny CONNECT !allowed_http_sites
> http_access allow allowed_https_sites
> http_access allow allowed_http_sites
> http_access deny all
>
> cache deny all
>
> debug_options "ALL,9"
>
>
> ------------------------------
>
> Message: 2
> Date: Tue, 11 Dec 2018 18:53:23 +0100
> From: Enrico Heine <flashdown at data-core.org>
> To: squid-users at lists.squid-cache.org, Mike Quentel
>         <mike.quentel.rbc at gmail.com>
> Subject: Re: [squid-users] SslBump Peek and Splice using Squid-4.1-5
>         in      Amazon1 Linux with Squid Helpers
> Message-ID: <33156AE7-7EFA-4B8D-8B75-059534D73251 at data-core.org>
> Content-Type: text/plain; charset=3D"utf-8"
>
> Dear Mike,
>
> Please checkout the following and let us know if you need further help.
>
> http://www.squid-cache.org/Doc/config/sslproxy_cert_error/
>
> Best regards,
>
> Flashdown
>
> Am 11. Dezember 2018 16:41:56 MEZ schrieb Mike Quentel <mike.quentel.rbc@=
gmail.com>:
> >Hi, I have been unsuccessfully trying to get Squid-4.1-5 in AWS
> >(Amazon 1 Linux) to allow transparent proxy of certain domains, as
> >well as IPs associated with those domains, whilst rejecting everything
> >else.
> >
> >I have been referencing documentation at
> >https://wiki.squid-cache.org/Features/SslPeekAndSplice
> >
> >Version of Squid: 4.1-5 for Amazon 1 Linux available at
> >http://faster.ngtech.co.il/repo/amzn/1/beta/x86_64/ (many thanks to
> >@elico for these packages) specifically, the following:
> >
> >1)
> >http://faster.ngtech.co.il/repo/amzn/1/beta/x86_64/squid-4.1-5.amzn1.x86=
_64.rpm
> >2)
> >http://faster.ngtech.co.il/repo/amzn/1/beta/x86_64/squid-helpers-4.1-5.a=
mzn1.x86_64.rpm
> >
> >Example of tests that I am running:
> >
> >1) curl -kv https://service.us2.sumologic.com (EXPECTED: successfully
> >accessed; OBSERVED: successfully accessed)
> >2) curl -kv https://54.149.155.70 (EXPECTED: successfully accessed
> >because it resolves to service.us2.sumologic.com; OBSERVED:
> >"Certificate does not match domainname"  [No Error] (TLS code:
> >SQUID_X509_V_ERR_DOMAIN_MISMATCH))
> >3) curl -kv https://www.google.com (EXPECTED: failed to access;
> >OBSERVED: failed to access)
> >4) curl -kv https://172.217.13.164 (EXPECTED: failed to access;
> >OBSERVED: "Certificate does not match domainname"  [No Error] (TLS
> >code: SQUID_X509_V_ERR_DOMAIN_MISMATCH))
> >
> >Below is the latest version of the squid.conf being used. Apologies
> >for any obvious errors--new to Squid here. I have been grappling with
> >this for weeks, with many iterations of squid.conf so any advice is
> >greatly appreciated; many thanks in advance.
> >
> >---
> >
> >visible_hostname squid
> >
> >host_verify_strict off
> >
> ># Handling HTTP requests
> >http_port 3128
> >http_port 3129 intercept
> >
> >sslcrtd_children 10
> >
> >acl CONNECT method CONNECT
> >
> ># AWS services domain
> >acl allowed_http_sites dstdomain .amazonaws.com
> ># docker hub registry
> >acl allowed_http_sites dstdomain .docker.io
> >acl allowed_http_sites dstdomain .docker.com
> >acl allowed_http_sites dstdomain www.congiu.net
> >
> ># Handling HTTPS requests
> ># https_port 3130 intercept ssl-bump generate-host-certificates=3Don
> >dynamic_cert_mem_cache_size=3D100MB cert=3D/etc/squid/squid.pem
> >https_port 3130 intercept ssl-bump dynamic_cert_mem_cache_size=3D100MB
> >cert=3D/etc/squid/squid.pem
> >acl SSL_port port 443
> >
> ># AWS services domain
> >acl allowed_https_sites ssl::server_name .amazonaws.com
> ># docker hub registry
> >acl allowed_https_sites ssl::server_name .docker.io
> >acl allowed_https_sites ssl::server_name .docker.com
> >
> ># project specific
> >acl allowed_https_sites ssl::server_name www.congiu.net
> >acl allowed_https_sites ssl::server_name mirrors.fedoraproject.org
> >acl allowed_https_sites ssl::server_name mirror.csclub.uwaterloo.ca
> >
> ># nslookup resolved IPs for collectors.sumologic.com
> ># workaround solution to support sumologic collector
> >acl allowed_https_sites ssl::server_name .sumologic.com
> ># THE FOLLOWING TWO LINES DO NOT SEEM TO WORK AS EXPECTED
> ># acl allowed_https_sites ssl::server_name --server-provided
> >service.sumologic.com sslflags=3DDONT_VERIFY_PEER
> ># acl allowed_https_sites ssl::server_name --server-provided
> >service.us2.sumologic.com sslflags=3DDONT_VERIFY_PEER
> >
> >acl step1 at_step SslBump1
> >acl step2 at_step SslBump2
> >acl step3 at_step SslBump3
> >
> >ssl_bump peek step1 all
> >ssl_bump peek step2 allowed_https_sites
> >#
> >http://lists.squid-cache.org/pipermail/squid-users/2018-September/019150=
.html
> >ssl_bump bump
> >ssl_bump splice step3 allowed_https_sites
> >ssl_bump bump
> >ssl_bump terminate step2 all
> >
> >http_access allow CONNECT
> >
> ># http_access allow SSL_port
> >
> >http_access deny CONNECT !allowed_https_sites
> >http_access deny CONNECT !allowed_http_sites
> >http_access allow allowed_https_sites
> >http_access allow allowed_http_sites
> >http_access deny all
> >
> >cache deny all
> >
> >debug_options "ALL,9"
> >_______________________________________________
> >squid-users mailing list
> >squid-users at lists.squid-cache.org
> >http://lists.squid-cache.org/listinfo/squid-users
>
> --
> Diese Nachricht wurde von meinem Android-Ger=C3=A4t mit K-9 Mail gesendet=
.
> -------------- next part --------------
> An HTML attachment was scrubbed...
> URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/2018=
1211/f46270b6/attachment-0001.html>
>
> ------------------------------
>
> Message: 3
> Date: Wed, 12 Dec 2018 12:07:49 +1300
> From: Amos Jeffries <squid3 at treenet.co.nz>
> To: squid-users at lists.squid-cache.org
> Subject: Re: [squid-users] SslBump Peek and Splice using Squid-4.1-5
>         in Amazon1 Linux with Squid Helpers
> Message-ID: <15dc44bc-ef9e-02a6-f4bb-0d05087f5a3b at treenet.co.nz>
> Content-Type: text/plain; charset=3Dutf-8
>
> On 12/12/18 6:53 am, Enrico Heine wrote:
> > Dear Mike,
> >
> > Please checkout the following and let us know if you need further help.
> >
> > http://www.squid-cache.org/Doc/config/sslproxy_cert_error/
> >
>
> Before you use it though, please consider what the words "Certificate
> does not match domainname" actually *mean*.
>
> This Squid is configured to deliver a single specific custom-built
> certificate to all clients who contact the proxy. Yet the proxy is being
> used to receive TLS traffic for any domain and the admin is passing test
> traffic for multiple different domains and raw-IP addresses.
>
>
>
> > Best regards,
> >
> > Flashdown
> >
> > Am 11. Dezember 2018 16:41:56 MEZ schrieb Mike Quentel:
> >
> >     Hi, I have been unsuccessfully trying to get Squid-4.1-5 in AWS
> >     (Amazon 1 Linux) to allow transparent proxy of certain domains, as
> >     well as IPs associated with those domains, whilst rejecting everyth=
ing
> >     else.
> >
> >     I have been referencing documentation at
> >     https://wiki.squid-cache.org/Features/SslPeekAndSplice
> >
> >     Version of Squid: 4.1-5 for Amazon 1 Linux available at
> >     http://faster.ngtech.co.il/repo/amzn/1/beta/x86_64/ (many thanks to
> >     @elico for these packages) specifically, the following:
> >
> >     1) http://faster.ngtech.co.il/repo/amzn/1/beta/x86_64/squid-4.1-5.a=
mzn1.x86_64.rpm
> >     2) http://faster.ngtech.co.il/repo/amzn/1/beta/x86_64/squid-helpers=
-4.1-5.amzn1.x86_64.rpm
> >
> >     Example of tests that I am running:
> >
> >     1) curl -kv https://service.us2.sumologic.com (EXPECTED: successful=
ly
> >     accessed; OBSERVED: successfully accessed)
>
> The TLS SNI contains "service.us2.sumologic.com", and
>  - the server produced an X.509 certificate for that domain, and
>  - your server_name ACL matches it as a sub-domain of ".sumologic.com"
>
>
> Note that the -k parameter for curl only disables security on the
> curl<->Squid TSL connection. It has nothing to do with the
> Squid<->origin connections.
>
> You should really be using "curl --cacert /etc/squid/squid.pem" or
> connections without the -k to test what actually happens for clients
> when their traffic goes through your system.
>
>
> >     2) curl -kv https://54.149.155.70 (EXPECTED: successfully accessed
> >     because it resolves to service.us2.sumologic.com; OBSERVED:
> >     "Certificate does not match domainname"  [No Error] (TLS code:
> >     SQUID_X509_V_ERR_DOMAIN_MISMATCH))
>
> IMO the expectation is what is wrong here.
>
> The TLS SNI does not exist, and
>  - being intercepted traffic the CONNECT authority is
> "54.149.155.70:443", and
>  - the server produced an X.509 certificate with SubjectName of either
> "54.149.155.70" or something else not matching your server_name ACL entri=
es.
>
> FYI: server_name is a text-string matching ACL. I expect you will find
> there is no reverse-DNS being performed during the ssl_bump testing,
> only later after contact the server has already been decided to allow.
> You can confirm that with the debug log your test produced. Look for the
> lines saying what each ACL is checking for and against.
>
>
> >     3) curl -kv https://www.google.com (EXPECTED: failed to access;
> >     OBSERVED: failed to access)
>
> "failed to access" is a gross over-simplification. This transaction is
> both allowed and not-allowed at the same time.
>
> If you look into the log I expect you will see this sequence happening:
>
>  * the http_access rules *allow* the CONNECT tunnel, then
>
>  * the ssl_bump rules select do "bump" action at Step-2 (aka. using only
> the TLS clientHello details), then
>
>  * curl -k ignores the small problem that you are not presenting the
> X.509 keys belonging to Google.
>
>  * the decrypted GET request inside the tunnel gets rejected because:
>  - the "allowed_https_sites" ACL has no X.509 server details to test
> against, so does not match.
>   - the "allowed_http_sites" ACL does not match either
>   - the "http_access deny all" matches everything reaching it.
>
>
> >     4) curl -kv https://172.217.13.164 (EXPECTED: failed to access;
> >     OBSERVED: "Certificate does not match domainname"  [No Error] (TLS
> >     code: SQUID_X509_V_ERR_DOMAIN_MISMATCH))
>
> Same thing going on as for test (2).
>
>
> >
> >     Below is the latest version of the squid.conf being used. Apologies
> >     for any obvious errors--new to Squid here. I have been grappling wi=
th
> >     this for weeks, with many iterations of squid.conf so any advice is
> >     greatly appreciated; many thanks in advance.
> >     -------------------------------------------------------------------=
-----
> >     visible_hostname squid
>
> You have connected this proxy to the Internet. The above is required by
> Internet RFCs to be a FQDN (fully qualified domain name).
>
> Even if you do not want to follow that requirements it MUST be a unique
> name.  If any of your HTTP traffic ever goes through another proxy
> sharing this *very common* config mistake you will encounter forwarding
> loop errors.
>
>
> >
> >     host_verify_strict off
>
> This is the default. No need to configure it.
>
> Also, if you added that because the errors you mentioned are talking
> about domain verification - be aware that HTTP "Host:" header
> verification is quite a different thing from TLS certificate verification=
.
>
>
> >
> >     # Handling HTTP requests
> >     http_port 3128
> >     http_port 3129 intercept
> >
> >     sslcrtd_children 10
> >
> >     acl CONNECT method CONNECT
> >
> >     # AWS services domain
> >     acl allowed_http_sites dstdomain .amazonaws.com
> >     # docker hub registry
> >     acl allowed_http_sites dstdomain .docker.io
> >     acl allowed_http_sites dstdomain .docker.com
> >     acl allowed_http_sites dstdomain www.congiu.net
> >
> >     # Handling HTTPS requests
> >     # https_port 3130 intercept ssl-bump generate-host-certificates=3Do=
n
> >     dynamic_cert_mem_cache_size=3D100MB cert=3D/etc/squid/squid.pem
> >     https_port 3130 intercept ssl-bump dynamic_cert_mem_cache_size=3D10=
0MB
> >     cert=3D/etc/squid/squid.pem
>
> FYI: both the lines above behave identical because the generate-*
> setting you removed was being set to its default value anyway.
>
>
> >     acl SSL_port port 443
> >
> >     # AWS services domain
> >     acl allowed_https_sites ssl::server_name .amazonaws.com
> >     # docker hub registry
> >     acl allowed_https_sites ssl::server_name .docker.io
> >     acl allowed_https_sites ssl::server_name .docker.com
> >
> >     # project specific
> >     acl allowed_https_sites ssl::server_name www.congiu.net
> >     acl allowed_https_sites ssl::server_name mirrors.fedoraproject.org
> >     acl allowed_https_sites ssl::server_name mirror.csclub.uwaterloo.ca
> >
> >     # nslookup resolved IPs for collectors.sumologic.com
> >     # workaround solution to support sumologic collector
> >     acl allowed_https_sites ssl::server_name .sumologic.com
> >     # THE FOLLOWING TWO LINES DO NOT SEEM TO WORK AS EXPECTED
>
> The expectation is wrong here.
>
> The string "sslflags=3DDONT_VERIFY_PEER" is not a valid domain nor server
> hostname. So highly unlikely that the X.509 certificate SubjectName or
> AltSubjectName from the origin server will contain that string.
>
> Also, the flag sets the ACL matching algorithm. When that is set the ACL
> cannot match during a ssl_bump "peek step1" cycle.
>
> So one should expect this ACL to stop working when these lines are
> added. Not expect that it would do anything useful.
>
>
> FYI: The string "sslflags=3DDONT_VERIFY_PEER" is the name and value of a
> option other directives elsewhere in squid.conf can use. But it is a
> very, very, very bad idea to do so - even 'just for testing'.
>
>  The flag DONT_VERIFY_PEER disables all of TLS security checks - meaning
> the connection actively becomes *less* safe than regular/plain-text TCP
> connections, while simultaneously hiding all resulting issues from
> *your* admin view. Users still have problems, you just cannot see any
> hint of them.
>  So please purge that setting from any configs and documents you come
> across. Investigate and fix any TLS problems that appear, don't just
> hide the error messages and pretend everything works.
>  Same reason not to be using the equivalent "curl -k" option for testing
> TLS validation/verification problems.
>
>
>
> >     # acl allowed_https_sites ssl::server_name --server-provided
> >     service.sumologic.com sslflags=3DDONT_VERIFY_PEER
> >     # acl allowed_https_sites ssl::server_name --server-provided
> >     service.us2.sumologic.com sslflags=3DDONT_VERIFY_PEER
> >
> >     acl step1 at_step SslBump1
> >     acl step2 at_step SslBump2
> >     acl step3 at_step SslBump3
> >
> >     ssl_bump peek step1 all
>
> The "all" here is useless and only adds confusion to anyone who thinks
> it has any meaning.
>
>
> >     ssl_bump peek step2 allowed_https_sites
> >     # http://lists.squid-cache.org/pipermail/squid-users/2018-September=
/019150.html
>
> The author of that did not understand how the ssl-bump processing was
> working. That entire message thread is them attempting to learn and IMO
> still not quite understanding the ideas in the end.
>
> Blindly copying into your config from experiments by someone who does
> not understand what they are doing is not a good idea. Use an actually
> known-working config example (the Squid wiki has several), or try to
> design your own based on your own understanding. At the very least we
> can see from your self-designed attempt what you may be thinking and
> hopefully teach you where any mistakes are visible.
>
>
> >     ssl_bump bump
>
> Please be aware that when this line when reached at step2 it performs
> "client-first" bumping of the TLS.
>
> That means bumping and performing TLS handshake without any real X.509
> server details for your allowed_https_sites ACL to use. Only
> client-provided claims about what server they are contacting (which may
> be outright lies). This has side-effects on what your ssl::server_name
> vs dstdomain ACL do in the later http_access checks.
>
> Specifically when the server_name and the URL domain are different
> things a simple as testing one first can change permissions for the
> client in the other.
>
>
> >     ssl_bump splice step3 allowed_https_sites
>
> So some of your traffic will splice from the above line - but only
> because of the peek (step2) then bump (step3) combination being impossibl=
e.
>
>
> >     ssl_bump bump
>
> There is already an unrestricted "bump" action earlier. This line does
> nothing even if it were possibly reached.
>
>
> >     ssl_bump terminate step2 all
>
> There is a peek action specified earlier for step2, with an unrestricted
> bump action as a fallback when allowed_https_sites fails to match. This
> line is never reachable.
>
> >
> >     http_access allow CONNECT
>
> Ouch. Really do not do the above. The default config file shipped with
> Squid starts with these lines for very good reasons:
>
> "
>   http_access deny !Safe_ports
>   http_access deny CONNECT !SSL_ports
>   http_access allow localhost manager
>   http_access deny manager
>
>   #
>   # INSERT YOUR OWN RULE(S) HERE TO ALLOW ACCESS FROM YOUR CLIENTS
>   #
> "
>
> Those reasons (DoS and proxy relay security vulnerabilities) are still
> very much relevant in your setup, with no reason to remove them. So
> please add them back before you continue testing things, with your
> custom http_access rules *underneath* that comment line.
>
> Also, the default SSL_ports is already setup in a way that meets your
> requirements. You can adjust Safe_ports to be only the same port, or use
> the default set of safe-for-HTTP ports.
>
> FYI: the config you have right now allows any malicious origin server
> receiving trafic on port 443 to present a X.509 certificate claiming to b=
e.
>
>
> >
> >     # http_access allow SSL_port
> >
> >     http_access deny CONNECT !allowed_https_sites
> >     http_access deny CONNECT !allowed_http_sites
>
> The above two lines do nothing in your current config. CONNECT requests
> are *always* allowed by the line you had earlier.
>
> Once you move back to the default security checks these will start to do
> things. It would probably be best to remove the two lines above to
> prevent the unexpected new behaviour from confusing you further.
>
>
> >     http_access allow allowed_https_sites
> >     http_access allow allowed_http_sites
>
> These ACLs are very badly named.
>
> * The one called "allowed_https_sites":
>  - will *not* match against HTTPS traffic arriving on port 3128 unless
> the CONNECT authority-uri names a domain in your list.
>
> - *will* match against HTTP traffic on port 3128 and port 3129 with
> "https://" URLs.
>
> * The one called "allowed_http_sites" *will* match against HTTPS traffic
> arriving on any port.
>
>  --> meaning that for both of them the "https" and "http" word in their
> name is deceptive.
>
> * Both these ACLs are used to *deny* traffic.
>  --> meaning the "allowed" word in their name is deceptive.
>
> What you are left with is just "sites" which is so vague as to be
> meaningless.
>
>
> It would be a lot clearer if you renamed them:
>  - "allowed_https_sites" to "tls_servers"
>  - "allowed_http_sites" to "url_domains"
>
>
> >     http_access deny all
> >
> >     cache deny all
> >
> >     debug_options "ALL,9"
>
> Cheers
> Amos
>
>
> ------------------------------
>
> Subject: Digest Footer
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>
>
> ------------------------------
>
> End of squid-users Digest, Vol 52, Issue 13
> *******************************************
_______________________________________________
squid-users mailing list
squid-users at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-users



From squidusers at daltontech.co.uk  Wed Dec 19 15:04:36 2018
From: squidusers at daltontech.co.uk (Squid users)
Date: Wed, 19 Dec 2018 15:04:36 +0000
Subject: [squid-users] Advice - Squid Proxy
In-Reply-To: <201812191419.24315.Antony.Stone@squid.open.source.it>
References: <AM6PR08MB3784A7A46CE95BF58F9C679C8DBE0@AM6PR08MB3784.eurprd08.prod.outlook.com>
 <AM6PR08MB3784839C5A2C221A5EA340FC8DBE0@AM6PR08MB3784.eurprd08.prod.outlook.com>
 <AM6PR08MB378448C5ADA66E0ED91EA33E8DBE0@AM6PR08MB3784.eurprd08.prod.outlook.com>
 <201812191419.24315.Antony.Stone@squid.open.source.it>
Message-ID: <AM6PR08MB3784398A12304D72CD4194BF8DBE0@AM6PR08MB3784.eurprd08.prod.outlook.com>

Hi,

Re network diagram - Mish Mash / blended / spaghetti  I think :p

Squid is installed on the Ubuntu virtual machine. Sorry forgot to draw that on.

The phone connects to mobile internet when out of the house, then reverts back to going via squid proxy when my laptop wifi is turned on. The phone detects my laptop and connects accordingly. The phone reconfigures to go via proxy when it connects to my laptop.

As for the TV - yeah my laptop needs to be in the house for that to work.

Internet Use - I'm happy to record websites called by 'user' so for example:
Tv=user1
Phone=user2
Laptop user=user3
Then each family member with their own user id /password.
I've configured this bit already

I have set my home internet router to only allocate my laptop mac a DHCP address....

I'll draw a better diagram later today. 
I may have gone a bit overboard with the control and monitoring :s

Thanks

-----Original Message-----
From: squid-users <squid-users-bounces at lists.squid-cache.org> On Behalf Of Antony Stone
Sent: 19 December 2018 13:19
To: squid-users at lists.squid-cache.org
Subject: Re: [squid-users] Advice - Squid Proxy

On Wednesday 19 December 2018 at 13:22:57, Squid users wrote:

> The attached configuration is currently in use on my computer.

It isn't a network diagram; I'm not quite sure what to describe it as, but I don't even see where Squid is on there.

> My aim is to use my laptop while I'm out and about (libraries, work 
> etc) and when I'm at home have my TV and Phone connect into the proxy server.
> This would allow caching by any device to my laptop so I'm minimising 
> my connections outbound.

So, Squid runs on your laptop?

What are the phone and TV supposed to do when the laptop isn't there?

> I also want it to record use by other people so I can monitor my 
> internet use at home.

Define "use".  What level of detail do you want to record?

> As you can see I run bitdefender parental control on my computer. 
> Would it be possible for someone to manipulate the proxy server to bypass this?
> Could the proxy server be used to hide / obscure actual sites visited?

Show us a rather more conventional network diagram, which shows how packets get to & from the Internet, and what filters / firewalls are in place between different bits of equipment, and we might be able to asnwer this.


Antony.

--
"Can you keep a secret?"
"Well, I shouldn't really tell you this, but... no."


                                                   Please reply to the list;
                                                         please *don't* CC me.
_______________________________________________
squid-users mailing list
squid-users at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-users

From Antony.Stone at squid.open.source.it  Wed Dec 19 16:17:26 2018
From: Antony.Stone at squid.open.source.it (Antony Stone)
Date: Wed, 19 Dec 2018 17:17:26 +0100
Subject: [squid-users] Advice - Squid Proxy
In-Reply-To: <AM6PR08MB3784398A12304D72CD4194BF8DBE0@AM6PR08MB3784.eurprd08.prod.outlook.com>
References: <AM6PR08MB3784A7A46CE95BF58F9C679C8DBE0@AM6PR08MB3784.eurprd08.prod.outlook.com>
 <201812191419.24315.Antony.Stone@squid.open.source.it>
 <AM6PR08MB3784398A12304D72CD4194BF8DBE0@AM6PR08MB3784.eurprd08.prod.outlook.com>
Message-ID: <201812191717.27006.Antony.Stone@squid.open.source.it>

On Wednesday 19 December 2018 at 16:04:36, Squid users wrote:

> Hi,
> 
> Re network diagram - Mish Mash / blended / spaghetti  I think :p
> 
> Squid is installed on the Ubuntu virtual machine. Sorry forgot to draw that
> on.

So, Squid is installed on an Ubuntu VM, which runs on your laptop?

> The phone connects to mobile internet when out of the house, then reverts
> back to going via squid proxy when my laptop wifi is turned on. The phone
> detects my laptop and connects accordingly. The phone reconfigures to go
> via proxy when it connects to my laptop.

So, the phone is either - direct connection via mobile Internet access, or via 
Squid and your home Internet connection - no way for the phone to use the 
Internet connection without going via Squid?

> As for the TV - yeah my laptop needs to be in the house for that to work.

Okay.

> Internet Use - I'm happy to record websites called by 'user' so for
> example: Tv=user1
> Phone=user2
> Laptop user=user3
> Then each family member with their own user id /password.
> I've configured this bit already

Configured it in Squid, so users have to authenticate there to get access?

> I have set my home internet router to only allocate my laptop mac a DHCP
> address....

So, where do any other devices (phone, TV, the three VMs) get their IP 
addresses from?  They must have them, otherwise they couldn't communicate with 
Squid...  What do these devices have as a gateway address?

> I'll draw a better diagram later today.

Okay.

> I may have gone a bit overboard with the control and monitoring :s

Yes, maybe :)


Antony.

-- 
Software development can be quick, high quality, or low cost.

The customer gets to pick any two out of three.

                                                   Please reply to the list;
                                                         please *don't* CC me.


From squid3 at treenet.co.nz  Wed Dec 19 16:49:10 2018
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 20 Dec 2018 05:49:10 +1300
Subject: [squid-users] squid https: using non-self-signed cert
In-Reply-To: <CAFfuDwxu7Eqey=P1YYRc2KH=fCB4m_Wu417J8cmYE-Cj+AjDBQ@mail.gmail.com>
References: <CAFfuDwztgw3yw7GAV9r3=jO5NzDzVXU_Dx52b-3FeaqHGbRsFg@mail.gmail.com>
 <735c7850-b46d-06a5-42df-816d5748bc0d@treenet.co.nz>
 <CAFfuDwxu7Eqey=P1YYRc2KH=fCB4m_Wu417J8cmYE-Cj+AjDBQ@mail.gmail.com>
Message-ID: <0ea76ed4-df2c-3695-7db2-c6c88d513b81@treenet.co.nz>

On 20/12/18 4:32 am, Meridoff wrote:
> 1) I just try to intercept https traffic. I use https_port ...
> cert=cert.pem capath=/dir . So squid can generate ?sertificates based on
> file.cert as Root CA.
> 

Yes.

> So, my file.cert is combined from cert and key files. And it is not
> sefl-signed.

Please change your focus away from the "self-signed" term. As I wrote
earlier it is just a way of saying "Root CA".

The relevant thing is the "CA" part. What is special about CA is that
those certificates can be used to sign other certificates.
 Or in other words: CA cert are "signing SSL certificate".


> Checked by openssl:?
> openssl verify cert.pem
> cert.pem: CN = *.xxx.com <http://xxx.com>
> error 20 at 0 depth lookup:unable to get local issuer certificate
> And squid complains too:?FATAL: No valid signing SSL certificate configured
> 

These are very different messages.

OpenSSL is complaining that the PEM file contains a certificate which
cannot be validated by any CA it trusts.

Squid is complaining that the PEM file does not contain a CA cert + CA
key it can use for signing when generating leaf certificates.


> I think squid want to know who signed this cert - all cert chain to root
> cert. From where squid sholud know where to get all intermidiate CA
> certs for this cert.pem file ?


You have this slightly backwards. The PEM file is where Squid gets the
CA chain.

The PEM file should contain the CA cert + CA key Squid will be using to
generate leaf certs, plus any extra CA chain up to some CA the clients
trust.


> 
> 2) In capath dir: is it neccesary to put here files in hash-format (as
> "man verify" for -CApath says): I mean for example 1234abcde.0 PEM-file
> in this dir.

Skip this. Like I said earlier that option is about other things not
relevant to your problem.

Your problem is that you are trying to use a leaf certificate for HTTPS
interception. You need a CA certificate.

The PEM file can contain:
 * an intermediary CA cert, or
 * a root CA cert (aka self-signed cert), or
 * a chain of intermediary CA certs, or
 * a chain of intermediary CA certs and their root CA cert.


Notice the constant detail in all those: "CA cert".


Amos


From m.hoffmann.bs at gmail.com  Wed Dec 19 16:52:27 2018
From: m.hoffmann.bs at gmail.com (Martin Hoffmann)
Date: Wed, 19 Dec 2018 17:52:27 +0100
Subject: [squid-users] Squid4 with GnuTLS - specify ciphers or disable
	protocols
In-Reply-To: <6a479c4f-9d53-2fea-759b-73f138434230@treenet.co.nz>
References: <mailman.3.1541851201.4312.squid-users@lists.squid-cache.org>
 <CADpL81bKoNSQqsXyCkgWKCyL0kOpqgYGkVea-zxNWG2Aoe6BsQ@mail.gmail.com>
 <a9578d84-5b0f-b953-8616-e5651a7433ea@treenet.co.nz>
 <CADpL81as3hyEnOHWfaYTDKj+GPA9s+P-9ky9OV2CxaTXK-i5_g@mail.gmail.com>
 <84bfd973-055d-cdcc-bb1b-9059f0d4112f@treenet.co.nz>
 <CADpL81ZCtgLMc6S+W-bpCGvZj2wA_dzZjQkQpJiDBRY1=wuCQA@mail.gmail.com>
 <6a479c4f-9d53-2fea-759b-73f138434230@treenet.co.nz>
Message-ID: <CADpL81YboUfELG0OvmRaecBZvtRbzf4U0uX9kXXKR7O88d=7Bw@mail.gmail.com>

Sorry my fault. Using the correct configure options makes OpenSSL support
indeed work :-) Thanks for pointing me to that. I will again try with
GnuTLS after getting everything up and running with OpenSSL.

Regards, Martin.

Am Di., 18. Dez. 2018 um 19:44 Uhr schrieb Amos Jeffries <
squid3 at treenet.co.nz>:

> On 19/12/18 3:44 am, Martin Hoffmann wrote:
> > Thanks that would be fine.
> > However meanwhile I have recompiled squid 4.4 with OpenSSL support
> > (added --enable-ssl
>
> Which does not exist any longer.
>
> > and --with-open-ssl=xxx
>
> Which never existed at all.
>
> The ./configure option name is " --with-openssl ". Add that to the
> debian/rules file and rebuild.
>
>
> > and removed --with-gnutls
> > to debian/rules) just to end with the same problems
>
> Nod. That is because the options you have to enable OpenSSL do not exist.
>
>
> # see './configure --help'
> "
>
>   --enable-ssl-crtd       Prevent Squid from directly generating TLS/SSL
>                           private key and certificate. Instead enables
>                           the certificate generator processes.
>
>   --without-gnutls        Do not use GnuTLS for SSL.
>                           Default: auto-detect
>
>   --with-openssl[=PATH]   Compile with the OpenSSL libraries. The path
>                           to the OpenSSL development libraries and
>                           headers installation can be specified if
>                           outside of the system standard directories
>
> "
>
> HTH
> Amos
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20181219/fad34c6f/attachment.htm>

From squidusers at daltontech.co.uk  Wed Dec 19 17:21:59 2018
From: squidusers at daltontech.co.uk (Squid users)
Date: Wed, 19 Dec 2018 17:21:59 +0000
Subject: [squid-users] Advice - Squid Proxy
In-Reply-To: <201812191717.27006.Antony.Stone@squid.open.source.it>
References: <AM6PR08MB3784A7A46CE95BF58F9C679C8DBE0@AM6PR08MB3784.eurprd08.prod.outlook.com>
 <201812191419.24315.Antony.Stone@squid.open.source.it>
 <AM6PR08MB3784398A12304D72CD4194BF8DBE0@AM6PR08MB3784.eurprd08.prod.outlook.com>
 <201812191717.27006.Antony.Stone@squid.open.source.it>
Message-ID: <AM6PR08MB37842F32D25FD2FEAF9BEA988DBE0@AM6PR08MB3784.eurprd08.prod.outlook.com>

> So, Squid is installed on an Ubuntu VM, which runs on your laptop?
Correct

> So, the phone is either - direct connection via mobile Internet access, or via Squid and your home Internet connection - no way for the phone to use the Internet connection without going via Squid?
Yeah - however I use bitdefender on top of squid. Once the phone detects and connects to my laptop it then uses the proxy server

> Configured it in Squid, so users have to authenticate there to get access?
Yeah - I have an ACL running in Squid

> So, where do any other devices (phone, TV, the three VMs) get their IP addresses from?  They must have them, otherwise they couldn't communicate with Squid...  What do these devices have as a gateway address?
I use dhcp allocated from ubuntu, the gateway address that?s broadcast is my Ubuntu address.


 I'm writing this and thinking I've gone a bit Orwellian. Still I think I've covered the bases. I was toying with the idea of running Asterix off my laptop too, but I figured I'd start with this project.

-----Original Message-----
From: squid-users <squid-users-bounces at lists.squid-cache.org> On Behalf Of Antony Stone
Sent: 19 December 2018 16:17
To: squid-users at lists.squid-cache.org
Subject: Re: [squid-users] Advice - Squid Proxy

On Wednesday 19 December 2018 at 16:04:36, Squid users wrote:

> Hi,
> 
> Re network diagram - Mish Mash / blended / spaghetti  I think :p
> 
> Squid is installed on the Ubuntu virtual machine. Sorry forgot to draw 
> that on.

So, Squid is installed on an Ubuntu VM, which runs on your laptop?

> The phone connects to mobile internet when out of the house, then 
> reverts back to going via squid proxy when my laptop wifi is turned 
> on. The phone detects my laptop and connects accordingly. The phone 
> reconfigures to go via proxy when it connects to my laptop.

So, the phone is either - direct connection via mobile Internet access, or via Squid and your home Internet connection - no way for the phone to use the Internet connection without going via Squid?

> As for the TV - yeah my laptop needs to be in the house for that to work.

Okay.

> Internet Use - I'm happy to record websites called by 'user' so for
> example: Tv=user1
> Phone=user2
> Laptop user=user3
> Then each family member with their own user id /password.
> I've configured this bit already

Configured it in Squid, so users have to authenticate there to get access?

> I have set my home internet router to only allocate my laptop mac a 
> DHCP address....

So, where do any other devices (phone, TV, the three VMs) get their IP addresses from?  They must have them, otherwise they couldn't communicate with Squid...  What do these devices have as a gateway address?

> I'll draw a better diagram later today.

Okay.

> I may have gone a bit overboard with the control and monitoring :s

Yes, maybe :)


Antony.

--
Software development can be quick, high quality, or low cost.

The customer gets to pick any two out of three.

                                                   Please reply to the list;
                                                         please *don't* CC me.
_______________________________________________
squid-users mailing list
squid-users at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-users

From patrick.chemla at performance-managers.com  Wed Dec 19 18:29:25 2018
From: patrick.chemla at performance-managers.com (Patrick Chemla)
Date: Wed, 19 Dec 2018 20:29:25 +0200
Subject: [squid-users] Multiple SSL certificates on same IP
Message-ID: <001a1d99-bc22-149b-753c-d55aa74ba46a@performance-managers.com>

An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20181219/c5c9023a/attachment.htm>

From squidusers at daltontech.co.uk  Wed Dec 19 18:35:00 2018
From: squidusers at daltontech.co.uk (Squid users)
Date: Wed, 19 Dec 2018 18:35:00 +0000
Subject: [squid-users] Multiple SSL certificates on same IP
In-Reply-To: <001a1d99-bc22-149b-753c-d55aa74ba46a@performance-managers.com>
References: <001a1d99-bc22-149b-753c-d55aa74ba46a@performance-managers.com>
Message-ID: <AM6PR08MB3784C792DA64EBF483D89A0B8DBE0@AM6PR08MB3784.eurprd08.prod.outlook.com>

Could you

A ? forward to different ports
B ? Use Network address translation?

Thoughts?

From: squid-users <squid-users-bounces at lists.squid-cache.org> On Behalf Of Patrick Chemla
Sent: 19 December 2018 18:29
To: squid-users at lists.squid-cache.org
Subject: [squid-users] Multiple SSL certificates on same IP

Hi all,

Thanks for the great work you do/provide with squid.

I am using squid for years, I like it very much, and I am now installing a SSL load-balancing unit for about 80 domains/sub-domains.

My OS release is Fedora release 29 (Twenty Nine)

My squid version and parameters are :

# squid -v
Squid Cache: Version 4.4
Service Name: squid

This binary uses OpenSSL 1.1.1-pre9 (beta) FIPS 21 Aug 2018. For legal restrictions on distribution see https://www.openssl.org/source/license.html

configure options:  '--build=x86_64-redhat-linux-gnu' '--host=x86_64-redhat-linux-gnu' '--program-prefix=' '--prefix=/usr' '--exec-prefix=/usr' '--bindir=/usr/bin' '--sbindir=/usr/sbin' '--sysconfdir=/etc' '--datadir=/usr/share' '--includedir=/usr/include' '--libdir=/usr/lib64' '--libexecdir=/usr/libexec' '--sharedstatedir=/var/lib' '--mandir=/usr/share/man' '--infodir=/usr/share/info' '--exec_prefix=/usr' '--libexecdir=/usr/lib64/squid' '--localstatedir=/var' '--datadir=/usr/share/squid' '--sysconfdir=/etc/squid' '--with-logdir=/var/log/squid' '--with-pidfile=/var/run/squid.pid' '--disable-dependency-tracking' '--enable-eui' '--enable-follow-x-forwarded-for' '--enable-auth' '--enable-auth-basic=DB,fake,getpwnam,LDAP,NCSA,PAM,POP3,RADIUS,SASL,SMB,SMB_LM' '--enable-auth-ntlm=SMB_LM,fake' '--enable-auth-digest=file,LDAP' '--enable-auth-negotiate=kerberos' '--enable-external-acl-helpers=LDAP_group,time_quota,session,unix_group,wbinfo_group,kerberos_ldap_group' '--enable-storeid-rewrite-helpers=file' '--enable-cache-digests' '--enable-cachemgr-hostname=localhost' '--enable-delay-pools' '--enable-epoll' '--enable-icap-client' '--enable-ident-lookups' '--enable-linux-netfilter' '--enable-removal-policies=heap,lru' '--enable-snmp' '--enable-ssl' '--enable-ssl-crtd' '--enable-storeio=aufs,diskd,ufs,rock' '--enable-diskio' '--enable-wccpv2' '--enable-esi' '--enable-ecap' '--with-aio' '--with-default-user=squid' '--with-dl' '--with-openssl' '--with-pthreads' '--disable-arch-native' '--with-pic' '--disable-security-cert-validators' 'build_alias=x86_64-redhat-linux-gnu' 'host_alias=x86_64-redhat-linux-gnu' 'CFLAGS=-O2 -g -pipe -Wall -Werror=format-security -Wp,-D_FORTIFY_SOURCE=2 -Wp,-D_GLIBCXX_ASSERTIONS -fexceptions -fstack-protector-strong -grecord-gcc-switches -specs=/usr/lib/rpm/redhat/redhat-hardened-cc1 -specs=/usr/lib/rpm/redhat/redhat-annobin-cc1 -m64 -mtune=generic -fasynchronous-unwind-tables -fstack-clash-protection -fcf-protection -fPIC' 'LDFLAGS=-Wl,-z,relro   -Wl,-z,now -specs=/usr/lib/rpm/redhat/redhat-hardened-ld -pie -Wl,-z,relro -Wl,-z,now -Wl,--warn-shared-textrel' 'CXXFLAGS=-O2 -g -pipe -Wall -Werror=format-security -Wp,-D_FORTIFY_SOURCE=2 -Wp,-D_GLIBCXX_ASSERTIONS -fexceptions -fstack-protector-strong -grecord-gcc-switches -specs=/usr/lib/rpm/redhat/redhat-hardened-cc1 -specs=/usr/lib/rpm/redhat/redhat-annobin-cc1 -m64 -mtune=generic -fasynchronous-unwind-tables -fstack-clash-protection -fcf-protection -fPIC' 'PKG_CONFIG_PATH=:/usr/lib64/pkgconfig:/usr/share/pkgconfig'

The problem I have is that all these domains are actually on one IP only, on a single server, running nginx with multiple SSL certificates on one single IP, and I would like to do the same with squid.

I did few years ago with HaProxy, but I would prefer to keep squid.

3 choices:

- Having more than one IP on the server, create SSL certificates from LetsEncrypt including each a list of some domains and sub-domains
- Create a very bing certificate to have squid using it (not the best choice because domains are of different content, far one to the other)
- Having squid managing all certificates on a single IP. (The best because some domains have very high encryption needs, and LetsEncrypt is not their preference)

Like a bottle in the sea: Is that possible, multiple certificates, with squid 4.4 on a single IP?

Thanks for your help.

Patrick

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20181219/ec244495/attachment.htm>

From bruno.larini at riosoft.com.br  Wed Dec 19 20:29:37 2018
From: bruno.larini at riosoft.com.br (Bruno de Paula Larini)
Date: Wed, 19 Dec 2018 18:29:37 -0200
Subject: [squid-users] Multiple SSL certificates on same IP
In-Reply-To: <001a1d99-bc22-149b-753c-d55aa74ba46a@performance-managers.com>
References: <001a1d99-bc22-149b-753c-d55aa74ba46a@performance-managers.com>
Message-ID: <3beec5ac-777e-17ad-6f1d-169eacd95b5a@riosoft.com.br>

Em 19/12/2018 16:29, Patrick Chemla escreveu:
>
> - Having more than one IP on the server, create SSL certificates from 
> LetsEncrypt including each a list of some domains and sub-domains
>
> - Create a very bing certificate to have squid using it (not the best 
> choice because domains are of different content, far one to the other)
>
> - Having squid managing all certificates on a single IP. (The best 
> because some domains have very high encryption needs, and LetsEncrypt 
> is not their preference)
>
>
> Like a bottle in the sea: Is that possible, multiple certificates, 
> with squid 4.4 on a single IP?
>
>
Based on what I had researched recently, Squid still doesn't handle SNI 
in accel mode, so you could set different, non-wildcard certificates to 
the websites. See: 
http://squid-web-proxy-cache.1019090.n4.nabble.com/Squid-4-0-x-SNI-Support-td4682018.html
But it would be nice if Amos could confirm if this information is still 
true for 4.4.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20181219/04e96852/attachment.htm>

From squid3 at treenet.co.nz  Wed Dec 19 22:09:05 2018
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 20 Dec 2018 11:09:05 +1300
Subject: [squid-users] Multiple SSL certificates on same IP
In-Reply-To: <3beec5ac-777e-17ad-6f1d-169eacd95b5a@riosoft.com.br>
References: <001a1d99-bc22-149b-753c-d55aa74ba46a@performance-managers.com>
 <3beec5ac-777e-17ad-6f1d-169eacd95b5a@riosoft.com.br>
Message-ID: <0f870713-6fa9-a7d1-d44d-e09a2274fc12@treenet.co.nz>

On 20/12/18 9:29 am, Bruno de Paula Larini wrote:
> Em 19/12/2018 16:29, Patrick Chemla escreveu:
>>
>> - Having more than one IP on the server, create SSL certificates from
>> LetsEncrypt including each a list of some domains and sub-domains
>>
>> - Create a very bing certificate to have squid using it (not the best
>> choice because domains are of different content, far one to the other)
>>
>> - Having squid managing all certificates on a single IP. (The best
>> because some domains have very high encryption needs, and LetsEncrypt
>> is not their preference)
>>
>>
>> Like a bottle in the sea: Is that possible, multiple certificates,
>> with squid 4.4 on a single IP?
>>
>>
> Based on what I had researched recently, Squid still doesn't handle SNI
> in accel mode, so you could set different, non-wildcard certificates to
> the websites. See:
> http://squid-web-proxy-cache.1019090.n4.nabble.com/Squid-4-0-x-SNI-Support-td4682018.html
> But it would be nice if Amos could confirm if this information is still
> true for 4.4.
> 


There has been some progress in that I have now tested this behaviour
both with multiple certs in different files and sharing a PEM file.


OpenSSL definitely can use only one certificate per http(s)_port. Either
the _last_ loaded if several PEM files are loaded (each call to the
OpenSSL API *replaces* the certs loaded), or if one tries to work around
that by merging everything into a single PEM and only loading it all at
once - only the _first_ cert chain is ever used from that set.

There also does not appear to be any alternative API capable of loading
multiple certs into a single security context and having them used as
leaf certs. If anyone is aware of such a mechanism I would *greatly*
appreciate hearing about it.


On the other hand the GnuTLS mechanism can simply load as many PEM's as
one wants with a single cert chain in each - it "just works". Providing
the appropriate cert chain for any requested domain in its serverHello,
or the first cert loaded if the domain has no cert at all.


FYI; there are other bugs apparently with the GnuTLS priority-string
settings (the tls-options= and tls-min-version=) which may prevent
advanced TLS tuning. And of course with GnuTLS builds one cannot yet
have a dual-purpose proxy also doing SSL-Bump on some traffic (if that
matters). So, YMMV as to whether GnuTLS is worthwhile switching to right
now.

If you do choose to switch the squid.conf for this feature in a GnuTLS
build would look like:

 https_port 443 accel \
    cert=/etc/squid/tls/default.example.com.pem \
    cert=/etc/squid/tls/example.net.pem \
    cert=/etc/squid/tls/example.org.pem \

 ... and so on with a PEM for each domain served by that port.

You should be able to reduce the list a bit by using wildcard certs for
the sub-domains, but I have not tested that possibility yet.


Amos


From squidusers at daltontech.co.uk  Thu Dec 20 10:06:58 2018
From: squidusers at daltontech.co.uk (Squid users)
Date: Thu, 20 Dec 2018 10:06:58 +0000
Subject: [squid-users] SSL / TLS
Message-ID: <AM6PR08MB37840F465B55B9E63CCD382C8DBF0@AM6PR08MB3784.eurprd08.prod.outlook.com>

Slightly off topic but am I correct in thinking TLS supersedes SSL?
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20181220/2488fa88/attachment.htm>

From Antony.Stone at squid.open.source.it  Thu Dec 20 10:12:52 2018
From: Antony.Stone at squid.open.source.it (Antony Stone)
Date: Thu, 20 Dec 2018 11:12:52 +0100
Subject: [squid-users] SSL / TLS
In-Reply-To: <AM6PR08MB37840F465B55B9E63CCD382C8DBF0@AM6PR08MB3784.eurprd08.prod.outlook.com>
References: <AM6PR08MB37840F465B55B9E63CCD382C8DBF0@AM6PR08MB3784.eurprd08.prod.outlook.com>
Message-ID: <201812201112.53101.Antony.Stone@squid.open.source.it>

On Thursday 20 December 2018 at 11:06:58, Squid users wrote:

> Slightly off topic but am I correct in thinking TLS supersedes SSL?

Short answer: yes.

Long answer: https://en.wikipedia.org/wiki/Transport_Layer_Security


Antony.

-- 
#define SIX 1+5
#define NINE 8+1

int main() {
    printf("%d\n", SIX * NINE);
}
	- thanks to ECB for bringing this to my attention

                                                   Please reply to the list;
                                                         please *don't* CC me.


From bruno.larini at riosoft.com.br  Thu Dec 20 12:45:49 2018
From: bruno.larini at riosoft.com.br (Bruno de Paula Larini)
Date: Thu, 20 Dec 2018 10:45:49 -0200
Subject: [squid-users] Multiple SSL certificates on same IP
In-Reply-To: <0f870713-6fa9-a7d1-d44d-e09a2274fc12@treenet.co.nz>
References: <001a1d99-bc22-149b-753c-d55aa74ba46a@performance-managers.com>
 <3beec5ac-777e-17ad-6f1d-169eacd95b5a@riosoft.com.br>
 <0f870713-6fa9-a7d1-d44d-e09a2274fc12@treenet.co.nz>
Message-ID: <0fdf6230-f435-fb14-dab1-5342c4a88318@riosoft.com.br>

Em 19/12/2018 20:09, Amos Jeffries escreveu:
> OpenSSL definitely can use only one certificate per http(s)_port. Either
> the _last_ loaded if several PEM files are loaded (each call to the
> OpenSSL API *replaces* the certs loaded), or if one tries to work around
> that by merging everything into a single PEM and only loading it all at
> once - only the _first_ cert chain is ever used from that set.
Sorry for maybe going a bit off-topic, just curious about it.
I'm mostly clueless about the implications and intricacies of "behind 
the scenes" of SNI, but most modern webservers support it (Apache, 
nginx, IIS). Apache, for instance, says it should be built with "OpenSSL 
with the TLS Extensions option enabled", since OpenSSL v0.9.8f. And 
their configuration for Virtual Hosts and SSL/TLS is rather simple on a 
user's view .

So, my question would be: why Squid would have problems with SNI and 
OpenSSL when other webservers/proxies have this feature using 
OpenSSL/LibreSSL libs?

In my (user's) opinion, Squid has far more complex features with SSL 
Bump and other forward proxy handling for SSL/TLS. Why SNI would be such 
a big deal?

-Bruno



From russel_mcdonald at swbell.net  Thu Dec 20 14:46:54 2018
From: russel_mcdonald at swbell.net (Russel McDonald)
Date: Thu, 20 Dec 2018 14:46:54 +0000 (UTC)
Subject: [squid-users] Windows Squid built using MinGW and running,
 trying to get adapter linked in at run time
References: <243801846.716845.1545317214995.ref@mail.yahoo.com>
Message-ID: <243801846.716845.1545317214995@mail.yahoo.com>

Hi,
I switched from Cygwin to building with MinGW, and after 8 squid code modifications, mostly include and define settings but one flat out access violation crash, I now have Squid proxy running and accessible via browser on Windows :)
And now I have built the ecap library, successfully rebuilt squid with --enable-ecap,? and built the adapter.All good!
However I'm at the apparent crux of getting the adapter loaded at squid run time and failing.
I understand that(1) squid needs to link in the same exact ecap library binary that the adapter does so they can communicate correctly through that ecap,(2) we don't want to statically link in the adapter to squid.exe,(3) so that adapter needs to be a shared such library, but(4) building with MinGW creates a static library.?
And I see a comment output during make:
> *** Warning: This system can not link to static lib archive
> /usr/local/lib/libecap.la. *** I have the capability to make that
> library automatically link in when *** you link to this library.
> But I can only do this if you have a *** shared version of the
> library, which you do not appear to have. *** But as you try to
> build a module library, libtool will still create *** a static
> module, that should work as long as the dlopening application ***
> is linked with the -dlopen flag to resolve symbols at runtime.

So I did use this ./configure for squid, specifying the dlopen:
./configure LDFLAGS="-dlopen=C:/MinGW/src/ecap_adapter_sample-1.0.0/src/.libs/ecap_adapter_modifying.la" --prefix=c:/squid? --enable-ecap --enable-default-hostsfile=none --disable-strict-error-checking? --enable-win32-service --disable-optimizations --enable-debug-cbdata --with-pidfile=/var/run/squid.pid --enable-delay-pools --disable-eui --with-filedescriptors=65536 --enable-removal-policies=lru,heap
my understanding further being that the dlopen will cause squid to not link in the adapter until run time even though that library is static. And since it is not a native Windows format library (not a dll) then squid has to run in a MinGW window. Correct?
Yet I still get:2018/12/19 22:57:32| Loading Squid module from 'C:/MinGW/src/ecap_adapter_sample-1.0.0/src/.libs/ecap_adapter_modifying.la'2018/12/19 22:57:32| FATAL: dying from an unhandled exception: file not foundterminate called after throwing an instance of 'TextException'? what():? file not found
If I change my squid.conf instead to load in .a instead of .la just as a test, I get this:2018/12/20 08:39:58| Loading Squid module from 'C:/MinGW/src/ecap_adapter_sample-1.0.0/src/.libs/ecap_adapter_modifying.a'2018/12/20 08:39:58| FATAL: dying from an unhandled exception: %1 is not a valid?Win32 application.terminate called after throwing an instance of 'TextException'? what():? %1 is not a valid Win32 application.
So I know it's actually accessing that .la, which does exist at that location, yet still complains that "file not found".Is it as well trying to link in another library that the adapter needs?
Russel

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20181220/287d860e/attachment.htm>

From rousskov at measurement-factory.com  Thu Dec 20 15:48:10 2018
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Thu, 20 Dec 2018 08:48:10 -0700
Subject: [squid-users] SSL / TLS
In-Reply-To: <AM6PR08MB37840F465B55B9E63CCD382C8DBF0@AM6PR08MB3784.eurprd08.prod.outlook.com>
References: <AM6PR08MB37840F465B55B9E63CCD382C8DBF0@AM6PR08MB3784.eurprd08.prod.outlook.com>
Message-ID: <dea05238-b28d-7267-44ae-7900a4f8fe49@measurement-factory.com>

On 12/20/18 3:06 AM, Squid users wrote:
> Slightly off topic but am I correct in thinking TLS supersedes SSL?

Yes, the protocol name has changed. Newer versions are called TLS.

However, please keep in mind that the term "SSL" is commonly used to
describe "secure" connections and related technologies, regardless of
the specific protocol being used for that security. Squid still uses
that classic "SSL" terminology, especially in old documentation and
code. In most cases where exact identifiers are not required, the
letters "SSL" and "TLS" are used interchangeably.

Alex.


From rousskov at measurement-factory.com  Thu Dec 20 16:26:35 2018
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Thu, 20 Dec 2018 09:26:35 -0700
Subject: [squid-users] Multiple SSL certificates on same IP
In-Reply-To: <0fdf6230-f435-fb14-dab1-5342c4a88318@riosoft.com.br>
References: <001a1d99-bc22-149b-753c-d55aa74ba46a@performance-managers.com>
 <3beec5ac-777e-17ad-6f1d-169eacd95b5a@riosoft.com.br>
 <0f870713-6fa9-a7d1-d44d-e09a2274fc12@treenet.co.nz>
 <0fdf6230-f435-fb14-dab1-5342c4a88318@riosoft.com.br>
Message-ID: <4dcac567-2755-b1f9-9677-0844a6354b23@measurement-factory.com>

On 12/20/18 5:45 AM, Bruno de Paula Larini wrote:
> why Squid would have problems with SNI and
> OpenSSL when other webservers/proxies have this feature using
> OpenSSL/LibreSSL libs?

Squid lacks the necessary code to support SNI in accelerator mode when
using OpenSSL.


> Why SNI would be such a big deal?

SNI support with OpenSSL is not a "big deal"[1]. Apparently, nobody has
needed that support badly enough to either add that support or sponsor
that addition.

[1]
https://stackoverflow.com/questions/5113333/how-to-implement-server-name-indication-sni

Alex.


From rousskov at measurement-factory.com  Thu Dec 20 16:40:23 2018
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Thu, 20 Dec 2018 09:40:23 -0700
Subject: [squid-users] Windows Squid built using MinGW and running,
 trying to get adapter linked in at run time
In-Reply-To: <243801846.716845.1545317214995@mail.yahoo.com>
References: <243801846.716845.1545317214995.ref@mail.yahoo.com>
 <243801846.716845.1545317214995@mail.yahoo.com>
Message-ID: <7d9d3f52-7fe4-383b-3330-21d3d9e6f6ca@measurement-factory.com>

On 12/20/18 7:46 AM, Russel McDonald wrote:

> 2018/12/19 22:57:32| Loading Squid module from 'C:/MinGW/src/ecap_adapter_sample-1.0.0/src/.libs/ecap_adapter_modifying.la'
> 2018/12/19 22:57:32| FATAL: dying from an unhandled exception: file not found

I do not know enough to help with Windows-specific questions, but please
note that the "file not found" error in this context is often lying.
IIRC, the libtool library that Squid uses to handle dynamic library
loading returns an essentially invalid/incorrect error message on many
dynamic loading errors. This is not something Squid itself can fix.

On Unix, one can use environment variables like LD_DEBUG=ALL and
LD_WARN=1 to see why the dynamic loader is unhappy with the adapter
library. I do not know whether something like that works on Windows.

Alex.


From info at schroeffu.ch  Fri Dec 21 15:42:44 2018
From: info at schroeffu.ch (info at schroeffu.ch)
Date: Fri, 21 Dec 2018 15:42:44 +0000
Subject: [squid-users] Squid 4.4 + sslbump cannot open specific URL: ESI
 Processing failed
In-Reply-To: <201812201112.53101.Antony.Stone@squid.open.source.it>
References: <201812201112.53101.Antony.Stone@squid.open.source.it>
 <AM6PR08MB37840F465B55B9E63CCD382C8DBF0@AM6PR08MB3784.eurprd08.prod.outlook.com>
Message-ID: <b2de1be341c64aa5219375e5d2e93897@schroeffu.ch>

Hi all,

I am getting the following error while opening https://www.hawesko.de with Squid 4.4 and sslbump.
Deactivate bumping makes the error disappear.

Error:
----------
The following error was encountered while trying to retrieve the URL: https://www.hawesko.de

ESI Processing failed.

The ESI processor returned:

esiProcess: Parse error at line 1:
not well-formed (invalid token)
This means that the surrogate was not able to process the ESI template. Please report this error to
the webmaster.
-----------

The only "special" thing I see is, this target website is using a wildcard certificate.

Anyone else can reproduce the issue with its squid 4.4 and sslbump?

Many Regards!
Schroeffu


From squid3 at treenet.co.nz  Fri Dec 21 17:34:44 2018
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sat, 22 Dec 2018 06:34:44 +1300
Subject: [squid-users] Squid 4.4 + sslbump cannot open specific URL: ESI
 Processing failed
In-Reply-To: <b2de1be341c64aa5219375e5d2e93897@schroeffu.ch>
References: <201812201112.53101.Antony.Stone@squid.open.source.it>
 <AM6PR08MB37840F465B55B9E63CCD382C8DBF0@AM6PR08MB3784.eurprd08.prod.outlook.com>
 <b2de1be341c64aa5219375e5d2e93897@schroeffu.ch>
Message-ID: <8c93fc58-bacf-5be4-4f82-95c120b8a1d5@treenet.co.nz>

On 22/12/18 4:42 am, info at schroeffu.ch wrote:
> Hi all,
> 
> I am getting the following error while opening https://www.hawesko.de with Squid 4.4 and sslbump.
> Deactivate bumping makes the error disappear.
> 
> Error:
> ----------
> The following error was encountered while trying to retrieve the URL: https://www.hawesko.de
> 
> ESI Processing failed.
> 

<https://bugs.squid-cache.org/show_bug.cgi?id=4880>

A quick check confirms the server is producing ESI Surrogate headers
without having been asked and without the content actually being ESI. It
is a known bug that SSL-Bump enables the ESI logic. A patch can be found
at <https://github.com/squid-cache/squid/pull/304>

Amos


From chip_pop at hotmail.com  Sun Dec 23 15:36:46 2018
From: chip_pop at hotmail.com (joseph)
Date: Sun, 23 Dec 2018 09:36:46 -0600 (CST)
Subject: [squid-users] mgr:store_id
Message-ID: <1545579406417-0.post@n4.nabble.com>

wen issuing squidclient  mgr:store_id
the   time value   1.000  is 1ms   or?



-----
************************** 
***** Crash to the future  ****
**************************
--
Sent from: http://squid-web-proxy-cache.1019090.n4.nabble.com/Squid-Users-f1019091.html


From rousskov at measurement-factory.com  Sun Dec 23 17:39:15 2018
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Sun, 23 Dec 2018 10:39:15 -0700
Subject: [squid-users] mgr:store_id
In-Reply-To: <1545579406417-0.post@n4.nabble.com>
References: <1545579406417-0.post@n4.nabble.com>
Message-ID: <ec6a604c-e2ec-d502-eef6-15d0baa237b8@measurement-factory.com>

On 12/23/18 8:36 AM, joseph wrote:
> wen issuing squidclient  mgr:store_id
> the   time value   1.000  is 1ms   or?

AFAICT, the values in the "Time" column are reported in seconds (with
millisecond precision).

Helper response times are measured with microsecond precision. Quality
pull requests increasing reported precision are welcomed. See
helper::packStatsInto() for a good starting point.

The above applies to most (all?) helpers, including store_id.

Please consider adding a new wiki page to document the above. Similar
pages are linked from
https://wiki.squid-cache.org/Features/CacheManager#Available_Reports


Thank you,

Alex.


From chip_pop at hotmail.com  Sun Dec 23 19:35:36 2018
From: chip_pop at hotmail.com (joseph)
Date: Sun, 23 Dec 2018 13:35:36 -0600 (CST)
Subject: [squid-users] mgr:store_id
In-Reply-To: <ec6a604c-e2ec-d502-eef6-15d0baa237b8@measurement-factory.com>
References: <1545579406417-0.post@n4.nabble.com>
 <ec6a604c-e2ec-d502-eef6-15d0baa237b8@measurement-factory.com>
Message-ID: <1545593736009-0.post@n4.nabble.com>

so 1.000  is sec thank you im trying to have my graph correct response time
thank you for your help



-----
************************** 
***** Crash to the future  ****
**************************
--
Sent from: http://squid-web-proxy-cache.1019090.n4.nabble.com/Squid-Users-f1019091.html


From marko.cupac at mimar.rs  Mon Dec 24 10:41:25 2018
From: marko.cupac at mimar.rs (Marko =?UTF-8?B?Q3VwYcSH?=)
Date: Mon, 24 Dec 2018 11:41:25 +0100
Subject: [squid-users] Squid 3.5.28 can't be built on FreeBSD 12.0-RELEASE
Message-ID: <20181224114125.5e16c10e@mephala.kappastar.com>

Hi,

it appears that Squid 3.5.28 (current FreeBSD port for www/squid3)
can't be built on latest FreeBSD release. The problem appears to be
related to openssl version in FreeBSD base, which is "OpenSSL
1.1.1a-freebsd  20 Nov 2018" according to `openssl version`.

Are there plans to patch Squid 3.5.X so it works on FreeBSD 12-RELEASE?

Here's excerpt from poudriere log:

---log-excerpt-start---
Making all in anyp
depbase=`echo PortCfg.lo | sed
's|[^/]*$|.deps/&|;s|\.lo$||'`; /bin/sh ../../libtool  --tag=CXX
--mode=compile c++ -DHAVE_CONFIG_H    -I../.. -I../../include
-I../../lib -I../../src  -I../../include  -I/usr/include
-I/usr/include  -I../../libltdl -I/usr/include -I/usr/local/include
-Werror -Qunused-arguments  -D_REENTRANT -I/usr/include -I/usr/include
-O2 -pipe -I/usr/local/include -fstack-protector -DLDAP_DEPRECATED
-fno-strict-aliasing  -Wno-unknown-warning-option
-Wno-undefined-bool-conversion -Wno-tautological-undefined-compare
-Wno-dynamic-class-memaccess  -I/usr/local/include -MT PortCfg.lo -MD
-MP -MF $depbase.Tpo -c -o PortCfg.lo PortCfg.cc && mv -f $depbase.Tpo
$depbase.Plo libtool: compile:  c++ -DHAVE_CONFIG_H -I../..
-I../../include -I../../lib -I../../src -I../../include -I/usr/include
-I/usr/include -I../../libltdl -I/usr/include -I/usr/local/include
-Werror -Qunused-arguments -D_REENTRANT -I/usr/include -I/usr/include
-O2 -pipe -I/usr/local/include -fstack-protector -DLDAP_DEPRECATED
-fno-strict-aliasing -Wno-unknown-warning-option
-Wno-undefined-bool-conversion -Wno-tautological-undefined-compare
-Wno-dynamic-class-memaccess -I/usr/local/include -MT PortCfg.lo -MD
-MP -MF .deps/PortCfg.Tpo -c PortCfg.cc  -fPIC -DPIC -o .libs/PortCfg.o
In file included from PortCfg.cc:10: In file included
from ../../src/anyp/PortCfg.h:18: ../../src/ssl/gadgets.h:83:45: error:
use of undeclared identifier 'CRYPTO_LOCK_X509' typedef
LockingPointer<X509, X509_free_cpp, CRYPTO_LOCK_X509> X509_Pointer;
^ ../../src/ssl/gadgets.h:89:53: error: use of undeclared identifier
'CRYPTO_LOCK_EVP_PKEY' typedef LockingPointer<EVP_PKEY,
EVP_PKEY_free_cpp, CRYPTO_LOCK_EVP_PKEY> EVP_PKEY_Pointer;
^ ../../src/ssl/gadgets.h:116:43: error: use of undeclared identifier
'CRYPTO_LOCK_SSL' typedef LockingPointer<SSL, SSL_free_cpp,
CRYPTO_LOCK_SSL> SSL_Pointer;
---log-excerpt-end---

Thank you in advance,
-- 
Before enlightenment - chop wood, draw water.
After  enlightenment - chop wood, draw water.

Marko Cupa?
https://www.mimar.rs/


From mkraju123 at gmail.com  Tue Dec 25 10:50:11 2018
From: mkraju123 at gmail.com (M K Raju)
Date: Tue, 25 Dec 2018 04:50:11 -0600 (CST)
Subject: [squid-users] Caching Vimeo Videos
In-Reply-To: <039901d48896$a0eb15a0$e2c140e0$@ngtech.co.il>
References: <CAGycgFh+RZq83hsTvGX3Q8M7Mf02pp8jrBX2Re9U87EYqHNjEQ@mail.gmail.com>
 <039901d48896$a0eb15a0$e2c140e0$@ngtech.co.il>
Message-ID: <1545735011205-0.post@n4.nabble.com>

Thanks Eliezer,

Our videos are streaming with HLS Dash only.
Please give me examples with SSL-BUMP and Store ID helper.

Regards.



--
Sent from: http://squid-web-proxy-cache.1019090.n4.nabble.com/Squid-Users-f1019091.html


From squid3 at treenet.co.nz  Tue Dec 25 13:30:49 2018
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Wed, 26 Dec 2018 02:30:49 +1300
Subject: [squid-users] Squid 3.5.28 can't be built on FreeBSD
	12.0-RELEASE
In-Reply-To: <20181224114125.5e16c10e@mephala.kappastar.com>
References: <20181224114125.5e16c10e@mephala.kappastar.com>
Message-ID: <de2ebc4c-e77d-f916-c5ae-bdb6f1a93afa@treenet.co.nz>

On 24/12/18 11:41 pm, Marko Cupa? wrote:
> Hi,
> 
> it appears that Squid 3.5.28 (current FreeBSD port for www/squid3)
> can't be built on latest FreeBSD release. The problem appears to be
> related to openssl version in FreeBSD base, which is "OpenSSL
> 1.1.1a-freebsd  20 Nov 2018" according to `openssl version`.
> 
> Are there plans to patch Squid 3.5.X so it works on FreeBSD 12-RELEASE?
> 

No. Squid-3.5 is no longer supported for anything but security
vulnerabilities.

The current production series (Squid-4) already contains all the code
re-writes for use with OpenSSL-1.1.

Amos


From johnrefwe at mail.com  Thu Dec 27 21:30:45 2018
From: johnrefwe at mail.com (johnr)
Date: Thu, 27 Dec 2018 15:30:45 -0600 (CST)
Subject: [squid-users] Squid 4.4 security_file_certgen helpers crashing
Message-ID: <1545946245329-0.post@n4.nabble.com>

Hi, 

I am having trouble running squid 4.4 on ubuntu 14.04. I have successfully
built squid, and it runs fine if I'm not trying to SSL bump, but once I SSL
bump traffic, it starts crashing.

I've tried various ssl bump configurations with the same net result, so I
don't believe the configuration is relevant, but here it is:
sslcrtd_children 2 startup=2 idle=1
http_port 3129 ssl-bump generate-host-certificates=on
cert=/home/ssl_bump.pem
acl step1 at_step SslBump1
ssl_bump stare step1
ssl_bump bump all

After browsing to a https site, squid crashes and I find the following in
the cache log:
2018/12/27 21:15:40 kid1| WARNING:
/usr/local/squid/libexec/security_file_certgen -s
/usr/local/squid/var/cache/squid/ssl_db -M 4MB #Hlpr1 exited
2018/12/27 21:15:40 kid1| FATAL: The
/usr/local/squid/libexec/security_file_certgen -s
/usr/local/squid/var/cache/squid/ssl_db -M 4MB helpers are crashing too
rapidly, need help!

I ran the security_gen_helper under GDB and it seems to be crashing here:
https://github.com/squid-cache/squid/blob/master/src/ssl/gadgets.cc#L218

My squid version output is as follows:
Squid Cache: Version 4.4
Service Name: squid

This binary uses OpenSSL 1.0.1f 6 Jan 2014. For legal restrictions on
distribution see https://www.openssl.org/source/license.html

configure options:  '--disable-arch-native' '--disable-dependency-tracking'
'--disable-eui' '--enable-auth'
'--enable-basic-auth-helpers=getpwnam,LDAP,PAM'
'--enable-digest-auth-helpers=password'
'--enable-external-acl-helpers=file_userip,unix_group'
'--enable-follow-x-forwarded-for' '--enable-ssl-crtd' '--with-openssl'
'--disable-ipv6'

I specifically mention ubuntu 14.04, because I compiled and ran squid 4.4 on
ubuntu 18.04 with the same config and it ran successfully. I was
successfully able to run squid 4.3 on ubuntu 14.04 and 18.04, so I think
this might be something newly introduced in the code? I saw a commit
supporting a newer version of openssl, I wonder if that may have mistakenly
broken support for older versions of openssl?

Thank you for any help!



--
Sent from: http://squid-web-proxy-cache.1019090.n4.nabble.com/Squid-Users-f1019091.html


From ziprasidone146939277 at gmail.com  Thu Dec 27 22:16:13 2018
From: ziprasidone146939277 at gmail.com (ziprasidone146939277 at gmail.com)
Date: Thu, 27 Dec 2018 19:16:13 -0300
Subject: [squid-users] Squid 4.4 security_file_certgen helpers crashing
In-Reply-To: <1545946245329-0.post@n4.nabble.com>
References: <1545946245329-0.post@n4.nabble.com>
Message-ID: <001401d49e31$c91e4fa0$5b5aeee0$@gmail.com>

> I've tried various ssl bump configurations with the same net result, so I don't
> believe the configuration is relevant, but here it is:

> sslcrtd_children 2 startup=2 idle=1

Try comment this line and then see if it fails/crash.
Note that this directive has a default value which is 32/5/1.

See: http://www.squid-cache.org/Doc/config/sslcrtd_children/

HTH,

Julian




From johnrefwe at mail.com  Thu Dec 27 23:09:02 2018
From: johnrefwe at mail.com (johnr)
Date: Thu, 27 Dec 2018 17:09:02 -0600 (CST)
Subject: [squid-users] Squid 4.4 security_file_certgen helpers crashing
In-Reply-To: <001401d49e31$c91e4fa0$5b5aeee0$@gmail.com>
References: <1545946245329-0.post@n4.nabble.com>
 <001401d49e31$c91e4fa0$5b5aeee0$@gmail.com>
Message-ID: <1545952142353-0.post@n4.nabble.com>

Thanks for the reply Julian.


ziprasidone146939277 wrote
> Try comment this line and then see if it fails/crash.
> Note that this directive has a default value which is 32/5/1.

I tried commenting out this line and saw the same behavior.




--
Sent from: http://squid-web-proxy-cache.1019090.n4.nabble.com/Squid-Users-f1019091.html


From rousskov at measurement-factory.com  Fri Dec 28 03:29:24 2018
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Thu, 27 Dec 2018 20:29:24 -0700
Subject: [squid-users] Squid 4.4 security_file_certgen helpers crashing
In-Reply-To: <1545946245329-0.post@n4.nabble.com>
References: <1545946245329-0.post@n4.nabble.com>
Message-ID: <cdd3e8bf-6a64-91d6-40b6-6e05133d851b@measurement-factory.com>

On 12/27/18 2:30 PM, johnr wrote:

> I find the following in the cache log:

> 2018/12/27 21:15:40 kid1| WARNING:
> /usr/local/squid/libexec/security_file_certgen -s
> /usr/local/squid/var/cache/squid/ssl_db -M 4MB #Hlpr1 exited

We need to figure out why the helper is exiting. If there are no error
messages in cache.log, then your system log may have additional
information such as the process signal that killed the helper. If it was
a crash, then your core dump directory should have the corresponding
core dump (make sure you enable core dumps!) that you can examine with gdb.


> I ran the security_gen_helper under GDB and it seems to be crashing here:
> https://github.com/squid-cache/squid/blob/master/src/ssl/gadgets.cc#L218

If you can reproduce helper crash while it has gdb attached, please post
the stack trace.


> I saw a commit supporting a newer version of openssl, I wonder if
> that may have mistakenly broken support for older versions of
> openssl?

Sure, it may have. Most likely, the changes are not tested in an
environment matching yours, and the bug may be environment-driven.


If you get more details such as a backtrace, please consider filing a
bug report with all the details.


Thank you,

Alex.


From wp.rauchholz at gmail.com  Fri Dec 28 09:18:27 2018
From: wp.rauchholz at gmail.com (Wolfgang Paul Rauchholz)
Date: Fri, 28 Dec 2018 10:18:27 +0100
Subject: [squid-users] Whitelisting youtube
Message-ID: <CAETVtpQqncRJ03KmUOM5pTtuDGqNae6PP5zM9qw+n_4pSCsHpQ@mail.gmail.com>

Problem staqtement: can't whitelist youtube.com

I run squid 3.5 and squiguard on a CENTOS 7 home linux server.
The blacklist database is created by a publicly available script called
getlists.sh. This script downloads and compiles blacklists  from several
sites (e.g. squidguard website)
To whitelist youtube which is blocked too,  I created the directory 'white'
within 'blacklist'. The squidguard config looks like this:

dest white {
        domainlist      white/domains
        urllist         white/urls
}

acl {
        default {
                pass    white !adv !porn !warez all
                redirect http://localhost/block.html
                }
}

the domaon file withi nwhite has these entries:
.2mdn.net:443
.accounts.google.com
.accounts.youtube.com
.dnld.googlevideo.com
.gmail.com:443-
.googleads4.g.doubleclick.net
.googlevideo.com
.i.ytimg.com
.nek.googlevideo.com
.play.google.com
.sb.scorecardresearch.com
.s.ytimg.com
.youtube.com
.ytimg.com

The entry I find in access.lof file reads like this:
1545988674.026      0 10.5.2.96 TAG_NONE/503 0 CONNECT www.youtube.com:443
- HIER_NONE/- -


I still cannot unblock youtube.
I'd appreciate your help in resolving this.

Wolfgang
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20181228/b3929049/attachment.htm>

From marcus.kool at urlfilterdb.com  Fri Dec 28 10:13:35 2018
From: marcus.kool at urlfilterdb.com (Marcus Kool)
Date: Fri, 28 Dec 2018 08:13:35 -0200
Subject: [squid-users] Whitelisting youtube
In-Reply-To: <CAETVtpQqncRJ03KmUOM5pTtuDGqNae6PP5zM9qw+n_4pSCsHpQ@mail.gmail.com>
References: <CAETVtpQqncRJ03KmUOM5pTtuDGqNae6PP5zM9qw+n_4pSCsHpQ@mail.gmail.com>
Message-ID: <2cc4c4f6-9512-1e18-324b-c04c033cbeaa@urlfilterdb.com>

Wolfgang, why don't you stop using squidguard which has no support for 5+ years and switch to ufdbGuard?

ufdbGuard is regularly maintained and has a Reference Manual that explains what and how to whitelist domains.

Marcus


On 28/12/2018 07:18, Wolfgang Paul Rauchholz wrote:
> Problem staqtement: can't whitelist youtube.com <http://youtube.com>
>
> I run squid 3.5 and squiguard on a CENTOS 7 home linux server.
> The blacklist database is created by a publicly?available script called getlists.sh. This script downloads and compiles blacklists? from several sites (e.g. squidguard website)
> To whitelist youtube which is blocked too,? I created the directory 'white' within 'blacklist'. The squidguard config looks like this:
>
> dest white {
> ? ? ? ? domainlist? ? ? white/domains
> ? ? ? ? urllist? ? ? ? ?white/urls
> }
>
> acl {
> ? ? ? ? default {
> ? ? ? ? ? ? ? ? pass? ? white !adv !porn !warez all
> ? ? ? ? ? ? ? ? redirect http://localhost/block.html
> ? ? ? ? ? ? ? ? }
> }
>
> the domaon file withi nwhite has these entries:
> .2mdn.net:443 <http://2mdn.net:443>
> .accounts.google.com <http://accounts.google.com>
> .accounts.youtube.com <http://accounts.youtube.com>
> .dnld.googlevideo.com <http://dnld.googlevideo.com>
> .gmail.com:443-
> .googleads4.g.doubleclick.net <http://googleads4.g.doubleclick.net>
> .googlevideo.com <http://googlevideo.com>
> .i.ytimg.com <http://i.ytimg.com>
> .nek.googlevideo.com <http://nek.googlevideo.com>
> .play.google.com <http://play.google.com>
> .sb.scorecardresearch.com <http://sb.scorecardresearch.com>
> .s.ytimg.com <http://s.ytimg.com>
> .youtube.com <http://youtube.com>
> .ytimg.com <http://ytimg.com>
>
> The entry I find in access.lof file reads like this:
> 1545988674.026? ? ? 0 10.5.2.96 TAG_NONE/503 0 CONNECT www.youtube.com:443 <http://www.youtube.com:443> - HIER_NONE/- -
>
>
> I still cannot?unblock youtube.
> I'd appreciate?your help in resolving this.
>
> Wolfgang
>
>
>
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20181228/9f0e2775/attachment.htm>

From squid3 at treenet.co.nz  Sat Dec 29 13:17:21 2018
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sun, 30 Dec 2018 02:17:21 +1300
Subject: [squid-users] Whitelisting youtube
In-Reply-To: <CAETVtpQqncRJ03KmUOM5pTtuDGqNae6PP5zM9qw+n_4pSCsHpQ@mail.gmail.com>
References: <CAETVtpQqncRJ03KmUOM5pTtuDGqNae6PP5zM9qw+n_4pSCsHpQ@mail.gmail.com>
Message-ID: <fd335c5b-49eb-78a3-c2d4-8876031fc626@treenet.co.nz>

On 28/12/18 10:18 pm, Wolfgang Paul Rauchholz wrote:
> Problem staqtement: can't whitelist youtube.com
> 
...
> The entry I find in access.lof file reads like this:
> 1545988674.026? ? ? 0 10.5.2.96 TAG_NONE/503 0 CONNECT
> www.youtube.com:443 - HIER_NONE/- -
> 

You cannot redirect a CONNECT tunnel with SquidGuard. You should prevent
this methods being sent to the redirector entirely, like so:

 url_rewrite_access deny CONNECT


Also, this is a 503 (unable to connect) error, not a denial. So
whitelisting is pointless.

Amos


From eliezer at ngtech.co.il  Sat Dec 29 17:22:21 2018
From: eliezer at ngtech.co.il (eliezer at ngtech.co.il)
Date: Sat, 29 Dec 2018 19:22:21 +0200
Subject: [squid-users] Whitelisting youtube
In-Reply-To: <2cc4c4f6-9512-1e18-324b-c04c033cbeaa@urlfilterdb.com>
References: <CAETVtpQqncRJ03KmUOM5pTtuDGqNae6PP5zM9qw+n_4pSCsHpQ@mail.gmail.com>
 <2cc4c4f6-9512-1e18-324b-c04c033cbeaa@urlfilterdb.com>
Message-ID: <02ce01d49f9b$0fdebd70$2f9c3850$@ngtech.co.il>

Markus,

 

Does ufdbGuard have a Debian package or build instructions?
The last time I tried to compile it on both Debian and Ubuntu I have encountered couple issues.

 

Thanks,

Eliezer

 

 

----

 <http://ngtech.co.il/main-en/> Eliezer Croitoru
Linux System Administrator
Mobile: +972-5-28704261
Email:  <mailto:eliezer at ngtech.co.il> eliezer at ngtech.co.il



 

From: squid-users <squid-users-bounces at lists.squid-cache.org> On Behalf Of Marcus Kool
Sent: Friday, December 28, 2018 12:14
To: squid-users at lists.squid-cache.org
Subject: Re: [squid-users] Whitelisting youtube

 

Wolfgang, why don't you stop using squidguard which has no support for 5+ years and switch to ufdbGuard?

ufdbGuard is regularly maintained and has a Reference Manual that explains what and how to whitelist domains.

Marcus

 

On 28/12/2018 07:18, Wolfgang Paul Rauchholz wrote:

Problem staqtement: can't whitelist youtube.com <http://youtube.com> 

 

I run squid 3.5 and squiguard on a CENTOS 7 home linux server.

The blacklist database is created by a publicly available script called getlists.sh. This script downloads and compiles blacklists  from several sites (e.g. squidguard website)

To whitelist youtube which is blocked too,  I created the directory 'white' within 'blacklist'. The squidguard config looks like this:

 

dest white {

        domainlist      white/domains

        urllist         white/urls

}

 

acl {

        default {

                pass    white !adv !porn !warez all

                redirect http://localhost/block.html

                }

}

 

the domaon file withi nwhite has these entries:

.2mdn.net:443 <http://2mdn.net:443> 

.accounts.google.com <http://accounts.google.com> 

.accounts.youtube.com <http://accounts.youtube.com> 

.dnld.googlevideo.com <http://dnld.googlevideo.com> 

.gmail.com:443-

.googleads4.g.doubleclick.net <http://googleads4.g.doubleclick.net> 

.googlevideo.com <http://googlevideo.com> 

.i.ytimg.com <http://i.ytimg.com> 

.nek.googlevideo.com <http://nek.googlevideo.com> 

.play.google.com <http://play.google.com> 

.sb.scorecardresearch.com <http://sb.scorecardresearch.com> 

.s.ytimg.com <http://s.ytimg.com> 

.youtube.com <http://youtube.com> 

.ytimg.com <http://ytimg.com> 

 

The entry I find in access.lof file reads like this:

1545988674.026      0 10.5.2.96 TAG_NONE/503 0 CONNECT www.youtube.com:443 <http://www.youtube.com:443>  - HIER_NONE/- -

 

 

I still cannot unblock youtube.

I'd appreciate your help in resolving this.

 

Wolfgang

 

 

 





_______________________________________________
squid-users mailing list
squid-users at lists.squid-cache.org <mailto:squid-users at lists.squid-cache.org> 
http://lists.squid-cache.org/listinfo/squid-users

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20181229/73f2396b/attachment.htm>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: image001.png
Type: image/png
Size: 11295 bytes
Desc: not available
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20181229/73f2396b/attachment.png>

From marcus.kool at urlfilterdb.com  Sat Dec 29 20:50:49 2018
From: marcus.kool at urlfilterdb.com (Marcus Kool)
Date: Sat, 29 Dec 2018 18:50:49 -0200
Subject: [squid-users] Whitelisting youtube
In-Reply-To: <02ce01d49f9b$0fdebd70$2f9c3850$@ngtech.co.il>
References: <CAETVtpQqncRJ03KmUOM5pTtuDGqNae6PP5zM9qw+n_4pSCsHpQ@mail.gmail.com>
 <2cc4c4f6-9512-1e18-324b-c04c033cbeaa@urlfilterdb.com>
 <02ce01d49f9b$0fdebd70$2f9c3850$@ngtech.co.il>
Message-ID: <633500a1-1f36-745f-fbd3-e3be91696d30@urlfilterdb.com>

Hi Eliezer,

If you mean compiler errors on debian 9 which has OpenSSL 1.1 ...

We will release ufdbGuard 1.34 soon which supports OpenSSL 1.1 since OpenSSL 1.1 is not compatible with OpenSSL 1.0.

Marcus


On 29/12/2018 15:22, eliezer at ngtech.co.il wrote:
>
> Markus,
>
> Does ufdbGuard have a Debian package or build instructions?
> The last time I tried to compile it on both Debian and Ubuntu I have encountered couple issues.
>
> Thanks,
>
> Eliezer
>
> ----
>
> Eliezer Croitoru <http://ngtech.co.il/main-en/>
> Linux System Administrator
> Mobile: +972-5-28704261
> Email: eliezer at ngtech.co.il <mailto:eliezer at ngtech.co.il>
>
> cid:image001.png at 01D2675E.DCF360D0
>
> *From:* squid-users <squid-users-bounces at lists.squid-cache.org> *On Behalf Of *Marcus Kool
> *Sent:* Friday, December 28, 2018 12:14
> *To:* squid-users at lists.squid-cache.org
> *Subject:* Re: [squid-users] Whitelisting youtube
>
> Wolfgang, why don't you stop using squidguard which has no support for 5+ years and switch to ufdbGuard?
>
> ufdbGuard is regularly maintained and has a Reference Manual that explains what and how to whitelist domains.
>
> Marcus
>
> On 28/12/2018 07:18, Wolfgang Paul Rauchholz wrote:
>
>     Problem staqtement: can't whitelist youtube.com <http://youtube.com>
>
>     I run squid 3.5 and squiguard on a CENTOS 7 home linux server.
>
>     The blacklist database is created by a publicly?available script called getlists.sh. This script downloads and compiles blacklists? from several sites (e.g. squidguard website)
>
>     To whitelist youtube which is blocked too,? I created the directory 'white' within 'blacklist'. The squidguard config looks like this:
>
>     dest white {
>
>     ? ? ? domainlist? ? ? white/domains
>
>     ? ? ? urllist? ? ? ? ?white/urls
>
>     }
>
>     acl {
>
>     ? ? ? default {
>
>     ? ? ? ? ? ? ? pass? ? white !adv !porn !warez all
>
>     ? ? ? ? ? ? ? redirect http://localhost/block.html
>
>     ? ? ? ? ? ? ? }
>
>     }
>
>     the domaon file withi nwhite has these entries:
>
>     .2mdn.net:443 <http://2mdn.net:443>
>
>     .accounts.google.com <http://accounts.google.com>
>
>     .accounts.youtube.com <http://accounts.youtube.com>
>
>     .dnld.googlevideo.com <http://dnld.googlevideo.com>
>
>     .gmail.com:443-
>
>     .googleads4.g.doubleclick.net <http://googleads4.g.doubleclick.net>
>
>     .googlevideo.com <http://googlevideo.com>
>
>     .i.ytimg.com <http://i.ytimg.com>
>
>     .nek.googlevideo.com <http://nek.googlevideo.com>
>
>     .play.google.com <http://play.google.com>
>
>     .sb.scorecardresearch.com <http://sb.scorecardresearch.com>
>
>     .s.ytimg.com <http://s.ytimg.com>
>
>     .youtube.com <http://youtube.com>
>
>     .ytimg.com <http://ytimg.com>
>
>     The entry I find in access.lof file reads like this:
>
>     1545988674.026 ? ? 0 10.5.2.96 TAG_NONE/503 0 CONNECT www.youtube.com:443 <http://www.youtube.com:443> - HIER_NONE/- -
>
>     I still cannot?unblock youtube.
>
>     I'd appreciate?your help in resolving this.
>
>     Wolfgang
>
>
>
>     _______________________________________________
>
>     squid-users mailing list
>
>     squid-users at lists.squid-cache.org  <mailto:squid-users at lists.squid-cache.org>
>
>     http://lists.squid-cache.org/listinfo/squid-users
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20181229/a111ff52/attachment.htm>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: image001.png
Type: image/png
Size: 11295 bytes
Desc: not available
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20181229/a111ff52/attachment.png>

From squid3 at treenet.co.nz  Sat Dec 29 21:17:22 2018
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sun, 30 Dec 2018 10:17:22 +1300
Subject: [squid-users] Squid4 with GnuTLS - specify ciphers or disable
 protocols
In-Reply-To: <CADpL81YboUfELG0OvmRaecBZvtRbzf4U0uX9kXXKR7O88d=7Bw@mail.gmail.com>
References: <mailman.3.1541851201.4312.squid-users@lists.squid-cache.org>
 <CADpL81bKoNSQqsXyCkgWKCyL0kOpqgYGkVea-zxNWG2Aoe6BsQ@mail.gmail.com>
 <a9578d84-5b0f-b953-8616-e5651a7433ea@treenet.co.nz>
 <CADpL81as3hyEnOHWfaYTDKj+GPA9s+P-9ky9OV2CxaTXK-i5_g@mail.gmail.com>
 <84bfd973-055d-cdcc-bb1b-9059f0d4112f@treenet.co.nz>
 <CADpL81ZCtgLMc6S+W-bpCGvZj2wA_dzZjQkQpJiDBRY1=wuCQA@mail.gmail.com>
 <6a479c4f-9d53-2fea-759b-73f138434230@treenet.co.nz>
 <CADpL81YboUfELG0OvmRaecBZvtRbzf4U0uX9kXXKR7O88d=7Bw@mail.gmail.com>
Message-ID: <5cd9ef9b-3c50-f5e6-23c1-f13efd9a6ca4@treenet.co.nz>

I think I have managed to track this down. It seems to be a side effect
of the session management being designed for OpenSSL where the context
implicitly shares details in the library between sessions linked to that
context. Under GnuTLS the sessions generated by clients connecting are
not inheriting details from the listening context+session state, where
they do under OpenSSL.

It may take a while to get that logic redesigned and the fix merged.

Amos


From eliezer at ngtech.co.il  Sun Dec 30 06:28:20 2018
From: eliezer at ngtech.co.il (Eliezer Croitoru)
Date: Sun, 30 Dec 2018 08:28:20 +0200
Subject: [squid-users] Whitelisting youtube
In-Reply-To: <633500a1-1f36-745f-fbd3-e3be91696d30@urlfilterdb.com>
References: <CAETVtpQqncRJ03KmUOM5pTtuDGqNae6PP5zM9qw+n_4pSCsHpQ@mail.gmail.com>
 <2cc4c4f6-9512-1e18-324b-c04c033cbeaa@urlfilterdb.com>
 <02ce01d49f9b$0fdebd70$2f9c3850$@ngtech.co.il>
 <633500a1-1f36-745f-fbd3-e3be91696d30@urlfilterdb.com>
Message-ID: <039e01d4a008$dbf38860$93da9920$@ngtech.co.il>

Sorry Marcus I totally forgot about this.

I was not talking about this issue but something about the init and other scripts that are being installed.

I will test it in the next weeks and will have a better overview and send it to you privately.

 

Thank,

Eliezer

 

----

 <http://ngtech.co.il/lmgtfy/> Eliezer Croitoru
Linux System Administrator
Mobile: +972-5-28704261
Email: eliezer at ngtech.co.il



 

From: Marcus Kool [mailto:marcus.kool at urlfilterdb.com] 
Sent: Saturday, December 29, 2018 22:51
To: eliezer at ngtech.co.il; squid-users at lists.squid-cache.org
Subject: Re: [squid-users] Whitelisting youtube

 

Hi Eliezer,

If you mean compiler errors on debian 9 which has OpenSSL 1.1 ...  

We will release ufdbGuard 1.34 soon which supports OpenSSL 1.1 since OpenSSL 1.1 is not compatible with OpenSSL 1.0.

Marcus

 

On 29/12/2018 15:22, eliezer at ngtech.co.il <mailto:eliezer at ngtech.co.il>  wrote:

Markus,

 

Does ufdbGuard have a Debian package or build instructions?
The last time I tried to compile it on both Debian and Ubuntu I have encountered couple issues.

 

Thanks,

Eliezer

 

 

----

 <http://ngtech.co.il/main-en/> Eliezer Croitoru
Linux System Administrator
Mobile: +972-5-28704261
Email:  <mailto:eliezer at ngtech.co.il> eliezer at ngtech.co.il



 

From: squid-users  <mailto:squid-users-bounces at lists.squid-cache.org> <squid-users-bounces at lists.squid-cache.org> On Behalf Of Marcus Kool
Sent: Friday, December 28, 2018 12:14
To: squid-users at lists.squid-cache.org <mailto:squid-users at lists.squid-cache.org> 
Subject: Re: [squid-users] Whitelisting youtube

 

Wolfgang, why don't you stop using squidguard which has no support for 5+ years and switch to ufdbGuard?

ufdbGuard is regularly maintained and has a Reference Manual that explains what and how to whitelist domains.

Marcus

 

On 28/12/2018 07:18, Wolfgang Paul Rauchholz wrote:

Problem staqtement: can't whitelist youtube.com <http://youtube.com> 

 

I run squid 3.5 and squiguard on a CENTOS 7 home linux server.

The blacklist database is created by a publicly available script called getlists.sh. This script downloads and compiles blacklists  from several sites (e.g. squidguard website)

To whitelist youtube which is blocked too,  I created the directory 'white' within 'blacklist'. The squidguard config looks like this:

 

dest white {

        domainlist      white/domains

        urllist         white/urls

}

 

acl {

        default {

                pass    white !adv !porn !warez all

                redirect http://localhost/block.html

                }

}

 

the domaon file withi nwhite has these entries:

.2mdn.net:443 <http://2mdn.net:443> 

.accounts.google.com <http://accounts.google.com> 

.accounts.youtube.com <http://accounts.youtube.com> 

.dnld.googlevideo.com <http://dnld.googlevideo.com> 

.gmail.com:443-

.googleads4.g.doubleclick.net <http://googleads4.g.doubleclick.net> 

.googlevideo.com <http://googlevideo.com> 

.i.ytimg.com <http://i.ytimg.com> 

.nek.googlevideo.com <http://nek.googlevideo.com> 

.play.google.com <http://play.google.com> 

.sb.scorecardresearch.com <http://sb.scorecardresearch.com> 

.s.ytimg.com <http://s.ytimg.com> 

.youtube.com <http://youtube.com> 

.ytimg.com <http://ytimg.com> 

 

The entry I find in access.lof file reads like this:

1545988674.026      0 10.5.2.96 TAG_NONE/503 0 CONNECT www.youtube.com:443 <http://www.youtube.com:443>  - HIER_NONE/- -

 

 

I still cannot unblock youtube.

I'd appreciate your help in resolving this.

 

Wolfgang

 

 

 






_______________________________________________
squid-users mailing list
squid-users at lists.squid-cache.org <mailto:squid-users at lists.squid-cache.org> 
http://lists.squid-cache.org/listinfo/squid-users

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20181230/35389907/attachment.htm>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: image002.png
Type: image/png
Size: 11295 bytes
Desc: not available
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20181230/35389907/attachment.png>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: image003.png
Type: image/png
Size: 11297 bytes
Desc: not available
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20181230/35389907/attachment-0001.png>

From eliezer at ngtech.co.il  Sun Dec 30 06:34:20 2018
From: eliezer at ngtech.co.il (Eliezer Croitoru)
Date: Sun, 30 Dec 2018 08:34:20 +0200
Subject: [squid-users] Squid 4.4 security_file_certgen helpers crashing
In-Reply-To: <cdd3e8bf-6a64-91d6-40b6-6e05133d851b@measurement-factory.com>
References: <1545946245329-0.post@n4.nabble.com>
 <cdd3e8bf-6a64-91d6-40b6-6e05133d851b@measurement-factory.com>
Message-ID: <054e01d4a009$b25546f0$16ffd4d0$@ngtech.co.il>

Hey Alex,

I didn't had the time to sit and compose a STDINT/OUT input and output that can be used to test the security_file_certgen.
Can you or anyone of the related developers post in the wiki a simple "example" input that can be sent over STDIN to debug this type of issues?
I can just load the software as squid or proxy user but...
Another option is to point us towards the debug options that will give the testing admins(or me) an option to copy and paste the data that squid is sending to the helper.

Thanks,
Eliezer

----
Eliezer Croitoru
Linux System Administrator
Mobile: +972-5-28704261
Email: eliezer at ngtech.co.il


-----Original Message-----
From: squid-users [mailto:squid-users-bounces at lists.squid-cache.org] On Behalf Of Alex Rousskov
Sent: Friday, December 28, 2018 05:29
To: squid-users at lists.squid-cache.org
Subject: Re: [squid-users] Squid 4.4 security_file_certgen helpers crashing

On 12/27/18 2:30 PM, johnr wrote:

> I find the following in the cache log:

> 2018/12/27 21:15:40 kid1| WARNING:
> /usr/local/squid/libexec/security_file_certgen -s
> /usr/local/squid/var/cache/squid/ssl_db -M 4MB #Hlpr1 exited

We need to figure out why the helper is exiting. If there are no error
messages in cache.log, then your system log may have additional
information such as the process signal that killed the helper. If it was
a crash, then your core dump directory should have the corresponding
core dump (make sure you enable core dumps!) that you can examine with gdb.


> I ran the security_gen_helper under GDB and it seems to be crashing here:
> https://github.com/squid-cache/squid/blob/master/src/ssl/gadgets.cc#L218

If you can reproduce helper crash while it has gdb attached, please post
the stack trace.


> I saw a commit supporting a newer version of openssl, I wonder if
> that may have mistakenly broken support for older versions of
> openssl?

Sure, it may have. Most likely, the changes are not tested in an
environment matching yours, and the bug may be environment-driven.


If you get more details such as a backtrace, please consider filing a
bug report with all the details.


Thank you,

Alex.
_______________________________________________
squid-users mailing list
squid-users at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-users



From eliezer at ngtech.co.il  Sun Dec 30 06:35:45 2018
From: eliezer at ngtech.co.il (Eliezer Croitoru)
Date: Sun, 30 Dec 2018 08:35:45 +0200
Subject: [squid-users] Caching Vimeo Videos
In-Reply-To: <1545735011205-0.post@n4.nabble.com>
References: <CAGycgFh+RZq83hsTvGX3Q8M7Mf02pp8jrBX2Re9U87EYqHNjEQ@mail.gmail.com>
 <039901d48896$a0eb15a0$e2c140e0$@ngtech.co.il>
 <1545735011205-0.post@n4.nabble.com>
Message-ID: <055001d4a009$e4f1f900$aed5eb00$@ngtech.co.il>

For what OS?

Eliezer

----
Eliezer Croitoru
Linux System Administrator
Mobile: +972-5-28704261
Email: eliezer at ngtech.co.il



-----Original Message-----
From: squid-users [mailto:squid-users-bounces at lists.squid-cache.org] On Behalf Of M K Raju
Sent: Tuesday, December 25, 2018 12:50
To: squid-users at lists.squid-cache.org
Subject: Re: [squid-users] Caching Vimeo Videos

Thanks Eliezer,

Our videos are streaming with HLS Dash only.
Please give me examples with SSL-BUMP and Store ID helper.

Regards.



--
Sent from: http://squid-web-proxy-cache.1019090.n4.nabble.com/Squid-Users-f1019091.html
_______________________________________________
squid-users mailing list
squid-users at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-users



From sampei02 at tiscali.it  Mon Dec 31 09:48:40 2018
From: sampei02 at tiscali.it (Sampei)
Date: Mon, 31 Dec 2018 10:48:40 +0100
Subject: [squid-users] https debug
Message-ID: <af2773d43fb792eb052c2b409cc4c35d@tiscali.it>

  
I'm using application which uses https protocol by Squid 3.5.20 and
It often occurs application timeout and other problems to use this
software. 
I tried to use application without Squid, bypassing it, and
It works fine. 
How can I find out where Squid could create problems to
this application? 
I tried to enable "debug_options ALL,2" feature but
it's very hard to understand, suggestions please?  


Con OpenStar hai Giga, SMS e i minuti che vuoi da 4,99? al mese, per sempre. Cambi gratis quando e come vuoi e in pi? hai 10? di credito omaggio e 6 mesi di INFINTY! http://tisca.li/myopen

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20181231/ca5142a2/attachment.htm>

From squid3 at treenet.co.nz  Mon Dec 31 13:28:32 2018
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Tue, 1 Jan 2019 02:28:32 +1300
Subject: [squid-users] https debug
In-Reply-To: <af2773d43fb792eb052c2b409cc4c35d@tiscali.it>
References: <af2773d43fb792eb052c2b409cc4c35d@tiscali.it>
Message-ID: <ca313cd2-4344-d480-4563-82f9f1e43900@treenet.co.nz>

On 31/12/18 10:48 pm, Sampei wrote:
> I'm using application which uses https protocol by Squid ?3.5.20 and It
> often occurs application timeout and other problems to use this software.

Please explain with more details about what this setup actually is.
There are many very different ways to "uses https protocol by Squid"
<https://wiki.squid-cache.org/Features/HTTPS>


> I tried to use application without Squid, bypassing it, and It works fine.
> How can I find out where Squid could create problems to this application?
> I tried to enable "debug_options ALL,2" feature but it's very hard to
> understand, suggestions please?
> 

Upgrade Squid? Current release is 4.4, soon to be 4.5.

The old 3.5.* series lacks a lot of TLS feature support and polishing of
the Squid behaviours.

If the Squid logs do not contain the desired info you can always look at
a packet trace of the traffic.

Amos


