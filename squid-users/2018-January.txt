From squid3 at treenet.co.nz  Tue Jan  2 08:20:49 2018
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Tue, 2 Jan 2018 21:20:49 +1300
Subject: [squid-users] Help with UA filtering in https connections
In-Reply-To: <1514565158717-0.post@n4.nabble.com>
References: <1514501960304-0.post@n4.nabble.com>
 <ecaab75f-7375-b641-0fea-582be9e33669@measurement-factory.com>
 <1514565158717-0.post@n4.nabble.com>
Message-ID: <e253f17a-5c7c-c0f5-ebc8-b0584686ed53@treenet.co.nz>

On 30/12/17 05:32, squidnoob wrote:
> Ahh that's it! Thank you for your help!
> 
> For anyone interested, i'm posting the working config i'm using. Hopefully
> this helps someone.
> 

This config allows clients to tunnel arbitrary traffic through your 
proxy to another one listening on port 80 without any controls.

Please restore the default security settings as your first http_access 
lines:

   http_access deny !Safe_Ports
   http_access deny CONNECT !SSL_Ports

... your existing config should still work fine when placed after those.

Amos


From ermalwa1 at gmail.com  Tue Jan  2 13:04:34 2018
From: ermalwa1 at gmail.com (squidnoob)
Date: Tue, 2 Jan 2018 06:04:34 -0700 (MST)
Subject: [squid-users] Help with UA filtering in https connections
In-Reply-To: <e253f17a-5c7c-c0f5-ebc8-b0584686ed53@treenet.co.nz>
References: <1514501960304-0.post@n4.nabble.com>
 <ecaab75f-7375-b641-0fea-582be9e33669@measurement-factory.com>
 <1514565158717-0.post@n4.nabble.com>
 <e253f17a-5c7c-c0f5-ebc8-b0584686ed53@treenet.co.nz>
Message-ID: <1514898274383-0.post@n4.nabble.com>

In my existing config, i have: 

# delay filtering decisions until we get to bumped requests 
http_access allow CONNECT safe_ports
http_access deny CONNECT


I understand adding this line that you suggested as it's not already there. 
http_access deny !safe_ports

However, i don't understand why i would need to add this (http_access deny
CONNECT !SSL_Ports ) given the two lines above in the existing config. I'm
probably just misunderstanding how this works. 

Thank you for the help!



--
Sent from: http://squid-web-proxy-cache.1019090.n4.nabble.com/Squid-Users-f1019091.html


From robertocarna36 at gmail.com  Tue Jan  2 13:48:24 2018
From: robertocarna36 at gmail.com (Roberto Carna)
Date: Tue, 2 Jan 2018 10:48:24 -0300
Subject: [squid-users] Transparent proxy for WiFi users
Message-ID: <CAG2Qp6vtJSY7=DuxnjwUAPc5BWFZgPEev8_mqDpELB=_gfXLnw@mail.gmail.com>

Dear, I've setup a Squid transparent proxy + Squidgard on pfSEnse 2.4
in order to filter HTTP and HTTPS web content for different types of
WiFi clients on my company:

- Android (different versions)
- Notebooks Windows 7/10
- Iphone
- Etc.

In some cases, depending on the device Operating System, some apps
experiment problems, for example Facebook and some others.

Which is the best solution in order to setup a TRANSPARENT proxy
service in a heterogeneous scenario with diferenbt types of devices,
and running in the best mode with the minimum number of problems???

Or do I have to move to a scenario with a defined proxy in another
server, and automatically established in clients with DHCP ???

Thanks a lot,

Roberto


From uhlar at fantomas.sk  Tue Jan  2 14:08:15 2018
From: uhlar at fantomas.sk (Matus UHLAR - fantomas)
Date: Tue, 2 Jan 2018 15:08:15 +0100
Subject: [squid-users] Help with UA filtering in https connections
In-Reply-To: <1514898274383-0.post@n4.nabble.com>
References: <1514501960304-0.post@n4.nabble.com>
 <ecaab75f-7375-b641-0fea-582be9e33669@measurement-factory.com>
 <1514565158717-0.post@n4.nabble.com>
 <e253f17a-5c7c-c0f5-ebc8-b0584686ed53@treenet.co.nz>
 <1514898274383-0.post@n4.nabble.com>
Message-ID: <20180102140815.GB504@fantomas.sk>

On 02.01.18 06:04, squidnoob wrote:
>In my existing config, i have:
>
># delay filtering decisions until we get to bumped requests
>http_access allow CONNECT safe_ports
>http_access deny CONNECT
>
>
>I understand adding this line that you suggested as it's not already there.
>http_access deny !safe_ports
>
>However, i don't understand why i would need to add this (http_access deny
>CONNECT !SSL_Ports ) given the two lines above in the existing config. I'm
>probably just misunderstanding how this works.

the two lines above unconditionally allow CONNECT anywhere, you can't deny
it further because no further checking is done.

when using:

http_access deny CONNECT !SSL_ports 

you deny CONNECT request to non-SSL ports and can deny them further.

-- 
Matus UHLAR - fantomas, uhlar at fantomas.sk ; http://www.fantomas.sk/
Warning: I wish NOT to receive e-mail advertising to this address.
Varovanie: na tuto adresu chcem NEDOSTAVAT akukolvek reklamnu postu.
99 percent of lawyers give the rest a bad name. 


From rousskov at measurement-factory.com  Tue Jan  2 16:06:43 2018
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Tue, 2 Jan 2018 09:06:43 -0700
Subject: [squid-users] Help with UA filtering in https connections
In-Reply-To: <20180102140815.GB504@fantomas.sk>
References: <1514501960304-0.post@n4.nabble.com>
 <ecaab75f-7375-b641-0fea-582be9e33669@measurement-factory.com>
 <1514565158717-0.post@n4.nabble.com>
 <e253f17a-5c7c-c0f5-ebc8-b0584686ed53@treenet.co.nz>
 <1514898274383-0.post@n4.nabble.com> <20180102140815.GB504@fantomas.sk>
Message-ID: <75beb196-f0c2-6e2c-1eee-17e92769a3e5@measurement-factory.com>

On 01/02/2018 07:08 AM, Matus UHLAR - fantomas wrote:
> On 02.01.18 06:04, squidnoob wrote:
>> http_access allow CONNECT safe_ports
>> http_access deny CONNECT


>> I understand adding this line that you suggested as it's not already
>> there.
>> http_access deny !safe_ports

Yes, this or similar line (and possibly other lines) is needed, provided
it matches your proxying environment. My sketch only dealt with your
original/specific problem, not general proxying protections...


>> However, i don't understand why i would need to add this (http_access
>> deny CONNECT !SSL_Ports ) given the two lines above in the existing config.

You do not need to add it AFAICT.


> the two lines above unconditionally allow CONNECT anywhere, 

This is incorrect. The lines deny CONNECT to unsafe ports. What Amos
correctly pointed out is that *non-CONNECT* transactions may go to
unsafe ports as well, and it is considered best practice to block such
traffic by default.

Please note that denying CONNECTs to unsafe ports at step1 may not work
well because the generated by Squid certificate will be rejected by the
browser in many cases. You may decide to simply terminate such CONNECT
transactions instead:

  # terminate malicious tunnels and bump everything else
  ssl_bump terminate !safe_ports
  ssl_bump stare all
  ssl_bump bump all


Alex.


From squid3 at treenet.co.nz  Tue Jan  2 20:13:13 2018
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Wed, 3 Jan 2018 09:13:13 +1300
Subject: [squid-users] Transparent proxy for WiFi users
In-Reply-To: <CAG2Qp6vtJSY7=DuxnjwUAPc5BWFZgPEev8_mqDpELB=_gfXLnw@mail.gmail.com>
References: <CAG2Qp6vtJSY7=DuxnjwUAPc5BWFZgPEev8_mqDpELB=_gfXLnw@mail.gmail.com>
Message-ID: <97086c2c-cb1d-ad3b-05d7-4f0988f8da3e@treenet.co.nz>

On 03/01/18 02:48, Roberto Carna wrote:
> Dear, I've setup a Squid transparent proxy + Squidgard on pfSEnse 2.4
> in order to filter HTTP and HTTPS web content for different types of
> WiFi clients on my company:
> 
> - Android (different versions)
> - Notebooks Windows 7/10
> - Iphone
> - Etc.
> 
> In some cases, depending on the device Operating System, some apps
> experiment problems, for example Facebook and some others.

The main cause of these problems is that when the same vendor is 
authoring both the server software and the client "app" (or client 
device OS). They can (and often do) hard-code TLS certificate checks 
into their client code to detect and immediately fail in the presence of 
MITM in the encryption.

Following that, SSL-Bump is still very much an ongoing project. 
Selecting even a slightly older Squid version can lead to TLS features 
not being supported. So when problems occur the best option is still to 
upgrade to the very latest release before debugging further - today that 
would be squid-4.0.22 beta.


> 
> Which is the best solution in order to setup a TRANSPARENT proxy
> service in a heterogeneous scenario with diferenbt types of devices,
> and running in the best mode with the minimum number of problems???

The _only_ solution is not to decrypt such traffic (the splice action). 
How you determine which traffic is having such special trust given to it 
is up to you. The TLS SNI is provided by the peek action at SSL-Bump step 1.


> 
> Or do I have to move to a scenario with a defined proxy in another
> server, and automatically established in clients with DHCP ???
> 

Explicit proxy is definitely better for HTTP than interception proxy. 
That is true regardless of what else is going on. So worth doing *if* 
you can.

That said, it is also unlikely to help much with the problem you are 
facing. Perhapse a small gain for clients not sending TLS SNI values - 
otherwise no change can be expected.

Amos


From yvoinov at gmail.com  Tue Jan  2 21:15:32 2018
From: yvoinov at gmail.com (Yuri)
Date: Wed, 3 Jan 2018 03:15:32 +0600
Subject: [squid-users] Transparent proxy for WiFi users
In-Reply-To: <97086c2c-cb1d-ad3b-05d7-4f0988f8da3e@treenet.co.nz>
References: <CAG2Qp6vtJSY7=DuxnjwUAPc5BWFZgPEev8_mqDpELB=_gfXLnw@mail.gmail.com>
 <97086c2c-cb1d-ad3b-05d7-4f0988f8da3e@treenet.co.nz>
Message-ID: <03aac2f3-4bf7-0f64-0953-8b0c65cc6c64@gmail.com>


03.01.2018 02:13, Amos Jeffries ?????:
> On 03/01/18 02:48, Roberto Carna wrote:
>> Dear, I've setup a Squid transparent proxy + Squidgard on pfSEnse 2.4
>> in order to filter HTTP and HTTPS web content for different types of
>> WiFi clients on my company:
>>
>> - Android (different versions)
>> - Notebooks Windows 7/10
>> - Iphone
>> - Etc.
>>
>> In some cases, depending on the device Operating System, some apps
>> experiment problems, for example Facebook and some others.
>
> The main cause of these problems is that when the same vendor is
> authoring both the server software and the client "app" (or client
> device OS). They can (and often do) hard-code TLS certificate checks
> into their client code to detect and immediately fail in the presence
> of MITM in the encryption.
>
> Following that, SSL-Bump is still very much an ongoing project.
> Selecting even a slightly older Squid version can lead to TLS features
> not being supported. So when problems occur the best option is still
> to upgrade to the very latest release before debugging further - today
> that would be squid-4.0.22 beta.
>
>
>>
>> Which is the best solution in order to setup a TRANSPARENT proxy
>> service in a heterogeneous scenario with diferenbt types of devices,
>> and running in the best mode with the minimum number of problems???
>
> The _only_ solution is not to decrypt such traffic (the splice
> action). How you determine which traffic is having such special trust
> given to it is up to you. The TLS SNI is provided by the peek action
> at SSL-Bump step 1.
Well, you can do it when you want. For example, take a look (for example):

https://stackoverflow.com/questions/4461360/how-to-install-trusted-ca-certificate-on-android-device

or on this:

http://wiki.cacert.org/FAQ/ImportRootCert

Or, in your case,? you can differentiate users by, for example, by
network and pass wifi users to splice rule. Much approaches exists.
>
>
>>
>> Or do I have to move to a scenario with a defined proxy in another
>> server, and automatically established in clients with DHCP ???
>>
>
> Explicit proxy is definitely better for HTTP than interception proxy.
> That is true regardless of what else is going on. So worth doing *if*
> you can.
Oh, really? You have forgotten about beatiful 252 option in DHCP, about
WCCP interception, and such other good things. In other world,
transparent interception requires just more experienced sysadmin. This
simple requires more experience with squid, LAN equipement, and a bit
more work. So, "definitely better" is very discussable statement.
>
> That said, it is also unlikely to help much with the problem you are
> facing. Perhapse a small gain for clients not sending TLS SNI values -
> otherwise no change can be expected.
>
> Amos
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users

-- 
*****************************
* C++20 : Bug to the future *
*****************************



From umuta at sabanciuniv.edu  Wed Jan  3 07:25:58 2018
From: umuta at sabanciuniv.edu (Umut Arus)
Date: Wed, 3 Jan 2018 10:25:58 +0300
Subject: [squid-users] Caching for download servers
Message-ID: <CALwryzEAZFPf4AWQKrSN68sUuGthGarb48LkDT9zbiyJjdjeZg@mail.gmail.com>

Hi,

I'd like to ask about how redirect a client to squid server for only some
destination domain zone (or IP addresses). We would like to cache some
download server without doing any setup on client side.

I appreciate you comments.

thanks.

-- 
*Umut Arus*
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20180103/785dd7cd/attachment.htm>

From mail at paul-neuwirth.nl  Wed Jan  3 07:30:36 2018
From: mail at paul-neuwirth.nl (Paul Neuwirth)
Date: Wed, 3 Jan 2018 08:30:36 +0100
Subject: [squid-users] access blocking using DNS -> "NO Address records in
 response to '....'
Message-ID: <20180103083036.34fb2730@iota.swabian.net>

Hello list,

named is configured to block (resulting in NXDOMAIN) some domains.
Using squid I have following problem:
Browser requests such a blocked URL  and named is not delivering an
error, request never times out... 
How can I make squid deliver an error in this case.

From the logs i see:
2018-01-03T08:00:49.750777+01:00 alpha squid[24532]: ipcacheParse: No
Address records in response to 'www.googletagmanager.com'

in time of request.

If request is aborted:

2018-01-03T08:03:00.163354+01:00 alpha squid[24532]: 1514962860.163
10414 172.18.0.26 TCP_MISS_ABORTED/000 0 GET
http://www.googletagmanager.com/ - HIER_NONE/- -

Thank you for help. If you need any further information, i may deliver.

Thank you

Paul

OS: OpenSUSE Leap 42.2

# zypper if squid
Information for package squid:
------------------------------
Repository     : opensuse_updates               
Name           : squid                          
Version        : 3.5.21-5.3.1                   
Arch           : x86_64                         
Vendor         : openSUSE                       
Installed Size : 10.0 MiB                       
Installed      : Yes                            
Status         : up-to-date                     
Source package : squid-3.5.21-5.3.1.src


From mail at paul-neuwirth.nl  Wed Jan  3 07:34:19 2018
From: mail at paul-neuwirth.nl (Paul Neuwirth)
Date: Wed, 3 Jan 2018 08:34:19 +0100
Subject: [squid-users] access blocking using DNS -> "NO Address records
 in response to '....'
In-Reply-To: <20180103083036.34fb2730@iota.swabian.net>
References: <20180103083036.34fb2730@iota.swabian.net>
Message-ID: <20180103083419.64bfcdda@iota.swabian.net>

On Wed, 3 Jan 2018 08:30:36 +0100
Paul Neuwirth <mail at paul-neuwirth.nl> wrote:

> Hello list,
> 
> named is configured to block (resulting in NXDOMAIN) some domains.
> Using squid I have following problem:
> Browser requests such a blocked URL  and named is not delivering an
> error, request never times out... 
> How can I make squid deliver an error in this case.
> 
> From the logs i see:
> 2018-01-03T08:00:49.750777+01:00 alpha squid[24532]: ipcacheParse: No
> Address records in response to 'www.googletagmanager.com'
> 
> in time of request.
> 
> If request is aborted:
> 
> 2018-01-03T08:03:00.163354+01:00 alpha squid[24532]: 1514962860.163
> 10414 172.18.0.26 TCP_MISS_ABORTED/000 0 GET
> http://www.googletagmanager.com/ - HIER_NONE/- -
> 
> Thank you for help. If you need any further information, i may
> deliver.
> 
> Thank you
> 
> Paul
> 
> OS: OpenSUSE Leap 42.2
> 
> # zypper if squid
> Information for package squid:
> ------------------------------
> Repository     : opensuse_updates               
> Name           : squid                          
> Version        : 3.5.21-5.3.1                   
> Arch           : x86_64                         
> Vendor         : openSUSE                       
> Installed Size : 10.0 MiB                       
> Installed      : Yes                            
> Status         : up-to-date                     
> Source package : squid-3.5.21-5.3.1.src

Sorry, just a minute after sending I found out, named is not delivering
NXDOMAIN, but nothing
# dig www-googletagmanager.l.google.com

; <<>> DiG 9.10.4-P5 <<>> www-googletagmanager.l.google.com
;; global options: +cmd
;; Got answer:
;; ->>HEADER<<- opcode: QUERY, status: NOERROR, id: 50108
;; flags: qr aa rd ra; QUERY: 1, ANSWER: 0, AUTHORITY: 1, ADDITIONAL: 1

;; OPT PSEUDOSECTION:
; EDNS: version: 0, flags:; udp: 4096
;; QUESTION SECTION:
;www-googletagmanager.l.google.com. IN	A

;; AUTHORITY SECTION:
www-googletagmanager.l.google.com. 21600 IN SOA	ns1.domain.com.
hostmaster.domain.com. 1 10800 3600 86400 21600

;; Query time: 0 msec
;; SERVER: 127.0.0.1#53(127.0.0.1)
;; WHEN: Wed Jan 03 08:33:08 CET 2018
;; MSG SIZE  rcvd: 120





From squid3 at treenet.co.nz  Wed Jan  3 12:16:27 2018
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 4 Jan 2018 01:16:27 +1300
Subject: [squid-users] Transparent proxy for WiFi users
In-Reply-To: <03aac2f3-4bf7-0f64-0953-8b0c65cc6c64@gmail.com>
References: <CAG2Qp6vtJSY7=DuxnjwUAPc5BWFZgPEev8_mqDpELB=_gfXLnw@mail.gmail.com>
 <97086c2c-cb1d-ad3b-05d7-4f0988f8da3e@treenet.co.nz>
 <03aac2f3-4bf7-0f64-0953-8b0c65cc6c64@gmail.com>
Message-ID: <c47859c8-12c8-7171-4110-802df60041b1@treenet.co.nz>

On 03/01/18 10:15, Yuri wrote:
> 
> 03.01.2018 02:13, Amos Jeffries ?????:
>> On 03/01/18 02:48, Roberto Carna wrote:
>>> Dear, I've setup a Squid transparent proxy + Squidgard on pfSEnse 2.4
>>> in order to filter HTTP and HTTPS web content for different types of
>>> WiFi clients on my company:
>>>
>>> - Android (different versions)
>>> - Notebooks Windows 7/10
>>> - Iphone
>>> - Etc.
>>>
>>> In some cases, depending on the device Operating System, some apps
>>> experiment problems, for example Facebook and some others.
>>
>> The main cause of these problems is that when the same vendor is
>> authoring both the server software and the client "app" (or client
>> device OS). They can (and often do) hard-code TLS certificate checks
>> into their client code to detect and immediately fail in the presence
>> of MITM in the encryption.
>>
>> Following that, SSL-Bump is still very much an ongoing project.
>> Selecting even a slightly older Squid version can lead to TLS features
>> not being supported. So when problems occur the best option is still
>> to upgrade to the very latest release before debugging further - today
>> that would be squid-4.0.22 beta.
>>
>>
>>>
>>> Which is the best solution in order to setup a TRANSPARENT proxy
>>> service in a heterogeneous scenario with diferenbt types of devices,
>>> and running in the best mode with the minimum number of problems???
>>
>> The _only_ solution is not to decrypt such traffic (the splice
>> action). How you determine which traffic is having such special trust
>> given to it is up to you. The TLS SNI is provided by the peek action
>> at SSL-Bump step 1.
> Well, you can do it when you want. For example, take a look (for example):
> 
> https://stackoverflow.com/questions/4461360/how-to-install-trusted-ca-certificate-on-android-device
> 
> or on this:
> 
> http://wiki.cacert.org/FAQ/ImportRootCert
> 
> Or, in your case,? you can differentiate users by, for example, by
> network and pass wifi users to splice rule. Much approaches exists.

NP: none of which work when the application is checking the fingerprint 
of the CA certificate against a hard-coded value defined by the vendor. 
These are simply ways to make the SSL-Bumping process work without 
"untrusted CA" warnings *if* bumping is already possible.

>>
>>>
>>> Or do I have to move to a scenario with a defined proxy in another
>>> server, and automatically established in clients with DHCP ???
>>>
>>
>> Explicit proxy is definitely better for HTTP than interception proxy.
>> That is true regardless of what else is going on. So worth doing *if*
>> you can.
> Oh, really? You have forgotten about beatiful 252 option in DHCP, about
> WCCP interception, and such other good things. In other world,

No, I am not.

DHCP, WPAD/PAC, and such are just ways to auto-configure explicit proxy 
*not* interception proxy.

WCCP is just a way to tunnel packets to the proxy machine. Explicit 
proxy does not require that additional tunnel complexity.
  If you combine WCCP with interception it becomes "interception proxy", 
not "explicit proxy".


Amos


From squid3 at treenet.co.nz  Wed Jan  3 12:24:57 2018
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 4 Jan 2018 01:24:57 +1300
Subject: [squid-users] access blocking using DNS -> "NO Address records
 in response to '....'
In-Reply-To: <20180103083419.64bfcdda@iota.swabian.net>
References: <20180103083036.34fb2730@iota.swabian.net>
 <20180103083419.64bfcdda@iota.swabian.net>
Message-ID: <983e9da5-be93-df1f-44c2-882d2b2dd2f7@treenet.co.nz>

On 03/01/18 20:34, Paul Neuwirth wrote:
> On Wed, 3 Jan 2018 08:30:36 +0100
> Paul Neuwirth wrote:
> 
>> Hello list,
>>
>> named is configured to block (resulting in NXDOMAIN) some domains.
>> Using squid I have following problem:
>> Browser requests such a blocked URL  and named is not delivering an
>> error, request never times out...
>> How can I make squid deliver an error in this case.
>>

...
> 
> Sorry, just a minute after sending I found out, named is not delivering
> NXDOMAIN, but nothing

Nod. That is the cause of the "NO address records" log entry.

The client appears to be disconnecting from Squid after ~10 seconds. You 
can probably get the Squid "unable to resolve" error page to show up by 
reducing dns_timeout to a value of 5-10 seconds 
(<http://www.squid-cache.org/Doc/config/dns_timeout/>).

Amos


From squid3 at treenet.co.nz  Wed Jan  3 12:25:28 2018
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 4 Jan 2018 01:25:28 +1300
Subject: [squid-users] Caching for download servers
In-Reply-To: <CALwryzEAZFPf4AWQKrSN68sUuGthGarb48LkDT9zbiyJjdjeZg@mail.gmail.com>
References: <CALwryzEAZFPf4AWQKrSN68sUuGthGarb48LkDT9zbiyJjdjeZg@mail.gmail.com>
Message-ID: <49a92522-bf38-4d80-f89b-8fad5b11f26e@treenet.co.nz>

On 03/01/18 20:25, Umut Arus wrote:
> Hi,
> 
> I'd like to ask about how redirect a client to squid server for only 
> some destination domain zone (or IP addresses). We would like to cache 
> some download server without doing any setup on client side.
> 

What do you mean by "cache some download server" ?

It sounds a bit like you are looking for NAT interception 
(<https://wiki.squid-cache.org/ConfigExamples/Intercept/LinuxRedirect>)

Amos


From uhlar at fantomas.sk  Wed Jan  3 12:52:33 2018
From: uhlar at fantomas.sk (Matus UHLAR - fantomas)
Date: Wed, 3 Jan 2018 13:52:33 +0100
Subject: [squid-users] Help with UA filtering in https connections
In-Reply-To: <75beb196-f0c2-6e2c-1eee-17e92769a3e5@measurement-factory.com>
References: <1514501960304-0.post@n4.nabble.com>
 <ecaab75f-7375-b641-0fea-582be9e33669@measurement-factory.com>
 <1514565158717-0.post@n4.nabble.com>
 <e253f17a-5c7c-c0f5-ebc8-b0584686ed53@treenet.co.nz>
 <1514898274383-0.post@n4.nabble.com>
 <20180102140815.GB504@fantomas.sk>
 <75beb196-f0c2-6e2c-1eee-17e92769a3e5@measurement-factory.com>
Message-ID: <20180103125233.GD504@fantomas.sk>

On 02.01.18 09:06, Alex Rousskov wrote:
>On 01/02/2018 07:08 AM, Matus UHLAR - fantomas wrote:
>> On 02.01.18 06:04, squidnoob wrote:
>>> http_access allow CONNECT safe_ports
>>> http_access deny CONNECT

>> the two lines above unconditionally allow CONNECT anywhere,
>
>This is incorrect. The lines deny CONNECT to unsafe ports.

You miss something.

Those lines unconditionally allow CONNECT requests to safe ports ANYWHERE,
which is apparently not what was wanted/expected.

the first line ALLOWS all CONNECT requests to safe ports in the way they
CAN NOT BE DISABLED later.

the second line denies connect to unsafe ports.

the difference between lines above and the following one:

http_access deny CONNECT !safe_ports

is, that in this case you can deny the connect request later, unlike the
previous example, where the CONNECT was allowed and further checks are done.

However, what Amos proposed and what is in the default config is:

http_access deny !Safe_ports
http_access deny CONNECT !SSL_ports

which denies all access to unsafe ports, and denies CONNECT to non-SSL
ports, but does not allow access anywhere, so it must be allowed further.

-- 
Matus UHLAR - fantomas, uhlar at fantomas.sk ; http://www.fantomas.sk/
Warning: I wish NOT to receive e-mail advertising to this address.
Varovanie: na tuto adresu chcem NEDOSTAVAT akukolvek reklamnu postu.
I wonder how much deeper the ocean would be without sponges. 


From mail at paul-neuwirth.nl  Wed Jan  3 13:01:19 2018
From: mail at paul-neuwirth.nl (Paul Neuwirth)
Date: Wed, 3 Jan 2018 14:01:19 +0100
Subject: [squid-users] access blocking using DNS -> "NO Address records
 in response to '....'
In-Reply-To: <983e9da5-be93-df1f-44c2-882d2b2dd2f7@treenet.co.nz>
References: <20180103083036.34fb2730@iota.swabian.net>
 <20180103083419.64bfcdda@iota.swabian.net>
 <983e9da5-be93-df1f-44c2-882d2b2dd2f7@treenet.co.nz>
Message-ID: <20180103140119.09667e91@iota.swabian.net>

On Thu, 4 Jan 2018 01:24:57 +1300
Amos Jeffries <squid3 at treenet.co.nz> wrote:

> On 03/01/18 20:34, Paul Neuwirth wrote:
> > On Wed, 3 Jan 2018 08:30:36 +0100
> > Paul Neuwirth wrote:
> >   
> >> Hello list,
> >>
> >> named is configured to block (resulting in NXDOMAIN) some domains.
> >> Using squid I have following problem:
> >> Browser requests such a blocked URL  and named is not delivering an
> >> error, request never times out...
> >> How can I make squid deliver an error in this case.
> >>  
> 
> ...
> > 
> > Sorry, just a minute after sending I found out, named is not
> > delivering NXDOMAIN, but nothing  
> 
> Nod. That is the cause of the "NO address records" log entry.
> 
> The client appears to be disconnecting from Squid after ~10 seconds.
> You can probably get the Squid "unable to resolve" error page to show
> up by reducing dns_timeout to a value of 5-10 seconds 
> (<http://www.squid-cache.org/Doc/config/dns_timeout/>).
> 
> Amos

thank you. But default is 60 seconds.. but the request never times out..

but never mind.. I found a better solution, reconfigured bind using
response policy zones to send NXDOMAIN.. this feature didn't exist at
that time I did the previous config.

have a nice year

Paul


From squid3 at treenet.co.nz  Wed Jan  3 13:49:39 2018
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 4 Jan 2018 02:49:39 +1300
Subject: [squid-users] access blocking using DNS -> "NO Address records
 in response to '....'
In-Reply-To: <20180103140119.09667e91@iota.swabian.net>
References: <20180103083036.34fb2730@iota.swabian.net>
 <20180103083419.64bfcdda@iota.swabian.net>
 <983e9da5-be93-df1f-44c2-882d2b2dd2f7@treenet.co.nz>
 <20180103140119.09667e91@iota.swabian.net>
Message-ID: <8ca72a0a-fc37-bbbb-33a6-3c912d875cac@treenet.co.nz>

On 04/01/18 02:01, Paul Neuwirth wrote:
> On Thu, 4 Jan 2018 01:24:57 +1300
> Amos Jeffries <squid3 at treenet.co.nz> wrote:
> 
>> On 03/01/18 20:34, Paul Neuwirth wrote:
>>> On Wed, 3 Jan 2018 08:30:36 +0100
>>> Paul Neuwirth wrote:
>>>    
>>>> Hello list,
>>>>
>>>> named is configured to block (resulting in NXDOMAIN) some domains.
>>>> Using squid I have following problem:
>>>> Browser requests such a blocked URL  and named is not delivering an
>>>> error, request never times out...
>>>> How can I make squid deliver an error in this case.
>>>>   
>>
>> ...
>>>
>>> Sorry, just a minute after sending I found out, named is not
>>> delivering NXDOMAIN, but nothing
>>
>> Nod. That is the cause of the "NO address records" log entry.
>>
>> The client appears to be disconnecting from Squid after ~10 seconds.
>> You can probably get the Squid "unable to resolve" error page to show
>> up by reducing dns_timeout to a value of 5-10 seconds
>> (<http://www.squid-cache.org/Doc/config/dns_timeout/>).
>>
>> Amos
> 
> thank you. But default is 60 seconds.. but the request never times out..

You missed the point. The access.log snippet presented said the 
connection got aborted after 10.140 seconds with 0 bytes delivered to 
the client - long before any Squid DNS lookups timeout.

Which implies strongly that the client is the one aborting the 
transaction. So to get that error page you wanted from Squid in that 
environment setup you would need to shorten dns_timeout to something 
that will make it produce an error page before the client disconnects.

OR, as you found anyway, changing the DNS systems behaviour to a faster 
response also changes the overall outcome ...

> 
> but never mind.. I found a better solution, reconfigured bind using
> response policy zones to send NXDOMAIN.. this feature didn't exist at
> that time I did the previous config.

Nod, that is a bit better if you do it only for intentionally blocked 
domains. Otherwise it will now present lies about domains not existing 
when the truth is their no-IP state, which might muck up your future 
debugging of domain issues. So YMMV.

> 
> have a nice year
> 

Cheers, and same to you.

Amos


From uhlar at fantomas.sk  Wed Jan  3 13:52:19 2018
From: uhlar at fantomas.sk (Matus UHLAR - fantomas)
Date: Wed, 3 Jan 2018 14:52:19 +0100
Subject: [squid-users] Help with UA filtering in https connections
In-Reply-To: <20180103125233.GD504@fantomas.sk>
References: <1514501960304-0.post@n4.nabble.com>
 <ecaab75f-7375-b641-0fea-582be9e33669@measurement-factory.com>
 <1514565158717-0.post@n4.nabble.com>
 <e253f17a-5c7c-c0f5-ebc8-b0584686ed53@treenet.co.nz>
 <1514898274383-0.post@n4.nabble.com>
 <20180102140815.GB504@fantomas.sk>
 <75beb196-f0c2-6e2c-1eee-17e92769a3e5@measurement-factory.com>
 <20180103125233.GD504@fantomas.sk>
Message-ID: <20180103135219.GF504@fantomas.sk>

On 03.01.18 13:52, Matus UHLAR - fantomas wrote:
>http_access deny CONNECT !safe_ports
>
>... in this case you can deny the connect request later, unlike the
>previous example, where the CONNECT was allowed and further checks are done.

corrected: _no_ futher checks are done.

-- 
Matus UHLAR - fantomas, uhlar at fantomas.sk ; http://www.fantomas.sk/
Warning: I wish NOT to receive e-mail advertising to this address.
Varovanie: na tuto adresu chcem NEDOSTAVAT akukolvek reklamnu postu.
Nothing is fool-proof to a talented fool. 


From robertocarna36 at gmail.com  Wed Jan  3 15:53:01 2018
From: robertocarna36 at gmail.com (Roberto Carna)
Date: Wed, 3 Jan 2018 12:53:01 -0300
Subject: [squid-users] Squid crash: assertion failed: store_swapout.cc:289:
 "mem->swapout.sio == self"
Message-ID: <CAG2Qp6sdheuVKHTEW6OHnnCCc7e2qYBq7TFoUspnr1g_siCaPQ@mail.gmail.com>

Dear, I have updated Squid on pfSense to 0.4.42_1 version. But after
start the service togeteher with squidGuard, Squid crashes.

I try running from CLI in debug mode:

# squid -d 10
[2.4.2-RELEASE][admin at FW-Pfsense-Guest.g-bapro.net]/var/log:
2018/01/03 12:46:44 kid1| Starting Squid Cache version 3.5.27 for
amd64-portbld-freebsd11.1...
2018/01/03 12:46:44 kid1| Service Name: squid
2018/01/03 12:46:50 kid1| assertion failed: store_swapout.cc:289:
"mem->swapout.sio == self"
2018/01/03 12:46:53 kid1| Starting Squid Cache version 3.5.27 for
amd64-portbld-freebsd11.1...
2018/01/03 12:46:53 kid1| Service Name: squid
2018/01/03 12:46:59 kid1| assertion failed: store_swapout.cc:289:
"mem->swapout.sio == self"
2018/01/03 12:47:02 kid1| Starting Squid Cache version 3.5.27 for
amd64-portbld-freebsd11.1...
2018/01/03 12:47:02 kid1| Service Name: squid
2018/01/03 12:47:04 kid1| assertion failed: store_swapout.cc:289:
"mem->swapout.sio == self"
2018/01/03 12:47:07 kid1| Starting Squid Cache version 3.5.27 for
amd64-portbld-freebsd11.1...
2018/01/03 12:47:07 kid1| Service Name: squid
2018/01/03 12:47:12 kid1| assertion failed: store_swapout.cc:289:
"mem->swapout.sio == self"
2018/01/03 12:47:16 kid1| Starting Squid Cache version 3.5.27 for
amd64-portbld-freebsd11.1...
2018/01/03 12:47:16 kid1| Service Name: squid
2018/01/03 12:47:20 kid1| assertion failed: store_swapout.cc:289:
"mem->swapout.sio == self"

How can I do ??? What's the problem ???

Thanks a lot.


From rousskov at measurement-factory.com  Wed Jan  3 15:55:40 2018
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Wed, 3 Jan 2018 08:55:40 -0700
Subject: [squid-users] Help with UA filtering in https connections
In-Reply-To: <20180103125233.GD504@fantomas.sk>
References: <1514501960304-0.post@n4.nabble.com>
 <ecaab75f-7375-b641-0fea-582be9e33669@measurement-factory.com>
 <1514565158717-0.post@n4.nabble.com>
 <e253f17a-5c7c-c0f5-ebc8-b0584686ed53@treenet.co.nz>
 <1514898274383-0.post@n4.nabble.com> <20180102140815.GB504@fantomas.sk>
 <75beb196-f0c2-6e2c-1eee-17e92769a3e5@measurement-factory.com>
 <20180103125233.GD504@fantomas.sk>
Message-ID: <eb325a12-3f27-adfe-6c3d-65aa48982092@measurement-factory.com>

On 01/03/2018 05:52 AM, Matus UHLAR - fantomas wrote:
> On 02.01.18 09:06, Alex Rousskov wrote:
>> On 01/02/2018 07:08 AM, Matus UHLAR - fantomas wrote:
>>> On 02.01.18 06:04, squidnoob wrote:
>>>> http_access allow CONNECT safe_ports
>>>> http_access deny CONNECT

>>> the two lines above unconditionally allow CONNECT anywhere,

>> This is incorrect. The lines deny CONNECT to unsafe ports.

> Those lines unconditionally allow CONNECT requests to safe ports ANYWHERE,

Yes, or, to be more precise, they (together with ssl_bump rules) allow
fetching of any server certificate from a reasonable(*) port. They do
not allow HTTP requests to arbitrary safe ports. Only Squid-generated
TLS handshakes.


> which is apparently not what was wanted/expected.

Why not?


> that in this case you can[not] deny the connect request later,

Denying CONNECTs at step1 does not really work well in a general case
because, during SslBump step1, Squid does not have enough information to
generate the right certificate for the access denial error page.

In a general case, the admin has to pick between two evils:

* Allow TLS handshakes with arbitrary servers on TLS ports (my sketch)

* or tell Squid to respond with error pages that the user cannot see
  (without bypassing browser security warnings).

Which evil is lesser is up to the admin to decide. Needless to say,
there are environments where both strategies should be used, depending
on the transaction parameters.


(*) We should allow CONNECTs to SSL_ports, not Safe_ports. I hope my
sketch did not use those ACLs.

Alex.


From uhlar at fantomas.sk  Wed Jan  3 17:38:34 2018
From: uhlar at fantomas.sk (Matus UHLAR - fantomas)
Date: Wed, 3 Jan 2018 18:38:34 +0100
Subject: [squid-users] Help with UA filtering in https connections
In-Reply-To: <eb325a12-3f27-adfe-6c3d-65aa48982092@measurement-factory.com>
References: <1514501960304-0.post@n4.nabble.com>
 <ecaab75f-7375-b641-0fea-582be9e33669@measurement-factory.com>
 <1514565158717-0.post@n4.nabble.com>
 <e253f17a-5c7c-c0f5-ebc8-b0584686ed53@treenet.co.nz>
 <1514898274383-0.post@n4.nabble.com>
 <20180102140815.GB504@fantomas.sk>
 <75beb196-f0c2-6e2c-1eee-17e92769a3e5@measurement-factory.com>
 <20180103125233.GD504@fantomas.sk>
 <eb325a12-3f27-adfe-6c3d-65aa48982092@measurement-factory.com>
Message-ID: <20180103173834.GA18556@fantomas.sk>

>On 01/03/2018 05:52 AM, Matus UHLAR - fantomas wrote:
>> On 02.01.18 09:06, Alex Rousskov wrote:
>>> On 01/02/2018 07:08 AM, Matus UHLAR - fantomas wrote:
>>>> On 02.01.18 06:04, squidnoob wrote:
>>>>> http_access allow CONNECT safe_ports
>>>>> http_access deny CONNECT
>
>>>> the two lines above unconditionally allow CONNECT anywhere,
>
>>> This is incorrect. The lines deny CONNECT to unsafe ports.
>
>> Those lines unconditionally allow CONNECT requests to safe ports ANYWHERE,

On 03.01.18 08:55, Alex Rousskov wrote:
>Yes, or, to be more precise, they (together with ssl_bump rules) allow
>fetching of any server certificate from a reasonable(*) port. They do
>not allow HTTP requests to arbitrary safe ports. Only Squid-generated
>TLS handshakes.


>> which is apparently not what was wanted/expected.
>
>Why not?

because there can be many reasons to deny CONNECT request for example
destined to localhost or internal network.

in the default config, these directives are at the beginning, before
checking for allowed clients and destinations is done.

in the provided config:
http://lists.squid-cache.org/pipermail/squid-users/2017-December/017267.html

there are no deny directives before "http_access allow SSL_port"
and so it's quite possible that all clients that should not have access will
be allowed.

of course, there MAY be other directives or measures to avoid that
but I really think it's better to put "deny CONNECT !SSL_ports"
than allow CONNECT and later wonder why some requests are not 


>> that in this case you can[not] deny the connect request later,
>
>Denying CONNECTs at step1 does not really work well in a general case
>because, during SslBump step1, Squid does not have enough information to
>generate the right certificate for the access denial error page.

I don't think this matters when we do have "http_access deny CONNECT" in
both cases.

>In a general case, the admin has to pick between two evils:
>
>* Allow TLS handshakes with arbitrary servers on TLS ports (my sketch)
>
>* or tell Squid to respond with error pages that the user cannot see
>  (without bypassing browser security warnings).
>
>Which evil is lesser is up to the admin to decide. Needless to say,
>there are environments where both strategies should be used, depending
>on the transaction parameters.
>
>
>(*) We should allow CONNECTs to SSL_ports, not Safe_ports. I hope my
>sketch did not use those ACLs.

I'm afraid you did.
+and I'm also afraid that your proposal also prevents us from disabling
CONNECTs later:

http://lists.squid-cache.org/pipermail/squid-users/2017-December/017268.html

-- 
Matus UHLAR - fantomas, uhlar at fantomas.sk ; http://www.fantomas.sk/
Warning: I wish NOT to receive e-mail advertising to this address.
Varovanie: na tuto adresu chcem NEDOSTAVAT akukolvek reklamnu postu.
Atheism is a non-prophet organization. 


From rousskov at measurement-factory.com  Wed Jan  3 18:31:39 2018
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Wed, 3 Jan 2018 11:31:39 -0700
Subject: [squid-users] Help with UA filtering in https connections
In-Reply-To: <20180103173834.GA18556@fantomas.sk>
References: <1514501960304-0.post@n4.nabble.com>
 <ecaab75f-7375-b641-0fea-582be9e33669@measurement-factory.com>
 <1514565158717-0.post@n4.nabble.com>
 <e253f17a-5c7c-c0f5-ebc8-b0584686ed53@treenet.co.nz>
 <1514898274383-0.post@n4.nabble.com> <20180102140815.GB504@fantomas.sk>
 <75beb196-f0c2-6e2c-1eee-17e92769a3e5@measurement-factory.com>
 <20180103125233.GD504@fantomas.sk>
 <eb325a12-3f27-adfe-6c3d-65aa48982092@measurement-factory.com>
 <20180103173834.GA18556@fantomas.sk>
Message-ID: <573d489a-1706-ee1f-5a45-1d1f5c8b1826@measurement-factory.com>

On 01/03/2018 10:38 AM, Matus UHLAR - fantomas wrote:

>> In a general case, the admin has to pick between two evils:
>>
>> * Allow TLS handshakes with arbitrary servers on TLS ports (my sketch)
>>
>> * or tell Squid to respond with error pages that the user cannot see
>> ?(without bypassing browser security warnings).
>>
>> Which evil is lesser is up to the admin to decide.


>> (*) We should allow CONNECTs to SSL_ports, not Safe_ports. I hope my
>> sketch did not use those ACLs.

> I'm afraid you did.

I did not:
http://lists.squid-cache.org/pipermail/squid-users/2017-December/017268.html

I used toSafePorts which is not one of the default ACLs (but may contain
them). The admin should define the ACLs left out of the sketch
correctly, of course. Moreover, I would rename toSafePorts to
toConnectableDestinations or similar to emphasize that this is the right
place to ban CONNECTs to wrong/dangerous/etc. addresses.


> I'm also afraid that your proposal also prevents us from disabling
> CONNECTs later

If you are saying that my simple sketch does not address all possible
use cases, then I certainly agree! I believe it addressed what OP
requested, but if I misinterpreted his or her desires, I apologize. I
hope the general description quoted at the start of this email combined
with Amos and yours warnings about undesirable CONNECT destinations will
allow them to fix their configuration as needed.

Alex.


From john at bluemarble.net  Wed Jan  3 20:06:42 2018
From: john at bluemarble.net (John Ratliff)
Date: Wed, 3 Jan 2018 15:06:42 -0500
Subject: [squid-users] questions setting up transparent proxy
Message-ID: <6b1d2ed1-640e-502a-876c-ac37b4345ed7@bluemarble.net>

When I try to setup squid as a transparent proxy, I never get any 
response from Squid.

I can make it work fine as a regular proxy using Firefox.

I've tried it on a Debian 9 server and a CentOS 7 server, and I get the 
same result.

This is my configuration for the CentOS 7 server. I've put it wide open 
right now.

acl localnet src 10.0.0.0/8     # RFC1918 possible internal network
acl localnet src 172.16.0.0/12  # RFC1918 possible internal network
acl localnet src 192.168.0.0/16 # RFC1918 possible internal network
acl localnet src fc00::/7       # RFC 4193 local private network range
acl localnet src fe80::/10      # RFC 4291 link-local
acl SSL_ports port 443
acl Safe_ports port 80          # http
acl Safe_ports port 21          # ftp
acl Safe_ports port 443         # https
acl Safe_ports port 70          # gopher
acl Safe_ports port 210         # wais
acl Safe_ports port 1025-65535  # unregistered ports
acl Safe_ports port 280         # http-mgmt
acl Safe_ports port 488         # gss-http
acl Safe_ports port 591         # filemaker
acl Safe_ports port 777         # multiling http
acl CONNECT method CONNECT
http_access deny !Safe_ports
http_access deny CONNECT !SSL_ports
http_access allow localhost manager
http_access deny manager
http_access allow localnet
http_access allow localhost
http_access allow all
http_port 3128 intercept
coredump_dir /var/spool/squid
refresh_pattern ^ftp:           1440    20%     10080
refresh_pattern ^gopher:        1440    0%      1440
refresh_pattern -i (/cgi-bin/|\?) 0     0%      0
refresh_pattern .               0       20%     4320

When I try a wget request from a server that is being redirected to 
Squid, I get this:

$ wget debian.org
--2018-01-03 14:50:24--  http://debian.org/
Resolving debian.org (debian.org)... 130.89.148.14, 149.20.4.15, 
128.31.0.62, ...
Connecting to debian.org (debian.org)|130.89.148.14|:80... connected.
HTTP request sent, awaiting response... No data received.
Retrying.

If I remove 'intercept' from the http_port directive, I get 400 Bad 
Request instead.

$ wget debian.org
--2018-01-03 14:49:22--  http://debian.org/
Resolving debian.org (debian.org)... 5.153.231.4, 130.89.148.14, 
149.20.4.15, ...
Connecting to debian.org (debian.org)|5.153.231.4|:80... connected.
HTTP request sent, awaiting response... 400 Bad Request
2018-01-03 14:49:22 ERROR 400: Bad Request.

Both machines are behind the same firewall. I used
iptables -t nat -A PREROUTING -p tcp --dport 80 -j DNAT --to 
10.77.9.120:3128

to do the traffic redirect.

Traffic flows to the server running squid. I can verify this with 
tcpdump. The packets are making it from wget to the server. I just don't 
know what happens after that.

Thanks.


From Antony.Stone at squid.open.source.it  Wed Jan  3 20:26:55 2018
From: Antony.Stone at squid.open.source.it (Antony Stone)
Date: Wed, 3 Jan 2018 21:26:55 +0100
Subject: [squid-users] questions setting up transparent proxy
In-Reply-To: <6b1d2ed1-640e-502a-876c-ac37b4345ed7@bluemarble.net>
References: <6b1d2ed1-640e-502a-876c-ac37b4345ed7@bluemarble.net>
Message-ID: <201801032126.56238.Antony.Stone@squid.open.source.it>

On Wednesday 03 January 2018 at 21:06:42, John Ratliff wrote:

> When I try to setup squid as a transparent proxy, I never get any
> response from Squid.

> When I try a wget request from a server that is being redirected

How (and more importantly, where) are you doing the redirect?

> Both machines are behind the same firewall. I used
> iptables -t nat -A PREROUTING -p tcp --dport 80 -j DNAT --to
> 10.77.9.120:3128

If that firewall is not on the machine running Squid, then that's your problem.

> Traffic flows to the server running squid. I can verify this with
> tcpdump. The packets are making it from wget to the server. I just don't
> know what happens after that.

https://wiki.squid-cache.org/ConfigExamples/Intercept/LinuxRedirect

"NOTE: This configuration is given for use *on the squid box*. This is required 
to perform intercept accurately and securely. To intercept from a gateway 
machine and direct traffic at a *separate squid box* use policy routing."

https://wiki.squid-cache.org/ConfigExamples/Intercept/IptablesPolicyRoute


Antony.

-- 
Schr?dinger's rule of data integrity: the condition of any backup is unknown 
until a restore is attempted.

                                                   Please reply to the list;
                                                         please *don't* CC me.


From brian.bergstrom at sportsengine.com  Wed Jan  3 22:30:29 2018
From: brian.bergstrom at sportsengine.com (brianbergstrom)
Date: Wed, 3 Jan 2018 15:30:29 -0700 (MST)
Subject: [squid-users] ALPN, HTTP/2 and sslbump
In-Reply-To: <177b9497-8b06-2049-c74b-02b77d151fe5@treenet.co.nz>
References: <BY2PR17MB018222156B827DDF84E7C6FCF7560@BY2PR17MB0182.namprd17.prod.outlook.com>
 <177b9497-8b06-2049-c74b-02b77d151fe5@treenet.co.nz>
Message-ID: <1515018629106-0.post@n4.nabble.com>

I am using Squid 3.5.27 and recently started having issues when I upgraded
from openssl 1.0.1 to 1.0.2 which I believe introduced support for h2/ALPN. 
I have narrowed down the issue to a request that fails but succeeds with
curl's --no-alpn flag.  

Here is the error message from Squid for the failure, though the request
ends up timing out with an EOF error.
Handshake with SSL server failed: error:140920E3:SSL
routines:ssl3_get_server_hello:parse tlsext

A tcpdump of the failure when curl sends ALPN which contains http/1.1 and h2
as its client protocols, of which the Server Hello replies and chooses h2.

A tcpdump of successful request with the --no-alpn flag verifies that no
ALPN TLS extension data is present.

If I understand the docs and this thread correctly, Squid should be removing
h2 from the ALPN in the Client Hello since Squid does not support it.  But
it appears to be passing it through and failing when the server chooses it.

The relavent lines from my squid.conf:
http_port 3130 ssl-bump cert=/etc/squid/squid.pem
follow_x_forwarded_for allow localnet

cache deny all

acl SSL_Port port 443
acl Proxy_port port 3130
http_access allow Proxy_port
http_access allow SSL_Port

acl allowed_http_sites dstdom_regex '/etc/squid/trusted_http_sites.lst'
acl allowed_https_sites ssl::server_name_regex
'/etc/squid/trusted_https_sites.lst'
acl allowed_https_ips dst '/etc/squid/trusted_https_ips.lst'

http_access allow allowed_http_sites

acl step1 at_step SslBump1
acl step2 at_step SslBump2
acl step3 at_step SslBump3

ssl_bump peek step1
ssl_bump peek step2 allowed_https_sites
ssl_bump peek step2 allowed_https_ips
ssl_bump splice step3 allowed_https_sites
ssl_bump splice step3 allowed_https_ips
ssl_bump terminate step2 all

http_access deny all




--
Sent from: http://squid-web-proxy-cache.1019090.n4.nabble.com/Squid-Users-f1019091.html


From rousskov at measurement-factory.com  Wed Jan  3 23:37:24 2018
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Wed, 3 Jan 2018 16:37:24 -0700
Subject: [squid-users] ALPN, HTTP/2 and sslbump
In-Reply-To: <1515018629106-0.post@n4.nabble.com>
References: <BY2PR17MB018222156B827DDF84E7C6FCF7560@BY2PR17MB0182.namprd17.prod.outlook.com>
 <177b9497-8b06-2049-c74b-02b77d151fe5@treenet.co.nz>
 <1515018629106-0.post@n4.nabble.com>
Message-ID: <cf99de5a-85e9-0695-73ff-db95c5236c42@measurement-factory.com>

On 01/03/2018 03:30 PM, brianbergstrom wrote:

> If I understand the docs and this thread correctly, Squid should be removing
> h2 from the ALPN in the Client Hello since Squid does not support it.

Please note that Squid cannot remove something when using "peek" and
"splice" actions.

I do not know whether Squid removes unsupported ALPN values when using
"stare" and "bump" actions, and I would not be surprised to learn that
Squid does not police those values at all (yet), but I want to emphasize
that the combination of "removing" and "splicing" is impossible.


> ssl_bump peek step1
> ssl_bump peek step2 allowed_https_sites
> ssl_bump peek step2 allowed_https_ips
> ssl_bump splice step3 allowed_https_sites
> ssl_bump splice step3 allowed_https_ips
> ssl_bump terminate step2 all


HTH,

Alex.


From squid3 at treenet.co.nz  Wed Jan  3 23:40:35 2018
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 4 Jan 2018 12:40:35 +1300
Subject: [squid-users] ALPN, HTTP/2 and sslbump
In-Reply-To: <1515018629106-0.post@n4.nabble.com>
References: <BY2PR17MB018222156B827DDF84E7C6FCF7560@BY2PR17MB0182.namprd17.prod.outlook.com>
 <177b9497-8b06-2049-c74b-02b77d151fe5@treenet.co.nz>
 <1515018629106-0.post@n4.nabble.com>
Message-ID: <06a143c7-00a5-9e4c-38b2-e4079d9f12b5@treenet.co.nz>

On 04/01/18 11:30, brianbergstrom wrote:
> I am using Squid 3.5.27 and recently started having issues when I upgraded
> from openssl 1.0.1 to 1.0.2 which I believe introduced support for h2/ALPN.
> I have narrowed down the issue to a request that fails but succeeds with
> curl's --no-alpn flag.
> 
> Here is the error message from Squid for the failure, though the request
> ends up timing out with an EOF error.
> Handshake with SSL server failed: error:140920E3:SSL
> routines:ssl3_get_server_hello:parse tlsext
> 
> A tcpdump of the failure when curl sends ALPN which contains http/1.1 and h2
> as its client protocols, of which the Server Hello replies and chooses h2.
> 
> A tcpdump of successful request with the --no-alpn flag verifies that no
> ALPN TLS extension data is present.
> 
> If I understand the docs and this thread correctly, Squid should be removing
> h2 from the ALPN in the Client Hello since Squid does not support it.  But
> it appears to be passing it through and failing when the server chooses it.

Sort of, but not quite.

What you have configured is 'splice' - so Squid is constrained to 
delivering both the TLS handshake and the encrypted data from the client 
exactly as-is to the server. The APLN removal happens when Squid is able 
to alter the handshake - eg for the 'bump' action.

The existence of things like ALPN should not matter to Squid when 
splicing as the HTTP inside the encryption is never even looked at.

However, for the 'peek' action to succeed your OpenSSL library on the 
Squid machine does need to support the TLS features being used by both 
client and server endpoints. In this case it appears that the TLS 
extension message in TLS itself is not being parsed correctly by the 
library.


Your best options (in order of preference) are:

* update to an even more recent OpsenSSL version (1.1 etc) in hopes that 
the ALPN support is more correct than the 1.0.2 code, and/or

* move your squid.conf "ssl_bump splice ..." above the "ssl_bump peek 
.." lines so the splicing happens earlier if the client SNI details are 
sufficient to make the decision, and/or

* try an upgrade to Squid-4. We have redesigned the handshake parsing in 
that version not to depend so much on OpenSSL.

If even the latest Squid with latest OpenSSL library have the issue 
please file a bug report. It will need a copy of your config settings, 
and the tcpdump full-packet trace of the server handshake which is failing.


Amos


From squid3 at treenet.co.nz  Wed Jan  3 23:47:53 2018
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 4 Jan 2018 12:47:53 +1300
Subject: [squid-users] ALPN, HTTP/2 and sslbump
In-Reply-To: <cf99de5a-85e9-0695-73ff-db95c5236c42@measurement-factory.com>
References: <BY2PR17MB018222156B827DDF84E7C6FCF7560@BY2PR17MB0182.namprd17.prod.outlook.com>
 <177b9497-8b06-2049-c74b-02b77d151fe5@treenet.co.nz>
 <1515018629106-0.post@n4.nabble.com>
 <cf99de5a-85e9-0695-73ff-db95c5236c42@measurement-factory.com>
Message-ID: <c13c5797-6ff0-36b5-403d-f8ea6618ec3b@treenet.co.nz>

On 04/01/18 12:37, Alex Rousskov wrote:
> On 01/03/2018 03:30 PM, brianbergstrom wrote:
> 
>> If I understand the docs and this thread correctly, Squid should be removing
>> h2 from the ALPN in the Client Hello since Squid does not support it.
> 
> Please note that Squid cannot remove something when using "peek" and
> "splice" actions.
> 
> I do not know whether Squid removes unsupported ALPN values when using
> "stare" and "bump" actions, and I would not be surprised to learn that
> Squid does not police those values at all (yet),

It does *unless* peeking at the server handshake: 
<https://github.com/squid-cache/squid/blob/v3.5/src/ssl/bio.cc#L1261>.

Amos


From john at bluemarble.net  Thu Jan  4 01:09:01 2018
From: john at bluemarble.net (John Ratliff)
Date: Wed, 3 Jan 2018 20:09:01 -0500
Subject: [squid-users] questions setting up transparent proxy
In-Reply-To: <201801032126.56238.Antony.Stone@squid.open.source.it>
References: <6b1d2ed1-640e-502a-876c-ac37b4345ed7@bluemarble.net>
 <201801032126.56238.Antony.Stone@squid.open.source.it>
Message-ID: <8fc1a8c6-eac9-97bf-8b3f-c11e75cbeeec@bluemarble.net>

On 1/3/2018 3:26 PM, Antony Stone wrote:
> On Wednesday 03 January 2018 at 21:06:42, John Ratliff wrote:
> 
>> When I try to setup squid as a transparent proxy, I never get any
>> response from Squid.
> 
>> When I try a wget request from a server that is being redirected
> 
> How (and more importantly, where) are you doing the redirect?
> 
>> Both machines are behind the same firewall. I used
>> iptables -t nat -A PREROUTING -p tcp --dport 80 -j DNAT --to
>> 10.77.9.120:3128
> 
> If that firewall is not on the machine running Squid, then that's your problem.
> 
>> Traffic flows to the server running squid. I can verify this with
>> tcpdump. The packets are making it from wget to the server. I just don't
>> know what happens after that.
> 
> https://wiki.squid-cache.org/ConfigExamples/Intercept/LinuxRedirect
> 
> "NOTE: This configuration is given for use *on the squid box*. This is required
> to perform intercept accurately and securely. To intercept from a gateway
> machine and direct traffic at a *separate squid box* use policy routing."
> 
> https://wiki.squid-cache.org/ConfigExamples/Intercept/IptablesPolicyRoute
> 
> 
> Antony.
> 

Thanks. I put squid on the firewall itself. It works for http, but not 
for https. I get errors with curl and wget.

$ curl https://debian.org
curl: (35) error:140770FC:SSL routines:SSL23_GET_SERVER_HELLO:unknown 
protocol

$ wget https://debian.org
--2018-01-03 20:02:45--  https://debian.org/
Resolving debian.org (debian.org)... 5.153.231.4, 128.31.0.62, 
130.89.148.14, ...
Connecting to debian.org (debian.org)|5.153.231.4|:443... connected.
GnuTLS: An unexpected TLS packet was received.
Unable to establish SSL connection.

I made some config changes:

http_port 3128 intercept
http_port 3129 intercept ssl-bump generate-host-certificates=on 
cert=/etc/squid/squid.pem

sslcrtd_program /usr/lib/squid/ssl_crtd -s /var/lib/ssl_db -M 4MB

ssl_bump bump all

Here are my PREROUTING nat table rules.

REDIRECT tcp  --  10.77.9.0/24 anywhere tcp dpt:http redir ports 3128
REDIRECT tcp  --  10.77.9.0/24 anywhere tcp dpt:https redir ports 3129

And in the INPUT chain of the filter table:

ACCEPT tcp  --  10.77.9.0/24 anywhere tcp dpt:3128
ACCEPT tcp  --  10.77.9.0/24 anywhere tcp dpt:3129

The server I am on has IP 10.77.9.102.



From squid3 at treenet.co.nz  Thu Jan  4 02:05:29 2018
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 4 Jan 2018 15:05:29 +1300
Subject: [squid-users] questions setting up transparent proxy
In-Reply-To: <8fc1a8c6-eac9-97bf-8b3f-c11e75cbeeec@bluemarble.net>
References: <6b1d2ed1-640e-502a-876c-ac37b4345ed7@bluemarble.net>
 <201801032126.56238.Antony.Stone@squid.open.source.it>
 <8fc1a8c6-eac9-97bf-8b3f-c11e75cbeeec@bluemarble.net>
Message-ID: <a4e70cbf-1463-8631-ed44-fee25124fda3@treenet.co.nz>

On 04/01/18 14:09, John Ratliff wrote:
> On 1/3/2018 3:26 PM, Antony Stone wrote:
>> On Wednesday 03 January 2018 at 21:06:42, John Ratliff wrote:
>>
>>> When I try to setup squid as a transparent proxy, I never get any
>>> response from Squid.
>>
>>> When I try a wget request from a server that is being redirected
>>
>> How (and more importantly, where) are you doing the redirect?
>>
>>> Both machines are behind the same firewall. I used
>>> iptables -t nat -A PREROUTING -p tcp --dport 80 -j DNAT --to
>>> 10.77.9.120:3128
>>
>> If that firewall is not on the machine running Squid, then that's your 
>> problem.
>>
>>> Traffic flows to the server running squid. I can verify this with
>>> tcpdump. The packets are making it from wget to the server. I just don't
>>> know what happens after that.
>>
>> https://wiki.squid-cache.org/ConfigExamples/Intercept/LinuxRedirect
>>
>> "NOTE: This configuration is given for use *on the squid box*. This is 
>> required
>> to perform intercept accurately and securely. To intercept from a gateway
>> machine and direct traffic at a *separate squid box* use policy routing."
>>
>> https://wiki.squid-cache.org/ConfigExamples/Intercept/IptablesPolicyRoute
>>
>>
>> Antony.
>>
> 
> Thanks. I put squid on the firewall itself. It works for http, but not 
> for https. I get errors with curl and wget.
> 
> $ curl https://debian.org
> curl: (35) error:140770FC:SSL routines:SSL23_GET_SERVER_HELLO:unknown 
> protocol
> 
> $ wget https://debian.org
> --2018-01-03 20:02:45--? https://debian.org/
> Resolving debian.org (debian.org)... 5.153.231.4, 128.31.0.62, 
> 130.89.148.14, ...
> Connecting to debian.org (debian.org)|5.153.231.4|:443... connected.
> GnuTLS: An unexpected TLS packet was received.
> Unable to establish SSL connection.
> 
> I made some config changes:
> 
> http_port 3128 intercept
> http_port 3129 intercept ssl-bump generate-host-certificates=on 
> cert=/etc/squid/squid.pem

That should be:

  https_port 3129 intercept ssl-bump generate-host-certificates=on \
    cert=/etc/squid/squid.pem

Note the 's' in https_port.


> 
> sslcrtd_program /usr/lib/squid/ssl_crtd -s /var/lib/ssl_db -M 4MB
> 
> ssl_bump bump all


This instructs Squid to bump before even receiving the client TLS 
handshake - ie. generate a server certificate with zero details to work 
with about what the client wants.
That leads to a LOT of problems and security issues. Please do not do that.

See <https://wiki.squid-cache.org/Features/SslPeekAndSplice> for better 
config examples.


> 
> Here are my PREROUTING nat table rules.
> 
> REDIRECT tcp? --? 10.77.9.0/24 anywhere tcp dpt:http redir ports 3128
> REDIRECT tcp? --? 10.77.9.0/24 anywhere tcp dpt:https redir ports 3129
> 
> And in the INPUT chain of the filter table:
> 
> ACCEPT tcp? --? 10.77.9.0/24 anywhere tcp dpt:3128
> ACCEPT tcp? --? 10.77.9.0/24 anywhere tcp dpt:3129
> 
> The server I am on has IP 10.77.9.102.
> 


You appear to be missing the MASQUERADE rule to send packets back to the 
client.

Also the mangle table (*not* filter) rules are important to block 
external traffic directly to those Squid ports without interfering with 
the NAT operations.

<https://wiki.squid-cache.org/ConfigExamples/Intercept/LinuxRedirect>

Amos


From umuta at sabanciuniv.edu  Thu Jan  4 06:43:22 2018
From: umuta at sabanciuniv.edu (Umut Arus)
Date: Thu, 4 Jan 2018 09:43:22 +0300
Subject: [squid-users] Caching for download servers
In-Reply-To: <49a92522-bf38-4d80-f89b-8fad5b11f26e@treenet.co.nz>
References: <CALwryzEAZFPf4AWQKrSN68sUuGthGarb48LkDT9zbiyJjdjeZg@mail.gmail.com>
 <49a92522-bf38-4d80-f89b-8fad5b11f26e@treenet.co.nz>
Message-ID: <CALwryzGMq2pMP3Dk=k6hxEbULjQGyVAQrY+raUJoM2bcqKQRAg@mail.gmail.com>

Thank you. It seems a nice guide. I mean caching some destinations used for
download without doing any setup on client side. Is it possible to use dns
to proxy redirection for some destination zones?

Regards.

On Wed, Jan 3, 2018 at 3:25 PM, Amos Jeffries <squid3 at treenet.co.nz> wrote:

> On 03/01/18 20:25, Umut Arus wrote:
>
>> Hi,
>>
>> I'd like to ask about how redirect a client to squid server for only some
>> destination domain zone (or IP addresses). We would like to cache some
>> download server without doing any setup on client side.
>>
>>
> What do you mean by "cache some download server" ?
>
> It sounds a bit like you are looking for NAT interception (<
> https://wiki.squid-cache.org/ConfigExamples/Intercept/LinuxRedirect>)
>
> Amos
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>



-- 
*Umut Arus*
System Specialist
Information Technology
Sabanc? University

Phone: +90216 483 9172
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20180104/bde1aa40/attachment.htm>

From rentorbuy at yahoo.com  Thu Jan  4 08:51:13 2018
From: rentorbuy at yahoo.com (Vieri)
Date: Thu, 4 Jan 2018 08:51:13 +0000 (UTC)
Subject: [squid-users] TCP out of memory
References: <601581447.203369.1515055873511.ref@mail.yahoo.com>
Message-ID: <601581447.203369.1515055873511@mail.yahoo.com>

Hi again,

I haven't taken a look at Squid's source code, but I guess that when Squid communicates with a c-icap service it acts as a typical socket client, right?
eg. connect(), write(), read(), close()

Does Squid consider forcing disconnection (close()) if the read() is "too long"?
Is there such a timeout? Is it configurable in squid.conf (only for the c-icap connection)?


Thanks,

Vieri


From john at bluemarble.net  Thu Jan  4 14:34:36 2018
From: john at bluemarble.net (John Ratliff)
Date: Thu, 4 Jan 2018 09:34:36 -0500
Subject: [squid-users] questions setting up transparent proxy
In-Reply-To: <a4e70cbf-1463-8631-ed44-fee25124fda3@treenet.co.nz>
References: <6b1d2ed1-640e-502a-876c-ac37b4345ed7@bluemarble.net>
 <201801032126.56238.Antony.Stone@squid.open.source.it>
 <8fc1a8c6-eac9-97bf-8b3f-c11e75cbeeec@bluemarble.net>
 <a4e70cbf-1463-8631-ed44-fee25124fda3@treenet.co.nz>
Message-ID: <90c6daf1-c2b5-09be-fe08-6a6059c301f8@bluemarble.net>

On 1/3/2018 9:05 PM, Amos Jeffries wrote:
 > On 04/01/18 14:09, John Ratliff wrote:
 >> On 1/3/2018 3:26 PM, Antony Stone wrote:
 >>> On Wednesday 03 January 2018 at 21:06:42, John Ratliff wrote:
 >>>
 >>>> When I try to setup squid as a transparent proxy, I never get any
 >>>> response from Squid.
 >>>
 >>>> When I try a wget request from a server that is being redirected
 >>>
 >>> How (and more importantly, where) are you doing the redirect?
 >>>
 >>>> Both machines are behind the same firewall. I used
 >>>> iptables -t nat -A PREROUTING -p tcp --dport 80 -j DNAT --to
 >>>> 10.77.9.120:3128
 >>>
 >>> If that firewall is not on the machine running Squid, then that's
 >>> your problem.
 >>>
 >>>> Traffic flows to the server running squid. I can verify this with
 >>>> tcpdump. The packets are making it from wget to the server. I just
 >>>> don't
 >>>> know what happens after that.
 >>>
 >>> https://wiki.squid-cache.org/ConfigExamples/Intercept/LinuxRedirect
 >>>
 >>> "NOTE: This configuration is given for use *on the squid box*. This
 >>> is required
 >>> to perform intercept accurately and securely. To intercept from a
 >>> gateway
 >>> machine and direct traffic at a *separate squid box* use policy
 >>> routing."
 >>>
 >>> 
https://wiki.squid-cache.org/ConfigExamples/Intercept/IptablesPolicyRoute
 >>>
 >>>
 >>>
 >>> Antony.
 >>>
 >>
 >> Thanks. I put squid on the firewall itself. It works for http, but not
 >> for https. I get errors with curl and wget.
 >>
 >> $ curl https://debian.org
 >> curl: (35) error:140770FC:SSL routines:SSL23_GET_SERVER_HELLO:unknown
 >> protocol
 >>
 >> $ wget https://debian.org
 >> --2018-01-03 20:02:45--  https://debian.org/
 >> Resolving debian.org (debian.org)... 5.153.231.4, 128.31.0.62,
 >> 130.89.148.14, ...
 >> Connecting to debian.org (debian.org)|5.153.231.4|:443... connected.
 >> GnuTLS: An unexpected TLS packet was received.
 >> Unable to establish SSL connection.
 >>
 >> I made some config changes:
 >>
 >> http_port 3128 intercept
 >> http_port 3129 intercept ssl-bump generate-host-certificates=on
 >> cert=/etc/squid/squid.pem
 >
 > That should be:
 >
 >   https_port 3129 intercept ssl-bump generate-host-certificates=on \
 >     cert=/etc/squid/squid.pem
 >
 > Note the 's' in https_port.
Thanks. This was the issue.

 >
 >
 >>
 >> sslcrtd_program /usr/lib/squid/ssl_crtd -s /var/lib/ssl_db -M 4MB
 >>
 >> ssl_bump bump all
I changed it to peek and splice.

 >
 >
 > This instructs Squid to bump before even receiving the client TLS
 > handshake - ie. generate a server certificate with zero details to work
 > with about what the client wants.
 > That leads to a LOT of problems and security issues. Please do not do 
that.
 >
 > See <https://wiki.squid-cache.org/Features/SslPeekAndSplice> for better
 > config examples.
 >
 >
 >>
 >> Here are my PREROUTING nat table rules.
 >>
 >> REDIRECT tcp  --  10.77.9.0/24 anywhere tcp dpt:http redir ports 3128
 >> REDIRECT tcp  --  10.77.9.0/24 anywhere tcp dpt:https redir ports 3129
 >>
 >> And in the INPUT chain of the filter table:
 >>
 >> ACCEPT tcp  --  10.77.9.0/24 anywhere tcp dpt:3128
 >> ACCEPT tcp  --  10.77.9.0/24 anywhere tcp dpt:3129
 >>
 >> The server I am on has IP 10.77.9.102.
 >>
 >
 >
 > You appear to be missing the MASQUERADE rule to send packets back to the
 > client.
I have SNAT rules instead. There are many IPs on this firewall.

 >
 > Also the mangle table (*not* filter) rules are important to block
 > external traffic directly to those Squid ports without interfering with
 > the NAT operations.
I didn't post these rules, but I made them. Thanks.


Thanks.


From squid3 at treenet.co.nz  Fri Jan  5 07:06:54 2018
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Fri, 5 Jan 2018 20:06:54 +1300
Subject: [squid-users] Caching for download servers
In-Reply-To: <CALwryzGMq2pMP3Dk=k6hxEbULjQGyVAQrY+raUJoM2bcqKQRAg@mail.gmail.com>
References: <CALwryzEAZFPf4AWQKrSN68sUuGthGarb48LkDT9zbiyJjdjeZg@mail.gmail.com>
 <49a92522-bf38-4d80-f89b-8fad5b11f26e@treenet.co.nz>
 <CALwryzGMq2pMP3Dk=k6hxEbULjQGyVAQrY+raUJoM2bcqKQRAg@mail.gmail.com>
Message-ID: <71a4b655-2df8-d5c3-91b8-c76d672df8ae@treenet.co.nz>

On 04/01/18 19:43, Umut Arus wrote:
> Thank you. It seems a nice guide. I mean caching some destinations used 
> for download without doing any setup on client side. Is it possible to 
> use dns to proxy redirection for some destination zones?

No. Well, it may be _possible_ but very, very far from safe.

When intercepting traffic there are some *extremely* nasty security 
issues involved with Host header that have to be avoided. The details 
can be found at 
<https://wiki.squid-cache.org/KnowledgeBase/HostHeaderForgery>.

The only way to safely avoid lots of false errors is to relay traffic to 
the dst-IP the client presents when the security checks fail.

But if you alter DNS so Squid and clients see different things then 
*all* the traffic shows up as forged and the dst-IP will be the proxies 
own IP.

So there is nowhere the proxy can connect to which will provide the 
content needed. Attempts to do so loops infinitely back to the proxy.


Amos


From squid3 at treenet.co.nz  Fri Jan  5 07:40:24 2018
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Fri, 5 Jan 2018 20:40:24 +1300
Subject: [squid-users] TCP out of memory
In-Reply-To: <601581447.203369.1515055873511@mail.yahoo.com>
References: <601581447.203369.1515055873511.ref@mail.yahoo.com>
 <601581447.203369.1515055873511@mail.yahoo.com>
Message-ID: <1042f27d-78e2-aa59-3fa7-804ded30ea13@treenet.co.nz>

On 04/01/18 21:51, Vieri wrote:
> Hi again,
> 
> I haven't taken a look at Squid's source code, but I guess that when Squid communicates with a c-icap service it acts as a typical socket client, right?
> eg. connect(), write(), read(), close()

Uh, Those are system calls for receiving TCP connections and data I/O.

Squid uses ICAP protocol to communicate with ICAP services. ICAP 
operates as a layer over TCP. So in a way yes, and in a way no.


> 
> Does Squid consider forcing disconnection (close()) if the read() is "too long"?

If you mean "too long" in terms of memory size - there is no such thing 
in TCP. Squid provides a buffer and tells the kernel how much space is 
available there. The OS writes up to that much, no more, maybe less.

If your title "TCP out of memory" is an error you are seeing somewhere. 
That is entirely your system kernel and networking stacks issue. Squid 
has nothing to do with the internal memory management of TCP traffic.


> Is there such a timeout? Is it configurable in squid.conf (only for the c-icap connection)?
> 

What timeout you speak of?

If you mean "too long" earlier in terms of duration to perform a read() 
- there is also no such thing. The OS tells Squid when data is ready and 
the data copy from OS memory to Squid buffer takes trivial amount of 
time to actually happen.


Timeouts can happen between different parts of the ICAP protocol waiting 
for I/O to happen for related message parts. But those have nothing to 
do with TCP memory unless you are suffering a bad case of buffer bloat 
in the network itself.

Amos


From squid3 at treenet.co.nz  Fri Jan  5 08:06:29 2018
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Fri, 5 Jan 2018 21:06:29 +1300
Subject: [squid-users] Squid crash: assertion failed:
 store_swapout.cc:289: "mem->swapout.sio == self"
In-Reply-To: <CAG2Qp6sdheuVKHTEW6OHnnCCc7e2qYBq7TFoUspnr1g_siCaPQ@mail.gmail.com>
References: <CAG2Qp6sdheuVKHTEW6OHnnCCc7e2qYBq7TFoUspnr1g_siCaPQ@mail.gmail.com>
Message-ID: <06d47215-f64f-fca2-2a66-628edf8425fc@treenet.co.nz>

On 04/01/18 04:53, Roberto Carna wrote:
> Dear, I have updated Squid on pfSense to 0.4.42_1 version. But after
> start the service togeteher with squidGuard, Squid crashes.
> 
> I try running from CLI in debug mode:
> 
> # squid -d 10
> [2.4.2-RELEASE][admin at FW-Pfsense-Guest.g-bapro.net]/var/log:
> 2018/01/03 12:46:44 kid1| Starting Squid Cache version 3.5.27 for
> amd64-portbld-freebsd11.1...
> 2018/01/03 12:46:44 kid1| Service Name: squid
> 2018/01/03 12:46:50 kid1| assertion failed: store_swapout.cc:289:
> "mem->swapout.sio == self"

> How can I do ??? What's the problem ???
> 

This is <https://bugs.squid-cache.org/show_bug.cgi?id=4624>.
See comment #1 for possible workaround, but other than that I'm not 
aware of any actual solution yet existing.

Amos


From rentorbuy at yahoo.com  Fri Jan  5 08:59:15 2018
From: rentorbuy at yahoo.com (Vieri)
Date: Fri, 5 Jan 2018 08:59:15 +0000 (UTC)
Subject: [squid-users] TCP out of memory
In-Reply-To: <1042f27d-78e2-aa59-3fa7-804ded30ea13@treenet.co.nz>
References: <601581447.203369.1515055873511.ref@mail.yahoo.com>
 <601581447.203369.1515055873511@mail.yahoo.com>
 <1042f27d-78e2-aa59-3fa7-804ded30ea13@treenet.co.nz>
Message-ID: <1464426577.854952.1515142755546@mail.yahoo.com>

The open sockets to 127.0.0.1:1344 keep increasing steadily even on high network usage, but they do not decrease when there's little or no traffic.
So, day after day the overall number keeps growing until I have to restart squid once or twice a week.

In other words, this value keeps growing:
Largest file desc currently in use:   xxxx
This other value can decrease at times, but in the long run it keeps growing too: 
Number of file desc currently in use: xxxx

I tried changing squid parameters such as:

icap_io_timeout time-units
icap_service_failure_limit
icap_persistent_connections on

I also tried changing c-icap parameters such as:

Timeout
MaxKeepAliveRequests
KeepAliveTimeout
StartServers
MaxServers
MinSpareThreads
MaxSpareThreads
ThreadsPerChild
MaxRequestsPerChild

However, I'm still seeing the same behavior so I reverted back to defaults.

This is a small sample of an icap_log taken from Squid:

1515056868.505      9 ffff:ffff:ffff:ffff:ffff:ffff:ffff:ffff ICAP_OPT/200 353 OPTIONS icap://127.0.0.1:1344/clamav - -/127.0.0.1 -
1515056868.506     10 10.215.246.143 ICAP_ECHO/204 105 RESPMOD icap://127.0.0.1:1344/clamav - -/127.0.0.1 -
1515056868.506     10 10.215.246.143 ICAP_ECHO/204 105 RESPMOD icap://127.0.0.1:1344/clamav - -/127.0.0.1 -
1515056868.511   1624 10.215.246.143 ICAP_MOD/200 306783 RESPMOD icap://127.0.0.1:1344/clamav - -/127.0.0.1 -
1515056868.547   1450 10.215.246.143 ICAP_ECHO/204 130 RESPMOD icap://127.0.0.1:1344/clamav - -/127.0.0.1 -
1515056868.560     64 10.215.246.143 ICAP_ECHO/204 105 RESPMOD icap://127.0.0.1:1344/clamav - -/127.0.0.1 -
1515056868.560     64 10.215.246.143 ICAP_ECHO/204 130 RESPMOD icap://127.0.0.1:1344/clamav - -/127.0.0.1 -
1515056868.561      0 10.215.248.99 ICAP_ECHO/204 105 RESPMOD icap://127.0.0.1:1344/clamav - -/127.0.0.1 -
1515056868.561     65 10.215.246.143 ICAP_ECHO/204 130 RESPMOD icap://127.0.0.1:1344/clamav - -/127.0.0.1 -
1515056868.615      5 10.215.246.143 ICAP_ECHO/204 130 RESPMOD icap://127.0.0.1:1344/clamav - -/127.0.0.1 -
1515056868.660      3 10.215.246.143 ICAP_ECHO/204 130 RESPMOD icap://127.0.0.1:1344/clamav - -/127.0.0.1 -
1515056868.669      3 10.215.246.143 ICAP_ECHO/204 130 RESPMOD icap://127.0.0.1:1344/clamav - -/127.0.0.1 -
1515056868.681      6 10.215.246.143 ICAP_ECHO/204 130 RESPMOD icap://127.0.0.1:1344/clamav - -/127.0.0.1 -
1515056868.701      5 10.215.248.31 ICAP_ECHO/204 105 RESPMOD icap://127.0.0.1:1344/clamav - -/127.0.0.1 -
1515056868.712     18 10.215.246.143 ICAP_ECHO/204 130 RESPMOD icap://127.0.0.1:1344/clamav - -/127.0.0.1 -
1515056868.720      0 10.215.246.143 ICAP_MOD/200 489 RESPMOD icap://127.0.0.1:1344/clamav - -/127.0.0.1 -
1515056868.739     29 10.215.246.143 ICAP_ECHO/204 130 RESPMOD icap://127.0.0.1:1344/clamav - -/127.0.0.1 -
1515056868.740      0 10.215.246.143 ICAP_ECHO/204 105 RESPMOD icap://127.0.0.1:1344/clamav - -/127.0.0.1 -
1515056868.750      0 10.215.246.143 ICAP_ECHO/204 105 RESPMOD icap://127.0.0.1:1344/clamav - -/127.0.0.1 -
1515056868.774      0 10.215.246.143 ICAP_ECHO/204 105 RESPMOD icap://127.0.0.1:1344/clamav - -/127.0.0.1 -
1515056868.848      0 10.215.246.143 ICAP_ECHO/204 105 RESPMOD icap://127.0.0.1:1344/clamav - -/127.0.0.1 -
1515056868.865      0 10.215.246.143 ICAP_ECHO/204 105 RESPMOD icap://127.0.0.1:1344/clamav - -/127.0.0.1 -
1515056868.866      6 10.215.246.143 ICAP_ECHO/204 130 RESPMOD icap://127.0.0.1:1344/clamav - -/127.0.0.1 -
1515056868.879      0 10.215.246.143 ICAP_ECHO/204 105 RESPMOD icap://127.0.0.1:1344/clamav - -/127.0.0.1 -
1515056868.891      0 10.215.248.31 ICAP_ECHO/204 105 RESPMOD icap://127.0.0.1:1344/clamav - -/127.0.0.1 -
1515056868.893      2 10.215.246.143 ICAP_ECHO/204 105 RESPMOD icap://127.0.0.1:1344/clamav - -/127.0.0.1 -
1515056868.906      2 10.215.246.143 ICAP_ECHO/204 105 RESPMOD icap://127.0.0.1:1344/clamav - -/127.0.0.1 -
1515056869.132      0 10.215.246.218 ICAP_ECHO/204 105 RESPMOD icap://127.0.0.1:1344/clamav - -/127.0.0.1 -
1515056869.149      0 10.215.246.143 ICAP_ECHO/204 130 RESPMOD icap://127.0.0.1:1344/clamav - -/127.0.0.1 -
1515056869.316      6 10.215.246.143 ICAP_ECHO/204 130 RESPMOD icap://127.0.0.1:1344/clamav - -/127.0.0.1 -
1515056869.322      3 10.215.246.143 ICAP_ECHO/204 130 RESPMOD icap://127.0.0.1:1344/clamav - -/127.0.0.1 -
1515056869.329      1 10.215.246.143 ICAP_ECHO/204 105 RESPMOD icap://127.0.0.1:1344/clamav - -/127.0.0.1 -
1515056869.338      1 10.215.246.143 ICAP_ECHO/204 105 RESPMOD icap://127.0.0.1:1344/clamav - -/127.0.0.1 -
1515056869.346      1 10.215.246.143 ICAP_ECHO/204 105 RESPMOD icap://127.0.0.1:1344/clamav - -/127.0.0.1 -
1515056869.354      1 10.215.246.143 ICAP_ECHO/204 105 RESPMOD icap://127.0.0.1:1344/clamav - -/127.0.0.1 -
1515056869.408      1 10.215.246.143 ICAP_ECHO/204 105 RESPMOD icap://127.0.0.1:1344/clamav - -/127.0.0.1 -
1515056869.421      2 10.215.246.143 ICAP_ECHO/204 130 RESPMOD icap://127.0.0.1:1344/clamav - -/127.0.0.1 -
1515056869.431      1 10.215.246.143 ICAP_ECHO/204 105 RESPMOD icap://127.0.0.1:1344/clamav - -/127.0.0.1 -
1515056869.444      4 10.215.246.143 ICAP_ECHO/204 130 RESPMOD icap://127.0.0.1:1344/clamav - -/127.0.0.1 -
1515056869.452      3 10.215.246.143 ICAP_ECHO/204 130 RESPMOD icap://127.0.0.1:1344/clamav - -/127.0.0.1 -
1515056869.487      0 10.215.145.222 ICAP_ECHO/204 105 RESPMOD icap://127.0.0.1:1344/clamav - -/127.0.0.1 -
1515056869.489      0 10.215.247.182 ICAP_ECHO/204 105 RESPMOD icap://127.0.0.1:1344/clamav - -/127.0.0.1 -
1515056869.737      0 10.215.248.31 ICAP_ECHO/204 105 RESPMOD icap://127.0.0.1:1344/clamav - -/127.0.0.1 -
1515056869.966    108 10.215.248.31 ICAP_MOD/200 65761 RESPMOD icap://127.0.0.1:1344/clamav - -/127.0.0.1 -
1515056870.419      3 10.215.248.31 ICAP_ECHO/204 130 RESPMOD icap://127.0.0.1:1344/clamav - -/127.0.0.1 -
1515056870.452      3 10.215.248.31 ICAP_MOD/200 23154 RESPMOD icap://127.0.0.1:1344/clamav - -/127.0.0.1 -
1515056870.831      2 10.215.248.31 ICAP_ECHO/204 105 RESPMOD icap://127.0.0.1:1344/clamav - -/127.0.0.1 -
1515056871.197      3 10.215.248.31 ICAP_ECHO/204 130 RESPMOD icap://127.0.0.1:1344/clamav - -/127.0.0.1 -
1515056871.317      0 10.215.248.31 ICAP_ECHO/204 105 RESPMOD icap://127.0.0.1:1344/clamav - -/127.0.0.1 -
1515056871.371  26103 10.215.247.234 ICAP_MOD/200 914 RESPMOD icap://127.0.0.1:1344/clamav - -/127.0.0.1 -
1515056871.910      0 10.215.247.182 ICAP_ECHO/204 105 RESPMOD icap://127.0.0.1:1344/clamav - -/127.0.0.1 -
1515056871.952  27493 10.215.246.245 ICAP_MOD/200 905 RESPMOD icap://127.0.0.1:1344/clamav - -/127.0.0.1 -
1515056872.020      1 10.215.144.48 ICAP_MOD/200 10205 RESPMOD icap://127.0.0.1:1344/clamav - -/127.0.0.1 -
1515056872.048      3 10.215.248.31 ICAP_ECHO/204 105 RESPMOD icap://127.0.0.1:1344/clamav - -/127.0.0.1 -
1515056872.112      1 10.215.144.171 ICAP_ECHO/204 105 RESPMOD icap://127.0.0.1:1344/clamav - -/127.0.0.1 -
1515056872.172    111 10.215.248.31 ICAP_MOD/200 46966 RESPMOD icap://127.0.0.1:1344/clamav - -/127.0.0.1 -
1515056872.257  60004 10.215.246.224 ICAP_MOD/200 6588 RESPMOD icap://127.0.0.1:1344/clamav - -/127.0.0.1 -
1515056872.260      0 10.215.246.224 ICAP_ECHO/204 105 RESPMOD icap://127.0.0.1:1344/clamav - -/127.0.0.1 -
1515056872.394      1 10.215.246.224 ICAP_ECHO/204 130 RESPMOD icap://127.0.0.1:1344/clamav - -/127.0.0.1 -
1515056872.473      0 10.215.247.182 ICAP_ECHO/204 105 RESPMOD icap://127.0.0.1:1344/clamav - -/127.0.0.1 -
1515056872.546      1 10.215.246.224 ICAP_ECHO/204 130 RESPMOD icap://127.0.0.1:1344/clamav - -/127.0.0.1 -
1515056872.672     54 10.215.248.31 ICAP_MOD/200 23153 RESPMOD icap://127.0.0.1:1344/clamav - -/127.0.0.1 -
1515056872.788      2 10.215.248.31 ICAP_ECHO/204 105 RESPMOD icap://127.0.0.1:1344/clamav - -/127.0.0.1 -
1515056872.856      3 10.215.248.31 ICAP_ECHO/204 105 RESPMOD icap://127.0.0.1:1344/clamav - -/127.0.0.1 -
1515056872.914      3 10.215.248.31 ICAP_ECHO/204 130 RESPMOD icap://127.0.0.1:1344/clamav - -/127.0.0.1 -
1515056872.989      1 10.215.248.99 ICAP_ECHO/204 105 RESPMOD icap://127.0.0.1:1344/clamav - -/127.0.0.1 -
1515056873.155      0 10.215.247.182 ICAP_MOD/200 993 RESPMOD icap://127.0.0.1:1344/clamav - -/127.0.0.1 -
1515056873.239      0 10.215.247.182 ICAP_MOD/200 993 RESPMOD icap://127.0.0.1:1344/clamav - -/127.0.0.1 -
1515056873.272      0 10.215.248.31 ICAP_ECHO/204 105 RESPMOD icap://127.0.0.1:1344/clamav - -/127.0.0.1 -
1515056873.970      0 10.215.248.31 ICAP_MOD/200 2764 RESPMOD icap://127.0.0.1:1344/clamav - -/127.0.0.1 -
1515056874.021      0 10.215.248.31 ICAP_ECHO/204 105 RESPMOD icap://127.0.0.1:1344/clamav - -/127.0.0.1 -
1515056874.192      0 10.215.248.31 ICAP_ECHO/204 130 RESPMOD icap://127.0.0.1:1344/clamav - -/127.0.0.1 -
1515056874.206      0 10.215.248.99 ICAP_ECHO/204 105 RESPMOD icap://127.0.0.1:1344/clamav - -/127.0.0.1 -
1515056874.226  59983 10.215.144.222 ICAP_MOD/200 6550 RESPMOD icap://127.0.0.1:1344/clamav - -/127.0.0.1 -
1515056874.228      0 10.215.144.222 ICAP_ECHO/204 105 RESPMOD icap://127.0.0.1:1344/clamav - -/127.0.0.1 -
1515056874.328      0 10.215.248.99 ICAP_ECHO/204 105 RESPMOD icap://127.0.0.1:1344/clamav - -/127.0.0.1 -
1515056874.351      1 10.215.144.222 ICAP_ECHO/204 130 RESPMOD icap://127.0.0.1:1344/clamav - -/127.0.0.1 -
1515056874.363      1 10.215.247.132 ICAP_ECHO/204 105 RESPMOD icap://127.0.0.1:1344/clamav - -/127.0.0.1 -
1515056874.502      2 10.215.248.31 ICAP_ECHO/204 105 RESPMOD icap://127.0.0.1:1344/clamav - -/127.0.0.1 -
1515056874.513      0 10.215.144.222 ICAP_ECHO/204 130 RESPMOD icap://127.0.0.1:1344/clamav - -/127.0.0.1 -
1515056874.617      1 10.215.145.40 ICAP_ECHO/204 105 RESPMOD icap://127.0.0.1:1344/clamav - -/127.0.0.1 -
1515056874.789      0 10.215.247.132 ICAP_ECHO/204 105 RESPMOD icap://127.0.0.1:1344/clamav - -/127.0.0.1 -
1515056874.995      0 10.215.145.40 ICAP_ECHO/204 105 RESPMOD icap://127.0.0.1:1344/clamav - -/127.0.0.1 -
1515056875.024      1 10.215.248.31 ICAP_ECHO/204 105 RESPMOD icap://127.0.0.1:1344/clamav - -/127.0.0.1 -
1515056875.039      0 10.215.248.99 ICAP_ECHO/204 105 RESPMOD icap://127.0.0.1:1344/clamav - -/127.0.0.1 -
1515056875.412      0 10.215.248.31 ICAP_ECHO/204 105 RESPMOD icap://127.0.0.1:1344/clamav - -/127.0.0.1 -
1515056875.582      0 10.215.248.152 ICAP_ECHO/204 105 RESPMOD icap://127.0.0.1:1344/clamav - -/127.0.0.1 -
1515056875.700      2 10.215.145.136 ICAP_ECHO/204 105 RESPMOD icap://127.0.0.1:1344/clamav - -/127.0.0.1 -
1515056876.291      2 10.215.248.31 ICAP_ECHO/204 105 RESPMOD icap://127.0.0.1:1344/clamav - -/127.0.0.1 -
1515056877.020     24 10.215.247.182 ICAP_MOD/200 65493 RESPMOD icap://127.0.0.1:1344/clamav - -/127.0.0.1 -
1515056877.134      0 10.215.247.182 ICAP_ECHO/204 105 RESPMOD icap://127.0.0.1:1344/clamav - -/127.0.0.1 -
1515056877.187      0 10.215.247.182 ICAP_ECHO/204 105 RESPMOD icap://127.0.0.1:1344/clamav - -/127.0.0.1 -
1515056877.358      0 10.215.247.182 ICAP_ECHO/204 105 RESPMOD icap://127.0.0.1:1344/clamav - -/127.0.0.1 -
1515056877.452      0 10.215.247.182 ICAP_ECHO/204 105 RESPMOD icap://127.0.0.1:1344/clamav - -/127.0.0.1 -
1515056877.502      0 10.215.247.182 ICAP_ECHO/204 105 RESPMOD icap://127.0.0.1:1344/clamav - -/127.0.0.1 -
1515056877.536  26334 10.215.246.136 ICAP_MOD/200 914 RESPMOD icap://127.0.0.1:1344/clamav - -/127.0.0.1 -
1515056877.809  25511 10.215.247.120 ICAP_MOD/200 916 RESPMOD icap://127.0.0.1:1344/clamav - -/127.0.0.1 -

Yes, the title refers to:
kernel: TCP: out of memory -- consider tuning tcp_mem

I'm trying to find out what's wrong on this system even though restarting Squid twice a week at night isn't too bad in my case.

Thanks,

Vieri


From squid3 at treenet.co.nz  Fri Jan  5 09:28:20 2018
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Fri, 5 Jan 2018 22:28:20 +1300
Subject: [squid-users] TCP out of memory
In-Reply-To: <1464426577.854952.1515142755546@mail.yahoo.com>
References: <601581447.203369.1515055873511.ref@mail.yahoo.com>
 <601581447.203369.1515055873511@mail.yahoo.com>
 <1042f27d-78e2-aa59-3fa7-804ded30ea13@treenet.co.nz>
 <1464426577.854952.1515142755546@mail.yahoo.com>
Message-ID: <33a4d101-51c5-aac9-bacf-6c95de183bc1@treenet.co.nz>

On 05/01/18 21:59, Vieri wrote:
> The open sockets to 127.0.0.1:1344 keep increasing steadily even on high network usage, but they do not decrease when there's little or no traffic.
> So, day after day the overall number keeps growing until I have to restart squid once or twice a week.
> 
> In other words, this value keeps growing:
> Largest file desc currently in use:   xxxx
> This other value can decrease at times, but in the long run it keeps growing too:
> Number of file desc currently in use: xxxx
> 

Ah. What does the cachemgr "filedescriptors" report show when there are 
a lot starting to accumulate?

And, are you able to get a cache.log trace with "debug_options 93,6" ?

Amos


From rentorbuy at yahoo.com  Sun Jan  7 22:13:33 2018
From: rentorbuy at yahoo.com (Vieri)
Date: Sun, 7 Jan 2018 22:13:33 +0000 (UTC)
Subject: [squid-users] TCP out of memory
In-Reply-To: <33a4d101-51c5-aac9-bacf-6c95de183bc1@treenet.co.nz>
References: <601581447.203369.1515055873511.ref@mail.yahoo.com>
 <601581447.203369.1515055873511@mail.yahoo.com>
 <1042f27d-78e2-aa59-3fa7-804ded30ea13@treenet.co.nz>
 <1464426577.854952.1515142755546@mail.yahoo.com>
 <33a4d101-51c5-aac9-bacf-6c95de183bc1@treenet.co.nz>
Message-ID: <775733626.1802411.1515363213807@mail.yahoo.com>


________________________________
From: Amos Jeffries <squid3 at treenet.co.nz>

>> The open sockets to 127.0.0.1:1344 keep increasing steadily even on high network usage, but they do not decrease when there's
>> little or no traffic.>> So, day after day the overall number keeps growing until I have to restart squid once or twice a week.
>> 
>> In other words, this value keeps growing:
>> Largest file desc currently in use:   xxxx
>> This other value can decrease at times, but in the long run it keeps growing too:
>> Number of file desc currently in use: xxxx
>> 
> Ah. What does the cachemgr "filedescriptors" report show when there are 
> a lot starting to accumulate?
>
> And, are you able to get a cache.log trace with "debug_options 93,6" ?


Here's my cache.log:

https://drive.google.com/file/d/1I8R5sCsIGhYa69QmGrOoHVITuom4uW0k/view?usp=sharing

squidclient's filedescriptors:

https://drive.google.com/file/d/1o6zn-o0atqeqFGSMRhPA9r1AAFJpnpBZ/view?usp=sharing

The info page:

https://drive.google.com/file/d/11iWqjgdt2KK1yWPMsr5o-IyWGyKS7joc/view?usp=sharing

The open fds are at around 7k, but they can easily reach 12k or 13k. That's when I start running into trouble.

Vieri


From squid3 at treenet.co.nz  Mon Jan  8 12:50:23 2018
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Tue, 9 Jan 2018 01:50:23 +1300
Subject: [squid-users] TCP out of memory
In-Reply-To: <775733626.1802411.1515363213807@mail.yahoo.com>
References: <601581447.203369.1515055873511.ref@mail.yahoo.com>
 <601581447.203369.1515055873511@mail.yahoo.com>
 <1042f27d-78e2-aa59-3fa7-804ded30ea13@treenet.co.nz>
 <1464426577.854952.1515142755546@mail.yahoo.com>
 <33a4d101-51c5-aac9-bacf-6c95de183bc1@treenet.co.nz>
 <775733626.1802411.1515363213807@mail.yahoo.com>
Message-ID: <db3dd9ad-c49d-f649-99c8-f199b7844054@treenet.co.nz>

On 08/01/18 11:13, Vieri wrote:
> 
> ________________________________
> From: Amos Jeffries <squid3 at treenet.co.nz>
> 
>>> The open sockets to 127.0.0.1:1344 keep increasing steadily even on high network usage, but they do not decrease when there's
>>> little or no traffic.>> So, day after day the overall number keeps growing until I have to restart squid once or twice a week.
>>>
>>> In other words, this value keeps growing:
>>> Largest file desc currently in use:   xxxx
>>> This other value can decrease at times, but in the long run it keeps growing too:
>>> Number of file desc currently in use: xxxx
>>>
>> Ah. What does the cachemgr "filedescriptors" report show when there are
>> a lot starting to accumulate?
>>
>> And, are you able to get a cache.log trace with "debug_options 93,6" ?
> 
> 
> Here's my cache.log:
> 
> https://drive.google.com/file/d/1I8R5sCsIGhYa69QmGrOoHVITuom4uW0k/view?usp=sharing
> 
> squidclient's filedescriptors:
> 
> https://drive.google.com/file/d/1o6zn-o0atqeqFGSMRhPA9r1AAFJpnpBZ/view?usp=sharing
> 
> The info page:
> 
> https://drive.google.com/file/d/11iWqjgdt2KK1yWPMsr5o-IyWGyKS7joc/view?usp=sharing
> 
> The open fds are at around 7k, but they can easily reach 12k or 13k. That's when I start running into trouble.
> 

Thank you.

I have only taken a brief look, but so far it looks like the problematic 
sockets are not participating in any ICAP activity. That implies they 
are possibly TCP connections which never complete their opening 
sequence, or at least the result of connection attempts does not make it 
back to the ICAP code somehow.

Amos


From squid3 at treenet.co.nz  Tue Jan  9 08:09:55 2018
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Tue, 9 Jan 2018 21:09:55 +1300
Subject: [squid-users] How to tell HTTPS traffic is using cache from
 access.log in 3.5.x when using ssl_bump
In-Reply-To: <CAPu9cN60whmvwGOe53hqF8DHs4pjjepvwtEm+ErzdZ0d=+NzbA@mail.gmail.com>
References: <CAPu9cN6omb+m8JMsNmJtSRD8z-oPRSRm1cSWo5YHNG0LbU4J4A@mail.gmail.com>
 <688b3f4f-c479-9750-56f5-5e6072c4359b@treenet.co.nz>
 <CAPu9cN7PWEzC436UPK=NiYK0eLX=6Bce8q7YYTy8_J8XJ-_4uw@mail.gmail.com>
 <6d753d7a-a9c6-3ca2-5fef-8d606418eb47@treenet.co.nz>
 <CAPu9cN7f478uYBs+dUTW+6v1eW_eSTM_2XftGLF_Tg9a9tLEEw@mail.gmail.com>
 <8b776e55-b5a1-dcbb-e859-a69250069d7d@treenet.co.nz>
 <CAPu9cN60whmvwGOe53hqF8DHs4pjjepvwtEm+ErzdZ0d=+NzbA@mail.gmail.com>
Message-ID: <92d88a4e-ac3c-77a9-c439-ae243daa0769@treenet.co.nz>

On 01/08/17 11:32, Lei Wen wrote:
> Hi Amos,
> 
> I tried your suggestion tried to tuned with some other options, no 
> matter what I've done, seems HTTPS traffic will not look at sibling 
> cache? it only look into it's own cache if there are only siblings in 
> the group?
>

For http:// URLs the peer security does not matter, for https:// the 
peer requires TLS connections to be used. Other than that limit they 
should behave the same.

That said there are some unresolved things related to 
<https://bugs.squid-cache.org/show_bug.cgi?id=4648> which can prevent 
HTTPS doing some things one would expect to be possible.


Amos


From rentorbuy at yahoo.com  Tue Jan  9 09:40:19 2018
From: rentorbuy at yahoo.com (Vieri)
Date: Tue, 9 Jan 2018 09:40:19 +0000 (UTC)
Subject: [squid-users] TCP out of memory
In-Reply-To: <db3dd9ad-c49d-f649-99c8-f199b7844054@treenet.co.nz>
References: <601581447.203369.1515055873511.ref@mail.yahoo.com>
 <601581447.203369.1515055873511@mail.yahoo.com>
 <1042f27d-78e2-aa59-3fa7-804ded30ea13@treenet.co.nz>
 <1464426577.854952.1515142755546@mail.yahoo.com>
 <33a4d101-51c5-aac9-bacf-6c95de183bc1@treenet.co.nz>
 <775733626.1802411.1515363213807@mail.yahoo.com>
 <db3dd9ad-c49d-f649-99c8-f199b7844054@treenet.co.nz>
Message-ID: <517226574.2508171.1515490819697@mail.yahoo.com>


________________________________
From: Amos Jeffries <squid3 at treenet.co.nz>
>
> I have only taken a brief look, but so far it looks like the problematic 

> sockets are not participating in any ICAP activity.

Do you see that from the cache.log, or from ":filedescriptors"?
If I list my current filedescriptors right now, I get this:

# squidclient mgr:filedescriptors | grep "127.0.0.1:1344"
20 Socket  899      25*   10001  127.0.0.1:1344        127.0.0.1
30 Socket    0   71648    72210  127.0.0.1:1344        127.0.0.1
38 Socket  900       0*    2564  127.0.0.1:1344        127.0.0.1
102 Socket    0   67222    67689  127.0.0.1:1344        127.0.0.1
107 Socket    0  102679   203677  127.0.0.1:1344        127.0.0.1
113 Socket    0   67222    67709  127.0.0.1:1344        127.0.0.1
115 Socket  886       0*    2588  127.0.0.1:1344        127.0.0.1
116 Socket  873      25*   10395  127.0.0.1:1344        127.0.0.1
129 Socket    0  114892   144095  127.0.0.1:1344        127.0.0.1
134 Socket  900      25*    8863  127.0.0.1:1344        127.0.0.1
160 Socket    0   67222    67687  127.0.0.1:1344        127.0.0.1
165 Socket    0   77833    78401  127.0.0.1:1344        127.0.0.1
166 Socket    0   67222    67702  127.0.0.1:1344        127.0.0.1
175 Socket    0   67222    67698  127.0.0.1:1344        127.0.0.1
176 Socket    0   67222    67698  127.0.0.1:1344        127.0.0.1
212 Socket    0   67222    67742  127.0.0.1:1344        127.0.0.1
213 Socket  878       0*    2533  127.0.0.1:1344        127.0.0.1
226 Socket  873       0*    2531  127.0.0.1:1344        127.0.0.1
236 Socket    0   78332   180786  127.0.0.1:1344        127.0.0.1
244 Socket    0   67222    67698  127.0.0.1:1344        127.0.0.1
281 Socket    0   67222    67685  127.0.0.1:1344        127.0.0.1
285 Socket    0   78253   149568  127.0.0.1:1344        127.0.0.1
298 Socket    0   77833    78451  127.0.0.1:1344        127.0.0.1
305 Socket    0   74366   168309  127.0.0.1:1344        127.0.0.1
307 Socket    0  114519   115068  127.0.0.1:1344        127.0.0.1
326 Socket    0   67222    67698  127.0.0.1:1344        127.0.0.1
327 Socket    0   67222    67687  127.0.0.1:1344        127.0.0.1
365 Socket    0   70248   114918  127.0.0.1:1344        127.0.0.1
372 Socket    0   67222    67698  127.0.0.1:1344        127.0.0.1
390 Socket    0   77833    78483  127.0.0.1:1344        127.0.0.1
404 Socket    0   90022    90703  127.0.0.1:1344        127.0.0.1
464 Socket    0   78253   144095  127.0.0.1:1344        127.0.0.1
472 Socket    0   67222    67698  127.0.0.1:1344        127.0.0.1
480 Socket  891       0*    2514  127.0.0.1:1344        127.0.0.1
491 Socket    0   67222    67685  127.0.0.1:1344        127.0.0.1
509 Socket    0   67222    67687  127.0.0.1:1344        127.0.0.1
512 Socket    0   67222    67703  127.0.0.1:1344        127.0.0.1
528 Socket    0  131176   155548  127.0.0.1:1344        127.0.0.1
536 Socket    0   70111   134058  127.0.0.1:1344        127.0.0.1
547 Socket    0   67222    67689  127.0.0.1:1344        127.0.0.1
554 Socket    0  131860   152673  127.0.0.1:1344        127.0.0.1
570 Socket    0   67222    67707  127.0.0.1:1344        127.0.0.1
572 Socket  893       0*    2706  127.0.0.1:1344        127.0.0.1
596 Socket    0   78390   114864  127.0.0.1:1344        127.0.0.1
602 Socket    0   67222    67691  127.0.0.1:1344        127.0.0.1
624 Socket    0   72678    73442  127.0.0.1:1344        127.0.0.1
631 Socket    0   71646    72250  127.0.0.1:1344        127.0.0.1
635 Socket    0  104333   104896  127.0.0.1:1344        127.0.0.1
641 Socket    0   67222    67687  127.0.0.1:1344        127.0.0.1
646 Socket    0   67222    67698  127.0.0.1:1344        127.0.0.1
662 Socket    0   67222    67698  127.0.0.1:1344        127.0.0.1
674 Socket    0   67222    67691  127.0.0.1:1344        127.0.0.1
678 Socket    0   67222    67687  127.0.0.1:1344        127.0.0.1
687 Socket    0   67222    67702  127.0.0.1:1344        127.0.0.1
730 Socket    0   67222    67691  127.0.0.1:1344        127.0.0.1
767 Socket    0   74465   152811  127.0.0.1:1344        127.0.0.1
772 Socket    0   67217    67747  127.0.0.1:1344        127.0.0.1
815 Socket    0   77864    78246  127.0.0.1:1344        127.0.0.1
848 Socket    0   67222    67743  127.0.0.1:1344        127.0.0.1
865 Socket    0   67222    67747  127.0.0.1:1344        127.0.0.1
890 Socket    0   67222    67699  127.0.0.1:1344        127.0.0.1
943 Socket    0   77833    78501  127.0.0.1:1344        127.0.0.1
1008 Socket    0   74212    78383  127.0.0.1:1344        127.0.0.1
1018 Socket    0   74466    90630  127.0.0.1:1344        127.0.0.1
1099 Socket    0   67222    67687  127.0.0.1:1344        127.0.0.1
1124 Socket    0   67222    67683  127.0.0.1:1344        127.0.0.1
1167 Socket    0   67222    67687  127.0.0.1:1344        127.0.0.1
1273 Socket    0   67258    67879  127.0.0.1:1344        127.0.0.1
1337 Socket    0   74243    78265  127.0.0.1:1344        127.0.0.1


Both Nread and Nwrite seem to be well over 0.

> That implies they > are possibly TCP connections which never complete their opening 
> sequence, or at least the result of connection attempts does not make it 
> back to the ICAP code somehow.


ICAP and Squid are both on localhost. I'd like to find out why this is happening.


I believe I already posted a tcpdump trace of the ICAP traffic, but I don't know if you had a chance to take a look at it. I had a quick look, but I'm not familiar with the ICAP protocol. In any case, I probably would see a lot of OPTIONS, REQMOD, RESPMOD methods, but I don't know if I would clearly detect initial TCP issues.


Anyway, here's a dumb question. Can't Squid "tell" when a TCP connection to an ICAP server has never completed correctly after x timeout, and close it down/reset it?
I'm using default values in squid.conf for the following:
connect_timeout
icap_connect_timeout
peer_connect_timeout

The docs say:
#  TAG: icap_connect_timeout
#       This parameter specifies how long to wait for the TCP connect to
#       the requested ICAP server to complete before giving up and either
#       terminating the HTTP transaction or bypassing the failure.


BTW I guess it's just a typo error because instead of an "HTTP transaction" I should read "ICAP transaction", right? 

Anyway, I have "bypass=0" for the ICAP service so I guess it should honor connect_timeout.
The default is connect_timeout 1 minute. 


With a 1-minute timeout it may be hard to see the sockets close when there's plenty of traffic, but I think I should see a substantial drop of open sockets when traffic is low (eg. at night). However, I don't see it.


What could I try?

Thank you very much for your time.

Vieri


From sekarit at gmail.com  Tue Jan  9 11:45:40 2018
From: sekarit at gmail.com (Sekar Duraisamy)
Date: Tue, 9 Jan 2018 17:15:40 +0530
Subject: [squid-users] How to enable caching for https websites on Squid
In-Reply-To: <565b24e8-8448-31fb-bc52-123f0536080f@treenet.co.nz>
References: <CADfQnU1Hx16+2DsZQHQjNk0Xat6fNVVhQuWG=TuSzcE9PZ4Xcw@mail.gmail.com>
 <1b1626a3-e210-fad2-2fa3-b80fdadab9b3@treenet.co.nz>
 <20171220122341.GB17304@fantomas.sk>
 <29d95c57-7c83-8790-9d2f-fe42bddba829@treenet.co.nz>
 <20171220134133.GA23710@fantomas.sk>
 <565b24e8-8448-31fb-bc52-123f0536080f@treenet.co.nz>
Message-ID: <CADfQnU1u9RDNRhTH49JAqvv8fE9RmT4pPD-eRyJcG_94YLAU6Q@mail.gmail.com>

Hi Amos,

Thanks for your information

"To cache encryption protected content you must first remove the
encryption. That destroys the "anonymous" part completely."

Could you please provide little more details about affecting anonymous
service. Do you meant it will affect customers anonymous or from proxy
server?

We used to disable via and forwarded_for header to make squid proxy as
anonymous in HTTP.

When we use certificate in the Proxy server to decrypt the content of
HTTPS, multiple customers will hit to the same HTTPS website in a day
through our proxy, that website always see single certificate even
though multiple customers from multiple IPs. Is there a chance from
website can block
because of they will see more requests from more IP's but single
certificate for the all the requests to the same doamin ?

On Wed, Dec 20, 2017 at 8:42 PM, Amos Jeffries <squid3 at treenet.co.nz> wrote:
> On 21/12/17 02:41, Matus UHLAR - fantomas wrote:
>>>
>>> On 21/12/17 01:23, Matus UHLAR - fantomas wrote:
>>>>
>>>> and I think you should read the last paragraph as:
>>>>
>>>>  "caching often will not happen, since most of web developers don't know
>>>> hot
>>>>   so use and benefit of it thus they try to disable caching globally"
>>
>>
>> On 21.12.17 02:20, Amos Jeffries wrote:
>>>
>>> That is nothing special for HTTPS, it happens worse in regular HTTP.
>>
>>
>> do you want to say that breaking into https can cause http caching more
>> efficient?
>> do you have any evidence of that?
>>
>
> No, I am saying that the problem you pointed at is a _larger_ problem in
> http:// because those dev are having to actively prevent caching. Many are
> also under the false impression that https:// goes end-to-end and caching
> does not happen there other than Browser cache. So those who develop sites
> with HTTPS in mind do not go to quite such extremes to block proxies
> caching.
>
> HTTPS has _other_ problems that impact on caching efficiency.
>
> Amos
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users


From uhlar at fantomas.sk  Tue Jan  9 12:12:47 2018
From: uhlar at fantomas.sk (Matus UHLAR - fantomas)
Date: Tue, 9 Jan 2018 13:12:47 +0100
Subject: [squid-users] How to enable caching for https websites on Squid
In-Reply-To: <CADfQnU1u9RDNRhTH49JAqvv8fE9RmT4pPD-eRyJcG_94YLAU6Q@mail.gmail.com>
References: <CADfQnU1Hx16+2DsZQHQjNk0Xat6fNVVhQuWG=TuSzcE9PZ4Xcw@mail.gmail.com>
 <1b1626a3-e210-fad2-2fa3-b80fdadab9b3@treenet.co.nz>
 <20171220122341.GB17304@fantomas.sk>
 <29d95c57-7c83-8790-9d2f-fe42bddba829@treenet.co.nz>
 <20171220134133.GA23710@fantomas.sk>
 <565b24e8-8448-31fb-bc52-123f0536080f@treenet.co.nz>
 <CADfQnU1u9RDNRhTH49JAqvv8fE9RmT4pPD-eRyJcG_94YLAU6Q@mail.gmail.com>
Message-ID: <20180109121247.GA10629@fantomas.sk>

On 09.01.18 17:15, Sekar Duraisamy wrote:
>"To cache encryption protected content you must first remove the
>encryption. That destroys the "anonymous" part completely."
>
>Could you please provide little more details about affecting anonymous
>service. Do you meant it will affect customers anonymous or from proxy
>server?

I believe you have been answered already multiple times, but once more:

the customer will have no privacy against proxy server - the proxy server
will see everything they access, all the content etc.

This is impossible with SSL - SSL has been created just to provide privacy
to users, so nobody sees the content, only the final server.

With HTTPS decrypting the destination server will only see your proxy
accessing, no IP, browser info (if you decide to hide it) but the proxy will
see everything.  Proxy admins will be able to see their passwords, their
mail, banking account information, etc.
  
If your users are OK with that, fine.  The question is if they really want
this kind of anonymity.

>When we use certificate in the Proxy server to decrypt the content of
>HTTPS, multiple customers will hit to the same HTTPS website in a day
>through our proxy, that website always see single certificate even
>though multiple customers from multiple IPs. Is there a chance from
>website can block
>because of they will see more requests from more IP's but single
>certificate for the all the requests to the same doamin ?

The end servers will not see your proxy certificate. 
The HTTP server certificate is used to authentize server, not the client.

-- 
Matus UHLAR - fantomas, uhlar at fantomas.sk ; http://www.fantomas.sk/
Warning: I wish NOT to receive e-mail advertising to this address.
Varovanie: na tuto adresu chcem NEDOSTAVAT akukolvek reklamnu postu.
The only substitute for good manners is fast reflexes. 


From squid3 at treenet.co.nz  Tue Jan  9 12:40:03 2018
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Wed, 10 Jan 2018 01:40:03 +1300
Subject: [squid-users] TCP out of memory
In-Reply-To: <517226574.2508171.1515490819697@mail.yahoo.com>
References: <601581447.203369.1515055873511.ref@mail.yahoo.com>
 <601581447.203369.1515055873511@mail.yahoo.com>
 <1042f27d-78e2-aa59-3fa7-804ded30ea13@treenet.co.nz>
 <1464426577.854952.1515142755546@mail.yahoo.com>
 <33a4d101-51c5-aac9-bacf-6c95de183bc1@treenet.co.nz>
 <775733626.1802411.1515363213807@mail.yahoo.com>
 <db3dd9ad-c49d-f649-99c8-f199b7844054@treenet.co.nz>
 <517226574.2508171.1515490819697@mail.yahoo.com>
Message-ID: <6eaaa26c-5226-5f5a-02f1-a9f5df690cc9@treenet.co.nz>

On 09/01/18 22:40, Vieri wrote:
> 
> ________________________________
> From: Amos Jeffries <squid3 at treenet.co.nz>
>>
>> I have only taken a brief look, but so far it looks like the problematic
> 
>> sockets are not participating in any ICAP activity.
> 
> Do you see that from the cache.log, or from ":filedescriptors"?

I went through the cache.log looking for the FD numbers mentioned to 
correlate with the filedescriptors report lines. The first dozen I found 
all seemed to be either in some clearly described state (not that 
"127.0.0.1" description).

My deeper look was going to make a script to do the reverse check. For 
each entry in the filedescriptors, seeing if there was any ICAP mentions 
and print where in the cache.log to look.


> 
> 
> Both Nread and Nwrite seem to be well over 0.
> 

That I think is really odd for sockets to port 1344 which are not having 
any ICAP protocol activity happening.


>> That implies they > are possibly TCP connections which never complete their opening
>> sequence, or at least the result of connection attempts does not make it
>> back to the ICAP code somehow.
> 
> 
> ICAP and Squid are both on localhost. I'd like to find out why this is happening.
> 


That is why I specifically asked for 93,* trace. The 93,* traces should 
not contains any of the HTTP traffic to Squid just the ICAP.

If you can get an 11,* trace as well separately to see if any of the 
stuck port 1344 FDs are using HTTP. There should be none. But if there 
are that could well be the problem on those ones.


> 
> I believe I already posted a tcpdump trace of the ICAP traffic, but I don't know if you had a chance to take a look at it. I had a quick look, but I'm not familiar with the ICAP protocol. In any case, I probably would see a lot of OPTIONS, REQMOD, RESPMOD methods, but I don't know if I would clearly detect initial TCP issues.
> 

You mean the trace in your "TCP out of memory" thread last month?
What I'm seeing in there is connections being used for ICAP, then the 
ICAP service requesting keep-alive which Squid honors. Nothing obviously 
broken.


> 
> Anyway, here's a dumb question. Can't Squid "tell" when a TCP connection to an ICAP server has never completed correctly after x timeout, and close it down/reset it?
> I'm using default values in squid.conf for the following:
> connect_timeout
> icap_connect_timeout
> peer_connect_timeout
> 
> The docs say:
> #  TAG: icap_connect_timeout
> #       This parameter specifies how long to wait for the TCP connect to
> #       the requested ICAP server to complete before giving up and either
> #       terminating the HTTP transaction or bypassing the failure.
> 
> 
> BTW I guess it's just a typo error because instead of an "HTTP transaction" I should read "ICAP transaction", right?

Nope. If ICAP fails for any reason and is mandatory (bypass=0) the HTTP 
transaction which is trying to use it gets broken. So the HTTP request 
(transaction) needs to be aborted with an error.
That does not necessarily mean the TCP connection used by the HTTP 
though, just the request/reply transaction.


> 
> Anyway, I have "bypass=0" for the ICAP service so I guess it should honor connect_timeout.
> The default is connect_timeout 1 minute.
> 

No, icap_connect_timeout is the only one that should be relevant on 
connections to ICAP services.

connect_timeout is for HTP origin servers, and peer_connect_timeout is 
for cache_peers.


> 
> With a 1-minute timeout it may be hard to see the sockets close when there's plenty of traffic, but I think I should see a substantial drop of open sockets when traffic is low (eg. at night). However, I don't see it.
> 

If you mean the *connect_timeout, that only affects the time Squid waits 
for the outgoing TCP SYN/SYN+ACK process to complete. Persistent and 
other connections already opened have other timeouts applied depending 
on what they are being used for.

The *idle_pconn_timeout directives are more likely to result in closed 
connections for overnight etc. cases if clients do not actively end them 
anyway.


Amos


From brian.bergstrom at sportsengine.com  Tue Jan  9 14:58:03 2018
From: brian.bergstrom at sportsengine.com (Brian Bergstrom)
Date: Tue, 9 Jan 2018 08:58:03 -0600
Subject: [squid-users] ALPN, HTTP/2 and sslbump
In-Reply-To: <c13c5797-6ff0-36b5-403d-f8ea6618ec3b@treenet.co.nz>
References: <BY2PR17MB018222156B827DDF84E7C6FCF7560@BY2PR17MB0182.namprd17.prod.outlook.com>
 <177b9497-8b06-2049-c74b-02b77d151fe5@treenet.co.nz>
 <1515018629106-0.post@n4.nabble.com>
 <cf99de5a-85e9-0695-73ff-db95c5236c42@measurement-factory.com>
 <c13c5797-6ff0-36b5-403d-f8ea6618ec3b@treenet.co.nz>
Message-ID: <CAH9f=keTW61j26jbW8Z8+onk0KnWFM5sL9u2mY=4RMuayzb4Bw@mail.gmail.com>

Thanks for the input.  Peeking less and splicing sooner appears to resolve
the issue I was having.  Since SNI is available at step 2 after peeking at
step 1, I there was no lose in functionality.  So my ssl_bump config ends
up looking like below:

ssl_bump peek step1
ssl_bump splice step2 allowed_https_sites
ssl_bump splice step2 allowed_https_ips
ssl_bump terminate step2 all


Thanks again!

On Wed, Jan 3, 2018 at 5:47 PM, Amos Jeffries <squid3 at treenet.co.nz> wrote:

> On 04/01/18 12:37, Alex Rousskov wrote:
>
>> On 01/03/2018 03:30 PM, brianbergstrom wrote:
>>
>> If I understand the docs and this thread correctly, Squid should be
>>> removing
>>> h2 from the ALPN in the Client Hello since Squid does not support it.
>>>
>>
>> Please note that Squid cannot remove something when using "peek" and
>> "splice" actions.
>>
>> I do not know whether Squid removes unsupported ALPN values when using
>> "stare" and "bump" actions, and I would not be surprised to learn that
>> Squid does not police those values at all (yet),
>>
>
> It does *unless* peeking at the server handshake: <
> https://github.com/squid-cache/squid/blob/v3.5/src/ssl/bio.cc#L1261>.
>
> Amos
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>



-- 
*Brian Bergstrom*
SOFTWARE ENGINEER

SportsEngine | 807 Broadway St NE | Suite 300 | Minneapolis, MN 55413
SportsEngine.com <http://sportsengine.com> | twitter.com/NBCSportsEngine |
facebook.com/NBCSportsEngine
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20180109/f6d4e288/attachment.htm>

From yoinier.hn at gmail.com  Tue Jan  9 20:28:37 2018
From: yoinier.hn at gmail.com (Yoinier Hernandez Nieves)
Date: Tue, 9 Jan 2018 15:28:37 -0500
Subject: [squid-users] Squid and SSL Bumb
Message-ID: <C735C3D2-8F2C-4967-8C96-56F9CBEFBC6B@gmail.com>

I try configure squid 3.5 on CentOS 7 with sslBump.

But I have some problems, the first:

Some HTTPs sites can access, because squid say what I am are not authenticated. And other sites, yes I can access.

I am authenticated.

Thanks.

Yoinier.

Fragment of my squid.conf.

http_port 3128 ssl-bump cert=/etc/squid/ssl_cert/ConAlza.pem generate-host-certificates=on dynamic_cert_mem_cache_size=4MB# options=NO_SSLv3 dhparams=/etc/squid/ssl_cert/dhparam.pem
sslcrtd_program /usr/lib64/squid/ssl_crtd -s /var/lib/ssl_db -M 4MB
sslproxy_options NO_SSLv2,NO_SSLv3,SINGLE_DH_USE
acl step1 at_step SslBump1
acl step2 at_step SslBump2
acl step3 at_step SslBump3
ssl_bump peek step1
ssl_bump bump all
authenticate_ip_ttl 60 seconds


-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20180109/8225fd5e/attachment.htm>

From Antony.Stone at squid.open.source.it  Tue Jan  9 21:27:57 2018
From: Antony.Stone at squid.open.source.it (Antony Stone)
Date: Tue, 9 Jan 2018 22:27:57 +0100
Subject: [squid-users] Squid and SSL Bump
In-Reply-To: <C735C3D2-8F2C-4967-8C96-56F9CBEFBC6B@gmail.com>
References: <C735C3D2-8F2C-4967-8C96-56F9CBEFBC6B@gmail.com>
Message-ID: <201801092227.57945.Antony.Stone@squid.open.source.it>

On Tuesday 09 January 2018 at 21:28:37, Yoinier Hernandez Nieves wrote:

> I try configure squid 3.5 on CentOS 7 with sslBump.
> 
> But I have some problems, the first:
> 
> Some HTTPs sites can access, because squid say what I am are not
> authenticated. And other sites, yes I can access.

Please give us information:

1. An example of sites can you access.

2. An example of sites can you not access.

3. For problems, show us error messages - quote us what the remote sites tell 
you.

4. Please rephrase "squid say what I am are not authenticated" - this is not 
clear - what do you mean?

> I am authenticated.

To what?  Squid, or the remote site?

How do you know you are authenticated - what confirmation do you have?

> Fragment of my squid.conf.
> 
> http_port 3128 ssl-bump cert=/etc/squid/ssl_cert/ConAlza.pem
> generate-host-certificates=on dynamic_cert_mem_cache_size=4MB#
> options=NO_SSLv3 dhparams=/etc/squid/ssl_cert/dhparam.pem sslcrtd_program
> /usr/lib64/squid/ssl_crtd -s /var/lib/ssl_db -M 4MB sslproxy_options
> NO_SSLv2,NO_SSLv3,SINGLE_DH_USE
> acl step1 at_step SslBump1
> acl step2 at_step SslBump2
> acl step3 at_step SslBump3
> ssl_bump peek step1
> ssl_bump bump all
> authenticate_ip_ttl 60 seconds

That looks a bit strange (and a bit incomplete) to me, but since I'm no expert 
on SSL interception, I'll let someone else step in here.

If you can provide more information in the meantime (eg: enough to help 
someone else replicate your problem) that would be good.


Antony.

-- 
Wanted: telepath.   You know where to apply.

                                                   Please reply to the list;
                                                         please *don't* CC me.


From yoinier.hn at gmail.com  Tue Jan  9 21:56:28 2018
From: yoinier.hn at gmail.com (Yoinier Hernandez Nieves)
Date: Tue, 9 Jan 2018 16:56:28 -0500
Subject: [squid-users] Squid and SSL Bump
In-Reply-To: <201801092227.57945.Antony.Stone@squid.open.source.it>
References: <C735C3D2-8F2C-4967-8C96-56F9CBEFBC6B@gmail.com>
 <201801092227.57945.Antony.Stone@squid.open.source.it>
Message-ID: <E6B47F15-D6FA-4655-9C46-09AC96521122@gmail.com>

I answer interline.

> El 9/01/2018, a las 4:27 p.m., Antony Stone <Antony.Stone at squid.open.source.it> escribi?:
> 
> On Tuesday 09 January 2018 at 21:28:37, Yoinier Hernandez Nieves wrote:
> 
>> I try configure squid 3.5 on CentOS 7 with sslBump.
>> 
>> But I have some problems, the first:
>> 
>> Some HTTPs sites can access, because squid say what I am are not
>> authenticated. And other sites, yes I can access.
> 
> Please give us information:
> 
> 1. An example of sites can you access.
not https

> 2. An example of sites can you not access.
https://www.ssllabs.com/ssltest/viewMyClient.html <https://www.ssllabs.com/ssltest/viewMyClient.html>
https://outlook.co.il/ <https://outlook.co.il/>
https://www.facebook.com <https://www.facebook.com/>

> 3. For problems, show us error messages - quote us what the remote sites tell 
> you.
Se encontr? el siguiente error al intentar recuperar la direcci?n URL: https://outlook.co.il/ <https://outlook.co.il/>
Acceso Denegado a la Cach?

Lo lamento, tu no est?s autorizado a solicitar https://outlook.co.il/ de este cach? hasta que te hayas autenticado.

Please contact the cache administrator <mailto:root?subject=CacheErrorInfo%20-%20ERR_CACHE_ACCESS_DENIED&body=CacheHost%3A%20artemisa.conalza.co.cu%0D%0AErrPage%3A%20ERR_CACHE_ACCESS_DENIED%0D%0AErr%3A%20%5Bnone%5D%0D%0ATimeStamp%3A%20Tue,%2009%20Jan%202018%2019%3A12%3A22%20GMT%0D%0A%0D%0AClientIP%3A%20172.25.100.4%0D%0A%0D%0AHTTP%20Request%3A%0D%0AGET%20%2F%20HTTP%2F1.1%0AUser-Agent%3A%20Mozilla%2F5.0%20(Macintosh%3B%20Intel%20Mac%20OS%20X%2010.12%3B%20rv%3A57.0)%20Gecko%2F20100101%20Firefox%2F57.0%0D%0AAccept%3A%20text%2Fhtml,application%2Fxhtml+xml,application%2Fxml%3Bq%3D0.9,*%2F*%3Bq%3D0.8%0D%0AAccept-Language%3A%20es-ES,es%3Bq%3D0.8,en-US%3Bq%3D0.5,en%3Bq%3D0.3%0D%0AAccept-Encoding%3A%20gzip,%20deflate,%20br%0D%0AConnection%3A%20keep-alive%0D%0AUpgrade-Insecure-Requests%3A%201%0D%0AHost%3A%20outlook.co.il%0D%0A%0D%0A%0D%0A> if you have difficulties authenticating yourself.

> 
> 4. Please rephrase "squid say what I am are not authenticated" - this is not 
> clear - what do you mean?
> 
>> I am authenticated.
> 
> To what?  Squid, or the remote site?
Squid, see message in Spanish for point 3.

Other error is that
https://www.kiosco.bandec.cu/kiosco <https://www.kiosco.bandec.cu/kiosco>
Error negotiating SSL on FD 16: error:14090086:SSL routines:ssl3_get_server_certificate:certificate verify failed (1/-1/0)
The following error was encountered while trying to retrieve the URL: https://www.kiosco.bandec.cu/* <https://www.kiosco.bandec.cu/*>
Failed to establish a secure connection to 190.6.64.132

The system returned:

(71) Protocol error (TLS code: X509_V_ERR_UNABLE_TO_GET_ISSUER_CERT_LOCALLY)
SSL Certficate error: certificate issuer (CA) not known: /CN=CX6.bandec.cu

This proxy and the remote host failed to negotiate a mutually acceptable security settings for handling your request. It is possible that the remote host does not support secure connections, or the proxy is not satisfied with the host security credentials.

> 
> How do you know you are authenticated - what confirmation do you have?
> 
>> Fragment of my squid.conf.
>> 
>> http_port 3128 ssl-bump cert=/etc/squid/ssl_cert/ConAlza.pem
>> generate-host-certificates=on dynamic_cert_mem_cache_size=4MB#
>> options=NO_SSLv3 dhparams=/etc/squid/ssl_cert/dhparam.pem sslcrtd_program
>> /usr/lib64/squid/ssl_crtd -s /var/lib/ssl_db -M 4MB sslproxy_options
>> NO_SSLv2,NO_SSLv3,SINGLE_DH_USE
>> acl step1 at_step SslBump1
>> acl step2 at_step SslBump2
>> acl step3 at_step SslBump3
>> ssl_bump peek step1
>> ssl_bump bump all
>> authenticate_ip_ttl 60 seconds
> 
> That looks a bit strange (and a bit incomplete) to me, but since I'm no expert 
> on SSL interception, I'll let someone else step in here.
> 
> If you can provide more information in the meantime (eg: enough to help 
> someone else replicate your problem) that would be good.
> 
I use too dansguardians before the squid proxy.

See the logs for one petition

1515534858.355   3720 aaa.aaa.aaa.aaa TAG_NONE/200 0 CONNECT www.ssllabs.com:443 ynieves HIER_DIRECT/64.41.200.100 -
1515534858.375      0 bbb.bbb.bbb.bbb TCP_DENIED/403 4457 GET https://www.ssllabs.com/ssltest/viewMyClient.html ynieves HIER_NONE/- text/html
1515534858.407      0 bbb.bbb.bbb.bbb TAG_NONE/503 4952 GET http://artemisa.conalza.co.cu:3128/squid-internal-static/icons/SN.png ynieves HIER_DIRECT/64.41.200.100 text/html

aaa.aaa.aaa.aaa is my pc.
bbb.bbb.bbb.bbb is the dansguardians

> 
> Antony.
> 
> -- 
> Wanted: telepath.   You know where to apply.
> 
>                                                   Please reply to the list;
>                                                         please *don't* CC me.
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20180109/13c254e1/attachment.htm>

From sekarit at gmail.com  Wed Jan 10 11:27:45 2018
From: sekarit at gmail.com (Sekar Duraisamy)
Date: Wed, 10 Jan 2018 16:57:45 +0530
Subject: [squid-users] How to enable caching for https websites on Squid
In-Reply-To: <20180109121247.GA10629@fantomas.sk>
References: <CADfQnU1Hx16+2DsZQHQjNk0Xat6fNVVhQuWG=TuSzcE9PZ4Xcw@mail.gmail.com>
 <1b1626a3-e210-fad2-2fa3-b80fdadab9b3@treenet.co.nz>
 <20171220122341.GB17304@fantomas.sk>
 <29d95c57-7c83-8790-9d2f-fe42bddba829@treenet.co.nz>
 <20171220134133.GA23710@fantomas.sk>
 <565b24e8-8448-31fb-bc52-123f0536080f@treenet.co.nz>
 <CADfQnU1u9RDNRhTH49JAqvv8fE9RmT4pPD-eRyJcG_94YLAU6Q@mail.gmail.com>
 <20180109121247.GA10629@fantomas.sk>
Message-ID: <CADfQnU3RP3+VQj5DWmN_sJSjuM0c62=HfHddmVJhXc1uimJqmA@mail.gmail.com>

Thanks for your reply. Yes. I agree about the customers privacy and we
should not decrypt. My users are not even using this proxy for their
personal purpose and not passing any personal information and simple
browse the pages to explore the wiki pages, technical information or
any images like that.

Just thought of enabling cache for https for few websites to just save
more internet bandwidth utilization and cost saving of internet usage
as most of the websites now moved from http to https.

On Tue, Jan 9, 2018 at 5:42 PM, Matus UHLAR - fantomas
<uhlar at fantomas.sk> wrote:
> On 09.01.18 17:15, Sekar Duraisamy wrote:
>>
>> "To cache encryption protected content you must first remove the
>> encryption. That destroys the "anonymous" part completely."
>>
>> Could you please provide little more details about affecting anonymous
>> service. Do you meant it will affect customers anonymous or from proxy
>> server?
>
>
> I believe you have been answered already multiple times, but once more:
>
> the customer will have no privacy against proxy server - the proxy server
> will see everything they access, all the content etc.
>
> This is impossible with SSL - SSL has been created just to provide privacy
> to users, so nobody sees the content, only the final server.
>
> With HTTPS decrypting the destination server will only see your proxy
> accessing, no IP, browser info (if you decide to hide it) but the proxy will
> see everything.  Proxy admins will be able to see their passwords, their
> mail, banking account information, etc.
>  If your users are OK with that, fine.  The question is if they really want
> this kind of anonymity.
>
>> When we use certificate in the Proxy server to decrypt the content of
>> HTTPS, multiple customers will hit to the same HTTPS website in a day
>> through our proxy, that website always see single certificate even
>> though multiple customers from multiple IPs. Is there a chance from
>> website can block
>> because of they will see more requests from more IP's but single
>> certificate for the all the requests to the same doamin ?
>
>
> The end servers will not see your proxy certificate. The HTTP server
> certificate is used to authentize server, not the client.
>
> --
> Matus UHLAR - fantomas, uhlar at fantomas.sk ; http://www.fantomas.sk/
> Warning: I wish NOT to receive e-mail advertising to this address.
> Varovanie: na tuto adresu chcem NEDOSTAVAT akukolvek reklamnu postu.
> The only substitute for good manners is fast reflexes.
> _______________________________________________
>
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users


From squid3 at treenet.co.nz  Wed Jan 10 13:47:55 2018
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 11 Jan 2018 02:47:55 +1300
Subject: [squid-users] Squid and SSL Bump
In-Reply-To: <E6B47F15-D6FA-4655-9C46-09AC96521122@gmail.com>
References: <C735C3D2-8F2C-4967-8C96-56F9CBEFBC6B@gmail.com>
 <201801092227.57945.Antony.Stone@squid.open.source.it>
 <E6B47F15-D6FA-4655-9C46-09AC96521122@gmail.com>
Message-ID: <96a99b08-b3c8-52e5-17bf-575a75a4c3c7@treenet.co.nz>

On 10/01/18 10:56, Yoinier Hernandez Nieves wrote:
> I answer interline.
> 
>> El 9/01/2018, a las 4:27 p.m., Antony Stone escribi?:
>>
>> On Tuesday 09 January 2018 at 21:28:37, Yoinier Hernandez Nieves wrote:
>>
>>> I try configure squid 3.5 on CentOS 7 with sslBump.
>>>
>>> But I have some problems, the first:
>>>
>>> Some HTTPs sites can access, because squid say what I am are not
>>> authenticated. And other sites, yes I can access.
>>
>> Please give us information:
>>
>> 1. An example of sites can you access.
> not https
> 
>> 2. An example of sites can you not access.
> https://www.ssllabs.com/ssltest/viewMyClient.html
> https://outlook.co.il/
> https://www.facebook.com
> 
>> 3. For problems, show us error messages - quote us what the remote 
>> sites tell
>> you.
> 
> Se encontr? el siguiente error al intentar recuperar la direcci?n URL: 
> https://outlook.co.il/
> 
>     *Acceso Denegado a la Cach?*
> 
> Lo lamento, tu no est?s autorizado a solicitar https://outlook.co.il/ de 
> este cach? hasta que te hayas autenticado.
> 
> Please contact the cache administrator 
> <mailto:root?subject=CacheErrorInfo%20-%20ERR_CACHE_ACCESS_DENIED&body=CacheHost%3A%20artemisa.conalza.co.cu%0D%0AErrPage%3A%20ERR_CACHE_ACCESS_DENIED%0D%0AErr%3A%20%5Bnone%5D%0D%0ATimeStamp%3A%20Tue,%2009%20Jan%202018%2019%3A12%3A22%20GMT%0D%0A%0D%0AClientIP%3A%20172.25.100.4%0D%0A%0D%0AHTTP%20Request%3A%0D%0AGET%20%2F%20HTTP%2F1.1%0AUser-Agent%3A%20Mozilla%2F5.0%20(Macintosh%3B%20Intel%20Mac%20OS%20X%2010.12%3B%20rv%3A57.0)%20Gecko%2F20100101%20Firefox%2F57.0%0D%0AAccept%3A%20text%2Fhtml,application%2Fxhtml+xml,application%2Fxml%3Bq%3D0.9,*%2F*%3Bq%3D0.8%0D%0AAccept-Language%3A%20es-ES,es%3Bq%3D0.8,en-US%3Bq%3D0.5,en%3Bq%3D0.3%0D%0AAccept-Encoding%3A%20gzip,%20deflate,%20br%0D%0AConnection%3A%20keep-alive%0D%0AUpgrade-Insecure-Requests%3A%201%0D%0AHost%3A%20outlook.co.il%0D%0A%0D%0A%0D%0A> 
> if you have difficulties authenticating yourself.
> 
>>
>> 4. Please rephrase "squid say what I am are not authenticated" - this 
>> is not
>> clear - what do you mean?
>>
>>> I am authenticated.
>>
>> To what? ?Squid, or the remote site?
> Squid, see message in Spanish for point 3.
> 

Your Squid log snippets you presented below say that the client which 
delivered a CONNECT message to Squid was authenticated. Things inside 
the tunnel encryption *cannot* be authenticated as separate things. 
Squid associates the credentials from the CONNECT tunnel for each 
request inside that tunnel.

That means that if you have any auth related config settings to the 
https:// request(s) which cause those credentials to need to be 
re-checked, to timeout, or any of a multitude of other situations that 
normally occur with auth - then the bumped traffic in that bump'd tunnel 
from that point onward cannot be serviced and you will start to have 
errors. The only viable solution is to avoid authentication checks on 
the decrypted / MITM'd / SSL-Bump'd traffic.



> Other error is that
> https://www.kiosco.bandec.cu/kiosco
> Error negotiating SSL on FD 16: error:14090086:SSL 
> routines:ssl3_get_server_certificate:certificate verify failed (1/-1/0)
> 
> The following error was encountered while trying to retrieve the URL: 
> https://www.kiosco.bandec.cu/*
> 
>     *Failed to establish a secure connection to 190.6.64.132*
> 
> The system returned:
> 
>     (71) Protocol error (TLS code: X509_V_ERR_UNABLE_TO_GET_ISSUER_CERT_LOCALLY)
> 
>     SSL Certficate error: certificate issuer (CA) not known:
>     /CN=CX6.bandec.cu
> 
> This proxy and the remote host failed to negotiate a mutually acceptable 
> security settings for handling your request. It is possible that the 
> remote host does not support secure connections, or the proxy is not 
> satisfied with the host security credentials.
> 

Please read the above error message carefully. It explains exactly what 
is going wrong, and from that you should be able to find the MANY 
discussion threads that exact same error message has had in here and 
elsewhere over the past few years.

Your options are to:

* configure 
<http://www.squid-cache.org/Doc/config/sslproxy_foreign_intermediate_certs/> 
(may require a Squid-3.5 upgrade), or

* upgrade to Squid-4 which auto-downloads these things.


>>
>> How do you know you are authenticated - what confirmation do you have?
>>
>>> Fragment of my squid.conf.
>>>
>>> http_port 3128 ssl-bump cert=/etc/squid/ssl_cert/ConAlza.pem
>>> generate-host-certificates=on dynamic_cert_mem_cache_size=4MB#
>>> options=NO_SSLv3 dhparams=/etc/squid/ssl_cert/dhparam.pem sslcrtd_program
>>> /usr/lib64/squid/ssl_crtd -s /var/lib/ssl_db -M 4MB sslproxy_options
>>> NO_SSLv2,NO_SSLv3,SINGLE_DH_USE
>>> acl step1 at_step SslBump1
>>> acl step2 at_step SslBump2
>>> acl step3 at_step SslBump3
>>> ssl_bump peek step1
>>> ssl_bump bump all
>>> authenticate_ip_ttl 60 seconds
>>
>> That looks a bit strange (and a bit incomplete) to me, but since I'm 
>> no expert
>> on SSL interception, I'll let someone else step in here.


The authenticate_ip_ttl is irrelevant except that if the auth system 
obeys it, the result will be all persistent connections producing errors 
60 seconds after they become authenticated.


>>
>> If you can provide more information in the meantime (eg: enough to help
>> someone else replicate your problem) that would be good.
>>
> I use too dansguardians before the squid proxy.
> 
> See the logs for one petition
> 
> 1515534858.355 ? 3720 aaa.aaa.aaa.aaa TAG_NONE/200 0 CONNECT 
> www.ssllabs.com:443 <http://www.ssllabs.com:443> ynieves 
> HIER_DIRECT/64.41.200.100 -
> 1515534858.375? ? ? 0 bbb.bbb.bbb.bbb TCP_DENIED/403 4457 GET 
> https://www.ssllabs.com/ssltest/viewMyClient.html ynieves HIER_NONE/- 
> text/html
> 1515534858.407? ? ? 0?bbb.bbb.bbb.bbb?TAG_NONE/503 4952 GET 
> http://artemisa.conalza.co.cu:3128/squid-internal-static/icons/SN.png 
> ynieves HIER_DIRECT/64.41.200.100 text/html
> 
> aaa.aaa.aaa.aaa is my pc.
> bbb.bbb.bbb.bbb is the dansguardians
> 

This is Squid delivering that above TLS error message to the 
client.Because of how browsers refuse to display errors presented by 
proxies to CONNECT requests. Squid is being forced to decrypt the HTTP 
message in the HTTPS tunnel and send the error page as a response to 
that encrypted request.



Amos


From yoinier.hn at gmail.com  Wed Jan 10 20:33:31 2018
From: yoinier.hn at gmail.com (Yoinier Hernandez Nieves)
Date: Wed, 10 Jan 2018 15:33:31 -0500
Subject: [squid-users] Squid and SSL Bump
In-Reply-To: <96a99b08-b3c8-52e5-17bf-575a75a4c3c7@treenet.co.nz>
References: <C735C3D2-8F2C-4967-8C96-56F9CBEFBC6B@gmail.com>
 <201801092227.57945.Antony.Stone@squid.open.source.it>
 <E6B47F15-D6FA-4655-9C46-09AC96521122@gmail.com>
 <96a99b08-b3c8-52e5-17bf-575a75a4c3c7@treenet.co.nz>
Message-ID: <EA144D48-E922-435F-8203-F72CBEE21D6F@gmail.com>


> El 10/01/2018, a las 8:47 a.m., Amos Jeffries <squid3 at treenet.co.nz> escribi?:
> 
> On 10/01/18 10:56, Yoinier Hernandez Nieves wrote:
>> I answer interline.
>>> El 9/01/2018, a las 4:27 p.m., Antony Stone escribi?:
>>> 
>>> On Tuesday 09 January 2018 at 21:28:37, Yoinier Hernandez Nieves wrote:
>>> 
>>>> I try configure squid 3.5 on CentOS 7 with sslBump.
>>>> 
>>>> But I have some problems, the first:
>>>> 
>>>> Some HTTPs sites can access, because squid say what I am are not
>>>> authenticated. And other sites, yes I can access.
>>> 
>>> Please give us information:
>>> 
>>> 1. An example of sites can you access.
>> not https
>>> 2. An example of sites can you not access.
>> https://www.ssllabs.com/ssltest/viewMyClient.html
>> https://outlook.co.il/
>> https://www.facebook.com
>>> 3. For problems, show us error messages - quote us what the remote sites tell
>>> you.
>> Se encontr? el siguiente error al intentar recuperar la direcci?n URL: https://outlook.co.il/
>>    *Acceso Denegado a la Cach?*
>> Lo lamento, tu no est?s autorizado a solicitar https://outlook.co.il/ de este cach? hasta que te hayas autenticado.
>> Please contact the cache administrator <mailto:root?subject=CacheErrorInfo%20-%20ERR_CACHE_ACCESS_DENIED&body=CacheHost%3A%20artemisa.conalza.co.cu%0D%0AErrPage%3A%20ERR_CACHE_ACCESS_DENIED%0D%0AErr%3A%20%5Bnone%5D%0D%0ATimeStamp%3A%20Tue,%2009%20Jan%202018%2019%3A12%3A22%20GMT%0D%0A%0D%0AClientIP%3A%20172.25.100.4%0D%0A%0D%0AHTTP%20Request%3A%0D%0AGET%20%2F%20HTTP%2F1.1%0AUser-Agent%3A%20Mozilla%2F5.0%20(Macintosh%3B%20Intel%20Mac%20OS%20X%2010.12%3B%20rv%3A57.0)%20Gecko%2F20100101%20Firefox%2F57.0%0D%0AAccept%3A%20text%2Fhtml,application%2Fxhtml+xml,application%2Fxml%3Bq%3D0.9,*%2F*%3Bq%3D0.8%0D%0AAccept-Language%3A%20es-ES,es%3Bq%3D0.8,en-US%3Bq%3D0.5,en%3Bq%3D0.3%0D%0AAccept-Encoding%3A%20gzip,%20deflate,%20br%0D%0AConnection%3A%20keep-alive%0D%0AUpgrade-Insecure-Requests%3A%201%0D%0AHost%3A%20outlook.co.il%0D%0A%0D%0A%0D%0A> if you have difficulties authenticating yourself.
>>> 
>>> 4. Please rephrase "squid say what I am are not authenticated" - this is not
>>> clear - what do you mean?
>>> 
>>>> I am authenticated.
>>> 
>>> To what?  Squid, or the remote site?
>> Squid, see message in Spanish for point 3.
> 
> Your Squid log snippets you presented below say that the client which delivered a CONNECT message to Squid was authenticated. Things inside the tunnel encryption *cannot* be authenticated as separate things. Squid associates the credentials from the CONNECT tunnel for each request inside that tunnel.
> 
> That means that if you have any auth related config settings to the https:// request(s) which cause those credentials to need to be re-checked, to timeout, or any of a multitude of other situations that normally occur with auth - then the bumped traffic in that bump'd tunnel from that point onward cannot be serviced and you will start to have errors. The only viable solution is to avoid authentication checks on the decrypted / MITM'd / SSL-Bump'd traffic.
> 
> 
> 
>> Other error is that
>> https://www.kiosco.bandec.cu/kiosco
>> Error negotiating SSL on FD 16: error:14090086:SSL routines:ssl3_get_server_certificate:certificate verify failed (1/-1/0)
>> The following error was encountered while trying to retrieve the URL: https://www.kiosco.bandec.cu/*
>>    *Failed to establish a secure connection to 190.6.64.132*
>> The system returned:
>>    (71) Protocol error (TLS code: X509_V_ERR_UNABLE_TO_GET_ISSUER_CERT_LOCALLY)
>>    SSL Certficate error: certificate issuer (CA) not known:
>>    /CN=CX6.bandec.cu
>> This proxy and the remote host failed to negotiate a mutually acceptable security settings for handling your request. It is possible that the remote host does not support secure connections, or the proxy is not satisfied with the host security credentials.
> 
> Please read the above error message carefully. It explains exactly what is going wrong, and from that you should be able to find the MANY discussion threads that exact same error message has had in here and elsewhere over the past few years.
> 
> Your options are to:
> 
> * configure <http://www.squid-cache.org/Doc/config/sslproxy_foreign_intermediate_certs/> (may require a Squid-3.5 upgrade), or
> 
> * upgrade to Squid-4 which auto-downloads these things.
> 
> 
>>> 
>>> How do you know you are authenticated - what confirmation do you have?
>>> 
>>>> Fragment of my squid.conf.
>>>> 
>>>> http_port 3128 ssl-bump cert=/etc/squid/ssl_cert/ConAlza.pem
>>>> generate-host-certificates=on dynamic_cert_mem_cache_size=4MB#
>>>> options=NO_SSLv3 dhparams=/etc/squid/ssl_cert/dhparam.pem sslcrtd_program
>>>> /usr/lib64/squid/ssl_crtd -s /var/lib/ssl_db -M 4MB sslproxy_options
>>>> NO_SSLv2,NO_SSLv3,SINGLE_DH_USE
>>>> acl step1 at_step SslBump1
>>>> acl step2 at_step SslBump2
>>>> acl step3 at_step SslBump3
>>>> ssl_bump peek step1
>>>> ssl_bump bump all
>>>> authenticate_ip_ttl 60 seconds
>>> 
>>> That looks a bit strange (and a bit incomplete) to me, but since I'm no expert
>>> on SSL interception, I'll let someone else step in here.
> 
> 
> The authenticate_ip_ttl is irrelevant except that if the auth system obeys it, the result will be all persistent connections producing errors 60 seconds after they become authenticated.
> 
> 
>>> 
>>> If you can provide more information in the meantime (eg: enough to help
>>> someone else replicate your problem) that would be good.
>>> 
>> I use too dansguardians before the squid proxy.
>> See the logs for one petition
>> 1515534858.355   3720 aaa.aaa.aaa.aaa TAG_NONE/200 0 CONNECT www.ssllabs.com:443 <http://www.ssllabs.com:443> ynieves HIER_DIRECT/64.41.200.100 -
>> 1515534858.375      0 bbb.bbb.bbb.bbb TCP_DENIED/403 4457 GET https://www.ssllabs.com/ssltest/viewMyClient.html ynieves HIER_NONE/- text/html
>> 1515534858.407      0 bbb.bbb.bbb.bbb TAG_NONE/503 4952 GET http://artemisa.conalza.co.cu:3128/squid-internal-static/icons/SN.png ynieves HIER_DIRECT/64.41.200.100 text/html
>> aaa.aaa.aaa.aaa is my pc.
>> bbb.bbb.bbb.bbb is the dansguardians
> 
> This is Squid delivering that above TLS error message to the client.Because of how browsers refuse to display errors presented by proxies to CONNECT requests. Squid is being forced to decrypt the HTTP message in the HTTPS tunnel and send the error page as a response to that encrypted request.
I try connect direct to the proxy, and this is the result

1515616366.189   1359 aaa.aaa.aaa.aaa TAG_NONE/200 0 CONNECT www.ssllabs.com:443 ynieves HIER_DIRECT/64.41.200.100 -
1515616366.207      0 aaa.aaa.aaa.aaa TCP_DENIED/403 4419 GET https://www.ssllabs.com/ssltest/viewMyClient.html ynieves HIER_NONE/- text/html
1515616366.244      0 aaa.aaa.aaa.aaa TAG_NONE/503 4914 GET http://artemisa.conalza.co.cu:3128/squid-internal-static/icons/SN.png ynieves HIER_DIRECT/64.41.200.100 text/html

How I can fix this.??

> 
> Amos
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20180110/2057be28/attachment.htm>

From xeron.oskom at gmail.com  Thu Jan 11 01:31:25 2018
From: xeron.oskom at gmail.com (Ivan Larionov)
Date: Wed, 10 Jan 2018 17:31:25 -0800
Subject: [squid-users] SMP mode and StoreID rewriter
Message-ID: <CAHvB88wUQvJ8H-ywxeVgJss7UwL+N8zmU1FRrs8hd9v_+HQmCQ@mail.gmail.com>

Hello.

We're currently testing squid in SMP mode. One of our services uses Store
ID feature. The interesting thing we see is that store_id_program started
for every squid process (except main). Process tree looks like this:

> squid
>  \_ (squid-coord-4)
>  |   \_ (rewriter_3)
>  |   \_ (rewriter_3)
>  \_ (squid-disk-3)
>  |   \_ (rewriter_3)
>  |   \_ (rewriter_3)
>  \_ (squid-2)
>  |   \_ (rewriter_3)
>  |   \_ (rewriter_3)
>  \_ (squid-1)
>      \_ (rewriter_3)
>      \_ (rewriter_3)

>From my brief testing it seems like rewrite is working as expected, but I
just wanted to make sure it's ok to see store_id_program started for every
child or may be it's sort of a bug.

May be it should be started only for "worker" or only for "disk"?

Relevant parts of the config:

> workers 2
> store_id_program /mnt/services/squid-url-rewriter/rewriter_3
> store_id_children 5 startup=2 idle=2 concurrency=10

>From the log file:

2018/01/10 16:56:12 kid4| helperOpenServers: Starting 2/5 'rewriter_3'
processes
2018/01/10 16:56:12 kid2| helperOpenServers: Starting 2/5 'rewriter_3'
processes
2018/01/10 16:56:12 kid3| helperOpenServers: Starting 2/5 'rewriter_3'
processes
2018/01/10 16:56:12 kid1| helperOpenServers: Starting 2/5 'rewriter_3'
processes

Squid Cache: Version 3.5.27
Service Name: squid
configure options:  '--program-prefix=' '--prefix=/usr'
'--exec-prefix=/usr' '--bindir=/usr/sbin' '--sbindir=/usr/sbin'
'--sysconfdir=/etc/squid' '--libdir=/usr/lib' '--libexecdir=/usr/lib/squid'
'--includedir=/usr/include' '--datadir=/usr/share'
'--sharedstatedir=/usr/com' '--localstatedir=/var'
'--mandir=/usr/share/man' '--infodir=/usr/share/info' '--enable-epoll'
'--enable-removal-policies=heap,lru' '--enable-storeio=aufs,rock'
'--enable-delay-pools' '--with-pthreads' '--enable-cache-digests'
'--with-large-files' '--with-maxfd=16384' '--enable-htcp'

-- 
With best regards, Ivan Larionov.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20180110/fc415a4c/attachment.htm>

From rousskov at measurement-factory.com  Thu Jan 11 04:30:45 2018
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Wed, 10 Jan 2018 21:30:45 -0700
Subject: [squid-users] SMP mode and StoreID rewriter
In-Reply-To: <CAHvB88wUQvJ8H-ywxeVgJss7UwL+N8zmU1FRrs8hd9v_+HQmCQ@mail.gmail.com>
References: <CAHvB88wUQvJ8H-ywxeVgJss7UwL+N8zmU1FRrs8hd9v_+HQmCQ@mail.gmail.com>
Message-ID: <1287464f-5368-be4d-e734-883a703c96c2@measurement-factory.com>

On 01/10/2018 06:31 PM, Ivan Larionov wrote:

> I just wanted to make sure it's ok to see store_id_program started for
> every child or may be it's sort of a bug.

It is "sort of a bug" -- Squid should not start store_id_program (and
many other helpers!) for some kids. Many Squid features are not made
SMP-aware yet, and features that "work fine" in SMP mode are often
ignored by developers and their sponsors. Quality patches welcome!

Ideally, workers should even be able to share helper processes, but that
is a huge change compared to simply not starting unnecessary helpers.


> May be it should be started only for "worker" or only for "disk"?

Yes, only for "worker" IIRC: Diskers do not deal with HTTP, and Store ID
is an HTTP-request-to-Store-ID translator.

Alex.


From squid3 at treenet.co.nz  Thu Jan 11 05:46:53 2018
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 11 Jan 2018 18:46:53 +1300
Subject: [squid-users] Squid and SSL Bump
In-Reply-To: <EA144D48-E922-435F-8203-F72CBEE21D6F@gmail.com>
References: <C735C3D2-8F2C-4967-8C96-56F9CBEFBC6B@gmail.com>
 <201801092227.57945.Antony.Stone@squid.open.source.it>
 <E6B47F15-D6FA-4655-9C46-09AC96521122@gmail.com>
 <96a99b08-b3c8-52e5-17bf-575a75a4c3c7@treenet.co.nz>
 <EA144D48-E922-435F-8203-F72CBEE21D6F@gmail.com>
Message-ID: <c5bb1474-c31a-a3ba-ca72-cdcf25d3eddc@treenet.co.nz>

On 11/01/18 09:33, Yoinier Hernandez Nieves wrote:
> 
> I try connect direct to the proxy, and this is the result
> 
> 1515616366.189 ? 1359 aaa.aaa.aaa.aaa TAG_NONE/200 0 CONNECT 
> www.ssllabs.com:443 <http://www.ssllabs.com:443> ynieves 
> HIER_DIRECT/64.41.200.100 -
> 1515616366.207? ? ? 0 aaa.aaa.aaa.aaa TCP_DENIED/403 4419 GET 
> https://www.ssllabs.com/ssltest/viewMyClient.html ynieves HIER_NONE/- 
> text/html
> 1515616366.244? ? ? 0 aaa.aaa.aaa.aaa TAG_NONE/503 4914 GET 
> http://artemisa.conalza.co.cu:3128/squid-internal-static/icons/SN.png 
> ynieves HIER_DIRECT/64.41.200.100 text/html
> 
> How I can fix this.??


What exactly do you think needs "fixing" ?


Amos


From ahmed.zaeem at netstream.ps  Thu Jan 11 06:36:18 2018
From: ahmed.zaeem at netstream.ps (--Ahmad--)
Date: Thu, 11 Jan 2018 08:36:18 +0200
Subject: [squid-users] want ignore if the ips added to the interface and
	force running it
Message-ID: <6415BC9C-43EF-454E-9023-3D47BA5D8225@netstream.ps>

Hello Guys .

sometimes i add like 100 ips on server interfaces then i run squid  including the 100 ips in the config 
config  like :

http_port 1.1.1.1:8080
acl ip1 myip 1.1.1.1
tcp_outgoing_address 1.1.1.1 ip1

and its ok ??




now say  the ip 1.1.1.1 wasn?t added to the interface config , when i run squid service ,  i will see error in squid say ?cant combined address ? and squid will crash .

so ?. what i need to do it is :

i want to force squid to be run even if the ip address not added to the network card .

is there any directive or edit c++ files ?


cheers 



From squid3 at treenet.co.nz  Thu Jan 11 07:01:13 2018
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 11 Jan 2018 20:01:13 +1300
Subject: [squid-users] want ignore if the ips added to the interface and
 force running it
In-Reply-To: <6415BC9C-43EF-454E-9023-3D47BA5D8225@netstream.ps>
References: <6415BC9C-43EF-454E-9023-3D47BA5D8225@netstream.ps>
Message-ID: <01f9f846-43c3-7ebf-b5fe-80f2bc9a5925@treenet.co.nz>

On 11/01/18 19:36, --Ahmad-- wrote:
> Hello Guys .
> 
> sometimes i add like 100 ips on server interfaces then i run squid  including the 100 ips in the config
> config  like :
> 
> http_port 1.1.1.1:8080
> acl ip1 myip 1.1.1.1
> tcp_outgoing_address 1.1.1.1 ip1
> 
> and its ok ??
> 
> 
> 
> 
> now say  the ip 1.1.1.1 wasn?t added to the interface config , when i run squid service ,  i will see error in squid say ?cant combined address ? and squid will crash .
> 
> so ?. what i need to do it is :
> 
> i want to force squid to be run even if the ip address not added to the network card .
> 
> is there any directive or edit c++ files ?


Use wildcard port(s) and a localip ACL. Like so:

  http_port 8080
  acl ip1 localip 1.1.1.1
  tcp_outgoing_address 1.1.1.1 ip1


Amos


From rentorbuy at yahoo.com  Thu Jan 11 08:34:20 2018
From: rentorbuy at yahoo.com (Vieri)
Date: Thu, 11 Jan 2018 08:34:20 +0000 (UTC)
Subject: [squid-users] TCP out of memory
In-Reply-To: <6eaaa26c-5226-5f5a-02f1-a9f5df690cc9@treenet.co.nz>
References: <601581447.203369.1515055873511.ref@mail.yahoo.com>
 <601581447.203369.1515055873511@mail.yahoo.com>
 <1042f27d-78e2-aa59-3fa7-804ded30ea13@treenet.co.nz>
 <1464426577.854952.1515142755546@mail.yahoo.com>
 <33a4d101-51c5-aac9-bacf-6c95de183bc1@treenet.co.nz>
 <775733626.1802411.1515363213807@mail.yahoo.com>
 <db3dd9ad-c49d-f649-99c8-f199b7844054@treenet.co.nz>
 <517226574.2508171.1515490819697@mail.yahoo.com>
 <6eaaa26c-5226-5f5a-02f1-a9f5df690cc9@treenet.co.nz>
Message-ID: <1359965755.990934.1515659660428@mail.yahoo.com>

Hi,

I don't know how to cleanly seperate the 93,* from the 11,* log lines. I posted the following:


https://drive.google.com/file/d/1PRJOc6czrA0QEDHkqn3MrmNh08K8JajR/view?usp=sharing

It contains a cache.log generated by:
debug_options rotate=1 ALL,0 93,6 11,6

I also ran :info and :filedescriptors when I applied the new debug_options (*1), and again when I reverted back the debug_options (*2).

I'm using c-icap with squidclamav. I'll try to use c-icap-modules instead asap so I can hopefully remove a few variables in the issue (if it keeps giving me this issue then it must be a c-icap service flaw).

Thanks,

Vieri


From ahmed.zaeem at netstream.ps  Thu Jan 11 08:50:21 2018
From: ahmed.zaeem at netstream.ps (--Ahmad--)
Date: Thu, 11 Jan 2018 10:50:21 +0200
Subject: [squid-users] want ignore if the ips added to the interface and
	force running it
In-Reply-To: <01f9f846-43c3-7ebf-b5fe-80f2bc9a5925@treenet.co.nz>
References: <6415BC9C-43EF-454E-9023-3D47BA5D8225@netstream.ps>
 <01f9f846-43c3-7ebf-b5fe-80f2bc9a5925@treenet.co.nz>
Message-ID: <2CF00933-95D3-45C6-A856-4A71F2D0A602@netstream.ps>

must the ip be attached on os interface so that squid use it as outgoing address ?

can squid use outgoing address that not being attached to the interface ?

i tried to outgoing address that not attached but squid gave an error :
2018/01/11 04:42:36 kid1| commBind: Cannot bind socket FD 11622 to [2abc:5ad1:1bc6:bc09:d18a:5fb4:239a:6277]: (99) Cannot assign requested address

whats your thoughts ?


> On Jan 11, 2018, at 9:01 AM, Amos Jeffries <squid3 at treenet.co.nz> wrote:
> 
> localip

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20180111/732aef32/attachment.htm>

From squid3 at treenet.co.nz  Thu Jan 11 10:07:40 2018
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 11 Jan 2018 23:07:40 +1300
Subject: [squid-users] want ignore if the ips added to the interface and
 force running it
In-Reply-To: <2CF00933-95D3-45C6-A856-4A71F2D0A602@netstream.ps>
References: <6415BC9C-43EF-454E-9023-3D47BA5D8225@netstream.ps>
 <01f9f846-43c3-7ebf-b5fe-80f2bc9a5925@treenet.co.nz>
 <2CF00933-95D3-45C6-A856-4A71F2D0A602@netstream.ps>
Message-ID: <58e9d93a-a69b-3012-e7ff-30082218e67e@treenet.co.nz>

On 11/01/18 21:50, --Ahmad-- wrote:
> must the ip be attached on os interface so that squid use it as outgoing 
> address ?
> 
> can squid use outgoing address that not being attached to the interface ?
> 

No it cannot.

But that is also why the config I suggested works where the one you 
attempted does not.

The ACL is what determines whether Squid attempts to bind an IP and it 
will never match *until* there is inbound traffic actually arriving with 
that local-IP.

So you configure Squid up-front with the IPs you are going to maybe 
assign. Then after Squid is running you assign and de-assign as 
necessary from that set, no need to reconfigure Squid constantly.



> i tried to outgoing address that not attached but squid gave an error :
> 2018/01/11 04:42:36 kid1| commBind: Cannot bind socket FD 11622 to 
> [2abc:5ad1:1bc6:bc09:d18a:5fb4:239a:6277]: (99) Cannot assign requested 
> address
> 
> whats your thoughts ?

I think the above is not IPv4. So your IPv4 specific settings are not 
relevant.

Amos


From ahmed.zaeem at netstream.ps  Thu Jan 11 10:22:47 2018
From: ahmed.zaeem at netstream.ps (--Ahmad--)
Date: Thu, 11 Jan 2018 12:22:47 +0200
Subject: [squid-users] want ignore if the ips added to the interface and
	force running it
In-Reply-To: <58e9d93a-a69b-3012-e7ff-30082218e67e@treenet.co.nz>
References: <6415BC9C-43EF-454E-9023-3D47BA5D8225@netstream.ps>
 <01f9f846-43c3-7ebf-b5fe-80f2bc9a5925@treenet.co.nz>
 <2CF00933-95D3-45C6-A856-4A71F2D0A602@netstream.ps>
 <58e9d93a-a69b-3012-e7ff-30082218e67e@treenet.co.nz>
Message-ID: <502DF22E-1B42-43B9-BB8C-3C531595024A@netstream.ps>

is this squid limitation ?

or

kernel limitation ?



> On Jan 11, 2018, at 12:07 PM, Amos Jeffries <squid3 at treenet.co.nz> wrote:
> 
> On 11/01/18 21:50, --Ahmad-- wrote:
>> must the ip be attached on os interface so that squid use it as outgoing address ?
>> can squid use outgoing address that not being attached to the interface ?
> 
> No it cannot.
> 
> But that is also why the config I suggested works where the one you attempted does not.
> 
> The ACL is what determines whether Squid attempts to bind an IP and it will never match *until* there is inbound traffic actually arriving with that local-IP.
> 
> So you configure Squid up-front with the IPs you are going to maybe assign. Then after Squid is running you assign and de-assign as necessary from that set, no need to reconfigure Squid constantly.
> 
> 
> 
>> i tried to outgoing address that not attached but squid gave an error :
>> 2018/01/11 04:42:36 kid1| commBind: Cannot bind socket FD 11622 to [2abc:5ad1:1bc6:bc09:d18a:5fb4:239a:6277]: (99) Cannot assign requested address
>> whats your thoughts ?
> 
> I think the above is not IPv4. So your IPv4 specific settings are not relevant.
> 
> Amos
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users



From hoangminhung at gmail.com  Thu Jan 11 10:39:04 2018
From: hoangminhung at gmail.com (=?UTF-8?B?bWluaCBoxrBuZyDEkeG7lyBob8Ogbmc=?=)
Date: Thu, 11 Jan 2018 17:39:04 +0700
Subject: [squid-users] How to block a https website with squid 3.5.3
Message-ID: <CAO0fBEoFiMUNM-_goCrW8ecm7DPoanRaBpbgChZtTcG2oyt9rw@mail.gmail.com>

Dear all, i using squid as a transparent proxy. But i can't deny a https
website like
https://remitano.com

My squid is compiled on ubuntu14 with this configure option
Squid Cache: Version 3.5.3
Service Name: squid
configure options:  '--prefix=/usr' '--includedir=/usr/include'
'--infodir=/usr/share/info' '--sysconfdir=/etc' '--localstatedir=/var'
'--libexecdir=/usr/lib/squid' '--srcdir=.' '--datadir=/usr/share/squid'
'--sysconfdir=/etc/squid' '--mandir=/usr/share/man' '--enable-inline'
'--enable-async-io=24' '--enable-storeio=ufs,aufs,diskd,rock'
'--enable-removal-policies=lru,heap' '--enable-gnuregex'
'--enable-cache-digests' '--enable-underscores' '--enable-icap-client'
'--enable-follow-x-forwarded-for' '--enable-eui' '--enable-esi'
'--enable-icmp' '--enable-zph-qos' '--enable-http-violations'
'--enable-ssl-crtd' '--enable-linux-netfilter' '--enable-ltdl-install'
'--enable-ltdl-convenience' '--enable-x-accelerator-vary'
'--disable-maintainer-mode' '--disable-dependency-tracking'
'--disable-silent-rules' '--disable-translation' '--disable-ipv6'
'--disable-ident-lookups' '--enable-delay-pools'
'--with-swapdir=/var/spool/squid' '--with-logdir=/var/log/squid'
'--with-pidfile=/var/run/squid.pid' '--with-aufs-threads=24'
'--with-filedescriptors=65536' '--with-large-files' '--with-maxfd=65536'
'--with-openssl' '--with-default-user=proxy' '--with-included-ltdl'

And here is my squid.conf

acl localnet src 192.168.10.0/24 #LAN
acl localnet src 10.10.10.0/24 #WIFI
acl localnet src 10.10.20.0/24 #WIFI
acl localnet src 172.18.18.0/24 #WIFI
acl localnet src 172.17.0.0/16
acl localnet src 10.10.1.0/24

acl Safe_ports port 21 # ftp
acl Safe_ports port 443 # https


acl Safe_ports port 70 # gopher
acl Safe_ports port 210 # wais
acl Safe_ports port 1025-65535 # unregistered ports
acl Safe_ports port 280 # http-mgmt
acl Safe_ports port 488 # gss-http
acl Safe_ports port 591 # filemaker
acl Safe_ports port 777 # multiling http
acl CONNECT method CONNECT

http_access deny !Safe_ports
http_access deny CONNECT !SSL_ports
http_access allow localhost manager
http_access deny manager
http_access allow localnet
http_access allow localhost
http_access deny all


acl step1 at_step SslBump1
ssl_bump peek step1
ssl_bump terminate blockregexurl
ssl_bump terminate domain
ssl_bump terminate block_domain
ssl_bump splice all


sslproxy_options NO_SSLv2,NO_SSLv3,No_Compression
sslproxy_cipher
ALL:!SSLv2:!SSLv3:!ADH:!DSS:!MD5:!EXP:!DES:!PSK:!SRP:!RC4:!IDEA:!SEED:!aNULL:!eNULL
sslproxy_cert_error deny all
sslproxy_flags  DONT_VERIFY_PEER
sslproxy_cafile /etc/squid/intermediate_ca.pem

sslcrtd_program /usr/lib/squid/ssl_crtd -s /var/lib/squid/ssl_db -M 4MB
sslcrtd_children 8 startup=1 idle=1

-----------------------
First , i can block facebook by use this command :
acl facebook dstdomain .facebook.com
http_access deny CONNECT facebook

But it is not effect with https://remitano.com

I try to use these command but it's not work:

acl blockregexurl url_regex -i ^http[s]?:\/\/.*\.remitano\.com\/(/vn)
http_access deny blockregexurl
http_access deny CONNECT blockregexurl

acl block_domain dstdomain remitano.com
acl domain dstdomain sso.remitano.com socket.remitano.com cdn.remitano.com
http_access deny block_domain
http_access deny CONNECT block_domain
http_access deny domain
http_access deny CONNECT domain


-- 
Thanks & Best Regards,
--------------
?? Ho?ng Minh H?ng
Gmail : hoangminhung at gmail.com
S?T : 01234454115
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20180111/675e88c6/attachment.htm>

From uhlar at fantomas.sk  Thu Jan 11 12:02:43 2018
From: uhlar at fantomas.sk (Matus UHLAR - fantomas)
Date: Thu, 11 Jan 2018 13:02:43 +0100
Subject: [squid-users] want ignore if the ips added to the interface and
 force running it
In-Reply-To: <502DF22E-1B42-43B9-BB8C-3C531595024A@netstream.ps>
References: <6415BC9C-43EF-454E-9023-3D47BA5D8225@netstream.ps>
 <01f9f846-43c3-7ebf-b5fe-80f2bc9a5925@treenet.co.nz>
 <2CF00933-95D3-45C6-A856-4A71F2D0A602@netstream.ps>
 <58e9d93a-a69b-3012-e7ff-30082218e67e@treenet.co.nz>
 <502DF22E-1B42-43B9-BB8C-3C531595024A@netstream.ps>
Message-ID: <20180111120243.GC7711@fantomas.sk>

>> On 11/01/18 21:50, --Ahmad-- wrote:
>>> must the ip be attached on os interface so that squid use it as outgoing address ?
>>> can squid use outgoing address that not being attached to the interface ?

>> On Jan 11, 2018, at 12:07 PM, Amos Jeffries <squid3 at treenet.co.nz> wrote:
>> No it cannot.

On 11.01.18 12:22, --Ahmad-- wrote:
>is this squid limitation ?
>
>or
>
>kernel limitation ?

what about logical limitation? in order for software to use an IP address,
that address must be configured in the system. 

-- 
Matus UHLAR - fantomas, uhlar at fantomas.sk ; http://www.fantomas.sk/
Warning: I wish NOT to receive e-mail advertising to this address.
Varovanie: na tuto adresu chcem NEDOSTAVAT akukolvek reklamnu postu.
I don't have lysdexia. The Dog wouldn't allow that.


From Antony.Stone at squid.open.source.it  Thu Jan 11 12:08:22 2018
From: Antony.Stone at squid.open.source.it (Antony Stone)
Date: Thu, 11 Jan 2018 13:08:22 +0100
Subject: [squid-users] want ignore if the ips added to the interface and
	force running it
In-Reply-To: <20180111120243.GC7711@fantomas.sk>
References: <6415BC9C-43EF-454E-9023-3D47BA5D8225@netstream.ps>
 <502DF22E-1B42-43B9-BB8C-3C531595024A@netstream.ps>
 <20180111120243.GC7711@fantomas.sk>
Message-ID: <201801111308.22838.Antony.Stone@squid.open.source.it>

On Thursday 11 January 2018 at 13:02:43, Matus UHLAR - fantomas wrote:

> >> On 11/01/18 21:50, --Ahmad-- wrote:
> >>> must the ip be attached on os interface so that squid use it as
> >>> outgoing address ? can squid use outgoing address that not being
> >>> attached to the interface ?
> >> 
> >> On Jan 11, 2018, at 12:07 PM, Amos Jeffries <squid3 at treenet.co.nz>
> >> wrote: No it cannot.
> 
> On 11.01.18 12:22, --Ahmad-- wrote:
> >is this squid limitation ?
> >
> >or
> >
> >kernel limitation ?
> 
> what about logical limitation? in order for software to use an IP address,
> that address must be configured in the system.

I'd say it's a networking limitation.  If Squid sends packets from an address 
which is not on the server, where will the reply packets end up and what use 
are they?


Antony.

-- 
Atheism is a non-prophet-making organisation.

                                                   Please reply to the list;
                                                         please *don't* CC me.


From squid3 at treenet.co.nz  Thu Jan 11 12:16:37 2018
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Fri, 12 Jan 2018 01:16:37 +1300
Subject: [squid-users] How to block a https website with squid 3.5.3
In-Reply-To: <CAO0fBEoFiMUNM-_goCrW8ecm7DPoanRaBpbgChZtTcG2oyt9rw@mail.gmail.com>
References: <CAO0fBEoFiMUNM-_goCrW8ecm7DPoanRaBpbgChZtTcG2oyt9rw@mail.gmail.com>
Message-ID: <89c7f65d-90b2-cc22-6386-949c4fb3a0e7@treenet.co.nz>

On 11/01/18 23:39, minh h?ng ?? ho?ng wrote:
> Dear all, i using squid as a transparent proxy. But i can't deny a https 
> website like
> https://remitano.com
> 

The first step is to upgrade your Squid. TLS hijacking is a very 
volatile area and things are changing often 3.5.3 is a very old Squid 
release now.
  The current Squid-3 version is 3.5.27.


> My squid is compiled on ubuntu14 with this configure option
> Squid Cache: Version 3.5.3
> Service Name: squid
> configure options:? '--prefix=/usr' '--includedir=/usr/include' 
> '--infodir=/usr/share/info' '--sysconfdir=/etc' '--localstatedir=/var' 
> '--libexecdir=/usr/lib/squid' '--srcdir=.' '--datadir=/usr/share/squid' 
> '--sysconfdir=/etc/squid' '--mandir=/usr/share/man' '--enable-inline' 
> '--enable-async-io=24' '--enable-storeio=ufs,aufs,diskd,rock' 
> '--enable-removal-policies=lru,heap' '--enable-gnuregex' 
> '--enable-cache-digests' '--enable-underscores' '--enable-icap-client' 
> '--enable-follow-x-forwarded-for' '--enable-eui' '--enable-esi' 
> '--enable-icmp' '--enable-zph-qos' '--enable-http-violations' 
> '--enable-ssl-crtd' '--enable-linux-netfilter' '--enable-ltdl-install' 
> '--enable-ltdl-convenience' '--enable-x-accelerator-vary' 
> '--disable-maintainer-mode' '--disable-dependency-tracking' 
> '--disable-silent-rules' '--disable-translation' '--disable-ipv6' 
> '--disable-ident-lookups' '--enable-delay-pools' 
> '--with-swapdir=/var/spool/squid' '--with-logdir=/var/log/squid' 
> '--with-pidfile=/var/run/squid.pid' '--with-aufs-threads=24' 
> '--with-filedescriptors=65536' '--with-large-files' '--with-maxfd=65536' 
> '--with-openssl' '--with-default-user=proxy' '--with-included-ltdl'
> 
> And here is my squid.conf
> 
> acl localnet src 192.168.10.0/24 <http://192.168.10.0/24> #LAN
> acl localnet src 10.10.10.0/24 <http://10.10.10.0/24> #WIFI
> acl localnet src 10.10.20.0/24 <http://10.10.20.0/24> #WIFI
> acl localnet src 172.18.18.0/24 <http://172.18.18.0/24> #WIFI
> acl localnet src 172.17.0.0/16 <http://172.17.0.0/16>
> acl localnet src 10.10.1.0/24 <http://10.10.1.0/24>
> 
> acl Safe_ports port 21 # ftp
> acl Safe_ports port 443 # https
> 
> 
> acl Safe_ports port 70 # gopher
> acl Safe_ports port 210 # wais
> acl Safe_ports port 1025-65535 # unregistered ports
> acl Safe_ports port 280 # http-mgmt
> acl Safe_ports port 488 # gss-http
> acl Safe_ports port 591 # filemaker
> acl Safe_ports port 777 # multiling http
> acl CONNECT method CONNECT
> 
> http_access deny !Safe_ports
> http_access deny CONNECT !SSL_ports
> http_access allow localhost manager
> http_access deny manager
> http_access allow localnet
> http_access allow localhost
> http_access deny all
> 
> 
> acl step1 at_step SslBump1
> ssl_bump peek step1
> ssl_bump terminate blockregexurl


TLS does not have URLs. So this will never work.


You need to use ssl::server_name ACLs instead of dstdomain for this 
directive.

ALso, maybe ssl::server_name_regex for the regex patterns *if* any are 
relevant after considering how URLs dont exist.

You are doing peek first, which should make the SNI details available 
for ssl::server_name* to use.



> ssl_bump terminate domain
> ssl_bump terminate block_domain
> ssl_bump splice all
> 
> 
> sslproxy_options NO_SSLv2,NO_SSLv3,No_Compression
> sslproxy_cipher  
> ALL:!SSLv2:!SSLv3:!ADH:!DSS:!MD5:!EXP:!DES:!PSK:!SRP:!RC4:!IDEA:!SEED:!aNULL:!eNULL
> sslproxy_cert_error deny all
> sslproxy_flags? DONT_VERIFY_PEER

Remove that DONT_VERIFY_PEER flag. It is only hiding problems from *you* 
the admin - while letting them still be problems for all your clients.

> sslproxy_cafile /etc/squid/intermediate_ca.pem
> 

This is a bit dangerous. Any non-intermediates Ca certs in that PEM file 
will allow remote hijacking of your proxy outbound connections by 
clients of that root CA.

That said,   you already completely disabled *ALL* verify checks on the 
server certs with DONT_VERIFY_PEER - so anyone can already hijack your 
traffic without needing to go to the trouble of even having their certs 
signed. All they need is some garbage bytes that use the correct X.509 
_format_ used by certs.

After you upgrade your Squid, change that to 
sslproxy_foreign_intermediate_certs which will only load intermediate 
certs for use. If your upgraded Squid does not accept that directive it 
is still too old to use safely for SSL-Bump.



> sslcrtd_program /usr/lib/squid/ssl_crtd -s /var/lib/squid/ssl_db -M 4MB
> sslcrtd_children 8 startup=1 idle=1
> 

What are the http_port and https_port lines you are using?

> -----------------------
> First , i can block facebook by use this command :
> acl facebook dstdomain .facebook.com <http://facebook.com>
> http_access deny CONNECT facebook
> 

You can only block domains like that if;
  a) you are using explicit proxy and the client sent a CONNECT with a 
domain name, or
  b) its IP address rDNS points back to the domain you are naming in the 
ACL, or
  c) the client sends TLS SNI details *and* your ssl_bump rules make 
that detail available to Squid (eg. peek).


see 
<https://wiki.squid-cache.org/Features/SslPeekAndSplice#Processing_steps> 
for details on what SSL-Bump actually does at each step of the TLS 
handshake.

Pay particular attention to what info is available at each "step" - and 
also what is *not* available.



> But it is not effect with https://remitano.com
> I try to use these command but it's not work:
> 
> acl blockregexurl url_regex -i ^http[s]?:\/\/.*\.remitano\.com\/(/vn)
> http_access deny blockregexurl
> http_access deny CONNECT blockregexurl

The regex pattern is looking for an absolute-form URL which will never 
exist in any CONNECT messages, since they always use authority-form URL.

That first http_access line might work *if* you already bumped the HTTPS 
traffic. The second never will.


> 
> acl block_domain dstdomain remitano.com <http://remitano.com>
> acl domain dstdomain sso.remitano.com <http://sso.remitano.com> 
> socket.remitano.com <http://socket.remitano.com> cdn.remitano.com 
> <http://cdn.remitano.com>
> http_access deny block_domain
> http_access deny CONNECT block_domain
> http_access deny domain
> http_access deny CONNECT domain
> 

Same issues mentioned above about the facebook dstdomain ACL as to when 
these dstdomain ACLs will match.

Except that here the "deny foo" lines that go first without mentioning 
CONNECT will match all the same things as the CONNECT line would - 
meaning they already block all traffic even stuff not using CONNECT 
tunnels. So the mention of CONNECT in these lines is pointless, and you 
can completely remove the lines which use it without changing the proxy 
behaviour.


Amos


From squid3 at treenet.co.nz  Thu Jan 11 12:37:56 2018
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Fri, 12 Jan 2018 01:37:56 +1300
Subject: [squid-users] want ignore if the ips added to the interface and
 force running it
In-Reply-To: <201801111308.22838.Antony.Stone@squid.open.source.it>
References: <6415BC9C-43EF-454E-9023-3D47BA5D8225@netstream.ps>
 <502DF22E-1B42-43B9-BB8C-3C531595024A@netstream.ps>
 <20180111120243.GC7711@fantomas.sk>
 <201801111308.22838.Antony.Stone@squid.open.source.it>
Message-ID: <60362063-12de-4dff-12af-8e3ad833e8ec@treenet.co.nz>

On 12/01/18 01:08, Antony Stone wrote:
> On Thursday 11 January 2018 at 13:02:43, Matus UHLAR - fantomas wrote:
> 
>>>> On 11/01/18 21:50, --Ahmad-- wrote:
>>>>> must the ip be attached on os interface so that squid use it as
>>>>> outgoing address ? can squid use outgoing address that not being
>>>>> attached to the interface ?
>>>>
>>>> On Jan 11, 2018, at 12:07 PM, Amos Jeffries wrote:
>>>> No it cannot.
>>
>> On 11.01.18 12:22, --Ahmad-- wrote:
>>> is this squid limitation ?
>>>
>>> or
>>>
>>> kernel limitation ?
>>
>> what about logical limitation? in order for software to use an IP address,
>> that address must be configured in the system.
> 
> I'd say it's a networking limitation.  If Squid sends packets from an address
> which is not on the server, where will the reply packets end up and what use
> are they?
> 

Indeed.

So to reply to Ahmad more clearly;

It is a limitation being _enforced_ by your kernel networking system. 
But that is only enforcement so don't think you can just patch around 
it. Patching around this one will just make you hit other errors 
elsewhere with the networking systems.


The only way to send non-assigned IPs from a machine is with mechanisms 
like TPROXY. Which places requirements on the *inbound* networking 
operates. Those inbound requirements prohibit Squid from being 
configured like you are wanting its inbound to operate.


Anyhow, I think we are getting well of track with this. My earlier 
suggested config was correct and the only way to reliably do what you 
said you wanted. Other problems can still occur, but are not related to
the problem you first posted nor to the config I suggested to make that 
requested behaviour happen.

Amos


From ahmed.zaeem at netstream.ps  Thu Jan 11 12:44:58 2018
From: ahmed.zaeem at netstream.ps (--Ahmad--)
Date: Thu, 11 Jan 2018 14:44:58 +0200
Subject: [squid-users] want ignore if the ips added to the interface and
	force running it
In-Reply-To: <60362063-12de-4dff-12af-8e3ad833e8ec@treenet.co.nz>
References: <6415BC9C-43EF-454E-9023-3D47BA5D8225@netstream.ps>
 <502DF22E-1B42-43B9-BB8C-3C531595024A@netstream.ps>
 <20180111120243.GC7711@fantomas.sk>
 <201801111308.22838.Antony.Stone@squid.open.source.it>
 <60362063-12de-4dff-12af-8e3ad833e8ec@treenet.co.nz>
Message-ID: <671FF257-F78F-4D8A-AEA3-6D983B33FD89@netstream.ps>

Guys you were great , thanks for all your replies .

you help me so much 


cheers 
> On Jan 11, 2018, at 2:37 PM, Amos Jeffries <squid3 at treenet.co.nz> wrote:
> 
> On 12/01/18 01:08, Antony Stone wrote:
>> On Thursday 11 January 2018 at 13:02:43, Matus UHLAR - fantomas wrote:
>>>>> On 11/01/18 21:50, --Ahmad-- wrote:
>>>>>> must the ip be attached on os interface so that squid use it as
>>>>>> outgoing address ? can squid use outgoing address that not being
>>>>>> attached to the interface ?
>>>>> 
>>>>> On Jan 11, 2018, at 12:07 PM, Amos Jeffries wrote:
>>>>> No it cannot.
>>> 
>>> On 11.01.18 12:22, --Ahmad-- wrote:
>>>> is this squid limitation ?
>>>> 
>>>> or
>>>> 
>>>> kernel limitation ?
>>> 
>>> what about logical limitation? in order for software to use an IP address,
>>> that address must be configured in the system.
>> I'd say it's a networking limitation.  If Squid sends packets from an address
>> which is not on the server, where will the reply packets end up and what use
>> are they?
> 
> Indeed.
> 
> So to reply to Ahmad more clearly;
> 
> It is a limitation being _enforced_ by your kernel networking system. But that is only enforcement so don't think you can just patch around it. Patching around this one will just make you hit other errors elsewhere with the networking systems.
> 
> 
> The only way to send non-assigned IPs from a machine is with mechanisms like TPROXY. Which places requirements on the *inbound* networking operates. Those inbound requirements prohibit Squid from being configured like you are wanting its inbound to operate.
> 
> 
> Anyhow, I think we are getting well of track with this. My earlier suggested config was correct and the only way to reliably do what you said you wanted. Other problems can still occur, but are not related to
> the problem you first posted nor to the config I suggested to make that requested behaviour happen.
> 
> Amos
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users



From yoinier.hn at gmail.com  Thu Jan 11 14:24:41 2018
From: yoinier.hn at gmail.com (Yoinier Hernandez Nieves)
Date: Thu, 11 Jan 2018 09:24:41 -0500
Subject: [squid-users] Squid and SSL Bump
In-Reply-To: <c5bb1474-c31a-a3ba-ca72-cdcf25d3eddc@treenet.co.nz>
References: <C735C3D2-8F2C-4967-8C96-56F9CBEFBC6B@gmail.com>
 <201801092227.57945.Antony.Stone@squid.open.source.it>
 <E6B47F15-D6FA-4655-9C46-09AC96521122@gmail.com>
 <96a99b08-b3c8-52e5-17bf-575a75a4c3c7@treenet.co.nz>
 <EA144D48-E922-435F-8203-F72CBEE21D6F@gmail.com>
 <c5bb1474-c31a-a3ba-ca72-cdcf25d3eddc@treenet.co.nz>
Message-ID: <4FC5E06B-32FF-4A45-BD50-F4AC9236B45F@gmail.com>


> El 11/01/2018, a las 12:46 a.m., Amos Jeffries <squid3 at treenet.co.nz> escribi?:
> 
> On 11/01/18 09:33, Yoinier Hernandez Nieves wrote:
>> I try connect direct to the proxy, and this is the result
>> 1515616366.189   1359 aaa.aaa.aaa.aaa TAG_NONE/200 0 CONNECT www.ssllabs.com:443 <http://www.ssllabs.com:443> ynieves HIER_DIRECT/64.41.200.100 -
>> 1515616366.207      0 aaa.aaa.aaa.aaa TCP_DENIED/403 4419 GET https://www.ssllabs.com/ssltest/viewMyClient.html ynieves HIER_NONE/- text/html
>> 1515616366.244      0 aaa.aaa.aaa.aaa TAG_NONE/503 4914 GET http://artemisa.conalza.co.cu:3128/squid-internal-static/icons/SN.png ynieves HIER_DIRECT/64.41.200.100 text/html
>> How I can fix this.??
> 
> 
> What exactly do you think needs "fixing? ?

I need fix the problem with the auth failure.

Hi say:

Sorry, you are not currently allowed to request https://www.google.com/search? from this cache until you have authenticated yourself.

But I stay authenticated, see the log, user, ynieves.

Thanks

Yoinier Hernandez Nieves

> 
> 
> Amos
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users



From yoinier.hn at gmail.com  Thu Jan 11 14:34:16 2018
From: yoinier.hn at gmail.com (Yoinier Hernandez Nieves)
Date: Thu, 11 Jan 2018 09:34:16 -0500
Subject: [squid-users] Squid and SSL Bump
In-Reply-To: <c5bb1474-c31a-a3ba-ca72-cdcf25d3eddc@treenet.co.nz>
References: <C735C3D2-8F2C-4967-8C96-56F9CBEFBC6B@gmail.com>
 <201801092227.57945.Antony.Stone@squid.open.source.it>
 <E6B47F15-D6FA-4655-9C46-09AC96521122@gmail.com>
 <96a99b08-b3c8-52e5-17bf-575a75a4c3c7@treenet.co.nz>
 <EA144D48-E922-435F-8203-F72CBEE21D6F@gmail.com>
 <c5bb1474-c31a-a3ba-ca72-cdcf25d3eddc@treenet.co.nz>
Message-ID: <1D233746-E0CB-4249-B440-5236C59E0968@gmail.com>


> El 11/01/2018, a las 12:46 a.m., Amos Jeffries <squid3 at treenet.co.nz> escribi?:
> 
> On 11/01/18 09:33, Yoinier Hernandez Nieves wrote:
>> I try connect direct to the proxy, and this is the result
>> 1515616366.189   1359 aaa.aaa.aaa.aaa TAG_NONE/200 0 CONNECT www.ssllabs.com:443 <http://www.ssllabs.com:443> ynieves HIER_DIRECT/64.41.200.100 -
>> 1515616366.207      0 aaa.aaa.aaa.aaa TCP_DENIED/403 4419 GET https://www.ssllabs.com/ssltest/viewMyClient.html ynieves HIER_NONE/- text/html
>> 1515616366.244      0 aaa.aaa.aaa.aaa TAG_NONE/503 4914 GET http://artemisa.conalza.co.cu:3128/squid-internal-static/icons/SN.png ynieves HIER_DIRECT/64.41.200.100 text/html
>> How I can fix this.??
> 
> 
> What exactly do you think needs "fixing? ?

1515681064.026  28086 10.22.1.40 TCP_MISS/200 41681 GET http://media2.coltiendas.com/img/p/7/6/2/2/7622-home_default.jpg ynieves HIER_DIRECT/67.205.111.16 image/jpeg
1515681064.026  28087 10.22.1.40 TCP_MISS/200 50331 GET http://media2.coltiendas.com/img/p/7/5/9/8/7598-home_default.jpg ynieves HIER_DIRECT/67.205.111.16 image/jpeg
1515681066.950  29978 10.22.1.40 TCP_MISS/200 57206 GET http://www.coltiendas.com/themes/default-bootstrap/fonts/fontawesome-webfont.woff2? ynieves HIER_DIRECT/192.155.83.60 -
1515681106.482   1247 10.22.1.40 TAG_NONE/200 0 CONNECT www.ssllabs.com:443 ynieves HIER_DIRECT/64.41.200.100 -
1515681106.497      0 10.22.1.40 TCP_DENIED/403 4419 GET https://www.ssllabs.com/ssltest/viewMyClient.html ynieves HIER_NONE/- text/html
1515681106.539      0 10.22.1.40 TAG_NONE/503 4914 GET http://artemisa.conalza.co.cu:3128/squid-internal-static/icons/SN.png ynieves HIER_DIRECT/64.41.200.100 text/html

See, first an access http correctly, after the denied petitions to ssllabs.com <http://ssllabs.com/>, all with the user authenticated. 

> 
> 
> Amos
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20180111/26d9752c/attachment.htm>

From squid3 at treenet.co.nz  Thu Jan 11 15:47:14 2018
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Fri, 12 Jan 2018 04:47:14 +1300
Subject: [squid-users] Squid and SSL Bump
In-Reply-To: <4FC5E06B-32FF-4A45-BD50-F4AC9236B45F@gmail.com>
References: <C735C3D2-8F2C-4967-8C96-56F9CBEFBC6B@gmail.com>
 <201801092227.57945.Antony.Stone@squid.open.source.it>
 <E6B47F15-D6FA-4655-9C46-09AC96521122@gmail.com>
 <96a99b08-b3c8-52e5-17bf-575a75a4c3c7@treenet.co.nz>
 <EA144D48-E922-435F-8203-F72CBEE21D6F@gmail.com>
 <c5bb1474-c31a-a3ba-ca72-cdcf25d3eddc@treenet.co.nz>
 <4FC5E06B-32FF-4A45-BD50-F4AC9236B45F@gmail.com>
Message-ID: <fcdb7101-6f12-6814-d069-f7841e35cc11@treenet.co.nz>

On 12/01/18 03:24, Yoinier Hernandez Nieves wrote:
> 
>> El 11/01/2018, a las 12:46 a.m., Amos Jeffries escribi?:
>>
>> On 11/01/18 09:33, Yoinier Hernandez Nieves wrote:
>>> I try connect direct to the proxy, and this is the result
>>> 1515616366.189   1359 aaa.aaa.aaa.aaa TAG_NONE/200 0 CONNECT www.ssllabs.com:443 <http://www.ssllabs.com:443> ynieves HIER_DIRECT/64.41.200.100 -
>>> 1515616366.207      0 aaa.aaa.aaa.aaa TCP_DENIED/403 4419 GET https://www.ssllabs.com/ssltest/viewMyClient.html ynieves HIER_NONE/- text/html
>>> 1515616366.244      0 aaa.aaa.aaa.aaa TAG_NONE/503 4914 GET http://artemisa.conalza.co.cu:3128/squid-internal-static/icons/SN.png ynieves HIER_DIRECT/64.41.200.100 text/html
>>> How I can fix this.??
>>
>>
>> What exactly do you think needs "fixing? ?
> 
> I need fix the problem with the auth failure.
> 
> Hi say:
> 
> Sorry, you are not currently allowed to request https://www.google.com/search? from this cache until you have authenticated yourself.
> 
> But I stay authenticated, see the log, user, ynieves.
> 

Then something in your squid.conf is forbidding username ynieves access 
to use the proxy and defining that other username might be allowed. But 
it provides that info far too late to re-authenticate the already 
finished CONNECT message with usable credentials.

Please post *all* of your squid.conf settings so we can look in places 
you might not have expected to find auth relationships. Just exclude 
empty lines and # comments.


Amos


From Brian.Snyder at beavercreek.k12.oh.us  Thu Jan 11 17:14:04 2018
From: Brian.Snyder at beavercreek.k12.oh.us (Snyder, Brian)
Date: Thu, 11 Jan 2018 17:14:04 +0000
Subject: [squid-users] Performance
Message-ID: <D2E66EB8081A7D4C86314759C29AAFE043353EE5@BHSMSX03>

Hello All,

I apologise for asking another squid performance question, but I have been banging my head against the wall for the better part of three months. Squid is installed and working. However, over time it slows down significantly. I have tried everything from turning off caching to trying to load balance several squid machines. Would someone mind giving my config a quick look.

Hardware:
Centos 7
32GB Ram
Xeon E5 4 cores
4x 140G SAS 15k (Cache)
2x 64G SSD (OS mirror)
10G Network connection

Users:
about 10k devices random web traffic

squid.conf scrubbed
# RADIUS Config
auth_param basic program /usr/lib64/squid/basic_radius_auth -h 10.10. -w password
auth_param basic children 2
auth_param basic realm BCS External Proxy
auth_param basic credentialsttl 24 hour

# ACL Lists
acl ACL_All src all
acl ACL_Password proxy_auth REQUIRED
acl ACL_SSL_Ports port 80 443 1443 7446 8443
acl ACL_Safe_Ports port "/etc/squid/ports.conf"
acl ACL_Connect method CONNECT
acl ACL_Purge method PURGE
acl ACL_Do_Not_Cache dstdomain "/etc/squid/lists.conf"
acl ACL_Query urlpath_regex cgi-bin \?
acl ACL_Deny_Url dstdomain "/etc/squid/deny.conf"
acl ACL_Allow_Url dstdomain "/etc/squid/allow.conf"
acl ACL_Web_Filter dst 10.10.18.1/32
acl ACL_Beavercreek_Clients src 10.10.0.0/16 172.16.0.0/16
acl ACL_Beavercreek_Networks dst 10.10.0.0/16 172.16.0.0/16
acl ACL_MVECA_Networks dst 10.3.0.0/16
acl ACL_Manager proto cache_object
acl ACL_Apple dstdomain .appldnld.apple.com .gspe19.ls.apple.com .init-p01md.apple.com .init-p01st.push.apple.com .init.ess.apple.com .iosapps.itunes.apple.com .mesu.apple.com .pancake.apple.com .phobos.apple.com .ocsp.apple.com
acl ACL_Apple_Dest dst 10.10.18.31/32 10.10.18.32/32 10.10.18.33/32
acl ACL_School_Hours time MTWHF 07:30-16:00
acl ACL_Block_Apps urlpath_regex -i \.ipa(\?.*)?$ \.mobileconfig(\?.*)?$ \.plist(\?.*)?$
#acl ACL_Block_IOS urlpath_regex -i
acl ACL_Full_Speed src 10.10.0.0/16 172.16.160.0/20

# Delay Pools
delay_pools 2
delay_class 1 2
delay_parameters 1 -1/-1 -1/-1
delay_access 1 allow ACL_Full_Speed
delay_access 1 deny ACL_All

delay_class 2 2
delay_parameters 2 -1/-1 200000/200000
delay_access 2 allow ACL_Apple ACL_School_Hours
delay_access 2 allow ACL_Apple_Dest ACL_School_Hours
delay_access 2 deny ACL_All

delay_initial_bucket_level 50

# Access Rules
http_access allow ACL_Web_Filter
#http_access deny ACL_Block_IOS
http_access allow ACL_Apple_Dest
http_access allow ACL_Allow_Url
http_access allow ACL_Apple
http_access deny ACL_Deny_Url
http_access deny !ACL_Safe_Ports
http_access deny ACL_Block_Apps
http_access allow ACL_Connect ACL_SSL_Ports
http_access deny ACL_Connect !ACL_SSL_Ports
http_access allow ACL_Beavercreek_Clients
http_access allow ACL_Purge ACL_Beavercreek_Clients
http_access allow ACL_Beavercreek_Clients ACL_Manager
http_access allow ACL_Password
http_access deny !ACL_Password
http_access deny ACL_All

#Forward
forwarded_for truncate
via on

#Do not cache rules
#cache deny all
always_direct allow ACL_Do_Not_Cache ACL_Apple_Dest ACL_Apple ACL_Beavercreek_Networks ACL_MVECA_Networks
cache deny ACL_Do_Not_Cache ACL_Query ACL_Apple ACL_Apple_Dest ACL_Beavercreek_Networks ACL_MVECA_Networks

# Network Info
http_port 10.10.:8888
http_port 10.10.:3128

#Worker info
workers 4
cpu_affinity_map process_numbers=1,2,3,4 cores=1,3,5,7

# DNS Config
dns_v4_first on
dns_nameservers 10.10. 10.10.
append_domain xxx

# Cache config
cache_mem 16 GB
cache_effective_user squid
cache_effective_group squid
memory_cache_mode always
memory_replacement_policy heap GDSF
cache_replacement_policy heap LFUDA
max_open_disk_fds 0
maximum_object_size_in_memory 1 MB
maximum_object_size 100 MB

cache_swap_high 95
cache_swap_low 90

max_filedesc 16384
fqdncache_size 8192
ipcache_size 8192

# MISC Settings
visible_hostname xxx
cache_mgr xxx
logfile_rotate 1
half_closed_clients off
dead_peer_timeout 30 second
dns_timeout 5 second
connect_timeout 30 second
shutdown_lifetime 10 second
server_persistent_connections off
authenticate_ttl 1 hour
authenticate_ip_ttl 1 hour
#ignore_expect_100 on
reply_header_max_size 128 KB

# Uncomment and adjust the following to add a disk cache directory.

cache_dir rock /var/spool/squid 20480 min-size=1 max-size=31000 max-swap-rate=100 swap-timeout=1000

if ${process_number} = 1
cache_dir diskd /squid/data1/aufs 51200 32 256 min-size=31001 max-size=104857600
endif
if ${process_number} = 2
cache_dir diskd /squid/data2/aufs 51200 32 256 min-size=31001 max-size=104857600
endif
if ${process_number} = 3
cache_dir diskd /squid/data3/aufs 51200 32 256 min-size=31001 max-size=104857600
endif
if ${process_number} = 4
cache_dir diskd /squid/data4/aufs 51200 32 256 min-size=31001 max-size=104857600
endif

logformat squid %tl.%03tu %6tr %>a %Ss/%03>Hs %<st %rm %ru %un %Sh/%<A %<a %mt
access_log stdio:/var/log/squid/access.log squid

# Leave coredumps in the first cache dir
coredump_dir /var/spool/squid

# We recommend you to use at least the following line.
#hierarchy_stoplist cgi-bin ?

# Add any of your own refresh_pattern entries above these.
refresh_pattern ^ftp:           1440    20%     10080
refresh_pattern ^gopher:        1440    0%      1440
refresh_pattern -i (/cgi-bin/|\?) 0     0%      0
refresh_pattern .               0       20%     4320

#Custom error URL
error_directory /etc/squid/custom


Info:
HTTP/1.1 200 OK
Server: squid/3.5.20
Mime-Version: 1.0
Date: Thu, 11 Jan 2018 17:04:50 GMT
Content-Type: text/plain
Expires: Thu, 11 Jan 2018 17:04:50 GMT
Last-Modified: Thu, 11 Jan 2018 17:04:50 GMT
Connection: close

Squid Object Cache: Version 3.5.20
Build Info:
Service Name: squid
Start Time:     Thu, 11 Jan 2018 03:26:58 GMT
Current Time:   Thu, 11 Jan 2018 17:04:50 GMT
Connection information for squid:
        Number of clients accessing cache:      35529
        Number of HTTP requests received:       4231669
        Number of ICP messages received:        0
        Number of ICP messages sent:    0
        Number of queued ICP replies:   0
        Number of HTCP messages received:       0
        Number of HTCP messages sent:   0
        Request failure ratio:   0.00
        Average HTTP requests per minute since start:   5174.0
        Average ICP messages per minute since start:    0.0
        Select loop called: 299015711 times, 12.376 ms avg
Cache information for squid:
        Hits as % of all requests:      5min: 5.4%, 60min: 3.6%
        Hits as % of bytes sent:        5min: 2.2%, 60min: 2.6%
        Memory hits as % of hit requests:       5min: 21.2%, 60min: 15.4%
        Disk hits as % of hit requests: 5min: 47.1%, 60min: 40.2%
        Storage Swap size:      60801452 KB
        Storage Swap capacity:  26.4% used, 73.6% free
        Storage Mem size:       2005632 KB
        Storage Mem capacity:   12.0% used, 88.0% free
        Mean Object Size:       119.80 KB
        Requests given to unlinkd:      0
Median Service Times (seconds)  5 min    60 min:
        HTTP Requests (All):   0.90173  0.47329   I have seen these go as high as 20 seconds
        Cache Misses:          0.04781  0.06103
        Cache Hits:            0.00000  0.00000
        Near Hits:             0.02599  0.04127
        Not-Modified Replies:  0.00000  0.00000
        DNS Lookups:           0.00019  0.00056
        ICP Queries:           0.00000  0.00000
Resource usage for squid:
        UP Time:        49072.590 seconds
        CPU Time:       4228.270 seconds
        CPU Usage:      8.62%
        CPU Usage, 5 minute avg:        10.12%
        CPU Usage, 60 minute avg:       15.49%
        Maximum Resident Size: 25868624 KB
        Page faults with physical i/o: 3
Memory accounted for:
        Total accounted:       212706 KB
        memPoolAlloc calls: 647869201
        memPoolFree calls:  656372502
File descriptor usage for squid:
        Maximum number of file descriptors:   81920
        Largest file desc currently in use:   10848
        Number of file desc currently in use: 9169
        Files queued for open:                   0
        Available number of file descriptors: 72751
        Reserved number of file descriptors:   500
        Store Disk files open:                   1
Internal Data Structures:
        162462 StoreEntries
           291 StoreEntries with MemObjects
         62657 Hot Object Cache Items
        507526 on-disk objects

60Min:
HTTP/1.1 200 OK
Server: squid/3.5.20
Mime-Version: 1.0
Date: Thu, 11 Jan 2018 17:06:23 GMT
Content-Type: text/plain
Expires: Thu, 11 Jan 2018 17:06:23 GMT
Last-Modified: Thu, 11 Jan 2018 17:06:23 GMT
Connection: close

sample_start_time = 1515686758.822127 (Thu, 11 Jan 2018 16:05:58 GMT)
sample_end_time = 1515690358.901608 (Thu, 11 Jan 2018 17:05:58 GMT)
client_http.requests = 184.169353/sec
client_http.hits = 7.716049/sec
client_http.errors = 13.524058/sec
client_http.kbytes_in = 549.705029/sec
client_http.kbytes_out = 13344.171474/sec
client_http.all_median_svc_time = 0.499589 seconds
client_http.miss_median_svc_time = 0.061028 seconds
client_http.nm_median_svc_time = 0.000000 seconds
client_http.nh_median_svc_time = 0.041120 seconds
client_http.hit_median_svc_time = 0.000000 seconds
server.all.requests = 164.990063/sec
server.all.errors = 0.000000/sec
server.all.kbytes_in = 13006.455302/sec
server.all.kbytes_out = 510.372012/sec
server.http.requests = 50.421262/sec
server.http.errors = 0.000000/sec
server.http.kbytes_in = 2179.449438/sec
server.http.kbytes_out = 69.110279/sec
server.ftp.requests = 0.000000/sec
server.ftp.errors = 0.000000/sec
server.ftp.kbytes_in = 0.000000/sec
server.ftp.kbytes_out = 0.000000/sec
server.other.requests = 114.568802/sec
server.other.errors = 0.000000/sec
server.other.kbytes_in = 10827.005308/sec
server.other.kbytes_out = 441.261456/sec
icp.pkts_sent = 0.000000/sec
icp.pkts_recv = 0.000000/sec
icp.queries_sent = 0.000000/sec
icp.replies_sent = 0.000000/sec
icp.queries_recv = 0.000000/sec
icp.replies_recv = 0.000000/sec
icp.replies_queued = 0.000000/sec
icp.query_timeouts = 0.000000/sec
icp.kbytes_sent = 0.000000/sec
icp.kbytes_recv = 0.000000/sec
icp.q_kbytes_sent = 0.000000/sec
icp.r_kbytes_sent = 0.000000/sec
icp.q_kbytes_recv = 0.000000/sec
icp.r_kbytes_recv = 0.000000/sec
icp.query_median_svc_time = 0.000000 seconds
icp.reply_median_svc_time = 0.000000 seconds
dns.median_svc_time = 0.000557 seconds
unlink.requests = 0.000000/sec
page_faults = 0.000833/sec
select_loops = 12880.416435/sec
select_fds = 9811.879014/sec
average_select_fd_period = 0.000000/fd
median_select_fds = 0.000000
swap.outs = 8.875762/sec
swap.ins = 8.034935/sec
swap.files_cleaned = 0.000000/sec
aborted_requests = 1.342211/sec
syscalls.disk.opens = 3.490250/sec
syscalls.disk.closes = 3.489972/sec
syscalls.disk.reads = 76.404351/sec
syscalls.disk.writes = 104.594693/sec
syscalls.disk.seeks = 0.000000/sec
syscalls.disk.unlinks = 0.146943/sec
syscalls.sock.accepts = 569.969450/sec
syscalls.sock.sockets = 197.239195/sec
syscalls.sock.connects = 164.998952/sec
syscalls.sock.binds = 0.000000/sec
syscalls.sock.closes = 363.784801/sec
syscalls.sock.reads = 4398.237408/sec
syscalls.sock.writes = 4622.031167/sec
syscalls.sock.recvfroms = 118.198756/sec
syscalls.sock.sendtos = 61.557552/sec
cpu_time = 558.603203 seconds
wall_time = 18000.159289 seconds
cpu_usage = 3.103324%

tail end of cache.log
[root at proxy ~]# tail -f /var/log/squid/cache.log
2018/01/11 12:05:39 kid3| urlParse: URL too large (12594 bytes)
2018/01/11 12:05:39 kid3| urlParse: URL too large (12602 bytes)
2018/01/11 12:05:47 kid1| fqdncacheParse: No PTR record for '91.212.150.79'
2018/01/11 12:06:10 kid3| urlParse: URL too large (12720 bytes)
2018/01/11 12:06:20 kid4| urlParse: URL too large (13122 bytes)
2018/01/11 12:06:20 kid3| urlParse: URL too large (13166 bytes)
2018/01/11 12:06:32 kid3| urlParse: URL too large (12599 bytes)
2018/01/11 12:06:34 kid3| urlParse: URL too large (12598 bytes)
2018/01/11 12:06:52 kid3| urlParse: URL too large (12724 bytes)
2018/01/11 12:06:53 kid3| urlParse: URL too large (12636 bytes)

I should also mention this proxy is behind a web content filter which I believe may be running proxy services. Any help would be appreciated.

Thanks,
Brian Snyder


-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20180111/6581de9c/attachment.htm>

From rousskov at measurement-factory.com  Thu Jan 11 19:23:43 2018
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Thu, 11 Jan 2018 12:23:43 -0700
Subject: [squid-users] Performance
In-Reply-To: <D2E66EB8081A7D4C86314759C29AAFE043353EE5@BHSMSX03>
References: <D2E66EB8081A7D4C86314759C29AAFE043353EE5@BHSMSX03>
Message-ID: <9902c898-d7c3-95d2-1d1b-351605b8bd2d@measurement-factory.com>

On 01/11/2018 10:14 AM, Snyder, Brian wrote:

> I apologise for asking another squid performance question, 

There is nothing wrong with that! It is often very difficult to solve
performance problems on the mailing list, but that does not imply folks
should not ask performance questions.


> over time it slows down significantly.

In case nobody looks for or finds problems in your configuration: What
is the bottleneck? CPU? RAM? Disk I/O? NIC interrupts?

A tool like atop may be able to answer that question for you, especially
if you let it collect stats from before Squid start to the time when the
Squid becomes very slow.

Alex.


From yoinier.hn at gmail.com  Fri Jan 12 13:00:40 2018
From: yoinier.hn at gmail.com (Yoinier Hernandez Nieves)
Date: Fri, 12 Jan 2018 08:00:40 -0500
Subject: [squid-users] Squid and SSL Bump
In-Reply-To: <fcdb7101-6f12-6814-d069-f7841e35cc11@treenet.co.nz>
References: <C735C3D2-8F2C-4967-8C96-56F9CBEFBC6B@gmail.com>
 <201801092227.57945.Antony.Stone@squid.open.source.it>
 <E6B47F15-D6FA-4655-9C46-09AC96521122@gmail.com>
 <96a99b08-b3c8-52e5-17bf-575a75a4c3c7@treenet.co.nz>
 <EA144D48-E922-435F-8203-F72CBEE21D6F@gmail.com>
 <c5bb1474-c31a-a3ba-ca72-cdcf25d3eddc@treenet.co.nz>
 <4FC5E06B-32FF-4A45-BD50-F4AC9236B45F@gmail.com>
 <fcdb7101-6f12-6814-d069-f7841e35cc11@treenet.co.nz>
Message-ID: <3ED445B2-F5AC-4DD2-A024-42015F26B10D@gmail.com>

A non-text attachment was scrubbed...
Name: squid.conf
Type: application/octet-stream
Size: 13414 bytes
Desc: not available
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20180112/aeb1b6b0/attachment.obj>
-------------- next part --------------

The user ynieves is member of ad groups ?internet?, ?socialNetwork?, ?youtube? and ?moderadoresSocNet"

Thanks.

Yoinier Hernandez Nieves.

> El 11/01/2018, a las 10:47 a.m., Amos Jeffries <squid3 at treenet.co.nz> escribi?:
> 
> On 12/01/18 03:24, Yoinier Hernandez Nieves wrote:
>>> El 11/01/2018, a las 12:46 a.m., Amos Jeffries escribi?:
>>> 
>>> On 11/01/18 09:33, Yoinier Hernandez Nieves wrote:
>>>> I try connect direct to the proxy, and this is the result
>>>> 1515616366.189   1359 aaa.aaa.aaa.aaa TAG_NONE/200 0 CONNECT www.ssllabs.com:443 <http://www.ssllabs.com:443> ynieves HIER_DIRECT/64.41.200.100 -
>>>> 1515616366.207      0 aaa.aaa.aaa.aaa TCP_DENIED/403 4419 GET https://www.ssllabs.com/ssltest/viewMyClient.html ynieves HIER_NONE/- text/html
>>>> 1515616366.244      0 aaa.aaa.aaa.aaa TAG_NONE/503 4914 GET http://artemisa.conalza.co.cu:3128/squid-internal-static/icons/SN.png ynieves HIER_DIRECT/64.41.200.100 text/html
>>>> How I can fix this.??
>>> 
>>> 
>>> What exactly do you think needs "fixing? ?
>> I need fix the problem with the auth failure.
>> Hi say:
>> Sorry, you are not currently allowed to request https://www.google.com/search? from this cache until you have authenticated yourself.
>> But I stay authenticated, see the log, user, ynieves.
> 
> Then something in your squid.conf is forbidding username ynieves access to use the proxy and defining that other username might be allowed. But it provides that info far too late to re-authenticate the already finished CONNECT message with usable credentials.
> 
> Please post *all* of your squid.conf settings so we can look in places you might not have expected to find auth relationships. Just exclude empty lines and # comments.
> 
> 
> Amos
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users


From brian at interlinx.bc.ca  Fri Jan 12 14:52:00 2018
From: brian at interlinx.bc.ca (Brian J. Murrell)
Date: Fri, 12 Jan 2018 09:52:00 -0500
Subject: [squid-users] persistent connections not being utilized with Chrome
Message-ID: <1515768720.6982.22.camel@interlinx.bc.ca>

I am noticing that my Squid 3.5.20 installation is not utilizing
persistent connections with a Chrome browser user.  My Squid
configuration is not disabling the default status of persistent
connections being enabled.

I can see Chrome including "Proxy-Connection: keep-alive" in it's
request and Squid responding with "Connection: keep-alive" but I'm only
seeing one request being processed in each of these connections.

Looking at the network traffic, it does look like it's Squid that it
closing the socket though.  It's the first one to send a FIN packet.

Some possibilities... are persistent connections not available for
CONNECT requests?  I don't believe that to be the case but just want to
confirm.  Or perhaps are persistent connections not available for
Negotiate'd requests?

Are there any other possible reasons that Squid would close down a
socket that appears to be opened persistently?

Cheers,
b.
-------------- next part --------------
A non-text attachment was scrubbed...
Name: signature.asc
Type: application/pgp-signature
Size: 488 bytes
Desc: This is a digitally signed message part
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20180112/97cec7d7/attachment.sig>

From robertocarna36 at gmail.com  Fri Jan 12 16:39:09 2018
From: robertocarna36 at gmail.com (Roberto Carna)
Date: Fri, 12 Jan 2018 13:39:09 -0300
Subject: [squid-users] Problem loading Facebook profiles
Message-ID: <CAG2Qp6vcXALJTcWe2RogmnZsFph35tp9yregbgmPE+=SOh_6fw@mail.gmail.com>

Dear, I have a Squid 3.5.27 proxy server with Squidguard in
transparent mode, for HTTP and HTTPS traffic.

Everything is OK, except when I access Facebook and select a profile
from any user: in this case, the profile load the name, picture, but
doesn't load the other data at all (pictures, friends, comments,
etc.).

I've tried to whitelist some hostname and IP's related to Facebook,
but it doesn't work.

Please can you help me in put to work Facebook through Squid in
transparent mode?

Special thanks,

Robert


From Antony.Stone at squid.open.source.it  Fri Jan 12 17:44:03 2018
From: Antony.Stone at squid.open.source.it (Antony Stone)
Date: Fri, 12 Jan 2018 18:44:03 +0100
Subject: [squid-users] Problem loading Facebook profiles
In-Reply-To: <CAG2Qp6vcXALJTcWe2RogmnZsFph35tp9yregbgmPE+=SOh_6fw@mail.gmail.com>
References: <CAG2Qp6vcXALJTcWe2RogmnZsFph35tp9yregbgmPE+=SOh_6fw@mail.gmail.com>
Message-ID: <201801121844.04087.Antony.Stone@squid.open.source.it>

On Friday 12 January 2018 at 17:39:09, Roberto Carna wrote:

> Dear, I have a Squid 3.5.27 proxy server with Squidguard in
> transparent mode, for HTTP and HTTPS traffic.

Please show us the Squid configuration file (omitting comments and blank lines) 
- that will show us what you've asked Squid to do.

> Everything is OK, except when I access Facebook and select a profile
> from any user: in this case, the profile load the name, picture, but
> doesn't load the other data at all (pictures, friends, comments,
> etc.).
> 
> I've tried to whitelist some hostname and IP's related to Facebook,
> but it doesn't work.
> 
> Please can you help me in put to work Facebook through Squid in
> transparent mode?

Show us what turns up in your Squid access log and your Squidguard blocking 
log (sorry, I'm not familiar with what this might properly be called, but I 
assume Squidguard has a log file showing which requests it has blocked) when 
such a request gets made.

Please try to:

a) restrict what you show us to only the user / IP making the request (no need 
to include other users' requests going through Squid at the same time)

b) include *everything* relating to that user, even if you think something 
might not be relevant

c) copy and paste the text from the log files, don't send a screenshot (osrry 
if that's obvious, but you might be surprised what some people do)


That will give us some idea of what's working and what isn't.


Regards,


Antony.

-- 
"It is easy to be blinded to the essential uselessness of them by the sense of 
achievement you get from getting them to work at all. In other words - and 
this is the rock solid principle on which the whole of the Corporation's 
Galaxy-wide success is founded - their fundamental design flaws are completely 
hidden by their superficial design flaws."

 - Douglas Noel Adams

                                                   Please reply to the list;
                                                         please *don't* CC me.


From squid3 at treenet.co.nz  Sat Jan 13 00:15:12 2018
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sat, 13 Jan 2018 13:15:12 +1300
Subject: [squid-users] persistent connections not being utilized with
 Chrome
In-Reply-To: <1515768720.6982.22.camel@interlinx.bc.ca>
References: <1515768720.6982.22.camel@interlinx.bc.ca>
Message-ID: <0ff41e4d-0a10-1553-ce4a-77bc9aac488e@treenet.co.nz>

On 13/01/18 03:52, Brian J. Murrell wrote:
> I am noticing that my Squid 3.5.20 installation is not utilizing
> persistent connections with a Chrome browser user.  My Squid
> configuration is not disabling the default status of persistent
> connections being enabled.
> 
> I can see Chrome including "Proxy-Connection: keep-alive" in it's
> request and Squid responding with "Connection: keep-alive" but I'm only
> seeing one request being processed in each of these connections.
> 
> Looking at the network traffic, it does look like it's Squid that it
> closing the socket though.  It's the first one to send a FIN packet.
> 
> Some possibilities... are persistent connections not available for
> CONNECT requests?


What do you mean "not available for?

CONNECT can be pipelined after other requests on a persistent 
connection. But since it is a tunnel nothing can be pipelined after it.



>  I don't believe that to be the case but just want to
> confirm.  Or perhaps are persistent connections not available for
> Negotiate'd requests?

None of the things that can be negotiated save keep-alive itself have 
any relevance to persistence / pipelining. They all interact with it in 
various ways but none outright conflict.


> 
> Are there any other possible reasons that Squid would close down a
> socket that appears to be opened persistently?
> 

Yes, literally everything. Connections can be terminated for any reason 
- intentionally or not.

CONNECT tunnels specifically end when the server sends FIN to Squid or 
any type of I/O error occurs.


Amos


From brian at interlinx.bc.ca  Sat Jan 13 03:23:28 2018
From: brian at interlinx.bc.ca (Brian J. Murrell)
Date: Fri, 12 Jan 2018 22:23:28 -0500
Subject: [squid-users] persistent connections not being utilized with
	Chrome
In-Reply-To: <0ff41e4d-0a10-1553-ce4a-77bc9aac488e@treenet.co.nz>
References: <1515768720.6982.22.camel@interlinx.bc.ca>
 <0ff41e4d-0a10-1553-ce4a-77bc9aac488e@treenet.co.nz>
Message-ID: <1515813808.6982.61.camel@interlinx.bc.ca>

On Sat, 2018-01-13 at 13:15 +1300, Amos Jeffries wrote:
> 
> What do you mean "not available for?

I mean, will not actually result in a persistent connection -- a socket
that is reused for multiple HTTP transactions.  I suppose for CONNECT
it would mean either multiple CONNECTs within a single socket or one
CONNECT with multiple "GET/POST" type transactions within it.

> CONNECT can be pipelined after other requests on a persistent 
> connection. But since it is a tunnel nothing can be pipelined after
> it.

So, for the purposes of the WWW's current move towards all-https
websites, persistent connections (perhaps only with proxy servers?) are
becoming useless?

> CONNECT tunnels specifically end when the server sends FIN to Squid

So this seems to affirm my question above that in a world where all
websites are https, persistent connections are no more and we are back
to open-fetch-close for every single object on a webpage, yes?

The problem I am trying to solve here is that opening Chrome with, say,
a few hundred tabs open, seems to take about an hour for it to finally
fetch all of the pages while most sit spinning on "waiting for an
available socket" or "proxy tunnel", etc., for a long time, which is
probably due to Chrome's limit on the number of concurrent sockets it
will open to a single destination, including proxy servers.  I was
hoping persistent connections would reduce the socket setup/teardown
overhead of all of that.

Cheers,
b.
-------------- next part --------------
A non-text attachment was scrubbed...
Name: signature.asc
Type: application/pgp-signature
Size: 488 bytes
Desc: This is a digitally signed message part
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20180112/ce27f587/attachment.sig>

From rousskov at measurement-factory.com  Sat Jan 13 04:34:29 2018
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Fri, 12 Jan 2018 21:34:29 -0700
Subject: [squid-users] persistent connections not being utilized with
 Chrome
In-Reply-To: <1515813808.6982.61.camel@interlinx.bc.ca>
References: <1515768720.6982.22.camel@interlinx.bc.ca>
 <0ff41e4d-0a10-1553-ce4a-77bc9aac488e@treenet.co.nz>
 <1515813808.6982.61.camel@interlinx.bc.ca>
Message-ID: <9ca490f3-e813-3c36-0a0f-2ad8d86ce8d3@measurement-factory.com>

On 01/12/2018 08:23 PM, Brian J. Murrell wrote:

> I mean, will not actually result in a persistent connection -- a socket
> that is reused for multiple HTTP transactions. 

It is best to think of HTTP persistency as applying to an HTTP
connection rather than a TCP connection/socket. Without a proxy, your
definition works well. With a proxy, there are several different use
cases. The one you seem to be interested in is encrypted HTTP connection
through an explicit proxy. In that case, there are two HTTP connections
in play:

  1. An HTTP connection from the client to the origin server.
  2. An HTTP connection from the client to the proxy.

Both HTTP connections use the same TCP client connection/socket (there
is also another TCP server connection socket that HTTP connection #1
also uses).

The first HTTP connection can carry lots of transactions. It can easily
be and often is "persistent".

The second HTTP connection typically carries just one HTTP transaction
(a CONNECT request followed by a 200 OK response establishing a TCP
tunnel through the proxy), but that is OK -- it does not hurt
performance because the useful transactions utilize HTTP connection #1
(which lives inside the tunnel established by the HTTP connection #2).


> I suppose for CONNECT
> it would mean either multiple CONNECTs within a single socket or one
> CONNECT with multiple "GET/POST" type transactions within it.

Authentication and other corner cases aside, there can be at most one
CONNECT request per HTTP connection, but you do not care (much) about
_that_ HTTP connection. The other HTTP connection (the one inside the
CONNECT tunnel) can carry lots of requests, and those transactions
actually deliver "pages" to your browser.


> So, for the purposes of the WWW's current move towards all-https
> websites, persistent connections (perhaps only with proxy servers?) are
> becoming useless?

No, the optimization is still there as far as client-origin traffic is
concerned.


> The problem I am trying to solve here is that opening Chrome with, say,
> a few hundred tabs open, seems to take about an hour for it to finally
> fetch all of the pages while most sit spinning on "waiting for an
> available socket" or "proxy tunnel", etc., for a long time, which is
> probably due to Chrome's limit on the number of concurrent sockets it
> will open to a single destination, including proxy servers.  

Yes, probably something like that is happening.


> I was hoping persistent connections would reduce the socket
> setup/teardown overhead of all of that.

Perhaps they do? How many requests does Chrome send inside a CONNECT
tunnel through Squid, on average? If you bump CONNECT tunnels using
SslBump, then you can use Squid to measure persistency. If you do not
bump, then you should still be able to use Chrome developer tools to
measure persistency.

Origin server response delays rather than TCP handshakes may be your
primary bottleneck because Chrome probably does not pipeline and,
without pipelining, there can be at most one concurrent request per HTTP
connection. To improve throughput in your environment (without raising
the number of TCP connections that Chrome is allowed to open), you would
need to wait for HTTP/2 support. In HTTP/2, a single HTTP connection can
carry lots of concurrent transactions.


HTH,

Alex.


From squid3 at treenet.co.nz  Sat Jan 13 18:28:00 2018
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sun, 14 Jan 2018 07:28:00 +1300
Subject: [squid-users] Squid and SSL Bump
In-Reply-To: <3ED445B2-F5AC-4DD2-A024-42015F26B10D@gmail.com>
References: <C735C3D2-8F2C-4967-8C96-56F9CBEFBC6B@gmail.com>
 <201801092227.57945.Antony.Stone@squid.open.source.it>
 <E6B47F15-D6FA-4655-9C46-09AC96521122@gmail.com>
 <96a99b08-b3c8-52e5-17bf-575a75a4c3c7@treenet.co.nz>
 <EA144D48-E922-435F-8203-F72CBEE21D6F@gmail.com>
 <c5bb1474-c31a-a3ba-ca72-cdcf25d3eddc@treenet.co.nz>
 <4FC5E06B-32FF-4A45-BD50-F4AC9236B45F@gmail.com>
 <fcdb7101-6f12-6814-d069-f7841e35cc11@treenet.co.nz>
 <3ED445B2-F5AC-4DD2-A024-42015F26B10D@gmail.com>
Message-ID: <c65e12d9-ea2b-a369-fd76-1440975da1fe@treenet.co.nz>

On 13/01/18 02:00, Yoinier Hernandez Nieves wrote:
> 
> The user ynieves is member of ad groups ?internet?, ?socialNetwork?, ?youtube? and ?moderadoresSocNet"
> 

So most of your http_access lines end with group checks. That could be a 
problem later. Right now its not clear which would be rejecting with 
that auth message, and the status being 403 indicates a hard failure 
rather than re-auth.


I suggest doing the usual thing of placing a single "http_access deny 
!users" line first, then appending " all" to the lines that normally end 
with a group check.

Like:

   http_access deny !users

   http_access allow cubaDomains cubaPC all
   http_access allow cubaDomains national all
   http_access allow cubaDomains internet all
   http_access deny SQUISHED1 all

   http_access allow socialDomains moderadoresSocNet all
   http_access allow socialTime socialDomains socialNetwork all
   http_access allow socialTime youtubeDomains youtuber all


For the delay pools there is no need to re-authenticate at all. Use the 
"note" ACL type to check that a username exists. Like so:

   acl loggedIn note user .

   delay_access 2 allow loggedIn workTime \
     !extDownloads !extDocuments !delaysFree


Also, the pool using only "-1/-1" as its paremeters should be removed. 
Squid links multiple pools to a transaction, so it is not doing what you 
think it does. To make certain transactions unlimited simply deny them 
being added to the other pools. That will also make your existing rules 
much simpler:

   denya_access 2 deny delaysFree
   delay_access 2 allow loggedIn workTime !extDownloads !extDocuments !
   delay_access 2 deny all


Also, your media and mediapr checks are slow regex tests. They should be 
placed after the default security checks.


If the problem remains after all the above changes are made you will 
need to track down what is generating the error page using cache.log 
trace with "debug_options ALL,5".

Amos


From chip_pop at hotmail.com  Sun Jan 14 14:23:54 2018
From: chip_pop at hotmail.com (joseph)
Date: Sun, 14 Jan 2018 07:23:54 -0700 (MST)
Subject: [squid-users] force full download and limit bandwith
Message-ID: <1515939834055-0.post@n4.nabble.com>

if the client   exit download  squid will force full speed on obj dose not
stay  at that rate

acl limit_ext_bandwith url_regex \.esd \.exe
delay_pools 1 
delay_class 1 1 
delay_parameters 1 128000/128000 
delay_access 1 allow limit_ext_bandwith 
delay_access 1 deny all 

range_offset_limit -1 limit_ext_bandwith
quick_abort_min -1 
quick_abort_max 0 KB 
quick_abort_pct 100 

this work fine  it keep downloading 128KB   until the client exit squid will
continue at full speed of the main bandwidth
is it bug  or im missing something ??
so i need to force cache full obj  at that specific rate  even if the client
exit 
tks



-----
************************** 
***** Crash to the future  ****
**************************
--
Sent from: http://squid-web-proxy-cache.1019090.n4.nabble.com/Squid-Users-f1019091.html


From squid3 at treenet.co.nz  Sun Jan 14 14:59:32 2018
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Mon, 15 Jan 2018 03:59:32 +1300
Subject: [squid-users] force full download and limit bandwith
In-Reply-To: <1515939834055-0.post@n4.nabble.com>
References: <1515939834055-0.post@n4.nabble.com>
Message-ID: <70d882f2-34b6-ca12-cb82-f19be79c0610@treenet.co.nz>

On 15/01/18 03:23, joseph wrote:
> if the client   exit download  squid will force full speed on obj dose not
> stay  at that rate
> 
> acl limit_ext_bandwith url_regex \.esd \.exe
> delay_pools 1
> delay_class 1 1
> delay_parameters 1 128000/128000
> delay_access 1 allow limit_ext_bandwith
> delay_access 1 deny all
> 
> range_offset_limit -1 limit_ext_bandwith
> quick_abort_min -1
> quick_abort_max 0 KB
> quick_abort_pct 100
> 
> this work fine  it keep downloading 128KB   until the client exit squid will
> continue at full speed of the main bandwidth
> is it bug  or im missing something ??

No bug, that is how delay pools are designed. Squid always fetches as 
fast as it can so as to finish with the more expensive network resource 
(port numbers) faster and free them up for other traffic sooner.


> so i need to force cache full obj  at that specific rate  even if the client
> exit

Use QoS controls in your operating system for that. Delay pools in Squid 
were designed long before QoS existed. Modern TOS/QoS capabilities are 
much better than Squid can do.

Amos


From xeron.oskom at gmail.com  Mon Jan 15 05:53:33 2018
From: xeron.oskom at gmail.com (Ivan Larionov)
Date: Sun, 14 Jan 2018 21:53:33 -0800
Subject: [squid-users] squid doesn't cache objects in memory when using SMP
 and shared memory cache
Message-ID: <CAHvB88yP0f5f0MzJ66RFiQFYY2Z4chKi4akeKtUh4qE4J9DJAQ@mail.gmail.com>

Hello!

After migrating squid from non-SMP/aufs to SMP/rock memory cache hit ratio
dropped significantly. Like from 50-100% to 1-5%. And disk cache hit ratio
went up from 15-50% to stable 60-65%. From the brief log file check it
looks like in SMP/rock mode squid avoids using memory for small files like
1-3KB but uses it for 10KB+ files.

I started tracking down the issue with disabling disk cache completely and
it didn't change anything, I just started to get MISS every time for the
URL which was getting MEM_HIT with an old configuration. Then I changed
"workers 2" to "workers 1" and started getting memory hits as before.

So it seems like the issue is with shared memory:

When squid doesn't use shared memory it works as expected. Even with
multiple workers.
When squid uses shared memory it caches very small amount of objects.

Am I doing anything wrong? Which debug options should I enable to provide
more information if it seems like a bug?

Config diff:

--- squid.conf.old  2018-01-14 02:01:19.000000000 -0800
+++ squid.conf.new  2018-01-14 02:01:16.000000000 -0800
@@ -1,5 +1,8 @@
 http_port 0.0.0.0:3128

+workers 2
+cpu_affinity_map process_numbers=1,2 cores=1,2
+
 acl localnet src 10.0.0.0/8     # RFC1918 possible internal network
 acl localnet src 172.16.0.0/12  # RFC1918 possible internal network
 acl localnet src 192.168.0.0/16 # RFC1918 possible internal network
@@ -94,13 +97,12 @@

 never_direct allow all

-cache_mem 9420328 KB
-maximum_object_size_in_memory 32 KB
+cache_mem 12560438 KB
+maximum_object_size_in_memory 64 KB
 memory_replacement_policy heap LRU
 cache_replacement_policy heap LRU

-cache_dir aufs /mnt/services/squid/cache/cache0 261120 16 256
-cache_dir aufs /mnt/services/squid/cache/cache1 261120 16 256
+cache_dir rock /mnt/services/squid/cache 522240 swap-timeout=500
max-swap-rate=1200 slot-size=16384

 minimum_object_size 64 bytes # none-zero so we dont cache mistakes
 maximum_object_size 102400 KB


All relevant config options together (SMP/rock):

workers 2
cpu_affinity_map process_numbers=1,2 cores=1,2

cache_mem 12560438 KB
maximum_object_size_in_memory 64 KB
memory_replacement_policy heap LRU
cache_replacement_policy heap LRU

cache_dir rock /mnt/services/squid/cache 522240 swap-timeout=500
max-swap-rate=1200 slot-size=16384

minimum_object_size 64 bytes # none-zero so we dont cache mistakes
maximum_object_size 102400 KB

negative_ttl 0 seconds
range_offset_limit none


Squid version:

Squid Cache: Version 3.5.27
Service Name: squid
configure options:  '--program-prefix=' '--prefix=/usr'
'--exec-prefix=/usr' '--bindir=/usr/sbin' '--sbindir=/usr/sbin'
'--sysconfdir=/etc/squid' '--libdir=/usr/lib' '--libexecdir=/usr/lib/squid'
'--includedir=/usr/include' '--datadir=/usr/share'
'--sharedstatedir=/usr/com' '--localstatedir=/var'
'--mandir=/usr/share/man' '--infodir=/usr/share/info' '--enable-epoll'
'--enable-removal-policies=heap,lru' '--enable-storeio=aufs,rock'
'--enable-delay-pools' '--with-pthreads' '--enable-cache-digests'
'--with-large-files' '--with-maxfd=16384' '--enable-htcp'

-- 
With best regards, Ivan Larionov.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20180114/b306ade4/attachment.htm>

From squid3 at treenet.co.nz  Mon Jan 15 15:22:36 2018
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Tue, 16 Jan 2018 04:22:36 +1300
Subject: [squid-users] squid doesn't cache objects in memory when using
 SMP and shared memory cache
In-Reply-To: <CAHvB88yP0f5f0MzJ66RFiQFYY2Z4chKi4akeKtUh4qE4J9DJAQ@mail.gmail.com>
References: <CAHvB88yP0f5f0MzJ66RFiQFYY2Z4chKi4akeKtUh4qE4J9DJAQ@mail.gmail.com>
Message-ID: <cf49ca6f-fd92-ed4a-57be-76c5331045d9@treenet.co.nz>

On 15/01/18 18:53, Ivan Larionov wrote:
> Hello!
> 
> After migrating squid from non-SMP/aufs to SMP/rock memory cache hit 
> ratio dropped significantly. Like from 50-100% to 1-5%. And disk cache 
> hit ratio went up from 15-50% to stable 60-65%. From the brief log file 
> check it looks like in SMP/rock mode squid avoids using memory for small 
> files like 1-3KB but uses it for 10KB+ files.

AIUI, SMP-mode rock operates as a fully separate process (a "Disker" 
kid) which delivers its results as objects already in shared memory to 
the worker process.

There should be little or no gain from that promotion process anymore - 
which would only be moving the object between memory locations. In fact 
if cache_mem were not operating as shared memory even with SMP active 
(which is possible) the promotion would be an actively bad idea as it 
prevents other workers using the object in future.

They show up as non- MEM_HIT because they are either REFRESH or stored 
in the Disker shared memory instead of the cache_mem shared memory. The 
Squid logging is not quite up to recording the slim distinction between 
which of multiple memory areas are being used.


> 
> I started tracking down the issue with disabling disk cache completely 
> and it didn't change anything, I just started to get MISS every time for 
> the URL which was getting MEM_HIT with an old configuration. Then I 
> changed "workers 2" to "workers 1" and started getting memory hits as 
> before.
> 
> So it seems like the issue is with shared memory:
> 
> When squid doesn't use shared memory it works as expected. Even with 
> multiple workers.
> When squid uses shared memory it caches very small amount of objects.
> 
> Am I doing anything wrong? Which debug options should I enable to 
> provide more information if it seems like a bug?
> 

Are you seeing an actual performance difference? if not I would not 
worry about it.

FYI: if you really want to track this down I suggest using Squid-4 to do 
that. Squid-3 is very near the end of its support lifetime and changes 
of a deep nature do not have much chance at all of getting in there now.

Amos


From brian at interlinx.bc.ca  Mon Jan 15 15:40:30 2018
From: brian at interlinx.bc.ca (Brian J. Murrell)
Date: Mon, 15 Jan 2018 10:40:30 -0500
Subject: [squid-users] persistent connections not being utilized with
	Chrome
In-Reply-To: <9ca490f3-e813-3c36-0a0f-2ad8d86ce8d3@measurement-factory.com>
References: <1515768720.6982.22.camel@interlinx.bc.ca>
 <0ff41e4d-0a10-1553-ce4a-77bc9aac488e@treenet.co.nz>
 <1515813808.6982.61.camel@interlinx.bc.ca>
 <9ca490f3-e813-3c36-0a0f-2ad8d86ce8d3@measurement-factory.com>
Message-ID: <1516030830.24075.27.camel@interlinx.bc.ca>

On Fri, 2018-01-12 at 21:34 -0700, Alex Rousskov wrote:
> In that case, there are two HTTP
> connections
> in play:
> 
>   1. An HTTP connection from the client to the origin server.

By this do you mean to say there is a connection from the client,
through the proxy server to the origin server?

>   2. An HTTP connection from the client to the proxy.
> 
> Both HTTP connections use the same TCP client connection/socket 

Understood.  So I do believe you are ACKing my question above.

> No, the optimization is still there as far as client-origin traffic
> is
> concerned.

Except that it is all bottle-necked through the same open-TCP-socket
limitations that Chrome has to a single destination.  I think what I
want to see is those limited number of TCP-sockets better utilized. 
But maybe that cannot happen without pipelining.

> Yes, probably something like that is happening.

So how do I ameliorate it?

> Perhaps they do? How many requests does Chrome send inside a CONNECT
> tunnel through Squid, on average?

My short investigation using packet sniffing seems to indicate just
one.

> If you bump CONNECT tunnels using
> SslBump, then you can use Squid to measure persistency. If you do not
> bump, then you should still be able to use Chrome developer tools to
> measure persistency.

Any clues about how do to do that?

> Origin server response delays rather than TCP handshakes may be your
> primary bottleneck because Chrome probably does not pipeline and,
> without pipelining, there can be at most one concurrent request per
> HTTP
> connection.

I think Chrome disabled pipelining a while back:

https://stackoverflow.com/questions/30477476/why-is-pipelining-disabled-in-modern-browsers

> To improve throughput in your environment (without raising
> the number of TCP connections that Chrome is allowed to open), you
> would
> need to wait for HTTP/2 support. In HTTP/2, a single HTTP connection
> can
> carry lots of concurrent transactions.

So are people without proxies suffering this same issue?  I don't think
they are because their few hundred tabs will all be much more
distributed to various servers and domains across the Internet allowing
their Chrome to open many (many!) more parallel TCP connections and
wait for them all to respond in parallel.

It's the concentration of all of those potential TCP connections
through a single host -- the proxy server -- that is greatly reducing
the parallelism of fetching lots of objects at the same time and
dragging it's wall-clock time out.

Perhaps there is no solution until HTTP/2.

I just find it surprising that every IT person that utilizes a proxy
has to tell their users, "yeah, that's just how it is here in this
network, very slow to start up your browser".  :-(

Cheers,
b.
-------------- next part --------------
A non-text attachment was scrubbed...
Name: signature.asc
Type: application/pgp-signature
Size: 488 bytes
Desc: This is a digitally signed message part
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20180115/30ca04d4/attachment.sig>

From rafael.akchurin at diladele.com  Mon Jan 15 16:06:34 2018
From: rafael.akchurin at diladele.com (Rafael Akchurin)
Date: Mon, 15 Jan 2018 16:06:34 +0000
Subject: [squid-users] persistent connections not being utilized
	with	Chrome
In-Reply-To: <1516030830.24075.27.camel@interlinx.bc.ca>
References: <1515768720.6982.22.camel@interlinx.bc.ca>
 <0ff41e4d-0a10-1553-ce4a-77bc9aac488e@treenet.co.nz>
 <1515813808.6982.61.camel@interlinx.bc.ca>
 <9ca490f3-e813-3c36-0a0f-2ad8d86ce8d3@measurement-factory.com>
 <1516030830.24075.27.camel@interlinx.bc.ca>
Message-ID: <AM4PR0401MB219422D71D0B702D672220698FEB0@AM4PR0401MB2194.eurprd04.prod.outlook.com>

Hello Brian,

Sorry not to flame it all out further - but I see the same annoying "waiting for proxy tunnel" in Chrome through SSL bumping AD integrated explicit Squid.
*but* the same 200 of tabs loads just fine from FF and the same Squid on the same machine at the same time - so might be a Chrome issue/architecture?

Best regards,
Rafael Akchurin
Diladele B.V.


-----Original Message-----
From: squid-users [mailto:squid-users-bounces at lists.squid-cache.org] On Behalf Of Brian J. Murrell
Sent: Monday, January 15, 2018 4:41 PM
To: squid-users at lists.squid-cache.org
Subject: Re: [squid-users] persistent connections not being utilized with Chrome

On Fri, 2018-01-12 at 21:34 -0700, Alex Rousskov wrote:
> In that case, there are two HTTP
> connections
> in play:
> 
>   1. An HTTP connection from the client to the origin server.

By this do you mean to say there is a connection from the client, through the proxy server to the origin server?

>   2. An HTTP connection from the client to the proxy.
> 
> Both HTTP connections use the same TCP client connection/socket

Understood.  So I do believe you are ACKing my question above.

> No, the optimization is still there as far as client-origin traffic is 
> concerned.

Except that it is all bottle-necked through the same open-TCP-socket limitations that Chrome has to a single destination.  I think what I want to see is those limited number of TCP-sockets better utilized. 
But maybe that cannot happen without pipelining.

> Yes, probably something like that is happening.

So how do I ameliorate it?

> Perhaps they do? How many requests does Chrome send inside a CONNECT 
> tunnel through Squid, on average?

My short investigation using packet sniffing seems to indicate just one.

> If you bump CONNECT tunnels using
> SslBump, then you can use Squid to measure persistency. If you do not 
> bump, then you should still be able to use Chrome developer tools to 
> measure persistency.

Any clues about how do to do that?

> Origin server response delays rather than TCP handshakes may be your 
> primary bottleneck because Chrome probably does not pipeline and, 
> without pipelining, there can be at most one concurrent request per 
> HTTP connection.

I think Chrome disabled pipelining a while back:

https://stackoverflow.com/questions/30477476/why-is-pipelining-disabled-in-modern-browsers

> To improve throughput in your environment (without raising the number 
> of TCP connections that Chrome is allowed to open), you would need to 
> wait for HTTP/2 support. In HTTP/2, a single HTTP connection can carry 
> lots of concurrent transactions.

So are people without proxies suffering this same issue?  I don't think they are because their few hundred tabs will all be much more distributed to various servers and domains across the Internet allowing their Chrome to open many (many!) more parallel TCP connections and wait for them all to respond in parallel.

It's the concentration of all of those potential TCP connections through a single host -- the proxy server -- that is greatly reducing the parallelism of fetching lots of objects at the same time and dragging it's wall-clock time out.

Perhaps there is no solution until HTTP/2.

I just find it surprising that every IT person that utilizes a proxy has to tell their users, "yeah, that's just how it is here in this network, very slow to start up your browser".  :-(

Cheers,
b.

From bruce.pennypacker at gmail.com  Mon Jan 15 16:26:36 2018
From: bruce.pennypacker at gmail.com (Bruce Pennypacker)
Date: Mon, 15 Jan 2018 11:26:36 -0500
Subject: [squid-users] Logging PROXY Protocol header
Message-ID: <0f7405df-84d2-7009-73fd-5e5cbc513c34@gmail.com>

Is it possible to configure Squid to log the details of the PROXY 
protocol when using it? We're running Squid 3.5.20 in AWS behind a TCP 
load balancer, which supports forwarding the PROXY protocol header. I'd 
like to be able to include the client IP as provided in the PROXY 
protocol header, but I'd be happy to log the entire header as well if 
necessary. I've spent some time searching for information on this but 
haven't had any luck so far.

Thanks,

-Bruce




From brian at interlinx.bc.ca  Mon Jan 15 16:49:17 2018
From: brian at interlinx.bc.ca (Brian J. Murrell)
Date: Mon, 15 Jan 2018 11:49:17 -0500
Subject: [squid-users] persistent connections not being utilized
	with	Chrome
In-Reply-To: <AM4PR0401MB219422D71D0B702D672220698FEB0@AM4PR0401MB2194.eurprd04.prod.outlook.com>
References: <1515768720.6982.22.camel@interlinx.bc.ca>
 <0ff41e4d-0a10-1553-ce4a-77bc9aac488e@treenet.co.nz>
 <1515813808.6982.61.camel@interlinx.bc.ca>
 <9ca490f3-e813-3c36-0a0f-2ad8d86ce8d3@measurement-factory.com>
 <1516030830.24075.27.camel@interlinx.bc.ca>
 <AM4PR0401MB219422D71D0B702D672220698FEB0@AM4PR0401MB2194.eurprd04.prod.outlook.com>
Message-ID: <1516034957.24075.42.camel@interlinx.bc.ca>

On Mon, 2018-01-15 at 16:06 +0000, Rafael Akchurin wrote:
> Hello Brian,

Hi,

> *but* the same 200 of tabs loads just fine from FF and the same Squid
> on the same machine at the same time - so might be a Chrome
> issue/architecture?

Interesting.  I'm not sure how I would do it, but it would be
interesting for sure to somehow convert my saved Chrome "session" to a
Firefox session to see if this is the case for me also.

If so, it would make a good bug report to the Chrome devs.

Cheers,
b.
-------------- next part --------------
A non-text attachment was scrubbed...
Name: signature.asc
Type: application/pgp-signature
Size: 488 bytes
Desc: This is a digitally signed message part
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20180115/ffa28851/attachment.sig>

From rousskov at measurement-factory.com  Mon Jan 15 17:56:45 2018
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Mon, 15 Jan 2018 10:56:45 -0700
Subject: [squid-users] persistent connections not being utilized with
 Chrome
In-Reply-To: <1516030830.24075.27.camel@interlinx.bc.ca>
References: <1515768720.6982.22.camel@interlinx.bc.ca>
 <0ff41e4d-0a10-1553-ce4a-77bc9aac488e@treenet.co.nz>
 <1515813808.6982.61.camel@interlinx.bc.ca>
 <9ca490f3-e813-3c36-0a0f-2ad8d86ce8d3@measurement-factory.com>
 <1516030830.24075.27.camel@interlinx.bc.ca>
Message-ID: <74eba86c-ef4c-9bf8-f65b-e8098c9ee8c0@measurement-factory.com>

On 01/15/2018 08:40 AM, Brian J. Murrell wrote:
> On Fri, 2018-01-12 at 21:34 -0700, Alex Rousskov wrote:
>> In that case, there are two HTTP connections in play:
>>
>>   1. An HTTP connection from the client to the origin server.


> By this do you mean to say there is a connection from the client,
> through the proxy server to the origin server?

No, I do not mean to say that. From HTTP point of view, that first HTTP
connection is from client to the origin server. The fact that a proxy
forwards those raw bytes is irrelevant in this context: You are not
saying "a connection from the client, through the WiFi router, through
the cat5e cable, through the Ethernet switch, through the proxy, through
the fiber optic cable, through ..., to the origin server, do you?
(Rhetorical).

If your Squid bumps the CONNECT tunnel, then Squid may interfere with
what happens on that connection, and, hence, Squid presence becomes
important. Otherwise, it is all between a browser and an origin server.
My classification is based on the classic/standard layout when Squid
does not perform a MitM attack. It is easier to understand persistency
on that level and then add exceptions for SslBump, etc. as/if needed.


>>   2. An HTTP connection from the client to the proxy.
>>
>> Both HTTP connections use the same TCP client connection/socket 


>> No, the optimization is still there as far as client-origin traffic
>> is concerned.

> Except that it is all bottle-necked through the same open-TCP-socket
> limitations that Chrome has to a single destination.  

Yes, but understanding where the bottleneck(s) are is obviously
important. There are currently two very different suspects discussed on
this thread:

* A limit on the number of TCP connections a browser can open.

* A limit on the number of raw HTTP transactions a browser can perform
for each TCP connection opened.

The HTTP persistency affects the latter but not the former. You asked
about persistency, and I tried to explain how it works in your context
so that you can distinguish between the two suspects above. It is
possible that there are two bottlenecks (i.e., removing only one of them
is not going to significantly improve your situation).


>> Yes, probably something like that is happening.
> 
> So how do I ameliorate it?

1. Configure your browser to open more TCP connections. If the current
limit is specific to using a forward proxy, consider complaining to
browser developers and using a transparent proxy.

2. If your Squid uses SslBump, investigate whether bumped HTTP
connections between the client and the origin server are needlessly
closed by Squid.

3. Check whether Chrome supports HTTP/2 proxies and, if it does,
facilitate HTTP/2 development in Squid.


>> Perhaps they do? How many requests does Chrome send inside a CONNECT
>> tunnel through Squid, on average?

> My short investigation using packet sniffing seems to indicate just
> one.

Just to double check: You only saw a single HTTP GET (or similar)
request inside most CONNECT tunnels? If yes, did the response indicate
the desire to keep the connection open? If yes, which TCP connection was
closed first and by which side? Client-Squid or Squid-origin?


>> If you bump CONNECT tunnels using
>> SslBump, then you can use Squid to measure persistency. If you do not
>> bump, then you should still be able to use Chrome developer tools to
>> measure persistency.

> Any clues about how do to do that?

I cannot detail the steps right now but others on this list might be
able to guide you. To improve your chances, you may want to specify
which of the two scenarios apply to you (and what you have tried already).


> I just find it surprising that every IT person that utilizes a proxy
> has to tell their users, "yeah, that's just how it is here in this
> network, very slow to start up your browser".  :-(

I speculate that most users do not have a few hundred browser tabs.

Alex.


From rousskov at measurement-factory.com  Mon Jan 15 18:26:22 2018
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Mon, 15 Jan 2018 11:26:22 -0700
Subject: [squid-users] squid doesn't cache objects in memory when using
 SMP and shared memory cache
In-Reply-To: <CAHvB88yP0f5f0MzJ66RFiQFYY2Z4chKi4akeKtUh4qE4J9DJAQ@mail.gmail.com>
References: <CAHvB88yP0f5f0MzJ66RFiQFYY2Z4chKi4akeKtUh4qE4J9DJAQ@mail.gmail.com>
Message-ID: <e00d90e5-4521-4cdc-177d-284a21e3c6f0@measurement-factory.com>

On 01/14/2018 10:53 PM, Ivan Larionov wrote:

> After migrating squid from non-SMP/aufs to SMP/rock memory cache hit
> ratio dropped significantly. Like from 50-100% to 1-5%. 

This could be a side effect of not supporting Vary caching in shared
memory: https://bugs.squid-cache.org/show_bug.cgi?id=3806#c9


> And disk cache hit ratio went up from 15-50% to stable 60-65%. 

I hope your total/combined hit ratio improved overall.


> it looks like in SMP/rock mode squid avoids using memory for small
> files like 1-3KB but uses it for 10KB+ files.

No, there is no such size-discrimination code in Squid.


> I started tracking down the issue with disabling disk cache completely
> and it didn't change anything, I just started to get MISS every time for
> the URL which was getting MEM_HIT with an old configuration. Then I
> changed "workers 2" to "workers 1" and started getting memory hits as
> before.

For a clean apples-to-apples test, make sure you use
"memory_cache_shared on" when using a single worker without rock cache_dirs.


> Am I doing anything wrong? Which debug options should I enable to
> provide more information if it seems like a bug?


Vary caching should be fixed as well, of course, but perhaps there is
another problem we do not know about. I would start by eliminating Vary
as the known problem. When using a test transaction, make sure the
response does not have a Vary header. Or configure Squid to log the Vary
header and remove the corresponding transactions when computing
adjusted-for-Vary memory cache hit ratio.


HTH,

Alex.


From brian at interlinx.bc.ca  Mon Jan 15 18:32:55 2018
From: brian at interlinx.bc.ca (Brian J. Murrell)
Date: Mon, 15 Jan 2018 13:32:55 -0500
Subject: [squid-users] persistent connections not being utilized with
	Chrome
In-Reply-To: <74eba86c-ef4c-9bf8-f65b-e8098c9ee8c0@measurement-factory.com>
References: <1515768720.6982.22.camel@interlinx.bc.ca>
 <0ff41e4d-0a10-1553-ce4a-77bc9aac488e@treenet.co.nz>
 <1515813808.6982.61.camel@interlinx.bc.ca>
 <9ca490f3-e813-3c36-0a0f-2ad8d86ce8d3@measurement-factory.com>
 <1516030830.24075.27.camel@interlinx.bc.ca>
 <74eba86c-ef4c-9bf8-f65b-e8098c9ee8c0@measurement-factory.com>
Message-ID: <1516041175.24075.61.camel@interlinx.bc.ca>

On Mon, 2018-01-15 at 10:56 -0700, Alex Rousskov wrote:
> On 01/15/2018 08:40 AM, Brian J. Murrell wrote:
> > On Fri, 2018-01-12 at 21:34 -0700, Alex Rousskov wrote:
> > > In that case, there are two HTTP connections in play:
> > > 
> > >   1. An HTTP connection from the client to the origin 
> > > server.
> 
> 
> > By this do you mean to say there is a connection from the client,
> > through the proxy server to the origin server?
> 
> No, I do not mean to say that. From HTTP point of view, that first
> HTTP
> connection is from client to the origin server.

Well, to my reading, to omit the clarification that the connection is
through the proxy server means that the connection is directly from the
client to origin server, bypassing the proxy server.

> The fact that a proxy
> forwards those raw bytes is irrelevant in this context: You are not
> saying "a connection from the client, through the WiFi router,
> through
> the cat5e cable, through the Ethernet switch, through the proxy,
> through
> the fiber optic cable, through ..., to the origin server, do you?

No.  Of course I don't name every component which is common to most/all
cases.  But a proxy server is not always in the path so it is worth
spelling out if you do actually mean a connection through the proxy. 
Because as I said above, for most people, there is no proxy involved
and so "client to the origin server" could very well mean a single
socket directly between the two.

It is also important to specify because it implies multiple TCP
connection between the client and the origin server and not just one
and it also implies a single, common IP address to which the browser
connects to get to the origin server, not the origin server's IP
address.  These are all very important factors in this situation.

> If your Squid bumps the CONNECT tunnel,

It does not.

> then Squid may interfere with
> what happens on that connection, and, hence, Squid presence becomes
> important. Otherwise, it is all between a browser and an origin
> server.

Except that it's not.  At the TCP socket level it's between the browser
and the proxy server and this is an important factor.

> Yes, but understanding where the bottleneck(s) are is obviously
> important. There are currently two very different suspects discussed
> on
> this thread:
> 
> * A limit on the number of TCP connections a browser can open.

Chrome has a fixed (unconfigurable) limit on the number of connections
it will open to a server and I don't believe it provides for any
distinction whether that server is an origin server or a proxy server. 
If I am wrong about this latter aspect, then much of this goes out the
window.

This limitation is what is important here.  Because in a non-proxied
world, the browser can hit this maximum many many times over, once for
each origin server.  But in a proxied world, it can only hit this
maximum once, for the proxy server.  For 10 tabs, open to 10 different
websites, this can mean that without a proxy, it can open 60
connections, vastly parallelizing the fetching of those pages.  But
when a proxy is involved it is limited to opening 6 connections and
trying to fetch all of those pages through those 6 instead of the 60 it
can use without a proxy.

> 1. Configure your browser to open more TCP connections.

I don't believe that is possible with Chrome.

> If the
> current
> limit is specific to using a forward proxy, consider complaining to
> browser developers and using a transparent proxy.

Not a viable solution.  Yes, I could complain, but my users don't
really care that I have complained.  They are still stuck with massive
browser startup times.

> Just to double check: You only saw a single HTTP GET (or similar)
> request inside most CONNECT tunnels?

I cannot look inside CONNECT tunnels.  I don't MitM my users' SSL
connections.  But I did see only one CONNECT per TCP connection to the
proxy server from the Chrome browser.

> If yes, did the response indicate
> the desire to keep the connection open?

Yes.

> If yes, which TCP connection was
> closed first and by which side? Client-Squid or Squid-origin?

Squid closed the connection to origin first.

> I speculate that most users do not have a few hundred browser tabs.

Perhaps that is the case.

b.
-------------- next part --------------
A non-text attachment was scrubbed...
Name: signature.asc
Type: application/pgp-signature
Size: 488 bytes
Desc: This is a digitally signed message part
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20180115/718cc488/attachment.sig>

From squid3 at treenet.co.nz  Mon Jan 15 18:48:08 2018
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Tue, 16 Jan 2018 07:48:08 +1300
Subject: [squid-users] Logging PROXY Protocol header
In-Reply-To: <0f7405df-84d2-7009-73fd-5e5cbc513c34@gmail.com>
References: <0f7405df-84d2-7009-73fd-5e5cbc513c34@gmail.com>
Message-ID: <81c71c0f-7827-c1c6-f975-69ca77fbdba8@treenet.co.nz>

On 16/01/18 05:26, Bruce R wrote:
> Is it possible to configure Squid to log the details of the PROXY 
> protocol when using it? We're running Squid 3.5.20 in AWS behind a TCP 
> load balancer, which supports forwarding the PROXY protocol header. I'd 
> like to be able to include the client IP as provided in the PROXY 
> protocol header, but I'd be happy to log the entire header as well if 
> necessary. I've spent some time searching for information on this but 
> haven't had any luck so far.

When the PROXY protocol is received the details it supplies replace the 
TCP connection supplied values. That means everything in Squid dealing 
with client-IP or port displays or uses the PROXY values.

In squid.conf add the option "require-proxy-header" on the http_port you 
are receiving traffic from the LB. It is then important that you prevent 
traffic arriving from anywhere else than trusted sources. It is left to 
you to configure your firewall appropriately.


If you really want to see PROXY happening it is recorded in cache.log 
with "debug_options 33,5"

Amos


From squid3 at treenet.co.nz  Mon Jan 15 18:55:28 2018
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Tue, 16 Jan 2018 07:55:28 +1300
Subject: [squid-users] persistent connections not being utilized with
 Chrome
In-Reply-To: <1516041175.24075.61.camel@interlinx.bc.ca>
References: <1515768720.6982.22.camel@interlinx.bc.ca>
 <0ff41e4d-0a10-1553-ce4a-77bc9aac488e@treenet.co.nz>
 <1515813808.6982.61.camel@interlinx.bc.ca>
 <9ca490f3-e813-3c36-0a0f-2ad8d86ce8d3@measurement-factory.com>
 <1516030830.24075.27.camel@interlinx.bc.ca>
 <74eba86c-ef4c-9bf8-f65b-e8098c9ee8c0@measurement-factory.com>
 <1516041175.24075.61.camel@interlinx.bc.ca>
Message-ID: <49d95bf0-2a2e-17bc-6c26-05a24a8b30d1@treenet.co.nz>

On 16/01/18 07:32, Brian J. Murrell wrote:
> On Mon, 2018-01-15 at 10:56 -0700, Alex Rousskov wrote:
>> On 01/15/2018 08:40 AM, Brian J. Murrell wrote:
>>> On Fri, 2018-01-12 at 21:34 -0700, Alex Rousskov wrote:
>>>> In that case, there are two HTTP connections in play:
>>>>
>>>>    1. An HTTP connection from the client to the origin
>>>> server.
>>
>>
>>> By this do you mean to say there is a connection from the client,
>>> through the proxy server to the origin server?
>>
>> No, I do not mean to say that. From HTTP point of view, that first
>> HTTP
>> connection is from client to the origin server.
> 
> Well, to my reading, to omit the clarification that the connection is
> through the proxy server means that the connection is directly from the
> client to origin server, bypassing the proxy server.
> 


Yes, it is. For values of "bypassing" which includes packets going 
through the proxy ... as opaque bytes being blindly relayed.

The proxy has no more relevance or interactions than your network router 
to the tunneled traffic.


Amos


From Brian.Snyder at beavercreek.k12.oh.us  Mon Jan 15 18:56:41 2018
From: Brian.Snyder at beavercreek.k12.oh.us (Snyder, Brian)
Date: Mon, 15 Jan 2018 18:56:41 +0000
Subject: [squid-users] Performance
In-Reply-To: <9902c898-d7c3-95d2-1d1b-351605b8bd2d@measurement-factory.com>
References: <D2E66EB8081A7D4C86314759C29AAFE043353EE5@BHSMSX03>,
 <9902c898-d7c3-95d2-1d1b-351605b8bd2d@measurement-factory.com>
Message-ID: <2F8ABA7B-25FF-49A3-8E20-F32C3C049691@beavercreek.k12.oh.us>

Thank you for your reply. I have not found an issue the hardware. Atop shows everything in normal ranges. I do know the squid is about 50% faster with our filter set up as a parent vs routing through it normally. Not sure why that would be. However, when I set it up as a parent I notice quite a few connect to parent ip failed. There does not seem to be a network issue.

Thanks,
Brian

On Jan 11, 2018, at 2:23 PM, Alex Rousskov <rousskov at measurement-factory.com<mailto:rousskov at measurement-factory.com>> wrote:

On 01/11/2018 10:14 AM, Snyder, Brian wrote:

I apologise for asking another squid performance question,

There is nothing wrong with that! It is often very difficult to solve
performance problems on the mailing list, but that does not imply folks
should not ask performance questions.


over time it slows down significantly.

In case nobody looks for or finds problems in your configuration: What
is the bottleneck? CPU? RAM? Disk I/O? NIC interrupts?

A tool like atop may be able to answer that question for you, especially
if you let it collect stats from before Squid start to the time when the
Squid becomes very slow.

Alex.


From rousskov at measurement-factory.com  Mon Jan 15 18:57:26 2018
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Mon, 15 Jan 2018 11:57:26 -0700
Subject: [squid-users] squid doesn't cache objects in memory when using
 SMP and shared memory cache
In-Reply-To: <cf49ca6f-fd92-ed4a-57be-76c5331045d9@treenet.co.nz>
References: <CAHvB88yP0f5f0MzJ66RFiQFYY2Z4chKi4akeKtUh4qE4J9DJAQ@mail.gmail.com>
 <cf49ca6f-fd92-ed4a-57be-76c5331045d9@treenet.co.nz>
Message-ID: <44d80338-8589-4c6a-d545-43e047aa969c@measurement-factory.com>

On 01/15/2018 08:22 AM, Amos Jeffries wrote:

> AIUI, SMP-mode rock operates as a fully separate process (a "Disker" 
> kid) which delivers its results as objects already in shared memory
> to the worker process.

Yes, disk hits are delivered to workers via shared memory, but...


> There should be little or no gain from that promotion process anymore -

I disagree: A disk hit is still a lot slower than a memory hit because a
disk hit involves reading from a disk, which is a major performance
bottleneck.

The shared memory pages used to deliver disk hits to workers are then
reused for delivering other disk hits. Those pages are not a part of a
cache. Ideally, those pages could be added to (swapped with) the memory
cache pages (to avoid memory copying; when other conditions are met),
but Squid does not have the code to do that yet. We do not even have the
code to serve hits from shared memory -- we copy the bytes from shared
to local memory first.

Alex.


From rousskov at measurement-factory.com  Mon Jan 15 19:26:40 2018
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Mon, 15 Jan 2018 12:26:40 -0700
Subject: [squid-users] persistent connections not being utilized with
 Chrome
In-Reply-To: <1516041175.24075.61.camel@interlinx.bc.ca>
References: <1515768720.6982.22.camel@interlinx.bc.ca>
 <0ff41e4d-0a10-1553-ce4a-77bc9aac488e@treenet.co.nz>
 <1515813808.6982.61.camel@interlinx.bc.ca>
 <9ca490f3-e813-3c36-0a0f-2ad8d86ce8d3@measurement-factory.com>
 <1516030830.24075.27.camel@interlinx.bc.ca>
 <74eba86c-ef4c-9bf8-f65b-e8098c9ee8c0@measurement-factory.com>
 <1516041175.24075.61.camel@interlinx.bc.ca>
Message-ID: <68585dfd-dcb9-81e4-1298-29f3254279e6@measurement-factory.com>

On 01/15/2018 11:32 AM, Brian J. Murrell wrote:
> On Mon, 2018-01-15 at 10:56 -0700, Alex Rousskov wrote:
>> On 01/15/2018 08:40 AM, Brian J. Murrell wrote:
>>> On Fri, 2018-01-12 at 21:34 -0700, Alex Rousskov wrote:
>>>> In that case, there are two HTTP connections in play:
>>>>
>>>>   1. An HTTP connection from the client to the origin 
>>>> server.

>>> By this do you mean to say there is a connection from the client,
>>> through the proxy server to the origin server?

>> No, I do not mean to say that. From HTTP point of view, that first
>> HTTP connection is from client to the origin server.

> Well, to my reading, to omit the clarification that the connection is
> through the proxy server means that the connection is directly from the
> client to origin server, bypassing the proxy server.

The HTTP connection bypasses the proxy server indeed (via a TCP tunnel).
The TCP connectionS do not bypass, but my list was about HTTP, not TCP.


> ...  These are all very important factors in this situation.

Indeed they are! They are irrelevant for HTTP persistency point of view
though, and that is the (part of your) question I was trying to answer.
If we keep mixing TCP and HTTP together, it would be difficult to
understand what is happening and what should be happening.


>> If your Squid bumps the CONNECT tunnel,

> It does not.

Great, it simplifies things a lot.


>> If the
>> current
>> limit is specific to using a forward proxy, consider complaining to
>> browser developers and using a transparent proxy.

> Not a viable solution.  Yes, I could complain, but my users don't
> really care that I have complained.  They are still stuck with massive
> browser startup times.

What about the transparent proxy part? Besides solving a specific
problem, it might also motivate the Chrome crew (that must hate
transparent proxies as much most of us do) to fix the alleged problem of
equating next TCP hop address with the origin server address for the
purpose of counting outgoing connections.


>> Just to double check: You only saw a single HTTP GET (or similar)
>> request inside most CONNECT tunnels?

> I cannot look inside CONNECT tunnels. 

You can -- built-in browser "developer tools" should tell you what TCP
connection the browser is using for every HTTP request.


> I did see only one CONNECT per TCP connection to the
> proxy server from the Chrome browser.

There can be at most one (successful) CONNECT per TCP connection. If N
browser tabs go to N different secure origin servers, then Chrome would
have to open N different TCP connections to the proxy. However, if
Chrome has to contact the same origin server many times, then Chrome can
and should reuse the same CONNECT tunnel where HTTP persistency rules
allow for HTTP connection reuse.


>> If yes, which TCP connection was
>> closed first and by which side? Client-Squid or Squid-origin?

> Squid closed the connection to origin first.

Outside the SslBump world, and perhaps with some (long) I/O timeouts
aside, if Squid closed a CONNECT tunnel _first_, then there is a Squid
bug: Under normal circumstances, Squid should only close a successfully
established CONNECT tunnel if the client or origin server close their
end first.


HTH,

Alex.


From brian at interlinx.bc.ca  Mon Jan 15 19:31:39 2018
From: brian at interlinx.bc.ca (Brian J. Murrell)
Date: Mon, 15 Jan 2018 14:31:39 -0500
Subject: [squid-users] persistent connections not being utilized with
	Chrome
In-Reply-To: <68585dfd-dcb9-81e4-1298-29f3254279e6@measurement-factory.com>
References: <1515768720.6982.22.camel@interlinx.bc.ca>
 <0ff41e4d-0a10-1553-ce4a-77bc9aac488e@treenet.co.nz>
 <1515813808.6982.61.camel@interlinx.bc.ca>
 <9ca490f3-e813-3c36-0a0f-2ad8d86ce8d3@measurement-factory.com>
 <1516030830.24075.27.camel@interlinx.bc.ca>
 <74eba86c-ef4c-9bf8-f65b-e8098c9ee8c0@measurement-factory.com>
 <1516041175.24075.61.camel@interlinx.bc.ca>
 <68585dfd-dcb9-81e4-1298-29f3254279e6@measurement-factory.com>
Message-ID: <1516044699.24075.65.camel@interlinx.bc.ca>

On Mon, 2018-01-15 at 12:26 -0700, Alex Rousskov wrote:
> 
> What about the transparent proxy part?

I already have that, but that is becoming more or less useless in the
everything-https world we are heading towards since you can't
transparently proxy https.  AFAIU.

> if Squid closed a CONNECT tunnel _first_, then there is a
> Squid
> bug: Under normal circumstances, Squid should only close a
> successfully
> established CONNECT tunnel if the client or origin server close their
> end first.

I'll see if I can double-check that next time I have to restart Chrome.

Cheers,
b.
-------------- next part --------------
A non-text attachment was scrubbed...
Name: signature.asc
Type: application/pgp-signature
Size: 488 bytes
Desc: This is a digitally signed message part
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20180115/586e8af3/attachment.sig>

From rousskov at measurement-factory.com  Mon Jan 15 20:48:46 2018
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Mon, 15 Jan 2018 13:48:46 -0700
Subject: [squid-users] persistent connections not being utilized with
 Chrome
In-Reply-To: <1516044699.24075.65.camel@interlinx.bc.ca>
References: <1515768720.6982.22.camel@interlinx.bc.ca>
 <0ff41e4d-0a10-1553-ce4a-77bc9aac488e@treenet.co.nz>
 <1515813808.6982.61.camel@interlinx.bc.ca>
 <9ca490f3-e813-3c36-0a0f-2ad8d86ce8d3@measurement-factory.com>
 <1516030830.24075.27.camel@interlinx.bc.ca>
 <74eba86c-ef4c-9bf8-f65b-e8098c9ee8c0@measurement-factory.com>
 <1516041175.24075.61.camel@interlinx.bc.ca>
 <68585dfd-dcb9-81e4-1298-29f3254279e6@measurement-factory.com>
 <1516044699.24075.65.camel@interlinx.bc.ca>
Message-ID: <685e23b0-7d00-2bf7-ece4-d2395ac8da29@measurement-factory.com>

On 01/15/2018 12:31 PM, Brian J. Murrell wrote:
> On Mon, 2018-01-15 at 12:26 -0700, Alex Rousskov wrote:
>> What about the transparent proxy part?

> I already have that, but that is becoming more or less useless in the
> everything-https world we are heading towards since you can't
> transparently proxy https.

That statement does not compute in the current context: A transparent
proxy has many disadvantages over a forward/explicit proxy, but both
transparent and forward/explicit proxies have approximately the same
support for HTTPS. In other words, if you find a forward/explicit proxy
useful for HTTPS, then a transparent proxy can be used similarly.

Alex.


From rousskov at measurement-factory.com  Mon Jan 15 21:01:32 2018
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Mon, 15 Jan 2018 14:01:32 -0700
Subject: [squid-users] Performance
In-Reply-To: <2F8ABA7B-25FF-49A3-8E20-F32C3C049691@beavercreek.k12.oh.us>
References: <D2E66EB8081A7D4C86314759C29AAFE043353EE5@BHSMSX03>
 <9902c898-d7c3-95d2-1d1b-351605b8bd2d@measurement-factory.com>
 <2F8ABA7B-25FF-49A3-8E20-F32C3C049691@beavercreek.k12.oh.us>
Message-ID: <f34cc976-777f-7fef-72d2-ef5c7d9f3c73@measurement-factory.com>

On 01/15/2018 11:56 AM, Snyder, Brian wrote:
> I have not found an issue the hardware. 

To avoid misunderstanding, I did not imply that there are
hardware-related issues. My question was about bottlenecks (i.e.,
resources that are being overused, including hardware resources like CPU
or RAM and soft resources like the number of file descriptors or
conntrack buckets).


> Atop shows everything in normal ranges.

If all resource usage is normal when the problem is apparent, then the
source of the "over time it slows down significantly" problem most
likely lies outside of Squid. An external agent (e.g., a parent proxy,
an adaptation service, a DNS server, etc.) must be delaying or dropping
messages. You can narrow down the list of suspects by investigating
Squid-reported errors and identifying Squid transaction stage(s) that
incur delays. Squid logs various transaction response times and error
details that may help in this analysis.


HTH,

Alex.


> On Jan 11, 2018, at 2:23 PM, Alex Rousskov <rousskov at measurement-factory.com<mailto:rousskov at measurement-factory.com>> wrote:
> 
> On 01/11/2018 10:14 AM, Snyder, Brian wrote:
> 
> I apologise for asking another squid performance question,
> 
> There is nothing wrong with that! It is often very difficult to solve
> performance problems on the mailing list, but that does not imply folks
> should not ask performance questions.
> 
> 
> over time it slows down significantly.
> 
> In case nobody looks for or finds problems in your configuration: What
> is the bottleneck? CPU? RAM? Disk I/O? NIC interrupts?
> 
> A tool like atop may be able to answer that question for you, especially
> if you let it collect stats from before Squid start to the time when the
> Squid becomes very slow.
> 
> Alex.
> 



From brian at interlinx.bc.ca  Mon Jan 15 21:12:23 2018
From: brian at interlinx.bc.ca (Brian J. Murrell)
Date: Mon, 15 Jan 2018 16:12:23 -0500
Subject: [squid-users] persistent connections not being utilized with
	Chrome
In-Reply-To: <685e23b0-7d00-2bf7-ece4-d2395ac8da29@measurement-factory.com>
References: <1515768720.6982.22.camel@interlinx.bc.ca>
 <0ff41e4d-0a10-1553-ce4a-77bc9aac488e@treenet.co.nz>
 <1515813808.6982.61.camel@interlinx.bc.ca>
 <9ca490f3-e813-3c36-0a0f-2ad8d86ce8d3@measurement-factory.com>
 <1516030830.24075.27.camel@interlinx.bc.ca>
 <74eba86c-ef4c-9bf8-f65b-e8098c9ee8c0@measurement-factory.com>
 <1516041175.24075.61.camel@interlinx.bc.ca>
 <68585dfd-dcb9-81e4-1298-29f3254279e6@measurement-factory.com>
 <1516044699.24075.65.camel@interlinx.bc.ca>
 <685e23b0-7d00-2bf7-ece4-d2395ac8da29@measurement-factory.com>
Message-ID: <1516050743.24075.71.camel@interlinx.bc.ca>

On Mon, 2018-01-15 at 13:48 -0700, Alex Rousskov wrote:
> 
> That statement does not compute in the current context: A transparent
> proxy has many disadvantages over a forward/explicit proxy,

Sure.  But it has advantages also.

> but both
> transparent and forward/explicit proxies have approximately the same
> support for HTTPS. In other words, if you find a forward/explicit
> proxy
> useful for HTTPS, then a transparent proxy can be used similarly.

And can be done *WITHOUT* doing a MitM attack on my users?

Cheers,
b.
-------------- next part --------------
A non-text attachment was scrubbed...
Name: signature.asc
Type: application/pgp-signature
Size: 488 bytes
Desc: This is a digitally signed message part
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20180115/ff7565f1/attachment.sig>

From rousskov at measurement-factory.com  Mon Jan 15 21:39:57 2018
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Mon, 15 Jan 2018 14:39:57 -0700
Subject: [squid-users] persistent connections not being utilized with
 Chrome
In-Reply-To: <1516050743.24075.71.camel@interlinx.bc.ca>
References: <1515768720.6982.22.camel@interlinx.bc.ca>
 <0ff41e4d-0a10-1553-ce4a-77bc9aac488e@treenet.co.nz>
 <1515813808.6982.61.camel@interlinx.bc.ca>
 <9ca490f3-e813-3c36-0a0f-2ad8d86ce8d3@measurement-factory.com>
 <1516030830.24075.27.camel@interlinx.bc.ca>
 <74eba86c-ef4c-9bf8-f65b-e8098c9ee8c0@measurement-factory.com>
 <1516041175.24075.61.camel@interlinx.bc.ca>
 <68585dfd-dcb9-81e4-1298-29f3254279e6@measurement-factory.com>
 <1516044699.24075.65.camel@interlinx.bc.ca>
 <685e23b0-7d00-2bf7-ece4-d2395ac8da29@measurement-factory.com>
 <1516050743.24075.71.camel@interlinx.bc.ca>
Message-ID: <b0ce302f-e26c-4ca0-336d-58ce2f6c821f@measurement-factory.com>

On 01/15/2018 02:12 PM, Brian J. Murrell wrote:
> On Mon, 2018-01-15 at 13:48 -0700, Alex Rousskov wrote:
>>
>> both transparent and forward/explicit proxies have approximately the same
>> support for HTTPS. In other words, if you find a forward/explicit
>> proxy useful for HTTPS, then a transparent proxy can be used similarly.

> And can be done *WITHOUT* doing a MitM attack on my users?

The answer depends on what you want the proxy to do, but proxy abilities
rarely depend on the proxy deployment mode (transparent or
explicit/forward) in this context.

Neither transparent nor explicit/forward proxy can decrypt traffic
without MitM attacks, of course. I have interpreted your earlier
response as essentially saying that, in the "TLS Everywhere" world, you
find explicit/forward proxy useful but a transparent proxy useless.
Since both can do approximately the same things, the statement did not
compute for me.

Alex.


From xeron.oskom at gmail.com  Tue Jan 16 04:18:21 2018
From: xeron.oskom at gmail.com (Ivan Larionov)
Date: Mon, 15 Jan 2018 20:18:21 -0800
Subject: [squid-users] squid doesn't cache objects in memory when using
 SMP and shared memory cache
In-Reply-To: <cf49ca6f-fd92-ed4a-57be-76c5331045d9@treenet.co.nz>
References: <CAHvB88yP0f5f0MzJ66RFiQFYY2Z4chKi4akeKtUh4qE4J9DJAQ@mail.gmail.com>
 <cf49ca6f-fd92-ed4a-57be-76c5331045d9@treenet.co.nz>
Message-ID: <CAHvB88ywPt0062F1WNPyGZne6rwPO_V-=y7ZSLvVv=4iE6s2KA@mail.gmail.com>

My disks are fast (SSD) so I didn't see performance issues but it doesn't
change the fact that memory hit ratio decreased in more than 10 times. And
with both rock and shared memory cache enabled most of the files were saved
into disk cache and not into memory cache and most of the hits were disk
hits (according to log file).

I already tried squid 4 and it works as expected.

So. Let's forget about rock because the issue I see is related to shared
memory. For my test file with only memory cache enabled:

squid 3.5.27 non-SMP - MISS first then always MEM_HIT
squid 3.5.27 SMP any amount of workers shared memory off - always MEM_HIT
after all workers handled the request once
squid 3.5.27 SMP any amount of workers shared memory on - MISS every time.
squid 4.0.22 SMP 2 workers shared memory on - MISS first then always MEM_HIT

I would like to use squid 4 in production and I probably will since looks
like SMP/shared_cache is broken in 3, but the fact that you still haven't
released it confuses me. IDK why it's still in beta/rc/whatever stage.

On Mon, Jan 15, 2018 at 7:22 AM, Amos Jeffries <squid3 at treenet.co.nz> wrote:

> On 15/01/18 18:53, Ivan Larionov wrote:
>
>> Hello!
>>
>> After migrating squid from non-SMP/aufs to SMP/rock memory cache hit
>> ratio dropped significantly. Like from 50-100% to 1-5%. And disk cache hit
>> ratio went up from 15-50% to stable 60-65%. From the brief log file check
>> it looks like in SMP/rock mode squid avoids using memory for small files
>> like 1-3KB but uses it for 10KB+ files.
>>
>
> AIUI, SMP-mode rock operates as a fully separate process (a "Disker" kid)
> which delivers its results as objects already in shared memory to the
> worker process.
>
> There should be little or no gain from that promotion process anymore -
> which would only be moving the object between memory locations. In fact if
> cache_mem were not operating as shared memory even with SMP active (which
> is possible) the promotion would be an actively bad idea as it prevents
> other workers using the object in future.
>
> They show up as non- MEM_HIT because they are either REFRESH or stored in
> the Disker shared memory instead of the cache_mem shared memory. The Squid
> logging is not quite up to recording the slim distinction between which of
> multiple memory areas are being used.
>
>
>
>> I started tracking down the issue with disabling disk cache completely
>> and it didn't change anything, I just started to get MISS every time for
>> the URL which was getting MEM_HIT with an old configuration. Then I changed
>> "workers 2" to "workers 1" and started getting memory hits as before.
>>
>> So it seems like the issue is with shared memory:
>>
>> When squid doesn't use shared memory it works as expected. Even with
>> multiple workers.
>> When squid uses shared memory it caches very small amount of objects.
>>
>> Am I doing anything wrong? Which debug options should I enable to provide
>> more information if it seems like a bug?
>>
>>
> Are you seeing an actual performance difference? if not I would not worry
> about it.
>
> FYI: if you really want to track this down I suggest using Squid-4 to do
> that. Squid-3 is very near the end of its support lifetime and changes of a
> deep nature do not have much chance at all of getting in there now.
>
> Amos
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>



-- 
With best regards, Ivan Larionov.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20180115/f823bf60/attachment.htm>

From xeron.oskom at gmail.com  Tue Jan 16 04:25:27 2018
From: xeron.oskom at gmail.com (Ivan Larionov)
Date: Mon, 15 Jan 2018 20:25:27 -0800
Subject: [squid-users] squid doesn't cache objects in memory when using
 SMP and shared memory cache
In-Reply-To: <e00d90e5-4521-4cdc-177d-284a21e3c6f0@measurement-factory.com>
References: <CAHvB88yP0f5f0MzJ66RFiQFYY2Z4chKi4akeKtUh4qE4J9DJAQ@mail.gmail.com>
 <e00d90e5-4521-4cdc-177d-284a21e3c6f0@measurement-factory.com>
Message-ID: <CAHvB88w_6LeRyG3HCaBmfz736QcLaf=ji_y7=BtJc9RUAk=dwg@mail.gmail.com>

My total hit ratio decreased in ~2 times from 40% to 20% (it could be cold
cache but it lasted at this level for a day without sign of improvement).

I'll retry tests with making sure there're no Vary header and will also try
1 worker with shared cache tomorrow. But even if it is this bug looks like
caching works as expected in squid 4 so it'll be better solution than
messing up with headers probably.

On Mon, Jan 15, 2018 at 10:26 AM, Alex Rousskov <
rousskov at measurement-factory.com> wrote:

> On 01/14/2018 10:53 PM, Ivan Larionov wrote:
>
> > After migrating squid from non-SMP/aufs to SMP/rock memory cache hit
> > ratio dropped significantly. Like from 50-100% to 1-5%.
>
> This could be a side effect of not supporting Vary caching in shared
> memory: https://bugs.squid-cache.org/show_bug.cgi?id=3806#c9
>
>
> > And disk cache hit ratio went up from 15-50% to stable 60-65%.
>
> I hope your total/combined hit ratio improved overall.
>
>
> > it looks like in SMP/rock mode squid avoids using memory for small
> > files like 1-3KB but uses it for 10KB+ files.
>
> No, there is no such size-discrimination code in Squid.
>
>
> > I started tracking down the issue with disabling disk cache completely
> > and it didn't change anything, I just started to get MISS every time for
> > the URL which was getting MEM_HIT with an old configuration. Then I
> > changed "workers 2" to "workers 1" and started getting memory hits as
> > before.
>
> For a clean apples-to-apples test, make sure you use
> "memory_cache_shared on" when using a single worker without rock
> cache_dirs.
>
>
> > Am I doing anything wrong? Which debug options should I enable to
> > provide more information if it seems like a bug?
>
>
> Vary caching should be fixed as well, of course, but perhaps there is
> another problem we do not know about. I would start by eliminating Vary
> as the known problem. When using a test transaction, make sure the
> response does not have a Vary header. Or configure Squid to log the Vary
> header and remove the corresponding transactions when computing
> adjusted-for-Vary memory cache hit ratio.
>
>
> HTH,
>
> Alex.
>



-- 
With best regards, Ivan Larionov.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20180115/11ae5bcb/attachment.htm>

From rentorbuy at yahoo.com  Tue Jan 16 13:37:05 2018
From: rentorbuy at yahoo.com (Vieri)
Date: Tue, 16 Jan 2018 13:37:05 +0000 (UTC)
Subject: [squid-users] TCP out of memory
In-Reply-To: <6eaaa26c-5226-5f5a-02f1-a9f5df690cc9@treenet.co.nz>
References: <601581447.203369.1515055873511.ref@mail.yahoo.com>
 <601581447.203369.1515055873511@mail.yahoo.com>
 <1042f27d-78e2-aa59-3fa7-804ded30ea13@treenet.co.nz>
 <1464426577.854952.1515142755546@mail.yahoo.com>
 <33a4d101-51c5-aac9-bacf-6c95de183bc1@treenet.co.nz>
 <775733626.1802411.1515363213807@mail.yahoo.com>
 <db3dd9ad-c49d-f649-99c8-f199b7844054@treenet.co.nz>
 <517226574.2508171.1515490819697@mail.yahoo.com>
 <6eaaa26c-5226-5f5a-02f1-a9f5df690cc9@treenet.co.nz>
Message-ID: <2121386754.4153838.1516109825922@mail.yahoo.com>

Hi,

Just a quick follow-up on this.

I dropped squidclamav so I could test c-icap-modules's clamd service instead.
The only difference between the two is that squidclamav was using unix sockets while c-icap-modules is using clamd.

At first, the results were good. The open fd numbers were fluctuating, but within the 1k-2k limit during the first days. However, today I'm getting 4k, and it's only day 5. I suspect I'll be getting 10k+ numbers within another week or two. That's when I'll have to restart squid if I don't want the system to come to a network crawl.

I'm posting info and filedescriptors here:

https://drive.google.com/file/d/1V7Horvvak62U-HjSh5pVEBvVnZhu-iQY/view?usp=sharing

https://drive.google.com/file/d/1P1DAX-dOfW0fzt1sAeyT35brQyoPVodX/view?usp=sharing

By the way, what does "Largest file desc currently in use" mean exactly? Should this value also drop (eventually) under sane conditions?

So I guess moving from squidclamav to c-icap-modules did improve things, but I'm still facing something wrong. I could try moving back to squidclamav in "clamd mode" instead of unix sockets just to see if I get the same partial improvement as the one I've witnessed this week.

Vieri


From rousskov at measurement-factory.com  Tue Jan 16 14:10:34 2018
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Tue, 16 Jan 2018 07:10:34 -0700
Subject: [squid-users] squid doesn't cache objects in memory when using
 SMP and shared memory cache
In-Reply-To: <CAHvB88w_6LeRyG3HCaBmfz736QcLaf=ji_y7=BtJc9RUAk=dwg@mail.gmail.com>
References: <CAHvB88yP0f5f0MzJ66RFiQFYY2Z4chKi4akeKtUh4qE4J9DJAQ@mail.gmail.com>
 <e00d90e5-4521-4cdc-177d-284a21e3c6f0@measurement-factory.com>
 <CAHvB88w_6LeRyG3HCaBmfz736QcLaf=ji_y7=BtJc9RUAk=dwg@mail.gmail.com>
Message-ID: <59fe29cb-e126-789d-bb42-a35590c79047@measurement-factory.com>

On 01/15/2018 09:25 PM, Ivan Larionov wrote:

> looks like caching works as expected in squid 4 so it'll be better
> solution than messing up with headers probably.

FYI: Vary caching in shared memory is not supported in any Squid
version, including v4.

Alex.


> On Mon, Jan 15, 2018 at 10:26 AM, Alex Rousskov wrote:
> 
>     On 01/14/2018 10:53 PM, Ivan Larionov wrote:
> 
>     > After migrating squid from non-SMP/aufs to SMP/rock memory cache hit
>     > ratio dropped significantly. Like from 50-100% to 1-5%.
> 
>     This could be a side effect of not supporting Vary caching in shared
>     memory: https://bugs.squid-cache.org/show_bug.cgi?id=3806#c9
>     <https://bugs.squid-cache.org/show_bug.cgi?id=3806#c9>
> 
> 
>     > And disk cache hit ratio went up from 15-50% to stable 60-65%.
> 
>     I hope your total/combined hit ratio improved overall.
> 
> 
>     > it looks like in SMP/rock mode squid avoids using memory for small
>     > files like 1-3KB but uses it for 10KB+ files.
> 
>     No, there is no such size-discrimination code in Squid.
> 
> 
>     > I started tracking down the issue with disabling disk cache completely
>     > and it didn't change anything, I just started to get MISS every time for
>     > the URL which was getting MEM_HIT with an old configuration. Then I
>     > changed "workers 2" to "workers 1" and started getting memory hits as
>     > before.
> 
>     For a clean apples-to-apples test, make sure you use
>     "memory_cache_shared on" when using a single worker without rock
>     cache_dirs.
> 
> 
>     > Am I doing anything wrong? Which debug options should I enable to
>     > provide more information if it seems like a bug?
> 
> 
>     Vary caching should be fixed as well, of course, but perhaps there is
>     another problem we do not know about. I would start by eliminating Vary
>     as the known problem. When using a test transaction, make sure the
>     response does not have a Vary header. Or configure Squid to log the Vary
>     header and remove the corresponding transactions when computing
>     adjusted-for-Vary memory cache hit ratio.
> 
> 
>     HTH,
> 
>     Alex.
> 
> 
> 
> 
> -- 
> With best regards, Ivan Larionov.



From squid3 at treenet.co.nz  Tue Jan 16 14:22:21 2018
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Wed, 17 Jan 2018 03:22:21 +1300
Subject: [squid-users] TCP out of memory
In-Reply-To: <2121386754.4153838.1516109825922@mail.yahoo.com>
References: <601581447.203369.1515055873511.ref@mail.yahoo.com>
 <601581447.203369.1515055873511@mail.yahoo.com>
 <1042f27d-78e2-aa59-3fa7-804ded30ea13@treenet.co.nz>
 <1464426577.854952.1515142755546@mail.yahoo.com>
 <33a4d101-51c5-aac9-bacf-6c95de183bc1@treenet.co.nz>
 <775733626.1802411.1515363213807@mail.yahoo.com>
 <db3dd9ad-c49d-f649-99c8-f199b7844054@treenet.co.nz>
 <517226574.2508171.1515490819697@mail.yahoo.com>
 <6eaaa26c-5226-5f5a-02f1-a9f5df690cc9@treenet.co.nz>
 <2121386754.4153838.1516109825922@mail.yahoo.com>
Message-ID: <9b6d3bee-f185-6ec2-6ce6-0759ce022480@treenet.co.nz>

On 17/01/18 02:37, Vieri wrote:
> Hi,
> 
> Just a quick follow-up on this.
> 
> I dropped squidclamav so I could test c-icap-modules's clamd service instead.
> The only difference between the two is that squidclamav was using unix sockets while c-icap-modules is using clamd.
> 
> At first, the results were good. The open fd numbers were fluctuating, but within the 1k-2k limit during the first days. However, today I'm getting 4k, and it's only day 5. I suspect I'll be getting 10k+ numbers within another week or two. That's when I'll have to restart squid if I don't want the system to come to a network crawl.
> 
> I'm posting info and filedescriptors here:
> 
> https://drive.google.com/file/d/1V7Horvvak62U-HjSh5pVEBvVnZhu-iQY/view?usp=sharing
> 
> https://drive.google.com/file/d/1P1DAX-dOfW0fzt1sAeyT35brQyoPVodX/view?usp=sharing
> 

Sorry I have a bit of a distraction going on ATM so have not got to that 
detailed check yet. Good to hear you found a slightly better situation 
though.


> By the way, what does "Largest file desc currently in use" mean exactly? Should this value also drop (eventually) under sane conditions?

The OS assigns FD numbers and prefers to assign with a strong bias 
towards the lowest values. So that can be seen as a fluctuating "water 
level" of approximately how many FD are currently in use. If there are a 
few very long-lived connections and many short ones it may be variably 
incorrect - but is good enough for a rough guide of FD usage.

In normal network conditions it should rise and fall with your peak vs 
off-peak traffic times. I expect with your particular trouble it will 
mostly just go upwards.

Amos


From rousskov at measurement-factory.com  Tue Jan 16 14:25:23 2018
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Tue, 16 Jan 2018 07:25:23 -0700
Subject: [squid-users] squid doesn't cache objects in memory when using
 SMP and shared memory cache
In-Reply-To: <CAHvB88w_6LeRyG3HCaBmfz736QcLaf=ji_y7=BtJc9RUAk=dwg@mail.gmail.com>
References: <CAHvB88yP0f5f0MzJ66RFiQFYY2Z4chKi4akeKtUh4qE4J9DJAQ@mail.gmail.com>
 <e00d90e5-4521-4cdc-177d-284a21e3c6f0@measurement-factory.com>
 <CAHvB88w_6LeRyG3HCaBmfz736QcLaf=ji_y7=BtJc9RUAk=dwg@mail.gmail.com>
Message-ID: <6458444f-8f31-375b-0ad5-4f3a76eef4fd@measurement-factory.com>

On 01/15/2018 09:25 PM, Ivan Larionov wrote:
> My total hit ratio decreased in ~2 times from 40% to 20%

>     On 01/14/2018 10:53 PM, Ivan Larionov wrote:
> 
>     > After migrating squid from non-SMP/aufs to SMP/rock memory cache hit
>     > ratio dropped significantly. Like from 50-100% to 1-5%.
> 
>     > And disk cache hit ratio went up from 15-50% to stable 60-65%.


The combination of the three statements above may be a sign of a problem
unrelated to Vary: Since the disk cache can cache everything the memory
cache can and is typically much larger than the memory cache, the
incapacitation of a memory cache (due to Vary) should not have a
significant effect on overall hit ratio. It should only affect hit
response time.

The only known culprit I can think of in this context are hits for
being-cached objects: Rock lacks code that allows Squid to read
being-written objects. The shared memory cache has that code already. If
your workload has a lot of cases where clients request a being-written
object, then the overall hit ratio should go down after the memory cache
incapacitation (due to Vary).

I suspect something else is in play here though, especially if you see a
different picture with Squid v4 -- the known problem discussed above is
present in all Squid versions. I second Amos's recommendation to focus
on v4 because it is unlikely that any complicated problems are going to
be fixed in v3, even if you triage them well.


HTH,

Alex.


From squid3 at treenet.co.nz  Tue Jan 16 14:46:49 2018
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Wed, 17 Jan 2018 03:46:49 +1300
Subject: [squid-users] squid doesn't cache objects in memory when using
 SMP and shared memory cache
In-Reply-To: <CAHvB88ywPt0062F1WNPyGZne6rwPO_V-=y7ZSLvVv=4iE6s2KA@mail.gmail.com>
References: <CAHvB88yP0f5f0MzJ66RFiQFYY2Z4chKi4akeKtUh4qE4J9DJAQ@mail.gmail.com>
 <cf49ca6f-fd92-ed4a-57be-76c5331045d9@treenet.co.nz>
 <CAHvB88ywPt0062F1WNPyGZne6rwPO_V-=y7ZSLvVv=4iE6s2KA@mail.gmail.com>
Message-ID: <0b717c57-dc7a-08bf-83f2-497a65ff4ed4@treenet.co.nz>

On 16/01/18 17:18, Ivan Larionov wrote:
> 
> I would like to use squid 4 in production and I probably will since 
> looks like SMP/shared_cache is broken in 3, but the fact that you still 
> haven't released it confuses me. IDK why it's still in beta/rc/whatever 
> stage.

FYI: 
<https://squidproxy.wordpress.com/2016/07/01/whats-going-on-with-squid-4/>. 
The situation has not changed much since July except that the bugs are 
now down to just 2 with one of those also nearly completed.

The bug which is nearly fixed 
(<https://bugs.squid-cache.org/show_bug.cgi?id=4505>) is about cache 
removals not working properly across multiple storage areas. So the fix 
which is soon to go in for that may also have an impact on your situation.

Amos


From christophe.colle at ac-nancy-metz.fr  Tue Jan 16 17:23:25 2018
From: christophe.colle at ac-nancy-metz.fr (Colle Christophe)
Date: Tue, 16 Jan 2018 18:23:25 +0100
Subject: [squid-users] SQUID with two authentications methods
Message-ID: <9e65c14c30dd91b.5a5e431d@ac-nancy-metz.fr>

Hello,

I want to configure SQUID with two authentications methods:

- Kerberos (to do SSO with posts in an ActiveDirectory domain)
- Basic (Open LDAP directory)

The LDAP directory contains all the "official" accounts of people, the AD directory contains some accounts (same identifiers as on LDAP) and generic accounts.

Everything works fine, but I would like to add an extra check: The Kerberos account must also exist in the LDAP directory in order to not allow use of generic accounts.

I managed to do that with Squid but I get this behavior:

- Account present in AD + LDAP: OK
- Account present in AD but not in LDAP: KO

Is it possible to force LDAP authentication if "check_ldap?" fail ?


My config :


# KERBEROS
auth_param negotiate program /usr/lib/squid/negotiate_kerberos_auth
auth_param negotiate children 50 startup=5 idle=1
auth_param negotiate keep_alive on


# LDAP
auth_param basic program /usr/lib/squid/basic_ldap_auth -v 3 -b "ou=official" -f "(uid=%s)" ldap.contonso.lan:389
auth_param basic children 50 startup=5 idle=1
auth_param basic credentialsttl 1 hours




# Extra check
external_acl_type check_ldap ipv4 ttl=3600 children-max=50 %LOGIN /etc/squid/check_ldap_aca.sh




acl authenticated proxy_auth REQUIRED
acl check_ldap external check_ldap




http_access allow http port_80 check_ldap
http_access allow https port_443 check_ldap
http_access allow ftp port_21 check_ldap


http_access deny !authenticated



http_access deny all



My check_ldap_aca.sh :



#!/bin/bash


while read user 
do
 identifiant=(${user//@/ }) 


 result=$(ldapsearch -LLL -h  ldap.contonso.lan  -p 389 -D "uid=usr-proxy" -w *****  -b "ou= official " " (uid=%s) " uid) 
 if [ ${#result} -gt 4 ]
 then
 echo "OK user=$identifiant"
 else
 echo "ERR user=$identifiant"
 fi
done



Thank !

--
Chris
<signatureafterquotedtext></signatureafterquotedtext>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20180116/f50b5494/attachment.htm>

From squid3 at treenet.co.nz  Tue Jan 16 18:10:28 2018
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Wed, 17 Jan 2018 07:10:28 +1300
Subject: [squid-users] SQUID with two authentications methods
In-Reply-To: <9e65c14c30dd91b.5a5e431d@ac-nancy-metz.fr>
References: <9e65c14c30dd91b.5a5e431d@ac-nancy-metz.fr>
Message-ID: <6b1579ce-963c-26a5-6732-a6ab5a35dcbb@treenet.co.nz>

On 17/01/18 06:23, Colle Christophe wrote:
> Hello,
> 
> I want to configure SQUID with two authentications methods:
> 
> - Kerberos (to do SSO with posts in an ActiveDirectory domain)
> - Basic (Open LDAP directory)
> 
> The LDAP directory contains all the "official" accounts of people, the 
> AD directory contains some accounts (same identifiers as on LDAP) and 
> generic accounts.
> 
> Everything works fine, but I would like to add an extra check: The 
> Kerberos account must also exist in the LDAP directory in order to not 
> allow use of generic accounts.
> 
> I managed to do that with Squid but I get this behavior:
> 
> - Account present in AD + LDAP: OK
> - Account present in AD but not in LDAP: KO
> 
> *Is it possible to force LDAP authentication if "check_ldap?" fail ?*

<https://wiki.squid-cache.org/Features/Authentication#Can_I_use_different_authentication_mechanisms_together.3F>

Please note:
  "Squid can not force the clients to choose one over the other."


You would be better to change your design and use an authentication 
helper of your own that performs these complex relationship checks 
instead of trying to warp Squid ACLs into doing things they are not 
supposed to do.

You already have custom ACL helper. Convert that to one which handles 
the auth credentials through the auth_param interface and does both the 
login check and the LDAP check before declaring Kerberos credentials as 
acceptable to Squid.


Also from the order of your squid.conf it is pretty confusing to do the 
login (deny !authenticated) *after* all your check_ldap that rely on 
credentials being valid.


Amos


From xeron.oskom at gmail.com  Tue Jan 16 21:40:24 2018
From: xeron.oskom at gmail.com (Ivan Larionov)
Date: Tue, 16 Jan 2018 13:40:24 -0800
Subject: [squid-users] squid doesn't cache objects in memory when using
 SMP and shared memory cache
In-Reply-To: <6458444f-8f31-375b-0ad5-4f3a76eef4fd@measurement-factory.com>
References: <CAHvB88yP0f5f0MzJ66RFiQFYY2Z4chKi4akeKtUh4qE4J9DJAQ@mail.gmail.com>
 <e00d90e5-4521-4cdc-177d-284a21e3c6f0@measurement-factory.com>
 <CAHvB88w_6LeRyG3HCaBmfz736QcLaf=ji_y7=BtJc9RUAk=dwg@mail.gmail.com>
 <6458444f-8f31-375b-0ad5-4f3a76eef4fd@measurement-factory.com>
Message-ID: <CAHvB88wNLeFwqeqd_whoSQke7u9X1YY2E-JS6ZsU=HwDZkZWaQ@mail.gmail.com>

So it's definitely not related to Vary, there's no such header in requests
I tried. Also this issue affects squid even with 1 worker if shared memory
is forced to on.

Interesting thing I noticed is that according to log file a lot of images
are actually cached in memory but sound files are not (mp3/wav). It's not
like it makes any sense but how about random example url:

http://techslides.com/demos/samples/sample.mp3

squid 4, 2 workers, shared cache, no disk cache ? MEM_HIT
squid 3, same config ? MISS every time
squid 3, no shared cache ? MEM_HIT

Could you do a brief test with this URL may be and confirm that I'm not the
only one who see this issue?

On Tue, Jan 16, 2018 at 6:25 AM, Alex Rousskov <
rousskov at measurement-factory.com> wrote:

> On 01/15/2018 09:25 PM, Ivan Larionov wrote:
> > My total hit ratio decreased in ~2 times from 40% to 20%
>
> >     On 01/14/2018 10:53 PM, Ivan Larionov wrote:
> >
> >     > After migrating squid from non-SMP/aufs to SMP/rock memory cache
> hit
> >     > ratio dropped significantly. Like from 50-100% to 1-5%.
> >
> >     > And disk cache hit ratio went up from 15-50% to stable 60-65%.
>
>
> The combination of the three statements above may be a sign of a problem
> unrelated to Vary: Since the disk cache can cache everything the memory
> cache can and is typically much larger than the memory cache, the
> incapacitation of a memory cache (due to Vary) should not have a
> significant effect on overall hit ratio. It should only affect hit
> response time.
>
> The only known culprit I can think of in this context are hits for
> being-cached objects: Rock lacks code that allows Squid to read
> being-written objects. The shared memory cache has that code already. If
> your workload has a lot of cases where clients request a being-written
> object, then the overall hit ratio should go down after the memory cache
> incapacitation (due to Vary).
>
> I suspect something else is in play here though, especially if you see a
> different picture with Squid v4 -- the known problem discussed above is
> present in all Squid versions. I second Amos's recommendation to focus
> on v4 because it is unlikely that any complicated problems are going to
> be fixed in v3, even if you triage them well.
>
>
> HTH,
>
> Alex.
>



-- 
With best regards, Ivan Larionov.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20180116/0e622fe8/attachment.htm>

From rousskov at measurement-factory.com  Tue Jan 16 23:17:32 2018
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Tue, 16 Jan 2018 16:17:32 -0700
Subject: [squid-users] squid doesn't cache objects in memory when using
 SMP and shared memory cache
In-Reply-To: <CAHvB88wNLeFwqeqd_whoSQke7u9X1YY2E-JS6ZsU=HwDZkZWaQ@mail.gmail.com>
References: <CAHvB88yP0f5f0MzJ66RFiQFYY2Z4chKi4akeKtUh4qE4J9DJAQ@mail.gmail.com>
 <e00d90e5-4521-4cdc-177d-284a21e3c6f0@measurement-factory.com>
 <CAHvB88w_6LeRyG3HCaBmfz736QcLaf=ji_y7=BtJc9RUAk=dwg@mail.gmail.com>
 <6458444f-8f31-375b-0ad5-4f3a76eef4fd@measurement-factory.com>
 <CAHvB88wNLeFwqeqd_whoSQke7u9X1YY2E-JS6ZsU=HwDZkZWaQ@mail.gmail.com>
Message-ID: <617f83e4-f8f2-264e-ecd5-3e57e3194970@measurement-factory.com>

On 01/16/2018 02:40 PM, Ivan Larionov wrote:
> So it's definitely not related to Vary, there's no such header in
> requests I tried.

Just to avoid misunderstanding, please note that Vary is a response header.


> Also this issue affects squid even with 1 worker if
> shared memory is forced to on.

This matches the current suspicion that there is something wrong with
the shared memory cache (in your environment).

> It's not like it makes any sense but how about random example url:
> 
> http://techslides.com/demos/samples/sample.mp3
> 
> squid 4, 2 workers, shared cache, no disk cache ? MEM_HIT
> squid 3, same config ? MISS every time
> squid 3, no shared cache ? MEM_HIT
> 
> Could you do a brief test with this URL may be and confirm that I'm not
> the only one who see this issue?

I cannot confirm that: In primitive wget tests with Squid v3.5 (bzr
r14182), I am getting shared memory hits with the above URL, with or
without cache_dirs, with or without SMP.

Alex.


> On Tue, Jan 16, 2018 at 6:25 AM, Alex Rousskov wrote:
> 
>     On 01/15/2018 09:25 PM, Ivan Larionov wrote:
>     > My total hit ratio decreased in ~2 times from 40% to 20%
> 
>     >? ? ?On 01/14/2018 10:53 PM, Ivan Larionov wrote:
>     >
>     >? ? ?> After migrating squid from non-SMP/aufs to SMP/rock memory cache hit
>     >? ? ?> ratio dropped significantly. Like from 50-100% to 1-5%.
>     >
>     >? ? ?> And disk cache hit ratio went up from 15-50% to stable 60-65%.
> 
> 
>     The combination of the three statements above may be a sign of a problem
>     unrelated to Vary: Since the disk cache can cache everything the memory
>     cache can and is typically much larger than the memory cache, the
>     incapacitation of a memory cache (due to Vary) should not have a
>     significant effect on overall hit ratio. It should only affect hit
>     response time.
> 
>     The only known culprit I can think of in this context are hits for
>     being-cached objects: Rock lacks code that allows Squid to read
>     being-written objects. The shared memory cache has that code already. If
>     your workload has a lot of cases where clients request a being-written
>     object, then the overall hit ratio should go down after the memory cache
>     incapacitation (due to Vary).
> 
>     I suspect something else is in play here though, especially if you see a
>     different picture with Squid v4 -- the known problem discussed above is
>     present in all Squid versions. I second Amos's recommendation to focus
>     on v4 because it is unlikely that any complicated problems are going to
>     be fixed in v3, even if you triage them well.
> 
> 
>     HTH,
> 
>     Alex.
> 
> 
> 
> 
> -- 
> With best regards, Ivan Larionov.



From xeron.oskom at gmail.com  Tue Jan 16 23:59:13 2018
From: xeron.oskom at gmail.com (Ivan Larionov)
Date: Tue, 16 Jan 2018 15:59:13 -0800
Subject: [squid-users] squid doesn't cache objects in memory when using
 SMP and shared memory cache
In-Reply-To: <617f83e4-f8f2-264e-ecd5-3e57e3194970@measurement-factory.com>
References: <CAHvB88yP0f5f0MzJ66RFiQFYY2Z4chKi4akeKtUh4qE4J9DJAQ@mail.gmail.com>
 <e00d90e5-4521-4cdc-177d-284a21e3c6f0@measurement-factory.com>
 <CAHvB88w_6LeRyG3HCaBmfz736QcLaf=ji_y7=BtJc9RUAk=dwg@mail.gmail.com>
 <6458444f-8f31-375b-0ad5-4f3a76eef4fd@measurement-factory.com>
 <CAHvB88wNLeFwqeqd_whoSQke7u9X1YY2E-JS6ZsU=HwDZkZWaQ@mail.gmail.com>
 <617f83e4-f8f2-264e-ecd5-3e57e3194970@measurement-factory.com>
Message-ID: <CAHvB88y0yy-sCkUWuVeKk8eZkvBS6TS3eFpbe1t-=D-nOMPLGg@mail.gmail.com>

Yeah I meant Very in response to requests I tried.

The fact that you can't reproduce it with this url and the fact that it
affects mostly mp3/wav files gave me an idea.

Our env specific is squid's cache_peer parent which transcodes audio files
to ulaw (format which our backend supports). This explains why audio files.
Doesn't explain why it works with squid 3 non-shared memory and squid 4
shared-memory.

I verified direct request and memory cache works with squid 3 +shared
memory.

The difference between direct and non-direct (transcoded) response for
http://techslides.com/demos/samples/sample.mp3:

* "Content-Type: audio/mpeg" for direct, "Content-Type: audio/ulaw" for
non-direct.
* No "Content-Length" header for non-direct.

What do you think? Could these 2 points lead to the issue I see? Why does
it work in all situations except squid 3 + shared memory?

On Tue, Jan 16, 2018 at 3:17 PM, Alex Rousskov <
rousskov at measurement-factory.com> wrote:

> On 01/16/2018 02:40 PM, Ivan Larionov wrote:
> > So it's definitely not related to Vary, there's no such header in
> > requests I tried.
>
> Just to avoid misunderstanding, please note that Vary is a response header.
>
>
> > Also this issue affects squid even with 1 worker if
> > shared memory is forced to on.
>
> This matches the current suspicion that there is something wrong with
> the shared memory cache (in your environment).
>
> > It's not like it makes any sense but how about random example url:
> >
> > http://techslides.com/demos/samples/sample.mp3
> >
> > squid 4, 2 workers, shared cache, no disk cache ? MEM_HIT
> > squid 3, same config ? MISS every time
> > squid 3, no shared cache ? MEM_HIT
> >
> > Could you do a brief test with this URL may be and confirm that I'm not
> > the only one who see this issue?
>
> I cannot confirm that: In primitive wget tests with Squid v3.5 (bzr
> r14182), I am getting shared memory hits with the above URL, with or
> without cache_dirs, with or without SMP.
>
> Alex.
>
>
> > On Tue, Jan 16, 2018 at 6:25 AM, Alex Rousskov wrote:
> >
> >     On 01/15/2018 09:25 PM, Ivan Larionov wrote:
> >     > My total hit ratio decreased in ~2 times from 40% to 20%
> >
> >     >     On 01/14/2018 10:53 PM, Ivan Larionov wrote:
> >     >
> >     >     > After migrating squid from non-SMP/aufs to SMP/rock memory
> cache hit
> >     >     > ratio dropped significantly. Like from 50-100% to 1-5%.
> >     >
> >     >     > And disk cache hit ratio went up from 15-50% to stable
> 60-65%.
> >
> >
> >     The combination of the three statements above may be a sign of a
> problem
> >     unrelated to Vary: Since the disk cache can cache everything the
> memory
> >     cache can and is typically much larger than the memory cache, the
> >     incapacitation of a memory cache (due to Vary) should not have a
> >     significant effect on overall hit ratio. It should only affect hit
> >     response time.
> >
> >     The only known culprit I can think of in this context are hits for
> >     being-cached objects: Rock lacks code that allows Squid to read
> >     being-written objects. The shared memory cache has that code
> already. If
> >     your workload has a lot of cases where clients request a
> being-written
> >     object, then the overall hit ratio should go down after the memory
> cache
> >     incapacitation (due to Vary).
> >
> >     I suspect something else is in play here though, especially if you
> see a
> >     different picture with Squid v4 -- the known problem discussed above
> is
> >     present in all Squid versions. I second Amos's recommendation to
> focus
> >     on v4 because it is unlikely that any complicated problems are going
> to
> >     be fixed in v3, even if you triage them well.
> >
> >
> >     HTH,
> >
> >     Alex.
> >
> >
> >
> >
> > --
> > With best regards, Ivan Larionov.
>
>


-- 
With best regards, Ivan Larionov.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20180116/1d620fde/attachment.htm>

From aismel.valle at museomusica.cult.cu  Tue Jan 16 21:17:42 2018
From: aismel.valle at museomusica.cult.cu (Aismel)
Date: Tue, 16 Jan 2018 16:17:42 -0500
Subject: [squid-users] Question with ACL and UrlRewrite ?
Message-ID: <000501d38f0f$71f88470$55e98d50$@museomusica.cult.cu>

Hi,

 

I need allow  all my users navigate through internet but starting at 14:00pm
to 20:00pm to X pages only so before no one can access to that X pages.

 

I need redirect when a user ask www.facebook.com <http://www.facebook.com>
to m.facebook.com

 

I found this script but do not why don't work

 

#!/usr/bin/perl

$mirror = "m.facebook.com";

 

$| = 1;

while (<>) {

    @line = split;

    $_ = $line[0];

    if (m/^http:\/\/((?:[a-z0-9]+\.)?\.facebook\.com)\/(.*)/ &&

        $1 ne $mirror) {

        print "http://" . $mirror . "/" . $2 . "\n";

    } else {

        print $_ . "\n";

    }

}

 

Pd: I set the chmod +x to the script.

 

Thanks any help

 

Best regards

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20180116/6d290e3e/attachment.htm>

From yvoinov at gmail.com  Wed Jan 17 01:04:22 2018
From: yvoinov at gmail.com (Yuri)
Date: Wed, 17 Jan 2018 07:04:22 +0600
Subject: [squid-users] Question with ACL and UrlRewrite ?
In-Reply-To: <000501d38f0f$71f88470$55e98d50$@museomusica.cult.cu>
References: <000501d38f0f$71f88470$55e98d50$@museomusica.cult.cu>
Message-ID: <aa900b55-8fe8-bee3-22be-c757466ac43d@gmail.com>

May be, because of FB is some years ago under HTTPS?


17.01.2018 03:17, Aismel ?????:
>
> Hi,
>
> ?
>
> I need allow ?all my users navigate through internet but starting at
> 14:00pm to 20:00pm to X pages only so before no one can access to that
> X pages.
>
> ?
>
> I need redirect when a user ask www.facebook.com
> <http://www.facebook.com> to m.facebook.com
>
> ?
>
> I found this script but do not why don?t work
>
> ?
>
> #!/usr/bin/perl
>
> $mirror = "m.facebook.com";
>
> ?
>
> $| = 1;
>
> while (<>) {
>
> ??? @line = split;
>
> ??? $_ = $line[0];
>
> ??? if (m/^http:\/\/((?:[a-z0-9]+\.)?\.facebook\.com)\/(.*)/ &&
>
> ??????? $1 ne $mirror) {
>
> ??????? print "http://" . $mirror . "/" . $2 . "\n";
>
> ??? } else {
>
> ??????? print $_ . "\n";
>
> ??? }
>
> }
>
> ?
>
> Pd: I set the chmod +x to the script.
>
> ?
>
> Thanks any help
>
> ?
>
> Best regards
>
>
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users

-- 
*****************************
* C++20 : Bug to the future *
*****************************

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20180117/11dd8fa6/attachment.htm>

From rousskov at measurement-factory.com  Wed Jan 17 03:41:29 2018
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Tue, 16 Jan 2018 20:41:29 -0700
Subject: [squid-users] squid doesn't cache objects in memory when using
 SMP and shared memory cache
In-Reply-To: <CAHvB88y0yy-sCkUWuVeKk8eZkvBS6TS3eFpbe1t-=D-nOMPLGg@mail.gmail.com>
References: <CAHvB88yP0f5f0MzJ66RFiQFYY2Z4chKi4akeKtUh4qE4J9DJAQ@mail.gmail.com>
 <e00d90e5-4521-4cdc-177d-284a21e3c6f0@measurement-factory.com>
 <CAHvB88w_6LeRyG3HCaBmfz736QcLaf=ji_y7=BtJc9RUAk=dwg@mail.gmail.com>
 <6458444f-8f31-375b-0ad5-4f3a76eef4fd@measurement-factory.com>
 <CAHvB88wNLeFwqeqd_whoSQke7u9X1YY2E-JS6ZsU=HwDZkZWaQ@mail.gmail.com>
 <617f83e4-f8f2-264e-ecd5-3e57e3194970@measurement-factory.com>
 <CAHvB88y0yy-sCkUWuVeKk8eZkvBS6TS3eFpbe1t-=D-nOMPLGg@mail.gmail.com>
Message-ID: <83aef54c-0c61-d119-e625-74f94a4692f3@measurement-factory.com>

On 01/16/2018 04:59 PM, Ivan Larionov wrote:

> The difference between direct and non-direct (transcoded) response
> for?http://techslides.com/demos/samples/sample.mp3:

> * No "Content-Length" header for non-direct.

I suspect that "recent" Squid changes improved handling of responses
with unknown size. IIRC, there were several changes in that area. I bet
not all of them were backported to v3.

Alex.


From chip_pop at hotmail.com  Wed Jan 17 11:41:40 2018
From: chip_pop at hotmail.com (joseph)
Date: Wed, 17 Jan 2018 04:41:40 -0700 (MST)
Subject: [squid-users] force full download and limit bandwith
In-Reply-To: <70d882f2-34b6-ca12-cb82-f19be79c0610@treenet.co.nz>
References: <1515939834055-0.post@n4.nabble.com>
 <70d882f2-34b6-ca12-cb82-f19be79c0610@treenet.co.nz>
Message-ID: <1516189300522-0.post@n4.nabble.com>

tks another question related to this

lets say client start downloading obj   and it reach 50% at the speed 128k
wen the client  leave or stop downloading the download continue at full
speed right from that 50% up to the end or it re start from the beginning ??
its important to understand those mixed combination delay_pools and
range_offset_limit   if they work as i tough or no tks 



-----
************************** 
***** Crash to the future  ****
**************************
--
Sent from: http://squid-web-proxy-cache.1019090.n4.nabble.com/Squid-Users-f1019091.html


From squid3 at treenet.co.nz  Wed Jan 17 12:13:14 2018
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 18 Jan 2018 01:13:14 +1300
Subject: [squid-users] force full download and limit bandwith
In-Reply-To: <1516189300522-0.post@n4.nabble.com>
References: <1515939834055-0.post@n4.nabble.com>
 <70d882f2-34b6-ca12-cb82-f19be79c0610@treenet.co.nz>
 <1516189300522-0.post@n4.nabble.com>
Message-ID: <14cae356-80f9-6892-5e18-307e7964a3ed@treenet.co.nz>

On 18/01/18 00:41, joseph wrote:
> tks another question related to this
> 
> lets say client start downloading obj   and it reach 50% at the speed 128k
> wen the client  leave or stop downloading the download continue at full
> speed right from that 50% up to the end or it re start from the beginning ??
> its important to understand those mixed combination delay_pools and
> range_offset_limit   if they work as i tough or no tks
> 

Each I/O read() Squid performs in receiving a response is assessed as to 
what buffer free space and pool speeds are affecting it. If a client 
disappears midway the pool(s) that client caused to be applied no longer 
are. So the transaction may speed up if the server and network can do 
more speed.

"range_offset_limit N" only affects the initial starting point for a 
transaction. If a client wants to start reading a range somewhere in the 
first N bytes Squid will request the full file in order to cache it for 
future requests. Otherwise only the range the client wants will be 
requested - and cannot (yet) be cached.

Amos


From chip_pop at hotmail.com  Wed Jan 17 12:47:59 2018
From: chip_pop at hotmail.com (joseph)
Date: Wed, 17 Jan 2018 05:47:59 -0700 (MST)
Subject: [squid-users] force full download and limit bandwith
In-Reply-To: <14cae356-80f9-6892-5e18-307e7964a3ed@treenet.co.nz>
References: <1515939834055-0.post@n4.nabble.com>
 <70d882f2-34b6-ca12-cb82-f19be79c0610@treenet.co.nz>
 <1516189300522-0.post@n4.nabble.com>
 <14cae356-80f9-6892-5e18-307e7964a3ed@treenet.co.nz>
Message-ID: <1516193279888-0.post@n4.nabble.com>

Amos Jeffries wrote
> "range_offset_limit N" only affects the initial starting point for a 
> transaction. If a client wants to start reading a range somewhere in the 
> first N bytes Squid will request the full file in order to cache it for 
> future requests. Otherwise only the range the client wants will be 
> requested - and cannot (yet) be cached.

meaning it will continue  dose not restart exelent that way file dose not
downloaded twice once wen active delay
and range_offset_limit none  
meaning one transaction happen



-----
************************** 
***** Crash to the future  ****
**************************
--
Sent from: http://squid-web-proxy-cache.1019090.n4.nabble.com/Squid-Users-f1019091.html


From squid3 at treenet.co.nz  Wed Jan 17 12:53:07 2018
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 18 Jan 2018 01:53:07 +1300
Subject: [squid-users] force full download and limit bandwith
In-Reply-To: <1516193279888-0.post@n4.nabble.com>
References: <1515939834055-0.post@n4.nabble.com>
 <70d882f2-34b6-ca12-cb82-f19be79c0610@treenet.co.nz>
 <1516189300522-0.post@n4.nabble.com>
 <14cae356-80f9-6892-5e18-307e7964a3ed@treenet.co.nz>
 <1516193279888-0.post@n4.nabble.com>
Message-ID: <61956452-4524-8343-e69b-49e488f49629@treenet.co.nz>

On 18/01/18 01:47, joseph wrote:
> Amos Jeffries wrote
>> "range_offset_limit N" only affects the initial starting point for a
>> transaction. If a client wants to start reading a range somewhere in the
>> first N bytes Squid will request the full file in order to cache it for
>> future requests. Otherwise only the range the client wants will be
>> requested - and cannot (yet) be cached.
> 
> meaning it will continue  dose not restart exelent that way file dose not
> downloaded twice once wen active delay
> and range_offset_limit none
> meaning one transaction happen
> 

Note that none of this relates to parallel transactions for the same 
object URL. The older your Squid is the more likely that it will be 
doing multiple parallel fetches, especially as the object size goes up.

That can only be avoided with some changes in the latest Squid and the 
collapsed_forwarding feature.

Amos


From chip_pop at hotmail.com  Wed Jan 17 13:02:35 2018
From: chip_pop at hotmail.com (joseph)
Date: Wed, 17 Jan 2018 06:02:35 -0700 (MST)
Subject: [squid-users] force full download and limit bandwith
In-Reply-To: <61956452-4524-8343-e69b-49e488f49629@treenet.co.nz>
References: <1515939834055-0.post@n4.nabble.com>
 <70d882f2-34b6-ca12-cb82-f19be79c0610@treenet.co.nz>
 <1516189300522-0.post@n4.nabble.com>
 <14cae356-80f9-6892-5e18-307e7964a3ed@treenet.co.nz>
 <1516193279888-0.post@n4.nabble.com>
 <61956452-4524-8343-e69b-49e488f49629@treenet.co.nz>
Message-ID: <1516194155839-0.post@n4.nabble.com>

using latest squid 5



-----
************************** 
***** Crash to the future  ****
**************************
--
Sent from: http://squid-web-proxy-cache.1019090.n4.nabble.com/Squid-Users-f1019091.html


From rousskov at measurement-factory.com  Wed Jan 17 15:36:00 2018
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Wed, 17 Jan 2018 08:36:00 -0700
Subject: [squid-users] force full download and limit bandwith
In-Reply-To: <1516193279888-0.post@n4.nabble.com>
References: <1515939834055-0.post@n4.nabble.com>
 <70d882f2-34b6-ca12-cb82-f19be79c0610@treenet.co.nz>
 <1516189300522-0.post@n4.nabble.com>
 <14cae356-80f9-6892-5e18-307e7964a3ed@treenet.co.nz>
 <1516193279888-0.post@n4.nabble.com>
Message-ID: <93fb484b-a96c-b1c6-e11c-563ff7c26115@measurement-factory.com>

On 01/17/2018 05:47 AM, joseph wrote:
> Amos Jeffries wrote
>> "range_offset_limit N" only affects the initial starting point for a 
>> transaction. If a client wants to start reading a range somewhere in the 
>> first N bytes Squid will request the full file in order to cache it for 
>> future requests. Otherwise only the range the client wants will be 
>> requested - and cannot (yet) be cached.

> meaning it will continue  dose not restart 

You have misinterpreted Amos's response: Whether Squid continues after
the client disappears depends on quick_abort_* settings, not
range_offset_limit settings.


HTH,

Alex.


From heiler.bemerguy at cinbesa.com.br  Wed Jan 17 16:15:21 2018
From: heiler.bemerguy at cinbesa.com.br (Heiler Bemerguy)
Date: Wed, 17 Jan 2018 13:15:21 -0300
Subject: [squid-users] force full download and limit bandwith
In-Reply-To: <93fb484b-a96c-b1c6-e11c-563ff7c26115@measurement-factory.com>
References: <1515939834055-0.post@n4.nabble.com>
 <70d882f2-34b6-ca12-cb82-f19be79c0610@treenet.co.nz>
 <1516189300522-0.post@n4.nabble.com>
 <14cae356-80f9-6892-5e18-307e7964a3ed@treenet.co.nz>
 <1516193279888-0.post@n4.nabble.com>
 <93fb484b-a96c-b1c6-e11c-563ff7c26115@measurement-factory.com>
Message-ID: <6fff06f5-ae41-c51f-1493-43f8c7f7928f@cinbesa.com.br>

An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20180117/f18e1420/attachment.htm>

From rahmatellahmohammed at gmail.com  Wed Jan 17 16:20:37 2018
From: rahmatellahmohammed at gmail.com (Mohammed Rahmatellah)
Date: Wed, 17 Jan 2018 17:20:37 +0100
Subject: [squid-users] squid office 365
In-Reply-To: <CAMgVhhHGV3EV4dhrDfWP-SOhOdN6CQ5qFjRkFHATnmSX6Ah9cw@mail.gmail.com>
References: <CAMgVhhHGV3EV4dhrDfWP-SOhOdN6CQ5qFjRkFHATnmSX6Ah9cw@mail.gmail.com>
Message-ID: <CAMgVhhGTVWeE3C8CZ6zw4-688t3DJ-ni6SP7b6iiFYCxmTczwA@mail.gmail.com>

hello guys, squid have problems with office 365 (outlook2016 exactly)??
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20180117/eea7b8a0/attachment.htm>

From yvoinov at gmail.com  Wed Jan 17 17:29:31 2018
From: yvoinov at gmail.com (Yuri)
Date: Wed, 17 Jan 2018 23:29:31 +0600
Subject: [squid-users] squid office 365
In-Reply-To: <CAMgVhhGTVWeE3C8CZ6zw4-688t3DJ-ni6SP7b6iiFYCxmTczwA@mail.gmail.com>
References: <CAMgVhhHGV3EV4dhrDfWP-SOhOdN6CQ5qFjRkFHATnmSX6Ah9cw@mail.gmail.com>
 <CAMgVhhGTVWeE3C8CZ6zw4-688t3DJ-ni6SP7b6iiFYCxmTczwA@mail.gmail.com>
Message-ID: <f7b94d3e-8eab-4ea8-8a83-f9d0332d9a4a@gmail.com>

Squid no, sysadmins - yes.


17.01.2018 22:20, Mohammed Rahmatellah ?????:
> hello guys, squid have problems with office 365 (outlook2016 exactly)??
>
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users

-- 
*****************************
* C++20 : Bug to the future *
*****************************

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20180117/56a69d58/attachment.htm>

From aismel.valle at museomusica.cult.cu  Wed Jan 17 18:50:02 2018
From: aismel.valle at museomusica.cult.cu (Aismel)
Date: Wed, 17 Jan 2018 13:50:02 -0500
Subject: [squid-users] RV:  Question with ACL and UrlRewrite ?
In-Reply-To: <aa900b55-8fe8-bee3-22be-c757466ac43d@gmail.com>
References: <000501d38f0f$71f88470$55e98d50$@museomusica.cult.cu>
 <aa900b55-8fe8-bee3-22be-c757466ac43d@gmail.com>
Message-ID: <002201d38fc3$fbc39550$f34abff0$@museomusica.cult.cu>

I was reading that it is necessary to make the function of man in the middle to be able to issue a security certificate ....

Question: The proxy does not act as such, because it is so complicated to change the user's request when 50% of the work is already done.

Someone has some other solution ... 

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20180117/0aa812e6/attachment.htm>

From aismel.valle at museomusica.cult.cu  Wed Jan 17 19:16:42 2018
From: aismel.valle at museomusica.cult.cu (Aismel)
Date: Wed, 17 Jan 2018 14:16:42 -0500
Subject: [squid-users] ACL question ?
Message-ID: <002c01d38fc7$b54bbd60$1fe33820$@museomusica.cult.cu>

Hello,

 

About this question

 

My users navigate through internet all day but starting at 14:00pm to
20:00pm I need they can access to X pages only like facebook, youtube,
gmail.

 

Ideas ??

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20180117/246c6402/attachment.htm>

From squid3 at treenet.co.nz  Wed Jan 17 19:19:13 2018
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 18 Jan 2018 08:19:13 +1300
Subject: [squid-users] RV: Question with ACL and UrlRewrite ?
In-Reply-To: <002201d38fc3$fbc39550$f34abff0$@museomusica.cult.cu>
References: <000501d38f0f$71f88470$55e98d50$@museomusica.cult.cu>
 <aa900b55-8fe8-bee3-22be-c757466ac43d@gmail.com>
 <002201d38fc3$fbc39550$f34abff0$@museomusica.cult.cu>
Message-ID: <dd8e2321-33b8-5faa-606e-e0cd2d797f0b@treenet.co.nz>

On 18/01/18 07:50, Aismel wrote:
> I was reading that it is necessary to make the function of man in the 
> middle to be able to issue a security certificate ....
> 
> Question: The proxy does not act as such, because it is so complicated 
> to change the user's request when 50% of the work is already done.
> 
> Someone has some other solution ...
> 


Yuri was referring to how your re-writer is looking for URLs that start 
with the exact string "http://". But Facebook and many others no longer 
use URLs that start that way.

No matter what you do to get the traffic, the helper needs to check for 
the real URLs if it is to do anything useful.



MITM is necessary to issue certificates for domains *belonging to other 
people or companies*. Re-writing URLs is very much *not* a good thing to 
do. It is possible, just not wise.

Part of the MITM often involves guaranteeing that the decoded traffic 
continues to go to the place it was already going. That and things like 
it place limitations different from http://

Amos


From yvoinov at gmail.com  Wed Jan 17 19:21:45 2018
From: yvoinov at gmail.com (Yuri)
Date: Thu, 18 Jan 2018 01:21:45 +0600
Subject: [squid-users] ACL question ?
In-Reply-To: <002c01d38fc7$b54bbd60$1fe33820$@museomusica.cult.cu>
References: <002c01d38fc7$b54bbd60$1fe33820$@museomusica.cult.cu>
Message-ID: <a9b56e5c-433c-238e-2902-6c7a5b3500e7@gmail.com>

#??? acl aclname time [day-abbrevs] [h1:m1-h2:m2]
#??? ? # [fast]
#??? ? #? day-abbrevs:
#??? ? #??? S - Sunday
#??? ? #??? M - Monday
#??? ? #??? T - Tuesday
#??? ? #??? W - Wednesday
#??? ? #??? H - Thursday
#??? ? #??? F - Friday
#??? ? #??? A - Saturday
#??? ? #? h1:m1 must be less than h2:m2
#
#??? acl aclname url_regex [-i] ^http:// ...
#??? ? # regex matching on whole URL [fast]
#??? acl aclname urllogin [-i] [^a-zA-Z0-9] ...
#??? ? # regex matching on URL login field
#??? acl aclname urlpath_regex [-i] \.gif$ ...
#??? ? # regex matching on URL path [fast]

This, huh?


18.01.2018 01:16, Aismel ?????:
>
> Hello,
>
> ?
>
> About this question
>
> ?
>
> My users navigate through internet all day but starting at 14:00pm to
> 20:00pm I need they can access to X pages only like facebook, youtube,
> gmail?
>
> ?
>
> Ideas ??
>
>
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users

-- 
*****************************
* C++20 : Bug to the future *
*****************************

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20180118/50442061/attachment.htm>

From squid3 at treenet.co.nz  Wed Jan 17 19:23:37 2018
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 18 Jan 2018 08:23:37 +1300
Subject: [squid-users] ACL question ?
In-Reply-To: <002c01d38fc7$b54bbd60$1fe33820$@museomusica.cult.cu>
References: <002c01d38fc7$b54bbd60$1fe33820$@museomusica.cult.cu>
Message-ID: <3aa1cd90-2bac-bf28-c41e-6bcf0117817a@treenet.co.nz>

On 18/01/18 08:16, Aismel wrote:
> Hello,
> 
> About this question
> 
> My users navigate through internet all day but starting at 14:00pm to 
> 20:00pm I need they can access to X pages only like facebook, youtube, 
> gmail?
> 
> Ideas ??
> 

The answers you seek are in the FAQ, and your access.log

<https://wiki.squid-cache.org/SquidFaq/SquidAcl#How_can_I_allow_some_clients_to_use_the_cache_at_specific_times.3F>

<https://wiki.squid-cache.org/ConfigExamples/>

Amos


From rousskov at measurement-factory.com  Wed Jan 17 19:27:14 2018
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Wed, 17 Jan 2018 12:27:14 -0700
Subject: [squid-users] force full download and limit bandwith
In-Reply-To: <6fff06f5-ae41-c51f-1493-43f8c7f7928f@cinbesa.com.br>
References: <1515939834055-0.post@n4.nabble.com>
 <70d882f2-34b6-ca12-cb82-f19be79c0610@treenet.co.nz>
 <1516189300522-0.post@n4.nabble.com>
 <14cae356-80f9-6892-5e18-307e7964a3ed@treenet.co.nz>
 <1516193279888-0.post@n4.nabble.com>
 <93fb484b-a96c-b1c6-e11c-563ff7c26115@measurement-factory.com>
 <6fff06f5-ae41-c51f-1493-43f8c7f7928f@cinbesa.com.br>
Message-ID: <a4460b67-a466-11b1-ff76-c7a87b5222ff@measurement-factory.com>

On 01/17/2018 09:15 AM, Heiler Bemerguy wrote:
> Everytime I enable this:
> 
>     range_offset_limit -1 fullDLext
>     quick_abort_min 0 KB
>     quick_abort_max 0 KB
>     quick_abort_pct 95
> 
> Our link is fully utilized for a whole day if at least 2 users tries to
> get the same file with range (206) requests (Windows Update does this)
> Rockstore with SMP and 3.5.27
> 
> The file will be simultaneously downloaded from the beginning by each of
> workers that got the request, and in the end it won't be cached, giving
> a SWAPFAIL_MISS message.. This easily TOPS our link for a whole day
> (maybe till the users turn their computers off..?)
> 
> It's a known bug, as I filled a bug report years ago.. it was never
> fixed...

To avoid misundertanding, there are at least three overlapping issues
here. Only one of them is a bug:

* The fact that Squid may fetch more data than is necessary when
configured to prefetch is not a bug. Both range_offset_limit and
quick_abort_* are essentially prefetching options -- they tell Squid to
fetch more than what the asking client needs.

* The fact that multiple workers may attempt concurrent downloads of the
same object is not necessarily a bug. Squid behavior in such cases is
controlled by collapsed_forwarding (and various HTTP rules).

* The fact that the concurrently fetched cachable object may never be
cached is a bug. It may be fixed as a side effect of the pending pull
request #46 changes, but I am not sure.

Alex.


> Em 17/01/2018 12:36, Alex Rousskov escreveu:
>> On 01/17/2018 05:47 AM, joseph wrote:
>>> Amos Jeffries wrote
>>>> "range_offset_limit N" only affects the initial starting point for a 
>>>> transaction. If a client wants to start reading a range somewhere in the 
>>>> first N bytes Squid will request the full file in order to cache it for 
>>>> future requests. Otherwise only the range the client wants will be 
>>>> requested - and cannot (yet) be cached.
>>> meaning it will continue  dose not restart 
>> You have misinterpreted Amos's response: Whether Squid continues after
>> the client disappears depends on quick_abort_* settings, not
>> range_offset_limit settings.
>>
>>
>> HTH,
>>
>> Alex.
>> _______________________________________________
>> squid-users mailing list
>> squid-users at lists.squid-cache.org
>> http://lists.squid-cache.org/listinfo/squid-users
> 
> 
> 
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
> 



From rentorbuy at yahoo.com  Thu Jan 18 11:57:48 2018
From: rentorbuy at yahoo.com (Vieri)
Date: Thu, 18 Jan 2018 11:57:48 +0000 (UTC)
Subject: [squid-users] TCP out of memory
In-Reply-To: <9b6d3bee-f185-6ec2-6ce6-0759ce022480@treenet.co.nz>
References: <601581447.203369.1515055873511.ref@mail.yahoo.com>
 <601581447.203369.1515055873511@mail.yahoo.com>
 <1042f27d-78e2-aa59-3fa7-804ded30ea13@treenet.co.nz>
 <1464426577.854952.1515142755546@mail.yahoo.com>
 <33a4d101-51c5-aac9-bacf-6c95de183bc1@treenet.co.nz>
 <775733626.1802411.1515363213807@mail.yahoo.com>
 <db3dd9ad-c49d-f649-99c8-f199b7844054@treenet.co.nz>
 <517226574.2508171.1515490819697@mail.yahoo.com>
 <6eaaa26c-5226-5f5a-02f1-a9f5df690cc9@treenet.co.nz>
 <2121386754.4153838.1516109825922@mail.yahoo.com>
 <9b6d3bee-f185-6ec2-6ce6-0759ce022480@treenet.co.nz>
Message-ID: <1074803696.387789.1516276668536@mail.yahoo.com>

________________________________
From: Amos Jeffries <squid3 at treenet.co.nz>
>
> Sorry I have a bit of a distraction going on ATM so have not got to that 

> detailed check yet. Good to hear you found a slightly better situation > though.
[...]
> In normal network conditions it should rise and fall with your peak vs 
> off-peak traffic times. I expect with your particular trouble it will 
> mostly just go upwards.


No worries. I'd like to confirm that I'm still seeing the same issue with c-icap-modules, even though it's slightly better in that the FD numbers grow slower, at least at first.
I must say that it seems to be growing faster now. I had 4k two days ago, now I have:
Largest file desc currently in use:   6664
Number of file desc currently in use: 6270
So it seesm that the more days go by, the faster the FD numbers rise.

Vieri


From squid3 at treenet.co.nz  Thu Jan 18 15:16:49 2018
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Fri, 19 Jan 2018 04:16:49 +1300
Subject: [squid-users] [squid-announce] RFC: Squid ESI processor changes
Message-ID: <04eefb10-68c8-f24e-f067-34a6df45b181@treenet.co.nz>

The Squid team are planning to remove the Custom XML parser used for ESI 
processing from the next Squid version.

At first this seemed like a simple removal if unused functionality. 
However during review of the changes it turns out this functionality may 
be used in many situations when it should not have been.


Can people using ESI please provide answers for these questions:


What is your setting for esi_parser squid.conf directive?

  a) did not set it
  b) "expat"
  c) "libxml2"
  d) "custom"

If you answered (d), why choose that one?


If you answer (a) does setting it explicitly to either of the other 
parsers cause a large impact on your traffic performance?

   Yes/No

If yes, which did you choose?

  ... and how much of an impact was seen?



If you are not comfortable answering this to the squid-dev mailing list 
please feel free to send your responses directly to me.

Amos Jeffries
The Squid Software Foundation

_______________________________________________
squid-announce mailing list
squid-announce at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-announce

From xeron.oskom at gmail.com  Thu Jan 18 22:16:44 2018
From: xeron.oskom at gmail.com (Ivan Larionov)
Date: Thu, 18 Jan 2018 14:16:44 -0800
Subject: [squid-users] rock storage and max-swap-rate
Message-ID: <CAHvB88xY6_40ge1M-W0vyguVeDQCy=s15PGkb_PECdAs91f-Kw@mail.gmail.com>

Hello.

cache_dir max-swap-rate documentation says that swap in requests contribute
to measured swap rate. However in our squid 4 load test we see that
read_ops + write_ops significantly exceeds the max-swap-rate we set and
squid doesn't limit it.

I tried to set it to 200 to confirm that it actually works and saw that it
does. Squid started warning about exceeding max-swap-rate. But looks like
real limit is higher than the value we set in configuration.

Hardware:

AWS GP2 EBS (SSD) 600GB, 1500 iops baseline performance, 3000 iops
burstable.

Config:

cache_dir rock /mnt/services/squid/cache 435200 swap-timeout=500
max-swap-rate=1200 slot-size=16384

IOPS squid pushes under our load test:

read ~800 ops/sec
write ~1100 ops/sec

In summary it gives us ~1900 ops/sec which exceeds AWS limit of 1500
ops/sec and after spending too much "burst balance" we started getting
throttled from AWS side.

Could you please comment on this behavior? What the limit should we set to
stay under 1500 ops/sec for swap in + swap out operations?

Thanks.

Squid version:

Squid Cache: Version 4.0.22
Service Name: squid
configure options:  '--program-prefix=' '--prefix=/usr'
'--exec-prefix=/usr' '--bindir=/usr/sbin' '--sbindir=/usr/sbin'
'--sysconfdir=/etc/squid' '--libdir=/usr/lib' '--libexecdir=/usr/lib/squid'
'--includedir=/usr/include' '--datadir=/usr/share'
'--sharedstatedir=/usr/com' '--localstatedir=/var'
'--mandir=/usr/share/man' '--infodir=/usr/share/info' '--enable-epoll'
'--enable-removal-policies=heap,lru' '--enable-storeio=aufs,rock'
'--enable-delay-pools' '--with-pthreads' '--enable-cache-digests'
'--with-large-files' '--with-maxfd=16384' '--enable-htcp'

-- 
With best regards, Ivan Larionov.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20180118/eec49743/attachment.htm>

From rousskov at measurement-factory.com  Thu Jan 18 22:54:01 2018
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Thu, 18 Jan 2018 15:54:01 -0700
Subject: [squid-users] rock storage and max-swap-rate
In-Reply-To: <CAHvB88xY6_40ge1M-W0vyguVeDQCy=s15PGkb_PECdAs91f-Kw@mail.gmail.com>
References: <CAHvB88xY6_40ge1M-W0vyguVeDQCy=s15PGkb_PECdAs91f-Kw@mail.gmail.com>
Message-ID: <4501260f-c86e-a29c-138e-52f7ad9aaf50@measurement-factory.com>

On 01/18/2018 03:16 PM, Ivan Larionov wrote:

> cache_dir max-swap-rate documentation says that swap in requests
> contribute to measured swap rate. However in our squid 4 load test we
> see that read_ops + write_ops significantly exceeds the?max-swap-rate we
> set and squid doesn't limit it.

In this context, a single Squid "op" is a read or write request from
worker to the disker process. These requests are up to one I/O page in
size. A single I/O page is 32*1024 bytes. See Ipc::Mem::PageSize().

* A single read request usually ends up being a single pread(2) system
call that reads at most one I/O page worth of data from disk. See
diskerRead().

* A single write request usually ends up being a single pwrite(2) system
call that writes at most one I/O page worth of data to disk. However, if
that single pwrite() does not write everything a worker has requested to
write, then Squid will make more pwrite() calls, up to 10 calls total.
See diskerWrite().

Within a single cache miss transaction, the rock code should accumulate
small swapout requests from Store into page-size write requests to
disker, but I do not remember how complete those optimizations are: It
is possible that smaller-than-page writes get through to diskers,
increasing the number of write requests. Same for reading cache hits.


What is the "op" in read_ops and write_ops you have measured?


Since Squid does not (and does not want to) have access to low-level
disk stats and since Squid cannot assume exlusive disk ownership, the
rate-limiting feature for rock cache_dirs does not know how many
low-level disk operations the disk is doing and how those operations
correspond to what Squid is asking the disk to do.


HTH,

Alex.


> I tried to set it to 200 to confirm that it actually works and saw that
> it does. Squid started warning about exceeding max-swap-rate. But looks
> like real limit is higher than the value we set in configuration.
> 
> Hardware:
> 
> AWS GP2 EBS (SSD) 600GB, 1500 iops baseline performance, 3000 iops
> burstable.
> 
> Config:
> 
> cache_dir rock /mnt/services/squid/cache 435200 swap-timeout=500
> max-swap-rate=1200 slot-size=16384
> 
> IOPS squid pushes under our load test:
> 
> read ~800 ops/sec
> write ~1100 ops/sec
> 
> In summary it gives us ~1900 ops/sec which exceeds AWS limit of 1500
> ops/sec and after spending too much "burst balance" we started getting
> throttled from AWS side.
> 
> Could you please comment on this behavior? What the limit should we set
> to stay under 1500 ops/sec for swap in + swap out operations?
> 
> Thanks.
> 
> Squid version:
> 
> Squid Cache: Version 4.0.22
> Service Name: squid
> configure options:? '--program-prefix=' '--prefix=/usr'
> '--exec-prefix=/usr' '--bindir=/usr/sbin' '--sbindir=/usr/sbin'
> '--sysconfdir=/etc/squid' '--libdir=/usr/lib'
> '--libexecdir=/usr/lib/squid' '--includedir=/usr/include'
> '--datadir=/usr/share' '--sharedstatedir=/usr/com'
> '--localstatedir=/var' '--mandir=/usr/share/man'
> '--infodir=/usr/share/info' '--enable-epoll'
> '--enable-removal-policies=heap,lru' '--enable-storeio=aufs,rock'
> '--enable-delay-pools' '--with-pthreads' '--enable-cache-digests'
> '--with-large-files' '--with-maxfd=16384' '--enable-htcp'
> 
> -- 
> With best regards, Ivan Larionov.
> 
> 
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
> 



From xeron.oskom at gmail.com  Thu Jan 18 23:04:39 2018
From: xeron.oskom at gmail.com (Ivan Larionov)
Date: Thu, 18 Jan 2018 15:04:39 -0800
Subject: [squid-users] rock storage and max-swap-rate
In-Reply-To: <4501260f-c86e-a29c-138e-52f7ad9aaf50@measurement-factory.com>
References: <CAHvB88xY6_40ge1M-W0vyguVeDQCy=s15PGkb_PECdAs91f-Kw@mail.gmail.com>
 <4501260f-c86e-a29c-138e-52f7ad9aaf50@measurement-factory.com>
Message-ID: <CAHvB88wvrfyKSeH1Yd-4D9U2c9azpnTho3kAXkX3pcJqYo2RyQ@mail.gmail.com>

Thank you for the fast reply!

read_ops and write_ops is AWS EBS metric and in general it correlates with
OS-level reads/s writes/s stats which iostat shows.

So if I understand you correctly max-swap-rate doesn't limit disk IOPS but
limits squid swap ops instead and every squid operation could in theory use
more than 1 disk IO operation. This means we can't really say "limit swap
ops to 1500 because our disk can handle 1500 iops" but should figure out
the number after testing different values.

Ok, I suppose I'll just do what Rock documentation says ? will test
different values and figure out what works for us.

Thanks again.

On Thu, Jan 18, 2018 at 2:54 PM, Alex Rousskov <
rousskov at measurement-factory.com> wrote:

> On 01/18/2018 03:16 PM, Ivan Larionov wrote:
>
> > cache_dir max-swap-rate documentation says that swap in requests
> > contribute to measured swap rate. However in our squid 4 load test we
> > see that read_ops + write_ops significantly exceeds the max-swap-rate we
> > set and squid doesn't limit it.
>
> In this context, a single Squid "op" is a read or write request from
> worker to the disker process. These requests are up to one I/O page in
> size. A single I/O page is 32*1024 bytes. See Ipc::Mem::PageSize().
>
> * A single read request usually ends up being a single pread(2) system
> call that reads at most one I/O page worth of data from disk. See
> diskerRead().
>
> * A single write request usually ends up being a single pwrite(2) system
> call that writes at most one I/O page worth of data to disk. However, if
> that single pwrite() does not write everything a worker has requested to
> write, then Squid will make more pwrite() calls, up to 10 calls total.
> See diskerWrite().
>
> Within a single cache miss transaction, the rock code should accumulate
> small swapout requests from Store into page-size write requests to
> disker, but I do not remember how complete those optimizations are: It
> is possible that smaller-than-page writes get through to diskers,
> increasing the number of write requests. Same for reading cache hits.
>
>
> What is the "op" in read_ops and write_ops you have measured?
>
>
> Since Squid does not (and does not want to) have access to low-level
> disk stats and since Squid cannot assume exlusive disk ownership, the
> rate-limiting feature for rock cache_dirs does not know how many
> low-level disk operations the disk is doing and how those operations
> correspond to what Squid is asking the disk to do.
>
>
> HTH,
>
> Alex.
>
>
> > I tried to set it to 200 to confirm that it actually works and saw that
> > it does. Squid started warning about exceeding max-swap-rate. But looks
> > like real limit is higher than the value we set in configuration.
> >
> > Hardware:
> >
> > AWS GP2 EBS (SSD) 600GB, 1500 iops baseline performance, 3000 iops
> > burstable.
> >
> > Config:
> >
> > cache_dir rock /mnt/services/squid/cache 435200 swap-timeout=500
> > max-swap-rate=1200 slot-size=16384
> >
> > IOPS squid pushes under our load test:
> >
> > read ~800 ops/sec
> > write ~1100 ops/sec
> >
> > In summary it gives us ~1900 ops/sec which exceeds AWS limit of 1500
> > ops/sec and after spending too much "burst balance" we started getting
> > throttled from AWS side.
> >
> > Could you please comment on this behavior? What the limit should we set
> > to stay under 1500 ops/sec for swap in + swap out operations?
> >
> > Thanks.
> >
> > Squid version:
> >
> > Squid Cache: Version 4.0.22
> > Service Name: squid
> > configure options:  '--program-prefix=' '--prefix=/usr'
> > '--exec-prefix=/usr' '--bindir=/usr/sbin' '--sbindir=/usr/sbin'
> > '--sysconfdir=/etc/squid' '--libdir=/usr/lib'
> > '--libexecdir=/usr/lib/squid' '--includedir=/usr/include'
> > '--datadir=/usr/share' '--sharedstatedir=/usr/com'
> > '--localstatedir=/var' '--mandir=/usr/share/man'
> > '--infodir=/usr/share/info' '--enable-epoll'
> > '--enable-removal-policies=heap,lru' '--enable-storeio=aufs,rock'
> > '--enable-delay-pools' '--with-pthreads' '--enable-cache-digests'
> > '--with-large-files' '--with-maxfd=16384' '--enable-htcp'
> >
> > --
> > With best regards, Ivan Larionov.
> >
> >
> > _______________________________________________
> > squid-users mailing list
> > squid-users at lists.squid-cache.org
> > http://lists.squid-cache.org/listinfo/squid-users
> >
>
>


-- 
With best regards, Ivan Larionov.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20180118/39e034e3/attachment.htm>

From squid3 at treenet.co.nz  Thu Jan 18 23:20:28 2018
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Fri, 19 Jan 2018 12:20:28 +1300
Subject: [squid-users] rock storage and max-swap-rate
In-Reply-To: <CAHvB88wvrfyKSeH1Yd-4D9U2c9azpnTho3kAXkX3pcJqYo2RyQ@mail.gmail.com>
References: <CAHvB88xY6_40ge1M-W0vyguVeDQCy=s15PGkb_PECdAs91f-Kw@mail.gmail.com>
 <4501260f-c86e-a29c-138e-52f7ad9aaf50@measurement-factory.com>
 <CAHvB88wvrfyKSeH1Yd-4D9U2c9azpnTho3kAXkX3pcJqYo2RyQ@mail.gmail.com>
Message-ID: <27371641-f2bc-ecfe-c7de-3dceae2421cb@treenet.co.nz>

On 19/01/18 12:04, Ivan Larionov wrote:
> Thank you for the fast reply!
> 
> read_ops and write_ops is AWS EBS metric and in general it correlates 
> with OS-level reads/s writes/s stats which iostat shows.
> 
> So if I understand you correctly max-swap-rate doesn't limit disk IOPS 
> but limits squid swap ops instead and every squid operation could in 
> theory use more than 1 disk IO operation. This means we can't really say 
> "limit swap ops to 1500 because our disk can handle 1500 iops" but 
> should figure out the number after testing different values.
> 
> Ok, I suppose I'll just do what Rock documentation says ? will test 
> different values and figure out what works for us.
> 


If you know what the OS level IOP size is (eg usually 4KB) and the Squid 
rock IOP size Alex mentioned of 32KB. That should give you a number to 
divide the disk IOPS limit you want with to get a rough estimate for the 
appropriate Squid setting.

The tuning bit is just to see how much variance from that is caused by 
your traffic objects being different from the 32KB slot size.


Amos


From rousskov at measurement-factory.com  Thu Jan 18 23:22:51 2018
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Thu, 18 Jan 2018 16:22:51 -0700
Subject: [squid-users] rock storage and max-swap-rate
In-Reply-To: <CAHvB88wvrfyKSeH1Yd-4D9U2c9azpnTho3kAXkX3pcJqYo2RyQ@mail.gmail.com>
References: <CAHvB88xY6_40ge1M-W0vyguVeDQCy=s15PGkb_PECdAs91f-Kw@mail.gmail.com>
 <4501260f-c86e-a29c-138e-52f7ad9aaf50@measurement-factory.com>
 <CAHvB88wvrfyKSeH1Yd-4D9U2c9azpnTho3kAXkX3pcJqYo2RyQ@mail.gmail.com>
Message-ID: <5b854095-d9b5-da2e-ab7a-a8a896ed5068@measurement-factory.com>

On 01/18/2018 04:04 PM, Ivan Larionov wrote:

> So if I understand you correctly max-swap-rate doesn't limit disk IOPS

Correct. Squid does not know what the disk- or OS-level stats are.


> but limits squid swap ops instead 

If you define a "swap op" as reading or writing a single I/O page for
the purpose of swapping in or swapping out a cached object, then yes.
Processing an HTTP transaction may involve reading and writing many I/O
pages. See my original response for a more precise definition.


> and every squid operation could in
> theory use more than 1 disk IO operation. 

Yes, depending on the file system, disk, etc.

Ideally, rock cache_dirs should use raw disk partitions, but we do not
have a sponsor for that work.


> This means we can't really say
> "limit swap ops to 1500 because our disk can handle 1500 iops" but
> should figure out the number after testing different values.

Correct.


> Ok, I suppose I'll just do what Rock documentation says ? will test
> different values and figure out what works for us.

Sounds like a good plan! You may even find a good/stable correlation
between Squid swap I/O ops and low-level disk I/O ops. Please consider
reporting your findings for others to reuse.


Cheers,

Alex.


> On Thu, Jan 18, 2018 at 2:54 PM, Alex Rousskov wrote:
> 
>     On 01/18/2018 03:16 PM, Ivan Larionov wrote:
> 
>     > cache_dir max-swap-rate documentation says that swap in requests
>     > contribute to measured swap rate. However in our squid 4 load test we
>     > see that read_ops + write_ops significantly exceeds the?max-swap-rate we
>     > set and squid doesn't limit it.
> 
>     In this context, a single Squid "op" is a read or write request from
>     worker to the disker process. These requests are up to one I/O page in
>     size. A single I/O page is 32*1024 bytes. See Ipc::Mem::PageSize().
> 
>     * A single read request usually ends up being a single pread(2) system
>     call that reads at most one I/O page worth of data from disk. See
>     diskerRead().
> 
>     * A single write request usually ends up being a single pwrite(2) system
>     call that writes at most one I/O page worth of data to disk. However, if
>     that single pwrite() does not write everything a worker has requested to
>     write, then Squid will make more pwrite() calls, up to 10 calls total.
>     See diskerWrite().
> 
>     Within a single cache miss transaction, the rock code should accumulate
>     small swapout requests from Store into page-size write requests to
>     disker, but I do not remember how complete those optimizations are: It
>     is possible that smaller-than-page writes get through to diskers,
>     increasing the number of write requests. Same for reading cache hits.
> 
> 
>     What is the "op" in read_ops and write_ops you have measured?
> 
> 
>     Since Squid does not (and does not want to) have access to low-level
>     disk stats and since Squid cannot assume exlusive disk ownership, the
>     rate-limiting feature for rock cache_dirs does not know how many
>     low-level disk operations the disk is doing and how those operations
>     correspond to what Squid is asking the disk to do.
> 
> 
>     HTH,
> 
>     Alex.
> 
> 
>     > I tried to set it to 200 to confirm that it actually works and saw
>     that
>     > it does. Squid started warning about exceeding max-swap-rate. But
>     looks
>     > like real limit is higher than the value we set in configuration.
>     >
>     > Hardware:
>     >
>     > AWS GP2 EBS (SSD) 600GB, 1500 iops baseline performance, 3000 iops
>     > burstable.
>     >
>     > Config:
>     >
>     > cache_dir rock /mnt/services/squid/cache 435200 swap-timeout=500
>     > max-swap-rate=1200 slot-size=16384
>     >
>     > IOPS squid pushes under our load test:
>     >
>     > read ~800 ops/sec
>     > write ~1100 ops/sec
>     >
>     > In summary it gives us ~1900 ops/sec which exceeds AWS limit of 1500
>     > ops/sec and after spending too much "burst balance" we started getting
>     > throttled from AWS side.
>     >
>     > Could you please comment on this behavior? What the limit should
>     we set
>     > to stay under 1500 ops/sec for swap in + swap out operations?
>     >
>     > Thanks.
>     >
>     > Squid version:
>     >
>     > Squid Cache: Version 4.0.22
>     > Service Name: squid
>     > configure options:? '--program-prefix=' '--prefix=/usr'
>     > '--exec-prefix=/usr' '--bindir=/usr/sbin' '--sbindir=/usr/sbin'
>     > '--sysconfdir=/etc/squid' '--libdir=/usr/lib'
>     > '--libexecdir=/usr/lib/squid' '--includedir=/usr/include'
>     > '--datadir=/usr/share' '--sharedstatedir=/usr/com'
>     > '--localstatedir=/var' '--mandir=/usr/share/man'
>     > '--infodir=/usr/share/info' '--enable-epoll'
>     > '--enable-removal-policies=heap,lru' '--enable-storeio=aufs,rock'
>     > '--enable-delay-pools' '--with-pthreads' '--enable-cache-digests'
>     > '--with-large-files' '--with-maxfd=16384' '--enable-htcp'
>     >
>     > --
>     > With best regards, Ivan Larionov.
>     >
>     >
>     > _______________________________________________
>     > squid-users mailing list
>     > squid-users at lists.squid-cache.org
>     <mailto:squid-users at lists.squid-cache.org>
>     > http://lists.squid-cache.org/listinfo/squid-users
>     <http://lists.squid-cache.org/listinfo/squid-users>
>     >
> 
> 
> 
> 
> -- 
> With best regards, Ivan Larionov.



From xeron.oskom at gmail.com  Fri Jan 19 00:35:08 2018
From: xeron.oskom at gmail.com (Ivan Larionov)
Date: Thu, 18 Jan 2018 16:35:08 -0800
Subject: [squid-users] rock storage and max-swap-rate
In-Reply-To: <27371641-f2bc-ecfe-c7de-3dceae2421cb@treenet.co.nz>
References: <CAHvB88xY6_40ge1M-W0vyguVeDQCy=s15PGkb_PECdAs91f-Kw@mail.gmail.com>
 <4501260f-c86e-a29c-138e-52f7ad9aaf50@measurement-factory.com>
 <CAHvB88wvrfyKSeH1Yd-4D9U2c9azpnTho3kAXkX3pcJqYo2RyQ@mail.gmail.com>
 <27371641-f2bc-ecfe-c7de-3dceae2421cb@treenet.co.nz>
Message-ID: <CAHvB88ygzxXemj7MUJ_ZmLzBdXkq8TJJ+HQJ+=F8nQyKUEbyxg@mail.gmail.com>

Thanks Amos.

According to AWS docs:

> I/O size is capped at 256 KiB for SSD volumes
> When small I/O operations are physically contiguous, Amazon EBS attempts
to merge them into a single I/O up to the maximum size. For example, for
SSD volumes, a single 1,024 KiB I/O operation counts as 4 operations
(1,024?256=4), while 8 contiguous I/O operations at 32 KiB each count as
1operation (8?32=256). However, 8 random I/O operations at 32 KiB each
count as 8 operations. Each I/O operation under 32 KiB counts as 1
operation.

So it's not so easy to figure out correlation between squid swap ops and
AWS EBS ops. What I see from here is:

* Multiple squid swap in or swap out ops reading/writing contiguous blocks
could be merged into one 256KB IO operation.
* Random squid operations could be handled as single 32KB IO operation.

On Thu, Jan 18, 2018 at 3:20 PM, Amos Jeffries <squid3 at treenet.co.nz> wrote:

> On 19/01/18 12:04, Ivan Larionov wrote:
>
>> Thank you for the fast reply!
>>
>> read_ops and write_ops is AWS EBS metric and in general it correlates
>> with OS-level reads/s writes/s stats which iostat shows.
>>
>> So if I understand you correctly max-swap-rate doesn't limit disk IOPS
>> but limits squid swap ops instead and every squid operation could in theory
>> use more than 1 disk IO operation. This means we can't really say "limit
>> swap ops to 1500 because our disk can handle 1500 iops" but should figure
>> out the number after testing different values.
>>
>> Ok, I suppose I'll just do what Rock documentation says ? will test
>> different values and figure out what works for us.
>>
>>
>
> If you know what the OS level IOP size is (eg usually 4KB) and the Squid
> rock IOP size Alex mentioned of 32KB. That should give you a number to
> divide the disk IOPS limit you want with to get a rough estimate for the
> appropriate Squid setting.
>
> The tuning bit is just to see how much variance from that is caused by
> your traffic objects being different from the 32KB slot size.
>
>
> Amos
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>



-- 
With best regards, Ivan Larionov.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20180118/ff37610b/attachment.htm>

From rousskov at measurement-factory.com  Fri Jan 19 01:18:47 2018
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Thu, 18 Jan 2018 18:18:47 -0700
Subject: [squid-users] rock storage and max-swap-rate
In-Reply-To: <CAHvB88ygzxXemj7MUJ_ZmLzBdXkq8TJJ+HQJ+=F8nQyKUEbyxg@mail.gmail.com>
References: <CAHvB88xY6_40ge1M-W0vyguVeDQCy=s15PGkb_PECdAs91f-Kw@mail.gmail.com>
 <4501260f-c86e-a29c-138e-52f7ad9aaf50@measurement-factory.com>
 <CAHvB88wvrfyKSeH1Yd-4D9U2c9azpnTho3kAXkX3pcJqYo2RyQ@mail.gmail.com>
 <27371641-f2bc-ecfe-c7de-3dceae2421cb@treenet.co.nz>
 <CAHvB88ygzxXemj7MUJ_ZmLzBdXkq8TJJ+HQJ+=F8nQyKUEbyxg@mail.gmail.com>
Message-ID: <5f191b30-161e-7cd4-d7c8-e3943c6777d2@measurement-factory.com>

On 01/18/2018 05:35 PM, Ivan Larionov wrote:

> * Multiple squid swap in or swap out ops reading/writing contiguous
> blocks could be merged into one 256KB IO operation.

> * Random squid operations could be handled as single 32KB IO operation.

FWIW, on a busy Squid with a large disk cache in a stable state, there
should be virtually no mergeable/contiguous swap requests (first bullet)
due to random nature of rock slot assignment.

Alex.


> On Thu, Jan 18, 2018 at 3:20 PM, Amos Jeffries wrote:
> 
>     On 19/01/18 12:04, Ivan Larionov wrote:
> 
>         Thank you for the fast reply!
> 
>         read_ops and write_ops is AWS EBS metric and in general it
>         correlates with OS-level reads/s writes/s stats which iostat shows.
> 
>         So if I understand you correctly max-swap-rate doesn't limit
>         disk IOPS but limits squid swap ops instead and every squid
>         operation could in theory use more than 1 disk IO operation.
>         This means we can't really say "limit swap ops to 1500 because
>         our disk can handle 1500 iops" but should figure out the number
>         after testing different values.
> 
>         Ok, I suppose I'll just do what Rock documentation says ? will
>         test different values and figure out what works for us.
> 
> 
> 
>     If you know what the OS level IOP size is (eg usually 4KB) and the
>     Squid rock IOP size Alex mentioned of 32KB. That should give you a
>     number to divide the disk IOPS limit you want with to get a rough
>     estimate for the appropriate Squid setting.
> 
>     The tuning bit is just to see how much variance from that is caused
>     by your traffic objects being different from the 32KB slot size.
> 
> 
>     Amos
> 
>     _______________________________________________
>     squid-users mailing list
>     squid-users at lists.squid-cache.org
>     <mailto:squid-users at lists.squid-cache.org>
>     http://lists.squid-cache.org/listinfo/squid-users
>     <http://lists.squid-cache.org/listinfo/squid-users>
> 
> 
> 
> 
> -- 
> With best regards, Ivan Larionov.
> 
> 
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
> 



From rousskov at measurement-factory.com  Sun Jan 21 03:25:32 2018
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Sat, 20 Jan 2018 20:25:32 -0700
Subject: [squid-users] Force revalidation on every hit
Message-ID: <34e2b560-67c4-6da3-9c5d-62d4bc531496@measurement-factory.com>

Hello,

    I would like to configure Squid to always revalidate a cached object
(identified by its URL). Revalidation must happen even if HTTP rules say
that the cached object is still fresh.

There are, of course, ways to do the opposite (i.e., prohibit
revalidation of a stale object), but I cannot find a way that is
_guaranteed_ to trigger (unnecessary from HTTP p.o.v.) revalidation. For
example, I can set

    refresh_pattern ... max-stale=0

but max-stale configuration is ignored for many fresh responses.

I do not control the client or the origin server. I could add a
Cache-Control:no-cache directive to a request or response using
adaptation, but I am looking for a solution that does not involve
message adaptation.

Am I missing a trick that triggers revalidation of fresh objects?


Thank you,

Alex.


From rafael.akchurin at diladele.com  Sun Jan 21 12:05:50 2018
From: rafael.akchurin at diladele.com (Rafael Akchurin)
Date: Sun, 21 Jan 2018 12:05:50 +0000
Subject: [squid-users] Policy based routing and Squid transparent
	interception tutorial
Message-ID: <AM4PR0401MB2194119810458182C080B70B8FED0@AM4PR0401MB2194.eurprd04.prod.outlook.com>

Greetings all,

I have written a step by step tutorial how to enable policy based routing of HTTP and HTTPS traffic with iptables on router (default gateway) and Squid running on a separate box. May be of interest for someone, hence posting it here.

If anything is not clear or plain wrong please say so.
See https://docs.diladele.com/tutorials/policy_based_routing_squid/index.html

Best regards,
Rafael Akchurin
Diladele B.V.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20180121/54f3fbd0/attachment.htm>

From ahmed.zaeem at netstream.ps  Sun Jan 21 20:53:42 2018
From: ahmed.zaeem at netstream.ps (--Ahmad--)
Date: Sun, 21 Jan 2018 22:53:42 +0200
Subject: [squid-users] squid random outgoing address
Message-ID: <C2CA3FA4-E9A1-475A-B478-EBF7B5700FD6@netstream.ps>

Hello Folks 

I?m looking for random squid function in outgoing address .

say i have the subnet 10.20.30.0/24 as an outgoing .

i want to setup small rule in squid that like below :

tcp _outgoing_address 10.20.30.x


as 1 line and that X be a random value from 0-255 that run on each request .

hope to help me with best solution or close one .


cheers 





From Antony.Stone at squid.open.source.it  Sun Jan 21 21:13:12 2018
From: Antony.Stone at squid.open.source.it (Antony Stone)
Date: Sun, 21 Jan 2018 22:13:12 +0100
Subject: [squid-users] squid random outgoing address
In-Reply-To: <C2CA3FA4-E9A1-475A-B478-EBF7B5700FD6@netstream.ps>
References: <C2CA3FA4-E9A1-475A-B478-EBF7B5700FD6@netstream.ps>
Message-ID: <201801212213.12655.Antony.Stone@squid.open.source.it>

On Sunday 21 January 2018 at 21:53:42, --Ahmad-- wrote:

> Hello Folks
> 
> I?m looking for random squid function in outgoing address.

Squid is not the right tool for this job.

> i want to setup small rule in squid that like below :
> 
> tcp _outgoing_address 10.20.30.x
> as 1 line and that X be a random value from 0-255 that run on each request
> 
> hope to help me with best solution or close one .

https://www.spinics.net/lists/netfilter/msg50326.html
https://www.spinics.net/lists/netfilter/msg50327.html
https://www.spinics.net/lists/netfilter/msg50334.html


Antony.

-- 
It may not seem obvious, but (6 x 5 + 5) x 5 - 55 equals 5!

                                                   Please reply to the list;
                                                         please *don't* CC me.


From augustus_meyer at gmx.net  Mon Jan 22 04:35:39 2018
From: augustus_meyer at gmx.net (reinerotto)
Date: Sun, 21 Jan 2018 21:35:39 -0700 (MST)
Subject: [squid-users] rock storage and max-swap-rate
In-Reply-To: <CAHvB88xY6_40ge1M-W0vyguVeDQCy=s15PGkb_PECdAs91f-Kw@mail.gmail.com>
References: <CAHvB88xY6_40ge1M-W0vyguVeDQCy=s15PGkb_PECdAs91f-Kw@mail.gmail.com>
Message-ID: <1516595739547-0.post@n4.nabble.com>

>1500 iops baseline performance< Does this include management operations of
the filesystem used ?
And which filesystem is used ? ext4 might be a bad choice, in case not
significantly "degenerated". 




--
Sent from: http://squid-web-proxy-cache.1019090.n4.nabble.com/Squid-Users-f1019091.html


From squid3 at treenet.co.nz  Sun Jan 21 07:52:07 2018
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sun, 21 Jan 2018 20:52:07 +1300
Subject: [squid-users] [squid-announce] Squid 4.0.23 beta is available
Message-ID: <613605c1-79a3-5a22-9eab-1e0fe62dffaf@treenet.co.nz>

The Squid HTTP Proxy team is very pleased to announce the availability
of the Squid-4.0.23 release!


This release is a security vulnerability and bug fix release resolving
several issues found in the prior Squid releases.


The major changes to be aware of:

* SQUID-2018:1 Denial of Service issue in ESI Response processing.

Squid would crash when receiving certain ESI syntax from its origin 
servers. This is particularly problematic for servers which only deliver 
the relevant syntax on uncommon responses so are not easily detected.

The SSL-Bump feature for HTTPS interception was entangled with 
reverse-proxy processing (and in some cases may still be). Making use of 
the SSL-Bump feature also at risk of encountering the responses from 
servers. Both splice and bump actions are affected.


The fix for Squid-4 is to remove the affected ESI custom parser 
entirely. The use of libxml2 or libexpat is now required for ESI 
support. The default behaviour is to auto-select the most preferred 
library built against.

Installations explicitly choosing "esi_parser custom" in their 
squid.conf will need to change to one of the above mentioned libraries.


Please see the accompanying ADVISORY for details on determining your 
proxy vulnerability and for patches applicable to older versions.


* SQUID-2018:2 Denial of Service issue in HTTP Message processing.

Squid generating ESI sub-requests and requests by the new auto-Download 
feature for intermediary TLS certificates could lead to crashes when 
preparing to log the transaction. This issue can be triggered on demand 
by clients.

Please see the accompanying ADVISORY for details on determining your 
proxy vulnerability and for patches applicable to older versions.


* Bug 4679: User names not sent to url_rewrite_program

This bug appeared as missing user name in url_rewrite_extras parameters 
to the re-writer program when that name was retrieved via an 
authorization mechanism instead of authorization. Specifically IDENT 
protocol or external ACL helpers.


* Bug 4631: security_file_certgen helper without disk cache

This helpers reliance on disk cache management can slow it down on some 
systems which are otherwise able to generate certificates fast. Running 
it purely from memory is now a possibility to avoid these performance 
issues. However, there is no memory cache as yet so this memory-only 
operation requires generating new certificates on every lookup.

Admin encountering significant speed issues with SSL-Bump are encouraged 
to try this helper behaviour. Others


* Nettle v3.4 support

The Nettle library API used by Squid has undergone several updates 
across its 3.3 and 3.4 releases which make recent Squid not able to 
build with these recent libraries.

This Squid now supports the Nettle-3.4 API, with backward compatibility 
provided if older Nettle versions are being used.


* Fix %<Hs, %<pt, %<tt, %<bs calculation bugs for error responses

These logformat macros/codes were not producing accurate outputs in 
certain transactions. Most issues were related to CONNECT tunnel 
transactions, although some issues occurred in other transactions. All 
known issues with these macros/codes are fixed in this Squid release.



  All users of Squid-4.x are urged to upgrade to this release as
soon as possible.

  All users of Squid-3 are encouraged to test this release out and plan
for upgrades where possible.


  See the ChangeLog for the full list of changes in this and earlier
  releases.

Please refer to the release notes at
http://www.squid-cache.org/Versions/v4/RELEASENOTES.html
when you are ready to make the switch to Squid-4

This new release can be downloaded from our HTTP or FTP servers

  http://www.squid-cache.org/Versions/v4/
  ftp://ftp.squid-cache.org/pub/squid/
  ftp://ftp.squid-cache.org/pub/archive/4/

or the mirrors. For a list of mirror sites see

  http://www.squid-cache.org/Download/http-mirrors.html
  http://www.squid-cache.org/Download/mirrors.html

If you encounter any issues with this release please file a bug report.
http://bugs.squid-cache.org/


Amos Jeffries
_______________________________________________
squid-announce mailing list
squid-announce at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-announce

From squid3 at treenet.co.nz  Sun Jan 21 07:52:24 2018
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sun, 21 Jan 2018 20:52:24 +1300
Subject: [squid-users] [squid-announce] [ADVISORY] SQUID-2018:2 Denial of
 Service issue in HTTP Message processing
Message-ID: <4e307c39-d1c1-eaca-2ce4-96a32892ccd5@treenet.co.nz>

__________________________________________________________________

     Squid Proxy Cache Security Update Advisory SQUID-2018:2
__________________________________________________________________

Advisory ID:        SQUID-2018:2
Date:               Jan 19, 2018
Summary:            Denial of Service issue
                     in HTTP Message processing.
Affected versions:  Squid 3.x -> 3.5.27
                     Squid 4.x -> 4.0.22
Fixed in version:   Squid 4.0.23
__________________________________________________________________

     http://www.squid-cache.org/Advisories/SQUID-2018_2.txt
__________________________________________________________________

Problem Description:

  Due to incorrect pointer handling Squid is vulnerable to denial
  of service attack when processing ESI responses or downloading
  intermediate CA certificates.

__________________________________________________________________

Severity:

  This problem allows a remote client delivering certain HTTP
  requests in conjunction with certain trusted server responses to
  trigger a denial of service for all clients accessing the Squid
  service.

__________________________________________________________________

Updated Packages:

  This bug is fixed by Squid version 4.0.23.

  In addition, patches addressing this problem for the stable
  releases can be found in our patch archives:

Squid 3.5:
  <http://www.squid-cache.org/Versions/v3/3.5/changesets/SQUID-2018_2.patch>

Squid 4:
  <http://www.squid-cache.org/Versions/v4/changesets/SQUID-2018_2.patch>

  If you are using a prepackaged version of Squid then please refer
  to the package vendor for availability information on updated
  packages.

__________________________________________________________________

Determining if your version is vulnerable:

  All Squid configured with "log_uses_indirect_client off" are not
  vulnerable.

  All Squid-3.0 versions built with --enable-esi and being used for
  reverse-proxy with squid.conf containing
  "log_uses_indirect_client on" are vulnerable.

  All Squid-3.1 and later versions up to and including
  Squid-3.5.27 being used for reverse-proxy with squid.conf
  containing "log_uses_indirect_client on" are vulnerable.

  All Squid-4 up to and including Squid-4.0.22 being used for
  reverse-proxy with squid.conf containing
  "log_uses_indirect_client on" are vulnerable.

  All unpatched Squid-4 up to and including Squid-4.0.22 being
  used for TLS/HTTPS intercept proxy with squid.conf containing
  "log_uses_indirect_client on" are vulnerable.

__________________________________________________________________

Workarounds:

  Configure "log_uses_indirect_client off" in squid.conf

__________________________________________________________________

Contact details for the Squid project:

  For installation / upgrade support on binary packaged versions
  of Squid: Your first point of contact should be your binary
  package vendor.

  If your install and build Squid from the original Squid sources
  then the squid-users at lists.squid-cache.org mailing list is your
  primary support point. For subscription details see
  <http://www.squid-cache.org/Support/mailing-lists.html>.

  For reporting of non-security bugs in the latest STABLE release
  the squid bugzilla database should be used
  <http://bugs.squid-cache.org/>.

  For reporting of security sensitive bugs send an email to the
  squid-bugs at lists.squid-cache.org mailing list. It's a closed
  list (though anyone can post) and security related bug reports
  are treated in confidence until the impact has been established.

__________________________________________________________________

Credits:

  The initial issue was reported by Louis Dion-Marcil on behalf of
  GoSecure.

  Fixed by Amos Jeffries from Treehouse Networks Ltd.

__________________________________________________________________

Revision history:

  2017-12-13 20:09:30 UTC Initial Report
  2018-01-18 23:10:00 UTC Patches Released
  2018-01-21 07:45:00 UTC Advisory and fixed packages released
__________________________________________________________________
END
_______________________________________________
squid-announce mailing list
squid-announce at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-announce

From squid3 at treenet.co.nz  Sun Jan 21 07:52:36 2018
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sun, 21 Jan 2018 20:52:36 +1300
Subject: [squid-users] [squid-announce] [ADVISORY] SQUID-2018:1 Denial of
 Service issue in ESI Response processing
Message-ID: <b04c0b88-9a23-5c07-1f26-55765b38d76b@treenet.co.nz>

__________________________________________________________________

     Squid Proxy Cache Security Update Advisory SQUID-2018:1
__________________________________________________________________

Advisory ID:        SQUID-2018:1
Date:               Jan 19, 2018
Summary:            Denial of Service issue
                     in ESI Response processing.
Affected versions:  Squid 3.x -> 3.5.27
                     Squid 4.x -> 4.0.22
Fixed in version:   Squid 4.0.23
__________________________________________________________________

     http://www.squid-cache.org/Advisories/SQUID-2018_1.txt
__________________________________________________________________

Problem Description:

  Due to incorrect pointer handling Squid is vulnerable to denial
  of service attack when processing ESI responses.

__________________________________________________________________

Severity:

  This problem allows a remote server delivering certain ESI
  response syntax to trigger a denial of service for all clients
  accessing the Squid service.

  This problem is limited to the Squid custom ESI parser.
  Squid built to use libxml2 or libexpat XML parsers do not have
  this problem.

__________________________________________________________________

Updated Packages:

  This bug is fixed by Squid version 4.0.23.

  In addition, patches addressing this problem for the stable
  releases can be found in our patch archives:

Squid 3.5:
  <http://www.squid-cache.org/Versions/v3/3.5/changesets/SQUID-2018_1.patch>

Squid 4:
  <http://www.squid-cache.org/Versions/v4/changesets/SQUID-2018_1.patch>

  If you are using a prepackaged version of Squid then please refer
  to the package vendor for availability information on updated
  packages.

__________________________________________________________________

Determining if your version is vulnerable:

  All Squid-2.x are not vulnerable.

  All Squid built with --disable-esi are not vulnerable.

  All Squid configured with "esi_parser expat" are not vulnerable.

  All Squid configured with "esi_parser libxml2" are not
  vulnerable.

  All Squid-3.0 versions built without --enable-esi are not
  vulnerable.

  All Squid-3.0 versions built with --enable-esi and using
  custom ESI parser for reverse-proxy are vulnerable.

  All Squid-3.1 and later versions up to and including
  Squid-3.5.27 being used for reverse-proxy are vulnerable.

  All Squid-3.1 and later versions up to and including
  Squid-3.5.27 being used for TLS / HTTPS interception are
  vulnerable.

  All unpatched Squid-4 up to and including Squid-4.0.22 being
  used as reverse-proxy are vulnerable.

  All unpatched Squid-4 up to and including Squid-4.0.22 being
  used as TLS/HTTPS intercept proxy are vulnerable.

__________________________________________________________________

Workarounds:

Either;

  Build Squid with --disable-esi

Or,

  Build Squid with "--enable-esi --with-libxml2" and in squid.conf
  configure "esi_parser libxml2"

Or,

  Build Squid with "--enable-esi --with-expat" and in squid.conf
  configure "esi_parser expat"

__________________________________________________________________

Contact details for the Squid project:

  For installation / upgrade support on binary packaged versions
  of Squid: Your first point of contact should be your binary
  package vendor.

  If your install and build Squid from the original Squid sources
  then the squid-users at lists.squid-cache.org mailing list is your
  primary support point. For subscription details see
  <http://www.squid-cache.org/Support/mailing-lists.html>.

  For reporting of non-security bugs in the latest STABLE release
  the squid bugzilla database should be used
  <http://bugs.squid-cache.org/>.

  For reporting of security sensitive bugs send an email to the
  squid-bugs at lists.squid-cache.org mailing list. It's a closed
  list (though anyone can post) and security related bug reports
  are treated in confidence until the impact has been established.

__________________________________________________________________

Credits:

  The initial issue was reported by Louis Dion-Marcil on behalf of
  GoSecure.

  Fixed by Amos Jeffries from Treehouse Networks Ltd.

__________________________________________________________________

Revision history:

  2017-12-13 20:09:30 UTC Initial Report
  2018-01-18 23:10:00 UTC Patches Released
  2018-01-21 07:45:00 UTC Advisory and fixed packages released
__________________________________________________________________
END
_______________________________________________
squid-announce mailing list
squid-announce at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-announce

From tarotapprentice at yahoo.com  Mon Jan 22 09:37:16 2018
From: tarotapprentice at yahoo.com (TarotApprentice)
Date: Mon, 22 Jan 2018 20:37:16 +1100
Subject: [squid-users] 4.0.23 release in Debian
Message-ID: <E447B375-ABA3-413A-B103-1E167AA39052@yahoo.com>

Given today?s announcement of squid 4.0.23, are there plans for Debian to pick it up?

Currently they have 4.0.21 in experimental and 3.5.23 in their other repos. Emails to the squid maintainers at Debian go unanswered.


From xeron.oskom at gmail.com  Mon Jan 22 09:39:32 2018
From: xeron.oskom at gmail.com (Ivan Larionov)
Date: Mon, 22 Jan 2018 01:39:32 -0800
Subject: [squid-users] rock storage and max-swap-rate
In-Reply-To: <1516595739547-0.post@n4.nabble.com>
References: <CAHvB88xY6_40ge1M-W0vyguVeDQCy=s15PGkb_PECdAs91f-Kw@mail.gmail.com>
 <1516595739547-0.post@n4.nabble.com>
Message-ID: <C235FA0D-C26F-4915-99AB-6184AEDDFCAA@gmail.com>

Could you please elaborate? What?s wrong with rock on ext4? Which filesystem works better for it?

1500 iops is EBS volume limit and it includes all IO operations and it has no idea about filesystem, it just provides block storage device.

On Jan 21, 2018, at 20:35, reinerotto <augustus_meyer at gmx.net> wrote:

>> 1500 iops baseline performance< Does this include management operations of
> the filesystem used ?
> And which filesystem is used ? ext4 might be a bad choice, in case not
> significantly "degenerated". 
> 
> 
> 
> 
> --
> Sent from: http://squid-web-proxy-cache.1019090.n4.nabble.com/Squid-Users-f1019091.html
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users


From squid3 at treenet.co.nz  Mon Jan 22 10:19:07 2018
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Mon, 22 Jan 2018 23:19:07 +1300
Subject: [squid-users] 4.0.23 release in Debian
In-Reply-To: <E447B375-ABA3-413A-B103-1E167AA39052@yahoo.com>
References: <E447B375-ABA3-413A-B103-1E167AA39052@yahoo.com>
Message-ID: <ead2f401-72db-5fdd-39e2-548fe7dc6bc9@treenet.co.nz>

On 22/01/18 22:37, TarotApprentice wrote:
> Given today?s announcement of squid 4.0.23, are there plans for Debian to pick it up?
> 
> Currently they have 4.0.21 in experimental and 3.5.23 in their other repos. Emails to the squid maintainers at Debian go unanswered.


Did you mail Luigi directly or the
pkg-squid-devel at lists.alioth.debian.org team list?
  I don't see anything coming to the list if so.

As to your question; Updates to the stable Debian are in the hands of
the Debian Security Team. There were no advance notices so all the OS
updates are yet to happen.
 I'm preparing the experimental package updates now. Others in the
pkg-squid team to check and upload, so time varies.

Amos


From amajer at suse.de  Mon Jan 22 10:43:18 2018
From: amajer at suse.de (Adam Majer)
Date: Mon, 22 Jan 2018 11:43:18 +0100
Subject: [squid-users] [squid-announce] [ADVISORY] SQUID-2018:2 Denial
 of Service issue in HTTP Message processing
In-Reply-To: <4e307c39-d1c1-eaca-2ce4-96a32892ccd5@treenet.co.nz>
References: <4e307c39-d1c1-eaca-2ce4-96a32892ccd5@treenet.co.nz>
Message-ID: <a00dfe35-923d-55b1-ec5d-3e84bfb2ef3b@suse.de>

On 01/21/2018 08:52 AM, Amos Jeffries wrote:
> __________________________________________________________________
> 
> ??? Squid Proxy Cache Security Update Advisory SQUID-2018:2
> __________________________________________________________________
> 
> Advisory ID:??????? SQUID-2018:2
> Date:?????????????? Jan 19, 2018
> Summary:??????????? Denial of Service issue
> ??????????????????? in HTTP Message processing.
> Affected versions:? Squid 3.x -> 3.5.27
> ??????????????????? Squid 4.x -> 4.0.22
> Fixed in version:?? Squid 4.0.23


Hi,

For this and the other advisories, have you requested CVEs?

Thanks,
- Adam


From squid3 at treenet.co.nz  Mon Jan 22 10:58:25 2018
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Mon, 22 Jan 2018 23:58:25 +1300
Subject: [squid-users] [squid-announce] [ADVISORY] SQUID-2018:2 Denial
 of Service issue in HTTP Message processing
In-Reply-To: <a00dfe35-923d-55b1-ec5d-3e84bfb2ef3b@suse.de>
References: <4e307c39-d1c1-eaca-2ce4-96a32892ccd5@treenet.co.nz>
 <a00dfe35-923d-55b1-ec5d-3e84bfb2ef3b@suse.de>
Message-ID: <8b1347b0-5637-0847-feb5-3006355be44d@treenet.co.nz>

On 22/01/18 23:43, Adam Majer wrote:
> On 01/21/2018 08:52 AM, Amos Jeffries wrote:
>> __________________________________________________________________
>>
>> ??? Squid Proxy Cache Security Update Advisory SQUID-2018:2
>> __________________________________________________________________
>>
>> Advisory ID:??????? SQUID-2018:2
>> Date:?????????????? Jan 19, 2018
>> Summary:??????????? Denial of Service issue
>> ??????????????????? in HTTP Message processing.
>> Affected versions:? Squid 3.x -> 3.5.27
>> ??????????????????? Squid 4.x -> 4.0.22
>> Fixed in version:?? Squid 4.0.23
> 
> 
> Hi,
> 
> For this and the other advisories, have you requested CVEs?
> 

Yes, awaiting DWF processing.

Amos


From rousskov at measurement-factory.com  Mon Jan 22 15:54:36 2018
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Mon, 22 Jan 2018 08:54:36 -0700
Subject: [squid-users] rock storage and max-swap-rate
In-Reply-To: <C235FA0D-C26F-4915-99AB-6184AEDDFCAA@gmail.com>
References: <CAHvB88xY6_40ge1M-W0vyguVeDQCy=s15PGkb_PECdAs91f-Kw@mail.gmail.com>
 <1516595739547-0.post@n4.nabble.com>
 <C235FA0D-C26F-4915-99AB-6184AEDDFCAA@gmail.com>
Message-ID: <0fa3814d-3afb-d20d-c373-c1d3a0a263da@measurement-factory.com>

On 01/22/2018 02:39 AM, Ivan Larionov wrote:
> What?s wrong with rock on ext4?

ext4 does a lot more than rock needs in most environments. More useless
work usually means more overhead/worse performance. YMMV.


> Which filesystem works better for it?
In most cases, the simpler/dumber the filesystem is, the better. If you
recall, rock's ultimate goal is using a raw disk partition or
equivalent... Rock already does 95+% of what a simple file system does.

Alex.


From gkjoshi at gmail.com  Mon Jan 22 18:01:04 2018
From: gkjoshi at gmail.com (Gopi Joshi)
Date: Mon, 22 Jan 2018 13:01:04 -0500
Subject: [squid-users] SSL Sites not redirecting and showing in logs in
 Transparent Mode using WCCP
Message-ID: <CAPEy1iALg1_dULoH3qp40zPn++AcVSZ6P0u1oieBwkf9s7weVA@mail.gmail.com>

Hello

I have installed Squid 3.5 on REdHat and configured it in transparent mode
using WCCP. On 4500 switch we are redirecting Port 80 and 443 , i am not
able to see SSL websites in access.logs , it shows only IP address.also we
are not able to webchain SSL websites based on URL , below is configuration
, rest are default

http_port 3128 transparent
https_port 3127 intercept ssl-bump cert=/opt/squid_certs/proxyCA.pem

### No decryption ##
ssl_bump none all
sslcrtd_program  /usr/lib64/squid/ssl_crtd -s /opt/squid_ssldb/ssl_db -M
40MB
sslcrtd_children 5

WCCP Configuration
==================

# WCCPv2 parameters
wccp2_router 10.1.1.1
wccp2_forwarding_method l2
wccp2_return_method l2
wccp2_assignment_method mask
wccp2_rebuild_wait off
wccp2_service standard 0
wccp2_service dynamic 70
wccp2_service_info 70 protocol=tcp
flags=dst_ip_hash,src_ip_alt_hash,src_port_alt_hash priority=231 ports=443
#wccp2_service_info 70 protocol=tcp priority=231 ports=443


is there a way for squid to see URL / Domain information for SSL Sites
without decrypting ?

Regards
GJoshi
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20180122/6bfd89b5/attachment.htm>

From rousskov at measurement-factory.com  Mon Jan 22 18:44:05 2018
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Mon, 22 Jan 2018 11:44:05 -0700
Subject: [squid-users] SSL Sites not redirecting and showing in logs in
 Transparent Mode using WCCP
In-Reply-To: <CAPEy1iALg1_dULoH3qp40zPn++AcVSZ6P0u1oieBwkf9s7weVA@mail.gmail.com>
References: <CAPEy1iALg1_dULoH3qp40zPn++AcVSZ6P0u1oieBwkf9s7weVA@mail.gmail.com>
Message-ID: <45ed0907-4eec-30b6-e54b-557893a8e7db@measurement-factory.com>

On 01/22/2018 11:01 AM, Gopi Joshi wrote:

> I have installed Squid 3.5 on REdHat and configured it in transparent
> mode using WCCP. On 4500 switch we are redirecting Port 80 and 443 , i
> am not able to see SSL websites in access.logs , it shows only IP
> address.also we are not able to webchain SSL websites based on URL ,

> ssl_bump none all

> is there a way for squid to see URL / Domain information for SSL Sites
> without decrypting ??

URLs -- no.

Domains -- yes, in most cases. Most SSL clients should send a TLS SNI
extension that contains some variation of the intended domain name. To
get access to SNI, you should tell your Squid to peek at the SSL client
handshake:

  ssl_bump peek step1
  ssl_bump splice all

If you also want to know the site certificate details, then you would
need to peek at the server handshake as well:

  ssl_bump peek all
  ssl_bump splice all


N.B. Please note that I do not know what "webchain websites" means.


HTH,

Alex.


From david at articatech.com  Mon Jan 22 22:38:48 2018
From: david at articatech.com (David Touzeau)
Date: Mon, 22 Jan 2018 23:38:48 +0100
Subject: [squid-users] v4.0.22 error:transaction-end-before-headers using
	transparent SSL method
Message-ID: <010e01d393d1$c66349c0$5329dd40$@articatech.com>

Hi

 

I'm using Squid Cache: Version 4.0.22 in transparent method

 

After several times the SSL port going into <  freeze  mode > and write in
logs

 

1516660011.849 000000 192.168.1.214 NONE/000 0 NONE
error:transaction-end-before-headers -

 

Doing a squid -k reconfigure release all freeze requests and proxy run in
normal behavior and return back to freeze mode after 1 or 2 hours

 

How to fix this issue ?

 

Using the defined configuration :

 

http_port 192.168.1.1:50634  intercept disable-pmtu-discovery=transparent
name=MyPortNameID27  

https_port 192.168.1.1:50635  intercept disable-pmtu-discovery=transparent
name=MyPortNameID28 ssl-bump  generate-host-certificates=on
dynamic_cert_mem_cache_size=4MB cert=/etc/squid3/ssl/cb623e9bf

c65772f68b84393604cd6ea.dyn tls-dh=/etc/squid3/ssl/dhparam.pem

sslcrtd_program /lib/squid3/security_file_certgen -s
/var/lib/squid/session/ssl/ssl_db -M 8MB

sslcrtd_children 16 startup=5 idle=1

acl FakeCert ssl::server_name .apple.com

acl FakeCert ssl::server_name .icloud.com

acl FakeCert ssl::server_name .mzstatic.com

acl FakeCert ssl::server_name .dropbox.com

acl ssl_step1 at_step SslBump1

acl ssl_step2 at_step SslBump2

acl ssl_step3 at_step SslBump3

ssl_bump peek ssl_step1

ssl_bump splice GlobalWhitelistDSTNet

ssl_bump splice GlobalWhitelistDomainsRx

ssl_bump splice GlobalWhitelistDomains

ssl_bump splice FakeCert

ssl_bump splice all

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20180122/29011a70/attachment.htm>

From david at articatech.com  Mon Jan 22 23:45:06 2018
From: david at articatech.com (David Touzeau)
Date: Tue, 23 Jan 2018 00:45:06 +0100
Subject: [squid-users] v4.0.22 error:transaction-end-before-headers
	using	transparent SSL method
In-Reply-To: <010e01d393d1$c66349c0$5329dd40$@articatech.com>
References: <010e01d393d1$c66349c0$5329dd40$@articatech.com>
Message-ID: <023101d393db$084225b0$18c67110$@articatech.com>

Notice, it appears on both http/https ports, not only SSL

 

 

De : squid-users [mailto:squid-users-bounces at lists.squid-cache.org] De la
part de David Touzeau
Envoy? : lundi 22 janvier 2018 23:39
? : squid-users at lists.squid-cache.org
Objet : [squid-users] v4.0.22 error:transaction-end-before-headers using
transparent SSL method

 

Hi

 

I?m using Squid Cache: Version 4.0.22 in transparent method

 

After several times the SSL port going into ?  freeze  mode ? and write in
logs

 

1516660011.849 000000 192.168.1.214 NONE/000 0 NONE
error:transaction-end-before-headers ?

 

Doing a squid -k reconfigure release all freeze requests and proxy run in
normal behavior and return back to freeze mode after 1 or 2 hours

 

How to fix this issue ?

 

Using the defined configuration :

 

http_port 192.168.1.1:50634  intercept disable-pmtu-discovery=transparent
name=MyPortNameID27  

https_port 192.168.1.1:50635  intercept disable-pmtu-discovery=transparent
name=MyPortNameID28 ssl-bump  generate-host-certificates=on
dynamic_cert_mem_cache_size=4MB cert=/etc/squid3/ssl/cb623e9bf

c65772f68b84393604cd6ea.dyn tls-dh=/etc/squid3/ssl/dhparam.pem

sslcrtd_program /lib/squid3/security_file_certgen -s
/var/lib/squid/session/ssl/ssl_db -M 8MB

sslcrtd_children 16 startup=5 idle=1

acl FakeCert ssl::server_name .apple.com

acl FakeCert ssl::server_name .icloud.com

acl FakeCert ssl::server_name .mzstatic.com

acl FakeCert ssl::server_name .dropbox.com

acl ssl_step1 at_step SslBump1

acl ssl_step2 at_step SslBump2

acl ssl_step3 at_step SslBump3

ssl_bump peek ssl_step1

ssl_bump splice GlobalWhitelistDSTNet

ssl_bump splice GlobalWhitelistDomainsRx

ssl_bump splice GlobalWhitelistDomains

ssl_bump splice FakeCert

ssl_bump splice all

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20180123/0f69ff71/attachment.htm>

From augustus_meyer at gmx.net  Tue Jan 23 00:47:32 2018
From: augustus_meyer at gmx.net (reinerotto)
Date: Mon, 22 Jan 2018 17:47:32 -0700 (MST)
Subject: [squid-users] rock storage and max-swap-rate
In-Reply-To: <C235FA0D-C26F-4915-99AB-6184AEDDFCAA@gmail.com>
References: <CAHvB88xY6_40ge1M-W0vyguVeDQCy=s15PGkb_PECdAs91f-Kw@mail.gmail.com>
 <1516595739547-0.post@n4.nabble.com>
 <C235FA0D-C26F-4915-99AB-6184AEDDFCAA@gmail.com>
Message-ID: <1516668452215-0.post@n4.nabble.com>

Privet !
>Could you please elaborate? What?s wrong with rock on ext4? <
Default ext4 uses a "journal" of the modifications. Which adds I/O.
Timestamps of filemods are other I/Os. I do not think, that these features
are required for rock. Disabling journal completely will cause loss of data
(cached) in case of disk failure, but this is a very rare event, and will be
recovered over time by filling up cache again. So, in case rock is on its
own private disk/partition, I feel very well to disable journal completely.
For a start:
https://askubuntu.com/questions/573957/disabling-journaling-in-ubuntu-14-04

Dunno whether possible using AWS. As I like to have full control, I do not
use AWS at all.



--
Sent from: http://squid-web-proxy-cache.1019090.n4.nabble.com/Squid-Users-f1019091.html


From uhlar at fantomas.sk  Tue Jan 23 14:09:48 2018
From: uhlar at fantomas.sk (Matus UHLAR - fantomas)
Date: Tue, 23 Jan 2018 15:09:48 +0100
Subject: [squid-users] rock storage and max-swap-rate
In-Reply-To: <1516668452215-0.post@n4.nabble.com>
References: <CAHvB88xY6_40ge1M-W0vyguVeDQCy=s15PGkb_PECdAs91f-Kw@mail.gmail.com>
 <1516595739547-0.post@n4.nabble.com>
 <C235FA0D-C26F-4915-99AB-6184AEDDFCAA@gmail.com>
 <1516668452215-0.post@n4.nabble.com>
Message-ID: <20180123140948.GA32324@fantomas.sk>

>>Could you please elaborate? What?s wrong with rock on ext4? <

On 22.01.18 17:47, reinerotto wrote:
>Default ext4 uses a "journal" of the modifications. Which adds I/O.
>Timestamps of filemods are other I/Os. I do not think, that these features
>are required for rock. Disabling journal completely will cause loss of data
>(cached) in case of disk failure, but this is a very rare event, and will be
>recovered over time by filling up cache again. So, in case rock is on its
>own private disk/partition, I feel very well to disable journal completely.

I believe that journal is only wirtten to, when you make change at
filesystem level, like creating or removing files. 

Since rock storage is one file, journaling only affects it when you create,
enlarge or shrink it, not otherwise.

Thus, ext4 and journalling should not affect the speed of standad work over
rock store.

Please correct me if I'm wrong.

-- 
Matus UHLAR - fantomas, uhlar at fantomas.sk ; http://www.fantomas.sk/
Warning: I wish NOT to receive e-mail advertising to this address.
Varovanie: na tuto adresu chcem NEDOSTAVAT akukolvek reklamnu postu.
WinError #99999: Out of error messages.


From augustus_meyer at gmx.net  Tue Jan 23 15:26:42 2018
From: augustus_meyer at gmx.net (reinerotto)
Date: Tue, 23 Jan 2018 08:26:42 -0700 (MST)
Subject: [squid-users] rock storage and max-swap-rate
In-Reply-To: <20180123140948.GA32324@fantomas.sk>
References: <CAHvB88xY6_40ge1M-W0vyguVeDQCy=s15PGkb_PECdAs91f-Kw@mail.gmail.com>
 <1516595739547-0.post@n4.nabble.com>
 <C235FA0D-C26F-4915-99AB-6184AEDDFCAA@gmail.com>
 <1516668452215-0.post@n4.nabble.com> <20180123140948.GA32324@fantomas.sk>
Message-ID: <1516721202294-0.post@n4.nabble.com>

>I believe that journal is only wirtten to, when you make change at
filesystem level, like creating or removing files.<
This is more or less correct only, in case the _default_ journal strategy
"ordered" is used.
But even then, according to the docs, "metadata" is journalled. Which also
includes timestamps.
Especially noteworthy here:  Last modification of file.  

>Since rock storage is one file, journaling only affects it when you create,
enlarge or shrink it, not otherwise.<
So this general statement is definitely not correct. 




--
Sent from: http://squid-web-proxy-cache.1019090.n4.nabble.com/Squid-Users-f1019091.html


From david at articatech.com  Tue Jan 23 16:00:05 2018
From: david at articatech.com (David Touzeau)
Date: Tue, 23 Jan 2018 17:00:05 +0100
Subject: [squid-users] v4.0.22 error:transaction-end-before-headers
	using transparent SSL method
Message-ID: <013301d39463$3c3cff60$b4b6fe20$@articatech.com>

Notice, it appears on both http/https ports 

Transparent Ports are freezing each 10 minutes.

I mention that in normal port there is no issue, the issue can be generated
only on transparent mode.

 

 

 

De : squid-users [mailto:squid-users-bounces at lists.squid-cache.org] De la
part de David Touzeau
Envoy? : lundi 22 janvier 2018 23:39
? : squid-users at lists.squid-cache.org
<mailto:squid-users at lists.squid-cache.org> 
Objet : [squid-users] v4.0.22 error:transaction-end-before-headers using
transparent SSL method

 

Hi

 

I?m using Squid Cache: Version 4.0.22 in transparent method

 

After several times the SSL port going into ?  freeze  mode ? and write in
logs

 

1516660011.849 000000 192.168.1.214 NONE/000 0 NONE
error:transaction-end-before-headers ?

 

Doing a squid -k reconfigure release all freeze requests and proxy run in
normal behavior and return back to freeze mode after 1 or 2 hours

 

How to fix this issue ?

 

Using the defined configuration :

 

http_port 192.168.1.1:50634  intercept disable-pmtu-discovery=transparent
name=MyPortNameID27  

https_port 192.168.1.1:50635  intercept disable-pmtu-discovery=transparent
name=MyPortNameID28 ssl-bump  generate-host-certificates=on
dynamic_cert_mem_cache_size=4MB cert=/etc/squid3/ssl/cb623e9bf

c65772f68b84393604cd6ea.dyn tls-dh=/etc/squid3/ssl/dhparam.pem

sslcrtd_program /lib/squid3/security_file_certgen -s
/var/lib/squid/session/ssl/ssl_db -M 8MB

sslcrtd_children 16 startup=5 idle=1

acl FakeCert ssl::server_name .apple.com

acl FakeCert ssl::server_name .icloud.com

acl FakeCert ssl::server_name .mzstatic.com

acl FakeCert ssl::server_name .dropbox.com

acl ssl_step1 at_step SslBump1

acl ssl_step2 at_step SslBump2

acl ssl_step3 at_step SslBump3

ssl_bump peek ssl_step1

ssl_bump splice GlobalWhitelistDSTNet

ssl_bump splice GlobalWhitelistDomainsRx

ssl_bump splice GlobalWhitelistDomains

ssl_bump splice FakeCert

ssl_bump splice all

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20180123/f70abfbb/attachment.htm>

From david at articatech.com  Tue Jan 23 23:41:32 2018
From: david at articatech.com (David Touzeau)
Date: Wed, 24 Jan 2018 00:41:32 +0100
Subject: [squid-users] 3.5.27: Compilation failed CRYPTO_LOCK_X509 on Debian
	9
Message-ID: <01b001d394a3$b2be9190$183bb4b0$@articatech.com>

Hi all

 

Did anyone have encountered and fixed this issue :

 

Make failed with the following error :

 

/bin/bash ../../libtool  --tag=CXX   --mode=compile g++ -DHAVE_CONFIG_H
-I../.. -I../../include -I../../lib -I../../src -I../../include  -isystem
/usr/include/mit-krb5  -I/usr/include/libxml2  -I/usr/include/libxml2 -Wall
-Wpointer-arith -Wwrite-strings -Wcomments -Wshadow -Woverloaded-virtual
-Werror -pipe -D_REENTRANT -m64  -I/usr/include/p11-kit-1 -g -O2 -c -o
PortCfg.lo PortCfg.cc

libtool: compile:  g++ -DHAVE_CONFIG_H -I../.. -I../../include -I../../lib
-I../../src -I../../include -isystem /usr/include/mit-krb5
-I/usr/include/libxml2 -I/usr/include/libxml2 -Wall -Wpointer-arith
-Wwrite-strings -Wcomments -Wshadow -Woverloaded-virtual -Werror -pipe
-D_REENTRANT -m64 -I/usr/include/p11-kit-1 -g -O2 -c PortCfg.cc  -fPIC -DPIC
-o .libs/PortCfg.o

In file included from ../../src/anyp/PortCfg.h:18:0,

                 from PortCfg.cc:10:

../../src/ssl/gadgets.h:83:45: error: 'CRYPTO_LOCK_X509' was not declared in
this scope

typedef LockingPointer<X509, X509_free_cpp, CRYPTO_LOCK_X509> X509_Pointer;

                                             ^~~~~~~~~~~~~~~~

../../src/ssl/gadgets.h:83:61: error: template argument 3 is invalid

typedef LockingPointer<X509, X509_free_cpp, CRYPTO_LOCK_X509> X509_Pointer;

                                                             ^

../../src/ssl/gadgets.h:89:53: error: 'CRYPTO_LOCK_EVP_PKEY' was not
declared in this scope

typedef LockingPointer<EVP_PKEY, EVP_PKEY_free_cpp, CRYPTO_LOCK_EVP_PKEY>
EVP_PKEY_Pointer;

                                                     ^~~~~~~~~~~~~~~~~~~~

../../src/ssl/gadgets.h:89:73: error: template argument 3 is invalid

typedef LockingPointer<EVP_PKEY, EVP_PKEY_free_cpp, CRYPTO_LOCK_EVP_PKEY>
EVP_PKEY_Pointer;

                                                                         ^

../../src/ssl/gadgets.h:116:43: error: 'CRYPTO_LOCK_SSL' was not declared in
this scope

typedef LockingPointer<SSL, SSL_free_cpp, CRYPTO_LOCK_SSL> SSL_Pointer;

                                           ^~~~~~~~~~~~~~~

../../src/ssl/gadgets.h:116:58: error: template argument 3 is invalid

typedef LockingPointer<SSL, SSL_free_cpp, CRYPTO_LOCK_SSL> SSL_Pointer;

 

 

Configure options :

./configure --prefix=/usr --build=x86_64-linux-gnu
--includedir=${prefix}/include --mandir=${prefix}/share/man
--infodir=${prefix}/share/info --localstatedir=/var
--libexecdir=${prefix}/lib/squid3 --disable-maintainer-mode
--disable-dependency-tracking --srcdir=. --datadir=/usr/share/squid3
--sysconfdir=/etc/squid3 --enable-gnuregex --enable-removal-policy=heap
--enable-follow-x-forwarded-for --disable-cache-digests
--enable-http-violations --enable-removal-policies=lru,heap --enable-arp-acl
--enable-truncate --with-large-files --with-pthreads --enable-esi
--enable-storeio=aufs,diskd,ufs,rock --enable-x-accelerator-vary --with-dl
--enable-linux-netfilter --with-netfilter-conntrack --enable-wccpv2
--enable-eui --enable-auth --enable-auth-basic --enable-snmp --enable-icmp
--enable-auth-digest --enable-log-daemon-helpers
--enable-url-rewrite-helpers --enable-auth-ntlm --with-default-user=squid
--enable-icap-client --disable-cache-digests --enable-poll --enable-epoll
--enable-async-io=128 --enable-zph-qos --enable-delay-pools
--enable-http-violations --enable-url-maps --enable-ecap --enable-ssl
--with-openssl --enable-ssl-crtd --enable-xmalloc-statistics
--enable-ident-lookups --with-filedescriptors=65536 --with-aufs-threads=128
--disable-arch-native

                                                          ^

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20180124/8ad43b9f/attachment.htm>

From squid3 at treenet.co.nz  Wed Jan 24 00:21:00 2018
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Wed, 24 Jan 2018 13:21:00 +1300
Subject: [squid-users] 3.5.27: Compilation failed CRYPTO_LOCK_X509 on
 Debian 9
In-Reply-To: <01b001d394a3$b2be9190$183bb4b0$@articatech.com>
References: <01b001d394a3$b2be9190$183bb4b0$@articatech.com>
Message-ID: <5b363a37-bd4b-962e-ee14-f23516284c85@treenet.co.nz>

Squid-3 on Debian 9+ requires the libssl1.0-dev package instead of the
normal libssl-dev package.

FYI: It is much better to be using the Squid-4 package currently in
Debian experimental (due to policy reasons only) if you are building for
SSL-Bump behaviours. That package also supports TLS to cache_peer or
origin servers natively with GnuTLS.

Amos


From david at articatech.com  Wed Jan 24 00:28:52 2018
From: david at articatech.com (David Touzeau)
Date: Wed, 24 Jan 2018 01:28:52 +0100
Subject: [squid-users] 3.5.27: Compilation failed CRYPTO_LOCK_X509 on
	Debian 9
In-Reply-To: <5b363a37-bd4b-962e-ee14-f23516284c85@treenet.co.nz>
References: <01b001d394a3$b2be9190$183bb4b0$@articatech.com>
 <5b363a37-bd4b-962e-ee14-f23516284c85@treenet.co.nz>
Message-ID: <01c701d394aa$51539660$f3fac320$@articatech.com>



-----Message d'origine-----
De : squid-users [mailto:squid-users-bounces at lists.squid-cache.org] De la
part de Amos Jeffries
Envoy? : mercredi 24 janvier 2018 01:21
? : squid-users at lists.squid-cache.org
Objet : Re: [squid-users] 3.5.27: Compilation failed CRYPTO_LOCK_X509 on
Debian 9

Squid-3 on Debian 9+ requires the libssl1.0-dev package instead of the
normal libssl-dev package.

FYI: It is much better to be using the Squid-4 package currently in Debian
experimental (due to policy reasons only) if you are building for SSL-Bump
behaviours. That package also supports TLS to cache_peer or origin servers
natively with GnuTLS.

Amos



Thanks Amos

That's why i have be done before but squid4 is seems unstable when using
transparent method.

We are facing on an issue on ports freeze  see topic " v4.0.22 
error:transaction-end-before-headers using transparent SSL method"





From squid3 at treenet.co.nz  Wed Jan 24 02:16:34 2018
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Wed, 24 Jan 2018 15:16:34 +1300
Subject: [squid-users] v4.0.22 error:transaction-end-before-headers
 using transparent SSL method
In-Reply-To: <013301d39463$3c3cff60$b4b6fe20$@articatech.com>
References: <013301d39463$3c3cff60$b4b6fe20$@articatech.com>
Message-ID: <19e6f5ee-ef24-f884-5963-75c86089cba1@treenet.co.nz>

As an experiment does the issue remain if you use the memory-only mode
for the security_file_certgen helper in 4.0.23?

Amos


From david at articatech.com  Wed Jan 24 09:49:36 2018
From: david at articatech.com (david at articatech.com)
Date: Wed, 24 Jan 2018 10:49:36 +0100
Subject: [squid-users] v4.0.22 error:transaction-end-before-headers
	using transparent SSL method
In-Reply-To: <19e6f5ee-ef24-f884-5963-75c86089cba1@treenet.co.nz>
References: <013301d39463$3c3cff60$b4b6fe20$@articatech.com>
 <19e6f5ee-ef24-f884-5963-75c86089cba1@treenet.co.nz>
Message-ID: <01df01d394f8$a53cc5a0$efb650e0$@articatech.com>


Hi Amos,

I did not find any documentation related to "memory-only" on sslcrtd_program 
features.

Did you have an example ?







From squid3 at treenet.co.nz  Wed Jan 24 11:01:32 2018
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 25 Jan 2018 00:01:32 +1300
Subject: [squid-users] v4.0.22 error:transaction-end-before-headers
 using transparent SSL method
In-Reply-To: <01df01d394f8$a53cc5a0$efb650e0$@articatech.com>
References: <013301d39463$3c3cff60$b4b6fe20$@articatech.com>
 <19e6f5ee-ef24-f884-5963-75c86089cba1@treenet.co.nz>
 <01df01d394f8$a53cc5a0$efb650e0$@articatech.com>
Message-ID: <a2d14bf5-6643-662a-b0f6-d45b519e1caa@treenet.co.nz>

On 24/01/18 22:49, david wrote:
> 
> Hi Amos,
> 
> I did not find any documentation related to "memory-only" on sslcrtd_program 
> features.
> 
> Did you have an example ?
> 

It is a new behaviour of the helper itself. Simply omit the -M and -s
options from its command line. Requires the helper be built from the v4
sources.

Amos


From alex at dvm.esines.cu  Wed Jan 24 13:59:33 2018
From: alex at dvm.esines.cu (=?UTF-8?Q?Alex_Guti=c3=a9rrez_Mart=c3=adnez?=)
Date: Wed, 24 Jan 2018 08:59:33 -0500
Subject: [squid-users] log problem
Message-ID: <cc248b89-1907-186a-d5c6-15278b7764fc@dvm.esines.cu>

Hello comunity, im using squid 3.3.8 on ubuntu 14.04.02 LTS. I have 
implemented sqstat on this server to monitor my bandwidth. My problem is 
simple, i need to remove from my log the line created by sqstat.

1516801891.375????? 1 10.28.27.36 TCP_MISS/200 25526 GET 
cache_object://localhost/active_requests - HIER_NONE/- text/plain


I tried using "access_log" directive, but until now the only thing i 
acomplish is stop my squid using a bad configuration.

Does anyone have an idea of how to solve this problem?

-- 
Saludos Cordiales

Lic. Alex Guti?rrez Mart?nez

Tel. +53 7 2710327

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20180124/952080e5/attachment.htm>

From squid3 at treenet.co.nz  Wed Jan 24 14:44:31 2018
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 25 Jan 2018 03:44:31 +1300
Subject: [squid-users] log problem
In-Reply-To: <cc248b89-1907-186a-d5c6-15278b7764fc@dvm.esines.cu>
References: <cc248b89-1907-186a-d5c6-15278b7764fc@dvm.esines.cu>
Message-ID: <32e5ab67-c0b0-6c37-42a9-b5e32a59e1fa@treenet.co.nz>

On 25/01/18 02:59, Alex Guti?rrez Mart?nez wrote:
> Hello comunity, im using squid 3.3.8 on ubuntu 14.04.02 LTS. I have
> implemented sqstat on this server to monitor my bandwidth. My problem is
> simple, i need to remove from my log the line created by sqstat.
> 
> 1516801891.375????? 1 10.28.27.36 TCP_MISS/200 25526 GET
> cache_object://localhost/active_requests - HIER_NONE/- text/plain
> 
> 
> I tried using "access_log" directive, but until now the only thing i
> acomplish is stop my squid using a bad configuration.
> 
> Does anyone have an idea of how to solve this problem?
> 

access_log is the way to go, using the 'manager' ACL.

Somewhat like this:

  access_log /var/log/squid/access.log squid !manager


... or if you want to log other manager access *except* for the sqstat
ones. Then you will need an ACL that uniquely identifies sqstat instead
of manager.


Amos


From yvoinov at gmail.com  Thu Jan 25 01:25:17 2018
From: yvoinov at gmail.com (Yuri)
Date: Thu, 25 Jan 2018 07:25:17 +0600
Subject: [squid-users] log problem
In-Reply-To: <32e5ab67-c0b0-6c37-42a9-b5e32a59e1fa@treenet.co.nz>
References: <cc248b89-1907-186a-d5c6-15278b7764fc@dvm.esines.cu>
 <32e5ab67-c0b0-6c37-42a9-b5e32a59e1fa@treenet.co.nz>
Message-ID: <46f89607-edf6-768b-c238-4fe7b1d18b97@gmail.com>


-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA256
?
Everything is a little worse. If you need a password to access the
cachemanager - it will shown in the logs. I believe that this is a bug
and a hole in security.

Preventing by ACL can be workaround, but hardly this is feature.


24.01.2018 20:44, Amos Jeffries ?????:
> On 25/01/18 02:59, Alex Guti?rrez Mart?nez wrote: >> Hello comunity, im using squid 3.3.8 on ubuntu 14.04.02 LTS. I have
>> implemented sqstat on this server to monitor my bandwidth. My problem
is >> simple, i need to remove from my log the line created by sqstat.
>> >> 1516801891.375 1 10.28.27.36 TCP_MISS/200 25526 GET >>
cache_object://localhost/active_requests - HIER_NONE/- text/plain >> >>
>> I tried using "access_log" directive, but until now the only thing i
>> acomplish is stop my squid using a bad configuration. >> >> Does
anyone have an idea of how to solve this problem? >> > > access_log is
the way to go, using the 'manager' ACL. > > Somewhat like this: > >
access_log /var/log/squid/access.log squid !manager > > > ... or if you
want to log other manager access *except* for the sqstat > ones. Then
you will need an ACL that uniquely identifies sqstat instead > of
manager. > > > Amos > _______________________________________________ >
squid-users mailing list > squid-users at lists.squid-cache.org >
http://lists.squid-cache.org/listinfo/squid-users
- -- 
*****************************
* C++20 : Bug to the future *
*****************************
-----BEGIN PGP SIGNATURE-----
?
iQGzBAEBCAAdFiEEUAYDxOalHloZaGP7S+6Uoz43Q6cFAlppMewACgkQS+6Uoz43
Q6f3Sgv/QAuPW4CQNrK5TGg91ZMW7DHwoLUHBpbu3VRz6WEYqaiLBHGfk2WytVKj
vprVQVtqdNqVCTbaqAa5RTL7vCH2XG8n91pRM0PWjtlN72SOOO7Cl0s2UBbBb/2Q
XJNwx/ALPAhk5AoBPm+CnuDI5pz9U3vNe6972bQK0uwRuVUWU6e+PGBJ/LeqqVnU
ngCyhzUJ5AJZiyEQyqIxqhb8kwKy0XaTzHM3lNFa8h3kFLzREc9fQnH+1P6n9Ny2
TBaIWW1WtzCePL159GYQ2TMryJ0SKOKIL45kvjzgaYoiOnWQSpykae4gRr124TMj
aJOyOr9gjlC5OXlORWXtLlU30Qad4B2hFnEwsdadUN5kxw2akPpia1NXPe4Hr7Uz
Jom88/V4fSYaTCHu3JpWP0amMNobvXTIWAo6uSbNehKNPW1Rclg11TdqHoXxjmGj
ZP9ZVZIUtH6X0gHiujnhxuFYx7STYq7D1utj/D1xZ/rtHCo8LsIA/sB0dy10bcx7
4he3sIRZ
=QD8X
-----END PGP SIGNATURE-----

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20180125/a04a17d6/attachment.htm>

From yvoinov at gmail.com  Thu Jan 25 01:39:58 2018
From: yvoinov at gmail.com (Yuri)
Date: Thu, 25 Jan 2018 07:39:58 +0600
Subject: [squid-users] log problem
In-Reply-To: <46f89607-edf6-768b-c238-4fe7b1d18b97@gmail.com>
References: <cc248b89-1907-186a-d5c6-15278b7764fc@dvm.esines.cu>
 <32e5ab67-c0b0-6c37-42a9-b5e32a59e1fa@treenet.co.nz>
 <46f89607-edf6-768b-c238-4fe7b1d18b97@gmail.com>
Message-ID: <9eddb19c-411e-c2dd-99bd-5a2be46a98f5@gmail.com>

In order not to be unfounded:

https://bugs.squid-cache.org/show_bug.cgi?id=4572

I found workaround more than year ago, however I believe but still exists.

PS. It's elementary to reproduce. Just specify cachemgr_passwd in
squid.conf and do not disable password access to cachemgr stats. Then
access to cachemgr from any tool like sqstat - with password (basic
auth) - and see what will in access.log. Congrats, you just show your
proxy manager password to all stats tool and anybody who watch your
statistics reports.

25.01.2018 07:25, Yuri ?????:
>
> Everything is a little worse. If you need a password to access the
> cachemanager - it will shown in the logs. I believe that this is a bug
> and a hole in security.
>
> Preventing by ACL can be workaround, but hardly this is feature.
>
>
> 24.01.2018 20:44, Amos Jeffries ?????:
> > On 25/01/18 02:59, Alex Guti?rrez Mart?nez wrote:
> >> Hello comunity, im using squid 3.3.8 on ubuntu 14.04.02 LTS. I have
> >> implemented sqstat on this server to monitor my bandwidth. My
> problem is
> >> simple, i need to remove from my log the line created by sqstat.
> >>
> >> 1516801891.375????? 1 10.28.27.36 TCP_MISS/200 25526 GET
> >> cache_object://localhost/active_requests - HIER_NONE/- text/plain
> >>
> >>
> >> I tried using "access_log" directive, but until now the only thing i
> >> acomplish is stop my squid using a bad configuration.
> >>
> >> Does anyone have an idea of how to solve this problem?
> >>
>
> > access_log is the way to go, using the 'manager' ACL.
>
> > Somewhat like this:
>
> >?? access_log /var/log/squid/access.log squid !manager
>
>
> > ... or if you want to log other manager access *except* for the sqstat
> > ones. Then you will need an ACL that uniquely identifies sqstat instead
> > of manager.
>
>
> > Amos
> > _______________________________________________
> > squid-users mailing list
> > squid-users at lists.squid-cache.org
> > http://lists.squid-cache.org/listinfo/squid-users
>
>
-- 
*****************************
* C++20 : Bug to the future *
*****************************

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20180125/2e993a9b/attachment.htm>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: signature.asc
Type: application/pgp-signature
Size: 659 bytes
Desc: OpenPGP digital signature
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20180125/2e993a9b/attachment.sig>

From squid3 at treenet.co.nz  Thu Jan 25 01:55:49 2018
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 25 Jan 2018 14:55:49 +1300
Subject: [squid-users] log problem
In-Reply-To: <46f89607-edf6-768b-c238-4fe7b1d18b97@gmail.com>
References: <cc248b89-1907-186a-d5c6-15278b7764fc@dvm.esines.cu>
 <32e5ab67-c0b0-6c37-42a9-b5e32a59e1fa@treenet.co.nz>
 <46f89607-edf6-768b-c238-4fe7b1d18b97@gmail.com>
Message-ID: <3ea6bc0e-d28e-f1c8-28c8-9491f2564adc@treenet.co.nz>

On 25/01/18 14:25, Yuri wrote:
> 
> Everything is a little worse. If you need a password to access the
> cachemanager - it will shown in the logs.

"worse" implies it was better some time beforehand.

The old manager API is the one which places password in clear-text in
the URLs. It may not have told you that was what it was doing, but still
the security was really crap.

If you are using the current API with http(s):// URLs they do not
contain any credentials in the URL and you can configure authentication
more secure than Basic to be used by using http_access permissions
instead of the cachemgr_passwd mechanism.


> I believe that this is a bug
> and a hole in security.
> 

Using the old insecure manager API is a hole yes. But not a new one.


> Preventing by ACL can be workaround, but hardly this is feature.
> 

This is backward compatibility feature for people still using tools that
require the old API. Making a crappy insecure API "secure" requires work.

Amos


From yvoinov at gmail.com  Thu Jan 25 01:59:10 2018
From: yvoinov at gmail.com (Yuri)
Date: Thu, 25 Jan 2018 07:59:10 +0600
Subject: [squid-users] log problem
In-Reply-To: <3ea6bc0e-d28e-f1c8-28c8-9491f2564adc@treenet.co.nz>
References: <cc248b89-1907-186a-d5c6-15278b7764fc@dvm.esines.cu>
 <32e5ab67-c0b0-6c37-42a9-b5e32a59e1fa@treenet.co.nz>
 <46f89607-edf6-768b-c238-4fe7b1d18b97@gmail.com>
 <3ea6bc0e-d28e-f1c8-28c8-9491f2564adc@treenet.co.nz>
Message-ID: <f85013c1-770a-bdc6-b636-055820fd7425@gmail.com>

Amos, this is good news.

Is this clear documented anywhere to write good article in wiki about it?


25.01.2018 07:55, Amos Jeffries ?????:
> On 25/01/18 14:25, Yuri wrote:
>> Everything is a little worse. If you need a password to access the
>> cachemanager - it will shown in the logs.
> "worse" implies it was better some time beforehand.
>
> The old manager API is the one which places password in clear-text in
> the URLs. It may not have told you that was what it was doing, but still
> the security was really crap.
>
> If you are using the current API with http(s):// URLs they do not
> contain any credentials in the URL and you can configure authentication
> more secure than Basic to be used by using http_access permissions
> instead of the cachemgr_passwd mechanism.
>
>
>> I believe that this is a bug
>> and a hole in security.
>>
> Using the old insecure manager API is a hole yes. But not a new one.
>
>
>> Preventing by ACL can be workaround, but hardly this is feature.
>>
> This is backward compatibility feature for people still using tools that
> require the old API. Making a crappy insecure API "secure" requires work.
>
> Amos
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users

-- 
*****************************
* C++20 : Bug to the future *
*****************************



From squid3 at treenet.co.nz  Thu Jan 25 02:18:45 2018
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 25 Jan 2018 15:18:45 +1300
Subject: [squid-users] log problem
In-Reply-To: <f85013c1-770a-bdc6-b636-055820fd7425@gmail.com>
References: <cc248b89-1907-186a-d5c6-15278b7764fc@dvm.esines.cu>
 <32e5ab67-c0b0-6c37-42a9-b5e32a59e1fa@treenet.co.nz>
 <46f89607-edf6-768b-c238-4fe7b1d18b97@gmail.com>
 <3ea6bc0e-d28e-f1c8-28c8-9491f2564adc@treenet.co.nz>
 <f85013c1-770a-bdc6-b636-055820fd7425@gmail.com>
Message-ID: <9c21b60f-9ffd-0312-8cfd-828b5538e0ac@treenet.co.nz>

The API change was documented in
<http://www.squid-cache.org/Versions/v3/3.2/RELEASENOTES.html#ss2.13>

It does not explicitly call out the authentication side effects or
provide a config example though. The expectation was that most people
use tools to access the reports and the config changes would be
documented with the tools as they changed.

(The https:// access still has a few bugs to work out related domain
aliases and cert CommonName's.)

Amos

On 25/01/18 14:59, Yuri wrote:
> Amos, this is good news.
> 
> Is this clear documented anywhere to write good article in wiki about it?
> 
> 
> 25.01.2018 07:55, Amos Jeffries ?????:
>> On 25/01/18 14:25, Yuri wrote:
>>> Everything is a little worse. If you need a password to access the
>>> cachemanager - it will shown in the logs.
>> "worse" implies it was better some time beforehand.
>>
>> The old manager API is the one which places password in clear-text in
>> the URLs. It may not have told you that was what it was doing, but still
>> the security was really crap.
>>
>> If you are using the current API with http(s):// URLs they do not
>> contain any credentials in the URL and you can configure authentication
>> more secure than Basic to be used by using http_access permissions
>> instead of the cachemgr_passwd mechanism.
>>
>>
>>> I believe that this is a bug
>>> and a hole in security.
>>>
>> Using the old insecure manager API is a hole yes. But not a new one.
>>
>>
>>> Preventing by ACL can be workaround, but hardly this is feature.
>>>
>> This is backward compatibility feature for people still using tools that
>> require the old API. Making a crappy insecure API "secure" requires work.
>>
>> Amos
>> _______________________________________________
>> squid-users mailing list
>> squid-users at lists.squid-cache.org
>> http://lists.squid-cache.org/listinfo/squid-users
> 


From tarotapprentice at yahoo.com  Thu Jan 25 14:18:21 2018
From: tarotapprentice at yahoo.com (TarotApprentice)
Date: Fri, 26 Jan 2018 01:18:21 +1100
Subject: [squid-users] Squid 4.0.23 from Debian experimental
Message-ID: <066754F9-A98C-4118-AAA0-55541BBFDE4A@yahoo.com>

A few issues with 4.0.23

1. It doesn?t auto start (4.0.21 did) upon reboot, but can be started via a ?service squid start? command

2. It seems to want to create the cache directories every time it starts up and then complains that the directories already exist.

3. When stopping it there are two assertion messages about ESIparser, but it seems to shutdown anyway.

Have gone back to 3.5.23 but can reinstall it again if further testing required.

MarkJ


From david at articatech.com  Thu Jan 25 14:50:26 2018
From: david at articatech.com (David Touzeau)
Date: Thu, 25 Jan 2018 15:50:26 +0100
Subject: [squid-users] v4.0.22 error:transaction-end-before-headers
	using transparent SSL method
In-Reply-To: <a2d14bf5-6643-662a-b0f6-d45b519e1caa@treenet.co.nz>
References: <013301d39463$3c3cff60$b4b6fe20$@articatech.com>
 <19e6f5ee-ef24-f884-5963-75c86089cba1@treenet.co.nz>
 <01df01d394f8$a53cc5a0$efb650e0$@articatech.com>
 <a2d14bf5-6643-662a-b0f6-d45b519e1caa@treenet.co.nz>
Message-ID: <013901d395eb$d6188260$82498720$@articatech.com>

Thanks Amos for the tips.

The error was a python  helper that works on 3.5 but freeze on v4.
Forward code to php fix the issue 

Thanks again !



From gsutherland at gmail.com  Thu Jan 25 15:24:44 2018
From: gsutherland at gmail.com (SnowyGrant)
Date: Thu, 25 Jan 2018 08:24:44 -0700 (MST)
Subject: [squid-users] Squid 2.7 alternative to " acl okStatus http_status
 302 http_reply_access allow okStatus "
Message-ID: <1516893884586-0.post@n4.nabble.com>

We have an installation stuck on Squid 2.7, and need a way to deal with 302
Temporarily Redirected not working.

I see on 3.0+ I could use :

 acl okStatus http_status 302 
   http_reply_access allow okStatus 

But this isn't supported on 2.7

I'm seeing some old threads on using "rep_header Location" but haven't been
able to get any of them working properly.

If anyone has gotten this working, could you let me know what is working for
you?



--
Sent from: http://squid-web-proxy-cache.1019090.n4.nabble.com/Squid-Users-f1019091.html


From squid3 at treenet.co.nz  Thu Jan 25 15:26:47 2018
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Fri, 26 Jan 2018 04:26:47 +1300
Subject: [squid-users] v4.0.22 error:transaction-end-before-headers
 using transparent SSL method
In-Reply-To: <013901d395eb$d6188260$82498720$@articatech.com>
References: <013301d39463$3c3cff60$b4b6fe20$@articatech.com>
 <19e6f5ee-ef24-f884-5963-75c86089cba1@treenet.co.nz>
 <01df01d394f8$a53cc5a0$efb650e0$@articatech.com>
 <a2d14bf5-6643-662a-b0f6-d45b519e1caa@treenet.co.nz>
 <013901d395eb$d6188260$82498720$@articatech.com>
Message-ID: <1c82d26c-af91-e772-56f0-cec1137dbd18@treenet.co.nz>

On 26/01/18 03:50, David Touzeau wrote:
> Thanks Amos for the tips.
> 
> The error was a python  helper that works on 3.5 but freeze on v4.
> Forward code to php fix the issue 
> 

Can you supply some more details in case someone else has the same issue
and cant figure it out?

Amos


From squid3 at treenet.co.nz  Thu Jan 25 15:37:58 2018
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Fri, 26 Jan 2018 04:37:58 +1300
Subject: [squid-users] Squid 2.7 alternative to " acl okStatus
 http_status 302 http_reply_access allow okStatus "
In-Reply-To: <1516893884586-0.post@n4.nabble.com>
References: <1516893884586-0.post@n4.nabble.com>
Message-ID: <ae620379-3b80-b429-baed-3ef26194602d@treenet.co.nz>

On 26/01/18 04:24, SnowyGrant wrote:
> We have an installation stuck on Squid 2.7, and need a way to deal with 302
> Temporarily Redirected not working.
> 

Can you explain what "not working" means exactly?

ie. What is it you are wanting to happen versus what is actually happening?




Also, what reason is forcing this installation to stay on 2.7?

Amos


From gsutherland at gmail.com  Thu Jan 25 15:41:52 2018
From: gsutherland at gmail.com (SnowyGrant)
Date: Thu, 25 Jan 2018 08:41:52 -0700 (MST)
Subject: [squid-users] Squid 2.7 alternative to " acl okStatus
 http_status 302 http_reply_access allow okStatus "
In-Reply-To: <ae620379-3b80-b429-baed-3ef26194602d@treenet.co.nz>
References: <1516893884586-0.post@n4.nabble.com>
 <ae620379-3b80-b429-baed-3ef26194602d@treenet.co.nz>
Message-ID: <1516894912745-0.post@n4.nabble.com>

"not working" means the client doesn't complete the redirect, just dies on a
blank page.



--
Sent from: http://squid-web-proxy-cache.1019090.n4.nabble.com/Squid-Users-f1019091.html


From squid3 at treenet.co.nz  Thu Jan 25 16:04:10 2018
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Fri, 26 Jan 2018 05:04:10 +1300
Subject: [squid-users] Squid 2.7 alternative to " acl okStatus
 http_status 302 http_reply_access allow okStatus "
In-Reply-To: <1516894912745-0.post@n4.nabble.com>
References: <1516893884586-0.post@n4.nabble.com>
 <ae620379-3b80-b429-baed-3ef26194602d@treenet.co.nz>
 <1516894912745-0.post@n4.nabble.com>
Message-ID: <cb8435e4-661a-1d8a-ce90-57e9bf1ab1a9@treenet.co.nz>

On 26/01/18 04:41, SnowyGrant wrote:
> "not working" means the client doesn't complete the redirect, just dies on a
> blank page.
> 

HTTP does not contain any such concept as "page".

So ... what is *actually* happening at the HTTP level that Squid
operates with?
 please provide details in your answer of HTTP messages that are
happening on both network connections, and your squid.conf *_access
rules - all of them not just the ones you think are relevant.


Amos


From acrow at integrafin.co.uk  Fri Jan 26 09:30:05 2018
From: acrow at integrafin.co.uk (Alex Crow)
Date: Fri, 26 Jan 2018 09:30:05 +0000
Subject: [squid-users] Squid 4 and missing intermediate certs
Message-ID: <1be13e2d-a6cc-edf3-1687-caebe6783749@integrafin.co.uk>

Hi List,

I've just set up a new SSL interception proxy using peek/splice/bump 
using squid 4.0.22 and I'm getting SSL errors on some site indicating 
missing intermediate certs as described here:

https://blog.diladele.com/2015/04/21/fixing-x509_v_err_unable_to_get_issuer_cert_locally-on-ssl-bumping-squid/

I have read the wiki and I see this on the SslBumpExplicit page:

"Squid-4 <https://wiki.squid-cache.org/Squid-4> is capable of 
downloading missing intermediate CA certificates, like popular browsers do."

However I'm finding that I have to follow the procedure in the diladele 
article and manually install the intermediate certs into the PKI trust 
to work around this.

My interception config is like this:

ssl_bump splice localhost
ssl_bump peek step1 all
ssl_bump splice nobumpdoms
ssl_bump stare step2 all
ssl_bump bump all

nobumpdoms is an acl pointing to a file listing domains that should not 
be subject to interception, and works fine.

Is there something else I have to specify to get squid4 to behave as 
described on the wiki?

Many thanks,

Alex


--
This message is intended only for the addressee and may contain
confidential information. Unless you are that person, you may not
disclose its contents or use it in any way and are requested to delete
the message along with any attachments and notify us immediately.
This email is not intended to, nor should it be taken to, constitute advice.
The information provided is correct to our knowledge & belief and must not
be used as a substitute for obtaining tax, regulatory, investment, legal or
any other appropriate advice.

"Transact" is operated by Integrated Financial Arrangements Ltd.
29 Clement's Lane, London EC4N 7AE. Tel: (020) 7608 4900 Fax: (020) 7608 5300.
(Registered office: as above; Registered in England and Wales under
number: 3727592). Authorised and regulated by the Financial Conduct
Authority (entered on the Financial Services Register; no. 190856).
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20180126/f936282c/attachment.htm>

From veiko at linux.ee  Fri Jan 26 09:48:58 2018
From: veiko at linux.ee (Veiko Kukk)
Date: Fri, 26 Jan 2018 11:48:58 +0200
Subject: [squid-users] Frequent ANY_OLD_PARENT in access log
Message-ID: <CAG8vQeOpHaJV_tHbm_+5e0TN0w-fgOyt8HvP+0t3373gHozTpw@mail.gmail.com>

Hi,

We have frequent ANY_OLD_PARENT in Squid 3.5.25 access log (reverse proxy
mode).
Most of them succeed with TCP_MISS/200, some fail with 504 or other errors,
no dead parent detected in cache log.

What does ANY_OLD_PARENT mean?

-- 
Veiko
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20180126/cb80df26/attachment.htm>

From rousskov at measurement-factory.com  Fri Jan 26 17:22:32 2018
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Fri, 26 Jan 2018 10:22:32 -0700
Subject: [squid-users] Frequent ANY_OLD_PARENT in access log
In-Reply-To: <CAG8vQeOpHaJV_tHbm_+5e0TN0w-fgOyt8HvP+0t3373gHozTpw@mail.gmail.com>
References: <CAG8vQeOpHaJV_tHbm_+5e0TN0w-fgOyt8HvP+0t3373gHozTpw@mail.gmail.com>
Message-ID: <64a76525-8849-e1dc-604a-89356dd7edf7@measurement-factory.com>

On 01/26/2018 02:48 AM, Veiko Kukk wrote:
> What does ANY_OLD_PARENT mean?

ANY_OLD_PARENT (former ANY_PARENT?) means that Squid used the first
considered-alive parent it could reach. Squid does that when none of the
specific parent cache selection algorithms (e.g., userhash or carp) were
enabled, all enabled algorithms failed to find a suitable parent, or all
suitable parents found by those algorithms failed when Squid tried to
forward the request to them.

This hierarchy code is now documented at
https://wiki.squid-cache.org/SquidFaq/SquidLogs#Hierarchy_Codes

If you find problems with the above description or can improve it,
please edit the above wiki page.


Thank you,

Alex.


From rousskov at measurement-factory.com  Fri Jan 26 17:50:50 2018
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Fri, 26 Jan 2018 10:50:50 -0700
Subject: [squid-users] Squid 4 and missing intermediate certs
In-Reply-To: <1be13e2d-a6cc-edf3-1687-caebe6783749@integrafin.co.uk>
References: <1be13e2d-a6cc-edf3-1687-caebe6783749@integrafin.co.uk>
Message-ID: <da22240c-2f49-e8f9-8e70-eea9db163db1@measurement-factory.com>

On 01/26/2018 02:30 AM, Alex Crow wrote:

> I've just set up a new SSL interception proxy using peek/splice/bump
> using squid 4.0.22 and I'm getting SSL errors on some site indicating
> missing intermediate certs as described here:
> 
> https://blog.diladele.com/2015/04/21/fixing-x509_v_err_unable_to_get_issuer_cert_locally-on-ssl-bumping-squid/
> 
> I have read the wiki and I see this on the SslBumpExplicit page:
> 
> "Squid-4 <https://wiki.squid-cache.org/Squid-4> is capable of
> downloading missing intermediate CA certificates, like popular browsers do."
> 
> However I'm finding that I have to follow the procedure in the diladele
> article and manually install the intermediate certs into the PKI trust
> to work around this.


Several cases are possible here:

1. Squid is missing the root certificate used by the origin server.
Neither Squid nor browsers can fetch root certificates automatically
(for hopefully obvious reasons).

2. Squid is missing an intermediate certificate used by the origin
server, and the origin server provided no instructions on how to fetch
that missing certificate automatically. Neither Squid (for sure) nor
browsers (AFAIK) can fetch missing intermediate certificates
automatically if they are not given origin server instructions of where
to get them. Those instructions are usually given as various extension
fields in signed certificates.

3. Squid is missing an intermediate certificate used by the origin
server, the origin server provided instructions on how to fetch that
missing certificate automatically, but Squid does not understand/support
those instructions. There are several instruction formats/variants, and
Squid does not support some of them. Please consider adding that support
to Squid (requires writing code or sponsoring development).

4. Squid is missing an intermediate certificate used by the origin
server, the origin server provided instructions on how to fetch that
missing certificate automatically, Squid followed those instructions,
but something went wrong. Study detailed Squid debugging logs or post
them for analysis by others.

You need to study each error to understand which case applies to it.

To make matters worse, a combination of #1 and other cases is possible:
Sometimes, automatically fetching a missing certificate leads to
certificate validation problems that could have been avoided if Squid
had the right (and different) trusted certificate in the first place:
https://github.com/squid-cache/squid/commit/9ef7d9d5ddef54283cea4f1fdb7b3bbc1715755c


I doubt Squid logs enough information (by default) to quickly and easily
distinguish the four cases for a given error -- you may need to study
the origin server certificates and Squid logs. For example, #4 should
manifest itself as access.log errors associated with failed certificate
fetching requests.


As the solution for #1-2 or workaround for #3-4, if you trust the
missing certificate, manually add it to your trust store (which is what
you were doing).


HTH,

Alex.


From balmeida at ecoa44.co.cu  Fri Jan 26 20:54:48 2018
From: balmeida at ecoa44.co.cu (Bladimir Almeida)
Date: Fri, 26 Jan 2018 15:54:48 -0500 (CST)
Subject: [squid-users] a host can not access web browsing
Message-ID: <1857445293.145675.1517000088397.JavaMail.zimbra@ecoa44.co.cu>

Hi, I'm a network administrator of my company, I've been dealing with a computer that can access all the services for days, except that I can not surf the Internet. 
The message the browser sends me is that the server is rejecting the connections, however the other domain pc access without any problem to the web, and reinstalled the pc 2 
times and nothing, the only thing that this pc I have to install windows xp, because it works with a tool that requires that OS Greetings, please help me. 
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20180126/36e66dd5/attachment.htm>

From Antony.Stone at squid.open.source.it  Fri Jan 26 21:06:20 2018
From: Antony.Stone at squid.open.source.it (Antony Stone)
Date: Fri, 26 Jan 2018 22:06:20 +0100
Subject: [squid-users] a host can not access web browsing
In-Reply-To: <1857445293.145675.1517000088397.JavaMail.zimbra@ecoa44.co.cu>
References: <1857445293.145675.1517000088397.JavaMail.zimbra@ecoa44.co.cu>
Message-ID: <201801262206.21221.Antony.Stone@squid.open.source.it>

On Friday 26 January 2018 at 21:54:48, Bladimir Almeida wrote:

> Hi, I'm a network administrator of my company,

I wonder how you react to emails from your users which contain so little 
specific information as I see below.

> I've been dealing with a computer that can access all the services for days,
> except that I can not surf the Internet.

Was it previously able to surf the Internet?

If yes, when did this change?

Did anything else happen on your network at the same time as this change?

> The message

Please show us the full text of the message.

> the browser

Which browser are you using?

> sends me is that the server

Which server?

> is rejecting the connections,

Is any reason given?  Any more detail at all?

> however the other domain pc

This is a Windows domain?  Not just a simple "bunch of PCs on a network doing 
their own thing"?  Do you need to authenticate to the domain (on *any* PC, I 
don't just mean on the one giving the problems) in order to be able to access 
websites?

> access without any problem to the web, and reinstalled the pc 2 times

Was the PC with the problem reinstalled or reconfigured shortly before it 
stopped being able to access websites?

I'm wondering what makes you think a reinstall might resolve the problem - you 
might be right, but is there any information which leads you to believe this?

> and nothing, the only thing that this pc I have to install windows xp,
> because it works with a tool that requires that OS

Do you have any other Windows XP machines on your network?

If so, are any of those able to access websites?

> Greetings, please help me.

Please tell us where Squid fits into your question.


Antony.

-- 
How I want a drink, alcoholic of course, after the heavy chapters involving 
quantum mechanics.

 - mnemonic for 3.14159265358979

                                                   Please reply to the list;
                                                         please *don't* CC me.


From yvoinov at gmail.com  Fri Jan 26 21:08:42 2018
From: yvoinov at gmail.com (Yuri)
Date: Sat, 27 Jan 2018 03:08:42 +0600
Subject: [squid-users] a host can not access web browsing
In-Reply-To: <201801262206.21221.Antony.Stone@squid.open.source.it>
References: <1857445293.145675.1517000088397.JavaMail.zimbra@ecoa44.co.cu>
 <201801262206.21221.Antony.Stone@squid.open.source.it>
Message-ID: <295cf274-32e1-2777-34e6-b15b6d407d9d@gmail.com>

(somebody's have too much unlimited personal time :-))


27.01.2018 03:06, Antony Stone ?????:
> On Friday 26 January 2018 at 21:54:48, Bladimir Almeida wrote:
>
>> Hi, I'm a network administrator of my company,
> I wonder how you react to emails from your users which contain so little 
> specific information as I see below.
>
>> I've been dealing with a computer that can access all the services for days,
>> except that I can not surf the Internet.
> Was it previously able to surf the Internet?
>
> If yes, when did this change?
>
> Did anything else happen on your network at the same time as this change?
>
>> The message
> Please show us the full text of the message.
>
>> the browser
> Which browser are you using?
>
>> sends me is that the server
> Which server?
>
>> is rejecting the connections,
> Is any reason given?  Any more detail at all?
>
>> however the other domain pc
> This is a Windows domain?  Not just a simple "bunch of PCs on a network doing 
> their own thing"?  Do you need to authenticate to the domain (on *any* PC, I 
> don't just mean on the one giving the problems) in order to be able to access 
> websites?
>
>> access without any problem to the web, and reinstalled the pc 2 times
> Was the PC with the problem reinstalled or reconfigured shortly before it 
> stopped being able to access websites?
>
> I'm wondering what makes you think a reinstall might resolve the problem - you 
> might be right, but is there any information which leads you to believe this?
>
>> and nothing, the only thing that this pc I have to install windows xp,
>> because it works with a tool that requires that OS
> Do you have any other Windows XP machines on your network?
>
> If so, are any of those able to access websites?
>
>> Greetings, please help me.
> Please tell us where Squid fits into your question.
>
>
> Antony.
>

-- 
*****************************
* C++20 : Bug to the future *
*****************************



From squid3 at treenet.co.nz  Fri Jan 26 21:10:02 2018
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sat, 27 Jan 2018 10:10:02 +1300
Subject: [squid-users] a host can not access web browsing
In-Reply-To: <1857445293.145675.1517000088397.JavaMail.zimbra@ecoa44.co.cu>
References: <1857445293.145675.1517000088397.JavaMail.zimbra@ecoa44.co.cu>
Message-ID: <521164f0-a77e-b09a-b85c-7c610e53712c@treenet.co.nz>

On 27/01/18 09:54, Bladimir Almeida wrote:
> Hi, I'm a network administrator of my company, I've been dealing with a
> computer that can access all the services for days, except that I can
> not surf the Internet.
> The message the browser sends me is that the server is rejecting the
> connections, however the other domain pc access without any problem to
> the web, and reinstalled the pc 2
> times and nothing, the only thing that this pc I have to install windows
> xp, because it works with a tool that requires that OS Greetings, please
> help me.
> 

1) What does this have to do with Squid?

2) What are your Squid settings?

3) What do your squid logs show happening?


Amos


From squid3 at treenet.co.nz  Sat Jan 27 04:48:35 2018
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sat, 27 Jan 2018 17:48:35 +1300
Subject: [squid-users] Squid 4.0.23 from Debian experimental
In-Reply-To: <066754F9-A98C-4118-AAA0-55541BBFDE4A@yahoo.com>
References: <066754F9-A98C-4118-AAA0-55541BBFDE4A@yahoo.com>
Message-ID: <e7170e5f-9bad-56a5-c744-d4325e34820f@treenet.co.nz>

On 26/01/18 03:18, TarotApprentice wrote:
> A few issues with 4.0.23
> 
> 1. It doesn?t auto start (4.0.21 did) upon reboot, but can be started via a ?service squid start? command
> 

It appears that you have to run "systemctl enable squid" manually after
upgrade to get systemd to learn that there is a squid.service file now
existing.


> 2. It seems to want to create the cache directories every time it starts up and then complains that the directories already exist.
> 

What do you men by "complain"?

Checking that the dirs are correctly formatted on each start/restart has
always been happening. There should be a _notice_ in cache.log about
what is being done and that they exist, but no complaints if they do.


> 3. When stopping it there are two assertion messages about ESIparser, but it seems to shutdown anyway.
> 

Thank you for spotting this. I'm working on a fix now.


Amos


From uhlar at fantomas.sk  Sat Jan 27 10:13:02 2018
From: uhlar at fantomas.sk (Matus UHLAR - fantomas)
Date: Sat, 27 Jan 2018 11:13:02 +0100
Subject: [squid-users] a host can not access web browsing
In-Reply-To: <1857445293.145675.1517000088397.JavaMail.zimbra@ecoa44.co.cu>
References: <1857445293.145675.1517000088397.JavaMail.zimbra@ecoa44.co.cu>
Message-ID: <20180127101302.GB23443@fantomas.sk>

On 26.01.18 15:54, Bladimir Almeida wrote:
>Hi, I'm a network administrator of my company, I've been dealing with a computer that can access all the services for days, except that I can not surf the Internet.
>The message the browser sends me is that the server is rejecting the connections, however the other domain pc access without any problem to the web, and reinstalled the pc 2
>times and nothing, the only thing that this pc I have to install windows xp, because it works with a tool that requires that OS Greetings, please help me.

How do you know it's a squid problem?
Do you use squid in your network?
Did you configure it manually, via WPAD or do you intercept connections to
the world?
What do the squid logs say about the machines' IP address?
-- 
Matus UHLAR - fantomas, uhlar at fantomas.sk ; http://www.fantomas.sk/
Warning: I wish NOT to receive e-mail advertising to this address.
Varovanie: na tuto adresu chcem NEDOSTAVAT akukolvek reklamnu postu.
The early bird may get the worm, but the second mouse gets the cheese. 


From rentorbuy at yahoo.com  Sat Jan 27 17:33:16 2018
From: rentorbuy at yahoo.com (Vieri)
Date: Sat, 27 Jan 2018 17:33:16 +0000 (UTC)
Subject: [squid-users] TCP out of memory
In-Reply-To: <9b6d3bee-f185-6ec2-6ce6-0759ce022480@treenet.co.nz>
References: <601581447.203369.1515055873511.ref@mail.yahoo.com>
 <601581447.203369.1515055873511@mail.yahoo.com>
 <1042f27d-78e2-aa59-3fa7-804ded30ea13@treenet.co.nz>
 <1464426577.854952.1515142755546@mail.yahoo.com>
 <33a4d101-51c5-aac9-bacf-6c95de183bc1@treenet.co.nz>
 <775733626.1802411.1515363213807@mail.yahoo.com>
 <db3dd9ad-c49d-f649-99c8-f199b7844054@treenet.co.nz>
 <517226574.2508171.1515490819697@mail.yahoo.com>
 <6eaaa26c-5226-5f5a-02f1-a9f5df690cc9@treenet.co.nz>
 <2121386754.4153838.1516109825922@mail.yahoo.com>
 <9b6d3bee-f185-6ec2-6ce6-0759ce022480@treenet.co.nz>
Message-ID: <2031365082.1726869.1517074396136@mail.yahoo.com>

Hi,

I just wanted to add some information to this topic, although I'm not sure if it's related.


I noticed that if I set bypass=1 in squid.conf (regarding ICAP), and if I stop the local clamd service (not the c-icap service), then the clients see Squid's ERR_ICAP_FAILURE page.
Is this expected?

Vieri


From rousskov at measurement-factory.com  Sat Jan 27 17:41:13 2018
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Sat, 27 Jan 2018 10:41:13 -0700
Subject: [squid-users] TCP out of memory
In-Reply-To: <2031365082.1726869.1517074396136@mail.yahoo.com>
References: <601581447.203369.1515055873511.ref@mail.yahoo.com>
 <601581447.203369.1515055873511@mail.yahoo.com>
 <1042f27d-78e2-aa59-3fa7-804ded30ea13@treenet.co.nz>
 <1464426577.854952.1515142755546@mail.yahoo.com>
 <33a4d101-51c5-aac9-bacf-6c95de183bc1@treenet.co.nz>
 <775733626.1802411.1515363213807@mail.yahoo.com>
 <db3dd9ad-c49d-f649-99c8-f199b7844054@treenet.co.nz>
 <517226574.2508171.1515490819697@mail.yahoo.com>
 <6eaaa26c-5226-5f5a-02f1-a9f5df690cc9@treenet.co.nz>
 <2121386754.4153838.1516109825922@mail.yahoo.com>
 <9b6d3bee-f185-6ec2-6ce6-0759ce022480@treenet.co.nz>
 <2031365082.1726869.1517074396136@mail.yahoo.com>
Message-ID: <9d039760-db1d-7a5c-e4b4-fa6416515b84@measurement-factory.com>

On 01/27/2018 10:33 AM, Vieri wrote:

> I noticed that if I set bypass=1 in squid.conf (regarding ICAP), and
> if I stop the local clamd service (not the c-icap service), then the
> clients see Squid's ERR_ICAP_FAILURE page. Is this expected?

Difficult to say for sure without knowing what went wrong between Squid
and c-icap: Not all ICAP errors can be bypassed. Consider sharing a
(link to) compressed ALL,9 cache.log collected while reproducing the
problem using a single HTTP transaction through an otherwise idle Squid.

Alex.


From yvoinov at gmail.com  Sat Jan 27 17:47:55 2018
From: yvoinov at gmail.com (Yuri)
Date: Sat, 27 Jan 2018 23:47:55 +0600
Subject: [squid-users] TCP out of memory
In-Reply-To: <9d039760-db1d-7a5c-e4b4-fa6416515b84@measurement-factory.com>
References: <601581447.203369.1515055873511.ref@mail.yahoo.com>
 <601581447.203369.1515055873511@mail.yahoo.com>
 <1042f27d-78e2-aa59-3fa7-804ded30ea13@treenet.co.nz>
 <1464426577.854952.1515142755546@mail.yahoo.com>
 <33a4d101-51c5-aac9-bacf-6c95de183bc1@treenet.co.nz>
 <775733626.1802411.1515363213807@mail.yahoo.com>
 <db3dd9ad-c49d-f649-99c8-f199b7844054@treenet.co.nz>
 <517226574.2508171.1515490819697@mail.yahoo.com>
 <6eaaa26c-5226-5f5a-02f1-a9f5df690cc9@treenet.co.nz>
 <2121386754.4153838.1516109825922@mail.yahoo.com>
 <9b6d3bee-f185-6ec2-6ce6-0759ce022480@treenet.co.nz>
 <2031365082.1726869.1517074396136@mail.yahoo.com>
 <9d039760-db1d-7a5c-e4b4-fa6416515b84@measurement-factory.com>
Message-ID: <afbe1ea1-9aee-fb63-0706-51bd3cfc09ec@gmail.com>


-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA256
?
He's just disabled icap-based service without disabling icap itself. So
- yes - this is as expected.

Vieri, bupass=1 is different thing. This permit squid to bypass
adaptation in case of overloading icap service. And irrelevant thing you
done.

27.01.2018 23:41, Alex Rousskov ?????:
> On 01/27/2018 10:33 AM, Vieri wrote: > >> I noticed that if I set bypass=1 in squid.conf (regarding ICAP),
and >> if I stop the local clamd service (not the c-icap service), then
the >> clients see Squid's ERR_ICAP_FAILURE page. Is this expected? > >
Difficult to say for sure without knowing what went wrong between Squid
> and c-icap: Not all ICAP errors can be bypassed. Consider sharing a >
(link to) compressed ALL,9 cache.log collected while reproducing the >
problem using a single HTTP transaction through an otherwise idle Squid.
> > Alex. > _______________________________________________ >
squid-users mailing list > squid-users at lists.squid-cache.org >
http://lists.squid-cache.org/listinfo/squid-users
- -- 
*****************************
* C++20 : Bug to the future *
*****************************
-----BEGIN PGP SIGNATURE-----
?
iQGzBAEBCAAdFiEEUAYDxOalHloZaGP7S+6Uoz43Q6cFAlpsuzEACgkQS+6Uoz43
Q6cogwv8D1lqLBtJeIMIbwgv/u6OBEtYAZbx7hAQpj4Hic8SBYsnHJdxUtHYchkC
pDFcrxPyp0b/59U26ngg5pObOHT5tynWYgH2tGp0/raJgPddD2G+xKyztvpawuz8
c1zHvUmyYhwHM96T99eTsI0r4qUq+e91krhK+6JxvSHh0CM8NnwUGycOhKBqNRHE
HGWOjXCLUf9QTw/C4QG2slpO5TJbVvKJjO+lnLyTxBZr+hFmORjbsAXMOsz83Txb
k0u4XuipQOqu3CcfLE/rSZPUX/r5YRQiV9roZdfy/IjYOdsU0+SvnO37RZLr9Z26
hKuu0OaZKGYcKtzy20lQJanIDzUOv7sDvNfqM3tWGTRNnxRD/1S3s2eFyuSZSx7/
0G4rbs/oj/Y7Ksa9mlGW02QNmjOmrUMB1iVwQu3IBWeef7WUQtgvFV+JuSGY8+q/
V605z627s6S8+fHAPzZyLGt/J0kMLWGbImOFUIIBBA+0g1yx41bUswo2feEivzzb
CwRJXfhc
=ZP4V
-----END PGP SIGNATURE-----



From rousskov at measurement-factory.com  Sat Jan 27 19:08:17 2018
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Sat, 27 Jan 2018 12:08:17 -0700
Subject: [squid-users] TCP out of memory
In-Reply-To: <afbe1ea1-9aee-fb63-0706-51bd3cfc09ec@gmail.com>
References: <601581447.203369.1515055873511.ref@mail.yahoo.com>
 <601581447.203369.1515055873511@mail.yahoo.com>
 <1042f27d-78e2-aa59-3fa7-804ded30ea13@treenet.co.nz>
 <1464426577.854952.1515142755546@mail.yahoo.com>
 <33a4d101-51c5-aac9-bacf-6c95de183bc1@treenet.co.nz>
 <775733626.1802411.1515363213807@mail.yahoo.com>
 <db3dd9ad-c49d-f649-99c8-f199b7844054@treenet.co.nz>
 <517226574.2508171.1515490819697@mail.yahoo.com>
 <6eaaa26c-5226-5f5a-02f1-a9f5df690cc9@treenet.co.nz>
 <2121386754.4153838.1516109825922@mail.yahoo.com>
 <9b6d3bee-f185-6ec2-6ce6-0759ce022480@treenet.co.nz>
 <2031365082.1726869.1517074396136@mail.yahoo.com>
 <9d039760-db1d-7a5c-e4b4-fa6416515b84@measurement-factory.com>
 <afbe1ea1-9aee-fb63-0706-51bd3cfc09ec@gmail.com>
Message-ID: <4d382013-0a57-2105-da9a-e7b114480846@measurement-factory.com>

On 01/27/2018 10:47 AM, Yuri wrote:

> He's just disabled icap-based service without disabling icap itself. So
> - yes - this is as expected.

The above logic is flawed: Vieri told Squid to bypass (bypassable) ICAP
errors, but Squid did not bypass an ICAP error. Whether that outcome is
expected depends on whether that specific ICAP error was bypassable.

Yes, I understand that c-icap did not successfully process the message
after clamd went down, but that fact is not important here. What is
important here is how c-icap relayed that problem to Squid. That part I
do not know, so I cannot say whether Squid just could not bypass this
particular ICAP problem (i.e., Squid behavior is expected) or there is a
bug in Squid's bypass code.


> bupass=1 permit squid to bypass
> adaptation in case of overloading icap service. 

Yes, but service overload is _not_ the only problem that bypass=1 can
bypass. The documentation for that option describes the option scope:

> If set to 'on' or '1', the ICAP service is treated as
> optional. If the service cannot be reached or malfunctions,
> Squid will try to ignore any errors and process the message as
> if the service was not enabled. No all ICAP errors can be
> bypassed.  If set to 0, the ICAP service is treated as
> essential and all ICAP errors will result in an error page
> returned to the HTTP client.
> 
> Bypass is off by default: services are treated as essential.

Alex.


From rentorbuy at yahoo.com  Mon Jan 29 08:01:21 2018
From: rentorbuy at yahoo.com (Vieri)
Date: Mon, 29 Jan 2018 08:01:21 +0000 (UTC)
Subject: [squid-users] TCP out of memory
In-Reply-To: <4d382013-0a57-2105-da9a-e7b114480846@measurement-factory.com>
References: <601581447.203369.1515055873511.ref@mail.yahoo.com>
 <601581447.203369.1515055873511@mail.yahoo.com>
 <1042f27d-78e2-aa59-3fa7-804ded30ea13@treenet.co.nz>
 <1464426577.854952.1515142755546@mail.yahoo.com>
 <33a4d101-51c5-aac9-bacf-6c95de183bc1@treenet.co.nz>
 <775733626.1802411.1515363213807@mail.yahoo.com>
 <db3dd9ad-c49d-f649-99c8-f199b7844054@treenet.co.nz>
 <517226574.2508171.1515490819697@mail.yahoo.com>
 <6eaaa26c-5226-5f5a-02f1-a9f5df690cc9@treenet.co.nz>
 <2121386754.4153838.1516109825922@mail.yahoo.com>
 <9b6d3bee-f185-6ec2-6ce6-0759ce022480@treenet.co.nz>
 <2031365082.1726869.1517074396136@mail.yahoo.com>
 <9d039760-db1d-7a5c-e4b4-fa6416515b84@measurement-factory.com>
 <afbe1ea1-9aee-fb63-0706-51bd3cfc09ec@gmail.com>
 <4d382013-0a57-2105-da9a-e7b114480846@measurement-factory.com>
Message-ID: <575277400.2417531.1517212881553@mail.yahoo.com>

Hi,

I reproduced the problem, and saw that the c-icap server (or its squidclamav module) reports a 500 internal server error when clamd is down. I guess that's not bypassable?


The c-icap server log reports:

Mon Jan 29 08:30:35 2018, 5134/1290311424, squidclamav.c(1934) dconnect: Mon Jan 29 08:30:35 2018, 5134/1290311424, entering.
Mon Jan 29 08:30:35 2018, 5134/1290311424, squidclamav.c(2015) connectINET: Mon Jan 29 08:30:35 2018, 5134/1290311424, ERROR Can't connect on 127.0.0.1:3310.
Mon Jan 29 08:30:35 2018, 5134/1290311424, squidclamav.c(2015) connectINET: Mon Jan 29 08:30:35 2018, 5134/1290311424, ERROR Can't connect on 127.0.0.1:3310.
Mon Jan 29 08:30:35 2018, 5134/1290311424, squidclamav.c(744) squidclamav_end_of_data_handler: Mon Jan 29 08:30:35 2018, 5134/1290311424, ERROR Can't connect to Clamd daemon.
Mon Jan 29 08:30:35 2018, 5134/1290311424, An error occured in end-of-data handler !return code : -1, req->allow204=1, req->allow206=0


Here's Squid's log:

https://drive.google.com/file/d/18HmM8pOuDQmE4W_vwmSncXEeJSvgDjDo/view?usp=sharing

I was hoping I could relate this to the original topic, but I'm afraid they are two different issues.


Thanks,

Vieri


From acrow at integrafin.co.uk  Mon Jan 29 09:48:50 2018
From: acrow at integrafin.co.uk (Alex Crow)
Date: Mon, 29 Jan 2018 09:48:50 +0000
Subject: [squid-users] Squid 4 and missing intermediate certs
In-Reply-To: <da22240c-2f49-e8f9-8e70-eea9db163db1@measurement-factory.com>
References: <1be13e2d-a6cc-edf3-1687-caebe6783749@integrafin.co.uk>
 <da22240c-2f49-e8f9-8e70-eea9db163db1@measurement-factory.com>
Message-ID: <2645f16c-f0ae-6816-6c06-cd6c0854c97e@integrafin.co.uk>

On 26/01/18 17:50, Alex Rousskov wrote:
> On 01/26/2018 02:30 AM, Alex Crow wrote:
>
>> I've just set up a new SSL interception proxy using peek/splice/bump
>> using squid 4.0.22 and I'm getting SSL errors on some site indicating
>> missing intermediate certs as described here:
>>
>> https://blog.diladele.com/2015/04/21/fixing-x509_v_err_unable_to_get_issuer_cert_locally-on-ssl-bumping-squid/
>>
>> I have read the wiki and I see this on the SslBumpExplicit page:
>>
>> "Squid-4 <https://wiki.squid-cache.org/Squid-4> is capable of
>> downloading missing intermediate CA certificates, like popular browsers do."
>>
>> However I'm finding that I have to follow the procedure in the diladele
>> article and manually install the intermediate certs into the PKI trust
>> to work around this.
>
> Several cases are possible here:
>
> 1. Squid is missing the root certificate used by the origin server.
> Neither Squid nor browsers can fetch root certificates automatically
> (for hopefully obvious reasons).
>
> 2. Squid is missing an intermediate certificate used by the origin
> server, and the origin server provided no instructions on how to fetch
> that missing certificate automatically. Neither Squid (for sure) nor
> browsers (AFAIK) can fetch missing intermediate certificates
> automatically if they are not given origin server instructions of where
> to get them. Those instructions are usually given as various extension
> fields in signed certificates.
>
> 3. Squid is missing an intermediate certificate used by the origin
> server, the origin server provided instructions on how to fetch that
> missing certificate automatically, but Squid does not understand/support
> those instructions. There are several instruction formats/variants, and
> Squid does not support some of them. Please consider adding that support
> to Squid (requires writing code or sponsoring development).
>
> 4. Squid is missing an intermediate certificate used by the origin
> server, the origin server provided instructions on how to fetch that
> missing certificate automatically, Squid followed those instructions,
> but something went wrong. Study detailed Squid debugging logs or post
> them for analysis by others.
>
> You need to study each error to understand which case applies to it.
>
> To make matters worse, a combination of #1 and other cases is possible:
> Sometimes, automatically fetching a missing certificate leads to
> certificate validation problems that could have been avoided if Squid
> had the right (and different) trusted certificate in the first place:
> https://github.com/squid-cache/squid/commit/9ef7d9d5ddef54283cea4f1fdb7b3bbc1715755c
>
>
> I doubt Squid logs enough information (by default) to quickly and easily
> distinguish the four cases for a given error -- you may need to study
> the origin server certificates and Squid logs. For example, #4 should
> manifest itself as access.log errors associated with failed certificate
> fetching requests.
>
>
> As the solution for #1-2 or workaround for #3-4, if you trust the
> missing certificate, manually add it to your trust store (which is what
> you were doing).
>
>
> HTH,
>
> Alex.

Thanks very much Alex. I thought it might be something like that. I'm 
guessing it's most likely #3 or #4 as the site works direct from the 
browser.

Cheers

Alex
--
This message is intended only for the addressee and may contain
confidential information. Unless you are that person, you may not
disclose its contents or use it in any way and are requested to delete
the message along with any attachments and notify us immediately.
This email is not intended to, nor should it be taken to, constitute advice.
The information provided is correct to our knowledge & belief and must not
be used as a substitute for obtaining tax, regulatory, investment, legal or
any other appropriate advice.

"Transact" is operated by Integrated Financial Arrangements Ltd.
29 Clement's Lane, London EC4N 7AE. Tel: (020) 7608 4900 Fax: (020) 7608 5300.
(Registered office: as above; Registered in England and Wales under
number: 3727592). Authorised and regulated by the Financial Conduct
Authority (entered on the Financial Services Register; no. 190856).


From squid3 at treenet.co.nz  Mon Jan 29 10:18:51 2018
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Mon, 29 Jan 2018 23:18:51 +1300
Subject: [squid-users] Squid 4 and missing intermediate certs
In-Reply-To: <2645f16c-f0ae-6816-6c06-cd6c0854c97e@integrafin.co.uk>
References: <1be13e2d-a6cc-edf3-1687-caebe6783749@integrafin.co.uk>
 <da22240c-2f49-e8f9-8e70-eea9db163db1@measurement-factory.com>
 <2645f16c-f0ae-6816-6c06-cd6c0854c97e@integrafin.co.uk>
Message-ID: <7c8c89b5-c92e-15fd-a048-0a98191824ea@treenet.co.nz>

On 29/01/18 22:48, Alex Crow wrote:
> 
> Thanks very much Alex. I thought it might be something like that. I'm
> guessing it's most likely #3 or #4 as the site works direct from the
> browser.
> 

That does not preclude #1 or #2 from being possibilities.

It is very common to have a server with outdated ca certificates package
installed. Whereas client Browsers get their CA certificates regularly
upgraded.


Amos


From jduff344 at gmail.com  Mon Jan 29 10:41:50 2018
From: jduff344 at gmail.com (John Duff)
Date: Mon, 29 Jan 2018 11:41:50 +0100
Subject: [squid-users] Squid monitoring / Is it possible using squidclient
 command to retrieve only one metric ?
Message-ID: <CAOH2dyrAXR3GC8up8YbYYiyp7BwEQeu4+onZC74SbLa8KienZw@mail.gmail.com>

Hello,

I'm need to monitor squid activity. There're only somes values reported by
squidclient mgr:utilization I need.

Is there any tool or trick to get back just one value without using "|grep
" command ?

For instance, I just need to register the

Totals since cache startup:
sample_time = 1517222408.387484 (Mon, 29 Jan 2018 10:40:08 GMT)
*client_http.requests* *= 25*

Thank you for your help.

Regards
John
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20180129/17fc91ef/attachment.htm>

From squid3 at treenet.co.nz  Mon Jan 29 13:17:35 2018
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Tue, 30 Jan 2018 02:17:35 +1300
Subject: [squid-users] Squid monitoring / Is it possible using
 squidclient command to retrieve only one metric ?
In-Reply-To: <CAOH2dyrAXR3GC8up8YbYYiyp7BwEQeu4+onZC74SbLa8KienZw@mail.gmail.com>
References: <CAOH2dyrAXR3GC8up8YbYYiyp7BwEQeu4+onZC74SbLa8KienZw@mail.gmail.com>
Message-ID: <37837afd-9f60-2726-51a4-a600dc57500b@treenet.co.nz>

On 29/01/18 23:41, John Duff wrote:
> Hello,
> 
> I'm need to monitor squid activity. There're only somes values reported
> by squidclient mgr:utilization I need.?
> 
> Is there any tool or trick to get back just one value without using
> "|grep " command ?
> 
> For instance, I just need to register the?
> 
> Totals since cache startup:
> sample_time = 1517222408.387484 (Mon, 29 Jan 2018 10:40:08 GMT)
> *client_http.requests* *= 25*
> *

Individual state measurements are available over SNMP:
 <https://wiki.squid-cache.org/Features/Snmp#Squid_OIDs>


Amos


From rousskov at measurement-factory.com  Mon Jan 29 17:24:04 2018
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Mon, 29 Jan 2018 10:24:04 -0700
Subject: [squid-users] ICAP 500 is not bypassed
In-Reply-To: <575277400.2417531.1517212881553@mail.yahoo.com>
References: <601581447.203369.1515055873511.ref@mail.yahoo.com>
 <601581447.203369.1515055873511@mail.yahoo.com>
 <1042f27d-78e2-aa59-3fa7-804ded30ea13@treenet.co.nz>
 <1464426577.854952.1515142755546@mail.yahoo.com>
 <33a4d101-51c5-aac9-bacf-6c95de183bc1@treenet.co.nz>
 <775733626.1802411.1515363213807@mail.yahoo.com>
 <db3dd9ad-c49d-f649-99c8-f199b7844054@treenet.co.nz>
 <517226574.2508171.1515490819697@mail.yahoo.com>
 <6eaaa26c-5226-5f5a-02f1-a9f5df690cc9@treenet.co.nz>
 <2121386754.4153838.1516109825922@mail.yahoo.com>
 <9b6d3bee-f185-6ec2-6ce6-0759ce022480@treenet.co.nz>
 <2031365082.1726869.1517074396136@mail.yahoo.com>
 <9d039760-db1d-7a5c-e4b4-fa6416515b84@measurement-factory.com>
 <afbe1ea1-9aee-fb63-0706-51bd3cfc09ec@gmail.com>
 <4d382013-0a57-2105-da9a-e7b114480846@measurement-factory.com>
 <575277400.2417531.1517212881553@mail.yahoo.com>
Message-ID: <7a220b9e-7634-a2f2-4336-7b56e45a4814@measurement-factory.com>

On 01/29/2018 01:01 AM, Vieri wrote:

> I reproduced the problem, and saw that the c-icap server (or its
> squidclamav module) reports a 500 internal server error when clamd is
> down. I guess that's not bypassable?

The ICAP 500 status code does not preclude bypass. In fact, your Squid
attempts to bypass this ICAP server error:


> 2018/01/29 08:30:01.479 ... bypassing ... exception: Unsupported ICAP status code


Unfortunately, Squid bypass code then triggers an internal exception
which kills the bypass attempt and ends the HTTP transaction with
ICAP_ERR_OTHER:

> 2018/01/29 08:30:01.480 ... Throw: ModXact.cc:1848: exception: !disabled()
> 2018/01/29 08:30:01.480 ... Warning: reseting outcome: from ICAP_ECHO to ICAP_ERR_OTHER

That second exception looks like a Squid bug to me, but I have not
investigated it enough to be sure about that designation (not to mention
propose a fix). Your next steps revolve around these standard options:

https://wiki.squid-cache.org/SquidFaq/AboutSquid#How_to_add_a_new_Squid_feature.2C_enhance.2C_of_fix_something.3F


HTH,

Alex.


From rentorbuy at yahoo.com  Tue Jan 30 13:21:14 2018
From: rentorbuy at yahoo.com (Vieri)
Date: Tue, 30 Jan 2018 13:21:14 +0000 (UTC)
Subject: [squid-users] ICAP 500 is not bypassed
In-Reply-To: <7a220b9e-7634-a2f2-4336-7b56e45a4814@measurement-factory.com>
References: <601581447.203369.1515055873511.ref@mail.yahoo.com>
 <601581447.203369.1515055873511@mail.yahoo.com>
 <1042f27d-78e2-aa59-3fa7-804ded30ea13@treenet.co.nz>
 <1464426577.854952.1515142755546@mail.yahoo.com>
 <33a4d101-51c5-aac9-bacf-6c95de183bc1@treenet.co.nz>
 <775733626.1802411.1515363213807@mail.yahoo.com>
 <db3dd9ad-c49d-f649-99c8-f199b7844054@treenet.co.nz>
 <517226574.2508171.1515490819697@mail.yahoo.com>
 <6eaaa26c-5226-5f5a-02f1-a9f5df690cc9@treenet.co.nz>
 <2121386754.4153838.1516109825922@mail.yahoo.com>
 <9b6d3bee-f185-6ec2-6ce6-0759ce022480@treenet.co.nz>
 <2031365082.1726869.1517074396136@mail.yahoo.com>
 <9d039760-db1d-7a5c-e4b4-fa6416515b84@measurement-factory.com>
 <afbe1ea1-9aee-fb63-0706-51bd3cfc09ec@gmail.com>
 <4d382013-0a57-2105-da9a-e7b114480846@measurement-factory.com>
 <575277400.2417531.1517212881553@mail.yahoo.com>
 <7a220b9e-7634-a2f2-4336-7b56e45a4814@measurement-factory.com>
Message-ID: <2059133810.3314051.1517318474285@mail.yahoo.com>

Alex, thanks for your time.

Vieri


From Ralf.Hildebrandt at charite.de  Tue Jan 30 13:27:12 2018
From: Ralf.Hildebrandt at charite.de (Ralf Hildebrandt)
Date: Tue, 30 Jan 2018 14:27:12 +0100
Subject: [squid-users] Bypass a ICAP error in Squid?
Message-ID: <20180130132712.GB27672@charite.de>

How can I bypass an ICAP error in Squid (currently squid5)?

Background: We're using Squid with C-icap, and recently had (like
anybody else) huge issues with clamd not working properly.

-- 
Ralf Hildebrandt                   Charite Universit?tsmedizin Berlin
ralf.hildebrandt at charite.de        Campus Benjamin Franklin
https://www.charite.de             Hindenburgdamm 30, 12203 Berlin
Gesch?ftsbereich IT, Abt. Netzwerk fon: +49-30-450.570.155


From rousskov at measurement-factory.com  Tue Jan 30 15:20:41 2018
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Tue, 30 Jan 2018 08:20:41 -0700
Subject: [squid-users] Bypass a ICAP error in Squid?
In-Reply-To: <20180130132712.GB27672@charite.de>
References: <20180130132712.GB27672@charite.de>
Message-ID: <aeabc81b-996b-a036-aaa4-17ac670b3108@measurement-factory.com>

On 01/30/2018 06:27 AM, Ralf Hildebrandt wrote:
> How can I bypass an ICAP error in Squid (currently squid5)?

See the bypass option of the icap_service directive but keep in mind
http://lists.squid-cache.org/pipermail/squid-users/2018-January/017484.html

Alex.


From Ralf.Hildebrandt at charite.de  Tue Jan 30 15:36:06 2018
From: Ralf.Hildebrandt at charite.de (Ralf Hildebrandt)
Date: Tue, 30 Jan 2018 16:36:06 +0100
Subject: [squid-users] Bypass a ICAP error in Squid?
In-Reply-To: <aeabc81b-996b-a036-aaa4-17ac670b3108@measurement-factory.com>
References: <20180130132712.GB27672@charite.de>
 <aeabc81b-996b-a036-aaa4-17ac670b3108@measurement-factory.com>
Message-ID: <20180130153606.GJ27672@charite.de>

* Alex Rousskov <rousskov at measurement-factory.com>:
> On 01/30/2018 06:27 AM, Ralf Hildebrandt wrote:
> > How can I bypass an ICAP error in Squid (currently squid5)?
> 
> See the bypass option of the icap_service directive but keep in mind
> http://lists.squid-cache.org/pipermail/squid-users/2018-January/017484.html

So that means that C-ICAP needs to fail in a a way that allows squid
to bypass it...
-- 
Ralf Hildebrandt                   Charite Universit?tsmedizin Berlin
ralf.hildebrandt at charite.de        Campus Benjamin Franklin
https://www.charite.de             Hindenburgdamm 30, 12203 Berlin
Gesch?ftsbereich IT, Abt. Netzwerk fon: +49-30-450.570.155
-------------- next part --------------
A non-text attachment was scrubbed...
Name: signature.asc
Type: application/pgp-signature
Size: 195 bytes
Desc: not available
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20180130/758a003d/attachment.sig>

From rousskov at measurement-factory.com  Tue Jan 30 15:58:18 2018
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Tue, 30 Jan 2018 08:58:18 -0700
Subject: [squid-users] Bypass a ICAP error in Squid?
In-Reply-To: <20180130153606.GJ27672@charite.de>
References: <20180130132712.GB27672@charite.de>
 <aeabc81b-996b-a036-aaa4-17ac670b3108@measurement-factory.com>
 <20180130153606.GJ27672@charite.de>
Message-ID: <6fcf9212-958b-513a-25e3-b8bac3e48b67@measurement-factory.com>

On 01/30/2018 08:36 AM, Ralf Hildebrandt wrote:
> * Alex Rousskov <rousskov at measurement-factory.com>:
>> On 01/30/2018 06:27 AM, Ralf Hildebrandt wrote:
>>> How can I bypass an ICAP error in Squid (currently squid5)?
>>
>> See the bypass option of the icap_service directive but keep in mind
>> http://lists.squid-cache.org/pipermail/squid-users/2018-January/017484.html

> So that means that C-ICAP needs to fail in a a way that allows squid
> to bypass it...

Squid bugs notwithstanding, yes. And it is likely that c-icap already
fails that way in the context of this and quoted threads.

However, I suspect there is a Squid bug in this area. It should be fixed
before bypass=on becomes a viable option in this context. It takes two
to tango.

Alex.


