From vh1988 at yahoo.com.ar  Mon Jul  2 17:34:19 2018
From: vh1988 at yahoo.com.ar (Julian Perconti)
Date: Mon, 2 Jul 2018 14:34:19 -0300
Subject: [squid-users] SSL errors with Squid 3.5.27 [SOLVED]
Message-ID: <005601d4122a$ea68a540$bf39efc0$@yahoo.com.ar>

Hi all,

Problem solved.

With squid 4 openssl 1.1

I realized that WhatsApp use the following ports:

5223, 5228, 4244, 5242, and 5222 in addition to 443, 80.

So I opened that ports on the firewall and everythhing worked.

Also I changed the cipher suite in squid.conf like this: (for the dropbox client problem)

tls_outgoing_options cipher=ALL:!SSLv2:!ADH:!DSS:!MD5:!EXP:!DES:!PSK:!SRP:!RC4:!IDEA:!SEED:!aNULL:!eNULL

software info:

Squid Cache: Version 4.0.25
Service Name: squid

This binary uses OpenSSL 1.1.0f  25 May 2017. For legal restrictions on distribution see https://www.openssl.org/source/license.html

configure options:  '--prefix=/usr' '--build=x86_64-linux-gnu' '--localstatedir=/var/squid' '--libexecdir=/lib/squid' '--srcdir=.' '--datadir=/share/squid' '--sysconfdir=/etc/squid' '--with-default-user=proxy' '--with-logdir=/var/log/squid' '--with-pidfile=/var/run/squid.pid' '--with-openssl' '--enable-ssl-crtd' '--mandir=/share/man' '--enable-storeio=ufs,aufs,diskd' '--enable-removal-policies=lru,heap' '--enable-delay-pools' '--enable-icap' '--enable-cache-digests' 'build_alias=x86_64-linux-gnu' --enable-ltdl-convenience

# openssl version
OpenSSL 1.1.0f  25 May 2017

Thanks everybody.

PS: I think the same would work with squid 3.5.27 on debian 7, because it was a firewall problem.



From vishali.somaskanthan at viptela.com  Mon Jul  2 23:34:19 2018
From: vishali.somaskanthan at viptela.com (Vishali Somaskanthan)
Date: Mon, 2 Jul 2018 16:34:19 -0700
Subject: [squid-users] Behavior of Squid with SSL Bump and server persistent
	connections
Message-ID: <CABfsTT522AoBc60_bWg0WYo5+tjLx=fAwDbaB711YZ+6rpnrzQ@mail.gmail.com>

Hello all,
I am trying out SSL Bump for my connections from Squid to server and trying
to put along server persistent connections as well. I would like to know
how squid behaves with both of these turned on??
I see info in the squid wiki page that SSL Bump creates fake CONNECT
requests and Peeking at Step1 creates another CONNECT request. According to
this, the idea behind using server_persistent_connections is knocked out
and do not serve as per requirements. I look forward to hearing some
comments on this.
Thank you all.


-- 
Regards,
Vishali Somaskanthan
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20180702/e9cdff55/attachment.htm>

From rousskov at measurement-factory.com  Mon Jul  2 23:57:54 2018
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Mon, 2 Jul 2018 17:57:54 -0600
Subject: [squid-users] Behavior of Squid with SSL Bump and server
 persistent connections
In-Reply-To: <CABfsTT522AoBc60_bWg0WYo5+tjLx=fAwDbaB711YZ+6rpnrzQ@mail.gmail.com>
References: <CABfsTT522AoBc60_bWg0WYo5+tjLx=fAwDbaB711YZ+6rpnrzQ@mail.gmail.com>
Message-ID: <7e2e6b3f-a642-307e-c53f-a59d3a32a877@measurement-factory.com>

On 07/02/2018 05:34 PM, Vishali Somaskanthan wrote:

> I am trying out SSL Bump for my connections from Squid to server and
> trying to put along server persistent connections as well. I would like
> to know how squid behaves with both of these turned on??

In modern Squids, all(*) bumped SSL client HTTP requests (from client
connection C) should use the corresponding bumped connection to the
server (S). After the first HTTP request, if more requests arrive on
connection C, and they are all regular/basic requests, then they can all
go through connection S. Once HTTP rules, timeouts, or other factors
prohibit connection S or connection C reuse, Squid should close both
connections.

Please note that I do not know whether Squid correctly forces all(*)
HTTP requests on connection C to connection S, but it should. If it does
not, file a bug report. Same for closing connection C when connection S
becomes unusable.


> I see info in the squid wiki page that SSL Bump creates fake CONNECT
> requests and Peeking at Step1 creates another CONNECT request. 

Peeking or staring may indeed produce internal fake CONNECT requests,
but they are unrelated to your question. They are used internally to
handle the client TLS connection and for giving adaptation services a
say in the matter. Persistency is an HTTP term that is applied to what
happens _after_ the TLS connections is bumped.

(Also, peeking is a part of the SslBump feature -- they are not two
different actions or stages as "and" in your summary implies).


HTH,

Alex.
P.S. (*) "all" should be interpreted as "all that need a server
connection" here -- pure cache hits, adaptation-satisfied requests, and
probably some erroneous requests (e.g., those blocked by http_access
rules?) do not use the server connection.


From anon.amish at gmail.com  Tue Jul  3 08:00:33 2018
From: anon.amish at gmail.com (Amish)
Date: Tue, 3 Jul 2018 13:30:33 +0530
Subject: [squid-users] squid 4.1 default queue-size should consider
	concurrency
Message-ID: <d2df8e0b-7317-6253-4e48-7b1638a834f7@gmail.com>

Hello,

In squid 4.1 new option "queue-size" was introduced.

In most (or all) cases default "queue-size" is set to children-max*2.

But I believe it should be higher of (children-max*2) OR (concurrency*2)

Or it can be some better formula but the point I am trying to make is 
that, "concurrency" should be taken in to account for calculating 
default value of "queue-size".

Please consider.

Thanks and regards.

Amish



From squid3 at treenet.co.nz  Tue Jul  3 08:16:23 2018
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Tue, 3 Jul 2018 20:16:23 +1200
Subject: [squid-users] squid 4.1 default queue-size should consider
 concurrency
In-Reply-To: <d2df8e0b-7317-6253-4e48-7b1638a834f7@gmail.com>
References: <d2df8e0b-7317-6253-4e48-7b1638a834f7@gmail.com>
Message-ID: <63fb8a89-cc9b-d053-b46c-aa5ab39f4ea3@treenet.co.nz>

On 03/07/18 20:00, Amish wrote:
> Hello,
> 
> In squid 4.1 new option "queue-size" was introduced.
> 
> In most (or all) cases default "queue-size" is set to children-max*2.
> 
> But I believe it should be higher of (children-max*2) OR (concurrency*2)
> 
> Or it can be some better formula but the point I am trying to make is
> that, "concurrency" should be taken in to account for calculating
> default value of "queue-size".
> 
> Please consider.

FYI; When we add a directive or option to control some behaviour that
already happens the default is usually set to the value all existing
Squid are using so nobody gets an unexpected surprise with upgrade.

That can change later once more people have experimented to find what a
better value actually is.

Cheers
Amos


From mikes at surcouf.co.uk  Tue Jul  3 10:12:05 2018
From: mikes at surcouf.co.uk (Mike Surcouf)
Date: Tue, 3 Jul 2018 10:12:05 +0000
Subject: [squid-users] Squid 4.1 for CentOS rpms
Message-ID: <2197768425D7F5479A0FFB3FEC212F7FF63D99C3@aesmail.surcouf.local>

Hi Eliezer

I have been using your repos on CentOS for many years thank you for your hard work.
Are you planning a stable repo for v4 now it's out.

Many Thanks

Mike




From anon.amish at gmail.com  Tue Jul  3 11:23:27 2018
From: anon.amish at gmail.com (Amish)
Date: Tue, 3 Jul 2018 16:53:27 +0530
Subject: [squid-users] will multiple tls_outgoing_options lines be combined?
Message-ID: <e309eee7-97f9-16e0-a410-037c3943886a@gmail.com>

Hello,

I am trying new options in squid 4.1.

For easy readability can I use tls_outgoing_options multiple times in 
squid.conf?

Like this: (Tips from 
https://wiki.squid-cache.org/ConfigExamples/Intercept/SslBumpExplicit)

tls_outgoing_options cafile=/etc/ssl/cert.pem
tls_outgoing_options options=NO_SSLv3,SINGLE_DH_USE,SINGLE_ECDH_USE
tls_outgoing_options 
cipher=HIGH:MEDIUM:!RC4:!aNULL:!eNULL:!LOW:!3DES:!MD5:!EXP:!PSK:!SRP:!DSS

Will it merge all 3 as if mentioned on 1 single line?
Or will it pick only last (or first) option and internal defaults for rest?

I am not an SSL/TLS expert so I do not know how to test this i.e. how to 
know if its really working.

For some reason "debug_options 3,9" is not generating anything in cache.log.

Note: Debug Section 3 = Configuration File Parsing

Thanks in advance,

Amish.



From Sarfaraz.Ahmad at deshaw.com  Tue Jul  3 12:19:56 2018
From: Sarfaraz.Ahmad at deshaw.com (Ahmad, Sarfaraz)
Date: Tue, 3 Jul 2018 12:19:56 +0000
Subject: [squid-users] Make websockets work without splicing TLS connections
Message-ID: <6c1894921c2b41f5be30382fde5f125b@mbxtoa3.winmail.deshaw.com>

Guys,

Can you think of a way to make websockets work without splicing TLS connections ?
I don't think on_unsupported _protocol would work here . Also would on_unsupported_protocol work where the remote server abuses 443 for something other than TLS ?

Regards,
Sarfaraz
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20180703/c4d65a61/attachment.htm>

From squid3 at treenet.co.nz  Tue Jul  3 12:37:22 2018
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Wed, 4 Jul 2018 00:37:22 +1200
Subject: [squid-users] will multiple tls_outgoing_options lines be
 combined?
In-Reply-To: <e309eee7-97f9-16e0-a410-037c3943886a@gmail.com>
References: <e309eee7-97f9-16e0-a410-037c3943886a@gmail.com>
Message-ID: <1544a6f0-7b3d-9c7a-c63b-5512c85d94bb@treenet.co.nz>

On 03/07/18 23:23, Amish wrote:
> Hello,
> 
> I am trying new options in squid 4.1.
> 
> For easy readability can I use tls_outgoing_options multiple times in
> squid.conf?
> 

Yes. Provided the options specified on each are different.

> Like this: (Tips from
> https://wiki.squid-cache.org/ConfigExamples/Intercept/SslBumpExplicit)
> 
> tls_outgoing_options cafile=/etc/ssl/cert.pem
> tls_outgoing_options options=NO_SSLv3,SINGLE_DH_USE,SINGLE_ECDH_USE
> tls_outgoing_options
> cipher=HIGH:MEDIUM:!RC4:!aNULL:!eNULL:!LOW:!3DES:!MD5:!EXP:!PSK:!SRP:!DSS
> 
> Will it merge all 3 as if mentioned on 1 single line?

The above will, yes.

> Or will it pick only last (or first) option and internal defaults for rest?

Each option in the directive replaces previous values of that same option.

For example;

  tls_outgoing_options cipher=A cipher=B

 is the same as:

  tls_outgoing_options cipher=B


Since above you are setting different named option on every line, your
setup is equivalent to all the options being on one line.


However, Squid understands line wrapping. So this question is not very
useful. You could simplify even further like this:

 tls_outgoing_options \
   cafile=/etc/ssl/cert.pem \
   options=NO_SSLv3,SINGLE_DH_USE,SINGLE_ECDH_USE \
   cipher=HIGH:MEDIUM:!RC4:!aNULL:!eNULL:!LOW:....




> 
> I am not an SSL/TLS expert so I do not know how to test this i.e. how to
> know if its really working.

Then its a good sign you are asking for help. TLS is a pit of troubles
for the unaware. To successfully manage use of SSL-Bump features you
need at least an understanding of how the TLS protocol operates.
 Note that is *not* the crypto, just how the messaging works.

This directive controls what capabilities Squid lets your OpenSSL
library advertise in clientHello and TLS extension messages sent to
servers (not cache_peer) during a TLS handshake.
 So to debug use something like wireshark to look at the packets and see
what the handshake contains.


Amos


From squid3 at treenet.co.nz  Tue Jul  3 12:44:43 2018
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Wed, 4 Jul 2018 00:44:43 +1200
Subject: [squid-users] Make websockets work without splicing TLS
 connections
In-Reply-To: <6c1894921c2b41f5be30382fde5f125b@mbxtoa3.winmail.deshaw.com>
References: <6c1894921c2b41f5be30382fde5f125b@mbxtoa3.winmail.deshaw.com>
Message-ID: <d7184620-7cc8-e1d5-487f-7f3800fece91@treenet.co.nz>

On 04/07/18 00:19, Ahmad, Sarfaraz wrote:
> Guys,
> 
> ?
> 
> Can you think of a way to make websockets work without splicing TLS
> connections ?
> 

Squid does not understand WebSocket protocol (yet). So splicing is the
only option once the traffic is already going into the proxy.

Squid does support enough WebSockets to trigger the HTTP failover
mechanism sin WebSockets. But many clients and/or servers apparently do
not actually support WebSockets properly and break when that proxy
compatibility mechanism is used.

WebSocket has its own port for native traffic. So letting that through
your firewall should theoretically be enough.



> I don?t think on_unsupported _protocol would work here .// Also would

It may, but I agree that is not expected. WebSockets uses HTTP-like
syntax in its first message to be compatible with HTTPS servers.


> on_unsupported_protocol work where the remote server abuses 443 for
> something other than TLS ?

It should. Weird non-standard crap abusing port 443 is what that
directive was designed to help workaround.

Amos


From Ralf.Hildebrandt at charite.de  Tue Jul  3 12:47:26 2018
From: Ralf.Hildebrandt at charite.de (Ralf Hildebrandt)
Date: Tue, 3 Jul 2018 14:47:26 +0200
Subject: [squid-users] [ext]  when will squid 4 be production ready?
In-Reply-To: <CAK0iFYyuRxLNHOXhLy3=xB7PdyKW5SGqM-E3a-LT5+Wf3NfgdA@mail.gmail.com>
References: <CAK0iFYyuRxLNHOXhLy3=xB7PdyKW5SGqM-E3a-LT5+Wf3NfgdA@mail.gmail.com>
Message-ID: <20180703124726.GK9141@charite.de>

* Gordon Hsiao <capcoding at gmail.com>:
> squid4 has been released for quite a while, when will it be production
> ready or any rough timeline on the horizon?

I'm using annotate_transaction extensively. Is that available in
Squid-4?

-- 
Ralf Hildebrandt                   Charite Universit?tsmedizin Berlin
ralf.hildebrandt at charite.de        Campus Benjamin Franklin
https://www.charite.de             Hindenburgdamm 30, 12203 Berlin
Gesch?ftsbereich IT, Abt. Netzwerk fon: +49-30-450.570.155


From Sarfaraz.Ahmad at deshaw.com  Tue Jul  3 12:59:18 2018
From: Sarfaraz.Ahmad at deshaw.com (Ahmad, Sarfaraz)
Date: Tue, 3 Jul 2018 12:59:18 +0000
Subject: [squid-users] Make websockets work without splicing TLS
 connections
In-Reply-To: <d7184620-7cc8-e1d5-487f-7f3800fece91@treenet.co.nz>
References: <6c1894921c2b41f5be30382fde5f125b@mbxtoa3.winmail.deshaw.com>
 <d7184620-7cc8-e1d5-487f-7f3800fece91@treenet.co.nz>
Message-ID: <37600e6c7e4b46218bf5068e6a7fcb41@mbxtoa3.winmail.deshaw.com>

>> Squid does not understand WebSocket protocol (yet).
Is supporting Websockets on the roadmap ? 



-----Original Message-----
From: squid-users <squid-users-bounces at lists.squid-cache.org> On Behalf Of Amos Jeffries
Sent: Tuesday, July 3, 2018 6:15 PM
To: squid-users at lists.squid-cache.org
Subject: Re: [squid-users] Make websockets work without splicing TLS connections

On 04/07/18 00:19, Ahmad, Sarfaraz wrote:
> Guys,
> 
> ?
> 
> Can you think of a way to make websockets work without splicing TLS 
> connections ?
> 

Squid does not understand WebSocket protocol (yet). So splicing is the only option once the traffic is already going into the proxy.

Squid does support enough WebSockets to trigger the HTTP failover mechanism sin WebSockets. But many clients and/or servers apparently do not actually support WebSockets properly and break when that proxy compatibility mechanism is used.

WebSocket has its own port for native traffic. So letting that through your firewall should theoretically be enough.



> I don?t think on_unsupported _protocol would work here .// Also would

It may, but I agree that is not expected. WebSockets uses HTTP-like syntax in its first message to be compatible with HTTPS servers.


> on_unsupported_protocol work where the remote server abuses 443 for 
> something other than TLS ?

It should. Weird non-standard crap abusing port 443 is what that directive was designed to help workaround.

Amos
_______________________________________________
squid-users mailing list
squid-users at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-users

From anon.amish at gmail.com  Tue Jul  3 13:45:06 2018
From: anon.amish at gmail.com (Amish)
Date: Tue, 3 Jul 2018 19:15:06 +0530
Subject: [squid-users] will multiple tls_outgoing_options lines be
 combined?
In-Reply-To: <1544a6f0-7b3d-9c7a-c63b-5512c85d94bb@treenet.co.nz>
References: <e309eee7-97f9-16e0-a410-037c3943886a@gmail.com>
 <1544a6f0-7b3d-9c7a-c63b-5512c85d94bb@treenet.co.nz>
Message-ID: <b8462f8c-61d9-b3a9-c88b-419ad2948030@gmail.com>


On Tuesday 03 July 2018 06:07 PM, Amos Jeffries wrote:

>> Or will it pick only last (or first) option and internal defaults for rest?
> Each option in the directive replaces previous values of that same option.
>
> For example;
>
>    tls_outgoing_options cipher=A cipher=B
>
>   is the same as:
>
>    tls_outgoing_options cipher=B
>
>
> Since above you are setting different named option on every line, your
> setup is equivalent to all the options being on one line.
>
>
> However, Squid understands line wrapping. So this question is not very
> useful. You could simplify even further like this:
>
>   tls_outgoing_options \
>     cafile=/etc/ssl/cert.pem \
>     options=NO_SSLv3,SINGLE_DH_USE,SINGLE_ECDH_USE \
>     cipher=HIGH:MEDIUM:!RC4:!aNULL:!eNULL:!LOW:....
>

Didn't know about line wrapping support.

But my question is still useful in my case. Writing small and quick sed 
script becomes easy.

Thank you for your super quick answers!

Amish


From anon.amish at gmail.com  Tue Jul  3 13:57:48 2018
From: anon.amish at gmail.com (Amish)
Date: Tue, 3 Jul 2018 19:27:48 +0530
Subject: [squid-users] squid 4.1 default queue-size should consider
 concurrency
In-Reply-To: <63fb8a89-cc9b-d053-b46c-aa5ab39f4ea3@treenet.co.nz>
References: <d2df8e0b-7317-6253-4e48-7b1638a834f7@gmail.com>
 <63fb8a89-cc9b-d053-b46c-aa5ab39f4ea3@treenet.co.nz>
Message-ID: <d56582e9-6cbc-07e5-4e15-691cf46b3cc4@gmail.com>



On Tuesday 03 July 2018 01:46 PM, Amos Jeffries wrote:
> On 03/07/18 20:00, Amish wrote:
>> Hello,
>>
>> In squid 4.1 new option "queue-size" was introduced.
>>
>> In most (or all) cases default "queue-size" is set to children-max*2.
>>
>> But I believe it should be higher of (children-max*2) OR (concurrency*2)
>>
>> Or it can be some better formula but the point I am trying to make is
>> that, "concurrency" should be taken in to account for calculating
>> default value of "queue-size".
>>
>> Please consider.
> FYI; When we add a directive or option to control some behaviour that
> already happens the default is usually set to the value all existing
> Squid are using so nobody gets an unexpected surprise with upgrade.
>
> That can change later once more people have experimented to find what a
> better value actually is.
>
> Cheers
> Amos

Oh ok. I thought "queue-size" was completely new thing and didnt even 
exist implicitly in earlier version.

In my case helper has only 1 process but 10 concurrency. So 2 was 
obviously not a right default value for my case. Hence my post.

But I agree with your point about unexpected surprises.

Thanks and regards,

Amish.


From marcus.kool at urlfilterdb.com  Tue Jul  3 14:19:25 2018
From: marcus.kool at urlfilterdb.com (Marcus Kool)
Date: Tue, 3 Jul 2018 11:19:25 -0300
Subject: [squid-users] squid 4.1 default queue-size should consider
 concurrency
In-Reply-To: <63fb8a89-cc9b-d053-b46c-aa5ab39f4ea3@treenet.co.nz>
References: <d2df8e0b-7317-6253-4e48-7b1638a834f7@gmail.com>
 <63fb8a89-cc9b-d053-b46c-aa5ab39f4ea3@treenet.co.nz>
Message-ID: <7747946a-212b-5248-794a-4e6fed1e30c7@urlfilterdb.com>

The original intention of this default value is have a queue that is twice the size of the messages being processed, so for helpers with concurrency=NCONC and num_children=NCHILD it makes a lot of 
sense to set the default queue length to 2*NCONC*NCHILD.
I do not understand that "compatibility" with doing the wrong calculation is a good thing.

Marcus


On 03/07/18 05:16, Amos Jeffries wrote:
> On 03/07/18 20:00, Amish wrote:
>> Hello,
>>
>> In squid 4.1 new option "queue-size" was introduced.
>>
>> In most (or all) cases default "queue-size" is set to children-max*2.
>>
>> But I believe it should be higher of (children-max*2) OR (concurrency*2)
>>
>> Or it can be some better formula but the point I am trying to make is
>> that, "concurrency" should be taken in to account for calculating
>> default value of "queue-size".
>>
>> Please consider.
> 
> FYI; When we add a directive or option to control some behaviour that
> already happens the default is usually set to the value all existing
> Squid are using so nobody gets an unexpected surprise with upgrade.
> 
> That can change later once more people have experimented to find what a
> better value actually is.
> 
> Cheers
> Amos
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
> 


From rousskov at measurement-factory.com  Tue Jul  3 15:00:46 2018
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Tue, 3 Jul 2018 09:00:46 -0600
Subject: [squid-users] Make websockets work without splicing TLS
 connections
In-Reply-To: <37600e6c7e4b46218bf5068e6a7fcb41@mbxtoa3.winmail.deshaw.com>
References: <6c1894921c2b41f5be30382fde5f125b@mbxtoa3.winmail.deshaw.com>
 <d7184620-7cc8-e1d5-487f-7f3800fece91@treenet.co.nz>
 <37600e6c7e4b46218bf5068e6a7fcb41@mbxtoa3.winmail.deshaw.com>
Message-ID: <ce8d26a9-6141-6e56-3ec8-32edef5bd707@measurement-factory.com>

On 07/03/2018 06:59 AM, Ahmad, Sarfaraz wrote:
>>> Squid does not understand WebSocket protocol (yet).
> Is supporting Websockets on the roadmap ? 

Yes, we are working on tunneling WebSockets traffic after a successful
HTTP Upgrade exchange with the server (with admin permission, of course).

Alex.



> -----Original Message-----
> From: squid-users <squid-users-bounces at lists.squid-cache.org> On Behalf Of Amos Jeffries
> Sent: Tuesday, July 3, 2018 6:15 PM
> To: squid-users at lists.squid-cache.org
> Subject: Re: [squid-users] Make websockets work without splicing TLS connections
> 
> On 04/07/18 00:19, Ahmad, Sarfaraz wrote:
>> Guys,
>>
>> ?
>>
>> Can you think of a way to make websockets work without splicing TLS 
>> connections ?
>>
> 
> Squid does not understand WebSocket protocol (yet). So splicing is the only option once the traffic is already going into the proxy.
> 
> Squid does support enough WebSockets to trigger the HTTP failover mechanism sin WebSockets. But many clients and/or servers apparently do not actually support WebSockets properly and break when that proxy compatibility mechanism is used.
> 
> WebSocket has its own port for native traffic. So letting that through your firewall should theoretically be enough.
> 
> 
> 
>> I don?t think on_unsupported _protocol would work here .// Also would
> 
> It may, but I agree that is not expected. WebSockets uses HTTP-like syntax in its first message to be compatible with HTTPS servers.
> 
> 
>> on_unsupported_protocol work where the remote server abuses 443 for 
>> something other than TLS ?
> 
> It should. Weird non-standard crap abusing port 443 is what that directive was designed to help workaround.
> 
> Amos
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
> 



From anon.amish at gmail.com  Tue Jul  3 15:07:11 2018
From: anon.amish at gmail.com (Amish)
Date: Tue, 3 Jul 2018 20:37:11 +0530
Subject: [squid-users] squid 4.1 default queue-size should consider
 concurrency
In-Reply-To: <7747946a-212b-5248-794a-4e6fed1e30c7@urlfilterdb.com>
References: <d2df8e0b-7317-6253-4e48-7b1638a834f7@gmail.com>
 <63fb8a89-cc9b-d053-b46c-aa5ab39f4ea3@treenet.co.nz>
 <7747946a-212b-5248-794a-4e6fed1e30c7@urlfilterdb.com>
Message-ID: <1ba00b33-67d8-f142-8cf7-9378164336dc@gmail.com>

2*NCONC*NCHILD will possibly lead to too high value as a default and the 
busy-ness will never be logged.

My proposal of higher of (2*NCONC) and (2*NCHILD) would mean that load 
is now regularly high enough that atleast 2 more children are needed.

We can start with that and then find a better formula.

Amish


On Tuesday 03 July 2018 07:49 PM, Marcus Kool wrote:
> The original intention of this default value is have a queue that is 
> twice the size of the messages being processed, so for helpers with 
> concurrency=NCONC and num_children=NCHILD it makes a lot of sense to 
> set the default queue length to 2*NCONC*NCHILD.
> I do not understand that "compatibility" with doing the wrong 
> calculation is a good thing.
>
> Marcus
>
>
> On 03/07/18 05:16, Amos Jeffries wrote:
>> On 03/07/18 20:00, Amish wrote:
>>> Hello,
>>>
>>> In squid 4.1 new option "queue-size" was introduced.
>>>
>>> In most (or all) cases default "queue-size" is set to children-max*2.
>>>
>>> But I believe it should be higher of (children-max*2) OR 
>>> (concurrency*2)
>>>
>>> Or it can be some better formula but the point I am trying to make is
>>> that, "concurrency" should be taken in to account for calculating
>>> default value of "queue-size".
>>>
>>> Please consider.
>>
>> FYI; When we add a directive or option to control some behaviour that
>> already happens the default is usually set to the value all existing
>> Squid are using so nobody gets an unexpected surprise with upgrade.
>>
>> That can change later once more people have experimented to find what a
>> better value actually is.
>>
>> Cheers
>> Amos
>> _______________________________________________
>> squid-users mailing list
>> squid-users at lists.squid-cache.org
>> http://lists.squid-cache.org/listinfo/squid-users
>>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users



From rousskov at measurement-factory.com  Tue Jul  3 15:35:58 2018
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Tue, 3 Jul 2018 09:35:58 -0600
Subject: [squid-users] [ext] when will squid 4 be production ready?
In-Reply-To: <20180703124726.GK9141@charite.de>
References: <CAK0iFYyuRxLNHOXhLy3=xB7PdyKW5SGqM-E3a-LT5+Wf3NfgdA@mail.gmail.com>
 <20180703124726.GK9141@charite.de>
Message-ID: <1e714d3f-667c-732e-caeb-04675a0f0392@measurement-factory.com>

On 07/03/2018 06:47 AM, Ralf Hildebrandt wrote:
> * Gordon Hsiao <capcoding at gmail.com>:
>> squid4 has been released for quite a while, when will it be production
>> ready or any rough timeline on the horizon?

> I'm using annotate_transaction extensively. Is that available in
> Squid-4?

No, Squid v4 does not support annotate_transaction (yet?).

Alex.


From marcus.kool at urlfilterdb.com  Tue Jul  3 15:47:43 2018
From: marcus.kool at urlfilterdb.com (Marcus Kool)
Date: Tue, 3 Jul 2018 12:47:43 -0300
Subject: [squid-users] squid 4.1 default queue-size should consider
 concurrency
In-Reply-To: <1ba00b33-67d8-f142-8cf7-9378164336dc@gmail.com>
References: <d2df8e0b-7317-6253-4e48-7b1638a834f7@gmail.com>
 <63fb8a89-cc9b-d053-b46c-aa5ab39f4ea3@treenet.co.nz>
 <7747946a-212b-5248-794a-4e6fed1e30c7@urlfilterdb.com>
 <1ba00b33-67d8-f142-8cf7-9378164336dc@gmail.com>
Message-ID: <facb1b27-fbb3-4276-68f5-c55cde074770@urlfilterdb.com>

If an admin finds it necessary to configure
    url_rewrite_children 16 concurrency=4
the helper subsystem is theoretically capable of processing 64 messages simultaneously.
It does not makes sens to use max(2*4,2*16)=32 for queue-size, but should be _at least_ 64.

Since Squid (before introducing the concurrency parameter) wanted to have a queue that is twice the capacity of the helper subsystem (most likely since this was found to be a good value), the logical 
thing to do is what I proposed and use 2*NCONC*NCHILD, which in the above example is 2*4*16 = 128.

There are many messages on this mailing list about problems with helpers.
One refers to
    external_acl_type session concurrency=100 ttl=3 negative_ttl=0 children-max=1 %LOGIN /usr/lib64/squid/ext_session_acl -a -T 60 -b /var/lib/squid/sessions/
with NCONC=100, NCHILD=1, it is not hard to imagine that a default queue-size of 2 is problematic.
Another one has
    store_id_children 100 startup=0 idle=1 concurrency=1000
with NCONC=1000 and NCHILD=100 the admin wants to process max 1 million requests simultaneously.
*I have a suspicion that admins use these large numbers in an attempt to have a good performance but they never reach it since the queue-size stays small.*
So, the impact of queue-size should be very clearly documented, better is to use a sane default.

Marcus


On 03/07/18 12:07, Amish wrote:
> 2*NCONC*NCHILD will possibly lead to too high value as a default and the busy-ness will never be logged.
> 
> My proposal of higher of (2*NCONC) and (2*NCHILD) would mean that load is now regularly high enough that atleast 2 more children are needed.
> 
> We can start with that and then find a better formula.
> 
> Amish
> 
> 
> On Tuesday 03 July 2018 07:49 PM, Marcus Kool wrote:
>> The original intention of this default value is have a queue that is twice the size of the messages being processed, so for helpers with concurrency=NCONC and num_children=NCHILD it makes a lot of 
>> sense to set the default queue length to 2*NCONC*NCHILD.
>> I do not understand that "compatibility" with doing the wrong calculation is a good thing.
>>
>> Marcus
>>
>>
>> On 03/07/18 05:16, Amos Jeffries wrote:
>>> On 03/07/18 20:00, Amish wrote:
>>>> Hello,
>>>>
>>>> In squid 4.1 new option "queue-size" was introduced.
>>>>
>>>> In most (or all) cases default "queue-size" is set to children-max*2.
>>>>
>>>> But I believe it should be higher of (children-max*2) OR (concurrency*2)
>>>>
>>>> Or it can be some better formula but the point I am trying to make is
>>>> that, "concurrency" should be taken in to account for calculating
>>>> default value of "queue-size".
>>>>
>>>> Please consider.
>>>
>>> FYI; When we add a directive or option to control some behaviour that
>>> already happens the default is usually set to the value all existing
>>> Squid are using so nobody gets an unexpected surprise with upgrade.
>>>
>>> That can change later once more people have experimented to find what a
>>> better value actually is.
>>>
>>> Cheers
>>> Amos
>>> _______________________________________________
>>> squid-users mailing list
>>> squid-users at lists.squid-cache.org
>>> http://lists.squid-cache.org/listinfo/squid-users
>>>
>> _______________________________________________
>> squid-users mailing list
>> squid-users at lists.squid-cache.org
>> http://lists.squid-cache.org/listinfo/squid-users
> 
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users


From rousskov at measurement-factory.com  Tue Jul  3 15:54:25 2018
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Tue, 3 Jul 2018 09:54:25 -0600
Subject: [squid-users] squid 4.1 default queue-size should consider
 concurrency
In-Reply-To: <7747946a-212b-5248-794a-4e6fed1e30c7@urlfilterdb.com>
References: <d2df8e0b-7317-6253-4e48-7b1638a834f7@gmail.com>
 <63fb8a89-cc9b-d053-b46c-aa5ab39f4ea3@treenet.co.nz>
 <7747946a-212b-5248-794a-4e6fed1e30c7@urlfilterdb.com>
Message-ID: <0eba6da6-d5ef-537e-c768-2bd8b9e9337d@measurement-factory.com>

On 07/03/2018 08:19 AM, Marcus Kool wrote:
> The original intention of this default value is have a queue that is
> twice the size of the messages being processed, 

AFAICT, the "original intention" was different. The original intention
was to preserve old/unpatched Squid behavior to the extent possible and
to treat all helper categories in a more uniform way. Helper category
inconsistencies in the old Squid and what was changed/preserved are well
documented at

  https://github.com/squid-cache/squid/commit/6825b10


> for helpers with
> concurrency=NCONC and num_children=NCHILD it makes a lot of sense to set
> the default queue length to 2*NCONC*NCHILD.

Using 2*NCONC*NCHILD by default would change what every Squid does after
an upgrade. Many admins do not like such changes and, like Amos said, we
try to avoid such surprises (with all other factors being equal).


> I do not understand that "compatibility" with doing the wrong
> calculation is a good thing.

Avoiding surprises during upgrades is a good thing.


If you think Squid should use a different default for all or some helper
categories, please post a proposal that documents pros and cons and
justifies the change. The URL above can be used as your guide to helper
categories.


Thank you,

Alex.


> On 03/07/18 05:16, Amos Jeffries wrote:
>> On 03/07/18 20:00, Amish wrote:
>>> Hello,
>>>
>>> In squid 4.1 new option "queue-size" was introduced.
>>>
>>> In most (or all) cases default "queue-size" is set to children-max*2.
>>>
>>> But I believe it should be higher of (children-max*2) OR (concurrency*2)
>>>
>>> Or it can be some better formula but the point I am trying to make is
>>> that, "concurrency" should be taken in to account for calculating
>>> default value of "queue-size".
>>>
>>> Please consider.
>>
>> FYI; When we add a directive or option to control some behaviour that
>> already happens the default is usually set to the value all existing
>> Squid are using so nobody gets an unexpected surprise with upgrade.
>>
>> That can change later once more people have experimented to find what a
>> better value actually is.
>>
>> Cheers
>> Amos
>> _______________________________________________
>> squid-users mailing list
>> squid-users at lists.squid-cache.org
>> http://lists.squid-cache.org/listinfo/squid-users
>>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users



From anon.amish at gmail.com  Tue Jul  3 16:00:43 2018
From: anon.amish at gmail.com (Amish)
Date: Tue, 3 Jul 2018 21:30:43 +0530
Subject: [squid-users] squid 4.1 default queue-size should consider
 concurrency
In-Reply-To: <facb1b27-fbb3-4276-68f5-c55cde074770@urlfilterdb.com>
References: <d2df8e0b-7317-6253-4e48-7b1638a834f7@gmail.com>
 <63fb8a89-cc9b-d053-b46c-aa5ab39f4ea3@treenet.co.nz>
 <7747946a-212b-5248-794a-4e6fed1e30c7@urlfilterdb.com>
 <1ba00b33-67d8-f142-8cf7-9378164336dc@gmail.com>
 <facb1b27-fbb3-4276-68f5-c55cde074770@urlfilterdb.com>
Message-ID: <59693dce-de56-678a-c83d-66929fbc251f@gmail.com>

Umm, may be I mis-interpreted queue-size.

I thought queue-size indicates messages "waiting" in the queue and not 
those are currently being processed.

So in case of:
 ?? url_rewrite_children 16 concurrency=4

When redirect process is busy --- its currently processing 64 urls.

So max(2*4,2*16)=32 means that 32 URLs are "waiting" in queue. (in 
addition to 64 being processed)

But if queue-size means ALL urls (64+32) then my formula would be

Default queue-size = (NCHILD*NCONC)+max(2*NCHILD, 2*NCONC)

Regards,

Amish.


On Tuesday 03 July 2018 09:17 PM, Marcus Kool wrote:
> If an admin finds it necessary to configure
> ?? url_rewrite_children 16 concurrency=4
> the helper subsystem is theoretically capable of processing 64 
> messages simultaneously.
> It does not makes sens to use max(2*4,2*16)=32 for queue-size, but 
> should be _at least_ 64.
>
> Since Squid (before introducing the concurrency parameter) wanted to 
> have a queue that is twice the capacity of the helper subsystem (most 
> likely since this was found to be a good value), the logical thing to 
> do is what I proposed and use 2*NCONC*NCHILD, which in the above 
> example is 2*4*16 = 128.
>
> There are many messages on this mailing list about problems with helpers.
> One refers to
> ?? external_acl_type session concurrency=100 ttl=3 negative_ttl=0 
> children-max=1 %LOGIN /usr/lib64/squid/ext_session_acl -a -T 60 -b 
> /var/lib/squid/sessions/
> with NCONC=100, NCHILD=1, it is not hard to imagine that a default 
> queue-size of 2 is problematic.
> Another one has
> ?? store_id_children 100 startup=0 idle=1 concurrency=1000
> with NCONC=1000 and NCHILD=100 the admin wants to process max 1 
> million requests simultaneously.
> *I have a suspicion that admins use these large numbers in an attempt 
> to have a good performance but they never reach it since the 
> queue-size stays small.*
> So, the impact of queue-size should be very clearly documented, better 
> is to use a sane default.
>
> Marcus
>
>
> On 03/07/18 12:07, Amish wrote:
>> 2*NCONC*NCHILD will possibly lead to too high value as a default and 
>> the busy-ness will never be logged.
>>
>> My proposal of higher of (2*NCONC) and (2*NCHILD) would mean that 
>> load is now regularly high enough that atleast 2 more children are 
>> needed.
>>
>> We can start with that and then find a better formula.
>>
>> Amish
>>
>>
>> On Tuesday 03 July 2018 07:49 PM, Marcus Kool wrote:
>>> The original intention of this default value is have a queue that is 
>>> twice the size of the messages being processed, so for helpers with 
>>> concurrency=NCONC and num_children=NCHILD it makes a lot of sense to 
>>> set the default queue length to 2*NCONC*NCHILD.
>>> I do not understand that "compatibility" with doing the wrong 
>>> calculation is a good thing.
>>>
>>> Marcus
>>>
>>>
>>> On 03/07/18 05:16, Amos Jeffries wrote:
>>>> On 03/07/18 20:00, Amish wrote:
>>>>> Hello,
>>>>>
>>>>> In squid 4.1 new option "queue-size" was introduced.
>>>>>
>>>>> In most (or all) cases default "queue-size" is set to children-max*2.
>>>>>
>>>>> But I believe it should be higher of (children-max*2) OR 
>>>>> (concurrency*2)
>>>>>
>>>>> Or it can be some better formula but the point I am trying to make is
>>>>> that, "concurrency" should be taken in to account for calculating
>>>>> default value of "queue-size".
>>>>>
>>>>> Please consider.
>>>>
>>>> FYI; When we add a directive or option to control some behaviour that
>>>> already happens the default is usually set to the value all existing
>>>> Squid are using so nobody gets an unexpected surprise with upgrade.
>>>>
>>>> That can change later once more people have experimented to find 
>>>> what a
>>>> better value actually is.
>>>>
>>>> Cheers
>>>> Amos
>>>> _______________________________________________
>>>> squid-users mailing list
>>>> squid-users at lists.squid-cache.org
>>>> http://lists.squid-cache.org/listinfo/squid-users
>>>>
>>> _______________________________________________
>>> squid-users mailing list
>>> squid-users at lists.squid-cache.org
>>> http://lists.squid-cache.org/listinfo/squid-users
>>
>> _______________________________________________
>> squid-users mailing list
>> squid-users at lists.squid-cache.org
>> http://lists.squid-cache.org/listinfo/squid-users
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users



From rousskov at measurement-factory.com  Tue Jul  3 16:09:49 2018
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Tue, 3 Jul 2018 10:09:49 -0600
Subject: [squid-users] squid 4.1 default queue-size should consider
 concurrency
In-Reply-To: <facb1b27-fbb3-4276-68f5-c55cde074770@urlfilterdb.com>
References: <d2df8e0b-7317-6253-4e48-7b1638a834f7@gmail.com>
 <63fb8a89-cc9b-d053-b46c-aa5ab39f4ea3@treenet.co.nz>
 <7747946a-212b-5248-794a-4e6fed1e30c7@urlfilterdb.com>
 <1ba00b33-67d8-f142-8cf7-9378164336dc@gmail.com>
 <facb1b27-fbb3-4276-68f5-c55cde074770@urlfilterdb.com>
Message-ID: <32581779-f565-8a4b-5166-ccc554051342@measurement-factory.com>

Marcus,

    Based on your examples, I suspect that you are misinterpreting what
the queue is. The request is queued only when no helper can accept it.
The queue is not used for requests sent to helpers.

Alex.



From anon.amish at gmail.com  Tue Jul  3 16:20:38 2018
From: anon.amish at gmail.com (Amish)
Date: Tue, 3 Jul 2018 21:50:38 +0530
Subject: [squid-users] squid 4.1 default queue-size should consider
 concurrency
In-Reply-To: <59693dce-de56-678a-c83d-66929fbc251f@gmail.com>
References: <d2df8e0b-7317-6253-4e48-7b1638a834f7@gmail.com>
 <63fb8a89-cc9b-d053-b46c-aa5ab39f4ea3@treenet.co.nz>
 <7747946a-212b-5248-794a-4e6fed1e30c7@urlfilterdb.com>
 <1ba00b33-67d8-f142-8cf7-9378164336dc@gmail.com>
 <facb1b27-fbb3-4276-68f5-c55cde074770@urlfilterdb.com>
 <59693dce-de56-678a-c83d-66929fbc251f@gmail.com>
Message-ID: <e806adca-2905-6701-4c2e-3004f8ca96b6@gmail.com>

On Tuesday 03 July 2018 09:39 PM, Alex Rousskov wrote:
> Marcus,
>
>      Based on your examples, I suspect that you are misinterpreting what
> the queue is. The request is queued only when no helper can accept it.
> The queue is not used for requests sent to helpers.
>
> Alex.
>

Which means my previous interpretation was correct! And I had not 
misinterpreted it :)

Amish.


On Tuesday 03 July 2018 09:30 PM, Amish wrote:
> Umm, may be I mis-interpreted queue-size.
>
> I thought queue-size indicates messages "waiting" in the queue and not 
> those are currently being processed.
>
> So in case of:
> ?? url_rewrite_children 16 concurrency=4
>
> When redirect process is busy --- its currently processing 64 urls.
>
> So max(2*4,2*16)=32 means that 32 URLs are "waiting" in queue. (in 
> addition to 64 being processed)
>
> But if queue-size means ALL urls (64+32) then my formula would be
>
> Default queue-size = (NCHILD*NCONC)+max(2*NCHILD, 2*NCONC)
>
> Regards,
>
> Amish.
>
>
> On Tuesday 03 July 2018 09:17 PM, Marcus Kool wrote:
>> If an admin finds it necessary to configure
>> ?? url_rewrite_children 16 concurrency=4
>> the helper subsystem is theoretically capable of processing 64 
>> messages simultaneously.
>> It does not makes sens to use max(2*4,2*16)=32 for queue-size, but 
>> should be _at least_ 64.
>>
>> Since Squid (before introducing the concurrency parameter) wanted to 
>> have a queue that is twice the capacity of the helper subsystem (most 
>> likely since this was found to be a good value), the logical thing to 
>> do is what I proposed and use 2*NCONC*NCHILD, which in the above 
>> example is 2*4*16 = 128.
>>
>> There are many messages on this mailing list about problems with 
>> helpers.
>> One refers to
>> ?? external_acl_type session concurrency=100 ttl=3 negative_ttl=0 
>> children-max=1 %LOGIN /usr/lib64/squid/ext_session_acl -a -T 60 -b 
>> /var/lib/squid/sessions/
>> with NCONC=100, NCHILD=1, it is not hard to imagine that a default 
>> queue-size of 2 is problematic.
>> Another one has
>> ?? store_id_children 100 startup=0 idle=1 concurrency=1000
>> with NCONC=1000 and NCHILD=100 the admin wants to process max 1 
>> million requests simultaneously.
>> *I have a suspicion that admins use these large numbers in an attempt 
>> to have a good performance but they never reach it since the 
>> queue-size stays small.*
>> So, the impact of queue-size should be very clearly documented, 
>> better is to use a sane default.
>>
>> Marcus
>>
>>
>> On 03/07/18 12:07, Amish wrote:
>>> 2*NCONC*NCHILD will possibly lead to too high value as a default and 
>>> the busy-ness will never be logged.
>>>
>>> My proposal of higher of (2*NCONC) and (2*NCHILD) would mean that 
>>> load is now regularly high enough that atleast 2 more children are 
>>> needed.
>>>
>>> We can start with that and then find a better formula.
>>>
>>> Amish
>>>
>>>
>>> On Tuesday 03 July 2018 07:49 PM, Marcus Kool wrote:
>>>> The original intention of this default value is have a queue that 
>>>> is twice the size of the messages being processed, so for helpers 
>>>> with concurrency=NCONC and num_children=NCHILD it makes a lot of 
>>>> sense to set the default queue length to 2*NCONC*NCHILD.
>>>> I do not understand that "compatibility" with doing the wrong 
>>>> calculation is a good thing.
>>>>
>>>> Marcus
>>>>
>>>>
>>>> On 03/07/18 05:16, Amos Jeffries wrote:
>>>>> On 03/07/18 20:00, Amish wrote:
>>>>>> Hello,
>>>>>>
>>>>>> In squid 4.1 new option "queue-size" was introduced.
>>>>>>
>>>>>> In most (or all) cases default "queue-size" is set to 
>>>>>> children-max*2.
>>>>>>
>>>>>> But I believe it should be higher of (children-max*2) OR 
>>>>>> (concurrency*2)
>>>>>>
>>>>>> Or it can be some better formula but the point I am trying to 
>>>>>> make is
>>>>>> that, "concurrency" should be taken in to account for calculating
>>>>>> default value of "queue-size".
>>>>>>
>>>>>> Please consider.
>>>>>
>>>>> FYI; When we add a directive or option to control some behaviour that
>>>>> already happens the default is usually set to the value all existing
>>>>> Squid are using so nobody gets an unexpected surprise with upgrade.
>>>>>
>>>>> That can change later once more people have experimented to find 
>>>>> what a
>>>>> better value actually is.
>>>>>
>>>>> Cheers
>>>>> Amos



From squid3 at treenet.co.nz  Tue Jul  3 16:44:44 2018
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Wed, 4 Jul 2018 04:44:44 +1200
Subject: [squid-users] will multiple tls_outgoing_options lines be
 combined?
In-Reply-To: <b8462f8c-61d9-b3a9-c88b-419ad2948030@gmail.com>
References: <e309eee7-97f9-16e0-a410-037c3943886a@gmail.com>
 <1544a6f0-7b3d-9c7a-c63b-5512c85d94bb@treenet.co.nz>
 <b8462f8c-61d9-b3a9-c88b-419ad2948030@gmail.com>
Message-ID: <f866c76b-b6db-103e-cb47-c6f6305de6b9@treenet.co.nz>

On 04/07/18 01:45, Amish wrote:
> 
> On Tuesday 03 July 2018 06:07 PM, Amos Jeffries wrote:
> 
>>> Or will it pick only last (or first) option and internal defaults for
>>> rest?
>> Each option in the directive replaces previous values of that same
>> option.
>>
>> For example;
>>
>> ?? tls_outgoing_options cipher=A cipher=B
>>
>> ? is the same as:
>>
>> ?? tls_outgoing_options cipher=B
>>
>>
>> Since above you are setting different named option on every line, your
>> setup is equivalent to all the options being on one line.
>>
>>
>> However, Squid understands line wrapping. So this question is not very
>> useful. You could simplify even further like this:
>>
>> ? tls_outgoing_options \
>> ??? cafile=/etc/ssl/cert.pem \
>> ??? options=NO_SSLv3,SINGLE_DH_USE,SINGLE_ECDH_USE \
>> ??? cipher=HIGH:MEDIUM:!RC4:!aNULL:!eNULL:!LOW:....
>>
> 
> Didn't know about line wrapping support.
> 
> But my question is still useful in my case. Writing small and quick sed
> script becomes easy.

Ah, you are thinking a automatic upgrades, right?

Watch out with that. The removal of SSLv2 settings (eg NO_SSLv2) at
times makes the new config directive halt with an OpenSSL unsupported
option error.

Amos


From marcus.kool at urlfilterdb.com  Tue Jul  3 16:45:14 2018
From: marcus.kool at urlfilterdb.com (Marcus Kool)
Date: Tue, 3 Jul 2018 13:45:14 -0300
Subject: [squid-users] squid 4.1 default queue-size should consider
 concurrency
In-Reply-To: <32581779-f565-8a4b-5166-ccc554051342@measurement-factory.com>
References: <d2df8e0b-7317-6253-4e48-7b1638a834f7@gmail.com>
 <63fb8a89-cc9b-d053-b46c-aa5ab39f4ea3@treenet.co.nz>
 <7747946a-212b-5248-794a-4e6fed1e30c7@urlfilterdb.com>
 <1ba00b33-67d8-f142-8cf7-9378164336dc@gmail.com>
 <facb1b27-fbb3-4276-68f5-c55cde074770@urlfilterdb.com>
 <32581779-f565-8a4b-5166-ccc554051342@measurement-factory.com>
Message-ID: <c745528f-b1b0-b034-8fbe-f650ba70800a@urlfilterdb.com>

Thanks for the clarification.  The squid.conf.documented file says
    The queue-size=N option sets the maximum number of queued requests to N.
which, for me at least, is hard to translate into
    maximum number of requests buffered because no helper can accept it.


On 03/07/18 13:09, Alex Rousskov wrote:
> Marcus,
> 
>      Based on your examples, I suspect that you are misinterpreting what
> the queue is. The request is queued only when no helper can accept it.
> The queue is not used for requests sent to helpers.
> 
> Alex.
> 
> 
> 


From marcus.kool at urlfilterdb.com  Tue Jul  3 16:52:50 2018
From: marcus.kool at urlfilterdb.com (Marcus Kool)
Date: Tue, 3 Jul 2018 13:52:50 -0300
Subject: [squid-users] squid 4.1 default queue-size should consider
 concurrency
In-Reply-To: <0eba6da6-d5ef-537e-c768-2bd8b9e9337d@measurement-factory.com>
References: <d2df8e0b-7317-6253-4e48-7b1638a834f7@gmail.com>
 <63fb8a89-cc9b-d053-b46c-aa5ab39f4ea3@treenet.co.nz>
 <7747946a-212b-5248-794a-4e6fed1e30c7@urlfilterdb.com>
 <0eba6da6-d5ef-537e-c768-2bd8b9e9337d@measurement-factory.com>
Message-ID: <f5f463b5-ea90-9c02-0782-cbb730b78b37@urlfilterdb.com>



On 03/07/18 12:54, Alex Rousskov wrote:
> On 07/03/2018 08:19 AM, Marcus Kool wrote:

> If you think Squid should use a different default for all or some helper
> categories, please post a proposal that documents pros and cons and
> justifies the change. The URL above can be used as your guide to helper
> categories.

With your clarification of what the queue is used for, I no longer consider my proposal for a different default queue size valid.

I do like to see better documentation for the new queue-size option.
Including your one-liner in squid.conf.documented is enough for me.

Marcus

> 
> Thank you,
> 
> Alex.


From vishali.somaskanthan at viptela.com  Tue Jul  3 18:02:54 2018
From: vishali.somaskanthan at viptela.com (Vishali Somaskanthan)
Date: Tue, 3 Jul 2018 11:02:54 -0700
Subject: [squid-users] Behavior of Squid with SSL Bump and server
 persistent connections
In-Reply-To: <7e2e6b3f-a642-307e-c53f-a59d3a32a877@measurement-factory.com>
References: <CABfsTT522AoBc60_bWg0WYo5+tjLx=fAwDbaB711YZ+6rpnrzQ@mail.gmail.com>
 <7e2e6b3f-a642-307e-c53f-a59d3a32a877@measurement-factory.com>
Message-ID: <CABfsTT4L29DD1KSHcpaQHPqCCYc83XUF5gVJF-RH6AW2FaZdxA@mail.gmail.com>

*Thanks for the quick reply. I want to explain my question further.*

*Consider C1 and S1 connections were created for a HTTPs connection using
ssl-bump. C1 has been served and closed from the client side.*

*Now, the client initiates another HTTPS connection, C2. Since, persistent
connection is enabled, expectation is to see that S1 gets re-used.*

*Behaviour seen now is that S2 gets created and a handshake ensues between
squid and server. After ~30seconds, S1 is re-used to serve the*

*request C2. Persistence seems to work since S1 is re-used. However, why
was S2 initiated and why was S1 re-used after ~30seconds?*


*PFA: pcap file and the squid.conf*

On Mon, Jul 2, 2018 at 4:57 PM, Alex Rousskov <
rousskov at measurement-factory.com> wrote:

> On 07/02/2018 05:34 PM, Vishali Somaskanthan wrote:
>
> > I am trying out SSL Bump for my connections from Squid to server and
> > trying to put along server persistent connections as well. I would like
> > to know how squid behaves with both of these turned on??
>
> In modern Squids, all(*) bumped SSL client HTTP requests (from client
> connection C) should use the corresponding bumped connection to the
> server (S). After the first HTTP request, if more requests arrive on
> connection C, and they are all regular/basic requests, then they can all
> go through connection S. Once HTTP rules, timeouts, or other factors
> prohibit connection S or connection C reuse, Squid should close both
> connections.
>
> Please note that I do not know whether Squid correctly forces all(*)
> HTTP requests on connection C to connection S, but it should. If it does
> not, file a bug report. Same for closing connection C when connection S
> becomes unusable.
>
>
> > I see info in the squid wiki page that SSL Bump creates fake CONNECT
> > requests and Peeking at Step1 creates another CONNECT request.
>
> Peeking or staring may indeed produce internal fake CONNECT requests,
> but they are unrelated to your question. They are used internally to
> handle the client TLS connection and for giving adaptation services a
> say in the matter. Persistency is an HTTP term that is applied to what
> happens _after_ the TLS connections is bumped.
>
> (Also, peeking is a part of the SslBump feature -- they are not two
> different actions or stages as "and" in your summary implies).
>
>
> HTH,
>
> Alex.
> P.S. (*) "all" should be interpreted as "all that need a server
> connection" here -- pure cache hits, adaptation-satisfied requests, and
> probably some erroneous requests (e.g., those blocked by http_access
> rules?) do not use the server connection.
>



-- 
Regards,
Vishali Somaskanthan
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20180703/c38ea9af/attachment.htm>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: bump-persistent-connections.pcap
Type: application/octet-stream
Size: 37710 bytes
Desc: not available
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20180703/c38ea9af/attachment.obj>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: squid.conf
Type: application/octet-stream
Size: 2129 bytes
Desc: not available
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20180703/c38ea9af/attachment-0001.obj>

From rousskov at measurement-factory.com  Tue Jul  3 19:57:31 2018
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Tue, 3 Jul 2018 13:57:31 -0600
Subject: [squid-users] Behavior of Squid with SSL Bump and server
 persistent connections
In-Reply-To: <CABfsTT4L29DD1KSHcpaQHPqCCYc83XUF5gVJF-RH6AW2FaZdxA@mail.gmail.com>
References: <CABfsTT522AoBc60_bWg0WYo5+tjLx=fAwDbaB711YZ+6rpnrzQ@mail.gmail.com>
 <7e2e6b3f-a642-307e-c53f-a59d3a32a877@measurement-factory.com>
 <CABfsTT4L29DD1KSHcpaQHPqCCYc83XUF5gVJF-RH6AW2FaZdxA@mail.gmail.com>
Message-ID: <10988743-069b-eb83-4876-ef99981da101@measurement-factory.com>

On 07/03/2018 12:02 PM, Vishali Somaskanthan wrote:

> Consider C1 and S1 connections were created for a HTTPs connection
> using ssl-bump. C1 has been served and closed from the client side.

Please note that C1/S1 could serve many requests before C1 got closed.
However, let's focus on the cross-client reuse you are asking about.

There are two different cases to consider:

* ssl_bump stare step2:  In my previous email, I was talking about the
(more typical) case of staring at the client and server before bumping
the connection. In that case, Squid can keep S1 open after closing C1
only if Squid can re-mimic S1 TLS properties when C2 comes. If Squid
cannot, Squid should close S1 as non-reusable. I am not sure, but I
suspect that Squid cannot mimic properties of an already open connection
(yet). If that suspicion is accurate, then Squid should close S1.

* ssl_bump bump step2: In your configuration, clients are bumped without
mimicking server details. In that case, one can view S1 as totally
independent from C1. When C1 is closed, S1 can remain open.


> Now, the client initiates another HTTPS connection, C2. Since,
> persistent connection is enabled, expectation is to see that S1 gets
> re-used.

Same two cases as above:

* ssl_bump stare step2: The TLS properties negotiated on the C2
connection depend on the server connection properties and, if S2 is
established, may end up being different than the TLS properties of C1
(which depends on S1). To honor its configuration, when C1 comes, Squid
should either open a new S2 connection or mimic the properties of the S1
connection. Again, I suspect that Squid does not know how to do the
latter so it opens S2.

* ssl_bump bump step2: In your configuration, clients are bumped without
mimicking server details. In that case, one can view S1 as totally
independent from C1. When C2 comes, S1 can be reused.


> Behaviour seen now is that S2 gets created and a handshake ensues
> between squid and server.


> After ~30seconds, S1 is re-used to serve the request C2.

If there are no other in-progress requests for S, then the combination
of the two seems like a bug to me: Squid (in your configuration) should
either reuse S1 right away or never reuse S1.

I am not sure which connections you capture represents, but it seems to
be missing packets for the second (client port 46550) TCP stream (no SYN
packet, for example). Without you publishing the keys for these
connections, it is not possible to see what happens at HTTP level.
Posting dedicated Squid debugging logs may be a better strategy.


HTH,

Alex.



> On Mon, Jul 2, 2018 at 4:57 PM, Alex Rousskov wrote:
> 
>     On 07/02/2018 05:34 PM, Vishali Somaskanthan wrote:
> 
>     > I am trying out SSL Bump for my connections from Squid to server and
>     > trying to put along server persistent connections as well. I would like
>     > to know how squid behaves with both of these turned on??
> 
>     In modern Squids, all(*) bumped SSL client HTTP requests (from client
>     connection C) should use the corresponding bumped connection to the
>     server (S). After the first HTTP request, if more requests arrive on
>     connection C, and they are all regular/basic requests, then they can all
>     go through connection S. Once HTTP rules, timeouts, or other factors
>     prohibit connection S or connection C reuse, Squid should close both
>     connections.
> 
>     Please note that I do not know whether Squid correctly forces all(*)
>     HTTP requests on connection C to connection S, but it should. If it does
>     not, file a bug report. Same for closing connection C when connection S
>     becomes unusable.
> 
> 
>     > I see info in the squid wiki page that SSL Bump creates fake CONNECT
>     > requests and Peeking at Step1 creates another CONNECT request. 
> 
>     Peeking or staring may indeed produce internal fake CONNECT requests,
>     but they are unrelated to your question. They are used internally to
>     handle the client TLS connection and for giving adaptation services a
>     say in the matter. Persistency is an HTTP term that is applied to what
>     happens _after_ the TLS connections is bumped.
> 
>     (Also, peeking is a part of the SslBump feature -- they are not two
>     different actions or stages as "and" in your summary implies).
> 
> 
>     HTH,
> 
>     Alex.
>     P.S. (*) "all" should be interpreted as "all that need a server
>     connection" here -- pure cache hits, adaptation-satisfied requests, and
>     probably some erroneous requests (e.g., those blocked by http_access
>     rules?) do not use the server connection.
> 
> 
> 
> 
> -- 
> Regards,
> Vishali Somaskanthan
> 
> 
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
> 



From rousskov at measurement-factory.com  Tue Jul  3 20:50:52 2018
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Tue, 3 Jul 2018 14:50:52 -0600
Subject: [squid-users] squid 4.1 default queue-size should consider
 concurrency
In-Reply-To: <f5f463b5-ea90-9c02-0782-cbb730b78b37@urlfilterdb.com>
References: <d2df8e0b-7317-6253-4e48-7b1638a834f7@gmail.com>
 <63fb8a89-cc9b-d053-b46c-aa5ab39f4ea3@treenet.co.nz>
 <7747946a-212b-5248-794a-4e6fed1e30c7@urlfilterdb.com>
 <0eba6da6-d5ef-537e-c768-2bd8b9e9337d@measurement-factory.com>
 <f5f463b5-ea90-9c02-0782-cbb730b78b37@urlfilterdb.com>
Message-ID: <dc740a62-5bba-98b1-29db-ddadb6dfccbb@measurement-factory.com>

On 07/03/2018 10:52 AM, Marcus Kool wrote:

> I do like to see better documentation for the new queue-size option.
> Including your one-liner in squid.conf.documented is enough for me.

I wish it were that simple! For starters, there are at least six
independent and slightly different contexts where this queuing should be
documented.

Please proof read: https://github.com/squid-cache/squid/pull/238

Alex.


From marcus.kool at urlfilterdb.com  Tue Jul  3 23:18:58 2018
From: marcus.kool at urlfilterdb.com (Marcus Kool)
Date: Tue, 3 Jul 2018 20:18:58 -0300
Subject: [squid-users] squid 4.1 default queue-size should consider
 concurrency
In-Reply-To: <dc740a62-5bba-98b1-29db-ddadb6dfccbb@measurement-factory.com>
References: <d2df8e0b-7317-6253-4e48-7b1638a834f7@gmail.com>
 <63fb8a89-cc9b-d053-b46c-aa5ab39f4ea3@treenet.co.nz>
 <7747946a-212b-5248-794a-4e6fed1e30c7@urlfilterdb.com>
 <0eba6da6-d5ef-537e-c768-2bd8b9e9337d@measurement-factory.com>
 <f5f463b5-ea90-9c02-0782-cbb730b78b37@urlfilterdb.com>
 <dc740a62-5bba-98b1-29db-ddadb6dfccbb@measurement-factory.com>
Message-ID: <55a47e20-931d-34af-877c-09f5e2c6ff93@urlfilterdb.com>

I read the changes and like them.

I also looked at the error messages that Squid produces when helpers are overloaded.
It would be nice if in external_acl.cc, helper.cc and redirect.cc the debugs( ... DBG_IMPORTANT ... ) messages have additional text like
    #children, concurrency or queue-size may need adjustment

Thanks
Marcus

On 03/07/18 17:50, Alex Rousskov wrote:
> On 07/03/2018 10:52 AM, Marcus Kool wrote:
> 
>> I do like to see better documentation for the new queue-size option.
>> Including your one-liner in squid.conf.documented is enough for me.
> 
> I wish it were that simple! For starters, there are at least six
> independent and slightly different contexts where this queuing should be
> documented.
> 
> Please proof read: https://github.com/squid-cache/squid/pull/238
> 
> Alex.
> 
> 


From vh1988 at yahoo.com.ar  Wed Jul  4 00:06:42 2018
From: vh1988 at yahoo.com.ar (Julian Perconti)
Date: Tue, 3 Jul 2018 21:06:42 -0300
Subject: [squid-users] Squid 4.1 Error negotiating SSL connection
Message-ID: <007e01d4132a$e5741050$b05c30f0$@yahoo.com.ar>

Hi all,

 

I have installed squid 4.1 on debian 9 with openssl 1.1.0f on transparent
mode.

 

I need to know how to track this error: (debbuging options is almost
impossible i mean examine the FD, etc.)

 

kid1| Error negotiating SSL connection on FD 19:
error:00000001:lib(0):func(0):reason(1) (1/-1)

 

There are a lot of them in cache.log when mobile devices uses (unsuccefully)
apps like instagram/Pinterest/Facebook/twitter, etc.

 

Neither is a "cipher-out" problem because I just tried: tls_outgoing_options
cipher=ALL (only for testing)

 

>From any PC those sites works well. So there is not a certificate missing
problem.

 

Here a copy of most relevant config: 

 

=================CFG==================

 

http_port 3128

http_port 3129 intercept

https_port 3130 intercept ssl-bump \

  cert=/etc/squid/ssl_cert/squid4ssl.pem \

  key=/etc/squid/ssl_cert/squid4ssl.pem \

  generate-host-certificates=on dynamic_cert_mem_cache_size=4MB

 

sslcrtd_program /lib/squid/security_file_certgen -s /var/lib/ssl_db -M 4MB

 

tls_outgoing_options cafile=/etc/ssl/certs/ca-certificates.crt

tls_outgoing_options cafile=/etc/squid/ssl_cert/cabundle.pem

tls_outgoing_options options=NO_SSLv3

tls_outgoing_options
cipher=ALL:!SSLv2:!ADH:!DSS:!MD5:!EXP:!DES:!PSK:!SRP:!RC4:!IDEA:!SEED:!aNULL
:!eNULL

 

acl step1 at_step SslBump1

acl step2 at_step SslBump2

acl step3 at_step SslBump3

 

acl noBumpSites ssl::server_name_regex -i "/etc/squid/url.nobump"

 

ssl_bump peek step1 all             

ssl_bump peek step2 noBumpSites     

ssl_bump splice step3 noBumpSites   

ssl_bump stare step2                

ssl_bump bump step3             

 

# cache ram

cache_mem 1024 MB

=================CFG==================

 

And so on..

 

Any suggestiong on the config above? Or a workaround the problem mentioned?

 

Thank you all!

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20180703/8f141a7e/attachment.htm>

From squid3 at treenet.co.nz  Wed Jul  4 03:50:51 2018
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Wed, 4 Jul 2018 15:50:51 +1200
Subject: [squid-users] squid 4.1 default queue-size should consider
 concurrency
In-Reply-To: <55a47e20-931d-34af-877c-09f5e2c6ff93@urlfilterdb.com>
References: <d2df8e0b-7317-6253-4e48-7b1638a834f7@gmail.com>
 <63fb8a89-cc9b-d053-b46c-aa5ab39f4ea3@treenet.co.nz>
 <7747946a-212b-5248-794a-4e6fed1e30c7@urlfilterdb.com>
 <0eba6da6-d5ef-537e-c768-2bd8b9e9337d@measurement-factory.com>
 <f5f463b5-ea90-9c02-0782-cbb730b78b37@urlfilterdb.com>
 <dc740a62-5bba-98b1-29db-ddadb6dfccbb@measurement-factory.com>
 <55a47e20-931d-34af-877c-09f5e2c6ff93@urlfilterdb.com>
Message-ID: <11967d19-cb83-2501-7222-dc33437aee06@treenet.co.nz>

On 04/07/18 11:18, Marcus Kool wrote:
> I read the changes and like them.
> 

FYI: if you have a github account you should be able to post an
approve/change review, aka vote for the PR merge bot.


> I also looked at the error messages that Squid produces when helpers are
> overloaded.
> It would be nice if in external_acl.cc, helper.cc and redirect.cc the
> debugs( ... DBG_IMPORTANT ... ) messages have additional text like
> ?? #children, concurrency or queue-size may need adjustment
> 
> Thanks
> Marcus
> 
> On 03/07/18 17:50, Alex Rousskov wrote:
>> On 07/03/2018 10:52 AM, Marcus Kool wrote:
>>
>>> I do like to see better documentation for the new queue-size option.
>>> Including your one-liner in squid.conf.documented is enough for me.
>>
>> I wish it were that simple! For starters, there are at least six
>> independent and slightly different contexts where this queuing should be
>> documented.
>>
>> Please proof read: https://github.com/squid-cache/squid/pull/238
>>
>> Alex.
>>

Amos


From squid3 at treenet.co.nz  Wed Jul  4 04:20:45 2018
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Wed, 4 Jul 2018 16:20:45 +1200
Subject: [squid-users] Squid 4.1 Error negotiating SSL connection
In-Reply-To: <007e01d4132a$e5741050$b05c30f0$@yahoo.com.ar>
References: <007e01d4132a$e5741050$b05c30f0$@yahoo.com.ar>
Message-ID: <e973f4ab-3137-c391-1982-7dc412aad506@treenet.co.nz>

On 04/07/18 12:06, Julian Perconti wrote:
> Hi all,
> 
> ?
> 
> I have installed squid 4.1 on debian 9 with openssl 1.1.0f on
> transparent mode.
> 
> ?
> 
> I need to know how to track this error: (debbuging options is almost
> impossible i mean examine the FD, etc.)
> 

The SSL-Bump activity is fairly complex at times and involves many
different layers and components. So an ALL,9 or ALL,7 debug log may be
necessary to trace the actions.

> ?
> 
> kid1| Error negotiating SSL connection on FD 19:
> error:00000001:lib(0):func(0):reason(1) (1/-1)
> 
> 


Those annoyingly opaque error messages are produced by your OpenSSL library.

Other programs showing that same string apparently are negotiating
protocol version for the messaging layer or handshake format which are
incompatible with the choice of ciphers. eg SSLv2 message syntax with
TLS ciphers, or SSLv3 message syntax with  TLS/1.2-only ciphers.

Since you have done the cipher test, it may be the SSLv2 issue or some
TLS extension being attempted.


If cache.log is too obscure a packet trace with wireshark may be less
so. The clear-text part of TLS at the start should have better hints
about the issue, whatever it is.


?
> 
> There are a lot of them in cache.log when mobile devices uses
> (unsuccefully) apps like instagram/Pinterest/Facebook/twitter, etc.
> 
> ?
> 
> Neither is a ?cipher-out? problem because I just tried:
> tls_outgoing_options cipher=ALL (only for testing)
> 

This test is mistaken.

"cipher=ALL" and "options=ALL" actually mean to actively *enable* lots
of things OpenSSL would normally disable. This still counts as
restriction, because only things compatible with the most obsolete or
broken cipher/option can be negotiated.

A correct test would be to _remove_ the cipher=* option entirely from
your config and see what changes.

With no manual restrictions the issues are then limited to natural
differences in OpenSSL version between client and Squid.


> 
> From any PC those sites works well. So there is not a certificate
> missing problem.
> 

When SSL-Bump is done crypto issues are the union of configured
capabilities at client (PC), proxy (Squid), server - plus the 3
particular crypto libraries on each of those uses. So 6 possible points
of failure, all affecting each other.

I find it is often a LOT easier (and more successful) to look at the TLS
handshake itself and see what is actually happening. Then figure out
from there what needs tuning to work around it.


> 
> Here a copy of most relevant config:
> 
> ?
> 
> =================CFG==================
> 
> ?
> 
> http_port 3128
> 
> http_port 3129 intercept
> 
> https_port 3130 intercept ssl-bump \
> 
> ? cert=/etc/squid/ssl_cert/squid4ssl.pem \
> 
> ? key=/etc/squid/ssl_cert/squid4ssl.pem \
> 
> ? generate-host-certificates=on dynamic_cert_mem_cache_size=4MB
> 
> ?
> 
> sslcrtd_program /lib/squid/security_file_certgen -s /var/lib/ssl_db -M 4MB
> 
> ?
> 
> tls_outgoing_options cafile=/etc/ssl/certs/ca-certificates.crt
> 
> tls_outgoing_options cafile=/etc/squid/ssl_cert/cabundle.pem
> 
> tls_outgoing_options options=NO_SSLv3
> 

This NO_SSLv3 may be part of issue. AFAIK when SSLv3 compatibility is no
longer required the latest OpenSSL is able to move to pure TLS message
syntax which has a few usually very minor differences which TLS/1.3 uses.

The services you mention are the ones IME most likely to be adopting
TLS/1.3 already when clients like your Squid accept it. Which is where
PC vs Squid library differences can lead to drastically different
visible outcomes.


> tls_outgoing_options
> cipher=ALL:!SSLv2:!ADH:!DSS:!MD5:!EXP:!DES:!PSK:!SRP:!RC4:!IDEA:!SEED:!aNULL:!eNULL
> 


HTH
Amos


From squid3 at treenet.co.nz  Wed Jul  4 05:02:50 2018
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Wed, 4 Jul 2018 17:02:50 +1200
Subject: [squid-users] [squid-announce] Squid 4.1 is available
Message-ID: <c8f9202f-47b1-c34e-5480-8191d57f8a2b@treenet.co.nz>

The Squid HTTP Proxy team is very pleased to announce the availability
of the Squid-4.1 release!


This release is we believe, stable enough for general production use.

Support for Squid-3.x bug fixes has now officially ceased. Bugs in 3.5
will continue to be fixed, however the fixes will be added to the 4.x
series. All users of Squid-3.x are encouraged to plan for upgrades.


A short list of the major new features is:

 * RFC 6176 compliance (SSLv2 support removal)
 * Secure ICAP service connections
 * Add url_lfs_rewrite: a URL-rewriter based on local file existence
 * on_unsupported_protocol directive to allow Non-HTTP bypass
 * Update external_acl_type directive to use logformat codes
 * Experimental GnuTLS support for some TLS features
 * TLS/SSL related helpers renamed


Several features have been removed in 4.1:

 * refresh_pattern ignore-auth and ignore-must-revalidate options
 * cache_peer_domain directive
 * basic_msnt_multi_domain_auth helper
 * ESI custom parser - use XML2 or Expat instead.

Further details can be found in the release notes or the wiki.
http://www.squid-cache.org/Versions/v4/RELEASENOTES.html
http://wiki.squid-cache.org/Squid-4


Please remember to run "squid -k parse" when testing upgrade to a new
version of Squid. It will audit your configuration files and report
any identifiable issues the new release will have in your installation
before you "press go".

Please be particularly aware that for the TLS features the removal of
SSLv2 support may require manual attention to configuration settings
when upgrading from any Squid-3 or older version.


All feature additions are considered *experimental* until they have
survived at least one series of releases in general production use.
Please be aware of that when rolling out features which are new in
this series. Not all use-cases have been well tested yet and some may
not even have been implemented. Assistance is still needed despite the
releases general stability level.


Plans for the next series of releases is already well underway. Our
future release plans and upcoming features can be found at:
http://wiki.squid-cache.org/RoadMap


 See the ChangeLog for the full list of changes in this and earlier
 releases.

  All users of Squid-4.0 beta releases are urged to upgrade to this
  release as soon as possible.

  All users of Squid-3 are encouraged to upgrades where possible.


See the ChangeLog for the full list of changes in this and earlier
releases.

Please refer to the release notes at
http://www.squid-cache.org/Versions/v4/RELEASENOTES.html
when you are ready to make the switch to Squid-4

This new release can be downloaded from our HTTP or FTP servers

  http://www.squid-cache.org/Versions/v4/
  ftp://ftp.squid-cache.org/pub/squid/
  ftp://ftp.squid-cache.org/pub/archive/4/

or the mirrors. For a list of mirror sites see

  http://www.squid-cache.org/Download/http-mirrors.html
  http://www.squid-cache.org/Download/mirrors.html

If you encounter any issues with this release please file a bug report.
  http://bugs.squid-cache.org/


Amos Jeffries
_______________________________________________
squid-announce mailing list
squid-announce at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-announce

From eliezer at ngtech.co.il  Wed Jul  4 21:04:58 2018
From: eliezer at ngtech.co.il (Eliezer Croitoru)
Date: Thu, 05 Jul 2018 00:04:58 +0300
Subject: [squid-users] Squid 4.1 for CentOS rpms
In-Reply-To: <2197768425D7F5479A0FFB3FEC212F7FF63D99C3@aesmail.surcouf.local>
References: <2197768425D7F5479A0FFB3FEC212F7FF63D99C3@aesmail.surcouf.local>
Message-ID: <c3df1de412e5a91301700941545fccfd@ngtech.co.il>

Hey Mike,

I am planning to add 4.1 into the repository but I first need to compile 
it..
I hope that I will have time for it next week.
For no 4.0.25 seems pretty nice but I have not tested it yet in:
- intercept
- tproxy
- on_unsupport..

And couple others.

When I will have the RPM's ready for installation I will first add them 
into the beta repo and run try to run a series of tests.
If 4.1 will run smoothly with the same setup that works with 4.0.25 I 
will add 4.1 to the main stable repository.

If i'm not wrong there have been changes with some versions of OpenSSL 
at 4.X and I do not remember what it was.

Ping this thread if in a week or two you don't see any post about the 
new version.
(I have been waiting to these release for more then a year)

Thanks,
Eliezer

On 2018-07-03 13:12, Mike Surcouf wrote:
> Hi Eliezer
> 
> I have been using your repos on CentOS for many years thank you for
> your hard work.
> Are you planning a stable repo for v4 now it's out.
> 
> Many Thanks
> 
> Mike
> 
> 
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users

-- 
----
Eliezer Croitoru
Linux System Administrator
Mobile: +972-5-28704261
Email: eliezer at ngtech.co.il


From eliezer at ngtech.co.il  Wed Jul  4 21:12:24 2018
From: eliezer at ngtech.co.il (Eliezer Croitoru)
Date: Thu, 05 Jul 2018 00:12:24 +0300
Subject: [squid-users] when will squid 4 be production ready?
In-Reply-To: <4bce3661-52e2-7efa-a57e-0aab8141d489@measurement-factory.com>
References: <CAK0iFYyuRxLNHOXhLy3=xB7PdyKW5SGqM-E3a-LT5+Wf3NfgdA@mail.gmail.com>
 <be353a9b-c0bc-02dc-e132-d4c0a91dde0e@treenet.co.nz>
 <613e11af6c4366c3fc1a722d3898ce8e@ngtech.co.il>
 <4d9d6bd0-e505-b343-1f86-7dcc135388a4@treenet.co.nz>
 <035b01d40f34$2fbf4fd0$8f3def70$@ngtech.co.il>
 <4bce3661-52e2-7efa-a57e-0aab8141d489@measurement-factory.com>
Message-ID: <74db2ccec2afbb311c4d2f9cad77db32@ngtech.co.il>

Just to be clear..(since I see that I didn't sent a "replay all" email)

I am talking about a ssl_db directory creation ie like in this line:
https://github.com/rockyluke/docker-squid/blob/fe98b0c27cc2757268c5a07bf862e7f5bfa9570d/4.0.21/Dockerfile#L69

> /bin/security_file_certgen -c -s /var/lib/ssl_db

it still requires a "-M" not when I run it as a helper of squid at 
"start\run" time.

Did the issue makes more sense now?

Eliezer

On 2018-06-29 05:28, Alex Rousskov wrote:
> On 06/28/2018 05:03 PM, Eliezer Croitoru wrote:
>> /usr/lib64/squid/security_file_certgen: security_file_certgen -s 
>> requires an -M parameter
> 
>> which is weird, why would I need the "-M" when creating the ssl_db 
>> directory which is a bunch of empty files?
> 
> The helper needs -M to know when to start deleting cached certificates.
> The -s parameter enables certificate caching by the helper. A cache
> needs to know its maximum size.
> 
> If you do not need a cache, do not specify -s (provided that recent
> feature was ported to v4).
> 
> Alex.

-- 
----
Eliezer Croitoru
Linux System Administrator
Mobile: +972-5-28704261
Email: eliezer at ngtech.co.il


From squid3 at treenet.co.nz  Thu Jul  5 05:03:48 2018
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 5 Jul 2018 17:03:48 +1200
Subject: [squid-users] Squid 4.1 for CentOS rpms
In-Reply-To: <c3df1de412e5a91301700941545fccfd@ngtech.co.il>
References: <2197768425D7F5479A0FFB3FEC212F7FF63D99C3@aesmail.surcouf.local>
 <c3df1de412e5a91301700941545fccfd@ngtech.co.il>
Message-ID: <53d85ea2-5cc7-944c-206b-df68ee84579f@treenet.co.nz>

On 05/07/18 09:04, Eliezer Croitoru wrote:
> 
> If i'm not wrong there have been changes with some versions of OpenSSL
> at 4.X and I do not remember what it was.

Basically Squid-4 supports OpenSSL v1.1. Squid-3 does not.

The other TLS/SSL changes in v4 are Squid features and behaviour not
specially OpenSSL itself.

Amos


From squid3 at treenet.co.nz  Thu Jul  5 05:11:45 2018
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 5 Jul 2018 17:11:45 +1200
Subject: [squid-users] when will squid 4 be production ready?
In-Reply-To: <74db2ccec2afbb311c4d2f9cad77db32@ngtech.co.il>
References: <CAK0iFYyuRxLNHOXhLy3=xB7PdyKW5SGqM-E3a-LT5+Wf3NfgdA@mail.gmail.com>
 <be353a9b-c0bc-02dc-e132-d4c0a91dde0e@treenet.co.nz>
 <613e11af6c4366c3fc1a722d3898ce8e@ngtech.co.il>
 <4d9d6bd0-e505-b343-1f86-7dcc135388a4@treenet.co.nz>
 <035b01d40f34$2fbf4fd0$8f3def70$@ngtech.co.il>
 <4bce3661-52e2-7efa-a57e-0aab8141d489@measurement-factory.com>
 <74db2ccec2afbb311c4d2f9cad77db32@ngtech.co.il>
Message-ID: <9460b90f-eb45-cf89-6976-06c803680f55@treenet.co.nz>

On 05/07/18 09:12, Eliezer Croitoru wrote:
> Just to be clear..(since I see that I didn't sent a "replay all" email)
> 
> I am talking about a ssl_db directory creation ie like in this line:
> https://github.com/rockyluke/docker-squid/blob/fe98b0c27cc2757268c5a07bf862e7f5bfa9570d/4.0.21/Dockerfile#L69
> 

That image is broken in other ways as well:
* The ./configure options are not placing the certgen binary where the
symlink points to (/usr/lib vs /lib).
* installing wget but using curl
* using chown manually without updating SELinux permissions


> 
>> /bin/security_file_certgen -c -s /var/lib/ssl_db
> 
> it still requires a "-M" not when I run it as a helper of squid at
> "start\run" time.
> 
> Did the issue makes more sense now?

It did before too. Alex answer is correct.

I have updated the wiki to match reality.


Amos


From laurent.verheirstraeten at univ-rennes1.fr  Thu Jul  5 12:49:31 2018
From: laurent.verheirstraeten at univ-rennes1.fr (Laurent Verheirstraeten)
Date: Thu, 5 Jul 2018 14:49:31 +0200
Subject: [squid-users] Problems with Splicing and DNS
Message-ID: <918a0ef0-99d7-1ad1-edc8-4f5352d22cd4@univ-rennes1.fr>

Hi,

We have to deal with to a problem when using the function ?Peak and 
Splice? on the version 3.5.27 of Squid.

We tried and set up a transparent proxy, but the rules we declared are 
not taken into account because both (squid) server and client are not 
using allways the same DNS.
(we?re using a pool off 2 different DNS servers, not using the same cache ).

We?ve noticed that the IP addresses taken into account by the server 
Squid and the client are not the same while solving the hostname.

In that special case, Squid sends an error during the ?Splice?. When the 
IP addresses are the same, then the function ?Splice? works perfectly.

Is there a way into Squid to specify the same IP address on both sides?

Have you already seen that kind of problem ?

Thanking ye,

Regards

 ??? Laurent Verheirstraeten

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20180705/c419c141/attachment.htm>

From squid3 at treenet.co.nz  Thu Jul  5 18:16:17 2018
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Fri, 6 Jul 2018 06:16:17 +1200
Subject: [squid-users] Problems with Splicing and DNS
In-Reply-To: <918a0ef0-99d7-1ad1-edc8-4f5352d22cd4@univ-rennes1.fr>
References: <918a0ef0-99d7-1ad1-edc8-4f5352d22cd4@univ-rennes1.fr>
Message-ID: <cefbcdb6-f5f1-cf89-6950-4fb5b5a3ddc1@treenet.co.nz>

On 06/07/18 00:49, Laurent Verheirstraeten wrote:
> Hi,
> 
> We have to deal with to a problem when using the function ?Peak and
> Splice? on the version 3.5.27 of Squid.
> 

Please upgrade to Squid-4.1. It resolves quite a number of annoying
SSL-Bump issues and has far better TLS support than Squid-3.


> We tried and set up a transparent proxy, but the rules we declared are
> not taken into account because both (squid) server and client are not
> using allways the same DNS.
> (we?re using a pool off 2 different DNS servers, not using the same cache ).
> 
> We?ve noticed that the IP addresses taken into account by the server
> Squid and the client are not the same while solving the hostname.
> 
> In that special case, Squid sends an error during the ?Splice?. When the
> IP addresses are the same, then the function ?Splice? works perfectly.
> 
> Is there a way into Squid to specify the same IP address on both sides?

Having Squid use the same DNS resolver as the client makes most
occurrences of this problem go away.

<https://wiki.squid-cache.org/KnowledgeBase/HostHeaderForgery>


> 
> Have you already seen that kind of problem ?
> 

Yes. It is a well-known issue with interception proxies.


Amos


From rousskov at measurement-factory.com  Thu Jul  5 20:07:47 2018
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Thu, 5 Jul 2018 14:07:47 -0600
Subject: [squid-users] Behavior of Squid with SSL Bump and server
 persistent connections
In-Reply-To: <CABfsTT7sLNczy9L3oteEkkrBOLsrr=9s2Daft1tWzjtsQy7M7Q@mail.gmail.com>
References: <CABfsTT522AoBc60_bWg0WYo5+tjLx=fAwDbaB711YZ+6rpnrzQ@mail.gmail.com>
 <7e2e6b3f-a642-307e-c53f-a59d3a32a877@measurement-factory.com>
 <CABfsTT4L29DD1KSHcpaQHPqCCYc83XUF5gVJF-RH6AW2FaZdxA@mail.gmail.com>
 <10988743-069b-eb83-4876-ef99981da101@measurement-factory.com>
 <CABfsTT6cZfjATAowC_CDU5qWUTBEPx12ph-hcyr0f+X-y+e4xA@mail.gmail.com>
 <6e91151c-4660-ad49-2e7e-6e83ca776754@measurement-factory.com>
 <CABfsTT7sLNczy9L3oteEkkrBOLsrr=9s2Daft1tWzjtsQy7M7Q@mail.gmail.com>
Message-ID: <0d02e6e2-d138-e3ff-6405-e596c802980d@measurement-factory.com>

On 07/05/2018 12:49 PM, Vishali Somaskanthan wrote:
> PFA - pcap and cachelog. Comments appreciated.?
> Curl timings:
> /lookup: ? ? ? ?0.062/
> /connect: ? ? ? 0.062/
> /appconnect: ? ?0.081/
> /pretransfer: ? 0.081/
> /redirect: ? ? ?0.000/
> /starttransfer: 35.193/
> /total: ? ? ? ? 35.245/


As far as connection reuse is concerned, the log may show what you have
been describing earlier -- Squid opens and abandons S2. Unfortunately I
cannot quickly tell what is going on there because it is not an ALL,9
log, but I still suspect there is a bug somewhere.

As far as delays are concerned, I see at least one pause or gap in your
cache.log, but because it is not an ALL,9 log, I cannot quickly tell
what was going on around that gap. It could be that Squid was just
waiting for the second request.

> 2018/07/03 14:57:40.191 kid1| 33,5| AsyncCallQueue.cc(57) fireNext: leaving Server::doClientRead(local=157.240.22.39:443 remote=10.239.141.26:44707 FD 11 flags=33, data=0x55a01eb64578)
> 2018/07/03 14:58:15.175 kid1| 83,3| client_side_request.cc(1721) doCallouts: Doing calloutContext->clientAccessCheck()


Others may be able to extract more details from your log. I am pretty
sure it is possible. I just do not have the time to do it. Sorry.


Alex.


> On Tue, Jul 3, 2018 at 9:28 PM, Alex Rousskov wrote:
> 
>     On 07/03/2018 06:50 PM, Vishali Somaskanthan wrote:
>     > Is there any default wait time by squid at any circumstance for getting
>     > the response from server??
> 
>     No, Squid does not delay regular request processing (unless you tell it
>     to do that via, for example, delay pools). Many external actors like
>     DNS, origin server, network, adaptation services, and helpers can
>     introduce delays, of course.
> 
>     Alex.


From tarotapprentice at yahoo.com  Fri Jul  6 02:27:18 2018
From: tarotapprentice at yahoo.com (TarotApprentice)
Date: Fri, 6 Jul 2018 12:27:18 +1000
Subject: [squid-users] [squid-announce] Squid 4.1 is available
In-Reply-To: <c8f9202f-47b1-c34e-5480-8191d57f8a2b@treenet.co.nz>
References: <c8f9202f-47b1-c34e-5480-8191d57f8a2b@treenet.co.nz>
Message-ID: <87CC8F2A-F402-48ED-BA32-5B54EE39B9C2@yahoo.com>

Hopefully the Debian guys will push this through to Stretch-backports this time. 3.5.27 only made it as far as buster (testing). Unfortunately libc 2.27 is in there and that meant it wanted to update many other packages.

MarkJ 

> On 4 Jul 2018, at 3:02 pm, Amos Jeffries <squid3 at treenet.co.nz> wrote:
> 
> The Squid HTTP Proxy team is very pleased to announce the availability
> of the Squid-4.1 release!
> 
> 
> This release is we believe, stable enough for general production use.
> 
> Support for Squid-3.x bug fixes has now officially ceased. Bugs in 3.5
> will continue to be fixed, however the fixes will be added to the 4.x
> series. All users of Squid-3.x are encouraged to plan for upgrades.
> 
> 
> A short list of the major new features is:
> 
> * RFC 6176 compliance (SSLv2 support removal)
> * Secure ICAP service connections
> * Add url_lfs_rewrite: a URL-rewriter based on local file existence
> * on_unsupported_protocol directive to allow Non-HTTP bypass
> * Update external_acl_type directive to use logformat codes
> * Experimental GnuTLS support for some TLS features
> * TLS/SSL related helpers renamed
> 
> 
> Several features have been removed in 4.1:
> 
> * refresh_pattern ignore-auth and ignore-must-revalidate options
> * cache_peer_domain directive
> * basic_msnt_multi_domain_auth helper
> * ESI custom parser - use XML2 or Expat instead.
> 
> Further details can be found in the release notes or the wiki.
> http://www.squid-cache.org/Versions/v4/RELEASENOTES.html
> http://wiki.squid-cache.org/Squid-4
> 
> 
> Please remember to run "squid -k parse" when testing upgrade to a new
> version of Squid. It will audit your configuration files and report
> any identifiable issues the new release will have in your installation
> before you "press go".
> 
> Please be particularly aware that for the TLS features the removal of
> SSLv2 support may require manual attention to configuration settings
> when upgrading from any Squid-3 or older version.
> 
> 
> All feature additions are considered *experimental* until they have
> survived at least one series of releases in general production use.
> Please be aware of that when rolling out features which are new in
> this series. Not all use-cases have been well tested yet and some may
> not even have been implemented. Assistance is still needed despite the
> releases general stability level.
> 
> 
> Plans for the next series of releases is already well underway. Our
> future release plans and upcoming features can be found at:
> http://wiki.squid-cache.org/RoadMap
> 
> 
> See the ChangeLog for the full list of changes in this and earlier
> releases.
> 
>  All users of Squid-4.0 beta releases are urged to upgrade to this
>  release as soon as possible.
> 
>  All users of Squid-3 are encouraged to upgrades where possible.
> 
> 
> See the ChangeLog for the full list of changes in this and earlier
> releases.
> 
> Please refer to the release notes at
> http://www.squid-cache.org/Versions/v4/RELEASENOTES.html
> when you are ready to make the switch to Squid-4
> 
> This new release can be downloaded from our HTTP or FTP servers
> 
>  http://www.squid-cache.org/Versions/v4/
>  ftp://ftp.squid-cache.org/pub/squid/
>  ftp://ftp.squid-cache.org/pub/archive/4/
> 
> or the mirrors. For a list of mirror sites see
> 
>  http://www.squid-cache.org/Download/http-mirrors.html
>  http://www.squid-cache.org/Download/mirrors.html
> 
> If you encounter any issues with this release please file a bug report.
>  http://bugs.squid-cache.org/
> 
> 
> Amos Jeffries
> _______________________________________________
> squid-announce mailing list
> squid-announce at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-announce
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users



From squid3 at treenet.co.nz  Fri Jul  6 05:52:37 2018
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Fri, 6 Jul 2018 17:52:37 +1200
Subject: [squid-users] [squid-announce] Squid 4.1 is available
In-Reply-To: <87CC8F2A-F402-48ED-BA32-5B54EE39B9C2@yahoo.com>
References: <c8f9202f-47b1-c34e-5480-8191d57f8a2b@treenet.co.nz>
 <87CC8F2A-F402-48ED-BA32-5B54EE39B9C2@yahoo.com>
Message-ID: <178021e9-d104-184c-3125-52a8e669367d@treenet.co.nz>

On 06/07/18 14:27, TarotApprentice wrote:
> Hopefully the Debian guys will push this through to Stretch-backports
> this time. 3.5.27 only made it as far as buster (testing).
> Unfortunately libc 2.27 is in there and that meant it wanted to
> update many other packages.
> 

You can post a request to squid at packages.debian.org, or file a bug. One
of the uploaders in the team may act earlier if they are aware of interest.

Amos


From laurent.verheirstraeten at univ-rennes1.fr  Fri Jul  6 07:06:01 2018
From: laurent.verheirstraeten at univ-rennes1.fr (Laurent Verheirstraeten)
Date: Fri, 6 Jul 2018 09:06:01 +0200
Subject: [squid-users] Problems with Splicing and DNS
In-Reply-To: <cefbcdb6-f5f1-cf89-6950-4fb5b5a3ddc1@treenet.co.nz>
References: <918a0ef0-99d7-1ad1-edc8-4f5352d22cd4@univ-rennes1.fr>
 <cefbcdb6-f5f1-cf89-6950-4fb5b5a3ddc1@treenet.co.nz>
Message-ID: <31cdd220-ad79-a94b-7a78-2209398e32b6@univ-rennes1.fr>

Hello, Thank you for your reply. ok I will try this beautiful version of 
Squid 4. Regards. Laurent



Le 05/07/2018 ? 20:16, Amos Jeffries a ?crit?:
> On 06/07/18 00:49, Laurent Verheirstraeten wrote:
>> Hi,
>>
>> We have to deal with to a problem when using the function ?Peak and
>> Splice? on the version 3.5.27 of Squid.
>>
> Please upgrade to Squid-4.1. It resolves quite a number of annoying
> SSL-Bump issues and has far better TLS support than Squid-3.
>
>
>> We tried and set up a transparent proxy, but the rules we declared are
>> not taken into account because both (squid) server and client are not
>> using allways the same DNS.
>> (we?re using a pool off 2 different DNS servers, not using the same cache ).
>>
>> We?ve noticed that the IP addresses taken into account by the server
>> Squid and the client are not the same while solving the hostname.
>>
>> In that special case, Squid sends an error during the ?Splice?. When the
>> IP addresses are the same, then the function ?Splice? works perfectly.
>>
>> Is there a way into Squid to specify the same IP address on both sides?
> Having Squid use the same DNS resolver as the client makes most
> occurrences of this problem go away.
>
> <https://wiki.squid-cache.org/KnowledgeBase/HostHeaderForgery>
>
>
>> Have you already seen that kind of problem ?
>>
> Yes. It is a well-known issue with interception proxies.
>
>
> Amos
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users


-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20180706/e86b4c9a/attachment.htm>

From vh1988 at yahoo.com.ar  Fri Jul  6 23:30:32 2018
From: vh1988 at yahoo.com.ar (Julian Perconti)
Date: Fri, 6 Jul 2018 23:30:32 +0000 (UTC)
Subject: [squid-users] Squid 4.1 Error negotiating SSL connection
In-Reply-To: <e973f4ab-3137-c391-1982-7dc412aad506@treenet.co.nz>
References: <007e01d4132a$e5741050$b05c30f0$@yahoo.com.ar>
 <e973f4ab-3137-c391-1982-7dc412aad506@treenet.co.nz>
Message-ID: <1215419345.313760.1530919832603@mail.yahoo.com>

>>>>??El ?mi?rcoles?, ?4? de ?julio? de ?2018? ?01?:?21?:?12? ?-03, Amos Jeffries <squid3 at treenet.co.nz> escribi?: 
>>>>??
>>>> 
>>>>??
>>>> 
>>>>??
>>>> On 04/07/18 12:06, Julian Perconti wrote:
>>>>> Hi all,
>>>>> 
>>>>> ?
>>>>> 
>>>>> I have installed squid 4.1 on debian 9 with openssl 1.1.0f on
>>>>> transparent mode.
>>>>> 
>>>>> ?
>>>>> 
>>>>> I need to know how to track this error: (debbuging options is almost
>>>>> impossible i mean examine the FD, etc.)
>>>>> 
>>>> 
>>>> The SSL-Bump activity is fairly complex at times and involves many
>>>> different layers and components. So an ALL,9 or ALL,7 debug log may be
>>>> necessary to trace the actions.
>>>> 
>>>>> ?
>>>>> 
>>>>> kid1| Error negotiating SSL connection on FD 19:
>>>>> error:00000001:lib(0):func(0):reason(1) (1/-1)
>>>>> 
>>>>> 
>>>> 
>>>> 
>>>> Those annoyingly opaque error messages are produced by your OpenSSL library.
>>>> 
>>>> Other programs showing that same string apparently are negotiating
>>>> protocol version for the messaging layer or handshake format which are
>>>> incompatible with the choice of ciphers. eg SSLv2 message syntax with
>>>> TLS ciphers, or SSLv3 message syntax with? TLS/1.2-only ciphers.
>>>> 
>>>> Since you have done the cipher test, it may be the SSLv2 issue or some
>>>> TLS extension being attempted.
>>>> 
>>>> 
>>>> If cache.log is too obscure a packet trace with wireshark may be less
>>>> so. The clear-text part of TLS at the start should have better hints
>>>> about the issue, whatever it is.
>>>> 
>>>> 
>>>> ?
>>>>> 
>>>>> There are a lot of them in cache.log when mobile devices uses
>>>>> (unsuccefully) apps like instagram/Pinterest/Facebook/twitter, etc.
>>>>> 
>>>>> ?
>>>>> 
>>>>> Neither is a ?cipher-out? problem because I just tried:
>>>>> tls_outgoing_options cipher=ALL (only for testing)
>>>>> 
>>>> 
>>>> This test is mistaken.
>>>> 
>>>> "cipher=ALL" and "options=ALL" actually mean to actively *enable* lots
>>>> of things OpenSSL would normally disable. This still counts as
>>>> restriction, because only things compatible with the most obsolete or
>>>> broken cipher/option can be negotiated.
>>>> 
>>>> A correct test would be to _remove_ the cipher=* option entirely from
>>>> your config and see what changes.
>>>> 
>>>> With no manual restrictions the issues are then limited to natural
>>>> differences in OpenSSL version between client and Squid.
>>>> 
>>>> 
>>>>> 
>>>>> From any PC those sites works well. So there is not a certificate
>>>>> missing problem.
>>>>> 
>>>> 
>>>> When SSL-Bump is done crypto issues are the union of configured
>>>> capabilities at client (PC), proxy (Squid), server - plus the 3
>>>> particular crypto libraries on each of those uses. So 6 possible points
>>>> of failure, all affecting each other.
>>>> 
>>>> I find it is often a LOT easier (and more successful) to look at the TLS
>>>> handshake itself and see what is actually happening. Then figure out
>>>> from there what needs tuning to work around it.
>>>> 
>>>> 
>>>>> 
>>>>> Here a copy of most relevant config:
>>>>> 
>>>>> ?
>>>>> 
>>>>> =================CFG==================
>>>>> 
>>>>> ?
>>>>> 
>>>>> http_port 3128
>>>>> 
>>>>> http_port 3129 intercept
>>>>> 
>>>>> https_port 3130 intercept ssl-bump \
>>>>> 
>>>>> ? cert=/etc/squid/ssl_cert/squid4ssl.pem \
>>>>> 
>>>>> ? key=/etc/squid/ssl_cert/squid4ssl.pem \
>>>>> 
>>>>> ? generate-host-certificates=on dynamic_cert_mem_cache_size=4MB
>>>>> 
>>>>> ?
>>>>> 
>>>>> sslcrtd_program /lib/squid/security_file_certgen -s /var/lib/ssl_db -M 4MB
>>>>> 
>>>>> ?
>>>>> 
>>>>> tls_outgoing_options cafile=/etc/ssl/certs/ca-certificates.crt
>>>>> 
>>>>> tls_outgoing_options cafile=/etc/squid/ssl_cert/cabundle.pem
>>>>> 
>>>>> tls_outgoing_options options=NO_SSLv3
>>>>> 
>>>> 
>>>> This NO_SSLv3 may be part of issue. AFAIK when SSLv3 compatibility is no
>>>> longer required the latest OpenSSL is able to move to pure TLS message
>>>> syntax which has a few usually very minor differences which TLS/1.3 uses.
>>>> 
>>>> The services you mention are the ones IME most likely to be adopting
>>>> TLS/1.3 already when clients like your Squid accept it. Which is where
>>>> PC vs Squid library differences can lead to drastically different
>>>> visible outcomes.
>>>> 
>>>> 
>>>>> tls_outgoing_options
>>>>> cipher=ALL:!SSLv2:!ADH:!DSS:!MD5:!EXP:!DES:!PSK:!SRP:!RC4:!IDEA:!SEED:!aNULL:!eNULL
>>>>> 
>>>> 
>>>> 
>>>> HTH
>>>> Amos
>>>> _______________________________________________
>>>> squid-users mailing list
>>>> squid-users at lists.squid-cache.org
>>>> http://lists.squid-cache.org/listinfo/squid-users



Hi Amos,

The temporary workaround I found was add the domains that causes those errors to the splice domains list. 

So, in cache.log now I only can see "TCP_TUNNEL" and (almost all) the Apps on mobile phones work fine. 

Thank You. 


From anon.amish at gmail.com  Sat Jul  7 15:43:47 2018
From: anon.amish at gmail.com (Amish)
Date: Sat, 7 Jul 2018 21:13:47 +0530
Subject: [squid-users] squid 4.1: is tls-cert same as cert for http_port?
Message-ID: <b7e89453-ed87-ee9b-90eb-335287947cdb@gmail.com>

Hello

While evaluating new squid 4.1, I noticed (possibly) "unannounced" 
change in http_port directive.

cert and key options appear to have changed to tls-cert and tls-key. Old 
"cert" option still seem to be working though.

In documentaton, description of tls-cert is too lengthy (and bit 
confusing) while that of cert was just one line.

So I would like to have more clarity if tls-cert is direct replacement 
of cert OR I need to consider some other things too?

Thanks and regards,

Amish.



From squid3 at treenet.co.nz  Sat Jul  7 17:15:07 2018
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sun, 8 Jul 2018 05:15:07 +1200
Subject: [squid-users] squid 4.1: is tls-cert same as cert for http_port?
In-Reply-To: <b7e89453-ed87-ee9b-90eb-335287947cdb@gmail.com>
References: <b7e89453-ed87-ee9b-90eb-335287947cdb@gmail.com>
Message-ID: <95aa390a-3a32-c78f-a112-8200ff9ac22d@treenet.co.nz>

On 08/07/18 03:43, Amish wrote:
> Hello
> 
> While evaluating new squid 4.1, I noticed (possibly) "unannounced"
> change in http_port directive.
> 
> cert and key options appear to have changed to tls-cert and tls-key. Old
> "cert" option still seem to be working though.
> 
> In documentaton, description of tls-cert is too lengthy (and bit
> confusing) while that of cert was just one line.
> 
> So I would like to have more clarity if tls-cert is direct replacement
> of cert OR I need to consider some other things too?
> 

It has extensions for GnuTLS and to detect common misconfigurations
people were doing with the cert= option.

If you had a working config with cert= then tls-cert= should also work
without changes in OpenSSL builds (and old cert= should also still work).


Amos


From eliezer at ngtech.co.il  Sun Jul  8 21:55:34 2018
From: eliezer at ngtech.co.il (Eliezer Croitoru)
Date: Mon, 09 Jul 2018 00:55:34 +0300
Subject: [squid-users] Squid 4.1 for CentOS rpms
In-Reply-To: <53d85ea2-5cc7-944c-206b-df68ee84579f@treenet.co.nz>
References: <2197768425D7F5479A0FFB3FEC212F7FF63D99C3@aesmail.surcouf.local>
 <c3df1de412e5a91301700941545fccfd@ngtech.co.il>
 <53d85ea2-5cc7-944c-206b-df68ee84579f@treenet.co.nz>
Message-ID: <79a8f8d8ac2d8da8e8a0949906e81643@ngtech.co.il>

OK so I released the next Squid-Cache 4.1 RPMS:
CentOS 7 (x86_64)
Oracle Linux 7 (x86_64)
Amazon Linux 1 (x86_64)
Amazon Linux 2 LTS (x86_64)
SLES 12 SP1 (x86_64)

It's still in the beta repos ie:
http://ngtech.co.il/repo/centos/7/beta/
http://ngtech.co.il/repo/oracle/7/beta/
http://ngtech.co.il/repo/amzn/1/beta/
http://ngtech.co.il/repo/amzn/2/beta/
http://ngtech.co.il/repo/sles/12sp1/beta/


Since I am still testing it but until not 4.0.25 was quite a success so 
I believe that 4.1 will also be the same.

I will not be available for the next few days due to couple personal 
errands but I will review my emails in couple days to a week.


All The Bests,
Eliezer


On 2018-07-05 08:03, Amos Jeffries wrote:
> On 05/07/18 09:04, Eliezer Croitoru wrote:
>> 
>> If i'm not wrong there have been changes with some versions of OpenSSL
>> at 4.X and I do not remember what it was.
> 
> Basically Squid-4 supports OpenSSL v1.1. Squid-3 does not.
> 
> The other TLS/SSL changes in v4 are Squid features and behaviour not
> specially OpenSSL itself.
> 
> Amos
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users

-- 
----
Eliezer Croitoru
Linux System Administrator
Mobile: +972-5-28704261
Email: eliezer at ngtech.co.il


From eliezer at ngtech.co.il  Mon Jul  9 07:13:57 2018
From: eliezer at ngtech.co.il (Eliezer Croitoru)
Date: Mon, 9 Jul 2018 10:13:57 +0300
Subject: [squid-users] Squid 4.1 for CentOS rpms
In-Reply-To: <79a8f8d8ac2d8da8e8a0949906e81643@ngtech.co.il>
References: <2197768425D7F5479A0FFB3FEC212F7FF63D99C3@aesmail.surcouf.local>
 <c3df1de412e5a91301700941545fccfd@ngtech.co.il>
 <53d85ea2-5cc7-944c-206b-df68ee84579f@treenet.co.nz>
 <79a8f8d8ac2d8da8e8a0949906e81643@ngtech.co.il>
Message-ID: <032b01d41754$67ffdc20$37ff9460$@ngtech.co.il>

I don't know why but sometimes my email application replaces words so... a fix:
...Since I am still testing it but until **now** 4.0.25 was quite a success so I believe that 4.1 will also be the same.

----
Eliezer Croitoru
Linux System Administrator
Mobile: +972-5-28704261
Email: eliezer at ngtech.co.il


-----Original Message-----
From: squid-users [mailto:squid-users-bounces at lists.squid-cache.org] On Behalf Of Eliezer Croitoru
Sent: Monday, July 9, 2018 12:56 AM
To: Amos Jeffries <squid3 at treenet.co.nz>; mikes at surcouf.co.uk
Cc: squid-users at lists.squid-cache.org
Subject: Re: [squid-users] Squid 4.1 for CentOS rpms

OK so I released the next Squid-Cache 4.1 RPMS:
CentOS 7 (x86_64)
Oracle Linux 7 (x86_64)
Amazon Linux 1 (x86_64)
Amazon Linux 2 LTS (x86_64)
SLES 12 SP1 (x86_64)

It's still in the beta repos ie:
http://ngtech.co.il/repo/centos/7/beta/
http://ngtech.co.il/repo/oracle/7/beta/
http://ngtech.co.il/repo/amzn/1/beta/
http://ngtech.co.il/repo/amzn/2/beta/
http://ngtech.co.il/repo/sles/12sp1/beta/


Since I am still testing it but until not 4.0.25 was quite a success so 
I believe that 4.1 will also be the same.

I will not be available for the next few days due to couple personal 
errands but I will review my emails in couple days to a week.


All The Bests,
Eliezer


On 2018-07-05 08:03, Amos Jeffries wrote:
> On 05/07/18 09:04, Eliezer Croitoru wrote:
>> 
>> If i'm not wrong there have been changes with some versions of OpenSSL
>> at 4.X and I do not remember what it was.
> 
> Basically Squid-4 supports OpenSSL v1.1. Squid-3 does not.
> 
> The other TLS/SSL changes in v4 are Squid features and behaviour not
> specially OpenSSL itself.
> 
> Amos
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users

-- 
----
Eliezer Croitoru
Linux System Administrator
Mobile: +972-5-28704261
Email: eliezer at ngtech.co.il
_______________________________________________
squid-users mailing list
squid-users at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-users



From chip_pop at hotmail.com  Mon Jul  9 14:09:12 2018
From: chip_pop at hotmail.com (joseph)
Date: Mon, 9 Jul 2018 07:09:12 -0700 (MST)
Subject: [squid-users] new store id helper design
Message-ID: <1531145352627-0.post@n4.nabble.com>

hi if any one interested  for high-performance multi-threaded Store ID helper
with command-line options
visit https://github.com/yvoinov/store-id-helper



-----
************************** 
***** Crash to the future  ****
**************************
--
Sent from: http://squid-web-proxy-cache.1019090.n4.nabble.com/Squid-Users-f1019091.html


From azufre.cmw at infomed.sld.cu  Mon Jul  9 16:01:39 2018
From: azufre.cmw at infomed.sld.cu (=?utf-8?B?TVNjLiBFcmljIEVucmlxdWUgU2VkZcOxbyBFc3RyYWRh?=)
Date: Mon, 9 Jul 2018 13:01:39 -0300
Subject: [squid-users] Connect Android apps behind Squid with authentication
Message-ID: <201807091301395284578@infomed.sld.cu>

Regards
Sorry to write, this is not my mother tongue.
I have Squid 3.5.23 and I can not access from Android to Facebook, Twitter, Play Store with applications, if I can browse from the browser.
Previously I had Kerio 9.0.1 with authentication and if I could access those applications from Android.
There will be some solution.



MSc. Eric Enrique Sede?o Estrada
Administrador de Sistemas Inform?ticos
Hospital Provincial Docente de Oncolog?a "Mar?a Curie"
Si deseas que tus sue?os se hagan realidad: ?Despierta!
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20180709/1b84fd19/attachment.htm>

From arsalan at preston.edu.pk  Mon Jul  9 17:13:30 2018
From: arsalan at preston.edu.pk (Arsalan Hussain)
Date: Mon, 9 Jul 2018 22:13:30 +0500
Subject: [squid-users] Connect Android apps behind Squid with
	authentication
In-Reply-To: <201807091301395284578@infomed.sld.cu>
References: <201807091301395284578@infomed.sld.cu>
Message-ID: <CAMwDxM3Q_6dSPw7LkjaUAbTArVY5kMQch_G5xaemk5qD9xbAvQ@mail.gmail.com>

 Eric Enrique,

The same problem i faced with Android Apps. Reason behind issue.

*Issue:* The proxy address & port of squid you defined in wireless setting
only applies to browsers not to Apps installed on smartphone.

*Solution: *you need to configure your squid as Transparent Proxy so that
all applications gets an access of internet without defined proxy setting
of Squid server.

In this way all apps will get an access to internet without setup proxy.


On Mon, Jul 9, 2018 at 9:01 PM, MSc. Eric Enrique Sede?o Estrada <
azufre.cmw at infomed.sld.cu> wrote:

> Regards
> Sorry to write, this is not my mother tongue.
> I have Squid 3.5.23 and I can not access from Android to Facebook,
> Twitter, Play Store with applications, if I can browse from the browser.
> Previously I had Kerio 9.0.1 with authentication and if I could access
> those applications from Android.
> There will be some solution.
>
> ------------------------------
>
> MSc. Eric Enrique Sede?o Estrada
>
> Administrador de Sistemas Inform?ticos
>
> Hospital Provincial Docente de Oncolog?a "Mar?a Curie"
>
> *Si deseas que tus sue?os se hagan realidad: ?Despierta!*
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>
>


-- 
With Regards,


*Arsalan Hussain*
*Assistant Director, Networks & Information System*

*PRESTON UNIVERSITY*
Add: Plot: 85, Street No: 3, Sector H-8/1, Islamabad, Pakistan
Cell: +92-322-5018611
UAN: (51) 111-707-808 (Ext: 443)

*Some things take time, nothing you can do otherwise*
*.*
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20180709/d9e872b6/attachment.htm>

From eliezer at ngtech.co.il  Mon Jul  9 21:37:57 2018
From: eliezer at ngtech.co.il (Eliezer Croitoru)
Date: Tue, 10 Jul 2018 00:37:57 +0300
Subject: [squid-users] iptables setup for tcp_outgoing_address
In-Reply-To: <16406cee145-c8b-9279@webjas-vad174.srv.aolmail.net>
References: <0ee9e237-9d93-f32e-f931-db1c4c9c4bec@measurement-factory.com>
 <16406cee145-c8b-9279@webjas-vad174.srv.aolmail.net>
Message-ID: <021c01d417cd$1a39b0f0$4ead12d0$@ngtech.co.il>

Just to make sure things are understood.

There is one big difference between windows and Linux handling connections and traffic.

Linux can accept traffic on a specific interface but route the outgoing packet via another interface.

It?s a feature of the Linux Routing and Networking Kernel stack.

Sometimes it can bite the admin/user and while on windows the connection(TCP) will always be routed or
put into the right cable in Linux you need a little be connection marking, mangling and routing marking to make sure that
the traffic will be passed to the right gateway.

 

It?s a bit hard to understand what happens currently on your system.

 

All The Bests,

Eliezer

 

----

 <http://ngtech.co.il/lmgtfy/> Eliezer Croitoru
Linux System Administrator
Mobile: +972-5-28704261
Email: eliezer at ngtech.co.il



 

From: squid-users [mailto:squid-users-bounces at lists.squid-cache.org] On Behalf Of davidjesse091 at aol.com
Sent: Saturday, June 16, 2018 7:16 AM
To: rousskov at measurement-factory.com; squid-users at lists.squid-cache.org
Subject: Re: [squid-users] iptables setup for tcp_outgoing_address

 

I tried curl --interface 172.16.11.107  <http://www.example.com/> http://www.example.com yesterday and it worked fine, but now it looks like it does not work. Just hangs forever. So there is an issue there for sure. I will try to find out why it's not working.



-----Original Message-----
From: Alex Rousskov <rousskov at measurement-factory.com <mailto:rousskov at measurement-factory.com> >
To: davidjesse091 <davidjesse091 at aol.com <mailto:davidjesse091 at aol.com> >; squid-users <squid-users at lists.squid-cache.org <mailto:squid-users at lists.squid-cache.org> >
Sent: Fri, Jun 15, 2018 11:43 pm
Subject: Re: [squid-users] iptables setup for tcp_outgoing_address

On 06/15/2018 05:12 PM, davidjesse091 at aol.com <mailto:davidjesse091 at aol.com>  wrote:

> if I use another interface's IP address
> for tcp_outgoing_address on my Linux machine then web pages don't load.

Does using "another interface" IP address work with curl or wget
executed on the Squid Linux box?

curl --interface 172.16.11.107 http://www.example.com
wget --bind-address=172.16.11.107 http://www.example.com


Alex.


> -----Original Message-----
> From: Alex Rousskov <rousskov at measurement-factory.com <mailto:rousskov at measurement-factory.com> >
> To: davidjesse091 <davidjesse091 at aol.com <mailto:davidjesse091 at aol.com> >; squid-users
> <squid-users at lists.squid <mailto:users at lists.squid> -cache.org>
> Sent: Fri, Jun 15, 2018 7:01 pm
> Subject: Re: [squid-users] iptables setup for tcp_outgoing_address
> 
> On 06/15/2018 04:42 PM, davidjesse091 at aol.com <mailto:davidjesse091 at aol.com> 
> <mailto:davidjesse091 at aol.com <mailto:davidjesse091 at aol.com?> > wrote:
> 
>> I want to connect to Squid proxy using 192.168.1.212 and if I am
>> connecting using port 11000,
> 
> I assume you meant "connecting to port 11000" (there is also the client
> source port, but it should not matter here).
> 
> 
>> I want squid to have the traffic go out of the 172.16.11.107 IP
> 
> 
>> http_port 11000 name=port_11000
>> acl port_11000_acl myportname port_11000
>> tcp_outgoing_address 172.16.11.107 port_11000_acl
> 
> Looks good to me, provided all your outgoing traffic goes to IPv4
> addresses (no IPv6).
> 
> 
>> What would I need to do with iptables to make this work?
> 
> Why do you think you need iptables? What does not work if you do not use
> IP tables?
> 
> 
> Alex.

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20180710/2d93eb72/attachment.htm>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: image002.png
Type: image/png
Size: 11307 bytes
Desc: not available
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20180710/2d93eb72/attachment.png>

From capcoding at gmail.com  Mon Jul  9 23:45:39 2018
From: capcoding at gmail.com (Gordon Hsiao)
Date: Mon, 9 Jul 2018 18:45:39 -0500
Subject: [squid-users] minimize squid memory usage
Message-ID: <CAK0iFYz2gvW5kVDk6boEXbG1KO9Wya2=Oib8rHcHFdpYZLy2Rw@mail.gmail.com>

I noticed my Squid doubles its memory usage even though I had:

cache deny all
cache_mem 0 MB
access_log none

Assuming I need _absolutely_ no cache what-so-ever(to the point to change
compile flags to disable that, if needed), no store-to-disk neither, i.e.
no objects need to be cached at all. I just need Squid to check a few ACLs
with absolutely minimal memory usage for now, what else am I missing to get
that work?

Thanks,
Gordon
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20180709/c3667dab/attachment.htm>

From Sarfaraz.Ahmad at deshaw.com  Tue Jul 10 06:27:34 2018
From: Sarfaraz.Ahmad at deshaw.com (Ahmad, Sarfaraz)
Date: Tue, 10 Jul 2018 06:27:34 +0000
Subject: [squid-users] Allow weaker ciphers for selected sites using an ACL?
Message-ID: <b452e8d475c24a19b12227c3bf4186d2@mbxtoa3.winmail.deshaw.com>

Hi,

I have disabled weak ciphers through tls_outgoing_options . Is there a way to allow weak ciphers for selected websites, say, using an ACL and without splicing the connections?

Regards,
Sarfaraz

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20180710/a4e890aa/attachment.htm>

From squid3 at treenet.co.nz  Tue Jul 10 07:06:00 2018
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Tue, 10 Jul 2018 19:06:00 +1200
Subject: [squid-users] minimize squid memory usage
In-Reply-To: <CAK0iFYz2gvW5kVDk6boEXbG1KO9Wya2=Oib8rHcHFdpYZLy2Rw@mail.gmail.com>
References: <CAK0iFYz2gvW5kVDk6boEXbG1KO9Wya2=Oib8rHcHFdpYZLy2Rw@mail.gmail.com>
Message-ID: <b8669604-6daf-68b6-467f-a042440fc462@treenet.co.nz>

On 10/07/18 11:45, Gordon Hsiao wrote:
> I noticed my Squid doubles its memory usage even though I had:
> 

What do you mean by "double" ? Squid memory is mostly used for things
that are not caches.
 <https://wiki.squid-cache.org/SquidFaq/SquidMemory>

And be careful that you are looking at *resident* size, not virtual
memory or the areas the OS calls "cache" memory (which is not related to
Squid).


> cache deny all
> cache_mem 0 MB
> access_log none
> 
> Assuming I need _absolutely_ no cache what-so-ever(to the point to
> change compile flags to disable that, if needed), no store-to-disk
> neither, i.e. no objects need to be cached at all. I just need Squid to
> check a few ACLs with absolutely minimal memory usage for now, what else
> am I missing to get that work?


Start with a minimal build. The set of ./configure build options to
disable for small Squid binaries can be found at
<https://github.com/squid-cache/squid/blob/v4/test-suite/buildtests/layer-01-minimal.opts>

There are some other options not listed there (or listed at the top with
reasons stated) which can be disabled. However they or the replacement
code requires OS-specific support. If your OS has that, you can shrink
it a bit more.

When you have a minimal build look at what the memory if being used for
and tune further.


Amos


From squid3 at treenet.co.nz  Tue Jul 10 07:12:40 2018
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Tue, 10 Jul 2018 19:12:40 +1200
Subject: [squid-users] Allow weaker ciphers for selected sites using an
 ACL?
In-Reply-To: <b452e8d475c24a19b12227c3bf4186d2@mbxtoa3.winmail.deshaw.com>
References: <b452e8d475c24a19b12227c3bf4186d2@mbxtoa3.winmail.deshaw.com>
Message-ID: <452067ac-fc3c-80fd-9204-e5fcd93dac33@treenet.co.nz>

On 10/07/18 18:27, Ahmad, Sarfaraz wrote:
> Hi,
> 
> I have disabled weak ciphers through tls_outgoing_options . Is there a
> way to allow weak ciphers for selected websites, say, using an ACL and
> without splicing the connections?

cache_peer directive is used to customize connectivity to specific servers.
 <http://www.squid-cache.org/Doc/config/cache_peer/>


Amos


From leolistas at solutti.com.br  Tue Jul 10 14:43:12 2018
From: leolistas at solutti.com.br (Leonardo Rodrigues)
Date: Tue, 10 Jul 2018 11:43:12 -0300
Subject: [squid-users] minimize squid memory usage
In-Reply-To: <CAK0iFYz2gvW5kVDk6boEXbG1KO9Wya2=Oib8rHcHFdpYZLy2Rw@mail.gmail.com>
References: <CAK0iFYz2gvW5kVDk6boEXbG1KO9Wya2=Oib8rHcHFdpYZLy2Rw@mail.gmail.com>
Message-ID: <6951b4aa-6c8e-d386-8e80-2471ccfa48e7@solutti.com.br>

Em 09/07/18 20:45, Gordon Hsiao escreveu:
>
> Assuming I need _absolutely_ no cache what-so-ever(to the point to 
> change compile flags to disable that, if needed), no store-to-disk 
> neither, i.e. no objects need to be cached at all. I just need Squid 
> to check a few ACLs with absolutely minimal memory usage for now, what 
> else am I missing to get that work?
>
 ??? If you don't need everything that squid can offer, maybe using 
other proxy software can be a better option. There are other software, 
with less options, that for sure will have a smaller memory footprint. 
But as you just need ACL capabilities, maybe those can be enough.

 ??? Have you tried checking that ?



-- 


	Atenciosamente / Sincerily,
	Leonardo Rodrigues
	Solutti Tecnologia
	http://www.solutti.com.br

	Minha armadilha de SPAM, N?O mandem email
	gertrudes at solutti.com.br
	My SPAMTRAP, do not email it





From ppolinpanic at gmail.com  Tue Jul 10 19:50:27 2018
From: ppolinpanic at gmail.com (Paolo Marzari)
Date: Tue, 10 Jul 2018 21:50:27 +0200
Subject: [squid-users] Delay pools in squid4 not working with https
Message-ID: <54444655-7fa0-ec9a-ea6e-505697d6bb0a@gmail.com>

My home server just updated from 3.5.27, everything is working fine, but 
delay pools seems broken to me.
I capped some devices to 240kb/s and tried to download a debian ISO with 
one of them...all good, 240kb/s.
Then I tried a speed test, results = 2.2mb/s, that's the whole ADSL speed.

So I tried youtube videos, no cap at all, same problem with facebook.
Revert to 3.5.27 and delays works again with every type of traffic.

I think there's something wrong with https traffic.

Here's my delay config section:

    acl group288 src 192.168.0.87/32 192.168.0.88/32 192.168.0.84/32
    acl groupapo src 192.168.0.56/32 #192.168.0.6/32
    acl group656 src 192.168.0.61/32 192.168.0.89/32
    acl group656b src 192.168.0.95/32 #192.168.0.112/32 192.168.0.96/32
    #192.168.0.6/32
    acl group1024 src 192.168.0.92/32
    #acl limit5conn maxconn 5
    delay_pools 4
    delay_class 1 1
    delay_class 2 1
    delay_class 3 1
    delay_class 4 1
    delay_parameters 1 288000/308000
    delay_parameters 2 595000/640000
    delay_parameters 3 595200/640400
    delay_parameters 4 972000/1024000
    delay_access 1 allow group288
    delay_access 1 allow groupapo
    delay_access 2 allow group656
    delay_access 3 allow group656b
    delay_access 4 allow group1024
    delay_access 1 deny all
    delay_access 2 deny all
    delay_access 3 deny all
    delay_access 4 deny all

Am I missing something in my config?
I need your help squid's gurus...and sorry for bad englando.

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20180710/f265365b/attachment.htm>

From ppolinpanic at gmail.com  Tue Jul 10 19:51:31 2018
From: ppolinpanic at gmail.com (prazola)
Date: Tue, 10 Jul 2018 12:51:31 -0700 (MST)
Subject: [squid-users] Delay pools in squid4 not working with https
Message-ID: <1531252291998-0.post@n4.nabble.com>

My home server just updated from 3.5.27, everything is working fine, but
delay pools seems broken to me.
I capped some devices to 240kb/s and tried to download a debian ISO with one
of them...all good, 240kb/s.
Then I tried a speed test, results = 2.2mb/s, that's the whole ADSL speed.

So I tried youtube videos, no cap at all, same problem with facebook.
Revert to 3.5.27 and delays works again with every type of traffic.

I think there's something wrong with https traffic.

Here's my delay config section:

    acl group288 src 192.168.0.87/32 192.168.0.88/32 192.168.0.84/32
    acl groupapo src 192.168.0.56/32 #192.168.0.6/32
    acl group656 src 192.168.0.61/32 192.168.0.89/32
    acl group656b src 192.168.0.95/32 #192.168.0.112/32 192.168.0.96/32
#192.168.0.6/32
    acl group1024 src 192.168.0.92/32
    #acl limit5conn maxconn 5
    delay_pools 4
    delay_class 1 1
    delay_class 2 1
    delay_class 3 1
    delay_class 4 1
    delay_parameters 1 288000/308000
    delay_parameters 2 595000/640000
    delay_parameters 3 595200/640400
    delay_parameters 4 972000/1024000
    delay_access 1 allow group288
    delay_access 1 allow groupapo
    delay_access 2 allow group656
    delay_access 3 allow group656b
    delay_access 4 allow group1024
    delay_access 1 deny all
    delay_access 2 deny all
    delay_access 3 deny all
    delay_access 4 deny all

Am I missing something in my config?
I need your help squid's gurus...and sorry for bad englando.



--
Sent from: http://squid-web-proxy-cache.1019090.n4.nabble.com/Squid-Users-f1019091.html


From squid3 at treenet.co.nz  Tue Jul 10 21:34:43 2018
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Wed, 11 Jul 2018 09:34:43 +1200
Subject: [squid-users] Delay pools in squid4 not working with https
In-Reply-To: <54444655-7fa0-ec9a-ea6e-505697d6bb0a@gmail.com>
References: <54444655-7fa0-ec9a-ea6e-505697d6bb0a@gmail.com>
Message-ID: <44ca2b8a-2f8d-2e47-b902-f7589e6d5fe4@treenet.co.nz>

On 11/07/18 07:50, Paolo Marzari wrote:
> My home server just updated from 3.5.27, everything is working fine, but
> delay pools seems broken to me.
> I capped some devices to 240kb/s and tried to download a debian ISO with
> one of them...all good, 240kb/s.
> Then I tried a speed test, results = 2.2mb/s, that's the whole ADSL speed.
> 
> So I tried youtube videos, no cap at all, same problem with facebook.
> Revert to 3.5.27 and delays works again with every type of traffic.
> 
> I think there's something wrong with https traffic.
> 

a) is it actually HTTPS traffic?

b) are the bytes going through the proxy 2.2Mbps or 240kbps ?

I ask because Google/YouTube and Facebook are services using HTTP/2 with
high compression features as much as possible. So while the proxy is set
to transfer X bytes per second, when hidden inside "HTTPS" those X bytes
may show up as 90*X bytes of traffic when decompressed by a Browser.

Or the transfer may be QUIC protocol, completely bypassing the HTTP the
proxy is counting.

Amos


From ppolinpanic at gmail.com  Tue Jul 10 21:45:14 2018
From: ppolinpanic at gmail.com (prazola)
Date: Tue, 10 Jul 2018 14:45:14 -0700 (MST)
Subject: [squid-users] Delay pools in squid4 not working with https
In-Reply-To: <44ca2b8a-2f8d-2e47-b902-f7589e6d5fe4@treenet.co.nz>
References: <1531252291998-0.post@n4.nabble.com>
 <44ca2b8a-2f8d-2e47-b902-f7589e6d5fe4@treenet.co.nz>
Message-ID: <1531259114161-0.post@n4.nabble.com>

A fast check with nbwmon shows 2.2Mbps when using squid 4.1.




--
Sent from: http://squid-web-proxy-cache.1019090.n4.nabble.com/Squid-Users-f1019091.html


From rousskov at measurement-factory.com  Tue Jul 10 21:57:24 2018
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Tue, 10 Jul 2018 15:57:24 -0600
Subject: [squid-users] Delay pools in squid4 not working with https
In-Reply-To: <54444655-7fa0-ec9a-ea6e-505697d6bb0a@gmail.com>
References: <54444655-7fa0-ec9a-ea6e-505697d6bb0a@gmail.com>
Message-ID: <c5468738-3efb-d3d4-843a-14f1e977ddae@measurement-factory.com>

On 07/10/2018 01:50 PM, Paolo Marzari wrote:
> My home server just updated from 3.5.27, everything is working fine, but
> delay pools seems broken to me.

> Revert to 3.5.27 and delays works again with every type of traffic.
> 
> I think there's something wrong with https traffic.

You are probably right. A few days ago, while working on an unrelated
project, we have found a bug in delay pools support for tunneled https
traffic. That support was probably broken by v4 commit 6b2b6cf. We have
not tested v3.5, so I can only confirm that v4 and v5 are broken.

The bug will be fixed as a side effect of "peering support for SslBump"
changes that should be ready for the official review soon. If you would
like to test our unofficial branch, the code is available at
https://github.com/measurement-factory/squid/tree/SQUID-360-peering-for-SslBump


HTH,

Alex.


From mailbox.kedar at gmail.com  Wed Jul 11 05:59:17 2018
From: mailbox.kedar at gmail.com (Kedar K)
Date: Wed, 11 Jul 2018 11:29:17 +0530
Subject: [squid-users] store_id_extras to access request header
Message-ID: <CAGUJt2BemytFAk+JFhH76WYtvDSQTRw=0JOgYS8szYxfqD8v8A@mail.gmail.com>

Hi,
I tried to get the request header to store id helper
with %>h option for store_id_extras; However, I get a '-' (and the default
k-v pairs intact)

Is this expected behaviour? Wouldn't request header be available before
sending a query to store-id helper?

?My use case was to pass custom fields either as part of URL (append at the
end) or request header.

Is it possible to use combination of store_id_program helper and
rewrite_url_program; such that the extra params from the url are used by
store-id helper to create a store-id and then the url_rewrite program can
strip them off before sending the request to origin server? ?


-- 

*- Kedar*
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20180711/f715b912/attachment.htm>

From mailbox.kedar at gmail.com  Wed Jul 11 06:12:42 2018
From: mailbox.kedar at gmail.com (Kedar K)
Date: Wed, 11 Jul 2018 11:42:42 +0530
Subject: [squid-users] store_id_extras to access request header
In-Reply-To: <CAGUJt2BemytFAk+JFhH76WYtvDSQTRw=0JOgYS8szYxfqD8v8A@mail.gmail.com>
References: <CAGUJt2BemytFAk+JFhH76WYtvDSQTRw=0JOgYS8szYxfqD8v8A@mail.gmail.com>
Message-ID: <CAGUJt2CDegeLtmownqhzTLJN9Q722sybcQ=6z=cbb7VL-qToUQ@mail.gmail.com>

additional note:
I do see both request and response header in access.log though.

On Wed, Jul 11, 2018 at 11:29 AM Kedar K <mailbox.kedar at gmail.com> wrote:

> Hi,
> I tried to get the request header to store id helper
> with %>h option for store_id_extras; However, I get a '-' (and the default
> k-v pairs intact)
>
> Is this expected behaviour? Wouldn't request header be available before
> sending a query to store-id helper?
>
> ?My use case was to pass custom fields either as part of URL (append at
> the end) or request header.
>
> Is it possible to use combination of store_id_program helper and
> rewrite_url_program; such that the extra params from the url are used by
> store-id helper to create a store-id and then the url_rewrite program can
> strip them off before sending the request to origin server? ?
>
>
> --
>
> *- Kedar*
>


-- 

*- Kedar Kekan*
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20180711/f64bbf61/attachment.htm>

From mailbox.kedar at gmail.com  Wed Jul 11 09:19:14 2018
From: mailbox.kedar at gmail.com (Kedar K)
Date: Wed, 11 Jul 2018 14:49:14 +0530
Subject: [squid-users] store_id_extras to access request header
In-Reply-To: <CAGUJt2CDegeLtmownqhzTLJN9Q722sybcQ=6z=cbb7VL-qToUQ@mail.gmail.com>
References: <CAGUJt2BemytFAk+JFhH76WYtvDSQTRw=0JOgYS8szYxfqD8v8A@mail.gmail.com>
 <CAGUJt2CDegeLtmownqhzTLJN9Q722sybcQ=6z=cbb7VL-qToUQ@mail.gmail.com>
Message-ID: <CAGUJt2DRdgu=Sox+FZx-4PNxt71E7fQVFOvDS+zZQOVGdUKu8Q@mail.gmail.com>

It worked with a combination of store-id helper and url rewriter.

- Kedar

On Wed, Jul 11, 2018 at 11:42 AM Kedar K <mailbox.kedar at gmail.com> wrote:

> additional note:
> I do see both request and response header in access.log though.
>
> On Wed, Jul 11, 2018 at 11:29 AM Kedar K <mailbox.kedar at gmail.com> wrote:
>
>> Hi,
>> I tried to get the request header to store id helper
>> with %>h option for store_id_extras; However, I get a '-' (and the
>> default k-v pairs intact)
>>
>> Is this expected behaviour? Wouldn't request header be available before
>> sending a query to store-id helper?
>>
>> ?My use case was to pass custom fields either as part of URL (append at
>> the end) or request header.
>>
>> Is it possible to use combination of store_id_program helper and
>> rewrite_url_program; such that the extra params from the url are used by
>> store-id helper to create a store-id and then the url_rewrite program can
>> strip them off before sending the request to origin server? ?
>>
>>
>> --
>>
>> *- Kedar*
>>
>
>
> --
>
> *- Kedar Kekan*
>


-- 

*- Kedar Kekan*
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20180711/cd4920b8/attachment.htm>

From mailbox.kedar at gmail.com  Wed Jul 11 09:59:39 2018
From: mailbox.kedar at gmail.com (Kedar K)
Date: Wed, 11 Jul 2018 15:29:39 +0530
Subject: [squid-users] store_id_extras to access request header
In-Reply-To: <CAGUJt2DRdgu=Sox+FZx-4PNxt71E7fQVFOvDS+zZQOVGdUKu8Q@mail.gmail.com>
References: <CAGUJt2BemytFAk+JFhH76WYtvDSQTRw=0JOgYS8szYxfqD8v8A@mail.gmail.com>
 <CAGUJt2CDegeLtmownqhzTLJN9Q722sybcQ=6z=cbb7VL-qToUQ@mail.gmail.com>
 <CAGUJt2DRdgu=Sox+FZx-4PNxt71E7fQVFOvDS+zZQOVGdUKu8Q@mail.gmail.com>
Message-ID: <CAGUJt2B=TwDLkpSoTDekEtQOJ5Sz_DS3paf4Mx9dR-UyJeVF6w@mail.gmail.com>

That was a false alarm; it actually cached only the redirected url and the
key generated by store-id helper was not used.

On Wed, Jul 11, 2018 at 2:49 PM Kedar K <mailbox.kedar at gmail.com> wrote:

> It worked with a combination of store-id helper and url rewriter.
>
> - Kedar
>
> On Wed, Jul 11, 2018 at 11:42 AM Kedar K <mailbox.kedar at gmail.com> wrote:
>
>> additional note:
>> I do see both request and response header in access.log though.
>>
>> On Wed, Jul 11, 2018 at 11:29 AM Kedar K <mailbox.kedar at gmail.com> wrote:
>>
>>> Hi,
>>> I tried to get the request header to store id helper
>>> with %>h option for store_id_extras; However, I get a '-' (and the
>>> default k-v pairs intact)
>>>
>>> Is this expected behaviour? Wouldn't request header be available before
>>> sending a query to store-id helper?
>>>
>>> ?My use case was to pass custom fields either as part of URL (append at
>>> the end) or request header.
>>>
>>> Is it possible to use combination of store_id_program helper and
>>> rewrite_url_program; such that the extra params from the url are used by
>>> store-id helper to create a store-id and then the url_rewrite program can
>>> strip them off before sending the request to origin server? ?
>>>
>>>
>>> --
>>>
>>> *- Kedar*
>>>
>>
>>
>> --
>>
>> *- Kedar Kekan*
>>
>
>
> --
>
> *- Kedar Kekan*
>


-- 

*- Kedar Kekan*
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20180711/1cbcff1e/attachment.htm>

From Pete_DawgG at gmx.net  Wed Jul 11 10:39:01 2018
From: Pete_DawgG at gmx.net (pete dawgg)
Date: Wed, 11 Jul 2018 12:39:01 +0200
Subject: [squid-users] squid 3.5.27 does not respect cache_dir-size but uses
 100% of partition and fails
Message-ID: <trinity-61214a10-8a85-49ba-baf2-9141166cb932-1531305541528@3c-app-gmx-bs66>

Hello list,

i run squid 3.5.27 with some special settings for windows updates as suggested here: https://wiki.squid-cache.org/ConfigExamples/Caching/WindowsUpdates It's been running almost trouble-free for some time, but for ~2 months the cache-partition has been filling up to 100% (space; inodes were OK) and squid then failed.

the cache-dir is on a 100GB ext2-partition and configured like this:

cache_dir aufs /mnt/cache/squid 75000 16 256
cache_swap_low 60
cache_swap_high 75
minimum_object_size 0 KB
maximum_object_size 6000 MB

some special settings for the windows updates:
range_offset_limit 6000 MB
maximum_object_size 6000 MB
quick_abort_min -1
quick_abort_max -1
quick_abort_pct -1

when i restart squid with its initscript it sometimes expunges some stuff from the cache but then fails again after a short while:
before restart:
/dev/sdb2        99G     93G  863M  100% /mnt/cache
after restart:
/dev/sdb2        99G     87G  7,4G   93% /mnt/cache

there are two types of errors in cache.log:
FATAL: Ipc::Mem::Segment::open failed to shm_open(/squid-cf__metadata.shm): (2) No such file or directory
FATAL: Failed to rename log file /mnt/cache/squid/swap.state.new to /mnt/cache/squid/swap.state

What should i do to make squid work with windows updates reliably again?



From Pedro.Guedes at rvolta.com  Wed Jul 11 11:41:28 2018
From: Pedro.Guedes at rvolta.com (Pedro Guedes)
Date: Wed, 11 Jul 2018 12:41:28 +0100 (WEST)
Subject: [squid-users] Exchange OWA 2016 behind squid
Message-ID: <60762.46.189.156.147.1531309288.squirrel@mailhost.rvolta.com>

Hi

I have been reading some material on this and
trying to reverse proxying squid on a diferent ssl port
like 2020 an then connect to port 443 on the exchange.

Al the examples follow the configs on the 443 port, same
on squid and exchange.

Looks like is no possible to putsquid  listening on a diferent
port than 443 and then connecting to port 443 on
exchange.

Is this true?
By the architecture it is not possible to make exchange owa
work on a diferent port than 443.






From mikes at surcouf.co.uk  Wed Jul 11 11:50:32 2018
From: mikes at surcouf.co.uk (Mike Surcouf)
Date: Wed, 11 Jul 2018 11:50:32 +0000
Subject: [squid-users] Exchange OWA 2016 behind squid
In-Reply-To: <60762.46.189.156.147.1531309288.squirrel@mailhost.rvolta.com>
References: <60762.46.189.156.147.1531309288.squirrel@mailhost.rvolta.com>
Message-ID: <2197768425D7F5479A0FFB3FEC212F7FF63E70FA@aesmail.surcouf.local>

I am sure Amos wont mind me saying but nginx is the right tool for that scenario.
Squid is a great  forward proxy and I use it for our network but form incoming connections nginx is more flexible and designed for the job.

-----Original Message-----
From: squid-users [mailto:squid-users-bounces at lists.squid-cache.org] On Behalf Of Pedro Guedes
Sent: 11 July 2018 12:41
To: squid-users at lists.squid-cache.org
Subject: [squid-users] Exchange OWA 2016 behind squid

Hi

I have been reading some material on this and
trying to reverse proxying squid on a diferent ssl port
like 2020 an then connect to port 443 on the exchange.

Al the examples follow the configs on the 443 port, same
on squid and exchange.

Looks like is no possible to putsquid  listening on a diferent
port than 443 and then connecting to port 443 on
exchange.

Is this true?
By the architecture it is not possible to make exchange owa
work on a diferent port than 443.




_______________________________________________
squid-users mailing list
squid-users at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-users

From mailbox.kedar at gmail.com  Wed Jul 11 12:08:54 2018
From: mailbox.kedar at gmail.com (Kedar K)
Date: Wed, 11 Jul 2018 17:38:54 +0530
Subject: [squid-users] store_id_extras to access request header
In-Reply-To: <CAGUJt2B=TwDLkpSoTDekEtQOJ5Sz_DS3paf4Mx9dR-UyJeVF6w@mail.gmail.com>
References: <CAGUJt2BemytFAk+JFhH76WYtvDSQTRw=0JOgYS8szYxfqD8v8A@mail.gmail.com>
 <CAGUJt2CDegeLtmownqhzTLJN9Q722sybcQ=6z=cbb7VL-qToUQ@mail.gmail.com>
 <CAGUJt2DRdgu=Sox+FZx-4PNxt71E7fQVFOvDS+zZQOVGdUKu8Q@mail.gmail.com>
 <CAGUJt2B=TwDLkpSoTDekEtQOJ5Sz_DS3paf4Mx9dR-UyJeVF6w@mail.gmail.com>
Message-ID: <CAGUJt2CL+mtgAG-aaJ89i+mUq40WDYyWNAh7enwqXqf9Zhoh=g@mail.gmail.com>

With following config to access request header;
store_id_extras "%>h %>a/%>A %un %>rm myip=%la myport=%lp"

the store_id_extras does not seem to forward header info.

Is there something that I might be missing?

Again, I see the header in access.log; However the same is missing from the
data send to store-id helper.


On Wed, Jul 11, 2018 at 3:29 PM Kedar K <mailbox.kedar at gmail.com> wrote:

> That was a false alarm; it actually cached only the redirected url and the
> key generated by store-id helper was not used.
>
> On Wed, Jul 11, 2018 at 2:49 PM Kedar K <mailbox.kedar at gmail.com> wrote:
>
>> It worked with a combination of store-id helper and url rewriter.
>>
>> - Kedar
>>
>> On Wed, Jul 11, 2018 at 11:42 AM Kedar K <mailbox.kedar at gmail.com> wrote:
>>
>>> additional note:
>>> I do see both request and response header in access.log though.
>>>
>>> On Wed, Jul 11, 2018 at 11:29 AM Kedar K <mailbox.kedar at gmail.com>
>>> wrote:
>>>
>>>> Hi,
>>>> I tried to get the request header to store id helper
>>>> with %>h option for store_id_extras; However, I get a '-' (and the
>>>> default k-v pairs intact)
>>>>
>>>> Is this expected behaviour? Wouldn't request header be available before
>>>> sending a query to store-id helper?
>>>>
>>>> ?My use case was to pass custom fields either as part of URL (append at
>>>> the end) or request header.
>>>>
>>>> Is it possible to use combination of store_id_program helper and
>>>> rewrite_url_program; such that the extra params from the url are used by
>>>> store-id helper to create a store-id and then the url_rewrite program can
>>>> strip them off before sending the request to origin server? ?
>>>>
>>>>
>>>> --
>>>>
>>>> *- Kedar*
>>>>
>>>
>>>
>>> --
>>>
>>>
>>>
>>
>> --
>>
>>
>>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20180711/442f386c/attachment.htm>

From squid3 at treenet.co.nz  Wed Jul 11 13:20:17 2018
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 12 Jul 2018 01:20:17 +1200
Subject: [squid-users] squid 3.5.27 does not respect cache_dir-size but
 uses 100% of partition and fails
In-Reply-To: <trinity-61214a10-8a85-49ba-baf2-9141166cb932-1531305541528@3c-app-gmx-bs66>
References: <trinity-61214a10-8a85-49ba-baf2-9141166cb932-1531305541528@3c-app-gmx-bs66>
Message-ID: <f6d29409-950a-5592-97e9-3e17a76ad16b@treenet.co.nz>

On 11/07/18 22:39, pete dawgg wrote:
> Hello list,
> 
> i run squid 3.5.27 with some special settings for windows updates as suggested here: https://wiki.squid-cache.org/ConfigExamples/Caching/WindowsUpdates It's been running almost trouble-free for some time, but for ~2 months the cache-partition has been filling up to 100% (space; inodes were OK) and squid then failed.
> 

That implies that either your cache_dir size accounting is VERY badly
broken, something else is filling the disk (eg failing to rotate
swap.state journals), or disk purging is not able to keep up with the
traffic flow.


> the cache-dir is on a 100GB ext2-partition and configured like this:
> 

Hmm, a partition. What else is using the same physical disk?
 Squid puts such random I/O pattern on cache disks its best not to be
using the actual physical drive for other things in parallel - they can
slow Squid down, and conversely Squid can cause problems to other uses
by flooding the disk controller queues.


> cache_dir aufs /mnt/cache/squid 75000 16 256

These numbers do matter for ext2 more than for other FS types. You need
them to be large enough not to allocate too many inodes per directory. I
would use "64 256" here, or even "128 256" for a bigger safety margin.

(I *think* modern ext2 implementations have resolved the core issue, but
that may be wrong and ext2 is old enough to be wary.)


> cache_swap_low 60
> cache_swap_high 75
> minimum_object_size 0 KB
> maximum_object_size 6000 MB

If you bumped this for the Win8 sizes mentioned in our wiki, the Win10
major updates have bumped sizes up again past 10GB. So you may need to
increase this.


> 
> some special settings for the windows updates:
> range_offset_limit 6000 MB

Add the ACLs necessary to restrict this to WU traffic. Its really hard
on cache space**, so should not be allowed to just any traffic.


** What I mean by that is it may result in N parallel fetches of the
entire object unless collapsed forwarding feature is used.
 In regards to your situation; consider a 10GB WU object being fetched
10 times -> 10*10 GB of disk space required just to fetch. Which
over-fills your available 45GB (60% of 75000 MB [cache_swap_low/100 *
cache_dir] ). And 11 will overflow your whole disk.



> maximum_object_size 6000 MB
> quick_abort_min -1
> quick_abort_max -1
> quick_abort_pct -1
> 
> when i restart squid with its initscript it sometimes expunges some stuff from the cache but then fails again after a short while:
> before restart:
> /dev/sdb2        99G     93G  863M  100% /mnt/cache
> after restart:
> /dev/sdb2        99G     87G  7,4G   93% /mnt/cache
> 

How much of that /mnt/cache size is in /mnt/cache/squid ?

Is it one physical HDD spindle (versus a RAID drive) ?


>
> there are two types of errors in cache.log:
> FATAL: Ipc::Mem::Segment::open failed to
shm_open(/squid-cf__metadata.shm): (2) No such file or directory

The cf__metadata.shm error is quite bad - it means your collapsed
forwarding is now working well. Which implies it is not preventing the
disk overflow on parallel huge WU fetches.

Are you able to try the new Squid-4? there are some collapsed forwarding
and cache management changes that may fix or allow better diagnosis of
these particularly and maybe your disk usage problem.


> FATAL: Failed to rename log file /mnt/cache/squid/swap.state.new to
/mnt/cache/squid/swap.state

This is suspicious, how large are those swap files?

Does your proxy have correct access permissions on them and the
directories in their path - both Unix filesystem and SELinux / AppArmour
/ whatever your system uses for advanced access matter here.

Same things to check for the /dev/shm device and *.shm file access error
above. But /dev/shm should be root things rather than Squid user access.


>
> What should i do to make squid work with windows updates reliably again?

Some other things you can check;

You can try to make the cache_swap_high/low be closer together and much
larger (eg the default 90 and 95 values). Current 3.5 have fixed the bug
which made smaller values necessary on some earlier installs.


If you can afford the delays it introduces to restart, you could run a
full scan of the cached data (stop Squid, delete the swap.state* files,
then restart Squid and wait).
 - you could do that with a copy of Squid not handling user traffic if
necessary, but the running one cannot use the cache while its happening.


Otherwise, have you tried purging the entire cache and starting Squid
with a clean slate?
 that would be a lot faster for recovery than the above scan. But does
have a bit more bandwidth spent short-term while re-filling the cache.


Amos



From squid at bloms.de  Wed Jul 11 13:27:07 2018
From: squid at bloms.de (Dieter Bloms)
Date: Wed, 11 Jul 2018 15:27:07 +0200
Subject: [squid-users] squid 4.1 works great ;)
Message-ID: <20180711132704.nhw2zqrptttcgdph@bloms.de>

Hi,

I run squid4.1 for several days in production and have to say it works
pretty good.
It is stable and it downloads the missing intermediate certificates
automatically.

Great work!

Thank you very much for this version.


-- 
Regards

  Dieter

--
I do not get viruses because I do not use MS software.
If you use Outlook then please do not put my email address in your
address-book so that WHEN you get a virus it won't use my address in the
>From field.


From Niklas.Hess at webit-wetterau.de  Wed Jul 11 13:33:02 2018
From: Niklas.Hess at webit-wetterau.de (Hess, Niklas)
Date: Wed, 11 Jul 2018 13:33:02 +0000
Subject: [squid-users] Problems with peek and slice through parent proxy
Message-ID: <1D5EA300AD31C04CB0E5E4DA3F3D402F0109C14744@DAGFBGWET02.wetteraukreis.local>

Hello list,

I?m setting up a Squid proxy specifically to scan the incoming traffic from a cloud platform.
ClamAV should scan the incoming traffic.

So far so good.

The cloud uses WebDAV over HTTPS, so I have to SSL-Bump the incoming traffic via Peek and Splice Feature.
That works indeed with the CA signed internal Certificate.

But as soon as I add a cache_peer as a "parent proxy" it does not work. (This request could not be forwarded to the origin server or to any parent caches.)
I just get "FwdState.cc(813) connectStart: fwdConnectStart: Ssl bumped connections through parent proxy are not allowed" in the cache.log

And yes I know ssl-bump through a parent proxy is an security issue and might be unsecure, but the connection to the parent is internal, save and secure.
I don't know how, but could there be a way to "comment out" the section in fwdConnectStart source file?

Squid Cache: Version 3.5.27
Service Name: squid
configure options:  '--with-openssl' '--enable-ssl-crtd'


Here?s my "minimal" SSL-Bump config:

### Start config

debug_options ALL,6
shutdown_lifetime 1 seconds

http_port 8080 ssl-bump cert=/usr/local/squid/etc/ssl_cert/Squidtest.pem generate-host-certificates=on dynamic_cert_mem_cache_size=4MB

sslcrtd_program /usr/local/squid/libexec/ssl_crtd -s /var/lib/ssl_db -M 4MB
sslcrtd_children 25 startup=5 idle=10

cache_peer 10.106.3.66 parent 8080 0 no-query no-digest name=parent

never_direct allow all

sslproxy_cert_error allow all
sslproxy_flags DONT_VERIFY_PEER

ssl_bump bump all

http_access allow all


### End config

Thanks for any help!
Niklas


Azubi Niklas Hess
Team Applikation-Management

Eigenbetrieb Informationstechnologie des Wetteraukreises
61169 Friedberg
Europaplatz
Geb?ude B
Tel.: 06031 83-6526
Mobil:
Fax.: 06031 83-916526
www.wetteraukreis.de<http://www.wetteraukreis.de>

Informationen zum Datenschutz erhalten sie ?ber unsere Datenschutzseite www.datenschutz.wetterau.de<http://www.datenschutz.wetterau.de/>
Diese E-Mail enth?lt vertrauliche und/oder rechtlich gesch?tzte Informationen. Wenn Sie nicht der richtige Adressat sind, informieren Sie bitte sofort den Absender und vernichten Sie diese E-Mail. Das unerlaubte Kopieren sowie die unbefugte Weitergabe dieser E-Mail ist nicht gestattet.

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20180711/785ce589/attachment.htm>

From mailbox.kedar at gmail.com  Wed Jul 11 13:43:55 2018
From: mailbox.kedar at gmail.com (Kedar K)
Date: Wed, 11 Jul 2018 19:13:55 +0530
Subject: [squid-users] Problems with peek and slice through parent proxy
In-Reply-To: <1D5EA300AD31C04CB0E5E4DA3F3D402F0109C14744@DAGFBGWET02.wetteraukreis.local>
References: <1D5EA300AD31C04CB0E5E4DA3F3D402F0109C14744@DAGFBGWET02.wetteraukreis.local>
Message-ID: <CAGUJt2CL8AB20PCYH98+BnwadTxsx8=XYudGV5+6f5UGpk8P_Q@mail.gmail.com>

On Wed, Jul 11, 2018 at 7:03 PM Hess, Niklas <Niklas.Hess at webit-wetterau.de>
wrote:

> Hello list,
>
>
>
> I?m setting up a Squid proxy specifically to scan the incoming traffic
> from a cloud platform.
>
> ClamAV should scan the incoming traffic.
>
>
>
> So far so good.
>
>
>
> The cloud uses WebDAV over HTTPS, so I have to SSL-Bump the incoming
> traffic via Peek and Splice Feature.
>
> That works indeed with the CA signed internal Certificate.
>
>
>
> But as soon as I add a cache_peer as a ?parent proxy? it does not work.
> (This request could not be forwarded to the origin server or to any parent
> caches.)
>
> I just get ?FwdState.cc(813) connectStart: fwdConnectStart: Ssl bumped
> connections through parent proxy are not allowed? in the cache.log
>
>
>
> And yes I know ssl-bump through a parent proxy is an security issue and
> might be unsecure, but the connection to the parent is internal, save and
> secure.
>
> I don?t know how, but could there be a way to ?comment out? the section in
> fwdConnectStart source file?
>
>
>
> Squid Cache: Version 3.5.27
>
> Service Name: squid
>
> configure options:  '--with-openssl' '--enable-ssl-crtd'
>
>
>
>
>
> Here?s my ?minimal? SSL-Bump config:
>
>
>
> ### Start config
>
>
>
> debug_options ALL,6
>
> shutdown_lifetime 1 seconds
>
>
>
> http_port 8080 ssl-bump cert=/usr/local/squid/etc/ssl_cert/Squidtest.pem
> generate-host-certificates=on dynamic_cert_mem_cache_size=4MB
>
>
>
> sslcrtd_program /usr/local/squid/libexec/ssl_crtd -s /var/lib/ssl_db -M 4MB
>
> sslcrtd_children 25 startup=5 idle=10
>
>
>
> cache_peer 10.106.3.66 parent 8080 0 no-query no-digest name=parent
>
>
>
> never_direct allow all
>
>
>
> sslproxy_cert_error allow all
>
> sslproxy_flags DONT_VERIFY_PEER
>
>
>
> ssl_bump bump all
>
?Did you forget to copy at_step acls?

acl step1 at_step SslBump1
acl step2 at_step SslBump2
acl step3 at_step SslBump3
?


>
>
> http_access allow all
>
>
>
>
>
> ### End config
>
>
>
> Thanks for any help!
>
> Niklas
>
>
>
> Azubi Niklas Hess
> *Team Applikation-Management*
>
> *Eigenbetrieb Informationstechnologie des Wetteraukreises*
> 61169 Friedberg
> Europaplatz
> Geb?ude B
> Tel.: 06031 83-6526
> Mobil:
> Fax.: 06031 83-916526
> www.wetteraukreis.de
>
> Informationen zum Datenschutz erhalten sie ?ber unsere Datenschutzseite
> www.datenschutz.wetterau.de
> Diese E-Mail enth
> ?lt vertrauliche und/oder rechtlich gesch?tzte Informationen. Wenn Sie
> nicht der richtige Adressat sind, informieren Sie bitte sofort den Absender
> und vernichten Sie diese E-Mail. Das unerlaubte Kopieren sowie die
> unbefugte Weitergabe dieser E-Mail ist nicht gestattet.
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>


-- 

- Kedar
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20180711/7f8c3f9a/attachment.htm>

From Pedro.Guedes at rvolta.com  Wed Jul 11 13:39:27 2018
From: Pedro.Guedes at rvolta.com (Pedro Guedes)
Date: Wed, 11 Jul 2018 14:39:27 +0100 (WEST)
Subject: [squid-users] Exchange OWA 2016 behind squid
In-Reply-To: <2197768425D7F5479A0FFB3FEC212F7FF63E70FA@aesmail.surcouf.local>
References: <60762.46.189.156.147.1531309288.squirrel@mailhost.rvolta.com>
 <2197768425D7F5479A0FFB3FEC212F7FF63E70FA@aesmail.surcouf.local>
Message-ID: <62159.46.189.156.147.1531316367.squirrel@mailhost.rvolta.com>

Hi thanks for your answer.

But it is to vague...
I first tried putting owa exchange on port 2020
(I could do it in exchange 2003).
No way. Besides exchange 2016 only works over ssl.

So the idea is putting apache or squid listening on port
2020 to the public and then redirecting to 443 on exchange
inside the intranet. Because of certificates and a lot
of url redirection it becomes quite impossible.

The most difficult being exchange url redirection giving
port 443 to clients instead of 2020.

Probably maintaining the same port would do but
would defeat my plans to not advertise 443 to the
public.

Anyway all the faq solutions use 443 on squid and
exchange.

Apache way seems cleaner as it has directives to
change urls on transit but exchange is so complex
and undocumented that probably is, again, impossible.

Thanks anyway







On Wed, July 11, 2018 12:50, Mike Surcouf wrote:
> I am sure Amos wont mind me saying but nginx is the right tool for that scenario.
> Squid is a great  forward proxy and I use it for our network but form incoming connections
> nginx is more flexible and designed for the job.
>
> -----Original Message-----
> From: squid-users [mailto:squid-users-bounces at lists.squid-cache.org] On Behalf Of Pedro Guedes
>  Sent: 11 July 2018 12:41
> To: squid-users at lists.squid-cache.org
> Subject: [squid-users] Exchange OWA 2016 behind squid
>
>
> Hi
>
>
> I have been reading some material on this and
> trying to reverse proxying squid on a diferent ssl port like 2020 an then connect to port 443
> on the exchange.
>
> Al the examples follow the configs on the 443 port, same
> on squid and exchange.
>
> Looks like is no possible to putsquid  listening on a diferent
> port than 443 and then connecting to port 443 on exchange.
>
> Is this true?
> By the architecture it is not possible to make exchange owa
> work on a diferent port than 443.
>
>
>
>
> _______________________________________________
> squid-users mailing list squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>
>
>




From squid3 at treenet.co.nz  Wed Jul 11 14:04:32 2018
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 12 Jul 2018 02:04:32 +1200
Subject: [squid-users] Exchange OWA 2016 behind squid
In-Reply-To: <2197768425D7F5479A0FFB3FEC212F7FF63E70FA@aesmail.surcouf.local>
References: <60762.46.189.156.147.1531309288.squirrel@mailhost.rvolta.com>
 <2197768425D7F5479A0FFB3FEC212F7FF63E70FA@aesmail.surcouf.local>
Message-ID: <81ef48d7-1045-b0c5-4700-bef38c215068@treenet.co.nz>

On 11/07/18 23:50, Mike Surcouf wrote:
> I am sure Amos wont mind me saying but nginx is the right tool for that scenario.

I don't mind the saying, but I disagree. The HTTP behaviour bugs I keep
hearing about NGinX having tend to make other non-Squid proxies /
servers be better when Squid itself is not top of the list.

The only situation I recommend NGinX is when the admin in question
already has a strong preference for using it. eg, being more trouble to
learn something different to solve the problem at hand.



That aside, the trouble with OWA is that it is email / SMTP software
which grew limited HTTP capabilities, and is proprietary so nobody in
our FOSS world actually knows what is intending to do with its messages
and connections.

Since HTTP and SMTP share message syntax but require very different
behaviour decrypting the TLS is a bit risky and may break rather badly
if the wrong connection happens to terminate at an HTTP proxy. Bugs and
limitations in the OWA HTTP(S) code make for a rather tricky situation
unless you can see exactly what is going on down to the TCP/IP level
when troubleshooting.



> -----Original Message-----
> From: Pedro Guedes
> 
> Hi
> 
> I have been reading some material on this and
> trying to reverse proxying squid on a diferent ssl port
> like 2020 an then connect to port 443 on the exchange.
> 
> Al the examples follow the configs on the 443 port, same
> on squid and exchange.
> 
> Looks like is no possible to putsquid  listening on a diferent
> port than 443 and then connecting to port 443 on
> exchange.
> 
> Is this true?

No. Squid can easily do that. Just setup the http(s)_port [OWA
client->Squid] and cache_peer [Squid->Exchange/OWA server] directives
however you want. Whether it "works" in context of what OWA is doing is
the questionable part, and not related to Squid.

The problem is what the OWA server can do, what the client software can
do - and what they tell each other in their messages. All of which has
to cope perfectly with the custom port you told Squid to use. Otherwise
you just see "broken".

 * Absolutely avoid URL-rewrite. This will only break things. Use proper
HTTP redirect if you really have to, and avoid changing anything at the
proxy if you can.

 * Avoid TPROXY and NAT intercept of the traffic. It can be coped with,
but adds MANY problems that are best to avoid here.

 * Be careful of the TLS settings on the proxy. OWA has some odd and
quite Microsoft specific things that is requires, and prefers.


As you found OWA itself does not permit port changes (easily?). I'm not
sure if it has improved in recent years with the "365" software
conversions, used to be not possible at all.

HTH
Amos


From squid3 at treenet.co.nz  Wed Jul 11 15:03:17 2018
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 12 Jul 2018 03:03:17 +1200
Subject: [squid-users] Problems with peek and slice through parent proxy
In-Reply-To: <CAGUJt2CL8AB20PCYH98+BnwadTxsx8=XYudGV5+6f5UGpk8P_Q@mail.gmail.com>
References: <1D5EA300AD31C04CB0E5E4DA3F3D402F0109C14744@DAGFBGWET02.wetteraukreis.local>
 <CAGUJt2CL8AB20PCYH98+BnwadTxsx8=XYudGV5+6f5UGpk8P_Q@mail.gmail.com>
Message-ID: <88fbcfd0-e29e-445e-d6aa-a5d78e4462be@treenet.co.nz>

On 12/07/18 01:43, Kedar K wrote:
> 
> 
> On Wed, Jul 11, 2018 at 7:03 PM Hess, Niklas wrote:
> 
>     Hello list,____
> 
>     __?__
> 
>     I?m setting up a Squid proxy specifically to scan the incoming
>     traffic from a cloud platform.____
> 

Please clarify what "incoming" means to you. Is the cloud platform
generating request messages or supplying the responses?

The HTTP definition of in/out is oriented with request flow. ie from the
origin server point of view, *opposite* to what an ISP would describe it as.



>     ClamAV should scan the incoming traffic.____
> 
>     __?__
> 
>     So far so good.____
> 
>     __?__

May appear to be, but your displayed config is absolutely *not* "all
good". Details below.


> 
>     The cloud uses WebDAV over HTTPS, so I have to SSL-Bump the incoming
>     traffic via Peek and Splice Feature.____
> 

Then do so. The config you show below is not peek+splice.

It is "bump all" which is the old client-first behaviour.


>     That works indeed with the CA signed internal Certificate.____
> 

Ah, if by CA you mean one of the global "Trusted CA". Then you probably
should not be using SSl-Bump at all. That is a feature for
forward/explicit proxy to access.

Otherwise if your CA is a self-signed/generated one then of course it
"works". All SSL-Bump variants use that type of CA certificate.


>     __?__
> 
>     But as soon as I add a cache_peer as a ?parent proxy? it does not
>     work. (This request could not be forwarded to the origin server or
>     to any parent caches.)____
> 
>     I just get ?FwdState.cc(813) connectStart: fwdConnectStart: Ssl
>     bumped connections through parent proxy are not allowed? in the
>     cache.log____
> 
>     __?__
> 
>     And yes I know ssl-bump through a parent proxy is an security issue
>     and might be unsecure, but the connection to the parent is internal,
>     save and secure.____
> 

Don't count on that. You configured an open proxy. Anyone who can open a
TCP connection to it pretty much has wide-open (and anonymized) access
to all your LAN internal services.

The decision to do that, even for testing, implies a potential for holes
which does not bode well.



>     I don?t know how, but could there be a way to ?comment out? the
>     section in fwdConnectStart source file?____
> 

You cannot comment out a *lack* of something. That is the problem here.
There is no TLS on this peer's connection, so no server-cert exists for
Squid to copy/forge in what it is sending the client. More on that below.


>     __?__
> 
>     Squid Cache: Version 3.5.27____
> 
>     Service Name: squid____
> 
>     configure options:? '--with-openssl' '--enable-ssl-crtd'____
> 
>     __?__
> 
>     __?__
> 
>     Here?s my ?minimal? SSL-Bump config:____
> 
>     __?__
> 
>     ### Start config____
> 
>     __?__
> 
>     debug_options ALL,6____
> 
>     shutdown_lifetime 1 seconds____
> 
>     __?__
> 
>     http_port 8080 ssl-bump
>     cert=/usr/local/squid/etc/ssl_cert/Squidtest.pem
>     generate-host-certificates=on dynamic_cert_mem_cache_size=4MB____
> 
>     __?__
> 
>     sslcrtd_program /usr/local/squid/libexec/ssl_crtd -s /var/lib/ssl_db
>     -M 4MB____
> 
>     sslcrtd_children 25 startup=5 idle=10____
> 
>     __?__
> 
>     cache_peer 10.106.3.66 parent 8080 0 no-query no-digest name=parent____
> 

The connection to this peer is not secured. TLS (thus HTTPS) traffic is
required to remain secure on outbound connections.
 Note that this does *not* mean it has to remain HTTPS - but it cannot
be plain-text HTTP like the above peer connection. Currently the only
security protocol cache_peer supports is TLS/SSL, so the "ssl" (v2.6 -
v3.5) or "tls" options (v4.0+) is required.

Also, the outgoing server cert is what Squid bases its forged
serverHello certificate on. So there are major problems added when the
cache_peer certificate is different from the origin servers one.

The ideal way to relay SSL-Bumped traffic between proxies is currently
to let Squid re-encrypt it. Then to repeat the NAT intercept directing
Squids outbound connections into the second proxy.

IIRC, Measurement Factory have an ongoing project adding ability for
Squid to generate CONNECT messages which will make cache_peer links work
better. But even so the second proxy will still have to do its own
SSL-Bump on the crypted traffic, because we *have* to get the origin
server cert parameters through the whole chain to the client for the TLS
to work properly.


>     __?__
> 
>     never_direct allow all____
> 
>     __?__
> 
>     sslproxy_cert_error allow all____
> 
>     sslproxy_flags DONT_VERIFY_PEER____
> 

Don't do this. Ever. It actively disables all the security which is the
whole point of TLS existence. Rendering any tests which you may do to
check for "working" invalid.

The server/peer could emit random garbage bytes in a syntax layout
resembling a TLS handshake and this Squid would blithely say "okay" and
send it the clients private data. While telling the client *and you*
that everything was find and working properly. So not even useful for
troubleshooting.


>     __?__
> 
>     ssl_bump bump all
> 
> ?Did you forget to copy at_step acls?
> 
> acl step1 at_step SslBump1
> acl step2 at_step SslBump2
> acl step3 at_step SslBump3
> ?

That alone won't help here. "acl ..." lines are simply telling Squid
what to test *for*, not when to test nor any action that test should
result in.

What is needed is an actual policy on what needs to happen, in terms of
these TLS handshake stages. From that you should be able to craft
ssl_bump rules to enact that policy.

The actual minimal is:

 ssl_bump splice all

A more usual splice for troubleshooting is:

 ssl_bump peek all
 ssl_bump splice all

 - which tells Squid to look at (peek) all stages of the handshake which
can be looked at, without doing any changes, then to splice (opaque
tunnel) the crypted data portion.


The minimal to decrypt is:

 acl step1 at_step SslBump1
 ssl_bump peek step1
 ssl_bump stare all
 ssl_bump bump all

- which tells Squid to look at clientHello in the TLS handshake, and
look at the serverHello part while filtering out any features that
prohibit bump, then decrypt the crypted portion.


>     __?__
> 
>     http_access allow all____
> 

Another don't.

Please keep the default http_access lines we provide as recommended
rules. Add whatever you need at the point the default config file says
"INSERT YOUR OWN RULE(S) HERE".

Restrictions should be based on what the LAN is (for ISP proxies), or on
what domains being serviced are (for CDN / reverse-proxy).


Amos


From rousskov at measurement-factory.com  Wed Jul 11 15:07:29 2018
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Wed, 11 Jul 2018 09:07:29 -0600
Subject: [squid-users] store_id_extras to access request header
In-Reply-To: <CAGUJt2BemytFAk+JFhH76WYtvDSQTRw=0JOgYS8szYxfqD8v8A@mail.gmail.com>
References: <CAGUJt2BemytFAk+JFhH76WYtvDSQTRw=0JOgYS8szYxfqD8v8A@mail.gmail.com>
Message-ID: <bed8d055-4658-90de-e5db-f6bbac656c99@measurement-factory.com>

On 07/10/2018 11:59 PM, Kedar K wrote:

> I tried to get the request header to store id helper
> with %>h option for store_id_extras; However, I get a '-'

> store_id_extras "%>h %>a/%>A %un %>rm myip=%la myport=%lp"

> Is this expected behaviour?

No, it is not expected. Consider filing a bug report with Squid bugzilla
and, if possible, attach an ALL,9 cache.log while reproducing the
problem with a single wget or curl transaction. Please do not forget to
specify your Squid version.

If you can reproduce the problem with Squid v4 or v5, please mention
that as well.


> Wouldn't request header be available before
> sending a query to store-id helper?

Yes, request headers are available at Store ID calculation time.


> Is it possible to use combination of store_id_program helper and
> rewrite_url_program; such that the extra params from the url are used by
> store-id helper to create a store-id and then the url_rewrite program
> can strip them off before sending the request to origin server? ?

That plan would not work because the Store ID helper is consulted after
the URL rewriter:
https://wiki.squid-cache.org/SquidFaq/OrderIsImportant#Callout_Sequence


Using custom headers is a much simpler/cleaner solution IMO.


HTH,

Alex.


From rousskov at measurement-factory.com  Wed Jul 11 15:11:53 2018
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Wed, 11 Jul 2018 09:11:53 -0600
Subject: [squid-users] Problems with peek and slice through parent proxy
In-Reply-To: <88fbcfd0-e29e-445e-d6aa-a5d78e4462be@treenet.co.nz>
References: <1D5EA300AD31C04CB0E5E4DA3F3D402F0109C14744@DAGFBGWET02.wetteraukreis.local>
 <CAGUJt2CL8AB20PCYH98+BnwadTxsx8=XYudGV5+6f5UGpk8P_Q@mail.gmail.com>
 <88fbcfd0-e29e-445e-d6aa-a5d78e4462be@treenet.co.nz>
Message-ID: <dd51bfe0-35d5-0ae7-e2b3-8218ab446965@measurement-factory.com>

On 07/11/2018 09:03 AM, Amos Jeffries wrote:
> IIRC, Measurement Factory have an ongoing project adding ability for
> Squid to generate CONNECT messages which will make cache_peer links work
> better. But even so the second proxy will still have to do its own
> SSL-Bump on the crypted traffic

Correct on all counts. Our unofficial code is available for testing at
https://github.com/measurement-factory/squid/tree/SQUID-360-peering-for-SslBump

Alex.


From mailbox.kedar at gmail.com  Wed Jul 11 15:16:15 2018
From: mailbox.kedar at gmail.com (Kedar K)
Date: Wed, 11 Jul 2018 20:46:15 +0530
Subject: [squid-users] store_id_extras to access request header
In-Reply-To: <bed8d055-4658-90de-e5db-f6bbac656c99@measurement-factory.com>
References: <CAGUJt2BemytFAk+JFhH76WYtvDSQTRw=0JOgYS8szYxfqD8v8A@mail.gmail.com>
 <bed8d055-4658-90de-e5db-f6bbac656c99@measurement-factory.com>
Message-ID: <CAGUJt2CnSZugD63QfjG71HgcaXYXHV8d6W0v5hHUfJzO5RUqdg@mail.gmail.com>

On Wed, Jul 11, 2018 at 8:37 PM Alex Rousskov <
rousskov at measurement-factory.com> wrote:

> On 07/10/2018 11:59 PM, Kedar K wrote:
>
> > I tried to get the request header to store id helper
> > with %>h option for store_id_extras; However, I get a '-'
>
> > store_id_extras "%>h %>a/%>A %un %>rm myip=%la myport=%lp"
>
> > Is this expected behaviour?
>
> No, it is not expected. Consider filing a bug report with Squid bugzilla
> and, if possible, attach an ALL,9 cache.log while reproducing the
> problem with a single wget or curl transaction. Please do not forget to
> specify your Squid version.
>
> If you can reproduce the problem with Squid v4 or v5, please mention
> that as well.

?Thank you Alex; yes it seems a bug; tested with both store_id &
url_rewrite?

?extras. Either of them send blank headers. BTW I am using version 3.5.20
I will test with ALL,9 and report the bug.?

>
>

> > Wouldn't request header be available before
> > sending a query to store-id helper?
>
> Yes, request headers are available at Store ID calculation time.
>
>
> > Is it possible to use combination of store_id_program helper and
> > rewrite_url_program; such that the extra params from the url are used by
> > store-id helper to create a store-id and then the url_rewrite program
> > can strip them off before sending the request to origin server? ?
>
> That plan would not work because the Store ID helper is consulted after
> the URL rewriter:
> https://wiki.squid-cache.org/SquidFaq/OrderIsImportant#Callout_Sequence
>
> ?this makes it clear now

>
> Using custom headers is a much simpler/cleaner solution IMO.
>
> ?Agree?


>
> HTH,
>
> Alex.
>


-- 

- Kedar
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20180711/6c44e28f/attachment.htm>

From rousskov at measurement-factory.com  Wed Jul 11 16:20:08 2018
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Wed, 11 Jul 2018 10:20:08 -0600
Subject: [squid-users] squid 3.5.27 does not respect cache_dir-size but
 uses 100% of partition and fails
In-Reply-To: <trinity-61214a10-8a85-49ba-baf2-9141166cb932-1531305541528@3c-app-gmx-bs66>
References: <trinity-61214a10-8a85-49ba-baf2-9141166cb932-1531305541528@3c-app-gmx-bs66>
Message-ID: <d868705a-c4ad-5431-8725-b5e8d815d70e@measurement-factory.com>

On 07/11/2018 04:39 AM, pete dawgg wrote:

> cache_dir aufs /mnt/cache/squid 75000 16 256

> FATAL: Ipc::Mem::Segment::open failed to shm_open(/squid-cf__metadata.shm): (2) No such file or directory

If you are using a combination of an SMP-unaware disk cache (AUFS) with
SMP features such as multiple workers or a shared memory cache, please
note that this combination is not supported.

The FATAL message above is about a shared memory segment used for
collapsed forwarding. IIRC, Squid v3 attempted to create those segments
even if they were not needed, so I cannot tell for sure whether you are
using an unsupported combination of SMP/non-SMP features.

I can tell you that you cannot use a combination of collapsed
forwarding, AUFS cache_dir, and multiple workers. Also, non-SMP
collapsed forwarding was primarily tested with UFS cache_dirs.


Unfortunately, I cannot answer your question regarding overflowing AUFS
cache directories. One possibility is that Squid is not cleaning up old
cache files fast enough. You already set cache_swap_low/cache_swap_high
aggressively. Does Squid actively remove objects from the full disk
cache when you start it up _without_ any traffic? If not, it could be a
Squid bug. Unfortunately, nobody has worked on AUFS code for years
(AFAIK) so it may be difficult to fix anything that might be broken there.


Cheers,

Alex.


From vh1988 at yahoo.com.ar  Wed Jul 11 23:39:14 2018
From: vh1988 at yahoo.com.ar (Julian Perconti)
Date: Wed, 11 Jul 2018 23:39:14 +0000 (UTC)
Subject: [squid-users] Delay pools in squid4 not working with https
In-Reply-To: <c5468738-3efb-d3d4-843a-14f1e977ddae@measurement-factory.com>
References: <54444655-7fa0-ec9a-ea6e-505697d6bb0a@gmail.com>
 <c5468738-3efb-d3d4-843a-14f1e977ddae@measurement-factory.com>
Message-ID: <1286952347.2556210.1531352354125@mail.yahoo.com>

>> 
>> El ?martes?, ?10? de ?julio? de ?2018? ?18?:?57?:?43? ?-03, Alex Rousskov <rousskov at measurement-factory.com> escribi?: 
>> 
>> 
>> On 07/10/2018 01:50 PM, Paolo Marzari wrote:
>>> My home server just updated from 3.5.27, everything is working fine, but
>>> delay pools seems broken to me.
>> 
>>> Revert to 3.5.27 and delays works again with every type of traffic.
>>> 
>>> I think there's something wrong with https traffic.
>> 
>> You are probably right. A few days ago, while working on an unrelated
>> project, we have found a bug in delay pools support for tunneled https
>> traffic. That support was probably broken by v4 commit 6b2b6cf. We have
>> not tested v3.5, so I can only confirm that v4 and v5 are broken.
>> 
>> The bug will be fixed as a side effect of "peering support for SslBump"
>> changes that should be ready for the official review soon. If you would
>> like to test our unofficial branch, the code is available at
>> https://github.com/measurement-factory/squid/tree/SQUID-360-peering-for-SslBump
>> 
>> 
>> HTH,
>> 
>> Alex.
>> 
>> _______________________________________________
>> squid-users mailing list
>> squid-users at lists.squid-cache.org
>> http://lists.squid-cache.org/listinfo/squid-users

I can confirm that delay_pools works fine both http and https protocols in squid 4 running debian 9 

Squid Cache: Version 4.1 
Service Name: squid 
?
Here the cfg: 
?
delay_pools 1 
delay_class 1 2 

delay_access 1 allow all 
?
delay_parameters 1 -1/-1 100000/104857600 # ~100KBs/~100MB 
delay_initial_bucket_level 50

Regards


From Niklas.Hess at webit-wetterau.de  Thu Jul 12 06:17:50 2018
From: Niklas.Hess at webit-wetterau.de (Hess, Niklas)
Date: Thu, 12 Jul 2018 06:17:50 +0000
Subject: [squid-users] Problems with peek and slice through parent proxy
Message-ID: <1D5EA300AD31C04CB0E5E4DA3F3D402F0109C14820@DAGFBGWET02.wetteraukreis.local>

Hello again,

Thanks for any help.

It?s an forward Proxy only and my users plan to connect to a cloud in the internet.
The parent proxy, that I have to deal with, is not administrated by me or any of my colleges. (It?s the ISP proxy)
I can't make any changes to the parent.

The plan is, that the proxy would decrypt the SSL traffic from the Webserver, scan it through ClamAV and then send it back to the client, encrypted with the Certificate from our internal CA.


<Client> - - - SSL (int) - - - <Proxy> - - - SSL (ext) - - - <Parent> - - - SSL (ext) - - - <Webserver>

Int = internal certificate
Ext = external (original) certificate

And I know my config is a security mess but I will correct that.
Even with your "minimal" config I get: " assertion failed: PeerConnector.cc:116: "peer->use_ssl" in the cache.log
And I probably have to add the option "ssl" to the parent in my config, but this isn?t too because the parent wouldn?t understand it.

Is there a way of doing that, without touching the parent proxy?

Thanks again for any help!
Niklas


On 12/07/18 01:43, Kedar K wrote:
>
>
> On Wed, Jul 11, 2018 at 7:03 PM Hess, Niklas wrote:
>
>     Hello list,____
>
>     __ __
>
>     I?m setting up a Squid proxy specifically to scan the incoming
>     traffic from a cloud platform.____
>

Please clarify what "incoming" means to you. Is the cloud platform generating request messages or supplying the responses?

The HTTP definition of in/out is oriented with request flow. ie from the origin server point of view, *opposite* to what an ISP would describe it as.



>     ClamAV should scan the incoming traffic.____
>
>     __ __
>
>     So far so good.____
>
>     __ __

May appear to be, but your displayed config is absolutely *not* "all good". Details below.


>
>     The cloud uses WebDAV over HTTPS, so I have to SSL-Bump the incoming
>     traffic via Peek and Splice Feature.____
>

Then do so. The config you show below is not peek+splice.

It is "bump all" which is the old client-first behaviour.


>     That works indeed with the CA signed internal Certificate.____
>

Ah, if by CA you mean one of the global "Trusted CA". Then you probably should not be using SSl-Bump at all. That is a feature for forward/explicit proxy to access.

Otherwise if your CA is a self-signed/generated one then of course it "works". All SSL-Bump variants use that type of CA certificate.


>     __ __
>
>     But as soon as I add a cache_peer as a ?parent proxy? it does not
>     work. (This request could not be forwarded to the origin server or
>     to any parent caches.)____
>
>     I just get ?FwdState.cc(813) connectStart: fwdConnectStart: Ssl
>     bumped connections through parent proxy are not allowed? in the
>     cache.log____
>
>     __ __
>
>     And yes I know ssl-bump through a parent proxy is an security issue
>     and might be unsecure, but the connection to the parent is internal,
>     save and secure.____
>

Don't count on that. You configured an open proxy. Anyone who can open a TCP connection to it pretty much has wide-open (and anonymized) access to all your LAN internal services.

The decision to do that, even for testing, implies a potential for holes which does not bode well.



>     I don?t know how, but could there be a way to ?comment out? the
>     section in fwdConnectStart source file?____
>

You cannot comment out a *lack* of something. That is the problem here.
There is no TLS on this peer's connection, so no server-cert exists for Squid to copy/forge in what it is sending the client. More on that below.


>     __ __
>
>     Squid Cache: Version 3.5.27____
>
>     Service Name: squid____
>
>     configure options:  '--with-openssl' '--enable-ssl-crtd'____
>
>     __ __
>
>     __ __
>
>     Here?s my ?minimal? SSL-Bump config:____
>
>     __ __
>
>     ### Start config____
>
>     __ __
>
>     debug_options ALL,6____
>
>     shutdown_lifetime 1 seconds____
>
>     __ __
>
>     http_port 8080 ssl-bump
>     cert=/usr/local/squid/etc/ssl_cert/Squidtest.pem
>     generate-host-certificates=on dynamic_cert_mem_cache_size=4MB____
>
>     __ __
>
>     sslcrtd_program /usr/local/squid/libexec/ssl_crtd -s /var/lib/ssl_db
>     -M 4MB____
>
>     sslcrtd_children 25 startup=5 idle=10____
>
>     __ __
>
>     cache_peer 10.106.3.66 parent 8080 0 no-query no-digest
> name=parent____
>

The connection to this peer is not secured. TLS (thus HTTPS) traffic is required to remain secure on outbound connections.
 Note that this does *not* mean it has to remain HTTPS - but it cannot be plain-text HTTP like the above peer connection. Currently the only security protocol cache_peer supports is TLS/SSL, so the "ssl" (v2.6 -
v3.5) or "tls" options (v4.0+) is required.

Also, the outgoing server cert is what Squid bases its forged serverHello certificate on. So there are major problems added when the cache_peer certificate is different from the origin servers one.

The ideal way to relay SSL-Bumped traffic between proxies is currently to let Squid re-encrypt it. Then to repeat the NAT intercept directing Squids outbound connections into the second proxy.

IIRC, Measurement Factory have an ongoing project adding ability for Squid to generate CONNECT messages which will make cache_peer links work better. But even so the second proxy will still have to do its own SSL-Bump on the crypted traffic, because we *have* to get the origin server cert parameters through the whole chain to the client for the TLS to work properly.


>     __ __
>
>     never_direct allow all____
>
>     __ __
>
>     sslproxy_cert_error allow all____
>
>     sslproxy_flags DONT_VERIFY_PEER____
>

Don't do this. Ever. It actively disables all the security which is the whole point of TLS existence. Rendering any tests which you may do to check for "working" invalid.

The server/peer could emit random garbage bytes in a syntax layout resembling a TLS handshake and this Squid would blithely say "okay" and send it the clients private data. While telling the client *and you* that everything was find and working properly. So not even useful for troubleshooting.


>     __ __
>
>     ssl_bump bump all
>
> ?Did you forget to copy at_step acls?
>
> acl step1 at_step SslBump1
> acl step2 at_step SslBump2
> acl step3 at_step SslBump3
> ?

That alone won't help here. "acl ..." lines are simply telling Squid what to test *for*, not when to test nor any action that test should result in.

What is needed is an actual policy on what needs to happen, in terms of these TLS handshake stages. From that you should be able to craft ssl_bump rules to enact that policy.

The actual minimal is:

 ssl_bump splice all

A more usual splice for troubleshooting is:

 ssl_bump peek all
 ssl_bump splice all

 - which tells Squid to look at (peek) all stages of the handshake which can be looked at, without doing any changes, then to splice (opaque
tunnel) the crypted data portion.


The minimal to decrypt is:

 acl step1 at_step SslBump1
 ssl_bump peek step1
 ssl_bump stare all
 ssl_bump bump all

- which tells Squid to look at clientHello in the TLS handshake, and look at the serverHello part while filtering out any features that prohibit bump, then decrypt the crypted portion.


>     __ __
>
>     http_access allow all____
>

Another don't.

Please keep the default http_access lines we provide as recommended rules. Add whatever you need at the point the default config file says "INSERT YOUR OWN RULE(S) HERE".

Restrictions should be based on what the LAN is (for ISP proxies), or on what domains being serviced are (for CDN / reverse-proxy).


Amos
Azubi Niklas Hess
Team Applikation-Management

Eigenbetrieb Informationstechnologie des Wetteraukreises
61169 Friedberg
Europaplatz
Geb?ude B
Tel.: 06031 83-6526
Mobil:
Fax.: 06031 83-916526
www.wetteraukreis.de



Informationen zum Datenschutz erhalten sie ?ber unsere Datenschutzseite www.datenschutz.wetterau.de
Dies ist eine vertrauliche Nachricht und nur f?r den Adressaten bestimmt. Es ist nicht erlaubt, diese Nachricht zu kopieren oder Dritten zug?nglich zu machen. Sollten Sie irrt?mlich diese Nachricht erhalten haben, bitte ich um Ihre Mitteilung per E-Mail oder unter der oben angegebenen Telefonnummer.

_______________________________________________
squid-users mailing list
squid-users at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-users


From loginmogin at gmail.com  Thu Jul 12 11:11:04 2018
From: loginmogin at gmail.com (login mogin)
Date: Thu, 12 Jul 2018 04:11:04 -0700
Subject: [squid-users] ERROR: Unknown TLS option clientca
Message-ID: <CAJbsrUkfukNyFsP3wBKu_1Sk+0SYaGa8H1JwKk41o=A197ymoA@mail.gmail.com>

Hi,

We have been using squid 3.5.23 on ubuntu 16 with the configuration
clientca=CERTPATH without any problem. We decided to run the new version
squid 4.1 on ubuntu 18 with the same config. But now client certificate
auth is not working anymore and we got this message on debug:

ERROR: Unknown TLS option 'clientca=/etc/squid/cert/ca/ca.crt'
...

Are we missing something or http://www.squid-cache.org/Doc/config/http_port/
clientca option is broken?

By the way we also tried tls-cafile and capath options, we didn't get any
error messages with these options but still squid server is not requesting
any client certificate.

Appreciate the help.

Regards,
Logan
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20180712/2cca3ccc/attachment.htm>

From Pete_DawgG at gmx.net  Thu Jul 12 11:53:23 2018
From: Pete_DawgG at gmx.net (pete dawgg)
Date: Thu, 12 Jul 2018 13:53:23 +0200
Subject: [squid-users] squid 3.5.27 does not respect cache_dir-size but
 uses 100% of partition and fails
In-Reply-To: <d868705a-c4ad-5431-8725-b5e8d815d70e@measurement-factory.com>
References: <trinity-61214a10-8a85-49ba-baf2-9141166cb932-1531305541528@3c-app-gmx-bs66>
 <d868705a-c4ad-5431-8725-b5e8d815d70e@measurement-factory.com>
Message-ID: <trinity-d7a1212c-5e48-43d4-b260-19149844b2b8-1531396403787@3c-app-gmx-bs16>

THX for your reply!

> Betreff: Re: [squid-users] squid 3.5.27 does not respect cache_dir-size but uses 100% of partition and fails
>
> On 07/11/2018 04:39 AM, pete dawgg wrote:
> 
> > cache_dir aufs /mnt/cache/squid 75000 16 256
> 
> > FATAL: Ipc::Mem::Segment::open failed to shm_open(/squid-cf__metadata.shm): (2) No such file or directory
> 
> If you are using a combination of an SMP-unaware disk cache (AUFS) with
> SMP features such as multiple workers or a shared memory cache, please
> note that this combination is not supported.
I have set workers 8 just recently; but the disk full error had definitely been occuring before.

> The FATAL message above is about a shared memory segment used for
> collapsed forwarding. IIRC, Squid v3 attempted to create those segments
> even if they were not needed, so I cannot tell for sure whether you are
> using an unsupported combination of SMP/non-SMP features.
> 
> I can tell you that you cannot use a combination of collapsed
> forwarding, AUFS cache_dir, and multiple workers. Also, non-SMP
> collapsed forwarding was primarily tested with UFS cache_dirs.
I was not aware of that - i can de-activate the workers 8 setting again.
"Collapsed forwarding" was not set intentionally. This error seems to occur when the disk is
really full and squid is restarted.

> 
> Unfortunately, I cannot answer your question regarding overflowing AUFS
> cache directories. One possibility is that Squid is not cleaning up old
> cache files fast enough. You already set cache_swap_low/cache_swap_high
> aggressively. Does Squid actively remove objects from the full disk
> cache when you start it up _without_ any traffic? If not, it could be a
> Squid bug. Unfortunately, nobody has worked on AUFS code for years
> (AFAIK) so it may be difficult to fix anything that might be broken there.
When there is no traffic squid seems to cleaning up well enough: over night (no traffic)
disk usage went down to 30GB (now it's at 50GB again)

There was another error i just fixed:
> FATAL: Failed to open swap log /mnt/cache/squid/swap.state.new
Not a permissions or diskspace problem, caused by workers 8.
I have deactivated workers 8 and this error went away.

THX for your input!
pete


From capcoding at gmail.com  Thu Jul 12 12:29:04 2018
From: capcoding at gmail.com (Gordon Hsiao)
Date: Thu, 12 Jul 2018 07:29:04 -0500
Subject: [squid-users] minimize squid memory usage
Message-ID: <CAK0iFYw0_h67W7--qcwaAMBMxa9N-nJxCARjq1-iy26ba+z_vg@mail.gmail.com>

Unfortunately, none of those alternatives can deal with https(peek or bump)
as far as I can tell, and these days https is a must for proxy.

Gordon

>From: Leonardo Rodrigues <leolistas at solutti.com.br>
>To: squid-users at lists.squid-cache.org
>Subject: Re: [squid-users] minimize squid memory usage
>Message-ID: <6951b4aa-6c8e-d386-8e80-2471ccfa48e7 at solutti.com.br>
>Content-Type: text/plain; charset=utf-8; format=flowed
>
>Em 09/07/18 20:45, Gordon Hsiao escreveu:
>>
>> Assuming I need _absolutely_ no cache what-so-ever(to the point to
>> change compile flags to disable that, if needed), no store-to-disk
>> neither, i.e. no objects need to be cached at all. I just need Squid
>> to check a few ACLs with absolutely minimal memory usage for now, what
>> else am I missing to get that work?
>>
>     If you don't need everything that squid can offer, maybe using
>other proxy software can be a better option. There are other software,
>with less options, that for sure will have a smaller memory footprint.
>But as you just need ACL capabilities, maybe those can be enough.
>
>     Have you tried checking that ?
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20180712/1fead859/attachment.htm>

From rousskov at measurement-factory.com  Thu Jul 12 16:16:48 2018
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Thu, 12 Jul 2018 10:16:48 -0600
Subject: [squid-users] squid 3.5.27 does not respect cache_dir-size but
 uses 100% of partition and fails
In-Reply-To: <trinity-d7a1212c-5e48-43d4-b260-19149844b2b8-1531396403787@3c-app-gmx-bs16>
References: <trinity-61214a10-8a85-49ba-baf2-9141166cb932-1531305541528@3c-app-gmx-bs66>
 <d868705a-c4ad-5431-8725-b5e8d815d70e@measurement-factory.com>
 <trinity-d7a1212c-5e48-43d4-b260-19149844b2b8-1531396403787@3c-app-gmx-bs16>
Message-ID: <c210bc1d-210e-e10a-2b50-2513fe5658f0@measurement-factory.com>

On 07/12/2018 05:53 AM, pete dawgg wrote:


> I have set workers 8 just recently; but the disk full error had
> definitely been occuring before.

AUFS cache_dirs are not compatible with SMP Squid. Removing workers was
the right thing to do even if that incompatibility was not causing disk
overflows.


> FATAL: Ipc::Mem::Segment::open failed to shm_open(/squid-cf__metadata.shm): (2) No such file or directory

> This error seems to occur when the disk is really full and squid is restarted.

Ah, then it could be a side effect of poor PID management (and
associated shared resource locking) in Squid v3. You can probably ignore
this error until you fix the restarts. FWIW, Squid v4 addressed those
shortcomings.


> When there is no traffic squid seems to cleaning up well enough: over
> night (no traffic) disk usage went down to 30GB (now it's at 50GB
> again)

This may be a sign that your Squid cannot keep up with the load. IIRC,
AUFS uses lazy garbage collection so it is possible for the stream of
new objects to outpace the stream of object deletion events, resulting
in a gradually increasing cache size. Using even more aggressive
cache_swap_high might help, but there is no good configuration solution
to this UFS problem AFAIK.


> There was another error i just fixed:
>> FATAL: Failed to open swap log /mnt/cache/squid/swap.state.new
> Not a permissions or diskspace problem, caused by workers 8.
> I have deactivated workers 8 and this error went away.

Yes, that error is one of the signs that AUFS cache_dirs are not SMP-aware.

Alex.


From ahmed.zaeem at netstream.ps  Thu Jul 12 19:17:34 2018
From: ahmed.zaeem at netstream.ps (--Ahmad--)
Date: Thu, 12 Jul 2018 22:17:34 +0300
Subject: [squid-users] question about squid and https connection .
Message-ID: <7866F6E4-CEC8-4513-A7FF-90DE4EC8EB8D@netstream.ps>

my 1st Q.

if i have pc# 1 
and that pc open facebook .


then i have other pc # 2 
and that other pc open facebook .


now  as we know facebook is https .

so is the key/ cert that used on pc # 1 is same as cert in pc # 2 to decrypt the fb encrypted traffic ?


now in the presence of squid .

if i used tcp connect method  , will it be different than above ?

my question in other way .


say i used 200 proxies in same squid machine and i used to access FB from the same pc same browser .

will facebook see my cert/key i used to decrypt its traffic ?

is the key/cert of FB to decrypt the https content is same on all browsers on all computers ?



kind regards 

From eliezer at ngtech.co.il  Thu Jul 12 20:17:23 2018
From: eliezer at ngtech.co.il (Eliezer Croitoru)
Date: Thu, 12 Jul 2018 23:17:23 +0300
Subject: [squid-users] Squid 4.1 "- TCP_DENIED/403' and IPv6 while
	"dns_v4_first on"
Message-ID: <037101d41a1d$586980b0$093c8210$@ngtech.co.il>

I'm testing Squid 4.1 and my proxy is showing TCP_DENIED when fetching
certificates like this:

 

1531425362.414 000000 - TCP_DENIED/403 3661 GET
http://www.microsoft.com/pki/certs/MicRooCerAut2011_2011_03_22.crt -
HIER_NONE/- text/html;charset=utf-8 Q-CC: "-" "-" Q-P: "-" "-" Q-RANGE: "-"
REP-CC: "-" REP-EXP: "-" VARY: "-" - REP-X-CACHE: "-" Adapted-X-Store-Id:
"-"

1531425364.299 000000 - TCP_DENIED/403 3661 GET
http://www.microsoft.com/pki/certs/MicRooCerAut2011_2011_03_22.crt -
HIER_NONE/- text/html;charset=utf-8 Q-CC: "-" "-" Q-P: "-" "-" Q-RANGE: "-"
REP-CC: "-" REP-EXP: "-" VARY: "-" - REP-X-CACHE: "-" Adapted-X-Store-Id:
"-"

 

If I'm not wrong Amos wrote that there is a special directive or ACL to
allow these since there is not originating from a client IP src address.

 

And also when I'm trying to access https://bugs.squid-cache.org/ with
SSL-BUMP on I am receiving the next page:


ERROR


The requested URL could not be retrieved

  _____  


The following error was encountered while trying to retrieve the URL:
https://bugs.squid-cache.org/*

Connection to 2001:4801:7827:102:ad34:6f78:b6dc:fbed failed.

The system returned: (101) Network is unreachable

The remote host or network may be down. Please try the request again.

Your cache administrator is webmaster.

 

  _____  

Generated Thu, 12 Jul 2018 20:01:40 GMT by squid4-testing (squid/4.1)

##END OF PAGE

 

With these access log lines:

1531425990.290 000000 - TCP_DENIED/403 3564 GET
http://cert.int-x3.letsencrypt.org/ - HIER_NONE/- text/html;charset=utf-8
Q-CC: "-" "-" Q-P: "-" "-" Q-RANGE: "-" REP-CC: "-" REP-EXP: "-" VARY: "-" -
REP-X-CACHE: "-" Adapted-X-Store-Id: "-"

1531425990.291 000355 10.0.0.28 NONE/200 0 CONNECT bugs.squid-cache.org:443
- HIER_DIRECT/2001:4801:7827:102:ad34:6f78:b6dc:fbed - Q-CC: "-" "-" Q-P:
"-" "-" Q-RANGE: "-" REP-CC: "-" REP-EXP: "-" VARY: "-" 00:00:00:00:00:00
REP-X-CACHE: "-" Adapted-X-Store-Id: "-"

1531425990.294 000000 - TCP_DENIED/403 3564 GET
http://cert.int-x3.letsencrypt.org/ - HIER_NONE/- text/html;charset=utf-8
Q-CC: "-" "-" Q-P: "-" "-" Q-RANGE: "-" REP-CC: "-" REP-EXP: "-" VARY: "-" -
REP-X-CACHE: "-" Adapted-X-Store-Id: "-"

1531425990.295 000359 10.0.0.28 NONE/200 0 CONNECT bugs.squid-cache.org:443
- HIER_DIRECT/2001:4801:7827:102:ad34:6f78:b6dc:fbed - Q-CC: "-" "-" Q-P:
"-" "-" Q-RANGE: "-" REP-CC: "-" REP-EXP: "-" VARY: "-" 00:00:00:00:00:00
REP-X-CACHE: "-" Adapted-X-Store-Id: "-"

1531425990.299 000000 10.0.0.28 NONE/503 4117 GET
https://bugs.squid-cache.org/index.cgi - HIER_NONE/- text/html Q-CC:
"no-cache" "no-cache" Q-P: "no-cache" "no-cache" Q-RANGE: "-" REP-CC: "-"
REP-EXP: "-" VARY: "Accept-Language" 00:00:00:00:00:00 REP-X-CACHE: "MISS
from squid4-testing" Adapted-X-Store-Id: "-"

1531425990.304 000000 - TCP_DENIED/403 3564 GET
http://cert.int-x3.letsencrypt.org/ - HIER_NONE/- text/html;charset=utf-8
Q-CC: "-" "-" Q-P: "-" "-" Q-RANGE: "-" REP-CC: "-" REP-EXP: "-" VARY: "-" -
REP-X-CACHE: "-" Adapted-X-Store-Id: "-"

1531425990.305 000365 10.0.0.28 NONE/200 0 CONNECT bugs.squid-cache.org:443
- HIER_DIRECT/2001:4801:7827:102:ad34:6f78:b6dc:fbed - Q-CC: "-" "-" Q-P:
"-" "-" Q-RANGE: "-" REP-CC: "-" REP-EXP: "-" VARY: "-" 00:00:00:00:00:00
REP-X-CACHE: "-" Adapted-X-Store-Id: "-"

1531425990.307 000000 - TCP_DENIED/403 3564 GET
http://cert.int-x3.letsencrypt.org/ - HIER_NONE/- text/html;charset=utf-8
Q-CC: "-" "-" Q-P: "-" "-" Q-RANGE: "-" REP-CC: "-" REP-EXP: "-" VARY: "-" -
REP-X-CACHE: "-" Adapted-X-Store-Id: "-"

1531425990.307 000000 - TCP_DENIED/403 3564 GET
http://cert.int-x3.letsencrypt.org/ - HIER_NONE/- text/html;charset=utf-8
Q-CC: "-" "-" Q-P: "-" "-" Q-RANGE: "-" REP-CC: "-" REP-EXP: "-" VARY: "-" -
REP-X-CACHE: "-" Adapted-X-Store-Id: "-"

1531425990.307 000372 10.0.0.28 NONE/200 0 CONNECT bugs.squid-cache.org:443
- HIER_DIRECT/2001:4801:7827:102:ad34:6f78:b6dc:fbed - Q-CC: "-" "-" Q-P:
"-" "-" Q-RANGE: "-" REP-CC: "-" REP-EXP: "-" VARY: "-" 00:00:00:00:00:00
REP-X-CACHE: "-" Adapted-X-Store-Id: "-"

1531425990.307 000368 10.0.0.28 NONE/200 0 CONNECT bugs.squid-cache.org:443
- HIER_DIRECT/2001:4801:7827:102:ad34:6f78:b6dc:fbed - Q-CC: "-" "-" Q-P:
"-" "-" Q-RANGE: "-" REP-CC: "-" REP-EXP: "-" VARY: "-" 00:00:00:00:00:00
REP-X-CACHE: "-" Adapted-X-Store-Id: "-"

1531425990.339 000000 10.0.0.28 NONE/503 4117 GET
http://squid4-testing:3128/squid-internal-static/icons/SN.png - HIER_NONE/-
text/html Q-CC: "no-cache" "no-cache" Q-P: "no-cache" "no-cache" Q-RANGE:
"-" REP-CC: "-" REP-EXP: "-" VARY: "Accept-Language" 00:00:00:00:00:00
REP-X-CACHE: "MISS from squid4-testing" Adapted-X-Store-Id: "-"

1531425990.374 000000 10.0.0.28 NONE/503 4117 GET
https://bugs.squid-cache.org/favicon.ico - HIER_NONE/- text/html Q-CC:
"no-cache" "no-cache" Q-P: "no-cache" "no-cache" Q-RANGE: "-" REP-CC: "-"
REP-EXP: "-" VARY: "Accept-Language" 00:00:00:00:00:00 REP-X-CACHE: "MISS
from squid4-testing" Adapted-X-Store-Id: "-"

 

So the issue is a bit strange, is the remote IP is the issue or another
thing?

I looked at the archives and also the docs and from what I managed to make
sure the next resolve both issues which are tangled to each other:

## START squid.conf addition

acl internal transaction_initiator internal

 

# Deny requests to certain unsafe ports

http_access deny !Safe_ports

 

# Deny CONNECT to other than secure SSL ports

http_access deny CONNECT !SSL_ports

http_access allow internal

## END squid.conf addition

 

http://www.squid-cache.org/Versions/v4/cfgman/acl.html

 

Clarify that  there is a new type of ACL named "transaction_initiator" which
does couple good things.

 

I am not sure but it seems to me that some wiki page is missing regarding
this issue.
I can try to write one if no one else will sit on it in the next month.

 

All The Bests,

Eliezer

 

----

Eliezer Croitoru <http://ngtech.co.il/lmgtfy/> 
Linux System Administrator
Mobile: +972-5-28704261
Email: eliezer at ngtech.co.il



 

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20180712/92fee28d/attachment.htm>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: image002.png
Type: image/png
Size: 11326 bytes
Desc: not available
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20180712/92fee28d/attachment.png>

From rousskov at measurement-factory.com  Thu Jul 12 20:27:00 2018
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Thu, 12 Jul 2018 14:27:00 -0600
Subject: [squid-users] question about squid and https connection .
In-Reply-To: <7866F6E4-CEC8-4513-A7FF-90DE4EC8EB8D@netstream.ps>
References: <7866F6E4-CEC8-4513-A7FF-90DE4EC8EB8D@netstream.ps>
Message-ID: <9da11737-ca55-e522-1150-cb9c26a079fb@measurement-factory.com>

On 07/12/2018 01:17 PM, --Ahmad-- wrote:

> if i have pc# 1 and that pc open facebook .
> 
> then i have other pc # 2 and that other pc open facebook .
> 
> 
> now  as we know facebook is https .
> 
> so is the key/ cert that used on pc # 1 is same as cert in pc # 2 to decrypt the fb encrypted traffic ?

Certificates themselves are not used (directly) to decrypt traffic
AFAIK, but yes, both PCs will see the same server certificate (ignoring
CDNs and other complications).



> now in the presence of squid .
> 
> if i used tcp connect method  , will it be different than above ?

If you are not bumping the connection, then both PCs will see the same
real Facebook certificate as if those PCs did not use a proxy.

If you are bumping the connection, then both PCs will see the same fake
certificate generated by Squid.



> say i used 200 proxies in same squid machine and i used to access FB from the same pc same browser .
> 
> will facebook see my cert/key i used to decrypt its traffic ?

If you are asking whether Facebook will know anything about the fake
certificate generated by Squid for clients, then the answer is "no,
unless Facebook runs some special client code to deliver (Squid)
certificate back to Facebook".

In general, the origin server assumes that the client is talking to it
directly. Clients may pin or otherwise restrict certificates that they
trust, but after the connection is successfully established, the server
may assume that it is talking to the client directly. A paranoid server
may deliver special code to double check that assumption, but there are
other, more standard methods to prevent bumping such as certificate
pinning and certificate transparency cervices.



> is the key/cert of FB to decrypt the https content is same on all browsers on all computers ?

If you are asking whether the generated certificates are going to be the
same for all clients, then the answer is "yes, provided all those 200
Squids use the same configuration (including the CA certificate) and
receive the same real certificate from Facebook". Squid's certificate
generation algorithm generates the same certificate given the same
configuration and the same origin server certificate.


HTH,

Alex.


From eliezer at ngtech.co.il  Thu Jul 12 20:35:46 2018
From: eliezer at ngtech.co.il (Eliezer Croitoru)
Date: Thu, 12 Jul 2018 23:35:46 +0300
Subject: [squid-users] question about squid and https connection .
In-Reply-To: <9da11737-ca55-e522-1150-cb9c26a079fb@measurement-factory.com>
References: <7866F6E4-CEC8-4513-A7FF-90DE4EC8EB8D@netstream.ps>
 <9da11737-ca55-e522-1150-cb9c26a079fb@measurement-factory.com>
Message-ID: <03a101d41a1f$e9ce5970$bd6b0c50$@ngtech.co.il>

Alex,

Just to be sure:
Every RSA key and certificate pair regardless to the origin server and the SSL-BUMP enabled proxy can be different.
If the key would be the exact same one then we will probably have a very big security issue/risk to my understanding (leaving aside DH).

Will it be more accurate to say that just as long as these 200 squid instances(different squid.conf and couple other local variables)
use the same exact ssl_db cache directory  then it's probable that they will use the same certificate.
Or these 200 squid instances are in SMP mode with 200 workers...
If these 200 instances do not share memory and certificate cache then there is a possibility that the same site from two different sources
will serve different certificates(due to the different RSA key which is different).

Thanks,
Eliezer

----
Eliezer Croitoru
Linux System Administrator
Mobile: +972-5-28704261
Email: eliezer at ngtech.co.il



-----Original Message-----
From: squid-users [mailto:squid-users-bounces at lists.squid-cache.org] On Behalf Of Alex Rousskov
Sent: Thursday, July 12, 2018 11:27 PM
To: --Ahmad-- <ahmed.zaeem at netstream.ps>; Squid Users <squid-users at lists.squid-cache.org>
Subject: Re: [squid-users] question about squid and https connection .

On 07/12/2018 01:17 PM, --Ahmad-- wrote:

> if i have pc# 1 and that pc open facebook .
> 
> then i have other pc # 2 and that other pc open facebook .
> 
> 
> now  as we know facebook is https .
> 
> so is the key/ cert that used on pc # 1 is same as cert in pc # 2 to decrypt the fb encrypted traffic ?

Certificates themselves are not used (directly) to decrypt traffic
AFAIK, but yes, both PCs will see the same server certificate (ignoring
CDNs and other complications).



> now in the presence of squid .
> 
> if i used tcp connect method  , will it be different than above ?

If you are not bumping the connection, then both PCs will see the same
real Facebook certificate as if those PCs did not use a proxy.

If you are bumping the connection, then both PCs will see the same fake
certificate generated by Squid.



> say i used 200 proxies in same squid machine and i used to access FB from the same pc same browser .
> 
> will facebook see my cert/key i used to decrypt its traffic ?

If you are asking whether Facebook will know anything about the fake
certificate generated by Squid for clients, then the answer is "no,
unless Facebook runs some special client code to deliver (Squid)
certificate back to Facebook".

In general, the origin server assumes that the client is talking to it
directly. Clients may pin or otherwise restrict certificates that they
trust, but after the connection is successfully established, the server
may assume that it is talking to the client directly. A paranoid server
may deliver special code to double check that assumption, but there are
other, more standard methods to prevent bumping such as certificate
pinning and certificate transparency cervices.



> is the key/cert of FB to decrypt the https content is same on all browsers on all computers ?

If you are asking whether the generated certificates are going to be the
same for all clients, then the answer is "yes, provided all those 200
Squids use the same configuration (including the CA certificate) and
receive the same real certificate from Facebook". Squid's certificate
generation algorithm generates the same certificate given the same
configuration and the same origin server certificate.


HTH,

Alex.
_______________________________________________
squid-users mailing list
squid-users at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-users



From ahmed.zaeem at netstream.ps  Thu Jul 12 22:15:02 2018
From: ahmed.zaeem at netstream.ps (--Ahmad--)
Date: Fri, 13 Jul 2018 01:15:02 +0300
Subject: [squid-users] question about squid and https connection .
In-Reply-To: <03a101d41a1f$e9ce5970$bd6b0c50$@ngtech.co.il>
References: <7866F6E4-CEC8-4513-A7FF-90DE4EC8EB8D@netstream.ps>
 <9da11737-ca55-e522-1150-cb9c26a079fb@measurement-factory.com>
 <03a101d41a1f$e9ce5970$bd6b0c50$@ngtech.co.il>
Message-ID: <4C4E77DC-D8C4-460D-A7F3-F3A0C01A7ECB@netstream.ps>

TAHNK YOU Guys ALL .


so my question is in another way is :


if i have squid proxy using it using the TCP_Connect way .

and from the same pc and same browser and try to open facebook from 200 different address .

then facebook wont have a footprint that there is 200 different addresses hit FB from the same public key /cert .

i just ant to make sure there is no footprint happen .

thats way i asked .

let me know concerns Guys , thanks alot Guys ! 

> On 12 Jul 2018, at 23:35, Eliezer Croitoru <eliezer at ngtech.co.il> wrote:
> 
> Alex,
> 
> Just to be sure:
> Every RSA key and certificate pair regardless to the origin server and the SSL-BUMP enabled proxy can be different.
> If the key would be the exact same one then we will probably have a very big security issue/risk to my understanding (leaving aside DH).
> 
> Will it be more accurate to say that just as long as these 200 squid instances(different squid.conf and couple other local variables)
> use the same exact ssl_db cache directory  then it's probable that they will use the same certificate.
> Or these 200 squid instances are in SMP mode with 200 workers...
> If these 200 instances do not share memory and certificate cache then there is a possibility that the same site from two different sources
> will serve different certificates(due to the different RSA key which is different).
> 
> Thanks,
> Eliezer
> 
> ----
> Eliezer Croitoru
> Linux System Administrator
> Mobile: +972-5-28704261
> Email: eliezer at ngtech.co.il
> 
> 
> 
> -----Original Message-----
> From: squid-users [mailto:squid-users-bounces at lists.squid-cache.org] On Behalf Of Alex Rousskov
> Sent: Thursday, July 12, 2018 11:27 PM
> To: --Ahmad-- <ahmed.zaeem at netstream.ps>; Squid Users <squid-users at lists.squid-cache.org>
> Subject: Re: [squid-users] question about squid and https connection .
> 
> On 07/12/2018 01:17 PM, --Ahmad-- wrote:
> 
>> if i have pc# 1 and that pc open facebook .
>> 
>> then i have other pc # 2 and that other pc open facebook .
>> 
>> 
>> now  as we know facebook is https .
>> 
>> so is the key/ cert that used on pc # 1 is same as cert in pc # 2 to decrypt the fb encrypted traffic ?
> 
> Certificates themselves are not used (directly) to decrypt traffic
> AFAIK, but yes, both PCs will see the same server certificate (ignoring
> CDNs and other complications).
> 
> 
> 
>> now in the presence of squid .
>> 
>> if i used tcp connect method  , will it be different than above ?
> 
> If you are not bumping the connection, then both PCs will see the same
> real Facebook certificate as if those PCs did not use a proxy.
> 
> If you are bumping the connection, then both PCs will see the same fake
> certificate generated by Squid.
> 
> 
> 
>> say i used 200 proxies in same squid machine and i used to access FB from the same pc same browser .
>> 
>> will facebook see my cert/key i used to decrypt its traffic ?
> 
> If you are asking whether Facebook will know anything about the fake
> certificate generated by Squid for clients, then the answer is "no,
> unless Facebook runs some special client code to deliver (Squid)
> certificate back to Facebook".
> 
> In general, the origin server assumes that the client is talking to it
> directly. Clients may pin or otherwise restrict certificates that they
> trust, but after the connection is successfully established, the server
> may assume that it is talking to the client directly. A paranoid server
> may deliver special code to double check that assumption, but there are
> other, more standard methods to prevent bumping such as certificate
> pinning and certificate transparency cervices.
> 
> 
> 
>> is the key/cert of FB to decrypt the https content is same on all browsers on all computers ?
> 
> If you are asking whether the generated certificates are going to be the
> same for all clients, then the answer is "yes, provided all those 200
> Squids use the same configuration (including the CA certificate) and
> receive the same real certificate from Facebook". Squid's certificate
> generation algorithm generates the same certificate given the same
> configuration and the same origin server certificate.
> 
> 
> HTH,
> 
> Alex.
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
> 



From squid3 at treenet.co.nz  Thu Jul 12 22:32:51 2018
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Fri, 13 Jul 2018 10:32:51 +1200
Subject: [squid-users] squid 3.5.27 does not respect cache_dir-size but
 uses 100% of partition and fails
In-Reply-To: <c210bc1d-210e-e10a-2b50-2513fe5658f0@measurement-factory.com>
References: <trinity-61214a10-8a85-49ba-baf2-9141166cb932-1531305541528@3c-app-gmx-bs66>
 <d868705a-c4ad-5431-8725-b5e8d815d70e@measurement-factory.com>
 <trinity-d7a1212c-5e48-43d4-b260-19149844b2b8-1531396403787@3c-app-gmx-bs16>
 <c210bc1d-210e-e10a-2b50-2513fe5658f0@measurement-factory.com>
Message-ID: <ffef4577-956d-509f-aeb3-89d8fbeb31dc@treenet.co.nz>

On 13/07/18 04:16, Alex Rousskov wrote:
> On 07/12/2018 05:53 AM, pete dawgg wrote:
> 
> 
>> When there is no traffic squid seems to cleaning up well enough: over
>> night (no traffic) disk usage went down to 30GB (now it's at 50GB
>> again)
> 
> This may be a sign that your Squid cannot keep up with the load. IIRC,
> AUFS uses lazy garbage collection so it is possible for the stream of
> new objects to outpace the stream of object deletion events, resulting
> in a gradually increasing cache size. Using even more aggressive
> cache_swap_high might help, but there is no good configuration solution
> to this UFS problem AFAIK.
> 

FYI, to be more aggressive place the two limits closer together.

I made the removal rate grow in steps of the difference between the
marks. A low of 60 and high of 70 means there are 4 steps of 10 between
60% and 100% full cache - so Squid will be removing 4*200 objects/sec
when the cache is 99.999% full. But a low of 90 and high 91 will remove
10*200 objects/sec at the same full point.

Low numbers like 60, 70 etc are only needed now if you have to push the
removal rate past 2K objects/sec - eg low 60 high 61 will be removing
40*200 = 8K objects/sec.


If you know your peak traffic rate in req/sec you should be able to tune
the purge rate to match that peak traffic rate. The speed traffic
reaches that peak should inform what the gap is between the watermarks.

Amos


From squid3 at treenet.co.nz  Thu Jul 12 22:52:37 2018
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Fri, 13 Jul 2018 10:52:37 +1200
Subject: [squid-users] Delay pools in squid4 not working with https
In-Reply-To: <1286952347.2556210.1531352354125@mail.yahoo.com>
References: <54444655-7fa0-ec9a-ea6e-505697d6bb0a@gmail.com>
 <c5468738-3efb-d3d4-843a-14f1e977ddae@measurement-factory.com>
 <1286952347.2556210.1531352354125@mail.yahoo.com>
Message-ID: <e67ce376-9ddb-c060-accd-314ef0572f03@treenet.co.nz>

On 12/07/18 11:39, Julian Perconti wrote:
>>>
>>> El ?martes?, ?10? de ?julio? de ?2018? ?18?:?57?:?43? ?-03, Alex Rousskov escribi?: 
>>>
>>>
>>> On 07/10/2018 01:50 PM, Paolo Marzari wrote:
>>>> My home server just updated from 3.5.27, everything is working fine, but
>>>> delay pools seems broken to me.
>>>
>>>> Revert to 3.5.27 and delays works again with every type of traffic.
>>>>
>>>> I think there's something wrong with https traffic.
>>>
>>> You are probably right. A few days ago, while working on an unrelated
>>> project, we have found a bug in delay pools support for tunneled https
>>> traffic. That support was probably broken by v4 commit 6b2b6cf. We have
>>> not tested v3.5, so I can only confirm that v4 and v5 are broken.
>>>
>>> The bug will be fixed as a side effect of "peering support for SslBump"
>>> changes that should be ready for the official review soon. If you would
>>> like to test our unofficial branch, the code is available at
>>> https://github.com/measurement-factory/squid/tree/SQUID-360-peering-for-SslBump
>>>
>>>
>>> HTH,
>>>
>>> Alex.
>>>
>>> _______________________________________________
>>> squid-users mailing list
>>> squid-users at lists.squid-cache.org
>>> http://lists.squid-cache.org/listinfo/squid-users
> 
> I can confirm that delay_pools works fine both http and https protocols in squid 4 running debian 9 
> 
> Squid Cache: Version 4.1 

When I looked at the code for Paolos report I found there to be a
difference between SSL-Bumped and non-Bumped traffic.

This hints to me that these opposite reports may due to how the traffic
is being handled.

So Julian, Paolo; if you don't mind can you please say whether you are
using SSL-Bump in your tests and if so whether the test traffic got
splice'd, bump'ed or no SSL-Bump feature use at all ?


There might also still be bugs specific to pool types. We have had a few
in the past that I'm not sure if ever got fixed. Though Paolo's mention
that 3.5 worked okay hints that its probably not those exact issues.


Amos


From loginmogin at gmail.com  Thu Jul 12 22:56:41 2018
From: loginmogin at gmail.com (login mogin)
Date: Thu, 12 Jul 2018 15:56:41 -0700
Subject: [squid-users] question about squid and https connection .
In-Reply-To: <4C4E77DC-D8C4-460D-A7F3-F3A0C01A7ECB@netstream.ps>
References: <7866F6E4-CEC8-4513-A7FF-90DE4EC8EB8D@netstream.ps>
 <9da11737-ca55-e522-1150-cb9c26a079fb@measurement-factory.com>
 <03a101d41a1f$e9ce5970$bd6b0c50$@ngtech.co.il>
 <4C4E77DC-D8C4-460D-A7F3-F3A0C01A7ECB@netstream.ps>
Message-ID: <CAJbsrUnt-mj_yKHQNvQCi0o8eXY-uSO++6+HBo0xLzoyE=YqRQ@mail.gmail.com>

Hi Ahmad,

Proxy will just change your ip when you are connecting FB in this way, But
FB probably has or at least should, so many other ways to detect if thats
the same person connecting, just to name one browser based profiling. They
have your user_agent, browser extensions, cookies, etc..
In other words you will have so many other footprints.

Best
Logan

--Ahmad-- <ahmed.zaeem at netstream.ps>, 12 Tem 2018 Per, 15:15 tarihinde ?unu
yazd?:

> TAHNK YOU Guys ALL .
>
>
> so my question is in another way is :
>
>
> if i have squid proxy using it using the TCP_Connect way .
>
> and from the same pc and same browser and try to open facebook from 200
> different address .
>
> then facebook wont have a footprint that there is 200 different addresses
> hit FB from the same public key /cert .
>
> i just ant to make sure there is no footprint happen .
>
> thats way i asked .
>
> let me know concerns Guys , thanks alot Guys !
>
> > On 12 Jul 2018, at 23:35, Eliezer Croitoru <eliezer at ngtech.co.il> wrote:
> >
> > Alex,
> >
> > Just to be sure:
> > Every RSA key and certificate pair regardless to the origin server and
> the SSL-BUMP enabled proxy can be different.
> > If the key would be the exact same one then we will probably have a very
> big security issue/risk to my understanding (leaving aside DH).
> >
> > Will it be more accurate to say that just as long as these 200 squid
> instances(different squid.conf and couple other local variables)
> > use the same exact ssl_db cache directory  then it's probable that they
> will use the same certificate.
> > Or these 200 squid instances are in SMP mode with 200 workers...
> > If these 200 instances do not share memory and certificate cache then
> there is a possibility that the same site from two different sources
> > will serve different certificates(due to the different RSA key which is
> different).
> >
> > Thanks,
> > Eliezer
> >
> > ----
> > Eliezer Croitoru
> > Linux System Administrator
> > Mobile: +972-5-28704261
> > Email: eliezer at ngtech.co.il
> >
> >
> >
> > -----Original Message-----
> > From: squid-users [mailto:squid-users-bounces at lists.squid-cache.org] On
> Behalf Of Alex Rousskov
> > Sent: Thursday, July 12, 2018 11:27 PM
> > To: --Ahmad-- <ahmed.zaeem at netstream.ps>; Squid Users <
> squid-users at lists.squid-cache.org>
> > Subject: Re: [squid-users] question about squid and https connection .
> >
> > On 07/12/2018 01:17 PM, --Ahmad-- wrote:
> >
> >> if i have pc# 1 and that pc open facebook .
> >>
> >> then i have other pc # 2 and that other pc open facebook .
> >>
> >>
> >> now  as we know facebook is https .
> >>
> >> so is the key/ cert that used on pc # 1 is same as cert in pc # 2 to
> decrypt the fb encrypted traffic ?
> >
> > Certificates themselves are not used (directly) to decrypt traffic
> > AFAIK, but yes, both PCs will see the same server certificate (ignoring
> > CDNs and other complications).
> >
> >
> >
> >> now in the presence of squid .
> >>
> >> if i used tcp connect method  , will it be different than above ?
> >
> > If you are not bumping the connection, then both PCs will see the same
> > real Facebook certificate as if those PCs did not use a proxy.
> >
> > If you are bumping the connection, then both PCs will see the same fake
> > certificate generated by Squid.
> >
> >
> >
> >> say i used 200 proxies in same squid machine and i used to access FB
> from the same pc same browser .
> >>
> >> will facebook see my cert/key i used to decrypt its traffic ?
> >
> > If you are asking whether Facebook will know anything about the fake
> > certificate generated by Squid for clients, then the answer is "no,
> > unless Facebook runs some special client code to deliver (Squid)
> > certificate back to Facebook".
> >
> > In general, the origin server assumes that the client is talking to it
> > directly. Clients may pin or otherwise restrict certificates that they
> > trust, but after the connection is successfully established, the server
> > may assume that it is talking to the client directly. A paranoid server
> > may deliver special code to double check that assumption, but there are
> > other, more standard methods to prevent bumping such as certificate
> > pinning and certificate transparency cervices.
> >
> >
> >
> >> is the key/cert of FB to decrypt the https content is same on all
> browsers on all computers ?
> >
> > If you are asking whether the generated certificates are going to be the
> > same for all clients, then the answer is "yes, provided all those 200
> > Squids use the same configuration (including the CA certificate) and
> > receive the same real certificate from Facebook". Squid's certificate
> > generation algorithm generates the same certificate given the same
> > configuration and the same origin server certificate.
> >
> >
> > HTH,
> >
> > Alex.
> > _______________________________________________
> > squid-users mailing list
> > squid-users at lists.squid-cache.org
> > http://lists.squid-cache.org/listinfo/squid-users
> >
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20180712/adfc39eb/attachment.htm>

From rousskov at measurement-factory.com  Thu Jul 12 23:01:17 2018
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Thu, 12 Jul 2018 17:01:17 -0600
Subject: [squid-users] question about squid and https connection .
In-Reply-To: <03a101d41a1f$e9ce5970$bd6b0c50$@ngtech.co.il>
References: <7866F6E4-CEC8-4513-A7FF-90DE4EC8EB8D@netstream.ps>
 <9da11737-ca55-e522-1150-cb9c26a079fb@measurement-factory.com>
 <03a101d41a1f$e9ce5970$bd6b0c50$@ngtech.co.il>
Message-ID: <0e875c86-f1db-9f5d-5c9c-d95bd901f70d@measurement-factory.com>

On 07/12/2018 02:35 PM, Eliezer Croitoru wrote:

> Every RSA key and certificate pair regardless to the origin server
> and the SSL-BUMP enabled proxy can be different.

I cannot find a reasonable interpretation of the above that would
contradict what I have said. Yes, each unique certificate has its own
private key, but that is not what Ahmad was asking about AFAICT.


> Will it be more accurate to say that just as long as these 200 squid
> instances(different squid.conf and couple other local variables) use
> the same exact ssl_db cache directory  then it's probable that they
> will use the same certificate.

That statement is incorrect. Squids configured with different CA
certificates will generate different fake certificates for the same real
certificate.

I assume that Ahmad was asking about a situation where 200 Squid
instances had the same configuration (including CA certificates).

Please note that the certificate generator helper gets the signing (CA)
certificate as a parameter with each generation request (because
different Squid ports may use different CA certificates). Also, Squid
probably does not officially support sharing the certificate directory
across Squid instances (even if it works).


> Or these 200 squid instances are in SMP mode with 200 workers... If
> these 200 instances do not share memory and certificate cache then
> there is a possibility that the same site from two different sources 
> will serve different certificates(due to the different RSA key which
> is different).

200 SMP workers or 200 identically-configured Squid instances will
generate the same fake certificates for the same real certificate.
"Stable certificates" is an important requirement for many distributed
Squid deployments.

Alex.



> -----Original Message-----
> From: squid-users [mailto:squid-users-bounces at lists.squid-cache.org] On Behalf Of Alex Rousskov
> Sent: Thursday, July 12, 2018 11:27 PM
> To: --Ahmad-- <ahmed.zaeem at netstream.ps>; Squid Users <squid-users at lists.squid-cache.org>
> Subject: Re: [squid-users] question about squid and https connection .
> 
> On 07/12/2018 01:17 PM, --Ahmad-- wrote:
> 
>> if i have pc# 1 and that pc open facebook .
>>
>> then i have other pc # 2 and that other pc open facebook .
>>
>>
>> now  as we know facebook is https .
>>
>> so is the key/ cert that used on pc # 1 is same as cert in pc # 2 to decrypt the fb encrypted traffic ?
> 
> Certificates themselves are not used (directly) to decrypt traffic
> AFAIK, but yes, both PCs will see the same server certificate (ignoring
> CDNs and other complications).
> 
> 
> 
>> now in the presence of squid .
>>
>> if i used tcp connect method  , will it be different than above ?
> 
> If you are not bumping the connection, then both PCs will see the same
> real Facebook certificate as if those PCs did not use a proxy.
> 
> If you are bumping the connection, then both PCs will see the same fake
> certificate generated by Squid.
> 
> 
> 
>> say i used 200 proxies in same squid machine and i used to access FB from the same pc same browser .
>>
>> will facebook see my cert/key i used to decrypt its traffic ?
> 
> If you are asking whether Facebook will know anything about the fake
> certificate generated by Squid for clients, then the answer is "no,
> unless Facebook runs some special client code to deliver (Squid)
> certificate back to Facebook".
> 
> In general, the origin server assumes that the client is talking to it
> directly. Clients may pin or otherwise restrict certificates that they
> trust, but after the connection is successfully established, the server
> may assume that it is talking to the client directly. A paranoid server
> may deliver special code to double check that assumption, but there are
> other, more standard methods to prevent bumping such as certificate
> pinning and certificate transparency cervices.
> 
> 
> 
>> is the key/cert of FB to decrypt the https content is same on all browsers on all computers ?
> 
> If you are asking whether the generated certificates are going to be the
> same for all clients, then the answer is "yes, provided all those 200
> Squids use the same configuration (including the CA certificate) and
> receive the same real certificate from Facebook". Squid's certificate
> generation algorithm generates the same certificate given the same
> configuration and the same origin server certificate.
> 
> 
> HTH,
> 
> Alex.
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
> 



From squid3 at treenet.co.nz  Thu Jul 12 23:07:24 2018
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Fri, 13 Jul 2018 11:07:24 +1200
Subject: [squid-users] question about squid and https connection .
In-Reply-To: <03a101d41a1f$e9ce5970$bd6b0c50$@ngtech.co.il>
References: <7866F6E4-CEC8-4513-A7FF-90DE4EC8EB8D@netstream.ps>
 <9da11737-ca55-e522-1150-cb9c26a079fb@measurement-factory.com>
 <03a101d41a1f$e9ce5970$bd6b0c50$@ngtech.co.il>
Message-ID: <a1c340b5-0d94-85f6-b92d-d50e44dde2ad@treenet.co.nz>

On 13/07/18 08:27, Eliezer Croitoru wrote:
> Alex,
> 
> Just to be sure:
> Every RSA key and certificate pair regardless to the origin server and the SSL-BUMP enabled proxy can be different.
> If the key would be the exact same one then we will probably have a very big security issue/risk to my understanding (leaving aside DH).
> 
> Will it be more accurate to say that just as long as these 200 squid instances(different squid.conf and couple other local variables)
> use the same exact ssl_db cache directory  then it's probable that they will use the same certificate.
> Or these 200 squid instances are in SMP mode with 200 workers...
> If these 200 instances do not share memory and certificate cache then there is a possibility that the same site from two different sources
> will serve different certificates(due to the different RSA key which is different).
> 

Instances (in terms of how we defined the term "Squid instance") cannot
share memory. They are completely separate processes. Even when in
SMP-aware operation, they are separate process groups. That is why you
have to use the -n name command line parameter to direct signals at
specific instances.


In regards to the certs. The generating of a fake cert is a hard-coded
algorithm - using the inputs Alex mentioned. The only way differences
occur between any two Squid fake certs is when the real origin server
cert given to each of them is different.
In that case you *do* absolutely want the fake ones to differ as well -
even (and especially) when they come from the same origin server.

Think of Squid as copy-n-pasting cert field values from the origin cert
to the fake cert. You wont be far off whats really happening.

Amos


From vh1988 at yahoo.com.ar  Thu Jul 12 23:19:23 2018
From: vh1988 at yahoo.com.ar (Julian Perconti)
Date: Thu, 12 Jul 2018 20:19:23 -0300
Subject: [squid-users] Delay pools in squid4 not working with https
In-Reply-To: <e67ce376-9ddb-c060-accd-314ef0572f03@treenet.co.nz>
References: <54444655-7fa0-ec9a-ea6e-505697d6bb0a@gmail.com>
 <c5468738-3efb-d3d4-843a-14f1e977ddae@measurement-factory.com>
 <1286952347.2556210.1531352354125@mail.yahoo.com>
 <e67ce376-9ddb-c060-accd-314ef0572f03@treenet.co.nz>
Message-ID: <020901d41a36$c6f92580$54eb7080$@yahoo.com.ar>

>When I looked at the code for Paolos report I found there to be a difference between SSL-Bumped and non-Bumped traffic.
>
>This hints to me that these opposite reports may due to how the traffic is being handled.
>
>So Julian, Paolo; if you don't mind can you please say whether you are using SSL-Bump in your tests and if so whether the test traffic got splice'd, bump'ed or no SSL-Bump feature use at all ?

>From my side, the tests were done with full SSL-Bump; downloading a file from: https://speed.hetzner.de/

No splice.

The delay cfg applies identically both to http/https protocols.

>There might also still be bugs specific to pool types. We have had a few in the past that I'm not sure if ever got fixed. Though Paolo's mention that 3.5 worked okay hints that its probably not those >exact issues.

Regards



From rousskov at measurement-factory.com  Thu Jul 12 23:31:18 2018
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Thu, 12 Jul 2018 17:31:18 -0600
Subject: [squid-users] Delay pools in squid4 not working with https
In-Reply-To: <020901d41a36$c6f92580$54eb7080$@yahoo.com.ar>
References: <54444655-7fa0-ec9a-ea6e-505697d6bb0a@gmail.com>
 <c5468738-3efb-d3d4-843a-14f1e977ddae@measurement-factory.com>
 <1286952347.2556210.1531352354125@mail.yahoo.com>
 <e67ce376-9ddb-c060-accd-314ef0572f03@treenet.co.nz>
 <020901d41a36$c6f92580$54eb7080$@yahoo.com.ar>
Message-ID: <af4bfee8-ddd1-1f0a-c0ac-211f9588b153@measurement-factory.com>

On 07/12/2018 05:19 PM, Julian Perconti wrote:

> From my side, the tests were done with full SSL-Bump; downloading a file from: https://speed.hetzner.de/
> 
> No splice.

My "not working" statement was specific to tunneling code. When Squid
bumps, it does not tunnel, so your tests did not tickle the broken code.
We do not yet know whether prazola is bumping HTTPS traffic.

Tunneling happens when handling CONNECT requests without SslBump and
when splicing TLS traffic with SslBump.

Alex.


From vh1988 at yahoo.com.ar  Thu Jul 12 23:42:33 2018
From: vh1988 at yahoo.com.ar (Julian Perconti)
Date: Thu, 12 Jul 2018 20:42:33 -0300
Subject: [squid-users] Delay pools in squid4 not working with https
In-Reply-To: <af4bfee8-ddd1-1f0a-c0ac-211f9588b153@measurement-factory.com>
References: <54444655-7fa0-ec9a-ea6e-505697d6bb0a@gmail.com>
 <c5468738-3efb-d3d4-843a-14f1e977ddae@measurement-factory.com>
 <1286952347.2556210.1531352354125@mail.yahoo.com>
 <e67ce376-9ddb-c060-accd-314ef0572f03@treenet.co.nz>
 <020901d41a36$c6f92580$54eb7080$@yahoo.com.ar>
 <af4bfee8-ddd1-1f0a-c0ac-211f9588b153@measurement-factory.com>
Message-ID: <021301d41a3a$02f2f810$08d8e830$@yahoo.com.ar>

> De: Alex Rousskov [mailto:rousskov at measurement-factory.com]
> Enviado el: jueves, 12 de julio de 2018 20:31
> Para: Julian Perconti <vh1988 at yahoo.com.ar>; squid-users at lists.squid-
> cache.org
> Asunto: Re: [squid-users] Delay pools in squid4 not working with https
> 
> On 07/12/2018 05:19 PM, Julian Perconti wrote:
> 
> > From my side, the tests were done with full SSL-Bump; downloading a
> > file from: https://speed.hetzner.de/
> >
> > No splice.
> 
> My "not working" statement was specific to tunneling code. When Squid
> bumps, it does not tunnel, so your tests did not tickle the broken code.
> We do not yet know whether prazola is bumping HTTPS traffic.
> 
> Tunneling happens when handling CONNECT requests without SslBump and
> when splicing TLS traffic with SslBump.
> 
> Alex.

My delay_pool cfg is working.

Without splice/tunneling the connection.
When I download a file from https://speed.hetzner.de/ with  the https prefix in the URL  downloaded file (without splice anything), the delay slows down the download once the limit is reached.

May be I missunderstood something.

Regards




From rousskov at measurement-factory.com  Fri Jul 13 00:02:35 2018
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Thu, 12 Jul 2018 18:02:35 -0600
Subject: [squid-users] Delay pools in squid4 not working with https
In-Reply-To: <021301d41a3a$02f2f810$08d8e830$@yahoo.com.ar>
References: <54444655-7fa0-ec9a-ea6e-505697d6bb0a@gmail.com>
 <c5468738-3efb-d3d4-843a-14f1e977ddae@measurement-factory.com>
 <1286952347.2556210.1531352354125@mail.yahoo.com>
 <e67ce376-9ddb-c060-accd-314ef0572f03@treenet.co.nz>
 <020901d41a36$c6f92580$54eb7080$@yahoo.com.ar>
 <af4bfee8-ddd1-1f0a-c0ac-211f9588b153@measurement-factory.com>
 <021301d41a3a$02f2f810$08d8e830$@yahoo.com.ar>
Message-ID: <acfe378c-dd9e-f155-3561-a14385e0a1d9@measurement-factory.com>

On 07/12/2018 05:42 PM, Julian Perconti wrote:
>> De: Alex Rousskov
>> On 07/12/2018 05:19 PM, Julian Perconti wrote:
>>
>>> From my side, the tests were done with full SSL-Bump; downloading a
>>> file from: https://speed.hetzner.de/
>>>
>>> No splice.


>> My "not working" statement was specific to tunneling code. When Squid
>> bumps, it does not tunnel, so your tests did not tickle the broken code.
>> We do not yet know whether prazola is bumping HTTPS traffic.
>>
>> Tunneling happens when handling CONNECT requests without SslBump and
>> when splicing TLS traffic with SslBump.


> My delay_pool cfg is working.

Yes, I understand. I do not think anybody has claimed that your config
should not be working. The only claim was that delay pools do not work
when Squid tunnels traffic. Your Squid does not tunnel traffic.


> Without splice/tunneling the connection.

... and that is why it is working. If you start splicing/tunneling, it
will probably stop working.


Alex.


From vh1988 at yahoo.com.ar  Fri Jul 13 00:16:47 2018
From: vh1988 at yahoo.com.ar (Julian Perconti)
Date: Thu, 12 Jul 2018 21:16:47 -0300
Subject: [squid-users] Delay pools in squid4 not working with https
In-Reply-To: <acfe378c-dd9e-f155-3561-a14385e0a1d9@measurement-factory.com>
References: <54444655-7fa0-ec9a-ea6e-505697d6bb0a@gmail.com>
 <c5468738-3efb-d3d4-843a-14f1e977ddae@measurement-factory.com>
 <1286952347.2556210.1531352354125@mail.yahoo.com>
 <e67ce376-9ddb-c060-accd-314ef0572f03@treenet.co.nz>
 <020901d41a36$c6f92580$54eb7080$@yahoo.com.ar>
 <af4bfee8-ddd1-1f0a-c0ac-211f9588b153@measurement-factory.com>
 <021301d41a3a$02f2f810$08d8e830$@yahoo.com.ar>
 <acfe378c-dd9e-f155-3561-a14385e0a1d9@measurement-factory.com>
Message-ID: <021f01d41a3e$cb254960$616fdc20$@yahoo.com.ar>

> -----Mensaje original-----
> De: Alex Rousskov [mailto:rousskov at measurement-factory.com]
> Enviado el: jueves, 12 de julio de 2018 21:03
> Para: Julian Perconti <vh1988 at yahoo.com.ar>; squid-users at lists.squid-
> cache.org
> Asunto: Re: [squid-users] Delay pools in squid4 not working with https
> 
> On 07/12/2018 05:42 PM, Julian Perconti wrote:
> >> De: Alex Rousskov
> >> On 07/12/2018 05:19 PM, Julian Perconti wrote:
> >>
> >>> From my side, the tests were done with full SSL-Bump; downloading a
> >>> file from: https://speed.hetzner.de/
> >>>
> >>> No splice.
> 
> 
> >> My "not working" statement was specific to tunneling code. When Squid
> >> bumps, it does not tunnel, so your tests did not tickle the broken code.
> >> We do not yet know whether prazola is bumping HTTPS traffic.
> >>
> >> Tunneling happens when handling CONNECT requests without SslBump
> and
> >> when splicing TLS traffic with SslBump.
> 
> 
> > My delay_pool cfg is working.
> 
> Yes, I understand. I do not think anybody has claimed that your config should
> not be working. The only claim was that delay pools do not work when Squid
> tunnels traffic. Your Squid does not tunnel traffic.
> 
> 
> > Without splice/tunneling the connection.
> 
> ... and that is why it is working. If you start splicing/tunneling, it will probably
> stop working.

Ok, but is not is supposed that this is the normal behaviour? 

I mean, TCP_TUNNEL = squid forward, so squid can not do nothing about the spliced connection.

I don't I am just a squid user... and BTW new in squid SSL intercepts.

> 
> 
> Alex.



From rousskov at measurement-factory.com  Fri Jul 13 00:20:09 2018
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Thu, 12 Jul 2018 18:20:09 -0600
Subject: [squid-users] Delay pools in squid4 not working with https
In-Reply-To: <021f01d41a3e$cb254960$616fdc20$@yahoo.com.ar>
References: <54444655-7fa0-ec9a-ea6e-505697d6bb0a@gmail.com>
 <c5468738-3efb-d3d4-843a-14f1e977ddae@measurement-factory.com>
 <1286952347.2556210.1531352354125@mail.yahoo.com>
 <e67ce376-9ddb-c060-accd-314ef0572f03@treenet.co.nz>
 <020901d41a36$c6f92580$54eb7080$@yahoo.com.ar>
 <af4bfee8-ddd1-1f0a-c0ac-211f9588b153@measurement-factory.com>
 <021301d41a3a$02f2f810$08d8e830$@yahoo.com.ar>
 <acfe378c-dd9e-f155-3561-a14385e0a1d9@measurement-factory.com>
 <021f01d41a3e$cb254960$616fdc20$@yahoo.com.ar>
Message-ID: <4e234a80-f2ad-269c-2b61-8523b1654fea@measurement-factory.com>

On 07/12/2018 06:16 PM, Julian Perconti wrote:
>> De: Alex Rousskov
>> If you start splicing/tunneling, it will probably stop working.


> Ok, but is not is supposed that this is the normal behaviour? 


No, Squid should apply delay pools to all traffic.


> I mean, TCP_TUNNEL = squid forward, so squid can not do nothing about the spliced connection.


Squid knows how many bytes it is forwarding, and that is all Squid needs
to know to shape traffic.

Alex.


From eliezer at ngtech.co.il  Fri Jul 13 00:20:24 2018
From: eliezer at ngtech.co.il (Eliezer Croitoru)
Date: Fri, 13 Jul 2018 03:20:24 +0300
Subject: [squid-users] squid 3.5.27 does not respect cache_dir-size but
	uses 100% of partition and fails
In-Reply-To: <ffef4577-956d-509f-aeb3-89d8fbeb31dc@treenet.co.nz>
References: <trinity-61214a10-8a85-49ba-baf2-9141166cb932-1531305541528@3c-app-gmx-bs66>
 <d868705a-c4ad-5431-8725-b5e8d815d70e@measurement-factory.com>
 <trinity-d7a1212c-5e48-43d4-b260-19149844b2b8-1531396403787@3c-app-gmx-bs16>
 <c210bc1d-210e-e10a-2b50-2513fe5658f0@measurement-factory.com>
 <ffef4577-956d-509f-aeb3-89d8fbeb31dc@treenet.co.nz>
Message-ID: <047901d41a3f$4aeaca30$e0c05e90$@ngtech.co.il>

Hey Amos,

>From the docs:
http://www.squid-cache.org/Versions/v4/cfgman/cache_swap_low.html

I see that this is only for UFS/AUFS/diskd and not rock cache_dir.
What about rock cache_dir?

Eliezer

----
Eliezer Croitoru
Linux System Administrator
Mobile: +972-5-28704261
Email: eliezer at ngtech.co.il



-----Original Message-----
From: squid-users [mailto:squid-users-bounces at lists.squid-cache.org] On Behalf Of Amos Jeffries
Sent: Friday, July 13, 2018 1:33 AM
To: squid-users at lists.squid-cache.org
Subject: Re: [squid-users] squid 3.5.27 does not respect cache_dir-size but uses 100% of partition and fails

On 13/07/18 04:16, Alex Rousskov wrote:
> On 07/12/2018 05:53 AM, pete dawgg wrote:
> 
> 
>> When there is no traffic squid seems to cleaning up well enough: over
>> night (no traffic) disk usage went down to 30GB (now it's at 50GB
>> again)
> 
> This may be a sign that your Squid cannot keep up with the load. IIRC,
> AUFS uses lazy garbage collection so it is possible for the stream of
> new objects to outpace the stream of object deletion events, resulting
> in a gradually increasing cache size. Using even more aggressive
> cache_swap_high might help, but there is no good configuration solution
> to this UFS problem AFAIK.
> 

FYI, to be more aggressive place the two limits closer together.

I made the removal rate grow in steps of the difference between the
marks. A low of 60 and high of 70 means there are 4 steps of 10 between
60% and 100% full cache - so Squid will be removing 4*200 objects/sec
when the cache is 99.999% full. But a low of 90 and high 91 will remove
10*200 objects/sec at the same full point.

Low numbers like 60, 70 etc are only needed now if you have to push the
removal rate past 2K objects/sec - eg low 60 high 61 will be removing
40*200 = 8K objects/sec.


If you know your peak traffic rate in req/sec you should be able to tune
the purge rate to match that peak traffic rate. The speed traffic
reaches that peak should inform what the gap is between the watermarks.

Amos
_______________________________________________
squid-users mailing list
squid-users at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-users



From rousskov at measurement-factory.com  Fri Jul 13 00:23:52 2018
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Thu, 12 Jul 2018 18:23:52 -0600
Subject: [squid-users] squid 3.5.27 does not respect cache_dir-size but
 uses 100% of partition and fails
In-Reply-To: <047901d41a3f$4aeaca30$e0c05e90$@ngtech.co.il>
References: <trinity-61214a10-8a85-49ba-baf2-9141166cb932-1531305541528@3c-app-gmx-bs66>
 <d868705a-c4ad-5431-8725-b5e8d815d70e@measurement-factory.com>
 <trinity-d7a1212c-5e48-43d4-b260-19149844b2b8-1531396403787@3c-app-gmx-bs16>
 <c210bc1d-210e-e10a-2b50-2513fe5658f0@measurement-factory.com>
 <ffef4577-956d-509f-aeb3-89d8fbeb31dc@treenet.co.nz>
 <047901d41a3f$4aeaca30$e0c05e90$@ngtech.co.il>
Message-ID: <59a46456-6f72-211a-8502-2bf06be1a752@measurement-factory.com>

On 07/12/2018 06:20 PM, Eliezer Croitoru wrote:

> From the docs:
> http://www.squid-cache.org/Versions/v4/cfgman/cache_swap_low.html
> 
> I see that this is only for UFS/AUFS/diskd and not rock cache_dir.
> What about rock cache_dir?

Rock cache_dirs cannot overflow by design. Rock reserves a configured
amount of disk space and uses nothing but that amount of disk space. Due
to optimistic allocation by file systems, you can still run out of disk
space if something else consumes space on the same partition, but the
rock database itself cannot overflow.

Alex.


> -----Original Message-----
> From: squid-users [mailto:squid-users-bounces at lists.squid-cache.org] On Behalf Of Amos Jeffries
> Sent: Friday, July 13, 2018 1:33 AM
> To: squid-users at lists.squid-cache.org
> Subject: Re: [squid-users] squid 3.5.27 does not respect cache_dir-size but uses 100% of partition and fails
> 
> On 13/07/18 04:16, Alex Rousskov wrote:
>> On 07/12/2018 05:53 AM, pete dawgg wrote:
>>
>>
>>> When there is no traffic squid seems to cleaning up well enough: over
>>> night (no traffic) disk usage went down to 30GB (now it's at 50GB
>>> again)
>>
>> This may be a sign that your Squid cannot keep up with the load. IIRC,
>> AUFS uses lazy garbage collection so it is possible for the stream of
>> new objects to outpace the stream of object deletion events, resulting
>> in a gradually increasing cache size. Using even more aggressive
>> cache_swap_high might help, but there is no good configuration solution
>> to this UFS problem AFAIK.
>>
> 
> FYI, to be more aggressive place the two limits closer together.
> 
> I made the removal rate grow in steps of the difference between the
> marks. A low of 60 and high of 70 means there are 4 steps of 10 between
> 60% and 100% full cache - so Squid will be removing 4*200 objects/sec
> when the cache is 99.999% full. But a low of 90 and high 91 will remove
> 10*200 objects/sec at the same full point.
> 
> Low numbers like 60, 70 etc are only needed now if you have to push the
> removal rate past 2K objects/sec - eg low 60 high 61 will be removing
> 40*200 = 8K objects/sec.
> 
> 
> If you know your peak traffic rate in req/sec you should be able to tune
> the purge rate to match that peak traffic rate. The speed traffic
> reaches that peak should inform what the gap is between the watermarks.
> 
> Amos
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
> 
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
> 



From vh1988 at yahoo.com.ar  Fri Jul 13 00:24:10 2018
From: vh1988 at yahoo.com.ar (Julian Perconti)
Date: Thu, 12 Jul 2018 21:24:10 -0300
Subject: [squid-users] Delay pools in squid4 not working with https
In-Reply-To: <4e234a80-f2ad-269c-2b61-8523b1654fea@measurement-factory.com>
References: <54444655-7fa0-ec9a-ea6e-505697d6bb0a@gmail.com>
 <c5468738-3efb-d3d4-843a-14f1e977ddae@measurement-factory.com>
 <1286952347.2556210.1531352354125@mail.yahoo.com>
 <e67ce376-9ddb-c060-accd-314ef0572f03@treenet.co.nz>
 <020901d41a36$c6f92580$54eb7080$@yahoo.com.ar>
 <af4bfee8-ddd1-1f0a-c0ac-211f9588b153@measurement-factory.com>
 <021301d41a3a$02f2f810$08d8e830$@yahoo.com.ar>
 <acfe378c-dd9e-f155-3561-a14385e0a1d9@measurement-factory.com>
 <021f01d41a3e$cb254960$616fdc20$@yahoo.com.ar>
 <4e234a80-f2ad-269c-2b61-8523b1654fea@measurement-factory.com>
Message-ID: <022101d41a3f$d31d94a0$7958bde0$@yahoo.com.ar>

> -----Mensaje original-----
> De: Alex Rousskov [mailto:rousskov at measurement-factory.com]
> Enviado el: jueves, 12 de julio de 2018 21:20
> Para: Julian Perconti <vh1988 at yahoo.com.ar>; squid-users at lists.squid-
> cache.org
> Asunto: Re: [squid-users] Delay pools in squid4 not working with https
> 
> On 07/12/2018 06:16 PM, Julian Perconti wrote:
> >> De: Alex Rousskov
> >> If you start splicing/tunneling, it will probably stop working.
> 
> 
> > Ok, but is not is supposed that this is the normal behaviour?
> 
> 
> No, Squid should apply delay pools to all traffic.

OK I Will splice https://speed.hetzner.de/ and then tell You what happened with delay pool.

An important thing, the delay_pool cfg that Paolo has is pretty complexthan mine.

> 
> 
> > I mean, TCP_TUNNEL = squid forward, so squid can not do nothing about
> the spliced connection.
> 
> 
> Squid knows how many bytes it is forwarding, and that is all Squid needs to
> know to shape traffic.
> 
> Alex.



From vh1988 at yahoo.com.ar  Fri Jul 13 00:56:06 2018
From: vh1988 at yahoo.com.ar (Julian Perconti)
Date: Thu, 12 Jul 2018 21:56:06 -0300
Subject: [squid-users] Delay pools in squid4 not working with https
References: <54444655-7fa0-ec9a-ea6e-505697d6bb0a@gmail.com>
 <c5468738-3efb-d3d4-843a-14f1e977ddae@measurement-factory.com>
 <1286952347.2556210.1531352354125@mail.yahoo.com>
 <e67ce376-9ddb-c060-accd-314ef0572f03@treenet.co.nz>
 <020901d41a36$c6f92580$54eb7080$@yahoo.com.ar>
 <af4bfee8-ddd1-1f0a-c0ac-211f9588b153@measurement-factory.com>
 <021301d41a3a$02f2f810$08d8e830$@yahoo.com.ar>
 <acfe378c-dd9e-f155-3561-a14385e0a1d9@measurement-factory.com>
 <021f01d41a3e$cb254960$616fdc20$@yahoo.com.ar>
 <4e234a80-f2ad-269c-2b61-8523b1654fea@measurement-factory.com> 
Message-ID: <022401d41a44$493a68d0$dbaf3a70$@yahoo.com.ar>

> -----Mensaje original-----
> De: Julian Perconti [mailto:vh1988 at yahoo.com.ar]
> Enviado el: jueves, 12 de julio de 2018 21:24
> Para: 'squid-users at lists.squid-cache.org' <squid-users at lists.squid-
> cache.org>
> Asunto: RE: [squid-users] Delay pools in squid4 not working with https
> 
> > -----Mensaje original-----
> > De: Alex Rousskov [mailto:rousskov at measurement-factory.com]
> > Enviado el: jueves, 12 de julio de 2018 21:20
> > Para: Julian Perconti <vh1988 at yahoo.com.ar>; squid-users at lists.squid-
> > cache.org
> > Asunto: Re: [squid-users] Delay pools in squid4 not working with https
> >
> > On 07/12/2018 06:16 PM, Julian Perconti wrote:
> > >> De: Alex Rousskov
> > >> If you start splicing/tunneling, it will probably stop working.
> >
> >
> > > Ok, but is not is supposed that this is the normal behaviour?
> >
> >
> > No, Squid should apply delay pools to all traffic.

Ok, I did not know that..

> 
> OK I Will splice https://speed.hetzner.de/ and then tell You what happened
> with delay pool.
> 
> An important thing, the delay_pool cfg that Paolo has is pretty complexthan
> mine.

Confirmed.

Splicing.. speed.hetzner.de

TCP_TUNNEL/200 4452 CONNECT 88.198.248.254:443 - ORIGINAL_DST/88.198.248.254 -

The delay_pool does not work.

Download speed never goes down.

delay_pool class 2 cfg:

delay_pools 1 
delay_class 1 2
delay_access 1 allow all

delay_parameters 1 -1/-1 100000/104857600

Version:

Squid Cache: Version 4.1
Service Name: squid

This binary uses OpenSSL 1.1.0f  25 May 2017. For legal restrictions on distribution see https://www.openssl.org/source/license.html

> 
> >
> >
> > > I mean, TCP_TUNNEL = squid forward, so squid can not do nothing
> > > about
> > the spliced connection.
> >
> >
> > Squid knows how many bytes it is forwarding, and that is all Squid
> > needs to know to shape traffic.
> >
> > Alex.



From eliezer at ngtech.co.il  Fri Jul 13 01:23:29 2018
From: eliezer at ngtech.co.il (Eliezer Croitoru)
Date: Fri, 13 Jul 2018 04:23:29 +0300
Subject: [squid-users] squid 3.5.27 does not respect cache_dir-size but
	uses 100% of partition and fails
In-Reply-To: <59a46456-6f72-211a-8502-2bf06be1a752@measurement-factory.com>
References: <trinity-61214a10-8a85-49ba-baf2-9141166cb932-1531305541528@3c-app-gmx-bs66>
 <d868705a-c4ad-5431-8725-b5e8d815d70e@measurement-factory.com>
 <trinity-d7a1212c-5e48-43d4-b260-19149844b2b8-1531396403787@3c-app-gmx-bs16>
 <c210bc1d-210e-e10a-2b50-2513fe5658f0@measurement-factory.com>
 <ffef4577-956d-509f-aeb3-89d8fbeb31dc@treenet.co.nz>
 <047901d41a3f$4aeaca30$e0c05e90$@ngtech.co.il>
 <59a46456-6f72-211a-8502-2bf06be1a752@measurement-factory.com>
Message-ID: <047d01d41a48$1b650600$522f1200$@ngtech.co.il>

Gear!

I am testing it with 4.1 since UFS and AUFS are great but... doesn't support SMP.

Eliezer

* another thread on the way to the list.

----
Eliezer Croitoru
Linux System Administrator
Mobile: +972-5-28704261
Email: eliezer at ngtech.co.il



-----Original Message-----
From: Alex Rousskov [mailto:rousskov at measurement-factory.com] 
Sent: Friday, July 13, 2018 3:24 AM
To: squid-users at lists.squid-cache.org
Cc: Eliezer Croitoru <eliezer at ngtech.co.il>
Subject: Re: [squid-users] squid 3.5.27 does not respect cache_dir-size but uses 100% of partition and fails

On 07/12/2018 06:20 PM, Eliezer Croitoru wrote:

> From the docs:
> http://www.squid-cache.org/Versions/v4/cfgman/cache_swap_low.html
> 
> I see that this is only for UFS/AUFS/diskd and not rock cache_dir.
> What about rock cache_dir?

Rock cache_dirs cannot overflow by design. Rock reserves a configured
amount of disk space and uses nothing but that amount of disk space. Due
to optimistic allocation by file systems, you can still run out of disk
space if something else consumes space on the same partition, but the
rock database itself cannot overflow.

Alex.


> -----Original Message-----
> From: squid-users [mailto:squid-users-bounces at lists.squid-cache.org] On Behalf Of Amos Jeffries
> Sent: Friday, July 13, 2018 1:33 AM
> To: squid-users at lists.squid-cache.org
> Subject: Re: [squid-users] squid 3.5.27 does not respect cache_dir-size but uses 100% of partition and fails
> 
> On 13/07/18 04:16, Alex Rousskov wrote:
>> On 07/12/2018 05:53 AM, pete dawgg wrote:
>>
>>
>>> When there is no traffic squid seems to cleaning up well enough: over
>>> night (no traffic) disk usage went down to 30GB (now it's at 50GB
>>> again)
>>
>> This may be a sign that your Squid cannot keep up with the load. IIRC,
>> AUFS uses lazy garbage collection so it is possible for the stream of
>> new objects to outpace the stream of object deletion events, resulting
>> in a gradually increasing cache size. Using even more aggressive
>> cache_swap_high might help, but there is no good configuration solution
>> to this UFS problem AFAIK.
>>
> 
> FYI, to be more aggressive place the two limits closer together.
> 
> I made the removal rate grow in steps of the difference between the
> marks. A low of 60 and high of 70 means there are 4 steps of 10 between
> 60% and 100% full cache - so Squid will be removing 4*200 objects/sec
> when the cache is 99.999% full. But a low of 90 and high 91 will remove
> 10*200 objects/sec at the same full point.
> 
> Low numbers like 60, 70 etc are only needed now if you have to push the
> removal rate past 2K objects/sec - eg low 60 high 61 will be removing
> 40*200 = 8K objects/sec.
> 
> 
> If you know your peak traffic rate in req/sec you should be able to tune
> the purge rate to match that peak traffic rate. The speed traffic
> reaches that peak should inform what the gap is between the watermarks.
> 
> Amos
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
> 
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
> 




From eliezer at ngtech.co.il  Fri Jul 13 01:25:31 2018
From: eliezer at ngtech.co.il (Eliezer Croitoru)
Date: Fri, 13 Jul 2018 04:25:31 +0300
Subject: [squid-users] parse URL too large (10001 bytes) error
Message-ID: <048401d41a48$63e73010$2bb59030$@ngtech.co.il>

I am seeing from google video links in the cache.log at squid 4.1:

2018/07/13 03:31:39 kid1| parse URL too large (10016 bytes)

2018/07/13 03:31:42 kid1| parse URL too large (10001 bytes)

 

I will file a report later.

 

Eliezer

 

----

Eliezer Croitoru <http://ngtech.co.il/lmgtfy/> 
Linux System Administrator
Mobile: +972-5-28704261
Email: eliezer at ngtech.co.il



 

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20180713/9f3d9910/attachment.htm>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: image001.png
Type: image/png
Size: 11298 bytes
Desc: not available
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20180713/9f3d9910/attachment.png>

From loginmogin at gmail.com  Fri Jul 13 01:58:17 2018
From: loginmogin at gmail.com (login mogin)
Date: Thu, 12 Jul 2018 18:58:17 -0700
Subject: [squid-users] ERROR: Unknown TLS option clientca
In-Reply-To: <CAJbsrUkfukNyFsP3wBKu_1Sk+0SYaGa8H1JwKk41o=A197ymoA@mail.gmail.com>
References: <CAJbsrUkfukNyFsP3wBKu_1Sk+0SYaGa8H1JwKk41o=A197ymoA@mail.gmail.com>
Message-ID: <CAJbsrU=5hQeM9kAuibggPFSLSeQ3W4wmPst4rqSkC5E5EDZoTw@mail.gmail.com>

Do you guys have any idea on this? Or should I report this as a bug?

On Thu, Jul 12, 2018 at 4:11 AM login mogin <loginmogin at gmail.com> wrote:

> Hi,
>
> We have been using squid 3.5.23 on ubuntu 16 with the configuration
> clientca=CERTPATH without any problem. We decided to run the new version
> squid 4.1 on ubuntu 18 with the same config. But now client certificate
> auth is not working anymore and we got this message on debug:
>
> ERROR: Unknown TLS option 'clientca=/etc/squid/cert/ca/ca.crt'
> ...
>
> Are we missing something or
> http://www.squid-cache.org/Doc/config/http_port/ clientca option is
> broken?
>
> By the way we also tried tls-cafile and capath options, we didn't get any
> error messages with these options but still squid server is not requesting
> any client certificate.
>
> Appreciate the help.
>
> Regards,
> Logan
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20180712/d3a741f5/attachment.htm>

From rousskov at measurement-factory.com  Fri Jul 13 05:03:45 2018
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Thu, 12 Jul 2018 23:03:45 -0600
Subject: [squid-users] ERROR: Unknown TLS option clientca
In-Reply-To: <CAJbsrU=5hQeM9kAuibggPFSLSeQ3W4wmPst4rqSkC5E5EDZoTw@mail.gmail.com>
References: <CAJbsrUkfukNyFsP3wBKu_1Sk+0SYaGa8H1JwKk41o=A197ymoA@mail.gmail.com>
 <CAJbsrU=5hQeM9kAuibggPFSLSeQ3W4wmPst4rqSkC5E5EDZoTw@mail.gmail.com>
Message-ID: <08764746-faf1-0aee-56ef-7accb2841dd3@measurement-factory.com>

On 07/12/2018 07:58 PM, login mogin wrote:
> Or should I report this as a bug?

Your call, but it is a bug. You can also try the following _untested_
patch: https://github.com/squid-cache/squid/pull/252.patch


Good luck,

Alex.


> On Thu, Jul 12, 2018 at 4:11 AM login mogin wrote:
> 
>     Hi,
> 
>     We have been using squid 3.5.23 on ubuntu 16 with the configuration
>     clientca=CERTPATH without any problem. We decided to run the new
>     version squid 4.1 on ubuntu 18 with the same config. But now client
>     certificate auth is not working anymore and we got this message on
>     debug:
> 
>     ERROR: Unknown TLS option 'clientca=/etc/squid/cert/ca/ca.crt'
>     ...
> 
>     Are we missing something
>     or?http://www.squid-cache.org/Doc/config/http_port/ clientca option
>     is broken?
> 
>     By the way we also tried tls-cafile and capath options, we didn't
>     get any error messages with these options but still squid server is
>     not requesting any client certificate.
> 
>     Appreciate the help.
> 
>     Regards,
>     Logan
> 
> 
> 
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
> 



From loginmogin at gmail.com  Fri Jul 13 05:35:54 2018
From: loginmogin at gmail.com (login mogin)
Date: Thu, 12 Jul 2018 22:35:54 -0700
Subject: [squid-users] ERROR: Unknown TLS option clientca
In-Reply-To: <08764746-faf1-0aee-56ef-7accb2841dd3@measurement-factory.com>
References: <CAJbsrUkfukNyFsP3wBKu_1Sk+0SYaGa8H1JwKk41o=A197ymoA@mail.gmail.com>
 <CAJbsrU=5hQeM9kAuibggPFSLSeQ3W4wmPst4rqSkC5E5EDZoTw@mail.gmail.com>
 <08764746-faf1-0aee-56ef-7accb2841dd3@measurement-factory.com>
Message-ID: <CAJbsrUnb+g59M900OeWzyNfq9KLCGP1FQs=Q6zTpxxmcn5b4VA@mail.gmail.com>

Thanks a lot, just tried the patch, sadly still not working.

Best
Logan

Alex Rousskov <rousskov at measurement-factory.com>, 12 Tem 2018 Per, 22:03
tarihinde ?unu yazd?:

> On 07/12/2018 07:58 PM, login mogin wrote:
> > Or should I report this as a bug?
>
> Your call, but it is a bug. You can also try the following _untested_
> patch: https://github.com/squid-cache/squid/pull/252.patch
>
>
> Good luck,
>
> Alex.
>
>
> > On Thu, Jul 12, 2018 at 4:11 AM login mogin wrote:
> >
> >     Hi,
> >
> >     We have been using squid 3.5.23 on ubuntu 16 with the configuration
> >     clientca=CERTPATH without any problem. We decided to run the new
> >     version squid 4.1 on ubuntu 18 with the same config. But now client
> >     certificate auth is not working anymore and we got this message on
> >     debug:
> >
> >     ERROR: Unknown TLS option 'clientca=/etc/squid/cert/ca/ca.crt'
> >     ...
> >
> >     Are we missing something
> >     or http://www.squid-cache.org/Doc/config/http_port/ clientca option
> >     is broken?
> >
> >     By the way we also tried tls-cafile and capath options, we didn't
> >     get any error messages with these options but still squid server is
> >     not requesting any client certificate.
> >
> >     Appreciate the help.
> >
> >     Regards,
> >     Logan
> >
> >
> >
> > _______________________________________________
> > squid-users mailing list
> > squid-users at lists.squid-cache.org
> > http://lists.squid-cache.org/listinfo/squid-users
> >
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20180712/0f678ad9/attachment.htm>

From rousskov at measurement-factory.com  Fri Jul 13 15:00:10 2018
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Fri, 13 Jul 2018 09:00:10 -0600
Subject: [squid-users] ERROR: Unknown TLS option clientca
In-Reply-To: <CAJbsrUnb+g59M900OeWzyNfq9KLCGP1FQs=Q6zTpxxmcn5b4VA@mail.gmail.com>
References: <CAJbsrUkfukNyFsP3wBKu_1Sk+0SYaGa8H1JwKk41o=A197ymoA@mail.gmail.com>
 <CAJbsrU=5hQeM9kAuibggPFSLSeQ3W4wmPst4rqSkC5E5EDZoTw@mail.gmail.com>
 <08764746-faf1-0aee-56ef-7accb2841dd3@measurement-factory.com>
 <CAJbsrUnb+g59M900OeWzyNfq9KLCGP1FQs=Q6zTpxxmcn5b4VA@mail.gmail.com>
Message-ID: <38f31e74-ec64-e4a9-09c4-3df37fe270de@measurement-factory.com>

On 07/12/2018 11:35 PM, login mogin wrote:
> Thanks a lot, just tried the patch, sadly still not working.

If you still get "Unknown TLS option" errors when specifying clientca,
then you may not have rebuilt Squid correctly. If you no longer get
those errors, but Squid still does not ask the client for the
certificate, then follow the pull request on GitHub for more
fixes/updates -- there is more work needed to fix the bug than my
configuration parsing patch.

https://github.com/squid-cache/squid/pull/252

Alex.


> Alex Rousskov, 12 Tem 2018 Per, 22:03
> tarihinde ?unu yazd?:
> 
>     On 07/12/2018 07:58 PM, login mogin wrote:
>     > Or should I report this as a bug?
> 
>     Your call, but it is a bug. You can also try the following _untested_
>     patch: https://github.com/squid-cache/squid/pull/252.patch
> 
> 
>     Good luck,
> 
>     Alex.
> 
> 
>     > On Thu, Jul 12, 2018 at 4:11 AM login mogin wrote:
>     >
>     >? ? ?Hi,
>     >
>     >? ? ?We have been using squid 3.5.23 on ubuntu 16 with the
>     configuration
>     >? ? ?clientca=CERTPATH without any problem. We decided to run the new
>     >? ? ?version squid 4.1 on ubuntu 18 with the same config. But now
>     client
>     >? ? ?certificate auth is not working anymore and we got this message on
>     >? ? ?debug:
>     >
>     >? ? ?ERROR: Unknown TLS option 'clientca=/etc/squid/cert/ca/ca.crt'
>     >? ? ?...
>     >
>     >? ? ?Are we missing something
>     >? ? ?or?http://www.squid-cache.org/Doc/config/http_port/ clientca
>     option
>     >? ? ?is broken?
>     >
>     >? ? ?By the way we also tried tls-cafile and capath options, we didn't
>     >? ? ?get any error messages with these options but still squid
>     server is
>     >? ? ?not requesting any client certificate.
>     >
>     >? ? ?Appreciate the help.
>     >
>     >? ? ?Regards,
>     >? ? ?Logan
>     >
>     >
>     >
>     > _______________________________________________
>     > squid-users mailing list
>     > squid-users at lists.squid-cache.org
>     <mailto:squid-users at lists.squid-cache.org>
>     > http://lists.squid-cache.org/listinfo/squid-users
>     >
> 



From loginmogin at gmail.com  Fri Jul 13 18:32:05 2018
From: loginmogin at gmail.com (login mogin)
Date: Fri, 13 Jul 2018 11:32:05 -0700
Subject: [squid-users] ERROR: Unknown TLS option clientca
In-Reply-To: <38f31e74-ec64-e4a9-09c4-3df37fe270de@measurement-factory.com>
References: <CAJbsrUkfukNyFsP3wBKu_1Sk+0SYaGa8H1JwKk41o=A197ymoA@mail.gmail.com>
 <CAJbsrU=5hQeM9kAuibggPFSLSeQ3W4wmPst4rqSkC5E5EDZoTw@mail.gmail.com>
 <08764746-faf1-0aee-56ef-7accb2841dd3@measurement-factory.com>
 <CAJbsrUnb+g59M900OeWzyNfq9KLCGP1FQs=Q6zTpxxmcn5b4VA@mail.gmail.com>
 <38f31e74-ec64-e4a9-09c4-3df37fe270de@measurement-factory.com>
Message-ID: <CAJbsrUnfY2z0H9Q+ebqwGBGU-QQooRcVyW-0h9Qtopw1Gr9X3A@mail.gmail.com>

Thanks for the help. Now I am not getting any error messages but as you
said I will follow the pull request.

Best
Logan

On Fri, Jul 13, 2018 at 8:00 AM Alex Rousskov <
rousskov at measurement-factory.com> wrote:

> On 07/12/2018 11:35 PM, login mogin wrote:
> > Thanks a lot, just tried the patch, sadly still not working.
>
> If you still get "Unknown TLS option" errors when specifying clientca,
> then you may not have rebuilt Squid correctly. If you no longer get
> those errors, but Squid still does not ask the client for the
> certificate, then follow the pull request on GitHub for more
> fixes/updates -- there is more work needed to fix the bug than my
> configuration parsing patch.
>
> https://github.com/squid-cache/squid/pull/252
>
> Alex.
>
>
> > Alex Rousskov, 12 Tem 2018 Per, 22:03
> > tarihinde ?unu yazd?:
> >
> >     On 07/12/2018 07:58 PM, login mogin wrote:
> >     > Or should I report this as a bug?
> >
> >     Your call, but it is a bug. You can also try the following _untested_
> >     patch: https://github.com/squid-cache/squid/pull/252.patch
> >
> >
> >     Good luck,
> >
> >     Alex.
> >
> >
> >     > On Thu, Jul 12, 2018 at 4:11 AM login mogin wrote:
> >     >
> >     >     Hi,
> >     >
> >     >     We have been using squid 3.5.23 on ubuntu 16 with the
> >     configuration
> >     >     clientca=CERTPATH without any problem. We decided to run the
> new
> >     >     version squid 4.1 on ubuntu 18 with the same config. But now
> >     client
> >     >     certificate auth is not working anymore and we got this
> message on
> >     >     debug:
> >     >
> >     >     ERROR: Unknown TLS option 'clientca=/etc/squid/cert/ca/ca.crt'
> >     >     ...
> >     >
> >     >     Are we missing something
> >     >     or http://www.squid-cache.org/Doc/config/http_port/ clientca
> >     option
> >     >     is broken?
> >     >
> >     >     By the way we also tried tls-cafile and capath options, we
> didn't
> >     >     get any error messages with these options but still squid
> >     server is
> >     >     not requesting any client certificate.
> >     >
> >     >     Appreciate the help.
> >     >
> >     >     Regards,
> >     >     Logan
> >     >
> >     >
> >     >
> >     > _______________________________________________
> >     > squid-users mailing list
> >     > squid-users at lists.squid-cache.org
> >     <mailto:squid-users at lists.squid-cache.org>
> >     > http://lists.squid-cache.org/listinfo/squid-users
> >     >
> >
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20180713/dd940436/attachment.htm>

From squid3 at treenet.co.nz  Fri Jul 13 20:18:41 2018
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sat, 14 Jul 2018 08:18:41 +1200
Subject: [squid-users] parse URL too large (10001 bytes) error
In-Reply-To: <048401d41a48$63e73010$2bb59030$@ngtech.co.il>
References: <048401d41a48$63e73010$2bb59030$@ngtech.co.il>
Message-ID: <2a996c08-80da-91c3-653a-2f152ecc640f@treenet.co.nz>

On 13/07/18 13:25, Eliezer Croitoru wrote:
> I am seeing from google video links in the cache.log at squid 4.1:
> 
> 2018/07/13 03:31:39 kid1| parse URL too large (10016 bytes)
> 
> 2018/07/13 03:31:42 kid1| parse URL too large (10001 bytes)
> 
> ?
> 
> I will file a report later.
> 
> ?

This has been an issue since the early 2000's and there is already many
bug reports about it. The current tracker one is:

<https://bugs.squid-cache.org/show_bug.cgi?id=4422>

FWIW: Squid already supports the 8000 char URL size required by specs.
Anything above that is just feature enhancement.

Also, applications which cannot cope with systems restricted to 8000
byte URLs are non-compliant with HTTP specs. Google Chrome devs helped
write decide that specific text be 8000 instead of the older 4000, so
the client should better work with the response Squid produces after
that log entry.

Amos


From krystyna.niesiolowska at interia.pl  Sat Jul 14 00:34:15 2018
From: krystyna.niesiolowska at interia.pl (Krystyna =?UTF-8?q?Niesio=C5=82owska?=)
Date: Sat, 14 Jul 2018 02:34:15 +0200
Subject: [squid-users] =?utf-8?q?=28no_subject=29?=
Message-ID: <jnxwqchqchfzbymphfad@nsvj>

Hi All,In&nbsp;my company, the HR uses an outsourced SaaS (on a unique public IP) configured with&nbsp;a commercial SSL certificate (i.e. I have both the private and the public key) accessed by our employees via a subdomain of our company domain (saas.company.com)&nbsp;. Unfortunately, we cannot control the data being transferred by the HR people and because of the GDPR the board wants to be able to get alerts if anyone tries to transfer personal data to the cloud + a general channel to check&nbsp;against any data exfiltration.My idea is to set to route&nbsp;all traffic going to sass.company.com&nbsp;via a box running Squid with SSL interception. I would like to install the same cert as the one used with the SaaS. This is to avoid the need of installing any additional certs on use's' machines. Unfortunately, I cannot find an option to set Squid with a single commercial cert instead of a CA (commonly used to intercept generate individual certs for all of the SSL traffic).Does anybody have any suggestions on the viable setup?Best wishes,Kristin
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20180714/7630c1e5/attachment.htm>

From loginmogin at gmail.com  Sat Jul 14 02:51:34 2018
From: loginmogin at gmail.com (login mogin)
Date: Fri, 13 Jul 2018 19:51:34 -0700
Subject: [squid-users] (no subject)
In-Reply-To: <jnxwqchqchfzbymphfad@nsvj>
References: <jnxwqchqchfzbymphfad@nsvj>
Message-ID: <CAJbsrUkYL5Q1cdzh7zsNtMx3yADxnkO=GkMkm0DbRNU9u9Yqtw@mail.gmail.com>

Hi,

I don?t get why you need a squid box for that purpose. If you have the
private key you could end the traffic on like nginx and just forward it to
saas, while doing that you could log the traffic as you want.

Best
Logan

On Fri, Jul 13, 2018 at 5:34 PM Krystyna Niesio?owska <
krystyna.niesiolowska at interia.pl> wrote:

> Hi All,
>
> In my company, the HR uses an outsourced SaaS (on a unique public IP)
> configured with a commercial SSL certificate (i.e. I have both the private
> and the public key) accessed by our employees via a subdomain of our
> company domain (saas.company.com) <http://saas.mycompany.com_> .
> Unfortunately, we cannot control the data being transferred by the HR
> people and because of the GDPR the board wants to be able to get alerts if
> anyone tries to transfer personal data to the cloud + a general channel to
> check against any data exfiltration.
>
> My idea is to set to route all traffic going to sass.company.com via a
> box running Squid with SSL interception. I would like to install the same
> cert as the one used with the SaaS. This is to avoid the need of installing
> any additional certs on use's' machines. Unfortunately, I cannot find an
> option to set Squid with a single commercial cert instead of a CA (commonly
> used to intercept generate individual certs for all of the SSL traffic).
>
> Does anybody have any suggestions on the viable setup?
>
> Best wishes,
>
> Kristin
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20180713/ac5aa148/attachment.htm>

From squid3 at treenet.co.nz  Sat Jul 14 06:58:44 2018
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sat, 14 Jul 2018 18:58:44 +1200
Subject: [squid-users] (no subject)
In-Reply-To: <jnxwqchqchfzbymphfad@nsvj>
References: <jnxwqchqchfzbymphfad@nsvj>
Message-ID: <221421f4-2c0c-2ab4-d6bd-853788549360@treenet.co.nz>

On 14/07/18 12:34, Krystyna Niesio?owska wrote:
> Hi All,
> 
> In?my company, the HR uses an outsourced SaaS (on a unique public IP)
> configured with?a commercial SSL certificate (i.e. I have both the
> private and the public key) accessed by our employees via a subdomain of
> our company domain (saas.company.com) <http://saas.mycompany.com_>?.
> Unfortunately, we cannot control the data being transferred by the HR
> people and because of the GDPR the board wants to be able to get alerts
> if anyone tries to transfer personal data to the cloud + a general
> channel to check?against any data exfiltration.
> 
> 
> My idea is to set to route?all traffic going to sass.company.com
> <http://sass.company.com>?via a box running Squid with SSL interception.
> I would like to install the same cert as the one used with the SaaS.
> This is to avoid the need of installing any additional certs on use's'
> machines. Unfortunately, I cannot find an option to set Squid with a
> single commercial cert instead of a CA (commonly used to intercept
> generate individual certs for all of the SSL traffic).
> 
> Does anybody have any suggestions on the viable setup?
> 

What you are calling a "commercial certificate" is actually a server
certificate. It is solely for use on servers and reverse-proxy.

So you need to setup Squid as a reverse-proxy (aka CDN frontend) to the
SAAS service instead.

To do that have your network DNS resolver provide the internal clients
with the proxy IP address instead of the remote servers IP and in
squid.conf:

 https_port 443 accel cert=... key=...
 acl saas dstdomain saas.example.com
 cache_peer saas.example.com 443 0 originserver ssl
 cache_peer_access allow saas
 cache_peer_access deny all


Amos


From david at articatech.com  Sat Jul 14 23:40:12 2018
From: david at articatech.com (David Touzeau)
Date: Sun, 15 Jul 2018 01:40:12 +0200
Subject: [squid-users] Squid v4.1: commBind Cannot bind [::1] on SNMP with
	no ipv6
Message-ID: <043801d41bcc$02b7df30$08279d90$@articatech.com>

Hi

 

Hi,

 

Ipv6 is not enabled on this Debian 9 system.

 

sysctl -a |grep ipv6|grep disable

sysctl: reading key "net.ipv6.conf.all.stable_secret"

sysctl: reading key "net.ipv6.conf.default.stable_secret"

sysctl: reading key "net.ipv6.conf.eth0.stable_secret"

net.ipv6.conf.all.disable_ipv6 = 1

net.ipv6.conf.default.disable_ipv6 = 1

sysctl: reading key "net.ipv6.conf.eth1.stable_secret"

sysctl: reading key "net.ipv6.conf.lo.stable_secret"

net.ipv6.conf.eth0.disable_ipv6 = 1

net.ipv6.conf.eth1.disable_ipv6 = 1

net.ipv6.conf.lo.disable_ipv6 = 1

 

Squid try to open a socket on ipv6 loopback.

 

2018/07/15 01:32:45 kid2| Sending SNMP messages from 0.0.0.0:3401

2018/07/15 01:32:45 kid2| commBind Cannot bind socket FD 155 to [::1]: (99)
Cannot assign requested address

2018/07/15 01:32:45 kid2| commBind Cannot bind socket FD 156 to [::1]: (99)
Cannot assign requested address

2018/07/15 01:32:45 kid2| ERROR: Failed to create helper child read FD:
UDP[::1]

 

2018/07/15 01:32:45 kid1| Sending SNMP messages from 0.0.0.0:3401

2018/07/15 01:32:45 kid1| commBind Cannot bind socket FD 117 to [::1]: (99)
Cannot assign requested address

2018/07/15 01:32:45 kid1| commBind Cannot bind socket FD 118 to [::1]: (99)
Cannot assign requested address

2018/07/15 01:32:45 kid1| ERROR: Failed to create helper child read FD:
UDP[::1]

 

How to avoid this error ?

 

Config :

snmp_port 3401

snmp_incoming_address 0.0.0.0

#snmp_outgoing_address 255.255.255.255

acl snmppublic snmp_community public

acl snmpConsole src 127.0.0.1

snmp_access allow snmpConsole

snmp_access allow snmppublic snmpConsole

snmp_access allow snmppublic localhost

snmp_access allow snmppublic MgRClient

snmp_access deny all

 

 

Squid Cache: Version 4.1

Service Name: squid

 

This binary uses OpenSSL 1.1.0f  25 May 2017. For legal restrictions on
distribution see https://www.openssl.org/source/license.html

 

configure options:  '--prefix=/usr' '--build=x86_64-linux-gnu'
'--includedir=/include' '--mandir=/share/man' '--infodir=/share/info'
'--localstatedir=/var' '--libexecdir=/lib/squid3'
'--disable-maintainer-mode' '--disable-dependency-tracking'
'--datadir=/usr/share/squid3' '--sysconfdir=/etc/squid3' '--enable-gnuregex'
'--enable-removal-policy=heap' '--enable-follow-x-forwarded-for'
'--enable-removal-policies=lru,heap' '--enable-arp-acl' '--enable-truncate'
'--with-large-files' '--with-pthreads' '--enable-esi'
'--enable-storeio=aufs,diskd,ufs,rock' '--enable-x-accelerator-vary'
'--with-dl' '--enable-linux-netfilter' '--with-netfilter-conntrack'
'--enable-wccpv2' '--enable-eui' '--enable-auth' '--enable-auth-basic'
'--enable-snmp' '--enable-icmp' '--enable-auth-digest'
'--enable-log-daemon-helpers' '--enable-url-rewrite-helpers'
'--enable-auth-ntlm' '--with-default-user=squid' '--enable-icap-client'
'--disable-cache-digests' '--enable-poll' '--enable-epoll'
'--enable-async-io=128' '--enable-zph-qos' '--enable-delay-pools'
'--enable-http-violations' '--enable-url-maps' '--enable-ecap'
'--enable-ssl' '--with-openssl' '--enable-ssl-crtd'
'--enable-xmalloc-statistics' '--enable-ident-lookups'
'--with-filedescriptors=65536' '--with-aufs-threads=128'
'--disable-arch-native' '--with-logdir=/var/log/squid'
'--with-pidfile=/var/run/squid/squid.pid' '--with-swapdir=/var/cache/squid'
'build_alias=x86_64-linux-gnu'

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20180715/622d975a/attachment.htm>

From squid3 at treenet.co.nz  Sun Jul 15 05:54:13 2018
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sun, 15 Jul 2018 17:54:13 +1200
Subject: [squid-users] Squid v4.1: commBind Cannot bind [::1] on SNMP
 with no ipv6
In-Reply-To: <043801d41bcc$02b7df30$08279d90$@articatech.com>
References: <043801d41bcc$02b7df30$08279d90$@articatech.com>
Message-ID: <09f94433-9b61-6644-7096-b8306b95ee10@treenet.co.nz>

On 15/07/18 11:40, David Touzeau wrote:
> Hi
> 
> ?
> 
> Hi,
> 
> ?
> 
> Ipv6 is not enabled on this Debian 9 system.
> 

Nod. That would be why is cannot open IPv6 sockets.

Squid is designed to comply with RFC 6540 (aka BCP 177), and to assume
the machine it is running on also complies:
 "IPv6 Support Required for All IP-Capable Nodes"

Anyhow ...

> 
> Squid try to open a socket on ipv6 loopback?
> 
> ?
> 
> 2018/07/15 01:32:45 kid2| Sending SNMP messages from 0.0.0.0:3401
> 

Above says SNMP is working fine.

Then _something else_ has issues ...

> 2018/07/15 01:32:45 kid2| commBind Cannot bind socket FD 155 to [::1]:
> (99) Cannot assign requested address
> 
> 2018/07/15 01:32:45 kid2| commBind Cannot bind socket FD 156 to [::1]:
> (99) Cannot assign requested address
> 
> 2018/07/15 01:32:45 kid2| ERROR: Failed to create helper child read FD:
> UDP[::1]
> 

One of the helpers you are using needs IPv6 to send UDP packets to/from
Squid.

I would look at external_acl_type helpers. That is usually the one which
surprises IPv4-only people.

When your Squid is built to assume enabled IPv6 and your machine is
setup to disable it, you need to add the "ipv4" option to your
external_acl_type helper config lines.

PS. from the config given it looks like you don't need that snmpConsole
ACL. The built-in localhost ACL covers the same case(s) and will also
continue working if/when you decide to enable IPv6 within your network.

Amos


From david at articatech.com  Sun Jul 15 21:56:55 2018
From: david at articatech.com (David Touzeau)
Date: Sun, 15 Jul 2018 23:56:55 +0200
Subject: [squid-users] Squid v4.1: commBind Cannot bind [::1] on SNMP
	with no ipv6
In-Reply-To: <09f94433-9b61-6644-7096-b8306b95ee10@treenet.co.nz>
References: <043801d41bcc$02b7df30$08279d90$@articatech.com>
 <09f94433-9b61-6644-7096-b8306b95ee10@treenet.co.nz>
Message-ID: <048701d41c86$bf42d2d0$3dc87870$@articatech.com>

We have this one too

2018/07/15 23:38:11 kid2| Accepting SNMP messages on 0.0.0.0:3401
2018/07/15 23:45:02 kid2| snmpHandleUdp: FD 23 recvfrom: (11) Resource temporarily unavailable
2018/07/15 23:45:02 kid2| snmpHandleUdp: FD 23 recvfrom: (11) Resource temporarily unavailable
2018/07/15 23:45:02 kid2| snmpHandleUdp: FD 23 recvfrom: (11) Resource temporarily unavailable
2018/07/15 23:45:02 kid2| snmpHandleUdp: FD 23 recvfrom: (11) Resource temporarily unavailable
2018/07/15 23:45:02 kid2| snmpHandleUdp: FD 23 recvfrom: (11) Resource temporarily unavailable
2018/07/15 23:45:02 kid2| snmpHandleUdp: FD 23 recvfrom: (11) Resource temporarily unavailable
2018/07/15 23:45:02 kid2| snmpHandleUdp: FD 23 recvfrom: (11) Resource temporarily unavailable
2018/07/15 23:45:02 kid2| snmpHandleUdp: FD 23 recvfrom: (11) Resource temporarily unavailable
2018/07/15 23:45:02 kid2| snmpHandleUdp: FD 23 recvfrom: (11) Resource temporarily unavailable
2018/07/15 23:45:02 kid2| snmpHandleUdp: FD 23 recvfrom: (11) Resource temporarily unavailable
2018/07/15 23:45:02 kid2| snmpHandleUdp: FD 23 recvfrom: (11) Resource temporarily unavailable
2018/07/15 23:45:02 kid2| snmpHandleUdp: FD 23 recvfrom: (11) Resource temporarily unavailable
2018/07/15 23:45:02 kid2| snmpHandleUdp: FD 23 recvfrom: (11) Resource temporarily unavailable
2018/07/15 23:45:02 kid2| snmpHandleUdp: FD 23 recvfrom: (11) Resource temporarily unavailable
2018/07/15 23:45:02 kid2| snmpHandleUdp: FD 23 recvfrom: (11) Resource temporarily unavailable
2018/07/15 23:45:02 kid2| snmpHandleUdp: FD 23 recvfrom: (11) Resource temporarily unavailable
2018/07/15 23:45:02 kid2| snmpHandleUdp: FD 23 recvfrom: (11) Resource temporarily unavailable
2018/07/15 23:45:02 kid2| snmpHandleUdp: FD 23 recvfrom: (11) Resource temporarily unavailable
2018/07/15 23:45:02 kid2| snmpHandleUdp: FD 23 recvfrom: (11) Resource temporarily unavailable
2018/07/15 23:45:02 kid2| snmpHandleUdp: FD 23 recvfrom: (11) Resource temporarily unavailable
2018/07/15 23:45:02 kid2| snmpHandleUdp: FD 23 recvfrom: (11) Resource temporarily unavailable
2018/07/15 23:45:02 kid2| snmpHandleUdp: FD 23 recvfrom: (11) Resource temporarily unavailable
2018/07/15 23:45:02 kid2| snmpHandleUdp: FD 23 recvfrom: (11) Resource temporarily unavailable
2018/07/15 23:45:02 kid2| snmpHandleUdp: FD 23 recvfrom: (11) Resource temporarily unavailable

I"m pretty sure this is the SNMP module...


-----Message d'origine-----
De : squid-users <squid-users-bounces at lists.squid-cache.org> De la part de Amos Jeffries
Envoy? : dimanche 15 juillet 2018 07:54
? : squid-users at lists.squid-cache.org
Objet : Re: [squid-users] Squid v4.1: commBind Cannot bind [::1] on SNMP with no ipv6

On 15/07/18 11:40, David Touzeau wrote:
> Hi
> 
>  
> 
> Hi,
> 
>  
> 
> Ipv6 is not enabled on this Debian 9 system.
> 

Nod. That would be why is cannot open IPv6 sockets.

Squid is designed to comply with RFC 6540 (aka BCP 177), and to assume the machine it is running on also complies:
 "IPv6 Support Required for All IP-Capable Nodes"

Anyhow ...

> 
> Squid try to open a socket on ipv6 loopback?
> 
>  
> 
> 2018/07/15 01:32:45 kid2| Sending SNMP messages from 0.0.0.0:3401
> 

Above says SNMP is working fine.

Then _something else_ has issues ...

> 2018/07/15 01:32:45 kid2| commBind Cannot bind socket FD 155 to [::1]:
> (99) Cannot assign requested address
> 
> 2018/07/15 01:32:45 kid2| commBind Cannot bind socket FD 156 to [::1]:
> (99) Cannot assign requested address
> 
> 2018/07/15 01:32:45 kid2| ERROR: Failed to create helper child read FD:
> UDP[::1]
> 

One of the helpers you are using needs IPv6 to send UDP packets to/from Squid.

I would look at external_acl_type helpers. That is usually the one which surprises IPv4-only people.

When your Squid is built to assume enabled IPv6 and your machine is setup to disable it, you need to add the "ipv4" option to your external_acl_type helper config lines.

PS. from the config given it looks like you don't need that snmpConsole ACL. The built-in localhost ACL covers the same case(s) and will also continue working if/when you decide to enable IPv6 within your network.

Amos
_______________________________________________
squid-users mailing list
squid-users at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-users



From eliezer at ngtech.co.il  Sun Jul 15 23:17:13 2018
From: eliezer at ngtech.co.il (Eliezer Croitoru)
Date: Mon, 16 Jul 2018 02:17:13 +0300
Subject: [squid-users] NgTech repo new service: fastest.ngtech.co.il/repo/
Message-ID: <!&!AAAAAAAAAAAuAAAAAAAAAL3wsAcc8JBJkMCbPuiQMGEBAMO2jhD3dRHOtM0AqgC7tuYAAAAAAA4AABAAAAATf+oDjkWUQpP1Q8qkrBgMAQAAAAA=@ngtech.co.il>

Hey Squid-Users,

 

I am running a trial period to see how it works for these who needs it.

The RPM's repository is sitting at:

http://fastest.ngtech.co.il/repo/

 

and will give faster speed ie 10Mbps++ compared to the local server which
has only 1Mbps upload with QOS on it.

Please use it will care since the service is there for you and these who
need it.

If the service bandwidth will be abused I will take it down.

 

Thanks,

Eliezer

 

----

Eliezer Croitoru <http://ngtech.co.il/lmgtfy/> 
Linux System Administrator
Mobile: +972-5-28704261
Email: eliezer at ngtech.co.il



 

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20180716/00326c63/attachment.htm>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: image001.png
Type: image/png
Size: 11308 bytes
Desc: not available
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20180716/00326c63/attachment.png>

From capcoding at gmail.com  Mon Jul 16 02:47:10 2018
From: capcoding at gmail.com (Gordon Hsiao)
Date: Sun, 15 Jul 2018 21:47:10 -0500
Subject: [squid-users] shared_memory_locking failed to mlock
Message-ID: <CAK0iFYxbnJbGcLY_=vsswdbaRq1jYpkKcEY3Pjf-_EjChnFrtw@mail.gmail.com>

Just upgraded squid to 4.1, however if I enabled shared_memory_locking I
failed to start squid:

"FATAL: shared_memory_locking on but failed to
mlock(/squid-tls_session_cache.shm, 2101212): (12) Out of memory"

How do I know how much memory it is trying to mlock? is 2101212(~2MB) the
shm size of not, any way to debug/looking-into/config this size?

Again I disabled cache etc for a memory restricted environment, also used
the minimal configuration with a few enable-flags, in the meantime I want
to avoid memory overcommit from squid(thus mlock)

Regards,
Gordon
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20180715/3b319164/attachment.htm>

From capcoding at gmail.com  Mon Jul 16 13:55:16 2018
From: capcoding at gmail.com (Gordon Hsiao)
Date: Mon, 16 Jul 2018 08:55:16 -0500
Subject: [squid-users] shared_memory_locking failed to mlock
In-Reply-To: <CAK0iFYxbnJbGcLY_=vsswdbaRq1jYpkKcEY3Pjf-_EjChnFrtw@mail.gmail.com>
References: <CAK0iFYxbnJbGcLY_=vsswdbaRq1jYpkKcEY3Pjf-_EjChnFrtw@mail.gmail.com>
Message-ID: <CAK0iFYzaJbQ9MQeVAFo8fCqVXQetLvGEP6=MWjQ62HZe3bNB4w@mail.gmail.com>

after increased shared memory(/dev/shm on linux), it does not help, still
don't know why mlock fails, or how much memory it needs to mlock to avoid
failing.

On Sun, Jul 15, 2018 at 9:47 PM Gordon Hsiao <capcoding at gmail.com> wrote:

> Just upgraded squid to 4.1, however if I enabled shared_memory_locking I
> failed to start squid:
>
> "FATAL: shared_memory_locking on but failed to
> mlock(/squid-tls_session_cache.shm, 2101212): (12) Out of memory"
>
> How do I know how much memory it is trying to mlock? is 2101212(~2MB) the
> shm size of not, any way to debug/looking-into/config this size?
>
> Again I disabled cache etc for a memory restricted environment, also used
> the minimal configuration with a few enable-flags, in the meantime I want
> to avoid memory overcommit from squid(thus mlock)
>
> Regards,
> Gordon
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20180716/8da93bf5/attachment.htm>

From mohammed.khallaf at gmail.com  Mon Jul 16 14:24:52 2018
From: mohammed.khallaf at gmail.com (MK2018)
Date: Mon, 16 Jul 2018 07:24:52 -0700 (MST)
Subject: [squid-users] Chrome 67.0.3396.99 irratic request behavior that
 evokes squid 4.0.21 TCP_DENIED_REPLY/403
Message-ID: <1531751092498-0.post@n4.nabble.com>

- Chrome 67.0.3396.99 erratic request behavior that evokes squid 4.0.21
TCP_DENIED_REPLY/403
- Problem exists only with ShutterStock, so far!
- Sample URL:
https://image.shutterstock.com/image-photo/chalk-rubbed-out-on-blackboard-260nw-523858522.jpg

- squid log of this URL GET...

For Firefox: 

1531750141.212     48 x.x.x.x TCP_MISS/304 431 GET
https://image.shutterstock.com/image-photo/chalk-rubbed-out-on-blackboard-260nw-523858522.jpg
- HIER_DIRECT/92.123.34.124 image/jpeg


For Chrome:

1531750158.912     53 x.x.x.x TCP_DENIED_REPLY/403 4526 GET
https://image.shutterstock.com/image-photo/chalk-rubbed-out-on-blackboard-260nw-523858522.jpg
- HIER_DIRECT/92.123.34.124 text/html

Any clue why or how this is happening? Thank you so much!



--
Sent from: http://squid-web-proxy-cache.1019090.n4.nabble.com/Squid-Users-f1019091.html


From mohammed.khallaf at gmail.com  Mon Jul 16 14:54:35 2018
From: mohammed.khallaf at gmail.com (MK2018)
Date: Mon, 16 Jul 2018 07:54:35 -0700 (MST)
Subject: [squid-users] Chrome 67.0.3396.99 irratic request behavior that
 evokes squid 4.0.21 TCP_DENIED_REPLY/403
In-Reply-To: <1531751092498-0.post@n4.nabble.com>
References: <1531751092498-0.post@n4.nabble.com>
Message-ID: <1531752875099-0.post@n4.nabble.com>

I just realized squid stable 4.1 has been released. Will try it first then
report back if problem is till there!



--
Sent from: http://squid-web-proxy-cache.1019090.n4.nabble.com/Squid-Users-f1019091.html


From mohammed.khallaf at gmail.com  Mon Jul 16 15:32:34 2018
From: mohammed.khallaf at gmail.com (MK2018)
Date: Mon, 16 Jul 2018 08:32:34 -0700 (MST)
Subject: [squid-users] Chrome 67.0.3396.99 irratic request behavior that
 evokes squid 4.0.21 TCP_DENIED_REPLY/403
In-Reply-To: <1531752875099-0.post@n4.nabble.com>
References: <1531751092498-0.post@n4.nabble.com>
 <1531752875099-0.post@n4.nabble.com>
Message-ID: <1531755154531-0.post@n4.nabble.com>

Yaaaaaay :) :)

fixed in squid 4.1 stable.

Thank you squid team, you rock!

I'll not delete this request just in case someone else wonders :)



--
Sent from: http://squid-web-proxy-cache.1019090.n4.nabble.com/Squid-Users-f1019091.html


From rousskov at measurement-factory.com  Mon Jul 16 16:58:20 2018
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Mon, 16 Jul 2018 10:58:20 -0600
Subject: [squid-users] shared_memory_locking failed to mlock
In-Reply-To: <CAK0iFYxbnJbGcLY_=vsswdbaRq1jYpkKcEY3Pjf-_EjChnFrtw@mail.gmail.com>
References: <CAK0iFYxbnJbGcLY_=vsswdbaRq1jYpkKcEY3Pjf-_EjChnFrtw@mail.gmail.com>
Message-ID: <f4ce5954-65ad-3a24-0a83-8925a7add7b2@measurement-factory.com>

On 07/15/2018 08:47 PM, Gordon Hsiao wrote:
> Just upgraded squid to 4.1, however if I enabled shared_memory_locking I
> failed to start squid:
> 
> "FATAL: shared_memory_locking on but failed to
> mlock(/squid-tls_session_cache.shm, 2101212): (12) Out of memory"

> How do I know how much memory it is trying to mlock? is 2101212(~2MB)
> the shm size of not,

Yes, Squid tried to lock a 2101212-byte segment and failed.


> any way to debug/looking-into/config this size?

I am not sure what you mean, but please keep in mind that the failed
segment could be the last straw -- most of the shared memory could be
allocated earlier. You can observe all allocations/locks with 54,7
debugging. Look for "mlock(".

You can also run "strace" or a similar command line tool to track
allocations, but analyzing strace output may be more difficult than
looking through Squid logs.


> Again I disabled cache etc for a memory restricted environment, also
> used the minimal configuration with a few enable-flags, in the meantime
> I want to avoid memory overcommit from squid(thus mlock)

I am glad the new code is working to prevent runtime crashes in your
memory-restricted environment. If studying previous mlock() calls does
not help, please suggest what else Squid could do not help you.


Thank you,

Alex.


From capcoding at gmail.com  Mon Jul 16 23:08:18 2018
From: capcoding at gmail.com (Gordon Hsiao)
Date: Mon, 16 Jul 2018 18:08:18 -0500
Subject: [squid-users] shared_memory_locking failed to mlock
In-Reply-To: <f4ce5954-65ad-3a24-0a83-8925a7add7b2@measurement-factory.com>
References: <CAK0iFYxbnJbGcLY_=vsswdbaRq1jYpkKcEY3Pjf-_EjChnFrtw@mail.gmail.com>
 <f4ce5954-65ad-3a24-0a83-8925a7add7b2@measurement-factory.com>
Message-ID: <CAK0iFYxqi3Q1yZqSFbG0RG3dbwwVsuxAEKNZ0jEHnh4Yy3DGwg@mail.gmail.com>

On a x86/64bit ubuntu machine if I set 'workers 4' and run:

squid --foreground -f /etc/squid.conf 2>&1 |grep mlock
  mlock(0x7f2e5bfb2000, 8)                = 0
  mlock(0x7f2e5bf9f000, 73912)            = -1 ENOMEM (Cannot allocate
memory)
squid -N -f /etc/squid.conf 2>& |grep mlock
  mlock(0x7f8e4b7c0000, 8)                = 0
  mlock(0x7f8e4b7ad000, 73912)            = -1 ENOMEM (Cannot allocate
memory)

Note 1; -N and --foreground made no difference as long as 'workers 4' is
set, I was expecting -N will ignore "worker 4", does it?

Now I set 'workers 2' and run the same two commands above and I got the
output(both are the same), which means squid started successfully:
  mlock(0x7f0c441cc000, 8)                = 0
  mlock(0x7f0c441c3000, 32852)            = 0
  mlock(0x7f0c441c2000, 52)               = 0

Note as long as "workers <=2" I can run squid as expected and mlock the
memory. I have more than 4GB RAM free(this is a 8GB RAM laptop) and this is
a Intel i7, the mlock failure is strange.

On my target system which has 512MB RAM, even 'workers 0' won't help, I
still get :

  mlock(0x778de000, 2101212)              = -1 ENOMEM (Out of memory)

I have to disable lock-memory for now and it puzzles me why the very first
2MB mlock can fail. I strace|grep shm_get and shmat and found nothing,
instead there are lots of mmap calls, so Squid is using mmap for its shared
memory mapping, the only question is that, is this mlock file-backed-up or
is it anonymous mmaped(in this case on Linux it will use /dev/shm by
default)?

Thanks a lot,

Gordon

On Mon, Jul 16, 2018 at 11:58 AM Alex Rousskov <
rousskov at measurement-factory.com> wrote:

> On 07/15/2018 08:47 PM, Gordon Hsiao wrote:
> > Just upgraded squid to 4.1, however if I enabled shared_memory_locking I
> > failed to start squid:
> >
> > "FATAL: shared_memory_locking on but failed to
> > mlock(/squid-tls_session_cache.shm, 2101212): (12) Out of memory"
>
> > How do I know how much memory it is trying to mlock? is 2101212(~2MB)
> > the shm size of not,
>
> Yes, Squid tried to lock a 2101212-byte segment and failed.
>
>
> > any way to debug/looking-into/config this size?
>
> I am not sure what you mean, but please keep in mind that the failed
> segment could be the last straw -- most of the shared memory could be
> allocated earlier. You can observe all allocations/locks with 54,7
> debugging. Look for "mlock(".
>
> You can also run "strace" or a similar command line tool to track
> allocations, but analyzing strace output may be more difficult than
> looking through Squid logs.
>
>
> > Again I disabled cache etc for a memory restricted environment, also
> > used the minimal configuration with a few enable-flags, in the meantime
> > I want to avoid memory overcommit from squid(thus mlock)
>
> I am glad the new code is working to prevent runtime crashes in your
> memory-restricted environment. If studying previous mlock() calls does
> not help, please suggest what else Squid could do not help you.
>
>
> Thank you,
>
> Alex.
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20180716/7ac570c8/attachment.htm>

From rousskov at measurement-factory.com  Mon Jul 16 23:38:17 2018
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Mon, 16 Jul 2018 17:38:17 -0600
Subject: [squid-users] shared_memory_locking failed to mlock
In-Reply-To: <CAK0iFYxqi3Q1yZqSFbG0RG3dbwwVsuxAEKNZ0jEHnh4Yy3DGwg@mail.gmail.com>
References: <CAK0iFYxbnJbGcLY_=vsswdbaRq1jYpkKcEY3Pjf-_EjChnFrtw@mail.gmail.com>
 <f4ce5954-65ad-3a24-0a83-8925a7add7b2@measurement-factory.com>
 <CAK0iFYxqi3Q1yZqSFbG0RG3dbwwVsuxAEKNZ0jEHnh4Yy3DGwg@mail.gmail.com>
Message-ID: <e1357a97-3f47-1064-1f0f-9c249e38f81d@measurement-factory.com>

On 07/16/2018 05:08 PM, Gordon Hsiao wrote:
> On a x86/64bit ubuntu machine if I set 'workers 4' and run:

> squid --foreground -f /etc/squid.conf 2>&1 |grep mlock
> ? mlock(0x7f2e5bfb2000, 8)? ? ? ? ? ? ? ? = 0
> ? mlock(0x7f2e5bf9f000, 73912)? ? ? ? ? ? = -1 ENOMEM

> squid -N -f /etc/squid.conf 2>& |grep mlock
> ? mlock(0x7f8e4b7c0000, 8)? ? ? ? ? ? ? ? = 0
> ? mlock(0x7f8e4b7ad000, 73912)? ? ? ? ? ? = -1 ENOMEM

> Note 1; -N and --foreground made no difference as long as 'workers 4' is
> set, I was expecting -N will ignore "worker 4", does it?

IIRC, -N does not start workers. However, some (memory allocation) code
may not honor -N and still allocate memory necessary for those (disabled
by -N) workers. That would be a bug AFAICT.


> Now I set 'workers 2' and run the same two commands above and I got the
> output(both are the same), which means squid started successfully:
> ? mlock(0x7f0c441cc000, 8)? ? ? ? ? ? ? ? = 0
> ? mlock(0x7f0c441c3000, 32852)? ? ? ? ? ? = 0
> ? mlock(0x7f0c441c2000, 52)? ? ? ? ? ? ? ?= 0

The second allocation is probably smaller because two workers need fewer
SMP queues (or similar shared memory resources) than four workers.


> I have more than 4GB RAM free(this is a 8GB RAM laptop) and this
> is a Intel i7, the mlock failure is strange.

The default amount of shared memory available to a program is often much
smaller than the total amount of RAM. I do not recall which Ubuntu
commands or sysctl settings control the former, but Squid wiki or other
web resources should have that info. The question you should ask
yourself is "How much shared memory is available for the Squid process"?


> On my target system which has 512MB RAM, even 'workers 0' won't help, I
> still get :
> 
> ? mlock(0x778de000, 2101212)? ? ? ? ? ? ? = -1 ENOMEM (Out of memory)

For "workers 0" concerns, please see the -N discussion above. The two
should be equivalent.


> I have to disable lock-memory for now and it puzzles me why the very
> first 2MB mlock can fail.

Most likely, your OS is configured (or defaults) to provide very little
shared memory to a process when the total RAM is only 512MB.


> I strace|grep shm_get and shmat and found nothing,

mlock() is a system call so strace should see it, but it may be called
something else.


> instead there are lots of mmap calls, so Squid is using mmap
> for its shared memory mapping,

Squid creates segments using shm_open() and attaches to them using mmap().


> the only question is that, is this mlock
> file-backed-up or is it anonymous mmaped(in this case on Linux it will
> use /dev/shm by default)?

On Ubuntu, Squid shared memory segments should all be in /dev/shm by
default. Squid does not want them to be backed by real files. See
shm_open(3).

Please note that some libc calls manipulating regular files are
translated into mmap() calls by the standard library (or some such). Not
all mmap() calls you see in strace are Squid mmap() calls.


HTH,

Alex.


> On Mon, Jul 16, 2018 at 11:58 AM Alex Rousskov wrote:
> 
>     On 07/15/2018 08:47 PM, Gordon Hsiao wrote:
>     > Just upgraded squid to 4.1, however if I enabled
>     shared_memory_locking I
>     > failed to start squid:
>     >
>     > "FATAL: shared_memory_locking on but failed to
>     > mlock(/squid-tls_session_cache.shm, 2101212): (12) Out of memory"
> 
>     > How do I know how much memory it is trying to mlock? is 2101212(~2MB)
>     > the shm size of not,
> 
>     Yes, Squid tried to lock a 2101212-byte segment and failed.
> 
> 
>     > any way to debug/looking-into/config this size?
> 
>     I am not sure what you mean, but please keep in mind that the failed
>     segment could be the last straw -- most of the shared memory could be
>     allocated earlier. You can observe all allocations/locks with 54,7
>     debugging. Look for "mlock(".
> 
>     You can also run "strace" or a similar command line tool to track
>     allocations, but analyzing strace output may be more difficult than
>     looking through Squid logs.
> 
> 
>     > Again I disabled cache etc for a memory restricted environment, also
>     > used the minimal configuration with a few enable-flags, in the
>     meantime
>     > I want to avoid memory overcommit from squid(thus mlock)
> 
>     I am glad the new code is working to prevent runtime crashes in your
>     memory-restricted environment. If studying previous mlock() calls does
>     not help, please suggest what else Squid could do not help you.
> 
> 
>     Thank you,
> 
>     Alex.
> 



From sudakov at sibptus.tomsk.ru  Tue Jul 17 02:20:16 2018
From: sudakov at sibptus.tomsk.ru (Victor Sudakov)
Date: Tue, 17 Jul 2018 09:20:16 +0700
Subject: [squid-users] Kerberos issues on 4.1
Message-ID: <20180717022016.GA44115@admin.sibptus.transneft.ru>

Dear Colleagues,

After upgrading to Squid 4.1 (from FreeBSD ports) I started having problems
with Kerberos authentication. 

A user complained about being denied access.  The strange things are that:

1. There was only one such user, others seemed to be authenticating
properly (or just did not complain).

2. The user seemed authenticated but still was denied (!), a sample access.log entry:

1531737712.384      7 212.73.124.190 TCP_DENIED/403 9976 GET http://yandex.ru/zzzzzzzzzzzz user at REA.LM HIER_NONE/- text/html

The user tried different browsers on different hosts, with the same result.

After downgrading to Squid 3.5.27 all went well again.

Sorry I cannot provide more debugging info at present, I had to
downgrade my two production Squids ASAP.

Was there any major change between Squid 3 and 4 in the way
Negotiate/Kerberos works?

-- 
Victor Sudakov,  VAS4-RIPE, VAS47-RIPN
AS43859


From Sarfaraz.Ahmad at deshaw.com  Tue Jul 17 07:17:41 2018
From: Sarfaraz.Ahmad at deshaw.com (Ahmad, Sarfaraz)
Date: Tue, 17 Jul 2018 07:17:41 +0000
Subject: [squid-users] Cache ran out of descriptors due to ICAP service/TCP
	SYNs ?
Message-ID: <073d52e4d31040e59ffaa3ff5e4d2040@mbxtoa3.winmail.deshaw.com>

Can somebody please explain what could have happened here?

First squid(4.0.25) encountered a URL > 8K bytes. I think this caused it to crash.

Jul 13 11:04:13 <hostname> squid[9102]: parse URL too large (9697 bytes)
Jul 13 11:04:13 <hostname> squid[29254]: Squid Parent: squid-1 process 9102 exited due to signal 11 with status 0

squid-1 was respawned by the parent squid process.

Then I see ,
WARNING: ICAP Max-Connections limit exceeded for service icap://127.0.0.1:1344/reqmod. Open connections now: 16, including 0 idle persistent connections.
The newly spawned squid-1  crashes yet again. As seen below,
Jul 13 11:16:14 <hostname> squid[29254]: Squid Parent: squid-1 process 10951 exited due to signal 11 with status 0
Logs don't explain why squid-1 crashed here. ICAP message above is just a warning.

squid-1 is respawned a second time and I see,

Jul 13 11:22:18 <hostname> squid[13123]: ERROR: negotiating TLS on FD 1722: error:14090086:SSL routines:ssl3_get_server_certificate:certificate verify failed (1/-1/0)
Jul 13 11:22:18 <hostname> squid[13123]: Error negotiating SSL connection on FD 1400: (104) Connection reset by peer
Jul 13 11:23:14 <hostname> squid[13123]: Error negotiating SSL connection on FD 1046: (104) Connection reset by peer
Jul 13 11:23:14 <hostname> squid[13123]: Error negotiating SSL connection on FD 582: (104) Connection reset by peer
Jul 13 11:23:15 <hostname> squid[13123]: Error negotiating SSL connection on FD 61: (104) Connection reset by peer
Jul 13 11:23:16 <hostname> squid[13123]: Error negotiating SSL connection on FD 1150: (104) Connection reset by peer
Jul 13 11:23:18 <hostname> squid[13123]: Error negotiating SSL connection on FD 1674: (104) Connection reset by peer
Jul 13 11:23:18 <hostname> squid[13123]: Error negotiating SSL connection on FD 1519: (104) Connection reset by peer
Jul 13 11:23:18 <hostname> squid[13123]: Error negotiating SSL connection on FD 1292: (104) Connection reset by peer
Jul 13 11:23:18 <hostname> squid[13123]: Error negotiating SSL connection on FD 1631: (104) Connection reset by peer
Jul 13 11:35:17 <hostname> squid[13123]: Error negotiating SSL connection on FD 1331: (104) Connection reset by peer
Jul 13 11:35:24 <hostname> squid[13123]: WARNING! Your cache is running out of filedescriptors
Jul 13 11:35:56 <hostname> squid[13123]: Error negotiating SSL connection on FD 1867: (104) Connection reset by peer
Jul 13 11:35:58 <hostname> squid[13123]: Error negotiating SSL connection on FD 1715: (104) Connection reset by peer
Jul 13 11:35:59 <hostname> squid[13123]: suspending ICAP service for too many failures
Jul 13 11:35:59 <hostname> squid[13123]: optional ICAP service is suspended: icap://127.0.0.1:1344/reqmod [down,susp,fail11]
Jul 13 11:36:00 <hostname> squid[13123]: comm_openex socket failure: (24) Too many open files
Jul 13 11:36:00 <hostname> squid[13123]: comm_openex socket failure: (24) Too many open files
Jul 13 11:36:00 <hostname> squid[13123]: comm_openex socket failure: (24) Too many open files
Jul 13 11:36:00 <hostname> squid[13123]: comm_openex socket failure: (24) Too many open files
Jul 13 11:36:00 <hostname> squid[13123]: comm_openex socket failure: (24) Too many open files


There is only one icap service defined as below :

icap_enable on
icap_service test_icap reqmod_precache icap://127.0.0.1:1344/reqmod bypass=on routing=off on-overload=wait

The open file ulimit is set to 16k. How many TCP connections would Squid have opened up that it exhausted 16k file descriptors ?  Some sort of file descriptor leak ?
I am unable to connect the dots where an unresponsive ICAP service lead to the proxy running out of file descriptors ?  Too many TCP SYN attempts ?

When in working condition, this is what it looks like, from cachemgr,

File descriptor usage for squid:
        Maximum number of file descriptors:   16384
        Largest file desc currently in use:     58
        Number of file desc currently in use:   27
        Files queued for open:                   0
        Available number of file descriptors: 16357
        Reserved number of file descriptors:   100
        Store Disk files open:                   0

I will be installing Squid4.1 shortly but I need an explanation for what happened here. Please provide some pointers or let me know if any other information is needed to figure this out.

Regards,
Sarfaraz
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20180717/fc15c9ef/attachment.htm>

From acrow at integrafin.co.uk  Tue Jul 17 11:23:30 2018
From: acrow at integrafin.co.uk (Alex Crow)
Date: Tue, 17 Jul 2018 12:23:30 +0100
Subject: [squid-users] NgTech repo new service:
	fastest.ngtech.co.il/repo/
In-Reply-To: <!&!AAAAAAAAAAAuAAAAAAAAAL3wsAcc8JBJkMCbPuiQMGEBAMO2jhD3dRHOtM0AqgC7tuYAAAAAAA4AABAAAAATf+oDjkWUQpP1Q8qkrBgMAQAAAAA=@ngtech.co.il>
References: <!&!AAAAAAAAAAAuAAAAAAAAAL3wsAcc8JBJkMCbPuiQMGEBAMO2jhD3dRHOtM0AqgC7tuYAAAAAAA4AABAAAAATf+oDjkWUQpP1Q8qkrBgMAQAAAAA=@ngtech.co.il>
Message-ID: <73e466f4-8628-1dbe-9522-3cf65046b83a@integrafin.co.uk>

On 16/07/18 00:17, Eliezer Croitoru wrote:
>
> Hey Squid-Users,
>
> I am running a trial period to see how it works for these who needs it.
>
> The RPM?s repository is sitting at:
>
> http://fastest.ngtech.co.il/repo/
>
> and will give faster speed ie 10Mbps++ compared to the local server 
> which has only 1Mbps upload with QOS on it.
>
> Please use it will care since the service is there for you and these 
> who need it.
>
> If the service bandwidth will be abused I will take it down.
>
> Thanks,
>
> Eliezer
>

Thanks Eleizer - *much* faster - I was having problems just getting the 
metadata on the old repo.

Alex



--
This message is intended only for the addressee and may contain
confidential information. Unless you are that person, you may not
disclose its contents or use it in any way and are requested to delete
the message along with any attachments and notify us immediately.
This email is not intended to, nor should it be taken to, constitute advice.
The information provided is correct to our knowledge & belief and must not
be used as a substitute for obtaining tax, regulatory, investment, legal or
any other appropriate advice.

"Transact" is operated by Integrated Financial Arrangements Ltd.
29 Clement's Lane, London EC4N 7AE. Tel: (020) 7608 4900 Fax: (020) 7608 5300.
(Registered office: as above; Registered in England and Wales under
number: 3727592). Authorised and regulated by the Financial Conduct
Authority (entered on the Financial Services Register; no. 190856).
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20180717/c018ab9d/attachment.htm>

From squid3 at treenet.co.nz  Tue Jul 17 12:51:56 2018
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Wed, 18 Jul 2018 00:51:56 +1200
Subject: [squid-users] Cache ran out of descriptors due to ICAP
 service/TCP SYNs ?
In-Reply-To: <073d52e4d31040e59ffaa3ff5e4d2040@mbxtoa3.winmail.deshaw.com>
References: <073d52e4d31040e59ffaa3ff5e4d2040@mbxtoa3.winmail.deshaw.com>
Message-ID: <2243eaa3-2d71-3696-2b6b-d82d238bf309@treenet.co.nz>

On 17/07/18 19:17, Ahmad, Sarfaraz wrote:
> Can somebody please explain what could have happened here?
> 
> ?
> 
> First squid(4.0.25) encountered a URL > 8K bytes. I think this caused it
> to crash.
> 

Unless you patched the MAX_URL definition to be larger than default,
that should not happen. So is a bug IMO.

If you did patch MAX_URL, then you have encountered one of the many
hidden issues why we keep it low and
<https://bugs.squid-cache.org/show_bug.cgi?id=4422> open. Any assistance
finding out where that crash occurs is VERY welcome.


> ?
> 
> Jul 13 11:04:13 <hostname> squid[9102]: parse URL too large (9697 bytes)
> 
> Jul 13 11:04:13 <hostname> squid[29254]: Squid Parent: squid-1 process
> 9102 exited due to signal 11 with status 0
> 
> ?
> 
> squid-1 was respawned by the parent squid process.
> 
> ?
> 
> Then I see ,
> 
> WARNING: ICAP Max-Connections limit exceeded for service
> icap://127.0.0.1:1344/reqmod. Open connections now: 16, including 0 idle
> persistent connections.
> 
> The newly spawned squid-1 ?crashes yet again. As seen below,
> 
> Jul 13 11:16:14 <hostname> squid[29254]: Squid Parent: squid-1 process
> 10951 exited due to signal 11 with status 0
> 
> Logs don?t explain why squid-1 crashed here. ICAP message above is just
> a warning.

In normal operation it is not serious, but you are already into abnormal
operation by the crashing. So not releasing sockets/FD fast enough makes
the overall problem worse.

>From the below log and config that this ICAP service is *optional*
(bypass=on). So Squid is free to ignore its use entirely if FD run out.
That is probably why it is only listed as WARNING. But is still
consuming FDs before it gets to that state.


> 
> squid-1 is respawned a second time and I see,
> 
> ?
> 
> Jul 13 11:22:18 <hostname> squid[13123]: ERROR: negotiating TLS on FD
> 1722: error:14090086:SSL
> routines:ssl3_get_server_certificate:certificate verify failed (1/-1/0)
> 

Look into why. This along with your crash are the top two issues adding
to the overall situation.
> ?
> 
> There is only one icap service defined as below :
> 
> ?
> 
> icap_enable on
> 
> icap_service test_icap reqmod_precache icap://127.0.0.1:1344/reqmod
> bypass=on routing=off on-overload=wait
> 


> ?
> 
> The open file ulimit is set to 16k. How many TCP connections would Squid
> have opened up that it exhausted 16k file descriptors ? ?Some sort of
> file descriptor leak ?

Only if your traffic is high enough to leak that fast.

More likely is a forwarding loop situation. Where one outbound server
connection consumes _infinite_ FD sockets.


> 
> I am unable to connect the dots where an unresponsive ICAP service lead
> to the proxy running out of file descriptors ? ?Too many TCP SYN attempts ?
> 

That two are probably unrelated. Unless it is the ICAP socket being
looped back to Squid, or your traffic req/sec is extremely high and one
of a few ICAP connection bugs occuring (eg lack of a way to cleanly
signal connection error to ICAP).


Amos


From squid3 at treenet.co.nz  Tue Jul 17 13:02:48 2018
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Wed, 18 Jul 2018 01:02:48 +1200
Subject: [squid-users] Kerberos issues on 4.1
In-Reply-To: <20180717022016.GA44115@admin.sibptus.transneft.ru>
References: <20180717022016.GA44115@admin.sibptus.transneft.ru>
Message-ID: <0c235a10-024b-48b6-fb0f-b6eeb87fe4e4@treenet.co.nz>

On 17/07/18 14:20, Victor Sudakov wrote:
> Dear Colleagues,
> 
> After upgrading to Squid 4.1 (from FreeBSD ports) I started having problems
> with Kerberos authentication. 
> 
> A user complained about being denied access.  The strange things are that:
> 
> 1. There was only one such user, others seemed to be authenticating
> properly (or just did not complain).
> 
> 2. The user seemed authenticated but still was denied (!), a sample access.log entry:
> 
> 1531737712.384      7 212.73.124.190 TCP_DENIED/403 9976 GET http://yandex.ru/zzzzzzzzzzzz user at REA.LM HIER_NONE/- text/html
> 
> The user tried different browsers on different hosts, with the same result.
> 
> After downgrading to Squid 3.5.27 all went well again.
> 
> Sorry I cannot provide more debugging info at present, I had to
> downgrade my two production Squids ASAP.
> 
> Was there any major change between Squid 3 and 4 in the way
> Negotiate/Kerberos works?
> 

The biggest change is that bundled Kerberos auth helpers are now using
the newer v3.4+ helper protocol. That prevents some malformations of
Unicode and whitespace characters in the username or password which
Squid-3 might have been ignoring when it should have rejected.

You may need to check both what you have on record in your AD/LDAP and
what the affected user thinks they need to enter.

There is also the less likely possibility that other non-auth ACLs are
rejecting the request for completely unrelated reasons.


For completeness; there are some other changes, but those seem
irrelevant to your case.

Amos


From squid3 at treenet.co.nz  Tue Jul 17 13:41:40 2018
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Wed, 18 Jul 2018 01:41:40 +1200
Subject: [squid-users] Squid v4.1: commBind Cannot bind [::1] on SNMP
 with no ipv6
In-Reply-To: <048701d41c86$bf42d2d0$3dc87870$@articatech.com>
References: <043801d41bcc$02b7df30$08279d90$@articatech.com>
 <09f94433-9b61-6644-7096-b8306b95ee10@treenet.co.nz>
 <048701d41c86$bf42d2d0$3dc87870$@articatech.com>
Message-ID: <5a90233f-4f75-3c1b-099d-3053bf863526@treenet.co.nz>


On 16/07/18 09:56, David Touzeau wrote:
> We have this one too
> 
> 2018/07/15 23:38:11 kid2| Accepting SNMP messages on 0.0.0.0:3401
> 2018/07/15 23:45:02 kid2| snmpHandleUdp: FD 23 recvfrom: (11) Resource temporarily unavailable

That FD 23 is from the above opened socket, not the unsuccessful ::1 one.

Do you have EDNS enabled on your network? If so you should be able to
rebuilds with SNMP_REQUEST_SIZE somewhat larger to avoid these. It
should be twice what your largest expected packet size is. Our default
is 4KB for Ethernet 1.5KB UDP packets.

Amos


From capcoding at gmail.com  Tue Jul 17 14:37:14 2018
From: capcoding at gmail.com (Gordon Hsiao)
Date: Tue, 17 Jul 2018 09:37:14 -0500
Subject: [squid-users] shared_memory_locking failed to mlock
In-Reply-To: <e1357a97-3f47-1064-1f0f-9c249e38f81d@measurement-factory.com>
References: <CAK0iFYxbnJbGcLY_=vsswdbaRq1jYpkKcEY3Pjf-_EjChnFrtw@mail.gmail.com>
 <f4ce5954-65ad-3a24-0a83-8925a7add7b2@measurement-factory.com>
 <CAK0iFYxqi3Q1yZqSFbG0RG3dbwwVsuxAEKNZ0jEHnh4Yy3DGwg@mail.gmail.com>
 <e1357a97-3f47-1064-1f0f-9c249e38f81d@measurement-factory.com>
Message-ID: <CAK0iFYwznGB+3WSWPR0nYHVBr4Z=JYfHM5eDD_QJvKk8gG_7=A@mail.gmail.com>

Turns out it is a ulimit-related issue, I bumped default mlock limit to a
large value and now I can start squid with memory-locked.

yes strace is only for syscalls, while ltrace shows all library calls.

Thanks for the help!

Gordon

On Mon, Jul 16, 2018 at 6:38 PM Alex Rousskov <
rousskov at measurement-factory.com> wrote:

> On 07/16/2018 05:08 PM, Gordon Hsiao wrote:
> > On a x86/64bit ubuntu machine if I set 'workers 4' and run:
>
> > squid --foreground -f /etc/squid.conf 2>&1 |grep mlock
> >   mlock(0x7f2e5bfb2000, 8)                = 0
> >   mlock(0x7f2e5bf9f000, 73912)            = -1 ENOMEM
>
> > squid -N -f /etc/squid.conf 2>& |grep mlock
> >   mlock(0x7f8e4b7c0000, 8)                = 0
> >   mlock(0x7f8e4b7ad000, 73912)            = -1 ENOMEM
>
> > Note 1; -N and --foreground made no difference as long as 'workers 4' is
> > set, I was expecting -N will ignore "worker 4", does it?
>
> IIRC, -N does not start workers. However, some (memory allocation) code
> may not honor -N and still allocate memory necessary for those (disabled
> by -N) workers. That would be a bug AFAICT.
>
>
> > Now I set 'workers 2' and run the same two commands above and I got the
> > output(both are the same), which means squid started successfully:
> >   mlock(0x7f0c441cc000, 8)                = 0
> >   mlock(0x7f0c441c3000, 32852)            = 0
> >   mlock(0x7f0c441c2000, 52)               = 0
>
> The second allocation is probably smaller because two workers need fewer
> SMP queues (or similar shared memory resources) than four workers.
>
>
> > I have more than 4GB RAM free(this is a 8GB RAM laptop) and this
> > is a Intel i7, the mlock failure is strange.
>
> The default amount of shared memory available to a program is often much
> smaller than the total amount of RAM. I do not recall which Ubuntu
> commands or sysctl settings control the former, but Squid wiki or other
> web resources should have that info. The question you should ask
> yourself is "How much shared memory is available for the Squid process"?
>
>
> > On my target system which has 512MB RAM, even 'workers 0' won't help, I
> > still get :
> >
> >   mlock(0x778de000, 2101212)              = -1 ENOMEM (Out of memory)
>
> For "workers 0" concerns, please see the -N discussion above. The two
> should be equivalent.
>
>
> > I have to disable lock-memory for now and it puzzles me why the very
> > first 2MB mlock can fail.
>
> Most likely, your OS is configured (or defaults) to provide very little
> shared memory to a process when the total RAM is only 512MB.
>
>
> > I strace|grep shm_get and shmat and found nothing,
>
> mlock() is a system call so strace should see it, but it may be called
> something else.
>
>
> > instead there are lots of mmap calls, so Squid is using mmap
> > for its shared memory mapping,
>
> Squid creates segments using shm_open() and attaches to them using mmap().
>
>
> > the only question is that, is this mlock
> > file-backed-up or is it anonymous mmaped(in this case on Linux it will
> > use /dev/shm by default)?
>
> On Ubuntu, Squid shared memory segments should all be in /dev/shm by
> default. Squid does not want them to be backed by real files. See
> shm_open(3).
>
> Please note that some libc calls manipulating regular files are
> translated into mmap() calls by the standard library (or some such). Not
> all mmap() calls you see in strace are Squid mmap() calls.
>
>
> HTH,
>
> Alex.
>
>
> > On Mon, Jul 16, 2018 at 11:58 AM Alex Rousskov wrote:
> >
> >     On 07/15/2018 08:47 PM, Gordon Hsiao wrote:
> >     > Just upgraded squid to 4.1, however if I enabled
> >     shared_memory_locking I
> >     > failed to start squid:
> >     >
> >     > "FATAL: shared_memory_locking on but failed to
> >     > mlock(/squid-tls_session_cache.shm, 2101212): (12) Out of memory"
> >
> >     > How do I know how much memory it is trying to mlock? is
> 2101212(~2MB)
> >     > the shm size of not,
> >
> >     Yes, Squid tried to lock a 2101212-byte segment and failed.
> >
> >
> >     > any way to debug/looking-into/config this size?
> >
> >     I am not sure what you mean, but please keep in mind that the failed
> >     segment could be the last straw -- most of the shared memory could be
> >     allocated earlier. You can observe all allocations/locks with 54,7
> >     debugging. Look for "mlock(".
> >
> >     You can also run "strace" or a similar command line tool to track
> >     allocations, but analyzing strace output may be more difficult than
> >     looking through Squid logs.
> >
> >
> >     > Again I disabled cache etc for a memory restricted environment,
> also
> >     > used the minimal configuration with a few enable-flags, in the
> >     meantime
> >     > I want to avoid memory overcommit from squid(thus mlock)
> >
> >     I am glad the new code is working to prevent runtime crashes in your
> >     memory-restricted environment. If studying previous mlock() calls
> does
> >     not help, please suggest what else Squid could do not help you.
> >
> >
> >     Thank you,
> >
> >     Alex.
> >
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20180717/b24e62f2/attachment.htm>

From squid3 at treenet.co.nz  Tue Jul 17 15:08:14 2018
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Wed, 18 Jul 2018 03:08:14 +1200
Subject: [squid-users] ERROR: Unknown TLS option clientca
In-Reply-To: <CAJbsrUnfY2z0H9Q+ebqwGBGU-QQooRcVyW-0h9Qtopw1Gr9X3A@mail.gmail.com>
References: <CAJbsrUkfukNyFsP3wBKu_1Sk+0SYaGa8H1JwKk41o=A197ymoA@mail.gmail.com>
 <CAJbsrU=5hQeM9kAuibggPFSLSeQ3W4wmPst4rqSkC5E5EDZoTw@mail.gmail.com>
 <08764746-faf1-0aee-56ef-7accb2841dd3@measurement-factory.com>
 <CAJbsrUnb+g59M900OeWzyNfq9KLCGP1FQs=Q6zTpxxmcn5b4VA@mail.gmail.com>
 <38f31e74-ec64-e4a9-09c4-3df37fe270de@measurement-factory.com>
 <CAJbsrUnfY2z0H9Q+ebqwGBGU-QQooRcVyW-0h9Qtopw1Gr9X3A@mail.gmail.com>
Message-ID: <273039d9-83c4-03c3-1caa-6bd9712e4b37@treenet.co.nz>

On 14/07/18 06:32, login mogin wrote:
> Thanks for the help. Now I am not getting any error messages but as you
> said I will follow the pull request.
> 

I've now managed to add what I think is the final bit of the fix to that
PR. Do you now see it fully working?

Amos


From chip_pop at hotmail.com  Tue Jul 17 17:22:05 2018
From: chip_pop at hotmail.com (joseph)
Date: Tue, 17 Jul 2018 12:22:05 -0500 (CDT)
Subject: [squid-users] new ecap gzip + deflat adapter
Message-ID: <1531848125192-0.post@n4.nabble.com>

https://github.com/yvoinov/squid-ecap-gzip

This Software is an eCAP adapter for HTTP compression with GZIP and DEFLATE.
It is fully re-worked, bugfixed and improved version, ready for production
use, based on Constantin Rack's https://github.com/c-rack/squid-ecap-gzip
adapter 
full source



-----
************************** 
***** Crash to the future  ****
**************************
--
Sent from: http://squid-web-proxy-cache.1019090.n4.nabble.com/Squid-Users-f1019091.html


From loginmogin at gmail.com  Tue Jul 17 19:03:40 2018
From: loginmogin at gmail.com (login mogin)
Date: Tue, 17 Jul 2018 12:03:40 -0700
Subject: [squid-users] ERROR: Unknown TLS option clientca
In-Reply-To: <273039d9-83c4-03c3-1caa-6bd9712e4b37@treenet.co.nz>
References: <CAJbsrUkfukNyFsP3wBKu_1Sk+0SYaGa8H1JwKk41o=A197ymoA@mail.gmail.com>
 <CAJbsrU=5hQeM9kAuibggPFSLSeQ3W4wmPst4rqSkC5E5EDZoTw@mail.gmail.com>
 <08764746-faf1-0aee-56ef-7accb2841dd3@measurement-factory.com>
 <CAJbsrUnb+g59M900OeWzyNfq9KLCGP1FQs=Q6zTpxxmcn5b4VA@mail.gmail.com>
 <38f31e74-ec64-e4a9-09c4-3df37fe270de@measurement-factory.com>
 <CAJbsrUnfY2z0H9Q+ebqwGBGU-QQooRcVyW-0h9Qtopw1Gr9X3A@mail.gmail.com>
 <273039d9-83c4-03c3-1caa-6bd9712e4b37@treenet.co.nz>
Message-ID: <CAJbsrU=nvZcZKS5tNyrxm1PTUTsRsnYjGmjUSx-uH2GX4keYLw@mail.gmail.com>

I'll give it a try today and let you know. Thanks a lot.

Logan

Amos Jeffries <squid3 at treenet.co.nz>, 17 Tem 2018 Sal, 08:08 tarihinde ?unu
yazd?:

> On 14/07/18 06:32, login mogin wrote:
> > Thanks for the help. Now I am not getting any error messages but as you
> > said I will follow the pull request.
> >
>
> I've now managed to add what I think is the final bit of the fix to that
> PR. Do you now see it fully working?
>
> Amos
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20180717/8a6f5f02/attachment.htm>

From Ralf.Hildebrandt at charite.de  Tue Jul 17 21:09:14 2018
From: Ralf.Hildebrandt at charite.de (Ralf Hildebrandt)
Date: Tue, 17 Jul 2018 23:09:14 +0200
Subject: [squid-users] [ext]  new ecap gzip + deflat adapter
In-Reply-To: <1531848125192-0.post@n4.nabble.com>
References: <1531848125192-0.post@n4.nabble.com>
Message-ID: <20180717210914.GD9855@charite.de>

* joseph <chip_pop at hotmail.com>:
> https://github.com/yvoinov/squid-ecap-gzip

URL returns 404!

-- 
Ralf Hildebrandt                   Charite Universit?tsmedizin Berlin
ralf.hildebrandt at charite.de        Campus Benjamin Franklin
https://www.charite.de             Hindenburgdamm 30, 12203 Berlin
Gesch?ftsbereich IT, Abt. Netzwerk fon: +49-30-450.570.155


From chip_pop at hotmail.com  Tue Jul 17 21:30:56 2018
From: chip_pop at hotmail.com (joseph)
Date: Tue, 17 Jul 2018 16:30:56 -0500 (CDT)
Subject: [squid-users] [ext]  new ecap gzip + deflat adapter
In-Reply-To: <20180717210914.GD9855@charite.de>
References: <1531848125192-0.post@n4.nabble.com>
 <20180717210914.GD9855@charite.de>
Message-ID: <1531863056732-0.post@n4.nabble.com>

Ralf Hildebrandt wrote
> * joseph &lt;

> chip_pop@

> &gt;:
>> https://github.com/yvoinov/squid-ecap-gzip
> 
>>>>URL returns 404!
> right  it will be posted again sorry for that  some small change require
> to be done
> i will announce wen its posted thanks
> -- 
> Ralf Hildebrandt                   Charite Universit?tsmedizin Berlin

> ralf.hildebrandt@

>         Campus Benjamin Franklin
> https://www.charite.de             Hindenburgdamm 30, 12203 Berlin
> Gesch?ftsbereich IT, Abt. Netzwerk fon: +49-30-450.570.155
> _______________________________________________
> squid-users mailing list

> squid-users at .squid-cache

> http://lists.squid-cache.org/listinfo/squid-users





-----
************************** 
***** Crash to the future  ****
**************************
--
Sent from: http://squid-web-proxy-cache.1019090.n4.nabble.com/Squid-Users-f1019091.html


From Sarfaraz.Ahmad at deshaw.com  Wed Jul 18 06:30:46 2018
From: Sarfaraz.Ahmad at deshaw.com (Ahmad, Sarfaraz)
Date: Wed, 18 Jul 2018 06:30:46 +0000
Subject: [squid-users] Cache ran out of descriptors due to ICAP
 service/TCP SYNs ?
In-Reply-To: <2243eaa3-2d71-3696-2b6b-d82d238bf309@treenet.co.nz>
References: <073d52e4d31040e59ffaa3ff5e4d2040@mbxtoa3.winmail.deshaw.com>
 <2243eaa3-2d71-3696-2b6b-d82d238bf309@treenet.co.nz>
Message-ID: <32fb6ea566ba460db61a184cdd89888f@mbxtoa3.winmail.deshaw.com>

Thanks for the reply. I haven't completely understood the revert and have a few more related questions.

I see these messages, 
Jul 17 19:21:14 proxy2.hyd.deshaw.com squid[5747]: suspending ICAP service for too many failures
Jul 17 19:21:14 proxy2.hyd.deshaw.com squid[5747]: optional ICAP service is suspended: icap://127.0.0.1:1344/reqmod [down,susp,fail11]
1)   If the ICAP service is unresponsive, Squid would not exhaust its file descriptors trying to reach the service again and again right (too many TCP SYNs for trying to connect to the ICAP service )? 



Max Connections returned by the ICAP service is 16. And given my ICAP settings, 
icap_enable on
icap_service test_icap reqmod_precache icap://127.0.0.1:1344/reqmod bypass=on routing=off on-overload=wait
On-overload is set to "wait". The documentation says " * wait:   wait (in a FIFO queue) for an ICAP connection slot" . This means that a new TCP connection would not be attempted if max connections is reached right ? 
2)   Am I right in saying that if the ICAP service is underperforming or has failed, this won't lead a sudden increase in the open file descriptors with on-overload set to "wait" ?


Also I have no way to explain the "connection reset by peer" messages.
Jul 13 11:23:18 <hostname> squid[13123]: Error negotiating SSL connection on FD 1292: (104) Connection reset by peer
Jul 13 11:23:18 <hostname> squid[13123]: Error negotiating SSL connection on FD 1631: (104) Connection reset by peer
Jul 13 11:35:17 <hostname> squid[13123]: Error negotiating SSL connection on FD 1331: (104) Connection reset by peer

I have a few proxies (running in separate virtual machines). All of them went unresponsive at around the same time, leading to an outage of the internet.
I am using WCCPv2 to redirect from firewall to these proxies.  I checked the logs there and WCCP communication was not intermittent.
The logs on the proxies are bombarded with " Error negotiating SSL connection on FD 1331: (104) Connection reset by peer " messages. 
Since the ICAP service in not SSL-protected I think these messages mostly imply receiving TCP RSTs from remote servers. (or could it be clients somehow ?). Once I removed WCCP direction rules from the firewall, internet was back up.
This hints that something in this proxy pipeline was amiss and not with the internet link itself. I don't see any outages on that. 
I am pretty sure ACLs weren't changed and there was no forwarding loop.
What could possibly explain the connection reset by peer messages ? Even if the internet was down, that won't lead to TCP RSTs. 
I cannot tie these TCP RSTs and the incoming requests getting held up and ultimately leading to FD exhaustion.

You earlier said 
>> In normal operation it is not serious, but you are already into abnormal operation by the crashing. So not releasing sockets/FD fast enough makes the overall problem worse.
If squid-1 is crashing and getting respawned, it will have its own 16K FD limit right, I wonder how the newer squid-1 serves older requests. Can you please elaborate on " So not releasing sockets/FD fast enough makes the overall problem worse." ?

Please share your thoughts.

Regards,
Sarfaraz


-----Original Message-----
From: squid-users <squid-users-bounces at lists.squid-cache.org> On Behalf Of Amos Jeffries
Sent: Tuesday, July 17, 2018 6:22 PM
To: squid-users at lists.squid-cache.org
Subject: Re: [squid-users] Cache ran out of descriptors due to ICAP service/TCP SYNs ?

On 17/07/18 19:17, Ahmad, Sarfaraz wrote:
> Can somebody please explain what could have happened here?
> 
> ?
> 
> First squid(4.0.25) encountered a URL > 8K bytes. I think this caused 
> it to crash.
> 

Unless you patched the MAX_URL definition to be larger than default, that should not happen. So is a bug IMO.

If you did patch MAX_URL, then you have encountered one of the many hidden issues why we keep it low and <https://bugs.squid-cache.org/show_bug.cgi?id=4422> open. Any assistance finding out where that crash occurs is VERY welcome.


> ?
> 
> Jul 13 11:04:13 <hostname> squid[9102]: parse URL too large (9697 
> bytes)
> 
> Jul 13 11:04:13 <hostname> squid[29254]: Squid Parent: squid-1 process
> 9102 exited due to signal 11 with status 0
> 
> ?
> 
> squid-1 was respawned by the parent squid process.
> 
> ?
> 
> Then I see ,
> 
> WARNING: ICAP Max-Connections limit exceeded for service 
> icap://127.0.0.1:1344/reqmod. Open connections now: 16, including 0 
> idle persistent connections.
> 
> The newly spawned squid-1 ?crashes yet again. As seen below,
> 
> Jul 13 11:16:14 <hostname> squid[29254]: Squid Parent: squid-1 process
> 10951 exited due to signal 11 with status 0
> 
> Logs don?t explain why squid-1 crashed here. ICAP message above is 
> just a warning.

In normal operation it is not serious, but you are already into abnormal operation by the crashing. So not releasing sockets/FD fast enough makes the overall problem worse.

From the below log and config that this ICAP service is *optional* (bypass=on). So Squid is free to ignore its use entirely if FD run out.
That is probably why it is only listed as WARNING. But is still consuming FDs before it gets to that state.


> 
> squid-1 is respawned a second time and I see,
> 
> ?
> 
> Jul 13 11:22:18 <hostname> squid[13123]: ERROR: negotiating TLS on FD
> 1722: error:14090086:SSL
> routines:ssl3_get_server_certificate:certificate verify failed 
> (1/-1/0)
> 

Look into why. This along with your crash are the top two issues adding to the overall situation.
> ?
> 
> There is only one icap service defined as below :
> 
> ?
> 
> icap_enable on
> 
> icap_service test_icap reqmod_precache icap://127.0.0.1:1344/reqmod 
> bypass=on routing=off on-overload=wait
> 


> ?
> 
> The open file ulimit is set to 16k. How many TCP connections would 
> Squid have opened up that it exhausted 16k file descriptors ? ?Some 
> sort of file descriptor leak ?

Only if your traffic is high enough to leak that fast.

More likely is a forwarding loop situation. Where one outbound server connection consumes _infinite_ FD sockets.


> 
> I am unable to connect the dots where an unresponsive ICAP service 
> lead to the proxy running out of file descriptors ? ?Too many TCP SYN attempts ?
> 

That two are probably unrelated. Unless it is the ICAP socket being looped back to Squid, or your traffic req/sec is extremely high and one of a few ICAP connection bugs occuring (eg lack of a way to cleanly signal connection error to ICAP).


Amos
_______________________________________________
squid-users mailing list
squid-users at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-users

From sudakov at sibptus.tomsk.ru  Wed Jul 18 07:16:57 2018
From: sudakov at sibptus.tomsk.ru (Victor Sudakov)
Date: Wed, 18 Jul 2018 14:16:57 +0700
Subject: [squid-users] Kerberos issues on 4.1
In-Reply-To: <0c235a10-024b-48b6-fb0f-b6eeb87fe4e4@treenet.co.nz>
References: <20180717022016.GA44115@admin.sibptus.transneft.ru>
 <0c235a10-024b-48b6-fb0f-b6eeb87fe4e4@treenet.co.nz>
Message-ID: <20180718071656.GA7092@admin.sibptus.transneft.ru>

Amos Jeffries wrote:
> On 17/07/18 14:20, Victor Sudakov wrote:
> > 
> > After upgrading to Squid 4.1 (from FreeBSD ports) I started having problems
> > with Kerberos authentication. 
> > 
> > A user complained about being denied access.  The strange things are that:
> > 
> > 1. There was only one such user, others seemed to be authenticating
> > properly (or just did not complain).
> > 
> > 2. The user seemed authenticated but still was denied (!), a sample access.log entry:
> > 
> > 1531737712.384      7 212.73.124.190 TCP_DENIED/403 9976 GET http://yandex.ru/zzzzzzzzzzzz user at REA.LM HIER_NONE/- text/html
> > 
> > The user tried different browsers on different hosts, with the same result.
> > 
> > After downgrading to Squid 3.5.27 all went well again.
> > 
> > Sorry I cannot provide more debugging info at present, I had to
> > downgrade my two production Squids ASAP.
> > 
> > Was there any major change between Squid 3 and 4 in the way
> > Negotiate/Kerberos works?
> > 
> 
> The biggest change is that bundled Kerberos auth helpers are now using
> the newer v3.4+ helper protocol. That prevents some malformations of
> Unicode and whitespace characters in the username or password which
> Squid-3 might have been ignoring when it should have rejected.
> 
> You may need to check both what you have on record in your AD/LDAP and
> what the affected user thinks they need to enter.

If the access.log line (like the one above) contained "user at REA.LM"
where the username and realm name are both correct and match those in
the user's AD ticket, doesn't it mean that the Kerberos authentication
has been successful ?

But for some reason this user was being TCP_DENIED though he was mentioned
in the "vip_users.txt" file.

acl vip_users proxy_auth_regex -i "/usr/home/sudakov/squid/vip_users.txt"
http_access allow sibptus vip_users

Why was he receiving a HTTP 403 I wonder? 403 is
authorization-related, isn't it ? The username and realm were correct
but still a 403.

I can reveal both log lines with the real username and realm, in a
private mail, if this can help. I can even provide a tcpdump.

> 
> There is also the less likely possibility that other non-auth ACLs are
> rejecting the request for completely unrelated reasons.
> 

Hmm. Looks quite the other way around, it looked like this user was
being rejected for some non-auth reason.

-- 
Victor Sudakov,  VAS4-RIPE, VAS47-RIPN
AS43859


From flashdown at data-core.org  Wed Jul 18 07:23:06 2018
From: flashdown at data-core.org (Enrico Heine)
Date: Wed, 18 Jul 2018 09:23:06 +0200
Subject: [squid-users] NgTech repo new service:
	fastest.ngtech.co.il/repo/
In-Reply-To: <!&!AAAAAAAAAAAuAAAAAAAAAL3wsAcc8JBJkMCbPuiQMGEBAMO2jhD3dRHOtM0AqgC7tuYAAAAAAA4AABAAAAATf+oDjkWUQpP1Q8qkrBgMAQAAAAA=@ngtech.co.il>
References: <!&!AAAAAAAAAAAuAAAAAAAAAL3wsAcc8JBJkMCbPuiQMGEBAMO2jhD3dRHOtM0AqgC7tuYAAAAAAA4AABAAAAATf+oDjkWUQpP1Q8qkrBgMAQAAAAA=@ngtech.co.il>
Message-ID: <7B5BD9CB-2D53-42D2-9CD2-B17E543EF7B2@data-core.org>

Thank you Eliezer, 

as soon as you need more bandwith or space, pls let me know, I have enough ressources avail for things that matter. Of course without financial interest. 

Am 16. Juli 2018 01:17:13 MESZ schrieb Eliezer Croitoru <eliezer at ngtech.co.il>:
>Hey Squid-Users,
>
> 
>
>I am running a trial period to see how it works for these who needs it.
>
>The RPM's repository is sitting at:
>
>http://fastest.ngtech.co.il/repo/
>
> 
>
>and will give faster speed ie 10Mbps++ compared to the local server
>which
>has only 1Mbps upload with QOS on it.
>
>Please use it will care since the service is there for you and these
>who
>need it.
>
>If the service bandwidth will be abused I will take it down.
>
> 
>
>Thanks,
>
>Eliezer
>
> 
>
>----
>
>Eliezer Croitoru <http://ngtech.co.il/lmgtfy/> 
>Linux System Administrator
>Mobile: +972-5-28704261
>Email: eliezer at ngtech.co.il
>
>
>
> 

-- 
Diese Nachricht wurde von meinem Android-Ger?t mit K-9 Mail gesendet.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20180718/9e3c26cb/attachment.htm>

From loginmogin at gmail.com  Wed Jul 18 11:54:20 2018
From: loginmogin at gmail.com (login mogin)
Date: Wed, 18 Jul 2018 04:54:20 -0700
Subject: [squid-users] ERROR: Unknown TLS option clientca
In-Reply-To: <CAJbsrU=nvZcZKS5tNyrxm1PTUTsRsnYjGmjUSx-uH2GX4keYLw@mail.gmail.com>
References: <CAJbsrUkfukNyFsP3wBKu_1Sk+0SYaGa8H1JwKk41o=A197ymoA@mail.gmail.com>
 <CAJbsrU=5hQeM9kAuibggPFSLSeQ3W4wmPst4rqSkC5E5EDZoTw@mail.gmail.com>
 <08764746-faf1-0aee-56ef-7accb2841dd3@measurement-factory.com>
 <CAJbsrUnb+g59M900OeWzyNfq9KLCGP1FQs=Q6zTpxxmcn5b4VA@mail.gmail.com>
 <38f31e74-ec64-e4a9-09c4-3df37fe270de@measurement-factory.com>
 <CAJbsrUnfY2z0H9Q+ebqwGBGU-QQooRcVyW-0h9Qtopw1Gr9X3A@mail.gmail.com>
 <273039d9-83c4-03c3-1caa-6bd9712e4b37@treenet.co.nz>
 <CAJbsrU=nvZcZKS5tNyrxm1PTUTsRsnYjGmjUSx-uH2GX4keYLw@mail.gmail.com>
Message-ID: <CAJbsrUm78T7fjsed8mygy7-VMa1-hsyrXAs-TUyfwKNYCKoDhg@mail.gmail.com>

Hi there,

I have just tried with the patch and it is still not working. Do you want
any particular log or debug output?

Thanks
Logan

login mogin <loginmogin at gmail.com>, 17 Tem 2018 Sal, 12:03 tarihinde ?unu
yazd?:

> I'll give it a try today and let you know. Thanks a lot.
>
> Logan
>
> Amos Jeffries <squid3 at treenet.co.nz>, 17 Tem 2018 Sal, 08:08 tarihinde
> ?unu yazd?:
>
>> On 14/07/18 06:32, login mogin wrote:
>> > Thanks for the help. Now I am not getting any error messages but as you
>> > said I will follow the pull request.
>> >
>>
>> I've now managed to add what I think is the final bit of the fix to that
>> PR. Do you now see it fully working?
>>
>> Amos
>> _______________________________________________
>> squid-users mailing list
>> squid-users at lists.squid-cache.org
>> http://lists.squid-cache.org/listinfo/squid-users
>>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20180718/4e476624/attachment.htm>

From eliezer at ngtech.co.il  Wed Jul 18 12:18:47 2018
From: eliezer at ngtech.co.il (Eliezer Croitoru)
Date: Wed, 18 Jul 2018 15:18:47 +0300
Subject: [squid-users] NgTech repo new service:
	fastest.ngtech.co.il/repo/
In-Reply-To: <7B5BD9CB-2D53-42D2-9CD2-B17E543EF7B2@data-core.org>
References: <!&!AAAAAAAAAAAuAAAAAAAAAL3wsAcc8JBJkMCbPuiQMGEBAMO2jhD3dRHOtM0AqgC7tuYAAAAAAA4AABAAAAATf+oDjkWUQpP1Q8qkrBgMAQAAAAA=@ngtech.co.il>
 <7B5BD9CB-2D53-42D2-9CD2-B17E543EF7B2@data-core.org>
Message-ID: <01cb01d41e91$7adafc50$7090f4f0$@ngtech.co.il>

Thanks for the offer.

 

I have seen that there  are couple users that uses lftp to mirror the repository.

If someone want?s to add a mirror site it would be nice but it needs to be registered somewhere?

I think that the wiki should be the main place which sysadmins can have a list of mirrors since it?s not the project service but the community.

I published a mirror script at:

http://gogs.ngtech.co.il/NgTech-LTD/ngtech-repo-mirror

 

The only restriction for now is to run it only twice a month since I do not update it more frequent the this.

Also notice that my repo have couple extra packages.

My mirror script is not erasing files that doesn?t exist ie removed from the repo so anyone that will run the mirror script should take it into account.

 

All The Bests,

Eliezer

 

?         I might add a rsyncd to make mirroring far easy but for now lftp is good enough.

 

----

 <http://ngtech.co.il/lmgtfy/> Eliezer Croitoru
Linux System Administrator
Mobile: +972-5-28704261
Email: eliezer at ngtech.co.il



 

From: Enrico Heine [mailto:flashdown at data-core.org] 
Sent: Wednesday, July 18, 2018 10:23 AM
To: squid-users at lists.squid-cache.org; Eliezer Croitoru <eliezer at ngtech.co.il>
Subject: Re: [squid-users] NgTech repo new service: fastest.ngtech.co.il/repo/

 

Thank you Eliezer, 

as soon as you need more bandwith or space, pls let me know, I have enough ressources avail for things that matter. Of course without financial interest. 

Am 16. Juli 2018 01:17:13 MESZ schrieb Eliezer Croitoru <eliezer at ngtech.co.il <mailto:eliezer at ngtech.co.il> >:

Hey Squid-Users,

 

I am running a trial period to see how it works for these who needs it.

The RPM?s repository is sitting at:

http://fastest.ngtech.co.il/repo/

 

and will give faster speed ie 10Mbps++ compared to the local server which has only 1Mbps upload with QOS on it.

Please use it will care since the service is there for you and these who need it.

If the service bandwidth will be abused I will take it down.

 

Thanks,

Eliezer

 

----

Eliezer Croitoru <http://ngtech.co.il/lmgtfy/> 
Linux System Administrator
Mobile: +972-5-28704261
Email: eliezer at ngtech.co.il <mailto:eliezer at ngtech.co.il> 



 


-- 
Diese Nachricht wurde von meinem Android-Ger?t mit K-9 Mail gesendet.

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20180718/5cf3a442/attachment.htm>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: image002.png
Type: image/png
Size: 11308 bytes
Desc: not available
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20180718/5cf3a442/attachment.png>

From eliezer at ngtech.co.il  Wed Jul 18 12:25:10 2018
From: eliezer at ngtech.co.il (Eliezer Croitoru)
Date: Wed, 18 Jul 2018 15:25:10 +0300
Subject: [squid-users] TCP_MISS_ABORTED/000 when accessing
	squid-internal-mgr page
Message-ID: <01e801d41e92$5f270c00$1d752400$@ngtech.co.il>

I have tried to access squid manage pages using curl and squidclient and got
the next weird results in the access.log.

 

TCP_MISS_ABORTED/000

 

The weird thing is that I am receiving 200 as a response:

 

 

 

Commands and logs:

### START

[root at squid4-testing check-systemd-squid]# curl
127.0.0.1:3128/squid-internal-mgr/info

Squid Object Cache: Version 4.1

Build Info:

Service Name: squid

Start Time:     Wed, 18 Jul 2018 11:45:21 GMT

Current Time:   Wed, 18 Jul 2018 12:19:18 GMT

<SNIP>

         28037 on-disk objects

[root at squid4-testing check-systemd-squid]# curl
127.0.0.1:3128/squid-internal-mgr/menu

index                  Cache Manager Interface                 public

menu                   Cache Manager Menu                      public

<SNIP>

server_list            Peer Cache Statistics                   public

[root at squid4-testing check-systemd-squid]# fg

tail /var/log/squid/access.log -f

1531916358.717 000000 127.0.0.1 TCP_MISS_ABORTED/000 0 GET
http://squid4-testing:3128/squid-internal-mgr/info - HIER_NONE/- - Q-CC: "-"
"-" Q-P: "-" "-" Q-RANGE: "-" REP-CC: "-" REP-EXP: "-" VARY: "-"
00:00:00:00:00:00 REP-X-CACHE: "-" Adapted-X-Store-Id: "-"

1531916361.505 000000 127.0.0.1 TCP_MISS_ABORTED/000 0 GET
http://squid4-testing:3128/squid-internal-mgr/menu - HIER_NONE/- - Q-CC: "-"
"-" Q-P: "-" "-" Q-RANGE: "-" REP-CC: "-" REP-EXP: "-" VARY: "-"
00:00:00:00:00:00 REP-X-CACHE: "-" Adapted-X-Store-Id: "-"

.

1531916504.216 000000 ::1 TCP_MISS_ABORTED/000 0 GET
cache_object://localhost/menu - HIER_NONE/- - Q-CC: "-" "-" Q-P: "-" "-"
Q-RANGE: "-" REP-CC: "-" REP-EXP: "-" VARY: "-" 00-00-00-00-00-00-00-00
REP-X-CACHE: "-" Adapted-X-Store-Id: "-"

### END

 

Also:

[root at squid4-testing check-systemd-squid]#  curl
127.0.0.1:3128/squid-internal-mgr/menu -I

HTTP/1.1 200 OK

Server: squid/4.1

Mime-Version: 1.0

Date: Wed, 18 Jul 2018 12:22:50 GMT

Content-Type: text/plain

Expires: Wed, 18 Jul 2018 12:22:50 GMT

Last-Modified: Wed, 18 Jul 2018 12:22:50 GMT

Connection: close

### END

 

So. a bug on 4.1?

 

Eliezer

 

----

Eliezer Croitoru <http://ngtech.co.il/lmgtfy/> 
Linux System Administrator
Mobile: +972-5-28704261
Email: eliezer at ngtech.co.il



 

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20180718/7b632357/attachment.htm>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: image003.png
Type: image/png
Size: 11317 bytes
Desc: not available
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20180718/7b632357/attachment.png>

From squid3 at treenet.co.nz  Wed Jul 18 12:54:00 2018
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 19 Jul 2018 00:54:00 +1200
Subject: [squid-users] Kerberos issues on 4.1
In-Reply-To: <20180718071656.GA7092@admin.sibptus.transneft.ru>
References: <20180717022016.GA44115@admin.sibptus.transneft.ru>
 <0c235a10-024b-48b6-fb0f-b6eeb87fe4e4@treenet.co.nz>
 <20180718071656.GA7092@admin.sibptus.transneft.ru>
Message-ID: <88cf5623-3cf8-6f99-54dc-8c47f3488cee@treenet.co.nz>

On 18/07/18 19:16, Victor Sudakov wrote:
> Amos Jeffries wrote:
>> On 17/07/18 14:20, Victor Sudakov wrote:
>>>
>>> After upgrading to Squid 4.1 (from FreeBSD ports) I started having problems
>>> with Kerberos authentication. 
>>>
>>> A user complained about being denied access.  The strange things are that:
>>>
>>> 1. There was only one such user, others seemed to be authenticating
>>> properly (or just did not complain).
>>>
>>> 2. The user seemed authenticated but still was denied (!), a sample access.log entry:
>>>
>>> 1531737712.384      7 212.73.124.190 TCP_DENIED/403 9976 GET http://yandex.ru/zzzzzzzzzzzz user at REA.LM HIER_NONE/- text/html
>>>
>>> The user tried different browsers on different hosts, with the same result.
>>>
>>> After downgrading to Squid 3.5.27 all went well again.
>>>
>>> Sorry I cannot provide more debugging info at present, I had to
>>> downgrade my two production Squids ASAP.
>>>
>>> Was there any major change between Squid 3 and 4 in the way
>>> Negotiate/Kerberos works?
>>>
>>
>> The biggest change is that bundled Kerberos auth helpers are now using
>> the newer v3.4+ helper protocol. That prevents some malformations of
>> Unicode and whitespace characters in the username or password which
>> Squid-3 might have been ignoring when it should have rejected.
>>
>> You may need to check both what you have on record in your AD/LDAP and
>> what the affected user thinks they need to enter.
> 
> If the access.log line (like the one above) contained "user at REA.LM"
> where the username and realm name are both correct and match those in
> the user's AD ticket, doesn't it mean that the Kerberos authentication
> has been successful ?

It means the authentication helper provided a user label for logging.

> 
> But for some reason this user was being TCP_DENIED though he was mentioned
> in the "vip_users.txt" file.
> 
> acl vip_users proxy_auth_regex -i "/usr/home/sudakov/squid/vip_users.txt"
> http_access allow sibptus vip_users
> 
> Why was he receiving a HTTP 403 I wonder? 403 is
> authorization-related, isn't it ? The username and realm were correct
> but still a 403.

Yes, exactly so. authenticate != authorized.

What is the sibptus definition? and what other http_access rules do you
have after that line?


Amos


From chip_pop at hotmail.com  Wed Jul 18 15:12:33 2018
From: chip_pop at hotmail.com (joseph)
Date: Wed, 18 Jul 2018 10:12:33 -0500 (CDT)
Subject: [squid-users] Mozilla Devise Solution To Encrypting SNI
Message-ID: <1531926753071-0.post@n4.nabble.com>

Encrypted SNI completely kills SSL Bump and all will follow that new SNI
Encryption
is there a hoop that start reworking adding this option to squid

https://appuals.com/apple-cloudflare-fastly-and-mozilla-devise-solution-to-encrypting-sni/




-----
************************** 
***** Crash to the future  ****
**************************
--
Sent from: http://squid-web-proxy-cache.1019090.n4.nabble.com/Squid-Users-f1019091.html


From sudakov at sibptus.tomsk.ru  Wed Jul 18 15:41:23 2018
From: sudakov at sibptus.tomsk.ru (Victor Sudakov)
Date: Wed, 18 Jul 2018 22:41:23 +0700
Subject: [squid-users] Kerberos issues on 4.1
In-Reply-To: <88cf5623-3cf8-6f99-54dc-8c47f3488cee@treenet.co.nz>
References: <20180717022016.GA44115@admin.sibptus.transneft.ru>
 <0c235a10-024b-48b6-fb0f-b6eeb87fe4e4@treenet.co.nz>
 <20180718071656.GA7092@admin.sibptus.transneft.ru>
 <88cf5623-3cf8-6f99-54dc-8c47f3488cee@treenet.co.nz>
Message-ID: <20180718154123.GA18194@admin.sibptus.transneft.ru>

Amos Jeffries wrote:
> >>>
> >>> After upgrading to Squid 4.1 (from FreeBSD ports) I started having problems
> >>> with Kerberos authentication. 
> >>>
> >>> A user complained about being denied access.  The strange things are that:
> >>>
> >>> 1. There was only one such user, others seemed to be authenticating
> >>> properly (or just did not complain).
> >>>
> >>> 2. The user seemed authenticated but still was denied (!), a sample access.log entry:
> >>>
> >>> 1531737712.384      7 212.73.124.190 TCP_DENIED/403 9976 GET http://yandex.ru/zzzzzzzzzzzz user at REA.LM HIER_NONE/- text/html
> >>>
> >>> The user tried different browsers on different hosts, with the same result.
> >>>
> >>> After downgrading to Squid 3.5.27 all went well again.
> >>>
> >>> Sorry I cannot provide more debugging info at present, I had to
> >>> downgrade my two production Squids ASAP.
> >>>
> >>> Was there any major change between Squid 3 and 4 in the way
> >>> Negotiate/Kerberos works?
> >>>
> >>
> >> The biggest change is that bundled Kerberos auth helpers are now using
> >> the newer v3.4+ helper protocol. That prevents some malformations of
> >> Unicode and whitespace characters in the username or password which
> >> Squid-3 might have been ignoring when it should have rejected.
> >>
> >> You may need to check both what you have on record in your AD/LDAP and
> >> what the affected user thinks they need to enter.
> > 
> > If the access.log line (like the one above) contained "user at REA.LM"
> > where the username and realm name are both correct and match those in
> > the user's AD ticket, doesn't it mean that the Kerberos authentication
> > has been successful ?
> 
> It means the authentication helper provided a user label for logging.

Fine, if the same user label is sufficient to authenticate/authorize a
user in Squid 3.5, why is it not sufficient in Squid 4.1? 

I did not touch the config during the upgrade and downgrade.


> > 
> > But for some reason this user was being TCP_DENIED though he was mentioned
> > in the "vip_users.txt" file.
> > 
> > acl vip_users proxy_auth_regex -i "/usr/home/sudakov/squid/vip_users.txt"
> > http_access allow sibptus vip_users
> > 
> > Why was he receiving a HTTP 403 I wonder? 403 is
> > authorization-related, isn't it ? The username and realm were correct
> > but still a 403.
> 
> Yes, exactly so. authenticate != authorized.

Please see above.
The user with this label *is* authorized by the "http_access allow sibptus vip_users" line.

> 
> What is the sibptus definition? 

acl sibptus src 212.73.124.190/32
acl sibptus src 212.73.125.240/28

All users use the same outside adresses.

> and what other http_access rules do you
> have after that line?

The complete list:

http_access deny !Safe_ports
http_access deny CONNECT !SSL_ports
http_access allow localhost manager
http_access deny manager
http_access allow cisa_servers
http_access deny badsites                                                     
http_access deny ads
http_access deny private                                                      
http_access allow sibptus business
http_access allow authenticated_sibptus ecology
http_access allow sibptus vip_users
http_access allow sibptus mailservers mail_users
http_access allow localhost
http_access deny all

The unfortunate user is in the 

acl vip_users proxy_auth_regex -i "/usr/home/sudakov/squid/vip_users.txt"

If there were an option to debug which "http_access" line rejects him
I could try it.


-- 
Victor Sudakov,  VAS4-RIPE, VAS47-RIPN
AS43859


From squid3 at treenet.co.nz  Wed Jul 18 15:52:33 2018
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 19 Jul 2018 03:52:33 +1200
Subject: [squid-users] Cache ran out of descriptors due to ICAP
 service/TCP SYNs ?
In-Reply-To: <32fb6ea566ba460db61a184cdd89888f@mbxtoa3.winmail.deshaw.com>
References: <073d52e4d31040e59ffaa3ff5e4d2040@mbxtoa3.winmail.deshaw.com>
 <2243eaa3-2d71-3696-2b6b-d82d238bf309@treenet.co.nz>
 <32fb6ea566ba460db61a184cdd89888f@mbxtoa3.winmail.deshaw.com>
Message-ID: <3ee3d9d1-4db8-10ad-4746-c8e1bf33f434@treenet.co.nz>

On 18/07/18 18:30, Ahmad, Sarfaraz wrote:
> Thanks for the reply. I haven't completely understood the revert and have a few more related questions.
> 
> I see these messages, 
> Jul 17 19:21:14 proxy2.hyd.deshaw.com squid[5747]: suspending ICAP service for too many failures
> Jul 17 19:21:14 proxy2.hyd.deshaw.com squid[5747]: optional ICAP service is suspended: icap://127.0.0.1:1344/reqmod [down,susp,fail11]
> 1)   If the ICAP service is unresponsive, Squid would not exhaust its file descriptors trying to reach the service again and again right (too many TCP SYNs for trying to connect to the ICAP service )? 
> 

Correct. It would not exhaust resources on *that* action. Other actions
possibly resulting from that state existing are another matter entirely.


> 
> 
> Max Connections returned by the ICAP service is 16. And given my ICAP settings, 
> icap_enable on
> icap_service test_icap reqmod_precache icap://127.0.0.1:1344/reqmod bypass=on routing=off on-overload=wait
> On-overload is set to "wait". The documentation says " * wait:   wait (in a FIFO queue) for an ICAP connection slot" . This means that a new TCP connection would not be attempted if max connections is reached right ? 
> 2)   Am I right in saying that if the ICAP service is underperforming or has failed, this won't lead a sudden increase in the open file descriptors with on-overload set to "wait" ?
> 

No. The side effects of the ICAP service not being used determine the
possible outcomes there.


> 
> Also I have no way to explain the "connection reset by peer" messages.

Neither, given the details provided.


> Jul 13 11:23:18 <hostname> squid[13123]: Error negotiating SSL connection on FD 1292: (104) Connection reset by peer
> Jul 13 11:23:18 <hostname> squid[13123]: Error negotiating SSL connection on FD 1631: (104) Connection reset by peer
> Jul 13 11:35:17 <hostname> squid[13123]: Error negotiating SSL connection on FD 1331: (104) Connection reset by peer
> 
> I have a few proxies (running in separate virtual machines). All of them went unresponsive at around the same time, leading to an outage of the internet.
> I am using WCCPv2 to redirect from firewall to these proxies.  I checked the logs there and WCCP communication was not intermittent.
> The logs on the proxies are bombarded with " Error negotiating SSL connection on FD 1331: (104) Connection reset by peer " messages.

A strong sign that forwarding loops are occuring, or something cut a
huge number of TCP connections at once.

Although syslog recording is limited by the network traffic. So
situations of high network flooding its timestamps can be very
inaccurate or unordered.


> Since the ICAP service in not SSL-protected I think these messages mostly imply receiving TCP RSTs from remote servers. (or could it be clients somehow ?).

Yes, another reason I am thinking along the lines of forwarding loops.

> Once I removed WCCP direction rules from the firewall, internet was
back up.
> This hints that something in this proxy pipeline was amiss and not with the internet link itself. I don't see any outages on that.

Nod. Keep in mind though that "proxy pipeline" includes the WCCP rules
in the router, NAT rules on the proxy machine, proxy config, connection
to/from the ICAP server, and NAT rules on the proxy machine outgoing,
and WCCP rules on the router a second time.

So a lot of parts, most outside of Squid - any one of which can screw up
the entire pathway.


> I am pretty sure ACLs weren't changed and there was no forwarding loop.
> What could possibly explain the connection reset by peer messages ? Even if the internet was down, that won't lead to TCP RSTs. 
> I cannot tie these TCP RSTs and the incoming requests getting held up and ultimately leading to FD exhaustion.

Too many possibilities to list here, and we do not have sufficient
information. You need to track down exactly which software is generating
them, and why.


> 
> You earlier said 
>>> In normal operation it is not serious, but you are already into abnormal operation by the crashing. So not releasing sockets/FD fast enough makes the overall problem worse.
> If squid-1 is crashing and getting respawned, it will have its own 16K FD limit right, I wonder how the newer squid-1 serves older requests. Can you please elaborate on " So not releasing sockets/FD fast enough makes the overall problem worse." ?
> 

Depending on your OS there are per-process and process group limits. The
latter may be applicable if you are using SMP workers.


Amos


From squid3 at treenet.co.nz  Wed Jul 18 15:56:17 2018
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 19 Jul 2018 03:56:17 +1200
Subject: [squid-users] Kerberos issues on 4.1
In-Reply-To: <20180718154123.GA18194@admin.sibptus.transneft.ru>
References: <20180717022016.GA44115@admin.sibptus.transneft.ru>
 <0c235a10-024b-48b6-fb0f-b6eeb87fe4e4@treenet.co.nz>
 <20180718071656.GA7092@admin.sibptus.transneft.ru>
 <88cf5623-3cf8-6f99-54dc-8c47f3488cee@treenet.co.nz>
 <20180718154123.GA18194@admin.sibptus.transneft.ru>
Message-ID: <ccf40e2c-373f-ce04-3a60-c0735b3703d7@treenet.co.nz>

On 19/07/18 03:41, Victor Sudakov wrote:
>
> If there were an option to debug which "http_access" line rejects him
> I could try it.
> 

Please try:
 debug_options ALL,1 28,5

... and have them login. Your cache.log should then list the ACLs being
tested and what their results are.

Amos


From rousskov at measurement-factory.com  Wed Jul 18 16:05:05 2018
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Wed, 18 Jul 2018 10:05:05 -0600
Subject: [squid-users] Mozilla Devise Solution To Encrypting SNI
In-Reply-To: <1531926753071-0.post@n4.nabble.com>
References: <1531926753071-0.post@n4.nabble.com>
Message-ID: <843a0de6-253a-8f9f-5e27-332b9532a1da@measurement-factory.com>

On 07/18/2018 09:12 AM, joseph wrote:
> Encrypted SNI completely kills SSL Bump and all will follow that new SNI
> Encryption
> is there a hoop that start reworking adding this option to squid
> 
> https://appuals.com/apple-cloudflare-fastly-and-mozilla-devise-solution-to-encrypting-sni/


I do not understand your question but hope that the following info may
be useful in this context.

The pictures in that article do not show encrypted SNI. They seem to
show a standard TLS v1.3 exchange where SNI is not encrypted but the
server certificate is. The article text is not technical/accurate enough
to tell us what exactly is being implemented.

The following draft could be a better source for eSNI information, but
it is far from its final stages, documenting two alternative
implementations, one of which will be eventually removed:

  https://tools.ietf.org/html/draft-ietf-tls-sni-encryption

If you have better sources of information about eSNI, please post them.

FWIW, my prediction is that plain SNI will still be available, but it
will become useless for avoiding bumping specific services. Both
solutions in the above draft rely on a "fronting service" that can be
reached using a "generic" bigc.example.com SNI (common to many services
offered by the Big Corporation).

We have started analyzing TLS v1.3 requirements as they apply to Squid,
but I am not aware of any specific work dealing with any of the proposed
eSNI techniques.


HTH,

Alex.


From eliezer at ngtech.co.il  Wed Jul 18 16:30:25 2018
From: eliezer at ngtech.co.il (Eliezer Croitoru)
Date: Wed, 18 Jul 2018 19:30:25 +0300
Subject: [squid-users] Mozilla Devise Solution To Encrypting SNI
In-Reply-To: <1531926753071-0.post@n4.nabble.com>
References: <1531926753071-0.post@n4.nabble.com>
Message-ID: <032901d41eb4$a18d8520$e4a88f60$@ngtech.co.il>

Hey Joseph,

It's nice to want security and I do think that security is important.
However there are other sides to security as well.
The standard user doesn't know what he can consider secure or not.
Some users think that if there is HTTPS(Let's encrypt) in the url it makes the connection secure and safe.
The reality is that HTTPS and TLS doesn't make the web more secure and SNI is not to blame.
In organizations which are required to inspect traffic, SNI encryption would be nice for some END to END security but..
It might leave the global security level of the organization in a very weird situation.
So even if specific companies will drive some level of "eSNI" for the "END USER" safety they are doing two things and maybe more:
- leaving the client vulnerable to their security level
- leaving many important organizations without any way of securing their data
...

Due to all the above I believe that any work on eSNI will require the developers to take into account specific organization's needs.
If banks will be required to develop their own browser or security stack due to the world being afraid, panic, tight and "secure" it's possible
that you I and many others will be required to pay for it from our pocket.

For example cloudflare and the others that are mentioned as a side note to the this article have interest to "secure" their clients...
Other companies around the globe do not share the same interests and their definition of securing their clients.

I think that China takes security in a specific level and they have enough CPU, RAM, Power and other resources which allows them
to ignore apple or cloudflare or any other company that wants to "secure everybody on the planet".

If you believe Google systems are safe and un-breakable then I would just say that they do enough phishing that many 
Chinese security experts know about and due to this decided to block them.
Is it good? You can decide who you can trust or not...

All The Bests,
Eliezer

* This email is sort of my personal opinion but I know that couple security experts share this or similar stand about this subject.

----
Eliezer Croitoru
Linux System Administrator
Mobile: +972-5-28704261
Email: eliezer at ngtech.co.il


-----Original Message-----
From: squid-users [mailto:squid-users-bounces at lists.squid-cache.org] On Behalf Of joseph
Sent: Wednesday, July 18, 2018 6:13 PM
To: squid-users at lists.squid-cache.org
Subject: [squid-users] Mozilla Devise Solution To Encrypting SNI

Encrypted SNI completely kills SSL Bump and all will follow that new SNI Encryption is there a hoop that start reworking adding this option to squid

https://appuals.com/apple-cloudflare-fastly-and-mozilla-devise-solution-to-encrypting-sni/




-----
**************************
***** Crash to the future  ****
**************************
--
Sent from: http://squid-web-proxy-cache.1019090.n4.nabble.com/Squid-Users-f1019091.html
_______________________________________________
squid-users mailing list
squid-users at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-users



From eliezer at ngtech.co.il  Wed Jul 18 16:47:07 2018
From: eliezer at ngtech.co.il (Eliezer Croitoru)
Date: Wed, 18 Jul 2018 19:47:07 +0300
Subject: [squid-users] Delay pools in squid4 not working with https
In-Reply-To: <44ca2b8a-2f8d-2e47-b902-f7589e6d5fe4@treenet.co.nz>
References: <54444655-7fa0-ec9a-ea6e-505697d6bb0a@gmail.com>
 <44ca2b8a-2f8d-2e47-b902-f7589e6d5fe4@treenet.co.nz>
Message-ID: <034101d41eb6$f6c53ef0$e44fbcd0$@ngtech.co.il>

Just to mention QUIC related wiki links:
- https://wiki.squid-cache.org/KnowledgeBase/Block%20QUIC%20protocol?highlight=%28QUIC%29
- https://wiki.squid-cache.org/ConfigExamples/Intercept/CiscoIOSv15Wccp2?highlight=%28QUIC%29#QUIC.2FSPDY_protocol_blocking

Eliezer

----
Eliezer Croitoru
Linux System Administrator
Mobile: +972-5-28704261
Email: eliezer at ngtech.co.il



-----Original Message-----
From: squid-users [mailto:squid-users-bounces at lists.squid-cache.org] On Behalf Of Amos Jeffries
Sent: Wednesday, July 11, 2018 12:35 AM
To: squid-users at lists.squid-cache.org
Subject: Re: [squid-users] Delay pools in squid4 not working with https

On 11/07/18 07:50, Paolo Marzari wrote:
> My home server just updated from 3.5.27, everything is working fine, 
> but delay pools seems broken to me.
> I capped some devices to 240kb/s and tried to download a debian ISO 
> with one of them...all good, 240kb/s.
> Then I tried a speed test, results = 2.2mb/s, that's the whole ADSL speed.
> 
> So I tried youtube videos, no cap at all, same problem with facebook.
> Revert to 3.5.27 and delays works again with every type of traffic.
> 
> I think there's something wrong with https traffic.
> 

a) is it actually HTTPS traffic?

b) are the bytes going through the proxy 2.2Mbps or 240kbps ?

I ask because Google/YouTube and Facebook are services using HTTP/2 with high compression features as much as possible. So while the proxy is set to transfer X bytes per second, when hidden inside "HTTPS" those X bytes may show up as 90*X bytes of traffic when decompressed by a Browser.

Or the transfer may be QUIC protocol, completely bypassing the HTTP the proxy is counting.

Amos
_______________________________________________
squid-users mailing list
squid-users at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-users



From chip_pop at hotmail.com  Wed Jul 18 19:18:02 2018
From: chip_pop at hotmail.com (joseph)
Date: Wed, 18 Jul 2018 14:18:02 -0500 (CDT)
Subject: [squid-users] Mozilla Devise Solution To Encrypting SNI
In-Reply-To: <032901d41eb4$a18d8520$e4a88f60$@ngtech.co.il>
References: <1531926753071-0.post@n4.nabble.com>
 <032901d41eb4$a18d8520$e4a88f60$@ngtech.co.il>
Message-ID: <1531941482790-0.post@n4.nabble.com>

Eliezer  i agree with you with all that
there is no such a secure think for client as long as the web bug exist :)
those large link with small size or so
the main reason i think  they ar going to have more secure is to kill https
proxy so big company 
can sale there Owen prox with very expensive key im shur they start saling
that to gov or isp can handle the $$$$$ its all about mony nothing else
squid or other proxy ar like firewall in midle so its the only solution to
get rid off so they can benefit 
as you sayd   there is no such a privacy at all




-----
************************** 
***** Crash to the future  ****
**************************
--
Sent from: http://squid-web-proxy-cache.1019090.n4.nabble.com/Squid-Users-f1019091.html


From eliezer at ngtech.co.il  Wed Jul 18 20:23:59 2018
From: eliezer at ngtech.co.il (Eliezer Croitoru)
Date: Wed, 18 Jul 2018 23:23:59 +0300
Subject: [squid-users] question about squid and https connection .
In-Reply-To: <0e875c86-f1db-9f5d-5c9c-d95bd901f70d@measurement-factory.com>
References: <7866F6E4-CEC8-4513-A7FF-90DE4EC8EB8D@netstream.ps>
 <9da11737-ca55-e522-1150-cb9c26a079fb@measurement-factory.com>
 <03a101d41a1f$e9ce5970$bd6b0c50$@ngtech.co.il>
 <0e875c86-f1db-9f5d-5c9c-d95bd901f70d@measurement-factory.com>
Message-ID: <044001d41ed5$42b80310$c8280930$@ngtech.co.il>

Alex,

Some properties of the certificate are static but...
A certificate is certifying a specific key.
If every certificate would be exactly the same as the other on all its properties including the key then we would be able to..
Fake any certificate in the world very very fast.

Correct me if I'm wrong:
Every certificate have the same properties of the original one except the "RSA key" part which it's certifiying.
There is a dynamic variable in every certificate when it's being created(not talking about time stamps...).

So what I'm saying is that you cannot say that every certificate which will be created with the same CA will be the same for two different 2048 bits RSA keys.

Let me know if I got it right.

Eliezer

----
Eliezer Croitoru
Linux System Administrator
Mobile: +972-5-28704261
Email: eliezer at ngtech.co.il



-----Original Message-----
From: squid-users [mailto:squid-users-bounces at lists.squid-cache.org] On Behalf Of Alex Rousskov
Sent: Friday, July 13, 2018 2:01 AM
To: 'Squid Users' <squid-users at lists.squid-cache.org>
Subject: Re: [squid-users] question about squid and https connection .

On 07/12/2018 02:35 PM, Eliezer Croitoru wrote:

> Every RSA key and certificate pair regardless to the origin server
> and the SSL-BUMP enabled proxy can be different.

I cannot find a reasonable interpretation of the above that would
contradict what I have said. Yes, each unique certificate has its own
private key, but that is not what Ahmad was asking about AFAICT.


> Will it be more accurate to say that just as long as these 200 squid
> instances(different squid.conf and couple other local variables) use
> the same exact ssl_db cache directory  then it's probable that they
> will use the same certificate.

That statement is incorrect. Squids configured with different CA
certificates will generate different fake certificates for the same real
certificate.

I assume that Ahmad was asking about a situation where 200 Squid
instances had the same configuration (including CA certificates).

Please note that the certificate generator helper gets the signing (CA)
certificate as a parameter with each generation request (because
different Squid ports may use different CA certificates). Also, Squid
probably does not officially support sharing the certificate directory
across Squid instances (even if it works).


> Or these 200 squid instances are in SMP mode with 200 workers... If
> these 200 instances do not share memory and certificate cache then
> there is a possibility that the same site from two different sources 
> will serve different certificates(due to the different RSA key which
> is different).

200 SMP workers or 200 identically-configured Squid instances will
generate the same fake certificates for the same real certificate.
"Stable certificates" is an important requirement for many distributed
Squid deployments.

Alex.



> -----Original Message-----
> From: squid-users [mailto:squid-users-bounces at lists.squid-cache.org] On Behalf Of Alex Rousskov
> Sent: Thursday, July 12, 2018 11:27 PM
> To: --Ahmad-- <ahmed.zaeem at netstream.ps>; Squid Users <squid-users at lists.squid-cache.org>
> Subject: Re: [squid-users] question about squid and https connection .
> 
> On 07/12/2018 01:17 PM, --Ahmad-- wrote:
> 
>> if i have pc# 1 and that pc open facebook .
>>
>> then i have other pc # 2 and that other pc open facebook .
>>
>>
>> now  as we know facebook is https .
>>
>> so is the key/ cert that used on pc # 1 is same as cert in pc # 2 to decrypt the fb encrypted traffic ?
> 
> Certificates themselves are not used (directly) to decrypt traffic
> AFAIK, but yes, both PCs will see the same server certificate (ignoring
> CDNs and other complications).
> 
> 
> 
>> now in the presence of squid .
>>
>> if i used tcp connect method  , will it be different than above ?
> 
> If you are not bumping the connection, then both PCs will see the same
> real Facebook certificate as if those PCs did not use a proxy.
> 
> If you are bumping the connection, then both PCs will see the same fake
> certificate generated by Squid.
> 
> 
> 
>> say i used 200 proxies in same squid machine and i used to access FB from the same pc same browser .
>>
>> will facebook see my cert/key i used to decrypt its traffic ?
> 
> If you are asking whether Facebook will know anything about the fake
> certificate generated by Squid for clients, then the answer is "no,
> unless Facebook runs some special client code to deliver (Squid)
> certificate back to Facebook".
> 
> In general, the origin server assumes that the client is talking to it
> directly. Clients may pin or otherwise restrict certificates that they
> trust, but after the connection is successfully established, the server
> may assume that it is talking to the client directly. A paranoid server
> may deliver special code to double check that assumption, but there are
> other, more standard methods to prevent bumping such as certificate
> pinning and certificate transparency cervices.
> 
> 
> 
>> is the key/cert of FB to decrypt the https content is same on all browsers on all computers ?
> 
> If you are asking whether the generated certificates are going to be the
> same for all clients, then the answer is "yes, provided all those 200
> Squids use the same configuration (including the CA certificate) and
> receive the same real certificate from Facebook". Squid's certificate
> generation algorithm generates the same certificate given the same
> configuration and the same origin server certificate.
> 
> 
> HTH,
> 
> Alex.
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
> 

_______________________________________________
squid-users mailing list
squid-users at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-users



From michael.adm at gmail.com  Wed Jul 18 20:29:17 2018
From: michael.adm at gmail.com (Michael Pro)
Date: Wed, 18 Jul 2018 23:29:17 +0300
Subject: [squid-users] store_id_extras and http Request Headers
Message-ID: <CAA+Mow4N+-72NEkYM=SFyF6GExW18Xuz_YWXVNLSWTMtuJZE5A@mail.gmail.com>

In squid.conf
store_id_extras "%>a/%>A %un %>rm %>h myip=%la myport=%lp
%{User-Agent}>h %{Referer}>h %{Host}>h %>rP"

Result incoming parameters in store_id_program are:
0:  https://2ip.ua/images/icon/IP_calculator.png <--- (requested url)
1:  127.0.0.119/127.0.0.119  <--- %>a/%>A
2:  -           <--- %un (this is ok, absent at all)
3:  GET  <--- %>rm
4:  -           <----- %>h ---> Where are they !?!?!?
5:  myip=127.0.0.1
6:  myport=20990
7:  Mozilla/5.0%20(iPhone;%20CPU%20iPhone%20OS%2...  <--- %{User-Agent}>h
8:  https://2ip.ua/ru/  <--- {Referer}>h
9:  2ip.ua  <--- %{Host}>h
10: 443  <--- %>rP

How do I get all the request header fields?
I do not need any separate, I need all fields in the request header.

Thanks in advance for the answer.


From rousskov at measurement-factory.com  Wed Jul 18 20:35:04 2018
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Wed, 18 Jul 2018 14:35:04 -0600
Subject: [squid-users] store_id_extras and http Request Headers
In-Reply-To: <CAA+Mow4N+-72NEkYM=SFyF6GExW18Xuz_YWXVNLSWTMtuJZE5A@mail.gmail.com>
References: <CAA+Mow4N+-72NEkYM=SFyF6GExW18Xuz_YWXVNLSWTMtuJZE5A@mail.gmail.com>
Message-ID: <8da965d4-81d7-cecb-a947-5e2acbf003af@measurement-factory.com>

On 07/18/2018 02:29 PM, Michael Pro wrote:
> In squid.conf
> store_id_extras "%>a/%>A %un %>rm %>h myip=%la myport=%lp
> %{User-Agent}>h %{Referer}>h %{Host}>h %>rP"
> 
> Result incoming parameters in store_id_program are:
> 0:  https://2ip.ua/images/icon/IP_calculator.png <--- (requested url)
> 1:  127.0.0.119/127.0.0.119  <--- %>a/%>A
> 2:  -           <--- %un (this is ok, absent at all)
> 3:  GET  <--- %>rm
> 4:  -           <----- %>h ---> Where are they !?!?!?
> 5:  myip=127.0.0.1
> 6:  myport=20990
> 7:  Mozilla/5.0%20(iPhone;%20CPU%20iPhone%20OS%2...  <--- %{User-Agent}>h
> 8:  https://2ip.ua/ru/  <--- {Referer}>h
> 9:  2ip.ua  <--- %{Host}>h
> 10: 443  <--- %>rP
> 
> How do I get all the request header fields?
> I do not need any separate, I need all fields in the request header.

The lack of %>h output when %{Name}>h output is present looks like a
Squid bug to me -- %>h output should be there as well.

Alex.


From rousskov at measurement-factory.com  Wed Jul 18 20:45:07 2018
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Wed, 18 Jul 2018 14:45:07 -0600
Subject: [squid-users] question about squid and https connection .
In-Reply-To: <044001d41ed5$42b80310$c8280930$@ngtech.co.il>
References: <7866F6E4-CEC8-4513-A7FF-90DE4EC8EB8D@netstream.ps>
 <9da11737-ca55-e522-1150-cb9c26a079fb@measurement-factory.com>
 <03a101d41a1f$e9ce5970$bd6b0c50$@ngtech.co.il>
 <0e875c86-f1db-9f5d-5c9c-d95bd901f70d@measurement-factory.com>
 <044001d41ed5$42b80310$c8280930$@ngtech.co.il>
Message-ID: <b3ab9209-a2fd-cc0d-e4d6-c3e4f61fc14e@measurement-factory.com>

On 07/18/2018 02:23 PM, Eliezer Croitoru wrote:


> Every certificate have the same properties of the original one except
> the "RSA key" part which it's certifiying.

Assuming you are talking about the generated certificates for the same
real certificate X, then yes, they will all have the same (mimicked)
fields. Whether they will be signed by the same CA depends on Squid
configuration. In my answers, I assumed that all those Squids are
configured with the same CA (including the same private key).


> So what I'm saying is that you cannot say that every certificate
> which will be created with the same CA will be the same for two
> different 2048 bits RSA keys.

... unless the keys are also the same, which was my and, AFAICT, OP
assumption.

Also, unless you are doing something nasty, it probably does not make
sense to configure a bumping Squid with a public CA certificate that is
identical to some other public CA certificate but has a different
private key. In other words, if you are using 200 Squids with a single
public CA certificate, then all those Squids should use the same private
key.

Alex.



> -----Original Message-----
> From: squid-users [mailto:squid-users-bounces at lists.squid-cache.org] On Behalf Of Alex Rousskov
> Sent: Friday, July 13, 2018 2:01 AM
> To: 'Squid Users' <squid-users at lists.squid-cache.org>
> Subject: Re: [squid-users] question about squid and https connection .
> 
> On 07/12/2018 02:35 PM, Eliezer Croitoru wrote:
> 
>> Every RSA key and certificate pair regardless to the origin server
>> and the SSL-BUMP enabled proxy can be different.
> 
> I cannot find a reasonable interpretation of the above that would
> contradict what I have said. Yes, each unique certificate has its own
> private key, but that is not what Ahmad was asking about AFAICT.
> 
> 
>> Will it be more accurate to say that just as long as these 200 squid
>> instances(different squid.conf and couple other local variables) use
>> the same exact ssl_db cache directory  then it's probable that they
>> will use the same certificate.
> 
> That statement is incorrect. Squids configured with different CA
> certificates will generate different fake certificates for the same real
> certificate.
> 
> I assume that Ahmad was asking about a situation where 200 Squid
> instances had the same configuration (including CA certificates).
> 
> Please note that the certificate generator helper gets the signing (CA)
> certificate as a parameter with each generation request (because
> different Squid ports may use different CA certificates). Also, Squid
> probably does not officially support sharing the certificate directory
> across Squid instances (even if it works).
> 
> 
>> Or these 200 squid instances are in SMP mode with 200 workers... If
>> these 200 instances do not share memory and certificate cache then
>> there is a possibility that the same site from two different sources 
>> will serve different certificates(due to the different RSA key which
>> is different).
> 
> 200 SMP workers or 200 identically-configured Squid instances will
> generate the same fake certificates for the same real certificate.
> "Stable certificates" is an important requirement for many distributed
> Squid deployments.
> 
> Alex.
> 
> 
> 
>> -----Original Message-----
>> From: squid-users [mailto:squid-users-bounces at lists.squid-cache.org] On Behalf Of Alex Rousskov
>> Sent: Thursday, July 12, 2018 11:27 PM
>> To: --Ahmad-- <ahmed.zaeem at netstream.ps>; Squid Users <squid-users at lists.squid-cache.org>
>> Subject: Re: [squid-users] question about squid and https connection .
>>
>> On 07/12/2018 01:17 PM, --Ahmad-- wrote:
>>
>>> if i have pc# 1 and that pc open facebook .
>>>
>>> then i have other pc # 2 and that other pc open facebook .
>>>
>>>
>>> now  as we know facebook is https .
>>>
>>> so is the key/ cert that used on pc # 1 is same as cert in pc # 2 to decrypt the fb encrypted traffic ?
>>
>> Certificates themselves are not used (directly) to decrypt traffic
>> AFAIK, but yes, both PCs will see the same server certificate (ignoring
>> CDNs and other complications).
>>
>>
>>
>>> now in the presence of squid .
>>>
>>> if i used tcp connect method  , will it be different than above ?
>>
>> If you are not bumping the connection, then both PCs will see the same
>> real Facebook certificate as if those PCs did not use a proxy.
>>
>> If you are bumping the connection, then both PCs will see the same fake
>> certificate generated by Squid.
>>
>>
>>
>>> say i used 200 proxies in same squid machine and i used to access FB from the same pc same browser .
>>>
>>> will facebook see my cert/key i used to decrypt its traffic ?
>>
>> If you are asking whether Facebook will know anything about the fake
>> certificate generated by Squid for clients, then the answer is "no,
>> unless Facebook runs some special client code to deliver (Squid)
>> certificate back to Facebook".
>>
>> In general, the origin server assumes that the client is talking to it
>> directly. Clients may pin or otherwise restrict certificates that they
>> trust, but after the connection is successfully established, the server
>> may assume that it is talking to the client directly. A paranoid server
>> may deliver special code to double check that assumption, but there are
>> other, more standard methods to prevent bumping such as certificate
>> pinning and certificate transparency cervices.
>>
>>
>>
>>> is the key/cert of FB to decrypt the https content is same on all browsers on all computers ?
>>
>> If you are asking whether the generated certificates are going to be the
>> same for all clients, then the answer is "yes, provided all those 200
>> Squids use the same configuration (including the CA certificate) and
>> receive the same real certificate from Facebook". Squid's certificate
>> generation algorithm generates the same certificate given the same
>> configuration and the same origin server certificate.
>>
>>
>> HTH,
>>
>> Alex.
>> _______________________________________________
>> squid-users mailing list
>> squid-users at lists.squid-cache.org
>> http://lists.squid-cache.org/listinfo/squid-users
>>
> 
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
> 



From michael.adm at gmail.com  Wed Jul 18 20:53:27 2018
From: michael.adm at gmail.com (Michael Pro)
Date: Wed, 18 Jul 2018 23:53:27 +0300
Subject: [squid-users] store_id_extras and http Request Headers
In-Reply-To: <8da965d4-81d7-cecb-a947-5e2acbf003af@measurement-factory.com>
References: <CAA+Mow4N+-72NEkYM=SFyF6GExW18Xuz_YWXVNLSWTMtuJZE5A@mail.gmail.com>
 <8da965d4-81d7-cecb-a947-5e2acbf003af@measurement-factory.com>
Message-ID: <CAA+Mow7qpkw66ojhCOQ7Ey7DaBbTXnG_5xeyZJ4TXksnF_03RA@mail.gmail.com>

root at f12rw0:~ # squid -v
Squid Cache: Version 5.0.0-20180717-r6956579
Service Name: squid

This binary uses OpenSSL 1.0.2o-freebsd  27 Mar 2018. For legal
restrictions on distribution see
https://www.openssl.org/source/license.html

configure options:  '--with-default-user=squid'
'--bindir=/usr/local/sbin' '--sbindir=/usr/local/sbin'
'--datadir=/usr/local/etc/squid'
'--libexecdir=/usr/local/libexec/squid' '--localstatedir=/var'
'--sysconfdir=/usr/local/etc/squid' '--with-logdir=/var/log/squid'
'--infodir=/usr/local/info/' '--with-large-files'
'--with-pidfile=/var/run/squid/squid.pid' '--mandir=/usr/local/man'
'--with-swapdir=/var/squid/cache' '--enable-diskio'
'--with-filedescriptors=262144'
'--enable-disk-io=DiskThreads,DiskDaemon,AIO,Blocking,IpcIo,Mmapped'
'--enable-storeio=aufs,rock,ufs,diskd'
'--enable-removal-policies=lru,heap'
'--enable-log-daemon-helpers=file' '--enable-url-rewrite-helpers=fake'
'--enable-storeid-rewrite-helpers=file'
'--enable-external-acl-helpers' '--enable-ssl' '--enable-ssl-crtd'
'--with-openssl' '--enable-security-cert-generators=file'
'--enable-security-cert-validators=fake' '--enable-build-info'
'--enable-loadable-modules' '--enable-icap-client'
'--enable-http-violations' '--enable-ipfw-transparent' '--enable-snmp'
'--enable-stacktraces' '--enable-pthreads' '--enable-kqueue'
'--enable-zph-qos' '--enable-async-io' '--enable-large-cache-files'
'--enable-delay-pools' '--disable-linux-netfilter'
'--disable-linux-tproxy' '--disable-ipf-transparent'
'--disable-pf-transparent' '--without-nat-devpf'
'--disable-ident-lookups' '--disable-silent-rules' '--without-gnutls'
'--without-gss' '--without-heimdal-krb5' '--without-mit-krb5' 'CC=cc'
'CFLAGS=-O2 -pipe -I/usr/local/include/event2 -I/usr/local/include -g
-fstack-protector -fno-strict-aliasing' 'LDFLAGS=-L/usr/local/lib/db5
-L/usr/local/lib -pthread -lunwind -L/usr/local/lib -fstack-protector'
'LIBOPENSSL_CFLAGS=-I/usr/include' 'LIBOPENSSL_LIBS=-lcrypto -lssl'
'CXX=c++' 'CXXFLAGS=-O2 -pipe -I/usr/local/include/event2
-I/usr/local/include -g -fstack-protector -fno-strict-aliasing
-Wno-unknown-warning-option -Wno-undefined-bool-conversion
-Wno-tautological-undefined-compare -Wno-dynamic-class-memaccess'
'CPP=cpp' 'CPPFLAGS=-I/usr/local/include/event2 -I/usr/local/include
-I/usr/local/include/db5 -g' 'PKG_CONFIG=pkgconf'
You have new mail.
root at f12rw0:~ #
??, 18 ???. 2018 ?. ? 23:35, Alex Rousskov <rousskov at measurement-factory.com>:
>
> On 07/18/2018 02:29 PM, Michael Pro wrote:
> > In squid.conf
> > store_id_extras "%>a/%>A %un %>rm %>h myip=%la myport=%lp
> > %{User-Agent}>h %{Referer}>h %{Host}>h %>rP"
> >
> > Result incoming parameters in store_id_program are:
> > 0:  https://2ip.ua/images/icon/IP_calculator.png <--- (requested url)
> > 1:  127.0.0.119/127.0.0.119  <--- %>a/%>A
> > 2:  -           <--- %un (this is ok, absent at all)
> > 3:  GET  <--- %>rm
> > 4:  -           <----- %>h ---> Where are they !?!?!?
> > 5:  myip=127.0.0.1
> > 6:  myport=20990
> > 7:  Mozilla/5.0%20(iPhone;%20CPU%20iPhone%20OS%2...  <--- %{User-Agent}>h
> > 8:  https://2ip.ua/ru/  <--- {Referer}>h
> > 9:  2ip.ua  <--- %{Host}>h
> > 10: 443  <--- %>rP
> >
> > How do I get all the request header fields?
> > I do not need any separate, I need all fields in the request header.
>
> The lack of %>h output when %{Name}>h output is present looks like a
> Squid bug to me -- %>h output should be there as well.
>
> Alex.


From vishali.somaskanthan at viptela.com  Wed Jul 18 21:03:52 2018
From: vishali.somaskanthan at viptela.com (Vishali Somaskanthan)
Date: Wed, 18 Jul 2018 14:03:52 -0700
Subject: [squid-users] server persistent connections and cache
Message-ID: <CABfsTT6_JLH01qmk+aaBCtJaqZ0t3Xry=UmePGwPvFn_6fRTCg@mail.gmail.com>

Dear Squid users,
          There is a connection C1 from the client to squid and that is
bumped at squid which forms a TCP connection with origin server S1. Having
server persistent connection turned on, for subsequent requests, I see that
the same TCP connection is used between the squid and the server and all
was fine.

I had a problem after sending too many requests to the same server where my
persistence stopped working suddenly. The connection was getting closed
from the squid side sending a FIN packet to the origin server. When
debugging on this, I found my access logs had states
TCP_REFRESH_UNMODIFIED, instead of TCP_MISS which I was seeing earlier when
all was fine.

I found that the results were from cache. I included the  cache deny
all  option
and found that the access logs had TCP_MISS since it wasn't using
cached results and my persistence is fine again.

I have three questions in this scenario:
1. What is the relationship between the caching and the persistence
connection established?
2. When will squid use cached results and when will it not if the cache
deny all directive weren't specified.
3. However, this didn't work fully. Even with *cache deny all* directive,
persistence wasn't working when I peeked at the sslBump step 1 and then
Bumped.
Persistence worked only when I directly did sslbump allow all (without
peeking at first step).


Comments are appreciated on the scenario.



-- 
Thanks,
Vishali Somaskanthan
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20180718/36b2ea79/attachment.htm>

From rousskov at measurement-factory.com  Thu Jul 19 00:00:32 2018
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Wed, 18 Jul 2018 18:00:32 -0600
Subject: [squid-users] server persistent connections and cache
In-Reply-To: <CABfsTT6_JLH01qmk+aaBCtJaqZ0t3Xry=UmePGwPvFn_6fRTCg@mail.gmail.com>
References: <CABfsTT6_JLH01qmk+aaBCtJaqZ0t3Xry=UmePGwPvFn_6fRTCg@mail.gmail.com>
Message-ID: <797fbde1-8585-6b71-30ae-d86575b0fbd1@measurement-factory.com>

On 07/18/2018 03:03 PM, Vishali Somaskanthan wrote:

> I had a problem after sending too many requests to the same server where
> my persistence stopped working suddenly.

Please note that there are many reasons why a proxy may close a
connection. For pinned to-server connections (like those created by
SslBump), it may not be possible to open a new to-server connection so
Squid should close both from-client and to-server connections.

In general, a client should not rely on a connections staying persistent
except in some very unusual/special circumstances.


> 1.?What is the relationship between the caching and the persistence
> connection established?

Virtually none. Caching decisions are done primarily based on request
and response headers.


> 2. When will squid use cached results and when will it not if the cache
> deny all directive weren't specified. 

Squid will deliver a response from the cache when HTTP rules and Squid
configuration allow a hit. The details are too complex to document here.


> 3. However, this?didn't?work fully. Even with /cache deny all/
> directive, persistence?wasn't working when I peeked at the sslBump step
> 1 and then Bumped.?
> Persistence worked only when I directly did sslbump allow all (without
> peeking at first step).

Bumping step should not affect connection persistence AFAICT. I do not
know why it does in your case. This could be a Squid bug or a
misunderstanding. If you can reproduce, Squid logs should contain
reasons for each connection closure (but it may be difficult to find).


HTH,

Alex.


From squid3 at treenet.co.nz  Thu Jul 19 08:35:27 2018
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 19 Jul 2018 20:35:27 +1200
Subject: [squid-users] ERROR: Unknown TLS option clientca
In-Reply-To: <CAJbsrUm78T7fjsed8mygy7-VMa1-hsyrXAs-TUyfwKNYCKoDhg@mail.gmail.com>
References: <CAJbsrUkfukNyFsP3wBKu_1Sk+0SYaGa8H1JwKk41o=A197ymoA@mail.gmail.com>
 <CAJbsrU=5hQeM9kAuibggPFSLSeQ3W4wmPst4rqSkC5E5EDZoTw@mail.gmail.com>
 <08764746-faf1-0aee-56ef-7accb2841dd3@measurement-factory.com>
 <CAJbsrUnb+g59M900OeWzyNfq9KLCGP1FQs=Q6zTpxxmcn5b4VA@mail.gmail.com>
 <38f31e74-ec64-e4a9-09c4-3df37fe270de@measurement-factory.com>
 <CAJbsrUnfY2z0H9Q+ebqwGBGU-QQooRcVyW-0h9Qtopw1Gr9X3A@mail.gmail.com>
 <273039d9-83c4-03c3-1caa-6bd9712e4b37@treenet.co.nz>
 <CAJbsrU=nvZcZKS5tNyrxm1PTUTsRsnYjGmjUSx-uH2GX4keYLw@mail.gmail.com>
 <CAJbsrUm78T7fjsed8mygy7-VMa1-hsyrXAs-TUyfwKNYCKoDhg@mail.gmail.com>
Message-ID: <cdb88da6-045f-a4ea-50da-d5a78bb27e94@treenet.co.nz>

On 18/07/18 23:54, login mogin wrote:
> Hi there,
> 
> I have just tried with the patch and it is still not working. Do you
> want any particular log or debug output?
> 

If you could provide the cache.log output with:
  debug_options ALL,1 3, 5, 83,9

... and a full-data packet trace of the TLS handshake.

There may be more clues as to what is happening in there.

(you can post that to me privately if you wish).

Amos


From tiraen at gmail.com  Thu Jul 19 11:31:22 2018
From: tiraen at gmail.com (Tiraen)
Date: Thu, 19 Jul 2018 14:31:22 +0300
Subject: [squid-users] Question about traffic calculate
In-Reply-To: <85ef0832-de5d-32bf-26ef-85b34ba8366b@measurement-factory.com>
References: <CANhj9oyvYkuxk5pWTT=FPFggZ2kmcFa+Gu3yUXvNiURmouCxmg@mail.gmail.com>
 <fa5edf17-c0ed-4ac1-c5fe-b77839752dfc@treenet.co.nz>
 <CANhj9oywS173VafQhNa1S4_qznc-hmadoOiSRLH1HEPef_ORdQ@mail.gmail.com>
 <CANhj9ozSz6k0HMFYHUQLYw473j-NP8Vau=j7-akephH-AGO+ow@mail.gmail.com>
 <af7646cf-0fe6-6194-16d7-3fcb3f4622e1@treenet.co.nz>
 <df41334f-0abf-2b10-2ae1-047325461c09@integrafin.co.uk>
 <CANhj9ozzyfO8m12R5BTRRhq5K+6ojaNFzNSapV_jfPifnwFZOQ@mail.gmail.com>
 <CANhj9oxpuTwtTEM1MdXeM=j0CNda+rw3RzxCfg4UhWTqczmW9w@mail.gmail.com>
 <20180613095453.GA26516@fantomas.sk>
 <CANhj9oze9XznmdBSFAU_qct7v_HdFm45ar-caVhi=rys5epyog@mail.gmail.com>
 <20180613130911.GA2947@fantomas.sk>
 <df89473e-4d15-0566-e800-b0431031517e@measurement-factory.com>
 <CANhj9ow_5j21bhmmudz4xvsqD=EZJNeLQ7MmZpvu9QesRyUXmw@mail.gmail.com>
 <85ef0832-de5d-32bf-26ef-85b34ba8366b@measurement-factory.com>
Message-ID: <CANhj9oyreFNxQpTMW7v768BwpJxTMS0Zin=51ZffNaKEcp4xXQ@mail.gmail.com>

live access.log streams is probably the most efficient way of doing this.

Concerning this moment

So in the logs only one half of the traffic, and if the incoming + outgoing

https://alter.org.ua/soft/fbsd/squid_tot_sz/

All the patches I found are related to the old versions of the SQUID
for 3.5 this is not

2018-06-21 19:20 GMT+03:00 Alex Rousskov <rousskov at measurement-factory.com>:

> On 06/21/2018 05:14 AM, Tiraen wrote:
> > where i can read more about this (I mean the development of custom
> > ICAP/eCAP modules and their connection to the proxy) ?
>
> The best place to start is probably
> https://wiki.squid-cache.org/SquidFaq/ContentAdaptation
>
> If you decide to go the ICAP route, you will need to find the right ICAP
> server for your project. After that, the development will revolve around
> writing a custom adapter for that ICAP server. The above URL links to a
> page with a list of ICAP servers:
> https://wiki.squid-cache.org/Features/ICAP
>
> If you decide to go the eCAP route, you will need to (find somebody to)
> write an eCAP adapter (no server required).
>
> In either case, the required development is similar to writing a plugin
> or loadable module. Any capable developer can do it, but understanding
> of HTTP concepts and familiarity with the ICAP server or eCAP API helps.
>
>
> HTH,
>
> Alex.
>
>
> > 2018-06-13 18:35 GMT+03:00 Alex Rousskov:
> >
> >     On 06/13/2018 07:09 AM, Matus UHLAR - fantomas wrote:
> >     > On 13.06.18 13:26, Tiraen wrote:
> >     >> ICAP will help provide data on incoming / outgoing traffic?
> >
> >     > icap can get the data and work with it.
> >     > you don't have to manipulate, just do the accounting.
> >     > you just need ICAP module that will do it.
> >
> >
> >     Yes, it is possible to collect more-or-less accurate incoming request
> >     and incoming response stats using an ICAP service, but doing so
> would be
> >     very inefficient. Using eCAP would improve performance, but
> interpreting
> >     live access.log streams is probably the most efficient way of doing
> >     this.
> >
> >     IIRC, both eCAP and ICAP interfaces do not see the exact incoming
> >     requests and incoming responses because Squid may strip hop-by-hop
> HTTP
> >     headers and decode chunked HTTP message bodies before forwarding the
> >     incoming message to the adaptation service. If you need exact headers
> >     and exact body sizes, then you need more than just the basic ICAP and
> >     eCAP interface. Again, access.log is probably an overall better
> choice
> >     for capturing that info.
> >
> >     Both eCAP and ICAP interfaces do not see outgoing requests and
> outgoing
> >     responses because Squid only supports pre-cache vectoring points.
> >
> >
> >     HTH,
> >
> >     Alex.
> >     P.S. In the above, "incoming" is "to Squid" and "outgoing" is "from
> >     Squid".
> >
> >
> >     >> 2018-06-13 12:54 GMT+03:00 Matus UHLAR - fantomas <
> uhlar at fantomas.sk <mailto:uhlar at fantomas.sk>>:
> >     >>
> >     >>> On 13.06.18 11:51, Tiraen wrote:
> >     >>>
> >     >>>> either such a question, perhaps someone in the course
> >     >>>>
> >     >>>> in the SQUID is still not implemented radius accounting?
> >     >>>>
> >     >>>
> >     >>> authentication - yes. But squid doese not support accounting
> (afaik).
> >     >>>
> >     >>> Maybe there are any third-party modules working correctly?
> >     >>>>
> >     >>>
> >     >>> maybe iCAP module.
> >
> >
> >     _______________________________________________
> >     squid-users mailing list
> >     squid-users at lists.squid-cache.org
> >     <mailto:squid-users at lists.squid-cache.org>
> >     http://lists.squid-cache.org/listinfo/squid-users
> >     <http://lists.squid-cache.org/listinfo/squid-users>
> >
> >
> >
> >
> > --
> > With best regards,
> >
> > Vyacheslav Yakushev,
> >
> > Unix system administrator
> >
> > https://t.me/kelewind
>
>


-- 
With best regards,

Vyacheslav Yakushev,

Unix system administrator

https://t.me/kelewind
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20180719/249c811a/attachment.htm>

From michael.adm at gmail.com  Thu Jul 19 11:58:09 2018
From: michael.adm at gmail.com (Michael Pro)
Date: Thu, 19 Jul 2018 14:58:09 +0300
Subject: [squid-users] store_id_extras and http Request Headers
In-Reply-To: <8da965d4-81d7-cecb-a947-5e2acbf003af@measurement-factory.com>
References: <CAA+Mow4N+-72NEkYM=SFyF6GExW18Xuz_YWXVNLSWTMtuJZE5A@mail.gmail.com>
 <8da965d4-81d7-cecb-a947-5e2acbf003af@measurement-factory.com>
Message-ID: <CAA+Mow4d=No72K5WO5gCDz26DUk_DYnp9kX1vkuWUKaXrdz-Qw@mail.gmail.com>

Not only I have this problem
http://lists.squid-cache.org/pipermail/squid-users/2018-July/018637.html
Is it worthwhile in the near future to expect to fix this bug, or to
hook, I hope temporarily, a bunch of icap-> mysql <->
store_id_program?
??, 18 ???. 2018 ?. ? 23:35, Alex Rousskov <rousskov at measurement-factory.com>:
>
> On 07/18/2018 02:29 PM, Michael Pro wrote:
> > In squid.conf
> > store_id_extras "%>a/%>A %un %>rm %>h myip=%la myport=%lp
> > %{User-Agent}>h %{Referer}>h %{Host}>h %>rP"
> >
> > Result incoming parameters in store_id_program are:
> > 0:  https://2ip.ua/images/icon/IP_calculator.png <--- (requested url)
> > 1:  127.0.0.119/127.0.0.119  <--- %>a/%>A
> > 2:  -           <--- %un (this is ok, absent at all)
> > 3:  GET  <--- %>rm
> > 4:  -           <----- %>h ---> Where are they !?!?!?
> > 5:  myip=127.0.0.1
> > 6:  myport=20990
> > 7:  Mozilla/5.0%20(iPhone;%20CPU%20iPhone%20OS%2...  <--- %{User-Agent}>h
> > 8:  https://2ip.ua/ru/  <--- {Referer}>h
> > 9:  2ip.ua  <--- %{Host}>h
> > 10: 443  <--- %>rP
> >
> > How do I get all the request header fields?
> > I do not need any separate, I need all fields in the request header.
>
> The lack of %>h output when %{Name}>h output is present looks like a
> Squid bug to me -- %>h output should be there as well.
>
> Alex.


From rousskov at measurement-factory.com  Thu Jul 19 14:46:27 2018
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Thu, 19 Jul 2018 08:46:27 -0600
Subject: [squid-users] store_id_extras and http Request Headers
In-Reply-To: <CAA+Mow4d=No72K5WO5gCDz26DUk_DYnp9kX1vkuWUKaXrdz-Qw@mail.gmail.com>
References: <CAA+Mow4N+-72NEkYM=SFyF6GExW18Xuz_YWXVNLSWTMtuJZE5A@mail.gmail.com>
 <8da965d4-81d7-cecb-a947-5e2acbf003af@measurement-factory.com>
 <CAA+Mow4d=No72K5WO5gCDz26DUk_DYnp9kX1vkuWUKaXrdz-Qw@mail.gmail.com>
Message-ID: <2fc15a11-4b4c-8b83-e6aa-8b69a50c9d44@measurement-factory.com>

On 07/19/2018 05:58 AM, Michael Pro wrote:
> Not only I have this problem
> http://lists.squid-cache.org/pipermail/squid-users/2018-July/018637.html
> Is it worthwhile in the near future to expect to fix this bug, or to
> hook, I hope temporarily, a bunch of icap-> mysql <->
> store_id_program?

I am not sure I understand the question -- only you can decide what is
worthwhile. FWIW, I am not aware of anybody working on this bug. In
fact, I am not sure this bug has even been properly reported (as
requested in the thread you linked to above).

https://wiki.squid-cache.org/SquidFaq/AboutSquid#How_to_add_a_new_Squid_feature.2C_enhance.2C_of_fix_something.3F

Alex.



> ??, 18 ???. 2018 ?. ? 23:35, Alex Rousskov <rousskov at measurement-factory.com>:
>>
>> On 07/18/2018 02:29 PM, Michael Pro wrote:
>>> In squid.conf
>>> store_id_extras "%>a/%>A %un %>rm %>h myip=%la myport=%lp
>>> %{User-Agent}>h %{Referer}>h %{Host}>h %>rP"
>>>
>>> Result incoming parameters in store_id_program are:
>>> 0:  https://2ip.ua/images/icon/IP_calculator.png <--- (requested url)
>>> 1:  127.0.0.119/127.0.0.119  <--- %>a/%>A
>>> 2:  -           <--- %un (this is ok, absent at all)
>>> 3:  GET  <--- %>rm
>>> 4:  -           <----- %>h ---> Where are they !?!?!?
>>> 5:  myip=127.0.0.1
>>> 6:  myport=20990
>>> 7:  Mozilla/5.0%20(iPhone;%20CPU%20iPhone%20OS%2...  <--- %{User-Agent}>h
>>> 8:  https://2ip.ua/ru/  <--- {Referer}>h
>>> 9:  2ip.ua  <--- %{Host}>h
>>> 10: 443  <--- %>rP
>>>
>>> How do I get all the request header fields?
>>> I do not need any separate, I need all fields in the request header.
>>
>> The lack of %>h output when %{Name}>h output is present looks like a
>> Squid bug to me -- %>h output should be there as well.
>>
>> Alex.



From michael.adm at gmail.com  Thu Jul 19 17:21:09 2018
From: michael.adm at gmail.com (Michael Pro)
Date: Thu, 19 Jul 2018 20:21:09 +0300
Subject: [squid-users] store_id_extras and http Request Headers
In-Reply-To: <2fc15a11-4b4c-8b83-e6aa-8b69a50c9d44@measurement-factory.com>
References: <CAA+Mow4N+-72NEkYM=SFyF6GExW18Xuz_YWXVNLSWTMtuJZE5A@mail.gmail.com>
 <8da965d4-81d7-cecb-a947-5e2acbf003af@measurement-factory.com>
 <CAA+Mow4d=No72K5WO5gCDz26DUk_DYnp9kX1vkuWUKaXrdz-Qw@mail.gmail.com>
 <2fc15a11-4b4c-8b83-e6aa-8b69a50c9d44@measurement-factory.com>
Message-ID: <CAA+Mow7Lxcg_wdRPEF47VYb4Q1T52P2v_gdW1waoni4UEDhavA@mail.gmail.com>

On your recommendation posted it in bug-story of squid
https://bugs.squid-cache.org/show_bug.cgi?id=4873

We will wait for the release of the patch with impatience.
??, 19 ???. 2018 ?. ? 17:46, Alex Rousskov <rousskov at measurement-factory.com>:
>
> On 07/19/2018 05:58 AM, Michael Pro wrote:
> > Not only I have this problem
> > http://lists.squid-cache.org/pipermail/squid-users/2018-July/018637.html
> > Is it worthwhile in the near future to expect to fix this bug, or to
> > hook, I hope temporarily, a bunch of icap-> mysql <->
> > store_id_program?
>
> I am not sure I understand the question -- only you can decide what is
> worthwhile. FWIW, I am not aware of anybody working on this bug. In
> fact, I am not sure this bug has even been properly reported (as
> requested in the thread you linked to above).
>
> https://wiki.squid-cache.org/SquidFaq/AboutSquid#How_to_add_a_new_Squid_feature.2C_enhance.2C_of_fix_something.3F
>
> Alex.
>
>
>
> > ??, 18 ???. 2018 ?. ? 23:35, Alex Rousskov <rousskov at measurement-factory.com>:
> >>
> >> On 07/18/2018 02:29 PM, Michael Pro wrote:
> >>> In squid.conf
> >>> store_id_extras "%>a/%>A %un %>rm %>h myip=%la myport=%lp
> >>> %{User-Agent}>h %{Referer}>h %{Host}>h %>rP"
> >>>
> >>> Result incoming parameters in store_id_program are:
> >>> 0:  https://2ip.ua/images/icon/IP_calculator.png <--- (requested url)
> >>> 1:  127.0.0.119/127.0.0.119  <--- %>a/%>A
> >>> 2:  -           <--- %un (this is ok, absent at all)
> >>> 3:  GET  <--- %>rm
> >>> 4:  -           <----- %>h ---> Where are they !?!?!?
> >>> 5:  myip=127.0.0.1
> >>> 6:  myport=20990
> >>> 7:  Mozilla/5.0%20(iPhone;%20CPU%20iPhone%20OS%2...  <--- %{User-Agent}>h
> >>> 8:  https://2ip.ua/ru/  <--- {Referer}>h
> >>> 9:  2ip.ua  <--- %{Host}>h
> >>> 10: 443  <--- %>rP
> >>>
> >>> How do I get all the request header fields?
> >>> I do not need any separate, I need all fields in the request header.
> >>
> >> The lack of %>h output when %{Name}>h output is present looks like a
> >> Squid bug to me -- %>h output should be there as well.
> >>
> >> Alex.
>


From eliezer at ngtech.co.il  Thu Jul 19 18:03:05 2018
From: eliezer at ngtech.co.il (Eliezer Croitoru)
Date: Thu, 19 Jul 2018 21:03:05 +0300
Subject: [squid-users] question about squid and https connection .
In-Reply-To: <b3ab9209-a2fd-cc0d-e4d6-c3e4f61fc14e@measurement-factory.com>
References: <7866F6E4-CEC8-4513-A7FF-90DE4EC8EB8D@netstream.ps>
 <9da11737-ca55-e522-1150-cb9c26a079fb@measurement-factory.com>
 <03a101d41a1f$e9ce5970$bd6b0c50$@ngtech.co.il>
 <0e875c86-f1db-9f5d-5c9c-d95bd901f70d@measurement-factory.com>
 <044001d41ed5$42b80310$c8280930$@ngtech.co.il>
 <b3ab9209-a2fd-cc0d-e4d6-c3e4f61fc14e@measurement-factory.com>
Message-ID: <032901d41f8a$be409b40$3ac1d1c0$@ngtech.co.il>

OK so it doesn't make any sense to store so many copies of the "KEY" in the ssl_db/certs files..
I took a sample from my certs directory and extracted the keys that are stored at












]\
[root at squid4-testing 1]# ll
total 12
-rw-r--r--. 1 root root 1704 Jul 19 20:58 key1.pem
-rw-r--r--. 1 root root 1704 Jul 19 20:58 key2.pem
-rw-r--r--. 1 root root 1704 Jul 19 20:59 rootCA-key.pem
[root at squid4-testing 1]# cat key1.pem |sha256sum
3db2a55499015a4166f8059d378d79032ee85797f92176d7a4d5ad8a2025bec7  -
[root at squid4-testing 1]# cat key2.pem |sha256sum
3db2a55499015a4166f8059d378d79032ee85797f92176d7a4d5ad8a2025bec7  -
[root at squid4-testing 1]# cat rootCA-key.pem |sha256sum
3db2a55499015a4166f8059d378d79032ee85797f92176d7a4d5ad8a2025bec7  -

----
Eliezer Croitoru
Linux System Administrator
Mobile: +972-5-28704261
Email: eliezer at ngtech.co.il


-----Original Message-----
From: Alex Rousskov [mailto:rousskov at measurement-factory.com] 
Sent: Wednesday, July 18, 2018 11:45 PM
To: Eliezer Croitoru <eliezer at ngtech.co.il>; 'Squid Users' <squid-users at lists.squid-cache.org>
Subject: Re: [squid-users] question about squid and https connection .

On 07/18/2018 02:23 PM, Eliezer Croitoru wrote:


> Every certificate have the same properties of the original one except
> the "RSA key" part which it's certifiying.

Assuming you are talking about the generated certificates for the same
real certificate X, then yes, they will all have the same (mimicked)
fields. Whether they will be signed by the same CA depends on Squid
configuration. In my answers, I assumed that all those Squids are
configured with the same CA (including the same private key).


> So what I'm saying is that you cannot say that every certificate
> which will be created with the same CA will be the same for two
> different 2048 bits RSA keys.

... unless the keys are also the same, which was my and, AFAICT, OP
assumption.

Also, unless you are doing something nasty, it probably does not make
sense to configure a bumping Squid with a public CA certificate that is
identical to some other public CA certificate but has a different
private key. In other words, if you are using 200 Squids with a single
public CA certificate, then all those Squids should use the same private
key.

Alex.



> -----Original Message-----
> From: squid-users [mailto:squid-users-bounces at lists.squid-cache.org] On Behalf Of Alex Rousskov
> Sent: Friday, July 13, 2018 2:01 AM
> To: 'Squid Users' <squid-users at lists.squid-cache.org>
> Subject: Re: [squid-users] question about squid and https connection .
> 
> On 07/12/2018 02:35 PM, Eliezer Croitoru wrote:
> 
>> Every RSA key and certificate pair regardless to the origin server
>> and the SSL-BUMP enabled proxy can be different.
> 
> I cannot find a reasonable interpretation of the above that would
> contradict what I have said. Yes, each unique certificate has its own
> private key, but that is not what Ahmad was asking about AFAICT.
> 
> 
>> Will it be more accurate to say that just as long as these 200 squid
>> instances(different squid.conf and couple other local variables) use
>> the same exact ssl_db cache directory  then it's probable that they
>> will use the same certificate.
> 
> That statement is incorrect. Squids configured with different CA
> certificates will generate different fake certificates for the same real
> certificate.
> 
> I assume that Ahmad was asking about a situation where 200 Squid
> instances had the same configuration (including CA certificates).
> 
> Please note that the certificate generator helper gets the signing (CA)
> certificate as a parameter with each generation request (because
> different Squid ports may use different CA certificates). Also, Squid
> probably does not officially support sharing the certificate directory
> across Squid instances (even if it works).
> 
> 
>> Or these 200 squid instances are in SMP mode with 200 workers... If
>> these 200 instances do not share memory and certificate cache then
>> there is a possibility that the same site from two different sources 
>> will serve different certificates(due to the different RSA key which
>> is different).
> 
> 200 SMP workers or 200 identically-configured Squid instances will
> generate the same fake certificates for the same real certificate.
> "Stable certificates" is an important requirement for many distributed
> Squid deployments.
> 
> Alex.
> 
> 
> 
>> -----Original Message-----
>> From: squid-users [mailto:squid-users-bounces at lists.squid-cache.org] On Behalf Of Alex Rousskov
>> Sent: Thursday, July 12, 2018 11:27 PM
>> To: --Ahmad-- <ahmed.zaeem at netstream.ps>; Squid Users <squid-users at lists.squid-cache.org>
>> Subject: Re: [squid-users] question about squid and https connection .
>>
>> On 07/12/2018 01:17 PM, --Ahmad-- wrote:
>>
>>> if i have pc# 1 and that pc open facebook .
>>>
>>> then i have other pc # 2 and that other pc open facebook .
>>>
>>>
>>> now  as we know facebook is https .
>>>
>>> so is the key/ cert that used on pc # 1 is same as cert in pc # 2 to decrypt the fb encrypted traffic ?
>>
>> Certificates themselves are not used (directly) to decrypt traffic
>> AFAIK, but yes, both PCs will see the same server certificate (ignoring
>> CDNs and other complications).
>>
>>
>>
>>> now in the presence of squid .
>>>
>>> if i used tcp connect method  , will it be different than above ?
>>
>> If you are not bumping the connection, then both PCs will see the same
>> real Facebook certificate as if those PCs did not use a proxy.
>>
>> If you are bumping the connection, then both PCs will see the same fake
>> certificate generated by Squid.
>>
>>
>>
>>> say i used 200 proxies in same squid machine and i used to access FB from the same pc same browser .
>>>
>>> will facebook see my cert/key i used to decrypt its traffic ?
>>
>> If you are asking whether Facebook will know anything about the fake
>> certificate generated by Squid for clients, then the answer is "no,
>> unless Facebook runs some special client code to deliver (Squid)
>> certificate back to Facebook".
>>
>> In general, the origin server assumes that the client is talking to it
>> directly. Clients may pin or otherwise restrict certificates that they
>> trust, but after the connection is successfully established, the server
>> may assume that it is talking to the client directly. A paranoid server
>> may deliver special code to double check that assumption, but there are
>> other, more standard methods to prevent bumping such as certificate
>> pinning and certificate transparency cervices.
>>
>>
>>
>>> is the key/cert of FB to decrypt the https content is same on all browsers on all computers ?
>>
>> If you are asking whether the generated certificates are going to be the
>> same for all clients, then the answer is "yes, provided all those 200
>> Squids use the same configuration (including the CA certificate) and
>> receive the same real certificate from Facebook". Squid's certificate
>> generation algorithm generates the same certificate given the same
>> configuration and the same origin server certificate.
>>
>>
>> HTH,
>>
>> Alex.
>> _______________________________________________
>> squid-users mailing list
>> squid-users at lists.squid-cache.org
>> http://lists.squid-cache.org/listinfo/squid-users
>>
> 
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
> 




From eliezer at ngtech.co.il  Thu Jul 19 18:08:53 2018
From: eliezer at ngtech.co.il (Eliezer Croitoru)
Date: Thu, 19 Jul 2018 21:08:53 +0300
Subject: [squid-users] question about squid and https connection .
References: <7866F6E4-CEC8-4513-A7FF-90DE4EC8EB8D@netstream.ps>
 <9da11737-ca55-e522-1150-cb9c26a079fb@measurement-factory.com>
 <03a101d41a1f$e9ce5970$bd6b0c50$@ngtech.co.il>
 <0e875c86-f1db-9f5d-5c9c-d95bd901f70d@measurement-factory.com>
 <044001d41ed5$42b80310$c8280930$@ngtech.co.il>
 <b3ab9209-a2fd-cc0d-e4d6-c3e4f61fc14e@measurement-factory.com> 
Message-ID: <032b01d41f8b$8dc54d20$a94fe760$@ngtech.co.il>

Sorry a keyboard key broke while reviewing the text...

OK so it doesn't make any sense to store so many copies of the exact same "KEY" in the ssl_db/certs files..
I took a sample from my certs directory and extracted the keys that are stored at the QA server:
## Start
[root at squid4-testing 1]# ll
total 12
-rw-r--r--. 1 root root 1704 Jul 19 20:58 key1.pem
-rw-r--r--. 1 root root 1704 Jul 19 20:58 key2.pem
-rw-r--r--. 1 root root 1704 Jul 19 20:59 rootCA-key.pem
[root at squid4-testing 1]# cat key1.pem |sha256sum
3db2a55499015a4166f8059d378d79032ee85797f92176d7a4d5ad8a2025bec7  -
[root at squid4-testing 1]# cat key2.pem |sha256sum
3db2a55499015a4166f8059d378d79032ee85797f92176d7a4d5ad8a2025bec7  -
[root at squid4-testing 1]# cat rootCA-key.pem |sha256sum
3db2a55499015a4166f8059d378d79032ee85797f92176d7a4d5ad8a2025bec7
## END

So the ROOT CA key which squid is using is being used for all the fake certificates, why do we need so many copies of it?
I think that the helper and the DB store can be simplified or added simplicity for single servers.
For small servers this space is nothing but... for large systems it's an issue.
Also for embedded devices which every IO r/w counts before the flash/nand dies I think we can do something about it.

Thanks,
Eliezer

-----
Eliezer Croitoru
Linux System Administrator
Mobile: +972-5-28704261
Email: eliezer at ngtech.co.il


-----Original Message-----
From: Alex Rousskov [mailto:rousskov at measurement-factory.com]
Sent: Wednesday, July 18, 2018 11:45 PM
To: Eliezer Croitoru <eliezer at ngtech.co.il>; 'Squid Users' <squid-users at lists.squid-cache.org>
Subject: Re: [squid-users] question about squid and https connection .

On 07/18/2018 02:23 PM, Eliezer Croitoru wrote:


> Every certificate have the same properties of the original one except 
> the "RSA key" part which it's certifiying.

Assuming you are talking about the generated certificates for the same real certificate X, then yes, they will all have the same (mimicked) fields. Whether they will be signed by the same CA depends on Squid configuration. In my answers, I assumed that all those Squids are configured with the same CA (including the same private key).


> So what I'm saying is that you cannot say that every certificate which 
> will be created with the same CA will be the same for two different 
> 2048 bits RSA keys.

... unless the keys are also the same, which was my and, AFAICT, OP assumption.

Also, unless you are doing something nasty, it probably does not make sense to configure a bumping Squid with a public CA certificate that is identical to some other public CA certificate but has a different private key. In other words, if you are using 200 Squids with a single public CA certificate, then all those Squids should use the same private key.

Alex.



> -----Original Message-----
> From: squid-users [mailto:squid-users-bounces at lists.squid-cache.org] 
> On Behalf Of Alex Rousskov
> Sent: Friday, July 13, 2018 2:01 AM
> To: 'Squid Users' <squid-users at lists.squid-cache.org>
> Subject: Re: [squid-users] question about squid and https connection .
> 
> On 07/12/2018 02:35 PM, Eliezer Croitoru wrote:
> 
>> Every RSA key and certificate pair regardless to the origin server 
>> and the SSL-BUMP enabled proxy can be different.
> 
> I cannot find a reasonable interpretation of the above that would 
> contradict what I have said. Yes, each unique certificate has its own 
> private key, but that is not what Ahmad was asking about AFAICT.
> 
> 
>> Will it be more accurate to say that just as long as these 200 squid 
>> instances(different squid.conf and couple other local variables) use 
>> the same exact ssl_db cache directory  then it's probable that they 
>> will use the same certificate.
> 
> That statement is incorrect. Squids configured with different CA 
> certificates will generate different fake certificates for the same 
> real certificate.
> 
> I assume that Ahmad was asking about a situation where 200 Squid 
> instances had the same configuration (including CA certificates).
> 
> Please note that the certificate generator helper gets the signing 
> (CA) certificate as a parameter with each generation request (because 
> different Squid ports may use different CA certificates). Also, Squid 
> probably does not officially support sharing the certificate directory 
> across Squid instances (even if it works).
> 
> 
>> Or these 200 squid instances are in SMP mode with 200 workers... If 
>> these 200 instances do not share memory and certificate cache then 
>> there is a possibility that the same site from two different sources 
>> will serve different certificates(due to the different RSA key which 
>> is different).
> 
> 200 SMP workers or 200 identically-configured Squid instances will 
> generate the same fake certificates for the same real certificate.
> "Stable certificates" is an important requirement for many distributed 
> Squid deployments.
> 
> Alex.
> 
> 
> 
>> -----Original Message-----
>> From: squid-users [mailto:squid-users-bounces at lists.squid-cache.org] 
>> On Behalf Of Alex Rousskov
>> Sent: Thursday, July 12, 2018 11:27 PM
>> To: --Ahmad-- <ahmed.zaeem at netstream.ps>; Squid Users 
>> <squid-users at lists.squid-cache.org>
>> Subject: Re: [squid-users] question about squid and https connection .
>>
>> On 07/12/2018 01:17 PM, --Ahmad-- wrote:
>>
>>> if i have pc# 1 and that pc open facebook .
>>>
>>> then i have other pc # 2 and that other pc open facebook .
>>>
>>>
>>> now  as we know facebook is https .
>>>
>>> so is the key/ cert that used on pc # 1 is same as cert in pc # 2 to decrypt the fb encrypted traffic ?
>>
>> Certificates themselves are not used (directly) to decrypt traffic 
>> AFAIK, but yes, both PCs will see the same server certificate 
>> (ignoring CDNs and other complications).
>>
>>
>>
>>> now in the presence of squid .
>>>
>>> if i used tcp connect method  , will it be different than above ?
>>
>> If you are not bumping the connection, then both PCs will see the 
>> same real Facebook certificate as if those PCs did not use a proxy.
>>
>> If you are bumping the connection, then both PCs will see the same 
>> fake certificate generated by Squid.
>>
>>
>>
>>> say i used 200 proxies in same squid machine and i used to access FB from the same pc same browser .
>>>
>>> will facebook see my cert/key i used to decrypt its traffic ?
>>
>> If you are asking whether Facebook will know anything about the fake 
>> certificate generated by Squid for clients, then the answer is "no, 
>> unless Facebook runs some special client code to deliver (Squid) 
>> certificate back to Facebook".
>>
>> In general, the origin server assumes that the client is talking to 
>> it directly. Clients may pin or otherwise restrict certificates that 
>> they trust, but after the connection is successfully established, the 
>> server may assume that it is talking to the client directly. A 
>> paranoid server may deliver special code to double check that 
>> assumption, but there are other, more standard methods to prevent 
>> bumping such as certificate pinning and certificate transparency cervices.
>>
>>
>>
>>> is the key/cert of FB to decrypt the https content is same on all browsers on all computers ?
>>
>> If you are asking whether the generated certificates are going to be 
>> the same for all clients, then the answer is "yes, provided all those 
>> 200 Squids use the same configuration (including the CA certificate) 
>> and receive the same real certificate from Facebook". Squid's 
>> certificate generation algorithm generates the same certificate given 
>> the same configuration and the same origin server certificate.
>>
>>
>> HTH,
>>
>> Alex.
>> _______________________________________________
>> squid-users mailing list
>> squid-users at lists.squid-cache.org
>> http://lists.squid-cache.org/listinfo/squid-users
>>
> 
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
> 




From eliezer at ngtech.co.il  Thu Jul 19 18:28:51 2018
From: eliezer at ngtech.co.il (Eliezer Croitoru)
Date: Thu, 19 Jul 2018 21:28:51 +0300
Subject: [squid-users] store_id_extras and http Request Headers
In-Reply-To: <CAA+Mow7Lxcg_wdRPEF47VYb4Q1T52P2v_gdW1waoni4UEDhavA@mail.gmail.com>
References: <CAA+Mow4N+-72NEkYM=SFyF6GExW18Xuz_YWXVNLSWTMtuJZE5A@mail.gmail.com>
 <8da965d4-81d7-cecb-a947-5e2acbf003af@measurement-factory.com>
 <CAA+Mow4d=No72K5WO5gCDz26DUk_DYnp9kX1vkuWUKaXrdz-Qw@mail.gmail.com>
 <2fc15a11-4b4c-8b83-e6aa-8b69a50c9d44@measurement-factory.com>
 <CAA+Mow7Lxcg_wdRPEF47VYb4Q1T52P2v_gdW1waoni4UEDhavA@mail.gmail.com>
Message-ID: <034401d41f8e$57ac1720$07044560$@ngtech.co.il>

Thanks!

I didn't had much time to file the report due to some overload at work.
And I kind of overcome some level of this issue with a tiny ICAP REQMOD hack.
It's not the best solution but if it works and removes from the StoreID helper the
burden to reach any form of DB like MySql or Redis or MemCached it will remove many
blocking operations and will help the server and service work faster.

Eliezer

* I will try to follow up in the bug report.

----
Eliezer Croitoru
Linux System Administrator
Mobile: +972-5-28704261
Email: eliezer at ngtech.co.il



-----Original Message-----
From: squid-users [mailto:squid-users-bounces at lists.squid-cache.org] On Behalf Of Michael Pro
Sent: Thursday, July 19, 2018 8:21 PM
To: Alex Rousskov <rousskov at measurement-factory.com>
Cc: squid-users at lists.squid-cache.org
Subject: Re: [squid-users] store_id_extras and http Request Headers

On your recommendation posted it in bug-story of squid
https://bugs.squid-cache.org/show_bug.cgi?id=4873

We will wait for the release of the patch with impatience.
??, 19 ???. 2018 ?. ? 17:46, Alex Rousskov <rousskov at measurement-factory.com>:
>
> On 07/19/2018 05:58 AM, Michael Pro wrote:
> > Not only I have this problem
> > http://lists.squid-cache.org/pipermail/squid-users/2018-July/018637.html
> > Is it worthwhile in the near future to expect to fix this bug, or to
> > hook, I hope temporarily, a bunch of icap-> mysql <->
> > store_id_program?
>
> I am not sure I understand the question -- only you can decide what is
> worthwhile. FWIW, I am not aware of anybody working on this bug. In
> fact, I am not sure this bug has even been properly reported (as
> requested in the thread you linked to above).
>
> https://wiki.squid-cache.org/SquidFaq/AboutSquid#How_to_add_a_new_Squid_feature.2C_enhance.2C_of_fix_something.3F
>
> Alex.
>
>
>
> > ??, 18 ???. 2018 ?. ? 23:35, Alex Rousskov <rousskov at measurement-factory.com>:
> >>
> >> On 07/18/2018 02:29 PM, Michael Pro wrote:
> >>> In squid.conf
> >>> store_id_extras "%>a/%>A %un %>rm %>h myip=%la myport=%lp
> >>> %{User-Agent}>h %{Referer}>h %{Host}>h %>rP"
> >>>
> >>> Result incoming parameters in store_id_program are:
> >>> 0:  https://2ip.ua/images/icon/IP_calculator.png <--- (requested url)
> >>> 1:  127.0.0.119/127.0.0.119  <--- %>a/%>A
> >>> 2:  -           <--- %un (this is ok, absent at all)
> >>> 3:  GET  <--- %>rm
> >>> 4:  -           <----- %>h ---> Where are they !?!?!?
> >>> 5:  myip=127.0.0.1
> >>> 6:  myport=20990
> >>> 7:  Mozilla/5.0%20(iPhone;%20CPU%20iPhone%20OS%2...  <--- %{User-Agent}>h
> >>> 8:  https://2ip.ua/ru/  <--- {Referer}>h
> >>> 9:  2ip.ua  <--- %{Host}>h
> >>> 10: 443  <--- %>rP
> >>>
> >>> How do I get all the request header fields?
> >>> I do not need any separate, I need all fields in the request header.
> >>
> >> The lack of %>h output when %{Name}>h output is present looks like a
> >> Squid bug to me -- %>h output should be there as well.
> >>
> >> Alex.
>
_______________________________________________
squid-users mailing list
squid-users at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-users



From michael.adm at gmail.com  Thu Jul 19 19:02:50 2018
From: michael.adm at gmail.com (Michael Pro)
Date: Thu, 19 Jul 2018 22:02:50 +0300
Subject: [squid-users] store_id_extras to access request header
Message-ID: <CAA+Mow69UTv7NA-7N4dz8OuiaNNYOPj6wib4hbTC8Mkh1XFfeg@mail.gmail.com>

If you want, please support this topic in the bug-report
http://lists.squid-cache.org/pipermail/squid-users/2018-July/018743.html


From rousskov at measurement-factory.com  Thu Jul 19 20:29:16 2018
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Thu, 19 Jul 2018 14:29:16 -0600
Subject: [squid-users] question about squid and https connection .
In-Reply-To: <032b01d41f8b$8dc54d20$a94fe760$@ngtech.co.il>
References: <7866F6E4-CEC8-4513-A7FF-90DE4EC8EB8D@netstream.ps>
 <9da11737-ca55-e522-1150-cb9c26a079fb@measurement-factory.com>
 <03a101d41a1f$e9ce5970$bd6b0c50$@ngtech.co.il>
 <0e875c86-f1db-9f5d-5c9c-d95bd901f70d@measurement-factory.com>
 <044001d41ed5$42b80310$c8280930$@ngtech.co.il>
 <b3ab9209-a2fd-cc0d-e4d6-c3e4f61fc14e@measurement-factory.com>
 <032b01d41f8b$8dc54d20$a94fe760$@ngtech.co.il>
Message-ID: <72b19a9c-0f12-1565-27f1-bf14e96304e8@measurement-factory.com>

On 07/19/2018 12:08 PM, Eliezer Croitoru wrote:

> So the ROOT CA key which squid is using is being used for all the fake certificates, why do we need so many copies of it?

FWIW, I cannot think of any reason to store the CA certificate key in
the database of generated certificates. That key is only used to sign a
freshly generated certificate, and the certificate generator never
regenerates certificates, so I do not see the need to reuse that CA key.

Alex.


> -----Original Message-----
> From: Alex Rousskov [mailto:rousskov at measurement-factory.com]
> Sent: Wednesday, July 18, 2018 11:45 PM
> To: Eliezer Croitoru <eliezer at ngtech.co.il>; 'Squid Users' <squid-users at lists.squid-cache.org>
> Subject: Re: [squid-users] question about squid and https connection .
> 
> On 07/18/2018 02:23 PM, Eliezer Croitoru wrote:
> 
> 
>> Every certificate have the same properties of the original one except 
>> the "RSA key" part which it's certifiying.
> 
> Assuming you are talking about the generated certificates for the same real certificate X, then yes, they will all have the same (mimicked) fields. Whether they will be signed by the same CA depends on Squid configuration. In my answers, I assumed that all those Squids are configured with the same CA (including the same private key).
> 
> 
>> So what I'm saying is that you cannot say that every certificate which 
>> will be created with the same CA will be the same for two different 
>> 2048 bits RSA keys.
> 
> ... unless the keys are also the same, which was my and, AFAICT, OP assumption.
> 
> Also, unless you are doing something nasty, it probably does not make sense to configure a bumping Squid with a public CA certificate that is identical to some other public CA certificate but has a different private key. In other words, if you are using 200 Squids with a single public CA certificate, then all those Squids should use the same private key.
> 
> Alex.
> 
> 
> 
>> -----Original Message-----
>> From: squid-users [mailto:squid-users-bounces at lists.squid-cache.org] 
>> On Behalf Of Alex Rousskov
>> Sent: Friday, July 13, 2018 2:01 AM
>> To: 'Squid Users' <squid-users at lists.squid-cache.org>
>> Subject: Re: [squid-users] question about squid and https connection .
>>
>> On 07/12/2018 02:35 PM, Eliezer Croitoru wrote:
>>
>>> Every RSA key and certificate pair regardless to the origin server 
>>> and the SSL-BUMP enabled proxy can be different.
>>
>> I cannot find a reasonable interpretation of the above that would 
>> contradict what I have said. Yes, each unique certificate has its own 
>> private key, but that is not what Ahmad was asking about AFAICT.
>>
>>
>>> Will it be more accurate to say that just as long as these 200 squid 
>>> instances(different squid.conf and couple other local variables) use 
>>> the same exact ssl_db cache directory  then it's probable that they 
>>> will use the same certificate.
>>
>> That statement is incorrect. Squids configured with different CA 
>> certificates will generate different fake certificates for the same 
>> real certificate.
>>
>> I assume that Ahmad was asking about a situation where 200 Squid 
>> instances had the same configuration (including CA certificates).
>>
>> Please note that the certificate generator helper gets the signing 
>> (CA) certificate as a parameter with each generation request (because 
>> different Squid ports may use different CA certificates). Also, Squid 
>> probably does not officially support sharing the certificate directory 
>> across Squid instances (even if it works).
>>
>>
>>> Or these 200 squid instances are in SMP mode with 200 workers... If 
>>> these 200 instances do not share memory and certificate cache then 
>>> there is a possibility that the same site from two different sources 
>>> will serve different certificates(due to the different RSA key which 
>>> is different).
>>
>> 200 SMP workers or 200 identically-configured Squid instances will 
>> generate the same fake certificates for the same real certificate.
>> "Stable certificates" is an important requirement for many distributed 
>> Squid deployments.
>>
>> Alex.
>>
>>
>>
>>> -----Original Message-----
>>> From: squid-users [mailto:squid-users-bounces at lists.squid-cache.org] 
>>> On Behalf Of Alex Rousskov
>>> Sent: Thursday, July 12, 2018 11:27 PM
>>> To: --Ahmad-- <ahmed.zaeem at netstream.ps>; Squid Users 
>>> <squid-users at lists.squid-cache.org>
>>> Subject: Re: [squid-users] question about squid and https connection .
>>>
>>> On 07/12/2018 01:17 PM, --Ahmad-- wrote:
>>>
>>>> if i have pc# 1 and that pc open facebook .
>>>>
>>>> then i have other pc # 2 and that other pc open facebook .
>>>>
>>>>
>>>> now  as we know facebook is https .
>>>>
>>>> so is the key/ cert that used on pc # 1 is same as cert in pc # 2 to decrypt the fb encrypted traffic ?
>>>
>>> Certificates themselves are not used (directly) to decrypt traffic 
>>> AFAIK, but yes, both PCs will see the same server certificate 
>>> (ignoring CDNs and other complications).
>>>
>>>
>>>
>>>> now in the presence of squid .
>>>>
>>>> if i used tcp connect method  , will it be different than above ?
>>>
>>> If you are not bumping the connection, then both PCs will see the 
>>> same real Facebook certificate as if those PCs did not use a proxy.
>>>
>>> If you are bumping the connection, then both PCs will see the same 
>>> fake certificate generated by Squid.
>>>
>>>
>>>
>>>> say i used 200 proxies in same squid machine and i used to access FB from the same pc same browser .
>>>>
>>>> will facebook see my cert/key i used to decrypt its traffic ?
>>>
>>> If you are asking whether Facebook will know anything about the fake 
>>> certificate generated by Squid for clients, then the answer is "no, 
>>> unless Facebook runs some special client code to deliver (Squid) 
>>> certificate back to Facebook".
>>>
>>> In general, the origin server assumes that the client is talking to 
>>> it directly. Clients may pin or otherwise restrict certificates that 
>>> they trust, but after the connection is successfully established, the 
>>> server may assume that it is talking to the client directly. A 
>>> paranoid server may deliver special code to double check that 
>>> assumption, but there are other, more standard methods to prevent 
>>> bumping such as certificate pinning and certificate transparency cervices.
>>>
>>>
>>>
>>>> is the key/cert of FB to decrypt the https content is same on all browsers on all computers ?
>>>
>>> If you are asking whether the generated certificates are going to be 
>>> the same for all clients, then the answer is "yes, provided all those 
>>> 200 Squids use the same configuration (including the CA certificate) 
>>> and receive the same real certificate from Facebook". Squid's 
>>> certificate generation algorithm generates the same certificate given 
>>> the same configuration and the same origin server certificate.
>>>
>>>
>>> HTH,
>>>
>>> Alex.
>>> _______________________________________________
>>> squid-users mailing list
>>> squid-users at lists.squid-cache.org
>>> http://lists.squid-cache.org/listinfo/squid-users
>>>
>>
>> _______________________________________________
>> squid-users mailing list
>> squid-users at lists.squid-cache.org
>> http://lists.squid-cache.org/listinfo/squid-users
>>
> 



From frio_cervesa at hotmail.com  Fri Jul 20 05:05:43 2018
From: frio_cervesa at hotmail.com (senor)
Date: Fri, 20 Jul 2018 05:05:43 +0000
Subject: [squid-users] FTP recursive directory CWD
Message-ID: <BY2PR17MB0182F052F4152CB49EE3767BF7510@BY2PR17MB0182.namprd17.prod.outlook.com>

Hi Guys,
I've run into an issue with proxy and FTP. Squid is doing more than a browser would do to display a directory listing but I'm not sure why.
The FTP site has a directory structure like /top/dir1/dir2/dir3/files where dir2 is not readable. Using a browser without proxy, you can navigate to /top/dir1/dir2/dir3/ and get a listing of files in dir3 as well as download any of them. Accessing via squid will fail and I've found through packet caps and poking around code that it does a CWD on each of the directories starting at top. It runs into dir2 where it has no permissions, receives and error and quits.

I'm sure there is a reason for recursively listing each directory but, in this case, I don't think a fail should be the end of the road. It may cause a specific possible future action unavailable. Is there a workaround? Is this something that should be submitted as a feature request? Am I misunderstanding something?

Enlightenment is appreciated. Thanks,

-Senor

[cid:part1.C62E7EC8.EA524190 at hotmail.com]
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20180720/d27d2556/attachment.htm>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: cleardot.gif
Type: image/gif
Size: 43 bytes
Desc: cleardot.gif
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20180720/d27d2556/attachment.gif>

From loginmogin at gmail.com  Fri Jul 20 05:46:12 2018
From: loginmogin at gmail.com (login mogin)
Date: Fri, 20 Jul 2018 08:46:12 +0300
Subject: [squid-users] ERROR: Unknown TLS option clientca
In-Reply-To: <cdb88da6-045f-a4ea-50da-d5a78bb27e94@treenet.co.nz>
References: <CAJbsrUkfukNyFsP3wBKu_1Sk+0SYaGa8H1JwKk41o=A197ymoA@mail.gmail.com>
 <CAJbsrU=5hQeM9kAuibggPFSLSeQ3W4wmPst4rqSkC5E5EDZoTw@mail.gmail.com>
 <08764746-faf1-0aee-56ef-7accb2841dd3@measurement-factory.com>
 <CAJbsrUnb+g59M900OeWzyNfq9KLCGP1FQs=Q6zTpxxmcn5b4VA@mail.gmail.com>
 <38f31e74-ec64-e4a9-09c4-3df37fe270de@measurement-factory.com>
 <CAJbsrUnfY2z0H9Q+ebqwGBGU-QQooRcVyW-0h9Qtopw1Gr9X3A@mail.gmail.com>
 <273039d9-83c4-03c3-1caa-6bd9712e4b37@treenet.co.nz>
 <CAJbsrU=nvZcZKS5tNyrxm1PTUTsRsnYjGmjUSx-uH2GX4keYLw@mail.gmail.com>
 <CAJbsrUm78T7fjsed8mygy7-VMa1-hsyrXAs-TUyfwKNYCKoDhg@mail.gmail.com>
 <cdb88da6-045f-a4ea-50da-d5a78bb27e94@treenet.co.nz>
Message-ID: <CAJbsrU=VWkxYxBEKLZRbceGzveOrU-TwSRJeZd5sUVjyZ+Vc=A@mail.gmail.com>

I have just checked with the debug_options and saw that
sslflags=DELAYED_AUTH made it skip the client cert request. Just commented
that on the config and now it works!

Thanks a lot!

Amos Jeffries <squid3 at treenet.co.nz>, 19 Tem 2018 Per, 11:35 tarihinde ?unu
yazd?:

> On 18/07/18 23:54, login mogin wrote:
> > Hi there,
> >
> > I have just tried with the patch and it is still not working. Do you
> > want any particular log or debug output?
> >
>
> If you could provide the cache.log output with:
>   debug_options ALL,1 3, 5, 83,9
>
> ... and a full-data packet trace of the TLS handshake.
>
> There may be more clues as to what is happening in there.
>
> (you can post that to me privately if you wish).
>
> Amos
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20180720/b42efd82/attachment.htm>

From Sarfaraz.Ahmad at deshaw.com  Fri Jul 20 06:45:22 2018
From: Sarfaraz.Ahmad at deshaw.com (Ahmad, Sarfaraz)
Date: Fri, 20 Jul 2018 06:45:22 +0000
Subject: [squid-users] Cache ran out of descriptors due to ICAP
 service/TCP SYNs ?
In-Reply-To: <3ee3d9d1-4db8-10ad-4746-c8e1bf33f434@treenet.co.nz>
References: <073d52e4d31040e59ffaa3ff5e4d2040@mbxtoa3.winmail.deshaw.com>
 <2243eaa3-2d71-3696-2b6b-d82d238bf309@treenet.co.nz>
 <32fb6ea566ba460db61a184cdd89888f@mbxtoa3.winmail.deshaw.com>
 <3ee3d9d1-4db8-10ad-4746-c8e1bf33f434@treenet.co.nz>
Message-ID: <2dc27d5a8752412c84e1204f206bcb00@mbxtoa3.winmail.deshaw.com>

Thanks for the explanation.
From your first email : 
>> "In normal operation it is not serious, but you are already into abnormal operation by the crashing. So not releasing sockets/FD fast enough makes the overall problem worse."
I see that all the relevant FDs are opened by squid-1 process.  If squid-1 crashes, won't the OS clean up its file descriptors. Why would the parent Squid process be bothered with these FDs? 
I don't see how frequent crashing slows down releasing sockets/FDs.  Can you please explain how this works ? Also I am not using SMP workers.

Regards,
Sarfaraz

-----Original Message-----
From: squid-users <squid-users-bounces at lists.squid-cache.org> On Behalf Of Amos Jeffries
Sent: Wednesday, July 18, 2018 9:23 PM
To: squid-users at lists.squid-cache.org
Subject: Re: [squid-users] Cache ran out of descriptors due to ICAP service/TCP SYNs ?

On 18/07/18 18:30, Ahmad, Sarfaraz wrote:
> Thanks for the reply. I haven't completely understood the revert and have a few more related questions.
> 
> I see these messages,
> Jul 17 19:21:14 proxy2.hyd.deshaw.com squid[5747]: suspending ICAP 
> service for too many failures Jul 17 19:21:14 proxy2.hyd.deshaw.com squid[5747]: optional ICAP service is suspended: icap://127.0.0.1:1344/reqmod [down,susp,fail11]
> 1)   If the ICAP service is unresponsive, Squid would not exhaust its file descriptors trying to reach the service again and again right (too many TCP SYNs for trying to connect to the ICAP service )? 
> 

Correct. It would not exhaust resources on *that* action. Other actions possibly resulting from that state existing are another matter entirely.


> 
> 
> Max Connections returned by the ICAP service is 16. And given my ICAP 
> settings, icap_enable on icap_service test_icap reqmod_precache 
> icap://127.0.0.1:1344/reqmod bypass=on routing=off on-overload=wait
> On-overload is set to "wait". The documentation says " * wait:   wait (in a FIFO queue) for an ICAP connection slot" . This means that a new TCP connection would not be attempted if max connections is reached right ? 
> 2)   Am I right in saying that if the ICAP service is underperforming or has failed, this won't lead a sudden increase in the open file descriptors with on-overload set to "wait" ?
> 

No. The side effects of the ICAP service not being used determine the possible outcomes there.


> 
> Also I have no way to explain the "connection reset by peer" messages.

Neither, given the details provided.


> Jul 13 11:23:18 <hostname> squid[13123]: Error negotiating SSL 
> connection on FD 1292: (104) Connection reset by peer Jul 13 11:23:18 
> <hostname> squid[13123]: Error negotiating SSL connection on FD 1631: 
> (104) Connection reset by peer Jul 13 11:35:17 <hostname> 
> squid[13123]: Error negotiating SSL connection on FD 1331: (104) 
> Connection reset by peer
> 
> I have a few proxies (running in separate virtual machines). All of them went unresponsive at around the same time, leading to an outage of the internet.
> I am using WCCPv2 to redirect from firewall to these proxies.  I checked the logs there and WCCP communication was not intermittent.
> The logs on the proxies are bombarded with " Error negotiating SSL connection on FD 1331: (104) Connection reset by peer " messages.

A strong sign that forwarding loops are occuring, or something cut a huge number of TCP connections at once.

Although syslog recording is limited by the network traffic. So situations of high network flooding its timestamps can be very inaccurate or unordered.


> Since the ICAP service in not SSL-protected I think these messages mostly imply receiving TCP RSTs from remote servers. (or could it be clients somehow ?).

Yes, another reason I am thinking along the lines of forwarding loops.

> Once I removed WCCP direction rules from the firewall, internet was
back up.
> This hints that something in this proxy pipeline was amiss and not with the internet link itself. I don't see any outages on that.

Nod. Keep in mind though that "proxy pipeline" includes the WCCP rules in the router, NAT rules on the proxy machine, proxy config, connection to/from the ICAP server, and NAT rules on the proxy machine outgoing, and WCCP rules on the router a second time.

So a lot of parts, most outside of Squid - any one of which can screw up the entire pathway.


> I am pretty sure ACLs weren't changed and there was no forwarding loop.
> What could possibly explain the connection reset by peer messages ? Even if the internet was down, that won't lead to TCP RSTs. 
> I cannot tie these TCP RSTs and the incoming requests getting held up and ultimately leading to FD exhaustion.

Too many possibilities to list here, and we do not have sufficient information. You need to track down exactly which software is generating them, and why.


> 
> You earlier said
>>> In normal operation it is not serious, but you are already into abnormal operation by the crashing. So not releasing sockets/FD fast enough makes the overall problem worse.
> If squid-1 is crashing and getting respawned, it will have its own 16K FD limit right, I wonder how the newer squid-1 serves older requests. Can you please elaborate on " So not releasing sockets/FD fast enough makes the overall problem worse." ?
> 

Depending on your OS there are per-process and process group limits. The latter may be applicable if you are using SMP workers.


Amos
_______________________________________________
squid-users mailing list
squid-users at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-users

From eliezer at ngtech.co.il  Fri Jul 20 09:04:45 2018
From: eliezer at ngtech.co.il (Eliezer Croitoru)
Date: Fri, 20 Jul 2018 12:04:45 +0300
Subject: [squid-users] question about squid and https connection .
In-Reply-To: <72b19a9c-0f12-1565-27f1-bf14e96304e8@measurement-factory.com>
References: <7866F6E4-CEC8-4513-A7FF-90DE4EC8EB8D@netstream.ps>
 <9da11737-ca55-e522-1150-cb9c26a079fb@measurement-factory.com>
 <03a101d41a1f$e9ce5970$bd6b0c50$@ngtech.co.il>
 <0e875c86-f1db-9f5d-5c9c-d95bd901f70d@measurement-factory.com>
 <044001d41ed5$42b80310$c8280930$@ngtech.co.il>
 <b3ab9209-a2fd-cc0d-e4d6-c3e4f61fc14e@measurement-factory.com>
 <032b01d41f8b$8dc54d20$a94fe760$@ngtech.co.il>
 <72b19a9c-0f12-1565-27f1-bf14e96304e8@measurement-factory.com>
Message-ID: <04fd01d42008$b4693d10$1d3bb730$@ngtech.co.il>

I think we can use MD5/SHA1/SHA256 or even CRC32 to show the "freshness" of the certificate.
Also this way the ssl_db folder will be free of the burden of tight 600 or 700 permissions.

Did I got it right?

Thanks,
Eliezer

----
Eliezer Croitoru
Linux System Administrator
Mobile: +972-5-28704261
Email: eliezer at ngtech.co.il



-----Original Message-----
From: Alex Rousskov [mailto:rousskov at measurement-factory.com] 
Sent: Thursday, July 19, 2018 11:29 PM
To: Eliezer Croitoru <eliezer at ngtech.co.il>; 'Squid Users' <squid-users at lists.squid-cache.org>
Subject: Re: [squid-users] question about squid and https connection .

On 07/19/2018 12:08 PM, Eliezer Croitoru wrote:

> So the ROOT CA key which squid is using is being used for all the fake certificates, why do we need so many copies of it?

FWIW, I cannot think of any reason to store the CA certificate key in
the database of generated certificates. That key is only used to sign a
freshly generated certificate, and the certificate generator never
regenerates certificates, so I do not see the need to reuse that CA key.

Alex.


> -----Original Message-----
> From: Alex Rousskov [mailto:rousskov at measurement-factory.com]
> Sent: Wednesday, July 18, 2018 11:45 PM
> To: Eliezer Croitoru <eliezer at ngtech.co.il>; 'Squid Users' <squid-users at lists.squid-cache.org>
> Subject: Re: [squid-users] question about squid and https connection .
> 
> On 07/18/2018 02:23 PM, Eliezer Croitoru wrote:
> 
> 
>> Every certificate have the same properties of the original one except 
>> the "RSA key" part which it's certifiying.
> 
> Assuming you are talking about the generated certificates for the same real certificate X, then yes, they will all have the same (mimicked) fields. Whether they will be signed by the same CA depends on Squid configuration. In my answers, I assumed that all those Squids are configured with the same CA (including the same private key).
> 
> 
>> So what I'm saying is that you cannot say that every certificate which 
>> will be created with the same CA will be the same for two different 
>> 2048 bits RSA keys.
> 
> ... unless the keys are also the same, which was my and, AFAICT, OP assumption.
> 
> Also, unless you are doing something nasty, it probably does not make sense to configure a bumping Squid with a public CA certificate that is identical to some other public CA certificate but has a different private key. In other words, if you are using 200 Squids with a single public CA certificate, then all those Squids should use the same private key.
> 
> Alex.
> 
> 
> 
>> -----Original Message-----
>> From: squid-users [mailto:squid-users-bounces at lists.squid-cache.org] 
>> On Behalf Of Alex Rousskov
>> Sent: Friday, July 13, 2018 2:01 AM
>> To: 'Squid Users' <squid-users at lists.squid-cache.org>
>> Subject: Re: [squid-users] question about squid and https connection .
>>
>> On 07/12/2018 02:35 PM, Eliezer Croitoru wrote:
>>
>>> Every RSA key and certificate pair regardless to the origin server 
>>> and the SSL-BUMP enabled proxy can be different.
>>
>> I cannot find a reasonable interpretation of the above that would 
>> contradict what I have said. Yes, each unique certificate has its own 
>> private key, but that is not what Ahmad was asking about AFAICT.
>>
>>
>>> Will it be more accurate to say that just as long as these 200 squid 
>>> instances(different squid.conf and couple other local variables) use 
>>> the same exact ssl_db cache directory  then it's probable that they 
>>> will use the same certificate.
>>
>> That statement is incorrect. Squids configured with different CA 
>> certificates will generate different fake certificates for the same 
>> real certificate.
>>
>> I assume that Ahmad was asking about a situation where 200 Squid 
>> instances had the same configuration (including CA certificates).
>>
>> Please note that the certificate generator helper gets the signing 
>> (CA) certificate as a parameter with each generation request (because 
>> different Squid ports may use different CA certificates). Also, Squid 
>> probably does not officially support sharing the certificate directory 
>> across Squid instances (even if it works).
>>
>>
>>> Or these 200 squid instances are in SMP mode with 200 workers... If 
>>> these 200 instances do not share memory and certificate cache then 
>>> there is a possibility that the same site from two different sources 
>>> will serve different certificates(due to the different RSA key which 
>>> is different).
>>
>> 200 SMP workers or 200 identically-configured Squid instances will 
>> generate the same fake certificates for the same real certificate.
>> "Stable certificates" is an important requirement for many distributed 
>> Squid deployments.
>>
>> Alex.
>>
>>
>>
>>> -----Original Message-----
>>> From: squid-users [mailto:squid-users-bounces at lists.squid-cache.org] 
>>> On Behalf Of Alex Rousskov
>>> Sent: Thursday, July 12, 2018 11:27 PM
>>> To: --Ahmad-- <ahmed.zaeem at netstream.ps>; Squid Users 
>>> <squid-users at lists.squid-cache.org>
>>> Subject: Re: [squid-users] question about squid and https connection .
>>>
>>> On 07/12/2018 01:17 PM, --Ahmad-- wrote:
>>>
>>>> if i have pc# 1 and that pc open facebook .
>>>>
>>>> then i have other pc # 2 and that other pc open facebook .
>>>>
>>>>
>>>> now  as we know facebook is https .
>>>>
>>>> so is the key/ cert that used on pc # 1 is same as cert in pc # 2 to decrypt the fb encrypted traffic ?
>>>
>>> Certificates themselves are not used (directly) to decrypt traffic 
>>> AFAIK, but yes, both PCs will see the same server certificate 
>>> (ignoring CDNs and other complications).
>>>
>>>
>>>
>>>> now in the presence of squid .
>>>>
>>>> if i used tcp connect method  , will it be different than above ?
>>>
>>> If you are not bumping the connection, then both PCs will see the 
>>> same real Facebook certificate as if those PCs did not use a proxy.
>>>
>>> If you are bumping the connection, then both PCs will see the same 
>>> fake certificate generated by Squid.
>>>
>>>
>>>
>>>> say i used 200 proxies in same squid machine and i used to access FB from the same pc same browser .
>>>>
>>>> will facebook see my cert/key i used to decrypt its traffic ?
>>>
>>> If you are asking whether Facebook will know anything about the fake 
>>> certificate generated by Squid for clients, then the answer is "no, 
>>> unless Facebook runs some special client code to deliver (Squid) 
>>> certificate back to Facebook".
>>>
>>> In general, the origin server assumes that the client is talking to 
>>> it directly. Clients may pin or otherwise restrict certificates that 
>>> they trust, but after the connection is successfully established, the 
>>> server may assume that it is talking to the client directly. A 
>>> paranoid server may deliver special code to double check that 
>>> assumption, but there are other, more standard methods to prevent 
>>> bumping such as certificate pinning and certificate transparency cervices.
>>>
>>>
>>>
>>>> is the key/cert of FB to decrypt the https content is same on all browsers on all computers ?
>>>
>>> If you are asking whether the generated certificates are going to be 
>>> the same for all clients, then the answer is "yes, provided all those 
>>> 200 Squids use the same configuration (including the CA certificate) 
>>> and receive the same real certificate from Facebook". Squid's 
>>> certificate generation algorithm generates the same certificate given 
>>> the same configuration and the same origin server certificate.
>>>
>>>
>>> HTH,
>>>
>>> Alex.
>>> _______________________________________________
>>> squid-users mailing list
>>> squid-users at lists.squid-cache.org
>>> http://lists.squid-cache.org/listinfo/squid-users
>>>
>>
>> _______________________________________________
>> squid-users mailing list
>> squid-users at lists.squid-cache.org
>> http://lists.squid-cache.org/listinfo/squid-users
>>
> 




From rousskov at measurement-factory.com  Fri Jul 20 15:16:55 2018
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Fri, 20 Jul 2018 09:16:55 -0600
Subject: [squid-users] question about squid and https connection .
In-Reply-To: <04fd01d42008$b4693d10$1d3bb730$@ngtech.co.il>
References: <7866F6E4-CEC8-4513-A7FF-90DE4EC8EB8D@netstream.ps>
 <9da11737-ca55-e522-1150-cb9c26a079fb@measurement-factory.com>
 <03a101d41a1f$e9ce5970$bd6b0c50$@ngtech.co.il>
 <0e875c86-f1db-9f5d-5c9c-d95bd901f70d@measurement-factory.com>
 <044001d41ed5$42b80310$c8280930$@ngtech.co.il>
 <b3ab9209-a2fd-cc0d-e4d6-c3e4f61fc14e@measurement-factory.com>
 <032b01d41f8b$8dc54d20$a94fe760$@ngtech.co.il>
 <72b19a9c-0f12-1565-27f1-bf14e96304e8@measurement-factory.com>
 <04fd01d42008$b4693d10$1d3bb730$@ngtech.co.il>
Message-ID: <fb81c5d0-b2da-5d57-004a-b1af5d72b8e8@measurement-factory.com>

On 07/20/2018 03:04 AM, Eliezer Croitoru wrote:
> I think we can use MD5/SHA1/SHA256 or even CRC32 to show the "freshness" of the certificate.

Sorry, you lost me: I see no connection between the previous discussion
about CA keys and your new statement about something you call
certificate "freshness".


> Also this way the ssl_db folder will be free of the burden of tight 600 or 700 permissions.
> 
> Did I got it right?

The stored generated certificates include their private keys so the
database should use tight permissions.


Alex.


> -----Original Message-----
> From: Alex Rousskov [mailto:rousskov at measurement-factory.com] 
> Sent: Thursday, July 19, 2018 11:29 PM
> To: Eliezer Croitoru <eliezer at ngtech.co.il>; 'Squid Users' <squid-users at lists.squid-cache.org>
> Subject: Re: [squid-users] question about squid and https connection .
> 
> On 07/19/2018 12:08 PM, Eliezer Croitoru wrote:
> 
>> So the ROOT CA key which squid is using is being used for all the fake certificates, why do we need so many copies of it?
> 
> FWIW, I cannot think of any reason to store the CA certificate key in
> the database of generated certificates. That key is only used to sign a
> freshly generated certificate, and the certificate generator never
> regenerates certificates, so I do not see the need to reuse that CA key.
> 
> Alex.
> 
> 
>> -----Original Message-----
>> From: Alex Rousskov [mailto:rousskov at measurement-factory.com]
>> Sent: Wednesday, July 18, 2018 11:45 PM
>> To: Eliezer Croitoru <eliezer at ngtech.co.il>; 'Squid Users' <squid-users at lists.squid-cache.org>
>> Subject: Re: [squid-users] question about squid and https connection .
>>
>> On 07/18/2018 02:23 PM, Eliezer Croitoru wrote:
>>
>>
>>> Every certificate have the same properties of the original one except 
>>> the "RSA key" part which it's certifiying.
>>
>> Assuming you are talking about the generated certificates for the same real certificate X, then yes, they will all have the same (mimicked) fields. Whether they will be signed by the same CA depends on Squid configuration. In my answers, I assumed that all those Squids are configured with the same CA (including the same private key).
>>
>>
>>> So what I'm saying is that you cannot say that every certificate which 
>>> will be created with the same CA will be the same for two different 
>>> 2048 bits RSA keys.
>>
>> ... unless the keys are also the same, which was my and, AFAICT, OP assumption.
>>
>> Also, unless you are doing something nasty, it probably does not make sense to configure a bumping Squid with a public CA certificate that is identical to some other public CA certificate but has a different private key. In other words, if you are using 200 Squids with a single public CA certificate, then all those Squids should use the same private key.
>>
>> Alex.
>>
>>
>>
>>> -----Original Message-----
>>> From: squid-users [mailto:squid-users-bounces at lists.squid-cache.org] 
>>> On Behalf Of Alex Rousskov
>>> Sent: Friday, July 13, 2018 2:01 AM
>>> To: 'Squid Users' <squid-users at lists.squid-cache.org>
>>> Subject: Re: [squid-users] question about squid and https connection .
>>>
>>> On 07/12/2018 02:35 PM, Eliezer Croitoru wrote:
>>>
>>>> Every RSA key and certificate pair regardless to the origin server 
>>>> and the SSL-BUMP enabled proxy can be different.
>>>
>>> I cannot find a reasonable interpretation of the above that would 
>>> contradict what I have said. Yes, each unique certificate has its own 
>>> private key, but that is not what Ahmad was asking about AFAICT.
>>>
>>>
>>>> Will it be more accurate to say that just as long as these 200 squid 
>>>> instances(different squid.conf and couple other local variables) use 
>>>> the same exact ssl_db cache directory  then it's probable that they 
>>>> will use the same certificate.
>>>
>>> That statement is incorrect. Squids configured with different CA 
>>> certificates will generate different fake certificates for the same 
>>> real certificate.
>>>
>>> I assume that Ahmad was asking about a situation where 200 Squid 
>>> instances had the same configuration (including CA certificates).
>>>
>>> Please note that the certificate generator helper gets the signing 
>>> (CA) certificate as a parameter with each generation request (because 
>>> different Squid ports may use different CA certificates). Also, Squid 
>>> probably does not officially support sharing the certificate directory 
>>> across Squid instances (even if it works).
>>>
>>>
>>>> Or these 200 squid instances are in SMP mode with 200 workers... If 
>>>> these 200 instances do not share memory and certificate cache then 
>>>> there is a possibility that the same site from two different sources 
>>>> will serve different certificates(due to the different RSA key which 
>>>> is different).
>>>
>>> 200 SMP workers or 200 identically-configured Squid instances will 
>>> generate the same fake certificates for the same real certificate.
>>> "Stable certificates" is an important requirement for many distributed 
>>> Squid deployments.
>>>
>>> Alex.
>>>
>>>
>>>
>>>> -----Original Message-----
>>>> From: squid-users [mailto:squid-users-bounces at lists.squid-cache.org] 
>>>> On Behalf Of Alex Rousskov
>>>> Sent: Thursday, July 12, 2018 11:27 PM
>>>> To: --Ahmad-- <ahmed.zaeem at netstream.ps>; Squid Users 
>>>> <squid-users at lists.squid-cache.org>
>>>> Subject: Re: [squid-users] question about squid and https connection .
>>>>
>>>> On 07/12/2018 01:17 PM, --Ahmad-- wrote:
>>>>
>>>>> if i have pc# 1 and that pc open facebook .
>>>>>
>>>>> then i have other pc # 2 and that other pc open facebook .
>>>>>
>>>>>
>>>>> now  as we know facebook is https .
>>>>>
>>>>> so is the key/ cert that used on pc # 1 is same as cert in pc # 2 to decrypt the fb encrypted traffic ?
>>>>
>>>> Certificates themselves are not used (directly) to decrypt traffic 
>>>> AFAIK, but yes, both PCs will see the same server certificate 
>>>> (ignoring CDNs and other complications).
>>>>
>>>>
>>>>
>>>>> now in the presence of squid .
>>>>>
>>>>> if i used tcp connect method  , will it be different than above ?
>>>>
>>>> If you are not bumping the connection, then both PCs will see the 
>>>> same real Facebook certificate as if those PCs did not use a proxy.
>>>>
>>>> If you are bumping the connection, then both PCs will see the same 
>>>> fake certificate generated by Squid.
>>>>
>>>>
>>>>
>>>>> say i used 200 proxies in same squid machine and i used to access FB from the same pc same browser .
>>>>>
>>>>> will facebook see my cert/key i used to decrypt its traffic ?
>>>>
>>>> If you are asking whether Facebook will know anything about the fake 
>>>> certificate generated by Squid for clients, then the answer is "no, 
>>>> unless Facebook runs some special client code to deliver (Squid) 
>>>> certificate back to Facebook".
>>>>
>>>> In general, the origin server assumes that the client is talking to 
>>>> it directly. Clients may pin or otherwise restrict certificates that 
>>>> they trust, but after the connection is successfully established, the 
>>>> server may assume that it is talking to the client directly. A 
>>>> paranoid server may deliver special code to double check that 
>>>> assumption, but there are other, more standard methods to prevent 
>>>> bumping such as certificate pinning and certificate transparency cervices.
>>>>
>>>>
>>>>
>>>>> is the key/cert of FB to decrypt the https content is same on all browsers on all computers ?
>>>>
>>>> If you are asking whether the generated certificates are going to be 
>>>> the same for all clients, then the answer is "yes, provided all those 
>>>> 200 Squids use the same configuration (including the CA certificate) 
>>>> and receive the same real certificate from Facebook". Squid's 
>>>> certificate generation algorithm generates the same certificate given 
>>>> the same configuration and the same origin server certificate.
>>>>
>>>>
>>>> HTH,
>>>>
>>>> Alex.
>>>> _______________________________________________
>>>> squid-users mailing list
>>>> squid-users at lists.squid-cache.org
>>>> http://lists.squid-cache.org/listinfo/squid-users
>>>>
>>>
>>> _______________________________________________
>>> squid-users mailing list
>>> squid-users at lists.squid-cache.org
>>> http://lists.squid-cache.org/listinfo/squid-users
>>>
>>
> 



From capcoding at gmail.com  Sat Jul 21 21:10:46 2018
From: capcoding at gmail.com (Gordon Hsiao)
Date: Sat, 21 Jul 2018 16:10:46 -0500
Subject: [squid-users] squid 4.1 and domain fronting
Message-ID: <CAK0iFYw6SwwKnyi-MV8C3xBkJKys1R2Jb9kCAjPFuDPXXkgEBQ@mail.gmail.com>

I just read "RFC 2616 compliant proxy will rewrite the Host header making
it impossible to do domain fronting over HTTP or where SSL/TLS interception
is taking place", also checked RFC 2616 page at squid site, it is unclear
to me that if squid can enforce host-header consistence with SNI to avoid
domain fronting whenever needed? or this should be done by
c-icap/redirector under peek+bump mode?

Gordon
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20180721/61fd4f40/attachment.htm>

From capcoding at gmail.com  Sun Jul 22 01:21:05 2018
From: capcoding at gmail.com (Gordon Hsiao)
Date: Sat, 21 Jul 2018 20:21:05 -0500
Subject: [squid-users] squid 4.1 and domain fronting
In-Reply-To: <CAK0iFYw6SwwKnyi-MV8C3xBkJKys1R2Jb9kCAjPFuDPXXkgEBQ@mail.gmail.com>
References: <CAK0iFYw6SwwKnyi-MV8C3xBkJKys1R2Jb9kCAjPFuDPXXkgEBQ@mail.gmail.com>
Message-ID: <CAK0iFYzO=z2O-w8MrnpKmtsXVJ7bBvMVXQ47vYMZa0X=s6JAWQ@mail.gmail.com>

http://www.squid-cache.org/Versions/v4/cfgman/host_verify_strict.html

looks like squid did handle this already.

On Sat, Jul 21, 2018 at 4:10 PM Gordon Hsiao <capcoding at gmail.com> wrote:

> I just read "RFC 2616 compliant proxy will rewrite the Host header making
> it impossible to do domain fronting over HTTP or where SSL/TLS interception
> is taking place", also checked RFC 2616 page at squid site, it is unclear
> to me that if squid can enforce host-header consistence with SNI to avoid
> domain fronting whenever needed? or this should be done by
> c-icap/redirector under peek+bump mode?
>
> Gordon
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20180721/7c780b58/attachment.htm>

From ml at netfence.it  Sun Jul 22 13:29:49 2018
From: ml at netfence.it (Andrea Venturoli)
Date: Sun, 22 Jul 2018 15:29:49 +0200
Subject: [squid-users] Regression after upgrading 3.5.27 -> 4.1
Message-ID: <da0ed6e3-71ee-0bf8-8a67-3905a1b44e7a@netfence.it>

Hello.

I'm maintaining several installations on FreeBSD and I've been notified 
a specific web application is not working anymore after the upgrade.



Accessing this app with FireFox and Squid 3.5.27, it works correctly.

Doing the same after the upgrade to 4.1 lets the user arrive up to a 
point and then get a "Loading" message which will never go away.



Using FireFox network debugger, I see a couple of 400 error and in fact, 
if I try to open those URL I get:

> Invalid Request error was encountered while trying to process the request:
> 
> Some possible problems are:
> 
>     Missing or unknown request method.
> 
>     Missing HTTP Identifier (HTTP/1.0).
> 
>     Request is too large.
> 
>     Content-Length missing for POST or PUT requests.
> 
>     Illegal character in hostname; underscores are not allowed.
> 
>     HTTP/1.1 "Expect:" feature is being asked from an HTTP/1.0 software.



The above error is not quite informative (too broad) and there's nothing 
useful in the logs.

Here are those two URL (which unfortunately I have to partially obfuscate):

> http://xxxxxxxxxxx.xxxxxxxxxxx.xx/rest?method=navi_path.add&opera=I029&tipo=0&descr=XXXXXXXXX%20-%20Xxxxxxx%20xxxxxxxxxxx%20xxx%2030/12/2014%20-%20XXXX&xxxxx_xxxx=0&params={idDoc:%27C0002019%27,clasDoc:%27XXXXXX%27,nomeDoc:%27XXXXXXXXX%20-%20Xxxxxxx%20xxxxxxxxxxx%20xxx%2030/12/2014%20-%20XXXX%27,_X_TRACK_ID:%xxxxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx%27}&_ts=1532264445584&_dc=1532264445584

> http://xxxxxxxxxxx.xxxxxxxxxxx.xx/php/ajax/openDocumentREST.php?core=xxxxxxxXXXX&query={%22field%22:%22id%22,%22mode%22:%22EQUAL%22,%22value%22:%xxxxxxxxxx_XXXXXXXXX%22}&nomeTab=&arts=&toHighlight=&XXXXXXXX=I029

(the x and X are always alphanumeric characters).




I'm seeking help on how to better diagnose this: how can I find what 
Squid 4 does not like in those URLs?

None of the above causes seems to apply, IMVHO.

Has some default changed from 3.5 to 4.1 which might trigger this problem?



  bye & Thanks
	av.


From squid3 at treenet.co.nz  Mon Jul 23 00:59:55 2018
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Mon, 23 Jul 2018 12:59:55 +1200
Subject: [squid-users] Regression after upgrading 3.5.27 -> 4.1
In-Reply-To: <da0ed6e3-71ee-0bf8-8a67-3905a1b44e7a@netfence.it>
References: <da0ed6e3-71ee-0bf8-8a67-3905a1b44e7a@netfence.it>
Message-ID: <411cc4b5-0ea3-9b5f-4864-3841f4db6d25@treenet.co.nz>

On 23/07/18 01:29, Andrea Venturoli wrote:
> Hello.
> 
> I'm maintaining several installations on FreeBSD and I've been notified
> a specific web application is not working anymore after the upgrade.
> 
> 
> 
> Accessing this app with FireFox and Squid 3.5.27, it works correctly.
> 
> Doing the same after the upgrade to 4.1 lets the user arrive up to a
> point and then get a "Loading" message which will never go away.
> 
> 
> 
> Using FireFox network debugger, I see a couple of 400 error and in fact,
> if I try to open those URL I get:
> 
>> Invalid Request error was encountered while trying to process the
>> request:
>>
>> Some possible problems are:
>>
>> ??? Missing or unknown request method.
>>
>> ??? Missing HTTP Identifier (HTTP/1.0).
>>
>> ??? Request is too large.
>>
>> ??? Content-Length missing for POST or PUT requests.
>>
>> ??? Illegal character in hostname; underscores are not allowed.
>>
>> ??? HTTP/1.1 "Expect:" feature is being asked from an HTTP/1.0 software.
> 
> 
> 
> The above error is not quite informative (too broad)

There are a lot of things that can be wrong about requests sent. One of
them did. That list is not comprehensive either, just the things which
can be easily checked by users/admin without access to the proxy.

FYI: The template delivered has inline javascript for hiding the
messages that are irrelevant to this particular request. If you open the
URL in the browser (not debugging) it should reduce down to the ones
which are relevant.

You could also look at the debugger info abut the request message sent
and compare those values yourself.


> and there's nothing
> useful in the logs.
> 
> Here are those two URL (which unfortunately I have to partially obfuscate):
> 
>> http://xxxxxxxxxxx.xxxxxxxxxxx.xx/rest?method=navi_path.add&opera=I029&tipo=0&descr=XXXXXXXXX%20-%20Xxxxxxx%20xxxxxxxxxxx%20xxx%2030/12/2014%20-%20XXXX&xxxxx_xxxx=0&params={idDoc:%27C0002019%27,clasDoc:%27XXXXXX%27,nomeDoc:%27XXXXXXXXX%20-%20Xxxxxxx%20xxxxxxxxxxx%20xxx%2030/12/2014%20-%20XXXX%27,_X_TRACK_ID:%xxxxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx%27}&_ts=1532264445584&_dc=1532264445584
>>
> 
>> http://xxxxxxxxxxx.xxxxxxxxxxx.xx/php/ajax/openDocumentREST.php?core=xxxxxxxXXXX&query={%22field%22:%22id%22,%22mode%22:%22EQUAL%22,%22value%22:%xxxxxxxxxx_XXXXXXXXX%22}&nomeTab=&arts=&toHighlight=&XXXXXXXX=I029
>>
> 
> (the x and X are always alphanumeric characters).
> 

That is not very helpful info. If anyone here is going to use it we need
the actual full URL to run tests on ourselves.


> 
> I'm seeking help on how to better diagnose this: how can I find what
> Squid 4 does not like in those URLs?
> 

see above for initial things to check on.

For more details in cache.log configure:

 debug_options ALL,1 11,4 25,5, 33,5


This is best done on a test proxy where you don't have a flood of other
traffic happening in parallel.


> None of the above causes seems to apply, IMVHO.
> 
> Has some default changed from 3.5 to 4.1 which might trigger this problem?
> 

There is increased HTTP compliance, checking and handling. The things
which are configurable are all listed in the release notes AFAIK.

Amos


From ml at netfence.it  Mon Jul 23 10:38:12 2018
From: ml at netfence.it (Andrea Venturoli)
Date: Mon, 23 Jul 2018 12:38:12 +0200
Subject: [squid-users] Regression after upgrading 3.5.27 -> 4.1
In-Reply-To: <411cc4b5-0ea3-9b5f-4864-3841f4db6d25@treenet.co.nz>
References: <da0ed6e3-71ee-0bf8-8a67-3905a1b44e7a@netfence.it>
 <411cc4b5-0ea3-9b5f-4864-3841f4db6d25@treenet.co.nz>
Message-ID: <dd6ef0eb-5180-86c9-b609-c8a7928bd443@netfence.it>

On 7/23/18 2:59 AM, Amos Jeffries wrote:

> FYI: The template delivered has inline javascript for hiding the
> messages that are irrelevant to this particular request.

Sorry, I'm not sure I understand: template = squid's error page?



> If you open the
> URL in the browser (not debugging) it should reduce down to the ones
> which are relevant.

That's what I've done (and what I reported came after I did this).



> You could also look at the debugger info abut the request message sent
> and compare those values yourself.

Again, please forgive me... maybe I'm too ignorant about web 
applications, but I'm not understanding what you suggest I should do.



  bye & Thanks
	av.


From darvin at reduc.edu.cu  Mon Jul 23 18:16:40 2018
From: darvin at reduc.edu.cu (Darvin Rivera Aguilar)
Date: Mon, 23 Jul 2018 14:16:40 -0400
Subject: [squid-users] Strange error to load http web pages in parents
	servers.
In-Reply-To: <1e573553-c813-ff22-ef24-16b896802a97@reduc.edu.cu>
References: <1e573553-c813-ff22-ef24-16b896802a97@reduc.edu.cu>
Message-ID: <8e855c75-4372-6707-4991-74607acad93f@reduc.edu.cu>


In all Machines

OS: Debian
Version: 9.5
Squid Cache: Version 3.5.23

I have one private ip address for public squid (10.20.0.183) and two
parents squid: one for facebook and other for *.ch domain

 ??? ??? ??? ??? ??? ??? ??? ??? ??? ? ----> Parent1 (10.20.0.41) (Only Facebook)
 ??? ??? ??? ??? ??? ??? ??? ??? ??? ?|
Client -> Public Squid (10.20.0.183) -----> All other traffic
 ??? ??? ??? ??? ??? ??? ??? ??? ??? ?|
 ??? ??? ??? ??? ??? ??? ??? ??? ??? ? ----> Parent2 (10.20.0.42) (Only *.ch domain)

Parent1 and parent2 configuration are the same.


The Problem:

Browser Url: http://films.server.ch/Ingles/Baby Daddy/

Client Error:

ERROR
The requested URL could not be retrieved

The following error was encountered while trying to retrieve the URL:
/Ingles/Baby%20Daddy/

 ??? Invalid URL

Some aspect of the requested URL is incorrect.

Some possible problems are:

 ??? Missing or incorrect access protocol (should be "http://" or similar)

 ??? Missing hostname

 ??? Illegal double-escape in the URL-Path

 ??? Illegal character in hostname; underscores are not allowed.

Your cache administrator is webmaster.


Log Public Squid (10.20.0.183)
1531925462.144??? 525 10.20.1.12 TCP_MISS/400 3875 GET http://films.server.ch/Ingles/Baby%20Daddy/ username FIRSTUP_PARENT/10.20.0.42 text/html

Log Squid Parrent2 (10.520.0.42)
1531928082.425????? 0 10.20.0.183 TAG_NONE/400 3586 GET /Ingles/Baby%20Daddy/ - HIER_NONE/- text/html


Nota: I user parent1 for facebook and never give this error. Facebook
use https and the error is only in parent2 with http.

How the client solve this error:
When i push F5 in browser the the page reload with out problem,
sometimes i need to push 5 or 8 times F5 to page reload.

Parent2 Full config:

http_port 3128
httpd_suppress_version_string on
visible_hostname parent2.localhost
dns_nameservers 10.20.0.61
acl proxy src 0.20.0.183/32
http_access allow proxy
http_access deny all
cache_access_log /var/log/squid/access.log

Public Squid Basic config

http_port 10.20.0.183:3128
http_port 127.0.0.1:3128
httpd_suppress_version_string on

#====================================================================================
# TAG: Recommended minimum configuration
#====================================================================================
acl port_80 port 80
acl port_443 port 443

acl Safe_method method CONNECT GET HEAD POST
acl CONNECT???? method CONNECT

http_access allow manager localhost
http_access deny manager
http_access deny !Safe_ports
http_access deny !Safe_method
http_access deny CONNECT !SSL_ports

#====================================================================================
# TAG: PARENT
#====================================================================================
acl redir_facebook????? dstdom_regex??? -i "/etc/squid/acl/cache_peer_facebook"
acl db_domain?????????? dstdom_regex??? -i "/etc/squid/acl/cache_peer_domain"

cache_peer 10.20.0.41 parent 3128 0 default
cache_peer 10.20.0.42 parent 3128 0 default

cache_peer_access 10.20.0.41 allow redir_facebook
cache_peer_access 10.20.0.42 allow db_domain

never_direct allow redir_facebook
never_direct allow db_domain




From eliezer at ngtech.co.il  Tue Jul 24 00:44:46 2018
From: eliezer at ngtech.co.il (Eliezer Croitoru)
Date: Tue, 24 Jul 2018 03:44:46 +0300
Subject: [squid-users] question about squid and https connection .
In-Reply-To: <fb81c5d0-b2da-5d57-004a-b1af5d72b8e8@measurement-factory.com>
References: <7866F6E4-CEC8-4513-A7FF-90DE4EC8EB8D@netstream.ps>
 <9da11737-ca55-e522-1150-cb9c26a079fb@measurement-factory.com>
 <03a101d41a1f$e9ce5970$bd6b0c50$@ngtech.co.il>
 <0e875c86-f1db-9f5d-5c9c-d95bd901f70d@measurement-factory.com>
 <044001d41ed5$42b80310$c8280930$@ngtech.co.il>
 <b3ab9209-a2fd-cc0d-e4d6-c3e4f61fc14e@measurement-factory.com>
 <032b01d41f8b$8dc54d20$a94fe760$@ngtech.co.il>
 <72b19a9c-0f12-1565-27f1-bf14e96304e8@measurement-factory.com>
 <04fd01d42008$b4693d10$1d3bb730$@ngtech.co.il>
 <fb81c5d0-b2da-5d57-004a-b1af5d72b8e8@measurement-factory.com>
Message-ID: <075201d422e7$84fe38f0$8efaaad0$@ngtech.co.il>

OK so it makes more sense when you say it's intentional.

I do not agree with this approach and it's a bit off topic but I got my answer.

Thanks,
Eliezer

----
Eliezer Croitoru
Linux System Administrator
Mobile: +972-5-28704261
Email: eliezer at ngtech.co.il


-----Original Message-----
From: Alex Rousskov [mailto:rousskov at measurement-factory.com] 
Sent: Friday, July 20, 2018 6:17 PM
To: Eliezer Croitoru <eliezer at ngtech.co.il>; 'Squid Users' <squid-users at lists.squid-cache.org>
Subject: Re: [squid-users] question about squid and https connection .

On 07/20/2018 03:04 AM, Eliezer Croitoru wrote:
> I think we can use MD5/SHA1/SHA256 or even CRC32 to show the "freshness" of the certificate.

Sorry, you lost me: I see no connection between the previous discussion
about CA keys and your new statement about something you call
certificate "freshness".


> Also this way the ssl_db folder will be free of the burden of tight 600 or 700 permissions.
> 
> Did I got it right?

The stored generated certificates include their private keys so the
database should use tight permissions.


Alex.


> -----Original Message-----
> From: Alex Rousskov [mailto:rousskov at measurement-factory.com] 
> Sent: Thursday, July 19, 2018 11:29 PM
> To: Eliezer Croitoru <eliezer at ngtech.co.il>; 'Squid Users' <squid-users at lists.squid-cache.org>
> Subject: Re: [squid-users] question about squid and https connection .
> 
> On 07/19/2018 12:08 PM, Eliezer Croitoru wrote:
> 
>> So the ROOT CA key which squid is using is being used for all the fake certificates, why do we need so many copies of it?
> 
> FWIW, I cannot think of any reason to store the CA certificate key in
> the database of generated certificates. That key is only used to sign a
> freshly generated certificate, and the certificate generator never
> regenerates certificates, so I do not see the need to reuse that CA key.
> 
> Alex.
> 
> 
>> -----Original Message-----
>> From: Alex Rousskov [mailto:rousskov at measurement-factory.com]
>> Sent: Wednesday, July 18, 2018 11:45 PM
>> To: Eliezer Croitoru <eliezer at ngtech.co.il>; 'Squid Users' <squid-users at lists.squid-cache.org>
>> Subject: Re: [squid-users] question about squid and https connection .
>>
>> On 07/18/2018 02:23 PM, Eliezer Croitoru wrote:
>>
>>
>>> Every certificate have the same properties of the original one except 
>>> the "RSA key" part which it's certifiying.
>>
>> Assuming you are talking about the generated certificates for the same real certificate X, then yes, they will all have the same (mimicked) fields. Whether they will be signed by the same CA depends on Squid configuration. In my answers, I assumed that all those Squids are configured with the same CA (including the same private key).
>>
>>
>>> So what I'm saying is that you cannot say that every certificate which 
>>> will be created with the same CA will be the same for two different 
>>> 2048 bits RSA keys.
>>
>> ... unless the keys are also the same, which was my and, AFAICT, OP assumption.
>>
>> Also, unless you are doing something nasty, it probably does not make sense to configure a bumping Squid with a public CA certificate that is identical to some other public CA certificate but has a different private key. In other words, if you are using 200 Squids with a single public CA certificate, then all those Squids should use the same private key.
>>
>> Alex.
>>
>>
>>
>>> -----Original Message-----
>>> From: squid-users [mailto:squid-users-bounces at lists.squid-cache.org] 
>>> On Behalf Of Alex Rousskov
>>> Sent: Friday, July 13, 2018 2:01 AM
>>> To: 'Squid Users' <squid-users at lists.squid-cache.org>
>>> Subject: Re: [squid-users] question about squid and https connection .
>>>
>>> On 07/12/2018 02:35 PM, Eliezer Croitoru wrote:
>>>
>>>> Every RSA key and certificate pair regardless to the origin server 
>>>> and the SSL-BUMP enabled proxy can be different.
>>>
>>> I cannot find a reasonable interpretation of the above that would 
>>> contradict what I have said. Yes, each unique certificate has its own 
>>> private key, but that is not what Ahmad was asking about AFAICT.
>>>
>>>
>>>> Will it be more accurate to say that just as long as these 200 squid 
>>>> instances(different squid.conf and couple other local variables) use 
>>>> the same exact ssl_db cache directory  then it's probable that they 
>>>> will use the same certificate.
>>>
>>> That statement is incorrect. Squids configured with different CA 
>>> certificates will generate different fake certificates for the same 
>>> real certificate.
>>>
>>> I assume that Ahmad was asking about a situation where 200 Squid 
>>> instances had the same configuration (including CA certificates).
>>>
>>> Please note that the certificate generator helper gets the signing 
>>> (CA) certificate as a parameter with each generation request (because 
>>> different Squid ports may use different CA certificates). Also, Squid 
>>> probably does not officially support sharing the certificate directory 
>>> across Squid instances (even if it works).
>>>
>>>
>>>> Or these 200 squid instances are in SMP mode with 200 workers... If 
>>>> these 200 instances do not share memory and certificate cache then 
>>>> there is a possibility that the same site from two different sources 
>>>> will serve different certificates(due to the different RSA key which 
>>>> is different).
>>>
>>> 200 SMP workers or 200 identically-configured Squid instances will 
>>> generate the same fake certificates for the same real certificate.
>>> "Stable certificates" is an important requirement for many distributed 
>>> Squid deployments.
>>>
>>> Alex.
>>>
>>>
>>>
>>>> -----Original Message-----
>>>> From: squid-users [mailto:squid-users-bounces at lists.squid-cache.org] 
>>>> On Behalf Of Alex Rousskov
>>>> Sent: Thursday, July 12, 2018 11:27 PM
>>>> To: --Ahmad-- <ahmed.zaeem at netstream.ps>; Squid Users 
>>>> <squid-users at lists.squid-cache.org>
>>>> Subject: Re: [squid-users] question about squid and https connection .
>>>>
>>>> On 07/12/2018 01:17 PM, --Ahmad-- wrote:
>>>>
>>>>> if i have pc# 1 and that pc open facebook .
>>>>>
>>>>> then i have other pc # 2 and that other pc open facebook .
>>>>>
>>>>>
>>>>> now  as we know facebook is https .
>>>>>
>>>>> so is the key/ cert that used on pc # 1 is same as cert in pc # 2 to decrypt the fb encrypted traffic ?
>>>>
>>>> Certificates themselves are not used (directly) to decrypt traffic 
>>>> AFAIK, but yes, both PCs will see the same server certificate 
>>>> (ignoring CDNs and other complications).
>>>>
>>>>
>>>>
>>>>> now in the presence of squid .
>>>>>
>>>>> if i used tcp connect method  , will it be different than above ?
>>>>
>>>> If you are not bumping the connection, then both PCs will see the 
>>>> same real Facebook certificate as if those PCs did not use a proxy.
>>>>
>>>> If you are bumping the connection, then both PCs will see the same 
>>>> fake certificate generated by Squid.
>>>>
>>>>
>>>>
>>>>> say i used 200 proxies in same squid machine and i used to access FB from the same pc same browser .
>>>>>
>>>>> will facebook see my cert/key i used to decrypt its traffic ?
>>>>
>>>> If you are asking whether Facebook will know anything about the fake 
>>>> certificate generated by Squid for clients, then the answer is "no, 
>>>> unless Facebook runs some special client code to deliver (Squid) 
>>>> certificate back to Facebook".
>>>>
>>>> In general, the origin server assumes that the client is talking to 
>>>> it directly. Clients may pin or otherwise restrict certificates that 
>>>> they trust, but after the connection is successfully established, the 
>>>> server may assume that it is talking to the client directly. A 
>>>> paranoid server may deliver special code to double check that 
>>>> assumption, but there are other, more standard methods to prevent 
>>>> bumping such as certificate pinning and certificate transparency cervices.
>>>>
>>>>
>>>>
>>>>> is the key/cert of FB to decrypt the https content is same on all browsers on all computers ?
>>>>
>>>> If you are asking whether the generated certificates are going to be 
>>>> the same for all clients, then the answer is "yes, provided all those 
>>>> 200 Squids use the same configuration (including the CA certificate) 
>>>> and receive the same real certificate from Facebook". Squid's 
>>>> certificate generation algorithm generates the same certificate given 
>>>> the same configuration and the same origin server certificate.
>>>>
>>>>
>>>> HTH,
>>>>
>>>> Alex.
>>>> _______________________________________________
>>>> squid-users mailing list
>>>> squid-users at lists.squid-cache.org
>>>> http://lists.squid-cache.org/listinfo/squid-users
>>>>
>>>
>>> _______________________________________________
>>> squid-users mailing list
>>> squid-users at lists.squid-cache.org
>>> http://lists.squid-cache.org/listinfo/squid-users
>>>
>>
> 




From tarotapprentice at yahoo.com  Tue Jul 24 04:44:38 2018
From: tarotapprentice at yahoo.com (TarotApprentice)
Date: Tue, 24 Jul 2018 14:44:38 +1000
Subject: [squid-users] [squid-announce] Squid 4.1 is available
In-Reply-To: <178021e9-d104-184c-3125-52a8e669367d@treenet.co.nz>
References: <c8f9202f-47b1-c34e-5480-8191d57f8a2b@treenet.co.nz>
 <87CC8F2A-F402-48ED-BA32-5B54EE39B9C2@yahoo.com>
 <178021e9-d104-184c-3125-52a8e669367d@treenet.co.nz>
Message-ID: <7B32E6D8-4802-4100-8441-E6E1FA48F037@yahoo.com>

I?ve emailed the packaging team. The more people that ask the more likely they are to act on it so if anyone else is interested please drop them an email.

MarkJ

> On 6 Jul 2018, at 3:52 pm, Amos Jeffries <squid3 at treenet.co.nz> wrote:
> 
>> On 06/07/18 14:27, TarotApprentice wrote:
>> Hopefully the Debian guys will push this through to Stretch-backports
>> this time. 3.5.27 only made it as far as buster (testing).
>> Unfortunately libc 2.27 is in there and that meant it wanted to
>> update many other packages.
>> 
> 
> You can post a request to squid at packages.debian.org, or file a bug. One
> of the uploaders in the team may act earlier if they are aware of interest.
> 
> Amos



From belle at bazuin.nl  Tue Jul 24 08:04:26 2018
From: belle at bazuin.nl (=?windows-1252?Q?L.P.H._van_Belle?=)
Date: Tue, 24 Jul 2018 10:04:26 +0200
Subject: [squid-users] [squid-announce] Squid 4.1 is available
In-Reply-To: <7B32E6D8-4802-4100-8441-E6E1FA48F037@yahoo.com>
References: <178021e9-d104-184c-3125-52a8e669367d@treenet.co.nz>
Message-ID: <vmime.5b56dd8a.28fd.10d152c61e91452a@ms249-lin-003.rotterdam.bazuin.nl>

Hai, 

If people want, i've create debian stretch packages for squid 4.1 yesterday. 

I am testing them today and if you want to test them, you can find them here: 
https://downloads.van-belle.nl 

Drop me a note how they are working, if they are ok, i'll put them on my repo.

I took the Debian Unstable sources for this rebuild. 
Build logs etc are avaible through the link. 


Greetz, 

Louis
 

> -----Oorspronkelijk bericht-----
> Van: squid-users 
> [mailto:squid-users-bounces at lists.squid-cache.org] Namens 
> TarotApprentice
> Verzonden: dinsdag 24 juli 2018 6:45
> Aan: squid-users at lists.squid-cache.org
> Onderwerp: Re: [squid-users] [squid-announce] Squid 4.1 is available
> 
> I?ve emailed the packaging team. The more people that ask the 
> more likely they are to act on it so if anyone else is 
> interested please drop them an email.
> 
> MarkJ
> 
> > On 6 Jul 2018, at 3:52 pm, Amos Jeffries 
> <squid3 at treenet.co.nz> wrote:
> > 
> >> On 06/07/18 14:27, TarotApprentice wrote:
> >> Hopefully the Debian guys will push this through to 
> Stretch-backports
> >> this time. 3.5.27 only made it as far as buster (testing).
> >> Unfortunately libc 2.27 is in there and that meant it wanted to
> >> update many other packages.
> >> 
> > 
> > You can post a request to squid at packages.debian.org, or 
> file a bug. One
> > of the uploaders in the team may act earlier if they are 
> aware of interest.
> > 
> > Amos
> 
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
> 



From squid3 at treenet.co.nz  Tue Jul 24 10:42:40 2018
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Tue, 24 Jul 2018 22:42:40 +1200
Subject: [squid-users] Strange error to load http web pages in parents
 servers.
In-Reply-To: <8e855c75-4372-6707-4991-74607acad93f@reduc.edu.cu>
References: <1e573553-c813-ff22-ef24-16b896802a97@reduc.edu.cu>
 <8e855c75-4372-6707-4991-74607acad93f@reduc.edu.cu>
Message-ID: <91f32ea4-5f22-5cb6-9721-17b7785e2b0a@treenet.co.nz>

On 24/07/18 06:16, Darvin Rivera Aguilar wrote:
> 
> In all Machines
> 
> OS: Debian
> Version: 9.5
> Squid Cache: Version 3.5.23
> 
> I have one private ip address for public squid (10.20.0.183) and two
> parents squid: one for facebook and other for *.ch domain
> 
> ??? ??? ??? ??? ??? ??? ??? ??? ??? ? ----> Parent1 (10.20.0.41) (Only
> Facebook)
> ??? ??? ??? ??? ??? ??? ??? ??? ??? ?|
> Client -> Public Squid (10.20.0.183) -----> All other traffic
> ??? ??? ??? ??? ??? ??? ??? ??? ??? ?|
> ??? ??? ??? ??? ??? ??? ??? ??? ??? ? ----> Parent2 (10.20.0.42) (Only
> *.ch domain)
> 
> Parent1 and parent2 configuration are the same.
> 
> 
> The Problem:
> 
> Browser Url: http://films.server.ch/Ingles/Baby Daddy/
> 
> Client Error:
> 
> ERROR
> The requested URL could not be retrieved
> 
> The following error was encountered while trying to retrieve the URL:
> /Ingles/Baby%20Daddy/
> 
> ??? Invalid URL
> 
> Some aspect of the requested URL is incorrect.
> 
> Some possible problems are:
> 
> ??? Missing or incorrect access protocol (should be "http://" or similar)
> 

That.

> ??? Missing hostname
> 

... and that.


This is an origin-form URL for use only on port 80 or 443 message types.
Not for use between proxies. So where is it coming from?

eg. Is there something like NAT at the network level diverting outbound
port 80 traffic into this Parent2 proxy?

eg. Is the public proxy receiving an HTTP message with a domain that is
not in *.ch and which resolves to the IP address of the parent2 proxy?


You can see what each of the proxies is sending and receiving by setting
"debug_options 11,2" in squid.conf and reloading/reconfiguring Squid.



> ??? Illegal double-escape in the URL-Path
> 
> ??? Illegal character in hostname; underscores are not allowed.
> 
> Your cache administrator is webmaster.
> 
> 
> Log Public Squid (10.20.0.183)
> 1531925462.144??? 525 10.20.1.12 TCP_MISS/400 3875 GET
> http://films.server.ch/Ingles/Baby%20Daddy/ username
> FIRSTUP_PARENT/10.20.0.42 text/html
> 
> Log Squid Parrent2 (10.520.0.42)
> 1531928082.425????? 0 10.20.0.183 TAG_NONE/400 3586 GET
> /Ingles/Baby%20Daddy/ - HIER_NONE/- text/html

Notice that these requests are 2620 seconds (43 minutes) apart. So they
are almost certainly not the same transaction, even though they resulted
in the same status code - it _may_ have been for different reasons.

> 
> 
> Nota: I user parent1 for facebook and never give this error. Facebook
> use https and the error is only in parent2 with http.
> 
> How the client solve this error:
> When i push F5 in browser the the page reload with out problem,
> sometimes i need to push 5 or 8 times F5 to page reload.
> 
> Parent2 Full config:
> 
> http_port 3128
> httpd_suppress_version_string on
> visible_hostname parent2.localhost
> dns_nameservers 10.20.0.61
> acl proxy src 0.20.0.183/32

Is that "0." just a typo in your mail here? If its in the config that
could be the cause.

Otherwise I'm not seeing anything that could lead to your problem in
these two config files. The notes below are just some polishing things
you could maybe do better.


> http_access allow proxy
> http_access deny all
> cache_access_log /var/log/squid/access.log

This directive name is access_log in 3.x, no "cache_" part.

And it should have either "stdio:" or "daemon:" as prefix on the
filename depending on whether you expect low or high traffic volumes
(daemon is higher performance than stdio).


> 
> Public Squid Basic config
> 
> http_port 10.20.0.183:3128
> http_port 127.0.0.1:3128
> httpd_suppress_version_string on
> 
> #====================================================================================
> 
> # TAG: Recommended minimum configuration
> #====================================================================================
> 
> acl port_80 port 80
> acl port_443 port 443
> 
> acl Safe_method method CONNECT GET HEAD POST
> acl CONNECT???? method CONNECT
> 
> http_access allow manager localhost
> http_access deny manager
> http_access deny !Safe_ports
> http_access deny !Safe_method
> http_access deny CONNECT !SSL_ports
> 
> #====================================================================================
> 
> # TAG: PARENT
> #====================================================================================
> 
> acl redir_facebook????? dstdom_regex??? -i
> "/etc/squid/acl/cache_peer_facebook"
> acl db_domain?????????? dstdom_regex??? -i
> "/etc/squid/acl/cache_peer_domain"

You said this peer only services .ch domains.
That implies you can use something like:
  acl db_domain dstdomain .ch


> 
> cache_peer 10.20.0.41 parent 3128 0 default
> cache_peer 10.20.0.42 parent 3128 0 default
> 

"
==== PEER SELECTION METHODS ====

default

  This is a parent cache which can be used as a "last-resort"
  if a peer cannot be located by any of the peer-selection methods.

 *** If specified more than once, only the first is used. ***
"

Given what you described the purpose of these parents to be I don't
think either of them should have "default" option set.


> cache_peer_access 10.20.0.41 allow redir_facebook
> cache_peer_access 10.20.0.42 allow db_domain
> 
> never_direct allow redir_facebook
> never_direct allow db_domain
> 



Amos


From ml at netfence.it  Wed Jul 25 07:12:17 2018
From: ml at netfence.it (Andrea Venturoli)
Date: Wed, 25 Jul 2018 09:12:17 +0200
Subject: [squid-users] Regression after upgrading 3.5.27 -> 4.1
In-Reply-To: <da0ed6e3-71ee-0bf8-8a67-3905a1b44e7a@netfence.it>
References: <da0ed6e3-71ee-0bf8-8a67-3905a1b44e7a@netfence.it>
Message-ID: <8f45b9f4-d1e4-3964-e65b-a1ca1530df99@netfence.it>

On 7/22/18 3:29 PM, Andrea Venturoli wrote:

>> http://xxxxxxxxxxx.xxxxxxxxxxx.xx/rest?method=navi_path.add&opera=I029&tipo=0&descr=XXXXXXXXX%20-%20Xxxxxxx%20xxxxxxxxxxx%20xxx%2030/12/2014%20-%20XXXX&xxxxx_xxxx=0&params={idDoc:%27C0002019%27,clasDoc:%27XXXXXX%27,nomeDoc:%27XXXXXXXXX%20-%20Xxxxxxx%20xxxxxxxxxxx%20xxx%2030/12/2014%20-%20XXXX%27,_X_TRACK_ID:%xxxxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx%27}&_ts=1532264445584&_dc=1532264445584 

Upon furhter investigations, I see the problems are the curly braces.
If I encode them (changing { to %7B and } to %7D), the request is 
successful.

While I was not able to determine if that URL is valid (seems not 
according to old RFC1738, but maybe yes, according to newer RFCs), I 
have no control on that side.
All my users see is that this won't work with Squid, but will work without.



Was disallowing curly brackets a choice or is it a bug?
Perhaps there's some option to tweak?

  bye & Thanks
	av.


From rousskov at measurement-factory.com  Wed Jul 25 14:54:16 2018
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Wed, 25 Jul 2018 08:54:16 -0600
Subject: [squid-users] Regression after upgrading 3.5.27 -> 4.1
In-Reply-To: <8f45b9f4-d1e4-3964-e65b-a1ca1530df99@netfence.it>
References: <da0ed6e3-71ee-0bf8-8a67-3905a1b44e7a@netfence.it>
 <8f45b9f4-d1e4-3964-e65b-a1ca1530df99@netfence.it>
Message-ID: <18e6138c-6d03-75e5-2cf0-a3f4e2f784c5@measurement-factory.com>

On 07/25/2018 01:12 AM, Andrea Venturoli wrote:
> On 7/22/18 3:29 PM, Andrea Venturoli wrote:
> 
>>> http://xxxxxxxxxxx.xxxxxxxxxxx.xx/rest?method=navi_path.add&opera=I029&tipo=0&descr=XXXXXXXXX%20-%20Xxxxxxx%20xxxxxxxxxxx%20xxx%2030/12/2014%20-%20XXXX&xxxxx_xxxx=0&params={idDoc:%27C0002019%27,clasDoc:%27XXXXXX%27,nomeDoc:%27XXXXXXXXX%20-%20Xxxxxxx%20xxxxxxxxxxx%20xxx%2030/12/2014%20-%20XXXX%27,_X_TRACK_ID:%xxxxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx%27}&_ts=1532264445584&_dc=1532264445584
> 
> 
> Upon furhter investigations, I see the problems are the curly braces.

> Was disallowing curly brackets a choice or is it a bug?

If your relaxed_header_parser is on, and Squid rejects URLs because they
have curly braces in the path, then this is a Squid bug.

N.B. relaxed_header_parser is on by default.

Alex.


From ml at netfence.it  Wed Jul 25 15:55:22 2018
From: ml at netfence.it (Andrea Venturoli)
Date: Wed, 25 Jul 2018 17:55:22 +0200
Subject: [squid-users] Regression after upgrading 3.5.27 -> 4.1
In-Reply-To: <18e6138c-6d03-75e5-2cf0-a3f4e2f784c5@measurement-factory.com>
References: <da0ed6e3-71ee-0bf8-8a67-3905a1b44e7a@netfence.it>
 <8f45b9f4-d1e4-3964-e65b-a1ca1530df99@netfence.it>
 <18e6138c-6d03-75e5-2cf0-a3f4e2f784c5@measurement-factory.com>
Message-ID: <ad194e7c-85e9-81ed-08be-da5fd8153199@netfence.it>

On 7/25/18 4:54 PM, Alex Rousskov wrote:
> On 07/25/2018 01:12 AM, Andrea Venturoli wrote:
>> On 7/22/18 3:29 PM, Andrea Venturoli wrote:
>>
>>>> http://xxxxxxxxxxx.xxxxxxxxxxx.xx/rest?method=navi_path.add&opera=I029&tipo=0&descr=XXXXXXXXX%20-%20Xxxxxxx%20xxxxxxxxxxx%20xxx%2030/12/2014%20-%20XXXX&xxxxx_xxxx=0&params={idDoc:%27C0002019%27,clasDoc:%27XXXXXX%27,nomeDoc:%27XXXXXXXXX%20-%20Xxxxxxx%20xxxxxxxxxxx%20xxx%2030/12/2014%20-%20XXXX%27,_X_TRACK_ID:%xxxxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx%27}&_ts=1532264445584&_dc=1532264445584
>>
>>
>> Upon furhter investigations, I see the problems are the curly braces.
> 
>> Was disallowing curly brackets a choice or is it a bug?
> 
> If your relaxed_header_parser is on, and Squid rejects URLs because they
> have curly braces in the path, then this is a Squid bug.
> 
> N.B. relaxed_header_parser is on by default.

I have no such option in my squid.conf, so it should be on.
I added it just to be sure the default wasn't off for some reason, but 
it did not change.

So, should I file a bug on https://bugs.squid-cache.org?

  bye & Thanks
	av.


From squid3 at treenet.co.nz  Wed Jul 25 16:46:50 2018
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 26 Jul 2018 04:46:50 +1200
Subject: [squid-users] Regression after upgrading 3.5.27 -> 4.1
In-Reply-To: <ad194e7c-85e9-81ed-08be-da5fd8153199@netfence.it>
References: <da0ed6e3-71ee-0bf8-8a67-3905a1b44e7a@netfence.it>
 <8f45b9f4-d1e4-3964-e65b-a1ca1530df99@netfence.it>
 <18e6138c-6d03-75e5-2cf0-a3f4e2f784c5@measurement-factory.com>
 <ad194e7c-85e9-81ed-08be-da5fd8153199@netfence.it>
Message-ID: <9b72e0f3-0209-feb6-6072-69d382c10073@treenet.co.nz>

On 26/07/18 03:55, Andrea Venturoli wrote:
> On 7/25/18 4:54 PM, Alex Rousskov wrote:
>> On 07/25/2018 01:12 AM, Andrea Venturoli wrote:
>>> On 7/22/18 3:29 PM, Andrea Venturoli wrote:
>>>
>>>>> http://xxxxxxxxxxx.xxxxxxxxxxx.xx/rest?method=navi_path.add&opera=I029&tipo=0&descr=XXXXXXXXX%20-%20Xxxxxxx%20xxxxxxxxxxx%20xxx%2030/12/2014%20-%20XXXX&xxxxx_xxxx=0&params={idDoc:%27C0002019%27,clasDoc:%27XXXXXX%27,nomeDoc:%27XXXXXXXXX%20-%20Xxxxxxx%20xxxxxxxxxxx%20xxx%2030/12/2014%20-%20XXXX%27,_X_TRACK_ID:%xxxxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx%27}&_ts=1532264445584&_dc=1532264445584
>>>>>
>>>
>>>
>>> Upon furhter investigations, I see the problems are the curly braces.
>>
>>> Was disallowing curly brackets a choice or is it a bug?
>>
>> If your relaxed_header_parser is on, and Squid rejects URLs because they
>> have curly braces in the path, then this is a Squid bug.
>>
>> N.B. relaxed_header_parser is on by default.
> 
> I have no such option in my squid.conf, so it should be on.
> I added it just to be sure the default wasn't off for some reason, but
> it did not change.
> 
> So, should I file a bug on https://bugs.squid-cache.org?
> 

What is your "squid -v" output?

If --disable-http-violations is used then relaxed parser will not
include those "must never be transmitted in un-escaped form" (RFC 2396)
characters.


Amos


From ml at netfence.it  Wed Jul 25 17:07:28 2018
From: ml at netfence.it (Andrea Venturoli)
Date: Wed, 25 Jul 2018 19:07:28 +0200
Subject: [squid-users] Regression after upgrading 3.5.27 -> 4.1
In-Reply-To: <9b72e0f3-0209-feb6-6072-69d382c10073@treenet.co.nz>
References: <da0ed6e3-71ee-0bf8-8a67-3905a1b44e7a@netfence.it>
 <8f45b9f4-d1e4-3964-e65b-a1ca1530df99@netfence.it>
 <18e6138c-6d03-75e5-2cf0-a3f4e2f784c5@measurement-factory.com>
 <ad194e7c-85e9-81ed-08be-da5fd8153199@netfence.it>
 <9b72e0f3-0209-feb6-6072-69d382c10073@treenet.co.nz>
Message-ID: <cf9d9a0c-3e75-031e-00da-0bb359e17865@netfence.it>

On 7/25/18 6:46 PM, Amos Jeffries wrote:

> What is your "squid -v" output?
> 
> If --disable-http-violations is used then relaxed parser will not
> include those "must never be transmitted in un-escaped form" (RFC 2396)
> characters.

It's there!!!

Thanks for pointing me in the correct direction.
I'm off recompiling... will let you know if this solves.

  bye & Thanks
	av.


From ml at netfence.it  Thu Jul 26 07:47:28 2018
From: ml at netfence.it (Andrea Venturoli)
Date: Thu, 26 Jul 2018 09:47:28 +0200
Subject: [squid-users] Regression after upgrading 3.5.27 -> 4.1
In-Reply-To: <cf9d9a0c-3e75-031e-00da-0bb359e17865@netfence.it>
References: <da0ed6e3-71ee-0bf8-8a67-3905a1b44e7a@netfence.it>
 <8f45b9f4-d1e4-3964-e65b-a1ca1530df99@netfence.it>
 <18e6138c-6d03-75e5-2cf0-a3f4e2f784c5@measurement-factory.com>
 <ad194e7c-85e9-81ed-08be-da5fd8153199@netfence.it>
 <9b72e0f3-0209-feb6-6072-69d382c10073@treenet.co.nz>
 <cf9d9a0c-3e75-031e-00da-0bb359e17865@netfence.it>
Message-ID: <962dd330-4211-3514-2194-8ca9e8c49334@netfence.it>

On 7/25/18 7:07 PM, Andrea Venturoli wrote:
> On 7/25/18 6:46 PM, Amos Jeffries wrote:
> 
>> What is your "squid -v" output?
>>
>> If --disable-http-violations is used then relaxed parser will not
>> include those "must never be transmitted in un-escaped form" (RFC 2396)
>> characters.
> 
> It's there!!!
> 
> Thanks for pointing me in the correct direction.
> I'm off recompiling... will let you know if this solves.

I can confirm removing this flag solved my problem.

Thanks to all.

  bye
	av.


From Ralf.Hildebrandt at charite.de  Thu Jul 26 13:15:30 2018
From: Ralf.Hildebrandt at charite.de (Ralf Hildebrandt)
Date: Thu, 26 Jul 2018 15:15:30 +0200
Subject: [squid-users] A logging only ACL?
Message-ID: <20180726131530.GA6270@charite.de>

Before destroying our Squid proxy with an ACL, I'd like to LOG ACL hits
instead of using "http_access deny" straight away:

--- nsip ---
acl markURLhaus annotate_transaction accessRule=URLhaus
acl URLhaus url_regex "/etc/squid5/generated-urlhaus.acl"
http_access deny URLhaus markURLhaus
--- nsip ---

How?

Underlying problem: https://urlhaus.abuse.ch/ is offering 
a plain-text URL list here https://urlhaus.abuse.ch/downloads/text/

But in squid I must used "url_regex" - meaning I'll have to escape the
likes of .^$*+?()[{\|

-- 
Ralf Hildebrandt                   Charite Universit?tsmedizin Berlin
ralf.hildebrandt at charite.de        Campus Benjamin Franklin
https://www.charite.de             Hindenburgdamm 30, 12203 Berlin
Gesch?ftsbereich IT, Abt. Netzwerk fon: +49-30-450.570.155


From uhlar at fantomas.sk  Thu Jul 26 13:29:51 2018
From: uhlar at fantomas.sk (Matus UHLAR - fantomas)
Date: Thu, 26 Jul 2018 15:29:51 +0200
Subject: [squid-users] A logging only ACL?
In-Reply-To: <20180726131530.GA6270@charite.de>
References: <20180726131530.GA6270@charite.de>
Message-ID: <20180726132951.GA30423@fantomas.sk>

On 26.07.18 15:15, Ralf Hildebrandt wrote:
>Before destroying our Squid proxy with an ACL, I'd like to LOG ACL hits
>instead of using "http_access deny" straight away:
>
>--- nsip ---
>acl markURLhaus annotate_transaction accessRule=URLhaus
>acl URLhaus url_regex "/etc/squid5/generated-urlhaus.acl"
>http_access deny URLhaus markURLhaus
>--- nsip ---
>
>How?

you can configure an ACL and only define it in access_log directive.
separate log file would be preferred for this.

there's also "note" directive that allows you log notes for requests
matching ACL. http://www.squid-cache.org/Doc/config/note/

>Underlying problem: https://urlhaus.abuse.ch/ is offering
>a plain-text URL list here https://urlhaus.abuse.ch/downloads/text/
>
>But in squid I must used "url_regex" - meaning I'll have to escape the
>likes of .^$*+?()[{\|

the main problem is that HTTPS URL is only known when you do SSL
deciphering, which may happen:
- in reverse proxy scenario (using squid for ssl offloading)
- when you use SSL bumping (which is quote complicated to implement).

also note that url_regex ACLs are quite CPU hungry.

-- 
Matus UHLAR - fantomas, uhlar at fantomas.sk ; http://www.fantomas.sk/
Warning: I wish NOT to receive e-mail advertising to this address.
Varovanie: na tuto adresu chcem NEDOSTAVAT akukolvek reklamnu postu.
   One OS to rule them all, One OS to find them, 
One OS to bring them all and into darkness bind them 


From rousskov at measurement-factory.com  Thu Jul 26 13:57:00 2018
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Thu, 26 Jul 2018 07:57:00 -0600
Subject: [squid-users] A logging only ACL?
In-Reply-To: <20180726131530.GA6270@charite.de>
References: <20180726131530.GA6270@charite.de>
Message-ID: <f2891bae-e3b6-fb85-ef4c-9d7d50c0e3ea@measurement-factory.com>

On 07/26/2018 07:15 AM, Ralf Hildebrandt wrote:
> Before destroying our Squid proxy with an ACL, I'd like to LOG ACL hits
> instead of using "http_access deny" straight away:
> 
> --- nsip ---
> acl markURLhaus annotate_transaction accessRule=URLhaus
> acl URLhaus url_regex "/etc/squid5/generated-urlhaus.acl"
> http_access deny URLhaus markURLhaus
> --- nsip ---
> 
> How?

In two steps:

1. Make sure your being-tested access rule never denies. For example:

    http_access deny URLhaus !markURLhaus

2. Use %note logformat code to log all annotations or, with an
accessRule parameter, just the accessRule annotation.

Alex.


From erdosain9 at gmail.com  Thu Jul 26 15:06:20 2018
From: erdosain9 at gmail.com (erdosain9)
Date: Thu, 26 Jul 2018 10:06:20 -0500 (CDT)
Subject: [squid-users] Wpad problem (DNS)
Message-ID: <1532617580762-0.post@n4.nabble.com>

Hi to all.
Im trying to put proxy trough DNS. Im working on a Windows Server 2012 r2.
I follow a lot of tutorial... and cant do it.
The best i have is this (and is strange).
When the pc start i see in log of squid the ip of that pc.

 tail -f /var/log/squid/access.log | grep 192.168.6.22
1532616150.629     77 192.168.6.22 TCP_REFRESH_UNMODIFIED/200 316 GET
http://www.msftncsi.com/ncsi.txt - HIER_DIRECT/200.81.17.41 text/plain

but, if i go throug a web browser, nothing appears in access.log... is like
the things that the system search (is a windows 7) goes trough proxy, but
not the thing that i search in the web browser (it's configured to "detect
automatic").

I do this in windows server.
Create a web with IIS, and put wpad.dat file. (create the mime)
In the DNS, create a new zone wpad, and put a new record txt with this
"service: wpad:!http://wpad.xxxx.xxx:80/wpad.dat"
and a CNAME in my domain with a A record name wpad, and fqdn: the hostname
of the server.

i unblock the wpad in the dns also.

And as i say, the system of the machine use the proxy, but not the web
browser... so... some help???

Thanks to all!



--
Sent from: http://squid-web-proxy-cache.1019090.n4.nabble.com/Squid-Users-f1019091.html


From Antony.Stone at squid.open.source.it  Thu Jul 26 15:22:56 2018
From: Antony.Stone at squid.open.source.it (Antony Stone)
Date: Thu, 26 Jul 2018 17:22:56 +0200
Subject: [squid-users] Wpad problem (DNS)
In-Reply-To: <1532617580762-0.post@n4.nabble.com>
References: <1532617580762-0.post@n4.nabble.com>
Message-ID: <201807261722.56516.Antony.Stone@squid.open.source.it>

On Thursday 26 July 2018 at 17:06:20, erdosain9 wrote:

> Hi to all.
> Im trying to put proxy trough DNS. Im working on a Windows Server 2012 r2.
> I follow a lot of tutorial... and cant do it.
> The best i have is this (and is strange).
> When the pc start i see in log of squid the ip of that pc.
> 
>  tail -f /var/log/squid/access.log | grep 192.168.6.22
> 1532616150.629     77 192.168.6.22 TCP_REFRESH_UNMODIFIED/200 316 GET
> http://www.msftncsi.com/ncsi.txt - HIER_DIRECT/200.81.17.41 text/plain
> 
> but, if i go throug a web browser,

Which browser?

Have you tried others?

> nothing appears in access.log... is like the things that the system search
> (is a windows 7) goes trough proxy, but not the thing that i search in the
> web browser (it's configured to "detect automatic").
> 
> I do this in windows server.
> Create a web with IIS, and put wpad.dat file. (create the mime)
> In the DNS, create a new zone wpad, and put a new record txt with this
> "service: wpad:!http://wpad.xxxx.xxx:80/wpad.dat"
> and a CNAME in my domain with a A record name wpad, and fqdn: the hostname
> of the server.
> 
> i unblock the wpad in the dns also.
> 
> And as i say, the system of the machine use the proxy, but not the web
> browser... so... some help???
> 
> Thanks to all!

Antony.

-- 
"I estimate there's a world market for about five computers."

 - Thomas J Watson, Chairman of IBM

                                                   Please reply to the list;
                                                         please *don't* CC me.


From erdosain9 at gmail.com  Thu Jul 26 15:32:18 2018
From: erdosain9 at gmail.com (erdosain9)
Date: Thu, 26 Jul 2018 10:32:18 -0500 (CDT)
Subject: [squid-users] Wpad problem (DNS)
In-Reply-To: <201807261722.56516.Antony.Stone@squid.open.source.it>
References: <1532617580762-0.post@n4.nabble.com>
 <201807261722.56516.Antony.Stone@squid.open.source.it>
Message-ID: <1532619138398-0.post@n4.nabble.com>

Hi, thanks
I try Explorer 8.0 and Chrome 68.0...




--
Sent from: http://squid-web-proxy-cache.1019090.n4.nabble.com/Squid-Users-f1019091.html


From Walter.H at mathemainzel.info  Thu Jul 26 18:20:55 2018
From: Walter.H at mathemainzel.info (Walter H.)
Date: Thu, 26 Jul 2018 20:20:55 +0200
Subject: [squid-users] Wpad problem (DNS)
In-Reply-To: <1532619138398-0.post@n4.nabble.com>
References: <1532617580762-0.post@n4.nabble.com>
 <201807261722.56516.Antony.Stone@squid.open.source.it>
 <1532619138398-0.post@n4.nabble.com>
Message-ID: <5B5A1107.80104@mathemainzel.info>

On 26.07.2018 17:32, erdosain9 wrote:
> Hi, thanks
> I try Explorer 8.0 and Chrome 68.0...
this can be deactivated on browser side; then wpad.... is for the cats ...

Walter

-------------- next part --------------
A non-text attachment was scrubbed...
Name: smime.p7s
Type: application/pkcs7-signature
Size: 3491 bytes
Desc: S/MIME Cryptographic Signature
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20180726/256e4da2/attachment.bin>

From vishali.somaskanthan at viptela.com  Thu Jul 26 20:49:38 2018
From: vishali.somaskanthan at viptela.com (Vishali Somaskanthan)
Date: Thu, 26 Jul 2018 13:49:38 -0700
Subject: [squid-users] server persistent connections and cache
In-Reply-To: <797fbde1-8585-6b71-30ae-d86575b0fbd1@measurement-factory.com>
References: <CABfsTT6_JLH01qmk+aaBCtJaqZ0t3Xry=UmePGwPvFn_6fRTCg@mail.gmail.com>
 <797fbde1-8585-6b71-30ae-d86575b0fbd1@measurement-factory.com>
Message-ID: <CABfsTT6m-=4mSnbez8kepDQTimtaO2p_0N9PnV6CLHYombK+Zw@mail.gmail.com>

Hi,

Resuming the above conversation; When looking at the cache log and the
code, I find that when peek is done at step 1 and then bumped, the
connection gets pinned after *httpsPeeked() *is called.

Log:

*2018/07/23 11:40:29.572 kid1| 17,4| AsyncCallQueue.cc(55) fireNext:
entering ConnStateData::ConnStateData::httpsPeeked(local=4.16.205.42:33596
<http://4.16.205.42:33596> remote=96.43.144.26:443
<http://96.43.144.26:443>        FD 15 flags=1, request=0x559f3b1a6ed0*5)*
* 40056 2018/07/23 11:40:29.572 kid1| 17,4| AsyncCall.cc(38) make: make
call ConnStateData::ConnStateData::httpsPeeked [call261]*
* 40057 2018/07/23 11:40:29.572 kid1| 45,9| cbdata.cc(419)
cbdataReferenceValid: 0x559f3b64b4a8*
* 40058 2018/07/23 11:40:29.572 kid1| 45,9| cbdata.cc(419)
cbdataReferenceValid: 0x559f3b64b4a8*
* 40059 2018/07/23 11:40:29.572 kid1| 45,9| cbdata.cc(419)
cbdataReferenceValid: 0x559f3b64b4a8*
* 40060 2018/07/23 11:40:29.572 kid1| 45,9| cbdata.cc(419)
cbdataReferenceValid: 0x559f3b64b4a8*
* 40061 2018/07/23 11:40:29.572 kid1| 17,4| AsyncJob.cc(123) callStart:
Http1::Server status in: [ job10]*
* 40062 2018/07/23 11:40:29.572 kid1| 45,9| cbdata.cc(419)
cbdataReferenceValid: 0x559f3b64b4a8*
* 40063 2018/07/23 11:40:29.572 kid1| 33,3| Pipeline.cc(35) front: Pipeline
0x559f3b64b4f0 front 0x559f3b1a9190*2*
* 40064 2018/07/23 11:40:29.572 kid1| 33,3| Pipeline.cc(35) front: Pipeline
0x559f3b64b4f0 front 0x559f3b1a9190*3*
* 40065 2018/07/23 11:40:29.572 kid1| 33,3| client_side.cc(4089)
unpinConnection:*
* 40066 2018/07/23 11:40:29.572 kid1| 33,3| client_side.cc(3927)
pinConnection: local=4.16.205.42:33596 <http://4.16.205.42:33596>
remote=96.43.144.26:443 <http://96.43.144.26:443> FD 15 flags=1*

I assume that a pinned connection means a server connection which MUST have
a 1:1 relationship with some client connection. Accordingly, if the client
terminates, then the server connection should also be closing. From the
post:
http://lists.squid-cache.org/pipermail/squid-users/2015-June/004298.html

1. Are there any security reasons behind *pinning the connection* when a
peek is done at Step1 such that server connection get closed if
corresponding client closes. Why is that done?

2.a)  I see a stmt "reusing a pinned conn" in the function
selectPeerForIntercepted().
>From the logs, I dont find this function getting called. Under what
scenario will this get called? In other words, what is the scenario where a
pinned connection can be reused?

   b) Which configure option is used to enable *#if STRICT_ORIGINAL_DST ?*


*#if STRICT_ORIGINAL_DST*
*if (isIntercepted && useOriginalDst) {*
*        selectPeerForIntercepted();*
*        // 3.2 does not suppro re-wrapping inside CONNECT.*
*        // our only alternative is to fake destination "found" and
continue with the forwarding.*
*        startConnectionOrFail();*
*        return;*
*    }*
*#endif*


On Wed, Jul 18, 2018 at 5:00 PM, Alex Rousskov <
rousskov at measurement-factory.com> wrote:

> On 07/18/2018 03:03 PM, Vishali Somaskanthan wrote:
>
> > I had a problem after sending too many requests to the same server where
> > my persistence stopped working suddenly.
>
> Please note that there are many reasons why a proxy may close a
> connection. For pinned to-server connections (like those created by
> SslBump), it may not be possible to open a new to-server connection so
> Squid should close both from-client and to-server connections.
>
> In general, a client should not rely on a connections staying persistent
> except in some very unusual/special circumstances.
>
>
> > 1. What is the relationship between the caching and the persistence
> > connection established?
>
> Virtually none. Caching decisions are done primarily based on request
> and response headers.
>
>
> > 2. When will squid use cached results and when will it not if the cache
> > deny all directive weren't specified.
>
> Squid will deliver a response from the cache when HTTP rules and Squid
> configuration allow a hit. The details are too complex to document here.
>
>
> > 3. However, this didn't work fully. Even with /cache deny all/
> > directive, persistence wasn't working when I peeked at the sslBump step
> > 1 and then Bumped.
> > Persistence worked only when I directly did sslbump allow all (without
> > peeking at first step).
>
> Bumping step should not affect connection persistence AFAICT. I do not
> know why it does in your case. This could be a Squid bug or a
> misunderstanding. If you can reproduce, Squid logs should contain
> reasons for each connection closure (but it may be difficult to find).
>
>
> HTH,
>
> Alex.
>



-- 
Regards,
Vishali Somaskanthan
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20180726/a357f369/attachment.htm>

From rousskov at measurement-factory.com  Thu Jul 26 21:13:18 2018
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Thu, 26 Jul 2018 15:13:18 -0600
Subject: [squid-users] server persistent connections and cache
In-Reply-To: <CABfsTT6m-=4mSnbez8kepDQTimtaO2p_0N9PnV6CLHYombK+Zw@mail.gmail.com>
References: <CABfsTT6_JLH01qmk+aaBCtJaqZ0t3Xry=UmePGwPvFn_6fRTCg@mail.gmail.com>
 <797fbde1-8585-6b71-30ae-d86575b0fbd1@measurement-factory.com>
 <CABfsTT6m-=4mSnbez8kepDQTimtaO2p_0N9PnV6CLHYombK+Zw@mail.gmail.com>
Message-ID: <68233cfa-ba76-82f3-8964-42781c56ca9e@measurement-factory.com>

On 07/26/2018 02:49 PM, Vishali Somaskanthan wrote:

> 1. Are there any security reasons behind /pinning the connection/ when a
> peek is done at Step1

I doubt there is some fundamental _security_ reason to pin if you bump
without forwarding the TLS client information to the server. The reasons
to pin in that case include:

* honoring client and server blind tunneling expectations
  (which may result in better compatibility or even security);

* development convenience (pinning _all_ SslBump connections is easier
  than pinning some of them).


> Why is that done?

I speculate it was done for the reasons stated in the above bullets.


> 2.a) ?what is the scenario where a pinned connection can be reused??

When a previously established Squid-to-server connection is used for the
next request on the corresponding client-to-Squid connection.


> ? ?b) Which configure option is used to?enable?/#if STRICT_ORIGINAL_DST ?/

There is no user-friendly way to control that macro AFAICT. You may
define it when building Squid from sources (e.g., by passing
-DSTRICT_ORIGINAL_DST=1 to the compiler via CPPFLAGS). Please do not
misinterpret my response as an indication that this is an officially
supported feature.


HTH,

Alex.



> On Wed, Jul 18, 2018 at 5:00 PM, Alex Rousskov wrote:
> 
>     On 07/18/2018 03:03 PM, Vishali Somaskanthan wrote:
> 
>     > I had a problem after sending too many requests to the same server where
>     > my persistence stopped working suddenly.
> 
>     Please note that there are many reasons why a proxy may close a
>     connection. For pinned to-server connections (like those created by
>     SslBump), it may not be possible to open a new to-server connection so
>     Squid should close both from-client and to-server connections.
> 
>     In general, a client should not rely on a connections staying persistent
>     except in some very unusual/special circumstances.
> 
> 
>     > 1.?What is the relationship between the caching and the persistence
>     > connection established?
> 
>     Virtually none. Caching decisions are done primarily based on request
>     and response headers.
> 
> 
>     > 2. When will squid use cached results and when will it not if the cache
>     > deny all directive weren't specified. 
> 
>     Squid will deliver a response from the cache when HTTP rules and Squid
>     configuration allow a hit. The details are too complex to document here.
> 
> 
>     > 3. However, this?didn't?work fully. Even with /cache deny all/
>     > directive, persistence?wasn't working when I peeked at the sslBump step
>     > 1 and then Bumped.?
>     > Persistence worked only when I directly did sslbump allow all (without
>     > peeking at first step).
> 
>     Bumping step should not affect connection persistence AFAICT. I do not
>     know why it does in your case. This could be a Squid bug or a
>     misunderstanding. If you can reproduce, Squid logs should contain
>     reasons for each connection closure (but it may be difficult to find).
> 
> 
>     HTH,
> 
>     Alex.
> 
> 
> 
> 
> -- 
> Regards,
> Vishali Somaskanthan
> 
> 
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
> 



From vishali.somaskanthan at viptela.com  Thu Jul 26 23:47:04 2018
From: vishali.somaskanthan at viptela.com (Vishali Somaskanthan)
Date: Thu, 26 Jul 2018 16:47:04 -0700
Subject: [squid-users] server persistent connections and cache
In-Reply-To: <68233cfa-ba76-82f3-8964-42781c56ca9e@measurement-factory.com>
References: <CABfsTT6_JLH01qmk+aaBCtJaqZ0t3Xry=UmePGwPvFn_6fRTCg@mail.gmail.com>
 <797fbde1-8585-6b71-30ae-d86575b0fbd1@measurement-factory.com>
 <CABfsTT6m-=4mSnbez8kepDQTimtaO2p_0N9PnV6CLHYombK+Zw@mail.gmail.com>
 <68233cfa-ba76-82f3-8964-42781c56ca9e@measurement-factory.com>
Message-ID: <CABfsTT5B6q29zKewLNNRjDQJo9RC-5xOKkDTynz1oa4QYttjNQ@mail.gmail.com>

Hi,

FYI, in all my examples below, one have the same client and same server



By re-use I meant to say that the server-connection S (TCP + SSL) is
re-used across 2 client connections (C1 and C2), from the same client one
after the other is torn down. I, presume that ?*server_persistent_connection
on*? allows for such a use-case. Is my understanding that Pinning means
binding C1 to S and then if C1 is closed, we unpin and then later if C2 is
created, we can pin it again to S?



There is some confusion in my understanding this statement ? ?pinning _all_
SslBump connections is easier than pinning some of them?, because I see
different behaviors when I bump at Step 1 (case 1) vs bump at Step 2 (case
2).

Case 1: We see that no pinning happens i.e. pinConnection() is not called
at all. C1->S gets established, C1 is closed and then C2 re-uses S

Case 2: We see that pinning happens i.e. httpsPeeked() calls
pinConnection(). Here, C1->S gets established. Closing C1 from the client
brings down S. Later, opening C2 opens a new server-connection S.



Is this the expected behavior? Please explain.


Thank you.

On Thu, Jul 26, 2018 at 2:13 PM, Alex Rousskov <
rousskov at measurement-factory.com> wrote:

> On 07/26/2018 02:49 PM, Vishali Somaskanthan wrote:
>
> > 1. Are there any security reasons behind /pinning the connection/ when a
> > peek is done at Step1
>
> I doubt there is some fundamental _security_ reason to pin if you bump
> without forwarding the TLS client information to the server. The reasons
> to pin in that case include:
>
> * honoring client and server blind tunneling expectations
>   (which may result in better compatibility or even security);
>
> * development convenience (pinning _all_ SslBump connections is easier
>   than pinning some of them).
>
>
> > Why is that done?
>
> I speculate it was done for the reasons stated in the above bullets.
>
>
> > 2.a)  what is the scenario where a pinned connection can be reused?
>
> When a previously established Squid-to-server connection is used for the
> next request on the corresponding client-to-Squid connection.
>
>
> >    b) Which configure option is used to enable /#if STRICT_ORIGINAL_DST
> ?/
>
> There is no user-friendly way to control that macro AFAICT. You may
> define it when building Squid from sources (e.g., by passing
> -DSTRICT_ORIGINAL_DST=1 to the compiler via CPPFLAGS). Please do not
> misinterpret my response as an indication that this is an officially
> supported feature.
>
>
> HTH,
>
> Alex.
>
>
>
> > On Wed, Jul 18, 2018 at 5:00 PM, Alex Rousskov wrote:
> >
> >     On 07/18/2018 03:03 PM, Vishali Somaskanthan wrote:
> >
> >     > I had a problem after sending too many requests to the same server
> where
> >     > my persistence stopped working suddenly.
> >
> >     Please note that there are many reasons why a proxy may close a
> >     connection. For pinned to-server connections (like those created by
> >     SslBump), it may not be possible to open a new to-server connection
> so
> >     Squid should close both from-client and to-server connections.
> >
> >     In general, a client should not rely on a connections staying
> persistent
> >     except in some very unusual/special circumstances.
> >
> >
> >     > 1. What is the relationship between the caching and the persistence
> >     > connection established?
> >
> >     Virtually none. Caching decisions are done primarily based on request
> >     and response headers.
> >
> >
> >     > 2. When will squid use cached results and when will it not if the
> cache
> >     > deny all directive weren't specified.
> >
> >     Squid will deliver a response from the cache when HTTP rules and
> Squid
> >     configuration allow a hit. The details are too complex to document
> here.
> >
> >
> >     > 3. However, this didn't work fully. Even with /cache deny all/
> >     > directive, persistence wasn't working when I peeked at the sslBump
> step
> >     > 1 and then Bumped.
> >     > Persistence worked only when I directly did sslbump allow all
> (without
> >     > peeking at first step).
> >
> >     Bumping step should not affect connection persistence AFAICT. I do
> not
> >     know why it does in your case. This could be a Squid bug or a
> >     misunderstanding. If you can reproduce, Squid logs should contain
> >     reasons for each connection closure (but it may be difficult to
> find).
> >
> >
> >     HTH,
> >
> >     Alex.
> >
> >
> >
> >
> > --
> > Regards,
> > Vishali Somaskanthan
> >
> >
> > _______________________________________________
> > squid-users mailing list
> > squid-users at lists.squid-cache.org
> > http://lists.squid-cache.org/listinfo/squid-users
> >
>
>


-- 
Regards,
Vishali Somaskanthan
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20180726/e6f82263/attachment.htm>

From rousskov at measurement-factory.com  Fri Jul 27 01:31:04 2018
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Thu, 26 Jul 2018 19:31:04 -0600
Subject: [squid-users] server persistent connections and cache
In-Reply-To: <CABfsTT5B6q29zKewLNNRjDQJo9RC-5xOKkDTynz1oa4QYttjNQ@mail.gmail.com>
References: <CABfsTT6_JLH01qmk+aaBCtJaqZ0t3Xry=UmePGwPvFn_6fRTCg@mail.gmail.com>
 <797fbde1-8585-6b71-30ae-d86575b0fbd1@measurement-factory.com>
 <CABfsTT6m-=4mSnbez8kepDQTimtaO2p_0N9PnV6CLHYombK+Zw@mail.gmail.com>
 <68233cfa-ba76-82f3-8964-42781c56ca9e@measurement-factory.com>
 <CABfsTT5B6q29zKewLNNRjDQJo9RC-5xOKkDTynz1oa4QYttjNQ@mail.gmail.com>
Message-ID: <8be71578-b79b-109f-3c9a-44b34e4cb09f@measurement-factory.com>

On 07/26/2018 05:47 PM, Vishali Somaskanthan wrote:

> By re-use I meant to say that the server-connection S (TCP + SSL) is
> re-used across 2 client connections (C1 and C2), from the same client
> one after the other is torn down. I, presume that
> ?/server_persistent_connection on/? allows for such a use-case.

server_persistent_connection controls whether a single Squid-to-server
connection can carry more than one request. It does not (or should not)
control re-pinning.


> Is my
> understanding that Pinning means binding C1 to S and then if C1 is
> closed, we unpin and then later if C2 is created, we can pin it again to S?

IIRC, a lot of code assumes that pinning ties C1 and S connection
lifetimes together. I do not know whether all code assumes that. I do
not know whether there is consensus that "same lifetimes" is the correct
approach for all pinned connections.


> There is some confusion in my understanding this statement ? ?pinning
> _all_ SslBump connections is easier?than pinning some of them?, because
> I see different behaviors when I bump at Step 1 (case 1) vs bump at Step
> 2 (case 2).

Just because something is easier in retrospect, does not mean it was
easy or even clear to the developers writing bits and pieces of that
code. There are a lot of inconsistencies (and bugs) that we are slowly
weeding out.


> Case 1: We see that no pinning happens i.e. pinConnection() is not
> called at all. C1->S gets established, C1 is closed and then C2 re-uses S
> 
> Case 2: We see that pinning happens i.e. httpsPeeked() calls
> pinConnection(). Here, C1->S gets established. Closing C1 from the
> client brings down S. Later, opening C2 opens a new server-connection S.
> 
> Is this the expected behavior?

The code just happens to work this way (evidently). It is not something
I would rely on until the matter is discussed and settled.


HTH,

Alex.


> On Thu, Jul 26, 2018 at 2:13 PM, Alex Rousskov wrote:
> 
>     On 07/26/2018 02:49 PM, Vishali Somaskanthan wrote:
> 
>     > 1. Are there any security reasons behind /pinning the connection/
>     when a
>     > peek is done at Step1
> 
>     I doubt there is some fundamental _security_ reason to pin if you bump
>     without forwarding the TLS client information to the server. The reasons
>     to pin in that case include:
> 
>     * honoring client and server blind tunneling expectations
>     ? (which may result in better compatibility or even security);
> 
>     * development convenience (pinning _all_ SslBump connections is easier
>     ? than pinning some of them).
> 
> 
>     > Why is that done?
> 
>     I speculate it was done for the reasons stated in the above bullets.
> 
> 
>     > 2.a) ?what is the scenario where a pinned connection can be reused??
> 
>     When a previously established Squid-to-server connection is used for the
>     next request on the corresponding client-to-Squid connection.
> 
> 
>     > ? ?b) Which configure option is used to?enable?/#if
>     STRICT_ORIGINAL_DST ?/
> 
>     There is no user-friendly way to control that macro AFAICT. You may
>     define it when building Squid from sources (e.g., by passing
>     -DSTRICT_ORIGINAL_DST=1 to the compiler via CPPFLAGS). Please do not
>     misinterpret my response as an indication that this is an officially
>     supported feature.
> 
> 
>     HTH,
> 
>     Alex.
> 
> 
> 
>     > On Wed, Jul 18, 2018 at 5:00 PM, Alex Rousskov wrote:
>     >
>     >? ? ?On 07/18/2018 03:03 PM, Vishali Somaskanthan wrote:
>     >
>     >? ? ?> I had a problem after sending too many requests to the same
>     server where
>     >? ? ?> my persistence stopped working suddenly.
>     >
>     >? ? ?Please note that there are many reasons why a proxy may close a
>     >? ? ?connection. For pinned to-server connections (like those
>     created by
>     >? ? ?SslBump), it may not be possible to open a new to-server
>     connection so
>     >? ? ?Squid should close both from-client and to-server connections.
>     >
>     >? ? ?In general, a client should not rely on a connections staying
>     persistent
>     >? ? ?except in some very unusual/special circumstances.
>     >
>     >
>     >? ? ?> 1.?What is the relationship between the caching and the
>     persistence
>     >? ? ?> connection established?
>     >
>     >? ? ?Virtually none. Caching decisions are done primarily based on
>     request
>     >? ? ?and response headers.
>     >
>     >
>     >? ? ?> 2. When will squid use cached results and when will it not
>     if the cache
>     >? ? ?> deny all directive weren't specified.
>     >
>     >? ? ?Squid will deliver a response from the cache when HTTP rules
>     and Squid
>     >? ? ?configuration allow a hit. The details are too complex to
>     document here.
>     >
>     >
>     >? ? ?> 3. However, this?didn't?work fully. Even with /cache deny all/
>     >? ? ?> directive, persistence?wasn't working when I peeked at the
>     sslBump step
>     >? ? ?> 1 and then Bumped.?
>     >? ? ?> Persistence worked only when I directly did sslbump allow
>     all (without
>     >? ? ?> peeking at first step).
>     >
>     >? ? ?Bumping step should not affect connection persistence AFAICT.
>     I do not
>     >? ? ?know why it does in your case. This could be a Squid bug or a
>     >? ? ?misunderstanding. If you can reproduce, Squid logs should contain
>     >? ? ?reasons for each connection closure (but it may be difficult
>     to find).
>     >
>     >


From squid3 at treenet.co.nz  Fri Jul 27 02:26:51 2018
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Fri, 27 Jul 2018 14:26:51 +1200
Subject: [squid-users] Wpad problem (DNS)
In-Reply-To: <1532617580762-0.post@n4.nabble.com>
References: <1532617580762-0.post@n4.nabble.com>
Message-ID: <4f2d0fce-84dd-be33-06fd-30de6d41a43f@treenet.co.nz>

On 27/07/18 03:06, erdosain9 wrote:
> Hi to all.
> Im trying to put proxy trough DNS. Im working on a Windows Server 2012 r2.
> I follow a lot of tutorial... and cant do it.
> The best i have is this (and is strange).
> When the pc start i see in log of squid the ip of that pc.
> 
>  tail -f /var/log/squid/access.log | grep 192.168.6.22
> 1532616150.629     77 192.168.6.22 TCP_REFRESH_UNMODIFIED/200 316 GET
> http://www.msftncsi.com/ncsi.txt - HIER_DIRECT/200.81.17.41 text/plain
> 
> but, if i go throug a web browser, nothing appears in access.log... is like
> the things that the system search (is a windows 7) goes trough proxy, but
> not the thing that i search in the web browser (it's configured to "detect
> automatic").
> 
> I do this in windows server.
> Create a web with IIS, and put wpad.dat file. (create the mime)
> In the DNS, create a new zone wpad, and put a new record txt with this
> "service: wpad:!http://wpad.xxxx.xxx:80/wpad.dat"
> and a CNAME in my domain with a A record name wpad, and fqdn: the hostname
> of the server.
> 

Okay, the TXT record is not one I've encountered before but the CNAME/A
records match what I know IE8 supported.

Does the wpad.xxxx.xxx:80 server provide the wpad.dat file with the
correct mime type "application/x-ns-proxy-autoconfig" in HTTP response
message?

Does the wpad.dat file contain valid PAC syntax pointing receivers to
use the proxy?
 <http://findproxyforurl.com/> has more info on what the PAC file needs
to contain.

What OS is the client machine using?
 Presence of IE 8 hints that it is an older one (XP?) where there may be
ipconfig related tricks needed.


If you have not already done so that site also has troubleshooting tools
and how-to at <http://findproxyforurl.com/troubleshooting-pac-wpad/>.


Amos


From squid3 at treenet.co.nz  Fri Jul 27 03:15:22 2018
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Fri, 27 Jul 2018 15:15:22 +1200
Subject: [squid-users] server persistent connections and cache
In-Reply-To: <8be71578-b79b-109f-3c9a-44b34e4cb09f@measurement-factory.com>
References: <CABfsTT6_JLH01qmk+aaBCtJaqZ0t3Xry=UmePGwPvFn_6fRTCg@mail.gmail.com>
 <797fbde1-8585-6b71-30ae-d86575b0fbd1@measurement-factory.com>
 <CABfsTT6m-=4mSnbez8kepDQTimtaO2p_0N9PnV6CLHYombK+Zw@mail.gmail.com>
 <68233cfa-ba76-82f3-8964-42781c56ca9e@measurement-factory.com>
 <CABfsTT5B6q29zKewLNNRjDQJo9RC-5xOKkDTynz1oa4QYttjNQ@mail.gmail.com>
 <8be71578-b79b-109f-3c9a-44b34e4cb09f@measurement-factory.com>
Message-ID: <3d11195e-fcd3-f617-cd09-d67b4a23dcfa@treenet.co.nz>

On 27/07/18 13:31, Alex Rousskov wrote:
> On 07/26/2018 05:47 PM, Vishali Somaskanthan wrote:
> 
>> By re-use I meant to say that the server-connection S (TCP + SSL) is
>> re-used across 2 client connections (C1 and C2), from the same client
>> one after the other is torn down. I, presume that
>> ?/server_persistent_connection on/? allows for such a use-case.
> 
> server_persistent_connection controls whether a single Squid-to-server
> connection can carry more than one request. It does not (or should not)
> control re-pinning.
> 

Also, pinned connections should never be added to the persistent pool
after pinning. Doing so is a bug IMO. See the sequence at the end of
this mail.

> 
>> Is my
>> understanding that Pinning means binding C1 to S and then if C1 is
>> closed, we unpin and then later if C2 is created, we can pin it again to S?
> 
> IIRC, a lot of code assumes that pinning ties C1 and S connection
> lifetimes together. I do not know whether all code assumes that. I do
> not know whether there is consensus that "same lifetimes" is the correct
> approach for all pinned connections.
> 

Correct (Alex). Pinning to us/Squid means the two connections have done
something stateful - which means shared fate is required from that point
onwards to avoid nasties outside Squid screwing with that statefulness.

Today I work to:
* If one dies they both do.
* A pinned server connection must not be usable by any other client
*after* being pinned.
* A connection not pinned may only be used by *one* client at a time.

There may be old code which does not check pinned state when it should
or makes wrong assumptions about a pinned connection usability. Please
fix as and when found.


> 
>> There is some confusion in my understanding this statement ? ?pinning
>> _all_ SslBump connections is easier?than pinning some of them?, because
>> I see different behaviors when I bump at Step 1 (case 1) vs bump at Step
>> 2 (case 2).
> 
> Just because something is easier in retrospect, does not mean it was
> easy or even clear to the developers writing bits and pieces of that
> code. There are a lot of inconsistencies (and bugs) that we are slowly
> weeding out.
> 

Sending the client handshake to a server makes the TLS end-to-end
stateful between the client<->server. So *that* is the first point at
which pinning is required by Squid.

Peeking at step1 alone does not add statefulness to the client
connection being peeked.

Peeking at step2 requires client data sent to the server. So pinning is
required.

Bumping at step1 does not involve any server information. So no pinning
required. The client is talking to *Squid* over TLS independent of how
the response is fetched.

Bumping at step2 or step3 the client is talking to the specific server
over TLS.

Splice at any time requires tunneling data both ways. So pinning is
required in all cases.

Stare AFAIK does send *some* client state (filtered) to the server so
pinning is probably always required here - but may not be depending on
what the filtering was.


> 
>> Case 1: We see that no pinning happens i.e. pinConnection() is not
>> called at all. C1->S gets established, C1 is closed and then C2 re-uses S
>>
>> Case 2: We see that pinning happens i.e. httpsPeeked() calls
>> pinConnection(). Here, C1->S gets established. Closing C1 from the
>> client brings down S. Later, opening C2 opens a new server-connection S.
>>
>> Is this the expected behavior?
> 
> The code just happens to work this way (evidently). It is not something
> I would rely on until the matter is discussed and settled.
> 


I think the described behaviour of C2 using S after C1 has pinned the
connection is a bug.

* S should not be pooled as persistent until the client which triggered
its TCP open has finished with it (eg after the TLS handshake completes).

* Once the handshake begins sending client data, whichever client was
opening it should pin it.

* The sequence of the above two should result in S being pinned before
it is ever considered for the persistence pool. Which should cause it to
be excluded from the servers pool.

So IMO when these things are working properly, C2 server selection
should never even see that S exists let alone be able to use it.

Amos



From capcoding at gmail.com  Fri Jul 27 04:10:51 2018
From: capcoding at gmail.com (Gordon Hsiao)
Date: Thu, 26 Jul 2018 23:10:51 -0500
Subject: [squid-users] HSTS and HPKP
Message-ID: <CAK0iFYz7ogN76g4_8JwRKn1aWmFP=oi+1hOsvVchjgZOoTJqeA@mail.gmail.com>

I'm running squid4.1 interception peek+splice mode.

Some sites with HSTS(max-age=0) will not work whenever squid is on, HSTS
max-age=0 is supposed to turn off HSTS, but chrome/firefox will keep
redirecting https<-->http until it failed(too many redirects). Once Squid
is removed all is good.

I also searched various lists and squid's website, it's still unclear to
me, for intercept proxy, can Squid deal with HSTS reliably these days?

A similar questions is HPKP, or the pinning certificate, can Squid 4.1
handle that?

When no HSTS/HPKP is involved, it seems all sites work well.

Gordon
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20180726/25ed7834/attachment.htm>

From rousskov at measurement-factory.com  Fri Jul 27 04:18:44 2018
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Thu, 26 Jul 2018 22:18:44 -0600
Subject: [squid-users] server persistent connections and cache
In-Reply-To: <3d11195e-fcd3-f617-cd09-d67b4a23dcfa@treenet.co.nz>
References: <CABfsTT6_JLH01qmk+aaBCtJaqZ0t3Xry=UmePGwPvFn_6fRTCg@mail.gmail.com>
 <797fbde1-8585-6b71-30ae-d86575b0fbd1@measurement-factory.com>
 <CABfsTT6m-=4mSnbez8kepDQTimtaO2p_0N9PnV6CLHYombK+Zw@mail.gmail.com>
 <68233cfa-ba76-82f3-8964-42781c56ca9e@measurement-factory.com>
 <CABfsTT5B6q29zKewLNNRjDQJo9RC-5xOKkDTynz1oa4QYttjNQ@mail.gmail.com>
 <8be71578-b79b-109f-3c9a-44b34e4cb09f@measurement-factory.com>
 <3d11195e-fcd3-f617-cd09-d67b4a23dcfa@treenet.co.nz>
Message-ID: <89556538-6d5e-54a4-6df0-74eaa7f8bf91@measurement-factory.com>

On 07/26/2018 09:15 PM, Amos Jeffries wrote:
> On 27/07/18 13:31, Alex Rousskov wrote:
>> On 07/26/2018 05:47 PM, Vishali Somaskanthan wrote:
>>
>>> By re-use I meant to say that the server-connection S (TCP + SSL) is
>>> re-used across 2 client connections (C1 and C2), from the same client
>>> one after the other is torn down. I, presume that
>>> ?/server_persistent_connection on/? allows for such a use-case.
>>
>> server_persistent_connection controls whether a single Squid-to-server
>> connection can carry more than one request. It does not (or should not)
>> control re-pinning.

> Also, pinned connections should never be added to the persistent pool
> after pinning. Doing so is a bug IMO.

Agreed.


> Sending the client handshake to a server makes the TLS end-to-end
> stateful between the client<->server. So *that* is the first point at
> which pinning is required by Squid.

At TLS level, yes. However, one could argue that Squid should honor a
(higher level) client and server assumption that they are talking to
each other (at HTTP+ level). We will probably break fewer transactions
that way. With that idea in mind, the "first point" becomes establishing
a TCP tunnel with the server, even if no client Hello pieces are forwarded.


> Bumping at step1 does not involve any server information. So no pinning
> required. The client is talking to *Squid* over TLS independent of how
> the response is fetched.

True for TLS level. Not necessarily true for higher levels as discussed
above.


>>> Case 1: We see that no pinning happens i.e. pinConnection() is not
>>> called at all. C1->S gets established, C1 is closed and then C2 re-uses S

>> The code just happens to work this way (evidently). It is not something
>> I would rely on until the matter is discussed and settled.

> I think the described behaviour of C2 using S after C1 has pinned the
> connection is a bug.

Your earlier "Sending the client handshake to a server" and "Bumping at
step1" statements contradict this statement AFAICT because both C1 and
C2 were bumped at step1 in "Case 1".

FWIW, I would default to honor the tunnel until somebody presents a
convincing argument for making the behavior configurable. That is, like
you said, treat C2 reusing S (or, more precisely, not closing S when C1
is closed) as a Squid bug.


> * S should not be pooled as persistent until the client which triggered
> its TCP open has finished with it (eg after the TLS handshake completes).

> * Once the handshake begins sending client data, whichever client was
> opening it should pin it.

I agree that pinned connections should not be pooled. Bugs
notwithstanding, they are not pooled today IIRC.

I am not sure you meant that, but I doubt a hypothetical feature of
pooling before-sending-handshake/before-pinning connections is a good
idea, but it does not contradict "pinned connections are not pooled"
principle. In practice, once the TCP connection is established, the
requester pins it immediately (if needed) so there is no opportunity or
need for pooling.


> * The sequence of the above two should result in S being pinned before
> it is ever considered for the persistence pool. Which should cause it to
> be excluded from the servers pool.

Agreed.


> So IMO when these things are working properly, C2 server selection
> should never even see that S exists let alone be able to use it.

The open question is whether S should be pinned in the case where C1 is
bumped at step1 (i.e., "Case 1" in Vishali's email). If S should be
pinned, then I agree with the above statement. If S should not be
pinned, then S is just a regular Squid-to-server connection that can be
pooled.


Cheers,

Alex.


From squid3 at treenet.co.nz  Fri Jul 27 04:34:54 2018
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Fri, 27 Jul 2018 16:34:54 +1200
Subject: [squid-users] HSTS and HPKP
In-Reply-To: <CAK0iFYz7ogN76g4_8JwRKn1aWmFP=oi+1hOsvVchjgZOoTJqeA@mail.gmail.com>
References: <CAK0iFYz7ogN76g4_8JwRKn1aWmFP=oi+1hOsvVchjgZOoTJqeA@mail.gmail.com>
Message-ID: <62a83bb7-40a5-f61c-0e57-f184cc81333e@treenet.co.nz>

On 27/07/18 16:10, Gordon Hsiao wrote:
> I'm running squid4.1 interception peek+splice mode.
> 
> Some sites with HSTS(max-age=0) will not work whenever squid is on, HSTS
> max-age=0 is supposed to turn off HSTS, but chrome/firefox will keep
> redirecting https<-->http until it failed(too many redirects). Once
> Squid is removed all is good.
> 
> I also searched various lists and squid's website, it's still unclear to
> me, for intercept proxy, can Squid deal with HSTS reliably these days?
> 

Handle yes. Reliably no.

Squid should be erasing the HSTS header completely whenever it can. The
problem is that HSTS can be delivered in several ways that Squid is not
in control of (spliced' traffic, non-HTTP protocols, and non-proxied
connections). You have to reliably seal off those other protocols and
connection types for the MITM proxy to have even a basic chance at success.

FWIW: any HSTS TTL value that gets through to the server breaks things.
Even though max-age=0 can be used to clear some of those other HSTS
avenues, it still breaks things just by turning on the HSTS handling at
the server.


> A similar questions is HPKP, or the pinning certificate, can Squid 4.1
> handle that?

No.

While HSTS was a train wreck from day-0, HPKP is technically closer to
how TLS was supposed to be used in the first place.

AFAIK, the only thing you can do in the presence of client application
using HPKP is splice. Server using it does not matter if the client is
not checking.

Amos


From squid3 at treenet.co.nz  Fri Jul 27 05:11:08 2018
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Fri, 27 Jul 2018 17:11:08 +1200
Subject: [squid-users] server persistent connections and cache
In-Reply-To: <89556538-6d5e-54a4-6df0-74eaa7f8bf91@measurement-factory.com>
References: <CABfsTT6_JLH01qmk+aaBCtJaqZ0t3Xry=UmePGwPvFn_6fRTCg@mail.gmail.com>
 <797fbde1-8585-6b71-30ae-d86575b0fbd1@measurement-factory.com>
 <CABfsTT6m-=4mSnbez8kepDQTimtaO2p_0N9PnV6CLHYombK+Zw@mail.gmail.com>
 <68233cfa-ba76-82f3-8964-42781c56ca9e@measurement-factory.com>
 <CABfsTT5B6q29zKewLNNRjDQJo9RC-5xOKkDTynz1oa4QYttjNQ@mail.gmail.com>
 <8be71578-b79b-109f-3c9a-44b34e4cb09f@measurement-factory.com>
 <3d11195e-fcd3-f617-cd09-d67b4a23dcfa@treenet.co.nz>
 <89556538-6d5e-54a4-6df0-74eaa7f8bf91@measurement-factory.com>
Message-ID: <f30cb3ec-4678-0c45-7fb6-41c07db61a60@treenet.co.nz>

On 27/07/18 16:18, Alex Rousskov wrote:
> On 07/26/2018 09:15 PM, Amos Jeffries wrote:
>> On 27/07/18 13:31, Alex Rousskov wrote:
>>> On 07/26/2018 05:47 PM, Vishali Somaskanthan wrote:
>>>
>>>> By re-use I meant to say that the server-connection S (TCP + SSL) is
>>>> re-used across 2 client connections (C1 and C2), from the same client
>>>> one after the other is torn down. I, presume that
>>>> ?/server_persistent_connection on/? allows for such a use-case.
>>>
>>> server_persistent_connection controls whether a single Squid-to-server
>>> connection can carry more than one request. It does not (or should not)
>>> control re-pinning.
> 
>> Also, pinned connections should never be added to the persistent pool
>> after pinning. Doing so is a bug IMO.
> 
> Agreed.
> 
> 
>> Sending the client handshake to a server makes the TLS end-to-end
>> stateful between the client<->server. So *that* is the first point at
>> which pinning is required by Squid.
> 
> At TLS level, yes. However, one could argue that Squid should honor a
> (higher level) client and server assumption that they are talking to
> each other (at HTTP+ level).

One could, but that is dependent on what the admin policy is.
Configuring persistence on/off is the way to control that, not pinning.

> We will probably break fewer transactions
> that way. With that idea in mind, the "first point" becomes establishing
> a TCP tunnel with the server, even if no client Hello pieces are forwarded.
> 

HTTP(S) being stateless, at that level any endpoint relying on implicit
state is non-compliant and buggy. HTTP(S) agents are explicitly required
to include any necessary state references in each message generated.

It is *nice* not to result in visible errors, but not a requirement we
should stick to at cost of proper behaviour. HTTP and extensions also
specify required error handling in most cases where it matters.


> 
>> Bumping at step1 does not involve any server information. So no pinning
>> required. The client is talking to *Squid* over TLS independent of how
>> the response is fetched.
> 
> True for TLS level. Not necessarily true for higher levels as discussed
> above.
> 

The client accepted proof that Squid *was* that origin (false or not)
order to reach said higher levels. Pinning at a later time due to higher
level stateful situation is not relevant to the bumping code actions and
not something we need to consider at the present C2 use of S after C1
should have pinned it.

The only cross-level requirement is to provide the surety that pinning
is a one-way action. Any level setting it makes the connections fate shared.


> 
>>>> Case 1: We see that no pinning happens i.e. pinConnection() is not
>>>> called at all. C1->S gets established, C1 is closed and then C2 re-uses S
> 
>>> The code just happens to work this way (evidently). It is not something
>>> I would rely on until the matter is discussed and settled.
> 
>> I think the described behaviour of C2 using S after C1 has pinned the
>> connection is a bug.
> 
> Your earlier "Sending the client handshake to a server" and "Bumping at
> step1" statements contradict this statement AFAICT because both C1 and
> C2 were bumped at step1 in "Case 1".
> 

 "after C1 has pinned" - if no pinning happens at all the whole
statement is irrelevant. Sorry for confusion.


> FWIW, I would default to honor the tunnel until somebody presents a
> convincing argument for making the behavior configurable. That is, like
> you said, treat C2 reusing S (or, more precisely, not closing S when C1
> is closed) as a Squid bug.
> 

What I'm most confused about here is why S is closed when C1 dies with
non-pinning.  _unless_ there is pinning between them they should not be
that closely fate sharing.

Is the HTTPS message on both C1 and S saying "Connection: close" perhapse?
 If so the closure relationship is an illusion.
 But then, why would C2 be using a connection currently in-use by C1 in
a way that starts *after* C1 closure? (so not collapsed forwarding AFAIK)

Something smells fishy.


> 
>> * S should not be pooled as persistent until the client which triggered
>> its TCP open has finished with it (eg after the TLS handshake completes).
> 
>> * Once the handshake begins sending client data, whichever client was
>> opening it should pin it.
> 
> I agree that pinned connections should not be pooled. Bugs
> notwithstanding, they are not pooled today IIRC.
> 
> I am not sure you meant that, but I doubt a hypothetical feature of
> pooling before-sending-handshake/before-pinning connections is a good
> idea, but it does not contradict "pinned connections are not pooled"
> principle. In practice, once the TCP connection is established, the
> requester pins it immediately (if needed) so there is no opportunity or
> need for pooling.
> 

Off-topic; TLS has connections that do not use handshakes at all, which
are becoming more common in TLS/1.3. So I believe such a feature may be
coming one day, but irrelevant right now.


> 
>> * The sequence of the above two should result in S being pinned before
>> it is ever considered for the persistence pool. Which should cause it to
>> be excluded from the servers pool.
> 
> Agreed.
> 
> 
>> So IMO when these things are working properly, C2 server selection
>> should never even see that S exists let alone be able to use it.
> 
> The open question is whether S should be pinned in the case where C1 is
> bumped at step1 (i.e., "Case 1" in Vishali's email). If S should be
> pinned, then I agree with the above statement. If S should not be
> pinned, then S is just a regular Squid-to-server connection that can be
> pooled.
> 

IMO in *that* specific case S should not be pinned. Because pinning
would prevent the very reasonable actions of handling server failures by
opening new Sn connections to finish incomplete responses to the client,
or retaining a server connection for the quick_abort features.

Squid should instead be ensuring that the server handshake at TLS level
matches what it would have used for C2 clean handshake. I may have
missed it, but that is not yet being done (just TCP equivalence, not TLS).

Amos


From rousskov at measurement-factory.com  Fri Jul 27 15:57:37 2018
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Fri, 27 Jul 2018 09:57:37 -0600
Subject: [squid-users] server persistent connections and cache
In-Reply-To: <f30cb3ec-4678-0c45-7fb6-41c07db61a60@treenet.co.nz>
References: <CABfsTT6_JLH01qmk+aaBCtJaqZ0t3Xry=UmePGwPvFn_6fRTCg@mail.gmail.com>
 <797fbde1-8585-6b71-30ae-d86575b0fbd1@measurement-factory.com>
 <CABfsTT6m-=4mSnbez8kepDQTimtaO2p_0N9PnV6CLHYombK+Zw@mail.gmail.com>
 <68233cfa-ba76-82f3-8964-42781c56ca9e@measurement-factory.com>
 <CABfsTT5B6q29zKewLNNRjDQJo9RC-5xOKkDTynz1oa4QYttjNQ@mail.gmail.com>
 <8be71578-b79b-109f-3c9a-44b34e4cb09f@measurement-factory.com>
 <3d11195e-fcd3-f617-cd09-d67b4a23dcfa@treenet.co.nz>
 <89556538-6d5e-54a4-6df0-74eaa7f8bf91@measurement-factory.com>
 <f30cb3ec-4678-0c45-7fb6-41c07db61a60@treenet.co.nz>
Message-ID: <b2c42ae3-0578-a246-2f08-88c787c5fdcd@measurement-factory.com>

[There is a potentially useful reframing of the question at the end if
you want to skip the details...]

On 07/26/2018 11:11 PM, Amos Jeffries wrote:
> On 27/07/18 16:18, Alex Rousskov wrote:

>> one could argue that Squid should honor a
>> (higher level) client and server assumption that they are talking to
>> each other (at HTTP+ level).

> Configuring persistence on/off is the way to control that, not pinning.

I disagree. Persistence is about a single connection. Pinning is about
the relationship between two connections. Honoring tunneling
expectations can be about pinning, but it is not about persistence.
Naturally, there is no pinning without persistence, but that is
irrelevant for designing how to support tunneling expectations.


>> We will probably break fewer transactions
>> that way. With that idea in mind, the "first point" becomes establishing
>> a TCP tunnel with the server, even if no client Hello pieces are forwarded.

> HTTP(S) being stateless, at that level any endpoint relying on implicit
> state is non-compliant and buggy.

What is known as HTTP statelessness is irrelevant here: HTTP CONNECT
tunnel itself is a "state"; agents have the right to rely on that state.
That state is not a state that relates to HTTP messages so there is no
contradiction with HTTP stateless principles. From HTTP point of view,
there are no HTTP messages inside that tunnel. The tunnel is a monolith
at HTTP level. Not treating it as such will break some HTTP-compliant
agents.

Same for intercepted TLS/TCP connections -- the preservation of those
TLS and TCP connections is something an HTTP-compliant agent can
legitimately rely on.


> It is *nice* not to result in visible errors, but not a requirement we
> should stick to at cost of proper behaviour.

The open question is how to define what is "proper". Any behavior will
have pros and cons associated with it. You cannot declare lack of
pinning a "proper behavior" just because most agents do not require
pinning. The decision is more complex than that because there are more
variables involved.


> The client accepted proof that Squid *was* that origin (false or not)
> order to reach said higher levels.

I disagree that being content with Squid certificate (TLS level)
constitutes acceptance of an intermediary that can switch from one
server to another at will (higher level).


> Pinning at a later time due to higher
> level stateful situation is not relevant to the bumping code actions and
> not something we need to consider at the present C2 use of S after C1
> should have pinned it.

Agreed. I do not think that contradicts what I was (and am) saying. The
question here is whether "C1 should have pinned S" after bumping C1 at
step1.


> "after C1 has pinned" - if no pinning happens at all the whole
> statement is irrelevant.

Correct. The question is whether C1 should pin S after bumping at step1.
Today, no such pinning happens, but it could be argued that this is a
bug. It is a complicated question with many variables and no obvious
answers.


> What I'm most confused about here is why S is closed when C1 dies with
> non-pinning.  _unless_ there is pinning between them they should not be
> that closely fate sharing.
> 
> Is the HTTPS message on both C1 and S saying "Connection: close" perhapse?

or perhaps some kind of still-linked FwdState/Client abort if C1 was not
closed properly/orderly.



> Off-topic; TLS has connections that do not use handshakes at all, which
> are becoming more common in TLS/1.3. So I believe such a feature may be
> coming one day, but irrelevant right now.

IIRC, all TLS versions, including v1.3, start a TLS connection with a
handshake. The scope/meaning of that handshake varies across versions,
but there is always some handshake.


>> The open question is whether S should be pinned in the case where C1 is
>> bumped at step1 (i.e., "Case 1" in Vishali's email).

> IMO in *that* specific case S should not be pinned. Because pinning
> would prevent the very reasonable actions of handling server failures by
> opening new Sn connections to finish incomplete responses to the client,
> or retaining a server connection for the quick_abort features.

IMO in that specific case S should be pinned by default because pinning
will break fewer transactions and any breakage would be easier to
triage/explain. That default may also be more "secure" because without
pinning, C1 can start requesting content from other servers, possibly
violating admin expectations that rely on vetting C1 based on TLS-level
information such as SNI).

What do we do now for intercepted HTTP clients -- are they allowed to
request content from a server they were _not_ going to when intercepted?



> Squid should instead be ensuring that the server handshake at TLS level
> matches what it would have used for C2 clean handshake. I may have
> missed it, but that is not yet being done (just TCP equivalence, not TLS).

If we do not honor the tunneling semantics (i.e., do not pin S1), then I
see no reason to ensure that S1 and S2 handshakes match. S2 can even go
to a different server.


The same question can be formulated in a more general way, which may be
useful when comparing what Squid already does across different
configurations/scenarios: Which of the following should be "pinned"?

0. Nothing (free to switch to a different origin server).
1. Destination server name (free to switch to a different IP).
2. Destination IP:port (free to reestablish or reuse a connection).
3. Destination connection (free to serve cache hits).
4. Destination (no freedom at all; like regular tunnels today).

For Case1, Squid probably uses #0 or #1 pinning today. We need to decide
whether Squid should use #2 or #3 instead.

For Case2, Squid uses #3 pinning today AFAICT.

#4 is probably best addressed using the serve_hit directive.


Alex.


From SKumpf at ouc.com  Fri Jul 27 20:48:39 2018
From: SKumpf at ouc.com (Kumpf, Scott)
Date: Fri, 27 Jul 2018 20:48:39 +0000
Subject: [squid-users] splunk 3.5.27-Sec Advisories
Message-ID: <0549660147b84efc8c0daf6f5c8f648b@ouc.com>

Greetings,

The organization I work for is running Splunk for Windows version 3.5.27 which is impacted by 3 security vulnerabilities that were released earlier this year.  From what I can tell, our squid implementation was installed using an MSI package from Diladele.  It is my understanding per the advisories, the first point of contact for support is the maintainer/package vendor.  Diladele referred me back to Squid Developers and the only version that they have made available is version 3.5.27.  As I am not too familiar with source code packaging or compiling, I am in search for some guidance on available options to mitigate or remediate these vulnerabilities.  I believe 2 of them have workarounds that can be implemented by modifying the squid.conf.
As I  am not aware of how to determine how this version was configured at time of build therefore am not 100% certain if my implementation is even vulnerable.  Supposing the software is at risk, the advisories indicate there are patches available for each issue, however, I'm not clear on what to do with the information that the patch link presents.

The vulnerabilities are:
SQUID-2018:3 (CVE-2018-1172), Apr 18, 2018
Fixed from 4.0.13
Denial of Service issue in ESI Response processing.
SQUID-2018:2 (CVE-2018-1000027), Jan 19, 2018
Fixed from 4.0.23
Denial of Service issue in HTTP Response processing.
SQUID-2018:1 (CVE-2018-1000024), Jan 19, 2018
Fixed from 4.0.23
Denial of Service issue in ESI Response processing.

Any and all feedback, guidance, and assistance is greatly appreciated.

Thanks,

Scott

Scott Kumpf
Sr. Network Engineer (Contractor)
Orlando Utilities Commission
Office: (407) 434-4305 / Cell: (386) 547-2698
Email: skumpf at ouc.com





________________________________

DISCLAIMER:
Florida has a very broad public records law. As a result, any written communication created or received by Orlando Utilities Commission officials and employees will be made available to the public and media, upon request, unless otherwise exempt. Under Florida law, email addresses are public records. If you do not want your email address released in response to a public records request, do not send electronic mail to this office. Instead, contact our office by phone or in writing.


From squid3 at treenet.co.nz  Sat Jul 28 05:22:55 2018
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sat, 28 Jul 2018 17:22:55 +1200
Subject: [squid-users] splunk 3.5.27-Sec Advisories
In-Reply-To: <0549660147b84efc8c0daf6f5c8f648b@ouc.com>
References: <0549660147b84efc8c0daf6f5c8f648b@ouc.com>
Message-ID: <060e0538-5851-71ae-8eeb-11a0a6fb7474@treenet.co.nz>

On 28/07/18 08:48, Kumpf, Scott wrote:
> Greetings,
> 
> The organization I work for is running Splunk for Windows version 3.5.27 which is impacted by 3 security vulnerabilities that were released earlier this year.  From what I can tell, our squid implementation was installed using an MSI package from Diladele.  It is my understanding per the advisories, the first point of contact for support is the maintainer/package vendor.  Diladele referred me back to Squid Developers and the only version that they have made available is version 3.5.27.  As I am not too familiar with source code packaging or compiling, I am in search for some guidance on available options to mitigate or remediate these vulnerabilities.  I believe 2 of them have workarounds that can be implemented by modifying the squid.conf.
> As I  am not aware of how to determine how this version was configured at time of build therefore am not 100% certain if my implementation is even vulnerable.  Supposing the software is at risk, the advisories indicate there are patches available for each issue, however, I'm not clear on what to do with the information that the patch link presents.
> 

The command line "squid -v" will list the build options used for your
particular binary along with its particular version. The advisory
section titled "Determining if your version is vulnerable:" is a
checklist to compare against your Squid. One statement there should
match your particular Squid installation.

The fixes for all these are in our 3.5.28 bundle from 10 days ago. I
have not made the official announcements yet (thanks for the reminder)
so Diladele may have not been aware.

I've cc'd Rafael on this reply and also opened an issue in the tracker
specifically notifying of the release so they can start on that while I
do the write-up. <https://github.com/diladele/squid-windows/issues/81>


HTH
Amos


From rafael.akchurin at diladele.com  Sat Jul 28 05:35:46 2018
From: rafael.akchurin at diladele.com (Rafael Akchurin)
Date: Sat, 28 Jul 2018 05:35:46 +0000
Subject: [squid-users] splunk 3.5.27-Sec Advisories
In-Reply-To: <060e0538-5851-71ae-8eeb-11a0a6fb7474@treenet.co.nz>
References: <0549660147b84efc8c0daf6f5c8f648b@ouc.com>,
 <060e0538-5851-71ae-8eeb-11a0a6fb7474@treenet.co.nz>
Message-ID: <A86ADD33-B029-496B-A9AC-BDE3C11D71C8@diladele.com>

Hello Amos, Scott,

Will try building now. Shall be possible by the end of next week I hope.

Best regards,
Rafael Akchurin

> Op 28 jul. 2018 om 07:23 heeft Amos Jeffries <squid3 at treenet.co.nz> het volgende geschreven:
> 
>> On 28/07/18 08:48, Kumpf, Scott wrote:
>> Greetings,
>> 
>> The organization I work for is running Splunk for Windows version 3.5.27 which is impacted by 3 security vulnerabilities that were released earlier this year.  From what I can tell, our squid implementation was installed using an MSI package from Diladele.  It is my understanding per the advisories, the first point of contact for support is the maintainer/package vendor.  Diladele referred me back to Squid Developers and the only version that they have made available is version 3.5.27.  As I am not too familiar with source code packaging or compiling, I am in search for some guidance on available options to mitigate or remediate these vulnerabilities.  I believe 2 of them have workarounds that can be implemented by modifying the squid.conf.
>> As I  am not aware of how to determine how this version was configured at time of build therefore am not 100% certain if my implementation is even vulnerable.  Supposing the software is at risk, the advisories indicate there are patches available for each issue, however, I'm not clear on what to do with the information that the patch link presents.
>> 
> 
> The command line "squid -v" will list the build options used for your
> particular binary along with its particular version. The advisory
> section titled "Determining if your version is vulnerable:" is a
> checklist to compare against your Squid. One statement there should
> match your particular Squid installation.
> 
> The fixes for all these are in our 3.5.28 bundle from 10 days ago. I
> have not made the official announcements yet (thanks for the reminder)
> so Diladele may have not been aware.
> 
> I've cc'd Rafael on this reply and also opened an issue in the tracker
> specifically notifying of the release so they can start on that while I
> do the write-up. <https://github.com/diladele/squid-windows/issues/81>
> 
> 
> HTH
> Amos


From capcoding at gmail.com  Sun Jul 29 04:11:43 2018
From: capcoding at gmail.com (Gordon Hsiao)
Date: Sat, 28 Jul 2018 23:11:43 -0500
Subject: [squid-users] block visit 80/443 browsing via IP(no domain name)
Message-ID: <CAK0iFYzxwt2gQ-+wM9bsrnJF3uLAhhRtpE4pU0Wb4O1qgp3yOA@mail.gmail.com>

is there a way to block any attempt to visit http/https by _any_ IP
directly, i.e.

http://my-IP or https://my-IP (yes this will give a warning for SSL most
likely). here my-IP could be any IPv4 address, for example.

Basically I want to have Squid to enforce all 80/443 access should be done
via a FQDN instead of an IP, is this possible? or should this be handled in
a redirector instead?

Thanks,
Gordon
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20180728/a65bf67a/attachment.htm>

From squid3 at treenet.co.nz  Sun Jul 29 06:32:45 2018
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sun, 29 Jul 2018 18:32:45 +1200
Subject: [squid-users] block visit 80/443 browsing via IP(no domain name)
In-Reply-To: <CAK0iFYzxwt2gQ-+wM9bsrnJF3uLAhhRtpE4pU0Wb4O1qgp3yOA@mail.gmail.com>
References: <CAK0iFYzxwt2gQ-+wM9bsrnJF3uLAhhRtpE4pU0Wb4O1qgp3yOA@mail.gmail.com>
Message-ID: <8883cf05-af98-6788-b42d-c1edd764a116@treenet.co.nz>

On 29/07/18 16:11, Gordon Hsiao wrote:
> is there a way to block any attempt to visit http/https by _any_ IP
> directly, i.e.?
> 
> http://my-IP or https://my-IP (yes this will give a warning for SSL most
> likely

Er, what makes you think that? Squid intercepting HTTPS has to already
be decrypting the TLS in order to see any https:// from the client.


> ). here my-IP could be any IPv4 address, for example.

To match transactions with raw-IP in their HTTP request-line URL use a
dstdom_regex ACL with -n parameter and regex that matches raw-IP.
<http://www.squid-cache.org/Doc/config/acl/>

You should use a regex that matches both IPv4 and IPv6 because they
*will* both be presented at times regardless of whether your systems are
IPv4-only.

You can find an example of a regex and how to use it in this page:
<https://wiki.squid-cache.org/ConfigExamples/Chat/Skype>. Though note
that Skype regex includes the port number ":443" at the end of the
pattern which you may not want.

Also, be aware that intercepted traffic does not operate with domain
names. It often only has access to the IP:port details from TCP SYN
packets. That especially includes intercepted port 443 traffic at the
early stages of SSL-Bump processing.

Is there something in particular you want to achieve with this blocking?

Amos


From capcoding at gmail.com  Sun Jul 29 13:45:53 2018
From: capcoding at gmail.com (Gordon Hsiao)
Date: Sun, 29 Jul 2018 08:45:53 -0500
Subject: [squid-users] block visit 80/443 browsing via IP(no domain name)
In-Reply-To: <mailman.3.1532865602.22048.squid-users@lists.squid-cache.org>
References: <mailman.3.1532865602.22048.squid-users@lists.squid-cache.org>
Message-ID: <CAK0iFYxfgMWv5KuR8--n1w=0gGHehV8s+oaF0-X43jF3CwxdVw@mail.gmail.com>

skype was blocking every raw-ip:443 instead of just its own IPs, a bit too
restricted, though it can have a list of its own IPs and dst might just
work.

I'm trying to see if some chat can be blocked as they uses raw-IP without
DNS at all(similar to what skype did)

yes I know ssl-bump uses IP from TCP-SYN to do fake-CONNECT (intercept
mode), that is still different from a raw-IP with 443/ssl, the latter will
warn because rarely any ssl certificate will have CN in IP format.

there might be some vpn over 443 port that uses raw-IP that I hope to
block, if any.

Thanks,
Gordon

On Sun, Jul 29, 2018 at 7:00 AM <squid-users-request at lists.squid-cache.org>
wrote:

> Send squid-users mailing list submissions to
>         squid-users at lists.squid-cache.org
>
> To subscribe or unsubscribe via the World Wide Web, visit
>         http://lists.squid-cache.org/listinfo/squid-users
> or, via email, send a message with subject or body 'help' to
>         squid-users-request at lists.squid-cache.org
>
> You can reach the person managing the list at
>         squid-users-owner at lists.squid-cache.org
>
> When replying, please edit your Subject line so it is more specific
> than "Re: Contents of squid-users digest..."
>
>
> Today's Topics:
>
>    1. block visit 80/443 browsing via IP(no domain name) (Gordon Hsiao)
>    2. Re: block visit 80/443 browsing via IP(no domain name)
>       (Amos Jeffries)
>
>
> ----------------------------------------------------------------------
>
> Message: 1
> Date: Sat, 28 Jul 2018 23:11:43 -0500
> From: Gordon Hsiao <capcoding at gmail.com>
> To: squid-users at lists.squid-cache.org
> Subject: [squid-users] block visit 80/443 browsing via IP(no domain
>         name)
> Message-ID:
>         <
> CAK0iFYzxwt2gQ-+wM9bsrnJF3uLAhhRtpE4pU0Wb4O1qgp3yOA at mail.gmail.com>
> Content-Type: text/plain; charset="utf-8"
>
> is there a way to block any attempt to visit http/https by _any_ IP
> directly, i.e.
>
> http://my-IP or https://my-IP (yes this will give a warning for SSL most
> likely). here my-IP could be any IPv4 address, for example.
>
> Basically I want to have Squid to enforce all 80/443 access should be done
> via a FQDN instead of an IP, is this possible? or should this be handled in
> a redirector instead?
>
> Thanks,
> Gordon
> -------------- next part --------------
> An HTML attachment was scrubbed...
> URL: <
> http://lists.squid-cache.org/pipermail/squid-users/attachments/20180728/a65bf67a/attachment-0001.html
> >
>
> ------------------------------
>
> Message: 2
> Date: Sun, 29 Jul 2018 18:32:45 +1200
> From: Amos Jeffries <squid3 at treenet.co.nz>
> To: squid-users at lists.squid-cache.org
> Subject: Re: [squid-users] block visit 80/443 browsing via IP(no
>         domain name)
> Message-ID: <8883cf05-af98-6788-b42d-c1edd764a116 at treenet.co.nz>
> Content-Type: text/plain; charset=utf-8
>
> On 29/07/18 16:11, Gordon Hsiao wrote:
> > is there a way to block any attempt to visit http/https by _any_ IP
> > directly, i.e.
> >
> > http://my-IP or https://my-IP (yes this will give a warning for SSL most
> > likely
>
> Er, what makes you think that? Squid intercepting HTTPS has to already
> be decrypting the TLS in order to see any https:// from the client.
>
>
> > ). here my-IP could be any IPv4 address, for example.
>
> To match transactions with raw-IP in their HTTP request-line URL use a
> dstdom_regex ACL with -n parameter and regex that matches raw-IP.
> <http://www.squid-cache.org/Doc/config/acl/>
>
> You should use a regex that matches both IPv4 and IPv6 because they
> *will* both be presented at times regardless of whether your systems are
> IPv4-only.
>
> You can find an example of a regex and how to use it in this page:
> <https://wiki.squid-cache.org/ConfigExamples/Chat/Skype>. Though note
> that Skype regex includes the port number ":443" at the end of the
> pattern which you may not want.
>
> Also, be aware that intercepted traffic does not operate with domain
> names. It often only has access to the IP:port details from TCP SYN
> packets. That especially includes intercepted port 443 traffic at the
> early stages of SSL-Bump processing.
>
> Is there something in particular you want to achieve with this blocking?
>
> Amos
>
>
> ------------------------------
>
> Subject: Digest Footer
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>
>
> ------------------------------
>
> End of squid-users Digest, Vol 47, Issue 58
> *******************************************
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20180729/76cfd4be/attachment.htm>

From Walter.H at mathemainzel.info  Sun Jul 29 16:59:28 2018
From: Walter.H at mathemainzel.info (Walter H.)
Date: Sun, 29 Jul 2018 18:59:28 +0200
Subject: [squid-users] block visit 80/443 browsing via IP(no domain name)
In-Reply-To: <CAK0iFYzxwt2gQ-+wM9bsrnJF3uLAhhRtpE4pU0Wb4O1qgp3yOA@mail.gmail.com>
References: <CAK0iFYzxwt2gQ-+wM9bsrnJF3uLAhhRtpE4pU0Wb4O1qgp3yOA@mail.gmail.com>
Message-ID: <5B5DF270.708@mathemainzel.info>

On 29.07.2018 06:11, Gordon Hsiao wrote:
> is there a way to block any attempt to visit http/https by _any_ IP 
> directly, i.e.
>
> http://my-IP or https://my-IP (yes this will give a warning for SSL 
> most likely). here my-IP could be any IPv4 address, for example.
>
> Basically I want to have Squid to enforce all 80/443 access should be 
> done via a FQDN instead of an IP, is this possible? or should this be 
> handled in a redirector instead?
>
Hi,

I use this

/etc/squid/blockdomains-iphost-acl.squid  contains this

^[12]?[0-9]{1,2}\.[12]?[0-9]{1,2}\.[12]?[0-9]{1,2}\.[12]?[0-9]{1,2}$
^\[([0-9a-f]{0,4})(:|:[0-9a-f]{0,4}){1,7}\]$

/etc/squid/squid.conf contains this

acl allow_domains_iphost dstdom_regex 
"/etc/squid/allowdomains-iphost-acl.squid"
acl block_domains_iphost dstdom_regex 
"/etc/squid/blockdomains-iphost-acl.squid"
...
deny_info ERR_DOMAIN_IPHOST_BLOCKED block_domains_iphost
...
http_access allow allow_domains_iphost
http_access deny block_domains_iphost



-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20180729/15f2cc68/attachment.htm>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: smime.p7s
Type: application/pkcs7-signature
Size: 3491 bytes
Desc: S/MIME Cryptographic Signature
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20180729/15f2cc68/attachment.bin>

From mzgmedia at gmail.com  Sun Jul 29 21:12:21 2018
From: mzgmedia at gmail.com (mzgmedia)
Date: Sun, 29 Jul 2018 16:12:21 -0500 (CDT)
Subject: [squid-users] socks on squid, still working?
Message-ID: <1532898741964-0.post@n4.nabble.com>

is still working to compile squid to be socks server? I didn't manage to do
it

"Intercept.cc:(.text._ZN2Ip9Intercept14ProbeForTproxyERNS_7AddressE+0x93):
undefined reference to `SOCKSbind'
Intercept.cc:(.text._ZN2Ip9Intercept14ProbeForTproxyERNS_7AddressE+0x13e):
undefined reference to `SOCKSbind'
"



--
Sent from: http://squid-web-proxy-cache.1019090.n4.nabble.com/Squid-Users-f1019091.html


From squid3 at treenet.co.nz  Mon Jul 30 04:44:40 2018
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Mon, 30 Jul 2018 16:44:40 +1200
Subject: [squid-users] socks on squid, still working?
In-Reply-To: <1532898741964-0.post@n4.nabble.com>
References: <1532898741964-0.post@n4.nabble.com>
Message-ID: <e57a1e85-ff11-2c23-350d-0a335ec50a25@treenet.co.nz>

On 30/07/18 09:12, mzgmedia wrote:
> is still working to compile squid to be socks server? I didn't manage to do
> it
> 
> "Intercept.cc:(.text._ZN2Ip9Intercept14ProbeForTproxyERNS_7AddressE+0x93):
> undefined reference to `SOCKSbind'
> Intercept.cc:(.text._ZN2Ip9Intercept14ProbeForTproxyERNS_7AddressE+0x13e):
> undefined reference to `SOCKSbind'
> "
> 

Should be possible still. Check the symbol names provided by your SOCKS
library.

SOCKSbind is a specific symbol name defined by SOCKSv4 libraries. The
SOCKSv5 library I use exports 'R' symbols instead. Rbind etc.

Amos


From squid3 at treenet.co.nz  Mon Jul 30 05:00:46 2018
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Mon, 30 Jul 2018 17:00:46 +1200
Subject: [squid-users] block visit 80/443 browsing via IP(no domain name)
In-Reply-To: <5B5DF270.708@mathemainzel.info>
References: <CAK0iFYzxwt2gQ-+wM9bsrnJF3uLAhhRtpE4pU0Wb4O1qgp3yOA@mail.gmail.com>
 <5B5DF270.708@mathemainzel.info>
Message-ID: <e68f7d21-d660-dd5f-db12-3c7dbbadec12@treenet.co.nz>

On 30/07/18 04:59, Walter H. wrote:
> On 29.07.2018 06:11, Gordon Hsiao wrote:
>> is there a way to block any attempt to visit http/https by _any_ IP
>> directly, i.e.?
>>
>> http://my-IP or https://my-IP (yes this will give a warning for SSL
>> most likely). here my-IP could be any IPv4 address, for example.
>>
>> Basically I want to have Squid to enforce all 80/443 access should be
>> done via a FQDN instead of an IP, is this possible? or should this be
>> handled in a redirector instead?
>>
> Hi,
> 
> I use this
> 
> /etc/squid/blockdomains-iphost-acl.squid? contains this
> 
> ^[12]?[0-9]{1,2}\.[12]?[0-9]{1,2}\.[12]?[0-9]{1,2}\.[12]?[0-9]{1,2}$
> ^\[([0-9a-f]{0,4})(:|:[0-9a-f]{0,4}){1,7}\]$
> 

FWIW, these patterns incorrectly match the following strings as valid
raw-IP:
 [:]
 [:::::::]
 [2]
 0.0.0.0

Walter is using them for a blacklist, so no particular harm done AFAIK.
But do not think they are useful to detect valid raw-IPs.

Also, keep in mind that regex is much slower than dstdomain, and risks
manual mistakes like the above pattern match.


> /etc/squid/squid.conf contains this
> 
> acl allow_domains_iphost dstdom_regex
> "/etc/squid/allowdomains-iphost-acl.squid"
> acl block_domains_iphost dstdom_regex
> "/etc/squid/blockdomains-iphost-acl.squid"
> ...
> deny_info ERR_DOMAIN_IPHOST_BLOCKED block_domains_iphost
> ...
> http_access allow allow_domains_iphost
> http_access deny block_domains_iphost
> 

Amos



From squid3 at treenet.co.nz  Mon Jul 30 05:20:18 2018
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Mon, 30 Jul 2018 17:20:18 +1200
Subject: [squid-users] block visit 80/443 browsing via IP(no domain name)
In-Reply-To: <CAK0iFYxfgMWv5KuR8--n1w=0gGHehV8s+oaF0-X43jF3CwxdVw@mail.gmail.com>
References: <mailman.3.1532865602.22048.squid-users@lists.squid-cache.org>
 <CAK0iFYxfgMWv5KuR8--n1w=0gGHehV8s+oaF0-X43jF3CwxdVw@mail.gmail.com>
Message-ID: <a975d266-07b2-f85e-6f0c-75fae693e179@treenet.co.nz>

On 30/07/18 04:59, Walter H. wrote:
> skype was blocking every raw-ip:443 instead of just its own IPs, a bit
> too restricted, though it can have a list of its own IPs and dst might
> just work.

That was the point. Skype is P2P software. Certain versions use raw-IP
to connect to arbitrary IPs. There are no "its IPs" to restrict the
match to. And the more recent versions owned by MS use the Azure cloud -
so any IP in Azure is valid raw-IP for Skype to connect to.


> 
> I'm trying to see if some chat can be blocked as they uses raw-IP
> without DNS at all(similar to what skype did)
> 
> yes I know ssl-bump uses IP from TCP-SYN to do fake-CONNECT (intercept
> mode), that is still different from a raw-IP with 443/ssl, the latter
> will warn because rarely any ssl certificate will have CN in IP format.

That does not make sense. There is a very good reason why we keep
dstdomain ans ssl:server_name as separate ACL types.

That reason is that both can exist simultaneously with different values.
The CN value is never part of https:// URLs.

I think you may be confusing the TLS SNI with X.509 certificate CN
Subject names.
 The former is used in http:// URLs reported by Squid, and the latter is
not.

> 
> there might be some vpn over 443 port that uses raw-IP that I hope to
> block, if any.

Use ssl::server_name_regex with the raw-IP pattern to match raw-IP in
certificate CN fields.

Please be aware that CN contains *multiple* values which may be (often
is) any combination of domain name, raw-IP, arbitrary text strings and
regex patterns. So take extreme care with your regex matching into it.

Your lack of certainty about what VPNs are actually doing indicates that
you probably do not know what you are dealing with here. Please base
your rules and config around what is *actually* happening on your
network. Half-way rules based on guesses are not sufficient protection
by any means if you intend paranoid levels of protection, and harmful if
you intend for opening useful holes in advance of a need existing.

Amos


From helen.rai at nepallink.net  Mon Jul 30 10:58:23 2018
From: helen.rai at nepallink.net (Helen Rai)
Date: Mon, 30 Jul 2018 16:43:23 +0545
Subject: [squid-users] Squid proxy server to log HTTPS traffic
Message-ID: <CAKWnCHTgDCupzAbhFPQBwZ6KkWvuFuaQKi5Vos=kJugb16Q0Zg@mail.gmail.com>

Hi,

I am using  Mikrotik Routerboard Model CRS125-24G-1S-2HnD which Links the
internal network with WAN. One of the port is connected with WiFi Router
which is broadcasting WiFi signal. Some of the devices work on WiFi
network. I have installed squid3 (Version 3.5.27) in Raspberry Pi which is
also connected with Mikrotik. Squid is installed with --enable-SSL.

Now, what I want to do is Redirect or log all the HTTP and HTTPS requests
are done from devices which are in WiFi network to squid proxy server.

I have tried doing it for months now, I am able to log all HTTP traffic but
am unable to do it for HTTPS.

Please help me with this and if you need any detail from me, please contact
me.

----
Helen Rai
Platform operations, Nepallink
44260822 || 9841262275 || support at nepallink.net <abuse at nepallink.net>
http://www.nepallink.net || Link Road, Khusibu
<http://www.facebook.com/pages/NepalLink-Network/334213043398796>
<http://twitter.com/nepallink>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20180730/adf1a416/attachment.htm>

From squid3 at treenet.co.nz  Mon Jul 30 13:49:21 2018
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Tue, 31 Jul 2018 01:49:21 +1200
Subject: [squid-users] Squid proxy server to log HTTPS traffic
In-Reply-To: <CAKWnCHTgDCupzAbhFPQBwZ6KkWvuFuaQKi5Vos=kJugb16Q0Zg@mail.gmail.com>
References: <CAKWnCHTgDCupzAbhFPQBwZ6KkWvuFuaQKi5Vos=kJugb16Q0Zg@mail.gmail.com>
Message-ID: <bc9a13a7-9463-57d6-bba9-982a37096176@treenet.co.nz>

On 30/07/18 22:58, Helen Rai wrote:
> Hi,
> 
> I am using? Mikrotik Routerboard Model CRS125-24G-1S-2HnD which Links
> the internal network with WAN. One of the port is connected with WiFi
> Router which is broadcasting WiFi signal. Some of the devices work on
> WiFi network. I have installed squid3 (Version 3.5.27) in Raspberry Pi
> which is also connected with Mikrotik. Squid is installed with --enable-SSL.
> 

The --enable-ssl option was removed in Squid-3.5.
<http://www.squid-cache.org/Versions/v3/3.5/squid-3.5.28-RELEASENOTES.html#ss4.3>.
Use --with-openssl instead, and ensure the Debian libssl1.0-dev package
is installed when building.

To see any URLs inside the HTTPS you will need --enable-ssl-crtd.


> Now, what I want to do is Redirect or log all the HTTP and HTTPS
> requests are done from devices which are in WiFi network to squid proxy
> server.
> 
> I have tried doing it for months now, I am able to log all HTTP traffic
> but am unable to do it for HTTPS.
> 
> Please help me with this and if you need any detail from me, please
> contact me.
> 

If using the correct build option(s) does not resolve your problem we
will need to know what the Mikrotik routign settings, Rasberry Pi
routing and NAT settings, and squid.conf *_port lines are to provide any
assistance.

Amos


From vishali.somaskanthan at viptela.com  Tue Jul 31 00:37:12 2018
From: vishali.somaskanthan at viptela.com (Vishali Somaskanthan)
Date: Mon, 30 Jul 2018 17:37:12 -0700
Subject: [squid-users] server persistent connections and cache
In-Reply-To: <b2c42ae3-0578-a246-2f08-88c787c5fdcd@measurement-factory.com>
References: <CABfsTT6_JLH01qmk+aaBCtJaqZ0t3Xry=UmePGwPvFn_6fRTCg@mail.gmail.com>
 <797fbde1-8585-6b71-30ae-d86575b0fbd1@measurement-factory.com>
 <CABfsTT6m-=4mSnbez8kepDQTimtaO2p_0N9PnV6CLHYombK+Zw@mail.gmail.com>
 <68233cfa-ba76-82f3-8964-42781c56ca9e@measurement-factory.com>
 <CABfsTT5B6q29zKewLNNRjDQJo9RC-5xOKkDTynz1oa4QYttjNQ@mail.gmail.com>
 <8be71578-b79b-109f-3c9a-44b34e4cb09f@measurement-factory.com>
 <3d11195e-fcd3-f617-cd09-d67b4a23dcfa@treenet.co.nz>
 <89556538-6d5e-54a4-6df0-74eaa7f8bf91@measurement-factory.com>
 <f30cb3ec-4678-0c45-7fb6-41c07db61a60@treenet.co.nz>
 <b2c42ae3-0578-a246-2f08-88c787c5fdcd@measurement-factory.com>
Message-ID: <CABfsTT6jK6v=ZoYX1XJKom1xL8zL5SF5uuPC0bzZyjnBYyZQjw@mail.gmail.com>

Hi Amos, Alex,

Reading your conversations, here is a table of rules for pinning.

*STEP 1* *STEP 2* *STEP 3* *EXPECTED* *IMPLEMENTED*
BUMP - - NO PIN NO PIN
PEEK BUMP - PIN PIN
PEEK STARE BUMP PIN PIN
PEEK PEEK BUMP PIN PIN
SPLICE - - PIN PIN
PEEK SPLICE - PIN PIN
PEEK PEEK SPLICE PIN PIN
PEEK STARE SPLICE PIN PIN

PS:

No pin refers to - Would mean the connection to server will be pooled and
available for re-use for a different client connection. For splice, the TCP
connection to server is available for re-use. For bump, the SSL+TCP is
available for re-use.

Pin - Would mean shared fate for both connections.

2. Can you provide an example where NOT PINNING and not having the shared
fate breaks *fewer transactions*??

3. Is Pinning maintained at SSL level?? When the connection is pooled, do
you save SSL session states ??

4. We are looking forward to do an optimization where the same
TCP connection from squid to server is re-used for multiple requests from
client. Will that be a valid process with squid??



-- 
Regards,
Vishali Somaskanthan
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20180730/89bc1915/attachment.htm>

From rousskov at measurement-factory.com  Tue Jul 31 03:45:54 2018
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Mon, 30 Jul 2018 21:45:54 -0600
Subject: [squid-users] server persistent connections and cache
In-Reply-To: <CABfsTT6jK6v=ZoYX1XJKom1xL8zL5SF5uuPC0bzZyjnBYyZQjw@mail.gmail.com>
References: <CABfsTT6_JLH01qmk+aaBCtJaqZ0t3Xry=UmePGwPvFn_6fRTCg@mail.gmail.com>
 <797fbde1-8585-6b71-30ae-d86575b0fbd1@measurement-factory.com>
 <CABfsTT6m-=4mSnbez8kepDQTimtaO2p_0N9PnV6CLHYombK+Zw@mail.gmail.com>
 <68233cfa-ba76-82f3-8964-42781c56ca9e@measurement-factory.com>
 <CABfsTT5B6q29zKewLNNRjDQJo9RC-5xOKkDTynz1oa4QYttjNQ@mail.gmail.com>
 <8be71578-b79b-109f-3c9a-44b34e4cb09f@measurement-factory.com>
 <3d11195e-fcd3-f617-cd09-d67b4a23dcfa@treenet.co.nz>
 <89556538-6d5e-54a4-6df0-74eaa7f8bf91@measurement-factory.com>
 <f30cb3ec-4678-0c45-7fb6-41c07db61a60@treenet.co.nz>
 <b2c42ae3-0578-a246-2f08-88c787c5fdcd@measurement-factory.com>
 <CABfsTT6jK6v=ZoYX1XJKom1xL8zL5SF5uuPC0bzZyjnBYyZQjw@mail.gmail.com>
Message-ID: <8b36ff7c-3fab-c606-7757-238d42dd5b5d@measurement-factory.com>

On 07/30/2018 06:37 PM, Vishali Somaskanthan wrote:

> 2. Can you provide an example where NOT PINNING?and not having the
> shared fate breaks *fewer transactions*???

I do not know of any specific services like that, but it is easy to
imagine one. For example, consider a server that sends the client the
number of requests that sever has seen on the current TCP connection. A
client expecting to receive "1" in its first response may break after
receiving "10" instead. And it gets worse when Squid-to-server
connections are picked at random for every request.

Like Amos said, HTTP allows the proxy to do juggle to-server connections
like that, so no reasonable plain text client that goes through a proxy
would expect a pinned server connection, but in our "secure" context, a
reasonable client can expect that it is talking directly to the service
at HTTP level.


> 3. Is Pinning maintained at SSL level?? When the?connection is pooled,
> do you save SSL session states ??

Yes and yes: The kind of pinning we are talking about is maintained at
"destination connection" level (see #3 in my earlier response), which
includes both TCP and SSL connection info.


> 4. We are looking forward to do an optimization?where the same
> TCP?connection from squid to server is re-used for multiple requests
> from client. Will that be a valid process with squid???

As the first step, I would post a specific RFC (to the squid-dev mailing
list) to see if there is consensus that what you want is officially
acceptable in principle. Please mention the motivation and include
risk/benefit analysis along with Squid configuration changes (if any).
Then comes implementation and official review.

For more details, please see https://wiki.squid-cache.org/MergeProcedure

N.B. The same TCP connection from squid to server is _already_ reused
for multiple requests from client so what you probably want to optimize
is something different (e.g., the same Squid-to-server TLS connection is
reused for multiple consecutive TLS clients).


HTH,

Alex.


From squid-user at tlinx.org  Tue Jul 31 06:01:31 2018
From: squid-user at tlinx.org (L A Walsh)
Date: Mon, 30 Jul 2018 23:01:31 -0700
Subject: [squid-users] squid as wpad server?
Message-ID: <5B5FFB3B.6070601@tlinx.org>

I seem to remember reading that one way to improve reliability is to have
your proxy software be able to double as a web-server for WPAD.dat.

Is there a published means for doing this w/squid or would something need
to be cobbled together?

I'm currently running a small internal-only webs-server (lighttpd) which
is still slowly suffering from creeping featurism and takes 
re-en-lightening
myself with new SW updates and just thinking some way to do the same
in squid might make for 1 less piece of SW to think about configuring and
keeping alive (not that it needs to do much).

Of course, in addition to updates, there's always the temptation to do
more with it, internally, and probably mangling my wpad server in the
process (at least until I caught it).

Anyway, given the advice in the wiki regarding WPAD and the reliability of
having it in your proxy, it seemed like it might be something that's come
up before...(?)  Would seem to be just a matter of listening for
requests on port 80 (wpad+proxy at same IP) and serving up the
wpad.dat static file. 



From squid-user at tlinx.org  Tue Jul 31 06:23:37 2018
From: squid-user at tlinx.org (L A Walsh)
Date: Mon, 30 Jul 2018 23:23:37 -0700
Subject: [squid-users] Wpad problem (DNS)
In-Reply-To: <1532617580762-0.post@n4.nabble.com>
References: <1532617580762-0.post@n4.nabble.com>
Message-ID: <5B600069.4090309@tlinx.org>

erdosain9 wrote:
>  tail -f /var/log/squid/access.log | grep 192.168.6.22
> 1532616150.629     77 192.168.6.22 TCP_REFRESH_UNMODIFIED/200 316 GET
> http://www.msftncsi.com/ncsi.txt - HIER_DIRECT/200.81.17.41 text/plain
>   
---
    You may have some different setup, but this is what works
for me and seems to work for IE, FF, Opera et al.

For wpad, the browsers should look up wpad locally, 1st --
with no domain, then FQDN like wpad.xxx.example.com, then
wpad.example.com.  I also have my internal hosts setup to
lookup hosts on nmb -- so it also serves the hostname.

    It shouldn't have to go through the proxy to get the wpad
file -- sorta defeats the purpose.

At a windows prompt, I can do nslookup:
>  nslookup wpad
Server:  ishtar.sc.tlinx.org
Address:  192.168.33.1

Name:    Ishtar.sc.tlinx.org
Address:  192.168.33.1
Aliases:  wpad.sc.tlinx.org

---
or from linux:
>  dig wpad

; <<>> DiG 9.9.5-rpz2+rl.14038.05-P1 <<>> wpad
;; global options: +cmd
;; Got answer:
;; ->>HEADER<<- opcode: QUERY, status: NOERROR, id: 34174
;; flags: qr aa rd ra; QUERY: 1, ANSWER: 2, AUTHORITY: 2, ADDITIONAL: 2

;; OPT PSEUDOSECTION:
; EDNS: version: 0, flags:; udp: 4096
;; QUESTION SECTION:
;wpad.sc.tlinx.org.   IN  A

;; ANSWER SECTION:
wpad.sc.tlinx.org.  28000 IN  CNAME Ishtar.sc.tlinx.org.
Ishtar.sc.tlinx.org.  28000 IN  A 192.168.33.1
...more stuff deleted...



--- then trying to get wpad.dat:
>  wget --no-proxy http://wpad/wpad.dat
--2018-07-30 23:09:51--  http://wpad/wpad.dat
Resolving wpad (wpad)... 192.168.33.1
Connecting to wpad (wpad)|192.168.33.1|:80... connected.
HTTP request sent, awaiting response... 200 OK
Length: 1480 (1.4K) [application/octet-stream]
Saving to: ?wpad.dat?
2018-07-30 23:09:51 (190 MB/s) - ?wpad.dat? saved [1480/1480]

and my wpad.dat looks sorta like (BTW -- my socks stuff has never worked 
right, but haven't bothered to track it down, so might wanna ignore it!).
also on my net, web-proxy, socks, etc...all point to same machine.



function FindProxyForURL(url, host) {
    // Some functions available:
    //    isPlainHostName(host)
    //    dnsDomainIs(host,".domainname.org")
    //    localHostOrDomainIs(host, "myhost.mydom.org")  (exact matching)
    //    isResolvable(host) - brwsr may 'longpause' if host !resolvable
    //    isInNet(host, 192.168.3.0, mask)
    //    dnsResolve(host) - returns IP from hostname
    //    shExpMatch(url, "*vpn.domain.com*")
    //    weekdayRange("MON, "FRI")
    //    dateRange("JAN", "MAR")
    //    timeRange(8, 18)
    //    myIpAddress() - my own numeric IP

    // References:
    // see http://en.wikipedia.org/wiki/Proxy_auto-config
    // or http://www.findproxyforurl.com

    var direct = "DIRECT";
    var sc_http_proxy = "PROXY web-proxy.sc.example.org:8118";
    var sc_socks_proxy = "SOCKS socks.sc.example.org:1080";
    var sc_http_w_socks_backup = sc_http_proxy + sc_socks_proxy;
    var news_proto_re = /^s?news://.*$/;
    var news_port_re = /^[^:]+://[^:/]+:(?:119|563)/.*$/;

    if (isInNet(host, "192.168.0.0", "255.255.255.0") ||
            isInNet(host, "192.168.1.0", "255.255.255.0") ||
            isInNet(host, "192.168.100.0", "255.255.255.0") ) {
            return sc_http_proxy;
    }
    if (isInNet(myIpAddress(), "192.168.33.0", "255.255.255.0") {
        if (isPlainHostName(host) || dnsDomainIs(host, ".sc.example.org")
                || dnsDomainIs(host, ".example.org")) {
            return direct;
        } else {
            // 119/563 for news
            if (news_proto_re.exec(url) ||
                news_port_re.exec(url)  ) {
                return sc_socks_proxy;
            }
            return sc_http_proxy;
        }
    }
    return direct;
}

// vim: ts=2 sw=2 syntax=javascript


================

Hope the above helps...it was pretty simple...





From squid3 at treenet.co.nz  Tue Jul 31 06:44:57 2018
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Tue, 31 Jul 2018 18:44:57 +1200
Subject: [squid-users] squid as wpad server?
In-Reply-To: <5B5FFB3B.6070601@tlinx.org>
References: <5B5FFB3B.6070601@tlinx.org>
Message-ID: <3e29f164-1857-f669-a275-d2d5e58dabfa@treenet.co.nz>

On 31/07/18 18:01, L A Walsh wrote:
> I seem to remember reading that one way to improve reliability is to have
> your proxy software be able to double as a web-server for WPAD.dat.
> 
> Is there a published means for doing this w/squid or would something need
> to be cobbled together?
> 

The work I was doing on Squid having an internal web server got mired in
the QA feature creep and abandoned.

Currently I use this config:

 acl WPAD urlpath_regex ^/wpad.dat$
 deny_info 200:wpad.dat WPAD
 http_access deny WPAD
 reply_header_access Content-Type deny WPAD
 reply_header_replace Content-Type application/x-ns-proxy-autoconfig


With a symlink from /usr/share/squid/errors/templates/wpad.dat pointing
to my /etc/squid/wpad.dat file.

This has the added benefit that the wpad.dat can use Squid custom error
page %h macro to insert the proxy hostname dynamically without needing a
different wpad.dat for each POP.

I do have to take care that the templates package being upgraded does
not erase the symlink though.

Amos


From ygirardin at olfeo.com  Tue Jul 31 07:35:17 2018
From: ygirardin at olfeo.com (ygirardin)
Date: Tue, 31 Jul 2018 09:35:17 +0200
Subject: [squid-users] url_rewrite_timeout
Message-ID: <79ab2337-b6d3-0168-f72c-f70e0e910806@olfeo.com>

Hi,


I'm trying to use the new squid4 directive url_rewrite_timeout.


In order to make sure it works my rewritter is blocking to enable the
timeout.

But nothing happen, I thought my browser will receive a 500 error with
the following configuration but nope nothing happend and no trace in
cache.log.


Here is my configuration :


url_rewrite_timeout 10 seconds on_timeout=fail


What am i doing wrong ?


Thank you



From belle at bazuin.nl  Tue Jul 31 07:43:04 2018
From: belle at bazuin.nl (=?windows-1252?Q?L.P.H._van_Belle?=)
Date: Tue, 31 Jul 2018 09:43:04 +0200
Subject: [squid-users] squid as wpad server?
In-Reply-To: <5B5FFB3B.6070601@tlinx.org>
References: <5B5FFB3B.6070601@tlinx.org>
Message-ID: <vmime.5b601308.6cc5.4cf05538366acf88@ms249-lin-003.rotterdam.bazuin.nl>

Read :
https://findproxyforurl.com/official-toolset/ 
That one helped me a lot, all you want to know is there imo. 

Greetz, 

Louis

> -----Oorspronkelijk bericht-----
> Van: squid-users 
> [mailto:squid-users-bounces at lists.squid-cache.org] Namens L A Walsh
> Verzonden: dinsdag 31 juli 2018 8:02
> Aan: squid-users at squid-cache.org
> Onderwerp: [squid-users] squid as wpad server?
> 
> I seem to remember reading that one way to improve 
> reliability is to have
> your proxy software be able to double as a web-server for WPAD.dat.
> 
> Is there a published means for doing this w/squid or would 
> something need
> to be cobbled together?
> 
> I'm currently running a small internal-only webs-server 
> (lighttpd) which
> is still slowly suffering from creeping featurism and takes 
> re-en-lightening
> myself with new SW updates and just thinking some way to do the same
> in squid might make for 1 less piece of SW to think about 
> configuring and
> keeping alive (not that it needs to do much).
> 
> Of course, in addition to updates, there's always the temptation to do
> more with it, internally, and probably mangling my wpad server in the
> process (at least until I caught it).
> 
> Anyway, given the advice in the wiki regarding WPAD and the 
> reliability of
> having it in your proxy, it seemed like it might be something 
> that's come
> up before...(?)  Would seem to be just a matter of listening for
> requests on port 80 (wpad+proxy at same IP) and serving up the
> wpad.dat static file. 
> 
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
> 



From squid3 at treenet.co.nz  Tue Jul 31 07:43:52 2018
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Tue, 31 Jul 2018 19:43:52 +1200
Subject: [squid-users] url_rewrite_timeout
In-Reply-To: <79ab2337-b6d3-0168-f72c-f70e0e910806@olfeo.com>
References: <79ab2337-b6d3-0168-f72c-f70e0e910806@olfeo.com>
Message-ID: <99c420e2-355d-e2e7-0999-0391b472f584@treenet.co.nz>

On 31/07/18 19:35, ygirardin wrote:
> Hi,
> 
> 
> I'm trying to use the new squid4 directive url_rewrite_timeout.
> 
> 
> In order to make sure it works my rewritter is blocking to enable the
> timeout.
> 
> But nothing happen, I thought my browser will receive a 500 error with
> the following configuration but nope nothing happend and no trace in
> cache.log.
> 
> 
> Here is my configuration :
> 
> 
> url_rewrite_timeout 10 seconds on_timeout=fail
> 
> 
> What am i doing wrong ?
> 

What URL are you requesting?

What shows up in access.log ?

Does the browser actually use the proxy?

Perhapse using something less likely to use non-HTTP protocols for the
fetch will show things better. Try squidclient to do the fetch.

Amos


From m_zouhairy at skno.by  Tue Jul 31 07:55:24 2018
From: m_zouhairy at skno.by (Vacheslav)
Date: Tue, 31 Jul 2018 10:55:24 +0300
Subject: [squid-users] microsoft list
Message-ID: <470501d428a3$d6a70f80$83f52e80$@skno.by>

Peace,
Anyone got a url list to block windows update, akamia, and spying from microsoft?




From ygirardin at olfeo.com  Tue Jul 31 07:56:24 2018
From: ygirardin at olfeo.com (ygirardin)
Date: Tue, 31 Jul 2018 09:56:24 +0200
Subject: [squid-users] url_rewrite_timeout
In-Reply-To: <99c420e2-355d-e2e7-0999-0391b472f584@treenet.co.nz>
References: <79ab2337-b6d3-0168-f72c-f70e0e910806@olfeo.com>
 <99c420e2-355d-e2e7-0999-0391b472f584@treenet.co.nz>
Message-ID: <31f77ce1-054a-efc8-c676-b211e52880bd@olfeo.com>

Hi


Of course my browser used my proxy.


I try to request www.perdu.com (that's a simple website with only http).


Indeed when I look on the access.log I have the following trace :


1533023517.502?? 4990 10.1.0.39 NONE_ABORTED/000 0 GET http://perdu.com/ 
- HIER_NONE/- -

So seems like the connection is abort, but this should not stop my 
rewriter ?

And if my rewritter reply OK after the 10 seconds timeout the page is 
well displayed in my browser. I was expecting to receive a 500 error.




On 31/07/2018 09:43, Amos Jeffries wrote:
> On 31/07/18 19:35, ygirardin wrote:
>> Hi,
>>
>>
>> I'm trying to use the new squid4 directive url_rewrite_timeout.
>>
>>
>> In order to make sure it works my rewritter is blocking to enable the
>> timeout.
>>
>> But nothing happen, I thought my browser will receive a 500 error with
>> the following configuration but nope nothing happend and no trace in
>> cache.log.
>>
>>
>> Here is my configuration :
>>
>>
>> url_rewrite_timeout 10 seconds on_timeout=fail
>>
>>
>> What am i doing wrong ?
>>
> What URL are you requesting?
>
> What shows up in access.log ?
>
> Does the browser actually use the proxy?
>
> Perhapse using something less likely to use non-HTTP protocols for the
> fetch will show things better. Try squidclient to do the fetch.
>
> Amos
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users



From squid-user at tlinx.org  Tue Jul 31 08:00:41 2018
From: squid-user at tlinx.org (L A Walsh)
Date: Tue, 31 Jul 2018 01:00:41 -0700
Subject: [squid-users] squid as wpad server?
In-Reply-To: <vmime.5b601308.6cc5.4cf05538366acf88@ms249-lin-003.rotterdam.bazuin.nl>
References: <5B5FFB3B.6070601@tlinx.org>
 <vmime.5b601308.6cc5.4cf05538366acf88@ms249-lin-003.rotterdam.bazuin.nl>
Message-ID: <5B601729.3060007@tlinx.org>

L.P.H. van Belle wrote:
> Read :
> https://findproxyforurl.com/official-toolset/ 
> That one helped me a lot, all you want to know is there imo. 
----
	Seems like that is mostly about setting one up.

	I already have a working one, but in reading docs on the squid wiki
it was suggested that having your proxy being able to 'double' as 
your web-proxy was one way to reduce some complexity -- so I wasn't 
trying to set one up -- already did that with a smallish webserver that
serves up my wpad.dat file, with dns and dhcp records both pointing at
that resource -- seems to work find for all tested browsers.

	It's the merge thingy that likely wouldn't be in a standard toolset
that does something like what Jeffery maybe had done that might suffice.

I'll have to try that to give it a spin.

To someone else's setup question, I threw my current dns-like settings at them
and mentioned how   wpad needs to be contacted before the proxy as wpad is 
supposed to tell a browser where the proxy is or which proxy to use.

Thanks for the pointers though!                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          


From squid3 at treenet.co.nz  Tue Jul 31 08:46:16 2018
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Tue, 31 Jul 2018 20:46:16 +1200
Subject: [squid-users] url_rewrite_timeout
In-Reply-To: <31f77ce1-054a-efc8-c676-b211e52880bd@olfeo.com>
References: <79ab2337-b6d3-0168-f72c-f70e0e910806@olfeo.com>
 <99c420e2-355d-e2e7-0999-0391b472f584@treenet.co.nz>
 <31f77ce1-054a-efc8-c676-b211e52880bd@olfeo.com>
Message-ID: <8cfa0594-578e-ef4f-c8dc-8addcc3cb4c6@treenet.co.nz>

On 31/07/18 19:56, ygirardin wrote:
> Hi
> 
> 
> Of course my browser used my proxy.
> 
> 
> I try to request www.perdu.com (that's a simple website with only http).
> 
> 
> Indeed when I look on the access.log I have the following trace :
> 
> 
> 1533023517.502?? 4990 10.1.0.39 NONE_ABORTED/000 0 GET http://perdu.com/
> - HIER_NONE/- -
> 
> So seems like the connection is abort,

It is. Client aborted/disconnected after 5 sec. Your re-writer timeout
is 10 sec.


> but this should not stop my
> rewriter ?

The transaction ended before any rewriter action was needed. There is
nothing to be done by Squid for this client.

> 
> And if my rewritter reply OK after the 10 seconds timeout the page is
> well displayed in my browser. I was expecting to receive a 500 error.
> 

Helper responses are cached when possible. If your test requests the
same URL twice in a row from the same Squid the second one can use a
result provided for the first client request. Like so:


1) Client A connects, requests http://perdu.com/

2) Squid asks helper for URL alterations of http://perdu.com/

3) client A disconnects,

4) helper request A times out, client already gone, nothing happens.

5) helper responds with result "OK", Squid caches that result.

6) client B connects, requests http://perdu.com/

7) Squid uses result "OK" for helper lookup http://perdu.com/



Amos


From mdacova at netpilot.com  Tue Jul 31 10:01:14 2018
From: mdacova at netpilot.com (Michael Da Cova)
Date: Tue, 31 Jul 2018 11:01:14 +0100
Subject: [squid-users] microsoft list
In-Reply-To: <470501d428a3$d6a70f80$83f52e80$@skno.by>
References: <470501d428a3$d6a70f80$83f52e80$@skno.by>
Message-ID: <0a3c04d2-8d48-a074-fa5c-1a7480d8158a@netpilot.com>

Hi


On 31/07/18 08:55, Vacheslav wrote:
> Peace,
> Anyone got a url list to block windows update, akamia,
you could start by looking at 
https://technet.microsoft.com/en-gb/library/bb693717.aspx
who you going to deal with updates?
> and spying from microsoft?
good luck with that or you could install a linux distro
>
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users

Michael


From m_zouhairy at skno.by  Tue Jul 31 11:32:21 2018
From: m_zouhairy at skno.by (Vacheslav)
Date: Tue, 31 Jul 2018 14:32:21 +0300
Subject: [squid-users] microsoft list
In-Reply-To: <0a3c04d2-8d48-a074-fa5c-1a7480d8158a@netpilot.com>
References: <470501d428a3$d6a70f80$83f52e80$@skno.by>
 <0a3c04d2-8d48-a074-fa5c-1a7480d8158a@netpilot.com>
Message-ID: <582201d428c2$2557bb20$70073160$@skno.by>

Satanic greetings, you really think you are the smartest of us all that we can't think of your stupid suggestion?

-----Original Message-----
From: Michael Da Cova <mdacova at netpilot.com> 
Sent: Tuesday, July 31, 2018 1:01 PM
To: Vacheslav <m_zouhairy at skno.by>; squid-users at lists.squid-cache.org
Subject: Re: [squid-users] microsoft list

Hi


On 31/07/18 08:55, Vacheslav wrote:
> Peace,
> Anyone got a url list to block windows update, akamia,
you could start by looking at 
https://technet.microsoft.com/en-gb/library/bb693717.aspx
who you going to deal with updates?
> and spying from microsoft?
good luck with that or you could install a linux distro
>
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users

Michael




From squid3 at treenet.co.nz  Tue Jul 31 16:33:31 2018
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Wed, 1 Aug 2018 04:33:31 +1200
Subject: [squid-users] microsoft list
In-Reply-To: <582201d428c2$2557bb20$70073160$@skno.by>
References: <470501d428a3$d6a70f80$83f52e80$@skno.by>
 <0a3c04d2-8d48-a074-fa5c-1a7480d8158a@netpilot.com>
 <582201d428c2$2557bb20$70073160$@skno.by>
Message-ID: <1fc09ac8-c53d-3c08-fb61-86f55f8af8d9@treenet.co.nz>

On 31/07/18 23:32, Vacheslav wrote:
> Satanic greetings, you really think you are the smartest of us all that we can't think of your stupid suggestion?
> 

Please be civil. The response you received was a reasonable answer to
your question.

Both the TechNet article linked and the Squid FAQ config example for
Windows Update contains lists of domains that service uses.

Blocking the rest of Microsoft and all of Akamai *by URL* is a much more
difficult proposition. Between them they host a very large percentage of
Internet domains.

Amos


From squid3 at treenet.co.nz  Tue Jul 31 06:08:36 2018
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Tue, 31 Jul 2018 18:08:36 +1200
Subject: [squid-users] [squid-announce] Squid 3.5.28 is available
Message-ID: <9613c3c7-4014-8b99-ec2a-2c71de224414@treenet.co.nz>

The Squid HTTP Proxy team is very pleased to announce the availability
of the Squid-3.5.28 release!



This release is a security fix release resolving several major issues
found in the prior Squid releases.

    REMINDER: This and older releases are already deprecated by
              Squid-4.1 availability.


The major changes to be aware of:

* SQUID-2018:1 / CVE-2018-1000024
  Crash processing SSL-Bumped traffic containing ESI

  http://www.squid-cache.org/Advisories/SQUID-2018_1.txt

This problem allows a remote server delivering certain ESI
response syntax to trigger a denial of service for all clients
accessing the Squid service.

Squid-3.5 is also vulnerable to some regular ESI server responses
also triggering this issue.

This problem is limited to the Squid custom ESI parser.
Squid built to use libxml2 or libexpat XML parsers do not have
this problem.


* SQUID-2018:2 / CVE-2018-1000027
  Crash handling responses to internally generated requests

  http://www.squid-cache.org/Advisories/SQUID-2018_2.txt

Due to incorrect pointer handling Squid is vulnerable to denial
of service attack when processing ESI responses or downloading
intermediate CA certificates.

This problem allows a remote client delivering certain HTTP
requests in conjunction with certain trusted server responses to
trigger a denial of service for all clients accessing the Squid
service.


* SQUID-2018:3 / CVE-2018-1172
  Crash in ESI Response processing

  http://www.squid-cache.org/Advisories/SQUID-2018_3.txt

This problem allows a remote server delivering ESI responses
to trigger a denial of service for all clients accessing the
Squid service.

This problem is limited to Squid operating as reverse proxy.


* Bug 4829: IPC shared memory leaks when disker queue overflows

This bug occurs when Squid is configured with rock only storage. After
a long period of high load or a shorter period of extremely high load,
disk IO drops entirely. Even after giving Squid time to recover and
then resuming a low load the diskers were just not doing anything.

A lot of "run out of shared memory pages for IPC I/O" errors may be
seen during the high load, which continues to remain on smaller loads
after the recovery time.


* Bug 4767: SMP breaks IPv6 SNMP and cache manager queries

This problem appears as a crash when Squid is operating with multiple
workers and receiving IPv6 SNMP queries.


* Bug 2821: Ignore Content-Range in non-206 responses

Squid used to honor Content-Range header in HTTP 200 OK (and possibly
other non-206) responses, truncating (and possibly enlarging) some
response bodies. RFC 7233 declares Content-Range meaningless for
standard HTTP status codes other than 206 and 416. Squid now relays
meaningless Content-Range as is, without using its value.


* SSL-Bump: fix authentication with schemes other than Basic

Squid-3.4.5 included a fix for handling Basic authentication of a
CONNECT tunnel which is being bump'ed. Requests within it were
intended to inherit the credentials of the tunnel. Allowing Squid ACLs
to use authentication tests on the bumped traffic.

This release finally extends that fix to make bumped traffic inherit
the authentication credentials from the CONNECT tunnel regardless of
authentication type.


* TPROXY: Fix clientside_mark and client port logging

The clientside_mark ACL was not working with TPROXY because a
conntrack query could not find connmark without a true client port.

This also affected helpers and ACLs using client dst-port number
prior to logging when traffic was received with TPROXY.


* Fix "Cannot assign requested address" for to-origin TPROXY FTP data

This release adds the capability for TPROXY to be used on Native FTP
traffic (received at ftp_port). Prior releases would present the above
error when establishing FTP data connection and abort the transaction.



 All users of Squid-3 with SSL-Bump functionality are encouraged to
upgrade to this release as soon as possible.

 All other users of Squid-3 are encouraged to upgrade to this release
as time permits.


 See the ChangeLog for the full list of changes in this and earlier
 releases.

Please refer to the release notes at
http://www.squid-cache.org/Versions/v3/3.5/RELEASENOTES.html
when you are ready to make the switch to Squid-3.5

Upgrade tip:
  "squid -k parse" is starting to display even more
   useful hints about squid.conf changes.

This new release can be downloaded from our HTTP or FTP servers

 http://www.squid-cache.org/Versions/v3/3.5/
 ftp://ftp.squid-cache.org/pub/squid/
 ftp://ftp.squid-cache.org/pub/archive/3.5/

or the mirrors. For a list of mirror sites see

 http://www.squid-cache.org/Download/http-mirrors.html
 http://www.squid-cache.org/Download/mirrors.html

If you encounter any issues with this release please file a bug report.
http://bugs.squid-cache.org/


Amos Jeffries
_______________________________________________
squid-announce mailing list
squid-announce at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-announce

