From squid3 at treenet.co.nz  Mon Jun  4 05:46:59 2018
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Mon, 4 Jun 2018 17:46:59 +1200
Subject: [squid-users] quiet week
Message-ID: <4578918d-3953-74ca-b65f-e7a148abbe4e@treenet.co.nz>

Hi anyone,
 just testing to see if the list server is still operational. Things
have been suspiciously quiet this week.

Amos


From dan at getbusi.com  Mon Jun  4 05:49:07 2018
From: dan at getbusi.com (Dan Charlesworth)
Date: Mon, 4 Jun 2018 15:49:07 +1000
Subject: [squid-users] quiet week
In-Reply-To: <4578918d-3953-74ca-b65f-e7a148abbe4e@treenet.co.nz>
References: <4578918d-3953-74ca-b65f-e7a148abbe4e@treenet.co.nz>
Message-ID: <CAN8nrKCVa_=BEoHwY_tLJ_FZaHip5xo6Jpb0s+yb176r2zn=WA@mail.gmail.com>

Copy, Amos ? receiving you loud and clear :)

On Mon, 4 Jun 2018 at 15:47, Amos Jeffries <squid3 at treenet.co.nz> wrote:

> Hi anyone,
>  just testing to see if the list server is still operational. Things
> have been suspiciously quiet this week.
>
> Amos
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>


-- 
Getbusi
p +61 3 6165 1555
e dan at getbusi.com
w getbusi.com
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20180604/38f36461/attachment.htm>

From eliezer at ngtech.co.il  Mon Jun  4 07:51:54 2018
From: eliezer at ngtech.co.il (Eliezer Croitoru)
Date: Mon, 4 Jun 2018 10:51:54 +0300
Subject: [squid-users] quiet week
In-Reply-To: <4578918d-3953-74ca-b65f-e7a148abbe4e@treenet.co.nz>
References: <4578918d-3953-74ca-b65f-e7a148abbe4e@treenet.co.nz>
Message-ID: <00b301d3fbd8$e83cc200$b8b64600$@ngtech.co.il>

Reply...

Indeed it has been quiet in the list but I believe it's also due to a milestone for the project.
The current 3.5.27 code proves to be pretty mature and my repo is pretty busy due to this.
I noticed that Ubuntu 18.04 has 3.5.27 in it so it's something that cannot be ignored at all.

I do not have too much free time but I am waiting for 4.X since most of the industry already have it's features.
I noticed something interesting about the choice between CentOS to Ubuntu:
CentOS is trying mainly to stabilize things in a very nice way while Ubuntu is stable but active on specific things.

I like them both but they fit for different purposes.
I am not sure but to me it seems that Squid-Cache cycle of development and CentOS are not 100% on the same roadmap as Debian and Ubuntu.

And an update... I have a fleet of Squid's up and running 3.5.27 under heavy duty as a WAF solution.
SNI is missing for the reverse proxy setup's but the clients and the admins do not whistle so it's good..
And for greedy cache admins that want's to try their luck converting the next setup into squid.conf here something nice:
https://github.com/bntjah/lancache

I didn't reviewed every part of the above setup but it seems to me that it deserves to be mentioned in this list.
Hmm what else?
Hoo and and there are couple commercial products that uses Squid-Cache in ways I never imagined.
I mean, they could have taken a developer to write for them a full solution(it's easy) but they choose Squid-Cache since they feel it's a solid ground.

Amos, am I right about the roadmap of Squid-Cache to be much closer to Debian and Ubuntu compared to CentOS?

Eliezer

----
Eliezer Croitoru
Linux System Administrator
Mobile: +972-5-28704261
Email: eliezer at ngtech.co.il


-----Original Message-----
From: squid-users <squid-users-bounces at lists.squid-cache.org> On Behalf Of Amos Jeffries
Sent: Monday, June 4, 2018 08:47
To: squid-users at lists.squid-cache.org
Subject: [squid-users] quiet week

Hi anyone,
 just testing to see if the list server is still operational. Things
have been suspiciously quiet this week.

Amos
_______________________________________________
squid-users mailing list
squid-users at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-users



From squid3 at treenet.co.nz  Mon Jun  4 08:38:56 2018
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Mon, 4 Jun 2018 20:38:56 +1200
Subject: [squid-users] quiet week
In-Reply-To: <00b301d3fbd8$e83cc200$b8b64600$@ngtech.co.il>
References: <4578918d-3953-74ca-b65f-e7a148abbe4e@treenet.co.nz>
 <00b301d3fbd8$e83cc200$b8b64600$@ngtech.co.il>
Message-ID: <9174de24-068b-ac75-4fed-fc0b84bc7d90@treenet.co.nz>

On 04/06/18 19:51, Eliezer Croitoru wrote:
> 
> Amos, am I right about the roadmap of Squid-Cache to be much closer to Debian and Ubuntu compared to CentOS?
> 

Debian is a "seed" type of distribution, like Fedora, Arch, Slack,
Gentoo, Mageus, and some others not packaging Squid (Desktop or IoT
focus, etc). They get their code directly from projects/communities like
us. Most distributions tend to be based on one of those seeds with only
a change to what they focus on.

CentOS is a third-level derivative from Fedora (Fedora->RHEL->CentOS),
whereas Ubuntu is a second-level derivative of Debian (Debian->Ubuntu).

That is probably where you are seeing the difference between CentOS
(waiting for RHEL stability, which itself only starts after Fedora
stability first) vs Ubuntu (waiting only for Debian stability).


I am trying to make the Squid stable releases have a regular cadence
that suits the seeder OS release timetables where possible (Jan or
July). So that the derivatives which wait for packages to get stable in
the seeder first are not left stuck for decades with outdated Squid
versions. That is still subject to bugs and what feature have been
committed though. So as yet the "regular" part does not quite exist and
we get delays like v4 with difficult bugs.

Amos


From skupko.sk at gmail.com  Mon Jun  4 09:55:36 2018
From: skupko.sk at gmail.com (Peter Viskup)
Date: Mon, 4 Jun 2018 11:55:36 +0200
Subject: [squid-users] quiet week
In-Reply-To: <9174de24-068b-ac75-4fed-fc0b84bc7d90@treenet.co.nz>
References: <4578918d-3953-74ca-b65f-e7a148abbe4e@treenet.co.nz>
 <00b301d3fbd8$e83cc200$b8b64600$@ngtech.co.il>
 <9174de24-068b-ac75-4fed-fc0b84bc7d90@treenet.co.nz>
Message-ID: <CAPa6PsH1zhzfynjXoG5FaVOfGz3GJxjyMoG=q5fQFNhULsMQfQ@mail.gmail.com>

There are significant differences between the Fedora->RHEL and
Debian->Ubuntu relations.
Fedora is development oriented with short lifecycle and without support of
previous versions. RHEL is built on Fedora with stability and enterprise
needs in mind (they always ).

Debian provides more release stages
 - stable -> Ubuntu TLS releases
 - testing -> standard Ubuntu releases
 - unstable

Ubuntu LTS releases are based on current Debian stable release. Standard
Ubuntu relases are based on Debian testing. Both developer communities
share parts of infra and are working "together".
More information can be read here https://www.ubuntu.com/community/debian

This is major difference.

The RHEL->CentOS relation is of different type. CentOS is RHEL without
copyrighted materials. Updates are usually released with delays. There are
some other repositories which extend/modify CentOS core
https://en.wikipedia.org/wiki/CentOS#REPOS

-- 
Peter

On Mon, Jun 4, 2018 at 10:38 AM, Amos Jeffries <squid3 at treenet.co.nz> wrote:

> On 04/06/18 19:51, Eliezer Croitoru wrote:
> >
> > Amos, am I right about the roadmap of Squid-Cache to be much closer to
> Debian and Ubuntu compared to CentOS?
> >
>
> Debian is a "seed" type of distribution, like Fedora, Arch, Slack,
> Gentoo, Mageus, and some others not packaging Squid (Desktop or IoT
> focus, etc). They get their code directly from projects/communities like
> us. Most distributions tend to be based on one of those seeds with only
> a change to what they focus on.
>
> CentOS is a third-level derivative from Fedora (Fedora->RHEL->CentOS),
> whereas Ubuntu is a second-level derivative of Debian (Debian->Ubuntu).
>
> That is probably where you are seeing the difference between CentOS
> (waiting for RHEL stability, which itself only starts after Fedora
> stability first) vs Ubuntu (waiting only for Debian stability).
>
>
> I am trying to make the Squid stable releases have a regular cadence
> that suits the seeder OS release timetables where possible (Jan or
> July). So that the derivatives which wait for packages to get stable in
> the seeder first are not left stuck for decades with outdated Squid
> versions. That is still subject to bugs and what feature have been
> committed though. So as yet the "regular" part does not quite exist and
> we get delays like v4 with difficult bugs.
>
> Amos
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20180604/3563dc43/attachment.htm>

From Edward.Cheadle at cambiahealth.com  Mon Jun  4 18:05:43 2018
From: Edward.Cheadle at cambiahealth.com (Cheadle, Edward)
Date: Mon, 4 Jun 2018 18:05:43 +0000
Subject: [squid-users] Connection Timeouts
Message-ID: <973AC2B7-9270-4378-97E5-0D6AEF24624A@contoso.com>

We had a person leave and I got selected to update and maintain our squid proxy.   We are talking to AWS and they told us that we needed to change the connection_timeout value from the default to 5 min.

We have people stress testing out installation and I was concerned that if connection timeouts are too long we may see congestion.

Should I be worried that connection timeouts will use up file descriptors at a higher rate?

And what might be the options?

Doing and internet search I found a web page at https://www.visolve.com/squid/squid30/timeout.php and in the TIMEOUT description I read


?TIMEOUT

Timeout parameters in Squid can be based on overall connection timeouts, peer-specific timeouts, site/domain-specific timeouts, request-specific timeouts etc. Proper setting of timeout values is critical to optimal Squid performance. Relevant parameters for timeout settings are listed?



Is it possible to narrow the connection timeout to a specific site?  I looked at the website information, squid documentation and did an internet search.



I did not see anything that narrowed the timeout to a specific timeout.



I am trying to set connection timeouts to AWS sites, but keep connection timeouts to the default, because it is working well.


IMPORTANT NOTICE: This communication, including any attachment, contains information that may be confidential or privileged, and is intended solely for the entity or individual to whom it is addressed.  If you are not the intended recipient, you should delete this message and are hereby notified that any disclosure, copying, or distribution of this message is strictly prohibited.  Nothing in this email, including any attachment, is intended to be a legally binding signature.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20180604/e2a10632/attachment.htm>

From eliezer at ngtech.co.il  Mon Jun  4 18:30:35 2018
From: eliezer at ngtech.co.il (Eliezer Croitoru)
Date: Mon, 4 Jun 2018 21:30:35 +0300
Subject: [squid-users] Connection Timeouts
In-Reply-To: <973AC2B7-9270-4378-97E5-0D6AEF24624A@contoso.com>
References: <973AC2B7-9270-4378-97E5-0D6AEF24624A@contoso.com>
Message-ID: <031401d3fc32$218ed710$64ac8530$@ngtech.co.il>

Hey Edward,

First congrats!.
I hope we can help you to figure out the relevant details.

I am not sure why you have spoken to AWS teams about Squid-Cache, may I ask what OS are you using in AWS?
Also what version of Squid are you using?
The timeout settings are "critical" indeed but depends on what you are using and doing with Squid-Cache.
Despite to the fact that https://www.visolve.com/squid/squid30/timeout Is in a way still a lead it's not "up-to-date"

Please note that without understanding what issues have you been facing and the purpose of the Squid-Cache instance(s?) there is no way to even guess what might fit your needs.

Eliezer 

----
http://ngtech.co.il/lmgtfy/
Linux System Administrator
Mobile: +972-5-28704261
Email: eliezer at ngtech.co.il


From: squid-users <squid-users-bounces at lists.squid-cache.org> On Behalf Of Cheadle, Edward
Sent: Monday, June 4, 2018 21:06
To: squid-users at lists.squid-cache.org
Subject: [squid-users] Connection Timeouts

We had a person leave and I got selected to update and maintain our squid proxy.   We are talking to AWS and they told us that we needed to change the connection_timeout value from the default to 5 min.

We have people stress testing out installation and I was concerned that if connection timeouts are too long we may see congestion. 

Should I be worried that connection timeouts will use up file descriptors at a higher rate?

And what might be the options?

Doing and internet search I found a web page at https://www.visolve.com/squid/squid30/timeout.php and in the TIMEOUT description I read

?TIMEOUT
Timeout parameters in Squid can be based on overall connection timeouts, peer-specific timeouts, site/domain-specific timeouts, request-specific timeouts etc. Proper setting of timeout values is critical to optimal Squid performance. Relevant parameters for timeout settings are listed?

Is it possible to narrow the connection timeout to a specific site?  I looked at the website information, squid documentation and did an internet search.

I did not see anything that narrowed the timeout to a specific timeout. 

I am trying to set connection timeouts to AWS sites, but keep connection timeouts to the default, because it is working well. 

IMPORTANT NOTICE: This communication, including any attachment, contains information that may be confidential or privileged, and is intended solely for the entity or individual to whom it is addressed. If you are not the intended recipient, you should delete this message and are hereby notified that any disclosure, copying, or distribution of this message is strictly prohibited. Nothing in this email, including any attachment, is intended to be a legally binding signature.



From Edward.Cheadle at cambiahealth.com  Mon Jun  4 20:07:21 2018
From: Edward.Cheadle at cambiahealth.com (Cheadle, Edward)
Date: Mon, 4 Jun 2018 20:07:21 +0000
Subject: [squid-users] Connection Timeouts
In-Reply-To: <031401d3fc32$218ed710$64ac8530$@ngtech.co.il>
References: <973AC2B7-9270-4378-97E5-0D6AEF24624A@contoso.com>
 <031401d3fc32$218ed710$64ac8530$@ngtech.co.il>
Message-ID: <88F530BE-0AF0-419B-91E2-5112EFF4C039@regence.com>

Eliezer, you are absolutely right.  I got in a hurry and forgot the basics such as version numbers and all the other details.

The version currently on our squid server is: squid-3.5.27-1.el6.x86_64.rpm
We are running AWS Linux:  Amazon Linux AMI 2018.03.0

We are a health care company.  We are using squid proxy to control what the servers in an account can connect to on the internet.  AWS looked at an issue we had with code deploy and they said connections were timing out because the default connection timeout is 1 min, and suggested we change the timeout to 5 min.  It issue has to do with Codedeploy.  Since AWS services are on the internet, I was thinking if we could set an overall timeout, and then one for services that are known to take more time, I thought it would be a way keep the length of the timeout down for most things and free up resources for the majority of tasks.

My concern, as stated below is that connections will take a while to timeout and it will put more pressure on the number of file descriptors we use.  We ran into an issue with the number of file descriptors used, but figured it out and we are fine, but increasing the timeout to 5 min set off a warning flag in my mind, not having a lot of experience with squid. I am not even sure it is an issue, but I thought I try to make sure before we ran into production issues.

The reason for including the link, is that it was the first one I found and in the description they mentioned the ability to set timeouts on a site/domain-specific basis, but in the info that followed and in subsequent searches, I did not see how it was done, so the failure to find information on the subject led me to join the list.  
 
In looking at the docs, there are a  number of other timeouts, so I obviously have some homework to do.

Thanks for the quick response.



?On 6/4/18, 12:31 PM, "Eliezer Croitoru" <eliezer at ngtech.co.il> wrote:

    Hey Edward,
    
    First congrats!.
    I hope we can help you to figure out the relevant details.
    
    I am not sure why you have spoken to AWS teams about Squid-Cache, may I ask what OS are you using in AWS?
    Also what version of Squid are you using?
    The timeout settings are "critical" indeed but depends on what you are using and doing with Squid-Cache.
    Despite to the fact that https://na01.safelinks.protection.outlook.com/?url=https%3A%2F%2Fwww.visolve.com%2Fsquid%2Fsquid30%2Ftimeout&data=02%7C01%7CEdward.Cheadle%40cambiahealth.com%7C8be888b30a484f0d8b4f08d5ca49570f%7Ce964274919d44f7fb4df802b2b75a809%7C0%7C0%7C636637338708424102&sdata=SpOxewYBxY1Y7qeK7fk5cEF0pWN2l%2B4UOM6IclHVrbw%3D&reserved=0 Is in a way still a lead it's not "up-to-date"
    
    Please note that without understanding what issues have you been facing and the purpose of the Squid-Cache instance(s?) there is no way to even guess what might fit your needs.
    
    Eliezer
    
    ----
    https://na01.safelinks.protection.outlook.com/?url=http%3A%2F%2Fngtech.co.il%2Flmgtfy%2F&data=02%7C01%7CEdward.Cheadle%40cambiahealth.com%7C8be888b30a484f0d8b4f08d5ca49570f%7Ce964274919d44f7fb4df802b2b75a809%7C0%7C0%7C636637338708424102&sdata=Mpu0Ottn255qQxnsXGT%2F%2ByR432Yz9%2FckeKTuVpZ6aUM%3D&reserved=0
    Linux System Administrator
    Mobile: +972-5-28704261
    Email: eliezer at ngtech.co.il
    
    
    From: squid-users <squid-users-bounces at lists.squid-cache.org> On Behalf Of Cheadle, Edward
    Sent: Monday, June 4, 2018 21:06
    To: squid-users at lists.squid-cache.org
    Subject: [squid-users] Connection Timeouts
    
    We had a person leave and I got selected to update and maintain our squid proxy.   We are talking to AWS and they told us that we needed to change the connection_timeout value from the default to 5 min.
    
    We have people stress testing out installation and I was concerned that if connection timeouts are too long we may see congestion.
    
    Should I be worried that connection timeouts will use up file descriptors at a higher rate?
    
    And what might be the options?
    
    Doing and internet search I found a web page at https://na01.safelinks.protection.outlook.com/?url=https%3A%2F%2Fwww.visolve.com%2Fsquid%2Fsquid30%2Ftimeout.php&data=02%7C01%7CEdward.Cheadle%40cambiahealth.com%7C8be888b30a484f0d8b4f08d5ca49570f%7Ce964274919d44f7fb4df802b2b75a809%7C0%7C0%7C636637338708424102&sdata=FSq%2FnnFycwsbQaw8xRMzHkBWFY4Iw5F8KeJtdd1hRyc%3D&reserved=0 and in the TIMEOUT description I read
    
    ?TIMEOUT
    Timeout parameters in Squid can be based on overall connection timeouts, peer-specific timeouts, site/domain-specific timeouts, request-specific timeouts etc. Proper setting of timeout values is critical to optimal Squid performance. Relevant parameters for timeout settings are listed?
    
    Is it possible to narrow the connection timeout to a specific site?  I looked at the website information, squid documentation and did an internet search.
    
    I did not see anything that narrowed the timeout to a specific timeout.
    
    I am trying to set connection timeouts to AWS sites, but keep connection timeouts to the default, because it is working well.
    
    IMPORTANT NOTICE: This communication, including any attachment, contains information that may be confidential or privileged, and is intended solely for the entity or individual to whom it is addressed. If you are not the intended recipient, you should delete this message and are hereby notified that any disclosure, copying, or distribution of this message is strictly prohibited. Nothing in this email, including any attachment, is intended to be a legally binding signature.
    
    Ensure a sustainable future - only print when necessary.
    


From tiraen at gmail.com  Mon Jun  4 23:34:10 2018
From: tiraen at gmail.com (Tiraen)
Date: Tue, 5 Jun 2018 02:34:10 +0300
Subject: [squid-users] Question about traffic calculate
Message-ID: <CANhj9oyRn5GvSDjUa405Tz90X8kZY4mEoWPv341M1eUFbr2WfQ@mail.gmail.com>

Good day. I apologize in advance if this has already been discussed, if so
- just give a link to the discussion

The proxy server has an interface for viewing current active sessions

http://{}:{}/squid-internal-mgr/active_requests

or

cache_object://%s/active_requests

There there is some set of parameters which allow to get the data on traffic

If the connection to the proxy goes directly and by http we see like this:

*Connection: 0x8050e0518*
*        FD 29, read 4247, wrote 13479*
*        FD desc: Reading next request*
*        in: buf 0x8045a6fe0, used 0, free 39*
*        remote: ????:50340*
*        local: ????:8080*
*        nrequests: 1*
*uri ???:443*
*logType TCP_TUNNEL*
*out.offset 0, out.size 13440*
*req_sz 235*
*entry 0x0/N/A*
*start 1527608373.902584 (73.252258 seconds ago)*
*username -*
*delay_pool 0*


We have both traffic stat

*out.offset 0, out.size 13440*
*req_sz 235*

But if there is a frontend in front of the SQUID (nghttpx for example and
https)

we have this

*Connection: 0x7f66a317ecf8*
*    FD 222, read 9192, wrote 526*
*    FD desc: Reading next request*
*    in: buf 0x7f66a294fb90, used 0, free 39*
*    remote: 127.0.0.1:2314 <http://127.0.0.1:2314>*
*    local: 127.0.0.1:8081 <http://127.0.0.1:8081>*
*    nrequests: 2*
*uri nererut.com:443 <http://nererut.com:443>*
*logType TAG_NONE*
*out.offset 0, out.size 0*
*req_sz 334*
*entry (nil)/N/A*
*start 1527526715.189831 (81017.831772 seconds ago)*
*username 8355fcec-94fd-496c-94d1-a195a5ca7148*

*delay_pool 0*
without out traffic

*out.offset 0, out.size 0*
*req_sz 334*

I certainly did not test why it happens - due to https or proxy, but is it
possible to clarify this case?

Thank you in advance for your help


-- 
With best regards,

Vyacheslav Yakushev,

Unix system administrator

https://t.me/kelewind
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20180605/e9fbb0c6/attachment.htm>

From eliezer at ngtech.co.il  Tue Jun  5 01:39:45 2018
From: eliezer at ngtech.co.il (Eliezer Croitoru)
Date: Tue, 5 Jun 2018 04:39:45 +0300
Subject: [squid-users] Connection Timeouts
In-Reply-To: <88F530BE-0AF0-419B-91E2-5112EFF4C039@regence.com>
References: <973AC2B7-9270-4378-97E5-0D6AEF24624A@contoso.com>
 <031401d3fc32$218ed710$64ac8530$@ngtech.co.il>
 <88F530BE-0AF0-419B-91E2-5112EFF4C039@regence.com>
Message-ID: <03d801d3fc6e$155bdba0$401392e0$@ngtech.co.il>

So just to make sure I understand.
Is squid acting only as an ACL proxy server from inside AWS internal network toward the outside world?
Increasing the timeout to 5 minutes will maybe increase the usage of FD but if this squid has only one worker(basic simple setup with ACL's)
then you can change a timeout and increase the FD the system can handle...
A single working instance of Squid can handle up to a certain amount of traffic and if the instance has let say 2 GB you can safely upper the limit to 64k FD.
My Atom based PC here can handle 64k FD just fine while the actual hardware technically limits it to something like 32k.
On my Xeon based Server I am building and packaging squid with 16k basic limit and it works for most of the business setups out there(not including ISP's).

If all these servers that are using the Squid service are on the same network segment then it would be very weird to change any timeout.
If these servers are not on the same network segment what you need is to turn on keep alive probe let say to 15 seconds per probe.
It will "increase" from n packets to n+(4*connection minutes duration) but as long it is a single worker basic proxy it's nothing.
Try to look at the cache manager interface output for the "info" page and see what is the average connections per second on the Squid service.
(let me know if you need help to get the info cache manager page)
With these numbers you would be able to understand what might causing service disruption.

Eliezer

* by any chance AWS Linux AMD 2018.03.0 has systemd in it or I am imagining that it still uses sysVinit?

----
Eliezer Croitoru
Linux System Administrator
Mobile: +972-5-28704261
Email: eliezer at ngtech.co.il



-----Original Message-----
From: Cheadle, Edward <Edward.Cheadle at cambiahealth.com> 
Sent: Monday, June 4, 2018 23:07
To: Eliezer Croitoru <eliezer at ngtech.co.il>
Cc: squid-users at lists.squid-cache.org
Subject: Re: [squid-users] Connection Timeouts

Eliezer, you are absolutely right.  I got in a hurry and forgot the basics such as version numbers and all the other details.

The version currently on our squid server is: squid-3.5.27-1.el6.x86_64.rpm
We are running AWS Linux:  Amazon Linux AMI 2018.03.0

We are a health care company.  We are using squid proxy to control what the servers in an account can connect to on the internet.  AWS looked at an issue we had with code deploy and they said connections were timing out because the default connection timeout is 1 min, and suggested we change the timeout to 5 min.  It issue has to do with Codedeploy.  Since AWS services are on the internet, I was thinking if we could set an overall timeout, and then one for services that are known to take more time, I thought it would be a way keep the length of the timeout down for most things and free up resources for the majority of tasks.

My concern, as stated below is that connections will take a while to timeout and it will put more pressure on the number of file descriptors we use.  We ran into an issue with the number of file descriptors used, but figured it out and we are fine, but increasing the timeout to 5 min set off a warning flag in my mind, not having a lot of experience with squid. I am not even sure it is an issue, but I thought I try to make sure before we ran into production issues.

The reason for including the link, is that it was the first one I found and in the description they mentioned the ability to set timeouts on a site/domain-specific basis, but in the info that followed and in subsequent searches, I did not see how it was done, so the failure to find information on the subject led me to join the list.  
 
In looking at the docs, there are a  number of other timeouts, so I obviously have some homework to do.

Thanks for the quick response.



?On 6/4/18, 12:31 PM, "Eliezer Croitoru" <eliezer at ngtech.co.il> wrote:

    Hey Edward,
    
    First congrats!.
    I hope we can help you to figure out the relevant details.
    
    I am not sure why you have spoken to AWS teams about Squid-Cache, may I ask what OS are you using in AWS?
    Also what version of Squid are you using?
    The timeout settings are "critical" indeed but depends on what you are using and doing with Squid-Cache.
    Despite to the fact that https://na01.safelinks.protection.outlook.com/?url=https%3A%2F%2Fwww.visolve.com%2Fsquid%2Fsquid30%2Ftimeout&data=02%7C01%7CEdward.Cheadle%40cambiahealth.com%7C8be888b30a484f0d8b4f08d5ca49570f%7Ce964274919d44f7fb4df802b2b75a809%7C0%7C0%7C636637338708424102&sdata=SpOxewYBxY1Y7qeK7fk5cEF0pWN2l%2B4UOM6IclHVrbw%3D&reserved=0 Is in a way still a lead it's not "up-to-date"
    
    Please note that without understanding what issues have you been facing and the purpose of the Squid-Cache instance(s?) there is no way to even guess what might fit your needs.
    
    Eliezer
    
    ----
    https://na01.safelinks.protection.outlook.com/?url=http%3A%2F%2Fngtech.co.il%2Flmgtfy%2F&data=02%7C01%7CEdward.Cheadle%40cambiahealth.com%7C8be888b30a484f0d8b4f08d5ca49570f%7Ce964274919d44f7fb4df802b2b75a809%7C0%7C0%7C636637338708424102&sdata=Mpu0Ottn255qQxnsXGT%2F%2ByR432Yz9%2FckeKTuVpZ6aUM%3D&reserved=0
    Linux System Administrator
    Mobile: +972-5-28704261
    Email: eliezer at ngtech.co.il
    
    
    From: squid-users <squid-users-bounces at lists.squid-cache.org> On Behalf Of Cheadle, Edward
    Sent: Monday, June 4, 2018 21:06
    To: squid-users at lists.squid-cache.org
    Subject: [squid-users] Connection Timeouts
    
    We had a person leave and I got selected to update and maintain our squid proxy.   We are talking to AWS and they told us that we needed to change the connection_timeout value from the default to 5 min.
    
    We have people stress testing out installation and I was concerned that if connection timeouts are too long we may see congestion.
    
    Should I be worried that connection timeouts will use up file descriptors at a higher rate?
    
    And what might be the options?
    
    Doing and internet search I found a web page at https://na01.safelinks.protection.outlook.com/?url=https%3A%2F%2Fwww.visolve.com%2Fsquid%2Fsquid30%2Ftimeout.php&data=02%7C01%7CEdward.Cheadle%40cambiahealth.com%7C8be888b30a484f0d8b4f08d5ca49570f%7Ce964274919d44f7fb4df802b2b75a809%7C0%7C0%7C636637338708424102&sdata=FSq%2FnnFycwsbQaw8xRMzHkBWFY4Iw5F8KeJtdd1hRyc%3D&reserved=0 and in the TIMEOUT description I read
    
    ?TIMEOUT
    Timeout parameters in Squid can be based on overall connection timeouts, peer-specific timeouts, site/domain-specific timeouts, request-specific timeouts etc. Proper setting of timeout values is critical to optimal Squid performance. Relevant parameters for timeout settings are listed?
    
    Is it possible to narrow the connection timeout to a specific site?  I looked at the website information, squid documentation and did an internet search.
    
    I did not see anything that narrowed the timeout to a specific timeout.
    
    I am trying to set connection timeouts to AWS sites, but keep connection timeouts to the default, because it is working well.
    
    IMPORTANT NOTICE: This communication, including any attachment, contains information that may be confidential or privileged, and is intended solely for the entity or individual to whom it is addressed. If you are not the intended recipient, you should delete this message and are hereby notified that any disclosure, copying, or distribution of this message is strictly prohibited. Nothing in this email, including any attachment, is intended to be a legally binding signature.
    
    Ensure a sustainable future - only print when necessary.
    




From squid3 at treenet.co.nz  Tue Jun  5 03:25:30 2018
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Tue, 5 Jun 2018 15:25:30 +1200
Subject: [squid-users] Question about traffic calculate
In-Reply-To: <CANhj9oyRn5GvSDjUa405Tz90X8kZY4mEoWPv341M1eUFbr2WfQ@mail.gmail.com>
References: <CANhj9oyRn5GvSDjUa405Tz90X8kZY4mEoWPv341M1eUFbr2WfQ@mail.gmail.com>
Message-ID: <4dd5dbf7-661a-3378-33be-a7a9da2912cf@treenet.co.nz>

On 05/06/18 11:34, Tiraen wrote:
> Good day.?I apologize in advance if this has already been discussed, if
> so - just give a link to the discussion
> 
> The proxy server has an interface for viewing current active sessions
> 
> http://{}:{}/squid-internal-mgr/active_requests?
> 

Please be aware these are *not* "sessions". These are transactions,
which  have one request, one response, and maybe some informational
messages.

A "session" as far as it relates to HTTP is a application level thing
which includes _multiple_ transactions, and possibly even multiple TCP
connections at the client end.


> or
> 
> cache_object://%s/active_requests
> 
> There there is some set of parameters which allow to get the data on traffic
> 
> If the connection to the proxy goes directly and by http we see like this:
> 
> /Connection: 0x8050e0518/
> /? ? ? ? FD 29, read 4247, wrote 13479/
> /? ? ? ? FD desc: Reading next request/
> /? ? ? ? in: buf 0x8045a6fe0, used 0, free 39/
> /? ? ? ? remote: ????:50340/
> /? ? ? ? local: ????:8080/
> /? ? ? ? nrequests: 1/
> /uri ???:443/
> /logType TCP_TUNNEL/
> /out.offset 0, out.size 13440/
> /req_sz 235/
> /entry 0x0/N/A/
> /start 1527608373.902584 (73.252258 seconds ago)/
> /username -/
> /delay_pool 0/
> 
> 
> We have both traffic stat
> 
> /out.offset 0, out.size 13440/
> /req_sz 235/
> 

The latest transactions request was 235 bytes, its reply was 13440 bytes
(so far).


> But if there is a frontend in front of the SQUID (nghttpx for example
> and https)?
> 
> we have this
> 
> /Connection: 0x7f66a317ecf8/
> /? ? FD 222, read 9192, wrote 526/
> /? ? FD desc: Reading next request/
> /? ? in: buf 0x7f66a294fb90, used 0, free 39/
> /? ? remote: 127.0.0.1:2314 <http://127.0.0.1:2314>/
> /? ? local: 127.0.0.1:8081 <http://127.0.0.1:8081>/
> /? ? nrequests: 2/
> /uri nererut.com:443 <http://nererut.com:443>/
> /logType TAG_NONE/
> /out.offset 0, out.size 0/
> /req_sz 334/
> /entry (nil)/N/A/
> /start 1527526715.189831 (81017.831772 seconds ago)/
> /username 8355fcec-94fd-496c-94d1-a195a5ca7148/
> /delay_pool 0
> /
> without out traffic
> 
> /out.offset 0, out.size 0/
> /req_sz 334/

This transaction request was 334 bytes, its reply was 0 bytes (so far).


> 
> I certainly did not test why it happens - due to https or proxy, but is
> it possible to clarify this case?

The first transaction has reached the stage where a reply has started.

The second transaction has not yet reached that state despite 81017sec
having past.

That is all we can say without more information about things like for
example, which Squid version you are using, whether you are SSL-Bumping
the HTTPS traffic for either of those transactions, network topology on
the outgoing side of Squid, etc.


Amos


From squid3 at treenet.co.nz  Tue Jun  5 04:12:47 2018
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Tue, 5 Jun 2018 16:12:47 +1200
Subject: [squid-users] quiet week
In-Reply-To: <CAPa6PsH1zhzfynjXoG5FaVOfGz3GJxjyMoG=q5fQFNhULsMQfQ@mail.gmail.com>
References: <4578918d-3953-74ca-b65f-e7a148abbe4e@treenet.co.nz>
 <00b301d3fbd8$e83cc200$b8b64600$@ngtech.co.il>
 <9174de24-068b-ac75-4fed-fc0b84bc7d90@treenet.co.nz>
 <CAPa6PsH1zhzfynjXoG5FaVOfGz3GJxjyMoG=q5fQFNhULsMQfQ@mail.gmail.com>
Message-ID: <d8d1f07d-d4b0-f78a-1834-d38671213f65@treenet.co.nz>

On 05/06/18 06:05, Cheadle, Edward wrote:
> There are significant differences between the Fedora->RHEL and
> Debian->Ubuntu relations.
> Fedora is development oriented with short lifecycle and without support
> of previous versions. RHEL is built on Fedora with stability and
> enterprise needs in mind (they always ).

Most of which is their business, not ours.

> 
> Debian provides more release stages
> ?- stable -> Ubuntu TLS releases
> ?- testing -> standard Ubuntu releases
> ?- unstable
> 
> Ubuntu LTS releases are based on current Debian stable release.

That is incorrect. Ubuntu is based on Debian "Testing" packages. It just
happens that every two years Debian freeze their Testing repository for
final QA which converts it to "Stable".

The Ubuntu release which is based on that particular content from Debian
Testing turns into the Ubuntu LTS. So it *looks* outwardly like Ubuntu
LTS is based on Debian Stable, but that is just a side effect of how the
two interact during the Debian stabilization freeze process.

It was probably planned intentionally that way, but packages are not
pulled from Debian Stable as a especially for Ubuntu LTS.

> 
> This is major difference.
> 
> The RHEL->CentOS relation is of different type. CentOS is RHEL without
> copyrighted materials. Updates are usually released with delays. There
> are some other repositories which extend/modify CentOS core
> https://en.wikipedia.org/wiki/CentOS#REPOS

AKA, CentOS is derived from RHEL with some differences in focus.

RHEL is derived from Fedora with some difference in focus.

So those "few days" you mention have already had several years of Fedora
delay happen before they even begin.  This is why Eliezers repository
gets traffic. If CentOS were really having only a few days _total_ (like
Fedora) nobody would need to download unofficial packages to stay up to
date.


In the overview what each OS focuses on providing to their respective
communities is not related to our release roadmap - all distros are
different. Their derivation relationships are the origin of package
version differences and does impact on how our release timing needs to
be planned out to minimize obsolete versions having ~20 years of support
cycles.

Amos


From christof.gerber1 at gmail.com  Tue Jun  5 09:27:20 2018
From: christof.gerber1 at gmail.com (chgerber)
Date: Tue, 5 Jun 2018 02:27:20 -0700 (MST)
Subject: [squid-users] Display eCAP meta-information on Squid error-page
In-Reply-To: <7a5581a4-a5bc-aa28-b026-eb99896622bd@measurement-factory.com>
References: <CAFyThp+u6Dfvs4mR55+rgWy8PqiXV7dzzMQ9-jF7kqx0HwYQBg@mail.gmail.com>
 <7a5581a4-a5bc-aa28-b026-eb99896622bd@measurement-factory.com>
Message-ID: <1528190840028-0.post@n4.nabble.com>

"It is best to use %note logformat %code for logging annotations. 
The %adapt::<last_h code is meant for adaptation services debugging (and 
to work around the current ICAP code lack of support for annotations)." 

How exactly can I use %note to log the same information to access.log? For
example assume I use "%{my-ecap-header}adapt::<last_h" how can I log the
same using %note as you suggested? 

Related question: 

Can I apply ACL's to annotations served by eCAP adapters. Say when
%{my-ecap-header}adapt::<last_h or the same solution with %note respectively
(see first part of post) returns "bad" I want squid to deny the access and
grant access when it returns "good"? I know about the eCAP specific
virginBlock() function.



--
Sent from: http://squid-web-proxy-cache.1019090.n4.nabble.com/Squid-Users-f1019091.html


From eliezer at ngtech.co.il  Tue Jun  5 09:41:37 2018
From: eliezer at ngtech.co.il (Eliezer Croitoru)
Date: Tue, 5 Jun 2018 12:41:37 +0300
Subject: [squid-users] Connection Timeouts
In-Reply-To: <88F530BE-0AF0-419B-91E2-5112EFF4C039@regence.com>
References: <973AC2B7-9270-4378-97E5-0D6AEF24624A@contoso.com>
 <031401d3fc32$218ed710$64ac8530$@ngtech.co.il>
 <88F530BE-0AF0-419B-91E2-5112EFF4C039@regence.com>
Message-ID: <049401d3fcb1$66425330$32c6f990$@ngtech.co.il>

Hey Edward,

I have just seen the AWS Linux container and it seems that they do not use system but they do have updates.
I do not know where did you downloaded the el6 3.5.27 package but their official current release is:
3.5.20-10.34.amzn1

Their squid -v output:
bash-4.2# squid -v
Squid Cache: Version 3.5.20
Service Name: squid
configure options:  '--build=x86_64-redhat-linux-gnu' '--host=x86_64-redhat-linux-gnu' '--target=x86_64-amazon-linux-gnu' '--program-prefix=' '--prefix=/usr' '--exec-prefix=/usr' '--bindir=/usr/bin' '--sbindir=/usr/sbin' '--sysconfdir=/etc' '--datadir=/usr/share' '--includedir=/usr/include' '--libdir=/usr/lib64' '--libexecdir=/usr/libexec' '--sharedstatedir=/var/lib' '--mandir=/usr/share/man' '--infodir=/usr/share/info' '--disable-strict-error-checking' '--exec_prefix=/usr' '--libexecdir=/usr/lib64/squid' '--localstatedir=/var' '--datadir=/usr/share/squid' '--sysconfdir=/etc/squid' '--with-logdir=$(localstatedir)/log/squid' '--with-pidfile=$(localstatedir)/run/squid.pid' '--disable-dependency-tracking' '--enable-eui' '--enable-follow-x-forwarded-for' '--enable-auth' '--enable-auth-basic=DB,LDAP,MSNT-multi-domain,NCSA,NIS,PAM,POP3,RADIUS,SASL,SMB,SMB_LM,getpwnam' '--enable-auth-ntlm=smb_lm,fake' '--enable-auth-digest=file,LDAP,eDirectory' '--enable-auth-negotiate=kerberos' '--enable-external-acl-helpers=file_userip,LDAP_group,unix_group,time_quota,session,wbinfo_group' '--enable-cache-digests' '--enable-cachemgr-hostname=localhost' '--enable-delay-pools' '--enable-epoll' '--enable-ident-lookups' '--enable-linux-netfilter' '--enable-removal-policies=heap,lru' '--enable-snmp' '--enable-ssl-crtd' '--enable-storeio=aufs,diskd,rock,ufs' '--enable-wccpv2' '--enable-esi' '--enable-ecap' '--with-aio' '--with-default-user=squid' '--with-dl' '--with-openssl' '--with-pthreads' '--disable-arch-native' 'build_alias=x86_64-redhat-linux-gnu' 'host_alias=x86_64-redhat-linux-gnu' 'target_alias=x86_64-amazon-linux-gnu' 'CFLAGS=-O2 -g -pipe -Wall -Wp,-D_FORTIFY_SOURCE=2 -fexceptions -fstack-protector --param=ssp-buffer-size=4 -m64 -mtune=generic -fpie' 'CXXFLAGS=-O2 -g -pipe -Wall -Wp,-D_FORTIFY_SOURCE=2 -fexceptions -fstack-protector --param=ssp-buffer-size=4 -m64 -mtune=generic -fpie' 'PKG_CONFIG_PATH=:/usr/lib64/pkgconfig:/usr/share/pkgconfig'

so it's basically a RHEL el6 *based* OS which also have support for ssl-bump and is actually el7 without all the system benefits....

I can try to port their current SRPM 3.5.20 to my version and since they do have 4.14 kernel I do believe it's worth the effort.
I have added it to my list of tasks...

And related to timeouts:
http://www.squid-cache.org/Versions/v3/3.5/cfgman/

TIMEOUTS
 -----------------------------------------------------------------------------

    forward_timeout
    connect_timeout
    peer_connect_timeout
    read_timeout
    write_timeout
    request_timeout
    client_idle_pconn_timeout
    ftp_client_idle_timeout
    client_lifetime
    half_closed_clients
    server_idle_pconn_timeout
    ident_timeout
    shutdown_lifetime


is probably the section their support wanted you to see.

But I really do not see if there is any need for such a change.

Also I do not know what AWS FW\NAT connection limits are so there should be taken into account when calculating what might be causing any issues.

Eliezer

----
Eliezer Croitoru
Linux System Administrator
Mobile: +972-5-28704261
Email: eliezer at ngtech.co.il



-----Original Message-----
From: Cheadle, Edward <Edward.Cheadle at cambiahealth.com> 
Sent: Monday, June 4, 2018 23:07
To: Eliezer Croitoru <eliezer at ngtech.co.il>
Cc: squid-users at lists.squid-cache.org
Subject: Re: [squid-users] Connection Timeouts

Eliezer, you are absolutely right.  I got in a hurry and forgot the basics such as version numbers and all the other details.

The version currently on our squid server is: squid-3.5.27-1.el6.x86_64.rpm
We are running AWS Linux:  Amazon Linux AMI 2018.03.0

We are a health care company.  We are using squid proxy to control what the servers in an account can connect to on the internet.  AWS looked at an issue we had with code deploy and they said connections were timing out because the default connection timeout is 1 min, and suggested we change the timeout to 5 min.  It issue has to do with Codedeploy.  Since AWS services are on the internet, I was thinking if we could set an overall timeout, and then one for services that are known to take more time, I thought it would be a way keep the length of the timeout down for most things and free up resources for the majority of tasks.

My concern, as stated below is that connections will take a while to timeout and it will put more pressure on the number of file descriptors we use.  We ran into an issue with the number of file descriptors used, but figured it out and we are fine, but increasing the timeout to 5 min set off a warning flag in my mind, not having a lot of experience with squid. I am not even sure it is an issue, but I thought I try to make sure before we ran into production issues.

The reason for including the link, is that it was the first one I found and in the description they mentioned the ability to set timeouts on a site/domain-specific basis, but in the info that followed and in subsequent searches, I did not see how it was done, so the failure to find information on the subject led me to join the list.  
 
In looking at the docs, there are a  number of other timeouts, so I obviously have some homework to do.

Thanks for the quick response.



?On 6/4/18, 12:31 PM, "Eliezer Croitoru" <eliezer at ngtech.co.il> wrote:

    Hey Edward,
    
    First congrats!.
    I hope we can help you to figure out the relevant details.
    
    I am not sure why you have spoken to AWS teams about Squid-Cache, may I ask what OS are you using in AWS?
    Also what version of Squid are you using?
    The timeout settings are "critical" indeed but depends on what you are using and doing with Squid-Cache.
    Despite to the fact that https://na01.safelinks.protection.outlook.com/?url=https%3A%2F%2Fwww.visolve.com%2Fsquid%2Fsquid30%2Ftimeout&data=02%7C01%7CEdward.Cheadle%40cambiahealth.com%7C8be888b30a484f0d8b4f08d5ca49570f%7Ce964274919d44f7fb4df802b2b75a809%7C0%7C0%7C636637338708424102&sdata=SpOxewYBxY1Y7qeK7fk5cEF0pWN2l%2B4UOM6IclHVrbw%3D&reserved=0 Is in a way still a lead it's not "up-to-date"
    
    Please note that without understanding what issues have you been facing and the purpose of the Squid-Cache instance(s?) there is no way to even guess what might fit your needs.
    
    Eliezer
    
    ----
    https://na01.safelinks.protection.outlook.com/?url=http%3A%2F%2Fngtech.co.il%2Flmgtfy%2F&data=02%7C01%7CEdward.Cheadle%40cambiahealth.com%7C8be888b30a484f0d8b4f08d5ca49570f%7Ce964274919d44f7fb4df802b2b75a809%7C0%7C0%7C636637338708424102&sdata=Mpu0Ottn255qQxnsXGT%2F%2ByR432Yz9%2FckeKTuVpZ6aUM%3D&reserved=0
    Linux System Administrator
    Mobile: +972-5-28704261
    Email: eliezer at ngtech.co.il
    
    
    From: squid-users <squid-users-bounces at lists.squid-cache.org> On Behalf Of Cheadle, Edward
    Sent: Monday, June 4, 2018 21:06
    To: squid-users at lists.squid-cache.org
    Subject: [squid-users] Connection Timeouts
    
    We had a person leave and I got selected to update and maintain our squid proxy.   We are talking to AWS and they told us that we needed to change the connection_timeout value from the default to 5 min.
    
    We have people stress testing out installation and I was concerned that if connection timeouts are too long we may see congestion.
    
    Should I be worried that connection timeouts will use up file descriptors at a higher rate?
    
    And what might be the options?
    
    Doing and internet search I found a web page at https://na01.safelinks.protection.outlook.com/?url=https%3A%2F%2Fwww.visolve.com%2Fsquid%2Fsquid30%2Ftimeout.php&data=02%7C01%7CEdward.Cheadle%40cambiahealth.com%7C8be888b30a484f0d8b4f08d5ca49570f%7Ce964274919d44f7fb4df802b2b75a809%7C0%7C0%7C636637338708424102&sdata=FSq%2FnnFycwsbQaw8xRMzHkBWFY4Iw5F8KeJtdd1hRyc%3D&reserved=0 and in the TIMEOUT description I read
    
    ?TIMEOUT
    Timeout parameters in Squid can be based on overall connection timeouts, peer-specific timeouts, site/domain-specific timeouts, request-specific timeouts etc. Proper setting of timeout values is critical to optimal Squid performance. Relevant parameters for timeout settings are listed?
    
    Is it possible to narrow the connection timeout to a specific site?  I looked at the website information, squid documentation and did an internet search.
    
    I did not see anything that narrowed the timeout to a specific timeout.
    
    I am trying to set connection timeouts to AWS sites, but keep connection timeouts to the default, because it is working well.
    
    IMPORTANT NOTICE: This communication, including any attachment, contains information that may be confidential or privileged, and is intended solely for the entity or individual to whom it is addressed. If you are not the intended recipient, you should delete this message and are hereby notified that any disclosure, copying, or distribution of this message is strictly prohibited. Nothing in this email, including any attachment, is intended to be a legally binding signature.
    
    Ensure a sustainable future - only print when necessary.
    




From eliezer at ngtech.co.il  Tue Jun  5 09:53:51 2018
From: eliezer at ngtech.co.il (Eliezer Croitoru)
Date: Tue, 5 Jun 2018 12:53:51 +0300
Subject: [squid-users] Connection Timeouts
In-Reply-To: <049401d3fcb1$66425330$32c6f990$@ngtech.co.il>
References: <973AC2B7-9270-4378-97E5-0D6AEF24624A@contoso.com>
 <031401d3fc32$218ed710$64ac8530$@ngtech.co.il>
 <88F530BE-0AF0-419B-91E2-5112EFF4C039@regence.com>
 <049401d3fcb1$66425330$32c6f990$@ngtech.co.il>
Message-ID: <049601d3fcb3$1baffbe0$530ff3a0$@ngtech.co.il>

Sorry the auto words correction changed every single "systemd" to "system" in the body of my email.
I like auto correction but: really???
Ho, I get it.. it's a manual system so I need to add Systemd to the dictionary.

I hope this makes more sense to the body of the email.

Eliezer

----
Eliezer Croitoru
Linux System Administrator
Mobile: +972-5-28704261
Email: eliezer at ngtech.co.il



-----Original Message-----
From: squid-users <squid-users-bounces at lists.squid-cache.org> On Behalf Of Eliezer Croitoru
Sent: Tuesday, June 5, 2018 12:42
To: 'Cheadle, Edward' <Edward.Cheadle at cambiahealth.com>
Cc: squid-users at lists.squid-cache.org
Subject: Re: [squid-users] Connection Timeouts

Hey Edward,

I have just seen the AWS Linux container and it seems that they do not use system but they do have updates.
I do not know where did you downloaded the el6 3.5.27 package but their official current release is:
3.5.20-10.34.amzn1

Their squid -v output:
bash-4.2# squid -v
Squid Cache: Version 3.5.20
Service Name: squid
configure options:  '--build=x86_64-redhat-linux-gnu' '--host=x86_64-redhat-linux-gnu' '--target=x86_64-amazon-linux-gnu' '--program-prefix=' '--prefix=/usr' '--exec-prefix=/usr' '--bindir=/usr/bin' '--sbindir=/usr/sbin' '--sysconfdir=/etc' '--datadir=/usr/share' '--includedir=/usr/include' '--libdir=/usr/lib64' '--libexecdir=/usr/libexec' '--sharedstatedir=/var/lib' '--mandir=/usr/share/man' '--infodir=/usr/share/info' '--disable-strict-error-checking' '--exec_prefix=/usr' '--libexecdir=/usr/lib64/squid' '--localstatedir=/var' '--datadir=/usr/share/squid' '--sysconfdir=/etc/squid' '--with-logdir=$(localstatedir)/log/squid' '--with-pidfile=$(localstatedir)/run/squid.pid' '--disable-dependency-tracking' '--enable-eui' '--enable-follow-x-forwarded-for' '--enable-auth' '--enable-auth-basic=DB,LDAP,MSNT-multi-domain,NCSA,NIS,PAM,POP3,RADIUS,SASL,SMB,SMB_LM,getpwnam' '--enable-auth-ntlm=smb_lm,fake' '--enable-auth-digest=file,LDAP,eDirectory' '--enable-auth-negotiate=kerberos' '--enable-external-acl-helpers=file_userip,LDAP_group,unix_group,time_quota,session,wbinfo_group' '--enable-cache-digests' '--enable-cachemgr-hostname=localhost' '--enable-delay-pools' '--enable-epoll' '--enable-ident-lookups' '--enable-linux-netfilter' '--enable-removal-policies=heap,lru' '--enable-snmp' '--enable-ssl-crtd' '--enable-storeio=aufs,diskd,rock,ufs' '--enable-wccpv2' '--enable-esi' '--enable-ecap' '--with-aio' '--with-default-user=squid' '--with-dl' '--with-openssl' '--with-pthreads' '--disable-arch-native' 'build_alias=x86_64-redhat-linux-gnu' 'host_alias=x86_64-redhat-linux-gnu' 'target_alias=x86_64-amazon-linux-gnu' 'CFLAGS=-O2 -g -pipe -Wall -Wp,-D_FORTIFY_SOURCE=2 -fexceptions -fstack-protector --param=ssp-buffer-size=4 -m64 -mtune=generic -fpie' 'CXXFLAGS=-O2 -g -pipe -Wall -Wp,-D_FORTIFY_SOURCE=2 -fexceptions -fstack-protector --param=ssp-buffer-size=4 -m64 -mtune=generic -fpie' 'PKG_CONFIG_PATH=:/usr/lib64/pkgconfig:/usr/share/pkgconfig'

so it's basically a RHEL el6 *based* OS which also have support for ssl-bump and is actually el7 without all the system benefits....

I can try to port their current SRPM 3.5.20 to my version and since they do have 4.14 kernel I do believe it's worth the effort.
I have added it to my list of tasks...

And related to timeouts:
http://www.squid-cache.org/Versions/v3/3.5/cfgman/

TIMEOUTS
 -----------------------------------------------------------------------------

    forward_timeout
    connect_timeout
    peer_connect_timeout
    read_timeout
    write_timeout
    request_timeout
    client_idle_pconn_timeout
    ftp_client_idle_timeout
    client_lifetime
    half_closed_clients
    server_idle_pconn_timeout
    ident_timeout
    shutdown_lifetime


is probably the section their support wanted you to see.

But I really do not see if there is any need for such a change.

Also I do not know what AWS FW\NAT connection limits are so there should be taken into account when calculating what might be causing any issues.

Eliezer

----
Eliezer Croitoru
Linux System Administrator
Mobile: +972-5-28704261
Email: eliezer at ngtech.co.il



-----Original Message-----
From: Cheadle, Edward <Edward.Cheadle at cambiahealth.com>
Sent: Monday, June 4, 2018 23:07
To: Eliezer Croitoru <eliezer at ngtech.co.il>
Cc: squid-users at lists.squid-cache.org
Subject: Re: [squid-users] Connection Timeouts

Eliezer, you are absolutely right.  I got in a hurry and forgot the basics such as version numbers and all the other details.

The version currently on our squid server is: squid-3.5.27-1.el6.x86_64.rpm We are running AWS Linux:  Amazon Linux AMI 2018.03.0

We are a health care company.  We are using squid proxy to control what the servers in an account can connect to on the internet.  AWS looked at an issue we had with code deploy and they said connections were timing out because the default connection timeout is 1 min, and suggested we change the timeout to 5 min.  It issue has to do with Codedeploy.  Since AWS services are on the internet, I was thinking if we could set an overall timeout, and then one for services that are known to take more time, I thought it would be a way keep the length of the timeout down for most things and free up resources for the majority of tasks.

My concern, as stated below is that connections will take a while to timeout and it will put more pressure on the number of file descriptors we use.  We ran into an issue with the number of file descriptors used, but figured it out and we are fine, but increasing the timeout to 5 min set off a warning flag in my mind, not having a lot of experience with squid. I am not even sure it is an issue, but I thought I try to make sure before we ran into production issues.

The reason for including the link, is that it was the first one I found and in the description they mentioned the ability to set timeouts on a site/domain-specific basis, but in the info that followed and in subsequent searches, I did not see how it was done, so the failure to find information on the subject led me to join the list.  
 
In looking at the docs, there are a  number of other timeouts, so I obviously have some homework to do.

Thanks for the quick response.



?On 6/4/18, 12:31 PM, "Eliezer Croitoru" <eliezer at ngtech.co.il> wrote:

    Hey Edward,
    
    First congrats!.
    I hope we can help you to figure out the relevant details.
    
    I am not sure why you have spoken to AWS teams about Squid-Cache, may I ask what OS are you using in AWS?
    Also what version of Squid are you using?
    The timeout settings are "critical" indeed but depends on what you are using and doing with Squid-Cache.
    Despite to the fact that https://na01.safelinks.protection.outlook.com/?url=https%3A%2F%2Fwww.visolve.com%2Fsquid%2Fsquid30%2Ftimeout&data=02%7C01%7CEdward.Cheadle%40cambiahealth.com%7C8be888b30a484f0d8b4f08d5ca49570f%7Ce964274919d44f7fb4df802b2b75a809%7C0%7C0%7C636637338708424102&sdata=SpOxewYBxY1Y7qeK7fk5cEF0pWN2l%2B4UOM6IclHVrbw%3D&reserved=0 Is in a way still a lead it's not "up-to-date"
    
    Please note that without understanding what issues have you been facing and the purpose of the Squid-Cache instance(s?) there is no way to even guess what might fit your needs.
    
    Eliezer
    
    ----
    https://na01.safelinks.protection.outlook.com/?url=http%3A%2F%2Fngtech.co.il%2Flmgtfy%2F&data=02%7C01%7CEdward.Cheadle%40cambiahealth.com%7C8be888b30a484f0d8b4f08d5ca49570f%7Ce964274919d44f7fb4df802b2b75a809%7C0%7C0%7C636637338708424102&sdata=Mpu0Ottn255qQxnsXGT%2F%2ByR432Yz9%2FckeKTuVpZ6aUM%3D&reserved=0
    Linux System Administrator
    Mobile: +972-5-28704261
    Email: eliezer at ngtech.co.il
    
    
    From: squid-users <squid-users-bounces at lists.squid-cache.org> On Behalf Of Cheadle, Edward
    Sent: Monday, June 4, 2018 21:06
    To: squid-users at lists.squid-cache.org
    Subject: [squid-users] Connection Timeouts
    
    We had a person leave and I got selected to update and maintain our squid proxy.   We are talking to AWS and they told us that we needed to change the connection_timeout value from the default to 5 min.
    
    We have people stress testing out installation and I was concerned that if connection timeouts are too long we may see congestion.
    
    Should I be worried that connection timeouts will use up file descriptors at a higher rate?
    
    And what might be the options?
    
    Doing and internet search I found a web page at https://na01.safelinks.protection.outlook.com/?url=https%3A%2F%2Fwww.visolve.com%2Fsquid%2Fsquid30%2Ftimeout.php&data=02%7C01%7CEdward.Cheadle%40cambiahealth.com%7C8be888b30a484f0d8b4f08d5ca49570f%7Ce964274919d44f7fb4df802b2b75a809%7C0%7C0%7C636637338708424102&sdata=FSq%2FnnFycwsbQaw8xRMzHkBWFY4Iw5F8KeJtdd1hRyc%3D&reserved=0 and in the TIMEOUT description I read
    
    ?TIMEOUT
    Timeout parameters in Squid can be based on overall connection timeouts, peer-specific timeouts, site/domain-specific timeouts, request-specific timeouts etc. Proper setting of timeout values is critical to optimal Squid performance. Relevant parameters for timeout settings are listed?
    
    Is it possible to narrow the connection timeout to a specific site?  I looked at the website information, squid documentation and did an internet search.
    
    I did not see anything that narrowed the timeout to a specific timeout.
    
    I am trying to set connection timeouts to AWS sites, but keep connection timeouts to the default, because it is working well.
    
    IMPORTANT NOTICE: This communication, including any attachment, contains information that may be confidential or privileged, and is intended solely for the entity or individual to whom it is addressed. If you are not the intended recipient, you should delete this message and are hereby notified that any disclosure, copying, or distribution of this message is strictly prohibited. Nothing in this email, including any attachment, is intended to be a legally binding signature.
    
    Ensure a sustainable future - only print when necessary.
    


_______________________________________________
squid-users mailing list
squid-users at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-users



From squid3 at treenet.co.nz  Tue Jun  5 10:51:27 2018
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Tue, 5 Jun 2018 22:51:27 +1200
Subject: [squid-users] Display eCAP meta-information on Squid error-page
In-Reply-To: <1528190840028-0.post@n4.nabble.com>
References: <CAFyThp+u6Dfvs4mR55+rgWy8PqiXV7dzzMQ9-jF7kqx0HwYQBg@mail.gmail.com>
 <7a5581a4-a5bc-aa28-b026-eb99896622bd@measurement-factory.com>
 <1528190840028-0.post@n4.nabble.com>
Message-ID: <a1532a27-0ad2-11de-b402-e6b79d6191df@treenet.co.nz>

On 05/06/18 21:27, chgerber wrote:
> "It is best to use %note logformat %code for logging annotations. 
> The %adapt::<last_h code is meant for adaptation services debugging (and 
> to work around the current ICAP code lack of support for annotations)." 
> 
> How exactly can I use %note to log the same information to access.log? For
> example assume I use "%{my-ecap-header}adapt::<last_h" how can I log the
> same using %note as you suggested? 

%note{key-name}

or %<h{header-name}

or %>h{header-name}

Depending on how your adaptor produces it. As an annotation (note) or as
an HTTP message header to be delivered to the client or server.


> 
> Related question: 
> 
> Can I apply ACL's to annotations served by eCAP adapters. Say when
> %{my-ecap-header}adapt::<last_h or the same solution with %note respectively
> (see first part of post) returns "bad" I want squid to deny the access and
> grant access when it returns "good"? I know about the eCAP specific
> virginBlock() function.
> 

That sounds like a rather inefficient use of an adaptor.

The adaptor API purpose is to alter HTTP messages as they travel through
the proxy, not to be a substitute for access control logic already
available in the proxy. So what your adaptor SHOULD be doing is simply
producing the 403 Forbidden message itself.

By using a header as described you are forcing Squid to:
  receive adapted message from eCAP
  re-parse that altered message,
  erase that altered message,
  generate a new denial (403) message, and
  deliver to client.

Instead of:
 receive adapted (403) message from eCAP
  re-parse that altered message,
 deliver to client.

As you can see its a whole extra round of message processing and memory
allocation. Doubling the CPU cycles spent, and the traffic latency costs
incurred by using the proxy.


There is an "external ACL" interface provided for complex authorization
logics to be offloaded to a helper process with more capabilities than
the proxy. That should be used instead of eCAP/ICAP adaptors.


Amos


From christof.gerber1 at gmail.com  Tue Jun  5 11:29:54 2018
From: christof.gerber1 at gmail.com (chgerber)
Date: Tue, 5 Jun 2018 04:29:54 -0700 (MST)
Subject: [squid-users] Display eCAP meta-information on Squid error-page
In-Reply-To: <a1532a27-0ad2-11de-b402-e6b79d6191df@treenet.co.nz>
References: <CAFyThp+u6Dfvs4mR55+rgWy8PqiXV7dzzMQ9-jF7kqx0HwYQBg@mail.gmail.com>
 <7a5581a4-a5bc-aa28-b026-eb99896622bd@measurement-factory.com>
 <1528190840028-0.post@n4.nabble.com>
 <a1532a27-0ad2-11de-b402-e6b79d6191df@treenet.co.nz>
Message-ID: <1528198194219-0.post@n4.nabble.com>

Does that mean it is possible to apply ACL to headers/notes after ecap
processing or not? 

I agree with your efficiency considerations but can you tell me how you
would solve the following requirement with squid and without eCAP/ICAP:

Parse the body of all requests with a non-empty body and block all requests
containing a certain string "foo".

This is not content adaptation as you mentioned as main use case of ecap but
access control based on content analysis. I see no way other than using
ecap/icap. The reason why we would like to avoid to create the error message
in the ecap adapter itself is that we would like to do the access control
within squid only and using the ecap adapter as content analysis tool with
feedback on which basis squid could decide (ACL). 




--
Sent from: http://squid-web-proxy-cache.1019090.n4.nabble.com/Squid-Users-f1019091.html


From rousskov at measurement-factory.com  Tue Jun  5 15:19:51 2018
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Tue, 5 Jun 2018 09:19:51 -0600
Subject: [squid-users] Display eCAP meta-information on Squid error-page
In-Reply-To: <1528190840028-0.post@n4.nabble.com>
References: <CAFyThp+u6Dfvs4mR55+rgWy8PqiXV7dzzMQ9-jF7kqx0HwYQBg@mail.gmail.com>
 <7a5581a4-a5bc-aa28-b026-eb99896622bd@measurement-factory.com>
 <1528190840028-0.post@n4.nabble.com>
Message-ID: <47e787d7-280c-356a-9e88-e6d9d6f6724d@measurement-factory.com>

On 06/05/2018 03:27 AM, chgerber wrote:
> "It is best to use %note logformat %code for logging annotations. 
> The %adapt::<last_h code is meant for adaptation services debugging (and 
> to work around the current ICAP code lack of support for annotations)." 

> How exactly can I use %note to log the same information to access.log? For
> example assume I use "%{my-ecap-header}adapt::<last_h" how can I log the
> same using %note as you suggested? 


  logformat myLog ... adapter-decision=%{my-ecap-header}note ...
  access_log ... myLog ...


Newer Squids may also support a more natural %note{my-ecap-header}
syntax. Use that if you can.


> Can I apply ACL's to annotations served by eCAP adapters.

Yes, of course. The note ACL does not know where the annotation came
from. Just make sure that you are using the directives that are checked
after Squid receives transaction annotations from your eCAP adapter.

For example, using http_access will not work in most cases because
(SslBump exceptions aside) that directive is only checked before Squid
talks to eCAP adapter(s). However, there is adapted_http_access that is
checked after request adaptations.


> Say when
> %{my-ecap-header}adapt::<last_h or the same solution with %note respectively
> (see first part of post) returns "bad" I want squid to deny the access and
> grant access when it returns "good"?

This sketch is a possible starting point:

  acl badRequest note my-ecap-header bad
  adapted_http_access deny badRequest
  adapted_http_access allow all

The exact correct configuration would depend on the specifics of your
use case. For example, the above allows unrated requests, but you may
want to block (some of) them.


HTH,

Alex.


From rousskov at measurement-factory.com  Tue Jun  5 15:29:19 2018
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Tue, 5 Jun 2018 09:29:19 -0600
Subject: [squid-users] Display eCAP meta-information on Squid error-page
In-Reply-To: <1528198194219-0.post@n4.nabble.com>
References: <CAFyThp+u6Dfvs4mR55+rgWy8PqiXV7dzzMQ9-jF7kqx0HwYQBg@mail.gmail.com>
 <7a5581a4-a5bc-aa28-b026-eb99896622bd@measurement-factory.com>
 <1528190840028-0.post@n4.nabble.com>
 <a1532a27-0ad2-11de-b402-e6b79d6191df@treenet.co.nz>
 <1528198194219-0.post@n4.nabble.com>
Message-ID: <603b2da2-a66b-d681-f963-4cf19487b376@measurement-factory.com>

On 06/05/2018 05:29 AM, chgerber wrote:

> can you tell me how you
> would solve the following requirement with squid and without eCAP/ICAP:

> Parse the body of all requests with a non-empty body and block all requests
> containing a certain string "foo".

You cannot satisfy the above requirement without eCAP or ICAP. Amos
probably assumed that your adapter does not care about the message body,
but that assumption is incorrect in your environment.


> This is not content adaptation as you mentioned as main use case of ecap

Lots of eCAP adapters and ICAP services do not modify message content.
In this context, the unfortunate standard "content adaptation" term
should be interpreted broadly to include content analysis. Content
analysis is one of the primary use cases for eCAP adapters and ICAP
services.


HTH,

Alex.


From christof.gerber1 at gmail.com  Tue Jun  5 15:48:30 2018
From: christof.gerber1 at gmail.com (chgerber)
Date: Tue, 5 Jun 2018 08:48:30 -0700 (MST)
Subject: [squid-users] Display eCAP meta-information on Squid error-page
In-Reply-To: <47e787d7-280c-356a-9e88-e6d9d6f6724d@measurement-factory.com>
References: <CAFyThp+u6Dfvs4mR55+rgWy8PqiXV7dzzMQ9-jF7kqx0HwYQBg@mail.gmail.com>
 <7a5581a4-a5bc-aa28-b026-eb99896622bd@measurement-factory.com>
 <1528190840028-0.post@n4.nabble.com>
 <47e787d7-280c-356a-9e88-e6d9d6f6724d@measurement-factory.com>
Message-ID: <1528213710727-0.post@n4.nabble.com>

> logformat myLog ... adapter-decision=%{my-ecap-header}note ... 
>  access_log ... myLog ..

I tried this but it didn't work as it does with
"%{my-ecap-header}adapt::<last_h". I am not sure if "my-ecap-header" is a
really a header as I called it as I hand it over to squid as
libecap::NamedValueVisitor when visitEachOption() is called. I guess it is
an ecap option or ecap meta information rather than a header, right? I guess
when you talk about ecap headers you mean http headers set by the eCAP
adapter. 

Should the logging with %note and applying ACL also work with ecap meta
information and am I doing something wrong or is this not supported with
ecap meta information? In the latter case I guess I would have to change it
to a solution where the eCAP adapter sets a proper http header which can
then be logged as described above.

Source of confusion:
"adapt::<last_h	The header of the last ICAP response or meta-information
from the last eCAP transaction related to the HTTP transaction."
http://www.squid-cache.org/Versions/v3/3.5/cfgman/logformat.html




--
Sent from: http://squid-web-proxy-cache.1019090.n4.nabble.com/Squid-Users-f1019091.html


From rousskov at measurement-factory.com  Tue Jun  5 15:59:38 2018
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Tue, 5 Jun 2018 09:59:38 -0600
Subject: [squid-users] Display eCAP meta-information on Squid error-page
In-Reply-To: <a1532a27-0ad2-11de-b402-e6b79d6191df@treenet.co.nz>
References: <CAFyThp+u6Dfvs4mR55+rgWy8PqiXV7dzzMQ9-jF7kqx0HwYQBg@mail.gmail.com>
 <7a5581a4-a5bc-aa28-b026-eb99896622bd@measurement-factory.com>
 <1528190840028-0.post@n4.nabble.com>
 <a1532a27-0ad2-11de-b402-e6b79d6191df@treenet.co.nz>
Message-ID: <b88b3aca-8184-2a2e-6bab-a7898c996d01@measurement-factory.com>

On 06/05/2018 04:51 AM, Amos Jeffries wrote:

> The adaptor API purpose is to alter HTTP messages as they travel through
> the proxy, not to be a substitute for access control logic already
> available in the proxy.

This statement is incorrect for legitimate use cases where the required
access control logic is not supported by Squid internally. A typical
alternation-free example is content analysis (which Squid ACLs cannot
perform).


> So what your adaptor SHOULD be doing is simply
> producing the 403 Forbidden message itself.

Sometimes, that is the best solution indeed, but it may also be a bad
solution in some cases because it can be slower and because it
duplicates (or discards) a lot of advanced functionality already
implemented in Squid. The rule of thumb here is "If Squid can generate
the right blocking message, use Squid (instead of the adapter) to
generate the right blocking message".


> By using a header as described you are forcing Squid to:
>   receive adapted message from eCAP
>   re-parse that altered message,
>   erase that altered message,
>   generate a new denial (403) message, and
>   deliver to client.

Adapter implementors are not "forced" to use the above sequence of
steps: The first three steps do not have to happen (they are optional).
An optimized adapter implementation that lets Squid generate the
blocking message is limited to the last two steps from your list:

   generate a new denial (403) message, and
   deliver to client.


> There is an "external ACL" interface provided for complex authorization
> logics to be offloaded to a helper process with more capabilities than
> the proxy. That should be used instead of eCAP/ICAP adaptors.

An adapter may be a better solution than the external ACL in some cases.
The actual decision logic here is roughly as follows:

* If built-in ACLs alone are sufficient, then
  use just the built-in ACLs. They are usually simpler and faster.

* If an external ACL is sufficient and performance is not an issue, then
  use an external ACL. It is a lot simpler to implement than an adapter.

* If the decision logic involves message body analysis, then
  use an eCAP adapter or an ICAP service. Others do not get content.

* Otherwise, carefully evaluate external ACLs vs eCAP adapter choice
  given your use case specifics. eCAP can be faster than an external
  ACL or vice versa.


HTH,

Alex.


From rousskov at measurement-factory.com  Tue Jun  5 16:19:25 2018
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Tue, 5 Jun 2018 10:19:25 -0600
Subject: [squid-users] Display eCAP meta-information on Squid error-page
In-Reply-To: <1528213710727-0.post@n4.nabble.com>
References: <CAFyThp+u6Dfvs4mR55+rgWy8PqiXV7dzzMQ9-jF7kqx0HwYQBg@mail.gmail.com>
 <7a5581a4-a5bc-aa28-b026-eb99896622bd@measurement-factory.com>
 <1528190840028-0.post@n4.nabble.com>
 <47e787d7-280c-356a-9e88-e6d9d6f6724d@measurement-factory.com>
 <1528213710727-0.post@n4.nabble.com>
Message-ID: <d010be16-b35d-8041-1854-a450a0f75114@measurement-factory.com>

On 06/05/2018 09:48 AM, chgerber wrote:
>> logformat myLog ... adapter-decision=%{my-ecap-header}note ... 
>>  access_log ... myLog ..
> 
> I tried this but it didn't work as it does with
> "%{my-ecap-header}adapt::<last_h".

If it does not work, then most likely there is a bug in your adapter, in
your Squid configuration, or in Squid.


> I am not sure if "my-ecap-header" is a
> really a header as I called it as I hand it over to squid as
> libecap::NamedValueVisitor when visitEachOption() is called. I guess it is
> an ecap option or ecap meta information rather than a header, right?

It is a meta-header or annotation, similar to the ICAP response header
(not to be confused with the HTTP header embedded in an ICAP response).


> I guess when you talk about ecap headers you mean http headers set by
> the eCAP adapter.
If I used the "ecap headers" terminology in this context, then I did not
mean the HTTP header. I meant the meta-header or annotation set by your
eCAP adapter via the libecap::Options API presented by your
libecap::adapter::Xaction implementation.


> Should the logging with %note and applying ACL also work with ecap meta
> information and am I doing something wrong or is this not supported with
> ecap meta information?

AFAIK, it is supported in modern Squids. It used to work when I last
tested it. I believe it is supported in v3.5 as well, but I have not
tested it recently.


> Source of confusion:
> "adapt::<last_h	The header of the last ICAP response or meta-information
> from the last eCAP transaction related to the HTTP transaction."
> http://www.squid-cache.org/Versions/v3/3.5/cfgman/logformat.html

That definition sounds correct to me. Once you figure it out, please
consider improving the documentation to be less confusing.


Thank you,

Alex.




From tiraen at gmail.com  Tue Jun  5 19:12:14 2018
From: tiraen at gmail.com (Tiraen)
Date: Tue, 5 Jun 2018 22:12:14 +0300
Subject: [squid-users] Question about traffic calculate
In-Reply-To: <4dd5dbf7-661a-3378-33be-a7a9da2912cf@treenet.co.nz>
References: <CANhj9oyRn5GvSDjUa405Tz90X8kZY4mEoWPv341M1eUFbr2WfQ@mail.gmail.com>
 <4dd5dbf7-661a-3378-33be-a7a9da2912cf@treenet.co.nz>
Message-ID: <CANhj9oyvYkuxk5pWTT=FPFggZ2kmcFa+Gu3yUXvNiURmouCxmg@mail.gmail.com>

*The second transaction has not yet reached that state despite
81017sechaving past. *
Thank you for clarification.

About squid version

*squid -v*
*Squid Cache: Version 3.5.27*
*Service Name: squid*
*configure options:  '--build=x86_64-linux-gnu' '--prefix=/usr'
'--includedir=/include' '--mandir=/share/man' '--infodir=/share/info'
'--sysconfdir=/etc' '--localstatedir=/var' '--libexecdir=/lib/squid3'
'--srcdir=.' '--disable-maintainer-mode' '--disable-dependency-tracking'
'--disable-silent-rules' '--datadir=/usr/share/squid3'
'--sysconfdir=/etc/squid3' '--mandir=/usr/share/man' '--enable-inline'
'--disable-arch-native' '--enable-async-io=8'
'--enable-storeio=ufs,aufs,diskd,rock' '--enable-removal-policies=lru,heap'
'--enable-delay-pools' '--enable-cache-digests' '--enable-icap-client'
'--enable-follow-x-forwarded-for'
'--enable-auth-basic=DB,fake,getpwnam,LDAP,NCSA,NIS,PAM,POP3,RADIUS,SASL,SMB'
'--enable-basic-auth-helpers=squid_radius_auth'
'--enable-auth-digest=file,LDAP' '--enable-auth-negotiate=kerberos,wrapper'
'--enable-auth-ntlm=fake,smb_lm'
'--enable-external-acl-helpers=file_userip,kerberos_ldap_group,LDAP_group,session,SQL_session,unix_group,wbinfo_group'
'--enable-url-rewrite-helpers=fake' '--enable-eui' '--enable-esi'
'--enable-http-violations' '--enable-icmp' '--enable-zph-qos'
'--disable-translation' '--with-swapdir=/var/spool/squid3'
'--with-logdir=/var/log/squid3' '--with-pidfile=/var/run/squid3.pid'
'--with-filedescriptors=65536' '--with-large-files'
'--with-default-user=proxy' '--enable-ssl'
'--with-open-ssl=/etc/ssl/openssl.cnf' '--enable-linux-netfilter'
'CFLAGS=-g -O2 -fPIE -fstack-protector-strong -Wformat
-Werror=format-security -Wall' 'LDFLAGS=-fPIE -pie -Wl,-z,relro -Wl,-z,now'
'CPPFLAGS=-D_FORTIFY_SOURCE=2' 'CXXFLAGS=-g -O2 -fPIE
-fstack-protector-strong -Wformat -Werror=format-security'
'build_alias=x86_64-linux-gnu'*

Regarding the configuration when there is no out data

Squid himself listen localhost without https in SNP mode (i checked without
SNP with same result)

*netstat -anp | grep squid*
*tcp        0      0 127.0.0.1:8080 <http://127.0.0.1:8080>
0.0.0.0:*               LISTEN      835/(squid-coord-3)*
*tcp        0      0 127.0.0.1:8081 <http://127.0.0.1:8081>
0.0.0.0:*               LISTEN      835/(squid-coord-3)*

In front of the SQUID stand nghtttpx as ssl/spdy frontend with backend squid


*frontend=0.0.0.0,3000*
*backend=127.0.0.1,8080*
*backend=127.0.0.1,8081*

In the specified configuration, there are no out data



2018-06-05 6:25 GMT+03:00 Amos Jeffries <squid3 at treenet.co.nz>:

> On 05/06/18 11:34, Tiraen wrote:
> > Good day. I apologize in advance if this has already been discussed, if
> > so - just give a link to the discussion
> >
> > The proxy server has an interface for viewing current active sessions
> >
> > http://{}:{}/squid-internal-mgr/active_requests
> >
>
> Please be aware these are *not* "sessions". These are transactions,
> which  have one request, one response, and maybe some informational
> messages.
>
> A "session" as far as it relates to HTTP is a application level thing
> which includes _multiple_ transactions, and possibly even multiple TCP
> connections at the client end.
>
>
> > or
> >
> > cache_object://%s/active_requests
> >
> > There there is some set of parameters which allow to get the data on
> traffic
> >
> > If the connection to the proxy goes directly and by http we see like
> this:
> >
> > /Connection: 0x8050e0518/
> > /        FD 29, read 4247, wrote 13479/
> > /        FD desc: Reading next request/
> > /        in: buf 0x8045a6fe0, used 0, free 39/
> > /        remote: ????:50340/
> > /        local: ????:8080/
> > /        nrequests: 1/
> > /uri ???:443/
> > /logType TCP_TUNNEL/
> > /out.offset 0, out.size 13440/
> > /req_sz 235/
> > /entry 0x0/N/A/
> > /start 1527608373.902584 (73.252258 seconds ago)/
> > /username -/
> > /delay_pool 0/
> >
> >
> > We have both traffic stat
> >
> > /out.offset 0, out.size 13440/
> > /req_sz 235/
> >
>
> The latest transactions request was 235 bytes, its reply was 13440 bytes
> (so far).
>
>
> > But if there is a frontend in front of the SQUID (nghttpx for example
> > and https)
> >
> > we have this
> >
> > /Connection: 0x7f66a317ecf8/
> > /    FD 222, read 9192, wrote 526/
> > /    FD desc: Reading next request/
> > /    in: buf 0x7f66a294fb90, used 0, free 39/
> > /    remote: 127.0.0.1:2314 <http://127.0.0.1:2314>/
> > /    local: 127.0.0.1:8081 <http://127.0.0.1:8081>/
> > /    nrequests: 2/
> > /uri nererut.com:443 <http://nererut.com:443>/
> > /logType TAG_NONE/
> > /out.offset 0, out.size 0/
> > /req_sz 334/
> > /entry (nil)/N/A/
> > /start 1527526715.189831 (81017.831772 seconds ago)/
> > /username 8355fcec-94fd-496c-94d1-a195a5ca7148/
> > /delay_pool 0
> > /
> > without out traffic
> >
> > /out.offset 0, out.size 0/
> > /req_sz 334/
>
> This transaction request was 334 bytes, its reply was 0 bytes (so far).
>
>
> >
> > I certainly did not test why it happens - due to https or proxy, but is
> > it possible to clarify this case?
>
> The first transaction has reached the stage where a reply has started.
>
> The second transaction has not yet reached that state despite 81017sec
> having past.
>
> That is all we can say without more information about things like for
> example, which Squid version you are using, whether you are SSL-Bumping
> the HTTPS traffic for either of those transactions, network topology on
> the outgoing side of Squid, etc.
>
>
> Amos
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>



-- 
With best regards,

Vyacheslav Yakushev,

Unix system administrator

https://t.me/kelewind
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20180605/004b8c62/attachment.htm>

From squid3 at treenet.co.nz  Wed Jun  6 04:51:36 2018
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Wed, 6 Jun 2018 16:51:36 +1200
Subject: [squid-users] Question about traffic calculate
In-Reply-To: <CANhj9oyvYkuxk5pWTT=FPFggZ2kmcFa+Gu3yUXvNiURmouCxmg@mail.gmail.com>
References: <CANhj9oyRn5GvSDjUa405Tz90X8kZY4mEoWPv341M1eUFbr2WfQ@mail.gmail.com>
 <4dd5dbf7-661a-3378-33be-a7a9da2912cf@treenet.co.nz>
 <CANhj9oyvYkuxk5pWTT=FPFggZ2kmcFa+Gu3yUXvNiURmouCxmg@mail.gmail.com>
Message-ID: <fa5edf17-c0ed-4ac1-c5fe-b77839752dfc@treenet.co.nz>

On 06/06/18 07:12, Tiraen wrote:
> /The second transaction has not yet reached that state despite 81017sec
> having past.
> /
> Thank you for clarification.
> 
> About squid version
> 
> /squid -v/
> /Squid Cache: Version 3.5.27/
...

If you are using SSL-Bump features, please consider Squid-4 instead. The
strangely long timeouts on transactions is likely to be a side effect of
on old behaviour in Squid-3 seen with transactions that were bumped.


> '--enable-ssl'
> '--with-open-ssl=/etc/ssl/openssl.cnf'

Two problems with the above:

 1) the option name is "--with-openssl".

 2) that option takes the directory PATH where the OpenSSL development
files were installed. If using the OS provided library package *omit*
the =PATH portion.


Amos


From eliezer at ngtech.co.il  Wed Jun  6 07:28:47 2018
From: eliezer at ngtech.co.il (Eliezer Croitoru)
Date: Wed, 6 Jun 2018 10:28:47 +0300
Subject: [squid-users] AMI 1+2 RPM packages ready!
Message-ID: <008001d3fd68$02259580$0670c080$@ngtech.co.il>

OK so I took some time to understand and hack amazon linux 1 and. based on
the docker container I created a kvm guest VM build node.

I could have used the docker container but I like a static VM much more.

 

The result packages are at:

http://ngtech.co.il/repo/amzn/

 

which have packages for both amzn 1 and 2 and for both 3.5.27 and 4.0.24.

 

While working on the VM and the SPEC files I noticed that there was a typo
in all of my older versions until today.

For some reason despite to the typo Squid was built and ran fine in
production for a long time.

I was unsure if to patch 3.5.27 with the latest 2018.03 patch. but since I
worked on the different SPEC files I assume that in the next couple weeks I
will release 3.5.27+patches.

.If there will be another release as 3.5.28 I will not create a specially
patched RPM release.

 

The AMI RPM build sources(else then the SRPMS at the repo) are at:

http://gogs.ngtech.co.il/NgTech-LTD/squid-amzn2-squid35-rpms

 

http://gogs.ngtech.co.il/NgTech-LTD/squid-amzn2-squid4-rpms

 

http://gogs.ngtech.co.il/NgTech-LTD/squid-amzn1-squid4-rpms

 

And the amzn1 3.5.27 has SRPM:

http://ngtech.co.il/repo/amzn/1/SRPMS/squid-3.5.27-2.amzn1.src.rpm

 

Which should be enough for any AWS developer that works with AMI 1 or 2.

 

The AMI amzn1 qcow2 image is available at:

http://ngtech.co.il/static/amzn/amzn1-rc1.qcow2

 

just add "faster." To the domain and it should push the packets faster.

AMI machines are based on a combination of stability and cutting edge so
amzn1 is pretty solid.

However amzn2 offers systemd which I recommend to plan to use if you don't
already.

I also noticed that there are couple tiny things which blocks amzn2 LTS
moving from the RC state of it but compared to Ubuntu I think it get's more
development and upgrades.

It's probably because globally there is more money opportunity in it..

 

OK this is it for today.

 

Eliezer

 

----

Eliezer Croitoru <http://ngtech.co.il/lmgtfy/> 
Linux System Administrator
Mobile: +972-5-28704261
Email: eliezer at ngtech.co.il



 

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20180606/3ded6c9d/attachment.htm>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: image003.png
Type: image/png
Size: 11317 bytes
Desc: not available
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20180606/3ded6c9d/attachment.png>

From tiraen at gmail.com  Wed Jun  6 09:21:51 2018
From: tiraen at gmail.com (Tiraen)
Date: Wed, 6 Jun 2018 12:21:51 +0300
Subject: [squid-users] Question about traffic calculate
In-Reply-To: <fa5edf17-c0ed-4ac1-c5fe-b77839752dfc@treenet.co.nz>
References: <CANhj9oyRn5GvSDjUa405Tz90X8kZY4mEoWPv341M1eUFbr2WfQ@mail.gmail.com>
 <4dd5dbf7-661a-3378-33be-a7a9da2912cf@treenet.co.nz>
 <CANhj9oyvYkuxk5pWTT=FPFggZ2kmcFa+Gu3yUXvNiURmouCxmg@mail.gmail.com>
 <fa5edf17-c0ed-4ac1-c5fe-b77839752dfc@treenet.co.nz>
Message-ID: <CANhj9oywS173VafQhNa1S4_qznc-hmadoOiSRLH1HEPef_ORdQ@mail.gmail.com>

>If you are using SSL-Bump features, please consider Squid-4 instead

It is not used at all.Squid does not work with ssl. Frontend only


Concerning incorrectly specified options at build

Here on this squid happens the same thing:

* squid3 -v*
*Squid Cache: Version 3.4.8*
* linux*
*configure options:  '--build=x86_64-linux-gnu' '--prefix=/usr'
'--includedir=${prefix}/include' '--mandir=${prefix}/share/man'
'--infodir=${prefix}/share/info' '--sysconfdir=/etc' '--localstatedir=/var'
'--libexecdir=${prefix}/lib/squid3' '--srcdir=.'
'--disable-maintainer-mode' '--disable-dependency-tracking'
'--disable-silent-rules' '--datadir=/usr/share/squid3'
'--sysconfdir=/etc/squid3' '--mandir=/usr/share/man' '--enable-inline'
'--disable-arch-native' '--enable-async-io=8'
'--enable-storeio=ufs,aufs,diskd,rock' '--enable-removal-policies=lru,heap'
'--enable-delay-pools' '--enable-cache-digests' '--enable-icap-client'
'--enable-follow-x-forwarded-for'
'--enable-auth-basic=DB,fake,getpwnam,LDAP,MSNT,MSNT-multi-domain,NCSA,NIS,PAM,POP3,RADIUS,SASL,SMB'
'--enable-auth-digest=file,LDAP' '--enable-auth-negotiate=kerberos,wrapper'
'--enable-auth-ntlm=fake,smb_lm'
'--enable-external-acl-helpers=file_userip,kerberos_ldap_group,LDAP_group,session,SQL_session,unix_group,wbinfo_group'
'--enable-url-rewrite-helpers=fake' '--enable-eui' '--enable-esi'
'--enable-icmp' '--enable-zph-qos' '--enable-ecap' '--disable-translation'
'--with-swapdir=/var/spool/squid3' '--with-logdir=/var/log/squid3'
'--with-pidfile=/var/run/squid3.pid' '--with-filedescriptors=65536'
'--with-large-files' '--with-default-user=proxy' '--enable-build-info=
linux' '--enable-linux-netfilter' 'build_alias=x86_64-linux-gnu' 'CFLAGS=-g
-O2 -fPIE -fstack-protector-strong -Wformat -Werror=format-security -Wall'
'LDFLAGS=-fPIE -pie -Wl,-z,relro -Wl,-z,now' 'CPPFLAGS=-D_FORTIFY_SOURCE=2'
'CXXFLAGS=-g -O2 -fPIE -fstack-protector-strong -Wformat
-Werror=format-security'*


*no out data*


*Connection: 0x7f18ee951c58*
* FD 15, read 10070, wrote 19018*
* FD desc: Reading next request*
* in: buf 0x7f18ee952070, offset 0, size 4096*
* remote: 127.0.0.1:52827 <http://127.0.0.1:52827>*
* local: 127.0.0.1:8080 <http://127.0.0.1:8080>*
* nrequests: 38*
*uri http://icanhazip.com/ <http://icanhazip.com/>*
*logType TCP_MISS*
*out.offset 0, out.size 0*
*req_sz 265*
*entry 0x7f18ee3bc740/F9929050DEE6E67D2DF51EDCBC0CB80F*
*start 1528276856.390709 (2.640371 seconds ago)*
*username*
*delay_pool 0*

*Connection: 0x7f18ee874168*
* FD 13, read 10070, wrote 19018*
* FD desc: Reading next request*
* in: buf 0x7f18ee86bb60, offset 0, size 4096*
* remote: 127.0.0.1:52825 <http://127.0.0.1:52825>*
* local: 127.0.0.1:8080 <http://127.0.0.1:8080>*
* nrequests: 38*
*uri http://icanhazip.com/ <http://icanhazip.com/>*
*logType TCP_MISS*
*out.offset 0, out.size 0*
*req_sz 265*
*entry 0x7f18ee87fde0/560E3AC236A180ECB815B5B41527D2BA*
*start 1528276856.368609 (2.662471 seconds ago)*
*username*


*delay_pool 0*



2018-06-06 7:51 GMT+03:00 Amos Jeffries <squid3 at treenet.co.nz>:

> On 06/06/18 07:12, Tiraen wrote:
> > /The second transaction has not yet reached that state despite 81017sec
> > having past.
> > /
> > Thank you for clarification.
> >
> > About squid version
> >
> > /squid -v/
> > /Squid Cache: Version 3.5.27/
> ...
>
> If you are using SSL-Bump features, please consider Squid-4 instead. The
> strangely long timeouts on transactions is likely to be a side effect of
> on old behaviour in Squid-3 seen with transactions that were bumped.
>
>
> > '--enable-ssl'
> > '--with-open-ssl=/etc/ssl/openssl.cnf'
>
> Two problems with the above:
>
>  1) the option name is "--with-openssl".
>
>  2) that option takes the directory PATH where the OpenSSL development
> files were installed. If using the OS provided library package *omit*
> the =PATH portion.
>
>
> Amos
>



-- 
With best regards,

Vyacheslav Yakushev,

Unix system administrator

https://t.me/kelewind
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20180606/e3016e0f/attachment.htm>

From Edward.Cheadle at cambiahealth.com  Wed Jun  6 23:11:56 2018
From: Edward.Cheadle at cambiahealth.com (Cheadle, Edward)
Date: Wed, 6 Jun 2018 23:11:56 +0000
Subject: [squid-users] Squid.out reports errors for lines that do not exist
	in squid.conf
Message-ID: <5B3F54FC-DE6E-40F5-B08A-4873B5BA258A@regence.com>

Squid version 3.5.27-1.el6
Linux:  Amazon Linux AMI 2018.03.0

I saw the warning messages below in squid.out, did some research and found out that the messagesseemed to be caused the two lines:
acl localhost src 127.0.0.1/32
acl to_localhost dst 127.0.0.0/8 0.0.0.0/32

From http://www.squid-cache.org/Versions/v3/3.5/cfgman/acl.html I see that both these two acl?s are now predefined and are not shown in the recommended configuration.
I suspect the person that installed squid for us found an older conf file from somewhere.

2018/06/06 20:49:29| WARNING: (B) '127.0.0.1' is a subnetwork of (A) '127.0.0.1'
2018/06/06 20:49:29| WARNING: because of this '127.0.0.1' is ignored to keep splay tree searching predictable
2018/06/06 20:49:29| WARNING: You should probably remove '127.0.0.1' from the ACL named 'localhost'


The FATAL error messages below  is because I added a line with dstdomain in it when there were already an entry with dstdom_regex in it.
I assume that is the ?type? that is talked about.

2018/06/06 20:49:29| aclParseAclLine: ACL 'aws_s3' already exists with different type.
FATAL: Bungled /etc/squid/squid.conf line 254: acl aws_s3   dstdomain -n .s3.amazonaws.com

The issue is that I removed line 254 and I removed both of the lines above and yet I restart squid and continue to get these errors.

Does squid compile the rules and keep them somewhere, and I need to do something else than service squid restart to get rid of rules?

IMPORTANT NOTICE: This communication, including any attachment, contains information that may be confidential or privileged, and is intended solely for the entity or individual to whom it is addressed.  If you are not the intended recipient, you should delete this message and are hereby notified that any disclosure, copying, or distribution of this message is strictly prohibited.  Nothing in this email, including any attachment, is intended to be a legally binding signature.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20180606/25a8c8a8/attachment.htm>

From Edward.Cheadle at cambiahealth.com  Wed Jun  6 23:21:03 2018
From: Edward.Cheadle at cambiahealth.com (Cheadle, Edward)
Date: Wed, 6 Jun 2018 23:21:03 +0000
Subject: [squid-users] Connection Timeouts
In-Reply-To: <03d801d3fc6e$155bdba0$401392e0$@ngtech.co.il>
References: <973AC2B7-9270-4378-97E5-0D6AEF24624A@contoso.com>
 <031401d3fc32$218ed710$64ac8530$@ngtech.co.il>
 <88F530BE-0AF0-419B-91E2-5112EFF4C039@regence.com>
 <03d801d3fc6e$155bdba0$401392e0$@ngtech.co.il>
Message-ID: <4C9CAA88-ACB7-4539-ADFC-667407301460@regence.com>

Eliezer,
It now seems from working on the squid issue that the problem is not in squid.  All of your suggestions were helpful when I started working through the issues.
We now think that something in the Code used to build the ASG caused whatever timeout we were seeing.

I am now trying to get to the cache manager as you suggested. I suspect I am having problems because of the Security groups.   But I just wanted to tell you your information was very valuable and got 
me thinking more about squid. 

Thanks for the information and the time you took to look at the issue.  


?On 6/4/18, 7:40 PM, "Eliezer Croitoru" <eliezer at ngtech.co.il> wrote:

    So just to make sure I understand.
    Is squid acting only as an ACL proxy server from inside AWS internal network toward the outside world?
    Increasing the timeout to 5 minutes will maybe increase the usage of FD but if this squid has only one worker(basic simple setup with ACL's)
    then you can change a timeout and increase the FD the system can handle...
    A single working instance of Squid can handle up to a certain amount of traffic and if the instance has let say 2 GB you can safely upper the limit to 64k FD.
    My Atom based PC here can handle 64k FD just fine while the actual hardware technically limits it to something like 32k.
    On my Xeon based Server I am building and packaging squid with 16k basic limit and it works for most of the business setups out there(not including ISP's).
    
    If all these servers that are using the Squid service are on the same network segment then it would be very weird to change any timeout.
    If these servers are not on the same network segment what you need is to turn on keep alive probe let say to 15 seconds per probe.
    It will "increase" from n packets to n+(4*connection minutes duration) but as long it is a single worker basic proxy it's nothing.
    Try to look at the cache manager interface output for the "info" page and see what is the average connections per second on the Squid service.
    (let me know if you need help to get the info cache manager page)
    With these numbers you would be able to understand what might causing service disruption.
    
    Eliezer
    
    * by any chance AWS Linux AMD 2018.03.0 has systemd in it or I am imagining that it still uses sysVinit?
    
    ----
    Eliezer Croitoru
    Linux System Administrator
    Mobile: +972-5-28704261
    Email: eliezer at ngtech.co.il
    
    
    
    -----Original Message-----
    From: Cheadle, Edward <Edward.Cheadle at cambiahealth.com> 
    Sent: Monday, June 4, 2018 23:07
    To: Eliezer Croitoru <eliezer at ngtech.co.il>
    Cc: squid-users at lists.squid-cache.org
    Subject: Re: [squid-users] Connection Timeouts
    
    Eliezer, you are absolutely right.  I got in a hurry and forgot the basics such as version numbers and all the other details.
    
    The version currently on our squid server is: squid-3.5.27-1.el6.x86_64.rpm
    We are running AWS Linux:  Amazon Linux AMI 2018.03.0
    
    We are a health care company.  We are using squid proxy to control what the servers in an account can connect to on the internet.  AWS looked at an issue we had with code deploy and they said connections were timing out because the default connection timeout is 1 min, and suggested we change the timeout to 5 min.  It issue has to do with Codedeploy.  Since AWS services are on the internet, I was thinking if we could set an overall timeout, and then one for services that are known to take more time, I thought it would be a way keep the length of the timeout down for most things and free up resources for the majority of tasks.
    
    My concern, as stated below is that connections will take a while to timeout and it will put more pressure on the number of file descriptors we use.  We ran into an issue with the number of file descriptors used, but figured it out and we are fine, but increasing the timeout to 5 min set off a warning flag in my mind, not having a lot of experience with squid. I am not even sure it is an issue, but I thought I try to make sure before we ran into production issues.
    
    The reason for including the link, is that it was the first one I found and in the description they mentioned the ability to set timeouts on a site/domain-specific basis, but in the info that followed and in subsequent searches, I did not see how it was done, so the failure to find information on the subject led me to join the list.  
     
    In looking at the docs, there are a  number of other timeouts, so I obviously have some homework to do.
    
    Thanks for the quick response.
    
    
    
    On 6/4/18, 12:31 PM, "Eliezer Croitoru" <eliezer at ngtech.co.il> wrote:
    
        Hey Edward,
        
        First congrats!.
        I hope we can help you to figure out the relevant details.
        
        I am not sure why you have spoken to AWS teams about Squid-Cache, may I ask what OS are you using in AWS?
        Also what version of Squid are you using?
        The timeout settings are "critical" indeed but depends on what you are using and doing with Squid-Cache.
        Despite to the fact that https://na01.safelinks.protection.outlook.com/?url=https%3A%2F%2Fwww.visolve.com%2Fsquid%2Fsquid30%2Ftimeout&data=02%7C01%7CEdward.Cheadle%40cambiahealth.com%7C8be888b30a484f0d8b4f08d5ca49570f%7Ce964274919d44f7fb4df802b2b75a809%7C0%7C0%7C636637338708424102&sdata=SpOxewYBxY1Y7qeK7fk5cEF0pWN2l%2B4UOM6IclHVrbw%3D&reserved=0 Is in a way still a lead it's not "up-to-date"
        
        Please note that without understanding what issues have you been facing and the purpose of the Squid-Cache instance(s?) there is no way to even guess what might fit your needs.
        
        Eliezer
        
        ----
        https://na01.safelinks.protection.outlook.com/?url=http%3A%2F%2Fngtech.co.il%2Flmgtfy%2F&data=02%7C01%7CEdward.Cheadle%40cambiahealth.com%7C8be888b30a484f0d8b4f08d5ca49570f%7Ce964274919d44f7fb4df802b2b75a809%7C0%7C0%7C636637338708424102&sdata=Mpu0Ottn255qQxnsXGT%2F%2ByR432Yz9%2FckeKTuVpZ6aUM%3D&reserved=0
        Linux System Administrator
        Mobile: +972-5-28704261
        Email: eliezer at ngtech.co.il
        
        
        From: squid-users <squid-users-bounces at lists.squid-cache.org> On Behalf Of Cheadle, Edward
        Sent: Monday, June 4, 2018 21:06
        To: squid-users at lists.squid-cache.org
        Subject: [squid-users] Connection Timeouts
        
        We had a person leave and I got selected to update and maintain our squid proxy.   We are talking to AWS and they told us that we needed to change the connection_timeout value from the default to 5 min.
        
        We have people stress testing out installation and I was concerned that if connection timeouts are too long we may see congestion.
        
        Should I be worried that connection timeouts will use up file descriptors at a higher rate?
        
        And what might be the options?
        
        Doing and internet search I found a web page at https://na01.safelinks.protection.outlook.com/?url=https%3A%2F%2Fwww.visolve.com%2Fsquid%2Fsquid30%2Ftimeout.php&data=02%7C01%7CEdward.Cheadle%40cambiahealth.com%7C8be888b30a484f0d8b4f08d5ca49570f%7Ce964274919d44f7fb4df802b2b75a809%7C0%7C0%7C636637338708424102&sdata=FSq%2FnnFycwsbQaw8xRMzHkBWFY4Iw5F8KeJtdd1hRyc%3D&reserved=0 and in the TIMEOUT description I read
        
        ?TIMEOUT
        Timeout parameters in Squid can be based on overall connection timeouts, peer-specific timeouts, site/domain-specific timeouts, request-specific timeouts etc. Proper setting of timeout values is critical to optimal Squid performance. Relevant parameters for timeout settings are listed?
        
        Is it possible to narrow the connection timeout to a specific site?  I looked at the website information, squid documentation and did an internet search.
        
        I did not see anything that narrowed the timeout to a specific timeout.
        
        I am trying to set connection timeouts to AWS sites, but keep connection timeouts to the default, because it is working well.
        
        IMPORTANT NOTICE: This communication, including any attachment, contains information that may be confidential or privileged, and is intended solely for the entity or individual to whom it is addressed. If you are not the intended recipient, you should delete this message and are hereby notified that any disclosure, copying, or distribution of this message is strictly prohibited. Nothing in this email, including any attachment, is intended to be a legally binding signature.
        
        Ensure a sustainable future - only print when necessary.
        
    
    
    


From squid3 at treenet.co.nz  Thu Jun  7 02:39:16 2018
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 7 Jun 2018 14:39:16 +1200
Subject: [squid-users] Squid.out reports errors for lines that do not
 exist in squid.conf
In-Reply-To: <5B3F54FC-DE6E-40F5-B08A-4873B5BA258A@regence.com>
References: <5B3F54FC-DE6E-40F5-B08A-4873B5BA258A@regence.com>
Message-ID: <08cd358c-2209-7628-beeb-82456e6440cf@treenet.co.nz>

On 07/06/18 11:11, Cheadle, Edward wrote:
> Squid version 3.5.27-1.el6
>
...
> 
> The FATAL error messages below ?is because I added a line with dstdomain
> in it when there were already an entry with dstdom_regex in it.
> 
> I assume that is the ?type? that is talked about.
> 

Correct.

> 
> 2018/06/06 20:49:29| aclParseAclLine: ACL 'aws_s3' already exists with
> different type.
> 
> FATAL: Bungled /etc/squid/squid.conf line 254: acl aws_s3?? dstdomain -n
> .s3.amazonaws.com
> 
> ?
> 
> The issue is that I removed line 254 and I removed both of the lines
> above and yet I restart squid and continue to get these errors.
> 

Exact same line and issue? or other ones elsewhere in the config?

NP: "squid -k parse" should be used to find any issues after an upgrade.
It does not halt on the first FATAL/ERROR if there are many.


> 
> Does squid compile the rules and keep them somewhere, and I need to do
> something else than service squid restart to get rid of rules?

By "service squid restart" I take it that you are using systemd to
control a Squid-3 proxy. systemd cannot cope at all well with software
like Squid which is itself a daemon manager.

Try stopping Squid with the "squid -k shutdown" command (repeat of
necessary) and making sure Squid is fully stopped with no processes
still running before you start it again.

With systemd the "squid -k ..." commands (or init.d script, if any)
should be used to manage Squid-3 instead of systemd's "service ..."
commands.

NP: these issues have been resolved in Squid-4. So this is a temporary
situation until you can upgrade.

Amos


From eliezer at ngtech.co.il  Thu Jun  7 06:13:59 2018
From: eliezer at ngtech.co.il (Eliezer Croitoru)
Date: Thu, 7 Jun 2018 09:13:59 +0300
Subject: [squid-users] Squid.out reports errors for lines that do not
	exist in squid.conf
In-Reply-To: <08cd358c-2209-7628-beeb-82456e6440cf@treenet.co.nz>
References: <5B3F54FC-DE6E-40F5-B08A-4873B5BA258A@regence.com>
 <08cd358c-2209-7628-beeb-82456e6440cf@treenet.co.nz>
Message-ID: <045101d3fe26$bc3f3e30$34bdba90$@ngtech.co.il>

Amos,

Systemd can be define to run a specific command for a "reload" and even if nobody wrote the line in a service file it's there since almost day one of systemd services.

And.. if the version is el6 I believe it's still a sysVinit based system.
Squid -kparse should detect and squid -kreconf should resolve any issue if it's not a fatal one that stopped the service.

Not directly related but.. only if Squid doesn't release at all any memory it catches then a restart would be a must at some point.
>From what I have seen in the 2.7 and 3.x code in the past it seems that there should be some level of memory cleanup\release.
Also I have systems that has up-time of almost a year so I am a bit confused why should a restart would be requied?
... if I have acls handled by an external acl or ICAP service then why should I restart?

Eliezer

----
Eliezer Croitoru
Linux System Administrator
Mobile: +972-5-28704261
Email: eliezer at ngtech.co.il



-----Original Message-----
From: squid-users <squid-users-bounces at lists.squid-cache.org> On Behalf Of Amos Jeffries
Sent: Thursday, June 7, 2018 05:39
To: squid-users at lists.squid-cache.org
Subject: Re: [squid-users] Squid.out reports errors for lines that do not exist in squid.conf

On 07/06/18 11:11, Cheadle, Edward wrote:
> Squid version 3.5.27-1.el6
>
...
> 
> The FATAL error messages below  is because I added a line with dstdomain
> in it when there were already an entry with dstdom_regex in it.
> 
> I assume that is the ?type? that is talked about.
> 

Correct.

> 
> 2018/06/06 20:49:29| aclParseAclLine: ACL 'aws_s3' already exists with
> different type.
> 
> FATAL: Bungled /etc/squid/squid.conf line 254: acl aws_s3   dstdomain -n
> .s3.amazonaws.com
> 
>  
> 
> The issue is that I removed line 254 and I removed both of the lines
> above and yet I restart squid and continue to get these errors.
> 

Exact same line and issue? or other ones elsewhere in the config?

NP: "squid -k parse" should be used to find any issues after an upgrade.
It does not halt on the first FATAL/ERROR if there are many.


> 
> Does squid compile the rules and keep them somewhere, and I need to do
> something else than service squid restart to get rid of rules?

By "service squid restart" I take it that you are using systemd to
control a Squid-3 proxy. systemd cannot cope at all well with software
like Squid which is itself a daemon manager.

Try stopping Squid with the "squid -k shutdown" command (repeat of
necessary) and making sure Squid is fully stopped with no processes
still running before you start it again.

With systemd the "squid -k ..." commands (or init.d script, if any)
should be used to manage Squid-3 instead of systemd's "service ..."
commands.

NP: these issues have been resolved in Squid-4. So this is a temporary
situation until you can upgrade.

Amos
_______________________________________________
squid-users mailing list
squid-users at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-users



From squid3 at treenet.co.nz  Thu Jun  7 10:11:51 2018
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 7 Jun 2018 22:11:51 +1200
Subject: [squid-users] Squid.out reports errors for lines that do not
 exist in squid.conf
In-Reply-To: <045101d3fe26$bc3f3e30$34bdba90$@ngtech.co.il>
References: <5B3F54FC-DE6E-40F5-B08A-4873B5BA258A@regence.com>
 <08cd358c-2209-7628-beeb-82456e6440cf@treenet.co.nz>
 <045101d3fe26$bc3f3e30$34bdba90$@ngtech.co.il>
Message-ID: <9dc90778-b817-7dad-c96d-fc3a621fd51f@treenet.co.nz>

On 07/06/18 18:13, Eliezer Croitoru wrote:
> Amos,
> 
> Systemd can be define to run a specific command for a "reload" and even if nobody wrote the line in a service file it's there since almost day one of systemd services.
> 

*If* that mechanism is used there is no difference in the commands. If
it is not used, the systemd ones are actively dangerous. So no harm in
advising the safe one be used in either case.


> And.. if the version is el6 I believe it's still a sysVinit based system.

Cheadle was using systemd's "service ..." commands. Which I am advising
to avoid because something indeterminate is going wrong with the config
loading and startup process. If the OS is actually SysV those systemd
commands are even more inappropriate.


> Squid -kparse should detect and squid -kreconf should resolve any issue if it's not a fatal one that stopped the service.
> 
> Not directly related but.. only if Squid doesn't release at all any memory it catches then a restart would be a must at some point.
> From what I have seen in the 2.7 and 3.x code in the past it seems that there should be some level of memory cleanup\release.
> Also I have systems that has up-time of almost a year so I am a bit confused why should a restart would be requied?

The admin has apparently got themselves into a difficult situation and
it is no longer clear whether systemd or Squid master process is in
control of the worker processes which are running and with what config.
They both fight over "service ..." commands.

The only thing which is guaranteed to restore Squid to known state is a
full shutdown. Ensuring that everything has exited before proceeding
with the start action.

Amos


From bogdan.stoica at epfl.ch  Thu Jun  7 10:17:30 2018
From: bogdan.stoica at epfl.ch (Stoica Bogdan Alexandru)
Date: Thu, 7 Jun 2018 10:17:30 +0000
Subject: [squid-users] Squid test-suite / benchmarks
Message-ID: <ecb7106901294fbf9060c47a58ec91cf@rexe.intranet.epfl.ch>

Hi all,

We're a small research team interested in benchmarking Squid for a research project.
Ideally, we would like to have good code coverage while doing so. We have searched online for alternatives, but found little info.
Are there any good benchmarks used for such purpose? Or, even better, is there a more comprehensive test suite apart from the one Squid comes with?
Any suggestions are highly appreciated.

Thanks a lot!

B.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20180607/9b07a907/attachment.htm>

From akismpa at gmail.com  Thu Jun  7 10:24:21 2018
From: akismpa at gmail.com (Panagiotis Bariamis)
Date: Thu, 7 Jun 2018 13:24:21 +0300
Subject: [squid-users] Squid test-suite / benchmarks
In-Reply-To: <ecb7106901294fbf9060c47a58ec91cf@rexe.intranet.epfl.ch>
References: <ecb7106901294fbf9060c47a58ec91cf@rexe.intranet.epfl.ch>
Message-ID: <CAPxN_PVGN7FzmO5YEbpQB9ZDB3reS5mQQ2SmSw6HnmCK0dYPKA@mail.gmail.com>

Hello,
I just finished a same project . Only platform that worked well with most
configurations of Squid is Web Polygraph.


On Thu, Jun 7, 2018, 13:17 Stoica Bogdan Alexandru <bogdan.stoica at epfl.ch>
wrote:

> Hi all,
>
>
>
> We?re a small research team interested in benchmarking Squid for a
> research project.
>
> Ideally, we would like to have good code coverage while doing so. We have
> searched online for alternatives, but found little info.
>
> Are there any good benchmarks used for such purpose? Or, even better, is
> there a more comprehensive test suite apart from the one Squid comes with?
>
> Any suggestions are highly appreciated.
>
>
>
> Thanks a lot!
>
>
>
> B.
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20180607/f3a24760/attachment.htm>

From squid3 at treenet.co.nz  Thu Jun  7 10:54:32 2018
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 7 Jun 2018 22:54:32 +1200
Subject: [squid-users] Squid test-suite / benchmarks
In-Reply-To: <CAPxN_PVGN7FzmO5YEbpQB9ZDB3reS5mQQ2SmSw6HnmCK0dYPKA@mail.gmail.com>
References: <ecb7106901294fbf9060c47a58ec91cf@rexe.intranet.epfl.ch>
 <CAPxN_PVGN7FzmO5YEbpQB9ZDB3reS5mQQ2SmSw6HnmCK0dYPKA@mail.gmail.com>
Message-ID: <228af977-6953-cc31-156d-fa6114e306bd@treenet.co.nz>

On 07/06/18 22:24, Panagiotis Bariamis wrote:
> Hello,
> I just finished a same project . Only platform that worked well with
> most configurations of Squid is Web Polygraph.?
> 
> 
> On Thu, Jun 7, 2018, 13:17 Stoica Bogdan Alexandru wrote:
> 
>     Hi all,____
> 
>     __?__
> 
>     We?re a small research team interested in benchmarking Squid for a
>     research project. ____
> 
>     Ideally, we would like to have good code coverage while doing so. We
>     have searched online for alternatives, but found little info. ____
> 
>     Are there any good benchmarks used for such purpose? Or, even
>     better, is there a more comprehensive test suite apart from the one
>     Squid comes with?____
> 
>     Any suggestions are highly appreciated.____
> 


Any chance the details of these research are going to be publicly available?

It has been a while since anyone contributed to the Squid Projects
benchmark collection
<https://wiki.squid-cache.org/KnowledgeBase/Benchmarks>.


Amos


From akismpa at gmail.com  Thu Jun  7 15:48:39 2018
From: akismpa at gmail.com (Panagiotis Bariamis)
Date: Thu, 7 Jun 2018 18:48:39 +0300
Subject: [squid-users] Squid test-suite / benchmarks
In-Reply-To: <228af977-6953-cc31-156d-fa6114e306bd@treenet.co.nz>
References: <ecb7106901294fbf9060c47a58ec91cf@rexe.intranet.epfl.ch>
 <CAPxN_PVGN7FzmO5YEbpQB9ZDB3reS5mQQ2SmSw6HnmCK0dYPKA@mail.gmail.com>
 <228af977-6953-cc31-156d-fa6114e306bd@treenet.co.nz>
Message-ID: <CAPxN_PXs+4k9Q=zzv_00Bubqm16T=ywSV1ip4Ph5Jiibo3-CrA@mail.gmail.com>

 >Any chance the details of these research are going to be publicly
available?

>It has been a while since anyone contributed to the Squid Projects
>benchmark collection
><https://wiki.squid-cache.org/KnowledgeBase/Benchmarks>.

Hi Amos,
As far as my research is concerned it will be publicly available after I
present my thesis on Monday (Load Balancing between 16 squid servers with
authentication for the Greek School Network).
However that will be on testing environment.
For the production squid servers I will have results early next month,
which will become publicly available as well
(16 squid servers - 800.000 registered users , caching only on updates of
windows, mostly used for url filtering with ufdbguard) .
If you want I can send you some grafana reports for squid usage in
intercept mode vs squid with ldap authentication vs sslbump with ldap
authentication


On Thu, Jun 7, 2018 at 1:54 PM, Amos Jeffries <squid3 at treenet.co.nz> wrote:

> On 07/06/18 22:24, Panagiotis Bariamis wrote:
> > Hello,
> > I just finished a same project . Only platform that worked well with
> > most configurations of Squid is Web Polygraph.
> >
> >
> > On Thu, Jun 7, 2018, 13:17 Stoica Bogdan Alexandru wrote:
> >
> >     Hi all,____
> >
> >     __ __
> >
> >     We?re a small research team interested in benchmarking Squid for a
> >     research project. ____
> >
> >     Ideally, we would like to have good code coverage while doing so. We
> >     have searched online for alternatives, but found little info. ____
> >
> >     Are there any good benchmarks used for such purpose? Or, even
> >     better, is there a more comprehensive test suite apart from the one
> >     Squid comes with?____
> >
> >     Any suggestions are highly appreciated.____
> >
>
>
> Any chance the details of these research are going to be publicly
> available?
>
> It has been a while since anyone contributed to the Squid Projects
> benchmark collection
> <https://wiki.squid-cache.org/KnowledgeBase/Benchmarks>.
>
>
> Amos
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20180607/5cc6b0ec/attachment.htm>

From Edward.Cheadle at cambiahealth.com  Thu Jun  7 16:07:39 2018
From: Edward.Cheadle at cambiahealth.com (Cheadle, Edward)
Date: Thu, 7 Jun 2018 16:07:39 +0000
Subject: [squid-users] Squid.out reports errors for lines that do not
	exist in squid.conf
Message-ID: <3A7CAB16-AA10-4062-BC0A-FE9E2638E985@regence.com>

Thanks a lot for all the information; It is helping.   

But the service command is not a systemd command.  On servers with systemd on them, the service command is some sort of alias.
This version of Amazon linux does not use systemd, I got on one and ran systemctl and all that is returned is command not found. (I wanted to prove it to myself, amazon linux is new to me.)

I don't remember the history of the service command, but redhat had it in RHEL5 and maybe even before.   It was used instead of running
commands like /etc/init.d/squid restart.  Like systemd it made managing services easier.

The other suggestions people have made are useful, I haven't tracked down the issue, but my approach will be to get all of the servers reporting
no errors on boot, then after I get them all settled, I will upgrade my development servers with the latest version and move forward.  We are beginning 
the move to next version of AWS linux, so it is time to make the transition.   

Thanks for all of the suggestions. I have the Packt book and am beginning to read the documentation on the website, but this list has been valuable to speed up my learning.
    
    Message: 1
    Date: Thu, 7 Jun 2018 14:39:16 +1200
    From: Amos Jeffries <squid3 at treenet.co.nz>
    To: squid-users at lists.squid-cache.org
    Subject: Re: [squid-users] Squid.out reports errors for lines that do
            not exist in squid.conf
    Message-ID: <08cd358c-2209-7628-beeb-82456e6440cf at treenet.co.nz>
    Content-Type: text/plain; charset=utf-8
    
    On 07/06/18 11:11, Cheadle, Edward wrote:
    > Squid version 3.5.27-1.el6
    >
    ...
    >
    > The FATAL error messages below  is because I added a line with dstdomain
    > in it when there were already an entry with dstdom_regex in it.
    >
    > I assume that is the ?type? that is talked about.
    >
    
    Correct.
    
    >
    > 2018/06/06 20:49:29| aclParseAclLine: ACL 'aws_s3' already exists with
    > different type.
    >
    > FATAL: Bungled /etc/squid/squid.conf line 254: acl aws_s3   dstdomain -n
    > .s3.amazonaws.com
    >
    >
    >
    > The issue is that I removed line 254 and I removed both of the lines
    > above and yet I restart squid and continue to get these errors.
    >
    
    Exact same line and issue? or other ones elsewhere in the config?
    
    NP: "squid -k parse" should be used to find any issues after an upgrade.
    It does not halt on the first FATAL/ERROR if there are many.
    
    
    >
    > Does squid compile the rules and keep them somewhere, and I need to do
    > something else than service squid restart to get rid of rules?
    
    By "service squid restart" I take it that you are using systemd to
    control a Squid-3 proxy. systemd cannot cope at all well with software
    like Squid which is itself a daemon manager.
    
    Try stopping Squid with the "squid -k shutdown" command (repeat of
    necessary) and making sure Squid is fully stopped with no processes
    still running before you start it again.
    
    With systemd the "squid -k ..." commands (or init.d script, if any)
    should be used to manage Squid-3 instead of systemd's "service ..."
    commands.
    
    NP: these issues have been resolved in Squid-4. So this is a temporary
    situation until you can upgrade.
    
    Amos
    
    
    ------------------------------
    
    Message: 2
    Date: Thu, 7 Jun 2018 09:13:59 +0300
    From: "Eliezer Croitoru" <eliezer at ngtech.co.il>
    To: <squid-users at lists.squid-cache.org>
    Subject: Re: [squid-users] Squid.out reports errors for lines that do
            not     exist in squid.conf
    Message-ID: <045101d3fe26$bc3f3e30$34bdba90$@ngtech.co.il>
    Content-Type: text/plain;       charset="UTF-8"
    
    Amos,
    
    Systemd can be define to run a specific command for a "reload" and even if nobody wrote the line in a service file it's there since almost day one of systemd services.
    
    And.. if the version is el6 I believe it's still a sysVinit based system.
    Squid -kparse should detect and squid -kreconf should resolve any issue if it's not a fatal one that stopped the service.
    
    Not directly related but.. only if Squid doesn't release at all any memory it catches then a restart would be a must at some point.
    >From what I have seen in the 2.7 and 3.x code in the past it seems that there should be some level of memory cleanup\release.
    Also I have systems that has up-time of almost a year so I am a bit confused why should a restart would be requied?
    ... if I have acls handled by an external acl or ICAP service then why should I restart?
    
    Eliezer
    
    ----
    Eliezer Croitoru
    Linux System Administrator
    Mobile: +972-5-28704261
    Email: eliezer at ngtech.co.il
    
    
    
    -----Original Message-----
    From: squid-users <squid-users-bounces at lists.squid-cache.org> On Behalf Of Amos Jeffries
    Sent: Thursday, June 7, 2018 05:39
    To: squid-users at lists.squid-cache.org
    Subject: Re: [squid-users] Squid.out reports errors for lines that do not exist in squid.conf
    
    On 07/06/18 11:11, Cheadle, Edward wrote:
    > Squid version 3.5.27-1.el6
    >
    ...
    >
    > The FATAL error messages below  is because I added a line with dstdomain
    > in it when there were already an entry with dstdom_regex in it.
    >
    > I assume that is the ?type? that is talked about.
    >
    
    Correct.
    
    >
    > 2018/06/06 20:49:29| aclParseAclLine: ACL 'aws_s3' already exists with
    > different type.
    >
    > FATAL: Bungled /etc/squid/squid.conf line 254: acl aws_s3   dstdomain -n
    > .s3.amazonaws.com
    >
    >
    >
    > The issue is that I removed line 254 and I removed both of the lines
    > above and yet I restart squid and continue to get these errors.
    >
    
    Exact same line and issue? or other ones elsewhere in the config?
    
    NP: "squid -k parse" should be used to find any issues after an upgrade.
    It does not halt on the first FATAL/ERROR if there are many.
    
    
    >
    > Does squid compile the rules and keep them somewhere, and I need to do
    > something else than service squid restart to get rid of rules?
    
    By "service squid restart" I take it that you are using systemd to
    control a Squid-3 proxy. systemd cannot cope at all well with software
    like Squid which is itself a daemon manager.
    
    Try stopping Squid with the "squid -k shutdown" command (repeat of
    necessary) and making sure Squid is fully stopped with no processes
    still running before you start it again.
    
    With systemd the "squid -k ..." commands (or init.d script, if any)
    should be used to manage Squid-3 instead of systemd's "service ..."
    commands.
    
    NP: these issues have been resolved in Squid-4. So this is a temporary
    situation until you can upgrade.
    
    Amos
    _______________________________________________
    squid-users mailing list
    squid-users at lists.squid-cache.org
    https://na01.safelinks.protection.outlook.com/?url=http%3A%2F%2Flists.squid-cache.org%2Flistinfo%2Fsquid-users&data=02%7C01%7CEdward.Cheadle%40cambiahealth.com%7Cd08b3294d08e4355c2cb08d5cc60e485%7Ce964274919d44f7fb4df802b2b75a809%7C0%7C0%7C636639638878569750&sdata=SkhtUK%2BBouyVYnUFuI1LRXThdMEXMYUHxy%2F9iTBThhA%3D&reserved=0
    
    
    
    ------------------------------
    
    Message: 3
    Date: Thu, 7 Jun 2018 22:11:51 +1200
    From: Amos Jeffries <squid3 at treenet.co.nz>
    To: Eliezer Croitoru <eliezer at ngtech.co.il>,
            squid-users at lists.squid-cache.org
    Subject: Re: [squid-users] Squid.out reports errors for lines that do
            not exist in squid.conf
    Message-ID: <9dc90778-b817-7dad-c96d-fc3a621fd51f at treenet.co.nz>
    Content-Type: text/plain; charset=utf-8
    
    On 07/06/18 18:13, Eliezer Croitoru wrote:
    > Amos,
    >
    > Systemd can be define to run a specific command for a "reload" and even if nobody wrote the line in a service file it's there since almost day one of systemd services.
    >
    
    *If* that mechanism is used there is no difference in the commands. If
    it is not used, the systemd ones are actively dangerous. So no harm in
    advising the safe one be used in either case.
    
    
    > And.. if the version is el6 I believe it's still a sysVinit based system.
    
    Cheadle was using systemd's "service ..." commands. Which I am advising
    to avoid because something indeterminate is going wrong with the config
    loading and startup process. If the OS is actually SysV those systemd
    commands are even more inappropriate.
    
    
    > Squid -kparse should detect and squid -kreconf should resolve any issue if it's not a fatal one that stopped the service.
    >
    > Not directly related but.. only if Squid doesn't release at all any memory it catches then a restart would be a must at some point.
    > From what I have seen in the 2.7 and 3.x code in the past it seems that there should be some level of memory cleanup\release.
    > Also I have systems that has up-time of almost a year so I am a bit confused why should a restart would be requied?
    
    The admin has apparently got themselves into a difficult situation and
    it is no longer clear whether systemd or Squid master process is in
    control of the worker processes which are running and with what config.
    They both fight over "service ..." commands.
    
    The only thing which is guaranteed to restore Squid to known state is a
    full shutdown. Ensuring that everything has exited before proceeding
    with the start action.
    
    Amos
    
    
    ------------------------------
    
    Message: 4
    Date: Thu, 7 Jun 2018 10:17:30 +0000
    From: Stoica Bogdan Alexandru <bogdan.stoica at epfl.ch>
    To: "squid-users at lists.squid-cache.org"
            <squid-users at lists.squid-cache.org>
    Subject: [squid-users] Squid test-suite / benchmarks
    Message-ID: <ecb7106901294fbf9060c47a58ec91cf at rexe.intranet.epfl.ch>
    Content-Type: text/plain; charset="utf-8"
    
    Hi all,
    
    We're a small research team interested in benchmarking Squid for a research project.
    Ideally, we would like to have good code coverage while doing so. We have searched online for alternatives, but found little info.
    Are there any good benchmarks used for such purpose? Or, even better, is there a more comprehensive test suite apart from the one Squid comes with?
    Any suggestions are highly appreciated.
    
    Thanks a lot!
    
    B.
    -------------- next part --------------
    An HTML attachment was scrubbed...
    URL: <https://na01.safelinks.protection.outlook.com/?url=http%3A%2F%2Flists.squid-cache.org%2Fpipermail%2Fsquid-users%2Fattachments%2F20180607%2F9b07a907%2Fattachment-0001.html&data=02%7C01%7CEdward.Cheadle%40cambiahealth.com%7Cd08b3294d08e4355c2cb08d5cc60e485%7Ce964274919d44f7fb4df802b2b75a809%7C0%7C0%7C636639638878569750&sdata=nmB4gzsSbSFTqXq%2FRGClqy9ZGas3gQPurqoDxTdoqIU%3D&reserved=0>
    
    ------------------------------
    
    Message: 5
    Date: Thu, 7 Jun 2018 13:24:21 +0300
    From: Panagiotis Bariamis <akismpa at gmail.com>
    To: Stoica Bogdan Alexandru <bogdan.stoica at epfl.ch>
    Cc: squid-users at lists.squid-cache.org
    Subject: Re: [squid-users] Squid test-suite / benchmarks
    Message-ID:
            <CAPxN_PVGN7FzmO5YEbpQB9ZDB3reS5mQQ2SmSw6HnmCK0dYPKA at mail.gmail.com>
    Content-Type: text/plain; charset="utf-8"
    
    Hello,
    I just finished a same project . Only platform that worked well with most
    configurations of Squid is Web Polygraph.
    
    
    On Thu, Jun 7, 2018, 13:17 Stoica Bogdan Alexandru <bogdan.stoica at epfl.ch>
    wrote:
    
    > Hi all,
    >
    >
    >
    > We?re a small research team interested in benchmarking Squid for a
    > research project.
    >
    > Ideally, we would like to have good code coverage while doing so. We have
    > searched online for alternatives, but found little info.
    >
    > Are there any good benchmarks used for such purpose? Or, even better, is
    > there a more comprehensive test suite apart from the one Squid comes with?
    >
    > Any suggestions are highly appreciated.
    >
    >
    >
    > Thanks a lot!
    >
    >
    >
    > B.
    > _______________________________________________
    > squid-users mailing list
    > squid-users at lists.squid-cache.org
    > https://na01.safelinks.protection.outlook.com/?url=http%3A%2F%2Flists.squid-cache.org%2Flistinfo%2Fsquid-users&data=02%7C01%7CEdward.Cheadle%40cambiahealth.com%7Cd08b3294d08e4355c2cb08d5cc60e485%7Ce964274919d44f7fb4df802b2b75a809%7C0%7C0%7C636639638878569750&sdata=SkhtUK%2BBouyVYnUFuI1LRXThdMEXMYUHxy%2F9iTBThhA%3D&reserved=0
    >
    -------------- next part --------------
    An HTML attachment was scrubbed...
    URL: <https://na01.safelinks.protection.outlook.com/?url=http%3A%2F%2Flists.squid-cache.org%2Fpipermail%2Fsquid-users%2Fattachments%2F20180607%2Ff3a24760%2Fattachment.html&data=02%7C01%7CEdward.Cheadle%40cambiahealth.com%7Cd08b3294d08e4355c2cb08d5cc60e485%7Ce964274919d44f7fb4df802b2b75a809%7C0%7C0%7C636639638878569750&sdata=V9hHJL6HD6%2BYshdosCVxiUe50JeIlVaRTbkXDe17iIY%3D&reserved=0>
    
    ------------------------------
    
    Subject: Digest Footer
    
    _______________________________________________
    squid-users mailing list
    squid-users at lists.squid-cache.org
    https://na01.safelinks.protection.outlook.com/?url=http%3A%2F%2Flists.squid-cache.org%2Flistinfo%2Fsquid-users&data=02%7C01%7CEdward.Cheadle%40cambiahealth.com%7Cd08b3294d08e4355c2cb08d5cc60e485%7Ce964274919d44f7fb4df802b2b75a809%7C0%7C0%7C636639638878569750&sdata=SkhtUK%2BBouyVYnUFuI1LRXThdMEXMYUHxy%2F9iTBThhA%3D&reserved=0
    
    
    ------------------------------
    
    End of squid-users Digest, Vol 46, Issue 10
    *******************************************
    Ensure a sustainable future - only print when necessary.
    


IMPORTANT NOTICE: This communication, including any attachment, contains information that may be confidential or privileged, and is intended solely for the entity or individual to whom it is addressed.  If you are not the intended recipient, you should delete this message and are hereby notified that any disclosure, copying, or distribution of this message is strictly prohibited.  Nothing in this email, including any attachment, is intended to be a legally binding signature.

From eliezer at ngtech.co.il  Thu Jun  7 17:03:36 2018
From: eliezer at ngtech.co.il (Eliezer Croitoru)
Date: Thu, 7 Jun 2018 20:03:36 +0300
Subject: [squid-users] Squid.out reports errors for lines that do not
	exist in squid.conf
In-Reply-To: <9dc90778-b817-7dad-c96d-fc3a621fd51f@treenet.co.nz>
References: <5B3F54FC-DE6E-40F5-B08A-4873B5BA258A@regence.com>
 <08cd358c-2209-7628-beeb-82456e6440cf@treenet.co.nz>
 <045101d3fe26$bc3f3e30$34bdba90$@ngtech.co.il>
 <9dc90778-b817-7dad-c96d-fc3a621fd51f@treenet.co.nz>
Message-ID: <01fc01d3fe81$79a7e940$6cf7bbc0$@ngtech.co.il>

Thanks Amos,

Now it's much clear to me.
Since you know me I have used many different Unix based systems and the only times I have used the service commands was really un-naturally....
The times I have used service was only when some guide/tutorial instructed a set of commands.
I am using either squid directly or the default OS documented commands.

I hope to be able to test 4.0.24 again (I tested it couple times but not for more then a week).

Eliezer

----
Eliezer Croitoru
Linux System Administrator
Mobile: +972-5-28704261
Email: eliezer at ngtech.co.il



-----Original Message-----
From: Amos Jeffries <squid3 at treenet.co.nz> 
Sent: Thursday, June 7, 2018 13:12
To: Eliezer Croitoru <eliezer at ngtech.co.il>; squid-users at lists.squid-cache.org
Subject: Re: [squid-users] Squid.out reports errors for lines that do not exist in squid.conf

On 07/06/18 18:13, Eliezer Croitoru wrote:
> Amos,
> 
> Systemd can be define to run a specific command for a "reload" and even if nobody wrote the line in a service file it's there since almost day one of systemd services.
> 

*If* that mechanism is used there is no difference in the commands. If
it is not used, the systemd ones are actively dangerous. So no harm in
advising the safe one be used in either case.


> And.. if the version is el6 I believe it's still a sysVinit based system.

Cheadle was using systemd's "service ..." commands. Which I am advising
to avoid because something indeterminate is going wrong with the config
loading and startup process. If the OS is actually SysV those systemd
commands are even more inappropriate.


> Squid -kparse should detect and squid -kreconf should resolve any issue if it's not a fatal one that stopped the service.
> 
> Not directly related but.. only if Squid doesn't release at all any memory it catches then a restart would be a must at some point.
> From what I have seen in the 2.7 and 3.x code in the past it seems that there should be some level of memory cleanup\release.
> Also I have systems that has up-time of almost a year so I am a bit confused why should a restart would be requied?

The admin has apparently got themselves into a difficult situation and
it is no longer clear whether systemd or Squid master process is in
control of the worker processes which are running and with what config.
They both fight over "service ..." commands.

The only thing which is guaranteed to restore Squid to known state is a
full shutdown. Ensuring that everything has exited before proceeding
with the start action.

Amos



From rousskov at measurement-factory.com  Thu Jun  7 17:20:38 2018
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Thu, 7 Jun 2018 11:20:38 -0600
Subject: [squid-users] Squid test-suite / benchmarks
In-Reply-To: <ecb7106901294fbf9060c47a58ec91cf@rexe.intranet.epfl.ch>
References: <ecb7106901294fbf9060c47a58ec91cf@rexe.intranet.epfl.ch>
Message-ID: <27872f62-8d80-1913-28ec-4b47593f8fa5@measurement-factory.com>

On 06/07/2018 04:17 AM, Stoica Bogdan Alexandru wrote:

> We?re a small research team interested in benchmarking Squid for a
> research project.

> Ideally, we would like to have good code coverage while doing so.

> Are there any good benchmarks used for such purpose?

Performance benchmarks usually focus on things other than code coverage.
It is very difficult to write a quality benchmark for a proxy, even
without code coverage as a goal!

One the other hand, a decent proxy benchmark has enough knobs to tickle
most "interesting" code paths in Squid (or any other proxy). Web
Polygraph[1] (mentioned on this thread earlier) is a good example -- you
can trigger cache revalidation, simulate heavy tailed hit distributions
that stress disk caching, exercise the code that handles aborted
transactions, persistent connection races, etc., etc.


> Or, even better, is
> there a more comprehensive test suite apart from the one Squid comes with?

Squid does not come with a comprehensive test suite (yet) and the tests
distributed with Squid are not performance tests (a.k.a. "benchmarks").
If you are looking for functionality rather than performance testing,
then there is Co-Advisor[2]. Squid is tested with Co-Advisor, but those
tests have not been automated (yet).

  [1] http://www.web-polygraph.org/
  [2] http://coad.measurement-factory.com/


HTH,

Alex.
P.S. Disclaimer: The company I work for is responsible for both of the
test tools mentioned above.


From coenraad at wish.org.za  Thu Jun  7 18:25:55 2018
From: coenraad at wish.org.za (Coenraad Loubser)
Date: Thu, 7 Jun 2018 20:25:55 +0200
Subject: [squid-users] Squid test-suite / benchmarks
In-Reply-To: <27872f62-8d80-1913-28ec-4b47593f8fa5@measurement-factory.com>
References: <ecb7106901294fbf9060c47a58ec91cf@rexe.intranet.epfl.ch>
 <27872f62-8d80-1913-28ec-4b47593f8fa5@measurement-factory.com>
Message-ID: <CADZv+uaEqc_e9dvig=-OadzqzhooPJpeBmrv_BZZuanO9zifoQ@mail.gmail.com>

My first port of call would be apachebench with and without your proxies. A
web search for "squid apachebench" might yield some leads to people who
have done this. (I'm sure apachebench is well tested.)

Eg. the third hit on Google:
https://2bits.com/articles/using-apachebench-benchmarking-logged-users-automated-approach.html

On Thu, Jun 7, 2018 at 7:20 PM, Alex Rousskov <
rousskov at measurement-factory.com> wrote:

> On 06/07/2018 04:17 AM, Stoica Bogdan Alexandru wrote:
>
> > We?re a small research team interested in benchmarking Squid for a
> > research project.
>
> > Ideally, we would like to have good code coverage while doing so.
>
> > Are there any good benchmarks used for such purpose?
>
> Performance benchmarks usually focus on things other than code coverage.
> It is very difficult to write a quality benchmark for a proxy, even
> without code coverage as a goal!
>
> One the other hand, a decent proxy benchmark has enough knobs to tickle
> most "interesting" code paths in Squid (or any other proxy). Web
> Polygraph[1] (mentioned on this thread earlier) is a good example -- you
> can trigger cache revalidation, simulate heavy tailed hit distributions
> that stress disk caching, exercise the code that handles aborted
> transactions, persistent connection races, etc., etc.
>
>
> > Or, even better, is
> > there a more comprehensive test suite apart from the one Squid comes
> with?
>
> Squid does not come with a comprehensive test suite (yet) and the tests
> distributed with Squid are not performance tests (a.k.a. "benchmarks").
> If you are looking for functionality rather than performance testing,
> then there is Co-Advisor[2]. Squid is tested with Co-Advisor, but those
> tests have not been automated (yet).
>
>   [1] http://www.web-polygraph.org/
>   [2] http://coad.measurement-factory.com/
>
>
> HTH,
>
> Alex.
> P.S. Disclaimer: The company I work for is responsible for both of the
> test tools mentioned above.
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>



-- 

Coenraad Loubser

Wireless Internet Services & Hardware (Pty) Ltd.
210 Long Street, Cape Town, 8001, ZA

Office: +27 21 481 1824
Skype: Coenraad_Loubser
Email: coenraad at wish.org.za
Cell: +27 73 772 1223

-- Spending Money is like watering a plant.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20180607/ee323621/attachment.htm>

From jlay at slave-tothe-box.net  Fri Jun  8 14:33:14 2018
From: jlay at slave-tothe-box.net (James Lay)
Date: Fri, 08 Jun 2018 08:33:14 -0600
Subject: [squid-users] About to upgrade from 3 to 4
Message-ID: <c36fa63222d2e5b5cfc858b3aba1cc02d04b6d78.camel@slave-tothe-box.net>

Hey all!

Topic says it....I'm starting to look at doing an upgrade from 3 to 4. 
Any glaring surprises?  Doing a transparent forward proxy with some
peek/splice for content filtering only (no decryption).  Has anyone
gone through an upgrade, and how painful was it, if at all?  Thank you.

James
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20180608/3e2c53fc/attachment.htm>

From tiraen at gmail.com  Fri Jun  8 14:56:31 2018
From: tiraen at gmail.com (Tiraen)
Date: Fri, 8 Jun 2018 17:56:31 +0300
Subject: [squid-users] Question about traffic calculate
In-Reply-To: <CANhj9oywS173VafQhNa1S4_qznc-hmadoOiSRLH1HEPef_ORdQ@mail.gmail.com>
References: <CANhj9oyRn5GvSDjUa405Tz90X8kZY4mEoWPv341M1eUFbr2WfQ@mail.gmail.com>
 <4dd5dbf7-661a-3378-33be-a7a9da2912cf@treenet.co.nz>
 <CANhj9oyvYkuxk5pWTT=FPFggZ2kmcFa+Gu3yUXvNiURmouCxmg@mail.gmail.com>
 <fa5edf17-c0ed-4ac1-c5fe-b77839752dfc@treenet.co.nz>
 <CANhj9oywS173VafQhNa1S4_qznc-hmadoOiSRLH1HEPef_ORdQ@mail.gmail.com>
Message-ID: <CANhj9ozSz6k0HMFYHUQLYw473j-NP8Vau=j7-akephH-AGO+ow@mail.gmail.com>

Small clarification

If the normal behavior of the proxy server described above is correct, then
maybe there are other methods of gathering information on traffic in online
mode?

Perhaps there are other solutions besides log parsing?

2018-06-06 12:21 GMT+03:00 Tiraen <tiraen at gmail.com>:

> >If you are using SSL-Bump features, please consider Squid-4 instead
>
> It is not used at all.Squid does not work with ssl. Frontend only
>
>
> Concerning incorrectly specified options at build
>
> Here on this squid happens the same thing:
>
> * squid3 -v*
> *Squid Cache: Version 3.4.8*
> * linux*
> *configure options:  '--build=x86_64-linux-gnu' '--prefix=/usr'
> '--includedir=${prefix}/include' '--mandir=${prefix}/share/man'
> '--infodir=${prefix}/share/info' '--sysconfdir=/etc' '--localstatedir=/var'
> '--libexecdir=${prefix}/lib/squid3' '--srcdir=.'
> '--disable-maintainer-mode' '--disable-dependency-tracking'
> '--disable-silent-rules' '--datadir=/usr/share/squid3'
> '--sysconfdir=/etc/squid3' '--mandir=/usr/share/man' '--enable-inline'
> '--disable-arch-native' '--enable-async-io=8'
> '--enable-storeio=ufs,aufs,diskd,rock' '--enable-removal-policies=lru,heap'
> '--enable-delay-pools' '--enable-cache-digests' '--enable-icap-client'
> '--enable-follow-x-forwarded-for'
> '--enable-auth-basic=DB,fake,getpwnam,LDAP,MSNT,MSNT-multi-domain,NCSA,NIS,PAM,POP3,RADIUS,SASL,SMB'
> '--enable-auth-digest=file,LDAP' '--enable-auth-negotiate=kerberos,wrapper'
> '--enable-auth-ntlm=fake,smb_lm'
> '--enable-external-acl-helpers=file_userip,kerberos_ldap_group,LDAP_group,session,SQL_session,unix_group,wbinfo_group'
> '--enable-url-rewrite-helpers=fake' '--enable-eui' '--enable-esi'
> '--enable-icmp' '--enable-zph-qos' '--enable-ecap' '--disable-translation'
> '--with-swapdir=/var/spool/squid3' '--with-logdir=/var/log/squid3'
> '--with-pidfile=/var/run/squid3.pid' '--with-filedescriptors=65536'
> '--with-large-files' '--with-default-user=proxy' '--enable-build-info=
> linux' '--enable-linux-netfilter' 'build_alias=x86_64-linux-gnu' 'CFLAGS=-g
> -O2 -fPIE -fstack-protector-strong -Wformat -Werror=format-security -Wall'
> 'LDFLAGS=-fPIE -pie -Wl,-z,relro -Wl,-z,now' 'CPPFLAGS=-D_FORTIFY_SOURCE=2'
> 'CXXFLAGS=-g -O2 -fPIE -fstack-protector-strong -Wformat
> -Werror=format-security'*
>
>
> *no out data*
>
>
> *Connection: 0x7f18ee951c58*
> * FD 15, read 10070, wrote 19018*
> * FD desc: Reading next request*
> * in: buf 0x7f18ee952070, offset 0, size 4096*
> * remote: 127.0.0.1:52827 <http://127.0.0.1:52827>*
> * local: 127.0.0.1:8080 <http://127.0.0.1:8080>*
> * nrequests: 38*
> *uri http://icanhazip.com/ <http://icanhazip.com/>*
> *logType TCP_MISS*
> *out.offset 0, out.size 0*
> *req_sz 265*
> *entry 0x7f18ee3bc740/F9929050DEE6E67D2DF51EDCBC0CB80F*
> *start 1528276856.390709 (2.640371 seconds ago)*
> *username*
> *delay_pool 0*
>
> *Connection: 0x7f18ee874168*
> * FD 13, read 10070, wrote 19018*
> * FD desc: Reading next request*
> * in: buf 0x7f18ee86bb60, offset 0, size 4096*
> * remote: 127.0.0.1:52825 <http://127.0.0.1:52825>*
> * local: 127.0.0.1:8080 <http://127.0.0.1:8080>*
> * nrequests: 38*
> *uri http://icanhazip.com/ <http://icanhazip.com/>*
> *logType TCP_MISS*
> *out.offset 0, out.size 0*
> *req_sz 265*
> *entry 0x7f18ee87fde0/560E3AC236A180ECB815B5B41527D2BA*
> *start 1528276856.368609 (2.662471 seconds ago)*
> *username*
>
>
> *delay_pool 0*
>
>
>
> 2018-06-06 7:51 GMT+03:00 Amos Jeffries <squid3 at treenet.co.nz>:
>
>> On 06/06/18 07:12, Tiraen wrote:
>> > /The second transaction has not yet reached that state despite 81017sec
>> > having past.
>> > /
>> > Thank you for clarification.
>> >
>> > About squid version
>> >
>> > /squid -v/
>> > /Squid Cache: Version 3.5.27/
>> ...
>>
>> If you are using SSL-Bump features, please consider Squid-4 instead. The
>> strangely long timeouts on transactions is likely to be a side effect of
>> on old behaviour in Squid-3 seen with transactions that were bumped.
>>
>>
>> > '--enable-ssl'
>> > '--with-open-ssl=/etc/ssl/openssl.cnf'
>>
>> Two problems with the above:
>>
>>  1) the option name is "--with-openssl".
>>
>>  2) that option takes the directory PATH where the OpenSSL development
>> files were installed. If using the OS provided library package *omit*
>> the =PATH portion.
>>
>>
>> Amos
>>
>
>
>
> --
> With best regards,
>
> Vyacheslav Yakushev,
>
> Unix system administrator
>
> https://t.me/kelewind
>



-- 
With best regards,

Vyacheslav Yakushev,

Unix system administrator

https://t.me/kelewind
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20180608/7927f34d/attachment.htm>

From squid3 at treenet.co.nz  Fri Jun  8 15:04:30 2018
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sat, 9 Jun 2018 03:04:30 +1200
Subject: [squid-users] About to upgrade from 3 to 4
In-Reply-To: <c36fa63222d2e5b5cfc858b3aba1cc02d04b6d78.camel@slave-tothe-box.net>
References: <c36fa63222d2e5b5cfc858b3aba1cc02d04b6d78.camel@slave-tothe-box.net>
Message-ID: <0fb5d332-01a5-6065-0b2c-250329e85bcb@treenet.co.nz>

On 09/06/18 02:33, James Lay wrote:
> Hey all!
> 
> Topic says it....I'm starting to look at doing an upgrade from 3 to 4.
> Any glaring surprises? Doing a transparent forward proxy with some
> peek/splice for content filtering only (no decryption). Has anyone gone
> through an upgrade, and how painful was it, if at all? Thank you.
> 

Which 3.x you are starting from is the issue.

>From 3.5 to 4 should be the same as any of the 3.x single version bumps.
There is nothing special about v4 from a user perspective.

Amos


From jlay at slave-tothe-box.net  Fri Jun  8 15:36:18 2018
From: jlay at slave-tothe-box.net (James Lay)
Date: Fri, 08 Jun 2018 09:36:18 -0600
Subject: [squid-users] About to upgrade from 3 to 4
In-Reply-To: <0fb5d332-01a5-6065-0b2c-250329e85bcb@treenet.co.nz>
References: <c36fa63222d2e5b5cfc858b3aba1cc02d04b6d78.camel@slave-tothe-box.net>
 <0fb5d332-01a5-6065-0b2c-250329e85bcb@treenet.co.nz>
Message-ID: <6616d61ae837620247da660cdfe38380f9f280fa.camel@slave-tothe-box.net>

On Sat, 2018-06-09 at 03:04 +1200, Amos Jeffries wrote:
> On 09/06/18 02:33, James Lay wrote:
> Hey all!
> Topic says it....I'm starting to look at doing an upgrade from 3 to
> 4.Any glaring surprises? Doing a transparent forward proxy with
> somepeek/splice for content filtering only (no decryption). Has
> anyone gonethrough an upgrade, and how painful was it, if at all?
> Thank you.
> 
> Which 3.x you are starting from is the issue.
> From 3.5 to 4 should be the same as any of the 3.x single version
> bumps.There is nothing special about v4 from a user perspective.
> Amos_______________________________________________squid-users
> mailing listsquid-users at lists.squid-cache.orghttp://lists.squid-cache
> .org/listinfo/squid-users
> 

Thanks Amos...I'm going from 3.5.2 ?

James
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20180608/b7d810ab/attachment.htm>

From squid3 at treenet.co.nz  Fri Jun  8 16:29:15 2018
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sat, 9 Jun 2018 04:29:15 +1200
Subject: [squid-users] Question about traffic calculate
In-Reply-To: <CANhj9ozSz6k0HMFYHUQLYw473j-NP8Vau=j7-akephH-AGO+ow@mail.gmail.com>
References: <CANhj9oyRn5GvSDjUa405Tz90X8kZY4mEoWPv341M1eUFbr2WfQ@mail.gmail.com>
 <4dd5dbf7-661a-3378-33be-a7a9da2912cf@treenet.co.nz>
 <CANhj9oyvYkuxk5pWTT=FPFggZ2kmcFa+Gu3yUXvNiURmouCxmg@mail.gmail.com>
 <fa5edf17-c0ed-4ac1-c5fe-b77839752dfc@treenet.co.nz>
 <CANhj9oywS173VafQhNa1S4_qznc-hmadoOiSRLH1HEPef_ORdQ@mail.gmail.com>
 <CANhj9ozSz6k0HMFYHUQLYw473j-NP8Vau=j7-akephH-AGO+ow@mail.gmail.com>
Message-ID: <af7646cf-0fe6-6194-16d7-3fcb3f4622e1@treenet.co.nz>

On 09/06/18 02:56, Tiraen wrote:
> Small clarification
> 
> If the normal behavior of the proxy server described above is correct,
> then maybe there are other methods of gathering information on traffic
> in online mode?

What is "online mode" ?

> 
> Perhaps there are other solutions besides log parsing?

What information are you trying to get exactly?


Amos


From acrow at integrafin.co.uk  Fri Jun  8 17:04:52 2018
From: acrow at integrafin.co.uk (Alex Crow)
Date: Fri, 8 Jun 2018 18:04:52 +0100
Subject: [squid-users] Question about traffic calculate
In-Reply-To: <af7646cf-0fe6-6194-16d7-3fcb3f4622e1@treenet.co.nz>
References: <CANhj9oyRn5GvSDjUa405Tz90X8kZY4mEoWPv341M1eUFbr2WfQ@mail.gmail.com>
 <4dd5dbf7-661a-3378-33be-a7a9da2912cf@treenet.co.nz>
 <CANhj9oyvYkuxk5pWTT=FPFggZ2kmcFa+Gu3yUXvNiURmouCxmg@mail.gmail.com>
 <fa5edf17-c0ed-4ac1-c5fe-b77839752dfc@treenet.co.nz>
 <CANhj9oywS173VafQhNa1S4_qznc-hmadoOiSRLH1HEPef_ORdQ@mail.gmail.com>
 <CANhj9ozSz6k0HMFYHUQLYw473j-NP8Vau=j7-akephH-AGO+ow@mail.gmail.com>
 <af7646cf-0fe6-6194-16d7-3fcb3f4622e1@treenet.co.nz>
Message-ID: <df41334f-0abf-2b10-2ae1-047325461c09@integrafin.co.uk>



On 08/06/18 17:29, Amos Jeffries wrote:
> On 09/06/18 02:56, Tiraen wrote:
>> Small clarification
>>
>> If the normal behavior of the proxy server described above is correct,
>> then maybe there are other methods of gathering information on traffic
>> in online mode?
> What is "online mode" ?

SNMP is built in to squid. You can use it in conjunction with net-snmp 
proxy mode to gather far more granular performance/caching/response 
time/per-ip stats than squidclient or logs if that's what you're after.


--
This message is intended only for the addressee and may contain
confidential information. Unless you are that person, you may not
disclose its contents or use it in any way and are requested to delete
the message along with any attachments and notify us immediately.
This email is not intended to, nor should it be taken to, constitute advice.
The information provided is correct to our knowledge & belief and must not
be used as a substitute for obtaining tax, regulatory, investment, legal or
any other appropriate advice.

"Transact" is operated by Integrated Financial Arrangements Ltd.
29 Clement's Lane, London EC4N 7AE. Tel: (020) 7608 4900 Fax: (020) 7608 5300.
(Registered office: as above; Registered in England and Wales under
number: 3727592). Authorised and regulated by the Financial Conduct
Authority (entered on the Financial Services Register; no. 190856).


From tiraen at gmail.com  Fri Jun  8 19:55:46 2018
From: tiraen at gmail.com (Tiraen)
Date: Fri, 8 Jun 2018 22:55:46 +0300
Subject: [squid-users] Question about traffic calculate
In-Reply-To: <df41334f-0abf-2b10-2ae1-047325461c09@integrafin.co.uk>
References: <CANhj9oyRn5GvSDjUa405Tz90X8kZY4mEoWPv341M1eUFbr2WfQ@mail.gmail.com>
 <4dd5dbf7-661a-3378-33be-a7a9da2912cf@treenet.co.nz>
 <CANhj9oyvYkuxk5pWTT=FPFggZ2kmcFa+Gu3yUXvNiURmouCxmg@mail.gmail.com>
 <fa5edf17-c0ed-4ac1-c5fe-b77839752dfc@treenet.co.nz>
 <CANhj9oywS173VafQhNa1S4_qznc-hmadoOiSRLH1HEPef_ORdQ@mail.gmail.com>
 <CANhj9ozSz6k0HMFYHUQLYw473j-NP8Vau=j7-akephH-AGO+ow@mail.gmail.com>
 <af7646cf-0fe6-6194-16d7-3fcb3f4622e1@treenet.co.nz>
 <df41334f-0abf-2b10-2ae1-047325461c09@integrafin.co.uk>
Message-ID: <CANhj9ozzyfO8m12R5BTRRhq5K+6ojaNFzNSapV_jfPifnwFZOQ@mail.gmail.com>

*What is "online mode" ?> > Perhaps there are other solutions besides log
parsing?What information are you trying to get exactly? *

There are actual data on incoming / outgoing traffic per user (when
autorization by login is on)

In principle, all the data is, except for the thing that I wrote above -
and I do not understand whether it is possible through this method to get
the actual



*SNMP is built in to squid.  *
Can I get data on traffic by users? If it's not difficult to give a link to
a piece of documentation ?


2018-06-08 20:04 GMT+03:00 Alex Crow <acrow at integrafin.co.uk>:

>
>
> On 08/06/18 17:29, Amos Jeffries wrote:
>
>> On 09/06/18 02:56, Tiraen wrote:
>>
>>> Small clarification
>>>
>>> If the normal behavior of the proxy server described above is correct,
>>> then maybe there are other methods of gathering information on traffic
>>> in online mode?
>>>
>> What is "online mode" ?
>>
>
> SNMP is built in to squid. You can use it in conjunction with net-snmp
> proxy mode to gather far more granular performance/caching/response
> time/per-ip stats than squidclient or logs if that's what you're after.
>
>
> --
> This message is intended only for the addressee and may contain
> confidential information. Unless you are that person, you may not
> disclose its contents or use it in any way and are requested to delete
> the message along with any attachments and notify us immediately.
> This email is not intended to, nor should it be taken to, constitute
> advice.
> The information provided is correct to our knowledge & belief and must not
> be used as a substitute for obtaining tax, regulatory, investment, legal or
> any other appropriate advice.
>
> "Transact" is operated by Integrated Financial Arrangements Ltd.
> 29 Clement's Lane, London EC4N 7AE. Tel: (020) 7608 4900 Fax: (020) 7608
> 5300.
> (Registered office: as above; Registered in England and Wales under
> number: 3727592). Authorised and regulated by the Financial Conduct
> Authority (entered on the Financial Services Register; no. 190856).
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>



-- 
With best regards,

Vyacheslav Yakushev,

Unix system administrator

https://t.me/kelewind
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20180608/4aa0bef5/attachment.htm>

From vh1988 at yahoo.com.ar  Fri Jun  8 23:15:31 2018
From: vh1988 at yahoo.com.ar (Julian Perconti)
Date: Fri, 8 Jun 2018 20:15:31 -0300
Subject: [squid-users] SSL errors with Squid 3.5.27
Message-ID: <010201d3ff7e$9b259510$d170bf30$@yahoo.com.ar>

Hello community, I am new to the list and, I hope everyone is well.

I have running a squid server on debian 7.

My squid version is 3.5.27 manually compiled with LibreSSL 2.6.0 due to
problems with Dropbox. After compiling squid with LibreSSL, the error
"unknown cipher returned" has disappeared and dropbox worked correctly.

Everything works quite well, except that in /var/log/squid/cache.log there
are 5 types of problems (at least):

[1] 2018/06/08 17:14:05 kid1| Error negotiating SSL connection on FD 7:
error:14037418:SSL routines:ACCEPT_SR_KEY_EXCH:tlsv1 alert unknown ca (1/0)
[2] 2018/06/08 17:14:39 kid1| Error negotiating SSL on FD 11:
error:14007086:SSL routines:CONNECT_CR_CERT:certificate verify failed
(1/-1/0)
[3] 2018/06/08 18:35:43 kid1| Error negotiating SSL connection on FD 10:
(104) Connection reset by peer
[4] 2018/06/08 18:56:52 kid1| Error negotiating SSL on FD 13:
error:00000000:lib(0):func(0):reason(0) (5/-1/104)
[5] 2018/06/08 19:20:06 kid1| Error negotiating SSL connection on FD 9:
error:06FFF064:digital envelope routines:CRYPTO_internal:bad decrypt (1/-1)

However I think (I'm not sure but ...), that the most serious is the number
[2]:
SSL negotiating error on FD 11: error: 14007086: SSL routines:
CONNECT_CR_CERT:certificate verify failed (1/-1/0)

The problem I have it with WhatsApp from mobile devices ... the application
tries to connect to the network indefinitely without success, and the error
that appears (at that moment) is [2]: (...) certificate verify failed
(1/-1/0)

This is the most relevant configuration of squid currently:

http_port 3128

http_port 3129 intercept

https_port 3130 intercept ssl-bump \
  cert=/etc/squid/ssl_cert/squidCA.pem \
  key=/etc/squid/ssl_cert/squidCA.pem \
  generate-host-certificates=on dynamic_cert_mem_cache_size=4MB
tls-dh=/etc/squid/ssl_cert/dhparam.pem

sslcrtd_program /lib/squid/ssl_crtd -s /var/lib/ssl_db -M 4MB
sslproxy_cafile /etc/squid/ssl_cert/cert.pem # LibreSSL SLL CA Bundle

sslproxy_foreign_intermediate_certs /etc/squid/ssl_cert/intermediate.pem

sslproxy_options SINGLE_DH_USE

sslproxy_cipher
EECDH+ECDSA+AESGCM:EECDH+aRSA+AESGCM:EECDH+ECDSA+SHA384:EECDH+ECDSA+SHA256:E
ECDH+aRSA+SHA384:EECDH+aRSA+SHA256:EECDH+aRSA+RC4:EECDH:EDH+aRSA:HIGH:!RC4:!
aNULL:!eNULL:!LOW:!3DES:!MD5:!EXP:!PSK:!SRP:!DSS

acl noBumpSites ssl::server_name_regex -i "/etc/squid/url.nobump"

acl step1 at_step SslBump1
acl step2 at_step SslBump2
acl step3 at_step SslBump3

ssl_bump peek step1 all
ssl_bump peek step2 nobumpSites
ssl_bump splice step3 nobumpSites
ssl_bump stare step2 all
ssl_bump bump step3 all
(...)

In the file "/etc/squid/url.nobump", I have expressions like these:

(...)
# IM
\.skype\.com$

\.whatsapp\.com$
\.whatsapp\.net$
(...)

I have read whatsapp, facebook, and many others servers use "Certificate
Pinning" to avoid "Man-in-the-middle" attacks.

But I can not find any solution/fix or workaround.
The server certificate is installed on mobile devices. The flaw occurs with
both Android and iOS devices.

Any kind of suggestion is welcome; both if there is something wrong in the
configuration written above, or better yet if someone knows the cause and
solution of this problem.

Thank you very much to all!



From squid3 at treenet.co.nz  Sat Jun  9 05:31:37 2018
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sat, 9 Jun 2018 17:31:37 +1200
Subject: [squid-users] SSL errors with Squid 3.5.27
In-Reply-To: <010201d3ff7e$9b259510$d170bf30$@yahoo.com.ar>
References: <010201d3ff7e$9b259510$d170bf30$@yahoo.com.ar>
Message-ID: <9fcd2760-281d-63f2-52e4-983ac8c5b052@treenet.co.nz>

On 09/06/18 11:15, Julian Perconti wrote:
> Hello community, I am new to the list and, I hope everyone is well.
> 
> I have running a squid server on debian 7.
> 
> My squid version is 3.5.27 manually compiled with LibreSSL 2.6.0 due to
> problems with Dropbox. After compiling squid with LibreSSL, the error
> "unknown cipher returned" has disappeared and dropbox worked correctly.
> 
> Everything works quite well, except that in /var/log/squid/cache.log there
> are 5 types of problems (at least):
> 
> [1] 2018/06/08 17:14:05 kid1| Error negotiating SSL connection on FD 7:
> error:14037418:SSL routines:ACCEPT_SR_KEY_EXCH:tlsv1 alert unknown ca (1/0)
> [2] 2018/06/08 17:14:39 kid1| Error negotiating SSL on FD 11:
> error:14007086:SSL routines:CONNECT_CR_CERT:certificate verify failed
> (1/-1/0)
> [3] 2018/06/08 18:35:43 kid1| Error negotiating SSL connection on FD 10:
> (104) Connection reset by peer
> [4] 2018/06/08 18:56:52 kid1| Error negotiating SSL on FD 13:
> error:00000000:lib(0):func(0):reason(0) (5/-1/104)
> [5] 2018/06/08 19:20:06 kid1| Error negotiating SSL connection on FD 9:
> error:06FFF064:digital envelope routines:CRYPTO_internal:bad decrypt (1/-1)

This one may need you to check the ciphers you are allowing. Or be a
sign of a bug in the library.

Trying to connect to the server manually with a CLI tool that can debug
the verify procedure would be the best way forward. You may want to look
at the handshake the client is sending to Squid and Squid to the server
for what to test with.


> 
> However I think (I'm not sure but ...), that the most serious is the number
> [2]:
> SSL negotiating error on FD 11: error: 14007086: SSL routines:
> CONNECT_CR_CERT:certificate verify failed (1/-1/0)
> 
> The problem I have it with WhatsApp from mobile devices ... the application
> tries to connect to the network indefinitely without success, and the error
> that appears (at that moment) is [2]: (...) certificate verify failed
> (1/-1/0)
> 

For 3.5.27 you need to find out what their CA is and decide whether its
worth adding to sslproxy_foreign_intermediate_certs (for intermediates),
OR to sslproxy_cafile or the system root CAs if it's self-signed.

If its an intermediate you might have better behaviour with Squid-4. But
be aware that LibreSSL is not tested by any of us dev, so technically
"not supported" even if it usually works.


> 
> https_port 3130 intercept ssl-bump \
>   cert=/etc/squid/ssl_cert/squidCA.pem \
>   key=/etc/squid/ssl_cert/squidCA.pem \
>   generate-host-certificates=on dynamic_cert_mem_cache_size=4MB
> tls-dh=/etc/squid/ssl_cert/dhparam.pem

These DH parameters are for old DH not for ECDHE (missing curve name).
So this may be restricting what your Squid can do to match up the client
and server crypto requirements.


> 
> Any kind of suggestion is welcome; both if there is something wrong in the
> configuration written above, or better yet if someone knows the cause and
> solution of this problem.

Most of these are probably just the side effects of an untrusted CA.
This is normal for TLS/SSL.


Amos


From jlay at slave-tothe-box.net  Sat Jun  9 13:02:49 2018
From: jlay at slave-tothe-box.net (James Lay)
Date: Sat, 09 Jun 2018 07:02:49 -0600
Subject: [squid-users] About to upgrade from 3 to 4
In-Reply-To: <6616d61ae837620247da660cdfe38380f9f280fa.camel@slave-tothe-box.net>
References: <c36fa63222d2e5b5cfc858b3aba1cc02d04b6d78.camel@slave-tothe-box.net>
 <0fb5d332-01a5-6065-0b2c-250329e85bcb@treenet.co.nz>
 <6616d61ae837620247da660cdfe38380f9f280fa.camel@slave-tothe-box.net>
Message-ID: <6f500c6237500397dc74205819760ccbb6dac1ff.camel@slave-tothe-box.net>

On Fri, 2018-06-08 at 09:36 -0600, James Lay wrote:
> On Sat, 2018-06-09 at 03:04 +1200, Amos Jeffries wrote:
> > On 09/06/18 02:33, James Lay wrote:
> > Hey all!
> > Topic says it....I'm starting to look at doing an upgrade from 3 to
> > 4.Any glaring surprises? Doing a transparent forward proxy with
> > somepeek/splice for content filtering only (no decryption). Has
> > anyone gonethrough an upgrade, and how painful was it, if at all?
> > Thank you.
> > 
> > Which 3.x you are starting from is the issue.
> > From 3.5 to 4 should be the same as any of the 3.x single version
> > bumps.There is nothing special about v4 from a user perspective.
> > Amos_______________________________________________squid-users
> > mailing listsquid-users at lists.squid-cache.orghttp://lists.squid-cac
> > he.org/listinfo/squid-users
> 
> Thanks Amos...I'm going from 3.5.2 ?
> 
> James
> _______________________________________________squid-users mailing
> listsquid-users at lists.squid-cache.orghttp://lists.squid-cache.org/lis
> tinfo/squid-users

So in my config file I have:
sslcrtd_program /opt/libexec/ssl_crtd -s /opt/var/ssl_db -M 4MB
However I do not see this after compiling and installing.  Has this
gone away in 4?  Thank you.
James
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20180609/2b75322e/attachment.htm>

From squid3 at treenet.co.nz  Sat Jun  9 13:13:59 2018
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sun, 10 Jun 2018 01:13:59 +1200
Subject: [squid-users] About to upgrade from 3 to 4
In-Reply-To: <6f500c6237500397dc74205819760ccbb6dac1ff.camel@slave-tothe-box.net>
References: <c36fa63222d2e5b5cfc858b3aba1cc02d04b6d78.camel@slave-tothe-box.net>
 <0fb5d332-01a5-6065-0b2c-250329e85bcb@treenet.co.nz>
 <6616d61ae837620247da660cdfe38380f9f280fa.camel@slave-tothe-box.net>
 <6f500c6237500397dc74205819760ccbb6dac1ff.camel@slave-tothe-box.net>
Message-ID: <2bc37937-2868-f982-9f7d-bf35525f1210@treenet.co.nz>

On 10/06/18 01:02, James Lay wrote:
> 
> So in my config file I have:
> 
> sslcrtd_program /opt/libexec/ssl_crtd -s /opt/var/ssl_db -M 4MB
> 
> However I do not see this after compiling and installing. Has this gone
> away in 4? Thank you.
> 
> James


It's now called security_file_certgen.

<http://www.squid-cache.org/Versions/v4/squid-4.0.24-RELEASENOTES.html#ss2.4>

Amos


From jlay at slave-tothe-box.net  Sat Jun  9 13:17:08 2018
From: jlay at slave-tothe-box.net (James Lay)
Date: Sat, 09 Jun 2018 07:17:08 -0600
Subject: [squid-users] About to upgrade from 3 to 4
In-Reply-To: <2bc37937-2868-f982-9f7d-bf35525f1210@treenet.co.nz>
References: <c36fa63222d2e5b5cfc858b3aba1cc02d04b6d78.camel@slave-tothe-box.net>
 <0fb5d332-01a5-6065-0b2c-250329e85bcb@treenet.co.nz>
 <6616d61ae837620247da660cdfe38380f9f280fa.camel@slave-tothe-box.net>
 <6f500c6237500397dc74205819760ccbb6dac1ff.camel@slave-tothe-box.net>
 <2bc37937-2868-f982-9f7d-bf35525f1210@treenet.co.nz>
Message-ID: <2937026024c57e93da5e9d45d3fa79a3b6bc14f5.camel@slave-tothe-box.net>

On Sun, 2018-06-10 at 01:13 +1200, Amos Jeffries wrote:
> On 10/06/18 01:02, James Lay wrote:
> 
> So in my config file I have:
> sslcrtd_program /opt/libexec/ssl_crtd -s /opt/var/ssl_db -M 4MB
> However I do not see this after compiling and installing. Has this
> goneaway in 4? Thank you.
> James
> 
> It's now called security_file_certgen.
> <http://www.squid-cache.org/Versions/v4/squid-4.0.24-RELEASENOTES.htm
> l#ss2.4>
> Amos

Thanks Amos...I'll read this before asking anymore questions ?

James
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20180609/f105b34d/attachment.htm>

From rightkicktech at gmail.com  Sat Jun  9 13:30:31 2018
From: rightkicktech at gmail.com (Alex K)
Date: Sat, 9 Jun 2018 16:30:31 +0300
Subject: [squid-users] PID file /var/run/squid.pid not readable AND
 Supervising process XXX which is not our child
In-Reply-To: <2cd09b57-faab-fd11-2381-19d5d7c5db4d@treenet.co.nz>
References: <CAG2Qp6t_J4h56rQ0ZE9owc=bK4vuiaz9zktAcXHMytHCWKskUg@mail.gmail.com>
 <2cd09b57-faab-fd11-2381-19d5d7c5db4d@treenet.co.nz>
Message-ID: <CABMULtJWhzYhQrA07ibBPt2ZvaJz3kCAw62YcAji5g4OdYhKHw@mail.gmail.com>

Getting back to this, I face also issues that seems to be related with how
systemd handles squid.
Frequently when I try restart the VM the VM is stuch at stopping squid and
it never restarts.

Checking the differences between the autogenerated service file and the one
shipped with squid I see:

diff /run/systemd/generator.late/squid.service
squid3-3.5.23/tools/systemd/squid.service
1c1,6
< # Automatically generated by systemd-sysv-generator
---
> ## Copyright (C) 1996-2016 The Squid Software Foundation and contributors
> ##
> ## Squid software is distributed under GPLv2+ license and includes
> ## contributions from numerous individuals and organizations.
> ## Please see the COPYING and CONTRIBUTORS files for details.
> ##
4,14c9,10
< Documentation=man:systemd-sysv-generator(8)
< SourcePath=/etc/init.d/squid
< Description=LSB: Squid HTTP Proxy version 3.x
< Before=multi-user.target
< Before=multi-user.target
< Before=multi-user.target
< Before=graphical.target
< After=network-online.target
< After=remote-fs.target
< After=nss-lookup.target
< Wants=network-online.target
---
> Description=Squid Web Proxy Server
> After=network.target
17,20c13,15
< Type=forking
< Restart=no
< TimeoutSec=5min
< IgnoreSIGPIPE=no
---
> Type=simple
> ExecStart=/usr/sbin/squid -sYC -N
> ExecReload=/bin/kill -HUP $MAINPID
22,28c17,19
< GuessMainPID=no
< RemainAfterExit=no
< PIDFile=/var/run/squid.pid
< SuccessExitStatus=5 6
< ExecStart=/etc/init.d/squid start
< ExecStop=/etc/init.d/squid stop
< ExecReload=/etc/init.d/squid reload
---
>
> [Install]
> WantedBy=multi-user.target

So do I just overwrite the  squid.service of the system with the one
shipped with squid?

Thanx,
Alex

On Thu, May 10, 2018 at 5:09 AM, Amos Jeffries <squid3 at treenet.co.nz> wrote:

> On 10/05/18 11:53, Roberto Carna wrote:
> > Dear, I have Squid/Dansguardian in a Debian 9 server.
> >
> > My Squid packages is from Debian repo, it is the stable version:
> >
> > squid                          3.5.23-5+deb9u1
> ...
> >
> > But when I read I notice two curious lines:
> >
> > systemd[1]: squid.service: PID file /var/run/squid.pid not readable
> > (yet?) after start: No such file or directory
> > systemd[1]: squid.service: Supervising process 895 which is not our
> > child. We'll most likely not notice when it exits.
> >
> >
> > Is it normal or do I have to solve these? I repeat Squid is running OK...
>
> systemd cannot cope with daemons like Squid-3. All you can do for now is
> ensure that you use the init.d scripts to manage Squid. Do not use the
> "service ..." commands provided by systemd.
>
> Squid-4 packages that resolve these issues are in Debian experimental
> awaiting an official upstream stable release.
>  NP: the major bugs preventing upstream stable are not affecting the
> Debian package features. You can use the Squid-4 package now if you wish
> by adding that "experimental" repository to your apt sources.list,
> update apt, then install/upgrade Squid with "apt-get -t experimental
> install squid".
>
> Amos
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20180609/727450c8/attachment.htm>

From jlay at slave-tothe-box.net  Sat Jun  9 14:23:05 2018
From: jlay at slave-tothe-box.net (James Lay)
Date: Sat, 09 Jun 2018 08:23:05 -0600
Subject: [squid-users] About to upgrade from 3 to 4
In-Reply-To: <2937026024c57e93da5e9d45d3fa79a3b6bc14f5.camel@slave-tothe-box.net>
References: <c36fa63222d2e5b5cfc858b3aba1cc02d04b6d78.camel@slave-tothe-box.net>
 <0fb5d332-01a5-6065-0b2c-250329e85bcb@treenet.co.nz>
 <6616d61ae837620247da660cdfe38380f9f280fa.camel@slave-tothe-box.net>
 <6f500c6237500397dc74205819760ccbb6dac1ff.camel@slave-tothe-box.net>
 <2bc37937-2868-f982-9f7d-bf35525f1210@treenet.co.nz>
 <2937026024c57e93da5e9d45d3fa79a3b6bc14f5.camel@slave-tothe-box.net>
Message-ID: <f5476286d6e87f9325ec5e508d9a49906fc868b4.camel@slave-tothe-box.net>

On Sat, 2018-06-09 at 07:17 -0600, James Lay wrote:
> On Sun, 2018-06-10 at 01:13 +1200, Amos Jeffries wrote:
> > On 10/06/18 01:02, James Lay wrote:
> > 
> > So in my config file I have:
> > sslcrtd_program /opt/libexec/ssl_crtd -s /opt/var/ssl_db -M 4MB
> > However I do not see this after compiling and installing. Has this
> > goneaway in 4? Thank you.
> > James
> > 
> > It's now called security_file_certgen.
> > <http://www.squid-cache.org/Versions/v4/squid-4.0.24-RELEASENOTES.h
> > tml#ss2.4>
> > Amos
> 
> Thanks Amos...I'll read this before asking anymore questions ?
> 
> James
> _______________________________________________squid-users mailing
> listsquid-users at lists.squid-cache.orghttp://lists.squid-cache.org/lis
> tinfo/squid-users

So ok...after making the changes to the config to account for
new  security_file_certgen and tls_outgoing_options (thanks Amos!) I am
greeted with (hostname changed from real):
FATAL: mimeLoadIcon: cannot parse internal URL: http://<hostname>:0/squ
id-internal-static/icons/silk/image.png
Here's my config line:
./configure --prefix=/opt/squid --with-openssl=/opt/libressl --
sysconfdir=/opt/squid/etc --enable-ssl --enable-ssl-crtd --enable-
linux-netfilter --enable-follow-x-forwarded-for --with-large-files --
enable-xternal-acl-helpers=none
full config (I realize this might not be the most secure on the planet,
for now this is a dev box and I'm just testing functionality):
acl localnet src 192.168.1.0/24acl SSL_ports port 443acl Safe_ports
port 80acl Safe_ports port 443acl CONNECT method CONNECTacl
allowed_http_sites url_regex "/opt/squid/etc/http_url.txt"
http_access deny !Safe_portshttp_access deny CONNECT
!SSL_Portshttp_access allow SSL_portshttp_access allow
allowed_http_siteshttp_access deny all
acl broken_ips dst "/opt/squid/etc/broken_ips.txt"ssl_bump splice
broken_ipsacl broken_https_sites ssl::server_name_regex
"/opt/squid/etc/broken_url.txt"ssl_bump splice
broken_https_sitesssl_bump peek allacl allowed_https_sites
ssl::server_name_regex "/opt/squid/etc/http_url.txt"ssl_bump splice
allowed_https_sitesssl_bump terminate all
sslproxy_cert_error allow alltls_outgoing_options capath=/etc/ssl/certs
flags=DONT_VERIFY_PEER
sslcrtd_program /opt/squid/libexec/security_file_certgen -s
/opt/squid/var/ -M 4MBsslcrtd_children 5
http_port gateway:3128 intercepthttps_port gateway:3129 intercept ssl-
bump cert=/opt/squid/etc/certs/sslsplit_ca_cert.pem
cafile=/opt/squid/etc/certs/sslsplit_ca_cert.pem
key=/opt/squid/etc/certs/sslsplit_ca_key.pem generate-host-
certificates=on dynamic_cert_mem_cache_size=4MB
sslflags=NO_SESSION_REUSE
logformat mine %>a %[ui %[un [%tl] "%rm %ru HTTP/%rv" %ssl::>sni
%ssl::>cert_subject %>Hs %<st %Ss:%Sh
access_log syslog:daemon.info mine
refresh_pattern -i (cgi-bin|\?) 0       0%      0refresh_pattern
.               0       20%     4320
coredump_dir /opt/squid/var
At this point I have no clue what to do next...any troubleshooting
steps would be wonderful.  Thank you.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20180609/86485da1/attachment.htm>

From rightkicktech at gmail.com  Sat Jun  9 15:10:37 2018
From: rightkicktech at gmail.com (Alex K)
Date: Sat, 9 Jun 2018 18:10:37 +0300
Subject: [squid-users] PID file /var/run/squid.pid not readable AND
 Supervising process XXX which is not our child
In-Reply-To: <CABMULtJWhzYhQrA07ibBPt2ZvaJz3kCAw62YcAji5g4OdYhKHw@mail.gmail.com>
References: <CAG2Qp6t_J4h56rQ0ZE9owc=bK4vuiaz9zktAcXHMytHCWKskUg@mail.gmail.com>
 <2cd09b57-faab-fd11-2381-19d5d7c5db4d@treenet.co.nz>
 <CABMULtJWhzYhQrA07ibBPt2ZvaJz3kCAw62YcAji5g4OdYhKHw@mail.gmail.com>
Message-ID: <CABMULtJy6nVBernFwX84U_4V0BEy47fWL0qXTg8xGnO2mUZW-Q@mail.gmail.com>

After proceeding with using the shipped service file, then systemctl
daemon-reload I do not experience any stuck reboots at the moment.

Alex

On Sat, Jun 9, 2018 at 4:30 PM, Alex K <rightkicktech at gmail.com> wrote:

> Getting back to this, I face also issues that seems to be related with how
> systemd handles squid.
> Frequently when I try restart the VM the VM is stuch at stopping squid and
> it never restarts.
>
> Checking the differences between the autogenerated service file and the
> one shipped with squid I see:
>
> diff /run/systemd/generator.late/squid.service
> squid3-3.5.23/tools/systemd/squid.service
> 1c1,6
> < # Automatically generated by systemd-sysv-generator
> ---
> > ## Copyright (C) 1996-2016 The Squid Software Foundation and contributors
> > ##
> > ## Squid software is distributed under GPLv2+ license and includes
> > ## contributions from numerous individuals and organizations.
> > ## Please see the COPYING and CONTRIBUTORS files for details.
> > ##
> 4,14c9,10
> < Documentation=man:systemd-sysv-generator(8)
> < SourcePath=/etc/init.d/squid
> < Description=LSB: Squid HTTP Proxy version 3.x
> < Before=multi-user.target
> < Before=multi-user.target
> < Before=multi-user.target
> < Before=graphical.target
> < After=network-online.target
> < After=remote-fs.target
> < After=nss-lookup.target
> < Wants=network-online.target
> ---
> > Description=Squid Web Proxy Server
> > After=network.target
> 17,20c13,15
> < Type=forking
> < Restart=no
> < TimeoutSec=5min
> < IgnoreSIGPIPE=no
> ---
> > Type=simple
> > ExecStart=/usr/sbin/squid -sYC -N
> > ExecReload=/bin/kill -HUP $MAINPID
> 22,28c17,19
> < GuessMainPID=no
> < RemainAfterExit=no
> < PIDFile=/var/run/squid.pid
> < SuccessExitStatus=5 6
> < ExecStart=/etc/init.d/squid start
> < ExecStop=/etc/init.d/squid stop
> < ExecReload=/etc/init.d/squid reload
> ---
> >
> > [Install]
> > WantedBy=multi-user.target
>
> So do I just overwrite the  squid.service of the system with the one
> shipped with squid?
>
> Thanx,
> Alex
>
> On Thu, May 10, 2018 at 5:09 AM, Amos Jeffries <squid3 at treenet.co.nz>
> wrote:
>
>> On 10/05/18 11:53, Roberto Carna wrote:
>> > Dear, I have Squid/Dansguardian in a Debian 9 server.
>> >
>> > My Squid packages is from Debian repo, it is the stable version:
>> >
>> > squid                          3.5.23-5+deb9u1
>> ...
>> >
>> > But when I read I notice two curious lines:
>> >
>> > systemd[1]: squid.service: PID file /var/run/squid.pid not readable
>> > (yet?) after start: No such file or directory
>> > systemd[1]: squid.service: Supervising process 895 which is not our
>> > child. We'll most likely not notice when it exits.
>> >
>> >
>> > Is it normal or do I have to solve these? I repeat Squid is running
>> OK...
>>
>> systemd cannot cope with daemons like Squid-3. All you can do for now is
>> ensure that you use the init.d scripts to manage Squid. Do not use the
>> "service ..." commands provided by systemd.
>>
>> Squid-4 packages that resolve these issues are in Debian experimental
>> awaiting an official upstream stable release.
>>  NP: the major bugs preventing upstream stable are not affecting the
>> Debian package features. You can use the Squid-4 package now if you wish
>> by adding that "experimental" repository to your apt sources.list,
>> update apt, then install/upgrade Squid with "apt-get -t experimental
>> install squid".
>>
>> Amos
>> _______________________________________________
>> squid-users mailing list
>> squid-users at lists.squid-cache.org
>> http://lists.squid-cache.org/listinfo/squid-users
>>
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20180609/f8f54c0a/attachment.htm>

From vh1988 at yahoo.com.ar  Sat Jun  9 15:46:06 2018
From: vh1988 at yahoo.com.ar (Julian Perconti)
Date: Sat, 9 Jun 2018 12:46:06 -0300
Subject: [squid-users] SSL errors with Squid 3.5.27
In-Reply-To: <9fcd2760-281d-63f2-52e4-983ac8c5b052@treenet.co.nz>
References: <010201d3ff7e$9b259510$d170bf30$@yahoo.com.ar>
 <9fcd2760-281d-63f2-52e4-983ac8c5b052@treenet.co.nz>
Message-ID: <01b501d40008$fc418e00$f4c4aa00$@yahoo.com.ar>

>> https_port 3130 intercept ssl-bump \
>>   cert=/etc/squid/ssl_cert/squidCA.pem \
>>   key=/etc/squid/ssl_cert/squidCA.pem \
>>   generate-host-certificates=on dynamic_cert_mem_cache_size=4MB 
>> tls-dh=/etc/squid/ssl_cert/dhparam.pem
>
>These DH parameters are for old DH not for ECDHE (missing curve name).
>So this may be restricting what your Squid can do to match up the client and server crypto requirements.

Hi Amos,

I have commented the line: "tls-dh=/etc/squid/ssl_cert/dhparam.pem"

And, it seems that many errors (SSL errors) in cache.log have disappeared.
I will confirm later if WhatsApp works from iOS/Android.

Thank You!

PS: I used this option (tls-dh, dhparam, etc..) following the official documentation of squid-cache.org for the "hardening" ... or "improve security", etc.



From squid3 at treenet.co.nz  Sun Jun 10 06:49:56 2018
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sun, 10 Jun 2018 18:49:56 +1200
Subject: [squid-users] SSL errors with Squid 3.5.27
In-Reply-To: <01b501d40008$fc418e00$f4c4aa00$@yahoo.com.ar>
References: <010201d3ff7e$9b259510$d170bf30$@yahoo.com.ar>
 <9fcd2760-281d-63f2-52e4-983ac8c5b052@treenet.co.nz>
 <01b501d40008$fc418e00$f4c4aa00$@yahoo.com.ar>
Message-ID: <8a1755bc-5257-1db0-8430-ab8fc62fdb77@treenet.co.nz>

On 10/06/18 03:46, Julian Perconti wrote:
>>> https_port 3130 intercept ssl-bump \
>>>   cert=/etc/squid/ssl_cert/squidCA.pem \
>>>   key=/etc/squid/ssl_cert/squidCA.pem \
>>>   generate-host-certificates=on dynamic_cert_mem_cache_size=4MB 
>>> tls-dh=/etc/squid/ssl_cert/dhparam.pem
>>
>> These DH parameters are for old DH not for ECDHE (missing curve name).
>> So this may be restricting what your Squid can do to match up the client and server crypto requirements.
> 
> Hi Amos,
> 
> I have commented the line: "tls-dh=/etc/squid/ssl_cert/dhparam.pem"
> 
> And, it seems that many errors (SSL errors) in cache.log have disappeared.
> I will confirm later if WhatsApp works from iOS/Android.
> 
> Thank You!
> 
> PS: I used this option (tls-dh, dhparam, etc..) following the official documentation of squid-cache.org for the "hardening" ... or "improve security", etc.

Interesting.

The main issue was that you configured only params for the Diffi-Helman
(DH and DHE) ciphers - no curve name. That meant your specified EEC*
ciphers were disabled since they require a curve name as well.

Removing this option completely disables both DH and ECDH cipher types.
Leaving your proxy with only the RSA based ciphers.

Amos


From squid3 at treenet.co.nz  Sun Jun 10 07:55:41 2018
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sun, 10 Jun 2018 19:55:41 +1200
Subject: [squid-users] About to upgrade from 3 to 4
In-Reply-To: <f5476286d6e87f9325ec5e508d9a49906fc868b4.camel@slave-tothe-box.net>
References: <c36fa63222d2e5b5cfc858b3aba1cc02d04b6d78.camel@slave-tothe-box.net>
 <0fb5d332-01a5-6065-0b2c-250329e85bcb@treenet.co.nz>
 <6616d61ae837620247da660cdfe38380f9f280fa.camel@slave-tothe-box.net>
 <6f500c6237500397dc74205819760ccbb6dac1ff.camel@slave-tothe-box.net>
 <2bc37937-2868-f982-9f7d-bf35525f1210@treenet.co.nz>
 <2937026024c57e93da5e9d45d3fa79a3b6bc14f5.camel@slave-tothe-box.net>
 <f5476286d6e87f9325ec5e508d9a49906fc868b4.camel@slave-tothe-box.net>
Message-ID: <e6e9ae8d-9041-c21c-45bf-8a71975354ae@treenet.co.nz>

On 10/06/18 02:23, James Lay wrote:
> On Sat, 2018-06-09 at 07:17 -0600, James Lay wrote:
>> On Sun, 2018-06-10 at 01:13 +1200, Amos Jeffries wrote:
>>> On 10/06/18 01:02, James Lay wrote:
>>>
>>> So in my config file I have:
>>>
>>> sslcrtd_program /opt/libexec/ssl_crtd -s /opt/var/ssl_db -M 4MB
>>>
>>> However I do not see this after compiling and installing. Has this gone
>>> away in 4? Thank you.
>>>
>>> James
>>>
>>>
>>> It's now called security_file_certgen.
>>>
>>> <http://www.squid-cache.org/Versions/v4/squid-4.0.24-RELEASENOTES.html#ss2.4>
>>>
>>> Amos
>>>
>>
>> Thanks Amos...I'll read this before asking anymore questions ?
>>
>>
> 
> So ok...after making the changes to the config to account for new
> security_file_certgen and tls_outgoing_options (thanks Amos!) I am
> greeted with (hostname changed from real):
> 
> FATAL: mimeLoadIcon: cannot parse internal URL:
> http://<hostname>:0/squid-internal-static/icons/silk/image.png
> 

There should be an error about no forward-proxy port as well. Squid
requires at least one port able to receive requests for those URLs from
clients. Port 3128 is normally that port, but you have repurposed it for
interception, which disqualifies it.

The hostname in these URLs is taken from that port's IP address
reverse-DNS name, or the proxies public/visible hostname. Whichever
meets the requirement of being resolvable in DNS.


> Here's my config line:
> 
> ./configure --prefix=/opt/squid --with-openssl=/opt/libressl
> --sysconfdir=/opt/squid/etc --enable-ssl --enable-ssl-crtd
> --enable-linux-netfilter --enable-follow-x-forwarded-for
> --with-large-files --enable-xternal-acl-helpers=none

Missing 'e' on --enable-external-acl-helpers.

...
> 
> sslproxy_cert_error allow all
> tls_outgoing_options capath=/etc/ssl/certs flags=DONT_VERIFY_PEER

Please avoid DONT_VERIFY_PEER and "allow all" for cert errors. They are
useless for both production AND debugging since all they do is hide
security issues from *you*.

It is best to watch for security issues and fix them. Not just ignore
everything.

Amos


From Walter.H at mathemainzel.info  Sun Jun 10 08:42:53 2018
From: Walter.H at mathemainzel.info (Walter H.)
Date: Sun, 10 Jun 2018 10:42:53 +0200
Subject: [squid-users] SSL errors with Squid 3.5.27
In-Reply-To: <8a1755bc-5257-1db0-8430-ab8fc62fdb77@treenet.co.nz>
References: <010201d3ff7e$9b259510$d170bf30$@yahoo.com.ar>
 <9fcd2760-281d-63f2-52e4-983ac8c5b052@treenet.co.nz>
 <01b501d40008$fc418e00$f4c4aa00$@yahoo.com.ar>
 <8a1755bc-5257-1db0-8430-ab8fc62fdb77@treenet.co.nz>
Message-ID: <5B1CE48D.4050709@mathemainzel.info>

On 10.06.2018 08:49, Amos Jeffries wrote:
>
> Interesting.
>
> The main issue was that you configured only params for the Diffi-Helman
> (DH and DHE) ciphers - no curve name. That meant your specified EEC*
> ciphers were disabled since they require a curve name as well.
>
> Removing this option completely disables both DH and ECDH cipher types.
> Leaving your proxy with only the RSA based ciphers.
>
can you please tell, how to configure this correct

I mean how to specify the curve name ...
and which curves are possible

Thanks,
Walter

-------------- next part --------------
A non-text attachment was scrubbed...
Name: smime.p7s
Type: application/pkcs7-signature
Size: 3491 bytes
Desc: S/MIME Cryptographic Signature
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20180610/2f4fee07/attachment.bin>

From jlay at slave-tothe-box.net  Sun Jun 10 10:08:32 2018
From: jlay at slave-tothe-box.net (James Lay)
Date: Sun, 10 Jun 2018 04:08:32 -0600
Subject: [squid-users] About to upgrade from 3 to 4
In-Reply-To: <e6e9ae8d-9041-c21c-45bf-8a71975354ae@treenet.co.nz>
References: <c36fa63222d2e5b5cfc858b3aba1cc02d04b6d78.camel@slave-tothe-box.net>
 <0fb5d332-01a5-6065-0b2c-250329e85bcb@treenet.co.nz>
 <6616d61ae837620247da660cdfe38380f9f280fa.camel@slave-tothe-box.net>
 <6f500c6237500397dc74205819760ccbb6dac1ff.camel@slave-tothe-box.net>
 <2bc37937-2868-f982-9f7d-bf35525f1210@treenet.co.nz>
 <2937026024c57e93da5e9d45d3fa79a3b6bc14f5.camel@slave-tothe-box.net>
 <f5476286d6e87f9325ec5e508d9a49906fc868b4.camel@slave-tothe-box.net>
 <e6e9ae8d-9041-c21c-45bf-8a71975354ae@treenet.co.nz>
Message-ID: <86969bdb620b7f2e9fb2d883c654c135241a04ec.camel@slave-tothe-box.net>

On Sun, 2018-06-10 at 19:55 +1200, Amos Jeffries wrote:
> On 10/06/18 02:23, James Lay wrote:
> On Sat, 2018-06-09 at 07:17 -0600, James Lay wrote:
> On Sun, 2018-06-10 at 01:13 +1200, Amos Jeffries wrote:
> On 10/06/18 01:02, James Lay wrote:
> So in my config file I have:
> sslcrtd_program /opt/libexec/ssl_crtd -s /opt/var/ssl_db -M 4MB
> However I do not see this after compiling and installing. Has this
> goneaway in 4? Thank you.
> James
> 
> It's now called security_file_certgen.
> <http://www.squid-cache.org/Versions/v4/squid-4.0.24-RELEASENOTES.htm
> l#ss2.4>
> Amos
> 
> Thanks Amos...I'll read this before asking anymore questions ?
> 
> 
> So ok...after making the changes to the config to account for
> newsecurity_file_certgen and tls_outgoing_options (thanks Amos!) I
> amgreeted with (hostname changed from real):
> FATAL: mimeLoadIcon: cannot parse internal URL:http://<hostname>:0/sq
> uid-internal-static/icons/silk/image.png
> 
> There should be an error about no forward-proxy port as well.
> Squidrequires at least one port able to receive requests for those
> URLs fromclients. Port 3128 is normally that port, but you have
> repurposed it forinterception, which disqualifies it.
> The hostname in these URLs is taken from that port's IP
> addressreverse-DNS name, or the proxies public/visible hostname.
> Whichevermeets the requirement of being resolvable in DNS.
> 
> Here's my config line:
> ./configure --prefix=/opt/squid --with-openssl=/opt/libressl
> --sysconfdir=/opt/squid/etc --enable-ssl --enable-ssl-crtd--enable-
> linux-netfilter --enable-follow-x-forwarded-for--with-large-files --
> enable-xternal-acl-helpers=none
> Missing 'e' on --enable-external-acl-helpers.
> ...
> 
> sslproxy_cert_error allow alltls_outgoing_options
> capath=/etc/ssl/certs flags=DONT_VERIFY_PEER
> Please avoid DONT_VERIFY_PEER and "allow all" for cert errors. They
> areuseless for both production AND debugging since all they do is
> hidesecurity issues from *you*.
> It is best to watch for security issues and fix them. Not just
> ignoreeverything.
> Amos_______________________________________________squid-users
> mailing listsquid-users at lists.squid-cache.orghttp://lists.squid-cache
> .org/listinfo/squid-users

Thanks Amos...your insight always helps.  You were right on point...I
did have the no forward proxy error.  After adding an additional
http_port squid came right up...thanks again.

James
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20180610/9c2c7ee2/attachment.htm>

From yochi at trilogic.co.za  Sun Jun 10 17:22:40 2018
From: yochi at trilogic.co.za (Yochi Lipschitz)
Date: Sun, 10 Jun 2018 20:22:40 +0300
Subject: [squid-users] Whitelist everything except
Message-ID: <CA+etuaT8uFC8rbFiwme=X+O1OoxiQSFvNPh588YXg9gKEUVS0Q@mail.gmail.com>

Hi,

I am currently using Squid 3. Is it possible to Whitelist a domain but
exclude specific subdomains? I do not want to have to list all the
subdomains that I want to Whitelist, I want to Whitelist everything except
1 or 2 specifics.

EG: Whitelist everything in .domain.com except for 123.domain.com

Thanking you in advance.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20180610/5317ff31/attachment.htm>

From eliezer at ngtech.co.il  Mon Jun 11 02:03:02 2018
From: eliezer at ngtech.co.il (Eliezer Croitoru)
Date: Mon, 11 Jun 2018 05:03:02 +0300
Subject: [squid-users] PID file /var/run/squid.pid not readable AND
	Supervising process XXX which is not our child
In-Reply-To: <CABMULtJWhzYhQrA07ibBPt2ZvaJz3kCAw62YcAji5g4OdYhKHw@mail.gmail.com>
References: <CAG2Qp6t_J4h56rQ0ZE9owc=bK4vuiaz9zktAcXHMytHCWKskUg@mail.gmail.com>
 <2cd09b57-faab-fd11-2381-19d5d7c5db4d@treenet.co.nz>
 <CABMULtJWhzYhQrA07ibBPt2ZvaJz3kCAw62YcAji5g4OdYhKHw@mail.gmail.com>
Message-ID: <06a201d40128$543276e0$fc9764a0$@ngtech.co.il>

Hey Alex,

 

What OS exactly is shipping this service file?


Thanks,

Eliezer

 

----

 <http://ngtech.co.il/lmgtfy/> Eliezer Croitoru
Linux System Administrator
Mobile: +972-5-28704261
Email: eliezer at ngtech.co.il



 

From: squid-users <squid-users-bounces at lists.squid-cache.org> On Behalf Of Alex K
Sent: Saturday, June 9, 2018 16:31
To: Amos Jeffries <squid3 at treenet.co.nz>
Cc: squid-users at lists.squid-cache.org
Subject: Re: [squid-users] PID file /var/run/squid.pid not readable AND Supervising process XXX which is not our child

 

Getting back to this, I face also issues that seems to be related with how systemd handles squid. 

Frequently when I try restart the VM the VM is stuch at stopping squid and it never restarts. 

 

Checking the differences between the autogenerated service file and the one shipped with squid I see: 

 

diff /run/systemd/generator.late/squid.service squid3-3.5.23/tools/systemd/squid.service
1c1,6
< # Automatically generated by systemd-sysv-generator
---
> ## Copyright (C) 1996-2016 The Squid Software Foundation and contributors
> ##
> ## Squid software is distributed under GPLv2+ license and includes
> ## contributions from numerous individuals and organizations.
> ## Please see the COPYING and CONTRIBUTORS files for details.
> ##
4,14c9,10
< Documentation=man:systemd-sysv-generator(8)
< SourcePath=/etc/init.d/squid
< Description=LSB: Squid HTTP Proxy version 3.x
< Before=multi-user.target
< Before=multi-user.target
< Before=multi-user.target
< Before=graphical.target
< After=network-online.target
< After=remote-fs.target
< After=nss-lookup.target
< Wants=network-online.target
---
> Description=Squid Web Proxy Server
> After=network.target
17,20c13,15
< Type=forking
< Restart=no
< TimeoutSec=5min
< IgnoreSIGPIPE=no
---
> Type=simple
> ExecStart=/usr/sbin/squid -sYC -N
> ExecReload=/bin/kill -HUP $MAINPID
22,28c17,19
< GuessMainPID=no
< RemainAfterExit=no
< PIDFile=/var/run/squid.pid
< SuccessExitStatus=5 6
< ExecStart=/etc/init.d/squid start
< ExecStop=/etc/init.d/squid stop
< ExecReload=/etc/init.d/squid reload
---
>
> [Install]
> WantedBy=multi-user.target

 

So do I just overwrite the  squid.service of the system with the one shipped with squid?

 

Thanx, 

Alex

 

On Thu, May 10, 2018 at 5:09 AM, Amos Jeffries <squid3 at treenet.co.nz <mailto:squid3 at treenet.co.nz> > wrote:

On 10/05/18 11:53, Roberto Carna wrote:
> Dear, I have Squid/Dansguardian in a Debian 9 server.
> 
> My Squid packages is from Debian repo, it is the stable version:
> 
> squid                          3.5.23-5+deb9u1
...
> 
> But when I read I notice two curious lines:
> 
> systemd[1]: squid.service: PID file /var/run/squid.pid not readable
> (yet?) after start: No such file or directory
> systemd[1]: squid.service: Supervising process 895 which is not our
> child. We'll most likely not notice when it exits.
> 
> 
> Is it normal or do I have to solve these? I repeat Squid is running OK...

systemd cannot cope with daemons like Squid-3. All you can do for now is
ensure that you use the init.d scripts to manage Squid. Do not use the
"service ..." commands provided by systemd.

Squid-4 packages that resolve these issues are in Debian experimental
awaiting an official upstream stable release.
 NP: the major bugs preventing upstream stable are not affecting the
Debian package features. You can use the Squid-4 package now if you wish
by adding that "experimental" repository to your apt sources.list,
update apt, then install/upgrade Squid with "apt-get -t experimental
install squid".

Amos

_______________________________________________
squid-users mailing list
squid-users at lists.squid-cache.org <mailto:squid-users at lists.squid-cache.org> 
http://lists.squid-cache.org/listinfo/squid-users

 

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20180611/bb20a463/attachment.htm>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: image002.png
Type: image/png
Size: 11298 bytes
Desc: not available
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20180611/bb20a463/attachment.png>

From squid3 at treenet.co.nz  Mon Jun 11 06:02:49 2018
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Mon, 11 Jun 2018 18:02:49 +1200
Subject: [squid-users] SSL errors with Squid 3.5.27
In-Reply-To: <5B1CE48D.4050709@mathemainzel.info>
References: <010201d3ff7e$9b259510$d170bf30$@yahoo.com.ar>
 <9fcd2760-281d-63f2-52e4-983ac8c5b052@treenet.co.nz>
 <01b501d40008$fc418e00$f4c4aa00$@yahoo.com.ar>
 <8a1755bc-5257-1db0-8430-ab8fc62fdb77@treenet.co.nz>
 <5B1CE48D.4050709@mathemainzel.info>
Message-ID: <f2d69224-2787-7680-99f7-3e77342f9583@treenet.co.nz>

On 10/06/18 20:42, Walter H. wrote:
> On 10.06.2018 08:49, Amos Jeffries wrote:
>>
>> Interesting.
>>
>> The main issue was that you configured only params for the Diffi-Helman
>> (DH and DHE) ciphers - no curve name. That meant your specified EEC*
>> ciphers were disabled since they require a curve name as well.
>>
>> Removing this option completely disables both DH and ECDH cipher types.
>> Leaving your proxy with only the RSA based ciphers.
>>
> can you please tell, how to configure this correct
> 
> I mean how to specify the curve name ...
> and which curves are possible


The documentation covers that.

<http://www.squid-cache.org/Doc/config/http_port/>
"
  tls-dh=[curve:]file

  File containing DH parameters for temporary/ephemeral DH key
  exchanges, optionally prefixed by a curve for ephemeral ECDH
  key exchanges.

  See OpenSSL documentation for details on how to create the
  DH parameter file. Supported curves for ECDH can be listed
  using the "openssl ecparam -list_curves" command.

  WARNING: EDH and EECDH ciphers will be silently disabled if
  this option is not set.
"

Curve names depend on library, so you have to check your own library for
them as described above.

Amos


From squid3 at treenet.co.nz  Mon Jun 11 06:12:29 2018
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Mon, 11 Jun 2018 18:12:29 +1200
Subject: [squid-users] Whitelist everything except
In-Reply-To: <CA+etuaT8uFC8rbFiwme=X+O1OoxiQSFvNPh588YXg9gKEUVS0Q@mail.gmail.com>
References: <CA+etuaT8uFC8rbFiwme=X+O1OoxiQSFvNPh588YXg9gKEUVS0Q@mail.gmail.com>
Message-ID: <c189957a-d502-ad76-d1e3-74bb4ade25bd@treenet.co.nz>

On 11/06/18 05:22, Yochi Lipschitz wrote:
> Hi,
> 
> I am currently using Squid 3. Is it possible to Whitelist a domain but
> exclude specific subdomains? I do not want to have to list all the
> subdomains that I want to Whitelist, I want to Whitelist everything
> except 1 or 2 specifics.

Of course. Anything that can be expressed as a boolean condition can be
done. Just use two ACL names and deny the sub-domains before allowing
the general domain.

These docs cover AND/OR logic and ordering
<https://wiki.squid-cache.org/SquidFaq/SquidAcl#Common_Mistakes>

Amos


From squid3 at treenet.co.nz  Mon Jun 11 06:24:51 2018
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Mon, 11 Jun 2018 18:24:51 +1200
Subject: [squid-users] PID file /var/run/squid.pid not readable AND
 Supervising process XXX which is not our child
In-Reply-To: <06a201d40128$543276e0$fc9764a0$@ngtech.co.il>
References: <CAG2Qp6t_J4h56rQ0ZE9owc=bK4vuiaz9zktAcXHMytHCWKskUg@mail.gmail.com>
 <2cd09b57-faab-fd11-2381-19d5d7c5db4d@treenet.co.nz>
 <CABMULtJWhzYhQrA07ibBPt2ZvaJz3kCAw62YcAji5g4OdYhKHw@mail.gmail.com>
 <06a201d40128$543276e0$fc9764a0$@ngtech.co.il>
Message-ID: <2443723c-f648-21a9-98f2-ebf4adfcec66@treenet.co.nz>

On 11/06/18 14:03, Eliezer Croitoru wrote:
> Hey Alex,
> 
> What OS exactly is shipping this service file?
> 

>From the first post:

> 3.5.23-5+deb9u1

Current Debian stable. It is not shipped exactly, but generates on
install from the init.d file inserv headers in the absence of a shipped
.service.

"
 # Automatically generated by systemd-sysv-generator
 SourcePath=/etc/init.d/squid
"


Amos


From eliezer at ngtech.co.il  Mon Jun 11 07:11:40 2018
From: eliezer at ngtech.co.il (Eliezer Croitoru)
Date: Mon, 11 Jun 2018 10:11:40 +0300
Subject: [squid-users] PID file /var/run/squid.pid not readable AND
	Supervising process XXX which is not our child
In-Reply-To: <2443723c-f648-21a9-98f2-ebf4adfcec66@treenet.co.nz>
References: <CAG2Qp6t_J4h56rQ0ZE9owc=bK4vuiaz9zktAcXHMytHCWKskUg@mail.gmail.com>
 <2cd09b57-faab-fd11-2381-19d5d7c5db4d@treenet.co.nz>
 <CABMULtJWhzYhQrA07ibBPt2ZvaJz3kCAw62YcAji5g4OdYhKHw@mail.gmail.com>
 <06a201d40128$543276e0$fc9764a0$@ngtech.co.il>
 <2443723c-f648-21a9-98f2-ebf4adfcec66@treenet.co.nz>
Message-ID: <070b01d40153$72407fd0$56c17f70$@ngtech.co.il>

It was hard for me to understand the diff\patch without the original file in plain text.
< GuessMainPID=no
< RemainAfterExit=no
< PIDFile=/var/run/squid.pid
< SuccessExitStatus=5 6
< ExecStart=/etc/init.d/squid start
< ExecStop=/etc/init.d/squid stop
< ExecReload=/etc/init.d/squid reload

The above now makes more sense but.. replacing the service file and removing the /etc/init.d/squid file should be the real way to run squid.
I do not know if and when Debian Stable would upgrade their package but as long as you don't upgrade it automatically it would be suffice.

Eliezer

----
Eliezer Croitoru
Linux System Administrator
Mobile: +972-5-28704261
Email: eliezer at ngtech.co.il



-----Original Message-----
From: Amos Jeffries <squid3 at treenet.co.nz> 
Sent: Monday, June 11, 2018 09:25
To: Eliezer Croitoru <eliezer at ngtech.co.il>; 'Alex K' <rightkicktech at gmail.com>
Cc: squid-users at lists.squid-cache.org
Subject: Re: [squid-users] PID file /var/run/squid.pid not readable AND Supervising process XXX which is not our child

On 11/06/18 14:03, Eliezer Croitoru wrote:
> Hey Alex,
> 
> What OS exactly is shipping this service file?
> 

>From the first post:

> 3.5.23-5+deb9u1

Current Debian stable. It is not shipped exactly, but generates on
install from the init.d file inserv headers in the absence of a shipped
.service.

"
 # Automatically generated by systemd-sysv-generator
 SourcePath=/etc/init.d/squid
"


Amos



From squid3 at treenet.co.nz  Mon Jun 11 08:16:02 2018
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Mon, 11 Jun 2018 20:16:02 +1200
Subject: [squid-users] PID file /var/run/squid.pid not readable AND
 Supervising process XXX which is not our child
In-Reply-To: <070b01d40153$72407fd0$56c17f70$@ngtech.co.il>
References: <CAG2Qp6t_J4h56rQ0ZE9owc=bK4vuiaz9zktAcXHMytHCWKskUg@mail.gmail.com>
 <2cd09b57-faab-fd11-2381-19d5d7c5db4d@treenet.co.nz>
 <CABMULtJWhzYhQrA07ibBPt2ZvaJz3kCAw62YcAji5g4OdYhKHw@mail.gmail.com>
 <06a201d40128$543276e0$fc9764a0$@ngtech.co.il>
 <2443723c-f648-21a9-98f2-ebf4adfcec66@treenet.co.nz>
 <070b01d40153$72407fd0$56c17f70$@ngtech.co.il>
Message-ID: <34e2a17a-d896-bbf4-9c10-4fe4f02b0ced@treenet.co.nz>

On 11/06/18 19:11, Eliezer Croitoru wrote:
> It was hard for me to understand the diff\patch without the original file in plain text.
> < GuessMainPID=no
> < RemainAfterExit=no
> < PIDFile=/var/run/squid.pid
> < SuccessExitStatus=5 6
> < ExecStart=/etc/init.d/squid start
> < ExecStop=/etc/init.d/squid stop
> < ExecReload=/etc/init.d/squid reload
> 
> The above now makes more sense but.. replacing the service file and removing the /etc/init.d/squid file should be the real way to run squid.

That assumes that systemd is going to be the init. Debian allows user
replacement of the init system with at least three to choose from. So
multiple init files need to be installed. This is just how it handles
the absence of a .service file for the newer default init while packages
transition to supporting both.

Amos


From robertocarna36 at gmail.com  Tue Jun 12 12:49:44 2018
From: robertocarna36 at gmail.com (Roberto Carna)
Date: Tue, 12 Jun 2018 09:49:44 -0300
Subject: [squid-users] BIND for complementary records for the same
	authoritative domain
Message-ID: <CAG2Qp6ttdYncHt3LSg-icwRg0L5f32vcUX3w7-0Ef8s=QK23Tg@mail.gmail.com>

Dear, our company has an internal Windows DNS with the "company.com"
authoritative domain. Suppose within it we have the following records:

a.company.com
b.company.com
c.company.com

Now we need to have several records maintained by other IT area
exclusively, in the same autoritative domain "company.com", so let's
say:

x.company.com
y.company.com
z.company.com

Is it possible to build a BIND DNS server for these last records, and
tell Windows DNS server something like this:

"Search the record x.company.com within company.com, if it is not
there search this record in the BIND server".

Windows DNS server is setup in the clients computers, and it can
contact BIND server for records it doesn't contain for the same
authoritative domain.

Thanks a lot!!!


From vh1988 at yahoo.com.ar  Tue Jun 12 19:54:47 2018
From: vh1988 at yahoo.com.ar (Julian Perconti)
Date: Tue, 12 Jun 2018 16:54:47 -0300
Subject: [squid-users] SSL errors with Squid 3.5.27
In-Reply-To: <8a1755bc-5257-1db0-8430-ab8fc62fdb77@treenet.co.nz>
References: <010201d3ff7e$9b259510$d170bf30$@yahoo.com.ar>
 <9fcd2760-281d-63f2-52e4-983ac8c5b052@treenet.co.nz>
 <01b501d40008$fc418e00$f4c4aa00$@yahoo.com.ar>
 <8a1755bc-5257-1db0-8430-ab8fc62fdb77@treenet.co.nz>
Message-ID: <00f801d40287$39940730$acbc1590$@yahoo.com.ar>

>Interesting.
>
>The main issue was that you configured only params for the Diffi-Helman (DH and DHE) ciphers - no >curve name. That meant your specified EEC* ciphers were disabled since they require a curve name as >well.
>
>Removing this option completely disables both DH and ECDH cipher types.
>Leaving your proxy with only the RSA based ciphers.
>
>Amos

kid1| Error negotiating SSL on FD 60: error:14007086:SSL routines:CONNECT_CR_CERT:certificate verify failed (1/-1/0)

Hi Amos,

I still have no look to connect with WhatsApp from iOS.

How do I can track this error?:

kid1| Error negotiating SSL on FD 60: error:14007086:SSL routines:CONNECT_CR_CERT:certificate verify failed (1/-1/0)

I mean examine the FD, ...or.. what? How? Because from iOS i cant see any error, it just tries to connect indefinitely.

Some whatsapp/Facebook server with the command:

Openssl s_client -connect -showcerts x.x.x.x:443 

Does not shows any cert and establishes a connection with TLS 1.2...

Any idea?

Thank You



From belle at bazuin.nl  Wed Jun 13 08:19:41 2018
From: belle at bazuin.nl (=?windows-1252?Q?L.P.H._van_Belle?=)
Date: Wed, 13 Jun 2018 10:19:41 +0200
Subject: [squid-users] SSL errors with Squid 3.5.27
In-Reply-To: <00f801d40287$39940730$acbc1590$@yahoo.com.ar>
References: <8a1755bc-5257-1db0-8430-ab8fc62fdb77@treenet.co.nz>
Message-ID: <vmime.5b20d39d.15e.6d5721447348f75c@ms249-lin-003.rotterdam.bazuin.nl>

Hai, 

I would say facebook protected there certificates with TSLA. 
Then you cant use ssl bump if im correct. 

Greetz, 

Louis 

> -----Oorspronkelijk bericht-----
> Van: squid-users 
> [mailto:squid-users-bounces at lists.squid-cache.org] Namens 
> Julian Perconti
> Verzonden: dinsdag 12 juni 2018 21:55
> Aan: squid-users at lists.squid-cache.org
> Onderwerp: Re: [squid-users] SSL errors with Squid 3.5.27
> 
> >Interesting.
> >
> >The main issue was that you configured only params for the 
> Diffi-Helman (DH and DHE) ciphers - no >curve name. That 
> meant your specified EEC* ciphers were disabled since they 
> require a curve name as >well.
> >
> >Removing this option completely disables both DH and ECDH 
> cipher types.
> >Leaving your proxy with only the RSA based ciphers.
> >
> >Amos
> 
> kid1| Error negotiating SSL on FD 60: error:14007086:SSL 
> routines:CONNECT_CR_CERT:certificate verify failed (1/-1/0)
> 
> Hi Amos,
> 
> I still have no look to connect with WhatsApp from iOS.
> 
> How do I can track this error?:
> 
> kid1| Error negotiating SSL on FD 60: error:14007086:SSL 
> routines:CONNECT_CR_CERT:certificate verify failed (1/-1/0)
> 
> I mean examine the FD, ...or.. what? How? Because from iOS i 
> cant see any error, it just tries to connect indefinitely.
> 
> Some whatsapp/Facebook server with the command:
> 
> Openssl s_client -connect -showcerts x.x.x.x:443 
> 
> Does not shows any cert and establishes a connection with TLS 1.2...
> 
> Any idea?
> 
> Thank You
> 
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
> 



From tiraen at gmail.com  Wed Jun 13 08:51:56 2018
From: tiraen at gmail.com (Tiraen)
Date: Wed, 13 Jun 2018 11:51:56 +0300
Subject: [squid-users] Question about traffic calculate
In-Reply-To: <CANhj9ozzyfO8m12R5BTRRhq5K+6ojaNFzNSapV_jfPifnwFZOQ@mail.gmail.com>
References: <CANhj9oyRn5GvSDjUa405Tz90X8kZY4mEoWPv341M1eUFbr2WfQ@mail.gmail.com>
 <4dd5dbf7-661a-3378-33be-a7a9da2912cf@treenet.co.nz>
 <CANhj9oyvYkuxk5pWTT=FPFggZ2kmcFa+Gu3yUXvNiURmouCxmg@mail.gmail.com>
 <fa5edf17-c0ed-4ac1-c5fe-b77839752dfc@treenet.co.nz>
 <CANhj9oywS173VafQhNa1S4_qznc-hmadoOiSRLH1HEPef_ORdQ@mail.gmail.com>
 <CANhj9ozSz6k0HMFYHUQLYw473j-NP8Vau=j7-akephH-AGO+ow@mail.gmail.com>
 <af7646cf-0fe6-6194-16d7-3fcb3f4622e1@treenet.co.nz>
 <df41334f-0abf-2b10-2ae1-047325461c09@integrafin.co.uk>
 <CANhj9ozzyfO8m12R5BTRRhq5K+6ojaNFzNSapV_jfPifnwFZOQ@mail.gmail.com>
Message-ID: <CANhj9oxpuTwtTEM1MdXeM=j0CNda+rw3RzxCfg4UhWTqczmW9w@mail.gmail.com>

either such a question, perhaps someone in the course

in the SQUID is still not implemented radius accounting?

Maybe there are any third-party modules working correctly?

2018-06-08 22:55 GMT+03:00 Tiraen <tiraen at gmail.com>:

>
>
>
>
>
>
> *What is "online mode" ?> > Perhaps there are other solutions besides log
> parsing?What information are you trying to get exactly? *
>
> There are actual data on incoming / outgoing traffic per user (when
> autorization by login is on)
>
> In principle, all the data is, except for the thing that I wrote above -
> and I do not understand whether it is possible through this method to get
> the actual
>
>
>
> *SNMP is built in to squid.  *
> Can I get data on traffic by users? If it's not difficult to give a link
> to a piece of documentation ?
>
>
> 2018-06-08 20:04 GMT+03:00 Alex Crow <acrow at integrafin.co.uk>:
>
>>
>>
>> On 08/06/18 17:29, Amos Jeffries wrote:
>>
>>> On 09/06/18 02:56, Tiraen wrote:
>>>
>>>> Small clarification
>>>>
>>>> If the normal behavior of the proxy server described above is correct,
>>>> then maybe there are other methods of gathering information on traffic
>>>> in online mode?
>>>>
>>> What is "online mode" ?
>>>
>>
>> SNMP is built in to squid. You can use it in conjunction with net-snmp
>> proxy mode to gather far more granular performance/caching/response
>> time/per-ip stats than squidclient or logs if that's what you're after.
>>
>>
>> --
>> This message is intended only for the addressee and may contain
>> confidential information. Unless you are that person, you may not
>> disclose its contents or use it in any way and are requested to delete
>> the message along with any attachments and notify us immediately.
>> This email is not intended to, nor should it be taken to, constitute
>> advice.
>> The information provided is correct to our knowledge & belief and must not
>> be used as a substitute for obtaining tax, regulatory, investment, legal
>> or
>> any other appropriate advice.
>>
>> "Transact" is operated by Integrated Financial Arrangements Ltd.
>> 29 Clement's Lane, London EC4N 7AE. Tel: (020) 7608 4900 Fax: (020) 7608
>> 5300.
>> (Registered office: as above; Registered in England and Wales under
>> number: 3727592). Authorised and regulated by the Financial Conduct
>> Authority (entered on the Financial Services Register; no. 190856).
>>
>> _______________________________________________
>> squid-users mailing list
>> squid-users at lists.squid-cache.org
>> http://lists.squid-cache.org/listinfo/squid-users
>>
>
>
>
> --
> With best regards,
>
> Vyacheslav Yakushev,
>
> Unix system administrator
>
> https://t.me/kelewind
>



-- 
With best regards,

Vyacheslav Yakushev,

Unix system administrator

https://t.me/kelewind
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20180613/6f0a517c/attachment.htm>

From uhlar at fantomas.sk  Wed Jun 13 09:54:54 2018
From: uhlar at fantomas.sk (Matus UHLAR - fantomas)
Date: Wed, 13 Jun 2018 11:54:54 +0200
Subject: [squid-users] Question about traffic calculate
In-Reply-To: <CANhj9oxpuTwtTEM1MdXeM=j0CNda+rw3RzxCfg4UhWTqczmW9w@mail.gmail.com>
References: <CANhj9oyRn5GvSDjUa405Tz90X8kZY4mEoWPv341M1eUFbr2WfQ@mail.gmail.com>
 <4dd5dbf7-661a-3378-33be-a7a9da2912cf@treenet.co.nz>
 <CANhj9oyvYkuxk5pWTT=FPFggZ2kmcFa+Gu3yUXvNiURmouCxmg@mail.gmail.com>
 <fa5edf17-c0ed-4ac1-c5fe-b77839752dfc@treenet.co.nz>
 <CANhj9oywS173VafQhNa1S4_qznc-hmadoOiSRLH1HEPef_ORdQ@mail.gmail.com>
 <CANhj9ozSz6k0HMFYHUQLYw473j-NP8Vau=j7-akephH-AGO+ow@mail.gmail.com>
 <af7646cf-0fe6-6194-16d7-3fcb3f4622e1@treenet.co.nz>
 <df41334f-0abf-2b10-2ae1-047325461c09@integrafin.co.uk>
 <CANhj9ozzyfO8m12R5BTRRhq5K+6ojaNFzNSapV_jfPifnwFZOQ@mail.gmail.com>
 <CANhj9oxpuTwtTEM1MdXeM=j0CNda+rw3RzxCfg4UhWTqczmW9w@mail.gmail.com>
Message-ID: <20180613095453.GA26516@fantomas.sk>

On 13.06.18 11:51, Tiraen wrote:
>either such a question, perhaps someone in the course
>
>in the SQUID is still not implemented radius accounting?

authentication - yes. But squid doese not support accounting (afaik).

>Maybe there are any third-party modules working correctly?

maybe iCAP module.

-- 
Matus UHLAR - fantomas, uhlar at fantomas.sk ; http://www.fantomas.sk/
Warning: I wish NOT to receive e-mail advertising to this address.
Varovanie: na tuto adresu chcem NEDOSTAVAT akukolvek reklamnu postu.
Micro$oft random number generator: 0, 0, 0, 4.33e+67, 0, 0, 0...


From tiraen at gmail.com  Wed Jun 13 10:26:08 2018
From: tiraen at gmail.com (Tiraen)
Date: Wed, 13 Jun 2018 13:26:08 +0300
Subject: [squid-users] Question about traffic calculate
In-Reply-To: <20180613095453.GA26516@fantomas.sk>
References: <CANhj9oyRn5GvSDjUa405Tz90X8kZY4mEoWPv341M1eUFbr2WfQ@mail.gmail.com>
 <4dd5dbf7-661a-3378-33be-a7a9da2912cf@treenet.co.nz>
 <CANhj9oyvYkuxk5pWTT=FPFggZ2kmcFa+Gu3yUXvNiURmouCxmg@mail.gmail.com>
 <fa5edf17-c0ed-4ac1-c5fe-b77839752dfc@treenet.co.nz>
 <CANhj9oywS173VafQhNa1S4_qznc-hmadoOiSRLH1HEPef_ORdQ@mail.gmail.com>
 <CANhj9ozSz6k0HMFYHUQLYw473j-NP8Vau=j7-akephH-AGO+ow@mail.gmail.com>
 <af7646cf-0fe6-6194-16d7-3fcb3f4622e1@treenet.co.nz>
 <df41334f-0abf-2b10-2ae1-047325461c09@integrafin.co.uk>
 <CANhj9ozzyfO8m12R5BTRRhq5K+6ojaNFzNSapV_jfPifnwFZOQ@mail.gmail.com>
 <CANhj9oxpuTwtTEM1MdXeM=j0CNda+rw3RzxCfg4UhWTqczmW9w@mail.gmail.com>
 <20180613095453.GA26516@fantomas.sk>
Message-ID: <CANhj9oze9XznmdBSFAU_qct7v_HdFm45ar-caVhi=rys5epyog@mail.gmail.com>

ICAP will help provide data on incoming / outgoing traffic?

2018-06-13 12:54 GMT+03:00 Matus UHLAR - fantomas <uhlar at fantomas.sk>:

> On 13.06.18 11:51, Tiraen wrote:
>
>> either such a question, perhaps someone in the course
>>
>> in the SQUID is still not implemented radius accounting?
>>
>
> authentication - yes. But squid doese not support accounting (afaik).
>
> Maybe there are any third-party modules working correctly?
>>
>
> maybe iCAP module.
>
> --
> Matus UHLAR - fantomas, uhlar at fantomas.sk ; http://www.fantomas.sk/
> Warning: I wish NOT to receive e-mail advertising to this address.
> Varovanie: na tuto adresu chcem NEDOSTAVAT akukolvek reklamnu postu.
> Micro$oft random number generator: 0, 0, 0, 4.33e+67, 0, 0, 0...
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>



-- 
With best regards,

Vyacheslav Yakushev,

Unix system administrator

https://t.me/kelewind
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20180613/055970d9/attachment.htm>

From jlay at slave-tothe-box.net  Wed Jun 13 12:27:55 2018
From: jlay at slave-tothe-box.net (James Lay)
Date: Wed, 13 Jun 2018 06:27:55 -0600
Subject: [squid-users] Squid and systemd
Message-ID: <1c59ab2a4caf2cb93a54e64655863b3baf61f17d.camel@slave-tothe-box.net>

Well....I'll just say up front that systemd is not my friend.  When
running squid via cli:  sudo /opt/squid/sbin/squid it runs like a
champ.  But using the service file at:
https://raw.githubusercontent.com/squid-cache/squid/master/tools/system
d/squid.service
it times out after a few:
06:20:11 gateway squid[3669]: Created PID file
(/opt/squid/var/run/squid.pid)06:20:11 gateway squid[3669]: Squid
Parent: will start 1 kids06:20:11 gateway squid[3669]: Squid Parent:
(squid-1) process 3678 started06:20:11 gateway squid[3678]: Set Current
Directory to /opt/squid/var06:20:11 gateway squid[3678]: Starting Squid
Cache version 4.0.24 for x86_64-pc-linux-gnu...06:20:11 gateway
squid[3678]: Service Name: squid06:20:11 gateway squid[3678]: Process
ID 367806:20:11 gateway squid[3678]: Process Roles: worker06:20:11
gateway squid[3678]: With 1024 file descriptors available06:20:11
gateway squid[3678]: Initializing IP Cache...06:20:11 gateway
squid[3678]: DNS Socket created at [::], FD 506:20:11 gateway
squid[3678]: DNS Socket created at 0.0.0.0, FD 1006:20:11 gateway
squid[3678]: Adding nameserver 192.168.1.253 from
/etc/resolv.conf06:20:11 gateway squid[3678]: Adding nameserver
205.171.3.65 from /etc/resolv.conf06:20:11 gateway squid[3678]: Adding
nameserver 205.171.2.65 from /etc/resolv.conf06:20:11 gateway
squid[3678]: Adding domain slave-tothe-box.net from
/etc/resolv.conf06:20:11 gateway squid[3678]: Adding domain slave-
tothe-box.net from /etc/resolv.conf06:20:11 gateway squid[3678]:
helperOpenServers: Starting 5/5 'security_file_certgen'
processes06:20:11 gateway squid[3678]: Logfile: opening log
syslog:daemon.info06:20:11 gateway squid[3678]: Store logging
disabled06:20:11 gateway squid[3678]: Swap maxSize 0 + 262144 KB,
estimated 20164 objects06:20:11 gateway squid[3678]: Target number of
buckets: 100806:20:11 gateway squid[3678]: Using 8192 Store
buckets06:20:11 gateway squid[3678]: Max Mem  size: 262144 KB06:20:11
gateway squid[3678]: Max Swap size: 0 KB06:20:11 gateway squid[3678]:
Using Least Load store dir selection06:20:11 gateway squid[3678]: Set
Current Directory to /opt/squid/var06:20:11 gateway squid[3678]:
Finished loading MIME types and icons.06:20:11 gateway squid[3678]:
HTCP Disabled.06:20:11 gateway squid[3678]: Squid plugin modules
loaded: 006:20:11 gateway squid[3678]: Adaptation support is
off.06:20:11 gateway squid[3678]: Accepting HTTP Socket connections at
local=x.x.x.x:3127 remote=[::] FD 21 flags=906:20:11 gateway
squid[3678]: Accepting NAT intercepted HTTP Socket connections at
local=x.x.x.x:3128 remote=[::] FD 22 flags=4106:20:11 gateway
squid[3678]: Accepting NAT intercepted SSL bumped HTTPS Socket
connections at local=x.x.x.x:3129 remote=[::] FD 23 flags=4106:20:12
gateway squid[3678]: storeLateRelease: released 0 objects06:21:41
gateway systemd[1]: squid.service: Start operation timed out.
Terminating.06:21:41 gateway systemd[1]: squid.service: Killing process
3669 (squid) with signal SIGKILL.06:21:41 gateway sudo:
pam_unix(sudo:session): session closed for user root06:21:41 gateway
systemd[1]: squid.service: Killing process 3678 (squid) with signal
SIGKILL.06:21:41 gateway jlay[2415] 192.168.1.2 46692 192.168.1.252 22:
sudo systemctl start squid06:21:41 gateway systemd[1]: squid.service:
Killing process 3680 (security_file_c) with signal SIGKILL.06:21:41
gateway systemd[1]: squid.service: Killing process 3682
(security_file_c) with signal SIGKILL.06:21:41 gateway systemd[1]:
squid.service: Killing process 3683 (security_file_c) with signal
SIGKILL.06:21:41 gateway systemd[1]: squid.service: Killing process
3684 (security_file_c) with signal SIGKILL.06:21:41 gateway systemd[1]:
squid.service: Killing process 3685 (security_file_c) with signal
SIGKILL.06:21:41 gateway systemd[1]: squid.service: Failed with result
'timeout'.06:21:41 gateway systemd[1]: Failed to start Squid Web Proxy
Server.
I've modded the service file to reflect different binary location, but
that's about it.  Thank you.
James
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20180613/bd80c49b/attachment.htm>

From marcus.kool at urlfilterdb.com  Wed Jun 13 13:03:13 2018
From: marcus.kool at urlfilterdb.com (Marcus Kool)
Date: Wed, 13 Jun 2018 10:03:13 -0300
Subject: [squid-users] Squid and systemd
In-Reply-To: <1c59ab2a4caf2cb93a54e64655863b3baf61f17d.camel@slave-tothe-box.net>
References: <1c59ab2a4caf2cb93a54e64655863b3baf61f17d.camel@slave-tothe-box.net>
Message-ID: <187e7a50-1003-271b-5ae4-f4772d743f22@urlfilterdb.com>

I have seen systemd killing daemons when it times out waiting for the pid file to appear.
I suggest to doublecheck that the pid filename in the service file and in squid.conf are the same.

Marcus

On 13/06/18 09:27, James Lay wrote:
> Well....I'll just say up front that systemd is not my friend. When running squid via cli: sudo /opt/squid/sbin/squid it runs like a champ. But using the service file at:
> 
> https://raw.githubusercontent.com/squid-cache/squid/master/tools/systemd/squid.service
> 
> it times out after a few:
> 
> 06:20:11 gateway squid[3669]: Created PID file (/opt/squid/var/run/squid.pid)
> 06:20:11 gateway squid[3669]: Squid Parent: will start 1 kids
> 06:20:11 gateway squid[3669]: Squid Parent: (squid-1) process 3678 started
> 06:20:11 gateway squid[3678]: Set Current Directory to /opt/squid/var
> 06:20:11 gateway squid[3678]: Starting Squid Cache version 4.0.24 for x86_64-pc-linux-gnu...
> 06:20:11 gateway squid[3678]: Service Name: squid
> 06:20:11 gateway squid[3678]: Process ID 3678
> 06:20:11 gateway squid[3678]: Process Roles: worker
> 06:20:11 gateway squid[3678]: With 1024 file descriptors available
> 06:20:11 gateway squid[3678]: Initializing IP Cache...
> 06:20:11 gateway squid[3678]: DNS Socket created at [::], FD 5
> 06:20:11 gateway squid[3678]: DNS Socket created at 0.0.0.0, FD 10
> 06:20:11 gateway squid[3678]: Adding nameserver 192.168.1.253 from /etc/resolv.conf
> 06:20:11 gateway squid[3678]: Adding nameserver 205.171.3.65 from /etc/resolv.conf
> 06:20:11 gateway squid[3678]: Adding nameserver 205.171.2.65 from /etc/resolv.conf
> 06:20:11 gateway squid[3678]: Adding domain slave-tothe-box.net from /etc/resolv.conf
> 06:20:11 gateway squid[3678]: Adding domain slave-tothe-box.net from /etc/resolv.conf
> 06:20:11 gateway squid[3678]: helperOpenServers: Starting 5/5 'security_file_certgen' processes
> 06:20:11 gateway squid[3678]: Logfile: opening log syslog:daemon.info
> 06:20:11 gateway squid[3678]: Store logging disabled
> 06:20:11 gateway squid[3678]: Swap maxSize 0 + 262144 KB, estimated 20164 objects
> 06:20:11 gateway squid[3678]: Target number of buckets: 1008
> 06:20:11 gateway squid[3678]: Using 8192 Store buckets
> 06:20:11 gateway squid[3678]: Max Mem??size: 262144 KB
> 06:20:11 gateway squid[3678]: Max Swap size: 0 KB
> 06:20:11 gateway squid[3678]: Using Least Load store dir selection
> 06:20:11 gateway squid[3678]: Set Current Directory to /opt/squid/var
> 06:20:11 gateway squid[3678]: Finished loading MIME types and icons.
> 06:20:11 gateway squid[3678]: HTCP Disabled.
> 06:20:11 gateway squid[3678]: Squid plugin modules loaded: 0
> 06:20:11 gateway squid[3678]: Adaptation support is off.
> 06:20:11 gateway squid[3678]: Accepting HTTP Socket connections at local=x.x.x.x:3127 remote=[::] FD 21 flags=9
> 06:20:11 gateway squid[3678]: Accepting NAT intercepted HTTP Socket connections at local=x.x.x.x:3128 remote=[::] FD 22 flags=41
> 06:20:11 gateway squid[3678]: Accepting NAT intercepted SSL bumped HTTPS Socket connections at local=x.x.x.x:3129 remote=[::] FD 23 flags=41
> 06:20:12 gateway squid[3678]: storeLateRelease: released 0 objects
> 06:21:41 gateway systemd[1]: squid.service: Start operation timed out. Terminating.
> 06:21:41 gateway systemd[1]: squid.service: Killing process 3669 (squid) with signal SIGKILL.
> 06:21:41 gateway sudo: pam_unix(sudo:session): session closed for user root
> 06:21:41 gateway systemd[1]: squid.service: Killing process 3678 (squid) with signal SIGKILL.
> 06:21:41 gateway jlay[2415] 192.168.1.2 46692 192.168.1.252 22: sudo systemctl start squid
> 06:21:41 gateway systemd[1]: squid.service: Killing process 3680 (security_file_c) with signal SIGKILL.
> 06:21:41 gateway systemd[1]: squid.service: Killing process 3682 (security_file_c) with signal SIGKILL.
> 06:21:41 gateway systemd[1]: squid.service: Killing process 3683 (security_file_c) with signal SIGKILL.
> 06:21:41 gateway systemd[1]: squid.service: Killing process 3684 (security_file_c) with signal SIGKILL.
> 06:21:41 gateway systemd[1]: squid.service: Killing process 3685 (security_file_c) with signal SIGKILL.
> 06:21:41 gateway systemd[1]: squid.service: Failed with result 'timeout'.
> 06:21:41 gateway systemd[1]: Failed to start Squid Web Proxy Server.
> 
> I've modded the service file to reflect different binary location, but that's about it. Thank you.
> 
> James
> 
> 
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
> 


From uhlar at fantomas.sk  Wed Jun 13 13:09:11 2018
From: uhlar at fantomas.sk (Matus UHLAR - fantomas)
Date: Wed, 13 Jun 2018 15:09:11 +0200
Subject: [squid-users] Question about traffic calculate
In-Reply-To: <CANhj9oze9XznmdBSFAU_qct7v_HdFm45ar-caVhi=rys5epyog@mail.gmail.com>
References: <CANhj9oyvYkuxk5pWTT=FPFggZ2kmcFa+Gu3yUXvNiURmouCxmg@mail.gmail.com>
 <fa5edf17-c0ed-4ac1-c5fe-b77839752dfc@treenet.co.nz>
 <CANhj9oywS173VafQhNa1S4_qznc-hmadoOiSRLH1HEPef_ORdQ@mail.gmail.com>
 <CANhj9ozSz6k0HMFYHUQLYw473j-NP8Vau=j7-akephH-AGO+ow@mail.gmail.com>
 <af7646cf-0fe6-6194-16d7-3fcb3f4622e1@treenet.co.nz>
 <df41334f-0abf-2b10-2ae1-047325461c09@integrafin.co.uk>
 <CANhj9ozzyfO8m12R5BTRRhq5K+6ojaNFzNSapV_jfPifnwFZOQ@mail.gmail.com>
 <CANhj9oxpuTwtTEM1MdXeM=j0CNda+rw3RzxCfg4UhWTqczmW9w@mail.gmail.com>
 <20180613095453.GA26516@fantomas.sk>
 <CANhj9oze9XznmdBSFAU_qct7v_HdFm45ar-caVhi=rys5epyog@mail.gmail.com>
Message-ID: <20180613130911.GA2947@fantomas.sk>

On 13.06.18 13:26, Tiraen wrote:
>ICAP will help provide data on incoming / outgoing traffic?

icap can get the data and work with it.

you don't have to manipulate, just do the accounting.

you just need ICAP module that will do it.


>2018-06-13 12:54 GMT+03:00 Matus UHLAR - fantomas <uhlar at fantomas.sk>:
>
>> On 13.06.18 11:51, Tiraen wrote:
>>
>>> either such a question, perhaps someone in the course
>>>
>>> in the SQUID is still not implemented radius accounting?
>>>
>>
>> authentication - yes. But squid doese not support accounting (afaik).
>>
>> Maybe there are any third-party modules working correctly?
>>>
>>
>> maybe iCAP module.

-- 
Matus UHLAR - fantomas, uhlar at fantomas.sk ; http://www.fantomas.sk/
Warning: I wish NOT to receive e-mail advertising to this address.
Varovanie: na tuto adresu chcem NEDOSTAVAT akukolvek reklamnu postu.
REALITY.SYS corrupted. Press any key to reboot Universe.


From rousskov at measurement-factory.com  Wed Jun 13 15:35:58 2018
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Wed, 13 Jun 2018 09:35:58 -0600
Subject: [squid-users] Question about traffic calculate
In-Reply-To: <20180613130911.GA2947@fantomas.sk>
References: <CANhj9oyvYkuxk5pWTT=FPFggZ2kmcFa+Gu3yUXvNiURmouCxmg@mail.gmail.com>
 <fa5edf17-c0ed-4ac1-c5fe-b77839752dfc@treenet.co.nz>
 <CANhj9oywS173VafQhNa1S4_qznc-hmadoOiSRLH1HEPef_ORdQ@mail.gmail.com>
 <CANhj9ozSz6k0HMFYHUQLYw473j-NP8Vau=j7-akephH-AGO+ow@mail.gmail.com>
 <af7646cf-0fe6-6194-16d7-3fcb3f4622e1@treenet.co.nz>
 <df41334f-0abf-2b10-2ae1-047325461c09@integrafin.co.uk>
 <CANhj9ozzyfO8m12R5BTRRhq5K+6ojaNFzNSapV_jfPifnwFZOQ@mail.gmail.com>
 <CANhj9oxpuTwtTEM1MdXeM=j0CNda+rw3RzxCfg4UhWTqczmW9w@mail.gmail.com>
 <20180613095453.GA26516@fantomas.sk>
 <CANhj9oze9XznmdBSFAU_qct7v_HdFm45ar-caVhi=rys5epyog@mail.gmail.com>
 <20180613130911.GA2947@fantomas.sk>
Message-ID: <df89473e-4d15-0566-e800-b0431031517e@measurement-factory.com>

On 06/13/2018 07:09 AM, Matus UHLAR - fantomas wrote:
> On 13.06.18 13:26, Tiraen wrote:
>> ICAP will help provide data on incoming / outgoing traffic?

> icap can get the data and work with it.
> you don't have to manipulate, just do the accounting.
> you just need ICAP module that will do it.


Yes, it is possible to collect more-or-less accurate incoming request
and incoming response stats using an ICAP service, but doing so would be
very inefficient. Using eCAP would improve performance, but interpreting
live access.log streams is probably the most efficient way of doing this.

IIRC, both eCAP and ICAP interfaces do not see the exact incoming
requests and incoming responses because Squid may strip hop-by-hop HTTP
headers and decode chunked HTTP message bodies before forwarding the
incoming message to the adaptation service. If you need exact headers
and exact body sizes, then you need more than just the basic ICAP and
eCAP interface. Again, access.log is probably an overall better choice
for capturing that info.

Both eCAP and ICAP interfaces do not see outgoing requests and outgoing
responses because Squid only supports pre-cache vectoring points.


HTH,

Alex.
P.S. In the above, "incoming" is "to Squid" and "outgoing" is "from Squid".


>> 2018-06-13 12:54 GMT+03:00 Matus UHLAR - fantomas <uhlar at fantomas.sk>:
>>
>>> On 13.06.18 11:51, Tiraen wrote:
>>>
>>>> either such a question, perhaps someone in the course
>>>>
>>>> in the SQUID is still not implemented radius accounting?
>>>>
>>>
>>> authentication - yes. But squid doese not support accounting (afaik).
>>>
>>> Maybe there are any third-party modules working correctly?
>>>>
>>>
>>> maybe iCAP module.




From squid3 at treenet.co.nz  Wed Jun 13 17:16:42 2018
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 14 Jun 2018 05:16:42 +1200
Subject: [squid-users] SSL errors with Squid 3.5.27
In-Reply-To: <00f801d40287$39940730$acbc1590$@yahoo.com.ar>
References: <010201d3ff7e$9b259510$d170bf30$@yahoo.com.ar>
 <9fcd2760-281d-63f2-52e4-983ac8c5b052@treenet.co.nz>
 <01b501d40008$fc418e00$f4c4aa00$@yahoo.com.ar>
 <8a1755bc-5257-1db0-8430-ab8fc62fdb77@treenet.co.nz>
 <00f801d40287$39940730$acbc1590$@yahoo.com.ar>
Message-ID: <48966683-1714-9daa-673a-594a0a06609c@treenet.co.nz>

On 13/06/18 07:54, Julian Perconti wrote:
>> Interesting.
>>
>> The main issue was that you configured only params for the Diffi-Helman (DH and DHE) ciphers - no >curve name. That meant your specified EEC* ciphers were disabled since they require a curve name as >well.
>>
>> Removing this option completely disables both DH and ECDH cipher types.
>> Leaving your proxy with only the RSA based ciphers.
>>
>> Amos
> 
> kid1| Error negotiating SSL on FD 60: error:14007086:SSL routines:CONNECT_CR_CERT:certificate verify failed (1/-1/0)
> 
> Hi Amos,
> 
> I still have no look to connect with WhatsApp from iOS.
> 
> How do I can track this error?:
> 
> kid1| Error negotiating SSL on FD 60: error:14007086:SSL routines:CONNECT_CR_CERT:certificate verify failed (1/-1/0)
> 
> I mean examine the FD, ...or.. what? How? Because from iOS i cant see any error, it just tries to connect indefinitely.

Yes. With "debug_options ALL,9" and a "grep --context=10 'FD nn'" f the
resulting cache.log for whatever the FD number is in the test after you
update the logging content. Some of those lines should show what is
happening on that FD, maybe some clues in there.


> 
> Some whatsapp/Facebook server with the command:
> 
> Openssl s_client -connect -showcerts x.x.x.x:443 
> 
> Does not shows any cert and establishes a connection with TLS 1.2...
> 
> Any idea?

Probably something you are not noticing, or think is irrelevant but
actually is.

Since you are hiding the details of what is going on we cannot replicate
and see for ourselves if there is any hint in those hidden results which
anyone with more knowledge might find.

Amos


From baretomas at protonmail.com  Wed Jun 13 19:28:27 2018
From: baretomas at protonmail.com (baretomas)
Date: Wed, 13 Jun 2018 12:28:27 -0700 (MST)
Subject: [squid-users] HTTPS cache for Java application - only getting
	TCP_MISS
Message-ID: <1528918107577-0.post@n4.nabble.com>

Hello,

I'm setting up a Squid proxy as a cache for a number (as many as possible)
of identical JAVA applications to run their web calls through. The calls are
ofc identical, and the response they get can safely be cached for 5-10
seconds. 
I do this because most of the calls is directed at a single server on the
internet that I don't want to hammer, since I will ofc be locked out of it
then.

Currently Im simply testing this on a single computer: the application and
squid

The calls from the application is done using ssl / https by telling java to
use Squid as a proxy (-Dhttps.proxyHost and -Dhttp.proxyHost). I've set up
squid and JAVA with self-signed certificates, and the application sends its
calls through squid and gets the reponse. No problem there (wasnt easy that
either I must say :P ).

The problem is that none of the calls get cached: All rows in the access.log
hava a TCP_MISS/200 tag in them. 

I've searched all through the web for a solution to this, and have tried
everything people have suggested. So I was hoping someone could help me?

Anyone have any tips on what to try?

MY config (note Ive set the refresh_pattern like that just to see if I could
catch anything. The plan is to modify it so it actualyl does refresh the
responses frmo the web calls in 5-10 seconds intervals. There are commented
out pats Ive tried with no luck there too):


#
# Recommended minimum configuration:
#

debug_options ALL,2

# Example rule allowing access from your local networks.
# Adapt to list your (internal) IP networks from where browsing
# should be allowed

acl localnet src 10.0.0.0/8	# RFC1918 possible internal network
#acl localnet src 172.16.0.0/12	# RFC1918 possible internal network
#acl localnet src 192.168.0.0/16	# RFC1918 possible internal network
acl localnet src fc00::/7       # RFC 4193 local private network range
acl localnet src fe80::/10      # RFC 4291 link-local (directly plugged)
machines

acl SSL_ports port 443
acl Safe_ports port 80		# http
acl Safe_ports port 21		# ftp
acl Safe_ports port 443		# https
acl Safe_ports port 70		# gopher
acl Safe_ports port 210		# wais
acl Safe_ports port 1025-65535	# unregistered ports
acl Safe_ports port 280		# http-mgmt
acl Safe_ports port 488		# gss-http
acl Safe_ports port 591		# filemaker
acl Safe_ports port 777		# multiling http
acl CONNECT method CONNECT

#
# Recommended minimum Access Permission configuration:
#

# Only allow cachemgr access from localhost
http_access allow localhost manager
http_access deny manager

# Deny requests to certain unsafe ports
http_access deny !Safe_ports

# Deny CONNECT to other than secure SSL ports
http_access deny CONNECT !SSL_ports

# We strongly recommend the following be uncommented to protect innocent
# web applications running on the proxy server who think the only
# one who can access services on "localhost" is a local user
#http_access deny to_localhost

#
# INSERT YOUR OWN RULE(S) HERE TO ALLOW ACCESS FROM YOUR CLIENTS
#



# Example rule allowing access from your local networks.
# Adapt localnet in the ACL section to list your (internal) IP networks
# from where browsing should be allowed
http_access allow localnet
http_access allow localhost

# And finally deny all other access to this proxy
http_access deny all


# Squid normally listens to port 3128
#http_port 3128 ssl-bump generate-host-certificates=on
dynamic_cert_mem_cache_size=4MB cert=/cygdrive/c/squid/etc/squid/correct.pem
key=/cygdrive/c/squid/etc/squid/ssl/myca.key 

http_port 3128 ssl-bump generate-host-certificates=on
dynamic_cert_mem_cache_size=4MB
cert=/cygdrive/c/squid/etc/squid/proxyCAx.pem
key=/cygdrive/c/squid/etc/squid/proxyCA.pem

#https_port 3129 cert=/cygdrive/c/squid/etc/squid/proxyCAx.pem
key=/cygdrive/c/squid/etc/squid/proxyCA.pem



# Uncomment the line below to enable disk caching - path format is
/cygdrive/<full path to cache folder>, i.e.
#cache_dir aufs /cygdrive/c/squid/var/cache/ 3000 16 256

# certificate generation program
sslcrtd_program /cygdrive/c/squid/lib/squid/ssl_crtd -s
/cygdrive/c/squid/var/cache/squid_ssldb -M 4MB

# Leave coredumps in the first cache dir
coredump_dir /var/cache/squid

# Add any of your own refresh_pattern entries above these.
#refresh_pattern ^ftp:		1440	20%	10080
#refresh_pattern ^gopher:	1440	0%	1440
#refresh_pattern -i (/cgi-bin/|\?) 0	0%	0
#refresh_pattern -i (/cgi-bin/|\?) 1440 100% 4320 ignore-no-store
override-lastmod override-expire ignore-must-revalidate ignore-reload
ignore-private ignore-auth
refresh_pattern .		1440	100%	4320 ignore-no-store override-lastmod
override-expire ignore-must-revalidate ignore-reload ignore-private
ignore-auth override-lastmod 

# Bumped requests have relative URLs so Squid has to use reverse proxy
# or accelerator code. By default, that code denies direct forwarding.
# The need for this option may disappear in the future.
#always_direct allow all

dns_nameservers 8.8.8.8 208.67.222.222

max_filedescriptors 3200

# Max Object Size Cache
maximum_object_size 10240 KB


acl step1 at_step SslBump1

ssl_bump peek step1
ssl_bump bump all


#acl step1 at_step SslBump1
#acl step2 at_step SslBump2
#acl step3 at_step SslBump3

#acl ssl_exclude_domains ssl::server_name
"/cygdrive/c/squid/etc/squid/ssl_exclude_domains.conf"
#acl ssl_exclude_ips     dst             
"/cygdrive/c/squid/etc/squid/ssl_exclude_ips.conf"

#ssl_bump splice localhost
#ssl_bump peek step1 all
#ssl_bump splice ssl_exclude_domains
#ssl_bump splice ssl_exclude_ips
#ssl_bump stare step2 all
#ssl_bump bump all




--
Sent from: http://squid-web-proxy-cache.1019090.n4.nabble.com/Squid-Users-f1019091.html


From Antony.Stone at squid.open.source.it  Wed Jun 13 19:37:08 2018
From: Antony.Stone at squid.open.source.it (Antony Stone)
Date: Wed, 13 Jun 2018 21:37:08 +0200
Subject: [squid-users] HTTPS cache for Java application - only getting
	TCP_MISS
In-Reply-To: <1528918107577-0.post@n4.nabble.com>
References: <1528918107577-0.post@n4.nabble.com>
Message-ID: <201806132137.08853.Antony.Stone@squid.open.source.it>

On Wednesday 13 June 2018 at 21:28:27, baretomas wrote:

> Hello,
> 
> I'm setting up a Squid proxy as a cache for a number (as many as possible)
> of identical JAVA applications to run their web calls through.

> The problem is that none of the calls get cached: All rows in the
> access.log hava a TCP_MISS/200 tag in them.
> 
> I've searched all through the web for a solution to this, and have tried
> everything people have suggested. So I was hoping someone could help me?

Show us the response you get (at least the full headers, content is neither 
here nor there) from the remote server.

My bet is that the website manager has used one or more "don't cache" 
directives which Squid is simply faithfully obeying.


Antony.

-- 
Please apologise my errors, since I have a very small device.

                                                   Please reply to the list;
                                                         please *don't* CC me.


From Antony.Stone at squid.open.source.it  Wed Jun 13 19:44:51 2018
From: Antony.Stone at squid.open.source.it (Antony Stone)
Date: Wed, 13 Jun 2018 21:44:51 +0200
Subject: [squid-users] HTTPS cache for Java application - only getting
	TCP_MISS
In-Reply-To: <1528918107577-0.post@n4.nabble.com>
References: <1528918107577-0.post@n4.nabble.com>
Message-ID: <201806132144.51339.Antony.Stone@squid.open.source.it>

On Wednesday 13 June 2018 at 21:28:27, baretomas wrote:

> The calls from the application is done using ssl / https by telling java to
> use Squid as a proxy (-Dhttps.proxyHost and -Dhttp.proxyHost).

Okay, but...

> http_port 3128 ssl-bump generate-host-certificates=on
> dynamic_cert_mem_cache_size=4MB
> cert=/cygdrive/c/squid/etc/squid/proxyCAx.pem
> key=/cygdrive/c/squid/etc/squid/proxyCA.pem

> # certificate generation program
> sslcrtd_program /cygdrive/c/squid/lib/squid/ssl_crtd -s
> /cygdrive/c/squid/var/cache/squid_ssldb -M 4MB

> acl step1 at_step SslBump1
> 
> ssl_bump peek step1
> ssl_bump bump all

Surely all this peeking and bumping is only needed if you're running Squid in 
interception mode, whereas you've said that you've configured your Java 
application to explicitly use Squid as a proxy?


Have you tried your Squid configuration with a plain browser, configured to use 
the proxy, with (a) a few random websites, and (b) the specific resource you're 
trying to access from your Java application, to see whether it is actually 
working as a caching proxy?


Antony.

-- 
This sentence contains exacly three erors.

                                                   Please reply to the list;
                                                         please *don't* CC me.


From vh1988 at yahoo.com.ar  Wed Jun 13 21:20:59 2018
From: vh1988 at yahoo.com.ar (Julian Perconti)
Date: Wed, 13 Jun 2018 18:20:59 -0300
Subject: [squid-users] SSL errors with Squid 3.5.27
In-Reply-To: <48966683-1714-9daa-673a-594a0a06609c@treenet.co.nz>
References: <010201d3ff7e$9b259510$d170bf30$@yahoo.com.ar>
 <9fcd2760-281d-63f2-52e4-983ac8c5b052@treenet.co.nz>
 <01b501d40008$fc418e00$f4c4aa00$@yahoo.com.ar>
 <8a1755bc-5257-1db0-8430-ab8fc62fdb77@treenet.co.nz>
 <00f801d40287$39940730$acbc1590$@yahoo.com.ar>
 <48966683-1714-9daa-673a-594a0a06609c@treenet.co.nz>
Message-ID: <00da01d4035c$6e7e1be0$4b7a53a0$@yahoo.com.ar>

>Yes. With "debug_options ALL,9" and a "grep --context=10 'FD nn'" f the resulting cache.log for whatever the FD number is in the test after you update the logging content. Some of those lines should show >what is happening on >that FD, maybe some clues in there.
>

OK Amos,
I Will try that debug options and then post here...

> 
> Some whatsapp/Facebook server with the command:
> 
> Openssl s_client -connect -showcerts x.x.x.x:443
> 
> Does not shows any cert and establishes a connection with TLS 1.2...
> 
> Any idea?
>
>Probably something you are not noticing, or think is irrelevant but actually is.
>
>Since you are hiding the details of what is going on we cannot replicate and see for ourselves if there is any hint in those hidden results which anyone with more knowledge might find.
>
>Amos

#####
Here a example:
#####

openssl s_client -connect 31.13.94.54:443
CONNECTED(00000003)
write:errno=104
---
no peer certificate available
---
No client certificate CA names sent
---
SSL handshake has read 0 bytes and written 290 bytes
---
New, (NONE), Cipher is (NONE)
Secure Renegotiation IS NOT supported
Compression: NONE
Expansion: NONE
SSL-Session:
    Protocol  : TLSv1.2
    Cipher    : 0000
    Session-ID:
    Session-ID-ctx:
    Master-Key:
    Key-Arg   : None
    PSK identity: None
    PSK identity hint: None
    SRP username: None
    Start Time: 1528924452
    Timeout   : 300 (sec)
    Verify return code: 0 (ok)

#####
And the whois that server:
#####

whois 31.13.94.54
% This is the RIPE Database query service.
% The objects are in RPSL format.
%
% The RIPE Database is subject to Terms and Conditions.
% See http://www.ripe.net/db/support/db-terms-conditions.pdf

% Note: this output has been filtered.
%       To receive output for a database update, use the "-B" flag.

% Information related to '31.13.94.0 - 31.13.94.255'

% Abuse contact for '31.13.94.0 - 31.13.94.255' is 'domain at fb.com'

inetnum:        31.13.94.0 - 31.13.94.255
netname:        MNL1
descr:          Facebook
country:        PH
admin-c:        RD4299-RIPE
tech-c:         RD4299-RIPE
status:         ASSIGNED PA
mnt-by:         fb-neteng
mnt-lower:      fb-neteng
mnt-routes:     fb-neteng
created:        2014-06-11T19:03:34Z
last-modified:  2014-06-11T19:03:34Z
source:         RIPE

role:           RIPE DBM
address:        1601 Willow Rd.
address:        Menlo Park, CA, 94025
admin-c:        PH4972-RIPE
tech-c:         PH4972-RIPE
nic-hdl:        RD4299-RIPE
mnt-by:         fb-neteng
created:        2011-04-11T18:49:50Z
last-modified:  2013-08-14T15:49:58Z
source:         RIPE # Filtered
abuse-mailbox:  domain at fb.com

% This query was served by the RIPE Database Query Service version 1.91.2 (ANGUS)

#####

The same if the server had been whatsapp, etc...

Thanks!



From baretomas at protonmail.com  Thu Jun 14 07:09:05 2018
From: baretomas at protonmail.com (=?UTF-8?Q?Tomas_Finn=C3=B8y?=)
Date: Thu, 14 Jun 2018 03:09:05 -0400
Subject: [squid-users] HTTPS cache for Java application - only getting
	TCP_MISS
Message-ID: <U4oin7DPcXOQQQeNhpkxRw6ghkUsErZHOvwMOUqsGWdD7uM0nNkFMzpHKoBZ1EJaadIuicfIlndpOBhmcNHMY4tQzCjGrLG5j0MPZ2UARAY=@protonmail.com>

> Surely all this peeking and bumping is only needed if you're running Squid in
> interception mode, whereas you've said that you've configured your Java
> application to explicitly use Squid as a proxy?

I found some "how-to's" and posts that were explaining how to make a https cache proxy, and they were all mentioning bumping. Isn't the bump needed to decrypt the response, so it is possible to store it in the cache? I dont need any acl with peek and bump for my scenario at all, is what you are saying?

> Have you tried your Squid configuration with a plain browser, configured to use
> the proxy, with (a) a few random websites, and (b) the specific resource you're
> trying to access from your Java application, to see whether it is actually
> working as a caching proxy?

No. And something I will do now. Thanks for tips.

Sorry for the messy formatting here, but I didnt get your responses to my mail. I only saw it in the archives and copied it over to my mail here....

/Tomas
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20180614/c6feb3d5/attachment.htm>

From Antony.Stone at squid.open.source.it  Thu Jun 14 08:25:54 2018
From: Antony.Stone at squid.open.source.it (Antony Stone)
Date: Thu, 14 Jun 2018 10:25:54 +0200
Subject: [squid-users] HTTPS cache for Java application - only getting
	TCP_MISS
In-Reply-To: <U4oin7DPcXOQQQeNhpkxRw6ghkUsErZHOvwMOUqsGWdD7uM0nNkFMzpHKoBZ1EJaadIuicfIlndpOBhmcNHMY4tQzCjGrLG5j0MPZ2UARAY=@protonmail.com>
References: <U4oin7DPcXOQQQeNhpkxRw6ghkUsErZHOvwMOUqsGWdD7uM0nNkFMzpHKoBZ1EJaadIuicfIlndpOBhmcNHMY4tQzCjGrLG5j0MPZ2UARAY=@protonmail.com>
Message-ID: <201806141025.55166.Antony.Stone@squid.open.source.it>

On Thursday 14 June 2018 at 09:09:05, Tomas Finn?y wrote:

> > Surely all this peeking and bumping is only needed if you're running
> > Squid in interception mode, whereas you've said that you've configured
> > your Java application to explicitly use Squid as a proxy?
> 
> I found some "how-to's" and posts that were explaining how to make a https
> cache proxy, and they were all mentioning bumping. Isn't the bump needed
> to decrypt the response, so it is possible to store it in the cache?

No, because when you explicitly configure a browser (or in your case a Java 
application) to use a proxy, it sends a request to the proxy saying "please go 
and fetch something from this URI for me", and Squid then does all the HTTPS 
negotiations needed to talk to the remote server.  What Squid gets back is the 
plain unencrypted content, which it can then pass on to the browser (or 
application), and if it's allowed to (by whatever it finds in the headers of 
the response) it can also cache it.

> I dont need any acl with peek and bump for my scenario at all, is what you
> are saying?

Correct.

> > Have you tried your Squid configuration with a plain browser, configured
> > to use the proxy, with (a) a few random websites, and (b) the specific
> > resource you're trying to access from your Java application, to see
> > whether it is actually working as a caching proxy?
> 
> No. And something I will do now. Thanks for tips.

No problem.  Just suggesting "start simple" before moving on to several 
complex things interacting with each other...

> Sorry for the messy formatting here, but I didnt get your responses to my
> mail. I only saw it in the archives and copied it over to my mail here....

Hm, odd, I see my reply on the list just as normal.


Antony.

-- 
I thought of going into banking, until I lost interest.

                                                   Please reply to the list;
                                                         please *don't* CC me.


From baretomas at protonmail.com  Thu Jun 14 08:31:48 2018
From: baretomas at protonmail.com (=?UTF-8?Q?Tomas_Finn=C3=B8y?=)
Date: Thu, 14 Jun 2018 04:31:48 -0400
Subject: [squid-users] HTTPS cache for Java application - only getting
	TCP_MISS
In-Reply-To: <201806141025.55166.Antony.Stone@squid.open.source.it>
References: <U4oin7DPcXOQQQeNhpkxRw6ghkUsErZHOvwMOUqsGWdD7uM0nNkFMzpHKoBZ1EJaadIuicfIlndpOBhmcNHMY4tQzCjGrLG5j0MPZ2UARAY=@protonmail.com>
 <201806141025.55166.Antony.Stone@squid.open.source.it>
Message-ID: <O6WUrnZRM7RnYC36X_A5jJib1ADsH7UerpUD2I15xSHIAcS7vPcCKpJRG1iucgCj-7PHBrfiEBQk2OTG4uJYl-qDuB2q8x0wfNLMeO2915w=@protonmail.com>


On June 14, 2018 10:25 AM, Antony Stone <Antony.Stone at squid.open.source.it> wrote:

> On Thursday 14 June 2018 at 09:09:05, Tomas Finn?y wrote:
> 
> > > Surely all this peeking and bumping is only needed if you're running
> > > 
> > > Squid in interception mode, whereas you've said that you've configured
> > > 
> > > your Java application to explicitly use Squid as a proxy?
> > 
> > I found some "how-to's" and posts that were explaining how to make a https
> > 
> > cache proxy, and they were all mentioning bumping. Isn't the bump needed
> > 
> > to decrypt the response, so it is possible to store it in the cache?
> 
> No, because when you explicitly configure a browser (or in your case a Java
> 
> application) to use a proxy, it sends a request to the proxy saying "please go
> 
> and fetch something from this URI for me", and Squid then does all the HTTPS
> 
> negotiations needed to talk to the remote server. What Squid gets back is the
> 
> plain unencrypted content, which it can then pass on to the browser (or
> 
> application), and if it's allowed to (by whatever it finds in the headers of
> 
> the response) it can also cache it.
> 
> > I dont need any acl with peek and bump for my scenario at all, is what you
> > 
> > are saying?
> 
> Correct.
> 
> > > Have you tried your Squid configuration with a plain browser, configured
> > > 
> > > to use the proxy, with (a) a few random websites, and (b) the specific
> > > 
> > > resource you're trying to access from your Java application, to see
> > > 
> > > whether it is actually working as a caching proxy?
> > 
> > No. And something I will do now. Thanks for tips.
> 
> No problem. Just suggesting "start simple" before moving on to several
> 
> complex things interacting with each other...
> 
> > Sorry for the messy formatting here, but I didnt get your responses to my
> > 
> > mail. I only saw it in the archives and copied it over to my mail here....
> 
> Hm, odd, I see my reply on the list just as normal.
> 

Ok now it arrived like it should!

Thanks for your tips! Very much appreciated!

/Tomas




From squid3 at treenet.co.nz  Thu Jun 14 09:53:35 2018
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 14 Jun 2018 21:53:35 +1200
Subject: [squid-users] SSL errors with Squid 3.5.27
In-Reply-To: <00da01d4035c$6e7e1be0$4b7a53a0$@yahoo.com.ar>
References: <010201d3ff7e$9b259510$d170bf30$@yahoo.com.ar>
 <9fcd2760-281d-63f2-52e4-983ac8c5b052@treenet.co.nz>
 <01b501d40008$fc418e00$f4c4aa00$@yahoo.com.ar>
 <8a1755bc-5257-1db0-8430-ab8fc62fdb77@treenet.co.nz>
 <00f801d40287$39940730$acbc1590$@yahoo.com.ar>
 <48966683-1714-9daa-673a-594a0a06609c@treenet.co.nz>
 <00da01d4035c$6e7e1be0$4b7a53a0$@yahoo.com.ar>
Message-ID: <43611c66-c75d-d38f-12de-da5c4b2510df@treenet.co.nz>

On 14/06/18 09:20, Julian Perconti wrote:
> 
> #####
> Here a example:
> #####
> 
> openssl s_client -connect 31.13.94.54:443
> CONNECTED(00000003)
> write:errno=104
> ---
> no peer certificate available
> ---
> No client certificate CA names sent
> ---
> SSL handshake has read 0 bytes and written 290 bytes
> ---
> New, (NONE), Cipher is (NONE)
> Secure Renegotiation IS NOT supported
> Compression: NONE
> Expansion: NONE
> SSL-Session:
>     Protocol  : TLSv1.2
>     Cipher    : 0000
>     Session-ID:
>     Session-ID-ctx:
>     Master-Key:
>     Key-Arg   : None
>     PSK identity: None
>     PSK identity hint: None
>     SRP username: None
>     Start Time: 1528924452
>     Timeout   : 300 (sec)
>     Verify return code: 0 (ok)
> 

The above says:
 * do not encrypt this content
 * disable all security checks
 * disable all ability to becomes secure later
 * send everything in plain-text format.


This is the "NULL" cipher (0000) which is forbidden in your
sslproxy_cipher config by "!aNULL:!eNULL:!LOW".


The existence of this cipher is one reason why a) TLS does not
necessarily make things secure, and b) making the proxy always "just
work" is not necessarily a good idea.



... so you now have the choice:

 Do you *actually* want security?
    if so let the proxy block the traffic.

OR,

 Do you want users to have same experience as no-proxy gives?
   if so remove the cipher etc restrictions you have improving security
at the proxy.


Amos


From uhlar at fantomas.sk  Thu Jun 14 09:56:06 2018
From: uhlar at fantomas.sk (Matus UHLAR - fantomas)
Date: Thu, 14 Jun 2018 11:56:06 +0200
Subject: [squid-users] SSL errors with Squid 3.5.27
In-Reply-To: <00da01d4035c$6e7e1be0$4b7a53a0$@yahoo.com.ar>
References: <010201d3ff7e$9b259510$d170bf30$@yahoo.com.ar>
 <9fcd2760-281d-63f2-52e4-983ac8c5b052@treenet.co.nz>
 <01b501d40008$fc418e00$f4c4aa00$@yahoo.com.ar>
 <8a1755bc-5257-1db0-8430-ab8fc62fdb77@treenet.co.nz>
 <00f801d40287$39940730$acbc1590$@yahoo.com.ar>
 <48966683-1714-9daa-673a-594a0a06609c@treenet.co.nz>
 <00da01d4035c$6e7e1be0$4b7a53a0$@yahoo.com.ar>
Message-ID: <20180614095606.GA9532@fantomas.sk>

On 13.06.18 18:20, Julian Perconti wrote:
>> Does not shows any cert and establishes a connection with TLS 1.2...

>openssl s_client -connect 31.13.94.54:443
>CONNECTED(00000003)
>write:errno=104
>---
>no peer certificate available
>---
>No client certificate CA names sent
>---
>SSL handshake has read 0 bytes and written 290 bytes
>---
>New, (NONE), Cipher is (NONE)
>Secure Renegotiation IS NOT supported
>Compression: NONE
>Expansion: NONE
>SSL-Session:
>    Protocol  : TLSv1.2
>    Cipher    : 0000
>    Session-ID:
>    Session-ID-ctx:
>    Master-Key:
>    Key-Arg   : None
>    PSK identity: None
>    PSK identity hint: None
>    SRP username: None
>    Start Time: 1528924452
>    Timeout   : 300 (sec)
>    Verify return code: 0 (ok)

have you tried -servername option for setting SNI extension?

-- 
Matus UHLAR - fantomas, uhlar at fantomas.sk ; http://www.fantomas.sk/
Warning: I wish NOT to receive e-mail advertising to this address.
Varovanie: na tuto adresu chcem NEDOSTAVAT akukolvek reklamnu postu.
"Where do you want to go to die?" [Microsoft]


From Ralf.Hildebrandt at charite.de  Thu Jun 14 11:04:40 2018
From: Ralf.Hildebrandt at charite.de (Ralf Hildebrandt)
Date: Thu, 14 Jun 2018 13:04:40 +0200
Subject: [squid-users] cacheHttpAllSvcTime quite high
Message-ID: <20180614110440.GM24776@charite.de>

We're using squid 5.0.0-20180202-r51e09c0 and I recently realized that
the values for "cacheHttpAllSvcTime" are quite high

cacheHttpAllSvcTime.5 = 288
cacheHttpMissSvcTime.5 = 45
cacheHttpNmSvcTime.5 = 0
cacheHttpNhSvcTime.5 = 23
cacheHttpHitSvcTime.5 = 1
cacheIcpQuerySvcTime.5 = 0
cacheIcpReplySvcTime.5 = 0
cacheDnsSvcTime.5 = 30

Why is cacheHttpAllSvcTime so much higher than cacheHttpMissSvcTime.5 ?
The proxy doesn't appear to be slow or sluggish.

-- 
Ralf Hildebrandt                   Charite Universit?tsmedizin Berlin
ralf.hildebrandt at charite.de        Campus Benjamin Franklin
https://www.charite.de             Hindenburgdamm 30, 12203 Berlin
Gesch?ftsbereich IT, Abt. Netzwerk fon: +49-30-450.570.155


From squid3 at treenet.co.nz  Thu Jun 14 11:25:29 2018
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 14 Jun 2018 23:25:29 +1200
Subject: [squid-users] HTTPS cache for Java application - only getting
 TCP_MISS
In-Reply-To: <1528918107577-0.post@n4.nabble.com>
References: <1528918107577-0.post@n4.nabble.com>
Message-ID: <c239d220-96d0-c25b-7160-fb95f14ebbe5@treenet.co.nz>

On 14/06/18 07:28, baretomas wrote:
> Hello,
> 
> I'm setting up a Squid proxy as a cache for a number (as many as possible)
> of identical JAVA applications to run their web calls through. The calls are
> ofc identical, and the response they get can safely be cached for 5-10
> seconds. 
> I do this because most of the calls is directed at a single server on the
> internet that I don't want to hammer, since I will ofc be locked out of it
> then.
> 
> Currently Im simply testing this on a single computer: the application and
> squid
> 
> The calls from the application is done using ssl / https by telling java to
> use Squid as a proxy (-Dhttps.proxyHost and -Dhttp.proxyHost). I've set up
> squid and JAVA with self-signed certificates, and the application sends its
> calls through squid and gets the reponse. No problem there (wasnt easy that
> either I must say :P ).

I was going to ask what was so hard about it. Then I looked at your
config and see that your are in fact using NAT interception instead of
the easy way.

So what _exactly_ do those -D options cause the Java applications to do
with the proxy?
 I have some suspicions, but am not familiar enough with Java API and
the specific details are critical to what you need the proxy to be doing.


> 
> The problem is that none of the calls get cached: All rows in the access.log
> hava a TCP_MISS/200 tag in them. 
> 
> I've searched all through the web for a solution to this, and have tried
> everything people have suggested. So I was hoping someone could help me?
> 
> Anyone have any tips on what to try?
> 

There are three ways to do this:

1) if you own the domain the apps are connecting to. Setup the proxy as
a normal TLS / HTTPS reverse-proxy.

2) if you have enough control of the apps to get them connecting with
TLS *to the proxy* and sending their requests there. Do that.

3) the (relatively) complicated SSL-Bump way you found. The proxy is
fully at the mercy of the the messages sent by apps and servers. Caching
is a luxury here, easily broken / prevented.

Well, there is a forth way with intercept. But that is a VERY last
resort and you already have (3) going and that is already better than
intercept. Getting to (1) or (2) would be simplest if you meet the "if
..." requirements for those.



> MY config (note Ive set the refresh_pattern like that just to see if I could
> catch anything. The plan is to modify it so it actualyl does refresh the
> responses frmo the web calls in 5-10 seconds intervals. There are commented
> out pats Ive tried with no luck there too):
> 
...

Ah. The way you write that implies a misunderstanding about refresh_pattern.

HTTP has some fixed algorithms written into the protocol that caches are
required to perform to determine if any object stored can be used or
requires replacement.

The parameters used by these algorithms come in the form of headers in
the originally stored reply message, the current clients request.
Sometimes they require revalidation, which is a quick check with the
server for updated instructions and/or content.

What refresh_pattern actually does is provide default values for those
algorithm parameters IF any one (or more) of them are missing from those
HTTP messages.


The proper way to make caching happen with your desired behaviour is for
the server to present HTTP Cache-Control header saying the object is
cacheable (ie does not forbid caching), but not for more than 10seconds.
 Cache-Control: max-age=10
OR to say that objects need revalidation, but presents a 304 status for
revalidation checks. (ie Cache-Control:no-cache)  (yeah, thats right,
"no-cache" means *do* cache).

That said, I doubt you really are wanting to force that and would be
happy if the server was instructing the the proxy as being safe to cache
an object for several minutes or any value larger than 10sec.


So what we circle back to is that you are probably trying to force
things to cache and be used long past their actual safe-to-use lifetimes
as specified by the devs most authoritative on that subject (under
10sec?). As you should be aware, this is highly unsafe thing to be doing
unless you are one of those devs - be very careful what you choose to do.


> 
> 
> # Squid normally listens to port 3128
> #http_port 3128 ssl-bump generate-host-certificates=on
> dynamic_cert_mem_cache_size=4MB cert=/cygdrive/c/squid/etc/squid/correct.pem
> key=/cygdrive/c/squid/etc/squid/ssl/myca.key 
> 
> http_port 3128 ssl-bump generate-host-certificates=on
> dynamic_cert_mem_cache_size=4MB
> cert=/cygdrive/c/squid/etc/squid/proxyCAx.pem
> key=/cygdrive/c/squid/etc/squid/proxyCA.pem
> 
> #https_port 3129 cert=/cygdrive/c/squid/etc/squid/proxyCAx.pem
> key=/cygdrive/c/squid/etc/squid/proxyCA.pem
> 

Hmm. This is a Windows machine running Cygwin?
FYI: Performance is going to be terrible. It may not be super relevant
yet. Just be aware that Windows imposes limitations on usable sockets
per application - which is much smaller than a typical proxy requires.
The Cygwin people do a lot but they cannot solve some OS limitation
problems.

To meet your very first sentence "as many as possible" requirement you
will need a non-Windows machine to run the proxy on. That simple change
will get you something around 3 orders of magnitude higher peak client
capacity on the proxy.


> 
> # Uncomment the line below to enable disk caching - path format is
> /cygdrive/<full path to cache folder>, i.e.
> #cache_dir aufs /cygdrive/c/squid/var/cache/ 3000 16 256
> 
> # certificate generation program
> sslcrtd_program /cygdrive/c/squid/lib/squid/ssl_crtd -s
> /cygdrive/c/squid/var/cache/squid_ssldb -M 4MB
> 
> # Leave coredumps in the first cache dir
> coredump_dir /var/cache/squid
> 
> # Add any of your own refresh_pattern entries above these.
> #refresh_pattern ^ftp:		1440	20%	10080
> #refresh_pattern ^gopher:	1440	0%	1440
> #refresh_pattern -i (/cgi-bin/|\?) 0	0%	0
> #refresh_pattern -i (/cgi-bin/|\?) 1440 100% 4320 ignore-no-store
> override-lastmod override-expire ignore-must-revalidate ignore-reload
> ignore-private ignore-auth
> refresh_pattern .		1440	100%	4320 ignore-no-store override-lastmod
> override-expire ignore-must-revalidate ignore-reload ignore-private
> ignore-auth override-lastmod 
> 


* ignore-must-revalidate actively *reduces* caching. Because it disables
several of the widely used HTTP mechanisms that rely on revalidation to
allow things to be stored in a cache.
 It is *only* beneficial if the server is broken; requiring revalidation
plus not supporting revalidation.


* ignore-auth same un-intuitive effects as ignoring revalidation, again
reducing caching ability.
 This is only useful if you want to prevent caching of contents which
require any form of login to view. High security networks dealing with
classified or confidential materials find this useful - regular Internet
admin not so much.


* ignore-no-store is highly dangerous and rarely necessary. The "nuclear
option" for caching. It has the potential to eradicate user privacy and
scramble up any server personalized content (not in a good way).
 This is a last resort intended only to copy with severely braindead
applications. YMMV whether you have to deal with any of those - just
treat this an absolute last resort rather than something to play with.


Overall - in order to use these refresh-pattern controls you *need* to
know what the HTTP(S) messages going through your proxy contain in terms
of caching headers AND what those messages are doing semantically /
content wise for the client application. Using any of them as a generic
"makes caching better" thing only leads to problems in todays HTTP protocol.


> # Bumped requests have relative URLs so Squid has to use reverse proxy
> # or accelerator code. By default, that code denies direct forwarding.
> # The need for this option may disappear in the future.
> #always_direct allow all
> 
> dns_nameservers 8.8.8.8 208.67.222.222

Use of 8.8.8.8 is known to be explicitly detrimental to caching
intercepted traffic.

Those servers present different result sets based on the timing and IP
sending the query. The #1 requirement of caching intercepted (or
SSL-Bump'ed) content is that the client and proxy have the exact same
view of DNS system contents. Having the DNS reply contents change
between two consecutive and identical queries breaks that requirement.


> 
> max_filedescriptors 3200
> 
> # Max Object Size Cache
> maximum_object_size 10240 KB
> 
> 
> acl step1 at_step SslBump1
> 
> ssl_bump peek step1
> ssl_bump bump all

This causes the proxy to attempt decryption of the traffic using crypto
algorithms based solely on the ClientHello details and its own
capabilities. There is zero server crypto capabilities known for the
proxy to use to ensure traffic can actually make it to the server.

You are rather lucky that it actually worked at all. Almost any
deviation (ie emergency security updates in future) at either client or
server or proxy endpoints risks breaking the communication through this
proxy.

Ideally there would be a stare action for step2 and them bump only at
step 3.




So in summary to the things to try to get better caching:

* ditch 8.8.8.8. Use a local DNS resolver within your own network,
shared by clients and proxy. That can use 8.8.8.8 itself, the important
part is that it should be responsible for caching DNS results and
ensuring the app clients and Squid see as much the same records as possible.

* try "debug_options 11,2" to get a cache.log of the HTTP(S) headers for
message being decrypted in the proxy. Look at those headers to see why
they are not caching normally. Use that info to inform your next
actions. It cannot tell you how the message is used by the application,
hopefully you can figure that out somehow before forcing anything unnatural.

* if you can, try pasting some of the transaction URLs into the tool at
redbot.org to see if there are any HTTP level mistakes in the apps that
could be fixed for better cacheability.

Amos


From squid3 at treenet.co.nz  Thu Jun 14 11:33:36 2018
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 14 Jun 2018 23:33:36 +1200
Subject: [squid-users] HTTPS cache for Java application - only getting
 TCP_MISS
In-Reply-To: <201806132144.51339.Antony.Stone@squid.open.source.it>
References: <1528918107577-0.post@n4.nabble.com>
 <201806132144.51339.Antony.Stone@squid.open.source.it>
Message-ID: <79d81300-6ab6-a55f-0c23-11e3f8025400@treenet.co.nz>

On 14/06/18 07:44, Antony Stone wrote:
> On Wednesday 13 June 2018 at 21:28:27, baretomas wrote:
> 
>> The calls from the application is done using ssl / https by telling java to
>> use Squid as a proxy (-Dhttps.proxyHost and -Dhttp.proxyHost).
> 
> Okay, but...
> 
>> http_port 3128 ssl-bump generate-host-certificates=on
>> dynamic_cert_mem_cache_size=4MB
>> cert=/cygdrive/c/squid/etc/squid/proxyCAx.pem
>> key=/cygdrive/c/squid/etc/squid/proxyCA.pem
> 
>> # certificate generation program
>> sslcrtd_program /cygdrive/c/squid/lib/squid/ssl_crtd -s
>> /cygdrive/c/squid/var/cache/squid_ssldb -M 4MB
> 
>> acl step1 at_step SslBump1
>>
>> ssl_bump peek step1
>> ssl_bump bump all
> 
> Surely all this peeking and bumping is only needed if you're running Squid in 
> interception mode,

Not quite. SSL-Bump is interception of the TLS layer. Regular / forward
/ explicit proxies use it to decrypt the CONNECT messages transporting
HTTPS traffic through tunnels.


> whereas you've said that you've configured your Java 
> application to explicitly use Squid as a proxy?
> 

The proxy port and SSL-Bump config is consistent with a SSL-Bumping
forward proxy.

I suspect the -Dhttp.proxyHost is probably the Java apps equivalent to
the Linux http_proxy environment variables we are more familiar with
seeing applications use to connect to that type of proxy.

> 
> Have you tried your Squid configuration with a plain browser, configured to use 
> the proxy, with (a) a few random websites, and (b) the specific resource you're 
> trying to access from your Java application, to see whether it is actually 
> working as a caching proxy?
> 

Good idea.


Amos


From squid3 at treenet.co.nz  Thu Jun 14 11:41:50 2018
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 14 Jun 2018 23:41:50 +1200
Subject: [squid-users] cacheHttpAllSvcTime quite high
In-Reply-To: <20180614110440.GM24776@charite.de>
References: <20180614110440.GM24776@charite.de>
Message-ID: <4e3c12e1-b32e-a023-52d4-28ba65c9e92a@treenet.co.nz>

On 14/06/18 23:04, Ralf Hildebrandt wrote:
> We're using squid 5.0.0-20180202-r51e09c0 and I recently realized that
> the values for "cacheHttpAllSvcTime" are quite high
> 
> cacheHttpAllSvcTime.5 = 288
> cacheHttpMissSvcTime.5 = 45
> cacheHttpNmSvcTime.5 = 0
> cacheHttpNhSvcTime.5 = 23
> cacheHttpHitSvcTime.5 = 1
> cacheIcpQuerySvcTime.5 = 0
> cacheIcpReplySvcTime.5 = 0
> cacheDnsSvcTime.5 = 30
> 
> Why is cacheHttpAllSvcTime so much higher than cacheHttpMissSvcTime.5 ?
> The proxy doesn't appear to be slow or sluggish.
> 

"All" includes things like CONNECT tunnels which can potentially have
very long duration and are no longer recorded under the "Miss" category.

IIRC they used to be combined, but with the growth of HTTPS usage we had
to separate so people could see if the non-tunnel MISS messages were
having latency problems.

Amos


From Ralf.Hildebrandt at charite.de  Thu Jun 14 11:44:32 2018
From: Ralf.Hildebrandt at charite.de (Ralf Hildebrandt)
Date: Thu, 14 Jun 2018 13:44:32 +0200
Subject: [squid-users] [ext] Re:  cacheHttpAllSvcTime quite high
In-Reply-To: <4e3c12e1-b32e-a023-52d4-28ba65c9e92a@treenet.co.nz>
References: <20180614110440.GM24776@charite.de>
 <4e3c12e1-b32e-a023-52d4-28ba65c9e92a@treenet.co.nz>
Message-ID: <20180614114432.GO24776@charite.de>

* Amos Jeffries <squid3 at treenet.co.nz>:
> On 14/06/18 23:04, Ralf Hildebrandt wrote:
> > We're using squid 5.0.0-20180202-r51e09c0 and I recently realized that
> > the values for "cacheHttpAllSvcTime" are quite high
> > 
> > cacheHttpAllSvcTime.5 = 288
> > cacheHttpMissSvcTime.5 = 45
> > cacheHttpNmSvcTime.5 = 0
> > cacheHttpNhSvcTime.5 = 23
> > cacheHttpHitSvcTime.5 = 1
> > cacheIcpQuerySvcTime.5 = 0
> > cacheIcpReplySvcTime.5 = 0
> > cacheDnsSvcTime.5 = 30
> > 
> > Why is cacheHttpAllSvcTime so much higher than cacheHttpMissSvcTime.5 ?
> > The proxy doesn't appear to be slow or sluggish.
> > 
> 
> "All" includes things like CONNECT tunnels which can potentially have
> very long duration and are no longer recorded under the "Miss" category.

OK. This all started in January 2018 (probably while using
5.0.0-something)
 
> IIRC they used to be combined, but with the growth of HTTPS usage we had
> to separate so people could see if the non-tunnel MISS messages were
> having latency problems.

Makes perfect sense.

-- 
Ralf Hildebrandt                   Charite Universit?tsmedizin Berlin
ralf.hildebrandt at charite.de        Campus Benjamin Franklin
https://www.charite.de             Hindenburgdamm 30, 12203 Berlin
Gesch?ftsbereich IT, Abt. Netzwerk fon: +49-30-450.570.155


From baretomas at protonmail.com  Thu Jun 14 15:49:59 2018
From: baretomas at protonmail.com (=?UTF-8?Q?Tomas_Finn=C3=B8y?=)
Date: Thu, 14 Jun 2018 11:49:59 -0400
Subject: [squid-users] HTTPS cache for Java application - only getting
	TCP_MISS
In-Reply-To: <c239d220-96d0-c25b-7160-fb95f14ebbe5@treenet.co.nz>
References: <1528918107577-0.post@n4.nabble.com>
 <c239d220-96d0-c25b-7160-fb95f14ebbe5@treenet.co.nz>
Message-ID: <JdXq3pJFV_94n9mhtaks-iNxdcviA2En7umcT0nz328hxjaCsPTEPQhbyFFQCaOy0a6dlH6rSAa6YwK1hJNUFr40eB3qyZ7s_HBacwWGu38=@protonmail.com>

??????? Original Message ???????

On June 14, 2018 1:25 PM, Amos Jeffries <squid3 at treenet.co.nz> wrote:

> On 14/06/18 07:28, baretomas wrote:
> 
> > Hello,
> > 
> > I'm setting up a Squid proxy as a cache for a number (as many as possible)
> > 
> > of identical JAVA applications to run their web calls through. The calls are
> > 
> > ofc identical, and the response they get can safely be cached for 5-10
> > 
> > seconds.
> > 
> > I do this because most of the calls is directed at a single server on the
> > 
> > internet that I don't want to hammer, since I will ofc be locked out of it
> > 
> > then.
> > 
> > Currently Im simply testing this on a single computer: the application and
> > 
> > squid
> > 
> > The calls from the application is done using ssl / https by telling java to
> > 
> > use Squid as a proxy (-Dhttps.proxyHost and -Dhttp.proxyHost). I've set up
> > 
> > squid and JAVA with self-signed certificates, and the application sends its
> > 
> > calls through squid and gets the reponse. No problem there (wasnt easy that
> > 
> > either I must say :P ).
> 
> I was going to ask what was so hard about it. Then I looked at your
> 
> config and see that your are in fact using NAT interception instead of
> 
> the easy way.
> 
> So what exactly do those -D options cause the Java applications to do
> 
> with the proxy?
> 
> I have some suspicions, but am not familiar enough with Java API and
> 
> the specific details are critical to what you need the proxy to be doing.
> 
> > The problem is that none of the calls get cached: All rows in the access.log
> > 
> > hava a TCP_MISS/200 tag in them.
> > 
> > I've searched all through the web for a solution to this, and have tried
> > 
> > everything people have suggested. So I was hoping someone could help me?
> > 
> > Anyone have any tips on what to try?
> 
> There are three ways to do this:
> 
> 1.  if you own the domain the apps are connecting to. Setup the proxy as
>     
>     a normal TLS / HTTPS reverse-proxy.
>     
> 2.  if you have enough control of the apps to get them connecting with
>     
>     TLS to the proxy and sending their requests there. Do that.
>     
> 3.  the (relatively) complicated SSL-Bump way you found. The proxy is
>     
>     fully at the mercy of the the messages sent by apps and servers. Caching
>     
>     is a luxury here, easily broken / prevented.
>     
>     Well, there is a forth way with intercept. But that is a VERY last
>     
>     resort and you already have (3) going and that is already better than
>     
>     intercept. Getting to (1) or (2) would be simplest if you meet the "if
>     
>     ..." requirements for those.
>     
> 
> > MY config (note Ive set the refresh_pattern like that just to see if I could
> > 
> > catch anything. The plan is to modify it so it actualyl does refresh the
> > 
> > responses frmo the web calls in 5-10 seconds intervals. There are commented
> > 
> > out pats Ive tried with no luck there too):
> 
> ...
> 
> Ah. The way you write that implies a misunderstanding about refresh_pattern.
> 
> HTTP has some fixed algorithms written into the protocol that caches are
> 
> required to perform to determine if any object stored can be used or
> 
> requires replacement.
> 
> The parameters used by these algorithms come in the form of headers in
> 
> the originally stored reply message, the current clients request.
> 
> Sometimes they require revalidation, which is a quick check with the
> 
> server for updated instructions and/or content.
> 
> What refresh_pattern actually does is provide default values for those
> 
> algorithm parameters IF any one (or more) of them are missing from those
> 
> HTTP messages.
> 
> The proper way to make caching happen with your desired behaviour is for
> 
> the server to present HTTP Cache-Control header saying the object is
> 
> cacheable (ie does not forbid caching), but not for more than 10seconds.
> 
> Cache-Control: max-age=10
> 
> OR to say that objects need revalidation, but presents a 304 status for
> 
> revalidation checks. (ie Cache-Control:no-cache) (yeah, thats right,
> 
> "no-cache" means do cache).
> 
> That said, I doubt you really are wanting to force that and would be
> 
> happy if the server was instructing the the proxy as being safe to cache
> 
> an object for several minutes or any value larger than 10sec.
> 
> So what we circle back to is that you are probably trying to force
> 
> things to cache and be used long past their actual safe-to-use lifetimes
> 
> as specified by the devs most authoritative on that subject (under
> 
> 10sec?). As you should be aware, this is highly unsafe thing to be doing
> 
> unless you are one of those devs - be very careful what you choose to do.
> 
> > Squid normally listens to port 3128
> > ===================================
> > 
> > #http_port 3128 ssl-bump generate-host-certificates=on
> > 
> > dynamic_cert_mem_cache_size=4MB cert=/cygdrive/c/squid/etc/squid/correct.pem
> > 
> > key=/cygdrive/c/squid/etc/squid/ssl/myca.key
> > 
> > http_port 3128 ssl-bump generate-host-certificates=on
> > 
> > dynamic_cert_mem_cache_size=4MB
> > 
> > cert=/cygdrive/c/squid/etc/squid/proxyCAx.pem
> > 
> > key=/cygdrive/c/squid/etc/squid/proxyCA.pem
> > 
> > #https_port 3129 cert=/cygdrive/c/squid/etc/squid/proxyCAx.pem
> > 
> > key=/cygdrive/c/squid/etc/squid/proxyCA.pem
> 
> Hmm. This is a Windows machine running Cygwin?
> 
> FYI: Performance is going to be terrible. It may not be super relevant
> 
> yet. Just be aware that Windows imposes limitations on usable sockets
> 
> per application - which is much smaller than a typical proxy requires.
> 
> The Cygwin people do a lot but they cannot solve some OS limitation
> 
> problems.
> 
> To meet your very first sentence "as many as possible" requirement you
> 
> will need a non-Windows machine to run the proxy on. That simple change
> 
> will get you something around 3 orders of magnitude higher peak client
> 
> capacity on the proxy.
> 
> > Uncomment the line below to enable disk caching - path format is
> > ================================================================
> > 
> > /cygdrive/<full path to cache folder>, i.e.
> > 
> > #cache_dir aufs /cygdrive/c/squid/var/cache/ 3000 16 256
> > 
> > certificate generation program
> > ==============================
> > 
> > sslcrtd_program /cygdrive/c/squid/lib/squid/ssl_crtd -s
> > 
> > /cygdrive/c/squid/var/cache/squid_ssldb -M 4MB
> > 
> > Leave coredumps in the first cache dir
> > ======================================
> > 
> > coredump_dir /var/cache/squid
> > 
> > Add any of your own refresh_pattern entries above these.
> > ========================================================
> > 
> > #refresh_pattern ^ftp: 1440 20% 10080
> > 
> > #refresh_pattern ^gopher: 1440 0% 1440
> > 
> > #refresh_pattern -i (/cgi-bin/|?) 0 0% 0
> > 
> > #refresh_pattern -i (/cgi-bin/|?) 1440 100% 4320 ignore-no-store
> > 
> > override-lastmod override-expire ignore-must-revalidate ignore-reload
> > 
> > ignore-private ignore-auth
> > 
> > refresh_pattern . 1440 100% 4320 ignore-no-store override-lastmod
> > 
> > override-expire ignore-must-revalidate ignore-reload ignore-private
> > 
> > ignore-auth override-lastmod
> 
> -   ignore-must-revalidate actively reduces caching. Because it disables
>     
>     several of the widely used HTTP mechanisms that rely on revalidation to
>     
>     allow things to be stored in a cache.
>     
>     It is only beneficial if the server is broken; requiring revalidation
>     
>     plus not supporting revalidation.
>     
> -   ignore-auth same un-intuitive effects as ignoring revalidation, again
>     
>     reducing caching ability.
>     
>     This is only useful if you want to prevent caching of contents which
>     
>     require any form of login to view. High security networks dealing with
>     
>     classified or confidential materials find this useful - regular Internet
>     
>     admin not so much.
>     
> -   ignore-no-store is highly dangerous and rarely necessary. The "nuclear
>     
>     option" for caching. It has the potential to eradicate user privacy and
>     
>     scramble up any server personalized content (not in a good way).
>     
>     This is a last resort intended only to copy with severely braindead
>     
>     applications. YMMV whether you have to deal with any of those - just
>     
>     treat this an absolute last resort rather than something to play with.
>     
>     Overall - in order to use these refresh-pattern controls you need to
>     
>     know what the HTTP(S) messages going through your proxy contain in terms
>     
>     of caching headers AND what those messages are doing semantically /
>     
>     content wise for the client application. Using any of them as a generic
>     
>     "makes caching better" thing only leads to problems in todays HTTP protocol.
>     
> 
> > Bumped requests have relative URLs so Squid has to use reverse proxy
> > ====================================================================
> > 
> > or accelerator code. By default, that code denies direct forwarding.
> > ====================================================================
> > 
> > The need for this option may disappear in the future.
> > =====================================================
> > 
> > #always_direct allow all
> > 
> > dns_nameservers 8.8.8.8 208.67.222.222
> 
> Use of 8.8.8.8 is known to be explicitly detrimental to caching
> 
> intercepted traffic.
> 
> Those servers present different result sets based on the timing and IP
> 
> sending the query. The #1 requirement of caching intercepted (or
> 
> SSL-Bump'ed) content is that the client and proxy have the exact same
> 
> view of DNS system contents. Having the DNS reply contents change
> 
> between two consecutive and identical queries breaks that requirement.
> 
> > max_filedescriptors 3200
> > 
> > Max Object Size Cache
> > =====================
> > 
> > maximum_object_size 10240 KB
> > 
> > acl step1 at_step SslBump1
> > 
> > ssl_bump peek step1
> > 
> > ssl_bump bump all
> 
> This causes the proxy to attempt decryption of the traffic using crypto
> 
> algorithms based solely on the ClientHello details and its own
> 
> capabilities. There is zero server crypto capabilities known for the
> 
> proxy to use to ensure traffic can actually make it to the server.
> 
> You are rather lucky that it actually worked at all. Almost any
> 
> deviation (ie emergency security updates in future) at either client or
> 
> server or proxy endpoints risks breaking the communication through this
> 
> proxy.
> 
> Ideally there would be a stare action for step2 and them bump only at
> 
> step 3.
> 
> So in summary to the things to try to get better caching:
> 
> -   ditch 8.8.8.8. Use a local DNS resolver within your own network,
>     
>     shared by clients and proxy. That can use 8.8.8.8 itself, the important
>     
>     part is that it should be responsible for caching DNS results and
>     
>     ensuring the app clients and Squid see as much the same records as possible.
>     
> -   try "debug_options 11,2" to get a cache.log of the HTTP(S) headers for
>     
>     message being decrypted in the proxy. Look at those headers to see why
>     
>     they are not caching normally. Use that info to inform your next
>     
>     actions. It cannot tell you how the message is used by the application,
>     
>     hopefully you can figure that out somehow before forcing anything unnatural.
>     
> -   if you can, try pasting some of the transaction URLs into the tool at
>     
>     redbot.org to see if there are any HTTP level mistakes in the apps that
>     
>     could be fixed for better cacheability.
>     
>     Amos


Very much thanks for this very informative post to my question! I will spend some time understanding it, and try out the things you suggest!
Thanks again!



From baretomas at protonmail.com  Thu Jun 14 19:32:20 2018
From: baretomas at protonmail.com (baretomas)
Date: Thu, 14 Jun 2018 12:32:20 -0700 (MST)
Subject: [squid-users] HTTPS cache for Java application - only getting
	TCP_MISS
In-Reply-To: <c239d220-96d0-c25b-7160-fb95f14ebbe5@treenet.co.nz>
References: <201806132137.08853.Antony.Stone@squid.open.source.it>
 <c239d220-96d0-c25b-7160-fb95f14ebbe5@treenet.co.nz>
Message-ID: <1529004740348-0.post@n4.nabble.com>

Ok Im back. Still confused as ever. Look below for my story.

On 14 June 2018 1:25 PM, Amos Jeffries <squid3 at treenet.co.nz> wrote:


> There are three ways to do this:
> 
> 1.  if you own the domain the apps are connecting to. Setup the proxy as
>     a normal TLS / HTTPS reverse-proxy.
> 2.  if you have enough control of the apps to get them connecting with
>     TLS to the proxy and sending their requests there. Do that.
> 3.  the (relatively) complicated SSL-Bump way you found. The proxy is
>     fully at the mercy of the the messages sent by apps and servers.
> Caching
>     is a luxury here, easily broken / prevented.
>     Well, there is a forth way with intercept. But that is a VERY last
>     resort and you already have (3) going and that is already better than
>     intercept. Getting to (1) or (2) would be simplest if you meet the "if
>     ..." requirements for those.

1. Both the proxy and the apps are on the same machine on my home network.
The server they are calling is not mine and have no way of modifying its
behaviour. That rules out 1, if I understood correctly?

2. According to the java docs, the https_proxy (-Dhttps.proxyHost and
-Dhttps.proxyPort should redirect all ssl traffic to that destination.)
should cover this. And I already have done that. 

So I seemed to have combined 2 and 3 here? 

But I *only* need 2 you are saying. 


> The proper way to make caching happen with your desired behaviour is for
> the server to present HTTP Cache-Control header saying the object is
> cacheable (ie does not forbid caching), but not for more than 10seconds.
> Cache-Control: max-age=10
> OR to say that objects need revalidation, but presents a 304 status for
> revalidation checks. (ie Cache-Control:no-cache) (yeah, thats right,
> "no-cache" means do cache).
> 
> That said, I doubt you really are wanting to force that and would be
> happy if the server was instructing the the proxy as being safe to cache
> an object for several minutes or any value larger than 10sec.
> So what we circle back to is that you are probably trying to force
> things to cache and be used long past their actual safe-to-use lifetimes
> as specified by the devs most authoritative on that subject (under
> 10sec?). As you should be aware, this is highly unsafe thing to be doing
> unless you are one of those devs - be very careful what you choose to do.

I'm well aware of the issues this might pose, and yes: the server *is*
sending out cache-control. Look below for the header. My wish is to ignore
those headers and still cache the content.

To repeat, Im well aware of the issues this might pose, and is ready to run
side-by-side tests continuosly to make certain all the app behave like they
should, even if they are only getting cached content. 

Ive analyzed the apps web calls for about a day of data, and figured out how
many and what type of calls it does, and I know the response content very
well, since I have written apps against that API myself.

What I am actually doing is writing a test bench for the application in
question by running many of them simultaneously against the same data sets,
with differing configurations to compare the result.

No other person is to touch this project, so Im fairly sure it wont affect
anyone but my own free time :)

It's not meant for production environment of any kind, and Im aware of the
potential failures of the project should anything out of my control happen. 

But thanks for the warnings of course. You have no way of knowing what my
intentions with the project were when I first asked my questions, and
probably should have had done that from the start.

I hope that makes evrything a bit clearer? Anyway,

So...is it all possible to override the cache controls of the headers below?

server reply header:

HTTP/1.1 200 OK
Content-Type: application/json
Transfer-Encoding: chunked
Connection: keep-alive
Date: Wed, 13 Jun 2018 17:18:33 GMT
Server: nginx
Vary: Accept-Encoding
Strict-Transport-Security: max-age=31536000; includeSubdomains
X-Frame-Options: SAMEORIGIN
X-Xss-Protection: 1; mode=block
X-Content-Type-Options: nosniff
Content-Security-Policy: default-src 'self'
X-Content-Security-Policy: default-src 'self'
X-WebKit-CSP: default-src 'self'
Cache-Control: no-cache, no-store, must-revalidate
Pragma: no-cache
Expires: 0
X-Cache: Miss from cloudfront
Via: 1.1 21258ec71c1aa4499bcd08c6ad0eba38.cloudfront.net (CloudFront)
X-Amz-Cf-Id: gdqZScePve6zvtHqlFa8TmCmmh0rKGrwD2Gwx46PbUSqd94QiJhkPQ==


> > Squid normally listens to port 3128
> > ===================================
> > #http_port 3128 ssl-bump generate-host-certificates=on
> > dynamic_cert_mem_cache_size=4MB
> cert=/cygdrive/c/squid/etc/squid/correct.pem
> > key=/cygdrive/c/squid/etc/squid/ssl/myca.key
> > http_port 3128 ssl-bump generate-host-certificates=on
> > 
> > dynamic_cert_mem_cache_size=4MB
> > cert=/cygdrive/c/squid/etc/squid/proxyCAx.pem
> > key=/cygdrive/c/squid/etc/squid/proxyCA.pem
> > #https_port 3129 cert=/cygdrive/c/squid/etc/squid/proxyCAx.pem
> > key=/cygdrive/c/squid/etc/squid/proxyCA.pem
> 
> Hmm. This is a Windows machine running Cygwin?
> FYI: Performance is going to be terrible. It may not be super relevant
> yet. Just be aware that Windows imposes limitations on usable sockets
> per application - which is much smaller than a typical proxy requires.
> 
> The Cygwin people do a lot but they cannot solve some OS limitation
> problems.
> 
> To meet your very first sentence "as many as possible" requirement you
> will need a non-Windows machine to run the proxy on. That simple change
> will get you something around 3 orders of magnitude higher peak client
> capacity on the proxy.

The current setup is on my windows laptop. Im only doing PoCs on it before
doing it all over again on a debian box with docker, where the applications
are to run. (Im already looking forward to bug hunting the differences in
behaviour for the whole thing after moving it, btw!) 
Sounds ok? If anyone have any suggestions on how I would run this even more
efficiently, I would very much like to know!  It's a critical facet of the
proejhct to get as many of these applciations running at the same time.

>     Overall - in order to use these refresh-pattern controls you need to
>     know what the HTTP(S) messages going through your proxy contain in
> terms
>     of caching headers AND what those messages are doing semantically /
>     content wise for the client application. Using any of them as a
> generic
>     "makes caching better" thing only leads to problems in todays HTTP
> protocol.

I've pulled out the relevant bits of the headers:

Cache-Control: no-cache, no-store, must-revalidate
Pragma: no-cache
Expires: 0

I guess I went slightly overboard with my config. I tried to add one
directive after another to see if there was any change. I couldn't spot any.
Would this be the correct setup for my refresh_pattern to be able to cache
reponses with this header?

ignore-no-store ignore-must-revalidate  override-expire   ?

What about the Expires: 0? Is that covered by override-expire?


> > dns_nameservers 8.8.8.8 208.67.222.222
> 
> Use of 8.8.8.8 is known to be explicitly detrimental to caching
> intercepted traffic.
> 
> Those servers present different result sets based on the timing and IP
> sending the query. The #1 requirement of caching intercepted (or
> SSL-Bump'ed) content is that the client and proxy have the exact same
> view of DNS system contents. Having the DNS reply contents change
> between two consecutive and identical queries breaks that requirement.

Thanks. Using my own dns now!

> > max_filedescriptors 3200
> > 
> > Max Object Size Cache
> > =====================
> > maximum_object_size 10240 KB
> > acl step1 at_step SslBump1
> > ssl_bump peek step1
> > ssl_bump bump all
> 
> This causes the proxy to attempt decryption of the traffic using crypto
> algorithms based solely on the ClientHello details and its own
> capabilities. There is zero server crypto capabilities known for the
> proxy to use to ensure traffic can actually make it to the server.
> You are rather lucky that it actually worked at all. Almost any
> deviation (ie emergency security updates in future) at either client or
> server or proxy endpoints risks breaking the communication through this
> proxy.
> 
> Ideally there would be a stare action for step2 and them bump only at
> 
> step 3.
> 
> So in summary to the things to try to get better caching:
> 
> -   ditch 8.8.8.8. Use a local DNS resolver within your own network,
>     shared by clients and proxy. That can use 8.8.8.8 itself, the
> important
>     part is that it should be responsible for caching DNS results and
>     ensuring the app clients and Squid see as much the same records as
> possible.
>     
> -   try "debug_options 11,2" to get a cache.log of the HTTP(S) headers for
>     message being decrypted in the proxy. Look at those headers to see why
>     they are not caching normally. Use that info to inform your next
>     actions. It cannot tell you how the message is used by the
> application,
>     hopefully you can figure that out somehow before forcing anything
> unnatural.
>     
> -   if you can, try pasting some of the transaction URLs into the tool at
>     redbot.org to see if there are any HTTP level mistakes in the apps
> that
>     could be fixed for better cacheability.


What Im still confused about all this, and it's probably because I dont
understand the squid system well enough, and that I have possibly read too
many suggestions on other forums that have pointed me in the wrong
direction.

What I have gathered from your posts and the others:

If I want to cache https/ssl I need to decrypt the requests and responses
using certificates.
Otherwise the application will only tunnel its requests to the destination
and the proxy wont know what's in the messages.

However, I *dont* have to bump and peek and do a fancy dance to get squid to
cache it. 

I also see that Amos is suggesting a reverse proxy (accel mode) on the
proxy. So...

... I assume this is not needed?
http_port 3128 ssl-bump generate-host-certificates=on
dynamic_cert_mem_cache_size=4MB
cert=/cygdrive/c/squid/etc/squid/proxyCAx.pem
key=/cygdrive/c/squid/etc/squid/proxyCA.pem

And I need something like this:

https_port 3128 accel generate-host-certificates=on
dynamic_cert_mem_cache_size=4MB
cert=/cygdrive/c/squid/etc/squid/proxyCAx.pem
key=/cygdrive/c/squid/etc/squid/proxyCA.pem

I tried this here now, and it seems to fail even more than before. No idea
why. I get this error message that seems to point me in the right direction,
but haven't found anything on google to give me an indication as to what it
actually points towards.

Squid_SSL_accept: Error negotiating SSL connection on FD 10:
error:00000005:lib(0):func(0):DH lib

Am I right in my musings so far? :)

Oh. Ive tried to use the browser as source of my tests. There is little
change in result to what the java apps are doing. 

Any thoughts or ideas are wildly appreciated!







--
Sent from: http://squid-web-proxy-cache.1019090.n4.nabble.com/Squid-Users-f1019091.html


From rousskov at measurement-factory.com  Thu Jun 14 21:25:10 2018
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Thu, 14 Jun 2018 15:25:10 -0600
Subject: [squid-users] HTTPS cache for Java application - only getting
 TCP_MISS
In-Reply-To: <1529004740348-0.post@n4.nabble.com>
References: <201806132137.08853.Antony.Stone@squid.open.source.it>
 <c239d220-96d0-c25b-7160-fb95f14ebbe5@treenet.co.nz>
 <1529004740348-0.post@n4.nabble.com>
Message-ID: <fb230ec2-f21b-9816-5637-b9335948d2ed@measurement-factory.com>

On 06/14/2018 01:32 PM, baretomas wrote:

> On 14 June 2018 1:25 PM, Amos Jeffries <squid3 at treenet.co.nz> wrote:
>> 2.  if you have enough control of the apps to get them connecting with
>>     TLS to the proxy and sending their requests there. Do that.

You are not doing this if your Squid receives CONNECT requests. If you
can get your apps to do the right thing, then Squid would be receiving
GET requests (and such) with https:// URLs instead of CONNECT requests.


>> 3.  the (relatively) complicated SSL-Bump way you found. The proxy is
>>     fully at the mercy of the the messages sent by apps and servers.

You are doing this right now. Some Java magic encrypts your app requests
and sends encrypted requests through Squid via CONNECT tunnels. You bump
those encrypted tunnels to get to the HTTP requests and cache responses.

Alex.


> According to the java docs, the https_proxy (-Dhttps.proxyHost and
> -Dhttps.proxyPort should redirect all ssl traffic to that destination.)


From jlay at slave-tothe-box.net  Fri Jun 15 00:56:13 2018
From: jlay at slave-tothe-box.net (James Lay)
Date: Thu, 14 Jun 2018 18:56:13 -0600
Subject: [squid-users] Squid and systemd
In-Reply-To: <187e7a50-1003-271b-5ae4-f4772d743f22@urlfilterdb.com>
References: <1c59ab2a4caf2cb93a54e64655863b3baf61f17d.camel@slave-tothe-box.net>
 <187e7a50-1003-271b-5ae4-f4772d743f22@urlfilterdb.com>
Message-ID: <4d8e6dc0a7ffd94c0431722043db2357a48d3b1d.camel@slave-tothe-box.net>

Thanks...still a newb at systemd and that was totally the fix.

James

On Wed, 2018-06-13 at 10:03 -0300, Marcus Kool wrote:
> I have seen systemd killing daemons when it times out waiting for the
> pid file to appear.I suggest to doublecheck that the pid filename in
> the service file and in squid.conf are the same.
> Marcus
> On 13/06/18 09:27, James Lay wrote:
> Well....I'll just say up front that systemd is not my friend. When
> running squid via cli: sudo /opt/squid/sbin/squid it runs like a
> champ. But using the service file at:
> https://raw.githubusercontent.com/squid-cache/squid/master/tools/syst
> emd/squid.service
> it times out after a few:
> 06:20:11 gateway squid[3669]: Created PID file
> (/opt/squid/var/run/squid.pid)06:20:11 gateway squid[3669]: Squid
> Parent: will start 1 kids06:20:11 gateway squid[3669]: Squid Parent:
> (squid-1) process 3678 started06:20:11 gateway squid[3678]: Set
> Current Directory to /opt/squid/var06:20:11 gateway squid[3678]:
> Starting Squid Cache version 4.0.24 for x86_64-pc-linux-
> gnu...06:20:11 gateway squid[3678]: Service Name: squid06:20:11
> gateway squid[3678]: Process ID 367806:20:11 gateway squid[3678]:
> Process Roles: worker06:20:11 gateway squid[3678]: With 1024 file
> descriptors available06:20:11 gateway squid[3678]: Initializing IP
> Cache...06:20:11 gateway squid[3678]: DNS Socket created at [::], FD
> 506:20:11 gateway squid[3678]: DNS Socket created at 0.0.0.0, FD
> 1006:20:11 gateway squid[3678]: Adding nameserver 192.168.1.253 from
> /etc/resolv.conf06:20:11 gateway squid[3678]: Adding nameserver
> 205.171.3.65 from /etc/resolv.conf06:20:11 gateway squid[3678]:
> Adding nameserver 205.171.2.65 from /etc/resolv.conf06:20:11 gateway
> squid[3678]: Adding domain slave-tothe-box.net from
> /etc/resolv.conf06:20:11 gateway squid[3678]: Adding domain slave-
> tothe-box.net from /etc/resolv.conf06:20:11 gateway squid[3678]:
> helperOpenServers: Starting 5/5 'security_file_certgen'
> processes06:20:11 gateway squid[3678]: Logfile: opening log
> syslog:daemon.info06:20:11 gateway squid[3678]: Store logging
> disabled06:20:11 gateway squid[3678]: Swap maxSize 0 + 262144 KB,
> estimated 20164 objects06:20:11 gateway squid[3678]: Target number of
> buckets: 100806:20:11 gateway squid[3678]: Using 8192 Store
> buckets06:20:11 gateway squid[3678]: Max Mem  size: 262144 KB06:20:11
> gateway squid[3678]: Max Swap size: 0 KB06:20:11 gateway squid[3678]:
> Using Least Load store dir selection06:20:11 gateway squid[3678]: Set
> Current Directory to /opt/squid/var06:20:11 gateway squid[3678]:
> Finished loading MIME types and icons.06:20:11 gateway squid[3678]:
> HTCP Disabled.06:20:11 gateway squid[3678]: Squid plugin modules
> loaded: 006:20:11 gateway squid[3678]: Adaptation support is
> off.06:20:11 gateway squid[3678]: Accepting HTTP Socket connections
> at local=x.x.x.x:3127 remote=[::] FD 21 flags=906:20:11 gateway
> squid[3678]: Accepting NAT intercepted HTTP Socket connections at
> local=x.x.x.x:3128 remote=[::] FD 22 flags=4106:20:11 gateway
> squid[3678]: Accepting NAT intercepted SSL bumped HTTPS Socket
> connections at local=x.x.x.x:3129 remote=[::] FD 23 flags=4106:20:12
> gateway squid[3678]: storeLateRelease: released 0 objects06:21:41
> gateway systemd[1]: squid.service: Start operation timed out.
> Terminating.06:21:41 gateway systemd[1]: squid.service: Killing
> process 3669 (squid) with signal SIGKILL.06:21:41 gateway sudo:
> pam_unix(sudo:session): session closed for user root06:21:41 gateway
> systemd[1]: squid.service: Killing process 3678 (squid) with signal
> SIGKILL.06:21:41 gateway jlay[2415] 192.168.1.2 46692 192.168.1.252
> 22: sudo systemctl start squid06:21:41 gateway systemd[1]:
> squid.service: Killing process 3680 (security_file_c) with signal
> SIGKILL.06:21:41 gateway systemd[1]: squid.service: Killing process
> 3682 (security_file_c) with signal SIGKILL.06:21:41 gateway
> systemd[1]: squid.service: Killing process 3683 (security_file_c)
> with signal SIGKILL.06:21:41 gateway systemd[1]: squid.service:
> Killing process 3684 (security_file_c) with signal SIGKILL.06:21:41
> gateway systemd[1]: squid.service: Killing process 3685
> (security_file_c) with signal SIGKILL.06:21:41 gateway systemd[1]:
> squid.service: Failed with result 'timeout'.06:21:41 gateway
> systemd[1]: Failed to start Squid Web Proxy Server.
> I've modded the service file to reflect different binary location,
> but that's about it. Thank you.
> James
> 
> _______________________________________________squid-users mailing
> listsquid-users at lists.squid-cache.orghttp://lists.squid-cache.org/lis
> tinfo/squid-users
> _______________________________________________squid-users mailing
> listsquid-users at lists.squid-cache.orghttp://lists.squid-cache.org/lis
> tinfo/squid-users
> 
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20180614/3b6d07ee/attachment.htm>

From davidjesse091 at aol.com  Fri Jun 15 05:05:29 2018
From: davidjesse091 at aol.com (davidjesse091 at aol.com)
Date: Fri, 15 Jun 2018 01:05:29 -0400
Subject: [squid-users] tcp_outgoing_address working on Windows machine but
	not on Ubuntu
Message-ID: <16401d58c76-c8d-18a7c@webjas-vae217.srv.aolmail.net>


On my Windows machine I can successfully connect to interface 1 and have the connections go out from interface 2 using "tcp_outgoing_address", but this does not work on my Linux Ubuntu machine. Anyone else notice this and know the reason and solution for this?
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20180615/79d6883e/attachment.htm>

From uhlar at fantomas.sk  Fri Jun 15 07:01:23 2018
From: uhlar at fantomas.sk (Matus UHLAR - fantomas)
Date: Fri, 15 Jun 2018 09:01:23 +0200
Subject: [squid-users] tcp_outgoing_address working on Windows machine
 but not on Ubuntu
In-Reply-To: <16401d58c76-c8d-18a7c@webjas-vae217.srv.aolmail.net>
References: <16401d58c76-c8d-18a7c@webjas-vae217.srv.aolmail.net>
Message-ID: <20180615070123.GA15018@fantomas.sk>

On 15.06.18 01:05, davidjesse091 at aol.com wrote:
>On my Windows machine I can successfully connect to interface 1 and have
> the connections go out from interface 2 using "tcp_outgoing_address", but
> this does not work on my Linux Ubuntu machine.  Anyone else notice this
> and know the reason and solution for this?

do those connections come from IP address defined in tcp_outgoing_address?

tcp_outgoing_address does not define interface but an address. The interface
is tken from kernel routing table.

you must apparently configure routing in order to get what you want.

-- 
Matus UHLAR - fantomas, uhlar at fantomas.sk ; http://www.fantomas.sk/
Warning: I wish NOT to receive e-mail advertising to this address.
Varovanie: na tuto adresu chcem NEDOSTAVAT akukolvek reklamnu postu.
Enter any 12-digit prime number to continue.


From robertocarna36 at gmail.com  Fri Jun 15 12:39:16 2018
From: robertocarna36 at gmail.com (Roberto Carna)
Date: Fri, 15 Jun 2018 09:39:16 -0300
Subject: [squid-users] BIND for complementary records for the same
	authoritative domain
Message-ID: <CAG2Qp6s8Mwq0pNA2tPPGgCK8_aT1EbL15kVG=CXmXczDnzTCVQ@mail.gmail.com>

Dear, our company has an internal Windows DNS with the "company.com"
authoritative domain. Suppose within it we have the following records:

a.company.com
b.company.com
c.company.com

Now we need to have several records maintained by other IT area
exclusively, in the same autoritative domain "company.com", so let's
say:

x.company.com
y.company.com
z.company.com

Is it possible to build a BIND DNS server for these last records, and
tell Windows DNS server something like this:

"Search the record x.company.com within company.com, if it is not
there search this record in the BIND server".

Windows DNS server is setup in the clients computers, and it can
contact BIND server for records it doesn't contain for the same
authoritative domain.

Thanks a lot!!!


From davidjesse091 at aol.com  Fri Jun 15 13:18:08 2018
From: davidjesse091 at aol.com (davidjesse091 at aol.com)
Date: Fri, 15 Jun 2018 09:18:08 -0400
Subject: [squid-users] tcp_outgoing_address working on Windows machine
 but not on Ubuntu
In-Reply-To: <20180615070123.GA15018@fantomas.sk>
Message-ID: <16403982f5e-c8b-19e62@webjas-vaa069.srv.aolmail.net>

That's right, I'm using the IP address of the interface for tcp_outgoing_address. I want to be using tcp_outgoing_address based on the port number of the incoming connection. Would that be possible by using Linux routing? 


-----Original Message-----
From: Matus UHLAR - fantomas <uhlar at fantomas.sk>
To: squid-users <squid-users at lists.squid-cache.org>
Sent: Fri, Jun 15, 2018 3:01 am
Subject: Re: [squid-users] tcp_outgoing_address working on Windows machine but not on Ubuntu

On 15.06.18 01:05, davidjesse091 at aol.com wrote:>On my Windows machine I can successfully connect to interface 1 and have> the connections go out from interface 2 using "tcp_outgoing_address", but> this does not work on my Linux Ubuntu machine.  Anyone else notice this> and know the reason and solution for this?do those connections come from IP address defined in tcp_outgoing_address?tcp_outgoing_address does not define interface but an address. The interfaceis tken from kernel routing table.you must apparently configure routing in order to get what you want.-- Matus UHLAR - fantomas, uhlar at fantomas.sk ; http://www.fantomas.sk/Warning: I wish NOT to receive e-mail advertising to this address.Varovanie: na tuto adresu chcem NEDOSTAVAT akukolvek reklamnu postu.Enter any 12-digit prime number to continue._______________________________________________squid-users mailing listsquid-users at lists.squid-cache.orghttp://lists.squid-cache.org/listinfo/squid-users
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20180615/0ee2951b/attachment.htm>

From davidjesse091 at aol.com  Fri Jun 15 13:20:27 2018
From: davidjesse091 at aol.com (davidjesse091 at aol.com)
Date: Fri, 15 Jun 2018 09:20:27 -0400
Subject: [squid-users] tcp_outgoing_address working on Windows machine
 but not on Ubuntu
In-Reply-To: <20180615070123.GA15018@fantomas.sk>
Message-ID: <164039ab28a-c92-19cff@webjas-vab126.srv.aolmail.net>

That's right, I'm using the IP address of the interface for tcp_outgoing_address. I want to be using tcp_outgoing_address based on the port number of the incoming connection. Would that be possible by using Linux routing? 


-----Original Message-----
From: Matus UHLAR - fantomas <uhlar at fantomas.sk>
To: squid-users <squid-users at lists.squid-cache.org>
Sent: Fri, Jun 15, 2018 3:01 am
Subject: Re: [squid-users] tcp_outgoing_address working on Windows machine but not on Ubuntu

On 15.06.18 01:05, davidjesse091 at aol.com wrote:>On my Windows machine I can successfully connect to interface 1 and have> the connections go out from interface 2 using "tcp_outgoing_address", but> this does not work on my Linux Ubuntu machine.  Anyone else notice this> and know the reason and solution for this?do those connections come from IP address defined in tcp_outgoing_address?tcp_outgoing_address does not define interface but an address. The interfaceis tken from kernel routing table.you must apparently configure routing in order to get what you want.-- Matus UHLAR - fantomas, uhlar at fantomas.sk ; http://www.fantomas.sk/Warning: I wish NOT to receive e-mail advertising to this address.Varovanie: na tuto adresu chcem NEDOSTAVAT akukolvek reklamnu postu.Enter any 12-digit prime number to continue._______________________________________________squid-users mailing listsquid-users at lists.squid-cache.orghttp://lists.squid-cache.org/listinfo/squid-users
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20180615/e98a32b6/attachment.htm>

From Antony.Stone at squid.open.source.it  Fri Jun 15 13:28:32 2018
From: Antony.Stone at squid.open.source.it (Antony Stone)
Date: Fri, 15 Jun 2018 15:28:32 +0200
Subject: [squid-users] BIND for complementary records for the same
	authoritative domain
In-Reply-To: <CAG2Qp6s8Mwq0pNA2tPPGgCK8_aT1EbL15kVG=CXmXczDnzTCVQ@mail.gmail.com>
References: <CAG2Qp6s8Mwq0pNA2tPPGgCK8_aT1EbL15kVG=CXmXczDnzTCVQ@mail.gmail.com>
Message-ID: <201806151528.32274.Antony.Stone@squid.open.source.it>

On Friday 15 June 2018 at 14:39:16, Roberto Carna wrote:

> Dear, our company has an internal Windows DNS with the "company.com"
> authoritative domain.

> Is it possible to ... tell Windows DNS server something like this:
> 
> "Search the record x.company.com within company.com, if it is not
> there search this record in the BIND server".

I do not believe this is possible, however you might be better off asking on a 
list or in a forum about Windows DNS.

Regards,


Antony.

-- 
"Good health" is merely the slowest rate at which you can die.

                                                   Please reply to the list;
                                                         please *don't* CC me.


From davidjesse091 at aol.com  Fri Jun 15 22:42:57 2018
From: davidjesse091 at aol.com (davidjesse091 at aol.com)
Date: Fri, 15 Jun 2018 18:42:57 -0400
Subject: [squid-users] iptables setup for tcp_outgoing_address
Message-ID: <164059da5b1-c8b-1d08c@webjas-vab121.srv.aolmail.net>


I have two network interfaces on my machine. I'm trying to setup incoming through the enp1s0's IP address and if the connection comes from port 11000 then I want squid to use wlx74da388c32c7's IP address.
IPs on my machine
root at poweredge:/var/log/squid# ip addr show
1: lo: <LOOPBACK,UP,LOWER_UP> mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000
    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
    inet 127.0.0.1/8 scope host lo
       valid_lft forever preferred_lft forever
    inet6 ::1/128 scope host 
       valid_lft forever preferred_lft forever
2: enp1s0: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc mq state UP group default qlen 1000
    link/ether 00:1e:4f:cd:c1:5f brd ff:ff:ff:ff:ff:ff
    inet 192.168.1.212/24 brd 192.168.1.255 scope global enp1s0
       valid_lft forever preferred_lft forever
    inet6 fe80::21e:4fff:fecd:c15f/64 scope link 
       valid_lft forever preferred_lft forever
3: wlx74da388c32c7: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc mq state UP group default qlen 1000
    link/ether 74:da:38:8c:32:c7 brd ff:ff:ff:ff:ff:ff
    inet 172.16.11.107/24 brd 172.16.11.255 scope global dynamic noprefixroute wlx74da388c32c7
       valid_lft 3531sec preferred_lft 3531sec
    inet6 fe80::4e86:c190:1e45:4722/64 scope link noprefixroute 
       valid_lft forever preferred_lft forever

I want to connect to Squid proxy using the 192.168.1.212 and if I am connecting using port 11000, I want squid to have the traffic go out of the 172.16.11.107 IP
Below is the relevant part of my squid.conf
http_port 11000 name=port_11000
acl port_11000_acl myportname port_11000
tcp_outgoing_address 172.16.11.107 port_11000_acl

>From what I have read the above configuration should be enough for Squid, but on Linux machines, I also need to use iptables. I have never used IP tables.
What would I need to do with iptables to make this work?
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20180615/c23e60c5/attachment.htm>

From Antony.Stone at squid.open.source.it  Fri Jun 15 22:54:49 2018
From: Antony.Stone at squid.open.source.it (Antony Stone)
Date: Sat, 16 Jun 2018 00:54:49 +0200
Subject: [squid-users] iptables setup for tcp_outgoing_address
In-Reply-To: <164059da5b1-c8b-1d08c@webjas-vab121.srv.aolmail.net>
References: <164059da5b1-c8b-1d08c@webjas-vab121.srv.aolmail.net>
Message-ID: <201806160054.49988.Antony.Stone@squid.open.source.it>

On Saturday 16 June 2018 at 00:42:57, davidjesse091 at aol.com wrote:

> I have two network interfaces on my machine. I'm trying to setup incoming
> through the enp1s0's IP address and if the connection comes from port
> 11000 then I want squid to use wlx74da388c32c7's IP address.

> IPs on my machine

> 2: enp1s0: 
>     inet 192.168.1.212/24 brd 192.168.1.255 scope global enp1s0

> 3: wlx74da388c32c7: 
>     inet 172.16.11.107/24 brd 172.16.11.255 scope global dynamic

> I want to connect to Squid proxy using the 192.168.1.212 and if I am
> connecting using port 11000, I want squid to have the traffic go out of
> the 172.16.11.107 IP

That makes no sense to me.

If I understand you correctly, it will also make no sense to the machine 
connecting to your Squid proxy.

Squid must reply to the client from the same address as the client connected 
to Squid on.

In other words, if you get a connection in to Squid on IP 192.168.1.212 port 
3128 then you must also reply (to whichever client sent that request) from IP 
192.168.1.212 port 3128.

If Squid is going to reply from IP 172.16.11.107 then then request needs to 
come in to IP 172.16.11.107 in the first place.

Nothing else can work in TCP/IP terms.

If I have misunderstood, please explain in more detail what you are trying to 
achieve.


Antony.

-- 
I want to build a machine that will be proud of me.

 - Danny Hillis, creator of The Connection Machine

                                                   Please reply to the list;
                                                         please *don't* CC me.


From rousskov at measurement-factory.com  Fri Jun 15 23:01:53 2018
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Fri, 15 Jun 2018 17:01:53 -0600
Subject: [squid-users] iptables setup for tcp_outgoing_address
In-Reply-To: <164059da5b1-c8b-1d08c@webjas-vab121.srv.aolmail.net>
References: <164059da5b1-c8b-1d08c@webjas-vab121.srv.aolmail.net>
Message-ID: <0f529f09-1001-15b7-faaf-b81bdf1a437b@measurement-factory.com>

On 06/15/2018 04:42 PM, davidjesse091 at aol.com wrote:

> I want to connect to Squid proxy using 192.168.1.212 and if I am
> connecting using port 11000, 

I assume you meant "connecting to port 11000" (there is also the client
source port, but it should not matter here).


> I want squid to have the traffic go out of the 172.16.11.107 IP


> http_port 11000 name=port_11000
> acl port_11000_acl myportname port_11000
> tcp_outgoing_address 172.16.11.107 port_11000_acl

Looks good to me, provided all your outgoing traffic goes to IPv4
addresses (no IPv6).


> What would I need to do with iptables to make this work?

Why do you think you need iptables? What does not work if you do not use
IP tables?


Alex.


From davidjesse091 at aol.com  Fri Jun 15 23:12:21 2018
From: davidjesse091 at aol.com (davidjesse091 at aol.com)
Date: Fri, 15 Jun 2018 19:12:21 -0400
Subject: [squid-users] iptables setup for tcp_outgoing_address
In-Reply-To: <0f529f09-1001-15b7-faaf-b81bdf1a437b@measurement-factory.com>
Message-ID: <16405b89b80-c8d-1c98a@webjas-vad075.srv.aolmail.net>

I just tried the same configuration on my Windows machine and it works fine. Must be some Linux networking getting in the way. 
I tried a few things, if I use another interface's IP address for tcp_outgoing_address on my Linux machine then web pages don't load. If I use the same IP as I connect to Squid then it works.


But on Windows, I can connect to one ip and have an IP of another interface for tcp_outgoing_address and the outgoing traffic works as expected


-----Original Message-----
From: Alex Rousskov <rousskov at measurement-factory.com>
To: davidjesse091 <davidjesse091 at aol.com>; squid-users <squid-users at lists.squid-cache.org>
Sent: Fri, Jun 15, 2018 7:01 pm
Subject: Re: [squid-users] iptables setup for tcp_outgoing_address

On 06/15/2018 04:42 PM, davidjesse091 at aol.com wrote:

> I want to connect to Squid proxy using 192.168.1.212 and if I am
> connecting using port 11000, 

I assume you meant "connecting to port 11000" (there is also the client
source port, but it should not matter here).


> I want squid to have the traffic go out of the 172.16.11.107 IP


> http_port 11000 name=port_11000
> acl port_11000_acl myportname port_11000
> tcp_outgoing_address 172.16.11.107 port_11000_acl

Looks good to me, provided all your outgoing traffic goes to IPv4
addresses (no IPv6).


> What would I need to do with iptables to make this work?

Why do you think you need iptables? What does not work if you do not use
IP tables?


Alex.

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20180615/c821cac0/attachment.htm>

From rousskov at measurement-factory.com  Sat Jun 16 03:43:09 2018
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Fri, 15 Jun 2018 21:43:09 -0600
Subject: [squid-users] iptables setup for tcp_outgoing_address
In-Reply-To: <16405b89b80-c8d-1c98a@webjas-vad075.srv.aolmail.net>
References: <16405b89b80-c8d-1c98a@webjas-vad075.srv.aolmail.net>
Message-ID: <0ee9e237-9d93-f32e-f931-db1c4c9c4bec@measurement-factory.com>

On 06/15/2018 05:12 PM, davidjesse091 at aol.com wrote:

> if I use another interface's IP address
> for?tcp_outgoing_address on my Linux machine then web pages don't load.

Does using "another interface" IP address work with curl or wget
executed on the Squid Linux box?

  curl --interface 172.16.11.107 http://www.example.com
  wget --bind-address=172.16.11.107 http://www.example.com


Alex.


> -----Original Message-----
> From: Alex Rousskov <rousskov at measurement-factory.com>
> To: davidjesse091 <davidjesse091 at aol.com>; squid-users
> <squid-users at lists.squid-cache.org>
> Sent: Fri, Jun 15, 2018 7:01 pm
> Subject: Re: [squid-users] iptables setup for tcp_outgoing_address
> 
> On 06/15/2018 04:42 PM, davidjesse091 at aol.com
> <mailto:davidjesse091 at aol.com> wrote:
> 
>> I want to connect to Squid proxy using 192.168.1.212 and if I am
>> connecting using port 11000,
> 
> I assume you meant "connecting to port 11000" (there is also the client
> source port, but it should not matter here).
> 
> 
>> I want squid to have the traffic go out of the 172.16.11.107 IP
> 
> 
>> http_port 11000 name=port_11000
>> acl port_11000_acl myportname port_11000
>> tcp_outgoing_address 172.16.11.107 port_11000_acl
> 
> Looks good to me, provided all your outgoing traffic goes to IPv4
> addresses (no IPv6).
> 
> 
>> What would I need to do with iptables to make this work?
> 
> Why do you think you need iptables? What does not work if you do not use
> IP tables?
> 
> 
> Alex.



From davidjesse091 at aol.com  Sat Jun 16 04:16:19 2018
From: davidjesse091 at aol.com (davidjesse091 at aol.com)
Date: Sat, 16 Jun 2018 00:16:19 -0400
Subject: [squid-users] iptables setup for tcp_outgoing_address
In-Reply-To: <0ee9e237-9d93-f32e-f931-db1c4c9c4bec@measurement-factory.com>
Message-ID: <16406cee145-c8b-9279@webjas-vad174.srv.aolmail.net>

I tried curl --interface 172.16.11.107 http://www.example.com yesterday and it worked fine, but now it looks like it does not work. Just hangs forever. So there is an issue there for sure. I will try to find out why it's not working.



-----Original Message-----
From: Alex Rousskov <rousskov at measurement-factory.com>
To: davidjesse091 <davidjesse091 at aol.com>; squid-users <squid-users at lists.squid-cache.org>
Sent: Fri, Jun 15, 2018 11:43 pm
Subject: Re: [squid-users] iptables setup for tcp_outgoing_address

On 06/15/2018 05:12 PM, davidjesse091 at aol.com wrote:

> if I use another interface's IP address
> for tcp_outgoing_address on my Linux machine then web pages don't load.

Does using "another interface" IP address work with curl or wget
executed on the Squid Linux box?

  curl --interface 172.16.11.107 http://www.example.com
  wget --bind-address=172.16.11.107 http://www.example.com


Alex.


> -----Original Message-----
> From: Alex Rousskov <rousskov at measurement-factory.com>
> To: davidjesse091 <davidjesse091 at aol.com>; squid-users
> <squid-users at lists.squid-cache.org>
> Sent: Fri, Jun 15, 2018 7:01 pm
> Subject: Re: [squid-users] iptables setup for tcp_outgoing_address
> 
> On 06/15/2018 04:42 PM, davidjesse091 at aol.com
> <mailto:davidjesse091 at aol.com> wrote:
> 
>> I want to connect to Squid proxy using 192.168.1.212 and if I am
>> connecting using port 11000,
> 
> I assume you meant "connecting to port 11000" (there is also the client
> source port, but it should not matter here).
> 
> 
>> I want squid to have the traffic go out of the 172.16.11.107 IP
> 
> 
>> http_port 11000 name=port_11000
>> acl port_11000_acl myportname port_11000
>> tcp_outgoing_address 172.16.11.107 port_11000_acl
> 
> Looks good to me, provided all your outgoing traffic goes to IPv4
> addresses (no IPv6).
> 
> 
>> What would I need to do with iptables to make this work?
> 
> Why do you think you need iptables? What does not work if you do not use
> IP tables?
> 
> 
> Alex.


-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20180616/c52b3984/attachment.htm>

From squid3 at treenet.co.nz  Sat Jun 16 13:18:35 2018
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sun, 17 Jun 2018 01:18:35 +1200
Subject: [squid-users] tcp_outgoing_address working on Windows machine
 but not on Ubuntu
In-Reply-To: <16403982f5e-c8b-19e62@webjas-vaa069.srv.aolmail.net>
References: <16403982f5e-c8b-19e62@webjas-vaa069.srv.aolmail.net>
Message-ID: <aed5da32-f7a5-9ade-a965-a7416b5d9af0@treenet.co.nz>

On 16/06/18 01:18, davidjesse091 wrote:
> That's right, I'm using the IP address of the interface for
> tcp_outgoing_address. I want to be using?tcp_outgoing_address based on
> the port number of the incoming connection. Would that be possible by
> using Linux routing?


You are asking the wrong question now.

 * squid.conf tcp_outgoing_address directives ONLY determines the
*Squid* actions on its outgoing connections *if* the request message
being handled matches the ACLs on that directives line.


 * Each machines own OS routing setup determines the route selection
part(s). (On Linux that tool is "ip route" [note the space].)

These are strictly separated layers in networking. You are dealing with
networking here, almost everything involves multiple moving parts
individually tuned so they interact together to get things done.


Your first post asked *why* machine routing behaviour was different
between your machines, and implicitly what to do about it. Matus answerd
you (and correctly), which for *that* question is:
 A: your machines routing configuration are different.


 ie. If you want both machines to behave the same given *the same
squid.conf settings*, the thing to change is the routing setup of the
machine thats not doing what you want. In this case the Linux one.


If you go and change squid.conf settings on that "not working" machine.
All its going to do "break" the Squid part as well.


Amos


From squid3 at treenet.co.nz  Fri Jun 15 00:14:44 2018
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Fri, 15 Jun 2018 12:14:44 +1200
Subject: [squid-users] [squid-announce] Squid 4.0.25 beta is available
Message-ID: <35a8f4c2-7b47-3b22-3a8d-e9e1b4cf7927@treenet.co.nz>

The Squid HTTP Proxy team is very pleased to announce the availability
of the Squid-4.0.25 release!


This release is a bug fix and stability release resolving several issues
found in the prior Squid releases.


The major changes to be aware of:

* Various regressions

 - Bug 4855: querying private entries for HTCP/ICP
 - Bug 4852: deny_info %R macro not being expanded
 - Bug 4847: proxy_auth ACL -i/+i flags not working
 - Bug 4831: filter chain certificates for validity when loading
 - Regression fix: Transient reader locking broken in 4.0.24

These are all fairly recent regressions, mostly found in the 4.0.24
release with some from earlier. Anyone having issues with these in older
betas please upgrade to this release.


* Bug 4845: NegotiateSsl crash on aborting transaction

This bug has been plaguing people since at least Squid-3.3. It has
turned out to be a timing race between TCP connection closure and the
TLS handshake callback event. As such it appears with unpredictable
times and varying frequency. Being most problematic at high traffic loads.


* Bug 4829: IPC shared memory leaks when disker queue overflows

This issue only affects proxies under high load. It was showing up as
"run out of shared memory pages for IPC I/O" errors in the logs at peak
traffic times and may have required a restart of Squid to recover normal
behaviour.


* Bug 4816: update negotiate_kerberos_auth helper protocol to v3.4

Squids' older helper protocol cannot easily handle whitespace or
non-ASCII characters in user names, group names, and passwords. This
results in partial usernames being logged, and possibly also some users
being denied login when they should have been permitted.

With this update to the newer helper protocol all these issues should
now be resolved for anyone using this helper.

NOTE: The NTLM and some other helpers still need to be updated. Which
means this issues behaviour may still remain IF multiple helpers are in use.


* Bug 4707: purge tool does not obey --sysconfdir= build option

This issue was showing up as purge (aka. "squid-purge") tool being
unable to locate the squid.conf file unless it was explicitly provided
in command line arguments.

Effective immediately the tool obeys the --sysconfdir= build option
which is the correct way to set the squid.conf location. Packagers
setting build flags or patching the config location will have to update
their packaging.


* Add timestamps to (most) FATAL messages

Effective immediately. Most cache.log "FATAL: ..." messages are being
recorded with the timestamp prefix as used on other log entries. This
should make it a lot clearer whether the line(s) above a FATAL message
are related or happen much earlier.

Anyone responsible for log parsers scanning cache.log needs to check
that their parsers can cope with the updated log format.



  All users of Squid-4.x are urged to upgrade to this release as
  soon as possible.

  All users of Squid-3 are encouraged to test this release out and plan
  for upgrades where possible.


See the ChangeLog for the full list of changes in this and earlier
releases.

Please refer to the release notes at
http://www.squid-cache.org/Versions/v4/RELEASENOTES.html
when you are ready to make the switch to Squid-4

This new release can be downloaded from our HTTP or FTP servers

  http://www.squid-cache.org/Versions/v4/
  ftp://ftp.squid-cache.org/pub/squid/
  ftp://ftp.squid-cache.org/pub/archive/4/

or the mirrors. For a list of mirror sites see

  http://www.squid-cache.org/Download/http-mirrors.html
  http://www.squid-cache.org/Download/mirrors.html

If you encounter any issues with this release please file a bug report.
  http://bugs.squid-cache.org/


Amos Jeffries
_______________________________________________
squid-announce mailing list
squid-announce at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-announce

From davidjesse091 at aol.com  Sat Jun 16 18:26:52 2018
From: davidjesse091 at aol.com (davidjesse091 at aol.com)
Date: Sat, 16 Jun 2018 14:26:52 -0400
Subject: [squid-users] iptables setup for tcp_outgoing_address
In-Reply-To: <0ee9e237-9d93-f32e-f931-db1c4c9c4bec@measurement-factory.com>
Message-ID: <16409d996e3-c8b-1822b@webjas-vab187.srv.aolmail.net>

I found out how to solve this issue. The answer lies here


https://serverfault.com/questions/487891/cant-ping-multihomed-linux-machine-on-non-default-interface



-----Original Message-----
From: Alex Rousskov <rousskov at measurement-factory.com>
To: davidjesse091 <davidjesse091 at aol.com>; squid-users <squid-users at lists.squid-cache.org>
Sent: Fri, Jun 15, 2018 11:43 pm
Subject: Re: [squid-users] iptables setup for tcp_outgoing_address

On 06/15/2018 05:12 PM, davidjesse091 at aol.com wrote:

> if I use another interface's IP address
> for tcp_outgoing_address on my Linux machine then web pages don't load.

Does using "another interface" IP address work with curl or wget
executed on the Squid Linux box?

  curl --interface 172.16.11.107 http://www.example.com
  wget --bind-address=172.16.11.107 http://www.example.com


Alex.


> -----Original Message-----
> From: Alex Rousskov <rousskov at measurement-factory.com>
> To: davidjesse091 <davidjesse091 at aol.com>; squid-users
> <squid-users at lists.squid-cache.org>
> Sent: Fri, Jun 15, 2018 7:01 pm
> Subject: Re: [squid-users] iptables setup for tcp_outgoing_address
> 
> On 06/15/2018 04:42 PM, davidjesse091 at aol.com
> <mailto:davidjesse091 at aol.com> wrote:
> 
>> I want to connect to Squid proxy using 192.168.1.212 and if I am
>> connecting using port 11000,
> 
> I assume you meant "connecting to port 11000" (there is also the client
> source port, but it should not matter here).
> 
> 
>> I want squid to have the traffic go out of the 172.16.11.107 IP
> 
> 
>> http_port 11000 name=port_11000
>> acl port_11000_acl myportname port_11000
>> tcp_outgoing_address 172.16.11.107 port_11000_acl
> 
> Looks good to me, provided all your outgoing traffic goes to IPv4
> addresses (no IPv6).
> 
> 
>> What would I need to do with iptables to make this work?
> 
> Why do you think you need iptables? What does not work if you do not use
> IP tables?
> 
> 
> Alex.


-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20180616/21b912ee/attachment.htm>

From rightkicktech at gmail.com  Sun Jun 17 06:17:04 2018
From: rightkicktech at gmail.com (Alex K)
Date: Sun, 17 Jun 2018 09:17:04 +0300
Subject: [squid-users] PID file /var/run/squid.pid not readable AND
 Supervising process XXX which is not our child
In-Reply-To: <070b01d40153$72407fd0$56c17f70$@ngtech.co.il>
References: <CAG2Qp6t_J4h56rQ0ZE9owc=bK4vuiaz9zktAcXHMytHCWKskUg@mail.gmail.com>
 <2cd09b57-faab-fd11-2381-19d5d7c5db4d@treenet.co.nz>
 <CABMULtJWhzYhQrA07ibBPt2ZvaJz3kCAw62YcAji5g4OdYhKHw@mail.gmail.com>
 <06a201d40128$543276e0$fc9764a0$@ngtech.co.il>
 <2443723c-f648-21a9-98f2-ebf4adfcec66@treenet.co.nz>
 <070b01d40153$72407fd0$56c17f70$@ngtech.co.il>
Message-ID: <CABMULtLjMR14VycnWUidgZb1zPntkvQOaEzSTkeCN=_4=9+AHQ@mail.gmail.com>

Hi,

Had no more issues after using the service file shipped with source code.

What I did was:
cp squid.service /etc/systemd/system
systemctl daemon-reload
systemctl enable squid
systemctl start squid

Alex



On Mon, Jun 11, 2018, 10:11 Eliezer Croitoru <eliezer at ngtech.co.il> wrote:

> It was hard for me to understand the diff\patch without the original file
> in plain text.
> < GuessMainPID=no
> < RemainAfterExit=no
> < PIDFile=/var/run/squid.pid
> < SuccessExitStatus=5 6
> < ExecStart=/etc/init.d/squid start
> < ExecStop=/etc/init.d/squid stop
> < ExecReload=/etc/init.d/squid reload
>
> The above now makes more sense but.. replacing the service file and
> removing the /etc/init.d/squid file should be the real way to run squid.
> I do not know if and when Debian Stable would upgrade their package but as
> long as you don't upgrade it automatically it would be suffice.
>
> Eliezer
>
> ----
> Eliezer Croitoru
> Linux System Administrator
> Mobile: +972-5-28704261
> Email: eliezer at ngtech.co.il
>
>
>
> -----Original Message-----
> From: Amos Jeffries <squid3 at treenet.co.nz>
> Sent: Monday, June 11, 2018 09:25
> To: Eliezer Croitoru <eliezer at ngtech.co.il>; 'Alex K' <
> rightkicktech at gmail.com>
> Cc: squid-users at lists.squid-cache.org
> Subject: Re: [squid-users] PID file /var/run/squid.pid not readable AND
> Supervising process XXX which is not our child
>
> On 11/06/18 14:03, Eliezer Croitoru wrote:
> > Hey Alex,
> >
> > What OS exactly is shipping this service file?
> >
>
> From the first post:
>
> > 3.5.23-5+deb9u1
>
> Current Debian stable. It is not shipped exactly, but generates on
> install from the init.d file inserv headers in the absence of a shipped
> .service.
>
> "
>  # Automatically generated by systemd-sysv-generator
>  SourcePath=/etc/init.d/squid
> "
>
>
> Amos
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20180617/f9bf554b/attachment.htm>

From pheriko.support at gmail.com  Sun Jun 17 06:36:16 2018
From: pheriko.support at gmail.com (Periko Support)
Date: Sat, 16 Jun 2018 23:36:16 -0700
Subject: [squid-users] Active Directory Integration?
Message-ID: <CAK2yrTagu78DJ6NR05bOqjc9B1mD=TJyRKc6GGv-Y9gk68b_UQ@mail.gmail.com>

Hi people.

If we need to integrate squid 3.5+ with a windows domain AD(2008+) and
authenticated users from the domain.

Linux(squid) need to be part of the domain?
Or we can just enable a common LDAP query to the AD server?

Thanks.


From squid-user at tlinx.org  Sun Jun 17 08:28:22 2018
From: squid-user at tlinx.org (L A Walsh)
Date: Sun, 17 Jun 2018 01:28:22 -0700
Subject: [squid-users] building 4.0.25
Message-ID: <5B261BA6.1090008@tlinx.org>

I unpacked the tar and ran configure via a script.

Ran make, but am running into this:

  CCLD     libmiscencoding.la
../libtool: line 7979: func_quote_for_eval: command not found
...
../libtool: line 7979: func_quote_for_eval: command not found
  CXXLD    libmisccontainers.la
../libtool: line 7979: func_quote_for_eval: command not found
...
../libtool: line 7979: func_quote_for_eval: command not found
  CXXLD    libmiscutil.la
../libtool: line 7979: func_quote_for_eval: command not found
...
../libtool: line 7979: func_quote_for_eval: command not found
make[2]: Leaving directory '/home/tools/squid/squid-4.0.25/lib'
make[1]: Leaving directory '/home/tools/squid/squid-4.0.25/lib'
Making all in libltdl
make[1]: Entering directory '/home/tools/squid/squid-4.0.25/libltdl'
CDPATH="${ZSH_VERSION+.}:" && cd . && /bin/sh 
/home/tools/squid/squid-4.0.25/cfgaux/missing aclocal-1.15 -I m4
CDPATH="${ZSH_VERSION+.}:" && cd . && /bin/sh 
/home/tools/squid/squid-4.0.25/cfgaux/missing autoconf
 cd . && /bin/sh /home/tools/squid/squid-4.0.25/cfgaux/missing 
automake-1.15 --foreign
/home/tools/squid/squid-4.0.25/cfgaux/missing: line 81: automake-1.15: 
command not found
WARNING: 'automake-1.15' is missing on your system.
         You should only need it if you modified 'Makefile.am' or
         'configure.ac' or m4 files included by 'configure.ac'.

----
Is the libtool failure what is causing it to think automake is missing
(it isn't on my system and suse is shipping automake-1.16 which I haven't
installed yet).

My question is the WARNING above -- saying I shouldn't need it unless
I modified various files.  I haven't.  Just ran configure, which I'm
assuming doesn't modify those files? 

I didn't want to go off and find am1.15 without finding out why it
thinks I need it (since haven't changed any source files at this point).
Also wondering if am1.16 would work in place of am1.15 -- I know
sometimes one needs to keep multiple versions of am around.

Also, tried recompiling 3.5.21+22 and ran into some issues (which
don't really need solving if I get 4.0.25 up and running, but thought
I'd mention them as they seem to be related to me using a more recent
gcc toolchain (7.1.0).
Got a few warnings that were escalated into errors:

1 was due to something deprecated (from 3.5.21 build)

In file included from ../../include/util.h:37:0,
                 from ntlmauth.cc:20:
../../include/SquidNew.h:21:51: error: dynamic exception specifications 
are deprecated in C++11 [-Werror=deprecated]
 _SQUID_EXTERNNEW_ void *operator new(size_t size) throw (std::bad_alloc)
                                                   ^~~~~
../../include/SquidNew.h:29:54: error: dynamic exception specifications 
are deprecated in C++11 [-Werror=deprecated]
 _SQUID_EXTERNNEW_ void *operator new[] (size_t size) throw (std::bad_alloc)
--------

After disabling that fatal warning, I ran into a different warning
in the FtpGateway.cc file:
FtpGateway.cc: In member function ?const char* Ftp::Gateway::ftpRealm()?:
FtpGateway.cc:1288:1: error: ?%s? directive output may be truncated 
writing up to 8191 bytes into a region of size 8188 
[-Werror=format-truncation=]
 Ftp::Gateway::ftpRealm()
 ^~~
FtpGateway.cc:1294:17: note: ?snprintf? output between 13 and 8204 bytes 
into a destination of size 8192
         snprintf(realm, 8192, "FTP %s unknown", user);
         ~~~~~~~~^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
FtpGateway.cc:1288:1: error: ?%s? directive output may be truncated 
writing up to 8191 bytes into a region of size 8188 
[-Werror=format-truncation=]
 Ftp::Gateway::ftpRealm()
 ^~~
...
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
FtpGateway.cc: In function ?void ftpSendUser(Ftp::Gateway*)?:
FtpGateway.cc:1304:1: error: ?%s? directive output may be truncated 
writing up to 8191 bytes into a region of size 1019 
[-Werror=format-truncation=]
 ftpSendUser(Ftp::Gateway * ftpState)
 ^~~~~~~~~~~
cc1plus: all warnings being treated as errors

There were others, of the same sort that I didn't copy here.  I turned off
fatal warnings, as this used to work under an older compiler,
but then some things didn't want to link:

  CXXLD    pinger
debug.o: In function `FileNameHashCached(char const*)':
debug.cc:(.text+0x4e): undefined reference to 
`TextException::FileNameHash(char const*)'
globals.o: In function `FileNameHashCached(char const*)':
globals.cc:(.text+0x2c): undefined reference to 
`TextException::FileNameHash(char const*)'
SquidConfig.o: In function `FileNameHashCached(char const*)':
SquidConfig.cc:(.text+0x2c): undefined reference to 
`TextException::FileNameHash(char const*)'
stub_HelperChildConfig.o: In function `FileNameHashCached(char const*)':
stub_HelperChildConfig.cc:(.text+0x2c): undefined reference to 
`TextException::FileNameHash(char const*)'
collect2: error: ld returned 1 exit status
Makefile:880: recipe for target 'pinger' failed
make[3]: *** [pinger] Error 1

----
Made wonder if maybe gcc V7.1.0 wasn't supported or tried?

---

That's when I decided to try 4.0.25...which points back to the
top of this email...

so...missing 'func_quote_for_eval'?  relation to am-1.15?
am-1.16 usable instead?

Thanks,
-linda







From bogdan.stoica at epfl.ch  Sun Jun 17 08:28:44 2018
From: bogdan.stoica at epfl.ch (Stoica Bogdan Alexandru)
Date: Sun, 17 Jun 2018 08:28:44 +0000
Subject: [squid-users] Squid test-suite / benchmarks
In-Reply-To: <CADZv+uaEqc_e9dvig=-OadzqzhooPJpeBmrv_BZZuanO9zifoQ@mail.gmail.com>
References: <ecb7106901294fbf9060c47a58ec91cf@rexe.intranet.epfl.ch>
 <27872f62-8d80-1913-28ec-4b47593f8fa5@measurement-factory.com>
 <CADZv+uaEqc_e9dvig=-OadzqzhooPJpeBmrv_BZZuanO9zifoQ@mail.gmail.com>
Message-ID: <0643268722434eaeb58448d7a47fb2dc@rexe.intranet.epfl.ch>

Thank you all for your suggestions. Polygraph is a good benchmark, but unfortunately it has a strict terms & conditions when it comes to publishing results and we plan to make the measurements part of a research paper.

Thanks again!

From: Coenraad Loubser [mailto:coenraad at wish.org.za]
Sent: Thursday, June 7, 2018 11:26 AM
To: Stoica Bogdan Alexandru <bogdan.stoica at epfl.ch>
Cc: squid-users at lists.squid-cache.org; Alex Rousskov <rousskov at measurement-factory.com>
Subject: Re: [squid-users] Squid test-suite / benchmarks

My first port of call would be apachebench with and without your proxies. A web search for "squid apachebench" might yield some leads to people who have done this. (I'm sure apachebench is well tested.)
Eg. the third hit on Google: https://2bits.com/articles/using-apachebench-benchmarking-logged-users-automated-approach.html

On Thu, Jun 7, 2018 at 7:20 PM, Alex Rousskov <rousskov at measurement-factory.com<mailto:rousskov at measurement-factory.com>> wrote:
On 06/07/2018 04:17 AM, Stoica Bogdan Alexandru wrote:

> We?re a small research team interested in benchmarking Squid for a
> research project.

> Ideally, we would like to have good code coverage while doing so.

> Are there any good benchmarks used for such purpose?

Performance benchmarks usually focus on things other than code coverage.
It is very difficult to write a quality benchmark for a proxy, even
without code coverage as a goal!

One the other hand, a decent proxy benchmark has enough knobs to tickle
most "interesting" code paths in Squid (or any other proxy). Web
Polygraph[1] (mentioned on this thread earlier) is a good example -- you
can trigger cache revalidation, simulate heavy tailed hit distributions
that stress disk caching, exercise the code that handles aborted
transactions, persistent connection races, etc., etc.


> Or, even better, is
> there a more comprehensive test suite apart from the one Squid comes with?

Squid does not come with a comprehensive test suite (yet) and the tests
distributed with Squid are not performance tests (a.k.a. "benchmarks").
If you are looking for functionality rather than performance testing,
then there is Co-Advisor[2]. Squid is tested with Co-Advisor, but those
tests have not been automated (yet).

  [1] http://www.web-polygraph.org/
  [2] http://coad.measurement-factory.com/


HTH,

Alex.
P.S. Disclaimer: The company I work for is responsible for both of the
test tools mentioned above.
_______________________________________________
squid-users mailing list
squid-users at lists.squid-cache.org<mailto:squid-users at lists.squid-cache.org>
http://lists.squid-cache.org/listinfo/squid-users



--

Coenraad Loubser

Wireless Internet Services & Hardware (Pty) Ltd.
210 Long Street, Cape Town, 8001, ZA

Office: +27 21 481 1824
Skype: Coenraad_Loubser
Email: coenraad at wish.org.za<mailto:coenraad at wish.org.za>
Cell: +27 73 772 1223

-- Spending Money is like watering a plant.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20180617/92ea05a5/attachment.htm>

From squid-user at tlinx.org  Sun Jun 17 10:04:11 2018
From: squid-user at tlinx.org (L A Walsh)
Date: Sun, 17 Jun 2018 03:04:11 -0700
Subject: [squid-users] 4.0.25 -- build, ok... probs on 3.x still there,
 but may not be so important?
In-Reply-To: <5B261BA6.1090008@tlinx.org>
References: <5B261BA6.1090008@tlinx.org>
Message-ID: <5B26321B.7050102@tlinx.org>

It seems I didn't patch it correctly...weird didn't see
any output.

Tried applying .25 patch to copy of .24 dir and it didn't
apply correctly, since I d/l the tarball, and now it build
fine...so.... gonna go play with this now...

FWIW, though the 3.x issues -- compiler changed...not sure
what else would be causing weird issues...
one of them was deprecation warnings and that could easily
be the new compiler...
Anyway...gonna go try to get 4.0.25 running...

sorry for bother.



L A Walsh wrote:

> Also, tried recompiling 3.5.21+22 and ran into some issues (which
> don't really need solving if I get 4.0.25 up and running, but thought
> I'd mention them as they seem to be related to me using a more recent
> gcc toolchain (7.1.0).
> Got a few warnings that were escalated into errors:
> 
> 1 was due to something deprecated (from 3.5.21 build)
> 
> In file included from ../../include/util.h:37:0,
>                  from ntlmauth.cc:20:
> ../../include/SquidNew.h:21:51: error: dynamic exception specifications 
> are deprecated in C++11 [-Werror=deprecated]
>  _SQUID_EXTERNNEW_ void *operator new(size_t size) throw (std::bad_alloc)
>                                                    ^~~~~
> ../../include/SquidNew.h:29:54: error: dynamic exception specifications 
> are deprecated in C++11 [-Werror=deprecated]
>  _SQUID_EXTERNNEW_ void *operator new[] (size_t size) throw (std::bad_alloc)
> --------
> 
> After disabling that fatal warning, I ran into a different warning
> in the FtpGateway.cc file:
> FtpGateway.cc: In member function ?const char* Ftp::Gateway::ftpRealm()?:
> FtpGateway.cc:1288:1: error: ?%s? directive output may be truncated 
> writing up to 8191 bytes into a region of size 8188 
> [-Werror=format-truncation=]
>  Ftp::Gateway::ftpRealm()
>  ^~~
> FtpGateway.cc:1294:17: note: ?snprintf? output between 13 and 8204 bytes 
> into a destination of size 8192
>          snprintf(realm, 8192, "FTP %s unknown", user);
>          ~~~~~~~~^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
> FtpGateway.cc:1288:1: error: ?%s? directive output may be truncated 
> writing up to 8191 bytes into a region of size 8188 
> [-Werror=format-truncation=]
>  Ftp::Gateway::ftpRealm()
>  ^~~
> ...
> ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
> FtpGateway.cc: In function ?void ftpSendUser(Ftp::Gateway*)?:
> FtpGateway.cc:1304:1: error: ?%s? directive output may be truncated 
> writing up to 8191 bytes into a region of size 1019 
> [-Werror=format-truncation=]
>  ftpSendUser(Ftp::Gateway * ftpState)
>  ^~~~~~~~~~~
> cc1plus: all warnings being treated as errors
> 
> There were others, of the same sort that I didn't copy here.  I turned off
> fatal warnings, as this used to work under an older compiler,
> but then some things didn't want to link:
> 
>   CXXLD    pinger
> debug.o: In function `FileNameHashCached(char const*)':
> debug.cc:(.text+0x4e): undefined reference to 
> `TextException::FileNameHash(char const*)'
> globals.o: In function `FileNameHashCached(char const*)':
> globals.cc:(.text+0x2c): undefined reference to 
> `TextException::FileNameHash(char const*)'
> SquidConfig.o: In function `FileNameHashCached(char const*)':
> SquidConfig.cc:(.text+0x2c): undefined reference to 
> `TextException::FileNameHash(char const*)'
> stub_HelperChildConfig.o: In function `FileNameHashCached(char const*)':
> stub_HelperChildConfig.cc:(.text+0x2c): undefined reference to 
> `TextException::FileNameHash(char const*)'
> collect2: error: ld returned 1 exit status
> Makefile:880: recipe for target 'pinger' failed
> make[3]: *** [pinger] Error 1
> 
> ----
> Made wonder if maybe gcc V7.1.0 wasn't supported or tried?
> 
> ---
> 
> That's when I decided to try 4.0.25...which points back to the
> top of this email...
> 
> so...missing 'func_quote_for_eval'?  relation to am-1.15?
> am-1.16 usable instead?
> 
> Thanks,
> -linda
> 
> 
> 
> 
> 
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
> 



From mailbox.kedar at gmail.com  Sun Jun 17 14:08:47 2018
From: mailbox.kedar at gmail.com (Kedar K)
Date: Sun, 17 Jun 2018 19:38:47 +0530
Subject: [squid-users] host header forgery check in docker environment
Message-ID: <CAGUJt2Cz82xBdMhKW_TUhy2mKuEbHDMr6Ct=seKWTz1-xNAydg@mail.gmail.com>

Hello,

I am hitting this issue when running squid in a docker with ssl parent
cache_peer.

Host header forgery detected on local=11 72.19.0.2:443 remote=
172.19.0.1:44522 FD 15 flags=33 (local IP does not match any domain IP)

?The host ip of the docker would not resolve to a domain. How to
work-around this problem??

*- Keda?r?*
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20180617/4b718488/attachment.htm>

From squid3 at treenet.co.nz  Mon Jun 18 03:55:16 2018
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Mon, 18 Jun 2018 15:55:16 +1200
Subject: [squid-users] 4.0.25 -- build, ok... probs on 3.x still there,
 but may not be so important?
In-Reply-To: <5B26321B.7050102@tlinx.org>
References: <5B261BA6.1090008@tlinx.org> <5B26321B.7050102@tlinx.org>
Message-ID: <c8406972-8c8b-8ec3-020f-aabd8e0913cb@treenet.co.nz>

On 17/06/18 22:04, L A Walsh wrote:
> It seems I didn't patch it correctly...weird didn't see
> any output.
> 
> Tried applying .25 patch to copy of .24 dir and it didn't
> apply correctly, since I d/l the tarball, and now it build
> fine...so.... gonna go play with this now...
> 

FYI: if you are applying the inter-version patches instead of building
from a clean copy of the sources in the new release's tarball then you
are definitely changing the .am etc files those WARNING are talking
about. It is just that you are also changing the Makefile as if automake
was being used to generate from that file. So the change is not truly
relevant, but lbltdl still detects and warns.

You can usually avoid auto-tools mismatch issues by using the
--with-included-ltdl or --without-included-ltdl build options, depending
on which of them you are using now that produces the warning.


> FWIW, though the 3.x issues -- compiler changed...not sure
> what else would be causing weird issues...
> one of them was deprecation warnings and that could easily
> be the new compiler...

These were fixed in the 3.5 release since .21 came out, current release
in that series is 3.5.27.

Amos


From squid3 at treenet.co.nz  Mon Jun 18 04:13:56 2018
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Mon, 18 Jun 2018 16:13:56 +1200
Subject: [squid-users] host header forgery check in docker environment
In-Reply-To: <CAGUJt2Cz82xBdMhKW_TUhy2mKuEbHDMr6Ct=seKWTz1-xNAydg@mail.gmail.com>
References: <CAGUJt2Cz82xBdMhKW_TUhy2mKuEbHDMr6Ct=seKWTz1-xNAydg@mail.gmail.com>
Message-ID: <e1a165de-7201-5e67-9cb9-e085625db625@treenet.co.nz>

On 18/06/18 02:08, Kedar K wrote:
> Hello,
> 
> I am hitting this issue when running squid in a docker with ssl parent
> cache_peer.
> 

Can you describe that a bit clearer please? An end-client, two proxies
and origin server makes four HTTP agents involved with this traffic.

 Which of those proxies (and/or server) is inside the container?

 And how are you getting the traffic from the client to the first proxy?


> Host header forgery detected on local=11 72.19.0.2:443
> remote=172.19.0.1:44522 
> FD 15 flags=33 (local IP does not match any domain IP)
> 
> ?The host ip of the docker would not resolve to a domain. How to
> work-around this problem??

The agent being client for the proxy reporting this message apparently
thinks there is a origin server running at "72.19.0.2:443" hosting some
domain name. They are trying to contact that origin server.



Amos


From squid3 at treenet.co.nz  Mon Jun 18 04:30:44 2018
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Mon, 18 Jun 2018 16:30:44 +1200
Subject: [squid-users] Active Directory Integration?
In-Reply-To: <CAK2yrTagu78DJ6NR05bOqjc9B1mD=TJyRKc6GGv-Y9gk68b_UQ@mail.gmail.com>
References: <CAK2yrTagu78DJ6NR05bOqjc9B1mD=TJyRKc6GGv-Y9gk68b_UQ@mail.gmail.com>
Message-ID: <14c3e016-280e-25fe-9cc9-6b9fdb1ba5a6@treenet.co.nz>

On 17/06/18 18:36, Periko Support wrote:
> Hi people.
> 
> If we need to integrate squid 3.5+ with a windows domain AD(2008+) and
> authenticated users from the domain.
> 
> Linux(squid) need to be part of the domain?

Depends on what authentication types you want to use, and how you want
to use them.

The proxy needs the ability to ask the DC about credentials.  If you use
a prepared helper, decide the auth scheme and look at what the helpers
for that scheme can do.


> Or we can just enable a common LDAP query to the AD server?

What do you mean by "common" ?

LDAP requires credentials for the proxy to login to the DC with, in
order to check credentials etc. So it may or may not work off-domain.

Amos


From mailbox.kedar at gmail.com  Mon Jun 18 04:54:01 2018
From: mailbox.kedar at gmail.com (Kedar K)
Date: Mon, 18 Jun 2018 10:24:01 +0530
Subject: [squid-users] host header forgery check in docker environment
In-Reply-To: <e1a165de-7201-5e67-9cb9-e085625db625@treenet.co.nz>
References: <CAGUJt2Cz82xBdMhKW_TUhy2mKuEbHDMr6Ct=seKWTz1-xNAydg@mail.gmail.com>
 <e1a165de-7201-5e67-9cb9-e085625db625@treenet.co.nz>
Message-ID: <CAGUJt2C1rW5yisbA-fLpRS8LbULShHKmOmSyZyBhmwfS3Q1VyA@mail.gmail.com>

Hi Amos,
Here is the topology:

client (curl from host running docker) --> squid_child (docker, using
ssl-bump with intercept) --> squid_parent (VM with internet connection,
https_port without ssl-bump) --> origin server.

local - 72.19.0.2:443 is the container running squid child
remote - remote=172.19.0.1:44522  is the host machine where containers are
running, I am using a curl to do initial tests. Eventually, request would
come from other containers or external hosts on the docker daemon host.

With http traffic this works fine; wherein the request is forwarded to
Parent and then to origin server. However, with https header forgery kicks
in and tls is terminated.

- Kedar

On Mon, Jun 18, 2018 at 9:44 AM Amos Jeffries <squid3 at treenet.co.nz> wrote:

> On 18/06/18 02:08, Kedar K wrote:
> > Hello,
> >
> > I am hitting this issue when running squid in a docker with ssl parent
> > cache_peer.
> >
>
> Can you describe that a bit clearer please? An end-client, two proxies
> and origin server makes four HTTP agents involved with this traffic.
>
>  Which of those proxies (and/or server) is inside the container?
>
>  And how are you getting the traffic from the client to the first proxy?
>
>
> > Host header forgery detected on local=11 72.19.0.2:443
> > remote=172.19.0.1:44522
> > FD 15 flags=33 (local IP does not match any domain IP)
> >
> > ?The host ip of the docker would not resolve to a domain. How to
> > work-around this problem??
>
> The agent being client for the proxy reporting this message apparently
> thinks there is a origin server running at "72.19.0.2:443" hosting some
> domain name. They are trying to contact that origin server.
>
>
>
> Amos
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>


-- 

*- Kedar Kekan*
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20180618/5ca4aa11/attachment.htm>

From squid3 at treenet.co.nz  Mon Jun 18 07:43:19 2018
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Mon, 18 Jun 2018 19:43:19 +1200
Subject: [squid-users] host header forgery check in docker environment
In-Reply-To: <CAGUJt2C1rW5yisbA-fLpRS8LbULShHKmOmSyZyBhmwfS3Q1VyA@mail.gmail.com>
References: <CAGUJt2Cz82xBdMhKW_TUhy2mKuEbHDMr6Ct=seKWTz1-xNAydg@mail.gmail.com>
 <e1a165de-7201-5e67-9cb9-e085625db625@treenet.co.nz>
 <CAGUJt2C1rW5yisbA-fLpRS8LbULShHKmOmSyZyBhmwfS3Q1VyA@mail.gmail.com>
Message-ID: <90baae2b-1856-3128-c39a-58126155d6b3@treenet.co.nz>

On 18/06/18 16:54, Kedar K wrote:
> Hi Amos,?
> Here is the topology:
> 
> client (curl from host running docker) --> squid_child (docker, using
> ssl-bump with intercept) --> squid_parent (VM with internet connection,
> https_port without ssl-bump) --> origin server.

Consider where/how the child proxy is getting the origin servers' TLS
certificate details with which to forge a server certificate in the bump
action.


> 
> local -?72.19.0.2:443 <http://72.19.0.2:443/>?is the container running
> squid child
> remote -?remote=172.19.0.1:44522 <http://172.19.0.1:44522/>? is the host
> machine where containers are running, I am using a curl to do initial
> tests. Eventually, request would come from other containers or external
> hosts on the docker daemon host.
> 
> With http traffic this works fine; wherein the request is forwarded to
> Parent and then to origin server. However, with https header forgery
> kicks in and tls is terminated.

Given that you are essentially void'ing what little security TLS
provides, there is no point in using it to secure any of these
connections. Just use curl (or squidclient) to send https:// URLs in
plain text HTTP messages. It is just as (in)secure as your current setup
and works much more reliably.

Amos


From vh1988 at yahoo.com.ar  Mon Jun 18 11:31:39 2018
From: vh1988 at yahoo.com.ar (Julian Perconti)
Date: Mon, 18 Jun 2018 08:31:39 -0300
Subject: [squid-users] SSL errors with Squid 3.5.27
In-Reply-To: <20180614095606.GA9532@fantomas.sk>
References: <010201d3ff7e$9b259510$d170bf30$@yahoo.com.ar>
 <9fcd2760-281d-63f2-52e4-983ac8c5b052@treenet.co.nz>
 <01b501d40008$fc418e00$f4c4aa00$@yahoo.com.ar>
 <8a1755bc-5257-1db0-8430-ab8fc62fdb77@treenet.co.nz>
 <00f801d40287$39940730$acbc1590$@yahoo.com.ar>
 <48966683-1714-9daa-673a-594a0a06609c@treenet.co.nz>
 <00da01d4035c$6e7e1be0$4b7a53a0$@yahoo.com.ar>
 <20180614095606.GA9532@fantomas.sk>
Message-ID: <002a01d406f7$eecc4440$cc64ccc0$@yahoo.com.ar>

> have you tried -servername option for setting SNI extension?

How can i do this?



Well, debbuging cache.log i found this:

2018/06/18 08:22:08.822 kid1| 83,5| support.cc(300) ssl_verify_cb: Self signed certificate in certificate chain: /CN=courier.push.apple.com/O=Apple Inc./ST=California/C=US
2018/06/18 08:22:08.822 kid1| 83,7| bio.cc(168) stateChanged: FD 16 now: 0x4008 3RSC_B (SSLv3 read server certificate B)
2018/06/18 08:22:08.822 kid1| 83,7| bio.cc(168) stateChanged: FD 16 now: 0x1002 3RSC_B (SSLv3 read server certificate B)
2018/06/18 08:22:08.823 kid1| Error negotiating SSL on FD 16: error:14007086:SSL routines:CONNECT_CR_CERT:certificate verify failed (1/-1/0)
2018/06/18 08:22:08.825 kid1| 4,3| errorpage.cc(1100) Convert: errorConvert: %%D --> 'Self-signed SSL Certificate in chain: /C=US/O=Apple Inc./OU=Apple Certification Authority/CN=Apple Root CA'
2018/06/18 08:22:08.830 kid1| 33,5| client_side.cc(4185) getSslContextStart: Generating SSL certificate for courier.push.apple.com using ssl_crtd.
2018/06/18 08:22:08.831 kid1| 33,5| client_side.cc(4189) getSslContextStart: SSL crtd request: new_certificate 3294 host=courier.push.apple.com
-----BEGIN CERTIFICATE-----
-----END CERTIFICATE-----
2018/06/18 08:22:08.831 kid1| 84,9| helper.cc(386) helperSubmit:  buf[3316]=new_certificate] 3294 host=courier.push.apple.com
-----BEGIN CERTIFICATE-----
-----END CERTIFICATE-----
2018/06/18 08:22:08.835 kid1| 84,9| helper.cc(875) helperHandleRead:  accumulated[3002]=OK] 2993 -----BEGIN CERTIFICATE-----
-----END CERTIFICATE-----

On Android devices WhatsApp Works fine, slow but it woks.

I think that the main problem resides in this line:

ssl_verify_cb: Self signed certificate in certificate chain:

courier.push.apple.com is entrust L1K chain...( if I?m not wrong)

Any idea?



From pamrtj at gmail.com  Mon Jun 18 16:53:59 2018
From: pamrtj at gmail.com (Beto Moreno)
Date: Mon, 18 Jun 2018 09:53:59 -0700
Subject: [squid-users] Active Directory Authentication?
Message-ID: <CAAJD-mDFULhJRd=CNrzXTvLLcbKpvX0TknrNhBSVSZ2sCsMj+A@mail.gmail.com>

Hi guys.

Just wondering, if we want squid Authenticate users from our Active
Directory Windows 2012 server, do we need to have our Linux-Squid
3.5.x be part of the domain or a LDAP query can work without be part
of the domain?

Thanks.


From uhlar at fantomas.sk  Mon Jun 18 17:05:52 2018
From: uhlar at fantomas.sk (Matus UHLAR - fantomas)
Date: Mon, 18 Jun 2018 19:05:52 +0200
Subject: [squid-users] SSL errors with Squid 3.5.27
In-Reply-To: <002a01d406f7$eecc4440$cc64ccc0$@yahoo.com.ar>
References: <010201d3ff7e$9b259510$d170bf30$@yahoo.com.ar>
 <9fcd2760-281d-63f2-52e4-983ac8c5b052@treenet.co.nz>
 <01b501d40008$fc418e00$f4c4aa00$@yahoo.com.ar>
 <8a1755bc-5257-1db0-8430-ab8fc62fdb77@treenet.co.nz>
 <00f801d40287$39940730$acbc1590$@yahoo.com.ar>
 <48966683-1714-9daa-673a-594a0a06609c@treenet.co.nz>
 <00da01d4035c$6e7e1be0$4b7a53a0$@yahoo.com.ar>
 <20180614095606.GA9532@fantomas.sk>
 <002a01d406f7$eecc4440$cc64ccc0$@yahoo.com.ar>
Message-ID: <20180618170551.GA10370@fantomas.sk>

>> have you tried -servername option for setting SNI extension?

On 18.06.18 08:31, Julian Perconti wrote:
>How can i do this?

man s_client:\

        -servername name
            Set the TLS SNI (Server Name Indication) extension in the
            ClientHello message.

-- 
Matus UHLAR - fantomas, uhlar at fantomas.sk ; http://www.fantomas.sk/
Warning: I wish NOT to receive e-mail advertising to this address.
Varovanie: na tuto adresu chcem NEDOSTAVAT akukolvek reklamnu postu.
Eagles may soar, but weasels don't get sucked into jet engines. 


From vh1988 at yahoo.com.ar  Mon Jun 18 20:01:07 2018
From: vh1988 at yahoo.com.ar (Julian Perconti)
Date: Mon, 18 Jun 2018 17:01:07 -0300
Subject: [squid-users] SSL errors with Squid 3.5.27
References: <010201d3ff7e$9b259510$d170bf30$@yahoo.com.ar>
 <9fcd2760-281d-63f2-52e4-983ac8c5b052@treenet.co.nz>
 <01b501d40008$fc418e00$f4c4aa00$@yahoo.com.ar>
 <8a1755bc-5257-1db0-8430-ab8fc62fdb77@treenet.co.nz>
 <00f801d40287$39940730$acbc1590$@yahoo.com.ar>
 <48966683-1714-9daa-673a-594a0a06609c@treenet.co.nz>
 <00da01d4035c$6e7e1be0$4b7a53a0$@yahoo.com.ar>
 <20180614095606.GA9532@fantomas.sk> 
Message-ID: <009701d4073f$1a029d20$4e07d760$@yahoo.com.ar>

Googling i foind this cfg lines:

acl SSLERR ssl_error X509_V_ERR_DEPTH_ZERO_SELF_SIGNED_CERT X509_V_ERR_SELF_SIGNED_CERT_IN_CHAIN

sslproxy_cert_error allow SSLERR

sslproxy_cert_error deny all

The error " certificate verify failed has deissappeared, I refer  to this error:

routines:CONNECT_CR_CERT:certificate verify failed (1/-1/0)
2018/06/18 08:22:08.825 kid1| 4,3| errorpage.cc(1100) Convert: errorConvert: %%D --> 'Self-signed SSL Certificate in chain: /C=US/O=Apple Inc./OU=Apple Certification Authority/CN=Apple Root CA'

But... WhatsApp on iOS devices still not working. 
So that was not the root cause of the problem.

On Android WhatsApp Works fine.

Any other idea?

Thank You.




From rousskov at measurement-factory.com  Mon Jun 18 20:33:21 2018
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Mon, 18 Jun 2018 14:33:21 -0600
Subject: [squid-users] Squid test-suite / benchmarks
In-Reply-To: <0643268722434eaeb58448d7a47fb2dc@rexe.intranet.epfl.ch>
References: <ecb7106901294fbf9060c47a58ec91cf@rexe.intranet.epfl.ch>
 <27872f62-8d80-1913-28ec-4b47593f8fa5@measurement-factory.com>
 <CADZv+uaEqc_e9dvig=-OadzqzhooPJpeBmrv_BZZuanO9zifoQ@mail.gmail.com>
 <0643268722434eaeb58448d7a47fb2dc@rexe.intranet.epfl.ch>
Message-ID: <0bd2f205-e6ea-b504-dfc0-396e56f280ae@measurement-factory.com>

On 06/17/2018 02:28 AM, Stoica Bogdan Alexandru wrote:
> Thank you all for your suggestions. Polygraph is a good benchmark, but
> unfortunately it has a strict terms & conditions when it comes to
> publishing results and we plan to make the measurements part of a
> research paper.

Polygraph license (Apache v2) does not restrict or regulate publication
of the results.

Alex.

 ?

> *From:*Coenraad Loubser [mailto:coenraad at wish.org.za]
> *Sent:* Thursday, June 7, 2018 11:26 AM
> *To:* Stoica Bogdan Alexandru <bogdan.stoica at epfl.ch>
> *Cc:* squid-users at lists.squid-cache.org; Alex Rousskov
> <rousskov at measurement-factory.com>
> *Subject:* Re: [squid-users] Squid test-suite / benchmarks
> 
> ?
> 
> My first port of call would be apachebench with and without your
> proxies. A web search for "squid apachebench" might yield some leads to
> people who have done this. (I'm sure apachebench is well tested.)
> 
> Eg. the third hit on Google:
> https://2bits.com/articles/using-apachebench-benchmarking-logged-users-automated-approach.html
> 
> ?
> 
> On Thu, Jun 7, 2018 at 7:20 PM, Alex Rousskov
> <rousskov at measurement-factory.com
> <mailto:rousskov at measurement-factory.com>> wrote:
> 
>     On 06/07/2018 04:17 AM, Stoica Bogdan Alexandru wrote:
> 
>     > We?re a small research team interested in benchmarking Squid for a
>     > research project.
> 
>     > Ideally, we would like to have good code coverage while doing so.
> 
>     > Are there any good benchmarks used for such purpose?
> 
>     Performance benchmarks usually focus on things other than code coverage.
>     It is very difficult to write a quality benchmark for a proxy, even
>     without code coverage as a goal!
> 
>     One the other hand, a decent proxy benchmark has enough knobs to tickle
>     most "interesting" code paths in Squid (or any other proxy). Web
>     Polygraph[1] (mentioned on this thread earlier) is a good example -- you
>     can trigger cache revalidation, simulate heavy tailed hit distributions
>     that stress disk caching, exercise the code that handles aborted
>     transactions, persistent connection races, etc., etc.
> 
> 
>     > Or, even better, is
>     > there a more comprehensive test suite apart from the one Squid
>     comes with?
> 
>     Squid does not come with a comprehensive test suite (yet) and the tests
>     distributed with Squid are not performance tests (a.k.a. "benchmarks").
>     If you are looking for functionality rather than performance testing,
>     then there is Co-Advisor[2]. Squid is tested with Co-Advisor, but those
>     tests have not been automated (yet).
> 
>     ? [1] http://www.web-polygraph.org/
>     ? [2] http://coad.measurement-factory.com/
> 
> 
>     HTH,
> 
>     Alex.
>     P.S. Disclaimer: The company I work for is responsible for both of the
>     test tools mentioned above.
> 
>     _______________________________________________
>     squid-users mailing list
>     squid-users at lists.squid-cache.org
>     <mailto:squid-users at lists.squid-cache.org>
>     http://lists.squid-cache.org/listinfo/squid-users
> 
> 
> 
> 
> -- 
> 
> 
> Coenraad Loubser
> 
> Wireless Internet Services & Hardware (Pty) Ltd.
> 210 Long Street, Cape Town, 8001, ZA
> 
> Office: +27 21 481 1824
> Skype: Coenraad_Loubser
> Email: coenraad at wish.org.za <mailto:coenraad at wish.org.za>
> Cell: +27 73 772 1223
> 
> -- Spending Money is like watering a plant.
> 



From Sarfaraz.Ahmad at deshaw.com  Tue Jun 19 03:36:46 2018
From: Sarfaraz.Ahmad at deshaw.com (Ahmad, Sarfaraz)
Date: Tue, 19 Jun 2018 03:36:46 +0000
Subject: [squid-users] Use additional details in SAN field to build ACLs
Message-ID: <281dfa83a5944117a0b24a2cc6f610a5@mbxtoa3.winmail.deshaw.com>

Hi,

Can I leverage other information available in a server certificates's SAN field to build my ACLs ?
Here's a sample from the SAN field ,
DNS Name=abc.example.com
IP Address=10.0.97.72

I haven't tried it but would using ssl::server_name_regex work to match IP=10.0.97.* work?
Also I couldn't find a way to capture ssl::server_name (that Squid builds as described in the "acl" directive doc) in the logs. Logformat directive has only some bits of ssl information.

Regards,
Sarfaraz
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20180619/a3dccb3e/attachment.htm>

From ahmed.zaeem at netstream.ps  Tue Jun 19 05:12:29 2018
From: ahmed.zaeem at netstream.ps (--Ahmad--)
Date: Tue, 19 Jun 2018 08:12:29 +0300
Subject: [squid-users] how debug google status codes in log file
Message-ID: <234760E2-B23D-4F1D-80D0-DC3C7D47B8A3@netstream.ps>

hello folks 
how debug google status codes in log file  ?

in wiki i see we have :

1529368601.307  60038 184.154.133.146 TAG_NONE/503 0 CONNECT www.google.com.et:443 fifoxy HIER_NONE/- -

the question is how can i see the http status code of connection in proxy ?

regards 



From ahmed.zaeem at netstream.ps  Tue Jun 19 05:24:46 2018
From: ahmed.zaeem at netstream.ps (--Ahmad--)
Date: Tue, 19 Jun 2018 08:24:46 +0300
Subject: [squid-users] how debug google status codes in log file
In-Reply-To: <234760E2-B23D-4F1D-80D0-DC3C7D47B8A3@netstream.ps>
References: <234760E2-B23D-4F1D-80D0-DC3C7D47B8A3@netstream.ps>
Message-ID: <39360DB8-5A17-4CC6-BD90-CEAFD9ABB89F@netstream.ps>

also how about if the tcp was tcp_tunnel

like below :

17/Jun/2018:08:18:09 -0400    559 6xxxxxxxxxxx33833 xxxxx 2000 TCP_TUNNEL/200 1974 CONNECT www.google.com:443 xxxHIER_DIRECT/ www.google.com 2607:f8b0:4005:809::2004 xxxxc9f0:dfde:2da5:c4c0:7148:3646




all my logs from google is TCP_TUNNEL/200 for all types of message 

any way to differentiate the http status code ?

im hitting google https 


let me know 

thanks 



> On 19 Jun 2018, at 8:12 AM, --Ahmad-- <ahmed.zaeem at netstream.ps> wrote:
> 
> hello folks 
> how debug google status codes in log file  ?
> 
> in wiki i see we have :
> 
> 1529368601.307  60038 184.154.133.146 TAG_NONE/503 0 CONNECT www.google.com.et:443 fifoxy HIER_NONE/- -
> 
> the question is how can i see the http status code of connection in proxy ?
> 
> regards 
> 



From jascha.sticher at tds.fujitsu.com  Tue Jun 19 08:50:46 2018
From: jascha.sticher at tds.fujitsu.com (Sticher, Jascha)
Date: Tue, 19 Jun 2018 08:50:46 +0000
Subject: [squid-users] how debug google status codes in log file
In-Reply-To: <39360DB8-5A17-4CC6-BD90-CEAFD9ABB89F@netstream.ps>
References: <234760E2-B23D-4F1D-80D0-DC3C7D47B8A3@netstream.ps>
 <39360DB8-5A17-4CC6-BD90-CEAFD9ABB89F@netstream.ps>
Message-ID: <E286ADE35F919742812E3076A36122E701BFCBD41E@tdsnsumbx2vp>

Hi Ahmad,

every HTTPS connection is TUNNELED through squid, as long as you do not intercept SSL traffic (SSLbump). You can only get either a 200 return code or error codes connected to failure of the connection, e.g. proxy authentication required, deny (by proxy) or network/dns issues. You will not see any return codes the likes of 301 (redirect), server authentication or 404 (not found). That's pretty much the point of a tunnel.


Kind regards,

Jascha

-----Urspr?ngliche Nachricht-----
Von: squid-users <squid-users-bounces at lists.squid-cache.org> Im Auftrag von --Ahmad--
Gesendet: Dienstag, 19. Juni 2018 07:25
An: Squid Users <squid-users at lists.squid-cache.org>
Betreff: Re: [squid-users] how debug google status codes in log file

also how about if the tcp was tcp_tunnel

like below :

17/Jun/2018:08:18:09 -0400    559 6xxxxxxxxxxx33833 xxxxx 2000 TCP_TUNNEL/200 1974 CONNECT www.google.com:443 xxxHIER_DIRECT/ www.google.com 2607:f8b0:4005:809::2004 xxxxc9f0:dfde:2da5:c4c0:7148:3646




all my logs from google is TCP_TUNNEL/200 for all types of message 

any way to differentiate the http status code ?

im hitting google https 


let me know 

thanks 



> On 19 Jun 2018, at 8:12 AM, --Ahmad-- <ahmed.zaeem at netstream.ps> wrote:
> 
> hello folks 
> how debug google status codes in log file  ?
> 
> in wiki i see we have :
> 
> 1529368601.307  60038 184.154.133.146 TAG_NONE/503 0 CONNECT www.google.com.et:443 fifoxy HIER_NONE/- -
> 
> the question is how can i see the http status code of connection in proxy ?
> 
> regards 
> 

_______________________________________________
squid-users mailing list
squid-users at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-users

From Christian.Wohlgemuth at raiffeisen.it  Tue Jun 19 13:13:14 2018
From: Christian.Wohlgemuth at raiffeisen.it (Christian Wohlgemuth)
Date: Tue, 19 Jun 2018 15:13:14 +0200
Subject: [squid-users] YUM Repo
Message-ID: <OF5B33EE8E.5C27908E-ONC12582B1.00480B18-C12582B1.00489FB3@raiffeisen.it>

Hi,

there are problems with the yum repo?

> http://www1.ngtech.co.il/repo/centos/6/x86_64/

When i try to sync with our local pulp server, come e timeout.


thanks
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20180619/9fa65ac9/attachment.htm>

From rousskov at measurement-factory.com  Tue Jun 19 15:37:42 2018
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Tue, 19 Jun 2018 09:37:42 -0600
Subject: [squid-users] Use additional details in SAN field to build ACLs
In-Reply-To: <281dfa83a5944117a0b24a2cc6f610a5@mbxtoa3.winmail.deshaw.com>
References: <281dfa83a5944117a0b24a2cc6f610a5@mbxtoa3.winmail.deshaw.com>
Message-ID: <1c3fd257-5579-630a-86df-89f3a81c4164@measurement-factory.com>

On 06/18/2018 09:36 PM, Ahmad, Sarfaraz wrote:

> Can I leverage other information available in a server certificates?s
> SAN field to build my ACLs ?

Unfortunately, Squid does not have ACLs that can match non-dNSName[1]
parts of the Subject Alternative Name extension.

[1] https://tools.ietf.org/html/rfc5280#section-4.2.1.6


> I haven?t tried it but would using ssl::server_name_regex work to match
> IP=10.0.97.* work?

No, it should not work. When looking at SAN, Squid only looks at dNSName.


> Also I couldn?t find a way to capture ssl::server_name (that Squid
> builds as described in the ?acl? directive doc) in the logs. Logformat
> directive has only some bits of ssl information.

Squid does not have a logformat %code that would always contain the same
name as the one examined by the ssl::server_name ACL. Moreover, since
ssl::server_name ACL examines different names (depending on the
evaluation timing/context), logging a single value at the end of the
transaction would not tell you what ssl::server_name ACL was dealing with.

Needless to say, it is possible to modify Squid to add ACL(s) that would
interrogate other SAN names and logformat %codes that would log SAN
dNSName and other server certificate details. Same for logging the
equivalent of the final ssl::server_name is also possible.

https://wiki.squid-cache.org/SquidFaq/AboutSquid#How_to_add_a_new_Squid_feature.2C_enhance.2C_of_fix_something.3F

Alex.


From squid3 at treenet.co.nz  Wed Jun 20 05:02:00 2018
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Wed, 20 Jun 2018 17:02:00 +1200
Subject: [squid-users] how debug google status codes in log file
In-Reply-To: <234760E2-B23D-4F1D-80D0-DC3C7D47B8A3@netstream.ps>
References: <234760E2-B23D-4F1D-80D0-DC3C7D47B8A3@netstream.ps>
Message-ID: <9d12ad3a-dcca-31db-ed49-26fbaa904608@treenet.co.nz>

On 19/06/18 17:12, --Ahmad-- wrote:
> hello folks 
> how debug google status codes in log file  ?
> 

see the FAQ
<https://wiki.squid-cache.org/SquidFaq/SquidLogs#access.log>


> in wiki i see we have :
> 
> 1529368601.307  60038 184.154.133.146 TAG_NONE/503 0 CONNECT www.google.com.et:443 fifoxy HIER_NONE/- -
> 
> the question is how can i see the http status code of connection in proxy ?


Which "the" ? there are possibly many connections used for each
transaction (aka access.log line).


The above log line says Squid spent 60sec on this transaction, no
servers were able to be connected to in that time.

Squid also *tried* to deliver a 503 HTTP status message to the client,
but 0 bytes got written. Probably a TCP RST got sent to the client
instead (eg. SSL-Bump "terminate" action).



The *debugging* to happen is nothing to do with status codes. It is
actions you need to perform to figure out why no server contact
happened, and/or why Squid thought it had to send an error.

That assumes that you do not already know what happened, and that it is
not something you wanted to configure (eg SSL-Bump terminate certain TLS
handshakes).

Amos


From Sarfaraz.Ahmad at deshaw.com  Wed Jun 20 09:04:18 2018
From: Sarfaraz.Ahmad at deshaw.com (Ahmad, Sarfaraz)
Date: Wed, 20 Jun 2018 09:04:18 +0000
Subject: [squid-users] Ignore SSL error and splice by ssl::server_name at
	the same time
Message-ID: <52d9c7f3965948c0bc8cf6ceb1846d7a@mbxtoa3.winmail.deshaw.com>

Hi,

I need to provide access to a API service exposed on the internet to my clients. That API uses a certificate signed by a private CA.
I don't want to trust that private CA in my proxies (lest it gets abused and I end up trusting certificates in the proxy that I shouldn't be.  My clients would be unaware since I am bumping all the TLS connections unless explicitly configured. )
To avoid that I tried ignoring the ssl validation error with  sslproxy_cert_error directive and then splicing the connection. But its not working out.

SubjectCN in that services' certificate is "kube-apiserver"

Ignore settings :

acl broken_kubernetes ssl::server_name kube-apiserver
sslproxy_cert_error allow broken_kubernetes
sslproxy_cert_error deny all


Splicing settings:

acl no_ssl_bump_kubernetes ssl::server_name kube-apiserver
acl step1 at_step SslBump1
acl step2 at_step SslBump2
acl step3 at_step SslBump3
ssl_bump peek step1
ssl_bump splice no_ssl_bump_kubernetes
ssl_bump bump all

Splicing settings are in the lower half of my config.
But I am still getting MITM'ed (bumped) and on the clients, I get a "Not Trusted by MyCA" certificate is being shown. Any ideas ?

Regards,
Sarfaraz
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20180620/16ebdea3/attachment.htm>

From Sarfaraz.Ahmad at deshaw.com  Wed Jun 20 09:55:12 2018
From: Sarfaraz.Ahmad at deshaw.com (Ahmad, Sarfaraz)
Date: Wed, 20 Jun 2018 09:55:12 +0000
Subject: [squid-users] Ignore SSL error and splice by ssl::server_name
	at the same time
Message-ID: <2f7b858522314bf995f2226792b9ed45@mbxtoa3.winmail.deshaw.com>

Forgot to add. Remote IP addresses are not expected to remain constant. So I cannot build ACLs that way. So ssl::server_name is the only other hope.

From: Ahmad, Sarfaraz
Sent: Wednesday, June 20, 2018 2:34 PM
To: 'squid-users at lists.squid-cache.org' <squid-users at lists.squid-cache.org>
Subject: Ignore SSL error and splice by ssl::server_name at the same time

Hi,

I need to provide access to a API service exposed on the internet to my clients. That API uses a certificate signed by a private CA.
I don't want to trust that private CA in my proxies (lest it gets abused and I end up trusting certificates in the proxy that I shouldn't be.  My clients would be unaware since I am bumping all the TLS connections unless explicitly configured. )
To avoid that I tried ignoring the ssl validation error with  sslproxy_cert_error directive and then splicing the connection. But its not working out.

SubjectCN in that services' certificate is "kube-apiserver"

Ignore settings :

acl broken_kubernetes ssl::server_name kube-apiserver
sslproxy_cert_error allow broken_kubernetes
sslproxy_cert_error deny all


Splicing settings:

acl no_ssl_bump_kubernetes ssl::server_name kube-apiserver
acl step1 at_step SslBump1
acl step2 at_step SslBump2
acl step3 at_step SslBump3
ssl_bump peek step1
ssl_bump splice no_ssl_bump_kubernetes
ssl_bump bump all

Splicing settings are in the lower half of my config.
But I am still getting MITM'ed (bumped) and on the clients, I get a "Not Trusted by MyCA" certificate is being shown. Any ideas ?

Regards,
Sarfaraz
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20180620/d6e15796/attachment.htm>

From Sarfaraz.Ahmad at deshaw.com  Wed Jun 20 12:25:31 2018
From: Sarfaraz.Ahmad at deshaw.com (Ahmad, Sarfaraz)
Date: Wed, 20 Jun 2018 12:25:31 +0000
Subject: [squid-users] Ignore SSL error and splice by ssl::server_name
	at the same time
Message-ID: <45be4c80c58842568fd50b1825755da5@mbxtoa3.winmail.deshaw.com>

I found the answer to my problem. The SNI and Subject CN were different in my case and I was not peeking at step2 (meaning not looking at the server certificate) that is why my ACLs were ineffective.

Regards,
Sarfaraz

From: Ahmad, Sarfaraz
Sent: Wednesday, June 20, 2018 3:25 PM
To: 'squid-users at lists.squid-cache.org' <squid-users at lists.squid-cache.org>
Subject: RE: Ignore SSL error and splice by ssl::server_name at the same time

Forgot to add. Remote IP addresses are not expected to remain constant. So I cannot build ACLs that way. So ssl::server_name is the only other hope.

From: Ahmad, Sarfaraz
Sent: Wednesday, June 20, 2018 2:34 PM
To: 'squid-users at lists.squid-cache.org' <squid-users at lists.squid-cache.org>
Subject: Ignore SSL error and splice by ssl::server_name at the same time

Hi,

I need to provide access to a API service exposed on the internet to my clients. That API uses a certificate signed by a private CA.
I don't want to trust that private CA in my proxies (lest it gets abused and I end up trusting certificates in the proxy that I shouldn't be.  My clients would be unaware since I am bumping all the TLS connections unless explicitly configured. )
To avoid that I tried ignoring the ssl validation error with  sslproxy_cert_error directive and then splicing the connection. But its not working out.

SubjectCN in that services' certificate is "kube-apiserver"

Ignore settings :

acl broken_kubernetes ssl::server_name kube-apiserver
sslproxy_cert_error allow broken_kubernetes
sslproxy_cert_error deny all


Splicing settings:

acl no_ssl_bump_kubernetes ssl::server_name kube-apiserver
acl step1 at_step SslBump1
acl step2 at_step SslBump2
acl step3 at_step SslBump3
ssl_bump peek step1
ssl_bump splice no_ssl_bump_kubernetes
ssl_bump bump all

Splicing settings are in the lower half of my config.
But I am still getting MITM'ed (bumped) and on the clients, I get a "Not Trusted by MyCA" certificate is being shown. Any ideas ?

Regards,
Sarfaraz
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20180620/c1a64841/attachment.htm>

From squid3 at treenet.co.nz  Wed Jun 20 13:23:02 2018
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 21 Jun 2018 01:23:02 +1200
Subject: [squid-users] Ignore SSL error and splice by ssl::server_name
 at the same time
In-Reply-To: <45be4c80c58842568fd50b1825755da5@mbxtoa3.winmail.deshaw.com>
References: <45be4c80c58842568fd50b1825755da5@mbxtoa3.winmail.deshaw.com>
Message-ID: <0dd73a24-97d2-4836-6321-cbdb6508a6ab@treenet.co.nz>

On 21/06/18 00:25, Ahmad, Sarfaraz wrote:
> I found the answer to my problem. The SNI and Subject CN were different
> in my case and I was not peeking at step2 (meaning not looking at the
> server certificate) that is why my ACLs were ineffective.
> 

Ah, excellent. Does that mean your problem is now resolved?

Amos


From squid3 at treenet.co.nz  Wed Jun 20 13:29:18 2018
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 21 Jun 2018 01:29:18 +1200
Subject: [squid-users] Active Directory Authentication?
In-Reply-To: <CAAJD-mDFULhJRd=CNrzXTvLLcbKpvX0TknrNhBSVSZ2sCsMj+A@mail.gmail.com>
References: <CAAJD-mDFULhJRd=CNrzXTvLLcbKpvX0TknrNhBSVSZ2sCsMj+A@mail.gmail.com>
Message-ID: <db7ea68a-4126-686a-afd7-5e1e8e0c4aa8@treenet.co.nz>

On 19/06/18 04:53, Beto Moreno wrote:
> Hi guys.
> 
> Just wondering, if we want squid Authenticate users from our Active
> Directory Windows 2012 server, do we need to have our Linux-Squid
> 3.5.x be part of the domain or a LDAP query can work without be part
> of the domain?

See my response to the identical question a few days ago
<http://lists.squid-cache.org/pipermail/squid-users/2018-June/018472.html>

You will need to provide more details about what _exactly_ you want to
do in order to get a proper answer. Your description is too vague right
now to answer.


Amos


From Sarfaraz.Ahmad at deshaw.com  Wed Jun 20 14:04:44 2018
From: Sarfaraz.Ahmad at deshaw.com (Ahmad, Sarfaraz)
Date: Wed, 20 Jun 2018 14:04:44 +0000
Subject: [squid-users] Ignore SSL error and splice by ssl::server_name
 at the same time
In-Reply-To: <0dd73a24-97d2-4836-6321-cbdb6508a6ab@treenet.co.nz>
References: <45be4c80c58842568fd50b1825755da5@mbxtoa3.winmail.deshaw.com>
 <0dd73a24-97d2-4836-6321-cbdb6508a6ab@treenet.co.nz>
Message-ID: <70e04c38862b4c6cad13bb23e2ac9952@mbxtoa3.winmail.deshaw.com>

Yes.  As always appreciate the quick support this community provides. :)
Thank you guys !

Regards,
Sarfaraz

-----Original Message-----
From: squid-users <squid-users-bounces at lists.squid-cache.org> On Behalf Of Amos Jeffries
Sent: Wednesday, June 20, 2018 6:53 PM
To: squid-users at lists.squid-cache.org
Subject: Re: [squid-users] Ignore SSL error and splice by ssl::server_name at the same time

On 21/06/18 00:25, Ahmad, Sarfaraz wrote:
> I found the answer to my problem. The SNI and Subject CN were 
> different in my case and I was not peeking at step2 (meaning not 
> looking at the server certificate) that is why my ACLs were ineffective.
> 

Ah, excellent. Does that mean your problem is now resolved?

Amos
_______________________________________________
squid-users mailing list
squid-users at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-users

From michael.adm at gmail.com  Wed Jun 20 19:44:28 2018
From: michael.adm at gmail.com (Michael Pro)
Date: Wed, 20 Jun 2018 22:44:28 +0300
Subject: [squid-users] =?utf-8?b?U3BlZWQg4oCL4oCLbGltaXQgZnJvbSBmaWxlIHNp?=
	=?utf-8?q?ze?=
Message-ID: <CAA+Mow5nyotHcmVSS2kair1ca9BzYgQmwEEPtJWuJ8WC99954Q@mail.gmail.com>

Is it possible to limit the speed of receiving the file to the client
depending on the file size?
Let's say,
file =<1MB client download with 400Kbit/s
file 1MB...5MB client donload with 4Mbit/s
file >5MB - no speed limit

I try fore some settings
delay_pools 1
delay_class 1 2
delay_access 1 allow storeid_app_2
delay_access 1 allow storeid_app_6
delay_access 1 deny all
delay_parameters 1 -1/-1 50000/1000000
delay_parameters 1 -1/-1 500000/5000000
delay_parameters 1 -1/-1 -1/-1
...
but it does't work

Can some method take place to solve this problem?


From eliezer at ngtech.co.il  Thu Jun 21 00:14:57 2018
From: eliezer at ngtech.co.il (Eliezer Croitoru)
Date: Thu, 21 Jun 2018 03:14:57 +0300
Subject: [squid-users] YUM Repo
In-Reply-To: <OF5B33EE8E.5C27908E-ONC12582B1.00480B18-C12582B1.00489FB3@raiffeisen.it>
References: <OF5B33EE8E.5C27908E-ONC12582B1.00480B18-C12582B1.00489FB3@raiffeisen.it>
Message-ID: <33c6d57493f36b44ff1bab200c1293fe@ngtech.co.il>

Hey Christian,

I had the server down for maintenance and also blocked couple lechers.
If the repo is up for you then great.
If it's still not working for you as expected contact me via email and 
we can try to find out why it doesn't work for you.

Eliezer

On 2018-06-19 16:13, Christian Wohlgemuth wrote:
> Hi,
> 
> there are problems with the yum repo?
> 
>> http://www1.ngtech.co.il/repo/centos/6/x86_64/
> 
> When i try to sync with our local pulp server, come e timeout.
> 
> thanks
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users

-- 
----
Eliezer Croitoru
Linux System Administrator
Mobile: +972-5-28704261
Email: eliezer at ngtech.co.il


From rousskov at measurement-factory.com  Thu Jun 21 05:18:50 2018
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Wed, 20 Jun 2018 23:18:50 -0600
Subject: [squid-users]
 =?utf-8?b?U3BlZWQg4oCL4oCLbGltaXQgZnJvbSBmaWxlIHNp?= =?utf-8?q?ze?=
In-Reply-To: <CAA+Mow5nyotHcmVSS2kair1ca9BzYgQmwEEPtJWuJ8WC99954Q@mail.gmail.com>
References: <CAA+Mow5nyotHcmVSS2kair1ca9BzYgQmwEEPtJWuJ8WC99954Q@mail.gmail.com>
Message-ID: <ddad83d9-e314-2878-62ec-06df058c037c@measurement-factory.com>

On 06/20/2018 01:44 PM, Michael Pro wrote:
> Is it possible to limit the speed of receiving the file to the client
> depending on the file size?

> Let's say,
> file =<1MB client download with 400Kbit/s
> file 1MB...5MB client donload with 4Mbit/s
> file >5MB - no speed limit


For HTTP responses with a Content-Length header, you can create poor
man's equivalent of a missing response_body_size ACL. For example:

  acl smallerThanTenBytes rep_header Content-Length ^[0-9]$

With an ACL like that, you can direct responses to the right pool. IIRC,
pool assignment may happen multiple times during the lifetime of a
transaction so you need to be careful to deal with assignments that are
done before there is a response. Delay pools are not my area of
expertise but others on the list can help you with the details.


For HTTP responses without a Content-Length header, you can use an
adaptation service (ICAP or eCAP) to tag transactions that start to
exceed 1MB and 5MB boundaries. A "note" ACL can check for that tag, but
I do not think Squid delay pools re-check pool ACLs after the response
body bytes start to flow. If I am right, then you would not be able to
(re)route response body bytes to the right delay pool for responses
without a Content-Length header.

You could use an adaptation service itself to slow responses without
Content-Length down (partially duplicating Squid's delay pools
capabilities), but the precision of such an approach may be too low due
to Squid internal buffering. YMMV.


For HTTP CONNECT tunnels (i.e., HTTPS traffic) even the adaptation
tricks would not work because adaptation services do not get CONNECT
tunnel bytes (yet?). You would have to bump the tunnel to get access to
the HTTP responses inside (and bumping opens another huge Pandora box).


HTH,

Alex.


From squid3 at treenet.co.nz  Thu Jun 21 05:44:36 2018
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 21 Jun 2018 17:44:36 +1200
Subject: [squid-users] =?utf-8?b?IFNwZWVkIOKAi+KAi2xpbWl0IGZyb20gZmlsZSBz?=
 =?utf-8?q?ize?=
In-Reply-To: <CAA+Mow5nyotHcmVSS2kair1ca9BzYgQmwEEPtJWuJ8WC99954Q@mail.gmail.com>
References: <CAA+Mow5nyotHcmVSS2kair1ca9BzYgQmwEEPtJWuJ8WC99954Q@mail.gmail.com>
Message-ID: <ba3982fc-0b50-3b81-1011-81ee5a1200ed@treenet.co.nz>

On 21/06/18 21:44,  Michael Pro wrote:
> Is it possible to limit the speed of receiving the file to the client
> depending on the file size?

NP: HTTP has no concept of "file". There are only messages. A message
may contain 0 or more *pieces* of those things we humans call "files".


> Let's say,
> file =<1MB client download with 400Kbit/s
> file 1MB...5MB client donload with 4Mbit/s
> file >5MB - no speed limit
> 
> I try fore some settings
> delay_pools 1
> delay_class 1 2
> delay_access 1 allow storeid_app_2
> delay_access 1 allow storeid_app_6
> delay_access 1 deny all
> delay_parameters 1 -1/-1 50000/1000000
> delay_parameters 1 -1/-1 500000/5000000
> delay_parameters 1 -1/-1 -1/-1

NP: each repetition of delay_parameters *replaces* previous settings for
that pool #1. The above spend lots of CPU calculating bandwidth for the
related requests, but does nothing.


> ...
> but it does't work
> 
> Can some method take place to solve this problem?


To receive the message at full speed into Squid, then dole it out slowly
to clients use the client_delay_* directives. The ones above only affect
read's from servers.
 <http://www.squid-cache.org/Doc/config/client_delay_parameters/>

Secondly, you need a different pool for each speed limit. The *_access
controls determine which transactions get which pool(s) speed limit
applied to them.


That said, there is almost no way to know how big an object is going to
be until it has finished arriving. What gets transmitted can be
compressed, split into ranges, chunked as a stream rather than a single
object or any combination of the above.

The best that can be done is checking Content-Length from the reply
message headers - *IF* the reply message even has one. Relevance and use
of that header is declining as dynamic content grows in popularity.


I recommend looking into OS level QoS controls and applying more general
bandwidth shaping instead. That way it does not matter what a client is
doing, if they are doing a lot of it they wont impact others on the
network quite as badly as uncontrolled traffic flow.

Amos


From Sarfaraz.Ahmad at deshaw.com  Thu Jun 21 09:11:44 2018
From: Sarfaraz.Ahmad at deshaw.com (Ahmad, Sarfaraz)
Date: Thu, 21 Jun 2018 09:11:44 +0000
Subject: [squid-users] Ignore SSL error and splice by ssl::server_name
 at the same time
References: <45be4c80c58842568fd50b1825755da5@mbxtoa3.winmail.deshaw.com>
 <0dd73a24-97d2-4836-6321-cbdb6508a6ab@treenet.co.nz> 
Message-ID: <9f435ee3b2674feea4c89f12a435069b@mbxtoa3.winmail.deshaw.com>

I was wrong. There is no way to read the remote certificate and then decide whether to bump/splice the connection. 

-----Original Message-----
From: Ahmad, Sarfaraz 
Sent: Wednesday, June 20, 2018 7:35 PM
To: 'Amos Jeffries' <squid3 at treenet.co.nz>; squid-users at lists.squid-cache.org
Subject: RE: [squid-users] Ignore SSL error and splice by ssl::server_name at the same time

Yes.  As always appreciate the quick support this community provides. :)
Thank you guys !

Regards,
Sarfaraz

-----Original Message-----
From: squid-users <squid-users-bounces at lists.squid-cache.org> On Behalf Of Amos Jeffries
Sent: Wednesday, June 20, 2018 6:53 PM
To: squid-users at lists.squid-cache.org
Subject: Re: [squid-users] Ignore SSL error and splice by ssl::server_name at the same time

On 21/06/18 00:25, Ahmad, Sarfaraz wrote:
> I found the answer to my problem. The SNI and Subject CN were 
> different in my case and I was not peeking at step2 (meaning not 
> looking at the server certificate) that is why my ACLs were ineffective.
> 

Ah, excellent. Does that mean your problem is now resolved?

Amos
_______________________________________________
squid-users mailing list
squid-users at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-users

From michael.adm at gmail.com  Thu Jun 21 11:11:34 2018
From: michael.adm at gmail.com (Michael Pro)
Date: Thu, 21 Jun 2018 14:11:34 +0300
Subject: [squid-users]
	=?utf-8?b?U3BlZWQg4oCL4oCLbGltaXQgZnJvbSBmaWxlIHNp?= =?utf-8?q?ze?=
In-Reply-To: <ddad83d9-e314-2878-62ec-06df058c037c@measurement-factory.com>
References: <CAA+Mow5nyotHcmVSS2kair1ca9BzYgQmwEEPtJWuJ8WC99954Q@mail.gmail.com>
 <ddad83d9-e314-2878-62ec-06df058c037c@measurement-factory.com>
Message-ID: <CAA+Mow4Z1=EkMLdmy=Z1_7EzPjmT_vDc+O9JzBd0bRX1kmEDDA@mail.gmail.com>

Simplify the task, let's say that I need to limit the speed only for
what is in the cache, for each client. Example
-----
client_side_reply.cc(2191) sendMoreData: local=127.0.0.1:20880
remote=10.0.0.18:32611 FD 99 flags=1
'http://app-2.squid.internal/file.exe' out.offset=3681475
-----
for this type of traffic is it possible made this logic ?
file =<1MB client download with 400Kbit/s
file 1MB...5MB client donload with 4Mbit/s
file >5MB - no speed limit


??, 21 ???. 2018 ?. ? 8:18, Alex Rousskov <rousskov at measurement-factory.com>:
>
> On 06/20/2018 01:44 PM, Michael Pro wrote:
> > Is it possible to limit the speed of receiving the file to the client
> > depending on the file size?
>
> > Let's say,
> > file =<1MB client download with 400Kbit/s
> > file 1MB...5MB client donload with 4Mbit/s
> > file >5MB - no speed limit
>
>
> For HTTP responses with a Content-Length header, you can create poor
> man's equivalent of a missing response_body_size ACL. For example:
>
>   acl smallerThanTenBytes rep_header Content-Length ^[0-9]$
>
> With an ACL like that, you can direct responses to the right pool. IIRC,
> pool assignment may happen multiple times during the lifetime of a
> transaction so you need to be careful to deal with assignments that are
> done before there is a response. Delay pools are not my area of
> expertise but others on the list can help you with the details.
>
>
> For HTTP responses without a Content-Length header, you can use an
> adaptation service (ICAP or eCAP) to tag transactions that start to
> exceed 1MB and 5MB boundaries. A "note" ACL can check for that tag, but
> I do not think Squid delay pools re-check pool ACLs after the response
> body bytes start to flow. If I am right, then you would not be able to
> (re)route response body bytes to the right delay pool for responses
> without a Content-Length header.
>
> You could use an adaptation service itself to slow responses without
> Content-Length down (partially duplicating Squid's delay pools
> capabilities), but the precision of such an approach may be too low due
> to Squid internal buffering. YMMV.
>
>
> For HTTP CONNECT tunnels (i.e., HTTPS traffic) even the adaptation
> tricks would not work because adaptation services do not get CONNECT
> tunnel bytes (yet?). You would have to bump the tunnel to get access to
> the HTTP responses inside (and bumping opens another huge Pandora box).
>
>
> HTH,
>
> Alex.


From tiraen at gmail.com  Thu Jun 21 11:14:26 2018
From: tiraen at gmail.com (Tiraen)
Date: Thu, 21 Jun 2018 14:14:26 +0300
Subject: [squid-users] Question about traffic calculate
In-Reply-To: <df89473e-4d15-0566-e800-b0431031517e@measurement-factory.com>
References: <CANhj9oyvYkuxk5pWTT=FPFggZ2kmcFa+Gu3yUXvNiURmouCxmg@mail.gmail.com>
 <fa5edf17-c0ed-4ac1-c5fe-b77839752dfc@treenet.co.nz>
 <CANhj9oywS173VafQhNa1S4_qznc-hmadoOiSRLH1HEPef_ORdQ@mail.gmail.com>
 <CANhj9ozSz6k0HMFYHUQLYw473j-NP8Vau=j7-akephH-AGO+ow@mail.gmail.com>
 <af7646cf-0fe6-6194-16d7-3fcb3f4622e1@treenet.co.nz>
 <df41334f-0abf-2b10-2ae1-047325461c09@integrafin.co.uk>
 <CANhj9ozzyfO8m12R5BTRRhq5K+6ojaNFzNSapV_jfPifnwFZOQ@mail.gmail.com>
 <CANhj9oxpuTwtTEM1MdXeM=j0CNda+rw3RzxCfg4UhWTqczmW9w@mail.gmail.com>
 <20180613095453.GA26516@fantomas.sk>
 <CANhj9oze9XznmdBSFAU_qct7v_HdFm45ar-caVhi=rys5epyog@mail.gmail.com>
 <20180613130911.GA2947@fantomas.sk>
 <df89473e-4d15-0566-e800-b0431031517e@measurement-factory.com>
Message-ID: <CANhj9ow_5j21bhmmudz4xvsqD=EZJNeLQ7MmZpvu9QesRyUXmw@mail.gmail.com>

and where i can read more about this (I mean the development of custom
ICAP/eCAP modules and their connection to the proxy) ?

2018-06-13 18:35 GMT+03:00 Alex Rousskov <rousskov at measurement-factory.com>:

> On 06/13/2018 07:09 AM, Matus UHLAR - fantomas wrote:
> > On 13.06.18 13:26, Tiraen wrote:
> >> ICAP will help provide data on incoming / outgoing traffic?
>
> > icap can get the data and work with it.
> > you don't have to manipulate, just do the accounting.
> > you just need ICAP module that will do it.
>
>
> Yes, it is possible to collect more-or-less accurate incoming request
> and incoming response stats using an ICAP service, but doing so would be
> very inefficient. Using eCAP would improve performance, but interpreting
> live access.log streams is probably the most efficient way of doing this.
>
> IIRC, both eCAP and ICAP interfaces do not see the exact incoming
> requests and incoming responses because Squid may strip hop-by-hop HTTP
> headers and decode chunked HTTP message bodies before forwarding the
> incoming message to the adaptation service. If you need exact headers
> and exact body sizes, then you need more than just the basic ICAP and
> eCAP interface. Again, access.log is probably an overall better choice
> for capturing that info.
>
> Both eCAP and ICAP interfaces do not see outgoing requests and outgoing
> responses because Squid only supports pre-cache vectoring points.
>
>
> HTH,
>
> Alex.
> P.S. In the above, "incoming" is "to Squid" and "outgoing" is "from Squid".
>
>
> >> 2018-06-13 12:54 GMT+03:00 Matus UHLAR - fantomas <uhlar at fantomas.sk>:
> >>
> >>> On 13.06.18 11:51, Tiraen wrote:
> >>>
> >>>> either such a question, perhaps someone in the course
> >>>>
> >>>> in the SQUID is still not implemented radius accounting?
> >>>>
> >>>
> >>> authentication - yes. But squid doese not support accounting (afaik).
> >>>
> >>> Maybe there are any third-party modules working correctly?
> >>>>
> >>>
> >>> maybe iCAP module.
>
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>



-- 
With best regards,

Vyacheslav Yakushev,

Unix system administrator

https://t.me/kelewind
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20180621/1e95614f/attachment.htm>

From rousskov at measurement-factory.com  Thu Jun 21 16:09:35 2018
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Thu, 21 Jun 2018 10:09:35 -0600
Subject: [squid-users]
 =?utf-8?b?U3BlZWQg4oCL4oCLbGltaXQgZnJvbSBmaWxlIHNp?= =?utf-8?q?ze?=
In-Reply-To: <CAA+Mow4Z1=EkMLdmy=Z1_7EzPjmT_vDc+O9JzBd0bRX1kmEDDA@mail.gmail.com>
References: <CAA+Mow5nyotHcmVSS2kair1ca9BzYgQmwEEPtJWuJ8WC99954Q@mail.gmail.com>
 <ddad83d9-e314-2878-62ec-06df058c037c@measurement-factory.com>
 <CAA+Mow4Z1=EkMLdmy=Z1_7EzPjmT_vDc+O9JzBd0bRX1kmEDDA@mail.gmail.com>
Message-ID: <af61d5b5-944b-f9f3-931a-337c36b177d6@measurement-factory.com>

On 06/21/2018 05:11 AM, Michael Pro wrote:
> Simplify the task, let's say that I need to limit the speed only for
> what is in the cache, for each client.
> for this type of traffic is it possible made this logic ?
> file =<1MB client download with 400Kbit/s
> file 1MB...5MB client donload with 4Mbit/s
> file >5MB - no speed limit

The Content-Length part of my earlier response still applies.

The adaptation parts of my earlier response do not apply to cache hits:
Squid does not support the post-cache RESPMOD vectoring point.

If your use case is limited to cache hits, then modifying Squid to add a
response size-like ACL would be even more tempting because fully cached
hits without a Content-Length header have known (to Squid) sizes. Please
note cache hits that are still in the process of being downloaded (and
cached) may lack the Content-Length header. In that case, those cache
hits will not have a known (to Squid) size.

You could try using client delay pools (see Amos response for details)
but please note that those pools aggregate based on client IP addresses.
If you have many clients with the same source IP address (NAT or a peer
proxy), then this aggregation may present problems.

You may prefer response delay pools, but they are only available in v5:
https://github.com/squid-cache/squid/commit/b27668e


Overall, you may be better off redesigning your bandwidth limiting plans
away from "file size" and towards something more directly related to
bandwidth consumption. The further you get from HTTP, the more difficult
mapping your requirements to HTTP tool capabilities becomes...


HTH,

Alex.


> ??, 21 ???. 2018 ?. ? 8:18, Alex Rousskov <rousskov at measurement-factory.com>:
>>
>> On 06/20/2018 01:44 PM, Michael Pro wrote:
>>> Is it possible to limit the speed of receiving the file to the client
>>> depending on the file size?
>>
>>> Let's say,
>>> file =<1MB client download with 400Kbit/s
>>> file 1MB...5MB client donload with 4Mbit/s
>>> file >5MB - no speed limit
>>
>>
>> For HTTP responses with a Content-Length header, you can create poor
>> man's equivalent of a missing response_body_size ACL. For example:
>>
>>   acl smallerThanTenBytes rep_header Content-Length ^[0-9]$
>>
>> With an ACL like that, you can direct responses to the right pool. IIRC,
>> pool assignment may happen multiple times during the lifetime of a
>> transaction so you need to be careful to deal with assignments that are
>> done before there is a response. Delay pools are not my area of
>> expertise but others on the list can help you with the details.
>>
>>
>> For HTTP responses without a Content-Length header, you can use an
>> adaptation service (ICAP or eCAP) to tag transactions that start to
>> exceed 1MB and 5MB boundaries. A "note" ACL can check for that tag, but
>> I do not think Squid delay pools re-check pool ACLs after the response
>> body bytes start to flow. If I am right, then you would not be able to
>> (re)route response body bytes to the right delay pool for responses
>> without a Content-Length header.
>>
>> You could use an adaptation service itself to slow responses without
>> Content-Length down (partially duplicating Squid's delay pools
>> capabilities), but the precision of such an approach may be too low due
>> to Squid internal buffering. YMMV.
>>
>>
>> For HTTP CONNECT tunnels (i.e., HTTPS traffic) even the adaptation
>> tricks would not work because adaptation services do not get CONNECT
>> tunnel bytes (yet?). You would have to bump the tunnel to get access to
>> the HTTP responses inside (and bumping opens another huge Pandora box).
>>
>>
>> HTH,
>>
>> Alex.
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
> 



From rousskov at measurement-factory.com  Thu Jun 21 16:20:31 2018
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Thu, 21 Jun 2018 10:20:31 -0600
Subject: [squid-users] Question about traffic calculate
In-Reply-To: <CANhj9ow_5j21bhmmudz4xvsqD=EZJNeLQ7MmZpvu9QesRyUXmw@mail.gmail.com>
References: <CANhj9oyvYkuxk5pWTT=FPFggZ2kmcFa+Gu3yUXvNiURmouCxmg@mail.gmail.com>
 <fa5edf17-c0ed-4ac1-c5fe-b77839752dfc@treenet.co.nz>
 <CANhj9oywS173VafQhNa1S4_qznc-hmadoOiSRLH1HEPef_ORdQ@mail.gmail.com>
 <CANhj9ozSz6k0HMFYHUQLYw473j-NP8Vau=j7-akephH-AGO+ow@mail.gmail.com>
 <af7646cf-0fe6-6194-16d7-3fcb3f4622e1@treenet.co.nz>
 <df41334f-0abf-2b10-2ae1-047325461c09@integrafin.co.uk>
 <CANhj9ozzyfO8m12R5BTRRhq5K+6ojaNFzNSapV_jfPifnwFZOQ@mail.gmail.com>
 <CANhj9oxpuTwtTEM1MdXeM=j0CNda+rw3RzxCfg4UhWTqczmW9w@mail.gmail.com>
 <20180613095453.GA26516@fantomas.sk>
 <CANhj9oze9XznmdBSFAU_qct7v_HdFm45ar-caVhi=rys5epyog@mail.gmail.com>
 <20180613130911.GA2947@fantomas.sk>
 <df89473e-4d15-0566-e800-b0431031517e@measurement-factory.com>
 <CANhj9ow_5j21bhmmudz4xvsqD=EZJNeLQ7MmZpvu9QesRyUXmw@mail.gmail.com>
Message-ID: <85ef0832-de5d-32bf-26ef-85b34ba8366b@measurement-factory.com>

On 06/21/2018 05:14 AM, Tiraen wrote:
> where i can read more about this (I mean the development of custom
> ICAP/eCAP modules and their connection to the proxy) ?

The best place to start is probably
https://wiki.squid-cache.org/SquidFaq/ContentAdaptation

If you decide to go the ICAP route, you will need to find the right ICAP
server for your project. After that, the development will revolve around
writing a custom adapter for that ICAP server. The above URL links to a
page with a list of ICAP servers:
https://wiki.squid-cache.org/Features/ICAP

If you decide to go the eCAP route, you will need to (find somebody to)
write an eCAP adapter (no server required).

In either case, the required development is similar to writing a plugin
or loadable module. Any capable developer can do it, but understanding
of HTTP concepts and familiarity with the ICAP server or eCAP API helps.


HTH,

Alex.


> 2018-06-13 18:35 GMT+03:00 Alex Rousskov:
> 
>     On 06/13/2018 07:09 AM, Matus UHLAR - fantomas wrote:
>     > On 13.06.18 13:26, Tiraen wrote:
>     >> ICAP will help provide data on incoming / outgoing traffic?
> 
>     > icap can get the data and work with it.
>     > you don't have to manipulate, just do the accounting.
>     > you just need ICAP module that will do it.
> 
> 
>     Yes, it is possible to collect more-or-less accurate incoming request
>     and incoming response stats using an ICAP service, but doing so would be
>     very inefficient. Using eCAP would improve performance, but interpreting
>     live access.log streams is probably the most efficient way of doing
>     this.
> 
>     IIRC, both eCAP and ICAP interfaces do not see the exact incoming
>     requests and incoming responses because Squid may strip hop-by-hop HTTP
>     headers and decode chunked HTTP message bodies before forwarding the
>     incoming message to the adaptation service. If you need exact headers
>     and exact body sizes, then you need more than just the basic ICAP and
>     eCAP interface. Again, access.log is probably an overall better choice
>     for capturing that info.
> 
>     Both eCAP and ICAP interfaces do not see outgoing requests and outgoing
>     responses because Squid only supports pre-cache vectoring points.
> 
> 
>     HTH,
> 
>     Alex.
>     P.S. In the above, "incoming" is "to Squid" and "outgoing" is "from
>     Squid".
> 
> 
>     >> 2018-06-13 12:54 GMT+03:00 Matus UHLAR - fantomas <uhlar at fantomas.sk <mailto:uhlar at fantomas.sk>>:
>     >>
>     >>> On 13.06.18 11:51, Tiraen wrote:
>     >>>
>     >>>> either such a question, perhaps someone in the course
>     >>>>
>     >>>> in the SQUID is still not implemented radius accounting?
>     >>>>
>     >>>
>     >>> authentication - yes. But squid doese not support accounting (afaik).
>     >>>
>     >>> Maybe there are any third-party modules working correctly?
>     >>>>
>     >>>
>     >>> maybe iCAP module.
> 
> 
>     _______________________________________________
>     squid-users mailing list
>     squid-users at lists.squid-cache.org
>     <mailto:squid-users at lists.squid-cache.org>
>     http://lists.squid-cache.org/listinfo/squid-users
>     <http://lists.squid-cache.org/listinfo/squid-users>
> 
> 
> 
> 
> -- 
> With best regards,
> 
> Vyacheslav Yakushev,
> 
> Unix system administrator
> 
> https://t.me/kelewind



From michael.adm at gmail.com  Thu Jun 21 18:08:31 2018
From: michael.adm at gmail.com (Michael Pro)
Date: Thu, 21 Jun 2018 21:08:31 +0300
Subject: [squid-users]
	=?utf-8?b?U3BlZWQg4oCL4oCLbGltaXQgZnJvbSBmaWxlIHNp?= =?utf-8?q?ze?=
In-Reply-To: <af61d5b5-944b-f9f3-931a-337c36b177d6@measurement-factory.com>
References: <CAA+Mow5nyotHcmVSS2kair1ca9BzYgQmwEEPtJWuJ8WC99954Q@mail.gmail.com>
 <ddad83d9-e314-2878-62ec-06df058c037c@measurement-factory.com>
 <CAA+Mow4Z1=EkMLdmy=Z1_7EzPjmT_vDc+O9JzBd0bRX1kmEDDA@mail.gmail.com>
 <af61d5b5-944b-f9f3-931a-337c36b177d6@measurement-factory.com>
Message-ID: <CAA+Mow7EctZp1e6wUEyOgnB2g78O9dFhv9Aob-BjNYu23eqk+Q@mail.gmail.com>

??, 21 ???. 2018 ?. ? 19:09, Alex Rousskov <rousskov at measurement-factory.com>:
> ...
> You may prefer response delay pools, but they are only available in v5:
> https://github.com/squid-cache/squid/commit/b27668e

Very interesting. Maybe this is what I need. Especially since I use v5 squid.
But to use this I understood that I need to use the store_id_program mechanism.
A related question is how to pass the size (Content-Length) to store_id_extras.
I tried it and so %{Content-Length}>h and so
%{Content-Length}<st  %{Content-Length}>st  %st  %>st  %<st
, but nothing is transmitted.


From michael.adm at gmail.com  Thu Jun 21 18:49:41 2018
From: michael.adm at gmail.com (Michael Pro)
Date: Thu, 21 Jun 2018 21:49:41 +0300
Subject: [squid-users]
	=?utf-8?b?U3BlZWQg4oCL4oCLbGltaXQgZnJvbSBmaWxlIHNp?= =?utf-8?q?ze?=
In-Reply-To: <CAA+Mow7EctZp1e6wUEyOgnB2g78O9dFhv9Aob-BjNYu23eqk+Q@mail.gmail.com>
References: <CAA+Mow5nyotHcmVSS2kair1ca9BzYgQmwEEPtJWuJ8WC99954Q@mail.gmail.com>
 <ddad83d9-e314-2878-62ec-06df058c037c@measurement-factory.com>
 <CAA+Mow4Z1=EkMLdmy=Z1_7EzPjmT_vDc+O9JzBd0bRX1kmEDDA@mail.gmail.com>
 <af61d5b5-944b-f9f3-931a-337c36b177d6@measurement-factory.com>
 <CAA+Mow7EctZp1e6wUEyOgnB2g78O9dFhv9Aob-BjNYu23eqk+Q@mail.gmail.com>
Message-ID: <CAA+Mow6+FE6AJPvpQXq6RG9pm6atLZw3ygMOeJvk9p15cTyZkA@mail.gmail.com>

In addition, I can say that I successfully use the store_id mechanism
and bookmark
the new links that interest me in mySQL database.
My working version options
store_id_extras "%>a/%>A %un %>rm myip=%la myport=%lp %{User-Agent}>h
%{Referer}>h"

??, 21 ???. 2018 ?. ? 21:08, Michael Pro <michael.adm at gmail.com>:
>
> ??, 21 ???. 2018 ?. ? 19:09, Alex Rousskov <rousskov at measurement-factory.com>:
> > ...
> > You may prefer response delay pools, but they are only available in v5:
> > https://github.com/squid-cache/squid/commit/b27668e
>
> Very interesting. Maybe this is what I need. Especially since I use v5 squid.
> But to use this I understood that I need to use the store_id_program mechanism.
> A related question is how to pass the size (Content-Length) to store_id_extras.
> I tried it and so %{Content-Length}>h and so
> %{Content-Length}<st  %{Content-Length}>st  %st  %>st  %<st
> , but nothing is transmitted.


From rousskov at measurement-factory.com  Thu Jun 21 19:52:29 2018
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Thu, 21 Jun 2018 13:52:29 -0600
Subject: [squid-users]
 =?utf-8?b?U3BlZWQg4oCL4oCLbGltaXQgZnJvbSBmaWxlIHNp?= =?utf-8?q?ze?=
In-Reply-To: <CAA+Mow7EctZp1e6wUEyOgnB2g78O9dFhv9Aob-BjNYu23eqk+Q@mail.gmail.com>
References: <CAA+Mow5nyotHcmVSS2kair1ca9BzYgQmwEEPtJWuJ8WC99954Q@mail.gmail.com>
 <ddad83d9-e314-2878-62ec-06df058c037c@measurement-factory.com>
 <CAA+Mow4Z1=EkMLdmy=Z1_7EzPjmT_vDc+O9JzBd0bRX1kmEDDA@mail.gmail.com>
 <af61d5b5-944b-f9f3-931a-337c36b177d6@measurement-factory.com>
 <CAA+Mow7EctZp1e6wUEyOgnB2g78O9dFhv9Aob-BjNYu23eqk+Q@mail.gmail.com>
Message-ID: <f2f75c9e-192d-aa63-c892-8031e40c7741@measurement-factory.com>

On 06/21/2018 12:08 PM, Michael Pro wrote:
> ??, 21 ???. 2018 ?. ? 19:09, Alex Rousskov <rousskov at measurement-factory.com>:
>> ...
>> You may prefer response delay pools, but they are only available in v5:
>> https://github.com/squid-cache/squid/commit/b27668e

> But to use this I understood that I need to use the store_id_program mechanism.

There is no relationship between response delay pools and Store IDs.


> A related question is how to pass the size (Content-Length) to store_id_extras.

You cannot pass response Content-Length header to the store_id_program
because that helper is consulted _before_ there is a response (step #2):

  1. parse client request X

  2. adapt X into request Y (ICAP, eCAP, url_rewriter, store_id, etc.)

  3. get the resource requested by Y
     (from the cache, peer, or origin server)


HTH,

Alex.


From michael.adm at gmail.com  Fri Jun 22 04:34:55 2018
From: michael.adm at gmail.com (Michael Pro)
Date: Fri, 22 Jun 2018 07:34:55 +0300
Subject: [squid-users]
	=?utf-8?b?U3BlZWQg4oCL4oCLbGltaXQgZnJvbSBmaWxlIHNp?= =?utf-8?q?ze?=
In-Reply-To: <f2f75c9e-192d-aa63-c892-8031e40c7741@measurement-factory.com>
References: <CAA+Mow5nyotHcmVSS2kair1ca9BzYgQmwEEPtJWuJ8WC99954Q@mail.gmail.com>
 <ddad83d9-e314-2878-62ec-06df058c037c@measurement-factory.com>
 <CAA+Mow4Z1=EkMLdmy=Z1_7EzPjmT_vDc+O9JzBd0bRX1kmEDDA@mail.gmail.com>
 <af61d5b5-944b-f9f3-931a-337c36b177d6@measurement-factory.com>
 <CAA+Mow7EctZp1e6wUEyOgnB2g78O9dFhv9Aob-BjNYu23eqk+Q@mail.gmail.com>
 <f2f75c9e-192d-aa63-c892-8031e40c7741@measurement-factory.com>
Message-ID: <CAA+Mow6LxO0t_KcmQu_dOMZf79uGC1NJ3+Bji+H9QXBhmLT21g@mail.gmail.com>

Good.
As I said, I put all the necessary links in the database in step #2.
I'm quite satisfied with entering this value (Content-Length) after step #3.
How can I update the data for the link?
??, 21 ???. 2018 ?. ? 22:52, Alex Rousskov <rousskov at measurement-factory.com>:
>
> On 06/21/2018 12:08 PM, Michael Pro wrote:
> > ??, 21 ???. 2018 ?. ? 19:09, Alex Rousskov <rousskov at measurement-factory.com>:
> >> ...
> >> You may prefer response delay pools, but they are only available in v5:
> >> https://github.com/squid-cache/squid/commit/b27668e
>
> > But to use this I understood that I need to use the store_id_program mechanism.
>
> There is no relationship between response delay pools and Store IDs.
>
>
> > A related question is how to pass the size (Content-Length) to store_id_extras.
>
> You cannot pass response Content-Length header to the store_id_program
> because that helper is consulted _before_ there is a response (step #2):
>
>   1. parse client request X
>
>   2. adapt X into request Y (ICAP, eCAP, url_rewriter, store_id, etc.)
>
>   3. get the resource requested by Y
>      (from the cache, peer, or origin server)
>
>
> HTH,
>
> Alex.


From christof.gerber1 at gmail.com  Fri Jun 22 09:14:23 2018
From: christof.gerber1 at gmail.com (chgerber)
Date: Fri, 22 Jun 2018 02:14:23 -0700 (MST)
Subject: [squid-users] Display eCAP meta-information on Squid error-page
In-Reply-To: <d010be16-b35d-8041-1854-a450a0f75114@measurement-factory.com>
References: <CAFyThp+u6Dfvs4mR55+rgWy8PqiXV7dzzMQ9-jF7kqx0HwYQBg@mail.gmail.com>
 <7a5581a4-a5bc-aa28-b026-eb99896622bd@measurement-factory.com>
 <1528190840028-0.post@n4.nabble.com>
 <47e787d7-280c-356a-9e88-e6d9d6f6724d@measurement-factory.com>
 <1528213710727-0.post@n4.nabble.com>
 <d010be16-b35d-8041-1854-a450a0f75114@measurement-factory.com>
Message-ID: <1529658863486-0.post@n4.nabble.com>

>> If it does not work, then most likely there is a bug in your adapter, in 
>> your Squid configuration, or in Squid. 

By now I see no other way than this being a misunderstanding between us or a
bug in Squid 3.5.

##########
Squid 3.5 XactionRep.cc line 492:

// Store received meta headers for adapt::<last_h logformat code use.
Adaptation::History::Pointer ah = request->adaptLogHistory();
    if (ah != NULL) {
        HttpHeader meta(hoReply);
        OptionsExtractor extractor(meta);
        theMaster->visitEachOption(extractor);
        ah->recordMeta(&meta);
    }
###########

This is where Squid demands the ecap meta headers. This succeeds as I follow
fromt the fact that I can log them with the adapt::<last_h logformat. The
comment also indicates the usage of these ecap meta headers with
adapt::<last_h logformat. But with %{my-ecap-header}note it does not work.
Even when I add %note which should log all notes according to the logformat
configuration docs the result is the same ("-"). I tested this in both
Respmode and Reqmode.  Why are you so sure that %note should work with ecap
meta headers?

Is it due to the changelog of Squid-3.4:
"New format code %note to log a transaction annotation linked to the
transaction by ICAP, eCAP, a helper, or the note squid.conf directive."

To elaborate again why this is important to me: I want to apply ACL's on
information provided by the eCAP adapter. As ACL's can be applied to notes
and because you mentioned that eCAP can provide ecap meta headers as notes I
found this as the way to go. As a starting point I first wanted to verify
that ecap meta headers really can be provided to Squid in the form of notes
so I tried to log them with %{my-ecap-header}note and %note which failed.

squid.conf logformats tried:
logformat test %{my-ecap-header}note                     // resulting
output: "-"
logformat test %note                                                //
resulting output: "-"
logformat test %{my-ecap-header}adapt::<last_h     //  resulting output:
"value-of-my-ecap-header"
         



--
Sent from: http://squid-web-proxy-cache.1019090.n4.nabble.com/Squid-Users-f1019091.html


From rousskov at measurement-factory.com  Fri Jun 22 15:22:49 2018
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Fri, 22 Jun 2018 09:22:49 -0600
Subject: [squid-users]
 =?utf-8?b?U3BlZWQg4oCL4oCLbGltaXQgZnJvbSBmaWxlIHNp?= =?utf-8?q?ze?=
In-Reply-To: <CAA+Mow6LxO0t_KcmQu_dOMZf79uGC1NJ3+Bji+H9QXBhmLT21g@mail.gmail.com>
References: <CAA+Mow5nyotHcmVSS2kair1ca9BzYgQmwEEPtJWuJ8WC99954Q@mail.gmail.com>
 <ddad83d9-e314-2878-62ec-06df058c037c@measurement-factory.com>
 <CAA+Mow4Z1=EkMLdmy=Z1_7EzPjmT_vDc+O9JzBd0bRX1kmEDDA@mail.gmail.com>
 <af61d5b5-944b-f9f3-931a-337c36b177d6@measurement-factory.com>
 <CAA+Mow7EctZp1e6wUEyOgnB2g78O9dFhv9Aob-BjNYu23eqk+Q@mail.gmail.com>
 <f2f75c9e-192d-aa63-c892-8031e40c7741@measurement-factory.com>
 <CAA+Mow6LxO0t_KcmQu_dOMZf79uGC1NJ3+Bji+H9QXBhmLT21g@mail.gmail.com>
Message-ID: <624f6f34-aa88-848c-51e8-97020700f489@measurement-factory.com>

On 06/21/2018 10:34 PM, Michael Pro wrote:

> I put all the necessary links in the database in step #2.
> I'm quite satisfied with entering this value (Content-Length) after step #3.
> How can I update the data for the link?

I am not sure I understand the question, but if you are asking how to
add response Content-Length info to some external database, then you
have a few options, including:

1. An external ACL used with http_reply_access
   (gets notified after receiving response headers)
   http://www.squid-cache.org/Doc/config/http_reply_access/

2. An access.log daemon (gets notified after response delivery)

3. An access.log parser (gets notified after response delivery
   and log flushing)

Alex.


> ??, 21 ???. 2018 ?. ? 22:52, Alex Rousskov <rousskov at measurement-factory.com>:
>>
>> On 06/21/2018 12:08 PM, Michael Pro wrote:
>>> ??, 21 ???. 2018 ?. ? 19:09, Alex Rousskov <rousskov at measurement-factory.com>:
>>>> ...
>>>> You may prefer response delay pools, but they are only available in v5:
>>>> https://github.com/squid-cache/squid/commit/b27668e
>>
>>> But to use this I understood that I need to use the store_id_program mechanism.
>>
>> There is no relationship between response delay pools and Store IDs.
>>
>>
>>> A related question is how to pass the size (Content-Length) to store_id_extras.
>>
>> You cannot pass response Content-Length header to the store_id_program
>> because that helper is consulted _before_ there is a response (step #2):
>>
>>   1. parse client request X
>>
>>   2. adapt X into request Y (ICAP, eCAP, url_rewriter, store_id, etc.)
>>
>>   3. get the resource requested by Y
>>      (from the cache, peer, or origin server)
>>
>>
>> HTH,
>>
>> Alex.
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
> 



From rousskov at measurement-factory.com  Fri Jun 22 15:45:51 2018
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Fri, 22 Jun 2018 09:45:51 -0600
Subject: [squid-users] Display eCAP meta-information on Squid error-page
In-Reply-To: <1529658863486-0.post@n4.nabble.com>
References: <CAFyThp+u6Dfvs4mR55+rgWy8PqiXV7dzzMQ9-jF7kqx0HwYQBg@mail.gmail.com>
 <7a5581a4-a5bc-aa28-b026-eb99896622bd@measurement-factory.com>
 <1528190840028-0.post@n4.nabble.com>
 <47e787d7-280c-356a-9e88-e6d9d6f6724d@measurement-factory.com>
 <1528213710727-0.post@n4.nabble.com>
 <d010be16-b35d-8041-1854-a450a0f75114@measurement-factory.com>
 <1529658863486-0.post@n4.nabble.com>
Message-ID: <aaf8eae0-0b76-5096-72fd-48fa662a21ae@measurement-factory.com>

On 06/22/2018 03:14 AM, chgerber wrote:
>>> If it does not work, then most likely there is a bug in your adapter, in 
>>> your Squid configuration, or in Squid. 

> By now I see no other way than this being a misunderstanding between us or a
> bug in Squid 3.5.

> Why are you so sure that %note should work with ecap meta headers?

I have not carefully investigated or tested this, but I believe it
should work (bugs notwithstanding) because I recall projects that
added/used that feature and because documentation/code seem to support
my recollection.

If you think you found a Squid bug, I recommend reporting it. Posting
minimal adapter code that illustrates the bug would increase your
chances of somebody volunteering to triage your bug report.


HTH,

Alex.


From masih.enter at gmail.com  Fri Jun 22 20:48:38 2018
From: masih.enter at gmail.com (Masih Nazari)
Date: Sat, 23 Jun 2018 01:18:38 +0430
Subject: [squid-users] max_user_ip not work
Message-ID: <CAGL1ZiPUe+YjS6=R_C6bKk4q2QQA6GrK6-bUNDLSMMBbE+Q98g@mail.gmail.com>

 hello
why max_user_ip not work ?
when i change my ip for test still able to access squid
squid : 3.5.20
os : centos 7  64 bit
squid installed by yum
i use radius server and its work fine
this is my config


cache deny all
acl LocalOpenvpnPort port 80 # openvpn
acl LocalOpenvpn dst 127.0.0.1 # openvpn
http_access allow LocalOpenvpn LocalOpenvpnPort # openvpn
auth_param basic program /usr/lib64/squid/basic_radius_auth -f
/etc/squid/radius_config
auth_param basic children 5
auth_param basic realm Web-Proxy
auth_param basic credentialsttl 5 minute
auth_param basic casesensitive off
authenticate_cache_garbage_interval 5 minute
authenticate_ttl 5 minute
authenticate_ip_ttl 1 minute
acl radius_auth proxy_auth REQUIRED
acl maxuser max_user_ip -s 1
deny_info ERR_MAX_IP maxuser
http_access deny maxuser
http_access allow radius_auth
http_access deny all
http_port 7080
debug_options "ALL,9"




this is my squid info :


Squid Cache: Version 3.5.20
Service Name: squid
configure options:  '--build=x86_64-redhat-linux-gnu'
'--host=x86_64-redhat-linux-gnu' '--program-prefix=' '--prefix=/usr'
'--exec-prefix=/usr' '--bindir=/usr/bin' '--sbindir=/usr/sbin'
'--sysconfdir=/etc' '--datadir=/usr/share' '--includedir=/usr/include'
'--libdir=/usr/lib64' '--libexecdir=/usr/libexec'
'--sharedstatedir=/var/lib' '--mandir=/usr/share/man'
'--infodir=/usr/share/info' '--disable-strict-error-checking'
'--exec_prefix=/usr' '--libexecdir=/usr/lib64/squid' '--localstatedir=/var'
'--datadir=/usr/share/squid' '--sysconfdir=/etc/squid'
'--with-logdir=$(localstatedir)/log/squid'
'--with-pidfile=$(localstatedir)/run/squid.pid'
'--disable-dependency-tracking' '--enable-eui'
'--enable-follow-x-forwarded-for' '--enable-auth'
'--enable-auth-basic=DB,LDAP,MSNT-multi-domain,NCSA,NIS,PAM,POP3,RADIUS,SASL,SMB,SMB_LM,getpwnam'
'--enable-auth-ntlm=smb_lm,fake'
'--enable-auth-digest=file,LDAP,eDirectory'
'--enable-auth-negotiate=kerberos'
'--enable-external-acl-helpers=file_userip,LDAP_group,time_quota,session,unix_group,wbinfo_group,kerberos_ldap_group'
'--enable-cache-digests' '--enable-cachemgr-hostname=localhost'
'--enable-delay-pools' '--enable-epoll' '--enable-ident-lookups'
'--enable-linux-netfilter' '--enable-removal-policies=heap,lru'
'--enable-snmp' '--enable-ssl-crtd' '--enable-storeio=aufs,diskd,rock,ufs'
'--enable-wccpv2' '--enable-esi' '--enable-ecap' '--with-aio'
'--with-default-user=squid' '--with-dl' '--with-openssl' '--with-pthreads'
'--disable-arch-native' 'build_alias=x86_64-redhat-linux-gnu'
'host_alias=x86_64-redhat-linux-gnu' 'CFLAGS=-O2 -g -pipe -Wall
-Wp,-D_FORTIFY_SOURCE=2 -fexceptions -fstack-protector-strong
--param=ssp-buffer-size=4 -grecord-gcc-switches   -m64 -mtune=generic
-fpie' 'LDFLAGS=-Wl,-z,relro  -pie -Wl,-z,relro -Wl,-z,now' 'CXXFLAGS=-O2
-g -pipe -Wall -Wp,-D_FORTIFY_SOURCE=2 -fexceptions
-fstack-protector-strong --param=ssp-buffer-size=4 -grecord-gcc-switches
 -m64 -mtune=generic -fpie'
'PKG_CONFIG_PATH=:/usr/lib64/pkgconfig:/usr/share/pkgconfig'
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20180623/0bc9ec48/attachment.htm>

From frio_cervesa at hotmail.com  Sat Jun 23 22:38:07 2018
From: frio_cervesa at hotmail.com (senor)
Date: Sat, 23 Jun 2018 22:38:07 +0000
Subject: [squid-users] TUNNEL logging
Message-ID: <BY2PR17MB0182F4E9560286CC420930E1F7740@BY2PR17MB0182.namprd17.prod.outlook.com>

Hi all,

I've noticed that a tunneled 443 request is not logged to access.log 
until the client or server terminate which can be a long time. Is it 
possible to get squid to log the CONNECT at tunnel initiation? I realize 
there will be some data not available like total bytes (which I would 
like to have but need timely logging more). I tried a logformat that 
excluded any data not known at initiation time with no luck. I'm hoping 
I just missed something.

Thanks for any help.

Senor


From rousskov at measurement-factory.com  Sun Jun 24 00:09:50 2018
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Sat, 23 Jun 2018 18:09:50 -0600
Subject: [squid-users] TUNNEL logging
In-Reply-To: <BY2PR17MB0182F4E9560286CC420930E1F7740@BY2PR17MB0182.namprd17.prod.outlook.com>
References: <BY2PR17MB0182F4E9560286CC420930E1F7740@BY2PR17MB0182.namprd17.prod.outlook.com>
Message-ID: <71e4ce89-38cd-1722-cef3-1e9e06f6dd7d@measurement-factory.com>

On 06/23/2018 04:38 PM, senor wrote:
> Hi all,
> 
> I've noticed that a tunneled 443 request is not logged to access.log 
> until the client or server terminate which can be a long time. 

Yes, CONNECT tunnels are logged when the tunnel is over (i.e., Squid is
done talking to the client and server). This log-at-the-end approach is
similar to other transactions (which may also take a very long time).


> Is it  possible to get squid to log the CONNECT at tunnel initiation?

It is possible to be notified about CONNECT requests via eCAP and ICAP
interfaces as well as via external ACL helpers.

It is not possible to log the CONNECT request/response before the tunnel
is over. One could, in principle, separate CONNECT request/response
messages from the established tunnel, and log each "phase" of the tunnel
transaction separately, but I am not sure that is a good idea -- it is
not clear to me why a CONNECT tunnel should be treated differently from
any other HTTP transaction where the both client and server may send
request and response body bytes concurrently (and for a long time).


What problem are you trying to solve?


Alex.


From squid3 at treenet.co.nz  Sun Jun 24 05:39:40 2018
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sun, 24 Jun 2018 17:39:40 +1200
Subject: [squid-users] max_user_ip not work
In-Reply-To: <CAGL1ZiPUe+YjS6=R_C6bKk4q2QQA6GrK6-bUNDLSMMBbE+Q98g@mail.gmail.com>
References: <CAGL1ZiPUe+YjS6=R_C6bKk4q2QQA6GrK6-bUNDLSMMBbE+Q98g@mail.gmail.com>
Message-ID: <d6bf890b-5859-3288-2726-c533b2b146d8@treenet.co.nz>

On 23/06/18 08:48, Masih Nazari wrote:
> hello?
> why max_user_ip not work ?
> when i change my ip for test still able to access squid
> squid : 3.5.20
> os : centos 7? 64 bit
> squid installed by yum?
> i use radius server and its work fine
> this is my config 


Could be several reasons;
 how are you sending test requests?
 are they http:// or https:// URLs?
 and what tool(s) are you using to fetch them?


> 
> 
> cache deny all
> acl LocalOpenvpnPort port 80# openvpn
> acl LocalOpenvpn dst 127.0.0.1# openvpn
> http_access allow LocalOpenvpn LocalOpenvpnPort # openvpn

You should know this, but stating it for completeness anyway:

Any traffic allowed by the above lines does not get authenticated at
all. So it cannot have this ACL applied. If your tests URLs resolve to
127.0.0.1:80 then they will always "fail" tests about user related
things due to this "allow".


> auth_param basic program /usr/lib64/squid/basic_radius_auth -f
> /etc/squid/radius_config
> auth_param basic children 5
> auth_param basic realm Web-Proxy
> auth_param basic credentialsttl 5 minute
> auth_param basic casesensitive off
> authenticate_cache_garbage_interval 5 minute
> authenticate_ttl 5 minute
> authenticate_ip_ttl 1 minute

>From the documentation for this directive:
"
 Use a small value (e.g., 60 seconds)
 if your users might change addresses
 quickly, as is the case with dialup.
"

The above line configures Squid to *ignore* (and discard/replace) IP
address info about a user login if it was added more than 1 minute earlier.

If you want IP to be fixed for long periods, *definitely* set that to a
longer time. I suggest much longer than the discard timer on the
credentials themselves (currently 5min).

(I don't recall right now if it updates/restarts that TTL timer on IP
info for every request related to it. You test result suggests that it
probably does not).



> acl radius_auth proxy_auth REQUIRED
> acl maxuser max_user_ip -s 1> deny_info ERR_MAX_IP maxuser
> http_access deny maxuser
> http_access allow radius_auth
> http_access deny all
> http_port 7080
> debug_options "ALL,9"
> 
> 
> 
> 
> this is my squid info :
> 
> 
> Squid Cache: Version 3.5.20
> Service Name: squid
...
> '--enable-ssl-crtd' '--with-openssl'
Not related to your question, but important:

This is using OpenSSL support with an outdated Squid version (more than
1 year). The TLS environment and code in Squid is *very* volatile, and
the code in versions older than 3.5.24 has known security
vulnerabilities. Please upgrade. The current 3.5 release is 3.5.27.

(You may also want to check your OpenSSL library version is up to date
first.)

Amos


From capcoding at gmail.com  Sun Jun 24 17:15:05 2018
From: capcoding at gmail.com (Gordon Hsiao)
Date: Sun, 24 Jun 2018 12:15:05 -0500
Subject: [squid-users] squid callout sequence
Message-ID: <CAK0iFYzY+5h702ptw6epWmfmxxxsXE3fztp2_X170nc9icBLxQ@mail.gmail.com>

at https://wiki.squid-cache.org/SquidFaq/OrderIsImportant I noticed
redirectors are way ahead of ssl-bump in the callout order, in a
https-ssl-bump case you will need ssl-bump to run (so you can get full URL
for example), then you can run redirector based on the result of ssl-bump,
correct? why is redirector run before ssl-bump?

Thanks,
Gordon
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20180624/59aa1b49/attachment.htm>

From michael.adm at gmail.com  Sun Jun 24 17:32:54 2018
From: michael.adm at gmail.com (Michael Pro)
Date: Sun, 24 Jun 2018 20:32:54 +0300
Subject: [squid-users]
	=?utf-8?b?U3BlZWQg4oCL4oCLbGltaXQgZnJvbSBmaWxlIHNp?= =?utf-8?q?ze?=
In-Reply-To: <624f6f34-aa88-848c-51e8-97020700f489@measurement-factory.com>
References: <CAA+Mow5nyotHcmVSS2kair1ca9BzYgQmwEEPtJWuJ8WC99954Q@mail.gmail.com>
 <ddad83d9-e314-2878-62ec-06df058c037c@measurement-factory.com>
 <CAA+Mow4Z1=EkMLdmy=Z1_7EzPjmT_vDc+O9JzBd0bRX1kmEDDA@mail.gmail.com>
 <af61d5b5-944b-f9f3-931a-337c36b177d6@measurement-factory.com>
 <CAA+Mow7EctZp1e6wUEyOgnB2g78O9dFhv9Aob-BjNYu23eqk+Q@mail.gmail.com>
 <f2f75c9e-192d-aa63-c892-8031e40c7741@measurement-factory.com>
 <CAA+Mow6LxO0t_KcmQu_dOMZf79uGC1NJ3+Bji+H9QXBhmLT21g@mail.gmail.com>
 <624f6f34-aa88-848c-51e8-97020700f489@measurement-factory.com>
Message-ID: <CAA+Mow71MQDo4rW6GOs7HcJbsZ=oZ32GOp2xFQuE0N0=EwivRg@mail.gmail.com>

Yes! I realized it!
In store_id_program, I use the third-party utility "curl -sI
http://url.to/real.file.zip"
to define Content-Length and return to squid using the "note" mechanism
in the desired ACL for response_delay_pool_access. Similarly,
using the "note" mechanism, I force the files, depending on the extension
and content in the cache, to use a specific tcp_outgoing_address
or the specific FIB.

Could not you do this with the help of squid itself?
PS: The average user of excellent knowledge of programming languages
will not do this, even if they guess how to do it.
??, 22 ???. 2018 ?. ? 18:22, Alex Rousskov <rousskov at measurement-factory.com>:
>
> On 06/21/2018 10:34 PM, Michael Pro wrote:
>
> > I put all the necessary links in the database in step #2.
> > I'm quite satisfied with entering this value (Content-Length) after step #3.
> > How can I update the data for the link?
>
> I am not sure I understand the question, but if you are asking how to
> add response Content-Length info to some external database, then you
> have a few options, including:
>
> 1. An external ACL used with http_reply_access
>    (gets notified after receiving response headers)
>    http://www.squid-cache.org/Doc/config/http_reply_access/
>
> 2. An access.log daemon (gets notified after response delivery)
>
> 3. An access.log parser (gets notified after response delivery
>    and log flushing)
>
> Alex.
>
>
> > ??, 21 ???. 2018 ?. ? 22:52, Alex Rousskov <rousskov at measurement-factory.com>:
> >>
> >> On 06/21/2018 12:08 PM, Michael Pro wrote:
> >>> ??, 21 ???. 2018 ?. ? 19:09, Alex Rousskov <rousskov at measurement-factory.com>:
> >>>> ...
> >>>> You may prefer response delay pools, but they are only available in v5:
> >>>> https://github.com/squid-cache/squid/commit/b27668e
> >>
> >>> But to use this I understood that I need to use the store_id_program mechanism.
> >>
> >> There is no relationship between response delay pools and Store IDs.
> >>
> >>
> >>> A related question is how to pass the size (Content-Length) to store_id_extras.
> >>
> >> You cannot pass response Content-Length header to the store_id_program
> >> because that helper is consulted _before_ there is a response (step #2):
> >>
> >>   1. parse client request X
> >>
> >>   2. adapt X into request Y (ICAP, eCAP, url_rewriter, store_id, etc.)
> >>
> >>   3. get the resource requested by Y
> >>      (from the cache, peer, or origin server)
> >>
> >>
> >> HTH,
> >>
> >> Alex.
> > _______________________________________________
> > squid-users mailing list
> > squid-users at lists.squid-cache.org
> > http://lists.squid-cache.org/listinfo/squid-users
> >
>


From frio_cervesa at hotmail.com  Sun Jun 24 23:46:08 2018
From: frio_cervesa at hotmail.com (senor)
Date: Sun, 24 Jun 2018 23:46:08 +0000
Subject: [squid-users] TUNNEL logging
In-Reply-To: <71e4ce89-38cd-1722-cef3-1e9e06f6dd7d@measurement-factory.com>
References: <BY2PR17MB0182F4E9560286CC420930E1F7740@BY2PR17MB0182.namprd17.prod.outlook.com>
 <71e4ce89-38cd-1722-cef3-1e9e06f6dd7d@measurement-factory.com>
Message-ID: <BY2PR17MB0182AA56941A848386A571E5F74B0@BY2PR17MB0182.namprd17.prod.outlook.com>

Thanks. That's the answer I expected.

The problem is only customer expectation. When there is a mix of bumped 
and tunneled traffic the logging is not consistent but the helpers 
provide what's needed as you mentioned. I understand why it is the way 
it is and will simply explain this to the customer.

Thanks for the weekend response.

Senor

On 6/23/2018 17:09, Alex Rousskov wrote:
> On 06/23/2018 04:38 PM, senor wrote:
>> Hi all,
>>
>> I've noticed that a tunneled 443 request is not logged to access.log
>> until the client or server terminate which can be a long time.
> Yes, CONNECT tunnels are logged when the tunnel is over (i.e., Squid is
> done talking to the client and server). This log-at-the-end approach is
> similar to other transactions (which may also take a very long time).
>
>
>> Is it  possible to get squid to log the CONNECT at tunnel initiation?
> It is possible to be notified about CONNECT requests via eCAP and ICAP
> interfaces as well as via external ACL helpers.
>
> It is not possible to log the CONNECT request/response before the tunnel
> is over. One could, in principle, separate CONNECT request/response
> messages from the established tunnel, and log each "phase" of the tunnel
> transaction separately, but I am not sure that is a good idea -- it is
> not clear to me why a CONNECT tunnel should be treated differently from
> any other HTTP transaction where the both client and server may send
> request and response body bytes concurrently (and for a long time).
>
>
> What problem are you trying to solve?
>
>
> Alex.


From squid3 at treenet.co.nz  Mon Jun 25 02:17:50 2018
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Mon, 25 Jun 2018 14:17:50 +1200
Subject: [squid-users] squid callout sequence
In-Reply-To: <CAK0iFYzY+5h702ptw6epWmfmxxxsXE3fztp2_X170nc9icBLxQ@mail.gmail.com>
References: <CAK0iFYzY+5h702ptw6epWmfmxxxsXE3fztp2_X170nc9icBLxQ@mail.gmail.com>
Message-ID: <af8d56cd-06e9-73a1-a046-9d343eabdb81@treenet.co.nz>

On 25/06/18 05:15, Gordon Hsiao wrote:
> at https://wiki.squid-cache.org/SquidFaq/OrderIsImportant I noticed
> redirectors are way ahead of ssl-bump in the callout order, in a
> https-ssl-bump case

There is not really any "https-ssl-bump" case.

There is SSL-Bump (decrypting a TLS stream - or not), and there is HTTPS
(HTTP messages inside TLS).


> you will need ssl-bump to run (so you can get full
> URL for example), then you can run redirector based on the result of
> ssl-bump, correct?

No. SSL-Bump is an operation applied to a CONNECT message, when setting
up the TLS tunnel. There are maybe also *multiple* CONNECT messages when
SSL-Bump gets involved - which the FAQ text following that sequence
describes.


HTTP is stateless protocol. So the CONNECT message(s) are independent of
both each other, and anything decrypted from inside the tunnel. Each and
every message Squid handles gets its own cycle through the callout sequence.


> why is redirector run before ssl-bump?

Because Squid needs to know _where_ it is going before it can connect
there. SSL-Bump is part of tunnel/connection setup.

Amos


From squid3 at treenet.co.nz  Mon Jun 25 02:30:52 2018
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Mon, 25 Jun 2018 14:30:52 +1200
Subject: [squid-users]
 =?utf-8?b?U3BlZWQg4oCL4oCLbGltaXQgZnJvbSBmaWxlIHNp?= =?utf-8?q?ze?=
In-Reply-To: <CAA+Mow71MQDo4rW6GOs7HcJbsZ=oZ32GOp2xFQuE0N0=EwivRg@mail.gmail.com>
References: <CAA+Mow5nyotHcmVSS2kair1ca9BzYgQmwEEPtJWuJ8WC99954Q@mail.gmail.com>
 <ddad83d9-e314-2878-62ec-06df058c037c@measurement-factory.com>
 <CAA+Mow4Z1=EkMLdmy=Z1_7EzPjmT_vDc+O9JzBd0bRX1kmEDDA@mail.gmail.com>
 <af61d5b5-944b-f9f3-931a-337c36b177d6@measurement-factory.com>
 <CAA+Mow7EctZp1e6wUEyOgnB2g78O9dFhv9Aob-BjNYu23eqk+Q@mail.gmail.com>
 <f2f75c9e-192d-aa63-c892-8031e40c7741@measurement-factory.com>
 <CAA+Mow6LxO0t_KcmQu_dOMZf79uGC1NJ3+Bji+H9QXBhmLT21g@mail.gmail.com>
 <624f6f34-aa88-848c-51e8-97020700f489@measurement-factory.com>
 <CAA+Mow71MQDo4rW6GOs7HcJbsZ=oZ32GOp2xFQuE0N0=EwivRg@mail.gmail.com>
Message-ID: <92b089b8-1e09-ab94-29e0-53f8a089ed0b@treenet.co.nz>

On 25/06/18 05:32, Michael Pro wrote:
> Yes! I realized it!
> In store_id_program, I use the third-party utility "curl -sI
> http://url.to/real.file.zip"
> to define Content-Length and return to squid using the "note" mechanism
> in the desired ACL for response_delay_pool_access. Similarly,
> using the "note" mechanism, I force the files, depending on the extension
> and content in the cache, to use a specific tcp_outgoing_address
> or the specific FIB.
> 
> Could not you do this with the help of squid itself?

Infinite loop : To determine *how* to contact a server for that URL,
first contact a server for that URL.

The only reason your test "works" at all is that curl request is *not*
going through Squid. If you were to optimize it to send its traffic to
Squid you should see the loop immediately.

Also, there is no guarantee that a curl request to a server using curl
U-A and Accept-* headers and transfer compression capabilities - has any
relation to the actual response that same server will generate to a
request containing the clients U-A, Accept-* header, Cookies, and
transfer compression capabilities.

NP: I've seen a LOT of servers which produce 404 pages or short web
redirection pages  when robots or CLI like curl connect to them - but
the real download content when Browsers connect.

Amos


From capcoding at gmail.com  Mon Jun 25 02:59:59 2018
From: capcoding at gmail.com (Gordon Hsiao)
Date: Sun, 24 Jun 2018 21:59:59 -0500
Subject: [squid-users] squid callout sequence (Amos Jeffries)
In-Reply-To: <mailman.9322.1529893858.4557.squid-users@lists.squid-cache.org>
References: <mailman.9322.1529893858.4557.squid-users@lists.squid-cache.org>
Message-ID: <CAK0iFYz-iz5F+FAkRDfXJQcmJ-+m=QRRqXuui00c9DiGnWmSLQ@mail.gmail.com>

>
>  On 25/06/18 05:15, Gordon Hsiao wrote:
> > at https://wiki.squid-cache.org/SquidFaq/OrderIsImportant I noticed
> > redirectors are way ahead of ssl-bump in the callout order, in a
> > https-ssl-bump case
>
> There is not really any "https-ssl-bump" case.
>
> There is SSL-Bump (decrypting a TLS stream - or not), and there is HTTPS
> (HTTP messages inside TLS).
>
>
> > you will need ssl-bump to run (so you can get full
> > URL for example), then you can run redirector based on the result of
> > ssl-bump, correct?
>
> No. SSL-Bump is an operation applied to a CONNECT message, when setting
> up the TLS tunnel. There are maybe also *multiple* CONNECT messages when
> SSL-Bump gets involved - which the FAQ text following that sequence
> describes.
>
>
> HTTP is stateless protocol. So the CONNECT message(s) are independent of
> both each other, and anything decrypted from inside the tunnel. Each and
> every message Squid handles gets its own cycle through the callout
> sequence.
>
>
> > why is redirector run before ssl-bump?
>
> Because Squid needs to know _where_ it is going before it can connect
> there. SSL-Bump is part of tunnel/connection setup.
>
> Amos
>
>
will SSL-Bump(not 'peek+splice', but the 'peek+bump' mode) decrypt all the
tcp packets? For example I connect to youtube.com/myvideo, will peek+bump
only decrypt the pseudo CONNECT messages(I'm doing transparent proxy), or
will it decrypt all the video streams too? if it's the latter case the
proxy will be cpu intensive.

Thanks for the hellp

Gordon
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20180624/715b9543/attachment.htm>

From squid3 at treenet.co.nz  Mon Jun 25 03:24:26 2018
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Mon, 25 Jun 2018 15:24:26 +1200
Subject: [squid-users] squid callout sequence (Amos Jeffries)
In-Reply-To: <CAK0iFYz-iz5F+FAkRDfXJQcmJ-+m=QRRqXuui00c9DiGnWmSLQ@mail.gmail.com>
References: <mailman.9322.1529893858.4557.squid-users@lists.squid-cache.org>
 <CAK0iFYz-iz5F+FAkRDfXJQcmJ-+m=QRRqXuui00c9DiGnWmSLQ@mail.gmail.com>
Message-ID: <f985a899-7592-299d-ed2d-8e4df1092bdd@treenet.co.nz>

On 25/06/18 14:59, Gordon Hsiao wrote:
>     ?On 25/06/18 05:15, Gordon Hsiao wrote:
>     > at https://wiki.squid-cache.org/SquidFaq/OrderIsImportant I noticed
>     > redirectors are way ahead of ssl-bump in the callout order, in a
>     > https-ssl-bump case
> 
>     There is not really any "https-ssl-bump" case.
> 
>     There is SSL-Bump (decrypting a TLS stream - or not), and there is HTTPS
>     (HTTP messages inside TLS).
> 
> 
>     > you will need ssl-bump to run (so you can get full
>     > URL for example), then you can run redirector based on the result of
>     > ssl-bump, correct?
> 
>     No. SSL-Bump is an operation applied to a CONNECT message, when setting
>     up the TLS tunnel. There are maybe also *multiple* CONNECT messages when
>     SSL-Bump gets involved - which the FAQ text following that sequence
>     describes.
> 
> 
>     HTTP is stateless protocol. So the CONNECT message(s) are independent of
>     both each other, and anything decrypted from inside the tunnel. Each and
>     every message Squid handles gets its own cycle through the callout
>     sequence.
> 
> 
>     > why is redirector run before ssl-bump?
> 
>     Because Squid needs to know _where_ it is going before it can connect
>     there. SSL-Bump is part of tunnel/connection setup.
> 
>     Amos
> 
> 
> will SSL-Bump(not 'peek+splice', but the 'peek+bump' mode)?decrypt all
> the tcp packets? For example I connect to youtube.com/myvideo
> <http://youtube.com/myvideo>, will peek+bump only decrypt the pseudo
> CONNECT messages(I'm doing transparent proxy), or will it decrypt all
> the video streams too? if it's the latter case the proxy will be cpu
> intensive.

Sorry if I wasn't clear. The ssl_bump (directive and CONNECT handling)
part is the TLS handshake at the beginning of TLS connections. Once the
decrypt or splice is setup it just continues indefinitely. Whatever is
being decrypted from within that TLS is completely separate from the
bumping itself.

Amos


From ahmed.zaeem at netstream.ps  Mon Jun 25 11:27:51 2018
From: ahmed.zaeem at netstream.ps (--Ahmad--)
Date: Mon, 25 Jun 2018 14:27:51 +0300
Subject: [squid-users] how debug google status codes in log file
In-Reply-To: <9d12ad3a-dcca-31db-ed49-26fbaa904608@treenet.co.nz>
References: <234760E2-B23D-4F1D-80D0-DC3C7D47B8A3@netstream.ps>
 <9d12ad3a-dcca-31db-ed49-26fbaa904608@treenet.co.nz>
Message-ID: <CDF1D27C-FE40-4DDD-B0D1-AF5AFFF0510B@netstream.ps>

Hi Amos

thanks for the reply .

actually the sample i put is seems incorrect  im supposed to push the request as below :

25/Jun/2018:12:22:16 +0100   4057 32.175.99.98 16993 188.157.235.133 2000 TCP_TUNNEL/200 224466 CONNECT www.google.com:443 dfrrew HIER_DIRECT/ www.google.com 2a00:1450:4009:815::2004 2406:a901:416f:bdd9:392:4b51:d110:c6b9
25/Jun/2018:12:22:17 +0100   3456 32.175.99.98 17317 188.157.235.133 2000 TCP_TUNNEL/200 211560 CONNECT www.google.com:443 dfrrew HIER_DIRECT/ www.google.com 2a00:1450:4009:815::2004 2406:a901:6963:b915:91dd:ac97:af6b:843e
25/Jun/2018:12:22:17 +0100   2351 32.175.99.98 17607 188.157.235.133 2000 TCP_TUNNEL/200 220144 CONNECT www.google.com:443 dfrrew HIER_DIRECT/ www.google.com 2a00:1450:4009:815::2004 2406:a901:d64b:2c12:29a0:3422:f505:a689
25/Jun/2018:12:22:17 +0100   2299 32.175.99.98 17491 188.157.235.133 2000 TCP_TUNNEL/200 174475 CONNECT www.google.com:443 dfrrew HIER_DIRECT/ www.google.com 2a00:1450:4009:815::2004 2406:a901:52f5:b367:b482:40da:36f7:7bf6


so above is what is what logs i say about .


all what  i need is to know if the request gone correctly 
or there was a captcha page 

is there any footprints can i look for to know ?
like message reply size or so as connection encrypted 
many thanks 




> On 20 Jun 2018, at 8:02, Amos Jeffries <squid3 at treenet.co.nz> wrote:
> 
> On 19/06/18 17:12, --Ahmad-- wrote:
>> hello folks 
>> how debug google status codes in log file  ?
>> 
> 
> see the FAQ
> <https://wiki.squid-cache.org/SquidFaq/SquidLogs#access.log>
> 
> 
>> in wiki i see we have :
>> 
>> 1529368601.307  60038 184.154.133.146 TAG_NONE/503 0 CONNECT www.google.com.et:443 fifoxy HIER_NONE/- -
>> 
>> the question is how can i see the http status code of connection in proxy ?
> 
> 
> Which "the" ? there are possibly many connections used for each
> transaction (aka access.log line).
> 
> 
> The above log line says Squid spent 60sec on this transaction, no
> servers were able to be connected to in that time.
> 
> Squid also *tried* to deliver a 503 HTTP status message to the client,
> but 0 bytes got written. Probably a TCP RST got sent to the client
> instead (eg. SSL-Bump "terminate" action).
> 
> 
> 
> The *debugging* to happen is nothing to do with status codes. It is
> actions you need to perform to figure out why no server contact
> happened, and/or why Squid thought it had to send an error.
> 
> That assumes that you do not already know what happened, and that it is
> not something you wanted to configure (eg SSL-Bump terminate certain TLS
> handshakes).
> 
> Amos
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users



From capcoding at gmail.com  Mon Jun 25 13:56:08 2018
From: capcoding at gmail.com (Gordon Hsiao)
Date: Mon, 25 Jun 2018 08:56:08 -0500
Subject: [squid-users] squid callout sequence
In-Reply-To: <mailman.1.1529928001.835.squid-users@lists.squid-cache.org>
References: <mailman.1.1529928001.835.squid-users@lists.squid-cache.org>
Message-ID: <CAK0iFYxGzyedDHuNjrVOqcXvfK3FSXTgcHMwc0Q7f6p2z5X+-Q@mail.gmail.com>

>
> On 25/06/18 14:59, Gordon Hsiao wrote:
> >      On 25/06/18 05:15, Gordon Hsiao wrote:
> >     > at https://wiki.squid-cache.org/SquidFaq/OrderIsImportant I
> noticed
> >     > redirectors are way ahead of ssl-bump in the callout order, in a
> >     > https-ssl-bump case
> >
> >     There is not really any "https-ssl-bump" case.
> >
> >     There is SSL-Bump (decrypting a TLS stream - or not), and there is
> HTTPS
> >     (HTTP messages inside TLS).
> >
> >
> >     > you will need ssl-bump to run (so you can get full
> >     > URL for example), then you can run redirector based on the result
> of
> >     > ssl-bump, correct?
> >
> >     No. SSL-Bump is an operation applied to a CONNECT message, when
> setting
> >     up the TLS tunnel. There are maybe also *multiple* CONNECT messages
> when
> >     SSL-Bump gets involved - which the FAQ text following that sequence
> >     describes.
> >
> >
> >     HTTP is stateless protocol. So the CONNECT message(s) are
> independent of
> >     both each other, and anything decrypted from inside the tunnel. Each
> and
> >     every message Squid handles gets its own cycle through the callout
> >     sequence.
> >
> >
> >     > why is redirector run before ssl-bump?
> >
> >     Because Squid needs to know _where_ it is going before it can connect
> >     there. SSL-Bump is part of tunnel/connection setup.
> >
> >     Amos
> >
> >
> > will SSL-Bump(not 'peek+splice', but the 'peek+bump' mode) decrypt all
> > the tcp packets? For example I connect to youtube.com/myvideo
> > <http://youtube.com/myvideo>, will peek+bump only decrypt the pseudo
> > CONNECT messages(I'm doing transparent proxy), or will it decrypt all
> > the video streams too? if it's the latter case the proxy will be cpu
> > intensive.
>
> Sorry if I wasn't clear. The ssl_bump (directive and CONNECT handling)
> part is the TLS handshake at the beginning of TLS connections. Once the
> decrypt or splice is setup it just continues indefinitely. Whatever is
> being decrypted from within that TLS is completely separate from the
> bumping itself.
>
> Amos
>
>
> So, peek+bump itself will only deal with TLS handshake part(e.g. to get
FQDN/full-URL for redirectors) , still the proxy will have to do
aes-decrypt-and-encrypt for the same TCP stream when peek+bump is used,
which could be very cpu intensive, correct? Because once peek+bump is used,
the proxy split the ssl stream into two segments and will have to deal with
everything for both ends.

peek+splice is totally different and it will not need the
aes-decrypt-and-encrypt, basically just probe for SNI then tunnel the whole
connection, so proxy's cpu should not be overloaded at all.

Thanks a lot for the explanations,

Gordon
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20180625/4fb2def7/attachment.htm>

From rousskov at measurement-factory.com  Mon Jun 25 16:17:04 2018
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Mon, 25 Jun 2018 10:17:04 -0600
Subject: [squid-users] squid callout sequence
In-Reply-To: <CAK0iFYzY+5h702ptw6epWmfmxxxsXE3fztp2_X170nc9icBLxQ@mail.gmail.com>
References: <CAK0iFYzY+5h702ptw6epWmfmxxxsXE3fztp2_X170nc9icBLxQ@mail.gmail.com>
Message-ID: <76f67cfe-d12d-c65c-7f9f-d07aa3250e8c@measurement-factory.com>

On 06/24/2018 11:15 AM, Gordon Hsiao wrote:
> why is redirector run before ssl-bump?

Adding to Amos' response: Please note that the redirector runs both
before SslBump for CONNECT URLs and "after" SslBump for each of the
decrypted HTTP requests inside the CONNECT tunnel (if the tunnel was
bumped). In other words, the redirector can attempt to "redirect"
virtually any HTTP request allowed by Squid.

What would happen to a bumped HTTPS GET request if it gets redirected to
another _domain_? I am not sure, and I am not sure that whatever happens
today will happen tomorrow. On one hand, admins do not want Squid to
accidentally change the domain that the client thinks it is securely
communicating with. On the other hand, some admins may have a legitimate
need to do exactly that for some of the requests.

Alex.


From rousskov at measurement-factory.com  Mon Jun 25 16:35:38 2018
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Mon, 25 Jun 2018 10:35:38 -0600
Subject: [squid-users]
 =?utf-8?b?U3BlZWQg4oCL4oCLbGltaXQgZnJvbSBmaWxlIHNp?= =?utf-8?q?ze?=
In-Reply-To: <CAA+Mow71MQDo4rW6GOs7HcJbsZ=oZ32GOp2xFQuE0N0=EwivRg@mail.gmail.com>
References: <CAA+Mow5nyotHcmVSS2kair1ca9BzYgQmwEEPtJWuJ8WC99954Q@mail.gmail.com>
 <ddad83d9-e314-2878-62ec-06df058c037c@measurement-factory.com>
 <CAA+Mow4Z1=EkMLdmy=Z1_7EzPjmT_vDc+O9JzBd0bRX1kmEDDA@mail.gmail.com>
 <af61d5b5-944b-f9f3-931a-337c36b177d6@measurement-factory.com>
 <CAA+Mow7EctZp1e6wUEyOgnB2g78O9dFhv9Aob-BjNYu23eqk+Q@mail.gmail.com>
 <f2f75c9e-192d-aa63-c892-8031e40c7741@measurement-factory.com>
 <CAA+Mow6LxO0t_KcmQu_dOMZf79uGC1NJ3+Bji+H9QXBhmLT21g@mail.gmail.com>
 <624f6f34-aa88-848c-51e8-97020700f489@measurement-factory.com>
 <CAA+Mow71MQDo4rW6GOs7HcJbsZ=oZ32GOp2xFQuE0N0=EwivRg@mail.gmail.com>
Message-ID: <658780f3-9edd-1e06-c21d-4548ab398959@measurement-factory.com>

On 06/24/2018 11:32 AM, Michael Pro wrote:
> In store_id_program, I use the third-party utility "curl -sI
> http://url.to/real.file.zip"
> to define Content-Length and return to squid using the "note" mechanism
> in the desired ACL for response_delay_pool_access.

> Could not you do this with the help of squid itself?


I see two ways to interpret your question:


A) Is it possible for my curl probing requests to go through Squid?

Sure, curl supports HTTP and HTTPS proxies. Just add ACLs that would
detect and treat your curl requests specially (e.g., avoid sending them
back to the store_id_program and do not cache their responses).


B) Is it possible for Squid itself to initiate probing requests?

Yes, it is possible to modify Squid to send probing requests that would
determine the expected response content length. Probing requests are
costly (you double network traffic in many cases and block the user
while the probe runs). Probes are also difficult to implement correctly
(see Amos response for some of the complications).

If you want to implement probing in Squid, and want your changes to be
officially accepted, then please discuss your design (on squid-dev)
before implementing anything -- many designs are likely to be rejected
for the reasons mentioned above.


HTH,

Alex.



> ??, 22 ???. 2018 ?. ? 18:22, Alex Rousskov <rousskov at measurement-factory.com>:
>>
>> On 06/21/2018 10:34 PM, Michael Pro wrote:
>>
>>> I put all the necessary links in the database in step #2.
>>> I'm quite satisfied with entering this value (Content-Length) after step #3.
>>> How can I update the data for the link?
>>
>> I am not sure I understand the question, but if you are asking how to
>> add response Content-Length info to some external database, then you
>> have a few options, including:
>>
>> 1. An external ACL used with http_reply_access
>>    (gets notified after receiving response headers)
>>    http://www.squid-cache.org/Doc/config/http_reply_access/
>>
>> 2. An access.log daemon (gets notified after response delivery)
>>
>> 3. An access.log parser (gets notified after response delivery
>>    and log flushing)
>>
>> Alex.
>>
>>
>>> ??, 21 ???. 2018 ?. ? 22:52, Alex Rousskov <rousskov at measurement-factory.com>:
>>>>
>>>> On 06/21/2018 12:08 PM, Michael Pro wrote:
>>>>> ??, 21 ???. 2018 ?. ? 19:09, Alex Rousskov <rousskov at measurement-factory.com>:
>>>>>> ...
>>>>>> You may prefer response delay pools, but they are only available in v5:
>>>>>> https://github.com/squid-cache/squid/commit/b27668e
>>>>
>>>>> But to use this I understood that I need to use the store_id_program mechanism.
>>>>
>>>> There is no relationship between response delay pools and Store IDs.
>>>>
>>>>
>>>>> A related question is how to pass the size (Content-Length) to store_id_extras.
>>>>
>>>> You cannot pass response Content-Length header to the store_id_program
>>>> because that helper is consulted _before_ there is a response (step #2):
>>>>
>>>>   1. parse client request X
>>>>
>>>>   2. adapt X into request Y (ICAP, eCAP, url_rewriter, store_id, etc.)
>>>>
>>>>   3. get the resource requested by Y
>>>>      (from the cache, peer, or origin server)
>>>>
>>>>
>>>> HTH,
>>>>
>>>> Alex.
>>> _______________________________________________
>>> squid-users mailing list
>>> squid-users at lists.squid-cache.org
>>> http://lists.squid-cache.org/listinfo/squid-users
>>>
>>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
> 



From squid at mail.verwaiser.de  Mon Jun 25 20:09:11 2018
From: squid at mail.verwaiser.de (Verwaiser)
Date: Mon, 25 Jun 2018 13:09:11 -0700 (MST)
Subject: [squid-users] Adobe CC behing Squid
Message-ID: <1529957351000-0.post@n4.nabble.com>

Hello,
I'm trying to realize the Adobe CC Programs to work on workstations behind
our Squid (Debian 8, Squid 3.5). We are using user authentification to a
LDAP-Server.

Is there anyone with experiences how to make this work?

Adobe gives some informations:
https://helpx.adobe.com/de/creative-cloud/kb/proxy-authentication-support-creative-cloud.html

But I don't exactly know if Squid is a proxy with "PAC-URL", but it seems to
me to be so.

This link shows the adresses needed for access:
https://helpx.adobe.com/content/dam/help/attachments/Creative_Cloud_for_enterprise_Service_Endpoints.pdf

It shows a long address list:
*.adobesc.com
*.licenses.adobe.com
*.adobelogin.com
*.ftcdn.net
*.behance.net
*.adobedtm.com
*.demdex.net
*.demandbase.com
*.adobeoobe.com
*.macromedia.com adbemdigitalmediarebootprod2.112.2o7.net
*.edgefonts.net
*.adobejanus.com
*.adobesunbreak.com
*. adobeccstatic.com
[...]

and many more, but which acls should I use to open access for those urls?
I tried:

######################
acl adobedl dstdomain .adobe.com
http_access allow adobedl
always_direct allow adobedl
######################

or:

######################
acl alleRechner src 192.168.2.0/20
acl w7aktivierung dstdomain "/etc/squid3/w7akt"
http_access allow w7aktivierung alleRechner
######################

with file w7akt containing:

swupmf.adobe.com
swupdl.adobe.com
amparex.net
adobelogin.com
adobeoobe.com
[...]

and many others



Any ideas?

Thanks for your help,

Holger!




--
Sent from: http://squid-web-proxy-cache.1019090.n4.nabble.com/Squid-Users-f1019091.html


From squid3 at treenet.co.nz  Mon Jun 25 22:55:10 2018
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Tue, 26 Jun 2018 10:55:10 +1200
Subject: [squid-users] how debug google status codes in log file
In-Reply-To: <CDF1D27C-FE40-4DDD-B0D1-AF5AFFF0510B@netstream.ps>
References: <234760E2-B23D-4F1D-80D0-DC3C7D47B8A3@netstream.ps>
 <9d12ad3a-dcca-31db-ed49-26fbaa904608@treenet.co.nz>
 <CDF1D27C-FE40-4DDD-B0D1-AF5AFFF0510B@netstream.ps>
Message-ID: <21552717-783d-6dc0-29d8-9f150b4a3273@treenet.co.nz>

On 25/06/18 23:27, --Ahmad-- wrote:
> Hi Amos
> 
> thanks for the reply .
> 
> actually the sample i put is seems incorrect  im supposed to push the request as below :
> 
> 25/Jun/2018:12:22:16 +0100   4057 32.175.99.98 16993 188.157.235.133 2000 TCP_TUNNEL/200 224466 CONNECT www.google.com:443 dfrrew HIER_DIRECT/ www.google.com 2a00:1450:4009:815::2004 2406:a901:416f:bdd9:392:4b51:d110:c6b9
> 25/Jun/2018:12:22:17 +0100   3456 32.175.99.98 17317 188.157.235.133 2000 TCP_TUNNEL/200 211560 CONNECT www.google.com:443 dfrrew HIER_DIRECT/ www.google.com 2a00:1450:4009:815::2004 2406:a901:6963:b915:91dd:ac97:af6b:843e
> 25/Jun/2018:12:22:17 +0100   2351 32.175.99.98 17607 188.157.235.133 2000 TCP_TUNNEL/200 220144 CONNECT www.google.com:443 dfrrew HIER_DIRECT/ www.google.com 2a00:1450:4009:815::2004 2406:a901:d64b:2c12:29a0:3422:f505:a689
> 25/Jun/2018:12:22:17 +0100   2299 32.175.99.98 17491 188.157.235.133 2000 TCP_TUNNEL/200 174475 CONNECT www.google.com:443 dfrrew HIER_DIRECT/ www.google.com 2a00:1450:4009:815::2004 2406:a901:52f5:b367:b482:40da:36f7:7bf6
> 
> 
> so above is what is what logs i say about .
> 
> 
> all what  i need is to know if the request gone correctly 
> or there was a captcha page 

There is no way to tell from that. CONNECT is a tunnel containing many
encrypted/hidden requests.

Since that is a custom format, I hesitate to say what the above means.


> 
> is there any footprints can i look for to know ?

Not without SSL-Bump decrypting the tunnel contents.

> like message reply size or so as connection encrypted 
> many thanks 
> 

The size of data in the tunnel can give a rough view of whether it did
*something*, vs was dropped by the server. eg sending KB or more data
likely did at least one HTTP request/reply (assuming its actually HTTPS).

Other than that, no. TLS is designed explicitly to hide that type of
info you are looking for.

Amos


From squid3 at treenet.co.nz  Mon Jun 25 23:35:30 2018
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Tue, 26 Jun 2018 11:35:30 +1200
Subject: [squid-users] squid callout sequence
In-Reply-To: <CAK0iFYxGzyedDHuNjrVOqcXvfK3FSXTgcHMwc0Q7f6p2z5X+-Q@mail.gmail.com>
References: <mailman.1.1529928001.835.squid-users@lists.squid-cache.org>
 <CAK0iFYxGzyedDHuNjrVOqcXvfK3FSXTgcHMwc0Q7f6p2z5X+-Q@mail.gmail.com>
Message-ID: <f069d863-78c6-36f3-7c25-cb6a64f0115d@treenet.co.nz>

On 26/06/18 01:56, Gordon Hsiao wrote:
>     On 25/06/18 14:59, Gordon Hsiao wrote:
>     >? ? ??On 25/06/18 05:15, Gordon Hsiao wrote:
>     >? ? ?> at https://wiki.squid-cache.org/SquidFaq/OrderIsImportant I
>     noticed
>     >? ? ?> redirectors are way ahead of ssl-bump in the callout order, in a
>     >? ? ?> https-ssl-bump case
>     >
>     >? ? ?There is not really any "https-ssl-bump" case.
>     >
>     >? ? ?There is SSL-Bump (decrypting a TLS stream - or not), and
>     there is HTTPS
>     >? ? ?(HTTP messages inside TLS).
>     >
>     >
>     >? ? ?> you will need ssl-bump to run (so you can get full
>     >? ? ?> URL for example), then you can run redirector based on the
>     result of
>     >? ? ?> ssl-bump, correct?
>     >
>     >? ? ?No. SSL-Bump is an operation applied to a CONNECT message,
>     when setting
>     >? ? ?up the TLS tunnel. There are maybe also *multiple* CONNECT
>     messages when
>     >? ? ?SSL-Bump gets involved - which the FAQ text following that
>     sequence
>     >? ? ?describes.
>     >
>     >
>     >? ? ?HTTP is stateless protocol. So the CONNECT message(s) are
>     independent of
>     >? ? ?both each other, and anything decrypted from inside the
>     tunnel. Each and
>     >? ? ?every message Squid handles gets its own cycle through the callout
>     >? ? ?sequence.
>     >
>     >
>     >? ? ?> why is redirector run before ssl-bump?
>     >
>     >? ? ?Because Squid needs to know _where_ it is going before it can
>     connect
>     >? ? ?there. SSL-Bump is part of tunnel/connection setup.
>     >
>     >? ? ?Amos
>     >
>     >
>     > will SSL-Bump(not 'peek+splice', but the 'peek+bump' mode)?decrypt all
>     > the tcp packets? For example I connect to youtube.com/myvideo
>     <http://youtube.com/myvideo>
>     > <http://youtube.com/myvideo>, will peek+bump only decrypt the pseudo
>     > CONNECT messages(I'm doing transparent proxy), or will it decrypt all
>     > the video streams too? if it's the latter case the proxy will be cpu
>     > intensive.
> 
>     Sorry if I wasn't clear. The ssl_bump (directive and CONNECT handling)
>     part is the TLS handshake at the beginning of TLS connections. Once the
>     decrypt or splice is setup it just continues indefinitely. Whatever is
>     being decrypted from within that TLS is completely separate from the
>     bumping itself.
> 
>     Amos
> 
> 
> So, peek+bump itself will only deal with TLS handshake part(e.g. to get
> FQDN/full-URL for redirectors)

Using the correct terms helps understanding. see RFC 7230
<https://tools.ietf.org/html/rfc7230#section-5.3>

'peek' and 'bump' are actions the SSL-Bump feature does.


* SSL-Bump 'peek' action gives the TLS SNI. Which is *only* a domain
name / FQDN
   - so an authority-form URI (FQDN:port) is known for the server.


* SSL-Bump 'bump' action exposes the encrypted requests which contain
their own URI-scheme, domain, port, path, query etc.
  - so the full absolute-form URI is known for each decrypted message.

  - these may not match the authority-form URI from the CONNECT message.



>, still the proxy will have to do
> aes-decrypt-and-encrypt for the same TCP stream when peek+bump is used,

To get full-URL (ie path and query parts).

> which could be very cpu intensive, correct? Because once peek+bump is
> used, the proxy split the ssl stream into two segments and will have to
> deal with everything for both ends.

Yes.

> 
> peek+splice is totally different and it will not need the
> aes-decrypt-and-encrypt, basically just probe for SNI then tunnel the
> whole connection, so proxy's cpu should not be overloaded at all.

Close yes.

Though to complicate things the presence of a proxy at all splits the
stream into "two parts" to get that peek ability. It's just that splice
action afterwards avoids the decrypt+encrypt CPU cycles.

Amos


From squid3 at treenet.co.nz  Tue Jun 26 00:31:16 2018
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Tue, 26 Jun 2018 12:31:16 +1200
Subject: [squid-users] Adobe CC behing Squid
In-Reply-To: <1529957351000-0.post@n4.nabble.com>
References: <1529957351000-0.post@n4.nabble.com>
Message-ID: <00d25824-a021-f2c0-9240-3ac700f95f9d@treenet.co.nz>

On 26/06/18 08:09, Verwaiser wrote:
> Hello,
> I'm trying to realize the Adobe CC Programs to work on workstations behind
> our Squid (Debian 8, Squid 3.5). We are using user authentification to a
> LDAP-Server.
> 
> Is there anyone with experiences how to make this work?

Question is:

* do these applications use HTTP?

 The docs you reference seem to indicate the answer is yes.


* what do you see happening?


> 
> Adobe gives some informations:
> https://helpx.adobe.com/de/creative-cloud/kb/proxy-authentication-support-creative-cloud.html
> 
> But I don't exactly know if Squid is a proxy with "PAC-URL", but it seems to
> me to be so.

Please read the FAQ section on auto-configuration:
 <https://wiki.squid-cache.org/SquidFaq/ConfiguringBrowsers>

> 
> Any ideas?

Follow the Adobe instructions?

They do not mention having to add any special bypass rules for the proxy
to work. If your clients are allowed to use the proxy these applications
should "just work".

Amos


From Sarfaraz.Ahmad at deshaw.com  Tue Jun 26 05:42:13 2018
From: Sarfaraz.Ahmad at deshaw.com (Ahmad, Sarfaraz)
Date: Tue, 26 Jun 2018 05:42:13 +0000
Subject: [squid-users] Splice using SubjectCN/SAN from remote server
	certificate
Message-ID: <8d4cbba88d5047688f50aeb85ec2c76d@mbxtoa3.winmail.deshaw.com>

I realize that unlike other proprietary MITM appliances, Squid doesn't fiddle with the original client hello.
I think this magnifies into the fact that we cannot look at the SubjectCN/SAN in the remote server certificate and then decide whether we want to splice or bump. (peeking at step 2 really restricts our options)
Is my understanding correct ? Or is there a way to accomplish this ?

Best Regards,
Sarfaraz

From squid3 at treenet.co.nz  Tue Jun 26 07:08:12 2018
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Tue, 26 Jun 2018 19:08:12 +1200
Subject: [squid-users] Splice using SubjectCN/SAN from remote server
 certificate
In-Reply-To: <8d4cbba88d5047688f50aeb85ec2c76d@mbxtoa3.winmail.deshaw.com>
References: <8d4cbba88d5047688f50aeb85ec2c76d@mbxtoa3.winmail.deshaw.com>
Message-ID: <734dfb4c-8296-4964-b75f-f72d7ee8f3ba@treenet.co.nz>

On 26/06/18 17:42, Ahmad, Sarfaraz wrote:
> I realize that unlike other proprietary MITM appliances, Squid doesn't fiddle with the original client hello.

That is not strictly true. It depends on what you have configured Squid
to do.

Squid does adjust the TLS extensions to only allow features that are
supported (ie ALPN to remove HTTP/2, etc which is not yet supported by
Squid).


> I think this magnifies into the fact that we cannot look at the SubjectCN/SAN in the remote server certificate and then decide whether we want to splice or bump. (peeking at step 2 really restricts our options)
> Is my understanding correct ?

No. Peeking at the client Hello does not impact the final decision,
whether you peek or stare at the server Hello is what does that.


> Or is there a way to accomplish this ?

If the client and proxy capabilities and OpenSSL config are identical
(or nearly so) then theoretically Squid can still splice after a stare
action. But whether the current SSL-Bump implementation is smart enough
to detect that case I'm not sure.

Amos


From Sarfaraz.Ahmad at deshaw.com  Tue Jun 26 13:22:16 2018
From: Sarfaraz.Ahmad at deshaw.com (Ahmad, Sarfaraz)
Date: Tue, 26 Jun 2018 13:22:16 +0000
Subject: [squid-users] Trust a particular CA only for a limited domain
Message-ID: <f714dd26a62d49a798d3b24864caf406@mbxtoa3.winmail.deshaw.com>

I need to provide access to my clients to a service on the internet that is using a private CA.
I do not want to trust that CA outside the scope of that destination domain.  (The thought is to not just blindly trust a random CA, rather if we have to, we limit it to the particular domain.)
Can something like this be achieved without toying with the squid's code ?

BR,
Sarfaraz

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20180626/7c0d2fb7/attachment.htm>

From amit at xsinfosol.com  Tue Jun 26 15:22:45 2018
From: amit at xsinfosol.com (Amit Pasari - XS INFOSOL Inc. USA)
Date: Tue, 26 Jun 2018 20:52:45 +0530
Subject: [squid-users] Chrome 67 Issue with SSL Bump
Message-ID: <e0ef9bb7-507c-be47-334c-a1f72be8b0a3@xsinfosol.com>

Dear All,

I am using squid ver.3.5.26  on centos 6.7 with below configuration .

=============================

http_port 3128  intercept
https_port 3129 intercept ssl-bump generate-host-certificates=on 
dynamic_cert_mem_cache_size=4MB cert=/etc/myssl/public.pem 
capath=/etc/ssl/certs options=NO_SSLv3 key=/etc/myssl/private.pem

ssl_bump peek step1 all
ssl_bump peek step2 serverIsBank
ssl_bump splice step3 serverIsBank
ssl_bump bump all

==========================

I am using squid in transparent mode . Everything working fine in 
Firefox and IE after i have imported the certificate in both the 
browser  , but in Chrome 67 version on Windows 10 i am facing the below 
issue

NET::ERR_CERT_WEAK_SIGNATURE_ALGORITHM

When i open https://facebook.com , https://linkedin.com etc .

I am clueless on the same now .

Amit


-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20180626/c20dbedb/attachment.htm>

From capcoding at gmail.com  Tue Jun 26 16:12:47 2018
From: capcoding at gmail.com (Gordon Hsiao)
Date: Tue, 26 Jun 2018 11:12:47 -0500
Subject: [squid-users] when will squid 4 be production ready?
Message-ID: <CAK0iFYyuRxLNHOXhLy3=xB7PdyKW5SGqM-E3a-LT5+Wf3NfgdA@mail.gmail.com>

squid4 has been released for quite a while, when will it be production
ready or any rough timeline on the horizon?

Some little features are attractive such as automatic intermediate CA
download.

on another notes, it would be great if someone can update Squid book on
3.5/4.x, especially on ssl-bump and other new stuff.

Cheers,
Gordon
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20180626/33fd5848/attachment.htm>

From Walter.H at mathemainzel.info  Tue Jun 26 16:13:08 2018
From: Walter.H at mathemainzel.info (Walter H.)
Date: Tue, 26 Jun 2018 18:13:08 +0200
Subject: [squid-users] Chrome 67 Issue with SSL Bump
In-Reply-To: <e0ef9bb7-507c-be47-334c-a1f72be8b0a3@xsinfosol.com>
References: <e0ef9bb7-507c-be47-334c-a1f72be8b0a3@xsinfosol.com>
Message-ID: <5B326614.3030701@mathemainzel.info>

On 26.06.2018 17:22, Amit Pasari - XS INFOSOL Inc. USA wrote:
>
> I am using squid in transparent mode . Everything working fine in 
> Firefox and IE after i have imported the certificate in both the 
> browser  , but in Chrome 67 version on Windows 10 i am facing the 
> below issue
>
> NET::ERR_CERT_WEAK_SIGNATURE_ALGORITHM
>
> When i open https://facebook.com , https://linkedin.com etc .
>
> I am clueless on the same now .
>
> Amit
>
Have you generated a SHA1 or SHA-256 certificate?

Walter

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20180626/c5ebe678/attachment.htm>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: smime.p7s
Type: application/pkcs7-signature
Size: 3491 bytes
Desc: S/MIME Cryptographic Signature
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20180626/c5ebe678/attachment.bin>

From amit at xsinfosol.com  Tue Jun 26 17:03:20 2018
From: amit at xsinfosol.com (Amit pasari)
Date: Tue, 26 Jun 2018 22:33:20 +0530
Subject: [squid-users] Chrome 67 Issue with SSL Bump
In-Reply-To: <5B326614.3030701@mathemainzel.info>
References: <e0ef9bb7-507c-be47-334c-a1f72be8b0a3@xsinfosol.com>
 <5B326614.3030701@mathemainzel.info>
Message-ID: <9DF2E539-F1C9-4C5E-B22B-243CCA0D6323@xsinfosol.com>

Dear Walter 
 
I have tried with both SHA1 and SHA256 cert . 


Sent from my iPhone

> On Jun 26, 2018, at 9:43 PM, Walter H. <Walter.H at mathemainzel.info> wrote:
> 
>> On 26.06.2018 17:22, Amit Pasari - XS INFOSOL Inc. USA wrote:
>> I am using squid in transparent mode . Everything working fine in Firefox and IE after i have imported the certificate in both the browser  , but in Chrome 67 version on Windows 10 i am facing the below issue 
>> NET::ERR_CERT_WEAK_SIGNATURE_ALGORITHM
>> When i open https://facebook.com , https://linkedin.com etc .
>> I am clueless on the same now . 
>> Amit
>> 
> Have you generated a SHA1 or SHA-256 certificate?
> 
> Walter
> 
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20180626/b1fe9d4d/attachment.htm>

From Walter.H at mathemainzel.info  Tue Jun 26 17:08:06 2018
From: Walter.H at mathemainzel.info (Walter H.)
Date: Tue, 26 Jun 2018 19:08:06 +0200
Subject: [squid-users] Chrome 67 Issue with SSL Bump
In-Reply-To: <9DF2E539-F1C9-4C5E-B22B-243CCA0D6323@xsinfosol.com>
References: <e0ef9bb7-507c-be47-334c-a1f72be8b0a3@xsinfosol.com>
 <5B326614.3030701@mathemainzel.info>
 <9DF2E539-F1C9-4C5E-B22B-243CCA0D6323@xsinfosol.com>
Message-ID: <5B3272F6.7050500@mathemainzel.info>

On 26.06.2018 19:03, Amit pasari wrote:
> Dear Walter
> I have tried with both SHA1 and SHA256 cert .
>
>
> Sent from my iPhone
>
> On Jun 26, 2018, at 9:43 PM, Walter H. <Walter.H at mathemainzel.info 
> <mailto:Walter.H at mathemainzel.info>> wrote:
>
>> On 26.06.2018 17:22, Amit Pasari - XS INFOSOL Inc. USA wrote:
>>>
>>> I am using squid in transparent mode . Everything working fine in 
>>> Firefox and IE after i have imported the certificate in both the 
>>> browser  , but in Chrome 67 version on Windows 10 i am facing the 
>>> below issue
>>>
>>> NET::ERR_CERT_WEAK_SIGNATURE_ALGORITHM
>>>
>>> When i open https://facebook.com , https://linkedin.com etc .
>>>
>>> I am clueless on the same now .
>>>
>>> Amit
>>>
>> Have you generated a SHA1 or SHA-256 certificate?
>>
>> Walter
>>
can you try this:

sslproxy_cert_sign_hash sha256

and use a SHA-256  certificate

Walter
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20180626/616617b9/attachment.htm>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: smime.p7s
Type: application/pkcs7-signature
Size: 3491 bytes
Desc: S/MIME Cryptographic Signature
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20180626/616617b9/attachment.bin>

From rousskov at measurement-factory.com  Tue Jun 26 17:54:25 2018
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Tue, 26 Jun 2018 11:54:25 -0600
Subject: [squid-users] Splice using SubjectCN/SAN from remote server
 certificate
In-Reply-To: <8d4cbba88d5047688f50aeb85ec2c76d@mbxtoa3.winmail.deshaw.com>
References: <8d4cbba88d5047688f50aeb85ec2c76d@mbxtoa3.winmail.deshaw.com>
Message-ID: <18d9100f-7208-7892-5707-936e8a8e46a4@measurement-factory.com>

On 06/25/2018 11:42 PM, Ahmad, Sarfaraz wrote:

> we cannot look at the SubjectCN/SAN in the remote server certificate
> and then decide whether we want to splice or bump. (peeking at step 
> 2 really restricts our options) Is my understanding correct ? Or is
> there a way to accomplish this ?

In some rare cases, it is possible to peek at the server and then bump
the connections: For that to work, Squid must fool OpenSSL into
believing that OpenSSL generated the forwarded ClientHello message. This
requires adjusting internal OpenSSL state. That adjustment is possible
for some OpenSSL versions. Relying on this trick is unsafe because the
server may use a cipher (or another TLS feature) that Squid does not
actually support, precluding bumping.

In most modern scenarios, the adjustment is either impossible or unsafe.
Moderns Squids do not enable this feature by default:

> checking whether hello message can be overwritten in SSL struct... possibly; to try, set SQUID_USE_OPENSSL_HELLO_OVERWRITE_HACK macro value to 1


Similarly, there are rare cases where it is possible to stare at the
server and then splice the connections. Doing so requires using the same
hack as described above: Squid forwards ClientHello intact while
allowing OpenSSL to later bump the connection because OpenSSL thinks
that it sent that ClientHello.


FWIW, please note that it is not possible to forward a modified
ClientHello and then splice TLS connections. Splicing requires
forwarding intact ClientHello and ServerHello messages because TLS
agents exchange their checksums in the Finished messages.

Also, TLS v1.3 will make most of this irrelevant because it encrypts the
server certificate. You would have to make most of your decisions during
step2.


HTH,

Alex.


From amit at xsinfosol.com  Tue Jun 26 17:55:42 2018
From: amit at xsinfosol.com (Amit Pasari - XS INFOSOL Inc. USA)
Date: Tue, 26 Jun 2018 23:25:42 +0530
Subject: [squid-users] Chrome 67 Issue with SSL Bump
In-Reply-To: <5B3272F6.7050500@mathemainzel.info>
References: <e0ef9bb7-507c-be47-334c-a1f72be8b0a3@xsinfosol.com>
 <5B326614.3030701@mathemainzel.info>
 <9DF2E539-F1C9-4C5E-B22B-243CCA0D6323@xsinfosol.com>
 <5B3272F6.7050500@mathemainzel.info>
Message-ID: <076b5e23-3009-9a67-4abd-298bd487891c@xsinfosol.com>

Let me try the below solution , but if thats the case it shouldn't work 
with other browsers as well  , what i think is chrome is either not 
reading my cert or rejecting it .

Unsure .

Amit

On 6/26/18 10:38 PM, Walter H. wrote:
> On 26.06.2018 19:03, Amit pasari wrote:
>> Dear Walter
>> I have tried with both SHA1 and SHA256 cert .
>>
>>
>> Sent from my iPhone
>>
>> On Jun 26, 2018, at 9:43 PM, Walter H. <Walter.H at mathemainzel.info 
>> <mailto:Walter.H at mathemainzel.info>> wrote:
>>
>>> On 26.06.2018 17:22, Amit Pasari - XS INFOSOL Inc. USA wrote:
>>>>
>>>> I am using squid in transparent mode . Everything working fine in 
>>>> Firefox and IE after i have imported the certificate in both the 
>>>> browser  , but in Chrome 67 version on Windows 10 i am facing the 
>>>> below issue
>>>>
>>>> NET::ERR_CERT_WEAK_SIGNATURE_ALGORITHM
>>>>
>>>> When i open https://facebook.com , https://linkedin.com etc .
>>>>
>>>> I am clueless on the same now .
>>>>
>>>> Amit
>>>>
>>> Have you generated a SHA1 or SHA-256 certificate?
>>>
>>> Walter
>>>
> can you try this:
>
> sslproxy_cert_sign_hash sha256
>
> and use a SHA-256  certificate
>
> Walter


-- 
XS Infosol
	
*Amit Pasari*
CEO
*XS Infosol Pvt Ltd*

<https://www.facebook.com/XSInfosol.Inc> 
<https://www.linkedin.com/company/xs-infosol-inc-/> 
<https://twitter.com/xsinfosol> <https://plus.google.com/+Xsinfosol/posts0>

	
*Call* : +91-120-4978080, Extn.101
*Mobile* : +91-9953007901
*Skype Id* : amitpasari
*Mail id* : amit at xsinfosol.com
*Website* : www.xsinfosol.com

<http://www.xsinfosol.com>

<http://www.xsinfosol.com>

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20180626/ecacfc69/attachment.htm>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: amit-pasari.jpg
Type: image/jpeg
Size: 4187 bytes
Desc: not available
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20180626/ecacfc69/attachment.jpg>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: f.jpg
Type: image/jpeg
Size: 9644 bytes
Desc: not available
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20180626/ecacfc69/attachment-0001.jpg>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: in.jpg
Type: image/jpeg
Size: 9803 bytes
Desc: not available
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20180626/ecacfc69/attachment-0002.jpg>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: t.jpg
Type: image/jpeg
Size: 9637 bytes
Desc: not available
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20180626/ecacfc69/attachment-0003.jpg>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: g.jpg
Type: image/jpeg
Size: 9482 bytes
Desc: not available
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20180626/ecacfc69/attachment-0004.jpg>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: logo.jpg
Type: image/jpeg
Size: 4967 bytes
Desc: not available
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20180626/ecacfc69/attachment-0005.jpg>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: banner.jpg
Type: image/jpeg
Size: 16606 bytes
Desc: not available
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20180626/ecacfc69/attachment-0006.jpg>

From rousskov at measurement-factory.com  Tue Jun 26 18:37:27 2018
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Tue, 26 Jun 2018 12:37:27 -0600
Subject: [squid-users] Trust a particular CA only for a limited domain
In-Reply-To: <f714dd26a62d49a798d3b24864caf406@mbxtoa3.winmail.deshaw.com>
References: <f714dd26a62d49a798d3b24864caf406@mbxtoa3.winmail.deshaw.com>
Message-ID: <f8e6b127-3878-d9e1-8e32-ac2483338e68@measurement-factory.com>

On 06/26/2018 07:22 AM, Ahmad, Sarfaraz wrote:
> I need to provide access to my clients to a service on the internet that
> is using a private CA.
> 
> I do not want to trust that CA outside the scope of that destination
> domain. ?(The thought is to not just blindly trust a random CA, rather
> if we have to, we limit it to the particular domain.)
> 
> Can something like this be achieved without toying with the squid?s code ?


I believe this can be done with a sslcrtvalidator_program helper:

* http://www.squid-cache.org/Doc/config/sslcrtvalidator_program/
*
https://wiki.squid-cache.org/Features/AddonHelpers#SSL_server_certificate_validator

Alternatively, you may be able to block (wrong) responses signed by that
CA using an external ACL that is supplied %ssl::>cert_issuer and origin
domain information.

The validator helper approach prevents untrusted HTTP messages from
reaching Squid, but the external ACL approach is easier to implement.


HTH,

Alex.


From capcoding at gmail.com  Tue Jun 26 21:51:50 2018
From: capcoding at gmail.com (Gordon Hsiao)
Date: Tue, 26 Jun 2018 16:51:50 -0500
Subject: [squid-users] ACL vs redirector order
Message-ID: <CAK0iFYyXQYP8VY5_hvJABa7F2gE_6sV3xsaz8U+3BLw=LmVzEg@mail.gmail.com>

Assuming I allow a domain to pass in ACL, but deny it in my redirector,
which one will work?

Also, assuming I deny a domain in squid.conf, but allow in in the
redirector, which one will take precedence?

Will there be a difference for the above when peek+splice / peek+bump was
used?

Thanks,
Gordon
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20180626/8eca1d59/attachment.htm>

From squid3 at treenet.co.nz  Tue Jun 26 23:04:50 2018
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Wed, 27 Jun 2018 11:04:50 +1200
Subject: [squid-users] when will squid 4 be production ready?
In-Reply-To: <CAK0iFYyuRxLNHOXhLy3=xB7PdyKW5SGqM-E3a-LT5+Wf3NfgdA@mail.gmail.com>
References: <CAK0iFYyuRxLNHOXhLy3=xB7PdyKW5SGqM-E3a-LT5+Wf3NfgdA@mail.gmail.com>
Message-ID: <be353a9b-c0bc-02dc-e132-d4c0a91dde0e@treenet.co.nz>

On 27/06/18 04:12, Gordon Hsiao wrote:
> squid4 has been released for quite a while, when will it be production
> ready or any rough timeline on the horizon?
> 

<http://lists.squid-cache.org/pipermail/squid-dev/2018-June/009416.html>

Here's hoping.

Amos


From squid3 at treenet.co.nz  Tue Jun 26 23:19:06 2018
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Wed, 27 Jun 2018 11:19:06 +1200
Subject: [squid-users] ACL vs redirector order
In-Reply-To: <CAK0iFYyXQYP8VY5_hvJABa7F2gE_6sV3xsaz8U+3BLw=LmVzEg@mail.gmail.com>
References: <CAK0iFYyXQYP8VY5_hvJABa7F2gE_6sV3xsaz8U+3BLw=LmVzEg@mail.gmail.com>
Message-ID: <19e3ff89-645c-5e9e-8d0a-d000f1340add@treenet.co.nz>

On 27/06/18 09:51, Gordon Hsiao wrote:
> Assuming I allow a domain to pass in ACL, but deny it in my redirector,
> which one will work?
> 
> Also, assuming I deny a domain in squid.conf, but allow in in the
> redirector, which one will take precedence?
> 
> Will there be a difference for the above when peek+splice / peek+bump
> was used?
> 

Your questions are very generic. Please be specific about what exactly
you are configuring, showing your config would be best.

Amos


From squid3 at treenet.co.nz  Tue Jun 26 23:26:53 2018
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Wed, 27 Jun 2018 11:26:53 +1200
Subject: [squid-users] Adobe CC behing Squid
Message-ID: <db3b6d6a-eea0-971f-d824-8f17ffa72f3e@treenet.co.nz>

On 26/06/18 20:53, admin wrote:
> Hello Amos,
>
> Adobe Cloud starts and asks correctly  for proxy-authentification.
> Then it tries to connect and gets a timeout and tries and...
>
> In Access.log I only see a connect to Adobe.com:
> TCP_TUNNEL:HIER_DIRECT
>

Hmm, that sounds like the traffic is either a) not going to the proxy
like it should, or b) going inside the tunnel.

If (a) it could be a routing issue, or a bug in the ACC software.

If (b) the credentials are not relevant except to the CONNECT message.
Do those CONNECT messages you see in the log ever contain the required
credentials?


Amos


From capcoding at gmail.com  Wed Jun 27 04:09:32 2018
From: capcoding at gmail.com (Gordon Hsiao)
Date: Tue, 26 Jun 2018 23:09:32 -0500
Subject: [squid-users] can squid use dns server on random port(non-53)?
Message-ID: <CAK0iFYwwsQnRVYfgBRzVbtBj3Xr7t2N9RGq03rs-tQ9oaR=bZg@mail.gmail.com>

checked the manual it seems I can only set dnsserver with a new IP, is it
possible to make squid support non-standard DNS port, e.g. 5353?

Thanks,
Gordon
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20180626/bb0a7f96/attachment.htm>

From squid at bloms.de  Wed Jun 27 04:29:30 2018
From: squid at bloms.de (Dieter Bloms)
Date: Wed, 27 Jun 2018 06:29:30 +0200
Subject: [squid-users] can squid use dns server on random port(non-53)?
In-Reply-To: <CAK0iFYwwsQnRVYfgBRzVbtBj3Xr7t2N9RGq03rs-tQ9oaR=bZg@mail.gmail.com>
References: <CAK0iFYwwsQnRVYfgBRzVbtBj3Xr7t2N9RGq03rs-tQ9oaR=bZg@mail.gmail.com>
Message-ID: <20180627042930.gjq5f23ztrre3bnr@bloms.de>

Hello,

On Tue, Jun 26, Gordon Hsiao wrote:

> checked the manual it seems I can only set dnsserver with a new IP, is it
> possible to make squid support non-standard DNS port, e.g. 5353?

maybe you can use a dns resolver like unbound, dnscache, dnsmasq, ....
which can be configure to listen on localhost port 53, so only squid can
access it via localhost and no other servers.
These dns resolvers can be configure to use a non standard port like
5353 for the destination dns servers.

But in the past I've never seen a dns server listening on port 5353, so
maybe the setup is a little broken.


-- 
Regards

  Dieter

--
I do not get viruses because I do not use MS software.
If you use Outlook then please do not put my email address in your
address-book so that WHEN you get a virus it won't use my address in the
>From field.


From mika.ristimaki at gmail.com  Wed Jun 27 06:18:04 2018
From: mika.ristimaki at gmail.com (=?utf-8?Q?Mika_Ristim=C3=A4ki?=)
Date: Wed, 27 Jun 2018 09:18:04 +0300
Subject: [squid-users] Adobe CC behing Squid
In-Reply-To: <00d25824-a021-f2c0-9240-3ac700f95f9d@treenet.co.nz>
References: <1529957351000-0.post@n4.nabble.com>
 <00d25824-a021-f2c0-9240-3ac700f95f9d@treenet.co.nz>
Message-ID: <9910f939-179e-47cb-b51b-504a86f21678@Spark>

Hi,

IIRC Adobe CC connects to a HTTPS server in localhost. This seems to confirm it
https://helpx.adobe.com/creative-cloud/kb/proxy-authentication-support-creative-cloud.html#Unterst%C3%BCtzungf%C3%BCrPACDateien

>?In an enterprise environment, Creative Cloud Libraries must connect to?localhost?for the Libraries panel in applications to sync correctly.?Therefore, if you're using Libraries, set?localhost?and 127.0.0.1 to bypass the proxy server for the enterprise environment.

You need to make sure that connections to localhost do not go to the proxy.

-Mika
On 26 Jun 2018, 3.31 +0300, Amos Jeffries <squid3 at treenet.co.nz>, wrote:
> On 26/06/18 08:09, Verwaiser wrote:
> > Hello,
> > I'm trying to realize the Adobe CC Programs to work on workstations behind
> > our Squid (Debian 8, Squid 3.5). We are using user authentification to a
> > LDAP-Server.
> >
> > Is there anyone with experiences how to make this work?
>
> Question is:
>
> * do these applications use HTTP?
>
> The docs you reference seem to indicate the answer is yes.
>
>
> * what do you see happening?
>
>
> >
> > Adobe gives some informations:
> > https://helpx.adobe.com/de/creative-cloud/kb/proxy-authentication-support-creative-cloud.html
> >
> > But I don't exactly know if Squid is a proxy with "PAC-URL", but it seems to
> > me to be so.
>
> Please read the FAQ section on auto-configuration:
> <https://wiki.squid-cache.org/SquidFaq/ConfiguringBrowsers>
>
> >
> > Any ideas?
>
> Follow the Adobe instructions?
>
> They do not mention having to add any special bypass rules for the proxy
> to work. If your clients are allowed to use the proxy these applications
> should "just work".
>
> Amos
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20180627/9ff80276/attachment.htm>

From phackmann at gmail.com  Wed Jun 27 14:57:05 2018
From: phackmann at gmail.com (Paul Hackmann)
Date: Wed, 27 Jun 2018 09:57:05 -0500
Subject: [squid-users] Windows 10 Feature Updates not coming through
Message-ID: <CADQL5rPH-3G78ZoBwC5e4yLPT0yJxFDKEvotjqVUcoLjEkRDdA@mail.gmail.com>

Hello.  I can't figure out why, but I can get regular windows 10 updates
through the proxy without problem, but the larger feature updates (1803)
always fail to download.  I can do the windows 10 update assistant
manually, and that seems to work ok.  I'm not sure what I am missing.  Do I
have a problem with my configuration?  I am trying to do the download
through port 4120.

http_port 3120
http_port 4120 #intercept

cache_dir ufs /var/spool/squid 10000 16 256

refresh_pattern ^ftp:           1440    20%     10080
refresh_pattern ^gopher:        1440    0%      1440
refresh_pattern -i (/cgi-bin/|\?) 0     0%      0
refresh_pattern .               0       20%     4320

acl whitelist dstdomain "/etc/squid/whitelist.conf"
#acl deny_websites dstdomain "/etc/squid/deny_websites.conf"

acl windowsupdate dstdomain windowsupdate.microsoft.com
acl windowsupdate dstdomain .update.microsoft.com
acl windowsupdate dstdomain download.windowsupdate.com
acl windowsupdate dstdomain redir.metaservices.microsoft.com
acl windowsupdate dstdomain images.metaservices.microsoft.com
acl windowsupdate dstdomain c.microsoft.com
acl windowsupdate dstdomain www.download.windowsupdate.com
acl windowsupdate dstdomain wustat.windows.com
acl windowsupdate dstdomain crl.microsoft.com
acl windowsupdate dstdomain sls.microsoft.com
acl windowsupdate dstdomain productactivation.one.microsoft.com
acl windowsupdate dstdomain ntservicepack.microsoft.com
acl windowsupdate dstdomain .live.com
acl windowsupdate dstdomain .digicert.com
acl windowsupdate dstdomain .mp.microsoft.com
acl windowsupdate dstdomain .cms.msn.com

acl CONNECT method CONNECT
acl wuCONNECT dstdomain http://www.update.microsoft.com

range_offset_limit 10000 MB windowsupdate
maximum_object_size 10000 MB
quick_abort_min -1

auth_param basic program /usr/lib/squid/basic_ncsa_auth /etc/squid/passwd
auth_param basic children 6
auth_param basic realm Squid proxy-caching web server
auth_param basic credentialsttl 4 hours
auth_param basic casesensitive off

acl ncsa_users proxy_auth REQUIRED

#acl manager url_regex -i ^cache_object:// +i ^https?://[^/]+/squid-
internal-mgr/

#acl localhost src 127.0.0.1/32 ::1
#acl to_localhost dst 127.0.0.0/8 0.0.0.0/32 ::1

acl localnet src 10.0.0.0/8     # RFC 1918 possible internal network
acl localnet src 172.16.0.0/12  # RFC 1918 possible internal network
acl localnet src 192.168.0.0/16 # RFC 1918 possible internal network
acl localnet src fc00::/7       # RFC 4193 local private network range
acl localnet src fe80::/10      # RFC 4291 link-local (directly plugged)
machines

#acl http proto http
acl SSL_ports port 443
acl port_80 port 80
acl Safe_ports port 80          # http
acl Safe_ports port 21          # ftp
acl Safe_ports port 443         # https
acl Safe_ports port 70          # gopher
acl Safe_ports port 210         # wais
acl Safe_ports port 1025-65535  # unregistered ports
acl Safe_ports port 280         # http-mgmt
acl Safe_ports port 488         # gss-http
acl Safe_ports port 591         # filemaker
acl Safe_ports port 777         # multiling http

#list of computers that have access by ip address
acl allowed_clients src 192.168.0.9-192.168.0.45 192.168.0.53 192.168.0.65
192.168.0.83 192.168.0.90 192.168.0.91 192.168.0.179 192.168.0.186
192.168.0.220 192.168.0.221 192.168.0.244

acl portX myportname 4120
#ip addresses for 8x8.com webinar software
acl 8x8 dst 8.5.248.0/23 8.28.0.0/22 63.209.12.0/24 162.221.236.0/23
162.221.238.0/23 192.84.16.0/22

acl CONNECT method CONNECT

http_access allow CONNECT wuCONNECT localnet
http_access allow windowsupdate localnet

#rule allowing nonauthenticated users
#http_access allow http port_80 whitelist
http_access allow CONNECT SSL_ports whitelist

http_access deny !Safe_ports
http_access deny CONNECT !SSL_ports
http_access allow manager localhost
http_access deny manager

# domains in deny_websites are DENIED for everybody.
#http_access deny deny_websites

# domains in whitelist are ALLOWED for everybody
http_access allow whitelist

# 8x8.com ip addresses are Allowed for everybody
http_access allow 8x8

# port 4120 traffic is restricted to the above whitelisted domains
http_access deny portX

# otherwise; for port 3120 traffic ...

# only specific clients with whitelisted IPs can use the proxy ...
http_access deny !allowed_clients

# ... and must also login
http_access deny !ncsa_users

http_access allow localnet

http_access deny all

Thanks.

Paul
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20180627/e9398a74/attachment.htm>

From capcoding at gmail.com  Wed Jun 27 15:49:26 2018
From: capcoding at gmail.com (Gordon Hsiao)
Date: Wed, 27 Jun 2018 10:49:26 -0500
Subject: [squid-users] sslproxy_foreign_intermediate_certs -- where to
	locate a bundle
Message-ID: <CAK0iFYwZMvM5Qji5AGxbrOTErtMxO0tdYsc3UpZ9PqBOuWhP3Q@mail.gmail.com>

does it exist somewhere? Just notice this option in 3.5 but google does not
say any location I can fetch like the way a typical ca-bundle is.

Gordon
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20180627/91ca56d5/attachment.htm>

From capcoding at gmail.com  Wed Jun 27 16:55:29 2018
From: capcoding at gmail.com (Gordon Hsiao)
Date: Wed, 27 Jun 2018 11:55:29 -0500
Subject: [squid-users] http_port vs https_port
Message-ID: <CAK0iFYxX6_jYmE1HDsdSvoOf5_pbMEVoaTaVnbzH56ULjNi9NQ@mail.gmail.com>

Reading all the cfg options in Squid 3.5 I noticed http_port has lots of
SSL related options(which it should not), plus https_port is referring to
http_port for those options, should http_port have nothing to do with
ssl-specific options and those ssl-options could be better moved to
https_port section instead?

http://www.squid-cache.org/Versions/v3/3.5/cfgman/http_port.html
http://www.squid-cache.org/Versions/v3/3.5/cfgman/https_port.html

Gordon
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20180627/53c8530f/attachment.htm>

From rousskov at measurement-factory.com  Wed Jun 27 17:23:22 2018
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Wed, 27 Jun 2018 11:23:22 -0600
Subject: [squid-users] http_port vs https_port
In-Reply-To: <CAK0iFYxX6_jYmE1HDsdSvoOf5_pbMEVoaTaVnbzH56ULjNi9NQ@mail.gmail.com>
References: <CAK0iFYxX6_jYmE1HDsdSvoOf5_pbMEVoaTaVnbzH56ULjNi9NQ@mail.gmail.com>
Message-ID: <ac390312-1c93-627f-fb9a-5b2ff6a564f5@measurement-factory.com>

On 06/27/2018 10:55 AM, Gordon Hsiao wrote:
> Reading all the cfg options in Squid 3.5 I noticed http_port has lots of
> SSL related options(which it should not), plus https_port is referring
> to http_port for those options, should http_port have nothing to do with
> ssl-specific options and those ssl-options could be better moved to
> https_port section instead?

http_port uses SSL options when bumping HTTP CONNECT tunnels.

Alex.


From amit at xsinfosol.com  Wed Jun 27 17:55:22 2018
From: amit at xsinfosol.com (Amit Pasari - XS INFOSOL Inc. USA)
Date: Wed, 27 Jun 2018 23:25:22 +0530
Subject: [squid-users] Chrome 67 Issue with SSL Bump
In-Reply-To: <2862c2cf-4b97-bedf-4a14-8653cb991d37@xsinfosol.com>
References: <e0ef9bb7-507c-be47-334c-a1f72be8b0a3@xsinfosol.com>
 <5B326614.3030701@mathemainzel.info>
 <9DF2E539-F1C9-4C5E-B22B-243CCA0D6323@xsinfosol.com>
 <5B3272F6.7050500@mathemainzel.info>
 <076b5e23-3009-9a67-4abd-298bd487891c@xsinfosol.com>
 <2862c2cf-4b97-bedf-4a14-8653cb991d37@xsinfosol.com>
Message-ID: <9bde61e6-041f-21ae-1cb5-ebaa4e0841cc@xsinfosol.com>

On 6/27/18 11:20 PM, Amit Pasari - XS INFOSOL Inc. USA wrote:
> Dear Walter ,
>
> I use
>
> sslproxy_cert_sign_hash sha256
>
> and use a SHA-256  certificate
>
> The result is still the same .
>
> "NET::ERR_CERT_WEAK_SIGNATURE_ALGORITHM"
>
> Also one more thing , when i open yahoo.com with any of those 
> certificates in CHROME , the content of yahoo comes inline i,e without 
> any CSS etc ...
>
> One more strange thing i noticed , when i browse using Firefox , 
> safari , IE , all URLs are coming in squid/access.log where as when i 
> use CHROME only few IPs comes in access logs with CONNECT on 443 .
>
> I also noticed with using CHROME the below type of requests :
> POST 
> http://safebrowsing.googleusercontent.com/safebrowsing/clientreport/chrome-certs
>
>
> Amit
>
>
> On 6/26/18 11:25 PM, Amit Pasari - XS INFOSOL Inc. USA wrote:
>> Let me try the below solution , but if thats the case it shouldn't 
>> work with other browsers as well  , what i think is chrome is either 
>> not reading my cert or rejecting it .
>>
>> Unsure .
>>
>> Amit
>>
>> On 6/26/18 10:38 PM, Walter H. wrote:
>>> On 26.06.2018 19:03, Amit pasari wrote:
>>>> Dear Walter
>>>> I have tried with both SHA1 and SHA256 cert .
>>>>
>>>>
>>>> Sent from my iPhone
>>>>
>>>> On Jun 26, 2018, at 9:43 PM, Walter H. <Walter.H at mathemainzel.info 
>>>> <mailto:Walter.H at mathemainzel.info>> wrote:
>>>>
>>>>> On 26.06.2018 17:22, Amit Pasari - XS INFOSOL Inc. USA wrote:
>>>>>>
>>>>>> I am using squid in transparent mode . Everything working fine in 
>>>>>> Firefox and IE after i have imported the certificate in both the 
>>>>>> browser  , but in Chrome 67 version on Windows 10 i am facing the 
>>>>>> below issue
>>>>>>
>>>>>> NET::ERR_CERT_WEAK_SIGNATURE_ALGORITHM
>>>>>>
>>>>>> When i open https://facebook.com , https://linkedin.com etc .
>>>>>>
>>>>>> I am clueless on the same now .
>>>>>>
>>>>>> Amit
>>>>>>
>>>>> Have you generated a SHA1 or SHA-256 certificate?
>>>>>
>>>>> Walter
>>>>>
>>> can you try this:
>>>
>>> sslproxy_cert_sign_hash sha256
>>>
>>> and use a SHA-256  certificate
>>>
>>> Walter

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20180627/6c40b5d3/attachment.htm>

From squid3 at treenet.co.nz  Wed Jun 27 18:27:22 2018
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 28 Jun 2018 06:27:22 +1200
Subject: [squid-users] Chrome 67 Issue with SSL Bump
In-Reply-To: <9bde61e6-041f-21ae-1cb5-ebaa4e0841cc@xsinfosol.com>
References: <e0ef9bb7-507c-be47-334c-a1f72be8b0a3@xsinfosol.com>
 <5B326614.3030701@mathemainzel.info>
 <9DF2E539-F1C9-4C5E-B22B-243CCA0D6323@xsinfosol.com>
 <5B3272F6.7050500@mathemainzel.info>
 <076b5e23-3009-9a67-4abd-298bd487891c@xsinfosol.com>
 <2862c2cf-4b97-bedf-4a14-8653cb991d37@xsinfosol.com>
 <9bde61e6-041f-21ae-1cb5-ebaa4e0841cc@xsinfosol.com>
Message-ID: <5c5455de-c0bf-73a9-1eee-8d943c68f82f@treenet.co.nz>

On 28/06/18 05:55, Amit Pasari - XS INFOSOL Inc. USA wrote:
> On 6/27/18 11:20 PM, Amit Pasari - XS INFOSOL Inc. USA wrote:
>> Dear Walter ,
>>
>> I use
>>
>> sslproxy_cert_sign_hash sha256
>>
>> and use a SHA-256? certificate
>>
>> The result is still the same .
>> ?
>> "NET::ERR_CERT_WEAK_SIGNATURE_ALGORITHM"


Based on <https://bugs.chromium.org/p/chromium/issues/detail?id=655318>

v67 may have moved on to SHA-512 now, or this site be using SHA-386.


Is there any way you can debug *which* certificate in the certificate
chain is producing that error?
 It could be the server cert, or an intermediary, or the root CA.

Also, there are other uses of signatures in TLS/SSL that you could
check. eg the signature on serverHello messages. The error does point at
certs, but all Browsers have a history of wrongly re-using error
messages for only slightly related things at times if their translators
did not produce new texts fast enough for their release cycle.


>>
>> Also one more thing , when i open yahoo.com with any of those
>> certificates in CHROME , the content of yahoo comes inline i,e without
>> any CSS etc ...
>>

This may be a side effect of the same issue affecting separate
connections those background objects are fetched over. OR, it could e
something completely unrelated. They are not use-visible so error
messages not as clearly "in your face".
 Either way concentrate on one problem at a time.


>> One more strange thing i noticed , when i browse using Firefox ,
>> safari , IE , all URLs are coming in squid/access.log where as when i
>> use CHROME only few IPs comes in access logs with CONNECT on 443 .


Not strange at all. Different browsers/clients do different things. You
only get the decrypted messages if you successfully decrypted them.


>>
>> I also noticed with using CHROME the below type of requests :
>> POST
>> http://safebrowsing.googleusercontent.com/safebrowsing/clientreport/chrome-certs
>>

I suggest you look that domain and/or URL up. What its used for impacts
your ability to perform SSL-Bump.

Amos


From squid3 at treenet.co.nz  Wed Jun 27 18:58:27 2018
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 28 Jun 2018 06:58:27 +1200
Subject: [squid-users] sslproxy_foreign_intermediate_certs -- where to
 locate a bundle
In-Reply-To: <CAK0iFYwZMvM5Qji5AGxbrOTErtMxO0tdYsc3UpZ9PqBOuWhP3Q@mail.gmail.com>
References: <CAK0iFYwZMvM5Qji5AGxbrOTErtMxO0tdYsc3UpZ9PqBOuWhP3Q@mail.gmail.com>
Message-ID: <19037061-542e-4d06-61e1-03dcb6f777d5@treenet.co.nz>

On 28/06/18 03:49, Gordon Hsiao wrote:
> does it exist somewhere? Just notice this option in 3.5 but google does
> not say any location I can fetch like the way a typical ca-bundle is.
> 

IIRC, Yuri published the bundle they had accumulated a while back. The
link seems not to be working now though.


You can easily accumulate your own if you like. Simply by watching for
reports about sites/services not working do to certificate verification
errors. Check that it is missing an intermediate rather than other
TLS/SSL errors. Manually download the missing intermediate cert and
append it in PEM format to your bundle file.


However, I recommend just upgrading to Squid-4. That version has many
crypto related fixes that make life easier - including the ability to
auto-download most of these missing intermediate certs. The directive
may still be needed for some servers that do very weird things, but not
nearly as many as Squid-3 needs attending to.


Amos


From squid3 at treenet.co.nz  Wed Jun 27 19:06:14 2018
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 28 Jun 2018 07:06:14 +1200
Subject: [squid-users] can squid use dns server on random port(non-53)?
In-Reply-To: <20180627042930.gjq5f23ztrre3bnr@bloms.de>
References: <CAK0iFYwwsQnRVYfgBRzVbtBj3Xr7t2N9RGq03rs-tQ9oaR=bZg@mail.gmail.com>
 <20180627042930.gjq5f23ztrre3bnr@bloms.de>
Message-ID: <4563f027-a210-deeb-df82-f5a238887410@treenet.co.nz>

On 27/06/18 16:29, Dieter Bloms wrote:
> Hello,
> 
> On Tue, Jun 26, Gordon Hsiao wrote:
> 
>> checked the manual it seems I can only set dnsserver with a new IP, is it
>> possible to make squid support non-standard DNS port, e.g. 5353?

Squid only contains a minimal stub resolver. It requires a recursive
resolver on port 53 (UDP *and* TCP) to do the actual DNS resolving and
any fancy things like strange ports.

So what Dieter said:

> 
> maybe you can use a dns resolver like unbound, dnscache, dnsmasq, ....
> which can be configure to listen on localhost port 53, so only squid can
> access it via localhost and no other servers.
> These dns resolvers can be configure to use a non standard port like
> 5353 for the destination dns servers.
> 
> But in the past I've never seen a dns server listening on port 5353, so
> maybe the setup is a little broken.
> 


Amos


From squid at mail.verwaiser.de  Wed Jun 27 19:06:22 2018
From: squid at mail.verwaiser.de (Verwaiser)
Date: Wed, 27 Jun 2018 12:06:22 -0700 (MST)
Subject: [squid-users] Adobe CC behing Squid
In-Reply-To: <1529957351000-0.post@n4.nabble.com>
References: <1529957351000-0.post@n4.nabble.com>
Message-ID: <1530126382227-0.post@n4.nabble.com>

Hello,
what would be the right way to implement the authentification bypass list
linked from adobe:
https://helpx.adobe.com/content/dam/help/attachments/Creative_Cloud_for_enterprise_Service_Endpoints.pdf

I can write the list into a file, ok, but how can I setup the acl for
correct bypassig all the adresses from this list?
Is the "allways_direct" acl right? Should I place it before the LDAP
authentication part in squid.conf? Is there more to work on?

Holger



--
Sent from: http://squid-web-proxy-cache.1019090.n4.nabble.com/Squid-Users-f1019091.html


From squid3 at treenet.co.nz  Wed Jun 27 19:31:59 2018
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 28 Jun 2018 07:31:59 +1200
Subject: [squid-users] Windows 10 Feature Updates not coming through
In-Reply-To: <CADQL5rPH-3G78ZoBwC5e4yLPT0yJxFDKEvotjqVUcoLjEkRDdA@mail.gmail.com>
References: <CADQL5rPH-3G78ZoBwC5e4yLPT0yJxFDKEvotjqVUcoLjEkRDdA@mail.gmail.com>
Message-ID: <14df8c87-3ed7-1160-bd32-52b51d58caf4@treenet.co.nz>

On 28/06/18 02:57, Paul Hackmann wrote:
> Hello.? I can't figure out why, but I can get regular windows 10 updates
> through the proxy without problem, but the larger feature updates (1803)
> always fail to download.

Have you refreshed your knowledge of what the relevant config settings
are and what they do?
<https://wiki.squid-cache.org/SquidFaq/WindowsUpdate>

NP: the FAQ is heavy on what they do and values are indications only -
so that as these things change you can know what to tweak.



>? I can do the windows 10 update assistant
> manually, and that seems to work ok.? I'm not sure what I am missing.?
> Do I have a problem with my configuration?? I am trying to do the
> download through port 4120.
> 
> http_port 3120
> http_port 4120 #intercept
> 
> cache_dir ufs /var/spool/squid 10000 16 256
...

> 
> acl CONNECT method CONNECT
> acl wuCONNECT dstdomain http://www.update.microsoft.com

This should be a FQDN or domain wildcard, not a URL. Remove the
'http://' portion.

IIRC, this wuCONNECT stuff is for CONNECT messages where the reverse-DNS
is needing to match the exact call-home sever WU uses/used. With this
broken it cannot do raw-IP fetches it sometimes needs.
 The "allow windowsupdate localnet" will be allowing CONNECT things this
does not match for most fetches.




> 
> 
> range_offset_limit 10000 MB windowsupdate

So very, very large files (ie up to 9GB) will download the *entire*
object just to fetch and deliver the final, say 1MB (or worse 32KB) of
data to the client.

This limit should be much smaller IMO. Each range segment that the WU
downloader breaks the update into causes a new full copy to be fetched.
Wasting hundreds of multiples of 100-200 MB might be (relatively)
acceptable. But many _thousands_ of multiples of 10 GB, not so much.



Other than that I'm not seeing anything particularly notable about your
config. It may be related to how the D/L is being done. For example, if
it uses the Win10 P2P functionality then its unlikely to be going over
HTTP as the proxy requires.

Any way you can find out if there is a new domain being used for these
Win10 features?
 The list in the FAQ page was last updated for Win8.0 IIRC, so there may
be something needing adding there.


Amos


From squid3 at treenet.co.nz  Wed Jun 27 19:56:46 2018
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 28 Jun 2018 07:56:46 +1200
Subject: [squid-users] Adobe CC behing Squid
In-Reply-To: <1530126382227-0.post@n4.nabble.com>
References: <1529957351000-0.post@n4.nabble.com>
 <1530126382227-0.post@n4.nabble.com>
Message-ID: <67a5f1c5-7b7a-9a6a-995f-4589640691d6@treenet.co.nz>

On 28/06/18 07:06, Verwaiser wrote:
> Hello,
> what would be the right way to implement the authentification bypass list
> linked from adobe:
> https://helpx.adobe.com/content/dam/help/attachments/Creative_Cloud_for_enterprise_Service_Endpoints.pdf
> 

Ouch. Rather a lot of domain names and explicitly states that it is
incomplete.

Some of them are *extremely* popular (eg Twitter, Google Maps, Google
Play Store). WTF why does ACC need Google Maps access?


Maybe looking for a User-Agent header string matching the tools that
break will narrow it down to not allowing just anyone access to all
those services.


> I can write the list into a file, ok, but how can I setup the acl for
> correct bypassig all the adresses from this list?
> Is the "allways_direct" acl right?

No. 'always_direct allow' means "dont use any cache_peer for this request".

There is no "bypass" directive. Every directive that you have configured
a need for auth to happen needs adjusting such that it also works
without that auth requirement when your new ACL(s) match the transaction.


> Should I place it before the LDAP
> authentication part in squid.conf?

Yes. For every directive which currently requires an auth related test,
place a test which matches the 'bypass' ACL first, OR make it so that
you don't have to require the auth details at that point.
 NP: The latest Squid versions note ACL type which can be useful here to
test username (the note named 'user' contains the username) without
requiring that it exists nor triggering auth.


The 'best practice' design is to configure http_access with an ordered
structure like so:

 # The default / recommended security checks at the top
 # ending at that default line "INSERT YOUR CUSTOM RULES BELOW HERE."

 # custom allow/deny rules that do not need auth

 # authenticate
 http_access deny !login

 # custom allow/deny rules that need auth credentials

 # and finally ...
 http_access deny all


The rest of your settings can assume that auth has taken place already
(*if* necessary) and not re-test it themselves.



> Is there more to work on?

Everything which uses an authentication, username, or group ACL test
needs looking at to see whether a bypass is needed.


Amos


From donmuller22 at outlook.com  Wed Jun 27 20:21:54 2018
From: donmuller22 at outlook.com (Donald Muller)
Date: Wed, 27 Jun 2018 20:21:54 +0000
Subject: [squid-users] Setting up a Whitelist
Message-ID: <CY1PR16MB04594FFDCBB8EEF5D7ECBF41B6480@CY1PR16MB0459.namprd16.prod.outlook.com>

Hi,

Don't know if what I want to do is even possible but here is the situation. I have Squid set up on my QNAP NAS. It is running fine. I am using it with the blacklist and sites get blocked as they should. However there a number of sites that I do not want blacklisted so I thought I'd set up a whitelist for them. What I did was to add an include statement to the squid.conf file. The included file has the directives for the whitelist.

Here are my config files.


Squid.conf

# The user name and group name Squid will operate as
cache_effective_user httpdusr
#cache_effective_group everyone

#
# Recommended minimum configuration:
#
# Auth Method
#
#auth_param basic program /usr/local/squid/libexec/basic_ncsa_auth /usr/local/squid/etc/ac
#auth_param basic program /usr/local/squid/libexec/basic_pam_auth
#auth_param basic program /usr/local/squid/libexec/basic_getpwnam_auth
auth_param basic program /usr/local/squid/etc/auth.py
auth_param basic children 5
auth_param basic realm QNAP Proxy Server
auth_param basic credentialsttl 2 hours

# Example rule allowing access from your local networks.
# Adapt to list your (internal) IP networks from where browsing
# should be allowed
acl localnet src 10.0.0.0/8              # RFC1918 possible internal network
acl localnet src 172.16.0.0/12       # RFC1918 possible internal network
acl localnet src 192.168.0.0/16    # RFC1918 possible internal network
acl localnet src fc00::/7       # RFC 4193 local private network range
acl localnet src fe80::/10      # RFC 4291 link-local (directly plugged) machines

acl allnet src all                  # All Net

acl SSL_ports port 443
acl Safe_ports port 80                    # http
acl Safe_ports port 21                    # ftp
acl Safe_ports port 443                  # https
acl Safe_ports port 70                    # gopher
acl Safe_ports port 210                  # wais
acl Safe_ports port 1025-65535  # unregistered ports
acl Safe_ports port 280                  # http-mgmt
acl Safe_ports port 488                  # gss-http
acl Safe_ports port 591                  # filemaker
acl Safe_ports port 777                  # multiling http
acl CONNECT method CONNECT
include /usr/local/squid/etc/acl.conf
include /share/CACHEDEV1_DATA/UserData/Configs/Proxy/whitelist.conf       <---------- I added this line


acl snmppublic snmp_community public
snmp_port 3401
snmp_access allow snmppublic all


acl ncsa_users proxy_auth REQUIRED
external_acl_type unix_group %LOGIN /usr/local/squid/libexec/ext_unix_group_acl -p
acl group_administrators external unix_group administrators
acl nas_user proxy_auth admin
#
# Recommended minimum Access Permission configuration:
#
# Only allow cachemgr access from localhost
http_access allow manager localhost
http_access deny manager

#Set the acl http_access using acl.conf
#DO NOT MODIFY THIS PART

# Deny requests to certain unsafe ports
http_access deny !Safe_ports

# Deny CONNECT to other than secure SSL ports
http_access deny CONNECT !SSL_ports

# We strongly recommend the following be uncommented to protect innocent
# web applications running on the proxy server who think the only
# one who can access services on "localhost" is a local user
#http_access deny to_localhost

#
# INSERT YOUR OWN RULE(S) HERE TO ALLOW ACCESS FROM YOUR CLIENTS
#


include /usr/local/squid/etc/acl_http.conf
#http_access allow allnet ncsa_users
#http_access allow allnet group_administrators
#http_access allow allnet nas_user
http_access allow allnet
#http_access deny allnet
# And finally deny all other access to this proxy

http_access deny all

# Squid normally listens to port 3128
http_port 3128 #Forward
#http_port 3129 intercept #Transparent

# We recommend you to use at least the following line.
# Uncomment and adjust the following to add a disk cache directory.
cache_dir ufs /share/CACHEDEV1_DATA/.qpkg/ProxyServer/opt/var/cache 10240 16 256

cache_mem 64 MB
maximum_object_size_in_memory 16 KB

minimum_object_size 0 KB
maximum_object_size 2097152 KB
cache_swap_low 90
cache_swap_high 95

# Leave coredumps in the first cache dir
coredump_dir /share/CACHEDEV1_DATA/.qpkg/ProxyServer/opt/var/cache

#access_log /share/CACHEDEV1_DATA/.qpkg/ProxyServer/opt/var/logs/access.log
#####access_log udp://127.0.0.1:514
access_log none
#cache_log /usr/local/squid/var/logs/cache.log
#cache_store_log /usr/local/squid/var/squid/logs/store.log

#
mime_table /usr/local/squid/etc/mime.conf
pid_filename /usr/local/squid/var/run/squid.pid
diskd_program /usr/local/squid/libexec/diskd
unlinkd_program /usr/local/squid/libexec/unlinkd
icon_directory /usr/local/squid/share/icons
err_page_stylesheet /usr/local/squid/etc/errorpage.css
error_default_language en-us
error_directory /usr/local/squid/share/errors/en-us

icap_enable off
icap_send_client_ip on
icap_send_client_username on
icap_client_username_header X-Authenticated-User
icap_service service_req reqmod_precache bypass=1 icap://127.0.0.1:1344/squidclamav
adaptation_access service_req allow all
icap_service service_resp respmod_precache bypass=1 icap://127.0.0.1:1344/squidclamav
adaptation_access service_resp allow all
url_rewrite_children 50
url_rewrite_program /usr/local/squid/squidGuard/bin/squidGuard -c  /usr/local/squid/squidGuard/conf/squidGuard.conf

# Add any of your own refresh_pattern entries above these.
refresh_pattern ^ftp:                     1440       20%        10080
refresh_pattern ^gopher:            1440       0%          1440
refresh_pattern -i (/cgi-bin/|\?) 0             0%          0
refresh_pattern .                             0              20%        4320


whitelist.conf

acl whitelist dstdomain "/share/CACHEDEV1_DATA/UserData/Configs/Proxy/whitelist.txt"
http_access allow whitelist

whitelist.txt

.mohegansun.com
.youtube.com


Thanks
Don
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20180627/f5d09c30/attachment.htm>

From squid3 at treenet.co.nz  Wed Jun 27 20:59:07 2018
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 28 Jun 2018 08:59:07 +1200
Subject: [squid-users] Setting up a Whitelist
In-Reply-To: <CY1PR16MB04594FFDCBB8EEF5D7ECBF41B6480@CY1PR16MB0459.namprd16.prod.outlook.com>
References: <CY1PR16MB04594FFDCBB8EEF5D7ECBF41B6480@CY1PR16MB0459.namprd16.prod.outlook.com>
Message-ID: <99b0099f-55ff-f152-e7c1-22245e6e35b4@treenet.co.nz>

On 28/06/18 08:21, Donald Muller wrote:
> Hi,
> 
> ?
> 
> Don?t know if what I want to do is even possible but here is the
> situation. I have Squid set up on my QNAP NAS. It is running fine. I am
> using it with the blacklist and sites get blocked as they should.
> However there a number of sites that I do not want blacklisted so I
> thought I?d set up a whitelist for them. What I did was to add an
> include statement to the squid.conf file. The included file has the
> directives for the whitelist.
> 
> ?
> 
> Here are my config files.
> 
> ?
> 
> ?
> 
> Squid.conf
> 
> ?
> 
> # The user name and group name Squid will operate as
> 
> cache_effective_user httpdusr
> 

The above username requires read access to the included file *and* any
other files which it instructs Squid to load.

That access may be granted though group access to the file. IF the above
member is part of a permitted group. Be careful, Do Not assign Squid
into root group nor any equivalent on the machine.


...
> ?
> 
> acl allnet src all????????????????? # All Net
> 

Why?
 you are not doing anything like deny_info which might need "allnet"

Using the built-in "all" ACL would be simpler.

> 
> include /usr/local/squid/etc/acl.conf
> 
> include
> /share/CACHEDEV1_DATA/UserData/Configs/Proxy/whitelist.conf??????
> ?-------- I added this line
> 

The only thing to be aware of is order dependence. Squid loads and
operates as if the contents of these files were copy-and-pasted exactly
at the line where the include directive is.

That means any directives like http_access which contain order-specific
behaviours retain those behaviours between files in the specific order
of the include lines.

So, if acl.conf contains "http_access deny blacklist" and whitelist.conf
contains "http_access allow whitelist" then:
 a) blacklist is *still* denying requests before whitelist is even tested.
 b) whitelist.conf is (only) adding a bypass of all the
default/recommended squid.conf security lines

I'm pointing out (b) because you should really only place custom rules
(especially http_access related ones) at the point in squid.conf labeled
"INSERT YOUR OWN RULE(S) HERE".

You have not stated whether you are trying to whitelist against entries
in the blakclist, or against the proxies default security rules to
prevent unsafe behaviour (ie spam email using the proxy as a relay,
non-HTTPS tuynnels).
 If you want the former; then the includes need to be done the other way
around (whitelist.conf include first, then acl.conf).
 If you want the latter; then you have it now.


...

> 
> include /usr/local/squid/etc/acl_http.conf
> 
> #http_access allow allnet ncsa_users
> 
> #http_access allow allnet group_administrators
> 
> #http_access allow allnet nas_user

NP: Placing "all" on a line with other ACL checks is a hack to prevent
authentication process being initiated by lines if the credentials are
known but not allowed certain access. It only works if the "all" is
placed at the RHS end of lines.
 So "allnet" is pointless on the above.


> 
> http_access allow allnet
> 
> #http_access deny allnet
> 
> # And finally deny all other access to this proxy
> 

But "allnet" was defined as "all". Which overrides this safety net
config line and makes your proxy an open-proxy by default.
 That would be clearer if you had used "all" instead of custom "allnet".



> #
> 
> mime_table /usr/local/squid/etc/mime.conf
> 
> pid_filename /usr/local/squid/var/run/squid.pid
> 
> diskd_program /usr/local/squid/libexec/diskd
> 
> unlinkd_program /usr/local/squid/libexec/unlinkd
> 
> icon_directory /usr/local/squid/share/icons
> 
> err_page_stylesheet /usr/local/squid/etc/errorpage.css

None of the above lines should be necessary. If you are custom building
Squid it should be built with ./configure options setting defaults
appropriate for the OS its going to run on.
You only need these squid.conf directives if you have one or a few files
in really weird placement unusual for the OS.

Same for any directive which is setting default values. You can simplify
the config a huge amount by removing them entirely these days.
(Squid-2.x needed them, Squid-3.x does not).


> whitelist.conf
> 
> ?
> 
> acl whitelist dstdomain
> "/share/CACHEDEV1_DATA/UserData/Configs/Proxy/whitelist.txt"
> 
> http_access allow whitelist
> 


Amos


From capcoding at gmail.com  Wed Jun 27 21:16:43 2018
From: capcoding at gmail.com (Gordon Hsiao)
Date: Wed, 27 Jun 2018 16:16:43 -0500
Subject: [squid-users] can squid use dns server on random port(non-53)?
In-Reply-To: <mailman.9523.1530127922.4557.squid-users@lists.squid-cache.org>
References: <mailman.9523.1530127922.4557.squid-users@lists.squid-cache.org>
Message-ID: <CAK0iFYzqB+9uEyhh3NYY7O21_r=2zY3SS21Fva3TNMSGcuzuMQ@mail.gmail.com>

Date: Thu, 28 Jun 2018 07:06:14 +1200
> From: Amos Jeffries <squid3 at treenet.co.nz>
> To: squid-users at lists.squid-cache.org
> Subject: Re: [squid-users] can squid use dns server on random
>         port(non-53)?
> Message-ID: <4563f027-a210-deeb-df82-f5a238887410 at treenet.co.nz>
> Content-Type: text/plain; charset=utf-8
>
> On 27/06/18 16:29, Dieter Bloms wrote:
> > Hello,
> >
> > On Tue, Jun 26, Gordon Hsiao wrote:
> >
> >> checked the manual it seems I can only set dnsserver with a new IP, is
> it
> >> possible to make squid support non-standard DNS port, e.g. 5353?
>
> Squid only contains a minimal stub resolver. It requires a recursive
> resolver on port 53 (UDP *and* TCP) to do the actual DNS resolving and
> any fancy things like strange ports.
>
> So what Dieter said:
>
> >
> > maybe you can use a dns resolver like unbound, dnscache, dnsmasq, ....
> > which can be configure to listen on localhost port 53, so only squid can
> > access it via localhost and no other servers.
> > These dns resolvers can be configure to use a non standard port like
> > 5353 for the destination dns servers.
> >
> > But in the past I've never seen a dns server listening on port 5353, so
> > maybe the setup is a little broken.
> >
>
>
> Amos
>
>
> I agree it's a bit unusual, but adding a nameserver port option will be
nice if the changes are not intrusive.

Thanks,
Gordon
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20180627/d5bd2004/attachment.htm>

From webmaster at squidblacklist.org  Wed Jun 27 21:28:53 2018
From: webmaster at squidblacklist.org (Benjamin E. Nichols)
Date: Wed, 27 Jun 2018 16:28:53 -0500
Subject: [squid-users] can squid use dns server on random port(non-53)?
In-Reply-To: <CAK0iFYzqB+9uEyhh3NYY7O21_r=2zY3SS21Fva3TNMSGcuzuMQ@mail.gmail.com>
References: <mailman.9523.1530127922.4557.squid-users@lists.squid-cache.org>
 <CAK0iFYzqB+9uEyhh3NYY7O21_r=2zY3SS21Fva3TNMSGcuzuMQ@mail.gmail.com>
Message-ID: <b9b6264a-ae6c-87bd-58d3-f4169486e1b3@squidblacklist.org>

This is actually standard practice, it is very easy and common for 
administrators to configure their firewalls to redirect all 53 tcp/udp 
requests to a specific host to prevent those people and/or malicious 
applications which may be smart enough to change their dns server 
settings in an attempt to bypass a dns based filtering solution.

A solution to your problem would seem obvious to some but, I think that 
you may consider redirecting all requests to udp/tcp 53 from the host 
running Squid to your intended destination port using firewall 
rules.ssentially, you use a firewall to forward requests destined for 
port 53 to whatever port you want. ( yes? you can do this without 
forwarding to a specific host )

I hope that helps.

-- 
Signed,

Benjamin E. Nichols
Founder & Chief Architect
1-(405)-301-9516
http://www.squidblacklist.org



From squid3 at treenet.co.nz  Wed Jun 27 21:43:05 2018
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 28 Jun 2018 09:43:05 +1200
Subject: [squid-users] can squid use dns server on random port(non-53)?
In-Reply-To: <CAK0iFYzqB+9uEyhh3NYY7O21_r=2zY3SS21Fva3TNMSGcuzuMQ@mail.gmail.com>
References: <mailman.9523.1530127922.4557.squid-users@lists.squid-cache.org>
 <CAK0iFYzqB+9uEyhh3NYY7O21_r=2zY3SS21Fva3TNMSGcuzuMQ@mail.gmail.com>
Message-ID: <f69cbd5d-a3f2-bf09-1dd5-206ad6c54c88@treenet.co.nz>

On 28/06/18 09:16, Gordon Hsiao wrote:>
> I agree it's a bit unusual, but adding a nameserver port option will be
> nice if the changes are not intrusive.

So what protocol is used on this non-53 port?

How does "HTTP" sound? yes DNS-over-X is a thing these days and only
port 53 has the assurance that DNS protocol is actually being used.

This is part of why OpenBSD actively *removed* their custom feature of
supporting ports in /etc/resolv.conf "nameserver" setting (which this
directive is based on).

HTH
Amos


From capcoding at gmail.com  Wed Jun 27 22:00:56 2018
From: capcoding at gmail.com (Gordon Hsiao)
Date: Wed, 27 Jun 2018 17:00:56 -0500
Subject: [squid-users] dns_packet_max
Message-ID: <CAK0iFYz7=fMr48Cjd0vC0Msw+vCDSHMKaBXuHaZ8c7vzegOUcA@mail.gmail.com>

Still reading all the options, noticed dns_packet_max is off by default. My
squid uses dnsmasq, that has EDNS on by default and it "defaults to 4096,
which is the RFC5625-recommended size"

In this case what will happen then? dnsmasq may receive EDNS up to 4K,
which squid by default only takes 512Byte.
http://www.squid-cache.org/Versions/v3/3.5/cfgman/dns_packet_max.html
warns some older resolver does not like EDNS, but dnsmasq has this feature
on by default...

Thinking about setting up "dns_packet_max 4096" and see what happens...

Gordon
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20180627/032f4977/attachment.htm>

From rousskov at measurement-factory.com  Wed Jun 27 22:12:34 2018
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Wed, 27 Jun 2018 16:12:34 -0600
Subject: [squid-users] can squid use dns server on random port(non-53)?
In-Reply-To: <CAK0iFYzqB+9uEyhh3NYY7O21_r=2zY3SS21Fva3TNMSGcuzuMQ@mail.gmail.com>
References: <mailman.9523.1530127922.4557.squid-users@lists.squid-cache.org>
 <CAK0iFYzqB+9uEyhh3NYY7O21_r=2zY3SS21Fva3TNMSGcuzuMQ@mail.gmail.com>
Message-ID: <7971746d-b1e7-1f7c-d9c4-914ea25a49d3@measurement-factory.com>

On 06/27/2018 03:16 PM, Gordon Hsiao wrote:

> adding a nameserver port option will be nice if the changes are not intrusive.

Agreed. There are legitimate use cases for custom DNS ports. Just
because somebody might misuse a custom DNS port does not mean it should
not be supported (port 53 itself has been abused a lot).

Alex.


From eliezer at ngtech.co.il  Wed Jun 27 22:23:51 2018
From: eliezer at ngtech.co.il (Eliezer Croitoru)
Date: Thu, 28 Jun 2018 01:23:51 +0300
Subject: [squid-users] Adobe CC behing Squid
In-Reply-To: <67a5f1c5-7b7a-9a6a-995f-4589640691d6@treenet.co.nz>
References: <1529957351000-0.post@n4.nabble.com>
 <1530126382227-0.post@n4.nabble.com>
 <67a5f1c5-7b7a-9a6a-995f-4589640691d6@treenet.co.nz>
Message-ID: <0182bd1643814e996a07155faa9a258d@ngtech.co.il>

Hey Amos,

Today in many environments there is a very wide usage of ON-LINE 
"libraries" since...
the server or a cache node is just "2 meters" from the developer.
(Picture the nearby Internet BOX being pointed as "This is the 
Internet")
For me a 1MB file is still seems like too much for an Android APP in 
many case but
the world is changing and a kernel of more then 1MB is embedded in 
everyday devices around the globe.
I used to have huge disks for 80MB but today the in the same disk size 
you can store TB's of data(20+++).
I am sure that it's a global issue but the demand for traffic and 
on-line content is rising.

Just 10 years ago I had to have a huge wall filled with books to do 
little research but today I have a local DB
which contains literally rooms filled with books and is searchable.

I believe that the admin should understand a bit http\https to allow all 
these.
The next step is Google ROOT CA but... SSL-BUMP bumped everybody so not 
only Google and FaceBook have their own ROOT CA.

This thread proves that there are out-there admins that think and ask 
which makes me be happy.
It means that stupidity has not spread to some places like this list.

Eliezer


On 2018-06-27 22:56, Amos Jeffries wrote:
> On 28/06/18 07:06, Verwaiser wrote:
>> Hello,
>> what would be the right way to implement the authentification bypass 
>> list
>> linked from adobe:
>> https://helpx.adobe.com/content/dam/help/attachments/Creative_Cloud_for_enterprise_Service_Endpoints.pdf
>> 
> 
> Ouch. Rather a lot of domain names and explicitly states that it is
> incomplete.
> 
> Some of them are *extremely* popular (eg Twitter, Google Maps, Google
> Play Store). WTF why does ACC need Google Maps access?
> 
> 
> Maybe looking for a User-Agent header string matching the tools that
> break will narrow it down to not allowing just anyone access to all
> those services.
> 
> 
>> I can write the list into a file, ok, but how can I setup the acl for
>> correct bypassig all the adresses from this list?
>> Is the "allways_direct" acl right?
> 
> No. 'always_direct allow' means "dont use any cache_peer for this 
> request".
> 
> There is no "bypass" directive. Every directive that you have 
> configured
> a need for auth to happen needs adjusting such that it also works
> without that auth requirement when your new ACL(s) match the 
> transaction.
> 
> 
>> Should I place it before the LDAP
>> authentication part in squid.conf?
> 
> Yes. For every directive which currently requires an auth related test,
> place a test which matches the 'bypass' ACL first, OR make it so that
> you don't have to require the auth details at that point.
>  NP: The latest Squid versions note ACL type which can be useful here 
> to
> test username (the note named 'user' contains the username) without
> requiring that it exists nor triggering auth.
> 
> 
> The 'best practice' design is to configure http_access with an ordered
> structure like so:
> 
>  # The default / recommended security checks at the top
>  # ending at that default line "INSERT YOUR CUSTOM RULES BELOW HERE."
> 
>  # custom allow/deny rules that do not need auth
> 
>  # authenticate
>  http_access deny !login
> 
>  # custom allow/deny rules that need auth credentials
> 
>  # and finally ...
>  http_access deny all
> 
> 
> The rest of your settings can assume that auth has taken place already
> (*if* necessary) and not re-test it themselves.
> 
> 
> 
>> Is there more to work on?
> 
> Everything which uses an authentication, username, or group ACL test
> needs looking at to see whether a bypass is needed.
> 
> 
> Amos
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users

-- 
----
Eliezer Croitoru
Linux System Administrator
Mobile: +972-5-28704261
Email: eliezer at ngtech.co.il


From squid3 at treenet.co.nz  Wed Jun 27 22:33:12 2018
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 28 Jun 2018 10:33:12 +1200
Subject: [squid-users] dns_packet_max
In-Reply-To: <CAK0iFYz7=fMr48Cjd0vC0Msw+vCDSHMKaBXuHaZ8c7vzegOUcA@mail.gmail.com>
References: <CAK0iFYz7=fMr48Cjd0vC0Msw+vCDSHMKaBXuHaZ8c7vzegOUcA@mail.gmail.com>
Message-ID: <f3bee8d3-c480-5a10-320d-e0c28404f560@treenet.co.nz>

On 28/06/18 10:00, Gordon Hsiao wrote:
> Still reading all the options, noticed dns_packet_max is off by default.
> My squid uses dnsmasq, that has EDNS on by default and it "defaults to
> 4096, which is the RFC5625-recommended size"
> 
> In this case what will happen then? dnsmasq may receive EDNS up to 4K,
> which squid by default only takes
> 512Byte.??http://www.squid-cache.org/Versions/v3/3.5/cfgman/dns_packet_max.html?
> warns some older resolver does not like EDNS, but dnsmasq has this
> feature on by default...

That being about the external dnsmasq<->Internet behaviour should not
affect Squid. Though I'm surprised they did not hit the same problems we
did (see below).

The connection between Squid and the dnsmasq should always use the
traditional DNS fallback of TCP/53 if UDP/53 packets are not large
enough for a full response. That remains true even if an EDNS message
from Squid makes larger than 512 byte UDP packets be possible.


> 
> Thinking about setting up "dns_packet_max 4096" and see what happens...
> 

It worked fine for me when I added EDNS support to Squid. But others
reported that EDNS usage could crash their home routers. Since one of
the Squid use-cases is being an appliance used in residential situations
to limit upstream bandwidth we could not enable it by default.


Note paragraph #3 of that directives documentation about JumboGram
support at the network level. I suspect it was bugs in that TCP/IP
feature which was crashing peoples routers when 1500+ byte replies were
attempted.

Amos


From capcoding at gmail.com  Wed Jun 27 23:04:36 2018
From: capcoding at gmail.com (Gordon Hsiao)
Date: Wed, 27 Jun 2018 18:04:36 -0500
Subject: [squid-users] http_port vs https_port (Alex Rousskov)
In-Reply-To: <mailman.9520.1530122124.4557.squid-users@lists.squid-cache.org>
References: <mailman.9520.1530122124.4557.squid-users@lists.squid-cache.org>
Message-ID: <CAK0iFYyv6Di7STKS5APiu=Tjx5egSMgF1KRXoT+hyDKyg6d5nw@mail.gmail.com>

>
> Date: Wed, 27 Jun 2018 11:55:29 -0500
> From: Gordon Hsiao <capcoding at gmail.com>
> To: squid-users at lists.squid-cache.org
> Subject: [squid-users] http_port vs https_port
> Message-ID:
>         <
> CAK0iFYxX6_jYmE1HDsdSvoOf5_pbMEVoaTaVnbzH56ULjNi9NQ at mail.gmail.com>
> Content-Type: text/plain; charset="utf-8"
>
> Reading all the cfg options in Squid 3.5 I noticed http_port has lots of
> SSL related options(which it should not), plus https_port is referring to
> http_port for those options, should http_port have nothing to do with
> ssl-specific options and those ssl-options could be better moved to
> https_port section instead?
>
> http://www.squid-cache.org/Versions/v3/3.5/cfgman/http_port.html
> http://www.squid-cache.org/Versions/v3/3.5/cfgman/https_port.html
>
> Gordon
> -------------- next part --------------
> An HTML attachment was scrubbed...
> URL: <
> http://lists.squid-cache.org/pipermail/squid-users/attachments/20180627/53c8530f/attachment-0001.html
> >
>
> ------------------------------
>
> Message: 4
> Date: Wed, 27 Jun 2018 11:23:22 -0600
> From: Alex Rousskov <rousskov at measurement-factory.com>
> To: Gordon Hsiao <capcoding at gmail.com>,
>         squid-users at lists.squid-cache.org
> Subject: Re: [squid-users] http_port vs https_port
> Message-ID:
>         <ac390312-1c93-627f-fb9a-5b2ff6a564f5 at measurement-factory.com>
> Content-Type: text/plain; charset=utf-8
>
> On 06/27/2018 10:55 AM, Gordon Hsiao wrote:
> > Reading all the cfg options in Squid 3.5 I noticed http_port has lots of
> > SSL related options(which it should not), plus https_port is referring
> > to http_port for those options, should http_port have nothing to do with
> > ssl-specific options and those ssl-options could be better moved to
> > https_port section instead?
>
> http_port uses SSL options when bumping HTTP CONNECT tunnels.
>
> Alex.
>
>
 Keep reading http_port vs https_port here...

1. http_port does not require openssl, https_port does, however http_port
can do ssl-bump so I would think http_port is conditionally depending on
openssl
2. reading cfgman v3.5 page I could not really tell their difference when
openssl/ssl-bump is involved, it seems http_port is a superset of
https_port and they behave the same when ssl-bump(splice or bump) is to be
used.

Since http_port (--with-openssl) seems can do everything https_port can do,
why do we have https_port at all? in which circumstances I must use
https_port?

Thanks,
Gordon
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20180627/b87d7156/attachment.htm>

From squid3 at treenet.co.nz  Wed Jun 27 23:26:57 2018
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 28 Jun 2018 11:26:57 +1200
Subject: [squid-users] http_port vs https_port (Alex Rousskov)
In-Reply-To: <CAK0iFYyv6Di7STKS5APiu=Tjx5egSMgF1KRXoT+hyDKyg6d5nw@mail.gmail.com>
References: <mailman.9520.1530122124.4557.squid-users@lists.squid-cache.org>
 <CAK0iFYyv6Di7STKS5APiu=Tjx5egSMgF1KRXoT+hyDKyg6d5nw@mail.gmail.com>
Message-ID: <5aa9e5b5-4b68-f52e-b689-7565c49ec37a@treenet.co.nz>

On 28/06/18 11:04, Gordon Hsiao wrote:
> ?
> ?Keep reading http_port vs https_port here...
> 
> 1. http_port does not require openssl, https_port does, however
> http_port can do ssl-bump so I would think http_port is conditionally
> depending on openssl

Yes.

> 2. reading cfgman v3.5 page I could not really tell their difference
> when openssl/ssl-bump is involved, it seems http_port is a superset of
> https_port and they behave the same when ssl-bump(splice or bump) is to
> be used.

No, the behaviour is very different at all times:
 One is HTTP maybe containing TLS.
 One is TLS maybe containing HTTP.

The "unwrapping" of layered protocols is different in the two cases.
Modern Squid require ports tuned to the explicit protocol syntax
expected, and the mode of delivery from the client.


> 
> Since http_port (--with-openssl) seems can do everything https_port can
> do, why do we have https_port at all? in which circumstances I must use
> https_port?

When *HTTPS* is being directly received by the port.


http_port is for when the client sends HTTP (no 'S') traffic to Squid.
For example, port 80 or port 3128 syntax traffic.

https_port is for when the client sends HTTPS (TLS with HTTP inside) to
Squid. For example; port 443 syntax or port 3128 with encryption.

ftp_port is for when the client sends FTP. For example; port 21 syntax

icp_port for ICP protocol ...

htcp_port for HTCP protocol ...

snmp_port for SNMP protocol ...

see the pattern?



Naturally since you always expect TLS on https_port that is why simply
adding it requires crypto library support. The other port(s) only
_sometimes_ have to deal with crypto, so its the crypto related options
being set which is when the requirements get mentioned.

Also, OpenSSL support is optional in Squid-4. GnuTLS can be used
instead, depending on what your TLS needs are.

Amos


From donmuller22 at outlook.com  Thu Jun 28 04:31:11 2018
From: donmuller22 at outlook.com (Donald Muller)
Date: Thu, 28 Jun 2018 04:31:11 +0000
Subject: [squid-users] Setting up a Whitelist
In-Reply-To: <99b0099f-55ff-f152-e7c1-22245e6e35b4@treenet.co.nz>
References: <CY1PR16MB04594FFDCBB8EEF5D7ECBF41B6480@CY1PR16MB0459.namprd16.prod.outlook.com>
 <99b0099f-55ff-f152-e7c1-22245e6e35b4@treenet.co.nz>
Message-ID: <CY1PR16MB0459BF35E11215D17FB114AFB64F0@CY1PR16MB0459.namprd16.prod.outlook.com>

Still not working.

> -----Original Message-----
> From: squid-users <squid-users-bounces at lists.squid-cache.org> On Behalf
> Of Amos Jeffries
> Sent: Wednesday, June 27, 2018 4:59 PM
> To: squid-users at lists.squid-cache.org
> Subject: Re: [squid-users] Setting up a Whitelist
> 
> On 28/06/18 08:21, Donald Muller wrote:
> > Hi,
> >
> >
> >
> > Don?t know if what I want to do is even possible but here is the
> > situation. I have Squid set up on my QNAP NAS. It is running fine. I
> > am using it with the blacklist and sites get blocked as they should.
> > However there a number of sites that I do not want blacklisted so I
> > thought I?d set up a whitelist for them. What I did was to add an
> > include statement to the squid.conf file. The included file has the
> > directives for the whitelist.
> >
> >
> >
> > Here are my config files.
> >
> >
> >
> >
> >
> > Squid.conf
> >
> >
> >
> > # The user name and group name Squid will operate as
> >
> > cache_effective_user httpdusr
> >
> 
> The above username requires read access to the included file *and* any
> other files which it instructs Squid to load.
> 
> That access may be granted though group access to the file. IF the above
> member is part of a permitted group. Be careful, Do Not assign Squid into
> root group nor any equivalent on the machine.
> 

Good catch. Owner/group of whitelist files changed.

> 
> ...
> >
> >
> > acl allnet src all????????????????? # All Net
> >
> 
> Why?
>  you are not doing anything like deny_info which might need "allnet"
> 
> Using the built-in "all" ACL would be simpler.
> 

I did not build the Squid package. It was built and distributed by QNAP.

> >
> > include /usr/local/squid/etc/acl.conf
> >
> > include
> > /share/CACHEDEV1_DATA/UserData/Configs/Proxy/whitelist.conf
> > ?-------- I added this line
> >
> 
> The only thing to be aware of is order dependence. Squid loads and operates
> as if the contents of these files were copy-and-pasted exactly at the line
> where the include directive is.
> 
> That means any directives like http_access which contain order-specific
> behaviours retain those behaviours between files in the specific order of the
> include lines.
> 
> So, if acl.conf contains "http_access deny blacklist" and whitelist.conf
> contains "http_access allow whitelist" then:
>  a) blacklist is *still* denying requests before whitelist is even tested.
>  b) whitelist.conf is (only) adding a bypass of all the default/recommended
> squid.conf security lines

acl.conf is empty

> 
> I'm pointing out (b) because you should really only place custom rules
> (especially http_access related ones) at the point in squid.conf labeled
> "INSERT YOUR OWN RULE(S) HERE".
> 
> You have not stated whether you are trying to whitelist against entries in the
> blakclist, or against the proxies default security rules to prevent unsafe
> behaviour (ie spam email using the proxy as a relay, non-HTTPS tuynnels).
>  If you want the former; then the includes need to be done the other way
> around (whitelist.conf include first, then acl.conf).
>  If you want the latter; then you have it now.
> 
> 

Sorry. I am trying to whitelist against sites that are in the blacklist from squidguard.mesd.k12.or.us/blacklists.tgz. So where should my whitelist.conf be?
I tried it after "INSERT YOUR OWN RULE(S) HERE" and also at the end of the squid.conf file.

> ...
> 
> >
> > include /usr/local/squid/etc/acl_http.conf
> >
> > #http_access allow allnet ncsa_users
> >
> > #http_access allow allnet group_administrators
> >
> > #http_access allow allnet nas_user
> 
> NP: Placing "all" on a line with other ACL checks is a hack to prevent
> authentication process being initiated by lines if the credentials are known
> but not allowed certain access. It only works if the "all" is placed at the RHS
> end of lines.
>  So "allnet" is pointless on the above.
> 

It is also commented out.

> 
> >
> > http_access allow allnet
> >
> > #http_access deny allnet
> >
> > # And finally deny all other access to this proxy
> >
> 
> But "allnet" was defined as "all". Which overrides this safety net config line
> and makes your proxy an open-proxy by default.
>  That would be clearer if you had used "all" instead of custom "allnet".
> 
> 
> 
> > #
> >
> > mime_table /usr/local/squid/etc/mime.conf
> >
> > pid_filename /usr/local/squid/var/run/squid.pid
> >
> > diskd_program /usr/local/squid/libexec/diskd
> >
> > unlinkd_program /usr/local/squid/libexec/unlinkd
> >
> > icon_directory /usr/local/squid/share/icons
> >
> > err_page_stylesheet /usr/local/squid/etc/errorpage.css
> 
> None of the above lines should be necessary. If you are custom building
> Squid it should be built with ./configure options setting defaults appropriate
> for the OS its going to run on.
> You only need these squid.conf directives if you have one or a few files in
> really weird placement unusual for the OS.
> 
> Same for any directive which is setting default values. You can simplify the
> config a huge amount by removing them entirely these days.
> (Squid-2.x needed them, Squid-3.x does not).
> 

I did not build the Squid package. It was built and distributed by QNAP.

> 
> > whitelist.conf
> >
> >
> >
> > acl whitelist dstdomain
> > "/share/CACHEDEV1_DATA/UserData/Configs/Proxy/whitelist.txt"
> >
> > http_access allow whitelist
> >
> 
> 
> Amos
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users

From squid3 at treenet.co.nz  Thu Jun 28 09:04:28 2018
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 28 Jun 2018 21:04:28 +1200
Subject: [squid-users] Setting up a Whitelist
In-Reply-To: <CY1PR16MB0459BF35E11215D17FB114AFB64F0@CY1PR16MB0459.namprd16.prod.outlook.com>
References: <CY1PR16MB04594FFDCBB8EEF5D7ECBF41B6480@CY1PR16MB0459.namprd16.prod.outlook.com>
 <99b0099f-55ff-f152-e7c1-22245e6e35b4@treenet.co.nz>
 <CY1PR16MB0459BF35E11215D17FB114AFB64F0@CY1PR16MB0459.namprd16.prod.outlook.com>
Message-ID: <aa76ce3f-254d-de3e-5e30-2c4298fb8d05@treenet.co.nz>

On 28/06/18 16:31, Donald Muller wrote:
> 
> Sorry. I am trying to whitelist against sites that are in the blacklist from squidguard.mesd.k12.or.us/blacklists.tgz. So where should my whitelist.conf be?
> I tried it after "INSERT YOUR OWN RULE(S) HERE" and also at the end of the squid.conf file.
> 

In that case you need to either change the SquidGuard config or prevent
SquidGuard being asked what to do with the request(s).

The later is done by adding this to your whitelist.conf (instead of the
http_access line you have there):

 url_rewrite_access deny whitelist


Amos


From vh1988 at yahoo.com.ar  Thu Jun 28 16:32:55 2018
From: vh1988 at yahoo.com.ar (Julian Perconti)
Date: Thu, 28 Jun 2018 13:32:55 -0300
Subject: [squid-users] SSL errors with Squid 3.5.27
In-Reply-To: <vmime.5b20d39d.15e.6d5721447348f75c@ms249-lin-003.rotterdam.bazuin.nl>
References: <8a1755bc-5257-1db0-8430-ab8fc62fdb77@treenet.co.nz>
 <vmime.5b20d39d.15e.6d5721447348f75c@ms249-lin-003.rotterdam.bazuin.nl>
Message-ID: <005301d40efd$ad002690$070073b0$@yahoo.com.ar>

Hi all:

Finally I migrate everything to debian 9 with openssl 1.1 and squid 4 (june 22/18) rel?ase (the last one).

Everything seems to go very well.

However, the dropbox client logs this error in cache.log:

kid1| ERROR: negotiating TLS on FD 35: error:141710F8:SSL routines:tls_process_server_hello:unknown cipher returned (1/-1/0)

squid version:

Squid Cache: Version 4.0.25-20180621-r887c98a
Service Name: squid

This binary uses OpenSSL 1.1.0f  25 May 2017. For legal restrictions on distribution see https://www.openssl.org/source/license.html

configure options:  '--prefix=/usr' '--build=x86_64-linux-gnu' '--localstatedir=/var/squid' '--libexecdir=/lib/squid' '--srcdir=.' '--datadir=/share/squid' '--sysconfdir=/etc/squid' '--with-default-user=proxy' '--with-logdir=/var/log/squid' '--with-pidfile=/var/run/squid.pid' '--with-openssl' '--enable-ssl-crtd' '--mandir=/share/man' '--enable-storeio=ufs,aufs,diskd' '--enable-removal-policies=lru,heap' '--enable-delay-pools' '--enable-icap' '--enable-cache-digests' 'build_alias=x86_64-linux-gnu' --enable-ltdl-convenience

openssl version and ciphers:

OpenSSL 1.1.0f  25 May 2017

ECDHE-ECDSA-AES256-GCM-SHA384:ECDHE-RSA-AES256-GCM-SHA384:DHE-RSA-AES256-GCM-SHA384:ECDHE-ECDSA-CHACHA20-POLY1305:ECDHE-RSA-CHACHA20-POLY1305:DHE-RSA-CHACHA20-POLY1305:ECDHE-ECDSA-AES128-GCM-SHA256:ECDHE-RSA-AES128-GCM-SHA256:DHE-RSA-AES128-GCM-SHA256:ECDHE-ECDSA-AES256-SHA384:ECDHE-RSA-AES256-SHA384:DHE-RSA-AES256-SHA256:ECDHE-ECDSA-AES128-SHA256:ECDHE-RSA-AES128-SHA256:DHE-RSA-AES128-SHA256:ECDHE-ECDSA-AES256-SHA:ECDHE-RSA-AES256-SHA:DHE-RSA-AES256-SHA:ECDHE-ECDSA-AES128-SHA:ECDHE-RSA-AES128-SHA:DHE-RSA-AES128-SHA:RSA-PSK-AES256-GCM-SHA384:DHE-PSK-AES256-GCM-SHA384:RSA-PSK-CHACHA20-POLY1305:DHE-PSK-CHACHA20-POLY1305:ECDHE-PSK-CHACHA20-POLY1305:AES256-GCM-SHA384:PSK-AES256-GCM-SHA384:PSK-CHACHA20-POLY1305:RSA-PSK-AES128-GCM-SHA256:DHE-PSK-AES128-GCM-SHA256:AES128-GCM-SHA256:PSK-AES128-GCM-SHA256:AES256-SHA256:AES128-SHA256:ECDHE-PSK-AES256-CBC-SHA384:ECDHE-PSK-AES256-CBC-SHA:SRP-RSA-AES-256-CBC-SHA:SRP-AES-256-CBC-SHA:RSA-PSK-AES256-CBC-SHA384:DHE-PSK-AES256-CBC-SHA384:RSA-PSK-AES256-CBC-SHA:DHE-PSK-AES256-CBC-SHA:AES256-SHA:PSK-AES256-CBC-SHA384:PSK-AES256-CBC-SHA:ECDHE-PSK-AES128-CBC-SHA256:ECDHE-PSK-AES128-CBC-SHA:SRP-RSA-AES-128-CBC-SHA:SRP-AES-128-CBC-SHA:RSA-PSK-AES128-CBC-SHA256:DHE-PSK-AES128-CBC-SHA256:RSA-PSK-AES128-CBC-SHA:DHE-PSK-AES128-CBC-SHA:AES128-SHA:PSK-AES128-CBC-SHA256:PSK-AES128-CBC-SHA

squid.conf ciphers:

tls_outgoing_options cipher=HIGH:MEDIUM:!RC4:!aNULL:!eNULL:!LOW:!3DES:!MD5:!EXP:!PSK:!SRP:!DSS

I still have not tried what happen with whatsapp from iOS (original problem)

Any ideas? Fix?



From rousskov at measurement-factory.com  Fri Jun 29 02:28:22 2018
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Thu, 28 Jun 2018 20:28:22 -0600
Subject: [squid-users] when will squid 4 be production ready?
In-Reply-To: <035b01d40f34$2fbf4fd0$8f3def70$@ngtech.co.il>
References: <CAK0iFYyuRxLNHOXhLy3=xB7PdyKW5SGqM-E3a-LT5+Wf3NfgdA@mail.gmail.com>
 <be353a9b-c0bc-02dc-e132-d4c0a91dde0e@treenet.co.nz>
 <613e11af6c4366c3fc1a722d3898ce8e@ngtech.co.il>
 <4d9d6bd0-e505-b343-1f86-7dcc135388a4@treenet.co.nz>
 <035b01d40f34$2fbf4fd0$8f3def70$@ngtech.co.il>
Message-ID: <4bce3661-52e2-7efa-a57e-0aab8141d489@measurement-factory.com>

On 06/28/2018 05:03 PM, Eliezer Croitoru wrote:
> /usr/lib64/squid/security_file_certgen: security_file_certgen -s requires an -M parameter

> which is weird, why would I need the "-M" when creating the ssl_db directory which is a bunch of empty files?

The helper needs -M to know when to start deleting cached certificates.
The -s parameter enables certificate caching by the helper. A cache
needs to know its maximum size.

If you do not need a cache, do not specify -s (provided that recent
feature was ported to v4).

Alex.


From cunzhihuang at gmail.com  Sat Jun 30 04:54:44 2018
From: cunzhihuang at gmail.com (CZ Huang)
Date: Fri, 29 Jun 2018 21:54:44 -0700 (MST)
Subject: [squid-users] ERR_ACCESS_DENIED when using transparent https proxy
Message-ID: <1530334484969-0.post@n4.nabble.com>

I used the following command to send requests (see details below) but got
"HTTP/1.1 403 Forbidden".

curl https://www.online.citi.com -x https://10.192.197.200:3130 --verbose
--proxy-insecure

I understand the error was caused by "CONNECT 10.192.197.200:3130 HTTP/1.1".
But curl did not send it so where did it come from?

If I change "https_port 10.192.197.200:3130 ssl-bump intercept" to
"https_port 10.192.197.200:3130" in the config file, then there is no error
(proxy does not take part in the 2nd SSL handshake anymore).

Please help me fix the errors. Thanks!

========================================================

2018/06/29 21:07:38.718 kid1| 11,2| client_side.cc(2372) parseHttpRequest:
HTTP Client local=10.192.197.200:3130 remote=172.18.78.222:53759 FD 10
flags=33
2018/06/29 21:07:38.718 kid1| 11,2| client_side.cc(2373) parseHttpRequest:
HTTP Client REQUEST:
---------
CONNECT 10.192.197.200:3130 HTTP/1.1
Host: 10.192.197.200:3130
----------

---------
CONNECT www.online.citi.com:443 HTTP/1.1
Host: www.online.citi.com:443
User-Agent: curl/7.59.0
Proxy-Connection: Keep-Alive
----------

2018/06/29 21:07:38.718 kid1| 5,3| comm.cc(553) commSetConnTimeout:
local=10.192.197.200:3130 remote=172.18.78.222:53759 FD 10 flags=33 timeout
86400
2018/06/29 21:07:38.718 kid1| 23,3| url.cc(371) urlParse: urlParse: Split
URL '10.192.197.200:3130' into proto='', host='10.192.197.200', port='3130',
path=''
2018/06/29 21:07:38.718 kid1| 23,3| HttpRequest.h(82) SetHost:
HttpRequest::SetHost() given IP: 10.192.197.200
2018/06/29 21:07:38.718 kid1| 33,3| client_side.cc(891)
clientSetKeepaliveFlag: http_ver = HTTP/1.1
2018/06/29 21:07:38.718 kid1| 33,3| client_side.cc(892)
clientSetKeepaliveFlag: method = CONNECT
2018/06/29 21:07:38.718 kid1| 33,3| client_side.h(98) mayUseConnection: This
0x564037b7d2f8 marked 1
2018/06/29 21:07:38.719 kid1| 85,3| client_side_request.cc(130)
ClientRequestContext: 0x564037b80648 ClientRequestContext constructed
2018/06/29 21:07:38.719 kid1| 83,3| client_side_request.cc(1708) doCallouts:
Doing calloutContext->hostHeaderVerify()
2018/06/29 21:07:38.719 kid1| 85,3| client_side_request.cc(635)
hostHeaderVerify: validate host=10.192.197.200, port=3130, portStr=3130
2018/06/29 21:07:38.719 kid1| 85,3| client_side_request.cc(526)
hostHeaderIpVerify: validate IP 10.192.197.200:3130 possible from Host:
2018/06/29 21:07:38.719 kid1| 83,3| client_side_request.cc(1715) doCallouts:
Doing calloutContext->clientAccessCheck()
2018/06/29 21:07:38.719 kid1| 28,3| Checklist.cc(70) preCheck:
0x564037b807f8 checking slow rules
2018/06/29 21:07:38.719 kid1| 28,3| Acl.cc(158) matches: checked: Safe_ports
= 1
2018/06/29 21:07:38.719 kid1| 28,3| Acl.cc(158) matches: checked:
!Safe_ports = 0
2018/06/29 21:07:38.719 kid1| 28,3| Acl.cc(158) matches: checked:
http_access#1 = 0
2018/06/29 21:07:38.719 kid1| 28,3| Acl.cc(158) matches: checked: CONNECT =
1
2018/06/29 21:07:38.719 kid1| 28,3| Acl.cc(158) matches: checked: SSL_ports
= 0
2018/06/29 21:07:38.719 kid1| 28,3| Acl.cc(158) matches: checked: !SSL_ports
= 1
2018/06/29 21:07:38.719 kid1| 28,3| Acl.cc(158) matches: checked:
http_access#2 = 1
2018/06/29 21:07:38.719 kid1| 28,3| Acl.cc(158) matches: checked:
http_access = 1
2018/06/29 21:07:38.719 kid1| 28,3| Checklist.cc(63) markFinished:
0x564037b807f8 answer DENIED for match
2018/06/29 21:07:38.719 kid1| 28,3| Checklist.cc(163) checkCallback:
ACLChecklist::checkCallback: 0x564037b807f8 answer=DENIED
2018/06/29 21:07:38.719 kid1| 85,2| client_side_request.cc(745)
clientAccessCheckDone: The request CONNECT 10.192.197.200:3130 is DENIED;
last ACL checked: SSL_ports


---------
HTTP/1.1 403 Forbidden
Server: squid/3.5.27
Mime-Version: 1.0
Date: Sat, 30 Jun 2018 04:07:38 GMT
Content-Type: text/html;charset=utf-8
Content-Length: 3477
X-Squid-Error: ERR_ACCESS_DENIED 0
Vary: Accept-Language
Content-Language: en
X-Cache: MISS from xxxxxxx
Via: 1.1 xxxxxxx (squid/3.5.27)
Connection: close
----------

========================================================

$ sudo iptables -L -n -v -t nat
Chain PREROUTING (policy ACCEPT 1738 packets, 191K bytes)
 pkts bytes target     prot opt in     out     source              
destination

Chain INPUT (policy ACCEPT 1638 packets, 177K bytes)
 pkts bytes target     prot opt in     out     source              
destination

Chain OUTPUT (policy ACCEPT 35154 packets, 2119K bytes)
 pkts bytes target     prot opt in     out     source              
destination

Chain POSTROUTING (policy ACCEPT 35154 packets, 2119K bytes)
 pkts bytes target     prot opt in     out     source              
destination

========================================================

$ sudo squid -v
Squid Cache: Version 3.5.27
Service Name: squid

This binary uses OpenSSL 1.0.2g  1 Mar 2016. For legal restrictions on
distribution see https://www.openssl.org/source/license.html

configure options:  '--prefix=/usr' '--exec-prefix=/usr'
'--includedir=/usr/include' '--datadir=/usr/share' '--libdir=/usr/lib64'
'--libexecdir=/usr/lib64/squid' '--localstatedir=/var'
'--sysconfdir=/etc/squid' '--sharedstatedir=/var/lib'
'--with-logdir=/var/log/squid' '--with-pidfile=/var/run/squid.pid'
'--with-default-user=squid' '--enable-silent-rules'
'--enable-dependency-tracking' '--enable-icmp' '--enable-delay-pools'
'--enable-useragent-log' '--enable-esi' '--enable-follow-x-forwarded-for'
'--enable-auth' '--enable-ssl-crtd' '--disable-arch-native'
'--enable-linux-netfilter' '--with-openssl' --enable-ltdl-convenience

========================================================

$ sudo squid -k parse
2018/06/29 21:17:03| Startup: Initializing Authentication Schemes ...
2018/06/29 21:17:03| Startup: Initialized Authentication Scheme 'basic'
2018/06/29 21:17:03| Startup: Initialized Authentication Scheme 'digest'
2018/06/29 21:17:03| Startup: Initialized Authentication Scheme 'negotiate'
2018/06/29 21:17:03| Startup: Initialized Authentication Scheme 'ntlm'
2018/06/29 21:17:03| Startup: Initialized Authentication.
2018/06/29 21:17:03| Processing Configuration File: /etc/squid/squid.conf
(depth 0)
2018/06/29 21:17:03| Processing: acl localnet src 10.0.0.0/8    # RFC1918
possible internal network
2018/06/29 21:17:03| Processing: acl localnet src 172.16.0.0/12 # RFC1918
possible internal network
2018/06/29 21:17:03| Processing: acl localnet src 192.168.0.0/16        #
RFC1918 possible internal network
2018/06/29 21:17:03| Processing: acl localnet src fc00::/7       # RFC 4193
local private network range
2018/06/29 21:17:03| Processing: acl localnet src fe80::/10      # RFC 4291
link-local (directly plugged) machines
2018/06/29 21:17:03| Processing: acl SSL_ports port 443
2018/06/29 21:17:03| Processing: acl Safe_ports port 80         # http
2018/06/29 21:17:03| Processing: acl Safe_ports port 21         # ftp
2018/06/29 21:17:03| Processing: acl Safe_ports port 443                #
https
2018/06/29 21:17:03| Processing: acl Safe_ports port 70         # gopher
2018/06/29 21:17:03| Processing: acl Safe_ports port 210                #
wais
2018/06/29 21:17:03| Processing: acl Safe_ports port 1025-65535 #
unregistered ports
2018/06/29 21:17:03| Processing: acl Safe_ports port 280                #
http-mgmt
2018/06/29 21:17:03| Processing: acl Safe_ports port 488                #
gss-http
2018/06/29 21:17:03| Processing: acl Safe_ports port 591                #
filemaker
2018/06/29 21:17:03| Processing: acl Safe_ports port 777                #
multiling http
2018/06/29 21:17:03| Processing: acl CONNECT method CONNECT
2018/06/29 21:17:03| Processing: debug_options ALL,3
2018/06/29 21:17:03| Processing: http_access deny !Safe_ports
2018/06/29 21:17:03| Processing: http_access deny CONNECT !SSL_ports
2018/06/29 21:17:03| Processing: http_access allow localhost manager
2018/06/29 21:17:03| Processing: http_access deny manager
2018/06/29 21:17:03| Processing: http_access allow localnet
2018/06/29 21:17:03| Processing: http_access allow localhost
2018/06/29 21:17:03| Processing: http_access  allow all
2018/06/29 21:17:03| Processing: http_port 3128
2018/06/29 21:17:03| Processing: http_port 10.192.197.200:3129 ssl-bump 
cert=/etc/squid/ssl_cert/myCA.pem  generate-host-certificates=on
dynamic_cert_mem_cache_size=4MB
2018/06/29 21:17:03| Processing: https_port 10.192.197.200:3130 ssl-bump
intercept  cert=/etc/squid/ssl_cert/myCA.pem generate-host-certificates=on
dynamic_cert_mem_cache_size=4MB
2018/06/29 21:17:03| Starting Authentication on port 10.192.197.200:3130
2018/06/29 21:17:03| Disabling Authentication on port 10.192.197.200:3130
(interception enabled)
2018/06/29 21:17:03| Processing: acl step1 at_step SslBump1
2018/06/29 21:17:03| Processing: ssl_bump peek step1
2018/06/29 21:17:03| Processing: ssl_bump bump all
2018/06/29 21:17:03| Processing: ssl_bump stare all
2018/06/29 21:17:03| Processing: always_direct allow all
2018/06/29 21:17:03| Processing: coredump_dir /var/cache/squid
2018/06/29 21:17:03| Processing: refresh_pattern ^ftp:          1440    20%    
10080
2018/06/29 21:17:03| Processing: refresh_pattern ^gopher:       1440    0%     
1440
2018/06/29 21:17:03| Processing: refresh_pattern -i (/cgi-bin/|\?) 0    0%     
0
2018/06/29 21:17:03| Processing: refresh_pattern .              0       20%    
4320
2018/06/29 21:17:03| Initializing https proxy context
2018/06/29 21:17:03| Initializing http_port 10.192.197.200:3129 SSL context
2018/06/29 21:17:03| Using certificate in /etc/squid/ssl_cert/myCA.pem
2018/06/29 21:17:03| Initializing https_port 10.192.197.200:3130 SSL context
2018/06/29 21:17:03| Using certificate in /etc/squid/ssl_cert/myCA.pem

========================================================

$ curl  https://www.online.citi.com -x https://10.192.197.200:3130 --verbose
--proxy-insecure
* STATE: INIT => CONNECT handle 0x6000579c0; line 1404 (connection #-5000)
* Rebuilt URL to: https://www.online.citi.com/
* Added connection 0. The cache now contains 1 members
*   Trying 10.192.197.200...
* TCP_NODELAY set
* STATE: CONNECT => WAITCONNECT handle 0x6000579c0; line 1456 (connection
#0)
* Connected to 10.192.197.200 (10.192.197.200) port 3130 (#0)
* STATE: WAITCONNECT => WAITPROXYCONNECT handle 0x6000579c0; line 1566
(connection #0)
* Marked for [keep alive]: HTTP default
* ALPN, offering http/1.1
* Cipher selection:
ALL:!EXPORT:!EXPORT40:!EXPORT56:!aNULL:!LOW:!RC4:@STRENGTH
* ignoring certificate verify locations due to disabled peer verification
* TLSv1.2 (OUT), TLS header, Certificate Status (22):
* TLSv1.2 (OUT), TLS handshake, Client hello (1):
* TLSv1.2 (IN), TLS handshake, Server hello (2):
* TLSv1.2 (IN), TLS handshake, Certificate (11):
* TLSv1.2 (IN), TLS handshake, Server finished (14):
* TLSv1.2 (OUT), TLS handshake, Client key exchange (16):
* TLSv1.2 (OUT), TLS change cipher, Client hello (1):
* TLSv1.2 (OUT), TLS handshake, Finished (20):
* TLSv1.2 (IN), TLS change cipher, Client hello (1):
* TLSv1.2 (IN), TLS handshake, Finished (20):
* SSL connection using TLSv1.2 / AES256-GCM-SHA384
* ALPN, server did not agree to a protocol
* Proxy certificate:
*  subject: CN=10.192.197.200
*  start date: Jun 29 04:28:09 2018 GMT
*  expire date: Jun 29 04:28:09 2019 GMT
*  issuer: C=AU; ST=Some-State; O=Internet Widgits Pty Ltd
*  SSL certificate verify result: self signed certificate in certificate
chain (19), continuing anyway.
* allocate connect buffer!
* Establish HTTP proxy tunnel to www.online.citi.com:443
> CONNECT www.online.citi.com:443 HTTP/1.1
> Host: www.online.citi.com:443
> User-Agent: curl/7.59.0
> Proxy-Connection: Keep-Alive
>
< HTTP/1.1 403 Forbidden
< Server: squid/3.5.27
< Mime-Version: 1.0
< Date: Sat, 30 Jun 2018 04:07:38 GMT
< Content-Type: text/html;charset=utf-8
< Content-Length: 3477
< X-Squid-Error: ERR_ACCESS_DENIED 0
< Vary: Accept-Language
< Content-Language: en
< X-Cache: MISS from cxxxxx
< Via: 1.1 xxxxx (squid/3.5.27)
< Connection: close
<
* Marked for [closure]: proxy CONNECT failure
* Received HTTP code 403 from proxy after CONNECT
* CONNECT phase completed!
* multi_done
* Closing connection 0
* The cache now contains 0 members
curl: (56) Received HTTP code 403 from proxy after CONNECT


========================================================




--
Sent from: http://squid-web-proxy-cache.1019090.n4.nabble.com/Squid-Users-f1019091.html


From squid3 at treenet.co.nz  Sat Jun 30 09:03:51 2018
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sat, 30 Jun 2018 21:03:51 +1200
Subject: [squid-users] ERR_ACCESS_DENIED when using transparent https
 proxy
In-Reply-To: <1530334484969-0.post@n4.nabble.com>
References: <1530334484969-0.post@n4.nabble.com>
Message-ID: <635653ff-7806-caad-f1d3-538d6d26b7a4@treenet.co.nz>

On 30/06/18 16:54, CZ Huang wrote:
> I used the following command to send requests (see details below) but got
> "HTTP/1.1 403 Forbidden".
> 
> curl https://www.online.citi.com -x https://10.192.197.200:3130 --verbose
> --proxy-insecure
> 
> I understand the error was caused by "CONNECT 10.192.197.200:3130 HTTP/1.1".
> But curl did not send it so where did it come from?

It came from your machines NAT system, in combination with SSL-Bump.

> 
> If I change "https_port 10.192.197.200:3130 ssl-bump intercept" to
> "https_port 10.192.197.200:3130" in the config file, then there is no error
> (proxy does not take part in the 2nd SSL handshake anymore).
> 
> Please help me fix the errors. Thanks!
> 

The error is that you told Squid to contact the system NAT tables to
find out where the client connection was going (the "intercept" option
on http_port) without having configured any NAT rules, AND when testing
explicit-proxy message syntax with curl.

 <http://www.squid-cache.org/Doc/config/http_port/>
"
Modes:

   intercept	Support for IP-Layer NAT interception delivering
   		traffic to this Squid port.
"

You do not use NAT, so "intercept" is absolutely the wrong thing to be
configuring. That is why removing it "fixed" the problem.

If you intend the proxy to be an explicit/forward proxy, then removal of
the "intercept" option is your full solution.

If you intend the proxy to be intercepting traffic, then you need to
start there and get the NAT intercept part working _first_. Things that
work for explicit/forward proxy don't work with intercept, and the
reverse. So its just a waste of effort testing irrelevant traffic "modes".


Amos


