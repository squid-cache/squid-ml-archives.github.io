<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<HTML>
 <HEAD>
   <TITLE> [squid-users] HTTPS cache for Java application - only getting TCP_MISS
   </TITLE>
   <LINK REL="Index" HREF="index.html" >
   <LINK REL="made" HREF="mailto:squid-users%40lists.squid-cache.org?Subject=Re%3A%20%5Bsquid-users%5D%20HTTPS%20cache%20for%20Java%20application%20-%20only%20getting%0A%20TCP_MISS&In-Reply-To=%3Cc239d220-96d0-c25b-7160-fb95f14ebbe5%40treenet.co.nz%3E">
   <META NAME="robots" CONTENT="index,nofollow">
   <style type="text/css">
       pre {
           white-space: pre-wrap;       /* css-2.1, curent FF, Opera, Safari */
           }
   </style>
   <META http-equiv="Content-Type" content="text/html; charset=us-ascii">
   <LINK REL="Previous"  HREF="018437.html">
   <LINK REL="Next"  HREF="018445.html">
 </HEAD>
 <BODY BGCOLOR="#ffffff">
   <H1>[squid-users] HTTPS cache for Java application - only getting TCP_MISS</H1>
    <B>Amos Jeffries</B> 
    <A HREF="mailto:squid-users%40lists.squid-cache.org?Subject=Re%3A%20%5Bsquid-users%5D%20HTTPS%20cache%20for%20Java%20application%20-%20only%20getting%0A%20TCP_MISS&In-Reply-To=%3Cc239d220-96d0-c25b-7160-fb95f14ebbe5%40treenet.co.nz%3E"
       TITLE="[squid-users] HTTPS cache for Java application - only getting TCP_MISS">squid3 at treenet.co.nz
       </A><BR>
    <I>Thu Jun 14 11:25:29 UTC 2018</I>
    <P><UL>
        <LI>Previous message (by thread): <A HREF="018437.html">[squid-users] HTTPS cache for Java application - only getting	TCP_MISS
</A></li>
        <LI>Next message (by thread): <A HREF="018445.html">[squid-users] HTTPS cache for Java application - only getting	TCP_MISS
</A></li>
         <LI> <B>Messages sorted by:</B> 
              <a href="date.html#18441">[ date ]</a>
              <a href="thread.html#18441">[ thread ]</a>
              <a href="subject.html#18441">[ subject ]</a>
              <a href="author.html#18441">[ author ]</a>
         </LI>
       </UL>
    <HR>  
<!--beginarticle-->
<PRE>On 14/06/18 07:28, baretomas wrote:
&gt;<i> Hello,
</I>&gt;<i> 
</I>&gt;<i> I'm setting up a Squid proxy as a cache for a number (as many as possible)
</I>&gt;<i> of identical JAVA applications to run their web calls through. The calls are
</I>&gt;<i> ofc identical, and the response they get can safely be cached for 5-10
</I>&gt;<i> seconds. 
</I>&gt;<i> I do this because most of the calls is directed at a single server on the
</I>&gt;<i> internet that I don't want to hammer, since I will ofc be locked out of it
</I>&gt;<i> then.
</I>&gt;<i> 
</I>&gt;<i> Currently Im simply testing this on a single computer: the application and
</I>&gt;<i> squid
</I>&gt;<i> 
</I>&gt;<i> The calls from the application is done using ssl / https by telling java to
</I>&gt;<i> use Squid as a proxy (-Dhttps.proxyHost and -Dhttp.proxyHost). I've set up
</I>&gt;<i> squid and JAVA with self-signed certificates, and the application sends its
</I>&gt;<i> calls through squid and gets the reponse. No problem there (wasnt easy that
</I>&gt;<i> either I must say :P ).
</I>
I was going to ask what was so hard about it. Then I looked at your
config and see that your are in fact using NAT interception instead of
the easy way.

So what _exactly_ do those -D options cause the Java applications to do
with the proxy?
 I have some suspicions, but am not familiar enough with Java API and
the specific details are critical to what you need the proxy to be doing.


&gt;<i> 
</I>&gt;<i> The problem is that none of the calls get cached: All rows in the access.log
</I>&gt;<i> hava a TCP_MISS/200 tag in them. 
</I>&gt;<i> 
</I>&gt;<i> I've searched all through the web for a solution to this, and have tried
</I>&gt;<i> everything people have suggested. So I was hoping someone could help me?
</I>&gt;<i> 
</I>&gt;<i> Anyone have any tips on what to try?
</I>&gt;<i> 
</I>
There are three ways to do this:

1) if you own the domain the apps are connecting to. Setup the proxy as
a normal TLS / HTTPS reverse-proxy.

2) if you have enough control of the apps to get them connecting with
TLS *to the proxy* and sending their requests there. Do that.

3) the (relatively) complicated SSL-Bump way you found. The proxy is
fully at the mercy of the the messages sent by apps and servers. Caching
is a luxury here, easily broken / prevented.

Well, there is a forth way with intercept. But that is a VERY last
resort and you already have (3) going and that is already better than
intercept. Getting to (1) or (2) would be simplest if you meet the &quot;if
...&quot; requirements for those.



&gt;<i> MY config (note Ive set the refresh_pattern like that just to see if I could
</I>&gt;<i> catch anything. The plan is to modify it so it actualyl does refresh the
</I>&gt;<i> responses frmo the web calls in 5-10 seconds intervals. There are commented
</I>&gt;<i> out pats Ive tried with no luck there too):
</I>&gt;<i> 
</I>...

Ah. The way you write that implies a misunderstanding about refresh_pattern.

HTTP has some fixed algorithms written into the protocol that caches are
required to perform to determine if any object stored can be used or
requires replacement.

The parameters used by these algorithms come in the form of headers in
the originally stored reply message, the current clients request.
Sometimes they require revalidation, which is a quick check with the
server for updated instructions and/or content.

What refresh_pattern actually does is provide default values for those
algorithm parameters IF any one (or more) of them are missing from those
HTTP messages.


The proper way to make caching happen with your desired behaviour is for
the server to present HTTP Cache-Control header saying the object is
cacheable (ie does not forbid caching), but not for more than 10seconds.
 Cache-Control: max-age=10
OR to say that objects need revalidation, but presents a 304 status for
revalidation checks. (ie Cache-Control:no-cache)  (yeah, thats right,
&quot;no-cache&quot; means *do* cache).

That said, I doubt you really are wanting to force that and would be
happy if the server was instructing the the proxy as being safe to cache
an object for several minutes or any value larger than 10sec.


So what we circle back to is that you are probably trying to force
things to cache and be used long past their actual safe-to-use lifetimes
as specified by the devs most authoritative on that subject (under
10sec?). As you should be aware, this is highly unsafe thing to be doing
unless you are one of those devs - be very careful what you choose to do.


&gt;<i> 
</I>&gt;<i> 
</I>&gt;<i> # Squid normally listens to port 3128
</I>&gt;<i> #http_port 3128 ssl-bump generate-host-certificates=on
</I>&gt;<i> dynamic_cert_mem_cache_size=4MB cert=/cygdrive/c/squid/etc/squid/correct.pem
</I>&gt;<i> key=/cygdrive/c/squid/etc/squid/ssl/myca.key 
</I>&gt;<i> 
</I>&gt;<i> http_port 3128 ssl-bump generate-host-certificates=on
</I>&gt;<i> dynamic_cert_mem_cache_size=4MB
</I>&gt;<i> cert=/cygdrive/c/squid/etc/squid/proxyCAx.pem
</I>&gt;<i> key=/cygdrive/c/squid/etc/squid/proxyCA.pem
</I>&gt;<i> 
</I>&gt;<i> #https_port 3129 cert=/cygdrive/c/squid/etc/squid/proxyCAx.pem
</I>&gt;<i> key=/cygdrive/c/squid/etc/squid/proxyCA.pem
</I>&gt;<i> 
</I>
Hmm. This is a Windows machine running Cygwin?
FYI: Performance is going to be terrible. It may not be super relevant
yet. Just be aware that Windows imposes limitations on usable sockets
per application - which is much smaller than a typical proxy requires.
The Cygwin people do a lot but they cannot solve some OS limitation
problems.

To meet your very first sentence &quot;as many as possible&quot; requirement you
will need a non-Windows machine to run the proxy on. That simple change
will get you something around 3 orders of magnitude higher peak client
capacity on the proxy.


&gt;<i> 
</I>&gt;<i> # Uncomment the line below to enable disk caching - path format is
</I>&gt;<i> /cygdrive/&lt;full path to cache folder&gt;, i.e.
</I>&gt;<i> #cache_dir aufs /cygdrive/c/squid/var/cache/ 3000 16 256
</I>&gt;<i> 
</I>&gt;<i> # certificate generation program
</I>&gt;<i> sslcrtd_program /cygdrive/c/squid/lib/squid/ssl_crtd -s
</I>&gt;<i> /cygdrive/c/squid/var/cache/squid_ssldb -M 4MB
</I>&gt;<i> 
</I>&gt;<i> # Leave coredumps in the first cache dir
</I>&gt;<i> coredump_dir /var/cache/squid
</I>&gt;<i> 
</I>&gt;<i> # Add any of your own refresh_pattern entries above these.
</I>&gt;<i> #refresh_pattern ^ftp:		1440	20%	10080
</I>&gt;<i> #refresh_pattern ^gopher:	1440	0%	1440
</I>&gt;<i> #refresh_pattern -i (/cgi-bin/|\?) 0	0%	0
</I>&gt;<i> #refresh_pattern -i (/cgi-bin/|\?) 1440 100% 4320 ignore-no-store
</I>&gt;<i> override-lastmod override-expire ignore-must-revalidate ignore-reload
</I>&gt;<i> ignore-private ignore-auth
</I>&gt;<i> refresh_pattern .		1440	100%	4320 ignore-no-store override-lastmod
</I>&gt;<i> override-expire ignore-must-revalidate ignore-reload ignore-private
</I>&gt;<i> ignore-auth override-lastmod 
</I>&gt;<i> 
</I>

* ignore-must-revalidate actively *reduces* caching. Because it disables
several of the widely used HTTP mechanisms that rely on revalidation to
allow things to be stored in a cache.
 It is *only* beneficial if the server is broken; requiring revalidation
plus not supporting revalidation.


* ignore-auth same un-intuitive effects as ignoring revalidation, again
reducing caching ability.
 This is only useful if you want to prevent caching of contents which
require any form of login to view. High security networks dealing with
classified or confidential materials find this useful - regular Internet
admin not so much.


* ignore-no-store is highly dangerous and rarely necessary. The &quot;nuclear
option&quot; for caching. It has the potential to eradicate user privacy and
scramble up any server personalized content (not in a good way).
 This is a last resort intended only to copy with severely braindead
applications. YMMV whether you have to deal with any of those - just
treat this an absolute last resort rather than something to play with.


Overall - in order to use these refresh-pattern controls you *need* to
know what the HTTP(S) messages going through your proxy contain in terms
of caching headers AND what those messages are doing semantically /
content wise for the client application. Using any of them as a generic
&quot;makes caching better&quot; thing only leads to problems in todays HTTP protocol.


&gt;<i> # Bumped requests have relative URLs so Squid has to use reverse proxy
</I>&gt;<i> # or accelerator code. By default, that code denies direct forwarding.
</I>&gt;<i> # The need for this option may disappear in the future.
</I>&gt;<i> #always_direct allow all
</I>&gt;<i> 
</I>&gt;<i> dns_nameservers 8.8.8.8 208.67.222.222
</I>
Use of 8.8.8.8 is known to be explicitly detrimental to caching
intercepted traffic.

Those servers present different result sets based on the timing and IP
sending the query. The #1 requirement of caching intercepted (or
SSL-Bump'ed) content is that the client and proxy have the exact same
view of DNS system contents. Having the DNS reply contents change
between two consecutive and identical queries breaks that requirement.


&gt;<i> 
</I>&gt;<i> max_filedescriptors 3200
</I>&gt;<i> 
</I>&gt;<i> # Max Object Size Cache
</I>&gt;<i> maximum_object_size 10240 KB
</I>&gt;<i> 
</I>&gt;<i> 
</I>&gt;<i> acl step1 at_step SslBump1
</I>&gt;<i> 
</I>&gt;<i> ssl_bump peek step1
</I>&gt;<i> ssl_bump bump all
</I>
This causes the proxy to attempt decryption of the traffic using crypto
algorithms based solely on the ClientHello details and its own
capabilities. There is zero server crypto capabilities known for the
proxy to use to ensure traffic can actually make it to the server.

You are rather lucky that it actually worked at all. Almost any
deviation (ie emergency security updates in future) at either client or
server or proxy endpoints risks breaking the communication through this
proxy.

Ideally there would be a stare action for step2 and them bump only at
step 3.




So in summary to the things to try to get better caching:

* ditch 8.8.8.8. Use a local DNS resolver within your own network,
shared by clients and proxy. That can use 8.8.8.8 itself, the important
part is that it should be responsible for caching DNS results and
ensuring the app clients and Squid see as much the same records as possible.

* try &quot;debug_options 11,2&quot; to get a cache.log of the HTTP(S) headers for
message being decrypted in the proxy. Look at those headers to see why
they are not caching normally. Use that info to inform your next
actions. It cannot tell you how the message is used by the application,
hopefully you can figure that out somehow before forcing anything unnatural.

* if you can, try pasting some of the transaction URLs into the tool at
redbot.org to see if there are any HTTP level mistakes in the apps that
could be fixed for better cacheability.

Amos

</PRE>

<!--endarticle-->
    <HR>
    <P><UL>
        <!--threads-->
	<LI>Previous message (by thread): <A HREF="018437.html">[squid-users] HTTPS cache for Java application - only getting	TCP_MISS
</A></li>
	<LI>Next message (by thread): <A HREF="018445.html">[squid-users] HTTPS cache for Java application - only getting	TCP_MISS
</A></li>
         <LI> <B>Messages sorted by:</B> 
              <a href="date.html#18441">[ date ]</a>
              <a href="thread.html#18441">[ thread ]</a>
              <a href="subject.html#18441">[ subject ]</a>
              <a href="author.html#18441">[ author ]</a>
         </LI>
       </UL>

<hr>
<a href="https://lists.squid-cache.org/listinfo/squid-users">More information about the squid-users
mailing list</a><br>
</body></html>
