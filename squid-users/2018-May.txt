From squid3 at treenet.co.nz  Tue May  1 03:00:53 2018
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Tue, 1 May 2018 15:00:53 +1200
Subject: [squid-users] Squid 3.5.27 - While access https website,
 always "Your connection is not secure"
In-Reply-To: <1524973947097-0.post@n4.nabble.com>
References: <1524905783708-0.post@n4.nabble.com>
 <fa25a86b-6f6c-0ba2-2373-ae785831a4b2@treenet.co.nz>
 <1524973947097-0.post@n4.nabble.com>
Message-ID: <4b5ed25c-b26d-9d92-899b-c46fe1db7f38@treenet.co.nz>

On 29/04/18 15:52, fourirakbar wrote:
> 
> So how make the correct configuration in squid to let client access https
> website? I've struggle about this configuration

The second block of config settings you had using peek/splice/bump
looked okay to me. It was just the "server-first" line probably causing
issues by its very existence.

(sorry for the slow reply, been a busy week IRL)
Amos


From squid3 at treenet.co.nz  Tue May  1 03:40:13 2018
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Tue, 1 May 2018 15:40:13 +1200
Subject: [squid-users] deny_info and squid's own IP address?
In-Reply-To: <88295b8a-aab4-c02c-5a59-a582f5215e7c@gmail.com>
References: <88295b8a-aab4-c02c-5a59-a582f5215e7c@gmail.com>
Message-ID: <92821481-40de-b0c9-c396-e501a15c727f@treenet.co.nz>

On 01/05/18 00:54, Amish wrote:
> Hello
> 
> I have 2 LAN interface on squid box, say department A (192.168.1.1/24)
> and department B (192.168.2.1/24)
> 
> I have few banned sites. Say Facebook.
> 
> I have HTTP server (running on same server as squid) which shows custom
> pages with custom logo based on IP address.
> 
> When request comes for a banned site I would like client to be
> redirected based on squid's own IP.

Firstly, is there any particular reason you are requiring it to be a
redirect?
 from what you have said it appears you can achieve the same outcome
without the extra web server by using a custom error page.

> 
> Something like this:
> 
> acl blockedsites url_regex facebook
> http_access deny blockedsites
> deny_info http://SQUID-IP/banned.html blockedsites
> 
> I need SQUID-IP to be replaced by 192.168.1.1 or 192.168.2.1 depending
> on the IP on which connection came to.
> 

Secondly, I think you are probably looking at this from the wrong
direction. With the topology you have described each of these "Squid
IPs" is actually just the IP facing a certain client subnet. So the
client subnet is what you want to be detecting, not the specific Squid IP.


Thirdly, on the issue of %h - the Squid hostname is *required* to
resolve in DNS explicitly so clients can access things like these URLs.
If your network and DNS is configured correctly each client subnet
should resolve that hostname to the relevant IP which you are trying to
"pass" to the web server in your redirect URL. So they will naturally
(and only) connect to the web server (or Squid itself) using the right
IP anyway - the web server should be able to detect what it needs from
its own inbound TCP/IP connection instead of using raw-IPs in the traffic.


There are three options available to work around broken DNS:


Option 1) to do exactly (and only) what you asked for.

Currently this can be done with an external helper:

 external_acl_type getIp concurrency=100 %MYADDR /path/to/script
 deny_info 302:http://%et/banned.html getIp

where the script just echos back to Squid the IP it was given like so:
    [channel-id] OK message="<input-IP>"\n


Option 2) to use the client IP and have your web server respond based on
those subnets instead of Squid IP.

 acl clients1 src 192.168.1.0/24
 deny_info 302:http://%h/banned.html?%i clients1
 http_access deny blockedsites clients1

 acl clients2 src 192.168.2.0/24
 deny_info 302:http://%h/banned.html?%i clients2
 http_access deny blockedsites clients2


** If you really *have* to use Squid-IP, this can work with localip ACL
type instead of src. But then you have to bake each Squid-IP variation
into the deny_info URL instead of using %i.



Option 3) to use a custom error page instead of a redirect.

Place your banned.html page into /etc/squid/banned.html and either a)
write it with javascripts that pull in the right images/branding based
on client IPs.

  deny_info 403:/etc/squid/banned.html blockedsites

** Like (2) above this can use Squid-IP (via localip ACL type) if you
really have to. But with the same limitation of using different files
for branding instead of javascript for dynamic sub-resource/image fetching.


Amos


From squid3 at treenet.co.nz  Tue May  1 03:48:20 2018
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Tue, 1 May 2018 15:48:20 +1200
Subject: [squid-users] SSL accelerator
In-Reply-To: <CAPxN_PWr--ai5D+1zHD+y7P3xKRVi9=3GbjqEr3YH8H+3xUWXg@mail.gmail.com>
References: <CAPxN_PWr--ai5D+1zHD+y7P3xKRVi9=3GbjqEr3YH8H+3xUWXg@mail.gmail.com>
Message-ID: <0816f48b-cdbe-d8e0-b6f9-c1a7e708449d@treenet.co.nz>

On 01/05/18 03:37, Panagiotis Bariamis wrote:
> Hello ,
> Has anyone used ssl accelerators cards for squid under FreeBSD ?
> I want mostly to offload processors of ssl bump .
> 

Squid just uses the OpenSSL (or recently GnuTLS) APIs for the actual
encrypt/decrypt functionality. We only reimplement parts of the TLS
handshake and session management in Squid code.

So the answer to your question is more whether the TLS/SSL library Squid
is built with uses accelerators. Changing your search to focus on that
aspect may lead you to an answer better/faster.

AFAIK; OpenSSL does for most algorithms, LibreSSL likewise for those
which it has not purged yet, and GnuTLS does for some algorithms but not
others.

Amos


From squid3 at treenet.co.nz  Tue May  1 03:52:14 2018
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Tue, 1 May 2018 15:52:14 +1200
Subject: [squid-users] deny_info and squid's own IP address?
In-Reply-To: <92821481-40de-b0c9-c396-e501a15c727f@treenet.co.nz>
References: <88295b8a-aab4-c02c-5a59-a582f5215e7c@gmail.com>
 <92821481-40de-b0c9-c396-e501a15c727f@treenet.co.nz>
Message-ID: <83d17937-119d-e41e-928a-be20445b8db9@treenet.co.nz>



On 01/05/18 15:40, Amos Jeffries wrote:
> On 01/05/18 00:54, Amish wrote:
>> Hello
>>
>> I have 2 LAN interface on squid box, say department A (192.168.1.1/24)
>> and department B (192.168.2.1/24)
>>
>> I have few banned sites. Say Facebook.
>>
>> I have HTTP server (running on same server as squid) which shows custom
>> pages with custom logo based on IP address.
>>
>> When request comes for a banned site I would like client to be
>> redirected based on squid's own IP.
> 
> Firstly, is there any particular reason you are requiring it to be a
> redirect?
>  from what you have said it appears you can achieve the same outcome
> without the extra web server by using a custom error page.
> 
>>
>> Something like this:
>>
>> acl blockedsites url_regex facebook
>> http_access deny blockedsites
>> deny_info http://SQUID-IP/banned.html blockedsites
>>
>> I need SQUID-IP to be replaced by 192.168.1.1 or 192.168.2.1 depending
>> on the IP on which connection came to.
>>
> 
> Secondly, I think you are probably looking at this from the wrong
> direction. With the topology you have described each of these "Squid
> IPs" is actually just the IP facing a certain client subnet. So the
> client subnet is what you want to be detecting, not the specific Squid IP.
> 
> 
> Thirdly, on the issue of %h - the Squid hostname is *required* to
> resolve in DNS explicitly so clients can access things like these URLs.
> If your network and DNS is configured correctly each client subnet
> should resolve that hostname to the relevant IP which you are trying to
> "pass" to the web server in your redirect URL. So they will naturally
> (and only) connect to the web server (or Squid itself) using the right
> IP anyway - the web server should be able to detect what it needs from
> its own inbound TCP/IP connection instead of using raw-IPs in the traffic.
> 
> 
> There are three options available to work around broken DNS:
> 
> 
> Option 1) to do exactly (and only) what you asked for.
> 
> Currently this can be done with an external helper:
> 
>  external_acl_type getIp concurrency=100 %MYADDR /path/to/script
>  deny_info 302:http://%et/banned.html getIp
> 
> where the script just echos back to Squid the IP it was given like so:
>     [channel-id] OK message="<input-IP>"\n
> 
> 
> Option 2) to use the client IP and have your web server respond based on
> those subnets instead of Squid IP.
> 
>  acl clients1 src 192.168.1.0/24
>  deny_info 302:http://%h/banned.html?%i clients1
>  http_access deny blockedsites clients1
> 
>  acl clients2 src 192.168.2.0/24
>  deny_info 302:http://%h/banned.html?%i clients2
>  http_access deny blockedsites clients2
> 
> 
> ** If you really *have* to use Squid-IP, this can work with localip ACL
> type instead of src. But then you have to bake each Squid-IP variation
> into the deny_info URL instead of using %i.
> 
> 
> 
> Option 3) to use a custom error page instead of a redirect.
> 
> Place your banned.html page into /etc/squid/banned.html and either a)
> write it with javascripts that pull in the right images/branding based
> on client IPs.

or b) use multiple pages with different branding.

> 
>   deny_info 403:/etc/squid/banned.html blockedsites
> 
> ** Like (2) above this can use Squid-IP (via localip ACL type) if you
> really have to. But with the same limitation of using different files
> for branding instead of javascript for dynamic sub-resource/image fetching.
> 


Amos


From CHerzog at IntouchHealth.com  Tue May  1 06:04:50 2018
From: CHerzog at IntouchHealth.com (Cody Herzog)
Date: Tue, 1 May 2018 06:04:50 +0000
Subject: [squid-users] Question about shutdown_lifetime behavior.
Message-ID: <2AF4E0DB5D20734E8939C5EF204ACBAC01AEC3DD65@ITHMail2.ith.local>

Hello.

I have a question about shutdown_lifetime:

http://www.squid-cache.org/Doc/config/shutdown_lifetime/

Does Squid always wait the full amount of time before shutting down, even after all active connections have closed?

Based on my testing, it seems like it does.

However, I found some documentation which indicates that Squid should close as soon as all active connections have closed:

https://www.safaribooksonline.com/library/view/squid-the-definitive/0596001622/re91.html

"Squid finally exits when all client connections have been closed or when this timeout occurs."

Is that the expected behavior, or will Squid always stay open for the full timeout, as I'm observing in my testing?

I searched the FAQ and the Internet at large, but couldn't find a definitive answer.

I'm running Squid 3.5.27 on Ubuntu 16.04.

If Squid does not have an internal mechanism to complete the shutdown when all active connections have closed, then I may have to create my own based on polling with 'netstat'.

Thanks.


From squid3 at treenet.co.nz  Tue May  1 06:25:39 2018
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Tue, 1 May 2018 18:25:39 +1200
Subject: [squid-users] Question about shutdown_lifetime behavior.
In-Reply-To: <2AF4E0DB5D20734E8939C5EF204ACBAC01AEC3DD65@ITHMail2.ith.local>
References: <2AF4E0DB5D20734E8939C5EF204ACBAC01AEC3DD65@ITHMail2.ith.local>
Message-ID: <fd7793a8-5761-2cdc-6154-bd9e8a9ee0a0@treenet.co.nz>

On 01/05/18 18:04, Cody Herzog wrote:
> Hello.
> 
> I have a question about shutdown_lifetime:
> 
> http://www.squid-cache.org/Doc/config/shutdown_lifetime/
> 
> Does Squid always wait the full amount of time before shutting down, even after all active connections have closed?

For now yes. Long-term the plan is to have it exit if no clients are
connected, but we have not yet finalized how that is to work internally.


> 
> I'm running Squid 3.5.27 on Ubuntu 16.04.
> 
> If Squid does not have an internal mechanism to complete the shutdown when all active connections have closed, then I may have to create my own based on polling with 'netstat'.
> 

You can try. Squid maintains a number of helpers though which may still
be doing things with sockets after clients are gone. Besides dev time
detecting when all that is completed is the largest blocker at present
to the long-term project.

If you really need a fast shutdown feel free to set the lifetime config
value to a shorter time, or simply call "squid -k shutdown" (or
equivalent init script command).

The "-k shutdown" behaviour is; on first use to begin the shutdown
period, on second call to trigger the end of shutdown as if
shutdown_lifetime was reached.


Amos


From anon.amish at gmail.com  Tue May  1 07:44:32 2018
From: anon.amish at gmail.com (Amish)
Date: Tue, 1 May 2018 13:14:32 +0530
Subject: [squid-users] deny_info and squid's own IP address?
In-Reply-To: <92821481-40de-b0c9-c396-e501a15c727f@treenet.co.nz>
References: <88295b8a-aab4-c02c-5a59-a582f5215e7c@gmail.com>
 <92821481-40de-b0c9-c396-e501a15c727f@treenet.co.nz>
Message-ID: <12a68dab-dcde-7c31-0466-ce3eb98ae8a8@gmail.com>

Hello,

First of thanks a lot for taking your time out for replying to my query.

My replies are inline.

On Tuesday 01 May 2018 09:10 AM, Amos Jeffries wrote:
> On 01/05/18 00:54, Amish wrote:
>> Hello
>>
>> I have 2 LAN interface on squid box, say department A (192.168.1.1/24)
>> and department B (192.168.2.1/24)
>>
>> I have few banned sites. Say Facebook.
>>
>> I have HTTP server (running on same server as squid) which shows custom
>> pages with custom logo based on IP address.
>>
>> When request comes for a banned site I would like client to be
>> redirected based on squid's own IP.
> Firstly, is there any particular reason you are requiring it to be a
> redirect?
>   from what you have said it appears you can achieve the same outcome
> without the extra web server by using a custom error page.

No I cant use custom error page as Javascript will leak the IP range of 
department A to department B.
(I had simplified my example, its actually two companies and not two 
departments infact I have 4-5 companies/subnets)

> Thirdly, on the issue of %h - the Squid hostname is *required* to
> resolve in DNS explicitly so clients can access things like these URLs.
> If your network and DNS is configured correctly each client subnet
> should resolve that hostname to the relevant IP which you are trying to
> "pass" to the web server in your redirect URL. So they will naturally
> (and only) connect to the web server (or Squid itself) using the right
> IP anyway - the web server should be able to detect what it needs from
> its own inbound TCP/IP connection instead of using raw-IPs in the traffic.
>
Some company uses OpenDNS, other Cloudflare, other Google etc.

So DNS will not resolve the hostname to same as %MYADDR.

> There are three options available to work around broken DNS:
>
>
> Option 1) to do exactly (and only) what you asked for.
>
> Currently this can be done with an external helper:
>
>   external_acl_type getIp concurrency=100 %MYADDR /path/to/script
>   deny_info 302:http://%et/banned.html getIp
>
> where the script just echos back to Squid the IP it was given like so:
>      [channel-id] OK message="<input-IP>"\n
>

Based on documentation of FORMAT for deny_info, I think you mean %o and 
not %et

Also will this "message" be available if I change by http_access line to:
deny_info 302:http://%o/banned.html blockedsites
http_access deny blockedsites getIp

will "message" of getIp be available to deny_info of blockedsites?

I will give this a try*, **however please see the end of the e-mail for 
a feature request.*

> Option 2) to use the client IP and have your web server respond based on
> those subnets instead of Squid IP.
>
>   acl clients1 src 192.168.1.0/24
>   deny_info 302:http://%h/banned.html?%i clients1
>   http_access deny blockedsites clients1
>
>   acl clients2 src 192.168.2.0/24
>   deny_info 302:http://%h/banned.html?%i clients2
>   http_access deny blockedsites clients2
>
>
> ** If you really *have* to use Squid-IP, this can work with localip ACL
> type instead of src. But then you have to bake each Squid-IP variation
> into the deny_info URL instead of using %i.
>

I will have to do this for each company. But I would like to keep 
squid.conf simple and minimal.

>
> Option 3) to use a custom error page instead of a redirect.
>
> Place your banned.html page into /etc/squid/banned.html and either a)
> write it with javascripts that pull in the right images/branding based
> on client IPs.
>
>    deny_info 403:/etc/squid/banned.html blockedsites
>
> ** Like (2) above this can use Squid-IP (via localip ACL type) if you
> really have to. But with the same limitation of using different files
> for branding instead of javascript for dynamic sub-resource/image fetching.

As stated earlier, this would leak IP range information.


_*Feature request:*_
Can we have the following switch-case in file errorpage.cc?

Source: 
https://github.com/squid-cache/squid/blob/master/src/errorpage.cc#L857

Currently case 'I' (capital i) for building_deny_info_url returns string 
"[unknown]"

Can it be modified to return "interface" address? i.e. same as MYADDR

I believe it would be just few (may be one) line change in code.

I can create a PR if required but can you or someone guide me on how to 
fetch MYADDR?

After this feature - all I would need to do is:

deny_info http://%I/banned.html blockedsites

Thank you again for your help.

Amish

>
>
> Amos
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20180501/5041253b/attachment.htm>

From squid3 at treenet.co.nz  Tue May  1 09:11:33 2018
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Tue, 1 May 2018 21:11:33 +1200
Subject: [squid-users] deny_info and squid's own IP address?
In-Reply-To: <12a68dab-dcde-7c31-0466-ce3eb98ae8a8@gmail.com>
References: <88295b8a-aab4-c02c-5a59-a582f5215e7c@gmail.com>
 <92821481-40de-b0c9-c396-e501a15c727f@treenet.co.nz>
 <12a68dab-dcde-7c31-0466-ce3eb98ae8a8@gmail.com>
Message-ID: <e466cd2d-02ac-7723-1fec-b631652dfcb8@treenet.co.nz>

On 01/05/18 19:44, Amish wrote:
> Hello,
> 
> First of thanks a lot for taking your time out for replying to my query.
> 
> My replies are inline.
> 
> On Tuesday 01 May 2018 09:10 AM, Amos Jeffries wrote:
>> On 01/05/18 00:54, Amish wrote:
>>> Hello
>>>
>>> I have 2 LAN interface on squid box, say department A (192.168.1.1/24)
>>> and department B (192.168.2.1/24)
>>>
>>> I have few banned sites. Say Facebook.
>>>
>>> I have HTTP server (running on same server as squid) which shows custom
>>> pages with custom logo based on IP address.
>>>
>>> When request comes for a banned site I would like client to be
>>> redirected based on squid's own IP.
>> Firstly, is there any particular reason you are requiring it to be a
>> redirect?
>>  from what you have said it appears you can achieve the same outcome
>> without the extra web server by using a custom error page.
> 
> No I cant use custom error page as Javascript will leak the IP range of
> department A to department B.
> (I had simplified my example, its actually two companies and not two
> departments infact I have 4-5 companies/subnets)
> 
>> Thirdly, on the issue of %h - the Squid hostname is *required* to
>> resolve in DNS explicitly so clients can access things like these URLs.
>> If your network and DNS is configured correctly each client subnet
>> should resolve that hostname to the relevant IP which you are trying to
>> "pass" to the web server in your redirect URL. So they will naturally
>> (and only) connect to the web server (or Squid itself) using the right
>> IP anyway - the web server should be able to detect what it needs from
>> its own inbound TCP/IP connection instead of using raw-IPs in the traffic.
>>
> Some company uses OpenDNS, other Cloudflare, other Google etc.
> 
> So DNS will not resolve the hostname to same as %MYADDR.

I suspect something is going screwy there. How are these clients getting
to the proxy if they resolve its name to a different IP than they
connect to?


> 
>> There are three options available to work around broken DNS:
>>
>>
>> Option 1) to do exactly (and only) what you asked for.
>>
>> Currently this can be done with an external helper:
>>
>>  external_acl_type getIp concurrency=100 %MYADDR /path/to/script
>>  deny_info 302:http://%et/banned.html getIp
>>
>> where the script just echos back to Squid the IP it was given like so:
>>     [channel-id] OK message="<input-IP>"\n
>>
> 
> Based on documentation of FORMAT for deny_info, I think you mean %o and
> not %et

Ah, yes. Sorry. Getting my legacy formats mixed up.

> 
> Also will this "message" be available if I change by http_access line to:
> deny_info 302:http://%o/banned.html blockedsites
> http_access deny blockedsites getIp
> 
> will "message" of getIp be available to deny_info of blockedsites?

The message will persist as an annotation in the transaction, but only
from the point the external ACL is tested. So the deny_info has to be
attached to the external ACL or something following it.

Also, deny_info only works if it is attached to the *last* ACL named on
a line.

So:

 deny_info 302:http://%o/banned.html getIp
 http_access deny blockedsites getIp

or,

 deny_info 302:http://%o/banned.html blockedsites
 http_access deny getIp blockedsites

or,
 deny_info 302:http://%o/banned.html blockedsites
 http_access deny getIp !all
 ...
 http_access deny blockedsites


should work, but other orderings do not.


> 
> I will give this a try*, **however please see the end of the e-mail for
> a feature request.*
> 
>> Option 2) to use the client IP and have your web server respond based on
>> those subnets instead of Squid IP.
>>
>>  acl clients1 src 192.168.1.0/24
>>  deny_info 302:http://%h/banned.html?%i clients1
>>  http_access deny blockedsites clients1
>>
>>  acl clients2 src 192.168.2.0/24
>>  deny_info 302:http://%h/banned.html?%i clients2
>>  http_access deny blockedsites clients2
>>
>>
>> ** If you really *have* to use Squid-IP, this can work with localip ACL
>> type instead of src. But then you have to bake each Squid-IP variation
>> into the deny_info URL instead of using %i.
>>
> 
> I will have to do this for each company. But I would like to keep
> squid.conf simple and minimal.
> 
>>
>> Option 3) to use a custom error page instead of a redirect.
>>
>> Place your banned.html page into /etc/squid/banned.html and either a)
>> write it with javascripts that pull in the right images/branding based
>> on client IPs.
>>
>>   deny_info 403:/etc/squid/banned.html blockedsites
>>
>> ** Like (2) above this can use Squid-IP (via localip ACL type) if you
>> really have to. But with the same limitation of using different files
>> for branding instead of javascript for dynamic sub-resource/image fetching.
> 
> As stated earlier, this would leak IP range information.
> 
> 
> _*Feature request:*_
> Can we have the following switch-case in file errorpage.cc?
> 
> Source:
> https://github.com/squid-cache/squid/blob/master/src/errorpage.cc#L857
> 
> Currently case 'I' (capital i) for building_deny_info_url returns string
> "[unknown]"
> 
> Can it be modified to return "interface" address? i.e. same as MYADDR
> 
> I believe it would be just few (may be one) line change in code.
> 
> I can create a PR if required but can you or someone guide me on how to
> fetch MYADDR?

A PR is welcome, but re-using a %macro which already has a different
definition will add problems in the long-term plan of conversion to
logformat %macro codes. So picking a letter that has not yet been used
for anything would be best.

The Squid IP:port on client requests should be available to that code as
request->masterXaction->tcpClient->local , the request and tcpClient
pointers may be nil since not all transactions have a client or the
error may be about the lack of an HTTP request on the TCP connection.


Amos


From anon.amish at gmail.com  Tue May  1 11:10:30 2018
From: anon.amish at gmail.com (Amish)
Date: Tue, 1 May 2018 16:40:30 +0530
Subject: [squid-users] deny_info and squid's own IP address?
In-Reply-To: <e466cd2d-02ac-7723-1fec-b631652dfcb8@treenet.co.nz>
References: <88295b8a-aab4-c02c-5a59-a582f5215e7c@gmail.com>
 <92821481-40de-b0c9-c396-e501a15c727f@treenet.co.nz>
 <12a68dab-dcde-7c31-0466-ce3eb98ae8a8@gmail.com>
 <e466cd2d-02ac-7723-1fec-b631652dfcb8@treenet.co.nz>
Message-ID: <c77df3c5-957a-57eb-e5ff-318efe89edb4@gmail.com>

On Tuesday 01 May 2018 02:41 PM, Amos Jeffries wrote:
> On 01/05/18 19:44, Amish wrote:
>> Hello,
>>
>> First of thanks a lot for taking your time out for replying to my query.
>>
>> My replies are inline.
>>
>> On Tuesday 01 May 2018 09:10 AM, Amos Jeffries wrote:
>>> On 01/05/18 00:54, Amish wrote:
>>>> Hello
>>>>
>>>> I have 2 LAN interface on squid box, say department A (192.168.1.1/24)
>>>> and department B (192.168.2.1/24)
>>>>
>>>> I have few banned sites. Say Facebook.
>>>>
>>>> I have HTTP server (running on same server as squid) which shows custom
>>>> pages with custom logo based on IP address.
>>>>
>>>> When request comes for a banned site I would like client to be
>>>> redirected based on squid's own IP.
>>> Firstly, is there any particular reason you are requiring it to be a
>>> redirect?
>>>   from what you have said it appears you can achieve the same outcome
>>> without the extra web server by using a custom error page.
>> No I cant use custom error page as Javascript will leak the IP range of
>> department A to department B.
>> (I had simplified my example, its actually two companies and not two
>> departments infact I have 4-5 companies/subnets)
>>
>>> Thirdly, on the issue of %h - the Squid hostname is *required* to
>>> resolve in DNS explicitly so clients can access things like these URLs.
>>> If your network and DNS is configured correctly each client subnet
>>> should resolve that hostname to the relevant IP which you are trying to
>>> "pass" to the web server in your redirect URL. So they will naturally
>>> (and only) connect to the web server (or Squid itself) using the right
>>> IP anyway - the web server should be able to detect what it needs from
>>> its own inbound TCP/IP connection instead of using raw-IPs in the traffic.
>>>
>> Some company uses OpenDNS, other Cloudflare, other Google etc.
>>
>> So DNS will not resolve the hostname to same as %MYADDR.
> I suspect something is going screwy there. How are these clients getting
> to the proxy if they resolve its name to a different IP than they
> connect to?

They connect by putting IP address in Proxy setting.

>
>>> There are three options available to work around broken DNS:
>>>
>>>
>>> Option 1) to do exactly (and only) what you asked for.
>>>
>>> Currently this can be done with an external helper:
>>>
>>>   external_acl_type getIp concurrency=100 %MYADDR /path/to/script
>>>   deny_info 302:http://%et/banned.html getIp
>>>
>>> where the script just echos back to Squid the IP it was given like so:
>>>      [channel-id] OK message="<input-IP>"\n
>>>
>> Based on documentation of FORMAT for deny_info, I think you mean %o and
>> not %et
> Ah, yes. Sorry. Getting my legacy formats mixed up.
>
>> Also will this "message" be available if I change by http_access line to:
>> deny_info 302:http://%o/banned.html blockedsites
>> http_access deny blockedsites getIp
>>
>> will "message" of getIp be available to deny_info of blockedsites?
> The message will persist as an annotation in the transaction, but only
> from the point the external ACL is tested. So the deny_info has to be
> attached to the external ACL or something following it.
>
> Also, deny_info only works if it is attached to the *last* ACL named on
> a line.
>
> So:
>
>   deny_info 302:http://%o/banned.html getIp
>   http_access deny blockedsites getIp
>
> or,
>
>   deny_info 302:http://%o/banned.html blockedsites
>   http_access deny getIp blockedsites
>
> or,
>   deny_info 302:http://%o/banned.html blockedsites
>   http_access deny getIp !all
>   ...
>   http_access deny blockedsites
>
>
> should work, but other orderings do not.
>

Tried this and it works as I expect it to.
>> _*Feature request:*_
>> Can we have the following switch-case in file errorpage.cc?
>>
>> Source:
>> https://github.com/squid-cache/squid/blob/master/src/errorpage.cc#L857
>>
>> Currently case 'I' (capital i) for building_deny_info_url returns string
>> "[unknown]"
>>
>> Can it be modified to return "interface" address? i.e. same as MYADDR
>>
>> I believe it would be just few (may be one) line change in code.
>>
>> I can create a PR if required but can you or someone guide me on how to
>> fetch MYADDR?
> A PR is welcome, but re-using a %macro which already has a different
> definition will add problems in the long-term plan of conversion to
> logformat %macro codes. So picking a letter that has not yet been used
> for anything would be best.
>
> The Squid IP:port on client requests should be available to that code as
> request->masterXaction->tcpClient->local , the request and tcpClient
> pointers may be nil since not all transactions have a client or the
> error may be about the lack of an HTTP request on the TCP connection.

I chose I (capital i) as it is not used for deny_info (and not 
documented either) and also properly reflects that it means interface 
address.

Document source: http://www.squid-cache.org/Doc/config/deny_info/

%i (small i) is used for client IP address
%I (capital i) may be used for interface (own) IP address

Let me know if its ok and I would attempt to create a PR.

Thank you again.

Amish

> Amos
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users



From squid3 at treenet.co.nz  Tue May  1 14:17:42 2018
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Wed, 2 May 2018 02:17:42 +1200
Subject: [squid-users] deny_info and squid's own IP address?
In-Reply-To: <c77df3c5-957a-57eb-e5ff-318efe89edb4@gmail.com>
References: <88295b8a-aab4-c02c-5a59-a582f5215e7c@gmail.com>
 <92821481-40de-b0c9-c396-e501a15c727f@treenet.co.nz>
 <12a68dab-dcde-7c31-0466-ce3eb98ae8a8@gmail.com>
 <e466cd2d-02ac-7723-1fec-b631652dfcb8@treenet.co.nz>
 <c77df3c5-957a-57eb-e5ff-318efe89edb4@gmail.com>
Message-ID: <1fa374e3-2896-2bd9-2527-8b18bff319cd@treenet.co.nz>

On 01/05/18 23:10, Amish wrote:
> On Tuesday 01 May 2018 02:41 PM, Amos Jeffries wrote:
>> On 01/05/18 19:44, Amish wrote:
>>> Hello,
>>>
>>> First of thanks a lot for taking your time out for replying to my query.
>>>
>>> My replies are inline.
>>>
>>> On Tuesday 01 May 2018 09:10 AM, Amos Jeffries wrote:
>>>> On 01/05/18 00:54, Amish wrote:
>>>>> Hello
>>>>>
>>>>> I have 2 LAN interface on squid box, say department A (192.168.1.1/24)
>>>>> and department B (192.168.2.1/24)
>>>>>
>>>>> I have few banned sites. Say Facebook.
>>>>>
>>>>> I have HTTP server (running on same server as squid) which shows
>>>>> custom
>>>>> pages with custom logo based on IP address.
>>>>>
>>>>> When request comes for a banned site I would like client to be
>>>>> redirected based on squid's own IP.
>>>> Firstly, is there any particular reason you are requiring it to be a
>>>> redirect?
>>>> ? from what you have said it appears you can achieve the same outcome
>>>> without the extra web server by using a custom error page.
>>> No I cant use custom error page as Javascript will leak the IP range of
>>> department A to department B.
>>> (I had simplified my example, its actually two companies and not two
>>> departments infact I have 4-5 companies/subnets)
>>>
>>>> Thirdly, on the issue of %h - the Squid hostname is *required* to
>>>> resolve in DNS explicitly so clients can access things like these URLs.
>>>> If your network and DNS is configured correctly each client subnet
>>>> should resolve that hostname to the relevant IP which you are trying to
>>>> "pass" to the web server in your redirect URL. So they will naturally
>>>> (and only) connect to the web server (or Squid itself) using the right
>>>> IP anyway - the web server should be able to detect what it needs from
>>>> its own inbound TCP/IP connection instead of using raw-IPs in the
>>>> traffic.
>>>>
>>> Some company uses OpenDNS, other Cloudflare, other Google etc.
>>>
>>> So DNS will not resolve the hostname to same as %MYADDR.
>> I suspect something is going screwy there. How are these clients getting
>> to the proxy if they resolve its name to a different IP than they
>> connect to?
> 
> They connect by putting IP address in Proxy setting.

Then all their traffic goes through the proxy, which does the DNS
portion on their behalf - including the fetch for the redirection URL.

That means you can have the proxy do whatever you want with it on the
second fetch.
For example;

 http_port 3128

 acl toSquid dstdomain squid-domain.example.com
 acl banUrl urlpath_regex ^/banned.html$
 deny_info 302:http://%h/banned.html blockedsites
 http_access deny blockedsites

... the simplest way is just to pass a Forwarded header for the server
to use:

 request_header_add Forwarded "for=%>a;by=%la" toSquid banUrl

 OR, you can setup explicit hostname replacement with cache_peer
forcedomain= for each client "interface":

 acl clients1 localip 192.168.1.1
 cache_peer localhost 80 0 name=server1 originserver \
  forcedomain=192.168.1.1
 cache_peer_access server1 allow clients1 toSquid banUrl

 acl clients2 localip 192.168.2.1
 cache_peer localhost 80 0 name=server2 originserver \
  forcedomain=192.168.2.1
 cache_peer_access server2 allow clients2 toSquid banUrl



>>> _*Feature request:*_
>>> Can we have the following switch-case in file errorpage.cc?
>>>
>>> Source:
>>> https://github.com/squid-cache/squid/blob/master/src/errorpage.cc#L857
>>>
>>> Currently case 'I' (capital i) for building_deny_info_url returns string
>>> "[unknown]"
>>>
>>> Can it be modified to return "interface" address? i.e. same as MYADDR
>>>
>>> I believe it would be just few (may be one) line change in code.
>>>
>>> I can create a PR if required but can you or someone guide me on how to
>>> fetch MYADDR?
>> A PR is welcome, but re-using a %macro which already has a different
>> definition will add problems in the long-term plan of conversion to
>> logformat %macro codes. So picking a letter that has not yet been used
>> for anything would be best.
>>
>> The Squid IP:port on client requests should be available to that code as
>> request->masterXaction->tcpClient->local , the request and tcpClient
>> pointers may be nil since not all transactions have a client or the
>> error may be about the lack of an HTTP request on the TCP connection.
> 
> I chose I (capital i) as it is not used for deny_info (and not
> documented either) and also properly reflects that it means interface
> address.

The issue is that deny_info is a subset of ERR_* %macros and "%I"
already means server-IP to the Convert() function. So when the Convert()
function is replaced with the generic logformat macros we will have to
add extra code complexity to determine the use instead of adding it as
an alias for the logformat "%<a" (your data is actually %la in logformat
terms).

Since we already know that conversion is going to happen it is a bad
idea to knowingly make it harder to do. Which means picking a completely
unused letter - "AbCGjJkKnNOqQrvVXyYZ" are available, or numbers.


> 
> Document source: http://www.squid-cache.org/Doc/config/deny_info/
> 
> %i (small i) is used for client IP address
> %I (capital i) may be used for interface (own) IP address

Squid has no knowledge of "interfaces" all it has is a TCP connection,
so that definition is not consistent with what Squid has available. L
for 'local address/IP' would be better but is also already taken by
another definition.

There is not really any meaningful mapping for these one-letter codes
and has not been for years. Which is part of why the logformat
conversion is planned.


Amos


From CHerzog at IntouchHealth.com  Tue May  1 17:17:54 2018
From: CHerzog at IntouchHealth.com (Cody Herzog)
Date: Tue, 1 May 2018 17:17:54 +0000
Subject: [squid-users] Question about shutdown_lifetime behavior.
Message-ID: <2AF4E0DB5D20734E8939C5EF204ACBAC01AEC3F474@ITHMail2.ith.local>

Thanks very much for the quick response, Amos.

For my use case, I would like Squid to exit when all client connections have been closed or when the timeout occurs, whichever comes first.

My instances of Squid may be handling several persistent WebSocket connections, and I don't want to disrupt those. I will occasionally need to perform maintenance, so I want a safe way to stop Squid without disrupting user activity.

I am using a fairly simple Squid configuration, with no caching, so I suspect that I can simply monitor the number of active Squid TCP connections using 'netstat', and then execute the second shutdown command when I detect that all those connections are closed.

I've been using the following command to count the number of active Squid TCP connections of port 443, which is the only port I use:

netstat -nat | grep ".*:443.*:" | grep ESTABLISHED | wc -l

That seems to give me what I want.

Is it possible that bad things could happen by stopping Squid when I see that all the TCP connections have closed?

Thanks.

-Cody
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20180501/c0b86f39/attachment.htm>

From chip_pop at hotmail.com  Tue May  1 18:05:30 2018
From: chip_pop at hotmail.com (joseph)
Date: Tue, 1 May 2018 11:05:30 -0700 (MST)
Subject: [squid-users] missing file to patch
Message-ID: <1525197930408-0.post@n4.nabble.com>

hi i tried to patch
http://www.squid-cache.org/Versions/v5/changesets/squid-5-23da195f75b394d00ddac4fa67ce6895d96292d7.patch

file dose not exist src/ssl/stub_libsslutil.cc  should i ignore this and
consider it an extra mistake in that patch or ??
even i download latest release and dose not exist
http://www.squid-cache.org/Versions/v5/squid-5.0.0-20180426-re863656




-----
************************** 
***** Crash to the future  ****
**************************
--
Sent from: http://squid-web-proxy-cache.1019090.n4.nabble.com/Squid-Users-f1019091.html


From spinic at gmail.com  Tue May  1 19:15:37 2018
From: spinic at gmail.com (Rick Ellis)
Date: Tue, 1 May 2018 12:15:37 -0700
Subject: [squid-users] Squid 4 %R in deny_info
In-Reply-To: <966eb265-7c11-b5d9-2d94-6bb4cc2019d6@treenet.co.nz>
References: <CA+s7q3yAvRSyX8THhjeM6ub8oogX-v9MfSPXka6=61xcPmwbdA@mail.gmail.com>
 <966eb265-7c11-b5d9-2d94-6bb4cc2019d6@treenet.co.nz>
Message-ID: <CA+s7q3wj4z+E=sQp5NanMDiESmt-VNUimBreUdmR-osyO2vTDg@mail.gmail.com>

On Sat, Apr 28, 2018 at 4:20 PM, Amos Jeffries <squid3 at treenet.co.nz> wrote:

>
> That seems to be a bug. I think I can already see what is causing it, if
> you open a bug report I'll attach a test patch there for you.
>

Thanks! That fixed it.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20180501/264814f6/attachment.htm>

From squid3 at treenet.co.nz  Wed May  2 02:06:32 2018
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Wed, 2 May 2018 14:06:32 +1200
Subject: [squid-users] Question about shutdown_lifetime behavior.
In-Reply-To: <2AF4E0DB5D20734E8939C5EF204ACBAC01AEC3F474@ITHMail2.ith.local>
References: <2AF4E0DB5D20734E8939C5EF204ACBAC01AEC3F474@ITHMail2.ith.local>
Message-ID: <10982aad-a613-7d2d-6e6b-793c53698ba2@treenet.co.nz>

On 02/05/18 05:17, Cody Herzog wrote:
> Thanks very much for the quick response, Amos.
> 
> ?
> 
> For my use case, I would like Squid to exit when all client connections
> have been closed or when the timeout occurs, whichever comes first.
> 

Then Squids behaviour already matches your requirements. The "or when
timeout occurs" is shutdown_lifetime and you do not have to do anything.

> 
> My instances of Squid may be handling several persistent WebSocket
> connections, and I don't want to disrupt those. I will occasionally need
> to perform maintenance, so I want a safe way to stop Squid without
> disrupting user activity.
> 

Nod. It will also likely be handling CONNECT tunnels for other things.
Be aware that these connections can last indefinitely - some have been
known to last on a timescale of several weeks.

If your maintenance is with squid.conf or things loaded by it use "squid
-k reconfigure" instead of a restart cycle. Squid can reload its config
fine with just a pause for any active clients - your netstat approach
could be useful to pick a time with minimal connections to reload the
config.

> 
> I am using a fairly simple Squid configuration, with no caching, so I
> suspect that I can simply monitor the number of active Squid TCP
> connections using 'netstat', and then execute the second shutdown
> command when I detect that all those connections are closed.
> 

Since WebSockets is part of your situation netstat will almost certainly
not work as well as you suspect. These connections can be very
surprising in their lifetimes.


> 
> I've been using the following command to count the number of active
> Squid TCP connections of port 443, which is the only port I use:
> 
> ?
> 
> netstat -nat | grep ".*:443.*:" | grep ESTABLISHED | wc -l
> 
> ?
> 
> That seems to give me what I want.
> 

Hmm, you might be able to get more useful info from the Squid
filedescriptors report.
  squidclient mgr:filedescriptors


> ?
> 
> Is it possible that bad things could happen by stopping Squid when I see
> that all the TCP connections have closed?

Should not be any bad things if you just use -k shutdown (twice). Squid
will take the time it needs for a clean (but immediate) shutdown so long
as you only do it twice, not more and do not use "kill" command.

Amos


From anon.amish at gmail.com  Wed May  2 04:20:09 2018
From: anon.amish at gmail.com (Amish)
Date: Wed, 2 May 2018 09:50:09 +0530
Subject: [squid-users] deny_info and squid's own IP address?
In-Reply-To: <1fa374e3-2896-2bd9-2527-8b18bff319cd@treenet.co.nz>
References: <88295b8a-aab4-c02c-5a59-a582f5215e7c@gmail.com>
 <92821481-40de-b0c9-c396-e501a15c727f@treenet.co.nz>
 <12a68dab-dcde-7c31-0466-ce3eb98ae8a8@gmail.com>
 <e466cd2d-02ac-7723-1fec-b631652dfcb8@treenet.co.nz>
 <c77df3c5-957a-57eb-e5ff-318efe89edb4@gmail.com>
 <1fa374e3-2896-2bd9-2527-8b18bff319cd@treenet.co.nz>
Message-ID: <d089fb6f-a425-8d84-ff6f-a1b1fbdd8e27@gmail.com>



On Tuesday 01 May 2018 07:47 PM, Amos Jeffries wrote:
> On 01/05/18 23:10, Amish wrote:
>> On Tuesday 01 May 2018 02:41 PM, Amos Jeffries wrote:
>>> On 01/05/18 19:44, Amish wrote:
>>>> Hello,
>>>>
>>>> First of thanks a lot for taking your time out for replying to my query.
>>>>
>>>> My replies are inline.
>>>>
>>>> On Tuesday 01 May 2018 09:10 AM, Amos Jeffries wrote:
>>>>> On 01/05/18 00:54, Amish wrote:
>>>>>> Hello
>>>>>>
>>>>>> I have 2 LAN interface on squid box, say department A (192.168.1.1/24)
>>>>>> and department B (192.168.2.1/24)
>>>>>>
>>>>>> I have few banned sites. Say Facebook.
>>>>>>
>>>>>> I have HTTP server (running on same server as squid) which shows
>>>>>> custom
>>>>>> pages with custom logo based on IP address.
>>>>>>
>>>>>> When request comes for a banned site I would like client to be
>>>>>> redirected based on squid's own IP.
>>>>> Firstly, is there any particular reason you are requiring it to be a
>>>>> redirect?
>>>>>  ? from what you have said it appears you can achieve the same outcome
>>>>> without the extra web server by using a custom error page.
>>>> No I cant use custom error page as Javascript will leak the IP range of
>>>> department A to department B.
>>>> (I had simplified my example, its actually two companies and not two
>>>> departments infact I have 4-5 companies/subnets)
>>>>
>>>>> Thirdly, on the issue of %h - the Squid hostname is *required* to
>>>>> resolve in DNS explicitly so clients can access things like these URLs.
>>>>> If your network and DNS is configured correctly each client subnet
>>>>> should resolve that hostname to the relevant IP which you are trying to
>>>>> "pass" to the web server in your redirect URL. So they will naturally
>>>>> (and only) connect to the web server (or Squid itself) using the right
>>>>> IP anyway - the web server should be able to detect what it needs from
>>>>> its own inbound TCP/IP connection instead of using raw-IPs in the
>>>>> traffic.
>>>>>
>>>> Some company uses OpenDNS, other Cloudflare, other Google etc.
>>>>
>>>> So DNS will not resolve the hostname to same as %MYADDR.
>>> I suspect something is going screwy there. How are these clients getting
>>> to the proxy if they resolve its name to a different IP than they
>>> connect to?
>> They connect by putting IP address in Proxy setting.
> Then all their traffic goes through the proxy, which does the DNS
> portion on their behalf - including the fetch for the redirection URL.
>
> That means you can have the proxy do whatever you want with it on the
> second fetch.
> For example;
>
>   http_port 3128
>
>   acl toSquid dstdomain squid-domain.example.com
>   acl banUrl urlpath_regex ^/banned.html$
>   deny_info 302:http://%h/banned.html blockedsites
>   http_access deny blockedsites
>
> ... the simplest way is just to pass a Forwarded header for the server
> to use:
>
>   request_header_add Forwarded "for=%>a;by=%la" toSquid banUrl
>
>   OR, you can setup explicit hostname replacement with cache_peer
> forcedomain= for each client "interface":
>
>   acl clients1 localip 192.168.1.1
>   cache_peer localhost 80 0 name=server1 originserver \
>    forcedomain=192.168.1.1
>   cache_peer_access server1 allow clients1 toSquid banUrl
>
>   acl clients2 localip 192.168.2.1
>   cache_peer localhost 80 0 name=server2 originserver \
>    forcedomain=192.168.2.1
>   cache_peer_access server2 allow clients2 toSquid banUrl
>
That all makes it complicated and I prefer simpler solution. (which I 
now know)

Some clients are intercepted too. (so they may not have proxy configured 
in browser)
>>>> _*Feature request:*_
>>>> Can we have the following switch-case in file errorpage.cc?
>>>>
>>>> Source:
>>>> https://github.com/squid-cache/squid/blob/master/src/errorpage.cc#L857
>>>>
>>>> Currently case 'I' (capital i) for building_deny_info_url returns string
>>>> "[unknown]"
>>>>
>>>> Can it be modified to return "interface" address? i.e. same as MYADDR
>>>>
>>>> I believe it would be just few (may be one) line change in code.
>>>>
>>>> I can create a PR if required but can you or someone guide me on how to
>>>> fetch MYADDR?
>>> A PR is welcome, but re-using a %macro which already has a different
>>> definition will add problems in the long-term plan of conversion to
>>> logformat %macro codes. So picking a letter that has not yet been used
>>> for anything would be best.
>>>
>>> The Squid IP:port on client requests should be available to that code as
>>> request->masterXaction->tcpClient->local , the request and tcpClient
>>> pointers may be nil since not all transactions have a client or the
>>> error may be about the lack of an HTTP request on the TCP connection.
>> I chose I (capital i) as it is not used for deny_info (and not
>> documented either) and also properly reflects that it means interface
>> address.
Does request->masterXaction->tcpClient->local hold Squid IP incase of 
intercepted traffic too?

> The issue is that deny_info is a subset of ERR_* %macros and "%I"
> already means server-IP to the Convert() function. So when the Convert()
> function is replaced with the generic logformat macros we will have to
> add extra code complexity to determine the use instead of adding it as
> an alias for the logformat "%<a" (your data is actually %la in logformat
> terms).
>
> Since we already know that conversion is going to happen it is a bad
> idea to knowingly make it harder to do. Which means picking a completely
> unused letter - "AbCGjJkKnNOqQrvVXyYZ" are available, or numbers.
>
>
>> Document source: http://www.squid-cache.org/Doc/config/deny_info/
>>
>> %i (small i) is used for client IP address
>> %I (capital i) may be used for interface (own) IP address
> Squid has no knowledge of "interfaces" all it has is a TCP connection,
> so that definition is not consistent with what Squid has available. L
> for 'local address/IP' would be better but is also already taken by
> another definition.
>
> There is not really any meaningful mapping for these one-letter codes
> and has not been for years. Which is part of why the logformat
> conversion is planned.
Yes by interface I meant - the IP on which packet landed on / redirected 
to. (which is most cases is also interface IP)
>
> Amos
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users

Thanks,

Amish.


From squid3 at treenet.co.nz  Wed May  2 04:35:39 2018
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Wed, 2 May 2018 16:35:39 +1200
Subject: [squid-users] deny_info and squid's own IP address?
In-Reply-To: <d089fb6f-a425-8d84-ff6f-a1b1fbdd8e27@gmail.com>
References: <88295b8a-aab4-c02c-5a59-a582f5215e7c@gmail.com>
 <92821481-40de-b0c9-c396-e501a15c727f@treenet.co.nz>
 <12a68dab-dcde-7c31-0466-ce3eb98ae8a8@gmail.com>
 <e466cd2d-02ac-7723-1fec-b631652dfcb8@treenet.co.nz>
 <c77df3c5-957a-57eb-e5ff-318efe89edb4@gmail.com>
 <1fa374e3-2896-2bd9-2527-8b18bff319cd@treenet.co.nz>
 <d089fb6f-a425-8d84-ff6f-a1b1fbdd8e27@gmail.com>
Message-ID: <c1acd07e-e685-7ef2-5e6a-cb4b8701994c@treenet.co.nz>

On 02/05/18 16:20, Amish wrote:
> 
> Does request->masterXaction->tcpClient->local hold Squid IP incase of
> intercepted traffic too?

The listening address (if any) will be in
request->masterXaction->squidPort->listenConn->local instead. It has no
relation to the client TCP connection and may be :: or 0.0.0.0.
 In this chain case request, squidPort, and listenConn may be nil.

Amos


From anon.amish at gmail.com  Wed May  2 15:01:56 2018
From: anon.amish at gmail.com (Amish)
Date: Wed, 2 May 2018 20:31:56 +0530
Subject: [squid-users] deny_info and squid's own IP address?
In-Reply-To: <c1acd07e-e685-7ef2-5e6a-cb4b8701994c@treenet.co.nz>
References: <88295b8a-aab4-c02c-5a59-a582f5215e7c@gmail.com>
 <92821481-40de-b0c9-c396-e501a15c727f@treenet.co.nz>
 <12a68dab-dcde-7c31-0466-ce3eb98ae8a8@gmail.com>
 <e466cd2d-02ac-7723-1fec-b631652dfcb8@treenet.co.nz>
 <c77df3c5-957a-57eb-e5ff-318efe89edb4@gmail.com>
 <1fa374e3-2896-2bd9-2527-8b18bff319cd@treenet.co.nz>
 <d089fb6f-a425-8d84-ff6f-a1b1fbdd8e27@gmail.com>
 <c1acd07e-e685-7ef2-5e6a-cb4b8701994c@treenet.co.nz>
Message-ID: <0c4de2cf-4a30-68f3-9ffe-6c28f9ceb985@gmail.com>

On Wednesday 02 May 2018 10:05 AM, Amos Jeffries wrote:
> On 02/05/18 16:20, Amish wrote:
>> Does request->masterXaction->tcpClient->local hold Squid IP incase of
>> intercepted traffic too?
> The listening address (if any) will be in
> request->masterXaction->squidPort->listenConn->local instead. It has no
> relation to the client TCP connection and may be :: or 0.0.0.0.
>   In this chain case request, squidPort, and listenConn may be nil.
>
> Amos
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users

I am getting confused actually.

Squid 3.5
http://www.squid-cache.org/Versions/v3/3.5/cfgman/external_acl_type.html

Above says %MYADDR = Squid interface address

Squid 4 (external_acl_type uses logformat FORMATs)
And http://www.squid-cache.org/Doc/config/logformat/

This says %la = Local listening IP address the client connection was 
connected to

So description of %MYADDR and %la is different, but from source code 
(src/format/Token.cc) both appear to be same thing i.e. 
LFT_LOCAL_LISTENING_IP

But the code in Format.cc looks complicated then simple one line:

 ??????? case LFT_LOCAL_LISTENING_IP: {
 ??????????? // avoid logging a dash if we have reliable info
 ??????????? const bool interceptedAtKnownPort = al->request ?
(al->request->flags.interceptTproxy ||
al->request->flags.intercepted) && al->cache.port != NULL :
 ??????????????????????????????????????????????? false;
 ??????????? if (interceptedAtKnownPort) {
 ??????????????? const bool portAddressConfigured = 
!al->cache.port->s.isAnyAddr();
 ??????????????? if (portAddressConfigured)
 ??????????????????? out = al->cache.port->s.toStr(tmp, sizeof(tmp));
 ??????????? } else if (al->tcpClient != NULL)
 ??????????????? out = al->tcpClient->local.toStr(tmp, sizeof(tmp));
 ??????? }

So which is right way? Above code which considers interception too?

OR one of the lines below?

request->masterXaction->tcpClient->local
request->masterXaction->squidPort->listenConn->local

i.e. something like (in errorpage.cc)
case 'A':
 ??? if (request && request->masterXaction->squidPort && 
request->masterXaction->squidPort->listenConn)
 ??????? mb.appendf("%s", 
request->masterXaction->squidPort->listenConn->local.toStr(ntoabuf,MAX_IPSTRLEN));
 ??? else
 ??????? mb.appendf("%s", getMyHostname());


Note: Here %A would be same as %h if required information is not available.

Amish.

PS: Off for few days vacation - so may not be able to reply
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20180502/9176e421/attachment.htm>

From squid3 at treenet.co.nz  Wed May  2 15:41:28 2018
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 3 May 2018 03:41:28 +1200
Subject: [squid-users] deny_info and squid's own IP address?
In-Reply-To: <0c4de2cf-4a30-68f3-9ffe-6c28f9ceb985@gmail.com>
References: <88295b8a-aab4-c02c-5a59-a582f5215e7c@gmail.com>
 <92821481-40de-b0c9-c396-e501a15c727f@treenet.co.nz>
 <12a68dab-dcde-7c31-0466-ce3eb98ae8a8@gmail.com>
 <e466cd2d-02ac-7723-1fec-b631652dfcb8@treenet.co.nz>
 <c77df3c5-957a-57eb-e5ff-318efe89edb4@gmail.com>
 <1fa374e3-2896-2bd9-2527-8b18bff319cd@treenet.co.nz>
 <d089fb6f-a425-8d84-ff6f-a1b1fbdd8e27@gmail.com>
 <c1acd07e-e685-7ef2-5e6a-cb4b8701994c@treenet.co.nz>
 <0c4de2cf-4a30-68f3-9ffe-6c28f9ceb985@gmail.com>
Message-ID: <0e5acedc-5297-9439-79a4-d3b8d5d58a1c@treenet.co.nz>

On 03/05/18 03:01, Amish wrote:
> On Wednesday 02 May 2018 10:05 AM, Amos Jeffries wrote:
>> On 02/05/18 16:20, Amish wrote:
>>> Does request->masterXaction->tcpClient->local hold Squid IP incase of
>>> intercepted traffic too?
>> The listening address (if any) will be in
>> request->masterXaction->squidPort->listenConn->local instead. It has no
>> relation to the client TCP connection and may be :: or 0.0.0.0.
>>  In this chain case request, squidPort, and listenConn may be nil.
>>
>> Amos
> 
> I am getting confused actually.
> 
> Squid 3.5
> http://www.squid-cache.org/Versions/v3/3.5/cfgman/external_acl_type.html
> 
> Above says %MYADDR = Squid interface address
> 
> Squid 4 (external_acl_type uses logformat FORMATs)
> And http://www.squid-cache.org/Doc/config/logformat/
> 
> This says %la = Local listening IP address the client connection was
> connected to
> 
> So description of %MYADDR and %la is different, but from source code
> (src/format/Token.cc) both appear to be same thing i.e.
> LFT_LOCAL_LISTENING_IP
> 

Yes.

> But the code in Format.cc looks complicated then simple one line:
> 
> ??????? case LFT_LOCAL_LISTENING_IP: {
> ??????????? // avoid logging a dash if we have reliable info
> ??????????? const bool interceptedAtKnownPort = al->request ?
> ???????????????????????????????????????????????
> (al->request->flags.interceptTproxy ||
> ????????????????????????????????????????????????
> al->request->flags.intercepted) && al->cache.port != NULL :
> ??????????????????????????????????????????????? false;
> ??????????? if (interceptedAtKnownPort) {
> ??????????????? const bool portAddressConfigured =
> !al->cache.port->s.isAnyAddr();
> ??????????????? if (portAddressConfigured)
> ??????????????????? out = al->cache.port->s.toStr(tmp, sizeof(tmp));
> ??????????? } else if (al->tcpClient != NULL)
> ??????????????? out = al->tcpClient->local.toStr(tmp, sizeof(tmp));
> ??????? }
> 
> So which is right way? Above code which considers interception too?

The above is the right logic to work with both types of traffic. Except
that code is working from an 'ALE' object "al". The error page code you
are working with does not currently have access to that.

NP: The TCP connection data is more reliable (never being :: or
0.0.0.0). But when interception is happening the TCP details are only
about client and server, not Squid - so the port config has to be used.

The Convert() equivalent of "al->request" is just "request".

The Convert() equivalent of "al->tcpClient" is
"request->masterXaction->tcpClient".

The Convert() equivalent of "al->cache.port" is
"request->masterXaction->squidPort".

> 
> OR one of the lines below?
> 
> request->masterXaction->tcpClient->local
> request->masterXaction->squidPort->listenConn->local
> 

These are the variables where you find the data. You still have to use
the logic from (or similar to) "case LFT_LOCAL_LISTENING_IP" to produce
the right value from them for both intercepted and non-intercepted traffic.


> i.e. something like (in errorpage.cc)
> case 'A':
> ??? if (request && request->masterXaction->squidPort &&
> request->masterXaction->squidPort->listenConn)
> ??????? mb.appendf("%s",
> request->masterXaction->squidPort->listenConn->local.toStr(ntoabuf,MAX_IPSTRLEN));
> ??? else
> ??????? mb.appendf("%s", getMyHostname());
> 
> 
> Note: Here %A would be same as %h if required information is not available.
> 
> Amish.
> 
> PS: Off for few days vacation - so may not be able to reply
> 

Amos


From squid3 at treenet.co.nz  Wed May  2 15:51:20 2018
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 3 May 2018 03:51:20 +1200
Subject: [squid-users] missing file to patch
In-Reply-To: <1525197930408-0.post@n4.nabble.com>
References: <1525197930408-0.post@n4.nabble.com>
Message-ID: <12ccfec2-6f23-f38e-c773-0acd4e782abf@treenet.co.nz>

On 02/05/18 06:05, joseph wrote:
> hi i tried to patch
> http://www.squid-cache.org/Versions/v5/changesets/squid-5-23da195f75b394d00ddac4fa67ce6895d96292d7.patch
> 
> file dose not exist src/ssl/stub_libsslutil.cc  should i ignore this and
> consider it an extra mistake in that patch or ??
> even i download latest release and dose not exist
> http://www.squid-cache.org/Versions/v5/squid-5.0.0-20180426-re863656
> 

It is a file that exists for uses outside those needed to build and test
the current release. So yes you can ignore it when applying the patch.

Amos


From anon.amish at gmail.com  Wed May  2 17:26:46 2018
From: anon.amish at gmail.com (Amish)
Date: Wed, 2 May 2018 22:56:46 +0530
Subject: [squid-users] deny_info and squid's own IP address?
In-Reply-To: <0e5acedc-5297-9439-79a4-d3b8d5d58a1c@treenet.co.nz>
References: <88295b8a-aab4-c02c-5a59-a582f5215e7c@gmail.com>
 <92821481-40de-b0c9-c396-e501a15c727f@treenet.co.nz>
 <12a68dab-dcde-7c31-0466-ce3eb98ae8a8@gmail.com>
 <e466cd2d-02ac-7723-1fec-b631652dfcb8@treenet.co.nz>
 <c77df3c5-957a-57eb-e5ff-318efe89edb4@gmail.com>
 <1fa374e3-2896-2bd9-2527-8b18bff319cd@treenet.co.nz>
 <d089fb6f-a425-8d84-ff6f-a1b1fbdd8e27@gmail.com>
 <c1acd07e-e685-7ef2-5e6a-cb4b8701994c@treenet.co.nz>
 <0c4de2cf-4a30-68f3-9ffe-6c28f9ceb985@gmail.com>
 <0e5acedc-5297-9439-79a4-d3b8d5d58a1c@treenet.co.nz>
Message-ID: <6ce0e07d-8ece-564e-1625-78aff3670a1b@gmail.com>



On Wednesday 02 May 2018 09:11 PM, Amos Jeffries wrote:
> On 03/05/18 03:01, Amish wrote:
>> But the code in Format.cc looks complicated then simple one line:
>>
>>  ??????? case LFT_LOCAL_LISTENING_IP: {
>>  ??????????? // avoid logging a dash if we have reliable info
>>  ??????????? const bool interceptedAtKnownPort = al->request ?
>>                                                 
>> (al->request->flags.interceptTproxy ||
>>                                                  
>> al->request->flags.intercepted) && al->cache.port != NULL :
>>  ??????????????????????????????????????????????? false;
>>  ??????????? if (interceptedAtKnownPort) {
>>  ??????????????? const bool portAddressConfigured =
>> !al->cache.port->s.isAnyAddr();
>>  ??????????????? if (portAddressConfigured)
>>  ??????????????????? out = al->cache.port->s.toStr(tmp, sizeof(tmp));
>>  ??????????? } else if (al->tcpClient != NULL)
>>  ??????????????? out = al->tcpClient->local.toStr(tmp, sizeof(tmp));
>>  ??????? }
>>
>> So which is right way? Above code which considers interception too?
> The above is the right logic to work with both types of traffic. Except
> that code is working from an 'ALE' object "al". The error page code you
> are working with does not currently have access to that.
>
> NP: The TCP connection data is more reliable (never being :: or
> 0.0.0.0). But when interception is happening the TCP details are only
> about client and server, not Squid - so the port config has to be used.
>
> The Convert() equivalent of "al->request" is just "request".
>
> The Convert() equivalent of "al->tcpClient" is
> "request->masterXaction->tcpClient".
>
> The Convert() equivalent of "al->cache.port" is
> "request->masterXaction->squidPort".
>
>> OR one of the lines below?
>>
>> request->masterXaction->tcpClient->local
>> request->masterXaction->squidPort->listenConn->local
>>
> These are the variables where you find the data. You still have to use
> the logic from (or similar to) "case LFT_LOCAL_LISTENING_IP" to produce
> the right value from them for both intercepted and non-intercepted traffic.
>

Created PR:
https://github.com/squid-cache/squid/pull/198

May be any further discussion can now continue there.

Thank you

Amish

>
> Amos
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users



From CHerzog at IntouchHealth.com  Wed May  2 21:07:35 2018
From: CHerzog at IntouchHealth.com (Cody Herzog)
Date: Wed, 2 May 2018 21:07:35 +0000
Subject: [squid-users] Question about shutdown_lifetime behavior.
Message-ID: <2AF4E0DB5D20734E8939C5EF204ACBAC01AEC439A1@ITHMail2.ith.local>

Thanks again, Amos.

>Then Squids behaviour already matches your requirements. The "or when timeout occurs" is shutdown_lifetime and you do not have to do anything.

I'm confused by this. After issuing the first shutdown command, my desired behavior is for Squid to shut itself down fully as soon as it detects that there is no more client activity. My understanding from your first response is that Squid will always wait the full timeout, regardless of whether activity seems to have stopped.

Ultimately, I ended up having to implement something custom anyway.

My clients have multiple persistent WebSocket connections to different services. Some of those services are critical, and some are not. Shutdown must be postponed until there are no more active connections to critical services. Connections to the non-critical services can last a very long time, and I don't want to postpone shutdown because of those connections.
 
To get my desired behavior, I ended up polling 'cache_object://cache.host.name/active_requests' to check if any critical requests are active, and if not, then I issue the shutdown command.

The tricky thing is that 'cache_object' cannot be queried after the first shutdown command has been issued, because Squid does not accept any new connections. Therefore, I had to find a way to prevent new client connections, while still allowing 'cache_object' to keep working. I was able to accomplish this by modifying squid.conf and issuing a 'reconfigure'.  Thankfully, the 'reconfigure' which prevents new client connections does not seem to break any of the active WebSocket connections.

So, here is my shutdown sequence:

1.) Modify config file to prevent new client connections and 'reconfigure'.
2.) Poll active requests until there are no connections to critical services.
3.) Issue the shutdown command with a small value for shutdown_lifetime.

Does that sounds reasonable?


From rousskov at measurement-factory.com  Wed May  2 21:22:40 2018
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Wed, 2 May 2018 15:22:40 -0600
Subject: [squid-users] Question about shutdown_lifetime behavior.
In-Reply-To: <2AF4E0DB5D20734E8939C5EF204ACBAC01AEC439A1@ITHMail2.ith.local>
References: <2AF4E0DB5D20734E8939C5EF204ACBAC01AEC439A1@ITHMail2.ith.local>
Message-ID: <7cd4d076-3bc8-c73b-c1f6-f136b72d07e4@measurement-factory.com>

On 05/02/2018 03:07 PM, Cody Herzog wrote:

> So, here is my shutdown sequence:
> 
> 1.) Modify config file to prevent new client connections and 'reconfigure'.
> 2.) Poll active requests until there are no connections to critical services.
> 3.) Issue the shutdown command with a small value for shutdown_lifetime.
> 
> Does that sounds reasonable?

It sounds like a reasonable (and clever!) workaround to me.

Ideally, a single "squid -k shutdown" should result in everything you
need done by Squid automatically, with a new ACL-driven directive to
identify "connections to critical services".

Please keep in mind that Squid reconfiguration is still a disruptive
action (unfortunately). If it does not usually affect your critical
services, great, but I am fairly sure it is possible to come up with
specific cases where reconfiguration kills in-progress transactions.

Alex.


From CHerzog at IntouchHealth.com  Fri May  4 04:59:16 2018
From: CHerzog at IntouchHealth.com (Cody Herzog)
Date: Fri, 4 May 2018 04:59:16 +0000
Subject: [squid-users] Question about shutdown_lifetime behavior.
Message-ID: <2AF4E0DB5D20734E8939C5EF204ACBAC01AEC464A6@ITHMail2.ith.local>

Thanks, Alex.

I do have some concerns that 'reconfigure' may cause disruptions in certain situations, but I haven't seen it yet.

Perhaps it is most likely to cause problems when connections are first being established, or when they are changing states.

It seems to do a good job of not disrupting established WebSocket connections.

One other idea I had was to use 'iptables' to prevent new TCP connections to port 443 as the first phase of shutdown. I think that would probably work, and would not have the potential bad behavior of 'reconfigure'.

For my 'reconfigure', I'm simply commenting out the https_port line which causes Squid to listen on 443.

Thanks again.


From squid at pson.org  Fri May  4 08:44:57 2018
From: squid at pson.org (Matt Pson)
Date: Fri, 4 May 2018 10:44:57 +0200 (CEST)
Subject: [squid-users] Squid logging to UDP logs multiple lines at the same
	time
Message-ID: <584668471.20243.1525423497004.JavaMail.zimbra@pson.org>

Hello,
I have set up several Squid servers running 3.5.27 on Ubuntu 16.04 LTS (behind keepalived and haproxy to create a ha/load balanced setup) and having are some problems with logging. In order to have a single logfile I decided to send the log directly using UDP to a separate server running rsyslogd. 

Each Squid is configured with:

access_log     udp://squid-logserver.domain.com:10514

The rsyslogd server is configured with:

Module (load="imudp")
# rsyslog Input Modules
input(type="imudp" port="10514" ruleset="Squid")
# Squid RulesSet
# make all squid input from the frontends end up here
ruleset(name="Squid") {
        action(type="omfile"
                file="/var/log/squid/access.log")
        }

This works quite well and logs are sent to the server which receives them and puts them in the log file as expected. But the logs are not sent line by line but instead multiple lines at the same time (the size is approx. a UDP packet of lines, ~1300 bytes) which makes the log file quite messed up.

What am I missing here? 
I want my log to be readable ;)

/Matt Pson


From squid3 at treenet.co.nz  Fri May  4 09:45:50 2018
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Fri, 4 May 2018 21:45:50 +1200
Subject: [squid-users] Squid logging to UDP logs multiple lines at the
 same time
In-Reply-To: <584668471.20243.1525423497004.JavaMail.zimbra@pson.org>
References: <584668471.20243.1525423497004.JavaMail.zimbra@pson.org>
Message-ID: <1d84dfe0-a066-24d8-d968-9588cda23a9a@treenet.co.nz>

On 04/05/18 20:44, Matt Pson wrote:
> Hello,
> I have set up several Squid servers running 3.5.27 on Ubuntu 16.04 LTS (behind keepalived and haproxy to create a ha/load balanced setup) and having are some problems with logging. In order to have a single logfile I decided to send the log directly using UDP to a separate server running rsyslogd. 
> 
> Each Squid is configured with:
> 
> access_log     udp://squid-logserver.domain.com:10514
> 
> The rsyslogd server is configured with:
> 
> Module (load="imudp")
> # rsyslog Input Modules
> input(type="imudp" port="10514" ruleset="Squid")
> # Squid RulesSet
> # make all squid input from the frontends end up here
> ruleset(name="Squid") {
>         action(type="omfile"
>                 file="/var/log/squid/access.log")
>         }
> 
> This works quite well and logs are sent to the server which receives them and puts them in the log file as expected. But the logs are not sent line by line but instead multiple lines at the same time (the size is approx. a UDP packet of lines, ~1300 bytes) which makes the log file quite messed up.

1400 bytes or your network UDP MSS - whichever is smaller. Squid tries
to fit liens in there until one does not fit then sends off the batch.

It's a bug if the buffered_logs directive and access_log buffer-size=
option do not work to control what Squid is *sending*, but UDP can also
be aggregated by the network stacks I/O buffers of both sender and
receiver machines - so make sure you check for that.

<http://www.squid-cache.org/Doc/config/buffered_logs/>
"
Note that even when buffered_logs are off, Squid may have to buffer
records if it cannot write/send them immediately due to pending I/Os
(e.g., the I/O writing the previous log record) or connectivity loss.
"

> 
> What am I missing here? 
> I want my log to be readable ;)

FWIW; The UDP module is intended for a logger which is capable of
handling the log as a raw data stream. syslog systems are not very good
at that.

You will probably find sending these to the Squid machines syslog and
having that relay them out to the remote server in the syslog format
works better than sending a raw UDP stream.

Or maybe using the TCP module and avoiding the complexity of syslog
entirely.

Amos


From uhlar at fantomas.sk  Fri May  4 10:22:52 2018
From: uhlar at fantomas.sk (Matus UHLAR - fantomas)
Date: Fri, 4 May 2018 12:22:52 +0200
Subject: [squid-users] large rock store and shared large objects
Message-ID: <20180504102252.GA13685@fantomas.sk>

Hello,

According to rock store documentation[1], the
"
Objects larger than 32,000 bytes cannot be cached when cache_dirs are shared
among workers.
"

does this still apply with large rock store[2]?

[1] https://wiki.squid-cache.org/Features/RockStore
[2] https://wiki.squid-cache.org/Features/LargeRockStore
-- 
Matus UHLAR - fantomas, uhlar at fantomas.sk ; http://www.fantomas.sk/
Warning: I wish NOT to receive e-mail advertising to this address.
Varovanie: na tuto adresu chcem NEDOSTAVAT akukolvek reklamnu postu.
A day without sunshine is like, night.


From squid3 at treenet.co.nz  Fri May  4 10:29:47 2018
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Fri, 4 May 2018 22:29:47 +1200
Subject: [squid-users] large rock store and shared large objects
In-Reply-To: <20180504102252.GA13685@fantomas.sk>
References: <20180504102252.GA13685@fantomas.sk>
Message-ID: <556c9bc2-c5a9-0fd5-d3be-c442ce45f13b@treenet.co.nz>

On 04/05/18 22:22, Matus UHLAR - fantomas wrote:
> Hello,
> 
> According to rock store documentation[1], the
> "
> Objects larger than 32,000 bytes cannot be cached when cache_dirs are
> shared
> among workers.
> "
> 
> does this still apply with large rock store[2]?
> 
> [1] https://wiki.squid-cache.org/Features/RockStore
> [2] https://wiki.squid-cache.org/Features/LargeRockStore


"Definitions

    Small: fits in one db slot (in Small Rock, the slot size is the same
as max-size and all cached entries are small).
    Large: not small.
    Huge: gigabytes in size.
"

Not being smaller than a 32KB 'slot' is the definition of "large".

Amos


From uhlar at fantomas.sk  Fri May  4 10:50:09 2018
From: uhlar at fantomas.sk (Matus UHLAR - fantomas)
Date: Fri, 4 May 2018 12:50:09 +0200
Subject: [squid-users] large rock store and shared large objects
In-Reply-To: <556c9bc2-c5a9-0fd5-d3be-c442ce45f13b@treenet.co.nz>
References: <20180504102252.GA13685@fantomas.sk>
 <556c9bc2-c5a9-0fd5-d3be-c442ce45f13b@treenet.co.nz>
Message-ID: <20180504105009.GC13685@fantomas.sk>

>On 04/05/18 22:22, Matus UHLAR - fantomas wrote:
>> Hello,
>>
>> According to rock store documentation[1], the
>> "
>> Objects larger than 32,000 bytes cannot be cached when cache_dirs are
>> shared
>> among workers.
>> "
>>
>> does this still apply with large rock store[2]?
>>
>> [1] https://wiki.squid-cache.org/Features/RockStore
>> [2] https://wiki.squid-cache.org/Features/LargeRockStore

On 04.05.18 22:29, Amos Jeffries wrote:
>"Definitions
>
>    Small: fits in one db slot (in Small Rock, the slot size is the same
>as max-size and all cached entries are small).
>    Large: not small.
>    Huge: gigabytes in size.
>"
>
>Not being smaller than a 32KB 'slot' is the definition of "large".

Thank you, but how does it answer my question?

does that mean that LargeRockStore allows to split and store objects larger
than slot size, but sharing them between workers is a different story?

-- 
Matus UHLAR - fantomas, uhlar at fantomas.sk ; http://www.fantomas.sk/
Warning: I wish NOT to receive e-mail advertising to this address.
Varovanie: na tuto adresu chcem NEDOSTAVAT akukolvek reklamnu postu.
42.7 percent of all statistics are made up on the spot. 


From squid at pson.org  Fri May  4 11:24:59 2018
From: squid at pson.org (Matt Pson)
Date: Fri, 4 May 2018 13:24:59 +0200 (CEST)
Subject: [squid-users] Squid logging to UDP logs multiple lines at the
 same time
In-Reply-To: <1d84dfe0-a066-24d8-d968-9588cda23a9a@treenet.co.nz>
References: <584668471.20243.1525423497004.JavaMail.zimbra@pson.org>
 <1d84dfe0-a066-24d8-d968-9588cda23a9a@treenet.co.nz>
Message-ID: <1416850388.20641.1525433099893.JavaMail.zimbra@pson.org>



----- Original Message -----
> From: "Amos Jeffries" <squid3 at treenet.co.nz>
> To: "squid-users" <squid-users at lists.squid-cache.org>
> Sent: Friday, 4 May, 2018 11:45:50
> Subject: Re: [squid-users] Squid logging to UDP logs multiple lines at the same time

> On 04/05/18 20:44, Matt Pson wrote:

>> 
>> This works quite well and logs are sent to the server which receives them and
>> puts them in the log file as expected. But the logs are not sent line by line
>> but instead multiple lines at the same time (the size is approx. a UDP packet
>> of lines, ~1300 bytes) which makes the log file quite messed up.
> 
> 1400 bytes or your network UDP MSS - whichever is smaller. Squid tries
> to fit liens in there until one does not fit then sends off the batch.

So in theory it could work if I set the buffer-size low enough, like 16 bytes, so the log line would never fit in the buffer and thus being sent asap?

> It's a bug if the buffered_logs directive and access_log buffer-size=
> option do not work to control what Squid is *sending*, but UDP can also
> be aggregated by the network stacks I/O buffers of both sender and
> receiver machines - so make sure you check for that.
> 
> <http://www.squid-cache.org/Doc/config/buffered_logs/>
> "
> Note that even when buffered_logs are off, Squid may have to buffer
> records if it cannot write/send them immediately due to pending I/Os
> (e.g., the I/O writing the previous log record) or connectivity loss.
> "

Thanks for the clarification. Reading "To send each log line as text data to a UDP receiver." (at http://www.squid-cache.org/Doc/config/access_log/ ) made me think that each line would be sent individually without buffering.

>> 
>> What am I missing here?
>> I want my log to be readable ;)
> 
> FWIW; The UDP module is intended for a logger which is capable of
> handling the log as a raw data stream. syslog systems are not very good
> at that.

I see. It will be very handy when we get around to make our own logging/stats engine take the input from Squid like we do with syslog messages from our firewalls. But that is in the future of another project.

> You will probably find sending these to the Squid machines syslog and
> having that relay them out to the remote server in the syslog format
> works better than sending a raw UDP stream.
> 
> Or maybe using the TCP module and avoiding the complexity of syslog
> entirely.

Yep, I will go via the syslog on the squid machines. I was trying to remove as many dependencies as possible but I guess syslog is one I can live with since it it required by other services.

Thanks for a quick and very helpful answer.

/Matt Pson


From squid3 at treenet.co.nz  Fri May  4 12:39:46 2018
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sat, 5 May 2018 00:39:46 +1200
Subject: [squid-users] large rock store and shared large objects
In-Reply-To: <20180504105009.GC13685@fantomas.sk>
References: <20180504102252.GA13685@fantomas.sk>
 <556c9bc2-c5a9-0fd5-d3be-c442ce45f13b@treenet.co.nz>
 <20180504105009.GC13685@fantomas.sk>
Message-ID: <f2907d21-53de-8c1a-3ea2-2aa2c0b51108@treenet.co.nz>

On 04/05/18 22:50, Matus UHLAR - fantomas wrote:
>> On 04/05/18 22:22, Matus UHLAR - fantomas wrote:
>>> Hello,
>>>
>>> According to rock store documentation[1], the
>>> "
>>> Objects larger than 32,000 bytes cannot be cached when cache_dirs are
>>> shared
>>> among workers.
>>> "
>>>
>>> does this still apply with large rock store[2]?
>>>
>>> [1] https://wiki.squid-cache.org/Features/RockStore
>>> [2] https://wiki.squid-cache.org/Features/LargeRockStore
> 
> On 04.05.18 22:29, Amos Jeffries wrote:
>> "Definitions
>>
>> ?? Small: fits in one db slot (in Small Rock, the slot size is the same
>> as max-size and all cached entries are small).
>> ?? Large: not small.
>> ?? Huge: gigabytes in size.
>> "
>>
>> Not being smaller than a 32KB 'slot' is the definition of "large".
> 
> Thank you, but how does it answer my question?
> 
> does that mean that LargeRockStore allows to split and store objects larger
> than slot size, but sharing them between workers is a different story?
> 

If you configure the slot size over 32KB (actually a small bit less)
then SMP workers cannot share them. Original rock store defaulted to a
slot size that would work, but you can make them larger if wanted (at
cost of SMP support).

"large rock store" is an extension to rock store which allows objects to
be split over many slots. So you don't have to increase the slot size
above that 32KB limit.

Amos


From squid3 at treenet.co.nz  Fri May  4 14:44:20 2018
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sat, 5 May 2018 02:44:20 +1200
Subject: [squid-users] Squid logging to UDP logs multiple lines at the
 same time
In-Reply-To: <1416850388.20641.1525433099893.JavaMail.zimbra@pson.org>
References: <584668471.20243.1525423497004.JavaMail.zimbra@pson.org>
 <1d84dfe0-a066-24d8-d968-9588cda23a9a@treenet.co.nz>
 <1416850388.20641.1525433099893.JavaMail.zimbra@pson.org>
Message-ID: <a0c8d49b-8239-5aff-b393-425e93df7372@treenet.co.nz>

On 04/05/18 23:24, Matt Pson wrote:
> 
> 
> ----- Original Message -----
>> From: "Amos Jeffries"
> 
>> On 04/05/18 20:44, Matt Pson wrote:
> 
>>>
>>> This works quite well and logs are sent to the server which receives them and
>>> puts them in the log file as expected. But the logs are not sent line by line
>>> but instead multiple lines at the same time (the size is approx. a UDP packet
>>> of lines, ~1300 bytes) which makes the log file quite messed up.
>>
>> 1400 bytes or your network UDP MSS - whichever is smaller. Squid tries
>> to fit liens in there until one does not fit then sends off the batch.
> 
> So in theory it could work if I set the buffer-size low enough, like 16 bytes, so the log line would never fit in the buffer and thus being sent asap?
> 

In theory, probably. But that would require patching Squid. So it would
be better to add a patch that makes the buffer-size= option work for UDP
instead of having it hard-coded.

Amos


From rousskov at measurement-factory.com  Fri May  4 16:40:00 2018
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Fri, 4 May 2018 10:40:00 -0600
Subject: [squid-users] large rock store and shared large objects
In-Reply-To: <20180504102252.GA13685@fantomas.sk>
References: <20180504102252.GA13685@fantomas.sk>
Message-ID: <34c5617e-2077-e976-2ea9-e6601556b8e9@measurement-factory.com>

On 05/04/2018 04:22 AM, Matus UHLAR - fantomas wrote:

> According to rock store documentation[1], the
> "
> Objects larger than 32,000 bytes cannot be cached when cache_dirs are
> shared among workers.
> "
> 
> does this still apply with large rock store[2]?


No, this does not apply to Large Rock. Modern rock caches share all
cached objects.

Just wanted to post a direct/simple answer for the record. What Amos has
detailed on this thread can be interpreted to match my answer AFAICT...

Alex.
P.S. This confusion is a bad side effect of using a simple wiki for
feature documentation and keeping old Feature pages around. SslBump
suffers the same fate. We need a better approach, and a documentation
volunteer to make it happen.


> [1] https://wiki.squid-cache.org/Features/RockStore
> [2] https://wiki.squid-cache.org/Features/LargeRockStore



From rightkicktech at gmail.com  Fri May  4 22:20:08 2018
From: rightkicktech at gmail.com (Alex K)
Date: Sat, 5 May 2018 01:20:08 +0300
Subject: [squid-users] Collecting squid logs to DB
Message-ID: <CABMULtLisrtCimu1xsk+K4EydUypbA2N_1XRbUkO6CtGBubRrQ@mail.gmail.com>

Hi all,

I had a previous setup on Debian 7 with squid and I was using mysar to
collect squid logs and store them to DB and provide some browsing report at
the end of the day.
Now at Debian 9, trying to upgrade the whole setup, I see that mysar does
not compile.

Checking around I found mysar-ng but this has compilation issues on Debian
9 also.
Do you suggest any tool that does this job? Does squid support logging to
DB natively? (I am using mysql/mariadb)

Some other tool I stumbled on is https://github.com/paranormal/blooper.

Thanx a bunch,
Alex
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20180505/06766b29/attachment.htm>

From squid3 at treenet.co.nz  Sat May  5 05:19:30 2018
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sat, 5 May 2018 17:19:30 +1200
Subject: [squid-users] Collecting squid logs to DB
In-Reply-To: <CABMULtLisrtCimu1xsk+K4EydUypbA2N_1XRbUkO6CtGBubRrQ@mail.gmail.com>
References: <CABMULtLisrtCimu1xsk+K4EydUypbA2N_1XRbUkO6CtGBubRrQ@mail.gmail.com>
Message-ID: <18e02953-7503-a5ee-09f9-a1415f0f9bdf@treenet.co.nz>

On 05/05/18 10:20, Alex K wrote:
> Hi all,
> 
> I had a previous setup on Debian 7 with squid and I was using mysar to
> collect squid logs and store them to DB and provide some browsing report
> at the end of the day.
> Now at Debian 9, trying to upgrade the whole setup, I see that mysar
> does not compile.
> 
> Checking around I found mysar-ng but this has compilation issues on
> Debian 9 also.
> Do you suggest any tool that does this job? Does squid support logging
> to DB natively? (I am using mysql/mariadb)
> 

Squid-3 comes with log_db_daemon helper which stores to any SQL database
in realtime. You still need something else to do the analysis of that data.
 <http://www.squid-cache.org/Versions/v3/3.5/manuals/log_db_daemon.html>


> Some other tool I stumbled on is https://github.com/paranormal/blooper.

Blooper is a fork of the logmysqldaemon (aka log_db_daemon) re-written
in ruby instead of native C/C++ code.

Amos


From squid3 at treenet.co.nz  Sat May  5 06:25:19 2018
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sat, 5 May 2018 18:25:19 +1200
Subject: [squid-users] Collecting squid logs to DB
In-Reply-To: <18e02953-7503-a5ee-09f9-a1415f0f9bdf@treenet.co.nz>
References: <CABMULtLisrtCimu1xsk+K4EydUypbA2N_1XRbUkO6CtGBubRrQ@mail.gmail.com>
 <18e02953-7503-a5ee-09f9-a1415f0f9bdf@treenet.co.nz>
Message-ID: <3ced613b-863e-80e2-b662-5ea9939737ac@treenet.co.nz>

On 05/05/18 17:19, Amos Jeffries wrote:
> On 05/05/18 10:20, Alex K wrote:
>> Hi all,
>>
>> I had a previous setup on Debian 7 with squid and I was using mysar to
>> collect squid logs and store them to DB and provide some browsing report
>> at the end of the day.
>> Now at Debian 9, trying to upgrade the whole setup, I see that mysar
>> does not compile.
>>
>> Checking around I found mysar-ng but this has compilation issues on
>> Debian 9 also.
>> Do you suggest any tool that does this job? Does squid support logging
>> to DB natively? (I am using mysql/mariadb)
>>
> 
> Squid-3 comes with log_db_daemon helper which stores to any SQL database
> in realtime. You still need something else to do the analysis of that data.
>  <http://www.squid-cache.org/Versions/v3/3.5/manuals/log_db_daemon.html>
> 
> 
>> Some other tool I stumbled on is https://github.com/paranormal/blooper.
> 
> Blooper is a fork of the logmysqldaemon (aka log_db_daemon) re-written
> in ruby instead of native C/C++ code.

Sorry, that should have been Ruby instead of Perl.

Amos


From michael.pelletier at palmbeachschools.org  Mon May  7 00:42:16 2018
From: michael.pelletier at palmbeachschools.org (Michael Pelletier)
Date: Sun, 6 May 2018 20:42:16 -0400
Subject: [squid-users] Collecting squid logs to DB
In-Reply-To: <3ced613b-863e-80e2-b662-5ea9939737ac@treenet.co.nz>
References: <CABMULtLisrtCimu1xsk+K4EydUypbA2N_1XRbUkO6CtGBubRrQ@mail.gmail.com>
 <18e02953-7503-a5ee-09f9-a1415f0f9bdf@treenet.co.nz>
 <3ced613b-863e-80e2-b662-5ea9939737ac@treenet.co.nz>
Message-ID: <CAEnCSG6GZE4TcFeDB98dFQ+PP-VtPRGmYDrUG_zoGe2oYyQL3A@mail.gmail.com>

Check out Logstash

*https://www.elastic.co/products/logstash
<https://www.elastic.co/products/logstash>*

On Sat, May 5, 2018 at 2:25 AM, Amos Jeffries <squid3 at treenet.co.nz> wrote:

> On 05/05/18 17:19, Amos Jeffries wrote:
> > On 05/05/18 10:20, Alex K wrote:
> >> Hi all,
> >>
> >> I had a previous setup on Debian 7 with squid and I was using mysar to
> >> collect squid logs and store them to DB and provide some browsing report
> >> at the end of the day.
> >> Now at Debian 9, trying to upgrade the whole setup, I see that mysar
> >> does not compile.
> >>
> >> Checking around I found mysar-ng but this has compilation issues on
> >> Debian 9 also.
> >> Do you suggest any tool that does this job? Does squid support logging
> >> to DB natively? (I am using mysql/mariadb)
> >>
> >
> > Squid-3 comes with log_db_daemon helper which stores to any SQL database
> > in realtime. You still need something else to do the analysis of that
> data.
> >  <http://www.squid-cache.org/Versions/v3/3.5/manuals/log_db_daemon.html>
> >
> >
> >> Some other tool I stumbled on is https://github.com/paranormal/blooper.
> >
> > Blooper is a fork of the logmysqldaemon (aka log_db_daemon) re-written
> > in ruby instead of native C/C++ code.
>
> Sorry, that should have been Ruby instead of Perl.
>
> Amos
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>

-- 






*Disclaimer:?*Under Florida law, e-mail addresses are public records. 
If you do not want your e-mail address released in response to a public 
records request, do not send electronic mail to this entity. Instead, 
contact this office by phone or in writing.







-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20180506/91df4105/attachment.htm>

From rightkicktech at gmail.com  Mon May  7 12:24:30 2018
From: rightkicktech at gmail.com (Alex K)
Date: Mon, 7 May 2018 15:24:30 +0300
Subject: [squid-users] Squid configuration sanity check
Message-ID: <CABMULt+atE7wPGGyQwL5OZA_PGJFWVOQTASyzvcEBnJkivX-TQ@mail.gmail.com>

Hi all,

I wanted to check with your accumulated wisdom the following squid
configuration.

The config is working both for splice or bump (by commenting/uncommenting
the respective line) using TPROXY. It is a config ported form an old
installation of squid 3.1 for the new 3.5 and although I did some cleanup I
am wondering if I am misusing any directive or missing any crucial one for
better performance or just for sake of cleanliness.

At the moment for filtering I am using squidGuard and considering to go
with ufdbGuard instead as pointed from Amos (thanx for that).

To avoid issues with some sites I am considering to use only splicing,
although this has some caveats as bumping also does. I could go with a
hybrid approach (splice some and bump all) but this sounds that this will
cause periodically more administrative overhead to sort out the sites that
need splicing.

The config has also some ACLs as an attempt to block media streaming by
those seem to not work.

The hardware running the squid is somehow small with 4 GB of RAM, 4 CPU
cores and 100 GB SSD in case one wonders.


http_port 192.168.200.1:3128 tproxy
https_port 192.168.200.1:3129 tproxy ssl-bump generate-host-certificates=on
dynamic_cert_mem_cache_size=4MB cert=/etc/squid/ssl_cert/myCA.pem

sslcrtd_program /usr/lib/squid/ssl_crtd -s /usr/local/squid/var/lib/ssl_db
-M 4MB
sslcrtd_children 5

shutdown_lifetime 5 seconds

# ACL
#acl ncsa_users proxy_auth REQUIRED
#acl all src 0.0.0.0/0.0.0.0
acl manager proto cache_object
acl localhost src 192.168.200.1/32

acl SSL_ports port 443
acl Safe_ports port 80
acl Safe_ports port 21
acl Safe_ports port 443
acl Safe_ports port 10080
acl Safe_ports port 10443
acl SSL method CONNECT
acl CONNECT method CONNECT # multiling http
#acl block_url dstdomain "/etc/squid/block_url.squid"
#acl allow_url dstdomain "/etc/squid/allow_url.squid"
acl ELAN src 192.168.200.0/24

acl QUERY urlpath_regex cgi-bin \?

# SSL
always_direct allow all

# Video Streaming ACLs
acl media rep_mime_type ^.*mms.*
acl media rep_mime_type ^.*ms-hdr.*
acl media rep_mime_type ^.*x-fcs.*
acl media rep_mime_type ^.*x-ms-asf.*
acl media2 urlpath_regex dvrplayer mediastream mms://
acl media2 urlpath_regex \.asf$ \.afx$ \.flv$ \.swf$
acl flashvideo rep_mime_type -i video/flv
acl flashvideo rep_mime_type -i video/x-flv
acl shockwave rep_mime_type -i ^application/x-shockwave-flash$
acl x-type req_mime_type -i ^application/octet-stream$
acl x-type req_mime_type -i application/octet-stream
acl x-type req_mime_type -i ^application/x-mplayer2$
acl x-type req_mime_type -i application/x-mplayer2
acl x-type req_mime_type -i ^application/x-oleobject$
acl x-type req_mime_type -i application/x-oleobject
acl x-type req_mime_type -i application/x-pncmd
acl x-type req_mime_type -i ^video/x-ms-asf$
acl x-type2 rep_mime_type -i ^application/octet-stream$
acl x-type2 rep_mime_type -i application/octet-stream
acl x-type2 rep_mime_type -i ^application/x-mplayer2$
acl x-type2 rep_mime_type -i application/x-mplayer2
acl x-type2 rep_mime_type -i ^application/x-oleobject$
acl x-type2 rep_mime_type -i application/x-oleobject
acl x-type2 rep_mime_type -i application/x-pncmd
acl x-type2 rep_mime_type -i ^video/x-ms-asf$

# Block Media Streaming
http_reply_access deny flashvideo
http_reply_access deny shockwave
http_reply_access deny media
http_reply_access deny media2
http_reply_access deny x-type
http_reply_access deny x-type2

#
http_access deny manager
http_access deny !Safe_ports
http_access deny CONNECT !SSL_ports
#http_access deny block_url
#http_access allow allow_url
http_access allow LAN
http_access allow ELAN

http_access allow localhost
#http_access allow ncsa_users
http_reply_access allow all

deny_info ERR_CUSTOM LAN ELAN media media2 flashvideo shockwave x-type
x-type2
error_directory /usr/share/squid-langpack/en

#icp_access allow all

# Logging
logfile_daemon /usr/lib/squid/log_db_daemon
access_log daemon:/127.0.0.1/squid_log/access_log/squid/squid squid
icap_log stdio:/var/log/squid/icap.log squid
cache_store_log stdio:/var/log/squid/store.log

# DNS
dns_nameservers 127.0.0.1
positive_dns_ttl 8 hours
negative_dns_ttl 30 seconds
ipcache_size 2048
ipcache_low 95
ipcache_high 97
fqdncache_size 2048

# Leave coredumps in the first cache dir
coredump_dir /var/spool/squid
cache_dir ufs /var/spool/squid 10240 16 256
minimum_object_size 0 KB
maximum_object_size 30 MB
maximum_object_size_in_memory 1024 KB

# HTTPS filtering
acl step1 at_step SslBump1

ssl_bump peek step1
ssl_bump splice all
#ssl_bump bump all

# SquidGuard
url_rewrite_program /usr/bin/squidGuard -c /etc/squidguard/squidGuard.conf
url_rewrite_children 5


Your input is highly appreciated.

Alex
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20180507/0787a525/attachment.htm>

From squid3 at treenet.co.nz  Mon May  7 16:30:02 2018
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Tue, 8 May 2018 04:30:02 +1200
Subject: [squid-users] Squid configuration sanity check
In-Reply-To: <CABMULt+atE7wPGGyQwL5OZA_PGJFWVOQTASyzvcEBnJkivX-TQ@mail.gmail.com>
References: <CABMULt+atE7wPGGyQwL5OZA_PGJFWVOQTASyzvcEBnJkivX-TQ@mail.gmail.com>
Message-ID: <3a048ca2-72c5-956f-e236-dd459d34e1fb@treenet.co.nz>

On 08/05/18 00:24, Alex K wrote:
> Hi all,
> 
> I wanted to check with your accumulated wisdom the following squid
> configuration.
> 
> The config is working both for splice or bump (by
> commenting/uncommenting the respective line) using TPROXY. It is a
> config ported form an old installation of squid 3.1 for the new 3.5 and
> although I did some cleanup I am wondering if I am misusing any
> directive or missing any crucial one for better performance or just for
> sake of cleanliness.
> 
> At the moment for filtering I am using squidGuard and considering to go
> with ufdbGuard instead as pointed from Amos (thanx for that).
> ?
> To avoid issues with some sites I am considering to use only splicing,
> although this has some caveats as bumping also does. I could go with a
> hybrid approach (splice some and bump all) but this sounds that this
> will cause periodically more administrative overhead to sort out the
> sites that need splicing.
> 
> The config has also some ACLs as an attempt to block media streaming by
> those seem to not work.

The ACL checking for mms:// URL will not work because MMS protocol is
not HTTP. Any client using that protocol will not be going through
Squid. So quite likely none of the other checks will work for its
non-proxied traffic either.

"working" can also depend on what you are looking at. Your rules are
only blocking *reply* access. Which means only that the client does not
get the response delivered. It still gets fetched from the server -
maybe in full. So checking your logs etc can still show things arriving
and lots of bandwidth usage.

The urlpath and req_mime_type can be checked in http_access instead to
block those requests from ever happening. That MAY work better, but no
guarantees.



> 
> The hardware running the squid is somehow small with 4 GB of RAM, 4 CPU
> cores and 100 GB SSD in case one wonders.
> 
> 
> http_port 192.168.200.1:3128 tproxy
> https_port 192.168.200.1:3129 tproxy \
>   ssl-bump generate-host-certificates=on dynamic_cert_mem_cache_size=4MB \
>   cert=/etc/squid/ssl_cert/myCA.pem
> 
> sslcrtd_program /usr/lib/squid/ssl_crtd -s
> /usr/local/squid/var/lib/ssl_db -M 4MB
> sslcrtd_children 5
> 
> shutdown_lifetime 5 seconds
> 
> # ACL
> #acl ncsa_users proxy_auth REQUIRED
> #acl all src 0.0.0.0/0.0.0.0
> acl manager proto cache_object

'manager' ACL is now built-in, and has a different type signature. The
above needs to be removed. Same with 'all'. It is not a good idea to
leave them even commented out because the old definitions are no longer
true.


> acl localhost src 192.168.200.1/32

192.168.200.1 is assigned to your lo interface?

> 
> acl SSL_ports port 443
> acl Safe_ports port 80
> acl Safe_ports port 21
> acl Safe_ports port 443
> acl Safe_ports port 10080
> acl Safe_ports port 10443
> acl SSL method CONNECT

The above can be quite deceptive,

> acl CONNECT method CONNECT # multiling http
> #acl block_url dstdomain "/etc/squid/block_url.squid"
> #acl allow_url dstdomain "/etc/squid/allow_url.squid"
> acl ELAN src 192.168.200.0/24
> 
> acl QUERY urlpath_regex cgi-bin \?

The QUERY is not being used. It is also no longer necessary so can be
removed.

> 
> # SSL
> always_direct allow all

That should not be. You do not have any cache_peer configured.

> 
> # Video Streaming ACLs
> acl media rep_mime_type ^.*mms.*
> acl media rep_mime_type ^.*ms-hdr.*
> acl media rep_mime_type ^.*x-fcs.*
> acl media rep_mime_type ^.*x-ms-asf.*
> acl media2 urlpath_regex dvrplayer mediastream mms://
> acl media2 urlpath_regex \.asf$ \.afx$ \.flv$ \.swf$
> acl flashvideo rep_mime_type -i video/flv
> acl flashvideo rep_mime_type -i video/x-flv
> acl shockwave rep_mime_type -i ^application/x-shockwave-flash$

> acl x-type req_mime_type -i ^application/octet-stream$
> acl x-type req_mime_type -i application/octet-stream

All the lines like the two above are duplicates.
The "^foo$" pattern is a sub-set of "foo" pattern.


> acl x-type req_mime_type -i ^application/x-mplayer2$
> acl x-type req_mime_type -i application/x-mplayer2
> acl x-type req_mime_type -i ^application/x-oleobject$
> acl x-type req_mime_type -i application/x-oleobject
> acl x-type req_mime_type -i application/x-pncmd
> acl x-type req_mime_type -i ^video/x-ms-asf$
> acl x-type2 rep_mime_type -i ^application/octet-stream$
> acl x-type2 rep_mime_type -i application/octet-stream
> acl x-type2 rep_mime_type -i ^application/x-mplayer2$
> acl x-type2 rep_mime_type -i application/x-mplayer2
> acl x-type2 rep_mime_type -i ^application/x-oleobject$
> acl x-type2 rep_mime_type -i application/x-oleobject
> acl x-type2 rep_mime_type -i application/x-pncmd
> acl x-type2 rep_mime_type -i ^video/x-ms-asf$
> 
> # Block Media Streaming
> http_reply_access deny flashvideo
> http_reply_access deny shockwave
> http_reply_access deny media
> http_reply_access deny media2
> http_reply_access deny x-type
> http_reply_access deny x-type2
> 
> #
> http_access deny manager
> http_access deny !Safe_ports
> http_access deny CONNECT !SSL_ports

FYI: current best-practice recommendation is to place the manager access
line down here after the faster port checks.

> #http_access deny block_url
> #http_access allow allow_url
> http_access allow LAN
> http_access allow ELAN
> 
> http_access allow localhost
> #http_access allow ncsa_users
> http_reply_access allow all

This http_reply_access line should be up with the others so it does not
fool anyone into thinking its placement here with the http_access lines
has any meaning.

> 
> deny_info ERR_CUSTOM LAN ELAN media media2 flashvideo shockwave x-type
> x-type2
> error_directory /usr/share/squid-langpack/en
> 
> #icp_access allow all
> 
> # Logging
> logfile_daemon /usr/lib/squid/log_db_daemon
> access_log daemon:/127.0.0.1/squid_log/access_log/squid/squid squid
> icap_log stdio:/var/log/squid/icap.log squid
> cache_store_log stdio:/var/log/squid/store.log
> 
> # DNS
> dns_nameservers 127.0.0.1
> positive_dns_ttl 8 hours
> negative_dns_ttl 30 seconds
> ipcache_size 2048
> ipcache_low 95
> ipcache_high 97
> fqdncache_size 2048
> 
> # Leave coredumps in the first cache dir
> coredump_dir /var/spool/squid
> cache_dir ufs /var/spool/squid 10240 16 256
> minimum_object_size 0 KB
> maximum_object_size 30 MB
> maximum_object_size_in_memory 1024 KB
> 
> # HTTPS filtering
> acl step1 at_step SslBump1
> 
> ssl_bump peek step1
> ssl_bump splice all
> #ssl_bump bump all
> 
> # SquidGuard
> url_rewrite_program /usr/bin/squidGuard -c /etc/squidguard/squidGuard.conf
> url_rewrite_children 5
> 
> 
> Your input is highly appreciated.
> 
> Alex
> 
> 
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
> 


From rightkicktech at gmail.com  Mon May  7 16:56:57 2018
From: rightkicktech at gmail.com (Alex K)
Date: Mon, 7 May 2018 19:56:57 +0300
Subject: [squid-users] Squid configuration sanity check
In-Reply-To: <3a048ca2-72c5-956f-e236-dd459d34e1fb@treenet.co.nz>
References: <CABMULt+atE7wPGGyQwL5OZA_PGJFWVOQTASyzvcEBnJkivX-TQ@mail.gmail.com>
 <3a048ca2-72c5-956f-e236-dd459d34e1fb@treenet.co.nz>
Message-ID: <CABMULtJ6tL4hkTz6oCz+=N=iJOPU0NU37ewF=nA2ttGe58h1Vw@mail.gmail.com>

Hi Amos,

On Mon, May 7, 2018 at 7:30 PM, Amos Jeffries <squid3 at treenet.co.nz> wrote:

> On 08/05/18 00:24, Alex K wrote:
> > Hi all,
> >
> > I wanted to check with your accumulated wisdom the following squid
> > configuration.
> >
> > The config is working both for splice or bump (by
> > commenting/uncommenting the respective line) using TPROXY. It is a
> > config ported form an old installation of squid 3.1 for the new 3.5 and
> > although I did some cleanup I am wondering if I am misusing any
> > directive or missing any crucial one for better performance or just for
> > sake of cleanliness.
> >
> > At the moment for filtering I am using squidGuard and considering to go
> > with ufdbGuard instead as pointed from Amos (thanx for that).
> >
> > To avoid issues with some sites I am considering to use only splicing,
> > although this has some caveats as bumping also does. I could go with a
> > hybrid approach (splice some and bump all) but this sounds that this
> > will cause periodically more administrative overhead to sort out the
> > sites that need splicing.
> >
> > The config has also some ACLs as an attempt to block media streaming by
> > those seem to not work.
>
> The ACL checking for mms:// URL will not work because MMS protocol is
> not HTTP. Any client using that protocol will not be going through
> Squid. So quite likely none of the other checks will work for its
> non-proxied traffic either.
>
> "working" can also depend on what you are looking at. Your rules are
> only blocking *reply* access. Which means only that the client does not
> get the response delivered. It still gets fetched from the server -
> maybe in full. So checking your logs etc can still show things arriving
> and lots of bandwidth usage.
>
> The urlpath and req_mime_type can be checked in http_access instead to
> block those requests from ever happening. That MAY work better, but no
> guarantees.
>
>
>
> >
> > The hardware running the squid is somehow small with 4 GB of RAM, 4 CPU
> > cores and 100 GB SSD in case one wonders.
> >
> >
> > http_port 192.168.200.1:3128 tproxy
> > https_port 192.168.200.1:3129 tproxy \
> >   ssl-bump generate-host-certificates=on dynamic_cert_mem_cache_size=4MB
> \
> >   cert=/etc/squid/ssl_cert/myCA.pem
> >
> > sslcrtd_program /usr/lib/squid/ssl_crtd -s
> > /usr/local/squid/var/lib/ssl_db -M 4MB
> > sslcrtd_children 5
> >
> > shutdown_lifetime 5 seconds
> >
> > # ACL
> > #acl ncsa_users proxy_auth REQUIRED
> > #acl all src 0.0.0.0/0.0.0.0
> > acl manager proto cache_object
>
> 'manager' ACL is now built-in, and has a different type signature. The
> above needs to be removed. Same with 'all'. It is not a good idea to
> leave them even commented out because the old definitions are no longer
> true.
>
> ok, removed these entries (ncsa_users, all, manager)
>


> > acl localhost src 192.168.200.1/32
>
> 192.168.200.1 is assigned to your lo interface?
>
Yes, this is the IP of one of the interfaces of the device at the network
where the users use squid to reach Internet.

>
> >
> > acl SSL_ports port 443
> > acl Safe_ports port 80
> > acl Safe_ports port 21
> > acl Safe_ports port 443
> > acl Safe_ports port 10080
> > acl Safe_ports port 10443
> > acl SSL method CONNECT
>
> The above can be quite deceptive,
>
I removed port 21 as I don't think I am using FTP.


>
> > acl CONNECT method CONNECT # multiling http
> > #acl block_url dstdomain "/etc/squid/block_url.squid"
> > #acl allow_url dstdomain "/etc/squid/allow_url.squid"
> > acl ELAN src 192.168.200.0/24
> >
> > acl QUERY urlpath_regex cgi-bin \?
>
> The QUERY is not being used. It is also no longer necessary so can be
> removed.
>
Removed.

>
> >
> > # SSL
> > always_direct allow all
>
> That should not be. You do not have any cache_peer configured.
>
> Removed


> >
> > # Video Streaming ACLs
> > acl media rep_mime_type ^.*mms.*
> > acl media rep_mime_type ^.*ms-hdr.*
> > acl media rep_mime_type ^.*x-fcs.*
> > acl media rep_mime_type ^.*x-ms-asf.*
> > acl media2 urlpath_regex dvrplayer mediastream mms://
> > acl media2 urlpath_regex \.asf$ \.afx$ \.flv$ \.swf$
> > acl flashvideo rep_mime_type -i video/flv
> > acl flashvideo rep_mime_type -i video/x-flv
> > acl shockwave rep_mime_type -i ^application/x-shockwave-flash$
>
> > acl x-type req_mime_type -i ^application/octet-stream$
> > acl x-type req_mime_type -i application/octet-stream
>
> All the lines like the two above are duplicates.
> The "^foo$" pattern is a sub-set of "foo" pattern.
>
> Removed duplicates

>
> > acl x-type req_mime_type -i ^application/x-mplayer2$
> > acl x-type req_mime_type -i application/x-mplayer2
> > acl x-type req_mime_type -i ^application/x-oleobject$
> > acl x-type req_mime_type -i application/x-oleobject
> > acl x-type req_mime_type -i application/x-pncmd
> > acl x-type req_mime_type -i ^video/x-ms-asf$
> > acl x-type2 rep_mime_type -i ^application/octet-stream$
> > acl x-type2 rep_mime_type -i application/octet-stream
> > acl x-type2 rep_mime_type -i ^application/x-mplayer2$
> > acl x-type2 rep_mime_type -i application/x-mplayer2
> > acl x-type2 rep_mime_type -i ^application/x-oleobject$
> > acl x-type2 rep_mime_type -i application/x-oleobject
> > acl x-type2 rep_mime_type -i application/x-pncmd
> > acl x-type2 rep_mime_type -i ^video/x-ms-asf$
> >
> > # Block Media Streaming
> > http_reply_access deny flashvideo
> > http_reply_access deny shockwave
> > http_reply_access deny media
> > http_reply_access deny media2
> > http_reply_access deny x-type
> > http_reply_access deny x-type2
> >
> > #
> > http_access deny manager
> > http_access deny !Safe_ports
> > http_access deny CONNECT !SSL_ports
>
> FYI: current best-practice recommendation is to place the manager access
> line down here after the faster port checks.
>
> Placed the manager line after the  "http_access deny CONNECT !SSL_ports".

> #http_access deny block_url
> > #http_access allow allow_url
> > http_access allow LAN
> > http_access allow ELAN
> >
> > http_access allow localhost
> > #http_access allow ncsa_users
> > http_reply_access allow all
>
> This http_reply_access line should be up with the others so it does not
> fool anyone into thinking its placement here with the http_access lines
> has any meaning.
>
Moved it at http_reply block

>
> >
> > deny_info ERR_CUSTOM LAN ELAN media media2 flashvideo shockwave x-type
> > x-type2
> > error_directory /usr/share/squid-langpack/en
> >
> > #icp_access allow all
> >
> > # Logging
> > logfile_daemon /usr/lib/squid/log_db_daemon
> > access_log daemon:/127.0.0.1/squid_log/access_log/squid/squid squid
> > icap_log stdio:/var/log/squid/icap.log squid
> > cache_store_log stdio:/var/log/squid/store.log
> >
> > # DNS
> > dns_nameservers 127.0.0.1
> > positive_dns_ttl 8 hours
> > negative_dns_ttl 30 seconds
> > ipcache_size 2048
> > ipcache_low 95
> > ipcache_high 97
> > fqdncache_size 2048
> >
> > # Leave coredumps in the first cache dir
> > coredump_dir /var/spool/squid
> > cache_dir ufs /var/spool/squid 10240 16 256
> > minimum_object_size 0 KB
> > maximum_object_size 30 MB
> > maximum_object_size_in_memory 1024 KB
> >
>
> # HTTPS filtering
> > acl step1 at_step SslBump1
> >
> > ssl_bump peek step1
> > ssl_bump splice all
> > #ssl_bump bump all
> >
> > # SquidGuard
> > url_rewrite_program /usr/bin/squidGuard -c /etc/squidguard/squidGuard.
> conf
> > url_rewrite_children 5
> >
> >
> > Your input is highly appreciated.
> >
> > Alex
> >
> >
> > _______________________________________________
> > squid-users mailing list
> > squid-users at lists.squid-cache.org
> > http://lists.squid-cache.org/listinfo/squid-users
> >
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20180507/d85eade3/attachment.htm>

From akismpa at gmail.com  Mon May  7 22:22:43 2018
From: akismpa at gmail.com (Panagiotis Bariamis)
Date: Mon, 07 May 2018 22:22:43 +0000
Subject: [squid-users] Kerberos authentication on mobile phones
Message-ID: <CAPxN_PXe=KhsRyxTp54uF4t0Ub731YzHMc62+zAV=rBuGoL3Hg@mail.gmail.com>

Hello,
Is it possible with a squid kerberos only authentication  setup be able to
authenticate ie android phones to squid?
A second question. If a non domain joined machine tries to use the proxy
will there be a username password prompt where if correct credentials are
presented he will be able to get a ticket to use squid?
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20180507/f921d722/attachment.htm>

From squid3 at treenet.co.nz  Tue May  8 05:55:17 2018
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Tue, 8 May 2018 17:55:17 +1200
Subject: [squid-users] Squid configuration sanity check
In-Reply-To: <CABMULtJ6tL4hkTz6oCz+=N=iJOPU0NU37ewF=nA2ttGe58h1Vw@mail.gmail.com>
References: <CABMULt+atE7wPGGyQwL5OZA_PGJFWVOQTASyzvcEBnJkivX-TQ@mail.gmail.com>
 <3a048ca2-72c5-956f-e236-dd459d34e1fb@treenet.co.nz>
 <CABMULtJ6tL4hkTz6oCz+=N=iJOPU0NU37ewF=nA2ttGe58h1Vw@mail.gmail.com>
Message-ID: <f8badb59-1297-4b08-fa65-5307ae38468a@treenet.co.nz>

On 08/05/18 04:56, Alex K wrote:
> Hi Amos,
> 
> On Mon, May 7, 2018 at 7:30 PM, Amos Jeffries wrote:
> 
>     On 08/05/18 00:24, Alex K wrote:
>     > Hi all,
>     > 
...
>     > acl localhost src 192.168.200.1/32 <http://192.168.200.1/32>
> 
>     192.168.200.1 is assigned to your lo interface?
> 
> Yes, this is the IP of one of the interfaces of the device at the
> network where the users use squid to reach Internet.?
> 

No, I mean specifically the interface named "lo" which has ::1 and
127.0.0.0/8 assigned by the system. It has some special security
properties like hardware restriction preventing globally routable IPs
being used as dst-IP of packets even routed through it result in rejections.



> 
>     > 
>     > acl SSL_ports port 443
>     > acl Safe_ports port 80
>     > acl Safe_ports port 21
>     > acl Safe_ports port 443
>     > acl Safe_ports port 10080
>     > acl Safe_ports port 10443
>     > acl SSL method CONNECT
> 
>     The above can be quite deceptive,
> 
> I removed port 21 as I don't think I am using FTP.
> ?

Sorry, I missed out the last half of that text. I was meaning the "SSL"
ACL definition specifically. CONNECT method is not restricted to SSL
protocol even when all you are doing is intercepting port 443 (think
HTTP/2, WebSockets, QUIC, etc). It would be better to use the provided
CONNECT ACL in place of "SSL" - they are identical in definition and
CONNECT is clearer to see if/when some access control is not as tightly
restricted as "SSL" would make it seem.


Cheers
Amos


From squid3 at treenet.co.nz  Tue May  8 06:03:19 2018
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Tue, 8 May 2018 18:03:19 +1200
Subject: [squid-users] Kerberos authentication on mobile phones
In-Reply-To: <CAPxN_PXe=KhsRyxTp54uF4t0Ub731YzHMc62+zAV=rBuGoL3Hg@mail.gmail.com>
References: <CAPxN_PXe=KhsRyxTp54uF4t0Ub731YzHMc62+zAV=rBuGoL3Hg@mail.gmail.com>
Message-ID: <36775d21-090a-e22a-bec0-78edc57541a9@treenet.co.nz>

On 08/05/18 10:22, Panagiotis Bariamis wrote:
> Hello,
> Is it possible with a squid kerberos only authentication? setup be able
> to authenticate ie android phones to squid?

I don't have an answer for that, maybe someone else has experience. If
you have the environment available you could try it yourself.


> A second question. If a non domain joined machine tries to use the proxy
> will there be a username password prompt where if correct credentials
> are presented he will be able to get a ticket to use squid? 

Maybe, unlikely though IMO. Getting a ticket requires first joining the
domain. Some client software may provide a popup and then try to contact
a DC and join a domain.

But whether a) the specific client software does that, and b) whether
info about the domain DC server is available in DNS records, and c)
whether the Kerberos realm "domain" matches the proxy DNS record domain
- all those effect the possibilities AFAIK.

Amos


From rightkicktech at gmail.com  Tue May  8 10:35:14 2018
From: rightkicktech at gmail.com (Alex K)
Date: Tue, 8 May 2018 13:35:14 +0300
Subject: [squid-users] Squid configuration sanity check
In-Reply-To: <f8badb59-1297-4b08-fa65-5307ae38468a@treenet.co.nz>
References: <CABMULt+atE7wPGGyQwL5OZA_PGJFWVOQTASyzvcEBnJkivX-TQ@mail.gmail.com>
 <3a048ca2-72c5-956f-e236-dd459d34e1fb@treenet.co.nz>
 <CABMULtJ6tL4hkTz6oCz+=N=iJOPU0NU37ewF=nA2ttGe58h1Vw@mail.gmail.com>
 <f8badb59-1297-4b08-fa65-5307ae38468a@treenet.co.nz>
Message-ID: <CABMULtJwZp6pkeK3uwrGZoeuOe4WVG4nC-4_ZQpDvHgsKNiWeQ@mail.gmail.com>

Hi Amos,

On Tue, May 8, 2018 at 8:55 AM, Amos Jeffries <squid3 at treenet.co.nz> wrote:

> On 08/05/18 04:56, Alex K wrote:
> > Hi Amos,
> >
> > On Mon, May 7, 2018 at 7:30 PM, Amos Jeffries wrote:
> >
> >     On 08/05/18 00:24, Alex K wrote:
> >     > Hi all,
> >     >
> ...
> >     > acl localhost src 192.168.200.1/32 <http://192.168.200.1/32>
> >
> >     192.168.200.1 is assigned to your lo interface?
> >
> > Yes, this is the IP of one of the interfaces of the device at the
> > network where the users use squid to reach Internet.
> >
>
> No, I mean specifically the interface named "lo" which has ::1 and
> 127.0.0.0/8 assigned by the system. It has some special security
> properties like hardware restriction preventing globally routable IPs
> being used as dst-IP of packets even routed through it result in
> rejections.
>
I have not assigned 192.168.200.1 at lo. It is assigned to an interface
(eth3 for example). localhost is here misleading. it could say "proxy"


>
>
> >
> >     >
> >     > acl SSL_ports port 443
> >     > acl Safe_ports port 80
> >     > acl Safe_ports port 21
> >     > acl Safe_ports port 443
> >     > acl Safe_ports port 10080
> >     > acl Safe_ports port 10443
> >     > acl SSL method CONNECT
> >
> >     The above can be quite deceptive,
> >
> > I removed port 21 as I don't think I am using FTP.
> >
>
> Sorry, I missed out the last half of that text. I was meaning the "SSL"
> ACL definition specifically. CONNECT method is not restricted to SSL
> protocol even when all you are doing is intercepting port 443 (think
> HTTP/2, WebSockets, QUIC, etc). It would be better to use the provided
> CONNECT ACL in place of "SSL" - they are identical in definition and
> CONNECT is clearer to see if/when some access control is not as tightly
> restricted as "SSL" would make it seem.

You mean remove  "acl CONNECT method CONNECT" and leave only "acl CONNECT
method CONNECT" ?

>
>
> Cheers
> Amos
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20180508/d1e85e48/attachment.htm>

From rightkicktech at gmail.com  Tue May  8 10:36:06 2018
From: rightkicktech at gmail.com (Alex K)
Date: Tue, 8 May 2018 13:36:06 +0300
Subject: [squid-users] Squid configuration sanity check
In-Reply-To: <CABMULtJwZp6pkeK3uwrGZoeuOe4WVG4nC-4_ZQpDvHgsKNiWeQ@mail.gmail.com>
References: <CABMULt+atE7wPGGyQwL5OZA_PGJFWVOQTASyzvcEBnJkivX-TQ@mail.gmail.com>
 <3a048ca2-72c5-956f-e236-dd459d34e1fb@treenet.co.nz>
 <CABMULtJ6tL4hkTz6oCz+=N=iJOPU0NU37ewF=nA2ttGe58h1Vw@mail.gmail.com>
 <f8badb59-1297-4b08-fa65-5307ae38468a@treenet.co.nz>
 <CABMULtJwZp6pkeK3uwrGZoeuOe4WVG4nC-4_ZQpDvHgsKNiWeQ@mail.gmail.com>
Message-ID: <CABMULt+iCsGHcfjoD8qtLLONn4evO7rHjvP5spwmJSw6OpZEfw@mail.gmail.com>

Correction:

On Tue, May 8, 2018 at 1:35 PM, Alex K <rightkicktech at gmail.com> wrote:

> Hi Amos,
>
> On Tue, May 8, 2018 at 8:55 AM, Amos Jeffries <squid3 at treenet.co.nz>
> wrote:
>
>> On 08/05/18 04:56, Alex K wrote:
>> > Hi Amos,
>> >
>> > On Mon, May 7, 2018 at 7:30 PM, Amos Jeffries wrote:
>> >
>> >     On 08/05/18 00:24, Alex K wrote:
>> >     > Hi all,
>> >     >
>> ...
>> >     > acl localhost src 192.168.200.1/32 <http://192.168.200.1/32>
>> >
>> >     192.168.200.1 is assigned to your lo interface?
>> >
>> > Yes, this is the IP of one of the interfaces of the device at the
>> > network where the users use squid to reach Internet.
>> >
>>
>> No, I mean specifically the interface named "lo" which has ::1 and
>> 127.0.0.0/8 assigned by the system. It has some special security
>> properties like hardware restriction preventing globally routable IPs
>> being used as dst-IP of packets even routed through it result in
>> rejections.
>>
> I have not assigned 192.168.200.1 at lo. It is assigned to an interface
> (eth3 for example). localhost is here misleading. it could say "proxy"
>
>
>>
>>
>> >
>> >     >
>> >     > acl SSL_ports port 443
>> >     > acl Safe_ports port 80
>> >     > acl Safe_ports port 21
>> >     > acl Safe_ports port 443
>> >     > acl Safe_ports port 10080
>> >     > acl Safe_ports port 10443
>> >     > acl SSL method CONNECT
>> >
>> >     The above can be quite deceptive,
>> >
>> > I removed port 21 as I don't think I am using FTP.
>> >
>>
>> Sorry, I missed out the last half of that text. I was meaning the "SSL"
>> ACL definition specifically. CONNECT method is not restricted to SSL
>> protocol even when all you are doing is intercepting port 443 (think
>> HTTP/2, WebSockets, QUIC, etc). It would be better to use the provided
>> CONNECT ACL in place of "SSL" - they are identical in definition and
>> CONNECT is clearer to see if/when some access control is not as tightly
>> restricted as "SSL" would make it seem.
>
> You mean remove  "acl SSL method CONNECT" and leave only "acl CONNECT
> method CONNECT" ?
>
>>
>>
>> Cheers
>> Amos
>>
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20180508/3c97ef52/attachment.htm>

From akismpa at gmail.com  Tue May  8 14:47:39 2018
From: akismpa at gmail.com (Panagiotis Bariamis)
Date: Tue, 8 May 2018 17:47:39 +0300
Subject: [squid-users] Kerberos authentication on mobile phones
In-Reply-To: <36775d21-090a-e22a-bec0-78edc57541a9@treenet.co.nz>
References: <CAPxN_PXe=KhsRyxTp54uF4t0Ub731YzHMc62+zAV=rBuGoL3Hg@mail.gmail.com>
 <36775d21-090a-e22a-bec0-78edc57541a9@treenet.co.nz>
Message-ID: <CAPxN_PVKXoYDJWqwZ0rd6yxk5hC8FOfD=TtQESiEgaNq2wuHnQ@mail.gmail.com>

On Tue, May 8, 2018 at 9:03 AM, Amos Jeffries <squid3 at treenet.co.nz> wrote:

> On 08/05/18 10:22, Panagiotis Bariamis wrote:
>
>
>
> >> A second question. If a non domain joined machine tries to use the proxy
> >> will there be a username password prompt where if correct credentials
> >> are presented he will be able to get a ticket to use squid?
>
> >Maybe, unlikely though IMO. Getting a ticket requires first joining the
> >domain. Some client software may provide a popup and then try to contact
> >a DC and join a domain.
>
> >But whether a) the specific client software does that, and b) whether
> >info about the domain DC server is available in DNS records, and c)
> >whether the Kerberos realm "domain" matches the proxy DNS record domain
> >- all those effect the possibilities AFAIK.
>
> Given the fact that all DNS entries are ok across the domain and we use
MIT Kerberos ,
can a BYOD scenario be implemented ? I mean if the machine does not start a
kinit session ,
will the browser start such a session and get a ticket ?

Thank you ,
Bariamis Panagiotis
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20180508/58fd7262/attachment.htm>

From squid3 at treenet.co.nz  Tue May  8 16:49:20 2018
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Wed, 9 May 2018 04:49:20 +1200
Subject: [squid-users] Squid configuration sanity check
In-Reply-To: <CABMULt+iCsGHcfjoD8qtLLONn4evO7rHjvP5spwmJSw6OpZEfw@mail.gmail.com>
References: <CABMULt+atE7wPGGyQwL5OZA_PGJFWVOQTASyzvcEBnJkivX-TQ@mail.gmail.com>
 <3a048ca2-72c5-956f-e236-dd459d34e1fb@treenet.co.nz>
 <CABMULtJ6tL4hkTz6oCz+=N=iJOPU0NU37ewF=nA2ttGe58h1Vw@mail.gmail.com>
 <f8badb59-1297-4b08-fa65-5307ae38468a@treenet.co.nz>
 <CABMULtJwZp6pkeK3uwrGZoeuOe4WVG4nC-4_ZQpDvHgsKNiWeQ@mail.gmail.com>
 <CABMULt+iCsGHcfjoD8qtLLONn4evO7rHjvP5spwmJSw6OpZEfw@mail.gmail.com>
Message-ID: <aa1bf065-8ad3-ced8-fb3e-d74af8406e26@treenet.co.nz>

On 08/05/18 22:36, Alex K wrote:
> Correction:
> 
> On Tue, May 8, 2018 at 1:35 PM, Alex K wrote:
> 
>     Hi Amos,
> 
>     On Tue, May 8, 2018 at 8:55 AM, Amos Jeffries wrote:
> 
>         On 08/05/18 04:56, Alex K wrote:
>         > Hi Amos,
>         > 
>         > On Mon, May 7, 2018 at 7:30 PM, Amos Jeffries wrote:
>         > 
>         >? ? ?On 08/05/18 00:24, Alex K wrote:
>         >? ? ?> Hi all,
>         >? ? ?> 
>         ...
>         >? ? ?> acl localhost src 192.168.200.1/32
>         > 
>         >? ? ?192.168.200.1 is assigned to your lo interface?
>         > 
>         > Yes, this is the IP of one of the interfaces of the device at the
>         > network where the users use squid to reach Internet.?
>         > 
> 
>         No, I mean specifically the interface named "lo" which has ::1 and
>         127.0.0.0/8 assigned by the system. It has
>         some special security
>         properties like hardware restriction preventing globally
>         routable IPs
>         being used as dst-IP of packets even routed through it result in
>         rejections.
> 
>     I have not assigned 192.168.200.1 at lo. It is assigned to an
>     interface (eth3 for example). localhost is here misleading. it could
>     say "proxy"

Yes, it should be different. "localhost" ACL is used for some defaults.
What you are doing here is adding 192.168.200.1 to the ::! etc
definition of the predefined localhost ACL.


> 
>         > 
>         >? ? ?> 
>         >? ? ?> acl SSL_ports port 443
>         >? ? ?> acl Safe_ports port 80
>         >? ? ?> acl Safe_ports port 21
>         >? ? ?> acl Safe_ports port 443
>         >? ? ?> acl Safe_ports port 10080
>         >? ? ?> acl Safe_ports port 10443
>         >? ? ?> acl SSL method CONNECT
>         > 
>         >? ? ?The above can be quite deceptive,
>         > 
>         > I removed port 21 as I don't think I am using FTP.
>         > ?
> 
>         Sorry, I missed out the last half of that text. I was meaning
>         the "SSL"
>         ACL definition specifically. CONNECT method is not restricted to SSL
>         protocol even when all you are doing is intercepting port 443 (think
>         HTTP/2, WebSockets, QUIC, etc). It would be better to use the
>         provided
>         CONNECT ACL in place of "SSL" - they are identical in definition and
>         CONNECT is clearer to see if/when some access control is not as
>         tightly
>         restricted as "SSL" would make it seem.?
> 
>     You mean remove? "acl SSL method CONNECT" and leave only "acl
>     CONNECT method CONNECT" ?
> 

Yes. Exactly so.

Amos


From adilias3 at gmx.com  Wed May  9 04:21:55 2018
From: adilias3 at gmx.com (Ilias Clifton)
Date: Wed, 9 May 2018 06:21:55 +0200
Subject: [squid-users] Help with WCCP: Cisco 1841 to Squid 3.5.25 on Ubuntu
	16
Message-ID: <trinity-07e4cfb1-84d9-42fc-aeab-969784ed615a-1525839715367@3c-app-mailcom-bs11>


Hello,
?
I've been trying to get WCCP working but have been banging my head against a wall, so thought I would ask for help.
?
There are 2 internal subnets that I would like to use the squid proxy: 172.28.30.128/25 and 172.28.29.0/25
?
I have squid v3.5.25 running on Ubuntu 16 : 172.28.28.252
?
I have a Cisco 1841 - Adv IP - 12.4, see relevent config:
?
#Inside Interface
interface FastEthernet0/1
?ip address 172.28.28.1 255.255.255.240
?ip wccp web-cache redirect in
?ip nat inside
?ip virtual-reassembly max-reassemblies 64
?no ip mroute-cache
?duplex auto
?speed auto
?
#Loopback for wccp router ID
interface Loopback0
?ip address 172.28.28.33 255.255.255.255
?
ip wccp web-cache redirect-list PROXY_USERS group-list SQUID
?
ip access-list extended PROXY_USERS
?deny ? tcp host 172.28.28.252 any
?permit tcp 172.28.30.128 0.0.0.127 any eq www
?permit tcp 172.28.29.0 0.0.0.127 any eq www
?deny ? ip any any
?
ip access-list standard SQUID
?permit 172.28.28.252
?
?
?
On the Ubuntu box, I have the squid with the following config:
?
http_port 3128
http_port 3129 intercept?
acl localnet src 172.28.28.0/22 ??
http_access allow localnet
http_access allow localhost
http_access deny all
visible_hostname Squid
wccp2_router 172.28.28.1
wccp2_forwarding_method gre
wccp2_return_method gre
wccp2_service standard 0
?
If clients are manually set to use the proxy on port 3128, they work correctly.
?
Again on the Ubuntu box, I have setup the following gre tunnel.
?
ip tunnel add wccp0 mode gre remote 172.28.28.33 local 172.28.28.252 dev ens33 ttl 255
?
and the following redirect using iptables..
?
iptables -t nat?-A PREROUTING -i wccp0 -p tcp -m tcp --dport 80 -j REDIRECT --to-ports 3129
?
In sysctl.conf, I have disabled reverse path filtering and enabled ip forarding.
?
net.ipv4.conf.default.rp_filter=0
net.ipv4.conf.all.rp_filter=0
net.ipv4.ip_forward=1

When starting squid, using tcpdump, i see traffic between the Ubuntu box and the router on udp port 2048

00:39:34.587799 IP 172.28.28.252.2048 > 172.28.28.1.2048: UDP, length 144
00:39:34.590399 IP 172.28.28.1.2048 > 172.28.28.252.2048: UDP, length 140

I see the following message on the router..
%WCCP-5-SERVICEFOUND: Service web-cache acquired on WCCP client 172.28.28.252

So looks like it's working ok so far...

When I try and browse to a site from a client..
$ wget http://www.google.com

On the Ubuntu box, I see gre traffic on the ethernet interface..
00:44:22.340734 IP 172.28.28.33 > 172.28.28.252: GREv0, length 72: gre-proto-0x883e


I see the un-encapsulated traffic on the wccp0 interface:
00:56:26.888519 IP 172.28.29.4.52128 > 216.58.203.100.80

Which is correctly showing original client IP and destination IP.

I can see hits on the iptable redirect rule:
pkts bytes target     prot opt in     out     source               destination         
  429 26280 REDIRECT   tcp  --  wccp0  any     anywhere             anywhere             tcp dpt:http redir ports 3129


But there is no response from squid on the Ubuntu box :-(

I don't see anything helpful in either access.log or cache.log.

I'm not sure if there is anything else that could be dropping the packet apart from return path filtering..

If someone could give me some pointers or any further debugging I could try, that would be great.


Thanks.







?
?
?
?
?
?
?
?


From rightkicktech at gmail.com  Wed May  9 04:37:12 2018
From: rightkicktech at gmail.com (Alex K)
Date: Wed, 09 May 2018 04:37:12 +0000
Subject: [squid-users] Help with WCCP: Cisco 1841 to Squid 3.5.25 on
 Ubuntu 16
In-Reply-To: <trinity-07e4cfb1-84d9-42fc-aeab-969784ed615a-1525839715367@3c-app-mailcom-bs11>
References: <trinity-07e4cfb1-84d9-42fc-aeab-969784ed615a-1525839715367@3c-app-mailcom-bs11>
Message-ID: <CABMULtKBtepT-pYJm4jxkHumHq9=FtVW5vQQ85kKsQ5DBR3LVw@mail.gmail.com>

Hi,

At the wccp0  interface do you see bidirectional http traffic? If the squid
box has multiple interfaces, do you see traffic on its wan interface? That
traffic might need NATing. Also I would check if squidbox drops any
packages in case you have firewall configured on it.

Alex



On Wed, May 9, 2018, 07:22 Ilias Clifton <adilias3 at gmx.com> wrote:

>
> Hello,
>
> I've been trying to get WCCP working but have been banging my head against
> a wall, so thought I would ask for help.
>
> There are 2 internal subnets that I would like to use the squid proxy:
> 172.28.30.128/25 and 172.28.29.0/25
>
> I have squid v3.5.25 running on Ubuntu 16 : 172.28.28.252
>
> I have a Cisco 1841 - Adv IP - 12.4, see relevent config:
>
> #Inside Interface
> interface FastEthernet0/1
>  ip address 172.28.28.1 255.255.255.240
>  ip wccp web-cache redirect in
>  ip nat inside
>  ip virtual-reassembly max-reassemblies 64
>  no ip mroute-cache
>  duplex auto
>  speed auto
>
> #Loopback for wccp router ID
> interface Loopback0
>  ip address 172.28.28.33 255.255.255.255
>
> ip wccp web-cache redirect-list PROXY_USERS group-list SQUID
>
> ip access-list extended PROXY_USERS
>  deny   tcp host 172.28.28.252 any
>  permit tcp 172.28.30.128 0.0.0.127 any eq www
>  permit tcp 172.28.29.0 0.0.0.127 any eq www
>  deny   ip any any
>
> ip access-list standard SQUID
>  permit 172.28.28.252
>
>
>
> On the Ubuntu box, I have the squid with the following config:
>
> http_port 3128
> http_port 3129 intercept
> acl localnet src 172.28.28.0/22
> http_access allow localnet
> http_access allow localhost
> http_access deny all
> visible_hostname Squid
> wccp2_router 172.28.28.1
> wccp2_forwarding_method gre
> wccp2_return_method gre
> wccp2_service standard 0
>
> If clients are manually set to use the proxy on port 3128, they work
> correctly.
>
> Again on the Ubuntu box, I have setup the following gre tunnel.
>
> ip tunnel add wccp0 mode gre remote 172.28.28.33 local 172.28.28.252 dev
> ens33 ttl 255
>
> and the following redirect using iptables..
>
> iptables -t nat -A PREROUTING -i wccp0 -p tcp -m tcp --dport 80 -j
> REDIRECT --to-ports 3129
>
> In sysctl.conf, I have disabled reverse path filtering and enabled ip
> forarding.
>
> net.ipv4.conf.default.rp_filter=0
> net.ipv4.conf.all.rp_filter=0
> net.ipv4.ip_forward=1
>
> When starting squid, using tcpdump, i see traffic between the Ubuntu box
> and the router on udp port 2048
>
> 00:39:34.587799 IP 172.28.28.252.2048 > 172.28.28.1.2048: UDP, length 144
> 00:39:34.590399 IP 172.28.28.1.2048 > 172.28.28.252.2048: UDP, length 140
>
> I see the following message on the router..
> %WCCP-5-SERVICEFOUND: Service web-cache acquired on WCCP client
> 172.28.28.252
>
> So looks like it's working ok so far...
>
> When I try and browse to a site from a client..
> $ wget http://www.google.com
>
> On the Ubuntu box, I see gre traffic on the ethernet interface..
> 00:44:22.340734 IP 172.28.28.33 > 172.28.28.252: GREv0, length 72:
> gre-proto-0x883e
>
>
> I see the un-encapsulated traffic on the wccp0 interface:
> 00:56:26.888519 IP 172.28.29.4.52128 > 216.58.203.100.80
>
> Which is correctly showing original client IP and destination IP.
>
> I can see hits on the iptable redirect rule:
> pkts bytes target     prot opt in     out     source
>  destination
>   429 26280 REDIRECT   tcp  --  wccp0  any     anywhere
>  anywhere             tcp dpt:http redir ports 3129
>
>
> But there is no response from squid on the Ubuntu box :-(
>
> I don't see anything helpful in either access.log or cache.log.
>
> I'm not sure if there is anything else that could be dropping the packet
> apart from return path filtering..
>
> If someone could give me some pointers or any further debugging I could
> try, that would be great.
>
>
> Thanks.
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20180509/83e5f41f/attachment.htm>

From adilias3 at gmx.com  Wed May  9 04:59:53 2018
From: adilias3 at gmx.com (Ilias Clifton)
Date: Wed, 9 May 2018 06:59:53 +0200
Subject: [squid-users] Help with WCCP: Cisco 1841 to Squid 3.5.25 on
 Ubuntu 16
In-Reply-To: <CABMULtKBtepT-pYJm4jxkHumHq9=FtVW5vQQ85kKsQ5DBR3LVw@mail.gmail.com>
References: <trinity-07e4cfb1-84d9-42fc-aeab-969784ed615a-1525839715367@3c-app-mailcom-bs11>
 <CABMULtKBtepT-pYJm4jxkHumHq9=FtVW5vQQ85kKsQ5DBR3LVw@mail.gmail.com>
Message-ID: <trinity-6ccac5f7-f3d9-4001-bb4f-f698b88ebc66-1525841993776@3c-app-mailcom-bs05>


?Hi Alex,

On the wccp0 interface I only see traffic arriving in 1 direction - original client ip to destination ip.

The ubuntu box only has a single ethernet interface -  Sorry, that should have been in my original question. I see the gre traffic arriving from the router, but again - no response.

I tried adding a MASQUERADE line to the iptables rules, just to see if it made a difference.. but same result.


?

Sent:?Wednesday, May 09, 2018 at 2:37 PM
From:?"Alex K" <rightkicktech at gmail.com>
To:?"Ilias Clifton" <adilias3 at gmx.com>
Cc:?squid-users at lists.squid-cache.org
Subject:?Re: [squid-users] Help with WCCP: Cisco 1841 to Squid 3.5.25 on Ubuntu 16

Hi,
?
At the?wccp0??interface do you see bidirectional http traffic? If the squid box has multiple interfaces, do you see traffic on its wan interface? That traffic might need NATing. Also I would check if squidbox drops any packages in case you have firewall configured on it.
?
Alex
??

On Wed, May 9, 2018, 07:22 Ilias Clifton <adilias3 at gmx.com[mailto:adilias3 at gmx.com]> wrote:
Hello,
?
I've been trying to get WCCP working but have been banging my head against a wall, so thought I would ask for help.
?
There are 2 internal subnets that I would like to use the squid proxy: 172.28.30.128/25[http://172.28.30.128/25] and 172.28.29.0/25[http://172.28.29.0/25]
?
I have squid v3.5.25 running on Ubuntu 16 : 172.28.28.252
?
I have a Cisco 1841 - Adv IP - 12.4, see relevent config:
?
#Inside Interface
interface FastEthernet0/1
?ip address 172.28.28.1 255.255.255.240
?ip wccp web-cache redirect in
?ip nat inside
?ip virtual-reassembly max-reassemblies 64
?no ip mroute-cache
?duplex auto
?speed auto
?
#Loopback for wccp router ID
interface Loopback0
?ip address 172.28.28.33 255.255.255.255
?
ip wccp web-cache redirect-list PROXY_USERS group-list SQUID
?
ip access-list extended PROXY_USERS
?deny ? tcp host 172.28.28.252 any
?permit tcp 172.28.30.128 0.0.0.127 any eq www
?permit tcp 172.28.29.0 0.0.0.127 any eq www
?deny ? ip any any
?
ip access-list standard SQUID
?permit 172.28.28.252
?
?
?
On the Ubuntu box, I have the squid with the following config:
?
http_port 3128
http_port 3129 intercept?
acl localnet src 172.28.28.0/22[http://172.28.28.0/22] ??
http_access allow localnet
http_access allow localhost
http_access deny all
visible_hostname Squid
wccp2_router 172.28.28.1
wccp2_forwarding_method gre
wccp2_return_method gre
wccp2_service standard 0
?
If clients are manually set to use the proxy on port 3128, they work correctly.
?
Again on the Ubuntu box, I have setup the following gre tunnel.
?
ip tunnel add wccp0 mode gre remote 172.28.28.33 local 172.28.28.252 dev ens33 ttl 255
?
and the following redirect using iptables..
?
iptables -t nat?-A PREROUTING -i wccp0 -p tcp -m tcp --dport 80 -j REDIRECT --to-ports 3129
?
In sysctl.conf, I have disabled reverse path filtering and enabled ip forarding.
?
net.ipv4.conf.default.rp_filter=0
net.ipv4.conf.all.rp_filter=0
net.ipv4.ip_forward=1

When starting squid, using tcpdump, i see traffic between the Ubuntu box and the router on udp port 2048

00:39:34.587799 IP 172.28.28.252.2048 > 172.28.28.1.2048: UDP, length 144
00:39:34.590399 IP 172.28.28.1.2048 > 172.28.28.252.2048: UDP, length 140

I see the following message on the router..
%WCCP-5-SERVICEFOUND: Service web-cache acquired on WCCP client 172.28.28.252

So looks like it's working ok so far...

When I try and browse to a site from a client..
$ wget http://www.google.com[http://www.google.com]

On the Ubuntu box, I see gre traffic on the ethernet interface..
00:44:22.340734 IP 172.28.28.33 > 172.28.28.252[http://172.28.28.252]: GREv0, length 72: gre-proto-0x883e


I see the un-encapsulated traffic on the wccp0 interface:
00:56:26.888519 IP 172.28.29.4.52128 > 216.58.203.100.80

Which is correctly showing original client IP and destination IP.

I can see hits on the iptable redirect rule:
pkts bytes target? ? ?prot opt in? ? ?out? ? ?source? ? ? ? ? ? ? ?destination? ? ? ? ?
? 429 26280 REDIRECT? ?tcp? --? wccp0? any? ? ?anywhere? ? ? ? ? ? ?anywhere? ? ? ? ? ? ?tcp dpt:http redir ports 3129


But there is no response from squid on the Ubuntu box :-(

I don't see anything helpful in either access.log or cache.log.

I'm not sure if there is anything else that could be dropping the packet apart from return path filtering..

If someone could give me some pointers or any further debugging I could try, that would be great.


Thanks.







?
?
?
?
?
?
?
?
_______________________________________________
squid-users mailing list
squid-users at lists.squid-cache.org[mailto:squid-users at lists.squid-cache.org]
http://lists.squid-cache.org/listinfo/squid-users


From rightkicktech at gmail.com  Wed May  9 05:08:17 2018
From: rightkicktech at gmail.com (Alex K)
Date: Wed, 09 May 2018 05:08:17 +0000
Subject: [squid-users] Help with WCCP: Cisco 1841 to Squid 3.5.25 on
 Ubuntu 16
In-Reply-To: <trinity-6ccac5f7-f3d9-4001-bb4f-f698b88ebc66-1525841993776@3c-app-mailcom-bs05>
References: <trinity-07e4cfb1-84d9-42fc-aeab-969784ed615a-1525839715367@3c-app-mailcom-bs11>
 <CABMULtKBtepT-pYJm4jxkHumHq9=FtVW5vQQ85kKsQ5DBR3LVw@mail.gmail.com>
 <trinity-6ccac5f7-f3d9-4001-bb4f-f698b88ebc66-1525841993776@3c-app-mailcom-bs05>
Message-ID: <CABMULt+i=_h301tAqmNPJCefT4cYxNynptes1hR0NPzvYrCMpg@mail.gmail.com>

Is the ubuntu able to reach Internet?
Do you see any events at squid access log?

Alex


On Wed, May 9, 2018, 07:59 Ilias Clifton <adilias3 at gmx.com> wrote:

>
>  Hi Alex,
>
> On the wccp0 interface I only see traffic arriving in 1 direction -
> original client ip to destination ip.
>
> The ubuntu box only has a single ethernet interface -  Sorry, that should
> have been in my original question. I see the gre traffic arriving from the
> router, but again - no response.
>
> I tried adding a MASQUERADE line to the iptables rules, just to see if it
> made a difference.. but same result.
>
>
>
>
> Sent: Wednesday, May 09, 2018 at 2:37 PM
> From: "Alex K" <rightkicktech at gmail.com>
> To: "Ilias Clifton" <adilias3 at gmx.com>
> Cc: squid-users at lists.squid-cache.org
> Subject: Re: [squid-users] Help with WCCP: Cisco 1841 to Squid 3.5.25 on
> Ubuntu 16
>
> Hi,
>
> At the wccp0  interface do you see bidirectional http traffic? If the
> squid box has multiple interfaces, do you see traffic on its wan interface?
> That traffic might need NATing. Also I would check if squidbox drops any
> packages in case you have firewall configured on it.
>
> Alex
>
>
> On Wed, May 9, 2018, 07:22 Ilias Clifton <adilias3 at gmx.com[mailto:
> adilias3 at gmx.com]> wrote:
> Hello,
>
> I've been trying to get WCCP working but have been banging my head against
> a wall, so thought I would ask for help.
>
> There are 2 internal subnets that I would like to use the squid proxy:
> 172.28.30.128/25[http://172.28.30.128/25]
> <http://172.28.30.128/25%5Bhttp://172.28.30.128/25%5D> and
> 172.28.29.0/25[http://172.28.29.0/25]
> <http://172.28.29.0/25%5Bhttp://172.28.29.0/25%5D>
>
> I have squid v3.5.25 running on Ubuntu 16 : 172.28.28.252
>
> I have a Cisco 1841 - Adv IP - 12.4, see relevent config:
>
> #Inside Interface
> interface FastEthernet0/1
>  ip address 172.28.28.1 255.255.255.240
>  ip wccp web-cache redirect in
>  ip nat inside
>  ip virtual-reassembly max-reassemblies 64
>  no ip mroute-cache
>  duplex auto
>  speed auto
>
> #Loopback for wccp router ID
> interface Loopback0
>  ip address 172.28.28.33 255.255.255.255
>
> ip wccp web-cache redirect-list PROXY_USERS group-list SQUID
>
> ip access-list extended PROXY_USERS
>  deny   tcp host 172.28.28.252 any
>  permit tcp 172.28.30.128 0.0.0.127 any eq www
>  permit tcp 172.28.29.0 0.0.0.127 any eq www
>  deny   ip any any
>
> ip access-list standard SQUID
>  permit 172.28.28.252
>
>
>
> On the Ubuntu box, I have the squid with the following config:
>
> http_port 3128
> http_port 3129 intercept
> acl localnet src 172.28.28.0/22[http://172.28.28.0/22]
> <http://172.28.28.0/22%5Bhttp://172.28.28.0/22%5D>
> http_access allow localnet
> http_access allow localhost
> http_access deny all
> visible_hostname Squid
> wccp2_router 172.28.28.1
> wccp2_forwarding_method gre
> wccp2_return_method gre
> wccp2_service standard 0
>
> If clients are manually set to use the proxy on port 3128, they work
> correctly.
>
> Again on the Ubuntu box, I have setup the following gre tunnel.
>
> ip tunnel add wccp0 mode gre remote 172.28.28.33 local 172.28.28.252 dev
> ens33 ttl 255
>
> and the following redirect using iptables..
>
> iptables -t nat -A PREROUTING -i wccp0 -p tcp -m tcp --dport 80 -j
> REDIRECT --to-ports 3129
>
> In sysctl.conf, I have disabled reverse path filtering and enabled ip
> forarding.
>
> net.ipv4.conf.default.rp_filter=0
> net.ipv4.conf.all.rp_filter=0
> net.ipv4.ip_forward=1
>
> When starting squid, using tcpdump, i see traffic between the Ubuntu box
> and the router on udp port 2048
>
> 00:39:34.587799 IP 172.28.28.252.2048 > 172.28.28.1.2048: UDP, length 144
> 00:39:34.590399 IP 172.28.28.1.2048 > 172.28.28.252.2048: UDP, length 140
>
> I see the following message on the router..
> %WCCP-5-SERVICEFOUND: Service web-cache acquired on WCCP client
> 172.28.28.252
>
> So looks like it's working ok so far...
>
> When I try and browse to a site from a client..
> $ wget http://www.google.com[http://www.google.com]
>
> On the Ubuntu box, I see gre traffic on the ethernet interface..
> 00:44:22.340734 IP 172.28.28.33 > 172.28.28.252[http://172.28.28.252]:
> GREv0, length 72: gre-proto-0x883e
>
>
> I see the un-encapsulated traffic on the wccp0 interface:
> 00:56:26.888519 IP 172.28.29.4.52128 > 216.58.203.100.80
>
> Which is correctly showing original client IP and destination IP.
>
> I can see hits on the iptable redirect rule:
> pkts bytes target     prot opt in     out     source
>  destination
>   429 26280 REDIRECT   tcp  --  wccp0  any     anywhere
>  anywhere             tcp dpt:http redir ports 3129
>
>
> But there is no response from squid on the Ubuntu box :-(
>
> I don't see anything helpful in either access.log or cache.log.
>
> I'm not sure if there is anything else that could be dropping the packet
> apart from return path filtering..
>
> If someone could give me some pointers or any further debugging I could
> try, that would be great.
>
>
> Thanks.
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org[mailto:squid-users at lists.squid-cache.org
> ]
> http://lists.squid-cache.org/listinfo/squid-users
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20180509/5af275f9/attachment.htm>

From squid3 at treenet.co.nz  Wed May  9 05:15:03 2018
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Wed, 9 May 2018 17:15:03 +1200
Subject: [squid-users] Help with WCCP: Cisco 1841 to Squid 3.5.25 on
 Ubuntu 16
In-Reply-To: <trinity-6ccac5f7-f3d9-4001-bb4f-f698b88ebc66-1525841993776@3c-app-mailcom-bs05>
References: <trinity-07e4cfb1-84d9-42fc-aeab-969784ed615a-1525839715367@3c-app-mailcom-bs11>
 <CABMULtKBtepT-pYJm4jxkHumHq9=FtVW5vQQ85kKsQ5DBR3LVw@mail.gmail.com>
 <trinity-6ccac5f7-f3d9-4001-bb4f-f698b88ebc66-1525841993776@3c-app-mailcom-bs05>
Message-ID: <f4e8a252-0b08-937c-d974-8145e95ddc54@treenet.co.nz>

On 09/05/18 16:59, Ilias Clifton wrote:
> 
> ?Hi Alex,
> 
> On the wccp0 interface I only see traffic arriving in 1 direction - original client ip to destination ip.
> 
> The ubuntu box only has a single ethernet interface -  Sorry, that should have been in my original question. I see the gre traffic arriving from the router, but again - no response.
> 
> I tried adding a MASQUERADE line to the iptables rules, just to see if it made a difference.. but same result.
> 

The MASQUERADE (or an equivalent SNAT) on the reply traffic going from
Squid back to the router is *definitely* needed to balance the REDIRECT
rule. Otherwise the router will reject or mishandle packets Squid sends
over the gre when you do get that part working.



> 
> Sent:?Wednesday, May 09, 2018 at 2:37 PM
> From:?"Alex K"
> 
> When I try and browse to a site from a client..
> $ wget http://www.google.com[http://www.google.com]
> 
> On the Ubuntu box, I see gre traffic on the ethernet interface..
> 00:44:22.340734 IP 172.28.28.33 > 172.28.28.252[http://172.28.28.252]: GREv0, length 72: gre-proto-0x883e
> 
> 
> I see the un-encapsulated traffic on the wccp0 interface:
> 00:56:26.888519 IP 172.28.29.4.52128 > 216.58.203.100.80
> 
> Which is correctly showing original client IP and destination IP.
> 
> I can see hits on the iptable redirect rule:
> pkts bytes target? ? ?prot opt in? ? ?out? ? ?source? ? ? ? ? ? ? ?destination? ? ? ? ?
> ? 429 26280 REDIRECT? ?tcp? --? wccp0? any? ? ?anywhere? ? ? ? ? ? ?anywhere? ? ? ? ? ? ?tcp dpt:http redir ports 3129
> 
> 
> But there is no response from squid on the Ubuntu box :-(

Is there outbound Squid<->server traffic happening? and what does that
look like?

Amos


From adilias3 at gmx.com  Wed May  9 06:36:14 2018
From: adilias3 at gmx.com (Ilias Clifton)
Date: Wed, 9 May 2018 08:36:14 +0200
Subject: [squid-users] Help with WCCP: Cisco 1841 to Squid 3.5.25 on
 Ubuntu 16
In-Reply-To: <CABMULt+i=_h301tAqmNPJCefT4cYxNynptes1hR0NPzvYrCMpg@mail.gmail.com>
References: <trinity-07e4cfb1-84d9-42fc-aeab-969784ed615a-1525839715367@3c-app-mailcom-bs11>
 <CABMULtKBtepT-pYJm4jxkHumHq9=FtVW5vQQ85kKsQ5DBR3LVw@mail.gmail.com>
 <trinity-6ccac5f7-f3d9-4001-bb4f-f698b88ebc66-1525841993776@3c-app-mailcom-bs05>
 <CABMULt+i=_h301tAqmNPJCefT4cYxNynptes1hR0NPzvYrCMpg@mail.gmail.com>
Message-ID: <trinity-8665a7a3-d08e-41a1-9140-a646fa46def5-1525847774339@3c-app-mailcom-bs05>

Ubuntu box is able to connect to the internet ok. If client PCs are configured to use the Ubuntu box as proxy on port 3128 it works correctly.

No hits in access.log for any transparent clients via wccp.. No network response at all from Ubuntu.


If I change the iptables REDIRECT to a DNAT
iptables -t nat -A PREROUTING -i wccp0 -p tcp -m tcp --dport 80 -j DNAT --to-destination 172.28.28.252:3129


I do get part of the TCP handshake done..

On the Ubuntu proxy I get :

on the wccp0 interface:
IP 172.28.29.4.53057 > 216.58.203.100.80 SYN

on the ens33 interface:
IP 216.58.203.100.80 > 172.28.29.4.53057 SYN,ACK

The client sees the SYN,ACK, it replies and thinks it has a session
IP 172.28.29.4.53057 > 216.58.203.100.80 ACK
IP 172.28.29.4.53057 > 216.58.203.100.80 GET / HTTP/1.1

But really these packets are lost and never make it back to the proxy.

I've tried adding the following iptables rules, but reply packets still have the source ip as the original destination.

iptables -t nat -A POSTROUTING -o ens33 -j MASQUERADE
iptables -t nat -A POSTROUTING -o wccp0 -j MASQUERADE

Still no hits in the access.log

Should I be attempting to reply back down the gre tunnel with the REDIRECT, or replying directly to the client via DNAT. Is there any change to the squid config between these 2 options?

The clients are in a different subnet to the Ubuntu box if that makes any difference to how I should be replying.



?
?

Sent:?Wednesday, May 09, 2018 at 3:08 PM
From:?"Alex K" <rightkicktech at gmail.com>
To:?"Ilias Clifton" <adilias3 at gmx.com>
Cc:?squid-users at lists.squid-cache.org
Subject:?Re: [squid-users] Help with WCCP: Cisco 1841 to Squid 3.5.25 on Ubuntu 16

Is the ubuntu able to reach Internet?
Do you see any events at squid access log?
?
Alex
??

On Wed, May 9, 2018, 07:59 Ilias Clifton <adilias3 at gmx.com[mailto:adilias3 at gmx.com]> wrote:
?Hi Alex,

On the wccp0 interface I only see traffic arriving in 1 direction - original client ip to destination ip.

The ubuntu box only has a single ethernet interface -? Sorry, that should have been in my original question. I see the gre traffic arriving from the router, but again - no response.

I tried adding a MASQUERADE line to the iptables rules, just to see if it made a difference.. but same result.


?

Sent:?Wednesday, May 09, 2018 at 2:37 PM
From:?"Alex K" <rightkicktech at gmail.com[mailto:rightkicktech at gmail.com]>
To:?"Ilias Clifton" <adilias3 at gmx.com[mailto:adilias3 at gmx.com]>
Cc:?squid-users at lists.squid-cache.org[mailto:squid-users at lists.squid-cache.org]
Subject:?Re: [squid-users] Help with WCCP: Cisco 1841 to Squid 3.5.25 on Ubuntu 16

Hi,
?
At the?wccp0??interface do you see bidirectional http traffic? If the squid box has multiple interfaces, do you see traffic on its wan interface? That traffic might need NATing. Also I would check if squidbox drops any packages in case you have firewall configured on it.
?
Alex
??

On Wed, May 9, 2018, 07:22 Ilias Clifton <adilias3 at gmx.com[mailto:adilias3 at gmx.com][mailto:adilias3 at gmx.com[mailto:adilias3 at gmx.com]]> wrote:
Hello,
?
I've been trying to get WCCP working but have been banging my head against a wall, so thought I would ask for help.
?
There are 2 internal subnets that I would like to use the squid proxy: 172.28.30.128/25[http://172.28.30.128/25][http://172.28.30.128/25%5Bhttp://172.28.30.128/25%5D] and 172.28.29.0/25[http://172.28.29.0/25][http://172.28.29.0/25%5Bhttp://172.28.29.0/25%5D]
?
I have squid v3.5.25 running on Ubuntu 16 : 172.28.28.252
?
I have a Cisco 1841 - Adv IP - 12.4, see relevent config:
?
#Inside Interface
interface FastEthernet0/1
?ip address 172.28.28.1 255.255.255.240
?ip wccp web-cache redirect in
?ip nat inside
?ip virtual-reassembly max-reassemblies 64
?no ip mroute-cache
?duplex auto
?speed auto
?
#Loopback for wccp router ID
interface Loopback0
?ip address 172.28.28.33 255.255.255.255
?
ip wccp web-cache redirect-list PROXY_USERS group-list SQUID
?
ip access-list extended PROXY_USERS
?deny ? tcp host 172.28.28.252 any
?permit tcp 172.28.30.128 0.0.0.127 any eq www
?permit tcp 172.28.29.0 0.0.0.127 any eq www
?deny ? ip any any
?
ip access-list standard SQUID
?permit 172.28.28.252
?
?
?
On the Ubuntu box, I have the squid with the following config:
?
http_port 3128
http_port 3129 intercept?
acl localnet src 172.28.28.0/22[http://172.28.28.0/22][http://172.28.28.0/22%5Bhttp://172.28.28.0/22%5D] ??
http_access allow localnet
http_access allow localhost
http_access deny all
visible_hostname Squid
wccp2_router 172.28.28.1
wccp2_forwarding_method gre
wccp2_return_method gre
wccp2_service standard 0
?
If clients are manually set to use the proxy on port 3128, they work correctly.
?
Again on the Ubuntu box, I have setup the following gre tunnel.
?
ip tunnel add wccp0 mode gre remote 172.28.28.33 local 172.28.28.252 dev ens33 ttl 255
?
and the following redirect using iptables..
?
iptables -t nat?-A PREROUTING -i wccp0 -p tcp -m tcp --dport 80 -j REDIRECT --to-ports 3129
?
In sysctl.conf, I have disabled reverse path filtering and enabled ip forarding.
?
net.ipv4.conf.default.rp_filter=0
net.ipv4.conf.all.rp_filter=0
net.ipv4.ip_forward=1

When starting squid, using tcpdump, i see traffic between the Ubuntu box and the router on udp port 2048

00:39:34.587799 IP 172.28.28.252.2048 > 172.28.28.1.2048: UDP, length 144
00:39:34.590399 IP 172.28.28.1.2048 > 172.28.28.252.2048: UDP, length 140

I see the following message on the router..
%WCCP-5-SERVICEFOUND: Service web-cache acquired on WCCP client 172.28.28.252

So looks like it's working ok so far...

When I try and browse to a site from a client..
$ wget http://www.google.com[http://www.google.com][http://www.google.com[http://www.google.com]]

On the Ubuntu box, I see gre traffic on the ethernet interface..
00:44:22.340734 IP 172.28.28.33 > 172.28.28.252[http://172.28.28.252[http://172.28.28.252]]: GREv0, length 72: gre-proto-0x883e


I see the un-encapsulated traffic on the wccp0 interface:
00:56:26.888519 IP 172.28.29.4.52128 > 216.58.203.100.80

Which is correctly showing original client IP and destination IP.

I can see hits on the iptable redirect rule:
pkts bytes target? ? ?prot opt in? ? ?out? ? ?source? ? ? ? ? ? ? ?destination? ? ? ? ?
? 429 26280 REDIRECT? ?tcp? --? wccp0? any? ? ?anywhere? ? ? ? ? ? ?anywhere? ? ? ? ? ? ?tcp dpt:http redir ports 3129


But there is no response from squid on the Ubuntu box :-(

I don't see anything helpful in either access.log or cache.log.

I'm not sure if there is anything else that could be dropping the packet apart from return path filtering..

If someone could give me some pointers or any further debugging I could try, that would be great.


Thanks.







?
?
?
?
?
?
?
?
_______________________________________________
squid-users mailing list
squid-users at lists.squid-cache.org[mailto:squid-users at lists.squid-cache.org][mailto:squid-users at lists.squid-cache.org[mailto:squid-users at lists.squid-cache.org]]
http://lists.squid-cache.org/listinfo/squid-users[http://lists.squid-cache.org/listinfo/squid-users]


From squid3 at treenet.co.nz  Wed May  9 08:50:41 2018
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Wed, 9 May 2018 20:50:41 +1200
Subject: [squid-users] Help with WCCP: Cisco 1841 to Squid 3.5.25 on
 Ubuntu 16
In-Reply-To: <trinity-8665a7a3-d08e-41a1-9140-a646fa46def5-1525847774339@3c-app-mailcom-bs05>
References: <trinity-07e4cfb1-84d9-42fc-aeab-969784ed615a-1525839715367@3c-app-mailcom-bs11>
 <CABMULtKBtepT-pYJm4jxkHumHq9=FtVW5vQQ85kKsQ5DBR3LVw@mail.gmail.com>
 <trinity-6ccac5f7-f3d9-4001-bb4f-f698b88ebc66-1525841993776@3c-app-mailcom-bs05>
 <CABMULt+i=_h301tAqmNPJCefT4cYxNynptes1hR0NPzvYrCMpg@mail.gmail.com>
 <trinity-8665a7a3-d08e-41a1-9140-a646fa46def5-1525847774339@3c-app-mailcom-bs05>
Message-ID: <9fdb01a3-3e26-1287-c051-b60ee519d62f@treenet.co.nz>

On 09/05/18 18:36, Ilias Clifton wrote:
> Ubuntu box is able to connect to the internet ok. If client PCs are configured to use the Ubuntu box as proxy on port 3128 it works correctly.
> 
> No hits in access.log for any transparent clients via wccp.. No network response at all from Ubuntu.
> 
> 
> If I change the iptables REDIRECT to a DNAT
> iptables -t nat -A PREROUTING -i wccp0 -p tcp -m tcp --dport 80 -j DNAT --to-destination 172.28.28.252:3129
> 
> 
> I do get part of the TCP handshake done..
> 
> On the Ubuntu proxy I get :
> 
> on the wccp0 interface:
> IP 172.28.29.4.53057 > 216.58.203.100.80 SYN
> 
> on the ens33 interface:
> IP 216.58.203.100.80 > 172.28.29.4.53057 SYN,ACK
> 
> The client sees the SYN,ACK, it replies and thinks it has a session
> IP 172.28.29.4.53057 > 216.58.203.100.80 ACK
> IP 172.28.29.4.53057 > 216.58.203.100.80 GET / HTTP/1.1
> 
> But really these packets are lost and never make it back to the proxy.

So the problem is likely the router settings for how those packets are
handled. Anything you can figure to find out where they are going would
be useful.


> 
> I've tried adding the following iptables rules, but reply packets still have the source ip as the original destination.
> 

Ah, that sounds like it is correct to me. The client thinks it is
talking to the origin server, not the proxy. So all the src-IP on the
reply packets have to be masqueraded as the origin server IP.


> iptables -t nat -A POSTROUTING -o ens33 -j MASQUERADE
> iptables -t nat -A POSTROUTING -o wccp0 -j MASQUERADE
> 
> Still no hits in the access.log
> 
> Should I be attempting to reply back down the gre tunnel with the REDIRECT, or replying directly to the client via DNAT. Is there any change to the squid config between these 2 options?

You configured Squid's return method as gre, so the gre tunnel should be
used for those packets. Or you could try configuring the router and
Squid as L2 return method - which seems to be the one half-working now.


Amos


From akismpa at gmail.com  Wed May  9 13:30:01 2018
From: akismpa at gmail.com (Panagiotis Bariamis)
Date: Wed, 9 May 2018 16:30:01 +0300
Subject: [squid-users] Kerberos Heimdal Server Authentication
Message-ID: <CAPxN_PVp9RETXBPZG6ZX5rzNK6Hu-HLxdAagSfgXVcg=DcdGsw@mail.gmail.com>

Hello my setup is as follows :
Freebsd 11 Heimdal Kerberos Server and DNS properly configured (testlab
enviroment for example.com domain)
Freebsd 11 squid proxy server
Windows Client


I have created a keytab from the Kerberos Server for http/squid.example.com
Proxy server machine has no problem kinit ing with the keytab file and gets
a ticket

# klist
Credentials cache: FILE:/tmp/krb5cc_0
        Principal: http/squid.example.com at EXAMPLE.COM

  Issued                Expires               Principal
May  9 15:38:36 2018  May 10 01:38:37 2018  krbtgt/EXAMPLE.COM at EXAMPLE.COM

My squid.conf is as follows concerning the authentication :
auth_param negotiate program
/usr/local/libexec/squid/negotiate_kerberos_auth
auth_param negotiate children 10 startup=1
auth_param negotiate keep_alive on

Trying to use :
 # /usr/local/libexec/squid/negotiate_kerberos_auth_test squid.example.com
| awk '{sub(/Token:/,"YR"); print $0}END{print "QQ"}'
| /usr/local/libexec/squid/negotiate_kerberos_auth -r -s http/
squid.example.com

fails with :
| negotiate_kerberos_auth_test: gss_init_sec_context() failed:  An
unsupported mechanism was requested. unknown mech-code 0 for mech unknown
BH gss_accept_sec_context() failed:  A token was invalid. unknown mech-code
0 for mech unknown
BH quit command


Any ideas ?

Thank you ,
Bariamis Panagiotis
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20180509/66aa7adc/attachment.htm>

From robertocarna36 at gmail.com  Wed May  9 23:53:29 2018
From: robertocarna36 at gmail.com (Roberto Carna)
Date: Wed, 9 May 2018 20:53:29 -0300
Subject: [squid-users] PID file /var/run/squid.pid not readable AND
 Supervising process XXX which is not our child
Message-ID: <CAG2Qp6t_J4h56rQ0ZE9owc=bK4vuiaz9zktAcXHMytHCWKskUg@mail.gmail.com>

Dear, I have Squid/Dansguardian in a Debian 9 server.

My Squid packages is from Debian repo, it is the stable version:

squid                          3.5.23-5+deb9u1

After execute "squid -k parse" everything is OK:

# systemctl status squid
? squid.service - LSB: Squid HTTP Proxy version 3.x
   Loaded: loaded (/etc/init.d/squid; generated; vendor preset: enabled)
   Active: active (running) since Wed 2018-05-09 20:30:36 -03; 2min 24s ago
     Docs: man:systemd-sysv-generator(8)
  Process: 829 ExecStart=/etc/init.d/squid start (code=exited, status=0/SUCCESS)
 Main PID: 895 (squid)
    Tasks: 7 (limit: 4915)
   CGroup: /system.slice/squid.service
           ??893 /usr/sbin/squid -YC -f /etc/squid/squid.conf
           ??895 (squid-1) -YC -f /etc/squid/squid.conf
           ??896 (pinger)
           ??932 (squid_ldap_auth) -b dc=company,dc=com -f uid=%s -h
ldap.company.com -v 3
           ??933 (squid_ldap_auth) -b dc=company,dc=com -f uid=%s -h
ldap.company.com -v 3
           ??934 (squid_ldap_auth) -b dc=company,dc=com -f uid=%s -h
ldap.company.com -v 3
           ??935 (squid_ldap_auth) -b dc=company,dc=com  -f uid=%s -h
ldap.company.com -v 3

May 09 20:30:35 proxy-nuevo systemd[1]: Starting LSB: Squid HTTP Proxy
version 3.x...
May 09 20:30:35 proxy-nuevo squid[893]: Squid Parent: will start 1 kids
May 09 20:30:35 proxy-nuevo squid[829]: Starting Squid HTTP Proxy: squid.
May 09 20:30:35 proxy-nuevo squid[893]: Squid Parent: (squid-1)
process 895 started
May 09 20:30:35 proxy-nuevo systemd[1]: squid.service: PID file
/var/run/squid.pid not readable (yet?) after start: No such file or
directory
May 09 20:30:36 proxy-nuevo systemd[1]: squid.service: Supervising
process 895 which is not our child. We'll most likely not notice when
it exits.
May 09 20:30:36 proxy-nuevo systemd[1]: Started LSB: Squid HTTP Proxy
version 3.x.

But when I read I notice two curious lines:

systemd[1]: squid.service: PID file /var/run/squid.pid not readable
(yet?) after start: No such file or directory
systemd[1]: squid.service: Supervising process 895 which is not our
child. We'll most likely not notice when it exits.


Is it normal or do I have to solve these? I repeat Squid is running OK...

Thanks


From h1179169 at nwytg.com  Thu May 10 00:31:51 2018
From: h1179169 at nwytg.com (OlegH1)
Date: Wed, 9 May 2018 17:31:51 -0700 (MST)
Subject: [squid-users] Getting WebSocket without Upgrage/Connection headers
Message-ID: <1525912311222-0.post@n4.nabble.com>

Hi,

I'm seeing some users of mine use squid proxy and cannot connect via
WebSocket because the Upgrade and Connection headers are striped. All
connections are over wss:// (TLS).

My question is how possible that squid decrypts the packet, omit those
headers and encrypt. I saw there is a "SSL bump" extension that allows it (I
guess the "fake" cert should be install in the computers explicitly) - but
even if it's possible, why those headers are stripped?

I saw several topics regard this issue, but didn't really understand the
problem. If someone set a squid proxy with SSL bump, how is it even possible
that squid blocks it? I tried to use squid locally on my computer and it
looks that those headers passed just fine (connection initiated correctly),
so why is it mentioned that those headers are not supported?

Sorry - I'm confused a bit:)

Thanks!



--
Sent from: http://squid-web-proxy-cache.1019090.n4.nabble.com/Squid-Users-f1019091.html


From squid3 at treenet.co.nz  Thu May 10 02:09:39 2018
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 10 May 2018 14:09:39 +1200
Subject: [squid-users] PID file /var/run/squid.pid not readable AND
 Supervising process XXX which is not our child
In-Reply-To: <CAG2Qp6t_J4h56rQ0ZE9owc=bK4vuiaz9zktAcXHMytHCWKskUg@mail.gmail.com>
References: <CAG2Qp6t_J4h56rQ0ZE9owc=bK4vuiaz9zktAcXHMytHCWKskUg@mail.gmail.com>
Message-ID: <2cd09b57-faab-fd11-2381-19d5d7c5db4d@treenet.co.nz>

On 10/05/18 11:53, Roberto Carna wrote:
> Dear, I have Squid/Dansguardian in a Debian 9 server.
> 
> My Squid packages is from Debian repo, it is the stable version:
> 
> squid                          3.5.23-5+deb9u1
...
> 
> But when I read I notice two curious lines:
> 
> systemd[1]: squid.service: PID file /var/run/squid.pid not readable
> (yet?) after start: No such file or directory
> systemd[1]: squid.service: Supervising process 895 which is not our
> child. We'll most likely not notice when it exits.
> 
> 
> Is it normal or do I have to solve these? I repeat Squid is running OK...

systemd cannot cope with daemons like Squid-3. All you can do for now is
ensure that you use the init.d scripts to manage Squid. Do not use the
"service ..." commands provided by systemd.

Squid-4 packages that resolve these issues are in Debian experimental
awaiting an official upstream stable release.
 NP: the major bugs preventing upstream stable are not affecting the
Debian package features. You can use the Squid-4 package now if you wish
by adding that "experimental" repository to your apt sources.list,
update apt, then install/upgrade Squid with "apt-get -t experimental
install squid".

Amos


From squid3 at treenet.co.nz  Thu May 10 02:32:08 2018
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 10 May 2018 14:32:08 +1200
Subject: [squid-users] Getting WebSocket without Upgrage/Connection
 headers
In-Reply-To: <1525912311222-0.post@n4.nabble.com>
References: <1525912311222-0.post@n4.nabble.com>
Message-ID: <d2fff31e-2664-f428-e5fb-f4a42c0ea24b@treenet.co.nz>

On 10/05/18 12:31, OlegH1 wrote:
> Hi,
> 
> I'm seeing some users of mine use squid proxy and cannot connect via
> WebSocket because the Upgrade and Connection headers are striped. All
> connections are over wss:// (TLS).
> 
> My question is how possible that squid decrypts the packet, omit those
> headers and encrypt. I saw there is a "SSL bump" extension that allows it (I
> guess the "fake" cert should be install in the computers explicitly) - but
> even if it's possible, why those headers are stripped?

Unless you configured Squid SSL-Bump features to be enabled Squid does
not decrypt anything. So it will only be dealing with the HTTP side of
the traffic.

WebSockets is a sub-protocol that operates over HTTP as well as HTTPS.
So the headers are can be available to Squid in un-encrypted traffic -
that is a client software choice. If HTTP is used Squid lack of support
for HTTP/1.1 Upgrade is in effect.

A client that properly follows the WebSocket protocol should have
several fallback connection types it can use instead of HTTP through the
proxy. Those choices include native WebSockets protocol on a non-HTTP(S)
port, or continuing to use HTTP messaging (working) instead of WebSocket
(not working).


> 
> I saw several topics regard this issue, but didn't really understand the
> problem. If someone set a squid proxy with SSL bump, how is it even possible
> that squid blocks it?

If SSL-Bump is configured, Squid decrypts the TLS layer. Exposing the
fact that the supposedly "HTTPS" tunnel actually contains WebSocket
protocol instead of HTTP(S). Since WebSocket and HTTP/1.1 Upgrade to
start it is not supported by Squid one of two things happen;
 a) the HTTP/1.1 Upgrade header (if any) gets removed when the request
is relayed to the server, or
 b) WebSocket native protocol has Squid on_unsupported_protocol action
performed - if your Squid is one of the older version that lack that
directive the connection just gets rejected.


> I tried to use squid locally on my computer and it
> looks that those headers passed just fine (connection initiated correctly),
> so why is it mentioned that those headers are not supported?
> 

They should not have been relayed. If properly working the Upgrade
header should be listed in the Connection: header which causes it to be
removed by Squid (or any other middleware without Upgrade:WebSocket
support) on the purely HTTP(S) traffic sent to servers.

I suspect you omitted that mandatory Connection:Upgrade header in your test.


Amos


From robertocarna36 at gmail.com  Thu May 10 12:48:19 2018
From: robertocarna36 at gmail.com (Roberto Carna)
Date: Thu, 10 May 2018 09:48:19 -0300
Subject: [squid-users] PID file /var/run/squid.pid not readable AND
 Supervising process XXX which is not our child
In-Reply-To: <2cd09b57-faab-fd11-2381-19d5d7c5db4d@treenet.co.nz>
References: <CAG2Qp6t_J4h56rQ0ZE9owc=bK4vuiaz9zktAcXHMytHCWKskUg@mail.gmail.com>
 <2cd09b57-faab-fd11-2381-19d5d7c5db4d@treenet.co.nz>
Message-ID: <CAG2Qp6ub7ZbV_BwxiahSrO24mE4FHvro5hcBQs_B2ygp55aMCg@mail.gmail.com>

Dear Amos, thanks for your response.

Just a short question: if I continue using my currenbt Squid version
with " squid.service: PID file /var/run/squid.pid not readable"
warning...can I have any problem or it doesn't represent any problem?

Regards!!!

2018-05-09 23:09 GMT-03:00 Amos Jeffries <squid3 at treenet.co.nz>:
> On 10/05/18 11:53, Roberto Carna wrote:
>> Dear, I have Squid/Dansguardian in a Debian 9 server.
>>
>> My Squid packages is from Debian repo, it is the stable version:
>>
>> squid                          3.5.23-5+deb9u1
> ...
>>
>> But when I read I notice two curious lines:
>>
>> systemd[1]: squid.service: PID file /var/run/squid.pid not readable
>> (yet?) after start: No such file or directory
>> systemd[1]: squid.service: Supervising process 895 which is not our
>> child. We'll most likely not notice when it exits.
>>
>>
>> Is it normal or do I have to solve these? I repeat Squid is running OK...
>
> systemd cannot cope with daemons like Squid-3. All you can do for now is
> ensure that you use the init.d scripts to manage Squid. Do not use the
> "service ..." commands provided by systemd.
>
> Squid-4 packages that resolve these issues are in Debian experimental
> awaiting an official upstream stable release.
>  NP: the major bugs preventing upstream stable are not affecting the
> Debian package features. You can use the Squid-4 package now if you wish
> by adding that "experimental" repository to your apt sources.list,
> update apt, then install/upgrade Squid with "apt-get -t experimental
> install squid".
>
> Amos
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users


From squid3 at treenet.co.nz  Thu May 10 16:55:28 2018
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Fri, 11 May 2018 04:55:28 +1200
Subject: [squid-users] PID file /var/run/squid.pid not readable AND
 Supervising process XXX which is not our child
In-Reply-To: <CAG2Qp6ub7ZbV_BwxiahSrO24mE4FHvro5hcBQs_B2ygp55aMCg@mail.gmail.com>
References: <CAG2Qp6t_J4h56rQ0ZE9owc=bK4vuiaz9zktAcXHMytHCWKskUg@mail.gmail.com>
 <2cd09b57-faab-fd11-2381-19d5d7c5db4d@treenet.co.nz>
 <CAG2Qp6ub7ZbV_BwxiahSrO24mE4FHvro5hcBQs_B2ygp55aMCg@mail.gmail.com>
Message-ID: <84c38b12-21ce-644e-7c59-1532223251ca@treenet.co.nz>

On 11/05/18 00:48, Roberto Carna wrote:
> Dear Amos, thanks for your response.
> 
> Just a short question: if I continue using my currenbt Squid version
> with " squid.service: PID file /var/run/squid.pid not readable"
> warning...can I have any problem or it doesn't represent any problem?

It represents a problem, because systemd may interfere with the
auto-restart built into Squid in the event of crashes etc.

However, that problem wont affect normal operation, just the crash
handling and startup/shutdown processes of Squid. So YMMV whether that
is an issue relevant to your installation.


HTH
Amos


From robertocarna36 at gmail.com  Thu May 10 19:11:29 2018
From: robertocarna36 at gmail.com (Roberto Carna)
Date: Thu, 10 May 2018 16:11:29 -0300
Subject: [squid-users] PID file /var/run/squid.pid not readable AND
 Supervising process XXX which is not our child
In-Reply-To: <84c38b12-21ce-644e-7c59-1532223251ca@treenet.co.nz>
References: <CAG2Qp6t_J4h56rQ0ZE9owc=bK4vuiaz9zktAcXHMytHCWKskUg@mail.gmail.com>
 <2cd09b57-faab-fd11-2381-19d5d7c5db4d@treenet.co.nz>
 <CAG2Qp6ub7ZbV_BwxiahSrO24mE4FHvro5hcBQs_B2ygp55aMCg@mail.gmail.com>
 <84c38b12-21ce-644e-7c59-1532223251ca@treenet.co.nz>
Message-ID: <CAG2Qp6stx0GT+-v_yRy-c7sx=JeS_OfV7W53Nb9+rd8egRHEsg@mail.gmail.com>

Dear, my problem is that I have two different implementations of Squid
/ Dansguardian for just 40/50 users for navigation, and sometimes the
web browsing is too slow without any clue about this, it's just better
if I restart both daemons:

1) squid                          3.5.23-5+deb9u1 / dansguardian
            2.10.1.1-5.1+b4
2) squid3                             3.1.20-2.2+deb7u8   /
dansguardian                       2.10.1.1-5+b1

I've adjusted both configurations but the problem of slow navigation
appears always.

So I'm thinking maybe is a problem of this packages combination (squid
+ dansguardian)...

Do you recommend upgrade to Debian testing version packages or just to
usetry with Squid + Squidguard ??'?

Special thanks !!!

Regards.

2018-05-10 13:55 GMT-03:00 Amos Jeffries <squid3 at treenet.co.nz>:
> On 11/05/18 00:48, Roberto Carna wrote:
>> Dear Amos, thanks for your response.
>>
>> Just a short question: if I continue using my currenbt Squid version
>> with " squid.service: PID file /var/run/squid.pid not readable"
>> warning...can I have any problem or it doesn't represent any problem?
>
> It represents a problem, because systemd may interfere with the
> auto-restart built into Squid in the event of crashes etc.
>
> However, that problem wont affect normal operation, just the crash
> handling and startup/shutdown processes of Squid. So YMMV whether that
> is an issue relevant to your installation.
>
>
> HTH
> Amos
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users


From squid3 at treenet.co.nz  Fri May 11 08:39:38 2018
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Fri, 11 May 2018 20:39:38 +1200
Subject: [squid-users] PID file /var/run/squid.pid not readable AND
 Supervising process XXX which is not our child
In-Reply-To: <CAG2Qp6stx0GT+-v_yRy-c7sx=JeS_OfV7W53Nb9+rd8egRHEsg@mail.gmail.com>
References: <CAG2Qp6t_J4h56rQ0ZE9owc=bK4vuiaz9zktAcXHMytHCWKskUg@mail.gmail.com>
 <2cd09b57-faab-fd11-2381-19d5d7c5db4d@treenet.co.nz>
 <CAG2Qp6ub7ZbV_BwxiahSrO24mE4FHvro5hcBQs_B2ygp55aMCg@mail.gmail.com>
 <84c38b12-21ce-644e-7c59-1532223251ca@treenet.co.nz>
 <CAG2Qp6stx0GT+-v_yRy-c7sx=JeS_OfV7W53Nb9+rd8egRHEsg@mail.gmail.com>
Message-ID: <64260abf-600b-f462-b59a-5f7b645594e4@treenet.co.nz>

On 11/05/18 07:11, Roberto Carna wrote:
> Dear, my problem is that I have two different implementations of Squid
> / Dansguardian for just 40/50 users for navigation, and sometimes the
> web browsing is too slow without any clue about this, it's just better
> if I restart both daemons:
> 
> 1) squid                          3.5.23-5+deb9u1 / dansguardian
>             2.10.1.1-5.1+b4
> 2) squid3                             3.1.20-2.2+deb7u8   /
> dansguardian                       2.10.1.1-5+b1
> 
> I've adjusted both configurations but the problem of slow navigation
> appears always.
> 
> So I'm thinking maybe is a problem of this packages combination (squid
> + dansguardian)...
> 
> Do you recommend upgrade to Debian testing version packages or just to
> usetry with Squid + Squidguard ??'?


I certainly recommend upgrading the 3.1 package to 3.5 on general
principles - it has a lot of problems that are resolved in 3.5. Is/was
there a specific reason for that old version to still be present?


You should probably try the v4 squid package and see if it is better.
Without knowing the actual reason for the delays I'm reluctant to make a
specific recommendation for any software.


Amos


From alex at dvm.esines.cu  Fri May 11 12:30:36 2018
From: alex at dvm.esines.cu (=?UTF-8?Q?Alex_Guti=c3=a9rrez_Mart=c3=adnez?=)
Date: Fri, 11 May 2018 08:30:36 -0400
Subject: [squid-users] passwd problem
Message-ID: <68c582b1-286a-f1c4-59c5-1b5896e21b7d@dvm.esines.cu>

Hello community, yesterday i noticed that one of my users is using a 
private squid to avoid the tedious problem of typing the password every 
time he starts session in his browser. The problem is that the squid 
suddenly started to return in plain text the username and password. 
Therefore, that information is reflected in my proxy log.

The directive that the user used is:

"cache_peer proxy.mynetwork.cu parent 3229 0 no-query no-digest default 
login = user1: passwd1"

where "proxy.mynetwork.cu" is my proxy, "user1" the user and "passwd1" 
the password.

Is there any way to prevent this behavior from happening again?

Thanks in advance

-- 
Saludos Cordiales

Lic. Alex Guti?rrez Mart?nez

Tel. +53 7 2710327

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20180511/5d520d81/attachment.htm>

From squid3 at treenet.co.nz  Fri May 11 14:24:34 2018
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sat, 12 May 2018 02:24:34 +1200
Subject: [squid-users] passwd problem
In-Reply-To: <68c582b1-286a-f1c4-59c5-1b5896e21b7d@dvm.esines.cu>
References: <68c582b1-286a-f1c4-59c5-1b5896e21b7d@dvm.esines.cu>
Message-ID: <8eb69461-637c-5bad-e116-eb9219d2c83b@treenet.co.nz>

On 12/05/18 00:30, Alex Guti?rrez Mart?nez wrote:
> Hello community, yesterday i noticed that one of my users is using a
> private squid to avoid the tedious problem of typing the password every
> time he starts session in his browser. The problem is that the squid
> suddenly started to return in plain text the username and password.
> Therefore, that information is reflected in my proxy log.
> 
> The directive that the user used is:
> 
> "cache_peer proxy.mynetwork.cu parent 3229 0 no-query no-digest default
> login = user1: passwd1"

Ensure that there are no whitespace in that login string. It should just be:
 login=user1:passwd1

Also ensure that the user1 and passwd1 are correct values accepted by
the peer for Basic auth.

Amos


From huaraz at moeller.plus.com  Fri May 11 18:49:07 2018
From: huaraz at moeller.plus.com (Markus Moeller)
Date: Fri, 11 May 2018 19:49:07 +0100
Subject: [squid-users] Kerberos authentication on mobile phones
In-Reply-To: <36775d21-090a-e22a-bec0-78edc57541a9@treenet.co.nz>
References: <CAPxN_PXe=KhsRyxTp54uF4t0Ub731YzHMc62+zAV=rBuGoL3Hg@mail.gmail.com>
 <36775d21-090a-e22a-bec0-78edc57541a9@treenet.co.nz>
Message-ID: <pd4of1$9l0$1@blaine.gmane.org>

You don't have to join a domain.  You only need a Kerberos authentication 
server to get a ticket.

You only need AD (or Samba) if you want also authorisation (PAC data) in you 
Kerberos ticket.

As Amos said you need a Kerberos client and a Browser supporting 
Proxy-Negotiate.

Markus

"Amos Jeffries"  wrote in message 
news:36775d21-090a-e22a-bec0-78edc57541a9 at treenet.co.nz...

On 08/05/18 10:22, Panagiotis Bariamis wrote:
> Hello,
> Is it possible with a squid kerberos only authentication  setup be able
> to authenticate ie android phones to squid?

I don't have an answer for that, maybe someone else has experience. If
you have the environment available you could try it yourself.


> A second question. If a non domain joined machine tries to use the proxy
> will there be a username password prompt where if correct credentials
> are presented he will be able to get a ticket to use squid?

Maybe, unlikely though IMO. Getting a ticket requires first joining the
domain. Some client software may provide a popup and then try to contact
a DC and join a domain.

But whether a) the specific client software does that, and b) whether
info about the domain DC server is available in DNS records, and c)
whether the Kerberos realm "domain" matches the proxy DNS record domain
- all those effect the possibilities AFAIK.

Amos
_______________________________________________
squid-users mailing list
squid-users at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-users 




From huaraz at moeller.plus.com  Fri May 11 19:00:51 2018
From: huaraz at moeller.plus.com (Markus Moeller)
Date: Fri, 11 May 2018 20:00:51 +0100
Subject: [squid-users] Kerberos Heimdal Server Authentication
In-Reply-To: <CAPxN_PVp9RETXBPZG6ZX5rzNK6Hu-HLxdAagSfgXVcg=DcdGsw@mail.gmail.com>
References: <CAPxN_PVp9RETXBPZG6ZX5rzNK6Hu-HLxdAagSfgXVcg=DcdGsw@mail.gmail.com>
Message-ID: <pd4p50$ntq$1@blaine.gmane.org>

Can you capture the traffic on port 88 ? Heimdal has not helpful messages, so seeing the real traffic may help identifying the issue.

Kinit should create an AS req/rep
the test program creates a TGS req/rep

Example attached if it gets through.

Markus

"Panagiotis Bariamis" <akismpa at gmail.com> wrote in message news:CAPxN_PVp9RETXBPZG6ZX5rzNK6Hu-HLxdAagSfgXVcg=DcdGsw at mail.gmail.com...
Hello my setup is as follows :

Freebsd 11 Heimdal Kerberos Server and DNS properly configured (testlab enviroment for example.com domain) 

Freebsd 11 squid proxy server 

Windows Client 



I have created a keytab from the Kerberos Server for http/squid.example.com

Proxy server machine has no problem kinit ing with the keytab file and gets a ticket 

# klist
Credentials cache: FILE:/tmp/krb5cc_0
        Principal: http/squid.example.com at EXAMPLE.COM

  Issued                Expires               Principal
May  9 15:38:36 2018  May 10 01:38:37 2018  krbtgt/EXAMPLE.COM at EXAMPLE.COM


My squid.conf is as follows concerning the authentication :
auth_param negotiate program /usr/local/libexec/squid/negotiate_kerberos_auth
auth_param negotiate children 10 startup=1
auth_param negotiate keep_alive on


Trying to use :
# /usr/local/libexec/squid/negotiate_kerberos_auth_test squid.example.com 
| awk '{sub(/Token:/,"YR"); print $0}END{print "QQ"}' 
| /usr/local/libexec/squid/negotiate_kerberos_auth -r -s http/squid.example.com


fails with :
| negotiate_kerberos_auth_test: gss_init_sec_context() failed:  An unsupported mechanism was requested. unknown mech-code 0 for mech unknown
BH gss_accept_sec_context() failed:  A token was invalid. unknown mech-code 0 for mech unknown
BH quit command



Any ideas ?


Thank you , 

Bariamis Panagiotis 





--------------------------------------------------------------------------------
_______________________________________________
squid-users mailing list
squid-users at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-users
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20180511/91a08ab1/attachment.htm>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: krb5.pcap
Type: application/octet-stream
Size: 2865 bytes
Desc: not available
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20180511/91a08ab1/attachment.obj>

From eliezer at ngtech.co.il  Sat May 12 22:25:34 2018
From: eliezer at ngtech.co.il (Eliezer Croitoru)
Date: Sun, 13 May 2018 01:25:34 +0300
Subject: [squid-users] Collecting squid logs to DB
In-Reply-To: <CABMULtLisrtCimu1xsk+K4EydUypbA2N_1XRbUkO6CtGBubRrQ@mail.gmail.com>
References: <CABMULtLisrtCimu1xsk+K4EydUypbA2N_1XRbUkO6CtGBubRrQ@mail.gmail.com>
Message-ID: <0c5901d3ea40$2541eb80$6fc5c280$@ngtech.co.il>

Hey Alex,

 

How did you used to log into the DB? What configuration lines have you used?

Also what log format have you used?

Is it important to have realtime data in the DB or a periodic parsing is also an option?

 

Eliezer

 

----

 <http://ngtech.co.il/lmgtfy/> Eliezer Croitoru
Linux System Administrator
Mobile: +972-5-28704261
Email: eliezer at ngtech.co.il



 

From: squid-users <squid-users-bounces at lists.squid-cache.org> On Behalf Of Alex K
Sent: Saturday, May 5, 2018 01:20
To: squid-users at lists.squid-cache.org
Subject: [squid-users] Collecting squid logs to DB

 

Hi all, 

I had a previous setup on Debian 7 with squid and I was using mysar to collect squid logs and store them to DB and provide some browsing report at the end of the day. 

Now at Debian 9, trying to upgrade the whole setup, I see that mysar does not compile. 

Checking around I found mysar-ng but this has compilation issues on Debian 9 also. 

Do you suggest any tool that does this job? Does squid support logging to DB natively? (I am using mysql/mariadb)

Some other tool I stumbled on is https://github.com/paranormal/blooper. 

 

Thanx a bunch,

Alex

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20180513/05deff5a/attachment.htm>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: image002.png
Type: image/png
Size: 11308 bytes
Desc: not available
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20180513/05deff5a/attachment.png>

From rightkicktech at gmail.com  Sat May 12 22:55:47 2018
From: rightkicktech at gmail.com (Alex K)
Date: Sun, 13 May 2018 01:55:47 +0300
Subject: [squid-users] Collecting squid logs to DB
In-Reply-To: <0c5901d3ea40$2541eb80$6fc5c280$@ngtech.co.il>
References: <CABMULtLisrtCimu1xsk+K4EydUypbA2N_1XRbUkO6CtGBubRrQ@mail.gmail.com>
 <0c5901d3ea40$2541eb80$6fc5c280$@ngtech.co.il>
Message-ID: <CABMULt++CYhn7+SLKaDi6vuisBZkNXPbSw5LHY8oxu_ApmWOWw@mail.gmail.com>

+++ Including list +++

Hi Eliezer,

I have used the following lines to instruct squid to log at mariadb:

logfile_daemon /usr/lib/squid/log_db_daemon
access_log daemon:/127.0.0.1/squid_log/access_log/squid/squid squid

Through testing it seems that sometimes squid is not logging anything. I
don't know why. After a restart it seems to unblock and write to DB.
The access_log table is currently InnoDB and I am wondering if MyISAM will
behave better.

I would prefer if I could have real time access log. My scenario is that
when a user disconnects from squid, an aggregated report of the sites that
the user browsed will be available under some web portal where the user has
access. Usually there will be up to 20 users connected concurrently so I
have to check if this approach is scalable. If this approach is not stable
then I might go with log parsing (perhaps logstash or some custom parser)
which will parse and generate an aggregated report once per hour or day.

Is there a way I format the log and pipe to DB only some interesting fields
in order to lessen the stress to DB?


On Sun, May 13, 2018 at 1:25 AM, Eliezer Croitoru <eliezer at ngtech.co.il>
wrote:

> Hey Alex,
>
>
>
> How did you used to log into the DB? What configuration lines have you
> used?
>
> Also what log format have you used?
>
> Is it important to have realtime data in the DB or a periodic parsing is
> also an option?
>
>
>
> Eliezer
>
>
>
> ----
>
> Eliezer Croitoru
> Linux System Administrator
> Mobile: +972-5-28704261
> Email: eliezer at ngtech.co.il
>
>
>
> *From:* squid-users <squid-users-bounces at lists.squid-cache.org> *On
> Behalf Of *Alex K
> *Sent:* Saturday, May 5, 2018 01:20
> *To:* squid-users at lists.squid-cache.org
> *Subject:* [squid-users] Collecting squid logs to DB
>
>
>
> Hi all,
>
> I had a previous setup on Debian 7 with squid and I was using mysar to
> collect squid logs and store them to DB and provide some browsing report at
> the end of the day.
>
> Now at Debian 9, trying to upgrade the whole setup, I see that mysar does
> not compile.
>
> Checking around I found mysar-ng but this has compilation issues on Debian
> 9 also.
>
> Do you suggest any tool that does this job? Does squid support logging to
> DB natively? (I am using mysql/mariadb)
>
> Some other tool I stumbled on is https://github.com/paranormal/blooper.
>
>
>
> Thanx a bunch,
>
> Alex
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20180513/4e08301a/attachment.htm>

From huaraz at moeller.plus.com  Sat May 12 23:58:53 2018
From: huaraz at moeller.plus.com (Markus Moeller)
Date: Sun, 13 May 2018 00:58:53 +0100
Subject: [squid-users] Kerberos authentication on mobile phones
In-Reply-To: <36775d21-090a-e22a-bec0-78edc57541a9@treenet.co.nz>
References: <CAPxN_PXe=KhsRyxTp54uF4t0Ub731YzHMc62+zAV=rBuGoL3Hg@mail.gmail.com>
 <36775d21-090a-e22a-bec0-78edc57541a9@treenet.co.nz>
Message-ID: <pd7uvs$nlq$1@blaine.gmane.org>

You don't have to join a domain.  You only need a Kerberos authentication
server to get a ticket.

You only need AD (or Samba) if you want also authorisation (PAC data) in you
Kerberos ticket.

As Amos said you need a Kerberos client and a Browser supporting
Proxy-Negotiate.

Markus

"Amos Jeffries"  wrote in message 
news:36775d21-090a-e22a-bec0-78edc57541a9 at treenet.co.nz...

On 08/05/18 10:22, Panagiotis Bariamis wrote:
> Hello,
> Is it possible with a squid kerberos only authentication  setup be able
> to authenticate ie android phones to squid?

I don't have an answer for that, maybe someone else has experience. If
you have the environment available you could try it yourself.


> A second question. If a non domain joined machine tries to use the proxy
> will there be a username password prompt where if correct credentials
> are presented he will be able to get a ticket to use squid?

Maybe, unlikely though IMO. Getting a ticket requires first joining the
domain. Some client software may provide a popup and then try to contact
a DC and join a domain.

But whether a) the specific client software does that, and b) whether
info about the domain DC server is available in DNS records, and c)
whether the Kerberos realm "domain" matches the proxy DNS record domain
- all those effect the possibilities AFAIK.

Amos
_______________________________________________
squid-users mailing list
squid-users at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-users 




From eliezer at ngtech.co.il  Sun May 13 01:21:18 2018
From: eliezer at ngtech.co.il (Eliezer Croitoru)
Date: Sun, 13 May 2018 04:21:18 +0300
Subject: [squid-users] Collecting squid logs to DB
In-Reply-To: <CABMULt++CYhn7+SLKaDi6vuisBZkNXPbSw5LHY8oxu_ApmWOWw@mail.gmail.com>
References: <CABMULtLisrtCimu1xsk+K4EydUypbA2N_1XRbUkO6CtGBubRrQ@mail.gmail.com>
 <0c5901d3ea40$2541eb80$6fc5c280$@ngtech.co.il>
 <CABMULt++CYhn7+SLKaDi6vuisBZkNXPbSw5LHY8oxu_ApmWOWw@mail.gmail.com>
Message-ID: <0c8001d3ea58$b22272a0$166757e0$@ngtech.co.il>

I have a daemon written in Ruby and GoLang which can do a better job.

Specifically for your scenario I think the better option is to use a tcp server such as in:

https://wiki.squid-cache.org/Features/LogModules#Module:_TCP_Receiver

 

Ubuntu and Debian use systemd and you can spawn a log daemon which is not related directly to squid.

MyISAM and InnoDB is not a question InnoDB is the only relevant choice for couple reasons from my expirence.

I will try to update you here with the relevant script that you might be able to use for real time logging.

 

You should be able to use the next line for your purpose:

access_log tcp://127.0.0.1:5000 squid

 

Eliezer

 

----

 <http://ngtech.co.il/lmgtfy/> Eliezer Croitoru
Linux System Administrator
Mobile: +972-5-28704261
Email: eliezer at ngtech.co.il



 

From: Alex K <rightkicktech at gmail.com> 
Sent: Sunday, May 13, 2018 01:56
To: Eliezer Croitoru <eliezer at ngtech.co.il>
Cc: squid-users at lists.squid-cache.org
Subject: Re: [squid-users] Collecting squid logs to DB

 

+++ Including list +++

Hi Eliezer, 

I have used the following lines to instruct squid to log at mariadb: 

logfile_daemon /usr/lib/squid/log_db_daemon
access_log daemon:/127.0.0.1/squid_log/access_log/squid/squid <http://127.0.0.1/squid_log/access_log/squid/squid>  squid

Through testing it seems that sometimes squid is not logging anything. I don't know why. After a restart it seems to unblock and write to DB. 

The access_log table is currently InnoDB and I am wondering if MyISAM will behave better. 

 

I would prefer if I could have real time access log. My scenario is that when a user disconnects from squid, an aggregated report of the sites that the user browsed will be available under some web portal where the user has access. Usually there will be up to 20 users connected concurrently so I have to check if this approach is scalable. If this approach is not stable then I might go with log parsing (perhaps logstash or some custom parser) which will parse and generate an aggregated report once per hour or day. 

Is there a way I format the log and pipe to DB only some interesting fields in order to lessen the stress to DB?

 

 

On Sun, May 13, 2018 at 1:25 AM, Eliezer Croitoru <eliezer at ngtech.co.il <mailto:eliezer at ngtech.co.il> > wrote:

Hey Alex,

 

How did you used to log into the DB? What configuration lines have you used?

Also what log format have you used?

Is it important to have realtime data in the DB or a periodic parsing is also an option?

 

Eliezer

 

----

Eliezer Croitoru
Linux System Administrator
Mobile: +972-5-28704261
Email: eliezer at ngtech.co.il <mailto:eliezer at ngtech.co.il> 



 

From: squid-users <squid-users-bounces at lists.squid-cache.org <http://squid-cache.org> > On Behalf Of Alex K
Sent: Saturday, May 5, 2018 01:20
To: squid-users at lists.squid-cache.org <mailto:squid-users at lists.squid-cache.org> 
Subject: [squid-users] Collecting squid logs to DB

 

Hi all, 

I had a previous setup on Debian 7 with squid and I was using mysar to collect squid logs and store them to DB and provide some browsing report at the end of the day. 

Now at Debian 9, trying to upgrade the whole setup, I see that mysar does not compile. 

Checking around I found mysar-ng but this has compilation issues on Debian 9 also. 

Do you suggest any tool that does this job? Does squid support logging to DB natively? (I am using mysql/mariadb)

Some other tool I stumbled on is https://github.com/paranormal/blooper. 

 

Thanx a bunch,

Alex

 

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20180513/389221d0/attachment.htm>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: image001.png
Type: image/png
Size: 11308 bytes
Desc: not available
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20180513/389221d0/attachment.png>

From greencoppermine at yandex.com  Sun May 13 01:55:56 2018
From: greencoppermine at yandex.com (Martin Hanson)
Date: Sun, 13 May 2018 03:55:56 +0200
Subject: [squid-users] Whitelist ONLY exception isn't working correctly
Message-ID: <3460451526176556@web17o.yandex.ru>

Hi,

I have a setup with a PF firewall that intercepts HTTP and HTTPS traffic and forwards that to Squid. Squid is setup to log all traffic and it uses a SSL bump for the HTTPS traffic.

In the setup I have a whitelist of domains that doesn't get logged, the rest of the traffic gets logged and redirected to SquidGuard for further blacklisting. All that works great.

What I cannot figure out is how to add a couple of local IP addresses that can ONLY access the whitelist (or possibly ANOTHER whitelist) and nothing else on the Internet.

The ACL for the "windows_boxes" is the one that is supposed to ONLY access the whitelisted domains. However, when I enter the URL https://www.mojang.com I get a "Access denied". However, if I add a NON-SSL domain to the whitelist, then those works.

This is my current squid.conf. I know I am overlooking something, but I cannot figure out what I am doing wrong.

<SNIP>
acl step1 at_step SslBump1
acl localnet src 192.168.1.0/24

# These boxes may ONLY access the whitelist.
acl windows_boxes src 192.168.1.201 192.168.1.202

acl whitelist ssl::server_name .mojang.com .minecraft.net d2pi0bc9ewx28h.cloudfront.net mcupdate.tumblr.com minecraft-textures-1196058387.us-east-1.elb.amazonaws.com .steampowered.com .steamcommunity.com .steamgames.com .steamusercontent.com .steamcontent.com .steamstatic.com .akamaihd.net .ubuntu.com

# We don't want the whitelist to be cached.
cache deny whitelist

# We want direct access on the whitelist.
always_direct allow whitelist

# Don't redirect to SquidGuard.
redirector_access deny whitelist

# We only redirect HTTP and HTTPS.
acl SSL_ports port 443
acl Safe_ports port 80          # http
acl Safe_ports port 443         # https
acl CONNECT method CONNECT
http_access deny !Safe_ports
http_access deny CONNECT !SSL_ports

# Only allow cachemgr access from localhost
http_access allow localhost manager
http_access deny manager

# !!! THIS ISN'T WORKING !!! ubuntu.com, mojang.com still gets blocked on these boxes.
http_access deny windows_boxes !whitelist

http_access allow localhost
http_access allow localnet

http_access deny all

# We'll intercept trafic using PF.
http_port 127.0.0.1:3129 intercept
https_port 127.0.0.1:3130 intercept ssl-bump cert=/etc/squid/ssl_cert/myCA.pem generate-host-certificates=on dynamic_cert_mem_cache_size=4MB
sslcrtd_program /usr/local/libexec/squid/ssl_crtd -s /var/lib/ssl_db -M 4MB
sslproxy_cafile /usr/local/openssl/cabundle.file

# Become a TCP tunnel without decrypting proxied traffic for the whitelist.
ssl_bump splice whitelist
ssl_bump peek step1
ssl_bump bump all

# We want the query strings as well.
strip_query_terms off

# Leave coredumps in the first cache dir
coredump_dir /var/squid/cache

redirect_program /usr/local/bin/squidGuard -c /etc/squidguard/squidguard.conf
</SNIP>

Any feedback would be greatly appreciated! Thank you in advance!

Kindest regards. 


From rousskov at measurement-factory.com  Sun May 13 02:17:46 2018
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Sat, 12 May 2018 20:17:46 -0600
Subject: [squid-users] Whitelist ONLY exception isn't working correctly
In-Reply-To: <3460451526176556@web17o.yandex.ru>
References: <3460451526176556@web17o.yandex.ru>
Message-ID: <c6aa7338-58b2-a803-d9ae-bd66758a2b68@measurement-factory.com>

On 05/12/2018 07:55 PM, Martin Hanson wrote:
> # !!! THIS ISN'T WORKING !!! ubuntu.com, mojang.com still gets blocked on these boxes.
> http_access deny windows_boxes !whitelist
...
> http_access deny all

You have no rules that allow windows_boxes to access whitelist servers,
and you have a "deny all" default. Consider this alternative:

  http_access allow windows_boxes whitelist
  http_access deny windows_boxes
...
  http_access deny all


HTH,

Alex.


From eliezer at ngtech.co.il  Sun May 13 03:19:09 2018
From: eliezer at ngtech.co.il (Eliezer Croitoru)
Date: Sun, 13 May 2018 06:19:09 +0300
Subject: [squid-users] Collecting squid logs to DB
In-Reply-To: <CABMULt++CYhn7+SLKaDi6vuisBZkNXPbSw5LHY8oxu_ApmWOWw@mail.gmail.com>
References: <CABMULtLisrtCimu1xsk+K4EydUypbA2N_1XRbUkO6CtGBubRrQ@mail.gmail.com>
 <0c5901d3ea40$2541eb80$6fc5c280$@ngtech.co.il>
 <CABMULt++CYhn7+SLKaDi6vuisBZkNXPbSw5LHY8oxu_ApmWOWw@mail.gmail.com>
Message-ID: <0ca001d3ea69$288273e0$79875ba0$@ngtech.co.il>

Hey Alex,

 

I wrote a simple service that now run?s here locally with my server.

The sources and installation instruction can be seen here:

http://gogs.ngtech.co.il/elicro/squid-sql-logger

 

Let me know if it works for you.

 

Eliezer

?         I will try to write a daemon in GoLang which might be better for some systems.

 

----

 <http://ngtech.co.il/lmgtfy/> Eliezer Croitoru
Linux System Administrator
Mobile: +972-5-28704261
Email: eliezer at ngtech.co.il



 

From: Alex K <rightkicktech at gmail.com> 
Sent: Sunday, May 13, 2018 01:56
To: Eliezer Croitoru <eliezer at ngtech.co.il>
Cc: squid-users at lists.squid-cache.org
Subject: Re: [squid-users] Collecting squid logs to DB

 

+++ Including list +++

Hi Eliezer, 

I have used the following lines to instruct squid to log at mariadb: 

logfile_daemon /usr/lib/squid/log_db_daemon
access_log daemon:/127.0.0.1/squid_log/access_log/squid/squid <http://127.0.0.1/squid_log/access_log/squid/squid>  squid

Through testing it seems that sometimes squid is not logging anything. I don't know why. After a restart it seems to unblock and write to DB. 

The access_log table is currently InnoDB and I am wondering if MyISAM will behave better. 

 

I would prefer if I could have real time access log. My scenario is that when a user disconnects from squid, an aggregated report of the sites that the user browsed will be available under some web portal where the user has access. Usually there will be up to 20 users connected concurrently so I have to check if this approach is scalable. If this approach is not stable then I might go with log parsing (perhaps logstash or some custom parser) which will parse and generate an aggregated report once per hour or day. 

Is there a way I format the log and pipe to DB only some interesting fields in order to lessen the stress to DB?

 

 

On Sun, May 13, 2018 at 1:25 AM, Eliezer Croitoru <eliezer at ngtech.co.il <mailto:eliezer at ngtech.co.il> > wrote:

Hey Alex,

 

How did you used to log into the DB? What configuration lines have you used?

Also what log format have you used?

Is it important to have realtime data in the DB or a periodic parsing is also an option?

 

Eliezer

 

----

Eliezer Croitoru
Linux System Administrator
Mobile: +972-5-28704261
Email: eliezer at ngtech.co.il <mailto:eliezer at ngtech.co.il> 



 

From: squid-users <squid-users-bounces at lists.squid-cache.org <http://squid-cache.org> > On Behalf Of Alex K
Sent: Saturday, May 5, 2018 01:20
To: squid-users at lists.squid-cache.org <mailto:squid-users at lists.squid-cache.org> 
Subject: [squid-users] Collecting squid logs to DB

 

Hi all, 

I had a previous setup on Debian 7 with squid and I was using mysar to collect squid logs and store them to DB and provide some browsing report at the end of the day. 

Now at Debian 9, trying to upgrade the whole setup, I see that mysar does not compile. 

Checking around I found mysar-ng but this has compilation issues on Debian 9 also. 

Do you suggest any tool that does this job? Does squid support logging to DB natively? (I am using mysql/mariadb)

Some other tool I stumbled on is https://github.com/paranormal/blooper. 

 

Thanx a bunch,

Alex

 

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20180513/3965046b/attachment.htm>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: image002.png
Type: image/png
Size: 11308 bytes
Desc: not available
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20180513/3965046b/attachment.png>

From squid3 at treenet.co.nz  Sun May 13 07:57:18 2018
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sun, 13 May 2018 19:57:18 +1200
Subject: [squid-users] Whitelist ONLY exception isn't working correctly
In-Reply-To: <c6aa7338-58b2-a803-d9ae-bd66758a2b68@measurement-factory.com>
References: <3460451526176556@web17o.yandex.ru>
 <c6aa7338-58b2-a803-d9ae-bd66758a2b68@measurement-factory.com>
Message-ID: <a8fd1c3f-35e4-2949-2453-6b4e2b198db6@treenet.co.nz>

On 13/05/18 14:17, Alex Rousskov wrote:
> On 05/12/2018 07:55 PM, Martin Hanson wrote:
>> # !!! THIS ISN'T WORKING !!! ubuntu.com, mojang.com still gets blocked on these boxes.
>> http_access deny windows_boxes !whitelist
> ...
>> http_access deny all
> 
> You have no rules that allow windows_boxes to access whitelist servers,
> and you have a "deny all" default. Consider this alternative:

I don't think that is the problem. There is "allow localnet" which
includes those windows_boxes.

It is probably !whitelist matching true for the initial CONNECT before
ssl::server_name is available.

Amos


From squid3 at treenet.co.nz  Sun May 13 08:11:57 2018
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sun, 13 May 2018 20:11:57 +1200
Subject: [squid-users] Whitelist ONLY exception isn't working correctly
In-Reply-To: <3460451526176556@web17o.yandex.ru>
References: <3460451526176556@web17o.yandex.ru>
Message-ID: <93eab393-1ff5-5c3e-ae5d-0ad49269fb27@treenet.co.nz>

On 13/05/18 13:55, Martin Hanson wrote:
> 
> This is my current squid.conf. I know I am overlooking something, but I cannot figure out what I am doing wrong.
> 

The comments on this config tell a story of some misunderstandings ...

> <SNIP>
> acl step1 at_step SslBump1
> acl localnet src 192.168.1.0/24
> 
> # These boxes may ONLY access the whitelist.
> acl windows_boxes src 192.168.1.201 192.168.1.202
> 
> acl whitelist ssl::server_name .mojang.com .minecraft.net d2pi0bc9ewx28h.cloudfront.net mcupdate.tumblr.com minecraft-textures-1196058387.us-east-1.elb.amazonaws.com .steampowered.com .steamcommunity.com .steamgames.com .steamusercontent.com .steamcontent.com .steamstatic.com .akamaihd.net .ubuntu.com
> 
> # We don't want the whitelist to be cached.
> cache deny whitelist

"store_miss deny whitelist" is the current way to do this.


> 
> # We want direct access on the whitelist.
> always_direct allow whitelist

"DIRECT" access is always allowed, all this does is prevents cache_peer
being used. You have no cache_peer - so the above line is pointless.

> 
> # Don't redirect to SquidGuard.
> redirector_access deny whitelist

Er. The idea that SquidGuard is redirected *to* is wrong. SG is the
agent _doing_ the redirection.

What the above does is prevent SG having any part of the whitelist
transactions. And redirector_access was deprecated by url_rewrite_access
years ago.

> 
> # We only redirect HTTP and HTTPS.
> acl SSL_ports port 443
> acl Safe_ports port 80          # http
> acl Safe_ports port 443         # https
> acl CONNECT method CONNECT
> http_access deny !Safe_ports
> http_access deny CONNECT !SSL_ports
> 
> # Only allow cachemgr access from localhost
> http_access allow localhost manager
> http_access deny manager
> 
> # !!! THIS ISN'T WORKING !!! ubuntu.com, mojang.com still gets blocked on these boxes.
> http_access deny windows_boxes !whitelist
> 
> http_access allow localhost
> http_access allow localnet
> 
> http_access deny all
> 
> # We'll intercept trafic using PF.
> http_port 127.0.0.1:3129 intercept
> https_port 127.0.0.1:3130 intercept ssl-bump cert=/etc/squid/ssl_cert/myCA.pem generate-host-certificates=on dynamic_cert_mem_cache_size=4MB
> sslcrtd_program /usr/local/libexec/squid/ssl_crtd -s /var/lib/ssl_db -M 4MB
> sslproxy_cafile /usr/local/openssl/cabundle.file
> 
> # Become a TCP tunnel without decrypting proxied traffic for the whitelist.
> ssl_bump splice whitelist
> ssl_bump peek step1
> ssl_bump bump all
> 
> # We want the query strings as well.
> strip_query_terms off
> 
> # Leave coredumps in the first cache dir
> coredump_dir /var/squid/cache
> 
> redirect_program /usr/local/bin/squidGuard -c /etc/squidguard/squidguard.conf


redirect_program is deprecated by url_rewrite_program.

Also, Squidguard is deprecated. Convert basic SG actions into squid.conf
actions where possible. And/or, use ufdbguard as a direct drop-in
replacement that can handle HTTPS and modern Squid helper protocols.

Amos


From squid3 at treenet.co.nz  Sun May 13 08:17:23 2018
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sun, 13 May 2018 20:17:23 +1200
Subject: [squid-users] Collecting squid logs to DB
In-Reply-To: <CABMULt++CYhn7+SLKaDi6vuisBZkNXPbSw5LHY8oxu_ApmWOWw@mail.gmail.com>
References: <CABMULtLisrtCimu1xsk+K4EydUypbA2N_1XRbUkO6CtGBubRrQ@mail.gmail.com>
 <0c5901d3ea40$2541eb80$6fc5c280$@ngtech.co.il>
 <CABMULt++CYhn7+SLKaDi6vuisBZkNXPbSw5LHY8oxu_ApmWOWw@mail.gmail.com>
Message-ID: <73ed0289-4bf6-4260-1f9d-da5ff8347ac2@treenet.co.nz>

On 13/05/18 10:55, Alex K wrote:
> 
> Is there a way I format the log and pipe to DB only some interesting
> fields in order to lessen the stress to DB?
> 

You can use the logformat directive to define a format of your choice
and log that instead of the default Squid format.
 <http://www.squid-cache.org/Doc/config/logformat/>

Amos


From eliezer at ngtech.co.il  Sun May 13 08:37:07 2018
From: eliezer at ngtech.co.il (Eliezer Croitoru)
Date: Sun, 13 May 2018 11:37:07 +0300
Subject: [squid-users] Collecting squid logs to DB
In-Reply-To: <CABMULt++CYhn7+SLKaDi6vuisBZkNXPbSw5LHY8oxu_ApmWOWw@mail.gmail.com>
References: <CABMULtLisrtCimu1xsk+K4EydUypbA2N_1XRbUkO6CtGBubRrQ@mail.gmail.com>
 <0c5901d3ea40$2541eb80$6fc5c280$@ngtech.co.il>
 <CABMULt++CYhn7+SLKaDi6vuisBZkNXPbSw5LHY8oxu_ApmWOWw@mail.gmail.com>
Message-ID: <0ce201d3ea95$942f4970$bc8ddc50$@ngtech.co.il>

To lose the stress on the DB you can use a custom format as Amos suggested but..

I think that when you will define and write what you want to log exactly you will get what you need and want.

 

The general squid access log is pretty lose and I believe that with these days hardware the difference will only be seen on systems with thousands or millions of clients requests.

If this is a small place it?s not required.

 

All The Bests,

Eliezer

 

----

 <http://ngtech.co.il/lmgtfy/> Eliezer Croitoru
Linux System Administrator
Mobile: +972-5-28704261
Email: eliezer at ngtech.co.il



 

From: Alex K <rightkicktech at gmail.com> 
Sent: Sunday, May 13, 2018 01:56
To: Eliezer Croitoru <eliezer at ngtech.co.il>
Cc: squid-users at lists.squid-cache.org
Subject: Re: [squid-users] Collecting squid logs to DB

 

+++ Including list +++

Hi Eliezer, 

I have used the following lines to instruct squid to log at mariadb: 

logfile_daemon /usr/lib/squid/log_db_daemon
access_log daemon:/127.0.0.1/squid_log/access_log/squid/squid <http://127.0.0.1/squid_log/access_log/squid/squid>  squid

Through testing it seems that sometimes squid is not logging anything. I don't know why. After a restart it seems to unblock and write to DB. 

The access_log table is currently InnoDB and I am wondering if MyISAM will behave better. 

 

I would prefer if I could have real time access log. My scenario is that when a user disconnects from squid, an aggregated report of the sites that the user browsed will be available under some web portal where the user has access. Usually there will be up to 20 users connected concurrently so I have to check if this approach is scalable. If this approach is not stable then I might go with log parsing (perhaps logstash or some custom parser) which will parse and generate an aggregated report once per hour or day. 

Is there a way I format the log and pipe to DB only some interesting fields in order to lessen the stress to DB?

 

 

On Sun, May 13, 2018 at 1:25 AM, Eliezer Croitoru <eliezer at ngtech.co.il <mailto:eliezer at ngtech.co.il> > wrote:

Hey Alex,

 

How did you used to log into the DB? What configuration lines have you used?

Also what log format have you used?

Is it important to have realtime data in the DB or a periodic parsing is also an option?

 

Eliezer

 

----

Eliezer Croitoru
Linux System Administrator
Mobile: +972-5-28704261
Email: eliezer at ngtech.co.il <mailto:eliezer at ngtech.co.il> 



 

From: squid-users <squid-users-bounces at lists.squid-cache.org <http://squid-cache.org> > On Behalf Of Alex K
Sent: Saturday, May 5, 2018 01:20
To: squid-users at lists.squid-cache.org <mailto:squid-users at lists.squid-cache.org> 
Subject: [squid-users] Collecting squid logs to DB

 

Hi all, 

I had a previous setup on Debian 7 with squid and I was using mysar to collect squid logs and store them to DB and provide some browsing report at the end of the day. 

Now at Debian 9, trying to upgrade the whole setup, I see that mysar does not compile. 

Checking around I found mysar-ng but this has compilation issues on Debian 9 also. 

Do you suggest any tool that does this job? Does squid support logging to DB natively? (I am using mysql/mariadb)

Some other tool I stumbled on is https://github.com/paranormal/blooper. 

 

Thanx a bunch,

Alex

 

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20180513/e3d9f192/attachment.htm>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: image002.png
Type: image/png
Size: 11307 bytes
Desc: not available
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20180513/e3d9f192/attachment.png>

From rightkicktech at gmail.com  Sun May 13 11:22:31 2018
From: rightkicktech at gmail.com (Alex K)
Date: Sun, 13 May 2018 14:22:31 +0300
Subject: [squid-users] Collecting squid logs to DB
In-Reply-To: <0ce201d3ea95$942f4970$bc8ddc50$@ngtech.co.il>
References: <CABMULtLisrtCimu1xsk+K4EydUypbA2N_1XRbUkO6CtGBubRrQ@mail.gmail.com>
 <0c5901d3ea40$2541eb80$6fc5c280$@ngtech.co.il>
 <CABMULt++CYhn7+SLKaDi6vuisBZkNXPbSw5LHY8oxu_ApmWOWw@mail.gmail.com>
 <0ce201d3ea95$942f4970$bc8ddc50$@ngtech.co.il>
Message-ID: <CABMULtJgGDUk6TOjv1phfYhoT9QTNETt=M59wimHNvRhXiF-pA@mail.gmail.com>

Thanx Eliezer and Amos for the feedback. I just saw the logformat directive
and will experiment with that.
Yes, I have a small group of users (up to 30 - 40 devices) but the hardware
is a relatively small appliance (4G RAM, 4 cores 2GHz, SSD).

Alex


On Sun, May 13, 2018, 11:37 Eliezer Croitoru <eliezer at ngtech.co.il> wrote:

> To lose the stress on the DB you can use a custom format as Amos suggested
> but..
>
> I think that when you will define and write what you want to log exactly
> you will get what you need and want.
>
>
>
> The general squid access log is pretty lose and I believe that with these
> days hardware the difference will only be seen on systems with thousands or
> millions of clients requests.
>
> If this is a small place it?s not required.
>
>
>
> All The Bests,
>
> Eliezer
>
>
>
> ----
>
> Eliezer Croitoru
> Linux System Administrator
> Mobile: +972-5-28704261
> Email: eliezer at ngtech.co.il
>
>
>
> *From:* Alex K <rightkicktech at gmail.com>
> *Sent:* Sunday, May 13, 2018 01:56
> *To:* Eliezer Croitoru <eliezer at ngtech.co.il>
> *Cc:* squid-users at lists.squid-cache.org
> *Subject:* Re: [squid-users] Collecting squid logs to DB
>
>
>
> +++ Including list +++
>
> Hi Eliezer,
>
> I have used the following lines to instruct squid to log at mariadb:
>
> logfile_daemon /usr/lib/squid/log_db_daemon
> access_log daemon:/127.0.0.1/squid_log/access_log/squid/squid squid
>
> Through testing it seems that sometimes squid is not logging anything. I
> don't know why. After a restart it seems to unblock and write to DB.
>
> The access_log table is currently InnoDB and I am wondering if MyISAM will
> behave better.
>
>
>
> I would prefer if I could have real time access log. My scenario is that
> when a user disconnects from squid, an aggregated report of the sites that
> the user browsed will be available under some web portal where the user has
> access. Usually there will be up to 20 users connected concurrently so I
> have to check if this approach is scalable. If this approach is not stable
> then I might go with log parsing (perhaps logstash or some custom parser)
> which will parse and generate an aggregated report once per hour or day.
>
> Is there a way I format the log and pipe to DB only some interesting
> fields in order to lessen the stress to DB?
>
>
>
>
>
> On Sun, May 13, 2018 at 1:25 AM, Eliezer Croitoru <eliezer at ngtech.co.il>
> wrote:
>
> Hey Alex,
>
>
>
> How did you used to log into the DB? What configuration lines have you
> used?
>
> Also what log format have you used?
>
> Is it important to have realtime data in the DB or a periodic parsing is
> also an option?
>
>
>
> Eliezer
>
>
>
> ----
>
> Eliezer Croitoru
> Linux System Administrator
> Mobile: +972-5-28704261
> Email: eliezer at ngtech.co.il
>
>
>
> *From:* squid-users <squid-users-bounces at lists.squid-cache.org> *On
> Behalf Of *Alex K
> *Sent:* Saturday, May 5, 2018 01:20
> *To:* squid-users at lists.squid-cache.org
> *Subject:* [squid-users] Collecting squid logs to DB
>
>
>
> Hi all,
>
> I had a previous setup on Debian 7 with squid and I was using mysar to
> collect squid logs and store them to DB and provide some browsing report at
> the end of the day.
>
> Now at Debian 9, trying to upgrade the whole setup, I see that mysar does
> not compile.
>
> Checking around I found mysar-ng but this has compilation issues on Debian
> 9 also.
>
> Do you suggest any tool that does this job? Does squid support logging to
> DB natively? (I am using mysql/mariadb)
>
> Some other tool I stumbled on is https://github.com/paranormal/blooper.
>
>
>
> Thanx a bunch,
>
> Alex
>
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20180513/c92005f4/attachment.htm>

From frio_cervesa at hotmail.com  Sun May 13 21:47:45 2018
From: frio_cervesa at hotmail.com (senor)
Date: Sun, 13 May 2018 21:47:45 +0000
Subject: [squid-users] restarting dead worker
Message-ID: <BY2PR17MB018264BDE212908A74501D45F79D0@BY2PR17MB0182.namprd17.prod.outlook.com>

Hi All,

I had to change a monitor script to use squidclient ping instead of 
'squid -k check' because the check would succeed even if all workers had 
died except for the coordinator and disker. Obviously, the real problem 
is that the workers are dying but what I'm looking for ATM is a way to 
restart individual workers. It seems a complete restart is necessary 
once the coordinator gives up after a few attempts. I'd like to trigger 
the coordinator to retry or something similar. Anything possible?

Bonus question: Where, if anywhere, is the cache manager use of "kid=N" 
documented? I haven't gone to the source code yet and would like not to.

Thanks in advance,
Senor


From greencoppermine at yandex.com  Sun May 13 23:53:10 2018
From: greencoppermine at yandex.com (Martin Hanson)
Date: Mon, 14 May 2018 01:53:10 +0200
Subject: [squid-users] Whitelist ONLY exception isn't working correctly
In-Reply-To: <c6aa7338-58b2-a803-d9ae-bd66758a2b68@measurement-factory.com>
References: <3460451526176556@web17o.yandex.ru>
 <c6aa7338-58b2-a803-d9ae-bd66758a2b68@measurement-factory.com>
Message-ID: <8830901526255590@web60j.yandex.ru>

I also tried the above, but the result is the same.


From greencoppermine at yandex.com  Mon May 14 00:15:08 2018
From: greencoppermine at yandex.com (Martin Hanson)
Date: Mon, 14 May 2018 02:15:08 +0200
Subject: [squid-users] Whitelist ONLY exception isn't working correctly
In-Reply-To: <93eab393-1ff5-5c3e-ae5d-0ad49269fb27@treenet.co.nz>
References: <3460451526176556@web17o.yandex.ru>
 <93eab393-1ff5-5c3e-ae5d-0ad49269fb27@treenet.co.nz>
Message-ID: <4356061526256908@web19o.yandex.ru>

>> This is my current squid.conf. I know I am overlooking something, but I cannot figure out what I am doing wrong.
> 
> The comments on this config tell a story of some misunderstandings ...

Thank you for all the valuable feedback. I originally set this up years ago. I have updated the different options.

However, I am still faced with the same problem.

If I add a HTTP domain to the whitelist, then the whitelist works for the HTTP domains ONLY, but the domains in the list which are HTTPS based are still blocked.
 
I have also changed the "localnet" in order to avoid any "clashes" between the boxes, but that doesn't change anything.

This is my updated configuration:

Sitting on one of the "windows_boxes" and going to https://www.ubuntu.com/ gives: "Access Denied" from Squid.

<SNIP>
max_filedesc 4096

acl step1 at_step SslBump1
#acl localnet src 192.168.1.0/24
acl localnet src 192.168.1.2-192.168.1.200

# These boxes may ONLY access the whitelist.
acl windows_boxes src 192.168.1.201 192.168.1.202

acl whitelist ssl::server_name .mojang.com .minecraft.net d2pi0bc9ewx28h.cloudfront.net mcupdate.tumblr.com minecraft-textures-1196058387.us-east-1.elb.amazonaws.com .steampowered.com .steamcommunity.com .steamgames.com .steamusercontent.com .steamcontent.com .steamstatic.com .akamaihd.net .launchpad.net .ubuntu.com

# We don't want these to be cached.
store_miss deny whitelist

# Don't let SquidGuard do anything with the whitelisted domains.
url_rewrite_access deny whitelist

# We only redirect HTTP and HTTPS.
acl SSL_ports port 443
acl Safe_ports port 80          # http
acl Safe_ports port 443         # https
acl CONNECT method CONNECT
http_access deny !Safe_ports
http_access deny CONNECT !SSL_ports

# Only allow cachemgr access from localhost
http_access allow localhost manager
http_access deny manager

# THIS ISN'T WORKING!!!
# https://www.ubuntu.com is blocked with "Access Denied" from Squid.
http_access allow windows_boxes whitelist

http_access allow localhost
http_access allow localnet

http_access deny all

# We'll intercept trafic using PF from clan.
http_port 127.0.0.1:3129 intercept
https_port 127.0.0.1:3130 intercept ssl-bump cert=/etc/squid/ssl_cert/myCA.pem generate-host-certificates=on dynamic_cert_mem_cache_size=4MB

sslcrtd_program /usr/local/libexec/squid/ssl_crtd -s /var/lib/ssl_db -M 4MB
sslproxy_cafile /usr/local/openssl/cabundle.file

# Become a TCP tunnel without decrypting proxied traffic for the whitelist.
ssl_bump splice whitelist
ssl_bump peek step1
ssl_bump bump all

# We want the query strings as well.
strip_query_terms off

# Leave coredumps in the first cache dir
coredump_dir /var/squid/cache

redirect_program /usr/local/bin/squidGuard -c /etc/squidguard/squidguard.conf
</SNIP>

Thanks for all the help!

Kind regards.


From rousskov at measurement-factory.com  Mon May 14 00:35:26 2018
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Sun, 13 May 2018 18:35:26 -0600
Subject: [squid-users] Whitelist ONLY exception isn't working correctly
In-Reply-To: <4356061526256908@web19o.yandex.ru>
References: <3460451526176556@web17o.yandex.ru>
 <93eab393-1ff5-5c3e-ae5d-0ad49269fb27@treenet.co.nz>
 <4356061526256908@web19o.yandex.ru>
Message-ID: <a4997dba-82b4-cc62-b475-5d491971016e@measurement-factory.com>

On 05/13/2018 06:15 PM, Martin Hanson wrote:

> # THIS ISN'T WORKING!!!
> # https://www.ubuntu.com is blocked with "Access Denied" from Squid.
> http_access allow windows_boxes whitelist

I suspect the request is blocked during SslBump step1 because there is
not enough information in the fake CONNECT request for ssl::server_name
to match ubuntu.com. Please keep in mind that ssl::server_name does not
do (reverse) DNS lookups, and the fake CONNECT request during step1 only
has an IP address, not a domain name.

One way to test this theory is to (temporary) http_access allow CONNECT
requests to (ubuntu) IP addresses. Does that get you to SslBump step2,
where the fake CONNECT usually gets a domain name?


HTH,

Alex.


From rousskov at measurement-factory.com  Mon May 14 00:46:26 2018
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Sun, 13 May 2018 18:46:26 -0600
Subject: [squid-users] restarting dead worker
In-Reply-To: <BY2PR17MB018264BDE212908A74501D45F79D0@BY2PR17MB0182.namprd17.prod.outlook.com>
References: <BY2PR17MB018264BDE212908A74501D45F79D0@BY2PR17MB0182.namprd17.prod.outlook.com>
Message-ID: <5057c015-367d-36dd-d2d7-2f1be2910af1@measurement-factory.com>

On 05/13/2018 03:47 PM, senor wrote:

> I'm looking for ATM is a way to 
> restart individual workers. It seems a complete restart is necessary 
> once the coordinator gives up after a few attempts. I'd like to trigger 
> the coordinator to retry or something similar. Anything possible?

Yes, in Squid v4:
http://www.squid-cache.org/Doc/config/hopeless_kid_revival_delay/


> Bonus question: Where, if anywhere, is the cache manager use of "kid=N" 
> documented? I haven't gone to the source code yet and would like not to.

That cache manager parameter is not documented because it is not yet
supported. There is a workers=N parameter documented at

https://github.com/squid-cache/squid/commit/80f629d79e830cd60da2efa2d7c7edd93e3fbaa4

I do not know whether it currently works.

Alex.


From squid3 at treenet.co.nz  Mon May 14 00:47:22 2018
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Mon, 14 May 2018 12:47:22 +1200
Subject: [squid-users] restarting dead worker
In-Reply-To: <BY2PR17MB018264BDE212908A74501D45F79D0@BY2PR17MB0182.namprd17.prod.outlook.com>
References: <BY2PR17MB018264BDE212908A74501D45F79D0@BY2PR17MB0182.namprd17.prod.outlook.com>
Message-ID: <491cede9-a8e9-bca6-24f9-53d01357ddda@treenet.co.nz>

On 14/05/18 09:47, senor wrote:
> Hi All,
> 
> I had to change a monitor script to use squidclient ping instead of 
> 'squid -k check' because the check would succeed even if all workers had 
> died except for the coordinator and disker. Obviously, the real problem 
> is that the workers are dying but what I'm looking for ATM is a way to 
> restart individual workers. It seems a complete restart is necessary 
> once the coordinator gives up after a few attempts. I'd like to trigger 
> the coordinator to retry or something similar. Anything possible?

As you said; the coordinator already did retry many times before it gave
up. That is pretty clear evidence that coordinator retrying is not working.


> 
> Bonus question: Where, if anywhere, is the cache manager use of "kid=N" 
> documented? I haven't gone to the source code yet and would like not to.

Cache manager is a REST web API. So parameters like that are passed as
query-string in the URLs. Support for parameters is report-specific and
most of the reports are undocumented. It is mentioned in the SMP pages
somewhere IIRC, but only as a mention that it exists.

In regards to your problem. The cachemgr reports are executed by
workers, not the coordinator. So that API in its current form is highly
unlikely to help with restarting of a single worker. But if you find a
report that pays attention to that parameter it might be useful to see
if any workers are running.
 Ultimately watching the cache.log for worker death and restart messages
will be your best source of info about the deaths. You should even find
the reason the workers are dying in there.

Amos


From greencoppermine at yandex.com  Mon May 14 00:49:19 2018
From: greencoppermine at yandex.com (Martin Hanson)
Date: Mon, 14 May 2018 02:49:19 +0200
Subject: [squid-users] Whitelist ONLY exception isn't working correctly
In-Reply-To: <4356061526256908@web19o.yandex.ru>
References: <3460451526176556@web17o.yandex.ru>
 <93eab393-1ff5-5c3e-ae5d-0ad49269fb27@treenet.co.nz>
 <4356061526256908@web19o.yandex.ru>
Message-ID: <9073591526258959@web3o.yandex.ru>

I have enabled debugging and found something quite strange.

In order to better debug I have limited the whitelist to two domains, one HTTP and one with HTTPS:

acl whitelist ssl::server_name .ubuntu.com .sundkat.dk

When I go to http://www.sundkat.dk, which is a HTTP domain, I get the following:

2018/05/14 02:42:49.859 kid1| 85,2| src/client_side_request.cc(745) clientAccessCheckDone: The request GET http://www.sundkat.dk/ is ALLOWED; last ACL checked: whitelist

But when I go to https://www.ubuntu.com, I get the following:

2018/05/14 02:43:44.262 kid1| 85,2| src/client_side_request.cc(745) clientAccessCheckDone: The request CONNECT 91.189.89.103:443 is DENIED; last ACL checked: all

It's like when the traffic is HTTP the whitelist is working, but when the traffic is HTTPS the whitelist isn't working. But this is ONLY for the "windows_boxes", for everything else it's working as it should.

I don't understand what's going on here.

I am re-posting my entire squid.conf here again just to keep things complete:

<SNIP>
debug_options ALL,2

max_filedesc 4096

acl step1 at_step SslBump1
acl localnet src 192.168.1.2-192.168.1.200

# These boxes may ONLY access the whitelist.
acl windows_boxes src 192.168.1.201 192.168.1.202

acl whitelist ssl::server_name .ubuntu.com .sundkat.dk

# We don't want these to be cached.
store_miss deny whitelist

# Don't let SquidGuard do anything with the whitelisted domains.
url_rewrite_access deny whitelist

# We only redirect HTTP and HTTPS.
acl SSL_ports port 443
acl Safe_ports port 80          # http
acl Safe_ports port 443         # https
acl CONNECT method CONNECT
http_access deny !Safe_ports
http_access deny CONNECT !SSL_ports

# Only allow cachemgr access from localhost
http_access allow localhost manager
http_access deny manager

# THIS ISN'T WORKING!!!
http_access allow windows_boxes whitelist

http_access allow localhost
http_access allow localnet

http_access deny all

# We'll intercept trafic using PF from clan.
http_port 127.0.0.1:3129 intercept
https_port 127.0.0.1:3130 intercept ssl-bump cert=/etc/squid/ssl_cert/myCA.pem generate-host-certificates=on dynamic_cert_mem_cache_size=4MB

sslcrtd_program /usr/local/libexec/squid/ssl_crtd -s /var/lib/ssl_db -M 4MB
sslproxy_cafile /usr/local/openssl/cabundle.file

# Become a TCP tunnel without decrypting proxied traffic for the whitelist.
ssl_bump splice whitelist
ssl_bump peek step1
ssl_bump bump all

# We want the query strings as well.
strip_query_terms off

# Leave coredumps in the first cache dir
coredump_dir /var/squid/cache

redirect_program /usr/local/bin/squidGuard -c /etc/squidguard/squidguard.conf
</SNIP>

Kind regards.


From squid3 at treenet.co.nz  Mon May 14 00:50:50 2018
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Mon, 14 May 2018 12:50:50 +1200
Subject: [squid-users] Whitelist ONLY exception isn't working correctly
In-Reply-To: <a4997dba-82b4-cc62-b475-5d491971016e@measurement-factory.com>
References: <3460451526176556@web17o.yandex.ru>
 <93eab393-1ff5-5c3e-ae5d-0ad49269fb27@treenet.co.nz>
 <4356061526256908@web19o.yandex.ru>
 <a4997dba-82b4-cc62-b475-5d491971016e@measurement-factory.com>
Message-ID: <e468df24-6abb-57f0-4537-299b2f258f83@treenet.co.nz>

On 14/05/18 12:35, Alex Rousskov wrote:
> On 05/13/2018 06:15 PM, Martin Hanson wrote:
> 
>> # THIS ISN'T WORKING!!!
>> # https://www.ubuntu.com is blocked with "Access Denied" from Squid.
>> http_access allow windows_boxes whitelist
> 
> I suspect the request is blocked during SslBump step1 because there is
> not enough information in the fake CONNECT request for ssl::server_name
> to match ubuntu.com. Please keep in mind that ssl::server_name does not
> do (reverse) DNS lookups, and the fake CONNECT request during step1 only
> has an IP address, not a domain name.
> 
> One way to test this theory is to (temporary) http_access allow CONNECT
> requests to (ubuntu) IP addresses. Does that get you to SslBump step2,
> where the fake CONNECT usually gets a domain name?
> 

Alex:
 since you mentioned earlier that the SSL-Bump info based ACLs should
work in following transaction access controls should this work?

 acl step1 at_step SslBump1
 http_access allow CONNECT step1


Amos


From squid3 at treenet.co.nz  Mon May 14 00:51:46 2018
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Mon, 14 May 2018 12:51:46 +1200
Subject: [squid-users] Collecting squid logs to DB
In-Reply-To: <CABMULtJgGDUk6TOjv1phfYhoT9QTNETt=M59wimHNvRhXiF-pA@mail.gmail.com>
References: <CABMULtLisrtCimu1xsk+K4EydUypbA2N_1XRbUkO6CtGBubRrQ@mail.gmail.com>
 <0c5901d3ea40$2541eb80$6fc5c280$@ngtech.co.il>
 <CABMULt++CYhn7+SLKaDi6vuisBZkNXPbSw5LHY8oxu_ApmWOWw@mail.gmail.com>
 <0ce201d3ea95$942f4970$bc8ddc50$@ngtech.co.il>
 <CABMULtJgGDUk6TOjv1phfYhoT9QTNETt=M59wimHNvRhXiF-pA@mail.gmail.com>
Message-ID: <571d457e-90db-0ce7-b637-25bfa107e92e@treenet.co.nz>

On 13/05/18 23:22, Alex K wrote:
> Thanx Eliezer and Amos for the feedback. I just saw the logformat
> directive and will experiment with that.
> Yes, I have a small group of users (up to 30 - 40 devices) but the
> hardware is a relatively small appliance (4G RAM, 4 cores 2GHz, SSD).
> 

That should be more than enough for Squid.

Amos


From greencoppermine at yandex.com  Mon May 14 01:12:53 2018
From: greencoppermine at yandex.com (Martin Hanson)
Date: Mon, 14 May 2018 03:12:53 +0200
Subject: [squid-users] Whitelist ONLY exception isn't working correctly
In-Reply-To: <a4997dba-82b4-cc62-b475-5d491971016e@measurement-factory.com>
References: <3460451526176556@web17o.yandex.ru>
 <93eab393-1ff5-5c3e-ae5d-0ad49269fb27@treenet.co.nz>
 <4356061526256908@web19o.yandex.ru>
 <a4997dba-82b4-cc62-b475-5d491971016e@measurement-factory.com>
Message-ID: <4237101526260373@web18o.yandex.ru>

> On 05/13/2018 06:15 PM, Martin Hanson wrote:
> 
>> # THIS ISN'T WORKING!!!
>> # https://www.ubuntu.com is blocked with "Access Denied" from Squid.
>> http_access allow windows_boxes whitelist
> 
> I suspect the request is blocked during SslBump step1 because there is
> not enough information in the fake CONNECT request for ssl::server_name
> to match ubuntu.com. Please keep in mind that ssl::server_name does not
> do (reverse) DNS lookups, and the fake CONNECT request during step1 only
> has an IP address, not a domain name.
> 
> One way to test this theory is to (temporary) http_access allow CONNECT
> requests to (ubuntu) IP addresses. Does that get you to SslBump step2,
> where the fake CONNECT usually gets a domain name?

Hi Alex,

That makes sense and it seems you're right.

I tried adding (the IP's the box currently sees as ubuntu.com, checked the log that it didn't change):

acl ubuntu dst 91.189.89.103 91.189.89.110
http_access allow CONNECT ubuntu

Then it works!

How do I fix it then?

Kind regards.


From squid3 at treenet.co.nz  Mon May 14 01:24:37 2018
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Mon, 14 May 2018 13:24:37 +1200
Subject: [squid-users] Whitelist ONLY exception isn't working correctly
In-Reply-To: <9073591526258959@web3o.yandex.ru>
References: <3460451526176556@web17o.yandex.ru>
 <93eab393-1ff5-5c3e-ae5d-0ad49269fb27@treenet.co.nz>
 <4356061526256908@web19o.yandex.ru> <9073591526258959@web3o.yandex.ru>
Message-ID: <8b3bc321-0a89-23ba-1677-bd9a7dead83a@treenet.co.nz>

On 14/05/18 12:49, Martin Hanson wrote:
> I have enabled debugging and found something quite strange.
> 
> In order to better debug I have limited the whitelist to two domains, one HTTP and one with HTTPS:
> 
> acl whitelist ssl::server_name .ubuntu.com .sundkat.dk
> 
> When I go to http://www.sundkat.dk, which is a HTTP domain, I get the following:
> 
> 2018/05/14 02:42:49.859 kid1| 85,2| src/client_side_request.cc(745) clientAccessCheckDone: The request GET http://www.sundkat.dk/ is ALLOWED; last ACL checked: whitelist
> 
> But when I go to https://www.ubuntu.com, I get the following:
> 
> 2018/05/14 02:43:44.262 kid1| 85,2| src/client_side_request.cc(745) clientAccessCheckDone: The request CONNECT 91.189.89.103:443 is DENIED; last ACL checked: all
> 
> It's like when the traffic is HTTP the whitelist is working, but when the traffic is HTTPS the whitelist isn't working.

Yes, that is exactly what is happening.


* When intercepting HTTP (port 80) traffic the protocol is HTTP. Squid
is receiving messages generated by the client *naming* the server it
wants to connect with, OR with just a raw-IP if client wants to do it
that way.

* When handling explicit proxy (port 3128) traffic the protocol is HTTP.
Squid is receiving CONNECT messages generated by the client again
*naming* the server it wants to connect with, OR with just a raw-IP if
client wants to do it that way.

* When intercepting HTTPS (port 443) traffic the protocol is initially
just TCP. Squid is receiving TCP SYN packet and fakes/generates a
CONNECT message to represent this opaque connection (ie. CONNECT to a
raw-IP).

If (and only if) a CONNECT is itself allowed into the proxy does
SSL-Bump begin for the TLS wrapped inside that message. That goes for
both types of CONNECT message - Squid or client generated.

It should be obvious from the above why you see different behaviour for
the two methods of using the proxy.



> But this is ONLY for the "windows_boxes", for everything else it's working as it should.
> 
> I don't understand what's going on here.
> 

If the fake CONNECT with raw-IP at SSL-Bump step1 is not allowed to go
through the proxy then the TLS handshake cannot even start to happen. So
there will never be a ssl::server_name for the whitelist ACL to match.

Now that you have altered localnet to exclude the *.201 and *.202 IPs
the "allow localnet" is no longer permitting them to use the proxy.
  AND the whitelist ACL is still not matching the raw-IPs which occur in
CONNECT messages. Which leaves Squid with "deny all".

Amos


From greencoppermine at yandex.com  Mon May 14 01:31:07 2018
From: greencoppermine at yandex.com (Martin Hanson)
Date: Mon, 14 May 2018 03:31:07 +0200
Subject: [squid-users] SOLVED - Whitelist ONLY exception isn't working
	correctly
In-Reply-To: <8b3bc321-0a89-23ba-1677-bd9a7dead83a@treenet.co.nz>
References: <3460451526176556@web17o.yandex.ru>
 <93eab393-1ff5-5c3e-ae5d-0ad49269fb27@treenet.co.nz>
 <4356061526256908@web19o.yandex.ru> <9073591526258959@web3o.yandex.ru>
 <8b3bc321-0a89-23ba-1677-bd9a7dead83a@treenet.co.nz>
Message-ID: <8829361526261467@web57j.yandex.ru>

>> It's like when the traffic is HTTP the whitelist is working, but when the traffic is HTTPS the whitelist isn't working.
> 
> Yes, that is exactly what is happening.
> 
> * When intercepting HTTP (port 80) traffic the protocol is HTTP. Squid
> is receiving messages generated by the client *naming* the server it
> wants to connect with, OR with just a raw-IP if client wants to do it
> that way.
> 
> * When handling explicit proxy (port 3128) traffic the protocol is HTTP.
> Squid is receiving CONNECT messages generated by the client again
> *naming* the server it wants to connect with, OR with just a raw-IP if
> client wants to do it that way.
> 
> * When intercepting HTTPS (port 443) traffic the protocol is initially
> just TCP. Squid is receiving TCP SYN packet and fakes/generates a
> CONNECT message to represent this opaque connection (ie. CONNECT to a
> raw-IP).
> 
> If (and only if) a CONNECT is itself allowed into the proxy does
> SSL-Bump begin for the TLS wrapped inside that message. That goes for
> both types of CONNECT message - Squid or client generated.
> 
> It should be obvious from the above why you see different behaviour for
> the two methods of using the proxy.
> 
>> But this is ONLY for the "windows_boxes", for everything else it's working as it should.
>>
>> I don't understand what's going on here.
> 
> If the fake CONNECT with raw-IP at SSL-Bump step1 is not allowed to go
> through the proxy then the TLS handshake cannot even start to happen. So
> there will never be a ssl::server_name for the whitelist ACL to match.
> 
> Now that you have altered localnet to exclude the *.201 and *.202 IPs
> the "allow localnet" is no longer permitting them to use the proxy.
> AND the whitelist ACL is still not matching the raw-IPs which occur in
> CONNECT messages. Which leaves Squid with "deny all".
> 
> Amos

Thank you very very much Alex and Amos for all the help!

For future references, if anyone needs this, this is the working config:

<SNIP>
acl step1 at_step SslBump1

acl localnet src 192.168.1.0/24

# These boxes may ONLY access the whitelist.
acl windows_boxes src 192.168.1.201 192.168.1.202

acl whitelist ssl::server_name .mojang.com .minecraft.net d2pi0bc9ewx28h.cloudfront.net mcupdate.tumblr.com minecraft-textures-1196058387.us-east-1.elb.amazonaws.com .steampowered.com .steamcommunity.com .steamgames.com .steamusercontent.com .steamcontent.com .steamstatic.com .akamaihd.net .launchpad.net .ubuntu.com

# We don't want these to be cached.
store_miss deny whitelist

# Don't let SquidGuard do anything with the whitelisted domains.
url_rewrite_access deny whitelist

# We only redirect HTTP and HTTPS.
acl SSL_ports port 443
acl Safe_ports port 80          # http
acl Safe_ports port 443         # https
acl CONNECT method CONNECT
http_access deny !Safe_ports
http_access deny CONNECT !SSL_ports

# We need this for the whitelist for the windows boxes because
# requests are blocked during SslBump step1 because there is not
# enough information in the fake CONNECT request for ssl::server_name
# to match domains in the whitelist.
http_access allow CONNECT step1

# Only allow cachemgr access from localhost
http_access allow localhost manager
http_access deny manager

# Windows boxes are only allowed access to the whitelist.
http_access allow windows_boxes whitelist
http_access deny windows_boxes

http_access allow localhost
http_access allow localnet

http_access deny all

http_port 127.0.0.1:3129 intercept
https_port 127.0.0.1:3130 intercept ssl-bump cert=/etc/squid/ssl_cert/myCA.pem generate-host-certificates=on dynamic_cert_mem_cache_size=4MB

sslcrtd_program /usr/local/libexec/squid/ssl_crtd -s /var/lib/ssl_db -M 4MB
sslproxy_cafile /usr/local/openssl/cabundle.file

# Become a TCP tunnel without decrypting proxied traffic for the whitelist.
ssl_bump splice whitelist
ssl_bump peek step1 all
ssl_bump bump all

# We want the query strings as well.
strip_query_terms off

# Leave coredumps in the first cache dir
coredump_dir /var/squid/cache

redirect_program /usr/local/bin/squidGuard -c /etc/squidguard/squidguard.conf
</SNIP>

Kind regards


From greencoppermine at yandex.com  Mon May 14 05:50:45 2018
From: greencoppermine at yandex.com (Martin Hanson)
Date: Mon, 14 May 2018 07:50:45 +0200
Subject: [squid-users] SECURITY ALERT: Host header forgery detected
Message-ID: <7963861526277045@web11o.yandex.ru>

So I finally got the whitelist working, but now every other box on the "localnet", when trying to access the whitelist, gets a:

2018/05/14 07:40:18 kid1| SECURITY ALERT: on URL: www.ubuntu.com:443
2018/05/14 07:40:18 kid1| SECURITY ALERT: Host header forgery detected on local=91.189.89.118:443 remote=192.168.1.4:43354 FD 23 flags=33 (local IP does not match any domain IP)

The config file as before:

<SNIP>
max_filedesc 4096

acl step1 at_step SslBump1

acl localnet src 192.168.1.0/24

# These boxes may ONLY access the whitelist.
acl windows_boxes src 192.168.1.201 192.168.1.202

acl whitelist ssl::server_name .mojang.com .minecraft.net d2pi0bc9ewx28h.cloudfront.net mcupdate.tumblr.com minecraft-textures-1196058387.us-east-1.elb.amazonaws.com .steampowered.com .steamcommunity.com .steamgames.com .steamusercontent.com .steamcontent.com .steamstatic.com .akamaihd.net .launchpad.net .streamlabs.com .ubuntu.com 

# We don't want these to be cached.
store_miss deny whitelist

# Don't let SquidGuard do anything with the whitelisted domains.
url_rewrite_access deny whitelist

# We only redirect HTTP and HTTPS.
acl SSL_ports port 443
acl Safe_ports port 80          # http
acl Safe_ports port 443         # https
acl CONNECT method CONNECT
http_access deny !Safe_ports
http_access deny CONNECT !SSL_ports

# We need this for the whitelist for the windows boxes because
# requests are blocked during SslBump step1 because there is not
# enough information in the fake CONNECT request for ssl::server_name
# to match domains in the whitelist.
http_access allow CONNECT step1

# Only allow cachemgr access from localhost
http_access allow localhost manager
http_access deny manager

# Windows boxes are only allowed access to the whitelist.
http_access allow windows_boxes whitelist
http_access deny windows_boxes

http_access allow localhost
http_access allow localnet

http_access deny all

# We'll intercept trafic using PF from clan.
http_port 127.0.0.1:3129 intercept
https_port 127.0.0.1:3130 intercept ssl-bump cert=/etc/squid/ssl_cert/myCA.pem generate-host-certificates=on dynamic_cert_mem_cache_size=4MB

sslcrtd_program /usr/local/libexec/squid/ssl_crtd -s /var/lib/ssl_db -M 4MB
sslproxy_cafile /usr/local/openssl/cabundle.file

# Become a TCP tunnel without decrypting proxied traffic for the whitelist.
ssl_bump splice whitelist
ssl_bump peek step1 all
ssl_bump bump all

# We want the query strings as well.
strip_query_terms off

# Leave coredumps in the first cache dir
coredump_dir /var/squid/cache

redirect_program /usr/local/bin/squidGuard -c /etc/squidguard/squidguard.conf
</SNIP>

What am I missing now?

Kind regards.



From greencoppermine at yandex.com  Mon May 14 05:59:44 2018
From: greencoppermine at yandex.com (Martin Hanson)
Date: Mon, 14 May 2018 07:59:44 +0200
Subject: [squid-users] SOLVED - SECURITY ALERT: Host header forgery
	detected
In-Reply-To: <7963861526277045@web11o.yandex.ru>
References: <7963861526277045@web11o.yandex.ru>
Message-ID: <4575021526277584@web30j.yandex.ru>

> So I finally got the whitelist working, but now every other box on the "localnet", when trying to access the whitelist, gets a:
> 
> 2018/05/14 07:40:18 kid1| SECURITY ALERT: on URL: www.ubuntu.com:443
> 2018/05/14 07:40:18 kid1| SECURITY ALERT: Host header forgery detected on local=91.189.89.118:443 remote=192.168.1.4:43354 FD 23 flags=33 (local IP does not match any domain IP)

I made a mistake..

".. ensure that the DNS servers Squid uses are the same as those used by the client(s)"

Fixed.

Kind regards.


From Sarfaraz.Ahmad at deshaw.com  Mon May 14 08:59:19 2018
From: Sarfaraz.Ahmad at deshaw.com (Ahmad, Sarfaraz)
Date: Mon, 14 May 2018 08:59:19 +0000
Subject: [squid-users] TCP_TUNNEL_ABORTED/200 with spliced windows updates
Message-ID: <f31dd555eed647e3bd84654e6c97b8bc@mbxtoa3.winmail.deshaw.com>

Hi Folks,

I am using WCCP and redirecting traffic to Squid for both HTTP/HTTPS interception.
In this setup, I have spliced most of the Windows updates's services using SNI in squid's acls. Yet even with TCP tunnel, I am getting failures with these messages in the accesslog.
Why could that response time be so high and is that causing the client to close the connection ? When I take the proxy out of the picture(no redirection through WCCP) the updates run just fine.

1526277713.535 119962 10.240.167.24 TCP_TUNNEL_ABORTED/200 3898 CONNECT sls.update.microsoft.com:443<http://sls.update.microsoft.com:443/> - ORIGINAL_DST/13.78.168.230<http://13.78.168.230/> -
1526277833.538 119735 10.240.167.24 TCP_TUNNEL_ABORTED/200 3898 CONNECT sls.update.microsoft.com:443<http://sls.update.microsoft.com:443/> - ORIGINAL_DST/52.229.171.202<http://52.229.171.202/> -
1526277953.501 119808 10.240.167.24 TCP_TUNNEL_ABORTED/200 3898 CONNECT sls.update.microsoft.com:443<http://sls.update.microsoft.com:443/> - ORIGINAL_DST/52.229.171.202<http://52.229.171.202/> -

Any inputs are welcome.

Regards,
Sarfaraz

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20180514/c4572316/attachment.htm>

From liwang2015 at nudt.edu.cn  Mon May 14 12:29:57 2018
From: liwang2015 at nudt.edu.cn (=?UTF-8?B?6YOm5pe6?=)
Date: Mon, 14 May 2018 20:29:57 +0800 (GMT+08:00)
Subject: [squid-users] About functional testing
Message-ID: <2aa0bdf8.41c2.1635ea0f68e.Coremail.liwang2015@nudt.edu.cn>

To whom it may concern,




I am a student who is interested in software reliability. After read the Squid administrator's guide and FAQ, I only found the tests like ?./test-builds.sh? to test the build before installation.




For the reason that I need to do some experiments, I wonder if there are some official functional tests against an existing Installation of OpenLDAP.




Thanks,

Wang
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20180514/8e64424b/attachment.htm>

From rousskov at measurement-factory.com  Mon May 14 14:23:00 2018
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Mon, 14 May 2018 08:23:00 -0600
Subject: [squid-users] Whitelist ONLY exception isn't working correctly
In-Reply-To: <e468df24-6abb-57f0-4537-299b2f258f83@treenet.co.nz>
References: <3460451526176556@web17o.yandex.ru>
 <93eab393-1ff5-5c3e-ae5d-0ad49269fb27@treenet.co.nz>
 <4356061526256908@web19o.yandex.ru>
 <a4997dba-82b4-cc62-b475-5d491971016e@measurement-factory.com>
 <e468df24-6abb-57f0-4537-299b2f258f83@treenet.co.nz>
Message-ID: <266fd854-b90f-84f7-79f0-12da158134c5@measurement-factory.com>

On 05/13/2018 06:50 PM, Amos Jeffries wrote:
> should this work?
> 
>  acl step1 at_step SslBump1
>  http_access allow CONNECT step1

Yes, step1 should work in http_access IMO, but I do not know whether it
does. According to the latest Amish email, it does work, which is good news!


Cheers,

Alex.


From squid3 at treenet.co.nz  Tue May 15 07:41:27 2018
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Tue, 15 May 2018 19:41:27 +1200
Subject: [squid-users] About functional testing
In-Reply-To: <2aa0bdf8.41c2.1635ea0f68e.Coremail.liwang2015@nudt.edu.cn>
References: <2aa0bdf8.41c2.1635ea0f68e.Coremail.liwang2015@nudt.edu.cn>
Message-ID: <1c95de2e-dacd-4368-e740-9396726e576e@treenet.co.nz>

On 15/05/18 00:29, ?? wrote:
> To whom it may concern,
> 
> 
> I am a student who is interested in software reliability. After read the
> Squid administrator's guide and FAQ,?I only found the tests
> like?*/?/*/*./test-builds.sh**?*/?to test the build before installation.
> 
> 
> For the reason that I need to do some experiments,?I wonder if there are
> some official functional tests against an existing Installation of OpenLDAP.
> 

There are none currently. If you want to contribute any please contact
squid-dev mailing list to discuss the design.

Amos


From squid3 at treenet.co.nz  Tue May 15 07:51:20 2018
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Tue, 15 May 2018 19:51:20 +1200
Subject: [squid-users] TCP_TUNNEL_ABORTED/200 with spliced windows
	updates
In-Reply-To: <f31dd555eed647e3bd84654e6c97b8bc@mbxtoa3.winmail.deshaw.com>
References: <f31dd555eed647e3bd84654e6c97b8bc@mbxtoa3.winmail.deshaw.com>
Message-ID: <edc96dad-c51e-25e2-90e7-b44e2ed85048@treenet.co.nz>

On 14/05/18 20:59, Ahmad, Sarfaraz wrote:
> Hi Folks,
> 
> I am using WCCP and redirecting traffic to Squid for both HTTP/HTTPS
> interception.
> 
> In this setup, I have spliced most of the Windows updates's services
> using SNI in squid's acls. Yet even with TCP tunnel, I am getting
> failures with these messages in the accesslog.?
> 
> Why could that response time be so high and is that causing the client
> to close the connection ? When I take the proxy out of the picture(no
> redirection through WCCP) the updates run just fine.
> 

1) A client may disconnect at any time, for any reason.

2) WCCP is not doing the interception part. It is routing packets to the
Squid box. The intercept should ONLY be done there.

3) Maybe those 3898 bytes that very consistently get delivered to the
client contain a hint.

  and/or, try the intercept part without WCCP if you can. Simplify the
network path and test each part independently to find the point of breakage.


Amos


From Sarfaraz.Ahmad at deshaw.com  Tue May 15 12:27:41 2018
From: Sarfaraz.Ahmad at deshaw.com (Ahmad, Sarfaraz)
Date: Tue, 15 May 2018 12:27:41 +0000
Subject: [squid-users]  TCP_TUNNEL_ABORTED/200 with spliced windowsupdates
Message-ID: <4f568534db59403d8a47a9bddd2379be@mbxtoa3.winmail.deshaw.com>

Thanks Amos.
Turns out it had nothing to do with the proxy but different MTU on the networks. I now have a little better understanding of this amazing piece of software.

Sarfaraz
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20180515/ae0abeb2/attachment.htm>

From Sarfaraz.Ahmad at deshaw.com  Tue May 15 13:15:53 2018
From: Sarfaraz.Ahmad at deshaw.com (Ahmad, Sarfaraz)
Date: Tue, 15 May 2018 13:15:53 +0000
Subject: [squid-users] TCP FIN,ACK after ServerHelloDone with pcmag.com
Message-ID: <baf25fdf7595434e840f333afc8e1dfc@mbxtoa3.winmail.deshaw.com>

Hi Folks,

I am using Squid as a HTTPS interception proxy. When I try to access https://www.pcmag.com , (which is supposed to be bumped in my environment ), I get
"unable to forward request at this time" even though the website is perfectly accessible outside of the proxy.

A packet capture suggests that after Client Hello -> ServerHello -> ServerCertificate,Server Key Exchange, ServerHelloDone, the remote server just sends a FIN,ACK packet, killing off the TCP connection. Nothing else looks out of the ordinary.  ( Without squid, firefox successfully opens the site and the negotiation is TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256, TLS1.2)

The only weird thing that stands out about that website is that the list of SubjectAlternateNames is huge. Could this be a possible bug with Squid ?

My TLS options in Squid.conf :

tls_outgoing_options cafile=/etc/pki/tls/certs/ca-bundle.crt \
    options=NO_SSLv3,SINGLE_DH_USE,SINGLE_ECDH_USE \
    cipher=HIGH:!RC4:!aNULL:!eNULL:!LOW:!3DES:!MD5:!EXP:!PSK:!SRP:!DSS:!EXPORT:!DES:!aECDH:!EDH-DSS-DES-CBC3-SHA:!EDH-RSA-DES-CBC3-SHA:!KRB5-DES-CBC3-SHA

https_port :

https_port 23129 intercept ssl-bump \
    generate-host-certificates=on \
    dynamic_cert_mem_cache_size=4MB \
    cert=/etc/squid/InternetCA/InternetCA.pem \
    key=/etc/squid/InternetCA/InternetCA.key \
    tls-cafile=/etc/squid/InternetCA/InternetCA.chain.pem \
    capath=/etc/pki/tls/certs/certs.d \
    options=NO_SSLv3,SINGLE_DH_USE,SINGLE_ECDH_USE \
    tls-dh=prime256v1:/etc/squid/dhparam.pem

Please advise.

Regards,
Sarfaraz
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20180515/112ad159/attachment.htm>

From marcus.kool at urlfilterdb.com  Tue May 15 13:32:27 2018
From: marcus.kool at urlfilterdb.com (Marcus Kool)
Date: Tue, 15 May 2018 10:32:27 -0300
Subject: [squid-users] TCP FIN,ACK after ServerHelloDone with pcmag.com
In-Reply-To: <baf25fdf7595434e840f333afc8e1dfc@mbxtoa3.winmail.deshaw.com>
References: <baf25fdf7595434e840f333afc8e1dfc@mbxtoa3.winmail.deshaw.com>
Message-ID: <ac103759-8b43-4223-2050-86d049cf4aa5@urlfilterdb.com>

pcmag.com also does not load here, although my config parameters are slightly different.
The certificate is indeed huge...
Do you have
    ERROR: negotiating TLS on FD NNN: error:14090086:SSL routines:ssl3_get_server_certificate:certificate verify failed (1/-1/0)
or other errors in cache.log ?

Marcus

On 15/05/18 10:15, Ahmad, Sarfaraz wrote:
> Hi Folks,
> 
> I am using Squid as a HTTPS interception proxy. When I try to access https://www.pcmag.com , (which is supposed to be bumped in my environment ), I get
> 
> ?unable to forward request at this time? even though the website is perfectly accessible outside of the proxy.
> 
> A packet capture suggests that after Client Hello -> ServerHello -> ServerCertificate,Server Key Exchange, ServerHelloDone, the remote server just sends a FIN,ACK packet, killing off the TCP 
> connection. Nothing else looks out of the ordinary. ?( Without squid, firefox successfully opens the site and the negotiation is TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256, TLS1.2)
> 
> The only weird thing that stands out about that website is that the list of SubjectAlternateNames is huge. Could this be a possible bug with Squid ?
> 
> My TLS options in Squid.conf :
> 
> tls_outgoing_options cafile=/etc/pki/tls/certs/ca-bundle.crt \
> 
>  ??? options=NO_SSLv3,SINGLE_DH_USE,SINGLE_ECDH_USE \
> 
>  ??? cipher=HIGH:!RC4:!aNULL:!eNULL:!LOW:!3DES:!MD5:!EXP:!PSK:!SRP:!DSS:!EXPORT:!DES:!aECDH:!EDH-DSS-DES-CBC3-SHA:!EDH-RSA-DES-CBC3-SHA:!KRB5-DES-CBC3-SHA
> 
> https_port :
> 
> https_port 23129 intercept ssl-bump \
> 
>  ??? generate-host-certificates=on \
> 
>  ??? dynamic_cert_mem_cache_size=4MB \
> 
>  ??? cert=/etc/squid/InternetCA/InternetCA.pem \
> 
>  ??? key=/etc/squid/InternetCA/InternetCA.key \
> 
>  ??? tls-cafile=/etc/squid/InternetCA/InternetCA.chain.pem \
> 
>  ??? capath=/etc/pki/tls/certs/certs.d \
> 
>  ??? options=NO_SSLv3,SINGLE_DH_USE,SINGLE_ECDH_USE \
> 
>  ??? tls-dh=prime256v1:/etc/squid/dhparam.pem
> 
> Please advise.
> 
> Regards,
> 
> Sarfaraz
> 
> 
> 
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
> 


From eliezer at ngtech.co.il  Tue May 15 14:02:08 2018
From: eliezer at ngtech.co.il (Eliezer Croitoru)
Date: Tue, 15 May 2018 17:02:08 +0300
Subject: [squid-users] SOLVED - SECURITY ALERT: Host header
	forgery	detected
In-Reply-To: <4575021526277584@web30j.yandex.ru>
References: <7963861526277045@web11o.yandex.ru>
 <4575021526277584@web30j.yandex.ru>
Message-ID: <!&!AAAAAAAAAAAuAAAAAAAAAGDVRrtF13VDjYYZR9MgvgYBAMO2jhD3dRHOtM0AqgC7tuYAAAAAAA4AABAAAAC+enB6+uZLSqnkfRQoy+89AQAAAAA=@ngtech.co.il>

Hey Martin,

Technically there should be a way to inform Squid-Cache about multiple addresses for the same destination.
If Squid doesn't know that it's a real IP of the domains a partial solution is to use the same DNS service but it can also be something else.
For example there should be a way\option for squid to decide if this address of the client or server is secured.

Amos what do you think?
Can a Host header forgery detection override acl be added? Should it be added?
I believe that  if there are some properties to the remote certificate we can flag the service as "Secure"
IE if the OS runs a "openssl s_client -host www.ubuntnu.com -connect 91.189.89.118:443
 And the certificate is fine then... it's there is no place for any SECURITY ALERT.

I believe that a simple ACL addition which will depend on an external acl helper could be a good option.

Eliezer

----
Eliezer Croitoru
Linux System Administrator
Mobile: +972-5-28704261
Email: eliezer at ngtech.co.il


-----Original Message-----
From: squid-users <squid-users-bounces at lists.squid-cache.org> On Behalf Of Martin Hanson
Sent: Monday, May 14, 2018 09:00
To: squid-users at lists.squid-cache.org
Subject: Re: [squid-users] SOLVED - SECURITY ALERT: Host header forgery detected

> So I finally got the whitelist working, but now every other box on the "localnet", when trying to access the whitelist, gets a:
> 
> 2018/05/14 07:40:18 kid1| SECURITY ALERT: on URL: www.ubuntu.com:443
> 2018/05/14 07:40:18 kid1| SECURITY ALERT: Host header forgery detected on local=91.189.89.118:443 remote=192.168.1.4:43354 FD 23 flags=33 (local IP does not match any domain IP)

I made a mistake..

".. ensure that the DNS servers Squid uses are the same as those used by the client(s)"

Fixed.

Kind regards.
_______________________________________________
squid-users mailing list
squid-users at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-users



From acrow at integrafin.co.uk  Tue May 15 14:27:18 2018
From: acrow at integrafin.co.uk (Alex Crow)
Date: Tue, 15 May 2018 15:27:18 +0100
Subject: [squid-users] Sibling cache with ssl peek/splice/bump?
Message-ID: <df737586-837c-432b-1d70-32bc20ff7b42@integrafin.co.uk>

Hi list,

Is it currently possible in v4 with bumping to have a cache_peer setup 
so that https:// resources can be fetched from a peer if they are 
available there?

Many thanks

Alex

--
This message is intended only for the addressee and may contain
confidential information. Unless you are that person, you may not
disclose its contents or use it in any way and are requested to delete
the message along with any attachments and notify us immediately.
This email is not intended to, nor should it be taken to, constitute advice.
The information provided is correct to our knowledge & belief and must not
be used as a substitute for obtaining tax, regulatory, investment, legal or
any other appropriate advice.

"Transact" is operated by Integrated Financial Arrangements Ltd.
29 Clement's Lane, London EC4N 7AE. Tel: (020) 7608 4900 Fax: (020) 7608 5300.
(Registered office: as above; Registered in England and Wales under
number: 3727592). Authorised and regulated by the Financial Conduct
Authority (entered on the Financial Services Register; no. 190856).


From rousskov at measurement-factory.com  Tue May 15 14:48:34 2018
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Tue, 15 May 2018 08:48:34 -0600
Subject: [squid-users] Sibling cache with ssl peek/splice/bump?
In-Reply-To: <df737586-837c-432b-1d70-32bc20ff7b42@integrafin.co.uk>
References: <df737586-837c-432b-1d70-32bc20ff7b42@integrafin.co.uk>
Message-ID: <0630a996-2fca-5bcd-d77b-e208287f327c@measurement-factory.com>

On 05/15/2018 08:27 AM, Alex Crow wrote:

> Is it currently possible in v4 with bumping to have a cache_peer setup
> so that https:// resources can be fetched from a peer if they are
> available there?


If I am interpreting the "if available" part of your question correctly,
then what you want is unsupported in most SslBump environments because a
bumping Squid does not receive requests for HTTP resources and, hence,
cannot check whether a resource is available somewhere. Squid receives
requests for blind TCP tunnels.

Yes, SslBump converts blind TCP tunnels into HTTP transactions, but in
nearly all practical setups, that conversion happens _after_ the TCP
connection is established and pinned to the requested server. At the TCP
connection establishment time, the HTTP resource (to be requested inside
the tunnel) is still unknown.

FWIW, with an experimental patch, you can route TCP tunnels to peers:
https://github.com/squid-cache/squid/compare/53fdd3f...measurement-factory:7a4c4ed.patch


Squid could disregard connection pinning and request the HTTP resource
by establishing a new HTTPS connection (via a secure cache_peer if
necessary). I have not tested this, but I suspect that Squid does not do
that today: After bumping, you may get local cache hits, but no
HTTP-level peering.


HTH,

Alex.


From eliezer at ngtech.co.il  Tue May 15 17:57:09 2018
From: eliezer at ngtech.co.il (Eliezer Croitoru)
Date: Tue, 15 May 2018 20:57:09 +0300
Subject: [squid-users] Collecting squid logs to DB
In-Reply-To: <CABMULtJgGDUk6TOjv1phfYhoT9QTNETt=M59wimHNvRhXiF-pA@mail.gmail.com>
References: <CABMULtLisrtCimu1xsk+K4EydUypbA2N_1XRbUkO6CtGBubRrQ@mail.gmail.com>
 <0c5901d3ea40$2541eb80$6fc5c280$@ngtech.co.il>
 <CABMULt++CYhn7+SLKaDi6vuisBZkNXPbSw5LHY8oxu_ApmWOWw@mail.gmail.com>
 <0ce201d3ea95$942f4970$bc8ddc50$@ngtech.co.il>
 <CABMULtJgGDUk6TOjv1phfYhoT9QTNETt=M59wimHNvRhXiF-pA@mail.gmail.com>
Message-ID: <070b01d3ec76$24e7e670$6eb7b350$@ngtech.co.il>

I updated the repo: http://gogs.ngtech.co.il/elicro/squid-sql-logger

 

The additions are:

-          GoLang mysql logging service source code.

-          Static pre-compiled binaries for(linux, windows, all bsd, Darwin, linux_arm.. , linux_mips...).

-          Installation instructions for the pre-compiled binaries for linux_amd64 version.

Tested to work under very high load.

There is one improvement I had in mind but I will add it later on.


All The Bests,

Eliezer

 

----

 <http://ngtech.co.il/lmgtfy/> Eliezer Croitoru
Linux System Administrator
Mobile: +972-5-28704261
Email: eliezer at ngtech.co.il



 

From: Alex K <rightkicktech at gmail.com> 
Sent: Sunday, May 13, 2018 14:23
To: Eliezer Croitoru <eliezer at ngtech.co.il>
Cc: squid-users at lists.squid-cache.org
Subject: Re: [squid-users] Collecting squid logs to DB

 

Thanx Eliezer and Amos for the feedback. I just saw the logformat directive and will experiment with that.

Yes, I have a small group of users (up to 30 - 40 devices) but the hardware is a relatively small appliance (4G RAM, 4 cores 2GHz, SSD).

 

Alex

 

On Sun, May 13, 2018, 11:37 Eliezer Croitoru <eliezer at ngtech.co.il <mailto:eliezer at ngtech.co.il> > wrote:

To lose the stress on the DB you can use a custom format as Amos suggested but..

I think that when you will define and write what you want to log exactly you will get what you need and want.

 

The general squid access log is pretty lose and I believe that with these days hardware the difference will only be seen on systems with thousands or millions of clients requests.

If this is a small place it?s not required.

 

All The Bests,

Eliezer

 

----

Eliezer Croitoru
Linux System Administrator
Mobile: +972-5-28704261
Email: eliezer at ngtech.co.il <mailto:eliezer at ngtech.co.il> 



 

From: Alex K <rightkicktech at gmail.com <mailto:rightkicktech at gmail.com> > 
Sent: Sunday, May 13, 2018 01:56
To: Eliezer Croitoru <eliezer at ngtech.co.il <mailto:eliezer at ngtech.co.il> >
Cc: squid-users at lists.squid-cache.org <mailto:squid-users at lists.squid-cache.org> 
Subject: Re: [squid-users] Collecting squid logs to DB

 

+++ Including list +++

Hi Eliezer, 

I have used the following lines to instruct squid to log at mariadb: 

logfile_daemon /usr/lib/squid/log_db_daemon
access_log daemon:/127.0.0.1/squid_log/access_log/squid/squid squid

Through testing it seems that sometimes squid is not logging anything. I don't know why. After a restart it seems to unblock and write to DB. 

The access_log table is currently InnoDB and I am wondering if MyISAM will behave better. 

 

I would prefer if I could have real time access log. My scenario is that when a user disconnects from squid, an aggregated report of the sites that the user browsed will be available under some web portal where the user has access. Usually there will be up to 20 users connected concurrently so I have to check if this approach is scalable. If this approach is not stable then I might go with log parsing (perhaps logstash or some custom parser) which will parse and generate an aggregated report once per hour or day. 

Is there a way I format the log and pipe to DB only some interesting fields in order to lessen the stress to DB?

 

 

On Sun, May 13, 2018 at 1:25 AM, Eliezer Croitoru <eliezer at ngtech.co.il <mailto:eliezer at ngtech.co.il> > wrote:

Hey Alex,

 

How did you used to log into the DB? What configuration lines have you used?

Also what log format have you used?

Is it important to have realtime data in the DB or a periodic parsing is also an option?

 

Eliezer

 

----

Eliezer Croitoru
Linux System Administrator
Mobile: +972-5-28704261
Email: eliezer at ngtech.co.il <mailto:eliezer at ngtech.co.il> 

 

From: squid-users <squid-users-bounces at lists.squid-cache.org <mailto:squid-users-bounces at lists.squid-cache.org> > On Behalf Of Alex K
Sent: Saturday, May 5, 2018 01:20
To: squid-users at lists.squid-cache.org <mailto:squid-users at lists.squid-cache.org> 
Subject: [squid-users] Collecting squid logs to DB

 

Hi all, 

I had a previous setup on Debian 7 with squid and I was using mysar to collect squid logs and store them to DB and provide some browsing report at the end of the day. 

Now at Debian 9, trying to upgrade the whole setup, I see that mysar does not compile. 

Checking around I found mysar-ng but this has compilation issues on Debian 9 also. 

Do you suggest any tool that does this job? Does squid support logging to DB natively? (I am using mysql/mariadb)

Some other tool I stumbled on is https://github.com/paranormal/blooper. 

 

Thanx a bunch,

Alex

 

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20180515/33ec0547/attachment.htm>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: image003.png
Type: image/png
Size: 11308 bytes
Desc: not available
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20180515/33ec0547/attachment.png>

From squid3 at treenet.co.nz  Tue May 15 18:28:17 2018
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Wed, 16 May 2018 06:28:17 +1200
Subject: [squid-users] SOLVED - SECURITY ALERT: Host header forgery
 detected
In-Reply-To: <!&!AAAAAAAAAAAuAAAAAAAAAGDVRrtF13VDjYYZR9MgvgYBAMO2jhD3dRHOtM0AqgC7tuYAAAAAAA4AABAAAAC+enB6+uZLSqnkfRQoy+89AQAAAAA=@ngtech.co.il>
References: <7963861526277045@web11o.yandex.ru>
 <4575021526277584@web30j.yandex.ru>
 <!&!AAAAAAAAAAAuAAAAAAAAAGDVRrtF13VDjYYZR9MgvgYBAMO2jhD3dRHOtM0AqgC7tuYAAAAAAA4AABAAAAC+enB6+uZLSqnkfRQoy+89AQAAAAA=@ngtech.co.il>
Message-ID: <f21de1ba-76d0-705a-b8a7-54dcc6ac208a@treenet.co.nz>

On 16/05/18 02:02, Eliezer Croitoru wrote:
> Hey Martin,
> 
> Technically there should be a way to inform Squid-Cache about multiple addresses for the same destination.
> If Squid doesn't know that it's a real IP of the domains a partial solution is to use the same DNS service but it can also be something else.
> For example there should be a way\option for squid to decide if this address of the client or server is secured.
> 
> Amos what do you think?
> Can a Host header forgery detection override acl be added? Should it be added?
> I believe that  if there are some properties to the remote certificate we can flag the service as "Secure"
> IE if the OS runs a "openssl s_client -host www.ubuntnu.com -connect 91.189.89.118:443
>  And the certificate is fine then... it's there is no place for any SECURITY ALERT.

A malicious actor would simply forward the TLS handshake to the real
server they are spoofing. Same way Squid does for SSL-Bump.

The counter argument of not sending SNI to that suspicious server will
have failures with these exact same mega-corp services. Think
foo.example.com hosted on Google hosting where the generic server cert
is "foo.1e1.net" not "foo.example.com", nor even google.com".


The "problem" that needs to be resolved is simply that the genuine
servers do not have a reliable match between their IP and client
presented domain name(s).

Amos


From squid3 at treenet.co.nz  Tue May 15 18:40:34 2018
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Wed, 16 May 2018 06:40:34 +1200
Subject: [squid-users] TCP FIN,ACK after ServerHelloDone with pcmag.com
In-Reply-To: <ac103759-8b43-4223-2050-86d049cf4aa5@urlfilterdb.com>
References: <baf25fdf7595434e840f333afc8e1dfc@mbxtoa3.winmail.deshaw.com>
 <ac103759-8b43-4223-2050-86d049cf4aa5@urlfilterdb.com>
Message-ID: <3ee9b2cd-757a-48a7-d573-6ac064607130@treenet.co.nz>

On 16/05/18 01:32, Marcus Kool wrote:
> pcmag.com also does not load here, although my config parameters are
> slightly different.
> The certificate is indeed huge...
> Do you have
> ?? ERROR: negotiating TLS on FD NNN: error:14090086:SSL
> routines:ssl3_get_server_certificate:certificate verify failed (1/-1/0)
> or other errors in cache.log ?
> 
> Marcus
> 

Are these Squid-4.0.24 ? There is a regression[1] in the cafile=
parameter handling in the latest release.
 <https://bugs.squid-cache.org/show_bug.cgi?id=4831>

Amos


From marcus.kool at urlfilterdb.com  Tue May 15 20:11:02 2018
From: marcus.kool at urlfilterdb.com (Marcus Kool)
Date: Tue, 15 May 2018 17:11:02 -0300
Subject: [squid-users] TCP FIN,ACK after ServerHelloDone with pcmag.com
In-Reply-To: <3ee9b2cd-757a-48a7-d573-6ac064607130@treenet.co.nz>
References: <baf25fdf7595434e840f333afc8e1dfc@mbxtoa3.winmail.deshaw.com>
 <ac103759-8b43-4223-2050-86d049cf4aa5@urlfilterdb.com>
 <3ee9b2cd-757a-48a7-d573-6ac064607130@treenet.co.nz>
Message-ID: <6fcfa15c-e198-c767-4b16-9e32afa95fd5@urlfilterdb.com>

The proxies that I used for the test have Squid 4.0.22 and Squid 4.0.23.

Marcus


On 15/05/18 15:40, Amos Jeffries wrote:
> On 16/05/18 01:32, Marcus Kool wrote:
>> pcmag.com also does not load here, although my config parameters are
>> slightly different.
>> The certificate is indeed huge...
>> Do you have
>>  ?? ERROR: negotiating TLS on FD NNN: error:14090086:SSL
>> routines:ssl3_get_server_certificate:certificate verify failed (1/-1/0)
>> or other errors in cache.log ?
>>
>> Marcus
>>
> 
> Are these Squid-4.0.24 ? There is a regression[1] in the cafile=
> parameter handling in the latest release.
>   <https://bugs.squid-cache.org/show_bug.cgi?id=4831>
> 
> Amos
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
> 


From Sarfaraz.Ahmad at deshaw.com  Wed May 16 05:06:03 2018
From: Sarfaraz.Ahmad at deshaw.com (Ahmad, Sarfaraz)
Date: Wed, 16 May 2018 05:06:03 +0000
Subject: [squid-users] TCP FIN,ACK after ServerHelloDone with pcmag.com
In-Reply-To: <6fcfa15c-e198-c767-4b16-9e32afa95fd5@urlfilterdb.com>
References: <baf25fdf7595434e840f333afc8e1dfc@mbxtoa3.winmail.deshaw.com>
 <ac103759-8b43-4223-2050-86d049cf4aa5@urlfilterdb.com>
 <3ee9b2cd-757a-48a7-d573-6ac064607130@treenet.co.nz>
 <6fcfa15c-e198-c767-4b16-9e32afa95fd5@urlfilterdb.com>
Message-ID: <505f2ea4a7284d6292e09036fb7a1e8a@mbxtoa3.winmail.deshaw.com>

I see a message similar to Marcus' in cache.log.

2018/05/16 00:20:10 kid1| ERROR: negotiating TLS on FD 77: error:14090086:SSL routines:ssl3_get_server_certificate:certificate verify failed (1/-1/0)

And I am running squid-4.0.24.

Sarfaraz

-----Original Message-----
From: squid-users <squid-users-bounces at lists.squid-cache.org> On Behalf Of Marcus Kool
Sent: Wednesday, May 16, 2018 1:41 AM
To: squid-users at lists.squid-cache.org
Subject: Re: [squid-users] TCP FIN,ACK after ServerHelloDone with pcmag.com

The proxies that I used for the test have Squid 4.0.22 and Squid 4.0.23.

Marcus


On 15/05/18 15:40, Amos Jeffries wrote:
> On 16/05/18 01:32, Marcus Kool wrote:
>> pcmag.com also does not load here, although my config parameters are 
>> slightly different.
>> The certificate is indeed huge...
>> Do you have
>>  ?? ERROR: negotiating TLS on FD NNN: error:14090086:SSL 
>> routines:ssl3_get_server_certificate:certificate verify failed 
>> (1/-1/0) or other errors in cache.log ?
>>
>> Marcus
>>
> 
> Are these Squid-4.0.24 ? There is a regression[1] in the cafile= 
> parameter handling in the latest release.
>   <https://bugs.squid-cache.org/show_bug.cgi?id=4831>
> 
> Amos
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
> 
_______________________________________________
squid-users mailing list
squid-users at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-users

From rightkicktech at gmail.com  Wed May 16 06:17:45 2018
From: rightkicktech at gmail.com (Alex K)
Date: Wed, 16 May 2018 09:17:45 +0300
Subject: [squid-users] Squid configuration sanity check
In-Reply-To: <aa1bf065-8ad3-ced8-fb3e-d74af8406e26@treenet.co.nz>
References: <CABMULt+atE7wPGGyQwL5OZA_PGJFWVOQTASyzvcEBnJkivX-TQ@mail.gmail.com>
 <3a048ca2-72c5-956f-e236-dd459d34e1fb@treenet.co.nz>
 <CABMULtJ6tL4hkTz6oCz+=N=iJOPU0NU37ewF=nA2ttGe58h1Vw@mail.gmail.com>
 <f8badb59-1297-4b08-fa65-5307ae38468a@treenet.co.nz>
 <CABMULtJwZp6pkeK3uwrGZoeuOe4WVG4nC-4_ZQpDvHgsKNiWeQ@mail.gmail.com>
 <CABMULt+iCsGHcfjoD8qtLLONn4evO7rHjvP5spwmJSw6OpZEfw@mail.gmail.com>
 <aa1bf065-8ad3-ced8-fb3e-d74af8406e26@treenet.co.nz>
Message-ID: <CABMULtLh5bENz2eZxK0ALnVLsx8rL7pZj3W_AhLSD=0B2avxGA@mail.gmail.com>

Hi again,

With this config I get:

ERROR: No forward-proxy ports configured.

I am wondering if I could just add a dummy entry:

http_port 3130

to suppress this error.

But not sure how this is useful when reading:

https://wiki.squid-cache.org/KnowledgeBase/NoForwardProxyPorts

Alex

On Tue, May 8, 2018 at 7:49 PM, Amos Jeffries <squid3 at treenet.co.nz> wrote:

> On 08/05/18 22:36, Alex K wrote:
> > Correction:
> >
> > On Tue, May 8, 2018 at 1:35 PM, Alex K wrote:
> >
> >     Hi Amos,
> >
> >     On Tue, May 8, 2018 at 8:55 AM, Amos Jeffries wrote:
> >
> >         On 08/05/18 04:56, Alex K wrote:
> >         > Hi Amos,
> >         >
> >         > On Mon, May 7, 2018 at 7:30 PM, Amos Jeffries wrote:
> >         >
> >         >     On 08/05/18 00:24, Alex K wrote:
> >         >     > Hi all,
> >         >     >
> >         ...
> >         >     > acl localhost src 192.168.200.1/32
> >         >
> >         >     192.168.200.1 is assigned to your lo interface?
> >         >
> >         > Yes, this is the IP of one of the interfaces of the device at
> the
> >         > network where the users use squid to reach Internet.
> >         >
> >
> >         No, I mean specifically the interface named "lo" which has ::1
> and
> >         127.0.0.0/8 assigned by the system. It has
> >         some special security
> >         properties like hardware restriction preventing globally
> >         routable IPs
> >         being used as dst-IP of packets even routed through it result in
> >         rejections.
> >
> >     I have not assigned 192.168.200.1 at lo. It is assigned to an
> >     interface (eth3 for example). localhost is here misleading. it could
> >     say "proxy"
>
> Yes, it should be different. "localhost" ACL is used for some defaults.
> What you are doing here is adding 192.168.200.1 to the ::! etc
> definition of the predefined localhost ACL.
>
>
> >
> >         >
> >         >     >
> >         >     > acl SSL_ports port 443
> >         >     > acl Safe_ports port 80
> >         >     > acl Safe_ports port 21
> >         >     > acl Safe_ports port 443
> >         >     > acl Safe_ports port 10080
> >         >     > acl Safe_ports port 10443
> >         >     > acl SSL method CONNECT
> >         >
> >         >     The above can be quite deceptive,
> >         >
> >         > I removed port 21 as I don't think I am using FTP.
> >         >
> >
> >         Sorry, I missed out the last half of that text. I was meaning
> >         the "SSL"
> >         ACL definition specifically. CONNECT method is not restricted to
> SSL
> >         protocol even when all you are doing is intercepting port 443
> (think
> >         HTTP/2, WebSockets, QUIC, etc). It would be better to use the
> >         provided
> >         CONNECT ACL in place of "SSL" - they are identical in definition
> and
> >         CONNECT is clearer to see if/when some access control is not as
> >         tightly
> >         restricted as "SSL" would make it seem.
> >
> >     You mean remove  "acl SSL method CONNECT" and leave only "acl
> >     CONNECT method CONNECT" ?
> >
>
> Yes. Exactly so.
>
> Amos
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20180516/b3a5c937/attachment.htm>

From squid3 at treenet.co.nz  Wed May 16 12:33:56 2018
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 17 May 2018 00:33:56 +1200
Subject: [squid-users] Squid configuration sanity check
In-Reply-To: <CABMULtLh5bENz2eZxK0ALnVLsx8rL7pZj3W_AhLSD=0B2avxGA@mail.gmail.com>
References: <CABMULt+atE7wPGGyQwL5OZA_PGJFWVOQTASyzvcEBnJkivX-TQ@mail.gmail.com>
 <3a048ca2-72c5-956f-e236-dd459d34e1fb@treenet.co.nz>
 <CABMULtJ6tL4hkTz6oCz+=N=iJOPU0NU37ewF=nA2ttGe58h1Vw@mail.gmail.com>
 <f8badb59-1297-4b08-fa65-5307ae38468a@treenet.co.nz>
 <CABMULtJwZp6pkeK3uwrGZoeuOe4WVG4nC-4_ZQpDvHgsKNiWeQ@mail.gmail.com>
 <CABMULt+iCsGHcfjoD8qtLLONn4evO7rHjvP5spwmJSw6OpZEfw@mail.gmail.com>
 <aa1bf065-8ad3-ced8-fb3e-d74af8406e26@treenet.co.nz>
 <CABMULtLh5bENz2eZxK0ALnVLsx8rL7pZj3W_AhLSD=0B2avxGA@mail.gmail.com>
Message-ID: <e0dab395-d977-0853-39b8-0df90d396a59@treenet.co.nz>

On 16/05/18 18:17, Alex K wrote:
> Hi again,
> 
> With this config I get:
> 
> ERROR: No forward-proxy ports configured.
> 
> I am wondering if I could just add a dummy entry:
> 
> http_port 3130
> 
> to suppress this error.
> 
> But not sure how this is useful when reading:
> 
> https://wiki.squid-cache.org/KnowledgeBase/NoForwardProxyPorts
> 

As the wiki page says Squid generates URLs sometimes which require the
client to contact the proxy directly for something(s). That cannot be
done through a port used for TPROXY or NAT interception traffic.

The port 3130 (if you choose that over the well-known 3128 port) should
not be a "dummy" that does nothing. Squid *will* open and listen for
traffic there. Clients will at times be told to fetch URLs from the
Squid machines public hostname at that port.

You can firewall the port off from all access if you really want to.
Just be aware that will add error messages about the proxy port not
being accessible to whatever problem the client is having that required
direct contact with Squid in the first place (usually trying to display
an error page).

Amos


From rightkicktech at gmail.com  Wed May 16 18:08:20 2018
From: rightkicktech at gmail.com (Alex K)
Date: Wed, 16 May 2018 21:08:20 +0300
Subject: [squid-users] Squid configuration sanity check
In-Reply-To: <e0dab395-d977-0853-39b8-0df90d396a59@treenet.co.nz>
References: <CABMULt+atE7wPGGyQwL5OZA_PGJFWVOQTASyzvcEBnJkivX-TQ@mail.gmail.com>
 <3a048ca2-72c5-956f-e236-dd459d34e1fb@treenet.co.nz>
 <CABMULtJ6tL4hkTz6oCz+=N=iJOPU0NU37ewF=nA2ttGe58h1Vw@mail.gmail.com>
 <f8badb59-1297-4b08-fa65-5307ae38468a@treenet.co.nz>
 <CABMULtJwZp6pkeK3uwrGZoeuOe4WVG4nC-4_ZQpDvHgsKNiWeQ@mail.gmail.com>
 <CABMULt+iCsGHcfjoD8qtLLONn4evO7rHjvP5spwmJSw6OpZEfw@mail.gmail.com>
 <aa1bf065-8ad3-ced8-fb3e-d74af8406e26@treenet.co.nz>
 <CABMULtLh5bENz2eZxK0ALnVLsx8rL7pZj3W_AhLSD=0B2avxGA@mail.gmail.com>
 <e0dab395-d977-0853-39b8-0df90d396a59@treenet.co.nz>
Message-ID: <CABMULt+Nzv+d+x21=ktYzfEkRBk63U0i0+pqk4pi5N0SABaYFQ@mail.gmail.com>

Ok, clear.
Thank you Amos.

Alex

On Wed, May 16, 2018 at 3:33 PM, Amos Jeffries <squid3 at treenet.co.nz> wrote:

> On 16/05/18 18:17, Alex K wrote:
> > Hi again,
> >
> > With this config I get:
> >
> > ERROR: No forward-proxy ports configured.
> >
> > I am wondering if I could just add a dummy entry:
> >
> > http_port 3130
> >
> > to suppress this error.
> >
> > But not sure how this is useful when reading:
> >
> > https://wiki.squid-cache.org/KnowledgeBase/NoForwardProxyPorts
> >
>
> As the wiki page says Squid generates URLs sometimes which require the
> client to contact the proxy directly for something(s). That cannot be
> done through a port used for TPROXY or NAT interception traffic.
>
> The port 3130 (if you choose that over the well-known 3128 port) should
> not be a "dummy" that does nothing. Squid *will* open and listen for
> traffic there. Clients will at times be told to fetch URLs from the
> Squid machines public hostname at that port.
>
> You can firewall the port off from all access if you really want to.
> Just be aware that will add error messages about the proxy port not
> being accessible to whatever problem the client is having that required
> direct contact with Squid in the first place (usually trying to display
> an error page).
>
> Amos
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20180516/f7db5b31/attachment.htm>

From eliezer at ngtech.co.il  Wed May 16 22:05:45 2018
From: eliezer at ngtech.co.il (Eliezer Croitoru)
Date: Thu, 17 May 2018 01:05:45 +0300
Subject: [squid-users] SOLVED - SECURITY ALERT: Host header forgery
	detected
In-Reply-To: <f21de1ba-76d0-705a-b8a7-54dcc6ac208a@treenet.co.nz>
References: <7963861526277045@web11o.yandex.ru>
 <4575021526277584@web30j.yandex.ru>
 <!&!AAAAAAAAAAAuAAAAAAAAAGDVRrtF13VDjYYZR9MgvgYBAMO2jhD3dRHOtM0AqgC7tuYAAAAAAA4AABAAAAC+enB6+uZLSqnkfRQoy+89AQAAAAA=@ngtech.co.il>
 <f21de1ba-76d0-705a-b8a7-54dcc6ac208a@treenet.co.nz>
Message-ID: <0c2d01d3ed62$0a6f98a0$1f4ec9e0$@ngtech.co.il>

Amos,

And this issue is kind of big\mega corp services or CDN services.
Now I am really not sure I understand what this security host forgery is about.
There are couple cases:
- Simple forward proxy with ssl-bump which no header forgery should ever happen when the client requests for a specific domain and no IP
- Intercept proxy  with ssl-bump enabled that has no SNI host
- Intercept proxy with ssl-bump enabled that has SNI and squid passes the clients SNI host

Which one of the above is this specific case?
And if there are other cases it's good to list them and I will try to wiki these details.

Thanks,
Eliezer

----
Eliezer Croitoru
Linux System Administrator
Mobile: +972-5-28704261
Email: eliezer at ngtech.co.il



-----Original Message-----
From: squid-users <squid-users-bounces at lists.squid-cache.org> On Behalf Of Amos Jeffries
Sent: Tuesday, May 15, 2018 21:28
To: squid-users at lists.squid-cache.org
Subject: Re: [squid-users] SOLVED - SECURITY ALERT: Host header forgery detected

On 16/05/18 02:02, Eliezer Croitoru wrote:
> Hey Martin,
> 
> Technically there should be a way to inform Squid-Cache about multiple addresses for the same destination.
> If Squid doesn't know that it's a real IP of the domains a partial solution is to use the same DNS service but it can also be something else.
> For example there should be a way\option for squid to decide if this address of the client or server is secured.
> 
> Amos what do you think?
> Can a Host header forgery detection override acl be added? Should it be added?
> I believe that  if there are some properties to the remote certificate we can flag the service as "Secure"
> IE if the OS runs a "openssl s_client -host www.ubuntnu.com -connect 91.189.89.118:443
>  And the certificate is fine then... it's there is no place for any SECURITY ALERT.

A malicious actor would simply forward the TLS handshake to the real
server they are spoofing. Same way Squid does for SSL-Bump.

The counter argument of not sending SNI to that suspicious server will
have failures with these exact same mega-corp services. Think
foo.example.com hosted on Google hosting where the generic server cert
is "foo.1e1.net" not "foo.example.com", nor even google.com".


The "problem" that needs to be resolved is simply that the genuine
servers do not have a reliable match between their IP and client
presented domain name(s).

Amos
_______________________________________________
squid-users mailing list
squid-users at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-users



From eliezer at ngtech.co.il  Wed May 16 22:08:00 2018
From: eliezer at ngtech.co.il (Eliezer Croitoru)
Date: Thu, 17 May 2018 01:08:00 +0300
Subject: [squid-users] Squid configuration sanity check
In-Reply-To: <CABMULt+Nzv+d+x21=ktYzfEkRBk63U0i0+pqk4pi5N0SABaYFQ@mail.gmail.com>
References: <CABMULt+atE7wPGGyQwL5OZA_PGJFWVOQTASyzvcEBnJkivX-TQ@mail.gmail.com>
 <3a048ca2-72c5-956f-e236-dd459d34e1fb@treenet.co.nz>
 <CABMULtJ6tL4hkTz6oCz+=N=iJOPU0NU37ewF=nA2ttGe58h1Vw@mail.gmail.com>
 <f8badb59-1297-4b08-fa65-5307ae38468a@treenet.co.nz>
 <CABMULtJwZp6pkeK3uwrGZoeuOe4WVG4nC-4_ZQpDvHgsKNiWeQ@mail.gmail.com>
 <CABMULt+iCsGHcfjoD8qtLLONn4evO7rHjvP5spwmJSw6OpZEfw@mail.gmail.com>
 <aa1bf065-8ad3-ced8-fb3e-d74af8406e26@treenet.co.nz>
 <CABMULtLh5bENz2eZxK0ALnVLsx8rL7pZj3W_AhLSD=0B2avxGA@mail.gmail.com>
 <e0dab395-d977-0853-39b8-0df90d396a59@treenet.co.nz>
 <CABMULt+Nzv+d+x21=ktYzfEkRBk63U0i0+pqk4pi5N0SABaYFQ@mail.gmail.com>
Message-ID: <0c2f01d3ed62$5abf22d0$103d6870$@ngtech.co.il>

And..

If there are objects you don?t want to be served from the proxy directly you can try to edit the templates.

 

Eliezer

 

----

 <http://ngtech.co.il/lmgtfy/> Eliezer Croitoru
Linux System Administrator
Mobile: +972-5-28704261
Email: eliezer at ngtech.co.il



 

From: squid-users <squid-users-bounces at lists.squid-cache.org> On Behalf Of Alex K
Sent: Wednesday, May 16, 2018 21:08
To: Amos Jeffries <squid3 at treenet.co.nz>
Cc: squid-users at lists.squid-cache.org
Subject: Re: [squid-users] Squid configuration sanity check

 

Ok, clear.

Thank you Amos. 

Alex

 

On Wed, May 16, 2018 at 3:33 PM, Amos Jeffries <squid3 at treenet.co.nz <mailto:squid3 at treenet.co.nz> > wrote:

On 16/05/18 18:17, Alex K wrote:
> Hi again,
> 
> With this config I get:
> 
> ERROR: No forward-proxy ports configured.
> 
> I am wondering if I could just add a dummy entry:
> 
> http_port 3130
> 
> to suppress this error.
> 
> But not sure how this is useful when reading:
> 
> https://wiki.squid-cache.org/KnowledgeBase/NoForwardProxyPorts
> 

As the wiki page says Squid generates URLs sometimes which require the
client to contact the proxy directly for something(s). That cannot be
done through a port used for TPROXY or NAT interception traffic.

The port 3130 (if you choose that over the well-known 3128 port) should
not be a "dummy" that does nothing. Squid *will* open and listen for
traffic there. Clients will at times be told to fetch URLs from the
Squid machines public hostname at that port.

You can firewall the port off from all access if you really want to.
Just be aware that will add error messages about the proxy port not
being accessible to whatever problem the client is having that required
direct contact with Squid in the first place (usually trying to display
an error page).

Amos

 

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20180517/1a9e09ba/attachment.htm>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: image002.png
Type: image/png
Size: 11297 bytes
Desc: not available
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20180517/1a9e09ba/attachment.png>

From justinandrosey at gmail.com  Thu May 17 04:15:26 2018
From: justinandrosey at gmail.com (Justin & Roseanne James)
Date: Thu, 17 May 2018 00:15:26 -0400
Subject: [squid-users] Very High Response Times to Certain Websites with
	Squid
Message-ID: <CACS1Gi9f26NJ9DVQXHte4JYZCoG2hMhUM1QCzmi4jmq4bCagAw@mail.gmail.com>

 Hello,
I have a proxy server setup with Squid 3.5.23, and am having issues with
high response times to a lot of websites. As you can see below, I'm seeing
response times of over 1,000,000 milliseconds!

1526528717.346 1032187 10.10.10.5 TAG_NONE/200 0 CONNECT 54.239.29.128:443 -
HIER_NONE/- -
1526528717.346 1032158 10.10.10.5 TAG_NONE/409 0 CONNECT api.amazon.com:443 -
HIER_NONE/- text/html;charset=utf-8
1526528717.346 1000764 10.10.10.5 TAG_NONE/200 0 CONNECT 23.34.22.196:443 -
HIER_NONE/- -
1526528717.346 1000734 10.10.10.5 TAG_NONE/409 0 CONNECT www.amazon.com:443 -
HIER_NONE/- text/html;charset=utf-8
1526528717.346 1032128 10.10.10.5 TAG_NONE/200 0 CONNECT 72.21.206.140:443 -
HIER_NONE/- -
1526528717.346 1032124 10.10.10.5 TAG_NONE/409 0 CONNECT
s.amazon-adsystem.com:443 - HIER_NONE/- text/html;charset=utf-8
1526528717.346 1000903 10.10.10.5 TAG_NONE/200 0 CONNECT 52.94.237.193:443 -
HIER_NONE/- -
1526528717.346 1000899 10.10.10.5 TAG_NONE/409 0 CONNECT
transient.amazon.com:443 - HIER_NONE/- text/html;charset=utf-8
1526528717.346 1031571 10.10.10.5 TAG_NONE/200 0 CONNECT 72.21.206.141:443 -
HIER_NONE/- -
1526528717.346 1031564 10.10.10.5 TAG_NONE/409 0 CONNECT
aax-us-east.amazon-adsystem.com:443 - HIER_NONE/- text/html;charset=utf-8
1526528717.346 1000902 10.10.10.5 TAG_NONE/200 0 CONNECT 23.21.218.68:443 -
HIER_NONE/- -
1526528717.346 1000897 10.10.10.5 TAG_NONE/409 0 CONNECT
settings.crashlytics.com:443 - HIER_NONE/- text/html;charset=utf-8
1526528717.346 803516 10.10.10.2 TAG_NONE/200 0 CONNECT 54.88.34.154:443 -
HIER_NONE/- -
1526528717.346 803506 10.10.10.2 TAG_NONE/409 0 CONNECT
fls-na.amazon.com:443 - HIER_NONE/- text/html;charset=utf-8
1526528717.346 803516 10.10.10.2 TAG_NONE/200 0 CONNECT 54.88.34.154:443 -
HIER_NONE/- -
1526528717.346 803506 10.10.10.2 TAG_NONE/409 0 CONNECT
fls-na.amazon.com:443 - HIER_NONE/- text/html;charset=utf-8
1526528717.346 1000765 10.10.10.5 TAG_NONE/200 0 CONNECT 52.94.236.223:443 -
HIER_NONE/- -
1526528717.346 1000734 10.10.10.5 TAG_NONE/409 0 CONNECT msh.amazon.com:443 -
HIER_NONE/- text/html;charset=utf-8

I'm not doing anything special. Squid is running transparently and I have
iptables rules setup to forward port 80 and 443 traffic appropriately from
my firewall to my squid box. I can curl the above sites on my server and
results are returned almost instantly (not going through squid).

Many thanks for any help you can provide!
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20180517/8dbf7bba/attachment.htm>

From m_zouhairy at skno.by  Thu May 17 05:45:08 2018
From: m_zouhairy at skno.by (Vacheslav)
Date: Thu, 17 May 2018 08:45:08 +0300
Subject: [squid-users] kaspersky and ufdbguard
Message-ID: <001201d3eda2$36fb4c80$a4f1e580$@skno.by>

Peace,

When I configured Kaspersky to use proxy, I started getting as an example:

BLOCK -                10.96.0.104     config     https-option  195.122.177.165:443 CONNECT

I have require https hostname. Kaspersky is updating fine.

Anyone has an idea what Kaspersky is connecting ?

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20180517/b1f0fe00/attachment.htm>

From squid3 at treenet.co.nz  Thu May 17 10:45:49 2018
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 17 May 2018 22:45:49 +1200
Subject: [squid-users] Very High Response Times to Certain Websites with
 Squid
In-Reply-To: <CACS1Gi9f26NJ9DVQXHte4JYZCoG2hMhUM1QCzmi4jmq4bCagAw@mail.gmail.com>
References: <CACS1Gi9f26NJ9DVQXHte4JYZCoG2hMhUM1QCzmi4jmq4bCagAw@mail.gmail.com>
Message-ID: <c2098081-c2de-d93c-38b4-28ce8393155e@treenet.co.nz>

On 17/05/18 16:15, Justin & Roseanne James wrote:
> 
> I'm not doing anything special. Squid is running transparently and I
> have iptables rules setup to forward port 80 and 443 traffic
> appropriately from my firewall to my squid box.

Traffic must be *routed* between machines. The NAT step must be done
only on the Squid machine itself.

Amos


From Sarfaraz.Ahmad at deshaw.com  Thu May 17 10:47:36 2018
From: Sarfaraz.Ahmad at deshaw.com (Ahmad, Sarfaraz)
Date: Thu, 17 May 2018 10:47:36 +0000
Subject: [squid-users] TCP FIN,ACK after ServerHelloDone with pcmag.com
References: <baf25fdf7595434e840f333afc8e1dfc@mbxtoa3.winmail.deshaw.com>
 <ac103759-8b43-4223-2050-86d049cf4aa5@urlfilterdb.com>
 <3ee9b2cd-757a-48a7-d573-6ac064607130@treenet.co.nz>
 <6fcfa15c-e198-c767-4b16-9e32afa95fd5@urlfilterdb.com> 
Message-ID: <e7847af93b42400f8afb9e2f4f4f6187@mbxtoa3.winmail.deshaw.com>

Guys,

Any thoughts ?

Regards,
Sarfaraz

-----Original Message-----
From: Ahmad, Sarfaraz 
Sent: Wednesday, May 16, 2018 10:36 AM
To: 'Marcus Kool' <marcus.kool at urlfilterdb.com>; squid-users at lists.squid-cache.org
Subject: RE: [squid-users] TCP FIN,ACK after ServerHelloDone with pcmag.com

I see a message similar to Marcus' in cache.log.

2018/05/16 00:20:10 kid1| ERROR: negotiating TLS on FD 77: error:14090086:SSL routines:ssl3_get_server_certificate:certificate verify failed (1/-1/0)

And I am running squid-4.0.24.

Sarfaraz

-----Original Message-----
From: squid-users <squid-users-bounces at lists.squid-cache.org> On Behalf Of Marcus Kool
Sent: Wednesday, May 16, 2018 1:41 AM
To: squid-users at lists.squid-cache.org
Subject: Re: [squid-users] TCP FIN,ACK after ServerHelloDone with pcmag.com

The proxies that I used for the test have Squid 4.0.22 and Squid 4.0.23.

Marcus


On 15/05/18 15:40, Amos Jeffries wrote:
> On 16/05/18 01:32, Marcus Kool wrote:
>> pcmag.com also does not load here, although my config parameters are 
>> slightly different.
>> The certificate is indeed huge...
>> Do you have
>>  ?? ERROR: negotiating TLS on FD NNN: error:14090086:SSL 
>> routines:ssl3_get_server_certificate:certificate verify failed 
>> (1/-1/0) or other errors in cache.log ?
>>
>> Marcus
>>
> 
> Are these Squid-4.0.24 ? There is a regression[1] in the cafile= 
> parameter handling in the latest release.
>   <https://bugs.squid-cache.org/show_bug.cgi?id=4831>
> 
> Amos
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
> 
_______________________________________________
squid-users mailing list
squid-users at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-users

From squid3 at treenet.co.nz  Thu May 17 10:55:36 2018
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 17 May 2018 22:55:36 +1200
Subject: [squid-users] kaspersky and ufdbguard
In-Reply-To: <001201d3eda2$36fb4c80$a4f1e580$@skno.by>
References: <001201d3eda2$36fb4c80$a4f1e580$@skno.by>
Message-ID: <5758e0ea-496d-7b7e-0fbd-a4c3e955d996@treenet.co.nz>

On 17/05/18 17:45, Vacheslav wrote:
> Peace,
> 
> When I configured Kaspersky to use proxy, I started getting as an example:
> 
> BLOCK -??????????????? 10.96.0.104???? config???? https-option?
> 195.122.177.165:443 CONNECT
> 
> I have require https hostname. Kaspersky is updating fine.
> 
> Anyone has an idea what Kaspersky is connecting ?
> 

That is a custom log format, you have not provided any info about what
each field is. So no, we don't have much of a clue what it means.

Amos


From m_zouhairy at skno.by  Thu May 17 11:03:23 2018
From: m_zouhairy at skno.by (Vacheslav)
Date: Thu, 17 May 2018 14:03:23 +0300
Subject: [squid-users] kaspersky and ufdbguard
In-Reply-To: <5758e0ea-496d-7b7e-0fbd-a4c3e955d996@treenet.co.nz>
References: <001201d3eda2$36fb4c80$a4f1e580$@skno.by>
 <5758e0ea-496d-7b7e-0fbd-a4c3e955d996@treenet.co.nz>
Message-ID: <008f01d3edce$ac29c1e0$047d45a0$@skno.by>

I have this: 
acl {
   allSystems  {
      ### EDIT THE NEXT LINE FOR LOCAL CONFIGURATION:
      pass 
	   alwaysallow
	   # !always-block
	    !ms-data-collection
	   !adult !security
	    !proxies !malware !warez
	   !gambling !violence !drugs 
      	  !phishtank !spyware
	   chat dating !games religion  finance jobs shops sports travel news
	   webmail forum socialnet youtube
           !webtv webradio audiovideo
	   !ads
           searchengine
	   # with "logall on" or "logpass on" it makes sense to have the category "checked" in the ACL.
	   any
	   # NOTE: ALL categories are part of the ACL for logging purposes.
	   # Only when logall is off, one can remove the allowed categories from the ACL.
   }

I don't have a similar config acl.

-----Original Message-----
From: squid-users <squid-users-bounces at lists.squid-cache.org> On Behalf Of Amos Jeffries
Sent: Thursday, May 17, 2018 1:56 PM
To: squid-users at lists.squid-cache.org
Subject: Re: [squid-users] kaspersky and ufdbguard

On 17/05/18 17:45, Vacheslav wrote:
> Peace,
> 
> When I configured Kaspersky to use proxy, I started getting as an example:
> 
> BLOCK -                10.96.0.104     config     https-option
> 195.122.177.165:443 CONNECT
> 
> I have require https hostname. Kaspersky is updating fine.
> 
> Anyone has an idea what Kaspersky is connecting ?
> 

That is a custom log format, you have not provided any info about what each field is. So no, we don't have much of a clue what it means.

Amos
_______________________________________________
squid-users mailing list
squid-users at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-users




From marcus.kool at urlfilterdb.com  Thu May 17 12:21:57 2018
From: marcus.kool at urlfilterdb.com (Marcus Kool)
Date: Thu, 17 May 2018 09:21:57 -0300
Subject: [squid-users] kaspersky and ufdbguard
In-Reply-To: <008f01d3edce$ac29c1e0$047d45a0$@skno.by>
References: <001201d3eda2$36fb4c80$a4f1e580$@skno.by>
 <5758e0ea-496d-7b7e-0fbd-a4c3e955d996@treenet.co.nz>
 <008f01d3edce$ac29c1e0$047d45a0$@skno.by>
Message-ID: <ef611a45-8321-6912-8fbb-21137458a9c8@urlfilterdb.com>

195.122.177.165 is an IP address of Kaspersky (see whois 195.122.177.165).
ufdbguardd blocks this IP address since it is configured to do so which is indicated by 'https-option', most likely because the config has
    option enforce-https-with-hostname on # default is off.

Marcus


On 17/05/18 08:03, Vacheslav wrote:
> I have this:
> acl {
>     allSystems  {
>        ### EDIT THE NEXT LINE FOR LOCAL CONFIGURATION:
>        pass
> 	   alwaysallow
> 	   # !always-block
> 	    !ms-data-collection
> 	   !adult !security
> 	    !proxies !malware !warez
> 	   !gambling !violence !drugs
>        	  !phishtank !spyware
> 	   chat dating !games religion  finance jobs shops sports travel news
> 	   webmail forum socialnet youtube
>             !webtv webradio audiovideo
> 	   !ads
>             searchengine
> 	   # with "logall on" or "logpass on" it makes sense to have the category "checked" in the ACL.
> 	   any
> 	   # NOTE: ALL categories are part of the ACL for logging purposes.
> 	   # Only when logall is off, one can remove the allowed categories from the ACL.
>     }
> 
> I don't have a similar config acl.
> 
> -----Original Message-----
> From: squid-users <squid-users-bounces at lists.squid-cache.org> On Behalf Of Amos Jeffries
> Sent: Thursday, May 17, 2018 1:56 PM
> To: squid-users at lists.squid-cache.org
> Subject: Re: [squid-users] kaspersky and ufdbguard
> 
> On 17/05/18 17:45, Vacheslav wrote:
>> Peace,
>>
>> When I configured Kaspersky to use proxy, I started getting as an example:
>>
>> BLOCK -                10.96.0.104     config     https-option
>> 195.122.177.165:443 CONNECT
>>
>> I have require https hostname. Kaspersky is updating fine.
>>
>> Anyone has an idea what Kaspersky is connecting ?
>>
> 
> That is a custom log format, you have not provided any info about what each field is. So no, we don't have much of a clue what it means.
> 
> Amos
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
> 
> 
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
> 


From ldclakmal at gmail.com  Thu May 17 12:23:42 2018
From: ldclakmal at gmail.com (Chanaka Lakmal)
Date: Thu, 17 May 2018 17:53:42 +0530
Subject: [squid-users] Squid with HTTP/2
Message-ID: <CAH-FGDh9LMZ_y1BG8ePxxtpmxSfEZDfh4OMFX_Ke-yNUhojMkg@mail.gmail.com>

Hi,

Does Squid supports HTTP/2 protocol? If so, what is the version it supports?

Regards,
Chanaka Lakmal
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20180517/91be21e4/attachment.htm>

From m_zouhairy at skno.by  Thu May 17 12:30:29 2018
From: m_zouhairy at skno.by (Vacheslav)
Date: Thu, 17 May 2018 15:30:29 +0300
Subject: [squid-users] kaspersky and ufdbguard
In-Reply-To: <ef611a45-8321-6912-8fbb-21137458a9c8@urlfilterdb.com>
References: <001201d3eda2$36fb4c80$a4f1e580$@skno.by>
 <5758e0ea-496d-7b7e-0fbd-a4c3e955d996@treenet.co.nz>
 <008f01d3edce$ac29c1e0$047d45a0$@skno.by>
 <ef611a45-8321-6912-8fbb-21137458a9c8@urlfilterdb.com>
Message-ID: <00a601d3edda$d77d4180$8677c480$@skno.by>

Yeah all that I know, The million dollar question is should I continue blocking it?

-----Original Message-----
From: squid-users <squid-users-bounces at lists.squid-cache.org> On Behalf Of Marcus Kool
Sent: Thursday, May 17, 2018 3:22 PM
To: squid-users at lists.squid-cache.org
Subject: Re: [squid-users] kaspersky and ufdbguard

195.122.177.165 is an IP address of Kaspersky (see whois 195.122.177.165).
ufdbguardd blocks this IP address since it is configured to do so which is indicated by 'https-option', most likely because the config has
    option enforce-https-with-hostname on # default is off.

Marcus


On 17/05/18 08:03, Vacheslav wrote:
> I have this:
> acl {
>     allSystems  {
>        ### EDIT THE NEXT LINE FOR LOCAL CONFIGURATION:
>        pass
> 	   alwaysallow
> 	   # !always-block
> 	    !ms-data-collection
> 	   !adult !security
> 	    !proxies !malware !warez
> 	   !gambling !violence !drugs
>        	  !phishtank !spyware
> 	   chat dating !games religion  finance jobs shops sports travel news
> 	   webmail forum socialnet youtube
>             !webtv webradio audiovideo
> 	   !ads
>             searchengine
> 	   # with "logall on" or "logpass on" it makes sense to have the category "checked" in the ACL.
> 	   any
> 	   # NOTE: ALL categories are part of the ACL for logging purposes.
> 	   # Only when logall is off, one can remove the allowed categories from the ACL.
>     }
> 
> I don't have a similar config acl.
> 
> -----Original Message-----
> From: squid-users <squid-users-bounces at lists.squid-cache.org> On Behalf Of Amos Jeffries
> Sent: Thursday, May 17, 2018 1:56 PM
> To: squid-users at lists.squid-cache.org
> Subject: Re: [squid-users] kaspersky and ufdbguard
> 
> On 17/05/18 17:45, Vacheslav wrote:
>> Peace,
>>
>> When I configured Kaspersky to use proxy, I started getting as an example:
>>
>> BLOCK -                10.96.0.104     config     https-option
>> 195.122.177.165:443 CONNECT
>>
>> I have require https hostname. Kaspersky is updating fine.
>>
>> Anyone has an idea what Kaspersky is connecting ?
>>
> 
> That is a custom log format, you have not provided any info about what each field is. So no, we don't have much of a clue what it means.
> 
> Amos
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
> 
> 
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
> 
_______________________________________________
squid-users mailing list
squid-users at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-users




From squid3 at treenet.co.nz  Thu May 17 13:10:18 2018
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Fri, 18 May 2018 01:10:18 +1200
Subject: [squid-users] Squid with HTTP/2
In-Reply-To: <CAH-FGDh9LMZ_y1BG8ePxxtpmxSfEZDfh4OMFX_Ke-yNUhojMkg@mail.gmail.com>
References: <CAH-FGDh9LMZ_y1BG8ePxxtpmxSfEZDfh4OMFX_Ke-yNUhojMkg@mail.gmail.com>
Message-ID: <1fa2742c-ad46-c034-861a-50f059bb1fd4@treenet.co.nz>

On 18/05/18 00:23, Chanaka Lakmal wrote:
> Hi,
> 
> Does Squid supports HTTP/2 protocol? If so, what is the version it supports?
> 

No and "sort of". Squid does not yet support it natively. Squid does
support h2 tunneled inside TLS (except when SSL-Bumping) in the same way
HTTPS has always been supported.

Amos


From akismpa at gmail.com  Thu May 17 16:32:16 2018
From: akismpa at gmail.com (Panagiotis Bariamis)
Date: Thu, 17 May 2018 19:32:16 +0300
Subject: [squid-users] Auth bearer support for forward proxy
Message-ID: <CAPxN_PVf5Fz851SE9j3=PC0Hz68PfJ5hGY7OhhvPJ0bC0YmsLg@mail.gmail.com>

Hello ,
Only thing I have found concerning the subject is this 4 years old thread :

>From: Amos Jeffries <squid3 at treenet.co.nz>
>Date: Sun, 08 Jun 2014 14:46:09 +1200
>Message-ID: <5393CE71.5070609 at treenet.co.nz>
>To: "ietf-http-wg at w3.org" <ietf-http-wg at w3.org>

>I have implemented Bearer authentication support in Squid and have found
>a noticible lack of support in clients for the 407 status Proxy-Auth*
>headers, even where Bearer support is advertized and working via 401
>status WWW-Auth*.

>Can any of the client implementers (Browsers in particular) please point
>out if they do support Bearer login to a proxy and what restrictions
>they have on it actually working?
>
>Amos Jeffries
>The Squid Software Foundation

Does  anyone know if browsers have implemented since then support for
bearer authentication for Proxy-Auth ?
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20180517/eea1db53/attachment.htm>

From marcus.kool at urlfilterdb.com  Thu May 17 18:11:25 2018
From: marcus.kool at urlfilterdb.com (Marcus Kool)
Date: Thu, 17 May 2018 15:11:25 -0300
Subject: [squid-users] kaspersky and ufdbguard
In-Reply-To: <00a601d3edda$d77d4180$8677c480$@skno.by>
References: <001201d3eda2$36fb4c80$a4f1e580$@skno.by>
 <5758e0ea-496d-7b7e-0fbd-a4c3e955d996@treenet.co.nz>
 <008f01d3edce$ac29c1e0$047d45a0$@skno.by>
 <ef611a45-8321-6912-8fbb-21137458a9c8@urlfilterdb.com>
 <00a601d3edda$d77d4180$8677c480$@skno.by>
Message-ID: <7258912d-1d03-82d7-a6ff-cfdef8479fb4@urlfilterdb.com>

I do not block my Kaspersky AV.
Do you want the Kaspersky software contact the servers of Kaspersky ?

On 17/05/18 09:30, Vacheslav wrote:
> Yeah all that I know, The million dollar question is should I continue blocking it?
> 
> -----Original Message-----
> From: squid-users <squid-users-bounces at lists.squid-cache.org> On Behalf Of Marcus Kool
> Sent: Thursday, May 17, 2018 3:22 PM
> To: squid-users at lists.squid-cache.org
> Subject: Re: [squid-users] kaspersky and ufdbguard
> 
> 195.122.177.165 is an IP address of Kaspersky (see whois 195.122.177.165).
> ufdbguardd blocks this IP address since it is configured to do so which is indicated by 'https-option', most likely because the config has
>      option enforce-https-with-hostname on # default is off.
> 
> Marcus
> 
> 
> On 17/05/18 08:03, Vacheslav wrote:
>> I have this:
>> acl {
>>      allSystems  {
>>         ### EDIT THE NEXT LINE FOR LOCAL CONFIGURATION:
>>         pass
>> 	   alwaysallow
>> 	   # !always-block
>> 	    !ms-data-collection
>> 	   !adult !security
>> 	    !proxies !malware !warez
>> 	   !gambling !violence !drugs
>>         	  !phishtank !spyware
>> 	   chat dating !games religion  finance jobs shops sports travel news
>> 	   webmail forum socialnet youtube
>>              !webtv webradio audiovideo
>> 	   !ads
>>              searchengine
>> 	   # with "logall on" or "logpass on" it makes sense to have the category "checked" in the ACL.
>> 	   any
>> 	   # NOTE: ALL categories are part of the ACL for logging purposes.
>> 	   # Only when logall is off, one can remove the allowed categories from the ACL.
>>      }
>>
>> I don't have a similar config acl.
>>
>> -----Original Message-----
>> From: squid-users <squid-users-bounces at lists.squid-cache.org> On Behalf Of Amos Jeffries
>> Sent: Thursday, May 17, 2018 1:56 PM
>> To: squid-users at lists.squid-cache.org
>> Subject: Re: [squid-users] kaspersky and ufdbguard
>>
>> On 17/05/18 17:45, Vacheslav wrote:
>>> Peace,
>>>
>>> When I configured Kaspersky to use proxy, I started getting as an example:
>>>
>>> BLOCK -                10.96.0.104     config     https-option
>>> 195.122.177.165:443 CONNECT
>>>
>>> I have require https hostname. Kaspersky is updating fine.
>>>
>>> Anyone has an idea what Kaspersky is connecting ?
>>>
>>
>> That is a custom log format, you have not provided any info about what each field is. So no, we don't have much of a clue what it means.
>>
>> Amos
>> _______________________________________________
>> squid-users mailing list
>> squid-users at lists.squid-cache.org
>> http://lists.squid-cache.org/listinfo/squid-users
>>
>>
>> _______________________________________________
>> squid-users mailing list
>> squid-users at lists.squid-cache.org
>> http://lists.squid-cache.org/listinfo/squid-users
>>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
> 
> 
> 
> 


From robertocarna36 at gmail.com  Thu May 17 19:40:20 2018
From: robertocarna36 at gmail.com (Roberto Carna)
Date: Thu, 17 May 2018 16:40:20 -0300
Subject: [squid-users] Go to intranet server through Squid
Message-ID: <CAG2Qp6vZCwi-9EGijhYOFejHTaOZ2RAfhah+9ZQ-Sw=w91cu1g@mail.gmail.com>

Hi people, I have a Squid 3.1.20 for web browsing.

I have a local intranet server called "intranet.com.ar.com" resolving
to 192.168.10.10. This resolution is defined in my local DNS servers
and in /etc/hosts file from Squid.

Is there any way to send the web connections to intranet.company.com
through the Squid proxy and not defining an excecption in the each
user browsers ?

Thanks a lot, regards.


From aaron.hall at oath.com  Thu May 17 20:24:40 2018
From: aaron.hall at oath.com (Aaron Hall)
Date: Thu, 17 May 2018 16:24:40 -0400
Subject: [squid-users] Running Squid fully as root
Message-ID: <CAOA5C67o0mr7wunn97USeSznYviH-d=ffyNmRn4mqTSukxryeg@mail.gmail.com>

Greetings everyone.

Does anyone a "proper" way to run squid directly as "root" rather than the
squid user on linux?

Basic internet searches don't appear to give much of an answer.

OS: Centos 7.x
Squid Ver: 3.5.20

Cheers.
--
Aaron Hall
The Paranoids
Network Security
Aaron.Hall at oath.com
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20180517/16e0dd16/attachment.htm>

From Antony.Stone at squid.open.source.it  Thu May 17 20:51:00 2018
From: Antony.Stone at squid.open.source.it (Antony Stone)
Date: Thu, 17 May 2018 22:51:00 +0200
Subject: [squid-users] Running Squid fully as root
In-Reply-To: <CAOA5C67o0mr7wunn97USeSznYviH-d=ffyNmRn4mqTSukxryeg@mail.gmail.com>
References: <CAOA5C67o0mr7wunn97USeSznYviH-d=ffyNmRn4mqTSukxryeg@mail.gmail.com>
Message-ID: <201805172251.00741.Antony.Stone@squid.open.source.it>

On Thursday 17 May 2018 at 22:24:40, Aaron Hall wrote:

> Greetings everyone.
> 
> Does anyone a "proper" way to run squid directly as "root" rather than the
> squid user on linux?

Why do you want to?

There are good reasons not to do this.  What is a good reason to want to do 
this?

> Basic internet searches don't appear to give much of an answer.

That may be a clue that it's not a good idea to try to do it :)


Antony.

-- 
It may not seem obvious, but (6 x 5 + 5) x 5 - 55 equals 5!

                                                   Please reply to the list;
                                                         please *don't* CC me.


From alessio.troiano at leonardocompany.com  Thu May 17 21:00:23 2018
From: alessio.troiano at leonardocompany.com (Troiano Alessio)
Date: Thu, 17 May 2018 21:00:23 +0000
Subject: [squid-users] R:  Go to intranet server through Squid
In-Reply-To: <2030_1526586029_5AFDDAAD_2030_16192_1_CAG2Qp6vZCwi-9EGijhYOFejHTaOZ2RAfhah+9ZQ-Sw=w91cu1g@mail.gmail.com>
References: <2030_1526586029_5AFDDAAD_2030_16192_1_CAG2Qp6vZCwi-9EGijhYOFejHTaOZ2RAfhah+9ZQ-Sw=w91cu1g@mail.gmail.com>
Message-ID: <c2a7d80ccd5c416b940ca71c4611d67b@ocgepvsw3101.ocr.priv>

If you do not define any exception all the web requests are forwarded to squid proxy. So, the only thing you have to check is that squid can reach the server 192.168.10.10 (routing and/or firewall policy). Also check that squid uses your local DNS server and resolves the correct IP address.


Il presente messaggio e-mail e ogni suo allegato devono intendersi indirizzati esclusivamente al destinatario indicato e considerarsi dal contenuto strettamente riservato e confidenziale. Se non siete l'effettivo destinatario o avete ricevuto il messaggio e-mail per errore, siete pregati di avvertire immediatamente il mittente e di cancellare il suddetto messaggio e ogni suo allegato dal vostro sistema informatico. Qualsiasi utilizzo, diffusione, copia o archiviazione del presente messaggio da parte di chi non ne ? il destinatario ? strettamente proibito e pu? dar luogo a responsabilit? di carattere civile e penale punibili ai sensi di legge.
Questa e-mail ha valore legale solo se firmata digitalmente ai sensi della normativa vigente.

The contents of this email message and any attachments are intended solely for the addressee(s) and contain confidential and/or privileged information.
If you are not the intended recipient of this message, or if this message has been addressed to you in error, please immediately notify the sender and then delete this message and any attachments from your system. If you are not the intended recipient, you are hereby notified that any use, dissemination, copying, or storage of this message or its attachments is strictly prohibited. Unauthorized disclosure and/or use of information contained in this email message may result in civil and criminal liability. ?
This e-mail has legal value according to the applicable laws only if it is digitally signed by the sender
-----Messaggio originale-----
Da: squid-users [mailto:squid-users-bounces at lists.squid-cache.org] Per conto di Roberto Carna
Inviato: gioved? 17 maggio 2018 21:40
A: squid-users at squid-cache.org
Oggetto: [squid-users] Go to intranet server through Squid

Hi people, I have a Squid 3.1.20 for web browsing.

I have a local intranet server called "intranet.com.ar.com" resolving to 192.168.10.10. This resolution is defined in my local DNS servers and in /etc/hosts file from Squid.

Is there any way to send the web connections to intranet.company.com through the Squid proxy and not defining an excecption in the each user browsers ?

Thanks a lot, regards.
_______________________________________________
squid-users mailing list
squid-users at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-users

From uhlar at fantomas.sk  Fri May 18 07:56:48 2018
From: uhlar at fantomas.sk (Matus UHLAR - fantomas)
Date: Fri, 18 May 2018 09:56:48 +0200
Subject: [squid-users] Go to intranet server through Squid
In-Reply-To: <CAG2Qp6vZCwi-9EGijhYOFejHTaOZ2RAfhah+9ZQ-Sw=w91cu1g@mail.gmail.com>
References: <CAG2Qp6vZCwi-9EGijhYOFejHTaOZ2RAfhah+9ZQ-Sw=w91cu1g@mail.gmail.com>
Message-ID: <20180518075648.GA11190@fantomas.sk>

On 17.05.18 16:40, Roberto Carna wrote:
>Hi people, I have a Squid 3.1.20 for web browsing.
>
>I have a local intranet server called "intranet.com.ar.com" resolving
>to 192.168.10.10. This resolution is defined in my local DNS servers
>and in /etc/hosts file from Squid.
>
>Is there any way to send the web connections to intranet.company.com
>through the Squid proxy and not defining an excecption in the each
>user browsers ?

what exception? if users are configured to use the proxy, they will direct
everything there, aren't they?

-- 
Matus UHLAR - fantomas, uhlar at fantomas.sk ; http://www.fantomas.sk/
Warning: I wish NOT to receive e-mail advertising to this address.
Varovanie: na tuto adresu chcem NEDOSTAVAT akukolvek reklamnu postu.
Linux - It's now safe to turn on your computer.
Linux - Teraz mozete pocitac bez obav zapnut.


From ldclakmal at gmail.com  Fri May 18 12:16:03 2018
From: ldclakmal at gmail.com (Chanaka Lakmal)
Date: Fri, 18 May 2018 17:46:03 +0530
Subject: [squid-users] Squid with HTTP/2
In-Reply-To: <1fa2742c-ad46-c034-861a-50f059bb1fd4@treenet.co.nz>
References: <CAH-FGDh9LMZ_y1BG8ePxxtpmxSfEZDfh4OMFX_Ke-yNUhojMkg@mail.gmail.com>
 <1fa2742c-ad46-c034-861a-50f059bb1fd4@treenet.co.nz>
Message-ID: <CAH-FGDieyGK37CK8zkVHAZgc02fz5osQYo8qAsjDO6bt=Gt6RQ@mail.gmail.com>

Thanks for the response.

Regards,
Chanaka

On Thu, May 17, 2018 at 6:40 PM Amos Jeffries <squid3 at treenet.co.nz> wrote:

> On 18/05/18 00:23, Chanaka Lakmal wrote:
> > Hi,
> >
> > Does Squid supports HTTP/2 protocol? If so, what is the version it
> supports?
> >
>
> No and "sort of". Squid does not yet support it natively. Squid does
> support h2 tunneled inside TLS (except when SSL-Bumping) in the same way
> HTTPS has always been supported.
>
> Amos
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20180518/80cd6840/attachment.htm>

From steve.albrechtsen at gmail.com  Fri May 18 17:23:18 2018
From: steve.albrechtsen at gmail.com (albydarned)
Date: Fri, 18 May 2018 10:23:18 -0700 (MST)
Subject: [squid-users] Access Proxy By Url
Message-ID: <1526664198922-0.post@n4.nabble.com>

I have a squid proxy set up, and I can get it to work by adding the settings
in my browser. I have also set up HTTP basic auth.

Is it possible to hit the mydomain.com:3128 in a browser address bar and
pass a url as a parameter with the basic auth? When I do this I just get
this screen.

Currently i'm just going to mydomain.com:3128/https://google.com

Thank you

<http://squid-web-proxy-cache.1019090.n4.nabble.com/file/t377454/Screen_Shot_2018-05-18_at_11.png> 



--
Sent from: http://squid-web-proxy-cache.1019090.n4.nabble.com/Squid-Users-f1019091.html


From squid-users at filter.luko.org  Sat May 19 02:14:34 2018
From: squid-users at filter.luko.org (squid-users at filter.luko.org)
Date: Sat, 19 May 2018 12:14:34 +1000
Subject: [squid-users] Possible access via v6 when no interfaces present,
	fixable with dns_v4_first
Message-ID: <005b01d3ef17$21512a50$63f37ef0$@filter.luko.org>

Hello squid users,

I'm trying to understand a strange problem with requests to edge.apple.com,
which I think may be related to IPv6 DNS resolution.

To set the scene - we operate a large (1,000+) fleet of Squid 3.5.25 caches.
Each runs on a separate LAN, connected to the internet via another upstream
proxy, accessed over a wide-area network.  Each local cache runs on a CentOS
6 box, incuding BIND for name resolution.  For DNS resolution, each local
CentOS server runs BIND, which is configured to resolve against a local
Microsoft DNS server, which then resolves internet queries using a
whole-of-WAN BIND service operated by the carrier.  The WAN does not support
IPv6, and CentOS does not have any v6 network interfaces configured.

Recently we became aware of a fault on a single cache serving requests for
edge.icloud.com.  Requests would time out with a TAG_NONE/503 written to the
log.  The error could be replicated with cURL at the CLI using this URL:
https://edge.icloud.com/perf.css.  This was a strange error, because at the
time it happened, it was possible to connect to edge.icloud.com on port 443.
The error was happening in just one site.

To isolate the fault we stripped the Squid config at the affected site right
back to the following:

	# Skeleton Squid 3.5.25 config
	shutdown_lifetime 2 seconds
	max_filedesc 16384
	coredump_dir /var/spool/squid
	dns_timeout 5 seconds
	error_directory /var/www/squid-errors
	logfile_rotate 0
	http_port 3128
	cache_dir ufs /var/spool/squid 8192 16 256
	maximum_object_size 536870912 bytes
	cache_replacement_policy heap LFUDA
	http_access allow localhost
	debug_options ALL,5

Here's the messages written to the log when fetching
https://edge.icloud.com/perf.css with curl:

2018/05/08 16:25:46.321 kid1| 14,3| ipcache.cc(362) ipcacheParse: 18 answers
for 'edge.icloud.com'
2018/05/08 16:25:46.322 kid1| 14,3| ipcache.cc(431) ipcacheParse:
edge.icloud.com #0 [2403:300:a50:105::f]
2018/05/08 16:25:46.322 kid1| 14,3| ipcache.cc(431) ipcacheParse:
edge.icloud.com #1 [2403:300:a50:105::9]
2018/05/08 16:25:46.322 kid1| 14,3| ipcache.cc(431) ipcacheParse:
edge.icloud.com #2 [2403:300:a50:100::e]
2018/05/08 16:25:46.322 kid1| 14,3| ipcache.cc(431) ipcacheParse:
edge.icloud.com #3 [2403:300:a50:101::5]
2018/05/08 16:25:46.322 kid1| 14,3| ipcache.cc(431) ipcacheParse:
edge.icloud.com #4 [2403:300:a50:104::e]
2018/05/08 16:25:46.322 kid1| 14,3| ipcache.cc(431) ipcacheParse:
edge.icloud.com #5 [2403:300:a50:104::9]
2018/05/08 16:25:46.322 kid1| 14,3| ipcache.cc(431) ipcacheParse:
edge.icloud.com #6 [2403:300:a50:104::5]
2018/05/08 16:25:46.322 kid1| 14,3| ipcache.cc(431) ipcacheParse:
edge.icloud.com #7 [2403:300:a50:101::6]
2018/05/08 16:25:46.322 kid1| 14,3| ipcache.cc(420) ipcacheParse:
edge.icloud.com #8 17.248.155.107
2018/05/08 16:25:46.322 kid1| 14,3| ipcache.cc(420) ipcacheParse:
edge.icloud.com #9 17.248.155.142
2018/05/08 16:25:46.322 kid1| 14,3| ipcache.cc(420) ipcacheParse:
edge.icloud.com #10 17.248.155.110
2018/05/08 16:25:46.322 kid1| 14,3| ipcache.cc(420) ipcacheParse:
edge.icloud.com #11 17.248.155.80
2018/05/08 16:25:46.322 kid1| 14,3| ipcache.cc(420) ipcacheParse:
edge.icloud.com #12 17.248.155.114
2018/05/08 16:25:46.322 kid1| 14,3| ipcache.cc(420) ipcacheParse:
edge.icloud.com #13 17.248.155.77
2018/05/08 16:25:46.322 kid1| 14,3| ipcache.cc(420) ipcacheParse:
edge.icloud.com #14 17.248.155.145
2018/05/08 16:25:46.322 kid1| 14,3| ipcache.cc(420) ipcacheParse:
edge.icloud.com #15 17.248.155.89
2018/05/08 16:25:46.322 kid1| 44,2| peer_select.cc(280) peerSelectDnsPaths:
Found sources for 'edge.icloud.com:443'
2018/05/08 16:25:46.322 kid1| 44,2| peer_select.cc(281) peerSelectDnsPaths:
always_direct = DENIED
2018/05/08 16:25:46.322 kid1| 44,2| peer_select.cc(282) peerSelectDnsPaths:
never_direct = DENIED
2018/05/08 16:25:46.322 kid1| 44,2| peer_select.cc(286) peerSelectDnsPaths:
DIRECT = local=[::] remote=[2403:300:a50:105::f]:443 flags=1
2018/05/08 16:25:46.322 kid1| 44,2| peer_select.cc(286) peerSelectDnsPaths:
DIRECT = local=[::] remote=[2403:300:a50:105::9]:443 flags=1
2018/05/08 16:25:46.322 kid1| 44,2| peer_select.cc(286) peerSelectDnsPaths:
DIRECT = local=[::] remote=[2403:300:a50:100::e]:443 flags=1
2018/05/08 16:25:46.322 kid1| 44,2| peer_select.cc(286) peerSelectDnsPaths:
DIRECT = local=[::] remote=[2403:300:a50:101::5]:443 flags=1
2018/05/08 16:25:46.322 kid1| 44,2| peer_select.cc(286) peerSelectDnsPaths:
DIRECT = local=[::] remote=[2403:300:a50:104::e]:443 flags=1
2018/05/08 16:25:46.322 kid1| 44,2| peer_select.cc(286) peerSelectDnsPaths:
DIRECT = local=[::] remote=[2403:300:a50:104::9]:443 flags=1
2018/05/08 16:25:46.322 kid1| 44,2| peer_select.cc(286) peerSelectDnsPaths:
DIRECT = local=[::] remote=[2403:300:a50:104::5]:443 flags=1
2018/05/08 16:25:46.322 kid1| 44,2| peer_select.cc(286) peerSelectDnsPaths:
DIRECT = local=[::] remote=[2403:300:a50:101::6]:443 flags=1
2018/05/08 16:25:46.322 kid1| 44,2| peer_select.cc(286) peerSelectDnsPaths:
DIRECT = local=0.0.0.0 remote=17.248.155.107:443 flags=1
2018/05/08 16:25:46.322 kid1| 44,2| peer_select.cc(286) peerSelectDnsPaths:
DIRECT = local=0.0.0.0 remote=17.248.155.142:443 flags=1
2018/05/08 16:25:46.322 kid1| 44,2| peer_select.cc(286) peerSelectDnsPaths:
DIRECT = local=0.0.0.0 remote=17.248.155.110:443 flags=1
2018/05/08 16:25:46.322 kid1| 44,2| peer_select.cc(286) peerSelectDnsPaths:
DIRECT = local=0.0.0.0 remote=17.248.155.80:443 flags=1
2018/05/08 16:25:46.322 kid1| 44,2| peer_select.cc(286) peerSelectDnsPaths:
DIRECT = local=0.0.0.0 remote=17.248.155.114:443 flags=1
2018/05/08 16:25:46.322 kid1| 44,2| peer_select.cc(286) peerSelectDnsPaths:
DIRECT = local=0.0.0.0 remote=17.248.155.77:443 flags=1
2018/05/08 16:25:46.322 kid1| 44,2| peer_select.cc(286) peerSelectDnsPaths:
DIRECT = local=0.0.0.0 remote=17.248.155.145:443 flags=1
2018/05/08 16:25:46.322 kid1| 44,2| peer_select.cc(286) peerSelectDnsPaths:
DIRECT = local=0.0.0.0 remote=17.248.155.89:443 flags=1
2018/05/08 16:25:46.322 kid1| 44,2| peer_select.cc(295) peerSelectDnsPaths:
timedout = 0

After adding "dns_v4_first on" to the config, this allowed the request to
proceed.  Great!

We're interested to know why this copy of Squid acted differently to all the
others.  One potential clue we noticed is that this particular LAN has a lot
of v6 trafflic flying around on it, compared to the rest of our networks.  I
understand Squid runs it's own DNS resolver.  Could something on the local
LAN be announcing itself and the resolver in Squid picking up on that,
switching around the order of DNS resolution so that it tries that v6
address first?

Anything else we should be looking for?  Guidance appreciated!

Cheers
Luke




From squid3 at treenet.co.nz  Sat May 19 20:48:13 2018
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sun, 20 May 2018 08:48:13 +1200
Subject: [squid-users] Possible access via v6 when no interfaces present,
 fixable with dns_v4_first
In-Reply-To: <005b01d3ef17$21512a50$63f37ef0$@filter.luko.org>
References: <005b01d3ef17$21512a50$63f37ef0$@filter.luko.org>
Message-ID: <a66f087f-25ab-4bfc-652d-b82de4bd82a4@treenet.co.nz>

On 19/05/18 14:14, Luke wrote:
> 
> We're interested to know why this copy of Squid acted differently to all the
> others.  One potential clue we noticed is that this particular LAN has a lot
> of v6 trafflic flying around on it, compared to the rest of our networks.  I
> understand Squid runs it's own DNS resolver.  Could something on the local
> LAN be announcing itself and the resolver in Squid picking up on that,
> switching around the order of DNS resolution so that it tries that v6
> address first?

You have not mentioned overriding Squids normal DNS hijacking protection
with the ignore_unknown_nameservers directive. So probably not.

What you are seeing in that DNS result ordering is RFC 6540 (aka BCP
177) required behaviour. When a server is advertising IPv6 addresses,
they have preference over IPv4 ones.

The dns_v4_first option is provided to allow to you "force" use of the
outdated IPv4 protocol. It is a workaround, not a fix.

In normal operation Squid will test those addresses and immediately be
told by the network they are not routed, so move on to the IPv4 ones.
Updating its internal DNS cache appropriately not to try them again
until DNS TTL expires the record or IPv4 starts failing too.


> 
> Anything else we should be looking for?  Guidance appreciated!
> 

Anything that would cause an IPv6 TCP SYN request to result in a timeout
instead of ICMPv6 "not available" response packet. If IPv6 between Squid
and those edge.icloud.com IPv6 servers was working you would not have
noticed anything. That includes values of working such as "blocked at
the firewall".

Since Squid is actually getting DNS results I assume DNS lag is not the
problem. But if it is taking a long time to get those results it can
still be playing a part of the issue as it reduces the amount of time
that can be spent on IPv6 connection attempts.

Next thing after DNS to check is ICMP(v6). ICMP is not an optional
protocol and admin blocking it in firewalls can cause major problems
when IPv6 relies on it for routing. Specifically for MTU detection.
 <https://tools.ietf.org/html/rfc4890>
 <https://sites.google.com/site/ipv6center/icmpv6-is-non-optional>

With port 443 traffic is it increasingly likely that the protocol is not
compatible with HTTP/1.1. If the TCP SYN works fine, but the HTTP
messaging hangs (ICMP fault usually) or produces some weird protocol
response that Squid does not handle it can result in errors.

Some web servers have IPv6 records but hang or crash when IPv6 is
delivered to them. Cloudflare have pretty good IPv6 service, so that is
highly unlikely but worth a check if all else fails to bring up a clue.

Amos


From squid3 at treenet.co.nz  Sat May 19 20:57:06 2018
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sun, 20 May 2018 08:57:06 +1200
Subject: [squid-users] Access Proxy By Url
In-Reply-To: <1526664198922-0.post@n4.nabble.com>
References: <1526664198922-0.post@n4.nabble.com>
Message-ID: <b34e43a1-71fb-5c81-aa0c-5fe317adcbb2@treenet.co.nz>

On 19/05/18 05:23, albydarned wrote:
> I have a squid proxy set up, and I can get it to work by adding the settings
> in my browser. I have also set up HTTP basic auth.
> 
> Is it possible to hit the mydomain.com:3128 in a browser address bar and
> pass a url as a parameter with the basic auth? When I do this I just get
> this screen.
> 
> Currently i'm just going to mydomain.com:3128/https://google.com
> 

Don't do that.

There are hacks to make it look like it is "working" which you may find
mentioned. But those will break the authentication - the browser MUST
know that it is talking to a proxy to send the correct proxy-auth
credentials.

Proxy credentials are explicitly separate from web-server credentials so
that you (and your users) do not end up sharing your proxy login with
any remote web servers and your website credentials with the proxy.

Configuring the browser is required/mandatory to perform authentication
to the proxy. There are several auto-configuration methods that work
well if manually entering the proxy details is too onerous:
  <https://wiki.squid-cache.org/SquidFaq/ConfiguringBrowsers>


Amos


From squid3 at treenet.co.nz  Sat May 19 21:04:22 2018
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sun, 20 May 2018 09:04:22 +1200
Subject: [squid-users] Auth bearer support for forward proxy
In-Reply-To: <CAPxN_PVf5Fz851SE9j3=PC0Hz68PfJ5hGY7OhhvPJ0bC0YmsLg@mail.gmail.com>
References: <CAPxN_PVf5Fz851SE9j3=PC0Hz68PfJ5hGY7OhhvPJ0bC0YmsLg@mail.gmail.com>
Message-ID: <b4d3646e-25df-459a-6136-80bb06e8b59b@treenet.co.nz>

On 18/05/18 04:32, Panagiotis Bariamis wrote:
> Hello ,
> Only thing I have found concerning the subject is this 4 years old thread :
> 
>>From: Amos Jeffries
>>Date: Sun, 08 Jun 2014 14:46:09 +1200
> 
>>I have implemented Bearer authentication support in Squid and have found
>>a noticible lack of support in clients for the 407 status Proxy-Auth*
>>headers, even where Bearer support is advertized and working via 401
>>status WWW-Auth*.
> 
>>Can any of the client implementers (Browsers in particular) please point
>>out if they do support Bearer login to a proxy and what restrictions
>>they have on it actually working?
>>
>>Amos Jeffries
>>The Squid Software Foundation
> 
> Does? anyone know if browsers have implemented since then support for
> bearer authentication for Proxy-Auth ?
> 

AFAIK the situation has not changed. Except that there are now several
CDN providers using Bearer authentication in reverse-proxies.

I have been keeping the auth-bearer branch up to date with Squid-5. So
if you wish to try it out it should work okay (and safely). You will
need to provide your own authentication helper to interface with
whatever OAuth systems you use.

Amos


From akismpa at gmail.com  Sun May 20 09:18:48 2018
From: akismpa at gmail.com (Panagiotis Bariamis)
Date: Sun, 20 May 2018 12:18:48 +0300
Subject: [squid-users] Auth bearer support for forward proxy
In-Reply-To: <b4d3646e-25df-459a-6136-80bb06e8b59b@treenet.co.nz>
References: <CAPxN_PVf5Fz851SE9j3=PC0Hz68PfJ5hGY7OhhvPJ0bC0YmsLg@mail.gmail.com>
 <b4d3646e-25df-459a-6136-80bb06e8b59b@treenet.co.nz>
Message-ID: <CAPxN_PU2w9NTofZ30mXGQi-se+vf_a9Pj7yTox=ayUi5tTEyHg@mail.gmail.com>

On Sun, May 20, 2018 at 12:04 AM, Amos Jeffries <squid3 at treenet.co.nz>
wrote:

> On 18/05/18 04:32, Panagiotis Bariamis wrote:
> >> Hello ,
> >> Only thing I have found concerning the subject is this 4 years old
> thread :
> >>
> >>>From: Amos Jeffries
> >>>Date: Sun, 08 Jun 2014 14:46:09 +1200
> >>>
> >>>I have implemented Bearer authentication support in Squid and have found
> >>>a noticible lack of support in clients for the 407 status Proxy-Auth*
> >>>headers, even where Bearer support is advertized and working via 401
> >>>status WWW-Auth*.
> >>>
> >>>Can any of the client implementers (Browsers in particular) please point
> >>>out if they do support Bearer login to a proxy and what restrictions
> >>>they have on it actually working?
> >>>
> >>>Amos Jeffries
> >>>The Squid Software Foundation
> >>>
> >> Does  anyone know if browsers have implemented since then support for
> >> bearer authentication for Proxy-Auth ?
> >>
>
> >AFAIK the situation has not changed. Except that there are now several
> >CDN providers using Bearer authentication in reverse-proxies.
> >
> >I have been keeping the auth-bearer branch up to date with Squid-5. So
> >if you wish to try it out it should work okay (and safely). You will
> >need to provide your own authentication helper to interface with
> >whatever OAuth systems you use.
>
> >Amos
>
> Thank you very much for the info .
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20180520/4bf5bd29/attachment.htm>

From ziegleka at gmail.com  Mon May 21 12:08:56 2018
From: ziegleka at gmail.com (kAja Ziegler)
Date: Mon, 21 May 2018 14:08:56 +0200
Subject: [squid-users] The right way how to increase max_filedescriptors on
	Linux
Message-ID: <CAMuNeAtxp7TgxhRqkfe0Zy05c=8=EJxCOCqgUddsJFg0_GCMMg@mail.gmail.com>

Hi,

  I want to ask, if it is really needed to use ulimit or
/etc/security/limits.conf to increase max_filedescriptors value? From my
testing, it seems not.


*= my environment:*

CentOS 6.9
Squid 3.1.23 / 3.4.14

*- default ulimits for root and other users:*

[root at ...]# ulimit -Sa | grep -- '-n'
open files                      (-n) 1024
[root at ...]# ulimit -Ha | grep -- '-n'
open files                      (-n) 4096

*- default ulimits for squid user:*

[root at ...]# sudo -u squid /bin/bash
bash-4.1$ id
uid=23(squid) gid=23(squid) groups=23(squid),...
bash-4.1$ ulimit -Sa | grep -- '-n'
open files                      (-n) 1024
bash-4.1$ ulimit -Ha | grep -- '-n'
open files                      (-n) 4096

*- processes:*

[root at ...]# ps aux | grep squid
root      7194  0.0  0.1  73524  3492 ?        Ss   May17   0:00 squid -f
/etc/squid/squid.conf
squid     7197  0.2 10.9 276080 210156 ?       S    May17   4:53 (squid) -f
/etc/squid/squid.conf
squid     7198  0.0  0.0  20080  1084 ?        S    May17   0:00 (unlinkd)

*- error and warning messages from cache.log:*

client_side.cc(3070) okToAccept: WARNING! Your cache is running out of
filedescriptors

comm_open: socket failure: (24) Too many open files

IpIntercept.cc(137) NetfilterInterception:  NF getsockopt(SO_ORIGINAL_DST)
failed on FD 68: (2) No such file or directory ... (many with different FD)



I found many How-tos like these - https://access.redhat.com/solutions/63027
and
https://www.cyberciti.biz/faq/squid-proxy-server-running-out-filedescriptors/.
Both how-tos mention editing the file /etc/security/limits.conf and adding
the line "* - nofile 4096" to increase the nofile limit for all users
except root - I don't like this. According to my test, see below,  this is
not necessary, but I want to be sure, so I'm writing here.


*a) Squid default configuration (max_filedesc 0) and default nofile limit
(1024/4096):*

[root at ...]# ps aux | grep squid
root     17837  0.0  0.1  73524  3496 ?        Ss   13:45   0:00 squid -f
/etc/squid/squid.conf
squid    17840  0.3  0.5  76552 10860 ?        S    13:45   0:00 (squid) -f
/etc/squid/squid.conf
squid    17841  0.0  0.0  20080  1084 ?        S    13:45   0:00 (unlinkd)

[root at ...]# grep -E "Limit|Max open files" /proc/17837/limits
Limit                     Soft Limit           Hard Limit           Units
Max open files            1024                 4096                 files

[root at ...]# grep -E "Limit|Max open files" /proc/17840/limits
Limit                     Soft Limit           Hard Limit           Units
Max open files            1024                 4096                 files

[root at ...]# grep -E "Limit|Max open files" /proc/17841/limits
Limit                     Soft Limit           Hard Limit           Units
Max open files            1024                 4096                 files


*b) Squid configuration with max_filedesc 2048 and default nofile limit
(1024/4096):*

[root at ...]# ps aux | grep squid
root      7194  0.0  0.1  73524  3492 ?        Ss   May17   0:00 squid -f
/etc/squid/squid.conf
squid     7197  0.2 10.9 276080 210156 ?       S    May17   4:53 (squid) -f
/etc/squid/squid.conf
squid     7198  0.0  0.0  20080  1084 ?        S    May17   0:00 (unlinkd)

[root at ...]# grep -E "Limit|Max open files" /proc/7194/limits
Limit                     Soft Limit           Hard Limit           Units
Max open files            1024                 4096                 files

[root at ...]# grep -E "Limit|Max open files" /proc/7197/limits
Limit                     Soft Limit           Hard Limit           Units
Max open files            *2048*                 4096                 files

[root at ...]# grep -E "Limit|Max open files" /proc/7198/limits
Limit                     Soft Limit           Hard Limit           Units
Max open files            *2048*                 4096                 files

- soft nofile limit was increased for processes running under squid user


*c) Squid configuration with max_filedesc 8192 and default nofile limit
(1024/4096):*

[root at ...]# ps aux | grep squid
root     18734  0.0  0.1  73524  3492 ?        Ss   14:00   0:00 squid -f
/etc/squid/squid.conf
squid    18737  0.3  0.6  80244 11860 ?        S    14:00   0:00 (squid) -f
/etc/squid/squid.conf
squid    18740  0.0  0.0  20080  1088 ?        S    14:00   0:00 (unlinkd)

[root at ...]# grep -E "Limit|Max open files" /proc/18734/limits
Limit                     Soft Limit           Hard Limit           Units
Max open files            1024                 4096                 files

[root at ...]# grep -E "Limit|Max open files" /proc/18737/limits
Limit                     Soft Limit           Hard Limit           Units
Max open files            *8192*                 *8192*
files

[root at ...]# grep -E "Limit|Max open files" /proc/18740/limits
Limit                     Soft Limit           Hard Limit           Units
Max open files            *8192*                 *8192*
files

- both soft and hard nofile limits were increased for processes running
under squid user


I think, that the limits could be increased in tests b) and c) because the
master process runs under the root user. Am I right or not?
Or need I to increase the limits for the master proccess too?

Thank you and with best regards,
-- 
Karel Ziegler
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20180521/66f06df5/attachment.htm>

From alex at dvm.esines.cu  Mon May 21 12:35:35 2018
From: alex at dvm.esines.cu (=?UTF-8?Q?Alex_Guti=c3=a9rrez_Mart=c3=adnez?=)
Date: Mon, 21 May 2018 08:35:35 -0400
Subject: [squid-users] passwd problem
Message-ID: <3e648035-88a6-172a-13fc-c5eca1736e2d@dvm.esines.cu>

> On 12/05/18 00:30, Alex Guti?rrez Mart?nez wrote:
>> Hello community, yesterday i noticed that one of my users is using a
>> private squid to avoid the tedious problem of typing the password every
>> time he starts session in his browser. The problem is that the squid
>> suddenly started to return in plain text the username and password.
>> Therefore, that information is reflected in my proxy log.
>>
>> The directive that the user used is:
>>
>> "cache_peer proxy.mynetwork.cu parent 3229 0 no-query no-digest default
>> login = user1: passwd1"
> Ensure that there are no whitespace in that login string. It should just be:
>   login=user1:passwd1
>
> Also ensure that the user1 and passwd1 are correct values accepted by
> the peer for Basic auth.
>
> Amos


Im sorry the delay. It seem that thunderbird add the whitespaces to my 
parent directive when i paste the text. Im still facing the same 
problem, any sugestions??


"cache_peer proxy.mynetwork.cu parent 3229 0 no-query no-digest default login=user1:passwd1"

-- 
Saludos Cordiales

Lic. Alex Guti?rrez Mart?nez

Tel. +53 7 2710327

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20180521/e15594a1/attachment.htm>

From squid3 at treenet.co.nz  Mon May 21 13:29:52 2018
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Tue, 22 May 2018 01:29:52 +1200
Subject: [squid-users] The right way how to increase max_filedescriptors
 on Linux
In-Reply-To: <CAMuNeAtxp7TgxhRqkfe0Zy05c=8=EJxCOCqgUddsJFg0_GCMMg@mail.gmail.com>
References: <CAMuNeAtxp7TgxhRqkfe0Zy05c=8=EJxCOCqgUddsJFg0_GCMMg@mail.gmail.com>
Message-ID: <753601ff-8034-2735-fb85-dcd33c2938da@treenet.co.nz>

On 22/05/18 00:08, kAja Ziegler wrote:
> Hi,
> 
> ? I want to ask, if it is really needed to use ulimit or
> /etc/security/limits.conf to increase max_filedescriptors value? From my
> testing, it seems not.

Sometimes yes, sometimes no. It depends on what the systems normal
settings are and whether the Squid binary was built with full or partial
rlimit() support.

> 
> 
> *= my environment:*
> 
> CentOS 6.9
> Squid 3.1.23 / 3.4.14
> 
> *- default ulimits for root and other users:*
> 
> [root at ...]# ulimit -Sa | grep -- '-n'
> open files????????????????????? (-n) 1024
> [root at ...]# ulimit -Ha | grep -- '-n'
> open files????????????????????? (-n) 4096
> 
> *- default ulimits for squid user:*
> 
> [root at ...]# sudo -u squid /bin/bash
> bash-4.1$ id
> uid=23(squid) gid=23(squid) groups=23(squid),...
> bash-4.1$ ulimit -Sa | grep -- '-n'
> open files????????????????????? (-n) 1024
> bash-4.1$ ulimit -Ha | grep -- '-n'
> open files????????????????????? (-n) 4096
> 
> *- processes:*
> 
> [root at ...]# ps aux | grep squid
> root????? 7194? 0.0? 0.1? 73524? 3492 ???????? Ss?? May17?? 0:00 squid
> -f /etc/squid/squid.conf
> squid???? 7197? 0.2 10.9 276080 210156 ??????? S??? May17?? 4:53 (squid)
> -f /etc/squid/squid.conf
> squid???? 7198? 0.0? 0.0? 20080? 1084 ???????? S??? May17?? 0:00 (unlinkd)
> 
> *- error and warning messages from cache.log:*
> 
> client_side.cc(3070) okToAccept: WARNING! Your cache is running out of
> filedescriptors
> 
> comm_open: socket failure: (24) Too many open files
> 
> IpIntercept.cc(137) NetfilterInterception:? NF
> getsockopt(SO_ORIGINAL_DST) failed on FD 68: (2) No such file or
> directory ... (many with different FD)
> 

These should not be related to FD numbers running out. As you can see FD
68 was already allocated to this TCP connection and the socket accept()'ed.

NAT errors are usually caused by explicit-proxy traffic arriving at a
NAT interception port. Such traffic is prohibited.
or by NAT table overflowing under extreme traffic loads. Either way
current Squid versions will terminate that connection immediately since
it cannot identify where the packets were supposed to be going.

> 
> I found many How-tos like these -
> https://access.redhat.com/solutions/63027 and
> https://www.cyberciti.biz/faq/squid-proxy-server-running-out-filedescriptors/.
> Both how-tos mention editing the file /etc/security/limits.conf and
> adding the line "* - nofile 4096" to increase the nofile limit for all
> users except root - I don't like this. According to my test, see below,?
> this is not necessary, but I want to be sure, so I'm writing here.

Note that neither of those are the official Squid FAQ.

The official recommendation is to use those data sources to *check* what
the system limits are.

The official Best Practice varies depending on ones needs. Packagers
distributing Squid are advised to set reasonable limits in the init
script starting Squid. End users to use the configuration file best
suited to their need (MAY be limits.conf, but usually squid.conf).


> 
> *a) Squid default configuration (max_filedesc 0) and default nofile
> limit (1024/4096):*
> 

Do not set the limit to "0". That actually means *no* filedescriptors
for the newer Squid versions.

Remove the directive entirely from your squid.conf for the default
behaviour.

Also "max_filedescriptors" is teh directive name. "max_filedesc" was
only for the experimental RHEL patch many decades ago.

> 
> *c) Squid configuration with max_filedesc 8192 and default nofile limit
> (1024/4096):*
> 
> [root at ...]# ps aux | grep squid
> root???? 18734? 0.0? 0.1? 73524? 3492 ???????? Ss?? 14:00?? 0:00 squid
> -f /etc/squid/squid.conf
> squid??? 18737? 0.3? 0.6? 80244 11860 ???????? S??? 14:00?? 0:00 (squid)
> -f /etc/squid/squid.conf
> squid??? 18740? 0.0? 0.0? 20080? 1088 ???????? S??? 14:00?? 0:00 (unlinkd)
> 
> [root at ...]# grep -E "Limit|Max open files" /proc/18734/limits
> Limit???????????????????? Soft Limit?????????? Hard Limit?????????? Units
> Max open files??????????? 1024???????????????? 4096???????????????? files
> 
> [root at ...]# grep -E "Limit|Max open files" /proc/18737/limits
> Limit???????????????????? Soft Limit?????????? Hard Limit?????????? Units
> Max open files??????????? *8192*???????????????? *8192*????????????????
> files
> 
> [root at ...]# grep -E "Limit|Max open files" /proc/18740/limits
> Limit???????????????????? Soft Limit?????????? Hard Limit?????????? Units
> Max open files????????????*8192*?????????????????*8192*????????????????
> files
> 
> - both soft and hard nofile limits were increased for processes running
> under squid user
> 
> 
> I think, that the limits could be increased in tests b) and c) because
> the master process runs under the root user. Am I right or not?

AFAIK, Hard limit can be changed by root (or ulimit tool itself would
not work). Soft limit can be changed by any user to any value below hard
limit.

What you see in (c) is the master process changing the hard limit for
its spawned child processes so that they can use the value in squid.conf
without errors.


> Or need I to increase the limits for the master proccess too?

Not if squid is correctly setting the limits for you. Doing that
automatically is one of the reasons the master exists separately from
the workers. The init script use of ulimit is a workaround for builds
where rlimit() support is lacking or broken.

Amos


From squid3 at treenet.co.nz  Mon May 21 13:37:23 2018
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Tue, 22 May 2018 01:37:23 +1200
Subject: [squid-users] passwd problem
In-Reply-To: <3e648035-88a6-172a-13fc-c5eca1736e2d@dvm.esines.cu>
References: <3e648035-88a6-172a-13fc-c5eca1736e2d@dvm.esines.cu>
Message-ID: <5c7dc065-9227-ae8a-9926-7e14f370d101@treenet.co.nz>

On 22/05/18 00:35, Alex Guti?rrez Mart?nez wrote:
>> On 12/05/18 00:30, Alex Guti?rrez Mart?nez wrote:
>>> Hello community, yesterday i noticed that one of my users is using a
>>> private squid to avoid the tedious problem of typing the password every
>>> time he starts session in his browser. The problem is that the squid
>>> suddenly started to return in plain text the username and password.
>>> Therefore, that information is reflected in my proxy log.
>>>
>>> The directive that the user used is:
>>>
>>> "cache_peer proxy.mynetwork.cu parent 3229 0 no-query no-digest default
>>> login = user1: passwd1"
>> Ensure that there are no whitespace in that login string. It should just be:
>>  login=user1:passwd1
>>
>> Also ensure that the user1 and passwd1 are correct values accepted by
>> the peer for Basic auth.
>>
>> Amos
> 
> 
> Im sorry the delay. It seem that thunderbird add the whitespaces to my
> parent directive when i paste the text. Im still facing the same
> problem, any sugestions??
> 
> 
> "cache_peer proxy.mynetwork.cu parent 3229 0 no-query no-digest default login=user1:passwd1"
> 

Okay so that directive tells Squid to use Basic auth with that parent proxy.


What type of authentication is supposed to be going on if not HTTP Basic
auth?

And what does your log contain exactly compared to what you want to see
there?
replace username and passsword with fake values of course, but we do
need to see exactly what is going on to know what fix is best.

Amos


From carlos.caballero at cfg.jovenclub.cu  Mon May 21 16:22:50 2018
From: carlos.caballero at cfg.jovenclub.cu (=?UTF-8?Q?Carlos_Cesar_Caballero_D=c3=adaz?=)
Date: Mon, 21 May 2018 12:22:50 -0400
Subject: [squid-users] cache_peer to SSL/TLS proxy
Message-ID: <9e776af3-4eea-9841-918a-e548b2f0ed0b@cfg.jovenclub.cu>

Hi, I am really new with squid, an I am trying to solve an issue.

Right now I am working against a squid proxy wich is using SSL/TLS 
(encrypted browser-squid connection) and as you know there are a lot of 
applications that does not work with this kind of proxy configuration. 
In other ocations, I have been able to avoid some proxy issues 
installing a local squid and using cache_peer, so that my local squid 
handle the nasty parent configurations and my applications can work 
cleanly against the local instance. So, can I use cache_peer against a 
parent proxy whish is using SSL/TLS for encrypted browser-squid 
connection? An if it is possible, How?

Greetings.




From squid3 at treenet.co.nz  Mon May 21 16:49:37 2018
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Tue, 22 May 2018 04:49:37 +1200
Subject: [squid-users] cache_peer to SSL/TLS proxy
In-Reply-To: <9e776af3-4eea-9841-918a-e548b2f0ed0b@cfg.jovenclub.cu>
References: <9e776af3-4eea-9841-918a-e548b2f0ed0b@cfg.jovenclub.cu>
Message-ID: <143505f7-9dc0-f457-78fb-baa7a30bd680@treenet.co.nz>

On 22/05/18 04:22, Carlos Cesar Caballero D?az wrote:
>  can I use cache_peer against a
> parent proxy whish is using SSL/TLS for encrypted browser-squid
> connection? An if it is possible, How?
> 

Add the "ssl" (Squid-3) or "tls" (Squid-4) option to your cache_peer
line and all traffic to that peer will be encrypted with TLS/SSL.

See <http://www.squid-cache.org/Doc/config/cache_peer/> in the section
called "SSL / HTTPS / TLS OPTIONS" for more options related to securing
the connection between the proxies.
 If you have Squid-3 the option names are a bit different, so see the
doc page for your specific Squid series number.

Amos


From rousskov at measurement-factory.com  Mon May 21 16:58:55 2018
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Mon, 21 May 2018 10:58:55 -0600
Subject: [squid-users] cache_peer to SSL/TLS proxy
In-Reply-To: <9e776af3-4eea-9841-918a-e548b2f0ed0b@cfg.jovenclub.cu>
References: <9e776af3-4eea-9841-918a-e548b2f0ed0b@cfg.jovenclub.cu>
Message-ID: <6746e19b-9db0-1181-f17d-6f9b9968b95e@measurement-factory.com>

On 05/21/2018 10:22 AM, Carlos Cesar Caballero D?az wrote:

> Right now I am working against a squid proxy wich is using SSL/TLS
> (encrypted browser-squid connection) and as you know there are a lot of
> applications that does not work with this kind of proxy configuration.
> In other ocations, I have been able to avoid some proxy issues
> installing a local squid and using cache_peer, so that my local squid
> handle the nasty parent configurations and my applications can work
> cleanly against the local instance. So, can I use cache_peer against a
> parent proxy whish is using SSL/TLS for encrypted browser-squid
> connection? An if it is possible, How?

Do you want to configure your Squid proxy to use proxy B as a parent
when proxy B insists on all connections to it being encrypted? If yes,
please see the various cache_peer options that start with letters "tls"
and "ssl":

> 	==== SSL / HTTPS / TLS OPTIONS ====
> 	
> 	tls		Encrypt connections to this peer with TLS.
...


I have not tested the HTTPS parent setup discussed above, but it looks
like it should work in principle.

Please note that, AFAIK, Squid does not support HTTPS parents for many
SslBump configurations that require looking at TLS server Hello packets
-- there is currently no support for TLS inside TLS.

Alex.


From Sarfaraz.Ahmad at deshaw.com  Tue May 22 04:59:48 2018
From: Sarfaraz.Ahmad at deshaw.com (Ahmad, Sarfaraz)
Date: Tue, 22 May 2018 04:59:48 +0000
Subject: [squid-users] Cert download from AIA information succeeds yet Squid
 reports ERR_UNABLE_TO_GET_ISSUER_CERT_LOCALLY
Message-ID: <f20f8270488741d7b91ede1195a7d124@mbxtoa3.winmail.deshaw.com>

Hi,

I have setup Squid as a SSL MITM proxy.
I am also using the cert download feature with these configurations in my squid.conf

acl cert_fetch transaction_initiator certificate-fetching
http_access allow cert_fetch

Websites where certificates just share AIA information using CA-issuer method, those work just fine.

But try this one, https://community.verizonwireless.com/welcome (this gets bumped in my setup)
Here the AIA information Is provided using both OCSP/CAissuer methods.
>From Squid's access logs, I can tell that the certificate gets downloaded.

1526964147.929    160 - TCP_MISS/200 1868 GET http://cacert.omniroot.com/vpssg142.crt - HIER_DIRECT/64.18.25.46 application/x-x509-ca-cert

But squid still reports:

(71) Protocol error (TLS code: X509_V_ERR_UNABLE_TO_GET_ISSUER_CERT_LOCALLY)
SSL Certficate error: certificate issuer (CA) not known: /C=NL/L=Amsterdam/O=Verizon Enterprise Solutions/OU=Cybertrust/CN=Verizon Public SureServer CA G14-SHA2

That is the only intermediate certificate needed in the chain.  Here: https://www.ssllabs.com/ssltest/analyze.html?d=community.verizonwireless.com&latest

When I download the intermediate certificate locally and try connecting to the remote server using openssl -Cafile option, Openssl reports OK (0).

openssl s_client -connect 204.93.84.201:443 -showcerts -CAfile vpssg142.crt -servername community.verizon.com
>>     Verify return code: 0 (ok)

Regards,
Sarfaraz

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20180522/cbc5fb44/attachment.htm>

From ziegleka at gmail.com  Tue May 22 08:46:44 2018
From: ziegleka at gmail.com (kAja Ziegler)
Date: Tue, 22 May 2018 10:46:44 +0200
Subject: [squid-users] The right way how to increase max_filedescriptors
 on Linux
In-Reply-To: <753601ff-8034-2735-fb85-dcd33c2938da@treenet.co.nz>
References: <CAMuNeAtxp7TgxhRqkfe0Zy05c=8=EJxCOCqgUddsJFg0_GCMMg@mail.gmail.com>
 <753601ff-8034-2735-fb85-dcd33c2938da@treenet.co.nz>
Message-ID: <CAMuNeAsz=uDp4oOBANj+2wXNv5+nWFSg_hVZmna2Cjx7Wc1UHg@mail.gmail.com>

On Mon, May 21, 2018 at 3:29 PM, Amos Jeffries <squid3 at treenet.co.nz> wrote:

> On 22/05/18 00:08, kAja Ziegler wrote:
> > Hi,
> >
> >   I want to ask, if it is really needed to use ulimit or
> > /etc/security/limits.conf to increase max_filedescriptors value? From my
> > testing, it seems not.
>
> Sometimes yes, sometimes no. It depends on what the systems normal
> settings are and whether the Squid binary was built with full or partial
> rlimit() support.
>
> >
> >
> > *= my environment:*
> >
> > CentOS 6.9
> > Squid 3.1.23 / 3.4.14
> >
> > *- default ulimits for root and other users:*
> >
> > [root at ...]# ulimit -Sa | grep -- '-n'
> > open files                      (-n) 1024
> > [root at ...]# ulimit -Ha | grep -- '-n'
> > open files                      (-n) 4096
> >
> > *- default ulimits for squid user:*
> >
> > [root at ...]# sudo -u squid /bin/bash
> > bash-4.1$ id
> > uid=23(squid) gid=23(squid) groups=23(squid),...
> > bash-4.1$ ulimit -Sa | grep -- '-n'
> > open files                      (-n) 1024
> > bash-4.1$ ulimit -Ha | grep -- '-n'
> > open files                      (-n) 4096
> >
> > *- processes:*
> >
> > [root at ...]# ps aux | grep squid
> > root      7194  0.0  0.1  73524  3492 ?        Ss   May17   0:00 squid
> > -f /etc/squid/squid.conf
> > squid     7197  0.2 10.9 276080 210156 ?       S    May17   4:53 (squid)
> > -f /etc/squid/squid.conf
> > squid     7198  0.0  0.0  20080  1084 ?        S    May17   0:00
> (unlinkd)
> >
> > *- error and warning messages from cache.log:*
> >
> > client_side.cc(3070) okToAccept: WARNING! Your cache is running out of
> > filedescriptors
> >
> > comm_open: socket failure: (24) Too many open files
> >
> > IpIntercept.cc(137) NetfilterInterception:  NF
> > getsockopt(SO_ORIGINAL_DST) failed on FD 68: (2) No such file or
> > directory ... (many with different FD)
> >
>
> These should not be related to FD numbers running out. As you can see FD
> 68 was already allocated to this TCP connection and the socket accept()'ed.
>
> NAT errors are usually caused by explicit-proxy traffic arriving at a
> NAT interception port. Such traffic is prohibited.
> or by NAT table overflowing under extreme traffic loads. Either way
> current Squid versions will terminate that connection immediately since
> it cannot identify where the packets were supposed to be going.
>
> >
> > I found many How-tos like these -
> > https://access.redhat.com/solutions/63027 and
> > https://www.cyberciti.biz/faq/squid-proxy-server-running-
> out-filedescriptors/.
> > Both how-tos mention editing the file /etc/security/limits.conf and
> > adding the line "* - nofile 4096" to increase the nofile limit for all
> > users except root - I don't like this. According to my test, see below,
> > this is not necessary, but I want to be sure, so I'm writing here.
>
> Note that neither of those are the official Squid FAQ.
>
> The official recommendation is to use those data sources to *check* what
> the system limits are.
>
> The official Best Practice varies depending on ones needs. Packagers
> distributing Squid are advised to set reasonable limits in the init
> script starting Squid. End users to use the configuration file best
> suited to their need (MAY be limits.conf, but usually squid.conf).
>
>
> >
> > *a) Squid default configuration (max_filedesc 0) and default nofile
> > limit (1024/4096):*
> >
>
> Do not set the limit to "0". That actually means *no* filedescriptors
> for the newer Squid versions.
>
> Remove the directive entirely from your squid.conf for the default
> behaviour.
>
> Also "max_filedescriptors" is teh directive name. "max_filedesc" was
> only for the experimental RHEL patch many decades ago.
>
> >
> > *c) Squid configuration with max_filedesc 8192 and default nofile limit
> > (1024/4096):*
> >
> > [root at ...]# ps aux | grep squid
> > root     18734  0.0  0.1  73524  3492 ?        Ss   14:00   0:00 squid
> > -f /etc/squid/squid.conf
> > squid    18737  0.3  0.6  80244 11860 ?        S    14:00   0:00 (squid)
> > -f /etc/squid/squid.conf
> > squid    18740  0.0  0.0  20080  1088 ?        S    14:00   0:00
> (unlinkd)
> >
> > [root at ...]# grep -E "Limit|Max open files" /proc/18734/limits
> > Limit                     Soft Limit           Hard Limit           Units
> > Max open files            1024                 4096                 files
> >
> > [root at ...]# grep -E "Limit|Max open files" /proc/18737/limits
> > Limit                     Soft Limit           Hard Limit           Units
> > Max open files            *8192*                 *8192*
> > files
> >
> > [root at ...]# grep -E "Limit|Max open files" /proc/18740/limits
> > Limit                     Soft Limit           Hard Limit           Units
> > Max open files            *8192*                 *8192*
> > files
> >
> > - both soft and hard nofile limits were increased for processes running
> > under squid user
> >
> >
> > I think, that the limits could be increased in tests b) and c) because
> > the master process runs under the root user. Am I right or not?
>
> AFAIK, Hard limit can be changed by root (or ulimit tool itself would
> not work). Soft limit can be changed by any user to any value below hard
> limit.
>
> What you see in (c) is the master process changing the hard limit for
> its spawned child processes so that they can use the value in squid.conf
> without errors.
>
>
> > Or need I to increase the limits for the master proccess too?
>
> Not if squid is correctly setting the limits for you. Doing that
> automatically is one of the reasons the master exists separately from
> the workers. The init script use of ulimit is a workaround for builds
> where rlimit() support is lacking or broken.
>
> Amos
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>


Thank you Amos for your clarification/explanation and confirmation of my
presumptions.

About the NAT errors I'm going to write new email.


Do not set the limit to "0". That actually means *no* filedescriptors for
> the newer Squid versions.
>
> Remove the directive entirely from your squid.conf for the default
> behaviour.


Thank you for the warning, I came up with the documentation for Squid 3.1.


Also "max_filedescriptors" is teh directive name. "max_filedesc" was only
> for the experimental RHEL patch many decades ago.


Yep, I made copy & paste error from the old RHEL 5.x page -
https://access.redhat.com/solutions/63027.
I use "max_filedescriptors" in my configuration of course.


Best regards
-- 
Karel Ziegler
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20180522/4e621804/attachment.htm>

From ziegleka at gmail.com  Tue May 22 10:06:07 2018
From: ziegleka at gmail.com (kAja Ziegler)
Date: Tue, 22 May 2018 12:06:07 +0200
Subject: [squid-users] NetfilterInterception: NF >
	getsockopt(SO_ORIGINAL_DST) errors
Message-ID: <CAMuNeAsV+xS7QNRr+9tHHHktv4g3bOh816NfS8XB7-KQy_6n=g@mail.gmail.com>

Hi,

  this question/problem is extracted from the other email "The right way
how to increase max_filedescriptors on Linux".

*- my environment:*

CentOS 6.9
Squid 3.1.23 / 3.4.14
IPv4 and IPv6 addresses on interfaces

*- error and warning messages from cache.log:*

IpIntercept.cc(137) NetfilterInterception:  NF getsockopt(SO_ORIGINAL_DST)
failed on FD NN: (2) No such file or directory

NN ... many error log entries with different FD value

On Mon, May 21, 2018 at 3:29 PM, Amos Jeffries <squid3 at treenet.co.nz> wrote:

> These should not be related to FD numbers running out. As you can see FD
> 68 was already allocated to this TCP connection and the socket accept()'ed.
>
> NAT errors are usually caused by explicit-proxy traffic arriving at a
> NAT interception port. Such traffic is prohibited.
> or by NAT table overflowing under extreme traffic loads. Either way
> current Squid versions will terminate that connection immediately since
> it cannot identify where the packets were supposed to be going.
>

This is strange because I don't use any NAT iptables/netfilter rules on
this server:

[root at ...]# iptables -n -L -v -t nat
Chain PREROUTING (policy ACCEPT 26964 packets, 1870K bytes)
 pkts bytes target     prot opt in     out     source
destination

Chain POSTROUTING (policy ACCEPT 11013 packets, 817K bytes)
 pkts bytes target     prot opt in     out     source
destination

Chain OUTPUT (policy ACCEPT 11015 packets, 817K bytes)
 pkts bytes target     prot opt in     out     source
destination-


Only one weird thing I found in my Squid configuration - I had defined only
one http_port (http_port 3128 intercept) and this port was used to access
proxy via explicit definitions in systems or applications - without any
REDIRECT or marking in iptables/netfilter rules


I thank for every response that makes the error messages more clear.
-- 
Karel Ziegler
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20180522/0613eb59/attachment.htm>

From squid3 at treenet.co.nz  Tue May 22 10:24:08 2018
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Tue, 22 May 2018 22:24:08 +1200
Subject: [squid-users] NetfilterInterception: NF >
 getsockopt(SO_ORIGINAL_DST) errors
In-Reply-To: <CAMuNeAsV+xS7QNRr+9tHHHktv4g3bOh816NfS8XB7-KQy_6n=g@mail.gmail.com>
References: <CAMuNeAsV+xS7QNRr+9tHHHktv4g3bOh816NfS8XB7-KQy_6n=g@mail.gmail.com>
Message-ID: <71efba03-b4aa-84be-b6ec-da9c81a9921f@treenet.co.nz>

On 22/05/18 22:06, kAja Ziegler wrote:
> This is strange because I don't use any NAT iptables/netfilter rules on
> this server:
> 
> [root at ...]# iptables -n -L -v -t nat
> Chain PREROUTING (policy ACCEPT 26964 packets, 1870K bytes)
> ?pkts bytes target ? ? prot opt in ? ? out ? ? source ? ? ? ? ? ? ?
> destination
> 
> Chain POSTROUTING (policy ACCEPT 11013 packets, 817K bytes)
> ?pkts bytes target ? ? prot opt in ? ? out ? ? source ? ? ? ? ? ? ?
> destination
> 
> Chain OUTPUT (policy ACCEPT 11015 packets, 817K bytes)
> ?pkts bytes target ? ? prot opt in ? ? out ? ? source ? ? ? ? ? ? ?
> destination-

That lack of NAT rules would be why Squid cannot find any entries for
the traffic in the kernels NAT state table.


> 
> 
> Only one weird thing I found in my Squid configuration - I had defined
> only one http_port (http_port 3128 intercept) and this port was used to
> access proxy via explicit definitions in systems or applications -
> without any REDIRECT or marking in iptables/netfilter rules

There is the problem. That "intercept" mode/flag means NAT intercepted
traffic is the only type you are going to receive there.

Explicit / forward proxy is the "normal" traffic case for proxies. A
port to receive that traffic is configured without any special mode
flag. Just:
  http_port 3128


Amos


From ziegleka at gmail.com  Tue May 22 12:30:10 2018
From: ziegleka at gmail.com (kAja Ziegler)
Date: Tue, 22 May 2018 14:30:10 +0200
Subject: [squid-users] NetfilterInterception: NF >
 getsockopt(SO_ORIGINAL_DST) errors
In-Reply-To: <71efba03-b4aa-84be-b6ec-da9c81a9921f@treenet.co.nz>
References: <CAMuNeAsV+xS7QNRr+9tHHHktv4g3bOh816NfS8XB7-KQy_6n=g@mail.gmail.com>
 <71efba03-b4aa-84be-b6ec-da9c81a9921f@treenet.co.nz>
Message-ID: <CAMuNeAs6-mcXgNpU=KoJhsEMx-Pqm_U9x3UL0UyKf3r=XF+cMg@mail.gmail.com>

On Tue, May 22, 2018 at 12:24 PM, Amos Jeffries <squid3 at treenet.co.nz>
wrote:

> On 22/05/18 22:06, kAja Ziegler wrote:
> > This is strange because I don't use any NAT iptables/netfilter rules on
> > this server:
> >
> > [root at ...]# iptables -n -L -v -t nat
> > Chain PREROUTING (policy ACCEPT 26964 packets, 1870K bytes)
> >  pkts bytes target     prot opt in     out     source
> > destination
> >
> > Chain POSTROUTING (policy ACCEPT 11013 packets, 817K bytes)
> >  pkts bytes target     prot opt in     out     source
> > destination
> >
> > Chain OUTPUT (policy ACCEPT 11015 packets, 817K bytes)
> >  pkts bytes target     prot opt in     out     source
> > destination-
>
> That lack of NAT rules would be why Squid cannot find any entries for
> the traffic in the kernels NAT state table.
>
>
> >
> >
> > Only one weird thing I found in my Squid configuration - I had defined
> > only one http_port (http_port 3128 intercept) and this port was used to
> > access proxy via explicit definitions in systems or applications -
> > without any REDIRECT or marking in iptables/netfilter rules
>
> There is the problem. That "intercept" mode/flag means NAT intercepted
> traffic is the only type you are going to receive there.
>
> Explicit / forward proxy is the "normal" traffic case for proxies. A
> port to receive that traffic is configured without any special mode
> flag. Just:
>   http_port 3128
>
>
> Amos
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>

Hi Amos,

It's silly that I did not notice these errors earlier. I found them in the
log just recently.

Communication via proxy in this configuration (with http_port 3128
intercept) has worked well for years.

I've removed the intercept from the configuration, so I'll see.


Thank you and with best regqards
-- 
Karel Ziegler
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20180522/be4e0220/attachment.htm>

From rousskov at measurement-factory.com  Tue May 22 15:50:14 2018
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Tue, 22 May 2018 09:50:14 -0600
Subject: [squid-users] Cert download from AIA information succeeds yet
 Squid reports ERR_UNABLE_TO_GET_ISSUER_CERT_LOCALLY
In-Reply-To: <f20f8270488741d7b91ede1195a7d124@mbxtoa3.winmail.deshaw.com>
References: <f20f8270488741d7b91ede1195a7d124@mbxtoa3.winmail.deshaw.com>
Message-ID: <f0798541-08ac-73ee-5c1c-bfc669d86774@measurement-factory.com>

On 05/21/2018 10:59 PM, Ahmad, Sarfaraz wrote:

> Websites where certificates just share AIA information using CA-issuer
> method, those work just fine.
> 
> ?
> 
> But try this one, https://community.verizonwireless.com/welcome (this
> gets bumped in my setup)
> 
> Here the AIA information Is provided using both OCSP/CAissuer methods.
> 
> From Squid?s access logs, I can tell that the certificate gets downloaded.
> 
> ?
> 
> 1526964147.929??? 160 - TCP_MISS/200 1868 GET
> http://cacert.omniroot.com/vpssg142.crt - HIER_DIRECT/64.18.25.46
> application/x-x509-ca-cert
> 
> ?
> 
> But squid still reports*:*
> 
> *(71) Protocol error (TLS code: X509_V_ERR_UNABLE_TO_GET_ISSUER_CERT_LOCALLY)
> *SSL Certficate error: certificate issuer (CA) not known:
> /C=NL/L=Amsterdam/O=Verizon Enterprise
> Solutions/OU=Cybertrust/CN=Verizon Public SureServer CA G14-SHA2
> 
> *?*
> 
> That is the only intermediate certificate needed in the chain. ?Here:
> https://www.ssllabs.com/ssltest/analyze.html?d=community.verizonwireless.com&latest
> 
> ?
> 
> When I download the intermediate certificate locally and try connecting
> to the remote server using openssl ?Cafile option, Openssl reports OK (0).
> 
> ?
> 
> openssl s_client -connect 204.93.84.201:443 -showcerts -CAfile
> vpssg142.crt ?servername community.verizon.com
> 
>>> ????Verify return code: 0 (ok)


Nice triage! I do not know what went wrong, unfortunately. If you do not
find a solution on the mailing list, I recommend posting a bug report.
If possible, attach compressed partial cache.log (with debug_options set
to ALL,9) collected while reproducing the above problem without any
other transactions. This log might speed up resolution by exposing the
problem without the need to reproduce it locally.

Alex.


From rejaine at bhz.jamef.com.br  Tue May 22 16:54:21 2018
From: rejaine at bhz.jamef.com.br (Rejaine Monteiro)
Date: Tue, 22 May 2018 13:54:21 -0300
Subject: [squid-users] Squid 3.5 TAG_NONE/503 HIER_NONE
Message-ID: <CAMTrDfWcAgbj=jb_4CrbYz-wZ6k=1Y17G=V56rnLE0BH21Uzpg@mail.gmail.com>

Hello guys,

I had a squid-proxy (squid-3.5.21-5.3.1.x86_64) running on a OpenSUSE
Leap 42.2

Everything is working well, access all the sites (http or https
sites), except  these two:

   www.uai.com.br or www.em.com.br
?
There is no blocking on the firewall (if bypass squid , access is
normally done) and the ip address is resolved normally.

And I am using a basic setup and of squid, without rules or access policies...

The only error you have in access.log is this:

1527006832.956     15 ::1 TAG_NONE/503 0 CONNECT www.em.com.br:443 -
HIER_NONE/- -
1527006834.376      0 ::1 TAG_NONE/503 0 CONNECT www.em.com.br:443 -
HIER_NONE/- -
1527006834.544      0 ::1 TAG_NONE/503 0 CONNECT www.em.com.br:443 -
HIER_NONE/- -
1527006834.686      0 ::1 TAG_NONE/503 0 CONNECT www.em.com.br:443 -
HIER_NONE/- -
1527006836.382     17 ::1 TAG_NONE/503 0 CONNECT www.uai.com.br:443 -
HIER_NONE/- -
1527006836.494      0 ::1 TAG_NONE/503 0 CONNECT www.uai.com.br:443 -
HIER_NONE/- -

The cache.log show only this:

2018/05/22 13:51:16.555 kid1| 33,2| client_side.cc(817) swanSong:
local=[::1]:3128 remote=[::1]:39166 flags=1
2018/05/22 13:53:28.197 kid1| 33,2| client_side.cc(817) swanSong:
local=[::1]:3128 remote=[::1]:39180 flags=1
2018/05/22 13:53:28.316 kid1| 33,2| client_side.cc(817) swanSong:
local=[::1]:3128 remote=[::1]:39184 flags=1
2018/05/22 13:53:36.972 kid1| 33,2| client_side.cc(817) swanSong:
local=[::1]:3128 remote=[::1]:39188 flags=1
2018/05/22 13:53:37.139 kid1| 33,2| client_side.cc(817) swanSong:
local=[::1]:3128 remote=[::1]:39190 flags=1


# My squid.conf
========================

acl localnet src 10.0.0.0/8
acl SSL_ports port 443
acl Safe_ports port 80
acl Safe_ports port 21
acl Safe_ports port 443
acl Safe_ports port 70
acl Safe_ports port 210
acl Safe_ports port 1025-65535
acl Safe_ports port 280
acl Safe_ports port 488
acl Safe_ports port 591
acl Safe_ports port 777
acl CONNECT method CONNECT

visible_hostname localhost
debug_options ALL, 1 33.2

#Test (putting this did not help at all)
acl no-cache url_regex -i .uai.com.br .em.com.br
cache deny no-cache
acl no-parent url_regex -i .uai.com.br .em.com.br
always_direct allow no-parent

http_access deny! Safe_ports
http_access allow localhost manager
http_access deny manager

http_access allow localnet
http_access allow localhost
http_access deny all
http_port 3128
coredump_dir / var / cache / squid
========================

I already tried using dns_v4_first on/off , via on/off , forwarded_for on/off
I also tried: ssl_bump peek all, ssl_bump splice all or  ssl_bump bump all

And this others configurations:

  pinger_enable off
  half_closed_clients off
  quick_abort_min 0 KB
  quick_abort_max 0 KB
  quick_abort_pct 95
  client_persistent_connections off
  server_persistent_connections off
?
None of this worked. Any tips?


From rousskov at measurement-factory.com  Tue May 22 17:56:40 2018
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Tue, 22 May 2018 11:56:40 -0600
Subject: [squid-users] Squid 3.5 TAG_NONE/503 HIER_NONE
In-Reply-To: <CAMTrDfWcAgbj=jb_4CrbYz-wZ6k=1Y17G=V56rnLE0BH21Uzpg@mail.gmail.com>
References: <CAMTrDfWcAgbj=jb_4CrbYz-wZ6k=1Y17G=V56rnLE0BH21Uzpg@mail.gmail.com>
Message-ID: <bc4348ef-424b-c224-f4a8-269fc8d0c6a5@measurement-factory.com>

On 05/22/2018 10:54 AM, Rejaine Monteiro wrote:

> debug_options ALL, 1 33.2

If the above is your actual debug_options line, then you should file a
bug report that Squid accepts such a bogus debug_options value and then
replace your value with something like "ALL,1" (without quotes). That
may expose more errors or warnings that you are not seeing now.


HTH,

Alex.


From rejaine at bhz.jamef.com.br  Tue May 22 18:30:30 2018
From: rejaine at bhz.jamef.com.br (Rejaine Monteiro)
Date: Tue, 22 May 2018 15:30:30 -0300
Subject: [squid-users] Squid 3.5 TAG_NONE/503 HIER_NONE
In-Reply-To: <bc4348ef-424b-c224-f4a8-269fc8d0c6a5@measurement-factory.com>
References: <CAMTrDfWcAgbj=jb_4CrbYz-wZ6k=1Y17G=V56rnLE0BH21Uzpg@mail.gmail.com>
 <bc4348ef-424b-c224-f4a8-269fc8d0c6a5@measurement-factory.com>
Message-ID: <CAMTrDfUaK8Vwn22_2bsoNd-_dqt2kpkdvcOLKFnHvN0_JOL0NA@mail.gmail.com>

has now worked after inserting this parameter:

dns_v4_first on

the strange thing is that I had tried this parameter before, but it
had not worked (maybe I did something wrong)


2018-05-22 14:56 GMT-03:00 Alex Rousskov <rousskov at measurement-factory.com>:
> On 05/22/2018 10:54 AM, Rejaine Monteiro wrote:
>
>> debug_options ALL, 1 33.2
>
> If the above is your actual debug_options line, then you should file a
> bug report that Squid accepts such a bogus debug_options value and then
> replace your value with something like "ALL,1" (without quotes). That
> may expose more errors or warnings that you are not seeing now.
>
>
> HTH,
>
> Alex.


From andy.roberts at wexinc.co.nz  Tue May 22 21:27:43 2018
From: andy.roberts at wexinc.co.nz (monopot)
Date: Tue, 22 May 2018 14:27:43 -0700 (MST)
Subject: [squid-users] original_dst wrong when using intercept
Message-ID: <1527024463173-0.post@n4.nabble.com>

I'm trying to setup a Squid proxy to use in one of our AWS accounts.   If I
put Squid in with a basic http/https filtering setup (no interception) it
all works great, but the problem is my company wants us to do HTTPS
interception so I need to use ssl bump.

Now if I set Squid up following the AWS guide for SSL numping (
https://aws.amazon.com/blogs/security/how-to-add-dns-filtering-to-your-nat-instance-with-squid/
<https://aws.amazon.com/blogs/security/how-to-add-dns-filtering-to-your-nat-instance-with-squid/> 
) and make the Squid instance have traffic default routed too it, everything
works perfect.  BUT, we can't route direct to it as we already have NAT
Gateways we need to keep, so i need to send traffic to it either direct or
via an ELB and have clients configured to where Squid is.   I thought if I
just take the same instance which works when routed and direct traffic to
it, everything would work but no.

Turns out as soon as I turn on interception such as /http_port 3129
*intercept*/ traffic drops.   In the access log the ORIGINAL_DST is
incorrect with the intercept option in use as it shows the local IP of the
squid proxy.   If I remove intercept it works, shown below.

1527022455.315    178 10.10.7.36 TCP_MISS/503 3944 GET
http://www.google.com/ - ORIGINAL_DST/10.10.3.214 text/html   
*(original_dst is squid local IP, not working)*
1527022535.319    170 10.10.7.36 TCP_MISS/200 12194 GET
http://www.google.com/ - HIER_DIRECT/216.58.196.132 text/html   * (remove
intercept, original_dst is correct and works)*

Any ideas why this is and why it works OK if I route traffic direct and
don't tell the clients specifically to talk to the proxy with
export_httpproxy etc?

My squid.conf is this (no blocking atm)

visible_hostname squid

#Handling HTTP requests
http_port 3129 intercept
http_access allow all

#Handling HTTPS requests
https_port 3130 cert=/etc/squid/ssl/squid.pem ssl-bump intercept
acl SSL_port port 443
http_access allow SSL_port
acl step1 at_step SslBump1
acl step2 at_step SslBump2
acl step3 at_step SslBump3
ssl_bump peek step1 all
ssl_bump peek step2 all
ssl_bump splice step3 all
ssl_bump terminate step2 all

http_access deny all


And my IP tables simply has 

iptables -t nat -A PREROUTING -p tcp --dport 80 -j REDIRECT --to-port 3129
iptables -t nat -A PREROUTING -p tcp --dport 443 -j REDIRECT --to-port 3130


Thanks guys!







--
Sent from: http://squid-web-proxy-cache.1019090.n4.nabble.com/Squid-Users-f1019091.html


From rousskov at measurement-factory.com  Wed May 23 01:36:50 2018
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Tue, 22 May 2018 19:36:50 -0600
Subject: [squid-users] original_dst wrong when using intercept
In-Reply-To: <1527024463173-0.post@n4.nabble.com>
References: <1527024463173-0.post@n4.nabble.com>
Message-ID: <6c92da35-ac57-72b6-9484-f545d0c73093@measurement-factory.com>

On 05/22/2018 03:27 PM, monopot wrote:
> ssl_bump peek step1 all
> ssl_bump peek step2 all
> ssl_bump splice step3 all
> ssl_bump terminate step2 all

Sorry, I cannot answer your primary question, but please note that the
above SslBump configuration is equivalent to

  ssl_bump peek all
  ssl_bump splice all

and your HTTP access rules:

> http_access allow all
> http_access allow SSL_port
> http_access deny all

are equivalent to the (most likely incorrect)

  http_access allow all

With http_access rules, please keep in mind that they are not applied to
each http_port directive. They are interepreted as one set of rules
across the whole configuration.


Cheers,

Alex.


From squid3 at treenet.co.nz  Wed May 23 05:53:29 2018
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Wed, 23 May 2018 17:53:29 +1200
Subject: [squid-users] original_dst wrong when using intercept
In-Reply-To: <1527024463173-0.post@n4.nabble.com>
References: <1527024463173-0.post@n4.nabble.com>
Message-ID: <e0a1bd33-e09b-e530-6206-b86bc4579971@treenet.co.nz>

On 23/05/18 09:27, monopot wrote:
> I'm trying to setup a Squid proxy to use in one of our AWS accounts.   If I
> put Squid in with a basic http/https filtering setup (no interception) it
> all works great, but the problem is my company wants us to do HTTPS
> interception so I need to use ssl bump.
> 
> Now if I set Squid up following the AWS guide for SSL numping (
> https://aws.amazon.com/blogs/security/how-to-add-dns-filtering-to-your-nat-instance-with-squid/
> <https://aws.amazon.com/blogs/security/how-to-add-dns-filtering-to-your-nat-instance-with-squid/> 
> ) and make the Squid instance have traffic default routed too it, everything
> works perfect.  BUT, we can't route direct to it as we already have NAT
> Gateways we need to keep, so i need to send traffic to it either direct or
> via an ELB and have clients configured to where Squid is.   I thought if I
> just take the same instance which works when routed and direct traffic to
> it, everything would work but no.
> 
> Turns out as soon as I turn on interception such as /http_port 3129
> *intercept*/ traffic drops.   In the access log the ORIGINAL_DST is
> incorrect with the intercept option in use as it shows the local IP of the
> squid proxy.   If I remove intercept it works, shown below.

The ORIGINAL_DST is correct. It is the dst-IP of the TCP packets when
that traffic arrived at your Squid machines NAT system.

That can happen due to any of these reasons:

0) you have explicit/forward-proxy traffic arriving at the NAT intercept
port.

1) you have destination-NAT operating no some machine between Squid and
the client.

2) you have destination-NAT operating somewhere in the network *after*
Squid forwarding that traffic back at the Squid machine.

3) you have DNS interception operating on the clients DNS lookups
telling them that your Squid IP is "the origin" for those domains.

4) you have DNS interception operating on Squid DNS lookups telling
Squid itself is the "the origin" for those domains.



> 
> 1527022455.315    178 10.10.7.36 TCP_MISS/503 3944 GET
> http://www.google.com/ - ORIGINAL_DST/10.10.3.214 text/html   
> *(original_dst is squid local IP, not working)*
> 1527022535.319    170 10.10.7.36 TCP_MISS/200 12194 GET
> http://www.google.com/ - HIER_DIRECT/216.58.196.132 text/html   * (remove
> intercept, original_dst is correct and works)*
> 
> Any ideas why this is and why it works OK if I route traffic direct and
> don't tell the clients specifically to talk to the proxy with
> export_httpproxy etc?

The intercept works okay *because* you did not configure the client(s)
that way.

NAT interception MUST only be done on the Squid machine itself. The
clients traffic MUST be going to somewhere other than your Squid when
NAT intercepts it. ORIGINAL_DST will be that other place - origin or
external proxy.

To send different types of traffic to the proxy requires different
listening ports to receive them. One for each type of traffic.
 "HTTP" can be an of three types of traffic - explicit/forward, reverse,
and origin syntax.
 "HTTPS" can be any of those three inside TLS wrapper, plus a number of
non-HTTP protocols.

Your Squid will need:
 - http_port for explicit/forward (port 3128) traffic, and
 - http_port for NAT intercept traffic (port 80), and
 - https_port for NAT intercept HTTPS (port 443) traffic, and
 - (maybe) an https_port for TLS explicit proxy traffic.

When ssl-bump is involved each of the above ports should have it as a
configured option as well as their mode type (no mode flag for the
explicit proxy ports).

Overall if you are in a position to configure the clients to be aware of
the proxy that is the best setup to use. Interception of any type
imposes a lot of major problems to the traffic handling and should be
avoided if possible. Intercept is a last-resort type of traffic handling
for applications which are not properly HTTP compliant.


> 
> My squid.conf is this (no blocking atm)
> 
> visible_hostname squid

Te above should be a FQDN so your clients can fetch and render error
page contents correctly.

Alex has already mentioned the other major issues with this squid.conf
setup so I shall skip them here.


> 
> #Handling HTTP requests
> http_port 3129 intercept
> http_access allow all
> 
> #Handling HTTPS requests
> https_port 3130 cert=/etc/squid/ssl/squid.pem ssl-bump intercept
> acl SSL_port port 443
> http_access allow SSL_port
> acl step1 at_step SslBump1
> acl step2 at_step SslBump2
> acl step3 at_step SslBump3
> ssl_bump peek step1 all
> ssl_bump peek step2 all
> ssl_bump splice step3 all
> ssl_bump terminate step2 all
> 
> http_access deny all
> 
> 
> And my IP tables simply has 
> 
> iptables -t nat -A PREROUTING -p tcp --dport 80 -j REDIRECT --to-port 3129
> iptables -t nat -A PREROUTING -p tcp --dport 443 -j REDIRECT --to-port 3130
> 

see <https://wiki.squid-cache.org/ConfigExamples/Intercept/LinuxRedirect>

Please add the corresponding mangle table rule(s) to prevent case (1),
(3) and (4); and the POSTROUTING nat table rule(s) to prevents case (2).

Amos


From squid3 at treenet.co.nz  Wed May 23 06:11:10 2018
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Wed, 23 May 2018 18:11:10 +1200
Subject: [squid-users] Squid 3.5 TAG_NONE/503 HIER_NONE
In-Reply-To: <CAMTrDfUaK8Vwn22_2bsoNd-_dqt2kpkdvcOLKFnHvN0_JOL0NA@mail.gmail.com>
References: <CAMTrDfWcAgbj=jb_4CrbYz-wZ6k=1Y17G=V56rnLE0BH21Uzpg@mail.gmail.com>
 <bc4348ef-424b-c224-f4a8-269fc8d0c6a5@measurement-factory.com>
 <CAMTrDfUaK8Vwn22_2bsoNd-_dqt2kpkdvcOLKFnHvN0_JOL0NA@mail.gmail.com>
Message-ID: <e59c7919-c686-53c6-01db-e7759811e0ed@treenet.co.nz>

On 23/05/18 06:30, Rejaine Monteiro wrote:
> has now worked after inserting this parameter:
> 
> dns_v4_first on
> 
> the strange thing is that I had tried this parameter before, but it
> had not worked (maybe I did something wrong)
> 

That directive is a workaround for IPv6 or ICMP misconfiguration on your
network or one of those which exist between your Squid and the IPv6
origin servers for those domains. Whether it works or not is dependent
on the state of the Internet - which can be quite volatile.

If you have disabled IPv6 please ensure that your method if disabling is
firewall reject rules with appropriate ICMPv6 responses rather than
kernel hacks or dropping packets. A lot of old texts says things like
disabling kernel modules or interfaces does it - they are wrong. All
that does is break IPv6 on the relevant machine, which can result in
this type of behaviour for software like Squid following IP handling
requirements.

If you have disabled all ICMP traffic on your network please fix that.
ICMP is not an optional protocol and admin blocking it in firewalls can
cause major problems when IPv6 relies on it for routing. Specifically
for MTU detection:
 <https://tools.ietf.org/html/rfc4890>
 <https://sites.google.com/site/ipv6center/icmpv6-is-non-optional>


If you are able to track the IPv6 issue down to a network beyond yours
please help everyone if you can by contacting the relevant admin and
trying to get them to fix their network.

Cheers
Amos


From Sarfaraz.Ahmad at deshaw.com  Wed May 23 07:11:50 2018
From: Sarfaraz.Ahmad at deshaw.com (Ahmad, Sarfaraz)
Date: Wed, 23 May 2018 07:11:50 +0000
Subject: [squid-users] GET requests remain in pending state with Squid and
	Kerberos auth
Message-ID: <135eec8dc0d14b8eae8aab0ecf826309@mbxtoa3.winmail.deshaw.com>

Hi,

I am using Squid as an explicit proxy (configured in the browsers) and have configured it to authenticate all users with Kerberos.
Here are the relevant bits from squid.conf

auth_param negotiate program /usr/lib64/squid/negotiate_kerberos_auth -r -s HTTP/proxytest1.mydomain.com at MYDOMAIN.COM -k /etc/squid/HTTP.keytab
auth_param negotiate children 10
auth_param negotiate keep_alive on

I know I should be expecting 407s for new TCP connections and pages do load a bit slower compared to Basic Auth.
But that isn't the problem. The problem is at times some of the web pages resources (GET requests mostly) just hang there endlessly. (Chrome just says pending)
When I do a refresh, the browser loads that very same resource(say a .js/.css file) just fine.

This is just a test setup and I looked at the negotiate helper stats. Here
   ID #          FD         PID     # Requests          # Replies            # Timed-out       Flags        Time   Offset  Request
      1            13        99482          551                       551                          0                    0.014         0       (none)
      2            17        99485           74          74           0                    0.016         0       (none)
      3            21      106324                           4             4             0                    0.025         0       (none)

I don't think they are the problem.
Any thoughts on what could be going on here? I don't have a way to reproduce this reliably so far and this happens intermittently.

Regards,
Sarfaraz
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20180523/3b0bf7b6/attachment.htm>

From squid3 at treenet.co.nz  Wed May 23 12:18:18 2018
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 24 May 2018 00:18:18 +1200
Subject: [squid-users] GET requests remain in pending state with Squid
 and Kerberos auth
In-Reply-To: <135eec8dc0d14b8eae8aab0ecf826309@mbxtoa3.winmail.deshaw.com>
References: <135eec8dc0d14b8eae8aab0ecf826309@mbxtoa3.winmail.deshaw.com>
Message-ID: <8f99cbfb-06fc-b3c7-15c0-3db6be3b1d83@treenet.co.nz>

On 23/05/18 19:11, Ahmad, Sarfaraz wrote:
> Hi,
> 
> ?
> 
> I am using Squid as an explicit proxy (configured in the browsers) and
> have configured it to authenticate all users with Kerberos.
> 
> Here are the relevant bits from squid.conf
> 
> ?
> 
> auth_param negotiate program /usr/lib64/squid/negotiate_kerberos_auth -r
> -s HTTP/proxytest1.mydomain.com at MYDOMAIN.COM -k /etc/squid/HTTP.keytab
> 
> auth_param negotiate children 10
> 
> auth_param negotiate keep_alive on
> 
> ?
> 
> I know I should be expecting 407s for new TCP connections and pages do
> load a bit slower compared to Basic Auth.

There should be no difference in timing or message status with Kerberos
than with Basic auth.


> 
> But that isn?t the problem. The problem is at times some of the web
> pages resources (GET requests mostly) just hang there endlessly. (Chrome
> just says pending)

How long is this "hang" you mention?
 is there any specific timing to it?
 when it ends which piece of software is terminating the connection from
Browser to Squid?
 Do those transactions actually get sent to Squid? (it may seem dumb,
but Google have invented several protocols of their own which Chrome
uses instead of HTTP to fetch objects.)

Also,

 Do the request messages finish arriving at the proxy end of the
connections?
 Does the domain in the request resolve quickly?
 What does that request message look like on-wire?

Does Squid send anything to a server?
 Does the server respond?
 What does that response message look like on-wire?
 Does that response finish arriving to the proxy?
 Which endpoint on the Squid<->server connection closes it?


You may need to configure "debug_options 11,2" to record the HTTP
message traffic to find the above details.


> 
> When I do a refresh, the browser loads that very same resource(say a
> .js/.css file) just fine.
> 
> ?
> 
> This is just a test setup and I looked at the negotiate helper stats. Here
> 
...
> 
> I don?t think they are the problem.

Agreed, at least from those stats.

> 
> Any thoughts on what could be going on here? I don?t have a way to
> reproduce this reliably so far and this happens intermittently.
> 

You will have to dig a bit further into the traffic chain and hopefully
one of the questions above will lead you to find out what exactly is
hanging. What you have mentioned so far does not contain any clues.


HTH
Amos


From carlos.caballero at cfg.jovenclub.cu  Wed May 23 13:35:08 2018
From: carlos.caballero at cfg.jovenclub.cu (=?UTF-8?Q?Carlos_Cesar_Caballero_D=c3=adaz?=)
Date: Wed, 23 May 2018 09:35:08 -0400
Subject: [squid-users] cache_peer to SSL/TLS proxy
In-Reply-To: <143505f7-9dc0-f457-78fb-baa7a30bd680@treenet.co.nz>
References: <9e776af3-4eea-9841-918a-e548b2f0ed0b@cfg.jovenclub.cu>
 <143505f7-9dc0-f457-78fb-baa7a30bd680@treenet.co.nz>
Message-ID: <3aa3f4ee-3b94-f6e9-4d1a-01c0d9d038c4@cfg.jovenclub.cu>

Thanks @Amos and @Alex, I have been testing and playing with the 
options, but all I get is 502 (Bad Gateway) responses in my local proxy.

Greetings.


El 21/05/18 a las 12:49, Amos Jeffries escribi?:
> On 22/05/18 04:22, Carlos Cesar Caballero D?az wrote:
>>   can I use cache_peer against a
>> parent proxy whish is using SSL/TLS for encrypted browser-squid
>> connection? An if it is possible, How?
>>
> Add the "ssl" (Squid-3) or "tls" (Squid-4) option to your cache_peer
> line and all traffic to that peer will be encrypted with TLS/SSL.
>
> See <http://www.squid-cache.org/Doc/config/cache_peer/> in the section
> called "SSL / HTTPS / TLS OPTIONS" for more options related to securing
> the connection between the proxies.
>   If you have Squid-3 the option names are a bit different, so see the
> doc page for your specific Squid series number.
>
> Amos
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users




From montanezm at gmail.com  Wed May 23 16:25:54 2018
From: montanezm at gmail.com (tstatue)
Date: Wed, 23 May 2018 09:25:54 -0700 (MST)
Subject: [squid-users] multiple user + multiple ips
Message-ID: <1527092754948-0.post@n4.nabble.com>

Hello,

I been searching for few days but can't seem to find the answer - maybe it's
just not possible.

This is what I'm trying to do:

3 users:
user1
user2
user3

9 outgoing ips:
192.168.1.2
192.168.1.3
192.168.1.4
192.168.1.5
192.168.1.6
192.168.1.7
192.168.1.8
192.168.1.9
192.168.1.10

I would like only user1 to be able to login to 192.168.1.2:3128
192.168.1.3:3128 192.168.1.4:3128 and use them as outgoing ips. So if they
login to 192.168.1.2:3128 then 192.168.1.2 will be used as the outgoing IP.

User2 will use 192.168.1.5:3128 192.168.1.6:3128 etc...

Is this possible? 




--
Sent from: http://squid-web-proxy-cache.1019090.n4.nabble.com/Squid-Users-f1019091.html


From rousskov at measurement-factory.com  Wed May 23 18:47:53 2018
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Wed, 23 May 2018 12:47:53 -0600
Subject: [squid-users] multiple user + multiple ips
In-Reply-To: <1527092754948-0.post@n4.nabble.com>
References: <1527092754948-0.post@n4.nabble.com>
Message-ID: <b2edf877-4190-6d78-d174-9bd8ca40c77e@measurement-factory.com>

On 05/23/2018 10:25 AM, tstatue wrote:

> This is what I'm trying to do:
> 
> 3 users:
> user1
> user2
> user3
> 
> 9 outgoing ips:
> 192.168.1.2
> 192.168.1.3
> 192.168.1.4
> 192.168.1.5
> 192.168.1.6
> 192.168.1.7
> 192.168.1.8
> 192.168.1.9
> 192.168.1.10
> 
> I would like only user1 to be able to login to 192.168.1.2:3128
> 192.168.1.3:3128 192.168.1.4:3128 and use them as outgoing ips. So if they
> login to 192.168.1.2:3128 then 192.168.1.2 will be used as the outgoing IP.
> 
> User2 will use 192.168.1.5:3128 192.168.1.6:3128 etc...

Authentication feels orthogonal to outgoing IP usage in this context. I
would treat them as two separate issues.

What seems to be the primary difficulty in your attempts to make this
work? For controlling the outgoing IP address, have you tried a
combination of tcp_outgoing_address with, for example, a myportname ACL?

Alex.


From montanezm at gmail.com  Thu May 24 10:44:39 2018
From: montanezm at gmail.com (tstatue)
Date: Thu, 24 May 2018 03:44:39 -0700 (MST)
Subject: [squid-users] multiple user + multiple ips
In-Reply-To: <b2edf877-4190-6d78-d174-9bd8ca40c77e@measurement-factory.com>
References: <1527092754948-0.post@n4.nabble.com>
 <b2edf877-4190-6d78-d174-9bd8ca40c77e@measurement-factory.com>
Message-ID: <1527158679364-0.post@n4.nabble.com>

Hi Alex,

Thanks for your reply.

Been playing for squid for 10 days, so clear a novice. I can't figure out
how to allow just user1 to connect to 192.168.1.1, currently any user can
login to it.

I have the following in my config but not sure what to add to it to limit it
to a specific user login.

acl myip_192_168_1_1 myip 192.168.1.1
tcp_outgoing_address 192.168.1.1 myip_192_168_1_1



--
Sent from: http://squid-web-proxy-cache.1019090.n4.nabble.com/Squid-Users-f1019091.html


From uhlar at fantomas.sk  Thu May 24 11:25:05 2018
From: uhlar at fantomas.sk (Matus UHLAR - fantomas)
Date: Thu, 24 May 2018 13:25:05 +0200
Subject: [squid-users] multiple user + multiple ips
In-Reply-To: <1527158679364-0.post@n4.nabble.com>
References: <1527092754948-0.post@n4.nabble.com>
 <b2edf877-4190-6d78-d174-9bd8ca40c77e@measurement-factory.com>
 <1527158679364-0.post@n4.nabble.com>
Message-ID: <20180524112505.GA18530@fantomas.sk>

On 24.05.18 03:44, tstatue wrote:
>Been playing for squid for 10 days, so clear a novice. I can't figure out
>how to allow just user1 to connect to 192.168.1.1, currently any user can
>login to it.

>I have the following in my config but not sure what to add to it to limit it
>to a specific user login.
>
>acl myip_192_168_1_1 myip 192.168.1.1
>tcp_outgoing_address 192.168.1.1 myip_192_168_1_1

is 192.168.1.1 on your server?

Note that you can only firewall source IPs to prevent user from connecting.

the user is known only after it connects to your squid, so the only think
you can do is to deny any request to other users who have connected to
192.168.1.1.

-- 
Matus UHLAR - fantomas, uhlar at fantomas.sk ; http://www.fantomas.sk/
Warning: I wish NOT to receive e-mail advertising to this address.
Varovanie: na tuto adresu chcem NEDOSTAVAT akukolvek reklamnu postu.
LSD will make your ECS screen display 16.7 million colors


From aducker at lastline.com  Fri May 25 09:26:42 2018
From: aducker at lastline.com (Ant Ducker)
Date: Fri, 25 May 2018 10:26:42 +0100
Subject: [squid-users] Squid working with HSM
Message-ID: <63071f7a-35c9-000b-1bae-f81fc1437c3a@lastline.com>

Hi all,

I am interested in using an HSM (Hardware Security Module) to secure my 
certificate's private key when using Squid to perform SSL break.

Does anyone have any experience in doing this, and if so, could you give 
me any pointers ?

Thanks,
Ant.


From hugo.saavedra.oteiza at gmail.com  Fri May 25 18:47:31 2018
From: hugo.saavedra.oteiza at gmail.com (Hugo Saavedra)
Date: Fri, 25 May 2018 14:47:31 -0400
Subject: [squid-users] Copying SSL decrypted traffic to virtual interface
Message-ID: <CAL+iNzh-5sXL=8U4W4L_y+QHnnCG+RFCRbwPM1D3EswZHQN5eA@mail.gmail.com>

Hi all,

is there any chance to make a copy of the actual decrypted traffic,
and send it to a kind of virtual ethernet interface?, I want to
analyze this traffic with other tools like BRO IDS or Suricata.
Anyone with some ideas?
Regards,

-- 
Saludos,
Hugo Saavedra


From rousskov at measurement-factory.com  Fri May 25 20:57:46 2018
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Fri, 25 May 2018 14:57:46 -0600
Subject: [squid-users] Copying SSL decrypted traffic to virtual interface
In-Reply-To: <CAL+iNzh-5sXL=8U4W4L_y+QHnnCG+RFCRbwPM1D3EswZHQN5eA@mail.gmail.com>
References: <CAL+iNzh-5sXL=8U4W4L_y+QHnnCG+RFCRbwPM1D3EswZHQN5eA@mail.gmail.com>
Message-ID: <5fcca72a-268a-f30a-29ab-9f2707f8bca4@measurement-factory.com>

On 05/25/2018 12:47 PM, Hugo Saavedra wrote:

> is there any chance to make a copy of the actual decrypted traffic,
> and send it to a kind of virtual ethernet interface?, I want to
> analyze this traffic with other tools like BRO IDS or Suricata.

Yes, this can be (and has been) done using ICAP or eCAP services. Those
services receive decrypted traffic and can emulate a rough equivalent of
the original HTTP traffic (without the encryption), directing that
traffic at the external analysis tools. For those external tools, the
traffic will look like ordinary plain HTTP.

Alex.


From willsznet at gmail.com  Sat May 26 19:21:52 2018
From: willsznet at gmail.com (Willsz CS)
Date: Sun, 27 May 2018 02:21:52 +0700
Subject: [squid-users] Squid display garbage character.
Message-ID: <CAMby-etpT8YvEwvuk3oe5uK-722Q_CFL7JBhpysogfccD=Hteg@mail.gmail.com>

Hi,

I'am running squid with transparent mode, browsing several site from client
something like www.cintapoker88.com, but get strange garbage character (
https://ibb.co/kcEGfo for clearly picture). I guessing my ISP running
transparent proxy too for censorship. For temporary solution is
unredirection HTTP rule to Squid. Anyone help me for clue this problem?

Thank you.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20180527/8be57927/attachment.htm>

From squid3 at treenet.co.nz  Sat May 26 20:43:11 2018
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sun, 27 May 2018 08:43:11 +1200
Subject: [squid-users] Squid display garbage character.
In-Reply-To: <CAMby-etpT8YvEwvuk3oe5uK-722Q_CFL7JBhpysogfccD=Hteg@mail.gmail.com>
References: <CAMby-etpT8YvEwvuk3oe5uK-722Q_CFL7JBhpysogfccD=Hteg@mail.gmail.com>
Message-ID: <469119c5-ac02-0676-bc86-ae5385296caf@treenet.co.nz>

On 27/05/18 07:21, Willsz CS wrote:
> Hi,
> 
> I'am running squid with transparent mode, browsing several site from
> client something like www.cintapoker88.com
> <http://www.cintapoker88.com>, but get strange garbage character
> (https://ibb.co/kcEGfo for clearly picture). I guessing my ISP running
> transparent proxy too for censorship. For temporary solution is
> unredirection HTTP rule to Squid.?Anyone help me for clue this problem?
> 

This display happens when the browser is being told the response object
is or one type (eg HTML/XML text), but it is actually binary content (eg
an image, or compressed object). Usually when fetching the main HTML
index object, images, or video content - things which are displayed
directly to the user.


It usually occurs because:

A) an admin forces things to be cached by a proxy and served from cache
despite instructions from the website author on the HTTP response that
caching is not permitted for that object.
  - check your squid.conf for any refresh_pattern directives with
override-* or ignore-* options which might be forcing things to be
cached when they are not supposed to. BE VERY careful and conservative
with your use of regex patterns.
  - current Squid versions "squid -k parse" command should provide you
nice loud WARNING messages about any of these options if it is possibly
going to cause the types of issue you can see. The text will mention
violating HTTP and you being responsible for the issues cased (if any).

or,

B) the web server is not sending Vary headers consistently and the proxy
cache ends up thinking that a compressed object is possible to be sent
to clients only accepting plain-text objects.
 - this one there is not much you can do except to prevent these
particular URLs not to be cashed by your own proxy.


Both cases result in the client sometimes displaying binary octets as if
they were plain text, as you can see in that demo image. More often they
occur with scripts and nothing gets displayed - just parts of the
website dont work properly (no scrolling, missing content, or
unclickable buttons, etc).


The tool at
(<https://redbot.org/?descend=True&uri=http://www.cintapoker88.com/&check_name=default>)
indicates that some (but only a very few) of the javascripts being used
by that website have Vary header issues. The images pulled from Imgur
have some serious problems

So it depends on exactly which URL(s) you are having trouble with. You
can plug them into that redbot tool to get an analysis of particular URL.


Also, it would help to know which Squid version you are using and which
OS distro you use it on. If you are unsure of your regex_pattern rules
please post your squid.conf and we can assist pointing out any
unsuspected issues there.


HTH
Amos


From squid3 at treenet.co.nz  Sun May 27 06:36:08 2018
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sun, 27 May 2018 18:36:08 +1200
Subject: [squid-users] Squid working with HSM
In-Reply-To: <63071f7a-35c9-000b-1bae-f81fc1437c3a@lastline.com>
References: <63071f7a-35c9-000b-1bae-f81fc1437c3a@lastline.com>
Message-ID: <9ad9beb6-b15f-4bbc-20fa-0d5a5b6f890d@treenet.co.nz>

On 25/05/18 21:26, Ant Ducker wrote:
> Hi all,
> 
> I am interested in using an HSM (Hardware Security Module) to secure my
> certificate's private key when using Squid to perform SSL break.
> 
> Does anyone have any experience in doing this, and if so, could you give
> me any pointers ?

( NOTE: I have not done this myself, so this is just a "maybe" - if
anyone else has more direct knowledge for your situation go with that. )


If there is a password (or HSM token used as password?) needed for
access to the key file(s) you can configure a helper script in the
sslpassword_program directive to give Squid that password.
 <http://www.squid-cache.org/Doc/config/sslpassword_program/>

AFAIK, this helper is a bit special in that it is expected only to
provide the password and exit. Other helpers must run constantly.

Also if the HSM requires any special way to access the keying material
than password protection on the key file it is probably a matter for the
openssl config instead of Squid.

Amos


From willsznet at gmail.com  Sun May 27 07:20:15 2018
From: willsznet at gmail.com (Willsz.net Support)
Date: Sun, 27 May 2018 14:20:15 +0700
Subject: [squid-users] Squid display garbage character.
In-Reply-To: <469119c5-ac02-0676-bc86-ae5385296caf@treenet.co.nz>
References: <CAMby-etpT8YvEwvuk3oe5uK-722Q_CFL7JBhpysogfccD=Hteg@mail.gmail.com>
 <469119c5-ac02-0676-bc86-ae5385296caf@treenet.co.nz>
Message-ID: <004001d3f58b$2a5fb150$7f1f13f0$@com>

> This display happens when the browser is being told the response object
> is or one type (eg HTML/XML text), but it is actually binary content (eg
> an image, or compressed object). Usually when fetching the main HTML
> index object, images, or video content - things which are displayed
> directly to the user.
> 
> 
> It usually occurs because:
> 
> A) an admin forces things to be cached by a proxy and served from cache
> despite instructions from the website author on the HTTP response that
> caching is not permitted for that object.
>   - check your squid.conf for any refresh_pattern directives with
> override-* or ignore-* options which might be forcing things to be
> cached when they are not supposed to. BE VERY careful and conservative
> with your use of regex patterns.
>   - current Squid versions "squid -k parse" command should provide you
> nice loud WARNING messages about any of these options if it is possibly
> going to cause the types of issue you can see. The text will mention
> violating HTTP and you being responsible for the issues cased (if any).

Thank you, Amos

Not much modification in my Squid.conf, this output of squid -k parse:

root:~# squid -k parse
2018/05/27 13:53:42| Startup: Initializing Authentication Schemes ...
2018/05/27 13:53:42| Startup: Initialized Authentication Scheme 'basic'
2018/05/27 13:53:42| Startup: Initialized Authentication Scheme 'digest'
2018/05/27 13:53:42| Startup: Initialized Authentication Scheme 'ntlm'
2018/05/27 13:53:42| Startup: Initialized Authentication.
2018/05/27 13:53:42| Processing Configuration File: /usr/local/etc/squid/squid.conf (depth 0)
2018/05/27 13:53:42| Processing: acl proxyserv dst 192.168.100.250
2018/05/27 13:53:42| Processing: acl pccl03  src 192.168.100.3/32
2018/05/27 13:53:42| Processing: acl pccl04  src 192.168.100.4/32
2018/05/27 13:53:42| Processing: acl pccl05  src 192.168.100.5/32
2018/05/27 13:53:42| Processing: acl pccl08  src 192.168.100.8/32
2018/05/27 13:53:42| Processing: acl pccl22  src 192.168.100.22/32
2018/05/27 13:53:42| Processing: acl pccl23  src 192.168.100.23/32
2018/05/27 13:53:42| Processing: acl pccl24  src 192.168.100.24/32
2018/05/27 13:53:42| Processing: acl pccl25  src 192.168.100.25/32
2018/05/27 13:53:42| Processing: acl pccl26  src 192.168.100.26/32
2018/05/27 13:53:42| Processing: acl tvbox   src 192.168.100.50/32
2018/05/27 13:53:42| Processing: acl wicl80  src 192.168.100.80/32
2018/05/27 13:53:42| Processing: acl wicl81  src 192.168.100.81/32
2018/05/27 13:53:42| Processing: acl wicl82  src 192.168.100.82/32
2018/05/27 13:53:42| Processing: acl wicl83  src 192.168.100.83/32
2018/05/27 13:53:42| Processing: acl wicl84  src 192.168.100.84/32
2018/05/27 13:53:42| Processing: acl wicl85  src 192.168.100.85/32
2018/05/27 13:53:42| Processing: acl wicl86  src 192.168.100.86/32
2018/05/27 13:53:42| Processing: acl wicl87  src 192.168.100.87/32
2018/05/27 13:53:42| Processing: acl wicl88  src 192.168.100.88/32
2018/05/27 13:53:42| Processing: acl wicl89  src 192.168.100.89/32
2018/05/27 13:53:42| Processing: acl wicl90  src 192.168.100.90/32
2018/05/27 13:53:42| Processing: acl wicl91  src 192.168.100.91/32
2018/05/27 13:53:42| Processing: acl wicl92  src 192.168.100.92/32
2018/05/27 13:53:42| Processing: acl wicl93  src 192.168.100.93/32
2018/05/27 13:53:42| Processing: acl wicl94  src 192.168.100.94/32
2018/05/27 13:53:42| Processing: acl wicl95  src 192.168.100.95/32
2018/05/27 13:53:42| Processing: acl wicl96  src 192.168.100.96/32
2018/05/27 13:53:42| Processing: acl wicl97  src 192.168.100.97/32
2018/05/27 13:53:42| Processing: acl wicl98  src 192.168.100.98/32
2018/05/27 13:53:42| Processing: acl wicl99  src 192.168.100.99/32
2018/05/27 13:53:42| Processing: acl pcbill  src 192.168.100.100/32
2018/05/27 13:53:42| Processing: acl pchome  src 192.168.100.101/32
2018/05/27 13:53:42| Processing: acl domaindeny dstdom_regex -i "/usr/local/etc/squid/domain.deny"
2018/05/27 13:53:42| Processing: acl domainrdr dstdom_regex -i "/usr/local/etc/squid/domain.rdr"
2018/05/27 13:53:42| Processing: acl ipaddrdeny dst -n "/usr/local/etc/squid/ipaddr.deny"
2018/05/27 13:53:42| Processing: acl urlpathdeny urlpath_regex -i "/usr/local/etc/squid/urlpath.deny"
2018/05/27 13:53:42| Processing: acl windowsupdate dstdom_regex -i download\.windowsupdate\.com
2018/05/27 13:53:42| Processing: acl domainnocache dstdomain .garenanow.com .garena.co.id
2018/05/27 13:53:42| Processing: deny_info 302:http://ip.fo-ont-lo.willsz.net/null.png ipaddrdeny domaindeny urlpathdeny windowsupdate
2018/05/27 13:53:42| Processing: deny_info 302:http://unyil.willsz.net/index.html domainrdr
2018/05/27 13:53:42| Processing: always_direct allow domainnocache
2018/05/27 13:53:42| Processing: cache deny domainnocache
2018/05/27 13:53:42| Processing: acl SSL_ports port 443
2018/05/27 13:53:42| Processing: acl Safe_ports port 80
2018/05/27 13:53:42| Processing: acl Safe_ports port 8080
2018/05/27 13:53:42| Processing: acl Safe_ports port 8081
2018/05/27 13:53:42| Processing: acl CONNECT method CONNECT
2018/05/27 13:53:42| Processing: http_access deny !Safe_ports
2018/05/27 13:53:42| Processing: http_access deny CONNECT !SSL_ports
2018/05/27 13:53:42| Processing: http_access deny domaindeny
2018/05/27 13:53:42| Processing: http_access deny domainrdr
2018/05/27 13:53:42| Processing: http_access deny ipaddrdeny
2018/05/27 13:53:42| Processing: http_access deny urlpathdeny
2018/05/27 13:53:42| Processing: http_access allow wicl80 windowsupdate
2018/05/27 13:53:42| Processing: http_access allow wicl81 windowsupdate
2018/05/27 13:53:42| Processing: http_access allow wicl82 windowsupdate
2018/05/27 13:53:42| Processing: http_access allow wicl83 windowsupdate
2018/05/27 13:53:42| Processing: http_access allow wicl84 windowsupdate
2018/05/27 13:53:42| Processing: http_access allow wicl85 windowsupdate
2018/05/27 13:53:42| Processing: http_access allow wicl86 windowsupdate
2018/05/27 13:53:42| Processing: http_access allow wicl87 windowsupdate
2018/05/27 13:53:42| Processing: http_access allow wicl88 windowsupdate
2018/05/27 13:53:42| Processing: http_access allow wicl89 windowsupdate
2018/05/27 13:53:42| Processing: http_access allow wicl90 windowsupdate
2018/05/27 13:53:42| Processing: http_access allow wicl91 windowsupdate
2018/05/27 13:53:42| Processing: http_access allow wicl92 windowsupdate
2018/05/27 13:53:42| Processing: http_access allow wicl93 windowsupdate
2018/05/27 13:53:42| Processing: http_access allow wicl94 windowsupdate
2018/05/27 13:53:42| Processing: http_access allow wicl95 windowsupdate
2018/05/27 13:53:42| Processing: http_access allow wicl96 windowsupdate
2018/05/27 13:53:42| Processing: http_access allow wicl97 windowsupdate
2018/05/27 13:53:42| Processing: http_access allow wicl98 windowsupdate
2018/05/27 13:53:42| Processing: http_access allow wicl99 windowsupdate
2018/05/27 13:53:42| Processing: http_access allow pcbill windowsupdate
2018/05/27 13:53:42| Processing: http_access deny windowsupdate
2018/05/27 13:53:42| Processing: http_access allow proxyserv
2018/05/27 13:53:42| Processing: http_access allow pccl03
2018/05/27 13:53:42| Processing: http_access allow pccl04
2018/05/27 13:53:42| Processing: http_access allow pccl05
2018/05/27 13:53:42| Processing: http_access allow pccl08
2018/05/27 13:53:42| Processing: http_access allow pccl22
2018/05/27 13:53:42| Processing: http_access allow pccl23
2018/05/27 13:53:42| Processing: http_access allow pccl24
2018/05/27 13:53:42| Processing: http_access allow pccl25
2018/05/27 13:53:42| Processing: http_access allow pccl26
2018/05/27 13:53:42| Processing: http_access allow tvbox
2018/05/27 13:53:42| Processing: http_access allow wicl80
2018/05/27 13:53:42| Processing: http_access allow wicl81
2018/05/27 13:53:42| Processing: http_access allow wicl82
2018/05/27 13:53:42| Processing: http_access allow wicl83
2018/05/27 13:53:42| Processing: http_access allow wicl84
2018/05/27 13:53:42| Processing: http_access allow wicl85
2018/05/27 13:53:42| Processing: http_access allow wicl86
2018/05/27 13:53:42| Processing: http_access allow wicl87
2018/05/27 13:53:42| Processing: http_access allow wicl88
2018/05/27 13:53:42| Processing: http_access allow wicl89
2018/05/27 13:53:42| Processing: http_access allow wicl90
2018/05/27 13:53:42| Processing: http_access allow wicl91
2018/05/27 13:53:42| Processing: http_access allow wicl92
2018/05/27 13:53:42| Processing: http_access allow wicl93
2018/05/27 13:53:42| Processing: http_access allow wicl94
2018/05/27 13:53:42| Processing: http_access allow wicl95
2018/05/27 13:53:42| Processing: http_access allow wicl96
2018/05/27 13:53:42| Processing: http_access allow wicl97
2018/05/27 13:53:42| Processing: http_access allow wicl98
2018/05/27 13:53:42| Processing: http_access allow wicl99
2018/05/27 13:53:42| Processing: http_access allow pcbill
2018/05/27 13:53:42| Processing: http_access allow pchome
2018/05/27 13:53:42| Processing: http_access deny all
2018/05/27 13:53:42| Processing: always_direct deny all
2018/05/27 13:53:42| Processing: icp_port 0
2018/05/27 13:53:42| Processing: icp_access deny all
2018/05/27 13:53:42| Processing: http_port 127.0.0.1:3128
2018/05/27 13:53:42| Processing: http_port 192.168.100.250:7080 transparent
2018/05/27 13:53:42| Starting Authentication on port 192.168.100.250:7080
2018/05/27 13:53:42| Disabling Authentication on port 192.168.100.250:7080 (interception enabled)
2018/05/27 13:53:42| Processing: cache_mem 32 MB
2018/05/27 13:53:42| Processing: cache_swap_low 90
2018/05/27 13:53:42| Processing: cache_swap_high 95
2018/05/27 13:53:42| Processing: cache_dir ufs /var/cache/squid 2048 16 256
2018/05/27 13:53:42| Processing: store_dir_select_algorithm round-robin
2018/05/27 13:53:42| Processing: access_log daemon:/var/log/squid/access.log squid
2018/05/27 13:53:42| Processing: cache_log /var/log/squid/cache.log
2018/05/27 13:53:42| Processing: cache_store_log none
2018/05/27 13:53:42| Processing: pid_filename /var/run/squid.pid
2018/05/27 13:53:42| Processing: logfile_rotate 1
2018/05/27 13:53:42| Processing: log_icp_queries off
2018/05/27 13:53:42| Processing: buffered_logs off
2018/05/27 13:53:42| Processing: minimum_object_size 0 KB
2018/05/27 13:53:42| Processing: maximum_object_size 10 MB
2018/05/27 13:53:42| Processing: refresh_pattern -i (/cgi-bin/|\?) 0 0%  0
2018/05/27 13:53:42| Processing: refresh_pattern .       0   100% 10080
2018/05/27 13:53:42| Processing: memory_replacement_policy heap GDSF
2018/05/27 13:53:42| Processing: cache_replacement_policy heap LFUDA
2018/05/27 13:53:42| Processing: shutdown_lifetime 5 seconds
2018/05/27 13:53:42| Processing: half_closed_clients off
2018/05/27 13:53:42| Processing: client_persistent_connections off
2018/05/27 13:53:42| Processing: server_persistent_connections on
2018/05/27 13:53:42| Processing: pconn_timeout 15 seconds
2018/05/27 13:53:42| Processing: request_timeout 1 minute
2018/05/27 13:53:42| Processing: tcp_outgoing_tos 0x30 all
2018/05/27 13:53:42| Processing: retry_on_error on
2018/05/27 13:53:42| Processing: buffered_logs on
2018/05/27 13:53:42| Processing: global_internal_static off
2018/05/27 13:53:42| Processing: max_stale 10 years
2018/05/27 13:53:42| Processing: quick_abort_min -1 KB
2018/05/27 13:53:42| Processing: vary_ignore_expire on
2018/05/27 13:53:42| Processing: ie_refresh on
2018/05/27 13:53:42| Processing: cache_mgr cachemaster at willsz.net
2018/05/27 13:53:42| Processing: visible_hostname ip.proxy-cache.willsz.net
2018/05/27 13:53:42| Processing: cache_effective_user squid
2018/05/27 13:53:42| Processing: cache_effective_group squid
2018/05/27 13:53:42| Processing: check_hostnames on
2018/05/27 13:53:42| Processing: dns_retransmit_interval 2 seconds
2018/05/27 13:53:42| Processing: dns_timeout 1 minutes
2018/05/27 13:53:42| Processing: memory_pools off
2018/05/27 13:53:42| Processing: forwarded_for off
2018/05/27 13:53:42| Processing: client_persistent_connections on
2018/05/27 13:53:42| Processing: coredump_dir /tmp
2018/05/27 13:53:42| Processing: httpd_suppress_version_string on
  
> B) the web server is not sending Vary headers consistently and the proxy
> cache ends up thinking that a compressed object is possible to be sent
> to clients only accepting plain-text objects.
>  - this one there is not much you can do except to prevent these
> particular URLs not to be cashed by your own proxy.
> Both cases result in the client sometimes displaying binary octets as if
> they were plain text, as you can see in that demo image. More often they
> occur with scripts and nothing gets displayed - just parts of the
> website dont work properly (no scrolling, missing content, or
> unclickable buttons, etc).
 
I am more suspicious because of this, many of gambling site hosting will get the same problem.
I tested for http://www.bolaliga88.com/ with the same result https://redbot.org/?uri=http%3A%2F%2Fwww.bolaliga88.com%2F  
 
> The tool at
> (<https://redbot.org/?descend=True&uri=http://www.cintapoker88.com/&check_nam
> e=default>)
> indicates that some (but only a very few) of the javascripts being used
> by that website have Vary header issues. The images pulled from Imgur
> have some serious problems
> 
> So it depends on exactly which URL(s) you are having trouble with. You
> can plug them into that redbot tool to get an analysis of particular URL.
> 
> 
> Also, it would help to know which Squid version you are using and which
> OS distro you use it on. If you are unsure of your regex_pattern rules
> please post your squid.conf and we can assist pointing out any
> unsuspected issues there.

Sorry, I forgot for this. I compile squid from source under FreeBSD. 

root:~# uname -mrs
FreeBSD 10.4-STABLE i386

root:~# squid -v
Squid Cache: Version 3.5.27
Service Name: squid
configure options:  '--prefix=/usr/local' '--datadir=/usr/local/etc/squid' '--includedir=/usr/local/include' '--bindir=/usr/local/sbin' '--libexecdir=/usr/local/libexec/squid' '--sysconfdir=/usr/local/etc/squid' '--with-default-user=squid' '--localstatedir=/var/cache/squid' '--libdir=/usr/local/lib' '--with-logdir=/var/log/squid' '--with-pidfile=/var/run/squid.pid' '--with-swapdir=/var/cache/squid' '--without-gnutls' '--enable-build-info' '--enable-loadable-modules' '--enable-removal-policies=lru,heap' '--disable-epoll' '--disable-linux-netfilter' '--disable-linux-tproxy' '--disable-translation' '--disable-arch-native' '--mandir=/usr/local/man' '--infodir=/usr/local/info' '--disable-wccp' '--disable-wccpv2' '--enable-ipfw-transparent' '--with-large-files' '--disable-htcp' '--disable-eui' '--enable-cachemgr-hostname=ip.proxy-cache.willsz.net' '--disable-auth-negotiate' '--without-mit-krb5' '--without-heimdal-krb5'

Thank you for explanation, Amos.



From squid3 at treenet.co.nz  Sun May 27 08:30:13 2018
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sun, 27 May 2018 20:30:13 +1200
Subject: [squid-users] Squid display garbage character.
In-Reply-To: <004001d3f58b$2a5fb150$7f1f13f0$@com>
References: <CAMby-etpT8YvEwvuk3oe5uK-722Q_CFL7JBhpysogfccD=Hteg@mail.gmail.com>
 <469119c5-ac02-0676-bc86-ae5385296caf@treenet.co.nz>
 <004001d3f58b$2a5fb150$7f1f13f0$@com>
Message-ID: <79a778f1-dd21-44ae-e91d-b7efd6f3b1a0@treenet.co.nz>



On 27/05/18 19:20, Willsz.net Support wrote:
>> This display happens when the browser is being told the response object
>> is or one type (eg HTML/XML text), but it is actually binary content (eg
>> an image, or compressed object). Usually when fetching the main HTML
>> index object, images, or video content - things which are displayed
>> directly to the user.
>>
>>
>> It usually occurs because:
>>
>> A) an admin forces things to be cached by a proxy and served from cache
>> despite instructions from the website author on the HTTP response that
>> caching is not permitted for that object.
>>   - check your squid.conf for any refresh_pattern directives with
>> override-* or ignore-* options which might be forcing things to be
>> cached when they are not supposed to. BE VERY careful and conservative
>> with your use of regex patterns.
>>   - current Squid versions "squid -k parse" command should provide you
>> nice loud WARNING messages about any of these options if it is possibly
>> going to cause the types of issue you can see. The text will mention
>> violating HTTP and you being responsible for the issues cased (if any).
> 
> Thank you, Amos
> 
> Not much modification in my Squid.conf, this output of squid -k parse:
> 
> root:~# squid -k parse
> 2018/05/27 13:53:42| Startup: Initializing Authentication Schemes ...
> 2018/05/27 13:53:42| Startup: Initialized Authentication Scheme 'basic'
> 2018/05/27 13:53:42| Startup: Initialized Authentication Scheme 'digest'
> 2018/05/27 13:53:42| Startup: Initialized Authentication Scheme 'ntlm'
> 2018/05/27 13:53:42| Startup: Initialized Authentication.
> 2018/05/27 13:53:42| Processing Configuration File: /usr/local/etc/squid/squid.conf (depth 0)
> 2018/05/27 13:53:42| Processing: acl proxyserv dst 192.168.100.250
> 2018/05/27 13:53:42| Processing: acl pccl03  src 192.168.100.3/32
> 2018/05/27 13:53:42| Processing: acl pccl04  src 192.168.100.4/32
> 2018/05/27 13:53:42| Processing: acl pccl05  src 192.168.100.5/32
> 2018/05/27 13:53:42| Processing: acl pccl08  src 192.168.100.8/32
> 2018/05/27 13:53:42| Processing: acl pccl22  src 192.168.100.22/32
> 2018/05/27 13:53:42| Processing: acl pccl23  src 192.168.100.23/32
> 2018/05/27 13:53:42| Processing: acl pccl24  src 192.168.100.24/32
> 2018/05/27 13:53:42| Processing: acl pccl25  src 192.168.100.25/32
> 2018/05/27 13:53:42| Processing: acl pccl26  src 192.168.100.26/32
> 2018/05/27 13:53:42| Processing: acl tvbox   src 192.168.100.50/32

> 2018/05/27 13:53:42| Processing: acl wicl80  src 192.168.100.80/32
> 2018/05/27 13:53:42| Processing: acl wicl81  src 192.168.100.81/32
> 2018/05/27 13:53:42| Processing: acl wicl82  src 192.168.100.82/32
> 2018/05/27 13:53:42| Processing: acl wicl83  src 192.168.100.83/32
> 2018/05/27 13:53:42| Processing: acl wicl84  src 192.168.100.84/32
> 2018/05/27 13:53:42| Processing: acl wicl85  src 192.168.100.85/32
> 2018/05/27 13:53:42| Processing: acl wicl86  src 192.168.100.86/32
> 2018/05/27 13:53:42| Processing: acl wicl87  src 192.168.100.87/32
> 2018/05/27 13:53:42| Processing: acl wicl88  src 192.168.100.88/32
> 2018/05/27 13:53:42| Processing: acl wicl89  src 192.168.100.89/32
> 2018/05/27 13:53:42| Processing: acl wicl90  src 192.168.100.90/32
> 2018/05/27 13:53:42| Processing: acl wicl91  src 192.168.100.91/32
> 2018/05/27 13:53:42| Processing: acl wicl92  src 192.168.100.92/32
> 2018/05/27 13:53:42| Processing: acl wicl93  src 192.168.100.93/32
> 2018/05/27 13:53:42| Processing: acl wicl94  src 192.168.100.94/32
> 2018/05/27 13:53:42| Processing: acl wicl95  src 192.168.100.95/32
> 2018/05/27 13:53:42| Processing: acl wicl96  src 192.168.100.96/32
> 2018/05/27 13:53:42| Processing: acl wicl97  src 192.168.100.97/32
> 2018/05/27 13:53:42| Processing: acl wicl98  src 192.168.100.98/32
> 2018/05/27 13:53:42| Processing: acl wicl99  src 192.168.100.99/32
> 2018/05/27 13:53:42| Processing: acl pcbill  src 192.168.100.100/32


There is nothing special being done for all the above wicl80 - wicl99
and pcbill ACLs. They are all being either allowed or denied by rules at
the same time(s). So it seems a pointless waste of config lines.

You could replace wicl80 with:
 acl wicl80-99 src 192.168.100.80-192.168.100.100

then delete all config lines mentioning wicl81 thru wicl99, and pcbill.


> 2018/05/27 13:53:42| Processing: acl pchome  src 192.168.100.101/32
> 2018/05/27 13:53:42| Processing: acl domaindeny dstdom_regex -i "/usr/local/etc/squid/domain.deny"
> 2018/05/27 13:53:42| Processing: acl domainrdr dstdom_regex -i "/usr/local/etc/squid/domain.rdr"
> 2018/05/27 13:53:42| Processing: acl ipaddrdeny dst -n "/usr/local/etc/squid/ipaddr.deny"
> 2018/05/27 13:53:42| Processing: acl urlpathdeny urlpath_regex -i "/usr/local/etc/squid/urlpath.deny"
> 2018/05/27 13:53:42| Processing: acl windowsupdate dstdom_regex -i download\.windowsupdate\.com
> 2018/05/27 13:53:42| Processing: acl domainnocache dstdomain .garenanow.com .garena.co.id
> 2018/05/27 13:53:42| Processing: deny_info 302:http://ip.fo-ont-lo.willsz.net/null.png ipaddrdeny domaindeny urlpathdeny windowsupdate
> 2018/05/27 13:53:42| Processing: deny_info 302:http://unyil.willsz.net/index.html domainrdr
> 2018/05/27 13:53:42| Processing: always_direct allow domainnocache

You do not have any cache_peer configured. So always_direct has no meaning.

> 2018/05/27 13:53:42| Processing: cache deny domainnocache
> 2018/05/27 13:53:42| Processing: acl SSL_ports port 443
> 2018/05/27 13:53:42| Processing: acl Safe_ports port 80
> 2018/05/27 13:53:42| Processing: acl Safe_ports port 8080
> 2018/05/27 13:53:42| Processing: acl Safe_ports port 8081
> 2018/05/27 13:53:42| Processing: acl CONNECT method CONNECT
> 2018/05/27 13:53:42| Processing: http_access deny !Safe_ports
> 2018/05/27 13:53:42| Processing: http_access deny CONNECT !SSL_ports
> 2018/05/27 13:53:42| Processing: http_access deny domaindeny
> 2018/05/27 13:53:42| Processing: http_access deny domainrdr
> 2018/05/27 13:53:42| Processing: http_access deny ipaddrdeny
> 2018/05/27 13:53:42| Processing: http_access deny urlpathdeny
> 2018/05/27 13:53:42| Processing: http_access allow wicl80 windowsupdate
> 2018/05/27 13:53:42| Processing: http_access allow wicl81 windowsupdate
> 2018/05/27 13:53:42| Processing: http_access allow wicl82 windowsupdate
> 2018/05/27 13:53:42| Processing: http_access allow wicl83 windowsupdate
> 2018/05/27 13:53:42| Processing: http_access allow wicl84 windowsupdate
> 2018/05/27 13:53:42| Processing: http_access allow wicl85 windowsupdate
> 2018/05/27 13:53:42| Processing: http_access allow wicl86 windowsupdate
> 2018/05/27 13:53:42| Processing: http_access allow wicl87 windowsupdate
> 2018/05/27 13:53:42| Processing: http_access allow wicl88 windowsupdate
> 2018/05/27 13:53:42| Processing: http_access allow wicl89 windowsupdate
> 2018/05/27 13:53:42| Processing: http_access allow wicl90 windowsupdate
> 2018/05/27 13:53:42| Processing: http_access allow wicl91 windowsupdate
> 2018/05/27 13:53:42| Processing: http_access allow wicl92 windowsupdate
> 2018/05/27 13:53:42| Processing: http_access allow wicl93 windowsupdate
> 2018/05/27 13:53:42| Processing: http_access allow wicl94 windowsupdate
> 2018/05/27 13:53:42| Processing: http_access allow wicl95 windowsupdate
> 2018/05/27 13:53:42| Processing: http_access allow wicl96 windowsupdate
> 2018/05/27 13:53:42| Processing: http_access allow wicl97 windowsupdate
> 2018/05/27 13:53:42| Processing: http_access allow wicl98 windowsupdate
> 2018/05/27 13:53:42| Processing: http_access allow wicl99 windowsupdate
> 2018/05/27 13:53:42| Processing: http_access allow pcbill windowsupdate
> 2018/05/27 13:53:42| Processing: http_access deny windowsupdate
> 2018/05/27 13:53:42| Processing: http_access allow proxyserv
> 2018/05/27 13:53:42| Processing: http_access allow pccl03
> 2018/05/27 13:53:42| Processing: http_access allow pccl04
> 2018/05/27 13:53:42| Processing: http_access allow pccl05
> 2018/05/27 13:53:42| Processing: http_access allow pccl08
> 2018/05/27 13:53:42| Processing: http_access allow pccl22
> 2018/05/27 13:53:42| Processing: http_access allow pccl23
> 2018/05/27 13:53:42| Processing: http_access allow pccl24
> 2018/05/27 13:53:42| Processing: http_access allow pccl25
> 2018/05/27 13:53:42| Processing: http_access allow pccl26
> 2018/05/27 13:53:42| Processing: http_access allow tvbox
> 2018/05/27 13:53:42| Processing: http_access allow wicl80
> 2018/05/27 13:53:42| Processing: http_access allow wicl81
> 2018/05/27 13:53:42| Processing: http_access allow wicl82
> 2018/05/27 13:53:42| Processing: http_access allow wicl83
> 2018/05/27 13:53:42| Processing: http_access allow wicl84
> 2018/05/27 13:53:42| Processing: http_access allow wicl85
> 2018/05/27 13:53:42| Processing: http_access allow wicl86
> 2018/05/27 13:53:42| Processing: http_access allow wicl87
> 2018/05/27 13:53:42| Processing: http_access allow wicl88
> 2018/05/27 13:53:42| Processing: http_access allow wicl89
> 2018/05/27 13:53:42| Processing: http_access allow wicl90
> 2018/05/27 13:53:42| Processing: http_access allow wicl91
> 2018/05/27 13:53:42| Processing: http_access allow wicl92
> 2018/05/27 13:53:42| Processing: http_access allow wicl93
> 2018/05/27 13:53:42| Processing: http_access allow wicl94
> 2018/05/27 13:53:42| Processing: http_access allow wicl95
> 2018/05/27 13:53:42| Processing: http_access allow wicl96
> 2018/05/27 13:53:42| Processing: http_access allow wicl97
> 2018/05/27 13:53:42| Processing: http_access allow wicl98
> 2018/05/27 13:53:42| Processing: http_access allow wicl99
> 2018/05/27 13:53:42| Processing: http_access allow pcbill
> 2018/05/27 13:53:42| Processing: http_access allow pchome
> 2018/05/27 13:53:42| Processing: http_access deny all
> 2018/05/27 13:53:42| Processing: always_direct deny all
> 2018/05/27 13:53:42| Processing: icp_port 0
> 2018/05/27 13:53:42| Processing: icp_access deny all

Remove the icp_port and icp_access lines. You do not need them. ICP is
disabled by default in all Squid-3 and later versions.

> 2018/05/27 13:53:42| Processing: http_port 127.0.0.1:3128
> 2018/05/27 13:53:42| Processing: http_port 192.168.100.250:7080 transparent
> 2018/05/27 13:53:42| Starting Authentication on port 192.168.100.250:7080
> 2018/05/27 13:53:42| Disabling Authentication on port 192.168.100.250:7080 (interception enabled)
> 2018/05/27 13:53:42| Processing: cache_mem 32 MB
> 2018/05/27 13:53:42| Processing: cache_swap_low 90
> 2018/05/27 13:53:42| Processing: cache_swap_high 95

These cache_swap_* settings are the defaults. No need to configure them.

> 2018/05/27 13:53:42| Processing: cache_dir ufs /var/cache/squid 2048 16 256
> 2018/05/27 13:53:42| Processing: store_dir_select_algorithm round-robin
> 2018/05/27 13:53:42| Processing: access_log daemon:/var/log/squid/access.log squid
> 2018/05/27 13:53:42| Processing: cache_log /var/log/squid/cache.log
> 2018/05/27 13:53:42| Processing: cache_store_log none


> 2018/05/27 13:53:42| Processing: pid_filename /var/run/squid.pid
> 2018/05/27 13:53:42| Processing: logfile_rotate 1
> 2018/05/27 13:53:42| Processing: log_icp_queries off

With ICP disabled there are no queries to log. No need to disable
logging of non-existant things.

> 2018/05/27 13:53:42| Processing: buffered_logs off
> 2018/05/27 13:53:42| Processing: minimum_object_size 0 KB
> 2018/05/27 13:53:42| Processing: maximum_object_size 10 MB
> 2018/05/27 13:53:42| Processing: refresh_pattern -i (/cgi-bin/|\?) 0 0%  0
> 2018/05/27 13:53:42| Processing: refresh_pattern .       0   100% 10080
> 2018/05/27 13:53:42| Processing: memory_replacement_policy heap GDSF
> 2018/05/27 13:53:42| Processing: cache_replacement_policy heap LFUDA
> 2018/05/27 13:53:42| Processing: shutdown_lifetime 5 seconds
> 2018/05/27 13:53:42| Processing: half_closed_clients off
> 2018/05/27 13:53:42| Processing: client_persistent_connections off

You are turning "client_persistent_connections on" near the end of the
config file. This is not a directive that affects things in the config
file itself - so the later ON setting is what Squid will actually use.

I'm not sure whether this above line or the one later is what you
actually want. One of them should be removed to clarify that.


> 2018/05/27 13:53:42| Processing: server_persistent_connections on
> 2018/05/27 13:53:42| Processing: pconn_timeout 15 seconds
> 2018/05/27 13:53:42| Processing: request_timeout 1 minute
> 2018/05/27 13:53:42| Processing: tcp_outgoing_tos 0x30 all
> 2018/05/27 13:53:42| Processing: retry_on_error on
> 2018/05/27 13:53:42| Processing: buffered_logs on
> 2018/05/27 13:53:42| Processing: global_internal_static off
> 2018/05/27 13:53:42| Processing: max_stale 10 years
> 2018/05/27 13:53:42| Processing: quick_abort_min -1 KB
> 2018/05/27 13:53:42| Processing: vary_ignore_expire on

The above option may be the source of your problems. Notice the warning
message in its documentation:
 <http://www.squid-cache.org/Doc/config/vary_ignore_expire/>


> 2018/05/27 13:53:42| Processing: ie_refresh on

Do you really need to support MSIE older than 5.5?
 (aka Windows machines *older* than XP).

If not, please try removing this option now. It is deprecated and
removed from Squid-4.


> 2018/05/27 13:53:42| Processing: cache_mgr cachemaster at willsz.net
> 2018/05/27 13:53:42| Processing: visible_hostname ip.proxy-cache.willsz.net
> 2018/05/27 13:53:42| Processing: cache_effective_user squid
> 2018/05/27 13:53:42| Processing: cache_effective_group squid

Your build has " --with-default-user=squid ", so no need for the
cache_effective_* settings to override that with "squid".


> 2018/05/27 13:53:42| Processing: check_hostnames on
> 2018/05/27 13:53:42| Processing: dns_retransmit_interval 2 seconds
> 2018/05/27 13:53:42| Processing: dns_timeout 1 minutes
> 2018/05/27 13:53:42| Processing: memory_pools off
> 2018/05/27 13:53:42| Processing: forwarded_for off

"delete" or "transparent" are slightly better settings available in
current Squid. If you must fiddle with that headers contents at all
please consider those instead.
 <http://www.squid-cache.org/Doc/config/forwarded_for/>


> 2018/05/27 13:53:42| Processing: client_persistent_connections on
> 2018/05/27 13:53:42| Processing: coredump_dir /tmp
> 2018/05/27 13:53:42| Processing: httpd_suppress_version_string on
>   
>> B) the web server is not sending Vary headers consistently and the proxy
>> cache ends up thinking that a compressed object is possible to be sent
>> to clients only accepting plain-text objects.
>>  - this one there is not much you can do except to prevent these
>> particular URLs not to be cashed by your own proxy.
>> Both cases result in the client sometimes displaying binary octets as if
>> they were plain text, as you can see in that demo image. More often they
>> occur with scripts and nothing gets displayed - just parts of the
>> website dont work properly (no scrolling, missing content, or
>> unclickable buttons, etc).
>  
> I am more suspicious because of this, many of gambling site hosting will get the same problem.
> I tested for http://www.bolaliga88.com/ with the same result https://redbot.org/?uri=http%3A%2F%2Fwww.bolaliga88.com%2F  
>  

I am currently suspecting it is a combination of these broken Vary
headers usage by the web servers and your use of "vary_ignore_expire on"
which enables those broken objects to be cached.

Amos


From willsznet at gmail.com  Sun May 27 09:49:34 2018
From: willsznet at gmail.com (Willsz.net Support)
Date: Sun, 27 May 2018 16:49:34 +0700
Subject: [squid-users] Squid display garbage character.
In-Reply-To: <79a778f1-dd21-44ae-e91d-b7efd6f3b1a0@treenet.co.nz>
References: <CAMby-etpT8YvEwvuk3oe5uK-722Q_CFL7JBhpysogfccD=Hteg@mail.gmail.com>
 <469119c5-ac02-0676-bc86-ae5385296caf@treenet.co.nz>
 <004001d3f58b$2a5fb150$7f1f13f0$@com>
 <79a778f1-dd21-44ae-e91d-b7efd6f3b1a0@treenet.co.nz>
Message-ID: <000001d3f5a0$066ff790$134fe6b0$@com>

> > 2018/05/27 13:53:42| Processing: check_hostnames on
> > 2018/05/27 13:53:42| Processing: dns_retransmit_interval 2 seconds
> > 2018/05/27 13:53:42| Processing: dns_timeout 1 minutes
> > 2018/05/27 13:53:42| Processing: memory_pools off
> > 2018/05/27 13:53:42| Processing: forwarded_for off
> 
> "delete" or "transparent" are slightly better settings available in
> current Squid. If you must fiddle with that headers contents at all
> please consider those instead.
>  <http://www.squid-cache.org/Doc/config/forwarded_for/>
> 
> 
> > 2018/05/27 13:53:42| Processing: client_persistent_connections on
> > 2018/05/27 13:53:42| Processing: coredump_dir /tmp
> > 2018/05/27 13:53:42| Processing: httpd_suppress_version_string on
> >
> >> B) the web server is not sending Vary headers consistently and the proxy
> >> cache ends up thinking that a compressed object is possible to be sent
> >> to clients only accepting plain-text objects.
> >>  - this one there is not much you can do except to prevent these
> >> particular URLs not to be cashed by your own proxy.
> >> Both cases result in the client sometimes displaying binary octets as if
> >> they were plain text, as you can see in that demo image. More often they
> >> occur with scripts and nothing gets displayed - just parts of the
> >> website dont work properly (no scrolling, missing content, or
> >> unclickable buttons, etc).
> >
> > I am more suspicious because of this, many of gambling site hosting will
> get the same problem.
> > I tested for http://www.bolaliga88.com/ with the same result
> https://redbot.org/?uri=http%3A%2F%2Fwww.bolaliga88.com%2F
> >
> 
> I am currently suspecting it is a combination of these broken Vary
> headers usage by the web servers and your use of "vary_ignore_expire on"
> which enables those broken objects to be cached.
> 
> Amos

Thank you very much for all correction. My mistake is too lazy read ChangeLog every squid release.
Ex: I use my squid.conf since version 2.7, just do modification if fatal error shown in my /var/log/squid/cache.log.
I already reconfigure my squid.conf as you suggested. Now the problem is solved.



From tamilvalavanshanmugam at gmail.com  Mon May 28 07:17:25 2018
From: tamilvalavanshanmugam at gmail.com (Tamil S)
Date: Mon, 28 May 2018 00:17:25 -0700 (MST)
Subject: [squid-users] Squid on Vlan
Message-ID: <1527491845217-0.post@n4.nabble.com>

Hi,
Hello everyone,I have been trying to run the squid in transparent mode along
with Vlan.But i have achieved the Transparent mode in few days from start
but the *Squid Running on Vlan*  is Nightmare for me. Even i have been
stopped in a Loop its make me to leave my hope please help me to resolve
this one 
Thank you



--
Sent from: http://squid-web-proxy-cache.1019090.n4.nabble.com/Squid-Users-f1019091.html


From Antony.Stone at squid.open.source.it  Mon May 28 08:26:14 2018
From: Antony.Stone at squid.open.source.it (Antony Stone)
Date: Mon, 28 May 2018 10:26:14 +0200
Subject: [squid-users] Squid on Vlan
In-Reply-To: <1527491845217-0.post@n4.nabble.com>
References: <1527491845217-0.post@n4.nabble.com>
Message-ID: <201805281026.14937.Antony.Stone@squid.open.source.it>

On Monday 28 May 2018 at 09:17:25, Tamil S wrote:

> Hi,
> Hello everyone,I have been trying to run the squid in transparent mode
> along with Vlan.But i have achieved the Transparent mode in few days from
> start but the *Squid Running on Vlan*  is Nightmare for me. Even i have
> been stopped in a Loop its make me to leave my hope please help me to
> resolve this one

Unfortunately our team of telepaths is very busy right now, so please give us 
some details of what your network setup is, how you have configured 
"transparent mode", what problems you are experiencing and anything else which 
might tell us what your setup is so we might be able to suggest improvements.

Basically, give us some information about what you have done so we can comment 
on it.


Regards,


Antony.

-- 
Schr?dinger's rule of data integrity: the condition of any backup is unknown 
until a restore is attempted.

                                                   Please reply to the list;
                                                         please *don't* CC me.


From Sarfaraz.Ahmad at deshaw.com  Mon May 28 12:17:05 2018
From: Sarfaraz.Ahmad at deshaw.com (Ahmad, Sarfaraz)
Date: Mon, 28 May 2018 12:17:05 +0000
Subject: [squid-users] TCP FIN,ACK after ServerHelloDone with pcmag.com
References: <baf25fdf7595434e840f333afc8e1dfc@mbxtoa3.winmail.deshaw.com>
 <ac103759-8b43-4223-2050-86d049cf4aa5@urlfilterdb.com>
 <3ee9b2cd-757a-48a7-d573-6ac064607130@treenet.co.nz>
 <6fcfa15c-e198-c767-4b16-9e32afa95fd5@urlfilterdb.com>  
Message-ID: <369bf77140714c4eac973ca3a827c8ba@mbxtoa3.winmail.deshaw.com>

I was wrong. It is not the remote server but Squid itself which is sending a FIN,ACK after ServerHelloDone.
At 8 seconds, ServerKeyExchange, ServerHelloDone is received by Squid. The cipher suite looks like (ECDHE+RSA+SHA512 ,wireshark shows rsa_pkcs_sha512.)
After about 60 more seconds (there is no activity on the wire during this period), Squid sends a FIN/ACK to the remote server effectively closing the connection.
What debug_options should I be using for more relevant logging in cache.log ? 26,9 11,9 and 5,9 are not helping much. 

I am adding few loglines anyways. 

2018/05/28 07:20:13.603 kid1| 5,4| AsyncCall.cc(26) AsyncCall: The AsyncCall clientLifetimeTimeout constructed, this=0x1c5e5f0 [call136782]
2018/05/28 07:20:13.603 kid1| 5,3| comm.cc(559) commSetConnTimeout: local=<Squid_IP>:3128 remote=<Client_IP>:64774 FD 13 flags=1 timeout 86400
2018/05/28 07:20:13.603 kid1| 11,5| HttpRequest.cc(460) detailError: current error details: 12/-2
2018/05/28 07:20:13.603 kid1| 11,2| Stream.cc(266) sendStartOfMessage: HTTP Client local=<Squid_IP>:3128 remote=<Client_IP>:64774 FD 13 flags=1
2018/05/28 07:20:13.603 kid1| 11,2| Stream.cc(267) sendStartOfMessage: HTTP Client REPLY:
---------
HTTP/1.1 503 Service Unavailable

Post splicing the webpage opens just fine. That website (www.pcmag.com) has over 750 DNS names added to SAN field. The RFC does not set an upper bound on the number of DNS names you can have in there.

Regards,
Sarfaraz 

-----Original Message-----
From: Ahmad, Sarfaraz 
Sent: Thursday, May 17, 2018 4:18 PM
To: 'squid-users at lists.squid-cache.org' <squid-users at lists.squid-cache.org>
Cc: 'Marcus Kool' <marcus.kool at urlfilterdb.com>
Subject: RE: [squid-users] TCP FIN,ACK after ServerHelloDone with pcmag.com

Guys,

Any thoughts ?

Regards,
Sarfaraz

-----Original Message-----
From: Ahmad, Sarfaraz
Sent: Wednesday, May 16, 2018 10:36 AM
To: 'Marcus Kool' <marcus.kool at urlfilterdb.com>; squid-users at lists.squid-cache.org
Subject: RE: [squid-users] TCP FIN,ACK after ServerHelloDone with pcmag.com

I see a message similar to Marcus' in cache.log.

2018/05/16 00:20:10 kid1| ERROR: negotiating TLS on FD 77: error:14090086:SSL routines:ssl3_get_server_certificate:certificate verify failed (1/-1/0)

And I am running squid-4.0.24.

Sarfaraz

-----Original Message-----
From: squid-users <squid-users-bounces at lists.squid-cache.org> On Behalf Of Marcus Kool
Sent: Wednesday, May 16, 2018 1:41 AM
To: squid-users at lists.squid-cache.org
Subject: Re: [squid-users] TCP FIN,ACK after ServerHelloDone with pcmag.com

The proxies that I used for the test have Squid 4.0.22 and Squid 4.0.23.

Marcus


On 15/05/18 15:40, Amos Jeffries wrote:
> On 16/05/18 01:32, Marcus Kool wrote:
>> pcmag.com also does not load here, although my config parameters are 
>> slightly different.
>> The certificate is indeed huge...
>> Do you have
>>  ?? ERROR: negotiating TLS on FD NNN: error:14090086:SSL 
>> routines:ssl3_get_server_certificate:certificate verify failed
>> (1/-1/0) or other errors in cache.log ?
>>
>> Marcus
>>
> 
> Are these Squid-4.0.24 ? There is a regression[1] in the cafile= 
> parameter handling in the latest release.
>   <https://bugs.squid-cache.org/show_bug.cgi?id=4831>
> 
> Amos
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
> 
_______________________________________________
squid-users mailing list
squid-users at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-users

From rejaine at bhz.jamef.com.br  Mon May 28 16:45:33 2018
From: rejaine at bhz.jamef.com.br (Rejaine Monteiro)
Date: Mon, 28 May 2018 13:45:33 -0300
Subject: [squid-users] Your cache is running out of filedescriptors
Message-ID: <CAMTrDfUvWP1ALgNFYEczPtey7b4EBButFm876FNhbO8AMgEk7Q@mail.gmail.com>

Hello everyone.

My squid is running normally, but after a while it stops working and I
have to restart every time. The message that appears in the log is

     "Your cache is running out of filedescriptors"

I already configured the fs.file-max = 65535 parameter in /etc/sysctl.conf.

I am running openSuse 13.2 and also configured in the
/etc/sysconfig/squid to SQUID_DEFAULT_ULIMT = "65535" and  in the
startup script /etc/rc.d/squid  SQUID_ULIMIT =${SQUID_DEFAULT_ULIMT: =
"65535"}

Nothing solved !!! Any tips?


From rejaine at bhz.jamef.com.br  Mon May 28 18:54:22 2018
From: rejaine at bhz.jamef.com.br (Rejaine Monteiro)
Date: Mon, 28 May 2018 15:54:22 -0300
Subject: [squid-users] Your cache is running out of filedescriptors
In-Reply-To: <CAMTrDfUvWP1ALgNFYEczPtey7b4EBButFm876FNhbO8AMgEk7Q@mail.gmail.com>
References: <CAMTrDfUvWP1ALgNFYEczPtey7b4EBButFm876FNhbO8AMgEk7Q@mail.gmail.com>
Message-ID: <CAMTrDfWDGLNC3E-yh4_3y_by8-EkDwhrPJkximUdgb7G72CuRw@mail.gmail.com>

Solved putting the following configuration in /etc/security/limits.conf file:

squid           hard    nofile          4096
squid           soft    nofile          4096

2018-05-28 13:45 GMT-03:00 Rejaine Monteiro <rejaine at bhz.jamef.com.br>:
> Hello everyone.
>
> My squid is running normally, but after a while it stops working and I
> have to restart every time. The message that appears in the log is
>
>      "Your cache is running out of filedescriptors"
>
> I already configured the fs.file-max = 65535 parameter in /etc/sysctl.conf.
>
> I am running openSuse 13.2 and also configured in the
> /etc/sysconfig/squid to SQUID_DEFAULT_ULIMT = "65535" and  in the
> startup script /etc/rc.d/squid  SQUID_ULIMIT =${SQUID_DEFAULT_ULIMT: =
> "65535"}
>
> Nothing solved !!! Any tips?


From squid3 at treenet.co.nz  Mon May 28 23:44:33 2018
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Tue, 29 May 2018 11:44:33 +1200
Subject: [squid-users] TCP FIN,ACK after ServerHelloDone with pcmag.com
In-Reply-To: <369bf77140714c4eac973ca3a827c8ba@mbxtoa3.winmail.deshaw.com>
References: <baf25fdf7595434e840f333afc8e1dfc@mbxtoa3.winmail.deshaw.com>
 <ac103759-8b43-4223-2050-86d049cf4aa5@urlfilterdb.com>
 <3ee9b2cd-757a-48a7-d573-6ac064607130@treenet.co.nz>
 <6fcfa15c-e198-c767-4b16-9e32afa95fd5@urlfilterdb.com>
 <369bf77140714c4eac973ca3a827c8ba@mbxtoa3.winmail.deshaw.com>
Message-ID: <33d5d5e5-13b0-854a-e347-5b35348f88e9@treenet.co.nz>

On 29/05/18 00:17, Ahmad, Sarfaraz wrote:
> I was wrong. It is not the remote server but Squid itself which is sending a FIN,ACK after ServerHelloDone.
> At 8 seconds, ServerKeyExchange, ServerHelloDone is received by Squid. The cipher suite looks like (ECDHE+RSA+SHA512 ,wireshark shows rsa_pkcs_sha512.)
> After about 60 more seconds (there is no activity on the wire during this period), Squid sends a FIN/ACK to the remote server effectively closing the connection.
> What debug_options should I be using for more relevant logging in cache.log ? 26,9 11,9 and 5,9 are not helping much. 

If in doubt ALL,9 has everything.


Sounds normal symptoms for a verify failure, except odd that there is
still a 60sec timeout happening. It should FIN immediately on the verify
failure.

Amos


From uhlar at fantomas.sk  Tue May 29 06:58:12 2018
From: uhlar at fantomas.sk (Matus UHLAR - fantomas)
Date: Tue, 29 May 2018 08:58:12 +0200
Subject: [squid-users] Your cache is running out of filedescriptors
In-Reply-To: <CAMTrDfWDGLNC3E-yh4_3y_by8-EkDwhrPJkximUdgb7G72CuRw@mail.gmail.com>
References: <CAMTrDfUvWP1ALgNFYEczPtey7b4EBButFm876FNhbO8AMgEk7Q@mail.gmail.com>
 <CAMTrDfWDGLNC3E-yh4_3y_by8-EkDwhrPJkximUdgb7G72CuRw@mail.gmail.com>
Message-ID: <20180529065812.GA16377@fantomas.sk>

On 28.05.18 15:54, Rejaine Monteiro wrote:
>Solved putting the following configuration in /etc/security/limits.conf file:
>
>squid           hard    nofile          4096
>squid           soft    nofile          4096

was there anything like that before? (including "*" in first column)

It's usually better not to put any limits there.

>2018-05-28 13:45 GMT-03:00 Rejaine Monteiro <rejaine at bhz.jamef.com.br>:
>> Hello everyone.
>>
>> My squid is running normally, but after a while it stops working and I
>> have to restart every time. The message that appears in the log is
>>
>>      "Your cache is running out of filedescriptors"
>>
>> I already configured the fs.file-max = 65535 parameter in /etc/sysctl.conf.
>>
>> I am running openSuse 13.2 and also configured in the
>> /etc/sysconfig/squid to SQUID_DEFAULT_ULIMT = "65535" and  in the
>> startup script /etc/rc.d/squid  SQUID_ULIMIT =${SQUID_DEFAULT_ULIMT: =
>> "65535"}
>>
>> Nothing solved !!! Any tips?

-- 
Matus UHLAR - fantomas, uhlar at fantomas.sk ; http://www.fantomas.sk/
Warning: I wish NOT to receive e-mail advertising to this address.
Varovanie: na tuto adresu chcem NEDOSTAVAT akukolvek reklamnu postu.
"One World. One Web. One Program." - Microsoft promotional advertisement
"Ein Volk, ein Reich, ein Fuhrer!" - Adolf Hitler


From belle at bazuin.nl  Tue May 29 07:33:24 2018
From: belle at bazuin.nl (=?windows-1252?Q?L.P.H._van_Belle?=)
Date: Tue, 29 May 2018 09:33:24 +0200
Subject: [squid-users] Your cache is running out of filedescriptors
In-Reply-To: <20180529065812.GA16377@fantomas.sk>
References: <CAMTrDfWDGLNC3E-yh4_3y_by8-EkDwhrPJkximUdgb7G72CuRw@mail.gmail.com>
Message-ID: <vmime.5b0d0244.1958.c5e77dce0f0c84@ms249-lin-003.rotterdam.bazuin.nl>

Is squid starting with a systemd service startup>? 

If so try: 
systemctl edit squid.service

Add at the Service section: 
[Service]
LimitNOFILE=8192:65535



Greetz, 

Louis


> -----Oorspronkelijk bericht-----
> Van: squid-users 
> [mailto:squid-users-bounces at lists.squid-cache.org] Namens 
> Matus UHLAR - fantomas
> Verzonden: dinsdag 29 mei 2018 8:58
> Aan: squid-users at lists.squid-cache.org
> Onderwerp: Re: [squid-users] Your cache is running out of 
> filedescriptors
> 
> On 28.05.18 15:54, Rejaine Monteiro wrote:
> >Solved putting the following configuration in 
> /etc/security/limits.conf file:
> >
> >squid           hard    nofile          4096
> >squid           soft    nofile          4096
> 
> was there anything like that before? (including "*" in first column)
> 
> It's usually better not to put any limits there.
> 
> >2018-05-28 13:45 GMT-03:00 Rejaine Monteiro 
> <rejaine at bhz.jamef.com.br>:
> >> Hello everyone.
> >>
> >> My squid is running normally, but after a while it stops 
> working and I
> >> have to restart every time. The message that appears in the log is
> >>
> >>      "Your cache is running out of filedescriptors"
> >>
> >> I already configured the fs.file-max = 65535 parameter in 
> /etc/sysctl.conf.
> >>
> >> I am running openSuse 13.2 and also configured in the
> >> /etc/sysconfig/squid to SQUID_DEFAULT_ULIMT = "65535" and  in the
> >> startup script /etc/rc.d/squid  SQUID_ULIMIT 
> =${SQUID_DEFAULT_ULIMT: =
> >> "65535"}
> >>
> >> Nothing solved !!! Any tips?
> 
> -- 
> Matus UHLAR - fantomas, uhlar at fantomas.sk ; http://www.fantomas.sk/
> Warning: I wish NOT to receive e-mail advertising to this address.
> Varovanie: na tuto adresu chcem NEDOSTAVAT akukolvek reklamnu postu.
> "One World. One Web. One Program." - Microsoft promotional 
> advertisement
> "Ein Volk, ein Reich, ein Fuhrer!" - Adolf Hitler
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
> 



