From ygirardin at olfeo.com  Mon Oct  1 07:15:55 2018
From: ygirardin at olfeo.com (Yann Girardin)
Date: Mon, 1 Oct 2018 09:15:55 +0200
Subject: [squid-users] URL rewriter note
In-Reply-To: <61705e8a-6a2c-a6d6-113a-a3658eeafcad@measurement-factory.com>
References: <FBAFB216-9CC2-4854-A3ED-F8F6E363F9AE@olfeo.com>
 <61705e8a-6a2c-a6d6-113a-a3658eeafcad@measurement-factory.com>
Message-ID: <DEE553E0920CFA41BAF85A4B9C6FC3498F27D10B72@EXCHANGESRV1.olfeo-lab.net>

Hi,

Sorry I am using squid 4.2 and here is how I configure my url_rewriter program and here is how it is configured

url_rewrite_bypass on
url_rewrite_program my_program my_args
url_rewrite_children 100 startup=10 idle=2 concurrency=5

I recall my program return something that looks like :

OK my_note=30 

And the issue here is that I want to use this note in the logformat program by passing it using %note or %{my_note}note but this call my log_format program with empty note (even when I use %note), I got "-"

-----Message d'origine-----
De?: squid-users [mailto:squid-users-bounces at lists.squid-cache.org] De la part de Alex Rousskov
Envoy??: vendredi 28 septembre 2018 17:57
??: squid-users at lists.squid-cache.org
Objet?: Re: [squid-users] URL rewriter note

On 09/28/2018 07:27 AM, Yann Girardin wrote:
> 
> Hi 
> 
> I am using the url_rewrite_program directive on my squid 4 configuration. 
> 
> I want to use the note feature to use it in my logformat program. But when I pass %note to it my note looks empty squid send me ?-?. 
> 
> My url rewrite program return to squid line that looks something like : 
> 
> OK mynote=XX 
> 
> Where XX is always an integer. 
> 
> Is there something wrong ? 

It is difficult to say what is going on without more information. Please
start by posting your Squid version and the relevant parts of your Squid
configuration.


Thank you,

Alex.


_______________________________________________
squid-users mailing list
squid-users at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-users

From ahmed.zaeem at netstream.ps  Mon Oct  1 13:08:37 2018
From: ahmed.zaeem at netstream.ps (--Ahmad--)
Date: Mon, 1 Oct 2018 16:08:37 +0300
Subject: [squid-users] want to change squid name
In-Reply-To: <ee2ce272-33f3-bb7c-4022-9cdd2cd7d00e@measurement-factory.com>
References: <46DA2877-D62D-4271-9D3C-E95825FA8174@netstream.ps>
 <ee2ce272-33f3-bb7c-4022-9cdd2cd7d00e@measurement-factory.com>
Message-ID: <55236617-33AA-45E1-8793-851C72500BB1@netstream.ps>

i just need to have something not squid to run it on linux 

i dont want squid

i want identical thing to all stuff

want to use other word than squid in footprints and config files

can you help ?



> On 1 Oct 2018, at 1:50, Alex Rousskov <rousskov at measurement-factory.com> wrote:
> 
> On 09/30/2018 12:55 PM, --Ahmad-- wrote:
> 
>> i want to change everything in squid config files and rename it to ahmad.
> 
> Generally useful Squid code modifications should be discussed on the
> squid-dev mailing list, not squid-users. The modification you are
> describing is not generally useful so it is probably out of Squid
> Project support scope.
> 
> However, if you formulate the actual problem you are trying to solve,
> then somebody on this mailing list may know a solution that does not
> include blind (and, in some cases, illegal) changes of Squid sources.
> 
> What are you trying to accomplish? In other words, what problem do you
> think replacing "squid" to "ahmad" in Squid sources would solve?
> 
> Alex.
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users



From Antony.Stone at squid.open.source.it  Mon Oct  1 13:52:16 2018
From: Antony.Stone at squid.open.source.it (Antony Stone)
Date: Mon, 1 Oct 2018 15:52:16 +0200
Subject: [squid-users] want to change squid name
In-Reply-To: <55236617-33AA-45E1-8793-851C72500BB1@netstream.ps>
References: <46DA2877-D62D-4271-9D3C-E95825FA8174@netstream.ps>
 <ee2ce272-33f3-bb7c-4022-9cdd2cd7d00e@measurement-factory.com>
 <55236617-33AA-45E1-8793-851C72500BB1@netstream.ps>
Message-ID: <201810011552.16774.Antony.Stone@squid.open.source.it>

On Monday 01 October 2018 at 15:08:37, --Ahmad-- wrote:

> i just need to have something not squid to run it on linux
> 
> i dont want squid
> 
> i want identical thing to all stuff
> 
> want to use other word than squid in footprints and config files

What problems does the word "Squid" create for you?

It's still not clear what you are trying to achieve by making such a change.


Antony.

> > On 1 Oct 2018, at 1:50, Alex Rousskov wrote:
> > 
> > On 09/30/2018 12:55 PM, --Ahmad-- wrote:
> >> i want to change everything in squid config files and rename it to
> >> ahmad.
> > 
> > Generally useful Squid code modifications should be discussed on the
> > squid-dev mailing list, not squid-users. The modification you are
> > describing is not generally useful so it is probably out of Squid
> > Project support scope.
> > 
> > However, if you formulate the actual problem you are trying to solve,
> > then somebody on this mailing list may know a solution that does not
> > include blind (and, in some cases, illegal) changes of Squid sources.
> > 
> > What are you trying to accomplish? In other words, what problem do you
> > think replacing "squid" to "ahmad" in Squid sources would solve?
> > 
> > Alex.

-- 
I want to build a machine that will be proud of me.

 - Danny Hillis, creator of The Connection Machine

                                                   Please reply to the list;
                                                         please *don't* CC me.


From ahmed.zaeem at netstream.ps  Mon Oct  1 18:22:29 2018
From: ahmed.zaeem at netstream.ps (--Ahmad--)
Date: Mon, 1 Oct 2018 21:22:29 +0300
Subject: [squid-users] want to change squid name
In-Reply-To: <201810011552.16774.Antony.Stone@squid.open.source.it>
References: <46DA2877-D62D-4271-9D3C-E95825FA8174@netstream.ps>
 <ee2ce272-33f3-bb7c-4022-9cdd2cd7d00e@measurement-factory.com>
 <55236617-33AA-45E1-8793-851C72500BB1@netstream.ps>
 <201810011552.16774.Antony.Stone@squid.open.source.it>
Message-ID: <D09AAEFB-592A-4270-84F5-D522E4E87518@netstream.ps>

no problem at all antony 



just want to try something new .

thats all




> On 1 Oct 2018, at 16:52, Antony Stone <Antony.Stone at squid.open.source.it> wrote:
> 
> On Monday 01 October 2018 at 15:08:37, --Ahmad-- wrote:
> 
>> i just need to have something not squid to run it on linux
>> 
>> i dont want squid
>> 
>> i want identical thing to all stuff
>> 
>> want to use other word than squid in footprints and config files
> 
> What problems does the word "Squid" create for you?
> 
> It's still not clear what you are trying to achieve by making such a change.
> 
> 
> Antony.
> 
>>> On 1 Oct 2018, at 1:50, Alex Rousskov wrote:
>>> 
>>> On 09/30/2018 12:55 PM, --Ahmad-- wrote:
>>>> i want to change everything in squid config files and rename it to
>>>> ahmad.
>>> 
>>> Generally useful Squid code modifications should be discussed on the
>>> squid-dev mailing list, not squid-users. The modification you are
>>> describing is not generally useful so it is probably out of Squid
>>> Project support scope.
>>> 
>>> However, if you formulate the actual problem you are trying to solve,
>>> then somebody on this mailing list may know a solution that does not
>>> include blind (and, in some cases, illegal) changes of Squid sources.
>>> 
>>> What are you trying to accomplish? In other words, what problem do you
>>> think replacing "squid" to "ahmad" in Squid sources would solve?
>>> 
>>> Alex.
> 
> -- 
> I want to build a machine that will be proud of me.
> 
> - Danny Hillis, creator of The Connection Machine
> 
>                                                   Please reply to the list;
>                                                         please *don't* CC me.
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org <mailto:squid-users at lists.squid-cache.org>
> http://lists.squid-cache.org/listinfo/squid-users <http://lists.squid-cache.org/listinfo/squid-users>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20181001/816d140f/attachment.htm>

From service.mv at gmail.com  Mon Oct  1 19:12:45 2018
From: service.mv at gmail.com (neok)
Date: Mon, 1 Oct 2018 14:12:45 -0500 (CDT)
Subject: [squid-users] negotiate_kerberos_auth: ERROR
In-Reply-To: <1492e874-ba5a-d1bc-7b9d-87d54be92981@treenet.co.nz>
References: <1538164271714-0.post@n4.nabble.com>
 <1492e874-ba5a-d1bc-7b9d-87d54be92981@treenet.co.nz>
Message-ID: <1538421165864-0.post@n4.nabble.com>

Thank you for the answer Amos!
I will see the possibility of updating the version.
I also have to evaluate the possibility that this error is due to some of my
users using mail services on their smartphones and are authenticated against
my AD Servers . 
I'm not sure if it can be or not... I have to analyze it.

Thanks again.
Greetings !



--
Sent from: http://squid-web-proxy-cache.1019090.n4.nabble.com/Squid-Users-f1019091.html


From service.mv at gmail.com  Mon Oct  1 19:53:42 2018
From: service.mv at gmail.com (neok)
Date: Mon, 1 Oct 2018 14:53:42 -0500 (CDT)
Subject: [squid-users] Chrome 69
Message-ID: <1538423622420-0.post@n4.nabble.com>

Hello everyone!
I'm a bit lost with the behavior of Google Chrome 69.0 for Win 64 and my
squid rules 3.5.20.
Until a few days ago when browsing denied sites Chrome threw the error
"ERR_TUNNEL_CONNECTION_FAILED" which was fine for me.
Firefox 62 threw the error "The proxy server is refusing connections" which
was also fine for me.
Now Chrome shows me the login window every time I visit a denied site.
I suspect Chrome has been updated and changed its behavior. I'm currently
studying that possibility.
I'm also rethinking whether the way I'm denying sites is the right one.
I leave my settings so that someone with more experience can give me some
feedback.
I am very grateful for any indication.
Best regards,
Gabriel.


squid.conf 
### Negotiate/NTLM and Negotiate/Kerberos authentication
auth_param negotiate program /usr/sbin/negotiate_wrapper --ntlm
/usr/bin/ntlm_auth --helper-protocol=squid-2.5-ntlmssp --kerberos
/usr/lib64/squid/negotiate_kerberos_auth -r -i -s GSS_C_NO_NAME 
auth_param negotiate children 200
auth_param negotiate keep_alive on

### standard allowed ports
acl SSL_ports port 443 
acl Safe_ports port 80 # http 
acl Safe_ports port 21 # ftp 
acl Safe_ports port 443 # https 
acl Safe_ports port 70 # gopher 
acl Safe_ports port 210 # wais 
acl Safe_ports port 1025-65535 # unregistered ports 
acl Safe_ports port 280 # http-mgmt 
acl Safe_ports port 488 # gss-http 
acl Safe_ports port 591 # filemaker 
acl Safe_ports port 777 # multiling http 
acl CONNECT method CONNECT

### destination domains to be blocked in a HTTP access control policy
acl LS_adult dstdomain -i "/etc/squid/DBL/adult.txt"
acl LS_anonvpn dstdomain -i "/etc/squid/DBL/anonvpn.txt"
acl LS_hacking dstdomain -i "/etc/squid/DBL/hacking.txt"
acl LS_malicius dstdomain -i "/etc/squid/DBL/malicius.txt"
acl LS_remotecontrol dstdomain -i "/etc/squid/DBL/remotecontrol.txt"
acl LS_warez dstdomain -i "/etc/squid/DBL/warez.txt"
acl LS_youtube dstdomain -i "/etc/squid/DBL/youtube.txt"

### acl for proxy authentication (kerberos or ntlm)
acl auth proxy_auth REQUIRED

### LDAP group membership sources ###
external_acl_type AD_WEB_ACCESS %LOGIN /usr/lib64/squid/ext_ldap_group_acl
-P -R -b "OU=NETGOL,DC=netgol,DC=local" -D ldap -W
"/etc/squid/ldap_pass.txt" -f
"(&(sAMAccountname=%u)(memberof=cn=%g,OU=INTERNET,OU=PERMISOS,OU=NETGOL,DC=netgol,DC=local))"
-h s-dc1.netgol.local
acl WEB_ACCESS_1 external AD_WEB_ACCESS WEB_ACCESS_1
acl WEB_ACCESS_2 external AD_WEB_ACCESS WEB_ACCESS_2
acl WEB_ACCESS_3 external AD_WEB_ACCESS WEB_ACCESS_3
acl WEB_ACCESS_YT_ONLY external AD_WEB_ACCESS WEB_ACCESS_YT_ONLY

### HTTP access control policies
http_access deny !Safe_ports 
http_access deny CONNECT !SSL_ports 
http_access allow localhost manager 
http_access deny manager
http_access deny !auth
http_access allow localhost
http_access deny LS_malicius			# malicius sites denied for all

http_access allow WEB_ACCESS_1			# WEB_ACCESS_1 member users can browse
without restrictions

http_access deny WEB_ACCESS_2 LS_remotecontrol	# WEB_ACCESS_2 member users
can't browse Remote Control sites
http_access deny WEB_ACCESS_2 LS_warez		# WEB_ACCESS_2 member users can't
browse Warez sites
http_access allow WEB_ACCESS_2			# WEB_ACCESS_2 member users can browse the
rest of the sites not bloqued

http_access deny WEB_ACCESS_3 LS_adult		# WEB_ACCESS_3 member users can't
browse Adult sites
http_access deny WEB_ACCESS_3 LS_anonvpn	# WEB_ACCESS_3 member users can't
browse Anonymous VPN sites
http_access deny WEB_ACCESS_3 LS_hacking	# WEB_ACCESS_3 member users can't
browse Hacking sites
http_access deny WEB_ACCESS_3 LS_remotecontrol	# WEB_ACCESS_3 member users
can't browse Remote Control sites
http_access deny WEB_ACCESS_3 LS_warez		# WEB_ACCESS_3 member users can't
browse Warez sites
http_access allow WEB_ACCESS_3			# WEB_ACCESS_3 member users can browse the
rest of the sites not bloqued

http_access allow WEB_ACCESS_YT_ONLY LS_youtube # WEB_ACCESS_YT_ONLY member
users can browse YouTube
http_access deny WEB_ACCESS_YT_ONLY             # WEB_ACCESS_YT_ONLY member
users can't browse the rest of sites

http_access deny all

### PERSONALIZATION ###
http_port 8080 
coredump_dir /var/spool/squid 
refresh_pattern ^ftp: 1440 20% 10080 
refresh_pattern ^gopher: 1440 0% 1440 
refresh_pattern -i (/cgi-bin/|\?) 0 0% 0 
refresh_pattern .  0 20% 4320 
quick_abort_min 0 KB 
quick_abort_max 0 KB 
read_timeout 5 minutes 
request_timeout 3 minutes 
shutdown_lifetime 15 seconds 
ipcache_size 2048 
fqdncache_size 4096 
forwarded_for off 
httpd_suppress_version_string on 


Mi lab scenario:
- A VM CentOS 7 Core over VirtualBox 5.2, 1 NIC. 
- My VM is attached to my domain W2012R2 (following this post 
https://www.rootusers.com/how-to-join-centos-linux-to-an-active-directory-domain/) 
to achieve kerberos authentication transparent to the user. SElinux 
disabled. Owner permissions to user squid in all folders/files involved. 
- squid 3.5.20



--
Sent from: http://squid-web-proxy-cache.1019090.n4.nabble.com/Squid-Users-f1019091.html


From Antony.Stone at squid.open.source.it  Mon Oct  1 20:02:57 2018
From: Antony.Stone at squid.open.source.it (Antony Stone)
Date: Mon, 1 Oct 2018 22:02:57 +0200
Subject: [squid-users] Chrome 69
In-Reply-To: <1538423622420-0.post@n4.nabble.com>
References: <1538423622420-0.post@n4.nabble.com>
Message-ID: <201810012202.57404.Antony.Stone@squid.open.source.it>

On Monday 01 October 2018 at 21:53:42, neok wrote:

> Hello everyone!
> I'm a bit lost with the behavior of Google Chrome 69.0 for Win 64 and my
> squid rules 3.5.20.
> Until a few days ago

Any idea what changed a few days ago?

> when browsing denied sites Chrome threw the error
> "ERR_TUNNEL_CONNECTION_FAILED" which was fine for me.
> Firefox 62 threw the error "The proxy server is refusing connections" which
> was also fine for me.
> Now Chrome shows me the login window every time I visit a denied site.

What does Firefox do?

> I suspect Chrome has been updated and changed its behavior. I'm currently
> studying that possibility.
> I'm also rethinking whether the way I'm denying sites is the right one.
> I leave my settings so that someone with more experience can give me some
> feedback.

It would be useful if you showed us the Squid access log entries for a denied 
site you try to visit, which produces the above browser responses.


Antony.

-- 
I just got a new mobile phone, and I called it Titanic.  It's already syncing.

                                                   Please reply to the list;
                                                         please *don't* CC me.


From rousskov at measurement-factory.com  Mon Oct  1 20:04:13 2018
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Mon, 1 Oct 2018 14:04:13 -0600
Subject: [squid-users] URL rewriter note
In-Reply-To: <DEE553E0920CFA41BAF85A4B9C6FC3498F27D10B72@EXCHANGESRV1.olfeo-lab.net>
References: <FBAFB216-9CC2-4854-A3ED-F8F6E363F9AE@olfeo.com>
 <61705e8a-6a2c-a6d6-113a-a3658eeafcad@measurement-factory.com>
 <DEE553E0920CFA41BAF85A4B9C6FC3498F27D10B72@EXCHANGESRV1.olfeo-lab.net>
Message-ID: <7ce99054-1de7-3904-29cc-a14655434560@measurement-factory.com>

On 10/01/2018 01:15 AM, Yann Girardin wrote:
> Hi,
> 
> Sorry I am using squid 4.2 and here is how I configure my url_rewriter program and here is how it is configured
> 
> url_rewrite_bypass on
> url_rewrite_program my_program my_args
> url_rewrite_children 100 startup=10 idle=2 concurrency=5
> 
> I recall my program return something that looks like :
> 
> OK my_note=30 

IIRC, that is not a correct URL rewriter response when helper
concurrency is enabled. It is missing the Channel-ID field shown at
https://wiki.squid-cache.org/Features/AddonHelpers#HTTP_Redirection

If your helper does not send the channel ID field, please fix the
helper. Otherwise, please post your logformat line (you can replace any
private formatting info with "..." or similar).

Also, does your helper send my_note for all responses? Are you sure that
access log lines with missing annotations correspond to the cases where
the URL rewriting helper was used and sent my_note?


Thank you,

Alex.


> And the issue here is that I want to use this note in the logformat program by passing it using %note or %{my_note}note but this call my log_format program with empty note (even when I use %note), I got "-"
> 
> -----Message d'origine-----
> De?: squid-users [mailto:squid-users-bounces at lists.squid-cache.org] De la part de Alex Rousskov
> Envoy??: vendredi 28 septembre 2018 17:57
> ??: squid-users at lists.squid-cache.org
> Objet?: Re: [squid-users] URL rewriter note
> 
> On 09/28/2018 07:27 AM, Yann Girardin wrote:
>>
>> Hi 
>>
>> I am using the url_rewrite_program directive on my squid 4 configuration. 
>>
>> I want to use the note feature to use it in my logformat program. But when I pass %note to it my note looks empty squid send me ?-?. 
>>
>> My url rewrite program return to squid line that looks something like : 
>>
>> OK mynote=XX 
>>
>> Where XX is always an integer. 
>>
>> Is there something wrong ? 
> 
> It is difficult to say what is going on without more information. Please
> start by posting your Squid version and the relevant parts of your Squid
> configuration.
> 
> 
> Thank you,
> 
> Alex.
> 
> 
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
> 



From leolistas at solutti.com.br  Mon Oct  1 20:08:03 2018
From: leolistas at solutti.com.br (Leonardo Rodrigues)
Date: Mon, 1 Oct 2018 17:08:03 -0300
Subject: [squid-users] want to change squid name
In-Reply-To: <55236617-33AA-45E1-8793-851C72500BB1@netstream.ps>
References: <46DA2877-D62D-4271-9D3C-E95825FA8174@netstream.ps>
 <ee2ce272-33f3-bb7c-4022-9cdd2cd7d00e@measurement-factory.com>
 <55236617-33AA-45E1-8793-851C72500BB1@netstream.ps>
Message-ID: <7ed3a3e8-4ed4-e7e6-9ff4-8cc268a625fd@solutti.com.br>

Em 01/10/18 10:08, --Ahmad-- escreveu:
> i just need to have something not squid to run it on linux
>
> i dont want squid
>

 ??? so don't run squid ?!?! If someone finding that you're running 
squid and that's a problem to you, don't run it, period :)


-- 


	Atenciosamente / Sincerily,
	Leonardo Rodrigues
	Solutti Tecnologia
	http://www.solutti.com.br

	Minha armadilha de SPAM, N?O mandem email
	gertrudes at solutti.com.br
	My SPAMTRAP, do not email it





From service.mv at gmail.com  Mon Oct  1 20:28:07 2018
From: service.mv at gmail.com (neok)
Date: Mon, 1 Oct 2018 15:28:07 -0500 (CDT)
Subject: [squid-users] Chrome 69
In-Reply-To: <201810012202.57404.Antony.Stone@squid.open.source.it>
References: <1538423622420-0.post@n4.nabble.com>
 <201810012202.57404.Antony.Stone@squid.open.source.it>
Message-ID: <1538425687031-0.post@n4.nabble.com>

As far as I know, nothing's changed. The only thing that could have happened
is that Chrome was updated. I'm reviewing that.

This say Firefox when try acces to denied site:
The proxy server is refusing connections.
Firefox is configured to use a proxy server that is refusing connections.


Try access denied site through Firefox:

1538425226.377      0 10.10.11.154 TCP_DENIED/407 4147 CONNECT
www.amazon.com:443 - HIER_NONE/- text/html
1538425226.387      0 10.10.11.154 TCP_DENIED/407 4199 CONNECT
safebrowsing.googleapis.com:443 - HIER_NONE/- text/html
1538425226.406     23 10.10.11.154 TCP_DENIED/407 6996 CONNECT
tiles.services.mozilla.com:443 my.name HIER_NONE/- text/html
1538425226.409     17 10.10.11.154 TCP_DENIED/407 6944 CONNECT
www.amazon.com:443 my.name HIER_NONE/- text/html
1538425226.414     18 10.10.11.154 TCP_DENIED/407 6990 CONNECT
safebrowsing.googleapis.com:443 my.name HIER_NONE/- text/html
1538425226.454      0 10.10.11.154 TCP_DENIED/407 4195 CONNECT
tiles.services.mozilla.com:443 - HIER_NONE/- text/html
1538425226.457      7 10.10.11.154 TCP_DENIED/407 6998 CONNECT
tiles.services.mozilla.com:443 my.name HIER_NONE/- text/html
1538425226.482     10 10.10.11.154 TCP_DENIED/407 6988 CONNECT
tiles.services.mozilla.com:443 my.name HIER_NONE/- text/html


Try access denied site through Chrome:
1538425414.983      0 10.10.11.154 TCP_DENIED/407 4156 CONNECT
www.amazon.com:443 - HIER_NONE/- text/html
1538425414.983      0 10.10.11.154 TCP_DENIED/407 4156 CONNECT
www.amazon.com:443 - HIER_NONE/- text/html
1538425415.013     26 10.10.11.154 TCP_DENIED/407 6947 CONNECT
www.amazon.com:443 my.name HIER_NONE/- text/html
1538425415.016      0 10.10.11.154 TCP_DENIED/407 4201 CONNECT
www.amazon.com:443 my.name HIER_NONE/- text/html
1538425429.157      0 10.10.11.154 TCP_DENIED/407 4168 CONNECT
www.google.com.ar:443 - HIER_NONE/- text/html
1538425429.232      0 10.10.11.154 TCP_DENIED/407 4156 CONNECT
www.amazon.com:443 - HIER_NONE/- text/html
1538425429.233      0 10.10.11.154 TCP_DENIED/407 4156 CONNECT
www.amazon.com:443 - HIER_NONE/- text/html
1538425429.284     46 10.10.11.154 TCP_DENIED/407 6957 CONNECT
www.amazon.com:443 my.name HIER_NONE/- text/html
1538425429.286      0 10.10.11.154 TCP_DENIED/407 4201 CONNECT
www.amazon.com:443 my.name HIER_NONE/- text/html

In this case the authentication pop-up appears. When I use Firefox it never
appears.

Really weird...




--
Sent from: http://squid-web-proxy-cache.1019090.n4.nabble.com/Squid-Users-f1019091.html


From ahmed.zaeem at netstream.ps  Mon Oct  1 20:41:46 2018
From: ahmed.zaeem at netstream.ps (--Ahmad--)
Date: Mon, 1 Oct 2018 23:41:46 +0300
Subject: [squid-users] want to change squid name
In-Reply-To: <7ed3a3e8-4ed4-e7e6-9ff4-8cc268a625fd@solutti.com.br>
References: <46DA2877-D62D-4271-9D3C-E95825FA8174@netstream.ps>
 <ee2ce272-33f3-bb7c-4022-9cdd2cd7d00e@measurement-factory.com>
 <55236617-33AA-45E1-8793-851C72500BB1@netstream.ps>
 <7ed3a3e8-4ed4-e7e6-9ff4-8cc268a625fd@solutti.com.br>
Message-ID: <7094A6A2-B3A8-4BCC-8154-694E6F61A771@netstream.ps>

@ Atenciosamente 


im not forcing you to give us help , BTW  thank you for your magic words !




> On 1 Oct 2018, at 23:08, Leonardo Rodrigues <leolistas at solutti.com.br> wrote:
> 
> Em 01/10/18 10:08, --Ahmad-- escreveu:
>> i just need to have something not squid to run it on linux
>> 
>> i dont want squid
>> 
> 
>     so don't run squid ?!?! If someone finding that you're running squid and that's a problem to you, don't run it, period :)
> 
> 
> -- 
> 
> 
> 	Atenciosamente / Sincerily,
> 	Leonardo Rodrigues
> 	Solutti Tecnologia
> 	http://www.solutti.com.br
> 
> 	Minha armadilha de SPAM, N?O mandem email
> 	gertrudes at solutti.com.br
> 	My SPAMTRAP, do not email it
> 
> 
> 
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users



From service.mv at gmail.com  Mon Oct  1 20:52:23 2018
From: service.mv at gmail.com (neok)
Date: Mon, 1 Oct 2018 15:52:23 -0500 (CDT)
Subject: [squid-users] Help: squid restarts and squidGuard die
In-Reply-To: <!&!AAAAAAAAAAAuAAAAAAAAAL3wsAcc8JBJkMCbPuiQMGEBAMO2jhD3dRHOtM0AqgC7tuYAAAAAAA4AABAAAAD3jhNc4xlaR6tlvz7g9NgqAQAAAAA=@ngtech.co.il>
References: <CA+d==oEt8J-ie2Fm1SYrHSBKD6Y7TRF-v3iGiMXC7WQ36WV1vw@mail.gmail.com>
 <!&!AAAAAAAAAAAuAAAAAAAAAL3wsAcc8JBJkMCbPuiQMGEBAMO2jhD3dRHOtM0AqgC7tuYAAAAAAA4AABAAAAD3jhNc4xlaR6tlvz7g9NgqAQAAAAA=@ngtech.co.il>
Message-ID: <1538427143364-0.post@n4.nabble.com>

Hi Eliezer, I apologize! I don't know why I stopped receiving emails from the
squid users list.
Only today I see the thread in nabble.com and I see that it has 23 posts!

Regarding your question, I didn't investigate the error of squidGuard... I
started to migrate my lists to native squid lists as Amos recommended. I
really thought it was the best option. Of course it took work, but the
configuration is cleaner and faster in my opinion. 
There are other posts in which I share my configuration if you want to see
it.

Best regards...

Gabriel




--
Sent from: http://squid-web-proxy-cache.1019090.n4.nabble.com/Squid-Users-f1019091.html


From uhlar at fantomas.sk  Mon Oct  1 20:55:30 2018
From: uhlar at fantomas.sk (Matus UHLAR - fantomas)
Date: Mon, 1 Oct 2018 22:55:30 +0200
Subject: [squid-users] want to change squid name
In-Reply-To: <7094A6A2-B3A8-4BCC-8154-694E6F61A771@netstream.ps>
References: <46DA2877-D62D-4271-9D3C-E95825FA8174@netstream.ps>
 <ee2ce272-33f3-bb7c-4022-9cdd2cd7d00e@measurement-factory.com>
 <55236617-33AA-45E1-8793-851C72500BB1@netstream.ps>
 <7ed3a3e8-4ed4-e7e6-9ff4-8cc268a625fd@solutti.com.br>
 <7094A6A2-B3A8-4BCC-8154-694E6F61A771@netstream.ps>
Message-ID: <20181001205530.GB4039@fantomas.sk>

On 01.10.18 23:41, --Ahmad-- wrote:
>im not forcing you to give us help , BTW  thank you for your magic words !

we just don't understand the reason you are asking for this.

As was already mentioned (iirc), technically  you can change the name
"squid" to something else, but it is not supported (which means, there's no
standard way to do that) and you may expect problems (and we even don't know
what kind of problems).

>> Em 01/10/18 10:08, --Ahmad-- escreveu:
>>> i just need to have something not squid to run it on linux
>>>
>>> i dont want squid

>> On 1 Oct 2018, at 23:08, Leonardo Rodrigues <leolistas at solutti.com.br> wrote:
>>     so don't run squid ?!?! If someone finding that you're running squid and that's a problem to you, don't run it, period :)
-- 
Matus UHLAR - fantomas, uhlar at fantomas.sk ; http://www.fantomas.sk/
Warning: I wish NOT to receive e-mail advertising to this address.
Varovanie: na tuto adresu chcem NEDOSTAVAT akukolvek reklamnu postu.
I drive way too fast to worry about cholesterol. 


From squid3 at treenet.co.nz  Mon Oct  1 21:03:49 2018
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Tue, 2 Oct 2018 10:03:49 +1300
Subject: [squid-users] Chrome 69
In-Reply-To: <1538425687031-0.post@n4.nabble.com>
References: <1538423622420-0.post@n4.nabble.com>
 <201810012202.57404.Antony.Stone@squid.open.source.it>
 <1538425687031-0.post@n4.nabble.com>
Message-ID: <6b380983-66eb-ab1b-2594-4fee5c51a825@treenet.co.nz>

On 2/10/18 9:28 AM, neok wrote:
> As far as I know, nothing's changed. The only thing that could have happened
> is that Chrome was updated. I'm reviewing that.
> 
> This say Firefox when try acces to denied site:
> The proxy server is refusing connections.
> Firefox is configured to use a proxy server that is refusing connections.
> 

As you an see from the access.log Squid is presenting 407 auth
challenged in response to these CONNECT requests.

It is welcome news to hear Chrome is finally been fixed to actually
perform authentication for CONNECT tunnels. While it may have been okay
with you the previous behaviour is actually a long-standing Browser bug
and violation of HTTP on their part.




On 2/10/18 8:53 AM, neok wrote:
>
> Now Chrome shows me the login window every time I visit a denied site.
> I suspect Chrome has been updated and changed its behavior. I'm currently
> studying that possibility.
> I'm also rethinking whether the way I'm denying sites is the right one.

...
>
> http_access allow WEB_ACCESS_YT_ONLY LS_youtube
> http_access deny WEB_ACCESS_YT_ONLY

The "deny WEB_ACCESS_YT_ONLY" line is triggering the re-authentication.

When the reason for denial is due to credentials (ie %LOGIN for the
external ACL evaluation) a 407 authentication challenge is produced.

Notice that in the config I helped you develop a few days ago all the
other uses of these external ACL tests for "deny" action are followed by
another ACL test that is unrelated to the auth process. Usually a regex
or dstdomain check. It is these other ACL checks which prevent any of
those lines triggering similar auth challenges as they deny traffic.


Since the next thing your config is doing is a "deny all" you can remove
this "deny WEB_ACCESS_YT_ONLY" line entirely and the popups should
disappear while still having that traffic denied.


If you have other config policies to be added later there are some other
things you can do. But those will depend on what the other policies are
going to require, so I wont cover it now. Just be aware you may have to
revisit that YT access rule when adding other policies.


Amos


From squid3 at treenet.co.nz  Mon Oct  1 21:08:47 2018
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Tue, 2 Oct 2018 10:08:47 +1300
Subject: [squid-users] negotiate_kerberos_auth: ERROR
In-Reply-To: <1538421165864-0.post@n4.nabble.com>
References: <1538164271714-0.post@n4.nabble.com>
 <1492e874-ba5a-d1bc-7b9d-87d54be92981@treenet.co.nz>
 <1538421165864-0.post@n4.nabble.com>
Message-ID: <6910a504-af04-9e28-e28f-05b1e4951ed4@treenet.co.nz>

On 2/10/18 8:12 AM, neok wrote:
> Thank you for the answer Amos!
> I will see the possibility of updating the version.
> I also have to evaluate the possibility that this error is due to some of my
> users using mail services on their smartphones and are authenticated against
> my AD Servers . 
> I'm not sure if it can be or not... I have to analyze it.

That is highly unlikely.

It may be the reason NTLM credentials are being sent, but will not
itself cause the "*" behaviour.

That "*" is coming directly from the negotiate_wrapper helper and how it
is configured. Problems are known to occur with the wrapper helper when
the sub-helper is misconfigured, or when usernames contain whitespace
the old squid-2 helper protocol cannot cope with that. The former
usually results in these weird "*", and the latter shows up as truncated
usernames in your access.log.


Amos


From alex at nanogherkin.com  Tue Oct  2 06:02:28 2018
From: alex at nanogherkin.com (Alex Crow)
Date: Tue, 2 Oct 2018 07:02:28 +0100
Subject: [squid-users] want to change squid name
In-Reply-To: <20181001205530.GB4039@fantomas.sk>
References: <46DA2877-D62D-4271-9D3C-E95825FA8174@netstream.ps>
 <ee2ce272-33f3-bb7c-4022-9cdd2cd7d00e@measurement-factory.com>
 <55236617-33AA-45E1-8793-851C72500BB1@netstream.ps>
 <7ed3a3e8-4ed4-e7e6-9ff4-8cc268a625fd@solutti.com.br>
 <7094A6A2-B3A8-4BCC-8154-694E6F61A771@netstream.ps>
 <20181001205530.GB4039@fantomas.sk>
Message-ID: <d735ccb9-bdba-d5ec-b7c6-3894ca065f27@nanogherkin.com>

What about this?

http://www.squid-cache.org/Doc/config/via/


> we just don't understand the reason you are asking for this.
>
> As was already mentioned (iirc), technically? you can change the name
> "squid" to something else, but it is not supported (which means, 
> there's no
> standard way to do that) and you may expect problems (and we even 
> don't know
> what kind of problems).



From squid3 at treenet.co.nz  Tue Oct  2 07:40:41 2018
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Tue, 2 Oct 2018 20:40:41 +1300
Subject: [squid-users] want to change squid name
In-Reply-To: <d735ccb9-bdba-d5ec-b7c6-3894ca065f27@nanogherkin.com>
References: <46DA2877-D62D-4271-9D3C-E95825FA8174@netstream.ps>
 <ee2ce272-33f3-bb7c-4022-9cdd2cd7d00e@measurement-factory.com>
 <55236617-33AA-45E1-8793-851C72500BB1@netstream.ps>
 <7ed3a3e8-4ed4-e7e6-9ff4-8cc268a625fd@solutti.com.br>
 <7094A6A2-B3A8-4BCC-8154-694E6F61A771@netstream.ps>
 <20181001205530.GB4039@fantomas.sk>
 <d735ccb9-bdba-d5ec-b7c6-3894ca065f27@nanogherkin.com>
Message-ID: <6975395d-fde0-52fc-259e-e0c4d6acaf26@treenet.co.nz>

On 2/10/18 7:02 PM, Alex Crow wrote:
> What about this?
> 
> http://www.squid-cache.org/Doc/config/via/
> 

Irrelevant?

Amos


From ecoirier at olfeo.com  Tue Oct  2 11:32:26 2018
From: ecoirier at olfeo.com (Emmanuel Coirier)
Date: Tue, 2 Oct 2018 13:32:26 +0200
Subject: [squid-users] Uncomplete Negotiate negotiation with Kerberos
Message-ID: <DEE553E0920CFA41BAF85A4B9C6FC3498F27D10DEB@EXCHANGESRV1.olfeo-lab.net>

Hi everybody !

Is it desirable that the Negotiate/Kerberos proxy authentication method be interrupted as soon as Squid knows the identity of the browser's user, without letting the browser authenticates Squid in response ?

When a browser wants to connect to some random HTTP website, it sends a GET request. A proxy wanting to authenticate the browser, via Kerberos, responds with 407, which lets the browser knows that it has to authenticate. Then the browser sends a Kerberos authenticator (in base64 in the Proxy-Authorization header). Then Squid *should* answer back with a Proxy-Authentication-Info letting the browser finish the Kerberos process and validating that it is communicating with the real proxy it wants to use. It is how gss-api is intended to be used.

It was the case with Squid 3.3.8, where Squid answers back, along with the response (200 OK), the Proxy-Authentication-Info header with the final token. But with Squid 4.2, the browser doesn't anymore add this headers, preventing the browser to authenticate back the proxy.

The problem is that it enables some potentially Man in the Middle attack (since any malicious proxy where the traffic is diverted could then answers back without the client knowing it talks to a malicious proxy)

Here is an example of this phenomenon with GET and CONNECT :

GET http://perdu.com/ HTTP/1.1
Host: perdu.com
User-Agent: Mozilla/5.0 (X11; Linux x86_64; rv:52.0) Gecko/20100101 Firefox/52.0
Accept: text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8
Accept-Language: fr,en;q=0.5
Accept-Encoding: gzip, deflate
Connection: keep-alive
Upgrade-Insecure-Requests: 1

HTTP/1.1 407 Proxy Authentication Required
Server: squid/4.2
Mime-Version: 1.0
Date: Tue, 02 Oct 2018 08:35:27 GMT
Content-Type: text/html;charset=utf-8
Content-Length: 1852
X-Squid-Error: ERR_CACHE_ACCESS_DENIED 0
Vary: Accept-Language
Content-Language: fr
Proxy-Authenticate: Negotiate
X-Cache: MISS from ceriseconfite
X-Cache-Lookup: NONE from ceriseconfite:3129
Via: 1.1 ceriseconfite (squid/4.2)
Connection: keep-alive

<some really long webpage for something that shouldn't be displayed>

GET http://perdu.com/ HTTP/1.1
Host: perdu.com
User-Agent: Mozilla/5.0 (X11; Linux x86_64; rv:52.0) Gecko/20100101 Firefox/52.0
Accept: text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8
Accept-Language: fr,en;q=0.5
Accept-Encoding: gzip, deflate
Connection: keep-alive
Upgrade-Insecure-Requests: 1
Proxy-Authorization: Negotiate YIIChwYGKw[...blablabla some random authenticator...]W6m

HTTP/1.1 200 OK (with Squid 4.2)
Date: Tue, 02 Oct 2018 08:35:27 GMT
Server: Apache
Last-Modified: Thu, 02 Jun 2016 06:01:08 GMT
ETag: "cc-5344555136fe9"
Accept-Ranges: bytes
Vary: Accept-Encoding
Content-Encoding: gzip
Content-Length: 163
Content-Type: text/html
X-Cache: MISS from ceriseconfite
X-Cache-Lookup: MISS from ceriseconfite:3129
Via: 1.1 ceriseconfite (squid/4.2)
Connection: keep-alive
                     <------------- Missing the Proxy-Authentication-Info


HTTP/1.1 200 OK (with older Squid 3.3.8)
Date: Fri, 28 Sep 2018 08:09:27 GMT
Server: Apache
Last-Modified: Thu, 02 Jun 2016 06:01:08 GMT
ETag: "cc-5344555136fe9"
Accept-Ranges: bytes
Vary: Accept-Encoding
Content-Encoding: gzip
Content-Length: 163&
Content-Type: text/html
Proxy-Authentication-Info: Negotiate oRQwEqADCgEAoQsGCSqGSIb3EgECAg==  <--- here was the authentication response back to the client
X-Cache: MISS from sucre-SQUID-50118215
X-Cache-Lookup: MISS from sucre-SQUID-50118215:3127
Connection: keep-alive

..........5....0.Dw...H.....L.l..Cc.H%).S..'!....:...L..:B
:3.S.pU.0....4....#.O....\...-*KdmzE...m........."...v...R..0...$\.	j.....ny
2...0.4.B...>....|....-....


And here is the same exchange but with TLS :

CONNECT perdu.com:443 HTTP/1.1
User-Agent: Mozilla/5.0 (X11; Linux x86_64; rv:52.0) Gecko/20100101 Firefox/52.0
Proxy-Connection: keep-alive
Connection: keep-alive
Host: perdu.com:443

HTTP/1.1 407 Proxy Authentication Required
Server: squid/4.2
Mime-Version: 1.0
Date: Tue, 02 Oct 2018 08:30:51 GMT
Content-Type: text/html;charset=utf-8
Content-Length: 1813
X-Squid-Error: ERR_CACHE_ACCESS_DENIED 0
Vary: Accept-Language
Content-Language: en
Proxy-Authenticate: Negotiate
X-Cache: MISS from ceriseconfite
X-Cache-Lookup: NONE from ceriseconfite:3129
Via: 1.1 ceriseconfite (squid/4.2)
Connection: keep-alive

<some really long webpage for something that shouldn't be displayed>

CONNECT perdu.com:443 HTTP/1.1
User-Agent: Mozilla/5.0 (X11; Linux x86_64; rv:52.0) Gecko/20100101 Firefox/52.0
Proxy-Connection: keep-alive
Connection: keep-alive
Host: perdu.com:443
Proxy-Authorization: Negotiate YIIChwYGKwYBBQUCo[...blablabla some other random authenticator...]zCqD

HTTP/1.1 200 Connection established (same with Squid 4.2 and Squid 3.3.8)
                  <---- is the Proxy-Authentication-Info missing ?

(TLS tunnel exchange)

It this behavior endorsed, or is it a bug ?

Actually, I know that browsers seem not to care, but they won't be able to care if they don't receive all the needed information.

Thanks for reading and your response :-)

-- 
Emmanuel Coirier


From squid3 at treenet.co.nz  Tue Oct  2 12:55:18 2018
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Wed, 3 Oct 2018 01:55:18 +1300
Subject: [squid-users] Uncomplete Negotiate negotiation with Kerberos
In-Reply-To: <DEE553E0920CFA41BAF85A4B9C6FC3498F27D10DEB@EXCHANGESRV1.olfeo-lab.net>
References: <DEE553E0920CFA41BAF85A4B9C6FC3498F27D10DEB@EXCHANGESRV1.olfeo-lab.net>
Message-ID: <fcfa0f22-8f57-a901-5d38-d0a73b9f90b2@treenet.co.nz>

On 3/10/18 12:32 AM, Emmanuel Coirier wrote:
> Hi everybody !
> 
> Is it desirable that the Negotiate/Kerberos proxy authentication method be interrupted as soon as Squid knows the identity of the browser's user, without letting the browser authenticates Squid in response ?

That is not possible to happen. The process of authenticating finishes
by telling Squid what the username is.

> 
> When a browser wants to connect to some random HTTP website, it sends a GET request. A proxy wanting to authenticate the browser, via Kerberos, responds with 407, which lets the browser knows that it has to authenticate. Then the browser sends a Kerberos authenticator (in base64 in the Proxy-Authorization header). Then Squid *should* answer back with a Proxy-Authentication-Info letting the browser finish the Kerberos process and validating that it is communicating with the real proxy it wants to use. It is how gss-api is intended to be used.
> 

That is not true. RFC 4559 section 4
(<https://tools.ietf.org/html/rfc4559#section-4>) defines how Negotiate
scheme operates in HTTP.

Notice two things;

 * firstly that there is no mention of *-Authentication-Info headers
anywhere in that specification.

* secondly that the auth tokens are exclusively flowing from the browser
to the server or proxy, not the other way around.




> It was the case with Squid 3.3.8, where Squid answers back, along with the response (200 OK), the Proxy-Authentication-Info header with the final token. But with Squid 4.2, the browser doesn't anymore add this headers, preventing the browser to authenticate back the proxy.
> 

Squid-4 has been updated to conform to RFC 7615 requirements of these
*-Authentication-Info headers usage which was in fact only previously
defined for use with Digest authentication scheme - not Negotiate.


> The problem is that it enables some potentially Man in the Middle attack (since any malicious proxy where the traffic is diverted could then answers back without the client knowing it talks to a malicious proxy)
> 

Quite the opposite. The MITM issue you point out is part of the
fundamental design of the Negotiate scheme and exists for both Squid-3
and Squid-4 behaviours.


Having the client use a token provided in-channel from the proxy enables
an MITM observing that channel to inject changes giving itself control
over what the client does in future authentication. This extra control
point is prohibited by the Squid-4 behaviour, and has never been a
formal part of Negotiate scheme as you can see from the RFC 4559 texts.

The design of Negotiate/Kerberos has both client and proxy independently
contact the DC to respectively generate and verify the tokens. All token
operations are performed by the DC itself and contain secrets only the
DC knows. The flow of tokens is exclusively from client to proxy as
proof that the client is already authenticated with the DC. The proxy /
server response is intentionally lacking to starve any MITM of
information it might use to reliably affect changes to the client tokens.



...
> 
> It this behavior endorsed, or is it a bug ?
> 

The Squid-4 behaviour is "endorsed". The Squid-3 behaviour was arguably
a bug or more correctly an experimental feature which has now been
removed due to both the increased MITM risk I describe above and how its
content differs in syntax from the RFC 7615 specification.

Amos


From squid3 at treenet.co.nz  Tue Oct  2 11:27:44 2018
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Wed, 3 Oct 2018 00:27:44 +1300
Subject: [squid-users] [squid-announce] Squid 4.3 is available
Message-ID: <56e9cff4-5292-da8b-ffb6-905dcbcc44d8@treenet.co.nz>

The Squid HTTP Proxy team is very pleased to announce the availability
of the Squid-4.3 release!


This release is a bug fix release resolving several issues found in the
prior Squid releases.


The major changes to be aware of:


* Bug 4885: Excessive memory usage when running out of descriptors

When using some I/O select loops Squid can continue to allocate memory
for client connections after it has reached maximum available FD limits.
This release drops the memory to a fixed amount for each *_port
regardless of how many client connections arrive.


* Bug 4877: Add missing text about external_acl_type %DATA changes

The external_acl_type directive changes to encompass logformat codes
has a side effect on implicit %DATA parameters sent to the helpers.

Previous Squid versions would elide this field sometimes if no data
was contained - but only when implicit. Squid-4 will always send a
value, using '-' when no data exists.

This was omitted in earlier release documentation and caused some
surprises to admin with custom helpers not fully supporting the
current helper protocol.

Any users of this directive with custom helpers written for older
versions of Squid are advised to review what their helper is doing
and ensure that it can cope with the helper protocol fields documented
as 'optional'.


* Bug 4716: Blank lines in cachemgr.conf are not skipped

This bug appears as empty entries in the cachemgr.cgi web interface.
The Squid CGI tool will now elide such entries from display.


* Update systemd dependencies in squid.service

The squid.service file published with earlier releases lacked a
dependency on networking being fully operational. This resulted
in a mix of odd behaviours on machine startup when other networking
tools were slow to initialize NIC, DNS, or resolve.conf settings.

The squid.service file shipped with this release is expected to wait
until all networking resources are operational before Squid is started.



  All users of Squid-4 are encouraged to upgrade as time permits.

  All users of Squid-3 are encouraged to upgrade where possible.


See the ChangeLog for the full list of changes in this and earlier
releases.

Please refer to the release notes at
http://www.squid-cache.org/Versions/v4/RELEASENOTES.html
when you are ready to make the switch to Squid-4

This new release can be downloaded from our HTTP or FTP servers

  http://www.squid-cache.org/Versions/v4/
  ftp://ftp.squid-cache.org/pub/squid/
  ftp://ftp.squid-cache.org/pub/archive/4/

or the mirrors. For a list of mirror sites see

  http://www.squid-cache.org/Download/http-mirrors.html
  http://www.squid-cache.org/Download/mirrors.html

If you encounter any issues with this release please file a bug report.
  http://bugs.squid-cache.org/


Amos Jeffries
_______________________________________________
squid-announce mailing list
squid-announce at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-announce

From squid3 at treenet.co.nz  Tue Oct  2 11:27:44 2018
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Wed, 3 Oct 2018 00:27:44 +1300
Subject: [squid-users] [squid-announce] Squid 4.3 is available
Message-ID: <56e9cff4-5292-da8b-ffb6-905dcbcc44d8@treenet.co.nz>

The Squid HTTP Proxy team is very pleased to announce the availability
of the Squid-4.3 release!


This release is a bug fix release resolving several issues found in the
prior Squid releases.


The major changes to be aware of:


* Bug 4885: Excessive memory usage when running out of descriptors

When using some I/O select loops Squid can continue to allocate memory
for client connections after it has reached maximum available FD limits.
This release drops the memory to a fixed amount for each *_port
regardless of how many client connections arrive.


* Bug 4877: Add missing text about external_acl_type %DATA changes

The external_acl_type directive changes to encompass logformat codes
has a side effect on implicit %DATA parameters sent to the helpers.

Previous Squid versions would elide this field sometimes if no data
was contained - but only when implicit. Squid-4 will always send a
value, using '-' when no data exists.

This was omitted in earlier release documentation and caused some
surprises to admin with custom helpers not fully supporting the
current helper protocol.

Any users of this directive with custom helpers written for older
versions of Squid are advised to review what their helper is doing
and ensure that it can cope with the helper protocol fields documented
as 'optional'.


* Bug 4716: Blank lines in cachemgr.conf are not skipped

This bug appears as empty entries in the cachemgr.cgi web interface.
The Squid CGI tool will now elide such entries from display.


* Update systemd dependencies in squid.service

The squid.service file published with earlier releases lacked a
dependency on networking being fully operational. This resulted
in a mix of odd behaviours on machine startup when other networking
tools were slow to initialize NIC, DNS, or resolve.conf settings.

The squid.service file shipped with this release is expected to wait
until all networking resources are operational before Squid is started.



  All users of Squid-4 are encouraged to upgrade as time permits.

  All users of Squid-3 are encouraged to upgrade where possible.


See the ChangeLog for the full list of changes in this and earlier
releases.

Please refer to the release notes at
http://www.squid-cache.org/Versions/v4/RELEASENOTES.html
when you are ready to make the switch to Squid-4

This new release can be downloaded from our HTTP or FTP servers

  http://www.squid-cache.org/Versions/v4/
  ftp://ftp.squid-cache.org/pub/squid/
  ftp://ftp.squid-cache.org/pub/archive/4/

or the mirrors. For a list of mirror sites see

  http://www.squid-cache.org/Download/http-mirrors.html
  http://www.squid-cache.org/Download/mirrors.html

If you encounter any issues with this release please file a bug report.
  http://bugs.squid-cache.org/


Amos Jeffries
_______________________________________________
squid-announce mailing list
squid-announce at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-announce

From ecoirier at olfeo.com  Tue Oct  2 14:29:10 2018
From: ecoirier at olfeo.com (Emmanuel Coirier)
Date: Tue, 2 Oct 2018 16:29:10 +0200
Subject: [squid-users] Uncomplete Negotiate negotiation with Kerberos
In-Reply-To: <fcfa0f22-8f57-a901-5d38-d0a73b9f90b2@treenet.co.nz>
References: <DEE553E0920CFA41BAF85A4B9C6FC3498F27D10DEB@EXCHANGESRV1.olfeo-lab.net>
 <fcfa0f22-8f57-a901-5d38-d0a73b9f90b2@treenet.co.nz>
Message-ID: <DEE553E0920CFA41BAF85A4B9C6FC3498F27D10E88@EXCHANGESRV1.olfeo-lab.net>

Hi Amos and others,

Thanks for your response, but I'm afraid I'm not sure to have understood everything...

> De : squid-users [mailto:squid-users-bounces at lists.squid-cache.org] De la part 
> de Amos Jeffries

> > When a browser wants to connect to some random HTTP website, it sends a GET 
...
> to be used.
> > 
> 
> That is not true. RFC 4559 section 4
> (<https://tools.ietf.org/html/rfc4559#section-4>) defines how Negotiate scheme 
> operates in HTTP.

Ok

> > The problem is that it enables some potentially Man in the Middle 
> > attack (since any malicious proxy where the traffic is diverted could 
> > then answers back without the client knowing it talks to a malicious 
> > proxy)
> 
> Quite the opposite. The MITM issue you point out is part of the fundamental 
> design of the Negotiate scheme and exists for both Squid-3 and Squid-4 
> behaviours.

Are you telling me that the Negotiate scheme is fundamentally flawed ?
 
> Having the client use a token provided in-channel from the proxy enables an MITM 
> observing that channel to inject changes giving itself control over what the 
> client does in future authentication. This extra control point is prohibited by 

I don't understand this point. If we hypothesize that the proxy sends the tokens (including the last one) generated by the service gssapi back to the client (whatever the means), we can think that the gssapi tokens going from service to client are authenticated and encrypted.

With Kerberos, it is possible since the client previously sends a service ticket which contains a random session key encrypted with the service key. The service can use this session key to authentify its response to the client.

So it is harder for a MITM to fake this last gssapi token, especially if the client wait for it. So I don't see how a MITM could exploit this last token.

Clearly, the http body response could be altered, but it can also be altered in the current situation.

Or, is it right that a client cannot trust any form of service authentication based on Negotiate since it is fundamentally flawed ? And thus this last token has no use ?

> the Squid-4 behaviour, and has never been a formal part of Negotiate scheme as 
> you can see from the RFC 4559 texts.
> 
> The design of Negotiate/Kerberos has both client and proxy independently contact 
> the DC to respectively generate and verify the tokens. All token operations are 

In my undestanding and experiments of Kerberos, the service (here Squid and more precisely its negotiate_kerberos_auth) doesn't contact anything, but only trusts the admin provided keytab (which is just the service key). This service key decrypts a service ticket provided by the client, and if this ticket is fine, the client is authenticated. But it doesn't imply any communication with some KDC (Heimdal for my case) or DC (Or is Microsft Kerberos implementation working differently ?). This has been tested with a KDC shut off when surfing once the service ticket was retrieved.

The client contacts the KDC from time to time to get a service ticket, but it's far less than for each TCP connection.

> performed by the DC itself and contain secrets only the DC knows. The flow of 
> tokens is exclusively from client to proxy as proof that the client is already 
> authenticated with the DC. The proxy / server response is intentionally lacking 
> to starve any MITM of information it might use to reliably affect changes to the 
> client tokens.

This is the point I don't understand. Could you tell me more ?

Thanks

-- 
Emmanuel Coirier

From service.mv at gmail.com  Tue Oct  2 17:54:28 2018
From: service.mv at gmail.com (Service MV)
Date: Tue, 2 Oct 2018 14:54:28 -0300
Subject: [squid-users] Chrome 69
In-Reply-To: <6b380983-66eb-ab1b-2594-4fee5c51a825@treenet.co.nz>
References: <1538423622420-0.post@n4.nabble.com>
 <201810012202.57404.Antony.Stone@squid.open.source.it>
 <1538425687031-0.post@n4.nabble.com>
 <6b380983-66eb-ab1b-2594-4fee5c51a825@treenet.co.nz>
Message-ID: <CA+d==oEEMtco+VfxA1HbtqMm7R7HT-xqOsWfX5n8fZ-AMv_PRw@mail.gmail.com>

Excellent Amos!
I have to admit that I was approaching that conclusion. I did exactly as
you told me and problem solved.
I will consider your recommendations when I need to add more rules.
Best regards!

Gabriel

El lun., 1 de oct. de 2018 a la(s) 18:04, Amos Jeffries (
squid3 at treenet.co.nz) escribi?:

> On 2/10/18 9:28 AM, neok wrote:
> > As far as I know, nothing's changed. The only thing that could have
> happened
> > is that Chrome was updated. I'm reviewing that.
> >
> > This say Firefox when try acces to denied site:
> > The proxy server is refusing connections.
> > Firefox is configured to use a proxy server that is refusing connections.
> >
>
> As you an see from the access.log Squid is presenting 407 auth
> challenged in response to these CONNECT requests.
>
> It is welcome news to hear Chrome is finally been fixed to actually
> perform authentication for CONNECT tunnels. While it may have been okay
> with you the previous behaviour is actually a long-standing Browser bug
> and violation of HTTP on their part.
>
>
>
>
> On 2/10/18 8:53 AM, neok wrote:
> >
> > Now Chrome shows me the login window every time I visit a denied site.
> > I suspect Chrome has been updated and changed its behavior. I'm currently
> > studying that possibility.
> > I'm also rethinking whether the way I'm denying sites is the right one.
>
> ...
> >
> > http_access allow WEB_ACCESS_YT_ONLY LS_youtube
> > http_access deny WEB_ACCESS_YT_ONLY
>
> The "deny WEB_ACCESS_YT_ONLY" line is triggering the re-authentication.
>
> When the reason for denial is due to credentials (ie %LOGIN for the
> external ACL evaluation) a 407 authentication challenge is produced.
>
> Notice that in the config I helped you develop a few days ago all the
> other uses of these external ACL tests for "deny" action are followed by
> another ACL test that is unrelated to the auth process. Usually a regex
> or dstdomain check. It is these other ACL checks which prevent any of
> those lines triggering similar auth challenges as they deny traffic.
>
>
> Since the next thing your config is doing is a "deny all" you can remove
> this "deny WEB_ACCESS_YT_ONLY" line entirely and the popups should
> disappear while still having that traffic denied.
>
>
> If you have other config policies to be added later there are some other
> things you can do. But those will depend on what the other policies are
> going to require, so I wont cover it now. Just be aware you may have to
> revisit that YT access rule when adding other policies.
>
>
> Amos
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20181002/c3128e63/attachment.htm>

From mdacova at netpilot.com  Tue Oct  2 19:07:08 2018
From: mdacova at netpilot.com (Michael Da Cova)
Date: Tue, 2 Oct 2018 19:07:08 +0000
Subject: [squid-users] sky go App
In-Reply-To: <d32d7ddc-ea57-6554-5782-378580a39638@netpilot.com>
References: <d32d7ddc-ea57-6554-5782-378580a39638@netpilot.com>
Message-ID: <BBC83E0B-280C-4AAE-9C92-88E17A9C0AA9@netpilot.com>


> Hi
> 
> general question anyone using "Sky Go App" behind squid proxy, having issues with steaming movies
> 
> -- 
> Michael


From squid3 at treenet.co.nz  Tue Oct  2 22:11:12 2018
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Wed, 3 Oct 2018 11:11:12 +1300
Subject: [squid-users] Uncomplete Negotiate negotiation with Kerberos
In-Reply-To: <DEE553E0920CFA41BAF85A4B9C6FC3498F27D10E88@EXCHANGESRV1.olfeo-lab.net>
References: <DEE553E0920CFA41BAF85A4B9C6FC3498F27D10DEB@EXCHANGESRV1.olfeo-lab.net>
 <fcfa0f22-8f57-a901-5d38-d0a73b9f90b2@treenet.co.nz>
 <DEE553E0920CFA41BAF85A4B9C6FC3498F27D10E88@EXCHANGESRV1.olfeo-lab.net>
Message-ID: <32111d2d-7548-9ef8-67f2-5de9be61e474@treenet.co.nz>

On 3/10/18 3:29 AM, Emmanuel Coirier wrote:
> Hi Amos and others,
> 
> Thanks for your response, but I'm afraid I'm not sure to have understood everything...
> 
>> De : squid-users [mailto:squid-users-bounces at lists.squid-cache.org] De la part 
>> de Amos Jeffries
> 
>>> When a browser wants to connect to some random HTTP website, it sends a GET 
> ...
>> to be used.
>>>
>>
>> That is not true. RFC 4559 section 4
>> (<https://tools.ietf.org/html/rfc4559#section-4>) defines how Negotiate scheme 
>> operates in HTTP.
> 
> Ok
> 
>>> The problem is that it enables some potentially Man in the Middle 
>>> attack (since any malicious proxy where the traffic is diverted could 
>>> then answers back without the client knowing it talks to a malicious 
>>> proxy)
>>
>> Quite the opposite. The MITM issue you point out is part of the fundamental 
>> design of the Negotiate scheme and exists for both Squid-3 and Squid-4 
>> behaviours.
> 
> Are you telling me that the Negotiate scheme is fundamentally flawed ?

No more so than any other authentication scheme. If we assume there is
an MITM on the channel, it can as easily relay the tokens as see them.
The security in Negotiate comes from making an MITM not be able to know
anything about what secrets they hold, nor use those opaque secrets on
other connections.


>  
>> Having the client use a token provided in-channel from the proxy enables an MITM 
>> observing that channel to inject changes giving itself control over what the 
>> client does in future authentication. This extra control point is prohibited by 
> 
> I don't understand this point. If we hypothesize that the proxy sends the tokens (including the last one) generated by the service gssapi back to the client (whatever the means), we can think that the gssapi tokens going from service to client are authenticated and encrypted.

Sorry, I was a bit tired when I wrote that and thinking only of the
Negotiate/Kerberos exchange myself.

There *MAY* be tokens going from proxy to client (eg in Negotiate/NTLM),
but they do so by extending the handshake with extra challenge messages
containing *-Authenticate headers. Not via *-Authentication-Info.


> 
> With Kerberos, it is possible since the client previously sends a service ticket which contains a random session key encrypted with the service key.

The client sends such ticket to the proxy, the proxy sends it to the DC.
The DC tells the proxy ("OK allow" or "no deny"). The HTTP part of
Kerberos is over with the client delivering its token to the proxy.

This lack of to-and-fro with tokens in messages is what makes
Negotiate/Kerberos significantly faster than either NTLM or Negotiate/NTLM.


> The service can use this session key to authentify its response to the client.
> 

Responses are messages. Negotiate does not authenticate any message.

The *TCP connection* is what is being authenticated by Negotiate.

A hypothetical MITM is free to alter any message contents within that
connection, so long as it can replay the Negotiate ticket which gives it
permission to open a TCP connection to the proxy.


> So it is harder for a MITM to fake this last gssapi token, especially if the client wait for it. So I don't see how a MITM could exploit this last token.
> 

What I have been trying to say is that there is no such "last GSSAPI
token" in Kerberos. You are thinking of Negotiate/NTLM contexts here.

And *-Info is informational extra data, not part of the authentication
exchange process.


> Clearly, the http body response could be altered, but it can also be altered in the current situation.
> 

As can tokens in the HTTP headers. With the same 'but'.


> Or, is it right that a client cannot trust any form of service authentication based on Negotiate since it is fundamentally flawed ? And thus this last token has no use ?
> 

All the tokens in Negotiate and context exchange is done with
*-Authenticate headers. Not *-Authentication-Info.

There *may* be context exchange that uses reply headers depending on the
specific context. Or there may not be. That has not changed in Squid AFAIK.


>> the Squid-4 behaviour, and has never been a formal part of Negotiate scheme as 
>> you can see from the RFC 4559 texts.
>>
>> The design of Negotiate/Kerberos has both client and proxy independently contact 
>> the DC to respectively generate and verify the tokens. All token operations are 
> 
> In my undestanding and experiments of Kerberos, the service (here Squid and more precisely its negotiate_kerberos_auth) doesn't contact anything, but only trusts the admin provided keytab (which is just the service key). This service key decrypts a service ticket provided by the client, and if this ticket is fine, the client is authenticated. But it doesn't imply any communication with some KDC (Heimdal for my case) or DC (Or is Microsft Kerberos implementation working differently ?). This has been tested with a KDC shut off when surfing once the service ticket was retrieved.
> 

If that were the case there would be no need for auth_param helpers to
be configured. Squid passes tokens to the helper which handles all
contact and validation with the DC.

Once a token is validated it becomes *the* token for that TCP
connection. Any change to the token terminates that connection.


> The client contacts the KDC from time to time to get a service ticket, but it's far less than for each TCP connection.
> 

That is up to the client and DC settings. So far as Squid is concerned
every new TCP connection needs to be separately authenticated.
The helper informs the proxy what user label to log for that token/ticket.


>> performed by the DC itself and contain secrets only the DC knows. The flow of 
>> tokens is exclusively from client to proxy as proof that the client is already 
>> authenticated with the DC. The proxy / server response is intentionally lacking 
>> to starve any MITM of information it might use to reliably affect changes to the 
>> client tokens.
> 
> This is the point I don't understand. Could you tell me more ?
> 

That is about the best description I can come up with right now. It is
basic PKI key exchange, using separate side channels to the DC instead
of in-channel (HTTP) key/ticket negotiation.


Amos


From squid3 at treenet.co.nz  Tue Oct  2 22:14:02 2018
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Wed, 3 Oct 2018 11:14:02 +1300
Subject: [squid-users] sky go App
In-Reply-To: <BBC83E0B-280C-4AAE-9C92-88E17A9C0AA9@netpilot.com>
References: <d32d7ddc-ea57-6554-5782-378580a39638@netpilot.com>
 <BBC83E0B-280C-4AAE-9C92-88E17A9C0AA9@netpilot.com>
Message-ID: <7de9c26d-11fa-8401-b1e0-91fd64870f2b@treenet.co.nz>

On 3/10/18 8:07 AM, Michael Da Cova wrote:
> 
>> Hi
>>
>> general question anyone using "Sky Go App" behind squid proxy, having issues with steaming movies
>>

More important question:
   Does it use HTTP ?

If the answer is no, then probably no.

Amos



From mdacova at netpilot.com  Wed Oct  3 00:11:51 2018
From: mdacova at netpilot.com (Michael Da Cova)
Date: Wed, 3 Oct 2018 00:11:51 +0000
Subject: [squid-users] sky go App
In-Reply-To: <7de9c26d-11fa-8401-b1e0-91fd64870f2b@treenet.co.nz>
References: <d32d7ddc-ea57-6554-5782-378580a39638@netpilot.com>
 <BBC83E0B-280C-4AAE-9C92-88E17A9C0AA9@netpilot.com>,
 <7de9c26d-11fa-8401-b1e0-91fd64870f2b@treenet.co.nz>
Message-ID: <419FFF21-C7FE-4775-BDF4-444416B75415@netpilot.com>

Yes http protocol is used 

Sent from my iPad

> On 2 Oct 2018, at 23:14, Amos Jeffries <squid3 at treenet.co.nz> wrote:
> 
>> On 3/10/18 8:07 AM, Michael Da Cova wrote:
>> 
>>> Hi
>>> 
>>> general question anyone using "Sky Go App" behind squid proxy, having issues with steaming movies
>>> 
> 
> More important question:
>   Does it use HTTP ?
> 
> If the answer is no, then probably no.
> 
> Amos
> 
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users


From uhlar at fantomas.sk  Wed Oct  3 07:25:43 2018
From: uhlar at fantomas.sk (Matus UHLAR - fantomas)
Date: Wed, 3 Oct 2018 09:25:43 +0200
Subject: [squid-users] sky go App
In-Reply-To: <419FFF21-C7FE-4775-BDF4-444416B75415@netpilot.com>
References: <d32d7ddc-ea57-6554-5782-378580a39638@netpilot.com>
 <BBC83E0B-280C-4AAE-9C92-88E17A9C0AA9@netpilot.com>
 <7de9c26d-11fa-8401-b1e0-91fd64870f2b@treenet.co.nz>
 <419FFF21-C7FE-4775-BDF4-444416B75415@netpilot.com>
Message-ID: <20181003072543.GA4646@fantomas.sk>

>>> On 3/10/18 8:07 AM, Michael Da Cova wrote:
>>>> general question anyone using "Sky Go App" behind squid proxy, having issues with steaming movies

>> On 2 Oct 2018, at 23:14, Amos Jeffries <squid3 at treenet.co.nz> wrote:
>> More important question:
>>   Does it use HTTP ?

On 03.10.18 00:11, Michael Da Cova wrote:
>Yes http protocol is used

- HTTP or HTTPS?
- does it use proxy configuration or dooes it require interception?

-- 
Matus UHLAR - fantomas, uhlar at fantomas.sk ; http://www.fantomas.sk/
Warning: I wish NOT to receive e-mail advertising to this address.
Varovanie: na tuto adresu chcem NEDOSTAVAT akukolvek reklamnu postu.
"Two words: Windows survives." - Craig Mundie, Microsoft senior strategist
"So does syphillis. Good thing we have penicillin." - Matthew Alton


From gkitirisov at plotpad.ru  Wed Oct  3 07:47:47 2018
From: gkitirisov at plotpad.ru (=?UTF-8?B?0JPQtdC+0YDQs9C40Lkg0JrQuNGC0LjRgNC40YHQvtCy?=)
Date: Wed, 3 Oct 2018 10:47:47 +0300
Subject: [squid-users] Convertion special symbols in file's names
Message-ID: <CAH-eTJMe32vPsvhDw=pjhRSa3pgiD0tKwH-TEKN9A0iwoSqtcg@mail.gmail.com>

Hi, guys. I use squid proxy to download files from aws s3 buckets. If
file's name contain special symbols (like +, %, ...) squid convert it
(probably) and bucket can't find the file. For uploading to bucket it
use java service without squid. How can I prevent url convertion in
squid config?
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20181003/be8d1e74/attachment.htm>

From mdacova at netpilot.com  Wed Oct  3 09:12:10 2018
From: mdacova at netpilot.com (Michael Da Cova)
Date: Wed, 3 Oct 2018 09:12:10 +0000
Subject: [squid-users] sky go App
In-Reply-To: <20181003072543.GA4646@fantomas.sk>
References: <d32d7ddc-ea57-6554-5782-378580a39638@netpilot.com>
 <BBC83E0B-280C-4AAE-9C92-88E17A9C0AA9@netpilot.com>
 <7de9c26d-11fa-8401-b1e0-91fd64870f2b@treenet.co.nz>
 <419FFF21-C7FE-4775-BDF4-444416B75415@netpilot.com>
 <20181003072543.GA4646@fantomas.sk>
Message-ID: <fee63316-a2e7-76e8-cd31-1caece6a45cd@netpilot.com>

Hi


On 03/10/18 08:25, Matus UHLAR - fantomas wrote:
>>>> On 3/10/18 8:07 AM, Michael Da Cova wrote:
>>>>> general question anyone using "Sky Go App" behind squid proxy, 
>>>>> having issues with steaming movies
>
>>> On 2 Oct 2018, at 23:14, Amos Jeffries <squid3 at treenet.co.nz> wrote:
>>> More important question:
>>> ? Does it use HTTP ?
>
> On 03.10.18 00:11, Michael Da Cova wrote:
>> Yes http protocol is used
>
> - HTTP or HTTPS?
seems to use both on different urls for adverts and movies
> - does it use proxy configuration or dooes it require interception?
>
tested on both, displays different applications errors,

will provide more details after further testing

version we are using is squid-3.5.27 (i know not the latest)


Michael




From ecoirier at olfeo.com  Wed Oct  3 14:18:51 2018
From: ecoirier at olfeo.com (Emmanuel Coirier)
Date: Wed, 3 Oct 2018 16:18:51 +0200
Subject: [squid-users] Uncomplete Negotiate negotiation with Kerberos
In-Reply-To: <32111d2d-7548-9ef8-67f2-5de9be61e474@treenet.co.nz>
References: <DEE553E0920CFA41BAF85A4B9C6FC3498F27D10DEB@EXCHANGESRV1.olfeo-lab.net>
 <fcfa0f22-8f57-a901-5d38-d0a73b9f90b2@treenet.co.nz>
 <DEE553E0920CFA41BAF85A4B9C6FC3498F27D10E88@EXCHANGESRV1.olfeo-lab.net>
 <32111d2d-7548-9ef8-67f2-5de9be61e474@treenet.co.nz>
Message-ID: <DEE553E0920CFA41BAF85A4B9C6FC3498F27D1103B@EXCHANGESRV1.olfeo-lab.net>

Hi Amos !

First thank you for your patience and your answers.

I agree on the Negotiate/Kerberos workflow, and its security implication. But I have some
last extra questioning about Kerberos.

It seems that we disagree on how Kerberos is supposed to work. See lower :

> > [Client sends a Kerberos Service Ticket to Squid]
> The client sends such ticket to the proxy, the proxy sends it to the DC.

What do you consider as a DC ?

For me and until now, DC meant "Domain Controller", or "Distribution Center"
(shorthand for KDC, Kerberos' Key Distribution Center): the trusted third party
process that can be either MIT/Heimdal Kerberos or MS Active Directory.
Am I right ?

> The DC tells the proxy ("OK allow" or "no deny"). 

I didn't observed any communication between the negotiate_kerberos_auth helper
and the DC. The only form of communication is reading requests on the standard input
(from squid), reading files like a keytab (the service key) and the krb5.cfg
Kerberos Realm configuration file, and writing responses on the standard output (to Squid).
No connect() system call, no socket, no IPC to some DC. (And users where correctly
authenticated, of course)

And it should be fine since the Kerberos protocol doesn't define any communication
between a service and a DC.

Yesterday I tried a Debian Firefox with a Heimdal Kerberos server. This morning,
I've tried to use a Firefox on Windows 10, with an account provided from a
MS Windows Terminal Server 2008 running an Active Directory.

In either case Firefox was configured to pass trough a Squid instance.

This Squid instance was configured like this in the two scenarii :

	auth_param negotiate program /usr/lib/squid/negotiate_kerberos_auth -s GSS_C_NO_NAME
	auth_param negotiate children 1
	auth_param negotiate keep_alive on
	
	acl foobar proxy_auth REQUIRED
	http_access allow foobar

The keytab was configured via the KRB5_KTNAME environment variable like this :

	(in the /etc/default/squid file)
	export KRB5_KTNAME=FILE:/etc/squid/HTTP.keytab

This keytab was generated using the Samba "net ads keytab" command for Active Directory,
and with "kadmin ext" for Heimdal.

> If that were the case there would be no need for auth_param helpers to be 
> configured. Squid passes tokens to the helper which handles all contact and 
> validation with the DC.

My point is that the Kerberos helper doesn't need any contact with the DC. All it
has to know is the service key stored in the keytab file. So each new TCP connection
does not create a contact to the DC. It is what I have observed, but perhaps I'm
missing something.

So I need to be sure of this detail because we need to reduce the authentication
load of some Active Directories (which previously used NTLM), and Kerberos seems
to be the solution. By telling me that the helper contacts the DC on each TCP
connection with Kerberos, you puzzles me...


-- 
Emmanuel Coirier

From ahmed.zaeem at netstream.ps  Wed Oct  3 15:07:44 2018
From: ahmed.zaeem at netstream.ps (--Ahmad--)
Date: Wed, 3 Oct 2018 18:07:44 +0300
Subject: [squid-users] want to change squid name
In-Reply-To: <6975395d-fde0-52fc-259e-e0c4d6acaf26@treenet.co.nz>
References: <46DA2877-D62D-4271-9D3C-E95825FA8174@netstream.ps>
 <ee2ce272-33f3-bb7c-4022-9cdd2cd7d00e@measurement-factory.com>
 <55236617-33AA-45E1-8793-851C72500BB1@netstream.ps>
 <7ed3a3e8-4ed4-e7e6-9ff4-8cc268a625fd@solutti.com.br>
 <7094A6A2-B3A8-4BCC-8154-694E6F61A771@netstream.ps>
 <20181001205530.GB4039@fantomas.sk>
 <d735ccb9-bdba-d5ec-b7c6-3894ca065f27@nanogherkin.com>
 <6975395d-fde0-52fc-259e-e0c4d6acaf26@treenet.co.nz>
Message-ID: <A9A3D311-E411-4796-9CDB-95A7F3237524@netstream.ps>

@Amos

can you help me ?


Thanks

> On 2 Oct 2018, at 10:40, Amos Jeffries <squid3 at treenet.co.nz> wrote:
> 
> On 2/10/18 7:02 PM, Alex Crow wrote:
>> What about this?
>> 
>> http://www.squid-cache.org/Doc/config/via/
>> 
> 
> Irrelevant?
> 
> Amos
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users



From squid3 at treenet.co.nz  Wed Oct  3 16:35:49 2018
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 4 Oct 2018 05:35:49 +1300
Subject: [squid-users] Convertion special symbols in file's names
In-Reply-To: <CAH-eTJMe32vPsvhDw=pjhRSa3pgiD0tKwH-TEKN9A0iwoSqtcg@mail.gmail.com>
References: <CAH-eTJMe32vPsvhDw=pjhRSa3pgiD0tKwH-TEKN9A0iwoSqtcg@mail.gmail.com>
Message-ID: <b55e0e95-67d9-a3f1-581e-8567e7764824@treenet.co.nz>

On 3/10/18 8:47 PM, ??????? ????????? wrote:
> Hi, guys. I use squid proxy to download files from aws s3 buckets. If?
> file's name contain special symbols (like +, %, ...) squid convert it

The characters you point out above are reserved for use as
sub-delimiters in URI / URL syntax.

> (probably)

You are not sure its Squid doing anything at all? check to be sure what
is actually happening before you go looking for solutions to your guessed.

 debug_options 11,2



> and bucket can't find the file. For uploading to bucket it?
> use java service without squid. How can I prevent url convertion in?
> squid config?
> 

Filenames are not URL, and vice versa.

Sending software is responsible for properly URI-encoding any file path
embedded into URI. Receiving software for decoding it out again.


Squid (and HTTP itself) knows nothing of any such concept of "file" -
there are only messages. Squid does however obey RFC 3986 and 7230
requirements for URL/URI handling.


Amos


From squid3 at treenet.co.nz  Wed Oct  3 16:37:00 2018
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 4 Oct 2018 05:37:00 +1300
Subject: [squid-users] want to change squid name
In-Reply-To: <A9A3D311-E411-4796-9CDB-95A7F3237524@netstream.ps>
References: <46DA2877-D62D-4271-9D3C-E95825FA8174@netstream.ps>
 <ee2ce272-33f3-bb7c-4022-9cdd2cd7d00e@measurement-factory.com>
 <55236617-33AA-45E1-8793-851C72500BB1@netstream.ps>
 <7ed3a3e8-4ed4-e7e6-9ff4-8cc268a625fd@solutti.com.br>
 <7094A6A2-B3A8-4BCC-8154-694E6F61A771@netstream.ps>
 <20181001205530.GB4039@fantomas.sk>
 <d735ccb9-bdba-d5ec-b7c6-3894ca065f27@nanogherkin.com>
 <6975395d-fde0-52fc-259e-e0c4d6acaf26@treenet.co.nz>
 <A9A3D311-E411-4796-9CDB-95A7F3237524@netstream.ps>
Message-ID: <c7572f1c-2467-a160-b52d-b6dc2da52684@treenet.co.nz>

On 4/10/18 4:07 AM, --Ahmad-- wrote:
> @Amos
> 
> can you help me ?
> 


@Ahmed, can you ask reasonable questions with supporting detail?


Amos


From ahmed.zaeem at netstream.ps  Wed Oct  3 17:29:07 2018
From: ahmed.zaeem at netstream.ps (--Ahmad--)
Date: Wed, 3 Oct 2018 20:29:07 +0300
Subject: [squid-users] want to change squid name
In-Reply-To: <c7572f1c-2467-a160-b52d-b6dc2da52684@treenet.co.nz>
References: <46DA2877-D62D-4271-9D3C-E95825FA8174@netstream.ps>
 <ee2ce272-33f3-bb7c-4022-9cdd2cd7d00e@measurement-factory.com>
 <55236617-33AA-45E1-8793-851C72500BB1@netstream.ps>
 <7ed3a3e8-4ed4-e7e6-9ff4-8cc268a625fd@solutti.com.br>
 <7094A6A2-B3A8-4BCC-8154-694E6F61A771@netstream.ps>
 <20181001205530.GB4039@fantomas.sk>
 <d735ccb9-bdba-d5ec-b7c6-3894ca065f27@nanogherkin.com>
 <6975395d-fde0-52fc-259e-e0c4d6acaf26@treenet.co.nz>
 <A9A3D311-E411-4796-9CDB-95A7F3237524@netstream.ps>
 <c7572f1c-2467-a160-b52d-b6dc2da52684@treenet.co.nz>
Message-ID: <5CE955D7-8668-473F-94FC-BC7D669691D4@netstream.ps>

i just want to have my own copy that i can  run under my name  i put 


thats all 



> On 3 Oct 2018, at 19:37, Amos Jeffries <squid3 at treenet.co.nz> wrote:
> 
> On 4/10/18 4:07 AM, --Ahmad-- wrote:
>> @Amos
>> 
>> can you help me ?
>> 
> 
> 
> @Ahmed, can you ask reasonable questions with supporting detail?
> 
> 
> Amos



From squid3 at treenet.co.nz  Wed Oct  3 17:37:26 2018
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 4 Oct 2018 06:37:26 +1300
Subject: [squid-users] Uncomplete Negotiate negotiation with Kerberos
In-Reply-To: <DEE553E0920CFA41BAF85A4B9C6FC3498F27D1103B@EXCHANGESRV1.olfeo-lab.net>
References: <DEE553E0920CFA41BAF85A4B9C6FC3498F27D10DEB@EXCHANGESRV1.olfeo-lab.net>
 <fcfa0f22-8f57-a901-5d38-d0a73b9f90b2@treenet.co.nz>
 <DEE553E0920CFA41BAF85A4B9C6FC3498F27D10E88@EXCHANGESRV1.olfeo-lab.net>
 <32111d2d-7548-9ef8-67f2-5de9be61e474@treenet.co.nz>
 <DEE553E0920CFA41BAF85A4B9C6FC3498F27D1103B@EXCHANGESRV1.olfeo-lab.net>
Message-ID: <215ed8c5-b718-562b-2330-6876290de74a@treenet.co.nz>

On 4/10/18 3:18 AM, Emmanuel Coirier wrote:
> Hi Amos !
> 
> First thank you for your patience and your answers.
> 
> I agree on the Negotiate/Kerberos workflow, and its security implication. But I have some
> last extra questioning about Kerberos.
> 
> It seems that we disagree on how Kerberos is supposed to work. See lower :
> 
>>> [Client sends a Kerberos Service Ticket to Squid]
>> The client sends such ticket to the proxy, the proxy sends it to the DC.
> 
> What do you consider as a DC ?
> 
> For me and until now, DC meant "Domain Controller", or "Distribution Center"
> (shorthand for KDC, Kerberos' Key Distribution Center): the trusted third party
> process that can be either MIT/Heimdal Kerberos or MS Active Directory.
> Am I right ?

That matches my understanding.

> 
>> The DC tells the proxy ("OK allow" or "no deny"). 
> 
> I didn't observed any communication between the negotiate_kerberos_auth helper
> and the DC. The only form of communication is reading requests on the standard input
> (from squid), reading files like a keytab (the service key) and the krb5.cfg
> Kerberos Realm configuration file, and writing responses on the standard output (to Squid).
> No connect() system call, no socket, no IPC to some DC. (And users where correctly
> authenticated, of course)
> 
> And it should be fine since the Kerberos protocol doesn't define any communication
> between a service and a DC.
> 

I can only hazard a guess that you were not looking at the right place
or time for the server<->DC communication.

Maybe something to do with your "-s GSS_C_NO_NAME" setting.


> Yesterday I tried a Debian Firefox with a Heimdal Kerberos server. This morning,
> I've tried to use a Firefox on Windows 10, with an account provided from a
> MS Windows Terminal Server 2008 running an Active Directory.
> 
> In either case Firefox was configured to pass trough a Squid instance.
> 
> This Squid instance was configured like this in the two scenarii :
> 
> 	auth_param negotiate program /usr/lib/squid/negotiate_kerberos_auth -s GSS_C_NO_NAME
> 	auth_param negotiate children 1
> 	auth_param negotiate keep_alive on
> 	
> 	acl foobar proxy_auth REQUIRED
> 	http_access allow foobar
> 
> The keytab was configured via the KRB5_KTNAME environment variable like this :
> 
> 	(in the /etc/default/squid file)
> 	export KRB5_KTNAME=FILE:/etc/squid/HTTP.keytab
> 
> This keytab was generated using the Samba "net ads keytab" command for Active Directory,
> and with "kadmin ext" for Heimdal.
> 
>> If that were the case there would be no need for auth_param helpers to be 
>> configured. Squid passes tokens to the helper which handles all contact and 
>> validation with the DC.
> 
> My point is that the Kerberos helper doesn't need any contact with the DC. All it
> has to know is the service key stored in the keytab file. So each new TCP connection
> does not create a contact to the DC. It is what I have observed, but perhaps I'm
> missing something.

AFAIK the keytab file tells Squid helper what credentials *it* is to use
to login to the DC.


Consider, how is the Squid helper to know that a secret token delivered
by a random unknown client was actually generated by the DC and
validating that clients identity?


> 
> So I need to be sure of this detail because we need to reduce the authentication
> load of some Active Directories (which previously used NTLM), and Kerberos seems
> to be the solution. By telling me that the helper contacts the DC on each TCP
> connection with Kerberos, you puzzles me...
> 


I have not said that exactly. I have said that every connection needs to
be *authenticated*. There are optimizations built into kerberos that
*may* be used to minimize and/or speedup DC contact unless it is
specifically needed.


I also see another source of confusion there.


NTLM requires 2 HTTP messages to complete the handshake. Kerberos only
uses 1. This is achieved by removing the initial type-1 and type-2 token
exchange out of HTTP and into side-channels defined by the keytab's and
their related info (eg DNS).

By not causing HTTP requests to be repeated as much and waited for this
change alone produces a massive speed increase (seconds vs milliseconds
of latency) and load reduction (helpers not stuck waiting for next HTTP
message to arrive with more tokens for their incomplete auth process).

Notice that these are details at the HTTP level of things. Optimizations
at the level of things like contact with the DC are minuscule compared
to the above benefits.


Amos


From alex at nanogherkin.com  Wed Oct  3 17:49:28 2018
From: alex at nanogherkin.com (Alex Crow)
Date: Wed, 3 Oct 2018 18:49:28 +0100
Subject: [squid-users] want to change squid name
In-Reply-To: <5CE955D7-8668-473F-94FC-BC7D669691D4@netstream.ps>
References: <46DA2877-D62D-4271-9D3C-E95825FA8174@netstream.ps>
 <ee2ce272-33f3-bb7c-4022-9cdd2cd7d00e@measurement-factory.com>
 <55236617-33AA-45E1-8793-851C72500BB1@netstream.ps>
 <7ed3a3e8-4ed4-e7e6-9ff4-8cc268a625fd@solutti.com.br>
 <7094A6A2-B3A8-4BCC-8154-694E6F61A771@netstream.ps>
 <20181001205530.GB4039@fantomas.sk>
 <d735ccb9-bdba-d5ec-b7c6-3894ca065f27@nanogherkin.com>
 <6975395d-fde0-52fc-259e-e0c4d6acaf26@treenet.co.nz>
 <A9A3D311-E411-4796-9CDB-95A7F3237524@netstream.ps>
 <c7572f1c-2467-a160-b52d-b6dc2da52684@treenet.co.nz>
 <5CE955D7-8668-473F-94FC-BC7D669691D4@netstream.ps>
Message-ID: <c14a28fd-854d-06e4-dc1f-667c31d8faa8@nanogherkin.com>

Hi Ahmad,

I still don't understand properly. Do you want to run Squid as your own 
nonprivileged user, "ahmad" or "stinger", instead of the "squid" or 
"webproxy" user that is the usual in distros? That is easy, but trying 
to sed squid to <something else> in the codebase is likely to fail, 
imagine trying to do that with the Linux kernel!

If that is the case, just set the user and group in the squid.conf and 
make sure that said user/group has the right privileges to access the 
various directories and files it needs. No problem at all, I've done it 
myself.

There should be no need to edit all the source code and recompile for 
this. Then for "hiding" your proxy use there are the other parameters in 
squid.conf that remove HTTP headers (but by doing do, be aware it's an 
RFC violation and hiding that your clients are behind a proxy can cause 
lots of issues, no. 1 being that Google will keep telling you that 
you're launching a DoS attack). It may also have legal implications in 
some countries, especially if you're forwarding for devices/clients not 
owned by you or your organisation.

I'm just curious as to why you have so little detail of what you need in 
this request when other posts you have made have supplied much more 
detail, logs, etc.

I'm sorry if this is all down to a language barrier and English is not 
your first language.? I've posted to German mailing lists when I only 
did 2 years of it at school and it's really hard!

I hope this helps you,

Best regards

Alex




From ahmed.zaeem at netstream.ps  Wed Oct  3 18:05:32 2018
From: ahmed.zaeem at netstream.ps (--Ahmad--)
Date: Wed, 3 Oct 2018 21:05:32 +0300
Subject: [squid-users] want to change squid name
In-Reply-To: <c7572f1c-2467-a160-b52d-b6dc2da52684@treenet.co.nz>
References: <46DA2877-D62D-4271-9D3C-E95825FA8174@netstream.ps>
 <ee2ce272-33f3-bb7c-4022-9cdd2cd7d00e@measurement-factory.com>
 <55236617-33AA-45E1-8793-851C72500BB1@netstream.ps>
 <7ed3a3e8-4ed4-e7e6-9ff4-8cc268a625fd@solutti.com.br>
 <7094A6A2-B3A8-4BCC-8154-694E6F61A771@netstream.ps>
 <20181001205530.GB4039@fantomas.sk>
 <d735ccb9-bdba-d5ec-b7c6-3894ca065f27@nanogherkin.com>
 <6975395d-fde0-52fc-259e-e0c4d6acaf26@treenet.co.nz>
 <A9A3D311-E411-4796-9CDB-95A7F3237524@netstream.ps>
 <c7572f1c-2467-a160-b52d-b6dc2da52684@treenet.co.nz>
Message-ID: <99F180CA-A2F2-477C-8DDF-D9EAAA66A1B5@netstream.ps>

Guys i apologise if im making any disturbance .
consider this question answered .

thank you all for your time .

> On 3 Oct 2018, at 19:37, Amos Jeffries <squid3 at treenet.co.nz> wrote:
> 
> On 4/10/18 4:07 AM, --Ahmad-- wrote:
>> @Amos
>> 
>> can you help me ?
>> 
> 
> 
> @Ahmed, can you ask reasonable questions with supporting detail?
> 
> 
> Amos



From squid3 at treenet.co.nz  Wed Oct  3 18:12:30 2018
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 4 Oct 2018 07:12:30 +1300
Subject: [squid-users] want to change squid name
In-Reply-To: <5CE955D7-8668-473F-94FC-BC7D669691D4@netstream.ps>
References: <46DA2877-D62D-4271-9D3C-E95825FA8174@netstream.ps>
 <ee2ce272-33f3-bb7c-4022-9cdd2cd7d00e@measurement-factory.com>
 <55236617-33AA-45E1-8793-851C72500BB1@netstream.ps>
 <7ed3a3e8-4ed4-e7e6-9ff4-8cc268a625fd@solutti.com.br>
 <7094A6A2-B3A8-4BCC-8154-694E6F61A771@netstream.ps>
 <20181001205530.GB4039@fantomas.sk>
 <d735ccb9-bdba-d5ec-b7c6-3894ca065f27@nanogherkin.com>
 <6975395d-fde0-52fc-259e-e0c4d6acaf26@treenet.co.nz>
 <A9A3D311-E411-4796-9CDB-95A7F3237524@netstream.ps>
 <c7572f1c-2467-a160-b52d-b6dc2da52684@treenet.co.nz>
 <5CE955D7-8668-473F-94FC-BC7D669691D4@netstream.ps>
Message-ID: <f83f7fbd-3d64-49cc-4e26-89eaf6873543@treenet.co.nz>

On 4/10/18 6:29 AM, --Ahmad-- wrote:
> i just want to have my own copy that i can  run under my name  i put 
> 

Lets be blunt then ...

Most of the things I see you asking about are ways to make the Squid
software unidentifiable as being Squid.

Your earlier queries could be mistaken attempts to use Squid as an
"anonymous proxy" for evading local legal issues.


This request though is to obfuscate details right down to names of ABI
symbols used when building Squid. Details which are only visible to the
person or people compiling Squid, not even people looking at binary code
of the built binaries ever see those names.


The obvious conclusion one is led to by the extreme nature of that
changing and the details you provided so far - is that you likely intend
to take the Squid code and present it as some proprietary software of
your own making. In direct violation of the GPL copyright and great
disrespect for the many hundreds of contributors whose work has built
Squid over the past 40 years.

Yet you are expecting our community of people who use and care about
Squid to assist your doing such an action.


Neither that nor other less likely scenarios I can think give me any
confidence that this is a reasonable way to spend my time. So no, I will
not be helping you with this.

Amos


From kbyers at sbhsd.k12.ca.us  Wed Oct  3 20:00:35 2018
From: kbyers at sbhsd.k12.ca.us (Kevin Byers)
Date: Wed, 3 Oct 2018 13:00:35 -0700
Subject: [squid-users] Caching ChromeOS update files
Message-ID: <CAPV2RyDP0bedB2mceiBKNmi_Ht_VYtzrKKz1K1DtCPbuXKj6jQ@mail.gmail.com>

Hi there,

I am trying to setup squid to specifically cache ChromeOS updates for
Chromebooks but I am currently stuck and lacking the proper knowledge
to solve the issue.
Issue #1: The update files are not being cached by squid.
TCP_MISS_ABORTED/206 messages occur in the squid access.log. I haven't
seen a HIT yet.
Issue #2: The update process on the Chromebooks seems to stall when
they are using squid as the proxy. The Chromebook will show the update
progress as a percentage increasing in number, and eventually it stops
changing. After waiting a while with no changes, the Chromebook will
stop the update process.

Here's the Chromebook update download logic that squid needs to adapt to:
A) Chromebooks update by fetching a single file from dl.google.com.
B) The update files can be up to about 500 megabytes.
C) Chromebooks always request a partial download (HTTP 206) even if
they are downloading the full file from the beginning.
D) Chromebooks will resume downloading the file from a prior partial download.
E) The HTTP response headers for the files have age:0 and
cache-control: public,max-age=86400. There is no "partial" attribute.

Do any of these above things make it impossible to cache the update files?

Here's the squid configuration that I am using:  https://pastebin.com/m15g84xv
Essentially, I increased the cache file size limits to 1 GB and
allowed only Chromebook IP traffic to squid.
I am only caching the files in RAM so there is no hard disk cache.
I created a refresh pattern to keep the update files cached for 7 days.
The Chromebooks are using a PAC file which only sends dl.google.com
traffic to squid, so squid is only dealing with dl.google.com traffic.

I am using squid 3.5.27 on ubuntu 18.04 LTS server edition VM with 2
vCPUs and 8 GB RAM.

Kevin Byers


From ahmed.zaeem at netstream.ps  Wed Oct  3 20:13:06 2018
From: ahmed.zaeem at netstream.ps (--Ahmad--)
Date: Wed, 3 Oct 2018 23:13:06 +0300
Subject: [squid-users] want to change squid name
In-Reply-To: <f83f7fbd-3d64-49cc-4e26-89eaf6873543@treenet.co.nz>
References: <46DA2877-D62D-4271-9D3C-E95825FA8174@netstream.ps>
 <ee2ce272-33f3-bb7c-4022-9cdd2cd7d00e@measurement-factory.com>
 <55236617-33AA-45E1-8793-851C72500BB1@netstream.ps>
 <7ed3a3e8-4ed4-e7e6-9ff4-8cc268a625fd@solutti.com.br>
 <7094A6A2-B3A8-4BCC-8154-694E6F61A771@netstream.ps>
 <20181001205530.GB4039@fantomas.sk>
 <d735ccb9-bdba-d5ec-b7c6-3894ca065f27@nanogherkin.com>
 <6975395d-fde0-52fc-259e-e0c4d6acaf26@treenet.co.nz>
 <A9A3D311-E411-4796-9CDB-95A7F3237524@netstream.ps>
 <c7572f1c-2467-a160-b52d-b6dc2da52684@treenet.co.nz>
 <5CE955D7-8668-473F-94FC-BC7D669691D4@netstream.ps>
 <f83f7fbd-3d64-49cc-4e26-89eaf6873543@treenet.co.nz>
Message-ID: <931811FB-043C-43C6-AD8B-3F7A91A89440@netstream.ps>

Dear Amos thats not correct at all .

i no longer need help on my question .



i apologise if i make any disturbance .

Thank you all guys .



> On 3 Oct 2018, at 21:12, Amos Jeffries <squid3 at treenet.co.nz> wrote:
> 
> Lets be blunt then ...
> 
> Most of the things I see you asking about are ways to make the Squid
> software unidentifiable as being Squid.
> 
> Your earlier queries could be mistaken attempts to use Squid as an
> "anonymous proxy" for evading local legal issues.
> 
> 
> This request though is to obfuscate details right down to names of ABI
> symbols used when building Squid. Details which are only visible to the
> person or people compiling Squid, not even people looking at binary code
> of the built binaries ever see those names.
> 
> 
> The obvious conclusion one is led to by the extreme nature of that
> changing and the details you provided so far - is that you likely intend
> to take the Squid code and present it as some proprietary software of
> your own making. In direct violation of the GPL copyright and great
> disrespect for the many hundreds of contributors whose work has built
> Squid over the past 40 years.
> 
> Yet you are expecting our community of people who use and care about
> Squid to assist your doing such an action.
> 
> 
> Neither that nor other less likely scenarios I can think give me any
> confidence that this is a reasonable way to spend my time. So no, I will
> not be helping you with this.
> 
> Amos

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20181003/81cba537/attachment.htm>

From rousskov at measurement-factory.com  Wed Oct  3 23:50:39 2018
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Wed, 3 Oct 2018 17:50:39 -0600
Subject: [squid-users] Caching ChromeOS update files
In-Reply-To: <CAPV2RyDP0bedB2mceiBKNmi_Ht_VYtzrKKz1K1DtCPbuXKj6jQ@mail.gmail.com>
References: <CAPV2RyDP0bedB2mceiBKNmi_Ht_VYtzrKKz1K1DtCPbuXKj6jQ@mail.gmail.com>
Message-ID: <9827d7d2-d907-7459-4b09-6d0e46a863fc@measurement-factory.com>

On 10/03/2018 02:00 PM, Kevin Byers wrote:

> I am trying to setup squid to specifically cache ChromeOS updates for
> Chromebooks but I am currently stuck and lacking the proper knowledge
> to solve the issue.
> Issue #1: The update files are not being cached by squid.
> TCP_MISS_ABORTED/206 messages occur in the squid access.log. I haven't
> seen a HIT yet.
> Issue #2: The update process on the Chromebooks seems to stall when
> they are using squid as the proxy. The Chromebook will show the update
> progress as a percentage increasing in number, and eventually it stops
> changing. After waiting a while with no changes, the Chromebook will
> stop the update process.
> 
> Here's the Chromebook update download logic that squid needs to adapt to:
> A) Chromebooks update by fetching a single file from dl.google.com.
> B) The update files can be up to about 500 megabytes.
> C) Chromebooks always request a partial download (HTTP 206) even if
> they are downloading the full file from the beginning.
> D) Chromebooks will resume downloading the file from a prior partial download.
> E) The HTTP response headers for the files have age:0 and
> cache-control: public,max-age=86400. There is no "partial" attribute.

> Do any of these above things make it impossible to cache the update files?

Not necessarily.

As you already know, Squid does not support caching for partial
responses to Range requests (HTTP 206), but you can tell Squid to
request (and cache) the whole update object using a non-Range request
(HTTP 200) and then dole out its ranges as needed (HTTP 206). You are
already using range_offset_limit for that.

If Chromebook range requests are sequential (and start from the
beginning of the update object), this may work well.

You need to figure out where exactly things fail. For example:

1. On the first Chromebook request, does Squid convert Chromebook GET
Range request into a regular GET request (for the whole object; no Range
header)?

2. On the second Chromebook request, does Squid find the previously
fetched whole object in cache? Does Squid consider that cached entry fresh?

If you cannot figure this out, paste HTTP request and response headers
for relevant messages received _and_ sent by Squid (at least 8 messages
for the above two items), and somebody here will probably spot the
problem. Your "debug_options ALL,2" should log these headers to cache.log.


HTH,

Alex.



> Here's the squid configuration that I am using:  https://pastebin.com/m15g84xv
> Essentially, I increased the cache file size limits to 1 GB and
> allowed only Chromebook IP traffic to squid.
> I am only caching the files in RAM so there is no hard disk cache.
> I created a refresh pattern to keep the update files cached for 7 days.
> The Chromebooks are using a PAC file which only sends dl.google.com
> traffic to squid, so squid is only dealing with dl.google.com traffic.
> 
> I am using squid 3.5.27 on ubuntu 18.04 LTS server edition VM with 2
> vCPUs and 8 GB RAM.



From alex at dvm.esines.cu  Thu Oct  4 18:40:48 2018
From: alex at dvm.esines.cu (=?UTF-8?Q?Alex_Guti=c3=a9rrez_Mart=c3=adnez?=)
Date: Thu, 4 Oct 2018 14:40:48 -0400
Subject: [squid-users] acl declaration
Message-ID: <6427fb6a-eccd-e369-39aa-7f51da1f8770@dvm.esines.cu>

Hi comunity, i'm tryn to set a few acl but is not working, here is what 
i do:


1- time acl

I declare 3 acl, officina for workers, pcinternet for the boss and 
adminred for me, the network admin. Each rule links to a file.

acl oficina src "/etc/squid3/reglas/oc-cu"
acl pcinternet src "/etc/squid3/reglas/oc-internet"
acl adminred src "/etc/squid3/reglas/oc-inf"

this is the file content:

pc-interet : 192.168.0.253

oficina: 192.168.1.0/29

adminred : 192.168.2.0/30


i create a facebook time rule for my squid so i get like this


1rst i create the rule for facebook

acl sociales dstdomain -n "/etc/squid3/reglas/bloqueo/sociales"

this is what the file sociales contains

.facebook.com
.akamaihd.net
.fbcdn.net

2dn i create the schedule

acl trabajo time MTWHF 09:00-12:00 14:00-17:00

3rd i create 1 refusal rule

http_access deny trabajo sociales !pcinternet !adminred oficina


This is my question. Do i write the rule like above or like this, with a 
refusal rule for each acl

http_access deny trabajo sociales !pcinternet

http_access deny trabajo sociales !adminred

http_access deny trabajo sociales oficina


I make this question because both ways are given me trouble at the time 
to block facebook.


I also have trouble with delay_pools, this i what i do

1st i define my delays pools

delay_pools 1

#Canal Facebook
delay_class 1 2
delay_access 1 allow sociales !pc-internet !adminred
delay_access 1 deny all
delay_parameters 1 65536/49152 49152/32768

my problem is adminred is falling into delay pool 1 acording to sqstat.


Any one be so nice to clarify this for me.


Thanks for your time, remember, always attack ideas, never people.


Please forgive my English, this is not my native language


-- 
Saludos Cordiales

Lic. Alex Guti?rrez Mart?nez

Tel. +53 7 2710327

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20181004/f6d0f820/attachment.htm>

From kevin at rentec.com  Thu Oct  4 20:37:23 2018
From: kevin at rentec.com (Kevin Kretz)
Date: Thu, 4 Oct 2018 16:37:23 -0400 (EDT)
Subject: [squid-users] multiple requests from single URL on web page?
Message-ID: <1601203980.83151.1538685443575.JavaMail.zimbra@rentec.com>

Hi, 

I'm using external_acl_type to handle web traffic based on authenticated user and destination. I'm also passing the referer to the external acl. I know referer isn't to be relied on, but right off the bat I see two ACL queries for a single URL. One has a referrer and the other doesn't. 

Here's the acl line from my squid.conf: 

external_acl_type custom_acl_db cache=0 children-max=5 %ACL %DATA %ul %SRC %>rd %{Referer}>h (acl script name) 

Here's the HTML from a very simple page I'm accessing: 

< HTML > < HEAD > </ HEAD > < BODY > < IMG SRC =" [ view-source:http://upload.wikimedia.org/wikipedia/en/thumb/5/53/Snoopy_Peanuts.png/200px-Snoopy_Peanuts.png | http://domain1.com/image1.png ] "> < BR > < IMG SRC =" [ view-source:http://static.rgscdn.com/images/xl/702021.jpg | http://domain2.com/image2.jpg ] "> </ BODY > </ HTML > 

When I access that page through squid, I see one image's URL once, with no referrer, and the other one two times - once with a referrer and once without: 

CustomAclDB - kevin 172.18.127.4 domain2.com [ http://(web/ | http://server_hosting_test_page ] /test/ 

CustomAclDB - kevin 172.18.127.4 domain1.com - 

CustomAclDB - kevin 172.18.127.4 domain2.com - 


Why the inconsistency? 


Thanks 

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20181004/0e7f897a/attachment.htm>

From rousskov at measurement-factory.com  Thu Oct  4 20:55:34 2018
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Thu, 4 Oct 2018 14:55:34 -0600
Subject: [squid-users] multiple requests from single URL on web page?
In-Reply-To: <1601203980.83151.1538685443575.JavaMail.zimbra@rentec.com>
References: <1601203980.83151.1538685443575.JavaMail.zimbra@rentec.com>
Message-ID: <1a6a03a8-db9e-a3d6-7603-451e434abe1e@measurement-factory.com>

On 10/04/2018 02:37 PM, Kevin Kretz wrote:

> I'm using external_acl_type to handle web traffic based on authenticated
> user and destination.? I'm also passing the referer to the external
> acl.?? I know referer isn't to be relied on, but right off the bat I see
> two ACL queries for a single URL.? One has a referrer and the other doesn't.

> Here's the acl line from my squid.conf:

> external_acl_type custom_acl_db cache=0 children-max=5 %ACL %DATA %ul
> %SRC %>rd %{Referer}>h (acl script name)

> Here's the HTML from a very simple page I'm accessing:

> <IMG SRC="http://domain1.com/image1.png">
> <IMG SRC="http://domain2.com/image2.jpg">

> When I access that page through squid, I see one image's URL once, with
> no referrer, and the other one two times - once with a referrer and once
> without:

> ?CustomAclDB - kevin 172.18.127.4 domain2.com http://server_hosting_test_page/test/ 
> ?CustomAclDB - kevin 172.18.127.4 domain1.com - 
> ?CustomAclDB - kevin 172.18.127.4 domain2.com -


> Why the inconsistency?

Do the corresponding HTTP requests received by Squid contain the Referer
header? Use packet captures or "debug_options ALL,2" to see those requests.

Alex.


From rousskov at measurement-factory.com  Thu Oct  4 21:15:22 2018
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Thu, 4 Oct 2018 15:15:22 -0600
Subject: [squid-users] acl declaration
In-Reply-To: <6427fb6a-eccd-e369-39aa-7f51da1f8770@dvm.esines.cu>
References: <6427fb6a-eccd-e369-39aa-7f51da1f8770@dvm.esines.cu>
Message-ID: <3093938c-5ae8-36d1-7f56-b3917bb8b94f@measurement-factory.com>

On 10/04/2018 12:40 PM, Alex Guti?rrez Mart?nez wrote:

> http_access deny trabajo sociales !pcinternet !adminred oficina

This rule denies any request that matches each and every acl expression
listed in the rule. You have 5 such expressions: trabajo, sociales, not
pcinternet, not adminred, and oficina.


> Do i write the rule like above or like this

The correct configuration depends on what you want Squid to do. You have
not provided that information explicitly AFAICT, and I am not going to
guess.


> http_access deny trabajo sociales !pcinternet
> http_access deny trabajo sociales !adminred
> http_access deny trabajo sociales oficina

The above three rules deny any request that matches at least one of the
following three ACL expressions:

1. trabajo and sociales but not pcinternet
2. trabajo and sociales but not adminred
3. trabajo and sociales and oficina


Here is all you need to know to answer your specific question and many
similar ones:

* First, same-rule ACLs are ANDed together (logical conjunction).
  This tells you what an individual http_access rule matches.

* Next, all http_access rules are ORed together (logical disjunction).
  This tells you what http_access configuration, as a whole, matches.

rule[i] = acl1 AND acl2 AND acl3 AND ...
http_access = rule1 OR rule2 OR ...


HTH,

Alex.


From mujtaba21n at hotmail.com  Fri Oct  5 14:59:09 2018
From: mujtaba21n at hotmail.com (Mujtaba   Hassan Madani)
Date: Fri, 5 Oct 2018 14:59:09 +0000
Subject: [squid-users] Squid Cache Server
In-Reply-To: <DB6P193MB00085F75B1438190AFE7B508C61C0@DB6P193MB0008.EURP193.PROD.OUTLOOK.COM>
References: <DB6P193MB0008E0848E133CEB22448158C6040@DB6P193MB0008.EURP193.PROD.OUTLOOK.COM>
 <201809111054.06037.Antony.Stone@squid.open.source.it>
 <DB6P193MB00085E55BC2E60C264EE09A4C61B0@DB6P193MB0008.EURP193.PROD.OUTLOOK.COM>
 <b6e884e9-0aa3-a48a-b529-afcfe4c60f51@treenet.co.nz>
 <HE1P193MB0009EFBF04C8D339B5D23090C61A0@HE1P193MB0009.EURP193.PROD.OUTLOOK.COM>
 <DB6P193MB000820455863488DA32DDA21C6180@DB6P193MB0008.EURP193.PROD.OUTLOOK.COM>,
 <f49f6de9-e3be-360e-cd44-b030f4be4a70@treenet.co.nz>,
 <DB6P193MB00085F75B1438190AFE7B508C61C0@DB6P193MB0008.EURP193.PROD.OUTLOOK.COM>
Message-ID: <DB6P193MB00086BE177598A71A08A69EBC6EB0@DB6P193MB0008.EURP193.PROD.OUTLOOK.COM>

Hi Amos,

   I have my squid service running now on ubuntu desktop version 16.0 the path is /etc/squid/squid.config for the configuration file. my question is where can i check the cached file and traffic volume in KB that been cached ?

________________________________
From: squid-users <squid-users-bounces at lists.squid-cache.org> on behalf of Mujtaba Hassan Madani <mujtaba21n at hotmail.com>
Sent: Wednesday, September 19, 2018 1:46:44 PM
To: Amos Jeffries; squid-users at lists.squid-cache.org
Subject: Re: [squid-users] Squid Cache Server


Hi Amos,

 thanks for your concern, as I informed you Iam looking to install Squid on Ubuntu Linux server for Caching purpose once I kickoff i will notify you to have your assistant.

regards


Mujtaba H,

________________________________
From: squid-users <squid-users-bounces at lists.squid-cache.org> on behalf of Amos Jeffries <squid3 at treenet.co.nz>
Sent: Sunday, September 16, 2018 4:58:37 PM
To: squid-users at lists.squid-cache.org
Subject: Re: [squid-users] Squid Cache Server

On 15/09/18 12:13 PM, Mujtaba   Hassan Madani wrote:
> Hi Amos,
>
>    you did not get back to me about my below concern
>

I responded to your concern about copyright.

I do not see anything else in your messages as expressing a concern to
be responded to.

Amos


> ------------------------------------------------------------------------
> *From:* Mujtaba Hassan Madani
> *Sent:* Thursday, September 13, 2018 5:36:48 PM
>
>
> Hi Amos,
>
>    Iam looking for building a Squid proxy server on Ubuntu for my LAN
> serving up to 25 PC's I just want the maximum potential of the server
> capability to enhance the network performance and gain better users
> expectation of the service.
>

> ------------------------------------------------------------------------
> *From:* Amos Jeffries
> *Sent:* Wednesday, September 12, 2018 2:54:37 PM
>
> On 13/09/18 2:16 AM, Mujtaba   Hassan Madani wrote:
>> Dear Squid Team,
>>
>>      how does content provider prevent it from been cached while passing
>> through squid proxy it's by a copy right law
>
> No. Contents which can be transferred through a proxy are implicitly
> licensed for re-distribution.
>
> Legal issues are usually encountered only around interception or
> modification of content.
>
>
>> or some  encryption is
>
> Sometimes.
>
>> implemented in the traffic ?
>
> and other features built into HTTP protocol.
>
>
>> and where can I find the contents that been
>> cached on my squid proxy ?
>>
>
> Depends on your config. Usually in the machine RAM.
>
> What are you looking for exactly? and why?
>

_______________________________________________
squid-users mailing list
squid-users at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-users
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20181005/5d240cd4/attachment.htm>

From ncherukuri at partycity.com  Fri Oct  5 15:57:39 2018
From: ncherukuri at partycity.com (Cherukuri, Naresh)
Date: Fri, 5 Oct 2018 15:57:39 +0000
Subject: [squid-users] socket failure: (24) Too many open files
Message-ID: <89638057A560FB458C01C197F81C7F5DA6FBCD43@PACERS.amscan.corp>

Hello Squid Group,


I am using squid 3.5.20 as a proxy server.  I Increased the memory from 12 GB to 32 GB and Max file descriptors from "4096" to "8192" and deployed this server into production on 09/26/2018".

I don't have any problem from the past 10 days everything working as expected till today. Now after 10 days for the first time, I got following errors on cache log today. Can someone advise/suggest any ideas here?


Error(s) in /var/log/squid/cache.log: 2018/10/05 11:03:59 kid1| comm_open: socket failure: (24) Too many open files

2018/10/05 11:03:59 kid1| comm_open: socket failure: (24) Too many open files

2018/10/05 11:03:59 kid1| comm_open: socket failure: (24) Too many open files

2018/10/05 11:03:59 kid1| comm_open: socket failure: (24) Too many open files

2018/10/05 11:03:59 kid1| comm_open: socket failure: (24) Too many open files

2018/10/05 11:03:59 kid1| comm_open: socket failure: (24) Too many open files

2018/10/05 11:03:59 kid1| comm_open: socket failure: (24) Too many open files

2018/10/05 11:03:59 kid1| comm_open: socket failure: (24) Too many open files

2018/10/05 11:03:59 kid1| comm_open: socket failure: (24) Too many open files


[n*****@squidprod ~]$ free -m
             total       used       free     shared    buffers     cached
Mem:         32004      15907      16097        138        295      14132
-/+ buffers/cache:       1480      30524
Swap:        24999          0      24999

Thanks,
Naresh
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20181005/0b85d8a0/attachment.htm>

From Antony.Stone at squid.open.source.it  Fri Oct  5 16:34:46 2018
From: Antony.Stone at squid.open.source.it (Antony Stone)
Date: Fri, 5 Oct 2018 18:34:46 +0200
Subject: [squid-users] socket failure: (24) Too many open files
In-Reply-To: <89638057A560FB458C01C197F81C7F5DA6FBCD43@PACERS.amscan.corp>
References: <89638057A560FB458C01C197F81C7F5DA6FBCD43@PACERS.amscan.corp>
Message-ID: <201810051834.46357.Antony.Stone@squid.open.source.it>

On Friday 05 October 2018 at 17:57:39, Cherukuri, Naresh wrote:

> Hello Squid Group,
> 
> 
> I am using squid 3.5.20 as a proxy server.

On what Operating System?

> I Increased the memory from 12 GB to 32 GB

You mean you put more memory into the server, or you re-configured something in 
software (if so, what)?

> and Max file descriptors from "4096" to "8192"

How did you do that?

What does "ulimit -a" tell yoou?

> and deployed this server into production on 09/26/2018".
> 
> I don't have any problem from the past 10 days everything working as
> expected till today.

How many users do you have, what sort of number of connections per 
second/minute/hour (whatever is convenient for you to express) do you have 
going through this machine?

> Now after 10 days for the first time, I got following errors on cache log
> today. Can someone advise/suggest any ideas here?
> 
> 
> Error(s) in /var/log/squid/cache.log: 2018/10/05 11:03:59 kid1| comm_open:
> socket failure: (24) Too many open files
> 
> 2018/10/05 11:03:59 kid1| comm_open: socket failure: (24) Too many open
> files
> 
> 2018/10/05 11:03:59 kid1| comm_open: socket failure: (24) Too many open
> files
> 
> 2018/10/05 11:03:59 kid1| comm_open: socket failure: (24) Too many open
> files
> 
> 2018/10/05 11:03:59 kid1| comm_open: socket failure: (24) Too many open
> files
> 
> 2018/10/05 11:03:59 kid1| comm_open: socket failure: (24) Too many open
> files
> 
> 2018/10/05 11:03:59 kid1| comm_open: socket failure: (24) Too many open
> files
> 
> 2018/10/05 11:03:59 kid1| comm_open: socket failure: (24) Too many open
> files
> 
> 2018/10/05 11:03:59 kid1| comm_open: socket failure: (24) Too many open
> files

What do "cat /proc/sys/fs/file-max" and "cat /proc/sys/fs/file-nr" tell you?

> [n*****@squidprod ~]$ free -m
>              total       used       free     shared    buffers     cached
> Mem:         32004      15907      16097        138        295      14132
> -/+ buffers/cache:       1480      30524
> Swap:        24999          0      24999

However that has nothing to do with files or descriptors.

What does domething ike "ulimit -a" or "lsof | wc" tell you?


Antony.

-- 
Angela Merkel arrives at Paris airport.
"Nationality?" asks the immigration officer.
"German," she replies.
"Occupation?"
"No, just here for a summit conference."

                                                   Please reply to the list;
                                                         please *don't* CC me.


From ncherukuri at partycity.com  Fri Oct  5 19:51:36 2018
From: ncherukuri at partycity.com (Cherukuri, Naresh)
Date: Fri, 5 Oct 2018 19:51:36 +0000
Subject: [squid-users] socket failure: (24) Too many open files
In-Reply-To: <201810051834.46357.Antony.Stone@squid.open.source.it>
References: <89638057A560FB458C01C197F81C7F5DA6FBCD43@PACERS.amscan.corp>
 <201810051834.46357.Antony.Stone@squid.open.source.it>
Message-ID: <89638057A560FB458C01C197F81C7F5DA6FBDF5C@PACERS.amscan.corp>

Thanks for quick turnover!

Please find following details that you requested.

>On what Operating System?
Operating system : Red Hat 7.0

> I Increased the memory from 12 GB to 32 GB

> You mean you put more memory into the server, or you re-configured something in 
software (if so, what)?

We put more memory into server. Before the server has 12 GB and we increase to 32GB.

> and Max file descriptors from "4096" to "8192"

[root at squidprod ~]# cat /etc/squid/squid.conf | grep "max_filedescriptors"
max_filedescriptors 8192

>ulimit -a value
[root at squidprod ~]# ulimit -a
core file size          (blocks, -c) 0
data seg size           (kbytes, -d) unlimited
scheduling priority             (-e) 0
file size               (blocks, -f) unlimited
pending signals                 (-i) 255941
max locked memory       (kbytes, -l) 64
max memory size         (kbytes, -m) unlimited
open files                      (-n) 8192
pipe size            (512 bytes, -p) 8
POSIX message queues     (bytes, -q) 819200
real-time priority              (-r) 0
stack size              (kbytes, -s) 8192
cpu time               (seconds, -t) unlimited
max user processes              (-u) 255941
virtual memory          (kbytes, -v) unlimited
file locks                      (-x) unlimited

[root at squidprod ~]# lsof | wc -l
10875

Thanks,
Naresh
-----Original Message-----
From: squid-users [mailto:squid-users-bounces at lists.squid-cache.org] On Behalf Of Antony Stone
Sent: Friday, October 5, 2018 12:35 PM
To: squid-users at lists.squid-cache.org
Subject: Re: [squid-users] socket failure: (24) Too many open files

On Friday 05 October 2018 at 17:57:39, Cherukuri, Naresh wrote:

> Hello Squid Group,
> 
> 
> I am using squid 3.5.20 as a proxy server.

On what Operating System?

> I Increased the memory from 12 GB to 32 GB

You mean you put more memory into the server, or you re-configured something in 
software (if so, what)?

> and Max file descriptors from "4096" to "8192"

How did you do that?

What does "ulimit -a" tell yoou?

> and deployed this server into production on 09/26/2018".
> 
> I don't have any problem from the past 10 days everything working as
> expected till today.

How many users do you have, what sort of number of connections per 
second/minute/hour (whatever is convenient for you to express) do you have 
going through this machine?

> Now after 10 days for the first time, I got following errors on cache log
> today. Can someone advise/suggest any ideas here?
> 
> 
> Error(s) in /var/log/squid/cache.log: 2018/10/05 11:03:59 kid1| comm_open:
> socket failure: (24) Too many open files
> 
> 2018/10/05 11:03:59 kid1| comm_open: socket failure: (24) Too many open
> files
> 
> 2018/10/05 11:03:59 kid1| comm_open: socket failure: (24) Too many open
> files
> 
> 2018/10/05 11:03:59 kid1| comm_open: socket failure: (24) Too many open
> files
> 
> 2018/10/05 11:03:59 kid1| comm_open: socket failure: (24) Too many open
> files
> 
> 2018/10/05 11:03:59 kid1| comm_open: socket failure: (24) Too many open
> files
> 
> 2018/10/05 11:03:59 kid1| comm_open: socket failure: (24) Too many open
> files
> 
> 2018/10/05 11:03:59 kid1| comm_open: socket failure: (24) Too many open
> files
> 
> 2018/10/05 11:03:59 kid1| comm_open: socket failure: (24) Too many open
> files

What do "cat /proc/sys/fs/file-max" and "cat /proc/sys/fs/file-nr" tell you?

> [n*****@squidprod ~]$ free -m
>              total       used       free     shared    buffers     cached
> Mem:         32004      15907      16097        138        295      14132
> -/+ buffers/cache:       1480      30524
> Swap:        24999          0      24999

However that has nothing to do with files or descriptors.

What does domething ike "ulimit -a" or "lsof | wc" tell you?


Antony.

-- 
Angela Merkel arrives at Paris airport.
"Nationality?" asks the immigration officer.
"German," she replies.
"Occupation?"
"No, just here for a summit conference."

                                                   Please reply to the list;
                                                         please *don't* CC me.
_______________________________________________
squid-users mailing list
squid-users at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-users

From ncherukuri at partycity.com  Fri Oct  5 20:08:16 2018
From: ncherukuri at partycity.com (Cherukuri, Naresh)
Date: Fri, 5 Oct 2018 20:08:16 +0000
Subject: [squid-users] socket failure: (24) Too many open files
References: <89638057A560FB458C01C197F81C7F5DA6FBCD43@PACERS.amscan.corp>
 <201810051834.46357.Antony.Stone@squid.open.source.it> 
Message-ID: <89638057A560FB458C01C197F81C7F5DA6FBE031@PACERS.amscan.corp>

Antony,


For just squid process open files count.

[root at squidprod ~]# lsof -c squid | wc -l
4385

Thanks,
Naresh

-----Original Message-----
From: Cherukuri, Naresh 
Sent: Friday, October 5, 2018 3:52 PM
To: 'Antony Stone'; squid-users at lists.squid-cache.org
Subject: RE: [squid-users] socket failure: (24) Too many open files

Thanks for quick turnover!

Please find following details that you requested.

>On what Operating System?
Operating system : Red Hat 7.0

> I Increased the memory from 12 GB to 32 GB

> You mean you put more memory into the server, or you re-configured something in 
software (if so, what)?

We put more memory into server. Before the server has 12 GB and we increase to 32GB.

> and Max file descriptors from "4096" to "8192"

[root at squidprod ~]# cat /etc/squid/squid.conf | grep "max_filedescriptors"
max_filedescriptors 8192

>ulimit -a value
[root at squidprod ~]# ulimit -a
core file size          (blocks, -c) 0
data seg size           (kbytes, -d) unlimited
scheduling priority             (-e) 0
file size               (blocks, -f) unlimited
pending signals                 (-i) 255941
max locked memory       (kbytes, -l) 64
max memory size         (kbytes, -m) unlimited
open files                      (-n) 8192
pipe size            (512 bytes, -p) 8
POSIX message queues     (bytes, -q) 819200
real-time priority              (-r) 0
stack size              (kbytes, -s) 8192
cpu time               (seconds, -t) unlimited
max user processes              (-u) 255941
virtual memory          (kbytes, -v) unlimited
file locks                      (-x) unlimited

[root at squidprod ~]# lsof | wc -l
10875

Thanks,
Naresh
-----Original Message-----
From: squid-users [mailto:squid-users-bounces at lists.squid-cache.org] On Behalf Of Antony Stone
Sent: Friday, October 5, 2018 12:35 PM
To: squid-users at lists.squid-cache.org
Subject: Re: [squid-users] socket failure: (24) Too many open files

On Friday 05 October 2018 at 17:57:39, Cherukuri, Naresh wrote:

> Hello Squid Group,
> 
> 
> I am using squid 3.5.20 as a proxy server.

On what Operating System?

> I Increased the memory from 12 GB to 32 GB

You mean you put more memory into the server, or you re-configured something in 
software (if so, what)?

> and Max file descriptors from "4096" to "8192"

How did you do that?

What does "ulimit -a" tell yoou?

> and deployed this server into production on 09/26/2018".
> 
> I don't have any problem from the past 10 days everything working as
> expected till today.

How many users do you have, what sort of number of connections per 
second/minute/hour (whatever is convenient for you to express) do you have 
going through this machine?

> Now after 10 days for the first time, I got following errors on cache log
> today. Can someone advise/suggest any ideas here?
> 
> 
> Error(s) in /var/log/squid/cache.log: 2018/10/05 11:03:59 kid1| comm_open:
> socket failure: (24) Too many open files
> 
> 2018/10/05 11:03:59 kid1| comm_open: socket failure: (24) Too many open
> files
> 
> 2018/10/05 11:03:59 kid1| comm_open: socket failure: (24) Too many open
> files
> 
> 2018/10/05 11:03:59 kid1| comm_open: socket failure: (24) Too many open
> files
> 
> 2018/10/05 11:03:59 kid1| comm_open: socket failure: (24) Too many open
> files
> 
> 2018/10/05 11:03:59 kid1| comm_open: socket failure: (24) Too many open
> files
> 
> 2018/10/05 11:03:59 kid1| comm_open: socket failure: (24) Too many open
> files
> 
> 2018/10/05 11:03:59 kid1| comm_open: socket failure: (24) Too many open
> files
> 
> 2018/10/05 11:03:59 kid1| comm_open: socket failure: (24) Too many open
> files

What do "cat /proc/sys/fs/file-max" and "cat /proc/sys/fs/file-nr" tell you?

> [n*****@squidprod ~]$ free -m
>              total       used       free     shared    buffers     cached
> Mem:         32004      15907      16097        138        295      14132
> -/+ buffers/cache:       1480      30524
> Swap:        24999          0      24999

However that has nothing to do with files or descriptors.

What does domething ike "ulimit -a" or "lsof | wc" tell you?


Antony.

-- 
Angela Merkel arrives at Paris airport.
"Nationality?" asks the immigration officer.
"German," she replies.
"Occupation?"
"No, just here for a summit conference."

                                                   Please reply to the list;
                                                         please *don't* CC me.
_______________________________________________
squid-users mailing list
squid-users at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-users

From Antony.Stone at squid.open.source.it  Fri Oct  5 21:06:04 2018
From: Antony.Stone at squid.open.source.it (Antony Stone)
Date: Fri, 5 Oct 2018 23:06:04 +0200
Subject: [squid-users] socket failure: (24) Too many open files
In-Reply-To: <89638057A560FB458C01C197F81C7F5DA6FBE031@PACERS.amscan.corp>
References: <89638057A560FB458C01C197F81C7F5DA6FBCD43@PACERS.amscan.corp>
 <201810051834.46357.Antony.Stone@squid.open.source.it>
 <89638057A560FB458C01C197F81C7F5DA6FBE031@PACERS.amscan.corp>
Message-ID: <201810052306.05160.Antony.Stone@squid.open.source.it>

On Friday 05 October 2018 at 22:08:16, Cherukuri, Naresh wrote:

> For just squid process open files count.
> 
> [root at squidprod ~]# lsof -c squid | wc -l
> 4385

Squid is not the only thing running on this machine...

> [root at squidprod ~]# lsof | wc -l
> 10875

But you seem to have sufficient file descriptors configured *in Squid* (but maybe 
not in the O/S):

> [root at squidprod ~]# cat /etc/squid/squid.conf | grep "max_filedescriptors"
> max_filedescriptors 8192

So, Squid can have 8192 FDs.

> ulimit -a value
> [root at squidprod ~]# ulimit -a
> open files                      (-n) 8192

...and the system will provide 8192 FDs for every process combined...

> [root at squidprod ~]# lsof | wc -l
> 10875

I reckon that may well be your problem - you have a system-wide limit of 8192 
file descriptors, and yet you are trying to use 10875 open files (this will 
include local pipes, sockets, etc, so it's understandable that it's higher, 
but it indicates you're going over the limit).

> Thanks,
> Naresh

And, as I asked previously:

What do "cat /proc/sys/fs/file-max" and "cat /proc/sys/fs/file-nr" tell you?

How many users do you have, what sort of number of connections per 
second/minute/hour (whatever is convenient for you to express) do you have 
going through this machine?


Antony.

-- 
A good conversation is like a miniskirt;
short enought to retain interest,
but long enough to cover the subject.

 - Celeste Headlee


                                                   Please reply to the list;
                                                         please *don't* CC me.


From squid3 at treenet.co.nz  Sat Oct  6 04:41:23 2018
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sat, 6 Oct 2018 17:41:23 +1300
Subject: [squid-users] Squid Cache Server
In-Reply-To: <DB6P193MB000839D7CA40427D251760F2C6EB0@DB6P193MB0008.EURP193.PROD.OUTLOOK.COM>
References: <DB6P193MB0008E0848E133CEB22448158C6040@DB6P193MB0008.EURP193.PROD.OUTLOOK.COM>
 <201809111054.06037.Antony.Stone@squid.open.source.it>
 <DB6P193MB00085E55BC2E60C264EE09A4C61B0@DB6P193MB0008.EURP193.PROD.OUTLOOK.COM>
 <b6e884e9-0aa3-a48a-b529-afcfe4c60f51@treenet.co.nz>
 <HE1P193MB0009EFBF04C8D339B5D23090C61A0@HE1P193MB0009.EURP193.PROD.OUTLOOK.COM>
 <DB6P193MB000820455863488DA32DDA21C6180@DB6P193MB0008.EURP193.PROD.OUTLOOK.COM>
 <f49f6de9-e3be-360e-cd44-b030f4be4a70@treenet.co.nz>
 <DB6P193MB00085F75B1438190AFE7B508C61C0@DB6P193MB0008.EURP193.PROD.OUTLOOK.COM>
 <DB6P193MB00086BE177598A71A08A69EBC6EB0@DB6P193MB0008.EURP193.PROD.OUTLOOK.COM>
 <DB6P193MB000839D7CA40427D251760F2C6EB0@DB6P193MB0008.EURP193.PROD.OUTLOOK.COM>
Message-ID: <569c89ea-ea91-70c1-22db-4e6407716f23@treenet.co.nz>

On 6/10/18 6:22 AM, Mujtaba   Hassan Madani wrote:
> Hi,
> 
> ? I also get the attached error while been trying to access through web
> interface for private IP address. I managed to add the IP on the bypass
> proxy server for the local address but, i cant do for all since I have
> many private IP i use to login to?
> 

The "error" says you do not have any web server running at the IP
address 172.17.2.1 port 80.

Or if there is, the network does not permit Squid to make TCP
connections to it.


If you mean the *Squid* built-in web interface (aka manager reports).
You are missing the public hostname of the proxy, the forward-proxy port
and the report name (path) being fetched.
 http://sudasat-hp-compaq-dc7600-convertible-minitower.local:3128/squid-internal-mgr/info

(you really should find a simpler domain name for your proxy to use,
that is quite a long name).

> regards
> 
> 
> Mujtaba H
> 
> ------------------------------------------------------------------------
> *From:* Mujtaba Hassan Madani <mujtaba21n at hotmail.com>
> *Sent:* Friday, October 5, 2018 2:59:09 PM
> ?
> 
> Hi Amos,
> 
> ? ?I have my squid service running now on ubuntu desktop version 16.0
> the path is /etc/squid/squid.config for the configuration file.

Okay. I'm not sure what relevance the location of the squid.conf file
has. Usually the settings inside it are needed to answer questions about
your proxies behaviour. But there are no such questions so far in this
thread.


> my
> question is where can i check the cached file and?traffic volume in KB
> that been cached ?

The "info" management report from Squid will tell you that.

The URL for that will be like:
  http://your-proxies-domain-name:3128/squid-internal-mgr/info


Amos


From mujtaba21n at hotmail.com  Sat Oct  6 13:36:54 2018
From: mujtaba21n at hotmail.com (Mujtaba   Hassan Madani)
Date: Sat, 6 Oct 2018 13:36:54 +0000
Subject: [squid-users] Squid Cache Server
In-Reply-To: <569c89ea-ea91-70c1-22db-4e6407716f23@treenet.co.nz>
References: <DB6P193MB0008E0848E133CEB22448158C6040@DB6P193MB0008.EURP193.PROD.OUTLOOK.COM>
 <201809111054.06037.Antony.Stone@squid.open.source.it>
 <DB6P193MB00085E55BC2E60C264EE09A4C61B0@DB6P193MB0008.EURP193.PROD.OUTLOOK.COM>
 <b6e884e9-0aa3-a48a-b529-afcfe4c60f51@treenet.co.nz>
 <HE1P193MB0009EFBF04C8D339B5D23090C61A0@HE1P193MB0009.EURP193.PROD.OUTLOOK.COM>
 <DB6P193MB000820455863488DA32DDA21C6180@DB6P193MB0008.EURP193.PROD.OUTLOOK.COM>
 <f49f6de9-e3be-360e-cd44-b030f4be4a70@treenet.co.nz>
 <DB6P193MB00085F75B1438190AFE7B508C61C0@DB6P193MB0008.EURP193.PROD.OUTLOOK.COM>
 <DB6P193MB00086BE177598A71A08A69EBC6EB0@DB6P193MB0008.EURP193.PROD.OUTLOOK.COM>
 <DB6P193MB000839D7CA40427D251760F2C6EB0@DB6P193MB0008.EURP193.PROD.OUTLOOK.COM>,
 <569c89ea-ea91-70c1-22db-4e6407716f23@treenet.co.nz>
Message-ID: <DB6P193MB00087DBBD8CE1D0727E2F886C6E40@DB6P193MB0008.EURP193.PROD.OUTLOOK.COM>

if iam not mistaken then i need to change my proxy server host name to simpler one and log in by below URL to check the info management about caching files and it relevant size

http://sudasat-hp-compaq-dc7600-convertible-minitower.local:3128/squid-internal-mgr/info

am i right ?

________________________________
From: squid-users <squid-users-bounces at lists.squid-cache.org> on behalf of Amos Jeffries <squid3 at treenet.co.nz>
Sent: Saturday, October 6, 2018 4:41:23 AM
To: squid-users at lists.squid-cache.org
Subject: Re: [squid-users] Squid Cache Server

On 6/10/18 6:22 AM, Mujtaba   Hassan Madani wrote:
> Hi,
>
>   I also get the attached error while been trying to access through web
> interface for private IP address. I managed to add the IP on the bypass
> proxy server for the local address but, i cant do for all since I have
> many private IP i use to login to
>

The "error" says you do not have any web server running at the IP
address 172.17.2.1 port 80.

Or if there is, the network does not permit Squid to make TCP
connections to it.


If you mean the *Squid* built-in web interface (aka manager reports).
You are missing the public hostname of the proxy, the forward-proxy port
and the report name (path) being fetched.
 http://sudasat-hp-compaq-dc7600-convertible-minitower.local:3128/squid-internal-mgr/info

(you really should find a simpler domain name for your proxy to use,
that is quite a long name).

> regards
>
>
> Mujtaba H
>
> ------------------------------------------------------------------------
> *From:* Mujtaba Hassan Madani <mujtaba21n at hotmail.com>
> *Sent:* Friday, October 5, 2018 2:59:09 PM
>
>
> Hi Amos,
>
>    I have my squid service running now on ubuntu desktop version 16.0
> the path is /etc/squid/squid.config for the configuration file.

Okay. I'm not sure what relevance the location of the squid.conf file
has. Usually the settings inside it are needed to answer questions about
your proxies behaviour. But there are no such questions so far in this
thread.


> my
> question is where can i check the cached file and traffic volume in KB
> that been cached ?

The "info" management report from Squid will tell you that.

The URL for that will be like:
  http://your-proxies-domain-name:3128/squid-internal-mgr/info


Amos
_______________________________________________
squid-users mailing list
squid-users at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-users
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20181006/f479844f/attachment.htm>

From eliezer at ngtech.co.il  Sat Oct  6 19:41:03 2018
From: eliezer at ngtech.co.il (Eliezer Croitoru)
Date: Sat, 06 Oct 2018 22:41:03 +0300
Subject: [squid-users] redirect based on url (302)
In-Reply-To: <f35964b6-e83b-c5b6-5563-602ab666d964@treenet.co.nz>
References: <1537541034108-0.post@n4.nabble.com>
 <dde485b6-5742-d424-8e12-f700d0b975f2@treenet.co.nz>
 <1537771119646-0.post@n4.nabble.com>
 <f35964b6-e83b-c5b6-5563-602ab666d964@treenet.co.nz>
Message-ID: <756817fef810e5ce9a288daa8ed678d5@ngtech.co.il>

Amos,

Would an ICAP service that sits on the RESPMOD vector would be a better 
solution other then opening a new session?

Thanks,
Eliezer

On 2018-09-24 12:30, Amos Jeffries wrote:
> On 24/09/18 6:38 PM, uppsalanet wrote:
>> Hi Amos,
>> Today I have a conf like this:
>> ....
>> acl *LIB_domains* dstdomain .almedalsbiblioteket.se .alvin-portal.org
>> .bibliotekuppsala.se
>> http_access allow *LIB_domains*
>> ....
>> 
>> Now I also need to open for *.browzine.com*. The problem with
>> *.browzine.com* is that it is a portal with many links to other sites. 
>> So I
>> basically need to open up and maintain 400 sites in a squid ACL.
>> 
>> I would like to take another approach then (but I don't know if it's
>> possible):
>> I know that browzine.com will reply 302 when trying to access a link 
>> on
>> their site. *So I would like to accept all redirect (302) sites from
>> browzine.com*.
> 
> Aha, that is clearer. Thank you.
> 
> I think you can possibly achieve this, but *only* because of those 302
> existing. If the site were just a collection of links it would be very
> much more difficult.
> 
> 
> What I am thinking of is to use a custom external ACL script that
> creates a temporary browsing session for a client when the 302 arrives
> then the SQL session helper to allow matching traffic through for the
> followup request from that client.
> 
> You will need a database with a table created like this:
> 
>  CREATE TABLE sessions (
>   id VARCHAR(256) NOT NULL PRIMARY KEY,
>   enabled DATETIME NOT NULL DEFAULT CURRENT_TIMESTAMP
> )
> 
> You need to write a script which receives an IP and a URL from Squid,
> extracts the domain name from the URL, then adds a string "$ip $domain"
> to that table as the id column, then returns the "OK" result to Squid.
> 
> The page at
> <http://www.squid-cache.org/Versions/v4/manuals/ext_sql_session_acl.html> 
> has
> details of the SQL session helper that uses that table to check for
> whitelisted domains.
> 
> 
> Your config would look like:
> 
>  acl 302 http_status 302
>  acl browzine dstdomain .browzine.com
> 
>  external_acl_type whitelist_add %SRC %{Location} \
>   /path/to/whitelist_script
> 
>  acl add_to_whitelist external whitelist_add
> 
>  http_reply_access allow browzine 302 add_to_whitelist
>  http_reply_access allow all
> 
> 
>  external_acl_type whitelist ttl=60 %SRC %DST \
>    /usr/lib/squid/ext_session_db_acl \
>    --dsn ... --user ... --password ... \
>    --table sessions --cond ""
> 
>  acl whitelisted external whitelist
>  http_access allow whitelisted
> 
> 
> To have sessions expire simply remove them from the database table.
> Squid will start rejecting traffic there within 60 seconds of the 
> removal.
> 
> HTH
> Amos
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users

-- 
----
Eliezer Croitoru
Linux System Administrator
Mobile: +972-5-28704261
Email: eliezer at ngtech.co.il


From squid3 at treenet.co.nz  Sat Oct  6 22:44:45 2018
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sun, 7 Oct 2018 11:44:45 +1300
Subject: [squid-users] redirect based on url (302)
In-Reply-To: <756817fef810e5ce9a288daa8ed678d5@ngtech.co.il>
References: <1537541034108-0.post@n4.nabble.com>
 <dde485b6-5742-d424-8e12-f700d0b975f2@treenet.co.nz>
 <1537771119646-0.post@n4.nabble.com>
 <f35964b6-e83b-c5b6-5563-602ab666d964@treenet.co.nz>
 <756817fef810e5ce9a288daa8ed678d5@ngtech.co.il>
Message-ID: <0aaa62b3-3030-9976-a95c-3fc15b0089e1@treenet.co.nz>

On 7/10/18 8:41 AM, Eliezer Croitoru wrote:
> Amos,
> 
> Would an ICAP service that sits on the RESPMOD vector would be a better
> solution other then opening a new session?
> 

"Opening a new session" is what any such ICAP would have to do. It is
also overkill for that small action.

Amos


From nldo.tut.alberto at gmail.com  Sun Oct  7 15:33:14 2018
From: nldo.tut.alberto at gmail.com (Ikari C)
Date: Sun, 7 Oct 2018 10:33:14 -0500
Subject: [squid-users] Analize squid3 por conexion
Message-ID: <CACmb8SmTMaWzS_D7fu_pYNWXmgCSwy5DOuZvWo+kXvnnKXcPSw@mail.gmail.com>

Hi, i use squid3 for proxy in my lan, and it's works very well, but i wish
analize the log in real time per conecction and the speed that conection
use , for example, i want to see the traffic by  host 192.168.0.2 (all the
conecctions or talks to the host), somthing like htop but for the
"access.log", what kind of sofware con you recommend me? thanks for your
time.

P.D. sorry for my english
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20181007/53c8b218/attachment.htm>

From eliezer at ngtech.co.il  Sun Oct  7 15:37:45 2018
From: eliezer at ngtech.co.il (Eliezer Croitoru)
Date: Sun, 07 Oct 2018 18:37:45 +0300
Subject: [squid-users] redirect based on url (302)
In-Reply-To: <0aaa62b3-3030-9976-a95c-3fc15b0089e1@treenet.co.nz>
References: <1537541034108-0.post@n4.nabble.com>
 <dde485b6-5742-d424-8e12-f700d0b975f2@treenet.co.nz>
 <1537771119646-0.post@n4.nabble.com>
 <f35964b6-e83b-c5b6-5563-602ab666d964@treenet.co.nz>
 <756817fef810e5ce9a288daa8ed678d5@ngtech.co.il>
 <0aaa62b3-3030-9976-a95c-3fc15b0089e1@treenet.co.nz>
Message-ID: <f3ad55d501126ecc161f139352e5e0ee@ngtech.co.il>

Hey Amos,

I still believe that if squid will manage the connections and the ICAP 
service will maintain the ACL list based on these 302
it would be much faster then opening new connections to the WWW.
If bandwidth and CPU or other resources are not an issue and all the 
requests will receive only and only public domains..
then I would agree that an ICAP service is not required.

An external_acl is a good way and I believe that a proper DB should be 
used.
 From what I remember the last time I have used sqlite3 I had an issue 
when two helpers accessed the DB at the same time for write.

Eliezer

On 2018-10-07 01:44, Amos Jeffries wrote:
> On 7/10/18 8:41 AM, Eliezer Croitoru wrote:
>> Amos,
>> 
>> Would an ICAP service that sits on the RESPMOD vector would be a 
>> better
>> solution other then opening a new session?
>> 
> 
> "Opening a new session" is what any such ICAP would have to do. It is
> also overkill for that small action.
> 
> Amos

-- 
----
Eliezer Croitoru
Linux System Administrator
Mobile: +972-5-28704261
Email: eliezer at ngtech.co.il


From augustus_meyer at gmx.net  Sun Oct  7 18:09:47 2018
From: augustus_meyer at gmx.net (reinerotto)
Date: Sun, 7 Oct 2018 13:09:47 -0500 (CDT)
Subject: [squid-users] Running Squid fully as root
In-Reply-To: <201805172251.00741.Antony.Stone@squid.open.source.it>
References: <CAOA5C67o0mr7wunn97USeSznYviH-d=ffyNmRn4mqTSukxryeg@mail.gmail.com>
 <201805172251.00741.Antony.Stone@squid.open.source.it>
Message-ID: <1538935787950-0.post@n4.nabble.com>

At least, I have a good reason: Running squid on openwrt, where usually all
processes are root.
And external acl-helpers will not work, when started as nobody and trying to
run other processes.
Any answer to the original question ?



--
Sent from: http://squid-web-proxy-cache.1019090.n4.nabble.com/Squid-Users-f1019091.html


From squid3 at treenet.co.nz  Sun Oct  7 22:52:44 2018
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Mon, 8 Oct 2018 11:52:44 +1300
Subject: [squid-users] redirect based on url (302)
In-Reply-To: <f3ad55d501126ecc161f139352e5e0ee@ngtech.co.il>
References: <1537541034108-0.post@n4.nabble.com>
 <dde485b6-5742-d424-8e12-f700d0b975f2@treenet.co.nz>
 <1537771119646-0.post@n4.nabble.com>
 <f35964b6-e83b-c5b6-5563-602ab666d964@treenet.co.nz>
 <756817fef810e5ce9a288daa8ed678d5@ngtech.co.il>
 <0aaa62b3-3030-9976-a95c-3fc15b0089e1@treenet.co.nz>
 <f3ad55d501126ecc161f139352e5e0ee@ngtech.co.il>
Message-ID: <116e6615-3ad9-8c1e-72da-9f6c60f3cb4b@treenet.co.nz>

On 8/10/18 4:37 AM, Eliezer Croitoru wrote:
> Hey Amos,
> 
> I still believe that if squid will manage the connections and the ICAP
> service will maintain the ACL list based on these 302
> it would be much faster then opening new connections to the WWW.

Where are you getting this "new connections to the WWW" idea?

My suggestion does not involve any extra connections.

Amos


From squid3 at treenet.co.nz  Sun Oct  7 23:11:20 2018
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Mon, 8 Oct 2018 12:11:20 +1300
Subject: [squid-users] Running Squid fully as root
In-Reply-To: <1538935787950-0.post@n4.nabble.com>
References: <CAOA5C67o0mr7wunn97USeSznYviH-d=ffyNmRn4mqTSukxryeg@mail.gmail.com>
 <201805172251.00741.Antony.Stone@squid.open.source.it>
 <1538935787950-0.post@n4.nabble.com>
Message-ID: <127c23f5-3fd6-1427-4258-50d2cd4bf631@treenet.co.nz>

On 8/10/18 7:09 AM, reinerotto wrote:
> At least, I have a good reason: Running squid on openwrt, where usually all
> processes are root.

That does not sound right to me. OpenWRT is a Linux based operating
system. The security model in Linux systems is to *not* run processes as
root user unless absolutely necessary.

The Squid master process is started *by* root because it must be
assigned some SUID privileges to special network sockets and to
sub-assign regular privileges to the worker and helper processes that do
the actual networking I/O stuff.


> And external acl-helpers will not work, when started as nobody and trying to
> run other processes.
> Any answer to the original question ?
> 

The Squid worker and helper processes handle raw I/O from remote network
locations which cannot be trusted. It is extremely unsafe to run any
process handling such I/O with root level privileges.

Helpers do not have to be started as "nobody". They can be run as any
low-privilege account. "root" account is not low-privilege enough.

So the simple answer to your question is "no". But your problem may not
be what you think it is. Is there anything you can provide about any
error you are seeing when starting Squid?


Amos


From squid3 at treenet.co.nz  Sun Oct  7 23:20:49 2018
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Mon, 8 Oct 2018 12:20:49 +1300
Subject: [squid-users] Analize squid3 por conexion
In-Reply-To: <CACmb8SmTMaWzS_D7fu_pYNWXmgCSwy5DOuZvWo+kXvnnKXcPSw@mail.gmail.com>
References: <CACmb8SmTMaWzS_D7fu_pYNWXmgCSwy5DOuZvWo+kXvnnKXcPSw@mail.gmail.com>
Message-ID: <1870c9dd-19d5-c2e2-ee13-27ad216c91c3@treenet.co.nz>

On 8/10/18 4:33 AM, Ikari C wrote:
> Hi, i use squid3 for proxy in my lan, and it's works very well, but i
> wish analize the log in real time per conecction and the speed that
> conection use , for example, i want to see the traffic by? host
> 192.168.0.2 (all the conecctions or talks to the host), somthing like
> htop but for the "access.log", what kind of sofware con you recommend
> me? thanks for your time.
> 

There is a list of software known to be able to process Squid log files
at <http://www.squid-cache.org/Misc/log-analysis.html>. Most of these
are for post-processing of the log data though.

Personally I use the DB logging daemon shipped with Squid to store data
into a database and custom report generators based on squidview to
display the data customers need/want to see.

Amos


From rousskov at measurement-factory.com  Sun Oct  7 23:22:04 2018
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Sun, 7 Oct 2018 17:22:04 -0600
Subject: [squid-users] Analize squid3 por conexion
In-Reply-To: <CACmb8SmTMaWzS_D7fu_pYNWXmgCSwy5DOuZvWo+kXvnnKXcPSw@mail.gmail.com>
References: <CACmb8SmTMaWzS_D7fu_pYNWXmgCSwy5DOuZvWo+kXvnnKXcPSw@mail.gmail.com>
Message-ID: <355bcf38-8b81-9dc1-0012-1fb760bedcd4@measurement-factory.com>

On 10/07/2018 09:33 AM, Ikari C wrote:
> Hi, i use squid3 for proxy in my lan, and it's works very well, but i
> wish analize the log in real time per conecction and the speed that
> conection use , for example, i want to see the traffic by? host
> 192.168.0.2 (all the conecctions or talks to the host), somthing like
> htop but for the "access.log", what kind of sofware con you recommend
> me? thanks for your time.

No software can give you a "live" or "real time" view of Squid
connections based on access.log info because access.log only contains
records of past/completed transactions, not current/live ones.

I do not think there is any Squid interface that was designed (and is
appropriate) for true live traffic monitoring, but you may get some
"current usage" information from the Cache Manager interface:
https://wiki.squid-cache.org/Features/CacheManager

If you are content with only looking at past/finished/completed
transactions, then there are probably some tools that can be fed
access.log entries as Squid produces them and that can display nearly
"live" statistics based on those entries, but I personally cannot
recommend any.
 Alex.


From augustus_meyer at gmx.net  Mon Oct  8 05:41:05 2018
From: augustus_meyer at gmx.net (reinerotto)
Date: Mon, 8 Oct 2018 00:41:05 -0500 (CDT)
Subject: [squid-users] Running Squid fully as root
In-Reply-To: <127c23f5-3fd6-1427-4258-50d2cd4bf631@treenet.co.nz>
References: <CAOA5C67o0mr7wunn97USeSznYviH-d=ffyNmRn4mqTSukxryeg@mail.gmail.com>
 <201805172251.00741.Antony.Stone@squid.open.source.it>
 <1538935787950-0.post@n4.nabble.com>
 <127c23f5-3fd6-1427-4258-50d2cd4bf631@treenet.co.nz>
Message-ID: <1538977265040-0.post@n4.nabble.com>

The problem is the external ACL-helper, started by squid. It runs as "nobody"
actually, but it needs to start a special program, which must run as root. 
FYI, openwrt is a shrinked-down LINUX, for embedded systems with limited
resources, without any user besides the admin. 
openwrt often is used for smart routers, for example. Practically all
processes run as root.




--
Sent from: http://squid-web-proxy-cache.1019090.n4.nabble.com/Squid-Users-f1019091.html


From morteza1131 at gmail.com  Mon Oct  8 07:22:44 2018
From: morteza1131 at gmail.com (morteza omidian)
Date: Mon, 8 Oct 2018 07:22:44 +0000 (UTC)
Subject: [squid-users] Netfilter Mark
References: <1875709000.710975.1538983364782.ref@mail.yahoo.com>
Message-ID: <1875709000.710975.1538983364782@mail.yahoo.com>

HiIs it possible to create an acl based on netfilter mark in squid??for example:acl My_client NFMARK 0x1tcp_outgoing_mark 0x1 My_client
I want to keep clients packets mark after packets go out to the server!
tanx
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20181008/dbf9a3da/attachment.htm>

From squid3 at treenet.co.nz  Mon Oct  8 08:03:16 2018
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Mon, 8 Oct 2018 21:03:16 +1300
Subject: [squid-users] Netfilter Mark
In-Reply-To: <1875709000.710975.1538983364782@mail.yahoo.com>
References: <1875709000.710975.1538983364782.ref@mail.yahoo.com>
 <1875709000.710975.1538983364782@mail.yahoo.com>
Message-ID: <7422e1d6-055e-a100-cec3-ed2776a735fd@treenet.co.nz>

On 8/10/18 8:22 PM, morteza omidian wrote:
> Hi
> Is it possible to create an acl based on netfilter mark in squid??
> for example:
> acl My_client NFMARK 0x1
> tcp_outgoing_mark 0x1 My_client
> 
> I want to keep clients packets mark after packets go out to the server!
> 


What you are looking for is the "clientside_mark" ACL type, added in
Squid-4.

However, please be aware that not all traffic leaving Squid has a client
TCP connection. Also that HTTP itself is both multiplexed and pipelined.
So TCP packets on a server connection may contain HTTP messages
servicing multiple clients simultaneously.

Amos


From morteza1131 at yahoo.com  Mon Oct  8 13:08:39 2018
From: morteza1131 at yahoo.com (morteza omidian)
Date: Mon, 8 Oct 2018 13:08:39 +0000 (UTC)
Subject: [squid-users] netfilter mark and squid
References: <1756353853.813135.1539004119545.ref@mail.yahoo.com>
Message-ID: <1756353853.813135.1539004119545@mail.yahoo.com>

HiIs it possible to create an acl based on netfilter mark in squid??for example:acl My_client NFMARK 0x1tcp_outgoing_mark 0x1 My_client
I want to somehow know my source interface after new packet go out from squid to server.
I want to keep clients packets mark after packets go out to the server!
tanx
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20181008/e2cfec4d/attachment.htm>

From ncherukuri at partycity.com  Mon Oct  8 13:14:20 2018
From: ncherukuri at partycity.com (Cherukuri, Naresh)
Date: Mon, 8 Oct 2018 13:14:20 +0000
Subject: [squid-users] socket failure: (24) Too many open files
In-Reply-To: <201810052306.05160.Antony.Stone@squid.open.source.it>
References: <89638057A560FB458C01C197F81C7F5DA6FBCD43@PACERS.amscan.corp>
 <201810051834.46357.Antony.Stone@squid.open.source.it>
 <89638057A560FB458C01C197F81C7F5DA6FBE031@PACERS.amscan.corp>
 <201810052306.05160.Antony.Stone@squid.open.source.it>
Message-ID: <89638057A560FB458C01C197F81C7F5DA6FC6B44@PACERS.amscan.corp>

Yes, I have also splunk running on this machine.


>What do "cat /proc/sys/fs/file-max" and "cat /proc/sys/fs/file-nr" tell you?

[root at squidprod ~]# cat /proc/sys/fs/file-nr
4736    0       3256314
[root at squidprod ~]# cat /proc/sys/fs/file-max
3256314

>How many users do you have, what sort of number of connections per 
second/minute/hour (whatever is convenient for you to express) do you have 
going through this machine?

I am not sure, but I would say more than 3000 connections per minute.

Thanks,
Naresh

-----Original Message-----
From: squid-users [mailto:squid-users-bounces at lists.squid-cache.org] On Behalf Of Antony Stone
Sent: Friday, October 5, 2018 5:06 PM
To: squid-users at lists.squid-cache.org
Subject: Re: [squid-users] socket failure: (24) Too many open files

On Friday 05 October 2018 at 22:08:16, Cherukuri, Naresh wrote:

> For just squid process open files count.
> 
> [root at squidprod ~]# lsof -c squid | wc -l
> 4385

Squid is not the only thing running on this machine...

> [root at squidprod ~]# lsof | wc -l
> 10875

But you seem to have sufficient file descriptors configured *in Squid* (but maybe 
not in the O/S):

> [root at squidprod ~]# cat /etc/squid/squid.conf | grep "max_filedescriptors"
> max_filedescriptors 8192

So, Squid can have 8192 FDs.

> ulimit -a value
> [root at squidprod ~]# ulimit -a
> open files                      (-n) 8192

...and the system will provide 8192 FDs for every process combined...

> [root at squidprod ~]# lsof | wc -l
> 10875

I reckon that may well be your problem - you have a system-wide limit of 8192 
file descriptors, and yet you are trying to use 10875 open files (this will 
include local pipes, sockets, etc, so it's understandable that it's higher, 
but it indicates you're going over the limit).

> Thanks,
> Naresh

And, as I asked previously:

What do "cat /proc/sys/fs/file-max" and "cat /proc/sys/fs/file-nr" tell you?

How many users do you have, what sort of number of connections per 
second/minute/hour (whatever is convenient for you to express) do you have 
going through this machine?


Antony.

-- 
A good conversation is like a miniskirt;
short enought to retain interest,
but long enough to cover the subject.

 - Celeste Headlee


                                                   Please reply to the list;
                                                         please *don't* CC me.
_______________________________________________
squid-users mailing list
squid-users at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-users

From nldo.tut.alberto at gmail.com  Mon Oct  8 14:34:07 2018
From: nldo.tut.alberto at gmail.com (Ikari C)
Date: Mon, 8 Oct 2018 09:34:07 -0500
Subject: [squid-users] Analize squid3 por conexion
Message-ID: <CACmb8S=fvGsHexRs7AWLmt2MJ-M-Y6EYsWQzuqDTaGCy+XVTuQ@mail.gmail.com>

Thanks, for your time. I will take your advice into account. Have a nice
day.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20181008/a0ce3642/attachment.htm>

From eliezer at ngtech.co.il  Mon Oct  8 19:11:49 2018
From: eliezer at ngtech.co.il (Eliezer Croitoru)
Date: Mon, 08 Oct 2018 22:11:49 +0300
Subject: [squid-users] redirect based on url (302)
In-Reply-To: <f35964b6-e83b-c5b6-5563-602ab666d964@treenet.co.nz>
References: <1537541034108-0.post@n4.nabble.com>
 <dde485b6-5742-d424-8e12-f700d0b975f2@treenet.co.nz>
 <1537771119646-0.post@n4.nabble.com>
 <f35964b6-e83b-c5b6-5563-602ab666d964@treenet.co.nz>
Message-ID: <f4e495af9c93dbec9a25008a99a2e0a7@ngtech.co.il>

Amos I probably missed couple lines.
It's doable but probably if there is a specific set of domains or urls 
then I will need to try and see what and how it works.

Eliezer

On 2018-09-24 12:30, Amos Jeffries wrote:
> On 24/09/18 6:38 PM, uppsalanet wrote:
>> Hi Amos,
>> Today I have a conf like this:
>> ....
>> acl *LIB_domains* dstdomain .almedalsbiblioteket.se .alvin-portal.org
>> .bibliotekuppsala.se
>> http_access allow *LIB_domains*
>> ....
>> 
>> Now I also need to open for *.browzine.com*. The problem with
>> *.browzine.com* is that it is a portal with many links to other sites. 
>> So I
>> basically need to open up and maintain 400 sites in a squid ACL.
>> 
>> I would like to take another approach then (but I don't know if it's
>> possible):
>> I know that browzine.com will reply 302 when trying to access a link 
>> on
>> their site. *So I would like to accept all redirect (302) sites from
>> browzine.com*.
> 
> Aha, that is clearer. Thank you.
> 
> I think you can possibly achieve this, but *only* because of those 302
> existing. If the site were just a collection of links it would be very
> much more difficult.
> 
> 
> What I am thinking of is to use a custom external ACL script that
> creates a temporary browsing session for a client when the 302 arrives
> then the SQL session helper to allow matching traffic through for the
> followup request from that client.
> 
> You will need a database with a table created like this:
> 
>  CREATE TABLE sessions (
>   id VARCHAR(256) NOT NULL PRIMARY KEY,
>   enabled DATETIME NOT NULL DEFAULT CURRENT_TIMESTAMP
> )
> 
> You need to write a script which receives an IP and a URL from Squid,
> extracts the domain name from the URL, then adds a string "$ip $domain"
> to that table as the id column, then returns the "OK" result to Squid.
> 
> The page at
> <http://www.squid-cache.org/Versions/v4/manuals/ext_sql_session_acl.html> 
> has
> details of the SQL session helper that uses that table to check for
> whitelisted domains.
> 
> 
> Your config would look like:
> 
>  acl 302 http_status 302
>  acl browzine dstdomain .browzine.com
> 
>  external_acl_type whitelist_add %SRC %{Location} \
>   /path/to/whitelist_script
> 
>  acl add_to_whitelist external whitelist_add
> 
>  http_reply_access allow browzine 302 add_to_whitelist
>  http_reply_access allow all
> 
> 
>  external_acl_type whitelist ttl=60 %SRC %DST \
>    /usr/lib/squid/ext_session_db_acl \
>    --dsn ... --user ... --password ... \
>    --table sessions --cond ""
> 
>  acl whitelisted external whitelist
>  http_access allow whitelisted
> 
> 
> To have sessions expire simply remove them from the database table.
> Squid will start rejecting traffic there within 60 seconds of the 
> removal.
> 
> HTH
> Amos
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users

-- 
----
Eliezer Croitoru
Linux System Administrator
Mobile: +972-5-28704261
Email: eliezer at ngtech.co.il


From eliezer at ngtech.co.il  Mon Oct  8 19:18:04 2018
From: eliezer at ngtech.co.il (Eliezer Croitoru)
Date: Mon, 08 Oct 2018 22:18:04 +0300
Subject: [squid-users] socket failure: (24) Too many open files
In-Reply-To: <89638057A560FB458C01C197F81C7F5DA6FBCD43@PACERS.amscan.corp>
References: <89638057A560FB458C01C197F81C7F5DA6FBCD43@PACERS.amscan.corp>
Message-ID: <74cb1f53c3507974051cb8bf62708cda@ngtech.co.il>

I recommend Squid upgrade if possible due to couple bugs as something..
Try to bump the server to 32k open file descriptors and see what 
happens.
Depends on the load on the server it might need at peek times more then 
8k.
The cache manage info page should give you couple technical details on 
the status of the service.
It can also give some statistics which might shed some light on the 
scenario.
Others might be able to give you some more detail on the relevant cache 
manager pages.

https://wiki.squid-cache.org/Features/CacheManager

Eliezer

On 2018-10-05 18:57, Cherukuri, Naresh wrote:
> Hello Squid Group,
> 
> I am using squid 3.5.20 as a proxy server.  I Increased the memory
> from 12 GB to 32 GB and Max file descriptors from "4096" to "8192" and
> deployed this server into production on 09/26/2018".
> 
> I don't have any problem from the past 10 days everything working as
> expected till today. Now after 10 days for the first time, I got
> following errors on cache log today. Can someone advise/suggest any
> ideas here?
> 
> Error(s) in /var/log/squid/cache.log: 2018/10/05 11:03:59 kid1|
> comm_open: socket failure: (24) Too many open files
> 
> 2018/10/05 11:03:59 kid1| comm_open: socket failure: (24) Too many
> open files
> 
> 2018/10/05 11:03:59 kid1| comm_open: socket failure: (24) Too many
> open files
> 
> 2018/10/05 11:03:59 kid1| comm_open: socket failure: (24) Too many
> open files
> 
> 2018/10/05 11:03:59 kid1| comm_open: socket failure: (24) Too many
> open files
> 
> 2018/10/05 11:03:59 kid1| comm_open: socket failure: (24) Too many
> open files
> 
> 2018/10/05 11:03:59 kid1| comm_open: socket failure: (24) Too many
> open files
> 
> 2018/10/05 11:03:59 kid1| comm_open: socket failure: (24) Too many
> open files
> 
> 2018/10/05 11:03:59 kid1| comm_open: socket failure: (24) Too many
> open files
> 
> [n*****@squidprod ~]$ free -m
> 
>              total       used       free     shared    buffers
> cached
> 
> Mem:         32004      15907      16097        138        295
> 14132
> 
> -/+ buffers/cache:       1480      30524
> 
> Swap:        24999          0      24999
> 
> Thanks,
> 
> Naresh
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users

-- 
----
Eliezer Croitoru
Linux System Administrator
Mobile: +972-5-28704261
Email: eliezer at ngtech.co.il


From eliezer at ngtech.co.il  Mon Oct  8 19:21:01 2018
From: eliezer at ngtech.co.il (Eliezer Croitoru)
Date: Mon, 08 Oct 2018 22:21:01 +0300
Subject: [squid-users] Running Squid fully as root
In-Reply-To: <1538977265040-0.post@n4.nabble.com>
References: <CAOA5C67o0mr7wunn97USeSznYviH-d=ffyNmRn4mqTSukxryeg@mail.gmail.com>
 <201805172251.00741.Antony.Stone@squid.open.source.it>
 <1538935787950-0.post@n4.nabble.com>
 <127c23f5-3fd6-1427-4258-50d2cd4bf631@treenet.co.nz>
 <1538977265040-0.post@n4.nabble.com>
Message-ID: <806c292e3a90cb8a62698d9acd4d46d1@ngtech.co.il>

You can run squid as root if you want but it still not recommended.
If you have a specific application running ontop of linux you can set 
the setuid/stick bit and it will start with the user or group 
privileges.

Eliezer

On 2018-10-08 08:41, reinerotto wrote:
> The problem is the external ACL-helper, started by squid. It runs as 
> "nobody"
> actually, but it needs to start a special program, which must run as 
> root.
> FYI, openwrt is a shrinked-down LINUX, for embedded systems with 
> limited
> resources, without any user besides the admin.
> openwrt often is used for smart routers, for example. Practically all
> processes run as root.
> 
> 
> 
> 
> --
> Sent from:
> http://squid-web-proxy-cache.1019090.n4.nabble.com/Squid-Users-f1019091.html
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users

-- 
----
Eliezer Croitoru
Linux System Administrator
Mobile: +972-5-28704261
Email: eliezer at ngtech.co.il


From squid3 at treenet.co.nz  Tue Oct  9 00:55:04 2018
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Tue, 9 Oct 2018 13:55:04 +1300
Subject: [squid-users] socket failure: (24) Too many open files
In-Reply-To: <89638057A560FB458C01C197F81C7F5DA6FC6B44@PACERS.amscan.corp>
References: <89638057A560FB458C01C197F81C7F5DA6FBCD43@PACERS.amscan.corp>
 <201810051834.46357.Antony.Stone@squid.open.source.it>
 <89638057A560FB458C01C197F81C7F5DA6FBE031@PACERS.amscan.corp>
 <201810052306.05160.Antony.Stone@squid.open.source.it>
 <89638057A560FB458C01C197F81C7F5DA6FC6B44@PACERS.amscan.corp>
Message-ID: <be7dde8a-62d8-8799-7daa-3c2e398aec18@treenet.co.nz>

On 9/10/18 2:14 AM, Cherukuri, Naresh wrote:
> Yes, I have also splunk running on this machine.
> 
> 
>> What do "cat /proc/sys/fs/file-max" and "cat /proc/sys/fs/file-nr" tell you?
> 
> [root at squidprod ~]# cat /proc/sys/fs/file-nr
> 4736    0       3256314
> [root at squidprod ~]# cat /proc/sys/fs/file-max
> 3256314
> 
>> How many users do you have, what sort of number of connections per 
> second/minute/hour (whatever is convenient for you to express) do you have 
> going through this machine?
> 
> I am not sure, but I would say more than 3000 connections per minute.
> 

The Squid "info" manager report (squidclient mgr:info) contains the
req/min details. Since you are running out of FD the number Squid
reports as being used will be a lower bound on how many it could be
handling if there were enough FD.

Amos


From squid3 at treenet.co.nz  Tue Oct  9 00:58:10 2018
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Tue, 9 Oct 2018 13:58:10 +1300
Subject: [squid-users] Running Squid fully as root
In-Reply-To: <1538977265040-0.post@n4.nabble.com>
References: <CAOA5C67o0mr7wunn97USeSznYviH-d=ffyNmRn4mqTSukxryeg@mail.gmail.com>
 <201805172251.00741.Antony.Stone@squid.open.source.it>
 <1538935787950-0.post@n4.nabble.com>
 <127c23f5-3fd6-1427-4258-50d2cd4bf631@treenet.co.nz>
 <1538977265040-0.post@n4.nabble.com>
Message-ID: <f1db39d2-3434-b3bf-1912-d214a6850b19@treenet.co.nz>

On 8/10/18 6:41 PM, reinerotto wrote:
> The problem is the external ACL-helper, started by squid. It runs as "nobody"
> actually, but it needs to start a special program, which must run as root. 

Do you mind saying what that helper and its sub-program are supposed to
be doing? There may be a better alternative than running as root.


> FYI, openwrt is a shrinked-down LINUX, for embedded systems with limited
> resources, without any user besides the admin. 
> openwrt often is used for smart routers, for example. Practically all
> processes run as root.
> 

I've worked with OpenWRT and the Squid port for it. That statement still
sounds wrong. The fact that Squid is working fine until you have this
helper that needs root privileges to do its thing is an example of that.

Amos


From augustus_meyer at gmx.net  Tue Oct  9 13:25:01 2018
From: augustus_meyer at gmx.net (reinerotto)
Date: Tue, 9 Oct 2018 08:25:01 -0500 (CDT)
Subject: [squid-users] Running Squid fully as root
In-Reply-To: <f1db39d2-3434-b3bf-1912-d214a6850b19@treenet.co.nz>
References: <CAOA5C67o0mr7wunn97USeSznYviH-d=ffyNmRn4mqTSukxryeg@mail.gmail.com>
 <201805172251.00741.Antony.Stone@squid.open.source.it>
 <1538935787950-0.post@n4.nabble.com>
 <127c23f5-3fd6-1427-4258-50d2cd4bf631@treenet.co.nz>
 <1538977265040-0.post@n4.nabble.com>
 <f1db39d2-3434-b3bf-1912-d214a6850b19@treenet.co.nz>
Message-ID: <1539091501672-0.post@n4.nabble.com>

Found a workaround: Within my external helper (busybox shell-script) I start
the other process (chilli_query) using sudo. That works fine for me.
So we can cosider this issue solved.
Thanx a lot.




--
Sent from: http://squid-web-proxy-cache.1019090.n4.nabble.com/Squid-Users-f1019091.html


From ncherukuri at partycity.com  Tue Oct  9 14:49:09 2018
From: ncherukuri at partycity.com (Cherukuri, Naresh)
Date: Tue, 9 Oct 2018 14:49:09 +0000
Subject: [squid-users] socket failure: (24) Too many open files
In-Reply-To: <74cb1f53c3507974051cb8bf62708cda@ngtech.co.il>
References: <89638057A560FB458C01C197F81C7F5DA6FBCD43@PACERS.amscan.corp>
 <74cb1f53c3507974051cb8bf62708cda@ngtech.co.il>
Message-ID: <89638057A560FB458C01C197F81C7F5DA6FCC711@PACERS.amscan.corp>

Thank you for quickturnover!

I bumped  FD from 8k to 16k.

Thanks,
Naresh


-----Original Message-----
From: Eliezer Croitoru [mailto:eliezer at ngtech.co.il] 
Sent: Monday, October 8, 2018 3:18 PM
To: Cherukuri, Naresh
Cc: 'squid-users at lists.squid-cache.org'
Subject: Re: [squid-users] socket failure: (24) Too many open files

I recommend Squid upgrade if possible due to couple bugs as something..
Try to bump the server to 32k open file descriptors and see what 
happens.
Depends on the load on the server it might need at peek times more then 
8k.
The cache manage info page should give you couple technical details on 
the status of the service.
It can also give some statistics which might shed some light on the 
scenario.
Others might be able to give you some more detail on the relevant cache 
manager pages.

https://wiki.squid-cache.org/Features/CacheManager

Eliezer

On 2018-10-05 18:57, Cherukuri, Naresh wrote:
> Hello Squid Group,
> 
> I am using squid 3.5.20 as a proxy server.  I Increased the memory
> from 12 GB to 32 GB and Max file descriptors from "4096" to "8192" and
> deployed this server into production on 09/26/2018".
> 
> I don't have any problem from the past 10 days everything working as
> expected till today. Now after 10 days for the first time, I got
> following errors on cache log today. Can someone advise/suggest any
> ideas here?
> 
> Error(s) in /var/log/squid/cache.log: 2018/10/05 11:03:59 kid1|
> comm_open: socket failure: (24) Too many open files
> 
> 2018/10/05 11:03:59 kid1| comm_open: socket failure: (24) Too many
> open files
> 
> 2018/10/05 11:03:59 kid1| comm_open: socket failure: (24) Too many
> open files
> 
> 2018/10/05 11:03:59 kid1| comm_open: socket failure: (24) Too many
> open files
> 
> 2018/10/05 11:03:59 kid1| comm_open: socket failure: (24) Too many
> open files
> 
> 2018/10/05 11:03:59 kid1| comm_open: socket failure: (24) Too many
> open files
> 
> 2018/10/05 11:03:59 kid1| comm_open: socket failure: (24) Too many
> open files
> 
> 2018/10/05 11:03:59 kid1| comm_open: socket failure: (24) Too many
> open files
> 
> 2018/10/05 11:03:59 kid1| comm_open: socket failure: (24) Too many
> open files
> 
> [n*****@squidprod ~]$ free -m
> 
>              total       used       free     shared    buffers
> cached
> 
> Mem:         32004      15907      16097        138        295
> 14132
> 
> -/+ buffers/cache:       1480      30524
> 
> Swap:        24999          0      24999
> 
> Thanks,
> 
> Naresh
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users

-- 
----
Eliezer Croitoru
Linux System Administrator
Mobile: +972-5-28704261
Email: eliezer at ngtech.co.il


From arnt.r.rorvik at ntnu.no  Tue Oct  9 19:39:01 2018
From: arnt.r.rorvik at ntnu.no (=?utf-8?B?QXJudCBSaWNoYXJkIFLDuHJ2aWs=?=)
Date: Tue, 9 Oct 2018 19:39:01 +0000
Subject: [squid-users] Proxy client certificate authentication rewritten to
 username/password authentication
In-Reply-To: <EA87A166-14F9-48BC-B7F3-12D9E0A15BB4@ntnu.no>
References: <EA87A166-14F9-48BC-B7F3-12D9E0A15BB4@ntnu.no>
Message-ID: <56B3950E-3E65-4F4A-B430-587A558A2098@ntnu.no>

Hi, Squid users, strategists, designers and developers!

Can the Squid web proxy be used to request and verify the machine certificate of workstations trying to initiate a session towards a given web server (outside Squid), and, rewrite this session initiation on the way out (towards the the given web server), adding a username and password in the URL.

If not natively/directly supported, can it be achieved using any extensions or scripting options available with Squid?

Looking forward to hearing from you!
Best regards,
Arnt Richard R?rvik, NTNU, Norway.
----

Arnt Richard R?rvik
Senior engineer
Dept. of IT
Section of strategy and governance
Norwegian university of Science and Technology (NTNU), https://www.ntnu.edu/
7491 Trondheim
Norway

SfB-address: sip:arnt.r.rorvik at ntnu.no
Phone.: +47 73 55 91 67/ Mobile: +47 957 01 081
https://www.ntnu.no/ansatte/arnt.r.rorvik<https://emea01.safelinks.protection.outlook.com/?url=https%3A%2F%2Fwww.ntnu.no%2Fansatte%2Farnt.r.rorvik&data=02%7C01%7Coyvind.skeie.bauer%40ecoonline.com%7C0c928779c82a4538527208d54624cd6a%7C1d9f9dffd59244aebc5a878ba93ca75a%7C1%7C0%7C636492046230202126&sdata=rEd7YMjtl7OtsZy8k9ILcF2HaqP%2BCoqSmpb%2BCWbMbKc%3D&reserved=0>
https://no.linkedin.com/in/arrorvik<https://emea01.safelinks.protection.outlook.com/?url=https%3A%2F%2Fno.linkedin.com%2Fin%2Farrorvik&data=02%7C01%7Coyvind.skeie.bauer%40ecoonline.com%7C0c928779c82a4538527208d54624cd6a%7C1d9f9dffd59244aebc5a878ba93ca75a%7C1%7C0%7C636492046230202126&sdata=97cbD2zcBqwWeJwhu7xbffWQ3JtashIWmEpcUU%2FaqPI%3D&reserved=0>
https://www.youracclaim.com/badges/eaae291f-c686-4e84-b48d-96fa01a37401


-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20181009/aaec95f9/attachment.htm>

From rousskov at measurement-factory.com  Tue Oct  9 20:25:53 2018
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Tue, 9 Oct 2018 14:25:53 -0600
Subject: [squid-users] Proxy client certificate authentication rewritten
 to username/password authentication
In-Reply-To: <56B3950E-3E65-4F4A-B430-587A558A2098@ntnu.no>
References: <EA87A166-14F9-48BC-B7F3-12D9E0A15BB4@ntnu.no>
 <56B3950E-3E65-4F4A-B430-587A558A2098@ntnu.no>
Message-ID: <5c8fbae7-e4a2-7003-ab96-4229df43d1ee@measurement-factory.com>

On 10/09/2018 01:39 PM, Arnt Richard R?rvik wrote:

> Can the Squid web proxy be used to request and verify the machine
> certificate of workstations trying to initiate a session towards a given
> web server (outside Squid), 

Yes if by "machine certificate" you mean an X509 certificate that TLS
servers can request from TLS clients. Squid supports TLS client
certificate request/validation.

Please note that if Squid requests that certificate, then the TLS
connection has to be between the client and Squid, not between the
client and the origin server. It is impossible for Squid to request a
client certificate on a TLS connection that Squid does not participate
in (beyond shoveling TCP payload bytes).

If the origin server itself requests a TLS client certificate, then, in
theory, Squid can inspect the certificate returned by the client.
However, I doubt such requested-by-origin TLS client certificate
inspection works out of the box, and it usually would not make sense in
common deployments (because Squid would not have access to the
validating CA used to sign the client certificate), but it is
technically possible to extract that client certificate from a
client-origin connection IIRC -- it is not encrypted -- and validate it
against known-to-Squid CAs.


> and, rewrite this session initiation on the
> way out (towards the the given web server), 

If you want Squid to request the certificate, then the TLS connection
has to be between the client and Squid. If needed, Squid will open a TLS
connection to the origin server. The two TLS connections are unrelated
from TLS point of view.


> adding a username and password in the URL.

Yes, if client sends plain text requests to Squid, or if Squid bumps
encrypted client requests. However:

* TLS client certificate validation is currently not fully compatible
with TLS client connection bumping (i.e. SslBump) IIRC.

* When dealing with secure origin servers, popular browsers will not
send plain text requests to Squid (i.e. "GET https://example.com").
Instead, they will want to establish dumb CONNECT tunnels through Squid.
Those tunnels do not expose HTTP request URLs (unless Squid bumps them).

* IIRC, X509 certificates do not contain user names and passwords, at
least not in HTTP authentication sense. Where should Squid get the user
name and password to add to the URL?

* Rewriting URLs requires using a url_rewrite_program helper or an
ICAP/eCAP service.


In summary, various pieces of what you want are supported, but it is
unclear what combination of those pieces would be applicable to your
exact use case. Most likely, the combination you want is not and cannot
be supported.

If you detail whether your TLS clients know about Squid existence (i.e.
connect to/through Squid), whether your clients are regular browsers (or
custom software you control), and whether your origin servers request
client certificates, then it may be possible to determine whether what
you want is or can be supported. To detail your setup, consider
describing what happens at TCP, TLS, and HTTP layers between a typical
client, Squid, and origin server in your environment.


HTH,

Alex.


From arnt.r.rorvik at ntnu.no  Tue Oct  9 20:58:40 2018
From: arnt.r.rorvik at ntnu.no (=?utf-8?B?QXJudCBSaWNoYXJkIFLDuHJ2aWs=?=)
Date: Tue, 9 Oct 2018 20:58:40 +0000
Subject: [squid-users] Proxy client certificate authentication rewritten
 to username/password authentication
Message-ID: <26ef8f8c2ed74888afaad89b9db33c02@it-ex10.win.ntnu.no>

Hi, Alex!

Thank you for your very kind and intelligent reply!

The origin server does not request any certificates from the client. We can instruct the clients to use the proxy directly, if terminating traffic directly in Squid can easen the implementation.

Furthermore, commenting your additons (>):

> * IIRC, X509 certificates do not contain user names and passwords, at least not in HTTP authentication sense. Where should Squid get the user name and password to add to the URL?

The general idea is to have a table in Squid (or make accessible such a table from elsewhere) with a number of usernames and passwords, that would match certain placeholders in the startup URL issued by the clients  that would easily and uniquely match a certain pattern, such as

https://www.service4us.com/ login/dologin.php?username=usernameplaceholder1&password=passwordplaceholder1 for iPad 1
Here, the username- and password placeholder 1 would of course be replaced with the proper usernames and passwords looked up in the aforementioned table before being handed over to the origin server.

> If you detail whether your TLS clients know about Squid existence (i.e. connect to/through Squid),

That would be possible.

> whether your clients are regular browsers (or custom software you control), 

This would probably be the standard managed browser on iPads, that is, Safari with policies. They could in theory be anything, but manageability would normally dictate a managed browser.

> and whether your origin servers request client certificates
Nope! :-)

> , then it may be possible to determine whether what you want is or can be supported. To detail your setup, consider describing what happens at TCP, TLS, and HTTP layers between a typical client, Squid, and origin server in your environment.

We will indeed do a more thorough inspection of how the traffic is performed on the different levels, and ask the origin server vendor for assistance.

Once again - thank you for your very kind and insightful help!


Arnt Richard R?rvik
Senior engineer
Dept. of IT
Section of strategy and governance
Norwegian university of Science and Technology (NTNU), https://www.ntnu.edu/ 
7491 Trondheim
Norway

SfB-address: sip:arnt.r.rorvik at ntnu.no
Phone.: +47 73 55 91 67/ Mobile: +47 957 01 081
https://www.ntnu.no/ansatte/arnt.r.rorvik
https://no.linkedin.com/in/arrorvik 
https://www.youracclaim.com/badges/eaae291f-c686-4e84-b48d-96fa01a37401




-----Opprinnelig melding-----
Fra: Alex Rousskov <rousskov at measurement-factory.com> 
Sendt: 9. oktober 2018 22:26
Til: squid-users at lists.squid-cache.org; Arnt Richard R?rvik <arnt.r.rorvik at ntnu.no>
Emne: Re: [squid-users] Proxy client certificate authentication rewritten to username/password authentication

On 10/09/2018 01:39 PM, Arnt Richard R?rvik wrote:

> Can the Squid web proxy be used to request and verify the machine 
> certificate of workstations trying to initiate a session towards a 
> given web server (outside Squid),

Yes if by "machine certificate" you mean an X509 certificate that TLS servers can request from TLS clients. Squid supports TLS client certificate request/validation.

Please note that if Squid requests that certificate, then the TLS connection has to be between the client and Squid, not between the client and the origin server. It is impossible for Squid to request a client certificate on a TLS connection that Squid does not participate in (beyond shoveling TCP payload bytes).

If the origin server itself requests a TLS client certificate, then, in theory, Squid can inspect the certificate returned by the client.
However, I doubt such requested-by-origin TLS client certificate inspection works out of the box, and it usually would not make sense in common deployments (because Squid would not have access to the validating CA used to sign the client certificate), but it is technically possible to extract that client certificate from a client-origin connection IIRC -- it is not encrypted -- and validate it against known-to-Squid CAs.


> and, rewrite this session initiation on the way out (towards the the 
> given web server),

If you want Squid to request the certificate, then the TLS connection has to be between the client and Squid. If needed, Squid will open a TLS connection to the origin server. The two TLS connections are unrelated from TLS point of view.


> adding a username and password in the URL.

Yes, if client sends plain text requests to Squid, or if Squid bumps encrypted client requests. However:

* TLS client certificate validation is currently not fully compatible with TLS client connection bumping (i.e. SslBump) IIRC.

* When dealing with secure origin servers, popular browsers will not send plain text requests to Squid (i.e. "GET https://example.com").
Instead, they will want to establish dumb CONNECT tunnels through Squid.
Those tunnels do not expose HTTP request URLs (unless Squid bumps them).

* IIRC, X509 certificates do not contain user names and passwords, at least not in HTTP authentication sense. Where should Squid get the user name and password to add to the URL?

* Rewriting URLs requires using a url_rewrite_program helper or an ICAP/eCAP service.


In summary, various pieces of what you want are supported, but it is unclear what combination of those pieces would be applicable to your exact use case. Most likely, the combination you want is not and cannot be supported.

If you detail whether your TLS clients know about Squid existence (i.e.
connect to/through Squid), whether your clients are regular browsers (or custom software you control), and whether your origin servers request client certificates, then it may be possible to determine whether what you want is or can be supported. To detail your setup, consider describing what happens at TCP, TLS, and HTTP layers between a typical client, Squid, and origin server in your environment.


HTH,

Alex.

From rousskov at measurement-factory.com  Tue Oct  9 22:37:43 2018
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Tue, 9 Oct 2018 16:37:43 -0600
Subject: [squid-users] Proxy client certificate authentication rewritten
 to username/password authentication
In-Reply-To: <26ef8f8c2ed74888afaad89b9db33c02@it-ex10.win.ntnu.no>
References: <26ef8f8c2ed74888afaad89b9db33c02@it-ex10.win.ntnu.no>
Message-ID: <d7e2013b-1aa8-2583-b70c-49f23464e987@measurement-factory.com>

On 10/09/2018 02:58 PM, Arnt Richard R?rvik wrote:

> The origin server does not request any certificates from the client.
> We can instruct the clients to use the proxy directly, if terminating
> traffic directly in Squid can easen the implementation.

OK, I will assume that your clients will explicitly talk to the proxy
using TLS. In that case, certificate-based client authentication by
Squid should work OK.


> The general idea is to have a table in Squid (or make accessible such
> a table from elsewhere) with a number of usernames and passwords,
> that would match certain placeholders in the startup URL issued by
> the clients

Understood. From Squid (and TLS and HTTP) point of view, this URL
rewriting will have nothing to do with usernames and passwords then.

Your custom URL rewriting helper or adaptation service, not Squid, will
contain all the placeholder substitution logic and will access the
tables that drive those substitutions. This separation from Squid is
actually a good thing in most cases.

For more about Squid adaptation interfaces, see
https://wiki.squid-cache.org/SquidFaq/ContentAdaptation


>> whether your clients are regular browsers (or custom software you control), 

> This would probably be the standard managed browser on iPads, that
> is, Safari with policies. They could in theory be anything, but
> manageability would normally dictate a managed browser.

You will need to check whether your target browser(s) support:

1. HTTPS proxies: Establishing a TLS connection to the proxy and then
sending HTTP traffic, including CONNECT requests for secure sites inside
that TLS connection to the proxy. FireFox and Chrome (semi-secretly)
support HTTPS proxies, but I do not know much about Safari.

2. Certificate-based proxy authentication: Supplying a client
certificate when requested by the proxy on the other end of the
client-proxy TLS connection described in #1.

If both #1 and #2 are supported, then what you want is

* fairly easy for HTTP origin servers and

* currently unsupported for HTTPS origin servers (but can be supported
by enhancing SslBump to bump TLS-inside-TLS).


Please note that if you want to rewrite URLs of secure web sites (e.g.,
"https://example.com/"), then you will be fighting an increasingly
uphill battle with modern browsers, even if Squid can do (or can be
enhanced to do) what you want. In many cases, an overall better solution
in that case is to rewrite those secure URLs inside the browser instead,
even though that approach often requires instrumenting several browsers
that increasingly resist instrumentation (i.e. another uphill battle
with popular browsers!).


HTH,

Alex.


From squid3 at treenet.co.nz  Wed Oct 10 05:12:43 2018
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Wed, 10 Oct 2018 18:12:43 +1300
Subject: [squid-users] Proxy client certificate authentication rewritten
 to username/password authentication
In-Reply-To: <d7e2013b-1aa8-2583-b70c-49f23464e987@measurement-factory.com>
References: <26ef8f8c2ed74888afaad89b9db33c02@it-ex10.win.ntnu.no>
 <d7e2013b-1aa8-2583-b70c-49f23464e987@measurement-factory.com>
Message-ID: <f009582e-0f8e-7ac9-ca4d-7bd7f70355da@treenet.co.nz>

On 10/10/18 11:37 AM, Alex Rousskov wrote:
> 
> Please note that if you want to rewrite URLs of secure web sites (e.g.,
> "https://example.com/"), then you will be fighting an increasingly
> uphill battle with modern browsers, even if Squid can do (or can be
> enhanced to do) what you want. In many cases, an overall better solution
> in that case is to rewrite those secure URLs inside the browser instead,
> even though that approach often requires instrumenting several browsers
> that increasingly resist instrumentation (i.e. another uphill battle
> with popular browsers!).
> 


One other thing to consider here is whether the user+pass have to be
sent in the URL at all.

If possible, it would be better to use a cache_peer connection that
sends HTTP authentication headers to the upstream server. That gives you
ability to "internally" use the more secure forms of TLS which cannot be
MITM'd for the connection containing credentials.

Alternatively, you may be able to send a custom header with the
http_header_add mechanism with a custom value to the origin server for
processing.

Amos


From augustus_meyer at gmx.net  Wed Oct 10 07:18:37 2018
From: augustus_meyer at gmx.net (reinerotto)
Date: Wed, 10 Oct 2018 02:18:37 -0500 (CDT)
Subject: [squid-users] squid on openwrt: RAM usage and header forgery
Message-ID: <1539155917842-0.post@n4.nabble.com>


Using squid 4.0.24 on openwrt, I see it grabbing significant amount of
additional RAM after short period of activity, although I tried to downsize
squid as much as possible. Any suggestion for further significant reduction
of mem requirements after startup, or why is there such a growth (> 10MB)
after short period of time ?
Initial mem requirements OK, but then the huge increase in size afterwards
it not appreciated.
(Don't need caching at all. Compiled without IPv6)

First the (anon) squid.conf:
acl localnet src 192.168.182.0/24
acl ssl_ports port 443
acl safe_ports port 80
acl safe_ports port 443
acl safe_ports port 3128
acl connect method connect

http_access deny !safe_ports
http_access deny connect !ssl_ports

acl acl1 url_regex -i .*/string1$
acl acl2 url_regex -i .*/string2$
acl acl3 url_regex -i .*/string3$

external_acl_type check_test ttl=0 cache=0 %SRC /etc/squid/check_test.sh
external_acl_type check_test_2 ttl=30 negative_ttl=3 cache=32 %SRC
/etc/squid/check_test_2.sh
acl check_2 check_test_2
acl check  external check_test

http_access deny acl1 check
http_access deny acl2 check
http_access deny acl3 check

http_access allow localnet
http_access allow localhost
http_access deny all

cache deny all
access_log none
cache_log /var/log/squid/cache.log
cache_store_log stdio:/dev/null
logfile_rotate 0
logfile_daemon /dev/null

http_port 3128
http_port 8888 intercept

https_port 4443  intercept ssl-bump cert=/etc/squid/ssl_cert/myCA.pem \
  generate-host-certificates=off dynamic_cert_mem_cache_size=1MB
sslflags=NO_DEFAULT_CA
acl step1 at_step SslBump1
ssl_bump peek step1 all

acl sni_block ssl::server_name .a.com
acl sni_block ssl::server_name .b.com
acl sni_block ssl::server_name .c.com
ssl_bump terminate !check_2 sni_block check
ssl_bump splice all


cache_mem 0 MB
shutdown_lifetime 10 seconds
httpd_suppress_version_string on
dns_v4_first on
forwarded_for delete
via off
reply_header_access Cache deny all
client_idle_pconn_timeout 1 minute
server_idle_pconn_timeout 5 minute
memory_pools off
ipcache_size 128
fqdncache_size 128
reply_header_access Alternate-Protocol deny all
reply_header_access alternate-protocol deny all
reply_header_access alt-svc deny all
pinger_enable off
digest_generation off
netdb_filename none
dns_nameservers 127.0.0.1
reply_body_max_size 4 MB


Now mem requirements for kid-1, shortly after boot:
cat /proc/1447/status
Name:   squid
Umask:  0027
State:  S (sleeping)
Tgid:   1447
Ngid:   0
Pid:    1447
PPid:   1444
TracerPid:      0
Uid:    0       65534   0       65534
Gid:    65534   65534   65534   65534
FDSize: 32
Groups: 65534
VmPeak:    15836 kB <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
VmSize:    15836 kB
VmLck:         0 kB
VmPin:         0 kB
VmHWM:     11324 kB
VmRSS:     11324 kB
RssAnon:            4596 kB
RssFile:            6660 kB
RssShmem:             68 kB
VmData:     5708 kB <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
VmStk:       132 kB
VmExe:      3744 kB
VmLib:      4196 kB
VmPTE:        28 kB
VmPMD:         0 kB
VmSwap:        0 kB
Threads:        1
SigQ:   0/977
SigPnd: 00000000000000000000000000000000
ShdPnd: 00000000000000000000000000000000
SigBlk: 00000000000000000000000000000000
SigIgn: 00000000000000000000000000001004
SigCgt: 0000000000000000000000000203c603
CapInh: 0000000000000000
CapPrm: 0000003fffffffff
CapEff: 0000000000000400
CapBnd: 0000003fffffffff
CapAmb: 0000000000000000
NoNewPrivs:     0
Speculation_Store_Bypass:       unknown
Cpus_allowed:   1
Cpus_allowed_list:      0
voluntary_ctxt_switches:        275
nonvoluntary_ctxt_switches:     100

#1h later, after some usage:
 cat /proc/1447/status
Name:   squid
Umask:  0027
State:  S (sleeping)
Tgid:   1447
Ngid:   0
Pid:    1447
PPid:   1444
TracerPid:      0
Uid:    0       65534   0       65534
Gid:    65534   65534   65534   65534
FDSize: 512
Groups: 65534
VmPeak:    28844 kB <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
VmSize:    28844 kB
VmLck:         0 kB
VmPin:         0 kB
VmHWM:     23064 kB
VmRSS:     23064 kB
RssAnon:           15856 kB
RssFile:            7140 kB
RssShmem:             68 kB
VmData:    18716 kB <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
VmStk:       132 kB
VmExe:      3744 kB
VmLib:      4196 kB
VmPTE:        40 kB
VmPMD:         0 kB
VmSwap:        0 kB
Threads:        1
SigQ:   0/977
SigPnd: 00000000000000000000000000000000
ShdPnd: 00000000000000000000000000000000
SigBlk: 00000000000000000000000000000000
SigIgn: 00000000000000000000000000001004
SigCgt: 0000000000000000000000000203c603
CapInh: 0000000000000000
CapPrm: 0000003fffffffff
CapEff: 0000000000000400
CapBnd: 0000003fffffffff
CapAmb: 0000000000000000
NoNewPrivs:     0
Speculation_Store_Bypass:       unknown
Cpus_allowed:   1
Cpus_allowed_list:      0
voluntary_ctxt_switches:        16501
nonvoluntary_ctxt_switches:     9636

I get quite a lot of messages in cache.log:
2018/10/09 12:38:49 kid1| ALE missing adapted HttpRequest object
2018/10/09 12:38:49 kid1| ALE missing URL
2018/10/09 12:38:49 kid1| ALE missing adapted HttpRequest object
2018/10/09 12:40:18 kid1| SECURITY ALERT: Host header forgery detected on
local=212.95.165.32:443 remote=192.168.182.3:51304 FD 36 flags=33 (local IP
does not match any domain IP)
2018/10/09 12:40:18 kid1| SECURITY ALERT: on URL:
b.scorecardresearch.com:443
2018/10/09 12:40:28 kid1| SECURITY ALERT: Host header forgery detected on
local=104.193.83.156:443 remote=192.168.182.3:51400 FD 183 flags=33 (local
IP does not match any domain IP)
2018/10/09 12:40:28 kid1| SECURITY ALERT: on URL:
csm2waycm-atl.netmng.com:443
2018/10/09 12:40:28 kid1| SECURITY ALERT: Host header forgery detected on
local=104.193.83.156:443 remote=192.168.182.3:51402 FD 226 flags=33 (local
IP does not match any domain IP)

My guess is, that the "header forgery" might be caused be inconsistency
between browsers DNS-cache, my clients DNS-cache (Win 7) and the DNS-cache
on the device, running squid. As practically all these "header forgeries"
are for ad-networks, I consider it only an annoyance.Or is it _not_ ?






--
Sent from: http://squid-web-proxy-cache.1019090.n4.nabble.com/Squid-Users-f1019091.html


From christof.gerber1 at gmail.com  Wed Oct 10 11:58:38 2018
From: christof.gerber1 at gmail.com (Christof Gerber)
Date: Wed, 10 Oct 2018 13:58:38 +0200
Subject: [squid-users] Recent Squid 4 versions show ERR_CANNOT_FORWARD
	instead of ERR_DNS_FAIL
Message-ID: <CAFyThpJmJJo0SWh_iKGMR-Ui0acCQdmG+op13b-kSzd_r1KeZA@mail.gmail.com>

I am wondering why the recent Squid4
(v4-57a5679bae20e90ef73473e03327e37aa0263570) with a minimal config,
when accessing a non-existing domain (e.g.
http://fsdafasdfsadfklsdj.ch/) produces a ERR_CANNOT_FORWARD error
when the older Squid4 (v4-6d8f397398995c4512cb045920ee2747cc6b14f8)
produces a ERR_DNS_FAIL as it is also the case for Squid 3.5.

Why does this different behaviour occur?
Is it a bug or is it a feature?

Ps. Squid 4 v4-9d6a91ba608acadb9f3cb397d72748a711db3c91 is still
producing ERR_DNS_FAIL.
-- 
Christof Gerber
Email: christof.gerber1 at gmail.com


From squid3 at treenet.co.nz  Wed Oct 10 13:15:53 2018
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 11 Oct 2018 02:15:53 +1300
Subject: [squid-users] squid on openwrt: RAM usage and header forgery
In-Reply-To: <1539155917842-0.post@n4.nabble.com>
References: <1539155917842-0.post@n4.nabble.com>
Message-ID: <cb5c15e2-f788-2967-e7b5-c7543027ac7f@treenet.co.nz>

On 10/10/18 8:18 PM, reinerotto wrote:
> 
> Using squid 4.0.24 on openwrt,

Please upgrade. All 4.0.z versions are beta releases and no longer
supported.

> I see it grabbing significant amount of
> additional RAM after short period of activity, although I tried to downsize
> squid as much as possible. Any suggestion for further significant reduction
> of mem requirements after startup, or why is there such a growth (> 10MB)
> after short period of time ?

Your machines /proc details below show the large numbers to be virtual,
not real memory usage


> 
> Now mem requirements for kid-1, shortly after boot:
> cat /proc/1447/status
> Name:   squid
...
> VmPeak:    15836 kB <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
> VmSize:    15836 kB
> VmLck:         0 kB
> VmPin:         0 kB
> VmHWM:     11324 kB
> VmRSS:     11324 kB
> RssAnon:            4596 kB
> RssFile:            6660 kB
> RssShmem:             68 kB
> VmData:     5708 kB <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
> VmStk:       132 kB
> VmExe:      3744 kB
> VmLib:      4196 kB
> VmPTE:        28 kB
> VmPMD:         0 kB
> VmSwap:        0 kB

> 
> #1h later, after some usage:
>  cat /proc/1447/status
> Name:   squid
...
> VmPeak:    28844 kB <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
> VmSize:    28844 kB
> VmLck:         0 kB
> VmPin:         0 kB
> VmHWM:     23064 kB
> VmRSS:     23064 kB
> RssAnon:           15856 kB
> RssFile:            7140 kB
> RssShmem:             68 kB
> VmData:    18716 kB <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
> VmStk:       132 kB
> VmExe:      3744 kB
> VmLib:      4196 kB
> VmPTE:        40 kB
> VmPMD:         0 kB
> VmSwap:        0 kB


Note that the first value you are pointing at in the above report is
*peak* virtual memory usage. In other words the highest *ever* amount of
memory allocated, during all of that "previous usage" traffic going
through the proxy.

The second value is lower, so there is no memory leak nor problems
similar to leaks.


I expect it is the natural outcome of using helpers. The way fork()
operates on most systems is to allocate a block of virtual memory equal
in size to the real memory used at that time by the Squid worker process
starting the helper. It is never actually used by the child/helper
process. So as long as your machine can cope with the size existing in
virtual memory it can be ignored.

Your helpers are external ACL helpers, for which the default behaviour
is to not start any running until Squid is active and the first HTTP
message received.
 The second VmData value is just over 3x the initial VmData value. I
would not be surprised to see 3 helper processes running when that /proc
listing was done.


There have been a few bugs and code polish changes since that old beta
version which have been about memory reductions and improvements. But I
would not expect any major difference, just more streamlined use under load.



The other thing you can do to improve memory usage by Squid is to leave
the memory pools feature *enabled*. That allows you to define how much
memory Squid uses for its pools
(<http://www.squid-cache.org/Doc/config/memory_pools_limit/>). It also
allows you to look at the cachemgr "mem" report to see what Squid is
using memory for (showing allocation rates, active amounts and peak
values). Without the memory pooling one can only guess at these numbers.

FYI: The doc for that pools limit directive mentions pools as a way to
avoid memory thrashing. Squid allocates and deallocates memory in small
packet-sized blocks at a rate approx 4x that of the traffic being
handled. eg. a 1MB/sec traffic rate will be allocating and then
deallocating around 4MB of memory in that same second.
 The way OS tend to cope with such high turnover cycles from a process
like Squid is to leave free()'d memory reserved in virtual-memory state
for the process that used them last to re-grab. This is also what Squid
pools do, but in a way optimized to also prevent fragmentation issues
accumulating due to each HTTP message being ever so slightly different
in memory needs.


> Initial mem requirements OK, but then the huge increase in size afterwards
> it not appreciated.
> (Don't need caching at all. Compiled without IPv6)

You do however need to process traffic. Simply receiving that traffic
requires memory allocations, processing the messages requires more, and
so on. All this processing accumulates small bits of information about
the traffic and memory is used to store that data to both produce mgr
reports and to optimize later traffic handling speeds. Without it your
proxy would be very, very slow.



> 
> First the (anon) squid.conf:
> acl localnet src 192.168.182.0/24
> acl ssl_ports port 443
> acl safe_ports port 80
> acl safe_ports port 443
> acl safe_ports port 3128
> acl connect method connect
> 
> http_access deny !safe_ports
> http_access deny connect !ssl_ports
> 
> acl acl1 url_regex -i .*/string1$
> acl acl2 url_regex -i .*/string2$
> acl acl3 url_regex -i .*/string3$
> 
> external_acl_type check_test ttl=0 cache=0 %SRC /etc/squid/check_test.sh
> external_acl_type check_test_2 ttl=30 negative_ttl=3 cache=32 %SRC
> /etc/squid/check_test_2.sh
> acl check_2 check_test_2
> acl check  external check_test
> 
> http_access deny acl1 check
> http_access deny acl2 check
> http_access deny acl3 check
> 
> http_access allow localnet
> http_access allow localhost
> http_access deny all
> 
> cache deny all
> access_log none
> cache_log /var/log/squid/cache.log
> cache_store_log stdio:/dev/null

Don't do that. Just remove the cache_store_log line entirely from your
config. The store.log has not been enabled by default since Squid-3.0.


> logfile_rotate 0
> logfile_daemon /dev/null

The above logfile_daemon line can be removed once you upgrade. It was
only needed to workaround bug 4171, which is fixed in the more recent
Squid-4 releases.


> 
> http_port 3128
> http_port 8888 intercept
> 
> https_port 4443  intercept ssl-bump cert=/etc/squid/ssl_cert/myCA.pem \
>   generate-host-certificates=off dynamic_cert_mem_cache_size=1MB
> sslflags=NO_DEFAULT_CA

Squid-4 release notes:
 "New option tls-default-ca replaces sslflags=NO_DEFAULT_CA, the default
is also changed to OFF."

So you can remove the about sslflags= option. It does nothing but waste
memory during loading of the config file.



> acl step1 at_step SslBump1
> ssl_bump peek step1 all
> 
> acl sni_block ssl::server_name .a.com
> acl sni_block ssl::server_name .b.com
> acl sni_block ssl::server_name .c.com
> ssl_bump terminate !check_2 sni_block check
> ssl_bump splice all
> 
> 
> cache_mem 0 MB
> shutdown_lifetime 10 seconds
> httpd_suppress_version_string on
> dns_v4_first on
> forwarded_for delete
> via off
> reply_header_access Cache deny all

Do you actually see "Cache:" headers in any traffic? if so what do they
mean?

I ask because that header name has recently been proposed for
standardization by the HTTPbis working group at IETF as a formal
mechanism to replace the X-Cache and X-Cache-* headers. There should not
be any existing use of that "Cache:" name and I/we need to make the IETF
working group aware of any existing uses that may clash with the proposal.



> client_idle_pconn_timeout 1 minute
> server_idle_pconn_timeout 5 minute
> memory_pools off
> ipcache_size 128
> fqdncache_size 128
> reply_header_access Alternate-Protocol deny all
> reply_header_access alternate-protocol deny all

Header names are case-insensitive. The above is duplicate definition and
just wastes memory both at config parsing time, and in the permanent
memory representation of these header mangling rules.


> reply_header_access alt-svc deny all
> pinger_enable off
> digest_generation off
> netdb_filename none
> dns_nameservers 127.0.0.1
> reply_body_max_size 4 MB
> 
> 
...

> 
> I get quite a lot of messages in cache.log:
> 2018/10/09 12:38:49 kid1| ALE missing adapted HttpRequest object
> 2018/10/09 12:38:49 kid1| ALE missing URL
> 2018/10/09 12:38:49 kid1| ALE missing adapted HttpRequest object
> 2018/10/09 12:40:18 kid1| SECURITY ALERT: Host header forgery detected on
> local=212.95.165.32:443 remote=192.168.182.3:51304 FD 36 flags=33 (local IP
> does not match any domain IP)
> 2018/10/09 12:40:18 kid1| SECURITY ALERT: on URL:
> b.scorecardresearch.com:443
> 2018/10/09 12:40:28 kid1| SECURITY ALERT: Host header forgery detected on
> local=104.193.83.156:443 remote=192.168.182.3:51400 FD 183 flags=33 (local
> IP does not match any domain IP)
> 2018/10/09 12:40:28 kid1| SECURITY ALERT: on URL:
> csm2waycm-atl.netmng.com:443
> 2018/10/09 12:40:28 kid1| SECURITY ALERT: Host header forgery detected on
> local=104.193.83.156:443 remote=192.168.182.3:51402 FD 226 flags=33 (local
> IP does not match any domain IP)
> 
> My guess is, that the "header forgery" might be caused be inconsistency
> between browsers DNS-cache, my clients DNS-cache (Win 7) and the DNS-cache
> on the device, running squid. As practically all these "header forgeries"
> are for ad-networks, I consider it only an annoyance.Or is it _not_ ?
> 

As the warning message says: "local IP does not match any domain IP".
The client is trying to fetch data from a server which apparently is not
related to the origin server for the domain of the URL being fetched.

With your non-caching configuration it is just an annoyance. At worst it
means one of your clients is infected or being hijacked by a web-bug
script. So something to keep an eye on (thus the notices don't go away),
but not worry overly much about.

The issue of frequent false-positives is usually only seen when there
are very different recursive resolver chains being used to fetch the
information, or the parent resolver in that chain having non-standard
behaviour (as is the case with the 8.8.8.8 DNS services).
 Some things you can do to reduce these type of annoying warnings are
listed at <https://wiki.squid-cache.org/KnowledgeBase/HostHeaderForgery>.

Amos


From juju192 at yahoo.fr  Wed Oct 10 14:01:09 2018
From: juju192 at yahoo.fr (juju)
Date: Wed, 10 Oct 2018 14:01:09 +0000 (UTC)
Subject: [squid-users] SMP mode, workers, cache_dir with conditional
References: <902022008.10808634.1539180069037.ref@mail.yahoo.com>
Message-ID: <902022008.10808634.1539180069037@mail.yahoo.com>

Hello,Sorry if this question has already been posted ( I can't check as the "Search Function" is currently not working )I'm running?squid-3.5.20 on?Amazon Linux 2018.03. 2 cores with 8 GBMy squid is currently working but only on 1 core.I which to use the 2 cores. I understood that I have to use workers in squid.conf with separate cache directory.?So I modified default? squid.conf? with :--- [root at JujuServerTest2 ~]# egrep -v "^#|^$" /etc/squid/squid.confacl localnet src 10.0.0.0/8? ? ?# RFC1918 possible internal networkacl localnet src 172.16.0.0/12? # RFC1918 possible internal networkacl localnet src 192.168.0.0/16 # RFC1918 possible internal networkacl localnet src fc00::/7? ? ? ?# RFC 4193 local private network rangeacl localnet src fe80::/10? ? ? # RFC 4291 link-local (directly plugged) machinesacl SSL_ports port 443acl Safe_ports port 80? ? ? ? ? # httpacl Safe_ports port 21? ? ? ? ? # ftpacl Safe_ports port 443? ? ? ? ?# httpsacl Safe_ports port 70? ? ? ? ? # gopheracl Safe_ports port 210? ? ? ? ?# waisacl Safe_ports port 1025-65535? # unregistered portsacl Safe_ports port 280? ? ? ? ?# http-mgmtacl Safe_ports port 488? ? ? ? ?# gss-httpacl Safe_ports port 591? ? ? ? ?# filemakeracl Safe_ports port 777? ? ? ? ?# multiling httpacl CONNECT method CONNECThttp_access deny !Safe_portshttp_access deny CONNECT !SSL_portshttp_access allow localhost managerhttp_access deny managerhttp_access allow localnethttp_access allow localhosthttp_access deny allhttp_port 3128workers 2if ${process_number} = 1cache_dir rock /var/spool/squid1 1024endifif ${process_number} = 2cache_dir rock /var/spool/squid2 1024endifcoredump_dir /var/spool/squidrefresh_pattern ^ftp:? ? ? ? ? ?1440? ? 20%? ? ?10080refresh_pattern ^gopher:? ? ? ? 1440? ? 0%? ? ? 1440refresh_pattern -i (/cgi-bin/|\?) 0? ? ?0%? ? ? 0refresh_pattern .? ? ? ? ? ? ? ?0? ? ? ?20%? ? ?4320[root at JujuServerTest2 ~]#
---but/var/log/squid/squid.out :
[...]2018/10/10 13:52:15 kid2| Set Current Directory to /var/spool/squid2018/10/10 13:52:15 kid2| Creating missing swap directories2018/10/10 13:52:15 kid1| Set Current Directory to /var/spool/squid2018/10/10 13:52:15 kid1| Creating missing swap directories2018/10/10 13:52:15 kid3| Set Current Directory to /var/spool/squid2018/10/10 13:52:15 kid3| Creating missing swap directories2018/10/10 13:52:15 kid3| No cache_dir stores are configured.2018/10/10 13:52:15 kid3| Set Current Directory to /var/spool/squid2018/10/10 13:52:15 kid2| Set Current Directory to /var/spool/squid2018/10/10 13:52:15 kid1| Set Current Directory to /var/spool/squid2018/10/10 13:52:15 kid2| Creating missing swap directories2018/10/10 13:52:15 kid1| Creating missing swap directories2018/10/10 13:52:15 kid3| Creating missing swap directories2018/10/10 13:52:15 kid3| No cache_dir stores are configured?? ?<---------ps faux |grep squidroot? ? ? 4172? 0.0? 0.1 110512? 2012 pts/1? ? S+? ?14:00? ?0:00? ? ? ? ? ? ? ? ? ? ? ? ? \_ grep --color=auto squidroot? ? ? 4133? 0.0? 0.6 346844? 6848 ?? ? ? ? Ss? ?13:52? ?0:00 squid -f /etc/squid/squid.confsquid? ? ?4136? 0.0? 2.1 359828 21292 ?? ? ? ? S? ? 13:52? ?0:00? \_ (squid-coord-3) -f /etc/squid/squid.confsquid? ? ?4140? 0.0? 0.1? 28004? 1892 ?? ? ? ? S? ? 13:52? ?0:00? ? ? \_ (logfile-daemon) /var/log/squid/access.log


Looks like, my configuration is not good because "No cache_dir stores are configured"Coud you tell me where is my mistake ??
Thank you very much.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20181010/05e476db/attachment.htm>

From squid3 at treenet.co.nz  Wed Oct 10 14:24:08 2018
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 11 Oct 2018 03:24:08 +1300
Subject: [squid-users] Recent Squid 4 versions show ERR_CANNOT_FORWARD
 instead of ERR_DNS_FAIL
In-Reply-To: <CAFyThpJmJJo0SWh_iKGMR-Ui0acCQdmG+op13b-kSzd_r1KeZA@mail.gmail.com>
References: <CAFyThpJmJJo0SWh_iKGMR-Ui0acCQdmG+op13b-kSzd_r1KeZA@mail.gmail.com>
Message-ID: <b526da5c-6a7d-ad03-5f98-33cdd06f474c@treenet.co.nz>

On 11/10/18 12:58 AM, Christof Gerber wrote:
> I am wondering why the recent Squid4
> (v4-57a5679bae20e90ef73473e03327e37aa0263570) with a minimal config,

This is not how Squid versions are numbered.

The commit hash above matches to squid-5.0.0-20181008-r57a5679

Any build made from a checkout of that hash contains much code that has
never been in Squid-4 and never will.


> when accessing a non-existing domain (e.g.
> http://fsdafasdfsadfklsdj.ch/) produces a ERR_CANNOT_FORWARD error


Squid-5 code is currently in "alpha" state. Meaning it is not yet well
tested by many people beyond the particular change authors, and one
should expect to face not-yet documented behaviour changes and bugs when
running it.

If you did not intend to run the latest cutting-edge Squid code, you may
want to stick with the formal release bundles and snapshots rather than
building binaries by commit hash.

If you do want to run the latest development code, I thank you for the
interest and assistance testing. But please do refer to the commits by
their matching formal Squid versions (eg snapshot name or release
number). Doing so will clarify for you and for us both the expected
behaviour and the state of the code (alpha, beta, stable) you are comparing.


> when the older Squid4 (v4-6d8f397398995c4512cb045920ee2747cc6b14f8)

This hash being the Squid-4.2 stable release commit.

Which makes its version number(s) 4.2 or snapshot
squid-4.2-20180910-r6d8f397.


> produces a ERR_DNS_FAIL as it is also the case for Squid 3.5.
> 
> Why does this different behaviour occur?
> Is it a bug or is it a feature?


To answer your questions we will need to know what actual Squid version
you are running. Show the output of squid -v please.

Also the config that Squid is using. The contents of your squid.conf
please. Strip out #-commented lines if it is large.

Also what do DNS responses look like when the resolver being used by
Squid is asked to fetch those non-existent domains.
 Does it return an NXDOMAIN, a SERVFAIL, no response, a response with no
IPs, or a response with false IPs ?


> 
> Ps. Squid 4 v4-9d6a91ba608acadb9f3cb397d72748a711db3c91 is still
> producing ERR_DNS_FAIL.
> 

This is again a Squid-5 commit hash.

The equivalent Squid-4 hash was fcc58c36535edf58ecef3683762c051d4e7e93cc
and correlates to a 4.0.23 beta release snapshot.


Amos


From squid3 at treenet.co.nz  Wed Oct 10 15:10:26 2018
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 11 Oct 2018 04:10:26 +1300
Subject: [squid-users] SMP mode, workers, cache_dir with conditional
In-Reply-To: <902022008.10808634.1539180069037@mail.yahoo.com>
References: <902022008.10808634.1539180069037.ref@mail.yahoo.com>
 <902022008.10808634.1539180069037@mail.yahoo.com>
Message-ID: <9dad4be1-a258-04f9-e6e3-68d5f3e5fceb@treenet.co.nz>

On 11/10/18 3:01 AM, juju wrote:
> Hello,
> Sorry if this question has already been posted ( I can't check as the
> "Search Function" is currently not working )
> I'm running?squid-3.5.20 on?Amazon Linux 2018.03. 2 cores with 8 GB
> My squid is currently working but only on 1 core.
> I which to use the 2 cores. I understood that I have to use workers in
> squid.conf with separate cache directory.

This is not quite correct.

Per-worker configuration is only required for features which are not yet
SMP-aware.

The rock cache type *is* SMP-aware, so one directory can be shared by
workers.

The UFS/AUFS/diskd cache types are not SMP-aware yet, so they do need
separate directories per-worker currently.



Since you are using only rock type caches you do not have to do anything
special for SMP to work.


> So I modified default? squid.conf? with :
> ---
...
> workers 2
> if ${process_number} = 1
> cache_dir rock /var/spool/squid1 1024
> endif
> if ${process_number} = 2
> cache_dir rock /var/spool/squid2 1024
> endif
...
> 
> ---
> but
> /var/log/squid/squid.out :
> [...]
> 2018/10/10 13:52:15 kid3| Set Current Directory to /var/spool/squid
> 2018/10/10 13:52:15 kid2| Set Current Directory to /var/spool/squid
> 2018/10/10 13:52:15 kid1| Set Current Directory to /var/spool/squid
> 2018/10/10 13:52:15 kid2| Creating missing swap directories
> 2018/10/10 13:52:15 kid1| Creating missing swap directories
> 2018/10/10 13:52:15 kid3| Creating missing swap directories
> 2018/10/10 13:52:15 kid3| No cache_dir stores are configured?? ?<------

Notice the kid (process) numbers in those log lines.

You only defined cache directories for process #1 and #2. This line is
being reported by process #3 - which is the process coordinator process.
So it is correct for your config.

Also, this is informative message - not an error.


Amos



From mbarnes at orthotennessee.com  Wed Oct 10 17:16:41 2018
From: mbarnes at orthotennessee.com (Barnes, Michael)
Date: Wed, 10 Oct 2018 13:16:41 -0400
Subject: [squid-users] Problems with SSL setup with squid
Message-ID: <CAAx6SfHOUN7wnnMNcL9B4=u1WWzuSEsLvW_ZULJzSzQUsmJEhA@mail.gmail.com>

Good afternoon,

I am trying to setup squid with SSL inspection.  I'm currently running the
following OS and squid versions.
*lsb_release -a*
Distributor ID: Ubuntu
Description: Ubuntu 18.04.1 LTS
Release: 18.04
Codename: bionic

*squid -v*
Squid Cache: Version 3.5.28
Service Name: squid

This binary uses OpenSSL 1.0.2n  7 Dec 2017. For legal restrictions on
distribution see https://www.openssl.org/source/license.html

configure options:  '--enable-linux-netfilter' '--enable-icap-client'
'--enable-ssl' '--with-filedescriptors=65536' '--with-large-files'
'--prefix=/usr' '--localstatedir=/var' '--libexecdir=/lib/squid'
'--srcdir=.' '--datadir=/share/squid' '--sysconfdir=/etc/squid'
'--enable-ssl-crtd' '--with-openssl' --enable-ltdl-convenience

When I start squid or check squid while using sudo I get the following
errors.
*squid -s*
WARNING: Cannot write log file: /var/logs/cache.log
/var/logs/cache.log: Permission denied
         messages will be sent to 'stderr'.

*systemctl status squid*
? squid.service - LSB: Squid HTTP Proxy version 3.x
   Loaded: loaded (/etc/init.d/squid; generated)
   Active: active (exited) since Fri 2018-10-05 20:48:20 UTC; 4 days ago
     Docs: man:systemd-sysv-generator(8)
  Process: 1387 ExecStart=/etc/init.d/squid start (code=exited,
status=0/SUCCESS)

Oct 05 20:48:26 orthotnproxy (squid-1)[1937]: UFSSwapDir::openLog: Failed
to open swap log.
Oct 05 20:48:26 orthotnproxy squid[1625]: Squid Parent: (squid-1) process
1937 exited with status 1
Oct 05 20:48:29 orthotnproxy squid[1625]: Squid Parent: (squid-1) process
1946 started
Oct 05 20:48:29 orthotnproxy (squid-1)[1946]: UFSSwapDir::openLog: Failed
to open swap log.
Oct 05 20:48:29 orthotnproxy squid[1625]: Squid Parent: (squid-1) process
1946 exited with status 1
Oct 05 20:48:32 orthotnproxy squid[1625]: Squid Parent: (squid-1) process
1955 started
Oct 05 20:48:32 orthotnproxy (squid-1)[1955]: UFSSwapDir::openLog: Failed
to open swap log.
Oct 05 20:48:32 orthotnproxy squid[1625]: Squid Parent: (squid-1) process
1955 exited with status 1
Oct 05 20:48:32 orthotnproxy squid[1625]: Squid Parent: (squid-1) process
1955 will not be restarted due to repeated, frequent failures
Oct 05 20:48:32 orthotnproxy squid[1625]: Exiting due to repeated, frequent
failures

The SSL part of my config is:  (I can and will post more of the squid.conf
file if needed)
http_port 3128 ssl-bump cert=/etc/squid/ssl_cert/proxyCA.pem
generate-host-certificates=on dynamic_cert_mem_cache_size=4MB

acl step1 at_step SslBump1

ssl_bump peek step1
ssl_bump bump all

sslproxy_cafile /usr/local/openssl/cabundle.file

I'm lost and appreciate any help.

Thank you
Mike




-- 
--------------------------------------------
Michael Barnes
Operations Supervisor
OrthoTennessee
*865.769.4526*
mbarnes at orthotennessee.com <michael.barnes at orthotennessee.com>

*If you need help from the IT Dept. please email:*
support at orthotennessee.com

-- 


*The information contained in this transmission may contain privileged 
and confidential information, including patient information protected by 
federal and state privacy laws. It is intended only for the use of the 
person(s) named above. If you are not the intended recipient, you are 
hereby notified that any review, dissemination, distribution, or 
duplication of this communication is strictly prohibited. If you are not 
the intended recipient, please contact the sender by reply email and 
destroy all copies of the original message.*_he information contained in 
this transmission may contain privileged and confidential information, 
including patient information protected by federal and state privacy laws. 
It is intended only for the use of the person(s) named above. If you are 
not the intended recipient, you are hereby notified that any review, 
dissemination, distribution, or duplication of this communication is 
strictly prohibited. If you are not the intended recipient, please contact 
the sender by reply_
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20181010/39f2ef4e/attachment.htm>

From rousskov at measurement-factory.com  Wed Oct 10 17:38:36 2018
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Wed, 10 Oct 2018 11:38:36 -0600
Subject: [squid-users] SMP mode, workers, cache_dir with conditional
In-Reply-To: <9dad4be1-a258-04f9-e6e3-68d5f3e5fceb@treenet.co.nz>
References: <902022008.10808634.1539180069037.ref@mail.yahoo.com>
 <902022008.10808634.1539180069037@mail.yahoo.com>
 <9dad4be1-a258-04f9-e6e3-68d5f3e5fceb@treenet.co.nz>
Message-ID: <d9971606-6bc7-ad71-f80d-47e68867c20a@measurement-factory.com>

On 10/10/2018 09:10 AM, Amos Jeffries wrote:
> On 11/10/18 3:01 AM, juju wrote:
>> Hello,
>> Sorry if this question has already been posted ( I can't check as the
>> "Search Function" is currently not working )
>> I'm running?squid-3.5.20 on?Amazon Linux 2018.03. 2 cores with 8 GB
>> My squid is currently working but only on 1 core.
>> I which to use the 2 cores. I understood that I have to use workers in
>> squid.conf with separate cache directory.

> This is not quite correct.

> Per-worker configuration is only required for features which are not yet
> SMP-aware.


Just a couple of additional clarifications, not contradicting what Amos
has said:


> The rock cache type *is* SMP-aware, so one directory can be shared by
> workers.

"can be" and "must be". We do not support worker-specific cache_dirs.

In case of rock cache_dirs, I do not know for sure what would happen if
cache_dirs are configured on a per-worker basis, but I suspect that
there will be no disker processes to serve them. At best, you will get
no disk caching at all.


> The UFS/AUFS/diskd cache types are not SMP-aware yet, so they do need
> separate directories per-worker currently.

... but worker-specific cache_dirs are not supported, will violate HTTP
rules, etc.


> Since you are using only rock type caches you do not have to do anything
> special for SMP to work.

... with your simple configuration that does not use SMP-unaware features.

In fact, with rock cache_dirs, you do not even have to specify multiple
workers to get SMP Squid -- by default, Squid will run in SMP mode if
you have one worker (default) and at least one rock cache_dir. However,
multiple workers may be necessary to better utilize multicore CPUs.

Beware of hyperthreading: Do not expect two workers, each showing 80+%
CPU utilization to be able to share a single physical CPU core
efficiently. I do not know whether your two cores are hyperthreaded or real.

N.B. To search the mailing list archives, consider using your favorite
search engine. Many would accept a site:lists.squid-cache.org restriction.

Alex.


From squid3 at treenet.co.nz  Wed Oct 10 17:56:01 2018
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 11 Oct 2018 06:56:01 +1300
Subject: [squid-users] Problems with SSL setup with squid
In-Reply-To: <CAAx6SfHOUN7wnnMNcL9B4=u1WWzuSEsLvW_ZULJzSzQUsmJEhA@mail.gmail.com>
References: <CAAx6SfHOUN7wnnMNcL9B4=u1WWzuSEsLvW_ZULJzSzQUsmJEhA@mail.gmail.com>
Message-ID: <9f7f9ce2-a7a3-712d-b2ce-e3bfad973bec@treenet.co.nz>

On 11/10/18 6:16 AM, Barnes, Michael wrote:
> Good?afternoon,
> 
> I am trying to setup squid with SSL inspection.? I'm currently running
> the following OS and squid versions.
> *lsb_release -a*
> Distributor ID:Ubuntu
> Description:Ubuntu 18.04.1 LTS
> Release:18.04
> Codename:bionic
> 
> *squid -v*
> Squid Cache: Version 3.5.28
> Service Name: squid
> 
> This binary uses OpenSSL 1.0.2n? 7 Dec 2017. For legal restrictions on
> distribution see https://www.openssl.org/source/license.html
> 
> configure options:? '--enable-linux-netfilter' '--enable-icap-client'
> '--enable-ssl' '--with-filedescriptors=65536' '--with-large-files'
> '--prefix=/usr' '--localstatedir=/var' '--libexecdir=/lib/squid'
> '--srcdir=.' '--datadir=/share/squid' '--sysconfdir=/etc/squid'
> '--enable-ssl-crtd' '--with-openssl' --enable-ltdl-convenience
> 


This build is missing many of the integration options needed to install
Squid into the Linux filesystem used by Debian/Ubuntu. One of the
missing parts is setting up the correct logging paths.

You should be able to rebuild the Ubuntu Squid-3 package with OpenSSL
support using the following commands:

 apt-get build-dep squid
 apt-get install libssl1.0-dev squid-langpack
 mkdir /tmp/squid
 cd /tmp/squid
 apt-get source squid
 cd squid-*

then edit the debian/rules file to add:

  --enable-ssl-crtd --with-openssl

after the gnutls option already there (on the same line is easiest).


run this to build the package:
 dpkg-buildpackage && cd ..

then install:

 dpkg --install squid-common_*.deb squid_*.deb



If you can do so using the Squid v4 package from the cosmic repository
the resulting Squid binary will have better SSL-Bump abilities and less
annoying problems than the old v3.5 version has. That will need the
libssl-dev package to build instead of libssl1.0-dev.

If not then no matter, just using any properly integrated build will
resolve your current problem and several others you do not yet encounter.

Amos


From rafael.akchurin at diladele.com  Thu Oct 11 06:42:03 2018
From: rafael.akchurin at diladele.com (Rafael Akchurin)
Date: Thu, 11 Oct 2018 06:42:03 +0000
Subject: [squid-users] Ubuntu 18 LTS repository for Squid 4.3 (rebuilt with
 sslbump support from sources in Debian unstable)
Message-ID: <AM0PR04MB475357A5BCF72699B30EF7608FE10@AM0PR04MB4753.eurprd04.prod.outlook.com>

Greeting all,

The online repository with latest Squid 4.3 (rebuilt from Debian unstable with sslbump support) for Ubuntu 18 LTS 64-bit is available at squid43.diladele.com. Github repo at https://github.com/diladele/squid-ubuntu contains the scripts we used to make this compilation.

Hope you will find this helpful. Note that older repo of squid42.diladele.com will be taken down in two weeks.

Best regards,
Rafael Akchurin
Diladele B.V.

P.S. Here are simple instructions how to use the repo. For more information see readme at https://github.com/diladele/squid-ubuntu .

# add diladele apt key
wget -qO - http://packages.diladele.com/diladele_pub.asc | sudo apt-key add -

# add repo
echo "deb http://squid43.diladele.com/ubuntu/ bionic main" > /etc/apt/sources.list.d/squid43.diladele.com.list

# update the apt cache
apt-get update

# install
apt-get install squid-common
apt-get install squid
apt-get install squidclient


-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20181011/05ce94dc/attachment.htm>

From juju192 at yahoo.fr  Thu Oct 11 08:15:34 2018
From: juju192 at yahoo.fr (juju)
Date: Thu, 11 Oct 2018 08:15:34 +0000 (UTC)
Subject: [squid-users] SMP mode, workers, cache_dir with conditional
In-Reply-To: <d9971606-6bc7-ad71-f80d-47e68867c20a@measurement-factory.com>
References: <902022008.10808634.1539180069037.ref@mail.yahoo.com>
 <902022008.10808634.1539180069037@mail.yahoo.com>
 <9dad4be1-a258-04f9-e6e3-68d5f3e5fceb@treenet.co.nz>
 <d9971606-6bc7-ad71-f80d-47e68867c20a@measurement-factory.com>
Message-ID: <254583520.3398385.1539245734393@mail.yahoo.com>

 Hello !
Thank you very much Alex and AmosI have 2 real cores so it will be good.

on?http://www.squid-cache.org/Doc/config/cache_dir/? (available in: 4? ?3.5? ?3.4? ?3.3? ?3.2? ?2.7? ?3.1? ?3.0? ?2.6)???:"In SMP configurations, cache_dir must not precede the workers option and should use configuration macros or conditionals to give each worker interested in disk caching a dedicated cache directory"

Perhaps it would be good to specify for which version and which cache_dir type this note is for.
Best regards?
    Le mercredi 10 octobre 2018 ? 19:38:55 UTC+2, Alex Rousskov <rousskov at measurement-factory.com> a ?crit :  
 
 On 10/10/2018 09:10 AM, Amos Jeffries wrote:
> On 11/10/18 3:01 AM, juju wrote:
>> Hello,
>> Sorry if this question has already been posted ( I can't check as the
>> "Search Function" is currently not working )
>> I'm running?squid-3.5.20 on?Amazon Linux 2018.03. 2 cores with 8 GB
>> My squid is currently working but only on 1 core.
>> I which to use the 2 cores. I understood that I have to use workers in
>> squid.conf with separate cache directory.

> This is not quite correct.

> Per-worker configuration is only required for features which are not yet
> SMP-aware.


Just a couple of additional clarifications, not contradicting what Amos
has said:


> The rock cache type *is* SMP-aware, so one directory can be shared by
> workers.

"can be" and "must be". We do not support worker-specific cache_dirs.

In case of rock cache_dirs, I do not know for sure what would happen if
cache_dirs are configured on a per-worker basis, but I suspect that
there will be no disker processes to serve them. At best, you will get
no disk caching at all.


> The UFS/AUFS/diskd cache types are not SMP-aware yet, so they do need
> separate directories per-worker currently.

... but worker-specific cache_dirs are not supported, will violate HTTP
rules, etc.


> Since you are using only rock type caches you do not have to do anything
> special for SMP to work.

... with your simple configuration that does not use SMP-unaware features.

In fact, with rock cache_dirs, you do not even have to specify multiple
workers to get SMP Squid -- by default, Squid will run in SMP mode if
you have one worker (default) and at least one rock cache_dir. However,
multiple workers may be necessary to better utilize multicore CPUs.

Beware of hyperthreading: Do not expect two workers, each showing 80+%
CPU utilization to be able to share a single physical CPU core
efficiently. I do not know whether your two cores are hyperthreaded or real.

N.B. To search the mailing list archives, consider using your favorite
search engine. Many would accept a site:lists.squid-cache.org restriction.

Alex.
_______________________________________________
squid-users mailing list
squid-users at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-users
  
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20181011/adb57587/attachment.htm>

From josepjones at expedia.com  Thu Oct 11 12:30:15 2018
From: josepjones at expedia.com (Joseph Jones)
Date: Thu, 11 Oct 2018 12:30:15 +0000
Subject: [squid-users] Squid workers failing to restart after log rotation
	event.
Message-ID: <CY4PR02MB272576F3E472B634260BBEA8CFE10@CY4PR02MB2725.namprd02.prod.outlook.com>


I'm trying to find a root cause for failed workers. We have three squid instances that act as transparent forward proxies that limit internet connectivity for our network by doing url whitelisting. Current throughput per instances is about 90MB/s. after a restart of squid all workers seem to be working just fine, but after about an hour some of the workers fail and they never come back until we do a complete restart. These are EC2 instances in AWS (c5.4xlarge). so we have 16 vCPU to work with. but that's really 8 Cores and 2 Threads per core.

CPU and Memory loads are small. Disk IO could be a concern. We've ran some load test on a different instances with logging turned off and we were able to get a higher throughput without worker failure. We don't have caching enabled as most of our traffic is SSL anyway. I Was hoping someone could point us in a direction we should take our testing or if from the information I've given can tell use any obvious this we may be doing wrong.


$ uptime
?18:28:30 up 6 days,? 2:13,? 1 user,? load average: 0.88, 1.10, 1.09

$ free -m
????????????? total??????? used??????? free????? shared? buff/cache?? available
Mem:????????? 30987??????? 1728?????? 25264??????? 1156??????? 3994?????? 27523
Swap:???????????? 0?????????? 0?????????? 0



2018/10/10 18:19:42 kid2| Squid Cache (Version 4.1): Terminated abnormally.
CPU Usage: 0.036 seconds = 0.022 user + 0.014 sys
Maximum Resident Size: 92544 KB
Page faults with physical i/o: 0
2018/10/10 18:19:42 kid1| Closing HTTP(S) port [::]:3129
2018/10/10 18:19:42 kid1| Closing HTTP(S) port [::]:3128
2018/10/10 18:19:42 kid1| Closing HTTP(S) port [::]:3130
2018/10/10 18:19:42 kid1| storeDirWriteCleanLogs: Starting...
2018/10/10 18:19:42 kid1|?? Finished.? Wrote 0 entries.
2018/10/10 18:19:42 kid1|?? Took 0.00 seconds (? 0.00 entries/sec).
2018/10/10 18:19:42 kid1| FATAL: kid1 registration timed out
2018/10/10 18:19:42 kid1| Squid Cache (Version 4.1): Terminated abnormally.
CPU Usage: 0.034 seconds = 0.021 user + 0.013 sys
Maximum Resident Size: 92544 KB
Page faults with physical i/o: 0

$ cat /etc/redhat-release
CentOS Linux release 7.5.1804 (Core)

$ squid -v
Squid Cache: Version 4.1
Service Name: squid

This binary uses OpenSSL 1.0.2k-fips? 26 Jan 2017. For legal restrictions on distribution see https://www.openssl.org/source/license.html

configure options:? '--build=x86_64-redhat-linux-gnu' '--host=x86_64-redhat-linux-gnu' '--program-prefix=' '--prefix=/usr' '--exec-prefix=/usr' '--bindir=/usr/bin' '--sbindir=/usr/sbin' '--sysconfdir=/etc' '--datadir=/usr/share' '--includedir=/usr/include' '--libdir=/usr/lib64' '--libexecdir=/usr/libexec' '--sharedstatedir=/var/lib' '--mandir=/usr/share/man' '--infodir=/usr/share/info' '--exec_prefix=/usr' '--libexecdir=/usr/lib64/squid' '--localstatedir=/var' '--datadir=/usr/share/squid' '--sysconfdir=/etc/squid' '--with-logdir=$(localstatedir)/log/squid' '--with-pidfile=$(localstatedir)/run/squid.pid' '--disable-dependency-tracking' '--enable-follow-x-forwarded-for' '--enable-auth' '--enable-auth-basic=DB,LDAP,NCSA,NIS,PAM,POP3,RADIUS,SASL,SMB,getpwnam,fake' '--enable-auth-ntlm=fake' '--enable-auth-digest=file,LDAP,eDirectory' '--enable-auth-negotiate=kerberos,wrapper' '--enable-external-acl-helpers=wbinfo_group,kerberos_ldap_group,LDAP_group,delayer,file_userip,SQL_session,unix_group,session,time_quota' '--enable-cache-digests' '--enable-cachemgr-hostname=localhost' '--enable-delay-pools' '--enable-epoll' '--enable-smp' '--enable-icap-client' '--enable-ident-lookups' '--enable-linux-netfilter' '--enable-removal-policies=heap,lru' '--enable-snmp' '--enable-storeio=aufs,diskd,ufs,rock' '--enable-wccpv2' '--enable-esi' '--enable-security-cert-generators' '--enable-security-cert-validators' '--enable-icmp' '--with-aio' '--with-default-user=squid' '--with-filedescriptors=16384' '--with-dl' '--with-openssl' '--enable-ssl-crtd' '--with-pthreads' '--with-included-ltdl' '--disable-arch-native' '--enable-ecap' '--without-nettle' 'build_alias=x86_64-redhat-linux-gnu' 'host_alias=x86_64-redhat-linux-gnu' 'CFLAGS=-O2 -g -pipe -Wall -Wp,-D_FORTIFY_SOURCE=2 -fexceptions -fstack-protector-strong --param=ssp-buffer-size=4 -grecord-gcc-switches?? -m64 -mtune=generic' 'LDFLAGS=-Wl,-z,relro ' 'CXXFLAGS=-O2 -g -pipe -Wall -Wp,-D_FORTIFY_SOURCE=2 -fexceptions -fstack-protector-strong --param=ssp-buffer-size=4 -grecord-gcc-switches?? -m64 -mtune=generic -fPIC' 'PKG_CONFIG_PATH=:/usr/lib64/pkgconfig:/usr/share/pkgconfig' --enable-ltdl-convenience

$ cat /etc/squid/squid.conf
workers 12
hopeless_kid_revival_delay 5 minute
# Default 'squid' logformat with request size and TLS SNI added
logformat ean_squid %ts.%03tu %6tr %>a %Ss/%03>Hs %>st %<st %rm %ru %ssl::>sni %[un %Sh/%<a %mt
logfile_rotate 0
access_log daemon:/var/log/squid/access.log logformat=ean_squid
debug_options ALL,1

# Only allow cachemgr access from localhost
http_access allow localhost manager
http_access deny manager

acl localnet src 10.26.128.0/21

acl SSL_ports port 443
acl Safe_ports port 80??? # http
acl Safe_ports port 443?? # https

acl CONNECT method CONNECT

#
# Recommended minimum Access Permission configuration:
#
# Deny requests to certain unsafe ports
http_access deny !Safe_ports

# Deny CONNECT to other than secure SSL ports
http_access deny CONNECT !SSL_ports

# Allow requests from the local network (see acl at the top)
http_access allow localnet

#
# INSERT YOUR OWN RULE(S) HERE TO ALLOW ACCESS FROM YOUR CLIENTS
#

# Just for debugging
# debug_options ALL,1 33,2 rotate=0

acl https_whitelist ssl::server_name "/etc/squid/whitelist.txt"
acl http_whitelist dstdomain "/etc/squid/whitelist.txt"

acl step1 at_step SslBump1
acl step2 at_step SslBump2
acl step3 at_step SslBump3

http_access allow http_whitelist

ssl_bump peek step1 all

ssl_bump peek step2 https_whitelist
ssl_bump splice step3 https_whitelist
ssl_bump terminate step2 all


# disable caching
cache deny all

# And finally deny all other access to this proxy
http_access deny all

# Squid normally listens to port 3128
http_port 3129 intercept
http_port 3128
https_port 3130 cert=/etc/pki/tls/certs/squid.pem key=/etc/pki/tls/private/squid.key ssl-bump intercept

visible_hostname squid

# Uncomment and adjust the following to add a disk cache directory.
#cache_dir ufs /var/spool/squid 100 16 256

# Leave coredumps in the first cache dir
coredump_dir /var/spool/squid

#
# Add any of your own refresh_pattern entries above these.
#
refresh_pattern ^ftp:?? 1440? 20% 10080
refresh_pattern ^gopher:? 1440? 0%? 1440
refresh_pattern -i (/cgi-bin/|\?) 0 0%? 0
refresh_pattern .?? 0 20% 4320


 --?
 
 
 Joseph M Jones
 
 Senior Application Engineer
 Expedia Partner Solutions

From rousskov at measurement-factory.com  Thu Oct 11 15:56:55 2018
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Thu, 11 Oct 2018 09:56:55 -0600
Subject: [squid-users] SMP mode, workers, cache_dir with conditional
In-Reply-To: <254583520.3398385.1539245734393@mail.yahoo.com>
References: <902022008.10808634.1539180069037.ref@mail.yahoo.com>
 <902022008.10808634.1539180069037@mail.yahoo.com>
 <9dad4be1-a258-04f9-e6e3-68d5f3e5fceb@treenet.co.nz>
 <d9971606-6bc7-ad71-f80d-47e68867c20a@measurement-factory.com>
 <254583520.3398385.1539245734393@mail.yahoo.com>
Message-ID: <3ace4463-acbe-4af1-9e89-5cf906c0c011@measurement-factory.com>

On 10/11/2018 02:15 AM, juju wrote:

> on?http://www.squid-cache.org/Doc/config/cache_dir/:
> "In SMP configurations, cache_dir must not precede the workers option
> and should use configuration macros or conditionals to give each worker
> interested in disk caching a dedicated cache directory"

> Perhaps it would be good to specify for which version and which
> cache_dir type this note is for.

Yes, that stale note should be rephrased to discourage unsupported
configurations or even replaced with Squid warnings about unsupported
use of ufs-based cache_dirs in SMP configurations.

I am not sure, but I suspect the "must not precede" part is also stale
or, at the very least, should be unnecessary -- if the right order is
still required, Squid should detect wrong order and quit with an error
message.


Quality patches welcomed.

Alex.



> Le mercredi 10 octobre 2018 ? 19:38:55 UTC+2, Alex Rousskov a ?crit :
> 
> 
> On 10/10/2018 09:10 AM, Amos Jeffries wrote:
>> On 11/10/18 3:01 AM, juju wrote:
>>> Hello,
>>> Sorry if this question has already been posted ( I can't check as the
>>> "Search Function" is currently not working )
>>> I'm running?squid-3.5.20 on?Amazon Linux 2018.03. 2 cores with 8 GB
>>> My squid is currently working but only on 1 core.
>>> I which to use the 2 cores. I understood that I have to use workers in
>>> squid.conf with separate cache directory.
> 
>> This is not quite correct.
> 
>> Per-worker configuration is only required for features which are not yet
>> SMP-aware.
> 
> 
> Just a couple of additional clarifications, not contradicting what Amos
> has said:
> 
> 
>> The rock cache type *is* SMP-aware, so one directory can be shared by
>> workers.
> 
> "can be" and "must be". We do not support worker-specific cache_dirs.
> 
> In case of rock cache_dirs, I do not know for sure what would happen if
> cache_dirs are configured on a per-worker basis, but I suspect that
> there will be no disker processes to serve them. At best, you will get
> no disk caching at all.
> 
> 
>> The UFS/AUFS/diskd cache types are not SMP-aware yet, so they do need
>> separate directories per-worker currently.
> 
> ... but worker-specific cache_dirs are not supported, will violate HTTP
> rules, etc.
> 
> 
>> Since you are using only rock type caches you do not have to do anything
>> special for SMP to work.
> 
> ... with your simple configuration that does not use SMP-unaware features.
> 
> In fact, with rock cache_dirs, you do not even have to specify multiple
> workers to get SMP Squid -- by default, Squid will run in SMP mode if
> you have one worker (default) and at least one rock cache_dir. However,
> multiple workers may be necessary to better utilize multicore CPUs.
> 
> Beware of hyperthreading: Do not expect two workers, each showing 80+%
> CPU utilization to be able to share a single physical CPU core
> efficiently. I do not know whether your two cores are hyperthreaded or real.
> 
> N.B. To search the mailing list archives, consider using your favorite
> search engine. Many would accept a site:lists.squid-cache.org restriction.
> 
> Alex.
> 
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org <mailto:squid-users at lists.squid-cache.org>
> http://lists.squid-cache.org/listinfo/squid-users



From rousskov at measurement-factory.com  Thu Oct 11 16:44:00 2018
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Thu, 11 Oct 2018 10:44:00 -0600
Subject: [squid-users] Squid workers failing to restart after log
 rotation event.
In-Reply-To: <CY4PR02MB272576F3E472B634260BBEA8CFE10@CY4PR02MB2725.namprd02.prod.outlook.com>
References: <CY4PR02MB272576F3E472B634260BBEA8CFE10@CY4PR02MB2725.namprd02.prod.outlook.com>
Message-ID: <a3ee63ec-42b1-b1ad-9d8a-c7d0958d88ec@measurement-factory.com>

On 10/11/2018 06:30 AM, Joseph Jones wrote:

> I'm trying to find a root cause for failed workers.

My suggestions:

0. Upgrade to the latest Squid v4.

1. Disable memory cache:
   cache_mem 0

2. Maintain one cache.log per worker. For example:
cache_log /usr/local/squid/var/logs/cache-${process_number}.log

3. Focus on the logged messages of the worker that fails _first_. Zero
in on the first failure of that worker. Why did it fail? Your log
snippets appear to show what happened _after_ at least one failure. It
is usually best to focus on the original failure. Check system logs as
well -- some relevant messages may only appear there.

4. With the information from #3, check whether you are suffering from
bug #4796. It has a (hidden in the PR discussion) short-term fix:
https://bugs.squid-cache.org/show_bug.cgi?id=4796
https://github.com/squid-cache/squid/pull/257#issuecomment-427271426

If your bug is different, and the first failure of the failed-first
worker left a core dump, post a stack trace. Configure your OS to allow
core dumps, of course.


You can just skip to step #4 and see if the patch helps, but steps #0-3
are useful in general should you face similar problems in the future.


HTH,

Alex.


 We have three squid instances that act as transparent forward proxies
that limit internet connectivity for our network by doing url
whitelisting. Current throughput per instances is about 90MB/s. after a
restart of squid all workers seem to be working just fine, but after
about an hour some of the workers fail and they never come back until we
do a complete restart. These are EC2 instances in AWS (c5.4xlarge). so
we have 16 vCPU to work with. but that's really 8 Cores and 2 Threads
per core.
> 
> CPU and Memory loads are small. Disk IO could be a concern. We've ran some load test on a different instances with logging turned off and we were able to get a higher throughput without worker failure. We don't have caching enabled as most of our traffic is SSL anyway. I Was hoping someone could point us in a direction we should take our testing or if from the information I've given can tell use any obvious this we may be doing wrong.
> 
> 
> $ uptime
> ?18:28:30 up 6 days,? 2:13,? 1 user,? load average: 0.88, 1.10, 1.09
> 
> $ free -m
> ????????????? total??????? used??????? free????? shared? buff/cache?? available
> Mem:????????? 30987??????? 1728?????? 25264??????? 1156??????? 3994?????? 27523
> Swap:???????????? 0?????????? 0?????????? 0
> 
> 
> 
> 2018/10/10 18:19:42 kid2| Squid Cache (Version 4.1): Terminated abnormally.
> CPU Usage: 0.036 seconds = 0.022 user + 0.014 sys
> Maximum Resident Size: 92544 KB
> Page faults with physical i/o: 0
> 2018/10/10 18:19:42 kid1| Closing HTTP(S) port [::]:3129
> 2018/10/10 18:19:42 kid1| Closing HTTP(S) port [::]:3128
> 2018/10/10 18:19:42 kid1| Closing HTTP(S) port [::]:3130
> 2018/10/10 18:19:42 kid1| storeDirWriteCleanLogs: Starting...
> 2018/10/10 18:19:42 kid1|?? Finished.? Wrote 0 entries.
> 2018/10/10 18:19:42 kid1|?? Took 0.00 seconds (? 0.00 entries/sec).
> 2018/10/10 18:19:42 kid1| FATAL: kid1 registration timed out
> 2018/10/10 18:19:42 kid1| Squid Cache (Version 4.1): Terminated abnormally.
> CPU Usage: 0.034 seconds = 0.021 user + 0.013 sys
> Maximum Resident Size: 92544 KB
> Page faults with physical i/o: 0
> 
> $ cat /etc/redhat-release
> CentOS Linux release 7.5.1804 (Core)
> 
> $ squid -v
> Squid Cache: Version 4.1
> Service Name: squid
> 
> This binary uses OpenSSL 1.0.2k-fips? 26 Jan 2017. For legal restrictions on distribution see https://www.openssl.org/source/license.html
> 
> configure options:? '--build=x86_64-redhat-linux-gnu' '--host=x86_64-redhat-linux-gnu' '--program-prefix=' '--prefix=/usr' '--exec-prefix=/usr' '--bindir=/usr/bin' '--sbindir=/usr/sbin' '--sysconfdir=/etc' '--datadir=/usr/share' '--includedir=/usr/include' '--libdir=/usr/lib64' '--libexecdir=/usr/libexec' '--sharedstatedir=/var/lib' '--mandir=/usr/share/man' '--infodir=/usr/share/info' '--exec_prefix=/usr' '--libexecdir=/usr/lib64/squid' '--localstatedir=/var' '--datadir=/usr/share/squid' '--sysconfdir=/etc/squid' '--with-logdir=$(localstatedir)/log/squid' '--with-pidfile=$(localstatedir)/run/squid.pid' '--disable-dependency-tracking' '--enable-follow-x-forwarded-for' '--enable-auth' '--enable-auth-basic=DB,LDAP,NCSA,NIS,PAM,POP3,RADIUS,SASL,SMB,getpwnam,fake' '--enable-auth-ntlm=fake' '--enable-auth-digest=file,LDAP,eDirectory' '--enable-auth-negotiate=kerberos,wrapper' '--enable-external-acl-helpers=wbinfo_group,kerberos_ldap_group,LDAP_group,delayer,file_userip,SQL_session,unix_group,session,time_quota' '--enable-cache-digests' '--enable-cachemgr-hostname=localhost' '--enable-delay-pools' '--enable-epoll' '--enable-smp' '--enable-icap-client' '--enable-ident-lookups' '--enable-linux-netfilter' '--enable-removal-policies=heap,lru' '--enable-snmp' '--enable-storeio=aufs,diskd,ufs,rock' '--enable-wccpv2' '--enable-esi' '--enable-security-cert-generators' '--enable-security-cert-validators' '--enable-icmp' '--with-aio' '--with-default-user=squid' '--with-filedescriptors=16384' '--with-dl' '--with-openssl' '--enable-ssl-crtd' '--with-pthreads' '--with-included-ltdl' '--disable-arch-native' '--enable-ecap' '--without-nettle' 'build_alias=x86_64-redhat-linux-gnu' 'host_alias=x86_64-redhat-linux-gnu' 'CFLAGS=-O2 -g -pipe -Wall -Wp,-D_FORTIFY_SOURCE=2 -fexceptions -fstack-protector-strong --param=ssp-buffer-size=4 -grecord-gcc-switches?? -m64 -mtune=generic' 'LDFLAGS=-Wl,-z,relro ' 'CXXFLAGS=-O2 -g -pipe -Wall -Wp,-D_FORTIFY_SOURCE=2 -fexceptions -fstack-protector-strong --param=ssp-buffer-size=4 -grecord-gcc-switches?? -m64 -mtune=generic -fPIC' 'PKG_CONFIG_PATH=:/usr/lib64/pkgconfig:/usr/share/pkgconfig' --enable-ltdl-convenience
> 
> $ cat /etc/squid/squid.conf
> workers 12
> hopeless_kid_revival_delay 5 minute
> # Default 'squid' logformat with request size and TLS SNI added
> logformat ean_squid %ts.%03tu %6tr %>a %Ss/%03>Hs %>st %<st %rm %ru %ssl::>sni %[un %Sh/%<a %mt
> logfile_rotate 0
> access_log daemon:/var/log/squid/access.log logformat=ean_squid
> debug_options ALL,1
> 
> # Only allow cachemgr access from localhost
> http_access allow localhost manager
> http_access deny manager
> 
> acl localnet src 10.26.128.0/21
> 
> acl SSL_ports port 443
> acl Safe_ports port 80??? # http
> acl Safe_ports port 443?? # https
> 
> acl CONNECT method CONNECT
> 
> #
> # Recommended minimum Access Permission configuration:
> #
> # Deny requests to certain unsafe ports
> http_access deny !Safe_ports
> 
> # Deny CONNECT to other than secure SSL ports
> http_access deny CONNECT !SSL_ports
> 
> # Allow requests from the local network (see acl at the top)
> http_access allow localnet
> 
> #
> # INSERT YOUR OWN RULE(S) HERE TO ALLOW ACCESS FROM YOUR CLIENTS
> #
> 
> # Just for debugging
> # debug_options ALL,1 33,2 rotate=0
> 
> acl https_whitelist ssl::server_name "/etc/squid/whitelist.txt"
> acl http_whitelist dstdomain "/etc/squid/whitelist.txt"
> 
> acl step1 at_step SslBump1
> acl step2 at_step SslBump2
> acl step3 at_step SslBump3
> 
> http_access allow http_whitelist
> 
> ssl_bump peek step1 all
> 
> ssl_bump peek step2 https_whitelist
> ssl_bump splice step3 https_whitelist
> ssl_bump terminate step2 all
> 
> 
> # disable caching
> cache deny all
> 
> # And finally deny all other access to this proxy
> http_access deny all
> 
> # Squid normally listens to port 3128
> http_port 3129 intercept
> http_port 3128
> https_port 3130 cert=/etc/pki/tls/certs/squid.pem key=/etc/pki/tls/private/squid.key ssl-bump intercept
> 
> visible_hostname squid
> 
> # Uncomment and adjust the following to add a disk cache directory.
> #cache_dir ufs /var/spool/squid 100 16 256
> 
> # Leave coredumps in the first cache dir
> coredump_dir /var/spool/squid
> 
> #
> # Add any of your own refresh_pattern entries above these.
> #
> refresh_pattern ^ftp:?? 1440? 20% 10080
> refresh_pattern ^gopher:? 1440? 0%? 1440
> refresh_pattern -i (/cgi-bin/|\?) 0 0%? 0
> refresh_pattern .?? 0 20% 4320
> 
> 
>  --?
>  
>  
>  Joseph M Jones
>  
>  Senior Application Engineer
>  Expedia Partner Solutions
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
> 


From squid-user at tlinx.org  Thu Oct 11 23:34:04 2018
From: squid-user at tlinx.org (L A Walsh)
Date: Thu, 11 Oct 2018 16:34:04 -0700
Subject: [squid-users] tls_outgoing_options, cipher list not parseable
Message-ID: <5BBFDDEC.4020401@tlinx.org>

I seem to have a problem specifying the cipher list in the tls_outgoing 
options.
The line I have:
tls_outgoing_options 
options=NOSSLv3,SINGLE_DH_USE,SINGLE_ECDH_USE,cipher=EECDH+ECDSA+AESGCM:\
EECDH+aRSA+AESGCM:EECDH+ECDSA+SHA384:EECDH+ECDSA+SHA256:EECDH+aRSA+SHA384:\
EECDH+aRSA+SHA256:EECDH+aRSA+RC4:!RC4:!aNULL:!eNULL:!LOW:!3DES:!MD5:!EXP:!PSK:!SRP:!DSS

Of note, I split the line here in email with '\', but in the config
file, it is one long line (w/o the '\').

The error I get from squid 4.0.25 is: (using check)

# /usr/sbin/squid -k check
2018/10/11 16:14:31| FATAL: Unknown TLS option 
'=EECDH-ECDSA-AESGCM:EECDH-aRSA-AESGCM:EECDH-ECDSA-SHA384:EECDH-ECDSA-SHA256:\
EECDH-aRSA-SHA384:EECDH-aRSA-SHA256:EECDH-aRSA-RC4:!RC4:!aNULL:!eNULL:!LOW:!3DES:\
!MD5:!EXP:!PSK:!SRP:!DSS'

(w/o the splits).

I can't tell what it is objecting to.

To give it a rootcert, can I re-use the same rootcert
I had in 3.x?


Below is my config w/o comment lines.  This is a private proxy.


acl msdata dstdomain \.data\.microsoft\.com
acl localnet  src 127.0.0.0/8
acl localnet  src 192.168.3.0/24
acl sc_subnet src 192.168.3.0/24
acl robot_txt url_regex -i ^http.*/robots.txt$
acl SSL_ports port 443
acl Safe_ports port 80        # http
acl Safe_ports port 81        # http
acl Safe_ports port 82        # http
acl Safe_ports port 21        # ftp
acl Safe_ports port 443        # https
acl Safe_ports port 70        # gopher
acl Safe_ports port 210        # wais
acl Safe_ports port 1024-65535    # unregistered ports
acl Safe_ports port 280        # http-mgmt
acl Safe_ports port 488        # gss-http
acl Safe_ports port 591        # filemaker
acl Safe_ports port 777        # multiling http
acl Allowed_Connect port 1024-65535    #allowed non-SSL Connects to 
non-reserved ports
acl CONNECT method CONNECT
http_access deny !Safe_ports
http_access deny msdata
http_access allow CONNECT Safe_Ports
http_access allow localhost manager
http_access allow localnet manager
http_access deny manager
http_access allow localnet
http_access allow localhost
http_access allow all
http_port ishtar.sc.tlinx.org:8118 ignore-cc ssl-bump 
generate-host-certificates=on dynamic_cert_mem_cache_size=64MB
http_port ishtar.sc.tlinx.org:8080 ignore-cc
http_port 127.0.0.1:8118 ignore-cc
http_port 127.0.0.1:8080 ignore-cc
http_port wpad.sc.tlinx.org:80
acl WPAD urlpath_regex ^/wpad.dat$
deny_info 200:wpad.dat WPAD
http_access deny WPAD
reply_header_access Content-Type deny WPAD
reply_header_replace Content-Type application/x-ns-proxy-autoconfig
acl internal_net src 192.168.3.0/24
clientside_tos 0x54
tls_outgoing_options 
options=NOSSLv3,SINGLE_DH_USE,SINGLE_ECDH_USE,cipher=EECDH-ECDSA-AESGCM:EECDH-aRSA-AESGCM:EECDH-ECDSA-SHA384:EECDH-ECDSA-SHA256:EECDH-aRSA-SHA384:EECDH-aRSA-SHA256:EECDH-aRSA-RC4:!RC4:!aNULL:!eNULL:!LOW:!3DES:!MD5:!EXP:!PSK:!SRP:!DSS
sslproxy_session_ttl 900
sslproxy_session_cache_size 16 MB
sslcrtd_program /usr/lib64/squid/security_file_certgen -s 
/var/cache/squid/lib/ssl_db -M 128MB
maximum_object_size 2 GB
cache_dir aufs /var/cache/squid 98304 64 64
workers 1
log_mime_hdrs on
strip_query_terms off
buffered_logs on
cache_log /var/log/squid/cache.log squid
debug_options ALL,1,11,2 rotate=10
coredump_dir /var/cache/squid rotate=10
url_rewrite_host_header off
url_rewrite_access deny all
max_stale 60 days
refresh_pattern -i /robots.txt$  600 90% 3600 ignore-reload 
ignore-no-store ignore-must-revalidate ignore-private ignore-auth 
override-lastmod store-stale
refresh_pattern -i download 10 50% 100800 override-expire ignore-private 
ignore-must-revalidate
refresh_pattern -i \.flv 10080 90% 10080 override-expire ignore-private
refresh_pattern -i \.pdf 3600 90% 10080 ignore-no-store ignore-private 
override-expire
refresh_pattern -i \.(ico|gif|jpg|png)   600 20%   4320    
ignore-private override-expire
refresh_pattern ^http(s)?://bakabt.me   1200 30%   14320    
ignore-private override-expire ignore-no-store ignore-no-cache 
ignore-must-revalidate
refresh_pattern ^http(s)?://*.bakashots.me   1200 30%   14320    
ignore-private override-expire ignore-no-store ignore-no-cache 
ignore-must-revalidate
refresh_pattern -i \.html   0 20%   4320    ignore-private  ignore-no-store
refresh_pattern -i (/cgi-bin/|\?) 0    10%    1    ignore-private
refresh_pattern ^(http|https):   0 20%   4320    ignore-private
refresh_pattern ^ftp:        1440    20%    10080
refresh_pattern ^gopher:    1440    0%    1440
refresh_pattern .        0    20%    4320
quick_abort_min 16 MB
quick_abort_max 24 MB
quick_abort_pct 75
read_ahead_gap 768 MB
negative_ttl 2 seconds
range_offset_limit 1 MB
store_avg_object_size 256 KB
store_objects_per_bucket 32
request_header_max_size 1 MB
client_request_buffer_max_size 2 MB
vary_ignore_expire on
request_header_access Strict-Transport-Security deny all
request_header_replace Strict-Transport-Security max-age=0; 
includeSubDomains
reply_header_access Strict-Transport-Security deny all
reply_header_replace Strict-Transport-Security max-age=0; includeSubDomains
collapsed_forwarding on
forward_timeout 10 seconds
request_timeout 45 seconds
request_timeout 45 seconds
ident_timeout 1 seconds
shutdown_lifetime 8 seconds
visible_hostname    web-proxy
hostname_aliases ishtar ishtar.sc.tlinx.org web-proxy ns1.sc.tlinx.org  
webproxy
umask 002
always_direct allow all
dns_packet_max 1400 bytes
dns_defnames on
dns_v4_first on
memory_pools_limit 2 GB
forwarded_for transparent
reload_into_ims on
connect_retries 2
retry_on_error on
pipeline_prefetch 8
high_response_time_warning 15000
high_page_fault_warning 512





From squid3 at treenet.co.nz  Fri Oct 12 04:05:24 2018
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Fri, 12 Oct 2018 17:05:24 +1300
Subject: [squid-users] tls_outgoing_options, cipher list not parseable
In-Reply-To: <5BBFDDEC.4020401@tlinx.org>
References: <5BBFDDEC.4020401@tlinx.org>
Message-ID: <249dcdbe-b561-b28e-9353-89de43bab1ce@treenet.co.nz>

On 12/10/18 12:34 PM, L A Walsh wrote:
> I seem to have a problem specifying the cipher list in the tls_outgoing
> options.
> The line I have:
> tls_outgoing_options
> options=NOSSLv3,SINGLE_DH_USE,SINGLE_ECDH_USE,cipher=EECDH+ECDSA+AESGCM:\

 Comma  .....................................^^^^^

> EECDH+aRSA+AESGCM:EECDH+ECDSA+SHA384:EECDH+ECDSA+SHA256:EECDH+aRSA+SHA384:\
> EECDH+aRSA+SHA256:EECDH+aRSA+RC4:!RC4:!aNULL:!eNULL:!LOW:!3DES:!MD5:!EXP:!PSK:!SRP:!DSS
> 
> 
> Of note, I split the line here in email with '\', but in the config
> file, it is one long line (w/o the '\').

Squid understands line wrapping in the form of  '\' terminators and
whitespace prefix on the next line. So you can make the config easier to
read and fix bugs like above by using the wrapping.

tls_outgoing_options options=... \
  cipher=...


> 
> The error I get from squid 4.0.25 is: (using check)
> 
> # /usr/sbin/squid -k check
> 2018/10/11 16:14:31| FATAL: Unknown TLS option
> '=EECDH-ECDSA-AESGCM:EECDH-aRSA-AESGCM:EECDH-ECDSA-SHA384:EECDH-ECDSA-SHA256:\
> 
> EECDH-aRSA-SHA384:EECDH-aRSA-SHA256:EECDH-aRSA-RC4:!RC4:!aNULL:!eNULL:!LOW:!3DES:\
> 
> !MD5:!EXP:!PSK:!SRP:!DSS'
> 
> (w/o the splits).
> 
> I can't tell what it is objecting to.

There is no such "options=" setting as ",cipher=EECDH+..."


> 
> To give it a rootcert, can I re-use the same rootcert
> I had in 3.x?
> 

Yes.



Amos


From squid at bloms.de  Fri Oct 12 14:08:23 2018
From: squid at bloms.de (Dieter Bloms)
Date: Fri, 12 Oct 2018 16:08:23 +0200
Subject: [squid-users] Support for DistributionPoints in the dynamic creates
 certificate via sslbump
Message-ID: <20181012140823.jgsbaqx33i3sg6q7@bloms.de>

Hello,

we use the sslbump feature of squid, and it works very well.
One of our http clients expect a CRL distribution point in the dynamic
generated certificate.
I've setup a http server, which delivers this crl list, but don't know
how to configure squid to set this distribution point in every
dynamic gererated certificate.

Does anybody know whether squid support this feature ?

Thank you very much.


-- 
Regards

  Dieter

--
I do not get viruses because I do not use MS software.
If you use Outlook then please do not put my email address in your
address-book so that WHEN you get a virus it won't use my address in the
>From field.


From morteza1131 at yahoo.com  Fri Oct 12 14:41:44 2018
From: morteza1131 at yahoo.com (morteza omidian)
Date: Fri, 12 Oct 2018 14:41:44 +0000 (UTC)
Subject: [squid-users] squid and iptables
References: <1965406216.157260.1539355304217.ref@mail.yahoo.com>
Message-ID: <1965406216.157260.1539355304217@mail.yahoo.com>

HiI asked my question before.It seems that anybody can not answer it!!
As you know, we can mark packets when they go out of squid with "tcp_outgoing_mark", this mark is based on Source IP or Source MAC of client, but i want to mark them based on mark of input packet, something like this:
In iptables > mangle > PREROUTING:? 
iptables -t mangle -A PREROUTING -p tcp --dport 80 -j MARK --set-mark 1
In Squid Configuration:acl MARKED_PACKETS nfmark 1tcp_outgoing_mark 1 MARKED_PACKETS


Is that possible? How can i solve my problem??
Tanx
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20181012/85a5a85b/attachment.htm>

From Antony.Stone at squid.open.source.it  Fri Oct 12 15:07:22 2018
From: Antony.Stone at squid.open.source.it (Antony Stone)
Date: Fri, 12 Oct 2018 17:07:22 +0200
Subject: [squid-users] squid and iptables
In-Reply-To: <1965406216.157260.1539355304217@mail.yahoo.com>
References: <1965406216.157260.1539355304217.ref@mail.yahoo.com>
 <1965406216.157260.1539355304217@mail.yahoo.com>
Message-ID: <201810121707.22932.Antony.Stone@squid.open.source.it>

On Friday 12 October 2018 at 16:41:44, morteza omidian wrote:

> HiI asked my question before.It seems that anybody can not answer it!!

Did you miss the response from Amos?

http://lists.squid-cache.org/pipermail/squid-users/2018-October/019389.html

> As you know, we can mark packets when they go out of squid with
> "tcp_outgoing_mark", this mark is based on Source IP or Source MAC of
> client, but i want to mark them based on mark of input packet, something
> like this: In iptables > mangle > PREROUTING:
> iptables -t mangle -A PREROUTING -p tcp --dport 80 -j MARK --set-mark 1
> In Squid Configuration:acl MARKED_PACKETS nfmark 1tcp_outgoing_mark 1
> MARKED_PACKETS
> 
> 
> Is that possible? How can i solve my problem??
> Tanx

PS: Please do not set "Reply-to" on list emails.

Thanks,


Antony.

-- 
Schr?dinger's rule of data integrity: the condition of any backup is unknown 
until a restore is attempted.

                                                   Please reply to the list;
                                                         please *don't* CC me.


From morteza1131 at yahoo.com  Fri Oct 12 16:13:06 2018
From: morteza1131 at yahoo.com (morteza omidian)
Date: Fri, 12 Oct 2018 16:13:06 +0000 (UTC)
Subject: [squid-users] squid and iptables
In-Reply-To: <201810121707.22932.Antony.Stone@squid.open.source.it>
References: <1965406216.157260.1539355304217.ref@mail.yahoo.com>
 <1965406216.157260.1539355304217@mail.yahoo.com>
 <201810121707.22932.Antony.Stone@squid.open.source.it>
Message-ID: <1240561542.221562.1539360786756@mail.yahoo.com>

 
Tank you, I see it now.It does not help me, I want to have an acl to select traffic (HTTP traffic that comes from client to squid) that have a specific packet mark and then send them out with another mark. like this:In iptables-mangle-PREROUTING:? 
iptables -t mangle -A PREROUTING -p tcp --dport 80 -j MARK --set-mark 1
In Squid Configuration:acl MARKED_PACKETS nfmark 1tcp_outgoing_mark 1 MARKED_PACKETS

Is that possible or not?I want this kind of marks because I need to determine source interface of packets after they go out of squid! 
    On Friday, October 12, 2018, 6:37:44 PM GMT+3:30, Antony Stone <Antony.Stone at squid.open.source.it> wrote:  
 
 On Friday 12 October 2018 at 16:41:44, morteza omidian wrote:

> HiI asked my question before.It seems that anybody can not answer it!!

Did you miss the response from Amos?

http://lists.squid-cache.org/pipermail/squid-users/2018-October/019389.html

> As you know, we can mark packets when they go out of squid with
> "tcp_outgoing_mark", this mark is based on Source IP or Source MAC of
> client, but i want to mark them based on mark of input packet, something
> like this: In iptables > mangle > PREROUTING:
> iptables -t mangle -A PREROUTING -p tcp --dport 80 -j MARK --set-mark 1
> In Squid Configuration:acl MARKED_PACKETS nfmark 1tcp_outgoing_mark 1
> MARKED_PACKETS
> 
> 
> Is that possible? How can i solve my problem??
> Tanx

PS: Please do not set "Reply-to" on list emails.

Thanks,


Antony.

-- 
Schr?dinger's rule of data integrity: the condition of any backup is unknown 
until a restore is attempted.

? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? Please reply to the list;
? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? please *don't* CC me.
_______________________________________________
squid-users mailing list
squid-users at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-users
  
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20181012/19470e4b/attachment.htm>

From squid3 at treenet.co.nz  Sat Oct 13 02:17:26 2018
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sat, 13 Oct 2018 15:17:26 +1300
Subject: [squid-users] squid and iptables
In-Reply-To: <1240561542.221562.1539360786756@mail.yahoo.com>
References: <1965406216.157260.1539355304217.ref@mail.yahoo.com>
 <1965406216.157260.1539355304217@mail.yahoo.com>
 <201810121707.22932.Antony.Stone@squid.open.source.it>
 <1240561542.221562.1539360786756@mail.yahoo.com>
Message-ID: <e170a131-411c-3cd3-0067-546df0f3ac6e@treenet.co.nz>

On 13/10/18 5:13 AM, morteza omidian wrote:
> 
> Tank you, I see it now.
> It does not help me, I want to have an acl to select traffic (HTTP
> traffic that comes from client to squid) that have a specific packet
> mark and then send them out with another mark. like this:
> In iptables-mangle-PREROUTING:?
> 
> iptables -t mangle -A PREROUTING -p tcp --dport 80 -j MARK --set-mark 1
> 
> In Squid Configuration:
> acl MARKED_PACKETS nfmark 1
> tcp_outgoing_mark 1 MARKED_PACKETS
> 
> Is that possible or not?


What you ask for is not possible.

What you are trying to do *is* possible ...


> I want this kind of marks because I need to determine source interface
> of packets after they go out of squid!

Two things:

 1) the rules you have above *do not* do what you say you are wanting.
The iptables rule marks *everything* on every interface with 0x1.
Overwriting whatever Squid would set.


 2) MARK is the wrong iptables feature to be using. It only marks a
*single* packet per rule/table evaluation and is not accessible to any
software higher up the network stack than iptables itself.


What you should be using is -j CONNMARK. Once a CONNMARK is set on a
connection it is copied by iptables to each following packet on that
same connection. It is also available to layer-4 software like Squid
which have *nothing* to do with individual packets.

The clientside_mark ACL in Squid matches these values and does exactly
what you are wanting.



Think of thing this way:

 MARK - stays within nftables/iptables.

 CONNMARK - stays within the machine. Can go to other software within
the same machine.

 TOS - goes to other machines, and possibly networks.


Amos


From squid3 at treenet.co.nz  Sat Oct 13 02:29:10 2018
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sat, 13 Oct 2018 15:29:10 +1300
Subject: [squid-users] Support for DistributionPoints in the dynamic
 creates certificate via sslbump
In-Reply-To: <20181012140823.jgsbaqx33i3sg6q7@bloms.de>
References: <20181012140823.jgsbaqx33i3sg6q7@bloms.de>
Message-ID: <422ed507-0b42-21ed-2e9e-81fa8f82332f@treenet.co.nz>

On 13/10/18 3:08 AM, Dieter Bloms wrote:
> Hello,
> 
> we use the sslbump feature of squid, and it works very well.
> One of our http clients expect a CRL distribution point in the dynamic
> generated certificate.
> I've setup a http server, which delivers this crl list, but don't know
> how to configure squid to set this distribution point in every
> dynamic gererated certificate.
> 
> Does anybody know whether squid support this feature ?


AFAIK you should set it in the CA certificate you are using to sign
those dynamic ones.

The dynamic certs are exactly that - dynamic, created as needed and
erased when done with. When the proxy CA is changed all the dynamic
certs also change completely. So there should never exist a case where
Squid is emitting a dynamic cert with stale/different CA - that is
definitely a bug.

That just leaves the problem of clients configured to trust the stale CA
after Squid stops using it. So a CRL is only necessary to expire that CA
cert.


If that does not work then AFAIK the helper generating certs would need
extending to add the CRL reference. BUT ... carefully so as not to clash
with upstream server CRL details. Squid may need an extension to also
present the CRL itself (like it does icons etc.)


HTH
Amos


From morteza1131 at yahoo.com  Sat Oct 13 05:08:26 2018
From: morteza1131 at yahoo.com (morteza omidian)
Date: Sat, 13 Oct 2018 05:08:26 +0000 (UTC)
Subject: [squid-users] squid and iptables
In-Reply-To: <e170a131-411c-3cd3-0067-546df0f3ac6e@treenet.co.nz>
References: <1965406216.157260.1539355304217.ref@mail.yahoo.com>
 <1965406216.157260.1539355304217@mail.yahoo.com>
 <201810121707.22932.Antony.Stone@squid.open.source.it>
 <1240561542.221562.1539360786756@mail.yahoo.com>
 <e170a131-411c-3cd3-0067-546df0f3ac6e@treenet.co.nz>
Message-ID: <1191111337.505368.1539407306022@mail.yahoo.com>

 tanx again.Ok, if I want to know connmark of packets and connection in squid and then select them with an ACL inside of squid? and then again mark them with "tcp_outgoing_mark", is that possible?? 
In this page i don't see what you said!The ACL that be configured only match with clients source ip addresses or domain and ..., not connmark!
    On Saturday, October 13, 2018, 5:47:49 AM GMT+3:30, Amos Jeffries <squid3 at treenet.co.nz> wrote:  
 
 On 13/10/18 5:13 AM, morteza omidian wrote:
> 
> Tank you, I see it now.
> It does not help me, I want to have an acl to select traffic (HTTP
> traffic that comes from client to squid) that have a specific packet
> mark and then send them out with another mark. like this:
> In iptables-mangle-PREROUTING:?
> 
> iptables -t mangle -A PREROUTING -p tcp --dport 80 -j MARK --set-mark 1
> 
> In Squid Configuration:
> acl MARKED_PACKETS nfmark 1
> tcp_outgoing_mark 1 MARKED_PACKETS
> 
> Is that possible or not?


What you ask for is not possible.

What you are trying to do *is* possible ...


> I want this kind of marks because I need to determine source interface
> of packets after they go out of squid!

Two things:

 1) the rules you have above *do not* do what you say you are wanting.
The iptables rule marks *everything* on every interface with 0x1.
Overwriting whatever Squid would set.


 2) MARK is the wrong iptables feature to be using. It only marks a
*single* packet per rule/table evaluation and is not accessible to any
software higher up the network stack than iptables itself.


What you should be using is -j CONNMARK. Once a CONNMARK is set on a
connection it is copied by iptables to each following packet on that
same connection. It is also available to layer-4 software like Squid
which have *nothing* to do with individual packets.

The clientside_mark ACL in Squid matches these values and does exactly
what you are wanting.



Think of thing this way:

 MARK - stays within nftables/iptables.

 CONNMARK - stays within the machine. Can go to other software within
the same machine.

 TOS - goes to other machines, and possibly networks.


Amos
_______________________________________________
squid-users mailing list
squid-users at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-users
  
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20181013/13f1327f/attachment.htm>

From squid3 at treenet.co.nz  Sat Oct 13 06:33:14 2018
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sat, 13 Oct 2018 19:33:14 +1300
Subject: [squid-users] squid and iptables
In-Reply-To: <1191111337.505368.1539407306022@mail.yahoo.com>
References: <1965406216.157260.1539355304217.ref@mail.yahoo.com>
 <1965406216.157260.1539355304217@mail.yahoo.com>
 <201810121707.22932.Antony.Stone@squid.open.source.it>
 <1240561542.221562.1539360786756@mail.yahoo.com>
 <e170a131-411c-3cd3-0067-546df0f3ac6e@treenet.co.nz>
 <1191111337.505368.1539407306022@mail.yahoo.com>
Message-ID: <b1d72061-f918-278a-3a91-347732ac90e0@treenet.co.nz>

On 13/10/18 6:08 PM, morteza omidian wrote:
> tanx again.
> Ok, if I want to know connmark of packets and connection in squid and
> then select them with an ACL inside of squid? and then again mark them
> with "tcp_outgoing_mark", is that possible??
> 

Yes.

> In this page <http://www.squid-cache.org/Doc/config/clientside_mark/> i
> don't see what you said!
> The ACL that be configured only match with clients source ip addresses
> or domain and ..., not connmark!

That is a directive for marking inbound / client connections from the
Squid end - after they have been received.

To fetch an existing mark placed by iptables is an "acl" line. ACL types
are all documented in here:
 <http://www.squid-cache.org/Doc/config/acl/>

Amos


From squid3 at treenet.co.nz  Sat Oct 13 07:58:34 2018
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sat, 13 Oct 2018 20:58:34 +1300
Subject: [squid-users] Squid Cache Server
In-Reply-To: <DB6P193MB00083D2D58574BB7C243AC88C6E10@DB6P193MB0008.EURP193.PROD.OUTLOOK.COM>
References: <DB6P193MB0008E0848E133CEB22448158C6040@DB6P193MB0008.EURP193.PROD.OUTLOOK.COM>
 <201809111054.06037.Antony.Stone@squid.open.source.it>
 <DB6P193MB00085E55BC2E60C264EE09A4C61B0@DB6P193MB0008.EURP193.PROD.OUTLOOK.COM>
 <b6e884e9-0aa3-a48a-b529-afcfe4c60f51@treenet.co.nz>
 <HE1P193MB0009EFBF04C8D339B5D23090C61A0@HE1P193MB0009.EURP193.PROD.OUTLOOK.COM>
 <DB6P193MB000820455863488DA32DDA21C6180@DB6P193MB0008.EURP193.PROD.OUTLOOK.COM>
 <f49f6de9-e3be-360e-cd44-b030f4be4a70@treenet.co.nz>
 <DB6P193MB00085F75B1438190AFE7B508C61C0@DB6P193MB0008.EURP193.PROD.OUTLOOK.COM>
 <DB6P193MB00086BE177598A71A08A69EBC6EB0@DB6P193MB0008.EURP193.PROD.OUTLOOK.COM>
 <DB6P193MB000839D7CA40427D251760F2C6EB0@DB6P193MB0008.EURP193.PROD.OUTLOOK.COM>
 <569c89ea-ea91-70c1-22db-4e6407716f23@treenet.co.nz>
 <DB6P193MB00087DBBD8CE1D0727E2F886C6E40@DB6P193MB0008.EURP193.PROD.OUTLOOK.COM>
 <DB6P193MB00083D2D58574BB7C243AC88C6E10@DB6P193MB0008.EURP193.PROD.OUTLOOK.COM>
Message-ID: <97515b93-e1b8-5ce9-775a-f79b9882525d@treenet.co.nz>

On 12/10/18 7:55 AM, Mujtaba   Hassan Madani wrote:
> Hi Amos,
> 
> ? ? I have change my domain name to proxy instead of that long one per
> your advice i was wondering where to get information about my current
> caching files and it's size ? i login to
> http://proxy:3128/squid-internal-mgr/info?for that but with no success
> attached is web respond. please advise?
> 

The proxy hostname "proxy:3128" does need to resolve in DNS to access it
this way. That is what the browser is complaining about.

Alternatively you maybe can use the Linux/BSD command line tool on the
proxy machine itself:
   squidclient mgr:info

(but given this seems to be a NAS situation it may not be installed there).


Amos


From morteza1131 at yahoo.com  Sat Oct 13 08:06:27 2018
From: morteza1131 at yahoo.com (morteza omidian)
Date: Sat, 13 Oct 2018 08:06:27 +0000 (UTC)
Subject: [squid-users] squid and iptables
In-Reply-To: <b1d72061-f918-278a-3a91-347732ac90e0@treenet.co.nz>
References: <1965406216.157260.1539355304217.ref@mail.yahoo.com>
 <1965406216.157260.1539355304217@mail.yahoo.com>
 <201810121707.22932.Antony.Stone@squid.open.source.it>
 <1240561542.221562.1539360786756@mail.yahoo.com>
 <e170a131-411c-3cd3-0067-546df0f3ac6e@treenet.co.nz>
 <1191111337.505368.1539407306022@mail.yahoo.com>
 <b1d72061-f918-278a-3a91-347732ac90e0@treenet.co.nz>
Message-ID: <1389896784.523948.1539417987514@mail.yahoo.com>

 Cat I Keep Connection marks when I use Tproxy??!
Is "acl aclname clientside_mark mark[/mask]" directive only works on squid4 and up??Can i use it in squid3?Is only way to installing squid4 download the source code??
Tank You


    On Saturday, October 13, 2018, 10:03:33 AM GMT+3:30, Amos Jeffries <squid3 at treenet.co.nz> wrote:  
 
 On 13/10/18 6:08 PM, morteza omidian wrote:
> tanx again.
> Ok, if I want to know connmark of packets and connection in squid and
> then select them with an ACL inside of squid? and then again mark them
> with "tcp_outgoing_mark", is that possible??
> 

Yes.

> In this page <http://www.squid-cache.org/Doc/config/clientside_mark/> i
> don't see what you said!
> The ACL that be configured only match with clients source ip addresses
> or domain and ..., not connmark!

That is a directive for marking inbound / client connections from the
Squid end - after they have been received.

To fetch an existing mark placed by iptables is an "acl" line. ACL types
are all documented in here:
 <http://www.squid-cache.org/Doc/config/acl/>

Amos
_______________________________________________
squid-users mailing list
squid-users at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-users
  
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20181013/1da7d095/attachment.htm>

From morteza1131 at yahoo.com  Sun Oct 14 15:18:00 2018
From: morteza1131 at yahoo.com (morteza omidian)
Date: Sun, 14 Oct 2018 15:18:00 +0000 (UTC)
Subject: [squid-users] squid clientside_mark problem
References: <1397618449.867088.1539530280162.ref@mail.yahoo.com>
Message-ID: <1397618449.867088.1539530280162@mail.yahoo.com>

HiI installed squid4.3 in my linux OS,when I configure squid with clientside_mark i get an error tells me this is not possible!!I use this page-line75 as guidance for my configuration!!?My configuration is:acl MY_ACL clientside_mark 0x1
tcp_outgoing_mark 0x20 MY_ACL
What is wrong??
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20181014/b7ce26c7/attachment.htm>

From uhlar at fantomas.sk  Sun Oct 14 16:39:53 2018
From: uhlar at fantomas.sk (Matus UHLAR - fantomas)
Date: Sun, 14 Oct 2018 18:39:53 +0200
Subject: [squid-users] squid clientside_mark problem
In-Reply-To: <1397618449.867088.1539530280162@mail.yahoo.com>
References: <1397618449.867088.1539530280162.ref@mail.yahoo.com>
 <1397618449.867088.1539530280162@mail.yahoo.com>
Message-ID: <20181014163953.GA19569@fantomas.sk>

please don't post throgh yahoo's http gateway.
Theit plaintext formatting sucks.
try a mail client or other mail service.

On 14.10.18 15:18, morteza omidian wrote:
>HiI installed squid4.3 in my linux OS,when I configure squid with
>clientside_mark i get an error tells me this is not possible!!I use this
>page-line75 as guidance for my configuration!!?My configuration is:acl
>MY_ACL clientside_mark 0x1 tcp_outgoing_mark 0x20 MY_ACL What is wrong??

apparently not configured with proper options.
which linux version is that?
-- 
Matus UHLAR - fantomas, uhlar at fantomas.sk ; http://www.fantomas.sk/
Warning: I wish NOT to receive e-mail advertising to this address.
Varovanie: na tuto adresu chcem NEDOSTAVAT akukolvek reklamnu postu.
Your mouse has moved. Windows NT will now restart for changes to take
to take effect. [OK]


From augustus_meyer at gmx.net  Sun Oct 14 17:10:48 2018
From: augustus_meyer at gmx.net (reinerotto)
Date: Sun, 14 Oct 2018 12:10:48 -0500 (CDT)
Subject: [squid-users] squid on openwrt: RAM usage and header forgery
In-Reply-To: <cb5c15e2-f788-2967-e7b5-c7543027ac7f@treenet.co.nz>
References: <1539155917842-0.post@n4.nabble.com>
 <cb5c15e2-f788-2967-e7b5-c7543027ac7f@treenet.co.nz>
Message-ID: <1539537048225-0.post@n4.nabble.com>

Thanx a lot for clarification. 
After upgrading to 4.3 and streamlining squid.conf
according to your suggestions, mem requirements seem to be a bit reduced.
 



--
Sent from: http://squid-web-proxy-cache.1019090.n4.nabble.com/Squid-Users-f1019091.html


From augustus_meyer at gmx.net  Sun Oct 14 17:17:41 2018
From: augustus_meyer at gmx.net (reinerotto)
Date: Sun, 14 Oct 2018 12:17:41 -0500 (CDT)
Subject: [squid-users] squid on openwrt: aclhelper wrongly considered exited
Message-ID: <1539537461767-0.post@n4.nabble.com>

Running squid 4.3 on openwrt, I notice following warnings from time to time:

2018/10/14 16:36:39 kid1| WARNING: helper.sh #Hlpr2 exited
2018/10/14 16:36:39 kid1| Too few helper processes are running (need 1/5)
2018/10/14 16:36:39 kid1| Starting new helpers
2018/10/14 16:36:39 kid1| helperOpenServers: Starting 1/5 'helper.sh'
processes

which causes a new helper process to be created.

Howver, the process considered to have exited is still alive, according to
"ps".
I can kill it manually, not causing a new warning again.

Any idea, which debug level to activate, to get some more info ?
Because of limited resources, not too generous debugs, pls.




--
Sent from: http://squid-web-proxy-cache.1019090.n4.nabble.com/Squid-Users-f1019091.html


From rousskov at measurement-factory.com  Sun Oct 14 18:27:19 2018
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Sun, 14 Oct 2018 12:27:19 -0600
Subject: [squid-users] squid on openwrt: aclhelper wrongly considered
 exited
In-Reply-To: <1539537461767-0.post@n4.nabble.com>
References: <1539537461767-0.post@n4.nabble.com>
Message-ID: <c8ef3617-cd8c-2767-b14c-ab82c576db4e@measurement-factory.com>

On 10/14/2018 11:17 AM, reinerotto wrote:
> Running squid 4.3 on openwrt, I notice following warnings from time to time:
> 
> 2018/10/14 16:36:39 kid1| WARNING: helper.sh #Hlpr2 exited
> 2018/10/14 16:36:39 kid1| Too few helper processes are running (need 1/5)
> 2018/10/14 16:36:39 kid1| Starting new helpers
> 2018/10/14 16:36:39 kid1| helperOpenServers: Starting 1/5 'helper.sh'
> processes
> 
> which causes a new helper process to be created.
> 
> Howver, the process considered to have exited is still alive, according to
> "ps". I can kill it manually, not causing a new warning again.

IIRC, squid does not monitor helper processes as such. Squid monitors
helper standard output stream (that output stream closure is interpreted
by Squid as helper exit). Also, Squid closes helper standard input
stream as a signal to the helper that the helper should exit.


> Any idea, which debug level to activate, to get some more info ?
> Because of limited resources, not too generous debugs, pls.

You can start with

    debug_options ALL,2 54,5 84,5

If this is your own helper, then check that it does not close its
standard output except when exiting.


HTH,

Alex.


From ronthecon at gmail.com  Mon Oct 15 05:04:39 2018
From: ronthecon at gmail.com (RB)
Date: Mon, 15 Oct 2018 01:04:39 -0400
Subject: [squid-users] How to create a simple whitelist using regexes?
In-Reply-To: <CAM6dNWqt4HREv7tt32=-hfoxiLOapbytjtSdBuH_4+Uq3ZXthw@mail.gmail.com>
References: <CAM6dNWqt4HREv7tt32=-hfoxiLOapbytjtSdBuH_4+Uq3ZXthw@mail.gmail.com>
Message-ID: <CAM6dNWrB-=8p=ULEC3X=cyxtffZd6UOhE9dWhbmt4M5RuQT4fg@mail.gmail.com>

Hi everyone,

I'm trying to deny all urls except for only whitelisted regular
expressions. I have only this regular expression in my file
"squid_sites.txt"

^https://wiki.squid-cache.org/SquidFaq/SquidAcl.*


My "squid.conf"


debug_options 28,7

###
### Global settings define
###

http_port 3128

###
### Authorization rules define
###

###
### Networks define
###

acl localnet src 10.5.0.0/1
acl localnet src 172.16.0.0/16
acl localnet src fc00::/7
acl localnet src fe80::/10

###
### Ports define
###

acl SSL_ports port 443          # https
acl SSL_ports port 22           # SSH
acl Safe_ports port 80          # http
acl Safe_ports port 443         # https
acl Safe_ports port 22          # SSH

acl purge method PURGE

acl CONNECT method CONNECT

acl bastion src 10.5.0.0/1
acl whitelist url_regex "/vagrant/squid_sites.txt"

###
### Rules define
###

http_access allow manager localhost
http_access deny manager
http_access deny !Safe_ports
http_access allow localhost
http_access allow purge localhost
http_access deny purge
http_access deny CONNECT !SSL_ports

http_access allow bastion whitelist
http_access deny bastion all

# http_access deny all

###
### Secondary global settings define
###


# icp_access allow localnet
# icp_access deny all
#
# htcp_access allow localnet
# htcp_access deny all

# Add any of your own refresh_pattern entries above these.
access_log /var/log/squid3/access.log squid
cache_log /var/log/squid3/cache.log squid
cache_store_log /var/log/squid3/store.log squid

refresh_pattern      ^ftp:      1440  20%  10080
refresh_pattern     ^gopher:      1440  0%  1440
refresh_pattern      -i (/cgi-bin/|\?)    0  0%  0
refresh_pattern     (Release|Package(.gz)*)$  0  20%  2880

coredump_dir /var/spool/squid3
maximum_object_size 1024 MB
cache_mem 2048 MB


I tried enabling debugging and tailing /var/log/squid3/cache.log but my
curl statement keeps matching "all".

$ curl -sSL --proxy localhost:3128 -D - "
https://wiki.squid-cache.org/SquidFaq/SquidAcl" -o /dev/null 2>&1 | grep
Squid
X-Squid-Error: ERR_ACCESS_DENIED 0


Any ideas what I'm doing wrong?

Thank you.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20181015/f3216316/attachment.htm>

From uhlar at fantomas.sk  Mon Oct 15 08:49:24 2018
From: uhlar at fantomas.sk (Matus UHLAR - fantomas)
Date: Mon, 15 Oct 2018 10:49:24 +0200
Subject: [squid-users] How to create a simple whitelist using regexes?
In-Reply-To: <CAM6dNWrB-=8p=ULEC3X=cyxtffZd6UOhE9dWhbmt4M5RuQT4fg@mail.gmail.com>
References: <CAM6dNWqt4HREv7tt32=-hfoxiLOapbytjtSdBuH_4+Uq3ZXthw@mail.gmail.com>
 <CAM6dNWrB-=8p=ULEC3X=cyxtffZd6UOhE9dWhbmt4M5RuQT4fg@mail.gmail.com>
Message-ID: <20181015084924.GA709@fantomas.sk>

KOn 15.10.18 01:04, RB wrote:
>I'm trying to deny all urls except for only whitelisted regular
>expressions. I have only this regular expression in my file
>"squid_sites.txt"
>
>^https://wiki.squid-cache.org/SquidFaq/SquidAcl.*

are you aware that you can only see CONNECT in https requests, unless using
ssl_bump?


>acl bastion src 10.5.0.0/1
>acl whitelist url_regex "/vagrant/squid_sites.txt"
[...]
>http_access allow manager localhost
>http_access deny manager
>http_access deny !Safe_ports
>http_access allow localhost
>http_access allow purge localhost
>http_access deny purge
>http_access deny CONNECT !SSL_ports
>
>http_access allow bastion whitelist
>http_access deny bastion all

>I tried enabling debugging and tailing /var/log/squid3/cache.log but my
>curl statement keeps matching "all".

of course it matches all, everything should match "all".

I more wonder why doesn't it match "http_access allow localhost"

>$ curl -sSL --proxy localhost:3128 -D - "
>https://wiki.squid-cache.org/SquidFaq/SquidAcl" -o /dev/null 2>&1 | grep
>Squid
>X-Squid-Error: ERR_ACCESS_DENIED 0

>Any ideas what I'm doing wrong?

have you reloaded squid config after changing it?
Did squid confirm it?

-- 
Matus UHLAR - fantomas, uhlar at fantomas.sk ; http://www.fantomas.sk/
Warning: I wish NOT to receive e-mail advertising to this address.
Varovanie: na tuto adresu chcem NEDOSTAVAT akukolvek reklamnu postu.
It's now safe to throw off your computer.


From ronthecon at gmail.com  Mon Oct 15 15:11:31 2018
From: ronthecon at gmail.com (RB)
Date: Mon, 15 Oct 2018 11:11:31 -0400
Subject: [squid-users] How to create a simple whitelist using regexes?
In-Reply-To: <20181015084924.GA709@fantomas.sk>
References: <CAM6dNWqt4HREv7tt32=-hfoxiLOapbytjtSdBuH_4+Uq3ZXthw@mail.gmail.com>
 <CAM6dNWrB-=8p=ULEC3X=cyxtffZd6UOhE9dWhbmt4M5RuQT4fg@mail.gmail.com>
 <20181015084924.GA709@fantomas.sk>
Message-ID: <CAM6dNWrfq4NRBuEWOr02BRX5aTU4ZtZMJC1jfSAsOZ-XgFeATw@mail.gmail.com>

Hi Matus,

Thanks for responding so quickly. I uploaded my configurations here if that
is more helpful: https://bit.ly/2NF4zNb

The config that I previously shared is called squid_corp.conf. I also
noticed that if I don't use regular expressions and instead use domains, it
works correctly:

# acl whitelist url_regex "/vagrant/squid_sites.txt"
acl whitelist url_regex .squid-cache.org


Every time my squid.conf or my squid_sites.txt is modified, I restart the
squid service

sudo service squid3 restart


Then I use curl to test and now the url works.

$ curl -sSL --proxy localhost:3128 -D -
https://wiki.squid-cache.org/SquidFaq/SquidAcl -o /dev/null 2>&1
HTTP/1.1 200 Connection established

HTTP/1.1 200 OK
Date: Mon, 15 Oct 2018 14:47:33 GMT
Server: Apache/2.4.7 (Ubuntu)
Vary: Cookie,User-Agent,Accept-Encoding
Content-Length: 101912
Cache-Control: max-age=3600
Expires: Mon, 15 Oct 2018 15:47:33 GMT
Content-Type: text/html; charset=utf-8


But this does not allow me to get more granular. I can only allow all
subdomains and paths for the domain squid-cache.org but I'm unable to only
allow the regular expressions if I put them inline or put them in
squid_sites.txt.

# acl whitelist url_regex "/vagrant/squid_sites.txt"
acl whitelist url_regex ^https://wiki.squid-cache.org/SquidFaq/SquidAcl.*
acl whitelist url_regex .*squid-cache.org/SquidFaq/SquidAcl.*


If I put them inline like I have above, when I restarted squid it says the
following

2018/10/15 14:54:48 kid1| strtokFile: .*squid-cache.org/SquidFaq/SquidAcl.*
not found


If I put the expressions in the squid_sites.txt the above "not found"
message isn't shown and this is the debug output in
/var/log/squid3/cache.log (full output https://pastebin.com/NVwRxVmQ).

2018/10/15 15:05:45.083 kid1| Checklist.cc(275) matchNode: 0x7fb0068da2b8
matched=1 async=0 finished=0
2018/10/15 15:05:45.083 kid1| Acl.cc(336) matches: ACLList::matches:
checking whitelist
2018/10/15 15:05:45.083 kid1| Acl.cc(319) checklistMatches:
ACL::checklistMatches: checking 'whitelist'
2018/10/15 15:05:45.083 kid1| RegexData.cc(71) match: aclRegexData::match:
checking 'wiki.squid-cache.org:443'
2018/10/15 15:05:45.084 kid1| RegexData.cc(82) match: aclRegexData::match:
looking for '(^
https://wiki.squid-cache.org/SquidFaq/SquidAcl.*)|(squid-cache.org/SquidFaq/SquidAcl.*
)'
2018/10/15 15:05:45.084 kid1| Acl.cc(321) checklistMatches:
ACL::ChecklistMatches: result for 'whitelist' is 0
2018/10/15 15:05:45.084 kid1| Acl.cc(349) matches: whitelist mismatched.
2018/10/15 15:05:45.084 kid1| Acl.cc(354) matches: whitelist result is false


So it's failing the regular expression check. If I use grep to verify if
the regex works, it does.

$ echo https://wiki.squid-cache.org/SquidFaq/SquidAcl | grep "^
https://wiki.squid-cache.org/SquidFaq/SquidAcl.*"
https://wiki.squid-cache.org/SquidFaq/SquidAcl


> are you aware that you can only see CONNECT in https requests, unless
using
ssl_bump?

Ah interesting. Are you saying that my https connections will always fail
unless I use ssl_bump to decrypt https to http connections? How would this
work correctly in production? Does squid proxy only block urls if it
detects http? How do you configure ssl_bump to work in this case? and is
that viable in production?

> of course it matches all, everything should match "all".
> I more wonder why doesn't it match "http_access allow localhost"

> have you reloaded squid config after changing it?
> Did squid confirm it?

Would you have an example of one entire config file that would work to
whitelist an http/https url using a regular expression?

Best,


On Mon, Oct 15, 2018 at 4:49 AM Matus UHLAR - fantomas <uhlar at fantomas.sk>
wrote:

> KOn 15.10.18 01:04, RB wrote:
> >I'm trying to deny all urls except for only whitelisted regular
> >expressions. I have only this regular expression in my file
> >"squid_sites.txt"
> >
> >^https://wiki.squid-cache.org/SquidFaq/SquidAcl.*
>
> are you aware that you can only see CONNECT in https requests, unless using
> ssl_bump?
>
>
> >acl bastion src 10.5.0.0/1
> >acl whitelist url_regex "/vagrant/squid_sites.txt"
> [...]
> >http_access allow manager localhost
> >http_access deny manager
> >http_access deny !Safe_ports
> >http_access allow localhost
> >http_access allow purge localhost
> >http_access deny purge
> >http_access deny CONNECT !SSL_ports
> >
> >http_access allow bastion whitelist
> >http_access deny bastion all
>
> >I tried enabling debugging and tailing /var/log/squid3/cache.log but my
> >curl statement keeps matching "all".
>
> of course it matches all, everything should match "all".
>
> I more wonder why doesn't it match "http_access allow localhost"
>
> >$ curl -sSL --proxy localhost:3128 -D - "
> >https://wiki.squid-cache.org/SquidFaq/SquidAcl" -o /dev/null 2>&1 | grep
> >Squid
> >X-Squid-Error: ERR_ACCESS_DENIED 0
>
> >Any ideas what I'm doing wrong?
>
> have you reloaded squid config after changing it?
> Did squid confirm it?
>
> --
> Matus UHLAR - fantomas, uhlar at fantomas.sk ; http://www.fantomas.sk/
> Warning: I wish NOT to receive e-mail advertising to this address.
> Varovanie: na tuto adresu chcem NEDOSTAVAT akukolvek reklamnu postu.
> It's now safe to throw off your computer.
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20181015/2959fa88/attachment.htm>

From ronthecon at gmail.com  Mon Oct 15 15:56:50 2018
From: ronthecon at gmail.com (RB)
Date: Mon, 15 Oct 2018 11:56:50 -0400
Subject: [squid-users] How to create a simple whitelist using regexes?
In-Reply-To: <CAM6dNWrfq4NRBuEWOr02BRX5aTU4ZtZMJC1jfSAsOZ-XgFeATw@mail.gmail.com>
References: <CAM6dNWqt4HREv7tt32=-hfoxiLOapbytjtSdBuH_4+Uq3ZXthw@mail.gmail.com>
 <CAM6dNWrB-=8p=ULEC3X=cyxtffZd6UOhE9dWhbmt4M5RuQT4fg@mail.gmail.com>
 <20181015084924.GA709@fantomas.sk>
 <CAM6dNWrfq4NRBuEWOr02BRX5aTU4ZtZMJC1jfSAsOZ-XgFeATw@mail.gmail.com>
Message-ID: <CAM6dNWrEsYYKuZjdCe0LQR=vAwyqx_sVHOD2u94=V61zzX4tGA@mail.gmail.com>

I think I know what the issue is which can give us a clue to what is going
on.

2018/10/15 15:05:45.083 kid1| RegexData.cc(71) match: aclRegexData::match:
checking 'wiki.squid-cache.org:443'
2018/10/15 15:05:45.084 kid1| RegexData.cc(82) match: aclRegexData::match:
looking for '(^
https://wiki.squid-cache.org/SquidFaq/SquidAcl.*)|(squid-cache.org/SquidFaq/SquidAcl.*
)'
2018/10/15 15:05:45.084 kid1| Acl.cc(321) checklistMatches:
ACL::ChecklistMatches: result for 'whitelist' is 0

The above seems to be applying the regex to "wiki.squid-cache.org:443"
instead of to "https://wiki.squid-cache.org/SquidFaq/SquidAcl". I added the
regex ".*squid-cache.org.*" to my list of regular expressions and now I see
this.

2018/10/15 15:16:03.641 kid1| RegexData.cc(71) match: aclRegexData::match:
checking 'wiki.squid-cache.org:443'
2018/10/15 15:16:03.641 kid1| RegexData.cc(82) match: aclRegexData::match:
looking for '(^https?://[^/]+/
wiki.squid-cache.org/SquidFaq/SquidAcl.*)|(squid-cache.org.*)'
2018/10/15 15:16:03.641 kid1| RegexData.cc(93) match: aclRegexData::match:
match '(^https?://[^/]+/
wiki.squid-cache.org/SquidFaq/SquidAcl.*)|(squid-cache.org.*)' found in '
wiki.squid-cache.org:443'
2018/10/15 15:16:03.641 kid1| Acl.cc(321) checklistMatches:
ACL::ChecklistMatches: result for 'whitelist' is 1


Any idea why url_regex wouldn't try to match the full url and instead only
matches on the subdomain, host domain, and port?

The Squid FAQ <https://wiki.squid-cache.org/SquidFaq/SquidAcl> says the
following:

*url_regex*: URL regular expression pattern matching
*urlpath_regex*: URL-path regular expression pattern matching, leaves out
the protocol and hostname


with this example given

acl special_url url_regex ^http://www.squid-cache.org/Doc/FAQ/$


This seems to be the case between 3.3.8 (default on ubuntu 14.04) and
3.5.12 (default on ubuntu 16.04).

Is there another configuration that forces url_regex to match the entire
url? or should I use a different acl type?

Best,

On Mon, Oct 15, 2018 at 11:11 AM RB <ronthecon at gmail.com> wrote:

> Hi Matus,
>
> Thanks for responding so quickly. I uploaded my configurations here if
> that is more helpful: https://bit.ly/2NF4zNb
>
> The config that I previously shared is called squid_corp.conf. I also
> noticed that if I don't use regular expressions and instead use domains, it
> works correctly:
>
> # acl whitelist url_regex "/vagrant/squid_sites.txt"
> acl whitelist url_regex .squid-cache.org
>
>
> Every time my squid.conf or my squid_sites.txt is modified, I restart the
> squid service
>
> sudo service squid3 restart
>
>
> Then I use curl to test and now the url works.
>
> $ curl -sSL --proxy localhost:3128 -D -
> https://wiki.squid-cache.org/SquidFaq/SquidAcl -o /dev/null 2>&1
> HTTP/1.1 200 Connection established
>
> HTTP/1.1 200 OK
> Date: Mon, 15 Oct 2018 14:47:33 GMT
> Server: Apache/2.4.7 (Ubuntu)
> Vary: Cookie,User-Agent,Accept-Encoding
> Content-Length: 101912
> Cache-Control: max-age=3600
> Expires: Mon, 15 Oct 2018 15:47:33 GMT
> Content-Type: text/html; charset=utf-8
>
>
> But this does not allow me to get more granular. I can only allow all
> subdomains and paths for the domain squid-cache.org but I'm unable to
> only allow the regular expressions if I put them inline or put them in
> squid_sites.txt.
>
> # acl whitelist url_regex "/vagrant/squid_sites.txt"
> acl whitelist url_regex ^https://wiki.squid-cache.org/SquidFaq/SquidAcl.*
> acl whitelist url_regex .*squid-cache.org/SquidFaq/SquidAcl.*
>
>
> If I put them inline like I have above, when I restarted squid it says the
> following
>
> 2018/10/15 14:54:48 kid1| strtokFile: .*
> squid-cache.org/SquidFaq/SquidAcl.* not found
>
>
> If I put the expressions in the squid_sites.txt the above "not found"
> message isn't shown and this is the debug output in
> /var/log/squid3/cache.log (full output https://pastebin.com/NVwRxVmQ).
>
> 2018/10/15 15:05:45.083 kid1| Checklist.cc(275) matchNode: 0x7fb0068da2b8
> matched=1 async=0 finished=0
> 2018/10/15 15:05:45.083 kid1| Acl.cc(336) matches: ACLList::matches:
> checking whitelist
> 2018/10/15 15:05:45.083 kid1| Acl.cc(319) checklistMatches:
> ACL::checklistMatches: checking 'whitelist'
> 2018/10/15 15:05:45.083 kid1| RegexData.cc(71) match: aclRegexData::match:
> checking 'wiki.squid-cache.org:443'
> 2018/10/15 15:05:45.084 kid1| RegexData.cc(82) match: aclRegexData::match:
> looking for '(^
> https://wiki.squid-cache.org/SquidFaq/SquidAcl.*)|(squid-cache.org/SquidFaq/SquidAcl.*
> )'
> 2018/10/15 15:05:45.084 kid1| Acl.cc(321) checklistMatches:
> ACL::ChecklistMatches: result for 'whitelist' is 0
> 2018/10/15 15:05:45.084 kid1| Acl.cc(349) matches: whitelist mismatched.
> 2018/10/15 15:05:45.084 kid1| Acl.cc(354) matches: whitelist result is
> false
>
>
> So it's failing the regular expression check. If I use grep to verify if
> the regex works, it does.
>
> $ echo https://wiki.squid-cache.org/SquidFaq/SquidAcl | grep "^
> https://wiki.squid-cache.org/SquidFaq/SquidAcl.*"
> https://wiki.squid-cache.org/SquidFaq/SquidAcl
>
>
> > are you aware that you can only see CONNECT in https requests, unless
> using
> ssl_bump?
>
> Ah interesting. Are you saying that my https connections will always fail
> unless I use ssl_bump to decrypt https to http connections? How would this
> work correctly in production? Does squid proxy only block urls if it
> detects http? How do you configure ssl_bump to work in this case? and is
> that viable in production?
>
> > of course it matches all, everything should match "all".
> > I more wonder why doesn't it match "http_access allow localhost"
>
> > have you reloaded squid config after changing it?
> > Did squid confirm it?
>
> Would you have an example of one entire config file that would work to
> whitelist an http/https url using a regular expression?
>
> Best,
>
>
> On Mon, Oct 15, 2018 at 4:49 AM Matus UHLAR - fantomas <uhlar at fantomas.sk>
> wrote:
>
>> KOn 15.10.18 01:04, RB wrote:
>> >I'm trying to deny all urls except for only whitelisted regular
>> >expressions. I have only this regular expression in my file
>> >"squid_sites.txt"
>> >
>> >^https://wiki.squid-cache.org/SquidFaq/SquidAcl.*
>>
>> are you aware that you can only see CONNECT in https requests, unless
>> using
>> ssl_bump?
>>
>>
>> >acl bastion src 10.5.0.0/1
>> >acl whitelist url_regex "/vagrant/squid_sites.txt"
>> [...]
>> >http_access allow manager localhost
>> >http_access deny manager
>> >http_access deny !Safe_ports
>> >http_access allow localhost
>> >http_access allow purge localhost
>> >http_access deny purge
>> >http_access deny CONNECT !SSL_ports
>> >
>> >http_access allow bastion whitelist
>> >http_access deny bastion all
>>
>> >I tried enabling debugging and tailing /var/log/squid3/cache.log but my
>> >curl statement keeps matching "all".
>>
>> of course it matches all, everything should match "all".
>>
>> I more wonder why doesn't it match "http_access allow localhost"
>>
>> >$ curl -sSL --proxy localhost:3128 -D - "
>> >https://wiki.squid-cache.org/SquidFaq/SquidAcl" -o /dev/null 2>&1 | grep
>> >Squid
>> >X-Squid-Error: ERR_ACCESS_DENIED 0
>>
>> >Any ideas what I'm doing wrong?
>>
>> have you reloaded squid config after changing it?
>> Did squid confirm it?
>>
>> --
>> Matus UHLAR - fantomas, uhlar at fantomas.sk ; http://www.fantomas.sk/
>> Warning: I wish NOT to receive e-mail advertising to this address.
>> Varovanie: na tuto adresu chcem NEDOSTAVAT akukolvek reklamnu postu.
>> It's now safe to throw off your computer.
>> _______________________________________________
>> squid-users mailing list
>> squid-users at lists.squid-cache.org
>> http://lists.squid-cache.org/listinfo/squid-users
>>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20181015/eaabba43/attachment.htm>

From ronthecon at gmail.com  Mon Oct 15 16:48:53 2018
From: ronthecon at gmail.com (RB)
Date: Mon, 15 Oct 2018 12:48:53 -0400
Subject: [squid-users] How to create a simple whitelist using regexes?
In-Reply-To: <CAM6dNWrEsYYKuZjdCe0LQR=vAwyqx_sVHOD2u94=V61zzX4tGA@mail.gmail.com>
References: <CAM6dNWqt4HREv7tt32=-hfoxiLOapbytjtSdBuH_4+Uq3ZXthw@mail.gmail.com>
 <CAM6dNWrB-=8p=ULEC3X=cyxtffZd6UOhE9dWhbmt4M5RuQT4fg@mail.gmail.com>
 <20181015084924.GA709@fantomas.sk>
 <CAM6dNWrfq4NRBuEWOr02BRX5aTU4ZtZMJC1jfSAsOZ-XgFeATw@mail.gmail.com>
 <CAM6dNWrEsYYKuZjdCe0LQR=vAwyqx_sVHOD2u94=V61zzX4tGA@mail.gmail.com>
Message-ID: <CAM6dNWrrF629gQurFqJ5-t_eVCLjGqyoTfvpf_LN+yoX0A_puw@mail.gmail.com>

Hi again...

After some more research it looks like squid only has access to the url
domain if it's HTTPS and the only way to get the url path and query string
is to use ssl_bump to decrypt https so squid can see url path and query
arguments.

To use ssl_bump, I have to compile the code from source with --enable-ssl,
create a certificate, and add it to the chain of certs to every other vm
that proxies through squid, then squid can decrypt the https urls to see
paths and query args and finally apply the regex to those urls in order to
only allow explicit regex urls.

Is this correct?

On Mon, Oct 15, 2018 at 11:56 AM RB <ronthecon at gmail.com> wrote:

> I think I know what the issue is which can give us a clue to what is going
> on.
>
> 2018/10/15 15:05:45.083 kid1| RegexData.cc(71) match: aclRegexData::match:
> checking 'wiki.squid-cache.org:443'
> 2018/10/15 15:05:45.084 kid1| RegexData.cc(82) match: aclRegexData::match:
> looking for '(^
> https://wiki.squid-cache.org/SquidFaq/SquidAcl.*)|(squid-cache.org/SquidFaq/SquidAcl.*
> )'
> 2018/10/15 15:05:45.084 kid1| Acl.cc(321) checklistMatches:
> ACL::ChecklistMatches: result for 'whitelist' is 0
>
> The above seems to be applying the regex to "wiki.squid-cache.org:443"
> instead of to "https://wiki.squid-cache.org/SquidFaq/SquidAcl". I added
> the regex ".*squid-cache.org.*" to my list of regular expressions and now I
> see this.
>
> 2018/10/15 15:16:03.641 kid1| RegexData.cc(71) match: aclRegexData::match:
> checking 'wiki.squid-cache.org:443'
> 2018/10/15 15:16:03.641 kid1| RegexData.cc(82) match: aclRegexData::match:
> looking for '(^https?://[^/]+/
> wiki.squid-cache.org/SquidFaq/SquidAcl.*)|(squid-cache.org.*
> <http://wiki.squid-cache.org/SquidFaq/SquidAcl.*)%7C(squid-cache.org.*>)'
> 2018/10/15 15:16:03.641 kid1| RegexData.cc(93) match: aclRegexData::match:
> match '(^https?://[^/]+/
> wiki.squid-cache.org/SquidFaq/SquidAcl.*)|(squid-cache.org.*
> <http://wiki.squid-cache.org/SquidFaq/SquidAcl.*)%7C(squid-cache.org.*>)'
> found in 'wiki.squid-cache.org:443'
> 2018/10/15 15:16:03.641 kid1| Acl.cc(321) checklistMatches:
> ACL::ChecklistMatches: result for 'whitelist' is 1
>
>
> Any idea why url_regex wouldn't try to match the full url and instead only
> matches on the subdomain, host domain, and port?
>
> The Squid FAQ <https://wiki.squid-cache.org/SquidFaq/SquidAcl> says the
> following:
>
> *url_regex*: URL regular expression pattern matching
> *urlpath_regex*: URL-path regular expression pattern matching, leaves out
> the protocol and hostname
>
>
> with this example given
>
> acl special_url url_regex ^http://www.squid-cache.org/Doc/FAQ/$
>
>
> This seems to be the case between 3.3.8 (default on ubuntu 14.04) and
> 3.5.12 (default on ubuntu 16.04).
>
> Is there another configuration that forces url_regex to match the entire
> url? or should I use a different acl type?
>
> Best,
>
> On Mon, Oct 15, 2018 at 11:11 AM RB <ronthecon at gmail.com> wrote:
>
>> Hi Matus,
>>
>> Thanks for responding so quickly. I uploaded my configurations here if
>> that is more helpful: https://bit.ly/2NF4zNb
>>
>> The config that I previously shared is called squid_corp.conf. I also
>> noticed that if I don't use regular expressions and instead use domains, it
>> works correctly:
>>
>> # acl whitelist url_regex "/vagrant/squid_sites.txt"
>> acl whitelist url_regex .squid-cache.org
>>
>>
>> Every time my squid.conf or my squid_sites.txt is modified, I restart the
>> squid service
>>
>> sudo service squid3 restart
>>
>>
>> Then I use curl to test and now the url works.
>>
>> $ curl -sSL --proxy localhost:3128 -D -
>> https://wiki.squid-cache.org/SquidFaq/SquidAcl -o /dev/null 2>&1
>> HTTP/1.1 200 Connection established
>>
>> HTTP/1.1 200 OK
>> Date: Mon, 15 Oct 2018 14:47:33 GMT
>> Server: Apache/2.4.7 (Ubuntu)
>> Vary: Cookie,User-Agent,Accept-Encoding
>> Content-Length: 101912
>> Cache-Control: max-age=3600
>> Expires: Mon, 15 Oct 2018 15:47:33 GMT
>> Content-Type: text/html; charset=utf-8
>>
>>
>> But this does not allow me to get more granular. I can only allow all
>> subdomains and paths for the domain squid-cache.org but I'm unable to
>> only allow the regular expressions if I put them inline or put them in
>> squid_sites.txt.
>>
>> # acl whitelist url_regex "/vagrant/squid_sites.txt"
>> acl whitelist url_regex ^https://wiki.squid-cache.org/SquidFaq/SquidAcl.*
>> acl whitelist url_regex .*squid-cache.org/SquidFaq/SquidAcl.*
>>
>>
>> If I put them inline like I have above, when I restarted squid it says
>> the following
>>
>> 2018/10/15 14:54:48 kid1| strtokFile: .*
>> squid-cache.org/SquidFaq/SquidAcl.* not found
>>
>>
>> If I put the expressions in the squid_sites.txt the above "not found"
>> message isn't shown and this is the debug output in
>> /var/log/squid3/cache.log (full output https://pastebin.com/NVwRxVmQ).
>>
>> 2018/10/15 15:05:45.083 kid1| Checklist.cc(275) matchNode: 0x7fb0068da2b8
>> matched=1 async=0 finished=0
>> 2018/10/15 15:05:45.083 kid1| Acl.cc(336) matches: ACLList::matches:
>> checking whitelist
>> 2018/10/15 15:05:45.083 kid1| Acl.cc(319) checklistMatches:
>> ACL::checklistMatches: checking 'whitelist'
>> 2018/10/15 15:05:45.083 kid1| RegexData.cc(71) match:
>> aclRegexData::match: checking 'wiki.squid-cache.org:443'
>> 2018/10/15 15:05:45.084 kid1| RegexData.cc(82) match:
>> aclRegexData::match: looking for '(^
>> https://wiki.squid-cache.org/SquidFaq/SquidAcl.*)|(squid-cache.org/SquidFaq/SquidAcl.*
>> )'
>> 2018/10/15 15:05:45.084 kid1| Acl.cc(321) checklistMatches:
>> ACL::ChecklistMatches: result for 'whitelist' is 0
>> 2018/10/15 15:05:45.084 kid1| Acl.cc(349) matches: whitelist mismatched.
>> 2018/10/15 15:05:45.084 kid1| Acl.cc(354) matches: whitelist result is
>> false
>>
>>
>> So it's failing the regular expression check. If I use grep to verify if
>> the regex works, it does.
>>
>> $ echo https://wiki.squid-cache.org/SquidFaq/SquidAcl | grep "^
>> https://wiki.squid-cache.org/SquidFaq/SquidAcl.*"
>> https://wiki.squid-cache.org/SquidFaq/SquidAcl
>>
>>
>> > are you aware that you can only see CONNECT in https requests, unless
>> using
>> ssl_bump?
>>
>> Ah interesting. Are you saying that my https connections will always fail
>> unless I use ssl_bump to decrypt https to http connections? How would this
>> work correctly in production? Does squid proxy only block urls if it
>> detects http? How do you configure ssl_bump to work in this case? and is
>> that viable in production?
>>
>> > of course it matches all, everything should match "all".
>> > I more wonder why doesn't it match "http_access allow localhost"
>>
>> > have you reloaded squid config after changing it?
>> > Did squid confirm it?
>>
>> Would you have an example of one entire config file that would work to
>> whitelist an http/https url using a regular expression?
>>
>> Best,
>>
>>
>> On Mon, Oct 15, 2018 at 4:49 AM Matus UHLAR - fantomas <uhlar at fantomas.sk>
>> wrote:
>>
>>> KOn 15.10.18 01:04, RB wrote:
>>> >I'm trying to deny all urls except for only whitelisted regular
>>> >expressions. I have only this regular expression in my file
>>> >"squid_sites.txt"
>>> >
>>> >^https://wiki.squid-cache.org/SquidFaq/SquidAcl.*
>>>
>>> are you aware that you can only see CONNECT in https requests, unless
>>> using
>>> ssl_bump?
>>>
>>>
>>> >acl bastion src 10.5.0.0/1
>>> >acl whitelist url_regex "/vagrant/squid_sites.txt"
>>> [...]
>>> >http_access allow manager localhost
>>> >http_access deny manager
>>> >http_access deny !Safe_ports
>>> >http_access allow localhost
>>> >http_access allow purge localhost
>>> >http_access deny purge
>>> >http_access deny CONNECT !SSL_ports
>>> >
>>> >http_access allow bastion whitelist
>>> >http_access deny bastion all
>>>
>>> >I tried enabling debugging and tailing /var/log/squid3/cache.log but my
>>> >curl statement keeps matching "all".
>>>
>>> of course it matches all, everything should match "all".
>>>
>>> I more wonder why doesn't it match "http_access allow localhost"
>>>
>>> >$ curl -sSL --proxy localhost:3128 -D - "
>>> >https://wiki.squid-cache.org/SquidFaq/SquidAcl" -o /dev/null 2>&1 |
>>> grep
>>> >Squid
>>> >X-Squid-Error: ERR_ACCESS_DENIED 0
>>>
>>> >Any ideas what I'm doing wrong?
>>>
>>> have you reloaded squid config after changing it?
>>> Did squid confirm it?
>>>
>>> --
>>> Matus UHLAR - fantomas, uhlar at fantomas.sk ; http://www.fantomas.sk/
>>> Warning: I wish NOT to receive e-mail advertising to this address.
>>> Varovanie: na tuto adresu chcem NEDOSTAVAT akukolvek reklamnu postu.
>>> It's now safe to throw off your computer.
>>> _______________________________________________
>>> squid-users mailing list
>>> squid-users at lists.squid-cache.org
>>> http://lists.squid-cache.org/listinfo/squid-users
>>>
>>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20181015/4ded9531/attachment.htm>

From rousskov at measurement-factory.com  Mon Oct 15 17:08:05 2018
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Mon, 15 Oct 2018 11:08:05 -0600
Subject: [squid-users] How to create a simple whitelist using regexes?
In-Reply-To: <CAM6dNWrrF629gQurFqJ5-t_eVCLjGqyoTfvpf_LN+yoX0A_puw@mail.gmail.com>
References: <CAM6dNWqt4HREv7tt32=-hfoxiLOapbytjtSdBuH_4+Uq3ZXthw@mail.gmail.com>
 <CAM6dNWrB-=8p=ULEC3X=cyxtffZd6UOhE9dWhbmt4M5RuQT4fg@mail.gmail.com>
 <20181015084924.GA709@fantomas.sk>
 <CAM6dNWrfq4NRBuEWOr02BRX5aTU4ZtZMJC1jfSAsOZ-XgFeATw@mail.gmail.com>
 <CAM6dNWrEsYYKuZjdCe0LQR=vAwyqx_sVHOD2u94=V61zzX4tGA@mail.gmail.com>
 <CAM6dNWrrF629gQurFqJ5-t_eVCLjGqyoTfvpf_LN+yoX0A_puw@mail.gmail.com>
Message-ID: <406e8320-e57b-3f30-c3a6-23abaced69ec@measurement-factory.com>

On 10/15/2018 10:48 AM, RB wrote:

> After some more research it looks like squid only has access to the url
> domain if it's HTTPS and the only way to get the url path and query
> string is to use ssl_bump to decrypt https so squid can see url path and
> query arguments.

Replace "url domain" with "service name". In many cases, they are about
the same today, but there is a trend for SNI values to migrate from
identifying specific sites (e.g., foo.example.com) to identifying broad
services (e.g., everything.example.com), making SNIs increasingly imprecise.

Please note that you cannot bump sites that pin their certificates or
use other measures that prevent bumping. Long-term, most sites will
probably fall into that category by switching to TLS v1.3 and hiding
their true names behind essentially fake/generic SNIs.


> To use ssl_bump, I have to compile the code from source with
> --enable-ssl, create a certificate, and add it to the chain of certs to
> every other vm that proxies through squid, then squid can decrypt the
> https urls to see paths and query args and finally apply the regex to
> those urls in order to only allow explicit regex urls.
> 
> Is this correct?

Replace "add it to the chain of certs" with "add it to the set of
trusted CA certificates". CA certificates are not chained... And, yes,
every client (every "vm" in your case?) that proxies through Squid would
have to trust your CA certificate.

The above sounds correct (and will be painful) if your clients cannot
send unencrypted requests for https:... URLs to Squid. On the other
hand, if your clients can send unencrypted requests for https:... URLs
to Squid, then no bumping is necessary at all. Please note that those
unencrypted requests may be inside an encrypted TLS connection -- they
are not necessarily insecure or unsafe. Unfortunately, popular browsers
do _not_ support sending unencrypted requests for https:... URLs to proxies.


HTH,

Alex.


> On Mon, Oct 15, 2018 at 11:56 AM RB wrote:
> 
>     I think I know what the issue is which can give us a clue to what is
>     going on.
> 
>         2018/10/15 15:05:45.083 kid1| RegexData.cc(71) match:
>         aclRegexData::match: checking 'wiki.squid-cache.org:443
>         <http://wiki.squid-cache.org:443/>'
>         2018/10/15 15:05:45.084 kid1| RegexData.cc(82) match:
>         aclRegexData::match: looking for
>         '(^https://wiki.squid-cache.org/SquidFaq/SquidAcl.*)|(squid-cache.org/SquidFaq/SquidAcl.*
>         <https://wiki.squid-cache.org/SquidFaq/SquidAcl.*%29%7C%28squid-cache.org/SquidFaq/SquidAcl.*>)'
>         2018/10/15 15:05:45.084 kid1| Acl.cc(321) checklistMatches:
>         ACL::ChecklistMatches: result for 'whitelist' is 0
> 
>     The above seems to be applying the regex to
>     "wiki.squid-cache.org:443 <http://wiki.squid-cache.org:443>" instead
>     of to "https://wiki.squid-cache.org/SquidFaq/SquidAcl". I added the
>     regex ".*squid-cache.org.*" to my list of regular expressions and
>     now I see this.
> 
>         2018/10/15 15:16:03.641 kid1| RegexData.cc(71) match:
>         aclRegexData::match: checking 'wiki.squid-cache.org:443
>         <http://wiki.squid-cache.org:443>'
>         2018/10/15 15:16:03.641 kid1| RegexData.cc(82) match:
>         aclRegexData::match: looking for
>         '(^https?://[^/]+/wiki.squid-cache.org/SquidFaq/SquidAcl.*)|(squid-cache.org.*
>         <http://wiki.squid-cache.org/SquidFaq/SquidAcl.*%29%7C%28squid-cache.org.*>)'
>         2018/10/15 15:16:03.641 kid1| RegexData.cc(93) match:
>         aclRegexData::match: match
>         '(^https?://[^/]+/wiki.squid-cache.org/SquidFaq/SquidAcl.*)|(squid-cache.org.*
>         <http://wiki.squid-cache.org/SquidFaq/SquidAcl.*%29%7C%28squid-cache.org.*>)'
>         found in 'wiki.squid-cache.org:443
>         <http://wiki.squid-cache.org:443>'
>         2018/10/15 15:16:03.641 kid1| Acl.cc(321) checklistMatches:
>         ACL::ChecklistMatches: result for 'whitelist' is 1
> 
> 
>     Any idea why url_regex wouldn't try to match the full url and
>     instead only matches on the subdomain, host domain, and port??
> 
>     The Squid FAQ <https://wiki.squid-cache.org/SquidFaq/SquidAcl> says
>     the following:
> 
>         *url_regex*: URL regular expression pattern matching
>         *urlpath_regex*: URL-path regular expression pattern matching,
>         leaves out the protocol and hostname
> 
> 
>     with this example given
> 
>         acl special_url url_regex ^http://www.squid-cache.org/Doc/FAQ/$
> 
> 
>     This seems to be the case between 3.3.8 (default on ubuntu 14.04)
>     and 3.5.12 (default on ubuntu 16.04).
> 
>     Is there another configuration that forces url_regex to match the
>     entire url? or should I use a different acl type?
> 
>     Best,
> 
>     On Mon, Oct 15, 2018 at 11:11 AM RB <ronthecon at gmail.com
>     <mailto:ronthecon at gmail.com>> wrote:
> 
>         Hi Matus,
> 
>         Thanks for responding so quickly. I uploaded my configurations
>         here if that is more helpful: https://bit.ly/2NF4zNb
> 
>         The config that I previously shared is called squid_corp.conf. I
>         also noticed that if I don't use regular expressions and instead
>         use domains, it works correctly:
> 
>             # acl whitelist url_regex "/vagrant/squid_sites.txt"
>             acl whitelist url_regex .squid-cache.org
>             <http://squid-cache.org>
> 
> 
>         Every time my squid.conf or my squid_sites.txt is modified, I
>         restart the squid service
> 
>             sudo service squid3 restart
> 
> 
>         Then I use curl to test and now the url works.?
> 
>             $ curl -sSL --proxy localhost:3128 -D -
>             https://wiki.squid-cache.org/SquidFaq/SquidAcl-o /dev/null 2>&1
>             HTTP/1.1 200 Connection established
> 
>             HTTP/1.1 200 OK
>             Date: Mon, 15 Oct 2018 14:47:33 GMT
>             Server: Apache/2.4.7 (Ubuntu)
>             Vary: Cookie,User-Agent,Accept-Encoding
>             Content-Length: 101912
>             Cache-Control: max-age=3600
>             Expires: Mon, 15 Oct 2018 15:47:33 GMT
>             Content-Type: text/html; charset=utf-8
> 
> 
>         But this does not allow me to get more granular. I can only
>         allow all subdomains and paths for the domain squid-cache.org
>         <http://squid-cache.org> but I'm unable to only allow the
>         regular expressions if I put them inline or put them in
>         squid_sites.txt.
> 
>             # acl whitelist url_regex "/vagrant/squid_sites.txt"
>             acl whitelist url_regex
>             ^https://wiki.squid-cache.org/SquidFaq/SquidAcl.*
>             acl whitelist url_regex
>             .*squid-cache.org/SquidFaq/SquidAcl.*
>             <http://squid-cache.org/SquidFaq/SquidAcl.*>
> 
> 
>         If I put them inline like I have above, when I restarted squid
>         it says the following
> 
>             2018/10/15 14:54:48 kid1| strtokFile:
>             .*squid-cache.org/SquidFaq/SquidAcl.*
>             <http://squid-cache.org/SquidFaq/SquidAcl.*> not found
> 
> 
>         If I put the expressions in the squid_sites.txt the above "not
>         found" message isn't shown and this is the debug output in
>         /var/log/squid3/cache.log (full
>         output?https://pastebin.com/NVwRxVmQ).
> 
>             2018/10/15 15:05:45.083 kid1| Checklist.cc(275) matchNode:
>             0x7fb0068da2b8 matched=1 async=0 finished=0
>             2018/10/15 15:05:45.083 kid1| Acl.cc(336) matches:
>             ACLList::matches: checking whitelist
>             2018/10/15 15:05:45.083 kid1| Acl.cc(319) checklistMatches:
>             ACL::checklistMatches: checking 'whitelist'
>             2018/10/15 15:05:45.083 kid1| RegexData.cc(71) match:
>             aclRegexData::match: checking 'wiki.squid-cache.org:443
>             <http://wiki.squid-cache.org:443>'
>             2018/10/15 15:05:45.084 kid1| RegexData.cc(82) match:
>             aclRegexData::match: looking for
>             '(^https://wiki.squid-cache.org/SquidFaq/SquidAcl.*)|(squid-cache.org/SquidFaq/SquidAcl.*
>             <https://wiki.squid-cache.org/SquidFaq/SquidAcl.*%29%7C%28squid-cache.org/SquidFaq/SquidAcl.*>)'
>             2018/10/15 15:05:45.084 kid1| Acl.cc(321) checklistMatches:
>             ACL::ChecklistMatches: result for 'whitelist' is 0
>             2018/10/15 15:05:45.084 kid1| Acl.cc(349) matches: whitelist
>             mismatched.
>             2018/10/15 15:05:45.084 kid1| Acl.cc(354) matches: whitelist
>             result is false
> 
> 
>         So it's failing the regular expression check. If I use grep to
>         verify if the regex works, it does.
> 
>             $ echo https://wiki.squid-cache.org/SquidFaq/SquidAcl | grep
>             "^https://wiki.squid-cache.org/SquidFaq/SquidAcl.*"
>             https://wiki.squid-cache.org/SquidFaq/SquidAcl
> 
> 
>         > are you aware that you can only see CONNECT in https requests, unless using
>         ssl_bump?
> 
>         Ah interesting. Are you saying that my https connections will
>         always fail unless I use ssl_bump to decrypt https to http
>         connections? How would this work correctly in production? Does
>         squid proxy only block urls if it detects http? How do you
>         configure ssl_bump to work in this case? and is that viable in
>         production?
> 
>         > of course it matches all, everything should match "all".
>         > I more wonder why doesn't it match "http_access allow localhost"
> 
>         >?have you reloaded squid config after changing it?
>         > Did squid confirm it?
> 
>         Would you have an example of one entire config file that would
>         work to whitelist an http/https url using a regular expression?
> 
>         Best,
> 
> 
>         On Mon, Oct 15, 2018 at 4:49 AM Matus UHLAR - fantomas
>         <uhlar at fantomas.sk <mailto:uhlar at fantomas.sk>> wrote:
> 
>             KOn 15.10.18 01:04, RB wrote:
>             >I'm trying to deny all urls except for only whitelisted regular
>             >expressions. I have only this regular expression in my file
>             >"squid_sites.txt"
>             >
>             >^https://wiki.squid-cache.org/SquidFaq/SquidAcl.*
> 
>             are you aware that you can only see CONNECT in https
>             requests, unless using
>             ssl_bump?
> 
> 
>             >acl bastion src 10.5.0.0/1 <http://10.5.0.0/1>
>             >acl whitelist url_regex "/vagrant/squid_sites.txt"
>             [...]
>             >http_access allow manager localhost
>             >http_access deny manager
>             >http_access deny !Safe_ports
>             >http_access allow localhost
>             >http_access allow purge localhost
>             >http_access deny purge
>             >http_access deny CONNECT !SSL_ports
>             >
>             >http_access allow bastion whitelist
>             >http_access deny bastion all
> 
>             >I tried enabling debugging and tailing
>             /var/log/squid3/cache.log but my
>             >curl statement keeps matching "all".
> 
>             of course it matches all, everything should match "all".
> 
>             I more wonder why doesn't it match "http_access allow localhost"
> 
>             >$ curl -sSL --proxy localhost:3128 -D - "
>             >https://wiki.squid-cache.org/SquidFaq/SquidAcl" -o
>             /dev/null 2>&1 | grep
>             >Squid
>             >X-Squid-Error: ERR_ACCESS_DENIED 0
> 
>             >Any ideas what I'm doing wrong?
> 
>             have you reloaded squid config after changing it?
>             Did squid confirm it?
> 
>             -- 
>             Matus UHLAR - fantomas, uhlar at fantomas.sk
>             <mailto:uhlar at fantomas.sk> ; http://www.fantomas.sk/
>             Warning: I wish NOT to receive e-mail advertising to this
>             address.
>             Varovanie: na tuto adresu chcem NEDOSTAVAT akukolvek
>             reklamnu postu.
>             It's now safe to throw off your computer.
>             _______________________________________________
>             squid-users mailing list
>             squid-users at lists.squid-cache.org
>             <mailto:squid-users at lists.squid-cache.org>
>             http://lists.squid-cache.org/listinfo/squid-users
> 
> 
> 
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
> 



From uhlar at fantomas.sk  Mon Oct 15 17:25:38 2018
From: uhlar at fantomas.sk (Matus UHLAR - fantomas)
Date: Mon, 15 Oct 2018 19:25:38 +0200
Subject: [squid-users] How to create a simple whitelist using regexes?
In-Reply-To: <CAM6dNWrrF629gQurFqJ5-t_eVCLjGqyoTfvpf_LN+yoX0A_puw@mail.gmail.com>
References: <CAM6dNWqt4HREv7tt32=-hfoxiLOapbytjtSdBuH_4+Uq3ZXthw@mail.gmail.com>
 <CAM6dNWrB-=8p=ULEC3X=cyxtffZd6UOhE9dWhbmt4M5RuQT4fg@mail.gmail.com>
 <20181015084924.GA709@fantomas.sk>
 <CAM6dNWrfq4NRBuEWOr02BRX5aTU4ZtZMJC1jfSAsOZ-XgFeATw@mail.gmail.com>
 <CAM6dNWrEsYYKuZjdCe0LQR=vAwyqx_sVHOD2u94=V61zzX4tGA@mail.gmail.com>
 <CAM6dNWrrF629gQurFqJ5-t_eVCLjGqyoTfvpf_LN+yoX0A_puw@mail.gmail.com>
Message-ID: <20181015172538.GA8499@fantomas.sk>

On 15.10.18 12:48, RB wrote:
>After some more research it looks like squid only has access to the url
>domain if it's HTTPS and the only way to get the url path and query string
>is to use ssl_bump to decrypt https so squid can see url path and query
>arguments.

this is what I wrote before. Looking at it now, I should have explained more
deeply....

>>> > are you aware that you can only see CONNECT in https requests, unless
>>> > using ssl_bump?

>To use ssl_bump, I have to compile the code from source with --enable-ssl,
>create a certificate, and add it to the chain of certs to every other vm
>that proxies through squid, then squid can decrypt the https urls to see
>paths and query args and finally apply the regex to those urls in order to
>only allow explicit regex urls.
>
>Is this correct?

Alex has explained already.

I would like to note that the whole purpose of SSL encription in HTTPS is to
deny anyone between client and server to see what is the client accessing.
That includes your proxy.

And we often see complaints about SSL bump not working because different
clients expect certificates signed by their certificate autorities, not by
yours.

-- 
Matus UHLAR - fantomas, uhlar at fantomas.sk ; http://www.fantomas.sk/
Warning: I wish NOT to receive e-mail advertising to this address.
Varovanie: na tuto adresu chcem NEDOSTAVAT akukolvek reklamnu postu.
Windows 2000: 640 MB ought to be enough for anybody


From danilovt at gmail.com  Mon Oct 15 22:09:41 2018
From: danilovt at gmail.com (Danilo V)
Date: Mon, 15 Oct 2018 19:09:41 -0300
Subject: [squid-users] Delay pools and external acl
Message-ID: <CAHaQnLNHRJ-qFCdNOw1P+dpRJX7JPYz7zc6-MQyc==dyHFMgnQ@mail.gmail.com>

Hi all,

Has anyone succeeded applying delay pools on groups from AD?

I'm using squid 3.5.23 with basic_ldap_auth.
I initially tried to combine mapping groups with external acl type
(ext_ldap_group_acl) to delay pools. It's a trap :-(

After doing more search I found about class 5 and note acl.
Has anyone a pratical implementation in this scenario?

Thanks,
Danilo
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20181015/59f89ec7/attachment.htm>

From timur.lagutenko at gmail.com  Tue Oct 16 05:46:05 2018
From: timur.lagutenko at gmail.com (Timur Lagutenko)
Date: Tue, 16 Oct 2018 10:46:05 +0500
Subject: [squid-users] Unable to open youtube.com
Message-ID: <CAKK+L1cX9Xau0NivY1YobYrT27csuPK-p-FV5UtZUKbf_PH_UQ@mail.gmail.com>

Hello friends,

recently I've updated my freebsd gateway.
from 11.1 to 11.2.
also I've updated squid form 3.5 to 4.1
i have no transparency, no ssl-bump/splice etc..
simple installation.
browser is configured to use proxy.
squid configuration is default.
everything works fine except youtube.com
Browser freezes on "trying to set secure connection", and after gives
time-out error.
i've also tied to downgrade squid back to 3,5
no success.

nothing strange in logs
1539668124.660  30055 192.168.0.104 TCP_TUNNEL/200 39 CONNECT
www.youtube.com:443 - HIER_DIRECT/74.125.232.167 -

dns is the same as for a client as for a squid.
other sites works fine.
if i give direct access (without squid - direct NAT) - youtube is working.
any ideas?
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20181016/14d6c600/attachment.htm>

From morteza1131 at yahoo.com  Tue Oct 16 07:06:47 2018
From: morteza1131 at yahoo.com (morteza omidian)
Date: Tue, 16 Oct 2018 07:06:47 +0000 (UTC)
Subject: [squid-users] squid clientside_mark problem
In-Reply-To: <1397618449.867088.1539530280162@mail.yahoo.com>
References: <1397618449.867088.1539530280162.ref@mail.yahoo.com>
 <1397618449.867088.1539530280162@mail.yahoo.com>
Message-ID: <663781969.438362.1539673607809@mail.yahoo.com>

Hi
Witch options do i have to compile with to enable "clientside_mark"??

tank






On Sunday, October 14, 2018, 6:48:00 PM GMT+3:30, morteza omidian <morteza1131 at yahoo.com> wrote: 





Hi
I installed squid4.3 in my linux OS,
when I configure squid with clientside_mark i get an error tells me this is not possible!!
I use this page-line75 as guidance for my configuration!!
?My configuration is:
acl MY_ACL clientside_mark 0x1tcp_outgoing_mark 0x20 MY_ACLWhat is wrong??


From morteza1131 at yahoo.com  Tue Oct 16 09:38:53 2018
From: morteza1131 at yahoo.com (morteza omidian)
Date: Tue, 16 Oct 2018 09:38:53 +0000 (UTC)
Subject: [squid-users] acl "clientside_mark" problem
References: <94891910.478463.1539682733632.ref@mail.yahoo.com>
Message-ID: <94891910.478463.1539682733632@mail.yahoo.com>

Hi
why anybody don't answer me? Please help!
I compiled Squid 4.3 with this options in linux server:????????????????????./configure?? --enable-linux-netfilter? --enable-zph-qos 
when i change my squid configuration file and add this line "acl MY_CLIENTs clientside_mark 0x1" I got? error messages like this:
root at linux:~# /usr/local/squid/sbin/squid -k restart
2018/10/16 09:33:42| FATAL: Invalid ACL type 'clientside_mark'2018/10/16 09:33:42| FATAL: Bungled /usr/local/squid/etc/squid.conf line 5: acl CLIENT clientside_mark 12018/10/16 09:33:42| Squid Cache (Version 4.3): Terminated abnormally.CPU Usage: 0.025 seconds = 0.017 user + 0.008 sysMaximum Resident Size: 38592 KBPage faults with physical i/o: 0
What is wrong with my configuration??What should? i do?
Tanx
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20181016/44aa67f7/attachment.htm>

From Antony.Stone at squid.open.source.it  Tue Oct 16 10:13:09 2018
From: Antony.Stone at squid.open.source.it (Antony Stone)
Date: Tue, 16 Oct 2018 12:13:09 +0200
Subject: [squid-users] acl "clientside_mark" problem
In-Reply-To: <94891910.478463.1539682733632@mail.yahoo.com>
References: <94891910.478463.1539682733632.ref@mail.yahoo.com>
 <94891910.478463.1539682733632@mail.yahoo.com>
Message-ID: <201810161213.09376.Antony.Stone@squid.open.source.it>

On Tuesday 16 October 2018 at 11:38:53, morteza omidian wrote:

> Hi
> why anybody don't answer me?

Maybe we don't have experience with precisely what you're trying to do.

Maybe we're busy with our day jobs.

Maybe we're confused by the multiple messages you send about approximately the 
same topic but with different information in them.

Maybe because we're waiting for your answer to responses sent back to you (eg: 
http://lists.squid-cache.org/pipermail/squid-users/2018-October/019432.html )

> Please help!

Please remember this is free, open source software and you are writing to a 
mailing list and asking for help from volunteers who choose to spend some of 
their own time answering questions they feel able to help with.

This is not a commercial support service.

> I compiled Squid 4.3 with this options in linux
> server:                    ./configure   --enable-linux-netfilter 
> --enable-zph-qos when i change my squid configuration file and add this
> line "acl MY_CLIENTs clientside_mark 0x1" I got  error messages like this:
> root at linux:~# /usr/local/squid/sbin/squid -k restart
> 2018/10/16 09:33:42| FATAL: Invalid ACL type 'clientside_mark'2018/10/16
> 09:33:42| FATAL: Bungled /usr/local/squid/etc/squid.conf line 5: acl
> CLIENT clientside_mark 12018/10/16 09:33:42| Squid Cache (Version 4.3):
> Terminated abnormally.CPU Usage: 0.025 seconds = 0.017 user + 0.008
> sysMaximum Resident Size: 38592 KBPage faults with physical i/o: 0 What is
> wrong with my configuration??What should  i do?
> Tanx

Also, formatting your messages more clearly might help people to understand 
the question better.

Oh, yes, and Please Do Not set Reply-to on list messages.


Regards,


Antony.

-- 
Perfection in design is achieved not when there is nothing left to add, but 
rather when there is nothing left to take away.

 - Antoine de Saint-Exupery

                                                   Please reply to the list;
                                                         please *don't* CC me.


From anon.amish at gmail.com  Tue Oct 16 11:35:18 2018
From: anon.amish at gmail.com (Amish)
Date: Tue, 16 Oct 2018 17:05:18 +0530
Subject: [squid-users] deny_info and CONNECT for https request gives SSL
	error
Message-ID: <7f39ba71-493d-1b36-e238-191942e78435@gmail.com>

Hello

I have this simplified ACL in squid.conf

acl denyit src all
deny_info http://192.168.1.1/blocked.html denyit
http_access deny denyit

The purpose is to block everything and redirect to 
http://192.168.1.1/blocked.html

It works fine with http (non-secure) CONNECT request ...

 > curl -ix 192.168.1.1:8080 http://google.com
HTTP/1.1 302 Found
Server: squid/4.3
Mime-Version: 1.0
Date: Tue, 16 Oct 2018 11:02:05 GMT
Content-Type: text/html;charset=utf-8
Content-Length: 0
Location: http://192.168.1.1/blocked.html
X-Squid-Error: 403 Access Denied
X-Cache: MISS from somehost
X-Cache-Lookup: NONE from somehost:8080
Connection: keep-alive

 ?... but gives certificate error with https CONNECT request.

 > curl -ix 192.168.1.1:8080 https://google.com
HTTP/1.1 200 Connection established

curl: (60) SSL certificate problem: self signed certificate in 
certificate chain
More details here: https://curl.haxx.se/docs/sslcerts.html

curl failed to verify the legitimacy of the server and therefore could not
establish a secure connection to it. To learn more about this situation and
how to fix it, please visit the web page mentioned above.


I believe this is because squid first gives 200 status and hence browser 
thinks connection is established expects Google's certificate.

Can squid not return with 302/307 on CONNECT HTTPs requests for deny_info?

Like instead of HTTP/1.1 200 Connection established - it returns:

HTTP/1.1 307 Redirect
Location: http://192.168.1.1/blocked.html
(...)

RFC 7231 https://tools.ietf.org/html/rfc7231#section-4.3.6 states that:

Any response other than a successful response
    indicates that the tunnel has not yet been formed and that the
    connection remains governed by HTTP.


which means HTTP/1.1 307 Redirect should make browser treat connection 
as HTTP and hopefully also follow Location.

Any idea? Or any other workaround (except importing squid certificate)

Thanks and regards,

Amish
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20181016/f7065d7c/attachment.htm>

From anon.amish at gmail.com  Tue Oct 16 12:29:24 2018
From: anon.amish at gmail.com (Amish)
Date: Tue, 16 Oct 2018 17:59:24 +0530
Subject: [squid-users] deny_info and CONNECT for https request gives SSL
	error
In-Reply-To: <7f39ba71-493d-1b36-e238-191942e78435@gmail.com>
References: <7f39ba71-493d-1b36-e238-191942e78435@gmail.com>
Message-ID: <24f73db7-64c5-2cdf-b035-5f5e78937bb9@gmail.com>

Further to this:

I have ssl-bump setup on port 8080.

If I remove ssl-bump squid works just like I mentioned in my earlier e-mail.

 > curl -ix 192.168.1.1:8080 https://google.com
HTTP/1.1 307 Temporary Redirect
Server: squid/4.3
Mime-Version: 1.0
Date: Tue, 16 Oct 2018 12:01:41 GMT
Content-Type: text/html;charset=utf-8
Content-Length: 0
Location: http://192.168.1.1/blocked.html
X-Squid-Error: 403 Access Denied
X-Cache: MISS from somehost
X-Cache-Lookup: NONE from somehost:8080
Connection: keep-alive

curl: (56) Received HTTP code 307 from proxy after CONNECT


So then I reactivated ssl-bump and added this at top of squid.conf:

http_port 8080 ssl-bump ...
ssl_bump splice all

i.e. http_port has ssl-bump but squid is not really supposed to bump at all.

But squid still goes ahead and replies with 200 Connection established.

So it means that this is a bug in ssl-bump implementation.

It seems that current algorithm for ssl-bump is:

1) Browser sends CONNECT request
2) Squid sends status 200 Connection Established
3) Check ACL
4) If denied, bump the connection with squid certificate
5) If allowed, go ahead with tunneling / bumping as applicable


In my opinion correct flow should be like this:

1) Browser sends CONNECT request
2) Check ACL
3) If denied, return with 307 (or 302)
4) If allowed, go ahead with tunneling / bumping as applicable


Please test / check,

Thank you,

Amish.


On 16/10/18 5:05 PM, Amish wrote:
> Hello
>
> I have this simplified ACL in squid.conf
>
> acl denyit src all
> deny_info http://192.168.1.1/blocked.html denyit
> http_access deny denyit
>
> The purpose is to block everything and redirect to 
> http://192.168.1.1/blocked.html
>
> It works fine with http (non-secure) CONNECT request ...
>
> > curl -ix 192.168.1.1:8080 http://google.com
> HTTP/1.1 302 Found
> Server: squid/4.3
> Mime-Version: 1.0
> Date: Tue, 16 Oct 2018 11:02:05 GMT
> Content-Type: text/html;charset=utf-8
> Content-Length: 0
> Location: http://192.168.1.1/blocked.html
> X-Squid-Error: 403 Access Denied
> X-Cache: MISS from somehost
> X-Cache-Lookup: NONE from somehost:8080
> Connection: keep-alive
>
> ?... but gives certificate error with https CONNECT request.
>
> > curl -ix 192.168.1.1:8080 https://google.com
> HTTP/1.1 200 Connection established
>
> curl: (60) SSL certificate problem: self signed certificate in 
> certificate chain
> More details here: https://curl.haxx.se/docs/sslcerts.html
>
> curl failed to verify the legitimacy of the server and therefore could not
> establish a secure connection to it. To learn more about this 
> situation and
> how to fix it, please visit the web page mentioned above.
>
>
> I believe this is because squid first gives 200 status and hence 
> browser thinks connection is established expects Google's certificate.
>
> Can squid not return with 302/307 on CONNECT HTTPs requests for deny_info?
>
> Like instead of HTTP/1.1 200 Connection established - it returns:
>
> HTTP/1.1 307 Redirect
> Location: http://192.168.1.1/blocked.html
> (...)
>
> RFC 7231 https://tools.ietf.org/html/rfc7231#section-4.3.6 states that:
> Any response other than a successful response
>     indicates that the tunnel has not yet been formed and that the
>     connection remains governed by HTTP.
>
> which means HTTP/1.1 307 Redirect should make browser treat connection 
> as HTTP and hopefully also follow Location.
>
> Any idea? Or any other workaround (except importing squid certificate)
>
> Thanks and regards,
>
> Amish

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20181016/f3c946a4/attachment.htm>

From rousskov at measurement-factory.com  Tue Oct 16 15:35:34 2018
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Tue, 16 Oct 2018 09:35:34 -0600
Subject: [squid-users] deny_info and CONNECT for https request gives SSL
 error
In-Reply-To: <24f73db7-64c5-2cdf-b035-5f5e78937bb9@gmail.com>
References: <7f39ba71-493d-1b36-e238-191942e78435@gmail.com>
 <24f73db7-64c5-2cdf-b035-5f5e78937bb9@gmail.com>
Message-ID: <d3681df9-80a9-c37c-c351-398495814334@measurement-factory.com>

On 10/16/2018 06:29 AM, Amish wrote:

> It seems that current algorithm for ssl-bump is:
> 
> 1) Browser sends CONNECT request
> 2) Squid sends status 200 Connection Established
> 3) Check ACL
> 4) If denied, bump the connection with squid certificate
> 5) If allowed, go ahead with tunneling / bumping as applicable


Kind of. The actual sequence in this particular case is a bit different
because Squid discovers a denied CONNECT _before_ Squid decides how to
respond to that CONNECT request:

  1) Receive CONNECT request.
  3) Check access controls. The request is denied!
  2) Send HTTP 200 Connection Established (see below for "why").
  4) Bump the connections to deny/redirect the first bumped request.


> In my opinion correct flow should be like this:
> 
> 1) Browser sends CONNECT request
> 2) Check ACL
> 3) If denied, return with 307 (or 302)
> 4) If allowed, go ahead with tunneling / bumping as applicable

Unfortunately, that ideal sequence does not work well in practice
because popular browsers ignore CONNECT responses other than HTTP 200
and 407. As a consequence, if you want to redirect "secure" browser
traffic, Squid has to bump it first.


HTH,

Alex.


From anon.amish at gmail.com  Tue Oct 16 16:01:54 2018
From: anon.amish at gmail.com (Amish)
Date: Tue, 16 Oct 2018 21:31:54 +0530
Subject: [squid-users] deny_info and CONNECT for https request gives SSL
 error
In-Reply-To: <d3681df9-80a9-c37c-c351-398495814334@measurement-factory.com>
References: <7f39ba71-493d-1b36-e238-191942e78435@gmail.com>
 <24f73db7-64c5-2cdf-b035-5f5e78937bb9@gmail.com>
 <d3681df9-80a9-c37c-c351-398495814334@measurement-factory.com>
Message-ID: <3d4f12e2-cfde-20e5-4603-58f3306dfced@gmail.com>



On 16/10/18 9:05 PM, Alex Rousskov wrote:
> On 10/16/2018 06:29 AM, Amish wrote:
>
>> In my opinion correct flow should be like this:
>>
>> 1) Browser sends CONNECT request
>> 2) Check ACL
>> 3) If denied, return with 307 (or 302)
>> 4) If allowed, go ahead with tunneling / bumping as applicable
> Unfortunately, that ideal sequence does not work well in practice
> because popular browsers ignore CONNECT responses other than HTTP 200
> and 407. As a consequence, if you want to redirect "secure" browser
> traffic, Squid has to bump it first.
>
>
> HTH,
>
> Alex.

No thats not correct.

Thing is that squid behaves differently for 2 exactly same CONNECT 
request with only difference being ssl-bump

Case 1:
http_port 8080 #no ssl-bump
acl denyit src all
deny_info http://192.168.1.1/blocked.html denyit
http_access deny denyit

 > curl -ix 192.168.1.1:8080 https://google.com
HTTP/1.1 307 Temporary Redirect
Server: squid/4.3
Mime-Version: 1.0
Date: Tue, 16 Oct 2018 12:01:41 GMT
Content-Type: text/html;charset=utf-8
Content-Length: 0
Location: http://192.168.1.1/blocked.html
X-Squid-Error: 403 Access Denied
X-Cache: MISS from somehost
X-Cache-Lookup: NONE from somehost:8080
Connection: keep-alive


Notice that squid is indeed responding with code other than 200 or 407 
for CONNECT and HTTPS request.

So what you said does not seem to be correct.

Case 2:
http_port 8080 ssl-bump ...
acl denyit src all
deny_info http://192.168.1.1/blocked.html denyit
http_access deny denyit

 > curl -ix 192.168.1.1:8080 https://google.com
HTTP/1.1 200 Connection established

curl: (60) SSL certificate problem: self signed certificate in 
certificate chain
...



Case 1: Browser gives "Proxy connection refused" (or similar error).
Case 2: Browser gives "SSL certificate error".

Case 1 - Browser atleast makes it clear to end user that this is 
something that proxy is not allowing.
Case 2 - End user would be clueless on why SSL error? He will never know 
that its blocked by proxy.

To me case 1 is more appropriate response.

Please give a thought,

Thank you,

Amish.


From rousskov at measurement-factory.com  Tue Oct 16 16:37:36 2018
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Tue, 16 Oct 2018 10:37:36 -0600
Subject: [squid-users] deny_info and CONNECT for https request gives SSL
 error
In-Reply-To: <3d4f12e2-cfde-20e5-4603-58f3306dfced@gmail.com>
References: <7f39ba71-493d-1b36-e238-191942e78435@gmail.com>
 <24f73db7-64c5-2cdf-b035-5f5e78937bb9@gmail.com>
 <d3681df9-80a9-c37c-c351-398495814334@measurement-factory.com>
 <3d4f12e2-cfde-20e5-4603-58f3306dfced@gmail.com>
Message-ID: <407bed3d-46d3-e8f7-f523-5dbdd4d63272@measurement-factory.com>

On 10/16/2018 10:01 AM, Amish wrote:
> On 16/10/18 9:05 PM, Alex Rousskov wrote:
>> On 10/16/2018 06:29 AM, Amish wrote:
>>> In my opinion correct flow should be like this:
>>>
>>> 1) Browser sends CONNECT request
>>> 2) Check ACL
>>> 3) If denied, return with 307 (or 302)
>>> 4) If allowed, go ahead with tunneling / bumping as applicable

>> Unfortunately, that ideal sequence does not work well in practice
>> because popular browsers ignore CONNECT responses other than HTTP 200
>> and 407. As a consequence, if you want to redirect "secure" browser
>> traffic, Squid has to bump it first.

> Thing is that squid behaves differently for 2 exactly same CONNECT
> request with only difference being ssl-bump

Yes, Squid behaves differently when configured differently.

* My original response was specific to SslBump-enabled Squid ports.
Today, those configurations assume that the admin wants to bump CONNECTs
on errors (and has given Squid the certificate to enable such bumping).

* For SslBump-disabled ports (which is the default), Squid has no choice
but to deny/redirect the CONNECT request itself. Denied/redirected
CONNECT requests are mishandled by popular browsers -- Squid denial
errors are not shown to the user, and redirects are not followed.

Please note that the difference is not in matching ssl_bump actions, but
in whether the corresponding http_port was configured to use SslBump. In
the former case, whether the ssl_bump rules are checked depends on the
SslBump step where the CONNECT request is denied/redirected. In the
second/default case, ssl_bump rules are never checked.


If you prefer non-SslBump behavior, you should use it, of course! Some
admins find that browser-generated errors are insufficiently detailed
and/or produce more support queries than Squid-generated errors. YMMV.

If you want to change SslBump behavior when denying or redirecting
CONNECT requests, please make a specific proposal, keeping in mind that
many existing Squid deployments depend on Squid error pages being
displayed to the user (and/or on Squid redirects followed). Your
proposal will need to either convince folks that the existing behavior
should change or add options to optionally enable some new behavior.

Alex.


From bruno.larini at riosoft.com.br  Tue Oct 16 17:22:29 2018
From: bruno.larini at riosoft.com.br (Bruno de Paula Larini)
Date: Tue, 16 Oct 2018 14:22:29 -0300
Subject: [squid-users] Unable to open youtube.com
In-Reply-To: <CAKK+L1cX9Xau0NivY1YobYrT27csuPK-p-FV5UtZUKbf_PH_UQ@mail.gmail.com>
References: <CAKK+L1cX9Xau0NivY1YobYrT27csuPK-p-FV5UtZUKbf_PH_UQ@mail.gmail.com>
Message-ID: <1e605630-706c-8979-c76c-e513d83c3c4f@riosoft.com.br>


Em 16/10/2018 02:46, Timur Lagutenko escreveu:
> Hello friends,
>
> recently I've updated my freebsd gateway.
> from 11.1 to 11.2.
> also I've updated squid form 3.5 to 4.1
> i have no transparency, no ssl-bump/splice etc..
> simple installation.
> browser is configured to use proxy.
> squid configuration is default.
> everything works fine except youtube.com <http://youtube.com/>
> Browser freezes on "trying to set secure connection", and after gives 
> time-out error.
> i've also tied to downgrade squid back to 3,5
> no success.
>
> nothing strange in logs
> 1539668124.660? 30055 192.168.0.104 TCP_TUNNEL/200 39 CONNECT 
> www.youtube.com:443 <http://www.youtube.com:443/>?- 
> HIER_DIRECT/74.125.232.167 <http://74.125.232.167/>?-
>
> dns is the same as for a client as for a squid.
> other sites works fine.
> if i give direct access (without squid - direct NAT) - youtube is working.
> any ideas?
Hello Timur.
Are you certain it isn't a firewall/routing/port issue?
My guess is that the clients are being directed to an incorrect 
destination (or being dropped) specially because the time-out error.
I don't think Squid would consider YouTube different than any other 
website if you didn't told it to do so. Maybe there's a leftover rule 
somewhere in your firewall? Remember that it should contain INPUT and 
OUTPUT rules for Squid rather than FORWARD.

Good luck.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20181016/8e12d12f/attachment.htm>

From anon.amish at gmail.com  Wed Oct 17 02:15:24 2018
From: anon.amish at gmail.com (Amish)
Date: Wed, 17 Oct 2018 07:45:24 +0530
Subject: [squid-users] deny_info and CONNECT for https request gives SSL
 error
In-Reply-To: <407bed3d-46d3-e8f7-f523-5dbdd4d63272@measurement-factory.com>
References: <7f39ba71-493d-1b36-e238-191942e78435@gmail.com>
 <24f73db7-64c5-2cdf-b035-5f5e78937bb9@gmail.com>
 <d3681df9-80a9-c37c-c351-398495814334@measurement-factory.com>
 <3d4f12e2-cfde-20e5-4603-58f3306dfced@gmail.com>
 <407bed3d-46d3-e8f7-f523-5dbdd4d63272@measurement-factory.com>
Message-ID: <02a57bf8-d124-888a-4a89-80cb0c0aab37@gmail.com>

On 16/10/18 10:07 PM, Alex Rousskov wrote:
> On 10/16/2018 10:01 AM, Amish wrote:
>
>> Thing is that squid behaves differently for 2 exactly same CONNECT
>> request with only difference being ssl-bump
> Yes, Squid behaves differently when configured differently.
>
> * My original response was specific to SslBump-enabled Squid ports.
> Today, those configurations assume that the admin wants to bump CONNECTs
> on errors (and has given Squid the certificate to enable such bumping).
>
> * For SslBump-disabled ports (which is the default), Squid has no choice
> but to deny/redirect the CONNECT request itself. Denied/redirected
> CONNECT requests are mishandled by popular browsers -- Squid denial
> errors are not shown to the user, and redirects are not followed.
>
> Please note that the difference is not in matching ssl_bump actions, but
> in whether the corresponding http_port was configured to use SslBump. In
> the former case, whether the ssl_bump rules are checked depends on the
> SslBump step where the CONNECT request is denied/redirected. In the
> second/default case, ssl_bump rules are never checked.

So if I have following config:

http_port 8080 ssl-bump ...
acl denyit src all
deny_info http://192.168.1.1/blocked.html denyit
http_access deny denyit
ssl_bump splice all

i.e. ssl-bump enabled on port but splice everything. (test case)

In this case one would expect that squid would not bump the connection 
and return with 307 instead of 200.

But since it already sent 200 Connection Established - there is no 
returning back.
> If you prefer non-SslBump behavior, you should use it, of course! Some
> admins find that browser-generated errors are insufficiently detailed
> and/or produce more support queries than Squid-generated errors. YMMV.
>
> If you want to change SslBump behavior when denying or redirecting
> CONNECT requests, please make a specific proposal, keeping in mind that
> many existing Squid deployments depend on Squid error pages being
> displayed to the user (and/or on Squid redirects followed). Your
> proposal will need to either convince folks that the existing behavior
> should change or add options to optionally enable some new behavior.

My proposal for would be to add "-n" (nobump) option to deny_info.

If -n is specified then squid will send 307 directly instead of 200.

Case 1)
deny_info http://192.168.1.1/blocked.html denyit

Return with 200 and bump it (existing behaviour)

Case 2)
deny_info 3xx:http://192.168.1.1/blocked.html denyit

Return with 200 and bump it (existing behaviour)

Case 3)
deny_info -n http://192.168.1.1/blocked.html denyit

Return with 307 Temporary Redirect and Location: header

Case 4)
deny_info -n 302:http://192.168.1.1/blocked.html denyit

Return with 302 Found and Location: header.

Case 1 and 2 above applicable only for sslbump cases.

For non-sslbump it already behaves as 3) and 4) above.


This would not change anything for existing users who want existing 
behaviour.

But allow people like me to *NOT* bump connection when deny_info is 
activated.

Please give a thought,

Thank you,

Amish


From squid3 at treenet.co.nz  Wed Oct 17 02:46:04 2018
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Wed, 17 Oct 2018 15:46:04 +1300
Subject: [squid-users] How to create a simple whitelist using regexes?
In-Reply-To: <CAM6dNWrfq4NRBuEWOr02BRX5aTU4ZtZMJC1jfSAsOZ-XgFeATw@mail.gmail.com>
References: <CAM6dNWqt4HREv7tt32=-hfoxiLOapbytjtSdBuH_4+Uq3ZXthw@mail.gmail.com>
 <CAM6dNWrB-=8p=ULEC3X=cyxtffZd6UOhE9dWhbmt4M5RuQT4fg@mail.gmail.com>
 <20181015084924.GA709@fantomas.sk>
 <CAM6dNWrfq4NRBuEWOr02BRX5aTU4ZtZMJC1jfSAsOZ-XgFeATw@mail.gmail.com>
Message-ID: <d5166d90-9659-91dd-7cc9-0c5712bb001f@treenet.co.nz>

In addition to what Matus and Alex have already said about your problem,
you do not appear to understand regex patterns properly.


On 16/10/18 4:11 AM, RB wrote:
> Hi Matus,
> 
> Thanks for responding so quickly. I uploaded my configurations here if
> that is more helpful: https://bit.ly/2NF4zNb
> 
> The config that I previously shared is called squid_corp.conf. I also
> noticed that if I don't use regular expressions and instead use domains,
> it works correctly:
> 
>     # acl whitelist url_regex "/vagrant/squid_sites.txt"
>     acl whitelist url_regex .squid-cache.org

This is still a regex. The ACL type is "url_regex" which makes the
string a regex - no matter what it looks like to your human eyes. To
Squid it is a regex.

It will match things like http://example.com/sZsquid-cacheXORG just
easily as any sub-domain of squid-cache.org. For example any traffic
injecting our squid-cache.org domain into their path or query-string.



> 
> Every time my squid.conf or my squid_sites.txt is modified, I restart
> the squid service
> 
>     sudo service squid3 restart
> 

If Squid does not accept the config file it will not necessarily restart.

You should always run "squid -k parse" or "squid3 -k parse" to check the
config before attempting a restart.


The old Debian sysV init scripts had some protections that would protect
you from problems. But the newer systemd "service" systems are not able
to do that in a nice way. The habit is a good one to get into anyway.


> 
> Then I use curl to test and now the url works.?
> 
>     $ curl -sSL --proxy localhost:3128 -D -
>     https://wiki.squid-cache.org/SquidFaq/SquidAcl-o /dev/null 2>&1
>     HTTP/1.1 200 Connection established
> 
>     HTTP/1.1 200 OK
>     Date: Mon, 15 Oct 2018 14:47:33 GMT
>     Server: Apache/2.4.7 (Ubuntu)
>     Vary: Cookie,User-Agent,Accept-Encoding
>     Content-Length: 101912
>     Cache-Control: max-age=3600
>     Expires: Mon, 15 Oct 2018 15:47:33 GMT
>     Content-Type: text/html; charset=utf-8
> 
> 
> But this does not allow me to get more granular. I can only allow all
> subdomains and paths for the domain squid-cache.org
> <http://squid-cache.org> but I'm unable to only allow the regular
> expressions if I put them inline or put them in squid_sites.txt.
> 
>     # acl whitelist url_regex "/vagrant/squid_sites.txt"
>     acl whitelist url_regex
>     ^https://wiki.squid-cache.org/SquidFaq/SquidAcl.*
>     acl whitelist url_regex .*squid-cache.org/SquidFaq/SquidAcl.*

Any regex pattern that lacks the beginning (^) and ending ($) anchor
symbols is always a match against *anywhere* in the input string.

So starting it with an optional prefix (.* or .?) or ending it with an
optional suffix (.* or .?) is pointless and confusing.


Notice how the pattern Squid is actually using lacks these prefix/suffix
parts or your patterns:

>     aclRegexData::match: looking for
>     '(^https://wiki.squid-cache.org/SquidFaq/SquidAcl.*)|(squid-cache.org/SquidFaq/SquidAcl.*)'


> 
>> are you aware that you can only see CONNECT in https requests, unless using
> ssl_bump?
> 
> Ah interesting. Are you saying that my https connections will always
> fail 

They will always fail to match your current regex, because your current
regex contain characters which are only ever existing in path portions
of URLs (note the *L*). Never in a CONNECT message URI (note the *I*)
which never contains any path portion.


> unless I use ssl_bump to decrypt https to http connections? How
> would this work correctly in production? Does squid proxy only block
> urls if it detects http? How do you configure ssl_bump to work in this
> case? and is that viable in production?

SSL-Bump is to take the CONNECT tunnel data/payload portion and
_attempt_ decrypt any TLS inside. *If* the tunnel contains HTTPS traffic
(not guaranteed) that is where the full https:// ... URLs are found.

Matus and Alex have already mentioned the issues with that so I wont
cover it again.

Amos


From squid3 at treenet.co.nz  Wed Oct 17 03:01:31 2018
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Wed, 17 Oct 2018 16:01:31 +1300
Subject: [squid-users] squid clientside_mark problem
In-Reply-To: <663781969.438362.1539673607809@mail.yahoo.com>
References: <1397618449.867088.1539530280162.ref@mail.yahoo.com>
 <1397618449.867088.1539530280162@mail.yahoo.com>
 <663781969.438362.1539673607809@mail.yahoo.com>
Message-ID: <e8e12993-5119-89dd-7bfd-3e70a54632da@treenet.co.nz>

On 16/10/18 8:06 PM, morteza omidian wrote:
> Hi
> Witch options do i have to compile with to enable "clientside_mark"??
> 

Let me copy and paste the text from the ACL documentation page I
referenced you to read right at the beginning of these many splintered
discussion multi-thread(s).

Note the lines starting "Uses" and "Requires":

"
 acl aclname clientside_mark mark[/mask] ...

  # matches CONNMARK of an accepted connection [fast]
  #
  # mark and mask are unsigned integers (hex, octal, or decimal).
  # If multiple marks are given, then the ACL matches if at least
  # one mark matches.
  #
  # Uses netfilter-conntrack library.
  # Requires building Squid with --enable-linux-netfilter.
  #
  # The client, various intermediaries, and Squid itself may set
  # CONNMARK at various times. The last CONNMARK set wins. This ACL
  # checks the mark present on an accepted connection or set by
  # Squid afterwards, depending on the ACL check timing. This ACL
  # effectively ignores any mark set by other agents after Squid has
  # accepted the connection.
"

HTH
Amos


From squid3 at treenet.co.nz  Wed Oct 17 03:28:56 2018
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Wed, 17 Oct 2018 16:28:56 +1300
Subject: [squid-users] acl "clientside_mark" problem
In-Reply-To: <201810161213.09376.Antony.Stone@squid.open.source.it>
References: <94891910.478463.1539682733632.ref@mail.yahoo.com>
 <94891910.478463.1539682733632@mail.yahoo.com>
 <201810161213.09376.Antony.Stone@squid.open.source.it>
Message-ID: <5ac23786-c07a-c147-d123-622179b2a016@treenet.co.nz>

On 16/10/18 11:13 PM, Antony Stone wrote:
> On Tuesday 16 October 2018 at 11:38:53, morteza omidian wrote:
> 
>> Hi
>> why anybody don't answer me?
> 
> Maybe we don't have experience with precisely what you're trying to do.
> 
> Maybe we're busy with our day jobs.
> 
> Maybe we're confused by the multiple messages you send about approximately the 
> same topic but with different information in them.
> 
> Maybe because we're waiting for your answer to responses sent back to you (eg: 
> http://lists.squid-cache.org/pipermail/squid-users/2018-October/019432.html )
> 

[ leaving that in because it was so well-said.]


>> Please help!
> 
> Please remember this is free, open source software and you are writing to a 
> mailing list and asking for help from volunteers who choose to spend some of 
> their own time answering questions they feel able to help with.
> 
> This is not a commercial support service.
> 
>> I compiled Squid 4.3 with this options in linux
>> server:                    ./configure   --enable-linux-netfilter 
>> --enable-zph-qos when i change my squid configuration file and add this
>> line "acl MY_CLIENTs clientside_mark 0x1" I got  error messages like this:
>> root at linux:~# /usr/local/squid/sbin/squid -k restart
>> 2018/10/16 09:33:42| FATAL: Invalid ACL type 'clientside_mark'2018/10/16
>> 09:33:42| FATAL: Bungled /usr/local/squid/etc/squid.conf line 5: acl
>> CLIENT clientside_mark 12018/10/16 09:33:42| Squid Cache (Version 4.3):
>> Terminated abnormally.CPU Usage: 0.025 seconds = 0.017 user + 0.008
>> sysMaximum Resident Size: 38592 KBPage faults with physical i/o: 0 What is
>> wrong with my configuration??What should  i do?
>> Tanx
> 
> Also, formatting your messages more clearly might help people to understand 
> the question better.
> 
> Oh, yes, and Please Do Not set Reply-to on list messages.
> 


I was intending to keep this between the admin already working on the
issues. But maybe I should explain a few details that I uncovered in the
last few days.


There is something about the way you (morteza) are posting messages to
the mailing list which is causing a bunch of spam to be delivered along
with each of your emails - often this spam arrives first by 1-2 seconds.

These spam are clearly forging the same message-ID your emails use to
*uniquely* identify themselves. As a result it takes longer for your
real message to reach the list. The system has to detect and erase the
spam messages before your message is accepted as having a unique ID from
the leftover emails Yahoo is trying to deliver.

Another side-effect is that our servers rejecting the burst of spam
causes the Yahoo system to become extra suspicious of our servers for a
while. During that time Yahoo is either delaying or rejecting emails
which we are trying to deliver to you and other Yahoo users. That
includes your own request going back out.

So if the person who might have answered your question immediately
happened to be one of those Yahoo users - they are not going to receive
your message for many hours, maybe a whole day. Or sometimes never at all.

If one of us non-Yahoo users do receive it happen to reply quickly
during that slowdown period our message may take hours/days to reach
you, or be rejected entirely by Yahoo and you never see it.
Unfortunately the length of this time is not easily known, and also gets
restarted each time this situation happens.


I hope this clarifies what is going on here. I know the problem is
probably not your fault, but there are some things you can do to help
everyone have a better time communicating. Like, tweak your mailer
settings. Not use the web UI at all if you can. Patience, etc.

Is it probably a good idea also to get your computing device checked for
virus etc infections. I'm not sure how the spam gets your message-ID
value, but the timing of it is a bit suspicious to me.

Amos


From squid3 at treenet.co.nz  Wed Oct 17 03:38:03 2018
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Wed, 17 Oct 2018 16:38:03 +1300
Subject: [squid-users] Delay pools and external acl
In-Reply-To: <CAHaQnLNHRJ-qFCdNOw1P+dpRJX7JPYz7zc6-MQyc==dyHFMgnQ@mail.gmail.com>
References: <CAHaQnLNHRJ-qFCdNOw1P+dpRJX7JPYz7zc6-MQyc==dyHFMgnQ@mail.gmail.com>
Message-ID: <c672c10a-e18f-8f4b-325b-62dd7bd2272f@treenet.co.nz>

On 16/10/18 11:09 AM, Danilo V wrote:
> Hi all,
> 
> Has anyone succeeded applying delay pools on groups from AD?
> 
> I'm using squid 3.5.23 with basic_ldap_auth.
> I initially tried to combine mapping groups with external acl type
> (ext_ldap_group_acl) to delay pools. It's a trap :-(
> 

A trap?

For starters; "group" is an abstract concept buried in the depths of
authentication which has nothing to do with traffic. It is a purely
human scoping idea. Squid knows nothing of any "group".


The external ACL type which handles such complex non-traffic things is
clearly listed in the Squid FAQ (and the 'acl' directive documentation)
as being a "slow" / async ACL type.

Delay pools is also clearly listed as an access control which only works
with "fast" category ACL types.

<https://wiki.squid-cache.org/SquidFaq/SquidAcl#Fast_and_Slow_ACLs>



> After doing more search I found about class 5 and note acl.
> Has anyone a pratical implementation in this scenario?

Yes several admin have done so. But with custom helpers that integrate
with the new annotation system, or the Kerberos helpers that have been
upgraded to integrate as well. Other helpers have not been updated yet.


Your external ACL just needs to supply Squid with a "tag=XX" or
"group=XX " annotation to label the transaction with whichever group
matches.

 # login is required to do group checking...
 acl login proxy_auth REQUIRED
 http_access deny !login


 # the decision to allow the traffic into the proxy does group checks
and adds annotations...

 external_acl_type group %LOGIN ...
 acl some_group external group XX

 http_access allow some_group_check


 # the decision of what pool(s) to apply has to work FAST - so uses the
annotations already present or not present) as its decider:

 acl groupXX note group XX

 # or for older Squid
 acl groupXX note tag XX

 delay_access N allow groupXX


Amos


From squid3 at treenet.co.nz  Wed Oct 17 03:48:13 2018
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Wed, 17 Oct 2018 16:48:13 +1300
Subject: [squid-users] Unable to open youtube.com
In-Reply-To: <1e605630-706c-8979-c76c-e513d83c3c4f@riosoft.com.br>
References: <CAKK+L1cX9Xau0NivY1YobYrT27csuPK-p-FV5UtZUKbf_PH_UQ@mail.gmail.com>
 <1e605630-706c-8979-c76c-e513d83c3c4f@riosoft.com.br>
Message-ID: <d30bfe08-d4a1-0357-09da-e3904e5bc353@treenet.co.nz>

On 17/10/18 6:22 AM, Bruno de Paula Larini wrote:
> 
> Em 16/10/2018 02:46, Timur Lagutenko escreveu:
>> Hello friends,
>>
>> recently I've updated my freebsd gateway.
>> from 11.1 to 11.2.
>> also I've updated squid form 3.5 to 4.1
>> i have no transparency, no ssl-bump/splice etc..
>> simple installation.
>> browser is configured to use proxy.
>> squid configuration is default.

Then Squid interactino wit this traffic is a simple test of whether the
client IP address is within your LAN and then blindly shovel the HTTPS
traffic through.

Problems are limited to routing, MTU/MSS misconfiguration somewhere
(network VPM tunnel?), and problems with the endpoints TLS negotiation
(browser or upstream server).



>> everything works fine except?youtube.com <http://youtube.com/>
>> Browser freezes on "trying to set secure connection", and after gives
>> time-out error.
>> i've also tied to downgrade squid back to 3,5
>> no success.

That downgrade not resolving the issue indicates that it is not Squid
related.

As Bruno suggested, probably a change to the routing or firewall systems
that traffic is going through that appeared with the OS version bump. It
is pretty rare to see on small bumps, but can happen.

Amos


From timur.lagutenko at gmail.com  Wed Oct 17 04:17:53 2018
From: timur.lagutenko at gmail.com (Timur Lagutenko)
Date: Wed, 17 Oct 2018 09:17:53 +0500
Subject: [squid-users] Unable to open youtube.com
In-Reply-To: <d30bfe08-d4a1-0357-09da-e3904e5bc353@treenet.co.nz>
References: <CAKK+L1cX9Xau0NivY1YobYrT27csuPK-p-FV5UtZUKbf_PH_UQ@mail.gmail.com>
 <1e605630-706c-8979-c76c-e513d83c3c4f@riosoft.com.br>
 <d30bfe08-d4a1-0357-09da-e3904e5bc353@treenet.co.nz>
Message-ID: <CAKK+L1eHaiS7Aq-cC=9M3denn74rzyshp2L=gV6msXbXysMHQQ@mail.gmail.com>

i'm sure that the issue is not related to firewall rules.
because if I pass traffic from client IP (using NAT, browser is not
configured to use proxy) it works.
I think it is related to some SSL/TLS lib in the system.
Because today i've tried CLI browser - links.
Launching it directly from gateway (which has direct access to web), i was
able to browse any site in text mode.
Except youtube.
So i guess it is related to some missing ssl lib.
Could you please suggest how can i find all required libs for my squid?

# squid -v
Squid Cache: Version 3.5.28
Service Name: squid

This binary uses OpenSSL 1.0.2p  14 Aug 2018. For legal restrictions on
distribution see https://www.openssl.org/source/license.html

configure options:  '--with-default-user=squid' '--bindir=/usr/local/sbin'
'--sbindir=/usr/local/sbin' '--datadir=/usr/local/etc/squid'
'--libexecdir=/usr/local/libexec/squid' '--localstatedir=/var'
'--sysconfdir=/usr/local/etc/squid' '--with-logdir=/var/log/squid'
'--with-pidfile=/var/run/squid/squid.pid' '--with-swapdir=/var/squid/cache'
'--without-gnutls' '--with-included-ltdl' '--enable-auth'
'--enable-zph-qos' '--enable-build-info' '--enable-loadable-modules'
'--enable-removal-policies=lru heap' '--disable-epoll'
'--disable-linux-netfilter' '--disable-linux-tproxy'
'--disable-translation' '--disable-arch-native' '--disable-eui'
'--enable-cache-digests' '--disable-delay-pools' '--disable-ecap'
'--disable-esi' '--enable-follow-x-forwarded-for' '--without-heimdal-krb5'
'--without-mit-krb5' '--without-gss' '--disable-htcp'
'--disable-icap-client' '--disable-icmp' '--disable-ident-lookups'
'--disable-ipv6' '--enable-kqueue' '--with-large-files'
'--enable-http-violations' '--without-nettle' '--disable-snmp'
'--enable-ssl' '--with-openssl=/usr/local'
'LIBOPENSSL_CFLAGS=-I/usr/local/include' 'LIBOPENSSL_LIBS=-lcrypto -lssl'
'--disable-ssl-crtd' '--disable-stacktraces' '--disable-ipf-transparent'
'--disable-ipfw-transparent' '--disable-pf-transparent'
'--without-nat-devpf' '--enable-forw-via-db' '--enable-wccp'
'--enable-wccpv2' '--enable-auth-basic=DB SMB_LM MSNT-multi-domain NCSA PAM
POP3 RADIUS fake getpwnam' '--enable-auth-digest=file'
'--enable-external-acl-helpers=file_userip time_quota unix_group'
'--enable-auth-negotiate=none' '--enable-auth-ntlm=fake smb_lm'
'--enable-storeio=aufs ufs' '--enable-disk-io=DiskThreads AIO Blocking
IpcIo Mmapped' '--enable-log-daemon-helpers=file'
'--enable-url-rewrite-helpers=fake' '--enable-storeid-rewrite-helpers=file'
'--prefix=/usr/local' '--mandir=/usr/local/man' '--disable-silent-rules'
'--infodir=/usr/local/info/' '--build=amd64-portbld-freebsd11.2'
'build_alias=amd64-portbld-freebsd11.2' 'CC=cc' 'CFLAGS=-O2 -pipe
-fstack-protector -fno-strict-aliasing ' 'LDFLAGS= -pthread
-L/usr/local/lib -lpcreposix -lpcre -Wl,-rpath,/usr/local/lib
-fstack-protector ' 'LIBS=' 'CPPFLAGS=-I/usr/local/include' 'CXX=c++'
'CXXFLAGS=-O2 -pipe -fstack-protector -fno-strict-aliasing
-Wno-unknown-warning-option -Wno-undefined-bool-conversion
-Wno-tautological-undefined-compare -Wno-dynamic-class-memaccess '
'CPP=cpp' --enable-ltdl-convenience

# uname -a
FreeBSD gate.xxxxxx.local 11.2-RELEASE-p4 FreeBSD 11.2-RELEASE-p4 #0: Thu
Sep 27 08:16:24 UTC 2018
root at amd64-builder.daemonology.net:/usr/obj/usr/src/sys/GENERIC
amd64



??, 17 ???. 2018 ?. ? 8:48, Amos Jeffries <squid3 at treenet.co.nz>:

> On 17/10/18 6:22 AM, Bruno de Paula Larini wrote:
> >
> > Em 16/10/2018 02:46, Timur Lagutenko escreveu:
> >> Hello friends,
> >>
> >> recently I've updated my freebsd gateway.
> >> from 11.1 to 11.2.
> >> also I've updated squid form 3.5 to 4.1
> >> i have no transparency, no ssl-bump/splice etc..
> >> simple installation.
> >> browser is configured to use proxy.
> >> squid configuration is default.
>
> Then Squid interactino wit this traffic is a simple test of whether the
> client IP address is within your LAN and then blindly shovel the HTTPS
> traffic through.
>
> Problems are limited to routing, MTU/MSS misconfiguration somewhere
> (network VPM tunnel?), and problems with the endpoints TLS negotiation
> (browser or upstream server).
>
>
>
> >> everything works fine except youtube.com <http://youtube.com/>
> >> Browser freezes on "trying to set secure connection", and after gives
> >> time-out error.
> >> i've also tied to downgrade squid back to 3,5
> >> no success.
>
> That downgrade not resolving the issue indicates that it is not Squid
> related.
>
> As Bruno suggested, probably a change to the routing or firewall systems
> that traffic is going through that appeared with the OS version bump. It
> is pretty rare to see on small bumps, but can happen.
>
> Amos
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20181017/a3fca13b/attachment.htm>

From squid3 at treenet.co.nz  Wed Oct 17 05:06:37 2018
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Wed, 17 Oct 2018 18:06:37 +1300
Subject: [squid-users] Unable to open youtube.com
In-Reply-To: <CAKK+L1eHaiS7Aq-cC=9M3denn74rzyshp2L=gV6msXbXysMHQQ@mail.gmail.com>
References: <CAKK+L1cX9Xau0NivY1YobYrT27csuPK-p-FV5UtZUKbf_PH_UQ@mail.gmail.com>
 <1e605630-706c-8979-c76c-e513d83c3c4f@riosoft.com.br>
 <d30bfe08-d4a1-0357-09da-e3904e5bc353@treenet.co.nz>
 <CAKK+L1eHaiS7Aq-cC=9M3denn74rzyshp2L=gV6msXbXysMHQQ@mail.gmail.com>
Message-ID: <6e7a0b1a-d23d-a507-abb2-3915da215cec@treenet.co.nz>

On 17/10/18 5:17 PM, Timur Lagutenko wrote:
> i'm sure that the issue is not related to firewall rules.
> because if I pass traffic from client IP (using NAT, browser is not
> configured to use proxy) it works.

Ah, you said earlier that you did not have SSL-Bump features enabled.

How are you intercepting the port 443 HTTPS traffic with NAT and
converting it to port 80 or 3128 syntax HTTP for Squid to handle?

Squid cannot MITM the "raw" port 443 TLS without SSL-Bump being configured.


Also since it is a Google service it may not be using TCP port 443 at
all. It may actually be performing their QUIC protocol instead of HTTPS.
That has to be blocked entirely to be sure the proxy is actually
receiving all the relevant traffic.



> I think it is related to some SSL/TLS lib in the system.
> Because today i've tried CLI browser - links.
> Launching it directly from gateway (which has direct access to web), i
> was able to browse any site in text mode.
> Except youtube.
> So i guess it is related to some missing ssl lib.
> Could you please suggest how can i find all required libs for my squid?
> 

If Squid starts without crashing the libs it has been compiled to use
are present on your machine.

If you built it yourself on the same machine, it only uses library
features that machine had at time of the build - so maybe a rebuild is
needed to get access to newer library features.

When it comes to TLS though the library itself is doing the config parse
and setup for crypto things. So Squid does not particularly need to even
be configured to use features the library enables by default. Which
usually includes the current industry-standard ciphers etc.


If Squid accepts your config file and does not produce an ERROR or FATAL
message when you run "squid -k parse" all the libs required to run your
config have been compiled in and loaded.


> # squid -v
> Squid Cache: Version 3.5.28
> Service Name: squid
> 
> This binary uses OpenSSL 1.0.2p? 14 Aug 2018. For legal restrictions on
> distribution see https://www.openssl.org/source/license.html


Your problem may be TLS/1.3 related. OpenSSL 1.0.* only supports a max
of TLS/1.2. Squid-3.5 also only supports OpenSSL 1.0.* library.

AFAIK, Google are one of the organizations heavily pushing TLS changes
and bias their services towards forcing the latest crypto whenever they
can. It is strange that others have not reported issues en-mass, so this
is somewhat unlikely.


Other admin mentioning similar behaviour with YouTube have turned out to
be TLS restrictions that pretty much prohibit the weaker crypto Google
services still allow and only let the very advanced ones (not supported
by their Squid) work.

But also those restrictions were done via SSL-Bump configs. Since you
don't use SSL-Bump it is unlikely to be the same - which leaves us only
with the network/firewall level issues as known things to look at.

Amos


From squid3 at treenet.co.nz  Wed Oct 17 05:07:30 2018
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Wed, 17 Oct 2018 18:07:30 +1300
Subject: [squid-users] deny_info and CONNECT for https request gives SSL
 error
In-Reply-To: <02a57bf8-d124-888a-4a89-80cb0c0aab37@gmail.com>
References: <7f39ba71-493d-1b36-e238-191942e78435@gmail.com>
 <24f73db7-64c5-2cdf-b035-5f5e78937bb9@gmail.com>
 <d3681df9-80a9-c37c-c351-398495814334@measurement-factory.com>
 <3d4f12e2-cfde-20e5-4603-58f3306dfced@gmail.com>
 <407bed3d-46d3-e8f7-f523-5dbdd4d63272@measurement-factory.com>
 <02a57bf8-d124-888a-4a89-80cb0c0aab37@gmail.com>
Message-ID: <33f4fe65-da3d-4c54-cc5c-7ca5350bbb79@treenet.co.nz>

On 17/10/18 3:15 PM, Amish wrote:
> 
> My proposal for would be to add "-n" (nobump) option to deny_info.
> 
> If -n is specified then squid will send 307 directly instead of 200.
> 
> Case 1)
> deny_info http://192.168.1.1/blocked.html denyit
> 
> Return with 200 and bump it (existing behaviour)
> 
> Case 2)
> deny_info 3xx:http://192.168.1.1/blocked.html denyit
> 
> Return with 200 and bump it (existing behaviour)
> 
> Case 3)
> deny_info -n http://192.168.1.1/blocked.html denyit
> 
> Return with 307 Temporary Redirect and Location: header
> 
> Case 4)
> deny_info -n 302:http://192.168.1.1/blocked.html denyit
> 
> Return with 302 Found and Location: header.
> 
> Case 1 and 2 above applicable only for sslbump cases.
> 
> For non-sslbump it already behaves as 3) and 4) above.
> 
> 
> This would not change anything for existing users who want existing
> behaviour.
> 
> But allow people like me to *NOT* bump connection when deny_info is
> activated.
> 

IMO the deny_info is very much the wrong place to be making such
decisions. Its purpose is to supply the *content* of the denial message
itself. Nothing about how that message gets delivered.

If anything this would be an additional ssl-bump option on the port line
to say that traffic is not really being ssl-bump'ed despite the presence
of the ssl-bump setting.

So think about that - why bother putting "ssl-bump" on the port in the
first place if the behaviour that option enables is not wanted to ever
happen?

If your purpose is simply to convert port 443 traffic into HTTP CONNECT
for upstream software to receive there are other far simpler and more
efficient software to be using for that. httptunnel being the popular one.

Amos


From timur.lagutenko at gmail.com  Wed Oct 17 05:37:53 2018
From: timur.lagutenko at gmail.com (Timur Lagutenko)
Date: Wed, 17 Oct 2018 10:37:53 +0500
Subject: [squid-users] Unable to open youtube.com
In-Reply-To: <6e7a0b1a-d23d-a507-abb2-3915da215cec@treenet.co.nz>
References: <CAKK+L1cX9Xau0NivY1YobYrT27csuPK-p-FV5UtZUKbf_PH_UQ@mail.gmail.com>
 <1e605630-706c-8979-c76c-e513d83c3c4f@riosoft.com.br>
 <d30bfe08-d4a1-0357-09da-e3904e5bc353@treenet.co.nz>
 <CAKK+L1eHaiS7Aq-cC=9M3denn74rzyshp2L=gV6msXbXysMHQQ@mail.gmail.com>
 <6e7a0b1a-d23d-a507-abb2-3915da215cec@treenet.co.nz>
Message-ID: <CAKK+L1dW75W1oMoNCmA3nmhAaBRvCMqBObhHBbgV0sqQ407edA@mail.gmail.com>

I will try fresh installation of FreeBSD 11.2-RELEASE
And see how it works.
Maybe something was corrupted during upgrade.

Just FYI please look on my pf.conf and squid.conf:


*# cat /etc/pf.conf*
outif=re0                       #outer interface
inif=re1                        #iner interface
outip="(" $outif ")"            #outer ip
inip="(" $inif ")"              #iner ip
innw=$inif:network              #iner network
inbc=$inif:broadcast            #iner broadcast
bc="255.255.255.255"            #anycast

set skip on lo0
set block-policy drop
scrub in all

nat on $outif from $innw to any -> $outip
rdr on $inif proto {tcp,udp} from $innw to any port 123 -> $inip port 123

block log all

pass from $innw to $innw

# this is my machine client ip
# i have allowed full access form my PC
pass from 192.168.0.104 to any

# this 2 lines passes any traffic from gateway itself
pass from $outip to any
pass from $inip to any

# i don't know why but option "set skip on lo0" doesn't work
# so i additionally pass the whole traffic thru loopback interface
pass on lo0 from any to any


###########################################################################


*# cat /usr/local/etc/squid/squid.conf*
visible_hostname "Squid on freebsd"
acl localnet src 192.168.0.0/20 # RFC1918 possible internal network
shutdown_lifetime 5 seconds
access_log daemon:/var/log/squid/access.log squid

acl SSL_ports port 1-65535
acl Safe_ports port 1-65535
acl CONNECT method CONNECT

http_access deny !Safe_ports
http_access deny CONNECT !SSL_ports

http_access allow localnet manager
http_access deny manager

http_access deny to_localhost

#
# INSERT YOUR OWN RULE(S) HERE TO ALLOW ACCESS FROM YOUR CLIENTS
#


acl baddom dstdomain ardownload.adobe.com agsupdate.adobe.com \
.microsoft.com .windowsupdates.com .oneclient.sfx.ms \
.windows.com .windowsupdate.com

acl bdx dstdom_regex -n -i porn

http_access deny bdx
http_access deny baddom

http_access allow localnet
http_access allow localhost

http_access deny all

http_port 192.168.0.254:3128
# in future i have plans for 3129 port
# for now it simple listening additional port
http_port 192.168.0.254:3129

cache_dir ufs /var/squid/cache 10240 8 16
maximum_object_size 4096 MB
coredump_dir /var/squid/cache

quick_abort_min -1 KB

refresh_pattern ^ftp:           1440    20%     10080
refresh_pattern ^gopher:        1440    0%      1440
refresh_pattern -i (/cgi-bin/) 0        0%      0
refresh_pattern .               0       20%     4320






??, 17 ???. 2018 ?. ? 10:06, Amos Jeffries <squid3 at treenet.co.nz>:

> On 17/10/18 5:17 PM, Timur Lagutenko wrote:
> > i'm sure that the issue is not related to firewall rules.
> > because if I pass traffic from client IP (using NAT, browser is not
> > configured to use proxy) it works.
>
> Ah, you said earlier that you did not have SSL-Bump features enabled.
>
> How are you intercepting the port 443 HTTPS traffic with NAT and
> converting it to port 80 or 3128 syntax HTTP for Squid to handle?
>
> Squid cannot MITM the "raw" port 443 TLS without SSL-Bump being configured.
>
>
> Also since it is a Google service it may not be using TCP port 443 at
> all. It may actually be performing their QUIC protocol instead of HTTPS.
> That has to be blocked entirely to be sure the proxy is actually
> receiving all the relevant traffic.
>
>
>
> > I think it is related to some SSL/TLS lib in the system.
> > Because today i've tried CLI browser - links.
> > Launching it directly from gateway (which has direct access to web), i
> > was able to browse any site in text mode.
> > Except youtube.
> > So i guess it is related to some missing ssl lib.
> > Could you please suggest how can i find all required libs for my squid?
> >
>
> If Squid starts without crashing the libs it has been compiled to use
> are present on your machine.
>
> If you built it yourself on the same machine, it only uses library
> features that machine had at time of the build - so maybe a rebuild is
> needed to get access to newer library features.
>
> When it comes to TLS though the library itself is doing the config parse
> and setup for crypto things. So Squid does not particularly need to even
> be configured to use features the library enables by default. Which
> usually includes the current industry-standard ciphers etc.
>
>
> If Squid accepts your config file and does not produce an ERROR or FATAL
> message when you run "squid -k parse" all the libs required to run your
> config have been compiled in and loaded.
>
>
> > # squid -v
> > Squid Cache: Version 3.5.28
> > Service Name: squid
> >
> > This binary uses OpenSSL 1.0.2p  14 Aug 2018. For legal restrictions on
> > distribution see https://www.openssl.org/source/license.html
>
>
> Your problem may be TLS/1.3 related. OpenSSL 1.0.* only supports a max
> of TLS/1.2. Squid-3.5 also only supports OpenSSL 1.0.* library.
>
> AFAIK, Google are one of the organizations heavily pushing TLS changes
> and bias their services towards forcing the latest crypto whenever they
> can. It is strange that others have not reported issues en-mass, so this
> is somewhat unlikely.
>
>
> Other admin mentioning similar behaviour with YouTube have turned out to
> be TLS restrictions that pretty much prohibit the weaker crypto Google
> services still allow and only let the very advanced ones (not supported
> by their Squid) work.
>
> But also those restrictions were done via SSL-Bump configs. Since you
> don't use SSL-Bump it is unlikely to be the same - which leaves us only
> with the network/firewall level issues as known things to look at.
>
> Amos
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20181017/dfb9c0f7/attachment.htm>

From anon.amish at gmail.com  Wed Oct 17 12:08:11 2018
From: anon.amish at gmail.com (Amish)
Date: Wed, 17 Oct 2018 17:38:11 +0530
Subject: [squid-users] deny_info and CONNECT for https request gives SSL
 error
In-Reply-To: <33f4fe65-da3d-4c54-cc5c-7ca5350bbb79@treenet.co.nz>
References: <7f39ba71-493d-1b36-e238-191942e78435@gmail.com>
 <24f73db7-64c5-2cdf-b035-5f5e78937bb9@gmail.com>
 <d3681df9-80a9-c37c-c351-398495814334@measurement-factory.com>
 <3d4f12e2-cfde-20e5-4603-58f3306dfced@gmail.com>
 <407bed3d-46d3-e8f7-f523-5dbdd4d63272@measurement-factory.com>
 <02a57bf8-d124-888a-4a89-80cb0c0aab37@gmail.com>
 <33f4fe65-da3d-4c54-cc5c-7ca5350bbb79@treenet.co.nz>
Message-ID: <719c488d-abd1-4c6d-e854-383b86f26f8e@gmail.com>

On 17/10/18 10:37 AM, Amos Jeffries wrote:
> On 17/10/18 3:15 PM, Amish wrote:
>> My proposal for would be to add "-n" (nobump) option to deny_info.
>>
>> If -n is specified then squid will send 307 directly instead of 200.
>>
>> Case 1)
>> deny_info http://192.168.1.1/blocked.html denyit
>>
>> Return with 200 and bump it (existing behaviour)
>>
>> Case 2)
>> deny_info 3xx:http://192.168.1.1/blocked.html denyit
>>
>> Return with 200 and bump it (existing behaviour)
>>
>> Case 3)
>> deny_info -n http://192.168.1.1/blocked.html denyit
>>
>> Return with 307 Temporary Redirect and Location: header
>>
>> Case 4)
>> deny_info -n 302:http://192.168.1.1/blocked.html denyit
>>
>> Return with 302 Found and Location: header.
>>
>> Case 1 and 2 above applicable only for sslbump cases.
>>
>> For non-sslbump it already behaves as 3) and 4) above.
>>
>>
>> This would not change anything for existing users who want existing
>> behaviour.
>>
>> But allow people like me to *NOT* bump connection when deny_info is
>> activated.
>>
> IMO the deny_info is very much the wrong place to be making such
> decisions. Its purpose is to supply the *content* of the denial message
> itself. Nothing about how that message gets delivered.
>
> If anything this would be an additional ssl-bump option on the port line
> to say that traffic is not really being ssl-bump'ed despite the presence
> of the ssl-bump setting.

If its additional ssl-bump option then it would become a global thing.

If its on deny_info then for some ACLs I can select to bump and for 
others not. (i.e. gives finer control)

> So think about that - why bother putting "ssl-bump" on the port in the
> first place if the behaviour that option enables is not wanted to ever
> happen?

I need SSL bump because I am bumping few domains and splicing rest.

But for blocked domains I prefer browser to show "Proxy refused the 
connection" instead of SSL error.

Also browsers not respecting code other than 200 or 407 is actually a 
browser bug. Hopefully corrected in future someday.

May be browsers will start supporting 302/307 Location header someday, 
for CONNECT requests too?

(ofcourse I am not RFC expert so I may be completely wrong to interpret it)

In any case thank you for your responses. I have more clarity now.

Regards,

Amish.


From vedavyas.vayalpadu at accenture.com  Wed Oct 17 13:31:21 2018
From: vedavyas.vayalpadu at accenture.com (Vayalpadu, Vedavyas)
Date: Wed, 17 Oct 2018 13:31:21 +0000
Subject: [squid-users] SQUID Proxy - SSL Certificate error
Message-ID: <SN4P114MB02237FBDB0F091C555E5C3518FFF0@SN4P114MB0223.NAMP114.PROD.OUTLOOK.COM>

Hi All,

We have an existing SSL certificate for a WebShop URL. It has an external IP Natted to a Load Balancer and has 2 reverse-squid proxies configured for load balancing.

Now we need to on-board a new URL with same external IP, Same Load Balancers and r-Squid proxy servers ? Is it possible.

I have uploaded the new URL certificate and restarted the squid proxy services, when I try to access the URL iam getting below error, and Certificate error as below.

Can anyone help me on this ?


The hostname in the website's security certificate differs from the website you are trying to visit.
Error Code: DLG_FLAGS_SEC_CERT_CN_INVALID


Certificate error:

[cid:image003.png at 01D4664B.C79973D0]


Please let me know if there are more questions, or call me over skype.

VYAS  (Vedavyas Vayalpadu)
IBM-AIX-UNIX Support
vedavyas.vayalpadu at accenture.com<mailto:vedavyas.vayalpadu at accenture.com>



________________________________

This message is for the designated recipient only and may contain privileged, proprietary, or otherwise confidential information. If you have received it in error, please notify the sender immediately and delete the original. Any other use of the e-mail by you is prohibited. Where allowed by local law, electronic communications with Accenture and its affiliates, including e-mail and instant messaging (including content), may be scanned by our systems for the purposes of information security and assessment of internal compliance with Accenture policy. Your privacy is important to us. Accenture uses your personal data only in compliance with data protection laws. For further information on how Accenture processes your personal data, please see our privacy statement at https://www.accenture.com/us-en/privacy-policy.
______________________________________________________________________________________

www.accenture.com
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20181017/ca8b3145/attachment.htm>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: image003.png
Type: image/png
Size: 12344 bytes
Desc: image003.png
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20181017/ca8b3145/attachment.png>

From morteza1131 at yahoo.com  Wed Oct 17 13:42:38 2018
From: morteza1131 at yahoo.com (morteza omidian)
Date: Wed, 17 Oct 2018 13:42:38 +0000 (UTC)
Subject: [squid-users] squid clientside_mark problem
In-Reply-To: <e8e12993-5119-89dd-7bfd-3e70a54632da@treenet.co.nz>
References: <1397618449.867088.1539530280162.ref@mail.yahoo.com>
 <1397618449.867088.1539530280162@mail.yahoo.com>
 <663781969.438362.1539673607809@mail.yahoo.com>
 <e8e12993-5119-89dd-7bfd-3e70a54632da@treenet.co.nz>
Message-ID: <1143946883.232540.1539783758986@mail.yahoo.com>

HiTank you for your reply.I compiled Squid 4 with these Options and it works!:./configure \
    --enable-linux-netfilter \
    --with-netfilter-conntrack

Tanx.
 

    On Wednesday, October 17, 2018 6:31 AM, Amos Jeffries <squid3 at treenet.co.nz> wrote:
 

 On 16/10/18 8:06 PM, morteza omidian wrote:
> Hi
> Witch options do i have to compile with to enable "clientside_mark"??
> 

Let me copy and paste the text from the ACL documentation page I
referenced you to read right at the beginning of these many splintered
discussion multi-thread(s).

Note the lines starting "Uses" and "Requires":

"
 acl aclname clientside_mark mark[/mask] ...

? # matches CONNMARK of an accepted connection [fast]
? #
? # mark and mask are unsigned integers (hex, octal, or decimal).
? # If multiple marks are given, then the ACL matches if at least
? # one mark matches.
? #
? # Uses netfilter-conntrack library.
? # Requires building Squid with --enable-linux-netfilter.
? #
? # The client, various intermediaries, and Squid itself may set
? # CONNMARK at various times. The last CONNMARK set wins. This ACL
? # checks the mark present on an accepted connection or set by
? # Squid afterwards, depending on the ACL check timing. This ACL
? # effectively ignores any mark set by other agents after Squid has
? # accepted the connection.
"

HTH
Amos
_______________________________________________
squid-users mailing list
squid-users at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-users


   
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20181017/ad9e2bf7/attachment.htm>

From rousskov at measurement-factory.com  Wed Oct 17 14:58:34 2018
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Wed, 17 Oct 2018 08:58:34 -0600
Subject: [squid-users] deny_info and CONNECT for https request gives SSL
 error
In-Reply-To: <02a57bf8-d124-888a-4a89-80cb0c0aab37@gmail.com>
References: <7f39ba71-493d-1b36-e238-191942e78435@gmail.com>
 <24f73db7-64c5-2cdf-b035-5f5e78937bb9@gmail.com>
 <d3681df9-80a9-c37c-c351-398495814334@measurement-factory.com>
 <3d4f12e2-cfde-20e5-4603-58f3306dfced@gmail.com>
 <407bed3d-46d3-e8f7-f523-5dbdd4d63272@measurement-factory.com>
 <02a57bf8-d124-888a-4a89-80cb0c0aab37@gmail.com>
Message-ID: <53bee696-425e-71ad-e28b-d7fa5f42d121@measurement-factory.com>

On 10/16/2018 08:15 PM, Amish wrote:

> http_port 8080 ssl-bump ...
> http_access deny ...
> ssl_bump splice all

> In this case one would expect that squid would not bump the connection
> and return with 307 instead of 200.

FWIW, I do not think "one would expect" can be the driving argument for
designing SslBump error handling:

* Most humans do not "expect" errors at all -- it is natural for humans
to _ignore_ errors when forming initial expectations. This is why error
handling is so difficult and/or error-prone in virtually all areas of
human endeavor.

* Most likely, it is impossible to make SslBump work correctly _and_
match the first-glance expectations of the majority of admins. One has
to either study the feature documentation (to understand how the feature
works) or use working snippets certified by someone they trust (and can
get support from). If Squid documentation is lacking, there are ways to
fix that, of course.

For example, explicit configuration of error handling via ssl_bump rules
have been attempted in the past and rejected because it makes writing
correct rules very difficult and makes the rules themselves unreadable.


> But since it already sent 200 Connection Established - there is no
> returning back.

As I had tried to explain earlier, the steps are not done in the order
you continue to imply. First, Squid discovers the error and only then it
purposefully replies with "200 Connection Established" (to be able to
deliver the error message later, when the first bumped request comes).

The described (and implemented) behavior is what many of the admins
requesting and sponsoring SslBump improvements wanted based on their
deployment experience. Your needs may be different, of course, and the
Squid Project can often satisfy conflicting needs. However, please come
to this discussion not from the "I propose to fix this obvious Squid
bug" point of view, but from the "I propose to satisfy a new use case
while still addressing other reasonable admin needs" point of view.


> My proposal for would be to add "-n" (nobump) option to deny_info.

Whether forwarding errors should lead to bumped connections should not
be determined by deny_info because deny_info does not apply to many
errors. We need a mechanism that can be applied to all errors.

And again, please keep in mind that Squid decides to bump the connection
when it decides to block CONNECT. That happens _before_ Squid looks at
deny_info. Deny_info is an HTTP error adaptation mechanism. It is
orthogonal to whether the error is delivered securely to the user or
dumped on the ground by the browser.

> Also browsers not respecting code other than 200 or 407 is actually a
> browser bug. Hopefully corrected in future someday.

Very true, but based on my interpretation of browser makers' feedback on
the HTTP WG mailing list, I doubt that will happen in the foreseeable
future: Adding a proxy "security context" (in addition to the existing
"insecure" and "origin" contexts) is not a priority for them, especially
since it is only required to accommodate the needs of hated proxies.


HTH,

Alex.


From anon.amish at gmail.com  Thu Oct 18 00:53:32 2018
From: anon.amish at gmail.com (Amish)
Date: Thu, 18 Oct 2018 06:23:32 +0530
Subject: [squid-users] deny_info and CONNECT for https request gives SSL
 error
In-Reply-To: <53bee696-425e-71ad-e28b-d7fa5f42d121@measurement-factory.com>
References: <7f39ba71-493d-1b36-e238-191942e78435@gmail.com>
 <24f73db7-64c5-2cdf-b035-5f5e78937bb9@gmail.com>
 <d3681df9-80a9-c37c-c351-398495814334@measurement-factory.com>
 <3d4f12e2-cfde-20e5-4603-58f3306dfced@gmail.com>
 <407bed3d-46d3-e8f7-f523-5dbdd4d63272@measurement-factory.com>
 <02a57bf8-d124-888a-4a89-80cb0c0aab37@gmail.com>
 <53bee696-425e-71ad-e28b-d7fa5f42d121@measurement-factory.com>
Message-ID: <6da79579-66fb-318d-0e04-2a289441c6e9@gmail.com>

On 17/10/18 8:28 PM, Alex Rousskov wrote:
>
> Very true, but based on my interpretation of browser makers' feedback on
> the HTTP WG mailing list, I doubt that will happen in the foreseeable
> future: Adding a proxy "security context" (in addition to the existing
> "insecure" and "origin" contexts) is not a priority for them, especially
> since it is only required to accommodate the needs of hated proxies.

While I slightly disagree with you for certain things but
I also understand the reasons that you have explained behind it.

So I will leave it at that.

Thank you very much for elaborate replies.

Amish.


From squid3 at treenet.co.nz  Thu Oct 18 02:49:17 2018
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 18 Oct 2018 15:49:17 +1300
Subject: [squid-users] SQUID Proxy - SSL Certificate error
In-Reply-To: <SN4P114MB02237FBDB0F091C555E5C3518FFF0@SN4P114MB0223.NAMP114.PROD.OUTLOOK.COM>
References: <SN4P114MB02237FBDB0F091C555E5C3518FFF0@SN4P114MB0223.NAMP114.PROD.OUTLOOK.COM>
Message-ID: <adee6f8b-d11f-fbfb-c3b8-94582cfba5ab@treenet.co.nz>

On 18/10/18 2:31 AM, Vayalpadu, Vedavyas wrote:
> Hi All,
> 
> We have an existing SSL certificate for a WebShop URL. It has an
> external IP Natted to a Load Balancer and has 2 reverse-squid proxies
> configured for load balancing.
> 
> ?
> 
> Now we need to on-board a new URL with same external IP, Same Load
> Balancers and r-Squid proxy servers ? Is it possible.
> 
> ?
> 
> I have uploaded the new URL certificate and restarted the squid proxy
> services, when I try to access the URL iam getting below error, and
> Certificate error as below.
> 
> ?
> 
> Can anyone help me on this ?
> 

OpenSSL builds of Squid do not support multiple certificates per
listening port.

Squid-4 does support multiple certificates when built with GnuTLS
instead of OpenSSL. This is still an experimental feature though, so YMMV.

Amos


From squid3 at treenet.co.nz  Thu Oct 18 03:34:45 2018
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 18 Oct 2018 16:34:45 +1300
Subject: [squid-users] deny_info and CONNECT for https request gives SSL
 error
In-Reply-To: <719c488d-abd1-4c6d-e854-383b86f26f8e@gmail.com>
References: <7f39ba71-493d-1b36-e238-191942e78435@gmail.com>
 <24f73db7-64c5-2cdf-b035-5f5e78937bb9@gmail.com>
 <d3681df9-80a9-c37c-c351-398495814334@measurement-factory.com>
 <3d4f12e2-cfde-20e5-4603-58f3306dfced@gmail.com>
 <407bed3d-46d3-e8f7-f523-5dbdd4d63272@measurement-factory.com>
 <02a57bf8-d124-888a-4a89-80cb0c0aab37@gmail.com>
 <33f4fe65-da3d-4c54-cc5c-7ca5350bbb79@treenet.co.nz>
 <719c488d-abd1-4c6d-e854-383b86f26f8e@gmail.com>
Message-ID: <b02bb1c3-4b5f-0131-2df7-fa1caba19a20@treenet.co.nz>

On 18/10/18 1:08 AM, Amish wrote:
> On 17/10/18 10:37 AM, Amos Jeffries wrote:
>> On 17/10/18 3:15 PM, Amish wrote:
>>> My proposal for would be to add "-n" (nobump) option to deny_info.
>>>
>>> If -n is specified then squid will send 307 directly instead of 200.
>>>
>>> Case 1)
>>> deny_info http://192.168.1.1/blocked.html denyit
>>>
>>> Return with 200 and bump it (existing behaviour)
>>>
>>> Case 2)
>>> deny_info 3xx:http://192.168.1.1/blocked.html denyit
>>>
>>> Return with 200 and bump it (existing behaviour)
>>>
>>> Case 3)
>>> deny_info -n http://192.168.1.1/blocked.html denyit
>>>
>>> Return with 307 Temporary Redirect and Location: header
>>>
>>> Case 4)
>>> deny_info -n 302:http://192.168.1.1/blocked.html denyit
>>>
>>> Return with 302 Found and Location: header.
>>>
>>> Case 1 and 2 above applicable only for sslbump cases.
>>>
>>> For non-sslbump it already behaves as 3) and 4) above.
>>>
>>>
>>> This would not change anything for existing users who want existing
>>> behaviour.
>>>
>>> But allow people like me to *NOT* bump connection when deny_info is
>>> activated.
>>>
>> IMO the deny_info is very much the wrong place to be making such
>> decisions. Its purpose is to supply the *content* of the denial message
>> itself. Nothing about how that message gets delivered.
>>
>> If anything this would be an additional ssl-bump option on the port line
>> to say that traffic is not really being ssl-bump'ed despite the presence
>> of the ssl-bump setting.
> 
> If its additional ssl-bump option then it would become a global thing.
> 
> If its on deny_info then for some ACLs I can select to bump and for
> others not. (i.e. gives finer control)
> 
>> So think about that - why bother putting "ssl-bump" on the port in the
>> first place if the behaviour that option enables is not wanted to ever
>> happen?
> 
> I need SSL bump because I am bumping few domains and splicing rest.
> 

... and you have a third case:

 Replace TLS ServerHello with a plain-text HTTP 3xx response.

Your intended redirect of the CONNECT tunnel includes traffic where the
client has already sent the 0-RTT TLS clientHello along with the CONNECT.


> But for blocked domains I prefer browser to show "Proxy refused the
> connection" instead of SSL error.

When the proxy is *actually* refusing (403) the connection, that makes
sense. All the other non-200 status responses, including your 3xx
redirect also show that error page generated by the Browser.

Bumping is forced on us by that Browser behaviour. Without the bump your
redirect response has zero chance of working as you obviously want it
to. If you were happy with the Browsers error page you would not be
trying to redirect the CONNECT.


> 
> Also browsers not respecting code other than 200 or 407 is actually a
> browser bug. Hopefully corrected in future someday.
> 

We call it a bug they call it intentionally designed behaviour.

Browsers used to support non-200 responses. That support has actively
and systematically been removed.



> May be browsers will start supporting 302/307 Location header someday,
> for CONNECT requests too?

They used to support 301 until early 2010's. Then it was replaced with
that blanket "Proxy is refusing connections" error message on grounds
that MITM proxies were using 301 to display content to users.


> 
> (ofcourse I am not RFC expert so I may be completely wrong to interpret it)
> 

These issues have nothing to do with the RFCs (which mandate proxy
support), and everything to do with Browser folks interests and design
choices being different from other folks needs and interests.

Amos


From vedavyas.vayalpadu at accenture.com  Thu Oct 18 09:28:51 2018
From: vedavyas.vayalpadu at accenture.com (Vayalpadu, Vedavyas)
Date: Thu, 18 Oct 2018 09:28:51 +0000
Subject: [squid-users] [External] Re: SQUID Proxy - SSL Certificate error
In-Reply-To: <adee6f8b-d11f-fbfb-c3b8-94582cfba5ab@treenet.co.nz>
References: <SN4P114MB02237FBDB0F091C555E5C3518FFF0@SN4P114MB0223.NAMP114.PROD.OUTLOOK.COM>
 <adee6f8b-d11f-fbfb-c3b8-94582cfba5ab@treenet.co.nz>
Message-ID: <SN4P114MB022324409921E2694A322F168FF80@SN4P114MB0223.NAMP114.PROD.OUTLOOK.COM>

Hi Amos,

Both have 2 different certificates, below is the squid configuration. Cache_Peer is same for both the URLs ( Same data server and same name)

Old One: WORKING

#### Reverse Proxy for WebShop UK ####
http_port 10.XX.XX.XX:80 accel vhost defaultsite=webshop.XXX.co.uk name=80013
acl XXXUKwebshop_acl myportname 80013
http_access deny XXXXUKwebshop_acl
deny_info https://webshop.XXX.co.uk XXXXUKwebshop_acl
######
https_port 10.XX.XX.XX:443 accel vhost defaultsite=webshop.XXXX.co.uk cert=/etc/squid/certificate/webshop.XXXXX.co.uk.pfx_both.pem name=80014
cache_peer XXX.XXX.int parent 8070 0 no-query originserver name=XXXXUK_webshops
acl XXXXUKwebshop_acls myportname 80014 dst XXX.XXX.int
cache_peer_access XXXXUK_webshops allow XXXXUKwebshop_acls


New One: NOT-WORKING

#### Reverse Proxy for WebShopUK ####
http_port 10.YY.YY.YY:80 accel vhost defaultsite=webshopuk.YYYY.co.uk name=80013
acl YYYYUKwebshop_acl myportname 80013
http_access deny YYYYUKwebshop_acl
deny_info https://webshopuk.YYYY.co.uk YYYYUKwebshop_acl
######
https_port 10.YY.YY.YY:443 accel vhost defaultsite=webshopuk.YYYY.co.uk cert=/etc/squid/certificate/webshopuk.cert.pem name=80014
cache_peer XXX.XXX.int parent 8070 0 no-query originserver name=XXXXUK_webshops
acl XXXXUKwebshop_acls myportname 80014 dst XXX.XXX.int
cache_peer_access XXXXUK_webshops allow XXXXUKwebshop_acls





-----Original Message-----
From: squid-users <squid-users-bounces at lists.squid-cache.org> On Behalf Of Amos Jeffries
Sent: Thursday, October 18, 2018 8:19 AM
To: squid-users at lists.squid-cache.org
Subject: [External] Re: [squid-users] SQUID Proxy - SSL Certificate error

On 18/10/18 2:31 AM, Vayalpadu, Vedavyas wrote:
> Hi All,
>
> We have an existing SSL certificate for a WebShop URL. It has an
> external IP Natted to a Load Balancer and has 2 reverse-squid proxies
> configured for load balancing.
>
>
>
> Now we need to on-board a new URL with same external IP, Same Load
> Balancers and r-Squid proxy servers ? Is it possible.
>
>
>
> I have uploaded the new URL certificate and restarted the squid proxy
> services, when I try to access the URL iam getting below error, and
> Certificate error as below.
>
>
>
> Can anyone help me on this ?
>

OpenSSL builds of Squid do not support multiple certificates per listening port.

Squid-4 does support multiple certificates when built with GnuTLS instead of OpenSSL. This is still an experimental feature though, so YMMV.

Amos
_______________________________________________
squid-users mailing list
squid-users at lists.squid-cache.org<mailto:squid-users at lists.squid-cache.org>
https://urldefense.proofpoint.com/v2/url?u=http-3A__lists.squid-2Dcache.org_listinfo_squid-2Dusers&d=DwIGaQ&c=eIGjsITfXP_y-DLLX0uEHXJvU8nOHrUK8IrwNKOtkVU&r=tFxAuERmcRdMDY2ODYAvl6bEao1jdCMqbJq7uebMlVg&m=LemWGJCk_zI_BNi880abyP4vFLbKBqpsHNOfwGmWTeg&s=zG-T9PhS7SH74eqtG4DnQIXf0Y-ePm24dqiA7TPV_Ww&e=



  ________________________________

This message is for the designated recipient only and may contain privileged, proprietary, or otherwise confidential information. If you have received it in error, please notify the sender immediately and delete the original. Any other use of the e-mail by you is prohibited. Where allowed by local law, electronic communications with Accenture and its affiliates, including e-mail and instant messaging (including content), may be scanned by our systems for the purposes of information security and assessment of internal compliance with Accenture policy. Your privacy is important to us. Accenture uses your personal data only in compliance with data protection laws. For further information on how Accenture processes your personal data, please see our privacy statement at https://www.accenture.com/us-en/privacy-policy.
______________________________________________________________________________________

www.accenture.com
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20181018/6c82d8b5/attachment.htm>

From timur.lagutenko at gmail.com  Thu Oct 18 13:03:04 2018
From: timur.lagutenko at gmail.com (Timur Lagutenko)
Date: Thu, 18 Oct 2018 18:03:04 +0500
Subject: [squid-users] Unable to open youtube.com
In-Reply-To: <CAKK+L1dW75W1oMoNCmA3nmhAaBRvCMqBObhHBbgV0sqQ407edA@mail.gmail.com>
References: <CAKK+L1cX9Xau0NivY1YobYrT27csuPK-p-FV5UtZUKbf_PH_UQ@mail.gmail.com>
 <1e605630-706c-8979-c76c-e513d83c3c4f@riosoft.com.br>
 <d30bfe08-d4a1-0357-09da-e3904e5bc353@treenet.co.nz>
 <CAKK+L1eHaiS7Aq-cC=9M3denn74rzyshp2L=gV6msXbXysMHQQ@mail.gmail.com>
 <6e7a0b1a-d23d-a507-abb2-3915da215cec@treenet.co.nz>
 <CAKK+L1dW75W1oMoNCmA3nmhAaBRvCMqBObhHBbgV0sqQ407edA@mail.gmail.com>
Message-ID: <CAKK+L1dpNBSrLkTaxZQO_NK-Ccs53Nho=7V98Zxt77zAd9bVLQ@mail.gmail.com>

Dear friends,

I have good news!
i upgraded my openssl package from openssl-1.0.2 up to openssl111 (FreeBSD
11.2)
this action has resolved the issues with youtube.com and some other sites.
now everything works perfect.

thank you very much for your attention!
best regards!


??, 17 ???. 2018 ?. ? 10:37, Timur Lagutenko <timur.lagutenko at gmail.com>:

> I will try fresh installation of FreeBSD 11.2-RELEASE
> And see how it works.
> Maybe something was corrupted during upgrade.
>
> Just FYI please look on my pf.conf and squid.conf:
>
>
> *# cat /etc/pf.conf*
> outif=re0                       #outer interface
> inif=re1                        #iner interface
> outip="(" $outif ")"            #outer ip
> inip="(" $inif ")"              #iner ip
> innw=$inif:network              #iner network
> inbc=$inif:broadcast            #iner broadcast
> bc="255.255.255.255"            #anycast
>
> set skip on lo0
> set block-policy drop
> scrub in all
>
> nat on $outif from $innw to any -> $outip
> rdr on $inif proto {tcp,udp} from $innw to any port 123 -> $inip port 123
>
> block log all
>
> pass from $innw to $innw
>
> # this is my machine client ip
> # i have allowed full access form my PC
> pass from 192.168.0.104 to any
>
> # this 2 lines passes any traffic from gateway itself
> pass from $outip to any
> pass from $inip to any
>
> # i don't know why but option "set skip on lo0" doesn't work
> # so i additionally pass the whole traffic thru loopback interface
> pass on lo0 from any to any
>
>
> ###########################################################################
>
>
> *# cat /usr/local/etc/squid/squid.conf*
> visible_hostname "Squid on freebsd"
> acl localnet src 192.168.0.0/20 # RFC1918 possible internal network
> shutdown_lifetime 5 seconds
> access_log daemon:/var/log/squid/access.log squid
>
> acl SSL_ports port 1-65535
> acl Safe_ports port 1-65535
> acl CONNECT method CONNECT
>
> http_access deny !Safe_ports
> http_access deny CONNECT !SSL_ports
>
> http_access allow localnet manager
> http_access deny manager
>
> http_access deny to_localhost
>
> #
> # INSERT YOUR OWN RULE(S) HERE TO ALLOW ACCESS FROM YOUR CLIENTS
> #
>
>
> acl baddom dstdomain ardownload.adobe.com agsupdate.adobe.com \
> .microsoft.com .windowsupdates.com .oneclient.sfx.ms \
> .windows.com .windowsupdate.com
>
> acl bdx dstdom_regex -n -i porn
>
> http_access deny bdx
> http_access deny baddom
>
> http_access allow localnet
> http_access allow localhost
>
> http_access deny all
>
> http_port 192.168.0.254:3128
> # in future i have plans for 3129 port
> # for now it simple listening additional port
> http_port 192.168.0.254:3129
>
> cache_dir ufs /var/squid/cache 10240 8 16
> maximum_object_size 4096 MB
> coredump_dir /var/squid/cache
>
> quick_abort_min -1 KB
>
> refresh_pattern ^ftp:           1440    20%     10080
> refresh_pattern ^gopher:        1440    0%      1440
> refresh_pattern -i (/cgi-bin/) 0        0%      0
> refresh_pattern .               0       20%     4320
>
>
>
>
>
>
> ??, 17 ???. 2018 ?. ? 10:06, Amos Jeffries <squid3 at treenet.co.nz>:
>
>> On 17/10/18 5:17 PM, Timur Lagutenko wrote:
>> > i'm sure that the issue is not related to firewall rules.
>> > because if I pass traffic from client IP (using NAT, browser is not
>> > configured to use proxy) it works.
>>
>> Ah, you said earlier that you did not have SSL-Bump features enabled.
>>
>> How are you intercepting the port 443 HTTPS traffic with NAT and
>> converting it to port 80 or 3128 syntax HTTP for Squid to handle?
>>
>> Squid cannot MITM the "raw" port 443 TLS without SSL-Bump being
>> configured.
>>
>>
>> Also since it is a Google service it may not be using TCP port 443 at
>> all. It may actually be performing their QUIC protocol instead of HTTPS.
>> That has to be blocked entirely to be sure the proxy is actually
>> receiving all the relevant traffic.
>>
>>
>>
>> > I think it is related to some SSL/TLS lib in the system.
>> > Because today i've tried CLI browser - links.
>> > Launching it directly from gateway (which has direct access to web), i
>> > was able to browse any site in text mode.
>> > Except youtube.
>> > So i guess it is related to some missing ssl lib.
>> > Could you please suggest how can i find all required libs for my squid?
>> >
>>
>> If Squid starts without crashing the libs it has been compiled to use
>> are present on your machine.
>>
>> If you built it yourself on the same machine, it only uses library
>> features that machine had at time of the build - so maybe a rebuild is
>> needed to get access to newer library features.
>>
>> When it comes to TLS though the library itself is doing the config parse
>> and setup for crypto things. So Squid does not particularly need to even
>> be configured to use features the library enables by default. Which
>> usually includes the current industry-standard ciphers etc.
>>
>>
>> If Squid accepts your config file and does not produce an ERROR or FATAL
>> message when you run "squid -k parse" all the libs required to run your
>> config have been compiled in and loaded.
>>
>>
>> > # squid -v
>> > Squid Cache: Version 3.5.28
>> > Service Name: squid
>> >
>> > This binary uses OpenSSL 1.0.2p  14 Aug 2018. For legal restrictions on
>> > distribution see https://www.openssl.org/source/license.html
>>
>>
>> Your problem may be TLS/1.3 related. OpenSSL 1.0.* only supports a max
>> of TLS/1.2. Squid-3.5 also only supports OpenSSL 1.0.* library.
>>
>> AFAIK, Google are one of the organizations heavily pushing TLS changes
>> and bias their services towards forcing the latest crypto whenever they
>> can. It is strange that others have not reported issues en-mass, so this
>> is somewhat unlikely.
>>
>>
>> Other admin mentioning similar behaviour with YouTube have turned out to
>> be TLS restrictions that pretty much prohibit the weaker crypto Google
>> services still allow and only let the very advanced ones (not supported
>> by their Squid) work.
>>
>> But also those restrictions were done via SSL-Bump configs. Since you
>> don't use SSL-Bump it is unlikely to be the same - which leaves us only
>> with the network/firewall level issues as known things to look at.
>>
>> Amos
>> _______________________________________________
>> squid-users mailing list
>> squid-users at lists.squid-cache.org
>> http://lists.squid-cache.org/listinfo/squid-users
>>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20181018/6620f512/attachment.htm>

From timur.lagutenko at gmail.com  Thu Oct 18 13:13:34 2018
From: timur.lagutenko at gmail.com (Timur Lagutenko)
Date: Thu, 18 Oct 2018 18:13:34 +0500
Subject: [squid-users] Unable to open youtube.com
In-Reply-To: <CAKK+L1dpNBSrLkTaxZQO_NK-Ccs53Nho=7V98Zxt77zAd9bVLQ@mail.gmail.com>
References: <CAKK+L1cX9Xau0NivY1YobYrT27csuPK-p-FV5UtZUKbf_PH_UQ@mail.gmail.com>
 <1e605630-706c-8979-c76c-e513d83c3c4f@riosoft.com.br>
 <d30bfe08-d4a1-0357-09da-e3904e5bc353@treenet.co.nz>
 <CAKK+L1eHaiS7Aq-cC=9M3denn74rzyshp2L=gV6msXbXysMHQQ@mail.gmail.com>
 <6e7a0b1a-d23d-a507-abb2-3915da215cec@treenet.co.nz>
 <CAKK+L1dW75W1oMoNCmA3nmhAaBRvCMqBObhHBbgV0sqQ407edA@mail.gmail.com>
 <CAKK+L1dpNBSrLkTaxZQO_NK-Ccs53Nho=7V98Zxt77zAd9bVLQ@mail.gmail.com>
Message-ID: <CAKK+L1czUc_CV6UOX6GMK8tqU22CFQfbS8iBEW8hhfPwd7DV2A@mail.gmail.com>

sorry guys,

i was too hurry.
it doesn't work.
i've just passed thru NAT, i forgot to enable proxy in browser.
so i will dig deeper


??, 18 ???. 2018 ?. ? 18:03, Timur Lagutenko <timur.lagutenko at gmail.com>:

> Dear friends,
>
> I have good news!
> i upgraded my openssl package from openssl-1.0.2 up to openssl111 (FreeBSD
> 11.2)
> this action has resolved the issues with youtube.com and some other sites.
> now everything works perfect.
>
> thank you very much for your attention!
> best regards!
>
>
> ??, 17 ???. 2018 ?. ? 10:37, Timur Lagutenko <timur.lagutenko at gmail.com>:
>
>> I will try fresh installation of FreeBSD 11.2-RELEASE
>> And see how it works.
>> Maybe something was corrupted during upgrade.
>>
>> Just FYI please look on my pf.conf and squid.conf:
>>
>>
>> *# cat /etc/pf.conf*
>> outif=re0                       #outer interface
>> inif=re1                        #iner interface
>> outip="(" $outif ")"            #outer ip
>> inip="(" $inif ")"              #iner ip
>> innw=$inif:network              #iner network
>> inbc=$inif:broadcast            #iner broadcast
>> bc="255.255.255.255"            #anycast
>>
>> set skip on lo0
>> set block-policy drop
>> scrub in all
>>
>> nat on $outif from $innw to any -> $outip
>> rdr on $inif proto {tcp,udp} from $innw to any port 123 -> $inip port 123
>>
>> block log all
>>
>> pass from $innw to $innw
>>
>> # this is my machine client ip
>> # i have allowed full access form my PC
>> pass from 192.168.0.104 to any
>>
>> # this 2 lines passes any traffic from gateway itself
>> pass from $outip to any
>> pass from $inip to any
>>
>> # i don't know why but option "set skip on lo0" doesn't work
>> # so i additionally pass the whole traffic thru loopback interface
>> pass on lo0 from any to any
>>
>>
>>
>> ###########################################################################
>>
>>
>> *# cat /usr/local/etc/squid/squid.conf*
>> visible_hostname "Squid on freebsd"
>> acl localnet src 192.168.0.0/20 # RFC1918 possible internal network
>> shutdown_lifetime 5 seconds
>> access_log daemon:/var/log/squid/access.log squid
>>
>> acl SSL_ports port 1-65535
>> acl Safe_ports port 1-65535
>> acl CONNECT method CONNECT
>>
>> http_access deny !Safe_ports
>> http_access deny CONNECT !SSL_ports
>>
>> http_access allow localnet manager
>> http_access deny manager
>>
>> http_access deny to_localhost
>>
>> #
>> # INSERT YOUR OWN RULE(S) HERE TO ALLOW ACCESS FROM YOUR CLIENTS
>> #
>>
>>
>> acl baddom dstdomain ardownload.adobe.com agsupdate.adobe.com \
>> .microsoft.com .windowsupdates.com .oneclient.sfx.ms \
>> .windows.com .windowsupdate.com
>>
>> acl bdx dstdom_regex -n -i porn
>>
>> http_access deny bdx
>> http_access deny baddom
>>
>> http_access allow localnet
>> http_access allow localhost
>>
>> http_access deny all
>>
>> http_port 192.168.0.254:3128
>> # in future i have plans for 3129 port
>> # for now it simple listening additional port
>> http_port 192.168.0.254:3129
>>
>> cache_dir ufs /var/squid/cache 10240 8 16
>> maximum_object_size 4096 MB
>> coredump_dir /var/squid/cache
>>
>> quick_abort_min -1 KB
>>
>> refresh_pattern ^ftp:           1440    20%     10080
>> refresh_pattern ^gopher:        1440    0%      1440
>> refresh_pattern -i (/cgi-bin/) 0        0%      0
>> refresh_pattern .               0       20%     4320
>>
>>
>>
>>
>>
>>
>> ??, 17 ???. 2018 ?. ? 10:06, Amos Jeffries <squid3 at treenet.co.nz>:
>>
>>> On 17/10/18 5:17 PM, Timur Lagutenko wrote:
>>> > i'm sure that the issue is not related to firewall rules.
>>> > because if I pass traffic from client IP (using NAT, browser is not
>>> > configured to use proxy) it works.
>>>
>>> Ah, you said earlier that you did not have SSL-Bump features enabled.
>>>
>>> How are you intercepting the port 443 HTTPS traffic with NAT and
>>> converting it to port 80 or 3128 syntax HTTP for Squid to handle?
>>>
>>> Squid cannot MITM the "raw" port 443 TLS without SSL-Bump being
>>> configured.
>>>
>>>
>>> Also since it is a Google service it may not be using TCP port 443 at
>>> all. It may actually be performing their QUIC protocol instead of HTTPS.
>>> That has to be blocked entirely to be sure the proxy is actually
>>> receiving all the relevant traffic.
>>>
>>>
>>>
>>> > I think it is related to some SSL/TLS lib in the system.
>>> > Because today i've tried CLI browser - links.
>>> > Launching it directly from gateway (which has direct access to web), i
>>> > was able to browse any site in text mode.
>>> > Except youtube.
>>> > So i guess it is related to some missing ssl lib.
>>> > Could you please suggest how can i find all required libs for my squid?
>>> >
>>>
>>> If Squid starts without crashing the libs it has been compiled to use
>>> are present on your machine.
>>>
>>> If you built it yourself on the same machine, it only uses library
>>> features that machine had at time of the build - so maybe a rebuild is
>>> needed to get access to newer library features.
>>>
>>> When it comes to TLS though the library itself is doing the config parse
>>> and setup for crypto things. So Squid does not particularly need to even
>>> be configured to use features the library enables by default. Which
>>> usually includes the current industry-standard ciphers etc.
>>>
>>>
>>> If Squid accepts your config file and does not produce an ERROR or FATAL
>>> message when you run "squid -k parse" all the libs required to run your
>>> config have been compiled in and loaded.
>>>
>>>
>>> > # squid -v
>>> > Squid Cache: Version 3.5.28
>>> > Service Name: squid
>>> >
>>> > This binary uses OpenSSL 1.0.2p  14 Aug 2018. For legal restrictions on
>>> > distribution see https://www.openssl.org/source/license.html
>>>
>>>
>>> Your problem may be TLS/1.3 related. OpenSSL 1.0.* only supports a max
>>> of TLS/1.2. Squid-3.5 also only supports OpenSSL 1.0.* library.
>>>
>>> AFAIK, Google are one of the organizations heavily pushing TLS changes
>>> and bias their services towards forcing the latest crypto whenever they
>>> can. It is strange that others have not reported issues en-mass, so this
>>> is somewhat unlikely.
>>>
>>>
>>> Other admin mentioning similar behaviour with YouTube have turned out to
>>> be TLS restrictions that pretty much prohibit the weaker crypto Google
>>> services still allow and only let the very advanced ones (not supported
>>> by their Squid) work.
>>>
>>> But also those restrictions were done via SSL-Bump configs. Since you
>>> don't use SSL-Bump it is unlikely to be the same - which leaves us only
>>> with the network/firewall level issues as known things to look at.
>>>
>>> Amos
>>> _______________________________________________
>>> squid-users mailing list
>>> squid-users at lists.squid-cache.org
>>> http://lists.squid-cache.org/listinfo/squid-users
>>>
>>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20181018/08352c1b/attachment.htm>

From danilovt at gmail.com  Thu Oct 18 13:23:40 2018
From: danilovt at gmail.com (Danilo V)
Date: Thu, 18 Oct 2018 10:23:40 -0300
Subject: [squid-users] Delay pools and external acl
Message-ID: <CAHaQnLPf0w+aZAKzRdkoP6WYY4CUZ6XfVkeudh3R1XpujEChzg@mail.gmail.com>

Hi, thanks for your message.

Not working yet. Please check where is my mistake.

I implemented a custom external ACL that checks on active directory via
ldap if a user is member of a particular group. If success returns:
OK group=Internet_Access
Else returns:
ERR

Squid.conf:
http_port 3128
auth_param basic program /usr/lib/squid/basic_ldap_auth -v 3 -b
dc=lab-novo,dc=br,dc=local -R -D
cn=ldap_proxy,ou=gestao_proxy,dc=lab-novo,dc=br,dc=local -w xxx -f
"sAMAccountName=%s" -u uid -P 10.0.0.1:389
acl login proxy_auth REQUIRED
http_access deny !login
external_acl_type group ttl=360 ipv4 %LOGIN /ext_danilo_ldap_group.sh
acl some_group external group Internet_Access
acl groupInternet note group Internet_Access
delay_pools 1
delay_class 1 1
delay_parameters 1 128000/128000
delay_access 1 allow groupInternet

Sqstat confirms that the bandwidth is not being limited.


Date: Wed, 17 Oct 2018 16:38:03 +1300
From: Amos Jeffries <squid3 at treenet.co.nz>
To: squid-users at lists.squid-cache.org
Subject: Re: [squid-users] Delay pools and external acl
Message-ID: <c672c10a-e18f-8f4b-325b-62dd7bd2272f at treenet.co.nz>
Content-Type: text/plain; charset=utf-8

On 16/10/18 11:09 AM, Danilo V wrote:
> Hi all,
>
> Has anyone succeeded applying delay pools on groups from AD?
>
> I'm using squid 3.5.23 with basic_ldap_auth.
> I initially tried to combine mapping groups with external acl type
> (ext_ldap_group_acl) to delay pools. It's a trap :-(
>

A trap?

For starters; "group" is an abstract concept buried in the depths of
authentication which has nothing to do with traffic. It is a purely
human scoping idea. Squid knows nothing of any "group".


The external ACL type which handles such complex non-traffic things is
clearly listed in the Squid FAQ (and the 'acl' directive documentation)
as being a "slow" / async ACL type.

Delay pools is also clearly listed as an access control which only works
with "fast" category ACL types.

<https://wiki.squid-cache.org/SquidFaq/SquidAcl#Fast_and_Slow_ACLs>



> After doing more search I found about class 5 and note acl.
> Has anyone a pratical implementation in this scenario?

Yes several admin have done so. But with custom helpers that integrate
with the new annotation system, or the Kerberos helpers that have been
upgraded to integrate as well. Other helpers have not been updated yet.


Your external ACL just needs to supply Squid with a "tag=XX" or
"group=XX " annotation to label the transaction with whichever group
matches.

 # login is required to do group checking...
 acl login proxy_auth REQUIRED
 http_access deny !login


 # the decision to allow the traffic into the proxy does group checks
and adds annotations...

 external_acl_type group %LOGIN ...
 acl some_group external group XX

 http_access allow some_group_check


 # the decision of what pool(s) to apply has to work FAST - so uses the
annotations already present or not present) as its decider:

 acl groupXX note group XX

 # or for older Squid
 acl groupXX note tag XX

 delay_access N allow groupXX


Amos
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20181018/ec5d9c1b/attachment.htm>

From rousskov at measurement-factory.com  Thu Oct 18 17:40:20 2018
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Thu, 18 Oct 2018 11:40:20 -0600
Subject: [squid-users] Delay pools and external acl
In-Reply-To: <CAHaQnLPf0w+aZAKzRdkoP6WYY4CUZ6XfVkeudh3R1XpujEChzg@mail.gmail.com>
References: <CAHaQnLPf0w+aZAKzRdkoP6WYY4CUZ6XfVkeudh3R1XpujEChzg@mail.gmail.com>
Message-ID: <3ab38245-79bd-2d13-ad9b-41176436cd25@measurement-factory.com>

On 10/18/2018 07:23 AM, Danilo V wrote:

> Please check where is my mistake.
> 
> I implemented a custom external ACL

... but you are not _using_ that new "some_group" ACL. An ACL has no
effect unless it is actually used in some ACL-driven directive. You
probably want to add some_group to your http_access rules.

> http_port 3128
> auth_param basic program ...
> acl login proxy_auth REQUIRED
> http_access deny !login
> external_acl_type group ttl=360 ipv4 %LOGIN /ext_danilo_ldap_group.sh
> acl some_group external group Internet_Access
> acl groupInternet note group Internet_Access
> delay_pools 1
> delay_class 1 1
> delay_parameters 1 128000/128000
> delay_access 1 allow groupInternet

Alex.


> The external ACL type which handles such complex non-traffic things is
> clearly listed in the Squid FAQ (and the 'acl' directive documentation)
> as being a "slow" / async ACL type.
> 
> Delay pools is also clearly listed as an access control which only works
> with "fast" category ACL types.
> 
> <https://wiki.squid-cache.org/SquidFaq/SquidAcl#Fast_and_Slow_ACLs>


> Your external ACL just needs to supply Squid with a "tag=XX" or
> "group=XX " annotation to label the transaction with whichever group
> matches.
> 
> ?# login is required to do group checking...
> ?acl login proxy_auth REQUIRED
> ?http_access deny !login
> 
> 
> ?# the decision to allow the traffic into the proxy does group checks
> and adds annotations...
> 
> ?external_acl_type group %LOGIN ...
> ?acl some_group external group XX
> 
> ?http_access allow some_group_check
> 
> 
> ?# the decision of what pool(s) to apply has to work FAST - so uses the
> annotations already present or not present) as its decider:
> 
> ?acl groupXX note group XX
> 
> ?# or for older Squid
> ?acl groupXX note tag XX
> 
> ?delay_access N allow groupXX
> 
> 
> Amos



From squid3 at treenet.co.nz  Fri Oct 19 03:48:15 2018
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Fri, 19 Oct 2018 16:48:15 +1300
Subject: [squid-users] Unable to open youtube.com
In-Reply-To: <CAKK+L1czUc_CV6UOX6GMK8tqU22CFQfbS8iBEW8hhfPwd7DV2A@mail.gmail.com>
References: <CAKK+L1cX9Xau0NivY1YobYrT27csuPK-p-FV5UtZUKbf_PH_UQ@mail.gmail.com>
 <1e605630-706c-8979-c76c-e513d83c3c4f@riosoft.com.br>
 <d30bfe08-d4a1-0357-09da-e3904e5bc353@treenet.co.nz>
 <CAKK+L1eHaiS7Aq-cC=9M3denn74rzyshp2L=gV6msXbXysMHQQ@mail.gmail.com>
 <6e7a0b1a-d23d-a507-abb2-3915da215cec@treenet.co.nz>
 <CAKK+L1dW75W1oMoNCmA3nmhAaBRvCMqBObhHBbgV0sqQ407edA@mail.gmail.com>
 <CAKK+L1dpNBSrLkTaxZQO_NK-Ccs53Nho=7V98Zxt77zAd9bVLQ@mail.gmail.com>
 <CAKK+L1czUc_CV6UOX6GMK8tqU22CFQfbS8iBEW8hhfPwd7DV2A@mail.gmail.com>
Message-ID: <48c0143c-ce00-0e31-1861-c753053e53c4@treenet.co.nz>

On 19/10/18 2:13 AM, Timur Lagutenko wrote:
> sorry guys,
> 
> i was too hurry.
> it doesn't work.
> i've just passed thru NAT, i forgot to enable proxy in browser.
> so i will dig deeper
> 

FWIW I am not seeing any thing wrong with either the Squid or PF config.
Though my PF knowledge is not great.

Amos


From squid3 at treenet.co.nz  Fri Oct 19 04:06:23 2018
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Fri, 19 Oct 2018 17:06:23 +1300
Subject: [squid-users] [External] Re: SQUID Proxy - SSL Certificate error
In-Reply-To: <SN4P114MB022324409921E2694A322F168FF80@SN4P114MB0223.NAMP114.PROD.OUTLOOK.COM>
References: <SN4P114MB02237FBDB0F091C555E5C3518FFF0@SN4P114MB0223.NAMP114.PROD.OUTLOOK.COM>
 <adee6f8b-d11f-fbfb-c3b8-94582cfba5ab@treenet.co.nz>
 <SN4P114MB022324409921E2694A322F168FF80@SN4P114MB0223.NAMP114.PROD.OUTLOOK.COM>
Message-ID: <3faa1b25-4527-5356-7e2b-94b34db2d080@treenet.co.nz>

On 18/10/18 10:28 PM, Vayalpadu, Vedavyas wrote:
> Hi Amos,
> ?
> Both have 2 different certificates, below is the squid configuration.


What does "both" mean?

 both Squid?
 both peers?
 both PEM?
 both cert chains?
 both load balancers?
 both machines?
 both origin servers?
 both clients?
 both layers?
 both hop / TCP connections?


Amos


From eliezer at ngtech.co.il  Fri Oct 19 05:56:15 2018
From: eliezer at ngtech.co.il (Eliezer Croitoru)
Date: Fri, 19 Oct 2018 08:56:15 +0300
Subject: [squid-users] Is this the next step of SSL encryption? Fwd:
	Encrypted SNI
In-Reply-To: <CANuGLW0HvN7Ok8m_Y+v_BriRE_LAkj=Z8K73UDWDyeGmH_E7=g@mail.gmail.com>
References: <CANuGLW05iB5600kDBTi9W5wuiPE_2SbpyUQyM1qMqCLNTatfCQ@mail.gmail.com>
 <CANuGLW0HvN7Ok8m_Y+v_BriRE_LAkj=Z8K73UDWDyeGmH_E7=g@mail.gmail.com>
Message-ID: <96c68618322bd796be622f588d2f5366@ngtech.co.il>

I have seen this post and I was wondering, is this the next step of SSL 
encryption?

Eliezer

-------- Original Message --------
Subject: Fwd: Encrypted SNI
Date: 2018-10-03 20:40
 From: ????????? ???????? <tolmachev.vlad at gmail.com>
To: nginx at nginx.org
Reply-To: nginx at nginx.org

When nginx will emplemented Encrypted SNI support?
Cloudflare already do this,
https://www.cloudflare.com/ssl/encrypted-sni/
_______________________________________________
nginx mailing list
nginx at nginx.org
http://mailman.nginx.org/mailman/listinfo/nginx

-- 
----
Eliezer Croitoru
Linux System Administrator
Mobile: +972-5-28704261
Email: eliezer at ngtech.co.il


From houhm at greenet.net.cn  Fri Oct 19 07:10:48 2018
From: houhm at greenet.net.cn (houheming)
Date: Fri, 19 Oct 2018 15:10:48 +0800
Subject: [squid-users] https_port Listen on different IP
Message-ID: <001f01d4677a$dce1e5d0$96a5b170$@greenet.net.cn>

Hi squid-users,

         I have to use squid in a very uncommon way, here it is:

1.       Configure squid to be a https tproxy proxy;

2.       For some https server IPs , configure squid to send the client
browser certificates which signed by some specific root CAs;

3.       For other https server IPs, configure squid to send client browser
certificates which signed by default root CA;

 

Here is part of my squid.conf:

 

#default root CA for default https servers

https_port 443 tproxy ssl-bump cert=/usr/local/squid/server.crt
key=/usr/local/squid/server.key version=1 generate-host-certificates=on

#for special server baidu.com

https_port 180.97.33.107:443 tproxy ssl-bump
cert=/usr/local/squid/server_baidu.crt key=/usr/local/squid/server_baidu.key
version=1 generate-host-certificates=on

https_port 180.97.33.108:443 tproxy ssl-bump
cert=/usr/local/squid//server_baidu.crt
key=/usr/local/squid/server_baidu.key version=1
generate-host-certificates=on

 

 

The problem is if I configure squid like this, the line2 and line3 will
never work; If I switch line1 with line2 and line3(put line2 and line3 at
the above of line 1 in squid.conf), then only line2 and line3 will get its
chance to work, line1 will not work. 



 

>From the screenshot , you can see that when I use "squid -k parse" to check
squid.conf, it shows that it will initialize three SSL context, but if you
check the port, only the first SSL context is initialized.

 

Any help will be appreciated!

 

Horise Hou

 

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20181019/252bbcf7/attachment.htm>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: image001.png
Type: image/png
Size: 14789 bytes
Desc: not available
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20181019/252bbcf7/attachment.png>

From anon.amish at gmail.com  Fri Oct 19 08:01:42 2018
From: anon.amish at gmail.com (Amish)
Date: Fri, 19 Oct 2018 13:31:42 +0530
Subject: [squid-users] Is this the next step of SSL encryption? Fwd:
 Encrypted SNI
In-Reply-To: <96c68618322bd796be622f588d2f5366@ngtech.co.il>
References: <CANuGLW05iB5600kDBTi9W5wuiPE_2SbpyUQyM1qMqCLNTatfCQ@mail.gmail.com>
 <CANuGLW0HvN7Ok8m_Y+v_BriRE_LAkj=Z8K73UDWDyeGmH_E7=g@mail.gmail.com>
 <96c68618322bd796be622f588d2f5366@ngtech.co.il>
Message-ID: <d182beef-5d7c-62b2-7338-8a791a9b1617@gmail.com>

Today Cloudflare added more information that Firefox has already added 
the support for ESNI in Nightly.

https://blog.cloudflare.com/encrypt-that-sni-firefox-edition/

Looks like ssl_bump is going to break once ESNI and Encrypted DNS are 
universal. (Ofcourse it may be few years away)

Probably only way out to detect the domain name would be by implementing 
CONNECT proxy instead of transparent one.

I am happy with complete encryption all over but its going to be more 
and more difficult to convince bosses!! :D

Regards,

Amish.

On 19/10/18 11:26 AM, Eliezer Croitoru wrote:
> I have seen this post and I was wondering, is this the next step of 
> SSL encryption?
>
> Eliezer
>
> -------- Original Message --------
> Subject: Fwd: Encrypted SNI
> Date: 2018-10-03 20:40
> From: ????????? ???????? <tolmachev.vlad at gmail.com>
> To: nginx at nginx.org
> Reply-To: nginx at nginx.org
>
> When nginx will emplemented Encrypted SNI support?
> Cloudflare already do this,
> https://www.cloudflare.com/ssl/encrypted-sni/
> _______________________________________________
> nginx mailing list
> nginx at nginx.org
> http://mailman.nginx.org/mailman/listinfo/nginx
>



From jonh.bart14 at gmail.com  Fri Oct 19 11:26:05 2018
From: jonh.bart14 at gmail.com (Jonh Smith)
Date: Fri, 19 Oct 2018 06:26:05 -0500 (CDT)
Subject: [squid-users] FD 222 flags=1: read/write failure: (110) Connection
	timed out
Message-ID: <1539948365524-0.post@n4.nabble.com>

ey Folks, 


Sorry for bad sentence, my English isn't good when i expected.

I have a litle problem with two user with Outlook (using MAC).
Precisly, they using the latest version of Outlook, latest iOS and surfing
in the internet with squid proxy it's not a problem, everything works fine
exepct desynchronization every 30 minutes with their clien outlook. 

I have a message d'error in /var/log/squid/cache.log
local=XX.XX.XX.XX:XXXX remote=XX.XX.XX.XX:XXXXX FD 222 flags=1: read/write
failure: (110) Connection timed out

Configuration Squid : 
auth_param ntlm program  /usr/bin/ntlm_auth
--helper-protocol=squid-2.5-ntlmssp
auth_param ntlm children 150 startup=0 idle=1
auth_param ntlm keep_alive on


auth_param basic program /usr/bin/ntlm_auth
--helper-protocol=squid-2.5-basic
auth_param basic children 5 startup=5 idle=1
auth_param basic realm Proxy_SQUID
auth_param basic credentialsttl 8 hours

I insist in a fact, this problem appear on MAC. For windows everithing works
fine. 

Thanks in advance for your advice, tips and idea. 

Best regards.  






--
Sent from: http://squid-web-proxy-cache.1019090.n4.nabble.com/Squid-Users-f1019091.html


From rousskov at measurement-factory.com  Fri Oct 19 15:51:50 2018
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Fri, 19 Oct 2018 09:51:50 -0600
Subject: [squid-users] Is this the next step of SSL encryption? Fwd:
 Encrypted SNI
In-Reply-To: <d182beef-5d7c-62b2-7338-8a791a9b1617@gmail.com>
References: <CANuGLW05iB5600kDBTi9W5wuiPE_2SbpyUQyM1qMqCLNTatfCQ@mail.gmail.com>
 <CANuGLW0HvN7Ok8m_Y+v_BriRE_LAkj=Z8K73UDWDyeGmH_E7=g@mail.gmail.com>
 <96c68618322bd796be622f588d2f5366@ngtech.co.il>
 <d182beef-5d7c-62b2-7338-8a791a9b1617@gmail.com>
Message-ID: <721e4fba-48b4-1166-06bf-6dd1c243a530@measurement-factory.com>

On 10/19/2018 02:01 AM, Amish wrote:

> Looks like ssl_bump is going to break once ESNI and Encrypted DNS are
> universal. (Ofcourse it may be few years away)
> 
> Probably only way out to detect the domain name would be by implementing
> CONNECT proxy instead of transparent one.

Using forward proxies may not help as much: A CONNECT request that uses
an IP address (instead of a domain name) is pretty much as uninformative
as a TCP connection intercepted by a transparent proxy.

Alex.


From nebeduch at gmail.com  Fri Oct 19 16:42:00 2018
From: nebeduch at gmail.com (Uchenna Nebedum)
Date: Fri, 19 Oct 2018 17:42:00 +0100
Subject: [squid-users] ERROR: NAT/TPROXY lookup failed to locate original
	IPs on local
Message-ID: <CAHgpA++8tPvb7RLopwRAWVFmV2t1SUkqyrogUPpi=3ju0HFOYw@mail.gmail.com>

Good Day All,
i'm new to squid and i have configured squid as an http transparent proxy
with a mikrotik.
the squid server has only a single NIC, so i followed a tutorial and set up
a dst-nat to squid proxy for traffic on port 80,
Chain:dstnat.
Protocol:tcp
Dst-port:80
Action:dst-nat
To Addresses:192.168.2.2 (squid proxy)
To ports:8080
but after setup, only https traffic works correctly,
http traffic client error is "This page isn't working ERR_EMPTY_RESPONSE"
squid access.log is empty then in squid cache.log these are the errors

```
2018/10/19 17:08:54 kid1| ERROR: NF getsockopt(ORIGINAL_DST) failed on
local=192.168.2.2:8080 remote=192.168.1.254:41248 FD 10 flags=33: (92)
Protocol not available
2018/10/19 17:08:54 kid1| ERROR: NAT/TPROXY lookup failed to locate
original IPs on local=192.168.2.2:8080 remote=192.168.1.254:41248 FD 10
flags=33
```
please find below my squid.conf contents

```
acl localnet src 192.168.1.0/24
acl SSL_ports port 443
acl Safe_ports port 80
acl Safe_ports port 21
acl Safe_ports port 443
acl Safe_ports port 70
acl Safe_ports port 210
acl Safe_ports port 1025-65535
acl Safe_ports port 280
acl Safe_ports port 488
acl Safe_ports port 591
acl Safe_ports port 777
acl CONNECT method CONNECT
icap_enable off
icap_service service_req reqmod_precache 1 icap://127.0.0.1:1344/REQMOD
adaptation_service_set class_req service_req
adaptation_access class_req allow all
icap_service service_resp respmod_precache 0 icap://127.0.0.1:1344/RESPMOD
adaptation_service_set class_resp service_resp
adaptation_access class_resp allow all
http_access deny !Safe_ports
http_access deny CONNECT !SSL_ports
http_access allow localhost manager
http_access deny manager
http_access deny to_localhost
http_access allow localnet
http_access allow localhost
http_access allow all
http_port 3128
http_port 8080 transparent
 access_log daemon:/var/log/squid/access.log squid
coredump_dir /var/spool/squid
refresh_pattern ^ftp:        1440    20%    10080
refresh_pattern ^gopher:    1440    0%    1440
refresh_pattern -i (/cgi-bin/|\?) 0    0%    0
refresh_pattern (Release|Packages(.gz)*)$      0       20%     2880
refresh_pattern .        0    20%    4320
```
please any help or correction would be highly appreciated, i am not even
sure if the approach is correct.

-- 
Nebedum Uchenna
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20181019/3464f268/attachment.htm>

From uhlar at fantomas.sk  Fri Oct 19 16:47:17 2018
From: uhlar at fantomas.sk (Matus UHLAR - fantomas)
Date: Fri, 19 Oct 2018 18:47:17 +0200
Subject: [squid-users] Is this the next step of SSL encryption? Fwd:
 Encrypted SNI
In-Reply-To: <721e4fba-48b4-1166-06bf-6dd1c243a530@measurement-factory.com>
References: <CANuGLW05iB5600kDBTi9W5wuiPE_2SbpyUQyM1qMqCLNTatfCQ@mail.gmail.com>
 <CANuGLW0HvN7Ok8m_Y+v_BriRE_LAkj=Z8K73UDWDyeGmH_E7=g@mail.gmail.com>
 <96c68618322bd796be622f588d2f5366@ngtech.co.il>
 <d182beef-5d7c-62b2-7338-8a791a9b1617@gmail.com>
 <721e4fba-48b4-1166-06bf-6dd1c243a530@measurement-factory.com>
Message-ID: <20181019164717.GA28556@fantomas.sk>

>On 10/19/2018 02:01 AM, Amish wrote:
>> Looks like ssl_bump is going to break once ESNI and Encrypted DNS are
>> universal. (Ofcourse it may be few years away)
>>
>> Probably only way out to detect the domain name would be by implementing
>> CONNECT proxy instead of transparent one.

On 19.10.18 09:51, Alex Rousskov wrote:
>Using forward proxies may not help as much: A CONNECT request that uses
>an IP address (instead of a domain name) is pretty much as uninformative
>as a TCP connection intercepted by a transparent proxy.

disabling DNS in the internal network could help that a bit. That way
browser will have to use the proxy to resolve hostnames, so they will be
available to the proxy.

There are networks separated from the internet, where even the DNS may not
be available, so browsers can't rely on DNS being available.

-- 
Matus UHLAR - fantomas, uhlar at fantomas.sk ; http://www.fantomas.sk/
Warning: I wish NOT to receive e-mail advertising to this address.
Varovanie: na tuto adresu chcem NEDOSTAVAT akukolvek reklamnu postu.
Despite the cost of living, have you noticed how popular it remains? 


From Antony.Stone at squid.open.source.it  Fri Oct 19 16:51:47 2018
From: Antony.Stone at squid.open.source.it (Antony Stone)
Date: Fri, 19 Oct 2018 18:51:47 +0200
Subject: [squid-users] ERROR: NAT/TPROXY lookup failed to locate
	original IPs on local
In-Reply-To: <CAHgpA++8tPvb7RLopwRAWVFmV2t1SUkqyrogUPpi=3ju0HFOYw@mail.gmail.com>
References: <CAHgpA++8tPvb7RLopwRAWVFmV2t1SUkqyrogUPpi=3ju0HFOYw@mail.gmail.com>
Message-ID: <201810191851.48192.Antony.Stone@squid.open.source.it>

On Friday 19 October 2018 at 18:42:00, Uchenna Nebedum wrote:

> Good Day All,
> i'm new to squid and i have configured squid as an http transparent proxy
> with a mikrotik.
> the squid server has only a single NIC, so i followed a tutorial and set up
> a dst-nat to squid proxy for traffic on port 80,

Please contact whomever wrote that tutorial and ask them to remove it, because 
this will not work.

> please any help or correction would be highly appreciated, i am not even
> sure if the approach is correct.

https://wiki.squid-cache.org/ConfigExamples/Intercept/LinuxDnat explains that 
what you are trying to do cannot work, because the NAT *must* be done *on* the 
Squid server.

https://wiki.squid-cache.org/ConfigExamples/Intercept/IptablesPolicyRoute 
outlines the correct way to get packets redirected to Squid running as an 
intercepting proxy.


Regards,


Antony.

-- 
Schr?dinger's rule of data integrity: the condition of any backup is unknown 
until a restore is attempted.

                                                   Please reply to the list;
                                                         please *don't* CC me.


From rousskov at measurement-factory.com  Fri Oct 19 17:04:56 2018
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Fri, 19 Oct 2018 11:04:56 -0600
Subject: [squid-users] https_port Listen on different IP
In-Reply-To: <001f01d4677a$dce1e5d0$96a5b170$@greenet.net.cn>
References: <001f01d4677a$dce1e5d0$96a5b170$@greenet.net.cn>
Message-ID: <f83f3f93-0c5a-1bfa-7697-7907ec8d84c2@measurement-factory.com>

On 10/19/2018 01:10 AM, houheming wrote:

> Configure squid to be a https tproxy proxy

Terminology clarification: You are configuring an transparent proxy for
intercepting TLS/HTTPS traffic, not an (explicit) HTTPS proxy.


> configure squid to send the client browser certificates which signed by X

This phrase can be (mis)interpreted many ways:

1. Configure Squid to automatically generate origin server certificates
(signed by a configured CA X) and send them to browsers/clients that go
to those origin servers.

2. Configure Squid to use a configured client certificate (signed by
some CA X) and send it to origin servers that request client certificates.

Does any of the above match what you want to do?


> https_port 443 ...
> https_port 180.97.33.107:443 ...
> https_port 180.97.33.108:443 ...

I am not sure, but perhaps the first https_port line (the one without an
explicit IP address) should come _last_ so that Squid can listen on the
addresses that remain after 180.97.33.107 and 180.97.33.108 are taken by
the other two ports?

Also, if your Squid, when started without "-k parse", reports any
warnings or errors, please share them.

Alex.


From rousskov at measurement-factory.com  Fri Oct 19 17:09:07 2018
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Fri, 19 Oct 2018 11:09:07 -0600
Subject: [squid-users] Is this the next step of SSL encryption? Fwd:
 Encrypted SNI
In-Reply-To: <20181019164717.GA28556@fantomas.sk>
References: <CANuGLW05iB5600kDBTi9W5wuiPE_2SbpyUQyM1qMqCLNTatfCQ@mail.gmail.com>
 <CANuGLW0HvN7Ok8m_Y+v_BriRE_LAkj=Z8K73UDWDyeGmH_E7=g@mail.gmail.com>
 <96c68618322bd796be622f588d2f5366@ngtech.co.il>
 <d182beef-5d7c-62b2-7338-8a791a9b1617@gmail.com>
 <721e4fba-48b4-1166-06bf-6dd1c243a530@measurement-factory.com>
 <20181019164717.GA28556@fantomas.sk>
Message-ID: <44cd65fa-de40-3a44-ac18-ab25ed92bd92@measurement-factory.com>

On 10/19/2018 10:47 AM, Matus UHLAR - fantomas wrote:
>> On 10/19/2018 02:01 AM, Amish wrote:
>>> Looks like ssl_bump is going to break once ESNI and Encrypted DNS are
>>> universal. (Ofcourse it may be few years away)
>>>
>>> Probably only way out to detect the domain name would be by implementing
>>> CONNECT proxy instead of transparent one.

> On 19.10.18 09:51, Alex Rousskov wrote:
>> Using forward proxies may not help as much: A CONNECT request that uses
>> an IP address (instead of a domain name) is pretty much as uninformative
>> as a TCP connection intercepted by a transparent proxy.

> disabling DNS in the internal network could help that a bit.

... until the browser starts using DNS over HTTPS (with a pinned
certificate of the "resolving" HTTPS server)?
 Alex.


From marcus.kool at urlfilterdb.com  Fri Oct 19 17:19:09 2018
From: marcus.kool at urlfilterdb.com (Marcus Kool)
Date: Fri, 19 Oct 2018 14:19:09 -0300
Subject: [squid-users] Is this the next step of SSL encryption? Fwd:
 Encrypted SNI
In-Reply-To: <44cd65fa-de40-3a44-ac18-ab25ed92bd92@measurement-factory.com>
References: <CANuGLW05iB5600kDBTi9W5wuiPE_2SbpyUQyM1qMqCLNTatfCQ@mail.gmail.com>
 <CANuGLW0HvN7Ok8m_Y+v_BriRE_LAkj=Z8K73UDWDyeGmH_E7=g@mail.gmail.com>
 <96c68618322bd796be622f588d2f5366@ngtech.co.il>
 <d182beef-5d7c-62b2-7338-8a791a9b1617@gmail.com>
 <721e4fba-48b4-1166-06bf-6dd1c243a530@measurement-factory.com>
 <20181019164717.GA28556@fantomas.sk>
 <44cd65fa-de40-3a44-ac18-ab25ed92bd92@measurement-factory.com>
Message-ID: <d5c48c49-afaa-ae4a-db15-e38c83644e7c@urlfilterdb.com>



On 19/10/18 14:09, Alex Rousskov wrote:
> On 10/19/2018 10:47 AM, Matus UHLAR - fantomas wrote:
>>> On 10/19/2018 02:01 AM, Amish wrote:
>>>> Looks like ssl_bump is going to break once ESNI and Encrypted DNS are
>>>> universal. (Ofcourse it may be few years away)
>>>>
>>>> Probably only way out to detect the domain name would be by implementing
>>>> CONNECT proxy instead of transparent one.
> 
>> On 19.10.18 09:51, Alex Rousskov wrote:
>>> Using forward proxies may not help as much: A CONNECT request that uses
>>> an IP address (instead of a domain name) is pretty much as uninformative
>>> as a TCP connection intercepted by a transparent proxy.
> 
>> disabling DNS in the internal network could help that a bit.
> 
> ... until the browser starts using DNS over HTTPS (with a pinned
> certificate of the "resolving" HTTPS server)?
>   Alex.

It is relatively easy to block DNS over HTTPS and I think there will be demand for that.
And I predict that Squid will have a feature to selectively block connections with ESNI to force clients to use the plain text SNI.

Marcus


From rafael.akchurin at diladele.com  Fri Oct 19 17:30:00 2018
From: rafael.akchurin at diladele.com (Rafael Akchurin)
Date: Fri, 19 Oct 2018 17:30:00 +0000
Subject: [squid-users] ERROR: NAT/TPROXY lookup failed to locate
	original	IPs on local
In-Reply-To: <CAHgpA++8tPvb7RLopwRAWVFmV2t1SUkqyrogUPpi=3ju0HFOYw@mail.gmail.com>
References: <CAHgpA++8tPvb7RLopwRAWVFmV2t1SUkqyrogUPpi=3ju0HFOYw@mail.gmail.com>
Message-ID: <AM0PR04MB47539225C3F0C19097401BA98FF90@AM0PR04MB4753.eurprd04.prod.outlook.com>

Hello Uchenna,

May be this policy based routing with Mikrotik tutorial will be of any use
See https://docs.diladele.com/tutorials/mikrotik_transparent_squid/index.html

Best regards,
Rafael Akchurin
Diladele B.V.


From: squid-users <squid-users-bounces at lists.squid-cache.org> On Behalf Of Uchenna Nebedum
Sent: Friday, 19 October 2018 18:42
To: squid-users at lists.squid-cache.org
Subject: [squid-users] ERROR: NAT/TPROXY lookup failed to locate original IPs on local

Good Day All,
i'm new to squid and i have configured squid as an http transparent proxy with a mikrotik.
the squid server has only a single NIC, so i followed a tutorial and set up a dst-nat to squid proxy for traffic on port 80,
Chain:dstnat.
Protocol:tcp
Dst-port:80
Action:dst-nat
To Addresses:192.168.2.2 (squid proxy)
To ports:8080
but after setup, only https traffic works correctly,
http traffic client error is "This page isn't working ERR_EMPTY_RESPONSE"
squid access.log is empty then in squid cache.log these are the errors

```
2018/10/19 17:08:54 kid1| ERROR: NF getsockopt(ORIGINAL_DST) failed on local=192.168.2.2:8080<http://192.168.2.2:8080> remote=192.168.1.254:41248<http://192.168.1.254:41248> FD 10 flags=33: (92) Protocol not available
2018/10/19 17:08:54 kid1| ERROR: NAT/TPROXY lookup failed to locate original IPs on local=192.168.2.2:8080<http://192.168.2.2:8080> remote=192.168.1.254:41248<http://192.168.1.254:41248> FD 10 flags=33
```
please find below my squid.conf contents

```
acl localnet src 192.168.1.0/24<http://192.168.1.0/24>
acl SSL_ports port 443
acl Safe_ports port 80
acl Safe_ports port 21
acl Safe_ports port 443
acl Safe_ports port 70
acl Safe_ports port 210
acl Safe_ports port 1025-65535
acl Safe_ports port 280
acl Safe_ports port 488
acl Safe_ports port 591
acl Safe_ports port 777
acl CONNECT method CONNECT
icap_enable off
icap_service service_req reqmod_precache 1 icap://127.0.0.1:1344/REQMOD<http://127.0.0.1:1344/REQMOD>
adaptation_service_set class_req service_req
adaptation_access class_req allow all
icap_service service_resp respmod_precache 0 icap://127.0.0.1:1344/RESPMOD<http://127.0.0.1:1344/RESPMOD>
adaptation_service_set class_resp service_resp
adaptation_access class_resp allow all
http_access deny !Safe_ports
http_access deny CONNECT !SSL_ports
http_access allow localhost manager
http_access deny manager
http_access deny to_localhost
http_access allow localnet
http_access allow localhost
http_access allow all
http_port 3128
http_port 8080 transparent
 access_log daemon:/var/log/squid/access.log squid
coredump_dir /var/spool/squid
refresh_pattern ^ftp:        1440    20%    10080
refresh_pattern ^gopher:    1440    0%    1440
refresh_pattern -i (/cgi-bin/|\?) 0    0%    0
refresh_pattern (Release|Packages(.gz)*)$      0       20%     2880
refresh_pattern .        0    20%    4320
```
please any help or correction would be highly appreciated, i am not even sure if the approach is correct.

--
Nebedum Uchenna
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20181019/6b0ef11f/attachment.htm>

From nebeduch at gmail.com  Fri Oct 19 18:17:21 2018
From: nebeduch at gmail.com (Uchenna Nebedum)
Date: Fri, 19 Oct 2018 19:17:21 +0100
Subject: [squid-users] ERROR: NAT/TPROXY lookup failed to locate
 original IPs on local
In-Reply-To: <AM0PR04MB47539225C3F0C19097401BA98FF90@AM0PR04MB4753.eurprd04.prod.outlook.com>
References: <CAHgpA++8tPvb7RLopwRAWVFmV2t1SUkqyrogUPpi=3ju0HFOYw@mail.gmail.com>
 <AM0PR04MB47539225C3F0C19097401BA98FF90@AM0PR04MB4753.eurprd04.prod.outlook.com>
Message-ID: <CAHgpA+JGgGmHxpJZtSvpaHzpH7v2ZvHPXS_y5uCWp16km5h8Aw@mail.gmail.com>

Thanks a lot Rafael, I've gone through the documentation it looks to be
very promising, one reservation i have is I want to use greasyspoon for
icap and i see ecap is implemented already. I intend to install everything
as suggested on the link, then after this change squid.conf to remove ecap
connection.
Please, I hope this will work?

Thanks a lot again for the link, it really explained everything well enough
for a beginner.

Uchenna Nebedum

On Fri, Oct 19, 2018, 18:30 Rafael Akchurin <rafael.akchurin at diladele.com>
wrote:

> Hello Uchenna,
>
>
>
> May be this policy based routing with Mikrotik tutorial will be of any use
>
> See
> https://docs.diladele.com/tutorials/mikrotik_transparent_squid/index.html
>
>
>
> Best regards,
>
> Rafael Akchurin
>
> Diladele B.V.
>
>
>
>
>
> *From:* squid-users <squid-users-bounces at lists.squid-cache.org> *On
> Behalf Of *Uchenna Nebedum
> *Sent:* Friday, 19 October 2018 18:42
> *To:* squid-users at lists.squid-cache.org
> *Subject:* [squid-users] ERROR: NAT/TPROXY lookup failed to locate
> original IPs on local
>
>
>
> Good Day All,
>
> i'm new to squid and i have configured squid as an http transparent proxy
> with a mikrotik.
>
> the squid server has only a single NIC, so i followed a tutorial and set
> up a dst-nat to squid proxy for traffic on port 80,
>
> Chain:dstnat.
>
> Protocol:tcp
>
> Dst-port:80
>
> Action:dst-nat
>
> To Addresses:192.168.2.2 (squid proxy)
>
> To ports:8080
>
> but after setup, only https traffic works correctly,
>
> http traffic client error is "This page isn't working ERR_EMPTY_RESPONSE"
>
> squid access.log is empty then in squid cache.log these are the errors
>
>
>
> ```
>
> 2018/10/19 17:08:54 kid1| ERROR: NF getsockopt(ORIGINAL_DST) failed on
> local=192.168.2.2:8080 remote=192.168.1.254:41248 FD 10 flags=33: (92)
> Protocol not available
> 2018/10/19 17:08:54 kid1| ERROR: NAT/TPROXY lookup failed to locate
> original IPs on local=192.168.2.2:8080 remote=192.168.1.254:41248 FD 10
> flags=33
>
> ```
>
> please find below my squid.conf contents
>
>
>
> ```
>
> acl localnet src 192.168.1.0/24
> acl SSL_ports port 443
> acl Safe_ports port 80
> acl Safe_ports port 21
> acl Safe_ports port 443
> acl Safe_ports port 70
> acl Safe_ports port 210
> acl Safe_ports port 1025-65535
> acl Safe_ports port 280
> acl Safe_ports port 488
> acl Safe_ports port 591
> acl Safe_ports port 777
> acl CONNECT method CONNECT
> icap_enable off
> icap_service service_req reqmod_precache 1 icap://127.0.0.1:1344/REQMOD
> adaptation_service_set class_req service_req
> adaptation_access class_req allow all
> icap_service service_resp respmod_precache 0 icap://127.0.0.1:1344/RESPMOD
> adaptation_service_set class_resp service_resp
> adaptation_access class_resp allow all
> http_access deny !Safe_ports
> http_access deny CONNECT !SSL_ports
> http_access allow localhost manager
> http_access deny manager
> http_access deny to_localhost
> http_access allow localnet
> http_access allow localhost
> http_access allow all
> http_port 3128
> http_port 8080 transparent
>  access_log daemon:/var/log/squid/access.log squid
> coredump_dir /var/spool/squid
> refresh_pattern ^ftp:        1440    20%    10080
> refresh_pattern ^gopher:    1440    0%    1440
> refresh_pattern -i (/cgi-bin/|\?) 0    0%    0
> refresh_pattern (Release|Packages(.gz)*)$      0       20%     2880
> refresh_pattern .        0    20%    4320
>
> ```
>
> please any help or correction would be highly appreciated, i am not even
> sure if the approach is correct.
>
>
> --
>
> Nebedum Uchenna
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20181019/ebf16182/attachment.htm>

From rafael.akchurin at diladele.com  Fri Oct 19 19:09:08 2018
From: rafael.akchurin at diladele.com (Rafael Akchurin)
Date: Fri, 19 Oct 2018 19:09:08 +0000
Subject: [squid-users] ERROR: NAT/TPROXY lookup failed to locate
 original IPs on local
In-Reply-To: <CAHgpA+JGgGmHxpJZtSvpaHzpH7v2ZvHPXS_y5uCWp16km5h8Aw@mail.gmail.com>
References: <CAHgpA++8tPvb7RLopwRAWVFmV2t1SUkqyrogUPpi=3ju0HFOYw@mail.gmail.com>
 <AM0PR04MB47539225C3F0C19097401BA98FF90@AM0PR04MB4753.eurprd04.prod.outlook.com>
 <CAHgpA+JGgGmHxpJZtSvpaHzpH7v2ZvHPXS_y5uCWp16km5h8Aw@mail.gmail.com>
Message-ID: <AM0PR04MB4753D02F16BD52B1C0BB8E718FF90@AM0PR04MB4753.eurprd04.prod.outlook.com>

Yes you can use any ICAP/eCAP server you like, just adjust the docs as required and that is it.

From: Uchenna Nebedum <nebeduch at gmail.com>
Sent: Friday, 19 October 2018 20:17
To: Rafael Akchurin <rafael.akchurin at diladele.com>
Cc: squid-users at lists.squid-cache.org
Subject: Re: [squid-users] ERROR: NAT/TPROXY lookup failed to locate original IPs on local

Thanks a lot Rafael, I've gone through the documentation it looks to be very promising, one reservation i have is I want to use greasyspoon for icap and i see ecap is implemented already. I intend to install everything as suggested on the link, then after this change squid.conf to remove ecap connection.
Please, I hope this will work?

Thanks a lot again for the link, it really explained everything well enough for a beginner.
Uchenna Nebedum

On Fri, Oct 19, 2018, 18:30 Rafael Akchurin <rafael.akchurin at diladele.com<mailto:rafael.akchurin at diladele.com>> wrote:
Hello Uchenna,

May be this policy based routing with Mikrotik tutorial will be of any use
See https://docs.diladele.com/tutorials/mikrotik_transparent_squid/index.html

Best regards,
Rafael Akchurin
Diladele B.V.


From: squid-users <squid-users-bounces at lists.squid-cache.org<mailto:squid-users-bounces at lists.squid-cache.org>> On Behalf Of Uchenna Nebedum
Sent: Friday, 19 October 2018 18:42
To: squid-users at lists.squid-cache.org<mailto:squid-users at lists.squid-cache.org>
Subject: [squid-users] ERROR: NAT/TPROXY lookup failed to locate original IPs on local

Good Day All,
i'm new to squid and i have configured squid as an http transparent proxy with a mikrotik.
the squid server has only a single NIC, so i followed a tutorial and set up a dst-nat to squid proxy for traffic on port 80,
Chain:dstnat.
Protocol:tcp
Dst-port:80
Action:dst-nat
To Addresses:192.168.2.2 (squid proxy)
To ports:8080
but after setup, only https traffic works correctly,
http traffic client error is "This page isn't working ERR_EMPTY_RESPONSE"
squid access.log is empty then in squid cache.log these are the errors

```
2018/10/19 17:08:54 kid1| ERROR: NF getsockopt(ORIGINAL_DST) failed on local=192.168.2.2:8080<http://192.168.2.2:8080> remote=192.168.1.254:41248<http://192.168.1.254:41248> FD 10 flags=33: (92) Protocol not available
2018/10/19 17:08:54 kid1| ERROR: NAT/TPROXY lookup failed to locate original IPs on local=192.168.2.2:8080<http://192.168.2.2:8080> remote=192.168.1.254:41248<http://192.168.1.254:41248> FD 10 flags=33
```
please find below my squid.conf contents

```
acl localnet src 192.168.1.0/24<http://192.168.1.0/24>
acl SSL_ports port 443
acl Safe_ports port 80
acl Safe_ports port 21
acl Safe_ports port 443
acl Safe_ports port 70
acl Safe_ports port 210
acl Safe_ports port 1025-65535
acl Safe_ports port 280
acl Safe_ports port 488
acl Safe_ports port 591
acl Safe_ports port 777
acl CONNECT method CONNECT
icap_enable off
icap_service service_req reqmod_precache 1 icap://127.0.0.1:1344/REQMOD<http://127.0.0.1:1344/REQMOD>
adaptation_service_set class_req service_req
adaptation_access class_req allow all
icap_service service_resp respmod_precache 0 icap://127.0.0.1:1344/RESPMOD<http://127.0.0.1:1344/RESPMOD>
adaptation_service_set class_resp service_resp
adaptation_access class_resp allow all
http_access deny !Safe_ports
http_access deny CONNECT !SSL_ports
http_access allow localhost manager
http_access deny manager
http_access deny to_localhost
http_access allow localnet
http_access allow localhost
http_access allow all
http_port 3128
http_port 8080 transparent
 access_log daemon:/var/log/squid/access.log squid
coredump_dir /var/spool/squid
refresh_pattern ^ftp:        1440    20%    10080
refresh_pattern ^gopher:    1440    0%    1440
refresh_pattern -i (/cgi-bin/|\?) 0    0%    0
refresh_pattern (Release|Packages(.gz)*)$      0       20%     2880
refresh_pattern .        0    20%    4320
```
please any help or correction would be highly appreciated, i am not even sure if the approach is correct.

--
Nebedum Uchenna
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20181019/9942b230/attachment.htm>

From acrow at integrafin.co.uk  Fri Oct 19 19:28:46 2018
From: acrow at integrafin.co.uk (Alex Crow)
Date: Fri, 19 Oct 2018 20:28:46 +0100
Subject: [squid-users] Is this the next step of SSL encryption? Fwd:
 Encrypted SNI
In-Reply-To: <d5c48c49-afaa-ae4a-db15-e38c83644e7c@urlfilterdb.com>
References: <CANuGLW05iB5600kDBTi9W5wuiPE_2SbpyUQyM1qMqCLNTatfCQ@mail.gmail.com>
 <CANuGLW0HvN7Ok8m_Y+v_BriRE_LAkj=Z8K73UDWDyeGmH_E7=g@mail.gmail.com>
 <96c68618322bd796be622f588d2f5366@ngtech.co.il>
 <d182beef-5d7c-62b2-7338-8a791a9b1617@gmail.com>
 <721e4fba-48b4-1166-06bf-6dd1c243a530@measurement-factory.com>
 <20181019164717.GA28556@fantomas.sk>
 <44cd65fa-de40-3a44-ac18-ab25ed92bd92@measurement-factory.com>
 <d5c48c49-afaa-ae4a-db15-e38c83644e7c@urlfilterdb.com>
Message-ID: <951d7879-1f12-3a33-c0d2-e481f25c5d0f@integrafin.co.uk>


>> ... until the browser starts using DNS over HTTPS (with a pinned
>> certificate of the "resolving" HTTPS server)?
>> ? Alex.
>
> It is relatively easy to block DNS over HTTPS and I think there will 
> be demand for that.
> And I predict that Squid will have a feature to selectively block 
> connections with ESNI to force clients to use the plain text SNI.
>
> Marcus
>

I can still see the endpoint security companies will be raking it in. 
Any of those fallbacks could be disabled by the browsers.

We're going to have to make sure that the endpoint solution is able to 
see all content before it is rendered or interpreted in the browser too.

The problem is that the whole SSL/TLS trust management system is 
fundamentally broken and I can't see that changing soon. PGP's model was 
great in theory (web of trust) but most people simply don't care who 
sends them what and can't be bothered to complicate their lives any 
more. And why should they? If their bank site works, Farcebook works and 
Hotmail works, why worry? We've built an entire social structure on two 
basic principles - "if I've done nothing wrong..." and "who'd be 
interested in my data?".



--
This message is intended only for the addressee and may contain
confidential information. Unless you are that person, you may not
disclose its contents or use it in any way and are requested to delete
the message along with any attachments and notify us immediately.
This email is not intended to, nor should it be taken to, constitute advice.
The information provided is correct to our knowledge & belief and must not
be used as a substitute for obtaining tax, regulatory, investment, legal or
any other appropriate advice.

"Transact" is operated by Integrated Financial Arrangements Ltd.
29 Clement's Lane, London EC4N 7AE. Tel: (020) 7608 4900 Fax: (020) 7608 5300.
(Registered office: as above; Registered in England and Wales under
number: 3727592). Authorised and regulated by the Financial Conduct
Authority (entered on the Financial Services Register; no. 190856).


From squid3 at treenet.co.nz  Sat Oct 20 04:10:01 2018
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sat, 20 Oct 2018 17:10:01 +1300
Subject: [squid-users] https_port Listen on different IP
In-Reply-To: <f83f3f93-0c5a-1bfa-7697-7907ec8d84c2@measurement-factory.com>
References: <001f01d4677a$dce1e5d0$96a5b170$@greenet.net.cn>
 <f83f3f93-0c5a-1bfa-7697-7907ec8d84c2@measurement-factory.com>
Message-ID: <a416cc65-3f97-46b0-dcd1-b14cfa0b32ba@treenet.co.nz>

On 20/10/18 6:04 AM, Alex Rousskov wrote:
> On 10/19/2018 01:10 AM, houheming wrote:
>> https_port 443 ...
>> https_port 180.97.33.107:443 ...
>> https_port 180.97.33.108:443 ...
> 
> I am not sure, but perhaps the first https_port line (the one without an
> explicit IP address) should come _last_ so that Squid can listen on the
> addresses that remain after 180.97.33.107 and 180.97.33.108 are taken by
> the other two ports?

I think that is what was meant by "If I switch line1 with line2 and
line3 ..., then only line2 and line3 will get its chance to work, line1
will not work. "

The problem is that TCP does not permit any IP:port combination to have
two simultaneous listening sockets with different parameters. These
configuration lines differ in both address and protocol they are receiving.


houheming:
 you have to use different ports to receive the traffic into Squid.

Since you are using TPROXY there is no requirement for the proxy
listening port to be 443. Squid can listen on any port you want.

This problem should disappear if you set the wildcard port to another
number and update the TPROXY rule which is sending traffic to it.

Amos


From squid3 at treenet.co.nz  Sat Oct 20 04:22:51 2018
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sat, 20 Oct 2018 17:22:51 +1300
Subject: [squid-users] FD 222 flags=1: read/write failure: (110)
 Connection timed out
In-Reply-To: <1539948365524-0.post@n4.nabble.com>
References: <1539948365524-0.post@n4.nabble.com>
Message-ID: <65107b1c-e429-ed64-3013-0a98c2a04f91@treenet.co.nz>

On 20/10/18 12:26 AM, Jonh Smith wrote:
> ey Folks, 
> 
> 
> Sorry for bad sentence, my English isn't good when i expected.
> 
> I have a litle problem with two user with Outlook (using MAC).
> Precisly, they using the latest version of Outlook, latest iOS and surfing
> in the internet with squid proxy it's not a problem, everything works fine
> exepct desynchronization every 30 minutes with their clien outlook. 
> 
> I have a message d'error in /var/log/squid/cache.log
> local=XX.XX.XX.XX:XXXX remote=XX.XX.XX.XX:XXXXX FD 222 flags=1: read/write
> failure: (110) Connection timed out

This has nothing to do with authentication.

The TCP stack, NAT system, or some other software on one of the machines
between the local XX.XX.XX.XX:XXXX and remote XX.XX.XX.XX:XXXXX has
timed out its records about that TCP connection and closed it.

Not using a connection for 30min sounds like a long timeout. Normal TCP
timeouts are 5min. Each active TCP connection is using up many network
resources just by staying open so letting them stay around for long
periods is a bad idea.

Amos


From alessio.troiano at leonardocompany.com  Sat Oct 20 06:00:13 2018
From: alessio.troiano at leonardocompany.com (Troiano Alessio)
Date: Sat, 20 Oct 2018 06:00:13 +0000
Subject: [squid-users] R: Is this the next step of SSL encryption? Fwd:
 Encrypted SNI
In-Reply-To: <13423_1539977335_5BCA3077_13423_3866_1_951d7879-1f12-3a33-c0d2-e481f25c5d0f@integrafin.co.uk>
References: <CANuGLW05iB5600kDBTi9W5wuiPE_2SbpyUQyM1qMqCLNTatfCQ@mail.gmail.com>
 <CANuGLW0HvN7Ok8m_Y+v_BriRE_LAkj=Z8K73UDWDyeGmH_E7=g@mail.gmail.com>
 <96c68618322bd796be622f588d2f5366@ngtech.co.il>
 <d182beef-5d7c-62b2-7338-8a791a9b1617@gmail.com>
 <721e4fba-48b4-1166-06bf-6dd1c243a530@measurement-factory.com>
 <20181019164717.GA28556@fantomas.sk>
 <44cd65fa-de40-3a44-ac18-ab25ed92bd92@measurement-factory.com>
 <d5c48c49-afaa-ae4a-db15-e38c83644e7c@urlfilterdb.com>
 <13423_1539977335_5BCA3077_13423_3866_1_951d7879-1f12-3a33-c0d2-e481f25c5d0f@integrafin.co.uk>
Message-ID: <aa95d90f6d6f4e8d8abeef705d3b0d92@ocgepvsw3101.ocr.priv>

Sorry but, I'm loosing something...
Also if client will use ESNI, the server should response with a certificate that will be in clear and has all server information, like alias... So is it possible to know what is the resource the client is looking for. Only with wildcard certificate you can't.

Il presente messaggio e-mail e ogni suo allegato devono intendersi indirizzati esclusivamente al destinatario indicato e considerarsi dal contenuto strettamente riservato e confidenziale. Se non siete l'effettivo destinatario o avete ricevuto il messaggio e-mail per errore, siete pregati di avvertire immediatamente il mittente e di cancellare il suddetto messaggio e ogni suo allegato dal vostro sistema informatico. Qualsiasi utilizzo, diffusione, copia o archiviazione del presente messaggio da parte di chi non ne ? il destinatario ? strettamente proibito e pu? dar luogo a responsabilit? di carattere civile e penale punibili ai sensi di legge.
Questa e-mail ha valore legale solo se firmata digitalmente ai sensi della normativa vigente.

The contents of this email message and any attachments are intended solely for the addressee(s) and contain confidential and/or privileged information.
If you are not the intended recipient of this message, or if this message has been addressed to you in error, please immediately notify the sender and then delete this message and any attachments from your system. If you are not the intended recipient, you are hereby notified that any use, dissemination, copying, or storage of this message or its attachments is strictly prohibited. Unauthorized disclosure and/or use of information contained in this email message may result in civil and criminal liability. ?
This e-mail has legal value according to the applicable laws only if it is digitally signed by the sender

From mujtaba21n at hotmail.com  Sat Oct 20 12:56:33 2018
From: mujtaba21n at hotmail.com (Mujtaba   Hassan Madani)
Date: Sat, 20 Oct 2018 12:56:33 +0000
Subject: [squid-users] Squid Cache Server
In-Reply-To: <97515b93-e1b8-5ce9-775a-f79b9882525d@treenet.co.nz>
References: <DB6P193MB0008E0848E133CEB22448158C6040@DB6P193MB0008.EURP193.PROD.OUTLOOK.COM>
 <201809111054.06037.Antony.Stone@squid.open.source.it>
 <DB6P193MB00085E55BC2E60C264EE09A4C61B0@DB6P193MB0008.EURP193.PROD.OUTLOOK.COM>
 <b6e884e9-0aa3-a48a-b529-afcfe4c60f51@treenet.co.nz>
 <HE1P193MB0009EFBF04C8D339B5D23090C61A0@HE1P193MB0009.EURP193.PROD.OUTLOOK.COM>
 <DB6P193MB000820455863488DA32DDA21C6180@DB6P193MB0008.EURP193.PROD.OUTLOOK.COM>
 <f49f6de9-e3be-360e-cd44-b030f4be4a70@treenet.co.nz>
 <DB6P193MB00085F75B1438190AFE7B508C61C0@DB6P193MB0008.EURP193.PROD.OUTLOOK.COM>
 <DB6P193MB00086BE177598A71A08A69EBC6EB0@DB6P193MB0008.EURP193.PROD.OUTLOOK.COM>
 <DB6P193MB000839D7CA40427D251760F2C6EB0@DB6P193MB0008.EURP193.PROD.OUTLOOK.COM>
 <569c89ea-ea91-70c1-22db-4e6407716f23@treenet.co.nz>
 <DB6P193MB00087DBBD8CE1D0727E2F886C6E40@DB6P193MB0008.EURP193.PROD.OUTLOOK.COM>
 <DB6P193MB00083D2D58574BB7C243AC88C6E10@DB6P193MB0008.EURP193.PROD.OUTLOOK.COM>,
 <97515b93-e1b8-5ce9-775a-f79b9882525d@treenet.co.nz>
Message-ID: <DB6P193MB0008FAA6AA9A696F96D8D392C6FA0@DB6P193MB0008.EURP193.PROD.OUTLOOK.COM>

Hi Amos,

   I get attached message when trying to access cache manger through web interface below is my full URL

http://proxy.com:3128/squid-internal-mgr/info

according to squid.org feedback

 https://wiki.squid-cache.org/action/show/Features/CacheManager?action=show&redirect=SquidFaq%2FCacheManager

i should change the following command in the squid.config file i already did but with no avail

http_access allow localhost manager
http_access deny manager

i replaced deny manager by allow manager. please advice.

regards


Mujtaba H

________________________________
From: Amos Jeffries <squid3 at treenet.co.nz>
Sent: Saturday, October 13, 2018 7:58:34 AM
To: Mujtaba Hassan Madani; squid-users at lists.squid-cache.org
Subject: Re: [squid-users] Squid Cache Server

On 12/10/18 7:55 AM, Mujtaba   Hassan Madani wrote:
> Hi Amos,
>
>     I have change my domain name to proxy instead of that long one per
> your advice i was wondering where to get information about my current
> caching files and it's size ? i login to
> http://proxy:3128/squid-internal-mgr/info for that but with no success
> attached is web respond. please advise
>

The proxy hostname "proxy:3128" does need to resolve in DNS to access it
this way. That is what the browser is complaining about.

Alternatively you maybe can use the Linux/BSD command line tool on the
proxy machine itself:
   squidclient mgr:info

(but given this seems to be a NAS situation it may not be installed there).


Amos
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20181020/0223a677/attachment.htm>

From Antony.Stone at squid.open.source.it  Sat Oct 20 13:26:59 2018
From: Antony.Stone at squid.open.source.it (Antony Stone)
Date: Sat, 20 Oct 2018 15:26:59 +0200
Subject: [squid-users] Squid Cache Server
In-Reply-To: <DB6P193MB0008FAA6AA9A696F96D8D392C6FA0@DB6P193MB0008.EURP193.PROD.OUTLOOK.COM>
References: <DB6P193MB0008E0848E133CEB22448158C6040@DB6P193MB0008.EURP193.PROD.OUTLOOK.COM>
 <97515b93-e1b8-5ce9-775a-f79b9882525d@treenet.co.nz>
 <DB6P193MB0008FAA6AA9A696F96D8D392C6FA0@DB6P193MB0008.EURP193.PROD.OUTLOOK.COM>
Message-ID: <201810201526.59659.Antony.Stone@squid.open.source.it>

On Saturday 20 October 2018 at 14:56:33, Mujtaba Hassan Madani wrote:

> Hi Amos,
> 
>    I get attached message when trying to access cache manger through web
> interface below is my full URL
> 
> http://proxy.com:3128/squid-internal-mgr/info

1. What IP address does "proxy.com" resolve to on your network?

2. What is the IP address of your Squid server?

> according to squid.org feedback
> 
>  https://wiki.squid-cache.org/action/show/Features/CacheManager?action=show
> &redirect=SquidFaq%2FCacheManager
> 
> i should change the following command in the squid.config file i already
> did but with no avail
> 
> http_access allow localhost manager
> http_access deny manager

The above is the default, and is correct.  Where in the above instructions do 
you believe it tells you to change this?

> i replaced deny manager by allow manager. please advice.

I advise changing it back again.

Please let us know about the two IP addresses I asked about above.


Regards,


Antony.

-- 
BASIC is to computer languages what Roman numerals are to arithmetic.

                                                   Please reply to the list;
                                                         please *don't* CC me.


From houhm at greenet.net.cn  Sat Oct 20 13:47:54 2018
From: houhm at greenet.net.cn (houheming)
Date: Sat, 20 Oct 2018 21:47:54 +0800
Subject: [squid-users] https_port Listen on different IP
Message-ID: <005e01d4687b$815a1e60$840e5b20$@greenet.net.cn>

Hi Alex & Amos,

Thanks for your replies, sorry for my poor English, I will add more information.

 

#1

1. Configure Squid to automatically generate origin server certificates

(signed by a configured CA X) and send them to browsers/clients that go

to those origin servers.

 

This is my situation.

 

#2

If I configure squid like the following:

https_port 443 ...

https_port 180.97.33.107:443 ...

https_port 180.97.33.108:443 ...

 

when I reconfigure squid, no errors come out, but when I check the tcp port listening using "netstat -tln", the line2 and line3 with specific IP do not work:



 

#3

If I configure squid like:

https_port 180.97.33.107:443 ...

https_port 180.97.33.108:443

https_port 443

 

when I use ?squid ?k reconfigure? to reconfigure squid, no error message. But when I check the tcp ports listening:



 

 

#4

I also tried to specific a different port for IP 0.0.0.0, like this:

https_port 180.97.33.107:443 ...

https_port 180.97.33.108:443

https_port 4433   #here specify a different port from above

 

and it works.



 

https_port 0.0.0.0:443

https_port 1.1.1.1:443

I was just curious that the above two types of rules cannot exist together, whichever you put the first in squid.conf, it will overwrite the second one.

 

Using a different port for wildcard, it is a good solution in this case, it is just that ,if I could use the same tcp port for IP-0.0.0.0, it will be easier for my situation.

In my case, the destination server IP and root CA are dynamically send to me by anther configure server, so I choose to use default https port 443 to receive all https traffic.

If I have to use different ports for IP-0.0.0.0, I think I have to write one iptables rule for one https server IP, in normal case this is ok, in my case I have to do it dynamically, in another way of saying, whenever I receive a configure rule, I have to write an iptables rule for it. Anyway, that is my problem, thank you guys!

 

 

Heming Hou

-----????-----
???: Amos Jeffries [mailto:squid3 at treenet.co.nz] 
????: 2018?10?20? 12:10
???: squid-users at lists.squid-cache.org
??: Re: [squid-users] https_port Listen on different IP

 

On 20/10/18 6:04 AM, Alex Rousskov wrote:

> On 10/19/2018 01:10 AM, houheming wrote:

>> https_port 443 ...

>> https_port 180.97.33.107:443 ...

>> https_port 180.97.33.108:443 ...

> 

> I am not sure, but perhaps the first https_port line (the one without an

> explicit IP address) should come _last_ so that Squid can listen on the

> addresses that remain after 180.97.33.107 and 180.97.33.108 are taken by

> the other two ports?

 

I think that is what was meant by "If I switch line1 with line2 and

line3 ..., then only line2 and line3 will get its chance to work, line1

will not work. "

 

The problem is that TCP does not permit any IP:port combination to have

two simultaneous listening sockets with different parameters. These

configuration lines differ in both address and protocol they are receiving.

 

 

houheming:

you have to use different ports to receive the traffic into Squid.

 

Since you are using TPROXY there is no requirement for the proxy

listening port to be 443. Squid can listen on any port you want.

 

This problem should disappear if you set the wildcard port to another

number and update the TPROXY rule which is sending traffic to it.

 

Amos

_______________________________________________

squid-users mailing list

squid-users at lists.squid-cache.org

http://lists.squid-cache.org/listinfo/squid-users

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20181020/0fa185b5/attachment.htm>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: image001.png
Type: image/png
Size: 6363 bytes
Desc: not available
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20181020/0fa185b5/attachment.png>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: image002.png
Type: image/png
Size: 7094 bytes
Desc: not available
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20181020/0fa185b5/attachment-0001.png>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: image003.png
Type: image/png
Size: 7702 bytes
Desc: not available
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20181020/0fa185b5/attachment-0002.png>

From mujtaba21n at hotmail.com  Sat Oct 20 13:59:36 2018
From: mujtaba21n at hotmail.com (Mujtaba   Hassan Madani)
Date: Sat, 20 Oct 2018 13:59:36 +0000
Subject: [squid-users] Squid Cache Server
In-Reply-To: <201810201526.59659.Antony.Stone@squid.open.source.it>
References: <DB6P193MB0008E0848E133CEB22448158C6040@DB6P193MB0008.EURP193.PROD.OUTLOOK.COM>
 <97515b93-e1b8-5ce9-775a-f79b9882525d@treenet.co.nz>
 <DB6P193MB0008FAA6AA9A696F96D8D392C6FA0@DB6P193MB0008.EURP193.PROD.OUTLOOK.COM>,
 <201810201526.59659.Antony.Stone@squid.open.source.it>
Message-ID: <DB6P193MB0008AFD17BC575A1DABB2C1FC6FA0@DB6P193MB0008.EURP193.PROD.OUTLOOK.COM>

Hi Antony,

  this is the first IP Connection to 34.194.132.99 failed.

the IP of my server is 196.202.134.253

i got the instruction of changing of from below default setting form URL

http_access allow localhost manager
http_access deny manager <instead of deny to allow > so i can login through browser to the cache manager

https://wiki.squid-cache.org/action/show/Features/CacheManager?action=show&redirect=SquidFaq%2FCacheManager

regards


Mujtaba H,

________________________________
From: squid-users <squid-users-bounces at lists.squid-cache.org> on behalf of Antony Stone <Antony.Stone at squid.open.source.it>
Sent: Saturday, October 20, 2018 1:26:59 PM
To: squid-users at lists.squid-cache.org
Subject: Re: [squid-users] Squid Cache Server

On Saturday 20 October 2018 at 14:56:33, Mujtaba Hassan Madani wrote:

> Hi Amos,
>
>    I get attached message when trying to access cache manger through web
> interface below is my full URL
>
> http://proxy.com:3128/squid-internal-mgr/info

1. What IP address does "proxy.com" resolve to on your network?

2. What is the IP address of your Squid server?

> according to squid.org feedback
>
>  https://wiki.squid-cache.org/action/show/Features/CacheManager?action=show
> &redirect=SquidFaq%2FCacheManager
>
> i should change the following command in the squid.config file i already
> did but with no avail
>
> http_access allow localhost manager
> http_access deny manager

The above is the default, and is correct.  Where in the above instructions do
you believe it tells you to change this?

> i replaced deny manager by allow manager. please advice.

I advise changing it back again.

Please let us know about the two IP addresses I asked about above.


Regards,


Antony.

--
BASIC is to computer languages what Roman numerals are to arithmetic.

                                                   Please reply to the list;
                                                         please *don't* CC me.
_______________________________________________
squid-users mailing list
squid-users at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-users
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20181020/5ca6dc66/attachment.htm>

From Antony.Stone at squid.open.source.it  Sat Oct 20 14:08:32 2018
From: Antony.Stone at squid.open.source.it (Antony Stone)
Date: Sat, 20 Oct 2018 16:08:32 +0200
Subject: [squid-users] Squid Cache Server
In-Reply-To: <DB6P193MB0008AFD17BC575A1DABB2C1FC6FA0@DB6P193MB0008.EURP193.PROD.OUTLOOK.COM>
References: <DB6P193MB0008E0848E133CEB22448158C6040@DB6P193MB0008.EURP193.PROD.OUTLOOK.COM>
 <201810201526.59659.Antony.Stone@squid.open.source.it>
 <DB6P193MB0008AFD17BC575A1DABB2C1FC6FA0@DB6P193MB0008.EURP193.PROD.OUTLOOK.COM>
Message-ID: <201810201608.32888.Antony.Stone@squid.open.source.it>

On Saturday 20 October 2018 at 15:59:36, Mujtaba Hassan Madani wrote:

> Hi Antony,
> 
>   this is the first IP Connection to 34.194.132.99 failed.

That is the address which "proxy,com" resolves to on my machine too.

> the IP of my server is 196.202.134.253

So, does this give you any clues as to why trying to connect to "proxy.com" 
fails to connect to your Squid server?

> i got the instruction of changing of from below default setting form URL

I do not see anywhere in those instructions where it tells you to change this.

> http_access allow localhost manager
> http_access deny manager <instead of deny to allow > so i can login through
> browser to the cache manager
> 
> https://wiki.squid-cache.org/action/show/Features/CacheManager?action=show&
> redirect=SquidFaq%2FCacheManager

What does http://196.202.134.253:3128/squid-internal-mgr/info tell you?

Rather worryingly, it works for me from here - you URGENTLY need to review 
your firewall rules on a Squid server on a public IP address which accepts 
connections from anyone.


PS: Please do NOT set "Reply-to" on list emails.


Regards,


Antony.

-- 
Neurotics build castles in the sky;
Psychotics live in them;
Psychiatrists collect the rent.


                                                   Please reply to the list;
                                                         please *don't* CC me.


From mujtaba21n at hotmail.com  Sat Oct 20 14:53:12 2018
From: mujtaba21n at hotmail.com (Mujtaba   Hassan Madani)
Date: Sat, 20 Oct 2018 14:53:12 +0000
Subject: [squid-users] Squid Cache Server
In-Reply-To: <201810201608.32888.Antony.Stone@squid.open.source.it>
References: <DB6P193MB0008E0848E133CEB22448158C6040@DB6P193MB0008.EURP193.PROD.OUTLOOK.COM>
 <201810201526.59659.Antony.Stone@squid.open.source.it>
 <DB6P193MB0008AFD17BC575A1DABB2C1FC6FA0@DB6P193MB0008.EURP193.PROD.OUTLOOK.COM>,
 <201810201608.32888.Antony.Stone@squid.open.source.it>
Message-ID: <DB6P193MB00086E4A54554BCBE79245A9C6FA0@DB6P193MB0008.EURP193.PROD.OUTLOOK.COM>

Hi,

now it works through URL

http://196.202.134.253:3128/squid-internal-mgr/info instead of http://proxy.com:3128/squid-internal-mgr/info




________________________________
From: squid-users <squid-users-bounces at lists.squid-cache.org> on behalf of Antony Stone <Antony.Stone at squid.open.source.it>
Sent: Saturday, October 20, 2018 2:08:32 PM
To: squid-users at lists.squid-cache.org
Subject: Re: [squid-users] Squid Cache Server

On Saturday 20 October 2018 at 15:59:36, Mujtaba Hassan Madani wrote:

> Hi Antony,
>
>   this is the first IP Connection to 34.194.132.99 failed.

That is the address which "proxy,com" resolves to on my machine too.

> the IP of my server is 196.202.134.253

So, does this give you any clues as to why trying to connect to "proxy.com"
fails to connect to your Squid server?

> i got the instruction of changing of from below default setting form URL

I do not see anywhere in those instructions where it tells you to change this.

> http_access allow localhost manager
> http_access deny manager <instead of deny to allow > so i can login through
> browser to the cache manager
>
> https://wiki.squid-cache.org/action/show/Features/CacheManager?action=show&
> redirect=SquidFaq%2FCacheManager

What does http://196.202.134.253:3128/squid-internal-mgr/info tell you?

Rather worryingly, it works for me from here - you URGENTLY need to review
your firewall rules on a Squid server on a public IP address which accepts
connections from anyone.


PS: Please do NOT set "Reply-to" on list emails.


Regards,


Antony.

--
Neurotics build castles in the sky;
Psychotics live in them;
Psychiatrists collect the rent.


                                                   Please reply to the list;
                                                         please *don't* CC me.
_______________________________________________
squid-users mailing list
squid-users at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-users
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20181020/68a9d0c5/attachment.htm>

From Antony.Stone at squid.open.source.it  Sat Oct 20 15:12:06 2018
From: Antony.Stone at squid.open.source.it (Antony Stone)
Date: Sat, 20 Oct 2018 17:12:06 +0200
Subject: [squid-users] Squid Cache Server
In-Reply-To: <DB6P193MB00086E4A54554BCBE79245A9C6FA0@DB6P193MB0008.EURP193.PROD.OUTLOOK.COM>
References: <DB6P193MB0008E0848E133CEB22448158C6040@DB6P193MB0008.EURP193.PROD.OUTLOOK.COM>
 <201810201608.32888.Antony.Stone@squid.open.source.it>
 <DB6P193MB00086E4A54554BCBE79245A9C6FA0@DB6P193MB0008.EURP193.PROD.OUTLOOK.COM>
Message-ID: <201810201712.06553.Antony.Stone@squid.open.source.it>

On Saturday 20 October 2018 at 16:53:12, Mujtaba Hassan Madani wrote:

> Hi,
> 
> now it works through URL
> 
> http://196.202.134.253:3128/squid-internal-mgr/info instead of
> http://proxy.com:3128/squid-internal-mgr/info

Yes, that is because proxy,com does not belong to you - it points to someone 
else's machine, not *your* Squid server.

And, I repeat (especially now that this IP address has been discussed on a 
public mailing list) you URGENTLY need to review your firewall rules on that 
machine, because Squid is accepting requests from anyone.

It is even *proxying web pages* requested by *anyone*.

If you do not understand what I am telling you, or if you do not know how to 
change your firewall rules on that machine, *stop running Squid on it NOW* and 
do not turn it on again until you know how to secure it.

PS: Please do NOT set "Reply-to" on list emails.


Regards,


Antony.

-- 
"In fact I wanted to be John Cleese and it took me some time to realise that 
the job was already taken."

 - Douglas Adams

                                                   Please reply to the list;
                                                         please *don't* CC me.


From rousskov at measurement-factory.com  Sat Oct 20 17:12:16 2018
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Sat, 20 Oct 2018 11:12:16 -0600
Subject: [squid-users] https_port Listen on different IP
In-Reply-To: <005e01d4687b$815a1e60$840e5b20$@greenet.net.cn>
References: <005e01d4687b$815a1e60$840e5b20$@greenet.net.cn>
Message-ID: <e0c72df0-9323-8104-8f28-0c0738ab7dd8@measurement-factory.com>

On 10/20/18 7:47 AM, houheming wrote:
> 1. Configure Squid to automatically generate origin server certificates
> (signed by a configured CA X) and send them to browsers/clients that go
> to those origin servers.
 ?
OK.


> If I configure squid like:
> 
> https_port 180.97.33.107:443 ...
> https_port 180.97.33.108:443
> https_port 443

> when I use ?squid ?k reconfigure? to reconfigure squid, no error message.

In general, avoid using "-k anything" as the primary configuration test.
It just complicates matters by introducing a different error checking
context. Use a clean start. I am _not_ saying that a clean start would
have solved your problem in this particular case.


> But when I check the tcp ports listening:

[image showing the first two out of three configured :443 ports]

If Squid did not complain about anything but did not start listening on
one of the configured ports, then there is a Squid error reporting bug
somewhere. Feel free to report it to Squid bugzilla.

My suggestion to reorder those https_port lines was wrong. The wildcard
bind(INADDR_ANY) system call does not bind to "any available" address.
It binds to "all" addresses and, hence, fails if one of the addresses is
not available.

As Amos said and you have confirmed, using a different port for the
third https_port avoids these problems. Using a different/specific IP
address for the third https_port is another option.


> I was just curious that the above two types of rules cannot exist
> together, whichever you put the first in squid.conf, it will overwrite
> the second one.

https_port lines do not overwrite each other. The ports are independent,
but since they all consume a common resource (system IPs and ports),
they may conflict with each other. Use unique IPs and/or unique ports to
avoid conflicts.


Cheers,

Alex.


> -----????-----
> ???: Amos Jeffries [mailto:squid3 at treenet.co.nz]
> ????: 2018?10?20?12:10
> ???: squid-users at lists.squid-cache.org
> ??: Re: [squid-users] https_port Listen on different IP
> 
> ?
> 
> On 20/10/18 6:04 AM, Alex Rousskov wrote:
> 
>> On 10/19/2018 01:10 AM, houheming wrote:
> 
>>> https_port 443 ...
> 
>>> https_port 180.97.33.107:443 ...
> 
>>> https_port 180.97.33.108:443 ...
> 
>> 
> 
>> I am not sure, but perhaps the first https_port line (the one without an
> 
>> explicit IP address) should come _last_ so that Squid can listen on the
> 
>> addresses that remain after 180.97.33.107 and 180.97.33.108 are taken by
> 
>> the other two ports?
> 
> ?
> 
> I think that is what was meant by "If I switch line1 with line2 and
> 
> line3 ..., then only line2 and line3 will get its chance to work, line1
> 
> will not work. "
> 
> ?
> 
> The problem is that TCP does not permit any IP:port combination to have
> 
> two simultaneous listening sockets with different parameters. These
> 
> configuration lines differ in both address and protocol they are receiving.
> 
> ?
> 
> ?
> 
> houheming:
> 
> you have to use different ports to receive the traffic into Squid.
> 
> ?
> 
> Since you are using TPROXY there is no requirement for the proxy
> 
> listening port to be 443. Squid can listen on any port you want.
> 
> ?
> 
> This problem should disappear if you set the wildcard port to another
> 
> number and update the TPROXY rule which is sending traffic to it.
> 
> ?
> 
> Amos
> 
> _______________________________________________
> 
> squid-users mailing list
> 
> squid-users at lists.squid-cache.org
> 
> http://lists.squid-cache.org/listinfo/squid-users
> 



From rousskov at measurement-factory.com  Sat Oct 20 21:57:11 2018
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Sat, 20 Oct 2018 15:57:11 -0600
Subject: [squid-users] R: Is this the next step of SSL encryption? Fwd:
 Encrypted SNI
In-Reply-To: <aa95d90f6d6f4e8d8abeef705d3b0d92@ocgepvsw3101.ocr.priv>
References: <CANuGLW05iB5600kDBTi9W5wuiPE_2SbpyUQyM1qMqCLNTatfCQ@mail.gmail.com>
 <CANuGLW0HvN7Ok8m_Y+v_BriRE_LAkj=Z8K73UDWDyeGmH_E7=g@mail.gmail.com>
 <96c68618322bd796be622f588d2f5366@ngtech.co.il>
 <d182beef-5d7c-62b2-7338-8a791a9b1617@gmail.com>
 <721e4fba-48b4-1166-06bf-6dd1c243a530@measurement-factory.com>
 <20181019164717.GA28556@fantomas.sk>
 <44cd65fa-de40-3a44-ac18-ab25ed92bd92@measurement-factory.com>
 <d5c48c49-afaa-ae4a-db15-e38c83644e7c@urlfilterdb.com>
 <13423_1539977335_5BCA3077_13423_3866_1_951d7879-1f12-3a33-c0d2-e481f25c5d0f@integrafin.co.uk>
 <aa95d90f6d6f4e8d8abeef705d3b0d92@ocgepvsw3101.ocr.priv>
Message-ID: <fa2e0776-3b32-491a-bcf3-e1e22a4b6a6c@measurement-factory.com>

On 10/20/18 12:00 AM, Troiano Alessio wrote:

> Also if client will use ESNI, the server should response with a certificate that will be in clear

Starting with TLS v1.3, the server certificate is encrypted.

Alex.


From eliezer at ngtech.co.il  Sat Oct 20 22:25:38 2018
From: eliezer at ngtech.co.il (Eliezer Croitoru)
Date: Sun, 21 Oct 2018 01:25:38 +0300
Subject: [squid-users] R: Is this the next step of SSL encryption? Fwd:
 Encrypted SNI
In-Reply-To: <fa2e0776-3b32-491a-bcf3-e1e22a4b6a6c@measurement-factory.com>
References: <CANuGLW05iB5600kDBTi9W5wuiPE_2SbpyUQyM1qMqCLNTatfCQ@mail.gmail.com>
 <CANuGLW0HvN7Ok8m_Y+v_BriRE_LAkj=Z8K73UDWDyeGmH_E7=g@mail.gmail.com>
 <96c68618322bd796be622f588d2f5366@ngtech.co.il>
 <d182beef-5d7c-62b2-7338-8a791a9b1617@gmail.com>
 <721e4fba-48b4-1166-06bf-6dd1c243a530@measurement-factory.com>
 <20181019164717.GA28556@fantomas.sk>
 <44cd65fa-de40-3a44-ac18-ab25ed92bd92@measurement-factory.com>
 <d5c48c49-afaa-ae4a-db15-e38c83644e7c@urlfilterdb.com>
 <13423_1539977335_5BCA3077_13423_3866_1_951d7879-1f12-3a33-c0d2-e481f25c5d0f@integrafin.co.uk>
 <aa95d90f6d6f4e8d8abeef705d3b0d92@ocgepvsw3101.ocr.priv>
 <fa2e0776-3b32-491a-bcf3-e1e22a4b6a6c@measurement-factory.com>
Message-ID: <c43339d8a973ec53f8354291ec631f45@ngtech.co.il>

I believe that eventually some of these features would be targeted by 
the overall security community.
Security from one hand has encryption as a tool but it was already 
proven that obscurity is not really security.
In fact until now from what I know many of the more sane clients in the 
private and the public sector prefer to be able to cope with specific 
issues directly and not hide them.
Like in the hardware world complex functions were embedded into 
different types of CPU's or PU's and the large sector wants and needs 
it,
the Internet clients and users already understands that they want to be 
able to have some control either by a proxy service provider or by other 
means.
Just the other day I entered a "secured" site which is under the trust 
of a well know RootCA and what I got was a bunch of pop ups with exactly 
the things I want and need ie:
- Women underwear (with a demo how the body looks with and without the 
underwear) Whaaaat? yes I do not have an issue with that but Whaaat? I'm 
a man I need a working man shirts for work..
- The best Porn addiction solution. I am most of the day work with CLI 
or a text editor or System administration utilities.
- How to operate a specific software which I have never heard about.
....

It's nice to have some laugh while working but my co-worker (a women) 
didn't liked the idea and I agree with her.

And nothing less then that, CloudFlare was hosting or proxying to these 
sites acting as a frontend.


Thanks All,
Eliezer


On 2018-10-21 00:57, Alex Rousskov wrote:
> On 10/20/18 12:00 AM, Troiano Alessio wrote:
> 
>> Also if client will use ESNI, the server should response with a 
>> certificate that will be in clear
> 
> Starting with TLS v1.3, the server certificate is encrypted.
> 
> Alex.
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users

-- 
----
Eliezer Croitoru
Linux System Administrator
Mobile: +972-5-28704261
Email: eliezer at ngtech.co.il


From eliezer at ngtech.co.il  Sat Oct 20 22:40:57 2018
From: eliezer at ngtech.co.il (Eliezer Croitoru)
Date: Sun, 21 Oct 2018 01:40:57 +0300
Subject: [squid-users] 4.3 RPM's are out and an additional SCTP proxies I
	compiled
Message-ID: <f42d6399a72b77a922a868d0fb172141@ngtech.co.il>

I am happy to announce the release of Squid-Cache 4.3 RPM's and 
Debian+Ubuntu binaris.
I took a little project which some might like and other might not..
- TCP to SCP
- SCTP to TCP
Proxy servers.

The sources are at:
- https://github.com/elico/sctp-proxies
- http://gogs.ngtech.co.il/NgTech-LTD/sctp-proxies
or
- https://gogs.ngtech.co.il/NgTech-LTD/sctp-proxies
(With a self signed certificate SHA-256 
F0:CF:CD:71:0D:A5:E0:9E:7A:6B:D8:1D:09:5E:56:AB:AD:B1:39:5F:0B:9B:63:E5:A8:B7:88:E0:DC:5B:61:9A)

The installation scripts of Squid 4.3 on Debian and Ubuntu are at:
- http://gogs.ngtech.co.il/NgTech-LTD/Squid-installer
- https://gogs.ngtech.co.il/NgTech-LTD/Squid-installer

List of released 4.3 binaries:
## START OF LIST
RPMS
     [V] amazn1
     [V] amazn2
     [V] CentOS 7
     [V] Oracle 7
     [V] SLES 12 SP1

BIN
     [V] Debian 8 32bit
     [V] Debian 8 64bit

     [V] Debian 9 32bit
     [V] Debian 9 64bit

     [V] Ubuntu 14.04 32bit
     [V] Ubuntu 14.04 64bit

     [V] Ubuntu 16.04 32bit
     [V] Ubuntu 16.04 64bit

     [V] Ubuntu 18.04 64bit
## END OF LIST

I created the Debian/Ubuntu installer since It's too much pain to 
package squid inside a deb (for me..)
I removed the ECAP support from my 4.3 build but forgot to remove the 
package from the RPM's dependencies so a libecap pacakge will be 
installed with the RPM's but will not be used.
I will remove the dependency in the next release.

The 4.3 release is considered stable and is use in production for enough 
time for me to believe it's in a very good shape.

Thanks,
Eliezer

-- 
----
Eliezer Croitoru
Linux System Administrator
Mobile: +972-5-28704261
Email: eliezer at ngtech.co.il


From squid3 at treenet.co.nz  Sun Oct 21 06:15:47 2018
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sun, 21 Oct 2018 19:15:47 +1300
Subject: [squid-users] https_port Listen on different IP
In-Reply-To: <e0c72df0-9323-8104-8f28-0c0738ab7dd8@measurement-factory.com>
References: <005e01d4687b$815a1e60$840e5b20$@greenet.net.cn>
 <e0c72df0-9323-8104-8f28-0c0738ab7dd8@measurement-factory.com>
Message-ID: <820aed52-4636-67aa-a9ec-fd6e1f017239@treenet.co.nz>

On 21/10/18 6:12 AM, Alex Rousskov wrote:
> On 10/20/18 7:47 AM, houheming wrote:
>> 1. Configure Squid to automatically generate origin server certificates
>> (signed by a configured CA X) and send them to browsers/clients that go
>> to those origin servers.
>  ?
> OK.
> 
> 
>> If I configure squid like:
>>
>> https_port 180.97.33.107:443 ...
>> https_port 180.97.33.108:443
>> https_port 443
> 
>> when I use ?squid ?k reconfigure? to reconfigure squid, no error message.
> 
> In general, avoid using "-k anything" as the primary configuration test.
> It just complicates matters by introducing a different error checking
> context. Use a clean start. I am _not_ saying that a clean start would
> have solved your problem in this particular case.
> 
> 
>> But when I check the tcp ports listening:
> 
> [image showing the first two out of three configured :443 ports]
> 
> If Squid did not complain about anything but did not start listening on
> one of the configured ports, then there is a Squid error reporting bug
> somewhere. Feel free to report it to Squid bugzilla.
> 
> My suggestion to reorder those https_port lines was wrong. The wildcard
> bind(INADDR_ANY) system call does not bind to "any available" address.
> It binds to "all" addresses and, hence, fails if one of the addresses is
> not available.

Aye.

Just double-checked, Squid does report this problem:

 "2018/10/21 19:12:30 kid1| ERROR: listen( FD 21, [::] [ job2], 256):
(98) Address already in use"

... but only only for -k start / restart / reconfigure.
The -k parse does not check it.

Amos


From squid3 at treenet.co.nz  Sun Oct 21 16:39:20 2018
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Mon, 22 Oct 2018 05:39:20 +1300
Subject: [squid-users] SQUID Proxy - SSL Certificate error
In-Reply-To: <adee6f8b-d11f-fbfb-c3b8-94582cfba5ab@treenet.co.nz>
References: <SN4P114MB02237FBDB0F091C555E5C3518FFF0@SN4P114MB0223.NAMP114.PROD.OUTLOOK.COM>
 <adee6f8b-d11f-fbfb-c3b8-94582cfba5ab@treenet.co.nz>
Message-ID: <fbb9ebf5-e0ba-8fd3-3b30-a2c3259bbf2b@treenet.co.nz>

On 18/10/18 3:49 PM, Amos Jeffries wrote:
> On 18/10/18 2:31 AM, Vayalpadu, Vedavyas wrote:
>> Hi All,
>>
>> We have an existing SSL certificate for a WebShop URL. It has an
>> external IP Natted to a Load Balancer and has 2 reverse-squid proxies
>> configured for load balancing.
>>
>> ?
>>
>> Now we need to on-board a new URL with same external IP, Same Load
>> Balancers and r-Squid proxy servers ? Is it possible.
>>
>> ?
>>
>> I have uploaded the new URL certificate and restarted the squid proxy
>> services, when I try to access the URL iam getting below error, and
>> Certificate error as below.
>>
>> ?
>>
>> Can anyone help me on this ?
>>
> 
> OpenSSL builds of Squid do not support multiple certificates per
> listening port.
> 
> Squid-4 does support multiple certificates when built with GnuTLS
> instead of OpenSSL. This is still an experimental feature though, so YMMV.
> 

FYI: I have now added a config example to the wiki documenting this
GnuTLS feature at
<https://wiki.squid-cache.org/ConfigExamples/Reverse/HttpsVirtualHosting>.

Amos


From donmuller22 at outlook.com  Sun Oct 21 23:54:07 2018
From: donmuller22 at outlook.com (Donald Muller)
Date: Sun, 21 Oct 2018 23:54:07 +0000
Subject: [squid-users] Redirect certain sites to different gateway
Message-ID: <SN6PR07MB4622093CF93C2B4181C5D1BDB6FB0@SN6PR07MB4622.namprd07.prod.outlook.com>

I have had squid running well for a while now on my NAS. The browser on my PC was set up to use squid. A few weeks ago I started running a VPN client on the same NAS. Everything still ran well. The other day I change the VPN configuration so that all traffic on the NAS gets routed through the VPN (VPN became the default gateway). Everything still ran fine except for a few web sites. When I tried to reach my bank, let's say it is www.mybank.com, from my PC I received a "This site can't be reached" error. I'm assuming that the bank site won't allow connections from a VPN server.

Not sure if it is doable but is it possible via squid to redirect a request to a different gateway based on the URL (www.mybank.com<http://www.mybank.com>)? If possible how to accomplish this?

Thanks
Don
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20181021/a931eb9a/attachment.htm>

From squid3 at treenet.co.nz  Mon Oct 22 06:10:09 2018
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Mon, 22 Oct 2018 19:10:09 +1300
Subject: [squid-users] Redirect certain sites to different gateway
In-Reply-To: <SN6PR07MB4622093CF93C2B4181C5D1BDB6FB0@SN6PR07MB4622.namprd07.prod.outlook.com>
References: <SN6PR07MB4622093CF93C2B4181C5D1BDB6FB0@SN6PR07MB4622.namprd07.prod.outlook.com>
Message-ID: <15275882-0c1d-5bdc-58a1-fec6be570b6c@treenet.co.nz>

On 22/10/18 12:54 PM, Donald Muller wrote:
> I have had squid running well for a while now on my NAS. The browser on
> my PC was set up to use squid. A few weeks ago I started running a VPN
> client on the same NAS. Everything still ran well. The other day I
> change the VPN configuration so that all traffic on the NAS gets routed
> through the VPN (VPN became the default gateway). Everything still ran
> fine except for a few web sites. When I tried to reach my bank, let?s
> say it is www.mybank.com,

You bank with "United Bank & Trust"?

When making up fake domain names please use the reserved names in the
"example" namespace:  example.com, example.net, example.whatever

"mybank" is a registered domain name  - it may or may not be a real
bank. Either way no need to connect it with your problems.


> from my PC I received a ?This site can?t be
> reached? error. I?m assuming that the bank site won?t allow connections
> from a VPN server.
>

Assuming leads to problems and "solutions" that don't work. Test your
assumption
 - check your proxy cache.log for messages about traffic to that website
 - check your access.log for response status on traffic to that website
 - setup a test machine that makes requests via different gateways and
see what happens differently at the TCP (and if relevant TLS) layers.


> 
> Not sure if it is doable but is it possible via squid to redirect a
> request to a different gateway based on the URL (www.mybank.com
> <http://www.mybank.com>)? If possible how to accomplish this?
> 

What you are calling "redirect" is not possible for Squid to do itself.
The OS routing system is responsible for selecting which routing gateway
traffic goes through.

What Squid can do is mark traffic selectively with a TOS
(tp_uotgoing_tos) or nefilter/iptables MARK (tcp_outgoing_mark) the OS
uses to decide on a NIC gateway for. The dstdomain ACL can be used to
label traffic by domain.


But until you actually confirm your assumption was true, it may not
actually do anything useful.

Amos


From juan at mediaarchitecture.org  Mon Oct 22 12:26:19 2018
From: juan at mediaarchitecture.org (Juan Carvajal B.)
Date: Mon, 22 Oct 2018 14:26:19 +0200
Subject: [squid-users] squid transparent proxy forward loop
Message-ID: <CAN8OBP6Lhmsr7FgDTn_Z4+bO=dyhceG4dXiebZjLi_sY-Ot4CQ@mail.gmail.com>

Dear list,

I hope you can give me some hints for my current task.

I would like to achieve the following:

1. A user comes with the own device, for example phone or table.
2. The user connects to our own WLAN network
4. The user enters the addres of our website
3. The user can only access our website, which is hosted in a sever *not
connected* to the internet.

We have the following set up

Tablets / phones <---> WLAN router <---> server

please note that there is *no* connection to internet. The server is
connected to the "internet" port of the router.

The server runs ubuntu & apache.

I've been trying to achieve this with squid but I get a **warning of a
forwarding loop**. I do not know what I'm doing wrong.

I'm following this:
https://wiki.squid-cache.org/ConfigExamples/Intercept/LinuxRedirect
https://wiki.squid-cache.org/SquidFaq/ReverseProxy

Here are my conf files:

****squid.conf****

    http_port 3128 intercept
    http_port 192.168.0.188:80 accel defaultsite=our.domain.org
    http_port 192.168.0.188:443 accel defaultsite=our.domain.org
    cache_peer 127.0.0.1 parent 80 0 no-query originserver name=myAccel
    acl our_sites dstdomain our.domain.org
    http_access allow our_sites
    cache_peer_access myAccel allow our_sites
    cache_peer_access myAccel deny all
    visible_hostname squid.proxy
    acl SSL_ports port 443
    acl CONNECT method CONNECT
    http_access deny !Safe_ports
    http_access deny CONNECT !SSL_ports
    http_access allow localhost manager
    http_access deny manager
    http_access allow localhost
    http_access allow all
    coredump_dir /var/spool/squid
    refresh_pattern ^ftp:        1440    20%    10080
    refresh_pattern ^gopher:    1440    0%    1440
    refresh_pattern -i (/cgi-bin/|\?) 0    0%    0
    refresh_pattern (Release|Packages(.gz)*)$      0       20%     2880
    refresh_pattern .        0    20%    4320


****apache ports.conf****

    #With Squid
    Listen 127.0.0.1:80

    #Without Squid
    #Listen 80

    <IfModule ssl_module>
        Listen 443
    </IfModule>

    <IfModule mod_gnutls.c>
        Listen 443
    </IfModule>


****IPTABLES****

    # your proxy IP
    SQUIDIP=192.168.0.188
    # your proxy listening port
    SQUIDPORT=3128
    iptables -t nat -A PREROUTING -s $SQUIDIP -p tcp --dport 80 -j ACCEPT
    iptables -t nat -A PREROUTING -p tcp --dport 80 -j REDIRECT --to-port
$SQUIDPORT

apache: 2.4.18
squid: 3.5.12
ubuntu: 16.04

Thank you for your help!

*Juan Carlos Carvajal*

*Join our mailing list
<http://lists.mediaarchitecture.org/?p=subscribe&id=1> (Max 1-mail / month)*
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20181022/d5b24fd9/attachment.htm>

From uhlar at fantomas.sk  Mon Oct 22 12:54:23 2018
From: uhlar at fantomas.sk (Matus UHLAR - fantomas)
Date: Mon, 22 Oct 2018 14:54:23 +0200
Subject: [squid-users] squid transparent proxy forward loop
In-Reply-To: <CAN8OBP6Lhmsr7FgDTn_Z4+bO=dyhceG4dXiebZjLi_sY-Ot4CQ@mail.gmail.com>
References: <CAN8OBP6Lhmsr7FgDTn_Z4+bO=dyhceG4dXiebZjLi_sY-Ot4CQ@mail.gmail.com>
Message-ID: <20181022125423.xr6qluksy4t3nnlq@fantomas.sk>

On 22.10.18 14:26, Juan Carvajal B. wrote:
>I would like to achieve the following:
>
>1. A user comes with the own device, for example phone or table.
>2. The user connects to our own WLAN network
>4. The user enters the addres of our website
>3. The user can only access our website, which is hosted in a sever *not
>connected* to the internet.

well, if user enters address of your website, of course the user can only
access your website.
you apparently mean "user enters address of aany site".

>Tablets / phones <---> WLAN router <---> server
>
>please note that there is *no* connection to internet. The server is
>connected to the "internet" port of the router.

in this case it's also obious that user can not access any other site...

>I've been trying to achieve this with squid but I get a **warning of a
>forwarding loop**. I do not know what I'm doing wrong.

forwarding loop happens, when squid connects to itself instead of internet.

>I'm following this:
>https://wiki.squid-cache.org/ConfigExamples/Intercept/LinuxRedirect

maybe you have redirected connections from squid proxy back to the squid
proxy?

>https://wiki.squid-cache.org/SquidFaq/ReverseProxy

why do you follow reverse proxy configuration when you don't configure
reverse proxy?

-- 
Matus UHLAR - fantomas, uhlar at fantomas.sk ; http://www.fantomas.sk/
Warning: I wish NOT to receive e-mail advertising to this address.
Varovanie: na tuto adresu chcem NEDOSTAVAT akukolvek reklamnu postu.
The early bird may get the worm, but the second mouse gets the cheese. 


From squid3 at treenet.co.nz  Mon Oct 22 13:12:01 2018
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Tue, 23 Oct 2018 02:12:01 +1300
Subject: [squid-users] squid transparent proxy forward loop
In-Reply-To: <CAN8OBP6Lhmsr7FgDTn_Z4+bO=dyhceG4dXiebZjLi_sY-Ot4CQ@mail.gmail.com>
References: <CAN8OBP6Lhmsr7FgDTn_Z4+bO=dyhceG4dXiebZjLi_sY-Ot4CQ@mail.gmail.com>
Message-ID: <dd3a7f5f-8dbf-3fc4-a9b0-e1120374717b@treenet.co.nz>

On 23/10/18 1:26 AM, Juan Carvajal B. wrote:
> Dear list,
> 
> I hope you can give me some hints for my current task.
> 
> I would like to achieve the following:
> 
> 1. A user comes with the own device, for example phone or table.
> 2. The user connects to our own WLAN network
> 4. The user enters the addres of our website
> 3. The user can only access our website, which is hosted in a sever *not
> connected* to the internet.
> 
> We have the following set up
> 
> Tablets / phones <---> WLAN router <---> server
> 
> please note that there is *no* connection to internet. The server is
> connected to the "internet" port of the router.
> 
> The server runs ubuntu & apache.
> 
> I've been trying to achieve this with squid but I get a **warning of a
> forwarding loop**. I do not know what I'm doing wrong.
> 
> I'm following this:
> https://wiki.squid-cache.org/ConfigExamples/Intercept/LinuxRedirect
> https://wiki.squid-cache.org/SquidFaq/ReverseProxy
> 
> Here are my conf files:
> 
> ****squid.conf****
> 


>     visible_hostname squid.proxy

I have seen other people using this "squid.proxy" as the FQDN of their
proxy. It is likely that your chosen proxy hostname is not unique.

Since this is a reverse-proxy it is best to set this to the FQDN of the
primary website you are proxying.



> ??? http_port 3128 intercept
> ??? http_port 192.168.0.188:80 accel
> defaultsite=our.domain.org
> ??? http_port 192.168.0.188:443 accel
> defaultsite=our.domain.org

The above should be https_port and requires the TLS certificate for the
domain being virtual-hosted.

see
<https://wiki.squid-cache.org/ConfigExamples/Reverse/HttpsVirtualHosting> which
I added just yesterday.


> ??? cache_peer 127.0.0.1 parent 80 0 no-query originserver name=myAccel
> ??? acl our_sites dstdomain our.domain.org
> ??? http_access allow our_sites
> ??? cache_peer_access myAccel allow our_sites
> ??? cache_peer_access myAccel deny all

Move the above custom configuration down to ...

> ??? acl SSL_ports port 443
> ??? acl CONNECT method CONNECT
> ??? http_access deny !Safe_ports

You do not have any definition of Safe_ports above. It should contain at
least 80 and 443 for your proxy.

> ??? http_access deny CONNECT !SSL_ports
> ??? http_access allow localhost manager
> ??? http_access deny manager

... here.


You are missing good rules for traffic arriving on the port 3128. The
below "allow all" is very bad.


> ??? http_access allow localhost
> ??? http_access allow all

That should be:

 http_access deny all


...

> ****IPTABLES****
> 
> ??? # your proxy IP
> ??? SQUIDIP=192.168.0.188
> ??? # your proxy listening port
> ??? SQUIDPORT=3128
> ??? iptables -t nat -A PREROUTING -s $SQUIDIP -p tcp --dport 80 -j ACCEPT
> ??? iptables -t nat -A PREROUTING -p tcp --dport 80 -j REDIRECT
> --to-port $SQUIDPORT
> 

Here is your problem. You have not bypassed the traffic Squid is sending
to 127.0.0.1:80.

Traffic to/from localhost does not use global IP addresses such as
192.168.0.188. Thus Squid's attempts to send traffic to Apache is being
looped back into port 3128 by iptables.


What is the point of all this interception anyway?

Your proxy is already listening on port 80 and 443 for directly
receiving traffic to any domain with a DNS entry of 192.168.0.188.


Amos


From vh1988 at yahoo.com.ar  Mon Oct 22 14:38:11 2018
From: vh1988 at yahoo.com.ar (Julian Perconti)
Date: Mon, 22 Oct 2018 11:38:11 -0300
Subject: [squid-users] Squid 4.3 assertion failed
Message-ID: <001c01d46a14$dd4083f0$97c18bd0$@yahoo.com.ar>

Hi all,

Environment: 

Squid Cache: Version 4.3-20181014-r17614d5
Service Name: squid

This binary uses OpenSSL 1.1.0f  25 May 2017. For legal restrictions on
distribution see https://www.openssl.org/source/license.html

configure options:  '--prefix=/usr' '--build=x86_64-linux-gnu'
'--localstatedir=/var/squid' '--libexecdir=/lib/squid' '--srcdir=.'
'--datadir=/share/squid' '--with-cppunit-basedir=/usr' '--enable-inline'
'--enable-delay-pools' '--sysconfdir=/etc/squid'
'--with-logdir=/var/log/squid' '--with-pidfile=/var/run/squid.pid'
'--with-openssl' '--enable-ssl-crtd' '--mandir=/share/man'
'--enable-arp-acl' '--enable-wccpv2' '--with-large-files'
'--with-default-user=proxy' '--enable-linux-netfilter'
'--enable-storeio=ufs,aufs,diskd,rock' '--enable-removal-policies=lru,heap'
'--enable-icap' '--enable-icap-client' '--enable-cache-digests'
'--disable-ident-lookups' '--enable-poll' '--enable-truncate'
'--enable-heap-replacement' 'build_alias=x86_64-linux-gnu'

OS:

PRETTY_NAME="Debian GNU/Linux 9 (stretch)"
NAME="Debian GNU/Linux"
VERSION_ID="9"
VERSION="9 (stretch)"
ID=debian
HOME_URL="https://www.debian.org/"
SUPPORT_URL="https://www.debian.org/support"
BUG_REPORT_URL="https://bugs.debian.org/"

The error (bug?):

   assertion failed: http.cc:1530:
"!Comm::MonitorsRead(serverConnection->fd)"

Sometimes cache.log show this and squid goes down, and sometimes just it
appears in the log and squid does not stop.

I don't exactly remember when this start to happen but, I think that since
squid version >= 4.2 or 4.3

Any idea?

Thank You



From rousskov at measurement-factory.com  Mon Oct 22 17:45:05 2018
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Mon, 22 Oct 2018 11:45:05 -0600
Subject: [squid-users] Squid 4.3 assertion failed
In-Reply-To: <001c01d46a14$dd4083f0$97c18bd0$@yahoo.com.ar>
References: <001c01d46a14$dd4083f0$97c18bd0$@yahoo.com.ar>
Message-ID: <76aa8880-3651-67f0-d93a-c17701ffa551@measurement-factory.com>

On 10/22/18 8:38 AM, Julian Perconti wrote:

> assertion failed: http.cc:1530: "!Comm::MonitorsRead(serverConnection->fd)"

> Any idea?

Without the stack trace, it is difficult to say much about this bug.
Please collect a stack trace from the crash and post it to Squid
bugzilla. If the stack trace looks similar to that in Bug 4896, please
post it there: https://bugs.squid-cache.org/show_bug.cgi?id=4896

If you do not know how to enable core dumps and/or how to collect a
stack trace, please search Squid wiki for pointers or consult your local
sysadmin.


Thank you,

Alex.


From donmuller22 at outlook.com  Mon Oct 22 19:52:27 2018
From: donmuller22 at outlook.com (Donald Muller)
Date: Mon, 22 Oct 2018 19:52:27 +0000
Subject: [squid-users] Redirect certain sites to different gateway
In-Reply-To: <15275882-0c1d-5bdc-58a1-fec6be570b6c@treenet.co.nz>
References: <SN6PR07MB4622093CF93C2B4181C5D1BDB6FB0@SN6PR07MB4622.namprd07.prod.outlook.com>
 <15275882-0c1d-5bdc-58a1-fec6be570b6c@treenet.co.nz>
Message-ID: <SN6PR07MB46227F4258B0B059AD309564B6F40@SN6PR07MB4622.namprd07.prod.outlook.com>



> -----Original Message-----
> From: squid-users <squid-users-bounces at lists.squid-cache.org> On Behalf
> Of Amos Jeffries
> Sent: Monday, October 22, 2018 2:10 AM
> To: squid-users at lists.squid-cache.org
> Subject: Re: [squid-users] Redirect certain sites to different gateway
> 
> On 22/10/18 12:54 PM, Donald Muller wrote:
> > I have had squid running well for a while now on my NAS. The browser on
> > my PC was set up to use squid. A few weeks ago I started running a VPN
> > client on the same NAS. Everything still ran well. The other day I
> > change the VPN configuration so that all traffic on the NAS gets routed
> > through the VPN (VPN became the default gateway). Everything still ran
> > fine except for a few web sites. When I tried to reach my bank, let?s
> > say it is www.mybank.com,
> 
> You bank with "United Bank & Trust"?
> 
> When making up fake domain names please use the reserved names in the
> "example" namespace:  example.com, example.net, example.whatever
> 
> "mybank" is a registered domain name  - it may or may not be a real
> bank. Either way no need to connect it with your problems.
> 

Noted!

> 
> > from my PC I received a ?This site can?t be
> > reached? error. I?m assuming that the bank site won?t allow connections
> > from a VPN server.
> >
> 
> Assuming leads to problems and "solutions" that don't work. Test your
> assumption
>  - check your proxy cache.log for messages about traffic to that website
>  - check your access.log for response status on traffic to that website
>  - setup a test machine that makes requests via different gateways and
> see what happens differently at the TCP (and if relevant TLS) layers.
> 
> 

If I do not use the VPN as the default gateway I can reach the site with no issues. Once I enable the VPN to be the default gateway the site stops working.

> >
> > Not sure if it is doable but is it possible via squid to redirect a
> > request to a different gateway based on the URL (www.mybank.com
> > <http://www.mybank.com>)? If possible how to accomplish this?
> >
> 
> What you are calling "redirect" is not possible for Squid to do itself.
> The OS routing system is responsible for selecting which routing gateway
> traffic goes through.
> 

Setting up a static route is probably the preferred method but was hoping to be able to do it via a URL instead of figuring out all the IP addresses the site uses. I will attempt the static route method.

> What Squid can do is mark traffic selectively with a TOS
> (tp_uotgoing_tos) or nefilter/iptables MARK (tcp_outgoing_mark) the OS
> uses to decide on a NIC gateway for. The dstdomain ACL can be used to
> label traffic by domain.
> 
> 
> But until you actually confirm your assumption was true, it may not
> actually do anything useful.
> 
> Amos
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users

From josepjones at expedia.com  Mon Oct 22 20:04:28 2018
From: josepjones at expedia.com (Joseph Jones)
Date: Mon, 22 Oct 2018 20:04:28 +0000
Subject: [squid-users] Squid workers failing to restart after log
 rotation event.
In-Reply-To: <a3ee63ec-42b1-b1ad-9d8a-c7d0958d88ec@measurement-factory.com>
References: <CY4PR02MB272576F3E472B634260BBEA8CFE10@CY4PR02MB2725.namprd02.prod.outlook.com>,
 <a3ee63ec-42b1-b1ad-9d8a-c7d0958d88ec@measurement-factory.com>
Message-ID: <CY4PR02MB27253D9952AC667AD273FDD5CFF40@CY4PR02MB2725.namprd02.prod.outlook.com>



Hello Alex,

Just wanted to write back and say you advice helped. After digging into our problem a bit further we found that we were being effected by the bug#4796. Applying the patch you  suggested  to our build for squid fixed the problem and we are able to run squid under much greater loads with out issue now.
    
Thanks,
 
 --?
 
 
 Joseph M Jones
 
 
 Senior Application Engineer
 EAN ? Expedia Affiliate Network




From: squid-users <squid-users-bounces at lists.squid-cache.org> on behalf of Alex Rousskov <rousskov at measurement-factory.com>
Sent: Thursday, October 11, 2018 16:44
To: squid-users at lists.squid-cache.org
Subject: Re: [squid-users] Squid workers failing to restart after log rotation event.
? 

On 10/11/2018 06:30 AM, Joseph Jones wrote:

> I'm trying to find a root cause for failed workers.

My suggestions:

0. Upgrade to the latest Squid v4.

1. Disable memory cache:
?? cache_mem 0

2. Maintain one cache.log per worker. For example:
cache_log /usr/local/squid/var/logs/cache-${process_number}.log

3. Focus on the logged messages of the worker that fails _first_. Zero
in on the first failure of that worker. Why did it fail? Your log
snippets appear to show what happened _after_ at least one failure. It
is usually best to focus on the original failure. Check system logs as
well -- some relevant messages may only appear there.

4. With the information from #3, check whether you are suffering from
bug #4796. It has a (hidden in the PR discussion) short-term fix:
https://bugs.squid-cache.org/show_bug.cgi?id=4796
https://github.com/squid-cache/squid/pull/257#issuecomment-427271426

If your bug is different, and the first failure of the failed-first
worker left a core dump, post a stack trace. Configure your OS to allow
core dumps, of course.


You can just skip to step #4 and see if the patch helps, but steps #0-3
are useful in general should you face similar problems in the future.


HTH,

Alex.


?We have three squid instances that act as transparent forward proxies
that limit internet connectivity for our network by doing url
whitelisting. Current throughput per instances is about 90MB/s. after a
restart of squid all workers seem to be working just fine, but after
about an hour some of the workers fail and they never come back until we
do a complete restart. These are EC2 instances in AWS (c5.4xlarge). so
we have 16 vCPU to work with. but that's really 8 Cores and 2 Threads
per core.
> 
> CPU and Memory loads are small. Disk IO could be a concern. We've ran some load test on a different instances with logging turned off and we were able to get a higher throughput without worker failure. We don't have caching enabled as most of our traffic  is SSL anyway. I Was hoping someone could point us in a direction we should take our testing or if from the information I've given can tell use any obvious this we may be doing wrong.
> 
> 
> $ uptime
> ?18:28:30 up 6 days,? 2:13,? 1 user,? load average: 0.88, 1.10, 1.09
> 
> $ free -m
> ????????????? total??????? used??????? free????? shared? buff/cache?? available
> Mem:????????? 30987??????? 1728?????? 25264??????? 1156??????? 3994?????? 27523
> Swap:???????????? 0?????????? 0?????????? 0
> 
> 
> 
> 2018/10/10 18:19:42 kid2| Squid Cache (Version 4.1): Terminated abnormally.
> CPU Usage: 0.036 seconds = 0.022 user + 0.014 sys
> Maximum Resident Size: 92544 KB
> Page faults with physical i/o: 0
> 2018/10/10 18:19:42 kid1| Closing HTTP(S) port [::]:3129
> 2018/10/10 18:19:42 kid1| Closing HTTP(S) port [::]:3128
> 2018/10/10 18:19:42 kid1| Closing HTTP(S) port [::]:3130
> 2018/10/10 18:19:42 kid1| storeDirWriteCleanLogs: Starting...
> 2018/10/10 18:19:42 kid1|?? Finished.? Wrote 0 entries.
> 2018/10/10 18:19:42 kid1|?? Took 0.00 seconds (? 0.00 entries/sec).
> 2018/10/10 18:19:42 kid1| FATAL: kid1 registration timed out
> 2018/10/10 18:19:42 kid1| Squid Cache (Version 4.1): Terminated abnormally.
> CPU Usage: 0.034 seconds = 0.021 user + 0.013 sys
> Maximum Resident Size: 92544 KB
> Page faults with physical i/o: 0
> 
> $ cat /etc/redhat-release
> CentOS Linux release 7.5.1804 (Core)
> 
> $ squid -v
> Squid Cache: Version 4.1
> Service Name: squid
> 
> This binary uses OpenSSL 1.0.2k-fips? 26 Jan 2017. For legal restrictions on distribution see https://www.openssl.org/source/license.html
> 
> configure options:? '--build=x86_64-redhat-linux-gnu' '--host=x86_64-redhat-linux-gnu' '--program-prefix=' '--prefix=/usr' '--exec-prefix=/usr' '--bindir=/usr/bin' '--sbindir=/usr/sbin' '--sysconfdir=/etc' '--datadir=/usr/share' '--includedir=/usr/include'  '--libdir=/usr/lib64' '--libexecdir=/usr/libexec' '--sharedstatedir=/var/lib' '--mandir=/usr/share/man' '--infodir=/usr/share/info' '--exec_prefix=/usr' '--libexecdir=/usr/lib64/squid' '--localstatedir=/var' '--datadir=/usr/share/squid' '--sysconfdir=/etc/squid'  '--with-logdir=$(localstatedir)/log/squid' '--with-pidfile=$(localstatedir)/run/squid.pid' '--disable-dependency-tracking' '--enable-follow-x-forwarded-for' '--enable-auth' '--enable-auth-basic=DB,LDAP,NCSA,NIS,PAM,POP3,RADIUS,SASL,SMB,getpwnam,fake' '--enable-auth-ntlm=fake'  '--enable-auth-digest=file,LDAP,eDirectory' '--enable-auth-negotiate=kerberos,wrapper' '--enable-external-acl-helpers=wbinfo_group,kerberos_ldap_group,LDAP_group,delayer,file_userip,SQL_session,unix_group,session,time_quota' '--enable-cache-digests' '--enable-cachemgr-hostname=localhost'  '--enable-delay-pools' '--enable-epoll' '--enable-smp' '--enable-icap-client' '--enable-ident-lookups' '--enable-linux-netfilter' '--enable-removal-policies=heap,lru' '--enable-snmp' '--enable-storeio=aufs,diskd,ufs,rock' '--enable-wccpv2' '--enable-esi' '--enable-security-cert-generators'  '--enable-security-cert-validators' '--enable-icmp' '--with-aio' '--with-default-user=squid' '--with-filedescriptors=16384' '--with-dl' '--with-openssl' '--enable-ssl-crtd' '--with-pthreads' '--with-included-ltdl' '--disable-arch-native' '--enable-ecap' '--without-nettle'  'build_alias=x86_64-redhat-linux-gnu' 'host_alias=x86_64-redhat-linux-gnu' 'CFLAGS=-O2 -g -pipe -Wall -Wp,-D_FORTIFY_SOURCE=2 -fexceptions -fstack-protector-strong --param=ssp-buffer-size=4 -grecord-gcc-switches?? -m64 -mtune=generic' 'LDFLAGS=-Wl,-z,relro  ' 'CXXFLAGS=-O2 -g -pipe -Wall -Wp,-D_FORTIFY_SOURCE=2 -fexceptions -fstack-protector-strong --param=ssp-buffer-size=4 -grecord-gcc-switches?? -m64 -mtune=generic -fPIC' 'PKG_CONFIG_PATH=:/usr/lib64/pkgconfig:/usr/share/pkgconfig' --enable-ltdl-convenience
> 
> $ cat /etc/squid/squid.conf
> workers 12
> hopeless_kid_revival_delay 5 minute
> # Default 'squid' logformat with request size and TLS SNI added
> logformat ean_squid %ts.%03tu %6tr %>a %Ss/%03>Hs %>st %<st %rm %ru %ssl::>sni %[un %Sh/%<a %mt
> logfile_rotate 0
> access_log daemon:/var/log/squid/access.log logformat=ean_squid
> debug_options ALL,1
> 
> # Only allow cachemgr access from localhost
> http_access allow localhost manager
> http_access deny manager
> 
> acl localnet src 10.26.128.0/21
> 
> acl SSL_ports port 443
> acl Safe_ports port 80??? # http
> acl Safe_ports port 443?? # https
> 
> acl CONNECT method CONNECT
> 
> #
> # Recommended minimum Access Permission configuration:
> #
> # Deny requests to certain unsafe ports
> http_access deny !Safe_ports
> 
> # Deny CONNECT to other than secure SSL ports
> http_access deny CONNECT !SSL_ports
> 
> # Allow requests from the local network (see acl at the top)
> http_access allow localnet
> 
> #
> # INSERT YOUR OWN RULE(S) HERE TO ALLOW ACCESS FROM YOUR CLIENTS
> #
> 
> # Just for debugging
> # debug_options ALL,1 33,2 rotate=0
> 
> acl https_whitelist ssl::server_name "/etc/squid/whitelist.txt"
> acl http_whitelist dstdomain "/etc/squid/whitelist.txt"
> 
> acl step1 at_step SslBump1
> acl step2 at_step SslBump2
> acl step3 at_step SslBump3
> 
> http_access allow http_whitelist
> 
> ssl_bump peek step1 all
> 
> ssl_bump peek step2 https_whitelist
> ssl_bump splice step3 https_whitelist
> ssl_bump terminate step2 all
> 
> 
> # disable caching
> cache deny all
> 
> # And finally deny all other access to this proxy
> http_access deny all
> 
> # Squid normally listens to port 3128
> http_port 3129 intercept
> http_port 3128
> https_port 3130 cert=/etc/pki/tls/certs/squid.pem key=/etc/pki/tls/private/squid.key ssl-bump intercept
> 
> visible_hostname squid
> 
> # Uncomment and adjust the following to add a disk cache directory.
> #cache_dir ufs /var/spool/squid 100 16 256
> 
> # Leave coredumps in the first cache dir
> coredump_dir /var/spool/squid
> 
> #
> # Add any of your own refresh_pattern entries above these.
> #
> refresh_pattern ^ftp:?? 1440? 20% 10080
> refresh_pattern ^gopher:? 1440? 0%? 1440
> refresh_pattern -i (/cgi-bin/|\?) 0 0%? 0
> refresh_pattern .?? 0 20% 4320
> 
> 
>? --?
>? 
>? 
>? Joseph M Jones
>? 
>? Senior Application Engineer
>? Expedia Partner Solutions
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
> 

_______________________________________________
squid-users mailing list
squid-users at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-users
    

From ajiang at ouhk.edu.hk  Tue Oct 23 03:28:01 2018
From: ajiang at ouhk.edu.hk (Angus J.)
Date: Mon, 22 Oct 2018 22:28:01 -0500 (CDT)
Subject: [squid-users] Squid proxy not working when upgrade from 27 to 3.5
Message-ID: <1540265281548-0.post@n4.nabble.com>

Squid proxy not working when upgrade from 27 to 3.5

Squid proxy not working when upgrade to 3.5 and it will not caching anything 

----------------------------------------------------------------------------

#Default:
# windows_ipaddrchangemonitor on

visible_hostname oul163.hkbb.edu.hk
http_port 3128 accel vhost defaultsite=oul163.hkbb.edu.hk
https_port 80 accel cert=/etc/squid/certs/ouhk.crt
key=/etc/squid/certs/ouhk.key defaultsite=oul163.hkbb.edu.hk vhost
protocol=https options=NO_SSLv3:NO_SSLv2
https_port 8000 accel cert=/etc/squid/certs/ouhk.crt
key=/etc/squid/certs/ouhk.key defaultsite=oul163.hkbb.edu.hk vhost
protocol=https options=NO_SSLv3:NO_SSLv2
#https_port 8004 accel cert=/etc/squid/certs/ouhk.crt
key=/etc/squid/certs/ouhk.key defaultsite=oul163.hkbb.edu.hk vhost
protocol=https options=NO_SSLv3:NO_SSLv2
https_port 8004 accel cert=/etc/squid/certs/ouhk2.crt
key=/etc/squid/certs/ouhk2.key defaultsite=oul163.hkbb.edu.hk vhost
protocol=https options=NO_SSLv3:NO_SSLv2
#https_port 8005 accel cert=/etc/squid/certs/ouhk.crt
key=/etc/squid/certs/ouhk.key defaultsite=oul163.hkbb.edu.hk vhost
protocol=https options=NO_SSLv3:NO_SSLv2
https_port 8005 accel cert=/etc/squid/certs/ouhk3.crt
key=/etc/squid/certs/ouhk3.key defaultsite=oul163.hkbb.edu.hk vhost
protocol=https options=NO_SSLv3:NO_SSLv2
#ssl_bump allow all
#              Disable the following one
#ssl_bump options=NO_SSLv3
#always_direct allow all
#              Disable the following one
#sslproxy_cert_error allow all
sslproxy_options NO_SSLv3:NO_SSLv2
access_log /var/log/squid/access.log squid
cache_effective_user squid
cache_log /var/log/squid/cache.log
cache_store_log /var/log/squid/store.log

# the proxy-only indicates that caching will not be performed.
cache_peer 192.168.31.113 parent 8001 0 proxy-only name=prdhrms
cache_peer_domain prdhrms prdhrms.hkbb.edu.hk
cache_peer 192.168.31.134 parent 8005 0 ssl sslflags=DONT_VERIFY_PEER
proxy-only   name=uathrms ssloptions=NO_SSLv3:NO_SSLv2
#cache_peer 192.168.31.134 parent 8005 0 ssl sslflags=DONT_VERIFY_DOMAIN
proxy-only name=uathrms ssloptions=NO_SSLv3:NO_SSLv2
cache_peer_domain uathrms uathrms.hkbb.edu.hk
cache_peer 192.168.31.134 parent 8004 0 ssl sslflags=DONT_VERIFY_PEER
proxy-only   name=sithrms ssloptions=NO_SSLv3:NO_SSLv2
cache_peer_domain sithrms sithrms.hkbb.edu.hk
cache_peer 192.168.31.134 parent 8000 0 ssl sslflags=DONT_VERIFY_PEER
proxy-only  name=devhrms ssloptions=NO_SSLv3:NO_SSLv2
#cache_peer 192.168.31.134 parent 8000 0 proxy-only originserver
name=devhrms ssll sslcafile=/certs/star_ouhk_edu_hk.crt
cache_peer_domain devhrms devhrms.hkbb.edu.hk

# Create an additional ACL for local network access
acl localip src 192.168.31.0/24

# access control list
acl hrmsacl dstdomain .hkbb.edu.hk
http_access allow hrmsacl
#acl hrmsacl2 dstdomain devhrms.hkbb.edu.hk
#cache_peer_access devhrms allow hrmsacl2
cache_peer_access prdhrms allow hrmsacl
cache_peer_access uathrms allow hrmsacl
cache_peer_access sithrms allow hrmsacl
cache_peer_access devhrms allow hrmsacl
#cache_peer_access secure allow SSL_ports

# Additional ACL definitions
acl all src all
acl manager proto cache_object
acl localhost src 127.0.0.1/32
acl to_localhost dst 127.0.0.0/8 0.0.0.0/32
acl purge method PURGE
acl CONNECT method CONNECT

# Restrictions
http_access allow manager localhost
http_access deny manager
http_access allow purge localhost
http_access deny purge
http_access deny all

# Disable caching
cache deny all

logfile_rotate 10

oul163:/etc/squid # vi  squid.conf
cache_peer 192.168.31.113 parent 8001 0 proxy-only name=prdhrms
cache_peer_domain prdhrms prdhrms.hkbb.edu.hk
cache_peer 192.168.31.134 parent 8005 0 ssl sslflags=DONT_VERIFY_PEER
proxy-only name=uathrms ssloptions=NO_SSLv3:NO_SSLv2
#cache_peer 192.168.31.134 parent 8005 0 ssl sslflags=DONT_VERIFY_DOMAIN
proxy-only name=uathrms ssloptions=NO_SSLv3:NO_SSLv2
cache_peer_domain uathrms uathrms.hkbb.edu.hk
cache_peer 192.168.31.134 parent 8004 0 ssl sslflags=DONT_VERIFY_PEER
proxy-only name=sithrms ssloptions=NO_SSLv3:NO_SSLv2
cache_peer_domain sithrms sithrms.hkbb.edu.hk
cache_peer 192.168.31.134 parent 8000 0 ssl sslflags=DONT_VERIFY_PEER
proxy-only name=devhrms ssloptions=NO_SSLv3:NO_SSLv2
#cache_peer 192.168.31.134 parent 8000 0 proxy-only originserver
name=devhrms ssll sslcafile=/certs/star_ouhk_edu_hk.crt
cache_peer_domain devhrms devhrms.hkbb.edu.hk

# Create an additional ACL for local network access
acl localip src 192.168.31.0/24

# access control list
acl hrmsacl dstdomain .hkbb.edu.hk
http_access allow hrmsacl
#acl hrmsacl2 dstdomain devhrms.hkbb.edu.hk
#cache_peer_access devhrms allow hrmsacl2
cache_peer_access prdhrms allow hrmsacl
cache_peer_access uathrms allow hrmsacl
cache_peer_access sithrms allow hrmsacl
cache_peer_access devhrms allow hrmsacl
#cache_peer_access secure allow SSL_ports

# Additional ACL definitions
acl all src all
acl manager proto cache_object
acl localhost src 127.0.0.1/32
acl to_localhost dst 127.0.0.0/8 0.0.0.0/32
acl purge method PURGE
acl CONNECT method CONNECT

# Restrictions
http_access allow manager localhost
http_access deny manager
http_access allow purge localhost
http_access deny purge
http_access deny all

# Disable caching
cache deny all

logfile_rotate 10





--
Sent from: http://squid-web-proxy-cache.1019090.n4.nabble.com/Squid-Users-f1019091.html


From squid3 at treenet.co.nz  Tue Oct 23 07:05:49 2018
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Tue, 23 Oct 2018 20:05:49 +1300
Subject: [squid-users] Squid proxy not working when upgrade from 27 to
	3.5
In-Reply-To: <1540265281548-0.post@n4.nabble.com>
References: <1540265281548-0.post@n4.nabble.com>
Message-ID: <4fc98d5d-8ea8-915d-4076-b1baa35e0bee@treenet.co.nz>

On 23/10/18 4:28 PM, Angus J. wrote:
> Squid proxy not working when upgrade from 27 to 3.5
> 

Please run "squid -k parse" with the new Squid version. It is especially
important when jumping many versions like a 2.x to 3.5 does.

All issues it highlights as FATAL and ERROR must be fixed before you can
expect Squid to run properly. Anything labeled WARNING should also be
looked into and fixed where possible to avoid odd or annoying behaviours.

Have you checked the release notes for all the skipped Squid-3.x versions?
 While Squid operates mostly the same there have been some significant
changes to both HTTP and TLS/SSL in the last decade that result in some
very different visible behaviours at times.



If the problem(s) remain after doing the above please explain "not working".

What you do see _exactly_ which makes you think something is going
wrong? we need details of the problem to provide any useful help.


> Squid proxy not working when upgrade to 3.5 and it will not caching anything 
> 

The lack of caching is easily explained by reading the comments in your
config file(s):

> # Disable caching
> cache deny all

and

> # the proxy-only indicates that caching will not be performed.
> cache_peer ... proxy-only ...


You display two config files below. How does this relate to your Squid?
are you running two proxies and how are they related?


> ----------------------------------------------------------------------------
> 
> #Default:
> # windows_ipaddrchangemonitor on
> 
> visible_hostname oul163.hkbb.edu.hk
> http_port 3128 accel vhost defaultsite=oul163.hkbb.edu.hk

> https_port 80 accel cert=/etc/squid/certs/ouhk.crt
> key=/etc/squid/certs/ouhk.key defaultsite=oul163.hkbb.edu.hk vhost
> protocol=https options=NO_SSLv3:NO_SSLv2

Port 80 is a reserved port for HTTP traffic. Not for HTTPS traffic.


> https_port 8000 accel cert=/etc/squid/certs/ouhk.crt
> key=/etc/squid/certs/ouhk.key defaultsite=oul163.hkbb.edu.hk vhost
> protocol=https options=NO_SSLv3:NO_SSLv2
> #https_port 8004 accel cert=/etc/squid/certs/ouhk.crt
> key=/etc/squid/certs/ouhk.key defaultsite=oul163.hkbb.edu.hk vhost
> protocol=https options=NO_SSLv3:NO_SSLv2
> https_port 8004 accel cert=/etc/squid/certs/ouhk2.crt
> key=/etc/squid/certs/ouhk2.key defaultsite=oul163.hkbb.edu.hk vhost
> protocol=https options=NO_SSLv3:NO_SSLv2
> #https_port 8005 accel cert=/etc/squid/certs/ouhk.crt
> key=/etc/squid/certs/ouhk.key defaultsite=oul163.hkbb.edu.hk vhost
> protocol=https options=NO_SSLv3:NO_SSLv2
> https_port 8005 accel cert=/etc/squid/certs/ouhk3.crt
> key=/etc/squid/certs/ouhk3.key defaultsite=oul163.hkbb.edu.hk vhost
> protocol=https options=NO_SSLv3:NO_SSLv2

FYI: Squid does understands line wrapping in the config. For very long
lines you can end a line with slash '\' and start the next with
whitespace to make it easier to read.


> #ssl_bump allow all
> #              Disable the following one
> #ssl_bump options=NO_SSLv3
> #always_direct allow all
> #              Disable the following one
> #sslproxy_cert_error allow all
> sslproxy_options NO_SSLv3:NO_SSLv2
> access_log /var/log/squid/access.log squid
> cache_effective_user squid
> cache_log /var/log/squid/cache.log
> cache_store_log /var/log/squid/store.log
> 
> # the proxy-only indicates that caching will not be performed.
> cache_peer 192.168.31.113 parent 8001 0 proxy-only name=prdhrms
> cache_peer_domain prdhrms prdhrms.hkbb.edu.hk

> cache_peer 192.168.31.134 parent 8005 0 ssl sslflags=DONT_VERIFY_PEER
> proxy-only   name=uathrms ssloptions=NO_SSLv3:NO_SSLv2
> #cache_peer 192.168.31.134 parent 8005 0 ssl sslflags=DONT_VERIFY_DOMAIN
> proxy-only name=uathrms ssloptions=NO_SSLv3:NO_SSLv2
> cache_peer_domain uathrms uathrms.hkbb.edu.hk
> cache_peer 192.168.31.134 parent 8004 0 ssl sslflags=DONT_VERIFY_PEER
> proxy-only   name=sithrms ssloptions=NO_SSLv3:NO_SSLv2
> cache_peer_domain sithrms sithrms.hkbb.edu.hk
> cache_peer 192.168.31.134 parent 8000 0 ssl sslflags=DONT_VERIFY_PEER
> proxy-only  name=devhrms ssloptions=NO_SSLv3:NO_SSLv2
> #cache_peer 192.168.31.134 parent 8000 0 proxy-only originserver
> name=devhrms ssll sslcafile=/certs/star_ouhk_edu_hk.crt
> cache_peer_domain devhrms devhrms.hkbb.edu.hk


NP: cache_peer_domain is deprecated and has been removed from Squid-4
and later. You should replace these with cache_peer_access lines in
Squid-3 to avoid further problems on later upgrades.


Also, you are using reverse-proxy ports (accel vhost) but do not have
originserver set for any of the enabled cache_peer.

This is one of the major changes between HTTP/1.0 (Squid-2.x) and
HTTP/1.1 (Squid-3.x) that the origin servers have different syntax to
proxy traffic. Squid should be told accurately what type of peer it is
communicating with to properly optimize traffic performance and protocol
behaviours for the channel.


> 
> # Create an additional ACL for local network access
> acl localip src 192.168.31.0/24
> 

Squid-3 and later configs define the above as an ACL called "localnet".


> # access control list
> acl hrmsacl dstdomain .hkbb.edu.hk
> http_access allow hrmsacl
> #acl hrmsacl2 dstdomain devhrms.hkbb.edu.hk
> #cache_peer_access devhrms allow hrmsacl2
> cache_peer_access prdhrms allow hrmsacl
> cache_peer_access uathrms allow hrmsacl
> cache_peer_access sithrms allow hrmsacl
> cache_peer_access devhrms allow hrmsacl
> #cache_peer_access secure allow SSL_ports
> 
> # Additional ACL definitions
> acl all src all
> acl manager proto cache_object
> acl localhost src 127.0.0.1/32
> acl to_localhost dst 127.0.0.0/8 0.0.0.0/32
> acl purge method PURGE
> acl CONNECT method CONNECT


All of the above common ACL definitions are now built-in to Squid and
can be removed from the config file. They were incrementally changed
though, so see the output of squid -k parse for which ones in your
particular release.

> 
> # Restrictions
> http_access allow manager localhost
> http_access deny manager
> http_access allow purge localhost
> http_access deny purge
> http_access deny all

These can be simplified to:

 http_access deny !localhost
 http_access allow manager
 http_access allow purge
 http_access deny all

> 
> # Disable caching
> cache deny all
> 
> logfile_rotate 10
> 

The logfile_rotate default value is 10 unless your Squid has explicitly
been patched to use a different value (eg. as done by Debian/Ubuntu).

In Squid-3 and later there is no need to define anything to its default
value. So the above line can probably be removed.


The below appears to be a different config file, but contains all the
same issues with cache_peer.


> oul163:/etc/squid # vi  squid.conf
> cache_peer 192.168.31.113 parent 8001 0 proxy-only name=prdhrms
> cache_peer_domain prdhrms prdhrms.hkbb.edu.hk
> cache_peer 192.168.31.134 parent 8005 0 ssl sslflags=DONT_VERIFY_PEER
> proxy-only name=uathrms ssloptions=NO_SSLv3:NO_SSLv2
> #cache_peer 192.168.31.134 parent 8005 0 ssl sslflags=DONT_VERIFY_DOMAIN
> proxy-only name=uathrms ssloptions=NO_SSLv3:NO_SSLv2
> cache_peer_domain uathrms uathrms.hkbb.edu.hk
> cache_peer 192.168.31.134 parent 8004 0 ssl sslflags=DONT_VERIFY_PEER
> proxy-only name=sithrms ssloptions=NO_SSLv3:NO_SSLv2
> cache_peer_domain sithrms sithrms.hkbb.edu.hk
> cache_peer 192.168.31.134 parent 8000 0 ssl sslflags=DONT_VERIFY_PEER
> proxy-only name=devhrms ssloptions=NO_SSLv3:NO_SSLv2
> #cache_peer 192.168.31.134 parent 8000 0 proxy-only originserver
> name=devhrms ssll sslcafile=/certs/star_ouhk_edu_hk.crt
> cache_peer_domain devhrms devhrms.hkbb.edu.hk
> 
> # Create an additional ACL for local network access
> acl localip src 192.168.31.0/24
> 
> # access control list
> acl hrmsacl dstdomain .hkbb.edu.hk
> http_access allow hrmsacl
> #acl hrmsacl2 dstdomain devhrms.hkbb.edu.hk
> #cache_peer_access devhrms allow hrmsacl2
> cache_peer_access prdhrms allow hrmsacl
> cache_peer_access uathrms allow hrmsacl
> cache_peer_access sithrms allow hrmsacl
> cache_peer_access devhrms allow hrmsacl
> #cache_peer_access secure allow SSL_ports
> 
> # Additional ACL definitions
> acl all src all
> acl manager proto cache_object
> acl localhost src 127.0.0.1/32
> acl to_localhost dst 127.0.0.0/8 0.0.0.0/32
> acl purge method PURGE
> acl CONNECT method CONNECT
> 
> # Restrictions
> http_access allow manager localhost
> http_access deny manager
> http_access allow purge localhost
> http_access deny purge
> http_access deny all
> 
> # Disable caching
> cache deny all
> 
> logfile_rotate 10
> 
> 


Amos



From ajiang at ouhk.edu.hk  Tue Oct 23 08:52:08 2018
From: ajiang at ouhk.edu.hk (Angus J.)
Date: Tue, 23 Oct 2018 03:52:08 -0500 (CDT)
Subject: [squid-users] Squid proxy not working when upgrade from 27 to
	3.5
In-Reply-To: <4fc98d5d-8ea8-915d-4076-b1baa35e0bee@treenet.co.nz>
References: <1540265281548-0.post@n4.nabble.com>
 <4fc98d5d-8ea8-915d-4076-b1baa35e0bee@treenet.co.nz>
Message-ID: <1540284728102-0.post@n4.nabble.com>

The result of squid -k parse

oul163:/etc/squid # squid -k parse
2018/10/23 16:51:05| Startup: Initializing Authentication Schemes ...
2018/10/23 16:51:05| Startup: Initialized Authentication Scheme 'basic'
2018/10/23 16:51:05| Startup: Initialized Authentication Scheme 'digest'
2018/10/23 16:51:05| Startup: Initialized Authentication Scheme 'negotiate'
2018/10/23 16:51:05| Startup: Initialized Authentication Scheme 'ntlm'
2018/10/23 16:51:05| Startup: Initialized Authentication.
2018/10/23 16:51:05| Processing Configuration File: /etc/squid/squid.conf
(depth 0)
2018/10/23 16:51:05| Processing: acl localnet src 10.0.0.0/8
2018/10/23 16:51:05| Processing: acl localnet src 172.16.0.0/12
2018/10/23 16:51:05| Processing: acl localnet src 192.168.0.0/16
2018/10/23 16:51:05| Processing: acl localnet src fc00::/7
2018/10/23 16:51:05| Processing: acl localnet src fe80::/10
2018/10/23 16:51:05| Processing: acl localnet src 192.168.31.0/24
2018/10/23 16:51:05| WARNING: (A) '192.168.31.0/24' is a subnetwork of (B)
'192.168.0.0/16'
2018/10/23 16:51:05| WARNING: because of this '192.168.31.0/24' is ignored
to keep splay tree searching predictable
2018/10/23 16:51:05| WARNING: You should probably remove '192.168.31.0/24'
from the ACL named 'localnet'
2018/10/23 16:51:05| Processing: acl SSL_ports port 443 8000 8004 8005
2018/10/23 16:51:05| Processing: acl Safe_ports port 80
2018/10/23 16:51:05| Processing: acl Safe_ports port 21
2018/10/23 16:51:05| Processing: acl Safe_ports port 443
2018/10/23 16:51:05| Processing: acl Safe_ports port 70
2018/10/23 16:51:05| Processing: acl Safe_ports port 210
2018/10/23 16:51:05| Processing: acl Safe_ports port 1025-65535
2018/10/23 16:51:05| Processing: acl Safe_ports port 280
2018/10/23 16:51:05| Processing: acl Safe_ports port 488
2018/10/23 16:51:05| Processing: acl Safe_ports port 591
2018/10/23 16:51:05| Processing: acl Safe_ports port 777
2018/10/23 16:51:05| Processing: acl CONNECT method CONNECT
2018/10/23 16:51:05| Processing: access_log /var/log/squid/access.log
2018/10/23 16:51:05| Processing: http_access allow localnet
2018/10/23 16:51:05| Processing: http_access allow localhost
2018/10/23 16:51:05| Processing: http_port 3128
2018/10/23 16:51:05| Processing: coredump_dir /var/cache/squid
2018/10/23 16:51:05| Processing: refresh_pattern ^ftp: 1440 20 10080
2018/10/23 16:51:05| Processing: refresh_pattern ^gopher: 1440 0 1440
2018/10/23 16:51:05| Processing: refresh_pattern -i  (/cgi-bin/|\?) 0 0 0
2018/10/23 16:51:05| Processing: refresh_pattern . 0 20 4320
2018/10/23 16:51:05| Processing: cache_dir ufs /var/cache/squid 100 16 256
2018/10/23 16:51:05| Processing: cache_log /var/log/squid/cache.log
2018/10/23 16:51:05| Processing: cache_mem 8 MB
2018/10/23 16:51:05| Processing: cache_mgr webmaster
2018/10/23 16:51:05| Processing: cache_replacement_policy lru
2018/10/23 16:51:05| Processing: cache_store_log /var/log/squid/store.log
2018/10/23 16:51:05| Processing: cache_swap_high 95
2018/10/23 16:51:05| Processing: cache_swap_low 90
2018/10/23 16:51:05| Processing: client_lifetime 1 days
2018/10/23 16:51:05| Processing: connect_timeout 2 minutes
2018/10/23 16:51:05| Processing: error_directory /usr/share/squid/errors/en
2018/10/23 16:51:05| Processing: ftp_passive on
2018/10/23 16:51:05| Processing: maximum_object_size 4096 KB
2018/10/23 16:51:05| Processing: memory_replacement_policy lru
2018/10/23 16:51:05| Processing: minimum_object_size 0 KB
2018/10/23 16:51:05| Processing: visible_hostname oul299.ouhk.edu.hk
2018/10/23 16:51:05| Processing: http_port 3128 accel vhost
defaultsite=oul163.ouhk.edu.hk
2018/10/23 16:51:05| Processing: https_port 80 accel
cert=/etc/squid/certs/ouhk.crt key=/etc/squid/certs/ouhk.key
defaultsite=oul163.ouhk.edu.hk vhost protocol=https
options=NO_SSLv3:NO_SSLv2
2018/10/23 16:51:05| Processing: https_port 8000 accel
cert=/etc/squid/certs/ouhk.crt key=/etc/squid/certs/ouhk.key
defaultsite=oul163.ouhk.edu.hk vhost protocol=https
options=NO_SSLv3:NO_SSLv2
2018/10/23 16:51:05| Processing: https_port 8004 accel
cert=/etc/squid/certs/ouhk2.crt key=/etc/squid/certs/ouhk2.key
defaultsite=oul163.ouhk.edu.hk vhost protocol=https
options=NO_SSLv3:NO_SSLv2
2018/10/23 16:51:05| Processing: https_port 8005 accel
cert=/etc/squid/certs/ouhk3.crt key=/etc/squid/certs/ouhk3.key
defaultsite=oul163.ouhk.edu.hk vhost protocol=https
options=NO_SSLv3:NO_SSLv2
2018/10/23 16:51:05| Processing: sslproxy_options NO_SSLv3:NO_SSLv2
2018/10/23 16:51:05| Processing: access_log /var/log/squid/access.log squid
2018/10/23 16:51:05| Processing: cache_effective_user squid
2018/10/23 16:51:05| Processing: cache_log /var/log/squid/cache.log
2018/10/23 16:51:05| Processing: cache_store_log /var/log/squid/store.log
2018/10/23 16:51:05| Processing: cache_peer 192.168.31.113 parent 8001 0
proxy-only name=prdhrms
2018/10/23 16:51:05| Processing: cache_peer_domain prdhrms
prdhrms.ouhk.edu.hk
2018/10/23 16:51:05| Processing: cache_peer 192.168.31.134 parent 8005 0 ssl
sslflags=DONT_VERIFY_PEER proxy-only name=uathrms
ssloptions=NO_SSLv3:NO_SSLv2
2018/10/23 16:51:05| Processing: cache_peer_domain uathrms
uathrms.ouhk.edu.hk
2018/10/23 16:51:05| Processing: cache_peer 192.168.31.134 parent 8004 0 ssl
sslflags=DONT_VERIFY_PEER proxy-only name=sithrms
ssloptions=NO_SSLv3:NO_SSLv2
2018/10/23 16:51:05| Processing: cache_peer_domain sithrms
sithrms.ouhk.edu.hk
2018/10/23 16:51:05| Processing: cache_peer 192.168.31.134 parent 8000 0 ssl
sslflags=DONT_VERIFY_PEER proxy-only name=devhrms
ssloptions=NO_SSLv3:NO_SSLv2
2018/10/23 16:51:05| Processing: cache_peer_domain devhrms
devhrms.ouhk.edu.hk
2018/10/23 16:51:05| Processing: acl localip src 192.168.31.0/24
2018/10/23 16:51:05| Processing: acl hrmsacl dstdomain .ouhk.edu.hk
2018/10/23 16:51:05| Processing: http_access allow hrmsacl
2018/10/23 16:51:05| Processing: cache_peer_access prdhrms allow hrmsacl
2018/10/23 16:51:05| Processing: cache_peer_access uathrms allow hrmsacl
2018/10/23 16:51:05| Processing: cache_peer_access sithrms allow hrmsacl
2018/10/23 16:51:05| Processing: cache_peer_access devhrms allow hrmsacl
2018/10/23 16:51:05| Processing: acl all src all
2018/10/23 16:51:05| WARNING: (B) '::/0' is a subnetwork of (A) '::/0'
2018/10/23 16:51:05| WARNING: because of this '::/0' is ignored to keep
splay tree searching predictable
2018/10/23 16:51:05| WARNING: You should probably remove '::/0' from the ACL
named 'all'
2018/10/23 16:51:05| Processing: acl manager proto cache_object
2018/10/23 16:51:05| UPGRADE: ACL 'manager' is now a built-in ACL. Remove it
from your config file.
2018/10/23 16:51:05| Processing: acl localhost src 127.0.0.1/32
2018/10/23 16:51:05| WARNING: (B) '127.0.0.1' is a subnetwork of (A)
'127.0.0.1'
2018/10/23 16:51:05| WARNING: because of this '127.0.0.1' is ignored to keep
splay tree searching predictable
2018/10/23 16:51:05| WARNING: You should probably remove '127.0.0.1' from
the ACL named 'localhost'
2018/10/23 16:51:05| WARNING: (B) '127.0.0.1' is a subnetwork of (A)
'127.0.0.1'
2018/10/23 16:51:05| WARNING: because of this '127.0.0.1' is ignored to keep
splay tree searching predictable
2018/10/23 16:51:05| WARNING: You should probably remove '127.0.0.1' from
the ACL named 'localhost'
2018/10/23 16:51:05| Processing: acl to_localhost dst 127.0.0.0/8 0.0.0.0/32
2018/10/23 16:51:05| WARNING: (B) '127.0.0.0/8' is a subnetwork of (A)
'127.0.0.0/8'
2018/10/23 16:51:05| WARNING: because of this '127.0.0.0/8' is ignored to
keep splay tree searching predictable
2018/10/23 16:51:05| WARNING: You should probably remove '127.0.0.0/8' from
the ACL named 'to_localhost'
2018/10/23 16:51:05| WARNING: (B) '0.0.0.0' is a subnetwork of (A) '0.0.0.0'
2018/10/23 16:51:05| WARNING: because of this '0.0.0.0' is ignored to keep
splay tree searching predictable
2018/10/23 16:51:05| WARNING: You should probably remove '0.0.0.0' from the
ACL named 'to_localhost'
2018/10/23 16:51:05| WARNING: (B) '0.0.0.0' is a subnetwork of (A) '0.0.0.0'
2018/10/23 16:51:05| WARNING: because of this '0.0.0.0' is ignored to keep
splay tree searching predictable
2018/10/23 16:51:05| WARNING: You should probably remove '0.0.0.0' from the
ACL named 'to_localhost'
2018/10/23 16:51:05| Processing: acl purge method PURGE
2018/10/23 16:51:05| Processing: acl CONNECT method CONNECT
2018/10/23 16:51:05| Processing: http_access allow manager localhost
2018/10/23 16:51:05| Processing: http_access deny manager
2018/10/23 16:51:05| Processing: http_access allow purge localhost
2018/10/23 16:51:05| Processing: http_access deny purge
2018/10/23 16:51:05| Processing: http_access deny all
2018/10/23 16:51:05| Processing: logfile_rotate 10
2018/10/23 16:51:05| Initializing https proxy context
2018/10/23 16:51:05| Initializing cache_peer uathrms SSL context
2018/10/23 16:51:05| Initializing cache_peer sithrms SSL context
2018/10/23 16:51:05| Initializing cache_peer devhrms SSL context
2018/10/23 16:51:05| Initializing https_port [::]:80 SSL context
2018/10/23 16:51:05| Using certificate in /etc/squid/certs/ouhk.crt
2018/10/23 16:51:05| Initializing https_port [::]:8000 SSL context
2018/10/23 16:51:05| Using certificate in /etc/squid/certs/ouhk.crt
2018/10/23 16:51:05| Initializing https_port [::]:8004 SSL context
2018/10/23 16:51:05| Using certificate in /etc/squid/certs/ouhk2.crt
2018/10/23 16:51:05| Initializing https_port [::]:8005 SSL context
2018/10/23 16:51:05| Using certificate in /etc/squid/certs/ouhk3.crt




--
Sent from: http://squid-web-proxy-cache.1019090.n4.nabble.com/Squid-Users-f1019091.html


From uhlar at fantomas.sk  Tue Oct 23 10:00:39 2018
From: uhlar at fantomas.sk (Matus UHLAR - fantomas)
Date: Tue, 23 Oct 2018 12:00:39 +0200
Subject: [squid-users] Squid proxy not working when upgrade from 27 to
	3.5
In-Reply-To: <1540284728102-0.post@n4.nabble.com>
References: <1540265281548-0.post@n4.nabble.com>
 <4fc98d5d-8ea8-915d-4076-b1baa35e0bee@treenet.co.nz>
 <1540284728102-0.post@n4.nabble.com>
Message-ID: <20181023100039.a3tptxvtpkvggu3h@fantomas.sk>

What does mean "Squid proxy not working"?
On 23.10.18 03:52, Angus J. wrote:
>The result of squid -k parse

did you read this output?
did you do other thing Amos has recommended to you?

according to logs down, squid DOES listen for connections.

>oul163:/etc/squid # squid -k parse

>2018/10/23 16:51:05| Processing: acl localnet src 192.168.0.0/16

>2018/10/23 16:51:05| Processing: acl localnet src 192.168.31.0/24
>2018/10/23 16:51:05| WARNING: (A) '192.168.31.0/24' is a subnetwork of (B)
>'192.168.0.0/16'
>2018/10/23 16:51:05| WARNING: because of this '192.168.31.0/24' is ignored
>to keep splay tree searching predictable
>2018/10/23 16:51:05| WARNING: You should probably remove '192.168.31.0/24'
>from the ACL named 'localnet'

... there's no point in adding 192.168.0.0/16 and 192.168.31.0/24 both
- squid recommends you to remove 192.168.31.0/24

>2018/10/23 16:51:05| Processing: acl all src all

>2018/10/23 16:51:05| WARNING: (B) '::/0' is a subnetwork of (A) '::/0'
>2018/10/23 16:51:05| WARNING: because of this '::/0' is ignored to keep
>splay tree searching predictable
>2018/10/23 16:51:05| WARNING: You should probably remove '::/0' from the ACL
>named 'all'

acl "all" is built-in, you don't have to define it.


>2018/10/23 16:51:05| Processing: acl manager proto cache_object
>2018/10/23 16:51:05| UPGRADE: ACL 'manager' is now a built-in ACL. Remove it
>from your config file.

...the same applies for "manager" acl.

>2018/10/23 16:51:05| Processing: acl localhost src 127.0.0.1/32
>2018/10/23 16:51:05| WARNING: (B) '127.0.0.1' is a subnetwork of (A)
>'127.0.0.1'
>2018/10/23 16:51:05| WARNING: because of this '127.0.0.1' is ignored to keep
>splay tree searching predictable
>2018/10/23 16:51:05| WARNING: You should probably remove '127.0.0.1' from
>the ACL named 'localhost'

seems that you have localhost defined two times. 

>2018/10/23 16:51:05| Processing: acl to_localhost dst 127.0.0.0/8 0.0.0.0/32
>2018/10/23 16:51:05| WARNING: (B) '127.0.0.0/8' is a subnetwork of (A)
>'127.0.0.0/8'
>2018/10/23 16:51:05| WARNING: because of this '127.0.0.0/8' is ignored to
>keep splay tree searching predictable
>2018/10/23 16:51:05| WARNING: You should probably remove '127.0.0.0/8' from
>the ACL named 'to_localhost'

... and same applies to to_localhost


-- 
Matus UHLAR - fantomas, uhlar at fantomas.sk ; http://www.fantomas.sk/
Warning: I wish NOT to receive e-mail advertising to this address.
Varovanie: na tuto adresu chcem NEDOSTAVAT akukolvek reklamnu postu.
My mind is like a steel trap - rusty and illegal in 37 states. 


From squid3 at treenet.co.nz  Tue Oct 23 10:06:45 2018
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Tue, 23 Oct 2018 23:06:45 +1300
Subject: [squid-users] Squid 4.3 assertion failed
In-Reply-To: <76aa8880-3651-67f0-d93a-c17701ffa551@measurement-factory.com>
References: <001c01d46a14$dd4083f0$97c18bd0$@yahoo.com.ar>
 <76aa8880-3651-67f0-d93a-c17701ffa551@measurement-factory.com>
Message-ID: <063f059c-a420-47da-3b8f-02d05b059aec@treenet.co.nz>

On 23/10/18 6:45 AM, Alex Rousskov wrote:
> On 10/22/18 8:38 AM, Julian Perconti wrote:
> 
>> assertion failed: http.cc:1530: "!Comm::MonitorsRead(serverConnection->fd)"
> 
>> Any idea?
> 
> Without the stack trace, it is difficult to say much about this bug.
> Please collect a stack trace from the crash and post it to Squid
> bugzilla. If the stack trace looks similar to that in Bug 4896, please
> post it there: https://bugs.squid-cache.org/show_bug.cgi?id=4896
> 
> If you do not know how to enable core dumps and/or how to collect a
> stack trace, please search Squid wiki for pointers or consult your local
> sysadmin.
> 


FWIW I'm not seeing any changes in those Squid versions which relate to
persistent connection or pipeline handling.

There is <https://bugs.squid-cache.org/show_bug.cgi?id=4673> from last
year which still lacks any hints about what might be causing it.


FYI: Details on obtaining backtrace info can be found at
<https://wiki.squid-cache.org/SquidFaq/BugReporting>. This issue though
spans multiple transactions. So we will need a detailed (ALL,9 if
possible) cache.log trace of everything that FD has been used for in
addition to the trace. The trace itself is useful for identifying the
code path and FD value to look at within that log.

Amos


From ajiang at ouhk.edu.hk  Tue Oct 23 10:13:14 2018
From: ajiang at ouhk.edu.hk (Angus J.)
Date: Tue, 23 Oct 2018 05:13:14 -0500 (CDT)
Subject: [squid-users] Squid proxy not working when upgrade from 27 to
	3.5
In-Reply-To: <20181023100039.a3tptxvtpkvggu3h@fantomas.sk>
References: <1540265281548-0.post@n4.nabble.com>
 <4fc98d5d-8ea8-915d-4076-b1baa35e0bee@treenet.co.nz>
 <1540284728102-0.post@n4.nabble.com>
 <20181023100039.a3tptxvtpkvggu3h@fantomas.sk>
Message-ID: <1540289594180-0.post@n4.nabble.com>

Hi

I have revised the squid.conf

oul163:/etc/squid # squid -k parse
2018/10/23 18:12:35| Startup: Initializing Authentication Schemes ...
2018/10/23 18:12:35| Startup: Initialized Authentication Scheme 'basic'
2018/10/23 18:12:35| Startup: Initialized Authentication Scheme 'digest'
2018/10/23 18:12:35| Startup: Initialized Authentication Scheme 'negotiate'
2018/10/23 18:12:35| Startup: Initialized Authentication Scheme 'ntlm'
2018/10/23 18:12:35| Startup: Initialized Authentication.
2018/10/23 18:12:35| Processing Configuration File: /etc/squid/squid.conf
(depth 0)
2018/10/23 18:12:35| Processing: acl localnet src 10.0.0.0/8
2018/10/23 18:12:35| Processing: acl localnet src 172.16.0.0/12
2018/10/23 18:12:35| Processing: acl localnet src 192.168.0.0/16
2018/10/23 18:12:35| Processing: acl localnet src fc00::/7
2018/10/23 18:12:35| Processing: acl localnet src fe80::/10
2018/10/23 18:12:35| Processing: acl SSL_ports port 443 8000 8004 8005
2018/10/23 18:12:35| Processing: acl Safe_ports port 80
2018/10/23 18:12:35| Processing: acl Safe_ports port 21
2018/10/23 18:12:35| Processing: acl Safe_ports port 443
2018/10/23 18:12:35| Processing: acl Safe_ports port 70
2018/10/23 18:12:35| Processing: acl Safe_ports port 210
2018/10/23 18:12:35| Processing: acl Safe_ports port 1025-65535
2018/10/23 18:12:35| Processing: acl Safe_ports port 280
2018/10/23 18:12:35| Processing: acl Safe_ports port 488
2018/10/23 18:12:35| Processing: acl Safe_ports port 591
2018/10/23 18:12:35| Processing: acl Safe_ports port 777
2018/10/23 18:12:35| Processing: acl CONNECT method CONNECT
2018/10/23 18:12:35| Processing: access_log /var/log/squid/access.log
2018/10/23 18:12:35| Processing: http_access allow localnet
2018/10/23 18:12:35| Processing: http_access allow localhost
2018/10/23 18:12:35| Processing: http_port 3128
2018/10/23 18:12:35| Processing: coredump_dir /var/cache/squid
2018/10/23 18:12:35| Processing: refresh_pattern ^ftp: 1440 20 10080
2018/10/23 18:12:35| Processing: refresh_pattern ^gopher: 1440 0 1440
2018/10/23 18:12:35| Processing: refresh_pattern -i  (/cgi-bin/|\?) 0 0 0
2018/10/23 18:12:35| Processing: refresh_pattern . 0 20 4320
2018/10/23 18:12:35| Processing: cache_dir ufs /var/cache/squid 100 16 256
2018/10/23 18:12:35| Processing: cache_log /var/log/squid/cache.log
2018/10/23 18:12:35| Processing: cache_mem 8 MB
2018/10/23 18:12:35| Processing: cache_mgr webmaster
2018/10/23 18:12:35| Processing: cache_replacement_policy lru
2018/10/23 18:12:35| Processing: cache_store_log /var/log/squid/store.log
2018/10/23 18:12:35| Processing: cache_swap_high 95
2018/10/23 18:12:35| Processing: cache_swap_low 90
2018/10/23 18:12:35| Processing: client_lifetime 1 days
2018/10/23 18:12:35| Processing: connect_timeout 2 minutes
2018/10/23 18:12:35| Processing: error_directory /usr/share/squid/errors/en
2018/10/23 18:12:35| Processing: ftp_passive on
2018/10/23 18:12:35| Processing: maximum_object_size 4096 KB
2018/10/23 18:12:35| Processing: memory_replacement_policy lru
2018/10/23 18:12:35| Processing: minimum_object_size 0 KB
2018/10/23 18:12:35| Processing: visible_hostname oul299.ouhk.edu.hk
2018/10/23 18:12:35| Processing: http_port 3128 accel vhost
defaultsite=oul163.ouhk.edu.hk
2018/10/23 18:12:35| Processing: https_port 80 accel
cert=/etc/squid/certs/ouhk.crt key=/etc/squid/certs/ouhk.key
defaultsite=oul163.ouhk.edu.hk vhost protocol=https
options=NO_SSLv3:NO_SSLv2
2018/10/23 18:12:35| Processing: https_port 8000 accel
cert=/etc/squid/certs/ouhk.crt key=/etc/squid/certs/ouhk.key
defaultsite=oul163.ouhk.edu.hk vhost protocol=https
options=NO_SSLv3:NO_SSLv2
2018/10/23 18:12:35| Processing: https_port 8004 accel
cert=/etc/squid/certs/ouhk2.crt key=/etc/squid/certs/ouhk2.key
defaultsite=oul163.ouhk.edu.hk vhost protocol=https
options=NO_SSLv3:NO_SSLv2
2018/10/23 18:12:35| Processing: https_port 8005 accel
cert=/etc/squid/certs/ouhk3.crt key=/etc/squid/certs/ouhk3.key
defaultsite=oul163.ouhk.edu.hk vhost protocol=https
options=NO_SSLv3:NO_SSLv2
2018/10/23 18:12:35| Processing: sslproxy_options NO_SSLv3:NO_SSLv2
2018/10/23 18:12:35| Processing: access_log /var/log/squid/access.log squid
2018/10/23 18:12:35| Processing: cache_effective_user squid
2018/10/23 18:12:35| Processing: cache_log /var/log/squid/cache.log
2018/10/23 18:12:35| Processing: cache_store_log /var/log/squid/store.log
2018/10/23 18:12:35| Processing: cache_peer 192.168.31.113 parent 8001 0
proxy-only name=prdhrms
2018/10/23 18:12:35| Processing: cache_peer_domain prdhrms
prdhrms.ouhk.edu.hk
2018/10/23 18:12:35| Processing: cache_peer 192.168.31.134 parent 8005 0 ssl
sslflags=DONT_VERIFY_PEER proxy-only name=uathrms
ssloptions=NO_SSLv3:NO_SSLv2
2018/10/23 18:12:35| Processing: cache_peer_domain uathrms
uathrms.ouhk.edu.hk
2018/10/23 18:12:35| Processing: cache_peer 192.168.31.134 parent 8004 0 ssl
sslflags=DONT_VERIFY_PEER proxy-only name=sithrms
ssloptions=NO_SSLv3:NO_SSLv2
2018/10/23 18:12:35| Processing: cache_peer_domain sithrms
sithrms.ouhk.edu.hk
2018/10/23 18:12:35| Processing: cache_peer 192.168.31.134 parent 8000 0 ssl
sslflags=DONT_VERIFY_PEER proxy-only name=devhrms
ssloptions=NO_SSLv3:NO_SSLv2
2018/10/23 18:12:35| Processing: cache_peer_domain devhrms
devhrms.ouhk.edu.hk
2018/10/23 18:12:35| Processing: acl localip src 192.168.31.0/24
2018/10/23 18:12:35| Processing: acl hrmsacl dstdomain .ouhk.edu.hk
2018/10/23 18:12:35| Processing: http_access allow hrmsacl
2018/10/23 18:12:35| Processing: cache_peer_access prdhrms allow hrmsacl
2018/10/23 18:12:35| Processing: cache_peer_access uathrms allow hrmsacl
2018/10/23 18:12:35| Processing: cache_peer_access sithrms allow hrmsacl
2018/10/23 18:12:35| Processing: cache_peer_access devhrms allow hrmsacl
2018/10/23 18:12:35| Processing: acl purge method PURGE
2018/10/23 18:12:35| Processing: acl CONNECT method CONNECT
2018/10/23 18:12:35| Processing: http_access deny all
2018/10/23 18:12:35| Processing: logfile_rotate 10
2018/10/23 18:12:35| Initializing https proxy context
2018/10/23 18:12:35| Initializing cache_peer uathrms SSL context
2018/10/23 18:12:35| Initializing cache_peer sithrms SSL context
2018/10/23 18:12:35| Initializing cache_peer devhrms SSL context
2018/10/23 18:12:35| Initializing https_port [::]:80 SSL context
2018/10/23 18:12:35| Using certificate in /etc/squid/certs/ouhk.crt
2018/10/23 18:12:35| Initializing https_port [::]:8000 SSL context
2018/10/23 18:12:35| Using certificate in /etc/squid/certs/ouhk.crt
2018/10/23 18:12:35| Initializing https_port [::]:8004 SSL context
2018/10/23 18:12:35| Using certificate in /etc/squid/certs/ouhk2.crt
2018/10/23 18:12:35| Initializing https_port [::]:8005 SSL context
2018/10/23 18:12:35| Using certificate in /etc/squid/certs/ouhk3.crt




--
Sent from: http://squid-web-proxy-cache.1019090.n4.nabble.com/Squid-Users-f1019091.html


From squid3 at treenet.co.nz  Tue Oct 23 11:06:29 2018
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Wed, 24 Oct 2018 00:06:29 +1300
Subject: [squid-users] Squid proxy not working when upgrade from 27 to
	3.5
In-Reply-To: <1540289594180-0.post@n4.nabble.com>
References: <1540265281548-0.post@n4.nabble.com>
 <4fc98d5d-8ea8-915d-4076-b1baa35e0bee@treenet.co.nz>
 <1540284728102-0.post@n4.nabble.com>
 <20181023100039.a3tptxvtpkvggu3h@fantomas.sk>
 <1540289594180-0.post@n4.nabble.com>
Message-ID: <caf407eb-9eb6-ea5d-2da9-446e3d1f80b7@treenet.co.nz>

On 23/10/18 11:13 PM, Angus J. wrote:
> Hi
> 
> I have revised the squid.conf
> 


But still no hints about what "not working" means?


Amos


From vh1988 at yahoo.com.ar  Tue Oct 23 12:55:29 2018
From: vh1988 at yahoo.com.ar (Julian Perconti)
Date: Tue, 23 Oct 2018 09:55:29 -0300
Subject: [squid-users] Squid 4.3 assertion failed
In-Reply-To: <063f059c-a420-47da-3b8f-02d05b059aec@treenet.co.nz>
References: <001c01d46a14$dd4083f0$97c18bd0$@yahoo.com.ar>
 <76aa8880-3651-67f0-d93a-c17701ffa551@measurement-factory.com>
 <063f059c-a420-47da-3b8f-02d05b059aec@treenet.co.nz>
Message-ID: <001101d46acf$af68ea10$0e3abe30$@yahoo.com.ar>

> >> assertion failed: http.cc:1530: "!Comm::MonitorsRead(serverConnection-
> >fd)"
> >
> >> Any idea?
> >
> > Without the stack trace, it is difficult to say much about this bug.
> > Please collect a stack trace from the crash and post it to Squid
> > bugzilla. If the stack trace looks similar to that in Bug 4896, please
> > post it there: https://bugs.squid-cache.org/show_bug.cgi?id=4896
> >
> > If you do not know how to enable core dumps and/or how to collect a
> > stack trace, please search Squid wiki for pointers or consult your
> > local sysadmin.
> >
> 
> 
> FWIW I'm not seeing any changes in those Squid versions which relate to
> persistent connection or pipeline handling.
> 
> There is <https://bugs.squid-cache.org/show_bug.cgi?id=4673> from last
> year which still lacks any hints about what might be causing it.
> 
> 
> FYI: Details on obtaining backtrace info can be found at <https://wiki.squid-
> cache.org/SquidFaq/BugReporting>. This issue though spans multiple
> transactions. So we will need a detailed (ALL,9 if
> possible) cache.log trace of everything that FD has been used for in addition
> to the trace. The trace itself is useful for identifying the code path and FD
> value to look at within that log.
> 
> Amos

Hi Alex/Amos

Since yesterday squid is running via this method in a cron script:

trap "rm -f $$.gdb" 0
cat <<EOF >$$.gdb
handle SIGPIPE pass nostop noprint
handle SIGTERM pass nostop noprint
handle SIGUSR1 pass nostop noprint
handle SIGHUP  pass
handle SIGKILL pass
handle SIGSEGV stop
handle SIGABRT stop
run -NYCd3
backtrace
quit
EOF
while sleep 2; do
  gdb -x $$.gdb /path/to/squid 2>&1 | tee -a squid.out
done

For now (see below) squid has not generated the error / bug and I see which seems to be the cache.log in squid.out file.
Because sometimes squid works well for several days and other times it stays alive only a few hours, after a restart. (service or server restart)

When this bug happens I will let you know here and upload it to bugzilla

Thank you



From fredrik at pipemore.se  Tue Oct 23 14:31:11 2018
From: fredrik at pipemore.se (uppsalanet)
Date: Tue, 23 Oct 2018 09:31:11 -0500 (CDT)
Subject: [squid-users] redirect based on url (302)
In-Reply-To: <f35964b6-e83b-c5b6-5563-602ab666d964@treenet.co.nz>
References: <1537541034108-0.post@n4.nabble.com>
 <dde485b6-5742-d424-8e12-f700d0b975f2@treenet.co.nz>
 <1537771119646-0.post@n4.nabble.com>
 <f35964b6-e83b-c5b6-5563-602ab666d964@treenet.co.nz>
Message-ID: <1540305071062-0.post@n4.nabble.com>

Thanks Amos for all you help.
I've done a few of your suggested steps:
* Create the databas.
createdb.sql
<http://squid-web-proxy-cache.1019090.n4.nabble.com/file/t377569/createdb.sql>  
* The acl to fill upp the database with values works fine :-)
external_acl_type whitelist_add ttl=10 %SRC %<{Location}
/etc/squid/add2db.pl
add2db.pl
<http://squid-web-proxy-cache.1019090.n4.nabble.com/file/t377569/add2db.pl>  

So now i fill up the database with records like this:
dbdump.txt
<http://squid-web-proxy-cache.1019090.n4.nabble.com/file/t377569/dbdump.txt>  

My question is how i get the domains out from it? I don't really under stand
this part:
 external_acl_type whitelist ttl=60 %SRC %DST \
   /usr/lib/squid/ext_session_db_acl \
   --dsn ... --user ... --password ... \
   --table sessions --cond "" 

Do I need to write another script for that
"/usr/lib/squid/ext_session_db_acl" 

squid -v
squid_version.txt
<http://squid-web-proxy-cache.1019090.n4.nabble.com/file/t377569/squid_version.txt>  

Thanks in advance
Fredrik



--
Sent from: http://squid-web-proxy-cache.1019090.n4.nabble.com/Squid-Users-f1019091.html


From squid3 at treenet.co.nz  Tue Oct 23 15:06:03 2018
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Wed, 24 Oct 2018 04:06:03 +1300
Subject: [squid-users] redirect based on url (302)
In-Reply-To: <1540305071062-0.post@n4.nabble.com>
References: <1537541034108-0.post@n4.nabble.com>
 <dde485b6-5742-d424-8e12-f700d0b975f2@treenet.co.nz>
 <1537771119646-0.post@n4.nabble.com>
 <f35964b6-e83b-c5b6-5563-602ab666d964@treenet.co.nz>
 <1540305071062-0.post@n4.nabble.com>
Message-ID: <a2ca5119-39c1-c744-0e8b-6373f7f525a3@treenet.co.nz>

On 24/10/18 3:31 AM, uppsalanet wrote:
> Thanks Amos for all you help.
> I've done a few of your suggested steps:
> * Create the databas.
> createdb.sql
> <http://squid-web-proxy-cache.1019090.n4.nabble.com/file/t377569/createdb.sql>  
> * The acl to fill upp the database with values works fine :-)
> external_acl_type whitelist_add ttl=10 %SRC %<{Location}
> /etc/squid/add2db.pl
> add2db.pl
> <http://squid-web-proxy-cache.1019090.n4.nabble.com/file/t377569/add2db.pl>  
> 
> So now i fill up the database with records like this:
> dbdump.txt
> <http://squid-web-proxy-cache.1019090.n4.nabble.com/file/t377569/dbdump.txt>  
> 
> My question is how i get the domains out from it? I don't really under stand
> this part:
>  external_acl_type whitelist ttl=60 %SRC %DST \
>    /usr/lib/squid/ext_session_db_acl \
>    --dsn ... --user ... --password ... \
>    --table sessions --cond "" 
> 
> Do I need to write another script for that
> "/usr/lib/squid/ext_session_db_acl" 

Nope, Squid should have come with that helper. It may not be at that
exact path though.

All you should have to do now is find where that helper binary actually
is and setup those parameters so it can access your DB.

Amos


From nitu665 at gmail.com  Tue Oct 23 18:45:41 2018
From: nitu665 at gmail.com (NityaIyer)
Date: Tue, 23 Oct 2018 13:45:41 -0500 (CDT)
Subject: [squid-users] error in parsing Proxy protocol version 2 by Squid
	proxy protocol
Message-ID: <1540320341175-0.post@n4.nabble.com>

Hello,

I really need a help in this issue.

I have a squid application running on a instance  behind the Network load
balancer[NLB] in  AWS cloud. Due to my use case, I have enabled proxy
protocol on the load balancer so that my backend instance can receive the
proxy protocol header. 

Few details:
- The network load balancer is sending proxy protocol version 2 header. 
- Squid version  - 3.5.20
- TCP listening  on 3128 both load balancer and my instance

As per the release note [1], below is the configuration of my Squid
application
********************************************************************
acl abc src 10.9.0.0/21 #My local network
proxy_protocol_access allow abc
logformat squid %ts.%03tu %6tr %>a %Ss/%03>Hs %<st %rm %ru %[un %Sh/%<a %mt
%<la %<lp %<a %<p %<rd %>rd proxy_protocol_access allow abc
http_port 3128 accel require-proxy-header
http_port 3128
***********************************************************************************
On testing, I find below logs on cache file of the squid. Somehow, squid
application is not interpreting the proxy protocol header version 2[PPv2]:

2018/10/11 17:55:45 kid1| PROXY protocol error: invalid header from
local=10.9.7.165:3128 remote=10.9.7.170:43730 FD 10 flags=1

2018/10/11 17:55:45 kid1| PROXY protocol error: invalid header from
local=10.9.7.165:3128 remote=10.9.7.170:61432 FD 10 flags=1

2018/10/11 17:55:45 kid1| PROXY protocol error: invalid header from
local=10.9.7.165:3128 remote=10.9.7.170:16783 FD 10 flags=1 

10.9.7.170 is the private IP load balancer and 10.9.7.165 is instance
itself.

To be noted:
- I configured Apache on the same box and Apache can successfully parse the
proxy protocol version 2 header received from NLB. I can successfully see
client IP address on access log of Apache.
**************************************************************************************
10.9.7.170 80 18.222.29.158 45634 - - [11/Oct/2018:15:15:18 +0000] "GET /
HTTP/1.1" 200 166 "-" "curl/7.53.1"
10.9.7.170 80 18.222.29.158 45638 - - [11/Oct/2018:15:17:15 +0000] "GET /
HTTP/1.1" 200 166 "-" "curl/7.53.1"
*************************************************************************************
10.9.7.170 is private IP of my NLB and 18.222.29.158 is my dummy box i.e.
client IP address.

- I used the same squid configuration to intercept proxy protocol version 1
header and surprisingly it works for version 1 header:
******************************************************************************************************
1539282729.144 0 18.222.29.158 TCP_DENIED/403 470 HEAD
http://18.203.114.1:3128/ - HIER_NONE/- text/html - - - - 18.203.114.1
18.203.114.1 proxy_protocol_access allow abc

1539286165.754 0 18.222.29.158 TCP_DENIED/403 4027 GET
http://18.203.114.1:3128/ - HIER_NONE/- text/html - - - - 18.203.114.1
18.203.114.1 proxy_protocol_access allow abc

1539286185.310 0 18.222.29.158 TCP_DENIED/403 4027 GET
http://18.203.114.1:3128/ - HIER_NONE/- text/html - - - - 18.203.114.1
18.203.114.1 proxy_protocol_access allow abc
***************************************************************************************************
Where: 18.222.29.158 is my dummy box nothing but client IP.


On summary , based on my analysis there is something I am missing or I dont
know but squid is not intercepting with the PPv2 header. Any help is greatly
appreciated.

Thank you

Regards
Nitya 


Reference: [1]
http://www.squid-cache.org/Versions/v3/3.5/squid-3.5.20-RELEASENOTES.html#ss2.7



--
Sent from: http://squid-web-proxy-cache.1019090.n4.nabble.com/Squid-Users-f1019091.html


From timur.lagutenko at gmail.com  Tue Oct 23 19:56:43 2018
From: timur.lagutenko at gmail.com (Timur Lagutenko)
Date: Wed, 24 Oct 2018 00:56:43 +0500
Subject: [squid-users] Unable to open youtube.com
In-Reply-To: <48c0143c-ce00-0e31-1861-c753053e53c4@treenet.co.nz>
References: <CAKK+L1cX9Xau0NivY1YobYrT27csuPK-p-FV5UtZUKbf_PH_UQ@mail.gmail.com>
 <1e605630-706c-8979-c76c-e513d83c3c4f@riosoft.com.br>
 <d30bfe08-d4a1-0357-09da-e3904e5bc353@treenet.co.nz>
 <CAKK+L1eHaiS7Aq-cC=9M3denn74rzyshp2L=gV6msXbXysMHQQ@mail.gmail.com>
 <6e7a0b1a-d23d-a507-abb2-3915da215cec@treenet.co.nz>
 <CAKK+L1dW75W1oMoNCmA3nmhAaBRvCMqBObhHBbgV0sqQ407edA@mail.gmail.com>
 <CAKK+L1dpNBSrLkTaxZQO_NK-Ccs53Nho=7V98Zxt77zAd9bVLQ@mail.gmail.com>
 <CAKK+L1czUc_CV6UOX6GMK8tqU22CFQfbS8iBEW8hhfPwd7DV2A@mail.gmail.com>
 <48c0143c-ce00-0e31-1861-c753053e53c4@treenet.co.nz>
Message-ID: <CAKK+L1eGtmf_nJXu12AjnvhP5OAZtNSymYHkZMOEN7pJV-u+jw@mail.gmail.com>

Hello Dear Colleagues,

the issues with the youtube has been resolved by itself.
Seems like it was ISP tricks, or maybe something was wrong with the youtube
itself.

the short story:
youtube worked without proxy (pure NAT). Using CUIC protocol.
When client was configured to use proxy, and firewall was configured to
block access from client to internet,
youtube didn't work, other sites were working.
i even tried to install new system, but got fail.
so after few days i've found that it works.
YOUTUBE IS WORKING!!!

thanks to all
best regards
and long stable uptimes
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20181024/4fb15248/attachment.htm>

From squid3 at treenet.co.nz  Tue Oct 23 22:17:18 2018
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Wed, 24 Oct 2018 11:17:18 +1300
Subject: [squid-users] error in parsing Proxy protocol version 2 by
 Squid proxy protocol
In-Reply-To: <1540320341175-0.post@n4.nabble.com>
References: <1540320341175-0.post@n4.nabble.com>
Message-ID: <497376d5-98b9-68fd-71ff-bb1aab3c0c49@treenet.co.nz>

On 24/10/18 7:45 AM, NityaIyer wrote:
> Hello,
> 
> I really need a help in this issue.
> 
> I have a squid application running on a instance  behind the Network load
> balancer[NLB] in  AWS cloud. Due to my use case, I have enabled proxy
> protocol on the load balancer so that my backend instance can receive the
> proxy protocol header. 
> 
> Few details:
> - The network load balancer is sending proxy protocol version 2 header. 
> - Squid version  - 3.5.20
> - TCP listening  on 3128 both load balancer and my instance
> 

Please try Squid-4. One of the HTTP protections against Slow-Loris
attacks was found to conflict with the PROXYv2 detection. Squid-4
parsers have been redesigned to separate protocols better.

Amos


From rousskov at measurement-factory.com  Tue Oct 23 22:25:27 2018
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Tue, 23 Oct 2018 16:25:27 -0600
Subject: [squid-users] error in parsing Proxy protocol version 2 by
 Squid proxy protocol
In-Reply-To: <497376d5-98b9-68fd-71ff-bb1aab3c0c49@treenet.co.nz>
References: <1540320341175-0.post@n4.nabble.com>
 <497376d5-98b9-68fd-71ff-bb1aab3c0c49@treenet.co.nz>
Message-ID: <f92321c9-328a-a0bd-5f3e-16149cff65cf@measurement-factory.com>

On 10/23/18 4:17 PM, Amos Jeffries wrote:
> On 24/10/18 7:45 AM, NityaIyer wrote:
>> - The network load balancer is sending proxy protocol version 2 header. 
>> - Squid version  - 3.5.20
>> - TCP listening  on 3128 both load balancer and my instance

> Please try Squid-4. One of the HTTP protections against Slow-Loris
> attacks was found to conflict with the PROXYv2 detection. Squid-4
> parsers have been redesigned to separate protocols better.


And if Squid v4 fails, consider testing the following experimental
unofficial v5 code that enhances PROXY protocol handling while fixing a
few related bugs in the official code:
     https://github.com/measurement-factory/squid/pull/13


Good luck,

Alex.


From ajiang at ouhk.edu.hk  Wed Oct 24 03:15:49 2018
From: ajiang at ouhk.edu.hk (Angus J.)
Date: Tue, 23 Oct 2018 22:15:49 -0500 (CDT)
Subject: [squid-users] Squid proxy not working when upgrade from 27 to
	3.5
In-Reply-To: <caf407eb-9eb6-ea5d-2da9-446e3d1f80b7@treenet.co.nz>
References: <1540265281548-0.post@n4.nabble.com>
 <4fc98d5d-8ea8-915d-4076-b1baa35e0bee@treenet.co.nz>
 <1540284728102-0.post@n4.nabble.com>
 <20181023100039.a3tptxvtpkvggu3h@fantomas.sk>
 <1540289594180-0.post@n4.nabble.com>
 <caf407eb-9eb6-ea5d-2da9-446e3d1f80b7@treenet.co.nz>
Message-ID: <1540350949260-0.post@n4.nabble.com>

This error in IE browser when the connection is go through the squid proxy
server


This site can?t be reached
uathrms.oubb.edu.hk refused to connect.
Try:

Checking the connection
Checking the proxy and the firewall
ERR_CONNECTION_REFUSED



--
Sent from: http://squid-web-proxy-cache.1019090.n4.nabble.com/Squid-Users-f1019091.html


From ajiang at ouhk.edu.hk  Wed Oct 24 03:25:22 2018
From: ajiang at ouhk.edu.hk (Angus J.)
Date: Tue, 23 Oct 2018 22:25:22 -0500 (CDT)
Subject: [squid-users] Squid proxy not working when upgrade from 27 to
	3.5
In-Reply-To: <1540350949260-0.post@n4.nabble.com>
References: <1540265281548-0.post@n4.nabble.com>
 <4fc98d5d-8ea8-915d-4076-b1baa35e0bee@treenet.co.nz>
 <1540284728102-0.post@n4.nabble.com>
 <20181023100039.a3tptxvtpkvggu3h@fantomas.sk>
 <1540289594180-0.post@n4.nabble.com>
 <caf407eb-9eb6-ea5d-2da9-446e3d1f80b7@treenet.co.nz>
 <1540350949260-0.post@n4.nabble.com>
Message-ID: <1540351522970-0.post@n4.nabble.com>

oul163:/etc/squid # squid -k parse
2018/10/24 11:24:38| Startup: Initializing Authentication Schemes ...
2018/10/24 11:24:38| Startup: Initialized Authentication Scheme 'basic'
2018/10/24 11:24:38| Startup: Initialized Authentication Scheme 'digest'
2018/10/24 11:24:38| Startup: Initialized Authentication Scheme 'negotiate'
2018/10/24 11:24:38| Startup: Initialized Authentication Scheme 'ntlm'
2018/10/24 11:24:38| Startup: Initialized Authentication.
2018/10/24 11:24:38| Processing Configuration File: /etc/squid/squid.conf
(depth 0)
2018/10/24 11:24:38| Processing: acl localnet src 10.0.0.0/8
2018/10/24 11:24:38| Processing: acl localnet src 172.16.0.0/12
2018/10/24 11:24:38| Processing: acl localnet src 192.168.0.0/16
2018/10/24 11:24:38| Processing: acl localnet src fc00::/7
2018/10/24 11:24:38| Processing: acl localnet src fe80::/10
2018/10/24 11:24:38| Processing: acl SSL_ports port 443 8000 8004 8005
2018/10/24 11:24:38| Processing: acl Safe_ports port 80
2018/10/24 11:24:38| Processing: acl Safe_ports port 21
2018/10/24 11:24:38| Processing: acl Safe_ports port 443
2018/10/24 11:24:38| Processing: acl Safe_ports port 70
2018/10/24 11:24:38| Processing: acl Safe_ports port 210
2018/10/24 11:24:38| Processing: acl Safe_ports port 1025-65535
2018/10/24 11:24:38| Processing: acl Safe_ports port 280
2018/10/24 11:24:38| Processing: acl Safe_ports port 488
2018/10/24 11:24:38| Processing: acl Safe_ports port 591
2018/10/24 11:24:38| Processing: acl Safe_ports port 777
2018/10/24 11:24:38| Processing: acl CONNECT method CONNECT
2018/10/24 11:24:38| Processing: access_log /var/log/squid/access.log
2018/10/24 11:24:38| Processing: http_access deny !Safe_ports
2018/10/24 11:24:38| Processing: http_access deny CONNECT !SSL_ports
2018/10/24 11:24:38| Processing: http_access allow localhost manager
2018/10/24 11:24:38| Processing: http_access deny manager
2018/10/24 11:24:38| Processing: http_access allow localnet
2018/10/24 11:24:38| Processing: http_access allow localhost
2018/10/24 11:24:38| Processing: http_access deny all
2018/10/24 11:24:38| Processing: http_port 3128
2018/10/24 11:24:38| Processing: coredump_dir /var/cache/squid
2018/10/24 11:24:38| Processing: refresh_pattern ^ftp: 1440 20 10080
2018/10/24 11:24:38| Processing: refresh_pattern ^gopher: 1440 0 1440
2018/10/24 11:24:38| Processing: refresh_pattern -i  (/cgi-bin/|\?) 0 0 0
2018/10/24 11:24:38| Processing: refresh_pattern . 0 20 4320
2018/10/24 11:24:38| Processing: cache_dir ufs /var/cache/squid 100 16 256
2018/10/24 11:24:38| Processing: cache_log /var/log/squid/cache.log
2018/10/24 11:24:38| Processing: cache_mem 8 MB
2018/10/24 11:24:38| Processing: cache_mgr webmaster
2018/10/24 11:24:38| Processing: cache_replacement_policy lru
2018/10/24 11:24:38| Processing: cache_store_log /var/log/squid/store.log
2018/10/24 11:24:38| Processing: cache_swap_high 95
2018/10/24 11:24:38| Processing: cache_swap_low 90
2018/10/24 11:24:38| Processing: client_lifetime 1 days
2018/10/24 11:24:38| Processing: connect_timeout 2 minutes
2018/10/24 11:24:38| Processing: error_directory /usr/share/squid/errors/en
2018/10/24 11:24:38| Processing: ftp_passive on
2018/10/24 11:24:38| Processing: maximum_object_size 4096 KB
2018/10/24 11:24:38| Processing: memory_replacement_policy lru
2018/10/24 11:24:38| Processing: minimum_object_size 0 KB
2018/10/24 11:24:38| Processing: visible_hostname oul163.ouhk.edu.hk
2018/10/24 11:24:38| Processing: http_port 3128 accel vhost
defaultsite=oul163.ouhk.edu.hk
2018/10/24 11:24:38| Processing: https_port 80 accel
cert=/etc/squid/certs/ouhk.crt key=/etc/squid/certs/ouhk.key
defaultsite=oul163.ouhk.edu.hk vhost protocol=https
options=NO_SSLv3:NO_SSLv2
2018/10/24 11:24:38| Processing: https_port 8000 accel
cert=/etc/squid/certs/ouhk.crt key=/etc/squid/certs/ouhk.key
defaultsite=oul163.ouhk.edu.hk vhost protocol=https
options=NO_SSLv3:NO_SSLv2
2018/10/24 11:24:38| Processing: https_port 8004 accel
cert=/etc/squid/certs/ouhk2.crt key=/etc/squid/certs/ouhk2.key
defaultsite=oul163.ouhk.edu.hk vhost protocol=https
options=NO_SSLv3:NO_SSLv2
2018/10/24 11:24:38| Processing: https_port 8005 accel
cert=/etc/squid/certs/ouhk3.crt key=/etc/squid/certs/ouhk3.key
defaultsite=oul163.ouhk.edu.hk vhost protocol=https
options=NO_SSLv3:NO_SSLv2
2018/10/24 11:24:38| Processing: sslproxy_options NO_SSLv3:NO_SSLv2
2018/10/24 11:24:38| Processing: cache_peer 192.168.31.113 parent 8001 0
proxy-only name=prdhrms
2018/10/24 11:24:38| Processing: cache_peer_domain prdhrms
prdhrms.ouhk.edu.hk
2018/10/24 11:24:38| Processing: cache_peer 192.168.31.134 parent 8005 0 ssl
sslflags=DONT_VERIFY_PEER proxy-only name=uathrms
ssloptions=NO_SSLv3:NO_SSLv2
2018/10/24 11:24:38| Processing: cache_peer_domain uathrms
uathrms.ouhk.edu.hk
2018/10/24 11:24:38| Processing: cache_peer 192.168.31.134 parent 8004 0 ssl
sslflags=DONT_VERIFY_PEER proxy-only name=sithrms
ssloptions=NO_SSLv3:NO_SSLv2
2018/10/24 11:24:38| Processing: cache_peer_domain sithrms
sithrms.ouhk.edu.hk
2018/10/24 11:24:38| Processing: cache_peer 192.168.31.134 parent 8000 0 ssl
sslflags=DONT_VERIFY_PEER proxy-only name=devhrms
ssloptions=NO_SSLv3:NO_SSLv2
2018/10/24 11:24:38| Processing: cache_peer_domain devhrms
devhrms.ouhk.edu.hk
2018/10/24 11:24:38| Processing: acl localip src 192.168.31.0/24
2018/10/24 11:24:38| Processing: acl hrmsacl dstdomain .ouhk.edu.hk
2018/10/24 11:24:38| Processing: http_access allow hrmsacl
2018/10/24 11:24:38| Processing: cache_peer_access prdhrms allow hrmsacl
2018/10/24 11:24:38| Processing: cache_peer_access uathrms allow hrmsacl
2018/10/24 11:24:38| Processing: cache_peer_access sithrms allow hrmsacl
2018/10/24 11:24:38| Processing: cache_peer_access devhrms allow hrmsacl
2018/10/24 11:24:38| Initializing https proxy context
2018/10/24 11:24:38| Initializing cache_peer uathrms SSL context
2018/10/24 11:24:38| Initializing cache_peer sithrms SSL context
2018/10/24 11:24:38| Initializing cache_peer devhrms SSL context
2018/10/24 11:24:38| Initializing https_port [::]:80 SSL context
2018/10/24 11:24:38| Using certificate in /etc/squid/certs/ouhk.crt
2018/10/24 11:24:38| Initializing https_port [::]:8000 SSL context
2018/10/24 11:24:38| Using certificate in /etc/squid/certs/ouhk.crt
2018/10/24 11:24:38| Initializing https_port [::]:8004 SSL context
2018/10/24 11:24:38| Using certificate in /etc/squid/certs/ouhk2.crt
2018/10/24 11:24:38| Initializing https_port [::]:8005 SSL context
2018/10/24 11:24:38| Using certificate in /etc/squid/certs/ouhk3.crt




--
Sent from: http://squid-web-proxy-cache.1019090.n4.nabble.com/Squid-Users-f1019091.html


From ajiang at ouhk.edu.hk  Wed Oct 24 03:33:03 2018
From: ajiang at ouhk.edu.hk (Angus J.)
Date: Tue, 23 Oct 2018 22:33:03 -0500 (CDT)
Subject: [squid-users] Squid proxy not working when upgrade from 27 to
	3.5
In-Reply-To: <1540351522970-0.post@n4.nabble.com>
References: <1540265281548-0.post@n4.nabble.com>
 <4fc98d5d-8ea8-915d-4076-b1baa35e0bee@treenet.co.nz>
 <1540284728102-0.post@n4.nabble.com>
 <20181023100039.a3tptxvtpkvggu3h@fantomas.sk>
 <1540289594180-0.post@n4.nabble.com>
 <caf407eb-9eb6-ea5d-2da9-446e3d1f80b7@treenet.co.nz>
 <1540350949260-0.post@n4.nabble.com> <1540351522970-0.post@n4.nabble.com>
Message-ID: <1540351983527-0.post@n4.nabble.com>

https://uathrms.oubb.edu.hk:8005/OA_HTML/AppsLogin

8005 port is not working ?



--
Sent from: http://squid-web-proxy-cache.1019090.n4.nabble.com/Squid-Users-f1019091.html


From uhlar at fantomas.sk  Wed Oct 24 07:23:10 2018
From: uhlar at fantomas.sk (Matus UHLAR - fantomas)
Date: Wed, 24 Oct 2018 09:23:10 +0200
Subject: [squid-users] Squid proxy not working when upgrade from 27 to
	3.5
In-Reply-To: <1540350949260-0.post@n4.nabble.com>
References: <1540265281548-0.post@n4.nabble.com>
 <4fc98d5d-8ea8-915d-4076-b1baa35e0bee@treenet.co.nz>
 <1540284728102-0.post@n4.nabble.com>
 <20181023100039.a3tptxvtpkvggu3h@fantomas.sk>
 <1540289594180-0.post@n4.nabble.com>
 <caf407eb-9eb6-ea5d-2da9-446e3d1f80b7@treenet.co.nz>
 <1540350949260-0.post@n4.nabble.com>
Message-ID: <20181024072310.4pmv3th2tptgj2ve@fantomas.sk>

On 23.10.18 22:15, Angus J. wrote:
>This error in IE browser when the connection is go through the squid proxy
>server
>
>
>This site can?t be reached
>uathrms.oubb.edu.hk refused to connect.
>Try:
>
>Checking the connection
>Checking the proxy and the firewall
>ERR_CONNECTION_REFUSED

1. How is squid confdigured in windows (IE uses windows proxy settings)?
2. whats's in squid access and cache logs?
3. we have repeatedly asked you: why do you insist on using port 80 for
   HTTPS, when port 80 is HTTP non-SSL port?


-- 
Matus UHLAR - fantomas, uhlar at fantomas.sk ; http://www.fantomas.sk/
Warning: I wish NOT to receive e-mail advertising to this address.
Varovanie: na tuto adresu chcem NEDOSTAVAT akukolvek reklamnu postu.
Fucking windows! Bring Bill Gates! (Southpark the movie)


From timur.lagutenko at gmail.com  Wed Oct 24 10:55:30 2018
From: timur.lagutenko at gmail.com (Timur Lagutenko)
Date: Wed, 24 Oct 2018 15:55:30 +0500
Subject: [squid-users] skip IPs from log
Message-ID: <CAKK+L1cDydA1LQRJ1axSCN3uknN9Jn_7RR-f7BAPanbACXPT7g@mail.gmail.com>

hello dear friends

i would like to configure squid to skip some dst IPs from access.log.
for example:

# this contains IP ranges in CIDR format
acl goodip dst "/usr/local/etc/squid/goodip.txt"
always_direct allow all
cache deny all
access_log daemon:/var/log/squid/access.tj.log squid goodip
access_log daemon:/var/log/squid/access.log squid all !goodip


cat /usr/local/etc/squid/goodip.txt
5.59.96.0/24
37.98.152.0/21
46.20.192.0/20
*217.11.176.0/20 <http://217.11.176.0/20>*
....

unfortunately i still see some records in log
cat /var/log/squid/access.log
...
1540376534.310 519635 192.168.0.60 TCP_MISS/206 44040582 GET
http://upload.mix.tj/video/7/36/5bcec0355a2ab.mp4 - HIER_DIRECT/
*217.11.180.56* video/mp4
1540376595.811  44046 192.168.0.60 TCP_MISS/206 44040588 GET
http://upload.mix.tj/video/7/36/5bcec0355a2ab.mp4 - HIER_DIRECT/
*217.11.180.56* video/mp4
1540376643.381 252904 192.168.10.166 TCP_MISS/206 33025947 GET
http://cloudflare.mix.tj/video/6/10/590ffae3d7601.mp4 - HIER_DIRECT/
*217.11.180.50* video/mp4
...

please advise
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20181024/8b03c72e/attachment.htm>

From mzgmedia at gmail.com  Wed Oct 24 12:24:24 2018
From: mzgmedia at gmail.com (mzgmedia)
Date: Wed, 24 Oct 2018 07:24:24 -0500 (CDT)
Subject: [squid-users] exterlan_acl Can't use proxy auth because no
 authentication schemes are fully configured.
Message-ID: <1540383864278-0.post@n4.nabble.com>

hi

is possible to use external_acl also to login the users without an auth
script?

having an auth script plus an external_acl in the same time, it will make
the first requests very slow and our users will complain



--
Sent from: http://squid-web-proxy-cache.1019090.n4.nabble.com/Squid-Users-f1019091.html


From juan at mediaarchitecture.org  Wed Oct 24 12:28:02 2018
From: juan at mediaarchitecture.org (Juan Carvajal B.)
Date: Wed, 24 Oct 2018 14:28:02 +0200
Subject: [squid-users] squid transparent proxy forward loop
In-Reply-To: <dd3a7f5f-8dbf-3fc4-a9b0-e1120374717b@treenet.co.nz>
References: <CAN8OBP6Lhmsr7FgDTn_Z4+bO=dyhceG4dXiebZjLi_sY-Ot4CQ@mail.gmail.com>
 <dd3a7f5f-8dbf-3fc4-a9b0-e1120374717b@treenet.co.nz>
Message-ID: <CAN8OBP7iQxvCc-yL-mMNOBX3EBrJ9AvJBOZG0z2ZLekxQcvQAg@mail.gmail.com>

Thank you so much Matus,

we were indeed missing a DNS service:

Your proxy is already listening on port 80 and 443 for directly
receiving traffic to any domain with a DNS entry of 192.168.0.188.

best,

*Juan Carlos*

*Join our mailing list
<http://lists.mediaarchitecture.org/?p=subscribe&id=1> (Max 1-mail / month)*



Am Mo., 22. Okt. 2018 um 15:12 Uhr schrieb Amos Jeffries <
squid3 at treenet.co.nz>:

> On 23/10/18 1:26 AM, Juan Carvajal B. wrote:
> > Dear list,
> >
> > I hope you can give me some hints for my current task.
> >
> > I would like to achieve the following:
> >
> > 1. A user comes with the own device, for example phone or table.
> > 2. The user connects to our own WLAN network
> > 4. The user enters the addres of our website
> > 3. The user can only access our website, which is hosted in a sever *not
> > connected* to the internet.
> >
> > We have the following set up
> >
> > Tablets / phones <---> WLAN router <---> server
> >
> > please note that there is *no* connection to internet. The server is
> > connected to the "internet" port of the router.
> >
> > The server runs ubuntu & apache.
> >
> > I've been trying to achieve this with squid but I get a **warning of a
> > forwarding loop**. I do not know what I'm doing wrong.
> >
> > I'm following this:
> > https://wiki.squid-cache.org/ConfigExamples/Intercept/LinuxRedirect
> > https://wiki.squid-cache.org/SquidFaq/ReverseProxy
> >
> > Here are my conf files:
> >
> > ****squid.conf****
> >
>
>
> >     visible_hostname squid.proxy
>
> I have seen other people using this "squid.proxy" as the FQDN of their
> proxy. It is likely that your chosen proxy hostname is not unique.
>
> Since this is a reverse-proxy it is best to set this to the FQDN of the
> primary website you are proxying.
>
>
>
> >     http_port 3128 intercept
> >     http_port 192.168.0.188:80 accel
> > defaultsite=our.domain.org
> >     http_port 192.168.0.188:443 accel
> > defaultsite=our.domain.org
>
> The above should be https_port and requires the TLS certificate for the
> domain being virtual-hosted.
>
> see
> <https://wiki.squid-cache.org/ConfigExamples/Reverse/HttpsVirtualHosting>
> which
> I added just yesterday.
>
>
> >     cache_peer 127.0.0.1 parent 80 0 no-query originserver name=myAccel
> >     acl our_sites dstdomain our.domain.org
> >     http_access allow our_sites
> >     cache_peer_access myAccel allow our_sites
> >     cache_peer_access myAccel deny all
>
> Move the above custom configuration down to ...
>
> >     acl SSL_ports port 443
> >     acl CONNECT method CONNECT
> >     http_access deny !Safe_ports
>
> You do not have any definition of Safe_ports above. It should contain at
> least 80 and 443 for your proxy.
>
> >     http_access deny CONNECT !SSL_ports
> >     http_access allow localhost manager
> >     http_access deny manager
>
> ... here.
>
>
> You are missing good rules for traffic arriving on the port 3128. The
> below "allow all" is very bad.
>
>
> >     http_access allow localhost
> >     http_access allow all
>
> That should be:
>
>  http_access deny all
>
>
> ...
>
> > ****IPTABLES****
> >
> >     # your proxy IP
> >     SQUIDIP=192.168.0.188
> >     # your proxy listening port
> >     SQUIDPORT=3128
> >     iptables -t nat -A PREROUTING -s $SQUIDIP -p tcp --dport 80 -j ACCEPT
> >     iptables -t nat -A PREROUTING -p tcp --dport 80 -j REDIRECT
> > --to-port $SQUIDPORT
> >
>
> Here is your problem. You have not bypassed the traffic Squid is sending
> to 127.0.0.1:80.
>
> Traffic to/from localhost does not use global IP addresses such as
> 192.168.0.188. Thus Squid's attempts to send traffic to Apache is being
> looped back into port 3128 by iptables.
>
>
> What is the point of all this interception anyway?
>
> Your proxy is already listening on port 80 and 443 for directly
> receiving traffic to any domain with a DNS entry of 192.168.0.188.
>
>
> Amos
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20181024/44142daa/attachment.htm>

From vh1988 at yahoo.com.ar  Wed Oct 24 13:21:35 2018
From: vh1988 at yahoo.com.ar (Julian Perconti)
Date: Wed, 24 Oct 2018 10:21:35 -0300
Subject: [squid-users] Squid 4.3 assertion failed
References: <001c01d46a14$dd4083f0$97c18bd0$@yahoo.com.ar>
 <76aa8880-3651-67f0-d93a-c17701ffa551@measurement-factory.com>
 <063f059c-a420-47da-3b8f-02d05b059aec@treenet.co.nz> 
Message-ID: <001901d46b9c$7f0ebed0$7d2c3c70$@yahoo.com.ar>

> Hi Alex/Amos
> 
> Since yesterday squid is running via this method in a cron script:
> 
> trap "rm -f $$.gdb" 0
> cat <<EOF >$$.gdb
> handle SIGPIPE pass nostop noprint
> handle SIGTERM pass nostop noprint
> handle SIGUSR1 pass nostop noprint
> handle SIGHUP  pass
> handle SIGKILL pass
> handle SIGSEGV stop
> handle SIGABRT stop
> run -NYCd3
> backtrace
> quit
> EOF
> while sleep 2; do
>   gdb -x $$.gdb /path/to/squid 2>&1 | tee -a squid.out done

Hi all,

After 2/3 days  Squid has crashed:

First signal:

Program received signal SIGHUP, Hangup.
0x00007ffff41b00a3 in __epoll_wait_nocancel ()
    at ../sysdeps/unix/syscall-template.S:84
84	../sysdeps/unix/syscall-template.S: No such file or directory.
#0  0x00007ffff41b00a3 in __epoll_wait_nocancel ()
    at ../sysdeps/unix/syscall-template.S:84
#1  0x00005555559783f4 in Comm::DoSelect (msec=<optimized out>)
    at ModEpoll.cc:227
#2  0x000055555592197e in CommSelectEngine::checkEvents (
    this=<optimized out>, timeout=<optimized out>) at comm.cc:1847
#3  0x0000555555764345 in EventLoop::checkEngine (
    this=this at entry=0x7fffffffe940, engine=engine at entry=0x7fffffffe6a8, 
    primary=primary at entry=true) at EventLoop.cc:36
#4  0x000055555576452d in EventLoop::runOnce (this=this at entry=0x7fffffffe940)
    at EventLoop.cc:115
#5  0x0000555555764648 in EventLoop::run (this=0x7fffffffe940)
    at EventLoop.cc:83
#6  0x00005555557cf933 in SquidMain (argc=<optimized out>, 
    argv=<optimized out>) at main.cc:1707
#7  0x00005555556b53a4 in SquidMainSafe (argv=<optimized out>, argc=2)
    at main.cc:1415
#8  main (argc=2, argv=0x7fffffffed68) at main.cc:1403
A debugging session is active.

The crash (assertion failed)

2018/10/24 09:44:29| assertion failed: http.cc:1530: "!Comm::MonitorsRead(serverConnection->fd)"

Program received signal SIGABRT, Aborted.
__GI_raise (sig=sig at entry=6) at ../sysdeps/unix/sysv/linux/raise.c:51
51	../sysdeps/unix/sysv/linux/raise.c: No such file or directory.
#0  __GI_raise (sig=sig at entry=6) at ../sysdeps/unix/sysv/linux/raise.c:51
#1  0x00007ffff40fb42a in __GI_abort () at abort.c:89
#2  0x000055555573c57c in xassert (
    msg=msg at entry=0x555555aec788 "!Comm::MonitorsRead(serverConnection->fd)", 
    file=file at entry=0x555555aeb5c5 "http.cc", line=line at entry=1530)
    at debug.cc:618
#3  0x00005555557a1118 in HttpStateData::maybeReadVirginBody (
    this=0x5555574c6888) at http.cc:1530
#4  0x000055555579d3db in HttpStateData::sendRequest (
    this=this at entry=0x5555574c6888) at http.cc:2198
#5  0x000055555579df50 in HttpStateData::start (this=0x5555574c6888)
    at http.cc:2311
#6  0x00005555559156b2 in JobDialer<AsyncJob>::dial (this=0x55555db33390, 
    call=...) at ../../src/base/AsyncJobCalls.h:174
#7  0x00005555559110e1 in AsyncCall::make (this=this at entry=0x55555db33360)
    at AsyncCall.cc:40
#8  0x00005555559124b1 in AsyncCallQueue::fireNext (
    this=this at entry=0x555556105e00) at AsyncCallQueue.cc:56
#9  0x0000555555912819 in AsyncCallQueue::fire (this=0x555556105e00)
    at AsyncCallQueue.cc:42
#10 0x0000555555764549 in EventLoop::dispatchCalls (this=0x7fffffffe940)
    at EventLoop.cc:144
#11 EventLoop::runOnce (this=this at entry=0x7fffffffe940) at EventLoop.cc:121
#12 0x0000555555764648 in EventLoop::run (this=0x7fffffffe940)
    at EventLoop.cc:83
#13 0x00005555557cf933 in SquidMain (argc=<optimized out>, 
    argv=<optimized out>) at main.cc:1707
#14 0x00005555556b53a4 in SquidMainSafe (argv=<optimized out>, argc=2)
    at main.cc:1415
#15 main (argc=2, argv=0x7fffffffed68) at main.cc:1403
A debugging session is active.


The assertion seems to be similiar to bug https://bugs.squid-cache.org/show_bug.cgi?id=4896  as Alex said; but I am not sure.
Should I post there anyway?

Additional info: I can (now) reproduce the crash, it starts to happen when a client tried to connect to a server which (Chrome) returns: ERR_CONNECTION_RESET/EMPTY RESPONSE

I can confirm that site is not working (I tested without squid and from another internet access)

By other hand, what can I do with those squid's crashes?

Squid info (recompiled yesterday):

Squid Cache: Version 4.3-20181021-r17614d5
Service Name: squid

This binary uses OpenSSL 1.1.0f  25 May 2017. For legal restrictions on distribution see https://www.openssl.org/source/license.html

configure options:  '--prefix=/usr' '--build=x86_64-linux-gnu' '--libexecdir=/usr/lib/squid' '--srcdir=.' '--datadir=/usr/share/squid' '--with-cppunit-basedir=/usr' '--enable-inline' '--enable-delay-pools' '--sysconfdir=/etc/squid' '--with-logdir=/var/log/squid' '--with-pidfile=/var/run/squid.pid' '--with-openssl' '--enable-ssl-crtd' '--mandir=/usr/share/man' '--enable-arp-acl' '--enable-wccpv2' '--with-large-files' '--with-default-user=proxy' '--enable-linux-netfilter' '--enable-storeio=ufs,aufs,diskd,rock' '--enable-removal-policies=lru,heap' '--enable-icap' '--enable-icap-client' '--enable-cache-digests' '--enable-heap-replacement' 'build_alias=x86_64-linux-gnu'

If You want I post the config file.

Hope its helps,

Thank You



From squid3 at treenet.co.nz  Wed Oct 24 16:06:42 2018
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 25 Oct 2018 05:06:42 +1300
Subject: [squid-users] exterlan_acl Can't use proxy auth because no
 authentication schemes are fully configured.
In-Reply-To: <1540383864278-0.post@n4.nabble.com>
References: <1540383864278-0.post@n4.nabble.com>
Message-ID: <90556269-1c22-2121-5554-c3c33e1d8125@treenet.co.nz>

On 25/10/18 1:24 AM, mzgmedia wrote:
> hi
> 
> is possible to use external_acl also to login the users without an auth
> script?
> 

No. The closest external ACL alone can do is supply a user=X label (not
username) for logging and other purposes.

That is just a label though, based purely on guesswork by the ACL helper
using the parameters you configure Squid to pass it. So do not place as
much trust in it as you would a login or your users will start
complaining about other people using their access/accounts.


> having an auth script plus an external_acl in the same time, it will make
> the first requests very slow and our users will complain
> 

Er, helpers and "slow" ACLs are typically only slow in terms of nano or
micro seconds. Which are not humanly detectable.

The biggest delay when it comes to first-request and login is all the
time spent waiting for the users Browser to supply the credentials. A
user may spend whole seconds reading the login box title and typing
their password - either way they are too occupied to pay attention to
the timing.

There would have to be a huge amount of load or something very
inefficient about your system for users to even notice 1 vs 2 helper
checks on automated logins HTTP(S) performs once the UA / Browser has
access to the users credentials.

Amos


From rousskov at measurement-factory.com  Wed Oct 24 17:27:38 2018
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Wed, 24 Oct 2018 11:27:38 -0600
Subject: [squid-users] Squid 4.3 assertion failed
In-Reply-To: <001901d46b9c$7f0ebed0$7d2c3c70$@yahoo.com.ar>
References: <001c01d46a14$dd4083f0$97c18bd0$@yahoo.com.ar>
 <76aa8880-3651-67f0-d93a-c17701ffa551@measurement-factory.com>
 <063f059c-a420-47da-3b8f-02d05b059aec@treenet.co.nz>
 <001901d46b9c$7f0ebed0$7d2c3c70$@yahoo.com.ar>
Message-ID: <aae54790-0cd0-4e67-8f07-622f995197c6@measurement-factory.com>

On 10/24/18 7:21 AM, Julian Perconti wrote:

> Program received signal SIGHUP, Hangup.

This is normal. Ideally, you should configure your gdb to ignore SIGHUP
and pass that signal to Squid.

> The crash (assertion failed)
> 
> 2018/10/24 09:44:29| assertion failed: http.cc:1530: "!Comm::MonitorsRead(serverConnection->fd)"
> 
> Program received signal SIGABRT, Aborted.
> __GI_raise (sig=sig at entry=6) at ../sysdeps/unix/sysv/linux/raise.c:51
> 51	../sysdeps/unix/sysv/linux/raise.c: No such file or directory.
> #0  __GI_raise (sig=sig at entry=6) at ../sysdeps/unix/sysv/linux/raise.c:51
> #1  0x00007ffff40fb42a in __GI_abort () at abort.c:89
> #2  0x000055555573c57c in xassert (
>     msg=msg at entry=0x555555aec788 "!Comm::MonitorsRead(serverConnection->fd)", 
>     file=file at entry=0x555555aeb5c5 "http.cc", line=line at entry=1530)
>     at debug.cc:618
> #3  0x00005555557a1118 in HttpStateData::maybeReadVirginBody (
>     this=0x5555574c6888) at http.cc:1530


> The assertion seems to be similiar to bug
> https://bugs.squid-cache.org/show_bug.cgi?id=4896  as Alex said; but
> I am not sure. Should I post there anyway?

Thank you for posting your stack trace there. You did the right thing.


> By other hand, what can I do with those squid's crashes?

Please try the patch for bug 4864. That bug may be a duplicate of bug
4896, and it has an unpolished/unofficial fix:

  https://bugs.squid-cache.org/show_bug.cgi?id=4864

Please update bug 4896 to let us know whether bug 4864 fix worked for
you (including cases where the posted patch no longer applies).


Thank you,

Alex.


From John.Turnbull at rockets.utoledo.edu  Thu Oct 25 00:21:00 2018
From: John.Turnbull at rockets.utoledo.edu (Turnbull, John)
Date: Thu, 25 Oct 2018 00:21:00 +0000
Subject: [squid-users] Bumping TLS 1.3
Message-ID: <DM5PR0102MB3576E82C0AC8D0C912EBB03CB2F70@DM5PR0102MB3576.prod.exchangelabs.com>

I was wondering about bumping TLS 1.3 connections and if you think that will ever be supported.


Thanks,

John Turnbull
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20181025/32b57463/attachment.htm>

From vh1988 at yahoo.com.ar  Thu Oct 25 01:40:07 2018
From: vh1988 at yahoo.com.ar (Julian Perconti)
Date: Wed, 24 Oct 2018 22:40:07 -0300
Subject: [squid-users] Squid 4.3 assertion failed
In-Reply-To: <aae54790-0cd0-4e67-8f07-622f995197c6@measurement-factory.com>
References: <001c01d46a14$dd4083f0$97c18bd0$@yahoo.com.ar>
 <76aa8880-3651-67f0-d93a-c17701ffa551@measurement-factory.com>
 <063f059c-a420-47da-3b8f-02d05b059aec@treenet.co.nz>
 <001901d46b9c$7f0ebed0$7d2c3c70$@yahoo.com.ar>
 <aae54790-0cd0-4e67-8f07-622f995197c6@measurement-factory.com>
Message-ID: <000c01d46c03$aab5a640$0020f2c0$@yahoo.com.ar>

> > Program received signal SIGHUP, Hangup.

Yes, I did not realized that I have executed 'squid -k reconfigure', hence that SIGHUP signal.

I don not know if the following is relevant but:
When the exception occurred, I had executed (earlier) 'squid -k reconfigure'. 
Then, I made a full squid stop and clean squid start, then entered the site that caused Squid to crash and did not happen again, I tried many times without reach the crash again.

> > By other hand, what can I do with those squid's crashes?
> 
> Please try the patch for bug 4864. That bug may be a duplicate of bug 4896,
> and it has an unpolished/unofficial fix:

Let me know if I patched correctly:

~ (...)/squid-4.3-20181021-r17614d5# patch -p1 < /etc/squid/debug/patches/SQUID-385-Comm_MonitorsRead-assertion-t3.patch
patching file src/FwdState.cc (without errors nor warnings)

make/make install and start Squid from systemd (i.e.: systemctl start squid.service) instead of gdb.

And now when I try to connect to site that made squid to crash, I get an error page from squid (instead of chrome page, like before) telling the same thing with different words: "(...): (104) Connection reset by peer". And Squid does not crash.
Even doing a squid -k reconfigure and access again to that site, does not crash.

Anyway I am going to run squid via gdb again, and wait for a similar crash.

>   https://bugs.squid-cache.org/show_bug.cgi?id=4864
> 
> Please update bug 4896 to let us know whether bug 4864 fix worked for you
> (including cases where the posted patch no longer applies).

I'll let you know if the patch definitely solves the bug , after test it few days.

Thank You!



From squid3 at treenet.co.nz  Thu Oct 25 09:03:38 2018
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 25 Oct 2018 22:03:38 +1300
Subject: [squid-users] Bumping TLS 1.3
In-Reply-To: <DM5PR0102MB3576E82C0AC8D0C912EBB03CB2F70@DM5PR0102MB3576.prod.exchangelabs.com>
References: <DM5PR0102MB3576E82C0AC8D0C912EBB03CB2F70@DM5PR0102MB3576.prod.exchangelabs.com>
Message-ID: <44c5fa48-becd-d8fd-ff98-d32c705996f2@treenet.co.nz>

On 25/10/18 1:21 PM, Turnbull, John wrote:
> I was wondering about bumping TLS 1.3 connections and if you think that
> will ever be supported.
> 

Probably. ETA indeterminate.

To quote myself from the docs:
  "When used properly TLS cannot be bumped".

What Squid does now is take advantage of shortcuts and workarounds many
installations use(d) to avoid trouble or administration hassles with
TLS/SSL.

Bump only works at all when those shortcuts allow Squid to impose itself
as MITM into the handshake sequence. TLS/1.3 does not change that
situation - just the code needed to do the insertion will have to be
redesigned a fair bit (already underway AFAIK).


What TLS/1.3 brings to the situation differently is hiding a lot of
details like SNI and server cert that were previously available up-front
for the admin to selectively *avoid* bumping traffic they thought was okay.

So admin will soon / now be faced with having to bump *everything* and
block those relatively few parties actually using TLS "properly".

The reality is that *splice* is the ability TLS/1.3 makes harder to do
reliably.

Amos


From ajiang at ouhk.edu.hk  Thu Oct 25 09:19:34 2018
From: ajiang at ouhk.edu.hk (Angus J.)
Date: Thu, 25 Oct 2018 04:19:34 -0500 (CDT)
Subject: [squid-users] Squid proxy not working when upgrade from 27 to
	3.5
In-Reply-To: <20181024072310.4pmv3th2tptgj2ve@fantomas.sk>
References: <1540265281548-0.post@n4.nabble.com>
 <4fc98d5d-8ea8-915d-4076-b1baa35e0bee@treenet.co.nz>
 <1540284728102-0.post@n4.nabble.com>
 <20181023100039.a3tptxvtpkvggu3h@fantomas.sk>
 <1540289594180-0.post@n4.nabble.com>
 <caf407eb-9eb6-ea5d-2da9-446e3d1f80b7@treenet.co.nz>
 <1540350949260-0.post@n4.nabble.com>
 <20181024072310.4pmv3th2tptgj2ve@fantomas.sk>
Message-ID: <1540459174727-0.post@n4.nabble.com>

1. How is squid confdigured in windows (IE uses windows proxy settings)?
NO
 
2. whats's in squid access and cache logs? 
-rw-r----- 1 squid squid      0 Oct 22 12:21 access.log
-rw-r----- 1 squid squid      0 Oct 22 13:02 netdb.state
-rw-r----- 1 squid squid   6498 Oct 24 11:29 store.log
-rw-r----- 1 squid squid 141946 Oct 24 11:29 cache.log

2018/10/24 11:27:34 kid1| Swap maxSize 102400 + 8192 KB, estimated 8507
objects
2018/10/24 11:27:34 kid1| Target number of buckets: 425
2018/10/24 11:27:34 kid1| Using 8192 Store buckets
2018/10/24 11:27:34 kid1| Max Mem  size: 8192 KB
2018/10/24 11:27:34 kid1| Max Swap size: 102400 KB
2018/10/24 11:27:34 kid1| Rebuilding storage in /var/cache/squid (dirty log)
2018/10/24 11:27:34 kid1| Using Least Load store dir selection
2018/10/24 11:27:34 kid1| Set Current Directory to /var/cache/squid
2018/10/24 11:27:34 kid1| Finished loading MIME types and icons.
2018/10/24 11:27:34 kid1| HTCP Disabled.
2018/10/24 11:27:34 kid1| commBind: Cannot bind socket FD 24 to [::1]: (99)
Cannot assign requested address
2018/10/24 11:27:34 kid1| commBind: Cannot bind socket FD 25 to [::1]: (99)
Cannot assign requested address
2018/10/24 11:27:34 kid1| ERROR: Failed to create helper child read FD:
UDP[::1]
2018/10/24 11:27:34 kid1| Configuring Parent 192.168.31.113/8001/0
2018/10/24 11:27:34 kid1| Configuring Parent 192.168.31.134/8005/0
2018/10/24 11:27:34 kid1| Configuring Parent 192.168.31.134/8004/0
2018/10/24 11:27:34 kid1| Configuring Parent 192.168.31.134/8000/0
2018/10/24 11:27:34 kid1| Squid plugin modules loaded: 0
2018/10/24 11:27:34 kid1| Adaptation support is off.
2018/10/24 11:27:34 kid1| Accepting HTTP Socket connections at
local=[::]:3128 remote=[::] FD 18 flags=9
2018/10/24 11:27:34 kid1| Accepting reverse-proxy HTTP Socket connections at
local=[::]:3128 remote=[::] FD 19 flags=9
2018/10/24 11:27:34 kid1| Accepting reverse-proxy HTTPS Socket connections
at local=[::]:80 remote=[::] FD 20 flags=9
2018/10/24 11:27:34 kid1| Accepting reverse-proxy HTTPS Socket connections
at local=[::]:8000 remote=[::] FD 21 flags=9
2018/10/24 11:27:34 kid1| Accepting reverse-proxy HTTPS Socket connections
at local=[::]:8004 remote=[::] FD 22 flags=9
2018/10/24 11:27:34 kid1| Accepting reverse-proxy HTTPS Socket connections
at local=[::]:8005 remote=[::] FD 23 flags=9
2018/10/24 11:27:34 kid1| Done reading /var/cache/squid swaplog (0 entries)
2018/10/24 11:27:34 kid1| Store rebuilding is 0.00% complete
2018/10/24 11:27:34 kid1| Finished rebuilding storage from disk.
2018/10/24 11:27:34 kid1|         0 Entries scanned
2018/10/24 11:27:34 kid1|         0 Invalid entries.
2018/10/24 11:27:34 kid1|         0 With invalid flags.
2018/10/24 11:27:34 kid1|         0 Objects loaded.
2018/10/24 11:27:34 kid1|         0 Objects expired.
2018/10/24 11:27:34 kid1|         0 Objects cancelled.
2018/10/24 11:27:34 kid1|         0 Duplicate URLs purged.
2018/10/24 11:27:34 kid1|         0 Swapfile clashes avoided.
2018/10/24 11:27:34 kid1|   Took 0.02 seconds (  0.00 objects/sec).
2018/10/24 11:27:34 kid1| Beginning Validation Procedure
2018/10/24 11:27:34 kid1| ERROR: listen( FD 19, [::] [ job2], 1024): (98)
Address already in use
2018/10/24 11:27:34 kid1|   Completed Validation Procedure
2018/10/24 11:27:34 kid1|   Validated 0 Entries
2018/10/24 11:27:34 kid1|   store_swap_size = 0.00 KB
2018/10/24 11:27:35 kid1| storeLateRelease: released 0 objects
2018/10/24 11:29:31| Set Current Directory to /var/cache/squid
2018/10/24 11:29:31 kid1| Killing master process, pid 8464
2018/10/24 11:29:31 kid1| Preparing for shutdown after 0 requests
2018/10/24 11:29:31 kid1| Waiting 30 seconds for active connections to
finish
2018/10/24 11:29:31 kid1| Closing HTTP port [::]:3128
2018/10/24 11:29:31 kid1| Closing HTTP port [::]:3128
2018/10/24 11:29:31 kid1| Closing HTTPS port [::]:80
2018/10/24 11:29:31 kid1| Closing HTTPS port [::]:8000
2018/10/24 11:29:31 kid1| Closing HTTPS port [::]:8004
2018/10/24 11:29:31 kid1| Closing HTTPS port [::]:8005
2018/10/24 11:29:32 kid1| Set Current Directory to /var/cache/squid
2018/10/24 11:29:32 kid1| Starting Squid Cache version 3.5.21 for
x86_64-suse-linux-gnu...
2018/10/24 11:29:32 kid1| Service Name: squid
2018/10/24 11:29:32 kid1| Process ID 8497
2018/10/24 11:29:32 kid1| Process Roles: worker
2018/10/24 11:29:32 kid1| With 4096 file descriptors available
2018/10/24 11:29:32 kid1| Initializing IP Cache...
2018/10/24 11:29:32 kid1| DNS Socket created at [::], FD 6
2018/10/24 11:29:32 kid1| DNS Socket created at 0.0.0.0, FD 7
2018/10/24 11:29:32 kid1| Adding domain ouhk.edu.hk from /etc/resolv.conf
2018/10/24 11:29:32 kid1| Adding nameserver 192.207.91.2 from
/etc/resolv.conf
2018/10/24 11:29:32 kid1| Adding nameserver 192.207.91.1 from
/etc/resolv.conf
2018/10/24 11:29:32 kid1| Logfile: opening log /var/log/squid/access.log
2018/10/24 11:29:32 kid1| WARNING: log name now starts with a module name.
Use 'stdio:/var/log/squid/access.log'
2018/10/24 11:29:32 kid1| Unlinkd pipe opened on FD 14
2018/10/24 11:29:32 kid1| Local cache digest enabled; rebuild/rewrite every
3600/3600 sec
2018/10/24 11:29:32 kid1| Logfile: opening log /var/log/squid/store.log
2018/10/24 11:29:32 kid1| WARNING: log name now starts with a module name.
Use 'stdio:/var/log/squid/store.log'
2018/10/24 11:29:32 kid1| Swap maxSize 102400 + 8192 KB, estimated 8507
objects
2018/10/24 11:29:32 kid1| Target number of buckets: 425
2018/10/24 11:29:32 kid1| Using 8192 Store buckets
2018/10/24 11:29:32 kid1| Max Mem  size: 8192 KB
2018/10/24 11:29:32 kid1| Max Swap size: 102400 KB
2018/10/24 11:29:32 kid1| Rebuilding storage in /var/cache/squid (dirty log)
2018/10/24 11:29:32 kid1| Using Least Load store dir selection
2018/10/24 11:29:32 kid1| Set Current Directory to /var/cache/squid
2018/10/24 11:29:32 kid1| Finished loading MIME types and icons.
2018/10/24 11:29:32 kid1| HTCP Disabled.
2018/10/24 11:29:32 kid1| commBind: Cannot bind socket FD 24 to [::1]: (99)
Cannot assign requested address
2018/10/24 11:29:32 kid1| commBind: Cannot bind socket FD 25 to [::1]: (99)
Cannot assign requested address
2018/10/24 11:29:32 kid1| ERROR: Failed to create helper child read FD:
UDP[::1]
2018/10/24 11:29:32 kid1| Configuring Parent 192.168.31.113/8001/0
2018/10/24 11:29:32 kid1| Configuring Parent 192.168.31.134/8005/0
2018/10/24 11:29:32 kid1| Configuring Parent 192.168.31.134/8004/0
2018/10/24 11:29:32 kid1| Configuring Parent 192.168.31.134/8000/0
2018/10/24 11:29:32 kid1| Squid plugin modules loaded: 0
2018/10/24 11:29:32 kid1| Adaptation support is off.
2018/10/24 11:29:32 kid1| Accepting HTTP Socket connections at
local=[::]:3128 remote=[::] FD 18 flags=9
2018/10/24 11:29:32 kid1| Accepting reverse-proxy HTTP Socket connections at
local=[::]:3128 remote=[::] FD 19 flags=9
2018/10/24 11:29:32 kid1| Accepting reverse-proxy HTTPS Socket connections
at local=[::]:80 remote=[::] FD 20 flags=9
2018/10/24 11:29:32 kid1| Accepting reverse-proxy HTTPS Socket connections
at local=[::]:8000 remote=[::] FD 21 flags=9
2018/10/24 11:29:32 kid1| Accepting reverse-proxy HTTPS Socket connections
at local=[::]:8004 remote=[::] FD 22 flags=9
2018/10/24 11:29:32 kid1| Accepting reverse-proxy HTTPS Socket connections
at local=[::]:8005 remote=[::] FD 23 flags=9
2018/10/24 11:29:32 kid1| Done reading /var/cache/squid swaplog (0 entries)
2018/10/24 11:29:32 kid1| Store rebuilding is 0.00% complete
2018/10/24 11:29:32 kid1| Finished rebuilding storage from disk.
2018/10/24 11:29:32 kid1|         0 Entries scanned
2018/10/24 11:29:32 kid1|         0 Invalid entries.
2018/10/24 11:29:32 kid1|         0 With invalid flags.
2018/10/24 11:29:32 kid1|         0 Objects loaded.
2018/10/24 11:29:32 kid1|         0 Objects expired.
2018/10/24 11:29:32 kid1|         0 Objects cancelled.
2018/10/24 11:29:32 kid1|         0 Duplicate URLs purged.
2018/10/24 11:29:32 kid1|         0 Swapfile clashes avoided.
2018/10/24 11:29:32 kid1|   Took 0.02 seconds (  0.00 objects/sec).
2018/10/24 11:29:32 kid1| Beginning Validation Procedure
2018/10/24 11:29:32 kid1| ERROR: listen( FD 19, [::] [ job2], 1024): (98)
Address already in use
2018/10/24 11:29:32 kid1|   Completed Validation Procedure
2018/10/24 11:29:32 kid1|   Validated 0 Entries
2018/10/24 11:29:32 kid1|   store_swap_size = 0.00 KB
2018/10/24 11:29:33 kid1| storeLateRelease: released 0 objects
2018/10/24 11:29:34| Set Current Directory to /var/cache/squid
2018/10/24 11:29:34 kid1| Killing master process, pid 8495
2018/10/24 11:29:34 kid1| Preparing for shutdown after 0 requests
2018/10/24 11:29:34 kid1| Waiting 30 seconds for active connections to
finish
2018/10/24 11:29:34 kid1| Closing HTTP port [::]:3128
2018/10/24 11:29:34 kid1| Closing HTTP port [::]:3128
2018/10/24 11:29:34 kid1| Closing HTTPS port [::]:80
2018/10/24 11:29:34 kid1| Closing HTTPS port [::]:8000
2018/10/24 11:29:34 kid1| Closing HTTPS port [::]:8004
2018/10/24 11:29:34 kid1| Closing HTTPS port [::]:8005
2018/10/24 11:29:34 kid1| Set Current Directory to /var/cache/squid
2018/10/24 11:29:34 kid1| Starting Squid Cache version 3.5.21 for
x86_64-suse-linux-gnu...
2018/10/24 11:29:34 kid1| Service Name: squid
2018/10/24 11:29:34 kid1| Process ID 8525
2018/10/24 11:29:34 kid1| Process Roles: worker
2018/10/24 11:29:34 kid1| With 4096 file descriptors available
2018/10/24 11:29:34 kid1| Initializing IP Cache...
2018/10/24 11:29:34 kid1| DNS Socket created at [::], FD 6
2018/10/24 11:29:34 kid1| DNS Socket created at 0.0.0.0, FD 7
2018/10/24 11:29:34 kid1| Adding domain ouhk.edu.hk from /etc/resolv.conf
2018/10/24 11:29:34 kid1| Adding nameserver 192.207.91.2 from
/etc/resolv.conf
2018/10/24 11:29:34 kid1| Adding nameserver 192.207.91.1 from
/etc/resolv.conf
2018/10/24 11:29:34 kid1| Logfile: opening log /var/log/squid/access.log
2018/10/24 11:29:34 kid1| WARNING: log name now starts with a module name.
Use 'stdio:/var/log/squid/access.log'
2018/10/24 11:29:34 kid1| Unlinkd pipe opened on FD 14
2018/10/24 11:29:34 kid1| Local cache digest enabled; rebuild/rewrite every
3600/3600 sec
2018/10/24 11:29:34 kid1| Logfile: opening log /var/log/squid/store.log
2018/10/24 11:29:34 kid1| WARNING: log name now starts with a module name.
Use 'stdio:/var/log/squid/store.log'
2018/10/24 11:29:34 kid1| Swap maxSize 102400 + 8192 KB, estimated 8507
objects
2018/10/24 11:29:34 kid1| Target number of buckets: 425
2018/10/24 11:29:34 kid1| Using 8192 Store buckets
2018/10/24 11:29:34 kid1| Max Mem  size: 8192 KB
2018/10/24 11:29:34 kid1| Max Swap size: 102400 KB
2018/10/24 11:29:34 kid1| Rebuilding storage in /var/cache/squid (dirty log)
2018/10/24 11:29:34 kid1| Using Least Load store dir selection
2018/10/24 11:29:34 kid1| Set Current Directory to /var/cache/squid
2018/10/24 11:29:34 kid1| Finished loading MIME types and icons.
2018/10/24 11:29:34 kid1| HTCP Disabled.
2018/10/24 11:29:34 kid1| commBind: Cannot bind socket FD 24 to [::1]: (99)
Cannot assign requested address
2018/10/24 11:29:34 kid1| commBind: Cannot bind socket FD 25 to [::1]: (99)
Cannot assign requested address
2018/10/24 11:29:34 kid1| ERROR: Failed to create helper child read FD:
UDP[::1]
2018/10/24 11:29:34 kid1| Configuring Parent 192.168.31.113/8001/0
2018/10/24 11:29:34 kid1| Configuring Parent 192.168.31.134/8005/0
2018/10/24 11:29:34 kid1| Configuring Parent 192.168.31.134/8004/0
2018/10/24 11:29:34 kid1| Configuring Parent 192.168.31.134/8000/0
2018/10/24 11:29:34 kid1| Squid plugin modules loaded: 0
2018/10/24 11:29:34 kid1| Adaptation support is off.
2018/10/24 11:29:34 kid1| Accepting HTTP Socket connections at
local=[::]:3128 remote=[::] FD 18 flags=9
2018/10/24 11:29:34 kid1| Accepting reverse-proxy HTTP Socket connections at
local=[::]:3128 remote=[::] FD 19 flags=9
2018/10/24 11:29:34 kid1| Accepting reverse-proxy HTTPS Socket connections
at local=[::]:80 remote=[::] FD 20 flags=9
2018/10/24 11:29:34 kid1| Accepting reverse-proxy HTTPS Socket connections
at local=[::]:8000 remote=[::] FD 21 flags=9
2018/10/24 11:29:34 kid1| Accepting reverse-proxy HTTPS Socket connections
at local=[::]:8004 remote=[::] FD 22 flags=9
2018/10/24 11:29:34 kid1| Accepting reverse-proxy HTTPS Socket connections
at local=[::]:8005 remote=[::] FD 23 flags=9
2018/10/24 11:29:34 kid1| Done reading /var/cache/squid swaplog (0 entries)
2018/10/24 11:29:34 kid1| Store rebuilding is 0.00% complete
2018/10/24 11:29:34 kid1| Finished rebuilding storage from disk.
2018/10/24 11:29:34 kid1|         0 Entries scanned
2018/10/24 11:29:34 kid1|         0 Invalid entries.
2018/10/24 11:29:34 kid1|         0 With invalid flags.
2018/10/24 11:29:34 kid1|         0 Objects loaded.
2018/10/24 11:29:34 kid1|         0 Objects expired.
2018/10/24 11:29:34 kid1|         0 Objects cancelled.
2018/10/24 11:29:34 kid1|         0 Duplicate URLs purged.
2018/10/24 11:29:34 kid1|         0 Swapfile clashes avoided.
2018/10/24 11:29:34 kid1|   Took 0.02 seconds (  0.00 objects/sec).
2018/10/24 11:29:34 kid1| Beginning Validation Procedure
2018/10/24 11:29:34 kid1| ERROR: listen( FD 19, [::] [ job2], 1024): (98)
Address already in use
2018/10/24 11:29:34 kid1|   Completed Validation Procedure
2018/10/24 11:29:34 kid1|   Validated 0 Entries
2018/10/24 11:29:34 kid1|   store_swap_size = 0.00 KB
2018/10/24 11:29:35 kid1| storeLateRelease: released 0 objects




3. we have repeatedly asked you: why do you insist on using port 80 for 
   HTTPS, when port 80 is HTTP non-SSL port?  I will use 8005 for https



--
Sent from: http://squid-web-proxy-cache.1019090.n4.nabble.com/Squid-Users-f1019091.html


From squid3 at treenet.co.nz  Thu Oct 25 09:56:03 2018
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 25 Oct 2018 22:56:03 +1300
Subject: [squid-users] Squid proxy not working when upgrade from 27 to
	3.5
In-Reply-To: <1540459174727-0.post@n4.nabble.com>
References: <1540265281548-0.post@n4.nabble.com>
 <4fc98d5d-8ea8-915d-4076-b1baa35e0bee@treenet.co.nz>
 <1540284728102-0.post@n4.nabble.com>
 <20181023100039.a3tptxvtpkvggu3h@fantomas.sk>
 <1540289594180-0.post@n4.nabble.com>
 <caf407eb-9eb6-ea5d-2da9-446e3d1f80b7@treenet.co.nz>
 <1540350949260-0.post@n4.nabble.com>
 <20181024072310.4pmv3th2tptgj2ve@fantomas.sk>
 <1540459174727-0.post@n4.nabble.com>
Message-ID: <ae310bd0-f196-362a-8d0f-9410091b7d34@treenet.co.nz>


On 25/10/18 10:19 PM, Angus J. wrote:
> 1. How is squid confdigured in windows (IE uses windows proxy settings)?
> NO
>  
> 2. whats's in squid access and cache logs? 
> -rw-r----- 1 squid squid      0 Oct 22 12:21 access.log
> -rw-r----- 1 squid squid      0 Oct 22 13:02 netdb.state
> -rw-r----- 1 squid squid   6498 Oct 24 11:29 store.log
> -rw-r----- 1 squid squid 141946 Oct 24 11:29 cache.log
> 
> 2018/10/24 11:27:34 kid1| Swap maxSize 102400 + 8192 KB, estimated 8507
> objects
> 2018/10/24 11:27:34 kid1| Target number of buckets: 425
> 2018/10/24 11:27:34 kid1| Using 8192 Store buckets
> 2018/10/24 11:27:34 kid1| Max Mem  size: 8192 KB
> 2018/10/24 11:27:34 kid1| Max Swap size: 102400 KB
> 2018/10/24 11:27:34 kid1| Rebuilding storage in /var/cache/squid (dirty log)
> 2018/10/24 11:27:34 kid1| Using Least Load store dir selection
> 2018/10/24 11:27:34 kid1| Set Current Directory to /var/cache/squid
> 2018/10/24 11:27:34 kid1| Finished loading MIME types and icons.
> 2018/10/24 11:27:34 kid1| HTCP Disabled.
> 2018/10/24 11:27:34 kid1| commBind: Cannot bind socket FD 24 to [::1]: (99)
> Cannot assign requested address
> 2018/10/24 11:27:34 kid1| commBind: Cannot bind socket FD 25 to [::1]: (99)
> Cannot assign requested address
> 2018/10/24 11:27:34 kid1| ERROR: Failed to create helper child read FD:
> UDP[::1]

Hmm, that is odd. I expect there is something wrong with the pinger
install and/or its security permissions.

But seems not to be having too much impact on the proxy. So looking into
it can be delayed to later.



> 2018/10/24 11:27:34 kid1| Configuring Parent 192.168.31.113/8001/0
> 2018/10/24 11:27:34 kid1| Configuring Parent 192.168.31.134/8005/0
> 2018/10/24 11:27:34 kid1| Configuring Parent 192.168.31.134/8004/0
> 2018/10/24 11:27:34 kid1| Configuring Parent 192.168.31.134/8000/0
> 2018/10/24 11:27:34 kid1| Squid plugin modules loaded: 0
> 2018/10/24 11:27:34 kid1| Adaptation support is off.
> 2018/10/24 11:27:34 kid1| Accepting HTTP Socket connections at
> local=[::]:3128 remote=[::] FD 18 flags=9
> 2018/10/24 11:27:34 kid1| Accepting reverse-proxy HTTP Socket connections at
> local=[::]:3128 remote=[::] FD 19 flags=9


Two http_port lines using port number 3128 ...

> 2018/10/24 11:27:34 kid1| Accepting reverse-proxy HTTPS Socket connections
> at local=[::]:80 remote=[::] FD 20 flags=9
> 2018/10/24 11:27:34 kid1| Accepting reverse-proxy HTTPS Socket connections
> at local=[::]:8000 remote=[::] FD 21 flags=9
> 2018/10/24 11:27:34 kid1| Accepting reverse-proxy HTTPS Socket connections
> at local=[::]:8004 remote=[::] FD 22 flags=9
> 2018/10/24 11:27:34 kid1| Accepting reverse-proxy HTTPS Socket connections
> at local=[::]:8005 remote=[::] FD 23 flags=9
...
> 2018/10/24 11:27:34 kid1| ERROR: listen( FD 19, [::] [ job2], 1024): (98)
> Address already in use

FD 18 and FD 19 both conflicting over who gets to listen on port 3128
and what type of traffic is arriving there.

This port is a registered port for forward-proxy use. Reverse-proxy
(accel mode) traffic has a *different syntax* - the URLs and types of
message that can be delivered are different. So cannot be sharing a port
with forward-proxy traffic.


Log says "ERROR" but is actually something FATAL. That is a bug we need
to fix in the logging and error display.


> 2018/10/24 11:29:31 kid1| Preparing for shutdown after 0 requests
> 2018/10/24 11:29:31 kid1| Waiting 30 seconds for active connections to
> finish
> 2018/10/24 11:29:31 kid1| Closing HTTP port [::]:3128
> 2018/10/24 11:29:31 kid1| Closing HTTP port [::]:3128
> 2018/10/24 11:29:31 kid1| Closing HTTPS port [::]:80
> 2018/10/24 11:29:31 kid1| Closing HTTPS port [::]:8000
> 2018/10/24 11:29:31 kid1| Closing HTTPS port [::]:8004
> 2018/10/24 11:29:31 kid1| Closing HTTPS port [::]:8005
> 2018/10/24 11:29:32 kid1| Set Current Directory to /var/cache/squid
> 2018/10/24 11:29:32 kid1| Starting Squid Cache version 3.5.21 for
> x86_64-suse-linux-gnu...


... and the auto-restart cycle continues.



Amos


From nebeduch at gmail.com  Thu Oct 25 13:25:49 2018
From: nebeduch at gmail.com (Uchenna Nebedum)
Date: Thu, 25 Oct 2018 14:25:49 +0100
Subject: [squid-users] ERROR: NAT/TPROXY lookup failed to locate
 original IPs on local
In-Reply-To: <AM0PR04MB4753D02F16BD52B1C0BB8E718FF90@AM0PR04MB4753.eurprd04.prod.outlook.com>
References: <CAHgpA++8tPvb7RLopwRAWVFmV2t1SUkqyrogUPpi=3ju0HFOYw@mail.gmail.com>
 <AM0PR04MB47539225C3F0C19097401BA98FF90@AM0PR04MB4753.eurprd04.prod.outlook.com>
 <CAHgpA+JGgGmHxpJZtSvpaHzpH7v2ZvHPXS_y5uCWp16km5h8Aw@mail.gmail.com>
 <AM0PR04MB4753D02F16BD52B1C0BB8E718FF90@AM0PR04MB4753.eurprd04.prod.outlook.com>
Message-ID: <CAHgpA+Jqj9LWdqCuGr-P_YYfD8pFGHJ0adQ3yx0Zsd9+83iCfw@mail.gmail.com>

Hi Rafael,
First off i'd like to thank you for the link you provided, it was really
helpful.
I've successfully installed squid and routed the traffic to the proxy
thanks to the diladele link, unfortunately i couldn't install websafety as
ubuntu removed squid 4.1-1 from their repo and had 4.3-1, i tried with 4.3
but there were problems with websafety... so installed squid 3.5 from
source with ssl enabled, I used the websafety squid.conf as reference, now
i have my proxy working, and i have connected greasyspoon to it, but
greasyspoon only adapts http content, how do i get it to adapt https
traffic? and this is assuming the ssl_bumping works, because i don't know
how to confirm if it does, i see https connections in the access.log though.

please see attached my squid.conf
cache_effective_user proxy
acl localnet src 10.0.0.0/24
acl localnet src 172.16.0.0/12
acl localnet src 192.168.0.0/16
acl localnet src fc00::/7
acl localnet src fe80::/10
acl SSL_ports port 443
acl Safe_ports port 80          # http
acl Safe_ports port 21          # ftp
acl Safe_ports port 443         # https
acl Safe_ports port 70          # gopher
acl Safe_ports port 210         # wais
acl Safe_ports port 1025-65535  # unregistered ports
acl Safe_ports port 280         # http-mgmt
acl Safe_ports port 488         # gss-http
acl Safe_ports port 591         # filemaker
acl Safe_ports port 777         # multiling http
acl CONNECT method CONNECT
http_port 3128 ssl-bump  generate-host-certificates=on
dynamic_cert_mem_cache_size=4MB cert=/opt/websafety/etc/myca.pem
http_port 3126 intercept
https_port 3127 intercept ssl-bump generate-host-certificates=on
dynamic_cert_mem_cache_size=4MB cert=/opt/websafety/etc/myca.pem
http_access deny !Safe_ports
http_access deny CONNECT !SSL_ports
http_access allow localhost manager
http_access deny manager
http_access allow localnet
http_access allow localhost
http_access deny all
sslcrtd_program /usr/local/squid/libexec/ssl_crtd -s /var/spool/squid_ssldb
-M 4MB
acl step1 at_step SslBump1
acl step2 at_step SslBump2
acl step3 at_step SslBump3
ssl_bump peek step1 all
ssl_bump bump all
ssl_bump bump ssl_force_bump
ssl_bump splice localhost
acl ssl_error_domains dstdomain
"/opt/websafety/etc/squid/ssl/error/domains.conf"
acl ssl_error_ips     dst
"/opt/websafety/etc/squid/ssl/error/ips.conf"
acl ssl_error_ips     dst
"/opt/websafety/etc/squid/ssl/error/subnets.conf"
sslproxy_cert_error allow ssl_error_domains
sslproxy_cert_error allow ssl_error_ips
shutdown_lifetime 10 seconds
adaptation_access greasyspoon allow all
visible_hostname proxy.example.lan
acl cache_exclude_domainname dstdomain
"/opt/websafety/etc/squid/cache/exclude/domain_name.conf"
acl cache_exclude_domainaddr dst
"/opt/websafety/etc/squid/cache/exclude/domain_ip.conf"
acl cache_exclude_domainaddr dst
"/opt/websafety/etc/squid/cache/exclude/domain_subnet.conf"
acl cache_exclude_domainaddr dst
"/opt/websafety/etc/squid/cache/exclude/domain_range.conf"
acl cache_exclude_useraddr src
"/opt/websafety/etc/squid/cache/exclude/user_ip.conf"
acl cache_exclude_useraddr src
"/opt/websafety/etc/squid/cache/exclude/user_subnet.conf"
acl cache_exclude_useraddr src
"/opt/websafety/etc/squid/cache/exclude/user_range.conf"
acl cache_exclude_useragent   browser -i
"/opt/websafety/etc/squid/cache/exclude/user_agent.conf"
acl cache_exclude_schedule    time
"/opt/websafety/etc/squid/cache/exclude/schedule.conf"
cache deny cache_exclude_domainname
cache deny cache_exclude_domainaddr
cache deny cache_exclude_useraddr
cache deny cache_exclude_useragent
cache deny cache_exclude_schedule
acl cache_exclude_contenttype rep_mime_type
"/opt/websafety/etc/squid/cache/exclude/content_type.conf"
send_hit deny cache_exclude_contenttype
store_miss deny cache_exclude_contenttype
refresh_pattern ^ftp:           1440    20%     10080
refresh_pattern ^gopher:        1440    0%      1440
refresh_pattern -i (/cgi-bin/|\?) 0     0%      0
refresh_pattern .               0       20%     4320
cache_replacement_policy lru
minimum_object_size 0 KB
maximum_object_size 4096 KB
dns_timeout 30 seconds
dns_v4_first on
icap_enable on
icap_preview_enable off
icap_preview_size 2048
icap_persistent_connections on
adaptation_send_client_ip on
adaptation_send_username on
icap_service greasyspoon respmod_precache icap://127.0.0.1:1344/response
bypass=0
cache_mem 256 MB
maximum_object_size_in_memory 512 KB
memory_replacement_policy lru
forwarded_for on
forward_max_tries 25


also part of my access.log
1540473704.606   1021 10.0.0.250 TAG_NONE/200 0 CONNECT 52.97.133.226:443 -
HIER_NONE/- -
1540473711.552 465997 10.0.0.254 TCP_TUNNEL/200 4350 CONNECT
outlook.office365.com:443 - ORIGINAL_DST/52.97.131.242 -
1540473711.552 163713 10.0.0.254 TCP_TUNNEL/200 4320 CONNECT
inbox.google.com:443 - ORIGINAL_DST/216.58.223.197 -
1540473711.552 163689 10.0.0.254 TCP_TUNNEL/200 4231 CONNECT
inbox.google.com:443 - ORIGINAL_DST/216.58.223.197 -

and part of my cache.log
2018/10/25 11:36:21 kid1| Accepting SSL bumped HTTP Socket connections at
local=[::]:3128 remote=[::] FD 22 flags=9
2018/10/25 11:36:21 kid1| Accepting NAT intercepted HTTP Socket connections
at local=[::]:3126 remote=[::] FD 23 flags=41
2018/10/25 11:36:21 kid1| Accepting NAT intercepted SSL bumped HTTPS Socket
connections at local=[::]:3127 remote=[::] FD 24 flags=41
2018/10/25 11:36:22 kid1| storeLateRelease: released 0 objects
2018/10/25 11:42:08| Squid is already running!  Process ID 3497
2018/10/25 11:46:20| Squid is already running!  Process ID 3497
2018/10/25 11:46:24| Squid is already running!  Process ID 3497
2018/10/25 11:49:32 kid1| SECURITY ALERT: Host header forgery detected on
local=52.97.133.178:443 remote=10.0.0.250:39627 FD 39 flags=33 (local IP
does not match any domain IP)
2018/10/25 11:49:32 kid1| SECURITY ALERT: on URL: outlook.office365.com:443
2018/10/25 11:49:32 kid1| SECURITY ALERT: Host header forgery detected on
local=52.97.133.178:443 remote=10.0.0.250:39628 FD 39 flags=33 (local IP
does not match any domain IP)
2018/10/25 11:49:32 kid1| SECURITY ALERT: on URL: outlook.office365.com:443
2018/10/25 11:49:32 kid1| SECURITY ALERT: Host header forgery detected on
local=52.97.133.178:443 remote=10.0.0.250:39629 FD 39 flags=33 (local IP
does not match any domain IP)


please how do i get the adaptation to work for https traffic?
Thanks for everyones help.




Uchenna Nebedum

On Fri, Oct 19, 2018, 20:09 Rafael Akchurin <rafael.akchurin at diladele.com>
wrote:

> Yes you can use any ICAP/eCAP server you like, just adjust the docs as
> required and that is it.
>
>
>
> *From:* Uchenna Nebedum <nebeduch at gmail.com>
> *Sent:* Friday, 19 October 2018 20:17
> *To:* Rafael Akchurin <rafael.akchurin at diladele.com>
> *Cc:* squid-users at lists.squid-cache.org
> *Subject:* Re: [squid-users] ERROR: NAT/TPROXY lookup failed to locate
> original IPs on local
>
>
>
> Thanks a lot Rafael, I've gone through the documentation it looks to be
> very promising, one reservation i have is I want to use greasyspoon for
> icap and i see ecap is implemented already. I intend to install everything
> as suggested on the link, then after this change squid.conf to remove ecap
> connection.
>
> Please, I hope this will work?
>
>
>
> Thanks a lot again for the link, it really explained everything well
> enough for a beginner.
>
> Uchenna Nebedum
>
>
>
> On Fri, Oct 19, 2018, 18:30 Rafael Akchurin <rafael.akchurin at diladele.com>
> wrote:
>
> Hello Uchenna,
>
>
>
> May be this policy based routing with Mikrotik tutorial will be of any use
>
> See
> https://docs.diladele.com/tutorials/mikrotik_transparent_squid/index.html
>
>
>
> Best regards,
>
> Rafael Akchurin
>
> Diladele B.V.
>
>
>
>
>
> *From:* squid-users <squid-users-bounces at lists.squid-cache.org> *On
> Behalf Of *Uchenna Nebedum
> *Sent:* Friday, 19 October 2018 18:42
> *To:* squid-users at lists.squid-cache.org
> *Subject:* [squid-users] ERROR: NAT/TPROXY lookup failed to locate
> original IPs on local
>
>
>
> Good Day All,
>
> i'm new to squid and i have configured squid as an http transparent proxy
> with a mikrotik.
>
> the squid server has only a single NIC, so i followed a tutorial and set
> up a dst-nat to squid proxy for traffic on port 80,
>
> Chain:dstnat.
>
> Protocol:tcp
>
> Dst-port:80
>
> Action:dst-nat
>
> To Addresses:192.168.2.2 (squid proxy)
>
> To ports:8080
>
> but after setup, only https traffic works correctly,
>
> http traffic client error is "This page isn't working ERR_EMPTY_RESPONSE"
>
> squid access.log is empty then in squid cache.log these are the errors
>
>
>
> ```
>
> 2018/10/19 17:08:54 kid1| ERROR: NF getsockopt(ORIGINAL_DST) failed on
> local=192.168.2.2:8080 remote=192.168.1.254:41248 FD 10 flags=33: (92)
> Protocol not available
> 2018/10/19 17:08:54 kid1| ERROR: NAT/TPROXY lookup failed to locate
> original IPs on local=192.168.2.2:8080 remote=192.168.1.254:41248 FD 10
> flags=33
>
> ```
>
> please find below my squid.conf contents
>
>
>
> ```
>
> acl localnet src 192.168.1.0/24
> acl SSL_ports port 443
> acl Safe_ports port 80
> acl Safe_ports port 21
> acl Safe_ports port 443
> acl Safe_ports port 70
> acl Safe_ports port 210
> acl Safe_ports port 1025-65535
> acl Safe_ports port 280
> acl Safe_ports port 488
> acl Safe_ports port 591
> acl Safe_ports port 777
> acl CONNECT method CONNECT
> icap_enable off
> icap_service service_req reqmod_precache 1 icap://127.0.0.1:1344/REQMOD
> adaptation_service_set class_req service_req
> adaptation_access class_req allow all
> icap_service service_resp respmod_precache 0 icap://127.0.0.1:1344/RESPMOD
> adaptation_service_set class_resp service_resp
> adaptation_access class_resp allow all
> http_access deny !Safe_ports
> http_access deny CONNECT !SSL_ports
> http_access allow localhost manager
> http_access deny manager
> http_access deny to_localhost
> http_access allow localnet
> http_access allow localhost
> http_access allow all
> http_port 3128
> http_port 8080 transparent
>  access_log daemon:/var/log/squid/access.log squid
> coredump_dir /var/spool/squid
> refresh_pattern ^ftp:        1440    20%    10080
> refresh_pattern ^gopher:    1440    0%    1440
> refresh_pattern -i (/cgi-bin/|\?) 0    0%    0
> refresh_pattern (Release|Packages(.gz)*)$      0       20%     2880
> refresh_pattern .        0    20%    4320
>
> ```
>
> please any help or correction would be highly appreciated, i am not even
> sure if the approach is correct.
>
>
> --
>
> Nebedum Uchenna
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20181025/75243254/attachment.htm>

From nebeduch at gmail.com  Thu Oct 25 16:29:08 2018
From: nebeduch at gmail.com (Uchenna Nebedum)
Date: Thu, 25 Oct 2018 17:29:08 +0100
Subject: [squid-users] Https Adaptation not working after bumping
Message-ID: <CAHgpA+L0xQY=44E_GeQFY+1=F6d=dZfpubSz84QSkqNManZOTw@mail.gmail.com>

Hi all, thanks to Rafael and Amos, I've been able to set up a Squid Proxy
with a mikrotik. ssl bumping is enabled on squid and i have connected it to
greasyspoon for content adaptation, but i can't be sure if ssl bumping is
working because i only see adapted content over http and not https.

here is my squid.conf
cache_effective_user proxy
acl localnet src 10.0.0.0/24
acl localnet src 172.16.0.0/12
acl localnet src 192.168.0.0/16
acl localnet src fc00::/7
acl localnet src fe80::/10
acl SSL_ports port 443
acl Safe_ports port 80          # http
acl Safe_ports port 21          # ftp
acl Safe_ports port 443         # https
acl Safe_ports port 70          # gopher
acl Safe_ports port 210         # wais
acl Safe_ports port 1025-65535  # unregistered ports
acl Safe_ports port 280         # http-mgmt
acl Safe_ports port 488         # gss-http
acl Safe_ports port 591         # filemaker
acl Safe_ports port 777         # multiling http
acl CONNECT method CONNECT
http_port 3128 ssl-bump  generate-host-certificates=on
dynamic_cert_mem_cache_size=4MB cert=/opt/websafety/etc/myca.pem
http_port 3126 intercept
https_port 3127 intercept ssl-bump generate-host-certificates=on
dynamic_cert_mem_cache_size=4MB cert=/opt/websafety/etc/myca.pem
http_access deny !Safe_ports
http_access deny CONNECT !SSL_ports
http_access allow localhost manager
http_access deny manager
http_access allow localnet
http_access allow localhost
http_access deny all
sslcrtd_program /usr/local/squid/libexec/ssl_crtd -s /var/spool/squid_ssldb
-M 4MB
acl step1 at_step SslBump1
acl step2 at_step SslBump2
acl step3 at_step SslBump3
ssl_bump peek step1 all
ssl_bump bump all
ssl_bump bump ssl_force_bump
ssl_bump splice localhost
acl ssl_error_domains dstdomain
"/opt/websafety/etc/squid/ssl/error/domains.conf"
acl ssl_error_ips     dst
"/opt/websafety/etc/squid/ssl/error/ips.conf"
acl ssl_error_ips     dst
"/opt/websafety/etc/squid/ssl/error/subnets.conf"
sslproxy_cert_error allow ssl_error_domains
sslproxy_cert_error allow ssl_error_ips
shutdown_lifetime 10 seconds
adaptation_access greasyspoon allow all
visible_hostname proxy.example.lan
acl cache_exclude_domainname dstdomain
"/opt/websafety/etc/squid/cache/exclude/domain_name.conf"
acl cache_exclude_domainaddr dst
"/opt/websafety/etc/squid/cache/exclude/domain_ip.conf"
acl cache_exclude_domainaddr dst
"/opt/websafety/etc/squid/cache/exclude/domain_subnet.conf"
acl cache_exclude_domainaddr dst
"/opt/websafety/etc/squid/cache/exclude/domain_range.conf"
acl cache_exclude_useraddr src
"/opt/websafety/etc/squid/cache/exclude/user_ip.conf"
acl cache_exclude_useraddr src
"/opt/websafety/etc/squid/cache/exclude/user_subnet.conf"
acl cache_exclude_useraddr src
"/opt/websafety/etc/squid/cache/exclude/user_range.conf"
acl cache_exclude_useragent   browser -i
"/opt/websafety/etc/squid/cache/exclude/user_agent.conf"
acl cache_exclude_schedule    time
"/opt/websafety/etc/squid/cache/exclude/schedule.conf"
cache deny cache_exclude_domainname
cache deny cache_exclude_domainaddr
cache deny cache_exclude_useraddr
cache deny cache_exclude_useragent
cache deny cache_exclude_schedule
acl cache_exclude_contenttype rep_mime_type
"/opt/websafety/etc/squid/cache/exclude/content_type.conf"
send_hit deny cache_exclude_contenttype
store_miss deny cache_exclude_contenttype
refresh_pattern ^ftp:           1440    20%     10080
refresh_pattern ^gopher:        1440    0%      1440
refresh_pattern -i (/cgi-bin/|\?) 0     0%      0
refresh_pattern .               0       20%     4320
cache_replacement_policy lru
minimum_object_size 0 KB
maximum_object_size 4096 KB
dns_timeout 30 seconds
dns_v4_first on
icap_enable on
icap_preview_enable off
icap_preview_size 2048
icap_persistent_connections on
adaptation_send_client_ip on
adaptation_send_username on
icap_service greasyspoon respmod_precache icap://127.0.0.1:1344/response
bypass=0
cache_mem 256 MB
maximum_object_size_in_memory 512 KB
memory_replacement_policy lru
forwarded_for on
forward_max_tries 25

here is part of my access.log
1540473704.606   1021 10.0.0.250 TAG_NONE/200 0 CONNECT 52.97.133.226:443 -
HIER_NONE/- -
1540473711.552 465997 10.0.0.254 TCP_TUNNEL/200 4350 CONNECT
outlook.office365.com:443 - ORIGINAL_DST/52.97.131.242 -
1540473711.552 163713 10.0.0.254 TCP_TUNNEL/200 4320 CONNECT
inbox.google.com:443 - ORIGINAL_DST/216.58.223.197 -
1540473711.552 163689 10.0.0.254 TCP_TUNNEL/200 4231 CONNECT
inbox.google.com:443 - ORIGINAL_DST/216.58.223.197 -

and part of my cache.log
2018/10/25 11:36:21 kid1| Accepting SSL bumped HTTP Socket connections at
local=[::]:3128 remote=[::] FD 22 flags=9
2018/10/25 11:36:21 kid1| Accepting NAT intercepted HTTP Socket connections
at local=[::]:3126 remote=[::] FD 23 flags=41
2018/10/25 11:36:21 kid1| Accepting NAT intercepted SSL bumped HTTPS Socket
connections at local=[::]:3127 remote=[::] FD 24 flags=41
2018/10/25 11:36:22 kid1| storeLateRelease: released 0 objects
2018/10/25 11:42:08| Squid is already running!  Process ID 3497
2018/10/25 11:46:20| Squid is already running!  Process ID 3497
2018/10/25 11:46:24| Squid is already running!  Process ID 3497
2018/10/25 11:49:32 kid1| SECURITY ALERT: Host header forgery detected on
local=52.97.133.178:443 remote=10.0.0.250:39627 FD 39 flags=33 (local IP
does not match any domain IP)
2018/10/25 11:49:32 kid1| SECURITY ALERT: on URL: outlook.office365.com:443
2018/10/25 11:49:32 kid1| SECURITY ALERT: Host header forgery detected on
local=52.97.133.178:443 remote=10.0.0.250:39628 FD 39 flags=33 (local IP
does not match any domain IP)
2018/10/25 11:49:32 kid1| SECURITY ALERT: on URL: outlook.office365.com:443
2018/10/25 11:49:32 kid1| SECURITY ALERT: Host header forgery detected on
local=52.97.133.178:443 remote=10.0.0.250:39629 FD 39 flags=33 (local IP
does not match any domain IP)

please i don't know if traffic is being bumped correctly as i only see
adapted content over http. Thanks for the anticipated help.

-- 
Nebedum Uchenna
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20181025/fb6fabe8/attachment.htm>

From chip_pop at hotmail.com  Thu Oct 25 21:51:49 2018
From: chip_pop at hotmail.com (joseph)
Date: Thu, 25 Oct 2018 16:51:49 -0500 (CDT)
Subject: [squid-users] github code update ?
Message-ID: <1540504309748-0.post@n4.nabble.com>

wen a code get Approved  for merge it get closed without  updating the code
section is that normal
last update i see
ntlm_fake_auth: add ability to test delayed responses (#294)  ?

please re check 



-----
************************** 
***** Crash to the future  ****
**************************
--
Sent from: http://squid-web-proxy-cache.1019090.n4.nabble.com/Squid-Users-f1019091.html


From rousskov at measurement-factory.com  Fri Oct 26 00:10:02 2018
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Thu, 25 Oct 2018 18:10:02 -0600
Subject: [squid-users] github code update ?
In-Reply-To: <1540504309748-0.post@n4.nabble.com>
References: <1540504309748-0.post@n4.nabble.com>
Message-ID: <5e811ab8-c72b-e853-0916-49fca15d1f9b@measurement-factory.com>

On 10/25/18 3:51 PM, joseph wrote:
> wen a code get Approved  for merge it get closed without  updating the code
> section is that normal

I am not sure what you mean by the "code section". Merged PR code can be
found in the PR target branch (usually "master").


> last update i see
> ntlm_fake_auth: add ability to test delayed responses (#294)  ?

I do not know what GitHub output you are looking at exactly, but the
last master and v4 commits are different from the above, both in the
official repository and on the GitHub web site. Make sure that you are
looking at the right code _branch_ and do not forget to reload the page.

Alex.


From chip_pop at hotmail.com  Fri Oct 26 00:29:28 2018
From: chip_pop at hotmail.com (joseph)
Date: Thu, 25 Oct 2018 19:29:28 -0500 (CDT)
Subject: [squid-users] github code update ?
In-Reply-To: <5e811ab8-c72b-e853-0916-49fca15d1f9b@measurement-factory.com>
References: <1540504309748-0.post@n4.nabble.com>
 <5e811ab8-c72b-e853-0916-49fca15d1f9b@measurement-factory.com>
Message-ID: <1540513768417-0.post@n4.nabble.com>

this wat i see in master is this correct Commits on Oct 8, 2018 latest just
wondering
https://i.imgur.com/8bmvaBV.png



-----
************************** 
***** Crash to the future  ****
**************************
--
Sent from: http://squid-web-proxy-cache.1019090.n4.nabble.com/Squid-Users-f1019091.html


From chip_pop at hotmail.com  Fri Oct 26 00:36:32 2018
From: chip_pop at hotmail.com (joseph)
Date: Thu, 25 Oct 2018 19:36:32 -0500 (CDT)
Subject: [squid-users] github code update ?
In-Reply-To: <1540513768417-0.post@n4.nabble.com>
References: <1540504309748-0.post@n4.nabble.com>
 <5e811ab8-c72b-e853-0916-49fca15d1f9b@measurement-factory.com>
 <1540513768417-0.post@n4.nabble.com>
Message-ID: <1540514192955-0.post@n4.nabble.com>

suks reload dose not help but wen i re click on master it showed the latest
commit tks




-----
************************** 
***** Crash to the future  ****
**************************
--
Sent from: http://squid-web-proxy-cache.1019090.n4.nabble.com/Squid-Users-f1019091.html


From ajiang at ouhk.edu.hk  Fri Oct 26 02:06:00 2018
From: ajiang at ouhk.edu.hk (Angus J.)
Date: Thu, 25 Oct 2018 21:06:00 -0500 (CDT)
Subject: [squid-users] Squid proxy not working when upgrade from 27 to
	3.5
In-Reply-To: <ae310bd0-f196-362a-8d0f-9410091b7d34@treenet.co.nz>
References: <1540265281548-0.post@n4.nabble.com>
 <4fc98d5d-8ea8-915d-4076-b1baa35e0bee@treenet.co.nz>
 <1540284728102-0.post@n4.nabble.com>
 <20181023100039.a3tptxvtpkvggu3h@fantomas.sk>
 <1540289594180-0.post@n4.nabble.com>
 <caf407eb-9eb6-ea5d-2da9-446e3d1f80b7@treenet.co.nz>
 <1540350949260-0.post@n4.nabble.com>
 <20181024072310.4pmv3th2tptgj2ve@fantomas.sk>
 <1540459174727-0.post@n4.nabble.com>
 <ae310bd0-f196-362a-8d0f-9410091b7d34@treenet.co.nz>
Message-ID: <1540519560433-0.post@n4.nabble.com>

Hi Amos 

# Squid normally listens to port 3128
http_port 3128


http_port 3128 accel vhost defaultsite=oul163.ouhk.edu.hk


This two line of squid.conf , they will cause the ERROR" ?




--
Sent from: http://squid-web-proxy-cache.1019090.n4.nabble.com/Squid-Users-f1019091.html


From ajiang at ouhk.edu.hk  Fri Oct 26 02:09:10 2018
From: ajiang at ouhk.edu.hk (Angus J.)
Date: Thu, 25 Oct 2018 21:09:10 -0500 (CDT)
Subject: [squid-users] Squid proxy not working when upgrade from 27 to
	3.5
In-Reply-To: <ae310bd0-f196-362a-8d0f-9410091b7d34@treenet.co.nz>
References: <1540265281548-0.post@n4.nabble.com>
 <4fc98d5d-8ea8-915d-4076-b1baa35e0bee@treenet.co.nz>
 <1540284728102-0.post@n4.nabble.com>
 <20181023100039.a3tptxvtpkvggu3h@fantomas.sk>
 <1540289594180-0.post@n4.nabble.com>
 <caf407eb-9eb6-ea5d-2da9-446e3d1f80b7@treenet.co.nz>
 <1540350949260-0.post@n4.nabble.com>
 <20181024072310.4pmv3th2tptgj2ve@fantomas.sk>
 <1540459174727-0.post@n4.nabble.com>
 <ae310bd0-f196-362a-8d0f-9410091b7d34@treenet.co.nz>
Message-ID: <1540519750009-0.post@n4.nabble.com>



I have remarked this line , the  port 3128 problem is solved?

# Squid normally listens to port 3128
#http_port 3128



oul163:/var/log/squid # cat cache.log
2018/10/26 10:06:47| Set Current Directory to /var/cache/squid
2018/10/26 10:06:47 kid1| Set Current Directory to /var/cache/squid
2018/10/26 10:06:47 kid1| Starting Squid Cache version 3.5.21 for
x86_64-suse-li                                                                                                
nux-gnu...
2018/10/26 10:06:47 kid1| Service Name: squid
2018/10/26 10:06:47 kid1| Process ID 16743
2018/10/26 10:06:47 kid1| Process Roles: worker
2018/10/26 10:06:47 kid1| With 4096 file descriptors available
2018/10/26 10:06:47 kid1| Initializing IP Cache...
2018/10/26 10:06:47 kid1| DNS Socket created at [::], FD 6
2018/10/26 10:06:47 kid1| DNS Socket created at 0.0.0.0, FD 7
2018/10/26 10:06:47 kid1| Adding domain ouhk.edu.hk from /etc/resolv.conf
2018/10/26 10:06:47 kid1| Adding nameserver 192.207.91.2 from
/etc/resolv.conf
2018/10/26 10:06:47 kid1| Adding nameserver 192.207.91.1 from
/etc/resolv.conf
2018/10/26 10:06:47 kid1| Logfile: opening log /var/log/squid/access.log
2018/10/26 10:06:47 kid1| WARNING: log name now starts with a module name.
Use '                                                                                                
stdio:/var/log/squid/access.log'
2018/10/26 10:06:47 kid1| Unlinkd pipe opened on FD 14
2018/10/26 10:06:47 kid1| Local cache digest enabled; rebuild/rewrite every
3600                                                                                                
/3600 sec
2018/10/26 10:06:47 kid1| Logfile: opening log /var/log/squid/store.log
2018/10/26 10:06:47 kid1| WARNING: log name now starts with a module name.
Use '                                                                                                
stdio:/var/log/squid/store.log'
2018/10/26 10:06:47 kid1| Swap maxSize 102400 + 8192 KB, estimated 8507
objects
2018/10/26 10:06:47 kid1| Target number of buckets: 425
2018/10/26 10:06:47 kid1| Using 8192 Store buckets
2018/10/26 10:06:47 kid1| Max Mem  size: 8192 KB
2018/10/26 10:06:47 kid1| Max Swap size: 102400 KB
2018/10/26 10:06:47 kid1| Rebuilding storage in /var/cache/squid (dirty log)
2018/10/26 10:06:47 kid1| Using Least Load store dir selection
2018/10/26 10:06:47 kid1| Set Current Directory to /var/cache/squid
2018/10/26 10:06:47 kid1| Finished loading MIME types and icons.
2018/10/26 10:06:47 kid1| HTCP Disabled.
2018/10/26 10:06:47 kid1| commBind: Cannot bind socket FD 23 to [::1]: (99)
Cann                                                                                                
ot assign requested address
2018/10/26 10:06:47 kid1| commBind: Cannot bind socket FD 24 to [::1]: (99)
Cann                                                                                                
ot assign requested address
2018/10/26 10:06:47 kid1| ERROR: Failed to create helper child read FD:
UDP[::1]
2018/10/26 10:06:47 kid1| Configuring Parent 192.168.31.113/8001/0
2018/10/26 10:06:47 kid1| Configuring Parent 192.168.31.134/8005/0
2018/10/26 10:06:47 kid1| Configuring Parent 192.168.31.134/8004/0
2018/10/26 10:06:47 kid1| Configuring Parent 192.168.31.134/8000/0
2018/10/26 10:06:47 kid1| Squid plugin modules loaded: 0
2018/10/26 10:06:47 kid1| Adaptation support is off.
2018/10/26 10:06:47 kid1| Accepting reverse-proxy HTTP Socket connections at
loc                                                                                                
al=[::]:3128 remote=[::] FD 18 flags=9
2018/10/26 10:06:47 kid1| Accepting reverse-proxy HTTPS Socket connections
at lo                                                                                                
cal=[::]:80 remote=[::] FD 19 flags=9
2018/10/26 10:06:47 kid1| Accepting reverse-proxy HTTPS Socket connections
at lo                                                                                                
cal=[::]:8000 remote=[::] FD 20 flags=9
2018/10/26 10:06:47 kid1| Accepting reverse-proxy HTTPS Socket connections
at lo                                                                                                
cal=[::]:8004 remote=[::] FD 21 flags=9
2018/10/26 10:06:47 kid1| Accepting reverse-proxy HTTPS Socket connections
at lo                                                                                                
cal=[::]:8005 remote=[::] FD 22 flags=9
2018/10/26 10:06:47 kid1| Done reading /var/cache/squid swaplog (0 entries)
2018/10/26 10:06:47 kid1| Store rebuilding is 0.00% complete
2018/10/26 10:06:47 kid1| Finished rebuilding storage from disk.
2018/10/26 10:06:47 kid1|         0 Entries scanned
2018/10/26 10:06:47 kid1|         0 Invalid entries.
2018/10/26 10:06:47 kid1|         0 With invalid flags.
2018/10/26 10:06:47 kid1|         0 Objects loaded.
2018/10/26 10:06:47 kid1|         0 Objects expired.
2018/10/26 10:06:47 kid1|         0 Objects cancelled.
2018/10/26 10:06:47 kid1|         0 Duplicate URLs purged.
2018/10/26 10:06:47 kid1|         0 Swapfile clashes avoided.
2018/10/26 10:06:47 kid1|   Took 0.02 seconds (  0.00 objects/sec).
2018/10/26 10:06:47 kid1| Beginning Validation Procedure
2018/10/26 10:06:47 kid1|   Completed Validation Procedure
2018/10/26 10:06:47 kid1|   Validated 0 Entries
2018/10/26 10:06:47 kid1|   store_swap_size = 0.00 KB
2018/10/26 10:06:48 kid1| storeLateRelease: released 0 objects
oul163:/var/log/squid #




--
Sent from: http://squid-web-proxy-cache.1019090.n4.nabble.com/Squid-Users-f1019091.html


From ajiang at ouhk.edu.hk  Fri Oct 26 02:14:55 2018
From: ajiang at ouhk.edu.hk (Angus J.)
Date: Thu, 25 Oct 2018 21:14:55 -0500 (CDT)
Subject: [squid-users] Squid proxy not working when upgrade from 27 to
	3.5
In-Reply-To: <1540519750009-0.post@n4.nabble.com>
References: <4fc98d5d-8ea8-915d-4076-b1baa35e0bee@treenet.co.nz>
 <1540284728102-0.post@n4.nabble.com>
 <20181023100039.a3tptxvtpkvggu3h@fantomas.sk>
 <1540289594180-0.post@n4.nabble.com>
 <caf407eb-9eb6-ea5d-2da9-446e3d1f80b7@treenet.co.nz>
 <1540350949260-0.post@n4.nabble.com>
 <20181024072310.4pmv3th2tptgj2ve@fantomas.sk>
 <1540459174727-0.post@n4.nabble.com>
 <ae310bd0-f196-362a-8d0f-9410091b7d34@treenet.co.nz>
 <1540519750009-0.post@n4.nabble.com>
Message-ID: <1540520095144-0.post@n4.nabble.com>

oul163:/etc/squid # squid -k parse
2018/10/26 10:14:14| Startup: Initializing Authentication Schemes ...
2018/10/26 10:14:14| Startup: Initialized Authentication Scheme 'basic'
2018/10/26 10:14:14| Startup: Initialized Authentication Scheme 'digest'
2018/10/26 10:14:14| Startup: Initialized Authentication Scheme 'negotiate'
2018/10/26 10:14:14| Startup: Initialized Authentication Scheme 'ntlm'
2018/10/26 10:14:14| Startup: Initialized Authentication.
2018/10/26 10:14:14| Processing Configuration File: /etc/squid/squid.conf
(depth 0)
2018/10/26 10:14:14| Processing: acl localnet src 10.0.0.0/8
2018/10/26 10:14:14| Processing: acl localnet src 172.16.0.0/12
2018/10/26 10:14:14| Processing: acl localnet src 192.168.0.0/16
2018/10/26 10:14:14| Processing: acl localnet src fc00::/7
2018/10/26 10:14:14| Processing: acl localnet src fe80::/10
2018/10/26 10:14:14| Processing: acl SSL_ports port 443 8000 8004 8005
2018/10/26 10:14:14| Processing: acl Safe_ports port 80
2018/10/26 10:14:14| Processing: acl Safe_ports port 21
2018/10/26 10:14:14| Processing: acl Safe_ports port 443
2018/10/26 10:14:14| Processing: acl Safe_ports port 70
2018/10/26 10:14:14| Processing: acl Safe_ports port 210
2018/10/26 10:14:14| Processing: acl Safe_ports port 1025-65535
2018/10/26 10:14:14| Processing: acl Safe_ports port 280
2018/10/26 10:14:14| Processing: acl Safe_ports port 488
2018/10/26 10:14:14| Processing: acl Safe_ports port 591
2018/10/26 10:14:14| Processing: acl Safe_ports port 777
2018/10/26 10:14:14| Processing: acl CONNECT method CONNECT
2018/10/26 10:14:14| Processing: access_log /var/log/squid/access.log
2018/10/26 10:14:14| Processing: http_access deny !Safe_ports
2018/10/26 10:14:14| Processing: http_access deny CONNECT !SSL_ports
2018/10/26 10:14:14| Processing: http_access allow localhost manager
2018/10/26 10:14:14| Processing: http_access deny manager
2018/10/26 10:14:14| Processing: http_access allow localnet
2018/10/26 10:14:14| Processing: http_access allow localhost
2018/10/26 10:14:14| Processing: http_access deny all
2018/10/26 10:14:14| Processing: coredump_dir /var/cache/squid
2018/10/26 10:14:14| Processing: refresh_pattern ^ftp: 1440 20 10080
2018/10/26 10:14:14| Processing: refresh_pattern ^gopher: 1440 0 1440
2018/10/26 10:14:14| Processing: refresh_pattern -i  (/cgi-bin/|\?) 0 0 0
2018/10/26 10:14:14| Processing: refresh_pattern . 0 20 4320
2018/10/26 10:14:14| Processing: cache_dir ufs /var/cache/squid 100 16 256
2018/10/26 10:14:14| Processing: cache_log /var/log/squid/cache.log
2018/10/26 10:14:14| Processing: cache_mem 8 MB
2018/10/26 10:14:14| Processing: cache_mgr webmaster
2018/10/26 10:14:14| Processing: cache_replacement_policy lru
2018/10/26 10:14:14| Processing: cache_store_log /var/log/squid/store.log
2018/10/26 10:14:14| Processing: cache_swap_high 95
2018/10/26 10:14:14| Processing: cache_swap_low 90
2018/10/26 10:14:14| Processing: client_lifetime 1 days
2018/10/26 10:14:14| Processing: connect_timeout 2 minutes
2018/10/26 10:14:14| Processing: error_directory /usr/share/squid/errors/en
2018/10/26 10:14:14| Processing: ftp_passive on
2018/10/26 10:14:14| Processing: maximum_object_size 4096 KB
2018/10/26 10:14:14| Processing: memory_replacement_policy lru
2018/10/26 10:14:14| Processing: minimum_object_size 0 KB
2018/10/26 10:14:14| Processing: visible_hostname oul163.ouhk.edu.hk
2018/10/26 10:14:14| Processing: http_port 3128 accel vhost
defaultsite=oul163.ouhk.edu.hk
2018/10/26 10:14:14| Processing: https_port 8005 accel
cert=/etc/squid/certs/ouhk3.crt key=/etc/squid/certs/ouhk3.key
defaultsite=oul163.ouhk.edu.hk vhost protocol=https
options=NO_SSLv3:NO_SSLv2
2018/10/26 10:14:14| Processing: sslproxy_options NO_SSLv3:NO_SSLv2
2018/10/26 10:14:14| Processing: cache_peer 192.168.31.134 parent 8005 0 ssl
sslflags=DONT_VERIFY_PEER proxy-only name=uathrms
ssloptions=NO_SSLv3:NO_SSLv2
2018/10/26 10:14:14| Processing: cache_peer_domain uathrms
uathrms.ouhk.edu.hk
2018/10/26 10:14:14| Processing: acl localip src 192.168.0.0/24
2018/10/26 10:14:14| Processing: acl hrmsacl dstdomain .ouhk.edu.hk
2018/10/26 10:14:14| Processing: http_access allow hrmsacl
2018/10/26 10:14:14| Processing: cache_peer_access prdhrms allow hrmsacl
2018/10/26 10:14:14| /etc/squid/squid.conf, line 154: No cache_peer
'prdhrms'
2018/10/26 10:14:14| Processing: cache_peer_access uathrms allow hrmsacl
2018/10/26 10:14:14| Processing: cache_peer_access sithrms allow hrmsacl
2018/10/26 10:14:14| /etc/squid/squid.conf, line 156: No cache_peer
'sithrms'
2018/10/26 10:14:14| Processing: cache_peer_access devhrms allow hrmsacl
2018/10/26 10:14:14| /etc/squid/squid.conf, line 157: No cache_peer
'devhrms'
2018/10/26 10:14:14| Initializing https proxy context
2018/10/26 10:14:14| Initializing cache_peer uathrms SSL context
2018/10/26 10:14:14| Initializing https_port [::]:8005 SSL context
2018/10/26 10:14:14| Using certificate in /etc/squid/certs/ouhk3.crt




--
Sent from: http://squid-web-proxy-cache.1019090.n4.nabble.com/Squid-Users-f1019091.html


From eliezer at ngtech.co.il  Fri Oct 26 07:08:56 2018
From: eliezer at ngtech.co.il (Eliezer Croitoru)
Date: Fri, 26 Oct 2018 10:08:56 +0300
Subject: [squid-users] Squid proxy not working when upgrade from 27 to
	3.5
In-Reply-To: <1540519560433-0.post@n4.nabble.com>
References: <1540265281548-0.post@n4.nabble.com>
 <4fc98d5d-8ea8-915d-4076-b1baa35e0bee@treenet.co.nz>
 <1540284728102-0.post@n4.nabble.com>
 <20181023100039.a3tptxvtpkvggu3h@fantomas.sk>
 <1540289594180-0.post@n4.nabble.com>
 <caf407eb-9eb6-ea5d-2da9-446e3d1f80b7@treenet.co.nz>
 <1540350949260-0.post@n4.nabble.com>
 <20181024072310.4pmv3th2tptgj2ve@fantomas.sk>
 <1540459174727-0.post@n4.nabble.com>
 <ae310bd0-f196-362a-8d0f-9410091b7d34@treenet.co.nz>
 <1540519560433-0.post@n4.nabble.com>
Message-ID: <582f901ead8e9d3d0dd324e36bc39951@ngtech.co.il>

Hey Angus, 

There are couple times of configuration "definition". 
Some of them cannot overlap since they contain a full instruction. 
When a http_port line is parsed by squid it's a fix setup of
configuration argument. 
There are other services and/or servers that updates the configuration
argument with every line. 
For specific instructions like "listen on *:3128" in squid there is only
one line that can be accepted. 
If the service operator instruct's squid to do something which cannot be
done squid will not do that. 
Maybe in the future someone will enhance squid to allow "progressive"
http_port configuration but I believe it's wrong. 

All The Bests, 
Eliezer 

On 2018-10-26 05:06, Angus J. wrote: 

> Hi Amos 
> 
> # Squid normally listens to port 3128
> http_port 3128
> 
> http_port 3128 accel vhost defaultsite=oul163.ouhk.edu.hk
> 
> This two line of squid.conf , they will cause the ERROR" ?
> 
> --
> Sent from:
> http://squid-web-proxy-cache.1019090.n4.nabble.com/Squid-Users-f1019091.html
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users

-- 
----
Eliezer Croitoru
Linux System Administrator
Mobile: +972-5-28704261
Email: eliezer at ngtech.co.il
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20181026/288f8dc2/attachment.htm>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: e5d52439.png
Type: image/png
Size: 9459 bytes
Desc: not available
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20181026/288f8dc2/attachment.png>

From ajiang at ouhk.edu.hk  Fri Oct 26 07:26:54 2018
From: ajiang at ouhk.edu.hk (Angus J.)
Date: Fri, 26 Oct 2018 02:26:54 -0500 (CDT)
Subject: [squid-users] Squid proxy not working when upgrade from 27 to
	3.5
In-Reply-To: <582f901ead8e9d3d0dd324e36bc39951@ngtech.co.il>
References: <1540284728102-0.post@n4.nabble.com>
 <20181023100039.a3tptxvtpkvggu3h@fantomas.sk>
 <1540289594180-0.post@n4.nabble.com>
 <caf407eb-9eb6-ea5d-2da9-446e3d1f80b7@treenet.co.nz>
 <1540350949260-0.post@n4.nabble.com>
 <20181024072310.4pmv3th2tptgj2ve@fantomas.sk>
 <1540459174727-0.post@n4.nabble.com>
 <ae310bd0-f196-362a-8d0f-9410091b7d34@treenet.co.nz>
 <1540519560433-0.post@n4.nabble.com>
 <582f901ead8e9d3d0dd324e36bc39951@ngtech.co.il>
Message-ID: <1540538814938-0.post@n4.nabble.com>

What's wrong of my squid.conf from 27 to 3.5? 
The port 3128 issue has been fixed


# multiling http
acl localnet src 10.0.0.0/8
acl localnet src 172.16.0.0/12
acl localnet src 192.168.0.0/16
acl localnet src fc00::/7
acl localnet src fe80::/10
acl SSL_ports port 443 8000 8004 8005
acl Safe_ports port 80
acl Safe_ports port 21
acl Safe_ports port 443
acl Safe_ports port 70
acl Safe_ports port 210
acl Safe_ports port 1025-65535
acl Safe_ports port 280
acl Safe_ports port 488
acl Safe_ports port 591
acl Safe_ports port 777
acl CONNECT method CONNECT

access_log /var/log/squid/access.log

#
# Recommended minimum Access Permission configuration:
#
# Deny requests to certain unsafe ports
# Deny CONNECT to other than secure SSL ports
# Only allow cachemgr access from localhost
# Example rule allowing access from your local networks.
# Adapt localnet in the ACL section to list your (internal) IP networks
# from where browsing should be allowed
# Allow localhost always proxy functionality
# And finally deny all other access to this proxy
http_access deny !Safe_ports
http_access deny CONNECT !SSL_ports
http_access allow localhost manager
http_access deny manager
http_access allow localnet
http_access allow localhost
http_access deny all

# We strongly recommend the following be uncommented to protect innocent
# web applications running on the proxy server who think the only
# one who can access services on "localhost" is a local user
#http_access deny to_localhost

#
# INSERT YOUR OWN RULE(S) HERE TO ALLOW ACCESS FROM YOUR CLIENTS
#

# Squid normally listens to port 3128
#http_port 3128

# Uncomment and adjust the following to add a disk cache directory.
#cache_dir aufs /var/cache/squid 100 16 256

# Leave coredumps in the first cache dir
coredump_dir /var/cache/squid

#
# Add any of your own refresh_pattern entries above these.
#
refresh_pattern ^ftp: 1440 20 10080
refresh_pattern ^gopher: 1440 0 1440
refresh_pattern -i  (/cgi-bin/|\?) 0 0 0
refresh_pattern . 0 20 4320

cache_dir ufs /var/cache/squid 100 16 256

cache_log /var/log/squid/cache.log

cache_mem 8 MB

cache_mgr webmaster

cache_replacement_policy lru

cache_store_log /var/log/squid/store.log

cache_swap_high 95

cache_swap_low 90

client_lifetime 1 days

connect_timeout 2 minutes

error_directory /usr/share/squid/errors/en

ftp_passive on

maximum_object_size 4096 KB

memory_replacement_policy lru

minimum_object_size 0 KB

visible_hostname oul163.ouhk.edu.hk
http_port 3128 accel vhost defaultsite=oul163.ouhk.edu.hk
#https_port 80 accel cert=/etc/squid/certs/ouhk.crt
key=/etc/squid/certs/ouhk.key defaultsite=oul163.ouhk.edu.hk vhost
protocol=https options=NO_SSLv3:NO_SSLv2
#https_port 8000 accel cert=/etc/squid/certs/ouhk.crt
key=/etc/squid/certs/ouhk.key defaultsite=oul163.ouhk.edu.hk vhost
protocol=https options=NO_SSLv3:NO_SSLv2
#https_port 8004 accel cert=/etc/squid/certs/ouhk.crt
key=/etc/squid/certs/ouhk.key defaultsite=oul163.ouhk.edu.hk vhost
protocol=https options=NO_SSLv3:NO_SSLv2
#https_port 8004 accel cert=/etc/squid/certs/ouhk2.crt
key=/etc/squid/certs/ouhk2.key defaultsite=oul163.ouhk.edu.hk vhost
protocol=https options=NO_SSLv3:NO_SSLv2
#https_port 8005 accel cert=/etc/squid/certs/ouhk.crt
key=/etc/squid/certs/ouhk.key defaultsite=oul163.ouhk.edu.hk vhost
protocol=https options=NO_SSLv3:NO_SSLv2
https_port 8005 accel cert=/etc/squid/certs/ouhk3.crt
key=/etc/squid/certs/ouhk3.key defaultsite=oul163.ouhk.edu.hk vhost
protocol=https options=NO_SSLv3:NO_SSLv2
#ssl_bump allow all
#              Disable the following one
#ssl_bump options=NO_SSLv3
#always_direct allow all
#              Disable the following one
#sslproxy_cert_error allow all
sslproxy_options NO_SSLv3:NO_SSLv2

# the proxy-only indicates that caching will not be performed.
#cache_peer 192.168.31.113 parent 8001 0 proxy-only name=prdhrms
#cache_peer_domain prdhrms prdhrms.ouhk.edu.hk
cache_peer 192.168.31.134 parent 8005 0 ssl sslflags=DONT_VERIFY_PEER
proxy-only name=uathrms ssloptions=NO_SSLv3:NO_SSLv2
#cache_peer 192.168.31.134 parent 8005 0 ssl sslflags=DONT_VERIFY_DOMAIN
proxy-only name=uathrms ssloptions=NO_SSLv3:NO_SSLv2
cache_peer_domain uathrms uathrms.ouhk.edu.hk
#cache_peer 192.168.31.134 parent 8004 0 ssl sslflags=DONT_VERIFY_PEER
proxy-only name=sithrms ssloptions=NO_SSLv3:NO_SSLv2
#cache_peer_domain sithrms sithrms.ouhk.edu.hk
#cache_peer 192.168.31.134 parent 8000 0 ssl sslflags=DONT_VERIFY_PEER
proxy-only name=devhrms ssloptions=NO_SSLv3:NO_SSLv2
#cache_peer 192.168.31.134 parent 8000 0 proxy-only originserver
name=devhrms ssll sslcafile=/certs/star_ouhk_edu_hk.crt
#cache_peer_domain devhrms devhrms.ouhk.edu.hk

# Create an additional ACL for local network access
acl localip src 192.168.0.0/24

# access control list
acl hrmsacl dstdomain .ouhk.edu.hk
http_access allow hrmsacl
#acl hrmsacl2 dstdomain devhrms.ouhk.edu.hk
#cache_peer_access devhrms allow hrmsacl2
cache_peer_access prdhrms allow hrmsacl
cache_peer_access uathrms allow hrmsacl
cache_peer_access sithrms allow hrmsacl
cache_peer_access devhrms allow hrmsacl
#cache_peer_access secure allow SSL_ports




--
Sent from: http://squid-web-proxy-cache.1019090.n4.nabble.com/Squid-Users-f1019091.html


From squid3 at treenet.co.nz  Fri Oct 26 09:25:57 2018
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Fri, 26 Oct 2018 22:25:57 +1300
Subject: [squid-users] Squid proxy not working when upgrade from 27 to
	3.5
In-Reply-To: <1540538814938-0.post@n4.nabble.com>
References: <1540284728102-0.post@n4.nabble.com>
 <20181023100039.a3tptxvtpkvggu3h@fantomas.sk>
 <1540289594180-0.post@n4.nabble.com>
 <caf407eb-9eb6-ea5d-2da9-446e3d1f80b7@treenet.co.nz>
 <1540350949260-0.post@n4.nabble.com>
 <20181024072310.4pmv3th2tptgj2ve@fantomas.sk>
 <1540459174727-0.post@n4.nabble.com>
 <ae310bd0-f196-362a-8d0f-9410091b7d34@treenet.co.nz>
 <1540519560433-0.post@n4.nabble.com>
 <582f901ead8e9d3d0dd324e36bc39951@ngtech.co.il>
 <1540538814938-0.post@n4.nabble.com>
Message-ID: <edf1c4c0-2c7e-ee05-b8b5-a2d1ba40ffb3@treenet.co.nz>

On 26/10/18 8:26 PM, Angus J. wrote:
> What's wrong of my squid.conf from 27 to 3.5? 
> The port 3128 issue has been fixed
> 

Yes that one is fixed. Now it is complaining about what you changed in
cache_peer lines.


>
> oul163:/etc/squid # squid -k parse...> 2018/10/26 10:14:14|
Processing: cache_peer 192.168.31.134 parent 8005 0 ssl
> sslflags=DONT_VERIFY_PEER proxy-only name=uathrms
> ssloptions=NO_SSLv3:NO_SSLv2
> 2018/10/26 10:14:14| Processing: cache_peer_domain uathrms
> uathrms.ouhk.edu.hk

> 2018/10/26 10:14:14| Processing: cache_peer_access prdhrms allow
hrmsacl> 2018/10/26 10:14:14| /etc/squid/squid.conf, line 154: No cache_peer
> 'prdhrms'
> 2018/10/26 10:14:14| Processing: cache_peer_access uathrms allow hrmsacl
> 2018/10/26 10:14:14| Processing: cache_peer_access sithrms allow hrmsacl
> 2018/10/26 10:14:14| /etc/squid/squid.conf, line 156: No cache_peer
> 'sithrms'

> 2018/10/26 10:14:14| Processing: cache_peer_access devhrms allow hrmsacl
> 2018/10/26 10:14:14| /etc/squid/squid.conf, line 157: No cache_peer
> 'devhrms'


>From the config:

> 
> # the proxy-only indicates that caching will not be performed.
> #cache_peer 192.168.31.113 parent 8001 0 proxy-only name=prdhrms
> #cache_peer_domain prdhrms prdhrms.ouhk.edu.hk
> cache_peer 192.168.31.134 parent 8005 0 ssl sslflags=DONT_VERIFY_PEER
> proxy-only name=uathrms ssloptions=NO_SSLv3:NO_SSLv2
> #cache_peer 192.168.31.134 parent 8005 0 ssl sslflags=DONT_VERIFY_DOMAIN
> proxy-only name=uathrms ssloptions=NO_SSLv3:NO_SSLv2
> cache_peer_domain uathrms uathrms.ouhk.edu.hk
> #cache_peer 192.168.31.134 parent 8004 0 ssl sslflags=DONT_VERIFY_PEER
> proxy-only name=sithrms ssloptions=NO_SSLv3:NO_SSLv2
> #cache_peer_domain sithrms sithrms.ouhk.edu.hk
> #cache_peer 192.168.31.134 parent 8000 0 ssl sslflags=DONT_VERIFY_PEER
> proxy-only name=devhrms ssloptions=NO_SSLv3:NO_SSLv2
> #cache_peer 192.168.31.134 parent 8000 0 proxy-only originserver
> name=devhrms ssll sslcafile=/certs/star_ouhk_edu_hk.crt
> #cache_peer_domain devhrms devhrms.ouhk.edu.hk
> 


You commented out the cache_peer lines defining those peer connections
and Squid does not know what to peer the cache_peer_access definitions
are mentioning.

The only thing that needed removing/replacing was the
"cache_peer_domain" lines.

>From the config:

> # Create an additional ACL for local network access
> acl localip src 192.168.0.0/24
> 
> # access control list
> acl hrmsacl dstdomain .ouhk.edu.hk
> http_access allow hrmsacl
> #acl hrmsacl2 dstdomain devhrms.ouhk.edu.hk
> #cache_peer_access devhrms allow hrmsacl2
> cache_peer_access prdhrms allow hrmsacl
> cache_peer_access uathrms allow hrmsacl
> cache_peer_access sithrms allow hrmsacl
> cache_peer_access devhrms allow hrmsacl
> #cache_peer_access secure allow SSL_ports
> 

FYI: These rules are far more lenient than what you had before with
cache_peer_domain.

The previous config let *only* certain domains to each individual peer.
These rules now allows *any* sub-domain to any peer.

I suggest keeping to the minimal change until you are happy with the new
proxy behaviour. The exact equivalent of these lines:

  cache_peer 192.168.31.113 parent 8001 0 proxy-only name=prdhrms
  cache_peer_domain prdhrms prdhrms.hkbb.edu.hk

  cache_peer 192.168.31.134 parent 8005 ... name=uathrms
  cache_peer_domain uathrms uathrms.hkbb.edu.hk

  cache_peer 192.168.31.134 parent 8004 ... name=sithrms
  cache_peer_domain sithrms sithrms.hkbb.edu.hk

  cache_peer 192.168.31.134 parent 8000 ... name=devhrms
  cache_peer_domain devhrms devhrms.hkbb.edu.hk


Are these lines:

  cache_peer 192.168.31.113 parent 8001 0 proxy-only name=prdhrms
  acl prdhrms-domain dstdomain prdhrms.hkbb.edu.hk
  cache_peer_access prdhrms allow prdhrms-domain

  cache_peer 192.168.31.134 parent 8005 ... name=uathrms
  acl uathrms-domain dstdomain uathrms.hkbb.edu.hk
  cache_peer_access uathrms allow uathrms-domain

  cache_peer 192.168.31.134 parent 8004 ... name=sithrms
  acl sithrms-domain dstdomain sithrms.hkbb.edu.hk
  cache_peer_access sithrms allow sithrms-domain

  cache_peer 192.168.31.134 parent 8000 ... name=devhrms
  acl devhrms-domain dstdomain devhrms.hkbb.edu.hk
  cache_peer_access devhrms allow devhrms-domain



Note that use of the exact sub-domain names remains in place rather than
opening everything to the wildcards midway during a proxy upgrade.

Amos


From ajiang at ouhk.edu.hk  Fri Oct 26 09:45:03 2018
From: ajiang at ouhk.edu.hk (Angus J.)
Date: Fri, 26 Oct 2018 04:45:03 -0500 (CDT)
Subject: [squid-users] Squid proxy not working when upgrade from 27 to
	3.5
In-Reply-To: <edf1c4c0-2c7e-ee05-b8b5-a2d1ba40ffb3@treenet.co.nz>
References: <1540289594180-0.post@n4.nabble.com>
 <caf407eb-9eb6-ea5d-2da9-446e3d1f80b7@treenet.co.nz>
 <1540350949260-0.post@n4.nabble.com>
 <20181024072310.4pmv3th2tptgj2ve@fantomas.sk>
 <1540459174727-0.post@n4.nabble.com>
 <ae310bd0-f196-362a-8d0f-9410091b7d34@treenet.co.nz>
 <1540519560433-0.post@n4.nabble.com>
 <582f901ead8e9d3d0dd324e36bc39951@ngtech.co.il>
 <1540538814938-0.post@n4.nabble.com>
 <edf1c4c0-2c7e-ee05-b8b5-a2d1ba40ffb3@treenet.co.nz>
Message-ID: <1540547103606-0.post@n4.nabble.com>

oul163:/etc/squid # squid -k parse
2018/10/26 17:44:42| Startup: Initializing Authentication Schemes ...
2018/10/26 17:44:42| Startup: Initialized Authentication Scheme 'basic'
2018/10/26 17:44:42| Startup: Initialized Authentication Scheme 'digest'
2018/10/26 17:44:42| Startup: Initialized Authentication Scheme 'negotiate'
2018/10/26 17:44:42| Startup: Initialized Authentication Scheme 'ntlm'
2018/10/26 17:44:42| Startup: Initialized Authentication.
2018/10/26 17:44:42| Processing Configuration File: /etc/squid/squid.conf
(depth 0)
2018/10/26 17:44:42| Processing: acl localnet src 10.0.0.0/8
2018/10/26 17:44:42| Processing: acl localnet src 172.16.0.0/12
2018/10/26 17:44:42| Processing: acl localnet src 192.168.0.0/16
2018/10/26 17:44:42| Processing: acl localnet src fc00::/7
2018/10/26 17:44:42| Processing: acl localnet src fe80::/10
2018/10/26 17:44:42| Processing: acl SSL_ports port 443 8000 8004 8005
2018/10/26 17:44:42| Processing: acl Safe_ports port 80
2018/10/26 17:44:42| Processing: acl Safe_ports port 21
2018/10/26 17:44:42| Processing: acl Safe_ports port 443
2018/10/26 17:44:42| Processing: acl Safe_ports port 70
2018/10/26 17:44:42| Processing: acl Safe_ports port 210
2018/10/26 17:44:42| Processing: acl Safe_ports port 1025-65535
2018/10/26 17:44:42| Processing: acl Safe_ports port 280
2018/10/26 17:44:42| Processing: acl Safe_ports port 488
2018/10/26 17:44:42| Processing: acl Safe_ports port 591
2018/10/26 17:44:42| Processing: acl Safe_ports port 777
2018/10/26 17:44:42| Processing: acl CONNECT method CONNECT
2018/10/26 17:44:42| Processing: access_log /var/log/squid/access.log
2018/10/26 17:44:42| Processing: http_access deny !Safe_ports
2018/10/26 17:44:42| Processing: http_access deny CONNECT !SSL_ports
2018/10/26 17:44:42| Processing: http_access allow localhost manager
2018/10/26 17:44:42| Processing: http_access deny manager
2018/10/26 17:44:42| Processing: http_access allow localnet
2018/10/26 17:44:42| Processing: http_access allow localhost
2018/10/26 17:44:42| Processing: http_access deny all
2018/10/26 17:44:42| Processing: coredump_dir /var/cache/squid
2018/10/26 17:44:42| Processing: refresh_pattern ^ftp: 1440 20 10080
2018/10/26 17:44:42| Processing: refresh_pattern ^gopher: 1440 0 1440
2018/10/26 17:44:42| Processing: refresh_pattern -i  (/cgi-bin/|\?) 0 0 0
2018/10/26 17:44:42| Processing: refresh_pattern . 0 20 4320
2018/10/26 17:44:42| Processing: cache_dir ufs /var/cache/squid 100 16 256
2018/10/26 17:44:42| Processing: cache_log /var/log/squid/cache.log
2018/10/26 17:44:42| Processing: cache_mem 8 MB
2018/10/26 17:44:42| Processing: cache_mgr webmaster
2018/10/26 17:44:42| Processing: cache_replacement_policy lru
2018/10/26 17:44:42| Processing: cache_store_log /var/log/squid/store.log
2018/10/26 17:44:42| Processing: cache_swap_high 95
2018/10/26 17:44:42| Processing: cache_swap_low 90
2018/10/26 17:44:42| Processing: client_lifetime 1 days
2018/10/26 17:44:42| Processing: connect_timeout 2 minutes
2018/10/26 17:44:42| Processing: error_directory /usr/share/squid/errors/en
2018/10/26 17:44:42| Processing: ftp_passive on
2018/10/26 17:44:42| Processing: maximum_object_size 4096 KB
2018/10/26 17:44:42| Processing: memory_replacement_policy lru
2018/10/26 17:44:42| Processing: minimum_object_size 0 KB
2018/10/26 17:44:42| Processing: visible_hostname oul163.ouhk.edu.hk
2018/10/26 17:44:42| Processing: http_port 3128 accel vhost
defaultsite=oul163.ouhk.edu.hk
2018/10/26 17:44:42| Processing: sslproxy_options NO_SSLv3:NO_SSLv2
2018/10/26 17:44:42| Processing: acl localip src 192.168.0.0/24
2018/10/26 17:44:42| Processing: acl hrmsacl dstdomain .ouhk.edu.hk
2018/10/26 17:44:42| Processing: http_access allow hrmsacl
2018/10/26 17:44:42| Processing: cache_peer 192.168.31.113 parent 8001 1
proxy-only name=prdhrms
2018/10/26 17:44:42| Processing: acl prdhrms-domain dstdomain
prdhrms.ouhk.edu.hk
2018/10/26 17:44:42| Processing: cache_peer_access prdhrms allow
prdhrms-domain
2018/10/26 17:44:42| Processing: cache_peer 192.168.31.134 parent 8005 0
name=uathrms
2018/10/26 17:44:42| Processing: acl uathrms-domain dstdomain
uathrms.ouhk.edu.hk
2018/10/26 17:44:42| Processing: cache_peer_access uathrms allow
uathrms-domain
2018/10/26 17:44:42| Processing: cache_peer 192.168.31.134 parent 8004 2
name=sithrms
2018/10/26 17:44:42| Processing: acl sithrms-domain dstdomain
sithrms.ouhk.edu.hk
2018/10/26 17:44:42| Processing: cache_peer_access sithrms allow
sithrms-domain
2018/10/26 17:44:42| Processing: cache_peer 192.168.31.134 parent 8000 3
name=devhrms
2018/10/26 17:44:42| Processing: acl devhrms-domain dstdomain
devhrms.ouhk.edu.hk
2018/10/26 17:44:42| Processing: cache_peer_access devhrms allow
devhrms-domain
2018/10/26 17:44:42| Initializing https proxy context




--
Sent from: http://squid-web-proxy-cache.1019090.n4.nabble.com/Squid-Users-f1019091.html


From ajiang at ouhk.edu.hk  Fri Oct 26 09:46:26 2018
From: ajiang at ouhk.edu.hk (Angus J.)
Date: Fri, 26 Oct 2018 04:46:26 -0500 (CDT)
Subject: [squid-users] Squid proxy not working when upgrade from 27 to
	3.5
In-Reply-To: <1540547103606-0.post@n4.nabble.com>
References: <caf407eb-9eb6-ea5d-2da9-446e3d1f80b7@treenet.co.nz>
 <1540350949260-0.post@n4.nabble.com>
 <20181024072310.4pmv3th2tptgj2ve@fantomas.sk>
 <1540459174727-0.post@n4.nabble.com>
 <ae310bd0-f196-362a-8d0f-9410091b7d34@treenet.co.nz>
 <1540519560433-0.post@n4.nabble.com>
 <582f901ead8e9d3d0dd324e36bc39951@ngtech.co.il>
 <1540538814938-0.post@n4.nabble.com>
 <edf1c4c0-2c7e-ee05-b8b5-a2d1ba40ffb3@treenet.co.nz>
 <1540547103606-0.post@n4.nabble.com>
Message-ID: <1540547186478-0.post@n4.nabble.com>

I have updated the squid.conf as below:

# access control list
acl hrmsacl dstdomain .ouhk.edu.hk
http_access allow hrmsacl
#acl hrmsacl2 dstdomain devhrms.ouhk.edu.hk
#cache_peer_access devhrms allow hrmsacl2
#cache_peer_access prdhrms allow hrmsacl
#cache_peer_access uathrms allow hrmsacl
#cache_peer_access sithrms allow hrmsacl
#cache_peer_access devhrms allow hrmsacl
#cache_peer_access secure allow SSL_ports
cache_peer 192.168.31.113 parent 8001 1 proxy-only name=prdhrms
acl prdhrms-domain dstdomain prdhrms.ouhk.edu.hk
cache_peer_access prdhrms allow prdhrms-domain

cache_peer 192.168.31.134 parent 8005 0 name=uathrms
acl uathrms-domain dstdomain uathrms.ouhk.edu.hk
cache_peer_access uathrms allow uathrms-domain

cache_peer 192.168.31.134 parent 8004 2 name=sithrms
acl sithrms-domain dstdomain sithrms.ouhk.edu.hk
cache_peer_access sithrms allow sithrms-domain

cache_peer 192.168.31.134 parent 8000 3 name=devhrms
acl devhrms-domain dstdomain devhrms.ouhk.edu.hk
cache_peer_access devhrms allow devhrms-domain




--
Sent from: http://squid-web-proxy-cache.1019090.n4.nabble.com/Squid-Users-f1019091.html


From ajiang at ouhk.edu.hk  Fri Oct 26 09:49:46 2018
From: ajiang at ouhk.edu.hk (Angus J.)
Date: Fri, 26 Oct 2018 04:49:46 -0500 (CDT)
Subject: [squid-users] Squid proxy not working when upgrade from 27 to
	3.5
In-Reply-To: <1540547186478-0.post@n4.nabble.com>
References: <1540350949260-0.post@n4.nabble.com>
 <20181024072310.4pmv3th2tptgj2ve@fantomas.sk>
 <1540459174727-0.post@n4.nabble.com>
 <ae310bd0-f196-362a-8d0f-9410091b7d34@treenet.co.nz>
 <1540519560433-0.post@n4.nabble.com>
 <582f901ead8e9d3d0dd324e36bc39951@ngtech.co.il>
 <1540538814938-0.post@n4.nabble.com>
 <edf1c4c0-2c7e-ee05-b8b5-a2d1ba40ffb3@treenet.co.nz>
 <1540547103606-0.post@n4.nabble.com> <1540547186478-0.post@n4.nabble.com>
Message-ID: <1540547386669-0.post@n4.nabble.com>

https://uathrms.ouhk.edu.hk:8005/OA_HTML/AppsLogin


the screen will shown 

This site can?t be reached
uathrms.ouhk.edu.hk refused to connect.
Try:

Checking the connection
Checking the proxy and the firewall
ERR_CONNECTION_REFUSED



--
Sent from: http://squid-web-proxy-cache.1019090.n4.nabble.com/Squid-Users-f1019091.html


From squid3 at treenet.co.nz  Fri Oct 26 10:00:25 2018
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Fri, 26 Oct 2018 23:00:25 +1300
Subject: [squid-users] Squid proxy not working when upgrade from 27 to
	3.5
In-Reply-To: <1540547386669-0.post@n4.nabble.com>
References: <1540350949260-0.post@n4.nabble.com>
 <20181024072310.4pmv3th2tptgj2ve@fantomas.sk>
 <1540459174727-0.post@n4.nabble.com>
 <ae310bd0-f196-362a-8d0f-9410091b7d34@treenet.co.nz>
 <1540519560433-0.post@n4.nabble.com>
 <582f901ead8e9d3d0dd324e36bc39951@ngtech.co.il>
 <1540538814938-0.post@n4.nabble.com>
 <edf1c4c0-2c7e-ee05-b8b5-a2d1ba40ffb3@treenet.co.nz>
 <1540547103606-0.post@n4.nabble.com> <1540547186478-0.post@n4.nabble.com>
 <1540547386669-0.post@n4.nabble.com>
Message-ID: <8b3ece11-d04c-9856-67e4-bed0b274735c@treenet.co.nz>

On 26/10/18 10:49 PM, Angus J. wrote:
> https://uathrms.ouhk.edu.hk:8005/OA_HTML/AppsLogin
> 
> 
> the screen will shown 
> 
> This site can?t be reached
> uathrms.ouhk.edu.hk refused to connect.
> Try:
> 
> Checking the connection
> Checking the proxy and the firewall
> ERR_CONNECTION_REFUSED
> 


This time you removed the TLS/SSL settings from the cache_peer lines and
added port numbers to enable ICP communication between the proxies (are
the peer proxies?).

In that config snippet of my last email I used "..." to represent the
long set of config options on your previous peer lines. Leave all those
options set the way they were previously in your config.

Amos


From ajiang at ouhk.edu.hk  Fri Oct 26 10:05:46 2018
From: ajiang at ouhk.edu.hk (Angus J.)
Date: Fri, 26 Oct 2018 05:05:46 -0500 (CDT)
Subject: [squid-users] Squid proxy not working when upgrade from 27 to
	3.5
In-Reply-To: <8b3ece11-d04c-9856-67e4-bed0b274735c@treenet.co.nz>
References: <1540459174727-0.post@n4.nabble.com>
 <ae310bd0-f196-362a-8d0f-9410091b7d34@treenet.co.nz>
 <1540519560433-0.post@n4.nabble.com>
 <582f901ead8e9d3d0dd324e36bc39951@ngtech.co.il>
 <1540538814938-0.post@n4.nabble.com>
 <edf1c4c0-2c7e-ee05-b8b5-a2d1ba40ffb3@treenet.co.nz>
 <1540547103606-0.post@n4.nabble.com> <1540547186478-0.post@n4.nabble.com>
 <1540547386669-0.post@n4.nabble.com>
 <8b3ece11-d04c-9856-67e4-bed0b274735c@treenet.co.nz>
Message-ID: <1540548346870-0.post@n4.nabble.com>



oul163:/var/log/squid # squid -k parse
2018/10/26 18:04:25| Startup: Initializing Authentication Schemes ...
2018/10/26 18:04:25| Startup: Initialized Authentication Scheme 'basic'
2018/10/26 18:04:25| Startup: Initialized Authentication Scheme 'digest'
2018/10/26 18:04:25| Startup: Initialized Authentication Scheme 'negotiate'
2018/10/26 18:04:25| Startup: Initialized Authentication Scheme 'ntlm'
2018/10/26 18:04:25| Startup: Initialized Authentication.
2018/10/26 18:04:25| Processing Configuration File: /etc/squid/squid.conf
(depth 0)
2018/10/26 18:04:25| Processing: acl localnet src 10.0.0.0/8
2018/10/26 18:04:25| Processing: acl localnet src 172.16.0.0/12
2018/10/26 18:04:25| Processing: acl localnet src 192.168.0.0/16
2018/10/26 18:04:25| Processing: acl localnet src fc00::/7
2018/10/26 18:04:25| Processing: acl localnet src fe80::/10
2018/10/26 18:04:25| Processing: acl SSL_ports port 443 8000 8004 8005
2018/10/26 18:04:25| Processing: acl Safe_ports port 80
2018/10/26 18:04:25| Processing: acl Safe_ports port 21
2018/10/26 18:04:25| Processing: acl Safe_ports port 443
2018/10/26 18:04:25| Processing: acl Safe_ports port 70
2018/10/26 18:04:25| Processing: acl Safe_ports port 210
2018/10/26 18:04:25| Processing: acl Safe_ports port 1025-65535
2018/10/26 18:04:25| Processing: acl Safe_ports port 280
2018/10/26 18:04:25| Processing: acl Safe_ports port 488
2018/10/26 18:04:25| Processing: acl Safe_ports port 591
2018/10/26 18:04:25| Processing: acl Safe_ports port 777
2018/10/26 18:04:25| Processing: acl CONNECT method CONNECT
2018/10/26 18:04:25| Processing: access_log /var/log/squid/access.log
2018/10/26 18:04:25| Processing: http_access deny !Safe_ports
2018/10/26 18:04:25| Processing: http_access deny CONNECT !SSL_ports
2018/10/26 18:04:25| Processing: http_access allow localhost manager
2018/10/26 18:04:25| Processing: http_access deny manager
2018/10/26 18:04:25| Processing: http_access allow localnet
2018/10/26 18:04:25| Processing: http_access allow localhost
2018/10/26 18:04:25| Processing: http_access deny all
2018/10/26 18:04:25| Processing: coredump_dir /var/cache/squid
2018/10/26 18:04:25| Processing: refresh_pattern ^ftp: 1440 20 10080
2018/10/26 18:04:25| Processing: refresh_pattern ^gopher: 1440 0 1440
2018/10/26 18:04:25| Processing: refresh_pattern -i  (/cgi-bin/|\?) 0 0 0
2018/10/26 18:04:25| Processing: refresh_pattern . 0 20 4320
2018/10/26 18:04:25| Processing: cache_dir ufs /var/cache/squid 100 16 256
2018/10/26 18:04:25| Processing: cache_log /var/log/squid/cache.log
2018/10/26 18:04:25| Processing: cache_mem 8 MB
2018/10/26 18:04:25| Processing: cache_mgr webmaster
2018/10/26 18:04:25| Processing: cache_replacement_policy lru
2018/10/26 18:04:25| Processing: cache_store_log /var/log/squid/store.log
2018/10/26 18:04:25| Processing: cache_swap_high 95
2018/10/26 18:04:25| Processing: cache_swap_low 90
2018/10/26 18:04:25| Processing: client_lifetime 1 days
2018/10/26 18:04:25| Processing: connect_timeout 2 minutes
2018/10/26 18:04:25| Processing: error_directory /usr/share/squid/errors/en
2018/10/26 18:04:25| Processing: ftp_passive on
2018/10/26 18:04:25| Processing: maximum_object_size 4096 KB
2018/10/26 18:04:25| Processing: memory_replacement_policy lru
2018/10/26 18:04:25| Processing: minimum_object_size 0 KB
2018/10/26 18:04:25| Processing: visible_hostname oul163.ouhk.edu.hk
2018/10/26 18:04:25| Processing: http_port 3128 accel vhost
defaultsite=oul163.ouhk.edu.hk
2018/10/26 18:04:25| Processing: sslproxy_options NO_SSLv3:NO_SSLv2
2018/10/26 18:04:25| Processing: acl localip src 192.168.0.0/24
2018/10/26 18:04:25| Processing: acl hrmsacl dstdomain .ouhk.edu.hk
2018/10/26 18:04:25| Processing: http_access allow hrmsacl
2018/10/26 18:04:25| Processing: cache_peer 192.168.31.113 parent 8001 1
proxy-only name=prdhrms
2018/10/26 18:04:25| Processing: acl prdhrms-domain dstdomain
prdhrms.ouhk.edu.hk
2018/10/26 18:04:25| Processing: cache_peer_access prdhrms allow
prdhrms-domain
2018/10/26 18:04:25| Processing: cache_peer 192.168.31.134 parent 8005 0 
proxy-only name=uathrms
2018/10/26 18:04:25| Processing: acl uathrms-domain dstdomain
uathrms.ouhk.edu.hk
2018/10/26 18:04:25| Processing: cache_peer_access uathrms allow
uathrms-domain
2018/10/26 18:04:25| Processing: cache_peer 192.168.31.134 parent 8004 2 
proxy-only name=sithrms
2018/10/26 18:04:25| Processing: acl sithrms-domain dstdomain
sithrms.ouhk.edu.hk
2018/10/26 18:04:25| Processing: cache_peer_access sithrms allow
sithrms-domain
2018/10/26 18:04:25| Processing: cache_peer 192.168.31.134 parent 8000 3 
proxy-only name=devhrms
2018/10/26 18:04:25| Processing: acl devhrms-domain dstdomain
devhrms.ouhk.edu.hk
2018/10/26 18:04:25| Processing: cache_peer_access devhrms allow
devhrms-domain
2018/10/26 18:04:25| Initializing https proxy context




--
Sent from: http://squid-web-proxy-cache.1019090.n4.nabble.com/Squid-Users-f1019091.html


From ajiang at ouhk.edu.hk  Fri Oct 26 10:08:11 2018
From: ajiang at ouhk.edu.hk (Angus J.)
Date: Fri, 26 Oct 2018 05:08:11 -0500 (CDT)
Subject: [squid-users] Squid proxy not working when upgrade from 27 to
	3.5
In-Reply-To: <8b3ece11-d04c-9856-67e4-bed0b274735c@treenet.co.nz>
References: <1540459174727-0.post@n4.nabble.com>
 <ae310bd0-f196-362a-8d0f-9410091b7d34@treenet.co.nz>
 <1540519560433-0.post@n4.nabble.com>
 <582f901ead8e9d3d0dd324e36bc39951@ngtech.co.il>
 <1540538814938-0.post@n4.nabble.com>
 <edf1c4c0-2c7e-ee05-b8b5-a2d1ba40ffb3@treenet.co.nz>
 <1540547103606-0.post@n4.nabble.com> <1540547186478-0.post@n4.nabble.com>
 <1540547386669-0.post@n4.nabble.com>
 <8b3ece11-d04c-9856-67e4-bed0b274735c@treenet.co.nz>
Message-ID: <1540548491798-0.post@n4.nabble.com>

The squid.conf has been revised, i just want to 8005 port for ERP application
server.



acl localnet src 10.0.0.0/8
acl localnet src 172.16.0.0/12
acl localnet src 192.168.0.0/16
acl localnet src fc00::/7
acl localnet src fe80::/10
acl SSL_ports port 443 8000 8004 8005
acl Safe_ports port 80
acl Safe_ports port 21
acl Safe_ports port 443
acl Safe_ports port 70
acl Safe_ports port 210
acl Safe_ports port 1025-65535
acl Safe_ports port 280
acl Safe_ports port 488
acl Safe_ports port 591
acl Safe_ports port 777
acl CONNECT method CONNECT

access_log /var/log/squid/access.log

#
# Recommended minimum Access Permission configuration:
#
# Deny requests to certain unsafe ports
# Deny CONNECT to other than secure SSL ports
# Only allow cachemgr access from localhost
# Example rule allowing access from your local networks.
# Adapt localnet in the ACL section to list your (internal) IP networks
# from where browsing should be allowed
# Allow localhost always proxy functionality
# And finally deny all other access to this proxy
http_access deny !Safe_ports
http_access deny CONNECT !SSL_ports
http_access allow localhost manager
http_access deny manager
http_access allow localnet
http_access allow localhost
http_access deny all

# We strongly recommend the following be uncommented to protect innocent
# web applications running on the proxy server who think the only
# one who can access services on "localhost" is a local user
#http_access deny to_localhost

#
# INSERT YOUR OWN RULE(S) HERE TO ALLOW ACCESS FROM YOUR CLIENTS
#

# Squid normally listens to port 3128
#http_port 3128

# Uncomment and adjust the following to add a disk cache directory.
#cache_dir aufs /var/cache/squid 100 16 256

# Leave coredumps in the first cache dir
coredump_dir /var/cache/squid

#
# Add any of your own refresh_pattern entries above these.
#
refresh_pattern ^ftp: 1440 20 10080
refresh_pattern ^gopher: 1440 0 1440
refresh_pattern -i  (/cgi-bin/|\?) 0 0 0
refresh_pattern . 0 20 4320

cache_dir ufs /var/cache/squid 100 16 256

cache_log /var/log/squid/cache.log

cache_mem 8 MB

cache_mgr webmaster

cache_replacement_policy lru

cache_store_log /var/log/squid/store.log

cache_swap_high 95

cache_swap_low 90

client_lifetime 1 days

connect_timeout 2 minutes

error_directory /usr/share/squid/errors/en

ftp_passive on

maximum_object_size 4096 KB

memory_replacement_policy lru

minimum_object_size 0 KB

visible_hostname oul163.ouhk.edu.hk
http_port 3128 accel vhost defaultsite=oul163.ouhk.edu.hk
sslproxy_options NO_SSLv3:NO_SSLv2


# Create an additional ACL for local network access
acl localip src 192.168.0.0/24

# access control list
acl hrmsacl dstdomain .ouhk.edu.hk
http_access allow hrmsacl
cache_peer 192.168.31.113 parent 8001 1 proxy-only name=prdhrms
acl prdhrms-domain dstdomain prdhrms.ouhk.edu.hk
cache_peer_access prdhrms allow prdhrms-domain

cache_peer 192.168.31.134 parent 8005 0  proxy-only name=uathrms
acl uathrms-domain dstdomain uathrms.ouhk.edu.hk
cache_peer_access uathrms allow uathrms-domain

cache_peer 192.168.31.134 parent 8004 2  proxy-only name=sithrms
acl sithrms-domain dstdomain sithrms.ouhk.edu.hk
cache_peer_access sithrms allow sithrms-domain

cache_peer 192.168.31.134 parent 8000 3  proxy-only name=devhrms
acl devhrms-domain dstdomain devhrms.ouhk.edu.hk
cache_peer_access devhrms allow devhrms-domain




--
Sent from: http://squid-web-proxy-cache.1019090.n4.nabble.com/Squid-Users-f1019091.html


From uhlar at fantomas.sk  Fri Oct 26 11:58:20 2018
From: uhlar at fantomas.sk (Matus UHLAR - fantomas)
Date: Fri, 26 Oct 2018 13:58:20 +0200
Subject: [squid-users] Squid proxy not working when upgrade from 27 to
	3.5
In-Reply-To: <1540548491798-0.post@n4.nabble.com>
References: <ae310bd0-f196-362a-8d0f-9410091b7d34@treenet.co.nz>
 <1540519560433-0.post@n4.nabble.com>
 <582f901ead8e9d3d0dd324e36bc39951@ngtech.co.il>
 <1540538814938-0.post@n4.nabble.com>
 <edf1c4c0-2c7e-ee05-b8b5-a2d1ba40ffb3@treenet.co.nz>
 <1540547103606-0.post@n4.nabble.com>
 <1540547186478-0.post@n4.nabble.com>
 <1540547386669-0.post@n4.nabble.com>
 <8b3ece11-d04c-9856-67e4-bed0b274735c@treenet.co.nz>
 <1540548491798-0.post@n4.nabble.com>
Message-ID: <20181026115820.npyeivrstmo35qky@fantomas.sk>

On 26.10.18 05:08, Angus J. wrote:
>The squid.conf has been revised, i just want to 8005 port for ERP application
>server.

- does it work noe?

- if not, what is in the access log when you try to browse a site?

>access_log /var/log/squid/access.log

in this one.

-- 
Matus UHLAR - fantomas, uhlar at fantomas.sk ; http://www.fantomas.sk/
Warning: I wish NOT to receive e-mail advertising to this address.
Varovanie: na tuto adresu chcem NEDOSTAVAT akukolvek reklamnu postu.
Enter any 12-digit prime number to continue.


From squid3 at treenet.co.nz  Sun Oct 28 16:09:02 2018
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Mon, 29 Oct 2018 05:09:02 +1300
Subject: [squid-users] [squid-announce] Squid-4.4 is available
Message-ID: <642ea755-c458-fbb3-5bef-1e392e022f41@treenet.co.nz>

The Squid HTTP Proxy team is very pleased to announce the availability
of the Squid-4.4 release!


This release is a security and bug fix release resolving several issues
found in the prior Squid releases.


The major changes to be aware of:

* SQUID-1018:4
  Cross-Site Scripting issue in TLS error processing

 http://www.squid-cache.org/Advisories/SQUID-2018_4.txt

This problem allows a malicious HTTPS server to trigger error
page delivery to a client and also inject arbitrary HTML code
into the resulting error response.

This problem is limited to Squid built with TLS / SSL support.


* SQUID-2018:5
  Denial of Service issue in SNMP processing.

 http://www.squid-cache.org/Advisories/SQUID-2018_5.txt

This problem allows a remote attacker to consume all memory
available to the Squid process, causing it to crash.

In environments where per-process memory restrictions are not
enforced strictly, or configured to large values this may also
affect other processes operating on the same machine. Leading to
a much worse denial of service situation.

This problem is limited to Squid built with SNMP support and
receiving SNMP traffic.


* Bug 4893: Malformed %>ru URIs for CONNECT requests

This bug showed up as "://host:port" URLs being logged for some CONNECT
transactions in Squid-4.2 and 4.3. This release reverts Squid to the
previous log output.


* Fix %USER_CA_CERT_xx and %USER_CERT_xx

Previous Squid-4 would crash when these macros where used to pass values
to external ACL helpers. This issue is now fully resolved.


* Support compilation with minimal OpenSSL

Squid would not build successfully against an OpenSSL library
which had itself been built to omit deprecated features and API.
This Squid release should build in these minimized environments.



  All users of Squid-4 are urged to upgrade as soon as possible.

  All users of Squid-3 are encouraged to upgrade where possible.


See the ChangeLog for the full list of changes in this and earlier
releases.

Please refer to the release notes at
http://www.squid-cache.org/Versions/v4/RELEASENOTES.html
when you are ready to make the switch to Squid-4

This new release can be downloaded from our HTTP or FTP servers

  http://www.squid-cache.org/Versions/v4/
  ftp://ftp.squid-cache.org/pub/squid/
  ftp://ftp.squid-cache.org/pub/archive/4/

or the mirrors. For a list of mirror sites see

  http://www.squid-cache.org/Download/http-mirrors.html
  http://www.squid-cache.org/Download/mirrors.html

If you encounter any issues with this release please file a bug report.
  http://bugs.squid-cache.org/


Amos Jeffries
_______________________________________________
squid-announce mailing list
squid-announce at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-announce

From squid3 at treenet.co.nz  Sun Oct 28 16:09:05 2018
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Mon, 29 Oct 2018 05:09:05 +1300
Subject: [squid-users] [squid-announce] [ADVISORY] SQUID-2018:5 Denial of
	Service issue in	SNMP processing
Message-ID: <911fa5ae-f7d3-43fb-b452-c09abf73192b@treenet.co.nz>

__________________________________________________________________

    Squid Proxy Cache Security Update Advisory SQUID-2018:5
__________________________________________________________________

Advisory ID:        SQUID-2018:5
Date:               October 28, 2018
Summary:            Denial of Service issue
                    in SNMP processing.
Affected versions:  Squid 3.2.0.10 -> 3.5.28
                    Squid 4.x -> 4.3
Fixed in version:   Squid 4.4
__________________________________________________________________

    http://www.squid-cache.org/Advisories/SQUID-2018_5.txt
__________________________________________________________________

Problem Description:

 Due to a memory leak in SNMP query rejection code, Squid is
 vulnerable to a denial of service attack.

__________________________________________________________________

Severity:

 This problem allows a remote attacker to consume all memory
 available to the Squid process, causing it to crash.

 In environments where per-process memory restrictions are not
 enforced strictly, or configured to large values this may also
 affect other processes operating on the same machine. Leading to
 a much worse denial of service situation.

 This problem is limited to Squid built with SNMP support and
 receiving SNMP traffic.

__________________________________________________________________

Updated Packages:

 This bug is fixed by Squid version 4.4.

 In addition, patches addressing this problem for the stable
 releases can be found in our patch archives:

Squid 3.5:
 http://www.squid-cache.org/Versions/v3/3.5/changesets/squid-3.5-bc9786119f058a76ddf0625424bc33d36460b9a2.patch

Squid 4:
 http://www.squid-cache.org/Versions/v4/changesets/squid-4-983c5c36e5f109512ed1af38a329d0b5d0967498.patch

 If you are using a prepackaged version of Squid then please refer
 to the package vendor for availability information on updated
 packages.

__________________________________________________________________

Determining if your version is vulnerable:

 All Squid built with --disable-snmp are not vulnerable.

 All Squid-2.x and older versions are not vulnerable.

 All Squid-3.x up to and including 3.2.0.9 are not vulnerable.

 All Squid-3.x up to and including 3.5.28 configured with
 "snmp_port 0" are not vulnerable.

 All Squid-3.x up to and including 3.5.28 without snmp_port
 configured are not vulnerable.

 All Squid-3.2.0.10 and later 3.2.x versions with snmp_port
 configured to a non-0 value are vulnerable.

 All Squid-3.3 versions up to and including 3.3.14 with snmp_port
 configured to a non-0 value are vulnerable.

 All Squid-3.4 versions up to and including 3.4.14 with snmp_port
 configured to a non-0 value are vulnerable.

 All Squid-3.5 versions up to and including 3.5.28 with snmp_port
 configured to a non-0 value are vulnerable.

 All Squid-4.x versions up to and including 4.3 with snmp_port
 configured to a non-0 value are vulnerable.


To determine the version of your Squid and its build options use
the command:

 squid -v

To determine whether snmp_port is configured use the command:

 squid -k parse | grep snmp_port

__________________________________________________________________

Workarounds:

Either;

 Enable firewall inspection of SNMP packets to enforce blocking
 of any non-permitted packets prior to their arriving at Squid.

 This restriction of packet sources reduces the risk, but does not
 completely remove the vulnerability.

Or,

 Remove snmp_port and related configuration settings until Squid
 can be upgraded to a fixed build.

 This completely removes the vulnerability at cost of reduced
 management and monitoring capabilities.

__________________________________________________________________

Contact details for the Squid project:

 For installation / upgrade support on binary packaged versions
 of Squid: Your first point of contact should be your binary
 package vendor.

 If your install and build Squid from the original Squid sources
 then the squid-users at lists.squid-cache.org mailing list is your
 primary support point. For subscription details see
 <http://www.squid-cache.org/Support/mailing-lists.html>.

 For reporting of non-security bugs in the latest STABLE release
 the squid bugzilla database should be used
 <http://bugs.squid-cache.org/>.

 For reporting of security sensitive bugs send an email to the
 squid-bugs at lists.squid-cache.org mailing list. It's a closed
 list (though anyone can post) and security related bug reports
 are treated in confidence until the impact has been established.

__________________________________________________________________

Credits:

 This vulnerability was discovered and fixed by Florian Kohnh?user

__________________________________________________________________

Revision history:

 2018-10-23 06:15:46 UTC Initial Report
 2018-10-23 21:42:58 UTC Patch Released
 2018-10-27 21:19:00 UTC Packages Released
__________________________________________________________________
END
_______________________________________________
squid-announce mailing list
squid-announce at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-announce

From squid3 at treenet.co.nz  Sun Oct 28 16:09:09 2018
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Mon, 29 Oct 2018 05:09:09 +1300
Subject: [squid-users] [squid-announce] [ADVISORY] SQUID-2018:4 Cross-Site
 Scripting issue in TLS error processing
Message-ID: <cba3e2f4-bbc8-e639-3775-06b8277afe66@treenet.co.nz>

__________________________________________________________________

    Squid Proxy Cache Security Update Advisory SQUID-2018:4
__________________________________________________________________

Advisory ID:        SQUID-2018:4
Date:               October 28, 2018
Summary:            Cross-Site Scripting issue
                    in TLS error processing.
Affected versions:  Squid 3.1.12.1 -> 3.1.23
                    Squid 3.2.0.4 -> 3.5.28
                    Squid 4.0 -> 4.3
Fixed in version:   Squid 4.4
__________________________________________________________________

    http://www.squid-cache.org/Advisories/SQUID-2018_4.txt
__________________________________________________________________

Problem Description:

 Due to incorrect input handling, Squid is vulnerable to a
 Cross-Site Scripting vulnerability when generating HTTPS response
 messages about TLS errors.

__________________________________________________________________

Severity:

 This problem allows a malicious HTTPS server to trigger error
 page delivery to a client and also inject arbitrary HTML code
 into the resulting error response.

 This problem is limited to Squid built with TLS / SSL support.

__________________________________________________________________

Updated Packages:

 This bug is fixed by Squid version 4.4.

 In addition, patches addressing this problem for the stable
 releases can be found in our patch archives:

Squid 3.5:
 http://www.squid-cache.org/Versions/v3/3.5/changesets/squid-3.5-f1657a9decc820f748fa3aff68168d3145258031.patch

Squid 4:
 http://www.squid-cache.org/Versions/v4/changesets/squid-4-828245b90206602014ce057c3db39fb80fcc4b08.patch

 If you are using a prepackaged version of Squid then please refer
 to the package vendor for availability information on updated
 packages.

__________________________________________________________________

Determining if your version is vulnerable:

 All Squid-2.x and older are not vulnerable.

 All Squid-3.0 and older version are not vulnerable.

 All Squid-3.x versions up to and including 3.4.14 built with
 --disable-ssl are not vulnerable.

 All Squid-3.x versions up to and including 3.4.14 built without
 --enable-ssl are not vulnerable.

 All Squid-3.1.12.1 and later versions up to and including
 Squid-3.1.23 built with --enable-ssl are vulnerable.

 All Squid-3.2.0.4 and later versions up to and including
 Squid-3.2.14 built with --enable-ssl are vulnerable.

 All Squid-3.3 and later versions up to and including
 Squid-3.3.14 built with --enable-ssl are vulnerable.

 All Squid-3.4 and later versions up to and including
 Squid-3.4.14 built with --enable-ssl are vulnerable.

 All Squid-3.5 versions up to and including 3.5.28 built without
 --with-openssl are not vulnerable.

 All Squid-3.5 and later versions up to and including 3.5.28 built
 with --with-openssl are vulnerable.

 All Squid-4 versions built without --with-openssl are not
 vulnerable.

 All Squid-4 versions up to and including 4.3 built with
 --with-openssl are vulnerable.

__________________________________________________________________

Workarounds:

Either;

 Remove %D error page macro from ERR_SECURE_CONNECT_FAIL and any
 custom error pages.

Or,

 Build Squid-3.1 to 3.4.14 without "--enable-ssl"

Or,

 Build Squid-3.5 or later without "--with-openssl"

__________________________________________________________________

Contact details for the Squid project:

 For installation / upgrade support on binary packaged versions
 of Squid: Your first point of contact should be your binary
 package vendor.

 If your install and build Squid from the original Squid sources
 then the squid-users at lists.squid-cache.org mailing list is your
 primary support point. For subscription details see
 <http://www.squid-cache.org/Support/mailing-lists.html>.

 For reporting of non-security bugs in the latest STABLE release
 the squid bugzilla database should be used
 <http://bugs.squid-cache.org/>.

 For reporting of security sensitive bugs send an email to the
 squid-bugs at lists.squid-cache.org mailing list. It's a closed
 list (though anyone can post) and security related bug reports
 are treated in confidence until the impact has been established.

__________________________________________________________________

Credits:

 This vulnerability was discovered by Nikolas Lohmann of eBlocker
 GmbH.

 Fixed by Christos Tsantilas on behalf of Measurement Factory.

__________________________________________________________________

Revision history:

 2018-10-15 10:59:16 UTC Initial Report
 2018-10-17 15:13:41 UTC Patches Released
 2018-10-27 21:19:00 UTC Packages Released
__________________________________________________________________
END
_______________________________________________
squid-announce mailing list
squid-announce at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-announce

From jason_haar at trimble.com  Sun Oct 28 20:20:24 2018
From: jason_haar at trimble.com (Jason Haar)
Date: Mon, 29 Oct 2018 09:20:24 +1300
Subject: [squid-users] XSS issue only affects bump doesn't it?
Message-ID: <CAFChrgLE2Dd_3uS6YWM5bx2SDSQwYwQ+nnXVOFJu+PGVJduKow@mail.gmail.com>

Hi there

I'm running a vulnerable version of squid (ie "--with-openssl" and
"--enable-ssl") but due to issues with bumping not working well, don't
actually do it (ie sslcrtd_program and ssl_bump not defined/etc).

So does that mean we can't actually be affected by this vulnerability?

-- 
Cheers

Jason Haar
Information Security Manager, Trimble Navigation Ltd.
Phone: +1 408 481 8171
PGP Fingerprint: 7A2E 0407 C9A6 CAF6 2B9F 8422 C063 5EBB FE1D 66D1
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20181029/24f56a4b/attachment.htm>

From manuelfgarcia.nabble at xoxy.net  Sun Oct 28 22:13:15 2018
From: manuelfgarcia.nabble at xoxy.net (manuelfgarcia)
Date: Sun, 28 Oct 2018 17:13:15 -0500 (CDT)
Subject: [squid-users] Hyper-threading, SSD vs HDD,
 CentOS vs Ubuntu Server and best performer Squid version
Message-ID: <1540764795289-0.post@n4.nabble.com>

According to https://wiki.squid-cache.org/BestOsForSquid :
"CPU speed and core count:
few faster cores are better than many slow cores. SMP Squid can currently
operate most efficiently with 4-8 cores of 3GHz or more. multi-tenant
installations are better for machinery with very many cores.
only the physical cores are useful, hyper-threaded "cores" can actually be
worse.

Is that statement still up to date or is is it no longer the case from any
specific version of Squid?

Considering the statement that "hyper-threaded "cores" can actually be
worse.", what is supposed to handle faster and concurrent connections on
reverse proxy mode?:
- Intel Xeon X3430 (2.40GHz, 2,80 GHz turbo, 4 physical cores, 4 threads)
vs.
- Intel Xeon E3-1230-V6 3.40GHz (3.50 GHz, 3.90 GHz, 4 physical cores, 8
threads)

On another note, what is supposed to handle faster and more concurrent
connections on reverse proxy mode between these options?:

- SSD vs HDD

- CentOS 7 vs CentOS 6 vs Ubuntu Server 16.04 vs Ubuntu Server 18.04

- Squid 3.1 vs 3.5 vs 4.x...



--
Sent from: http://squid-web-proxy-cache.1019090.n4.nabble.com/Squid-Users-f1019091.html


From squid3 at treenet.co.nz  Mon Oct 29 00:39:51 2018
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Mon, 29 Oct 2018 13:39:51 +1300
Subject: [squid-users] XSS issue only affects bump doesn't it?
In-Reply-To: <CAFChrgLE2Dd_3uS6YWM5bx2SDSQwYwQ+nnXVOFJu+PGVJduKow@mail.gmail.com>
References: <CAFChrgLE2Dd_3uS6YWM5bx2SDSQwYwQ+nnXVOFJu+PGVJduKow@mail.gmail.com>
Message-ID: <85db6536-842e-78b3-95b5-4bd841bb19a7@treenet.co.nz>

On 29/10/18 9:20 AM, Jason Haar wrote:
> Hi there
> 
> I'm running a vulnerable version of squid (ie "--with-openssl" and
> "--enable-ssl") but due to issues with bumping not working well, don't
> actually do it (ie sslcrtd_program and ssl_bump?not defined/etc).
> 
> So does that mean we can't actually be affected by this vulnerability?

The problem is in the error page generated. So while it is most visible
with bump'ed traffic it also can occur whenever Squid is the agent
performing the TLS handshake with a server.

Amos


From squid3 at treenet.co.nz  Mon Oct 29 01:29:32 2018
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Mon, 29 Oct 2018 14:29:32 +1300
Subject: [squid-users] Hyper-threading, SSD vs HDD,
 CentOS vs Ubuntu Server and best performer Squid version
In-Reply-To: <1540764795289-0.post@n4.nabble.com>
References: <1540764795289-0.post@n4.nabble.com>
Message-ID: <de07d22d-2c66-08c1-f529-a9a7e7c3e861@treenet.co.nz>

On 29/10/18 11:13 AM, manuelfgarcia wrote:
> According to https://wiki.squid-cache.org/BestOsForSquid :
> "CPU speed and core count:
> few faster cores are better than many slow cores. SMP Squid can currently
> operate most efficiently with 4-8 cores of 3GHz or more. multi-tenant
> installations are better for machinery with very many cores.
> only the physical cores are useful, hyper-threaded "cores" can actually be
> worse.
> 
> Is that statement still up to date or is is it no longer the case from any
> specific version of Squid?
> 

It is still true and correct.


> Considering the statement that "hyper-threaded "cores" can actually be
> worse.", what is supposed to handle faster and concurrent connections on
> reverse proxy mode?

Reverse-proxy mode gains its performance benefits through only having to
service a limited range of URLs. Which means more predictability and
objects do not get evicted from memory cache quite as often as from
other proxies. Memory is faster than disk or network. Nothing to do with
CPU(s).


> - Intel Xeon X3430 (2.40GHz, 2,80 GHz turbo, 4 physical cores, 4 threads)
> vs.
> - Intel Xeon E3-1230-V6 3.40GHz (3.50 GHz, 3.90 GHz, 4 physical cores, 8
> threads)
> 
> On another note, what is supposed to handle faster and more concurrent
> connections on reverse proxy mode between these options?:
> 
> - SSD vs HDD
> 

These are disk drives. Not networks. Disks have nothing to do with
"concurrent connections". Squid can handle as many concurrent
connections as your servers TCP stack allows it to. No more, no less.


Squid performs random-access I/O to on-disk caches. With an order of
magnitude or so higher than normal write accesses. These non-normal I/O
patterns prevent typical disk controllers I/O prediction models from
working efficiently.

So be aware that whichever disk type you choose will have its lifetime
reduced to some fraction of what any manufacturer specs' it as being
capable of. How much depend on your specific traffic.

SSD are still faster hardware than HDD. But also typically wear out
faster and have smaller content capacity. So it is a matter of whether
you can afford the budget to replace SSD or HDD as they wear out. And
you can easily change between them at any time if you want to experiment.


> - CentOS 7 vs CentOS 6 vs Ubuntu Server 16.04 vs Ubuntu Server 18.04
> 
> - Squid 3.1 vs 3.5 vs 4.x...
> 

Squid-3.x are officially deprecated by Squid-4 being published. Similar
deprecation situations apply for the OS you mention.

A good rule of thumb for any new installation is to start with the
latest tech you are able. It will have the longest support / warranty
period as measured from the day you decide to put it into service. If
you start with tech that is old, you are cutting your support timespan
by that much.

Another (possibly conflicting) rule of thumb, is to reduce the amount of
change you are going to have to learn. If you are already familiar with
a particular OS go with that while you learn how to operate the Squid
part of the system. Squid operates approximately as well in any OS -
excluding Windows, which sadly imposes a few severe network limitations.


HTH
Amos


From ajiang at ouhk.edu.hk  Mon Oct 29 01:42:06 2018
From: ajiang at ouhk.edu.hk (Angus J.)
Date: Sun, 28 Oct 2018 20:42:06 -0500 (CDT)
Subject: [squid-users] Squid proxy not working when upgrade from 27 to
	3.5
In-Reply-To: <1540547386669-0.post@n4.nabble.com>
References: <20181024072310.4pmv3th2tptgj2ve@fantomas.sk>
 <1540459174727-0.post@n4.nabble.com>
 <ae310bd0-f196-362a-8d0f-9410091b7d34@treenet.co.nz>
 <1540519560433-0.post@n4.nabble.com>
 <582f901ead8e9d3d0dd324e36bc39951@ngtech.co.il>
 <1540538814938-0.post@n4.nabble.com>
 <edf1c4c0-2c7e-ee05-b8b5-a2d1ba40ffb3@treenet.co.nz>
 <1540547103606-0.post@n4.nabble.com> <1540547186478-0.post@n4.nabble.com>
 <1540547386669-0.post@n4.nabble.com>
Message-ID: <1540777326629-0.post@n4.nabble.com>

Hi

oul163:/var/log/squid # ls -rlt
total 84
-rw-r----- 1 squid squid     0 Oct 22 12:21 access.log
-rw-r----- 1 squid squid     0 Oct 22 13:02 netdb.state
-rw-r----- 1 squid squid 17784 Oct 29 09:13 store.log
-rw-r----- 1 squid squid 55296 Oct 29 09:33 cache.log
oul163:/var/log/squid #


access.log is empty



--
Sent from: http://squid-web-proxy-cache.1019090.n4.nabble.com/Squid-Users-f1019091.html


From ajiang at ouhk.edu.hk  Mon Oct 29 01:46:23 2018
From: ajiang at ouhk.edu.hk (Angus J.)
Date: Sun, 28 Oct 2018 20:46:23 -0500 (CDT)
Subject: [squid-users] Squid proxy not working when upgrade from 27 to
	3.5
In-Reply-To: <20181026115820.npyeivrstmo35qky@fantomas.sk>
References: <1540519560433-0.post@n4.nabble.com>
 <582f901ead8e9d3d0dd324e36bc39951@ngtech.co.il>
 <1540538814938-0.post@n4.nabble.com>
 <edf1c4c0-2c7e-ee05-b8b5-a2d1ba40ffb3@treenet.co.nz>
 <1540547103606-0.post@n4.nabble.com> <1540547186478-0.post@n4.nabble.com>
 <1540547386669-0.post@n4.nabble.com>
 <8b3ece11-d04c-9856-67e4-bed0b274735c@treenet.co.nz>
 <1540548491798-0.post@n4.nabble.com>
 <20181026115820.npyeivrstmo35qky@fantomas.sk>
Message-ID: <1540777583026-0.post@n4.nabble.com>

Hi Matus

The squid 3.5 cannot read the configure file of SUSE Linux 12 SP3

Regards
Angus



--
Sent from: http://squid-web-proxy-cache.1019090.n4.nabble.com/Squid-Users-f1019091.html


From ajiang at ouhk.edu.hk  Mon Oct 29 02:02:41 2018
From: ajiang at ouhk.edu.hk (Angus J.)
Date: Sun, 28 Oct 2018 21:02:41 -0500 (CDT)
Subject: [squid-users] Squid proxy not working when upgrade from 27 to
	3.5
In-Reply-To: <1540777583026-0.post@n4.nabble.com>
References: <582f901ead8e9d3d0dd324e36bc39951@ngtech.co.il>
 <1540538814938-0.post@n4.nabble.com>
 <edf1c4c0-2c7e-ee05-b8b5-a2d1ba40ffb3@treenet.co.nz>
 <1540547103606-0.post@n4.nabble.com> <1540547186478-0.post@n4.nabble.com>
 <1540547386669-0.post@n4.nabble.com>
 <8b3ece11-d04c-9856-67e4-bed0b274735c@treenet.co.nz>
 <1540548491798-0.post@n4.nabble.com>
 <20181026115820.npyeivrstmo35qky@fantomas.sk>
 <1540777583026-0.post@n4.nabble.com>
Message-ID: <1540778561463-0.post@n4.nabble.com>

oul163:/etc/squid # squid -k parse
2018/10/29 10:01:46| Startup: Initializing Authentication Schemes ...
2018/10/29 10:01:46| Startup: Initialized Authentication Scheme 'basic'
2018/10/29 10:01:46| Startup: Initialized Authentication Scheme 'digest'
2018/10/29 10:01:46| Startup: Initialized Authentication Scheme 'negotiate'
2018/10/29 10:01:46| Startup: Initialized Authentication Scheme 'ntlm'
2018/10/29 10:01:46| Startup: Initialized Authentication.
2018/10/29 10:01:46| Processing Configuration File: /etc/squid/squid.conf
(depth 0)
2018/10/29 10:01:46| Processing: acl localnet src 10.0.0.0/8
2018/10/29 10:01:46| Processing: acl localnet src 172.16.0.0/12
2018/10/29 10:01:46| Processing: acl localnet src 192.168.0.0/16
2018/10/29 10:01:46| Processing: acl localnet src fc00::/7
2018/10/29 10:01:46| Processing: acl localnet src fe80::/10
2018/10/29 10:01:46| Processing: acl SSL_ports port 443 8000 8004 8005
2018/10/29 10:01:46| Processing: acl Safe_ports port 80
2018/10/29 10:01:46| Processing: acl Safe_ports port 21
2018/10/29 10:01:46| Processing: acl Safe_ports port 443
2018/10/29 10:01:46| Processing: acl Safe_ports port 70
2018/10/29 10:01:46| Processing: acl Safe_ports port 210
2018/10/29 10:01:46| Processing: acl Safe_ports port 1025-65535
2018/10/29 10:01:46| Processing: acl Safe_ports port 280
2018/10/29 10:01:46| Processing: acl Safe_ports port 488
2018/10/29 10:01:46| Processing: acl Safe_ports port 591
2018/10/29 10:01:46| Processing: acl Safe_ports port 777
2018/10/29 10:01:46| Processing: acl CONNECT method CONNECT
2018/10/29 10:01:46| Processing: access_log /var/log/squid/access.log
2018/10/29 10:01:46| Processing: http_access deny !Safe_ports
2018/10/29 10:01:46| Processing: http_access deny CONNECT !SSL_ports
2018/10/29 10:01:46| Processing: http_access allow localhost manager
2018/10/29 10:01:46| Processing: http_access deny manager
2018/10/29 10:01:46| Processing: http_access allow localnet
2018/10/29 10:01:46| Processing: http_access allow localhost
2018/10/29 10:01:46| Processing: http_access deny all
2018/10/29 10:01:46| Processing: coredump_dir /var/cache/squid
2018/10/29 10:01:46| Processing: refresh_pattern ^ftp: 1440 20 10080
2018/10/29 10:01:46| Processing: refresh_pattern ^gopher: 1440 0 1440
2018/10/29 10:01:46| Processing: refresh_pattern -i  (/cgi-bin/|\?) 0 0 0
2018/10/29 10:01:46| Processing: refresh_pattern . 0 20 4320
2018/10/29 10:01:46| Processing: cache_dir ufs /var/cache/squid 100 16 256
2018/10/29 10:01:46| Processing: cache_log /var/log/squid/cache.log
2018/10/29 10:01:46| Processing: cache_mem 8 MB
2018/10/29 10:01:46| Processing: cache_mgr webmaster
2018/10/29 10:01:46| Processing: cache_replacement_policy lru
2018/10/29 10:01:46| Processing: cache_store_log /var/log/squid/store.log
2018/10/29 10:01:46| Processing: cache_swap_high 95
2018/10/29 10:01:46| Processing: cache_swap_low 90
2018/10/29 10:01:46| Processing: client_lifetime 1 days
2018/10/29 10:01:46| Processing: connect_timeout 2 minutes
2018/10/29 10:01:46| Processing: error_directory /usr/share/squid/errors/en
2018/10/29 10:01:46| Processing: ftp_passive on
2018/10/29 10:01:46| Processing: maximum_object_size 4096 KB
2018/10/29 10:01:46| Processing: memory_replacement_policy lru
2018/10/29 10:01:46| Processing: minimum_object_size 0 KB
2018/10/29 10:01:46| Processing: visible_hostname oul163.ouhk.edu.hk
2018/10/29 10:01:46| Processing: http_port 3128 accel vhost
defaultsite=oul163.ouhk.edu.hk
2018/10/29 10:01:46| Processing: sslproxy_options NO_SSLv3:NO_SSLv2
2018/10/29 10:01:46| Processing: acl localip src 192.168.0.0/24
2018/10/29 10:01:46| Processing: acl hrmsacl dstdomain .ouhk.edu.hk
2018/10/29 10:01:46| Processing: http_access allow hrmsacl
2018/10/29 10:01:46| Processing: cache_peer 192.168.31.113 parent 8001 1
proxy-only name=prdhrms
2018/10/29 10:01:46| Processing: acl prdhrms-domain dstdomain
prdhrms.ouhk.edu.hk
2018/10/29 10:01:46| Processing: cache_peer_access prdhrms allow
prdhrms-domain
2018/10/29 10:01:46| Processing: cache_peer 192.168.31.134 parent 8005 0 
proxy-only name=uathrms
2018/10/29 10:01:46| Processing: acl uathrms-domain dstdomain
uathrms.ouhk.edu.hk
2018/10/29 10:01:46| Processing: cache_peer_access uathrms allow
uathrms-domain
2018/10/29 10:01:46| Processing: cache_peer 192.168.31.134 parent 8004 2 
proxy-only name=sithrms
2018/10/29 10:01:46| Processing: acl sithrms-domain dstdomain
sithrms.ouhk.edu.hk
2018/10/29 10:01:46| Processing: cache_peer_access sithrms allow
sithrms-domain
2018/10/29 10:01:46| Processing: cache_peer 192.168.31.134 parent 8000 3 
proxy-only name=devhrms
2018/10/29 10:01:46| Processing: acl devhrms-domain dstdomain
devhrms.ouhk.edu.hk
2018/10/29 10:01:46| Processing: cache_peer_access devhrms allow
devhrms-domain
2018/10/29 10:01:46| Initializing https proxy context



--
Sent from: http://squid-web-proxy-cache.1019090.n4.nabble.com/Squid-Users-f1019091.html


From ajiang at ouhk.edu.hk  Mon Oct 29 02:05:48 2018
From: ajiang at ouhk.edu.hk (Angus J.)
Date: Sun, 28 Oct 2018 21:05:48 -0500 (CDT)
Subject: [squid-users] Squid proxy not working when upgrade from 27 to
	3.5
In-Reply-To: <1540777583026-0.post@n4.nabble.com>
References: <582f901ead8e9d3d0dd324e36bc39951@ngtech.co.il>
 <1540538814938-0.post@n4.nabble.com>
 <edf1c4c0-2c7e-ee05-b8b5-a2d1ba40ffb3@treenet.co.nz>
 <1540547103606-0.post@n4.nabble.com> <1540547186478-0.post@n4.nabble.com>
 <1540547386669-0.post@n4.nabble.com>
 <8b3ece11-d04c-9856-67e4-bed0b274735c@treenet.co.nz>
 <1540548491798-0.post@n4.nabble.com>
 <20181026115820.npyeivrstmo35qky@fantomas.sk>
 <1540777583026-0.post@n4.nabble.com>
Message-ID: <1540778748296-0.post@n4.nabble.com>

#
# Recommended minimum configuration:
#

# Example rule allowing access from your local networks.
# Adapt to list your (internal) IP networks from where browsing
# should be allowed
# RFC1918 possible internal network
# RFC1918 possible internal network
# RFC1918 possible internal network
# RFC 4193 local private network range
# RFC 4291 link-local (directly plugged) machines
# http
# ftp
# https
# gopher
# wais
# unregistered ports
# http-mgmt
# gss-http
# filemaker
# multiling http
acl localnet src 10.0.0.0/8
acl localnet src 172.16.0.0/12
acl localnet src 192.168.0.0/16
acl localnet src fc00::/7
acl localnet src fe80::/10
acl SSL_ports port 443 8000 8004 8005
acl Safe_ports port 80
acl Safe_ports port 21
acl Safe_ports port 443
acl Safe_ports port 70
acl Safe_ports port 210
acl Safe_ports port 1025-65535
acl Safe_ports port 280
acl Safe_ports port 488
acl Safe_ports port 591
acl Safe_ports port 777
acl CONNECT method CONNECT

access_log /var/log/squid/access.log

#
# Recommended minimum Access Permission configuration:
#
# Deny requests to certain unsafe ports
# Deny CONNECT to other than secure SSL ports
# Only allow cachemgr access from localhost
# Example rule allowing access from your local networks.
# Adapt localnet in the ACL section to list your (internal) IP networks
# from where browsing should be allowed
# Allow localhost always proxy functionality
# And finally deny all other access to this proxy
http_access deny !Safe_ports
http_access deny CONNECT !SSL_ports
http_access allow localhost manager
http_access deny manager
http_access allow localnet
http_access allow localhost
http_access deny all

# We strongly recommend the following be uncommented to protect innocent
# web applications running on the proxy server who think the only
# one who can access services on "localhost" is a local user
#http_access deny to_localhost

#
# INSERT YOUR OWN RULE(S) HERE TO ALLOW ACCESS FROM YOUR CLIENTS
#

# Squid normally listens to port 3128
#http_port 3128

# Uncomment and adjust the following to add a disk cache directory.
#cache_dir aufs /var/cache/squid 100 16 256

# Leave coredumps in the first cache dir
coredump_dir /var/cache/squid

#
# Add any of your own refresh_pattern entries above these.
#
refresh_pattern ^ftp: 1440 20 10080
refresh_pattern ^gopher: 1440 0 1440
refresh_pattern -i  (/cgi-bin/|\?) 0 0 0
refresh_pattern . 0 20 4320

cache_dir ufs /var/cache/squid 100 16 256

cache_log /var/log/squid/cache.log

cache_mem 8 MB

cache_mgr webmaster

cache_replacement_policy lru

cache_store_log /var/log/squid/store.log

cache_swap_high 95

cache_swap_low 90

client_lifetime 1 days

connect_timeout 2 minutes

error_directory /usr/share/squid/errors/en

ftp_passive on

maximum_object_size 4096 KB

memory_replacement_policy lru

minimum_object_size 0 KB

visible_hostname oul163.ouhk.edu.hk
http_port 3128 accel vhost defaultsite=oul163.ouhk.edu.hk
#https_port 80 accel cert=/etc/squid/certs/ouhk.crt
key=/etc/squid/certs/ouhk.key defaultsite=oul163.ouhk.edu.hk vhost
protocol=https options=NO_SSLv3:NO_SSLv2
#https_port 8000 accel cert=/etc/squid/certs/ouhk.crt
key=/etc/squid/certs/ouhk.key defaultsite=oul163.ouhk.edu.hk vhost
protocol=https options=NO_SSLv3:NO_SSLv2
#https_port 8004 accel cert=/etc/squid/certs/ouhk.crt
key=/etc/squid/certs/ouhk.key defaultsite=oul163.ouhk.edu.hk vhost
protocol=https options=NO_SSLv3:NO_SSLv2
#https_port 8004 accel cert=/etc/squid/certs/ouhk2.crt
key=/etc/squid/certs/ouhk2.key defaultsite=oul163.ouhk.edu.hk vhost
protocol=https options=NO_SSLv3:NO_SSLv2
#https_port 8005 accel cert=/etc/squid/certs/ouhk.crt
key=/etc/squid/certs/ouhk.key defaultsite=oul163.ouhk.edu.hk vhost
protocol=https options=NO_SSLv3:NO_SSLv2
#https_port 8005 accel cert=/etc/squid/certs/ouhk3.crt
key=/etc/squid/certs/ouhk3.key defaultsite=oul163.ouhk.edu.hk vhost
protocol=https options=NO_SSLv3:NO_SSLv2
#ssl_bump allow all
#              Disable the following one
#ssl_bump options=NO_SSLv3
#always_direct allow all
#              Disable the following one
#sslproxy_cert_error allow all
sslproxy_options NO_SSLv3:NO_SSLv2

# the proxy-only indicates that caching will not be performed.
#cache_peer 192.168.31.113 parent 8001 0 proxy-only name=prdhrms
#cache_peer_domain prdhrms prdhrms.ouhk.edu.hk
#cache_peer 192.168.31.134 parent 8005 0 ssl sslflags=DONT_VERIFY_PEER
proxy-only name=uathrms ssloptions=NO_SSLv3:NO_SSLv2
#cache_peer 192.168.31.134 parent 8005 0 ssl sslflags=DONT_VERIFY_DOMAIN
proxy-only name=uathrms ssloptions=NO_SSLv3:NO_SSLv2
#cache_peer_domain uathrms uathrms.ouhk.edu.hk
#cache_peer 192.168.31.134 parent 8004 0 ssl sslflags=DONT_VERIFY_PEER
proxy-only name=sithrms ssloptions=NO_SSLv3:NO_SSLv2
#cache_peer_domain sithrms sithrms.ouhk.edu.hk
#cache_peer 192.168.31.134 parent 8000 0 ssl sslflags=DONT_VERIFY_PEER
proxy-only name=devhrms ssloptions=NO_SSLv3:NO_SSLv2
#cache_peer 192.168.31.134 parent 8000 0 proxy-only originserver
name=devhrms ssll sslcafile=/certs/star_ouhk_edu_hk.crt
#cache_peer_domain devhrms devhrms.ouhk.edu.hk

# Create an additional ACL for local network access
acl localip src 192.168.0.0/24

# access control list
acl hrmsacl dstdomain .ouhk.edu.hk
http_access allow hrmsacl
#acl hrmsacl2 dstdomain devhrms.ouhk.edu.hk
#cache_peer_access devhrms allow hrmsacl2
#cache_peer_access prdhrms allow hrmsacl
#cache_peer_access uathrms allow hrmsacl
#cache_peer_access sithrms allow hrmsacl
#cache_peer_access devhrms allow hrmsacl
#cache_peer_access secure allow SSL_ports
cache_peer 192.168.31.113 parent 8001 1 proxy-only name=prdhrms
acl prdhrms-domain dstdomain prdhrms.ouhk.edu.hk
cache_peer_access prdhrms allow prdhrms-domain

cache_peer 192.168.31.134 parent 8005 0  proxy-only name=uathrms
acl uathrms-domain dstdomain uathrms.ouhk.edu.hk
cache_peer_access uathrms allow uathrms-domain

cache_peer 192.168.31.134 parent 8004 2  proxy-only name=sithrms
acl sithrms-domain dstdomain sithrms.ouhk.edu.hk
cache_peer_access sithrms allow sithrms-domain

cache_peer 192.168.31.134 parent 8000 3  proxy-only name=devhrms
acl devhrms-domain dstdomain devhrms.ouhk.edu.hk
cache_peer_access devhrms allow devhrms-domain




--
Sent from: http://squid-web-proxy-cache.1019090.n4.nabble.com/Squid-Users-f1019091.html


From squid3 at treenet.co.nz  Mon Oct 29 02:19:18 2018
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Mon, 29 Oct 2018 15:19:18 +1300
Subject: [squid-users] Squid proxy not working when upgrade from 27 to
	3.5
In-Reply-To: <1540778561463-0.post@n4.nabble.com>
References: <582f901ead8e9d3d0dd324e36bc39951@ngtech.co.il>
 <1540538814938-0.post@n4.nabble.com>
 <edf1c4c0-2c7e-ee05-b8b5-a2d1ba40ffb3@treenet.co.nz>
 <1540547103606-0.post@n4.nabble.com> <1540547186478-0.post@n4.nabble.com>
 <1540547386669-0.post@n4.nabble.com>
 <8b3ece11-d04c-9856-67e4-bed0b274735c@treenet.co.nz>
 <1540548491798-0.post@n4.nabble.com>
 <20181026115820.npyeivrstmo35qky@fantomas.sk>
 <1540777583026-0.post@n4.nabble.com> <1540778561463-0.post@n4.nabble.com>
Message-ID: <1b84fa1a-9f0b-e159-b2c9-468bc313e6ff@treenet.co.nz>

That shows a clean parse of the config.

Though you still have the cache_peer configuration not doing any TLS/SSL
and the "deny all" access permission above your custom access controls.

Amos


From ajiang at ouhk.edu.hk  Mon Oct 29 02:26:55 2018
From: ajiang at ouhk.edu.hk (Angus J.)
Date: Sun, 28 Oct 2018 21:26:55 -0500 (CDT)
Subject: [squid-users] Squid proxy not working when upgrade from 27 to
	3.5
In-Reply-To: <1b84fa1a-9f0b-e159-b2c9-468bc313e6ff@treenet.co.nz>
References: <edf1c4c0-2c7e-ee05-b8b5-a2d1ba40ffb3@treenet.co.nz>
 <1540547103606-0.post@n4.nabble.com> <1540547186478-0.post@n4.nabble.com>
 <1540547386669-0.post@n4.nabble.com>
 <8b3ece11-d04c-9856-67e4-bed0b274735c@treenet.co.nz>
 <1540548491798-0.post@n4.nabble.com>
 <20181026115820.npyeivrstmo35qky@fantomas.sk>
 <1540777583026-0.post@n4.nabble.com> <1540778561463-0.post@n4.nabble.com>
 <1b84fa1a-9f0b-e159-b2c9-468bc313e6ff@treenet.co.nz>
Message-ID: <1540780015772-0.post@n4.nabble.com>

Hi AMos

Thanks for your information

How to enable cache_peer configuration for any TLS/SSL for port 8005

and the "allow all" 192.168.0.0 and 172.18.0.0  access permission of custom
access controls? 

Regards
Angus



--
Sent from: http://squid-web-proxy-cache.1019090.n4.nabble.com/Squid-Users-f1019091.html


From uhlar at fantomas.sk  Mon Oct 29 13:20:42 2018
From: uhlar at fantomas.sk (Matus UHLAR - fantomas)
Date: Mon, 29 Oct 2018 14:20:42 +0100
Subject: [squid-users] Hyper-threading, SSD vs HDD,
 CentOS vs Ubuntu Server and best performer Squid version
In-Reply-To: <1540764795289-0.post@n4.nabble.com>
References: <1540764795289-0.post@n4.nabble.com>
Message-ID: <20181029132041.jc3htvyibofmrbq2@fantomas.sk>

On 28.10.18 17:13, manuelfgarcia wrote:
>only the physical cores are useful, hyper-threaded "cores" can actually be
>worse.

HT can add about 10-15% of CPU time when doing CPU inteensive tasks.
squid usually contains no CPU intensive tasks.

>Considering the statement that "hyper-threaded "cores" can actually be
>worse.", what is supposed to handle faster and concurrent connections on
>reverse proxy mode?:
>- Intel Xeon X3430 (2.40GHz, 2,80 GHz turbo, 4 physical cores, 4 threads)
>vs.
>- Intel Xeon E3-1230-V6 3.40GHz (3.50 GHz, 3.90 GHz, 4 physical cores, 8
>threads)

I would count both of them as 4-CPU machines, e.g. would not configure more
than four (apparently only three) workers for squid.

Luckily kernels seem to understand hyperthreading CPUs and don't use HT
unless needed, so the work will be spread over 4 cores in such case.
-- 
Matus UHLAR - fantomas, uhlar at fantomas.sk ; http://www.fantomas.sk/
Warning: I wish NOT to receive e-mail advertising to this address.
Varovanie: na tuto adresu chcem NEDOSTAVAT akukolvek reklamnu postu.
WinError #99999: Out of error messages.


From uhlar at fantomas.sk  Mon Oct 29 13:26:40 2018
From: uhlar at fantomas.sk (Matus UHLAR - fantomas)
Date: Mon, 29 Oct 2018 14:26:40 +0100
Subject: [squid-users] Squid proxy not working when upgrade from 27 to
	3.5
In-Reply-To: <1540777326629-0.post@n4.nabble.com>
References: <1540459174727-0.post@n4.nabble.com>
 <ae310bd0-f196-362a-8d0f-9410091b7d34@treenet.co.nz>
 <1540519560433-0.post@n4.nabble.com>
 <582f901ead8e9d3d0dd324e36bc39951@ngtech.co.il>
 <1540538814938-0.post@n4.nabble.com>
 <edf1c4c0-2c7e-ee05-b8b5-a2d1ba40ffb3@treenet.co.nz>
 <1540547103606-0.post@n4.nabble.com>
 <1540547186478-0.post@n4.nabble.com>
 <1540547386669-0.post@n4.nabble.com>
 <1540777326629-0.post@n4.nabble.com>
Message-ID: <20181029132640.c3qh7hdew5p2wuun@fantomas.sk>

On 28.10.18 20:42, Angus J. wrote:
>oul163:/var/log/squid # ls -rlt
>total 84
>-rw-r----- 1 squid squid     0 Oct 22 12:21 access.log
>-rw-r----- 1 squid squid     0 Oct 22 13:02 netdb.state
>-rw-r----- 1 squid squid 17784 Oct 29 09:13 store.log
>-rw-r----- 1 squid squid 55296 Oct 29 09:33 cache.log
>oul163:/var/log/squid #
>
>
>access.log is empty

that means your browser is not using the proxy.
First configure the browser to use the proxy.

no, first stop repeatedly posting your squid config and output of "squid -k
parse", we have seen both multiple times and can find them in archives, e.g.
here:

http://lists.squid-cache.org/pipermail/squid-users/2018-October/019575.html

-- 
Matus UHLAR - fantomas, uhlar at fantomas.sk ; http://www.fantomas.sk/
Warning: I wish NOT to receive e-mail advertising to this address.
Varovanie: na tuto adresu chcem NEDOSTAVAT akukolvek reklamnu postu.
Due to unexpected conditions Windows 2000 will be released
in first quarter of year 1901


From nebeduch at gmail.com  Mon Oct 29 15:20:01 2018
From: nebeduch at gmail.com (Uchenna Nebedum)
Date: Mon, 29 Oct 2018 16:20:01 +0100
Subject: [squid-users] ERROR The requested URL could not be retrieved
Message-ID: <CAHgpA+JZ1NX_JJD2py+YNtRn2-_hmw2Nfw=okAqhmRomP+LxGw@mail.gmail.com>

Good Day All,
I have setup squid 3.5 with mikrotik, and ssl bumping is enabled. after
accepting the certificate on the browser prompt, Squid throws an error on
the browser, "*unable to forward this request at this time.*" it throws
this error for http sites as well. please what could be causing this error.

*Please find attached my squid.conf*



































































*#cache_log /var/log/squid/cache.logcache_effective_user proxyacl localnet
src 10.0.0.0/24 <http://10.0.0.0/24>acl localnet src 172.16.0.0/12
<http://172.16.0.0/12>acl localnet src 192.168.0.0/16
<http://192.168.0.0/16>acl localnet src fc00::/7acl localnet src
fe80::/10acl SSL_ports port 443 acl Safe_ports port 80          # httpacl
Safe_ports port 21          # ftpacl Safe_ports port 443         # httpsacl
Safe_ports port 70          # gopheracl Safe_ports port 210         #
waisacl Safe_ports port 1025-65535  # unregistered portsacl Safe_ports port
280         # http-mgmtacl Safe_ports port 488         # gss-httpacl
Safe_ports port 591         # filemakeracl Safe_ports port 777         #
multiling httpacl CONNECT method CONNECThttp_access deny
!Safe_portshttp_access deny CONNECT !SSL_portsnever_direct allow
allhttp_access allow localhost managerhttp_access deny managerhttp_access
allow localnethttp_access allow localhosthttp_access deny
allvisible_hostname localhosthttp_port 3126 intercepthttp_port 3128
ssl-bump  generate-host-certificates=on dynamic_cert_mem_cache_size=4MB
cert=/opt/websafety/etc/myca.pem https_port 3127 intercept ssl-bump
generate-host-certificates=on dynamic_cert_mem_cache_size=4MB
cert=/opt/websafety/etc/myca.pemsslcrtd_program
/usr/local/squid/libexec/ssl_crtd -s /var/spool/squid_ssldb -M 4MB
sslcrtd_children 8 startup=1 idle=1sslproxy_cert_error allow
all#sslproxy_cert_error allow ssl_error_domains#sslproxy_cert_error allow
ssl_error_ipsacl step1 at_step SslBump1acl step2 at_step SslBump2acl step3
at_step SslBump3ssl_bump peek step1 allssl_bump stare step2 allssl_bump
bump step3 allssl_bump splice localhostssl_bump splice allvia
offforwarded_for onrequest_header_access From deny allrequest_header_access
Cache-Control deny allrequest_header_access Keep-Alive deny
allrequest_header_access Other deny allreply_header_access Set-Cookie deny
allreply_header_access Set-Cookie2 deny allreply_header_access Other deny
alladaptation_access greasyspoon allow alldns_timeout 30
secondsdns_v4_first on#ecap_enable officap_enable onicap_preview_enable
officap_preview_size 2048icap_persistent_connections
onadaptation_send_client_ip onadaptation_send_username onicap_service
greasyspoon respmod_precache icap://127.0.0.1:1344/response
<http://127.0.0.1:1344/response> bypass=0refresh_pattern ^ftp:
1440    20%    10080refresh_pattern ^gopher:    1440    0%
1440refresh_pattern -i (/cgi-bin/|\?) 0    0%    0refresh_pattern .
0    20%    4320shutdown_lifetime 10 seconds*


*and my access.log*




















































































*1540823796.041      1 10.0.0.252 TAG_NONE/200 0 CONNECT 52.114.76.34:443
<http://52.114.76.34:443> - HIER_NONE/- -1540823796.041      1 10.0.0.252
TAG_NONE/200 0 CONNECT 52.114.76.34:443 <http://52.114.76.34:443> -
HIER_NONE/- -1540823840.186      1 10.0.0.252 TAG_NONE/200 0 CONNECT
52.114.76.34:443 <http://52.114.76.34:443> - HIER_NONE/-
-1540823864.291      1 10.0.0.252 TAG_NONE/200 0 CONNECT 191.239.240.49:443
<http://191.239.240.49:443> - HIER_NONE/- -1540823864.297      8 10.0.0.252
TAG_NONE/200 0 CONNECT 191.239.240.49:443 <http://191.239.240.49:443> -
HIER_NONE/- -1540823864.342      1 10.0.0.252 TAG_NONE/200 0 CONNECT
191.239.240.49:443 <http://191.239.240.49:443> - HIER_NONE/-
-1540823864.628      1 10.0.0.252 TAG_NONE/200 0 CONNECT 152.199.19.161:443
<http://152.199.19.161:443> - HIER_NONE/- -1540823864.628      1 10.0.0.252
TAG_NONE/200 0 CONNECT 152.199.19.161:443 <http://152.199.19.161:443> -
HIER_NONE/- -1540823864.644      1 10.0.0.252 TAG_NONE/200 0 CONNECT
152.199.19.161:443 <http://152.199.19.161:443> - HIER_NONE/-
-1540824133.725    117 10.0.0.253 TCP_MISS/500 4215 GET
http://init-p01md.apple.com/bag <http://init-p01md.apple.com/bag> -
HIER_NONE/- text/html1540824133.725    114 10.0.0.253 TCP_MISS/500 4215 GET
http://init-p01md.apple.com/bag <http://init-p01md.apple.com/bag> -
HIER_NONE/- text/html1540824133.729    112 10.0.0.253 TCP_MISS/500 4310 GET
http://init.ess.apple.com/WebObjects/VCInit.woa/wa/getBag
<http://init.ess.apple.com/WebObjects/VCInit.woa/wa/getBag>? - HIER_NONE/-
text/html1540824133.729    109 10.0.0.253 TCP_MISS/500 4310 GET
http://init.ess.apple.com/WebObjects/VCInit.woa/wa/getBag
<http://init.ess.apple.com/WebObjects/VCInit.woa/wa/getBag>? - HIER_NONE/-
text/html1540824133.850     14 10.0.0.253 TAG_NONE/200 0 CONNECT
95.101.216.92:443 <http://95.101.216.92:443> - HIER_NONE/-
-1540824133.850     11 10.0.0.253 TAG_NONE/200 0 CONNECT 95.101.216.92:443
<http://95.101.216.92:443> - HIER_NONE/- -1540824133.854     12 10.0.0.253
TAG_NONE/200 0 CONNECT 95.101.216.92:443 <http://95.101.216.92:443> -
HIER_NONE/- -1540824133.966    122 10.0.0.253 TCP_MISS/500 4205 GET
http://init-p01st.push.apple.com/bag <http://init-p01st.push.apple.com/bag>
- HIER_NONE/- text/html1540824133.987    164 10.0.0.253 TAG_NONE/200 0
CONNECT 95.101.188.60:443 <http://95.101.188.60:443> - HIER_NONE/-
-1540824133.987    164 10.0.0.253 TAG_NONE/200 0 CONNECT 17.137.166.4:443
<http://17.137.166.4:443> - HIER_NONE/- -1540824134.251      4 10.0.0.253
TAG_NONE/200 0 CONNECT 95.101.188.60:443 <http://95.101.188.60:443> -
HIER_NONE/- -1540824134.336      4 10.0.0.253 TAG_NONE/200 0 CONNECT
17.167.193.43:443 <http://17.167.193.43:443> - HIER_NONE/-
-1540824136.162     17 10.0.0.253 TAG_NONE/200 0 CONNECT 192.12.31.78:443
<http://192.12.31.78:443> - HIER_NONE/- -1540824136.299      4 10.0.0.253
TAG_NONE/200 0 CONNECT 157.119.235.19:443 <http://157.119.235.19:443> -
HIER_NONE/- -1540824150.357      4 10.0.0.253 TAG_NONE/200 0 CONNECT
17.167.192.128:443 <http://17.167.192.128:443> - HIER_NONE/-
-1540824159.403      4 10.0.0.253 TAG_NONE/200 0 CONNECT 17.167.192.128:443
<http://17.167.192.128:443> - HIER_NONE/- -1540824769.945    601 10.0.0.253
TCP_MISS/500 4217 GET http://captive.apple.com/hotspot-detect.html
<http://captive.apple.com/hotspot-detect.html> - HIER_NONE/-
text/html1540824770.651    135 10.0.0.253 TAG_NONE/200 0 CONNECT
216.58.223.194:443 <http://216.58.223.194:443> - HIER_NONE/-
-1540824770.654    136 10.0.0.253 TAG_NONE/200 0 CONNECT 104.83.75.199:443
<http://104.83.75.199:443> - HIER_NONE/- -1540824771.204    351 10.0.0.253
TAG_NONE/200 0 CONNECT 17.151.240.36:443 <http://17.151.240.36:443> -
HIER_NONE/- -1540824771.451     10 10.0.0.253 TAG_NONE/200 0 CONNECT
17.120.225.140:443 <http://17.120.225.140:443> - HIER_NONE/-
-1540824771.452      7 10.0.0.253 TAG_NONE/200 0 CONNECT 17.120.225.140:443
<http://17.120.225.140:443> - HIER_NONE/- -1540824771.680    827 10.0.0.253
TAG_NONE/200 0 CONNECT 216.58.223.202:443 <http://216.58.223.202:443> -
HIER_NONE/- -1540824771.688    833 10.0.0.253 TAG_NONE/200 0 CONNECT
216.58.223.194:443 <http://216.58.223.194:443> - HIER_NONE/-
-1540824771.688      1 10.0.0.253 TAG_NONE/200 0 CONNECT 216.58.223.202:443
<http://216.58.223.202:443> - HIER_NONE/- -1540824771.693      6 10.0.0.253
TAG_NONE/200 0 CONNECT 104.83.64.191:443 <http://104.83.64.191:443> -
HIER_NONE/- -1540824771.847    159 10.0.0.253 TAG_NONE/200 0 CONNECT
17.151.240.36:443 <http://17.151.240.36:443> - HIER_NONE/-
-1540824771.882     30 10.0.0.253 TAG_NONE/200 0 CONNECT 216.58.223.202:443
<http://216.58.223.202:443> - HIER_NONE/- -1540824771.883     30 10.0.0.253
TAG_NONE/200 0 CONNECT 216.58.223.194:443 <http://216.58.223.194:443> -
HIER_NONE/- -1540824771.887     36 10.0.0.253 TAG_NONE/200 0 CONNECT
17.248.146.179:443 <http://17.248.146.179:443> - HIER_NONE/-
-1540824772.034     42 10.0.0.253 TAG_NONE/200 0 CONNECT 216.58.223.206:443
<http://216.58.223.206:443> - HIER_NONE/- -1540824772.036      6 10.0.0.253
TAG_NONE/200 0 CONNECT 216.58.223.194:443 <http://216.58.223.194:443> -
HIER_NONE/- -1540824772.042      1 10.0.0.253 TAG_NONE/200 0 CONNECT
17.151.240.36:443 <http://17.151.240.36:443> - HIER_NONE/-
-1540824772.078      5 10.0.0.253 TAG_NONE/200 0 CONNECT 216.58.223.194:443
<http://216.58.223.194:443> - HIER_NONE/- -1540824772.146     15 10.0.0.253
TAG_NONE/200 0 CONNECT 17.151.240.36:443 <http://17.151.240.36:443> -
HIER_NONE/- -1540824772.150      4 10.0.0.253 TAG_NONE/200 0 CONNECT
104.83.75.199:443 <http://104.83.75.199:443> - HIER_NONE/-
-1540824772.172      5 10.0.0.253 TAG_NONE/200 0 CONNECT 104.83.75.199:443
<http://104.83.75.199:443> - HIER_NONE/- -1540824772.243     90 10.0.0.253
TAG_NONE/200 0 CONNECT 216.58.223.194:443 <http://216.58.223.194:443> -
HIER_NONE/- -1540824772.278      5 10.0.0.253 TAG_NONE/200 0 CONNECT
17.248.146.179:443 <http://17.248.146.179:443> - HIER_NONE/-
-1540824772.296      4 10.0.0.253 TAG_NONE/200 0 CONNECT 216.58.223.194:443
<http://216.58.223.194:443> - HIER_NONE/- -1540824772.341      8 10.0.0.253
TAG_NONE/200 0 CONNECT 216.58.223.194:443 <http://216.58.223.194:443> -
HIER_NONE/- -1540824772.719     10 10.0.0.253 TAG_NONE/200 0 CONNECT
216.58.223.202:443 <http://216.58.223.202:443> - HIER_NONE/-
-1540824772.722      5 10.0.0.253 TAG_NONE/200 0 CONNECT 17.151.240.36:443
<http://17.151.240.36:443> - HIER_NONE/- -1540824772.787      9 10.0.0.253
TAG_NONE/200 0 CONNECT 17.248.146.179:443 <http://17.248.146.179:443> -
HIER_NONE/- -1540824772.868      4 10.0.0.253 TAG_NONE/200 0 CONNECT
216.58.223.202:443 <http://216.58.223.202:443> - HIER_NONE/-
-1540824773.239      5 10.0.0.253 TAG_NONE/200 0 CONNECT 216.58.223.202:443
<http://216.58.223.202:443> - HIER_NONE/- -1540824773.810      8 10.0.0.253
TAG_NONE/200 0 CONNECT 17.151.240.36:443 <http://17.151.240.36:443> -
HIER_NONE/- -1540824773.868      4 10.0.0.253 TAG_NONE/200 0 CONNECT
17.248.146.179:443 <http://17.248.146.179:443> - HIER_NONE/-
-1540824774.898      4 10.0.0.253 TAG_NONE/200 0 CONNECT 17.151.240.36:443
<http://17.151.240.36:443> - HIER_NONE/- -1540824774.964      7 10.0.0.253
TAG_NONE/200 0 CONNECT 17.248.146.179:443 <http://17.248.146.179:443> -
HIER_NONE/- -1540824776.218      4 10.0.0.253 TAG_NONE/200 0 CONNECT
104.83.75.199:443 <http://104.83.75.199:443> - HIER_NONE/-
-1540824956.204     56 10.0.0.253 TAG_NONE/200 0 CONNECT 104.83.75.199:443
<http://104.83.75.199:443> - HIER_NONE/- -1540824956.374    110 10.0.0.253
TCP_MISS/500 4205 GET http://init-p01st.push.apple.com/bag
<http://init-p01st.push.apple.com/bag> - HIER_NONE/-
text/html1540824956.966      5 10.0.0.253 TAG_NONE/200 0 CONNECT
17.151.240.36:443 <http://17.151.240.36:443> - HIER_NONE/-
-1540824957.034      7 10.0.0.253 TAG_NONE/200 0 CONNECT 17.151.240.36:443
<http://17.151.240.36:443> - HIER_NONE/- -1540824957.043      3 10.0.0.253
TAG_NONE/200 0 CONNECT 104.83.75.199:443 <http://104.83.75.199:443> -
HIER_NONE/- -1540824957.124     23 10.0.0.253 TAG_NONE/200 0 CONNECT
104.83.75.199:443 <http://104.83.75.199:443> - HIER_NONE/-
-1540824957.190     13 10.0.0.253 TAG_NONE/200 0 CONNECT 17.151.240.36:443
<http://17.151.240.36:443> - HIER_NONE/- -1540824957.273      4 10.0.0.253
TAG_NONE/200 0 CONNECT 104.83.75.199:443 <http://104.83.75.199:443> -
HIER_NONE/- -1540824957.355      4 10.0.0.253 TAG_NONE/200 0 CONNECT
17.151.240.36:443 <http://17.151.240.36:443> - HIER_NONE/-
-1540824957.495      4 10.0.0.253 TAG_NONE/200 0 CONNECT 104.83.75.199:443
<http://104.83.75.199:443> - HIER_NONE/- -1540824957.573      4 10.0.0.253
TAG_NONE/200 0 CONNECT 17.151.240.36:443 <http://17.151.240.36:443> -
HIER_NONE/- -1540824957.642      5 10.0.0.253 TAG_NONE/200 0 CONNECT
104.83.75.199:443 <http://104.83.75.199:443> - HIER_NONE/-
-1540824957.723      4 10.0.0.253 TAG_NONE/200 0 CONNECT 17.151.240.36:443
<http://17.151.240.36:443> - HIER_NONE/- -1540824957.783      4 10.0.0.253
TAG_NONE/200 0 CONNECT 104.83.75.199:443 <http://104.83.75.199:443> -
HIER_NONE/- -1540824967.333      5 10.0.0.253 TAG_NONE/200 0 CONNECT
104.83.75.199:443 <http://104.83.75.199:443> - HIER_NONE/-
-1540824967.398      5 10.0.0.253 TAG_NONE/200 0 CONNECT 17.151.240.36:443
<http://17.151.240.36:443> - HIER_NONE/- -1540824967.454      4 10.0.0.253
TAG_NONE/200 0 CONNECT 104.83.75.199:443 <http://104.83.75.199:443> -
HIER_NONE/- -1540824970.474      4 10.0.0.253 TAG_NONE/200 0 CONNECT
17.151.240.36:443 <http://17.151.240.36:443> - HIER_NONE/-
-1540824971.300      5 10.0.0.253 TAG_NONE/200 0 CONNECT 17.56.48.13:443
<http://17.56.48.13:443> - HIER_NONE/- -1540824971.625      9 10.0.0.253
TAG_NONE/200 0 CONNECT 92.122.44.112:443 <http://92.122.44.112:443> -
HIER_NONE/- -1540825078.056      4 10.0.0.253 TAG_NONE/200 0 CONNECT
17.151.240.36:443 <http://17.151.240.36:443> - HIER_NONE/-
-1540825078.058     14 10.0.0.253 TAG_NONE/200 0 CONNECT 104.83.75.199:443
<http://104.83.75.199:443> - HIER_NONE/- -1540825078.224      8 10.0.0.253
TAG_NONE/200 0 CONNECT 104.83.75.199:443 <http://104.83.75.199:443> -
HIER_NONE/- -1540825584.867    258 10.0.0.253 TCP_MISS/500 4217 GET
http://captive.apple.com/hotspot-detect.html
<http://captive.apple.com/hotspot-detect.html> - HIER_NONE/- text/html*

please i'll provide any other information required. please i really need
help. I noticed my last two questions weren't answered, i really need help.
I've noticed google and facebook are reachable.

-- 
Nebedum Uchenna
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20181029/d5d6d850/attachment.htm>

From squid at borrill.org.uk  Mon Oct 29 15:23:33 2018
From: squid at borrill.org.uk (Stephen Borrill)
Date: Mon, 29 Oct 2018 15:23:33 +0000
Subject: [squid-users] ERROR The requested URL could not be retrieved
In-Reply-To: <CAHgpA+JZ1NX_JJD2py+YNtRn2-_hmw2Nfw=okAqhmRomP+LxGw@mail.gmail.com>
References: <CAHgpA+JZ1NX_JJD2py+YNtRn2-_hmw2Nfw=okAqhmRomP+LxGw@mail.gmail.com>
Message-ID: <188c5209-d67d-c28b-4702-b28b2d874a6c@borrill.org.uk>

On 29/10/2018 15:20, Uchenna Nebedum wrote:
> Good Day All,
> I have setup squid 3.5 with mikrotik, and ssl bumping is enabled. after
> accepting the certificate on the browser prompt, Squid throws an error
> on the browser, "*unable to forward this request at this time.*" it
> throws this error for http sites as well. please what could be causing
> this error.

never_direct allow all

How is your proxy meant to forward on requests? You have no cache peers,
but have told it never to go direct (i.e. always use a cache peer).

> *Please find attached my squid.conf*
> /#cache_log /var/log/squid/cache.log
> cache_effective_user proxy
> acl localnet src 10.0.0.0/24 <http://10.0.0.0/24>
> acl localnet src 172.16.0.0/12 <http://172.16.0.0/12>
> acl localnet src 192.168.0.0/16 <http://192.168.0.0/16>
> acl localnet src fc00::/7
> acl localnet src fe80::/10
> acl SSL_ports port 443
> acl Safe_ports port 80????????? # http
> acl Safe_ports port 21????????? # ftp
> acl Safe_ports port 443???????? # https
> acl Safe_ports port 70????????? # gopher
> acl Safe_ports port 210???????? # wais
> acl Safe_ports port 1025-65535? # unregistered ports
> acl Safe_ports port 280???????? # http-mgmt
> acl Safe_ports port 488???????? # gss-http
> acl Safe_ports port 591???????? # filemaker
> acl Safe_ports port 777???????? # multiling http
> acl CONNECT method CONNECT
> http_access deny !Safe_ports
> http_access deny CONNECT !SSL_ports
> never_direct allow all
> http_access allow localhost manager
> http_access deny manager
> http_access allow localnet
> http_access allow localhost
> http_access deny all
> visible_hostname localhost
> http_port 3126 intercept
> http_port 3128 ssl-bump? generate-host-certificates=on
> dynamic_cert_mem_cache_size=4MB cert=/opt/websafety/etc/myca.pem
> https_port 3127 intercept ssl-bump generate-host-certificates=on
> dynamic_cert_mem_cache_size=4MB cert=/opt/websafety/etc/myca.pem
> sslcrtd_program /usr/local/squid/libexec/ssl_crtd -s
> /var/spool/squid_ssldb -M 4MB sslcrtd_children 8 startup=1 idle=1
> sslproxy_cert_error allow all
> #sslproxy_cert_error allow ssl_error_domains
> #sslproxy_cert_error allow ssl_error_ips
> acl step1 at_step SslBump1
> acl step2 at_step SslBump2
> acl step3 at_step SslBump3
> ssl_bump peek step1 all
> ssl_bump stare step2 all
> ssl_bump bump step3 all
> ssl_bump splice localhost
> ssl_bump splice all
> via off
> forwarded_for on
> request_header_access From deny all
> request_header_access Cache-Control deny all
> request_header_access Keep-Alive deny all
> request_header_access Other deny all
> reply_header_access Set-Cookie deny all
> reply_header_access Set-Cookie2 deny all
> reply_header_access Other deny all
> adaptation_access greasyspoon allow all
> dns_timeout 30 seconds
> dns_v4_first on
> #ecap_enable off
> icap_enable on
> icap_preview_enable off
> icap_preview_size 2048
> icap_persistent_connections on
> adaptation_send_client_ip on
> adaptation_send_username on
> icap_service greasyspoon respmod_precache icap://127.0.0.1:1344/response
> <http://127.0.0.1:1344/response> bypass=0
> refresh_pattern ^ftp:??? ??? 1440??? 20%??? 10080
> refresh_pattern ^gopher:??? 1440??? 0%??? 1440
> refresh_pattern -i (/cgi-bin/|\?) 0??? 0%??? 0
> refresh_pattern .??? ??? 0??? 20%??? 4320
> shutdown_lifetime 10 seconds/
> 
> 
> *and my access.log*
> /1540823796.041????? 1 10.0.0.252 TAG_NONE/200 0 CONNECT
> 52.114.76.34:443 <http://52.114.76.34:443> - HIER_NONE/- -
> 1540823796.041????? 1 10.0.0.252 TAG_NONE/200 0 CONNECT 52.114.76.34:443
> <http://52.114.76.34:443> - HIER_NONE/- -
> 1540823840.186????? 1 10.0.0.252 TAG_NONE/200 0 CONNECT 52.114.76.34:443
> <http://52.114.76.34:443> - HIER_NONE/- -
> 1540823864.291????? 1 10.0.0.252 TAG_NONE/200 0 CONNECT
> 191.239.240.49:443 <http://191.239.240.49:443> - HIER_NONE/- -
> 1540823864.297????? 8 10.0.0.252 TAG_NONE/200 0 CONNECT
> 191.239.240.49:443 <http://191.239.240.49:443> - HIER_NONE/- -
> 1540823864.342????? 1 10.0.0.252 TAG_NONE/200 0 CONNECT
> 191.239.240.49:443 <http://191.239.240.49:443> - HIER_NONE/- -
> 1540823864.628????? 1 10.0.0.252 TAG_NONE/200 0 CONNECT
> 152.199.19.161:443 <http://152.199.19.161:443> - HIER_NONE/- -
> 1540823864.628????? 1 10.0.0.252 TAG_NONE/200 0 CONNECT
> 152.199.19.161:443 <http://152.199.19.161:443> - HIER_NONE/- -
> 1540823864.644????? 1 10.0.0.252 TAG_NONE/200 0 CONNECT
> 152.199.19.161:443 <http://152.199.19.161:443> - HIER_NONE/- -
> 1540824133.725??? 117 10.0.0.253 TCP_MISS/500 4215 GET
> http://init-p01md.apple.com/bag - HIER_NONE/- text/html
> 1540824133.725??? 114 10.0.0.253 TCP_MISS/500 4215 GET
> http://init-p01md.apple.com/bag - HIER_NONE/- text/html
> 1540824133.729??? 112 10.0.0.253 TCP_MISS/500 4310 GET
> http://init.ess.apple.com/WebObjects/VCInit.woa/wa/getBag? - HIER_NONE/-
> text/html
> 1540824133.729??? 109 10.0.0.253 TCP_MISS/500 4310 GET
> http://init.ess.apple.com/WebObjects/VCInit.woa/wa/getBag? - HIER_NONE/-
> text/html
> 1540824133.850???? 14 10.0.0.253 TAG_NONE/200 0 CONNECT
> 95.101.216.92:443 <http://95.101.216.92:443> - HIER_NONE/- -
> 1540824133.850???? 11 10.0.0.253 TAG_NONE/200 0 CONNECT
> 95.101.216.92:443 <http://95.101.216.92:443> - HIER_NONE/- -
> 1540824133.854???? 12 10.0.0.253 TAG_NONE/200 0 CONNECT
> 95.101.216.92:443 <http://95.101.216.92:443> - HIER_NONE/- -
> 1540824133.966??? 122 10.0.0.253 TCP_MISS/500 4205 GET
> http://init-p01st.push.apple.com/bag - HIER_NONE/- text/html
> 1540824133.987??? 164 10.0.0.253 TAG_NONE/200 0 CONNECT
> 95.101.188.60:443 <http://95.101.188.60:443> - HIER_NONE/- -
> 1540824133.987??? 164 10.0.0.253 TAG_NONE/200 0 CONNECT 17.137.166.4:443
> <http://17.137.166.4:443> - HIER_NONE/- -
> 1540824134.251????? 4 10.0.0.253 TAG_NONE/200 0 CONNECT
> 95.101.188.60:443 <http://95.101.188.60:443> - HIER_NONE/- -
> 1540824134.336????? 4 10.0.0.253 TAG_NONE/200 0 CONNECT
> 17.167.193.43:443 <http://17.167.193.43:443> - HIER_NONE/- -
> 1540824136.162???? 17 10.0.0.253 TAG_NONE/200 0 CONNECT 192.12.31.78:443
> <http://192.12.31.78:443> - HIER_NONE/- -
> 1540824136.299????? 4 10.0.0.253 TAG_NONE/200 0 CONNECT
> 157.119.235.19:443 <http://157.119.235.19:443> - HIER_NONE/- -
> 1540824150.357????? 4 10.0.0.253 TAG_NONE/200 0 CONNECT
> 17.167.192.128:443 <http://17.167.192.128:443> - HIER_NONE/- -
> 1540824159.403????? 4 10.0.0.253 TAG_NONE/200 0 CONNECT
> 17.167.192.128:443 <http://17.167.192.128:443> - HIER_NONE/- -
> 1540824769.945??? 601 10.0.0.253 TCP_MISS/500 4217 GET
> http://captive.apple.com/hotspot-detect.html - HIER_NONE/- text/html
> 1540824770.651??? 135 10.0.0.253 TAG_NONE/200 0 CONNECT
> 216.58.223.194:443 <http://216.58.223.194:443> - HIER_NONE/- -
> 1540824770.654??? 136 10.0.0.253 TAG_NONE/200 0 CONNECT
> 104.83.75.199:443 <http://104.83.75.199:443> - HIER_NONE/- -
> 1540824771.204??? 351 10.0.0.253 TAG_NONE/200 0 CONNECT
> 17.151.240.36:443 <http://17.151.240.36:443> - HIER_NONE/- -
> 1540824771.451???? 10 10.0.0.253 TAG_NONE/200 0 CONNECT
> 17.120.225.140:443 <http://17.120.225.140:443> - HIER_NONE/- -
> 1540824771.452????? 7 10.0.0.253 TAG_NONE/200 0 CONNECT
> 17.120.225.140:443 <http://17.120.225.140:443> - HIER_NONE/- -
> 1540824771.680??? 827 10.0.0.253 TAG_NONE/200 0 CONNECT
> 216.58.223.202:443 <http://216.58.223.202:443> - HIER_NONE/- -
> 1540824771.688??? 833 10.0.0.253 TAG_NONE/200 0 CONNECT
> 216.58.223.194:443 <http://216.58.223.194:443> - HIER_NONE/- -
> 1540824771.688????? 1 10.0.0.253 TAG_NONE/200 0 CONNECT
> 216.58.223.202:443 <http://216.58.223.202:443> - HIER_NONE/- -
> 1540824771.693????? 6 10.0.0.253 TAG_NONE/200 0 CONNECT
> 104.83.64.191:443 <http://104.83.64.191:443> - HIER_NONE/- -
> 1540824771.847??? 159 10.0.0.253 TAG_NONE/200 0 CONNECT
> 17.151.240.36:443 <http://17.151.240.36:443> - HIER_NONE/- -
> 1540824771.882???? 30 10.0.0.253 TAG_NONE/200 0 CONNECT
> 216.58.223.202:443 <http://216.58.223.202:443> - HIER_NONE/- -
> 1540824771.883???? 30 10.0.0.253 TAG_NONE/200 0 CONNECT
> 216.58.223.194:443 <http://216.58.223.194:443> - HIER_NONE/- -
> 1540824771.887???? 36 10.0.0.253 TAG_NONE/200 0 CONNECT
> 17.248.146.179:443 <http://17.248.146.179:443> - HIER_NONE/- -
> 1540824772.034???? 42 10.0.0.253 TAG_NONE/200 0 CONNECT
> 216.58.223.206:443 <http://216.58.223.206:443> - HIER_NONE/- -
> 1540824772.036????? 6 10.0.0.253 TAG_NONE/200 0 CONNECT
> 216.58.223.194:443 <http://216.58.223.194:443> - HIER_NONE/- -
> 1540824772.042????? 1 10.0.0.253 TAG_NONE/200 0 CONNECT
> 17.151.240.36:443 <http://17.151.240.36:443> - HIER_NONE/- -
> 1540824772.078????? 5 10.0.0.253 TAG_NONE/200 0 CONNECT
> 216.58.223.194:443 <http://216.58.223.194:443> - HIER_NONE/- -
> 1540824772.146???? 15 10.0.0.253 TAG_NONE/200 0 CONNECT
> 17.151.240.36:443 <http://17.151.240.36:443> - HIER_NONE/- -
> 1540824772.150????? 4 10.0.0.253 TAG_NONE/200 0 CONNECT
> 104.83.75.199:443 <http://104.83.75.199:443> - HIER_NONE/- -
> 1540824772.172????? 5 10.0.0.253 TAG_NONE/200 0 CONNECT
> 104.83.75.199:443 <http://104.83.75.199:443> - HIER_NONE/- -
> 1540824772.243???? 90 10.0.0.253 TAG_NONE/200 0 CONNECT
> 216.58.223.194:443 <http://216.58.223.194:443> - HIER_NONE/- -
> 1540824772.278????? 5 10.0.0.253 TAG_NONE/200 0 CONNECT
> 17.248.146.179:443 <http://17.248.146.179:443> - HIER_NONE/- -
> 1540824772.296????? 4 10.0.0.253 TAG_NONE/200 0 CONNECT
> 216.58.223.194:443 <http://216.58.223.194:443> - HIER_NONE/- -
> 1540824772.341????? 8 10.0.0.253 TAG_NONE/200 0 CONNECT
> 216.58.223.194:443 <http://216.58.223.194:443> - HIER_NONE/- -
> 1540824772.719???? 10 10.0.0.253 TAG_NONE/200 0 CONNECT
> 216.58.223.202:443 <http://216.58.223.202:443> - HIER_NONE/- -
> 1540824772.722????? 5 10.0.0.253 TAG_NONE/200 0 CONNECT
> 17.151.240.36:443 <http://17.151.240.36:443> - HIER_NONE/- -
> 1540824772.787????? 9 10.0.0.253 TAG_NONE/200 0 CONNECT
> 17.248.146.179:443 <http://17.248.146.179:443> - HIER_NONE/- -
> 1540824772.868????? 4 10.0.0.253 TAG_NONE/200 0 CONNECT
> 216.58.223.202:443 <http://216.58.223.202:443> - HIER_NONE/- -
> 1540824773.239????? 5 10.0.0.253 TAG_NONE/200 0 CONNECT
> 216.58.223.202:443 <http://216.58.223.202:443> - HIER_NONE/- -
> 1540824773.810????? 8 10.0.0.253 TAG_NONE/200 0 CONNECT
> 17.151.240.36:443 <http://17.151.240.36:443> - HIER_NONE/- -
> 1540824773.868????? 4 10.0.0.253 TAG_NONE/200 0 CONNECT
> 17.248.146.179:443 <http://17.248.146.179:443> - HIER_NONE/- -
> 1540824774.898????? 4 10.0.0.253 TAG_NONE/200 0 CONNECT
> 17.151.240.36:443 <http://17.151.240.36:443> - HIER_NONE/- -
> 1540824774.964????? 7 10.0.0.253 TAG_NONE/200 0 CONNECT
> 17.248.146.179:443 <http://17.248.146.179:443> - HIER_NONE/- -
> 1540824776.218????? 4 10.0.0.253 TAG_NONE/200 0 CONNECT
> 104.83.75.199:443 <http://104.83.75.199:443> - HIER_NONE/- -
> 1540824956.204???? 56 10.0.0.253 TAG_NONE/200 0 CONNECT
> 104.83.75.199:443 <http://104.83.75.199:443> - HIER_NONE/- -
> 1540824956.374??? 110 10.0.0.253 TCP_MISS/500 4205 GET
> http://init-p01st.push.apple.com/bag - HIER_NONE/- text/html
> 1540824956.966????? 5 10.0.0.253 TAG_NONE/200 0 CONNECT
> 17.151.240.36:443 <http://17.151.240.36:443> - HIER_NONE/- -
> 1540824957.034????? 7 10.0.0.253 TAG_NONE/200 0 CONNECT
> 17.151.240.36:443 <http://17.151.240.36:443> - HIER_NONE/- -
> 1540824957.043????? 3 10.0.0.253 TAG_NONE/200 0 CONNECT
> 104.83.75.199:443 <http://104.83.75.199:443> - HIER_NONE/- -
> 1540824957.124???? 23 10.0.0.253 TAG_NONE/200 0 CONNECT
> 104.83.75.199:443 <http://104.83.75.199:443> - HIER_NONE/- -
> 1540824957.190???? 13 10.0.0.253 TAG_NONE/200 0 CONNECT
> 17.151.240.36:443 <http://17.151.240.36:443> - HIER_NONE/- -
> 1540824957.273????? 4 10.0.0.253 TAG_NONE/200 0 CONNECT
> 104.83.75.199:443 <http://104.83.75.199:443> - HIER_NONE/- -
> 1540824957.355????? 4 10.0.0.253 TAG_NONE/200 0 CONNECT
> 17.151.240.36:443 <http://17.151.240.36:443> - HIER_NONE/- -
> 1540824957.495????? 4 10.0.0.253 TAG_NONE/200 0 CONNECT
> 104.83.75.199:443 <http://104.83.75.199:443> - HIER_NONE/- -
> 1540824957.573????? 4 10.0.0.253 TAG_NONE/200 0 CONNECT
> 17.151.240.36:443 <http://17.151.240.36:443> - HIER_NONE/- -
> 1540824957.642????? 5 10.0.0.253 TAG_NONE/200 0 CONNECT
> 104.83.75.199:443 <http://104.83.75.199:443> - HIER_NONE/- -
> 1540824957.723????? 4 10.0.0.253 TAG_NONE/200 0 CONNECT
> 17.151.240.36:443 <http://17.151.240.36:443> - HIER_NONE/- -
> 1540824957.783????? 4 10.0.0.253 TAG_NONE/200 0 CONNECT
> 104.83.75.199:443 <http://104.83.75.199:443> - HIER_NONE/- -
> 1540824967.333????? 5 10.0.0.253 TAG_NONE/200 0 CONNECT
> 104.83.75.199:443 <http://104.83.75.199:443> - HIER_NONE/- -
> 1540824967.398????? 5 10.0.0.253 TAG_NONE/200 0 CONNECT
> 17.151.240.36:443 <http://17.151.240.36:443> - HIER_NONE/- -
> 1540824967.454????? 4 10.0.0.253 TAG_NONE/200 0 CONNECT
> 104.83.75.199:443 <http://104.83.75.199:443> - HIER_NONE/- -
> 1540824970.474????? 4 10.0.0.253 TAG_NONE/200 0 CONNECT
> 17.151.240.36:443 <http://17.151.240.36:443> - HIER_NONE/- -
> 1540824971.300????? 5 10.0.0.253 TAG_NONE/200 0 CONNECT 17.56.48.13:443
> <http://17.56.48.13:443> - HIER_NONE/- -
> 1540824971.625????? 9 10.0.0.253 TAG_NONE/200 0 CONNECT
> 92.122.44.112:443 <http://92.122.44.112:443> - HIER_NONE/- -
> 1540825078.056????? 4 10.0.0.253 TAG_NONE/200 0 CONNECT
> 17.151.240.36:443 <http://17.151.240.36:443> - HIER_NONE/- -
> 1540825078.058???? 14 10.0.0.253 TAG_NONE/200 0 CONNECT
> 104.83.75.199:443 <http://104.83.75.199:443> - HIER_NONE/- -
> 1540825078.224????? 8 10.0.0.253 TAG_NONE/200 0 CONNECT
> 104.83.75.199:443 <http://104.83.75.199:443> - HIER_NONE/- -
> 1540825584.867??? 258 10.0.0.253 TCP_MISS/500 4217 GET
> http://captive.apple.com/hotspot-detect.html - HIER_NONE/- text/html
> /*
> *
> 
> please i'll provide any other information required. please i really need
> help. I noticed my last two questions weren't answered, i really need
> help. I've noticed google and facebook are reachable.
> 
> -- 
> Nebedum Uchenna
> 
> 
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
> 



From rousskov at measurement-factory.com  Mon Oct 29 16:07:06 2018
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Mon, 29 Oct 2018 10:07:06 -0600
Subject: [squid-users] Hyper-threading, SSD vs HDD,
 CentOS vs Ubuntu Server and best performer Squid version
In-Reply-To: <1540764795289-0.post@n4.nabble.com>
References: <1540764795289-0.post@n4.nabble.com>
Message-ID: <a5ab9e33-5bf6-8d00-b881-0e66f456794e@measurement-factory.com>

On 10/28/18 4:13 PM, manuelfgarcia wrote:
> According to https://wiki.squid-cache.org/BestOsForSquid :
> "CPU speed and core count:
> few faster cores are better than many slow cores. SMP Squid can currently
> operate most efficiently with 4-8 cores of 3GHz or more. multi-tenant
> installations are better for machinery with very many cores.
> only the physical cores are useful, hyper-threaded "cores" can actually be
> worse.

> Is that statement still up to date or is is it no longer the case from any
> specific version of Squid?

There are several statements in "that statement":

* The statement about fewer faster cores being better can be true or
false for SMP Squids, depending on how much fewer and how much faster
those cores are. That statement should be removed or toned down IMO.

* The statement that only the physical cores are useful is bogus.
Virtual cores can be useful in some environments. For busy systems with
many cores, it is usually a bad idea to put multiple busy processes
(e.g., Squid workers) on one physical core, but that does not mean that
ignoring or disabling virtual cores is always a good idea. The optimal
process:core mapping depends on many factors.

* The statement about hyperthreading sometimes leading to worse
performance is still correct. Improper use of hyperthreading can
decrease performance compared to disabling hyperthreading. However, that
does not mean one should always disable hyperthreading.

The above applies to all modern Squids.


> Considering the statement that "hyper-threaded "cores" can actually be
> worse.", what is supposed to handle faster and concurrent connections on
> reverse proxy mode?:
> - Intel Xeon X3430 (2.40GHz, 2,80 GHz turbo, 4 physical cores, 4 threads)
> vs.
> - Intel Xeon E3-1230-V6 3.40GHz (3.50 GHz, 3.90 GHz, 4 physical cores, 8
> threads)

In most cases, it is not so much the _presence_ of virtual cores that
matters. It is whether you overload physical cores by placing too many
busy processes on them. Thus, if each of the two machines is correctly
configured/optimized, then I would bet on E3-1230-V6 because it has the
same number of physical cores but they are faster.


> On another note, what is supposed to handle faster and more concurrent
> connections on reverse proxy mode between these options?:
> 
> - SSD vs HDD

SSD if disk I/O is a bottleneck.


> - CentOS 7 vs CentOS 6 vs Ubuntu Server 16.04 vs Ubuntu Server 18.04

I would not expect a huge difference if each is properly configured and
tuned, but we have not compared these four in our lab.


> - Squid 3.1 vs 3.5 vs 4.x...

Squid v2 :-). If you are selecting a Squid version based primarily on
its speed, you will probably regret your choice. My suggestion is to
avoid Squid v3.1 in all cases and use Squid v4 if you can.


HTH,

Alex.


From squid3 at treenet.co.nz  Mon Oct 29 20:48:55 2018
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Tue, 30 Oct 2018 09:48:55 +1300
Subject: [squid-users] Squid proxy not working when upgrade from 27 to
	3.5
In-Reply-To: <20181029132640.c3qh7hdew5p2wuun@fantomas.sk>
References: <1540459174727-0.post@n4.nabble.com>
 <ae310bd0-f196-362a-8d0f-9410091b7d34@treenet.co.nz>
 <1540519560433-0.post@n4.nabble.com>
 <582f901ead8e9d3d0dd324e36bc39951@ngtech.co.il>
 <1540538814938-0.post@n4.nabble.com>
 <edf1c4c0-2c7e-ee05-b8b5-a2d1ba40ffb3@treenet.co.nz>
 <1540547103606-0.post@n4.nabble.com> <1540547186478-0.post@n4.nabble.com>
 <1540547386669-0.post@n4.nabble.com> <1540777326629-0.post@n4.nabble.com>
 <20181029132640.c3qh7hdew5p2wuun@fantomas.sk>
Message-ID: <bb233e2c-c06d-444a-3015-06d5182ed2c7@treenet.co.nz>

On 30/10/18 2:26 AM, Matus UHLAR - fantomas wrote:
> On 28.10.18 20:42, Angus J. wrote:
>> oul163:/var/log/squid # ls -rlt
>> total 84
>> -rw-r----- 1 squid squid???? 0 Oct 22 12:21 access.log
>> -rw-r----- 1 squid squid???? 0 Oct 22 13:02 netdb.state
>> -rw-r----- 1 squid squid 17784 Oct 29 09:13 store.log
>> -rw-r----- 1 squid squid 55296 Oct 29 09:33 cache.log
>> oul163:/var/log/squid #
>>
>>
>> access.log is empty
> 
> that means your browser is not using the proxy.
> First configure the browser to use the proxy.
> 

This is a revere-proxy config.

> no, first stop repeatedly posting your squid config and output of "squid -k
> parse", we have seen both multiple times and can find them in archives,
> e.g.
> here:
> 
> http://lists.squid-cache.org/pipermail/squid-users/2018-October/019575.html
> 

To be fair he does keep changing things in the config randomly each time.

Amos


From squid3 at treenet.co.nz  Mon Oct 29 20:50:44 2018
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Tue, 30 Oct 2018 09:50:44 +1300
Subject: [squid-users] Squid proxy not working when upgrade from 27 to
	3.5
In-Reply-To: <1540780015772-0.post@n4.nabble.com>
References: <edf1c4c0-2c7e-ee05-b8b5-a2d1ba40ffb3@treenet.co.nz>
 <1540547103606-0.post@n4.nabble.com> <1540547186478-0.post@n4.nabble.com>
 <1540547386669-0.post@n4.nabble.com>
 <8b3ece11-d04c-9856-67e4-bed0b274735c@treenet.co.nz>
 <1540548491798-0.post@n4.nabble.com>
 <20181026115820.npyeivrstmo35qky@fantomas.sk>
 <1540777583026-0.post@n4.nabble.com> <1540778561463-0.post@n4.nabble.com>
 <1b84fa1a-9f0b-e159-b2c9-468bc313e6ff@treenet.co.nz>
 <1540780015772-0.post@n4.nabble.com>
Message-ID: <3c15ef07-eb2e-fee3-b87d-1444e6c81a60@treenet.co.nz>

On 29/10/18 3:26 PM, Angus J. wrote:
> Hi AMos
> 
> Thanks for your information
> 
> How to enable cache_peer configuration for any TLS/SSL for port 8005
> 

Config line order is important to Squid.

Please *look* at the default squid.conf file provided here:
 <https://wiki.squid-cache.org/SquidFaq/ConfiguringSquid#Squid-3.5_default_config>

Compare it to the layout of your config file.

Notice the line the which says "INSERT YOUR OWN RULE(S) HERE" is _above_
the line which says "http_access deny all". Your config has all its
custom peer rules _below_ the deny line - that difference will be
breaking access to the peers.


You have also added a "#" in front of the cache_peer lines with TLS/SSL
setting for the peers. Use the cache_peer lines you started with.
 ===> Here I mean *exactly* the lines starting with "#cache_peer" and
"cache_peer",   not "cache_peer_domain" or "cache_peer_access".


> and the "allow all" 192.168.0.0 and 172.18.0.0  access permission of custom
> access controls? 

Those IPs are part of localnet and already allowed by your config.

HTH
Amos


From squid3 at treenet.co.nz  Mon Oct 29 21:21:33 2018
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Tue, 30 Oct 2018 10:21:33 +1300
Subject: [squid-users] ERROR The requested URL could not be retrieved
In-Reply-To: <188c5209-d67d-c28b-4702-b28b2d874a6c@borrill.org.uk>
References: <CAHgpA+JZ1NX_JJD2py+YNtRn2-_hmw2Nfw=okAqhmRomP+LxGw@mail.gmail.com>
 <188c5209-d67d-c28b-4702-b28b2d874a6c@borrill.org.uk>
Message-ID: <2e71134a-69f7-fc7b-ad27-6c9b1cd0c7b8@treenet.co.nz>

On 30/10/18 4:23 AM, Stephen Borrill wrote:
> On 29/10/2018 15:20, Uchenna Nebedum wrote:
>> Good Day All,
>> I have setup squid 3.5 with mikrotik, and ssl bumping is enabled. after
>> accepting the certificate on the browser prompt, Squid throws an error
>> on the browser, "*unable to forward this request at this time.*" it
>> throws this error for http sites as well. please what could be causing
>> this error.
> 
> never_direct allow all
> 
> How is your proxy meant to forward on requests? You have no cache peers,
> but have told it never to go direct (i.e. always use a cache peer).
> 
>> *Please find attached my squid.conf*

There are some other issues I can already see which will be coming up
when you resolve the above problem:


>> visible_hostname localhost

Any other proxy calling itself "localhost" will cause forwarding loops.
Either let Squid locate the proxy machines hostname automatically, or
configure a FQDN for the above. The name used should resolve to the
proxy IP when clients look it up in DNS.


>> acl step1 at_step SslBump1
>> acl step2 at_step SslBump2
>> acl step3 at_step SslBump3
>> ssl_bump peek step1 all
>> ssl_bump stare step2 all
>> ssl_bump bump step3 all

The lines above contain other non-splice actions required to always
happen at every step of the SSL-Bumping process.

So these splice lines will never happen:

>> ssl_bump splice localhost
>> ssl_bump splice all


>> via off

Removal of via is only a bandaid to make those forwarding loops created
by visible_hostname not be visible anymore. They can still happen and
annoy other admin elsewhere on the networks your traffic goes to.


>> forwarded_for on
>> request_header_access From deny all
>> request_header_access Cache-Control deny all
>> request_header_access Keep-Alive deny all
>> request_header_access Other deny all

Er, the above *only* affect requests sent to upstream servers.

Removing Cache-Control in particular is definitely going to lead to
major problems for your clients.

"Other" is also tricky. It removes all HTTP headers which Squid has not
explicitly bee coded to understand.

So removing headers with "Other" like this a) breaks any modern HTTP
features your Squid does not explicitly support, and b) lets through
many headers you probably don't want to just because Squid does "know" them.

Keep-Alive is unnecessary since Squid already removes that problematic
header on sight.


>> reply_header_access Set-Cookie deny all
>> reply_header_access Set-Cookie2 deny all
>> reply_header_access Other deny all

>> adaptation_access greasyspoon allow all
>> dns_timeout 30 seconds
>> dns_v4_first on
>> #ecap_enable off
>> icap_enable on
>> icap_preview_enable off
>> icap_preview_size 2048
>> icap_persistent_connections on
>> adaptation_send_client_ip on
>> adaptation_send_username on
>> icap_service greasyspoon respmod_precache icap://127.0.0.1:1344/response
>> bypass=0
>> refresh_pattern ^ftp:??? ??? 1440??? 20%??? 10080
>> refresh_pattern ^gopher:??? 1440??? 0%??? 1440
>> refresh_pattern -i (/cgi-bin/|\?) 0??? 0%??? 0
>> refresh_pattern .??? ??? 0??? 20%??? 4320
>> shutdown_lifetime 10 seconds/
>>
>>

>>
>> please i'll provide any other information required. please i really need
>> help. I noticed my last two questions weren't answered, i really need
>> help. I've noticed google and facebook are reachable.
>>

Meaning traffic to those does not go through the proxy or any of the
ports you are intercepting. Probably via QUIC or similar non-HTTP(S)
protocol.

If you are trying to do those weird header changes for privacy or
anonymity their traffic working is a very bad sign.

Amos


From ajiang at ouhk.edu.hk  Tue Oct 30 03:15:34 2018
From: ajiang at ouhk.edu.hk (Angus J.)
Date: Mon, 29 Oct 2018 22:15:34 -0500 (CDT)
Subject: [squid-users] Squid proxy not working when upgrade from 27 to
	3.5
In-Reply-To: <3c15ef07-eb2e-fee3-b87d-1444e6c81a60@treenet.co.nz>
References: <1540547186478-0.post@n4.nabble.com>
 <1540547386669-0.post@n4.nabble.com>
 <8b3ece11-d04c-9856-67e4-bed0b274735c@treenet.co.nz>
 <1540548491798-0.post@n4.nabble.com>
 <20181026115820.npyeivrstmo35qky@fantomas.sk>
 <1540777583026-0.post@n4.nabble.com> <1540778561463-0.post@n4.nabble.com>
 <1b84fa1a-9f0b-e159-b2c9-468bc313e6ff@treenet.co.nz>
 <1540780015772-0.post@n4.nabble.com>
 <3c15ef07-eb2e-fee3-b87d-1444e6c81a60@treenet.co.nz>
Message-ID: <1540869334203-0.post@n4.nabble.com>

Hi Amos

Can I just copy the all squid configure file at /etc/squid/ from 2.7 to 3.5
?

Regards
Angus



--
Sent from: http://squid-web-proxy-cache.1019090.n4.nabble.com/Squid-Users-f1019091.html


From SIDDH05 at gmail.com  Tue Oct 30 08:36:35 2018
From: SIDDH05 at gmail.com (Sid)
Date: Tue, 30 Oct 2018 03:36:35 -0500 (CDT)
Subject: [squid-users] Squid 4.3: SSL Bump fails to send client certificate
Message-ID: <1540888595659-0.post@n4.nabble.com>

Hi, 

I have following Squid version installed on CentOS 7: 
[root at localhost ~]# squid -v 
Squid Cache: Version 4.3 
Service Name: squid 

This binary uses OpenSSL 1.0.2k-fips  26 Jan 2017. For legal restrictions on
distribution see https://www.openssl.org/source/license.html

configure options:  '--prefix=/usr' '--includedir=/usr/include'
'--datadir=/usr/share' '--bindir=/usr/sbin' '--libexecdir=/usr/lib/squid'
'--localstatedir=/var' '--sysconfdir=/etc/squid' '--with-openssl'
'--enable-ssl-crtd' 

Squid.conf: 
# Recommended minimum configuration: 
# 

# Example rule allowing access from your local networks. 
# Adapt to list your (internal) IP networks from where browsing 
# should be allowed 
#acl localnet src 0.0.0.1-0.255.255.255 # RFC 1122 "this" network (LAN) 
#acl localnet src 10.0.0.0/8            # RFC 1918 local private network
(LAN) 
#acl localnet src 100.64.0.0/10         # RFC 6598 shared address space
(CGN) 
#acl localnet src 169.254.0.0/16        # RFC 3927 link-local (directly
plugged) machines 
#acl localnet src 172.16.0.0/12         # RFC 1918 local private network
(LAN) 
#acl localnet src 10.133.64.0/22                # RFC 1918 local private
network (LAN) 
acl localnet src 20.20.64.0/24 
acl localnet src 192.168.1.0/24 
#acl localnet src 10.133.65.0/22 
#acl localnet src 10.133.66.0/22 
#acl localnet src 10.133.67.0/22 
#acl localnet src fc00::/7              # RFC 4193 local private network
range 
#acl localnet src fe80::/10             # RFC 4291 link-local (directly
plugged) machines 

acl SSL_ports port 443 
acl SSL_ports port 8443 
acl Safe_ports port 80          # http 
acl Safe_ports port 21          # ftp 
acl Safe_ports port 443         # https 
acl Safe_ports port 8443         # https 
acl Safe_ports port 70          # gopher 
acl Safe_ports port 210         # wais 
acl Safe_ports port 1025-65535  # unregistered ports 
acl Safe_ports port 280         # http-mgmt 
acl Safe_ports port 488         # gss-http 
acl Safe_ports port 591         # filemaker 
acl Safe_ports port 777         # multiling http 
acl CONNECT method CONNECT 

# 
# Recommended minimum Access Permission configuration: 
# 
# Deny requests to certain unsafe ports 
http_access deny !Safe_ports 

# Deny CONNECT to other than secure SSL ports 
# Deny CONNECT to other than secure SSL ports 
http_access deny CONNECT !SSL_ports 

# Only allow cachemgr access from localhost 
http_access allow localhost manager 
http_access deny manager 

# We strongly recommend the following be uncommented to protect innocent 
# web applications running on the proxy server who think the only 
# one who can access services on "localhost" is a local user 
#http_access deny to_localhost 

# 
# INSERT YOUR OWN RULE(S) HERE TO ALLOW ACCESS FROM YOUR CLIENTS 
# 

# Example rule allowing access from your local networks. 
# Adapt localnet in the ACL section to list your (internal) IP networks 
# from where browsing should be allowed 
http_access allow localnet 
http_access allow localhost 

# And finally deny all other access to this proxy 
http_access deny all 

# Squid normally listens to port 3128 
#http_port 3128 

http_port 3128 ssl-bump \ 
  cert=/usr/local/squid/etc/ssl_cert/myCA.pem \ 
  generate-host-certificates=on dynamic_cert_mem_cache_size=4MB 

# For squid 3.5.x 
#sslcrtd_program /usr/local/squid/libexec/ssl_crtd -s /var/lib/ssl_db -M 4MB 

# For squid 4.x 
sslcrtd_program /usr/local/squid/libexec/security_file_certgen -s
/var/lib/ssl_db -M 4MB 

acl step1 at_step SslBump1 

ssl_bump peek step1 
ssl_bump bump all 

tls_outgoing_options cafile=/usr/local/squid/etc/UCAppsCA.pem 
sslproxy_foreign_intermediate_certs /usr/local/squid/etc/UCAppsCA.pem 

# Uncomment and adjust the following to add a disk cache directory. 
cache_effective_user squid 
cache_dir ufs /usr/local/squid/var/cache/squid 100 16 256 

# Leave coredumps in the first cache dir 
coredump_dir /usr/local/squid/var/cache/squid 

# 
# Add any of your own refresh_pattern entries above these. 
# 
refresh_pattern ^ftp:           1440    20%     10080 
refresh_pattern ^gopher:        1440    0%      1440 
refresh_pattern -i (/cgi-bin/|\?) 0     0%      0 
refresh_pattern .               0       20%     4320 
xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx
Browser & HTTP UA Client connections are working with SSL bump properly; but
except for one connection. 

Server sends certificate with SNI; but squid forwards it as IP Address to
client 
1540529604.672     49 20.20.64.91 NONE/200 0 CONNECT 20.20.64.56:443 -
HIER_DIRECT/20.20.64.56 - 

When I took wireshark on Squid; I can see Squid sends: 

61 Alert (Level: Fatal, Description: Internal Error) to certificate sent by
Server 

cache.log: 
2018/10/30 12:55:47 kid1| ERROR: negotiating TLS on FD 21:
error:14090086:SSL routines:ssl3_get_server_certificate:certificate verify
failed (1/-1/0) 
2018/10/30 12:55:47 kid1| ERROR: negotiating TLS on FD 21:
error:14090086:SSL routines:ssl3_get_server_certificate:certificate verify
failed (1/-1/0) 

This is an internal server with its own CA certs. 




--
Sent from: http://squid-web-proxy-cache.1019090.n4.nabble.com/Squid-Users-f1019091.html


From fredrik at pipemore.se  Tue Oct 30 11:49:04 2018
From: fredrik at pipemore.se (uppsalanet)
Date: Tue, 30 Oct 2018 06:49:04 -0500 (CDT)
Subject: [squid-users] redirect based on url (302)
In-Reply-To: <a2ca5119-39c1-c744-0e8b-6373f7f525a3@treenet.co.nz>
References: <1537541034108-0.post@n4.nabble.com>
 <dde485b6-5742-d424-8e12-f700d0b975f2@treenet.co.nz>
 <1537771119646-0.post@n4.nabble.com>
 <f35964b6-e83b-c5b6-5563-602ab666d964@treenet.co.nz>
 <1540305071062-0.post@n4.nabble.com>
 <a2ca5119-39c1-c744-0e8b-6373f7f525a3@treenet.co.nz>
Message-ID: <1540900144095-0.post@n4.nabble.com>

Thanks, 
Missed that I need to install squid-helpers "yum install squid-helpers" :-)
Now it's there.

Now I use it like this:

external_acl_type whitelist ttl=60 children-max=1 %SRC %DST
/usr/lib64/squid/ext_sql_session_acl --user root --password config  --table
sessions --cond "" --debug

But receive this:
/2018/10/30 12:38:37.279| 82,9| external_acl.cc(600) aclMatchExternal:
acl="whitelist"
2018/10/30 12:38:37.280| 82,9| external_acl.cc(629) aclMatchExternal: No
helper entry available
2018/10/30 12:38:37.280| 82,2| external_acl.cc(663) aclMatchExternal:
whitelist("130.238.171.59 muse.jhu.edu -") = lookup needed
2018/10/30 12:38:37.280| 82,2| external_acl.cc(667) aclMatchExternal:
"130.238.171.59 muse.jhu.edu -": queueing a call.
2018/10/30 12:38:37.280| 82,2| external_acl.cc(1031) Start: fg lookup in
'whitelist' for '130.238.171.59 muse.jhu.edu -'
2018/10/30 12:38:37.280| 82,4| external_acl.cc(1071) Start:
externalAclLookup: looking up for '130.238.171.59 muse.jhu.edu -' in
'whitelist'.
2018/10/30 12:38:37.280| Starting new whitelist helpers...
2018/10/30 12:38:37.282| 82,4| external_acl.cc(1086) Start:
externalAclLookup: will wait for the result of '130.238.171.59 muse.jhu.edu
-' in 'whitelist' (ch=0x26782c8).
2018/10/30 12:38:37.282| 82,2| external_acl.cc(670) aclMatchExternal:
"130.238.171.59 muse.jhu.edu -": return -1.
Received: Channel=, UID=''
Query: SELECT '' as 'user', '' as 'tag' FROM sessions WHERE (id = ?) UID
queried: ''
Rows: 0
2018/10/30 12:38:37.420| 82,2| external_acl.cc(958) externalAclHandleReply:
reply={result=Unknown, other: "ERR message="unknown UID ''""}/

Looking into the code of ext_sql_session_ac and line 190:
*my ($cid, $uid) = ($1, $2);*

I assume this will split the $_into $cid and $uid. But debug says:
*Received: Channel=, UID=''*
Query: SELECT '' as 'user', '' as 'tag' FROM sessions WHERE (id = ?) UID
queried: ''
Rows: 0

Do I have done something wrong?
/Fredrik






--
Sent from: http://squid-web-proxy-cache.1019090.n4.nabble.com/Squid-Users-f1019091.html


From rousskov at measurement-factory.com  Tue Oct 30 15:50:57 2018
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Tue, 30 Oct 2018 09:50:57 -0600
Subject: [squid-users] Squid 4.3: SSL Bump fails to send client
 certificate
In-Reply-To: <1540888595659-0.post@n4.nabble.com>
References: <1540888595659-0.post@n4.nabble.com>
Message-ID: <6a396e03-e800-3dab-2ea8-97342edff1c7@measurement-factory.com>

On 10/30/18 2:36 AM, Sid wrote:

> http_port 3128 ssl-bump \ 
>   cert=/usr/local/squid/etc/ssl_cert/myCA.pem \ 
>   generate-host-certificates=on dynamic_cert_mem_cache_size=4MB 

> ssl_bump peek step1 
> ssl_bump bump all 

> Browser & HTTP UA Client connections are working with SSL bump properly; but
> except for one connection. 

> Server sends certificate with SNI; but squid forwards it as IP Address to
> client

Two problems with that statement:

1. Servers never send SNI. Clients usually send SNI. Squid should
forward SNI it received from the client to the server, provided the
client actually sent SNI. Did your client send SNI?

2. Bugs notwithstanding, the implied order of events is not what
actually happens: Squid, as configured, does _not_ forward anything from
the server certificate to the client. Squid, as configured, generates a
certificate based on client-supplied information (not server-supplied
information). After sending that generated certificate to the client,
Squid establishes a TLS connection with the server.


> When I took wireshark on Squid; I can see Squid sends: 

For an accurate picture, in addition to Squid-server and server-Squid
traffic, look at what Squid has received from the client and what Squid
sent to the client, all in actual order.


> 61 Alert (Level: Fatal, Description: Internal Error) to certificate sent by
> Server 
> 
> cache.log: 
> 2018/10/30 12:55:47 kid1| ERROR: negotiating TLS on FD 21:
> error:14090086:SSL routines:ssl3_get_server_certificate:certificate verify
> failed (1/-1/0) 
> 2018/10/30 12:55:47 kid1| ERROR: negotiating TLS on FD 21:
> error:14090086:SSL routines:ssl3_get_server_certificate:certificate verify
> failed (1/-1/0) 
> 
> This is an internal server with its own CA certs. 

Is your Squid configured to trust those internal CAs? If not, Squid
would not be able to validate the server certificate.


HTH,

Alex.


From arunabha.saha at gmail.com  Wed Oct 31 00:45:35 2018
From: arunabha.saha at gmail.com (Arunabha Saha)
Date: Tue, 30 Oct 2018 17:45:35 -0700
Subject: [squid-users] Squid 3.5.25 does not recognise ICAP 408 status code
Message-ID: <CABCok=+yJTJs-bmgZe7iCwOALAmMeYvDB4X7D84rRGPgfE+sEw@mail.gmail.com>

Squid 3.5.25 does not seem to recognise the 408 request timeout error code
from ICAP.
The more troublesome issue for me is the exception it generates and then
declares ICAP down after a certain number of such exceptions.

I don't want to disable the failure limit entirely given that we can often
have genuine failures that squid needs to detect.

What i'd like to see is squid should not throw an exception in this case.
 The timeout is somewhat aggressive but works with an earlier version of
ICAP (0.1.x).  The one i'm testing is 0.5.3.

Any suggestions?  I tried looking at squid 3.5.26 through 3.5.28 but i
don't see any support for 408 timeout there (maybe i missed it?).

Debug Logs:

2018/10/29 20:20:25.656| 93,5| ModXact.cc(654) parseMore:
ICAP/1.0 408 Request timeout^M
Server: C-ICAP/0.5.3^M
Connection: close^M
ISTag: CI0001-XXXXXXXXX^M
^M

2018/10/29 20:20:25.656| 93,5| ModXact.cc(749) parseHeaders: parse ICAP
headers
2018/10/29 20:20:25.656| 93,5| ModXact.cc(1079) parseHead: have 98 head
bytes to parse; state: 0
2018/10/29 20:20:25.656| 93,5| ModXact.cc(1094) parseHead: parse success,
consume 98 bytes, return true
2018/10/29 20:20:25.656| 93,5| ModXact.cc(785) parseIcapHead: found
connection close
2018/10/29 20:20:25.656| 93,5| ModXact.cc(815) parseIcapHead: ICAP status
408
2018/10/29 20:20:25.656| 93,3| ../../../src/base/AsyncJobCalls.h(177) dial:
Adaptation::Icap::Xaction::noteCommRead threw exception: Unsupported ICAP
status code
2018/10/29 20:20:25.656| 11,5| HttpRequest.cc(472) detailError: current
error details: 35/396407234
2018/10/29 20:20:25.656| 93,4| Xaction.cc(514) setOutcome: ICAP_ERR_OTHER
2018/10/29 20:20:25.656| 93,4| ServiceRep.cc(80) noteFailure:  failure 4
out of 10 allowed in 0sec [up,fail4]
...
2018/10/29 20:21:06.981| 93,5| ModXact.cc(815) parseIcapHead: ICAP status
408
2018/10/29 20:21:06.981| 93,3| ../../../src/base/AsyncJobCalls.h(177) dial:
Adaptation::Icap::Xaction::noteCommRead threw exception: Unsupported ICAP
status code
2018/10/29 20:21:06.981| 11,5| HttpRequest.cc(472) detailError: current
error details: 35/396407234
2018/10/29 20:21:06.981| 93,4| Xaction.cc(514) setOutcome: ICAP_ERR_OTHER
2018/10/29 20:21:06.981| 93,4| ServiceRep.cc(80) noteFailure:  failure 11
out of 10 allowed in 0sec [up,fail11]
2018/10/29 20:21:06.981| suspending ICAP service for too many failures



-- 
regards,
Arun
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20181030/7bb79fd6/attachment.htm>

From squid3 at treenet.co.nz  Wed Oct 31 01:55:18 2018
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Wed, 31 Oct 2018 14:55:18 +1300
Subject: [squid-users] Squid proxy not working when upgrade from 27 to
	3.5
In-Reply-To: <1540869334203-0.post@n4.nabble.com>
References: <1540547186478-0.post@n4.nabble.com>
 <1540547386669-0.post@n4.nabble.com>
 <8b3ece11-d04c-9856-67e4-bed0b274735c@treenet.co.nz>
 <1540548491798-0.post@n4.nabble.com>
 <20181026115820.npyeivrstmo35qky@fantomas.sk>
 <1540777583026-0.post@n4.nabble.com> <1540778561463-0.post@n4.nabble.com>
 <1b84fa1a-9f0b-e159-b2c9-468bc313e6ff@treenet.co.nz>
 <1540780015772-0.post@n4.nabble.com>
 <3c15ef07-eb2e-fee3-b87d-1444e6c81a60@treenet.co.nz>
 <1540869334203-0.post@n4.nabble.com>
Message-ID: <229c8ded-9841-8551-dc54-c0bf9ee2d28d@treenet.co.nz>

On 30/10/18 4:15 PM, Angus J. wrote:
> Hi Amos
> 
> Can I just copy the all squid configure file at /etc/squid/ from 2.7 to 3.5
> ?
> 

Sometimes, but usually not *just* that.

The problem is that Squid-2.7 very old and also 2.6 & 2.7 was a fork of
the even older Squid-2.5 code. There have been quite a lot of changes to
squid.conf in the 12 years between 2.5 and 3.5, and v2.7 does some
things a bit differently to both.

The squid -k parse mechanism was added to help reduce the problems
encountered with upgrades. So you can have the squid-3 tell you what it
understands about the older config files.

As you should have noticed when I got you to run -k parse earlier there
were things coming up as FATAL and ERROR in your particular config file.
That means Squid will not even start until the config is changed to the
Squid-3.x settings.


FWIW: Marcus and I are trying to get you to the point where your Squid
will run and do what you have told us it is supposed to be doing. The
problems you are having in the latest few days are because you went and
changed other things (the cache_peer lines) beside what we pointed out -
which broken the proxy again in a different way.

It's okay that you don't know what you are doing. This list is in part
for helping people learn to operate Squid. Just try not to go too fast
of more breakage will occur.

Amos


From squid3 at treenet.co.nz  Wed Oct 31 02:07:47 2018
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Wed, 31 Oct 2018 15:07:47 +1300
Subject: [squid-users] ERROR The requested URL could not be retrieved
In-Reply-To: <CAHgpA++u4BZuAqLJKmcqGnc-Bb287Gzb=fQ-C2K+6OQ3_B3ygQ@mail.gmail.com>
References: <CAHgpA+JZ1NX_JJD2py+YNtRn2-_hmw2Nfw=okAqhmRomP+LxGw@mail.gmail.com>
 <188c5209-d67d-c28b-4702-b28b2d874a6c@borrill.org.uk>
 <2e71134a-69f7-fc7b-ad27-6c9b1cd0c7b8@treenet.co.nz>
 <CAHgpA++u4BZuAqLJKmcqGnc-Bb287Gzb=fQ-C2K+6OQ3_B3ygQ@mail.gmail.com>
Message-ID: <e8ba2952-42be-84da-c138-c384f0f7d3e7@treenet.co.nz>

On 31/10/18 1:45 AM, Uchenna Nebedum wrote:
> Thanks a lot it works now... I've added site bumping exceptions, and it
> still throws invalid certificate exceptions even though it uses the
> 'ssl_bump stare' configuration, is it possible to reduce the errors??
> 
> Uchenna Nebedum
> 

Maybe, the above is a bit vague on details.

What exactly do you have configured now after those changes?

And what exact error(s) are you seeing now?


Amos

PS. please reply to the list instead of me personally.

PPS. If you want dedicated support I do provide it commercially, but you
started this on-list so I assume you are not wanting to receive an
invoice for responses.


From squid3 at treenet.co.nz  Wed Oct 31 02:16:17 2018
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Wed, 31 Oct 2018 15:16:17 +1300
Subject: [squid-users] redirect based on url (302)
In-Reply-To: <1540900144095-0.post@n4.nabble.com>
References: <1537541034108-0.post@n4.nabble.com>
 <dde485b6-5742-d424-8e12-f700d0b975f2@treenet.co.nz>
 <1537771119646-0.post@n4.nabble.com>
 <f35964b6-e83b-c5b6-5563-602ab666d964@treenet.co.nz>
 <1540305071062-0.post@n4.nabble.com>
 <a2ca5119-39c1-c744-0e8b-6373f7f525a3@treenet.co.nz>
 <1540900144095-0.post@n4.nabble.com>
Message-ID: <23b5e473-3671-b473-572d-19a25daaefe8@treenet.co.nz>

On 31/10/18 12:49 AM, uppsalanet wrote:
> Thanks, 
> Missed that I need to install squid-helpers "yum install squid-helpers" :-)
> Now it's there.
> 
> Now I use it like this:
> 
> external_acl_type whitelist ttl=60 children-max=1 %SRC %DST
> /usr/lib64/squid/ext_sql_session_acl --user root --password config  --table
> sessions --cond "" --debug
> 
> But receive this:
> /2018/10/30 12:38:37.279| 82,9| external_acl.cc(600) aclMatchExternal:
> acl="whitelist"
> 2018/10/30 12:38:37.280| 82,9| external_acl.cc(629) aclMatchExternal: No
> helper entry available
> 2018/10/30 12:38:37.280| 82,2| external_acl.cc(663) aclMatchExternal:
> whitelist("130.238.171.59 muse.jhu.edu -") = lookup needed
> 2018/10/30 12:38:37.280| 82,2| external_acl.cc(667) aclMatchExternal:
> "130.238.171.59 muse.jhu.edu -": queueing a call.
> 2018/10/30 12:38:37.280| 82,2| external_acl.cc(1031) Start: fg lookup in
> 'whitelist' for '130.238.171.59 muse.jhu.edu -'

Oh darn. Sorry, I forgot about the implicit %DATA parameters on external
ACL yet again. One of the things on my long todo list is to make that
optionally ignored.

For now the easiest fix/workaround is to have your custom helper append
that " -" string to the IDs in the database.

Amos


From rousskov at measurement-factory.com  Wed Oct 31 02:27:57 2018
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Tue, 30 Oct 2018 20:27:57 -0600
Subject: [squid-users] Squid 3.5.25 does not recognise ICAP 408 status
 code
In-Reply-To: <CABCok=+yJTJs-bmgZe7iCwOALAmMeYvDB4X7D84rRGPgfE+sEw@mail.gmail.com>
References: <CABCok=+yJTJs-bmgZe7iCwOALAmMeYvDB4X7D84rRGPgfE+sEw@mail.gmail.com>
Message-ID: <f89571cf-a28e-64a3-472d-16d16ecfc80d@measurement-factory.com>

On 10/30/18 6:45 PM, Arunabha Saha wrote:

> Squid 3.5.25 does not seem to recognise the 408 request timeout error
> code from ICAP.

Squid effectively recognizes ICAP 408 response as an ICAP transaction
error response and blames the ICAP service for that error. That
(minimal) support can be improved, of course. See options #1 and #3 below.

As with any timeout, it is impossible to say in general which side of
the connection is at fault. This case has at least three sides: It could
be the HTTP agent, Squid, and/or the ICAP service. Did one of them stall
the transaction? Or was the ICAP service just too impatient? See option
#4 below.

Needless to say, treating all ICAP service timeouts as if nothing bad
happened would break some existing Squid deployments (while possibly
fixing yours). A proper general solution (option #3 below) would most
likely require making Squid behavior configurable.


> The more troublesome issue for me is the exception it generates and then
> declares ICAP down after a certain number of such exceptions.? ??
> 
> I don't want to disable the failure limit entirely given that we can
> often have genuine failures that squid needs to detect.? ?
> 
> What i'd like to see is squid should not throw an exception in this
> case.

The "exception" is a minor low-level/technical detail. What you really
want to see is Squid blaming itself (rather than the ICAP service) for
the problem. Squid indeed lacks that kind of functionality, but it can
be added if really needed. See options #1 and #3 below.


> The timeout is somewhat aggressive but works with an earlier
> version of ICAP (0.1.x).? The one i'm testing is 0.5.3.

Please note that ICAP is a protocol, not a product/software name. It
probably does not matter what ICAP service you are using though.


> Any suggestions?

I can suggest a few options, in no particular order:

1. Modify your Squid to treat 408 differently.
2. Modify your ICAP service to stop sending ICAP 408 responses to Squid.
3. Add proper ICAP timeout support feature to Squid.
4. Investigate why your ICAP service times out. If you are lucky, you
   may be able to fix or work around the problem by adjusting Squid
   and/or your ICAP service configuration.

For option #1, Adaptation::Icap::ModXact::parseIcapHead() may be a good
starting point.

For options #1 and #3, see also
https://wiki.squid-cache.org/SquidFaq/AboutSquid#How_to_add_a_new_Squid_feature.2C_enhance.2C_of_fix_something.3F

In most cases, option #4 is the best first step but YMMV.


HTH,

Alex.


From SIDDH05 at gmail.com  Wed Oct 31 04:59:18 2018
From: SIDDH05 at gmail.com (Sid)
Date: Tue, 30 Oct 2018 23:59:18 -0500 (CDT)
Subject: [squid-users] Squid 4.3: SSL Bump fails to send client
	certificate
In-Reply-To: <6a396e03-e800-3dab-2ea8-97342edff1c7@measurement-factory.com>
References: <1540888595659-0.post@n4.nabble.com>
 <6a396e03-e800-3dab-2ea8-97342edff1c7@measurement-factory.com>
Message-ID: <1540961958277-0.post@n4.nabble.com>

Thank you Alex for the reply.
 
Alex: 1. Servers never send SNI. Clients usually send SNI. Squid should
forward SNI it received from the client to the server, provided the client
actually sent SNI. Did your client send SNI? 

Sid: I can see in Client Hello IP Address being sent by Client; so there is
no SNI from client itself.

Alex: 2. Bugs notwithstanding, the implied order of events is not what
actually happens: Squid, as configured, does _not_ forward anything from the
server certificate to the client. Squid, as configured, generates a
certificate based on client-supplied information (not server-supplied
information). After sending that generated certificate to the client, Squid
establishes a TLS connection with the server. 

Sid: Thank you for explanation.

Alex: For an accurate picture, in addition to Squid-server and server-Squid
traffic, look at what Squid has received from the client and what Squid sent
to the client, all in actual order.

Sid: I took wireshark on Squid server (centOS 7); I took 2 wiresharks
between Client & Squid and then between Squid & Server. I can see client
being sent fake cert generated by Squid & client responds with "Client key
Exchange", "Change cipher spec", "Encrypted Handshake Message". But I can't
see actual client certificate sent to Squid. Is there a way to decypt in
Wireshark. In Wireshark between Squid & Server I can see Squid responding
with "61 Alert (Level: Fatal, Description: Internal Error)".

Alex: Is your Squid configured to trust those internal CAs? If not, Squid
would not be able to validate the server certificate. 

Sid: I have added those chained certificates as following in squid.conf
tls_outgoing_options cafile=/usr/local/squid/etc/UCAppsCA.pem
sslproxy_foreign_intermediate_certs /usr/local/squid/etc/UCAppsCA.pem




--
Sent from: http://squid-web-proxy-cache.1019090.n4.nabble.com/Squid-Users-f1019091.html


From fredrik at pipemore.se  Wed Oct 31 10:27:20 2018
From: fredrik at pipemore.se (uppsalanet)
Date: Wed, 31 Oct 2018 05:27:20 -0500 (CDT)
Subject: [squid-users] redirect based on url (302)
In-Reply-To: <23b5e473-3671-b473-572d-19a25daaefe8@treenet.co.nz>
References: <1537541034108-0.post@n4.nabble.com>
 <dde485b6-5742-d424-8e12-f700d0b975f2@treenet.co.nz>
 <1537771119646-0.post@n4.nabble.com>
 <f35964b6-e83b-c5b6-5563-602ab666d964@treenet.co.nz>
 <1540305071062-0.post@n4.nabble.com>
 <a2ca5119-39c1-c744-0e8b-6373f7f525a3@treenet.co.nz>
 <1540900144095-0.post@n4.nabble.com>
 <23b5e473-3671-b473-572d-19a25daaefe8@treenet.co.nz>
Message-ID: <1540981640859-0.post@n4.nabble.com>

Hi Amos,
Is there a git that I can use to push stuff up?

I think you need to split the string in an other way, look into this
example:
#!/usr/bin/perl
use strict;
use warnings;

$|=1;
while (<>) {
     my $string = $_;
     print "Received '\$_' = ".$_."\n";       

     $string =~ m/^(\d+)\s(.*)$/;
     print "After regexp '\$string' = ".$string."\n";
     print "After regexp '\$1' = ".$1."\n";           
     print "After regexp '\$2' = ".$2."\n"; 

     ### Original split from sorce ###
     ### This doesn't split anything looks like elements of an array?
     #my ($cid, $uid) = ($1, $2);
    
     ### Split the string ###
     ### Those two split based on one or more spaces
     #my ($cid, $uid) = split(/\s+/ ,$_);
     my ($cid, $uid) = split;
     $cid =~ s/%(..)/pack("H*", $1)/ge;
     $uid =~ s/%(..)/pack("H*", $1)/ge;
     print "After split \$cid = ".$cid."\n";
     print "After split \$uid = ".$uid."\n";
}

Output from above with intake value '*130.238.000.00 muse.jhu.edu -*':
Received '$_' = 130.238.000.00 muse.jhu.edu -
After regexp '$string' = 130.238.000.00 muse.jhu.edu -
/Use of uninitialized value $1 in concatenation (.) or string at
./sed_test_reg.pl line 13, <> line 1.
After regexp '$1' = 
Use of uninitialized value $2 in concatenation (.) or string at
./sed_test_reg.pl line 14, <> line 1.
After regexp '$2' = /
*After split $cid = 130.238.000.00
After split $uid = muse.jhu.edu*

Cheers
Fredrik



--
Sent from: http://squid-web-proxy-cache.1019090.n4.nabble.com/Squid-Users-f1019091.html


From m_zouhairy at skno.by  Wed Oct 31 14:41:59 2018
From: m_zouhairy at skno.by (Vacheslav)
Date: Wed, 31 Oct 2018 17:41:59 +0300
Subject: [squid-users] bank blocked
Message-ID: <952601d47127$e1863800$a492a800$@skno.by>

Peace,

Here is the log ufdbguard:

2018-10-31 17:34:45 [4270] TLSv1.2 certificate for i.bps-sberbank.by:443: UNRECOGNISED ISSUER  (maybe a certificate chain issue)  *****
2018-10-31 17:34:45 [4270]    issuer: /C=US/O=DigiCert Inc/OU=www.digicert.com/CN=GeoTrust RSA CA 2018
2018-10-31 17:34:45 [4270]    subject: /C=BY/L=Minsk/O=BPS-Sberbank OAO/OU=Head Office/CN=*.bps-sberbank.by
2018-10-31 17:34:45 [4270] TLSv1.2 connection to i.bps-sberbank.by:443 has error code 12. It is marked as a TLS/SSL certificate issue
2018-10-31 17:34:45 [4270] BLOCK -                10.17.10.17     config     https-option  i.bps-sberbank.by:443 CONNECT

What is wrong?




From uhlar at fantomas.sk  Wed Oct 31 14:45:51 2018
From: uhlar at fantomas.sk (Matus UHLAR - fantomas)
Date: Wed, 31 Oct 2018 15:45:51 +0100
Subject: [squid-users] bank blocked
In-Reply-To: <952601d47127$e1863800$a492a800$@skno.by>
References: <952601d47127$e1863800$a492a800$@skno.by>
Message-ID: <20181031144551.v25dlsu5ujp27njz@fantomas.sk>

On 31.10.18 17:41, Vacheslav wrote:
>2018-10-31 17:34:45 [4270] TLSv1.2 certificate for i.bps-sberbank.by:443: UNRECOGNISED ISSUER  (maybe a certificate chain issue)  *****
>2018-10-31 17:34:45 [4270]    issuer: /C=US/O=DigiCert Inc/OU=www.digicert.com/CN=GeoTrust RSA CA 2018

does your system recopgnize this authority? Do have actual list of CAs?

>2018-10-31 17:34:45 [4270]    subject: /C=BY/L=Minsk/O=BPS-Sberbank OAO/OU=Head Office/CN=*.bps-sberbank.by
>2018-10-31 17:34:45 [4270] TLSv1.2 connection to i.bps-sberbank.by:443 has error code 12. It is marked as a TLS/SSL certificate issue
>2018-10-31 17:34:45 [4270] BLOCK -                10.17.10.17     config     https-option  i.bps-sberbank.by:443 CONNECT
>
>What is wrong?

-- 
Matus UHLAR - fantomas, uhlar at fantomas.sk ; http://www.fantomas.sk/
Warning: I wish NOT to receive e-mail advertising to this address.
Varovanie: na tuto adresu chcem NEDOSTAVAT akukolvek reklamnu postu.
On the other hand, you have different fingers. 


From m_zouhairy at skno.by  Wed Oct 31 14:48:58 2018
From: m_zouhairy at skno.by (Vacheslav)
Date: Wed, 31 Oct 2018 17:48:58 +0300
Subject: [squid-users] bank blocked
In-Reply-To: <20181031144551.v25dlsu5ujp27njz@fantomas.sk>
References: <952601d47127$e1863800$a492a800$@skno.by>
 <20181031144551.v25dlsu5ujp27njz@fantomas.sk>
Message-ID: <994001d47128$db2b5e30$91821a90$@skno.by>

I do not use bump or splice if that is what you mean. I do not import certificates.. it works without proxy.

-----Original Message-----
From: squid-users <squid-users-bounces at lists.squid-cache.org> On Behalf Of Matus UHLAR - fantomas
Sent: Wednesday, October 31, 2018 5:46 PM
To: squid-users at lists.squid-cache.org
Subject: Re: [squid-users] bank blocked

On 31.10.18 17:41, Vacheslav wrote:
>2018-10-31 17:34:45 [4270] TLSv1.2 certificate for i.bps-sberbank.by:443: UNRECOGNISED ISSUER  (maybe a certificate chain issue)  *****
>2018-10-31 17:34:45 [4270]    issuer: /C=US/O=DigiCert Inc/OU=www.digicert.com/CN=GeoTrust RSA CA 2018

does your system recopgnize this authority? Do have actual list of CAs?

>2018-10-31 17:34:45 [4270]    subject: /C=BY/L=Minsk/O=BPS-Sberbank OAO/OU=Head Office/CN=*.bps-sberbank.by
>2018-10-31 17:34:45 [4270] TLSv1.2 connection to i.bps-sberbank.by:443 has error code 12. It is marked as a TLS/SSL certificate issue
>2018-10-31 17:34:45 [4270] BLOCK -                10.17.10.17     config     https-option  i.bps-sberbank.by:443 CONNECT
>
>What is wrong?

-- 
Matus UHLAR - fantomas, uhlar at fantomas.sk ; http://www.fantomas.sk/
Warning: I wish NOT to receive e-mail advertising to this address.
Varovanie: na tuto adresu chcem NEDOSTAVAT akukolvek reklamnu postu.
On the other hand, you have different fingers. 
_______________________________________________
squid-users mailing list
squid-users at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-users




From marcus.kool at urlfilterdb.com  Wed Oct 31 15:01:09 2018
From: marcus.kool at urlfilterdb.com (Marcus Kool)
Date: Wed, 31 Oct 2018 12:01:09 -0300
Subject: [squid-users] bank blocked
In-Reply-To: <994001d47128$db2b5e30$91821a90$@skno.by>
References: <952601d47127$e1863800$a492a800$@skno.by>
 <20181031144551.v25dlsu5ujp27njz@fantomas.sk>
 <994001d47128$db2b5e30$91821a90$@skno.by>
Message-ID: <8b9b194a-f249-0c96-12a9-971227fa4766@urlfilterdb.com>

When there is an issue with a certificate, it is good practice to go to ssllabs to verify what is going on.

https://www.ssllabs.com/ssltest/analyze.html?d=i.bps%2dsberbank.by&hideResults=on&latest
shows that there is an incomplete certificate chain issue (in orange) which means that the server of the bank does not send all (intermediate) certificates.
Click on the blue '+' of certification paths and it shows that the 'GeoTrust RSA CA 2018' (intermediate certificate) had to be downloaded.

The messages are not from Squid but from ufdbGuard which apparently is configured with an option to block the URL is case of a certificate issue.
Since Squid already checks for valid certificate chains, I suggest to turn this option off in ufdbGuard.

Marcus


On 31/10/2018 11:48, Vacheslav wrote:
> I do not use bump or splice if that is what you mean. I do not import certificates.. it works without proxy.
> 
> -----Original Message-----
> From: squid-users <squid-users-bounces at lists.squid-cache.org> On Behalf Of Matus UHLAR - fantomas
> Sent: Wednesday, October 31, 2018 5:46 PM
> To: squid-users at lists.squid-cache.org
> Subject: Re: [squid-users] bank blocked
> 
> On 31.10.18 17:41, Vacheslav wrote:
>> 2018-10-31 17:34:45 [4270] TLSv1.2 certificate for i.bps-sberbank.by:443: UNRECOGNISED ISSUER  (maybe a certificate chain issue)  *****
>> 2018-10-31 17:34:45 [4270]    issuer: /C=US/O=DigiCert Inc/OU=www.digicert.com/CN=GeoTrust RSA CA 2018
> 
> does your system recopgnize this authority? Do have actual list of CAs?
> 
>> 2018-10-31 17:34:45 [4270]    subject: /C=BY/L=Minsk/O=BPS-Sberbank OAO/OU=Head Office/CN=*.bps-sberbank.by
>> 2018-10-31 17:34:45 [4270] TLSv1.2 connection to i.bps-sberbank.by:443 has error code 12. It is marked as a TLS/SSL certificate issue
>> 2018-10-31 17:34:45 [4270] BLOCK -                10.17.10.17     config     https-option  i.bps-sberbank.by:443 CONNECT
>>
>> What is wrong?
> 


From nebeduch at gmail.com  Wed Oct 31 15:08:11 2018
From: nebeduch at gmail.com (Uchenna Nebedum)
Date: Wed, 31 Oct 2018 16:08:11 +0100
Subject: [squid-users] ERROR The requested URL could not be retrieved
In-Reply-To: <e8ba2952-42be-84da-c138-c384f0f7d3e7@treenet.co.nz>
References: <CAHgpA+JZ1NX_JJD2py+YNtRn2-_hmw2Nfw=okAqhmRomP+LxGw@mail.gmail.com>
 <188c5209-d67d-c28b-4702-b28b2d874a6c@borrill.org.uk>
 <2e71134a-69f7-fc7b-ad27-6c9b1cd0c7b8@treenet.co.nz>
 <CAHgpA++u4BZuAqLJKmcqGnc-Bb287Gzb=fQ-C2K+6OQ3_B3ygQ@mail.gmail.com>
 <e8ba2952-42be-84da-c138-c384f0f7d3e7@treenet.co.nz>
Message-ID: <CAHgpA+LeHwM9p4C3hsATBuHqw4rtJM8WiSZwg1_E643DJ+puwQ@mail.gmail.com>

Thanks a lot Amos, I really didn't notice I had been sending private
emails, Really sorry about that.

About the config, The proxy works fine now, it bumps the traffic
successfully.
I've added the sites i want to be bumped but the browser errors thrown are
too much, and it's a scenario where I can't install the certificate on
every device.

So i wanted to know if there was a way to reduce the privacy errors. thanks
a lot.

Uchenna Nebedum

On Wed, Oct 31, 2018, 03:07 Amos Jeffries <squid3 at treenet.co.nz> wrote:

> On 31/10/18 1:45 AM, Uchenna Nebedum wrote:
> > Thanks a lot it works now... I've added site bumping exceptions, and it
> > still throws invalid certificate exceptions even though it uses the
> > 'ssl_bump stare' configuration, is it possible to reduce the errors?
> >
> > Uchenna Nebedum
> >
>
> Maybe, the above is a bit vague on details.
>
> What exactly do you have configured now after those changes?
>
> And what exact error(s) are you seeing now?
>
>
> Amos
>
> PS. please reply to the list instead of me personally.
>
> PPS. If you want dedicated support I do provide it commercially, but you
> started this on-list so I assume you are not wanting to receive an
> invoice for responses.
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20181031/8a74ed6d/attachment.htm>

From arunabha.saha at gmail.com  Wed Oct 31 17:20:00 2018
From: arunabha.saha at gmail.com (Arunabha Saha)
Date: Wed, 31 Oct 2018 10:20:00 -0700
Subject: [squid-users] Squid 3.5.25 does not recognise ICAP 408 status
	code
In-Reply-To: <mailman.3.1540987202.15561.squid-users@lists.squid-cache.org>
References: <mailman.3.1540987202.15561.squid-users@lists.squid-cache.org>
Message-ID: <CABCok=LTQvxAkgoQT8bU2YQ7SNbTZN2OHzAj9SL2Ex9GV0QMyQ@mail.gmail.com>

>As with any timeout, it is impossible to say in general which side of
>the connection is at fault. This case has at least three sides: It could
>be the HTTP agent, Squid, and/or the ICAP service. Did one of them stall
>the transaction? Or was the ICAP service just too impatient? See option
>#4 below.
I've tried to track this down.   There are some  persistent sockets used by
SaaS apps for APIs (otservice api from google sites) and sometimes the HTTP
response takes a long time to trickle in.  I have seen upto 25 seconds for
the response body to trickle in after the response header.   I don't know
yet if this is due to network delays but given that it happens only for
this particular uri I'm theorizing that this is how it works.    I can
whitelist the one i am aware of that is causing this issue but again the
concern is what about others that might throw this exception.    The
timeout at 10 seconds is somewhat aggressive so moving that up should help
but some code changes in either squid or icap as suggested look necessary.

>Please note that ICAP is a protocol, not a product/software name. It
>probably does not matter what ICAP service you are using though.

Correct.  I was referring to the c-icap implementation.

On Wed, Oct 31, 2018 at 5:00 AM <squid-users-request at lists.squid-cache.org>
wrote:

> Send squid-users mailing list submissions to
>         squid-users at lists.squid-cache.org
>
> To subscribe or unsubscribe via the World Wide Web, visit
>         http://lists.squid-cache.org/listinfo/squid-users
> or, via email, send a message with subject or body 'help' to
>         squid-users-request at lists.squid-cache.org
>
> You can reach the person managing the list at
>         squid-users-owner at lists.squid-cache.org
>
> When replying, please edit your Subject line so it is more specific
> than "Re: Contents of squid-users digest..."
>
>
> Today's Topics:
>
>    1.
> On 10/30/18 6:45 PM, Arunabha Saha wrote:
>
> > Squid 3.5.25 does not seem to recognise the 408 request timeout error
> > code from ICAP.
>
> Squid effectively recognizes ICAP 408 response as an ICAP transaction
> error response and blames the ICAP service for that error. That
> (minimal) support can be improved, of course. See options #1 and #3 below.
>
>
>

>
> Needless to say, treating all ICAP service timeouts as if nothing bad
> happened would break some existing Squid deployments (while possibly
> fixing yours). A proper general solution (option #3 below) would most
> likely require making Squid behavior configurable.
>
>
> > The more troublesome issue for me is the exception it generates and then
> > declares ICAP down after a certain number of such exceptions.
> >
> > I don't want to disable the failure limit entirely given that we can
> > often have genuine failures that squid needs to detect.
> >
> > What i'd like to see is squid should not throw an exception in this
> > case.
>
> The "exception" is a minor low-level/technical detail. What you really
> want to see is Squid blaming itself (rather than the ICAP service) for
> the problem. Squid indeed lacks that kind of functionality, but it can
> be added if really needed. See options #1 and #3 below.
>
>
> > The timeout is somewhat aggressive but works with an earlier
> > version of ICAP (0.1.x).  The one i'm testing is 0.5.3.
>
> Please note that ICAP is a protocol, not a product/software name. It
> probably does not matter what ICAP service you are using though.
>
>
> > Any suggestions?
>
> I can suggest a few options, in no particular order:
>
> 1. Modify your Squid to treat 408 differently.
> 2. Modify your ICAP service to stop sending ICAP 408 responses to Squid.
> 3. Add proper ICAP timeout support feature to Squid.
> 4. Investigate why your ICAP service times out. If you are lucky, you
>    may be able to fix or work around the problem by adjusting Squid
>    and/or your ICAP service configuration.
>
> For option #1, Adaptation::Icap::ModXact::parseIcapHead() may be a good
> starting point.
>
> For options #1 and #3, see also
>
> https://wiki.squid-cache.org/SquidFaq/AboutSquid#How_to_add_a_new_Squid_feature.2C_enhance.2C_of_fix_something.3F
>
> In most cases, option #4 is the best first step but YMMV.
>
>
> HTH,
>
> Alex.
>
>
> ------------------------------
>
> Message: 2
> Date: Tue, 30 Oct 2018 23:59:18 -0500 (CDT)
> From: Sid <SIDDH05 at gmail.com>
> To: squid-users at lists.squid-cache.org
> Subject: Re: [squid-users] Squid 4.3: SSL Bump fails to send client
>         certificate
> Message-ID: <1540961958277-0.post at n4.nabble.com>
> Content-Type: text/plain; charset=us-ascii
>
> Thank you Alex for the reply.
>
> Alex: 1. Servers never send SNI. Clients usually send SNI. Squid should
> forward SNI it received from the client to the server, provided the client
> actually sent SNI. Did your client send SNI?
>
> Sid: I can see in Client Hello IP Address being sent by Client; so there is
> no SNI from client itself.
>
> Alex: 2. Bugs notwithstanding, the implied order of events is not what
> actually happens: Squid, as configured, does _not_ forward anything from
> the
> server certificate to the client. Squid, as configured, generates a
> certificate based on client-supplied information (not server-supplied
> information). After sending that generated certificate to the client, Squid
> establishes a TLS connection with the server.
>
> Sid: Thank you for explanation.
>
> Alex: For an accurate picture, in addition to Squid-server and server-Squid
> traffic, look at what Squid has received from the client and what Squid
> sent
> to the client, all in actual order.
>
> Sid: I took wireshark on Squid server (centOS 7); I took 2 wiresharks
> between Client & Squid and then between Squid & Server. I can see client
> being sent fake cert generated by Squid & client responds with "Client key
> Exchange", "Change cipher spec", "Encrypted Handshake Message". But I can't
> see actual client certificate sent to Squid. Is there a way to decypt in
> Wireshark. In Wireshark between Squid & Server I can see Squid responding
> with "61 Alert (Level: Fatal, Description: Internal Error)".
>
> Alex: Is your Squid configured to trust those internal CAs? If not, Squid
> would not be able to validate the server certificate.
>
> Sid: I have added those chained certificates as following in squid.conf
> tls_outgoing_options cafile=/usr/local/squid/etc/UCAppsCA.pem
> sslproxy_foreign_intermediate_certs /usr/local/squid/etc/UCAppsCA.pem
>
>
>
>
> --
> Sent from:
> http://squid-web-proxy-cache.1019090.n4.nabble.com/Squid-Users-f1019091.html
>
>
> ------------------------------
>
> Message: 3
> Date: Wed, 31 Oct 2018 05:27:20 -0500 (CDT)
> From: uppsalanet <fredrik at pipemore.se>
> To: squid-users at lists.squid-cache.org
> Subject: Re: [squid-users] redirect based on url (302)
> Message-ID: <1540981640859-0.post at n4.nabble.com>
> Content-Type: text/plain; charset=us-ascii
>
> Hi Amos,
> Is there a git that I can use to push stuff up?
>
> I think you need to split the string in an other way, look into this
> example:
> #!/usr/bin/perl
> use strict;
> use warnings;
>
> $|=1;
> while (<>) {
>      my $string = $_;
>      print "Received '\$_' = ".$_."\n";
>
>      $string =~ m/^(\d+)\s(.*)$/;
>      print "After regexp '\$string' = ".$string."\n";
>      print "After regexp '\$1' = ".$1."\n";
>      print "After regexp '\$2' = ".$2."\n";
>
>      ### Original split from sorce ###
>      ### This doesn't split anything looks like elements of an array?
>      #my ($cid, $uid) = ($1, $2);
>
>      ### Split the string ###
>      ### Those two split based on one or more spaces
>      #my ($cid, $uid) = split(/\s+/ ,$_);
>      my ($cid, $uid) = split;
>      $cid =~ s/%(..)/pack("H*", $1)/ge;
>      $uid =~ s/%(..)/pack("H*", $1)/ge;
>      print "After split \$cid = ".$cid."\n";
>      print "After split \$uid = ".$uid."\n";
> }
>
> Output from above with intake value '*130.238.000.00 muse.jhu.edu -*':
> Received '$_' = 130.238.000.00 muse.jhu.edu -
> After regexp '$string' = 130.238.000.00 muse.jhu.edu -
> /Use of uninitialized value $1 in concatenation (.) or string at
> ./sed_test_reg.pl line 13, <> line 1.
> After regexp '$1' =
> Use of uninitialized value $2 in concatenation (.) or string at
> ./sed_test_reg.pl line 14, <> line 1.
> After regexp '$2' = /
> *After split $cid = 130.238.000.00
> After split $uid = muse.jhu.edu*
>
> Cheers
> Fredrik
>
>
>
> --
> Sent from:
> http://squid-web-proxy-cache.1019090.n4.nabble.com/Squid-Users-f1019091.html
>
>
> ------------------------------
>
> Subject: Digest Footer
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>
>
> ------------------------------
>
> End of squid-users Digest, Vol 50, Issue 75
> *******************************************
>


-- 
regards,
Arun
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20181031/4f86ec84/attachment.htm>

From rafael.akchurin at diladele.com  Wed Oct 31 17:23:04 2018
From: rafael.akchurin at diladele.com (Rafael Akchurin)
Date: Wed, 31 Oct 2018 17:23:04 +0000
Subject: [squid-users] Ubuntu 18 LTS repository for Squid 4.4 (rebuilt with
 sslbump support from sources in Debian unstable)
Message-ID: <AM0PR04MB4753B39F948558DB3C372AEC8FCD0@AM0PR04MB4753.eurprd04.prod.outlook.com>

Greeting all,

The online repository with latest Squid 4.4 (rebuilt from Debian unstable with sslbump support) for Ubuntu 18 LTS 64-bit is available at squid44.diladele.com. Github repo at https://github.com/diladele/squid-ubuntu contains the scripts we used to make this compilation.

Hope you will find this helpful. Note that older repo of squid43.diladele.com will be taken down in two weeks.

Best regards,
Rafael Akchurin
Diladele B.V.

P.S. Here are simple instructions how to use the repo. For more information see readme at https://github.com/diladele/squid-ubuntu .

# add diladele apt key
wget -qO - http://packages.diladele.com/diladele_pub.asc | sudo apt-key add -

# add repo
echo "deb http://squid44.diladele.com/ubuntu/ bionic main" > /etc/apt/sources.list.d/squid44.diladele.com.list

# update the apt cache
apt-get update

# install
apt-get install squid-common
apt-get install squid
apt-get install squidclient



-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20181031/b22a9bab/attachment.htm>

From irlapati at gmail.com  Wed Oct 31 18:54:29 2018
From: irlapati at gmail.com (S Irlapati)
Date: Wed, 31 Oct 2018 13:54:29 -0500
Subject: [squid-users] Ubuntu 18 LTS repository for Squid 4.4 (rebuilt
 with sslbump support from sources in Debian unstable)
In-Reply-To: <AM0PR04MB4753B39F948558DB3C372AEC8FCD0@AM0PR04MB4753.eurprd04.prod.outlook.com>
References: <AM0PR04MB4753B39F948558DB3C372AEC8FCD0@AM0PR04MB4753.eurprd04.prod.outlook.com>
Message-ID: <04b440cd-3d0c-a119-e03b-ba18c0b7f8e6@gmail.com>

Is it possible to make a docker image for this?

On 10/31/18 12:23 PM, Rafael Akchurin wrote:
>
> Greeting all,
>
> The online repository with latest Squid 4.4 (rebuilt from Debian 
> unstable with sslbump support) for Ubuntu 18 LTS 64-bit is available 
> at squid44.diladele.com. Github repo at 
> https://github.com/diladele/squid-ubuntu contains the scripts we used 
> to make this compilation.
>
> Hope you will find this helpful. Note that older repo of 
> squid43.diladele.com will be taken down in two weeks.
>
> Best regards,
>
> Rafael Akchurin
>
> Diladele B.V.
>
> P.S. Here are simple instructions how to use the repo. For more 
> information see readme at https://github.com/diladele/squid-ubuntu .
>
> # add diladele apt key
>
> wget -qO - http://packages.diladele.com/diladele_pub.asc | sudo 
> apt-key add -
>
> # add repo
>
> echo "deb http://squid44.diladele.com/ubuntu/ 
> <http://squid44.diladele.com/ubuntu/> bionic main" > 
> /etc/apt/sources.list.d/squid44.diladele.com.list
>
> # update the apt cache
>
> apt-get update
>
> # install
>
> apt-get install squid-common
>
> apt-get install squid
>
> apt-get install squidclient
>
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20181031/3140b15c/attachment.htm>

From rafael.akchurin at diladele.com  Wed Oct 31 19:00:26 2018
From: rafael.akchurin at diladele.com (Rafael Akchurin)
Date: Wed, 31 Oct 2018 19:00:26 +0000
Subject: [squid-users] Ubuntu 18 LTS repository for Squid 4.4 (rebuilt
 with sslbump support from sources in Debian unstable)
In-Reply-To: <04b440cd-3d0c-a119-e03b-ba18c0b7f8e6@gmail.com>
References: <AM0PR04MB4753B39F948558DB3C372AEC8FCD0@AM0PR04MB4753.eurprd04.prod.outlook.com>
 <04b440cd-3d0c-a119-e03b-ba18c0b7f8e6@gmail.com>
Message-ID: <AM0PR04MB475368CCE8C37D818CB0827B8FCD0@AM0PR04MB4753.eurprd04.prod.outlook.com>

Hello Samuel,

Yes will make the Docker when 7.0 is in beta stage, now we need to polish it a little.
Added issue at https://github.com/diladele/websafety-issues/issues/1030

Best regards,
Rafael Akchurin
Diladele B.V.

From: squid-users <squid-users-bounces at lists.squid-cache.org> On Behalf Of S Irlapati
Sent: Wednesday, 31 October 2018 19:54
To: squid-users at lists.squid-cache.org
Subject: Re: [squid-users] Ubuntu 18 LTS repository for Squid 4.4 (rebuilt with sslbump support from sources in Debian unstable)


Is it possible to make a docker image for this?
On 10/31/18 12:23 PM, Rafael Akchurin wrote:
Greeting all,

The online repository with latest Squid 4.4 (rebuilt from Debian unstable with sslbump support) for Ubuntu 18 LTS 64-bit is available at squid44.diladele.com. Github repo at https://github.com/diladele/squid-ubuntu contains the scripts we used to make this compilation.

Hope you will find this helpful. Note that older repo of squid43.diladele.com will be taken down in two weeks.

Best regards,
Rafael Akchurin
Diladele B.V.

P.S. Here are simple instructions how to use the repo. For more information see readme at https://github.com/diladele/squid-ubuntu .

# add diladele apt key
wget -qO - http://packages.diladele.com/diladele_pub.asc | sudo apt-key add -

# add repo
echo "deb http://squid44.diladele.com/ubuntu/ bionic main" > /etc/apt/sources.list.d/squid44.diladele.com.list

# update the apt cache
apt-get update

# install
apt-get install squid-common
apt-get install squid
apt-get install squidclient






_______________________________________________

squid-users mailing list

squid-users at lists.squid-cache.org<mailto:squid-users at lists.squid-cache.org>

http://lists.squid-cache.org/listinfo/squid-users
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20181031/bdb22b19/attachment.htm>

From rousskov at measurement-factory.com  Wed Oct 31 19:37:13 2018
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Wed, 31 Oct 2018 13:37:13 -0600
Subject: [squid-users] Squid 3.5.25 does not recognise ICAP 408 status
 code
In-Reply-To: <CABCok=LTQvxAkgoQT8bU2YQ7SNbTZN2OHzAj9SL2Ex9GV0QMyQ@mail.gmail.com>
References: <mailman.3.1540987202.15561.squid-users@lists.squid-cache.org>
 <CABCok=LTQvxAkgoQT8bU2YQ7SNbTZN2OHzAj9SL2Ex9GV0QMyQ@mail.gmail.com>
Message-ID: <03668bc9-085e-6bd9-599f-602af2d7432b@measurement-factory.com>

On 10/31/18 11:20 AM, Arunabha Saha wrote:
>>As with any timeout, it is impossible to say in general which side of
>>the connection is at fault. This case has at least three sides: It could
>>be the HTTP agent, Squid, and/or the ICAP service. Did one of them stall
>>the transaction? Or was the ICAP service just too impatient? See option
>>#4 below.

> I've tried to track this down.? ?There are some? persistent sockets used
> by SaaS apps for APIs (otservice api from google sites) and sometimes
> the HTTP response takes a long time to trickle in.? I have seen upto 25
> seconds for the response body to trickle in after the response header.

Glad you found the culprit!

?
> The timeout at 10 seconds is somewhat aggressive so moving
> that up should help but some code changes in either squid or icap as
> suggested look necessary.

If you do not want to modify Squid, then the ICAP service should tell
Squid what to do with the timed out HTTP transaction (e.g., respond with
ICAP 204 or ICAP 200) instead of telling Squid that the service does not
know what to do (i.e. respond with ICAP 408). When the service does not
know what to do, Squid does not know what to do either, resulting in
transaction errors (that Squid blames the ICAP service for).

Please note that teaching Squid about the special meaning of ICAP 408
(Timeout) responses is not enough to address the problem -- Squid would
still need to know what to do with the HTTP transaction (e.g. block it,
pass through as is, or abort) and with the service (count a service
failure or ignore the timeout). Those decisions are likely to be
different for different admins/deployments.

Alex.

> I was referring to the c-icap implementation.

>     On 10/30/18 6:45 PM, Arunabha Saha wrote:
> 
>     > Squid 3.5.25 does not seem to recognise the 408 request timeout error
>     > code from ICAP.
> 
>     Squid effectively recognizes ICAP 408 response as an ICAP transaction
>     error response and blames the ICAP service for that error. That
>     (minimal) support can be improved, of course. See options #1 and #3
>     below.
> 
> 
> ?
> 
> 
>     Needless to say, treating all ICAP service timeouts as if nothing bad
>     happened would break some existing Squid deployments (while possibly
>     fixing yours). A proper general solution (option #3 below) would most
>     likely require making Squid behavior configurable.
> 
> 
>     > The more troublesome issue for me is the exception it generates
>     and then
>     > declares ICAP down after a certain number of such exceptions.? ??
>     >
>     > I don't want to disable the failure limit entirely given that we can
>     > often have genuine failures that squid needs to detect.? ?
>     >
>     > What i'd like to see is squid should not throw an exception in this
>     > case.
> 
>     The "exception" is a minor low-level/technical detail. What you really
>     want to see is Squid blaming itself (rather than the ICAP service) for
>     the problem. Squid indeed lacks that kind of functionality, but it can
>     be added if really needed. See options #1 and #3 below.
> 
> 
>     > The timeout is somewhat aggressive but works with an earlier
>     > version of ICAP (0.1.x).? The one i'm testing is 0.5.3.
> 
>     Please note that ICAP is a protocol, not a product/software name. It
>     probably does not matter what ICAP service you are using though.
> 
> 
>     > Any suggestions?
> 
>     I can suggest a few options, in no particular order:
> 
>     1. Modify your Squid to treat 408 differently.
>     2. Modify your ICAP service to stop sending ICAP 408 responses to Squid.
>     3. Add proper ICAP timeout support feature to Squid.
>     4. Investigate why your ICAP service times out. If you are lucky, you
>     ? ?may be able to fix or work around the problem by adjusting Squid
>     ? ?and/or your ICAP service configuration.
> 
>     For option #1, Adaptation::Icap::ModXact::parseIcapHead() may be a good
>     starting point.
> 
>     For options #1 and #3, see also
>     https://wiki.squid-cache.org/SquidFaq/AboutSquid#How_to_add_a_new_Squid_feature.2C_enhance.2C_of_fix_something.3F
> 
>     In most cases, option #4 is the best first step but YMMV.
> 
> 
>     HTH,
> 
>     Alex.
> 


From rousskov at measurement-factory.com  Wed Oct 31 21:38:44 2018
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Wed, 31 Oct 2018 15:38:44 -0600
Subject: [squid-users] Squid 4.3: SSL Bump fails to send client
 certificate
In-Reply-To: <1540961958277-0.post@n4.nabble.com>
References: <1540888595659-0.post@n4.nabble.com>
 <6a396e03-e800-3dab-2ea8-97342edff1c7@measurement-factory.com>
 <1540961958277-0.post@n4.nabble.com>
Message-ID: <407e2dc2-0473-46ed-469f-e2d8da2d0326@measurement-factory.com>

On 10/30/18 10:59 PM, Sid wrote:

> Sid: I took wireshark on Squid server (centOS 7); I took 2 wiresharks
> between Client & Squid and then between Squid & Server. I can see client
> being sent fake cert generated by Squid & client responds with "Client key
> Exchange", "Change cipher spec", "Encrypted Handshake Message". 

Sounds good. Does the generated fake certificate contain the right
origin server name?


> But I can't see actual client certificate sent to Squid.

Why do you expect the client to send a client certificate to Squid? In
most deployments, TLS servers do not request client certificates and,
hence, TLS clients do not send client certificates. IIRC, you did not
configure your Squid to request a client certificate from the client?

Or is there a terminology problem where "client certificate sent to
Squid" means something other than "an x509 certificate requested by a
TLS server and sent to that server by a TLS client during TLS
handshake"? Please note that Squid is a TLS server in this context.


> Is there a way to decypt in Wireshark. 

Yes, there are several ways, including giving Wireshark your Squid's
private certificate key. Sorry, I do not have detailed instructions.
Please note that the encrypted part probably does not matter -- in most
cases prior to TLS v1.3, it is the plain text Hellos that are important
when it comes to bumping the connection.


> In Wireshark between Squid & Server I can see Squid responding
> with "61 Alert (Level: Fatal, Description: Internal Error)".

> Alex: Is your Squid configured to trust those internal CAs? If not, Squid
> would not be able to validate the server certificate. 

> Sid: I have added those chained certificates as following in squid.conf
> tls_outgoing_options cafile=/usr/local/squid/etc/UCAppsCA.pem
> sslproxy_foreign_intermediate_certs /usr/local/squid/etc/UCAppsCA.pem

Perhaps the alert may not be related to certificate validation. If you
want to verify whether UCAppsCA.pem is enough to trust the origin
server, you can use "curl" or "openssl s_client" tools for a test. They
should fail to validate the server when not configured to use
UCAppsCA.pem and they should succeed otherwise.


HTH,

Alex.


