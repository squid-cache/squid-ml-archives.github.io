From david at articatech.com  Sat Sep  1 09:33:49 2018
From: david at articatech.com (David Touzeau)
Date: Sat, 1 Sep 2018 11:33:49 +0200
Subject: [squid-users] Transparent vs Tproxy: performance ?
Message-ID: <00ce01d441d6$e3af72c0$ab0e5840$@articatech.com>

Hi 

 

We have 2 ways to make the squid in < transparent mode. > 

 

The standard Transparent method and (with modern kernels)  the use of <
Tproxy > method

 

I would like to know which is the best according to the performance ?

 

Or is it the same ?

 

Best regards.

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20180901/cda489eb/attachment.htm>

From squid3 at treenet.co.nz  Sat Sep  1 15:07:02 2018
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sun, 2 Sep 2018 03:07:02 +1200
Subject: [squid-users] Transparent vs Tproxy: performance ?
In-Reply-To: <00ce01d441d6$e3af72c0$ab0e5840$@articatech.com>
References: <00ce01d441d6$e3af72c0$ab0e5840$@articatech.com>
Message-ID: <507029af-de71-eedb-306b-cb384130bb77@treenet.co.nz>

On 1/09/18 9:33 PM, David Touzeau wrote:
> Hi
> 
> We have 2 ways to make the squid in ??transparent mode.??
> 
> The standard Transparent method and (with modern kernels) ?the use of
> ??Tproxy?? method
> 

Please clarify what this "standard transparent" thing is you referring to?

I suspect that you actually mean "NAT" which is completely separate from
Squid and thus has no bearing on proxy performance.



> I would like to know which is the best according to the performance??
> 

This is a meaningless question. "comparing apples to oranges", etc.

You might as well ask if NAT is faster or slower than packet flow?


Both NAT and TPROXY involve kernel managing tables of active connections
and syscalls by Squid to search those tables on every accept(). Only the
timing of those syscalls and the state listed in the tables differ. The
limitations each imposes are more relevant than performance differences.

Specifically;

* TPROXY restricts the TCP ports available to clients to 31K, where
normally they are 63K.

* NAT systems restrict ports to (63*M)/N where N is number of clients on
the network, and M the number of IPs available to Squid outbound
(usually 1).

As you can see those will impose a cap on both performance and
capability of your network. How much is determined by your network size
and traffic peak flows. Not by anything related to Squid.


Squid performance should be essentially the same for all traffic
"modes". It is driven by the HTTP features used in the messages
happening, combined with what types of processing your config requires
to be done on those messages.
So by crafting the very extreme types of message one can flood a Gbps
network with a single HTTP request, or pass thousands of transactions
quickly over a 56Kbps modem link.

Amos


From david at articatech.com  Sun Sep  2 10:13:16 2018
From: david at articatech.com (David Touzeau)
Date: Sun, 2 Sep 2018 12:13:16 +0200
Subject: [squid-users] Transparent vs Tproxy: performance ?
In-Reply-To: <507029af-de71-eedb-306b-cb384130bb77@treenet.co.nz>
References: <00ce01d441d6$e3af72c0$ab0e5840$@articatech.com>
 <507029af-de71-eedb-306b-cb384130bb77@treenet.co.nz>
Message-ID: <004801d442a5$90ec9d90$b2c5d8b0$@articatech.com>

Thanks Amos, 

Yes my question was " if NAT is faster or slower than packet flow" and yes you are right. 

"Squid is not impacted to this question, this make sense."


I had a "feeling" (human sensation) - with 3.000 users that NAT was faster than Tproxy...

But you confirm that this is not relevant...

Best regards,


-----Message d'origine-----
De : squid-users <squid-users-bounces at lists.squid-cache.org> De la part de Amos Jeffries
Envoy? : samedi 1 septembre 2018 17:07
? : squid-users at lists.squid-cache.org
Objet : Re: [squid-users] Transparent vs Tproxy: performance ?

On 1/09/18 9:33 PM, David Touzeau wrote:
> Hi
> 
> We have 2 ways to make the squid in ? transparent mode. ?
> 
> The standard Transparent method and (with modern kernels)  the use of 
> ? Tproxy ? method
> 

Please clarify what this "standard transparent" thing is you referring to?

I suspect that you actually mean "NAT" which is completely separate from Squid and thus has no bearing on proxy performance.



> I would like to know which is the best according to the performance ?
> 

This is a meaningless question. "comparing apples to oranges", etc.

You might as well ask if NAT is faster or slower than packet flow?


Both NAT and TPROXY involve kernel managing tables of active connections and syscalls by Squid to search those tables on every accept(). Only the timing of those syscalls and the state listed in the tables differ. The limitations each imposes are more relevant than performance differences.

Specifically;

* TPROXY restricts the TCP ports available to clients to 31K, where normally they are 63K.

* NAT systems restrict ports to (63*M)/N where N is number of clients on the network, and M the number of IPs available to Squid outbound (usually 1).

As you can see those will impose a cap on both performance and capability of your network. How much is determined by your network size and traffic peak flows. Not by anything related to Squid.


Squid performance should be essentially the same for all traffic "modes". It is driven by the HTTP features used in the messages happening, combined with what types of processing your config requires to be done on those messages.
So by crafting the very extreme types of message one can flood a Gbps network with a single HTTP request, or pass thousands of transactions quickly over a 56Kbps modem link.

Amos
_______________________________________________
squid-users mailing list
squid-users at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-users



From squid3 at treenet.co.nz  Sun Sep  2 12:12:16 2018
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Mon, 3 Sep 2018 00:12:16 +1200
Subject: [squid-users] Transparent vs Tproxy: performance ?
In-Reply-To: <004801d442a5$90ec9d90$b2c5d8b0$@articatech.com>
References: <00ce01d441d6$e3af72c0$ab0e5840$@articatech.com>
 <507029af-de71-eedb-306b-cb384130bb77@treenet.co.nz>
 <004801d442a5$90ec9d90$b2c5d8b0$@articatech.com>
Message-ID: <56bbbc9e-abe7-0cd2-75a2-1985b4f64538@treenet.co.nz>

On 2/09/18 10:13 PM, David Touzeau wrote:
> Thanks Amos, 
> 
> Yes my question was " if NAT is faster or slower than packet flow" and yes you are right. 
> 
> "Squid is not impacted to this question, this make sense."
> 
> 
> I had a "feeling" (human sensation) - with 3.000 users that NAT was faster than Tproxy...
> 
> But you confirm that this is not relevant...
> 

It may be for you. That user count vs your bandwidth is more relevant
than the features performance.

For any given bandwidth total;
 NAT should cope better with higher user count,
 TPROXY should cope better with higher per-user consumption.

So "best" is a matter of how those stack up on your specific network.

Amos


From Sarfaraz.Ahmad at deshaw.com  Mon Sep  3 07:34:31 2018
From: Sarfaraz.Ahmad at deshaw.com (Ahmad, Sarfaraz)
Date: Mon, 3 Sep 2018 07:34:31 +0000
Subject: [squid-users] Squid fails to bump where there are too many DNS
	names in SAN field
Message-ID: <9d6b6ff547094102bd704428ea431d22@mbxpsc3.winmail.deshaw.com>

Hi,

I am using Squid in an interception role with WCCP.
I am peeking at Step1 to read the SNI and determining whether to splice or bump.

That interception/MITM appears to fail where remote certificates from origin servers have way too many dnsnames in the SAN field.
I have noticed this behavior with at least these 2 websites. In both the cases, my setup would be bumping the connections. (Obviously otherwise we won't be having this problem with splicing.)

https://www.pcmag.com/
https://www.extremetech.com/


The RFC doesn't set an upper bound on the number of dnsnames you can set in the SAN field.
If I splice these domains/URLs, browsers don't complain either. So this seems local to Squid.

Points to note:

1)      Even though openssl s_client can connect/negotiate just fine, Squid doesn't.

2)      This is the behavior that I gather from a packet capture.

a.       My client (say a workstation XYZ) tried to connect to 103.243.13.183:443 (That is https://www.extremetech.com)

b.       WCCP ships packet to the proxy over GRE tunnel and a TCP connection with the proxy acting as the origin server is established.

c.       XYZ sends ClientHello to the proxy.

d.       Squid starts conversing the origin server and sends a ClientHello.

e.       Origin server replies with ServerHello, ServerKeyExchange, Certificate packets, Squid just waits endlessly.

f.        The client, XYZ, ends up sending a FIN packet after ClientHello, since Squid doesn't revert back with a ServerHello.

I will have to file a bug ?

Regards,
Sarfaraz




-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20180903/428f25ab/attachment.htm>

From michael.thomas.sw20 at gmail.com  Mon Sep  3 09:32:44 2018
From: michael.thomas.sw20 at gmail.com (Michael Thomas)
Date: Mon, 3 Sep 2018 13:32:44 +0400
Subject: [squid-users] Squid intermittently not sending host header to peer
Message-ID: <CAEdx1fqoSNwbX4LkBcCbmuNzSTdjPWYwS6UbyAG9GugVfFp3YQ@mail.gmail.com>

I'm trying to figure out this weird intermittent issue.

I have two squid servers running, non-caching, non-transparent.

Client -> Squid1 -> Squid2 -> Internet

All HTTPS requests work as expected, but randomly, about 50% of the time,
HTTP requests fail.

The reason for the failure is that the first squid server (Squid1) is not
correctly forwarding the request to the second. It is stripping away the
hostname, and obviously the second squid server has no clue what to do with
it then!

Refreshing either create or resolves the issue, again, seemingly at random.

The following is a copy of the access logs for two successful requests,
followed by a failure:

Squid1:
1535965629.452     81 3.3.3.3 TCP_MISS/200 5766 GET
http://redacted.com/messages/391/ - FIRSTUP_PARENT/2.2.2.2 text/html
1535965634.678     71 3.3.3.3 TCP_MISS/200 5759 GET
http://redacted.com/messages/391/ - FIRSTUP_PARENT/2.2.2.2 text/html
1535965636.673      1 3.3.3.3 TCP_MISS/400 4009 GET
http://redacted.com/messages/391/ - FIRSTUP_PARENT/2.2.2.2 text/html

Squid2:
1535965629.447     79 1.1.1.1 TCP_MISS/200 5673 GET
http://redacted.com/messages/391/ connect HIER_DIRECT/4.4.4.4 text/html
1535965634.673     68 1.1.1.1 TCP_MISS/200 5671 GET
http://redacted.com/messages/391/ connect HIER_DIRECT/4.4.4.4 text/html
1535965636.668      0 1.1.1.1 TAG_NONE/400 3916 GET /messages/391/ -
HIER_NONE/- text/html

squid.conf from Squid1:
http_port 3128 name=port_3128
http_access allow all
nonhierarchical_direct off

acl port_3128_acl myportname port_3128

always_direct deny port_3128_acl

never_direct allow port_3128_acl

# 3128
cache_peer 2.2.2.2 parent 3128 0 no-query proxy-only default  name=proxy3128
cache_peer_access proxy3128 allow port_3128_acl
cache_peer_access proxy3128 deny all

squid.conf from Squid2:
http_access allow all
http_port 3128

Where:
1.1.1.1: Squid1
2.2.2.2: Squid2
3.3.3.3: client
4.4.4.4: Web Server

I can't see any obvious error in my configuration, and the intermittent
nature of it makes me think it might be some sort of bug. I'd love to hear
if anyone else has run into this.

Kind Regards,
Michael Thomas
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20180903/c48a6072/attachment.htm>

From bobbybn0809 at gmail.com  Mon Sep  3 11:27:00 2018
From: bobbybn0809 at gmail.com (Bobbybn0809)
Date: Mon, 3 Sep 2018 18:27:00 +0700
Subject: [squid-users] =?utf-8?q?=28no_subject=29?=
Message-ID: <CA+UAmPva2Y5nf3LWAqpxRm+n3y1FZwAA3Wm6G6h+Rukw+E1dQg@mail.gmail.com>

lists.squid-cache.org
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20180903/d04c3422/attachment.htm>

From bobbybn0809 at gmail.com  Mon Sep  3 12:18:35 2018
From: bobbybn0809 at gmail.com (Bobbybn0809)
Date: Mon, 3 Sep 2018 19:18:35 +0700
Subject: [squid-users] =?utf-8?q?=28no_subject=29?=
Message-ID: <CA+UAmPusoAUuA3WQReXO4shCpRvze34GhvxF-uDpEZB4xDT8cw@mail.gmail.com>

178.128.96.9
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20180903/5a8301da/attachment.htm>

From sis at open.ch  Mon Sep  3 13:02:43 2018
From: sis at open.ch (Simon Staeheli)
Date: Mon, 3 Sep 2018 13:02:43 +0000
Subject: [squid-users] Bearer Authentication in Squid 3.5.28 / 4.2
Message-ID: <B9518768-9B3E-40F7-8F25-36B719B09FBB@open.ch>

Hi there

According to squid-cache.org bearer authentication is nearly completed. Amos attached a patch in a mailing list entry back in 2014 [2]. 

What?s the state of this project? Will this feature become released eventually or became it rather stale? 

Background: I?m playing around with Squid and JWT token [3] based authentication. It looks like the bearer authentication interface is exactly what I?m looking for.

Best
Simon

[1] https://wiki.squid-cache.org/Features/BearerAuthentication 
[2] http://www.squid-cache.org/mail-archive/squid-dev/201407/0147.html
[3] https://jwt.io/
-------------- next part --------------
A non-text attachment was scrubbed...
Name: smime.p7s
Type: application/pkcs7-signature
Size: 4622 bytes
Desc: not available
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20180903/86f58b13/attachment.bin>

From squid3 at treenet.co.nz  Mon Sep  3 13:22:01 2018
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Tue, 4 Sep 2018 01:22:01 +1200
Subject: [squid-users] Squid intermittently not sending host header to
 peer
In-Reply-To: <CAEdx1fqoSNwbX4LkBcCbmuNzSTdjPWYwS6UbyAG9GugVfFp3YQ@mail.gmail.com>
References: <CAEdx1fqoSNwbX4LkBcCbmuNzSTdjPWYwS6UbyAG9GugVfFp3YQ@mail.gmail.com>
Message-ID: <f5eeeefd-6ab5-ce62-ded1-dd327ebfafb6@treenet.co.nz>

On 3/09/18 9:32 PM, Michael Thomas wrote:
> I'm trying to figure out this weird intermittent issue.
> 
> I have two squid servers running, non-caching, non-transparent.
> 
> Client -> Squid1 -> Squid2 -> Internet
> 
> All HTTPS requests work as expected, but randomly, about 50% of the
> time, HTTP requests fail.
> 
> The reason for the failure is that the first squid server (Squid1) is
> not correctly forwarding the request to the second. It is stripping away
> the hostname, and obviously the second squid server has no clue what to
> do with it then!

Can you please provide:

* details of your Squid version(s), the output of "squid -v" contains that.

* a copy of the HTTP message actually received by Squid2.

* a copy of the same HTTP message as it is leaving Squid1.

"debug_options 11,2" can provide that in your cache.log.


Also, is there any sign of CONNECT messages requesting tunnels to Squid2
being received at Squid1 ?


Your logs contain signs of authentication being performed, and you
mention HTTPS being handled by these proxies. Yet the config showed no
signs of either having been configured. Can you please provide the
actual config where you are seeing this behaviour and generating the
logs from.
 A minimal is fine, but it does need to be generating the logs, etc for
the info to correlate correctly.


Amos


From squid3 at treenet.co.nz  Mon Sep  3 13:27:08 2018
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Tue, 4 Sep 2018 01:27:08 +1200
Subject: [squid-users] Bearer Authentication in Squid 3.5.28 / 4.2
In-Reply-To: <B9518768-9B3E-40F7-8F25-36B719B09FBB@open.ch>
References: <B9518768-9B3E-40F7-8F25-36B719B09FBB@open.ch>
Message-ID: <12164ddb-fb09-750c-6306-1456dda38441@treenet.co.nz>

On 4/09/18 1:02 AM, Simon Staeheli wrote:
> Hi there
> 
> According to squid-cache.org bearer authentication is nearly completed. Amos attached a patch in a mailing list entry back in 2014 [2]. 
> 
> What?s the state of this project? Will this feature become released eventually or became it rather stale? 
> 

I am still maintaining the code (on git now) so it is relatively up to
date with current developments. The merge PR is still in QA though, so
no ETA on when it will be an officially supported feature of Squid.
 <https://github.com/squid-cache/squid/pull/30>

Amos


From michael.thomas.sw20 at gmail.com  Mon Sep  3 14:13:00 2018
From: michael.thomas.sw20 at gmail.com (Michael Thomas)
Date: Mon, 3 Sep 2018 18:13:00 +0400
Subject: [squid-users] Squid intermittently not sending host header to
	peer
In-Reply-To: <f5eeeefd-6ab5-ce62-ded1-dd327ebfafb6@treenet.co.nz>
References: <CAEdx1fqoSNwbX4LkBcCbmuNzSTdjPWYwS6UbyAG9GugVfFp3YQ@mail.gmail.com>
 <f5eeeefd-6ab5-ce62-ded1-dd327ebfafb6@treenet.co.nz>
Message-ID: <CAEdx1frbjA=4bcL1mnXu9FnReeO=VgsNZCpNYNFUPbsU+o8khA@mail.gmail.com>

HI Amos,

Thank you for responding.

To clarify, when I referred to HTTPS requests, I was referring to CONNECT
requests - I should have been more clear, my apologies. No authentication
is being performed by either server, so I'm not sure what you're seeing in
the logs that relates to that.

CONNECT requests are logged correctly on both squid servers and appear to
operate correctly for every request.

Interestingly, I was mistaken before. It's not the host header that's
missing - that's still present. It's the full URI within the GET request.

As requested, here is all the information:

*Squid1 version and build information:*
Squid Cache: Version 3.5.12
Service Name: squid
Ubuntu linux
configure options:  '--build=x86_64-linux-gnu' '--prefix=/usr'
'--includedir=${prefix}/include' '--mandir=${prefix}/share/man'
'--infodir=${prefix}/share/info' '--sysconfdir=/etc' '--localstatedir=/var'
'--libexecdir=${prefix}/lib/squid3' '--srcdir=.'
'--disable-maintainer-mode' '--disable-dependency-tracking'
'--disable-silent-rules' 'BUILDCXXFLAGS=-g -O2 -fPIE
-fstack-protector-strong -Wformat -Werror=format-security
-Wl,-Bsymbolic-functions -fPIE -pie -Wl,-z,relro -Wl,-z,now'
'--datadir=/usr/share/squid' '--sysconfdir=/etc/squid'
'--libexecdir=/usr/lib/squid' '--mandir=/usr/share/man' '--enable-inline'
'--disable-arch-native' '--enable-async-io=8'
'--enable-storeio=ufs,aufs,diskd,rock' '--enable-removal-policies=lru,heap'
'--enable-delay-pools' '--enable-cache-digests' '--enable-icap-client'
'--enable-follow-x-forwarded-for'
'--enable-auth-basic=DB,fake,getpwnam,LDAP,NCSA,NIS,PAM,POP3,RADIUS,SASL,SMB'
'--enable-auth-digest=file,LDAP' '--enable-auth-negotiate=kerberos,wrapper'
'--enable-auth-ntlm=fake,smb_lm'
'--enable-external-acl-helpers=file_userip,kerberos_ldap_group,LDAP_group,session,SQL_session,unix_group,wbinfo_group'
'--enable-url-rewrite-helpers=fake' '--enable-eui' '--enable-esi'
'--enable-icmp' '--enable-zph-qos' '--enable-ecap' '--disable-translation'
'--with-swapdir=/var/spool/squid' '--with-logdir=/var/log/squid'
'--with-pidfile=/var/run/squid.pid' '--with-filedescriptors=65536'
'--with-large-files' '--with-default-user=proxy'
'--enable-build-info=Ubuntu linux' '--enable-linux-netfilter'
'build_alias=x86_64-linux-gnu' 'CFLAGS=-g -O2 -fPIE
-fstack-protector-strong -Wformat -Werror=format-security -Wall'
'LDFLAGS=-Wl,-Bsymbolic-functions -fPIE -pie -Wl,-z,relro -Wl,-z,now'
'CPPFLAGS=-Wdate-time -D_FORTIFY_SOURCE=2' 'CXXFLAGS=-g -O2 -fPIE
-fstack-protector-strong -Wformat -Werror=format-security'

*Squid2 version and build information:*
Squid Cache: Version 3.5.27
Service Name: squid
Ubuntu linux

This binary uses OpenSSL 1.0.2g  1 Mar 2016. For legal restrictions on
distribution see https://www.openssl.org/source/license.html

configure options:  '--build=x86_64-linux-gnu' '--prefix=/usr'
'--includedir=${prefix}/include' '--mandir=${prefix}/share/man'
'--infodir=${prefix}/share/info' '--sysconfdir=/etc' '--localstatedir=/var'
'--libexecdir=${prefix}/lib/squid3' '--srcdir=.'
'--disable-maintainer-mode' '--disable-dependency-tracking'
'--disable-silent-rules' 'BUILDCXXFLAGS=-g -O2 -fPIE
-fstack-protector-strong -Wformat -Werror=format-security -Wdate-time
-D_FORTIFY_SOURCE=2 -Wl,-Bsymbolic-functions -fPIE -pie -Wl,-z,relro
-Wl,-z,now -Wl,--as-needed' 'CXX=g++' 'CC=gcc' '--datadir=/usr/share/squid'
'--sysconfdir=/etc/squid' '--libexecdir=/usr/lib/squid'
'--mandir=/usr/share/man' '--enable-inline' '--disable-arch-native'
'--enable-async-io=8' '--enable-storeio=ufs,aufs,diskd,rock'
'--enable-removal-policies=lru,heap' '--enable-delay-pools'
'--enable-cache-digests' '--enable-icap-client'
'--enable-follow-x-forwarded-for'
'--enable-auth-basic=DB,fake,getpwnam,LDAP,NCSA,NIS,PAM,POP3,RADIUS,SASL,SMB'
'--enable-auth-digest=file,LDAP' '--enable-auth-negotiate=kerberos,wrapper'
'--enable-auth-ntlm=fake,smb_lm'
'--enable-external-acl-helpers=file_userip,kerberos_ldap_group,LDAP_group,session,SQL_session,time_quota,unix_group,wbinfo_group'
'--enable-url-rewrite-helpers=fake' '--enable-eui' '--enable-esi'
'--enable-icmp' '--enable-zph-qos' '--enable-ecap' '--disable-translation'
'--with-swapdir=/var/spool/squid' '--with-logdir=/var/log/squid'
'--with-pidfile=/var/run/squid.pid' '--with-filedescriptors=65536'
'--with-large-files' '--with-default-user=proxy' '--with-openssl'
'--enable-ssl' '--enable-ssl-crtd' '--enable-build-info=Ubuntu linux'
'--enable-linux-netfilter' 'build_alias=x86_64-linux-gnu' 'CFLAGS=-g -O2
-fPIE -fstack-protector-strong -Wformat -Werror=format-security -Wall'
'LDFLAGS=-Wl,-Bsymbolic-functions -fPIE -pie -Wl,-z,relro -Wl,-z,now
-Wl,--as-needed' 'CPPFLAGS=-Wdate-time -D_FORTIFY_SOURCE=2' 'CXXFLAGS=-g
-O2 -fPIE -fstack-protector-strong -Wformat -Werror=format-security'



Here is a verbatim copy of both squid.conf files, with sensitive
information replaced:

*Squid1:*
http_port 3128 name=port_3128
http_access allow all
nonhierarchical_direct off

acl port_3128_acl myportname port_3128
always_direct deny port_3128_acl
never_direct allow port_3128_acl

# 3128
cache_peer 2.2.2.2 parent 3128 0 no-query proxy-only default  name=proxy3128
cache_peer_access proxy3128 allow port_3128_acl
cache_peer_access proxy3128 deny all
debug_options 11,2


*Squid2:*
http_access allow all
http_port 3128
debug_options 11,2


And here is a copy of the cache.log for a failed request:

*Squid1:*
----------
2018/09/03 13:36:43| pinger: Initialising ICMP pinger ...
2018/09/03 13:36:45.088 kid1| 11,2| client_side.cc(2346) parseHttpRequest:
HTTP Client local=1.1.1.1:3128 remote=3.3.3.3:52250 FD 8 flags=1
2018/09/03 13:36:45.088 kid1| 11,2| client_side.cc(2347) parseHttpRequest:
HTTP Client REQUEST:
---------
GET http://redacted.com/messages/391/ HTTP/1.1
Host: redacted.com
Proxy-Connection: keep-alive
Cache-Control: max-age=0
Upgrade-Insecure-Requests: 1
User-Agent: Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36
(KHTML, like Gecko) Chrome/68.0.3440.106 Safari/537.36
Accept:
text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8
Accept-Encoding: gzip, deflate
Accept-Language: en-US,en;q=0.9,en-GB;q=0.8
Cookie: __cfduid=redacted; csrftoken=redacted; sessionid=redacted;
_ga=redacted
AlexaToolbar-ALX_NS_PH: AlexaToolbar/alx-4.0.3


----------
2018/09/03 13:36:45.088 kid1| 11,2| http.cc(2234) sendRequest: HTTP Server
local=1.1.1.1:55718 remote=2.2.2.2:3128 FD 14 flags=1
2018/09/03 13:36:45.089 kid1| 11,2| http.cc(2235) sendRequest: HTTP Server
REQUEST:
---------
GET /messages/391/ HTTP/1.1
Upgrade-Insecure-Requests: 1
User-Agent: Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36
(KHTML, like Gecko) Chrome/68.0.3440.106 Safari/537.36
Accept:
text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8
Accept-Encoding: gzip, deflate
Accept-Language: en-US,en;q=0.9,en-GB;q=0.8
Cookie: __cfduid=redacted; csrftoken=redacted; sessionid=redacted;
_ga=redacted
AlexaToolbar-ALX_NS_PH: AlexaToolbar/alx-4.0.3
Host: redacted.com
Via: 1.1 Squid1 (squid/3.5.12)
X-Forwarded-For: 3.3.3.3
Cache-Control: max-age=0
Connection: keep-alive



*Squid2:*
2018/09/03 13:36:45.089 kid1| 11,2| client_side.cc(2372) parseHttpRequest:
HTTP Client local=2.2.2.2:3128 remote=1.1.1.1:55718 FD 15 flags=1
2018/09/03 13:36:45.089 kid1| 11,2| client_side.cc(2373) parseHttpRequest:
HTTP Client REQUEST:
---------
GET /messages/391/ HTTP/1.1
Upgrade-Insecure-Requests: 1
User-Agent: Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36
(KHTML, like Gecko) Chrome/68.0.3440.106 Safari/537.36
Accept:
text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8
Accept-Encoding: gzip, deflate
Accept-Language: en-US,en;q=0.9,en-GB;q=0.8
Cookie: __cfduid=redacted; csrftoken=redacted; sessionid=redacted;
_ga=redacted
AlexaToolbar-ALX_NS_PH: AlexaToolbar/alx-4.0.3
Host: redacted.com
Via: 1.1 Squid1 (squid/3.5.12)
X-Forwarded-For: 3.3.3.3
Cache-Control: max-age=0
Connection: keep-alive


----------
2018/09/03 13:36:45.089 kid1| 11,2| client_side.cc(1409)
sendStartOfMessage: HTTP Client local=2.2.2.2:3128 remote=1.1.1.1:55718 FD
15 flags=1
2018/09/03 13:36:45.089 kid1| 11,2| client_side.cc(1410)
sendStartOfMessage: HTTP Client REPLY:
---------
HTTP/1.1 400 Bad Request
Server: squid/3.5.27
Mime-Version: 1.0
Date: Mon, 03 Sep 2018 13:36:45 GMT
Content-Type: text/html;charset=utf-8
Content-Length: 3556
X-Squid-Error: ERR_INVALID_URL 0
Vary: Accept-Language
Content-Language: en
X-Cache: MISS from Squid2
X-Cache-Lookup: NONE from Squid2:3128
Via: 1.1 Squid2 (squid/3.5.27)
Connection: close



And the same again for a successful request:

*Squid1:*
2018/09/03 13:46:22.850 kid1| 11,2| client_side.cc(2346) parseHttpRequest:
HTTP Client local=1.1.1.1:3128 remote=3.3.3.3:53182 FD 8 flags=1
2018/09/03 13:46:22.850 kid1| 11,2| client_side.cc(2347) parseHttpRequest:
HTTP Client REQUEST:
---------
GET http://redacted.com/messages/391/ HTTP/1.1
Host: redacted.com
Proxy-Connection: keep-alive
Cache-Control: max-age=0
Upgrade-Insecure-Requests: 1
User-Agent: Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36
(KHTML, like Gecko) Chrome/68.0.3440.106 Safari/537.36
Accept:
text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8
Accept-Encoding: gzip, deflate
Accept-Language: en-US,en;q=0.9,en-GB;q=0.8
Cookie: __cfduid=redacted; csrftoken=redacted; sessionid=redacted;
_ga=redacted
AlexaToolbar-ALX_NS_PH: AlexaToolbar/alx-4.0.3


----------
2018/09/03 13:46:22.852 kid1| 11,2| http.cc(2234) sendRequest: HTTP Server
local=1.1.1.1:55798 remote=2.2.2.2:3128 FD 12 flags=1
2018/09/03 13:46:22.852 kid1| 11,2| http.cc(2235) sendRequest: HTTP Server
REQUEST:
---------
GET http://redacted.com/messages/391/ HTTP/1.1
Upgrade-Insecure-Requests: 1
User-Agent: Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36
(KHTML, like Gecko) Chrome/68.0.3440.106 Safari/537.36
Accept:
text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8
Accept-Encoding: gzip, deflate
Accept-Language: en-US,en;q=0.9,en-GB;q=0.8
Cookie: __cfduid=redacted; csrftoken=redacted; sessionid=redacted;
_ga=redacted
AlexaToolbar-ALX_NS_PH: AlexaToolbar/alx-4.0.3
Host: redacted.com
Via: 1.1 Squid1 (squid/3.5.12)
X-Forwarded-For: 3.3.3.3
Cache-Control: max-age=0
Connection: keep-alive


*Squid2:*
2018/09/03 13:46:22.853 kid1| 11,2| client_side.cc(2372) parseHttpRequest:
HTTP Client local=2.2.2.2:3128 remote=1.1.1.1:55798 FD 12 flags=1
2018/09/03 13:46:22.853 kid1| 11,2| client_side.cc(2373) parseHttpRequest:
HTTP Client REQUEST:
---------
GET http://redacted.com/messages/391/ HTTP/1.1
Upgrade-Insecure-Requests: 1
User-Agent: Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36
(KHTML, like Gecko) Chrome/68.0.3440.106 Safari/537.36
Accept:
text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8
Accept-Encoding: gzip, deflate
Accept-Language: en-US,en;q=0.9,en-GB;q=0.8
Cookie: __cfduid=redacted; csrftoken=redacted; sessionid=redacted;
_ga=redacted
AlexaToolbar-ALX_NS_PH: AlexaToolbar/alx-4.0.3
Host: redacted.com
Via: 1.1 Squid1 (squid/3.5.12)
X-Forwarded-For: 3.3.3.3
Cache-Control: max-age=0
Connection: keep-alive


----------
2018/09/03 13:46:22.859 kid1| 11,2| http.cc(2229) sendRequest: HTTP Server
local=2.2.2.2:58376 remote=4.4.4.4:80 <http://4.4.4.4/> FD 10 flags=1
2018/09/03 13:46:22.859 kid1| 11,2| http.cc(2230) sendRequest: HTTP Server
REQUEST:
---------
GET /messages/391/ HTTP/1.1
Upgrade-Insecure-Requests: 1
User-Agent: Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36
(KHTML, like Gecko) Chrome/68.0.3440.106 Safari/537.36
Accept:
text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8
Accept-Encoding: gzip, deflate
Accept-Language: en-US,en;q=0.9,en-GB;q=0.8
Cookie: __cfduid=redacted; csrftoken=redacted; sessionid=redacted;
_ga=redacted
AlexaToolbar-ALX_NS_PH: AlexaToolbar/alx-4.0.3
Host: redacted.com
Via: 1.1 Squid1 (squid/3.5.12), 1.1 Squid2 (squid/3.5.27)
X-Forwarded-For: 3.3.3.3, 1.1.1.1
Cache-Control: max-age=0
Connection: keep-alive

On Mon, Sep 3, 2018 at 5:22 PM Amos Jeffries <squid3 at treenet.co.nz> wrote:

> On 3/09/18 9:32 PM, Michael Thomas wrote:
> > I'm trying to figure out this weird intermittent issue.
> >
> > I have two squid servers running, non-caching, non-transparent.
> >
> > Client -> Squid1 -> Squid2 -> Internet
> >
> > All HTTPS requests work as expected, but randomly, about 50% of the
> > time, HTTP requests fail.
> >
> > The reason for the failure is that the first squid server (Squid1) is
> > not correctly forwarding the request to the second. It is stripping away
> > the hostname, and obviously the second squid server has no clue what to
> > do with it then!
>
> Can you please provide:
>
> * details of your Squid version(s), the output of "squid -v" contains that.
>
> * a copy of the HTTP message actually received by Squid2.
>
> * a copy of the same HTTP message as it is leaving Squid1.
>
> "debug_options 11,2" can provide that in your cache.log.
>
>
> Also, is there any sign of CONNECT messages requesting tunnels to Squid2
> being received at Squid1 ?
>
>
> Your logs contain signs of authentication being performed, and you
> mention HTTPS being handled by these proxies. Yet the config showed no
> signs of either having been configured. Can you please provide the
> actual config where you are seeing this behaviour and generating the
> logs from.
>  A minimal is fine, but it does need to be generating the logs, etc for
> the info to correlate correctly.
>
>
> Amos
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20180903/1efda7b6/attachment.htm>

From sis at open.ch  Mon Sep  3 14:16:25 2018
From: sis at open.ch (Simon Staeheli)
Date: Mon, 3 Sep 2018 14:16:25 +0000
Subject: [squid-users] Bearer Authentication in Squid 3.5.28 / 4.2
In-Reply-To: <mailman.13180.1535983991.4557.squid-users@lists.squid-cache.org>
References: <mailman.13180.1535983991.4557.squid-users@lists.squid-cache.org>
Message-ID: <BA02A955-A3DD-4B2E-BDF6-46806CBC38A0@open.ch>

> From: Amos Jeffries <squid3 at treenet.co.nz>
> Subject: Re: [squid-users] Bearer Authentication in Squid 3.5.28 / 4.2
> Date: 3 September 2018 at 15:27:08 GMT+2
> To: squid-users at lists.squid-cache.org
> 
> 
> On 4/09/18 1:02 AM, Simon Staeheli wrote:
>> Hi there
>> 
>> According to squid-cache.org bearer authentication is nearly completed. Amos attached a patch in a mailing list entry back in 2014 [2]. 
>> 
>> What?s the state of this project? Will this feature become released eventually or became it rather stale? 
>> 
> 
> I am still maintaining the code (on git now) so it is relatively up to
> date with current developments. The merge PR is still in QA though, so
> no ETA on when it will be an officially supported feature of Squid.
> <https://github.com/squid-cache/squid/pull/30>

Perfect, thanks for the pointer to the open PR.
-------------- next part --------------
A non-text attachment was scrubbed...
Name: smime.p7s
Type: application/pkcs7-signature
Size: 4622 bytes
Desc: not available
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20180903/d8eaabc9/attachment.bin>

From rousskov at measurement-factory.com  Mon Sep  3 22:39:01 2018
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Mon, 3 Sep 2018 16:39:01 -0600
Subject: [squid-users] Squid fails to bump where there are too many DNS
 names in SAN field
In-Reply-To: <9d6b6ff547094102bd704428ea431d22@mbxpsc3.winmail.deshaw.com>
References: <9d6b6ff547094102bd704428ea431d22@mbxpsc3.winmail.deshaw.com>
Message-ID: <ebea043a-6895-4ea8-15a7-019c6dbeb685@measurement-factory.com>

On 09/03/2018 01:34 AM, Ahmad, Sarfaraz wrote:

> interception/MITM appears to fail where remote certificates from
> origin servers have way too many dnsnames in the SAN field.
> 
> I have noticed this behavior with at least these 2 websites. In both the
> cases, my setup would be bumping the connections. ?
> 
> https://www.pcmag.com/ 
> https://www.extremetech.com/

> I will have to file a bug ?


You do not have to file a bug report, but if you are using a supported
Squid version and cannot fix this bug (and file a corresponding pull
request) yourself, then filing a bug report is the right thing to do: A
bug report increases the visibility of the problem, may speed up the
fix, and should inform you of that fix availability.

If you file a bug report, please do not forget to specify the Squid
version you are using.


Thank you,

Alex.


From squid3 at treenet.co.nz  Tue Sep  4 04:40:02 2018
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Tue, 4 Sep 2018 16:40:02 +1200
Subject: [squid-users] Squid fails to bump where there are too many DNS
 names in SAN field
In-Reply-To: <ebea043a-6895-4ea8-15a7-019c6dbeb685@measurement-factory.com>
References: <9d6b6ff547094102bd704428ea431d22@mbxpsc3.winmail.deshaw.com>
 <ebea043a-6895-4ea8-15a7-019c6dbeb685@measurement-factory.com>
Message-ID: <6f9af0b0-f11d-75ff-abb6-7b5e1867a3af@treenet.co.nz>

On 4/09/18 10:39 AM, Alex Rousskov wrote:
> On 09/03/2018 01:34 AM, Ahmad, Sarfaraz wrote:
> 
>> interception/MITM appears to fail where remote certificates from
>> origin servers have way too many dnsnames in the SAN field.
>>
>> I have noticed this behavior with at least these 2 websites. In both the
>> cases, my setup would be bumping the connections. ?
>>
>> https://www.pcmag.com/ 
>> https://www.extremetech.com/
> 
>> I will have to file a bug ?
> 

Does it look like a reoccurance of this bug?
 <https://bugs.squid-cache.org/show_bug.cgi?id=3665>

We did not have a concrete confirmation that the exact issue was
permanently gone, it may have just been shifted to larger more obscure
SAN field values.


Amos


From Sarfaraz.Ahmad at deshaw.com  Tue Sep  4 07:33:51 2018
From: Sarfaraz.Ahmad at deshaw.com (Ahmad, Sarfaraz)
Date: Tue, 4 Sep 2018 07:33:51 +0000
Subject: [squid-users] Squid fails to bump where there are too many DNS
 names in SAN field
In-Reply-To: <6f9af0b0-f11d-75ff-abb6-7b5e1867a3af@treenet.co.nz>
References: <9d6b6ff547094102bd704428ea431d22@mbxpsc3.winmail.deshaw.com>
 <ebea043a-6895-4ea8-15a7-019c6dbeb685@measurement-factory.com>
 <6f9af0b0-f11d-75ff-abb6-7b5e1867a3af@treenet.co.nz>
Message-ID: <7e3e510e374342058594f7cf1de9821b@mbxpsc3.winmail.deshaw.com>

With debug_options ALL,9 and retrieving just this page, I found the following relevant loglines (this is with an explicit CONNECT request) ,

2018/09/04 12:45:46.112 kid1| 24,8| SBuf.cc(30) SBuf: SBuf6005084 created
2018/09/04 12:45:46.112 kid1| 24,7| BinaryTokenizer.cc(65) got: TLSPlaintext.type=22 occupying 1 bytes @91 in 0xfa4d38;
2018/09/04 12:45:46.112 kid1| 24,7| BinaryTokenizer.cc(65) got: TLSPlaintext.version.major=3 occupying 1 bytes @92 in 0xfa4d38;
2018/09/04 12:45:46.112 kid1| 24,7| BinaryTokenizer.cc(65) got: TLSPlaintext.version.minor=3 occupying 1 bytes @93 in 0xfa4d38;
2018/09/04 12:45:46.112 kid1| 24,7| BinaryTokenizer.cc(65) got: TLSPlaintext.fragment.length=16384 occupying 2 bytes @94 in 0xfa4d38;
2018/09/04 12:45:46.112 kid1| 24,8| SBuf.cc(38) SBuf: SBuf6005085 created from id SBuf6005054
2018/09/04 12:45:46.112 kid1| 24,7| BinaryTokenizer.cc(74) got: TLSPlaintext.fragment.octets= <16384 OCTET Bytes fit here> 
2018/09/04 12:45:46.112 kid1| 24,8| SBuf.cc(70) ~SBuf: SBuf6005085 destructed
2018/09/04 12:45:46.112 kid1| 24,7| BinaryTokenizer.cc(57) got: TLSPlaintext occupying 16389 bytes @91 in 0xfa4d38;
2018/09/04 12:45:46.112 kid1| 24,7| SBuf.cc(160) rawSpace: reserving 16384 for SBuf6005052
2018/09/04 12:45:46.112 kid1| 24,8| SBuf.cc(886) cow: SBuf6005052 new size:16470
2018/09/04 12:45:46.112 kid1| 24,8| SBuf.cc(857) reAlloc: SBuf6005052 new size: 16470
2018/09/04 12:45:46.112 kid1| 24,9| MemBlob.cc(56) MemBlob: constructed, this=0x1dd2860 id=blob1555829 reserveSize=16470
2018/09/04 12:45:46.112 kid1| 24,8| MemBlob.cc(101) memAlloc: blob1555829 memAlloc: requested=16470, received=16470
2018/09/04 12:45:46.112 kid1| 24,7| SBuf.cc(865) reAlloc: SBuf6005052 new store capacity: 16470
2018/09/04 12:45:46.112 kid1| 24,7| SBuf.cc(85) assign: assigning SBuf6005056 from SBuf6005052
2018/09/04 12:45:46.112 kid1| 24,9| MemBlob.cc(82) ~MemBlob: destructed, this=0x1dd27a0 id=blob1555826 capacity=65535 size=8208
2018/09/04 12:45:46.112 kid1| 24,8| SBuf.cc(30) SBuf: SBuf6005086 created
2018/09/04 12:45:46.112 kid1| 24,7| BinaryTokenizer.cc(65) got: Handshake.msg_type=11 occupying 1 bytes @86 in 0xfa4d70;
2018/09/04 12:45:46.112 kid1| 24,7| BinaryTokenizer.cc(65) got: Handshake.msg_body.length=16900 occupying 3 bytes @87 in 0xfa4d70;
2018/09/04 12:45:46.112 kid1| 24,5| BinaryTokenizer.cc(47) want: 520 more bytes for Handshake.msg_body.octets occupying 16900 bytes @90 in 0xfa4d70;
2018/09/04 12:45:46.112 kid1| 24,8| SBuf.cc(70) ~SBuf: SBuf6005086 destructed
2018/09/04 12:45:46.112 kid1| 24,8| SBuf.cc(70) ~SBuf: SBuf6005084 destructed
2018/09/04 12:45:46.112 kid1| 83,5| Handshake.cc(532) parseHello: need more data
2018/09/04 12:45:46.112 kid1| 83,7| bio.cc(168) stateChanged: FD 15 now: 0x1002 23RSHA (SSLv2/v3 read server hello A)
2018/09/04 12:45:46.112 kid1| 83,5| PeerConnector.cc(451) noteWantRead: local=10.240.180.31:43716 remote=103.243.13.183:443 FD 15 flags=1
2018/09/04 12:45:46.112 kid1| 5,3| comm.cc(559) commSetConnTimeout: local=10.240.180.31:43716 remote=103.243.13.183:443 FD 15 flags=1 timeout 60
2018/09/04 12:45:46.112 kid1| 5,5| ModEpoll.cc(117) SetSelect: FD 15, type=1, handler=1, client_data=0x2818f58, timeout=0
2018/09/04 12:45:46.112 kid1| 45,9| cbdata.cc(419) cbdataReferenceValid: 0x2818f58
2018/09/04 12:45:46.112 kid1| 83,7| AsyncJob.cc(154) callEnd: Ssl::PeekingPeerConnector status out: [ FD 15 job194701]
2018/09/04 12:45:46.112 kid1| 83,7| AsyncCallQueue.cc(57) fireNext: leaving Security::PeerConnector::negotiate()
Later on after about 10 secs

2018/09/04 12:45:58.124 kid1| 83,7| AsyncJob.cc(123) callStart: Ssl::PeekingPeerConnector status in: [ FD 12 job194686]
2018/09/04 12:45:58.124 kid1| 45,9| cbdata.cc(419) cbdataReferenceValid: 0xf67698
2018/09/04 12:45:58.124 kid1| 83,5| PeerConnector.cc(187) negotiate: SSL_connect session=0x122c430
2018/09/04 12:45:58.124 kid1| 24,7| SBuf.cc(160) rawSpace: reserving 65535 for SBuf6002798
2018/09/04 12:45:58.124 kid1| 24,8| SBuf.cc(886) cow: SBuf6002798 new size:82887
2018/09/04 12:45:58.124 kid1| 24,8| SBuf.cc(857) reAlloc: SBuf6002798 new size: 82887
2018/09/04 12:45:58.124 kid1| 24,9| MemBlob.cc(56) MemBlob: constructed, this=0x1dd27a0 id=blob1555830 reserveSize=82887
2018/09/04 12:45:58.124 kid1| 24,8| MemBlob.cc(101) memAlloc: blob1555830 memAlloc: requested=82887, received=82887
2018/09/04 12:45:58.124 kid1| 24,7| SBuf.cc(865) reAlloc: SBuf6002798 new store capacity: 82887
2018/09/04 12:45:58.124 kid1| 24,8| SBuf.cc(139) rawAppendStart: SBuf6002798 start appending up to 65535 bytes
2018/09/04 12:45:58.124 kid1| 83,5| bio.cc(140) read: FD 12 read 0 <= 65535
2018/09/04 12:45:58.124 kid1| 83,5| NegotiationHistory.cc(83) retrieveNegotiatedInfo: SSL connection info on FD 12 SSL version NONE/0.0 negotiated cipher
2018/09/04 12:45:58.124 kid1| ERROR: negotiating TLS on FD 12: error:00000000:lib(0):func(0):reason(0) (5/0/0)
2018/09/04 12:45:58.125 kid1| 45,9| cbdata.cc(256) cbdataInternalAlloc: Allocating 0x110b508
2018/09/04 12:45:58.125 kid1| 45,9| cbdata.cc(419) cbdataReferenceValid: 0x17c3f18
2018/09/04 12:45:58.125 kid1| 45,9| cbdata.cc(419) cbdataReferenceValid: 0x17c3f18
2018/09/04 12:45:58.125 kid1| 45,9| cbdata.cc(419) cbdataReferenceValid: 0x17c3f18
2018/09/04 12:45:58.125 kid1| 45,9| cbdata.cc(419) cbdataReferenceValid: 0x17c3f18
2018/09/04 12:45:58.125 kid1| 45,9| cbdata.cc(351) cbdataInternalLock: 0x110b508=1
2018/09/04 12:45:58.125 kid1| 83,5| PeerConnector.cc(559) callBack: TLS setup ended for local=10.240.180.31:43674 remote=103.243.13.183:443 FD 12 flags=1
2018/09/04 12:45:58.125 kid1| 5,5| comm.cc(1030) comm_remove_close_handler: comm_remove_close_handler: FD 12, AsyncCall=0x1635fc0*2
2018/09/04 12:45:58.125 kid1| 9,5| AsyncCall.cc(56) cancel: will not call Security::PeerConnector::commCloseHandler [call2844544] because comm_remove_close_handler
2018/09/04 12:45:58.125 kid1| 45,9| cbdata.cc(419) cbdataReferenceValid: 0x1f6b778
2018/09/04 12:45:58.125 kid1| 17,4| AsyncCall.cc(93) ScheduleCall: PeerConnector.cc(572) will call FwdState::ConnectedToPeer(0x1f6b778, local=10.240.180.31:43674 remote=103.243.13.183:443 FD 12 flags=1, 0x110b508/0x110b508) [call2844542]
2018/09/04 12:45:58.125 kid1| 45,9| cbdata.cc(419) cbdataReferenceValid: 0xf67698
2018/09/04 12:45:58.125 kid1| 93,5| AsyncJob.cc(139) callEnd: Security::PeerConnector::negotiate() ends job [ FD 12 job194686]
2018/09/04 12:45:58.125 kid1| 83,5| PeerConnector.cc(48) ~PeerConnector: Security::PeerConnector destructed, this=0xf67698
2018/09/04 12:45:58.125 kid1| 45,9| cbdata.cc(383) cbdataInternalUnlock: 0xf67698=2
2018/09/04 12:45:58.125 kid1| 45,9| cbdata.cc(383) cbdataInternalUnlock: 0xf67698=1
2018/09/04 12:45:58.125 kid1| 93,5| AsyncJob.cc(40) ~AsyncJob: AsyncJob destructed, this=0xf67750 type=Ssl::PeekingPeerConnector [job194686]

Again as this is with an explicit CONNECT request, I do get ERR_CANNOT_FORWARD and that error page uses a certificate signed for www.extremetech.com by my internal CA without any thing in SAN field guessing ssl_crtd isn't crashing here unlike the previous bugreport.
Anything from these loglines ?

Regards,
Sarfaraz


-----Original Message-----
From: squid-users <squid-users-bounces at lists.squid-cache.org> On Behalf Of Amos Jeffries
Sent: Tuesday, September 4, 2018 10:10 AM
To: squid-users at lists.squid-cache.org
Subject: Re: [squid-users] Squid fails to bump where there are too many DNS names in SAN field

On 4/09/18 10:39 AM, Alex Rousskov wrote:
> On 09/03/2018 01:34 AM, Ahmad, Sarfaraz wrote:
> 
>> interception/MITM appears to fail where remote certificates from 
>> origin servers have way too many dnsnames in the SAN field.
>>
>> I have noticed this behavior with at least these 2 websites. In both 
>> the cases, my setup would be bumping the connections.
>>
>> https://www.pcmag.com/
>> https://www.extremetech.com/
> 
>> I will have to file a bug ?
> 

Does it look like a reoccurance of this bug?
 <https://bugs.squid-cache.org/show_bug.cgi?id=3665>

We did not have a concrete confirmation that the exact issue was permanently gone, it may have just been shifted to larger more obscure SAN field values.


Amos
_______________________________________________
squid-users mailing list
squid-users at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-users

From Sarfaraz.Ahmad at deshaw.com  Tue Sep  4 08:00:40 2018
From: Sarfaraz.Ahmad at deshaw.com (Ahmad, Sarfaraz)
Date: Tue, 4 Sep 2018 08:00:40 +0000
Subject: [squid-users] Squid fails to bump where there are too many DNS
 names in SAN field
References: <9d6b6ff547094102bd704428ea431d22@mbxpsc3.winmail.deshaw.com>
 <ebea043a-6895-4ea8-15a7-019c6dbeb685@measurement-factory.com>
 <6f9af0b0-f11d-75ff-abb6-7b5e1867a3af@treenet.co.nz> 
Message-ID: <e3d5168b24aa422a8ef86e4346dabc31@mbxpsc3.winmail.deshaw.com>

Forgot to mention, this is with Squid-4.0.24.

-----Original Message-----
From: Ahmad, Sarfaraz 
Sent: Tuesday, September 4, 2018 1:04 PM
To: 'Amos Jeffries' <squid3 at treenet.co.nz>; squid-users at lists.squid-cache.org
Cc: 'rousskov at measurement-factory.com' <rousskov at measurement-factory.com>
Subject: RE: [squid-users] Squid fails to bump where there are too many DNS names in SAN field

With debug_options ALL,9 and retrieving just this page, I found the following relevant loglines (this is with an explicit CONNECT request) ,

2018/09/04 12:45:46.112 kid1| 24,8| SBuf.cc(30) SBuf: SBuf6005084 created
2018/09/04 12:45:46.112 kid1| 24,7| BinaryTokenizer.cc(65) got: TLSPlaintext.type=22 occupying 1 bytes @91 in 0xfa4d38;
2018/09/04 12:45:46.112 kid1| 24,7| BinaryTokenizer.cc(65) got: TLSPlaintext.version.major=3 occupying 1 bytes @92 in 0xfa4d38;
2018/09/04 12:45:46.112 kid1| 24,7| BinaryTokenizer.cc(65) got: TLSPlaintext.version.minor=3 occupying 1 bytes @93 in 0xfa4d38;
2018/09/04 12:45:46.112 kid1| 24,7| BinaryTokenizer.cc(65) got: TLSPlaintext.fragment.length=16384 occupying 2 bytes @94 in 0xfa4d38;
2018/09/04 12:45:46.112 kid1| 24,8| SBuf.cc(38) SBuf: SBuf6005085 created from id SBuf6005054
2018/09/04 12:45:46.112 kid1| 24,7| BinaryTokenizer.cc(74) got: TLSPlaintext.fragment.octets= <16384 OCTET Bytes fit here> 
2018/09/04 12:45:46.112 kid1| 24,8| SBuf.cc(70) ~SBuf: SBuf6005085 destructed
2018/09/04 12:45:46.112 kid1| 24,7| BinaryTokenizer.cc(57) got: TLSPlaintext occupying 16389 bytes @91 in 0xfa4d38;
2018/09/04 12:45:46.112 kid1| 24,7| SBuf.cc(160) rawSpace: reserving 16384 for SBuf6005052
2018/09/04 12:45:46.112 kid1| 24,8| SBuf.cc(886) cow: SBuf6005052 new size:16470
2018/09/04 12:45:46.112 kid1| 24,8| SBuf.cc(857) reAlloc: SBuf6005052 new size: 16470
2018/09/04 12:45:46.112 kid1| 24,9| MemBlob.cc(56) MemBlob: constructed, this=0x1dd2860 id=blob1555829 reserveSize=16470
2018/09/04 12:45:46.112 kid1| 24,8| MemBlob.cc(101) memAlloc: blob1555829 memAlloc: requested=16470, received=16470
2018/09/04 12:45:46.112 kid1| 24,7| SBuf.cc(865) reAlloc: SBuf6005052 new store capacity: 16470
2018/09/04 12:45:46.112 kid1| 24,7| SBuf.cc(85) assign: assigning SBuf6005056 from SBuf6005052
2018/09/04 12:45:46.112 kid1| 24,9| MemBlob.cc(82) ~MemBlob: destructed, this=0x1dd27a0 id=blob1555826 capacity=65535 size=8208
2018/09/04 12:45:46.112 kid1| 24,8| SBuf.cc(30) SBuf: SBuf6005086 created
2018/09/04 12:45:46.112 kid1| 24,7| BinaryTokenizer.cc(65) got: Handshake.msg_type=11 occupying 1 bytes @86 in 0xfa4d70;
2018/09/04 12:45:46.112 kid1| 24,7| BinaryTokenizer.cc(65) got: Handshake.msg_body.length=16900 occupying 3 bytes @87 in 0xfa4d70;
2018/09/04 12:45:46.112 kid1| 24,5| BinaryTokenizer.cc(47) want: 520 more bytes for Handshake.msg_body.octets occupying 16900 bytes @90 in 0xfa4d70;
2018/09/04 12:45:46.112 kid1| 24,8| SBuf.cc(70) ~SBuf: SBuf6005086 destructed
2018/09/04 12:45:46.112 kid1| 24,8| SBuf.cc(70) ~SBuf: SBuf6005084 destructed
2018/09/04 12:45:46.112 kid1| 83,5| Handshake.cc(532) parseHello: need more data
2018/09/04 12:45:46.112 kid1| 83,7| bio.cc(168) stateChanged: FD 15 now: 0x1002 23RSHA (SSLv2/v3 read server hello A)
2018/09/04 12:45:46.112 kid1| 83,5| PeerConnector.cc(451) noteWantRead: local=10.240.180.31:43716 remote=103.243.13.183:443 FD 15 flags=1
2018/09/04 12:45:46.112 kid1| 5,3| comm.cc(559) commSetConnTimeout: local=10.240.180.31:43716 remote=103.243.13.183:443 FD 15 flags=1 timeout 60
2018/09/04 12:45:46.112 kid1| 5,5| ModEpoll.cc(117) SetSelect: FD 15, type=1, handler=1, client_data=0x2818f58, timeout=0
2018/09/04 12:45:46.112 kid1| 45,9| cbdata.cc(419) cbdataReferenceValid: 0x2818f58
2018/09/04 12:45:46.112 kid1| 83,7| AsyncJob.cc(154) callEnd: Ssl::PeekingPeerConnector status out: [ FD 15 job194701]
2018/09/04 12:45:46.112 kid1| 83,7| AsyncCallQueue.cc(57) fireNext: leaving Security::PeerConnector::negotiate()
Later on after about 10 secs

2018/09/04 12:45:58.124 kid1| 83,7| AsyncJob.cc(123) callStart: Ssl::PeekingPeerConnector status in: [ FD 12 job194686]
2018/09/04 12:45:58.124 kid1| 45,9| cbdata.cc(419) cbdataReferenceValid: 0xf67698
2018/09/04 12:45:58.124 kid1| 83,5| PeerConnector.cc(187) negotiate: SSL_connect session=0x122c430
2018/09/04 12:45:58.124 kid1| 24,7| SBuf.cc(160) rawSpace: reserving 65535 for SBuf6002798
2018/09/04 12:45:58.124 kid1| 24,8| SBuf.cc(886) cow: SBuf6002798 new size:82887
2018/09/04 12:45:58.124 kid1| 24,8| SBuf.cc(857) reAlloc: SBuf6002798 new size: 82887
2018/09/04 12:45:58.124 kid1| 24,9| MemBlob.cc(56) MemBlob: constructed, this=0x1dd27a0 id=blob1555830 reserveSize=82887
2018/09/04 12:45:58.124 kid1| 24,8| MemBlob.cc(101) memAlloc: blob1555830 memAlloc: requested=82887, received=82887
2018/09/04 12:45:58.124 kid1| 24,7| SBuf.cc(865) reAlloc: SBuf6002798 new store capacity: 82887
2018/09/04 12:45:58.124 kid1| 24,8| SBuf.cc(139) rawAppendStart: SBuf6002798 start appending up to 65535 bytes
2018/09/04 12:45:58.124 kid1| 83,5| bio.cc(140) read: FD 12 read 0 <= 65535
2018/09/04 12:45:58.124 kid1| 83,5| NegotiationHistory.cc(83) retrieveNegotiatedInfo: SSL connection info on FD 12 SSL version NONE/0.0 negotiated cipher
2018/09/04 12:45:58.124 kid1| ERROR: negotiating TLS on FD 12: error:00000000:lib(0):func(0):reason(0) (5/0/0)
2018/09/04 12:45:58.125 kid1| 45,9| cbdata.cc(256) cbdataInternalAlloc: Allocating 0x110b508
2018/09/04 12:45:58.125 kid1| 45,9| cbdata.cc(419) cbdataReferenceValid: 0x17c3f18
2018/09/04 12:45:58.125 kid1| 45,9| cbdata.cc(419) cbdataReferenceValid: 0x17c3f18
2018/09/04 12:45:58.125 kid1| 45,9| cbdata.cc(419) cbdataReferenceValid: 0x17c3f18
2018/09/04 12:45:58.125 kid1| 45,9| cbdata.cc(419) cbdataReferenceValid: 0x17c3f18
2018/09/04 12:45:58.125 kid1| 45,9| cbdata.cc(351) cbdataInternalLock: 0x110b508=1
2018/09/04 12:45:58.125 kid1| 83,5| PeerConnector.cc(559) callBack: TLS setup ended for local=10.240.180.31:43674 remote=103.243.13.183:443 FD 12 flags=1
2018/09/04 12:45:58.125 kid1| 5,5| comm.cc(1030) comm_remove_close_handler: comm_remove_close_handler: FD 12, AsyncCall=0x1635fc0*2
2018/09/04 12:45:58.125 kid1| 9,5| AsyncCall.cc(56) cancel: will not call Security::PeerConnector::commCloseHandler [call2844544] because comm_remove_close_handler
2018/09/04 12:45:58.125 kid1| 45,9| cbdata.cc(419) cbdataReferenceValid: 0x1f6b778
2018/09/04 12:45:58.125 kid1| 17,4| AsyncCall.cc(93) ScheduleCall: PeerConnector.cc(572) will call FwdState::ConnectedToPeer(0x1f6b778, local=10.240.180.31:43674 remote=103.243.13.183:443 FD 12 flags=1, 0x110b508/0x110b508) [call2844542]
2018/09/04 12:45:58.125 kid1| 45,9| cbdata.cc(419) cbdataReferenceValid: 0xf67698
2018/09/04 12:45:58.125 kid1| 93,5| AsyncJob.cc(139) callEnd: Security::PeerConnector::negotiate() ends job [ FD 12 job194686]
2018/09/04 12:45:58.125 kid1| 83,5| PeerConnector.cc(48) ~PeerConnector: Security::PeerConnector destructed, this=0xf67698
2018/09/04 12:45:58.125 kid1| 45,9| cbdata.cc(383) cbdataInternalUnlock: 0xf67698=2
2018/09/04 12:45:58.125 kid1| 45,9| cbdata.cc(383) cbdataInternalUnlock: 0xf67698=1
2018/09/04 12:45:58.125 kid1| 93,5| AsyncJob.cc(40) ~AsyncJob: AsyncJob destructed, this=0xf67750 type=Ssl::PeekingPeerConnector [job194686]

Again as this is with an explicit CONNECT request, I do get ERR_CANNOT_FORWARD and that error page uses a certificate signed for www.extremetech.com by my internal CA without any thing in SAN field guessing ssl_crtd isn't crashing here unlike the previous bugreport.
Anything from these loglines ?

Regards,
Sarfaraz


-----Original Message-----
From: squid-users <squid-users-bounces at lists.squid-cache.org> On Behalf Of Amos Jeffries
Sent: Tuesday, September 4, 2018 10:10 AM
To: squid-users at lists.squid-cache.org
Subject: Re: [squid-users] Squid fails to bump where there are too many DNS names in SAN field

On 4/09/18 10:39 AM, Alex Rousskov wrote:
> On 09/03/2018 01:34 AM, Ahmad, Sarfaraz wrote:
> 
>> interception/MITM appears to fail where remote certificates from 
>> origin servers have way too many dnsnames in the SAN field.
>>
>> I have noticed this behavior with at least these 2 websites. In both 
>> the cases, my setup would be bumping the connections.
>>
>> https://www.pcmag.com/
>> https://www.extremetech.com/
> 
>> I will have to file a bug ?
> 

Does it look like a reoccurance of this bug?
 <https://bugs.squid-cache.org/show_bug.cgi?id=3665>

We did not have a concrete confirmation that the exact issue was permanently gone, it may have just been shifted to larger more obscure SAN field values.


Amos
_______________________________________________
squid-users mailing list
squid-users at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-users

From rafael.akchurin at diladele.com  Tue Sep  4 12:01:40 2018
From: rafael.akchurin at diladele.com (Rafael Akchurin)
Date: Tue, 4 Sep 2018 12:01:40 +0000
Subject: [squid-users] [icap] Web Safety 6.4 web filter plugin for Squid
	proxy is available
Message-ID: <AM0PR04MB4753C18484E136AA5B4367908F030@AM0PR04MB4753.eurprd04.prod.outlook.com>

Greetings everyone,

Next version of Web Safety web filter for Squid proxy (version 6.4.0.2517 built on July 5, 2108) is now available for download.
This version contains the following fixes and improvements:


  *   YouTube Guard filtering daemon now runs as a separate process. This allows to filter traffic by both Google Safe Browsing and YouTube restriction modules at the same time.
  *   UI of YouTube filtering rules is completely rewritten, it is now possible to selectively filter YouTube videos by policies (enable for students, disable for staff).
  *   Fixed error in policy filtering exclusions by remote domain IP address.
  *   Added initial support for Ubuntu 18 LTS and Squid 4 (full support will be added in Web Safety 6.5)
  *   Added advanced field to manually manage additions to NIC management file /etc/network/interfaces on Ubuntu 16 and Debian 9.
  *   Builds for FreeBSD(pfSense) are not produced any more, please use version 6.3 if you require running Web Safety on FreeBSD(pfSense). We are now trying to build a separate product for pfSense platform.

Pre-configured virtual appliance is available from https://www.diladele.com/virtual_appliance.html (can be run in VMWare ESXi/vSphere or Microsoft Hyper-V).
The same virtual appliance can be easily deployed in Microsoft Azure with the following link https://azuremarketplace.microsoft.com/en-us/marketplace/apps/diladele.websafety?tab=Overview

GitHub repo with automation scripts we used to build this virtual appliance from stock Ubuntu 16 LTS image is at https://github.com/diladele/websafety-virtual-appliance
Your questions/issues/bugs are welcome at support at diladele.com<mailto:support at diladele.com>

Direct link to virtual appliance:


  *   http://packages.diladele.com/websafety/6.4.0.2517/va/ubuntu16/websafety.zip

Version 6.5 will include initial implementation of Application Control (like allow Spotify, block Facebook Messenger) module as well as support for Ubuntu 18 LTS and latest Squid 4. See the version history at https://docs.diladele.com/version_history/index.html

Thanks to all of you for making this possible!

Best regards,
Rafael Akchurin
Diladele B.V.

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20180904/07225ec8/attachment.htm>

From silamael at coronamundi.de  Tue Sep  4 13:24:21 2018
From: silamael at coronamundi.de (Silamael)
Date: Tue, 4 Sep 2018 15:24:21 +0200
Subject: [squid-users] Squid Kerberos helper leaking memory - OpenBSD 6.3
Message-ID: <7941b98a-06e8-a860-9572-f9589166ec3e@coronamundi.de>

Hello,

I'm currently investigating a memory leak in with the Kerberos negotiate 
authentication helper in Squid 3.5.27 under OpenBSD 6.3. It's a own port 
with added Kerberos support since OpenBSD's port does not support 
Kerberos at all.

As library Heimdal 7.5.0 is used. So far I had no luck in finding the 
memory leak itself.

Would it be safe for Squid, to patch the helper code so that it does a 
clean exit after every X processed requests?

Or will this bring new problems on Squid's side?


Thanks for any help!


-- Matthias



From squid3 at treenet.co.nz  Tue Sep  4 13:35:43 2018
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Wed, 5 Sep 2018 01:35:43 +1200
Subject: [squid-users] Squid intermittently not sending host header to
 peer
In-Reply-To: <CAEdx1frbjA=4bcL1mnXu9FnReeO=VgsNZCpNYNFUPbsU+o8khA@mail.gmail.com>
References: <CAEdx1fqoSNwbX4LkBcCbmuNzSTdjPWYwS6UbyAG9GugVfFp3YQ@mail.gmail.com>
 <f5eeeefd-6ab5-ce62-ded1-dd327ebfafb6@treenet.co.nz>
 <CAEdx1frbjA=4bcL1mnXu9FnReeO=VgsNZCpNYNFUPbsU+o8khA@mail.gmail.com>
Message-ID: <3dab3bfd-04b9-fb14-fdb7-37b119386a41@treenet.co.nz>

On 4/09/18 2:13 AM, Michael Thomas wrote:
> HI Amos,
> 
> Thank you for responding.
> 
> To clarify, when I referred to HTTPS requests, I was referring to
> CONNECT requests - I should have been more clear, my apologies. No
> authentication is being performed by either server, so I'm not sure what
> you're seeing in the logs that relates to that.

The log format looks like Squid native format. On all the 200 status
transactions there is "connect" instead of "-" where that format prints
username.


> 
> CONNECT requests are logged correctly on both squid servers and appear
> to operate correctly for every request.
> 
> Interestingly, I was mistaken before. It's not the host header that's
> missing - that's still present. It's the full URI within the GET request.
> 

Nod. Squid2 is receiving an origin-form request. Such as a client would
send *inside* a CONNECT tunnel, or Squid would send on DIRECT traffic.

The former was what I suspected at first, but the message does say Via:
with Squid1 details. So somehow Squid1 must think this connection is a
DIRECT (origin) connection.


> As requested, here is all the information:
> 
> *Squid1 version and build information:*
> Squid Cache: Version 3.5.12
> Service Name: squid
> Ubuntu linux

Please upgrade his machine if you can. All this may turn out to be a
side effect of one of the many bugs fixed already.


> 
> Here is a verbatim copy of both squid.conf files, with sensitive
> information replaced:
> 
> *Squid1:*
> http_port 3128 name=port_3128
> http_access allow all
> nonhierarchical_direct off
> 
> acl port_3128_acl myportname port_3128
> always_direct deny port_3128_acl
> never_direct allow port_3128_acl

If this is our actual config there is no need for these ACLs. This Squid
already accepts *everything* it is handed which has even vague
resemblance of HTTP syntax. All they are doing is making a false
illusion of some control existing.

It should be sufficient to use:
  never_direct allow all
  cache_peer_access proxy3128 allow all


Really you should leave the security checks we put into the default
config. They are there to prevent things like Squid being instructed to
send spam email, or worse DoS'ing your internal network.


> 
> # 3128
> cache_peer 2.2.2.2 parent 3128 0 no-query proxy-only default? name=proxy3128
> cache_peer_access proxy3128 allow port_3128_acl
> cache_peer_access proxy3128 deny all
> debug_options 11,2
> 
> 
> *Squid2:*
> http_access allow all
> http_port 3128
> debug_options 11,2
> 
> 
> And here is a copy of the cache.log for a failed request:
> 
> *Squid1:*
...
> ----------
> 2018/09/03 13:36:45.088 kid1| 11,2| http.cc(2234) sendRequest: HTTP
> Server local=1.1.1.1:55718 <http://1.1.1.1:55718/>?remote=2.2.2.2:3128
> <http://2.2.2.2:3128/>?FD 14 flags=1
> 2018/09/03 13:36:45.089 kid1| 11,2| http.cc(2235) sendRequest: HTTP
> Server REQUEST:
> ---------
> GET /messages/391/ HTTP/1.1
> Upgrade-Insecure-Requests: 1
> User-Agent: Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36
> (KHTML, like Gecko) Chrome/68.0.3440.106 Safari/537.36
> Accept:
> text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8
> Accept-Encoding: gzip, deflate
> Accept-Language: en-US,en;q=0.9,en-GB;q=0.8
> Cookie: __cfduid=redacted; csrftoken=redacted; sessionid=redacted;
> _ga=redacted
> AlexaToolbar-ALX_NS_PH: AlexaToolbar/alx-4.0.3
> Host:?redacted.com <http://redacted.com/>
> Via: 1.1 Squid1 (squid/3.5.12)
> X-Forwarded-For: 3.3.3.3
> Cache-Control: max-age=0
> Connection: keep-alive
> 


Okay. Next thing to do is identify what Squid1 thinks type of connection
this FD is used for. Please add debug level "44,3 51,3" to the Squid1
config and repeat the test.



Amos


From squid3 at treenet.co.nz  Tue Sep  4 13:51:54 2018
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Wed, 5 Sep 2018 01:51:54 +1200
Subject: [squid-users] Squid Kerberos helper leaking memory - OpenBSD 6.3
In-Reply-To: <7941b98a-06e8-a860-9572-f9589166ec3e@coronamundi.de>
References: <7941b98a-06e8-a860-9572-f9589166ec3e@coronamundi.de>
Message-ID: <b1636347-33ca-c939-3874-940acf6aa25a@treenet.co.nz>

On 5/09/18 1:24 AM, Silamael wrote:
> Hello,
> 
> I'm currently investigating a memory leak in with the Kerberos negotiate
> authentication helper in Squid 3.5.27 under OpenBSD 6.3. It's a own port
> with added Kerberos support since OpenBSD's port does not support
> Kerberos at all.
> 
> As library Heimdal 7.5.0 is used. So far I had no luck in finding the
> memory leak itself.

Have you tried valgrind and either GCC or clang static analysis features
on your helper and/or library?

> 
> Would it be safe for Squid, to patch the helper code so that it does a
> clean exit after every X processed requests?
> 
> Or will this bring new problems on Squid's side?
> 

Should be okay so long as the helpers do reply to at least some queries,
and do not exit all at once.

Squid-3.5 will log errors about helpers exiting unexpectedly, but should
only die if the helpers did so on their startup or many are dying within
a shifting 30sec window of time.

Squid-4 can use the auth_param on-persistent-overload=ERR option to
prevent even the death cases above.


Amos


From squid3 at treenet.co.nz  Tue Sep  4 13:58:06 2018
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Wed, 5 Sep 2018 01:58:06 +1200
Subject: [squid-users] Squid fails to bump where there are too many DNS
 names in SAN field
In-Reply-To: <e3d5168b24aa422a8ef86e4346dabc31@mbxpsc3.winmail.deshaw.com>
References: <9d6b6ff547094102bd704428ea431d22@mbxpsc3.winmail.deshaw.com>
 <ebea043a-6895-4ea8-15a7-019c6dbeb685@measurement-factory.com>
 <6f9af0b0-f11d-75ff-abb6-7b5e1867a3af@treenet.co.nz>
 <e3d5168b24aa422a8ef86e4346dabc31@mbxpsc3.winmail.deshaw.com>
Message-ID: <cc97dbf8-e040-de88-a035-7799d0e5580b@treenet.co.nz>

On 4/09/18 8:00 PM, Ahmad, Sarfaraz wrote:
> Forgot to mention, this is with Squid-4.0.24.
> 

Please upgrade to Squid-4.2 ASAP. All 4.0.* releases are beta code and
no longer supported.

Recent as it was there are already several rather major fixes to the
SSL-Bump code since that version. I don't think the upgrade will solve
this particular problem (but it may do if we are lucky) and those other
issues need to be avoided.

Amos


From squid3 at treenet.co.nz  Tue Sep  4 14:20:43 2018
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Wed, 5 Sep 2018 02:20:43 +1200
Subject: [squid-users] Squid fails to bump where there are too many DNS
 names in SAN field
In-Reply-To: <7e3e510e374342058594f7cf1de9821b@mbxpsc3.winmail.deshaw.com>
References: <9d6b6ff547094102bd704428ea431d22@mbxpsc3.winmail.deshaw.com>
 <ebea043a-6895-4ea8-15a7-019c6dbeb685@measurement-factory.com>
 <6f9af0b0-f11d-75ff-abb6-7b5e1867a3af@treenet.co.nz>
 <7e3e510e374342058594f7cf1de9821b@mbxpsc3.winmail.deshaw.com>
Message-ID: <d2277879-8da8-5fd0-b3ea-6634b8c145d2@treenet.co.nz>

On 4/09/18 7:33 PM, Ahmad, Sarfaraz wrote:
> With debug_options ALL,9 and retrieving just this page, I found the following relevant loglines (this is with an explicit CONNECT request) ,
> 

... skip TLS/1.2 clientHello arriving


> Later on after about 10 secs
> 
> 2018/09/04 12:45:58.124 kid1| 83,7| AsyncJob.cc(123) callStart: Ssl::PeekingPeerConnector status in: [ FD 12 job194686]
> 2018/09/04 12:45:58.124 kid1| 45,9| cbdata.cc(419) cbdataReferenceValid: 0xf67698
> 2018/09/04 12:45:58.124 kid1| 83,5| PeerConnector.cc(187) negotiate: SSL_connect session=0x122c430...
> 2018/09/04 12:45:58.124 kid1| 24,8| MemBlob.cc(101) memAlloc: blob1555830 memAlloc: requested=82887, received=82887
> 2018/09/04 12:45:58.124 kid1| 24,7| SBuf.cc(865) reAlloc: SBuf6002798 new store capacity: 82887
> 2018/09/04 12:45:58.124 kid1| 24,8| SBuf.cc(139) rawAppendStart: SBuf6002798 start appending up to 65535 bytes
> 2018/09/04 12:45:58.124 kid1| 83,5| bio.cc(140) read: FD 12 read 0 <= 65535
> 2018/09/04 12:45:58.124 kid1| 83,5| NegotiationHistory.cc(83) retrieveNegotiatedInfo: SSL connection info on FD 12 SSL version NONE/0.0 negotiated cipher
> 2018/09/04 12:45:58.124 kid1| ERROR: negotiating TLS on FD 12: error:00000000:lib(0):func(0):reason(0) (5/0/0)

... the server delivered 82KB of something which was not TLS/SSL syntax
according to OpenSSL.

...
> 2018/09/04 12:45:58.125 kid1| 83,5| PeerConnector.cc(559) callBack: TLS setup ended for local=10.240.180.31:43674 remote=103.243.13.183:443 FD 12 flags=1


> 
> Again as this is with an explicit CONNECT request, I do get ERR_CANNOT_FORWARD and that error page uses a certificate signed for www.extremetech.com by my internal CA without any thing in SAN field guessing ssl_crtd isn't crashing here unlike the previous bugreport.
> Anything from these loglines ?

Lacking any server TLS info (eg inability to TLS handshake with server),
the behaviour and output from Squid to the client is expected to be as
described above.

Amos


From marcus.kool at urlfilterdb.com  Tue Sep  4 15:11:45 2018
From: marcus.kool at urlfilterdb.com (Marcus Kool)
Date: Tue, 4 Sep 2018 12:11:45 -0300
Subject: [squid-users] Squid fails to bump where there are too many DNS
 names in SAN field
In-Reply-To: <d2277879-8da8-5fd0-b3ea-6634b8c145d2@treenet.co.nz>
References: <9d6b6ff547094102bd704428ea431d22@mbxpsc3.winmail.deshaw.com>
 <ebea043a-6895-4ea8-15a7-019c6dbeb685@measurement-factory.com>
 <6f9af0b0-f11d-75ff-abb6-7b5e1867a3af@treenet.co.nz>
 <7e3e510e374342058594f7cf1de9821b@mbxpsc3.winmail.deshaw.com>
 <d2277879-8da8-5fd0-b3ea-6634b8c145d2@treenet.co.nz>
Message-ID: <f8f53369-c5ba-3536-aff0-3853ea9156ad@urlfilterdb.com>



On 04/09/18 11:20, Amos Jeffries wrote:
> On 4/09/18 7:33 PM, Ahmad, Sarfaraz wrote:
>> With debug_options ALL,9 and retrieving just this page, I found the following relevant loglines (this is with an explicit CONNECT request) ,
>>
> 
> ... skip TLS/1.2 clientHello arriving
> 
> 
>> Later on after about 10 secs
>>
>> 2018/09/04 12:45:58.124 kid1| 83,7| AsyncJob.cc(123) callStart: Ssl::PeekingPeerConnector status in: [ FD 12 job194686]
>> 2018/09/04 12:45:58.124 kid1| 45,9| cbdata.cc(419) cbdataReferenceValid: 0xf67698
>> 2018/09/04 12:45:58.124 kid1| 83,5| PeerConnector.cc(187) negotiate: SSL_connect session=0x122c430...
>> 2018/09/04 12:45:58.124 kid1| 24,8| MemBlob.cc(101) memAlloc: blob1555830 memAlloc: requested=82887, received=82887
>> 2018/09/04 12:45:58.124 kid1| 24,7| SBuf.cc(865) reAlloc: SBuf6002798 new store capacity: 82887
>> 2018/09/04 12:45:58.124 kid1| 24,8| SBuf.cc(139) rawAppendStart: SBuf6002798 start appending up to 65535 bytes
>> 2018/09/04 12:45:58.124 kid1| 83,5| bio.cc(140) read: FD 12 read 0 <= 65535
>> 2018/09/04 12:45:58.124 kid1| 83,5| NegotiationHistory.cc(83) retrieveNegotiatedInfo: SSL connection info on FD 12 SSL version NONE/0.0 negotiated cipher
>> 2018/09/04 12:45:58.124 kid1| ERROR: negotiating TLS on FD 12: error:00000000:lib(0):func(0):reason(0) (5/0/0)
> 
> ... the server delivered 82KB of something which was not TLS/SSL syntax
> according to OpenSSL.

I ran 'ufdbpeek', an OpenSSL-based utility that I wrote that peeks at the TLS certificate of a website and it displays a large correct certificate and that (in my case) cipher 
ECDHE-RSA-AES256-GCM-SHA384 is used.
OpenSSL 1.0.2k and 1.1.0g  have no issues with the certificate nor handshake.

Also sslLabs shows that all is well and that all popular modern browsers and OpenSSL 0.9.8 and 1.0.1 can connect to the site:
https://www.ssllabs.com/ssltest/analyze.html?d=www.extremetech.com

Marcus

[...]


From silamael at coronamundi.de  Tue Sep  4 15:22:10 2018
From: silamael at coronamundi.de (Silamael)
Date: Tue, 4 Sep 2018 17:22:10 +0200
Subject: [squid-users] Squid Kerberos helper leaking memory - OpenBSD 6.3
In-Reply-To: <b1636347-33ca-c939-3874-940acf6aa25a@treenet.co.nz>
References: <7941b98a-06e8-a860-9572-f9589166ec3e@coronamundi.de>
 <b1636347-33ca-c939-3874-940acf6aa25a@treenet.co.nz>
Message-ID: <7cdcb552-8393-40f0-0dd2-dd2a0fc53754@coronamundi.de>

On 09/04/2018 03:51 PM, Amos Jeffries wrote:
> On 5/09/18 1:24 AM, Silamael wrote:
>> Hello,
>>
>> I'm currently investigating a memory leak in with the Kerberos negotiate
>> authentication helper in Squid 3.5.27 under OpenBSD 6.3. It's a own port
>> with added Kerberos support since OpenBSD's port does not support
>> Kerberos at all.
>>
>> As library Heimdal 7.5.0 is used. So far I had no luck in finding the
>> memory leak itself.
> 
> Have you tried valgrind and either GCC or clang static analysis features
> on your helper and/or library?

valgrind doesn't seem to work properly on OpenBSD. I get a bunch of 
nonsense output and then a segmentation fault...
What are the GCC/clang statistic features? I'm no C/C++ pro ;)

>>
>> Would it be safe for Squid, to patch the helper code so that it does a
>> clean exit after every X processed requests?
>>
>> Or will this bring new problems on Squid's side?
>>
> 
> Should be okay so long as the helpers do reply to at least some queries,
> and do not exit all at once.
> 
> Squid-3.5 will log errors about helpers exiting unexpectedly, but should
> only die if the helpers did so on their startup or many are dying within
> a shifting 30sec window of time.
At moment a helper will call exit(0) after 10000 requests. Don't know, 
how Squid distributes the requests over all helper processes and if we 
have too many helpers exiting within 30 seconds...
But good to know that there aren't any general objections.

> 
> Squid-4 can use the auth_param on-persistent-overload=ERR option to
> prevent even the death cases above.

Good to know.

-- Matthias


From rousskov at measurement-factory.com  Tue Sep  4 15:44:08 2018
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Tue, 4 Sep 2018 09:44:08 -0600
Subject: [squid-users] Squid fails to bump where there are too many DNS
 names in SAN field
In-Reply-To: <e3d5168b24aa422a8ef86e4346dabc31@mbxpsc3.winmail.deshaw.com>
References: <9d6b6ff547094102bd704428ea431d22@mbxpsc3.winmail.deshaw.com>
 <ebea043a-6895-4ea8-15a7-019c6dbeb685@measurement-factory.com>
 <6f9af0b0-f11d-75ff-abb6-7b5e1867a3af@treenet.co.nz>
 <e3d5168b24aa422a8ef86e4346dabc31@mbxpsc3.winmail.deshaw.com>
Message-ID: <4609deb9-d986-35c8-30f3-cfd2278eca22@measurement-factory.com>

On 09/04/2018 02:00 AM, Ahmad, Sarfaraz wrote:

> 2018/09/04 12:45:46.112 kid1| 24,5| BinaryTokenizer.cc(47) want: 520 more bytes for Handshake.msg_body.octets occupying 16900 bytes @90 in 0xfa4d70;
> 2018/09/04 12:45:46.112 kid1| 83,5| PeerConnector.cc(451) noteWantRead: local=10.240.180.31:43716 remote=103.243.13.183:443 FD 15 flags=1


Translation: Squid did not read enough data from the server to finish
parsing TLS server handshake. Squid needs to read at least 520 more
bytes from FD 15.


> Later on after about 10 secs

> 2018/09/04 12:45:58.124 kid1| 83,5| bio.cc(140) read: FD 12 read 0 <= 65535

And end-of-file on the wrong/different connection.


My recommendations remain the same, but please follow Amos advice and
upgrade to the latest v4 first.

Please note that I do _not_ recommend analyzing ALL,9 logs. On average,
such analysis by non-developers wastes more time than it saves.

Alex.


From rousskov at measurement-factory.com  Tue Sep  4 15:59:38 2018
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Tue, 4 Sep 2018 09:59:38 -0600
Subject: [squid-users] Squid Kerberos helper leaking memory - OpenBSD 6.3
In-Reply-To: <7cdcb552-8393-40f0-0dd2-dd2a0fc53754@coronamundi.de>
References: <7941b98a-06e8-a860-9572-f9589166ec3e@coronamundi.de>
 <b1636347-33ca-c939-3874-940acf6aa25a@treenet.co.nz>
 <7cdcb552-8393-40f0-0dd2-dd2a0fc53754@coronamundi.de>
Message-ID: <63ddace0-2bde-9ab6-1fd8-c53afa2dd33c@measurement-factory.com>

On 09/04/2018 09:22 AM, Silamael wrote:

> At moment a helper will call exit(0) after 10000 requests. 

> good to know that there aren't any general objections.


Here is one: Squid is currently not designed to gracefully handle a
helper-initiated exit/death. Helpers that decide to exit may kill
in-progress transactions, and/or may slow down or even kill Squid,
depending, in part, on your Squid version and/or configuration.

AFAICT, there are a few better options for going forward, including:

1. Fixing helper memory leak (just stating the obvious for completeness
sake).

2. Wrapping leaking/exiting helper process into a
non-leaking/non-exiting helper that is going to kill/restart the wrapped
helper after N requests (transparently to Squid).

3. Hacking Squid to kill/restart a helper process after N requests.

4. Enhancing Squid and helper protocol to handle helper-initiated exits.


HTH,

Alex.


From turgut at kalfaoglu.com  Tue Sep  4 16:44:43 2018
From: turgut at kalfaoglu.com (=?UTF-8?Q?turgut_kalfao=c4=9flu?=)
Date: Tue, 4 Sep 2018 19:44:43 +0300
Subject: [squid-users] a decent way to speed up Facebook?
Message-ID: <59802606-1188-c582-e479-b958d7b495d2@kalfaoglu.com>

Hello there. I have a transparent squid at my home to speed up the 
browsing by caching stuff.? And it works well for HTTP.

For HTTPS, I was only able to get it to "peek" and I'd like to able to 
bump the connections.

I installed the server certificate on the client, but still, the browser 
(firefox) keeps complaining:

Your connection is not secure
The owner of www.facebook.com has configured their website improperly. 
To protect your information from being stolen, Firefox has not connected 
to this website.
This site uses HTTP Strict Transport Security (HSTS) to specify that 
Firefox may only connect to it securely. As a result, it is not possible 
to add an exception for this certificate.

Here is what I have:
#
# serverIsBank is a list of domains that are banks essentially. They 
seem more picky.
#
ssl_bump splice serverIsBank
ssl_bump peek all
# ssl_bump bump all??? # this does not work, it gives the error above..

https_port 3129 intercept ssl-bump \
 ??????? generate-host-certificates=on dynamic_cert_mem_cache_size=4MB \
 ??????? cert=/etc/squid/ssl_cert/tk2ca.pem 
key=/etc/squid/ssl_cert/tk2ca.pem \
 ?????? sslflags=NO_SESSION_REUSE
tls_outgoing_options cafile=/etc/pki/tls/certs/ca-bundle.crt
sslproxy_cert_adapt setCommonName ssl::certDomainMismatch
sslproxy_cert_error allow all
sslcrtd_program? /usr/lib64/squid/security_file_certgen? -s 
/var/lib/ssl_db -M $
sslcrtd_children 50 startup=5 idle=5


Thanks, -turgut


-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20180904/11ac6418/attachment.htm>

From huaraz at moeller.plus.com  Tue Sep  4 17:42:23 2018
From: huaraz at moeller.plus.com (Markus Moeller)
Date: Tue, 4 Sep 2018 18:42:23 +0100
Subject: [squid-users] Squid Kerberos helper leaking memory - OpenBSD 6.3
In-Reply-To: <63ddace0-2bde-9ab6-1fd8-c53afa2dd33c@measurement-factory.com>
References: <7941b98a-06e8-a860-9572-f9589166ec3e@coronamundi.de>
 <b1636347-33ca-c939-3874-940acf6aa25a@treenet.co.nz>
 <7cdcb552-8393-40f0-0dd2-dd2a0fc53754@coronamundi.de>
 <63ddace0-2bde-9ab6-1fd8-c53afa2dd33c@measurement-factory.com>
Message-ID: <pmmg1u$f6u$1@blaine.gmane.org>

Can you run the helper standalone with valgrind ?

e.g.

./negotiate_kerberos_auth_test  squid.example.com  3 | awk
'{sub(/Token:/,"YR"); print $0}END{print "QQ"}' |
valgrind --log-file=./negotiate_kerberos_auth.val --leak-check=full --show-reachable=yes
-v ./negotiate_kerberos_auth -d -t none -k $dir/squid.keytab -s
GSS_C_NO_NAME


Markus


"Alex Rousskov"  wrote in message 
news:63ddace0-2bde-9ab6-1fd8-c53afa2dd33c at measurement-factory.com...

On 09/04/2018 09:22 AM, Silamael wrote:

> At moment a helper will call exit(0) after 10000 requests.

> good to know that there aren't any general objections.


Here is one: Squid is currently not designed to gracefully handle a
helper-initiated exit/death. Helpers that decide to exit may kill
in-progress transactions, and/or may slow down or even kill Squid,
depending, in part, on your Squid version and/or configuration.

AFAICT, there are a few better options for going forward, including:

1. Fixing helper memory leak (just stating the obvious for completeness
sake).

2. Wrapping leaking/exiting helper process into a
non-leaking/non-exiting helper that is going to kill/restart the wrapped
helper after N requests (transparently to Squid).

3. Hacking Squid to kill/restart a helper process after N requests.

4. Enhancing Squid and helper protocol to handle helper-initiated exits.


HTH,

Alex.
_______________________________________________
squid-users mailing list
squid-users at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-users 




From srnhari at gmail.com  Wed Sep  5 04:05:23 2018
From: srnhari at gmail.com (Hariharan Sethuraman)
Date: Wed, 5 Sep 2018 09:35:23 +0530
Subject: [squid-users] SSL reverse proxy cert error
Message-ID: <CANfnjg+onePoeq=ER9wjoYxJ5RcQmx_tJ7pbVE++mhFgRfcaiw@mail.gmail.com>

Hi All,

I have my https_port 443 in reverse proxy. When client sends a GET request,
the rewrite correctly rewrites the URL and that rewritten GET request fails
with below error.
2018/09/05 03:03:38| Error negotiating SSL on FD 15: error:14007086:SSL
routines:CONNECT_CR_CERT:certificate verify failed (1/-1/0)

I dont where to add the trusted certificates, because I dont know where to
specify the trusted certificates in /etc/ssl/certs directory.

I have two ways to support:
1) I may have cache_peer parent proxy (next proxy to internet)
2) I dont need to give any parent proxy (because this host is connected to
internet without next proxy)

Thanks,
Hari
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20180905/00a80d78/attachment.htm>

From squid3 at treenet.co.nz  Wed Sep  5 04:31:55 2018
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Wed, 5 Sep 2018 16:31:55 +1200
Subject: [squid-users] SSL reverse proxy cert error
In-Reply-To: <CANfnjg+onePoeq=ER9wjoYxJ5RcQmx_tJ7pbVE++mhFgRfcaiw@mail.gmail.com>
References: <CANfnjg+onePoeq=ER9wjoYxJ5RcQmx_tJ7pbVE++mhFgRfcaiw@mail.gmail.com>
Message-ID: <c511c83c-fcb2-7d0b-f073-59b98b17f96d@treenet.co.nz>

On 5/09/18 4:05 PM, Hariharan Sethuraman wrote:
> Hi All,
> 
> I have my https_port 443 in reverse proxy. When client sends a GET
> request, the rewrite correctly rewrites the URL and that rewritten GET
> request fails with below error.
> 2018/09/05 03:03:38| Error negotiating SSL on FD 15: error:14007086:SSL
> routines:CONNECT_CR_CERT:certificate verify failed (1/-1/0)
> 
> I dont where to add the trusted certificates, because I dont know where
> to specify the trusted certificates in /etc/ssl/certs directory.
> 
> I have two ways to support:?
> 1) I may have cache_peer parent proxy (next proxy to internet)

For reverse-proxy the peer should be (or be towards) the origin. Not
towards the public Internet.

Use the cache_peer tls-ca= option to tell Squid which specific CA that
peer/origin is supposed to be using.


> 2) I dont need to give any parent proxy (because this host is connected
> to internet without next proxy)

For connections directly to the Internet (which reverse-proxy cannot
make without being forced) the global "Trusted CA" are used by default,
there is nothing to be done in that regard.

You can choose to disable them with:

  tls_outgoing_options default-ca=off


If you need to make Squid trust a specific CA which is not one of the
global trusted set (eg private for your use, or self-signed) then use:

  tls_outgoing_options cafile=/path/to/ca.pem


You can also combine the above settings so only a few global CA which
you actually trust get loaded. The cafile= option can be repeated in
Squid-4 to load multiple CA details.

Amos


From squid3 at treenet.co.nz  Wed Sep  5 04:45:05 2018
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Wed, 5 Sep 2018 16:45:05 +1200
Subject: [squid-users] a decent way to speed up Facebook?
In-Reply-To: <59802606-1188-c582-e479-b958d7b495d2@kalfaoglu.com>
References: <59802606-1188-c582-e479-b958d7b495d2@kalfaoglu.com>
Message-ID: <e0c9324b-2c3a-f6f5-5e1c-ea98edddc75a@treenet.co.nz>

On 5/09/18 4:44 AM, turgut kalfao?lu wrote:
> Hello there. I have a transparent squid at my home to speed up the
> browsing by caching stuff.? And it works well for HTTP.
> 
> For HTTPS, I was only able to get it to "peek" and I'd like to able to
> bump the connections.
> 
> I installed the server certificate on the client, but still, the browser
> (firefox) keeps complaining:
> 
> Your connection is not secure
> The owner of www.facebook.com has configured their website improperly.
> To protect your information from being stolen, Firefox has not connected
> to this website.
> This site uses HTTP Strict Transport Security (HSTS) to specify that
> Firefox may only connect to it securely. As a result, it is not possible
> to add an exception for this certificate.

Squid removes HSTS from any network traffic it handles (except splice'd
traffic). So clearing the browser info and ensuring that the other
non-HTTP protocols Browser like to use these days (eg QUIC, SPDY,
WebSockets, HTTP/2) are not happening should resolve this issue.

If you do not (or cannot) clear the browser info the HSTS should only
last until the TTL it last mentioned in traffic expires - but that can
be a very long timeout.


> 
> Here is what I have:
> #
> # serverIsBank is a list of domains that are banks essentially. They
> seem more picky.
> #
> ssl_bump splice serverIsBank
> ssl_bump peek all
> # ssl_bump bump all??? # this does not work, it gives the error above..

Try:

 # splice as soon as detected
 ssl_bump splice serverIsBank

 # step 1 - peek to get TLS SNI
 acl step1 at_step SslBump1
 ssl_bump peek step1

 # step 2 - stare to get server cert details for bump
 ssl_bump stare all

 # step 3 - terminate if splice failed, bump everything else
 ssl_bump terminate serverIsBank
 ssl_bump bump all


> 
> https_port 3129 intercept ssl-bump \
> ??????? generate-host-certificates=on dynamic_cert_mem_cache_size=4MB \
> ??????? cert=/etc/squid/ssl_cert/tk2ca.pem
> key=/etc/squid/ssl_cert/tk2ca.pem \

When cert= and key= are in the same file you do not need to specify key=.


> ?????? sslflags=NO_SESSION_REUSE
> tls_outgoing_options cafile=/etc/pki/tls/certs/ca-bundle.crt

That ca-bundle.crt is the global trusted CA right?

If yes, you do not need to manually configure it. The system default CA
/ global Trusted CA are used by default on MITM outgoing connections.


> sslproxy_cert_adapt setCommonName ssl::certDomainMismatch
> sslproxy_cert_error allow all

Remove the above line. It prevents you being told about important problems.

Instead investigate errors that come up, and either fix or ignore on an
individual basis. Some errors are simple and easily avoided, others
depend on your policy about whether the client should be allowed to do
the operation.


HTH
Amos


From srnhari at gmail.com  Wed Sep  5 06:14:24 2018
From: srnhari at gmail.com (Hariharan Sethuraman)
Date: Wed, 5 Sep 2018 11:44:24 +0530
Subject: [squid-users] avocent protocol support in squid
Message-ID: <CANfnjgLGy3GVO2ts9_kZsUme9sw+0RCO3cyRHhS3mOtxgZptew@mail.gmail.com>

Hi,

Wanted to know if we support avocent protocol in squid - if yes, which
directive we should check? Couldnt get much details from google.

Thanks,
Hari
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20180905/05d2e3c4/attachment.htm>

From arshadansari at live.in  Wed Sep  5 07:02:45 2018
From: arshadansari at live.in (Arshad Ansari)
Date: Wed, 5 Sep 2018 07:02:45 +0000
Subject: [squid-users] Using CA signed certificate for SSL bump
Message-ID: <BN1PR12MB003539CBE68440F0D4F96067CE020@BN1PR12MB0035.namprd12.prod.outlook.com>

Hi All,



I have setup squid 4.2 for forward proxy and caching. It is working fine when I am using self-signed certificate for SSL bump.



However, our security requirement is to use only CA signed certificate and not self-signed certificate.



I have tried various options like using Https and intercept but nothing seems to be working.



My question is does SSL work with CA signed certificate?



Regards,
Arshad

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20180905/a2a68625/attachment.htm>

From Antony.Stone at squid.open.source.it  Wed Sep  5 08:29:55 2018
From: Antony.Stone at squid.open.source.it (Antony Stone)
Date: Wed, 5 Sep 2018 10:29:55 +0200
Subject: [squid-users] Using CA signed certificate for SSL bump
In-Reply-To: <BN1PR12MB003539CBE68440F0D4F96067CE020@BN1PR12MB0035.namprd12.prod.outlook.com>
References: <BN1PR12MB003539CBE68440F0D4F96067CE020@BN1PR12MB0035.namprd12.prod.outlook.com>
Message-ID: <201809051029.55754.Antony.Stone@squid.open.source.it>

On Wednesday 05 September 2018 at 09:02:45, Arshad Ansari wrote:

> Hi All,
> 
> I have setup squid 4.2 for forward proxy and caching. It is working fine
> when I am using self-signed certificate for SSL bump.

Good.  Well done.

> However, our security requirement is to use only CA signed certificate and
> not self-signed certificate.

That won't work.

> I have tried various options like using Https and intercept but nothing
> seems to be working.

Indeed.

> My question is does SSL work with CA signed certificate?

SSL?  Yes.

SSL Bump / interception, no - because if it did, you'd have a globally-trusted 
certificate which you could use to fake any website on the Internet.

Security?  The CA who gave you that certificate would disappear.


Antony.

-- 
Tinned food was developed for the British Navy in 1813.

The tin opener was not invented until 1858.

                                                   Please reply to the list;
                                                         please *don't* CC me.


From Sarfaraz.Ahmad at deshaw.com  Wed Sep  5 08:37:35 2018
From: Sarfaraz.Ahmad at deshaw.com (Ahmad, Sarfaraz)
Date: Wed, 5 Sep 2018 08:37:35 +0000
Subject: [squid-users] Squid fails to bump where there are too many DNS
 names in SAN field
In-Reply-To: <4609deb9-d986-35c8-30f3-cfd2278eca22@measurement-factory.com>
References: <9d6b6ff547094102bd704428ea431d22@mbxpsc3.winmail.deshaw.com>
 <ebea043a-6895-4ea8-15a7-019c6dbeb685@measurement-factory.com>
 <6f9af0b0-f11d-75ff-abb6-7b5e1867a3af@treenet.co.nz>
 <e3d5168b24aa422a8ef86e4346dabc31@mbxpsc3.winmail.deshaw.com>
 <4609deb9-d986-35c8-30f3-cfd2278eca22@measurement-factory.com>
Message-ID: <b9172cc9d8b94657837920c412d0bbeb@mbxpsc3.winmail.deshaw.com>

Tested with Squid-4.2 and ended with same results. 
How do we proceed here ?


-----Original Message-----
From: Alex Rousskov <rousskov at measurement-factory.com> 
Sent: Tuesday, September 4, 2018 9:14 PM
To: Ahmad, Sarfaraz <Sarfaraz.Ahmad at deshaw.com>; squid-users at lists.squid-cache.org
Subject: Re: [squid-users] Squid fails to bump where there are too many DNS names in SAN field

On 09/04/2018 02:00 AM, Ahmad, Sarfaraz wrote:

> 2018/09/04 12:45:46.112 kid1| 24,5| BinaryTokenizer.cc(47) want: 520 more bytes for Handshake.msg_body.octets occupying 16900 bytes @90 in 0xfa4d70;
> 2018/09/04 12:45:46.112 kid1| 83,5| PeerConnector.cc(451) noteWantRead: local=10.240.180.31:43716 remote=103.243.13.183:443 FD 15 flags=1


Translation: Squid did not read enough data from the server to finish
parsing TLS server handshake. Squid needs to read at least 520 more
bytes from FD 15.


> Later on after about 10 secs

> 2018/09/04 12:45:58.124 kid1| 83,5| bio.cc(140) read: FD 12 read 0 <= 65535

And end-of-file on the wrong/different connection.


My recommendations remain the same, but please follow Amos advice and
upgrade to the latest v4 first.

Please note that I do _not_ recommend analyzing ALL,9 logs. On average,
such analysis by non-developers wastes more time than it saves.

Alex.

From michael.thomas.sw20 at gmail.com  Wed Sep  5 09:04:57 2018
From: michael.thomas.sw20 at gmail.com (Michael Thomas)
Date: Wed, 5 Sep 2018 13:04:57 +0400
Subject: [squid-users] Squid intermittently not sending host header to
	peer
In-Reply-To: <3dab3bfd-04b9-fb14-fdb7-37b119386a41@treenet.co.nz>
References: <CAEdx1fqoSNwbX4LkBcCbmuNzSTdjPWYwS6UbyAG9GugVfFp3YQ@mail.gmail.com>
 <f5eeeefd-6ab5-ce62-ded1-dd327ebfafb6@treenet.co.nz>
 <CAEdx1frbjA=4bcL1mnXu9FnReeO=VgsNZCpNYNFUPbsU+o8khA@mail.gmail.com>
 <3dab3bfd-04b9-fb14-fdb7-37b119386a41@treenet.co.nz>
Message-ID: <CAEdx1fpM8AK7RojaQgqajcKcVRxAx6-WvvM5sqC7UDm33JHP1w@mail.gmail.com>

Thanks Amos,

I updated both servers to Squid 4.2 and the issue persisted. I understand
what you're saying about the configuration and lack of security - in
production, this will be in place. I was removed to try and resolve the
issue we encountered.

After adding the additional configuration to debug_options, I noticed that
the issue occurs when the same socket is re-used for a second request.

I believe what's happening is that Squid is 'forgetting' that it's not
connecting to the origin when a persistent connection is in use.
Setting server_persistent_connections off resolves the issue, albeit in a
way that will likely harm performance.

Here is an excerpt from cache.log on Squid1

Successful Request:
2018/09/05 08:20:19.401 kid1| 11,2| client_side.cc(1274) parseHttpRequest:
HTTP Client local=1.1.1.1:3128 remote=3.3.3.3:52210 FD 14 flags=1
2018/09/05 08:20:19.401 kid1| 11,2| client_side.cc(1278) parseHttpRequest:
HTTP Client REQUEST:
---------
GET http://redacted.com/messages/391/ HTTP/1.1
Host: redacted.com
Proxy-Connection: keep-alive
Cache-Control: max-age=0
Upgrade-Insecure-Requests: 1
User-Agent: Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36
(KHTML, like Gecko) Chrome/68.0.3440.106 Safari/537.36
Accept:
text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8
Accept-Encoding: gzip, deflate
Accept-Language: en-US,en;q=0.9,en-GB;q=0.8
Cookie: __cfduid=redacted; csrftoken=redacted; sessionid=redacted;
_ga=redacted
AlexaToolbar-ALX_NS_PH: AlexaToolbar/alx-4.0.3


----------
2018/09/05 08:20:19.401 kid1| 44,3| peer_select.cc(161) peerSelect:
e:=IV/0x21f7e50*2 http://redacted.com/messages/391/
2018/09/05 08:20:19.401 kid1| 44,3| peer_select.cc(458) peerSelectFoo: GET
redacted.com
2018/09/05 08:20:19.401 kid1| 44,3| peer_select.cc(463) peerSelectFoo:
peerSelectFoo: direct = DIRECT_UNKNOWN (always_direct to be checked)
2018/09/05 08:20:19.401 kid1| 44,3| peer_select.cc(218)
peerCheckAlwaysDirectDone: peerCheckAlwaysDirectDone: DENIED
2018/09/05 08:20:19.401 kid1| 44,3| peer_select.cc(458) peerSelectFoo: GET
redacted.com
2018/09/05 08:20:19.401 kid1| 44,3| peer_select.cc(471) peerSelectFoo:
peerSelectFoo: direct = DIRECT_UNKNOWN (never_direct to be checked)
2018/09/05 08:20:19.401 kid1| 44,3| peer_select.cc(195)
peerCheckNeverDirectDone: peerCheckNeverDirectDone: ALLOWED
2018/09/05 08:20:19.401 kid1| 44,3| peer_select.cc(201)
peerCheckNeverDirectDone: direct = DIRECT_NO (never_direct allow)
2018/09/05 08:20:19.401 kid1| 44,3| peer_select.cc(458) peerSelectFoo: GET
redacted.com
2018/09/05 08:20:19.401 kid1| 44,3| peer_select.cc(134) peerSelectIcpPing:
peerSelectIcpPing: http://redacted.com/messages/391/
2018/09/05 08:20:19.401 kid1| 44,3| peer_select.cc(145) peerSelectIcpPing:
peerSelectIcpPing: counted 0 neighbors
2018/09/05 08:20:19.401 kid1| 44,3| peer_select.cc(698) peerGetSomeParent:
GET redacted.com
2018/09/05 08:20:19.401 kid1| 44,3| peer_select.cc(964) peerAddFwdServer:
adding FIRSTUP_PARENT/2.2.2.2
2018/09/05 08:20:19.401 kid1| 44,3| peer_select.cc(958) peerAddFwdServer:
skipping ANY_OLD_PARENT/2.2.2.2; have FIRSTUP_PARENT/2.2.2.2
2018/09/05 08:20:19.401 kid1| 44,3| peer_select.cc(958) peerAddFwdServer:
skipping DEFAULT_PARENT/2.2.2.2; have FIRSTUP_PARENT/2.2.2.2
2018/09/05 08:20:19.401 kid1| 44,2| peer_select.cc(281) peerSelectDnsPaths:
Find IP destination for: http://redacted.com/messages/391/' via 2.2.2.2
2018/09/05 08:20:19.401 kid1| 44,2| peer_select.cc(302) peerSelectDnsPaths:
Found sources for 'http://redacted.com/messages/391/'
2018/09/05 08:20:19.401 kid1| 44,2| peer_select.cc(303)
peerSelectDnsPaths:   always_direct = DENIED
2018/09/05 08:20:19.401 kid1| 44,2| peer_select.cc(304)
peerSelectDnsPaths:    never_direct = ALLOWED
2018/09/05 08:20:19.401 kid1| 44,2| peer_select.cc(314)
peerSelectDnsPaths:      cache_peer = local=0.0.0.0 remote=2.2.2.2:3128
flags=1
2018/09/05 08:20:19.401 kid1| 44,2| peer_select.cc(317)
peerSelectDnsPaths:        timedout = 0
2018/09/05 08:20:19.401 kid1| 44,3| peer_select.cc(103) ~ps_state:
http://redacted.com/messages/391/
2018/09/05 08:20:19.401 kid1| 51,3| fd.cc(198) fd_open: fd_open() FD 9
2018/09/05 08:20:19.402 kid1| 11,2| http.cc(2260) sendRequest: HTTP Server
local=1.1.1.1:45688 remote=2.2.2.2:3128 FD 9 flags=1
2018/09/05 08:20:19.402 kid1| 11,2| http.cc(2261) sendRequest: HTTP Server
REQUEST:
---------
GET http://redacted.com/messages/391/ HTTP/1.1
Upgrade-Insecure-Requests: 1
User-Agent: Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36
(KHTML, like Gecko) Chrome/68.0.3440.106 Safari/537.36
Accept:
text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8
Accept-Encoding: gzip, deflate
Accept-Language: en-US,en;q=0.9,en-GB;q=0.8
Cookie: __cfduid=redacted; csrftoken=redacted; sessionid=redacted;
_ga=redacted
AlexaToolbar-ALX_NS_PH: AlexaToolbar/alx-4.0.3
Host: redacted.com
Via: 1.1 smtp01 (squid/4.2)
X-Forwarded-For: 3.3.3.3
Cache-Control: max-age=0
Connection: keep-alive


Failed Request, note: FD 9 is not closed/opened between these requests:
2018/09/05 08:20:22.124 kid1| 11,2| client_side.cc(1274) parseHttpRequest:
HTTP Client local=1.1.1.1:3128 remote=3.3.3.3:52219 FD 15 flags=1
2018/09/05 08:20:22.124 kid1| 11,2| client_side.cc(1278) parseHttpRequest:
HTTP Client REQUEST:
---------
GET http://redacted.com/messages/391/ HTTP/1.1
Host: redacted.com
Proxy-Connection: keep-alive
Cache-Control: max-age=0
Upgrade-Insecure-Requests: 1
User-Agent: Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36
(KHTML, like Gecko) Chrome/68.0.3440.106 Safari/537.36
Accept:
text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8
Accept-Encoding: gzip, deflate
Accept-Language: en-US,en;q=0.9,en-GB;q=0.8
Cookie: __cfduid=redacted; csrftoken=redacted; sessionid=redacted;
_ga=redacted
AlexaToolbar-ALX_NS_PH: AlexaToolbar/alx-4.0.3


----------
2018/09/05 08:20:22.124 kid1| 44,3| peer_select.cc(161) peerSelect:
e:=IV/0x21f7e50*2 http://redacted.com/messages/391/
2018/09/05 08:20:22.124 kid1| 44,3| peer_select.cc(458) peerSelectFoo: GET
redacted.com
2018/09/05 08:20:22.124 kid1| 44,3| peer_select.cc(463) peerSelectFoo:
peerSelectFoo: direct = DIRECT_UNKNOWN (always_direct to be checked)
2018/09/05 08:20:22.124 kid1| 44,3| peer_select.cc(218)
peerCheckAlwaysDirectDone: peerCheckAlwaysDirectDone: DENIED
2018/09/05 08:20:22.124 kid1| 44,3| peer_select.cc(458) peerSelectFoo: GET
redacted.com
2018/09/05 08:20:22.124 kid1| 44,3| peer_select.cc(471) peerSelectFoo:
peerSelectFoo: direct = DIRECT_UNKNOWN (never_direct to be checked)
2018/09/05 08:20:22.124 kid1| 44,3| peer_select.cc(195)
peerCheckNeverDirectDone: peerCheckNeverDirectDone: ALLOWED
2018/09/05 08:20:22.124 kid1| 44,3| peer_select.cc(201)
peerCheckNeverDirectDone: direct = DIRECT_NO (never_direct allow)
2018/09/05 08:20:22.124 kid1| 44,3| peer_select.cc(458) peerSelectFoo: GET
redacted.com
2018/09/05 08:20:22.124 kid1| 44,3| peer_select.cc(134) peerSelectIcpPing:
peerSelectIcpPing: http://redacted.com/messages/391/
2018/09/05 08:20:22.124 kid1| 44,3| peer_select.cc(145) peerSelectIcpPing:
peerSelectIcpPing: counted 0 neighbors
2018/09/05 08:20:22.124 kid1| 44,3| peer_select.cc(698) peerGetSomeParent:
GET redacted.com
2018/09/05 08:20:22.124 kid1| 44,3| peer_select.cc(964) peerAddFwdServer:
adding FIRSTUP_PARENT/2.2.2.2
2018/09/05 08:20:22.124 kid1| 44,3| peer_select.cc(958) peerAddFwdServer:
skipping ANY_OLD_PARENT/2.2.2.2; have FIRSTUP_PARENT/2.2.2.2
2018/09/05 08:20:22.124 kid1| 44,3| peer_select.cc(958) peerAddFwdServer:
skipping DEFAULT_PARENT/2.2.2.2; have FIRSTUP_PARENT/2.2.2.2
2018/09/05 08:20:22.124 kid1| 44,2| peer_select.cc(281) peerSelectDnsPaths:
Find IP destination for: http://redacted.com/messages/391/' via 2.2.2.2
2018/09/05 08:20:22.124 kid1| 44,2| peer_select.cc(302) peerSelectDnsPaths:
Found sources for 'http://redacted.com/messages/391/'
2018/09/05 08:20:22.124 kid1| 44,2| peer_select.cc(303)
peerSelectDnsPaths:   always_direct = DENIED
2018/09/05 08:20:22.124 kid1| 44,2| peer_select.cc(304)
peerSelectDnsPaths:    never_direct = ALLOWED
2018/09/05 08:20:22.124 kid1| 44,2| peer_select.cc(314)
peerSelectDnsPaths:      cache_peer = local=0.0.0.0 remote=2.2.2.2:3128
flags=1
2018/09/05 08:20:22.124 kid1| 44,2| peer_select.cc(317)
peerSelectDnsPaths:        timedout = 0
2018/09/05 08:20:22.125 kid1| 44,3| peer_select.cc(103) ~ps_state:
http://redacted.com/messages/391/
2018/09/05 08:20:22.125 kid1| 11,2| http.cc(2260) sendRequest: HTTP Server
local=1.1.1.1:45688 remote=2.2.2.2:3128 FD 9 flags=1
2018/09/05 08:20:22.125 kid1| 11,2| http.cc(2261) sendRequest: HTTP Server
REQUEST:
---------
GET /messages/391/ HTTP/1.1
Upgrade-Insecure-Requests: 1
User-Agent: Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36
(KHTML, like Gecko) Chrome/68.0.3440.106 Safari/537.36
Accept:
text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8
Accept-Encoding: gzip, deflate
Accept-Language: en-US,en;q=0.9,en-GB;q=0.8
Cookie: __cfduid=redacted; csrftoken=redacted; sessionid=redacted;
_ga=redacted
AlexaToolbar-ALX_NS_PH: AlexaToolbar/alx-4.0.3
Host: redacted.com
Via: 1.1 smtp01 (squid/4.2)
X-Forwarded-For: 3.3.3.3
Cache-Control: max-age=0
Connection: keep-alive
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20180905/50f7f79d/attachment.htm>

From flashdown at data-core.org  Wed Sep  5 10:02:36 2018
From: flashdown at data-core.org (Flashdown)
Date: Wed, 05 Sep 2018 12:02:36 +0200
Subject: [squid-users] Using CA signed certificate for SSL bump
In-Reply-To: <BN1PR12MB003539CBE68440F0D4F96067CE020@BN1PR12MB0035.namprd12.prod.outlook.com>
References: <BN1PR12MB003539CBE68440F0D4F96067CE020@BN1PR12MB0035.namprd12.prod.outlook.com>
Message-ID: <898B6E85-40E8-425A-A71C-A18A1983977A@data-core.org>

Hey,

How should that work? That would require an ca to sign your selfsigney ca to be able to issue valid public certs for all websites. If that would be possible, then the whole concept of ssl security would be worth nothing. You cant create valid certificates for such websites. You can only issue certs that are valid in your organisation only. Therefore the selfsigned ca needs to be trusted by your clients by adding it in the trust root authorities. There is no other way, wait, there is, do not try to intercept ssl secured connections. So you cant look in the traffic as it is supposed to be. Or break it and live with the needs required for this. If you have no valid reason to intercept sich traffic then just dont do it.

Am 5. September 2018 09:02:45 MESZ schrieb Arshad Ansari <arshadansari at live.in>:
>Hi All,
>
>
>
>I have setup squid 4.2 for forward proxy and caching. It is working
>fine when I am using self-signed certificate for SSL bump.
>
>
>
>However, our security requirement is to use only CA signed certificate
>and not self-signed certificate.
>
>
>
>I have tried various options like using Https and intercept but nothing
>seems to be working.
>
>
>
>My question is does SSL work with CA signed certificate?
>
>
>
>Regards,
>Arshad
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20180905/ef8327ed/attachment.htm>

From alex at nanogherkin.com  Wed Sep  5 11:05:17 2018
From: alex at nanogherkin.com (Alex Crow)
Date: Wed, 5 Sep 2018 12:05:17 +0100
Subject: [squid-users] Using CA signed certificate for SSL bump
In-Reply-To: <BN1PR12MB003539CBE68440F0D4F96067CE020@BN1PR12MB0035.namprd12.prod.outlook.com>
References: <BN1PR12MB003539CBE68440F0D4F96067CE020@BN1PR12MB0035.namprd12.prod.outlook.com>
Message-ID: <eef580fb-88e2-6218-5f6d-ecdef3e4b0f3@nanogherkin.com>

You can set up your own internal CA. You then have the CA key (so can 
generate certificates for any domain) and install the CA public 
certificate on all client machines.


That CA can be anything from a local CA on the squid box, using a 
central VM with something like XCA installed, all the way to an 
enterprise HSM.


But you must have the CA key. There is no way a commercial CA would give 
you a universal signing key.


Alex


On 05/09/18 08:02, Arshad Ansari wrote:
>
> Hi All,
>
> I have setup squid 4.2 for forward proxy and caching. It is working 
> fine when I am using self-signed certificate for SSL bump.
>
> However, our security requirement is to use only CA signed certificate 
> and not self-signed certificate.
>
> I have tried various options like using Https and intercept but 
> nothing seems to be working.
>
> My question is does SSL work with CA signed certificate?
>
> Regards,
> Arshad
>
>
>
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20180905/ba5bddae/attachment.htm>

From christophe.colle at ac-nancy-metz.fr  Wed Sep  5 13:16:00 2018
From: christophe.colle at ac-nancy-metz.fr (Colle Christophe)
Date: Wed, 05 Sep 2018 15:16:00 +0200
Subject: [squid-users] Radius and Squid transparent mode
Message-ID: <254cbaecc9652d7.5b8ff330@ac-nancy-metz.fr>

Hello,

I am working on a WiFi project: People connect to the network using a Radius server, then use the Internet using Squid in transparent mode.

I would like to improve this system by adding the identifier of the person logged in the Squid logs (It's easier to do research, it saves time!).

Is it easy or should use a specific helper authentication?



thank you.

Has anyone ever done that?





Regards,

Chris.
<signatureafterquotedtext></signatureafterquotedtext>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20180905/890b5af4/attachment.htm>

From squid3 at treenet.co.nz  Wed Sep  5 13:54:35 2018
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 6 Sep 2018 01:54:35 +1200
Subject: [squid-users] avocent protocol support in squid
In-Reply-To: <CANfnjgLGy3GVO2ts9_kZsUme9sw+0RCO3cyRHhS3mOtxgZptew@mail.gmail.com>
References: <CANfnjgLGy3GVO2ts9_kZsUme9sw+0RCO3cyRHhS3mOtxgZptew@mail.gmail.com>
Message-ID: <1724aa2b-8eea-3297-111a-18378e5e1f05@treenet.co.nz>

On 5/09/18 6:14 PM, Hariharan Sethuraman wrote:
> Hi,
> 
> Wanted to know if we support avocent protocol in squid - if yes, which
> directive we should check? Couldnt get much details from google.

What is this protocol? There seems to only be hardware products by a
company of that name - not even software, let alone a protocol.

So probably no. Squid is an HTTP proxy, not an "everything" proxy.

Amos


From squid3 at treenet.co.nz  Wed Sep  5 14:25:03 2018
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 6 Sep 2018 02:25:03 +1200
Subject: [squid-users] Radius and Squid transparent mode
In-Reply-To: <254cbaecc9652d7.5b8ff330@ac-nancy-metz.fr>
References: <254cbaecc9652d7.5b8ff330@ac-nancy-metz.fr>
Message-ID: <08c716cf-b337-021b-8714-daa393ca11a2@treenet.co.nz>

On 6/09/18 1:16 AM, Colle Christophe wrote:
> Hello,
> 
> I am working on a WiFi project: People connect to the network using a
> Radius server, then use the Internet using Squid in transparent mode.
> 
> I would like to improve this system by adding the identifier of the
> person logged in the Squid logs (It's easier to do research, it saves
> time!).


First lesson: there is no "person".

In the HTTP world we explicitly avoid the terms "user" or "person"
because a lot (most?) of traffic is from automated services and
machinery around any given network. Some of it is even generated by your
own Squid with no client involved at all.


> 
> Is it easy or should use a specific helper authentication?
> 

Second;
 When traffic is MITM'd the client believes it is talking to some other
endpoint. It will only ever authenticate with credentials suitable for
that endpoint.
 No sane client software will broadcast credentials without the remote
endpoint explicitly requesting them. Some clients are not that sane, but
they are the exception.


Third;
 Authentication of all types involves some secret known only to the
endpoints, often generated on-demand via some other channel. The MITM
proxy even holding the credentials cannot authenticate them, nor
reliably use them for anything other than relaying as-is on the *same*
transactions outbound request message.


BUT ... this is where "authorization" being different from
"authentication" matters a lot.


> 
> Has anyone ever done that?
> 

As I understand it RADIUS has ways to tie IP:port of TCP connections to
a user account (if any?).

It is possible to have a RADIUS helper used on external_acl_type
receiving those details and providing Squid with a label to log as
"username".

Or, alternatively just send the log through a daemon which uses the log
lines it gets passed to append any extra details you want it to add.

But be aware these only associate the machinery by-IP to an account. It
does not imply the "person" was actually present, nor even aware of the
transaction happening.


Amos


From rousskov at measurement-factory.com  Wed Sep  5 15:48:03 2018
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Wed, 5 Sep 2018 09:48:03 -0600
Subject: [squid-users] Squid fails to bump where there are too many DNS
 names in SAN field
In-Reply-To: <b9172cc9d8b94657837920c412d0bbeb@mbxpsc3.winmail.deshaw.com>
References: <9d6b6ff547094102bd704428ea431d22@mbxpsc3.winmail.deshaw.com>
 <ebea043a-6895-4ea8-15a7-019c6dbeb685@measurement-factory.com>
 <6f9af0b0-f11d-75ff-abb6-7b5e1867a3af@treenet.co.nz>
 <e3d5168b24aa422a8ef86e4346dabc31@mbxpsc3.winmail.deshaw.com>
 <4609deb9-d986-35c8-30f3-cfd2278eca22@measurement-factory.com>
 <b9172cc9d8b94657837920c412d0bbeb@mbxpsc3.winmail.deshaw.com>
Message-ID: <31a8168e-fc3c-4161-c0f0-b69c286a48e0@measurement-factory.com>

On 09/05/2018 02:37 AM, Ahmad, Sarfaraz wrote:
> Tested with Squid-4.2 and ended with same results. 
> How do we proceed here ?

At the risk of sounding like a broken record, I can only repeat my
earlier recommendation to file a bug report (assuming you cannot fix the
bug). Your overall options are summarized at
https://wiki.squid-cache.org/SquidFaq/AboutSquid#How_to_add_a_new_Squid_feature.2C_enhance.2C_of_fix_something.3F

If possible, please attach (to the bug report) a compressed ALL,9
cache.log showing a single transaction reproducing the problem. Squid
wiki has relevant suggestions:
https://wiki.squid-cache.org/SquidFaq/BugReporting#Debugging_a_single_transaction


Thank you,

Alex.


> -----Original Message-----
> From: Alex Rousskov <rousskov at measurement-factory.com> 
> Sent: Tuesday, September 4, 2018 9:14 PM
> To: Ahmad, Sarfaraz <Sarfaraz.Ahmad at deshaw.com>; squid-users at lists.squid-cache.org
> Subject: Re: [squid-users] Squid fails to bump where there are too many DNS names in SAN field
> 
> On 09/04/2018 02:00 AM, Ahmad, Sarfaraz wrote:
> 
>> 2018/09/04 12:45:46.112 kid1| 24,5| BinaryTokenizer.cc(47) want: 520 more bytes for Handshake.msg_body.octets occupying 16900 bytes @90 in 0xfa4d70;
>> 2018/09/04 12:45:46.112 kid1| 83,5| PeerConnector.cc(451) noteWantRead: local=10.240.180.31:43716 remote=103.243.13.183:443 FD 15 flags=1
> 
> 
> Translation: Squid did not read enough data from the server to finish
> parsing TLS server handshake. Squid needs to read at least 520 more
> bytes from FD 15.
> 
> 
>> Later on after about 10 secs
> 
>> 2018/09/04 12:45:58.124 kid1| 83,5| bio.cc(140) read: FD 12 read 0 <= 65535
> 
> And end-of-file on the wrong/different connection.
> 
> 
> My recommendations remain the same, but please follow Amos advice and
> upgrade to the latest v4 first.
> 
> Please note that I do _not_ recommend analyzing ALL,9 logs. On average,
> such analysis by non-developers wastes more time than it saves.
> 
> Alex.
> 



From vh1988 at yahoo.com.ar  Thu Sep  6 05:40:25 2018
From: vh1988 at yahoo.com.ar (Julian Perconti)
Date: Thu, 6 Sep 2018 02:40:25 -0300
Subject: [squid-users] Squid and DNS
Message-ID: <000401d445a4$1ec7a320$5c56e960$@yahoo.com.ar>

Hi all,

"I discovered" that if I use more than one *local* dns server/resolver, when
I use squid HTTPS, there are some problems accesing to the web.

For example:

I have a squid with TLS support in server "B"; the gateway and resolver of
the server "B" is server "A" and the server "A" has bind installed and
multiple or at least one (local) dns forwarders. (djbdns)

In this scenario squid; takes a long time to load some sites like Dropbox,
Twitter, (if it load succesfull, other times does not load in anyway).

If I remove the forwarders (local always, never publics one like 8.8.8.8) in
server "A", the problem disappears.

In this scenario, the dns forwarders in server "A" is not being directly
used by the clients nor squid (they are forwarders for bind in server "A"),
e.g. browsing by server "B" (squid) an resolving domains via server "A" with
forwarders.

So, the question: How can I use multiple DNS caching resolvers/server (local
or remote) like bind/djbdns without the issue mentioned above?

Is mandatory for squid to use only 1 dns/caching nameserver?

From: https://wiki.squid-cache.org/KnowledgeBase/HostHeaderForgery

>ensure that the DNS servers Squid uses are the same as those used by the
client(s). 
>Certain popular CDN hosting networks use load balancing systems to
determine which website IPs to return in the DNS query response. These are
based on the querying DNS resolvers IP. If Squid >and the client are using
different resolvers there is an increased chance of different results being
given. Which can lead to this alert


Thank You in advance!



From yosefmel at post.bgu.ac.il  Thu Sep  6 06:06:48 2018
From: yosefmel at post.bgu.ac.il (Yosef Meltser)
Date: Thu, 6 Sep 2018 01:06:48 -0500 (CDT)
Subject: [squid-users] content adaptation using squid
Message-ID: <1536214008679-0.post@n4.nabble.com>


Hi,


We have managed to create a proxy server using a squid in an intercept mode.

Now we would like to make a content adaptation, for example to show an alert
every time the user entered a website.

We are not familiar with this domain, so we are looking for the easiest way
to inject java script in the http response (of course only http sites).

 The main two adaptation mechanisms are:

1.       Icap

2.       Ecap

For example, in Icap there are some server frameworks that we can use (like
c-icap, ICAP-server and etc and etc), which one is the most recommended?

In contrast to the above, ECAP is not using any server, and the whole
process in embedded into the squid, so it sounds quite easier. Is it?

Thank you in advance.



--
Sent from: http://squid-web-proxy-cache.1019090.n4.nabble.com/Squid-Users-f1019091.html


From uhlar at fantomas.sk  Thu Sep  6 07:22:43 2018
From: uhlar at fantomas.sk (Matus UHLAR - fantomas)
Date: Thu, 6 Sep 2018 09:22:43 +0200
Subject: [squid-users] Squid and DNS
In-Reply-To: <000401d445a4$1ec7a320$5c56e960$@yahoo.com.ar>
References: <000401d445a4$1ec7a320$5c56e960$@yahoo.com.ar>
Message-ID: <20180906072243.GA9867@fantomas.sk>

On 06.09.18 02:40, Julian Perconti wrote:
>"I discovered" that if I use more than one *local* dns server/resolver, when
>I use squid HTTPS, there are some problems accesing to the web.

>I have a squid with TLS support in server "B"; the gateway and resolver of
>the server "B" is server "A" and the server "A" has bind installed and
>multiple or at least one (local) dns forwarders. (djbdns)

>If I remove the forwarders (local always, never publics one like 8.8.8.8) in
>server "A", the problem disappears.

>In this scenario, the dns forwarders in server "A" is not being directly
>used by the clients nor squid (they are forwarders for bind in server "A"),
>e.g. browsing by server "B" (squid) an resolving domains via server "A" with
>forwarders.

what do you mean forwarders? You need to send query to a DNS server that
makes the resolution.

It's OK when you have squid configured on server "B" and DNS on server "A"
and squid uses server "A" for resolution.

However, your repeated usage of word "forwarders" indicates there is
something broken in the configuration on server "A".

>So, the question: How can I use multiple DNS caching resolvers/server (local
>or remote) like bind/djbdns without the issue mentioned above?

do not use djbdns. ever.

simply configure bind on server "A", allow it to provide recursion for
server "B" and that's all. Forget forwarders.


>Is mandatory for squid to use only 1 dns/caching nameserver?

usually, people have multiple DNS servers configured to fail over in case
one of them fails.
in some cases, client can balance the load, or prefer server with faster
responses.

There should be no problem of this kind, unless one of your DNS servers is
broken.


-- 
Matus UHLAR - fantomas, uhlar at fantomas.sk ; http://www.fantomas.sk/
Warning: I wish NOT to receive e-mail advertising to this address.
Varovanie: na tuto adresu chcem NEDOSTAVAT akukolvek reklamnu postu.
There's a long-standing bug relating to the x86 architecture that
allows you to install Windows.   -- Matthew D. Fuller


From rafael.akchurin at diladele.com  Thu Sep  6 09:33:30 2018
From: rafael.akchurin at diladele.com (Rafael Akchurin)
Date: Thu, 6 Sep 2018 09:33:30 +0000
Subject: [squid-users] Squid 3.5.28 for Microsoft Windows 64-bit is available
Message-ID: <AM0PR04MB4753EF0E26EE5AF20B9B618A8F010@AM0PR04MB4753.eurprd04.prod.outlook.com>

Greetings everyone,

Sorry with even a more huge delay we would like to announce the availability of the CygWin based build of Squid proxy
for Microsoft Windows version 3.5.28 (amd64 only!). Delay was caused by our inability to code sign the MSI with new "token" requirements.

* Original release notes are at http://www.squid-cache.org/Versions/v3/3.5/squid-3.5.28-RELEASENOTES.html .
* Ready to use MSI package can be downloaded from http://squid.diladele.com .
* List of open issues for the installer - https://github.com/diladele/squid-windows/issues

Thanks a lot for Squid developers for making this great software!

Please join our humble efforts to provide ready to run MSI installer for Squid on Microsoft Windows with all required dependencies at GitHub -
https://github.com/diladele/squid-windows . Report all issues/bugs/feature requests at GitHub project.
Issues about the *MSI installer only* can also be reported to support at diladele.com<mailto:support at diladele.com> .

Best regards,
Rafael Akchurin
Diladele B.V.
https://www.diladele.com



-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20180906/8eb5a347/attachment.htm>

From squid3 at treenet.co.nz  Thu Sep  6 12:57:23 2018
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Fri, 7 Sep 2018 00:57:23 +1200
Subject: [squid-users] Squid and DNS
In-Reply-To: <20180906072243.GA9867@fantomas.sk>
References: <000401d445a4$1ec7a320$5c56e960$@yahoo.com.ar>
 <20180906072243.GA9867@fantomas.sk>
Message-ID: <fdee4933-e3f6-67d4-fc66-2a591145052f@treenet.co.nz>

On 6/09/18 7:22 PM, Matus UHLAR - fantomas wrote:
> On 06.09.18 02:40, Julian Perconti wrote:
> 
>> Is mandatory for squid to use only 1 dns/caching nameserver?
> 
> usually, people have multiple DNS servers configured to fail over in case
> one of them fails.
> in some cases, client can balance the load, or prefer server with faster
> responses.
> 
> There should be no problem of this kind, unless one of your DNS servers is
> broken.
> 

If it wasn't clear already, yes Squid can use multiple resolvers BUT
they need to be recursive resolvers and every one of them needs to be
able to resolve *all* possible domains Squid will ask about.

You cannot have one of them being authoritative for local domain names
and another for public queries. DNS don't work like that.

Amos




From squid3 at treenet.co.nz  Thu Sep  6 14:10:58 2018
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Fri, 7 Sep 2018 02:10:58 +1200
Subject: [squid-users] content adaptation using squid
In-Reply-To: <1536214008679-0.post@n4.nabble.com>
References: <1536214008679-0.post@n4.nabble.com>
Message-ID: <8843243b-e5f7-efe7-52e8-28a77f648669@treenet.co.nz>

On 6/09/18 6:06 PM, Yosef Meltser wrote:
> 
> Hi,
> 
> We have managed to create a proxy server using a squid in an intercept mode.
> 
> Now we would like to make a content adaptation, for example to show an alert
> every time the user entered a website.
> 
> We are not familiar with this domain, so we are looking for the easiest way
> to inject java script in the http response (of course only http sites).
> 

Simply put. Do not do that.

Please be aware that despite the content HTTP transferring being public
it is still under copyright by the authors who created it and/or the
owners of the domain where it is fetched from. They have implicitly
granted rights only to *view* and distribute the content as-is.

Please check with your legal department about the consequences of
altering copyright content without the copyright holders permission. The
Berne Convention copyright treaty forbidding this type of "piracy"
covers most countries, and the remainder usually have even more harsh
laws of their own.


A much better approach is a "splash page" - to have your proxy respond
with a 302 redirect to a page containing your message, and a
click-through link (or iframe, if that works) to the actual content.
That way the external content remains separate from your content and
everything is good.
 The traditional session splash page details can be found at
<https://wiki.squid-cache.org/ConfigExamples/Portal/Splash>.
 If you have existing system to manage when the messages are to be
displayed you may want the database session helper instead
<http://www.squid-cache.org/Versions/v4/manuals/ext_sql_session_acl.html> which
works from externally managed details in an SQL database.



>  The main two adaptation mechanisms are:
> 
> 1.       Icap
> 
> 2.       Ecap
> 
> For example, in Icap there are some server frameworks that we can use (like
> c-icap, ICAP-server and etc and etc), which one is the most recommended?
> 
> In contrast to the above, ECAP is not using any server, and the whole
> process in embedded into the squid, so it sounds quite easier. Is it?
> 

Neither is more easy or more difficult than the other. They are just
different ways to receive the traffic.


Amos


From rousskov at measurement-factory.com  Thu Sep  6 16:46:10 2018
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Thu, 6 Sep 2018 10:46:10 -0600
Subject: [squid-users] content adaptation using squid
In-Reply-To: <1536214008679-0.post@n4.nabble.com>
References: <1536214008679-0.post@n4.nabble.com>
Message-ID: <43b3d683-62ef-3e57-e835-8143762064b7@measurement-factory.com>

On 09/06/2018 12:06 AM, Yosef Meltser wrote:

> Now we would like to make a content adaptation, for example to show an alert
> every time the user entered a website.

If you have not already, please familiarize yourself with the content
adaptation problems. The following FAQ entry is a good starting point:
https://answers.launchpad.net/ecap/+faq/1793

Splash pages (mentioned by Amos) can be a form of content adaptation and
several problems in the above FAQ entry apply to them as well.


> For example, in Icap there are some server frameworks that we can use (like
> c-icap, ICAP-server and etc and etc), which one is the most recommended?

There is no general winner AFAICT. All servers have their pros and cons
(cost, ease of management, performance, ease of modification, quality of
support, licensing restrictions, etc.). Use whatever works best for your
needs. For example, the server you use for prototyping may differ from
the server you use as a long-term solution.


> In contrast to the above, ECAP is not using any server, and the whole
> process in embedded into the squid, so it sounds quite easier. Is it?

Overall, there is no drastic complexity difference between

* embedding your code into an ECAP-capable host like Squid and
* embedding your code into an ICAP server like c-icap.

In some cases, even the APIs are going to be similar! Some ICAP servers
are probably easier to work with than eCAP, and some are probably more
cumbersome, but you are doing the same work on the conceptual level --
writing a "plugin" where your adaptation code/logic lives.

An ICAP server introduces an additional stand-alone service/software
that you have to take care of, of course.

Needless to say, writing an eCAP adapter is much easier than writing a
(good) ICAP server -- writing an ICAP server from scratch is usually the
wrong answer.


HTH,

Alex.


From vh1988 at yahoo.com.ar  Thu Sep  6 16:53:50 2018
From: vh1988 at yahoo.com.ar (Julian Perconti)
Date: Thu, 6 Sep 2018 13:53:50 -0300
Subject: [squid-users] Squid and DNS
In-Reply-To: <fdee4933-e3f6-67d4-fc66-2a591145052f@treenet.co.nz>
References: <000401d445a4$1ec7a320$5c56e960$@yahoo.com.ar>
 <20180906072243.GA9867@fantomas.sk>
 <fdee4933-e3f6-67d4-fc66-2a591145052f@treenet.co.nz>
Message-ID: <000301d44602$31a20e50$94e62af0$@yahoo.com.ar>

> De: squid-users <squid-users-bounces at lists.squid-cache.org> En nombre de
> Amos Jeffries
> Enviado el: jueves, 6 de septiembre de 2018 09:57
> Para: squid-users at lists.squid-cache.org
> Asunto: Re: [squid-users] Squid and DNS
> 
> On 6/09/18 7:22 PM, Matus UHLAR - fantomas wrote:
> > On 06.09.18 02:40, Julian Perconti wrote:
> >
> >> Is mandatory for squid to use only 1 dns/caching nameserver?
> >
> > usually, people have multiple DNS servers configured to fail over in
> > case one of them fails.
> > in some cases, client can balance the load, or prefer server with
> > faster responses.
> >
> > There should be no problem of this kind, unless one of your DNS
> > servers is broken.
> >
> 
> If it wasn't clear already, yes Squid can use multiple resolvers BUT they need
> to be recursive resolvers and every one of them needs to be able to resolve
> *all* possible domains Squid will ask about.
> 
> You cannot have one of them being authoritative for local domain names and
> another for public queries. DNS don't work like that.

So squid can not use one resolver for a local and public domains/addresses and other or a second resolver to only public domains/ip? Both recursive resolvers.

Am i right?

> 
> Amos
> 
> 
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users



From squid3 at treenet.co.nz  Thu Sep  6 17:05:56 2018
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Fri, 7 Sep 2018 05:05:56 +1200
Subject: [squid-users] Squid and DNS
In-Reply-To: <000301d44602$31a20e50$94e62af0$@yahoo.com.ar>
References: <000401d445a4$1ec7a320$5c56e960$@yahoo.com.ar>
 <20180906072243.GA9867@fantomas.sk>
 <fdee4933-e3f6-67d4-fc66-2a591145052f@treenet.co.nz>
 <000301d44602$31a20e50$94e62af0$@yahoo.com.ar>
Message-ID: <d967df40-1ea8-7e9c-c913-a7489d26bd69@treenet.co.nz>

On 7/09/18 4:53 AM, Julian Perconti wrote:
>> De: squid-users <squid-users-bounces at lists.squid-cache.org> En nombre de
>> Amos Jeffries
>> Enviado el: jueves, 6 de septiembre de 2018 09:57
>> Para: squid-users at lists.squid-cache.org
>> Asunto: Re: [squid-users] Squid and DNS
>>
>> On 6/09/18 7:22 PM, Matus UHLAR - fantomas wrote:
>>> On 06.09.18 02:40, Julian Perconti wrote:
>>>
>>>> Is mandatory for squid to use only 1 dns/caching nameserver?
>>>
>>> usually, people have multiple DNS servers configured to fail over in
>>> case one of them fails.
>>> in some cases, client can balance the load, or prefer server with
>>> faster responses.
>>>
>>> There should be no problem of this kind, unless one of your DNS
>>> servers is broken.
>>>
>>
>> If it wasn't clear already, yes Squid can use multiple resolvers BUT they need
>> to be recursive resolvers and every one of them needs to be able to resolve
>> *all* possible domains Squid will ask about.
>>
>> You cannot have one of them being authoritative for local domain names and
>> another for public queries. DNS don't work like that.
> 
> So squid can not use one resolver for a local and public domains/addresses and other or a second resolver to only public domains/ip? Both recursive resolvers.
> 

Correct.


Amos


From vh1988 at yahoo.com.ar  Thu Sep  6 21:43:39 2018
From: vh1988 at yahoo.com.ar (Julian Perconti)
Date: Thu, 6 Sep 2018 18:43:39 -0300
Subject: [squid-users] Squid and DNS
In-Reply-To: <d967df40-1ea8-7e9c-c913-a7489d26bd69@treenet.co.nz>
References: <000401d445a4$1ec7a320$5c56e960$@yahoo.com.ar>
 <20180906072243.GA9867@fantomas.sk>
 <fdee4933-e3f6-67d4-fc66-2a591145052f@treenet.co.nz>
 <000301d44602$31a20e50$94e62af0$@yahoo.com.ar>
 <d967df40-1ea8-7e9c-c913-a7489d26bd69@treenet.co.nz>
Message-ID: <001b01d4462a$ae86fd40$0b94f7c0$@yahoo.com.ar>

> > So squid can not use one resolver for a local and public domains/addresses
> and other or a second resolver to only public domains/ip? Both recursive
> resolvers.
> >
> 
> Correct.

Thank you for the clarification.

> 
> 
> Amos
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users



From vh1988 at yahoo.com.ar  Fri Sep  7 01:48:02 2018
From: vh1988 at yahoo.com.ar (Julian Perconti)
Date: Thu, 6 Sep 2018 22:48:02 -0300
Subject: [squid-users] About SSL peek-n-splice/bump configurations
In-Reply-To: <6eb5660d-8025-eedd-3ed9-47d30bf22e62@measurement-factory.com>
References: <000301d43289$14ed4770$3ec7d650$@yahoo.com.ar>
 <58014777-2b9c-7630-80dd-877c2d2af9d2@measurement-factory.com>
 <001801d432a0$8d102a30$a7307e90$@yahoo.com.ar>
 <6eb5660d-8025-eedd-3ed9-47d30bf22e62@measurement-factory.com>
Message-ID: <001601d4464c$d1b38910$751a9b30$@yahoo.com.ar>

> De: Alex Rousskov <rousskov at measurement-factory.com>
> Enviado el: lunes, 13 de agosto de 2018 02:01
> Para: Julian Perconti <vh1988 at yahoo.com.ar>; squid-users at lists.squid-
> cache.org
> Asunto: Re: [squid-users] About SSL peek-n-splice/bump configurations
> 
> On 08/12/2018 06:57 PM, Julian Perconti wrote:
> >> De: Alex Rousskov <rousskov at measurement-factory.com>
> >> Enviado el: domingo, 12 de agosto de 2018 20:50
> >> Para: Julian Perconti <vh1988 at yahoo.com.ar>;
> >> squid-users at lists.squid-cache.org
> >> Asunto: Re: [squid-users] About SSL peek-n-splice/bump configurations
> >>
> >> On 08/12/2018 04:09 PM, Julian Perconti wrote:
> >>> I would like to know which of these two cfg's are "better" or "more
> secure"
> >>> when a site/domain is spliced, bumped, etc.
> 
> >> It is impossible to answer that question without knowing how _you_
> >> define "better" or "more secure".
> 
> 
> > I tried to meant, "security" from the client-side accessing to a
> > non-bumped or spliced site, i.g.: bank website... client-side
> > "privacy" or an a -real- man-in-the-middle attack due to squid in the
> > middle.
> 
> A splicing Squid does not perform a man-in-the-middle attack on TLS or HTTP
> traffic. It essentially acts as a TCP/IP-level proxy and can log TLS handshake
> details. In some environments, doing all that improves "privacy" and
> "security". In others, it makes things worse (for some definition of "privacy"
> and "security").
> 
> A bumping Squid performs a man-in-the-middle attack on TLS traffic.
> After a successful attack, it essentially acts as an HTTP-level proxy and can log
> or even alter TLS and HTTP traffic. In some environments, doing all that
> improves "privacy" and "security" (for some definition of "privacy" and
> "security"). In others, it makes things worse.
> 
> You would have to ask a much more specific question to get a more specific
> (but still correct) answer.
> 
> 
> > Is well-known that there is no system /network/o.s. 100% secure but, I
> > dont know why, I always thought or stil think that with a https
> > proxy/filtering, the security or "the things" tooggles more risky if
> > this one did not exist. Even squid 100% correctly configured and
> > server well secured.
> 
> There are examples where deploying a splicing or even bumping Squid
> improves security of the humans and/or machines that are trusting Squid to
> examine and/or police their traffic. There are counter-examples as well. And
> I am sure that many installations can be viewed as both, depending on who
> gets to define "privacy", "security", and the "right balance" between the
> two.
> 
> 
> > What does squid when I dont specify the step?
> 
> Bugs notwithstanding, Squid should either
> 
> * bump if you were staring during the previous (explicitly configured) step or
> 
> * splice otherwise (including cases when no previous step was explicitly
> configured or existed).
> 
> I would not rely on this (correct) behavior without testing (at least) your
> Squid version (at least). I know that early SslBump implementations had bugs
> in that area.
> 
> 
> > For example:
> >
> > What does squid do with..:
> > ssl_bump splice step3 noBumpSites
> 
> Assuming there are no other rules, Squid should splice at step1 (see the
> "splice otherwise" rule above).
> 
> 
> > ...And what it do instead with this?:
> > ssl_bump splice noBumpSites
> 
> Assuming there are no other rules, Squid should splice at step1. It will do that
> when noBumpSites matches (naturally) and if noBumpSites does not match
> (per the "splice otherwise" rule above).
> 
> 
> > So, Would You prefer option 2?
> 
> Sorry, I cannot answer this question -- too many unknown variables. It is like
> asking a doctor whether she prefers to treat the patient with drug A or drug
> B when the doctor does not know what the patient is suffering from and
> what the patient's treatment preferences/goals are.
> 
> 
> >>> with Option 1 I don't see the domain in "TUNNEL" line, just the IP
> >>> addr.)
> 
> >> I doubt that is how it is supposed to work. When splicing, Option 1
> >> should have the same or more information so it should log the domain
> >> name if Option 2 has the domain name. If you are comparing log lines
> >> for identical transactions, then this could be a Squid bug.
> 
> > I dont know, I just tell what happen in the access.log when I
> > switching between these ssl_bump configs.
> 
> Yes, and I am just describing what should be happening (IMO). If what is
> actually happening bothers you, and it does not match what should be
> happening, and nobody comes up with a better explanation, then consider
> filing a bug report and working with developers to address the problem.
> 
> 
> HTH,
> 
> Alex.

Hi all,

I have a new strange situation:

With this peek-n-splice configuration:

ssl_bump peek step1 all
ssl_bump peek step2 noBumpSites
ssl_bump splice step3 noBumpSites
ssl_bump bump

I got this error on spliced sites (a bank site):

The system return in the browser this error: (chrome 69):

(104) Connection reset by peer (TLS code: SQUID_ERR_SSL_HANDSHAKE)
Handshake with SSL server failed: [No Error]

This proxy and the remote host failed to negotiate a mutually acceptable security settings for handling your request. It is possible that the remote host does not support secure connections, or the proxy is not satisfied with the host security credentials.

cache.log:
2018/09/06 22:40:36 kid1| ERROR: negotiating TLS on FD 44: error:00000000:lib(0):func(0):reason(0) (5/-1/104)

But if i change the ssl bump(s) directive to:

ssl_bump peek step1
ssl_bump splice noBumpSites
ssl_bump bump all

I can Access to spliced site and no any kind of errors in access.log

Any idea?

Thanks in advance




From squid3 at treenet.co.nz  Fri Sep  7 04:18:10 2018
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Fri, 7 Sep 2018 16:18:10 +1200
Subject: [squid-users] About SSL peek-n-splice/bump configurations
In-Reply-To: <001601d4464c$d1b38910$751a9b30$@yahoo.com.ar>
References: <000301d43289$14ed4770$3ec7d650$@yahoo.com.ar>
 <58014777-2b9c-7630-80dd-877c2d2af9d2@measurement-factory.com>
 <001801d432a0$8d102a30$a7307e90$@yahoo.com.ar>
 <6eb5660d-8025-eedd-3ed9-47d30bf22e62@measurement-factory.com>
 <001601d4464c$d1b38910$751a9b30$@yahoo.com.ar>
Message-ID: <770057a5-3808-3fdc-5bf7-f78d09705913@treenet.co.nz>

On 7/09/18 1:48 PM, Julian Perconti wrote:>
> Hi all,
> 
> I have a new strange situation:
> 
> With this peek-n-splice configuration:
> 
> ssl_bump peek step1 all
> ssl_bump peek step2 noBumpSites
> ssl_bump splice step3 noBumpSites
> ssl_bump bump

So... (lets call this config A)

#step1 does this:

> ssl_bump peek step1 all

#step2 does this:

> ssl_bump peek step2 noBumpSites
> ssl_bump bump

If the bump at step2 happened, there is no step3.

#step3 does this:

> ssl_bump splice step3 noBumpSites



> 
> I got this error on spliced sites (a bank site):
> 
> The system return in the browser this error: (chrome 69):
> 
> (104) Connection reset by peer (TLS code: SQUID_ERR_SSL_HANDSHAKE)
> Handshake with SSL server failed: [No Error]
> 
> This proxy and the remote host failed to negotiate a mutually acceptable security settings for handling your request. It is possible that the remote host does not support secure connections, or the proxy is not satisfied with the host security credentials.
> 
> cache.log:
> 2018/09/06 22:40:36 kid1| ERROR: negotiating TLS on FD 44: error:00000000:lib(0):func(0):reason(0) (5/-1/104)
> 
> But if i change the ssl bump(s) directive to:
> 
> ssl_bump peek step1
> ssl_bump splice noBumpSites
> ssl_bump bump all
> 

So ... (lets call this config B)

#step does this:

> ssl_bump peek step1

#step2 does this:

> ssl_bump splice noBumpSites
> ssl_bump bump all

Notice there is never any step3, and the splice in this ruleset happens
at step2.


So config (A) is trying to do a step3 (handshake with server) when it
has only peek'ed and relayed the clientHello as-is (including any secret
tokens an unknown features the client is trying to use). The bump action
is bound to fail.
 ** "stare" is the action which sets up and filters the handshake ready
for bump action at step3 (server handshake with TLS features Squid knows
how to handle).


The config (B) bumps at step2. That is what the old and very broken
"client-first" behaviour used to be. It does not produce any errors from
the proxy BUT leads directly to a huge pile of security vulnerabilities
and nasty side effects that may never be seen by you. Use at your own risk.



> I can Access to spliced site and no any kind of errors in access.log
> 
> Any idea?

Have you read the documentation?
 <https://wiki.squid-cache.org/Features/SslPeekAndSplice>

Break your rules down into the stages as I have above and what is going
on becomes a bit more clear.

Then you can consider what ssl_bump is doing in terms of what info Squid
has available.
 step1: TCP IP:port or CONNECT URI (forward-proxy only)
 step2: TLS clientHello + TLS SNI (if any)
 step3: TLS serverHello + server cert

The entire directive set is interpreted from top-to-bottom left-to-right
each step. First line to fully match is what happens for that step.


Amos


From rousskov at measurement-factory.com  Fri Sep  7 04:20:21 2018
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Thu, 6 Sep 2018 22:20:21 -0600
Subject: [squid-users] About SSL peek-n-splice/bump configurations
In-Reply-To: <001601d4464c$d1b38910$751a9b30$@yahoo.com.ar>
References: <000301d43289$14ed4770$3ec7d650$@yahoo.com.ar>
 <58014777-2b9c-7630-80dd-877c2d2af9d2@measurement-factory.com>
 <001801d432a0$8d102a30$a7307e90$@yahoo.com.ar>
 <6eb5660d-8025-eedd-3ed9-47d30bf22e62@measurement-factory.com>
 <001601d4464c$d1b38910$751a9b30$@yahoo.com.ar>
Message-ID: <91d0f215-18cc-c904-aa48-523ab5ce7b39@measurement-factory.com>

On 09/06/2018 07:48 PM, Julian Perconti wrote:

> With this peek-n-splice configuration:
> 
> ssl_bump peek step1 all
> ssl_bump peek step2 noBumpSites
> ssl_bump splice step3 noBumpSites
> ssl_bump bump
> 
> I got this error on spliced sites (a bank site):

> (104) Connection reset by peer (TLS code: SQUID_ERR_SSL_HANDSHAKE)
> Handshake with SSL server failed: [No Error]

> 2018/09/06 22:40:36 kid1| ERROR: negotiating TLS on FD 44: error:00000000:lib(0):func(0):reason(0) (5/-1/104)

OK, so the origin server is resetting a connection when Squid talk to
it. Does that happen during the peek (step2), splice (step3), or bump
(step3) rule?

One way to answer that question is to post an ALL,9 log of the isolated
failing transaction for a developer to tell you what is going on.

Another way is to replace a suspected rule with, say, an "ssl_bump
terminate all" rule and see if anything changes (going from the last
suspected rule up towards the first one until something does change).

There are other ways as well, of course.


> But if i change the ssl bump(s) directive to:
> 
> ssl_bump peek step1
> ssl_bump splice noBumpSites
> ssl_bump bump all
> 
> I can Access to spliced site and no any kind of errors in access.log
> 
> Any idea?

My working theory is that your noBumpSites (i.e. ssl::server_name_regex)
ACL matches at step2 but does not match at step3. That is just a guess,
and, even if it is correct, it does not fully explain what Squid does in
that case and why the peer resets the connection.


HTH,

Alex.


From rousskov at measurement-factory.com  Fri Sep  7 04:47:15 2018
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Thu, 6 Sep 2018 22:47:15 -0600
Subject: [squid-users] About SSL peek-n-splice/bump configurations
In-Reply-To: <770057a5-3808-3fdc-5bf7-f78d09705913@treenet.co.nz>
References: <000301d43289$14ed4770$3ec7d650$@yahoo.com.ar>
 <58014777-2b9c-7630-80dd-877c2d2af9d2@measurement-factory.com>
 <001801d432a0$8d102a30$a7307e90$@yahoo.com.ar>
 <6eb5660d-8025-eedd-3ed9-47d30bf22e62@measurement-factory.com>
 <001601d4464c$d1b38910$751a9b30$@yahoo.com.ar>
 <770057a5-3808-3fdc-5bf7-f78d09705913@treenet.co.nz>
Message-ID: <8264c402-9b3c-5da7-0d2a-9d7b0d66952f@measurement-factory.com>

On 09/06/2018 10:18 PM, Amos Jeffries wrote:

> So... (lets call this config A)
> 
> #step1 does this:
> 
>> ssl_bump peek step1 all
> 
> #step2 does this:
> 
>> ssl_bump peek step2 noBumpSites
>> ssl_bump bump
> 
> If the bump at step2 happened, there is no step3.
> 
> #step3 does this:
> 
>> ssl_bump splice step3 noBumpSites

You should also add this to step3 if the bump did not happen at step2
(i.e. if noBumpSites matched at step2):

   ssl_bump bump


> So ... (lets call this config B)
> 
> #step does this:
> 
>> ssl_bump peek step1
> 
> #step2 does this:
> 
>> ssl_bump splice noBumpSites
>> ssl_bump bump all
> 
> Notice there is never any step3, and the splice in this ruleset happens
> at step2.

Correct (assuming noBumpSites matches at step2).


> So config (A) is trying to do a step3 (handshake with server) when it
> has only peek'ed and relayed the clientHello as-is (including any secret
> tokens an unknown features the client is trying to use).

The handshake with the server actually started during step2 (SSL Hellos
were exchanged). The SSL negotiation (if any) completes during step3,
most likely without Squid participation (because Squid is just splicing
at TCP level). If Squid does try to participate in that SSL negotiation,
then it is a Squid bug (see below), and, like you said below, the
handshake will fail because Squid has forwarded client information
without knowing client secrets.


> The bump action is bound to fail.

IIRC, the bump action should be ignored by Squid at step3 because it
becomes banned by peeking at step2. Bugs notwithstanding,
banned/impossible actions should be ignored. Instead, the connections
should be spliced at step3 if no rule matches at step3 because the last
matching rule at step2 was "peek", indicating a splicing intent. Again,
I do not know whether the Squid actually does what it is supposed to do
in this case.


> The config (B) bumps at step2.

It also splices banks at step2. It is possible that the working test
case is spliced at step2 when using config B (and that is why it works).


> Then you can consider what ssl_bump is doing in terms of what info Squid
> has available.
>  step1: TCP IP:port or CONNECT URI (forward-proxy only)
>  step2: TLS clientHello + TLS SNI (if any)
>  step3: TLS serverHello + server cert
> 
> The entire directive set is interpreted from top-to-bottom left-to-right
> each step. First line to fully match is what happens for that step.

Correct.


Cheers,

Alex.


From eliezer at ngtech.co.il  Fri Sep  7 09:29:28 2018
From: eliezer at ngtech.co.il (Eliezer Croitoru)
Date: Fri, 7 Sep 2018 12:29:28 +0300
Subject: [squid-users] content adaptation using squid
In-Reply-To: <43b3d683-62ef-3e57-e835-8143762064b7@measurement-factory.com>
References: <1536214008679-0.post@n4.nabble.com>
 <43b3d683-62ef-3e57-e835-8143762064b7@measurement-factory.com>
Message-ID: <017f01d4468d$497626c0$dc627440$@ngtech.co.il>

The list of the known ICAP servers that I can list are:
- https://github.com/elico/icap
- https://github.com/jburnim/GreasySpoon
- c-icap
- echelon-mod (Which I wrote from 0 but is not public)
- https://github.com/elico/drbl-icap-service

I wrote both eCAP and ICAP services/modules code and I found it very easy to start with an ICAP service.
Then and only if the ICAP service seems to make sense and works as expected try to asses either:
- eCAP module
- Proxy Service
- Other creative idea

I have worked couple years ago on a tproxy based p2p caching software but eventually erased the code...
It's nice to have some cash from cache but I like my green/white hat rather then the black one.

All The Bests,
Eliezer

----
Eliezer Croitoru
Linux System Administrator
Mobile: +972-5-28704261
Email: eliezer at ngtech.co.il


-----Original Message-----
From: squid-users [mailto:squid-users-bounces at lists.squid-cache.org] On Behalf Of Alex Rousskov
Sent: Thursday, September 6, 2018 7:46 PM
To: squid-users at lists.squid-cache.org
Subject: Re: [squid-users] content adaptation using squid

On 09/06/2018 12:06 AM, Yosef Meltser wrote:

<SNIP>


> In contrast to the above, ECAP is not using any server, and the whole
> process in embedded into the squid, so it sounds quite easier. Is it?

Overall, there is no drastic complexity difference between

* embedding your code into an ECAP-capable host like Squid and
* embedding your code into an ICAP server like c-icap.

In some cases, even the APIs are going to be similar! Some ICAP servers
are probably easier to work with than eCAP, and some are probably more
cumbersome, but you are doing the same work on the conceptual level --
writing a "plugin" where your adaptation code/logic lives.

An ICAP server introduces an additional stand-alone service/software
that you have to take care of, of course.

Needless to say, writing an eCAP adapter is much easier than writing a
(good) ICAP server -- writing an ICAP server from scratch is usually the
wrong answer.


HTH,

Alex.
_______________________________________________
squid-users mailing list
squid-users at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-users



From sekarit at gmail.com  Fri Sep  7 12:36:52 2018
From: sekarit at gmail.com (Sekar Duraisamy)
Date: Fri, 7 Sep 2018 18:06:52 +0530
Subject: [squid-users] Could compile squid with --enable-storeio
Message-ID: <CADfQnU3dXTVvc+E49bRu7SJiza=Yr4-g7TEQ7uo+7xw8C5Rn0w@mail.gmail.com>

Hi ,

I am trying to compile squid with --enable-storeio=null but it is giving error.

"configure: IO Modules built:  AIO Blocking DiskDaemon DiskThreads IpcIo Mmapped
configure: error: null not found in ./src/fs"

squid Version : squid-3.5.20 and squid-3.5.27

Basically I am trying to disable cache on squid completely with  below

cache_dir null /tmp

So please help me on this to move forward.

Thanks


From alex at dvm.esines.cu  Fri Sep  7 12:47:16 2018
From: alex at dvm.esines.cu (=?UTF-8?Q?Alex_Guti=c3=a9rrez_Mart=c3=adnez?=)
Date: Fri, 7 Sep 2018 08:47:16 -0400
Subject: [squid-users] css not loading
Message-ID: <8fb050b6-587b-b985-e8cb-116545406e00@dvm.esines.cu>

Hello community, 3 days ago my bosses asked me to optimize access to 
several pages prioritized in the company where i work, luckily for me 
these pages are accessible via http. The problem is that after i put the 
rules to cache the pages do not load the CSS and look in pure html. 
After reversing the changes and doing a "squid -z" the problem persists. 
Here i leave the configuration that i used to keep the pages in cache.


#########################################################################
#Cache #
#########################################################################
delay_initial_bucket_level 75
maximum_object_size 32 MB
#cache_dir aufs /var/cache/squid 10240 16 256
cache_dir aufs /opt/squid 1024 16 256
cache_mem 256 MB
cache_store_log /opt/squid/cache_store.log
coredump_dir /opt/squid/dump
minimum_expiry_time 550 seconds
############################
#uso cache
###########################
client_db off
offline_mode off
cache_swap_low 93
cache_swap_high 97
cache_replacement_policy heap LUDFA
memory_replacement_policy heap GDSF
maximum_object_size_in_memory 512 KB
half_closed_clients off
###############################################################################
# establecemos los archivos de volcado en /var/cache/squid/
coredump_dir /opt/squid/
###############################################################################
#Establecemos los patrones de refrescamiento de la cache #
#patron de refrescamiento -- tipo de archivo -- tiempo del objeto -- %de 
refrescamiento -- tiempo #
#1440 minutos equivalen a 24 horas #
################################
#Refrescamiento de la cache
################################
refresh_pattern ^ftp: 1440 20% 4320
refresh_pattern ^gopher: 1440 0% 4320
refresh_pattern -i \.(gif|png|jpg|jpeg|ico)$ 1440 90% 4320 
override-expire ignore-no-store ignore-private
refresh_pattern -i \.(iso|avi|wav|mp3|mp4|mpeg|swf|flv|x-flv)$ 1440 90% 
43200 override-expire ignore-no-store ignore-private
refresh_pattern -i 
\.(deb|exe|zip|tar|tgz|ram|rar|bin|ppt|doc|tiff|pdf|xls|docx|xlsx|pptx)$ 
1440 90% 4320 override-expire ignore-no-store ignore-private
refresh_pattern -i \.index.(html|htm)$ 1440 10% 4320
refresh_pattern -i \.(html|htm|css|js)$ 1440 10% 4320
################################
#prioritized in the company
################################
refresh_pattern \.web1\.org\/? 1440 99% 14400 override-expire 
override-lastmod ignore-reload ignore-private
refresh_pattern \.web2\.org\\/? 1440 99% 14400 override-expire 
override-lastmod ignore-reload ignore-private
refresh_pattern \.web3\.org\\/? 1440 99% 14400 override-expire 
override-lastmod ignore-reload ignore-private


PD: This is not a reverse proxy, i ony try to gain the best of my 
internet connection


Thanks in advance

-- 
Saludos Cordiales

Lic. Alex Guti?rrez Mart?nez

Tel. +53 7 2710327

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20180907/9c50f8a5/attachment.htm>

From squid3 at treenet.co.nz  Fri Sep  7 13:10:44 2018
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sat, 8 Sep 2018 01:10:44 +1200
Subject: [squid-users] Could compile squid with --enable-storeio
In-Reply-To: <CADfQnU3dXTVvc+E49bRu7SJiza=Yr4-g7TEQ7uo+7xw8C5Rn0w@mail.gmail.com>
References: <CADfQnU3dXTVvc+E49bRu7SJiza=Yr4-g7TEQ7uo+7xw8C5Rn0w@mail.gmail.com>
Message-ID: <694b6e1b-410b-7e09-1108-57396c9008dc@treenet.co.nz>

On 8/09/18 12:36 AM, Sekar Duraisamy wrote:
> Hi ,
> 
> I am trying to compile squid with --enable-storeio=null but it is giving error.
> 
> "configure: IO Modules built:  AIO Blocking DiskDaemon DiskThreads IpcIo Mmapped
> configure: error: null not found in ./src/fs"
> 

Indeed. "null" is not a way to do I/O.

>From the Squid-3.1 release notes:
"
cache_dir

 Default changed to 256MB in-memory cache. see cache_mem and
maximum_object_size_in_memory for size parameters.

 'null' storage type dropped. In-memory cache is always present. Remove
all cache_dir options to prevent on-disk caching.
"



> squid Version : squid-3.5.20 and squid-3.5.27
> 
> Basically I am trying to disable cache on squid completely with  below
> 
> cache_dir null /tmp
> 
> So please help me on this to move forward.


Remove that cache_dir line from your config.

NP: The above config has never had any effect on other types of caching.
So "disable cache on squid completely" is not what you have been doing.

To completely disable caching of HTTP objects you need:
  cache_mem 0
  cache deny all

and since Squid-3.1 *no* cache_dir lines.


Amos


From squid3 at treenet.co.nz  Fri Sep  7 14:54:02 2018
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sat, 8 Sep 2018 02:54:02 +1200
Subject: [squid-users] css not loading
In-Reply-To: <8fb050b6-587b-b985-e8cb-116545406e00@dvm.esines.cu>
References: <8fb050b6-587b-b985-e8cb-116545406e00@dvm.esines.cu>
Message-ID: <0474a8b1-1d9e-1350-582c-1163ac39254b@treenet.co.nz>

On 8/09/18 12:47 AM, Alex Guti?rrez Mart?nez wrote:
> Hello community, 3 days ago my bosses asked me to optimize access to
> several pages prioritized in the company where i work, luckily for me
> these pages are accessible via http. The problem is that after i put the
> rules to cache the pages do not load the CSS and look in pure html.
> After reversing the changes and doing a "squid -z" the problem persists.

squid -z only validates and repairs the *format* of the cache. It does
not affect content stored there.

If (and only if) the content was forced to stay in cache past its normal
expiry time by your rules. Then the change in config would normally
cause it to be dropped from cache on next use.

You can expedite that (for ufs, aufs, diskd caches only) by finding the
cache file containing the object and deleting it. Then restarting Squid
to clear the memory copies of things.

NP: the above all assumes that there is actually an object in cache
being the problem. There are a number of other things which can lead to
identical "missing CSS" issues.


> Here i leave the configuration that i used to keep the pages in cache.
> 

What rules exactly are the new ones you added?

There are several problems and mistakes in the below config. I have
pointed them out, but it is not clear what rules you had working (or
*appearing* to work) and what you added which "broke" things - so a
direct answer to your question is not possible with only this config to
work from.


> 
> #########################################################################
> #Cache #
> #########################################################################
> delay_initial_bucket_level 75
> maximum_object_size 32 MB
> #cache_dir aufs /var/cache/squid 10240 16 256
> cache_dir aufs /opt/squid 1024 16 256

> cache_mem 256 MB

This is the default cache_mem value for Squid-3. No need to configure it.

> cache_store_log /opt/squid/cache_store.log

Only useful if you are debugging storage problems (ie right now perhapse
- if you can understand what is being logged), usually just a waste of
I/O delays in regular proxy usage. So keep that in mind for optimization
later.

> coredump_dir /opt/squid/dump
> minimum_expiry_time 550 seconds

If you check the docs for this directive, notice the text talks about
situations where things are improved by making this timeout *shorter*
than the default.

Making it _long_ increases the chances that things will *not* be cached.
It is used in the event that server revalidation failed (some upstream
error occured). Only objects in cache whose TTL is _over_ this *minimum*
value will be honoured (eg treated as still valid). Things with shorter
TTL will be treated as stale (expired already) - and probably discarded.

So you want this to be relatively short, but not 0 for a regular proxy.
IIRC, it is set to 60s to allow quick detection of server errors when
they provide objects with very short (or zero) lifetimes, while allowing
other servers to have slightly slower auto-recovery form failures.


> ############################
> #uso cache
> ###########################
> client_db off
> offline_mode off

NP: offline_mode is not what its often thought to be. Since Squid-3.2
the HTTP/1.1 proxies pretty much do by default what this directive
caused the older HTTP/1.0-only Squid versions to do. What is left are
some very narrow and specific use-cases (eg migrating cache between two
proxies without downtime, or an origin server upgrade scenario).

OFF is also its default value, so best to just completely remove the
above directive from your config.


> cache_swap_low 93
> cache_swap_high 97
> cache_replacement_policy heap LUDFA
> memory_replacement_policy heap GDSF
> maximum_object_size_in_memory 512 KB
> half_closed_clients off
> ###############################################################################
> # establecemos los archivos de volcado en /var/cache/squid/
> coredump_dir /opt/squid/
> ###############################################################################
> #Establecemos los patrones de refrescamiento de la cache #
> #patron de refrescamiento -- tipo de archivo -- tiempo del objeto -- %de
> refrescamiento -- tiempo #
> #1440 minutos equivalen a 24 horas #
> ################################
> #Refrescamiento de la cache
> ################################
> refresh_pattern ^ftp: 1440 20% 4320
> refresh_pattern ^gopher: 1440 0% 4320
> refresh_pattern -i \.(gif|png|jpg|jpeg|ico)$ 1440 90% 4320
> override-expire ignore-no-store ignore-private
> refresh_pattern -i \.(iso|avi|wav|mp3|mp4|mpeg|swf|flv|x-flv)$ 1440 90%
> 43200 override-expire ignore-no-store ignore-private
> refresh_pattern -i
> \.(deb|exe|zip|tar|tgz|ram|rar|bin|ppt|doc|tiff|pdf|xls|docx|xlsx|pptx)$
> 1440 90% 4320 override-expire ignore-no-store ignore-private
> refresh_pattern -i \.index.(html|htm)$ 1440 10% 4320
> refresh_pattern -i \.(html|htm|css|js)$ 1440 10% 4320
> ################################
> #prioritized in the company

NP: by placing these lines underneath the above file type lines any
requests which match the above rules are *not* affected in any way by
the below.


> ################################
> refresh_pattern \.web1\.org\/? 1440 99% 14400 override-expire
> override-lastmod ignore-reload ignore-private
> refresh_pattern \.web2\.org\\/? 1440 99% 14400 override-expire
> override-lastmod ignore-reload ignore-private
> refresh_pattern \.web3\.org\\/? 1440 99% 14400 override-expire
> override-lastmod ignore-reload ignore-private
> 

The '\\/?' in those regex means the path section of this domain URLs
always starts with at '\' character - which is not a valid
first-character for URL paths. I think you probably wanted '/?' instead,
just like web1.org pattern has.

.. BUT, since the '/?' means '/' is optional there is no point in even
having it part of the pattern - anything which is not a '/' is already
allowed to follow the 'g' when the optional bit is missing.

So ... all the above lines can be reduced to just one :
 refresh_pattern \.web[123]\.org 1440 99% 14400 ...

If you want that to match only the end of the domain name remove the '?'
since Squid always matches the regex against effective-URL where at
least one path '/' exists.


Also, since your patterns looking for filename extensions are all ended
with just the "$" anchor they cannot match any URLs where there is a
query-string after the filename.
 Things like "http://example.com/file.css?hello" will not have the .css
rules applied to them. If your website uses a CMS like Wordpress or
similar they add a session ID in query-string to all page sub-resources.
That may be part of your problem.

To avoid that problem use patterns like:
  \.(css|js|html)(\?.*)?$



I don't see the default "(/cgi-bin/|\?)" and "." patterns. Having those
at the end of your refresh_pattern rules can avoid a lot of weird
caching problems, especially with dynamic content coming from broken
servers.



Since you say these are company websites you probably have a better
contact with the developers managing them than anyone here. The absolute
best thing to do is encourage them to design the site with caching in
mind and send HTTP headers that optimize storage. That way your company
can leverage cost savings from HTTP caches all around the world for free
on your public traffic to those sites.
 If you just have your own proxy being optimized to store a badly
designed website, you give a false sense of the site "working" to people
internally. When actually the external visitors are seeing bad things
happen and have a terrible experience visiting your site(s).



Some things about these ignore-* and override-* options which many
people are not aware of:


1) ignore-private - does not make objects with CC:private headers become
HITs. It does allow those objects to be cached "privately", but Squid
revalidates them on every use to check that they are valid for the next
user wanting it.

While this might save (some) *bandwidth* expenses, it is definitely not
a speed boost. It trades that unlikely gain against storage space needed
by other cacheable content that might have better savings for bandwidth
and faster speed.


2) ignore-reload - not as useful as rumors have it. All it does is stop
client software (or users) from being able to tell the proxy that the
content in cache is broken/outdated and needing a reload.

I expect that while looking into this problem one of the first things
you did when seeing the broken CSS on the web page was hit the browser
refresh/reload button? well, yeah.


3) override-lastmod - this one is a bit odd. It pretty much does the
exact *opposite* of causing things to stay in cache.

The header tells the proxy cache how much _older_ the object was at the
time it was received by the cache than the Date header indicates. In
heuristics LM-factor consideration the older an object was at time of
arrival the longer it is likely to still be usable as a HIT.

So by ignoring the header, Squid actually thinks the object is younger
and more likely to need replacing (MISS / REFRESH) than it normally
would be.

NP: If the Last-Modified being presented by your company server(s) is
*SO* bad that it has to be overridden to get a good experience. Think
how that appears in the view of other external visitors.



Sorry this is so long. Cache optimization is not a simple topic.

HTH
Amos


From rousskov at measurement-factory.com  Fri Sep  7 15:03:20 2018
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Fri, 7 Sep 2018 09:03:20 -0600
Subject: [squid-users] css not loading
In-Reply-To: <8fb050b6-587b-b985-e8cb-116545406e00@dvm.esines.cu>
References: <8fb050b6-587b-b985-e8cb-116545406e00@dvm.esines.cu>
Message-ID: <e7e385b6-ce4c-b74a-0fd3-445de9ed7c27@measurement-factory.com>

On 09/07/2018 06:47 AM, Alex Guti?rrez Mart?nez wrote:

> The problem is that after i put the
> rules to cache[,] the pages do not load the CSS

Please clear the browser cache and then detail the above statement by
using browser debugging and Squid access.log:

* Does the browser send requests for those CSS resources to Squid? And,
if yes:

* Does the browser receive an error response from Squid when attempting
to load those CSS resources?

* Does the browser receive a successful response when loading those CSS
resources?

Posting access.log lines (if any) may be useful.


> After reversing the changes and doing a "squid -z" the problem persists.

"squid -z" alone does nothing when the cache storage was already
initialized. Did you delete the entire cache contents before running
"squid -z" and starting Squid?


HTH,

Alex.


From sekarit at gmail.com  Fri Sep  7 16:13:47 2018
From: sekarit at gmail.com (Sekar Duraisamy)
Date: Fri, 7 Sep 2018 21:43:47 +0530
Subject: [squid-users] Could compile squid with --enable-storeio
In-Reply-To: <694b6e1b-410b-7e09-1108-57396c9008dc@treenet.co.nz>
References: <CADfQnU3dXTVvc+E49bRu7SJiza=Yr4-g7TEQ7uo+7xw8C5Rn0w@mail.gmail.com>
 <694b6e1b-410b-7e09-1108-57396c9008dc@treenet.co.nz>
Message-ID: <CADfQnU2MP__MdwToeAA5vSPzZf=Q2PHr0UVO4NYnG9N_yXV18g@mail.gmail.com>

Thanks for your prompt reply.

I could see the below message on cache.log even after removed
cache_dir  from squid.conf

"2018/09/07 02:48:35| Set Current Directory to
/opt/squid/squid3527/var/cache/squid"

Is this normal or I need to do anything else when i restart the squid?

On Fri, Sep 7, 2018 at 6:40 PM, Amos Jeffries <squid3 at treenet.co.nz> wrote:
> On 8/09/18 12:36 AM, Sekar Duraisamy wrote:
>> Hi ,
>>
>> I am trying to compile squid with --enable-storeio=null but it is giving error.
>>
>> "configure: IO Modules built:  AIO Blocking DiskDaemon DiskThreads IpcIo Mmapped
>> configure: error: null not found in ./src/fs"
>>
>
> Indeed. "null" is not a way to do I/O.
>
> From the Squid-3.1 release notes:
> "
> cache_dir
>
>  Default changed to 256MB in-memory cache. see cache_mem and
> maximum_object_size_in_memory for size parameters.
>
>  'null' storage type dropped. In-memory cache is always present. Remove
> all cache_dir options to prevent on-disk caching.
> "
>
>
>
>> squid Version : squid-3.5.20 and squid-3.5.27
>>
>> Basically I am trying to disable cache on squid completely with  below
>>
>> cache_dir null /tmp
>>
>> So please help me on this to move forward.
>
>
> Remove that cache_dir line from your config.
>
> NP: The above config has never had any effect on other types of caching.
> So "disable cache on squid completely" is not what you have been doing.
>
> To completely disable caching of HTTP objects you need:
>   cache_mem 0
>   cache deny all
>
> and since Squid-3.1 *no* cache_dir lines.
>
>
> Amos
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users


From srnhari at gmail.com  Fri Sep  7 16:46:35 2018
From: srnhari at gmail.com (Hariharan Sethuraman)
Date: Fri, 7 Sep 2018 22:16:35 +0530
Subject: [squid-users] Squid 4.2 : caching is not working
Message-ID: <CANfnjgLuEfJdCj-xvi33om7ESRnFxS4zANpckV-t4UUSusv5Sw@mail.gmail.com>

Hi team,

I have created directories using squid -z and then triggered squid -f
/etc/squid/squid.conf -NYCd 1. Find (1) debug info below. And below (2) are
the cache directory and squid-config.

(1) - debug info:
squidclient -h localhost cache_object://localhost/ mgr:objects >>> this was
showing the entry when the download was going on and disappeared after the
download complete(~290MB) on the browser. When I checked the du of cache
directory, it is intact with 200KB
---------------------------------------------------------------------------
bash-4.4# squid -v
Squid Cache: Version 4.2
Service Name: squid
This binary uses LibreSSL 2.7.4. For legal restrictions on distribution see
https://www.openssl.org/source/license.html
configure options:  '--build=x86_64-alpine-linux-musl'
'--host=x86_64-alpine-linux-musl' '--prefix=/usr'
'--datadir=/usr/share/squid' '--sysconfdir=/etc/squid'
'--libexecdir=/usr/lib/squid' '--localstatedir=/var'
'--with-logdir=/var/log/squid' '--disable-strict-error-checking'
'--disable-arch-native' '--enable-removal-policies=lru,heap'
'--enable-auth-digest'
'--enable-auth-basic=getpwnam,NCSA,SMB,SMB_LM,RADIUS' '--enable-epoll'
'--enable-external-acl-helpers=file_userip,unix_group,wbinfo_group,session'
'--enable-auth-ntlm=fake,SMB_LM' '--enable-auth-negotiate=kerberos,wrapper'
'--disable-mit' '--enable-heimdal' '--enable-delay-pools'
'--enable-openssl' '--enable-ssl-crtd' '--enable-linux-netfilter'
'--enable-ident-lookups' '--enable-useragent-log' '--enable-cache-digests'
'--enable-referer-log' '--enable-async-io' '--enable-truncate'
'--enable-arp-acl' '--enable-htcp' '--enable-carp' '--enable-poll'
'--enable-follow-x-forwarded-for' '--with-large-files'
'--with-default-user=squid' '--with-openssl'
'build_alias=x86_64-alpine-linux-musl'
'host_alias=x86_64-alpine-linux-musl' 'CC=gcc' 'CFLAGS=-Os
-fomit-frame-pointer' 'LDFLAGS=-Wl,--as-needed' 'CPPFLAGS=-Os
-fomit-frame-pointer' 'CXXFLAGS=-Os -fomit-frame-pointer'


(2) - cache directory and squid-config
---------------------------------------------------------------------------
bash-4.4# ls /var/spool/squid/cache
00          02          04          06          08          0A          0C
        0E          swap.state
01          03          05          07          09          0B          0D
        0F
bash-4.4#
..
cache allow all
strip_query_terms off
..
cache_dir ufs /var/spool/squid/cache 2000 16 256
maximum_object_size 300 MB
..
range_offset_limit -1
..
url_rewrite_access allow all
url_rewrite_program  /usr/bin/python /usr/share/proxypass.py

http_access deny all
...
always_direct deny all

(a) Please let me know what am missing to enable cache.
(b) Also "squidclient -h localhost cache_object://localhost/ mgr:objects"
hope this command will show the entry even after caching.


Thanks,
Hari
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20180907/c768727e/attachment.htm>

From vh1988 at yahoo.com.ar  Fri Sep  7 16:58:59 2018
From: vh1988 at yahoo.com.ar (Julian Perconti)
Date: Fri, 7 Sep 2018 13:58:59 -0300
Subject: [squid-users] About SSL peek-n-splice/bump configurations
In-Reply-To: <770057a5-3808-3fdc-5bf7-f78d09705913@treenet.co.nz>
References: <000301d43289$14ed4770$3ec7d650$@yahoo.com.ar>
 <58014777-2b9c-7630-80dd-877c2d2af9d2@measurement-factory.com>
 <001801d432a0$8d102a30$a7307e90$@yahoo.com.ar>
 <6eb5660d-8025-eedd-3ed9-47d30bf22e62@measurement-factory.com>
 <001601d4464c$d1b38910$751a9b30$@yahoo.com.ar>
 <770057a5-3808-3fdc-5bf7-f78d09705913@treenet.co.nz>
Message-ID: <002001d446cc$148bb580$3da32080$@yahoo.com.ar>

> De: squid-users <squid-users-bounces at lists.squid-cache.org> En nombre de
> Amos Jeffries
> Enviado el: viernes, 7 de septiembre de 2018 01:18
> Para: squid-users at lists.squid-cache.org
> Asunto: Re: [squid-users] About SSL peek-n-splice/bump configurations
> 
> On 7/09/18 1:48 PM, Julian Perconti wrote:>
> > Hi all,
> >
> > I have a new strange situation:
> >
> > With this peek-n-splice configuration:
> >
> > ssl_bump peek step1 all
> > ssl_bump peek step2 noBumpSites
> > ssl_bump splice step3 noBumpSites
> > ssl_bump bump
> 
> So... (lets call this config A)
> 
> #step1 does this:
> 
> > ssl_bump peek step1 all
> 
> #step2 does this:
> 
> > ssl_bump peek step2 noBumpSites
> > ssl_bump bump
> 
> If the bump at step2 happened, there is no step3.
> 
> #step3 does this:
> 
> > ssl_bump splice step3 noBumpSites
> 
> 
> 
> >
> > I got this error on spliced sites (a bank site):
> >
> > The system return in the browser this error: (chrome 69):
> >
> > (104) Connection reset by peer (TLS code: SQUID_ERR_SSL_HANDSHAKE)
> > Handshake with SSL server failed: [No Error]
> >
> > This proxy and the remote host failed to negotiate a mutually acceptable
> security settings for handling your request. It is possible that the remote host
> does not support secure connections, or the proxy is not satisfied with the
> host security credentials.
> >
> > cache.log:
> > 2018/09/06 22:40:36 kid1| ERROR: negotiating TLS on FD 44:
> > error:00000000:lib(0):func(0):reason(0) (5/-1/104)
> >
> > But if i change the ssl bump(s) directive to:
> >
> > ssl_bump peek step1
> > ssl_bump splice noBumpSites
> > ssl_bump bump all
> >
> 
> So ... (lets call this config B)
> 
> #step does this:
> 
> > ssl_bump peek step1
> 
> #step2 does this:
> 
> > ssl_bump splice noBumpSites
> > ssl_bump bump all
> 
> Notice there is never any step3, and the splice in this ruleset happens at
> step2.
> 
> 
> So config (A) is trying to do a step3 (handshake with server) when it has only
> peek'ed and relayed the clientHello as-is (including any secret tokens an
> unknown features the client is trying to use). The bump action is bound to
> fail.
>  ** "stare" is the action which sets up and filters the handshake ready for
> bump action at step3 (server handshake with TLS features Squid knows how
> to handle).


So from http://marek.helion.pl/install/squid.html

We have this configs:

ssl_bump peek step1 all
ssl_bump peek step2 noBumpSites
ssl_bump splice step3 noBumpSites
ssl_bump stare step2
ssl_bump bump step3

Is better to use the above conf (staring at step2)? Because you said that bump at step2 is insecure.

Is the same if a I change the order of the above conf to:

ssl_bump peek step1 all
ssl_bump peek step2 noBumpSites
ssl_bump stare step2 <<< order changed
ssl_bump splice step3 noBumpSites
ssl_bump bump step3

> 
> 
> The config (B) bumps at step2. That is what the old and very broken "client-
> first" behaviour used to be. It does not produce any errors from the proxy
> BUT leads directly to a huge pile of security vulnerabilities and nasty side
> effects that may never be seen by you. Use at your own risk.
> 
> 

So in a brief I think that  config A is more secure.

> 
> > I can Access to spliced site and no any kind of errors in access.log
> >
> > Any idea?
> 
> Have you read the documentation?
>  <https://wiki.squid-cache.org/Features/SslPeekAndSplice>

Yes I did, but the thing is (still for me) a bit complex, see what the autor of the link posted above said about the squid TLS.

> 
> Break your rules down into the stages as I have above and what is going on
> becomes a bit more clear.
> 
> Then you can consider what ssl_bump is doing in terms of what info Squid
> has available.
>  step1: TCP IP:port or CONNECT URI (forward-proxy only)
>  step2: TLS clientHello + TLS SNI (if any)
>  step3: TLS serverHello + server cert
> 
> The entire directive set is interpreted from top-to-bottom left-to-right each
> step. First line to fully match is what happens for that step.

Above in the current thread, there is a question about the order of steps.

However I test (today) the site that caused (yesterday) the handshake problem and with the original config and now works, so I dont know what to think what could be happened.
I refer to this with the term "original config":

ssl_bump peek step1 all
ssl_bump peek step2 noBumpSites
ssl_bump splice step3 noBumpSites
ssl_bump bump


Thank You

> 
> 
> Amos
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users



From squid3 at treenet.co.nz  Fri Sep  7 17:08:35 2018
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sat, 8 Sep 2018 05:08:35 +1200
Subject: [squid-users] Could compile squid with --enable-storeio
In-Reply-To: <CADfQnU2MP__MdwToeAA5vSPzZf=Q2PHr0UVO4NYnG9N_yXV18g@mail.gmail.com>
References: <CADfQnU3dXTVvc+E49bRu7SJiza=Yr4-g7TEQ7uo+7xw8C5Rn0w@mail.gmail.com>
 <694b6e1b-410b-7e09-1108-57396c9008dc@treenet.co.nz>
 <CADfQnU2MP__MdwToeAA5vSPzZf=Q2PHr0UVO4NYnG9N_yXV18g@mail.gmail.com>
Message-ID: <845bf941-050f-6743-e67f-97b61c4cc8f0@treenet.co.nz>

On 8/09/18 4:13 AM, Sekar Duraisamy wrote:
> Thanks for your prompt reply.
> 
> I could see the below message on cache.log even after removed
> cache_dir  from squid.conf
> 
> "2018/09/07 02:48:35| Set Current Directory to
> /opt/squid/squid3527/var/cache/squid"
> 
> Is this normal or I need to do anything else when i restart the squid?


That "cache" in that path is OS terminology from the FHS standard
(<http://www.pathname.com/fhs/pub/fhs-2.3.html#VARCACHEAPPLICATIONCACHEDATA>).
Not a Squid cache.

The "/var/cache/squid/" part as a whole is the FHS directory assigned to
Squid for storing anything that it needs to persist between executions
and across system reboot. That is all.
 One of those persistent things does happen to be cache_dir (*if* you
configure any), but other things also are needed to persist and may be
placed there.


For example; that message above is the Squid process CWD being set as
the location where Squid will drop core dumps *if* the OS lets cores be
created.

You can move it elsewhere with the coredump_dir directive in squid.conf,
but be aware that other locations may not be cleaned up automatically by
your package installation software if/when your Squid installation is
uninstalled, upgraded, etc.


Amos


From rousskov at measurement-factory.com  Fri Sep  7 17:30:20 2018
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Fri, 7 Sep 2018 11:30:20 -0600
Subject: [squid-users] Squid 4.2 : caching is not working
In-Reply-To: <CANfnjgLuEfJdCj-xvi33om7ESRnFxS4zANpckV-t4UUSusv5Sw@mail.gmail.com>
References: <CANfnjgLuEfJdCj-xvi33om7ESRnFxS4zANpckV-t4UUSusv5Sw@mail.gmail.com>
Message-ID: <44e584ae-1799-336a-b1e9-be9ca6293a1e@measurement-factory.com>

On 09/07/2018 10:46 AM, Hariharan Sethuraman wrote:

> squidclient -h localhost cache_object://localhost/ mgr:objects >>> this
> was showing the entry when the download was going on and disappeared
> after the download complete(~290MB) on the browser. When I checked the
> du of cache directory, it is intact with 200KB

Was the response cachable? You can use the redbot.org service to examine
the corresponding resource (URL).

If the service tells you that the resource was cachable in principle (or
if you cannot use the service), then you can post both HTTP request and
response headers (as received by Squid) here for further analysis. You
can collect those headers in cache.log by setting debug_options to ALL,2.


> (a) Please let me know what am missing to enable cache. 

I think your cache is enabled, but Squid refused to cache a particular
response you tested with. There is not enough information to say why.


> (b) Also "squidclient -h localhost cache_object://localhost/
> mgr:objects" hope this command will show the entry even after caching.

AFAICT, mgr:objects shows both in-progress transactions and cached
entries that do not belong to any in-progress transaction. However,
those cached entries will only be shown for UFS-based disk caches and
for a non-shared memory cache.

You are using a UFS-based cache. You should be (and probably are) using
a non-shared memory cache because you are using a UFS-based cache. In
summary, you most likely can use mgr:objects to see if the response was
cached. The above paragraph just answers your question in a way that may
be useful for others that have a different Squid configuration.


HTH,

Alex.


From squid3 at treenet.co.nz  Fri Sep  7 17:30:54 2018
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sat, 8 Sep 2018 05:30:54 +1200
Subject: [squid-users] Squid 4.2 : caching is not working
In-Reply-To: <CANfnjgLuEfJdCj-xvi33om7ESRnFxS4zANpckV-t4UUSusv5Sw@mail.gmail.com>
References: <CANfnjgLuEfJdCj-xvi33om7ESRnFxS4zANpckV-t4UUSusv5Sw@mail.gmail.com>
Message-ID: <ce991c90-bb50-a3ca-9113-038a5e8a133a@treenet.co.nz>

On 8/09/18 4:46 AM, Hariharan Sethuraman wrote:
> Hi team,
> 
> I have created directories using squid -z and then triggered squid -f
> /etc/squid/squid.conf -NYCd 1. Find (1) debug info below. And below (2)
> are the cache directory and squid-config.?
> 
> (1) - debug info:
> squidclient -h localhost cache_object://localhost/ mgr:objects >>> this

You do not need to pass squidclient the cache_object: URLs, nor
localhost as server. Just:

 squidclient mgr:objects

Also, what *exactly* did that report tell you?
 "cache" is more than just the disk storage area.


> was showing the entry when the download was going on and disappeared
> after the download complete(~290MB) on the browser.


What I am thinking reading that is that probably Squid used the cache
storage area as a temporary location for the bytes of a very large
object, but then removed it once the response was completely delivered
since it was not cacheable.

Details matter. The "~" means "approximately" and your config says
*exactly* 300 MByte is the upper limit.

 So an object which is "approximately 290" may in truth be *over* 300
and thus not permitted to cache.


NP: you can use the tool at redbot.org to check URL cacheability. It
will also tell you about any caching related HTTP compliance issues with
that resource.
 Or you can set "debug_options 11,2" in your squid.conf and check the
exact HTTP messages your proxy is dealing with.


 When I checked the
> du of cache directory, it is intact with 200KB

...
> ..
> cache allow all
> strip_query_terms off


Above are defaults. No need to configure since Squid-3.

> ..
> cache_dir ufs /var/spool/squid/cache 2000 16 256
> maximum_object_size 300 MB
> ..
> range_offset_limit -1
> ..
> url_rewrite_access allow all
> url_rewrite_program? /usr/bin/python /usr/share/proxypass.py

Not relevant, except that when testing the URL like with redbot.org you
need to use the URL this helper produces instead of what was passed into
Squid by the client.


> 
> http_access deny all
> ...
> always_direct deny all
> 
> (a) Please let me know what am missing to enable cache.

Cache is enabled and Squid caches as much as it can by default - within
the limits prescribed by HTTP specification and your config settings.

So the only thing to do is ensure that you do not actively *prevent*
caching from happening somehow.


> (b) Also "squidclient -h localhost cache_object://localhost/
> mgr:objects" hope this command will show the entry even after caching.
> 

It (well, "squidclient mgr:objects") should show all objects currently
known to the proxy. That will mostly be cached objects (both disk and
in-memory), but also included temporary in-transit objects.

Amos


From squid3 at treenet.co.nz  Fri Sep  7 18:19:08 2018
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sat, 8 Sep 2018 06:19:08 +1200
Subject: [squid-users] About SSL peek-n-splice/bump configurations
In-Reply-To: <002001d446cc$148bb580$3da32080$@yahoo.com.ar>
References: <000301d43289$14ed4770$3ec7d650$@yahoo.com.ar>
 <58014777-2b9c-7630-80dd-877c2d2af9d2@measurement-factory.com>
 <001801d432a0$8d102a30$a7307e90$@yahoo.com.ar>
 <6eb5660d-8025-eedd-3ed9-47d30bf22e62@measurement-factory.com>
 <001601d4464c$d1b38910$751a9b30$@yahoo.com.ar>
 <770057a5-3808-3fdc-5bf7-f78d09705913@treenet.co.nz>
 <002001d446cc$148bb580$3da32080$@yahoo.com.ar>
Message-ID: <c7a4d318-c335-03de-9e5c-73fc258e38c0@treenet.co.nz>

On 8/09/18 4:58 AM, Julian Perconti wrote:
>> De: Amos Jeffries
>> On 7/09/18 1:48 PM, Julian Perconti wrote:>
>>> Hi all,
>>>
>>> I have a new strange situation:
>>>
>>> With this peek-n-splice configuration:
>>>
>>> ssl_bump peek step1 all
>>> ssl_bump peek step2 noBumpSites
>>> ssl_bump splice step3 noBumpSites
>>> ssl_bump bump
>>
>> So... (lets call this config A)
>>
>> #step1 does this:
>>
>>> ssl_bump peek step1 all
>>
>> #step2 does this:
>>
>>> ssl_bump peek step2 noBumpSites
>>> ssl_bump bump
>>
>> If the bump at step2 happened, there is no step3.
>>
>> #step3 does this:
>>
>>> ssl_bump splice step3 noBumpSites
>>
>>
>>
>>>
>>> I got this error on spliced sites (a bank site):
>>>
>>> The system return in the browser this error: (chrome 69):
>>>
>>> (104) Connection reset by peer (TLS code: SQUID_ERR_SSL_HANDSHAKE)
>>> Handshake with SSL server failed: [No Error]
>>>
>>> This proxy and the remote host failed to negotiate a mutually acceptable
>> security settings for handling your request. It is possible that the remote host
>> does not support secure connections, or the proxy is not satisfied with the
>> host security credentials.
>>>
>>> cache.log:
>>> 2018/09/06 22:40:36 kid1| ERROR: negotiating TLS on FD 44:
>>> error:00000000:lib(0):func(0):reason(0) (5/-1/104)
>>>
>>> But if i change the ssl bump(s) directive to:
>>>
>>> ssl_bump peek step1
>>> ssl_bump splice noBumpSites
>>> ssl_bump bump all
>>>
>>
>> So ... (lets call this config B)
>>
>> #step does this:
>>
>>> ssl_bump peek step1
>>
>> #step2 does this:
>>
>>> ssl_bump splice noBumpSites
>>> ssl_bump bump all
>>
>> Notice there is never any step3, and the splice in this ruleset happens at
>> step2.
>>
>>
>> So config (A) is trying to do a step3 (handshake with server) when it has only
>> peek'ed and relayed the clientHello as-is (including any secret tokens an
>> unknown features the client is trying to use). The bump action is bound to
>> fail.
>>  ** "stare" is the action which sets up and filters the handshake ready for
>> bump action at step3 (server handshake with TLS features Squid knows how
>> to handle).
> 
> 
> So from http://marek.helion.pl/install/squid.html
> 
> We have this configs:
> 
> ssl_bump peek step1 all
> ssl_bump peek step2 noBumpSites
> ssl_bump splice step3 noBumpSites
> ssl_bump stare step2
> ssl_bump bump step3
> 
> Is better to use the above conf (staring at step2)? Because you said that bump at step2 is insecure.
> 
> Is the same if a I change the order of the above conf to:
> 
> ssl_bump peek step1 all
> ssl_bump peek step2 noBumpSites
> ssl_bump stare step2 <<< order changed
> ssl_bump splice step3 noBumpSites
> ssl_bump bump step3
> 

What exactly do you think the step1, step2, step3 ACLs here are doing?

I hoped it is obvious, but maybe not. Understanding that detail should
help resolve at least some of your confusion about these config snippets
and how "tiny" changes to them are affecting Squid behaviour in major ways.


>>
>>
>> The config (B) bumps at step2. That is what the old and very broken "client-
>> first" behaviour used to be. It does not produce any errors from the proxy
>> BUT leads directly to a huge pile of security vulnerabilities and nasty side
>> effects that may never be seen by you. Use at your own risk.
>>
>>
> 
> So in a brief I think that  config A is more secure.
> 

No. Config (A) from the earlier post actively *creates* insecurity by;

 1) hiding any information about the real server security level,
    - downgrade attacks. Right down to plaintext levels.

 2) hiding any information about the server certificate validity,
    - silent third-party MITM.
    - invalid certificate attacks.

 3) opening the server connection to multiplexed use from multiple
clients of Squid,
   - consider that in light of (1) and (2)




>>
>>> I can Access to spliced site and no any kind of errors in access.log
>>>
>>> Any idea?
>>
>> Have you read the documentation?
>>  <https://wiki.squid-cache.org/Features/SslPeekAndSplice>
> 
> Yes I did, but the thing is (still for me) a bit complex, see what the autor of the link posted above said about the squid TLS.
> 

I assume you mean Alex, that page is a true community effort with
several authors. He is "just" the latest to update the info as Squid
changed and/or better ways to write the info were found. Both of us are
in the main dev team on Squid and neither authored the actual ssl_bump
processing code.

Note that my descriptions are at time hedged with "should", "may", "if",
etc and Alex outright mentions possible bugs along with similar terms.


Any suggestions towards simplifying the wiki details, or presenting them
in an easier to read way would be welcome.


>>
>> Break your rules down into the stages as I have above and what is going on
>> becomes a bit more clear.
>>
>> Then you can consider what ssl_bump is doing in terms of what info Squid
>> has available.
>>  step1: TCP IP:port or CONNECT URI (forward-proxy only)
>>  step2: TLS clientHello + TLS SNI (if any)
>>  step3: TLS serverHello + server cert
>>
>> The entire directive set is interpreted from top-to-bottom left-to-right each
>> step. First line to fully match is what happens for that step.
> 
> Above in the current thread, there is a question about the order of steps.
> 

The "steps" are messages of the TLS handshake and Squid processing code.
They happen as per the wiki page section "Processing steps". My above is
a summary of the *data* available at each step at the time each ssl_bump
processing occurs.

The variable things are what info the client and server are providing,
and what your locally defined ACLs (like the "noBumpSites") are actually
matching on when tested.

There is also the fact that all this code is still quite volatile so the
code changes nearly every release or so, and parts of it all are buggy -
some we know about, some not yet. See above about both Alex and I
hedging our descriptions a bit at times where things become
non-deterministic.


> However I test (today) the site that caused (yesterday) the handshake problem and with the original config and now works, so I dont know what to think what could be happened.

TLS is quite a volatile environment. It could be many things.

Unfortunately it also means further learning we might all get about this
failure situation is not likely to happen any time soon.

> I refer to this with the term "original config":
> 
> ssl_bump peek step1 all
> ssl_bump peek step2 noBumpSites
> ssl_bump splice step3 noBumpSites
> ssl_bump bump
> 

Even though this works on your previously broken site as well as others
it still has the problems related to bump sometimes happening at step2
before server details are known to your Squid.


Amos


From srnhari at gmail.com  Sat Sep  8 04:14:28 2018
From: srnhari at gmail.com (Hariharan Sethuraman)
Date: Sat, 8 Sep 2018 09:44:28 +0530
Subject: [squid-users] Squid 4.2 : caching is not working
In-Reply-To: <ce991c90-bb50-a3ca-9113-038a5e8a133a@treenet.co.nz>
References: <CANfnjgLuEfJdCj-xvi33om7ESRnFxS4zANpckV-t4UUSusv5Sw@mail.gmail.com>
 <ce991c90-bb50-a3ca-9113-038a5e8a133a@treenet.co.nz>
Message-ID: <CANfnjgJufRBou6cyA51=CZr8c7xdOFvp-+Gqga9Y-cPh-SsVSA@mail.gmail.com>

Hi,
I see the response can be cached. Will try out increasing logging level of
cache.log


HTTP/1.1 200 OK
    Date: Sat, 08 Sep 2018 04:10:38 GMT
    Server: Apache/2.2
    Keep-Alive: timeout=5, max=100
    Connection: Keep-Alive
    Transfer-Encoding: chunked
    Content-Type: text/plain;; charset=ISO-8859-1

response headers: 203 bytes body: 21 bytes transfer overhead: 9 bytes
view body
<https://redbot.org/?uri=https%3A%2F%2Fdl.cisco.com%2Fpcgi-bin%2Fswdld%2Fdownload.cgi%3Fdwnld_code%3DxhMnkw8Z-oFg8Jvk7BeSZnkmIdO48GTAA01cTlDABiH7c9QJsq9s5ypIQWgQxY4cI66yKQsWgjvIpCgr8_DGemwm_6VqWfSJQ8Xonly7l-DcKbz9kqMQXOHnp2G1BHj_-wp1DhAskLOUpvRG2oeR_f_FxYTyLVHIN5esRF-LXOUKwkwyT0TOf6xO-AUm3KaM&req_hdr=Authentication%3ABearer+HQdHyp1lCj2cZxGYPlzEqlEuGxww#>
 view har
<https://redbot.org/?id=9ml2eaa9&req_hdr=Authentication%3ABearer%20HQdHyp1lCj2cZxGYPlzEqlEuGxww&req_hdr=User-Agent%3ARED/1%20(https://redbot.org/)&req_hdr=Referer%3Ahttps://dl.cisco.com/pcgi-bin/swdld/download.cgi?dwnld_code%3DxhMnkw8Z-oFg8Jvk7BeSZnkmIdO48GTAA01cTlDABiH7c9QJsq9s5ypIQWgQxY4cI66yKQsWgjvIpCgr8_DGemwm_6VqWfSJQ8Xonly7l-DcKbz9kqMQXOHnp2G1BHj_-wp1DhAskLOUpvRG2oeR_f_FxYTyLVHIN5esRF-LXOUKwkwyT0TOf6xO-AUm3KaM&check_name=default&format=har>
 save
<https://redbot.org/?uri=https%3A%2F%2Fdl.cisco.com%2Fpcgi-bin%2Fswdld%2Fdownload.cgi%3Fdwnld_code%3DxhMnkw8Z-oFg8Jvk7BeSZnkmIdO48GTAA01cTlDABiH7c9QJsq9s5ypIQWgQxY4cI66yKQsWgjvIpCgr8_DGemwm_6VqWfSJQ8Xonly7l-DcKbz9kqMQXOHnp2G1BHj_-wp1DhAskLOUpvRG2oeR_f_FxYTyLVHIN5esRF-LXOUKwkwyT0TOf6xO-AUm3KaM&req_hdr=Authentication%3ABearer+HQdHyp1lCj2cZxGYPlzEqlEuGxww#>
General

   - The Content-Type header's syntax isn't valid.
   - The Keep-Alive header is deprecated.
   - The server's clock is correct.

Caching

   - This response allows all caches to store it.
   - This response allows a cache to assign its own freshness lifetime.

Thanks,
Hari


On Fri, Sep 7, 2018 at 11:01 PM Amos Jeffries <squid3 at treenet.co.nz> wrote:

> On 8/09/18 4:46 AM, Hariharan Sethuraman wrote:
> > Hi team,
> >
> > I have created directories using squid -z and then triggered squid -f
> > /etc/squid/squid.conf -NYCd 1. Find (1) debug info below. And below (2)
> > are the cache directory and squid-config.
> >
> > (1) - debug info:
> > squidclient -h localhost cache_object://localhost/ mgr:objects >>> this
>
> You do not need to pass squidclient the cache_object: URLs, nor
> localhost as server. Just:
>
>  squidclient mgr:objects
>
> Also, what *exactly* did that report tell you?
>  "cache" is more than just the disk storage area.
>
>
> > was showing the entry when the download was going on and disappeared
> > after the download complete(~290MB) on the browser.
>
>
> What I am thinking reading that is that probably Squid used the cache
> storage area as a temporary location for the bytes of a very large
> object, but then removed it once the response was completely delivered
> since it was not cacheable.
>
> Details matter. The "~" means "approximately" and your config says
> *exactly* 300 MByte is the upper limit.
>
>  So an object which is "approximately 290" may in truth be *over* 300
> and thus not permitted to cache.
>
>
> NP: you can use the tool at redbot.org to check URL cacheability. It
> will also tell you about any caching related HTTP compliance issues with
> that resource.
>  Or you can set "debug_options 11,2" in your squid.conf and check the
> exact HTTP messages your proxy is dealing with.
>
>
>  When I checked the
> > du of cache directory, it is intact with 200KB
>
> ...
> > ..
> > cache allow all
> > strip_query_terms off
>
>
> Above are defaults. No need to configure since Squid-3.
>
> > ..
> > cache_dir ufs /var/spool/squid/cache 2000 16 256
> > maximum_object_size 300 MB
> > ..
> > range_offset_limit -1
> > ..
> > url_rewrite_access allow all
> > url_rewrite_program  /usr/bin/python /usr/share/proxypass.py
>
> Not relevant, except that when testing the URL like with redbot.org you
> need to use the URL this helper produces instead of what was passed into
> Squid by the client.
>
>
> >
> > http_access deny all
> > ...
> > always_direct deny all
> >
> > (a) Please let me know what am missing to enable cache.
>
> Cache is enabled and Squid caches as much as it can by default - within
> the limits prescribed by HTTP specification and your config settings.
>
> So the only thing to do is ensure that you do not actively *prevent*
> caching from happening somehow.
>
>
> > (b) Also "squidclient -h localhost cache_object://localhost/
> > mgr:objects" hope this command will show the entry even after caching.
> >
>
> It (well, "squidclient mgr:objects") should show all objects currently
> known to the proxy. That will mostly be cached objects (both disk and
> in-memory), but also included temporary in-transit objects.
>
> Amos
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20180908/4614c6f8/attachment.htm>

From srnhari at gmail.com  Sat Sep  8 04:18:56 2018
From: srnhari at gmail.com (Hariharan Sethuraman)
Date: Sat, 8 Sep 2018 09:48:56 +0530
Subject: [squid-users] Squid 4.2 : caching is not working
In-Reply-To: <CANfnjgJufRBou6cyA51=CZr8c7xdOFvp-+Gqga9Y-cPh-SsVSA@mail.gmail.com>
References: <CANfnjgLuEfJdCj-xvi33om7ESRnFxS4zANpckV-t4UUSusv5Sw@mail.gmail.com>
 <ce991c90-bb50-a3ca-9113-038a5e8a133a@treenet.co.nz>
 <CANfnjgJufRBou6cyA51=CZr8c7xdOFvp-+Gqga9Y-cPh-SsVSA@mail.gmail.com>
Message-ID: <CANfnjgJ7RY6SSFHHXxwnx8EsFbucqAxSOA=MCN1QoDhxObickQ@mail.gmail.com>

Hi Amos,

This is what I see when the download is in progress:

KEY 44000000000000000902000000000000
        STORE_PENDING NOT_IN_MEMORY SWAPOUT_NONE PING_DONE
        RELEASE_REQUEST,DISPATCHED,PRIVATE,VALIDATED
        LV:1536379799 LU:1536379801 LM:1532110990 EX:-1
        4 locks, 1 clients, 1 refs
        Swap Dir -1, File 0XFFFFFFFF
        GET
https://example.com/DhAskLOUpvRG2oeR_f_FxYTyLVHIN5esRF-LXOUKwkwyT0TOf6xO-AUm3KaM
        inmem_lo: 99225582
        inmem_hi: 99324372
        swapout: 0 bytes queued


On Sat, Sep 8, 2018 at 9:44 AM Hariharan Sethuraman <srnhari at gmail.com>
wrote:

> Hi,
> I see the response can be cached. Will try out increasing logging level of
> cache.log
>
>
> HTTP/1.1 200 OK
>     Date: Sat, 08 Sep 2018 04:10:38 GMT
>     Server: Apache/2.2
>     Keep-Alive: timeout=5, max=100
>     Connection: Keep-Alive
>     Transfer-Encoding: chunked
>     Content-Type: text/plain;; charset=ISO-8859-1
>
> response headers: 203 bytes body: 21 bytes transfer overhead: 9 bytes
> view body
> <https://redbot.org/?uri=https%3A%2F%2Fdl.cisco.com%2Fpcgi-bin%2Fswdld%2Fdownload.cgi%3Fdwnld_code%3DxhMnkw8Z-oFg8Jvk7BeSZnkmIdO48GTAA01cTlDABiH7c9QJsq9s5ypIQWgQxY4cI66yKQsWgjvIpCgr8_DGemwm_6VqWfSJQ8Xonly7l-DcKbz9kqMQXOHnp2G1BHj_-wp1DhAskLOUpvRG2oeR_f_FxYTyLVHIN5esRF-LXOUKwkwyT0TOf6xO-AUm3KaM&req_hdr=Authentication%3ABearer+HQdHyp1lCj2cZxGYPlzEqlEuGxww#>
>  view har
> <https://redbot.org/?id=9ml2eaa9&req_hdr=Authentication%3ABearer%20HQdHyp1lCj2cZxGYPlzEqlEuGxww&req_hdr=User-Agent%3ARED/1%20(https://redbot.org/)&req_hdr=Referer%3Ahttps://dl.cisco.com/pcgi-bin/swdld/download.cgi?dwnld_code%3DxhMnkw8Z-oFg8Jvk7BeSZnkmIdO48GTAA01cTlDABiH7c9QJsq9s5ypIQWgQxY4cI66yKQsWgjvIpCgr8_DGemwm_6VqWfSJQ8Xonly7l-DcKbz9kqMQXOHnp2G1BHj_-wp1DhAskLOUpvRG2oeR_f_FxYTyLVHIN5esRF-LXOUKwkwyT0TOf6xO-AUm3KaM&check_name=default&format=har>
>  save
> <https://redbot.org/?uri=https%3A%2F%2Fdl.cisco.com%2Fpcgi-bin%2Fswdld%2Fdownload.cgi%3Fdwnld_code%3DxhMnkw8Z-oFg8Jvk7BeSZnkmIdO48GTAA01cTlDABiH7c9QJsq9s5ypIQWgQxY4cI66yKQsWgjvIpCgr8_DGemwm_6VqWfSJQ8Xonly7l-DcKbz9kqMQXOHnp2G1BHj_-wp1DhAskLOUpvRG2oeR_f_FxYTyLVHIN5esRF-LXOUKwkwyT0TOf6xO-AUm3KaM&req_hdr=Authentication%3ABearer+HQdHyp1lCj2cZxGYPlzEqlEuGxww#>
> General
>
>    - The Content-Type header's syntax isn't valid.
>    - The Keep-Alive header is deprecated.
>    - The server's clock is correct.
>
> Caching
>
>    - This response allows all caches to store it.
>    - This response allows a cache to assign its own freshness lifetime.
>
> Thanks,
> Hari
>
>
> On Fri, Sep 7, 2018 at 11:01 PM Amos Jeffries <squid3 at treenet.co.nz>
> wrote:
>
>> On 8/09/18 4:46 AM, Hariharan Sethuraman wrote:
>> > Hi team,
>> >
>> > I have created directories using squid -z and then triggered squid -f
>> > /etc/squid/squid.conf -NYCd 1. Find (1) debug info below. And below (2)
>> > are the cache directory and squid-config.
>> >
>> > (1) - debug info:
>> > squidclient -h localhost cache_object://localhost/ mgr:objects >>> this
>>
>> You do not need to pass squidclient the cache_object: URLs, nor
>> localhost as server. Just:
>>
>>  squidclient mgr:objects
>>
>> Also, what *exactly* did that report tell you?
>>  "cache" is more than just the disk storage area.
>>
>>
>> > was showing the entry when the download was going on and disappeared
>> > after the download complete(~290MB) on the browser.
>>
>>
>> What I am thinking reading that is that probably Squid used the cache
>> storage area as a temporary location for the bytes of a very large
>> object, but then removed it once the response was completely delivered
>> since it was not cacheable.
>>
>> Details matter. The "~" means "approximately" and your config says
>> *exactly* 300 MByte is the upper limit.
>>
>>  So an object which is "approximately 290" may in truth be *over* 300
>> and thus not permitted to cache.
>>
>>
>> NP: you can use the tool at redbot.org to check URL cacheability. It
>> will also tell you about any caching related HTTP compliance issues with
>> that resource.
>>  Or you can set "debug_options 11,2" in your squid.conf and check the
>> exact HTTP messages your proxy is dealing with.
>>
>>
>>  When I checked the
>> > du of cache directory, it is intact with 200KB
>>
>> ...
>> > ..
>> > cache allow all
>> > strip_query_terms off
>>
>>
>> Above are defaults. No need to configure since Squid-3.
>>
>> > ..
>> > cache_dir ufs /var/spool/squid/cache 2000 16 256
>> > maximum_object_size 300 MB
>> > ..
>> > range_offset_limit -1
>> > ..
>> > url_rewrite_access allow all
>> > url_rewrite_program  /usr/bin/python /usr/share/proxypass.py
>>
>> Not relevant, except that when testing the URL like with redbot.org you
>> need to use the URL this helper produces instead of what was passed into
>> Squid by the client.
>>
>>
>> >
>> > http_access deny all
>> > ...
>> > always_direct deny all
>> >
>> > (a) Please let me know what am missing to enable cache.
>>
>> Cache is enabled and Squid caches as much as it can by default - within
>> the limits prescribed by HTTP specification and your config settings.
>>
>> So the only thing to do is ensure that you do not actively *prevent*
>> caching from happening somehow.
>>
>>
>> > (b) Also "squidclient -h localhost cache_object://localhost/
>> > mgr:objects" hope this command will show the entry even after caching.
>> >
>>
>> It (well, "squidclient mgr:objects") should show all objects currently
>> known to the proxy. That will mostly be cached objects (both disk and
>> in-memory), but also included temporary in-transit objects.
>>
>> Amos
>> _______________________________________________
>> squid-users mailing list
>> squid-users at lists.squid-cache.org
>> http://lists.squid-cache.org/listinfo/squid-users
>>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20180908/74d0749a/attachment.htm>

From thompsonm3301 at protonmail.com  Sat Sep  8 08:25:44 2018
From: thompsonm3301 at protonmail.com (thompsonm)
Date: Sat, 8 Sep 2018 03:25:44 -0500 (CDT)
Subject: [squid-users] Using SSL bump and reverse proxy for DNS sinkhole
Message-ID: <1536395144235-0.post@n4.nabble.com>

Hello, I have a question about squid SSL bump and reverse proxy. Basically
for a final project I want to create a DNS sinkhole, where the client tries
to query a domain that has a bad reputation or is known for drive-by
downloads etc, and the DNS server returns false information, such as an
internal IP. Then the client is redirected to this internal IP, where a web
server is listening, and makes the HTTP request as normal. All the HTTP
requests along with host, URL, client IP etc, are then logged. It's easy to
make this work with HTTP. However, I want it to work also with HTTPS. So
basically set up a MITM SSL proxy, where the proxy generates it's own
certificate for the suspicious website the client is trying to connect to,
and then HTTP requests are forwarded to a web server listening on the same
host. 

I'm not sure how to do this. Is there any way to do this with squid SSL bump
and reverse proxy? 



--
Sent from: http://squid-web-proxy-cache.1019090.n4.nabble.com/Squid-Users-f1019091.html


From Antony.Stone at squid.open.source.it  Sat Sep  8 08:41:54 2018
From: Antony.Stone at squid.open.source.it (Antony Stone)
Date: Sat, 8 Sep 2018 10:41:54 +0200
Subject: [squid-users] Using SSL bump and reverse proxy for DNS sinkhole
In-Reply-To: <1536395144235-0.post@n4.nabble.com>
References: <1536395144235-0.post@n4.nabble.com>
Message-ID: <201809081041.55087.Antony.Stone@squid.open.source.it>

On Saturday 08 September 2018 at 10:25:44, thompsonm wrote:

> Hello, I have a question about squid SSL bump and reverse proxy. Basically
> for a final project I want to create a DNS sinkhole, where the client tries
> to query a domain that has a bad reputation or is known for drive-by
> downloads etc, and the DNS server returns false information, such as an
> internal IP. Then the client is redirected to this internal IP, where a web
> server is listening, and makes the HTTP request as normal.

Okat, that makes sense (technically, at least) so far...

> All the HTTP requests along with host, URL, client IP etc, are then logged.

Yep, the web server (which I presume is run by you) will do that for you.

> It's easy to make this work with HTTP. However, I want it to work also with
> HTTPS.

What's the difference?  A web server can serve HTTPS as easily as it can serve 
HTTP.

> So basically set up a MITM

In The Middle of what?

Client is one end, but what's at the "other end" of some connection you're in 
the "middle" of?

Surely the other end is your own web server - I mean, you're trying to prevent 
people from connecting to (certain) real sites by giving the clients fake DNS 
replies, yes?  So, they never end up on the real site, and there's no 
connection for you to intercept.

> SSL proxy, where the proxy generates its own certificate for the suspicious
> website the client is trying to connect to, and then HTTP requests are
> forwarded to a web server listening on the same host.

This is over-complicated.  You just need one of:

1. a web server which will generate an SSL certificate on the fly and then serve 
HTT{S content back to the client using that certificate

or

2. a pile of SSL certificates which you generate using your own CA at the same 
time you put the fake entries into DNS.  After all, you know what domains 
you're putting into your "DNS sinkhole", so just generate an SSL certificate 
for each one as you do it, load them onto your web server, and there you go.

Basically, if you don't need to use Squid in intercept mode for the HTTP 
solution, you don't need to use SSL Bump for the HTTPS solution.

> I'm not sure how to do this. Is there any way to do this with squid SSL
> bump and reverse proxy?

Not that I can see, no, because there is no connection to be in the middle of 
that you want to intercept.  You want the client to be at one end, and your 
own server at the other end, whether it's HTTP or HTTPS - in neither case do 
you want clients to connect to the real servers.

Or, have I misunderstood something about your objective?


Antony.

-- 
<flopsie> yes, but this is #lbw, we don't do normal

                                                   Please reply to the list;
                                                         please *don't* CC me.


From thompsonm3301 at protonmail.com  Sat Sep  8 09:00:41 2018
From: thompsonm3301 at protonmail.com (thompsonm)
Date: Sat, 8 Sep 2018 04:00:41 -0500 (CDT)
Subject: [squid-users] Using SSL bump and reverse proxy for DNS sinkhole
In-Reply-To: <201809081041.55087.Antony.Stone@squid.open.source.it>
References: <1536395144235-0.post@n4.nabble.com>
 <201809081041.55087.Antony.Stone@squid.open.source.it>
Message-ID: <1536397241506-0.post@n4.nabble.com>

"1. a web server which will generate an SSL certificate on the fly and then
serve 
HTT{S content back to the client using that certificate "

Is there a way to do this? The only way I can find is to use wildcard
certificates. But that's not what I'm trying to do.

"2. a pile of SSL certificates which you generate using your own CA at the
same 
time you put the fake entries into DNS.  After all, you know what domains 
you're putting into your "DNS sinkhole", so just generate an SSL certificate 
for each one as you do it, load them onto your web server, and there you go.
"

This is not really feasible because the lists are always being updated. I
could write a script or something but I think it would be better just to
have a web server or proxy create the certificates when the client tries to
connect. 



--
Sent from: http://squid-web-proxy-cache.1019090.n4.nabble.com/Squid-Users-f1019091.html


From Antony.Stone at squid.open.source.it  Sat Sep  8 09:16:23 2018
From: Antony.Stone at squid.open.source.it (Antony Stone)
Date: Sat, 8 Sep 2018 11:16:23 +0200
Subject: [squid-users] Using SSL bump and reverse proxy for DNS sinkhole
In-Reply-To: <1536397241506-0.post@n4.nabble.com>
References: <1536395144235-0.post@n4.nabble.com>
 <201809081041.55087.Antony.Stone@squid.open.source.it>
 <1536397241506-0.post@n4.nabble.com>
Message-ID: <201809081116.23319.Antony.Stone@squid.open.source.it>

On Saturday 08 September 2018 at 11:00:41, thompsonm wrote:

> "1. a web server which will generate an SSL certificate on the fly and then
> serve HTTPS content back to the client using that certificate "
> 
> Is there a way to do this? The only way I can find is to use wildcard
> certificates. But that's not what I'm trying to do.

I don't have a recipe for it, but I'd thought that since Squid can create a 
certificate on demand, Apache or NGinx would be able to too.

If that's not feasible, though...

> "2. a pile of SSL certificates which you generate using your own CA at the
> same time you put the fake entries into DNS.  After all, you know what
> domains you're putting into your "DNS sinkhole", so just generate an SSL
> certificate for each one as you do it, load them onto your web server, and
> there you go. "
> 
> This is not really feasible because the lists are always being updated.

So?  Update the certificates at the same time as DNS.  It'll be a lot less work 
for your web server, too, just having to use a pre-existing certificate to 
service a request, rather than having to generate a certificate every time it 
sees the first request for a domain.

> I could write a script or something but I think it would be better just to
> have a web server or proxy create the certificates when the client tries to
> connect.

Agreed, but just in case it's not feasible, a script to generate SSL certs 
from your DNS list certainly would be.

Either way, I don't see that Squid's MITM SSL Bump facility is a solution, 
because as I said previously, you have no connection to be in the middle of.


Antony.

-- 
All generalisations are inaccurate.

                                                   Please reply to the list;
                                                         please *don't* CC me.


From srnhari at gmail.com  Sat Sep  8 14:43:16 2018
From: srnhari at gmail.com (Hariharan Sethuraman)
Date: Sat, 8 Sep 2018 20:13:16 +0530
Subject: [squid-users] Squid 4.2 : caching is not working
In-Reply-To: <22c4ec2c-dd11-7391-65ea-a5a9ca38e794@treenet.co.nz>
References: <CANfnjgLuEfJdCj-xvi33om7ESRnFxS4zANpckV-t4UUSusv5Sw@mail.gmail.com>
 <ce991c90-bb50-a3ca-9113-038a5e8a133a@treenet.co.nz>
 <CANfnjgJufRBou6cyA51=CZr8c7xdOFvp-+Gqga9Y-cPh-SsVSA@mail.gmail.com>
 <CANfnjgJ7RY6SSFHHXxwnx8EsFbucqAxSOA=MCN1QoDhxObickQ@mail.gmail.com>
 <CANfnjgKJTy3U_DBg=gmu0_1DmDhS4DZOHYjxqbepzO5y0JtSAA@mail.gmail.com>
 <22c4ec2c-dd11-7391-65ea-a5a9ca38e794@treenet.co.nz>
Message-ID: <CANfnjgL2vmVJXT5LRb=UUv0u_zx0NuNG_5ED6kV+eOEjLMCL2Q@mail.gmail.com>

But the partial data is been continuously sending back to client. Squid
didn't wait for the complete file to download

On Sat, 8 Sep 2018, 20:08 Amos Jeffries, <squid3 at treenet.co.nz> wrote:

> On 8/09/18 7:44 PM, Hariharan Sethuraman wrote:
> > And here is my HTTP request and reply:
> > 1. HEAD request to read image information
> > 2. GET request to download the image
> >
>
> Not quite. GET partial / Range request to fetch the content.
>
> Squid converted it into a full request for the backend server due to
> range_offset_limit -1. But that does mean Squid had to download ~240MB
> of data before anything starts being sent to the client.
>
>
>
> > (2)
> > 2018/09/08 07:28:39.155| 11,2| client_side.cc(1278) parseHttpRequest:
> > HTTP Client REQUEST:
> > ---------
> > GET /DcKbz9kqMQXK-zp95pv9LH11kjhTpxOJsJ-1FYEL4
> > Host: example.com:3129 <http://example.com:3129>^M
> > Range: bytes=242819145-^M
> > User-Agent: curl/7.56.1^M
> > Accept: */*^M
> >
>
>
> > 2018/09/08 07:28:39.938| 11,2| http.cc(2261) sendRequest: HTTP Server
> > REQUEST:
> > ---------
> > GET /DcKbz9kqMQXK-zp95pv9LH11kjhTpxOJsJ-1FYEL4
> > User-Agent: curl/7.56.1^M
> > Accept: */*^M
> > Host: exampletarget.com <http://exampletarget.com>^M
> > Via: 1.1 jb7mgd (squid/4.2)^M
> > Surrogate-Capability: jb7mgd="Surrogate/1.0"^M
> > X-Forwarded-For: **.**.**.**^M
> > Cache-Control: max-age=0^M
> > Connection: keep-alive^M
> >
> > 2018/09/08 07:28:44.359| 11,2| http.cc(723) processReplyHeader: HTTP
> > Server RESPONSE:
> > ---------
> > HTTP/1.1 200 OK
> > Date: Sat, 08 Sep 2018 07:28:40 GMT^M
> > Server: Apache/2.2^M
> > Content-Disposition: attachment; filename=somefile.iso;^M
> > Last-Modified: Fri, 20 Jul 2018 18:23:10 GMT^M
> > ETag: "4a54c59-11653800-571726350bf80"^M
> > Accept-Ranges: bytes^M
> > Content-Length: 291846144^M
> > Keep-Alive: timeout=5, max=100^M
> > Connection: Keep-Alive^M
> > Content-Type: application/unknown^M
> >
> > 2018/09/08 07:28:44.361| 11,2| Stream.cc(267) sendStartOfMessage: HTTP
> > Client REPLY:
> > ---------
> > HTTP/1.1 206 Partial Content^M
> > Date: Sat, 08 Sep 2018 07:28:40 GMT^M
> > Server: Apache/2.2^M
> > Content-Disposition: attachment; filename=somefile.iso;^M
> > Last-Modified: Fri, 20 Jul 2018 18:23:10 GMT^M
> > ETag: "4a54c59-11653800-571726350bf80"^M
> > Accept-Ranges: bytes^M
> > Content-Type: application/unknown^M
> > X-Cache: MISS from jb7mgd^M
> > X-Cache-Lookup: MISS from jb7mgd:3128^M
> > Via: 1.1 jb7mgd (squid/4.2)^M
> > Connection: keep-alive^M
> > Content-Range: bytes 242819145-291846143/291846144^M
> > Content-Length: 49026999^M
> >
> > Thanks,
> > Hari
> >
> > On Sat, Sep 8, 2018 at 9:48 AM Hariharan Sethuraman wrote:
> >
> >     Hi Amos,
> >
> >     This is what I see when the download is in progress:
> >
> >     KEY 44000000000000000902000000000000
> >             STORE_PENDING NOT_IN_MEMORY SWAPOUT_NONE PING_DONE
> >             RELEASE_REQUEST,DISPATCHED,PRIVATE,VALIDATED
>
> So file stored in memory and scheduled for removal.
>
> >             LV:1536379799 LU:1536379801 LM:1532110990 EX:-1
> >             4 locks, 1 clients, 1 refs
> >             Swap Dir -1, File 0XFFFFFFFF
> >             GET
> >
> https://example.com/DhAskLOUpvRG2oeR_f_FxYTyLVHIN5esRF-LXOUKwkwyT0TOf6xO-AUm3KaM
> >             inmem_lo: 99225582
> >             inmem_hi: 99324372
> >             swapout: 0 bytes queued
> >
>
> Amos
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20180908/ec44b847/attachment.htm>

From vh1988 at yahoo.com.ar  Sat Sep  8 17:45:10 2018
From: vh1988 at yahoo.com.ar (Julian Perconti)
Date: Sat, 8 Sep 2018 14:45:10 -0300
Subject: [squid-users] About SSL peek-n-splice/bump configurations
In-Reply-To: <c7a4d318-c335-03de-9e5c-73fc258e38c0@treenet.co.nz>
References: <000301d43289$14ed4770$3ec7d650$@yahoo.com.ar>
 <58014777-2b9c-7630-80dd-877c2d2af9d2@measurement-factory.com>
 <001801d432a0$8d102a30$a7307e90$@yahoo.com.ar>
 <6eb5660d-8025-eedd-3ed9-47d30bf22e62@measurement-factory.com>
 <001601d4464c$d1b38910$751a9b30$@yahoo.com.ar>
 <770057a5-3808-3fdc-5bf7-f78d09705913@treenet.co.nz>
 <002001d446cc$148bb580$3da32080$@yahoo.com.ar>
 <c7a4d318-c335-03de-9e5c-73fc258e38c0@treenet.co.nz>
Message-ID: <008001d4479b$b22b9290$1682b7b0$@yahoo.com.ar>

> -----Mensaje original-----
> De: squid-users <squid-users-bounces at lists.squid-cache.org> En nombre de
> Amos Jeffries
> Enviado el: viernes, 7 de septiembre de 2018 15:19
> Para: squid-users at lists.squid-cache.org
> Asunto: Re: [squid-users] About SSL peek-n-splice/bump configurations
> 
> > So from http://marek.helion.pl/install/squid.html
> >
> > We have this configs:
> >
> > ssl_bump peek step1 all
> > ssl_bump peek step2 noBumpSites
> > ssl_bump splice step3 noBumpSites
> > ssl_bump stare step2
> > ssl_bump bump step3
> >
> > Is better to use the above conf (staring at step2)? Because you said that
> bump at step2 is insecure.
> >
> > Is the same if a I change the order of the above conf to:
> >
> > ssl_bump peek step1 all
> > ssl_bump peek step2 noBumpSites
> > ssl_bump stare step2 <<< order changed ssl_bump splice step3
> > noBumpSites ssl_bump bump step3
> >
> 
> What exactly do you think the step1, step2, step3 ACLs here are doing?

I don not know what -exactly- these ACL are doing; that is what I trying to find out.
I have some ideas about them, but not the exactly knowledge, for that reason I asked if there is difference between those 2 configs order (because the step is the same)

> 
> I hoped it is obvious, but maybe not. Understanding that detail should
> help resolve at least some of your confusion about these config snippets
> and how "tiny" changes to them are affecting Squid behaviour in major ways.
> 

No, it isn't obviuos to me, and yes, I am still trying to understand by re-reading wiki squid doc and other sites about peek and splice decisions and about the steps too.

> 
> >
> > So in a brief I think that  config A is more secure.
> >
> 
> No. Config (A) from the earlier post actively *creates* insecurity by;

But,according to http://marek.helion.pl/install/squid.html; It's supposed that config  "A" check server certificate. Because it is peeking at step2 and splicing at step3 the whitelist sites.

> 
>  1) hiding any information about the real server security level,
>     - downgrade attacks. Right down to plaintext levels.
> 
>  2) hiding any information about the server certificate validity,
>     - silent third-party MITM.
>     - invalid certificate attacks.
> 
>  3) opening the server connection to multiplexed use from multiple
> clients of Squid,
>    - consider that in light of (1) and (2)

I dont understand, in earlier post:

>> ssl_bump peek step1 all
>> ssl_bump peek step2 noBumpSites
>> ssl_bump splice step3 noBumpSites
>> ssl_bump bump
>>
>So... (lets call this config A)

In this config I think the problem is that squid is peeking at step2 noBumpSites; but also bump all other sites at step2 (there is no step specified in the last line -the bump-)

Therefore I think that would be "better" or " less insecure" bumping at step3.

Conlusion based on these words:

> So config (A) is trying to do a step3 (handshake with server) when it 
> has only peek'ed and relayed the clientHello as-is (including any 
> secret tokens an unknown features the client is trying to use). The 
> bump action is bound to fail.
>  ** "stare" is the action which sets up and filters the handshake 
> ready for bump action at step3 (server handshake with TLS features 
> Squid knows how to handle).

I think that my config would be something like this:

ssl_bump peek step1 all

ssl_bump peek step2 noBumpSites 

>From squid doc:
"When a peek rule matches during step 2, Squid proceeds to step3 where it parses the TLS Server Hello and extracts server certificate while preserving the possibility of splicing the client and server connections; peeking at the server certificate usually precludes future bumping"

And from http://marek.helion.pl/install/squid.html
Peeking at step 2 will check the name stored in server certificate (CommonName, SubjectAltName) as well. So let's do it! you must enable peek at step 2 and finally splice at step3 (if certName matches the whitelist)

ssl_bump splice step3 noBumpSites
(following the ruleset explained above)

And here I believe that the final bump should be make at step3:

ssl_bump bump step3	

OR there is no difference if i dont specify the step in the bump line?

summarizing:

ssl_bump peek step1 all
ssl_bump peek step2 noBumpSites
ssl_bump splice step3 noBumpSites
ssl_bump bump step3

EDIT:
Before send this mail to list, I test the squid behaviour and if I dont add a stare (dont know why this happens, stare is the less used option I saw in many examples on the web) line at step2, all accesed site are spliced; so my final config would be taken from helion.pl: (almost defeated by this thread/topic)

ssl_bump peek step1 all               	# at step 1 we're peeking at client TLS-request in order to find the "SNI"
ssl_bump peek step2 nobumpSites       	# here we're peeking at server certificate
ssl_bump splice step3 nobumpSites     	# here we're splicing connections which match the whitelist
ssl_bump stare step2                  		# here we're staring at server certificate
ssl_bump bump step3                   	# finally we're bumping all other SSL connections at step 3

The autor of helion.pl says:

"Honestly I don't see a reason to stare at the server certificate before bumping. Even without "staring" the fake-certificate has the same attributes (Common Name etc.) like the original one. But it might change in the future..."

So I leave the stare line to avoid splice all the traffic.

Thank You again.

Squid Cache: Version 4.2-20180902-r6d8f397
Service Name: squid

This binary uses OpenSSL 1.1.0f  25 May 2017. For legal restrictions on distribution see https://www.openssl.org/source/license.html

configure options:  '--prefix=/usr' '--build=x86_64-linux-gnu' '--localstatedir=/var/squid' '--libexecdir=/lib/squid' '--srcdir=.' '--datadir=/share/squid' '--disable-maintainer-mode' '--disable-dependency-tracking' '--disable-silent-rules' '--with-cppunit-basedir=/usr' '--enable-inline' '--enable-async-io=8' '--enable-delay-pools' '--enable-underscores' '--sysconfdir=/etc/squid' '--with-logdir=/var/log/squid' '--with-pidfile=/var/run/squid.pid' '--with-openssl' '--enable-ssl-crtd' '--mandir=/share/man' '--enable-arp-acl' '--enable-esi' '--enable-zph-qos' '--enable-wccpv2' '--with-filedescriptors=65536' '--with-large-files' '--with-default-user=proxy' '--enable-linux-netfilter' '--enable-follow-x-forwarded-for' '--enable-storeio=ufs,aufs,diskd' '--enable-removal-policies=lru,heap' '--enable-icap' '--enable-icap-client' '--enable-cache-digests' '--enable-async-io' '--enable-poll' '--enable-truncate' '--disable-ident-lookups' '--disable-translation' 'build_alias=x86_64-linux-gnu' 'CFLAGS=-g -O2 -fPIE -fstack-protector --param=ssp-buffer-size=4 -Wformat -Werror=format-security -Wall' 'LDFLAGS=-fPIE -pie -Wl,-z,relro -Wl,-z,now' 'CPPFLAGS=-D_FORTIFY_SOURCE=2' 'CXXFLAGS=-g -O2 -fPIE -fstack-protector --param=ssp-buffer-size=4 -Wformat -Werror=format-security'

> 
> 
> 
> >>
> >>> I can Access to spliced site and no any kind of errors in access.log
> >>>
> >>> Any idea?
> >>
> >> Have you read the documentation?
> >>  <https://wiki.squid-cache.org/Features/SslPeekAndSplice>
> >
> > Yes I did, but the thing is (still for me) a bit complex, see what the autor of
> the link posted above said about the squid TLS.
> >
> 
> I assume you mean Alex, that page is a true community effort with
> several authors. He is "just" the latest to update the info as Squid
> changed and/or better ways to write the info were found. Both of us are
> in the main dev team on Squid and neither authored the actual ssl_bump
> processing code.

I refered to this page: http://marek.helion.pl/install/squid.html

> 
> Note that my descriptions are at time hedged with "should", "may", "if",
> etc and Alex outright mentions possible bugs along with similar terms.
> 
> 
> Any suggestions towards simplifying the wiki details, or presenting them
> in an easier to read way would be welcome.
> 
> 
> >>
> >> Break your rules down into the stages as I have above and what is going
> on
> >> becomes a bit more clear.
> >>
> >> Then you can consider what ssl_bump is doing in terms of what info Squid
> >> has available.
> >>  step1: TCP IP:port or CONNECT URI (forward-proxy only)
> >>  step2: TLS clientHello + TLS SNI (if any)
> >>  step3: TLS serverHello + server cert
> >>
> >> The entire directive set is interpreted from top-to-bottom left-to-right
> each
> >> step. First line to fully match is what happens for that step.
> >
> > Above in the current thread, there is a question about the order of steps.
> >
> 
> The "steps" are messages of the TLS handshake and Squid processing code.
> They happen as per the wiki page section "Processing steps". My above is
> a summary of the *data* available at each step at the time each ssl_bump
> processing occurs.
> 
> The variable things are what info the client and server are providing,
> and what your locally defined ACLs (like the "noBumpSites") are actually
> matching on when tested.
> 
> There is also the fact that all this code is still quite volatile so the
> code changes nearly every release or so, and parts of it all are buggy -
> some we know about, some not yet. See above about both Alex and I
> hedging our descriptions a bit at times where things become
> non-deterministic.
> 
> 
> > However I test (today) the site that caused (yesterday) the handshake
> problem and with the original config and now works, so I dont know what to
> think what could be happened.
> 
> TLS is quite a volatile environment. It could be many things.
> 
> Unfortunately it also means further learning we might all get about this
> failure situation is not likely to happen any time soon.
> 
> > I refer to this with the term "original config":
> >
> > ssl_bump peek step1 all
> > ssl_bump peek step2 noBumpSites
> > ssl_bump splice step3 noBumpSites
> > ssl_bump bump
> >
> 
> Even though this works on your previously broken site as well as others
> it still has the problems related to bump sometimes happening at step2
> before server details are known to your Squid.
> 
> 
> Amos
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users



From squid3 at treenet.co.nz  Sun Sep  9 05:34:30 2018
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sun, 9 Sep 2018 17:34:30 +1200
Subject: [squid-users] About SSL peek-n-splice/bump configurations
In-Reply-To: <008001d4479b$b22b9290$1682b7b0$@yahoo.com.ar>
References: <000301d43289$14ed4770$3ec7d650$@yahoo.com.ar>
 <58014777-2b9c-7630-80dd-877c2d2af9d2@measurement-factory.com>
 <001801d432a0$8d102a30$a7307e90$@yahoo.com.ar>
 <6eb5660d-8025-eedd-3ed9-47d30bf22e62@measurement-factory.com>
 <001601d4464c$d1b38910$751a9b30$@yahoo.com.ar>
 <770057a5-3808-3fdc-5bf7-f78d09705913@treenet.co.nz>
 <002001d446cc$148bb580$3da32080$@yahoo.com.ar>
 <c7a4d318-c335-03de-9e5c-73fc258e38c0@treenet.co.nz>
 <008001d4479b$b22b9290$1682b7b0$@yahoo.com.ar>
Message-ID: <04c7931e-cb29-80d1-2a2a-8a1ccb4761f5@treenet.co.nz>

On 9/09/18 5:45 AM, Julian Perconti wrote:
>> -----Mensaje original-----
>> De: squid-users <squid-users-bounces at lists.squid-cache.org> En nombre de
>> Amos Jeffries
>> Enviado el: viernes, 7 de septiembre de 2018 15:19
>> Para: squid-users at lists.squid-cache.org
>> Asunto: Re: [squid-users] About SSL peek-n-splice/bump configurations
>>
>>> So from http://marek.helion.pl/install/squid.html
>>>
>>> We have this configs:
>>>
>>> ssl_bump peek step1 all
>>> ssl_bump peek step2 noBumpSites
>>> ssl_bump splice step3 noBumpSites
>>> ssl_bump stare step2
>>> ssl_bump bump step3
>>>
>>> Is better to use the above conf (staring at step2)? Because you said that
>> bump at step2 is insecure.
>>>
>>> Is the same if a I change the order of the above conf to:
>>>
>>> ssl_bump peek step1 all
>>> ssl_bump peek step2 noBumpSites
>>> ssl_bump stare step2 <<< order changed ssl_bump splice step3
>>> noBumpSites ssl_bump bump step3
>>>
>>
>> What exactly do you think the step1, step2, step3 ACLs here are doing?
> 
> I don not know what -exactly- these ACL are doing; that is what I trying to find out.
> I have some ideas about them, but not the exactly knowledge, for that reason I asked if there is difference between those 2 configs order (because the step is the same)

Okay. When ssl_bump is being processed the first time SslBump1 matches
as true, the second time SslBump2 is true, and third time for SslBump3.
Outside their own step in the TLS handshake process they match false.

This is how you select that a certain line in ssl_bump is *only* to
match and happen at a certain part (step) of the handshake sequence.

> 
>>
>> I hoped it is obvious, but maybe not. Understanding that detail should
>> help resolve at least some of your confusion about these config snippets
>> and how "tiny" changes to them are affecting Squid behaviour in major ways.
>>
> 
> No, it isn't obviuos to me, and yes, I am still trying to understand by re-reading wiki squid doc and other sites about peek and splice decisions and about the steps too.
> 
>>
>>>
>>> So in a brief I think that  config A is more secure.
>>>
>>
>> No. Config (A) from the earlier post actively *creates* insecurity by;
> 
> But,according to http://marek.helion.pl/install/squid.html; It's supposed that config  "A" check server certificate. Because it is peeking at step2 and splicing at step3 the whitelist sites.
> 

The peek at step2 line has another ACL condition which must _also_ be
true for peek to actually happen. In every transaction where that
noBumpSites is *false* the ssl_bump ACL processing continues on and
finds the "bump" line.

(that much is just regular ACL processing logic, not SSL-Bump specific).


Also, Marek is another slightly confused admin like yourself. So that
page follows what he understands and has a few mistakes. It is also from
2 years ago, since then we have fixed some bugs and TLS has had features
added and removed (notably TLS/1.3 rollout begun and SSL formally
obsoleted).

>>
>>  1) hiding any information about the real server security level,
>>     - downgrade attacks. Right down to plaintext levels.
>>
>>  2) hiding any information about the server certificate validity,
>>     - silent third-party MITM.
>>     - invalid certificate attacks.
>>
>>  3) opening the server connection to multiplexed use from multiple
>> clients of Squid,
>>    - consider that in light of (1) and (2)
> 
> I dont understand, in earlier post:
> 
>>> ssl_bump peek step1 all
>>> ssl_bump peek step2 noBumpSites
>>> ssl_bump splice step3 noBumpSites
>>> ssl_bump bump
>>>
>> So... (lets call this config A)
> 
> In this config I think the problem is that squid is peeking at step2 noBumpSites;

Yes, exactly so.

>  but also bump all other sites at step2 (there is no step specified in
the last line -the bump-)
> 

Not quite. That line is the reason bump is being done in (A).

BUT if that line had a step3 ACL there would still be the same cases
hitting ... no lines -> an implict guess from Squid about splice vs bump.

I thought Squid chose bump to preserve the old client-first behaviour.
But your test below indicates that your version is splicing.


> Therefore I think that would be "better" or " less insecure" bumping at step3.
> 

Yes, exactly so - it is more secure to bump at step3.

BUT, as Alex said, bump at step3 does not work after a peek at step2.

The peek happening at step2 means any secret tokens have already been
passed around. Squid then cannot replace those tokens with its own ones.
 'stare' is the action which holds back things like that and filters the
features such that Squid can bump the encryption.


The other bumps, which happen at step2 are where the issues I pointed
out happen. Those vulnerabilities are directly cause by lack of
serverHello details and how insecure it is to trust the client *alone*
about what is going on.
 From Squid's perspective attacks usually come from clients (external
malicious ones), so yeah not a good idea to trust them based on their
own claims (clientHello).


> Conlusion based on these words:
> 
>> So config (A) is trying to do a step3 (handshake with server) when it 
>> has only peek'ed and relayed the clientHello as-is (including any 
>> secret tokens an unknown features the client is trying to use). The 
>> bump action is bound to fail.
>>  ** "stare" is the action which sets up and filters the handshake 
>> ready for bump action at step3 (server handshake with TLS features 
>> Squid knows how to handle).
> 
> I think that my config would be something like this:
> 
> ssl_bump peek step1 all
> 
> ssl_bump peek step2 noBumpSites 
> 
> From squid doc:
> "When a peek rule matches during step 2, Squid proceeds to step3 where it parses the TLS Server Hello and extracts server certificate while preserving the possibility of splicing the client and server connections; peeking at the server certificate usually precludes future bumping"
> 
> And from http://marek.helion.pl/install/squid.html
> Peeking at step 2 will check the name stored in server certificate (CommonName, SubjectAltName) as well. So let's do it! you must enable peek at step 2 and finally splice at step3 (if certName matches the whitelist)
> 

Pause.

What do you want/need to happen to the handshakes that do not match the
noBumpSites criteria for peeking?

Squid has to do something, your test below indicates that is splice.

Anyway, assume the implicit is doing something you don't want or likely
to change as bugs get fixed. Make sure one of your config lines always
matches to be clear.

Resume;

> ssl_bump splice step3 noBumpSites
> (following the ruleset explained above)
> 
> And here I believe that the final bump should be make at step3:
> 
> ssl_bump bump step3	
> 
> OR there is no difference if i dont specify the step in the bump line?
> 

If you omit a step ACL in any ssl_bump line that lines action will apply
at any step where; its ACLs match, the action is valid, and no prior
line has matched.


> summarizing:
> 
> ssl_bump peek step1 all
> ssl_bump peek step2 noBumpSites
> ssl_bump splice step3 noBumpSites
> ssl_bump bump step3
> 
> EDIT:
> Before send this mail to list, I test the squid behaviour and if I dont add a stare (dont know why this happens, stare is the less used option I saw in many examples on the web) line at step2, all accesed site are spliced; so my final config would be taken from helion.pl: (almost defeated by this thread/topic)
> 

Okay, so your Squid implicitly chooses to splice at step2.
Sorry about the confusion. The implicit action(s) have been different in
the past. Which is #1 reason to make an explicit config line for each case.


> ssl_bump peek step1 all               	# at step 1 we're peeking at client TLS-request in order to find the "SNI"
> ssl_bump peek step2 nobumpSites       	# here we're peeking at server certificate
> ssl_bump splice step3 nobumpSites     	# here we're splicing connections which match the whitelist
> ssl_bump stare step2                  		# here we're staring at server certificate
> ssl_bump bump step3                   	# finally we're bumping all other SSL connections at step 3
> 
> The autor of helion.pl says:
> 
> "Honestly I don't see a reason to stare at the server certificate before bumping. Even without "staring" the fake-certificate has the same attributes (Common Name etc.) like the original one. But it might change in the future..."
> 
> So I leave the stare line to avoid splice all the traffic.
> 

Good.


HTH
Amos


From vh1988 at yahoo.com.ar  Sun Sep  9 18:29:13 2018
From: vh1988 at yahoo.com.ar (Julian Perconti)
Date: Sun, 9 Sep 2018 15:29:13 -0300
Subject: [squid-users] About SSL peek-n-splice/bump configurations
In-Reply-To: <04c7931e-cb29-80d1-2a2a-8a1ccb4761f5@treenet.co.nz>
References: <000301d43289$14ed4770$3ec7d650$@yahoo.com.ar>
 <58014777-2b9c-7630-80dd-877c2d2af9d2@measurement-factory.com>
 <001801d432a0$8d102a30$a7307e90$@yahoo.com.ar>
 <6eb5660d-8025-eedd-3ed9-47d30bf22e62@measurement-factory.com>
 <001601d4464c$d1b38910$751a9b30$@yahoo.com.ar>
 <770057a5-3808-3fdc-5bf7-f78d09705913@treenet.co.nz>
 <002001d446cc$148bb580$3da32080$@yahoo.com.ar>
 <c7a4d318-c335-03de-9e5c-73fc258e38c0@treenet.co.nz>
 <008001d4479b$b22b9290$1682b7b0$@yahoo.com.ar>
 <04c7931e-cb29-80d1-2a2a-8a1ccb4761f5@treenet.co.nz>
Message-ID: <019301d4486b$049ebf00$0ddc3d00$@yahoo.com.ar>

> -----Mensaje original-----
> De: squid-users <squid-users-bounces at lists.squid-cache.org> En nombre de
> Amos Jeffries
> Enviado el: domingo, 9 de septiembre de 2018 02:35
> Para: squid-users at lists.squid-cache.org
> Asunto: Re: [squid-users] About SSL peek-n-splice/bump configurations
> 
> On 9/09/18 5:45 AM, Julian Perconti wrote:
> >> -----Mensaje original-----
> >> De: squid-users <squid-users-bounces at lists.squid-cache.org> En nombre
> >> de Amos Jeffries Enviado el: viernes, 7 de septiembre de 2018 15:19
> >> Para: squid-users at lists.squid-cache.org
> >> Asunto: Re: [squid-users] About SSL peek-n-splice/bump configurations
> >>
> >>> So from http://marek.helion.pl/install/squid.html
> >>>
> >>> We have this configs:
> >>>
> >>> ssl_bump peek step1 all
> >>> ssl_bump peek step2 noBumpSites
> >>> ssl_bump splice step3 noBumpSites
> >>> ssl_bump stare step2
> >>> ssl_bump bump step3
> >>>
> >>> Is better to use the above conf (staring at step2)? Because you said
> >>> that
> >> bump at step2 is insecure.
> >>>
> >>> Is the same if a I change the order of the above conf to:
> >>>
> >>> ssl_bump peek step1 all
> >>> ssl_bump peek step2 noBumpSites
> >>> ssl_bump stare step2 <<< order changed ssl_bump splice step3
> >>> noBumpSites ssl_bump bump step3
> >>>
> >>
> >> What exactly do you think the step1, step2, step3 ACLs here are doing?
> >
> > I don not know what -exactly- these ACL are doing; that is what I trying to
> find out.
> > I have some ideas about them, but not the exactly knowledge, for that
> > reason I asked if there is difference between those 2 configs order
> > (because the step is the same)
> 
> Okay. When ssl_bump is being processed the first time SslBump1 matches as
> true, the second time SslBump2 is true, and third time for SslBump3.
> Outside their own step in the TLS handshake process they match false.
> 
> This is how you select that a certain line in ssl_bump is *only* to match and
> happen at a certain part (step) of the handshake sequence.

Well. 

First of all thank You for your time and explanation, and patient of course. It's much appretiated.

I Hope this thread helps to others that have a similar confusion and doubts like me.
Still the things are not "entirely" clear, I will quote.


...So that means that squid processes the SslBump directives:

1: maybe more than one time in a single request...?

2: In a sequential order (as You or Alex said in an earlier post)

- ... and "automagically" determine what to do if the ACL match or not? 
With this I mean, for example.., that in a config could be first, in this order, a step1 directive then a step3 directive and finally a step2? With an ACL of course. 
To clarify the SslBump order is determinant but its also depends in what I want to do with steps and ACLs.

Lets say...it is *not* mandatory to tell squid SslBump steps directives like:

At step1 do x
At step 2 do y
At step3 do z

And so on...

> 
> >
> >>
> >> I hoped it is obvious, but maybe not. Understanding that detail
> >> should help resolve at least some of your confusion about these
> >> config snippets and how "tiny" changes to them are affecting Squid
> behaviour in major ways.
> >>
> >
> > No, it isn't obviuos to me, and yes, I am still trying to understand by re-
> reading wiki squid doc and other sites about peek and splice decisions and
> about the steps too.
> >
> >>
> >>>
> >>> So in a brief I think that  config A is more secure.
> >>>
> >>
> >> No. Config (A) from the earlier post actively *creates* insecurity
> >> by;
> >
> > But,according to http://marek.helion.pl/install/squid.html; It's supposed
> that config  "A" check server certificate. Because it is peeking at step2 and
> splicing at step3 the whitelist sites.
> >
> 
> The peek at step2 line has another ACL condition which must _also_ be true
> for peek to actually happen. In every transaction where that noBumpSites is
> *false* the ssl_bump ACL processing continues on and finds the "bump" line.
> 
> (that much is just regular ACL processing logic, not SSL-Bump specific).
> 
> 
> Also, Marek is another slightly confused admin like yourself. So that page
> follows what he understands and has a few mistakes. It is also from
> 2 years ago, since then we have fixed some bugs and TLS has had features
> added and removed (notably TLS/1.3 rollout begun and SSL formally
> obsoleted).
> 
> >>
> >>  1) hiding any information about the real server security level,
> >>     - downgrade attacks. Right down to plaintext levels.
> >>
> >>  2) hiding any information about the server certificate validity,
> >>     - silent third-party MITM.
> >>     - invalid certificate attacks.
> >>
> >>  3) opening the server connection to multiplexed use from multiple
> >> clients of Squid,
> >>    - consider that in light of (1) and (2)
> >
> > I dont understand, in earlier post:
> >
> >>> ssl_bump peek step1 all
> >>> ssl_bump peek step2 noBumpSites
> >>> ssl_bump splice step3 noBumpSites
> >>> ssl_bump bump
> >>>
> >> So... (lets call this config A)
> >
> > In this config I think the problem is that squid is peeking at step2
> > noBumpSites;
> 
> Yes, exactly so.
> 
> >  but also bump all other sites at step2 (there is no step specified in
> the last line -the bump-)
> >
> 
> Not quite. That line is the reason bump is being done in (A).
> 
> BUT if that line had a step3 ACL there would still be the same cases hitting ...
> no lines -> an implict guess from Squid about splice vs bump.

Therefore is always better to tell to squid the step. To avoid ambiguous decison, etc.

> 
> I thought Squid chose bump to preserve the old client-first behaviour.
> But your test below indicates that your version is splicing.

That is what I want to do. 
But I dont understand why sites are failling if I am splicing those ones. May be cause I am peeking at step2 and splice at step3 the noBumpSites.

> 
> 
> > Therefore I think that would be "better" or " less insecure" bumping at
> step3.
> >
> 
> Yes, exactly so - it is more secure to bump at step3.
> 
> BUT, as Alex said, bump at step3 does not work after a peek at step2.

Yes, according to the doc, peeking at step2 precules future bumping. I dont want to bump the whitelist, I want to tunnel instead.
That is why peek all at step1 but peek noBumpSites at step2.

> 
> The peek happening at step2 means any secret tokens have already been
> passed around. Squid then cannot replace those tokens with its own ones.
>  'stare' is the action which holds back things like that and filters the features
> such that Squid can bump the encryption.
> 

When I should stare?

> 
> The other bumps, which happen at step2 are where the issues I pointed out
> happen. Those vulnerabilities are directly cause by lack of serverHello details
> and how insecure it is to trust the client *alone* about what is going on.
>  From Squid's perspective attacks usually come from clients (external
> malicious ones), so yeah not a good idea to trust them based on their own
> claims (clientHello).
> 

Peeking at step2 does not prevent this?

> 
> > Conlusion based on these words:
> >
> >> So config (A) is trying to do a step3 (handshake with server) when it
> >> has only peek'ed and relayed the clientHello as-is (including any
> >> secret tokens an unknown features the client is trying to use). The
> >> bump action is bound to fail.
> >>  ** "stare" is the action which sets up and filters the handshake
> >> ready for bump action at step3 (server handshake with TLS features
> >> Squid knows how to handle).
> >
> > I think that my config would be something like this:
> >
> > ssl_bump peek step1 all
> >
> > ssl_bump peek step2 noBumpSites
> >
> > From squid doc:
> > "When a peek rule matches during step 2, Squid proceeds to step3 where it
> parses the TLS Server Hello and extracts server certificate while preserving
> the possibility of splicing the client and server connections; peeking at the
> server certificate usually precludes future bumping"
> >
> > And from http://marek.helion.pl/install/squid.html
> > Peeking at step 2 will check the name stored in server certificate
> > (CommonName, SubjectAltName) as well. So let's do it! you must enable
> > peek at step 2 and finally splice at step3 (if certName matches the
> > whitelist)
> >
> 
> Pause.
> 
> What do you want/need to happen to the handshakes that do not match the
> noBumpSites criteria for peeking?

Quick answer: Bump.

What I exactly  want to do is:
Secure bump to the sites that are not listed in "noBumoSites" ACL and secure splice/tunnel to sites listed in "noBumpSites". 

> 
> Squid has to do something, your test below indicates that is splice.
> 
> Anyway, assume the implicit is doing something you don't want or likely to
> change as bugs get fixed. Make sure one of your config lines always matches
> to be clear.
> 
> Resume;
> 
> > ssl_bump splice step3 noBumpSites
> > (following the ruleset explained above)
> >
> > And here I believe that the final bump should be make at step3:
> >
> > ssl_bump bump step3
> >
> > OR there is no difference if i dont specify the step in the bump line?
> >
> 
> If you omit a step ACL in any ssl_bump line that lines action will apply at any
> step where; its ACLs match, the action is valid, and no prior line has matched.

An old doubt, answered by Alex and now by You, thanks to both.
So it is always better to tell squid what "he" exactly has to do in each step. Or avoid to let him to "decide"...

> 
> 
> > summarizing:
> >
> > ssl_bump peek step1 all
> > ssl_bump peek step2 noBumpSites
> > ssl_bump splice step3 noBumpSites
> > ssl_bump bump step3
> >
> > EDIT:
> > Before send this mail to list, I test the squid behaviour and if I
> > dont add a stare (dont know why this happens, stare is the less used
> > option I saw in many examples on the web) line at step2, all accesed
> > site are spliced; so my final config would be taken from helion.pl:
> > (almost defeated by this thread/topic)
> >
> 
> Okay, so your Squid implicitly chooses to splice at step2.
> Sorry about the confusion. The implicit action(s) have been different in the
> past. Which is #1 reason to make an explicit config line for each case.
> 
> 
> > ssl_bump peek step1 all               	# at step 1 we're peeking at client TLS-request in order to find the "SNI"
> > ssl_bump peek step2 nobumpSites       	# here we're peeking at server certificate
> > ssl_bump splice step3 nobumpSites     	# here we're splicing connections which match the whitelist
> > ssl_bump stare step2                  		# here we're staring at server certificate
> > ssl_bump bump step3                   	# finally we're bumping all other SSL connections at step 3

Wouldn't be less ambiguous to squid if I do this change:?

ssl_bump peek step1 all > A question: I am not here peeking the noBumpSites list too? Should I add an !noBumpSites? to the end of this line? Just a doubt.
ssl_bump peek step2 nobumpSites 
ssl_bump splice step3 nobumpSites
ssl_bump stare step2 nobumpSites > explicit staring whitelist (I haven't test this) it is just an idea...it make sense? (also I dont understand what exactly the satre action do)
ssl_bump bump step3 

I think this config avoid the "old client-first insecure" behaviour. I am right? And squid check server-certitifacte before splice.

> >
> > The autor of helion.pl says:
> >
> > "Honestly I don't see a reason to stare at the server certificate before
> bumping. Even without "staring" the fake-certificate has the same attributes
> (Common Name etc.) like the original one. But it might change in the
> future..."
> >
> > So I leave the stare line to avoid splice all the traffic.
> >
> 
> Good.

Final Words, the site that start to fail from one day to antoher is www.santanderrio.com.ar. All other spliced sites never stops working.

I will use ssllabs.com to test the site.

The thing that cause a lot of confusion to me is that in peek-n-splice environment, I can peek, plice,bump,starte in many steps (1,2,3).
That make the things a bit complex to decide. And of course to understand.
> 
> 
> HTH

Yes, it really helps.

Thank You very much.

> Amos
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users



From squid3 at treenet.co.nz  Mon Sep 10 04:12:45 2018
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Mon, 10 Sep 2018 16:12:45 +1200
Subject: [squid-users] About SSL peek-n-splice/bump configurations
In-Reply-To: <019301d4486b$049ebf00$0ddc3d00$@yahoo.com.ar>
References: <000301d43289$14ed4770$3ec7d650$@yahoo.com.ar>
 <58014777-2b9c-7630-80dd-877c2d2af9d2@measurement-factory.com>
 <001801d432a0$8d102a30$a7307e90$@yahoo.com.ar>
 <6eb5660d-8025-eedd-3ed9-47d30bf22e62@measurement-factory.com>
 <001601d4464c$d1b38910$751a9b30$@yahoo.com.ar>
 <770057a5-3808-3fdc-5bf7-f78d09705913@treenet.co.nz>
 <002001d446cc$148bb580$3da32080$@yahoo.com.ar>
 <c7a4d318-c335-03de-9e5c-73fc258e38c0@treenet.co.nz>
 <008001d4479b$b22b9290$1682b7b0$@yahoo.com.ar>
 <04c7931e-cb29-80d1-2a2a-8a1ccb4761f5@treenet.co.nz>
 <019301d4486b$049ebf00$0ddc3d00$@yahoo.com.ar>
Message-ID: <6b762c34-f0c4-2d9a-6616-432305af160b@treenet.co.nz>

On 10/09/18 6:29 AM, Julian Perconti wrote:
>> -----Mensaje original-----
>> De: Amos Jeffries
>>
>> On 9/09/18 5:45 AM, Julian Perconti wrote:
>>>> -----Mensaje original-----
>>>> De: Amos Jeffries
>>>>
>>>>> So from http://marek.helion.pl/install/squid.html
>>>>>
>>>>> We have this configs:
>>>>>
>>>>> ssl_bump peek step1 all
>>>>> ssl_bump peek step2 noBumpSites
>>>>> ssl_bump splice step3 noBumpSites
>>>>> ssl_bump stare step2
>>>>> ssl_bump bump step3
>>>>>
>>>>> Is better to use the above conf (staring at step2)? Because you said
>>>>> that
>>>> bump at step2 is insecure.
>>>>>
>>>>> Is the same if a I change the order of the above conf to:
>>>>>
>>>>> ssl_bump peek step1 all
>>>>> ssl_bump peek step2 noBumpSites
>>>>> ssl_bump stare step2 <<< order changed ssl_bump splice step3
>>>>> noBumpSites ssl_bump bump step3
>>>>>
>>>>
>>>> What exactly do you think the step1, step2, step3 ACLs here are doing?
>>>
>>> I don not know what -exactly- these ACL are doing; that is what I trying to
>> find out.
>>> I have some ideas about them, but not the exactly knowledge, for that
>>> reason I asked if there is difference between those 2 configs order
>>> (because the step is the same)
>>
>> Okay. When ssl_bump is being processed the first time SslBump1 matches as
>> true, the second time SslBump2 is true, and third time for SslBump3.
>> Outside their own step in the TLS handshake process they match false.
>>
>> This is how you select that a certain line in ssl_bump is *only* to match and
>> happen at a certain part (step) of the handshake sequence.
> 
> Well. 
> 
> First of all thank You for your time and explanation, and patient of course. It's much appretiated.
> 
> I Hope this thread helps to others that have a similar confusion and doubts like me.
> Still the things are not "entirely" clear, I will quote.
> 
> 
> ...So that means that squid processes the SslBump directives:
> 
> 1: maybe more than one time in a single request...?
> 

Yes. Up to 3 times. A peek or splice action causes another check later.


> 2: In a sequential order (as You or Alex said in an earlier post)
> 
> - ... and "automagically" determine what to do if the ACL match or not? 
> With this I mean, for example.., that in a config could be first, in this order, a step1 directive then a step3 directive and finally a step2? With an ACL of course. 

No, this order is fixed and follows the TLS handshake stages/steps:
 step1, then step2, then step3. Exact same order as on the Squid wiki page.

The automagic is only applied when
 a) no ssl_bump lines at all match (auto-decide for you), and
 b) an action that matches is not valid for the step (auto-ignore that
line).


> To clarify the SslBump order is determinant but its also depends in what I want to do with steps and ACLs.
> 

Yes. Though what you understand by that statement still seems to differ
a bit from what we understand it to mean.


> Lets say...it is *not* mandatory to tell squid SslBump steps directives like:
> 
> At step1 do x
> At step 2 do y
> At step3 do z
> 
> And so on...
> 

Well, its true you don't *have* to . BUt also you don't have to use
SSL-Bump at all either.

If you want to be sure what Squid is doing, and that it will continue to
do that reliably then telling it for each step is a good idea.



>>
>>>
>>>>
>>>> I hoped it is obvious, but maybe not. Understanding that detail
>>>> should help resolve at least some of your confusion about these
>>>> config snippets and how "tiny" changes to them are affecting Squid
>> behaviour in major ways.
>>>>
>>>
>>> No, it isn't obviuos to me, and yes, I am still trying to understand by re-
>> reading wiki squid doc and other sites about peek and splice decisions and
>> about the steps too.
>>>
>>>>
>>>>>
>>>>> So in a brief I think that  config A is more secure.
>>>>>
>>>>
>>>> No. Config (A) from the earlier post actively *creates* insecurity
>>>> by;
>>>
>>> But,according to http://marek.helion.pl/install/squid.html; It's supposed
>> that config  "A" check server certificate. Because it is peeking at step2 and
>> splicing at step3 the whitelist sites.
>>>
>>
>> The peek at step2 line has another ACL condition which must _also_ be true
>> for peek to actually happen. In every transaction where that noBumpSites is
>> *false* the ssl_bump ACL processing continues on and finds the "bump" line.
>>
>> (that much is just regular ACL processing logic, not SSL-Bump specific).
>>
>>
>> Also, Marek is another slightly confused admin like yourself. So that page
>> follows what he understands and has a few mistakes. It is also from
>> 2 years ago, since then we have fixed some bugs and TLS has had features
>> added and removed (notably TLS/1.3 rollout begun and SSL formally
>> obsoleted).
>>
>>>>
>>>>  1) hiding any information about the real server security level,
>>>>     - downgrade attacks. Right down to plaintext levels.
>>>>
>>>>  2) hiding any information about the server certificate validity,
>>>>     - silent third-party MITM.
>>>>     - invalid certificate attacks.
>>>>
>>>>  3) opening the server connection to multiplexed use from multiple
>>>> clients of Squid,
>>>>    - consider that in light of (1) and (2)
>>>
>>> I dont understand, in earlier post:
>>>
>>>>> ssl_bump peek step1 all
>>>>> ssl_bump peek step2 noBumpSites
>>>>> ssl_bump splice step3 noBumpSites
>>>>> ssl_bump bump
>>>>>
>>>> So... (lets call this config A)
>>>
>>> In this config I think the problem is that squid is peeking at step2
>>> noBumpSites;
>>
>> Yes, exactly so.
>>
>>>  but also bump all other sites at step2 (there is no step specified in
>> the last line -the bump-)
>>>
>>
>> Not quite. That line is the reason bump is being done in (A).
>>
>> BUT if that line had a step3 ACL there would still be the same cases hitting ...
>> no lines -> an implict guess from Squid about splice vs bump.
> 
> Therefore is always better to tell to squid the step. To avoid ambiguous decison, etc.
> 
>>
>> I thought Squid chose bump to preserve the old client-first behaviour.
>> But your test below indicates that your version is splicing.
> 
> That is what I want to do. 
> But I dont understand why sites are failling if I am splicing those ones. May be cause I am peeking at step2 and splice at step3 the noBumpSites.
> 
>>
>>
>>> Therefore I think that would be "better" or " less insecure" bumping at
>> step3.
>>>
>>
>> Yes, exactly so - it is more secure to bump at step3.
>>
>> BUT, as Alex said, bump at step3 does not work after a peek at step2.
> 
> Yes, according to the doc, peeking at step2 precules future bumping. I dont want to bump the whitelist, I want to tunnel instead.
> That is why peek all at step1 but peek noBumpSites at step2.
> 
>>
>> The peek happening at step2 means any secret tokens have already been
>> passed around. Squid then cannot replace those tokens with its own ones.
>>  'stare' is the action which holds back things like that and filters the features
>> such that Squid can bump the encryption.
>>
> 
> When I should stare?

When you, as the admin with meta knowledge about the overall policy -
know that a bump is wanted to happen later.


> 
>>
>> The other bumps, which happen at step2 are where the issues I pointed out
>> happen. Those vulnerabilities are directly cause by lack of serverHello details
>> and how insecure it is to trust the client *alone* about what is going on.
>>  From Squid's perspective attacks usually come from clients (external
>> malicious ones), so yeah not a good idea to trust them based on their own
>> claims (clientHello).
>>
> 
> Peeking at step2 does not prevent this?

Peeking at step2 precludes / forbids later bumping, so yes.

What I have been trying to highlight is that there is traffic that
config (A) allows to go through *without* any peek at step2. It reaches
the "ssl_bump bump" line.



> 
>>
>>> Conlusion based on these words:
>>>
>>>> So config (A) is trying to do a step3 (handshake with server) when it
>>>> has only peek'ed and relayed the clientHello as-is (including any
>>>> secret tokens an unknown features the client is trying to use). The
>>>> bump action is bound to fail.
>>>>  ** "stare" is the action which sets up and filters the handshake
>>>> ready for bump action at step3 (server handshake with TLS features
>>>> Squid knows how to handle).
>>>
>>> I think that my config would be something like this:
>>>
>>> ssl_bump peek step1 all
>>>
>>> ssl_bump peek step2 noBumpSites
>>>
>>> From squid doc:
>>> "When a peek rule matches during step 2, Squid proceeds to step3 where it
>> parses the TLS Server Hello and extracts server certificate while preserving
>> the possibility of splicing the client and server connections; peeking at the
>> server certificate usually precludes future bumping"
>>>
>>> And from http://marek.helion.pl/install/squid.html
>>> Peeking at step 2 will check the name stored in server certificate
>>> (CommonName, SubjectAltName) as well. So let's do it! you must enable
>>> peek at step 2 and finally splice at step3 (if certName matches the
>>> whitelist)
>>>
>>
>> Pause.
>>
>> What do you want/need to happen to the handshakes that do not match the
>> noBumpSites criteria for peeking?
> 
> Quick answer: Bump.
> 

Then put the below line after your "peek step2 noBumPSites" line:

  ssl_bump stare step2


> What I exactly  want to do is:
> Secure bump to the sites that are not listed in "noBumoSites" ACL and secure splice/tunnel to sites listed in "noBumpSites". 
> 
>>
>> Squid has to do something, your test below indicates that is splice.
>>
>> Anyway, assume the implicit is doing something you don't want or likely to
>> change as bugs get fixed. Make sure one of your config lines always matches
>> to be clear.
>>
>> Resume;
>>
>>> ssl_bump splice step3 noBumpSites
>>> (following the ruleset explained above)
>>>
>>> And here I believe that the final bump should be make at step3:
>>>
>>> ssl_bump bump step3
>>>
>>> OR there is no difference if i dont specify the step in the bump line?
>>>
>>
>> If you omit a step ACL in any ssl_bump line that lines action will apply at any
>> step where; its ACLs match, the action is valid, and no prior line has matched.
> 
> An old doubt, answered by Alex and now by You, thanks to both.
> So it is always better to tell squid what "he" exactly has to do in each step. Or avoid to let him to "decide"...
> 
>>
>>
>>> summarizing:
>>>
>>> ssl_bump peek step1 all
>>> ssl_bump peek step2 noBumpSites
>>> ssl_bump splice step3 noBumpSites
>>> ssl_bump bump step3
>>>
>>> EDIT:
>>> Before send this mail to list, I test the squid behaviour and if I
>>> dont add a stare (dont know why this happens, stare is the less used
>>> option I saw in many examples on the web) line at step2, all accesed
>>> site are spliced; so my final config would be taken from helion.pl:
>>> (almost defeated by this thread/topic)
>>>
>>
>> Okay, so your Squid implicitly chooses to splice at step2.
>> Sorry about the confusion. The implicit action(s) have been different in the
>> past. Which is #1 reason to make an explicit config line for each case.
>>
>>
>>> ssl_bump peek step1 all               	# at step 1 we're peeking at client TLS-request in order to find the "SNI"
>>> ssl_bump peek step2 nobumpSites       	# here we're peeking at server certificate
>>> ssl_bump splice step3 nobumpSites     	# here we're splicing connections which match the whitelist
>>> ssl_bump stare step2                  		# here we're staring at server certificate
>>> ssl_bump bump step3                   	# finally we're bumping all other SSL connections at step 3
> 
> Wouldn't be less ambiguous to squid if I do this change:?
> 
> ssl_bump peek step1 all > A question: I am not here peeking the noBumpSites list too? Should I add an !noBumpSites? to the end of this line? Just a doubt.
> ssl_bump peek step2 nobumpSites 
> ssl_bump splice step3 nobumpSites
> ssl_bump stare step2 nobumpSites > explicit staring whitelist (I haven't test this) it is just an idea...it make sense?


The ACLs on the line above are the same as the peek line earlier. So the
peek line matched already, nothing reaches this line.
 <https://wiki.squid-cache.org/SquidFaq/SquidAcl#Common_Mistakes>

Less ambiguous, yes, if your knowledge of Squid ACLs is low. The FAQ
link above should help a bit here.


Your policy ("Quick answer: Bump.") was to prefer bump'ing. For that to
happen as step 3 it needs a stare first at step 2.

So consider the stare here as the normal action this step2 is supposed
to perform. With the peek line being the whitelist preventing stare+bump
for special cases.




>  (also I dont understand what exactly the satre action do)

Hmm. Think of "peek" as a postal worker reading postcards people send in
the mail. "stare" as the postal worker both reading and rewriting them
to remove words (s)he doesn't like or understand.

Say if the a postcard ended with the words "never qwertyuio". A peek'ing
postie would still deliver it unchanged, a stare'ing postie would
deliver a postcard with the last word "never".

If the sender/receiver of the postcard had agreed to start using crypto
every time a message ended with "qwertyuio" - the peeking postie would
then just see a bunch of garbage/crypted postcards start to happen. The
stare'ing one would be able to read the content, maybe even continue
changing things.

The exact details are more complex of course, but essentially the same
things going on.



> I think this config avoid the "old client-first insecure" behaviour. I am right? And squid check server-certitifacte before splice.
> 

Step 3 is where the "preclude" starts to matter.


The Step 2 action determines whether the original clientHello or one
rewritten by Squid gets sent to the server in order to get a serverHello
out of it.

AIUI, "stare step2" precludes "splice step3". So that line should be
ignored by Squid unless there was a peek done at step2.

To follow that postal analogy; client-first is like the postal worker
simply replying to peoples postcards instead of delivering them. If/when
they have to answer a question, writing a wholly new/different message
to find out for itself first before answering.


>>>
>>> The autor of helion.pl says:
>>>
>>> "Honestly I don't see a reason to stare at the server certificate before
>> bumping. Even without "staring" the fake-certificate has the same attributes
>> (Common Name etc.) like the original one. But it might change in the
>> future..."
>>>
>>> So I leave the stare line to avoid splice all the traffic.
>>>
>>
>> Good.
> 
> Final Words, the site that start to fail from one day to antoher is www.santanderrio.com.ar. All other spliced sites never stops working.
> 
> I will use ssllabs.com to test the site.
> 
> The thing that cause a lot of confusion to me is that in peek-n-splice environment, I can peek, plice,bump,starte in many steps (1,2,3).
> That make the things a bit complex to decide. And of course to understand.


Nod. One thing to be clear on is that TLS is designed to actively
prevent MITM doing things like bump'ing. A client and server which are
both using TLS properly cannot be bump'ed. They can only be peek'd and
splice'd.

There are many features in TLS that partially or fully work towards that
goal. So it depends on which of those are negotiated between the
endpoints (*if* negotiated) and also whether Squid is up to date enough
to detect and prevent them when stare'ing.

There are also a bunch of different protocols happening in the HTTP
environment these days (QUICK, CoAP, SPDY, HTTP/2 etc) - if one of those
which Squid does not (yet) support is used to transfer TLS related data
the endpoints may be transmitting info the proxy cannot affect.


There is still an arms-race going on in TLS these days (though slowing a
bit now). Thus the whole "only use the latest Squid release when
SSL-Bump'ing" line we push so hard.

Amos


From srnhari at gmail.com  Mon Sep 10 12:18:55 2018
From: srnhari at gmail.com (Hariharan Sethuraman)
Date: Mon, 10 Sep 2018 17:48:55 +0530
Subject: [squid-users] Squid 4.2 : caching is not working
In-Reply-To: <CANfnjgL2vmVJXT5LRb=UUv0u_zx0NuNG_5ED6kV+eOEjLMCL2Q@mail.gmail.com>
References: <CANfnjgLuEfJdCj-xvi33om7ESRnFxS4zANpckV-t4UUSusv5Sw@mail.gmail.com>
 <ce991c90-bb50-a3ca-9113-038a5e8a133a@treenet.co.nz>
 <CANfnjgJufRBou6cyA51=CZr8c7xdOFvp-+Gqga9Y-cPh-SsVSA@mail.gmail.com>
 <CANfnjgJ7RY6SSFHHXxwnx8EsFbucqAxSOA=MCN1QoDhxObickQ@mail.gmail.com>
 <CANfnjgKJTy3U_DBg=gmu0_1DmDhS4DZOHYjxqbepzO5y0JtSAA@mail.gmail.com>
 <22c4ec2c-dd11-7391-65ea-a5a9ca38e794@treenet.co.nz>
 <CANfnjgL2vmVJXT5LRb=UUv0u_zx0NuNG_5ED6kV+eOEjLMCL2Q@mail.gmail.com>
Message-ID: <CANfnjgJJPh123FugntBSrfV6t=Hk4Lj3eRQ8LUdW=+A=OCV-Lg@mail.gmail.com>

Hi All,

I have two things to clarify:
1) In earlier email (snipped below), Amos told that is caching and
scheduled to download - does it mean that we got the answer and do some
override?
--------------------
----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
>     This is what I see when the download is in progress:
>
>     KEY 44000000000000000902000000000000
>             STORE_PENDING NOT_IN_MEMORY SWAPOUT_NONE PING_DONE
>             RELEASE_REQUEST,DISPATCHED,PRIVATE,VALIDATED

So file stored in memory and scheduled for removal.
--------------------
----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

2) With more debug_options enabled, I see that it is not caching because
the response is part of authenticated flow. Is there a way I can override
this?
-   Initially when this file download starts, it gets authorized (I think
that is why I see HEAD request is sent to target which has Cache-Control:
max-age=0) and then the subsequent GET requests (dont have any
cache-control header) download the chunk using adjusted range and offset.
-   But the subsequent GET reply seems set with auth flag, see the code
below snipped from source (
https://github.com/squid-cache/squid/blob/4f1c93a7a0d14eec223e199275ce570d840f71bc/src/http.cc
).
        // RFC 2068, sec 14.9.4 - MUST NOT cache any response with
Authentication UNLESS certain CC controls are present
    // allow HTTP violations to IGNORE those controls (ie re-block caching
Auth)
    if (request && (request->flags.auth || request->flags.authSent)) {
        if (!rep->cache_control)
            return decision.make(ReuseDecision::reuseNot,
                                 "authenticated and server reply missing
Cache-Control");
-   I tried adding override-expire in the cgi-bin refresh pattern, but that
will override only for max-age in Cache-control but not relevant for auth
flag.

Please find the logs below:
Please let me know if I am missing something.

Thanks,
Hari

Refresh pattern (URL:
https://example.com/pcgi-bin/swdld/download.cgi?dwnld_code=xhM...):
refresh_pattern ^ftp:           1440    20%     10080
refresh_pattern ^gopher:        1440    0%      1440
refresh_pattern -i (/cgi-bin/|\?) 0     0%      0
refresh_pattern .               0       20%     4320

Logs:
2018/09/10 09:57:24.864| 11,2| http.cc(723) processReplyHeader: HTTP Server
RESPONSE:
---------
HTTP/1.1 206 Partial Content
Date: Mon, 10 Sep 2018 09:57:20 GMT^M
Server: Apache/2.2^M
Content-Disposition: attachment; filename=somefile.iso;^M
Last-Modified: Thu, 22 Mar 2018 02:11:11 GMT^M
ETag: "4ad8a61-1193b000-567f6d2466dc0"^M
Accept-Ranges: bytes^M
Content-Length: 294342471^M
Content-Range: bytes 549049-294891519/294891520^M
Keep-Alive: timeout=5, max=100^M
Connection: Keep-Alive^M
Content-Type: application/unknown^M
^M
----------
2018/09/10 09:57:24.864| 11,5| Client.cc(134) setVirginReply:
0x5560bf163d78 setting virgin reply to 0x5560bf162c20
2018/09/10 09:57:24.864| ctx: exit level  0
2018/09/10 09:57:24.864| 83,3| AccessCheck.cc(42) Start: adaptation off,
skipping
2018/09/10 09:57:24.864| 11,5| Client.cc(969) adaptOrFinalizeReply:
adaptationAccessCheckPending=0
2018/09/10 09:57:24.864| 11,5| Client.cc(152) setFinalReply: 0x5560bf163d78
setting final reply to 0x5560bf162c20
2018/09/10 09:57:24.864| 20,3| store.cc(1807) replaceHttpReply:
StoreEntry::replaceHttpReply:
https://example.com/pcgi-bin/swdld/download.cgi?dwnld_code=xhMnkw8Z-oECuFusb12luTTCm0rP8jZiRFu8gsXRtc
2018/09/10 09:57:24.864| ctx: enter level  0: '
https://example.com/pcgi-bin/swdld/download.cgi?dwnld_code=xhMnkw8Z-oECuFusb12luTTCm0rP8jZiRFu8gsXRtcoacGcTu6dv-dkLcT4lqtgvM70n8-ucJsj09lRYt_a0t7_M5
2018/09/10 09:57:24.864| 11,3| http.cc(907) haveParsedReplyHeaders: HTTP
CODE: 206
2018/09/10 09:57:24.864| 73,3| HttpRequest.cc(664) storeId: sent back
effectiveRequestUrl:
https://dl.cisco.com/pcgi-bin/swdld/download.cgi?dwnld_code=xhMnkw8Z-oECuFusb12luTTCm0rP8jZiRFu8gsXRtcoac
2018/09/10 09:57:24.864| 20,3| Controller.cc(386) peek:
76E544615E001DBF49EF0F94EE0A8F9A
2018/09/10 09:57:24.865| 20,4| Controller.cc(420) peek: cannot locate
76E544615E001DBF49EF0F94EE0A8F9A
2018/09/10 09:57:24.865| 20,3| store.cc(450) releaseRequest: 0
e:=p2IV/0x5560bf16ac60*3
2018/09/10 09:57:24.865| 20,3| store.cc(580) setPrivateKey: 01
e:=p2IV/0x5560bf16ac60*3
2018/09/10 09:57:24.865| 11,3| http.cc(982) haveParsedReplyHeaders:
decided: do not cache and do not share because authenticated and server
reply missing Cache-Control; HTTP status 206 e:=p2XIV/0x
2018/09/10 09:57:24.865| ctx: exit level  0


On Sat, Sep 8, 2018 at 8:13 PM Hariharan Sethuraman <srnhari at gmail.com>
wrote:

> But the partial data is been continuously sending back to client. Squid
> didn't wait for the complete file to download
>
> On Sat, 8 Sep 2018, 20:08 Amos Jeffries, <squid3 at treenet.co.nz> wrote:
>
>> On 8/09/18 7:44 PM, Hariharan Sethuraman wrote:
>> > And here is my HTTP request and reply:
>> > 1. HEAD request to read image information
>> > 2. GET request to download the image
>> >
>>
>> Not quite. GET partial / Range request to fetch the content.
>>
>> Squid converted it into a full request for the backend server due to
>> range_offset_limit -1. But that does mean Squid had to download ~240MB
>> of data before anything starts being sent to the client.
>>
>>
>>
>> > (2)
>> > 2018/09/08 07:28:39.155| 11,2| client_side.cc(1278) parseHttpRequest:
>> > HTTP Client REQUEST:
>> > ---------
>> > GET /DcKbz9kqMQXK-zp95pv9LH11kjhTpxOJsJ-1FYEL4
>> > Host: example.com:3129 <http://example.com:3129>^M
>> > Range: bytes=242819145-^M
>> > User-Agent: curl/7.56.1^M
>> > Accept: */*^M
>> >
>>
>>
>> > 2018/09/08 07:28:39.938| 11,2| http.cc(2261) sendRequest: HTTP Server
>> > REQUEST:
>> > ---------
>> > GET /DcKbz9kqMQXK-zp95pv9LH11kjhTpxOJsJ-1FYEL4
>> > User-Agent: curl/7.56.1^M
>> > Accept: */*^M
>> > Host: exampletarget.com <http://exampletarget.com>^M
>> > Via: 1.1 jb7mgd (squid/4.2)^M
>> > Surrogate-Capability: jb7mgd="Surrogate/1.0"^M
>> > X-Forwarded-For: **.**.**.**^M
>> > Cache-Control: max-age=0^M
>> > Connection: keep-alive^M
>> >
>> > 2018/09/08 07:28:44.359| 11,2| http.cc(723) processReplyHeader: HTTP
>> > Server RESPONSE:
>> > ---------
>> > HTTP/1.1 200 OK
>> > Date: Sat, 08 Sep 2018 07:28:40 GMT^M
>> > Server: Apache/2.2^M
>> > Content-Disposition: attachment; filename=somefile.iso;^M
>> > Last-Modified: Fri, 20 Jul 2018 18:23:10 GMT^M
>> > ETag: "4a54c59-11653800-571726350bf80"^M
>> > Accept-Ranges: bytes^M
>> > Content-Length: 291846144^M
>> > Keep-Alive: timeout=5, max=100^M
>> > Connection: Keep-Alive^M
>> > Content-Type: application/unknown^M
>> >
>> > 2018/09/08 07:28:44.361| 11,2| Stream.cc(267) sendStartOfMessage: HTTP
>> > Client REPLY:
>> > ---------
>> > HTTP/1.1 206 Partial Content^M
>> > Date: Sat, 08 Sep 2018 07:28:40 GMT^M
>> > Server: Apache/2.2^M
>> > Content-Disposition: attachment; filename=somefile.iso;^M
>> > Last-Modified: Fri, 20 Jul 2018 18:23:10 GMT^M
>> > ETag: "4a54c59-11653800-571726350bf80"^M
>> > Accept-Ranges: bytes^M
>> > Content-Type: application/unknown^M
>> > X-Cache: MISS from jb7mgd^M
>> > X-Cache-Lookup: MISS from jb7mgd:3128^M
>> > Via: 1.1 jb7mgd (squid/4.2)^M
>> > Connection: keep-alive^M
>> > Content-Range: bytes 242819145-291846143/291846144^M
>> > Content-Length: 49026999^M
>> >
>> > Thanks,
>> > Hari
>> >
>> > On Sat, Sep 8, 2018 at 9:48 AM Hariharan Sethuraman wrote:
>> >
>> >     Hi Amos,
>> >
>> >     This is what I see when the download is in progress:
>> >
>> >     KEY 44000000000000000902000000000000
>> >             STORE_PENDING NOT_IN_MEMORY SWAPOUT_NONE PING_DONE
>> >             RELEASE_REQUEST,DISPATCHED,PRIVATE,VALIDATED
>>
>> So file stored in memory and scheduled for removal.
>>
>> >             LV:1536379799 LU:1536379801 LM:1532110990 EX:-1
>> >             4 locks, 1 clients, 1 refs
>> >             Swap Dir -1, File 0XFFFFFFFF
>> >             GET
>> >
>> https://example.com/DhAskLOUpvRG2oeR_f_FxYTyLVHIN5esRF-LXOUKwkwyT0TOf6xO-AUm3KaM
>> >             inmem_lo: 99225582
>> >             inmem_hi: 99324372
>> >             swapout: 0 bytes queued
>> >
>>
>> Amos
>>
>>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20180910/9219a282/attachment.htm>

From squid3 at treenet.co.nz  Mon Sep 10 14:16:03 2018
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Tue, 11 Sep 2018 02:16:03 +1200
Subject: [squid-users] Squid 4.2 : caching is not working
In-Reply-To: <CANfnjgJJPh123FugntBSrfV6t=Hk4Lj3eRQ8LUdW=+A=OCV-Lg@mail.gmail.com>
References: <CANfnjgLuEfJdCj-xvi33om7ESRnFxS4zANpckV-t4UUSusv5Sw@mail.gmail.com>
 <ce991c90-bb50-a3ca-9113-038a5e8a133a@treenet.co.nz>
 <CANfnjgJufRBou6cyA51=CZr8c7xdOFvp-+Gqga9Y-cPh-SsVSA@mail.gmail.com>
 <CANfnjgJ7RY6SSFHHXxwnx8EsFbucqAxSOA=MCN1QoDhxObickQ@mail.gmail.com>
 <CANfnjgKJTy3U_DBg=gmu0_1DmDhS4DZOHYjxqbepzO5y0JtSAA@mail.gmail.com>
 <22c4ec2c-dd11-7391-65ea-a5a9ca38e794@treenet.co.nz>
 <CANfnjgL2vmVJXT5LRb=UUv0u_zx0NuNG_5ED6kV+eOEjLMCL2Q@mail.gmail.com>
 <CANfnjgJJPh123FugntBSrfV6t=Hk4Lj3eRQ8LUdW=+A=OCV-Lg@mail.gmail.com>
Message-ID: <6450cba7-3e41-4fa4-f12b-83da50d71ace@treenet.co.nz>

On 11/09/18 12:18 AM, Hariharan Sethuraman wrote:
> Hi All,
> 
> I have two things to clarify:
> 1) In earlier email (snipped below), Amos told that is caching and
> scheduled to download

Thats not what I wrote. There is data and it is scheduled for removal
(erase) as soon as the current client gets responded to. It is
specifically *not* caching.

That confirms why you are not seeing anything in the disk cache.

> - does it mean that we got the answer and do some
> override?


> --------------------
> ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------?
>>? ? ?This is what I see when the download is in progress:
>>?
>>? ? ?KEY 44000000000000000902000000000000
>>? ? ?? ? ? ? STORE_PENDING NOT_IN_MEMORY SWAPOUT_NONE PING_DONE
>>? ? ?? ? ? ? RELEASE_REQUEST,DISPATCHED,PRIVATE,VALIDATED
> 
> So file stored in memory and scheduled for removal.?
> --------------------
> ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
> ?
> 2) With more debug_options enabled, I see that it is not caching because
> the response is part of authenticated flow. Is there a way I can
> override this?

No. The server is supplying sufficient headers for caching to make it
appear that the site authors intentionally are sending what does get
delivered.


> -? ?Initially when this file download starts, it gets authorized (I
> think that is why I see HEAD request is sent to target which has
> Cache-Control: max-age=0) and then the subsequent GET requests (dont
> have any cache-control header) download the chunk using adjusted range
> and offset.

Client delivered Cache-Control do not matter. It is the *server*
Cache-Control which matters here.

The client can also *not* send the authentication header. That would let
Squid cache the object IF the server sent this same object without
credentials being needed.


> -? ?But the subsequent GET reply seems set with auth flag, see the code
> below snipped from source
> (https://github.com/squid-cache/squid/blob/4f1c93a7a0d14eec223e199275ce570d840f71bc/src/http.cc).?
> ? ? ? ? // RFC 2068, sec 14.9.4 - MUST NOT cache any response with
> Authentication UNLESS certain CC controls are present
> ? ? // allow HTTP violations to IGNORE those controls (ie re-block
> caching Auth)
> ? ? if (request && (request->flags.auth || request->flags.authSent)) {
> ? ? ? ? if (!rep->cache_control)
> ? ? ? ? ? ? return decision.make(ReuseDecision::reuseNot,
> ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?"authenticated and server reply missing
> Cache-Control");
> -? ?I tried adding override-expire in the cgi-bin refresh pattern, but
> that will override only for max-age in Cache-control but not relevant
> for auth flag.

Indeed. It also will only do anything for certain outdated dynamic
content URLs.

As far as Squid is able to tell the content was generated specifically
for this authenticated user. You need the server to send Cache-Control
with one of public, must-revalidate, or s-maxage which indicate that it
is actually cacheable by a shared cache (ie Squid).

Otherwise the object is "private", and the cache related settings are
intended for a client-specific cache, such as a Browser has.



Amos


From srnhari at gmail.com  Mon Sep 10 14:27:39 2018
From: srnhari at gmail.com (Hariharan Sethuraman)
Date: Mon, 10 Sep 2018 19:57:39 +0530
Subject: [squid-users] Squid 4.2 : caching is not working
In-Reply-To: <6450cba7-3e41-4fa4-f12b-83da50d71ace@treenet.co.nz>
References: <CANfnjgLuEfJdCj-xvi33om7ESRnFxS4zANpckV-t4UUSusv5Sw@mail.gmail.com>
 <ce991c90-bb50-a3ca-9113-038a5e8a133a@treenet.co.nz>
 <CANfnjgJufRBou6cyA51=CZr8c7xdOFvp-+Gqga9Y-cPh-SsVSA@mail.gmail.com>
 <CANfnjgJ7RY6SSFHHXxwnx8EsFbucqAxSOA=MCN1QoDhxObickQ@mail.gmail.com>
 <CANfnjgKJTy3U_DBg=gmu0_1DmDhS4DZOHYjxqbepzO5y0JtSAA@mail.gmail.com>
 <22c4ec2c-dd11-7391-65ea-a5a9ca38e794@treenet.co.nz>
 <CANfnjgL2vmVJXT5LRb=UUv0u_zx0NuNG_5ED6kV+eOEjLMCL2Q@mail.gmail.com>
 <CANfnjgJJPh123FugntBSrfV6t=Hk4Lj3eRQ8LUdW=+A=OCV-Lg@mail.gmail.com>
 <6450cba7-3e41-4fa4-f12b-83da50d71ace@treenet.co.nz>
Message-ID: <CANfnjgL0HumPCD31S6zrgtnCyEewCRyNjS_CJ8PijxHFn1RQAg@mail.gmail.com>

On Mon, 10 Sep 2018, 19:46 Amos Jeffries, <squid3 at treenet.co.nz> wrote:

> On 11/09/18 12:18 AM, Hariharan Sethuraman wrote:
> > Hi All,
> >
> > I have two things to clarify:
> > 1) In earlier email (snipped below), Amos told that is caching and
> > scheduled to download
>
> Thats not what I wrote. There is data and it is scheduled for removal
> (erase) as soon as the current client gets responded to. It is
> specifically *not* caching.
>
> That confirms why you are not seeing anything in the disk cache.
>
> > - does it mean that we got the answer and do some
> > override?
>
>
> > --------------------
> >
> ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
> >>     This is what I see when the download is in progress:
> >>
> >>     KEY 44000000000000000902000000000000
> >>             STORE_PENDING NOT_IN_MEMORY SWAPOUT_NONE PING_DONE
> >>             RELEASE_REQUEST,DISPATCHED,PRIVATE,VALIDATED
> >
> > So file stored in memory and scheduled for removal.
> > --------------------
> >
> ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
> >
> > 2) With more debug_options enabled, I see that it is not caching because
> > the response is part of authenticated flow. Is there a way I can
> > override this?
>
> No. The server is supplying sufficient headers for caching to make it
> appear that the site authors intentionally are sending what does get
> delivered.
>
If I understand correctly, you are saying the caching will not be done on
squid as the content is authorised by the specific client. We can't do
anything until I ask site owners to change cache control as public?

>
>
> > -   Initially when this file download starts, it gets authorized (I
> > think that is why I see HEAD request is sent to target which has
> > Cache-Control: max-age=0) and then the subsequent GET requests (dont
> > have any cache-control header) download the chunk using adjusted range
> > and offset.
>
> Client delivered Cache-Control do not matter. It is the *server*
> Cache-Control which matters here.
>
> The client can also *not* send the authentication header. That would let
> Squid cache the object IF the server sent this same object without
> credentials being needed.
>
>
> > -   But the subsequent GET reply seems set with auth flag, see the code
> > below snipped from source
> > (
> https://github.com/squid-cache/squid/blob/4f1c93a7a0d14eec223e199275ce570d840f71bc/src/http.cc
> ).
> >         // RFC 2068, sec 14.9.4 - MUST NOT cache any response with
> > Authentication UNLESS certain CC controls are present
> >     // allow HTTP violations to IGNORE those controls (ie re-block
> > caching Auth)
> >     if (request && (request->flags.auth || request->flags.authSent)) {
> >         if (!rep->cache_control)
> >             return decision.make(ReuseDecision::reuseNot,
> >                                  "authenticated and server reply missing
> > Cache-Control");
> > -   I tried adding override-expire in the cgi-bin refresh pattern, but
> > that will override only for max-age in Cache-control but not relevant
> > for auth flag.
>
> Indeed. It also will only do anything for certain outdated dynamic
> content URLs.
>
> As far as Squid is able to tell the content was generated specifically
> for this authenticated user. You need the server to send Cache-Control
> with one of public, must-revalidate, or s-maxage which indicate that it
> is actually cacheable by a shared cache (ie Squid).
>
> Otherwise the object is "private", and the cache related settings are
> intended for a client-specific cache, such as a Browser has.
>
>
>
> Amos
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20180910/9caa3430/attachment.htm>

From srnhari at gmail.com  Mon Sep 10 14:37:19 2018
From: srnhari at gmail.com (Hariharan Sethuraman)
Date: Mon, 10 Sep 2018 20:07:19 +0530
Subject: [squid-users] Squid 4.2 : caching is not working
In-Reply-To: <CANfnjgL0HumPCD31S6zrgtnCyEewCRyNjS_CJ8PijxHFn1RQAg@mail.gmail.com>
References: <CANfnjgLuEfJdCj-xvi33om7ESRnFxS4zANpckV-t4UUSusv5Sw@mail.gmail.com>
 <ce991c90-bb50-a3ca-9113-038a5e8a133a@treenet.co.nz>
 <CANfnjgJufRBou6cyA51=CZr8c7xdOFvp-+Gqga9Y-cPh-SsVSA@mail.gmail.com>
 <CANfnjgJ7RY6SSFHHXxwnx8EsFbucqAxSOA=MCN1QoDhxObickQ@mail.gmail.com>
 <CANfnjgKJTy3U_DBg=gmu0_1DmDhS4DZOHYjxqbepzO5y0JtSAA@mail.gmail.com>
 <22c4ec2c-dd11-7391-65ea-a5a9ca38e794@treenet.co.nz>
 <CANfnjgL2vmVJXT5LRb=UUv0u_zx0NuNG_5ED6kV+eOEjLMCL2Q@mail.gmail.com>
 <CANfnjgJJPh123FugntBSrfV6t=Hk4Lj3eRQ8LUdW=+A=OCV-Lg@mail.gmail.com>
 <6450cba7-3e41-4fa4-f12b-83da50d71ace@treenet.co.nz>
 <CANfnjgL0HumPCD31S6zrgtnCyEewCRyNjS_CJ8PijxHFn1RQAg@mail.gmail.com>
Message-ID: <CANfnjgKSfPUs_h2ct_k7LTqhcTUmxc5MnFO5htQL-UnP0=ZKHA@mail.gmail.com>

Also can I achieve using reply_header_replace directive? I know it is
violation, just to understand the available options.

On Mon, 10 Sep 2018, 19:57 Hariharan Sethuraman, <srnhari at gmail.com> wrote:

>
>
> On Mon, 10 Sep 2018, 19:46 Amos Jeffries, <squid3 at treenet.co.nz> wrote:
>
>> On 11/09/18 12:18 AM, Hariharan Sethuraman wrote:
>> > Hi All,
>> >
>> > I have two things to clarify:
>> > 1) In earlier email (snipped below), Amos told that is caching and
>> > scheduled to download
>>
>> Thats not what I wrote. There is data and it is scheduled for removal
>> (erase) as soon as the current client gets responded to. It is
>> specifically *not* caching.
>>
>> That confirms why you are not seeing anything in the disk cache.
>>
>> > - does it mean that we got the answer and do some
>> > override?
>>
>>
>> > --------------------
>> >
>> ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
>> >>     This is what I see when the download is in progress:
>> >>
>> >>     KEY 44000000000000000902000000000000
>> >>             STORE_PENDING NOT_IN_MEMORY SWAPOUT_NONE PING_DONE
>> >>             RELEASE_REQUEST,DISPATCHED,PRIVATE,VALIDATED
>> >
>> > So file stored in memory and scheduled for removal.
>> > --------------------
>> >
>> ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
>> >
>> > 2) With more debug_options enabled, I see that it is not caching because
>> > the response is part of authenticated flow. Is there a way I can
>> > override this?
>>
>> No. The server is supplying sufficient headers for caching to make it
>> appear that the site authors intentionally are sending what does get
>> delivered.
>>
> If I understand correctly, you are saying the caching will not be done on
> squid as the content is authorised by the specific client. We can't do
> anything until I ask site owners to change cache control as public?
>
>>
>>
>> > -   Initially when this file download starts, it gets authorized (I
>> > think that is why I see HEAD request is sent to target which has
>> > Cache-Control: max-age=0) and then the subsequent GET requests (dont
>> > have any cache-control header) download the chunk using adjusted range
>> > and offset.
>>
>> Client delivered Cache-Control do not matter. It is the *server*
>> Cache-Control which matters here.
>>
>> The client can also *not* send the authentication header. That would let
>> Squid cache the object IF the server sent this same object without
>> credentials being needed.
>>
>>
>> > -   But the subsequent GET reply seems set with auth flag, see the code
>> > below snipped from source
>> > (
>> https://github.com/squid-cache/squid/blob/4f1c93a7a0d14eec223e199275ce570d840f71bc/src/http.cc
>> ).
>> >         // RFC 2068, sec 14.9.4 - MUST NOT cache any response with
>> > Authentication UNLESS certain CC controls are present
>> >     // allow HTTP violations to IGNORE those controls (ie re-block
>> > caching Auth)
>> >     if (request && (request->flags.auth || request->flags.authSent)) {
>> >         if (!rep->cache_control)
>> >             return decision.make(ReuseDecision::reuseNot,
>> >                                  "authenticated and server reply missing
>> > Cache-Control");
>> > -   I tried adding override-expire in the cgi-bin refresh pattern, but
>> > that will override only for max-age in Cache-control but not relevant
>> > for auth flag.
>>
>> Indeed. It also will only do anything for certain outdated dynamic
>> content URLs.
>>
>> As far as Squid is able to tell the content was generated specifically
>> for this authenticated user. You need the server to send Cache-Control
>> with one of public, must-revalidate, or s-maxage which indicate that it
>> is actually cacheable by a shared cache (ie Squid).
>>
>> Otherwise the object is "private", and the cache related settings are
>> intended for a client-specific cache, such as a Browser has.
>>
>>
>>
>> Amos
>>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20180910/a6887610/attachment.htm>

From squid3 at treenet.co.nz  Mon Sep 10 14:45:52 2018
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Tue, 11 Sep 2018 02:45:52 +1200
Subject: [squid-users] Squid 4.2 : caching is not working
In-Reply-To: <CANfnjgKSfPUs_h2ct_k7LTqhcTUmxc5MnFO5htQL-UnP0=ZKHA@mail.gmail.com>
References: <CANfnjgLuEfJdCj-xvi33om7ESRnFxS4zANpckV-t4UUSusv5Sw@mail.gmail.com>
 <ce991c90-bb50-a3ca-9113-038a5e8a133a@treenet.co.nz>
 <CANfnjgJufRBou6cyA51=CZr8c7xdOFvp-+Gqga9Y-cPh-SsVSA@mail.gmail.com>
 <CANfnjgJ7RY6SSFHHXxwnx8EsFbucqAxSOA=MCN1QoDhxObickQ@mail.gmail.com>
 <CANfnjgKJTy3U_DBg=gmu0_1DmDhS4DZOHYjxqbepzO5y0JtSAA@mail.gmail.com>
 <22c4ec2c-dd11-7391-65ea-a5a9ca38e794@treenet.co.nz>
 <CANfnjgL2vmVJXT5LRb=UUv0u_zx0NuNG_5ED6kV+eOEjLMCL2Q@mail.gmail.com>
 <CANfnjgJJPh123FugntBSrfV6t=Hk4Lj3eRQ8LUdW=+A=OCV-Lg@mail.gmail.com>
 <6450cba7-3e41-4fa4-f12b-83da50d71ace@treenet.co.nz>
 <CANfnjgL0HumPCD31S6zrgtnCyEewCRyNjS_CJ8PijxHFn1RQAg@mail.gmail.com>
 <CANfnjgKSfPUs_h2ct_k7LTqhcTUmxc5MnFO5htQL-UnP0=ZKHA@mail.gmail.com>
Message-ID: <116d513f-127d-f5ed-4e37-ff458576c456@treenet.co.nz>

On 11/09/18 2:37 AM, Hariharan Sethuraman wrote:
> Also can I achieve using reply_header_replace directive? I know it is
> violation, just to understand the available options.
> 

No, the header replacement only alters the messages leaving Squid.

Amos


From squid3 at treenet.co.nz  Mon Sep 10 14:50:59 2018
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Tue, 11 Sep 2018 02:50:59 +1200
Subject: [squid-users] Squid 4.2 : caching is not working
In-Reply-To: <CANfnjgL0HumPCD31S6zrgtnCyEewCRyNjS_CJ8PijxHFn1RQAg@mail.gmail.com>
References: <CANfnjgLuEfJdCj-xvi33om7ESRnFxS4zANpckV-t4UUSusv5Sw@mail.gmail.com>
 <ce991c90-bb50-a3ca-9113-038a5e8a133a@treenet.co.nz>
 <CANfnjgJufRBou6cyA51=CZr8c7xdOFvp-+Gqga9Y-cPh-SsVSA@mail.gmail.com>
 <CANfnjgJ7RY6SSFHHXxwnx8EsFbucqAxSOA=MCN1QoDhxObickQ@mail.gmail.com>
 <CANfnjgKJTy3U_DBg=gmu0_1DmDhS4DZOHYjxqbepzO5y0JtSAA@mail.gmail.com>
 <22c4ec2c-dd11-7391-65ea-a5a9ca38e794@treenet.co.nz>
 <CANfnjgL2vmVJXT5LRb=UUv0u_zx0NuNG_5ED6kV+eOEjLMCL2Q@mail.gmail.com>
 <CANfnjgJJPh123FugntBSrfV6t=Hk4Lj3eRQ8LUdW=+A=OCV-Lg@mail.gmail.com>
 <6450cba7-3e41-4fa4-f12b-83da50d71ace@treenet.co.nz>
 <CANfnjgL0HumPCD31S6zrgtnCyEewCRyNjS_CJ8PijxHFn1RQAg@mail.gmail.com>
Message-ID: <e6eaded8-22e4-34c0-e42d-ad30d67e463c@treenet.co.nz>

On 11/09/18 2:27 AM, Hariharan Sethuraman wrote:
> 
> On Mon, 10 Sep 2018, 19:46 Amos Jeffries wrote:
> 
>     On 11/09/18 12:18 AM, Hariharan Sethuraman wrote:
>     > ?
>     > 2) With more debug_options enabled, I see that it is not caching
>     because
>     > the response is part of authenticated flow. Is there a way I can
>     > override this?
> 
>     No. The server is supplying sufficient headers for caching to make it
>     appear that the site authors intentionally are sending what does get
>     delivered.
> 
> If I understand correctly, you are saying the caching will not be done
> on squid as the content is authorised by the specific client. We can't

Authenticated, not authorized. This is one place where the difference
matters.

> do anything until I ask site owners to change cache control as public?
> 

Pretty much, yes. I know Chrome at least used to deliver binaries whose
installer contained details of the Google account of the user fetching
it. So its not very safe to assume even downloaders are safely transferable.

Amos


From srnhari at gmail.com  Mon Sep 10 15:03:50 2018
From: srnhari at gmail.com (Hariharan Sethuraman)
Date: Mon, 10 Sep 2018 20:33:50 +0530
Subject: [squid-users] Squid 4.2 : caching is not working
In-Reply-To: <e6eaded8-22e4-34c0-e42d-ad30d67e463c@treenet.co.nz>
References: <CANfnjgLuEfJdCj-xvi33om7ESRnFxS4zANpckV-t4UUSusv5Sw@mail.gmail.com>
 <ce991c90-bb50-a3ca-9113-038a5e8a133a@treenet.co.nz>
 <CANfnjgJufRBou6cyA51=CZr8c7xdOFvp-+Gqga9Y-cPh-SsVSA@mail.gmail.com>
 <CANfnjgJ7RY6SSFHHXxwnx8EsFbucqAxSOA=MCN1QoDhxObickQ@mail.gmail.com>
 <CANfnjgKJTy3U_DBg=gmu0_1DmDhS4DZOHYjxqbepzO5y0JtSAA@mail.gmail.com>
 <22c4ec2c-dd11-7391-65ea-a5a9ca38e794@treenet.co.nz>
 <CANfnjgL2vmVJXT5LRb=UUv0u_zx0NuNG_5ED6kV+eOEjLMCL2Q@mail.gmail.com>
 <CANfnjgJJPh123FugntBSrfV6t=Hk4Lj3eRQ8LUdW=+A=OCV-Lg@mail.gmail.com>
 <6450cba7-3e41-4fa4-f12b-83da50d71ace@treenet.co.nz>
 <CANfnjgL0HumPCD31S6zrgtnCyEewCRyNjS_CJ8PijxHFn1RQAg@mail.gmail.com>
 <e6eaded8-22e4-34c0-e42d-ad30d67e463c@treenet.co.nz>
Message-ID: <CANfnjgJyQXYphTfz1bUFDp0_eB1zd0PMeRpLwkeYV2RAwo3myQ@mail.gmail.com>

Thanks a lot Amos.

1) ok, the client does a GET /<resource> with authorization header. So I
cant cache unless I ask the site-owner to send the cache-control to
whatever it can enable the intermediate cache-server to persist it.
2) Does squid-cache allow a way where I can upload the file into cache?

On Mon, Sep 10, 2018 at 8:21 PM Amos Jeffries <squid3 at treenet.co.nz> wrote:

> On 11/09/18 2:27 AM, Hariharan Sethuraman wrote:
> >
> > On Mon, 10 Sep 2018, 19:46 Amos Jeffries wrote:
> >
> >     On 11/09/18 12:18 AM, Hariharan Sethuraman wrote:
> >     >
> >     > 2) With more debug_options enabled, I see that it is not caching
> >     because
> >     > the response is part of authenticated flow. Is there a way I can
> >     > override this?
> >
> >     No. The server is supplying sufficient headers for caching to make it
> >     appear that the site authors intentionally are sending what does get
> >     delivered.
> >
> > If I understand correctly, you are saying the caching will not be done
> > on squid as the content is authorised by the specific client. We can't
>
> Authenticated, not authorized. This is one place where the difference
> matters.
>
> > do anything until I ask site owners to change cache control as public?
> >
>
> Pretty much, yes. I know Chrome at least used to deliver binaries whose
> installer contained details of the Google account of the user fetching
> it. So its not very safe to assume even downloaders are safely
> transferable.
>
> Amos
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20180910/7e574961/attachment.htm>

From squid3 at treenet.co.nz  Mon Sep 10 15:26:11 2018
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Tue, 11 Sep 2018 03:26:11 +1200
Subject: [squid-users] Squid 4.2 : caching is not working
In-Reply-To: <CANfnjgJyQXYphTfz1bUFDp0_eB1zd0PMeRpLwkeYV2RAwo3myQ@mail.gmail.com>
References: <CANfnjgLuEfJdCj-xvi33om7ESRnFxS4zANpckV-t4UUSusv5Sw@mail.gmail.com>
 <ce991c90-bb50-a3ca-9113-038a5e8a133a@treenet.co.nz>
 <CANfnjgJufRBou6cyA51=CZr8c7xdOFvp-+Gqga9Y-cPh-SsVSA@mail.gmail.com>
 <CANfnjgJ7RY6SSFHHXxwnx8EsFbucqAxSOA=MCN1QoDhxObickQ@mail.gmail.com>
 <CANfnjgKJTy3U_DBg=gmu0_1DmDhS4DZOHYjxqbepzO5y0JtSAA@mail.gmail.com>
 <22c4ec2c-dd11-7391-65ea-a5a9ca38e794@treenet.co.nz>
 <CANfnjgL2vmVJXT5LRb=UUv0u_zx0NuNG_5ED6kV+eOEjLMCL2Q@mail.gmail.com>
 <CANfnjgJJPh123FugntBSrfV6t=Hk4Lj3eRQ8LUdW=+A=OCV-Lg@mail.gmail.com>
 <6450cba7-3e41-4fa4-f12b-83da50d71ace@treenet.co.nz>
 <CANfnjgL0HumPCD31S6zrgtnCyEewCRyNjS_CJ8PijxHFn1RQAg@mail.gmail.com>
 <e6eaded8-22e4-34c0-e42d-ad30d67e463c@treenet.co.nz>
 <CANfnjgJyQXYphTfz1bUFDp0_eB1zd0PMeRpLwkeYV2RAwo3myQ@mail.gmail.com>
Message-ID: <cbc3342f-ce34-d92f-9127-c028f890fcde@treenet.co.nz>

On 11/09/18 3:03 AM, Hariharan Sethuraman wrote:
> Thanks a lot Amos.
> 
> 1) ok, the client does a GET /<resource> with authorization header. So I
> cant cache unless I ask the site-owner to send the cache-control to
> whatever it can enable the intermediate cache-server to persist it.
> 2) Does squid-cache allow a way where I can upload the file into cache?


If you can fetch it without sending credentials, that response should be
cacheable.

Amos


From srnhari at gmail.com  Mon Sep 10 15:32:14 2018
From: srnhari at gmail.com (Hariharan Sethuraman)
Date: Mon, 10 Sep 2018 21:02:14 +0530
Subject: [squid-users] Squid 4.2 : caching is not working
In-Reply-To: <cbc3342f-ce34-d92f-9127-c028f890fcde@treenet.co.nz>
References: <CANfnjgLuEfJdCj-xvi33om7ESRnFxS4zANpckV-t4UUSusv5Sw@mail.gmail.com>
 <ce991c90-bb50-a3ca-9113-038a5e8a133a@treenet.co.nz>
 <CANfnjgJufRBou6cyA51=CZr8c7xdOFvp-+Gqga9Y-cPh-SsVSA@mail.gmail.com>
 <CANfnjgJ7RY6SSFHHXxwnx8EsFbucqAxSOA=MCN1QoDhxObickQ@mail.gmail.com>
 <CANfnjgKJTy3U_DBg=gmu0_1DmDhS4DZOHYjxqbepzO5y0JtSAA@mail.gmail.com>
 <22c4ec2c-dd11-7391-65ea-a5a9ca38e794@treenet.co.nz>
 <CANfnjgL2vmVJXT5LRb=UUv0u_zx0NuNG_5ED6kV+eOEjLMCL2Q@mail.gmail.com>
 <CANfnjgJJPh123FugntBSrfV6t=Hk4Lj3eRQ8LUdW=+A=OCV-Lg@mail.gmail.com>
 <6450cba7-3e41-4fa4-f12b-83da50d71ace@treenet.co.nz>
 <CANfnjgL0HumPCD31S6zrgtnCyEewCRyNjS_CJ8PijxHFn1RQAg@mail.gmail.com>
 <e6eaded8-22e4-34c0-e42d-ad30d67e463c@treenet.co.nz>
 <CANfnjgJyQXYphTfz1bUFDp0_eB1zd0PMeRpLwkeYV2RAwo3myQ@mail.gmail.com>
 <cbc3342f-ce34-d92f-9127-c028f890fcde@treenet.co.nz>
Message-ID: <CANfnjg+p1VbvbpVQ+QVxruyGwFi-8WCRJ2tK9ur2uYUmOswTtA@mail.gmail.com>

It requires Auth for download. Thanks, I will find out a way.

On Mon, 10 Sep 2018, 20:56 Amos Jeffries, <squid3 at treenet.co.nz> wrote:

> On 11/09/18 3:03 AM, Hariharan Sethuraman wrote:
> > Thanks a lot Amos.
> >
> > 1) ok, the client does a GET /<resource> with authorization header. So I
> > cant cache unless I ask the site-owner to send the cache-control to
> > whatever it can enable the intermediate cache-server to persist it.
> > 2) Does squid-cache allow a way where I can upload the file into cache?
>
>
> If you can fetch it without sending credentials, that response should be
> cacheable.
>
> Amos
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20180910/a59cdcad/attachment.htm>

From rousskov at measurement-factory.com  Mon Sep 10 15:40:57 2018
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Mon, 10 Sep 2018 09:40:57 -0600
Subject: [squid-users] Squid 4.2 : caching is not working
In-Reply-To: <CANfnjgJyQXYphTfz1bUFDp0_eB1zd0PMeRpLwkeYV2RAwo3myQ@mail.gmail.com>
References: <CANfnjgLuEfJdCj-xvi33om7ESRnFxS4zANpckV-t4UUSusv5Sw@mail.gmail.com>
 <ce991c90-bb50-a3ca-9113-038a5e8a133a@treenet.co.nz>
 <CANfnjgJufRBou6cyA51=CZr8c7xdOFvp-+Gqga9Y-cPh-SsVSA@mail.gmail.com>
 <CANfnjgJ7RY6SSFHHXxwnx8EsFbucqAxSOA=MCN1QoDhxObickQ@mail.gmail.com>
 <CANfnjgKJTy3U_DBg=gmu0_1DmDhS4DZOHYjxqbepzO5y0JtSAA@mail.gmail.com>
 <22c4ec2c-dd11-7391-65ea-a5a9ca38e794@treenet.co.nz>
 <CANfnjgL2vmVJXT5LRb=UUv0u_zx0NuNG_5ED6kV+eOEjLMCL2Q@mail.gmail.com>
 <CANfnjgJJPh123FugntBSrfV6t=Hk4Lj3eRQ8LUdW=+A=OCV-Lg@mail.gmail.com>
 <6450cba7-3e41-4fa4-f12b-83da50d71ace@treenet.co.nz>
 <CANfnjgL0HumPCD31S6zrgtnCyEewCRyNjS_CJ8PijxHFn1RQAg@mail.gmail.com>
 <e6eaded8-22e4-34c0-e42d-ad30d67e463c@treenet.co.nz>
 <CANfnjgJyQXYphTfz1bUFDp0_eB1zd0PMeRpLwkeYV2RAwo3myQ@mail.gmail.com>
Message-ID: <d8d44413-51ca-6970-c1a6-00cbccd9d456@measurement-factory.com>

On 09/10/2018 09:03 AM, Hariharan Sethuraman wrote:

> 1) ok, the client does a GET /<resource> with authorization header. So I
> cant cache unless I ask the site-owner to send the cache-control to
> whatever it can enable the intermediate cache-server to persist it.
> 2) Does squid-cache allow a way where I can upload the file into cache?

You may have one or two Squid-related options AFAICT:

1. Configure Squid to remove the authentication headers going to the
origin server (see request_header_access). If the origin server does not
actually require authentication for this specific resource, then Squid
will get a cachable response back. Assuming the server is not broken,
this approach is safe. However, this approach will _not_ work if the
server requires authentication for this resource.

2. Configure Squid to (use an adaptation service to) add Cache-Control
response headers that would allow Squid to cache the authenticated
response. Adding response headers pre-cache probably requires using an
adaptation service -- Squid itself does not have a directive that would
add response headers before the response is evaluated for cachability
(reply_header_access is a post-cache directive so it will not work
here). This approach should "work" regardless of the server behavior. As
Amos has said, this approach is _unsafe_ -- you may cache and share a
response with user-specific info in it. You should not do this unless
you are absolutely sure that the response is safe to share!

Changing the origin server behavior is the best option if it is
available to you.

Alex.


> On Mon, Sep 10, 2018 at 8:21 PM Amos Jeffries wrote:
> 
>     On 11/09/18 2:27 AM, Hariharan Sethuraman wrote:
>     >
>     > On Mon, 10 Sep 2018, 19:46 Amos Jeffries wrote:
>     >
>     >? ? ?On 11/09/18 12:18 AM, Hariharan Sethuraman wrote:
>     >? ? ?> ?
>     >? ? ?> 2) With more debug_options enabled, I see that it is not caching
>     >? ? ?because
>     >? ? ?> the response is part of authenticated flow. Is there a way I can
>     >? ? ?> override this?
>     >
>     >? ? ?No. The server is supplying sufficient headers for caching to
>     make it
>     >? ? ?appear that the site authors intentionally are sending what
>     does get
>     >? ? ?delivered.
>     >
>     > If I understand correctly, you are saying the caching will not be done
>     > on squid as the content is authorised by the specific client. We can't
> 
>     Authenticated, not authorized. This is one place where the difference
>     matters.
> 
>     > do anything until I ask site owners to change cache control as public?
>     >
> 
>     Pretty much, yes. I know Chrome at least used to deliver binaries whose
>     installer contained details of the Google account of the user fetching
>     it. So its not very safe to assume even downloaders are safely
>     transferable.
> 
>     Amos
> 
> 
> 
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
> 



From srnhari at gmail.com  Mon Sep 10 15:48:26 2018
From: srnhari at gmail.com (Hariharan Sethuraman)
Date: Mon, 10 Sep 2018 21:18:26 +0530
Subject: [squid-users] Squid 4.2 : caching is not working
In-Reply-To: <d8d44413-51ca-6970-c1a6-00cbccd9d456@measurement-factory.com>
References: <CANfnjgLuEfJdCj-xvi33om7ESRnFxS4zANpckV-t4UUSusv5Sw@mail.gmail.com>
 <ce991c90-bb50-a3ca-9113-038a5e8a133a@treenet.co.nz>
 <CANfnjgJufRBou6cyA51=CZr8c7xdOFvp-+Gqga9Y-cPh-SsVSA@mail.gmail.com>
 <CANfnjgJ7RY6SSFHHXxwnx8EsFbucqAxSOA=MCN1QoDhxObickQ@mail.gmail.com>
 <CANfnjgKJTy3U_DBg=gmu0_1DmDhS4DZOHYjxqbepzO5y0JtSAA@mail.gmail.com>
 <22c4ec2c-dd11-7391-65ea-a5a9ca38e794@treenet.co.nz>
 <CANfnjgL2vmVJXT5LRb=UUv0u_zx0NuNG_5ED6kV+eOEjLMCL2Q@mail.gmail.com>
 <CANfnjgJJPh123FugntBSrfV6t=Hk4Lj3eRQ8LUdW=+A=OCV-Lg@mail.gmail.com>
 <6450cba7-3e41-4fa4-f12b-83da50d71ace@treenet.co.nz>
 <CANfnjgL0HumPCD31S6zrgtnCyEewCRyNjS_CJ8PijxHFn1RQAg@mail.gmail.com>
 <e6eaded8-22e4-34c0-e42d-ad30d67e463c@treenet.co.nz>
 <CANfnjgJyQXYphTfz1bUFDp0_eB1zd0PMeRpLwkeYV2RAwo3myQ@mail.gmail.com>
 <d8d44413-51ca-6970-c1a6-00cbccd9d456@measurement-factory.com>
Message-ID: <CANfnjgLD6r3=WPuMrw48hwRadcPr7okVzUGxFkBL6P88BMNnbQ@mail.gmail.com>

Many thanks Alex, option 2 could work. Will check on security aspects.

Thanks again to everyone.

On Mon, 10 Sep 2018, 21:11 Alex Rousskov, <rousskov at measurement-factory.com>
wrote:

> On 09/10/2018 09:03 AM, Hariharan Sethuraman wrote:
>
> > 1) ok, the client does a GET /<resource> with authorization header. So I
> > cant cache unless I ask the site-owner to send the cache-control to
> > whatever it can enable the intermediate cache-server to persist it.
> > 2) Does squid-cache allow a way where I can upload the file into cache?
>
> You may have one or two Squid-related options AFAICT:
>
> 1. Configure Squid to remove the authentication headers going to the
> origin server (see request_header_access). If the origin server does not
> actually require authentication for this specific resource, then Squid
> will get a cachable response back. Assuming the server is not broken,
> this approach is safe. However, this approach will _not_ work if the
> server requires authentication for this resource.
>
> 2. Configure Squid to (use an adaptation service to) add Cache-Control
> response headers that would allow Squid to cache the authenticated
> response. Adding response headers pre-cache probably requires using an
> adaptation service -- Squid itself does not have a directive that would
> add response headers before the response is evaluated for cachability
> (reply_header_access is a post-cache directive so it will not work
> here). This approach should "work" regardless of the server behavior. As
> Amos has said, this approach is _unsafe_ -- you may cache and share a
> response with user-specific info in it. You should not do this unless
> you are absolutely sure that the response is safe to share!
>
> Changing the origin server behavior is the best option if it is
> available to you.
>
> Alex.
>
>
> > On Mon, Sep 10, 2018 at 8:21 PM Amos Jeffries wrote:
> >
> >     On 11/09/18 2:27 AM, Hariharan Sethuraman wrote:
> >     >
> >     > On Mon, 10 Sep 2018, 19:46 Amos Jeffries wrote:
> >     >
> >     >     On 11/09/18 12:18 AM, Hariharan Sethuraman wrote:
> >     >     >
> >     >     > 2) With more debug_options enabled, I see that it is not
> caching
> >     >     because
> >     >     > the response is part of authenticated flow. Is there a way I
> can
> >     >     > override this?
> >     >
> >     >     No. The server is supplying sufficient headers for caching to
> >     make it
> >     >     appear that the site authors intentionally are sending what
> >     does get
> >     >     delivered.
> >     >
> >     > If I understand correctly, you are saying the caching will not be
> done
> >     > on squid as the content is authorised by the specific client. We
> can't
> >
> >     Authenticated, not authorized. This is one place where the difference
> >     matters.
> >
> >     > do anything until I ask site owners to change cache control as
> public?
> >     >
> >
> >     Pretty much, yes. I know Chrome at least used to deliver binaries
> whose
> >     installer contained details of the Google account of the user
> fetching
> >     it. So its not very safe to assume even downloaders are safely
> >     transferable.
> >
> >     Amos
> >
> >
> >
> > _______________________________________________
> > squid-users mailing list
> > squid-users at lists.squid-cache.org
> > http://lists.squid-cache.org/listinfo/squid-users
> >
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20180910/4988d390/attachment.htm>

From vh1988 at yahoo.com.ar  Mon Sep 10 18:35:42 2018
From: vh1988 at yahoo.com.ar (Julian Perconti)
Date: Mon, 10 Sep 2018 15:35:42 -0300
Subject: [squid-users] About SSL peek-n-splice/bump configurations
In-Reply-To: <6b762c34-f0c4-2d9a-6616-432305af160b@treenet.co.nz>
References: <000301d43289$14ed4770$3ec7d650$@yahoo.com.ar>
 <58014777-2b9c-7630-80dd-877c2d2af9d2@measurement-factory.com>
 <001801d432a0$8d102a30$a7307e90$@yahoo.com.ar>
 <6eb5660d-8025-eedd-3ed9-47d30bf22e62@measurement-factory.com>
 <001601d4464c$d1b38910$751a9b30$@yahoo.com.ar>
 <770057a5-3808-3fdc-5bf7-f78d09705913@treenet.co.nz>
 <002001d446cc$148bb580$3da32080$@yahoo.com.ar>
 <c7a4d318-c335-03de-9e5c-73fc258e38c0@treenet.co.nz>
 <008001d4479b$b22b9290$1682b7b0$@yahoo.com.ar>
 <04c7931e-cb29-80d1-2a2a-8a1ccb4761f5@treenet.co.nz>
 <019301d4486b$049ebf00$0ddc3d00$@yahoo.com.ar>
 <6b762c34-f0c4-2d9a-6616-432305af160b@treenet.co.nz>
Message-ID: <009d01d44935$1648ef80$42dace80$@yahoo.com.ar>

> -----Mensaje original-----
> De: squid-users <squid-users-bounces at lists.squid-cache.org> En nombre de
> Amos Jeffries
> Enviado el: lunes, 10 de septiembre de 2018 01:13
> Para: squid-users at lists.squid-cache.org
> Asunto: Re: [squid-users] About SSL peek-n-splice/bump configurations
> 
> >
> > ...So that means that squid processes the SslBump directives:
> >
> > 1: maybe more than one time in a single request...?
> >
> 
> Yes. Up to 3 times. A peek or splice action causes another check later.
> 
> 
> > 2: In a sequential order (as You or Alex said in an earlier post)
> >
> > - ... and "automagically" determine what to do if the ACL match or not?
> > With this I mean, for example.., that in a config could be first, in this order,
> a step1 directive then a step3 directive and finally a step2? With an ACL of
> course.
> 
> No, this order is fixed and follows the TLS handshake stages/steps:
>  step1, then step2, then step3. Exact same order as on the Squid wiki page.
> 
> The automagic is only applied when
>  a) no ssl_bump lines at all match (auto-decide for you), and
>  b) an action that matches is not valid for the step (auto-ignore that line).
> 
> 
> > To clarify the SslBump order is determinant but its also depends in what I
> want to do with steps and ACLs.
> >
> 
> Yes. Though what you understand by that statement still seems to differ a bit
> from what we understand it to mean.
> 
> 
> > Lets say...it is *not* mandatory to tell squid SslBump steps directives like:
> >
> > At step1 do x
> > At step 2 do y
> > At step3 do z
> >
> > And so on...
> >
> 
> Well, its true you don't *have* to . BUt also you don't have to use SSL-Bump
> at all either.
> 
> If you want to be sure what Squid is doing, and that it will continue to do that
> reliably then telling it for each step is a good idea.

Yes, but see below..what my conclusi?n is.

> 
> 
> > When I should stare?
> 
> When you, as the admin with meta knowledge about the overall policy -
> know that a bump is wanted to happen later.
> 
> 
> >
> >
> > Peeking at step2 does not prevent this?
> 
> Peeking at step2 precludes / forbids later bumping, so yes.
> 
> What I have been trying to highlight is that there is traffic that config (A)
> allows to go through *without* any peek at step2. It reaches the "ssl_bump
> bump" line.
> 
> 
> 
> >
> >
> > Quick answer: Bump.

A better term would have been "Short answer"

> >
> 
> Then put the below line after your "peek step2 noBumPSites" line:
> 
>   ssl_bump stare step2
> 
> 
> >
> > Wouldn't be less ambiguous to squid if I do this change:?
> >
> > ssl_bump peek step1 all > A question: I am not here peeking the
> noBumpSites list too? Should I add an !noBumpSites? to the end of this line?
> Just a doubt.
> > ssl_bump peek step2 nobumpSites
> > ssl_bump splice step3 nobumpSites
> > ssl_bump stare step2 nobumpSites > explicit staring whitelist (I haven't test
> this) it is just an idea...it make sense?
> 
> 
> The ACLs on the line above are the same as the peek line earlier. So the
> peek line matched already, nothing reaches this line.
>  <https://wiki.squid-cache.org/SquidFaq/SquidAcl#Common_Mistakes>
> 
> Less ambiguous, yes, if your knowledge of Squid ACLs is low. The FAQ
> link above should help a bit here.
> 
> 
> Your policy ("Quick answer: Bump.") was to prefer bump'ing. For that to
> happen as step 3 it needs a stare first at step 2.
> 
> So consider the stare here as the normal action this step2 is supposed
> to perform. With the peek line being the whitelist preventing stare+bump
> for special cases.
> 
> >  (also I dont understand what exactly the satre action do)
> 
> Hmm. Think of "peek" as a postal worker reading postcards people send in
> the mail. "stare" as the postal worker both reading and rewriting them
> to remove words (s)he doesn't like or understand.
> 
> Say if the a postcard ended with the words "never qwertyuio". A peek'ing
> postie would still deliver it unchanged, a stare'ing postie would
> deliver a postcard with the last word "never".
> 
> If the sender/receiver of the postcard had agreed to start using crypto
> every time a message ended with "qwertyuio" - the peeking postie would
> then just see a bunch of garbage/crypted postcards start to happen. The
> stare'ing one would be able to read the content, maybe even continue
> changing things.
> 
> The exact details are more complex of course, but essentially the same
> things going on.
> 
> 
> 
> > I think this config avoid the "old client-first insecure" behaviour. I am right?
> And squid check server-certitifacte before splice.
> >
> 
> Step 3 is where the "preclude" starts to matter.
> 
> 
> The Step 2 action determines whether the original clientHello or one
> rewritten by Squid gets sent to the server in order to get a serverHello
> out of it.
> 
> AIUI, "stare step2" precludes "splice step3". So that line should be
> ignored by Squid unless there was a peek done at step2.
> 
> To follow that postal analogy; client-first is like the postal worker
> simply replying to peoples postcards instead of delivering them. If/when
> they have to answer a question, writing a wholly new/different message
> to find out for itself first before answering.
> 
> 
> >
> > The thing that cause a lot of confusion to me is that in peek-n-splice
> environment, I can peek, plice,bump,starte in many steps (1,2,3).
> > That make the things a bit complex to decide. And of course to understand.
> 
> 
> Nod. One thing to be clear on is that TLS is designed to actively
> prevent MITM doing things like bump'ing. A client and server which are
> both using TLS properly cannot be bump'ed. They can only be peek'd and
> splice'd.

That is what I want to do:
Sensitive sites like banks, I dont want to bump/intercept. Instead of that, do a secure tunnel, between the client and server. May be via peeki'ng at step2?

> 
> There are many features in TLS that partially or fully work towards that
> goal. So it depends on which of those are negotiated between the
> endpoints (*if* negotiated) and also whether Squid is up to date enough
> to detect and prevent them when stare'ing.
> 
> There are also a bunch of different protocols happening in the HTTP
> environment these days (QUICK, CoAP, SPDY, HTTP/2 etc) - if one of those
> which Squid does not (yet) support is used to transfer TLS related data
> the endpoints may be transmitting info the proxy cannot affect.

Yes, sad but true today, I think.

> 
> 
> There is still an arms-race going on in TLS these days (though slowing a
> bit now). Thus the whole "only use the latest Squid release when
> SSL-Bump'ing" line we push so hard.
> 

OK, let me show You the final config, explained (called config "F"):

acl noBumpSites ssl::server_name_regex -i "/etc/squid/url.nobump"

acl step1 at_step SslBump1
acl step2 at_step SslBump2
acl step3 at_step SslBump3

   ssl_bump peek step1 all

Here I am peek'ing all, to bump/splice/stare at next step.
The doc.: 
"Step 1 is the only step that is always performed." 
" When a peek rule matches during step1, Squid proceeds to step2 where it parses the TLS Client Hello and extracts SNI (if any)."

   ssl_bump peek step2 noBumpSites

Here I am peek'ing too, but at step2 so, this makes future bump impossble. 
The doc.: "Peeking at this step usually makes bumping at step 3 impossible."

I think that splice at step3 would be reduntant. Because if peek'ing here "usually" makes bump imposible at next step so squid will decide an "automagic splice noBumpSites"

   ssl_bump stare step2 all

Here we star'ing at step2.
The doc.: "Staring at this step usually makes splicing at step 3 impossible."

Unlike the previous example, stare'ing at this step makes impossbile  future splic'ing... therefore squid will decide an "automagic bump all"

And finally:
Due to what has been said above, these two next lines I think are redundant or not necessary.

#ssl_bump splice step3 noBumpSites
#ssl_bump bump step3 all

The doc.:
" Step 3 is only performed if a peek or stare rule matched during the previous step."
" In most cases, the only meaningful choice at step 3 is whether to terminate the connection. "
The key-line for me to understand was: "The splicing or bumping decision is usually dictated by either peeking or staring at the previous step."

So, in a brief the confi is:

ssl_bump peek step1 all
ssl_bump peek step2 noBumpSites
ssl_bump stare step2 all

Comments:
In access.log I saw the bumped traffic (sites that are not white-listed by noBumpSites ACL) and TCP_TUNNEL when a spliced connection ends.
But, also see *many* lines like these when access to a non-intercepted or spliced site, say ibm.com:

At the beggining:
NONE/200 0 CONNECT 2.19.111.47:443 - ORIGINAL_DST/2.19.111.47 -  

At the end:
TCP_TUNNEL/200 197238 CONNECT 2.19.111.47:443

Final questions:

1: Is this peek-n-splice ruleset insecure? Whether if the site is bumped or spliced, both cases.
2: It is correct to say that those lines are not necessary/redundant? (#ssl_bump splice step3 noBumpSites/#ssl_bump bump step3 all)
3: Is the order of each step "correct"?


*Best regards and thank You*






From rousskov at measurement-factory.com  Mon Sep 10 19:46:33 2018
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Mon, 10 Sep 2018 13:46:33 -0600
Subject: [squid-users] About SSL peek-n-splice/bump configurations
In-Reply-To: <009d01d44935$1648ef80$42dace80$@yahoo.com.ar>
References: <000301d43289$14ed4770$3ec7d650$@yahoo.com.ar>
 <58014777-2b9c-7630-80dd-877c2d2af9d2@measurement-factory.com>
 <001801d432a0$8d102a30$a7307e90$@yahoo.com.ar>
 <6eb5660d-8025-eedd-3ed9-47d30bf22e62@measurement-factory.com>
 <001601d4464c$d1b38910$751a9b30$@yahoo.com.ar>
 <770057a5-3808-3fdc-5bf7-f78d09705913@treenet.co.nz>
 <002001d446cc$148bb580$3da32080$@yahoo.com.ar>
 <c7a4d318-c335-03de-9e5c-73fc258e38c0@treenet.co.nz>
 <008001d4479b$b22b9290$1682b7b0$@yahoo.com.ar>
 <04c7931e-cb29-80d1-2a2a-8a1ccb4761f5@treenet.co.nz>
 <019301d4486b$049ebf00$0ddc3d00$@yahoo.com.ar>
 <6b762c34-f0c4-2d9a-6616-432305af160b@treenet.co.nz>
 <009d01d44935$1648ef80$42dace80$@yahoo.com.ar>
Message-ID: <5400815d-9a82-97a9-5889-0e76c04a9ba0@measurement-factory.com>

On 09/10/2018 12:35 PM, Julian Perconti wrote:

> So, in a brief the confi is:
> 
> ssl_bump peek step1 all
> ssl_bump peek step2 noBumpSites
> ssl_bump stare step2 all

... which should be equivalent to an even simpler config:

  ssl_bump peek step1
  ssl_bump peek noBumpSites
  ssl_bump stare all

... which, for many reasonable definitions of noBumpSites (that match
during step1 if and only if they should match during step1), can be
simplified even further:

  ssl_bump peek noBumpSites
  ssl_bump stare all


However, please note that the three configs above implicitly rely on
Squid splicing (or bumping) at step3 because of the previously matching
step2 peek (or stare) action and the lack of an explicit step3 rule.
Whether Squid v4.2 actually does what it should be doing, I do not know.


> 1: Is this peek-n-splice ruleset insecure?

Define "secure".


> 2: It is correct to say that those lines are not necessary/redundant?

They should be redundant, but I do not know whether Squid v4.2
implements this aspect of the specs correctly. I know that there were
related implementation bugs in some Squid v3 releases. You can test and,
if needed, file a bug report.


> (#ssl_bump splice step3 noBumpSites/#ssl_bump bump step3 all)

Please note that the meaning of your noBumpSites ACL changes from one
step to another (because it gets more/different info). Thus, it is
incorrect to say that

  ssl_bump peek step1
  ssl_bump peek step2 noBumpSites
  ssl_bump splice step3 noBumpSites
  ...

is always exactly equivalent to

  ssl_bump peek step1
  ssl_bump peek step2 noBumpSites
  ssl_bump splice step3 all # should be optional
  ...

When using the first configuration, it is possible that, in some
specific case, noBumpSites matches during step2 but does not match
during step3, and Squid proceeds to evaluating the remaining "..." rules
in that specific case. Such sequence of events is not possible in the
second configuration because splicing at step3 is unconditional there --
it does not rely on noBumpSites matches during step3.


HTH,

Alex.


From mujtaba21n at hotmail.com  Tue Sep 11 08:43:13 2018
From: mujtaba21n at hotmail.com (Mujtaba   Hassan Madani)
Date: Tue, 11 Sep 2018 08:43:13 +0000
Subject: [squid-users] Squid Cache Server
Message-ID: <DB6P193MB0008E0848E133CEB22448158C6040@DB6P193MB0008.EURP193.PROD.OUTLOOK.COM>

Hi Squid team,

    I just want to no if squid can cache software for example windows update, Java,....etc.

regards


Mujtaba Hassan
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20180911/6096a7e5/attachment.htm>

From Antony.Stone at squid.open.source.it  Tue Sep 11 08:54:05 2018
From: Antony.Stone at squid.open.source.it (Antony Stone)
Date: Tue, 11 Sep 2018 10:54:05 +0200
Subject: [squid-users] Squid Cache Server
In-Reply-To: <DB6P193MB0008E0848E133CEB22448158C6040@DB6P193MB0008.EURP193.PROD.OUTLOOK.COM>
References: <DB6P193MB0008E0848E133CEB22448158C6040@DB6P193MB0008.EURP193.PROD.OUTLOOK.COM>
Message-ID: <201809111054.06037.Antony.Stone@squid.open.source.it>

On Tuesday 11 September 2018 at 10:43:13, Mujtaba Hassan Madani wrote:

> Hi Squid team,
> 
>     I just want to no if squid can cache software for example windows
> update, Java,....etc.

Squid doesn't care what a file is for - whether it's "software", web pages, 
images, music, video...

Squid will try to cache anything which gets requested through it, no matter 
what it is.

Whether or not any given thing *can* be cached is far more up to the content 
provider to decide - there are various HTTP headers they can use to say "don't 
cache this" or similar, and some things which you can download have different 
URLs at different times, and Squid can't tell that they are actually the same 
thing.

So, yes, Squid _can_ cache "software".  But just as with any other type of 
content, the provider may tell Squid that is isn't allowed to.


Regards,


Antony.

-- 
What makes you think I know what I'm talking about?
I just have more O'Reilly books than most people.

                                                   Please reply to the list;
                                                         please *don't* CC me.


From morteza1131 at gmail.com  Tue Sep 11 15:09:39 2018
From: morteza1131 at gmail.com (morteza1131 at gmail.com)
Date: Tue, 11 Sep 2018 19:39:39 +0430
Subject: [squid-users] [NOC] Using Nfqueue or DAQ in squid
References: <1477986172.2894469.1536591263177.ref@mail.yahoo.com>
 <1477986172.2894469.1536591263177@mail.yahoo.com>
 <7cfd4829-d2d4-eeda-a941-f938a5d57da3@treenet.co.nz>
Message-ID: <tp8e1i-lxmj9zrtwv8ewmouh3-twbmab-3r43ov-roipes3p6zfc-lfdmxg-quvcnd-tngdvhay9qbtidcyvy1v7wxmkybi1fvkgs7c-xab34j-g9ai2s-9l7yi5-dqtvu2-ae12id-gjzre9dl2elip7kkp5.1536678579495@email.android.com>

An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20180911/a194d390/attachment.htm>

From squid3 at treenet.co.nz  Tue Sep 11 16:07:44 2018
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Wed, 12 Sep 2018 04:07:44 +1200
Subject: [squid-users] [NOC] Using Nfqueue or DAQ in squid
In-Reply-To: <tp8e1i-lxmj9zrtwv8ewmouh3-twbmab-3r43ov-roipes3p6zfc-lfdmxg-quvcnd-tngdvhay9qbtidcyvy1v7wxmkybi1fvkgs7c-xab34j-g9ai2s-9l7yi5-dqtvu2-ae12id-gjzre9dl2elip7kkp5.1536678579495@email.android.com>
References: <1477986172.2894469.1536591263177.ref@mail.yahoo.com>
 <1477986172.2894469.1536591263177@mail.yahoo.com>
 <7cfd4829-d2d4-eeda-a941-f938a5d57da3@treenet.co.nz>
 <tp8e1i-lxmj9zrtwv8ewmouh3-twbmab-3r43ov-roipes3p6zfc-lfdmxg-quvcnd-tngdvhay9qbtidcyvy1v7wxmkybi1fvkgs7c-xab34j-g9ai2s-9l7yi5-dqtvu2-ae12id-gjzre9dl2elip7kkp5.1536678579495@email.android.com>
Message-ID: <d030a4b8-6614-fcc9-e8bf-4836ac0ffcb9@treenet.co.nz>

On 12/09/18 3:09 AM, morteza1131 wrote:
> i know that.
> i want to somehow change source code of squid to accept packet from
> nfqueue, to customize netfilter packet flow for my application(squid).
> 
> is it possible!?


Sure Squid can be made to accept nfqueue messages. *Any* software can be
re-coded to do anything other software does. Then what?

nfqueue messages pass IP protocol packets individually - even when a
packet contains HTTP it only contains *part* of an HTTP message. It is
extremely unlikely that Squid will be able to do anything at all with
the data provided.

Then there is the matter of responding to any HTTP messages *if* they
can be deciphered from the opaque data. nfqueue expects a single integer
- which is the index that *original* packet is to be scheduled for
delivery. So there is absolutely zero things Squid can do in its role as
an HTTP proxy.


So again, what exactly are you trying to achieve?
 <https://perlmonks.org/?node=xy+problem>


Amos


From squid3 at treenet.co.nz  Tue Sep 11 18:55:09 2018
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Wed, 12 Sep 2018 06:55:09 +1200
Subject: [squid-users] [NOC] Using Nfqueue or DAQ in squid
In-Reply-To: <-y9igrn-h2hivovy5ams-lkv7ce-c6qgkfb8kut8hjgn4yd1m47-b3fgb7zcpwg7-kugwccsw1nhslziieg-i7by49cnsg64-blgkbvx8i0fqufh5e2-aaqq27ri1nlmcauhwa-bnpj66untly67pjcog.1536684609529@email.android.com>
References: <1477986172.2894469.1536591263177.ref@mail.yahoo.com>
 <1477986172.2894469.1536591263177@mail.yahoo.com>
 <7cfd4829-d2d4-eeda-a941-f938a5d57da3@treenet.co.nz>
 <tp8e1i-lxmj9zrtwv8ewmouh3-twbmab-3r43ov-roipes3p6zfc-lfdmxg-quvcnd-tngdvhay9qbtidcyvy1v7wxmkybi1fvkgs7c-xab34j-g9ai2s-9l7yi5-dqtvu2-ae12id-gjzre9dl2elip7kkp5.1536678579495@email.android.com>
 <d030a4b8-6614-fcc9-e8bf-4836ac0ffcb9@treenet.co.nz>
 <-y9igrn-h2hivovy5ams-lkv7ce-c6qgkfb8kut8hjgn4yd1m47-b3fgb7zcpwg7-kugwccsw1nhslziieg-i7by49cnsg64-blgkbvx8i0fqufh5e2-aaqq27ri1nlmcauhwa-bnpj66untly67pjcog.1536684609529@email.android.com>
Message-ID: <26802375-8374-9304-82e0-3e76b1585055@treenet.co.nz>

On 12/09/18 4:50 AM, morteza1131 wrote:
> i explaned what i want before in my first mail, but to be clear :
> in my linux iptables firewall i want to do iptables rules and controles
> in foward chain and after that do http filtering with squid, because of
> that i need to change netfilter packet flow and send packets to
> squid(app layer, user space) after forward chain, and then get them back
> to kernel space to continue their's way in forward chain and then go out.
> something like this:
> mangle:prerouting > nat:prerouting>filter:forward > sauid >
> mangle:postrouting >nat:postrouting
> 
> i thought that nfqueue can help me, maybe there are other ways that i
> don't know!!
> 
> what do you think!?
> 


I think you are very much misunderstanding how netfilter/iptables is
designed.

Basically INPUT, FORWARD, OUTPUT - every packet goes through one of
them, and no packet ever goes through two.

Which chain applies is determined by where the packet is coming from,
and where it is going to - at the hardware / link layer. Though
PREROUTING rules can affect that decision.

Packets going through FORWARD are going pretty much directly from input
NIC to output NIC.


Depending on what your rules are intended to do they *should* be spread
across those tables. Your desire to put everything only in FORWARD is
leaving the INPUT and OUTPUT packets completely free.


If you want to continue to only filter packets in FORWARD instead of
packets actually entering and leaving the machine. Then you will have to
redesign netfilter itself and possibly the hardware circuitry it uses
for FORWARD handling.

As you wrote above: "i need to change netfilter packet flow".

Squid has nothing to do with any of that level of packet handling. Once
a packet reaches any application layer software like Squid it ceases to
exist. Squid doesn't even get the packet header, just the payload -
streamed in with all the other packet payloads for that TCP connection.
So there is no re-processing of any packet, its gone completely.


Amos


From squid3 at treenet.co.nz  Wed Sep 12 07:44:58 2018
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Wed, 12 Sep 2018 19:44:58 +1200
Subject: [squid-users] [NOC] Using Nfqueue or DAQ in squid
In-Reply-To: <-frw3zxcmwdjh2nota7wanf82-vwfrf2-28vpwz4m9ypcefvxtn-5wr69j-xyzx9w-echphib9pfw5-wb580agx93jxg00do1-8pf4jno7mu6j8oxlwondcqa2-sbblqixs7eva-59xkyefgdtjjz37bu.1536693474866@email.android.com>
References: <1477986172.2894469.1536591263177.ref@mail.yahoo.com>
 <1477986172.2894469.1536591263177@mail.yahoo.com>
 <7cfd4829-d2d4-eeda-a941-f938a5d57da3@treenet.co.nz>
 <tp8e1i-lxmj9zrtwv8ewmouh3-twbmab-3r43ov-roipes3p6zfc-lfdmxg-quvcnd-tngdvhay9qbtidcyvy1v7wxmkybi1fvkgs7c-xab34j-g9ai2s-9l7yi5-dqtvu2-ae12id-gjzre9dl2elip7kkp5.1536678579495@email.android.com>
 <d030a4b8-6614-fcc9-e8bf-4836ac0ffcb9@treenet.co.nz>
 <-y9igrn-h2hivovy5ams-lkv7ce-c6qgkfb8kut8hjgn4yd1m47-b3fgb7zcpwg7-kugwccsw1nhslziieg-i7by49cnsg64-blgkbvx8i0fqufh5e2-aaqq27ri1nlmcauhwa-bnpj66untly67pjcog.1536684609529@email.android.com>
 <26802375-8374-9304-82e0-3e76b1585055@treenet.co.nz>
 <-frw3zxcmwdjh2nota7wanf82-vwfrf2-28vpwz4m9ypcefvxtn-5wr69j-xyzx9w-echphib9pfw5-wb580agx93jxg00do1-8pf4jno7mu6j8oxlwondcqa2-sbblqixs7eva-59xkyefgdtjjz37bu.1536693474866@email.android.com>
Message-ID: <7b405d9b-374d-0322-39e3-3b35d5499522@treenet.co.nz>

On 12/09/18 7:17 AM, morteza1131 wrote:
> Tanks for your response.
> I totally understand how iptables work.

Then you should already know very well the answers to all these
questions you ask, including why Squid cannot do what you want. You
attempting to troll?


> are you familiar with snort!?

I am relatively familiar with snort - what it does and its limitations.
I was working with the Netfilter dev team to get TPROXY working when
when NFQUEUE and related features were being designed and implemented.


> with advantages of daq and nfqueue they do those things that i want to do.
> snort get packets(packets that must be forward) from kernel space and
> get them back to kernel space. it works without any changes in packet
> flow with only one nfqueue rule in iptables.

If you are totally familiar with iptables, then you know the statement
"get them back to kernel space" you used above is false. NFQUEUE only
receives a 32-bit integer verdict on whether the packet is to be
discarded or queued with the given delay (hint is in the name).


> i want to change source code of squid to does what snort does.
> but you said that is not possible, why!?

I have answered that question thrice now. Because IP protocol is not
HTTP protocol. Network layer is not Application layer.

 Snort is network layer software for handling IP protocol.

 Squid is application layer software for handling HTTP protocol.

Completely and utterly different requirements and limitations. For
example; "packet" is a completely unknown/foreign concept to Squid.

PS. please keep the users mailing list in your replies.


Amos


From mujtaba21n at hotmail.com  Wed Sep 12 14:16:12 2018
From: mujtaba21n at hotmail.com (Mujtaba   Hassan Madani)
Date: Wed, 12 Sep 2018 14:16:12 +0000
Subject: [squid-users] Squid Cache Server
In-Reply-To: <201809111054.06037.Antony.Stone@squid.open.source.it>
References: <DB6P193MB0008E0848E133CEB22448158C6040@DB6P193MB0008.EURP193.PROD.OUTLOOK.COM>,
 <201809111054.06037.Antony.Stone@squid.open.source.it>
Message-ID: <DB6P193MB00085E55BC2E60C264EE09A4C61B0@DB6P193MB0008.EURP193.PROD.OUTLOOK.COM>

Dear Squid Team,

     how does content provider prevent it from been cached while passing through squid proxy it's by a copy right law or some  encryption is implemented in the traffic ? and where can I find the contents that been cached on my squid proxy ?

thanks for your assistant

Mujtaba H,

________________________________
From: squid-users <squid-users-bounces at lists.squid-cache.org> on behalf of Antony Stone <Antony.Stone at squid.open.source.it>
Sent: Tuesday, September 11, 2018 8:54:05 AM
To: squid-users at lists.squid-cache.org
Subject: Re: [squid-users] Squid Cache Server

On Tuesday 11 September 2018 at 10:43:13, Mujtaba Hassan Madani wrote:

> Hi Squid team,
>
>     I just want to no if squid can cache software for example windows
> update, Java,....etc.

Squid doesn't care what a file is for - whether it's "software", web pages,
images, music, video...

Squid will try to cache anything which gets requested through it, no matter
what it is.

Whether or not any given thing *can* be cached is far more up to the content
provider to decide - there are various HTTP headers they can use to say "don't
cache this" or similar, and some things which you can download have different
URLs at different times, and Squid can't tell that they are actually the same
thing.

So, yes, Squid _can_ cache "software".  But just as with any other type of
content, the provider may tell Squid that is isn't allowed to.


Regards,


Antony.

--
What makes you think I know what I'm talking about?
I just have more O'Reilly books than most people.

                                                   Please reply to the list;
                                                         please *don't* CC me.
_______________________________________________
squid-users mailing list
squid-users at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-users
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20180912/c7f99a73/attachment.htm>

From vh1988 at yahoo.com.ar  Wed Sep 12 14:28:11 2018
From: vh1988 at yahoo.com.ar (Julian Perconti)
Date: Wed, 12 Sep 2018 11:28:11 -0300
Subject: [squid-users] About SSL peek-n-splice/bump configurations
In-Reply-To: <5400815d-9a82-97a9-5889-0e76c04a9ba0@measurement-factory.com>
References: <000301d43289$14ed4770$3ec7d650$@yahoo.com.ar>
 <58014777-2b9c-7630-80dd-877c2d2af9d2@measurement-factory.com>
 <001801d432a0$8d102a30$a7307e90$@yahoo.com.ar>
 <6eb5660d-8025-eedd-3ed9-47d30bf22e62@measurement-factory.com>
 <001601d4464c$d1b38910$751a9b30$@yahoo.com.ar>
 <770057a5-3808-3fdc-5bf7-f78d09705913@treenet.co.nz>
 <002001d446cc$148bb580$3da32080$@yahoo.com.ar>
 <c7a4d318-c335-03de-9e5c-73fc258e38c0@treenet.co.nz>
 <008001d4479b$b22b9290$1682b7b0$@yahoo.com.ar>
 <04c7931e-cb29-80d1-2a2a-8a1ccb4761f5@treenet.co.nz>
 <019301d4486b$049ebf00$0ddc3d00$@yahoo.com.ar>
 <6b762c34-f0c4-2d9a-6616-432305af160b@treenet.co.nz>
 <009d01d44935$1648ef80$42dace80$@yahoo.com.ar>
 <5400815d-9a82-97a9-5889-0e76c04a9ba0@measurement-factory.com>
Message-ID: <003501d44aa4$d784e5d0$868eb170$@yahoo.com.ar>

> > So, in a brief the confi is:
> >
> > ssl_bump peek step1 all
> > ssl_bump peek step2 noBumpSites
> > ssl_bump stare step2 all
> 
> ... which should be equivalent to an even simpler config:
> 
>   ssl_bump peek step1 
>   ssl_bump peek noBumpSites
>   ssl_bump stare all

Yes, i've tested and squid log shows the same. So,it appears that squid takes the same actions.

> 
> ... which, for many reasonable definitions of noBumpSites (that match during
> step1 if and only if they should match during step1), can be simplified even
> further:
> 
>   ssl_bump peek noBumpSites
>   ssl_bump stare all

Same as above, this "compact" config seems to be the same as the three above bump ruleset.

Please, let me know if I understand why those cfg are equals or equivalent to config I've posted as a "final one".

First alternative difference:

>   ssl_bump peek step1  # implicit "all" at step1
>   ssl_bump peek noBumpSites # As there no step specified, squid match at any step then this line, match at step1 and then at step2 , so when a match occurs at step2 it precludes future bumping of the sites listed in the ACL.
>   ssl_bump stare all # Here there is either no step2 (and any step) specified but, because in the previous line You has (implicitly) peeked at step2, the stare'ing not (or can?t) applies to sites listed in ACL (they were peeked at step2).

Second alternative difference:

>   ssl_bump peek noBumpSites # Like previous example, but..I guess that as there is no "all" explicit, this line do a "peek all at step1" (implicitly) and at step2 sites listed in ACL are being peek'd. To clarify, if I would add an "all" at the end of this line, then all traffic would be spliced.
>   ssl_bump stare all # There is no change between this line in both configs you've posted, So my "explanation" would be the same as of the "First alternative"

> However, please note that the three configs above implicitly rely on Squid
> splicing (or bumping) at step3 because of the previously matching
> step2 peek (or stare) action and the lack of an explicit step3 rule.
> Whether Squid v4.2 actually does what it should be doing, I do not know.

Answered; squid "automagic " are working as spected. (Squid Cache: Version 4.2-20180902-r6d8f397)

> 
> > 1: Is this peek-n-splice ruleset insecure?
> 
> Define "secure".

Well, is not the same if there is a squid-TLS (in the LAN) between a client and sensitive external server when a TLS connection is being established as if there is nothing between they.

In this sense I would like to know how could I interference as less as possible with the squid in the middle when someone is accesing to a site that I wish not to bump.
Or let the less quantity of security holes as possible.

> > 2: It is correct to say that those lines are not necessary/redundant?
> 
> They should be redundant, but I do not know whether Squid v4.2
> implements this aspect of the specs correctly. I know that there were related
> implementation bugs in some Squid v3 releases. You can test and, if needed,
> file a bug report.
> 
> 
> > (#ssl_bump splice step3 noBumpSites/#ssl_bump bump step3 all)
> 
> Please note that the meaning of your noBumpSites ACL changes from one
> step to another (because it gets more/different info). Thus, it is incorrect to
> say that
> 
>   ssl_bump peek step1
>   ssl_bump peek step2 noBumpSites
>   ssl_bump splice step3 noBumpSites
>   ...
> 
> is always exactly equivalent to
> 
>   ssl_bump peek step1
>   ssl_bump peek step2 noBumpSites
>   ssl_bump splice step3 all # should be optional
>   ...
> 
> When using the first configuration, it is possible that, in some specific case,
> noBumpSites matches during step2 but does not match during step3, and
> Squid proceeds to evaluating the remaining "..." rules in that specific case.
> Such sequence of events is not possible in the second configuration because
> splicing at step3 is unconditional there -- it does not rely on noBumpSites
> matches during step3.

OK, thanks to clarify that.

Last question.

When I do this:

ssl_bump splice noBumpSites
ssl_bump stare all

It is supposed that in this config I am (guessing), implicity, peeking  (first?) and splice at any step and bumping (implicity) at step3 sites that does not match with whitelist by staring at step2. Maybe something like that, I dont know.

The thin is that I see in the logs a tunnel but, instead an IP address it shows a domain. (TCP_TUNNEL www.dropbox.com:433) *and* a security ALERT. Saying that there is no IP that match with the xyz.net domain, or some like that.

So, taking into account the needs that I have already mentioned, what is the way I should take?

> HTH,

Always helps.

Thank You!
 
> Alex.




From squid3 at treenet.co.nz  Wed Sep 12 14:54:37 2018
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 13 Sep 2018 02:54:37 +1200
Subject: [squid-users] Squid Cache Server
In-Reply-To: <DB6P193MB00085E55BC2E60C264EE09A4C61B0@DB6P193MB0008.EURP193.PROD.OUTLOOK.COM>
References: <DB6P193MB0008E0848E133CEB22448158C6040@DB6P193MB0008.EURP193.PROD.OUTLOOK.COM>
 <201809111054.06037.Antony.Stone@squid.open.source.it>
 <DB6P193MB00085E55BC2E60C264EE09A4C61B0@DB6P193MB0008.EURP193.PROD.OUTLOOK.COM>
Message-ID: <b6e884e9-0aa3-a48a-b529-afcfe4c60f51@treenet.co.nz>

On 13/09/18 2:16 AM, Mujtaba   Hassan Madani wrote:
> Dear Squid Team,
> 
> ? ? ?how does content provider prevent it from been cached while passing
> through squid proxy it's by a copy right law

No. Contents which can be transferred through a proxy are implicitly
licensed for re-distribution.

Legal issues are usually encountered only around interception or
modification of content.


> or some ?encryption is

Sometimes.

> implemented in the traffic ?

and other features built into HTTP protocol.


> and where can I find the contents that been
> cached on my squid proxy ?
> 

Depends on your config. Usually in the machine RAM.

What are you looking for exactly? and why?


Amos


From rousskov at measurement-factory.com  Wed Sep 12 16:41:26 2018
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Wed, 12 Sep 2018 10:41:26 -0600
Subject: [squid-users] About SSL peek-n-splice/bump configurations
In-Reply-To: <003501d44aa4$d784e5d0$868eb170$@yahoo.com.ar>
References: <000301d43289$14ed4770$3ec7d650$@yahoo.com.ar>
 <58014777-2b9c-7630-80dd-877c2d2af9d2@measurement-factory.com>
 <001801d432a0$8d102a30$a7307e90$@yahoo.com.ar>
 <6eb5660d-8025-eedd-3ed9-47d30bf22e62@measurement-factory.com>
 <001601d4464c$d1b38910$751a9b30$@yahoo.com.ar>
 <770057a5-3808-3fdc-5bf7-f78d09705913@treenet.co.nz>
 <002001d446cc$148bb580$3da32080$@yahoo.com.ar>
 <c7a4d318-c335-03de-9e5c-73fc258e38c0@treenet.co.nz>
 <008001d4479b$b22b9290$1682b7b0$@yahoo.com.ar>
 <04c7931e-cb29-80d1-2a2a-8a1ccb4761f5@treenet.co.nz>
 <019301d4486b$049ebf00$0ddc3d00$@yahoo.com.ar>
 <6b762c34-f0c4-2d9a-6616-432305af160b@treenet.co.nz>
 <009d01d44935$1648ef80$42dace80$@yahoo.com.ar>
 <5400815d-9a82-97a9-5889-0e76c04a9ba0@measurement-factory.com>
 <003501d44aa4$d784e5d0$868eb170$@yahoo.com.ar>
Message-ID: <500fbad5-d277-965f-d29a-a81c2f343d2e@measurement-factory.com>

On 09/12/2018 08:28 AM, Julian Perconti wrote:


> Please, let me know if I understand why those cfg are equals

I am afraid you do not. You are probably missing the fact that, at each
step, the rules after the matching applicable rule are not checked.
Also, you seem to insert some implicit peeking rules that are never
there. Finally, there may be some confusion regarding how multiple ACLs
on one line are evaluated (and/or you do not think that stepN is just an
ACL?). Details below.


> ssl_bump peek step1 
> ssl_bump peek noBumpSites
> ssl_bump stare all


>>   ssl_bump peek step1  # implicit "all" at step1

Yes, if you wish to think about it that way. In reality, the condition
is exactly "step1", rather than "step1 and all" or "step1 and true".


>>   ssl_bump peek noBumpSites # As there no step specified, squid match at any step

Not exactly. Squid will evaluate this rule at any step that (a) reaches
this line and (b) where the peek action is applicable. The intersection
of those two preconditions is "step2" rather than "any step".


> then this line, match at step1 

No, this line will not be evaluated at step1. Only the first rule is
evaluated at step1 (because that first rule always matches at step1).


> and then at step2, so when a match occurs at step2 it precludes future bumping of the sites listed in the ACL.

Yes, but that is kind of irrelevant here because there are no bump rules
to exclude. At step3, this previous/step2 peeking should result in Squid
applying the default "splice" rule (you can view that as excluding the
default "bump" rule if you wish).


>> ssl_bump stare all # Here there is either no step2 (and any step)
>> specified but, because in the previous line You has (implicitly)
>> peeked at step2, the stare'ing not (or can?t) applies to sites
>> listed in ACL (they were peeked at step2).

Something like that. Step2 always happens in this configuration (so "no
step2" does not make sense), and there is no such thing as "implicit
peeking", but I think you more-or-less got the right idea here.


> ssl_bump peek noBumpSites
> ssl_bump stare all


>> ssl_bump peek noBumpSites # Like previous example, but..I guess
>> that as there is no "all" explicit, this line do a "peek all at
>> step1" (implicitly)

No, this line does not do "peek all". It does "peek noBumpSites". That
is, it tells Squid to peek when and only when both of the conditions
below are true:

(a) the peeking action is applicable (i.e., step1 or step2)
(b) the noBumpSites ACL matches

The two conditions above are evaluated in the specified order. Condition
(b) is not evaluated unless condition (a) is satisfied.



> To clarify, if I would add an "all" at the end of this line, then all traffic would be spliced.

Adding "all" to any line changes nothing as far as line matching is
considered. The value of "foo and true" is equivalent to the value of "foo".



>>> 1: Is this peek-n-splice ruleset insecure?
>>
>> Define "secure".

> Well, is not the same if there is a squid-TLS (in the LAN) between a
> client and sensitive external server when a TLS connection is being
> established as if there is nothing between they.

I am not sure I interpret your definition correctly, but I hope the
following statements will answer your question regardless of that
interpretation:

* Staring (at step2) or bumping (at any step) alters TLS bytes on the
wire. The client and the origin server will see some TLS bytes that are
going to differ from the TLS bytes they would have seen if Squid was not
there.

* In this scope, the deprecated client-first and server-first actions
are equivalent to applying the "bump" action.

* If successful, ssl_bump peek and splice actions do not alter TLS
bytes. Peeking and/or splicing Squid can be viewed as a TCP proxy as far
as TLS bytes forwarding is concerned. The client and the origin server
will see the same TLS bytes they would have seen if Squid was not there.

* In this scope, various errors are usually equivalent to applying the
"bump" action.


If your definition of "secure" is "does not change TLS bytes exchanges
between client and server", then if your configuration has a "stare"
and/or "bump" actions, it is "insecure". If your configuration may lead
to certificate validation errors, it is also "insecure".


> In this sense I would like to know how could I interference as less
> as possible with the squid in the middle when someone is accesing to
> a site that I wish not to bump.

Peeking and/or splicing does not change TLS bytes. It is passive
monitoring. See the bullets above for a more complete/precise picture.


> When I do this:
> 
> ssl_bump splice noBumpSites
> ssl_bump stare all

> It is supposed that in this config I am (guessing), implicity,
> peeking  (first?) and splice at any step and bumping (implicity) at
> step3 sites that does not match with whitelist by staring at step2.
> Maybe something like that, I dont know.

I do not think your description of the above configuration is correct.
Squid uses default actions ("splice" or "bump") when no applicable rules
match. In the above configuration, one of the rules will always match
during step1 and during step2 (if any). Thus, there will be no implicit
splicing or bumping during the first steps. If Squid reaches step3, then
Squid will apply the default bump rule at that step (because "stare"
matched at the previous step).

I am not sure, but I think the above configuration is equivalent to the
following configuration that does not rely on default rules:

  ssl_bump splice step1 noBumpSites
  ssl_bump splice step2 noBumpSites
  ssl_bump stare step1
  ssl_bump stare step2
  ssl_bump bump step3


HTH,

Alex.


From squid at buglecreek.com  Wed Sep 12 21:47:09 2018
From: squid at buglecreek.com (squid at buglecreek.com)
Date: Wed, 12 Sep 2018 15:47:09 -0600
Subject: [squid-users] Unable to Disable sslv3
Message-ID: <1536788829.3128050.1506146552.01004947@webmail.messagingengine.com>

I asked this some time ago and am bringing it up again to see if there are any suggestions since we haven't been able to fix it.

We are using squid as reverse proxy and we have disabled SSLv3 :

https_port XXX.XXX.XXX.XXX:443 accel defaultsite=www.example.com vhost cert=/etc/....cert.pem key=/etc/....privkey.pem options=NO_SSLv2,NO_SSLv3,SINGLE_DH_USE,CIPHER_SERVER_PREFERENCE cipher=ECDHE-ECDSA . . .. dhparams=/etc/...dhparams.pem

We have also tried the sslproxy_options as well.  

Using Nessus scanning tool, it reports that SSLv3 is enabled, but not SSLv2.   

Version of Squid is  (3.1.23) which is stock RH6 which I know is old, but for now we need to use it. 

The only thing we have been able to do so far is add NO_TLSv1 to the https_port section.  Then the scan comes back clean.   Not sure what to look at next.  Any suggestions? 


From rousskov at measurement-factory.com  Thu Sep 13 00:54:26 2018
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Wed, 12 Sep 2018 18:54:26 -0600
Subject: [squid-users] Unable to Disable sslv3
In-Reply-To: <1536788829.3128050.1506146552.01004947@webmail.messagingengine.com>
References: <1536788829.3128050.1506146552.01004947@webmail.messagingengine.com>
Message-ID: <f93d5bac-0412-e618-96de-42e480473837@measurement-factory.com>

On 09/12/2018 03:47 PM, squid at buglecreek.com wrote:

> We are using squid as reverse proxy and we have disabled SSLv3 :

> https_port XXX.XXX.XXX.XXX:443 accel defaultsite=www.example.com
> vhost cert=/etc/....cert.pem key=/etc/....privkey.pem
> options=NO_SSLv2,NO_SSLv3,SINGLE_DH_USE,CIPHER_SERVER_PREFERENCE
> cipher=ECDHE-ECDSA . . .. dhparams=/etc/...dhparams.pem

> We have also tried the sslproxy_options as well.

> Using Nessus scanning tool, it reports that SSLv3 is enabled, but not
> SSLv2.

> Version of Squid is  (3.1.23) which is stock RH6 which I know is old,
> but for now we need to use it.

> The only thing we have been able to do so far is add NO_TLSv1 to the
> https_port section.  Then the scan comes back clean.   Not sure what
> to look at next.  Any suggestions?


I can nominate three suspects:

  1. Your OpenSSL version does not support/define SSL_OP_NO_SSLv3.
  2. Your scanning tool is confused/lying. SSLv3 is actually disabled.
  3. Your Squid mishandles SSL_OP_NO_SSLv3 or your configuration.

To detect #1, you can grep source code of your OpenSSL version for the
said constant.

To detect #2, you can try establishing an SSLv3-only connection to your
Squid https_port using OpenSSL s_client. Sorry, I do not have an exact
s_client command handy.

I cannot give you specific instructions for #3 detection, especially for
such an old Squid version, but a capable developer can confirm that the
configured option is applied successfully using a debugger or debugging
patches. With access to the right setup, it should not take more than an
hour or two (more without Squid knowledge).


HTH,

Alex.


From vh1988 at yahoo.com.ar  Thu Sep 13 03:02:56 2018
From: vh1988 at yahoo.com.ar (Julian Perconti)
Date: Thu, 13 Sep 2018 00:02:56 -0300
Subject: [squid-users] About SSL peek-n-splice/bump configurations
In-Reply-To: <500fbad5-d277-965f-d29a-a81c2f343d2e@measurement-factory.com>
References: <000301d43289$14ed4770$3ec7d650$@yahoo.com.ar>
 <58014777-2b9c-7630-80dd-877c2d2af9d2@measurement-factory.com>
 <001801d432a0$8d102a30$a7307e90$@yahoo.com.ar>
 <6eb5660d-8025-eedd-3ed9-47d30bf22e62@measurement-factory.com>
 <001601d4464c$d1b38910$751a9b30$@yahoo.com.ar>
 <770057a5-3808-3fdc-5bf7-f78d09705913@treenet.co.nz>
 <002001d446cc$148bb580$3da32080$@yahoo.com.ar>
 <c7a4d318-c335-03de-9e5c-73fc258e38c0@treenet.co.nz>
 <008001d4479b$b22b9290$1682b7b0$@yahoo.com.ar>
 <04c7931e-cb29-80d1-2a2a-8a1ccb4761f5@treenet.co.nz>
 <019301d4486b$049ebf00$0ddc3d00$@yahoo.com.ar>
 <6b762c34-f0c4-2d9a-6616-432305af160b@treenet.co.nz>
 <009d01d44935$1648ef80$42dace80$@yahoo.com.ar>
 <5400815d-9a82-97a9-5889-0e76c04a9ba0@measurement-factory.com>
 <003501d44aa4$d784e5d0$868eb170$@yahoo.com.ar>
 <500fbad5-d277-965f-d29a-a81c2f343d2e@measurement-factory.com>
Message-ID: <004401d44b0e$46cf40c0$d46dc240$@yahoo.com.ar>

> I am afraid you do not. You are probably missing the fact that, at each step,
> the rules after the matching applicable rule are not checked.
> Also, you seem to insert some implicit peeking rules that are never there.
> Finally, there may be some confusion regarding how multiple ACLs on one
> line are evaluated (and/or you do not think that stepN is just an ACL?).

You're right, it's just and ACL like any other. Maybe I lost sight of that point.

> Details below.

I will keep trying to understand the best I can.

> > ssl_bump peek step1
> > ssl_bump peek noBumpSites
> > ssl_bump stare all
> 
> 
> >>   ssl_bump peek step1  # implicit "all" at step1
> 
> Yes, if you wish to think about it that way. In reality, the condition
> is exactly "step1", rather than "step1 and all" or "step1 and true".
> 
> 
> >>   ssl_bump peek noBumpSites # As there no step specified, squid match
> at any step
> 
> Not exactly. Squid will evaluate this rule at any step that (a) reaches
> this line and (b) where the peek action is applicable. The intersection
> of those two preconditions is "step2" rather than "any step".

Ok, say that the most (not to say the *only*) important beyond any step or action is the *secuential order -line_by_line-* of the rules (steps) .

Example:

  ssl_bump splice noBumpSites # this will be totally ignored by Squid if a stare rule precedes this.

i.e.:

  ssl_bump stare noBumpSites # No matter what, here is the Squid first match and he is at step1...
  ssl_bump splice noBumpSites # ...Therefore here Squid is at step2, then this line will never match, even not having specified the step in both lines, because "noBumpSites" was already stared at first line.

Well, I am not really sure about the above example (Maybe I choosen the worst).. if I a read what the Actions do at wiki, appears doubts in mind, it's just an example about how implicit steps works.

Anyway, as an excercise I guess that in this example what Squid will do is a final "splice noBumpSites" at step2, because stare action always match at step1 (and at wiki, peek/stare description are the same)

I can not realize right now about what will happen at step3 or SslBump3.. guess that there will never be a bump, not sure.
*BUT* if in case that an implicit stare occurs at step2 due to first line, then squid will bump the "noBumpSites" and never-match/ignore the second line completely.

> > then this line, match at step1

"This line" was ssl_bump peek noBumpSites

> No, this line will not be evaluated at step1. Only the first rule is
> evaluated at step1 (because that first rule always matches at step1).
> 
> 
> > and then at step2, so when a match occurs at step2 it precludes future
> bumping of the sites listed in the ACL.
> 
> Yes, but that is kind of irrelevant here because there are no bump rules
> to exclude. At step3, this previous/step2 peeking should result in Squid
> applying the default "splice" rule (you can view that as excluding the
> default "bump" rule if you wish).

Yes, that's the idea, default/implicit bump all, except the "noBumpSites", but maybe is not the best way to do that.

> >> ssl_bump stare all # Here there is either no step2 (and any step)
> >> specified but, because in the previous line You has (implicitly)
> >> peeked at step2, the stare'ing not (or can?t) applies to sites
> >> listed in ACL (they were peeked at step2).
> 
> Something like that. Step2 always happens in this configuration (so "no
> step2" does not make sense), and there is no such thing as "implicit
> peeking", but I think you more-or-less got the right idea here.

I didn't know that no exists "implicit peeking" as you said above. Instead, I always thought that peeking was mandatory.
Resume: Implicit splice and bump exists aalways exists. Implicit peek, no. Is this correct?

See my doubt at the end...and conclusion.

> >> ssl_bump peek noBumpSites # Like previous example, but..I guess
> >> that as there is no "all" explicit, this line do a "peek all at
> >> step1" (implicitly)
> 
> No, this line does not do "peek all". It does "peek noBumpSites". That
> is, it tells Squid to peek when and only when both of the conditions
> below are true:
> 
> (a) the peeking action is applicable (i.e., step1 or step2)
> (b) the noBumpSites ACL matches
> 
> The two conditions above are evaluated in the specified order. Condition
> (b) is not evaluated unless condition (a) is satisfied.

Another important point to keep in mind what your are telling above.

> > To clarify, if I would add an "all" at the end of this line, then all traffic would
> be spliced.
> 
> Adding "all" to any line changes nothing as far as line matching is
> considered. The value of "foo and true" is equivalent to the value of "foo".

So the word "all" makes sense if its is "alone"? Or not even like that?
e.g.: ssl_bump peek step1 all = ssl_bump peek step1, *always*?

> I am not sure I interpret your definition correctly, but I hope the
> following statements will answer your question regardless of that
> interpretation:
> 
> * Staring (at step2) or bumping (at any step) alters TLS bytes on the
> wire. The client and the origin server will see some TLS bytes that are
> going to differ from the TLS bytes they would have seen if Squid was not
> there.
> 
> * In this scope, the deprecated client-first and server-first actions
> are equivalent to applying the "bump" action.
> 
> * If successful, ssl_bump peek and splice actions do not alter TLS
> bytes. Peeking and/or splicing Squid can be viewed as a TCP proxy as far
> as TLS bytes forwarding is concerned. The client and the origin server
> will see the same TLS bytes they would have seen if Squid was not there.
> 
> * In this scope, various errors are usually equivalent to applying the
> "bump" action.
 
Very clear and useful explanation

> 
> If your definition of "secure" is "does not change TLS bytes exchanges
> between client and server"

Yes, you have correctly understood what I tried to mean with the term "secure";  Say..: "Don't let squid touch sites that should not be touched" ...or some like that.

>, then if your configuration has a "stare"
> and/or "bump" actions, it is "insecure". If your configuration may lead
> to certificate validation errors, it is also "insecure".

Does not the splice at step1 and step2 action avoid this? I mean if squid act as a -TCP forward proxy only- for noBumpSites. "Don't touch TLS bytes"

> > When I do this:
> >
> > ssl_bump splice noBumpSites
> > ssl_bump stare all
> 
> > It is supposed that in this config I am (guessing), implicity,
> > peeking  (first?) and splice at any step and bumping (implicity) at
> > step3 sites that does not match with whitelist by staring at step2.
> > Maybe something like that, I dont know.
> 
> I do not think your description of the above configuration is correct.
> Squid uses default actions ("splice" or "bump") when no applicable rules
> match. In the above configuration, one of the rules will always match
> during step1 and during step2 (if any). Thus, there will be no implicit
> splicing or bumping during the first steps. If Squid reaches step3, then
> Squid will apply the default bump rule at that step (because "stare"
> matched at the previous step).
> 
> I am not sure, but I think the above configuration is equivalent to the
> following configuration that does not rely on default rules:
> 
>   ssl_bump splice step1 noBumpSites
>   ssl_bump splice step2 noBumpSites
>   ssl_bump stare step1
>   ssl_bump stare step2
>   ssl_bump bump step3

According to Amos: Always is better to be explicit and bump at step3 after stare at step2. (And of course more clearly to understand)

I have tested this above config (I think that this one you've posted is what I want to do) against the "compact/default one (the last "2-lines-config" above) and I almost sure that the squid logs reports the same behaviour, and maybe there are less lines with: "ssl" lib errors...and "Security Alert: there is no ip/domain match...." 

BUT here you are never peek'ing? How is that? 
You are stare'ing instead of peek'ing at step1 (3rd line), I would have done a peek at that line. I refered to this question when I said  "see the doubt at the end..." at almost at the middle of msg..

> HTH,
> 
> Alex.

OK....:

I think that is enough. We should make a pause or close the thread.
I *MUST* as soon as possible re-re and re-read the this thread entirely again and again.. And the Wiki page too. 
Because I am remembering (in English as well as I can) that Amos said things that You are telling me -again- (maybe in other words, but that isn't important neither the point)

All the best,

Thank You all



From squid3 at treenet.co.nz  Thu Sep 13 04:27:02 2018
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 13 Sep 2018 16:27:02 +1200
Subject: [squid-users] Unable to Disable sslv3
In-Reply-To: <f93d5bac-0412-e618-96de-42e480473837@measurement-factory.com>
References: <1536788829.3128050.1506146552.01004947@webmail.messagingengine.com>
 <f93d5bac-0412-e618-96de-42e480473837@measurement-factory.com>
Message-ID: <534391ef-b8b0-8bfe-8376-636560c32be3@treenet.co.nz>

On 13/09/18 12:54 PM, Alex Rousskov wrote:
> On 09/12/2018 03:47 PM, squid wrote:
> 
>> We are using squid as reverse proxy and we have disabled SSLv3 :
> 
>> https_port XXX.XXX.XXX.XXX:443 accel defaultsite=www.example.com
>> vhost cert=/etc/....cert.pem key=/etc/....privkey.pem
>> options=NO_SSLv2,NO_SSLv3,SINGLE_DH_USE,CIPHER_SERVER_PREFERENCE
>> cipher=ECDHE-ECDSA . . .. dhparams=/etc/...dhparams.pem
> 
>> We have also tried the sslproxy_options as well.
> 
>> Using Nessus scanning tool, it reports that SSLv3 is enabled, but not
>> SSLv2.
> 
>> Version of Squid is  (3.1.23) which is stock RH6 which I know is old,
>> but for now we need to use it.
> 

I assume you mean RHEL6 rather than RH6 from the 1990's, if not, then my
sympathies.

OpenSSL options to disable SSLv3 were not added until Squid-3.2 when
TLS-only support was added.


FYI: the list of currently known security vulnerabilities for Squid-3.1
is so long now that I have given up on trying to list them all in our
wiki. IMHO, even with RHEL patching SSLv3 being enabled is the least of
your worries with that Squid. *PLEASE* upgrade Squid.

The RHEL maintainer is providing a special package for later versions of
Squid (IIRC a Squid-3.4 build) to help get RHEL6 people off it. Also,
Eliezer here is providing packages of current Squid releases for the
Fedora/RHEL/CentOS OS family.


You can remove the EC* ciphers in your config. The extra settings
required to enable use any Elliptic Curve support in the library was not
added until late in the Squid-3.5 series.

Amos


From rousskov at measurement-factory.com  Thu Sep 13 15:00:14 2018
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Thu, 13 Sep 2018 09:00:14 -0600
Subject: [squid-users] About SSL peek-n-splice/bump configurations
In-Reply-To: <004401d44b0e$46cf40c0$d46dc240$@yahoo.com.ar>
References: <000301d43289$14ed4770$3ec7d650$@yahoo.com.ar>
 <58014777-2b9c-7630-80dd-877c2d2af9d2@measurement-factory.com>
 <001801d432a0$8d102a30$a7307e90$@yahoo.com.ar>
 <6eb5660d-8025-eedd-3ed9-47d30bf22e62@measurement-factory.com>
 <001601d4464c$d1b38910$751a9b30$@yahoo.com.ar>
 <770057a5-3808-3fdc-5bf7-f78d09705913@treenet.co.nz>
 <002001d446cc$148bb580$3da32080$@yahoo.com.ar>
 <c7a4d318-c335-03de-9e5c-73fc258e38c0@treenet.co.nz>
 <008001d4479b$b22b9290$1682b7b0$@yahoo.com.ar>
 <04c7931e-cb29-80d1-2a2a-8a1ccb4761f5@treenet.co.nz>
 <019301d4486b$049ebf00$0ddc3d00$@yahoo.com.ar>
 <6b762c34-f0c4-2d9a-6616-432305af160b@treenet.co.nz>
 <009d01d44935$1648ef80$42dace80$@yahoo.com.ar>
 <5400815d-9a82-97a9-5889-0e76c04a9ba0@measurement-factory.com>
 <003501d44aa4$d784e5d0$868eb170$@yahoo.com.ar>
 <500fbad5-d277-965f-d29a-a81c2f343d2e@measurement-factory.com>
 <004401d44b0e$46cf40c0$d46dc240$@yahoo.com.ar>
Message-ID: <0472b838-415e-e4d2-c71a-7fbce45de926@measurement-factory.com>

On 09/12/2018 09:02 PM, Julian Perconti wrote:

> ssl_bump peek step1
> ssl_bump peek noBumpSites
> ssl_bump stare all

>>>> ssl_bump peek noBumpSites # As there no step specified, squid match
>>>> at any step

>> Not exactly. Squid will evaluate this rule at any step that (a) reaches
>> this line and (b) where the peek action is applicable. The intersection
>> of those two preconditions is "step2" rather than "any step".

> Ok, say that the most (not to say the *only*) important beyond any step or action is the *secuential order -line_by_line-* of the rules (steps) .
> 
> Example:
> 
>   ssl_bump splice noBumpSites # this will be totally ignored by Squid if a stare rule precedes this.

No, this is incorrect. There are many cases were a previous stare rule
will not have the effect you state it will. For example:

  # Squid may splice at step2 despite the preceding stare rule
  # because staring at step1 does not preclude splicing.

  ssl_bump stare step1
  ssl_bump splice noBumpSites

and

  # Squid will splice at step1 despite the preceding stare rule
  # because the preceding stare rule never matches
  ssl_bump stare !all
  ssl_bump splice all


> ssl_bump stare noBumpSites # No matter what, here is the Squid first
> match and he is at step1...

> ssl_bump splice noBumpSites # ...Therefore here Squid is at step2,
> then this line will never match, even not having specified the step
> in both lines, because "noBumpSites" was already stared at first
> line.
You seem to be assuming that the action during the previous step is
important even if the rule with that action did not match during the
previous step. That is incorrect. The previous step action may be
important only if it was actually used during that previous step (i.e.
Squid reached that action's rule and that rule's ACLs matched).


> if in case that an implicit stare occurs at step2 due to first line,
> then squid will [...] never-match/ignore the second line completely.

That is correct. The part in [...] was a bit misleading because it
depended on the not shown (default?) rules, so I removed it.



> Resume: Implicit splice and bump exists aalways exists. Implicit peek, no. Is this correct?

There are ways to interpret this statement correctly, but I would not
phrase it this way.

A better statement would be that when no rule matched at a given step,
Squid uses the default rule. The default rule is "bump" if Squid stared
at the previous step. The default rule is "splice" in all other cases,
including the case where no rule matched during the first step (i.e.
there was no "previous step").


>> Adding "all" to any line changes nothing as far as line matching is
>> considered. The value of "foo and true" is equivalent to the value of "foo".
> 
> So the word "all" makes sense if its is "alone"?

The documented ssl_bump line syntax requires an ACL. If you want a rule
to always match, use an "all" ACL. Ideally, the syntax should be relaxed
to not require an ACL at all in those cases. I did not check whether the
implementation matches the documentation.


> e.g.: ssl_bump peek step1 all = ssl_bump peek step1, *always*?

Yes, as far as rule matching is concerned. The "all" ACL always matches.


>> if your configuration has a "stare"
>> and/or "bump" actions, it is "insecure". If your configuration may lead
>> to certificate validation errors, it is also "insecure".

> Does not the splice at step1 and step2 action avoid this? I mean if
> squid act as a -TCP forward proxy only- for noBumpSites. "Don't touch
> TLS bytes"

I am not sure what you mean by "this" exactly, but splicing (at any
step) does not guarantee the lack of errors. The earlier you tell Squid
to splice the connections, the fewer checks Squid will do, decreasing
the probability of an error. Errors lead to bumping the client
connection (to deliver the error message).


>>> When I do this:
>>>
>>> ssl_bump splice noBumpSites
>>> ssl_bump stare all
>>
>>> It is supposed that in this config I am (guessing), implicity,
>>> peeking  (first?) and splice at any step and bumping (implicity) at
>>> step3 sites that does not match with whitelist by staring at step2.
>>> Maybe something like that, I dont know.
>>
>> I do not think your description of the above configuration is correct.
>> Squid uses default actions ("splice" or "bump") when no applicable rules
>> match. In the above configuration, one of the rules will always match
>> during step1 and during step2 (if any). Thus, there will be no implicit
>> splicing or bumping during the first steps. If Squid reaches step3, then
>> Squid will apply the default bump rule at that step (because "stare"
>> matched at the previous step).
>>
>> I am not sure, but I think the above configuration is equivalent to the
>> following configuration that does not rely on default rules:
>>
>>   ssl_bump splice step1 noBumpSites
>>   ssl_bump splice step2 noBumpSites
>>   ssl_bump stare step1
>>   ssl_bump stare step2
>>   ssl_bump bump step3

> According to Amos: Always is better to be explicit and bump at step3
> after stare at step2. (And of course more clearly to understand)

That "always better to be explicit" recipe fails when you cannot tell (a
priori) what the previous step result was. In those cases, you may want
to rely on the default rules (after making sure they work) OR rewrite
the configuration so that you know for sure what the previous step
result was. In the two-line configuration above, we always know what the
previous step result was, so it is possible to add an explicit (and
correct) step3 action.

Which configuration is clearer depends on the observer. Use whatever
correct variant looks best to _you_.


> BUT here you are never peek'ing? How is that? 

Staring at step1 performs the same actions as peeking at step1. The only
difference is the side effect on the default rules choice for step2 (if
Squid needs to make that choice). In both of the above configurations,
there is no need for Squid to make that choice for step2.


Alex.


From rousskov at measurement-factory.com  Thu Sep 13 15:35:56 2018
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Thu, 13 Sep 2018 09:35:56 -0600
Subject: [squid-users] Unable to Disable sslv3
In-Reply-To: <534391ef-b8b0-8bfe-8376-636560c32be3@treenet.co.nz>
References: <1536788829.3128050.1506146552.01004947@webmail.messagingengine.com>
 <f93d5bac-0412-e618-96de-42e480473837@measurement-factory.com>
 <534391ef-b8b0-8bfe-8376-636560c32be3@treenet.co.nz>
Message-ID: <6d8dcf2b-2079-8cc0-d1d1-b1670e7adb39@measurement-factory.com>

On 09/12/2018 10:27 PM, Amos Jeffries wrote:

> OpenSSL options to disable SSLv3 were not added until Squid-3.2 when
> TLS-only support was added.

What makes you think that? AFAICT, support for disabling SSLv3 on
https_port was added years before Squid v3.1 was branched:
https://github.com/squid-cache/squid/commit/1f7c917

Alex.


From squid3 at treenet.co.nz  Thu Sep 13 16:36:32 2018
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Fri, 14 Sep 2018 04:36:32 +1200
Subject: [squid-users] Unable to Disable sslv3
In-Reply-To: <6d8dcf2b-2079-8cc0-d1d1-b1670e7adb39@measurement-factory.com>
References: <1536788829.3128050.1506146552.01004947@webmail.messagingengine.com>
 <f93d5bac-0412-e618-96de-42e480473837@measurement-factory.com>
 <534391ef-b8b0-8bfe-8376-636560c32be3@treenet.co.nz>
 <6d8dcf2b-2079-8cc0-d1d1-b1670e7adb39@measurement-factory.com>
Message-ID: <0e79916a-41c6-b640-7370-d42772533b92@treenet.co.nz>

On 14/09/18 3:35 AM, Alex Rousskov wrote:
> On 09/12/2018 10:27 PM, Amos Jeffries wrote:
> 
>> OpenSSL options to disable SSLv3 were not added until Squid-3.2 when
>> TLS-only support was added.
> 
> What makes you think that? AFAICT, support for disabling SSLv3 on
> https_port was added years before Squid v3.1 was branched:
> https://github.com/squid-cache/squid/commit/1f7c917
> 

Sorry, my mistake. Looking for the wrong macro definition in the source.

Amos


From mujtaba21n at hotmail.com  Thu Sep 13 17:36:48 2018
From: mujtaba21n at hotmail.com (Mujtaba   Hassan Madani)
Date: Thu, 13 Sep 2018 17:36:48 +0000
Subject: [squid-users] Squid Cache Server
In-Reply-To: <b6e884e9-0aa3-a48a-b529-afcfe4c60f51@treenet.co.nz>
References: <DB6P193MB0008E0848E133CEB22448158C6040@DB6P193MB0008.EURP193.PROD.OUTLOOK.COM>
 <201809111054.06037.Antony.Stone@squid.open.source.it>
 <DB6P193MB00085E55BC2E60C264EE09A4C61B0@DB6P193MB0008.EURP193.PROD.OUTLOOK.COM>,
 <b6e884e9-0aa3-a48a-b529-afcfe4c60f51@treenet.co.nz>
Message-ID: <HE1P193MB0009EFBF04C8D339B5D23090C61A0@HE1P193MB0009.EURP193.PROD.OUTLOOK.COM>

Hi Amos,

   Iam looking for building a Squid proxy server on Ubuntu for my LAN serving up to 25 PC's I just want the maximum potential of the server capability to enhance the network performance and gain better users expectation of the service.

regards



Mujtaba H,

________________________________
From: squid-users <squid-users-bounces at lists.squid-cache.org> on behalf of Amos Jeffries <squid3 at treenet.co.nz>
Sent: Wednesday, September 12, 2018 2:54:37 PM
To: squid-users at lists.squid-cache.org
Subject: Re: [squid-users] Squid Cache Server

On 13/09/18 2:16 AM, Mujtaba   Hassan Madani wrote:
> Dear Squid Team,
>
>      how does content provider prevent it from been cached while passing
> through squid proxy it's by a copy right law

No. Contents which can be transferred through a proxy are implicitly
licensed for re-distribution.

Legal issues are usually encountered only around interception or
modification of content.


> or some  encryption is

Sometimes.

> implemented in the traffic ?

and other features built into HTTP protocol.


> and where can I find the contents that been
> cached on my squid proxy ?
>

Depends on your config. Usually in the machine RAM.

What are you looking for exactly? and why?


Amos
_______________________________________________
squid-users mailing list
squid-users at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-users
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20180913/7b647ec9/attachment.htm>

From jimoe at sohnen-moe.com  Thu Sep 13 19:00:03 2018
From: jimoe at sohnen-moe.com (James Moe)
Date: Thu, 13 Sep 2018 12:00:03 -0700
Subject: [squid-users] Long delays with TLS
Message-ID: <b2573afa-993f-1100-ae81-e4b404d66c9f@sohnen-moe.com>

Hello,
  squid 4.0.23
  linux 4.12.14-lp150.12.7-default x86_64

  We have been seeing frequent, but not consistent, delays when proxying
TLS requests while browsing. By disabling the proxy, those delays
stopped occurring.
  I can see no obvious hint in either the access or cache logs.
  (Is there a way to use ISO time format in the logs?)

  Where should I look to find what is causing the delay?

----[ conf ]----
acl manager_admin src 192.168.69.115
#
# acl localnet src fc00::/7
# acl localnet src fe80::/10
#
acl SSL_ports port 443
acl SSL_ports port 631
#
# Jumpline cPanel ports
acl SSL_ports port 2083
acl SSL_ports port 2096
#
# sma-nas-02, cgatePro, webadmin
acl SSL_ports port 5000
acl SSL_ports port 5001
acl SSL_ports port 9010
acl SSL_ports port 9100
acl SSL_ports port 10000
#
acl Safe_ports port 80
acl Safe_ports port 21
acl Safe_ports port 443
acl Safe_ports port 563
acl Safe_ports port 631
acl Safe_ports port 70
acl Safe_ports port 210
acl Safe_ports port 1025-65535
acl Safe_ports port 280
acl Safe_ports port 488
acl Safe_ports port 591
acl Safe_ports port 777
acl Safe_ports port 9100
#
acl CONNECT method CONNECT
acl localnet src 192.168.69.0/24

access_log /var/log/squid/access.log
#
http_access deny !Safe_ports
http_access deny CONNECT !SSL_ports
http_access allow manager_admin
http_access allow manager localhost
http_access deny manager
http_access allow localnet
http_access deny all

# Squid normally listens to port 3128
http_port 3128

cache_dir ufs /data01/var/cache/squid 51200 16 256
maximum_object_size 99999 KB
cache_mem 256 MB
coredump_dir /var/cache/squid

refresh_pattern ^ftp: 1440 20 10080
refresh_pattern ^gopher: 1440 0 1440
refresh_pattern -i  (/cgi-bin/|\?) 0 0 0
refresh_pattern . 0 20 4320

cache_log /var/log/squid/cache.log
cache_mgr jimoe at sohnen-moe.com
cache_replacement_policy lru
cache_store_log /var/log/squid/store.log
cache_swap_high 95
cache_swap_low 90
client_lifetime 1 days
connect_timeout 2 minutes
error_directory /usr/share/squid/errors/en
ftp_passive on
memory_replacement_policy lru
minimum_object_size 0 KB
----[ end ]----

-- 
James Moe
moe dot james at sohnen-moe dot com
520.743.3936
Think.

-------------- next part --------------
A non-text attachment was scrubbed...
Name: signature.asc
Type: application/pgp-signature
Size: 195 bytes
Desc: OpenPGP digital signature
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20180913/2308dc25/attachment.sig>

From rousskov at measurement-factory.com  Thu Sep 13 19:46:04 2018
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Thu, 13 Sep 2018 13:46:04 -0600
Subject: [squid-users] Long delays with TLS
In-Reply-To: <b2573afa-993f-1100-ae81-e4b404d66c9f@sohnen-moe.com>
References: <b2573afa-993f-1100-ae81-e4b404d66c9f@sohnen-moe.com>
Message-ID: <bd95a86f-84ee-bf4f-308a-4e5405d13ffb@measurement-factory.com>

On 09/13/2018 01:00 PM, James Moe wrote:
> Hello,
>   squid 4.0.23
>   linux 4.12.14-lp150.12.7-default x86_64
> 
>   We have been seeing frequent, but not consistent, delays when proxying
> TLS requests while browsing. By disabling the proxy, those delays
> stopped occurring.

FYI: Your Squid is not configured to treat TLS specially. In that
configuration, your Squid does not know anything about TLS -- TLS
traffic is just opaque bytes to your Squid, hidden inside HTTP CONNECT
tunnels.


>   I can see no obvious hint in either the access or cache logs.

>   Where should I look to find what is causing the delay?

I would start by upgrading to the latest Squid v4 and then check for DNS
delays. DNS is used by both regular HTTP traffic and CONNECT tunnels
(carrying TLS bytes) so it may not fit your problem description
perfectly, but DNS delays are easier to check for than other suspects.
You can log DNS-related response times (%dt) and/or capture DNS traffic
to and from your Squid.


HTH,

Alex.


> ----[ conf ]----
> acl manager_admin src 192.168.69.115
> #
> # acl localnet src fc00::/7
> # acl localnet src fe80::/10
> #
> acl SSL_ports port 443
> acl SSL_ports port 631
> #
> # Jumpline cPanel ports
> acl SSL_ports port 2083
> acl SSL_ports port 2096
> #
> # sma-nas-02, cgatePro, webadmin
> acl SSL_ports port 5000
> acl SSL_ports port 5001
> acl SSL_ports port 9010
> acl SSL_ports port 9100
> acl SSL_ports port 10000
> #
> acl Safe_ports port 80
> acl Safe_ports port 21
> acl Safe_ports port 443
> acl Safe_ports port 563
> acl Safe_ports port 631
> acl Safe_ports port 70
> acl Safe_ports port 210
> acl Safe_ports port 1025-65535
> acl Safe_ports port 280
> acl Safe_ports port 488
> acl Safe_ports port 591
> acl Safe_ports port 777
> acl Safe_ports port 9100
> #
> acl CONNECT method CONNECT
> acl localnet src 192.168.69.0/24
> 
> access_log /var/log/squid/access.log
> #
> http_access deny !Safe_ports
> http_access deny CONNECT !SSL_ports
> http_access allow manager_admin
> http_access allow manager localhost
> http_access deny manager
> http_access allow localnet
> http_access deny all
> 
> # Squid normally listens to port 3128
> http_port 3128
> 
> cache_dir ufs /data01/var/cache/squid 51200 16 256
> maximum_object_size 99999 KB
> cache_mem 256 MB
> coredump_dir /var/cache/squid
> 
> refresh_pattern ^ftp: 1440 20 10080
> refresh_pattern ^gopher: 1440 0 1440
> refresh_pattern -i  (/cgi-bin/|\?) 0 0 0
> refresh_pattern . 0 20 4320
> 
> cache_log /var/log/squid/cache.log
> cache_mgr jimoe at sohnen-moe.com
> cache_replacement_policy lru
> cache_store_log /var/log/squid/store.log
> cache_swap_high 95
> cache_swap_low 90
> client_lifetime 1 days
> connect_timeout 2 minutes
> error_directory /usr/share/squid/errors/en
> ftp_passive on
> memory_replacement_policy lru
> minimum_object_size 0 KB
> ----[ end ]----



From vh1988 at yahoo.com.ar  Fri Sep 14 00:13:40 2018
From: vh1988 at yahoo.com.ar (Julian Perconti)
Date: Thu, 13 Sep 2018 21:13:40 -0300
Subject: [squid-users] About SSL peek-n-splice/bump configurations
In-Reply-To: <0472b838-415e-e4d2-c71a-7fbce45de926@measurement-factory.com>
References: <000301d43289$14ed4770$3ec7d650$@yahoo.com.ar>
 <58014777-2b9c-7630-80dd-877c2d2af9d2@measurement-factory.com>
 <001801d432a0$8d102a30$a7307e90$@yahoo.com.ar>
 <6eb5660d-8025-eedd-3ed9-47d30bf22e62@measurement-factory.com>
 <001601d4464c$d1b38910$751a9b30$@yahoo.com.ar>
 <770057a5-3808-3fdc-5bf7-f78d09705913@treenet.co.nz>
 <002001d446cc$148bb580$3da32080$@yahoo.com.ar>
 <c7a4d318-c335-03de-9e5c-73fc258e38c0@treenet.co.nz>
 <008001d4479b$b22b9290$1682b7b0$@yahoo.com.ar>
 <04c7931e-cb29-80d1-2a2a-8a1ccb4761f5@treenet.co.nz>
 <019301d4486b$049ebf00$0ddc3d00$@yahoo.com.ar>
 <6b762c34-f0c4-2d9a-6616-432305af160b@treenet.co.nz>
 <009d01d44935$1648ef80$42dace80$@yahoo.com.ar>
 <5400815d-9a82-97a9-5889-0e76c04a9ba0@measurement-factory.com>
 <003501d44aa4$d784e5d0$868eb170$@yahoo.com.ar>
 <500fbad5-d277-965f-d29a-a81c2f343d2e@measurement-factory.com>
 <004401d44b0e$46cf40c0$d46dc240$@yahoo.com.ar>
 <0472b838-415e-e4d2-c71a-7fbce45de926@measurement-factory.com>
Message-ID: <008e01d44bbf$ccb41e20$661c5a60$@yahoo.com.ar>

> > Example:
> >
> >   ssl_bump splice noBumpSites # this will be totally ignored by Squid if a
> stare rule precedes this.
> 
> No, this is incorrect. There are many cases were a previous stare rule will not
> have the effect you state it will. For example:
> 
>   # Squid may splice at step2 despite the preceding stare rule
>   # because staring at step1 does not preclude splicing.
> 
>   ssl_bump stare step1
>   ssl_bump splice noBumpSites

Well yes, I think You are right; but my example (or what I wanted to mean) was: -Maybe You have post that to give an example about how that rule could probably, not match, I don't know-

   ssl_bump stare noBumpSites  (at this line your example said: ssl_bump stare step1)
   ssl_bump splice noBumpSites

...And here appears a "key-question":

   ssl_bump stare noBumpSites # This is the first line of SslBumps ruleset.

So, when squid reaches this first rule and line (there is no explicit step)  ...does Squid make a "bucle of steps" only along the first line and go to next line only when the rule stop being applicable/matchable?
If the answer of my question is: "Yes" then the second line has not any effect because, I guess that squid will do a bump in more-or-less this way:

   ssl_bump stare  noBumpSites

... is the same as:

   ssl_bump stare  step1 noBumpSites
   ssl_bump stare  step2 noBumpSites # Here is where he second line stops making sense
   ssl_bump bump  step3 noBumpSites # Finally bump due to the previous step

Thus:

   ssl_bump splice noBumpSites # will never matchs.

Going a bit to the past, Amos explained the following when I asked:

>> ...So that means that squid processes the SslBump directives:
>> 1: maybe more than one time in a single request...?
>> 
>Yes. Up to 3 times. A peek or splice action causes another check later.

Well, Amos never mentioned a "stare" action here, so I dont know I a "stare" applies to this too. 
And even worse, maybe I did not understand him correctly.

>   # Squid will splice at step1 despite the preceding stare rule
>   # because the preceding stare rule never matches
>   ssl_bump stare !all
>   ssl_bump splice all

And this example is more obvious than the first one. It is like that previous line would not exists.

(...)

> > Does not the splice at step1 and step2 action avoid this? I mean if
> > squid act as a -TCP forward proxy only- for noBumpSites. "Don't touch
> > TLS bytes"
> 
> I am not sure what you mean by "this" exactly, but splicing (at any
> step) does not guarantee the lack of errors. 

Ok, but is Squid the culprit of those error? He is being a passive observer of that TLS traffic.
Here, I am talking about the idea of (explicitly) splice at step1 and then at step2 of a white list of sites.

Question based on words below:

>>>* If successful, ssl_bump peek and splice actions do not alter TLS
>>>bytes. Peeking and/or splicing Squid can be viewed as a TCP proxy as far
>>>as TLS bytes forwarding is concerned. The client and the origin server
>>>will see the same TLS bytes they would have seen if Squid was not there.
>>>
>>>* In this scope, various errors are usually equivalent to applying the
>>>"bump" action.

>The earlier you tell Squid to
> splice the connections, the fewer checks Squid will do, decreasing the
> probability of an error.

That is the idea with the noBumpSites ACL, the least amount of errors possible.

Lets say: "Let's remove as much responsibility as possible to Squid about what happens with really/special sensitive sites, If something goes wrong"
Talking with Squid/In other words: "Squid, do a *full* bump to msn.com and youtube.com too; but please *never do not nothing neither touch nothing*  with bankaust.com.au 
(Some like that)

> Errors lead to bumping the client connection (to
> deliver the error message).

What do You mean about those errors?

Thank You



From alessio.troiano at leonardocompany.com  Fri Sep 14 10:33:54 2018
From: alessio.troiano at leonardocompany.com (Troiano Alessio)
Date: Fri, 14 Sep 2018 10:33:54 +0000
Subject: [squid-users] SQUID does not insert server ip and port in logs for
 CONNECT method when the connection fails (error 503)
Message-ID: <53843021b2bf4dc3b21463fc8707d02a@ocgepvsw3101.ocr.priv>

Hello,
I'm seeing the problem as from subject. I'm interested in log fields %<a %<p %<lp. In HTTPS connections when the destination server does not answer (maybe blocked by our firewall because it is malicious) the destination ip is not logged. In this way we cannot find the source client IP related to the blocked connection logged by the firewall.
For GET method all works as expected.

Follow the squid.conf log settings and two logs of connection to http://sqm.telemetry.microsoft.com and https://sqm.telemetry.microsoft.com . The site is not reachable.

Squid.conf:
logformat custom_squid %%SQUID-4: %>a %>p [%tl] "%rm %ru HTTP/%rv" %<A %ui %un "%rp" %Hs %mt %<st "%{Referer}>h" "%{User-Agent}>h" %Ss:%Sh %<a %<p %<lp
access_log /var/log/squid/rsa/access.log custom_squid

accesso.log:
%SQUID-4: 172.x.x.x 56371 [14/Sep/2018:05:04:51 -0500] "CONNECT sqm.telemetry.microsoft.com:443 HTTP/1.1" - - - "-" 503 - 0 "-" "Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:62.0) Gecko/20100101 Firefox/62.0" TAG_NONE:HIER_NONE - - -
%SQUID-4: 172.x.x.x 56490 [14/Sep/2018:05:14:42 -0500] "GET http://sqm.telemetry.microsoft.com/ HTTP/1.1" sqm.telemetry.microsoft.com - - "/" 502 text/html 5405 "-" "Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:62.0) Gecko/20100101 Firefox/62.0" TCP_MISS:HIER_DIRECT 65.55.252.93 80 60796

OS info and process:
[root at HUB-XX-XX-XX squid]# squid -v
Squid Cache: Version 3.5.20
Service Name: squid
configure options:  '--build=x86_64-redhat-linux-gnu' '--host=x86_64-redhat-linux-gnu' '--program-prefix=' '--prefix=/usr' '--exec-prefix=/usr' '--bindir=/usr/bin' '--sbindir=/usr/sbin' '--sysconfdir=/etc' '--datadir=/usr/share' '--includedir=/usr/include' '--libdir=/usr/lib64' '--libexecdir=/usr/libexec' '--sharedstatedir=/var/lib' '--mandir=/usr/share/man' '--infodir=/usr/share/info' '--disable-strict-error-checking' '--exec_prefix=/usr' '--libexecdir=/usr/lib64/squid' '--localstatedir=/var' '--datadir=/usr/share/squid' '--sysconfdir=/etc/squid' '--with-logdir=$(localstatedir)/log/squid' '--with-pidfile=$(localstatedir)/run/squid.pid' '--disable-dependency-tracking' '--enable-eui' '--enable-follow-x-forwarded-for' '--enable-auth' '--enable-auth-basic=DB,LDAP,MSNT-multi-domain,NCSA,NIS,PAM,POP3,RADIUS,SASL,SMB,SMB_LM,getpwnam' '--enable-auth-ntlm=smb_lm,fake' '--enable-auth-digest=file,LDAP,eDirectory' '--enable-auth-negotiate=kerberos' '--enable-external-acl-helpers=file_userip,LDAP_group,time_quota,session,unix_group,wbinfo_group' '--enable-cache-digests' '--enable-cachemgr-hostname=localhost' '--enable-delay-pools' '--enable-epoll' '--enable-ident-lookups' '--enable-linux-netfilter' '--enable-removal-policies=heap,lru' '--enable-snmp' '--enable-ssl-crtd' '--enable-storeio=aufs,diskd,rock,ufs' '--enable-wccpv2' '--enable-esi' '--enable-ecap' '--with-aio' '--with-default-user=squid' '--with-dl' '--with-openssl' '--with-pthreads' '--disable-arch-native' 'build_alias=x86_64-redhat-linux-gnu' 'host_alias=x86_64-redhat-linux-gnu' 'CFLAGS=-O2 -g -pipe -Wall -Wp,-D_FORTIFY_SOURCE=2 -fexceptions -fstack-protector-strong --param=ssp-buffer-size=4 -grecord-gcc-switches   -m64 -mtune=generic -fpie' 'LDFLAGS=-Wl,-z,relro  -pie -Wl,-z,relro -Wl,-z,now' 'CXXFLAGS=-O2 -g -pipe -Wall -Wp,-D_FORTIFY_SOURCE=2 -fexceptions -fstack-protector-strong --param=ssp-buffer-size=4 -grecord-gcc-switches   -m64 -mtune=generic -fpie' 'PKG_CONFIG_PATH=:/usr/lib64/pkgconfig:/usr/share/pkgconfig'
[root@ HUB-XX-XX-XX squid]# cat /etc/redhat-release
Red Hat Enterprise Linux Server release 7.4 (Maipo)

We are using the last stable release of squid for Red Hat.

Thank you, Best Regards.

Il presente messaggio e-mail e ogni suo allegato devono intendersi indirizzati esclusivamente al destinatario indicato e considerarsi dal contenuto strettamente riservato e confidenziale. Se non siete l'effettivo destinatario o avete ricevuto il messaggio e-mail per errore, siete pregati di avvertire immediatamente il mittente e di cancellare il suddetto messaggio e ogni suo allegato dal vostro sistema informatico. Qualsiasi utilizzo, diffusione, copia o archiviazione del presente messaggio da parte di chi non ne ? il destinatario ? strettamente proibito e pu? dar luogo a responsabilit? di carattere civile e penale punibili ai sensi di legge.
Questa e-mail ha valore legale solo se firmata digitalmente ai sensi della normativa vigente.

The contents of this email message and any attachments are intended solely for the addressee(s) and contain confidential and/or privileged information.
If you are not the intended recipient of this message, or if this message has been addressed to you in error, please immediately notify the sender and then delete this message and any attachments from your system. If you are not the intended recipient, you are hereby notified that any use, dissemination, copying, or storage of this message or its attachments is strictly prohibited. Unauthorized disclosure and/or use of information contained in this email message may result in civil and criminal liability. ?
This e-mail has legal value according to the applicable laws only if it is digitally signed by the sender

From rousskov at measurement-factory.com  Fri Sep 14 15:17:34 2018
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Fri, 14 Sep 2018 09:17:34 -0600
Subject: [squid-users] About SSL peek-n-splice/bump configurations
In-Reply-To: <008e01d44bbf$ccb41e20$661c5a60$@yahoo.com.ar>
References: <000301d43289$14ed4770$3ec7d650$@yahoo.com.ar>
 <001801d432a0$8d102a30$a7307e90$@yahoo.com.ar>
 <6eb5660d-8025-eedd-3ed9-47d30bf22e62@measurement-factory.com>
 <001601d4464c$d1b38910$751a9b30$@yahoo.com.ar>
 <770057a5-3808-3fdc-5bf7-f78d09705913@treenet.co.nz>
 <002001d446cc$148bb580$3da32080$@yahoo.com.ar>
 <c7a4d318-c335-03de-9e5c-73fc258e38c0@treenet.co.nz>
 <008001d4479b$b22b9290$1682b7b0$@yahoo.com.ar>
 <04c7931e-cb29-80d1-2a2a-8a1ccb4761f5@treenet.co.nz>
 <019301d4486b$049ebf00$0ddc3d00$@yahoo.com.ar>
 <6b762c34-f0c4-2d9a-6616-432305af160b@treenet.co.nz>
 <009d01d44935$1648ef80$42dace80$@yahoo.com.ar>
 <5400815d-9a82-97a9-5889-0e76c04a9ba0@measurement-factory.com>
 <003501d44aa4$d784e5d0$868eb170$@yahoo.com.ar>
 <500fbad5-d277-965f-d29a-a81c2f343d2e@measurement-factory.com>
 <004401d44b0e$46cf40c0$d46dc240$@yahoo.com.ar>
 <0472b838-415e-e4d2-c71a-7fbce45de926@measurement-factory.com>
 <008e01d44bbf$ccb41e20$661c5a60$@yahoo.com.ar>
Message-ID: <2b2e8b70-7f45-039c-462f-ba78b2fc1563@measurement-factory.com>

On 09/13/2018 06:13 PM, Julian Perconti wrote:

>    ssl_bump stare noBumpSites # This is the first line of SslBumps ruleset.

> So, when squid reaches this first rule and line (there is no explicit
> step)  ...does Squid make a "bucle of steps" only along the first
> line and go to next line only when the rule stop being
> applicable/matchable?

I hesitate answering that question with a simple "yes" or "no" because
any such answer is likely to mislead folks reading this email.

The overall logic is like this:

  for each step
  do
      for each rule
      do
          if the rule action is possible and the rule ACLs match,
              then perform the rule action and either go to the next
              step or, after applying the final action, exit
      done
      apply the default action and exit
  done


The overall logic is _not_ like this:

  for each rule
  do
      for each step
      ...
  done

So, "yes", Squid only executes the first rule action _when_ the first
rule action is applicable and its ACLs match at every step, but, "no",
Squid does not make a bunch of steps with only the first rule in mind.


>>> Does not the splice at step1 and step2 action avoid this? I mean if
>>> squid act as a -TCP forward proxy only- for noBumpSites. "Don't touch
>>> TLS bytes"

>> I am not sure what you mean by "this" exactly, but splicing (at any
>> step) does not guarantee the lack of errors. 

> Ok, but is Squid the culprit of those error?

Usually not. Common errors include network connectivity errors, client
Host validation errors, server certificate validation errors, and being
fed a non-TLS protocol. However, it could be Squid itself. Bugs do happen.


> He is being a passive observer of that TLS traffic.

Squid also validates what it observes/forwards. And there is also TCP/IP
traffic before (and around) TLS traffic.


> Here, I am talking about the idea of (explicitly) splice at step1 and then at step2 of a white list of sites.

If you splice at step1, then the number of validations that Squid does
would be smaller (possibly zero, not sure) than if you splice at step2.
Same for the step2/step3 difference. TCP/IP-level errors may be present
at every step.


> Lets say: "Let's remove as much responsibility as possible to Squid
> about what happens with really/special sensitive sites, If something
> goes wrong"

Your overall intent was clear many emails ago, and it is a common
desire/need. The devil is in the details:

* A key detail here is determining whether the intended site _is_
"really/special sensitive". For example, the intercepted client is
connecting to b::a:d IPv6 address while claiming in the TLS Hello that
it is trying to get to sensitive.example.com. Should Squid trust the
intended destination IP address or the TLS SNI? Or should we wait for
the server to identify itself with a valid SSL certificate? Etc.

* The other key detail is what should happen when that sensitive site
refuses to communicate with Squid or otherwise misbehaves. Should Squid,
for example, simply close the browser connection, making it more likely
that the user (or their admin) blames the proxy? Or should Squid bump
the browser connection to explain what has happened, creating all the
headaches associated with bumping.

Your Squid configuration should reflect all these key decisions.

If Squid does not have enough configuration options or code to do
exactly what you want, then you (or others) can always add more
code/options. If your use case is common/general enough, then quality
implementations of those additional features should be officially accepted.


Alex.


From rousskov at measurement-factory.com  Fri Sep 14 15:44:52 2018
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Fri, 14 Sep 2018 09:44:52 -0600
Subject: [squid-users] SQUID does not insert server ip and port in logs
 for CONNECT method when the connection fails (error 503)
In-Reply-To: <53843021b2bf4dc3b21463fc8707d02a@ocgepvsw3101.ocr.priv>
References: <53843021b2bf4dc3b21463fc8707d02a@ocgepvsw3101.ocr.priv>
Message-ID: <f5a7c4ec-7581-87a1-35b2-87b9de23f9cf@measurement-factory.com>

On 09/14/2018 04:33 AM, Troiano Alessio wrote:

> In HTTPS connections when the destination server does not answer
> (maybe blocked by our firewall because it is malicious) the
> destination ip is not logged.

If Squid tried to contact the server, then you are right -- there is a
Squid bug here. If you can reproduce this bug with the latest Squid v4
(or, at the very least, with the latest Squid v3.5), please consider
filing a bug report with Squid bugzilla.


> Squid Cache: Version 3.5.20
> Red Hat Enterprise Linux Server release 7.4 (Maipo)
> 
> We are using the last stable release of squid for Red Hat.

Please note that you are not using the last stable release supported by
the Squid Project. Red Hat support may differ, of course, but you are
not posting this email to a Red Hat support mailing list...

Alex.


From alessio.troiano at leonardocompany.com  Fri Sep 14 16:06:32 2018
From: alessio.troiano at leonardocompany.com (Troiano Alessio)
Date: Fri, 14 Sep 2018 16:06:32 +0000
Subject: [squid-users] R: SQUID does not insert server ip and port in logs
 for CONNECT method when the connection fails (error 503)
In-Reply-To: <1838_1536939899_5B9BD77B_1838_5255_1_f5a7c4ec-7581-87a1-35b2-87b9de23f9cf@measurement-factory.com>
References: <53843021b2bf4dc3b21463fc8707d02a@ocgepvsw3101.ocr.priv>
 <1838_1536939899_5B9BD77B_1838_5255_1_f5a7c4ec-7581-87a1-35b2-87b9de23f9cf@measurement-factory.com>
Message-ID: <f06544ca994244e0ac35ea3bdf52faac@ocgepvsw3101.ocr.priv>

Ok, so reverting the question: can you reproduce with the latest version the same error? I cannot update squid, but you can test the link provided https://sqm.telemetry.microsoft.com to check if your squid fill the server IP field or not.



Il presente messaggio e-mail e ogni suo allegato devono intendersi indirizzati esclusivamente al destinatario indicato e considerarsi dal contenuto strettamente riservato e confidenziale. Se non siete l'effettivo destinatario o avete ricevuto il messaggio e-mail per errore, siete pregati di avvertire immediatamente il mittente e di cancellare il suddetto messaggio e ogni suo allegato dal vostro sistema informatico. Qualsiasi utilizzo, diffusione, copia o archiviazione del presente messaggio da parte di chi non ne ? il destinatario ? strettamente proibito e pu? dar luogo a responsabilit? di carattere civile e penale punibili ai sensi di legge.
Questa e-mail ha valore legale solo se firmata digitalmente ai sensi della normativa vigente.

The contents of this email message and any attachments are intended solely for the addressee(s) and contain confidential and/or privileged information.
If you are not the intended recipient of this message, or if this message has been addressed to you in error, please immediately notify the sender and then delete this message and any attachments from your system. If you are not the intended recipient, you are hereby notified that any use, dissemination, copying, or storage of this message or its attachments is strictly prohibited. Unauthorized disclosure and/or use of information contained in this email message may result in civil and criminal liability. ?
This e-mail has legal value according to the applicable laws only if it is digitally signed by the sender
-----Messaggio originale-----
Da: squid-users [mailto:squid-users-bounces at lists.squid-cache.org] Per conto di Alex Rousskov
Inviato: venerd? 14 settembre 2018 17:45
A: squid-users at lists.squid-cache.org
Oggetto: Re: [squid-users] SQUID does not insert server ip and port in logs for CONNECT method when the connection fails (error 503)

On 09/14/2018 04:33 AM, Troiano Alessio wrote:

> In HTTPS connections when the destination server does not answer
> (maybe blocked by our firewall because it is malicious) the
> destination ip is not logged.

If Squid tried to contact the server, then you are right -- there is a Squid bug here. If you can reproduce this bug with the latest Squid v4 (or, at the very least, with the latest Squid v3.5), please consider filing a bug report with Squid bugzilla.


> Squid Cache: Version 3.5.20
> Red Hat Enterprise Linux Server release 7.4 (Maipo)
>
> We are using the last stable release of squid for Red Hat.

Please note that you are not using the last stable release supported by the Squid Project. Red Hat support may differ, of course, but you are not posting this email to a Red Hat support mailing list...

Alex.
_______________________________________________
squid-users mailing list
squid-users at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-users

From rousskov at measurement-factory.com  Fri Sep 14 16:33:10 2018
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Fri, 14 Sep 2018 10:33:10 -0600
Subject: [squid-users] R: SQUID does not insert server ip and port in
 logs for CONNECT method when the connection fails (error 503)
In-Reply-To: <f06544ca994244e0ac35ea3bdf52faac@ocgepvsw3101.ocr.priv>
References: <53843021b2bf4dc3b21463fc8707d02a@ocgepvsw3101.ocr.priv>
 <1838_1536939899_5B9BD77B_1838_5255_1_f5a7c4ec-7581-87a1-35b2-87b9de23f9cf@measurement-factory.com>
 <f06544ca994244e0ac35ea3bdf52faac@ocgepvsw3101.ocr.priv>
Message-ID: <d03d95e1-9437-4800-979a-856d33363cbf@measurement-factory.com>

On 09/14/2018 10:06 AM, Troiano Alessio wrote:
> Ok, so reverting the question: can you reproduce with the latest version the same error?

Unfortunately, I do not have the free time required to do this testing
right now. Please note that you do not need to upgrade your existing
Squid installation to perform this testing.

Alex.


> This e-mail has legal value according to the applicable laws only if it is digitally signed by the sender
> -----Messaggio originale-----
> Da: squid-users [mailto:squid-users-bounces at lists.squid-cache.org] Per conto di Alex Rousskov
> Inviato: venerd? 14 settembre 2018 17:45
> A: squid-users at lists.squid-cache.org
> Oggetto: Re: [squid-users] SQUID does not insert server ip and port in logs for CONNECT method when the connection fails (error 503)
> 
> On 09/14/2018 04:33 AM, Troiano Alessio wrote:
> 
>> In HTTPS connections when the destination server does not answer
>> (maybe blocked by our firewall because it is malicious) the
>> destination ip is not logged.
> 
> If Squid tried to contact the server, then you are right -- there is a Squid bug here. If you can reproduce this bug with the latest Squid v4 (or, at the very least, with the latest Squid v3.5), please consider filing a bug report with Squid bugzilla.
> 
> 
>> Squid Cache: Version 3.5.20
>> Red Hat Enterprise Linux Server release 7.4 (Maipo)
>>
>> We are using the last stable release of squid for Red Hat.
> 
> Please note that you are not using the last stable release supported by the Squid Project. Red Hat support may differ, of course, but you are not posting this email to a Red Hat support mailing list...
> 
> Alex.
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
> 



From alessio.troiano at leonardocompany.com  Fri Sep 14 16:52:46 2018
From: alessio.troiano at leonardocompany.com (Troiano Alessio)
Date: Fri, 14 Sep 2018 16:52:46 +0000
Subject: [squid-users] R: R: SQUID does not insert server ip and port in
 logs for CONNECT method when the connection fails (error 503)
In-Reply-To: <13694_1536942799_5B9BE2CF_13694_7270_1_d03d95e1-9437-4800-979a-856d33363cbf@measurement-factory.com>
References: <53843021b2bf4dc3b21463fc8707d02a@ocgepvsw3101.ocr.priv>
 <1838_1536939899_5B9BD77B_1838_5255_1_f5a7c4ec-7581-87a1-35b2-87b9de23f9cf@measurement-factory.com>
 <f06544ca994244e0ac35ea3bdf52faac@ocgepvsw3101.ocr.priv>
 <13694_1536942799_5B9BE2CF_13694_7270_1_d03d95e1-9437-4800-979a-856d33363cbf@measurement-factory.com>
Message-ID: <0307d11e49414b0295537220b1834d34@ocgepvsw3101.ocr.priv>

I don't understand what I need to do... I already did my tests and see the problem. But I see that last 3.5 squid version is 3.5.28 so I can't reproduce the issue in that version without upgrade squid.
Anyway can you provide to a me a guide to report a bugzilla?


Il presente messaggio e-mail e ogni suo allegato devono intendersi indirizzati esclusivamente al destinatario indicato e considerarsi dal contenuto strettamente riservato e confidenziale. Se non siete l'effettivo destinatario o avete ricevuto il messaggio e-mail per errore, siete pregati di avvertire immediatamente il mittente e di cancellare il suddetto messaggio e ogni suo allegato dal vostro sistema informatico. Qualsiasi utilizzo, diffusione, copia o archiviazione del presente messaggio da parte di chi non ne ? il destinatario ? strettamente proibito e pu? dar luogo a responsabilit? di carattere civile e penale punibili ai sensi di legge.
Questa e-mail ha valore legale solo se firmata digitalmente ai sensi della normativa vigente.

The contents of this email message and any attachments are intended solely for the addressee(s) and contain confidential and/or privileged information.
If you are not the intended recipient of this message, or if this message has been addressed to you in error, please immediately notify the sender and then delete this message and any attachments from your system. If you are not the intended recipient, you are hereby notified that any use, dissemination, copying, or storage of this message or its attachments is strictly prohibited. Unauthorized disclosure and/or use of information contained in this email message may result in civil and criminal liability. ?
This e-mail has legal value according to the applicable laws only if it is digitally signed by the sender
-----Messaggio originale-----
Da: squid-users [mailto:squid-users-bounces at lists.squid-cache.org] Per conto di Alex Rousskov
Inviato: venerd? 14 settembre 2018 18:33
A: squid-users at lists.squid-cache.org
Oggetto: Re: [squid-users] R: SQUID does not insert server ip and port in logs for CONNECT method when the connection fails (error 503)

On 09/14/2018 10:06 AM, Troiano Alessio wrote:
> Ok, so reverting the question: can you reproduce with the latest version the same error?

Unfortunately, I do not have the free time required to do this testing right now. Please note that you do not need to upgrade your existing Squid installation to perform this testing.

Alex.


> This e-mail has legal value according to the applicable laws only if
> it is digitally signed by the sender -----Messaggio originale-----
> Da: squid-users [mailto:squid-users-bounces at lists.squid-cache.org] Per
> conto di Alex Rousskov
> Inviato: venerd? 14 settembre 2018 17:45
> A: squid-users at lists.squid-cache.org
> Oggetto: Re: [squid-users] SQUID does not insert server ip and port in
> logs for CONNECT method when the connection fails (error 503)
>
> On 09/14/2018 04:33 AM, Troiano Alessio wrote:
>
>> In HTTPS connections when the destination server does not answer
>> (maybe blocked by our firewall because it is malicious) the
>> destination ip is not logged.
>
> If Squid tried to contact the server, then you are right -- there is a Squid bug here. If you can reproduce this bug with the latest Squid v4 (or, at the very least, with the latest Squid v3.5), please consider filing a bug report with Squid bugzilla.
>
>
>> Squid Cache: Version 3.5.20
>> Red Hat Enterprise Linux Server release 7.4 (Maipo)
>>
>> We are using the last stable release of squid for Red Hat.
>
> Please note that you are not using the last stable release supported by the Squid Project. Red Hat support may differ, of course, but you are not posting this email to a Red Hat support mailing list...
>
> Alex.
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>

_______________________________________________
squid-users mailing list
squid-users at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-users

From Antony.Stone at squid.open.source.it  Fri Sep 14 17:21:45 2018
From: Antony.Stone at squid.open.source.it (Antony Stone)
Date: Fri, 14 Sep 2018 19:21:45 +0200
Subject: [squid-users] R: R: SQUID does not insert server ip and port in
	logs for CONNECT method when the connection fails (error 503)
In-Reply-To: <0307d11e49414b0295537220b1834d34@ocgepvsw3101.ocr.priv>
References: <53843021b2bf4dc3b21463fc8707d02a@ocgepvsw3101.ocr.priv>
 <13694_1536942799_5B9BE2CF_13694_7270_1_d03d95e1-9437-4800-979a-856d33363cbf@measurement-factory.com>
 <0307d11e49414b0295537220b1834d34@ocgepvsw3101.ocr.priv>
Message-ID: <201809141921.45687.Antony.Stone@squid.open.source.it>

On Friday 14 September 2018 at 18:52:46, Troiano Alessio wrote:

> I don't understand what I need to do... I already did my tests and see the
> problem.

Yes, but to file a bug we need to know whether it's still in the current code 
(it may already have been fixed).

> But I see that last 3.5 squid version is 3.5.28 so I can't reproduce the
> issue in that version without upgrade squid.

Install current Squid on a VM?


Antony.

-- 
The difference between theory and practice is that in theory there is no 
difference, whereas in practice there is.

                                                   Please reply to the list;
                                                         please *don't* CC me.


From alessio.troiano at leonardocompany.com  Fri Sep 14 17:26:24 2018
From: alessio.troiano at leonardocompany.com (Troiano Alessio)
Date: Fri, 14 Sep 2018 17:26:24 +0000
Subject: [squid-users] R: R: R: SQUID does not insert server ip and port
	in	logs for CONNECT method when the connection fails (error 503)
In-Reply-To: <1454_1536945721_5B9BEE39_1454_8131_1_201809141921.45687.Antony.Stone@squid.open.source.it>
References: <53843021b2bf4dc3b21463fc8707d02a@ocgepvsw3101.ocr.priv>
 <13694_1536942799_5B9BE2CF_13694_7270_1_d03d95e1-9437-4800-979a-856d33363cbf@measurement-factory.com>
 <0307d11e49414b0295537220b1834d34@ocgepvsw3101.ocr.priv>
 <1454_1536945721_5B9BEE39_1454_8131_1_201809141921.45687.Antony.Stone@squid.open.source.it>
Message-ID: <f260178671964c0aa8770b086661e5e8@ocgepvsw3101.ocr.priv>

I opened a bug.
Anyway I think that anyone in this list can check the problem in his squid in very simple way going to https://sqm.telemetry.microsoft.com and looking the logs (if you have %<a fields that is enabled by default in logformat squid)


Il presente messaggio e-mail e ogni suo allegato devono intendersi indirizzati esclusivamente al destinatario indicato e considerarsi dal contenuto strettamente riservato e confidenziale. Se non siete l'effettivo destinatario o avete ricevuto il messaggio e-mail per errore, siete pregati di avvertire immediatamente il mittente e di cancellare il suddetto messaggio e ogni suo allegato dal vostro sistema informatico. Qualsiasi utilizzo, diffusione, copia o archiviazione del presente messaggio da parte di chi non ne ? il destinatario ? strettamente proibito e pu? dar luogo a responsabilit? di carattere civile e penale punibili ai sensi di legge.
Questa e-mail ha valore legale solo se firmata digitalmente ai sensi della normativa vigente.

The contents of this email message and any attachments are intended solely for the addressee(s) and contain confidential and/or privileged information.
If you are not the intended recipient of this message, or if this message has been addressed to you in error, please immediately notify the sender and then delete this message and any attachments from your system. If you are not the intended recipient, you are hereby notified that any use, dissemination, copying, or storage of this message or its attachments is strictly prohibited. Unauthorized disclosure and/or use of information contained in this email message may result in civil and criminal liability. ?
This e-mail has legal value according to the applicable laws only if it is digitally signed by the sender
-----Messaggio originale-----
Da: squid-users [mailto:squid-users-bounces at lists.squid-cache.org] Per conto di Antony Stone
Inviato: venerd? 14 settembre 2018 19:22
A: squid-users at lists.squid-cache.org
Oggetto: Re: [squid-users] R: R: SQUID does not insert server ip and port in logs for CONNECT method when the connection fails (error 503)

On Friday 14 September 2018 at 18:52:46, Troiano Alessio wrote:

> I don't understand what I need to do... I already did my tests and see
> the problem.

Yes, but to file a bug we need to know whether it's still in the current code (it may already have been fixed).

> But I see that last 3.5 squid version is 3.5.28 so I can't reproduce
> the issue in that version without upgrade squid.

Install current Squid on a VM?


Antony.

--
The difference between theory and practice is that in theory there is no difference, whereas in practice there is.

                                                   Please reply to the list;
                                                         please *don't* CC me.
_______________________________________________
squid-users mailing list
squid-users at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-users

From johnrefwe at mail.com  Fri Sep 14 17:49:38 2018
From: johnrefwe at mail.com (John Refwe)
Date: Fri, 14 Sep 2018 19:49:38 +0200
Subject: [squid-users] Squid https_port
Message-ID: <trinity-e4a320b8-f425-470b-bfba-a48cdde3f7fd-1536947378658@3c-app-mailcom-lxa08>

An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20180914/eafa1adc/attachment.htm>

From yanier at eleccav.une.cu  Fri Sep 14 17:56:34 2018
From: yanier at eleccav.une.cu (Yanier Salazar Sanchez)
Date: Fri, 14 Sep 2018 13:56:34 -0400
Subject: [squid-users] Problem with kerb/ntlm authentication
Message-ID: <00a001d44c54$4804b960$d80e2c20$@eleccav.une.cu>

Sorry for my bad english.

 

This is the scenario

 

I have ubuntu 18.04.01 (with las update) with squid 4.2-2, samba and winbind
4.7.6,  AD on Windows Server 2012 R2/2016 with the las update, Client with
windows 10 1709 with the las update, firefox 60.2.0esr, google chrome
61.0.3163.79, firefox quantum 62.0 and internet explorer

 

I using this guide
https://blog.it-kb.ru/2014/06/16/forward-proxy-squid-3-3-on-ubuntu-server-14
-04-lts-part-1-install-os-on-hyper-v-generation-2-vm/ (Only to where
kerberos and NTLM are configured)

 

I joined the proxy to the active directory

 

All the commands seem to work correctly

 

I run this command

klist

Ticket cache: FILE:/tmp/krb5cc_0

Default principal: HTTP//srv-squid-krb.mired.lan at MIRED.LAN
<mailto:HTTP//srv-squid-krb.mired.lan at MIRED.LAN> 

Valid starting                 Expires    Service principal

09/13/2018 16:29:48    09/14/2018 02:29:48 krbtgt/MIRED.LAN at MIRED.LAN

09/13/2018 16:55:57    09/14/2018 02:29:48
host/srv-squid-krb.mired.lan at MIRED.LAN
<mailto:host/srv-squid-krb.mired.lan at MIRED.LAN> 

09/13/2018 16:56:13    09/14/2018 02:29:48 host/srv-dc.mired.lan at MIRED.LAN
<mailto:host/srv-dc.mired.lan at MIRED.LAN> 

 

I run this command

kinit squidtest

password for squidtest at MIRED.LAN <mailto:squidtest at MIRED.LAN> :

 

I create a proxy.keytab in my windows server 2012 r2 with this command 

ktpass -princ HTTP/srv-squid-krb.mired.lan at MIRED.LAN  -mapuser
MIRED\squidtest -pass password -crypto All -ptype KRB5_NT_PRINCIPAL -out
d:\proxy.keytab

proxy.keytab permission

rw-r-r root proxy proxy.keytab

 

 

My krb5.conf file

 

[libdefaults]

        default_realm = MIRED.LAN

        dns_lookup_kdc = yes

        dns_lookup_kdc = no

        ticket_lifetime = 24h

        default_keytab_name = /etc/squid/proxy.keytab

[realms]

        MIRED.LAN = {

                    kdc = srv-dc.mired.lan

                    admin_server = srv-dc.mired.lan

                    default_domain = mired.lan

}

[domain_relam]

        mired.lan = MIRED.LAN

       .mired.lan = MIRED.LAN

 

 

 

 

I run this command

klist -k /etc/squid/proxy.keytab

Keytab name: FILE/etc/squid/proxy.keytab

KVNO Principal

6      HTTP/srv-squid-krb.mired.lan at MIRED.LAN
<mailto:HTTP/srv-squid-krb.mired.lan at MIRED.LAN> 

6      HTTP/srv-squid-krb.mired.lan at MIRED.LAN
<mailto:HTTP/srv-squid-krb.mired.lan at MIRED.LAN> 

6      HTTP/srv-squid-krb.mired.lan at MIRED.LAN
<mailto:HTTP/srv-squid-krb.mired.lan at MIRED.LAN> 

6      HTTP/srv-squid-krb.mired.lan at MIRED.LAN
<mailto:HTTP/srv-squid-krb.mired.lan at MIRED.LAN> 

6      HTTP/srv-squid-krb.mired.lan at MIRED.LAN
<mailto:HTTP/srv-squid-krb.mired.lan at MIRED.LAN> 

 

I run this command

wbinfo -authenticate=squidtest%mypassword

Plaintest password athentication succeded

Challenge/response password authentication succeded

 

I run this command

wbinfo -krb5auth=squidtest%mypassword

Plaintest kerberos password athentication for [squidtest:mypassword]
succeded (requesting cctype: FILE) credential were put in; FILE/tmp/krbcc_0

 

I run this command

wbinfo -g  (List all groups in AD)

I run this command

wbinfo -u  (List all users in AD)

 

I run this command

/usr/lib/squid/negotiate_kerberos_auth_test srv-squid-krb.mired.lan

Token: YIICSAYGRKw... blabla   /B8VWAxn29WaG/j

 

 

The squid.conf it's basic configuration only with 

 

auth_program negotiate program /usr/lib/squid/negotiate_wrapper_auth -d
-ntlm /usr/bin/ntlm_auth -diagnostics -helper-protocol=2.5-ntlmssp
-domain=mired -kerberos /usr/lib/squid/negotiate_kerberos_auth -d -r -s
HTTP//srv-squid-krb.mired.lan at mired.lan
<mailto:HTTP//srv-squid-krb.mired.lan at mired.lan> 

auth_program negotiate children 10

auth_program negotiate keep_alive off

 

auth_param ntlm program /usr/bin/ntlm_auth --diagnostics
-helper-protocol=squid-2.5-ntlmssp 

auth_param ntlm children 10

auth_param ntlm keep_alive off

 

acl red src 192.168.0.0/24

acl auth proxy_auth REQUIRED

 

and 

http_access allow red auth

 

 

But the problem is that Kerberos don't work. Only NTLM.

cache.log

2018/09/14 06:25:02| negotiate_wrapper: Starting version 1.0.1

2018/09/14 06:25:02| negotiate_wrapper: NTLM command: /usr/bin/ntlm_auth
--diagnostics --helper-protocol=squid-2.5-ntlmssp 

2018/09/14 06:25:02| negotiate_wrapper: Kerberos command:
/usr/lib/squid/negotiate_kerberos_auth -d -r -s
HTTP/srv-squid-krb.mired.lan at MIRED.LAN 

negotiate_kerberos_auth.cc(487): pid=10816 :2018/09/14 06:25:02|
negotiate_kerberos_auth: INFO: Starting version 3.1.0sq

negotiate_kerberos_auth.cc(546): pid=10816 :2018/09/14 06:25:02|
negotiate_kerberos_auth: INFO: Setting keytab to /etc/squid/proxy.keytab

negotiate_kerberos_auth.cc(570): pid=10816 :2018/09/14 06:25:02|
negotiate_kerberos_auth: INFO: Changed keytab to
MEMORY:negotiate_kerberos_auth_10816

2018/09/14 13:39:18| negotiate_wrapper: Return 'TT
TlRMTVNTUAACAAAADgAOADgAAAAVgonigQb5TAh6RigAAAAAAAAAAJwAnABGAAAABgEAAAAAAA9F
AEwARQBDAEMAQQBWAAIADgBFAEwARQBDAEMAQQBWAAEAGgBTAFIAVgAtAFMAUQBVAEkARAAtAEsA
UgBCAAQAHABlAGwAZQBjAGMAYQB2AC4AdQBuAGUALgBjAHUAAwA4AHMAcgB2AC0AcwBxAHUAaQBk
AC0AawByAGIALgBlAGwAZQBjAGMAYQB2AC4AdQBuAGUALgBjAHUABwAIALqX2txRTNQBAAAAAA==

'

2018/09/14 13:39:18.197 kid1| 29,4| UserRequest.cc(311) HandleReply: Need to
challenge the client with a server token:
'TlRMTVNTUAACAAAADgAOADgAAAAVgonigQb5TAh6RigAAAAAAAAAAJwAnABGAAAABgEAAAAAAA9
FAEwARQBDAEMAQQBWAAIADgBFAEwARQBDAEMAQQBWAAEAGgBTAFIAVgAtAFMAUQBVAEkARAAtAEs
AUgBCAAQAHABlAGwAZQBjAGMAYQB2AC4AdQBuAGUALgBjAHUAAwA4AHMAcgB2AC0AcwBxAHUAaQB
kAC0AawByAGIALgBlAGwAZQBjAGMAYQB2AC4AdQBuAGUALgBjAHUABwAIALqX2txRTNQBAAAAAA=
='

2018/09/14 13:39:18.197 kid1| 29,2| UserRequest.cc(203) authenticate: need
to challenge client
'TlRMTVNTUAACAAAADgAOADgAAAAVgonigQb5TAh6RigAAAAAAAAAAJwAnABGAAAABgEAAAAAAA9
FAEwARQBDAEMAQQBWAAIADgBFAEwARQBDAEMAQQBWAAEAGgBTAFIAVgAtAFMAUQBVAEkARAAtAEs
AUgBCAAQAHABlAGwAZQBjAGMAYQB2AC4AdQBuAGUALgBjAHUAAwA4AHMAcgB2AC0AcwBxAHUAaQB
kAC0AawByAGIALgBlAGwAZQBjAGMAYQB2AC4AdQBuAGUALgBjAHUABwAIALqX2txRTNQBAAAAAA=
='!

2018/09/14 13:39:18| negotiate_wrapper: Return 'TT
TlRMTVNTUAACAAAADgAOADgAAAAVgonig5b0rgxfAqwAAAAAAAAAAJwAnABGAAAABgEAAAAAAA9F
AEwARQBDAEMAQQBWAAIADgBFAEwARQBDAEMAQQBWAAEAGgBTAFIAVgAtAFMAUQBVAEkARAAtAEsA
UgBCAAQAHABlAGwAZQBjAGMAYQB2AC4AdQBuAGUALgBjAHUAAwA4AHMAcgB2AC0AcwBxAHUAaQBk
AC0AawByAGIALgBlAGwAZQBjAGMAYQB2AC4AdQBuAGUALgBjAHUABwAIAD5e29xRTNQBAAAAAA==

'

2018/09/14 13:39:18.202 kid1| 29,4| UserRequest.cc(311) HandleReply: Need to
challenge the client with a server token:
'TlRMTVNTUAACAAAADgAOADgAAAAVgonig5b0rgxfAqwAAAAAAAAAAJwAnABGAAAABgEAAAAAAA9
FAEwARQBDAEMAQQBWAAIADgBFAEwARQBDAEMAQQBWAAEAGgBTAFIAVgAtAFMAUQBVAEkARAAtAEs
AUgBCAAQAHABlAGwAZQBjAGMAYQB2AC4AdQBuAGUALgBjAHUAAwA4AHMAcgB2AC0AcwBxAHUAaQB
kAC0AawByAGIALgBlAGwAZQBjAGMAYQB2AC4AdQBuAGUALgBjAHUABwAIAD5e29xRTNQBAAAAAA=
='

2018/09/14 13:39:18.202 kid1| 29,2| UserRequest.cc(203) authenticate: need
to challenge client
'TlRMTVNTUAACAAAADgAOADgAAAAVgonig5b0rgxfAqwAAAAAAAAAAJwAnABGAAAABgEAAAAAAA9
FAEwARQBDAEMAQQBWAAIADgBFAEwARQBDAEMAQQBWAAEAGgBTAFIAVgAtAFMAUQBVAEkARAAtAEs
AUgBCAAQAHABlAGwAZQBjAGMAYQB2AC4AdQBuAGUALgBjAHUAAwA4AHMAcgB2AC0AcwBxAHUAaQB
kAC0AawByAGIALgBlAGwAZQBjAGMAYQB2AC4AdQBuAGUALgBjAHUABwAIAD5e29xRTNQBAAAAAA=
='!

2018/09/14 13:39:18| negotiate_wrapper: Return 'TT
TlRMTVNTUAACAAAADgAOADgAAAAVgoni/IpqOarkGm0AAAAAAAAAAJwAnABGAAAABgEAAAAAAA9F
AEwARQBDAEMAQQBWAAIADgBFAEwARQBDAEMAQQBWAAEAGgBTAFIAVgAtAFMAUQBVAEkARAAtAEsA
UgBCAAQAHABlAGwAZQBjAGMAYQB2AC4AdQBuAGUALgBjAHUAAwA4AHMAcgB2AC0AcwBxAHUAaQBk
AC0AawByAGIALgBlAGwAZQBjAGMAYQB2AC4AdQBuAGUALgBjAHUABwAIAF7Y3NxRTNQBAAAAAA==

'

2018/09/14 13:39:18.212 kid1| 29,4| UserRequest.cc(311) HandleReply: Need to
challenge the client with a server token:
'TlRMTVNTUAACAAAADgAOADgAAAAVgoni/IpqOarkGm0AAAAAAAAAAJwAnABGAAAABgEAAAAAAA9
FAEwARQBDAEMAQQBWAAIADgBFAEwARQBDAEMAQQBWAAEAGgBTAFIAVgAtAFMAUQBVAEkARAAtAEs
AUgBCAAQAHABlAGwAZQBjAGMAYQB2AC4AdQBuAGUALgBjAHUAAwA4AHMAcgB2AC0AcwBxAHUAaQB
kAC0AawByAGIALgBlAGwAZQBjAGMAYQB2AC4AdQBuAGUALgBjAHUABwAIAF7Y3NxRTNQBAAAAAA=
='

2018/09/14 13:39:18.212 kid1| 29,2| UserRequest.cc(203) authenticate: need
to challenge client
'TlRMTVNTUAACAAAADgAOADgAAAAVgoni/IpqOarkGm0AAAAAAAAAAJwAnABGAAAABgEAAAAAAA9
FAEwARQBDAEMAQQBWAAIADgBFAEwARQBDAEMAQQBWAAEAGgBTAFIAVgAtAFMAUQBVAEkARAAtAEs
AUgBCAAQAHABlAGwAZQBjAGMAYQB2AC4AdQBuAGUALgBjAHUAAwA4AHMAcgB2AC0AcwBxAHUAaQB
kAC0AawByAGIALgBlAGwAZQBjAGMAYQB2AC4AdQBuAGUALgBjAHUABwAIAF7Y3NxRTNQBAAAAAA=
='!

2018/09/14 13:39:18| negotiate_wrapper: Return 'TT
TlRMTVNTUAACAAAADgAOADgAAAAVgoniE4M9MFIcoxQAAAAAAAAAAJwAnABGAAAABgEAAAAAAA9F
AEwARQBDAEMAQQBWAAIADgBFAEwARQBDAEMAQQBWAAEAGgBTAFIAVgAtAFMAUQBVAEkARAAtAEsA
UgBCAAQAHABlAGwAZQBjAGMAYQB2AC4AdQBuAGUALgBjAHUAAwA4AHMAcgB2AC0AcwBxAHUAaQBk
AC0AawByAGIALgBlAGwAZQBjAGMAYQB2AC4AdQBuAGUALgBjAHUABwAIAIoG3dxRTNQBAAAAAA==

'

2018/09/14 13:39:18.213 kid1| 29,4| UserRequest.cc(311) HandleReply: Need to
challenge the client with a server token:
'TlRMTVNTUAACAAAADgAOADgAAAAVgoniE4M9MFIcoxQAAAAAAAAAAJwAnABGAAAABgEAAAAAAA9
FAEwARQBDAEMAQQBWAAIADgBFAEwARQBDAEMAQQBWAAEAGgBTAFIAVgAtAFMAUQBVAEkARAAtAEs
AUgBCAAQAHABlAGwAZQBjAGMAYQB2AC4AdQBuAGUALgBjAHUAAwA4AHMAcgB2AC0AcwBxAHUAaQB
kAC0AawByAGIALgBlAGwAZQBjAGMAYQB2AC4AdQBuAGUALgBjAHUABwAIAIoG3dxRTNQBAAAAAA=
='

2018/09/14 13:39:18.213 kid1| 29,2| UserRequest.cc(203) authenticate: need
to challenge client
'TlRMTVNTUAACAAAADgAOADgAAAAVgoniE4M9MFIcoxQAAAAAAAAAAJwAnABGAAAABgEAAAAAAA9
FAEwARQBDAEMAQQBWAAIADgBFAEwARQBDAEMAQQBWAAEAGgBTAFIAVgAtAFMAUQBVAEkARAAtAEs
AUgBCAAQAHABlAGwAZQBjAGMAYQB2AC4AdQBuAGUALgBjAHUAAwA4AHMAcgB2AC0AcwBxAHUAaQB
kAC0AawByAGIALgBlAGwAZQBjAGMAYQB2AC4AdQBuAGUALgBjAHUABwAIAIoG3dxRTNQBAAAAAA=
='!

2018/09/14 13:39:18| negotiate_wrapper: Return 'TT
TlRMTVNTUAACAAAADgAOADgAAAAVgoniyKjYPBGi9DAAAAAAAAAAAJwAnABGAAAABgEAAAAAAA9F
AEwARQBDAEMAQQBWAAIADgBFAEwARQBDAEMAQQBWAAEAGgBTAFIAVgAtAFMAUQBVAEkARAAtAEsA
UgBCAAQAHABlAGwAZQBjAGMAYQB2AC4AdQBuAGUALgBjAHUAAwA4AHMAcgB2AC0AcwBxAHUAaQBk
AC0AawByAGIALgBlAGwAZQBjAGMAYQB2AC4AdQBuAGUALgBjAHUABwAIADpT4NxRTNQBAAAAAA==

'

2018/09/14 13:39:18.234 kid1| 29,4| UserRequest.cc(311) HandleReply: Need to
challenge the client with a server token:
'TlRMTVNTUAACAAAADgAOADgAAAAVgoniyKjYPBGi9DAAAAAAAAAAAJwAnABGAAAABgEAAAAAAA9
FAEwARQBDAEMAQQBWAAIADgBFAEwARQBDAEMAQQBWAAEAGgBTAFIAVgAtAFMAUQBVAEkARAAtAEs
AUgBCAAQAHABlAGwAZQBjAGMAYQB2AC4AdQBuAGUALgBjAHUAAwA4AHMAcgB2AC0AcwBxAHUAaQB
kAC0AawByAGIALgBlAGwAZQBjAGMAYQB2AC4AdQBuAGUALgBjAHUABwAIADpT4NxRTNQBAAAAAA=
='

2018/09/14 13:39:18.235 kid1| 29,2| UserRequest.cc(203) authenticate: need
to challenge client
'TlRMTVNTUAACAAAADgAOADgAAAAVgoniyKjYPBGi9DAAAAAAAAAAAJwAnABGAAAABgEAAAAAAA9
FAEwARQBDAEMAQQBWAAIADgBFAEwARQBDAEMAQQBWAAEAGgBTAFIAVgAtAFMAUQBVAEkARAAtAEs
AUgBCAAQAHABlAGwAZQBjAGMAYQB2AC4AdQBuAGUALgBjAHUAAwA4AHMAcgB2AC0AcwBxAHUAaQB
kAC0AawByAGIALgBlAGwAZQBjAGMAYQB2AC4AdQBuAGUALgBjAHUABwAIADpT4NxRTNQBAAAAAA=
='!

2018/09/14 13:39:18| negotiate_wrapper: Got 'KK
TlRMTVNTUAADAAAAGAAYAIwAAABIAUgBpAAAAA4ADgBYAAAAEAAQAGYAAAAWABYAdgAAABAAEADs
AQAAFYKI4goAqz8AAAAPTClxtLRmctSfR7SLfnA2O0UATABFAEMAQwBBAFYAYwByAHkAcwB0AGEA
bABsAEMATABJAC0AUgBFAEQARQBTAC0AMQAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAD6IIRx8JJe
sRgbGAVoQxrwAQEAAAAAAABe2NzcUUzUATZJB+HrulrgAAAAAAIADgBFAEwARQBDAEMAQQBWAAEA
GgBTAFIAVgAtAFMAUQBVAEkARAAtAEsAUgBCAAQAHABlAGwAZQBjAGMAYQB2AC4AdQBuAGUALgBj
AHUAAwA4AHMAcgB2AC0AcwBxAHUAaQBkAC0AawByAGIALgBlAGwAZQBjAGMAYQB2AC4AdQBuAGUA
LgBjAHUABwAIAF7Y3NxRTNQBBgAEAAIAAAAIADAAMAAAAAAAAAAAAAAAADAAANSO8haD472JTKKO
F8vBYpu8Z0WdTYbu7c7tqLmb/9ooCgAQAAAAAAAAAAAAAAAAAAAAAAAJACQASABUAFQAUAAvADEA
NwAyAC4AMQA5AC4AMgAyADQALgA0ADYAAAAAAAAAAAAAAAAAyh1ssj6oWg4B+eQjlqv2aA=='
from squid (length: 683).

2018/09/14 13:39:18| negotiate_wrapper: Decode
'TlRMTVNTUAADAAAAGAAYAIwAAABIAUgBpAAAAA4ADgBYAAAAEAAQAGYAAAAWABYAdgAAABAAEAD
sAQAAFYKI4goAqz8AAAAPTClxtLRmctSfR7SLfnA2O0UATABFAEMAQwBBAFYAYwByAHkAcwB0AGE
AbABsAEMATABJAC0AUgBFAEQARQBTAC0AMQAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAD6IIRx8JJ
esRgbGAVoQxrwAQEAAAAAAABe2NzcUUzUATZJB+HrulrgAAAAAAIADgBFAEwARQBDAEMAQQBWAAE
AGgBTAFIAVgAtAFMAUQBVAEkARAAtAEsAUgBCAAQAHABlAGwAZQBjAGMAYQB2AC4AdQBuAGUALgB
jAHUAAwA4AHMAcgB2AC0AcwBxAHUAaQBkAC0AawByAGIALgBlAGwAZQBjAGMAYQB2AC4AdQBuAGU
ALgBjAHUABwAIAF7Y3NxRTNQBBgAEAAIAAAAIADAAMAAAAAAAAAAAAAAAADAAANSO8haD472JTKK
OF8vBYpu8Z0WdTYbu7c7tqLmb/9ooCgAQAAAAAAAAAAAAAAAAAAAAAAAJACQASABUAFQAUAAvADE
ANwAyAC4AMQA5AC4AMgAyADQALgA0ADYAAAAAAAAAAAAAAAAAyh1ssj6oWg4B+eQjlqv2aA=='
(decoded length: 510).

2018/09/14 13:39:18| negotiate_wrapper: received type 3 NTLM token

2018/09/14 13:39:18| negotiate_wrapper: Return 'AF = crystall

'

2018/09/14 13:39:18.297 kid1| 29,4| UserRequest.cc(336) HandleReply:
authenticated user crystall

2018/09/14 13:39:18.297 kid1| 29,4| UserRequest.cc(355) HandleReply:
Successfully validated user via Negotiate. Username 'crystall'

2018/09/14 13:39:18.297 kid1| 29,2| User.cc(227) addIp: user 'crystall' has
been seen at a new IP address (192.168.0.2:53116)

2018/09/14 13:39:18| negotiate_wrapper: Got 'KK
TlRMTVNTUAADAAAAGAAYAIwAAABIAUgBpAAAAA4ADgBYAAAAEAAQAGYAAAAWABYAdgAAABAAEADs
AQAAFYKI4goAqz8AAAAPNdmk9QQok2ZtAJi07Aft0EUATABFAEMAQwBBAFYAYwByAHkAcwB0AGEA
bABsAEMATABJAC0AUgBFAEQARQBTAC0AMQAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAIdVVkN6Ul
IPyfEZl+tFbZAQEAAAAAAAA6U+DcUUzUAbDPblk1F39AAAAAAAIADgBFAEwARQBDAEMAQQBWAAEA
GgBTAFIAVgAtAFMAUQBVAEkARAAtAEsAUgBCAAQAHABlAGwAZQBjAGMAYQB2AC4AdQBuAGUALgBj
AHUAAwA4AHMAcgB2AC0AcwBxAHUAaQBkAC0AawByAGIALgBlAGwAZQBjAGMAYQB2AC4AdQBuAGUA
LgBjAHUABwAIADpT4NxRTNQBBgAEAAIAAAAIADAAMAAAAAAAAAAAAAAAADAAANSO8haD472JTKKO
F8vBYpu8Z0WdTYbu7c7tqLmb/9ooCgAQAAAAAAAAAAAAAAAAAAAAAAAJACQASABUAFQAUAAvADEA
NwAyAC4AMQA5AC4AMgAyADQALgA0ADYAAAAAAAAAAAAAAAAASlVupH80E90xsICozM0MDw=='
from squid (length: 683).

2018/09/14 13:39:18| negotiate_wrapper: Decode
'TlRMTVNTUAADAAAAGAAYAIwAAABIAUgBpAAAAA4ADgBYAAAAEAAQAGYAAAAWABYAdgAAABAAEAD
sAQAAFYKI4goAqz8AAAAPNdmk9QQok2ZtAJi07Aft0EUATABFAEMAQwBBAFYAYwByAHkAcwB0AGE
AbABsAEMATABJAC0AUgBFAEQARQBTAC0AMQAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAIdVVkN6U
lIPyfEZl+tFbZAQEAAAAAAAA6U+DcUUzUAbDPblk1F39AAAAAAAIADgBFAEwARQBDAEMAQQBWAAE
AGgBTAFIAVgAtAFMAUQBVAEkARAAtAEsAUgBCAAQAHABlAGwAZQBjAGMAYQB2AC4AdQBuAGUALgB
jAHUAAwA4AHMAcgB2AC0AcwBxAHUAaQBkAC0AawByAGIALgBlAGwAZQBjAGMAYQB2AC4AdQBuAGU
ALgBjAHUABwAIADpT4NxRTNQBBgAEAAIAAAAIADAAMAAAAAAAAAAAAAAAADAAANSO8haD472JTKK
OF8vBYpu8Z0WdTYbu7c7tqLmb/9ooCgAQAAAAAAAAAAAAAAAAAAAAAAAJACQASABUAFQAUAAvADE
ANwAyAC4AMQA5AC4AMgAyADQALgA0ADYAAAAAAAAAAAAAAAAASlVupH80E90xsICozM0MDw=='
(decoded length: 510).

2018/09/14 13:39:18| negotiate_wrapper: received type 3 NTLM token

2018/09/14 13:39:18| negotiate_wrapper: Got 'KK
TlRMTVNTUAADAAAAGAAYAIwAAABIAUgBpAAAAA4ADgBYAAAAEAAQAGYAAAAWABYAdgAAABAAEADs
AQAAFYKI4goAqz8AAAAPktrvZwcp20ZMz2vUT1MqrEUATABFAEMAQwBBAFYAYwByAHkAcwB0AGEA
bABsAEMATABJAC0AUgBFAEQARQBTAC0AMQAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAC/ktNdTT5c
V4Jw+7d4icVSAQEAAAAAAAA+XtvcUUzUAQvLOUptjrxtAAAAAAIADgBFAEwARQBDAEMAQQBWAAEA
GgBTAFIAVgAtAFMAUQBVAEkARAAtAEsAUgBCAAQAHABlAGwAZQBjAGMAYQB2AC4AdQBuAGUALgBj
AHUAAwA4AHMAcgB2AC0AcwBxAHUAaQBkAC0AawByAGIALgBlAGwAZQBjAGMAYQB2AC4AdQBuAGUA
LgBjAHUABwAIAD5e29xRTNQBBgAEAAIAAAAIADAAMAAAAAAAAAAAAAAAADAAANSO8haD472JTKKO
F8vBYpu8Z0WdTYbu7c7tqLmb/9ooCgAQAAAAAAAAAAAAAAAAAAAAAAAJACQASABUAFQAUAAvADEA
NwAyAC4AMQA5AC4AMgAyADQALgA0ADYAAAAAAAAAAAAAAAAAC9hZLo1JeXWlUlkutHco2Q=='
from squid (length: 683).

2018/09/14 13:39:18| negotiate_wrapper: Decode
'TlRMTVNTUAADAAAAGAAYAIwAAABIAUgBpAAAAA4ADgBYAAAAEAAQAGYAAAAWABYAdgAAABAAEAD
sAQAAFYKI4goAqz8AAAAPktrvZwcp20ZMz2vUT1MqrEUATABFAEMAQwBBAFYAYwByAHkAcwB0AGE
AbABsAEMATABJAC0AUgBFAEQARQBTAC0AMQAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAC/ktNdTT5
cV4Jw+7d4icVSAQEAAAAAAAA+XtvcUUzUAQvLOUptjrxtAAAAAAIADgBFAEwARQBDAEMAQQBWAAE
AGgBTAFIAVgAtAFMAUQBVAEkARAAtAEsAUgBCAAQAHABlAGwAZQBjAGMAYQB2AC4AdQBuAGUALgB
jAHUAAwA4AHMAcgB2AC0AcwBxAHUAaQBkAC0AawByAGIALgBlAGwAZQBjAGMAYQB2AC4AdQBuAGU
ALgBjAHUABwAIAD5e29xRTNQBBgAEAAIAAAAIADAAMAAAAAAAAAAAAAAAADAAANSO8haD472JTKK
OF8vBYpu8Z0WdTYbu7c7tqLmb/9ooCgAQAAAAAAAAAAAAAAAAAAAAAAAJACQASABUAFQAUAAvADE
ANwAyAC4AMQA5AC4AMgAyADQALgA0ADYAAAAAAAAAAAAAAAAAC9hZLo1JeXWlUlkutHco2Q=='
(decoded length: 510).

2018/09/14 13:39:18| negotiate_wrapper: received type 3 NTLM token

2018/09/14 13:39:18| negotiate_wrapper: Got 'KK
TlRMTVNTUAADAAAAGAAYAIwAAABIAUgBpAAAAA4ADgBYAAAAEAAQAGYAAAAWABYAdgAAABAAEADs
AQAAFYKI4goAqz8AAAAPcvqx2ByNCA8nHjzEmlCuRkUATABFAEMAQwBBAFYAYwByAHkAcwB0AGEA
bABsAEMATABJAC0AUgBFAEQARQBTAC0AMQAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAV9wKDCHbA
PUCj0iTVPM9cAQEAAAAAAACKBt3cUUzUAQ3p911SxBpmAAAAAAIADgBFAEwARQBDAEMAQQBWAAEA
GgBTAFIAVgAtAFMAUQBVAEkARAAtAEsAUgBCAAQAHABlAGwAZQBjAGMAYQB2AC4AdQBuAGUALgBj
AHUAAwA4AHMAcgB2AC0AcwBxAHUAaQBkAC0AawByAGIALgBlAGwAZQBjAGMAYQB2AC4AdQBuAGUA
LgBjAHUABwAIAIoG3dxRTNQBBgAEAAIAAAAIADAAMAAAAAAAAAAAAAAAADAAANSO8haD472JTKKO
F8vBYpu8Z0WdTYbu7c7tqLmb/9ooCgAQAAAAAAAAAAAAAAAAAAAAAAAJACQASABUAFQAUAAvADEA
NwAyAC4AMQA5AC4AMgAyADQALgA0ADYAAAAAAAAAAAAAAAAAWt0nSXk+Ix4DbAvzOrubNA=='
from squid (length: 683).

2018/09/14 13:39:18| negotiate_wrapper: Decode
'TlRMTVNTUAADAAAAGAAYAIwAAABIAUgBpAAAAA4ADgBYAAAAEAAQAGYAAAAWABYAdgAAABAAEAD
sAQAAFYKI4goAqz8AAAAPcvqx2ByNCA8nHjzEmlCuRkUATABFAEMAQwBBAFYAYwByAHkAcwB0AGE
AbABsAEMATABJAC0AUgBFAEQARQBTAC0AMQAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAV9wKDCHb
APUCj0iTVPM9cAQEAAAAAAACKBt3cUUzUAQ3p911SxBpmAAAAAAIADgBFAEwARQBDAEMAQQBWAAE
AGgBTAFIAVgAtAFMAUQBVAEkARAAtAEsAUgBCAAQAHABlAGwAZQBjAGMAYQB2AC4AdQBuAGUALgB
jAHUAAwA4AHMAcgB2AC0AcwBxAHUAaQBkAC0AawByAGIALgBlAGwAZQBjAGMAYQB2AC4AdQBuAGU
ALgBjAHUABwAIAIoG3dxRTNQBBgAEAAIAAAAIADAAMAAAAAAAAAAAAAAAADAAANSO8haD472JTKK
OF8vBYpu8Z0WdTYbu7c7tqLmb/9ooCgAQAAAAAAAAAAAAAAAAAAAAAAAJACQASABUAFQAUAAvADE
ANwAyAC4AMQA5AC4AMgAyADQALgA0ADYAAAAAAAAAAAAAAAAAWt0nSXk+Ix4DbAvzOrubNA=='
(decoded length: 510).

2018/09/14 13:39:18| negotiate_wrapper: received type 3 NTLM token

2018/09/14 13:39:18| negotiate_wrapper: Got 'KK
TlRMTVNTUAADAAAAGAAYAIwAAABIAUgBpAAAAA4ADgBYAAAAEAAQAGYAAAAWABYAdgAAABAAEADs
AQAAFYKI4goAqz8AAAAPsug30D9/WWwwJJE0C5LgOUUATABFAEMAQwBBAFYAYwByAHkAcwB0AGEA
bABsAEMATABJAC0AUgBFAEQARQBTAC0AMQAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAABB9ekE7/+
TbqkYU6Gx64qAQEAAAAAAAC6l9rcUUzUAS71gyhoa7SHAAAAAAIADgBFAEwARQBDAEMAQQBWAAEA
GgBTAFIAVgAtAFMAUQBVAEkARAAtAEsAUgBCAAQAHABlAGwAZQBjAGMAYQB2AC4AdQBuAGUALgBj
AHUAAwA4AHMAcgB2AC0AcwBxAHUAaQBkAC0AawByAGIALgBlAGwAZQBjAGMAYQB2AC4AdQBuAGUA
LgBjAHUABwAIALqX2txRTNQBBgAEAAIAAAAIADAAMAAAAAAAAAAAAAAAADAAANSO8haD472JTKKO
F8vBYpu8Z0WdTYbu7c7tqLmb/9ooCgAQAAAAAAAAAAAAAAAAAAAAAAAJACQASABUAFQAUAAvADEA
NwAyAC4AMQA5AC4AMgAyADQALgA0ADYAAAAAAAAAAAAAAAAA/6MII/C9uGEWH4s9EE+W/g=='
from squid (length: 683).

2018/09/14 13:39:18| negotiate_wrapper: Decode
'TlRMTVNTUAADAAAAGAAYAIwAAABIAUgBpAAAAA4ADgBYAAAAEAAQAGYAAAAWABYAdgAAABAAEAD
sAQAAFYKI4goAqz8AAAAPsug30D9/WWwwJJE0C5LgOUUATABFAEMAQwBBAFYAYwByAHkAcwB0AGE
AbABsAEMATABJAC0AUgBFAEQARQBTAC0AMQAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAABB9ekE7/
+TbqkYU6Gx64qAQEAAAAAAAC6l9rcUUzUAS71gyhoa7SHAAAAAAIADgBFAEwARQBDAEMAQQBWAAE
AGgBTAFIAVgAtAFMAUQBVAEkARAAtAEsAUgBCAAQAHABlAGwAZQBjAGMAYQB2AC4AdQBuAGUALgB
jAHUAAwA4AHMAcgB2AC0AcwBxAHUAaQBkAC0AawByAGIALgBlAGwAZQBjAGMAYQB2AC4AdQBuAGU
ALgBjAHUABwAIALqX2txRTNQBBgAEAAIAAAAIADAAMAAAAAAAAAAAAAAAADAAANSO8haD472JTKK
OF8vBYpu8Z0WdTYbu7c7tqLmb/9ooCgAQAAAAAAAAAAAAAAAAAAAAAAAJACQASABUAFQAUAAvADE
ANwAyAC4AMQA5AC4AMgAyADQALgA0ADYAAAAAAAAAAAAAAAAA/6MII/C9uGEWH4s9EE+W/g=='
(decoded length: 510).

2018/09/14 13:39:18| negotiate_wrapper: received type 3 NTLM token

2018/09/14 13:39:18| negotiate_wrapper: Return 'AF = crystall

'

2018/09/14 13:39:18.326 kid1| 29,4| UserRequest.cc(336) HandleReply:
authenticated user crystall

2018/09/14 13:39:18.326 kid1| 29,4| UserRequest.cc(355) HandleReply:
Successfully validated user via Negotiate. Username 'crystall'

2018/09/14 13:39:18| negotiate_wrapper: Return 'AF = crystall

'

2018/09/14 13:39:18.331 kid1| 29,4| UserRequest.cc(336) HandleReply:
authenticated user crystall

2018/09/14 13:39:18.331 kid1| 29,4| UserRequest.cc(355) HandleReply:
Successfully validated user via Negotiate. Username 'crystall'

2018/09/14 13:39:18| negotiate_wrapper: Return 'AF = crystall

'

2018/09/14 13:39:18.335 kid1| 29,4| UserRequest.cc(336) HandleReply:
authenticated user crystall

2018/09/14 13:39:18.335 kid1| 29,4| UserRequest.cc(355) HandleReply:
Successfully validated user via Negotiate. Username 'crystall'

2018/09/14 13:39:18| negotiate_wrapper: Return 'AF = crystall

'

2018/09/14 13:39:18.340 kid1| 29,4| UserRequest.cc(336) HandleReply:
authenticated user crystall

2018/09/14 13:39:18.340 kid1| 29,4| UserRequest.cc(355) HandleReply:
Successfully validated user via Negotiate. Username 'crystall'

2018/09/14 13:39:18.340 kid1| ICP is disabled! Cannot send ICP request to
peer.

2018/09/14 13:39:18.340 kid1| ICP is disabled! Cannot send ICP request to
peer.

2018/09/14 13:39:18.779 kid1| 29,4| UserRequest.cc(294) authenticate: No
Proxy-Auth header and no working alternative. Requesting auth header.

2018/09/14 13:39:18.782 kid1| 29,4| UserRequest.cc(354) authenticate: No
connection authentication type

2018/09/14 13:39:18| negotiate_wrapper: Got 'YR
TlRMTVNTUAABAAAAl4II4gAAAAAAAAAAAAAAAAAAAAAKAKs/AAAADw==' from squid
(length: 59).

2018/09/14 13:39:18| negotiate_wrapper: Decode
'TlRMTVNTUAABAAAAl4II4gAAAAAAAAAAAAAAAAAAAAAKAKs/AAAADw==' (decoded length:
42).

2018/09/14 13:39:18| negotiate_wrapper: received type 1 NTLM token

2018/09/14 13:39:18| negotiate_wrapper: Return 'TT
TlRMTVNTUAACAAAADgAOADgAAAAVgoni6re6l3Xwbr4AAAAAAAAAAJwAnABGAAAABgEAAAAAAA9F
AEwARQBDAEMAQQBWAAIADgBFAEwARQBDAEMAQQBWAAEAGgBTAFIAVgAtAFMAUQBVAEkARAAtAEsA
UgBCAAQAHABlAGwAZQBjAGMAYQB2AC4AdQBuAGUALgBjAHUAAwA4AHMAcgB2AC0AcwBxAHUAaQBk
AC0AawByAGIALgBlAGwAZQBjAGMAYQB2AC4AdQBuAGUALgBjAHUABwAIANJNNN1RTNQBAAAAAA==

'

2018/09/14 13:39:18.785 kid1| 29,4| UserRequest.cc(311) HandleReply: Need to
challenge the client with a server token:
'TlRMTVNTUAACAAAADgAOADgAAAAVgoni6re6l3Xwbr4AAAAAAAAAAJwAnABGAAAABgEAAAAAAA9
FAEwARQBDAEMAQQBWAAIADgBFAEwARQBDAEMAQQBWAAEAGgBTAFIAVgAtAFMAUQBVAEkARAAtAEs
AUgBCAAQAHABlAGwAZQBjAGMAYQB2AC4AdQBuAGUALgBjAHUAAwA4AHMAcgB2AC0AcwBxAHUAaQB
kAC0AawByAGIALgBlAGwAZQBjAGMAYQB2AC4AdQBuAGUALgBjAHUABwAIANJNNN1RTNQBAAAAAA=
='

2018/09/14 13:39:18.785 kid1| 29,2| UserRequest.cc(203) authenticate: need
to challenge client
'TlRMTVNTUAACAAAADgAOADgAAAAVgoni6re6l3Xwbr4AAAAAAAAAAJwAnABGAAAABgEAAAAAAA9
FAEwARQBDAEMAQQBWAAIADgBFAEwARQBDAEMAQQBWAAEAGgBTAFIAVgAtAFMAUQBVAEkARAAtAEs
AUgBCAAQAHABlAGwAZQBjAGMAYQB2AC4AdQBuAGUALgBjAHUAAwA4AHMAcgB2AC0AcwBxAHUAaQB
kAC0AawByAGIALgBlAGwAZQBjAGMAYQB2AC4AdQBuAGUALgBjAHUABwAIANJNNN1RTNQBAAAAAA=
='!

2018/09/14 13:39:18| negotiate_wrapper: Got 'KK
TlRMTVNTUAADAAAAGAAYAIwAAABIAUgBpAAAAA4ADgBYAAAAEAAQAGYAAAAWABYAdgAAABAAEADs
AQAAFYKI4goAqz8AAAAPZToO29GZi9mTSaZo7kC+uEUATABFAEMAQwBBAFYAYwByAHkAcwB0AGEA
bABsAEMATABJAC0AUgBFAEQARQBTAC0AMQAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAADMCXZljnEc
JGfczvMrEXsbAQEAAAAAAADSTTTdUUzUATkI4mevwzXdAAAAAAIADgBFAEwARQBDAEMAQQBWAAEA
GgBTAFIAVgAtAFMAUQBVAEkARAAtAEsAUgBCAAQAHABlAGwAZQBjAGMAYQB2AC4AdQBuAGUALgBj
AHUAAwA4AHMAcgB2AC0AcwBxAHUAaQBkAC0AawByAGIALgBlAGwAZQBjAGMAYQB2AC4AdQBuAGUA
LgBjAHUABwAIANJNNN1RTNQBBgAEAAIAAAAIADAAMAAAAAAAAAAAAAAAADAAANSO8haD472JTKKO
F8vBYpu8Z0WdTYbu7c7tqLmb/9ooCgAQAAAAAAAAAAAAAAAAAAAAAAAJACQASABUAFQAUAAvADEA
NwAyAC4AMQA5AC4AMgAyADQALgA0ADYAAAAAAAAAAAAAAAAAxTDFbTI2R1oQS5sjProTRQ=='
from squid (length: 683).

2018/09/14 13:39:18| negotiate_wrapper: Decode
'TlRMTVNTUAADAAAAGAAYAIwAAABIAUgBpAAAAA4ADgBYAAAAEAAQAGYAAAAWABYAdgAAABAAEAD
sAQAAFYKI4goAqz8AAAAPZToO29GZi9mTSaZo7kC+uEUATABFAEMAQwBBAFYAYwByAHkAcwB0AGE
AbABsAEMATABJAC0AUgBFAEQARQBTAC0AMQAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAADMCXZljnE
cJGfczvMrEXsbAQEAAAAAAADSTTTdUUzUATkI4mevwzXdAAAAAAIADgBFAEwARQBDAEMAQQBWAAE
AGgBTAFIAVgAtAFMAUQBVAEkARAAtAEsAUgBCAAQAHABlAGwAZQBjAGMAYQB2AC4AdQBuAGUALgB
jAHUAAwA4AHMAcgB2AC0AcwBxAHUAaQBkAC0AawByAGIALgBlAGwAZQBjAGMAYQB2AC4AdQBuAGU
ALgBjAHUABwAIANJNNN1RTNQBBgAEAAIAAAAIADAAMAAAAAAAAAAAAAAAADAAANSO8haD472JTKK
OF8vBYpu8Z0WdTYbu7c7tqLmb/9ooCgAQAAAAAAAAAAAAAAAAAAAAAAAJACQASABUAFQAUAAvADE
ANwAyAC4AMQA5AC4AMgAyADQALgA0ADYAAAAAAAAAAAAAAAAAxTDFbTI2R1oQS5sjProTRQ=='
(decoded length: 510).

2018/09/14 13:39:18| negotiate_wrapper: received type 3 NTLM token

2018/09/14 13:39:18| negotiate_wrapper: Return 'AF = crystall

 

 

Access.log

1536946843.113  66541 192.168.0.2 TCP_TUNNEL/200 3806 CONNECT
www.facebook.com:443 crystall FIRSTUP_PARENT/PARENT_PROXY_IP

 

The question is, that only NTLM works, I've tried with Internet Explorer,
Google Chrome and Firefox. The other thing is that he never asks for
username and password, he uses the user credentials that he initiates
session to work (I do not know if this is the correct operation).

What could be happening?

 

 

                Sorry for the long email.

 

 

Gretting Yanier

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20180914/4abc7b04/attachment.htm>

From johnrefwe at mail.com  Fri Sep 14 18:11:15 2018
From: johnrefwe at mail.com (John Refwe)
Date: Fri, 14 Sep 2018 20:11:15 +0200
Subject: [squid-users] Squid https_port
Message-ID: <trinity-0e32b813-1745-4a5b-b3c8-159c8e2a9069-1536948675515@3c-app-mailcom-lxa08>

Hi (sorry resending this because the original sent as an html email),
?
I have a couple of questions about the squid https_port.
?
1) Does it only exist for transparent connections? I know if I want to have a transparent proxy that can accept requests TLS requests, I need to have the port be a https_port rather than a http_port, but is that what it was created for?
?
2) How come the https_port does not support receiving proxy protocol? Perhaps I'm misunderstanding a bit here, but I thought?that HAProxy supports sending it before instantiating a TLS connection?
?
Thank you so much for the help and I'm sorry if I'm misunderstanding and these questions don't make sense.


From yanier at eleccav.une.cu  Fri Sep 14 18:51:51 2018
From: yanier at eleccav.une.cu (Yanier Salazar Sanchez)
Date: Fri, 14 Sep 2018 14:51:51 -0400
Subject: [squid-users] Problem with kerb/ntlm authentication
Message-ID: <00b801d44c5c$00282480$00786d80$@eleccav.une.cu>

Sorry for my bad english.

 

This is the scenario

 

I have ubuntu 18.04.01 (with las update) with squid 4.2-2, samba and winbind
4.7.6,  AD on Windows Server 2012 R2/2016 with the las update, Client with
windows 10 1709 with the las update, firefox 60.2.0esr, google chrome
61.0.3163.79, firefox quantum 62.0 and internet explorer

 

I using this guide
https://blog.it-kb.ru/2014/06/16/forward-proxy-squid-3-3-on-ubuntu-server-14
-04-lts-part-1-install-os-on-hyper-v-generation-2-vm/ (Only to where
kerberos and NTLM are configured)

 

I joined the proxy to the active directory

 

All the commands seem to work correctly

 

I run this command

klist

Ticket cache: FILE:/tmp/krb5cc_0

Default principal: HTTP//srv-squid-krb.mired.lan at MIRED.LAN
<mailto:HTTP//srv-squid-krb.mired.lan at MIRED.LAN> 

Valid starting                 Expires    Service principal

09/13/2018 16:29:48    09/14/2018 02:29:48 krbtgt/MIRED.LAN at MIRED.LAN
<mailto:krbtgt/MIRED.LAN at MIRED.LAN> 

09/13/2018 16:55:57    09/14/2018 02:29:48
host/srv-squid-krb.mired.lan at MIRED.LAN
<mailto:host/srv-squid-krb.mired.lan at MIRED.LAN> 

09/13/2018 16:56:13    09/14/2018 02:29:48 host/srv-dc.mired.lan at MIRED.LAN
<mailto:host/srv-dc.mired.lan at MIRED.LAN> 

 

I run this command

kinit squidtest

password for squidtest at MIRED.LAN <mailto:squidtest at MIRED.LAN> :

 

I create a proxy.keytab in my windows server 2012 r2 with this command 

ktpass -princ HTTP/srv-squid-krb.mired.lan at MIRED.LAN
<mailto:HTTP/srv-squid-krb.mired.lan at MIRED.LAN>   -mapuser MIRED\squidtest
-pass password -crypto All -ptype KRB5_NT_PRINCIPAL -out d:\proxy.keytab

proxy.keytab permission

rw-r-r root proxy proxy.keytab

 

 

My krb5.conf file

 

[libdefaults]

        default_realm = MIRED.LAN

        dns_lookup_kdc = yes

        dns_lookup_kdc = no

        ticket_lifetime = 24h

        default_keytab_name = /etc/squid/proxy.keytab

[realms]

        MIRED.LAN = {

                    kdc = srv-dc.mired.lan

                    admin_server = srv-dc.mired.lan

                    default_domain = mired.lan

}

[domain_relam]

        mired.lan = MIRED.LAN

       .mired.lan = MIRED.LAN

 

 

 

 

I run this command

klist -k /etc/squid/proxy.keytab

Keytab name: FILE/etc/squid/proxy.keytab

KVNO Principal

6      HTTP/srv-squid-krb.mired.lan at MIRED.LAN
<mailto:HTTP/srv-squid-krb.mired.lan at MIRED.LAN> 

6      HTTP/srv-squid-krb.mired.lan at MIRED.LAN
<mailto:HTTP/srv-squid-krb.mired.lan at MIRED.LAN> 

6      HTTP/srv-squid-krb.mired.lan at MIRED.LAN
<mailto:HTTP/srv-squid-krb.mired.lan at MIRED.LAN> 

6      HTTP/srv-squid-krb.mired.lan at MIRED.LAN
<mailto:HTTP/srv-squid-krb.mired.lan at MIRED.LAN> 

6      HTTP/srv-squid-krb.mired.lan at MIRED.LAN
<mailto:HTTP/srv-squid-krb.mired.lan at MIRED.LAN> 

 

I run this command

wbinfo -authenticate=squidtest%mypassword

Plaintest password athentication succeded

Challenge/response password authentication succeded

 

I run this command

wbinfo -krb5auth=squidtest%mypassword

Plaintest kerberos password athentication for [squidtest:mypassword]
succeded (requesting cctype: FILE) credential were put in; FILE/tmp/krbcc_0

 

I run this command

wbinfo -g  (List all groups in AD)

I run this command

wbinfo -u  (List all users in AD)

 

I run this command

/usr/lib/squid/negotiate_kerberos_auth_test srv-squid-krb.mired.lan

Token: YIICSAYGRKw... blabla   /B8VWAxn29WaG/j

 

 

The squid.conf it's basic configuration only with 

 

auth_program negotiate program /usr/lib/squid/negotiate_wrapper_auth -d
-ntlm /usr/bin/ntlm_auth -diagnostics -helper-protocol=2.5-ntlmssp
-domain=mired -kerberos /usr/lib/squid/negotiate_kerberos_auth -d -r -s
HTTP//srv-squid-krb.mired.lan at mired.lan
<mailto:HTTP//srv-squid-krb.mired.lan at mired.lan> 

auth_program negotiate children 10

auth_program negotiate keep_alive off

 

this lines are comment because in cache.log show the folloing messages
cache.log

username must be specified! 

Usage: [OPTION]

          --helper-protocol=

#auth_param ntlm program /usr/bin/ntlm_auth --diagnostics
-helper-protocol=squid-2.5-ntlmssp 

#auth_param ntlm children 10

#auth_param ntlm keep_alive off

 

acl red src 192.168.0.0/24

acl auth proxy_auth REQUIRED

 

and 

http_access allow red auth

 

if I run this commando on console

/usr/bin/ntlm_auth -help-protocol=squid-2.5-basic -username=user
-password=password

NT_STATUS_OK: The operation completed successfully (0x0)

/usr/bin/ntlm_auth -help-protocol=squid-2.5-ntlmssp -username=user
-password=password

NT_STATUS_OK: The operation completed successfully (0x0)

But if I run /usr/bin/ntlm_auth -help-protocol=squid-2.5-ntlmssp

The answer is username must be specified

 

 

But the problem is that Kerberos don't work. Only NTLM.

cache.log

2018/09/14 06:25:02| negotiate_wrapper: Starting version 1.0.1

2018/09/14 06:25:02| negotiate_wrapper: NTLM command: /usr/bin/ntlm_auth
--diagnostics --helper-protocol=squid-2.5-ntlmssp 

2018/09/14 06:25:02| negotiate_wrapper: Kerberos command:
/usr/lib/squid/negotiate_kerberos_auth -d -r -s
HTTP/srv-squid-krb.mired.lan at MIRED.LAN
<mailto:HTTP/srv-squid-krb.mired.lan at MIRED.LAN>  

negotiate_kerberos_auth.cc(487): pid=10816 :2018/09/14 06:25:02|
negotiate_kerberos_auth: INFO: Starting version 3.1.0sq

negotiate_kerberos_auth.cc(546): pid=10816 :2018/09/14 06:25:02|
negotiate_kerberos_auth: INFO: Setting keytab to /etc/squid/proxy.keytab

negotiate_kerberos_auth.cc(570): pid=10816 :2018/09/14 06:25:02|
negotiate_kerberos_auth: INFO: Changed keytab to
MEMORY:negotiate_kerberos_auth_10816

2018/09/14 13:39:18| negotiate_wrapper: Return 'TT
TlRMTVNTUAACAAAADgAOADgAAAAVgonigQb5TAh6RigAAAAAAAAAAJwAnABGAAAABgEAAAAAAA9F
AEwARQBDAEMAQQBWAAIADgBFAEwARQBDAEMAQQBWAAEAGgBTAFIAVgAtAFMAUQBVAEkARAAtAEsA
UgBCAAQAHABlAGwAZQBjAGMAYQB2AC4AdQBuAGUALgBjAHUAAwA4AHMAcgB2AC0AcwBxAHUAaQBk
AC0AawByAGIALgBlAGwAZQBjAGMAYQB2AC4AdQBuAGUALgBjAHUABwAIALqX2txRTNQBAAAAAA==

'

2018/09/14 13:39:18.197 kid1| 29,4| UserRequest.cc(311) HandleReply: Need to
challenge the client with a server token:
'TlRMTVNTUAACAAAADgAOADgAAAAVgonigQb5TAh6RigAAAAAAAAAAJwAnABGAAAABgEAAAAAAA9
FAEwARQBDAEMAQQBWAAIADgBFAEwARQBDAEMAQQBWAAEAGgBTAFIAVgAtAFMAUQBVAEkARAAtAEs
AUgBCAAQAHABlAGwAZQBjAGMAYQB2AC4AdQBuAGUALgBjAHUAAwA4AHMAcgB2AC0AcwBxAHUAaQB
kAC0AawByAGIALgBlAGwAZQBjAGMAYQB2AC4AdQBuAGUALgBjAHUABwAIALqX2txRTNQBAAAAAA=
='

2018/09/14 13:39:18.197 kid1| 29,2| UserRequest.cc(203) authenticate: need
to challenge client
'TlRMTVNTUAACAAAADgAOADgAAAAVgonigQb5TAh6RigAAAAAAAAAAJwAnABGAAAABgEAAAAAAA9
FAEwARQBDAEMAQQBWAAIADgBFAEwARQBDAEMAQQBWAAEAGgBTAFIAVgAtAFMAUQBVAEkARAAtAEs
AUgBCAAQAHABlAGwAZQBjAGMAYQB2AC4AdQBuAGUALgBjAHUAAwA4AHMAcgB2AC0AcwBxAHUAaQB
kAC0AawByAGIALgBlAGwAZQBjAGMAYQB2AC4AdQBuAGUALgBjAHUABwAIALqX2txRTNQBAAAAAA=
='!

2018/09/14 13:39:18| negotiate_wrapper: Return 'TT
TlRMTVNTUAACAAAADgAOADgAAAAVgonig5b0rgxfAqwAAAAAAAAAAJwAnABGAAAABgEAAAAAAA9F
AEwARQBDAEMAQQBWAAIADgBFAEwARQBDAEMAQQBWAAEAGgBTAFIAVgAtAFMAUQBVAEkARAAtAEsA
UgBCAAQAHABlAGwAZQBjAGMAYQB2AC4AdQBuAGUALgBjAHUAAwA4AHMAcgB2AC0AcwBxAHUAaQBk
AC0AawByAGIALgBlAGwAZQBjAGMAYQB2AC4AdQBuAGUALgBjAHUABwAIAD5e29xRTNQBAAAAAA==

'

2018/09/14 13:39:18.202 kid1| 29,4| UserRequest.cc(311) HandleReply: Need to
challenge the client with a server token:
'TlRMTVNTUAACAAAADgAOADgAAAAVgonig5b0rgxfAqwAAAAAAAAAAJwAnABGAAAABgEAAAAAAA9
FAEwARQBDAEMAQQBWAAIADgBFAEwARQBDAEMAQQBWAAEAGgBTAFIAVgAtAFMAUQBVAEkARAAtAEs
AUgBCAAQAHABlAGwAZQBjAGMAYQB2AC4AdQBuAGUALgBjAHUAAwA4AHMAcgB2AC0AcwBxAHUAaQB
kAC0AawByAGIALgBlAGwAZQBjAGMAYQB2AC4AdQBuAGUALgBjAHUABwAIAD5e29xRTNQBAAAAAA=
='

2018/09/14 13:39:18.202 kid1| 29,2| UserRequest.cc(203) authenticate: need
to challenge client
'TlRMTVNTUAACAAAADgAOADgAAAAVgonig5b0rgxfAqwAAAAAAAAAAJwAnABGAAAABgEAAAAAAA9
FAEwARQBDAEMAQQBWAAIADgBFAEwARQBDAEMAQQBWAAEAGgBTAFIAVgAtAFMAUQBVAEkARAAtAEs
AUgBCAAQAHABlAGwAZQBjAGMAYQB2AC4AdQBuAGUALgBjAHUAAwA4AHMAcgB2AC0AcwBxAHUAaQB
kAC0AawByAGIALgBlAGwAZQBjAGMAYQB2AC4AdQBuAGUALgBjAHUABwAIAD5e29xRTNQBAAAAAA=
='!

2018/09/14 13:39:18| negotiate_wrapper: Return 'TT
TlRMTVNTUAACAAAADgAOADgAAAAVgoni/IpqOarkGm0AAAAAAAAAAJwAnABGAAAABgEAAAAAAA9F
AEwARQBDAEMAQQBWAAIADgBFAEwARQBDAEMAQQBWAAEAGgBTAFIAVgAtAFMAUQBVAEkARAAtAEsA
UgBCAAQAHABlAGwAZQBjAGMAYQB2AC4AdQBuAGUALgBjAHUAAwA4AHMAcgB2AC0AcwBxAHUAaQBk
AC0AawByAGIALgBlAGwAZQBjAGMAYQB2AC4AdQBuAGUALgBjAHUABwAIAF7Y3NxRTNQBAAAAAA==

'

2018/09/14 13:39:18.212 kid1| 29,4| UserRequest.cc(311) HandleReply: Need to
challenge the client with a server token:
'TlRMTVNTUAACAAAADgAOADgAAAAVgoni/IpqOarkGm0AAAAAAAAAAJwAnABGAAAABgEAAAAAAA9
FAEwARQBDAEMAQQBWAAIADgBFAEwARQBDAEMAQQBWAAEAGgBTAFIAVgAtAFMAUQBVAEkARAAtAEs
AUgBCAAQAHABlAGwAZQBjAGMAYQB2AC4AdQBuAGUALgBjAHUAAwA4AHMAcgB2AC0AcwBxAHUAaQB
kAC0AawByAGIALgBlAGwAZQBjAGMAYQB2AC4AdQBuAGUALgBjAHUABwAIAF7Y3NxRTNQBAAAAAA=
='

2018/09/14 13:39:18.212 kid1| 29,2| UserRequest.cc(203) authenticate: need
to challenge client
'TlRMTVNTUAACAAAADgAOADgAAAAVgoni/IpqOarkGm0AAAAAAAAAAJwAnABGAAAABgEAAAAAAA9
FAEwARQBDAEMAQQBWAAIADgBFAEwARQBDAEMAQQBWAAEAGgBTAFIAVgAtAFMAUQBVAEkARAAtAEs
AUgBCAAQAHABlAGwAZQBjAGMAYQB2AC4AdQBuAGUALgBjAHUAAwA4AHMAcgB2AC0AcwBxAHUAaQB
kAC0AawByAGIALgBlAGwAZQBjAGMAYQB2AC4AdQBuAGUALgBjAHUABwAIAF7Y3NxRTNQBAAAAAA=
='!

2018/09/14 13:39:18| negotiate_wrapper: Return 'TT
TlRMTVNTUAACAAAADgAOADgAAAAVgoniE4M9MFIcoxQAAAAAAAAAAJwAnABGAAAABgEAAAAAAA9F
AEwARQBDAEMAQQBWAAIADgBFAEwARQBDAEMAQQBWAAEAGgBTAFIAVgAtAFMAUQBVAEkARAAtAEsA
UgBCAAQAHABlAGwAZQBjAGMAYQB2AC4AdQBuAGUALgBjAHUAAwA4AHMAcgB2AC0AcwBxAHUAaQBk
AC0AawByAGIALgBlAGwAZQBjAGMAYQB2AC4AdQBuAGUALgBjAHUABwAIAIoG3dxRTNQBAAAAAA==

'

2018/09/14 13:39:18.213 kid1| 29,4| UserRequest.cc(311) HandleReply: Need to
challenge the client with a server token:
'TlRMTVNTUAACAAAADgAOADgAAAAVgoniE4M9MFIcoxQAAAAAAAAAAJwAnABGAAAABgEAAAAAAA9
FAEwARQBDAEMAQQBWAAIADgBFAEwARQBDAEMAQQBWAAEAGgBTAFIAVgAtAFMAUQBVAEkARAAtAEs
AUgBCAAQAHABlAGwAZQBjAGMAYQB2AC4AdQBuAGUALgBjAHUAAwA4AHMAcgB2AC0AcwBxAHUAaQB
kAC0AawByAGIALgBlAGwAZQBjAGMAYQB2AC4AdQBuAGUALgBjAHUABwAIAIoG3dxRTNQBAAAAAA=
='

2018/09/14 13:39:18.213 kid1| 29,2| UserRequest.cc(203) authenticate: need
to challenge client
'TlRMTVNTUAACAAAADgAOADgAAAAVgoniE4M9MFIcoxQAAAAAAAAAAJwAnABGAAAABgEAAAAAAA9
FAEwARQBDAEMAQQBWAAIADgBFAEwARQBDAEMAQQBWAAEAGgBTAFIAVgAtAFMAUQBVAEkARAAtAEs
AUgBCAAQAHABlAGwAZQBjAGMAYQB2AC4AdQBuAGUALgBjAHUAAwA4AHMAcgB2AC0AcwBxAHUAaQB
kAC0AawByAGIALgBlAGwAZQBjAGMAYQB2AC4AdQBuAGUALgBjAHUABwAIAIoG3dxRTNQBAAAAAA=
='!

2018/09/14 13:39:18| negotiate_wrapper: Return 'TT
TlRMTVNTUAACAAAADgAOADgAAAAVgoniyKjYPBGi9DAAAAAAAAAAAJwAnABGAAAABgEAAAAAAA9F
AEwARQBDAEMAQQBWAAIADgBFAEwARQBDAEMAQQBWAAEAGgBTAFIAVgAtAFMAUQBVAEkARAAtAEsA
UgBCAAQAHABlAGwAZQBjAGMAYQB2AC4AdQBuAGUALgBjAHUAAwA4AHMAcgB2AC0AcwBxAHUAaQBk
AC0AawByAGIALgBlAGwAZQBjAGMAYQB2AC4AdQBuAGUALgBjAHUABwAIADpT4NxRTNQBAAAAAA==

'

2018/09/14 13:39:18.234 kid1| 29,4| UserRequest.cc(311) HandleReply: Need to
challenge the client with a server token:
'TlRMTVNTUAACAAAADgAOADgAAAAVgoniyKjYPBGi9DAAAAAAAAAAAJwAnABGAAAABgEAAAAAAA9
FAEwARQBDAEMAQQBWAAIADgBFAEwARQBDAEMAQQBWAAEAGgBTAFIAVgAtAFMAUQBVAEkARAAtAEs
AUgBCAAQAHABlAGwAZQBjAGMAYQB2AC4AdQBuAGUALgBjAHUAAwA4AHMAcgB2AC0AcwBxAHUAaQB
kAC0AawByAGIALgBlAGwAZQBjAGMAYQB2AC4AdQBuAGUALgBjAHUABwAIADpT4NxRTNQBAAAAAA=
='

2018/09/14 13:39:18.235 kid1| 29,2| UserRequest.cc(203) authenticate: need
to challenge client
'TlRMTVNTUAACAAAADgAOADgAAAAVgoniyKjYPBGi9DAAAAAAAAAAAJwAnABGAAAABgEAAAAAAA9
FAEwARQBDAEMAQQBWAAIADgBFAEwARQBDAEMAQQBWAAEAGgBTAFIAVgAtAFMAUQBVAEkARAAtAEs
AUgBCAAQAHABlAGwAZQBjAGMAYQB2AC4AdQBuAGUALgBjAHUAAwA4AHMAcgB2AC0AcwBxAHUAaQB
kAC0AawByAGIALgBlAGwAZQBjAGMAYQB2AC4AdQBuAGUALgBjAHUABwAIADpT4NxRTNQBAAAAAA=
='!

2018/09/14 13:39:18| negotiate_wrapper: Got 'KK
TlRMTVNTUAADAAAAGAAYAIwAAABIAUgBpAAAAA4ADgBYAAAAEAAQAGYAAAAWABYAdgAAABAAEADs
AQAAFYKI4goAqz8AAAAPTClxtLRmctSfR7SLfnA2O0UATABFAEMAQwBBAFYAYwByAHkAcwB0AGEA
bABsAEMATABJAC0AUgBFAEQARQBTAC0AMQAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAD6IIRx8JJe
sRgbGAVoQxrwAQEAAAAAAABe2NzcUUzUATZJB+HrulrgAAAAAAIADgBFAEwARQBDAEMAQQBWAAEA
GgBTAFIAVgAtAFMAUQBVAEkARAAtAEsAUgBCAAQAHABlAGwAZQBjAGMAYQB2AC4AdQBuAGUALgBj
AHUAAwA4AHMAcgB2AC0AcwBxAHUAaQBkAC0AawByAGIALgBlAGwAZQBjAGMAYQB2AC4AdQBuAGUA
LgBjAHUABwAIAF7Y3NxRTNQBBgAEAAIAAAAIADAAMAAAAAAAAAAAAAAAADAAANSO8haD472JTKKO
F8vBYpu8Z0WdTYbu7c7tqLmb/9ooCgAQAAAAAAAAAAAAAAAAAAAAAAAJACQASABUAFQAUAAvADEA
NwAyAC4AMQA5AC4AMgAyADQALgA0ADYAAAAAAAAAAAAAAAAAyh1ssj6oWg4B+eQjlqv2aA=='
from squid (length: 683).

2018/09/14 13:39:18| negotiate_wrapper: Decode
'TlRMTVNTUAADAAAAGAAYAIwAAABIAUgBpAAAAA4ADgBYAAAAEAAQAGYAAAAWABYAdgAAABAAEAD
sAQAAFYKI4goAqz8AAAAPTClxtLRmctSfR7SLfnA2O0UATABFAEMAQwBBAFYAYwByAHkAcwB0AGE
AbABsAEMATABJAC0AUgBFAEQARQBTAC0AMQAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAD6IIRx8JJ
esRgbGAVoQxrwAQEAAAAAAABe2NzcUUzUATZJB+HrulrgAAAAAAIADgBFAEwARQBDAEMAQQBWAAE
AGgBTAFIAVgAtAFMAUQBVAEkARAAtAEsAUgBCAAQAHABlAGwAZQBjAGMAYQB2AC4AdQBuAGUALgB
jAHUAAwA4AHMAcgB2AC0AcwBxAHUAaQBkAC0AawByAGIALgBlAGwAZQBjAGMAYQB2AC4AdQBuAGU
ALgBjAHUABwAIAF7Y3NxRTNQBBgAEAAIAAAAIADAAMAAAAAAAAAAAAAAAADAAANSO8haD472JTKK
OF8vBYpu8Z0WdTYbu7c7tqLmb/9ooCgAQAAAAAAAAAAAAAAAAAAAAAAAJACQASABUAFQAUAAvADE
ANwAyAC4AMQA5AC4AMgAyADQALgA0ADYAAAAAAAAAAAAAAAAAyh1ssj6oWg4B+eQjlqv2aA=='
(decoded length: 510).

2018/09/14 13:39:18| negotiate_wrapper: received type 3 NTLM token

2018/09/14 13:39:18| negotiate_wrapper: Return 'AF = crystall

'

2018/09/14 13:39:18.297 kid1| 29,4| UserRequest.cc(336) HandleReply:
authenticated user crystall

2018/09/14 13:39:18.297 kid1| 29,4| UserRequest.cc(355) HandleReply:
Successfully validated user via Negotiate. Username 'crystall'

2018/09/14 13:39:18.297 kid1| 29,2| User.cc(227) addIp: user 'crystall' has
been seen at a new IP address (192.168.0.2:53116)

2018/09/14 13:39:18| negotiate_wrapper: Got 'KK
TlRMTVNTUAADAAAAGAAYAIwAAABIAUgBpAAAAA4ADgBYAAAAEAAQAGYAAAAWABYAdgAAABAAEADs
AQAAFYKI4goAqz8AAAAPNdmk9QQok2ZtAJi07Aft0EUATABFAEMAQwBBAFYAYwByAHkAcwB0AGEA
bABsAEMATABJAC0AUgBFAEQARQBTAC0AMQAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAIdVVkN6Ul
IPyfEZl+tFbZAQEAAAAAAAA6U+DcUUzUAbDPblk1F39AAAAAAAIADgBFAEwARQBDAEMAQQBWAAEA
GgBTAFIAVgAtAFMAUQBVAEkARAAtAEsAUgBCAAQAHABlAGwAZQBjAGMAYQB2AC4AdQBuAGUALgBj
AHUAAwA4AHMAcgB2AC0AcwBxAHUAaQBkAC0AawByAGIALgBlAGwAZQBjAGMAYQB2AC4AdQBuAGUA
LgBjAHUABwAIADpT4NxRTNQBBgAEAAIAAAAIADAAMAAAAAAAAAAAAAAAADAAANSO8haD472JTKKO
F8vBYpu8Z0WdTYbu7c7tqLmb/9ooCgAQAAAAAAAAAAAAAAAAAAAAAAAJACQASABUAFQAUAAvADEA
NwAyAC4AMQA5AC4AMgAyADQALgA0ADYAAAAAAAAAAAAAAAAASlVupH80E90xsICozM0MDw=='
from squid (length: 683).

2018/09/14 13:39:18| negotiate_wrapper: Decode
'TlRMTVNTUAADAAAAGAAYAIwAAABIAUgBpAAAAA4ADgBYAAAAEAAQAGYAAAAWABYAdgAAABAAEAD
sAQAAFYKI4goAqz8AAAAPNdmk9QQok2ZtAJi07Aft0EUATABFAEMAQwBBAFYAYwByAHkAcwB0AGE
AbABsAEMATABJAC0AUgBFAEQARQBTAC0AMQAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAIdVVkN6U
lIPyfEZl+tFbZAQEAAAAAAAA6U+DcUUzUAbDPblk1F39AAAAAAAIADgBFAEwARQBDAEMAQQBWAAE
AGgBTAFIAVgAtAFMAUQBVAEkARAAtAEsAUgBCAAQAHABlAGwAZQBjAGMAYQB2AC4AdQBuAGUALgB
jAHUAAwA4AHMAcgB2AC0AcwBxAHUAaQBkAC0AawByAGIALgBlAGwAZQBjAGMAYQB2AC4AdQBuAGU
ALgBjAHUABwAIADpT4NxRTNQBBgAEAAIAAAAIADAAMAAAAAAAAAAAAAAAADAAANSO8haD472JTKK
OF8vBYpu8Z0WdTYbu7c7tqLmb/9ooCgAQAAAAAAAAAAAAAAAAAAAAAAAJACQASABUAFQAUAAvADE
ANwAyAC4AMQA5AC4AMgAyADQALgA0ADYAAAAAAAAAAAAAAAAASlVupH80E90xsICozM0MDw=='
(decoded length: 510).

2018/09/14 13:39:18| negotiate_wrapper: received type 3 NTLM token

2018/09/14 13:39:18| negotiate_wrapper: Got 'KK
TlRMTVNTUAADAAAAGAAYAIwAAABIAUgBpAAAAA4ADgBYAAAAEAAQAGYAAAAWABYAdgAAABAAEADs
AQAAFYKI4goAqz8AAAAPktrvZwcp20ZMz2vUT1MqrEUATABFAEMAQwBBAFYAYwByAHkAcwB0AGEA
bABsAEMATABJAC0AUgBFAEQARQBTAC0AMQAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAC/ktNdTT5c
V4Jw+7d4icVSAQEAAAAAAAA+XtvcUUzUAQvLOUptjrxtAAAAAAIADgBFAEwARQBDAEMAQQBWAAEA
GgBTAFIAVgAtAFMAUQBVAEkARAAtAEsAUgBCAAQAHABlAGwAZQBjAGMAYQB2AC4AdQBuAGUALgBj
AHUAAwA4AHMAcgB2AC0AcwBxAHUAaQBkAC0AawByAGIALgBlAGwAZQBjAGMAYQB2AC4AdQBuAGUA
LgBjAHUABwAIAD5e29xRTNQBBgAEAAIAAAAIADAAMAAAAAAAAAAAAAAAADAAANSO8haD472JTKKO
F8vBYpu8Z0WdTYbu7c7tqLmb/9ooCgAQAAAAAAAAAAAAAAAAAAAAAAAJACQASABUAFQAUAAvADEA
NwAyAC4AMQA5AC4AMgAyADQALgA0ADYAAAAAAAAAAAAAAAAAC9hZLo1JeXWlUlkutHco2Q=='
from squid (length: 683).

2018/09/14 13:39:18| negotiate_wrapper: Decode
'TlRMTVNTUAADAAAAGAAYAIwAAABIAUgBpAAAAA4ADgBYAAAAEAAQAGYAAAAWABYAdgAAABAAEAD
sAQAAFYKI4goAqz8AAAAPktrvZwcp20ZMz2vUT1MqrEUATABFAEMAQwBBAFYAYwByAHkAcwB0AGE
AbABsAEMATABJAC0AUgBFAEQARQBTAC0AMQAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAC/ktNdTT5
cV4Jw+7d4icVSAQEAAAAAAAA+XtvcUUzUAQvLOUptjrxtAAAAAAIADgBFAEwARQBDAEMAQQBWAAE
AGgBTAFIAVgAtAFMAUQBVAEkARAAtAEsAUgBCAAQAHABlAGwAZQBjAGMAYQB2AC4AdQBuAGUALgB
jAHUAAwA4AHMAcgB2AC0AcwBxAHUAaQBkAC0AawByAGIALgBlAGwAZQBjAGMAYQB2AC4AdQBuAGU
ALgBjAHUABwAIAD5e29xRTNQBBgAEAAIAAAAIADAAMAAAAAAAAAAAAAAAADAAANSO8haD472JTKK
OF8vBYpu8Z0WdTYbu7c7tqLmb/9ooCgAQAAAAAAAAAAAAAAAAAAAAAAAJACQASABUAFQAUAAvADE
ANwAyAC4AMQA5AC4AMgAyADQALgA0ADYAAAAAAAAAAAAAAAAAC9hZLo1JeXWlUlkutHco2Q=='
(decoded length: 510).

2018/09/14 13:39:18| negotiate_wrapper: received type 3 NTLM token

2018/09/14 13:39:18| negotiate_wrapper: Got 'KK
TlRMTVNTUAADAAAAGAAYAIwAAABIAUgBpAAAAA4ADgBYAAAAEAAQAGYAAAAWABYAdgAAABAAEADs
AQAAFYKI4goAqz8AAAAPcvqx2ByNCA8nHjzEmlCuRkUATABFAEMAQwBBAFYAYwByAHkAcwB0AGEA
bABsAEMATABJAC0AUgBFAEQARQBTAC0AMQAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAV9wKDCHbA
PUCj0iTVPM9cAQEAAAAAAACKBt3cUUzUAQ3p911SxBpmAAAAAAIADgBFAEwARQBDAEMAQQBWAAEA
GgBTAFIAVgAtAFMAUQBVAEkARAAtAEsAUgBCAAQAHABlAGwAZQBjAGMAYQB2AC4AdQBuAGUALgBj
AHUAAwA4AHMAcgB2AC0AcwBxAHUAaQBkAC0AawByAGIALgBlAGwAZQBjAGMAYQB2AC4AdQBuAGUA
LgBjAHUABwAIAIoG3dxRTNQBBgAEAAIAAAAIADAAMAAAAAAAAAAAAAAAADAAANSO8haD472JTKKO
F8vBYpu8Z0WdTYbu7c7tqLmb/9ooCgAQAAAAAAAAAAAAAAAAAAAAAAAJACQASABUAFQAUAAvADEA
NwAyAC4AMQA5AC4AMgAyADQALgA0ADYAAAAAAAAAAAAAAAAAWt0nSXk+Ix4DbAvzOrubNA=='
from squid (length: 683).

2018/09/14 13:39:18| negotiate_wrapper: Decode
'TlRMTVNTUAADAAAAGAAYAIwAAABIAUgBpAAAAA4ADgBYAAAAEAAQAGYAAAAWABYAdgAAABAAEAD
sAQAAFYKI4goAqz8AAAAPcvqx2ByNCA8nHjzEmlCuRkUATABFAEMAQwBBAFYAYwByAHkAcwB0AGE
AbABsAEMATABJAC0AUgBFAEQARQBTAC0AMQAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAV9wKDCHb
APUCj0iTVPM9cAQEAAAAAAACKBt3cUUzUAQ3p911SxBpmAAAAAAIADgBFAEwARQBDAEMAQQBWAAE
AGgBTAFIAVgAtAFMAUQBVAEkARAAtAEsAUgBCAAQAHABlAGwAZQBjAGMAYQB2AC4AdQBuAGUALgB
jAHUAAwA4AHMAcgB2AC0AcwBxAHUAaQBkAC0AawByAGIALgBlAGwAZQBjAGMAYQB2AC4AdQBuAGU
ALgBjAHUABwAIAIoG3dxRTNQBBgAEAAIAAAAIADAAMAAAAAAAAAAAAAAAADAAANSO8haD472JTKK
OF8vBYpu8Z0WdTYbu7c7tqLmb/9ooCgAQAAAAAAAAAAAAAAAAAAAAAAAJACQASABUAFQAUAAvADE
ANwAyAC4AMQA5AC4AMgAyADQALgA0ADYAAAAAAAAAAAAAAAAAWt0nSXk+Ix4DbAvzOrubNA=='
(decoded length: 510).

2018/09/14 13:39:18| negotiate_wrapper: received type 3 NTLM token

2018/09/14 13:39:18| negotiate_wrapper: Got 'KK
TlRMTVNTUAADAAAAGAAYAIwAAABIAUgBpAAAAA4ADgBYAAAAEAAQAGYAAAAWABYAdgAAABAAEADs
AQAAFYKI4goAqz8AAAAPsug30D9/WWwwJJE0C5LgOUUATABFAEMAQwBBAFYAYwByAHkAcwB0AGEA
bABsAEMATABJAC0AUgBFAEQARQBTAC0AMQAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAABB9ekE7/+
TbqkYU6Gx64qAQEAAAAAAAC6l9rcUUzUAS71gyhoa7SHAAAAAAIADgBFAEwARQBDAEMAQQBWAAEA
GgBTAFIAVgAtAFMAUQBVAEkARAAtAEsAUgBCAAQAHABlAGwAZQBjAGMAYQB2AC4AdQBuAGUALgBj
AHUAAwA4AHMAcgB2AC0AcwBxAHUAaQBkAC0AawByAGIALgBlAGwAZQBjAGMAYQB2AC4AdQBuAGUA
LgBjAHUABwAIALqX2txRTNQBBgAEAAIAAAAIADAAMAAAAAAAAAAAAAAAADAAANSO8haD472JTKKO
F8vBYpu8Z0WdTYbu7c7tqLmb/9ooCgAQAAAAAAAAAAAAAAAAAAAAAAAJACQASABUAFQAUAAvADEA
NwAyAC4AMQA5AC4AMgAyADQALgA0ADYAAAAAAAAAAAAAAAAA/6MII/C9uGEWH4s9EE+W/g=='
from squid (length: 683).

2018/09/14 13:39:18| negotiate_wrapper: Decode
'TlRMTVNTUAADAAAAGAAYAIwAAABIAUgBpAAAAA4ADgBYAAAAEAAQAGYAAAAWABYAdgAAABAAEAD
sAQAAFYKI4goAqz8AAAAPsug30D9/WWwwJJE0C5LgOUUATABFAEMAQwBBAFYAYwByAHkAcwB0AGE
AbABsAEMATABJAC0AUgBFAEQARQBTAC0AMQAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAABB9ekE7/
+TbqkYU6Gx64qAQEAAAAAAAC6l9rcUUzUAS71gyhoa7SHAAAAAAIADgBFAEwARQBDAEMAQQBWAAE
AGgBTAFIAVgAtAFMAUQBVAEkARAAtAEsAUgBCAAQAHABlAGwAZQBjAGMAYQB2AC4AdQBuAGUALgB
jAHUAAwA4AHMAcgB2AC0AcwBxAHUAaQBkAC0AawByAGIALgBlAGwAZQBjAGMAYQB2AC4AdQBuAGU
ALgBjAHUABwAIALqX2txRTNQBBgAEAAIAAAAIADAAMAAAAAAAAAAAAAAAADAAANSO8haD472JTKK
OF8vBYpu8Z0WdTYbu7c7tqLmb/9ooCgAQAAAAAAAAAAAAAAAAAAAAAAAJACQASABUAFQAUAAvADE
ANwAyAC4AMQA5AC4AMgAyADQALgA0ADYAAAAAAAAAAAAAAAAA/6MII/C9uGEWH4s9EE+W/g=='
(decoded length: 510).

2018/09/14 13:39:18| negotiate_wrapper: received type 3 NTLM token

2018/09/14 13:39:18| negotiate_wrapper: Return 'AF = crystall

'

2018/09/14 13:39:18.326 kid1| 29,4| UserRequest.cc(336) HandleReply:
authenticated user crystall

2018/09/14 13:39:18.326 kid1| 29,4| UserRequest.cc(355) HandleReply:
Successfully validated user via Negotiate. Username 'crystall'

2018/09/14 13:39:18| negotiate_wrapper: Return 'AF = crystall

'

2018/09/14 13:39:18.331 kid1| 29,4| UserRequest.cc(336) HandleReply:
authenticated user crystall

2018/09/14 13:39:18.331 kid1| 29,4| UserRequest.cc(355) HandleReply:
Successfully validated user via Negotiate. Username 'crystall'

2018/09/14 13:39:18| negotiate_wrapper: Return 'AF = crystall

'

2018/09/14 13:39:18.335 kid1| 29,4| UserRequest.cc(336) HandleReply:
authenticated user crystall

2018/09/14 13:39:18.335 kid1| 29,4| UserRequest.cc(355) HandleReply:
Successfully validated user via Negotiate. Username 'crystall'

2018/09/14 13:39:18| negotiate_wrapper: Return 'AF = crystall

'

2018/09/14 13:39:18.340 kid1| 29,4| UserRequest.cc(336) HandleReply:
authenticated user crystall

2018/09/14 13:39:18.340 kid1| 29,4| UserRequest.cc(355) HandleReply:
Successfully validated user via Negotiate. Username 'crystall'

2018/09/14 13:39:18.340 kid1| ICP is disabled! Cannot send ICP request to
peer.

2018/09/14 13:39:18.340 kid1| ICP is disabled! Cannot send ICP request to
peer.

2018/09/14 13:39:18.779 kid1| 29,4| UserRequest.cc(294) authenticate: No
Proxy-Auth header and no working alternative. Requesting auth header.

2018/09/14 13:39:18.782 kid1| 29,4| UserRequest.cc(354) authenticate: No
connection authentication type

2018/09/14 13:39:18| negotiate_wrapper: Got 'YR
TlRMTVNTUAABAAAAl4II4gAAAAAAAAAAAAAAAAAAAAAKAKs/AAAADw==' from squid
(length: 59).

2018/09/14 13:39:18| negotiate_wrapper: Decode
'TlRMTVNTUAABAAAAl4II4gAAAAAAAAAAAAAAAAAAAAAKAKs/AAAADw==' (decoded length:
42).

2018/09/14 13:39:18| negotiate_wrapper: received type 1 NTLM token

2018/09/14 13:39:18| negotiate_wrapper: Return 'TT
TlRMTVNTUAACAAAADgAOADgAAAAVgoni6re6l3Xwbr4AAAAAAAAAAJwAnABGAAAABgEAAAAAAA9F
AEwARQBDAEMAQQBWAAIADgBFAEwARQBDAEMAQQBWAAEAGgBTAFIAVgAtAFMAUQBVAEkARAAtAEsA
UgBCAAQAHABlAGwAZQBjAGMAYQB2AC4AdQBuAGUALgBjAHUAAwA4AHMAcgB2AC0AcwBxAHUAaQBk
AC0AawByAGIALgBlAGwAZQBjAGMAYQB2AC4AdQBuAGUALgBjAHUABwAIANJNNN1RTNQBAAAAAA==

'

2018/09/14 13:39:18.785 kid1| 29,4| UserRequest.cc(311) HandleReply: Need to
challenge the client with a server token:
'TlRMTVNTUAACAAAADgAOADgAAAAVgoni6re6l3Xwbr4AAAAAAAAAAJwAnABGAAAABgEAAAAAAA9
FAEwARQBDAEMAQQBWAAIADgBFAEwARQBDAEMAQQBWAAEAGgBTAFIAVgAtAFMAUQBVAEkARAAtAEs
AUgBCAAQAHABlAGwAZQBjAGMAYQB2AC4AdQBuAGUALgBjAHUAAwA4AHMAcgB2AC0AcwBxAHUAaQB
kAC0AawByAGIALgBlAGwAZQBjAGMAYQB2AC4AdQBuAGUALgBjAHUABwAIANJNNN1RTNQBAAAAAA=
='

2018/09/14 13:39:18.785 kid1| 29,2| UserRequest.cc(203) authenticate: need
to challenge client
'TlRMTVNTUAACAAAADgAOADgAAAAVgoni6re6l3Xwbr4AAAAAAAAAAJwAnABGAAAABgEAAAAAAA9
FAEwARQBDAEMAQQBWAAIADgBFAEwARQBDAEMAQQBWAAEAGgBTAFIAVgAtAFMAUQBVAEkARAAtAEs
AUgBCAAQAHABlAGwAZQBjAGMAYQB2AC4AdQBuAGUALgBjAHUAAwA4AHMAcgB2AC0AcwBxAHUAaQB
kAC0AawByAGIALgBlAGwAZQBjAGMAYQB2AC4AdQBuAGUALgBjAHUABwAIANJNNN1RTNQBAAAAAA=
='!

2018/09/14 13:39:18| negotiate_wrapper: Got 'KK
TlRMTVNTUAADAAAAGAAYAIwAAABIAUgBpAAAAA4ADgBYAAAAEAAQAGYAAAAWABYAdgAAABAAEADs
AQAAFYKI4goAqz8AAAAPZToO29GZi9mTSaZo7kC+uEUATABFAEMAQwBBAFYAYwByAHkAcwB0AGEA
bABsAEMATABJAC0AUgBFAEQARQBTAC0AMQAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAADMCXZljnEc
JGfczvMrEXsbAQEAAAAAAADSTTTdUUzUATkI4mevwzXdAAAAAAIADgBFAEwARQBDAEMAQQBWAAEA
GgBTAFIAVgAtAFMAUQBVAEkARAAtAEsAUgBCAAQAHABlAGwAZQBjAGMAYQB2AC4AdQBuAGUALgBj
AHUAAwA4AHMAcgB2AC0AcwBxAHUAaQBkAC0AawByAGIALgBlAGwAZQBjAGMAYQB2AC4AdQBuAGUA
LgBjAHUABwAIANJNNN1RTNQBBgAEAAIAAAAIADAAMAAAAAAAAAAAAAAAADAAANSO8haD472JTKKO
F8vBYpu8Z0WdTYbu7c7tqLmb/9ooCgAQAAAAAAAAAAAAAAAAAAAAAAAJACQASABUAFQAUAAvADEA
NwAyAC4AMQA5AC4AMgAyADQALgA0ADYAAAAAAAAAAAAAAAAAxTDFbTI2R1oQS5sjProTRQ=='
from squid (length: 683).

2018/09/14 13:39:18| negotiate_wrapper: Decode
'TlRMTVNTUAADAAAAGAAYAIwAAABIAUgBpAAAAA4ADgBYAAAAEAAQAGYAAAAWABYAdgAAABAAEAD
sAQAAFYKI4goAqz8AAAAPZToO29GZi9mTSaZo7kC+uEUATABFAEMAQwBBAFYAYwByAHkAcwB0AGE
AbABsAEMATABJAC0AUgBFAEQARQBTAC0AMQAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAADMCXZljnE
cJGfczvMrEXsbAQEAAAAAAADSTTTdUUzUATkI4mevwzXdAAAAAAIADgBFAEwARQBDAEMAQQBWAAE
AGgBTAFIAVgAtAFMAUQBVAEkARAAtAEsAUgBCAAQAHABlAGwAZQBjAGMAYQB2AC4AdQBuAGUALgB
jAHUAAwA4AHMAcgB2AC0AcwBxAHUAaQBkAC0AawByAGIALgBlAGwAZQBjAGMAYQB2AC4AdQBuAGU
ALgBjAHUABwAIANJNNN1RTNQBBgAEAAIAAAAIADAAMAAAAAAAAAAAAAAAADAAANSO8haD472JTKK
OF8vBYpu8Z0WdTYbu7c7tqLmb/9ooCgAQAAAAAAAAAAAAAAAAAAAAAAAJACQASABUAFQAUAAvADE
ANwAyAC4AMQA5AC4AMgAyADQALgA0ADYAAAAAAAAAAAAAAAAAxTDFbTI2R1oQS5sjProTRQ=='
(decoded length: 510).

2018/09/14 13:39:18| negotiate_wrapper: received type 3 NTLM token

2018/09/14 13:39:18| negotiate_wrapper: Return 'AF = crystall

 

 

Access.log

1536946843.113  66541 192.168.0.2 TCP_TUNNEL/200 3806 CONNECT
www.facebook.com:443 <http://www.facebook.com:443>  crystall
FIRSTUP_PARENT/PARENT_PROXY_IP

 

The question is, that only NTLM works, I've tried with Internet Explorer,
Google Chrome and Firefox. The other thing is that he never asks for
username and password, he uses the user credentials that he initiates
session to work (I do not know if this is the correct operation).

What could be happening?

 

 

                Sorry for the long email.

 

 

Gretting Yanier

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20180914/e55ff4b5/attachment.htm>
-------------- next part --------------
An embedded and charset-unspecified text was scrubbed...
Name: Untitled attachment 00174.txt
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20180914/e55ff4b5/attachment.txt>

From rousskov at measurement-factory.com  Fri Sep 14 21:25:28 2018
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Fri, 14 Sep 2018 15:25:28 -0600
Subject: [squid-users] Squid https_port
In-Reply-To: <trinity-0e32b813-1745-4a5b-b3c8-159c8e2a9069-1536948675515@3c-app-mailcom-lxa08>
References: <trinity-0e32b813-1745-4a5b-b3c8-159c8e2a9069-1536948675515@3c-app-mailcom-lxa08>
Message-ID: <e6cebb59-bfe9-4683-1508-86b632750d38@measurement-factory.com>

On 09/14/2018 12:11 PM, John Refwe wrote:
 ?
> I have a couple of questions about the squid https_port.
> ?
> 1) Does it only exist for transparent connections?

No, it does not. It also supports encrypted connections between the
client and Squid. In that scenario, Squid can be called an HTTPS proxy.
Many modern browsers and other clients (e.g., curl) support HTTPS proxies.


> I know if I want to have a transparent proxy that can accept requests
> TLS requests, I need to have the port be a https_port rather than a
> http_port, but is that what it was created for?

IIRC, it was created for the HTTPS proxy support. Inspection of
intercepted TLS connections came much later.


> 2) How come the https_port does not support receiving proxy protocol?

If it does not, then nobody added that support. There is nothing in the
PROXY protocol itself that would make it impossible to support on the
https_port AFAICT.


> I thought?that HAProxy supports sending it before instantiating a TLS connection?

I do not know what HAProxy does or whether it supports talking to HTTPS
proxies at all, but the whole idea behind HTTPS proxying is to
protect/encrypt client-proxy communication. I would expect HAProxy to
send the PROXY header _inside_ the TLS connection to the HTTPS proxy,
not outside it!

Alex.


From squid3 at treenet.co.nz  Fri Sep 14 23:08:51 2018
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sat, 15 Sep 2018 11:08:51 +1200
Subject: [squid-users] Squid https_port
In-Reply-To: <trinity-e4a320b8-f425-470b-bfba-a48cdde3f7fd-1536947378658@3c-app-mailcom-lxa08>
References: <trinity-e4a320b8-f425-470b-bfba-a48cdde3f7fd-1536947378658@3c-app-mailcom-lxa08>
Message-ID: <23f71386-34f3-d893-e93a-69751823c530@treenet.co.nz>

On 15/09/18 5:49 AM, John Refwe wrote:
> Hi,
> ?
> I have a couple of questions about the squid https_port.
> ?
> 1) Does it only exist for transparent connections? I know if I want to
> have a transparent proxy that can accept requests TLS requests, I need
> to have the port be a https_port rather than a http_port, but is that
> what it was created for?

https_port is for receiving port 443 https:// (HTTP over TLS) rather
than port 3128 or 80  http:// (HTTP over TCP).


> ?
> 2) How come the https_port does not support receiving proxy protocol?
> Perhaps I'm misunderstanding a bit here, but I thought?that HAProxy
> supports sending it before instantiating a TLS connection?

HAProxy does, Squid does not (yet). Mainly because OpenSSL was the code
receiving TLS handshakes. SSL-Bump changes that somewhat, but has not
stabilized enough yet to integrate PROXY protocol into the new TLS
parser. Patches welcome.

Amos


From mujtaba21n at hotmail.com  Sat Sep 15 00:13:52 2018
From: mujtaba21n at hotmail.com (Mujtaba   Hassan Madani)
Date: Sat, 15 Sep 2018 00:13:52 +0000
Subject: [squid-users] Squid Cache Server
In-Reply-To: <HE1P193MB0009EFBF04C8D339B5D23090C61A0@HE1P193MB0009.EURP193.PROD.OUTLOOK.COM>
References: <DB6P193MB0008E0848E133CEB22448158C6040@DB6P193MB0008.EURP193.PROD.OUTLOOK.COM>
 <201809111054.06037.Antony.Stone@squid.open.source.it>
 <DB6P193MB00085E55BC2E60C264EE09A4C61B0@DB6P193MB0008.EURP193.PROD.OUTLOOK.COM>,
 <b6e884e9-0aa3-a48a-b529-afcfe4c60f51@treenet.co.nz>,
 <HE1P193MB0009EFBF04C8D339B5D23090C61A0@HE1P193MB0009.EURP193.PROD.OUTLOOK.COM>
Message-ID: <DB6P193MB000820455863488DA32DDA21C6180@DB6P193MB0008.EURP193.PROD.OUTLOOK.COM>

Hi Amos,

   you did not get back to me about my below concern

Regards


Mujtaba H,



________________________________
From: Mujtaba Hassan Madani <mujtaba21n at hotmail.com>
Sent: Thursday, September 13, 2018 5:36:48 PM
To: Amos Jeffries; squid-users at lists.squid-cache.org
Subject: Re: [squid-users] Squid Cache Server


Hi Amos,

   Iam looking for building a Squid proxy server on Ubuntu for my LAN serving up to 25 PC's I just want the maximum potential of the server capability to enhance the network performance and gain better users expectation of the service.

regards



Mujtaba H,

________________________________
From: squid-users <squid-users-bounces at lists.squid-cache.org> on behalf of Amos Jeffries <squid3 at treenet.co.nz>
Sent: Wednesday, September 12, 2018 2:54:37 PM
To: squid-users at lists.squid-cache.org
Subject: Re: [squid-users] Squid Cache Server

On 13/09/18 2:16 AM, Mujtaba   Hassan Madani wrote:
> Dear Squid Team,
>
>      how does content provider prevent it from been cached while passing
> through squid proxy it's by a copy right law

No. Contents which can be transferred through a proxy are implicitly
licensed for re-distribution.

Legal issues are usually encountered only around interception or
modification of content.


> or some  encryption is

Sometimes.

> implemented in the traffic ?

and other features built into HTTP protocol.


> and where can I find the contents that been
> cached on my squid proxy ?
>

Depends on your config. Usually in the machine RAM.

What are you looking for exactly? and why?


Amos
_______________________________________________
squid-users mailing list
squid-users at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-users
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20180915/b7d83bd3/attachment.htm>

From morteza1131 at gmail.com  Sat Sep 15 05:23:19 2018
From: morteza1131 at gmail.com (morteza omidian)
Date: Sat, 15 Sep 2018 05:23:19 +0000 (UTC)
Subject: [squid-users] change packet flow to have transparent squid proxy
References: <2127671216.5705186.1536988999350.ref@mail.yahoo.com>
Message-ID: <2127671216.5705186.1536988999350@mail.yahoo.com>

Hi
I am in a dire need about using squid in my Linux iptables firewall as a transparent proxy.
In my linux iptables firewall i want to do iptables rules and controls in forward chain and after that do http filtering with squid, because of that i need to change netfilter packet flow and send packets to squid(app layer, user space) after forward chain, and then get them back to kernel space to continue their's way in forward chain and then go out, something like other firewals and utm(like Pfsense or opensense and ....) does.In my situation, i want squid to place after my forwards iptables rules,by default squid is listen on input port of machine but its not what i want and redirect packets to the input chain does not work for me.
I think NFqueue is a good solution for my problem but i don't know that is possible to change squid source code to get packets from nfqueue? or does nfqueue can keep the packet state and handle TCP connection?
I want to change My packet flow like this: client-request >>> prerouting > Nat > forward > squid-cache > post-routing >>>> HTTP(s)-server
The important part is that forward rules must check before packets forwards to squid. i don't want packets destinate to input chain of firewall.I thought maybe its possible to use DAQ ,like the way snort use or nfqueue in iptables. I need some help about that, please help me if its possible or there are any other ways to solve it.

Thanks a lot
Morteza Omidian
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20180915/af3b4b6d/attachment.htm>

From eliezer at ngtech.co.il  Sat Sep 15 18:44:33 2018
From: eliezer at ngtech.co.il (Eliezer Croitoru)
Date: Sat, 15 Sep 2018 21:44:33 +0300
Subject: [squid-users] change packet flow to have transparent squid proxy
In-Reply-To: <2127671216.5705186.1536988999350@mail.yahoo.com>
References: <2127671216.5705186.1536988999350.ref@mail.yahoo.com>
 <2127671216.5705186.1536988999350@mail.yahoo.com>
Message-ID: <454dfcf6ede8584b2275a832cac212dc@ngtech.co.il>

Hey,

What exactly are you trying to do?
HTTP proxies have their own ACL rules like in a firewall.
If you need to block specific traffic then you should enforce the ACL 
inside the proxy and not rely on the FW.
Adding an external acl helper that will do the same thing as iptables is 
only a matter of minutes of coding.
If you have one example I believe I can try to write some helper that 
will do what you need.

All The Bests,
Eliezer

On 2018-09-15 08:23, morteza omidian wrote:
> Hi
> I am in a dire need about using squid in my Linux iptables firewall as
> a transparent proxy.
> In my linux iptables firewall i want to do iptables rules and controls
> in forward chain and after that do http filtering with squid, because
> of that i need to change netfilter packet flow and send packets to
> squid(app layer, user space) after forward chain, and then get them
> back to kernel space to continue their's way in forward chain and then
> go out, SOMETHING LIKE OTHER FIREWALS AND UTM(like Pfsense or
> opensense and ....) does.
> In my situation, i want squid to place AFTER my FORWARDS iptables
> rules,by default squid is listen on input port of machine but its not
> what i want and redirect packets to the input chain does not work for
> me.
> I think NFQUEUE is a good solution for my problem but i don't know
> that is possible to change squid source code to get packets from
> nfqueue? or does nfqueue can keep the packet state and handle TCP
> connection?
> I want to change My packet flow like this: client-request >>>
> prerouting > Nat > forward > squid-cache > post-routing >>>>
> HTTP(s)-server
> The IMPORTANT part is that forward rules must check before packets
> forwards to squid. i don't want packets destinate to input chain of
> firewall. I thought maybe its possible to use DAQ ,like the way snort
> use or nfqueue in iptables. I need some help about that, please help
> me if its possible or THERE ARE ANY OTHER WAYS TO SOLVE IT.
> 
> Thanks a lot
> Morteza Omidian
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users

-- 
----
Eliezer Croitoru
Linux System Administrator
Mobile: +972-5-28704261
Email: eliezer at ngtech.co.il


From squid at the-wes.com  Sat Sep 15 23:22:06 2018
From: squid at the-wes.com (the-wes)
Date: Sat, 15 Sep 2018 18:22:06 -0500 (CDT)
Subject: [squid-users] TCP_MISS/500
In-Reply-To: <566bf4d10907020805w72d07276oa4735463e5054a7a@mail.gmail.com>
References: <566bf4d10907020805w72d07276oa4735463e5054a7a@mail.gmail.com>
Message-ID: <1537053726083-0.post@n4.nabble.com>

Since this thread is the top google search result for "TCP_MISS/500" I just
wanted to add that my root cause turned out to be a full hard drive.



--
Sent from: http://squid-web-proxy-cache.1019090.n4.nabble.com/Squid-Users-f1019091.html


From jlmtoi.ct at gmail.com  Sun Sep 16 13:31:54 2018
From: jlmtoi.ct at gmail.com (Toi Le Van)
Date: Sun, 16 Sep 2018 20:31:54 +0700
Subject: [squid-users] urlParse: URL too large
Message-ID: <CAAmFpiwfuhhJ4MpB6wJRD1Om2yW8oBpqUx664wtmjvWjAYfMPA@mail.gmail.com>

Hi,
My squid encountered this problem when process cache for an instant
messages app.

 2018/09/16 17:11:51| ctx: exit level  0
2018/09/16 17:11:51| WARNING: HTTP: Invalid Response: Bad header
encountered from
http://f8.group.zp.zdn.vn/1677739834110322227/8b455403e361033f5a70.jpg AKA
f8.group.zp.zdn.vn/1677739834110322227/8b455403e361033f5a70.jpg
2018/09/16 17:15:35| urlParse: URL too large (9135 bytes)
2018/09/16 17:15:37| urlParse: URL too large (9300 bytes)
2018/09/16 17:15:38| errorpage.cc(293) errorTryLoadText:
'/usr/share/squid/errors/es-xl/ERR_DNS_FAIL': (2) No such file or directory
2018/09/16 17:15:38| WARNING: Error Pages Missing Language: es-xl
2018/09/16 17:15:39| errorpage.cc(293) errorTryLoadText:
'/usr/share/squid/errors/es-xl/ERR_DNS_FAIL': (2) No such file or directory
2018/09/16 17:15:39| WARNING: Error Pages Missing Language: es-xl
2018/09/16 17:25:50| urlParse: URL too large (12031 bytes)
2018/09/16 17:26:32| ctx: enter level  0: '
http://f8.group.zp.zdn.vn/7458725568410189072/bfaf4c7cda1e3a40630f.jpg'
2018/09/16 17:26:32| WARNING: unparseable HTTP header field {TTP/1.1 200 OK}
2018/09/16 17:52:19| ctx: exit level  0
2018/09/16 17:52:19| errorpage.cc(293) errorTryLoadText:
'/usr/share/squid/errors/ca-es/ERR_DNS_FAIL': (2) No such file or directory
2018/09/16 17:52:19| WARNING: Error Pages Missing Language: ca-es
2018/09/16 17:52:19| errorpage.cc(293) errorTryLoadText:
'/usr/share/squid/errors/ca-es/ERR_DNS_FAIL': (2) No such file or directory
2018/09/16 17:52:19| WARNING: Error Pages Missing Language: ca-es
2018/09/16 17:52:19| errorpage.cc(293) errorTryLoadText:
'/usr/share/squid/errors/ca-es/ERR_DNS_FAIL': (2) No such file or directory
2018/09/16 17:52:19| WARNING: Error Pages Missing Language: ca-es
2018/09/16 17:52:19| errorpage.cc(293) errorTryLoadText:
'/usr/share/squid/errors/ca-es/ERR_DNS_FAIL': (2) No such file or directory
2018/09/16 17:52:19| WARNING: Error Pages Missing Language: ca-es
2018/09/16 17:52:19| errorpage.cc(293) errorTryLoadText:
'/usr/share/squid/errors/ca-es/ERR_DNS_FAIL': (2) No such file or directory
2018/09/16 17:52:19| WARNING: Error Pages Missing Language: ca-es
2018/09/16 17:52:19| errorpage.cc(293) errorTryLoadText:
'/usr/share/squid/errors/ca-es/ERR_DNS_FAIL': (2) No such file or directory
2018/09/16 17:52:19| WARNING: Error Pages Missing Language: ca-es
2018/09/16 17:52:19| errorpage.cc(293) errorTryLoadText:
'/usr/share/squid/errors/ca-es/ERR_DNS_FAIL': (2) No such file or directory
2018/09/16 17:52:19| WARNING: Error Pages Missing Language: ca-es
2018/09/16 17:52:19| errorpage.cc(293) errorTryLoadText:
'/usr/share/squid/errors/ca-es/ERR_DNS_FAIL': (2) No such file or directory
2018/09/16 17:52:19| WARNING: Error Pages Missing Language: ca-es
2018/09/16 17:52:19| errorpage.cc(293) errorTryLoadText:
'/usr/share/squid/errors/ca-es/ERR_DNS_FAIL': (2) No such file or directory
2018/09/16 17:52:19| WARNING: Error Pages Missing Language: ca-es
2018/09/16 17:52:19| errorpage.cc(293) errorTryLoadText:
'/usr/share/squid/errors/ca-es/ERR_DNS_FAIL': (2) No such file or directory
2018/09/16 17:52:19| WARNING: Error Pages Missing Language: ca-es
2018/09/16 17:52:19| errorpage.cc(293) errorTryLoadText:
'/usr/share/squid/errors/ca-es/ERR_DNS_FAIL': (2) No such file or directory
2018/09/16 17:52:19| WARNING: Error Pages Missing Language: ca-es
2018/09/16 18:05:41| errorpage.cc(293) errorTryLoadText:
'/usr/share/squid/errors/zh-hans-cn/ERR_READ_ERROR': (2) No such file or
directory
2018/09/16 18:05:41| WARNING: Error Pages Missing Language: zh-hans-cn
2018/09/16 18:09:49| urlParse: URL too large (12023 bytes)
2018/09/16 18:26:37| errorpage.cc(293) errorTryLoadText:
'/usr/share/squid/errors/zh-hans/ERR_DNS_FAIL': (2) No such file or
directory
2018/09/16 18:26:37| WARNING: Error Pages Missing Language: zh-hans
2018/09/16 18:34:49| urlParse: URL too large (12123 bytes)
2018/09/16 19:08:00| urlParse: URL too large (12053 bytes)
2018/09/16 19:09:19| errorpage.cc(293) errorTryLoadText:
'/usr/share/squid/errors/zh-hans-cn/ERR_READ_ERROR': (2) No such file or
directory
2018/09/16 19:09:19| WARNING: Error Pages Missing Language: zh-hans-cn
2018/09/16 19:09:19| errorpage.cc(293) errorTryLoadText:
'/usr/share/squid/errors/en-cn/ERR_READ_ERROR': (2) No such file or
directory
2018/09/16 19:09:19| WARNING: Error Pages Missing Language: en-cn
2018/09/16 19:09:31| urlParse: URL too large (12057 bytes)
2018/09/16 19:24:10| urlParse: URL too large (12017 bytes)
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20180916/a87a7524/attachment.htm>

From squid3 at treenet.co.nz  Sun Sep 16 16:05:28 2018
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Mon, 17 Sep 2018 04:05:28 +1200
Subject: [squid-users] urlParse: URL too large
In-Reply-To: <CAAmFpiwfuhhJ4MpB6wJRD1Om2yW8oBpqUx664wtmjvWjAYfMPA@mail.gmail.com>
References: <CAAmFpiwfuhhJ4MpB6wJRD1Om2yW8oBpqUx664wtmjvWjAYfMPA@mail.gmail.com>
Message-ID: <2887e7a7-004d-2f86-b31e-7a321e059bf3@treenet.co.nz>

On 17/09/18 1:31 AM, Toi Le Van wrote:
> Hi,
> My squid encountered this problem when process cache for an instant
> messages app.
> 
> ?2018/09/16 17:11:51| ctx: exit level? 0
> 2018/09/16 17:11:51| WARNING: HTTP: Invalid Response: Bad header
> encountered from
> http://f8.group.zp.zdn.vn/1677739834110322227/8b455403e361033f5a70.jpg

This is odd. That URL shows no issues of any kind from here or in
redbot. It may be related to your Squid being very old, or something
going wrong for this traffic - if you see many errors for this domain it
would be worth looking into.


> 2018/09/16 17:15:35| urlParse: URL too large (9135 bytes)
> 2018/09/16 17:15:37| urlParse: URL too large (9300 bytes)

Squid has an 8KB limit on URL length.

I have now added a page to the wiki documenting this log message
<https://wiki.squid-cache.org/KnowledgeBase/UrlTooLarge>


> 2018/09/16 17:15:38| errorpage.cc(293) errorTryLoadText:
> '/usr/share/squid/errors/es-xl/ERR_DNS_FAIL': (2) No such file or directory

Please upgrade your Squid. These messages only occur in very outdated
v3.1 or older Squid.

> 2018/09/16 17:15:38| WARNING: Error Pages Missing Language: es-xl
> 2018/09/16 17:52:19| WARNING: Error Pages Missing Language: ca-es
> 2018/09/16 18:05:41| WARNING: Error Pages Missing Language: zh-hans-cn
> 2018/09/16 18:26:37| WARNING: Error Pages Missing Language: zh-hans
> 2018/09/16 19:09:19| WARNING: Error Pages Missing Language: en-cn

Some of these can be resolved by updating your squid-langpack
translations and ensuring the alias symlinks are installed. The others I
am adding to the pack now. Thank you for the info.

Amos


From squid3 at treenet.co.nz  Sun Sep 16 16:58:37 2018
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Mon, 17 Sep 2018 04:58:37 +1200
Subject: [squid-users] Squid Cache Server
In-Reply-To: <DB6P193MB000820455863488DA32DDA21C6180@DB6P193MB0008.EURP193.PROD.OUTLOOK.COM>
References: <DB6P193MB0008E0848E133CEB22448158C6040@DB6P193MB0008.EURP193.PROD.OUTLOOK.COM>
 <201809111054.06037.Antony.Stone@squid.open.source.it>
 <DB6P193MB00085E55BC2E60C264EE09A4C61B0@DB6P193MB0008.EURP193.PROD.OUTLOOK.COM>
 <b6e884e9-0aa3-a48a-b529-afcfe4c60f51@treenet.co.nz>
 <HE1P193MB0009EFBF04C8D339B5D23090C61A0@HE1P193MB0009.EURP193.PROD.OUTLOOK.COM>
 <DB6P193MB000820455863488DA32DDA21C6180@DB6P193MB0008.EURP193.PROD.OUTLOOK.COM>
Message-ID: <f49f6de9-e3be-360e-cd44-b030f4be4a70@treenet.co.nz>

On 15/09/18 12:13 PM, Mujtaba   Hassan Madani wrote:
> Hi Amos,
> 
> ? ?you did not get back to me about my below concern?
> 

I responded to your concern about copyright.

I do not see anything else in your messages as expressing a concern to
be responded to.

Amos


> ------------------------------------------------------------------------
> *From:* Mujtaba Hassan Madani
> *Sent:* Thursday, September 13, 2018 5:36:48 PM
> ?
> 
> Hi Amos,
> 
> ? ?Iam looking for building a Squid proxy server on Ubuntu for?my LAN
> serving up to 25 PC's I just want the maximum potential of the server
> capability to enhance the network performance and gain better users
> expectation of the service.
> 

> ------------------------------------------------------------------------
> *From:* Amos Jeffries
> *Sent:* Wednesday, September 12, 2018 2:54:37 PM
> ?
> On 13/09/18 2:16 AM, Mujtaba?? Hassan Madani wrote:
>> Dear Squid Team,
>> 
>> ? ? ?how does content provider prevent it from been cached while passing
>> through squid proxy it's by a copy right law
> 
> No. Contents which can be transferred through a proxy are implicitly
> licensed for re-distribution.
> 
> Legal issues are usually encountered only around interception or
> modification of content.
> 
> 
>> or some ?encryption is
> 
> Sometimes.
> 
>> implemented in the traffic ?
> 
> and other features built into HTTP protocol.
> 
> 
>> and where can I find the contents that been
>> cached on my squid proxy ?
>> 
> 
> Depends on your config. Usually in the machine RAM.
> 
> What are you looking for exactly? and why?
> 



From sekarit at gmail.com  Mon Sep 17 13:10:09 2018
From: sekarit at gmail.com (Sekar Duraisamy)
Date: Mon, 17 Sep 2018 18:40:09 +0530
Subject: [squid-users] Could compile squid with --enable-storeio
In-Reply-To: <845bf941-050f-6743-e67f-97b61c4cc8f0@treenet.co.nz>
References: <CADfQnU3dXTVvc+E49bRu7SJiza=Yr4-g7TEQ7uo+7xw8C5Rn0w@mail.gmail.com>
 <694b6e1b-410b-7e09-1108-57396c9008dc@treenet.co.nz>
 <CADfQnU2MP__MdwToeAA5vSPzZf=Q2PHr0UVO4NYnG9N_yXV18g@mail.gmail.com>
 <845bf941-050f-6743-e67f-97b61c4cc8f0@treenet.co.nz>
Message-ID: <CADfQnU2_29C-QxaKDN7kMTZi21JXZ2Kdy_NUacu5eE2rhQuUHg@mail.gmail.com>

Thanks Amos. It works.

On Fri, Sep 7, 2018 at 10:38 PM, Amos Jeffries <squid3 at treenet.co.nz> wrote:
> On 8/09/18 4:13 AM, Sekar Duraisamy wrote:
>> Thanks for your prompt reply.
>>
>> I could see the below message on cache.log even after removed
>> cache_dir  from squid.conf
>>
>> "2018/09/07 02:48:35| Set Current Directory to
>> /opt/squid/squid3527/var/cache/squid"
>>
>> Is this normal or I need to do anything else when i restart the squid?
>
>
> That "cache" in that path is OS terminology from the FHS standard
> (<http://www.pathname.com/fhs/pub/fhs-2.3.html#VARCACHEAPPLICATIONCACHEDATA>).
> Not a Squid cache.
>
> The "/var/cache/squid/" part as a whole is the FHS directory assigned to
> Squid for storing anything that it needs to persist between executions
> and across system reboot. That is all.
>  One of those persistent things does happen to be cache_dir (*if* you
> configure any), but other things also are needed to persist and may be
> placed there.
>
>
> For example; that message above is the Squid process CWD being set as
> the location where Squid will drop core dumps *if* the OS lets cores be
> created.
>
> You can move it elsewhere with the coredump_dir directive in squid.conf,
> but be aware that other locations may not be cleaned up automatically by
> your package installation software if/when your Squid installation is
> uninstalled, upgraded, etc.
>
>
> Amos
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users


From sekarit at gmail.com  Mon Sep 17 13:15:43 2018
From: sekarit at gmail.com (Sekar Duraisamy)
Date: Mon, 17 Sep 2018 18:45:43 +0530
Subject: [squid-users] Delay pool not limiting each connection
Message-ID: <CADfQnU1zvZoZznTP9gTTxa8sn3J=qsBqUgoReD7BsHn5Y84b8g@mail.gmail.com>

Hello All,

I have tried to limit the bandwidth for each requests with 5Mbps speed
with below.

delay_pools 1
delay_class 1 2
delay_access 1 allow all
delay_parameters 1 625000/625000 625000/625000


But it is limiting total bandwidth of the squid only 625000 and not
allowing the same bandwdith for each and every requests coming through
squid.

Squid version : Squid-3.5.27


Please help me if i missed anything.


From squid3 at treenet.co.nz  Mon Sep 17 15:02:10 2018
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Tue, 18 Sep 2018 03:02:10 +1200
Subject: [squid-users] Delay pool not limiting each connection
In-Reply-To: <CADfQnU1zvZoZznTP9gTTxa8sn3J=qsBqUgoReD7BsHn5Y84b8g@mail.gmail.com>
References: <CADfQnU1zvZoZznTP9gTTxa8sn3J=qsBqUgoReD7BsHn5Y84b8g@mail.gmail.com>
Message-ID: <ba3a2dff-8573-439d-d762-afc37365317a@treenet.co.nz>

On 18/09/18 1:15 AM, Sekar Duraisamy wrote:
> Hello All,
> 
> I have tried to limit the bandwidth for each requests with 5Mbps speed
> with below.
> 
> delay_pools 1
> delay_class 1 2
> delay_access 1 allow all
> delay_parameters 1 625000/625000 625000/625000
> 
> 
> But it is limiting total bandwidth of the squid only 625000 and not
> allowing the same bandwdith for each and every requests coming through
> squid.

Yes. That behaviour is exactly what you configured above.

  delay_parameters 1 aggregate individual


The "aggregate" is total bandwidth to be shared between individual
clients with each getting up to (but not exceeding) however much is
available.

By configuring aggregate and individual as exactly the same values you
have defined that any one client _could_ use the entire 625000 Bytes/sec
available. A second client only gets what that first one leaves unused.
A third only what those two leave unused, etc.

 ... which is exactly the "class 1" behaviour despite using a class 2
pool to do it.


To use a per-client limit, use a class 2 pools with an aggregate limit
of "none".


Also, no delay pool limits per-request. They are per-network,
per-subnet, per-client, per-user, or per-tag.

The closest you will get to a per-request limit is to have an
external_acl_type helper tag each request with a unique "tag="
identifier and use a class 5 pool.

However, please be aware that most traffic these days in HTTP/1.1 is
actually revalidation which only takes up a few hundred bytes, rarely
even a KB per-request. So having a Mbps per-request allocation is
pointless extra calculation and checking for a huge amount of requests.

Amos


From service.mv at gmail.com  Mon Sep 17 15:37:51 2018
From: service.mv at gmail.com (Service MV)
Date: Mon, 17 Sep 2018 12:37:51 -0300
Subject: [squid-users] Help: squid restarts and squidGuard die
Message-ID: <CA+d==oEt8J-ie2Fm1SYrHSBKD6Y7TRF-v3iGiMXC7WQ36WV1vw@mail.gmail.com>

Dear Ones, I draw on your experience in seeking help to determine whether
or not it is possible to achieve the configuration I am looking for, due to
a strange error I am having.

Before commenting on the bug I describe my testing environment:
- A VM CentOS 7 Core over VirtualBox 5.2, 1 NIC.
- My VM is attached to my domain W2012R2 (following this post
https://www.rootusers.com/how-to-join-centos-linux-to-an-active-directory-domain/)
to achieve kerberos authentication transparent to the user. SElinux
disabled. Owner permissions to user squid in all folders/files involved.
- squid 3.5.20 installed and working great with kerberos, NTLM and basic
authentication. All authentication mechanisms tested and working great.
- SquidGuard: 1.4 Berkeley DB 5.3.21 installed and working great with
blacklists and acl default.

My problem starts when I try to use source acl using ldapusersearch in
squidGuard...

systemctl status squid:
(squid-1)[12627]: The redirector helpers are crashing too rapidly, need
help!

*squidGuard.conf*

dbhome /etc/squid/db
logdir /var/log/squidGuard
ldapbinddn
CN=ldap,OU=SERVICIOS,OU=SISTEMAS,OU=CANAL,OU=MYCOMPANY,DC=mydomain,DC=local
ldapbindpass myULTRAsecretPASS
ldapprotover 3


src WEB_BASIC {
ldapusersearch
ldap://dc-1.mydomain.local:3268/dc=mydomain,dc=local?sAMAccountName?sub?(&(sAMAccountName=%s)(memberOf=cn=WEB_BASIC%2cou=INTERNET%2cou=PERMISOS%2cou=MYCOMPANY%2cdc=mydomain%2cdc=local))
log block.log
}

dest BL_adv {
        domainlist adv/domains
        urllist adv/urls
        log block.log
}

dest BL_aggressive {
        domainlist aggressive/domains
        urllist aggressive/urls
        log block.log
}
#
dest BL_alcohol {
domainlist alcohol/domains
urllist alcohol/urls
log block.log
}
#
dest BL_anonvpn {
domainlist anonvpn/domains
urllist anonvpn/urls
log block.log
}
#
dest BL_chat {
domainlist chat/domains
urllist chat/urls
log block.log
}
#
dest BL_costtraps {
domainlist costtraps/domains
urllist costtraps/urls
log block.log
}
#
dest BL_downloads {
domainlist downloads/domains
urllist downloads/urls
log block.log
}
#
dest BL_drugs {
domainlist drugs/domains
urllist drugs/urls
log block.log
}
#
dest BL_dynamic {
domainlist dynamic/domains
log block.log
}
#
dest BL_fortunetelling {
domainlist fortunetelling/domains
urllist fortunetelling/urls
log block.log
}
#
dest BL_gamble {
domainlist gamble/domains
urllist gamble/urls
log block.log
}
#
dest BL_government {
domainlist government/domains
urllist government/urls
log block.log
}
#
dest BL_hacking {
domainlist hacking/domains
urllist hacking/urls
log block.log
}
#
dest BL_hobby_games-misc {
domainlist hobby/games-misc/domains
urllist hobby/games-misc/urls
log block.log
}
#
dest BL_hobby_games-online {
domainlist hobby/games-online/domains
urllist hobby/games-online/urls
log block.log
}
#
dest BL_movies {
domainlist movies/domains
urllist movies/urls
log block.log
}
#
dest BL_music {
domainlist music/domains
urllist music/urls
log block.log
}
#
dest BL_porn {
domainlist porn/domains
urllist porn/urls
log block.log
}
#
dest BL_radiotv {
domainlist radiotv/domains
urllist radiotv/urls
log block.log
}
#
dest BL_redirector {
domainlist redirector/domains
urllist redirector/urls
log block.log
}
#
dest BL_remotecontrol {
domainlist remotecontrol/domains
urllist remotecontrol/urls
log block.log
}
#
dest BL_ringtones {
domainlist ringtones/domains
urllist ringtones/urls
log block.log
}
#
dest BL_socialnet {
domainlist socialnet/domains
urllist socialnet/urls
log block.log
}
#
dest BL_spyware {
domainlist spyware/domains
urllist spyware/urls
log block.log
}
#
dest BL_tracker {
domainlist tracker/domains
urllist tracker/urls
log block.log
}
#
dest BL_updatesites {
domainlist updatesites/domains
urllist updatesites/urls
log block.log
}
#
dest BL_violence {
domainlist violence/domains
urllist violence/urls
log block.log
}
#
dest BL_warez {
domainlist warez/domains
urllist warez/urls
log block.log
}
#
dest BL_weapons {
domainlist weapons/domains
urllist weapons/urls
log block.log
}
#
dest BL_webphone {
domainlist webphone/domains
urllist webphone/urls
log block.log
}
#
dest BL_webradio {
domainlist webradio/domains
urllist webradio/urls
log block.log
}
#
dest BL_WEBTV {
domainlist webtv/domains
urllist webtv/urls
log block.log
}


dest whitelist {
domainlist whitelist/domains
log block.log
}

dest blacklist {
domainlist blacklist/domains
log block.log
}


acl {

WEB_BASIC {
pass whitelist !BL_porn !blacklist all
redirect
http://s-server1.mydomain.local/cgi-bin/squidGuard.cgi?clientaddr=%a&clientname=%n&clientuser=%i&clientgroup=%s&targetgroup=%t&url=%u
log block.log
}

default {
pass !blacklist all
redirect
http://s-server1.mydomain.local/cgi-bin/squidGuard.cgi?clientaddr=%a&clientname=%n&clientuser=%i&clientgroup=%s&targetgroup=%t&url=%u
log block.log
}

}


*squidGuard.log*

2018-09-17 11:13:39 [12663] New setting: dbhome: /etc/squid/db
2018-09-17 11:13:39 [12663] New setting: logdir: /var/log/squidGuard
2018-09-17 11:13:39 [12663] New setting: ldapbinddn:
CN=ldap,OU=SERVICIOS,OU=SISTEMAS,OU=CANAL,OU=MYCOMPANY,DC=mydomain,DC=local
2018-09-17 11:13:39 [12663] New setting: ldapbindpass: myULTRAsecretPASS
2018-09-17 11:13:39 [12663] New setting: ldapprotover: 3
2018-09-17 11:13:39 [12663] init domainlist /etc/squid/db/adv/domains
2018-09-17 11:13:39 [12663] loading dbfile /etc/squid/db/adv/domains.db
2018-09-17 11:13:39 [12663] init urllist /etc/squid/db/adv/urls
2018-09-17 11:13:39 [12663] loading dbfile /etc/squid/db/adv/urls.db
2018-09-17 11:13:39 [12663] init domainlist /etc/squid/db/aggressive/domains
2018-09-17 11:13:39 [12663] loading dbfile
/etc/squid/db/aggressive/domains.db
2018-09-17 11:13:39 [12663] init urllist /etc/squid/db/aggressive/urls
2018-09-17 11:13:39 [12663] loading dbfile /etc/squid/db/aggressive/urls.db
2018-09-17 11:13:39 [12663] init domainlist /etc/squid/db/alcohol/domains
2018-09-17 11:13:39 [12663] loading dbfile /etc/squid/db/alcohol/domains.db
2018-09-17 11:13:39 [12663] init urllist /etc/squid/db/alcohol/urls
2018-09-17 11:13:39 [12663] loading dbfile /etc/squid/db/alcohol/urls.db
2018-09-17 11:13:39 [12663] init domainlist /etc/squid/db/anonvpn/domains
2018-09-17 11:13:39 [12663] loading dbfile /etc/squid/db/anonvpn/domains.db
2018-09-17 11:13:39 [12663] init urllist /etc/squid/db/anonvpn/urls
2018-09-17 11:13:39 [12663] loading dbfile /etc/squid/db/anonvpn/urls.db
2018-09-17 11:13:39 [12663] init domainlist /etc/squid/db/chat/domains
2018-09-17 11:13:39 [12663] loading dbfile /etc/squid/db/chat/domains.db
2018-09-17 11:13:39 [12663] init urllist /etc/squid/db/chat/urls
2018-09-17 11:13:39 [12663] loading dbfile /etc/squid/db/chat/urls.db
2018-09-17 11:13:39 [12663] init domainlist /etc/squid/db/costtraps/domains
2018-09-17 11:13:39 [12663] loading dbfile
/etc/squid/db/costtraps/domains.db
2018-09-17 11:13:39 [12663] init urllist /etc/squid/db/costtraps/urls
2018-09-17 11:13:39 [12663] loading dbfile /etc/squid/db/costtraps/urls.db
2018-09-17 11:13:39 [12663] init domainlist /etc/squid/db/downloads/domains
2018-09-17 11:13:39 [12663] loading dbfile
/etc/squid/db/downloads/domains.db
2018-09-17 11:13:39 [12663] init urllist /etc/squid/db/downloads/urls
2018-09-17 11:13:39 [12663] loading dbfile /etc/squid/db/downloads/urls.db
2018-09-17 11:13:39 [12663] init domainlist /etc/squid/db/drugs/domains
2018-09-17 11:13:39 [12663] loading dbfile /etc/squid/db/drugs/domains.db
2018-09-17 11:13:39 [12663] init urllist /etc/squid/db/drugs/urls
2018-09-17 11:13:39 [12663] loading dbfile /etc/squid/db/drugs/urls.db
2018-09-17 11:13:39 [12663] init domainlist /etc/squid/db/dynamic/domains
2018-09-17 11:13:39 [12663] loading dbfile /etc/squid/db/dynamic/domains.db
2018-09-17 11:13:39 [12663] init domainlist
/etc/squid/db/fortunetelling/domains
2018-09-17 11:13:39 [12663] loading dbfile
/etc/squid/db/fortunetelling/domains.db
2018-09-17 11:13:39 [12663] init urllist /etc/squid/db/fortunetelling/urls
2018-09-17 11:13:39 [12663] loading dbfile
/etc/squid/db/fortunetelling/urls.db
2018-09-17 11:13:39 [12663] init domainlist /etc/squid/db/gamble/domains
2018-09-17 11:13:39 [12663] loading dbfile /etc/squid/db/gamble/domains.db
2018-09-17 11:13:39 [12663] init urllist /etc/squid/db/gamble/urls
2018-09-17 11:13:39 [12663] loading dbfile /etc/squid/db/gamble/urls.db
2018-09-17 11:13:39 [12663] init domainlist /etc/squid/db/government/domains
2018-09-17 11:13:39 [12663] loading dbfile
/etc/squid/db/government/domains.db
2018-09-17 11:13:39 [12663] init urllist /etc/squid/db/government/urls
2018-09-17 11:13:39 [12663] loading dbfile /etc/squid/db/government/urls.db
2018-09-17 11:13:39 [12663] init domainlist /etc/squid/db/hacking/domains
2018-09-17 11:13:39 [12663] loading dbfile /etc/squid/db/hacking/domains.db
2018-09-17 11:13:39 [12663] init urllist /etc/squid/db/hacking/urls
2018-09-17 11:13:39 [12663] loading dbfile /etc/squid/db/hacking/urls.db
2018-09-17 11:13:39 [12663] init domainlist
/etc/squid/db/hobby/games-misc/domains
2018-09-17 11:13:39 [12663] loading dbfile
/etc/squid/db/hobby/games-misc/domains.db
2018-09-17 11:13:39 [12663] init urllist /etc/squid/db/hobby/games-misc/urls
2018-09-17 11:13:39 [12663] loading dbfile
/etc/squid/db/hobby/games-misc/urls.db
2018-09-17 11:13:39 [12663] init domainlist
/etc/squid/db/hobby/games-online/domains
2018-09-17 11:13:39 [12663] loading dbfile
/etc/squid/db/hobby/games-online/domains.db
2018-09-17 11:13:39 [12663] init urllist
/etc/squid/db/hobby/games-online/urls
2018-09-17 11:13:39 [12663] loading dbfile
/etc/squid/db/hobby/games-online/urls.db
2018-09-17 11:13:39 [12663] init domainlist /etc/squid/db/movies/domains
2018-09-17 11:13:39 [12663] loading dbfile /etc/squid/db/movies/domains.db
2018-09-17 11:13:39 [12663] init urllist /etc/squid/db/movies/urls
2018-09-17 11:13:39 [12663] loading dbfile /etc/squid/db/movies/urls.db
2018-09-17 11:13:39 [12663] init domainlist /etc/squid/db/music/domains
2018-09-17 11:13:39 [12663] loading dbfile /etc/squid/db/music/domains.db
2018-09-17 11:13:39 [12663] init urllist /etc/squid/db/music/urls
2018-09-17 11:13:39 [12663] loading dbfile /etc/squid/db/music/urls.db
2018-09-17 11:13:39 [12663] init domainlist /etc/squid/db/porn/domains
2018-09-17 11:13:39 [12663] loading dbfile /etc/squid/db/porn/domains.db
2018-09-17 11:13:39 [12663] init urllist /etc/squid/db/porn/urls
2018-09-17 11:13:39 [12663] loading dbfile /etc/squid/db/porn/urls.db
2018-09-17 11:13:39 [12663] init domainlist /etc/squid/db/radiotv/domains
2018-09-17 11:13:39 [12663] loading dbfile /etc/squid/db/radiotv/domains.db
2018-09-17 11:13:39 [12663] init urllist /etc/squid/db/radiotv/urls
2018-09-17 11:13:39 [12663] loading dbfile /etc/squid/db/radiotv/urls.db
2018-09-17 11:13:39 [12663] init domainlist /etc/squid/db/redirector/domains
2018-09-17 11:13:39 [12663] loading dbfile
/etc/squid/db/redirector/domains.db
2018-09-17 11:13:39 [12663] init urllist /etc/squid/db/redirector/urls
2018-09-17 11:13:39 [12663] loading dbfile /etc/squid/db/redirector/urls.db
2018-09-17 11:13:39 [12663] init domainlist
/etc/squid/db/remotecontrol/domains
2018-09-17 11:13:39 [12663] loading dbfile
/etc/squid/db/remotecontrol/domains.db
2018-09-17 11:13:39 [12663] init urllist /etc/squid/db/remotecontrol/urls
2018-09-17 11:13:39 [12663] loading dbfile
/etc/squid/db/remotecontrol/urls.db
2018-09-17 11:13:39 [12663] init domainlist /etc/squid/db/ringtones/domains
2018-09-17 11:13:39 [12663] loading dbfile
/etc/squid/db/ringtones/domains.db
2018-09-17 11:13:39 [12663] init urllist /etc/squid/db/ringtones/urls
2018-09-17 11:13:39 [12663] loading dbfile /etc/squid/db/ringtones/urls.db
2018-09-17 11:13:39 [12663] init domainlist /etc/squid/db/socialnet/domains
2018-09-17 11:13:39 [12663] loading dbfile
/etc/squid/db/socialnet/domains.db
2018-09-17 11:13:39 [12663] init urllist /etc/squid/db/socialnet/urls
2018-09-17 11:13:39 [12663] loading dbfile /etc/squid/db/socialnet/urls.db
2018-09-17 11:13:39 [12663] init domainlist /etc/squid/db/spyware/domains
2018-09-17 11:13:39 [12663] loading dbfile /etc/squid/db/spyware/domains.db
2018-09-17 11:13:39 [12663] init urllist /etc/squid/db/spyware/urls
2018-09-17 11:13:39 [12663] loading dbfile /etc/squid/db/spyware/urls.db
2018-09-17 11:13:39 [12663] init domainlist /etc/squid/db/tracker/domains
2018-09-17 11:13:39 [12663] loading dbfile /etc/squid/db/tracker/domains.db
2018-09-17 11:13:39 [12663] init urllist /etc/squid/db/tracker/urls
2018-09-17 11:13:39 [12663] loading dbfile /etc/squid/db/tracker/urls.db
2018-09-17 11:13:39 [12663] init domainlist
/etc/squid/db/updatesites/domains
2018-09-17 11:13:39 [12663] loading dbfile
/etc/squid/db/updatesites/domains.db
2018-09-17 11:13:39 [12663] init urllist /etc/squid/db/updatesites/urls
2018-09-17 11:13:39 [12663] loading dbfile /etc/squid/db/updatesites/urls.db
2018-09-17 11:13:39 [12663] init domainlist /etc/squid/db/violence/domains
2018-09-17 11:13:39 [12663] loading dbfile /etc/squid/db/violence/domains.db
2018-09-17 11:13:39 [12663] init urllist /etc/squid/db/violence/urls
2018-09-17 11:13:39 [12663] loading dbfile /etc/squid/db/violence/urls.db
2018-09-17 11:13:39 [12663] init domainlist /etc/squid/db/warez/domains
2018-09-17 11:13:39 [12663] loading dbfile /etc/squid/db/warez/domains.db
2018-09-17 11:13:39 [12663] init urllist /etc/squid/db/warez/urls
2018-09-17 11:13:39 [12663] loading dbfile /etc/squid/db/warez/urls.db
2018-09-17 11:13:39 [12663] init domainlist /etc/squid/db/weapons/domains
2018-09-17 11:13:39 [12663] loading dbfile /etc/squid/db/weapons/domains.db
2018-09-17 11:13:39 [12663] init urllist /etc/squid/db/weapons/urls
2018-09-17 11:13:39 [12663] loading dbfile /etc/squid/db/weapons/urls.db
2018-09-17 11:13:39 [12663] init domainlist /etc/squid/db/webphone/domains
2018-09-17 11:13:39 [12663] loading dbfile /etc/squid/db/webphone/domains.db
2018-09-17 11:13:39 [12663] init urllist /etc/squid/db/webphone/urls
2018-09-17 11:13:39 [12663] loading dbfile /etc/squid/db/webphone/urls.db
2018-09-17 11:13:39 [12663] init domainlist /etc/squid/db/webradio/domains
2018-09-17 11:13:39 [12663] loading dbfile /etc/squid/db/webradio/domains.db
2018-09-17 11:13:39 [12663] init urllist /etc/squid/db/webradio/urls
2018-09-17 11:13:39 [12663] loading dbfile /etc/squid/db/webradio/urls.db
2018-09-17 11:13:39 [12663] init domainlist /etc/squid/db/webtv/domains
2018-09-17 11:13:39 [12663] loading dbfile /etc/squid/db/webtv/domains.db
2018-09-17 11:13:39 [12663] init urllist /etc/squid/db/webtv/urls
2018-09-17 11:13:39 [12663] loading dbfile /etc/squid/db/webtv/urls.db
2018-09-17 11:13:39 [12663] init domainlist /etc/squid/db/whitelist/domains
2018-09-17 11:13:39 [12663] loading dbfile
/etc/squid/db/whitelist/domains.db
2018-09-17 11:13:39 [12663] init domainlist /etc/squid/db/blacklist/domains
2018-09-17 11:13:39 [12663] loading dbfile
/etc/squid/db/blacklist/domains.db
2018-09-17 11:13:39 [12663] logfile not allowed in acl other than default
2018-09-17 11:13:39 [12663] squidGuard 1.4 started (1537193619.900)
2018-09-17 11:13:39 [12663] squidGuard ready for requests (1537193619.903)

*squid.conf*

acl localnet src 10.10.8.0/22   # LAN net
acl dmz src 192.168.20.0/27     # DMZ net

### negotiate kerberos & ntlm authentication
auth_param negotiate program /usr/sbin/negotiate_wrapper --ntlm
/usr/bin/ntlm_auth --helper-protocol=squid-2.5-ntlmssp --kerberos
/usr/lib64/squid/negotiate_kerberos_auth -r -i -s GSS_C_NO_NAME
auth_param negotiate children 10
auth_param negotiate keep_alive on

### basic authentication for not kerberos or ntlm authenticated users
auth_param basic program /usr/lib64/squid/basic_ldap_auth -R -b
"dc=mydomain,dc=local" -D "ldap at mydomain.local" -w " myULTRAsecretPASS  "
-f sAMAccountName=%s -h dc-1.mydomain.local
auth_param basic children 10
auth_param basic realm Identifiquese
auth_param basic credentialsttl 4 hours

### standard allowed ports
acl SSL_ports port 443
acl Safe_ports port 80 # http
acl Safe_ports port 21 # ftp
acl Safe_ports port 443 # https
acl Safe_ports port 70 # gopher
acl Safe_ports port 210 # wais
acl Safe_ports port 1025-65535 # unregistered ports
acl Safe_ports port 280 # http-mgmt
acl Safe_ports port 488 # gss-http
acl Safe_ports port 591 # filemaker
acl Safe_ports port 777 # multiling http
acl CONNECT method CONNECT

### acl for proxy authentication (kerberos or ntlm) and ldap authorizations
acl auth proxy_auth REQUIRED

# Define protocols used for redirects
acl HTTP proto HTTP
acl HTTPS proto HTTPS

### enforce authentication
http_access allow auth
http_access deny !auth

### standard access rules
http_access deny !Safe_ports
http_access deny CONNECT !SSL_ports
http_access allow localhost manager
http_access deny manager
http_access allow localnet
http_access allow dmz
http_access allow localhost
http_access deny all

### OPCIONES VARIAS ###
http_port 8080
coredump_dir /var/spool/squid
refresh_pattern ^ftp: 1440 20% 10080
refresh_pattern ^gopher: 1440 0% 1440
refresh_pattern -i (/cgi-bin/|\?) 0 0% 0
refresh_pattern .  0 20% 4320
quick_abort_min 0 KB
quick_abort_max 0 KB
read_timeout 5 minutes
request_timeout 3 minutes
half_closed_clients off
shutdown_lifetime 15 seconds
log_icp_queries off
dns_v4_first on
ipcache_size 2048
ipcache_low 90
fqdncache_size 4096
forwarded_for off
cache_mgr sistemas at mydomain.com
visible_hostname eren
httpd_suppress_version_string on
uri_whitespace strip


## squidGuard ##
url_rewrite_program /usr/bin/squidGuard -c /etc/squid/squidGuard.conf
url_rewrite_children 10 startup=5 idle=1 concurrency=0
url_rewrite_bypass off


*cache.log*

Squid Cache (Version 3.5.20): Terminated abnormally.
CPU Usage: 0.070 seconds = 0.055 user + 0.015 sys
Maximum Resident Size: 68768 KB
Page faults with physical i/o: 0
2018/09/17 11:13:36 kid1| Starting Squid Cache version 3.5.20 for
x86_64-redhat-linux-gnu...
2018/09/17 11:13:36 kid1| Service Name: squid
2018/09/17 11:13:36 kid1| Starting new negotiateauthenticator helpers...
2018/09/17 11:13:36 kid1| Starting new negotiateauthenticator helpers...
2018/09/17 11:13:36| negotiate_kerberos_auth: INFO: User my.name
authenticated
2018/09/17 11:13:36 kid1| WARNING: redirector #Hlpr1 exited
FATAL: The redirector helpers are crashing too rapidly, need help!

Squid Cache (Version 3.5.20): Terminated abnormally.
CPU Usage: 0.086 seconds = 0.057 user + 0.029 sys
Maximum Resident Size: 68752 KB
Page faults with physical i/o: 0
2018/09/17 11:13:36| negotiate_kerberos_auth: INFO: User my.name
authenticated
2018/09/17 11:13:39 kid1| Starting Squid Cache version 3.5.20 for
x86_64-redhat-linux-gnu...
2018/09/17 11:13:39 kid1| Service Name: squid

*access.log*

1537193586.999      0 10.10.11.154 TCP_DENIED/407 4137 CONNECT
www.google.com.ar:443 - HIER_NONE/- text/html
1537193587.242      0 10.10.11.154 TCP_DENIED/407 4185 CONNECT
clientservices.googleapis.com:443 - HIER_NONE/- text/html
1537193587.269      0 10.10.11.154 TCP_DENIED/407 4145 CONNECT
accounts.google.com:443 - HIER_NONE/- text/html
1537193587.269      0 10.10.11.154 TCP_DENIED/407 4137 CONNECT
www.google.com.ar:443 - HIER_NONE/- text/html
1537193613.322      0 10.10.11.154 TCP_DENIED/407 4185 CONNECT
clientservices.googleapis.com:443 - HIER_NONE/- text/html
1537193616.653      1 10.10.11.154 TCP_DENIED/407 4125 CONNECT
www.clarin.com:443 - HIER_NONE/- text/html
1537193616.732      0 10.10.11.154 TCP_DENIED/407 4145 CONNECT
accounts.google.com:443 - HIER_NONE/- text/html
1537193616.749      1 10.10.11.154 TCP_DENIED/407 4137 CONNECT
www.google.com.ar:443 - HIER_NONE/- text/html

*messages*

Sep 17 11:13:07 proxy kernel: squidGuard[12552]: segfault at
ffffffffd7706bb0 ip 00007fdbf2052e70 sp 00007fffd1b73c70 error 5 in
libldap-2.4.so.2.10.7[7fdbf2027000+52000]
Sep 17 11:13:07 proxy kernel: squidGuard[12553]: segfault at
ffffffffa3d27bb0 ip 00007fd79b787e70 sp 00007ffe47e9b880 error 5 in
libldap-2.4.so.2.10.7[7fd79b75c000+52000]
Sep 17 11:13:07 proxy (squid-1): The redirector helpers are crashing too
rapidly, need help!
Sep 17 11:13:07 proxy squid[12549]: Squid Parent: (squid-1) process 12551
exited with status 1
Sep 17 11:13:10 proxy squid[12549]: Squid Parent: (squid-1) process 12627
started
Sep 17 11:13:33 proxy kernel: squidGuard[12628]: segfault at 1fbd2bb0 ip
00007f452b305e70 sp 00007ffda8c714b0 error 4 in
libldap-2.4.so.2.10.7[7f452b2da000+52000]
Sep 17 11:13:33 proxy (squid-1): The redirector helpers are crashing too
rapidly, need help!
Sep 17 11:13:33 proxy squid[12549]: Squid Parent: (squid-1) process 12627
exited with status 1
Sep 17 11:13:36 proxy squid[12549]: Squid Parent: (squid-1) process 12643
started
Sep 17 11:13:36 proxy kernel: squidGuard[12644]: segfault at 540fdbb0 ip
00007fab84f2de70 sp 00007ffc1aa8d2a0 error 4 in
libldap-2.4.so.2.10.7[7fab84f02000+52000]
Sep 17 11:13:36 proxy (squid-1): The redirector helpers are crashing too
rapidly, need help!
Sep 17 11:13:36 proxy squid[12549]: Squid Parent: (squid-1) process 12643
exited with status 1
Sep 17 11:13:39 proxy squid[12549]: Squid Parent: (squid-1) process 12658
started


If I disable src and acl WEB_BASIC I have no problem. The default acl does
its thing without problems.
But when I enable src and acl WEB_BASIC squidGuard explodes and squid
restarts so I get to notice.
I see an error in a libldap library... Will it be a library error? Or am I
misconfiguring my squid ?

Just in case I've checked more than ten times the URLs of LDAP queries (,
%2c, etc etc)

Thank you very much for any help you can give me.
Best regards

Gabriel
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20180917/0ebaf05c/attachment.htm>

From vh1988 at yahoo.com.ar  Mon Sep 17 17:53:21 2018
From: vh1988 at yahoo.com.ar (Julian Perconti)
Date: Mon, 17 Sep 2018 14:53:21 -0300
Subject: [squid-users] About SSL peek-n-splice/bump configurations
In-Reply-To: <2b2e8b70-7f45-039c-462f-ba78b2fc1563@measurement-factory.com>
References: <000301d43289$14ed4770$3ec7d650$@yahoo.com.ar>
 <001801d432a0$8d102a30$a7307e90$@yahoo.com.ar>
 <6eb5660d-8025-eedd-3ed9-47d30bf22e62@measurement-factory.com>
 <001601d4464c$d1b38910$751a9b30$@yahoo.com.ar>
 <770057a5-3808-3fdc-5bf7-f78d09705913@treenet.co.nz>
 <002001d446cc$148bb580$3da32080$@yahoo.com.ar>
 <c7a4d318-c335-03de-9e5c-73fc258e38c0@treenet.co.nz>
 <008001d4479b$b22b9290$1682b7b0$@yahoo.com.ar>
 <04c7931e-cb29-80d1-2a2a-8a1ccb4761f5@treenet.co.nz>
 <019301d4486b$049ebf00$0ddc3d00$@yahoo.com.ar>
 <6b762c34-f0c4-2d9a-6616-432305af160b@treenet.co.nz>
 <009d01d44935$1648ef80$42dace80$@yahoo.com.ar>
 <5400815d-9a82-97a9-5889-0e76c04a9ba0@measurement-factory.com>
 <003501d44aa4$d784e5d0$868eb170$@yahoo.com.ar>
 <500fbad5-d277-965f-d29a-a81c2f343d2e@measurement-factory.com>
 <004401d44b0e$46cf40c0$d46dc240$@yahoo.com.ar>
 <0472b838-415e-e4d2-c71a-7fbce45de926@measurement-factory.com>
 <008e01d44bbf$ccb41e20$661c5a60$@yahoo.com.ar>
 <2b2e8b70-7f45-039c-462f-ba78b2fc1563@measurement-factory.com>
Message-ID: <006d01d44eaf$55ecc290$01c647b0$@yahoo.com.ar>

> > So, when squid reaches this first rule and line (there is no explicit
> > step)  ...does Squid make a "bucle of steps" only along the first line
> > and go to next line only when the rule stop being
> > applicable/matchable?
> 
> I hesitate answering that question with a simple "yes" or "no" because any
> such answer is likely to mislead folks reading this email.

Well yes, and I hope that this thread helps to others.

> 
> The overall logic is like this:
> 
>   for each step
>   do
>       for each rule
>       do
>           if the rule action is possible and the rule ACLs match,
>               then perform the rule action and either go to the next
>               step or, after applying the final action, exit
>       done
>       apply the default action and exit
>   done

Well, this explanation merits to copy it and paste into squid.conf as a comment.

Let me know if I understand what Squid does with the rules of SslBump through this logic:

>   for each step
>   do # This loop will execute as maximum up to three times; because there are 3 steps in the entire SslBump environment.

>       for each rule
>       do # ...and this loop, will execute as many times as the amount of the rules the config has.

Probably my interpretation of the nested loop is wrong.

Now, How does Squid takes  and retains decisions when the steps are implicit/explicit throught the rules? The developers knows the details.

> > He is being a passive observer of that TLS traffic.
> 
> Squid also validates what it observes/forwards. And there is also TCP/IP
> traffic before (and around) TLS traffic.

OK, so I will peek, instead of splice at step1 and step2; and the final action will be splice and it will happen at step3; the step where the final actions are always taken.

I think that splice at step1 does not make sense according to the doc. and also to the order of steps or the sequence, about how the rules are evaluated.
By other hand, lets squid to do more checks even if a sites will be spliced: to do that, as I said above, I (think that) have to peek instead splice at step1 and step2. 

Even more, about the step3 the squid doc. says: 
  I: Get TLS Server Hello info from the server, including the server certificate.
  II: Validate the TLS server certificate.

Finally, the thing that really does not makes sense is splice at step1 and then splice at step2:
Acording to squid doc.: "step2/step3 is only performed if a peek or stare rule matched during the previous step." (not a splice rule)

> > Here, I am talking about the idea of (explicitly) splice at step1 and then at
> step2 of a white list of sites.
> 
> If you splice at step1, then the number of validations that Squid does would
> be smaller (possibly zero, not sure) than if you splice at step2.

Again, following the documentation: "Step 2 is only performed if a peek or stare rule matched during the previous step." So, Is "correct" to splice at step1 or step2?

> The devil is in the details:

Always.

> * A key detail here is determining whether the intended site _is_
> "really/special sensitive". For example, the intercepted client is connecting to
> b::a:d IPv6 address while claiming in the TLS Hello that it is trying to get to
> sensitive.example.com. Should Squid trust the intended destination IP
> address or the TLS SNI? Or should we wait for the server to identify itself
> with a valid SSL certificate? Etc.

>From the "security side" I think that the second option. "...wait for the server to identify (...)"

Therefore, I think that as is "more secure" bump at step3 then should be more secure splice at step3 too.

> * The other key detail is what should happen when that sensitive site refuses
> to communicate with Squid or otherwise misbehaves. Should Squid, for
> example, simply close the browser connection, making it more likely that the
> user (or their admin) blames the proxy? Or should Squid bump the browser
> connection to explain what has happened, creating all the headaches
> associated with bumping.

Do not what to say at this point. Maybe I am missing something...

> Your Squid configuration should reflect all these key decisions.
> If Squid does not have enough configuration options or code to do exactly
> what you want, then you (or others) can always add more code/options. If
> your use case is common/general enough, then quality implementations of
> those additional features should be officially accepted.

Well, let me show You, what is my *second final* config to do the the most approximate actions I want Squid do and who has read the thread knows, taken as a conclusion of the thread:

Telling to Squid what exactly he has to do at each step explicitly:

   ssl_bump peek step1 noBumpSites # at step1 peak or stare do the same, but Amos says that stare alters "the letters" while peek no.
   ssl_bump peek step2 noBumpSites 
   ssl_bump splice step3 noBumpSites # This is probably reduntant
   ssl_bump stare step1 # Maybe It is the same if I peek here instead stare, because the difference about stare and peek happens at step2, the same come could be stay at first line, not sure.
   ssl_bump stare step2
   ssl_bump bump step3 # This is even more reduntant too.

If You have a look at helion.pl and/or the original config and compare his config and my own above are very similar (not to say identical)

As an excercise a more compact config that do more-or-less the same would be:

   ssl_bump peek noBumpSites
   ssl_bump stare

But, what happen if Squid decides automagically wrong? Or something does not match...?

Do You think that the above rules is more-or-less the more nearest what I want to do?
Excuse me but, I think that at this stage, I gues that You already know what I mean when I say "...what I want to do?"



From squid3 at treenet.co.nz  Mon Sep 17 18:38:06 2018
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Tue, 18 Sep 2018 06:38:06 +1200
Subject: [squid-users] Help: squid restarts and squidGuard die
In-Reply-To: <CA+d==oEt8J-ie2Fm1SYrHSBKD6Y7TRF-v3iGiMXC7WQ36WV1vw@mail.gmail.com>
References: <CA+d==oEt8J-ie2Fm1SYrHSBKD6Y7TRF-v3iGiMXC7WQ36WV1vw@mail.gmail.com>
Message-ID: <f8f51c90-6bd7-09d0-a90b-c9552748d813@treenet.co.nz>

On 18/09/18 3:37 AM, Service MV wrote:
> Dear Ones, I draw on your experience in seeking help to determine
> whether or not it is possible to achieve the configuration I am looking
> for, due to a strange error I am having.

FYI: SquidGuard has not been maintained for many years now.

I recommend you convert as many of your filtering rules as you can into
normal Squid ACLs. Traffic which is being blocked for simple reasons can
be done much more efficiently by Squid than a helper.

You can use the more up-to-date ufdbguard helper as a drop-in
replacement for squidguard during the conversion.



> 
> Before commenting on the bug I describe my testing environment:
> - A VM CentOS 7 Core over VirtualBox 5.2, 1 NIC.
> - My VM is attached to my domain W2012R2 (following this post
> https://www.rootusers.com/how-to-join-centos-linux-to-an-active-directory-domain/)
> to achieve kerberos authentication transparent to the user. SElinux
> disabled. Owner permissions to user squid in all folders/files involved.
> - squid 3.5.20 installed and working great with kerberos, NTLM and basic
> authentication. All authentication mechanisms tested and working great.
> - SquidGuard: 1.4 Berkeley DB 5.3.21 installed and working great with
> blacklists and acl default.
> 
> My problem starts when I try to use source acl using ldapusersearch in
> squidGuard...?
> 
> systemctl status squid:
> (squid-1)[12627]: The redirector helpers are crashing too rapidly, need
> help!
> 
> *squidGuard.conf*
> 
> dbhome /etc/squid/db
> logdir /var/log/squidGuard
> ldapbinddn
> CN=ldap,OU=SERVICIOS,OU=SISTEMAS,OU=CANAL,OU=MYCOMPANY,DC=mydomain,DC=local
> ldapbindpass myULTRAsecretPASS
> ldapprotover 3
> 
> 
> src WEB_BASIC {
> ldapusersearch
> ldap://dc-1.mydomain.local:3268/dc=mydomain,dc=local?sAMAccountName?sub?(&(sAMAccountName=%s)(memberOf=cn=WEB_BASIC%2cou=INTERNET%2cou=PERMISOS%2cou=MYCOMPANY%2cdc=mydomain%2cdc=local))
> log block.log
> }
> 
...
> 
> acl {
> 
> WEB_BASIC{
> pass whitelist !BL_porn !blacklist all
> redirect
> http://s-server1.mydomain.local/cgi-bin/squidGuard.cgi?clientaddr=%a&clientname=%n&clientuser=%i&clientgroup=%s&targetgroup=%t&url=%u
> log block.log
> }
> 
...


> *squid.conf*
> 
> acl localnet src 10.10.8.0/22 # LAN net
> acl dmz src 192.168.20.0/27   # DMZ net

These ACLs are never used dues to what you are doing with the "auth" ACL.

...
> 
> ### acl for proxy authentication (kerberos or ntlm) and ldap authorizations
> acl auth proxy_auth REQUIRED
> 
> # Define protocols used for redirects
> acl HTTP proto HTTP
> acl HTTPS proto HTTPS

These have nothing to do with redirects and are never used.

> 
> ### enforce authentication
> http_access allow auth?
> http_access deny !auth
> 

All possible traffic will match either "auth" or "!auth" above.

That means no http_access rules following this point do anything.


> ### standard access rules
> http_access deny !Safe_ports?
> http_access deny CONNECT !SSL_ports?
> http_access allow localhost manager?
> http_access deny manager

Your custom http_access rules (eg the auth checks) should be down here
so the basic security rules above have a chance to protect your proxy
again DoS, traffic smuggling attacks etc. before more complicated and
resource consuming things happen.


> http_access allow localnet
> http_access allow dmz
> http_access allow localhost?
> http_access deny all
> 

...
> visible_hostname eren 

The hostname needs to be a FQDN. It is delivered to clients in URLs
generated by Squid so they can fetch objects directly from the proxy.

FYI: Squid-3 should be able to automatically locate the hostname of the
machine it is running on. If that is not working then you need to fix
your machine, other software will be using the same mechanism and
likewise be encountering problems.


> httpd_suppress_version_string on?
> uri_whitespace strip
> 
> 
> ## squidGuard ##
> url_rewrite_program /usr/bin/squidGuard -c /etc/squid/squidGuard.conf
> url_rewrite_children 10 startup=5 idle=1 concurrency=0
> url_rewrite_bypass off
> 
> 

Your traffic in your access.log is all CONNECT requests. Those messages
cannot be re-written by SquidGuard. So at the very least you require
this config line:

 url_rewrite_access deny CONNECT


.. at this point you may notice your SG rules have no effect. This is
one of many reasons why you should do access control in the proxy
config, not externally in a complicated and slow helper.

> 
> *messages*
> 
> Sep 17 11:13:07 proxy kernel: squidGuard[12552]: segfault at
> ffffffffd7706bb0 ip 00007fdbf2052e70 sp 00007fffd1b73c70 error 5 in
> libldap-2.4.so.2.10.7[7fdbf2027000+52000]
> Sep 17 11:13:07 proxy kernel: squidGuard[12553]: segfault at
> ffffffffa3d27bb0 ip 00007fd79b787e70 sp 00007ffe47e9b880 error 5 in
> libldap-2.4.so.2.10.7[7fd79b75c000+52000]

...

> 
> If I disable src and acl WEB_BASIC I have no problem. The default acl
> does its thing without problems.
> But when I enable src and acl WEB_BASIC squidGuard explodes and squid
> restarts so I get to notice.
> I see an error in a libldap library... Will it be a library error? Or am
> I misconfiguring my squid ?
> 

It is not a Squid error. It is something in SquidGuard and/or the library.

Amos


From rousskov at measurement-factory.com  Mon Sep 17 19:57:12 2018
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Mon, 17 Sep 2018 13:57:12 -0600
Subject: [squid-users] About SSL peek-n-splice/bump configurations
In-Reply-To: <006d01d44eaf$55ecc290$01c647b0$@yahoo.com.ar>
References: <000301d43289$14ed4770$3ec7d650$@yahoo.com.ar>
 <001601d4464c$d1b38910$751a9b30$@yahoo.com.ar>
 <770057a5-3808-3fdc-5bf7-f78d09705913@treenet.co.nz>
 <002001d446cc$148bb580$3da32080$@yahoo.com.ar>
 <c7a4d318-c335-03de-9e5c-73fc258e38c0@treenet.co.nz>
 <008001d4479b$b22b9290$1682b7b0$@yahoo.com.ar>
 <04c7931e-cb29-80d1-2a2a-8a1ccb4761f5@treenet.co.nz>
 <019301d4486b$049ebf00$0ddc3d00$@yahoo.com.ar>
 <6b762c34-f0c4-2d9a-6616-432305af160b@treenet.co.nz>
 <009d01d44935$1648ef80$42dace80$@yahoo.com.ar>
 <5400815d-9a82-97a9-5889-0e76c04a9ba0@measurement-factory.com>
 <003501d44aa4$d784e5d0$868eb170$@yahoo.com.ar>
 <500fbad5-d277-965f-d29a-a81c2f343d2e@measurement-factory.com>
 <004401d44b0e$46cf40c0$d46dc240$@yahoo.com.ar>
 <0472b838-415e-e4d2-c71a-7fbce45de926@measurement-factory.com>
 <008e01d44bbf$ccb41e20$661c5a60$@yahoo.com.ar>
 <2b2e8b70-7f45-039c-462f-ba78b2fc1563@measurement-factory.com>
 <006d01d44eaf$55ecc290$01c647b0$@yahoo.com.ar>
Message-ID: <ee42d35f-2dc0-3bbc-f6c4-a1852374a832@measurement-factory.com>

On 09/17/2018 11:53 AM, Julian Perconti wrote:

>> The overall logic is like this:
>>
>>   for each step
>>   do
>>       for each rule
>>       do
>>           if the rule action is possible and the rule ACLs match,
>>               then perform the rule action and either go to the next
>>               step or, after applying the final action, exit
>>       done
>>       apply the default action and exit
>>   done

> Let me know if I understand what Squid does with the rules of SslBump through this logic:
> 
>>   for each step
>>   do # This loop will execute as maximum up to three times; because there are 3 steps in the entire SslBump environment.

Yes.


>>       for each rule
>>       do # ...and this loop, will execute as many times as the amount of the rules the config has.

For each other loop iteration, this inner loop will execute zero or more
times, depending on the number _and_ meaning/content of the rules.

Both loops can finish "early" (i.e. before three steps and/or before all
configured rules are evaluated).



> Now, How does Squid takes  and retains decisions when the steps are
> implicit/explicit throught the rules?

See my loops summary above: If the inner loop runs to its completion,
then Squid applies the default action (because the inner loop found no
usable explicit rules). There is no secret magic here (at least not at
this level of detail).


> OK, so I will peek, instead of splice at step1 and step2; and the
> final action will be splice and it will happen at step3; the step
> where the final actions are always taken.

Just to avoid misunderstanding: Final actions may be taken at any step,
but only final actions are possible at step3.


> I think that splice at step1 does not make sense according to the
> doc. and also to the order of steps or the sequence, about how the
> rules are evaluated.

I do not know what you mean. Splice at step1 is certainly possible and
even recommended for known non-TLS traffic.


> the thing that really does not makes sense is splice at step1 and then splice at step2:

It is not possible to splice twice. Splicing is one of the final
actions. No other action follows a final action (by definition). Search
for the two "exit" words in the loop summary to find where final actions
may be applied.


> Acording to squid doc.: "step2/step3 is only performed if a peek or
> stare rule matched during the previous step." (not a splice rule)

Correct.


> So, Is "correct" to splice at step1 or step2?

The best thing to do depends on your goals and the transaction. Splicing
at step1, step2, OR step3 makes sense in some cases and does not make
sense (or is impossible) in others.

You need to evaluate your rules in the context of a specific transaction
though: The same set of ssl_bump rules may splice transaction A at step1
and transaction C at step3. The loops summarized above are executed from
scratch for every transaction that reaches ssl_bump directive evaluation.


>> * A key detail here is determining whether the intended site _is_
>> "really/special sensitive". For example, the intercepted client is connecting to
>> b::a:d IPv6 address while claiming in the TLS Hello that it is trying to get to
>> sensitive.example.com. Should Squid trust the intended destination IP
>> address or the TLS SNI? Or should we wait for the server to identify itself
>> with a valid SSL certificate? Etc.

> From the "security side" I think that the second option. "...wait for the server to identify (...)"

> Therefore, I think that as is "more secure" bump at step3 then should be more secure splice at step3 too.

It is impossible to make "splice or bump" decision at step3 because
splicing at step3 requires peeking at step2 while bumping at step3
requires staring at step2. In a context of a single transaction, it is
impossible to both peek and stare at the same time!

Thus, you essentially have to make that "splice or bump" decision
earlier, at step1 or step2, when you have less information than you
would have at step3. It is almost like the dominant quantum physics
theory -- by measuring at step2, you determine the outcome of that
measurement (i.e. available actions at step3).


> Telling to Squid what exactly he has to do at each step explicitly:
> 
>    ssl_bump peek step1 noBumpSites # at step1 peak or stare do the same, but Amos says that stare alters "the letters" while peek no.

A matching peek rule at step1 results in (TLS Client Hello being parsed
during) step2. It also tells Squid to splice by default at step2 if
Squid needs to apply a default action at step2.

A matching stare rule at step1 results in (TLS Client Hello being parsed
during) step2. It also tells Squid to bump by default at step2 if Squid
needs to apply a default action at step2.

There are no TLS byte modifications during peeking or staring at step1.

I think this is all documented at
https://wiki.squid-cache.org/Features/SslPeekAndSplice



>    ssl_bump peek noBumpSites
>    ssl_bump stare
> 
> But, what happen if Squid decides automagically wrong? Or something does not match...?

I do not know what you mean by "Squid decides automagically wrong"

At step1 and at step2, if noBumpSites matches, then Squid will peek.

At step1 and at step2, if noBumpSites does not match, then Squid will stare.

At step3, no explicit rules can match so Squid will either splice or
bump, depending on whether noBumpSites matched at step2.




> Do You think that the above rules is more-or-less the more nearest
> what I want to do? Excuse me but, I think that at this stage, I gues
> that You already know what I mean when I say "...what I want to do?"

Sorry, I do not. And since there are many details that define what one
wants to (or should) do, it may be impractical to relay all of them on
an informal email thread. However, if you understand how SslBump rules
work, then you can either answer a vague "Am I doing what I want to be
doing?" question yourself or ask more specific questions that can be
answered on the mailing list.

Alex.


From rousskov at measurement-factory.com  Mon Sep 17 20:04:20 2018
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Mon, 17 Sep 2018 14:04:20 -0600
Subject: [squid-users] About SSL peek-n-splice/bump configurations
In-Reply-To: <ee42d35f-2dc0-3bbc-f6c4-a1852374a832@measurement-factory.com>
References: <000301d43289$14ed4770$3ec7d650$@yahoo.com.ar>
 <770057a5-3808-3fdc-5bf7-f78d09705913@treenet.co.nz>
 <002001d446cc$148bb580$3da32080$@yahoo.com.ar>
 <c7a4d318-c335-03de-9e5c-73fc258e38c0@treenet.co.nz>
 <008001d4479b$b22b9290$1682b7b0$@yahoo.com.ar>
 <04c7931e-cb29-80d1-2a2a-8a1ccb4761f5@treenet.co.nz>
 <019301d4486b$049ebf00$0ddc3d00$@yahoo.com.ar>
 <6b762c34-f0c4-2d9a-6616-432305af160b@treenet.co.nz>
 <009d01d44935$1648ef80$42dace80$@yahoo.com.ar>
 <5400815d-9a82-97a9-5889-0e76c04a9ba0@measurement-factory.com>
 <003501d44aa4$d784e5d0$868eb170$@yahoo.com.ar>
 <500fbad5-d277-965f-d29a-a81c2f343d2e@measurement-factory.com>
 <004401d44b0e$46cf40c0$d46dc240$@yahoo.com.ar>
 <0472b838-415e-e4d2-c71a-7fbce45de926@measurement-factory.com>
 <008e01d44bbf$ccb41e20$661c5a60$@yahoo.com.ar>
 <2b2e8b70-7f45-039c-462f-ba78b2fc1563@measurement-factory.com>
 <006d01d44eaf$55ecc290$01c647b0$@yahoo.com.ar>
 <ee42d35f-2dc0-3bbc-f6c4-a1852374a832@measurement-factory.com>
Message-ID: <bd82560a-3ecf-e9c3-71d5-2792628eb2f6@measurement-factory.com>

On 09/17/2018 01:57 PM, Alex Rousskov wrote:

> For each other loop iteration, this inner loop will execute zero or more
> times, depending on the number _and_ meaning/content of the rules.

Typo: s/other loop/outer loop/

Alex.



From flashdown at data-core.org  Mon Sep 17 23:10:54 2018
From: flashdown at data-core.org (Flashdown)
Date: Tue, 18 Sep 2018 01:10:54 +0200
Subject: [squid-users] Help: squid restarts and squidGuard die
In-Reply-To: <f8f51c90-6bd7-09d0-a90b-c9552748d813@treenet.co.nz>
References: <CA+d==oEt8J-ie2Fm1SYrHSBKD6Y7TRF-v3iGiMXC7WQ36WV1vw@mail.gmail.com>
 <f8f51c90-6bd7-09d0-a90b-c9552748d813@treenet.co.nz>
Message-ID: <96404267-7169-4A99-87D7-F5656592E900@data-core.org>

Just want to add, I use SquidGuard in two High load setups and never ran into issues. I didnt integrate it as url rewrite helper but as external acl helper and it works great with 800 Users.. 

Am 17. September 2018 20:38:06 MESZ schrieb Amos Jeffries <squid3 at treenet.co.nz>:
>On 18/09/18 3:37 AM, Service MV wrote:
>> Dear Ones, I draw on your experience in seeking help to determine
>> whether or not it is possible to achieve the configuration I am
>looking
>> for, due to a strange error I am having.
>
>FYI: SquidGuard has not been maintained for many years now.
>
>I recommend you convert as many of your filtering rules as you can into
>normal Squid ACLs. Traffic which is being blocked for simple reasons
>can
>be done much more efficiently by Squid than a helper.
>
>You can use the more up-to-date ufdbguard helper as a drop-in
>replacement for squidguard during the conversion.
>
>
>
>> 
>> Before commenting on the bug I describe my testing environment:
>> - A VM CentOS 7 Core over VirtualBox 5.2, 1 NIC.
>> - My VM is attached to my domain W2012R2 (following this post
>>
>https://www.rootusers.com/how-to-join-centos-linux-to-an-active-directory-domain/)
>> to achieve kerberos authentication transparent to the user. SElinux
>> disabled. Owner permissions to user squid in all folders/files
>involved.
>> - squid 3.5.20 installed and working great with kerberos, NTLM and
>basic
>> authentication. All authentication mechanisms tested and working
>great.
>> - SquidGuard: 1.4 Berkeley DB 5.3.21 installed and working great with
>> blacklists and acl default.
>> 
>> My problem starts when I try to use source acl using ldapusersearch
>in
>> squidGuard...?
>> 
>> systemctl status squid:
>> (squid-1)[12627]: The redirector helpers are crashing too rapidly,
>need
>> help!
>> 
>> *squidGuard.conf*
>> 
>> dbhome /etc/squid/db
>> logdir /var/log/squidGuard
>> ldapbinddn
>>
>CN=ldap,OU=SERVICIOS,OU=SISTEMAS,OU=CANAL,OU=MYCOMPANY,DC=mydomain,DC=local
>> ldapbindpass myULTRAsecretPASS
>> ldapprotover 3
>> 
>> 
>> src WEB_BASIC {
>> ldapusersearch
>>
>ldap://dc-1.mydomain.local:3268/dc=mydomain,dc=local?sAMAccountName?sub?(&(sAMAccountName=%s)(memberOf=cn=WEB_BASIC%2cou=INTERNET%2cou=PERMISOS%2cou=MYCOMPANY%2cdc=mydomain%2cdc=local))
>> log block.log
>> }
>> 
>...
>> 
>> acl {
>> 
>> WEB_BASIC{
>> pass whitelist !BL_porn !blacklist all
>> redirect
>>
>http://s-server1.mydomain.local/cgi-bin/squidGuard.cgi?clientaddr=%a&clientname=%n&clientuser=%i&clientgroup=%s&targetgroup=%t&url=%u
>> log block.log
>> }
>> 
>...
>
>
>> *squid.conf*
>> 
>> acl localnet src 10.10.8.0/22 # LAN net
>> acl dmz src 192.168.20.0/27   # DMZ net
>
>These ACLs are never used dues to what you are doing with the "auth"
>ACL.
>
>...
>> 
>> ### acl for proxy authentication (kerberos or ntlm) and ldap
>authorizations
>> acl auth proxy_auth REQUIRED
>> 
>> # Define protocols used for redirects
>> acl HTTP proto HTTP
>> acl HTTPS proto HTTPS
>
>These have nothing to do with redirects and are never used.
>
>> 
>> ### enforce authentication
>> http_access allow auth?
>> http_access deny !auth
>> 
>
>All possible traffic will match either "auth" or "!auth" above.
>
>That means no http_access rules following this point do anything.
>
>
>> ### standard access rules
>> http_access deny !Safe_ports?
>> http_access deny CONNECT !SSL_ports?
>> http_access allow localhost manager?
>> http_access deny manager
>
>Your custom http_access rules (eg the auth checks) should be down here
>so the basic security rules above have a chance to protect your proxy
>again DoS, traffic smuggling attacks etc. before more complicated and
>resource consuming things happen.
>
>
>> http_access allow localnet
>> http_access allow dmz
>> http_access allow localhost?
>> http_access deny all
>> 
>
>...
>> visible_hostname eren 
>
>The hostname needs to be a FQDN. It is delivered to clients in URLs
>generated by Squid so they can fetch objects directly from the proxy.
>
>FYI: Squid-3 should be able to automatically locate the hostname of the
>machine it is running on. If that is not working then you need to fix
>your machine, other software will be using the same mechanism and
>likewise be encountering problems.
>
>
>> httpd_suppress_version_string on?
>> uri_whitespace strip
>> 
>> 
>> ## squidGuard ##
>> url_rewrite_program /usr/bin/squidGuard -c /etc/squid/squidGuard.conf
>> url_rewrite_children 10 startup=5 idle=1 concurrency=0
>> url_rewrite_bypass off
>> 
>> 
>
>Your traffic in your access.log is all CONNECT requests. Those messages
>cannot be re-written by SquidGuard. So at the very least you require
>this config line:
>
> url_rewrite_access deny CONNECT
>
>
>.. at this point you may notice your SG rules have no effect. This is
>one of many reasons why you should do access control in the proxy
>config, not externally in a complicated and slow helper.
>
>> 
>> *messages*
>> 
>> Sep 17 11:13:07 proxy kernel: squidGuard[12552]: segfault at
>> ffffffffd7706bb0 ip 00007fdbf2052e70 sp 00007fffd1b73c70 error 5 in
>> libldap-2.4.so.2.10.7[7fdbf2027000+52000]
>> Sep 17 11:13:07 proxy kernel: squidGuard[12553]: segfault at
>> ffffffffa3d27bb0 ip 00007fd79b787e70 sp 00007ffe47e9b880 error 5 in
>> libldap-2.4.so.2.10.7[7fd79b75c000+52000]
>
>...
>
>> 
>> If I disable src and acl WEB_BASIC I have no problem. The default acl
>> does its thing without problems.
>> But when I enable src and acl WEB_BASIC squidGuard explodes and squid
>> restarts so I get to notice.
>> I see an error in a libldap library... Will it be a library error? Or
>am
>> I misconfiguring my squid ?
>> 
>
>It is not a Squid error. It is something in SquidGuard and/or the
>library.
>
>Amos
>_______________________________________________
>squid-users mailing list
>squid-users at lists.squid-cache.org
>http://lists.squid-cache.org/listinfo/squid-users
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20180918/f6692bfb/attachment.htm>

From sekarit at gmail.com  Tue Sep 18 03:07:51 2018
From: sekarit at gmail.com (Sekar Duraisamy)
Date: Tue, 18 Sep 2018 08:37:51 +0530
Subject: [squid-users] Delay pool not limiting each connection
In-Reply-To: <CADfQnU1zvZoZznTP9gTTxa8sn3J=qsBqUgoReD7BsHn5Y84b8g@mail.gmail.com>
References: <CADfQnU1zvZoZznTP9gTTxa8sn3J=qsBqUgoReD7BsHn5Y84b8g@mail.gmail.com>
Message-ID: <CADfQnU2MSQyPrEFhsMZZArCc_Xcv+-vH0o5tPyKS35riibr7rA@mail.gmail.com>

Anyone can help me on this?

On Mon, Sep 17, 2018 at 6:45 PM, Sekar Duraisamy <sekarit at gmail.com> wrote:
> Hello All,
>
> I have tried to limit the bandwidth for each requests with 5Mbps speed
> with below.
>
> delay_pools 1
> delay_class 1 2
> delay_access 1 allow all
> delay_parameters 1 625000/625000 625000/625000
>
>
> But it is limiting total bandwidth of the squid only 625000 and not
> allowing the same bandwdith for each and every requests coming through
> squid.
>
> Squid version : Squid-3.5.27
>
>
> Please help me if i missed anything.


From arsalan at preston.edu.pk  Tue Sep 18 03:53:48 2018
From: arsalan at preston.edu.pk (Arsalan Hussain)
Date: Tue, 18 Sep 2018 08:53:48 +0500
Subject: [squid-users] Delay pool not limiting each connection
In-Reply-To: <CADfQnU1zvZoZznTP9gTTxa8sn3J=qsBqUgoReD7BsHn5Y84b8g@mail.gmail.com>
References: <CADfQnU1zvZoZznTP9gTTxa8sn3J=qsBqUgoReD7BsHn5Y84b8g@mail.gmail.com>
Message-ID: <CAMwDxM3-mmu3aCOx_=RAjW_9eGHLZY_wkchbgatUgQJwVuGceA@mail.gmail.com>

Dear Sekarit

you are using class-1 pool which has a single aggregate bucket to allocate
to all users and every one can use the maximum available FCFS basis.

you need to use class-2 delay pool has an aggregate bucket and 256
individual buckets which distributed among clients t keep them in limit
what you defined.  (given below configuration as exampls)

acl    bw_users src 192.168.1.0/24         # The acl defined for the Network
acl work_time time MTWHF 09:00-18:00
delay_pools    1                                      # Number of Pool
delay_class    1 2                                    # Defines the
class of pool for the Pool Number 1
delay_parametes    1 62500/62500 25000/25000  # each user has given an
average of 25000 bytes of bandwidth
delay_access  1  allow work_time         # This is the access tag
which tie to the acl all and work_time.


On Mon, Sep 17, 2018 at 6:15 PM Sekar Duraisamy <sekarit at gmail.com> wrote:

> Hello All,
>
> I have tried to limit the bandwidth for each requests with 5Mbps speed
> with below.
>
> delay_pools 1
> delay_class 1 2
> delay_access 1 allow all
> delay_parameters 1 625000/625000 625000/625000
>
>
> But it is limiting total bandwidth of the squid only 625000 and not
> allowing the same bandwidth for each and every requests coming through
> squid.
>
> Squid version : Squid-3.5.27
>
>
> Please help me if i missed anything.
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>


-- 
With Regards,


*Arsalan Hussain*
*Assistant Director, Networks & Information System*

*PRESTON UNIVERSITY*

*Hurt me with the truth, but never comfort me with a lie.*
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20180918/19b015e0/attachment.htm>

From arsalan at preston.edu.pk  Tue Sep 18 03:58:34 2018
From: arsalan at preston.edu.pk (Arsalan Hussain)
Date: Tue, 18 Sep 2018 08:58:34 +0500
Subject: [squid-users] Delay pool not limiting each connection
In-Reply-To: <CAMwDxM3-mmu3aCOx_=RAjW_9eGHLZY_wkchbgatUgQJwVuGceA@mail.gmail.com>
References: <CADfQnU1zvZoZznTP9gTTxa8sn3J=qsBqUgoReD7BsHn5Y84b8g@mail.gmail.com>
 <CAMwDxM3-mmu3aCOx_=RAjW_9eGHLZY_wkchbgatUgQJwVuGceA@mail.gmail.com>
Message-ID: <CAMwDxM3zZA21MRXsDP30Bz5dLMwE8k_fzj-U2xqjw+4sURkAgA@mail.gmail.com>

you need to understand
delay_parameters 1 625000/625000 625000/625000

625000/625000 625000/625000( You defined 625000 max and also given 625000
to individual)

see this how you can limit each user bytes of bandwidht

delay_parametes    1 62500/62500 25000/25000   # each user has given
an average of 25000 bytes of bandwidth


w
On Tue, Sep 18, 2018 at 8:53 AM Arsalan Hussain <arsalan at preston.edu.pk>
wrote:

> Dear Sekarit
>
> you are using class-1 pool which has a single aggregate bucket to allocate
> to all users and every one can use the maximum available FCFS basis.
>
> you need to use class-2 delay pool has an aggregate bucket and 256
> individual buckets which distributed among clients t keep them in limit
> what you defined.  (given below configuration as exampls)
>
> acl    bw_users src 192.168.1.0/24         # The acl defined for the Network
> acl work_time time MTWHF 09:00-18:00
> delay_pools    1                                      # Number of Pool
> delay_class    1 2                                    # Defines the class of pool for the Pool Number 1
> delay_parametes    1 62500/62500 25000/25000  # each user has given an average of 25000 bytes of bandwidth
> delay_access  1  allow work_time         # This is the access tag which tie to the acl all and work_time.
>
>
> On Mon, Sep 17, 2018 at 6:15 PM Sekar Duraisamy <sekarit at gmail.com> wrote:
>
>> Hello All,
>>
>> I have tried to limit the bandwidth for each requests with 5Mbps speed
>> with below.
>>
>> delay_pools 1
>> delay_class 1 2
>> delay_access 1 allow all
>> delay_parameters 1 625000/625000 625000/625000
>>
>>
>> But it is limiting total bandwidth of the squid only 625000 and not
>> allowing the same bandwidth for each and every requests coming through
>> squid.
>>
>> Squid version : Squid-3.5.27
>>
>>
>> Please help me if i missed anything.
>> _______________________________________________
>> squid-users mailing list
>> squid-users at lists.squid-cache.org
>> http://lists.squid-cache.org/listinfo/squid-users
>>
>
>
> --
> With Regards,
>
>
> *Arsalan Hussain*
> *Assistant Director, Networks & Information System*
>
> *PRESTON UNIVERSITY*
>
> *Hurt me with the truth, but never comfort me with a lie.*
>


-- 
With Regards,


*Arsalan Hussain*
*Assistant Director, Networks & Information System*

*PRESTON UNIVERSITY*
Add: Plot: 85, Street No: 3, Sector H-8/1, Islamabad, Pakistan
Cell: +92-322-5018611
UAN: (51) 111-707-808 (Ext: 443)

*Hurt me with the truth, but never comfort me with a lie.*
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20180918/482de2a8/attachment.htm>

From Andreas.Doerfler at kempten.de  Tue Sep 18 12:07:06 2018
From: Andreas.Doerfler at kempten.de (=?utf-8?B?RMO2cmZsZXIsIEFuZHJlYXM=?=)
Date: Tue, 18 Sep 2018 12:07:06 +0000
Subject: [squid-users] TCP_MISS/502 - audio stream - none default http ports
Message-ID: <1537272426.27318.18.camel@kempten.de>

hello there,

i try to get mp3/audio streams working on a "kinda default" debian
stretch installation.

after i failed with the default debian squid configuration, i tried the
working config from the old proxy (Squid Cache: Version 3.1.20), 
but no luck either.

audio streams work when it's a web-based radio on port 80/443, but once
the radio leaves the default ports, it stops working.

since the new squid is 4 minor releases above the old server, i think
something major changed, but i have no clue, i touch the squid
configuration only once every few years...


###>
Squid Cache: Version 3.5.23
<###

config below is the one i copied from the old squid server, but as said,
it won't work. security is handled by the firewall, so "allow all" isn't
a issue.

###>config

acl manager proto cache_object
acl localhost src 127.0.0.1/32 ::1
acl to_localhost dst 127.0.0.0/8 0.0.0.0/32 ::1
acl SSL_ports port 443 9418 5001
acl Safe_ports port 80          # http
acl Safe_ports port 21          # ftp
acl Safe_ports port 443         # https
acl Safe_ports port 70          # gopher
acl Safe_ports port 210         # wais
acl Safe_ports port 1025-65535  # unregistered ports
acl Safe_ports port 280         # http-mgmt
acl Safe_ports port 488         # gss-http
acl Safe_ports port 591         # filemaker
acl Safe_ports port 777         # multiling http
acl CONNECT method CONNECT
http_access allow manager localhost
http_access deny manager
http_access deny !Safe_ports
http_access deny CONNECT !SSL_ports
http_access allow localhost
http_access allow all
http_port 8080
access_log /var/log/squid/access.log squid
debug_options ALL,1
coredump_dir /var/spool/squid
refresh_pattern ^ftp:           1440    20%     10080
refresh_pattern ^gopher:        1440    0%      1440
refresh_pattern -i (/cgi-bin/|\?) 0     0%      0
refresh_pattern .               0       20%     4320
                request_header_access From deny all
                request_header_access Referer deny all
                request_header_access Server deny all
                request_header_access WWW-Authenticate deny all
                request_header_access Link deny all
forwarded_for delete

<###


###>example stream
https://tunein.com/radio/Americana-Breakdown-s281469/
<###

###squid access.log errors

172.16.x.x TCP_MISS/502 4307 GET http://91.121.164.210:8104/ -
HIER_DIRECT/91.121.164.210 text/html

172.16.x.x TCP_MISS/502 4312 GET http://91.121.164.210:8104/; -
HIER_DIRECT/91.121.164.210 text/html
<###

and ideas?

thanks in advance,
andy
-------------- next part --------------
A non-text attachment was scrubbed...
Name: smime.p7s
Type: application/x-pkcs7-signature
Size: 6343 bytes
Desc: not available
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20180918/d1590847/attachment.bin>

From belle at bazuin.nl  Tue Sep 18 13:02:35 2018
From: belle at bazuin.nl (=?windows-1252?Q?L.P.H._van_Belle?=)
Date: Tue, 18 Sep 2018 15:02:35 +0200
Subject: [squid-users] TCP_MISS/502 - audio stream - none default http
 ports
In-Reply-To: <1537272426.27318.18.camel@kempten.de>
References: <1537272426.27318.18.camel@kempten.de>
Message-ID: <vmime.5ba0f76b.58bf.3dd501ed6b716e9b@ms249-lin-003.rotterdam.bazuin.nl>

Hai, 

You missed a few points in your config. 
And thank you for the music link, something different then the radio here. ;-) 

Ive installed a debian stretch server. 
This is the debian default config with 2 modifications. 

## Squid 3.5.23 
## First enable the acl for YOUR localnet ( here i enable all 5) 
: acl localnet src 10.0.0.0/8   # RFC1918 possible internal network
: acl localnet src 172.16.0.0/12        # RFC1918 possible internal network
: acl localnet src 192.168.0.0/16       # RFC1918 possible internal network
: acl localnet src fc00::/7       # RFC 4193 local private network range
: acl localnet src fe80::/10      # RFC 4291 link-local (directly plugged) machines
: acl SSL_ports port 443
## PS in your config you did miss to add the extra SSL_Ports also to the Safe_port. 
: acl Safe_ports port 80                # http
: acl Safe_ports port 21                # ftp
: acl Safe_ports port 443               # https
: acl Safe_ports port 70                # gopher
: acl Safe_ports port 210               # wais
: acl Safe_ports port 1025-65535        # unregistered ports
: acl Safe_ports port 280               # http-mgmt
: acl Safe_ports port 488               # gss-http
: acl Safe_ports port 591               # filemaker
: acl Safe_ports port 777               # multiling http

: acl CONNECT method CONNECT
: http_access deny !Safe_ports
: http_access deny CONNECT !SSL_ports
: http_access allow localhost manager
: http_access deny manager
: http_access allow localnet
## And here you missed the "allow localnet" 
: http_access allow localhost
: http_access deny all
: http_port 3128
: coredump_dir /var/spool/squid
: refresh_pattern ^ftp:         1440    20%     10080
: refresh_pattern ^gopher:      1440    0%      1440
: refresh_pattern -i (/cgi-bin/|\?) 0   0%      0
: refresh_pattern .             0       20%     4320


As extra test. 
I've installed squid 4.2 rebuilded from debian sid. 
This is the exact config used, the used script is below the email. 
I made 2 modifications. 
Configuration File: /etc/squid/conf.d/debian.conf (depth 1) : enabled localnet
Configuration File: /etc/squid/conf.d/headers.conf (depth 1): added your headers.

: acl localnet src 0.0.0.1-0.255.255.255        # RFC 1122 "this" network (LAN)
: acl localnet src 10.0.0.0/8           # RFC 1918 local private network (LAN)
: acl localnet src 100.64.0.0/10                # RFC 6598 shared address space (CGN)
: acl localnet src 169.254.0.0/16       # RFC 3927 link-local (directly plugged) machines
: acl localnet src 172.16.0.0/12                # RFC 1918 local private network (LAN)
: acl localnet src 192.168.0.0/16               # RFC 1918 local private network (LAN)
: acl localnet src fc00::/7             # RFC 4193 local private network range
: acl localnet src fe80::/10            # RFC 4291 link-local (directly plugged) machines
: acl SSL_ports port 443
: acl Safe_ports port 80                # http
: acl Safe_ports port 21                # ftp
: acl Safe_ports port 443               # https
: acl Safe_ports port 70                # gopher
: acl Safe_ports port 210               # wais
: acl Safe_ports port 1025-65535        # unregistered ports
: acl Safe_ports port 280               # http-mgmt
: acl Safe_ports port 488               # gss-http
: acl Safe_ports port 591               # filemaker
: acl Safe_ports port 777               # multiling http
: acl CONNECT method CONNECT
: http_access deny !Safe_ports
: http_access deny CONNECT !SSL_ports
: http_access allow localhost manager
: http_access deny manager
: include /etc/squid/conf.d/*
 Configuration File: /etc/squid/conf.d/debian.conf (depth 1)
: logfile_rotate 0
: http_access allow localnet
 Configuration File: /etc/squid/conf.d/headers.conf (depth 1)
: request_header_access From deny all
: request_header_access Referer deny all
: request_header_access Server deny all
: request_header_access WWW-Authenticate deny all
: request_header_access Link deny all
: forwarded_for delete
: http_access allow localhost
: http_access deny all
: http_port 3128
: coredump_dir /var/spool/squid
: refresh_pattern ^ftp:         1440    20%     10080
: refresh_pattern ^gopher:      1440    0%      1440
: refresh_pattern -i (/cgi-bin/|\?) 0   0%      0
: refresh_pattern .             0       20%     4320

If you want the 4.2 for stretch, you can find it here : 
https://downloads.van-belle.nl/squid/squid4.2/ 
Buildlogs are all included, or rebuild it yourself from sid/testing. 
Its a pretty easy rebuild imo. 

Greetz, 

Louis




> -----Oorspronkelijk bericht-----
> Van: squid-users 
> [mailto:squid-users-bounces at lists.squid-cache.org] Namens 
> D?rfler, Andreas
> Verzonden: dinsdag 18 september 2018 14:07
> Aan: squid-users at lists.squid-cache.org
> Onderwerp: [squid-users] TCP_MISS/502 - audio stream - none 
> default http ports
> 
> hello there,
> 
> i try to get mp3/audio streams working on a "kinda default" debian
> stretch installation.
> 
> after i failed with the default debian squid configuration, i 
> tried the
> working config from the old proxy (Squid Cache: Version 3.1.20), 
> but no luck either.
> 
> audio streams work when it's a web-based radio on port 
> 80/443, but once
> the radio leaves the default ports, it stops working.
> 
> since the new squid is 4 minor releases above the old server, i think
> something major changed, but i have no clue, i touch the squid
> configuration only once every few years...
> 
> 
> ###>
> Squid Cache: Version 3.5.23
> <###
> 
> config below is the one i copied from the old squid server, 
> but as said,
> it won't work. security is handled by the firewall, so "allow 
> all" isn't
> a issue.
> 
> ###>config
> 
> acl manager proto cache_object
> acl localhost src 127.0.0.1/32 ::1
> acl to_localhost dst 127.0.0.0/8 0.0.0.0/32 ::1
> acl SSL_ports port 443 9418 5001
> acl Safe_ports port 80          # http
> acl Safe_ports port 21          # ftp
> acl Safe_ports port 443         # https
> acl Safe_ports port 70          # gopher
> acl Safe_ports port 210         # wais
> acl Safe_ports port 1025-65535  # unregistered ports
> acl Safe_ports port 280         # http-mgmt
> acl Safe_ports port 488         # gss-http
> acl Safe_ports port 591         # filemaker
> acl Safe_ports port 777         # multiling http
> acl CONNECT method CONNECT
> http_access allow manager localhost
> http_access deny manager
> http_access deny !Safe_ports
> http_access deny CONNECT !SSL_ports
> http_access allow localhost
> http_access allow all
> http_port 8080
> access_log /var/log/squid/access.log squid
> debug_options ALL,1
> coredump_dir /var/spool/squid
> refresh_pattern ^ftp:           1440    20%     10080
> refresh_pattern ^gopher:        1440    0%      1440
> refresh_pattern -i (/cgi-bin/|\?) 0     0%      0
> refresh_pattern .               0       20%     4320
>                 request_header_access From deny all
>                 request_header_access Referer deny all
>                 request_header_access Server deny all
>                 request_header_access WWW-Authenticate deny all
>                 request_header_access Link deny all
> forwarded_for delete
> 
> <###
> 
> 
> ###>example stream
> https://tunein.com/radio/Americana-Breakdown-s281469/
> <###
> 
> ###squid access.log errors
> 
> 172.16.x.x TCP_MISS/502 4307 GET http://91.121.164.210:8104/ -
> HIER_DIRECT/91.121.164.210 text/html
> 
> 172.16.x.x TCP_MISS/502 4312 GET http://91.121.164.210:8104/; -
> HIER_DIRECT/91.121.164.210 text/html
> <###
> 
> and ideas?
> 
> thanks in advance,
> andy
> 



From yanier at eleccav.une.cu  Tue Sep 18 13:26:05 2018
From: yanier at eleccav.une.cu (Yanier Salazar Sanchez)
Date: Tue, 18 Sep 2018 09:26:05 -0400
Subject: [squid-users] Problem with kerb/ntlm authentication
In-Reply-To: <00a001d44c54$4804b960$d80e2c20$@eleccav.une.cu>
References: <00a001d44c54$4804b960$d80e2c20$@eleccav.une.cu>
Message-ID: <004d01d44f53$27887080$76995180$@eleccav.une.cu>

I already fixed the problem that caused NTLM authentication to work only.

Greetings yanier

 

 

Ing. Yanier Salazar S?nchez

Administrador de Red

Empresa El?ctrica Ciego de Avila

Tel?fonos: (33) 228613 ext 305

 



  

 

From: squid-users <squid-users-bounces at lists.squid-cache.org> On Behalf Of
Yanier Salazar Sanchez
Sent: Friday, September 14, 2018 13:57
To: squid-users at lists.squid-cache.org
Subject: [squid-users] Problem with kerb/ntlm authentication

 

Sorry for my bad english.

 

This is the scenario

 

I have ubuntu 18.04.01 (with las update) with squid 4.2-2, samba and winbind
4.7.6,  AD on Windows Server 2012 R2/2016 with the las update, Client with
windows 10 1709 with the las update, firefox 60.2.0esr, google chrome
61.0.3163.79, firefox quantum 62.0 and internet explorer

 

I using this guide
https://blog.it-kb.ru/2014/06/16/forward-proxy-squid-3-3-on-ubuntu-server-14
-04-lts-part-1-install-os-on-hyper-v-generation-2-vm/ (Only to where
kerberos and NTLM are configured)

 

I joined the proxy to the active directory

 

All the commands seem to work correctly

 

I run this command

klist

Ticket cache: FILE:/tmp/krb5cc_0

Default principal: HTTP//srv-squid-krb.mired.lan at MIRED.LAN
<mailto:HTTP//srv-squid-krb.mired.lan at MIRED.LAN> 

Valid starting                 Expires    Service principal

09/13/2018 16:29:48    09/14/2018 02:29:48 krbtgt/MIRED.LAN at MIRED.LAN
<mailto:krbtgt/MIRED.LAN at MIRED.LAN> 

09/13/2018 16:55:57    09/14/2018 02:29:48
host/srv-squid-krb.mired.lan at MIRED.LAN
<mailto:host/srv-squid-krb.mired.lan at MIRED.LAN> 

09/13/2018 16:56:13    09/14/2018 02:29:48 host/srv-dc.mired.lan at MIRED.LAN
<mailto:host/srv-dc.mired.lan at MIRED.LAN> 

 

I run this command

kinit squidtest

password for squidtest at MIRED.LAN <mailto:squidtest at MIRED.LAN> :

 

I create a proxy.keytab in my windows server 2012 r2 with this command 

ktpass -princ HTTP/srv-squid-krb.mired.lan at MIRED.LAN
<mailto:HTTP/srv-squid-krb.mired.lan at MIRED.LAN>   -mapuser MIRED\squidtest
-pass password -crypto All -ptype KRB5_NT_PRINCIPAL -out d:\proxy.keytab

proxy.keytab permission

rw-r?r root proxy proxy.keytab

 

 

My krb5.conf file

 

[libdefaults]

        default_realm = MIRED.LAN

        dns_lookup_kdc = yes

        dns_lookup_kdc = no

        ticket_lifetime = 24h

        default_keytab_name = /etc/squid/proxy.keytab

[realms]

        MIRED.LAN = {

                    kdc = srv-dc.mired.lan

                    admin_server = srv-dc.mired.lan

                    default_domain = mired.lan

}

[domain_relam]

        mired.lan = MIRED.LAN

       .mired.lan = MIRED.LAN

 

 

 

 

I run this command

klist ?k /etc/squid/proxy.keytab

Keytab name: FILE/etc/squid/proxy.keytab

KVNO Principal

6      HTTP/srv-squid-krb.mired.lan at MIRED.LAN
<mailto:HTTP/srv-squid-krb.mired.lan at MIRED.LAN> 

6      HTTP/srv-squid-krb.mired.lan at MIRED.LAN
<mailto:HTTP/srv-squid-krb.mired.lan at MIRED.LAN> 

6      HTTP/srv-squid-krb.mired.lan at MIRED.LAN
<mailto:HTTP/srv-squid-krb.mired.lan at MIRED.LAN> 

6      HTTP/srv-squid-krb.mired.lan at MIRED.LAN
<mailto:HTTP/srv-squid-krb.mired.lan at MIRED.LAN> 

6      HTTP/srv-squid-krb.mired.lan at MIRED.LAN
<mailto:HTTP/srv-squid-krb.mired.lan at MIRED.LAN> 

 

I run this command

wbinfo ?authenticate=squidtest%mypassword

Plaintest password athentication succeded

Challenge/response password authentication succeded

 

I run this command

wbinfo ?krb5auth=squidtest%mypassword

Plaintest kerberos password athentication for [squidtest:mypassword]
succeded (requesting cctype: FILE) credential were put in; FILE/tmp/krbcc_0

 

I run this command

wbinfo ?g  (List all groups in AD)

I run this command

wbinfo ?u  (List all users in AD)

 

I run this command

/usr/lib/squid/negotiate_kerberos_auth_test srv-squid-krb.mired.lan

Token: YIICSAYGRKw
.. blabla   /B8VWAxn29WaG/j

 

 

The squid.conf it?s basic configuration only with 

 

auth_program negotiate program /usr/lib/squid/negotiate_wrapper_auth ?d
?ntlm /usr/bin/ntlm_auth ?diagnostics ?helper-protocol=2.5-ntlmssp
?domain=mired ?kerberos /usr/lib/squid/negotiate_kerberos_auth ?d ?r ?s
HTTP//srv-squid-krb.mired.lan at mired.lan
<mailto:HTTP//srv-squid-krb.mired.lan at mired.lan> 

auth_program negotiate children 10

auth_program negotiate keep_alive off

 

auth_param ntlm program /usr/bin/ntlm_auth --diagnostics
?helper-protocol=squid-2.5-ntlmssp 

auth_param ntlm children 10

auth_param ntlm keep_alive off

 

acl red src 192.168.0.0/24

acl auth proxy_auth REQUIRED

 

and 

http_access allow red auth

 

 

But the problem is that Kerberos don?t work. Only NTLM.

cache.log

2018/09/14 06:25:02| negotiate_wrapper: Starting version 1.0.1

2018/09/14 06:25:02| negotiate_wrapper: NTLM command: /usr/bin/ntlm_auth
--diagnostics --helper-protocol=squid-2.5-ntlmssp 

2018/09/14 06:25:02| negotiate_wrapper: Kerberos command:
/usr/lib/squid/negotiate_kerberos_auth -d -r -s
HTTP/srv-squid-krb.mired.lan at MIRED.LAN
<mailto:HTTP/srv-squid-krb.mired.lan at MIRED.LAN>  

negotiate_kerberos_auth.cc(487): pid=10816 :2018/09/14 06:25:02|
negotiate_kerberos_auth: INFO: Starting version 3.1.0sq

negotiate_kerberos_auth.cc(546): pid=10816 :2018/09/14 06:25:02|
negotiate_kerberos_auth: INFO: Setting keytab to /etc/squid/proxy.keytab

negotiate_kerberos_auth.cc(570): pid=10816 :2018/09/14 06:25:02|
negotiate_kerberos_auth: INFO: Changed keytab to
MEMORY:negotiate_kerberos_auth_10816

2018/09/14 13:39:18| negotiate_wrapper: Return 'TT
TlRMTVNTUAACAAAADgAOADgAAAAVgonigQb5TAh6RigAAAAAAAAAAJwAnABGAAAABgEAAAAAAA9F
AEwARQBDAEMAQQBWAAIADgBFAEwARQBDAEMAQQBWAAEAGgBTAFIAVgAtAFMAUQBVAEkARAAtAEsA
UgBCAAQAHABlAGwAZQBjAGMAYQB2AC4AdQBuAGUALgBjAHUAAwA4AHMAcgB2AC0AcwBxAHUAaQBk
AC0AawByAGIALgBlAGwAZQBjAGMAYQB2AC4AdQBuAGUALgBjAHUABwAIALqX2txRTNQBAAAAAA==

'

2018/09/14 13:39:18.197 kid1| 29,4| UserRequest.cc(311) HandleReply: Need to
challenge the client with a server token:
'TlRMTVNTUAACAAAADgAOADgAAAAVgonigQb5TAh6RigAAAAAAAAAAJwAnABGAAAABgEAAAAAAA9
FAEwARQBDAEMAQQBWAAIADgBFAEwARQBDAEMAQQBWAAEAGgBTAFIAVgAtAFMAUQBVAEkARAAtAEs
AUgBCAAQAHABlAGwAZQBjAGMAYQB2AC4AdQBuAGUALgBjAHUAAwA4AHMAcgB2AC0AcwBxAHUAaQB
kAC0AawByAGIALgBlAGwAZQBjAGMAYQB2AC4AdQBuAGUALgBjAHUABwAIALqX2txRTNQBAAAAAA=
='

2018/09/14 13:39:18.197 kid1| 29,2| UserRequest.cc(203) authenticate: need
to challenge client
'TlRMTVNTUAACAAAADgAOADgAAAAVgonigQb5TAh6RigAAAAAAAAAAJwAnABGAAAABgEAAAAAAA9
FAEwARQBDAEMAQQBWAAIADgBFAEwARQBDAEMAQQBWAAEAGgBTAFIAVgAtAFMAUQBVAEkARAAtAEs
AUgBCAAQAHABlAGwAZQBjAGMAYQB2AC4AdQBuAGUALgBjAHUAAwA4AHMAcgB2AC0AcwBxAHUAaQB
kAC0AawByAGIALgBlAGwAZQBjAGMAYQB2AC4AdQBuAGUALgBjAHUABwAIALqX2txRTNQBAAAAAA=
='!

2018/09/14 13:39:18| negotiate_wrapper: Return 'TT
TlRMTVNTUAACAAAADgAOADgAAAAVgonig5b0rgxfAqwAAAAAAAAAAJwAnABGAAAABgEAAAAAAA9F
AEwARQBDAEMAQQBWAAIADgBFAEwARQBDAEMAQQBWAAEAGgBTAFIAVgAtAFMAUQBVAEkARAAtAEsA
UgBCAAQAHABlAGwAZQBjAGMAYQB2AC4AdQBuAGUALgBjAHUAAwA4AHMAcgB2AC0AcwBxAHUAaQBk
AC0AawByAGIALgBlAGwAZQBjAGMAYQB2AC4AdQBuAGUALgBjAHUABwAIAD5e29xRTNQBAAAAAA==

'

2018/09/14 13:39:18.202 kid1| 29,4| UserRequest.cc(311) HandleReply: Need to
challenge the client with a server token:
'TlRMTVNTUAACAAAADgAOADgAAAAVgonig5b0rgxfAqwAAAAAAAAAAJwAnABGAAAABgEAAAAAAA9
FAEwARQBDAEMAQQBWAAIADgBFAEwARQBDAEMAQQBWAAEAGgBTAFIAVgAtAFMAUQBVAEkARAAtAEs
AUgBCAAQAHABlAGwAZQBjAGMAYQB2AC4AdQBuAGUALgBjAHUAAwA4AHMAcgB2AC0AcwBxAHUAaQB
kAC0AawByAGIALgBlAGwAZQBjAGMAYQB2AC4AdQBuAGUALgBjAHUABwAIAD5e29xRTNQBAAAAAA=
='

2018/09/14 13:39:18.202 kid1| 29,2| UserRequest.cc(203) authenticate: need
to challenge client
'TlRMTVNTUAACAAAADgAOADgAAAAVgonig5b0rgxfAqwAAAAAAAAAAJwAnABGAAAABgEAAAAAAA9
FAEwARQBDAEMAQQBWAAIADgBFAEwARQBDAEMAQQBWAAEAGgBTAFIAVgAtAFMAUQBVAEkARAAtAEs
AUgBCAAQAHABlAGwAZQBjAGMAYQB2AC4AdQBuAGUALgBjAHUAAwA4AHMAcgB2AC0AcwBxAHUAaQB
kAC0AawByAGIALgBlAGwAZQBjAGMAYQB2AC4AdQBuAGUALgBjAHUABwAIAD5e29xRTNQBAAAAAA=
='!

2018/09/14 13:39:18| negotiate_wrapper: Return 'TT
TlRMTVNTUAACAAAADgAOADgAAAAVgoni/IpqOarkGm0AAAAAAAAAAJwAnABGAAAABgEAAAAAAA9F
AEwARQBDAEMAQQBWAAIADgBFAEwARQBDAEMAQQBWAAEAGgBTAFIAVgAtAFMAUQBVAEkARAAtAEsA
UgBCAAQAHABlAGwAZQBjAGMAYQB2AC4AdQBuAGUALgBjAHUAAwA4AHMAcgB2AC0AcwBxAHUAaQBk
AC0AawByAGIALgBlAGwAZQBjAGMAYQB2AC4AdQBuAGUALgBjAHUABwAIAF7Y3NxRTNQBAAAAAA==

'

2018/09/14 13:39:18.212 kid1| 29,4| UserRequest.cc(311) HandleReply: Need to
challenge the client with a server token:
'TlRMTVNTUAACAAAADgAOADgAAAAVgoni/IpqOarkGm0AAAAAAAAAAJwAnABGAAAABgEAAAAAAA9
FAEwARQBDAEMAQQBWAAIADgBFAEwARQBDAEMAQQBWAAEAGgBTAFIAVgAtAFMAUQBVAEkARAAtAEs
AUgBCAAQAHABlAGwAZQBjAGMAYQB2AC4AdQBuAGUALgBjAHUAAwA4AHMAcgB2AC0AcwBxAHUAaQB
kAC0AawByAGIALgBlAGwAZQBjAGMAYQB2AC4AdQBuAGUALgBjAHUABwAIAF7Y3NxRTNQBAAAAAA=
='

2018/09/14 13:39:18.212 kid1| 29,2| UserRequest.cc(203) authenticate: need
to challenge client
'TlRMTVNTUAACAAAADgAOADgAAAAVgoni/IpqOarkGm0AAAAAAAAAAJwAnABGAAAABgEAAAAAAA9
FAEwARQBDAEMAQQBWAAIADgBFAEwARQBDAEMAQQBWAAEAGgBTAFIAVgAtAFMAUQBVAEkARAAtAEs
AUgBCAAQAHABlAGwAZQBjAGMAYQB2AC4AdQBuAGUALgBjAHUAAwA4AHMAcgB2AC0AcwBxAHUAaQB
kAC0AawByAGIALgBlAGwAZQBjAGMAYQB2AC4AdQBuAGUALgBjAHUABwAIAF7Y3NxRTNQBAAAAAA=
='!

2018/09/14 13:39:18| negotiate_wrapper: Return 'TT
TlRMTVNTUAACAAAADgAOADgAAAAVgoniE4M9MFIcoxQAAAAAAAAAAJwAnABGAAAABgEAAAAAAA9F
AEwARQBDAEMAQQBWAAIADgBFAEwARQBDAEMAQQBWAAEAGgBTAFIAVgAtAFMAUQBVAEkARAAtAEsA
UgBCAAQAHABlAGwAZQBjAGMAYQB2AC4AdQBuAGUALgBjAHUAAwA4AHMAcgB2AC0AcwBxAHUAaQBk
AC0AawByAGIALgBlAGwAZQBjAGMAYQB2AC4AdQBuAGUALgBjAHUABwAIAIoG3dxRTNQBAAAAAA==

'

2018/09/14 13:39:18.213 kid1| 29,4| UserRequest.cc(311) HandleReply: Need to
challenge the client with a server token:
'TlRMTVNTUAACAAAADgAOADgAAAAVgoniE4M9MFIcoxQAAAAAAAAAAJwAnABGAAAABgEAAAAAAA9
FAEwARQBDAEMAQQBWAAIADgBFAEwARQBDAEMAQQBWAAEAGgBTAFIAVgAtAFMAUQBVAEkARAAtAEs
AUgBCAAQAHABlAGwAZQBjAGMAYQB2AC4AdQBuAGUALgBjAHUAAwA4AHMAcgB2AC0AcwBxAHUAaQB
kAC0AawByAGIALgBlAGwAZQBjAGMAYQB2AC4AdQBuAGUALgBjAHUABwAIAIoG3dxRTNQBAAAAAA=
='

2018/09/14 13:39:18.213 kid1| 29,2| UserRequest.cc(203) authenticate: need
to challenge client
'TlRMTVNTUAACAAAADgAOADgAAAAVgoniE4M9MFIcoxQAAAAAAAAAAJwAnABGAAAABgEAAAAAAA9
FAEwARQBDAEMAQQBWAAIADgBFAEwARQBDAEMAQQBWAAEAGgBTAFIAVgAtAFMAUQBVAEkARAAtAEs
AUgBCAAQAHABlAGwAZQBjAGMAYQB2AC4AdQBuAGUALgBjAHUAAwA4AHMAcgB2AC0AcwBxAHUAaQB
kAC0AawByAGIALgBlAGwAZQBjAGMAYQB2AC4AdQBuAGUALgBjAHUABwAIAIoG3dxRTNQBAAAAAA=
='!

2018/09/14 13:39:18| negotiate_wrapper: Return 'TT
TlRMTVNTUAACAAAADgAOADgAAAAVgoniyKjYPBGi9DAAAAAAAAAAAJwAnABGAAAABgEAAAAAAA9F
AEwARQBDAEMAQQBWAAIADgBFAEwARQBDAEMAQQBWAAEAGgBTAFIAVgAtAFMAUQBVAEkARAAtAEsA
UgBCAAQAHABlAGwAZQBjAGMAYQB2AC4AdQBuAGUALgBjAHUAAwA4AHMAcgB2AC0AcwBxAHUAaQBk
AC0AawByAGIALgBlAGwAZQBjAGMAYQB2AC4AdQBuAGUALgBjAHUABwAIADpT4NxRTNQBAAAAAA==

'

2018/09/14 13:39:18.234 kid1| 29,4| UserRequest.cc(311) HandleReply: Need to
challenge the client with a server token:
'TlRMTVNTUAACAAAADgAOADgAAAAVgoniyKjYPBGi9DAAAAAAAAAAAJwAnABGAAAABgEAAAAAAA9
FAEwARQBDAEMAQQBWAAIADgBFAEwARQBDAEMAQQBWAAEAGgBTAFIAVgAtAFMAUQBVAEkARAAtAEs
AUgBCAAQAHABlAGwAZQBjAGMAYQB2AC4AdQBuAGUALgBjAHUAAwA4AHMAcgB2AC0AcwBxAHUAaQB
kAC0AawByAGIALgBlAGwAZQBjAGMAYQB2AC4AdQBuAGUALgBjAHUABwAIADpT4NxRTNQBAAAAAA=
='

2018/09/14 13:39:18.235 kid1| 29,2| UserRequest.cc(203) authenticate: need
to challenge client
'TlRMTVNTUAACAAAADgAOADgAAAAVgoniyKjYPBGi9DAAAAAAAAAAAJwAnABGAAAABgEAAAAAAA9
FAEwARQBDAEMAQQBWAAIADgBFAEwARQBDAEMAQQBWAAEAGgBTAFIAVgAtAFMAUQBVAEkARAAtAEs
AUgBCAAQAHABlAGwAZQBjAGMAYQB2AC4AdQBuAGUALgBjAHUAAwA4AHMAcgB2AC0AcwBxAHUAaQB
kAC0AawByAGIALgBlAGwAZQBjAGMAYQB2AC4AdQBuAGUALgBjAHUABwAIADpT4NxRTNQBAAAAAA=
='!

2018/09/14 13:39:18| negotiate_wrapper: Got 'KK
TlRMTVNTUAADAAAAGAAYAIwAAABIAUgBpAAAAA4ADgBYAAAAEAAQAGYAAAAWABYAdgAAABAAEADs
AQAAFYKI4goAqz8AAAAPTClxtLRmctSfR7SLfnA2O0UATABFAEMAQwBBAFYAYwByAHkAcwB0AGEA
bABsAEMATABJAC0AUgBFAEQARQBTAC0AMQAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAD6IIRx8JJe
sRgbGAVoQxrwAQEAAAAAAABe2NzcUUzUATZJB+HrulrgAAAAAAIADgBFAEwARQBDAEMAQQBWAAEA
GgBTAFIAVgAtAFMAUQBVAEkARAAtAEsAUgBCAAQAHABlAGwAZQBjAGMAYQB2AC4AdQBuAGUALgBj
AHUAAwA4AHMAcgB2AC0AcwBxAHUAaQBkAC0AawByAGIALgBlAGwAZQBjAGMAYQB2AC4AdQBuAGUA
LgBjAHUABwAIAF7Y3NxRTNQBBgAEAAIAAAAIADAAMAAAAAAAAAAAAAAAADAAANSO8haD472JTKKO
F8vBYpu8Z0WdTYbu7c7tqLmb/9ooCgAQAAAAAAAAAAAAAAAAAAAAAAAJACQASABUAFQAUAAvADEA
NwAyAC4AMQA5AC4AMgAyADQALgA0ADYAAAAAAAAAAAAAAAAAyh1ssj6oWg4B+eQjlqv2aA=='
from squid (length: 683).

2018/09/14 13:39:18| negotiate_wrapper: Decode
'TlRMTVNTUAADAAAAGAAYAIwAAABIAUgBpAAAAA4ADgBYAAAAEAAQAGYAAAAWABYAdgAAABAAEAD
sAQAAFYKI4goAqz8AAAAPTClxtLRmctSfR7SLfnA2O0UATABFAEMAQwBBAFYAYwByAHkAcwB0AGE
AbABsAEMATABJAC0AUgBFAEQARQBTAC0AMQAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAD6IIRx8JJ
esRgbGAVoQxrwAQEAAAAAAABe2NzcUUzUATZJB+HrulrgAAAAAAIADgBFAEwARQBDAEMAQQBWAAE
AGgBTAFIAVgAtAFMAUQBVAEkARAAtAEsAUgBCAAQAHABlAGwAZQBjAGMAYQB2AC4AdQBuAGUALgB
jAHUAAwA4AHMAcgB2AC0AcwBxAHUAaQBkAC0AawByAGIALgBlAGwAZQBjAGMAYQB2AC4AdQBuAGU
ALgBjAHUABwAIAF7Y3NxRTNQBBgAEAAIAAAAIADAAMAAAAAAAAAAAAAAAADAAANSO8haD472JTKK
OF8vBYpu8Z0WdTYbu7c7tqLmb/9ooCgAQAAAAAAAAAAAAAAAAAAAAAAAJACQASABUAFQAUAAvADE
ANwAyAC4AMQA5AC4AMgAyADQALgA0ADYAAAAAAAAAAAAAAAAAyh1ssj6oWg4B+eQjlqv2aA=='
(decoded length: 510).

2018/09/14 13:39:18| negotiate_wrapper: received type 3 NTLM token

2018/09/14 13:39:18| negotiate_wrapper: Return 'AF = crystall

'

2018/09/14 13:39:18.297 kid1| 29,4| UserRequest.cc(336) HandleReply:
authenticated user crystall

2018/09/14 13:39:18.297 kid1| 29,4| UserRequest.cc(355) HandleReply:
Successfully validated user via Negotiate. Username 'crystall'

2018/09/14 13:39:18.297 kid1| 29,2| User.cc(227) addIp: user 'crystall' has
been seen at a new IP address (192.168.0.2:53116)

2018/09/14 13:39:18| negotiate_wrapper: Got 'KK
TlRMTVNTUAADAAAAGAAYAIwAAABIAUgBpAAAAA4ADgBYAAAAEAAQAGYAAAAWABYAdgAAABAAEADs
AQAAFYKI4goAqz8AAAAPNdmk9QQok2ZtAJi07Aft0EUATABFAEMAQwBBAFYAYwByAHkAcwB0AGEA
bABsAEMATABJAC0AUgBFAEQARQBTAC0AMQAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAIdVVkN6Ul
IPyfEZl+tFbZAQEAAAAAAAA6U+DcUUzUAbDPblk1F39AAAAAAAIADgBFAEwARQBDAEMAQQBWAAEA
GgBTAFIAVgAtAFMAUQBVAEkARAAtAEsAUgBCAAQAHABlAGwAZQBjAGMAYQB2AC4AdQBuAGUALgBj
AHUAAwA4AHMAcgB2AC0AcwBxAHUAaQBkAC0AawByAGIALgBlAGwAZQBjAGMAYQB2AC4AdQBuAGUA
LgBjAHUABwAIADpT4NxRTNQBBgAEAAIAAAAIADAAMAAAAAAAAAAAAAAAADAAANSO8haD472JTKKO
F8vBYpu8Z0WdTYbu7c7tqLmb/9ooCgAQAAAAAAAAAAAAAAAAAAAAAAAJACQASABUAFQAUAAvADEA
NwAyAC4AMQA5AC4AMgAyADQALgA0ADYAAAAAAAAAAAAAAAAASlVupH80E90xsICozM0MDw=='
from squid (length: 683).

2018/09/14 13:39:18| negotiate_wrapper: Decode
'TlRMTVNTUAADAAAAGAAYAIwAAABIAUgBpAAAAA4ADgBYAAAAEAAQAGYAAAAWABYAdgAAABAAEAD
sAQAAFYKI4goAqz8AAAAPNdmk9QQok2ZtAJi07Aft0EUATABFAEMAQwBBAFYAYwByAHkAcwB0AGE
AbABsAEMATABJAC0AUgBFAEQARQBTAC0AMQAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAIdVVkN6U
lIPyfEZl+tFbZAQEAAAAAAAA6U+DcUUzUAbDPblk1F39AAAAAAAIADgBFAEwARQBDAEMAQQBWAAE
AGgBTAFIAVgAtAFMAUQBVAEkARAAtAEsAUgBCAAQAHABlAGwAZQBjAGMAYQB2AC4AdQBuAGUALgB
jAHUAAwA4AHMAcgB2AC0AcwBxAHUAaQBkAC0AawByAGIALgBlAGwAZQBjAGMAYQB2AC4AdQBuAGU
ALgBjAHUABwAIADpT4NxRTNQBBgAEAAIAAAAIADAAMAAAAAAAAAAAAAAAADAAANSO8haD472JTKK
OF8vBYpu8Z0WdTYbu7c7tqLmb/9ooCgAQAAAAAAAAAAAAAAAAAAAAAAAJACQASABUAFQAUAAvADE
ANwAyAC4AMQA5AC4AMgAyADQALgA0ADYAAAAAAAAAAAAAAAAASlVupH80E90xsICozM0MDw=='
(decoded length: 510).

2018/09/14 13:39:18| negotiate_wrapper: received type 3 NTLM token

2018/09/14 13:39:18| negotiate_wrapper: Got 'KK
TlRMTVNTUAADAAAAGAAYAIwAAABIAUgBpAAAAA4ADgBYAAAAEAAQAGYAAAAWABYAdgAAABAAEADs
AQAAFYKI4goAqz8AAAAPktrvZwcp20ZMz2vUT1MqrEUATABFAEMAQwBBAFYAYwByAHkAcwB0AGEA
bABsAEMATABJAC0AUgBFAEQARQBTAC0AMQAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAC/ktNdTT5c
V4Jw+7d4icVSAQEAAAAAAAA+XtvcUUzUAQvLOUptjrxtAAAAAAIADgBFAEwARQBDAEMAQQBWAAEA
GgBTAFIAVgAtAFMAUQBVAEkARAAtAEsAUgBCAAQAHABlAGwAZQBjAGMAYQB2AC4AdQBuAGUALgBj
AHUAAwA4AHMAcgB2AC0AcwBxAHUAaQBkAC0AawByAGIALgBlAGwAZQBjAGMAYQB2AC4AdQBuAGUA
LgBjAHUABwAIAD5e29xRTNQBBgAEAAIAAAAIADAAMAAAAAAAAAAAAAAAADAAANSO8haD472JTKKO
F8vBYpu8Z0WdTYbu7c7tqLmb/9ooCgAQAAAAAAAAAAAAAAAAAAAAAAAJACQASABUAFQAUAAvADEA
NwAyAC4AMQA5AC4AMgAyADQALgA0ADYAAAAAAAAAAAAAAAAAC9hZLo1JeXWlUlkutHco2Q=='
from squid (length: 683).

2018/09/14 13:39:18| negotiate_wrapper: Decode
'TlRMTVNTUAADAAAAGAAYAIwAAABIAUgBpAAAAA4ADgBYAAAAEAAQAGYAAAAWABYAdgAAABAAEAD
sAQAAFYKI4goAqz8AAAAPktrvZwcp20ZMz2vUT1MqrEUATABFAEMAQwBBAFYAYwByAHkAcwB0AGE
AbABsAEMATABJAC0AUgBFAEQARQBTAC0AMQAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAC/ktNdTT5
cV4Jw+7d4icVSAQEAAAAAAAA+XtvcUUzUAQvLOUptjrxtAAAAAAIADgBFAEwARQBDAEMAQQBWAAE
AGgBTAFIAVgAtAFMAUQBVAEkARAAtAEsAUgBCAAQAHABlAGwAZQBjAGMAYQB2AC4AdQBuAGUALgB
jAHUAAwA4AHMAcgB2AC0AcwBxAHUAaQBkAC0AawByAGIALgBlAGwAZQBjAGMAYQB2AC4AdQBuAGU
ALgBjAHUABwAIAD5e29xRTNQBBgAEAAIAAAAIADAAMAAAAAAAAAAAAAAAADAAANSO8haD472JTKK
OF8vBYpu8Z0WdTYbu7c7tqLmb/9ooCgAQAAAAAAAAAAAAAAAAAAAAAAAJACQASABUAFQAUAAvADE
ANwAyAC4AMQA5AC4AMgAyADQALgA0ADYAAAAAAAAAAAAAAAAAC9hZLo1JeXWlUlkutHco2Q=='
(decoded length: 510).

2018/09/14 13:39:18| negotiate_wrapper: received type 3 NTLM token

2018/09/14 13:39:18| negotiate_wrapper: Got 'KK
TlRMTVNTUAADAAAAGAAYAIwAAABIAUgBpAAAAA4ADgBYAAAAEAAQAGYAAAAWABYAdgAAABAAEADs
AQAAFYKI4goAqz8AAAAPcvqx2ByNCA8nHjzEmlCuRkUATABFAEMAQwBBAFYAYwByAHkAcwB0AGEA
bABsAEMATABJAC0AUgBFAEQARQBTAC0AMQAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAV9wKDCHbA
PUCj0iTVPM9cAQEAAAAAAACKBt3cUUzUAQ3p911SxBpmAAAAAAIADgBFAEwARQBDAEMAQQBWAAEA
GgBTAFIAVgAtAFMAUQBVAEkARAAtAEsAUgBCAAQAHABlAGwAZQBjAGMAYQB2AC4AdQBuAGUALgBj
AHUAAwA4AHMAcgB2AC0AcwBxAHUAaQBkAC0AawByAGIALgBlAGwAZQBjAGMAYQB2AC4AdQBuAGUA
LgBjAHUABwAIAIoG3dxRTNQBBgAEAAIAAAAIADAAMAAAAAAAAAAAAAAAADAAANSO8haD472JTKKO
F8vBYpu8Z0WdTYbu7c7tqLmb/9ooCgAQAAAAAAAAAAAAAAAAAAAAAAAJACQASABUAFQAUAAvADEA
NwAyAC4AMQA5AC4AMgAyADQALgA0ADYAAAAAAAAAAAAAAAAAWt0nSXk+Ix4DbAvzOrubNA=='
from squid (length: 683).

2018/09/14 13:39:18| negotiate_wrapper: Decode
'TlRMTVNTUAADAAAAGAAYAIwAAABIAUgBpAAAAA4ADgBYAAAAEAAQAGYAAAAWABYAdgAAABAAEAD
sAQAAFYKI4goAqz8AAAAPcvqx2ByNCA8nHjzEmlCuRkUATABFAEMAQwBBAFYAYwByAHkAcwB0AGE
AbABsAEMATABJAC0AUgBFAEQARQBTAC0AMQAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAV9wKDCHb
APUCj0iTVPM9cAQEAAAAAAACKBt3cUUzUAQ3p911SxBpmAAAAAAIADgBFAEwARQBDAEMAQQBWAAE
AGgBTAFIAVgAtAFMAUQBVAEkARAAtAEsAUgBCAAQAHABlAGwAZQBjAGMAYQB2AC4AdQBuAGUALgB
jAHUAAwA4AHMAcgB2AC0AcwBxAHUAaQBkAC0AawByAGIALgBlAGwAZQBjAGMAYQB2AC4AdQBuAGU
ALgBjAHUABwAIAIoG3dxRTNQBBgAEAAIAAAAIADAAMAAAAAAAAAAAAAAAADAAANSO8haD472JTKK
OF8vBYpu8Z0WdTYbu7c7tqLmb/9ooCgAQAAAAAAAAAAAAAAAAAAAAAAAJACQASABUAFQAUAAvADE
ANwAyAC4AMQA5AC4AMgAyADQALgA0ADYAAAAAAAAAAAAAAAAAWt0nSXk+Ix4DbAvzOrubNA=='
(decoded length: 510).

2018/09/14 13:39:18| negotiate_wrapper: received type 3 NTLM token

2018/09/14 13:39:18| negotiate_wrapper: Got 'KK
TlRMTVNTUAADAAAAGAAYAIwAAABIAUgBpAAAAA4ADgBYAAAAEAAQAGYAAAAWABYAdgAAABAAEADs
AQAAFYKI4goAqz8AAAAPsug30D9/WWwwJJE0C5LgOUUATABFAEMAQwBBAFYAYwByAHkAcwB0AGEA
bABsAEMATABJAC0AUgBFAEQARQBTAC0AMQAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAABB9ekE7/+
TbqkYU6Gx64qAQEAAAAAAAC6l9rcUUzUAS71gyhoa7SHAAAAAAIADgBFAEwARQBDAEMAQQBWAAEA
GgBTAFIAVgAtAFMAUQBVAEkARAAtAEsAUgBCAAQAHABlAGwAZQBjAGMAYQB2AC4AdQBuAGUALgBj
AHUAAwA4AHMAcgB2AC0AcwBxAHUAaQBkAC0AawByAGIALgBlAGwAZQBjAGMAYQB2AC4AdQBuAGUA
LgBjAHUABwAIALqX2txRTNQBBgAEAAIAAAAIADAAMAAAAAAAAAAAAAAAADAAANSO8haD472JTKKO
F8vBYpu8Z0WdTYbu7c7tqLmb/9ooCgAQAAAAAAAAAAAAAAAAAAAAAAAJACQASABUAFQAUAAvADEA
NwAyAC4AMQA5AC4AMgAyADQALgA0ADYAAAAAAAAAAAAAAAAA/6MII/C9uGEWH4s9EE+W/g=='
from squid (length: 683).

2018/09/14 13:39:18| negotiate_wrapper: Decode
'TlRMTVNTUAADAAAAGAAYAIwAAABIAUgBpAAAAA4ADgBYAAAAEAAQAGYAAAAWABYAdgAAABAAEAD
sAQAAFYKI4goAqz8AAAAPsug30D9/WWwwJJE0C5LgOUUATABFAEMAQwBBAFYAYwByAHkAcwB0AGE
AbABsAEMATABJAC0AUgBFAEQARQBTAC0AMQAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAABB9ekE7/
+TbqkYU6Gx64qAQEAAAAAAAC6l9rcUUzUAS71gyhoa7SHAAAAAAIADgBFAEwARQBDAEMAQQBWAAE
AGgBTAFIAVgAtAFMAUQBVAEkARAAtAEsAUgBCAAQAHABlAGwAZQBjAGMAYQB2AC4AdQBuAGUALgB
jAHUAAwA4AHMAcgB2AC0AcwBxAHUAaQBkAC0AawByAGIALgBlAGwAZQBjAGMAYQB2AC4AdQBuAGU
ALgBjAHUABwAIALqX2txRTNQBBgAEAAIAAAAIADAAMAAAAAAAAAAAAAAAADAAANSO8haD472JTKK
OF8vBYpu8Z0WdTYbu7c7tqLmb/9ooCgAQAAAAAAAAAAAAAAAAAAAAAAAJACQASABUAFQAUAAvADE
ANwAyAC4AMQA5AC4AMgAyADQALgA0ADYAAAAAAAAAAAAAAAAA/6MII/C9uGEWH4s9EE+W/g=='
(decoded length: 510).

2018/09/14 13:39:18| negotiate_wrapper: received type 3 NTLM token

2018/09/14 13:39:18| negotiate_wrapper: Return 'AF = crystall

'

2018/09/14 13:39:18.326 kid1| 29,4| UserRequest.cc(336) HandleReply:
authenticated user crystall

2018/09/14 13:39:18.326 kid1| 29,4| UserRequest.cc(355) HandleReply:
Successfully validated user via Negotiate. Username 'crystall'

2018/09/14 13:39:18| negotiate_wrapper: Return 'AF = crystall

'

2018/09/14 13:39:18.331 kid1| 29,4| UserRequest.cc(336) HandleReply:
authenticated user crystall

2018/09/14 13:39:18.331 kid1| 29,4| UserRequest.cc(355) HandleReply:
Successfully validated user via Negotiate. Username 'crystall'

2018/09/14 13:39:18| negotiate_wrapper: Return 'AF = crystall

'

2018/09/14 13:39:18.335 kid1| 29,4| UserRequest.cc(336) HandleReply:
authenticated user crystall

2018/09/14 13:39:18.335 kid1| 29,4| UserRequest.cc(355) HandleReply:
Successfully validated user via Negotiate. Username 'crystall'

2018/09/14 13:39:18| negotiate_wrapper: Return 'AF = crystall

'

2018/09/14 13:39:18.340 kid1| 29,4| UserRequest.cc(336) HandleReply:
authenticated user crystall

2018/09/14 13:39:18.340 kid1| 29,4| UserRequest.cc(355) HandleReply:
Successfully validated user via Negotiate. Username 'crystall'

2018/09/14 13:39:18.340 kid1| ICP is disabled! Cannot send ICP request to
peer.

2018/09/14 13:39:18.340 kid1| ICP is disabled! Cannot send ICP request to
peer.

2018/09/14 13:39:18.779 kid1| 29,4| UserRequest.cc(294) authenticate: No
Proxy-Auth header and no working alternative. Requesting auth header.

2018/09/14 13:39:18.782 kid1| 29,4| UserRequest.cc(354) authenticate: No
connection authentication type

2018/09/14 13:39:18| negotiate_wrapper: Got 'YR
TlRMTVNTUAABAAAAl4II4gAAAAAAAAAAAAAAAAAAAAAKAKs/AAAADw==' from squid
(length: 59).

2018/09/14 13:39:18| negotiate_wrapper: Decode
'TlRMTVNTUAABAAAAl4II4gAAAAAAAAAAAAAAAAAAAAAKAKs/AAAADw==' (decoded length:
42).

2018/09/14 13:39:18| negotiate_wrapper: received type 1 NTLM token

2018/09/14 13:39:18| negotiate_wrapper: Return 'TT
TlRMTVNTUAACAAAADgAOADgAAAAVgoni6re6l3Xwbr4AAAAAAAAAAJwAnABGAAAABgEAAAAAAA9F
AEwARQBDAEMAQQBWAAIADgBFAEwARQBDAEMAQQBWAAEAGgBTAFIAVgAtAFMAUQBVAEkARAAtAEsA
UgBCAAQAHABlAGwAZQBjAGMAYQB2AC4AdQBuAGUALgBjAHUAAwA4AHMAcgB2AC0AcwBxAHUAaQBk
AC0AawByAGIALgBlAGwAZQBjAGMAYQB2AC4AdQBuAGUALgBjAHUABwAIANJNNN1RTNQBAAAAAA==

'

2018/09/14 13:39:18.785 kid1| 29,4| UserRequest.cc(311) HandleReply: Need to
challenge the client with a server token:
'TlRMTVNTUAACAAAADgAOADgAAAAVgoni6re6l3Xwbr4AAAAAAAAAAJwAnABGAAAABgEAAAAAAA9
FAEwARQBDAEMAQQBWAAIADgBFAEwARQBDAEMAQQBWAAEAGgBTAFIAVgAtAFMAUQBVAEkARAAtAEs
AUgBCAAQAHABlAGwAZQBjAGMAYQB2AC4AdQBuAGUALgBjAHUAAwA4AHMAcgB2AC0AcwBxAHUAaQB
kAC0AawByAGIALgBlAGwAZQBjAGMAYQB2AC4AdQBuAGUALgBjAHUABwAIANJNNN1RTNQBAAAAAA=
='

2018/09/14 13:39:18.785 kid1| 29,2| UserRequest.cc(203) authenticate: need
to challenge client
'TlRMTVNTUAACAAAADgAOADgAAAAVgoni6re6l3Xwbr4AAAAAAAAAAJwAnABGAAAABgEAAAAAAA9
FAEwARQBDAEMAQQBWAAIADgBFAEwARQBDAEMAQQBWAAEAGgBTAFIAVgAtAFMAUQBVAEkARAAtAEs
AUgBCAAQAHABlAGwAZQBjAGMAYQB2AC4AdQBuAGUALgBjAHUAAwA4AHMAcgB2AC0AcwBxAHUAaQB
kAC0AawByAGIALgBlAGwAZQBjAGMAYQB2AC4AdQBuAGUALgBjAHUABwAIANJNNN1RTNQBAAAAAA=
='!

2018/09/14 13:39:18| negotiate_wrapper: Got 'KK
TlRMTVNTUAADAAAAGAAYAIwAAABIAUgBpAAAAA4ADgBYAAAAEAAQAGYAAAAWABYAdgAAABAAEADs
AQAAFYKI4goAqz8AAAAPZToO29GZi9mTSaZo7kC+uEUATABFAEMAQwBBAFYAYwByAHkAcwB0AGEA
bABsAEMATABJAC0AUgBFAEQARQBTAC0AMQAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAADMCXZljnEc
JGfczvMrEXsbAQEAAAAAAADSTTTdUUzUATkI4mevwzXdAAAAAAIADgBFAEwARQBDAEMAQQBWAAEA
GgBTAFIAVgAtAFMAUQBVAEkARAAtAEsAUgBCAAQAHABlAGwAZQBjAGMAYQB2AC4AdQBuAGUALgBj
AHUAAwA4AHMAcgB2AC0AcwBxAHUAaQBkAC0AawByAGIALgBlAGwAZQBjAGMAYQB2AC4AdQBuAGUA
LgBjAHUABwAIANJNNN1RTNQBBgAEAAIAAAAIADAAMAAAAAAAAAAAAAAAADAAANSO8haD472JTKKO
F8vBYpu8Z0WdTYbu7c7tqLmb/9ooCgAQAAAAAAAAAAAAAAAAAAAAAAAJACQASABUAFQAUAAvADEA
NwAyAC4AMQA5AC4AMgAyADQALgA0ADYAAAAAAAAAAAAAAAAAxTDFbTI2R1oQS5sjProTRQ=='
from squid (length: 683).

2018/09/14 13:39:18| negotiate_wrapper: Decode
'TlRMTVNTUAADAAAAGAAYAIwAAABIAUgBpAAAAA4ADgBYAAAAEAAQAGYAAAAWABYAdgAAABAAEAD
sAQAAFYKI4goAqz8AAAAPZToO29GZi9mTSaZo7kC+uEUATABFAEMAQwBBAFYAYwByAHkAcwB0AGE
AbABsAEMATABJAC0AUgBFAEQARQBTAC0AMQAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAADMCXZljnE
cJGfczvMrEXsbAQEAAAAAAADSTTTdUUzUATkI4mevwzXdAAAAAAIADgBFAEwARQBDAEMAQQBWAAE
AGgBTAFIAVgAtAFMAUQBVAEkARAAtAEsAUgBCAAQAHABlAGwAZQBjAGMAYQB2AC4AdQBuAGUALgB
jAHUAAwA4AHMAcgB2AC0AcwBxAHUAaQBkAC0AawByAGIALgBlAGwAZQBjAGMAYQB2AC4AdQBuAGU
ALgBjAHUABwAIANJNNN1RTNQBBgAEAAIAAAAIADAAMAAAAAAAAAAAAAAAADAAANSO8haD472JTKK
OF8vBYpu8Z0WdTYbu7c7tqLmb/9ooCgAQAAAAAAAAAAAAAAAAAAAAAAAJACQASABUAFQAUAAvADE
ANwAyAC4AMQA5AC4AMgAyADQALgA0ADYAAAAAAAAAAAAAAAAAxTDFbTI2R1oQS5sjProTRQ=='
(decoded length: 510).

2018/09/14 13:39:18| negotiate_wrapper: received type 3 NTLM token

2018/09/14 13:39:18| negotiate_wrapper: Return 'AF = crystall

 

 

Access.log

1536946843.113  66541 192.168.0.2 TCP_TUNNEL/200 3806 CONNECT
www.facebook.com:443 <http://www.facebook.com:443>  crystall
FIRSTUP_PARENT/PARENT_PROXY_IP

 

The question is, that only NTLM works, I've tried with Internet Explorer,
Google Chrome and Firefox. The other thing is that he never asks for
username and password, he uses the user credentials that he initiates
session to work (I do not know if this is the correct operation).

What could be happening?

 

 

                Sorry for the long email.

 

 

Gretting Yanier

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20180918/c6710c60/attachment.htm>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: image001.jpg
Type: image/jpeg
Size: 4312 bytes
Desc: not available
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20180918/c6710c60/attachment.jpg>

From Andreas.Doerfler at kempten.de  Tue Sep 18 13:31:49 2018
From: Andreas.Doerfler at kempten.de (=?utf-8?B?RMO2cmZsZXIsIEFuZHJlYXM=?=)
Date: Tue, 18 Sep 2018 13:31:49 +0000
Subject: [squid-users] TCP_MISS/502 - audio stream - none default http
 ports
In-Reply-To: <vmime.5ba0f76b.58bf.3dd501ed6b716e9b@ms249-lin-003.rotterdam.bazuin.nl>
References: <1537272426.27318.18.camel@kempten.de>
 <vmime.5ba0f76b.58bf.3dd501ed6b716e9b@ms249-lin-003.rotterdam.bazuin.nl>
Message-ID: <1537277509.27318.30.camel@kempten.de>

hey louis,

thanks for you help and time!

found the problem, since there are firewalls in between, i forgot check
a specific point, and well, it was one of the firewalls, no wonder it
was driving me nuts. 
one of those double facepalm moments once i realized it.

> And thank you for the music link, something different then the radio here. ;-) 

no problem, was the first random google search result ;-)

> : acl SSL_ports port 443
> ## PS in your config you did miss to add the extra SSL_Ports also to the Safe_port. 

na, actualy there are 3 ports configured :D


> ## And here you missed the "allow localnet" 

"http_access allow all" makes localnet obsolete, access restrictions are
handled by a firewall.


> If you want the 4.2 for stretch, you can find it here : 
> https://downloads.van-belle.nl/squid/squid4.2/ 
> Buildlogs are all included, or rebuild it yourself from sid/testing. 
> Its a pretty easy rebuild imo. 

i'll check that the next days from a private server, thanks a bunch :D.

have a nice one,
andy
-------------- next part --------------
A non-text attachment was scrubbed...
Name: smime.p7s
Type: application/x-pkcs7-signature
Size: 6343 bytes
Desc: not available
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20180918/a7947e47/attachment.bin>

From service.mv at gmail.com  Tue Sep 18 13:54:15 2018
From: service.mv at gmail.com (neok)
Date: Tue, 18 Sep 2018 08:54:15 -0500 (CDT)
Subject: [squid-users] Help: squid restarts and squidGuard die
In-Reply-To: <f8f51c90-6bd7-09d0-a90b-c9552748d813@treenet.co.nz>
References: <CA+d==oEt8J-ie2Fm1SYrHSBKD6Y7TRF-v3iGiMXC7WQ36WV1vw@mail.gmail.com>
 <f8f51c90-6bd7-09d0-a90b-c9552748d813@treenet.co.nz>
Message-ID: <1537278855873-0.post@n4.nabble.com>

Thank you very much Amos for putting me in the right direction.
I successfully carried out the modifications you indicated to me.
Regarding ufdbGuard, if I understood correctly, what you recommend is to use
the ufdbConvertDB tool to convert my blacklists in plain text to the
ufdbGuard database format? And then use that/those databases in normal squid
ACL's?



--
Sent from: http://squid-web-proxy-cache.1019090.n4.nabble.com/Squid-Users-f1019091.html


From vh1988 at yahoo.com.ar  Tue Sep 18 15:11:25 2018
From: vh1988 at yahoo.com.ar (Julian Perconti)
Date: Tue, 18 Sep 2018 12:11:25 -0300
Subject: [squid-users] About SSL peek-n-splice/bump configurations
In-Reply-To: <ee42d35f-2dc0-3bbc-f6c4-a1852374a832@measurement-factory.com>
References: <000301d43289$14ed4770$3ec7d650$@yahoo.com.ar>
 <001601d4464c$d1b38910$751a9b30$@yahoo.com.ar>
 <770057a5-3808-3fdc-5bf7-f78d09705913@treenet.co.nz>
 <002001d446cc$148bb580$3da32080$@yahoo.com.ar>
 <c7a4d318-c335-03de-9e5c-73fc258e38c0@treenet.co.nz>
 <008001d4479b$b22b9290$1682b7b0$@yahoo.com.ar>
 <04c7931e-cb29-80d1-2a2a-8a1ccb4761f5@treenet.co.nz>
 <019301d4486b$049ebf00$0ddc3d00$@yahoo.com.ar>
 <6b762c34-f0c4-2d9a-6616-432305af160b@treenet.co.nz>
 <009d01d44935$1648ef80$42dace80$@yahoo.com.ar>
 <5400815d-9a82-97a9-5889-0e76c04a9ba0@measurement-factory.com>
 <003501d44aa4$d784e5d0$868eb170$@yahoo.com.ar>
 <500fbad5-d277-965f-d29a-a81c2f343d2e@measurement-factory.com>
 <004401d44b0e$46cf40c0$d46dc240$@yahoo.com.ar>
 <0472b838-415e-e4d2-c71a-7fbce45de926@measurement-factory.com>
 <008e01d44bbf$ccb41e20$661c5a60$@yahoo.com.ar>
 <2b2e8b70-7f45-039c-462f-ba78b2fc1563@measurement-factory.com>
 <006d01d44eaf$55ecc290$01c647b0$@yahoo.com.ar>
 <ee42d35f-2dc0-3bbc-f6c4-a1852374a832@measurement-factory.com>
Message-ID: <003601d44f61$dfd30e80$9f792b80$@yahoo.com.ar>

> Both loops can finish "early" (i.e. before three steps and/or before all
> configured rules are evaluated).

Yes, maybe I would have should say at least: "Well in really, depend on the rules.." Especially in the inner loop.
But I pointed to the maximum possibilities. (if exists)

> Just to avoid misunderstanding: Final actions may be taken at any step, but
> only final actions are possible at step3.

Good point.
My mistake, I forgot that. 
In fact, in the actions table its clear that a final action like terminate can occurs at any step and even worst, any action can occurs at step1.
>From another point of view: at step3 only final actions are allowed.

> > I think that splice at step1 does not make sense according to the doc.
> > and also to the order of steps or the sequence, about how the rules
> > are evaluated.
> 
> I do not know what you mean. Splice at step1 is certainly possible and even
> recommended for known non-TLS traffic.

Idem; same comments as above.

> > the thing that really does not makes sense is splice at step1 and then splice
> at step2:
> 
> It is not possible to splice twice. Splicing is one of the final actions. No other
> action follows a final action (by definition). Search for the two "exit" words in
> the loop summary to find where final actions may be applied.

So, if a rule "x" match a splice action at inner loop when the outer loop starts, then take the final action for the rule "x" and if no there is no more rules at step1 exit and proceed to evaluate the rules for the step2.
Some like that?

> The best thing to do depends on your goals and the transaction. Splicing at
> step1, step2, OR step3 makes sense in some cases and does not make sense
> (or is impossible) in others.
> 
> You need to evaluate your rules in the context of a specific transaction
> though: The same set of ssl_bump rules may splice transaction A at step1 and
> transaction C at step3. The loops summarized above are executed from
> scratch for every transaction that reaches ssl_bump directive evaluation.

I lost You here.

> It is impossible to make "splice or bump" decision at step3 because splicing at
> step3 requires peeking at step2 while bumping at step3 requires staring at
> step2. In a context of a single transaction, it is impossible to both peek and
> stare at the same time!
> Thus, you essentially have to make that "splice or bump" decision earlier, at
> step1 or step2, when you have less information than you would have at
> step3. It is almost like the dominant quantum physics theory -- by measuring
> at step2, you determine the outcome of that measurement (i.e. available
> actions at step3).

Wait, maybe I do not explain myself well or I don not understand what do You want to mean; the ACL at every step are not the same. See below.

> >    ssl_bump peek noBumpSites
> >    ssl_bump stare

It suppsed that here due to acl in the first line, squid will bump later, all except sites that matches the acl.

> > But, what happen if Squid decides automagically wrong? Or something
> does not match...?
> 
> I do not know what you mean by "Squid decides automagically wrong"

Well, it was just an (probably bad one) idea/thought.

> At step1 and at step2, if noBumpSites matches, then Squid will peek.
Therefore default splice...

> At step1 and at step2, if noBumpSites does not match, then Squid will stare.
...and default bump.

> At step3, no explicit rules can match so Squid will either splice or bump,
> depending on whether noBumpSites matched at step2.

Yes, just an aclaration: in this specific case "At step3, no explicit rules can match" (not anymore). 
All was already done in the previous steps.

> > Do You think that the above rules is more-or-less the more nearest
> > what I want to do? Excuse me but, I think that at this stage, I gues
> > that You already know what I mean when I say "...what I want to do?"
> 
> Sorry, I do not. And since there are many details that define what one wants
> to (or should) do, it may be impractical to relay all of them on an informal
> email thread. However, if you understand how SslBump rules work, then you
> can either answer a vague "Am I doing what I want to be doing?" question
> yourself or ask more specific questions that can be answered on the mailing
> list.

Ok I am sorry for that, I understood that You had an idea about what I want to do, in a earlier message.
I will answer with a "little" change in the last config to illustrate what I should/want to do in my scenario.

It was: (Again: With this cfg I dont see any domain in TCP_TUNNEL neither the Security alerts..)

   ssl_bump peek noBumpSites # Here two steps will happen. And final action (splice)  happens at step3 by default.
   ssl_bump stare

And now is: (And with this I see the domain:443 in TCP_TUNNEL and Security alerts about the domain and ip match in the logs.)

  ssl_bump splice noBumpSites # This line reachs a splice rule at step1 and that is a final action, without future proccesing.
  ssl_bump stare

Squid is telling to the client (in last config.) :
"I will not touch any TLS byte. I can not do much about the security risk you may run. Despite what I said, I will do as many checks as possible then You will be connected..."

Maybe splice at step1 I am leaving the client alone. But, what can I do? 
I have to choose: peek at 1 and 2 like before (and errors can happen; that is why I have reopened the thread... the cause was an error message from squid while a client was trying to access a bank site) or just splice (less checks,maybe less errors).

Thank You again.



From rousskov at measurement-factory.com  Tue Sep 18 16:36:10 2018
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Tue, 18 Sep 2018 10:36:10 -0600
Subject: [squid-users] About SSL peek-n-splice/bump configurations
In-Reply-To: <003601d44f61$dfd30e80$9f792b80$@yahoo.com.ar>
References: <000301d43289$14ed4770$3ec7d650$@yahoo.com.ar>
 <002001d446cc$148bb580$3da32080$@yahoo.com.ar>
 <c7a4d318-c335-03de-9e5c-73fc258e38c0@treenet.co.nz>
 <008001d4479b$b22b9290$1682b7b0$@yahoo.com.ar>
 <04c7931e-cb29-80d1-2a2a-8a1ccb4761f5@treenet.co.nz>
 <019301d4486b$049ebf00$0ddc3d00$@yahoo.com.ar>
 <6b762c34-f0c4-2d9a-6616-432305af160b@treenet.co.nz>
 <009d01d44935$1648ef80$42dace80$@yahoo.com.ar>
 <5400815d-9a82-97a9-5889-0e76c04a9ba0@measurement-factory.com>
 <003501d44aa4$d784e5d0$868eb170$@yahoo.com.ar>
 <500fbad5-d277-965f-d29a-a81c2f343d2e@measurement-factory.com>
 <004401d44b0e$46cf40c0$d46dc240$@yahoo.com.ar>
 <0472b838-415e-e4d2-c71a-7fbce45de926@measurement-factory.com>
 <008e01d44bbf$ccb41e20$661c5a60$@yahoo.com.ar>
 <2b2e8b70-7f45-039c-462f-ba78b2fc1563@measurement-factory.com>
 <006d01d44eaf$55ecc290$01c647b0$@yahoo.com.ar>
 <ee42d35f-2dc0-3bbc-f6c4-a1852374a832@measurement-factory.com>
 <003601d44f61$dfd30e80$9f792b80$@yahoo.com.ar>
Message-ID: <8c393e26-822f-ddea-50d4-9f80ebc9087f@measurement-factory.com>

On 09/18/2018 09:11 AM, Julian Perconti wrote:
>>> the thing that really does not makes sense is splice at step1 and then splice
>>> at step2

>> It is not possible to splice twice. Splicing is one of the final actions. No other
>> action follows a final action (by definition).

> So, if a rule "x" match a splice action at inner loop when the outer
> loop starts, then take the final action for the rule "x" and if no
> there is no more rules at step1 exit and proceed to evaluate the
> rules for the step2.

I think it is possible to interpret your summary as a correct statement,
but the reality is much simpler: After a splice rule is applied, SslBump
is over. No more rules are checked. No more loops are iterated. Squid
simply "exits" the SslBump feature (and becomes a TCP tunnel).


>>>    ssl_bump peek noBumpSites
>>>    ssl_bump stare


>> At step1 and at step2, if noBumpSites matches, then Squid will peek.

> Therefore default splice...

If noBumpSites matches at step2, then, yes, Squid will splice at step3
by default. Otherwise, no; Squid will bump at step3 by default.


>> At step1 and at step2, if noBumpSites does not match, then Squid will stare.

> ...and default bump.

If noBumpSites does not match at step2, then, yes, Squid will bump at
step3 by default. Otherwise, no; Squid will splice at step3 by default.


>   ssl_bump splice noBumpSites # This line reachs a splice rule at step1
>   ssl_bump stare

> Squid is telling to the client: "I will not touch any TLS byte. [...]
> I will do as many checks as possible then You will be connected..."

The configuration above does not match your summary because the
configuration has a "stare" action that may run at (step1 and) step2
(and, hence, a possibility of the bump action at step3). Staring at
step2 and bumping (at any step) modify TLS bytes, of course.

Perhaps your summary only applies to the cases where noBumpSites matches
(either at step1 or at step2), but the summary did not make that clear.
There is a big difference between explaining Squid actions for a
particular transaction and summarizing what a particular configuration
means (for all transactions). Unless noted otherwise, I am focusing on
the latter.

AFAICT, the primary difference between

  ssl_bump peek noBumpSites
  ssl_bump stare

and

  ssl_bump splice noBumpSites
  ssl_bump stare

is that the former requires a noBumpSites match at step2 for the
connections to be spliced. The latter does not require that; it is even
content with splicing based on step1 (i.e. TCP/IP) info, before knowing
step2 (i.e. the TLS client) details. You can see those differences when,
for example, noBumpSites does not match at step1.

The other differences include whether the TLS client- and
server-provided information is checked and logged in all cases or just
in some cases.

Alex.


From squid3 at treenet.co.nz  Wed Sep 19 02:03:39 2018
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Wed, 19 Sep 2018 14:03:39 +1200
Subject: [squid-users] Help: squid restarts and squidGuard die
In-Reply-To: <1537278855873-0.post@n4.nabble.com>
References: <CA+d==oEt8J-ie2Fm1SYrHSBKD6Y7TRF-v3iGiMXC7WQ36WV1vw@mail.gmail.com>
 <f8f51c90-6bd7-09d0-a90b-c9552748d813@treenet.co.nz>
 <1537278855873-0.post@n4.nabble.com>
Message-ID: <daf9e35b-5294-39aa-2116-ce36d663e330@treenet.co.nz>

On 19/09/18 1:54 AM, neok wrote:
> Thank you very much Amos for putting me in the right direction.
> I successfully carried out the modifications you indicated to me.
> Regarding ufdbGuard, if I understood correctly, what you recommend is to use
> the ufdbConvertDB tool to convert my blacklists in plain text to the
> ufdbGuard database format? And then use that/those databases in normal squid
> ACL's?

No, ufdbguard is a fork of SquidGuard that can be used as a drop-in
replacement which works better while you improve your config.

You should work towards less complexity. Squid / squid.conf is where
HTTP access control takes place. The helper is about re-writing the URL
(only) - which is a complex and destructive process.

Amos


From squid3 at treenet.co.nz  Wed Sep 19 02:38:03 2018
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Wed, 19 Sep 2018 14:38:03 +1200
Subject: [squid-users] TCP_MISS/502 - audio stream - none default http
 ports
In-Reply-To: <1537277509.27318.30.camel@kempten.de>
References: <1537272426.27318.18.camel@kempten.de>
 <vmime.5ba0f76b.58bf.3dd501ed6b716e9b@ms249-lin-003.rotterdam.bazuin.nl>
 <1537277509.27318.30.camel@kempten.de>
Message-ID: <f27f3d08-7d7e-484d-9273-a7e6dcec0719@treenet.co.nz>

On 19/09/18 1:31 AM, D?rfler, Andreas wrote:
> hey louis,
> 
> thanks for you help and time!
> 
> found the problem, since there are firewalls in between, i forgot check
> a specific point, and well, it was one of the firewalls, no wonder it
> was driving me nuts. 
> one of those double facepalm moments once i realized it.
> 
>> And thank you for the music link, something different then the radio here. ;-) 
> 
> no problem, was the first random google search result ;-)
> 
>> : acl SSL_ports port 443
>> ## PS in your config you did miss to add the extra SSL_Ports also to the Safe_port. 
> 
> na, actualy there are 3 ports configured :D
> 
> 
>> ## And here you missed the "allow localnet" 
> 
> "http_access allow all" makes localnet obsolete, access restrictions are
> handled by a firewall.
> 

This statement is false, and very bad security practice. Squid handles
HTTP-level access controls. Firewalls handle network-layer access
control. Either way multiple layers of security that work together are
better than one - in case that one is compromised.

 ... and by using "allow all" you have explicitly compromised the Squid
layer.


* HTTP is a multiplexing pipelined protocol. One TCP connection can
contain traffic from multiple clients mixed up in arbitrary ways the
firewall will never see.

* As far as the firewall can be aware all traffic leaving Squid is from
*Squid* IP:port's not from the clients.


==> So those details mean any low-privilege client who is trusted just
enough to use the proxy gets automatically and *silently* privilege
escalated at the firewall to maximum privilege level *any* proxy user is
allowed by that firewall. This leaves your network wide open to insider
attacks, client hijacking and viral compromised clients.


You cannot trust firewall alone unless you have a *full* HTTP proxy
built into your firewall to handle the HTTP level control. In which case
your squid.conf is missing settings integrating the two proxies.


Like the other default rules this "deny all" serves multiple purposes -
along with the obvious access control to the network it is about denying
"legitimate" clients trying to make Squid do extremely resource
consuming things which are not permitted by your policy. Such as flood
the internal network with Tbps of traffic, or port-scan services they
are not normally allowed access to by the firewall.

Amos


From Andreas.Doerfler at kempten.de  Wed Sep 19 07:09:30 2018
From: Andreas.Doerfler at kempten.de (=?utf-8?B?RMO2cmZsZXIsIEFuZHJlYXM=?=)
Date: Wed, 19 Sep 2018 07:09:30 +0000
Subject: [squid-users] TCP_MISS/502 - audio stream - none default http
 ports
In-Reply-To: <f27f3d08-7d7e-484d-9273-a7e6dcec0719@treenet.co.nz>
References: <1537272426.27318.18.camel@kempten.de>
 <vmime.5ba0f76b.58bf.3dd501ed6b716e9b@ms249-lin-003.rotterdam.bazuin.nl>
 <1537277509.27318.30.camel@kempten.de>
 <f27f3d08-7d7e-484d-9273-a7e6dcec0719@treenet.co.nz>
Message-ID: <1537340971.27318.45.camel@kempten.de>

Am Mittwoch, den 19.09.2018, 14:38 +1200 schrieb Amos Jeffries:

> This statement is false, and very bad security practice. Squid handles
> HTTP-level access controls. Firewalls handle network-layer access
> control. Either way multiple layers of security that work together are
> better than one - in case that one is compromised.
> ....
> Like the other default rules this "deny all" serves multiple purposes -
> along with the obvious access control to the network it is about denying
> "legitimate" clients trying to make Squid do extremely resource
> consuming things which are not permitted by your policy. Such as flood
> the internal network with Tbps of traffic, or port-scan services they
> are not normally allowed access to by the firewall.

hey amos,

thanks for your feedback, it's realy appreciated.

i re-enabled deny all, even when i still don't see any benifit, because:
without giving away to mutch internals, in my case allow all is still
ok, only a very few subnets have a route to this system and the
firewalls are working on a combination of layer 3 and 5-7 and also
running ssl-inspection to this specific squid.

but you are right, every layer counts.

greetings,
andy
-------------- next part --------------
A non-text attachment was scrubbed...
Name: smime.p7s
Type: application/x-pkcs7-signature
Size: 6343 bytes
Desc: not available
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20180919/808e5a75/attachment.bin>

From flashdown at data-core.org  Wed Sep 19 08:08:40 2018
From: flashdown at data-core.org (Enrico Heine)
Date: Wed, 19 Sep 2018 10:08:40 +0200
Subject: [squid-users] Help: squid restarts and squidGuard die
In-Reply-To: <daf9e35b-5294-39aa-2116-ce36d663e330@treenet.co.nz>
References: <CA+d==oEt8J-ie2Fm1SYrHSBKD6Y7TRF-v3iGiMXC7WQ36WV1vw@mail.gmail.com>
 <f8f51c90-6bd7-09d0-a90b-c9552748d813@treenet.co.nz>
 <1537278855873-0.post@n4.nabble.com>
 <daf9e35b-5294-39aa-2116-ce36d663e330@treenet.co.nz>
Message-ID: <9801CC61-1B49-4B4D-B96C-7B3EF64066CA@data-core.org>

Thank you for this information Amos! :) I had ufdbguard as possible replacement in my list, your info about it beeing a fork, is the reason that I will switch to it soon. Thanks :)

Am 19. September 2018 04:03:39 MESZ schrieb Amos Jeffries <squid3 at treenet.co.nz>:
>On 19/09/18 1:54 AM, neok wrote:
>> Thank you very much Amos for putting me in the right direction.
>> I successfully carried out the modifications you indicated to me.
>> Regarding ufdbGuard, if I understood correctly, what you recommend is
>to use
>> the ufdbConvertDB tool to convert my blacklists in plain text to the
>> ufdbGuard database format? And then use that/those databases in
>normal squid
>> ACL's?
>
>No, ufdbguard is a fork of SquidGuard that can be used as a drop-in
>replacement which works better while you improve your config.
>
>You should work towards less complexity. Squid / squid.conf is where
>HTTP access control takes place. The helper is about re-writing the URL
>(only) - which is a complex and destructive process.
>
>Amos
>_______________________________________________
>squid-users mailing list
>squid-users at lists.squid-cache.org
>http://lists.squid-cache.org/listinfo/squid-users

-- 
Diese Nachricht wurde von meinem Android-Ger?t mit K-9 Mail gesendet.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20180919/1cf4ecea/attachment.htm>

From mujtaba21n at hotmail.com  Wed Sep 19 13:46:44 2018
From: mujtaba21n at hotmail.com (Mujtaba   Hassan Madani)
Date: Wed, 19 Sep 2018 13:46:44 +0000
Subject: [squid-users] Squid Cache Server
In-Reply-To: <f49f6de9-e3be-360e-cd44-b030f4be4a70@treenet.co.nz>
References: <DB6P193MB0008E0848E133CEB22448158C6040@DB6P193MB0008.EURP193.PROD.OUTLOOK.COM>
 <201809111054.06037.Antony.Stone@squid.open.source.it>
 <DB6P193MB00085E55BC2E60C264EE09A4C61B0@DB6P193MB0008.EURP193.PROD.OUTLOOK.COM>
 <b6e884e9-0aa3-a48a-b529-afcfe4c60f51@treenet.co.nz>
 <HE1P193MB0009EFBF04C8D339B5D23090C61A0@HE1P193MB0009.EURP193.PROD.OUTLOOK.COM>
 <DB6P193MB000820455863488DA32DDA21C6180@DB6P193MB0008.EURP193.PROD.OUTLOOK.COM>,
 <f49f6de9-e3be-360e-cd44-b030f4be4a70@treenet.co.nz>
Message-ID: <DB6P193MB00085F75B1438190AFE7B508C61C0@DB6P193MB0008.EURP193.PROD.OUTLOOK.COM>

Hi Amos,

 thanks for your concern, as I informed you Iam looking to install Squid on Ubuntu Linux server for Caching purpose once I kickoff i will notify you to have your assistant.

regards


Mujtaba H,

________________________________
From: squid-users <squid-users-bounces at lists.squid-cache.org> on behalf of Amos Jeffries <squid3 at treenet.co.nz>
Sent: Sunday, September 16, 2018 4:58:37 PM
To: squid-users at lists.squid-cache.org
Subject: Re: [squid-users] Squid Cache Server

On 15/09/18 12:13 PM, Mujtaba   Hassan Madani wrote:
> Hi Amos,
>
>    you did not get back to me about my below concern
>

I responded to your concern about copyright.

I do not see anything else in your messages as expressing a concern to
be responded to.

Amos


> ------------------------------------------------------------------------
> *From:* Mujtaba Hassan Madani
> *Sent:* Thursday, September 13, 2018 5:36:48 PM
>
>
> Hi Amos,
>
>    Iam looking for building a Squid proxy server on Ubuntu for my LAN
> serving up to 25 PC's I just want the maximum potential of the server
> capability to enhance the network performance and gain better users
> expectation of the service.
>

> ------------------------------------------------------------------------
> *From:* Amos Jeffries
> *Sent:* Wednesday, September 12, 2018 2:54:37 PM
>
> On 13/09/18 2:16 AM, Mujtaba   Hassan Madani wrote:
>> Dear Squid Team,
>>
>>      how does content provider prevent it from been cached while passing
>> through squid proxy it's by a copy right law
>
> No. Contents which can be transferred through a proxy are implicitly
> licensed for re-distribution.
>
> Legal issues are usually encountered only around interception or
> modification of content.
>
>
>> or some  encryption is
>
> Sometimes.
>
>> implemented in the traffic ?
>
> and other features built into HTTP protocol.
>
>
>> and where can I find the contents that been
>> cached on my squid proxy ?
>>
>
> Depends on your config. Usually in the machine RAM.
>
> What are you looking for exactly? and why?
>

_______________________________________________
squid-users mailing list
squid-users at lists.squid-cache.org
http://lists.squid-cache.org/listinfo/squid-users
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20180919/65535344/attachment.htm>

From vh1988 at yahoo.com.ar  Wed Sep 19 16:23:28 2018
From: vh1988 at yahoo.com.ar (Julian Perconti)
Date: Wed, 19 Sep 2018 13:23:28 -0300
Subject: [squid-users] About SSL peek-n-splice/bump configurations
In-Reply-To: <8c393e26-822f-ddea-50d4-9f80ebc9087f@measurement-factory.com>
References: <000301d43289$14ed4770$3ec7d650$@yahoo.com.ar>
 <002001d446cc$148bb580$3da32080$@yahoo.com.ar>
 <c7a4d318-c335-03de-9e5c-73fc258e38c0@treenet.co.nz>
 <008001d4479b$b22b9290$1682b7b0$@yahoo.com.ar>
 <04c7931e-cb29-80d1-2a2a-8a1ccb4761f5@treenet.co.nz>
 <019301d4486b$049ebf00$0ddc3d00$@yahoo.com.ar>
 <6b762c34-f0c4-2d9a-6616-432305af160b@treenet.co.nz>
 <009d01d44935$1648ef80$42dace80$@yahoo.com.ar>
 <5400815d-9a82-97a9-5889-0e76c04a9ba0@measurement-factory.com>
 <003501d44aa4$d784e5d0$868eb170$@yahoo.com.ar>
 <500fbad5-d277-965f-d29a-a81c2f343d2e@measurement-factory.com>
 <004401d44b0e$46cf40c0$d46dc240$@yahoo.com.ar>
 <0472b838-415e-e4d2-c71a-7fbce45de926@measurement-factory.com>
 <008e01d44bbf$ccb41e20$661c5a60$@yahoo.com.ar>
 <2b2e8b70-7f45-039c-462f-ba78b2fc1563@measurement-factory.com>
 <006d01d44eaf$55ecc290$01c647b0$@yahoo.com.ar>
 <ee42d35f-2dc0-3bbc-f6c4-a1852374a832@measurement-factory.com>
 <003601d44f61$dfd30e80$9f792b80$@yahoo.com.ar>
 <8c393e26-822f-ddea-50d4-9f80ebc9087f@measurement-factor y.com>
Message-ID: <000901d45035$1ab16870$50143950$@yahoo.com.ar>

>After a splice rule is applied, SslBump is over. No  more rules are 
>checked. No more loops are iterated. Squid simply "exits" the  SslBump 
>feature (and becomes a TCP tunnel).

How is that? What about the meaning of the ACL's at step1 when splice?

e.g.:
There only these two rules for ssl_bump statements:

ssl_bump step1 splice sitesAB
ssl_bump step1 splice SitesCD

I guess that here, Squid has to do 2 loops at outer/main loop to evaluate step1 twice, due to rules differs (sitesAB and sitesCD ACL) and see if both match to splice. 
Probably this example does not make sense: "Why don't use just 1 ACL instead 2"? But it is an example to understand and fix ideas.

Are You (perhaps) talking about the examples in the thread and not what happens "in general"?

> If noBumpSites matches at step2, then, yes, Squid will splice at step3 
> by default. Otherwise, no; Squid will bump at step3 by default.

[... ]

You mentioned that explanation two times.
The question (maybe obvious) is: In which case the "noBumpSites" ACL could have not match? I mean if I tell a Squid: "splice at step1 this.site.net" How that matches can fail?
Maybe you refered in the case that a site is just not listed in the ACL.

> >   ssl_bump splice noBumpSites # This line reachs a splice rule at step1
> >   ssl_bump stare
> 
> > Squid is telling to the client: "I will not touch any TLS byte. 
> > [...] I will do as many checks as possible then You will be connected..."
> 
> The configuration above does not match your summary because the 
> configuration has a "stare" action that may run at (step1 and) step2 
> (and, hence, a possibility of the bump action at step3). Staring at
> step2 and bumping (at any step) modify TLS bytes, of course.
> 
> Perhaps your summary only applies to the cases where noBumpSites 
> matches (either at step1 or at step2), but the summary did not make 
> that clear.

Here borns more ore less the same doubt like above and the final one.

> There is a big difference between explaining Squid actions for a 
> particular transaction and summarizing what a particular configuration 
> means (for all transactions). Unless noted otherwise, I am focusing on the latter.
> 
> AFAICT, the primary difference between
> 
>   ssl_bump peek noBumpSites
>   ssl_bump stare
> 
> and
> 
>   ssl_bump splice noBumpSites
>   ssl_bump stare
> 
> is that the former requires a noBumpSites match at step2 for the 
> connections to be spliced.

Yes. The condition you say is mandatory but, again: Why that requirement could fail/no-match?

Thank You for the patience



From marcus.kool at urlfilterdb.com  Wed Sep 19 11:49:37 2018
From: marcus.kool at urlfilterdb.com (Marcus Kool)
Date: Wed, 19 Sep 2018 08:49:37 -0300
Subject: [squid-users] Help: squid restarts and squidGuard die
In-Reply-To: <daf9e35b-5294-39aa-2116-ce36d663e330@treenet.co.nz>
References: <CA+d==oEt8J-ie2Fm1SYrHSBKD6Y7TRF-v3iGiMXC7WQ36WV1vw@mail.gmail.com>
 <f8f51c90-6bd7-09d0-a90b-c9552748d813@treenet.co.nz>
 <1537278855873-0.post@n4.nabble.com>
 <daf9e35b-5294-39aa-2116-ce36d663e330@treenet.co.nz>
Message-ID: <69fda549-1455-a336-9bd6-057828f67e0e@urlfilterdb.com>



On 18/09/18 23:03, Amos Jeffries wrote:
> On 19/09/18 1:54 AM, neok wrote:
>> Thank you very much Amos for putting me in the right direction.
>> I successfully carried out the modifications you indicated to me.
>> Regarding ufdbGuard, if I understood correctly, what you recommend is to use
>> the ufdbConvertDB tool to convert my blacklists in plain text to the
>> ufdbGuard database format? And then use that/those databases in normal squid
>> ACL's?
> 
> No, ufdbguard is a fork of SquidGuard that can be used as a drop-in
> replacement which works better while you improve your config.
> 
> You should work towards less complexity. Squid / squid.conf is where
> HTTP access control takes place. The helper is about re-writing the URL
> (only) - which is a complex and destructive process.

ufdbGuard is a simple tool that has the same syntax in its configuration file as squidGuard has.
It is far from complex, has a great Reference Manual, exmaple config file and a responsive support desk.
Amos, I have never seen you calling a URL writer being a complex and destructive process.  What do you mean?

URL rewriters have been used for decades for HTTP access control but you state "squid.conf is where HTTP access control takes place".
Are you saying that you want it is the _only_ place for HTTP access control?

Marcus


> Amos


From donmuller22 at outlook.com  Wed Sep 19 20:47:16 2018
From: donmuller22 at outlook.com (Donald Muller)
Date: Wed, 19 Sep 2018 20:47:16 +0000
Subject: [squid-users] Help: squid restarts and squidGuard die
In-Reply-To: <daf9e35b-5294-39aa-2116-ce36d663e330@treenet.co.nz>
References: <CA+d==oEt8J-ie2Fm1SYrHSBKD6Y7TRF-v3iGiMXC7WQ36WV1vw@mail.gmail.com>
 <f8f51c90-6bd7-09d0-a90b-c9552748d813@treenet.co.nz>
 <1537278855873-0.post@n4.nabble.com>
 <daf9e35b-5294-39aa-2116-ce36d663e330@treenet.co.nz>
Message-ID: <CY1PR16MB045967CC3477DAE98EE094DFB61C0@CY1PR16MB0459.namprd16.prod.outlook.com>

Amos,

So instead of using squidguard are you saying  you should use something like the following?

acl ads dstdomain -i "/etc/squid/squid-ads.acl"
acl adult dstdomain -i "/etc/squid/squid-adult.acl"

http_access deny ads
http_access deny adult

Do the lists need to be sorted in alphabetical order?

Don

> -----Original Message-----
> From: squid-users <squid-users-bounces at lists.squid-cache.org> On Behalf
> Of Amos Jeffries
> Sent: Tuesday, September 18, 2018 10:04 PM
> To: squid-users at lists.squid-cache.org
> Subject: Re: [squid-users] Help: squid restarts and squidGuard die
> 
> On 19/09/18 1:54 AM, neok wrote:
> > Thank you very much Amos for putting me in the right direction.
> > I successfully carried out the modifications you indicated to me.
> > Regarding ufdbGuard, if I understood correctly, what you recommend is
> > to use the ufdbConvertDB tool to convert my blacklists in plain text
> > to the ufdbGuard database format? And then use that/those databases in
> > normal squid ACL's?
> 
> No, ufdbguard is a fork of SquidGuard that can be used as a drop-in
> replacement which works better while you improve your config.
> 
> You should work towards less complexity. Squid / squid.conf is where HTTP
> access control takes place. The helper is about re-writing the URL
> (only) - which is a complex and destructive process.
> 
> Amos
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users

From service.mv at gmail.com  Wed Sep 19 20:52:54 2018
From: service.mv at gmail.com (Service MV)
Date: Wed, 19 Sep 2018 17:52:54 -0300
Subject: [squid-users] Any suggestions or comments about my configuration?
	squid 3.5.20
Message-ID: <CA+d==oEEdkiBsx3adyW7OdTe=gQJc_GM+UJqmuOD2Q_+_d0UuQ@mail.gmail.com>

Dear Ones, the more I use Squid the more I realize how powerful it is.
And like all powerful software it can be complex at first.
I would like to share my settings and if possible listen (read actually)
your comments and suggestions.
My goals of using squid:
- Transparent authentication of my AD users (2012R2)
- Internet access rules based on users belonging to AD groups.
- Non-authenticated clients (Win PCs) cannot navigate through the proxy.
- That the clients (Win PCs) not belonging to an AD group allowed in squid,
cannot navigate through the proxy.

My test scenario:
- A VM CentOS 7 Core over VirtualBox 5.2, 1 NIC.
- My VM is attached to my domain W2012R2 (following this post
https://www.rootusers.com/how-to-join-centos-linux-to-an-active-directory-domain/)
to achieve kerberos authentication transparent to the user. SElinux
disabled. Owner permissions to user squid in all folders/files involved.
- squid 3.5.20 installed and working great with kerberos, NTLM and basic
authentication.

squid.conf
### negotiate kerberos & ntlm authentication
auth_param negotiate program /usr/sbin/negotiate_wrapper --ntlm
/usr/bin/ntlm_auth --helper-protocol=squid-2.5-ntlmssp --kerberos
/usr/lib64/squid/negotiate_kerberos_auth -r -i -s GSS_C_NO_NAME
auth_param negotiate children 10
auth_param negotiate keep_alive on

### standard allowed ports
acl SSL_ports port 443
acl Safe_ports port 80 # http
acl Safe_ports port 21 # ftp
acl Safe_ports port 443 # https
acl Safe_ports port 70 # gopher
acl Safe_ports port 210 # wais
acl Safe_ports port 1025-65535 # unregistered ports
acl Safe_ports port 280 # http-mgmt
acl Safe_ports port 488 # gss-http
acl Safe_ports port 591 # filemaker
acl Safe_ports port 777 # multiling http
acl CONNECT method CONNECT

### destination domains to be blocked in a HTTP access control
acl LS_malicius dstdomain -i "/etc/squid/DBL/malicius/malicius.txt"
acl LS_remotecontrol dstdomain -i
"/etc/squid/DBL/remotecontrol/remotecontrol.txt"

### LDAP group membership sources
# WEB_ACCESS_1
external_acl_type AD_WEB_ACCESS_1 %LOGIN
/usr/lib64/squid/ext_ldap_group_acl -P -R -b OU=USERS,DC=netgol,DC=local -D
ldap -W "/etc/squid/ldap_pass.txt" -f
(&(sAMAccountName=%u)(memberOf=cn=WEB_ACCESS_1,OU=INTERNET,OU=PERMISOS,OU=NETGOL,DC=netgol,DC=local))
-h s-dc1.netgol.local -p 3268
acl WEB_ACCESS_1 external AD_WEB_ACCESS_1 web-access-1

# WEB_ACCESS_2
external_acl_type AD_WEB_ACCESS_2 %LOGIN
/usr/lib64/squid/ext_ldap_group_acl -P -R -b OU=USERS,DC=netgol,DC=local -D
ldap -W "/etc/squid/ldap_pass.txt" -f
(&(sAMAccountName=%u)(memberOf=cn=WEB_ACCESS_2,OU=INTERNET,OU=PERMISOS,OU=NETGOL,DC=netgol,DC=local))
-h s-dc1.netgol.local -p 3268
acl WEB_ACCESS_2 external AD_WEB_ACCESS_2 web-access-2

# WEB_ACCESS_3
external_acl_type AD_WEB_ACCESS_3 %LOGIN
/usr/lib64/squid/ext_ldap_group_acl -P -R -b OU=USERS,DC=netgol,DC=local -D
ldap -W "/etc/squid/ldap_pass.txt" -f
(&(sAMAccountName=%u)(memberOf=cn=WEB_ACCESS_3,OU=INTERNET,OU=PERMISOS,OU=NETGOL,DC=netgol,DC=local))
-h s-dc1.netgol.local -p 3268
acl WEB_ACCESS_3 external AD_WEB_ACCESS_3 web-access-3

### HTTP access control policies
http_access deny !Safe_ports
http_access deny CONNECT !SSL_ports
http_access allow localhost manager
http_access deny manager
http_access deny WEB_ACCESS_1 LS_malicius
http_access deny WEB_ACCESS_2 LS_malicius
http_access deny WEB_ACCESS_3 LS_malicius
http_access deny WEB_ACCESS_1 LS_remotecontrol
http_access deny WEB_ACCESS_2 LS_remotecontrol
http_access allow WEB_ACCESS_1
http_access allow WEB_ACCESS_2
http_access allow WEB_ACCESS_3
http_access allow localhost
http_access deny all

### personalization ###
http_port 8080
coredump_dir /var/spool/squid
refresh_pattern ^ftp: 1440 20% 10080
refresh_pattern ^gopher: 1440 0% 1440
refresh_pattern -i (/cgi-bin/|\?) 0 0% 0
refresh_pattern .  0 20% 4320
quick_abort_min 0 KB
quick_abort_max 0 KB
read_timeout 5 minutes
request_timeout 3 minutes
half_closed_clients off
shutdown_lifetime 15 seconds
log_icp_queries off
dns_v4_first on
ipcache_size 2048
ipcache_low 90
fqdncache_size 4096
forwarded_for off
cache_mgr system at netgol.net
visible_hostname proxy.netgol.local
httpd_suppress_version_string on
uri_whitespace strip
logfile_rotate 7
debug_options rotate=7


Any suggestion or comment will be very useful to me and I thank you in
advance.
Best regards

Gabriel
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20180919/0632f004/attachment.htm>

From rousskov at measurement-factory.com  Wed Sep 19 21:35:29 2018
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Wed, 19 Sep 2018 15:35:29 -0600
Subject: [squid-users] About SSL peek-n-splice/bump configurations
In-Reply-To: <000901d45035$1ab16870$50143950$@yahoo.com.ar>
References: <000301d43289$14ed4770$3ec7d650$@yahoo.com.ar>
 <008001d4479b$b22b9290$1682b7b0$@yahoo.com.ar>
 <04c7931e-cb29-80d1-2a2a-8a1ccb4761f5@treenet.co.nz>
 <019301d4486b$049ebf00$0ddc3d00$@yahoo.com.ar>
 <6b762c34-f0c4-2d9a-6616-432305af160b@treenet.co.nz>
 <009d01d44935$1648ef80$42dace80$@yahoo.com.ar>
 <5400815d-9a82-97a9-5889-0e76c04a9ba0@measurement-factory.com>
 <003501d44aa4$d784e5d0$868eb170$@yahoo.com.ar>
 <500fbad5-d277-965f-d29a-a81c2f343d2e@measurement-factory.com>
 <004401d44b0e$46cf40c0$d46dc240$@yahoo.com.ar>
 <0472b838-415e-e4d2-c71a-7fbce45de926@measurement-factory.com>
 <008e01d44bbf$ccb41e20$661c5a60$@yahoo.com.ar>
 <2b2e8b70-7f45-039c-462f-ba78b2fc1563@measurement-factory.com>
 <006d01d44eaf$55ecc290$01c647b0$@yahoo.com.ar>
 <ee42d35f-2dc0-3bbc-f6c4-a1852374a832@measurement-factory.com>
 <003601d44f61$dfd30e80$9f792b80$@yahoo.com.ar>
 <8c393e26-822f-ddea-50d4-9f80ebc9087f@measurement-factor y.com>
 <000901d45035$1ab16870$50143950$@yahoo.com.ar>
Message-ID: <934754fd-e048-5b28-35c5-b8e2da1337db@measurement-factory.com>

On 09/19/2018 10:23 AM, Julian Perconti wrote:
>> Alex: After a splice rule is applied, SslBump is over. No  more rules are 
>> checked. No more loops are iterated. Squid simply "exits" the  SslBump 
>> feature (and becomes a TCP tunnel).

> What about the meaning of the ACL's at step1 when splice?

* If the splice rule ACLs match, the splice rule is applied. In that
case you can consult my statement above.

* If the splice rule ACLs do not match, then the splice rule is not
applied. My statement above explicitly does not cover this case -- it
starts with "after a splice rule is APPLIED".


> e.g.:
> There only these two rules for ssl_bump statements:
> 
> ssl_bump splice sitesAB
> ssl_bump splice SitesCD

> I guess that here, Squid has to do 2 loops at outer/main loop to
> evaluate step1 twice, due to rules differs (sitesAB and sitesCD ACL)
> and see if both match to splice.

I do not know why you are guessing instead of carefully applying the
already documented procedure, but you guessed wrong. At any step, the
first matching rule is applied. For example, if sitesAB matches, then
Squid splices without checking the second (i.e. SitesCD) rule.

N.B. I removed the (misplaced) "step1" ACLs from the above example. That
ACL does not affect the above discussion.


> Are You (perhaps) talking about the examples in the thread and not what happens "in general"?

My statements above are general except the "For example..." sentence
that refers to your specific example.


> In which case the "noBumpSites" ACL could have not match? I mean if I
> tell a Squid: "splice at step1 this.site.net" How that matches can
> fail?

Roughly speaking, the server_name ACL matches at step1 when the real or
fake CONNECT Host information match one of the configured server names.

For example, if you are intercepting or if the real CONNECT request
contains an IP address (rather than a host name), then the server_name
ACL matches if the reverse DNS lookup for that IP address is successful
and matches at least one of the configured server names. In other cases,
the ACL does not match during step1.

The reality is more complex than the above rough summary because domain
name comparison is a complex algorithm. Consult the latest Squid
documentation for details. Also, please do not forget that step2
matching adds checking TLS client SNI name, and step3 matching adds
checking certificate Subject names. It gets really complex...

For example, the Host header of a CONNECT request may not be the same as
the TLS client-supplied SNI name, and/or the server certificate subject
name may. These differences (and other random factors like DNS
inconsistencies) may result in the server_name ACL match result changes
across the steps.

Modern Squids have additional server_name options that control some of
the matching nuances discussed above.


Alex.


From vh1988 at yahoo.com.ar  Wed Sep 19 22:41:17 2018
From: vh1988 at yahoo.com.ar (Julian Perconti)
Date: Wed, 19 Sep 2018 19:41:17 -0300
Subject: [squid-users] About SSL peek-n-splice/bump configurations
References: <000301d43289$14ed4770$3ec7d650$@yahoo.com.ar>
 <002001d446cc$148bb580$3da32080$@yahoo.com.ar>
 <c7a4d318-c335-03de-9e5c-73fc258e38c0@treenet.co.nz>
 <008001d4479b$b22b9290$1682b7b0$@yahoo.com.ar>
 <04c7931e-cb29-80d1-2a2a-8a1ccb4761f5@treenet.co.nz>
 <019301d4486b$049ebf00$0ddc3d00$@yahoo.com.ar>
 <6b762c34-f0c4-2d9a-6616-432305af160b@treenet.co.nz>
 <009d01d44935$1648ef80$42dace80$@yahoo.com.ar>
 <5400815d-9a82-97a9-5889-0e76c04a9ba0@measurement-factory.com>
 <003501d44aa4$d784e5d0$868eb170$@yahoo.com.ar>
 <500fbad5-d277-965f-d29a-a81c2f343d2e@measurement-factory.com>
 <004401d44b0e$46cf40c0$d46dc240$@yahoo.com.ar>
 <0472b838-415e-e4d2-c71a-7fbce45de926@measurement-factory.com>
 <008e01d44bbf$ccb41e20$661c5a60$@yahoo.com.ar>
 <2b2e8b70-7f45-039c-462f-ba78b2fc1563@measurement-factory.com>
 <006d01d44eaf$55ecc290$01c647b0$@yahoo.com.ar>
 <ee42d35f-2dc0-3bbc-f6c4-a1852374a832@measurement-factory.com>
 <003601d44f61$dfd30e80$9f792b80$@yahoo.com.ar>
 <8c393e26-822f-ddea-50d4-9f80ebc9087f@measurement-factor y.com> 
Message-ID: <002a01d45069$e32fdd20$a98f9760$@yahoo.com.ar>

I reply to myself due to a bounce and I have to re-enable the membership to list at least 3 times at month. 
Maybe a problem with Yahoo.

>>> Alex: After a splice rule is applied, SslBump is over. No  more rules are
>>> checked. No more loops are iterated. Squid simply "exits" the  SslBump
>>> feature (and becomes a TCP tunnel).

OK, that is what makes me a noise, and therefore I asked about what you said.

>> What about the meaning of the ACL's at step1 when splice?
>
>* If the splice rule ACLs match, the splice rule is applied. In that
>case you can consult my statement above.
>
>* If the splice rule ACLs do not match, then the splice rule is not
>applied. My statement above explicitly does not cover this case -- it
>starts with "after a splice rule is APPLIED".
>
>
>> e.g.:
>> There only these two rules for ssl_bump statements:
>>
>> ssl_bump splice sitesAB
>> ssl_bump splice SitesCD
>
>> I guess that here, Squid has to do 2 loops at outer/main loop to
>> evaluate step1 twice, due to rules differs (sitesAB and sitesCD ACL)
>> and see if both match to splice.

I think that I made a mistake in above sentence. 
I have should said "(..) Squid has to do 2 loops at inner while he is at the main loop (at SslBump1)"

>I do not know why you are guessing instead of carefully applying the
>already documented procedure, but you guessed wrong. At any step, the
>first matching rule is applied. For example, if sitesAB matches, then
>Squid splices without checking the second (i.e. SitesCD) rule.

Well, I am guessing because many things are not completely clear to me and/or easy to understand, at all. I am new in TLS filtering.
For example I never would think that in the given example, the second rule (sitesCD) will not never be checked later.
I asked or write that example with the inner loop in mind; I'm sorry.

>> Are You (perhaps) talking about the examples in the thread and not what happens "in general"?
>
>My statements above are general except the "For example..." sentence
>that refers to your specific example.

Its good to know.

>> In which case the "noBumpSites" ACL could have not match? I mean if I
>> tell a Squid: "splice at step1 this.site.net" How that matches can
>> fail?
>
>Roughly speaking, the server_name ACL matches at step1 when the real or
>fake CONNECT Host information match one of the configured server names.
>
>For example, if you are intercepting or if the real CONNECT request
>contains an IP address (rather than a host name), then the server_name
>ACL matches if the reverse DNS lookup for that IP address is successful
>and matches at least one of the configured server names. In other cases,
>the ACL does not match during step1.
>
>The reality is more complex than the above rough summary because domain
>name comparison is a complex algorithm. Consult the latest Squid
>documentation for details. Also, please do not forget that step2
>matching adds checking TLS client SNI name, and step3 matching adds
>checking certificate Subject names. It gets really complex...
>
>For example, the Host header of a CONNECT request may not be the same as
>the TLS client-supplied SNI name, and/or the server certificate subject
>name may. These differences (and other random factors like DNS
>inconsistencies) may result in the server_name ACL match result changes
>across the steps.
>
>Modern Squids have additional server_name options that control some of
>the matching nuances discussed above.

That's what I imagined you meant (and worried too) -without any kind of knowledge-. And now you have just confirmed it. 
So things become a little more delicate. 
And *now* I understand why you have done so much emphasis saying: "If the rule match..."

>Alex.

Thank You.



From squid3 at treenet.co.nz  Thu Sep 20 10:41:23 2018
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 20 Sep 2018 22:41:23 +1200
Subject: [squid-users] Any suggestions or comments about my
 configuration? squid 3.5.20
In-Reply-To: <CA+d==oEEdkiBsx3adyW7OdTe=gQJc_GM+UJqmuOD2Q_+_d0UuQ@mail.gmail.com>
References: <CA+d==oEEdkiBsx3adyW7OdTe=gQJc_GM+UJqmuOD2Q_+_d0UuQ@mail.gmail.com>
Message-ID: <678580bb-dbfa-96d9-242c-ab5741f4bd28@treenet.co.nz>

On 20/09/18 8:52 AM, Service MV wrote:
> Dear Ones, the more I use Squid the more I realize how powerful it is.
> And like all powerful software it can be complex at first.
> I would like to share my settings and if possible listen (read actually)
> your comments and suggestions.
> My goals of using squid:
> - Transparent authentication of my AD users (2012R2)
> - Internet access rules based on users belonging to AD groups.
> - Non-authenticated clients (Win PCs) cannot navigate through the proxy.
> - That the clients (Win PCs) not belonging to an AD group allowed in
> squid, cannot navigate through the proxy.
> 
> My test scenario:
> - A VM CentOS 7 Core over VirtualBox 5.2, 1 NIC.
> - My VM is attached to my domain W2012R2 (following this
> post?https://www.rootusers.com/how-to-join-centos-linux-to-an-active-directory-domain/)
> to achieve kerberos authentication transparent to the user. SElinux
> disabled. Owner permissions to user squid in all folders/files involved.
> - squid 3.5.20 installed and working great with kerberos, NTLM and basic
> authentication.
> 
> squid.conf
> ### negotiate kerberos & ntlm authentication

FYI: the technical terms for these are Negotiate/NTLM and
Negotiate/Kerberos.  With the '/' being part of the type name, not the
usual meaning of alternative words.

> auth_param negotiate program /usr/sbin/negotiate_wrapper --ntlm
> /usr/bin/ntlm_auth --helper-protocol=squid-2.5-ntlmssp --kerberos
> /usr/lib64/squid/negotiate_kerberos_auth -r -i -s GSS_C_NO_NAME?
> auth_param negotiate children 10?
> auth_param negotiate keep_alive on
>
...
> 
> ### LDAP group membership sources
> # WEB_ACCESS_1
> external_acl_type AD_WEB_ACCESS_1 %LOGIN
> /usr/lib64/squid/ext_ldap_group_acl -P -R -b OU=USERS,DC=netgol,DC=local
> -D ldap -W "/etc/squid/ldap_pass.txt" -f
> (&(sAMAccountName=%u)(memberOf=cn=WEB_ACCESS_1,OU=INTERNET,OU=PERMISOS,OU=NETGOL,DC=netgol,DC=local))
> -h s-dc1.netgol.local -p 3268
> acl WEB_ACCESS_1 external AD_WEB_ACCESS_1 web-access-1
> 
> # WEB_ACCESS_2
> external_acl_type AD_WEB_ACCESS_2 %LOGIN
> /usr/lib64/squid/ext_ldap_group_acl -P -R -b OU=USERS,DC=netgol,DC=local
> -D ldap -W "/etc/squid/ldap_pass.txt" -f
> (&(sAMAccountName=%u)(memberOf=cn=WEB_ACCESS_2,OU=INTERNET,OU=PERMISOS,OU=NETGOL,DC=netgol,DC=local))
> -h s-dc1.netgol.local -p 3268
> acl WEB_ACCESS_2 external AD_WEB_ACCESS_2 web-access-2
> 
> # WEB_ACCESS_3
> external_acl_type AD_WEB_ACCESS_3 %LOGIN
> /usr/lib64/squid/ext_ldap_group_acl -P -R -b OU=USERS,DC=netgol,DC=local
> -D ldap -W "/etc/squid/ldap_pass.txt" -f
> (&(sAMAccountName=%u)(memberOf=cn=WEB_ACCESS_3,OU=INTERNET,OU=PERMISOS,OU=NETGOL,DC=netgol,DC=local))
> -h s-dc1.netgol.local -p 3268
> acl WEB_ACCESS_3 external AD_WEB_ACCESS_3 web-access-3

I notice several useful things about these external_acl_type definitions:

 1) that they are identical except for the "memberOf=cn=WEB_ACCESS_"
string in the LDAP query.

 2) that the group name you pass the helper ("web-access-1/2/3") is
never actually used (no %g in the filter syntax).

That means you can simplify quite a lot just by passing the real group
name to the helper and using %g.

  external_acl_type AD_WEB_ACCESS %LOGIN \
    /usr/lib64/squid/ext_ldap_group_acl -P -R \
   -b OU=USERS,DC=netgol,DC=local \
   -D ldap -W "/etc/squid/ldap_pass.txt" \
  -f
"(&(sAMAccountName=%u)(memberOf=cn=%g,OU=INTERNET,OU=PERMISOS,OU=NETGOL,DC=netgol,DC=local))"
\
  -h s-dc1.netgol.local -p 3268


   acl WEB_ACCESS_1 external AD_WEB_ACCESS WEB_ACCESS_1
   acl WEB_ACCESS_2 external AD_WEB_ACCESS WEB_ACCESS_2
   acl WEB_ACCESS_3 external AD_WEB_ACCESS WEB_ACCESS_3



(I suspect there is something you can do to simplify the AD tree for -b
and -f LDAP parameters. But someone more familiar with LDAP syntax will
have to go over that.)

> 
> ### HTTP access control policies
> http_access deny !Safe_ports?
> http_access deny CONNECT !SSL_ports?
> http_access allow localhost manager?
> http_access deny manager

Your policy said:
 "Non-authenticated clients (Win PCs) cannot navigate through the proxy."

So what you need to start with in your custom rules, is one which says
only that not-auth clients are forbidden.

 acl auth proxy_auth REQUIRED
 http_access deny !auth

... now to reach the below tests a client *must* be logged in. You can
safely assume that credentials are available for all tests below.


FYI: Squid will send an auth-challenge denial instead of a regular
forbid error page. If your clients are logged into AD correctly they
will authenticate transparently as you wish. If the client is *not*
logged into the domain they *might* see a popup - that has nothing to do
with Squid and can only be resolved at the browser.
  Note that Squid will have already been doing this for the external
ACLs, but this is a simpler and clearer way to write the config which
also allows you to make the group checks send a hard 403 forbidden
instead of having to always allow clients to re-login.



> http_access deny WEB_ACCESS_1 LS_malicius
> http_access deny WEB_ACCESS_2 LS_malicius
> http_access deny WEB_ACCESS_3 LS_malicius

Since none of the groups is allowed to LS_malicius you do not actually
have to know what group they are in to block that traffic.

Just use:
 http_access deny LS_malicius

You can further optimize performance by placing that above the login
line. Since login is a waste of cycles if you are only going to reject.

The only reason you might want to deny here is to log which user was
trying to go there (infected?). So you need to decide if that log detail
is worth the slower performance and heavier load on AD. The performance
benefit varies by network and may be extremely small (or not) - so you
may want to experiment and measure.


> http_access deny WEB_ACCESS_1 LS_remotecontrol
> http_access deny WEB_ACCESS_2 LS_remotecontrol

Similar, but slightly different here for LS_remotecontrol.

In this case note that group #3 is about to be allowed to do anythign,
including this traffic. So you can allow them now:

 http_access allow WEB_ACCESS_3

... then do the denial for the remaining groups without needing to check
which group is which.

   http_access deny LS_remotecontrol

... then the regular group #1 and #2 allow.

> http_access allow WEB_ACCESS_1
> http_access allow WEB_ACCESS_2

> http_access allow localhost

Your policy statements do not mention anything about localhost traffic
being allowed to use the proxy. Is this something you can remove?


> http_access deny all
> 

So to summarize, you http_access should look like this (modulo your
localhost decision):


  ### HTTP access control policies
  http_access deny !Safe_ports
  http_access deny CONNECT !SSL_ports
  http_access allow localhost manager
  http_access deny manager
  http_access deny !auth
  http_access allow localhost
  http_access deny LS_malicius
  http_access allow WEB_ACCESS_3
  http_access deny LS_remotecontrol
  http_access allow WEB_ACCESS_1
  http_access allow WEB_ACCESS_2
  http_access deny all



> ### personalization ###
> http_port 8080?
> coredump_dir /var/spool/squid?
> refresh_pattern ^ftp: 1440 20% 10080?
> refresh_pattern ^gopher: 1440 0% 1440?
> refresh_pattern -i (/cgi-bin/|\?) 0 0% 0?
> refresh_pattern .? 0 20% 4320?
> quick_abort_min 0 KB?
> quick_abort_max 0 KB?
> read_timeout 5 minutes?
> request_timeout 3 minutes?
> half_closed_clients off?
> shutdown_lifetime 15 seconds?
> log_icp_queries off?
> dns_v4_first on?
> ipcache_size 2048?
> ipcache_low 90?
> fqdncache_size 4096?
> forwarded_for off?
> cache_mgr system at netgol.net
> visible_hostname proxy.netgol.local?
> httpd_suppress_version_string on?
> uri_whitespace strip
> logfile_rotate 7
> debug_options rotate=7
> 

In the above, you might want to add config comments for yourself
mentioning why each of the above is different from the defaults.

If they are just copy-paste from a tutorial without understanding why,
then maybe the defaults are fine for your needs.

Some of what you have above *are* the modern defaults which do not need
configuring (eg half_closed_clients, uri_whitespace, ipcache_low, maybe
coredump_dir).

Some are irrelevant. eg log_icp_queries is pointless when ICP is
disabled by default, "debug_options rotate=N" defaults to the
logfile_rotate value.

Some are intended to resolve quite specific issues which are not
relevant on most networks (eg dns_v4_first, forwarded_for,
visible_hostname, half_closed_clients) and you may actually need
different settings than people commonly paste blindly into web
tutorials. Read the docs for those options particularly the descriptions
of use-cases, caveats, and any WARNINGS / NOTICE text.


And last but not least - always run "squid -k parse" after changing
config or upgrading the proxy.  Squid can detect a large number of
config problems and tells you about them, usually with instructions on
how to resolve it.


HTH
Amos


From uhlar at fantomas.sk  Thu Sep 20 11:16:12 2018
From: uhlar at fantomas.sk (Matus UHLAR - fantomas)
Date: Thu, 20 Sep 2018 13:16:12 +0200
Subject: [squid-users] Help: squid restarts and squidGuard die
In-Reply-To: <CY1PR16MB045967CC3477DAE98EE094DFB61C0@CY1PR16MB0459.namprd16.prod.outlook.com>
References: <CA+d==oEt8J-ie2Fm1SYrHSBKD6Y7TRF-v3iGiMXC7WQ36WV1vw@mail.gmail.com>
 <f8f51c90-6bd7-09d0-a90b-c9552748d813@treenet.co.nz>
 <1537278855873-0.post@n4.nabble.com>
 <daf9e35b-5294-39aa-2116-ce36d663e330@treenet.co.nz>
 <CY1PR16MB045967CC3477DAE98EE094DFB61C0@CY1PR16MB0459.namprd16.prod.outlook.com>
Message-ID: <20180920111612.GA26863@fantomas.sk>

On 19.09.18 20:47, Donald Muller wrote:
>So instead of using squidguard are you saying  you should use something like the following?
>
>acl ads dstdomain -i "/etc/squid/squid-ads.acl"
>acl adult dstdomain -i "/etc/squid/squid-adult.acl"
>
>http_access deny ads
>http_access deny adult
>
>Do the lists need to be sorted in alphabetical order?

I don't think so - the lists are parsed to in -memory format for faster
processing.

The case where sw like ufdbguard is important is where you use regular
expressions like url_regex (but srcdom_regex and dstdom_regex may neet it
too).

Processing of those is very inefficient inside of squid.


>> -----Original Message-----
>> From: squid-users <squid-users-bounces at lists.squid-cache.org> On Behalf
>> Of Amos Jeffries
>> Sent: Tuesday, September 18, 2018 10:04 PM
>> To: squid-users at lists.squid-cache.org
>> Subject: Re: [squid-users] Help: squid restarts and squidGuard die
>>
>> On 19/09/18 1:54 AM, neok wrote:
>> > Thank you very much Amos for putting me in the right direction.
>> > I successfully carried out the modifications you indicated to me.
>> > Regarding ufdbGuard, if I understood correctly, what you recommend is
>> > to use the ufdbConvertDB tool to convert my blacklists in plain text
>> > to the ufdbGuard database format? And then use that/those databases in
>> > normal squid ACL's?
>>
>> No, ufdbguard is a fork of SquidGuard that can be used as a drop-in
>> replacement which works better while you improve your config.
>>
>> You should work towards less complexity. Squid / squid.conf is where HTTP
>> access control takes place. The helper is about re-writing the URL
>> (only) - which is a complex and destructive process.

-- 
Matus UHLAR - fantomas, uhlar at fantomas.sk ; http://www.fantomas.sk/
Warning: I wish NOT to receive e-mail advertising to this address.
Varovanie: na tuto adresu chcem NEDOSTAVAT akukolvek reklamnu postu.
Linux is like a teepee: no Windows, no Gates and an apache inside...


From squid3 at treenet.co.nz  Thu Sep 20 11:46:14 2018
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 20 Sep 2018 23:46:14 +1200
Subject: [squid-users] Help: squid restarts and squidGuard die
In-Reply-To: <69fda549-1455-a336-9bd6-057828f67e0e@urlfilterdb.com>
References: <CA+d==oEt8J-ie2Fm1SYrHSBKD6Y7TRF-v3iGiMXC7WQ36WV1vw@mail.gmail.com>
 <f8f51c90-6bd7-09d0-a90b-c9552748d813@treenet.co.nz>
 <1537278855873-0.post@n4.nabble.com>
 <daf9e35b-5294-39aa-2116-ce36d663e330@treenet.co.nz>
 <69fda549-1455-a336-9bd6-057828f67e0e@urlfilterdb.com>
Message-ID: <bceeb9cf-128a-79e7-8fc4-e31e9c0e27c9@treenet.co.nz>

On 19/09/18 11:49 PM, Marcus Kool wrote:
> 
> On 18/09/18 23:03, Amos Jeffries wrote:
>> On 19/09/18 1:54 AM, neok wrote:
>>> Thank you very much Amos for putting me in the right direction.
>>> I successfully carried out the modifications you indicated to me.
>>> Regarding ufdbGuard, if I understood correctly, what you recommend is
>>> to use
>>> the ufdbConvertDB tool to convert my blacklists in plain text to the
>>> ufdbGuard database format? And then use that/those databases in
>>> normal squid
>>> ACL's?
>>
>> No, ufdbguard is a fork of SquidGuard that can be used as a drop-in
>> replacement which works better while you improve your config.
>>
>> You should work towards less complexity. Squid / squid.conf is where
>> HTTP access control takes place. The helper is about re-writing the URL
>> (only) - which is a complex and destructive process.
> 
> ufdbGuard is a simple tool that has the same syntax in its configuration
> file as squidGuard has.
> It is far from complex, has a great Reference Manual, exmaple config
> file and a responsive support desk.
> Amos, I have never seen you calling a URL writer being a complex and
> destructive process.? What do you mean?

Re-writing requires Squid to:
 * fork external helpers, and
 * maintain queues of lookups to those helpers, and
 * maintain cache of helper responses, and
 * maintain a whole extra copy of HTTP-request state, and
 * copy some (not all) of that state info between the two "client" requests.

 ... lots of complexity, memory, CPU time, traffic latency, etc.

Also when used for access control (re-write to an "error" URL) the
re-write helper needs extra complexity in itself to act as the altered
origin server for error pages, or have some fourth-party web server.


> 
> URL rewriters have been used for decades for HTTP access control but you
> state "squid.conf is where HTTP access control takes place".

Once upon a time, back at the dawn of the WWW (before the 1990s) Squid
lacked external_acl_type and modular ACLs.

That persisted for the first decade or so of Squid's life, with only the
re-write API for admin to use for complicated permissions.

Then one day about 2 decades or so ago, external ACL was added and the
ACLs were also made much easier to implement and plug in new checks.
Today we have hundreds of native ACLs and even a selection of custom ACL
helpers. Making the need for these abuses of the poor re-writers.

Old habits and online tutorials however are hard to get rid of.


> Are you saying that you want it is the _only_ place for HTTP access
> control?


I'm saying the purpose of the url_rewrite_* API in Squid is to tell
Squid whether the URL (only) needs some mangling in order for the
server/origin to understand it.
 It can re-write transparently with all the problems that causes to
security scopes and URL sync between the endpoints. Or redirect the
client to the "correct" URL.


The Squid http_access and similar *access controls* are the place for
access control - hint is in the naming. With external ACL type for
anything Squid does not support natively or well. As Flashdown mentioned
even calls to SquidGuard etc. can be wrapped and used as external ACLs.


Amos


From flashdown at data-core.org  Thu Sep 20 12:40:28 2018
From: flashdown at data-core.org (Flashdown)
Date: Thu, 20 Sep 2018 14:40:28 +0200
Subject: [squid-users] Help: squid restarts and squidGuard die
In-Reply-To: <bceeb9cf-128a-79e7-8fc4-e31e9c0e27c9@treenet.co.nz>
References: <CA+d==oEt8J-ie2Fm1SYrHSBKD6Y7TRF-v3iGiMXC7WQ36WV1vw@mail.gmail.com>
 <f8f51c90-6bd7-09d0-a90b-c9552748d813@treenet.co.nz>
 <1537278855873-0.post@n4.nabble.com>
 <daf9e35b-5294-39aa-2116-ce36d663e330@treenet.co.nz>
 <69fda549-1455-a336-9bd6-057828f67e0e@urlfilterdb.com>
 <bceeb9cf-128a-79e7-8fc4-e31e9c0e27c9@treenet.co.nz>
Message-ID: <951a117e55db9b246c36f74518eb07ac@data-core.org>

> I'm saying the purpose of the url_rewrite_* API in Squid is to tell
> Squid whether the URL (only) needs some mangling in order for the
> server/origin to understand it.
>  It can re-write transparently with all the problems that causes to
> security scopes and URL sync between the endpoints. Or redirect the
> client to the "correct" URL.
> 
> 
> The Squid http_access and similar *access controls* are the place for
> access control - hint is in the naming. With external ACL type for
> anything Squid does not support natively or well. As Flashdown 
> mentioned
> even calls to SquidGuard etc. can be wrapped and used as external ACLs.
> 

Just want to add, in the beginning I thought about using a wrapper or 
writing one but as I found out during testing during these time, 
SquidGuard gives back the right responses to Squid, so a wrapper was not 
needed, and the rewrite adding in such a respone is simply ignored by 
Squid and it works like a charm, hope ufdbguard can be used as external 
acl helper natively as well. My config line:
external_acl_type squidguard ipv4 concurrency=0 children-max=XXX 
children-startup=XX ttl=60 %URI %SRC %{-} %un %METHOD 
/usr/bin/squidGuard

Taken out from my internal documentation:

"Manual testing:

echo "website.com 10.0.0.1/ - - GET" | squidGuard

Explaination of Responses:

     ERR tells us: The access was not denied by Squidguard, so wether its 
not part of the blacklists or it is listed in the whitelist
     BH message=?squidGuard error parsing squid line? tells us: there was 
an error when checking your input, may you had a syntax error or there 
is an issue in SquidGuard, the message param gives more insight.
     OK rewrite-url=?https://127.0.0.1/? tells us: the item was found on 
the blacklists and is blocked. BTW Squid only sees the OK and ignores 
the rewrite command, since we didn't integrate it as an URL-rewrite 
program which would have many disadvantages.

PS: This is just how an external ACL Helper for Squid must work/respond. 
So Squid only takes ERR and BH including the message and OK. Thats why I 
was able to implement it this way without writing a wrapper for it. "

Hope it helps and hope I can do the same with ufdbguard, the SquidGuard 
Version I use is the latest one from the official Debian Repositories.



---
Best regards,
Flashdown


From donmuller22 at outlook.com  Thu Sep 20 15:46:15 2018
From: donmuller22 at outlook.com (Donald Muller)
Date: Thu, 20 Sep 2018 15:46:15 +0000
Subject: [squid-users] Help: squid restarts and squidGuard die
In-Reply-To: <20180920111612.GA26863@fantomas.sk>
References: <CA+d==oEt8J-ie2Fm1SYrHSBKD6Y7TRF-v3iGiMXC7WQ36WV1vw@mail.gmail.com>
 <f8f51c90-6bd7-09d0-a90b-c9552748d813@treenet.co.nz>
 <1537278855873-0.post@n4.nabble.com>
 <daf9e35b-5294-39aa-2116-ce36d663e330@treenet.co.nz>
 <CY1PR16MB045967CC3477DAE98EE094DFB61C0@CY1PR16MB0459.namprd16.prod.outlook.com>
 <20180920111612.GA26863@fantomas.sk>
Message-ID: <CY1PR16MB0459DBC3A42A489E531F323DB6130@CY1PR16MB0459.namprd16.prod.outlook.com>



> -----Original Message-----
> From: squid-users <squid-users-bounces at lists.squid-cache.org> On Behalf
> Of Matus UHLAR - fantomas
> Sent: Thursday, September 20, 2018 7:16 AM
> To: squid-users at lists.squid-cache.org
> Subject: Re: [squid-users] Help: squid restarts and squidGuard die
> 
> On 19.09.18 20:47, Donald Muller wrote:
> >So instead of using squidguard are you saying  you should use something
> like the following?
> >
> >acl ads dstdomain -i "/etc/squid/squid-ads.acl"
> >acl adult dstdomain -i "/etc/squid/squid-adult.acl"
> >
> >http_access deny ads
> >http_access deny adult
> >
> >Do the lists need to be sorted in alphabetical order?
> 
> I don't think so - the lists are parsed to in -memory format for faster
> processing.
> 

Does Squid monitor dstdomain files for changes and reload them or does a '-k reconfigure' need to be issued?

> The case where sw like ufdbguard is important is where you use regular
> expressions like url_regex (but srcdom_regex and dstdom_regex may neet it
> too).
> 
> Processing of those is very inefficient inside of squid.
> 
> 
> >> -----Original Message-----
> >> From: squid-users <squid-users-bounces at lists.squid-cache.org> On
> >> Behalf Of Amos Jeffries
> >> Sent: Tuesday, September 18, 2018 10:04 PM
> >> To: squid-users at lists.squid-cache.org
> >> Subject: Re: [squid-users] Help: squid restarts and squidGuard die
> >>
> >> On 19/09/18 1:54 AM, neok wrote:
> >> > Thank you very much Amos for putting me in the right direction.
> >> > I successfully carried out the modifications you indicated to me.
> >> > Regarding ufdbGuard, if I understood correctly, what you recommend
> >> > is to use the ufdbConvertDB tool to convert my blacklists in plain
> >> > text to the ufdbGuard database format? And then use that/those
> >> > databases in normal squid ACL's?
> >>
> >> No, ufdbguard is a fork of SquidGuard that can be used as a drop-in
> >> replacement which works better while you improve your config.
> >>
> >> You should work towards less complexity. Squid / squid.conf is where
> >> HTTP access control takes place. The helper is about re-writing the
> >> URL
> >> (only) - which is a complex and destructive process.
> 
> --
> Matus UHLAR - fantomas, uhlar at fantomas.sk ; http://www.fantomas.sk/
> Warning: I wish NOT to receive e-mail advertising to this address.
> Varovanie: na tuto adresu chcem NEDOSTAVAT akukolvek reklamnu postu.
> Linux is like a teepee: no Windows, no Gates and an apache inside...
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users

From marcus.kool at urlfilterdb.com  Thu Sep 20 15:46:56 2018
From: marcus.kool at urlfilterdb.com (Marcus Kool)
Date: Thu, 20 Sep 2018 12:46:56 -0300
Subject: [squid-users] Help: squid restarts and squidGuard die
In-Reply-To: <bceeb9cf-128a-79e7-8fc4-e31e9c0e27c9@treenet.co.nz>
References: <CA+d==oEt8J-ie2Fm1SYrHSBKD6Y7TRF-v3iGiMXC7WQ36WV1vw@mail.gmail.com>
 <f8f51c90-6bd7-09d0-a90b-c9552748d813@treenet.co.nz>
 <1537278855873-0.post@n4.nabble.com>
 <daf9e35b-5294-39aa-2116-ce36d663e330@treenet.co.nz>
 <69fda549-1455-a336-9bd6-057828f67e0e@urlfilterdb.com>
 <bceeb9cf-128a-79e7-8fc4-e31e9c0e27c9@treenet.co.nz>
Message-ID: <f926b42a-42eb-9497-8064-b1d87b82e7fb@urlfilterdb.com>



On 20/09/18 08:46, Amos Jeffries wrote:
> On 19/09/18 11:49 PM, Marcus Kool wrote:
>>
>> On 18/09/18 23:03, Amos Jeffries wrote:
>>> On 19/09/18 1:54 AM, neok wrote:
>>>> Thank you very much Amos for putting me in the right direction.
>>>> I successfully carried out the modifications you indicated to me.
>>>> Regarding ufdbGuard, if I understood correctly, what you recommend is
>>>> to use
>>>> the ufdbConvertDB tool to convert my blacklists in plain text to the
>>>> ufdbGuard database format? And then use that/those databases in
>>>> normal squid
>>>> ACL's?
>>>
>>> No, ufdbguard is a fork of SquidGuard that can be used as a drop-in
>>> replacement which works better while you improve your config.
>>>
>>> You should work towards less complexity. Squid / squid.conf is where
>>> HTTP access control takes place. The helper is about re-writing the URL
>>> (only) - which is a complex and destructive process.
>>
>> ufdbGuard is a simple tool that has the same syntax in its configuration
>> file as squidGuard has.
>> It is far from complex, has a great Reference Manual, exmaple config
>> file and a responsive support desk.
>> Amos, I have never seen you calling a URL writer being a complex and
>> destructive process.? What do you mean?
> 
> Re-writing requires Squid to:
>   * fork external helpers, and
>   * maintain queues of lookups to those helpers, and
>   * maintain cache of helper responses, and
>   * maintain a whole extra copy of HTTP-request state, and
>   * copy some (not all) of that state info between the two "client" requests.
> 
>   ... lots of complexity, memory, CPU time, traffic latency, etc.

Squid itself is complex and for any feature of Squid one can make a list like above to say that it is complex.
The fact that one can make such a list does not mean much to me.
One can make the same or a similar list for external acl helpers and even native acls.

> Also when used for access control (re-write to an "error" URL) the
> re-write helper needs extra complexity in itself to act as the altered
> origin server for error pages, or have some fourth-party web server.

Squid cannot do everything that a URL writer, and specifically ufdbGuard, can.
For example, Squid must restart and break all open connections when a tiny detail of the configuration changes.  With ufdbGuard this does not happen.
ufdbGuard supports dynamic lists of users, domains and source ip addresses which are updated every X minutes without any service interruption.
When other parameters change, ufdbGuard resets itself with zero service interruption for Squid and its users.
ufdbGuard can decide to probe a site to make a decision, and hence detect Skype, Teamviewer and other types of sites that an admin might want to block.  Squid cannot.
ufdbGuard can decide to do a lookup of a reverse IP lookup to make a decision.  Squid cannot.
ufdbGuard supports complex time restrictions for access. Squid support simple time restrictions.
ufdbGuard supports flat file domain/url lists and a commercial URL database.  Squid does not.
And the list goes on.

So when you state on the mailing list that users should unconditionally stop using a URL writer in favor of using Squid acls, you may be causing troubles for admins who do not know the implications of 
your advice.


>> URL rewriters have been used for decades for HTTP access control but you
>> state "squid.conf is where HTTP access control takes place".
> 
> Once upon a time, back at the dawn of the WWW (before the 1990s) Squid
> lacked external_acl_type and modular ACLs.
> 
> That persisted for the first decade or so of Squid's life, with only the
> re-write API for admin to use for complicated permissions.
> 
> Then one day about 2 decades or so ago, external ACL was added and the
> ACLs were also made much easier to implement and plug in new checks.
> Today we have hundreds of native ACLs and even a selection of custom ACL
> helpers. Making the need for these abuses of the poor re-writers.
> 
> Old habits and online tutorials however are hard to get rid of.

If you want to get rid of habits that in your view are old/obsolete, then why not start a discussion?
And in the event that at the end of the discussion, the decision is made that a particular interface should be removed, why not phase it out ?

>> Are you saying that you want it is the _only_ place for HTTP access
>> control?
> 
> 
> I'm saying the purpose of the url_rewrite_* API in Squid is to tell
> Squid whether the URL (only) needs some mangling in order for the
> server/origin to understand it.
>   It can re-write transparently with all the problems that causes to
> security scopes and URL sync between the endpoints. Or redirect the
> client to the "correct" URL.
> 
> 
> The Squid http_access and similar *access controls* are the place for
> access control - hint is in the naming. With external ACL type for
> anything Squid does not support natively or well. As Flashdown mentioned
> even calls to SquidGuard etc. can be wrapped and used as external ACLs.

Wrapping and externals ACLs adds the same complexity, memory, CPU time, traffic latency, etc that you use as an argument against a URL writer.
Is it only because of the name, 'external acl helper' vs 'url rewriter helper', that you dislike the url rewriter?

Marcus

> 
> Amos


From brett.anderson.ftw at gmail.com  Thu Sep 20 18:36:11 2018
From: brett.anderson.ftw at gmail.com (Brett)
Date: Thu, 20 Sep 2018 13:36:11 -0500 (CDT)
Subject: [squid-users] Is there any way to cache or forward https requests
 to an http proxy using Squid?
Message-ID: <1537468571079-0.post@n4.nabble.com>

I currently have squid setup to use a self-signed certificate for MITM to
cache HTTPS requests. This works. If an item is not in the cache I want to
request from an online proxy like Crawlera. Unfortunately Crawlera only
offer an http endpoint. When I try to forward to this endpoint, everything
works for HTTP, but for HTTPS I received the error: Handshake with SSL
server failed: error:140770FC:SSL routines:SSL23_GET_SERVER_HELLO:unknown
protocol

I'm using squid 4.2. Is there a way I can configure squid so I can specify
it as a proxy for an https request and then have it act as a cache or
forward to an HTTP proxy (that supports CONNECT)? If at some point I'm
transmitting in plain text it doesn't matter at all for this application.

The following is my configuration for Squid:

http_port 3128 ssl-bump \
  cert=/apps/server_crt.pem key=/apps/server_key.pem \
  generate-host-certificates=on dynamic_cert_mem_cache_size=4MB
sslcrtd_program /apps/squid/libexec/security_file_certgen -s
/apps/squid/var/lib/ssl_db -M 4MB sslcrtd_children 8 startup=1 idle=1 
acl step1 at_step SslBump1
ssl_bump peek step1
ssl_bump bump all
acl localnet src 10.0.0.0/8     # RFC1918 possible internal network
acl localnet src 172.16.0.0/12  # RFC1918 possible internal network
acl localnet src 192.168.0.0/16 # RFC1918 possible internal network
acl localnet src fc00::/7       # RFC 4193 local private network range
acl localnet src fe80::/10      # RFC 4291 link-local (directly plugged)
machines
acl SSL_ports port 443
acl Safe_ports port 80          # http
acl Safe_ports port 21          # ftp
acl Safe_ports port 443         # https
acl Safe_ports port 70          # gopher
acl Safe_ports port 210         # wais
acl Safe_ports port 280         # http-mgmt
acl Safe_ports port 488         # gss-http
acl Safe_ports port 591         # filemaker
acl Safe_ports port 777         # multiling http
acl Safe_ports port 1025-65535  # unregistered ports
acl CONNECT method CONNECT
http_access deny !Safe_ports
http_access deny CONNECT !SSL_ports
http_access allow localhost manager
http_access deny manager
coredump_dir /apps/squid/var/cache
maximum_object_size 10 GB
cache_dir ufs /apps/squid/var/cache/squid 100 16 256
cache_mem 256 MB
maximum_object_size_in_memory 512 KB
cache_replacement_policy heap LFUDA
range_offset_limit -1
quick_abort_min -1 KB
offline_mode on
http_access allow localnet
http_access allow localhost
http_access deny all
refresh_pattern . 525600 100% 525600 ignore-reload ignore-no-store
ignore-private ignore-auth ignore-must-revalidate store-stale

cache_peer proxy.crawlera.com parent 8010 0 ssl login=APIKEY:
never_direct allow all

Update


If I change the ssl_bump directives above to the following:

acl step1 at_step SslBump1
acl step2 at_step SslBump2
acl step3 at_step SslBump3

ssl_bump stare step2
ssl_bump bump step3

An HTTPS request will tunnel all the way through both proxies to the target
and correctly return the response to the caller, but it no longer has MITM
access at the Squid proxy to cache the results, so they CONNECT though to
Crawlera on subsequent requests for the same resource. HTTP on the other
hand will go through both proxies if it's not in the cache, otherwise it
does get returned from the cache.

This is still not the solution I'm looking for though, I would like to cache
HTTPS as well.



--
Sent from: http://squid-web-proxy-cache.1019090.n4.nabble.com/Squid-Users-f1019091.html


From rousskov at measurement-factory.com  Thu Sep 20 19:47:16 2018
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Thu, 20 Sep 2018 13:47:16 -0600
Subject: [squid-users] Is there any way to cache or forward https
 requests to an http proxy using Squid?
In-Reply-To: <1537468571079-0.post@n4.nabble.com>
References: <1537468571079-0.post@n4.nabble.com>
Message-ID: <c163d8cb-31be-5b5d-4c0c-0c876811ce4b@measurement-factory.com>

On 09/20/2018 12:36 PM, Brett wrote:
> I currently have squid setup to use a self-signed certificate for MITM to
> cache HTTPS requests. This works. [...]

> Is there a way I can configure squid so I can specify
> it as a proxy for an https request and then have it act as a cache or
> forward to an HTTP proxy (that supports CONNECT)?

AFAICT, you are asking about the missing "SslBump with cache_peer"
feature, which was covered in several recent threads, including this email:

http://lists.squid-cache.org/pipermail/squid-users/2018-July/018653.html


> ssl_bump peek step1
> ssl_bump bump all

This configuration bumps everything at step2.


> If I change the ssl_bump directives above to the following:

> ssl_bump stare step2
> ssl_bump bump step3

This (misleading!) configuration should splice everything at step1. In
other words, it should be equivalent to this (clear) configuration:

  ssl_bump splice all

or a disabled SslBump. According to your tests, that is exactly what
happens (and the lack of non-trivial SslBump involvement probably
explains why peering works in this corner case).


If you need more information about the equivalence of the last two
configurations, please consider studying the following wiki page and a
related recent email thread:

* https://wiki.squid-cache.org/Features/SslPeekAndSplice
*
http://lists.squid-cache.org/pipermail/squid-users/2018-September/019162.html


HTH,

Alex.


From squid3 at treenet.co.nz  Thu Sep 20 19:50:05 2018
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Fri, 21 Sep 2018 07:50:05 +1200
Subject: [squid-users] Help: squid restarts and squidGuard die
In-Reply-To: <CY1PR16MB0459DBC3A42A489E531F323DB6130@CY1PR16MB0459.namprd16.prod.outlook.com>
References: <CA+d==oEt8J-ie2Fm1SYrHSBKD6Y7TRF-v3iGiMXC7WQ36WV1vw@mail.gmail.com>
 <f8f51c90-6bd7-09d0-a90b-c9552748d813@treenet.co.nz>
 <1537278855873-0.post@n4.nabble.com>
 <daf9e35b-5294-39aa-2116-ce36d663e330@treenet.co.nz>
 <CY1PR16MB045967CC3477DAE98EE094DFB61C0@CY1PR16MB0459.namprd16.prod.outlook.com>
 <20180920111612.GA26863@fantomas.sk>
 <CY1PR16MB0459DBC3A42A489E531F323DB6130@CY1PR16MB0459.namprd16.prod.outlook.com>
Message-ID: <459c04a0-e09d-1c8f-fea2-12b650eb4f00@treenet.co.nz>

On 21/09/18 3:46 AM, Donald Muller wrote:
> 
>> -----Original Message-----
>> From: Matus UHLAR - fantomas
>> Sent: Thursday, September 20, 2018 7:16 AM
>>
>> On 19.09.18 20:47, Donald Muller wrote:
>>> So instead of using squidguard are you saying  you should use something
>> like the following?
>>>
>>> acl ads dstdomain -i "/etc/squid/squid-ads.acl"
>>> acl adult dstdomain -i "/etc/squid/squid-adult.acl"
>>>
>>> http_access deny ads
>>> http_access deny adult
>>>
>>> Do the lists need to be sorted in alphabetical order?
>>
>> I don't think so - the lists are parsed to in -memory format for faster
>> processing.
>>
> 
> Does Squid monitor dstdomain files for changes and reload them or does a '-k reconfigure' need to be issued?
> 

Not currently. I'm looking for a nice portable way to do file watching.

The Linux inotify system can apparently be used to send the -k
reconfigure command on FS changes in the config directory. Though I've
yet to see any working example and have not had the time myself to
experiment on it.

Patches and/or info welcome. This might be a good starter project if
anyone wants to dip their fingers into the Squid code.

Amos


From squid3 at treenet.co.nz  Thu Sep 20 20:41:48 2018
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Fri, 21 Sep 2018 08:41:48 +1200
Subject: [squid-users] Help: squid restarts and squidGuard die
In-Reply-To: <f926b42a-42eb-9497-8064-b1d87b82e7fb@urlfilterdb.com>
References: <CA+d==oEt8J-ie2Fm1SYrHSBKD6Y7TRF-v3iGiMXC7WQ36WV1vw@mail.gmail.com>
 <f8f51c90-6bd7-09d0-a90b-c9552748d813@treenet.co.nz>
 <1537278855873-0.post@n4.nabble.com>
 <daf9e35b-5294-39aa-2116-ce36d663e330@treenet.co.nz>
 <69fda549-1455-a336-9bd6-057828f67e0e@urlfilterdb.com>
 <bceeb9cf-128a-79e7-8fc4-e31e9c0e27c9@treenet.co.nz>
 <f926b42a-42eb-9497-8064-b1d87b82e7fb@urlfilterdb.com>
Message-ID: <22f8fd29-c1c0-f764-2343-2e95600b7cdd@treenet.co.nz>

On 21/09/18 3:46 AM, Marcus Kool wrote:
> 
> On 20/09/18 08:46, Amos Jeffries wrote:
>> On 19/09/18 11:49 PM, Marcus Kool wrote:
>>>
>>> On 18/09/18 23:03, Amos Jeffries wrote:
>>>> On 19/09/18 1:54 AM, neok wrote:
>>>>> Thank you very much Amos for putting me in the right direction.
>>>>> I successfully carried out the modifications you indicated to me.
>>>>> Regarding ufdbGuard, if I understood correctly, what you recommend is
>>>>> to use
>>>>> the ufdbConvertDB tool to convert my blacklists in plain text to the
>>>>> ufdbGuard database format? And then use that/those databases in
>>>>> normal squid
>>>>> ACL's?
>>>>
>>>> No, ufdbguard is a fork of SquidGuard that can be used as a drop-in
>>>> replacement which works better while you improve your config.
>>>>
>>>> You should work towards less complexity. Squid / squid.conf is where
>>>> HTTP access control takes place. The helper is about re-writing the URL
>>>> (only) - which is a complex and destructive process.
>>>
>>> ufdbGuard is a simple tool that has the same syntax in its configuration
>>> file as squidGuard has.
>>> It is far from complex, has a great Reference Manual, exmaple config
>>> file and a responsive support desk.
>>> Amos, I have never seen you calling a URL writer being a complex and
>>> destructive process.? What do you mean?
>>
>> Re-writing requires Squid to:
>> ? * fork external helpers, and
>> ? * maintain queues of lookups to those helpers, and
>> ? * maintain cache of helper responses, and
>> ? * maintain a whole extra copy of HTTP-request state, and
>> ? * copy some (not all) of that state info between the two "client"
>> requests.
>>
>> ? ... lots of complexity, memory, CPU time, traffic latency, etc.
> 
> Squid itself is complex and for any feature of Squid one can make a list
> like above to say that it is complex.
> The fact that one can make such a list does not mean much to me.
> One can make the same or a similar list for external acl helpers and
> even native acls.
> 
>> Also when used for access control (re-write to an "error" URL) the
>> re-write helper needs extra complexity in itself to act as the altered
>> origin server for error pages, or have some fourth-party web server.
> 
> Squid cannot do everything that a URL writer, and specifically
> ufdbGuard, can.
> For example, Squid must restart and break all open connections when a
> tiny detail of the configuration changes.? With ufdbGuard this does not
> happen.


Squid does not close or break any client connections when reconfigured.
Squid pauses active transactions, reconfigures then continues with the
new config.

Are you perhapse mistaking the fact that Squid shuts down the
*rewriters* on reconfigure for a full Squid shutdown?

(hmm, there is another downside to placing all the access control in a
helper - waiting for the helpers to restart on config changes. Though as
you say ufdbguard does it efficiently, others do not).



> ufdbGuard supports dynamic lists of users, domains and source ip
> addresses which are updated every X minutes without any service
> interruption.

So does Squid, via external ACL and/or authentication.


> When other parameters change, ufdbGuard resets itself with zero service
> interruption for Squid and its users.

This is not always true. If the helper pauses even for some milliseconds
it is holding up Squid and clients. Particularly if it is a bottleneck
process like URL-rewrite interface where the helper lookup queue limits
total traffic capacity of the entire proxy.

I think you mean that the helper has threading to do a load in the
background and swap in the config. Correct?

Squid is working (very slowly) towards that model and the SMP features
already reconfigure one worker at a time sequentially so effectively
there should always be a helper with either old or new config answering
incoming traffic while one "resets itself".


> ufdbGuard can decide to probe a site to make a decision, and hence
> detect Skype, Teamviewer and other types of sites that an admin might
> want to block.? Squid cannot.

Squid can, via external ACL. IIRC, Eliezer wrote an ICAP system that did
that too.

Also, the URL-rewrite helper cannot do anything if Squid cannot pass it
a URL. By nature of what the interface is designed to do.


> ufdbGuard can decide to do a lookup of a reverse IP lookup to make a
> decision.? Squid cannot.

Squid can via external ACL.

We have not had much (any?) requests for an ACL doing that. Patches welcome.


> ufdbGuard supports complex time restrictions for access. Squid support
> simple time restrictions.

Such as?

Squid supports complex time points and/or ranges. The time ACL is a
bitmap extending at 1 second intervals across an entire week. Further
extension is done with external ACL, note ACL and/or allof ACL.


> ufdbGuard supports flat file domain/url lists and a commercial URL
> database.? Squid does not.
> And the list goes on.

I am still looking for a feature Squid does not actually support in one
way or another.

As you can see from Flashdown posts "external ACL" can mean SquidGuard /
ufdbguard running on that other interface. So really *anything* they can
do so can Squid external ACL - if not one of the other mechanisms.
At no point is the URL-rewrite API _necessary_ for access control in a
modern Squid.


> 
> So when you state on the mailing list that users should unconditionally
> stop using a URL writer in favor of using Squid acls, you may be causing
> troubles for admins who do not know the implications of your advice.


Understood. I shall try harder to remember the disclaimers usually
added. Thank you for pointing out the omission.


> 
>>> URL rewriters have been used for decades for HTTP access control but you
>>> state "squid.conf is where HTTP access control takes place".
>>
>> Once upon a time, back at the dawn of the WWW (before the 1990s) Squid
>> lacked external_acl_type and modular ACLs.
>>
>> That persisted for the first decade or so of Squid's life, with only the
>> re-write API for admin to use for complicated permissions.
>>
>> Then one day about 2 decades or so ago, external ACL was added and the
>> ACLs were also made much easier to implement and plug in new checks.
>> Today we have hundreds of native ACLs and even a selection of custom ACL
>> helpers. Making the need for these abuses of the poor re-writers.
>>
>> Old habits and online tutorials however are hard to get rid of.
> 
> If you want to get rid of habits that in your view are old/obsolete,
> then why not start a discussion?

I have. This appears to be the latest one.


> And in the event that at the end of the discussion, the decision is made
> that a particular interface should be removed, why not phase it out ?

It still has uses as a URL-rewrite/redirect interface for actions not
related directly to access control.

> 
>>> Are you saying that you want it is the _only_ place for HTTP access
>>> control?
>>
>>
>> I'm saying the purpose of the url_rewrite_* API in Squid is to tell
>> Squid whether the URL (only) needs some mangling in order for the
>> server/origin to understand it.
>> ? It can re-write transparently with all the problems that causes to
>> security scopes and URL sync between the endpoints. Or redirect the
>> client to the "correct" URL.
>>
>>
>> The Squid http_access and similar *access controls* are the place for
>> access control - hint is in the naming. With external ACL type for
>> anything Squid does not support natively or well. As Flashdown mentioned
>> even calls to SquidGuard etc. can be wrapped and used as external ACLs.
> 
> Wrapping and externals ACLs adds the same complexity, memory, CPU time,
> traffic latency, etc that you use as an argument against a URL writer.
> Is it only because of the name, 'external acl helper' vs 'url rewriter
> helper', that you dislike the url rewriter?


The external ACL usage is better because it is run at times when Squid
is making decisions about access to the proxy and/or features supplied
by the proxy. The admin can decide exactly when in Squid processing it
is tested - limiting the delays and reducing the lookups to only when
necessary.
 eg. you cannot decide whether a transaction is allowed to be sent to
ICAP or not with a re-writer. Or decide whether ssl_bump is going to
splice vs bump with a URL re-writer.


URL-rewrite API requires access control to *prevent* it being used, and
runs *after* access control - so Squid has to actively allow the traffic
through its own access controls first in order to use the URL-rewrite
API. This leads to a lot of the "http_access allow all" type config issues.


Amos


From squid3 at treenet.co.nz  Thu Sep 20 20:47:55 2018
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Fri, 21 Sep 2018 08:47:55 +1200
Subject: [squid-users] About SSL peek-n-splice/bump configurations
In-Reply-To: <CY1PR16MB045967CC3477DAE98EE094DFB61C0@CY1PR16MB0459.namprd16.prod.outlook.com>
References: <CA+d==oEt8J-ie2Fm1SYrHSBKD6Y7TRF-v3iGiMXC7WQ36WV1vw@mail.gmail.com>
 <f8f51c90-6bd7-09d0-a90b-c9552748d813@treenet.co.nz>
 <1537278855873-0.post@n4.nabble.com>
 <daf9e35b-5294-39aa-2116-ce36d663e330@treenet.co.nz>
 <CY1PR16MB045967CC3477DAE98EE094DFB61C0@CY1PR16MB0459.namprd16.prod.outlook.com>
Message-ID: <07decdde-47df-d2a7-6396-6aed43319d03@treenet.co.nz>

On 20/09/18 9:35 AM, Donald Muller wrote:
> Amos,
> 
> So instead of using squidguard are you saying  you should use something like the following?
> 
> acl ads dstdomain -i "/etc/squid/squid-ads.acl"
> acl adult dstdomain -i "/etc/squid/squid-adult.acl"
> 

*If* those lists contain dstdomain format names. Otherwise, no some
other ACL may be better (dstdom_regex?).

NP: The -i should not be necessary on dstdomain since domain comparsions
are case insensitive and regex are not correct syntax for dstdomain.

Also, as Matus reminded me. I should have said up front this is
something to consider doing - you may decide no to for reasons. One of
which is if those lists are very large the helper can be faster.


> http_access deny ads
> http_access deny adult
> 
> Do the lists need to be sorted in alphabetical order?
> 
> Don

No. Squid does that. For dstdomain they do need to be reduced so you are
not adding a subdomain like "www.example.com" which overlaps a wildcard
domain like ".example.com" elsewhere in the list.

Amos


From brett.anderson.ftw at gmail.com  Thu Sep 20 21:26:09 2018
From: brett.anderson.ftw at gmail.com (Brett Anderson)
Date: Thu, 20 Sep 2018 14:26:09 -0700
Subject: [squid-users] Is there any way to cache or forward https
 requests to an http proxy using Squid?
In-Reply-To: <c163d8cb-31be-5b5d-4c0c-0c876811ce4b@measurement-factory.com>
References: <1537468571079-0.post@n4.nabble.com>
 <c163d8cb-31be-5b5d-4c0c-0c876811ce4b@measurement-factory.com>
Message-ID: <CAKueR1phrRK0o=S5fa42evUBrykN57kr4BhBoTnsMV46=Dgu3g@mail.gmail.com>

Thank you!

I reverted back to:

ssl_bump peek step1
ssl_bump bump all

And then based on that first link you sent me I rebuilt my Squid instance
from
https://github.com/measurement-factory/squid/tree/SQUID-360-peering-for-SslBump

Then tested and I think it's working now?

>From my access log:
# testing https
# first request
1537477894.828    310 172.27.0.3 NONE/200 0 CONNECT foo.com:443 -
FIRSTUP_PARENT/64.58.117.175 -
1537477895.645    797 172.27.0.3 TCP_MISS/200 32374 GET
https://foo.com/js/bootstrap.min.js - FIRSTUP_PARENT/64.58.117.175
application/javascript
# second request
1537477899.009    336 172.27.0.3 NONE/200 0 CONNECT foo.com:443 -
FIRSTUP_PARENT/64.58.117.175 -
1537477899.019      0 172.27.0.3 TCP_MEM_HIT/200 32384 GET
https://foo.com/js/bootstrap.min.js - HIER_NONE/- application/javascript

# testing http
# first request
1537477956.088   1051 172.27.0.3 TCP_MISS/200 28203 GET
http://websites.web.com/ - FIRSTUP_PARENT/64.58.117.175 text/html
# second request
1537477957.888      2 172.27.0.3 TCP_MEM_HIT/200 28198 GET
http://websites.web.com/ - HIER_NONE/- text/html

Should I change anything else for more improvement? Should I build from the
master or a more recent branch of  https://github.com/measurement-factory
<https://github.com/measurement-factory/squid/tree/SQUID-360-peering-for-SslBump>
?

Thanks again!
B.

On Thu, Sep 20, 2018 at 12:47 PM Alex Rousskov <
rousskov at measurement-factory.com> wrote:

> On 09/20/2018 12:36 PM, Brett wrote:
> > I currently have squid setup to use a self-signed certificate for MITM to
> > cache HTTPS requests. This works. [...]
>
> > Is there a way I can configure squid so I can specify
> > it as a proxy for an https request and then have it act as a cache or
> > forward to an HTTP proxy (that supports CONNECT)?
>
> AFAICT, you are asking about the missing "SslBump with cache_peer"
> feature, which was covered in several recent threads, including this email:
>
> http://lists.squid-cache.org/pipermail/squid-users/2018-July/018653.html
>
>
> > ssl_bump peek step1
> > ssl_bump bump all
>
> This configuration bumps everything at step2.
>
>
> > If I change the ssl_bump directives above to the following:
>
> > ssl_bump stare step2
> > ssl_bump bump step3
>
> This (misleading!) configuration should splice everything at step1. In
> other words, it should be equivalent to this (clear) configuration:
>
>   ssl_bump splice all
>
> or a disabled SslBump. According to your tests, that is exactly what
> happens (and the lack of non-trivial SslBump involvement probably
> explains why peering works in this corner case).
>
>
> If you need more information about the equivalence of the last two
> configurations, please consider studying the following wiki page and a
> related recent email thread:
>
> * https://wiki.squid-cache.org/Features/SslPeekAndSplice
> *
>
> http://lists.squid-cache.org/pipermail/squid-users/2018-September/019162.html
>
>
> HTH,
>
> Alex.
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20180920/c87aa33f/attachment.htm>

From rousskov at measurement-factory.com  Thu Sep 20 23:52:47 2018
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Thu, 20 Sep 2018 17:52:47 -0600
Subject: [squid-users] Help: squid restarts and squidGuard die
In-Reply-To: <459c04a0-e09d-1c8f-fea2-12b650eb4f00@treenet.co.nz>
References: <CA+d==oEt8J-ie2Fm1SYrHSBKD6Y7TRF-v3iGiMXC7WQ36WV1vw@mail.gmail.com>
 <f8f51c90-6bd7-09d0-a90b-c9552748d813@treenet.co.nz>
 <1537278855873-0.post@n4.nabble.com>
 <daf9e35b-5294-39aa-2116-ce36d663e330@treenet.co.nz>
 <CY1PR16MB045967CC3477DAE98EE094DFB61C0@CY1PR16MB0459.namprd16.prod.outlook.com>
 <20180920111612.GA26863@fantomas.sk>
 <CY1PR16MB0459DBC3A42A489E531F323DB6130@CY1PR16MB0459.namprd16.prod.outlook.com>
 <459c04a0-e09d-1c8f-fea2-12b650eb4f00@treenet.co.nz>
Message-ID: <11374f56-4a97-59f3-9317-1ea92fbb3792@measurement-factory.com>

On 09/20/2018 01:50 PM, Amos Jeffries wrote:
> On 21/09/18 3:46 AM, Donald Muller wrote:


>> Does Squid monitor dstdomain files for changes and reload them or does a '-k reconfigure' need to be issued?


> Not currently. I'm looking for a nice portable way to do file watching.

> Patches and/or info welcome. This might be a good starter project if
> anyone wants to dip their fingers into the Squid code.

... but please start with an RFC on squid-dev before writing any Squid
code. Implementing correct file watching support in Squid is not
trivial, and the feature itself may not be such a good idea. Please
discuss your plans before spending time on modifying Squid.


Thank you,

Alex.


From rousskov at measurement-factory.com  Fri Sep 21 00:02:43 2018
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Thu, 20 Sep 2018 18:02:43 -0600
Subject: [squid-users] Help: squid restarts and squidGuard die
In-Reply-To: <22f8fd29-c1c0-f764-2343-2e95600b7cdd@treenet.co.nz>
References: <CA+d==oEt8J-ie2Fm1SYrHSBKD6Y7TRF-v3iGiMXC7WQ36WV1vw@mail.gmail.com>
 <f8f51c90-6bd7-09d0-a90b-c9552748d813@treenet.co.nz>
 <1537278855873-0.post@n4.nabble.com>
 <daf9e35b-5294-39aa-2116-ce36d663e330@treenet.co.nz>
 <69fda549-1455-a336-9bd6-057828f67e0e@urlfilterdb.com>
 <bceeb9cf-128a-79e7-8fc4-e31e9c0e27c9@treenet.co.nz>
 <f926b42a-42eb-9497-8064-b1d87b82e7fb@urlfilterdb.com>
 <22f8fd29-c1c0-f764-2343-2e95600b7cdd@treenet.co.nz>
Message-ID: <18f89f88-7b11-6c49-861c-0962b5d2c46c@measurement-factory.com>

On 09/20/2018 02:41 PM, Amos Jeffries wrote:

> Squid does not close or break any client connections when reconfigured.

IIRC, this statement is inaccurate (unfortunately): Reconfiguring Squid
may break client connections that Squid has not started processing yet.
The connections already being processed by Squid are not closed, but the
new/arriving ones may be rejected for a short time period. Such
rejections may affect clients in some environments. This is a bug, so I
hope it will get fixed.

This correction does not affect the rewriter-vs-ACLs comparison, but I
wanted to make it in case that statement is used outside its context.


Cheers,

Alex.


From fredrik at pipemore.se  Fri Sep 21 14:43:54 2018
From: fredrik at pipemore.se (uppsalanet)
Date: Fri, 21 Sep 2018 09:43:54 -0500 (CDT)
Subject: [squid-users] redirect based on url (302)
Message-ID: <1537541034108-0.post@n4.nabble.com>

Hi,
We use squid to limit web traffic to a few internal sites, the computers are
in in public areas. That works good. Now I have a new case:

If a user goes to page "https://browzine.com" and choose to view a magazine
they get redirected (302) to an other site. I would like to open for that
redirect if it's "https://browzine.com" (api.thirdiron.com) who does the
redirect.

Ex:
https://browzine.com -> http://api.thirdiron.com ->
/https://www.sciencedirect.com (this last redirect differ alot based on
magazine provider)/
Header:
*General*
  Request URL:
http://api.thirdiron.com/v2/libraries/223/articles/203497919/content
  Request Method: GET
  *Status Code: 302 Found*
  Remote Address: 54.221.220.6:80
  Referrer Policy: no-referrer-when-downgrade

* Response Header*
  Access-Control-Allow-Headers: Content-Type, Authorization
  Access-Control-Allow-Methods: DELETE,GET,PATCH,POST,PUT
  Access-Control-Allow-Origin: *
  Connection: keep-alive
  Date: Fri, 21 Sep 2018 13:36:18 GMT
  *Location:
https://www.sciencedirect.com/science/article/pii/S2212671612001655*
  Server: Cowboy
  Transfer-Encoding: chunked
  Via: 1.1 vegur
  X-Powered-By: Express

Brgd
Fredrik




--
Sent from: http://squid-web-proxy-cache.1019090.n4.nabble.com/Squid-Users-f1019091.html


From service.mv at gmail.com  Fri Sep 21 14:44:51 2018
From: service.mv at gmail.com (neok)
Date: Fri, 21 Sep 2018 09:44:51 -0500 (CDT)
Subject: [squid-users] Any suggestions or comments about my
 configuration? squid 3.5.20
In-Reply-To: <678580bb-dbfa-96d9-242c-ab5741f4bd28@treenet.co.nz>
References: <CA+d==oEEdkiBsx3adyW7OdTe=gQJc_GM+UJqmuOD2Q_+_d0UuQ@mail.gmail.com>
 <678580bb-dbfa-96d9-242c-ab5741f4bd28@treenet.co.nz>
Message-ID: <1537541091887-0.post@n4.nabble.com>

Thank you very much Amos!
Your indications have helped me a lot. I have been able to apply them to
practically all of them.

I share my configuration in case someone with the same objectives can use
it.

Greetings!

squid.conf

### Negotiate/NTLM and Negotiate/Kerberos authentication
auth_param negotiate program /usr/sbin/negotiate_wrapper --ntlm
/usr/bin/ntlm_auth --helper-protocol=squid-2.5-ntlmssp --kerberos
/usr/lib64/squid/negotiate_kerberos_auth -r -i -s GSS_C_NO_NAME 
auth_param negotiate children 10 
auth_param negotiate keep_alive on

### standard allowed ports
acl SSL_ports port 443 
acl Safe_ports port 80 # http 
acl Safe_ports port 21 # ftp 
acl Safe_ports port 443 # https 
acl Safe_ports port 70 # gopher 
acl Safe_ports port 210 # wais 
acl Safe_ports port 1025-65535 # unregistered ports 
acl Safe_ports port 280 # http-mgmt 
acl Safe_ports port 488 # gss-http 
acl Safe_ports port 591 # filemaker 
acl Safe_ports port 777 # multiling http 
acl CONNECT method CONNECT

### destination domains to be blocked in a HTTP access control policy
acl LS_adult dstdomain -i "/etc/squid/DBL/adult.txt"
acl LS_anonvpn dstdomain -i "/etc/squid/DBL/anonvpn.txt"
acl LS_hacking dstdomain -i "/etc/squid/DBL/hacking.txt"
acl LS_malicius dstdomain -i "/etc/squid/DBL/malicius.txt"
acl LS_remotecontrol dstdomain -i "/etc/squid/DBL/remotecontrol.txt"
acl LS_warez dstdomain -i "/etc/squid/DBL/warez.txt"

### acl for proxy authentication (kerberos or ntlm)
acl auth proxy_auth REQUIRED

### LDAP group membership sources ###
external_acl_type AD_WEB_ACCESS %LOGIN /usr/lib64/squid/ext_ldap_group_acl
-P -R -b "OU=USERS,DC=netgol,DC=local" -D ldap -W "/etc/squid/ldap_pass.txt"
-f
"(&(sAMAccountname=%u)(memberof=cn=%g,OU=INTERNET,OU=PERMISSIONS,OU=USERS,DC=netgol,DC=local))"
-h s-dc1.netgol.local
acl WEB_USERS_1 external AD_WEB_ACCESS WEB_USERS_1
acl WEB_USERS_2 external AD_WEB_ACCESS WEB_USERS_2
acl WEB_USERS_3 external AD_WEB_ACCESS WEB_USERS_3

### HTTP access control policies
http_access deny !Safe_ports 
http_access deny CONNECT !SSL_ports 
http_access allow localhost manager 
http_access deny manager
http_access deny !auth
http_access allow localhost
http_access deny LS_malicius			# malicius sites denied for all

http_access allow WEB_USERS_1					# WEB_USERS_1 member users can browse
without restrictions

http_access deny WEB_USERS_2 LS_remotecontrol	# WEB_USERS_2 member users
can't browse Remote Control sites
http_access deny WEB_USERS_2 LS_warez			# WEB_USERS_2 member users can't
browse Warez sites
http_access allow WEB_USERS_2					# WEB_USERS_2 member users can browse the
rest of the sites not bloqued

http_access deny WEB_USERS_3 LS_adult			# WEB_USERS_3 member users can't
browse Adult sites
http_access deny WEB_USERS_3 LS_anonvpn			# WEB_USERS_3 member users can't
browse Anonymous VPN sites
http_access deny WEB_USERS_3 LS_hacking			# WEB_USERS_3 member users can't
browse Hacking sites
http_access deny WEB_USERS_3 LS_remotecontrol	# WEB_USERS_3 member users
can't browse Remote Control sites
http_access deny WEB_USERS_3 LS_warez			# WEB_USERS_3 member users can't
browse Warez sites
http_access allow WEB_USERS_3					# WEB_USERS_3 member users can browse the
rest of the sites not bloqued

http_access deny all

### PERSONALIZATION ###
http_port 8080 
coredump_dir /var/spool/squid 
refresh_pattern ^ftp: 1440 20% 10080 
refresh_pattern ^gopher: 1440 0% 1440 
refresh_pattern -i (/cgi-bin/|\?) 0 0% 0 
refresh_pattern .  0 20% 4320 
quick_abort_min 0 KB 
quick_abort_max 0 KB 
read_timeout 5 minutes 
request_timeout 3 minutes 
shutdown_lifetime 15 seconds 
ipcache_size 2048 
fqdncache_size 4096 
forwarded_for off 
cache_mgr support at netgol.net 
visible_hostname proxy.netgol.local 
httpd_suppress_version_string on 
logfile_rotate 7



--
Sent from: http://squid-web-proxy-cache.1019090.n4.nabble.com/Squid-Users-f1019091.html


From rousskov at measurement-factory.com  Fri Sep 21 14:53:02 2018
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Fri, 21 Sep 2018 08:53:02 -0600
Subject: [squid-users] Is there any way to cache or forward https
 requests to an http proxy using Squid?
In-Reply-To: <CAKueR1phrRK0o=S5fa42evUBrykN57kr4BhBoTnsMV46=Dgu3g@mail.gmail.com>
References: <1537468571079-0.post@n4.nabble.com>
 <c163d8cb-31be-5b5d-4c0c-0c876811ce4b@measurement-factory.com>
 <CAKueR1phrRK0o=S5fa42evUBrykN57kr4BhBoTnsMV46=Dgu3g@mail.gmail.com>
Message-ID: <124124ef-b81f-c68e-805f-571420c9f88d@measurement-factory.com>

On 09/20/2018 03:26 PM, Brett Anderson wrote:
> Should I build from the master or a more recent branch?

IIRC, the unofficial branch you are using is the only branch containing
SslBump with cache_peer" feature today. We are working on submitting
that code for the official review. Please note that any unofficial code
comes with additional risks and is not eligible for the official Squid
Project support.

Alex.



> On Thu, Sep 20, 2018 at 12:47 PM Alex Rousskov wrote:
> 
>     On 09/20/2018 12:36 PM, Brett wrote:
>     > I currently have squid setup to use a self-signed certificate for
>     MITM to
>     > cache HTTPS requests. This works. [...]
> 
>     > Is there a way I can configure squid so I can specify
>     > it as a proxy for an https request and then have it act as a cache or
>     > forward to an HTTP proxy (that supports CONNECT)?
> 
>     AFAICT, you are asking about the missing "SslBump with cache_peer"
>     feature, which was covered in several recent threads, including this
>     email:
> 
>     http://lists.squid-cache.org/pipermail/squid-users/2018-July/018653.html
> 
> 
>     > ssl_bump peek step1
>     > ssl_bump bump all
> 
>     This configuration bumps everything at step2.
> 
> 
>     > If I change the ssl_bump directives above to the following:
> 
>     > ssl_bump stare step2
>     > ssl_bump bump step3
> 
>     This (misleading!) configuration should splice everything at step1. In
>     other words, it should be equivalent to this (clear) configuration:
> 
>     ? ssl_bump splice all
> 
>     or a disabled SslBump. According to your tests, that is exactly what
>     happens (and the lack of non-trivial SslBump involvement probably
>     explains why peering works in this corner case).
> 
> 
>     If you need more information about the equivalence of the last two
>     configurations, please consider studying the following wiki page and a
>     related recent email thread:
> 
>     * https://wiki.squid-cache.org/Features/SslPeekAndSplice
>     *
>     http://lists.squid-cache.org/pipermail/squid-users/2018-September/019162.html
> 
> 
>     HTH,
> 
>     Alex.
> 



From vh1988 at yahoo.com.ar  Fri Sep 21 15:08:11 2018
From: vh1988 at yahoo.com.ar (Julian Perconti)
Date: Fri, 21 Sep 2018 12:08:11 -0300
Subject: [squid-users] About SSL peek-n-splice/bump configurations
References: <000301d43289$14ed4770$3ec7d650$@yahoo.com.ar>
 <002001d446cc$148bb580$3da32080$@yahoo.com.ar>
 <c7a4d318-c335-03de-9e5c-73fc258e38c0@treenet.co.nz>
 <008001d4479b$b22b9290$1682b7b0$@yahoo.com.ar>
 <04c7931e-cb29-80d1-2a2a-8a1ccb4761f5@treenet.co.nz>
 <019301d4486b$049ebf00$0ddc3d00$@yahoo.com.ar>
 <6b762c34-f0c4-2d9a-6616-432305af160b@treenet.co.nz>
 <009d01d44935$1648ef80$42dace80$@yahoo.com.ar>
 <5400815d-9a82-97a9-5889-0e76c04a9ba0@measurement-factory.com>
 <003501d44aa4$d784e5d0$868eb170$@yahoo.com.ar>
 <500fbad5-d277-965f-d29a-a81c2f343d2e@measurement-factory.com>
 <004401d44b0e$46cf40c0$d46dc240$@yahoo.com.ar>
 <0472b838-415e-e4d2-c71a-7fbce45de926@measurement-factory.com>
 <008e01d44bbf$ccb41e20$661c5a60$@yahoo.com.ar>
 <2b2e8b70-7f45-039c-462f-ba78b2fc1563@measurement-factory.com>
 <006d01d44eaf$55ecc290$01c647b0$@yahoo.com.ar>
 <ee42d35f-2dc0-3bbc-f6c4-a1852374a832@measurement-factory.com>
 <003601d44f61$dfd30e80$9f792b80$@yahoo.com.ar>
 <8c393e26-822f-ddea-50d4-9f80ebc9087f@measurement-factor y.com>  
Message-ID: <002e01d451bc$ec104de0$c430e9a0$@yahoo.com.ar>

Hi all.

I will go (finally) with this sslBump config. Although I still have some doubts...
I think that It?s time to finish this thread.

#  TLS CFG
acl noBumpSites ssl::server_name_regex -i "/etc/squid/url.nobump"

# steps ACL
acl step1 at_step SslBump1
acl step2 at_step SslBump2
acl step3 at_step SslBump3

#  SslBump actions
ssl_bump peek step1
ssl_bump splice noBumpSites
ssl_bump stare step2

The TLS config "explained" as  well as I can understand:
*Clarification*: maybe I will quote some words out of context; but Alex told me that he almost always speaks "In general terms" about what Squid does.

# First rule:
ssl_bump peek step1 #  Step 1 is the only step that is always performed.

If I no peek at step1, and instead directly splice, happens what the wiki warns (this was checked):

" Bump All Sites Except Banks
" Usually does not work for requests that go to non-banks -- they will not be bumped." (Verified)
" Depending on other settings, Squid may terminate connections to banks if Squid cannot validate client SNI (Host header forgery detection) or the server certificate."
The wiki example about this warn the config is:
  ssl_bump splice serverIsBank
  ssl_bump peek all
  ssl_bump bump all

So my conclusion is: "It's "better" (to avoid: ...not work for requests that go to non-banks) to peek step1"

# Second rule:
ssl_bump splice noBumpSites 

Here a doubt, I'm sorry.
Based on above words and the squid behaviour I mentioned, I think that this rule should implicity match only at step2.

Alex words: 

>"So, "yes", Squid only executes the first rule action _when_ the first
>rule action is applicable and its ACLs match at every step, but, "no",
>Squid does not make a bunch of steps with only the first rule in mind."

With the overall logic in mind, the first impression is that the second rule could match at step1 and at step2 too. Like this rule would the first one (but is the second).
However as I said above if the splice is the first rule instead the peek, the squid?s behaviour changes.

>After a splice rule is applied, SslBump is over. No  more rules are 
>checked. No more loops are iterated. Squid simply "exits" the  SslBump 
>feature (and becomes a TCP tunnel).

Here, probably (not sure) Alex rerefered here to "splice all" rule. In that case is clear "splice is a final action" then no more future checks.
"Actions splice, bump, and terminate are final actions: They prevent further processing of the ssl_bump rules."

But in my config next to splice there is an ACL. That is why I asked: "But, doesn't the ACL matters?" in earlier mail.

Therefore, due to above Alex?s statement:  Will Squid ignore the last rule?
I checked that the answer is no. If I remove the last rule (stare step2) all the traffic is spliced.
I think that the reason is: (explicit) peek step1 >  (implicit) peek step2 > result: default splice all. (peek at step2 precludes future bumping)
Even more, if I remove the last rule, the second rule I think that will be ingnored. In reallity will had not make sense.

# Third/last rule:
ssl_bump stare step2 # stare at step2 so implicit and "secure" default bump action at step3.

Probably I said something (or all) that is WRONG.

Thank You.



From rousskov at measurement-factory.com  Fri Sep 21 15:13:30 2018
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Fri, 21 Sep 2018 09:13:30 -0600
Subject: [squid-users] About SSL peek-n-splice/bump configurations
In-Reply-To: <002e01d451bc$ec104de0$c430e9a0$@yahoo.com.ar>
References: <000301d43289$14ed4770$3ec7d650$@yahoo.com.ar>
 <008001d4479b$b22b9290$1682b7b0$@yahoo.com.ar>
 <04c7931e-cb29-80d1-2a2a-8a1ccb4761f5@treenet.co.nz>
 <019301d4486b$049ebf00$0ddc3d00$@yahoo.com.ar>
 <6b762c34-f0c4-2d9a-6616-432305af160b@treenet.co.nz>
 <009d01d44935$1648ef80$42dace80$@yahoo.com.ar>
 <5400815d-9a82-97a9-5889-0e76c04a9ba0@measurement-factory.com>
 <003501d44aa4$d784e5d0$868eb170$@yahoo.com.ar>
 <500fbad5-d277-965f-d29a-a81c2f343d2e@measurement-factory.com>
 <004401d44b0e$46cf40c0$d46dc240$@yahoo.com.ar>
 <0472b838-415e-e4d2-c71a-7fbce45de926@measurement-factory.com>
 <008e01d44bbf$ccb41e20$661c5a60$@yahoo.com.ar>
 <2b2e8b70-7f45-039c-462f-ba78b2fc1563@measurement-factory.com>
 <006d01d44eaf$55ecc290$01c647b0$@yahoo.com.ar>
 <ee42d35f-2dc0-3bbc-f6c4-a1852374a832@measurement-factory.com>
 <003601d44f61$dfd30e80$9f792b80$@yahoo.com.ar>
 <8c393e26-822f-ddea-50d4-9f80ebc9087f@measurement-factor y.com>
 <002e01d451bc$ec104de0$c430e9a0$@yahoo.com.ar>
Message-ID: <4ae1b8c3-46cf-906f-956d-a02e48d81c82@measurement-factory.com>

On 09/21/2018 09:08 AM, Julian Perconti wrote:

> I will go (finally) with this sslBump config. Although I still have some doubts...
> I think that It?s time to finish this thread.

I am confused because "you think it is time to finish this thread" but
you are asking new questions. Please clarify, do you want answers to the
questions in your last email?

Alex.


From squid3 at treenet.co.nz  Fri Sep 21 16:35:25 2018
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sat, 22 Sep 2018 04:35:25 +1200
Subject: [squid-users] redirect based on url (302)
In-Reply-To: <1537541034108-0.post@n4.nabble.com>
References: <1537541034108-0.post@n4.nabble.com>
Message-ID: <dde485b6-5742-d424-8e12-f700d0b975f2@treenet.co.nz>

On 22/09/18 2:43 AM, uppsalanet wrote:
> Hi,
> We use squid to limit web traffic to a few internal sites, the computers are
> in in public areas. That works good. Now I have a new case:
> 
> If a user goes to page "https://browzine.com" and choose to view a magazine
> they get redirected (302) to an other site. I would like to open for that
> redirect if it's "https://browzine.com" (api.thirdiron.com) who does the
> redirect.


Can you explain that differently please?


Amos


From vh1988 at yahoo.com.ar  Fri Sep 21 16:41:25 2018
From: vh1988 at yahoo.com.ar (Julian Perconti)
Date: Fri, 21 Sep 2018 13:41:25 -0300
Subject: [squid-users] About SSL peek-n-splice/bump configurations
In-Reply-To: <4ae1b8c3-46cf-906f-956d-a02e48d81c82@measurement-factory.com>
References: <000301d43289$14ed4770$3ec7d650$@yahoo.com.ar>
 <008001d4479b$b22b9290$1682b7b0$@yahoo.com.ar>
 <04c7931e-cb29-80d1-2a2a-8a1ccb4761f5@treenet.co.nz>
 <019301d4486b$049ebf00$0ddc3d00$@yahoo.com.ar>
 <6b762c34-f0c4-2d9a-6616-432305af160b@treenet.co.nz>
 <009d01d44935$1648ef80$42dace80$@yahoo.com.ar>
 <5400815d-9a82-97a9-5889-0e76c04a9ba0@measurement-factory.com>
 <003501d44aa4$d784e5d0$868eb170$@yahoo.com.ar>
 <500fbad5-d277-965f-d29a-a81c2f343d2e@measurement-factory.com>
 <004401d44b0e$46cf40c0$d46dc240$@yahoo.com.ar>
 <0472b838-415e-e4d2-c71a-7fbce45de926@measurement-factory.com>
 <008e01d44bbf$ccb41e20$661c5a60$@yahoo.com.ar>
 <2b2e8b70-7f45-039c-462f-ba78b2fc1563@measurement-factory.com>
 <006d01d44eaf$55ecc290$01c647b0$@yahoo.com.ar>
 <ee42d35f-2dc0-3bbc-f6c4-a1852374a832@measurement-factory.com>
 <003601d44f61$dfd30e80$9f792b80$@yahoo.com.ar>
 <8c393e26-822f-ddea-50d4-9f80ebc9087f@measurement-factor y.com>
 <002e01d451bc$ec104de0$c430e9a0$@yahoo.com.ar>
 <4ae1b8c3-46cf-906f-956d-a02e48d81c82@measure ment-factory.com>
Message-ID: <003201d451c9$f1bb7e60$d5327b20$@yahoo.com.ar>

> > I will go (finally) with this sslBump config. Although I still have some
> doubts...
> > I think that It?s time to finish this thread.
> 
> I am confused because "you think it is time to finish this thread" but you are
> asking new questions. Please clarify, do you want answers to the questions
> in your last email? all right. If you think that at this point it is worth it then do it.

Yes maybe I was contradictory.
All right. If you think that at this point it is worth it then do it.
Since along the thread I draw many wrong conclusions.
It's my fault, not your fault.





From donmuller22 at outlook.com  Fri Sep 21 17:18:12 2018
From: donmuller22 at outlook.com (Donald Muller)
Date: Fri, 21 Sep 2018 17:18:12 +0000
Subject: [squid-users] Help: squid restarts and squidGuard die
In-Reply-To: <459c04a0-e09d-1c8f-fea2-12b650eb4f00@treenet.co.nz>
References: <CA+d==oEt8J-ie2Fm1SYrHSBKD6Y7TRF-v3iGiMXC7WQ36WV1vw@mail.gmail.com>
 <f8f51c90-6bd7-09d0-a90b-c9552748d813@treenet.co.nz>
 <1537278855873-0.post@n4.nabble.com>
 <daf9e35b-5294-39aa-2116-ce36d663e330@treenet.co.nz>
 <CY1PR16MB045967CC3477DAE98EE094DFB61C0@CY1PR16MB0459.namprd16.prod.outlook.com>
 <20180920111612.GA26863@fantomas.sk>
 <CY1PR16MB0459DBC3A42A489E531F323DB6130@CY1PR16MB0459.namprd16.prod.outlook.com>
 <459c04a0-e09d-1c8f-fea2-12b650eb4f00@treenet.co.nz>
Message-ID: <CY1PR16MB04597442B5820B3B64E97A53B6120@CY1PR16MB0459.namprd16.prod.outlook.com>



> -----Original Message-----
> From: squid-users <squid-users-bounces at lists.squid-cache.org> On Behalf
> Of Amos Jeffries
> Sent: Thursday, September 20, 2018 3:50 PM
> To: squid-users at lists.squid-cache.org
> Subject: Re: [squid-users] Help: squid restarts and squidGuard die
> 
> On 21/09/18 3:46 AM, Donald Muller wrote:
> >
> >> -----Original Message-----
> >> From: Matus UHLAR - fantomas
> >> Sent: Thursday, September 20, 2018 7:16 AM
> >>
> >> On 19.09.18 20:47, Donald Muller wrote:
> >>> So instead of using squidguard are you saying  you should use
> >>> something
> >> like the following?
> >>>
> >>> acl ads dstdomain -i "/etc/squid/squid-ads.acl"
> >>> acl adult dstdomain -i "/etc/squid/squid-adult.acl"
> >>>
> >>> http_access deny ads
> >>> http_access deny adult
> >>>
> >>> Do the lists need to be sorted in alphabetical order?
> >>
> >> I don't think so - the lists are parsed to in -memory format for
> >> faster processing.
> >>
> >
> > Does Squid monitor dstdomain files for changes and reload them or does a
> '-k reconfigure' need to be issued?
> >
> 
> Not currently. I'm looking for a nice portable way to do file watching.
> 
> The Linux inotify system can apparently be used to send the -k reconfigure
> command on FS changes in the config directory. Though I've yet to see any
> working example and have not had the time myself to experiment on it.
> 
> Patches and/or info welcome. This might be a good starter project if anyone
> wants to dip their fingers into the Squid code.
> 

I will be downloading the blacklists from the internet and I'm sure that there will be sites that I want to whitelist via

acl whitelist dstdomain "/some folder path/whitelist.acl"
http_access allow whitelist

What logging do I need to enable to capture when a domain is blacklisted?


> Amos
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users

From robertocarna36 at gmail.com  Fri Sep 21 20:32:02 2018
From: robertocarna36 at gmail.com (Roberto Carna)
Date: Fri, 21 Sep 2018 17:32:02 -0300
Subject: [squid-users] url_rewrite_program doesnt' work after cloning
Message-ID: <CAG2Qp6tOQWXvgDDz9SjQc7rq+9U-vYCN099NB4D065tD_n9iEw@mail.gmail.com>

Dear, I have a Squid 3.5 server working OK.

After cloning this server two times, both the new two servers don't execute
the following line from /etc/squid/squid.conf:

url_rewrite_program /usr/bin/squidGuard -c /etc/squidguard/squidGuard.conf

If I execute "/usr/bin/squidGuard -c /etc/squidguard/squidGuard.conf"
manually, the Squidguard works OK.

What can be the problem ? What can be wrong after cloning ? I compare files
and permissions and they are the same.

Thanks in advance, regards !!!
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20180921/c0c8285b/attachment.htm>

From brett.anderson.ftw at gmail.com  Fri Sep 21 21:00:15 2018
From: brett.anderson.ftw at gmail.com (Brett Anderson)
Date: Fri, 21 Sep 2018 14:00:15 -0700
Subject: [squid-users] Is there any way to cache or forward https
 requests to an http proxy using Squid?
In-Reply-To: <124124ef-b81f-c68e-805f-571420c9f88d@measurement-factory.com>
References: <1537468571079-0.post@n4.nabble.com>
 <c163d8cb-31be-5b5d-4c0c-0c876811ce4b@measurement-factory.com>
 <CAKueR1phrRK0o=S5fa42evUBrykN57kr4BhBoTnsMV46=Dgu3g@mail.gmail.com>
 <124124ef-b81f-c68e-805f-571420c9f88d@measurement-factory.com>
Message-ID: <CAD2npNYTpUjJpqq99Q07zmE94Omi6JGzKT8H69+kDR=Y7Xq8Jg@mail.gmail.com>

Thanks again Alex,

For anyone else trying to solve this issue, here's a repo I created which
sets everything up in Docker to allow ssl_bump and cache_peer to work.
https://github.com/brett--anderson/squid_proxy

On Fri, Sep 21, 2018 at 7:53 AM Alex Rousskov <
rousskov at measurement-factory.com> wrote:

> On 09/20/2018 03:26 PM, Brett Anderson wrote:
> > Should I build from the master or a more recent branch?
>
> IIRC, the unofficial branch you are using is the only branch containing
> SslBump with cache_peer" feature today. We are working on submitting
> that code for the official review. Please note that any unofficial code
> comes with additional risks and is not eligible for the official Squid
> Project support.
>
> Alex.
>
>
>
> > On Thu, Sep 20, 2018 at 12:47 PM Alex Rousskov wrote:
> >
> >     On 09/20/2018 12:36 PM, Brett wrote:
> >     > I currently have squid setup to use a self-signed certificate for
> >     MITM to
> >     > cache HTTPS requests. This works. [...]
> >
> >     > Is there a way I can configure squid so I can specify
> >     > it as a proxy for an https request and then have it act as a cache
> or
> >     > forward to an HTTP proxy (that supports CONNECT)?
> >
> >     AFAICT, you are asking about the missing "SslBump with cache_peer"
> >     feature, which was covered in several recent threads, including this
> >     email:
> >
> >
> http://lists.squid-cache.org/pipermail/squid-users/2018-July/018653.html
> >
> >
> >     > ssl_bump peek step1
> >     > ssl_bump bump all
> >
> >     This configuration bumps everything at step2.
> >
> >
> >     > If I change the ssl_bump directives above to the following:
> >
> >     > ssl_bump stare step2
> >     > ssl_bump bump step3
> >
> >     This (misleading!) configuration should splice everything at step1.
> In
> >     other words, it should be equivalent to this (clear) configuration:
> >
> >       ssl_bump splice all
> >
> >     or a disabled SslBump. According to your tests, that is exactly what
> >     happens (and the lack of non-trivial SslBump involvement probably
> >     explains why peering works in this corner case).
> >
> >
> >     If you need more information about the equivalence of the last two
> >     configurations, please consider studying the following wiki page and
> a
> >     related recent email thread:
> >
> >     * https://wiki.squid-cache.org/Features/SslPeekAndSplice
> >     *
> >
> http://lists.squid-cache.org/pipermail/squid-users/2018-September/019162.html
> >
> >
> >     HTH,
> >
> >     Alex.
> >
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20180921/168b1979/attachment.htm>

From rousskov at measurement-factory.com  Fri Sep 21 21:19:55 2018
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Fri, 21 Sep 2018 15:19:55 -0600
Subject: [squid-users] About SSL peek-n-splice/bump configurations
In-Reply-To: <002e01d451bc$ec104de0$c430e9a0$@yahoo.com.ar>
References: <000301d43289$14ed4770$3ec7d650$@yahoo.com.ar>
 <008001d4479b$b22b9290$1682b7b0$@yahoo.com.ar>
 <04c7931e-cb29-80d1-2a2a-8a1ccb4761f5@treenet.co.nz>
 <019301d4486b$049ebf00$0ddc3d00$@yahoo.com.ar>
 <6b762c34-f0c4-2d9a-6616-432305af160b@treenet.co.nz>
 <009d01d44935$1648ef80$42dace80$@yahoo.com.ar>
 <5400815d-9a82-97a9-5889-0e76c04a9ba0@measurement-factory.com>
 <003501d44aa4$d784e5d0$868eb170$@yahoo.com.ar>
 <500fbad5-d277-965f-d29a-a81c2f343d2e@measurement-factory.com>
 <004401d44b0e$46cf40c0$d46dc240$@yahoo.com.ar>
 <0472b838-415e-e4d2-c71a-7fbce45de926@measurement-factory.com>
 <008e01d44bbf$ccb41e20$661c5a60$@yahoo.com.ar>
 <2b2e8b70-7f45-039c-462f-ba78b2fc1563@measurement-factory.com>
 <006d01d44eaf$55ecc290$01c647b0$@yahoo.com.ar>
 <ee42d35f-2dc0-3bbc-f6c4-a1852374a832@measurement-factory.com>
 <003601d44f61$dfd30e80$9f792b80$@yahoo.com.ar>
 <8c393e26-822f-ddea-50d4-9f80ebc9087f@measurement-factor y.com>
 <002e01d451bc$ec104de0$c430e9a0$@yahoo.com.ar>
Message-ID: <d4245344-bc04-f8b3-e1ea-fd1956172413@measurement-factory.com>

On 09/21/2018 09:08 AM, Julian Perconti wrote:

> ssl_bump peek step1
> ssl_bump splice noBumpSites
> ssl_bump stare step2


> # Second rule:
> ssl_bump splice noBumpSites 
> 
> I think that this rule should implicity match only at step2.

I do not know what "implicitly match" means here, but yes, the splice
rule may only match at step2 in this configuration:

* It cannot match at step1 because the earlier "peek" rule matches at step1.

* It is always reached during step2 because no rules above it can match
during step2. Whether it matches during step2 depends on whether
noBumpSites matches a particular transaction during step2.

* It cannot match at step3 because for a splice rule to match at step3 a
peek rule has to match at step2, and there is no peek rule that can
match at step2 in your configuration.


> However as I said above if the splice is the first rule instead the
> peek, the squid?s behaviour changes.

Naturally. If you place the splice rule first, it may match during step1
as well. If you do not, it cannot.


>> After a splice rule is applied, SslBump is over. No  more rules are 
>> checked. No more loops are iterated. Squid simply "exits" the  SslBump 
>> feature (and becomes a TCP tunnel).

> Here, probably (not sure) Alex rerefered here to "splice all" rule.

I think you are ignoring or misinterpreting the verb "applied". Here,
"applied" means Squid has executed the rule action. Not just considered
the rule containing that action, but actually ran that action. Applying
a rule action implies that the rule ACLs (whatever they were) matched,
of course. A rule action is never applied when the rule ACLs do not match.


> In that case is clear "splice is a final action" then no more future checks.

The notion of a "final" action does not depend on rule ACLs. After Squid
applies the "splice" action (in whatever context, for whatever reason),
SslBump processing for that transaction is over. Same for "bump" and
"terminate" actions.


> But in my config next to splice there is an ACL. That is why I asked: "But, doesn't the ACL matters?" in earlier mail.

ACLs (and other things) determine which rules match. After a rule
matches, then Squid applies its action, and then the notion of a "final
action" starts to matter.


> Will Squid ignore the last rule?

No. The last rule will be applied at step2 whenever noBumpSites
mismatches at step2.


HTH,

Alex.


From squid3 at treenet.co.nz  Sat Sep 22 09:12:18 2018
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sat, 22 Sep 2018 21:12:18 +1200
Subject: [squid-users] url_rewrite_program doesnt' work after cloning
In-Reply-To: <CAG2Qp6tOQWXvgDDz9SjQc7rq+9U-vYCN099NB4D065tD_n9iEw@mail.gmail.com>
References: <CAG2Qp6tOQWXvgDDz9SjQc7rq+9U-vYCN099NB4D065tD_n9iEw@mail.gmail.com>
Message-ID: <341bab91-5712-6312-001c-6904086aa5e0@treenet.co.nz>

On 22/09/18 8:32 AM, Roberto Carna wrote:
> Dear, I have a Squid 3.5 server working OK.
> 
> After cloning this server two times, both the new two servers don't
> execute the following line from /etc/squid/squid.conf:
> 

What makes you think that?

> url_rewrite_program /usr/bin/squidGuard -c /etc/squidguard/squidGuard.conf
> 
> If I execute "/usr/bin/squidGuard -c /etc/squidguard/squidGuard.conf"
> manually, the Squidguard works OK.
> 
> What can be the problem ? What can be wrong after cloning ? I compare
> files and permissions and they are the same.

Details please. Starting with what detail is making you think there is a
problem. "Something don't work" is not nearly enough.

You say "clone" what type?  VBox, LxC, Docker, Flatpack, SMP,
multi-tenant, multi-instance, something else?

Be specific and detailed in what is going wrong, what you have done, and
what happened as a result.

Amos


From vh1988 at yahoo.com.ar  Sat Sep 22 16:40:33 2018
From: vh1988 at yahoo.com.ar (Julian Perconti)
Date: Sat, 22 Sep 2018 13:40:33 -0300
Subject: [squid-users] About SSL peek-n-splice/bump configurations
In-Reply-To: <d4245344-bc04-f8b3-e1ea-fd1956172413@measurement-factory.com>
References: <000301d43289$14ed4770$3ec7d650$@yahoo.com.ar>
 <008001d4479b$b22b9290$1682b7b0$@yahoo.com.ar>
 <04c7931e-cb29-80d1-2a2a-8a1ccb4761f5@treenet.co.nz>
 <019301d4486b$049ebf00$0ddc3d00$@yahoo.com.ar>
 <6b762c34-f0c4-2d9a-6616-432305af160b@treenet.co.nz>
 <009d01d44935$1648ef80$42dace80$@yahoo.com.ar>
 <5400815d-9a82-97a9-5889-0e76c04a9ba0@measurement-factory.com>
 <003501d44aa4$d784e5d0$868eb170$@yahoo.com.ar>
 <500fbad5-d277-965f-d29a-a81c2f343d2e@measurement-factory.com>
 <004401d44b0e$46cf40c0$d46dc240$@yahoo.com.ar>
 <0472b838-415e-e4d2-c71a-7fbce45de926@measurement-factory.com>
 <008e01d44bbf$ccb41e20$661c5a60$@yahoo.com.ar>
 <2b2e8b70-7f45-039c-462f-ba78b2fc1563@measurement-factory.com>
 <006d01d44eaf$55ecc290$01c647b0$@yahoo.com.ar>
 <ee42d35f-2dc0-3bbc-f6c4-a1852374a832@measurement-factory.com>
 <003601d44f61$dfd30e80$9f792b80$@yahoo.com.ar>
 <8c393e26-822f-ddea-50d4-9f80ebc9087f@measurement-factor y.com>
 <002e01d451bc$ec104de0$c430e9a0$@yahoo.com.ar>
 <d4245344-bc04-f8b3-e1ea-fd1956172413@measure ment-factory.com>
Message-ID: <003b01d45292$fd11c4d0$f7354e70$@yahoo.com.ar>

> > # Second rule:
> > ssl_bump splice noBumpSites
> >
> > I think that this rule should implicity match only at step2.
> 
> I do not know what "implicitly match" means here, but yes, the splice rule
> may only match at step2 in this configuration:

When I say "implicit" I want to mean that there is no any step specified in the rule.

> * It cannot match at step1 because the earlier "peek" rule matches at step1.

Yes, rule #1 "matches all" therefore the domains into "noBumpSites" ACL are also peeked. And, that first rule will always match.
 
> * It is always reached during step2 because no rules above it can match
> during step2.

Yes, first rule has an explicit peek at step1, hence it is impossible any kind of match at step2 before the 2nd rule or in the first rule.

>Whether it matches during step2 depends on whether
> noBumpSites matches a particular transaction during step2.

If I understood You correctly, I think that here you are pointing to an earlier message where You explained some reasons why the "noBumpSites" could not always match.

> * It cannot match at step3 because for a splice rule to match at step3 a peek
> rule has to match at step2, and there is no peek rule that can match at step2
> in your configuration.

Althought there is no any peek rule at step2, in the second rule a final action is applied to noBumpSites (if match)
In fact, in case that (for any reason) the 2nd rule can not match, there is a explicit stare rule at step2.
So, I think that its almost impossible that splice at step3 happens in this configuration for the noBumpSites.
In the worst of the cases, if at rule #2 no match, then noBumpSites will be bumped, due to stare at step2.

Is this reasoning correct?

> > However as I said above if the splice is the first rule instead the
> > peek, the squid?s behaviour changes.
> 
> Naturally. If you place the splice rule first, it may match during step1 as well.
> If you do not, it cannot.

That was a comment to confirm that the wiki doc warn and said at 2017's is what is  happening now with Squid 4.2 (18/9 source).

> >> After a splice rule is applied, SslBump is over. No  more rules are
> >> checked. No more loops are iterated. Squid simply "exits" the
> >> SslBump feature (and becomes a TCP tunnel).
> 
> > Here, probably (not sure) Alex rerefered here to "splice all" rule.
> 
> I think you are ignoring or misinterpreting the verb "applied". Here, "applied"
> means Squid has executed the rule action. Not just considered the rule
> containing that action, but actually ran that action. Applying a rule action
> implies that the rule ACLs (whatever they were) matched, of course. A rule
> action is never applied when the rule ACLs do not match.

Yes, I misinterpreted You more than one time; I'm sorry. (Because You are speaking in english, and I am reading/speaking in an "almost-english" as well as I can)
So, In the final action the ACL is important. This is what I tried to mean.
I insist, because when You said that, I thought (without understanding the logic): "OK, therefore if I splice at step2 some.site.net, the following lines are over; no more processing no matter whatever any ACL the rule has"

> > In that case is clear "splice is a final action" then no more future checks.
> 
> The notion of a "final" action does not depend on rule ACLs.

Here is where I your explanation breaks my head. Here is the most important confusion of all of my own other confusions/misunderstanding.

In the config I posted, there is a splice action in the middle, and only the "noBumpSites" are spliced (at least checked with logs).
And even with the splice action as second rule, the 3rd rule is processed (Squid is still processing rules after splice noBumpSites ACL).
It is checked because if I remove the last rule all the traffic is spliced (due to peek at step1 and splice at step2) and future defaults actions.
I think that this happens because, if there is no 3rd line stare'ing at step2, so:
  
   ssl_bump splice noBumpSites = ssl_bump splice noBumpSites all. (not sure, I will do a test with only one rule, to see what Squid does: ssl_bump peek step1)

If this were the last rule, but in this configuration there is a 3rd rule which is stare'ing at step2)

> After Squid applies the "splice" action (in whatever context, for whatever reason),
> SslBump processing for that transaction is over. Same for "bump" and
> "terminate" actions.

What do You exactly mean with "for that transaction"? Maybe that rule?
 
> > But in my config next to splice there is an ACL. That is why I asked: "But,
> doesn't the ACL matters?" in earlier mail.
> 
> ACLs (and other things) determine which rules match. After a rule matches,
> then Squid applies its action, and then the notion of a "final action" starts to
> matter.

Thats statement clarify the thing a bit more.

> > Will Squid ignore the last rule?
> 
> No. The last rule will be applied at step2 whenever noBumpSites mismatches
> at step2.

Yes, I said and verified that, the last rule is not ignored by Squid, even with the splice rule at previuos rule.

> HTH,
> 
> Alex.

Thank You very much.



From fredrik at pipemore.se  Mon Sep 24 06:38:39 2018
From: fredrik at pipemore.se (uppsalanet)
Date: Mon, 24 Sep 2018 01:38:39 -0500 (CDT)
Subject: [squid-users] redirect based on url (302)
In-Reply-To: <dde485b6-5742-d424-8e12-f700d0b975f2@treenet.co.nz>
References: <1537541034108-0.post@n4.nabble.com>
 <dde485b6-5742-d424-8e12-f700d0b975f2@treenet.co.nz>
Message-ID: <1537771119646-0.post@n4.nabble.com>

Hi Amos,
Today I have a conf like this:
....
acl *LIB_domains* dstdomain .almedalsbiblioteket.se .alvin-portal.org
.bibliotekuppsala.se
http_access allow *LIB_domains*
....

Now I also need to open for *.browzine.com*. The problem with
*.browzine.com* is that it is a portal with many links to other sites. So I
basically need to open up and maintain 400 sites in a squid ACL.

I would like to take another approach then (but I don't know if it's
possible):
I know that browzine.com will reply 302 when trying to access a link on
their site. *So I would like to accept all redirect (302) sites from
browzine.com*. 

Hope that clarify and thanks in advance
Fredrik



--
Sent from: http://squid-web-proxy-cache.1019090.n4.nabble.com/Squid-Users-f1019091.html


From squid3 at treenet.co.nz  Mon Sep 24 09:30:39 2018
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Mon, 24 Sep 2018 21:30:39 +1200
Subject: [squid-users] redirect based on url (302)
In-Reply-To: <1537771119646-0.post@n4.nabble.com>
References: <1537541034108-0.post@n4.nabble.com>
 <dde485b6-5742-d424-8e12-f700d0b975f2@treenet.co.nz>
 <1537771119646-0.post@n4.nabble.com>
Message-ID: <f35964b6-e83b-c5b6-5563-602ab666d964@treenet.co.nz>

On 24/09/18 6:38 PM, uppsalanet wrote:
> Hi Amos,
> Today I have a conf like this:
> ....
> acl *LIB_domains* dstdomain .almedalsbiblioteket.se .alvin-portal.org
> .bibliotekuppsala.se
> http_access allow *LIB_domains*
> ....
> 
> Now I also need to open for *.browzine.com*. The problem with
> *.browzine.com* is that it is a portal with many links to other sites. So I
> basically need to open up and maintain 400 sites in a squid ACL.
> 
> I would like to take another approach then (but I don't know if it's
> possible):
> I know that browzine.com will reply 302 when trying to access a link on
> their site. *So I would like to accept all redirect (302) sites from
> browzine.com*. 

Aha, that is clearer. Thank you.

I think you can possibly achieve this, but *only* because of those 302
existing. If the site were just a collection of links it would be very
much more difficult.


What I am thinking of is to use a custom external ACL script that
creates a temporary browsing session for a client when the 302 arrives
then the SQL session helper to allow matching traffic through for the
followup request from that client.

You will need a database with a table created like this:

 CREATE TABLE sessions (
  id VARCHAR(256) NOT NULL PRIMARY KEY,
  enabled DATETIME NOT NULL DEFAULT CURRENT_TIMESTAMP
)

You need to write a script which receives an IP and a URL from Squid,
extracts the domain name from the URL, then adds a string "$ip $domain"
to that table as the id column, then returns the "OK" result to Squid.

The page at
<http://www.squid-cache.org/Versions/v4/manuals/ext_sql_session_acl.html> has
details of the SQL session helper that uses that table to check for
whitelisted domains.


Your config would look like:

 acl 302 http_status 302
 acl browzine dstdomain .browzine.com

 external_acl_type whitelist_add %SRC %{Location} \
  /path/to/whitelist_script

 acl add_to_whitelist external whitelist_add

 http_reply_access allow browzine 302 add_to_whitelist
 http_reply_access allow all


 external_acl_type whitelist ttl=60 %SRC %DST \
   /usr/lib/squid/ext_session_db_acl \
   --dsn ... --user ... --password ... \
   --table sessions --cond ""

 acl whitelisted external whitelist
 http_access allow whitelisted


To have sessions expire simply remove them from the database table.
Squid will start rejecting traffic there within 60 seconds of the removal.

HTH
Amos


From alex at dvm.esines.cu  Mon Sep 24 14:36:46 2018
From: alex at dvm.esines.cu (=?UTF-8?Q?Alex_Guti=c3=a9rrez_Mart=c3=adnez?=)
Date: Mon, 24 Sep 2018 10:36:46 -0400
Subject: [squid-users] transparent squid not working
Message-ID: <6c815117-29b7-181d-3247-126e6b85083f@dvm.esines.cu>

Hi community, im tyin to configure a squid transparent proxy, i obtain 
internet thanks to a parent proxy. I have 3 Ip declare on my iptables 
config file 172.16.1.245 is the administration ip, 172.16.1.246 is my 
parent proxy ip and 192.168.137.0/24 is the lan segment where my client 
are. This is my config:

 ?#squid.conf

acl http port 80
http_access allow http
acl https port 443
http_access allow https
never_direct allow all
cache_peer 172.16.1.246 parent 804
http_port 3128 transparent
cache_mem 64 MB
cache_dir aufs /var/cache/squid 1024 16 256
acl mired src 192.168.137.0/24
acl localhost src 127.0.0.1/32
http_access allow mired
http_access allow localhost
http_access deny all
acl deny_ipaccess url_regex [0-9]{1,3}\.[0-9]{1,3}\.[0-9]{1,3}\.[0-9]{1,3}
http_access deny deny_ipaccess


#iptables config

##!/bin/sh
echo n Aplicando Reglas de Firewall...
## FLUSH de reglas
iptables -F
iptables -X
iptables -Z
iptables -t nat -F
## Establecemos politica por defecto
iptables -P INPUT DROP
iptables -P OUTPUT ACCEPT
iptables -P FORWARD ACCEPT
## Empezamos a filtrar
# El localhost se deja (por ejemplo conexiones locales a mysql)
/sbin/iptables -A INPUT -i lo -j ACCEPT
iptables -A INPUT -s 172.16.1.245 -j ACCEPT
iptables -A INPUT -s 172.16.1.246 -j ACCEPT
iptables -A INPUT -s 192.168.137.0/24 -j ACCEPT
#nat
iptables -t nat -A PREROUTING -p tcp -s 192.168.137.0/24 --dport 80 -j 
REDIRECT --to-port 3128
iptables -t nat -A PREROUTING -p tcp -s 192.168.137.0/24 --dport 443 -j 
REDIRECT --to-port 3128
iptables -t nat -A POSTROUTING -s 192.168.137.0/24 -d 172.16.1.14 -o 
enp0s3 -j MASQUERADE
#iptables save
iptables-save > /etc/iptables/rules.v4
echo " OK . Verifique que lo que se aplica con: iptables -L -n"
# Fin del scrip


Can someone be so nice to explain to me wy this is not working.


Thanks in advance. Remember, always attack ideas, never people.

-- 
Saludos Cordiales

Lic. Alex Guti?rrez Mart?nez

Tel. +53 7 2710327





From donmuller22 at outlook.com  Mon Sep 24 15:46:00 2018
From: donmuller22 at outlook.com (Donald Muller)
Date: Mon, 24 Sep 2018 15:46:00 +0000
Subject: [squid-users] Help: squid restarts and squidGuard die
In-Reply-To: <CY1PR16MB04597442B5820B3B64E97A53B6120@CY1PR16MB0459.namprd16.prod.outlook.com>
References: <CA+d==oEt8J-ie2Fm1SYrHSBKD6Y7TRF-v3iGiMXC7WQ36WV1vw@mail.gmail.com>
 <f8f51c90-6bd7-09d0-a90b-c9552748d813@treenet.co.nz>
 <1537278855873-0.post@n4.nabble.com>
 <daf9e35b-5294-39aa-2116-ce36d663e330@treenet.co.nz>
 <CY1PR16MB045967CC3477DAE98EE094DFB61C0@CY1PR16MB0459.namprd16.prod.outlook.com>
 <20180920111612.GA26863@fantomas.sk>
 <CY1PR16MB0459DBC3A42A489E531F323DB6130@CY1PR16MB0459.namprd16.prod.outlook.com>
 <459c04a0-e09d-1c8f-fea2-12b650eb4f00@treenet.co.nz>
 <CY1PR16MB04597442B5820B3B64E97A53B6120@CY1PR16MB0459.namprd16.prod.outlook.com>
Message-ID: <CY1PR16MB045968E551F20C869629CE31B6170@CY1PR16MB0459.namprd16.prod.outlook.com>

I will be downloading the blacklists from the internet and I'm sure that there
will be sites that I want to whitelist via

acl whitelist dstdomain "/some folder path/whitelist.acl"
http_access allow whitelist

What logging do I need to enable to capture when a site I am trying to access is blacklisted so I can add it to the whitelist?

Thanks

> -----Original Message-----
> From: squid-users <squid-users-bounces at lists.squid-cache.org> On Behalf
> Of Donald Muller
> Sent: Friday, September 21, 2018 1:18 PM
> To: Amos Jeffries <squid3 at treenet.co.nz>; squid-users at lists.squid-
> cache.org
> Subject: Re: [squid-users] Help: squid restarts and squidGuard die
> 
> 
> 
> > -----Original Message-----
> > From: squid-users <squid-users-bounces at lists.squid-cache.org> On
> > Behalf Of Amos Jeffries
> > Sent: Thursday, September 20, 2018 3:50 PM
> > To: squid-users at lists.squid-cache.org
> > Subject: Re: [squid-users] Help: squid restarts and squidGuard die
> >
> > On 21/09/18 3:46 AM, Donald Muller wrote:
> > >
> > >> -----Original Message-----
> > >> From: Matus UHLAR - fantomas
> > >> Sent: Thursday, September 20, 2018 7:16 AM
> > >>
> > >> On 19.09.18 20:47, Donald Muller wrote:
> > >>> So instead of using squidguard are you saying  you should use
> > >>> something
> > >> like the following?
> > >>>
> > >>> acl ads dstdomain -i "/etc/squid/squid-ads.acl"
> > >>> acl adult dstdomain -i "/etc/squid/squid-adult.acl"
> > >>>
> > >>> http_access deny ads
> > >>> http_access deny adult
> > >>>
> > >>> Do the lists need to be sorted in alphabetical order?
> > >>
> > >> I don't think so - the lists are parsed to in -memory format for
> > >> faster processing.
> > >>
> > >
> > > Does Squid monitor dstdomain files for changes and reload them or
> > > does a
> > '-k reconfigure' need to be issued?
> > >
> >
> > Not currently. I'm looking for a nice portable way to do file watching.
> >
> > The Linux inotify system can apparently be used to send the -k
> > reconfigure command on FS changes in the config directory. Though I've
> > yet to see any working example and have not had the time myself to
> experiment on it.
> >
> > Patches and/or info welcome. This might be a good starter project if
> > anyone wants to dip their fingers into the Squid code.
> >
> 
> I will be downloading the blacklists from the internet and I'm sure that there
> will be sites that I want to whitelist via
> 
> acl whitelist dstdomain "/some folder path/whitelist.acl"
> http_access allow whitelist
> 
> What logging do I need to enable to capture when a domain is blacklisted?
> 
> 
> > Amos
> > _______________________________________________
> > squid-users mailing list
> > squid-users at lists.squid-cache.org
> > http://lists.squid-cache.org/listinfo/squid-users
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users

From marcus.kool at urlfilterdb.com  Mon Sep 24 19:07:56 2018
From: marcus.kool at urlfilterdb.com (Marcus Kool)
Date: Mon, 24 Sep 2018 16:07:56 -0300
Subject: [squid-users] Help: squid restarts and squidGuard die
In-Reply-To: <22f8fd29-c1c0-f764-2343-2e95600b7cdd@treenet.co.nz>
References: <CA+d==oEt8J-ie2Fm1SYrHSBKD6Y7TRF-v3iGiMXC7WQ36WV1vw@mail.gmail.com>
 <f8f51c90-6bd7-09d0-a90b-c9552748d813@treenet.co.nz>
 <1537278855873-0.post@n4.nabble.com>
 <daf9e35b-5294-39aa-2116-ce36d663e330@treenet.co.nz>
 <69fda549-1455-a336-9bd6-057828f67e0e@urlfilterdb.com>
 <bceeb9cf-128a-79e7-8fc4-e31e9c0e27c9@treenet.co.nz>
 <f926b42a-42eb-9497-8064-b1d87b82e7fb@urlfilterdb.com>
 <22f8fd29-c1c0-f764-2343-2e95600b7cdd@treenet.co.nz>
Message-ID: <ca0a95a4-5c2e-6282-5cf2-a0ea217ce78a@urlfilterdb.com>

The sub-thread starts with "do not use the url rewriter helper because of complexity"
and ends with that the (not less complex) external acl helpers are fine to use.
And in between there is an attempt to kill the URL rewriter interface.

It would be a lot less confusing if you started with something like
    I do not like the URL rewriter interface, use the external acl one

 >> ufdbGuard supports dynamic lists of users, domains and source ip
 >> addresses which are updated every X minutes without any service
 >> interruption.
 >
 > So does Squid, via external ACL and/or authentication.

Aren't you confusing what Squid itself and what Squid+helpers can do?

Marcus


From marcio.merlone at a1.ind.br  Mon Sep 24 19:48:58 2018
From: marcio.merlone at a1.ind.br (Marcio Vogel Merlone dos Santos)
Date: Mon, 24 Sep 2018 16:48:58 -0300
Subject: [squid-users] external_acl_type LDAP for acl NOT related to auth
Message-ID: <89f2600a-ffb2-69ad-9ad2-ab1f76a686f1@a1.ind.br>

Hi,

Searched google but could not find anyone trying this. I want to use 
some LDAP data to create an ACL not related to authentication or 
users/groups. I want to create an ACL like this pseudo conf:

> external_acl_type myServers ttl=300 ipv4 %SRC 
> /usr/lib/squid/ldap_lookup_acl -p
>
> http_access allow myServers
>
Where 'myServers' contains a list of machines that don't need to auth 
the service, for example. Other uses would be to define a network list, 
mac lists, people with youtube access, and so on. Is it possible, has 
someone already made it, can anyone point me the direction?

Thanks, best regards.

-- 
*Marcio Merlone*
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20180924/937f7a53/attachment.htm>

From squid3 at treenet.co.nz  Mon Sep 24 23:08:36 2018
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Tue, 25 Sep 2018 11:08:36 +1200
Subject: [squid-users] external_acl_type LDAP for acl NOT related to auth
In-Reply-To: <89f2600a-ffb2-69ad-9ad2-ab1f76a686f1@a1.ind.br>
References: <89f2600a-ffb2-69ad-9ad2-ab1f76a686f1@a1.ind.br>
Message-ID: <593fa168-81b4-acb2-e14a-cc52d902f499@treenet.co.nz>

On 25/09/18 7:48 AM, Marcio Vogel Merlone dos Santos wrote:
> Hi,
> 
> Searched google but could not find anyone trying this. I want to use
> some LDAP data to create an ACL not related to authentication or
> users/groups. I want to create an ACL like this pseudo conf:
> 
>> external_acl_type myServers ttl=300 ipv4 %SRC
>> /usr/lib/squid/ldap_lookup_acl -p
>>
>> http_access allow myServers
>>
> Where 'myServers' contains a list of machines that don't need to auth
> the service, for example. Other uses would be to define a network list,
> mac lists, people with youtube access, and so on. Is it possible, has
> someone already made it, can anyone point me the direction?


Look at the example configuration for the eDirectory user-IP helper it
uses src-IP as "username" being looked up.
 <http://www.squid-cache.org/Versions/v3/3.5/manuals/ext_edirectory_userip_acl.html>

Wrapping is currently broken in that doc, I've fixed that below for clarity:

  external_acl_type IPUser %SRC /usr/sbin/ext_edirectory_userip_acl

  acl edirectory_users_allowed external IPUser \
    cn=Internet_Allowed,ou=ORG,o=BASE

  acl edirectory_users_denied external IPUser \
    cn=Internet_Denied,ou=ORG,o=BASE

  http_access deny edirectory_users_denied
  http_access allow edirectory_users_allowed
  http_access deny all


The above config passes the LDAP path details as %DATA, so the helper
gets told to always use the src-IP as the "username" and each acl line
tells it which LDAP path/directory to check for that particular ACL
test. Allowing multiple tables for different whitelist or blacklist
checks by the same helper.


You may be able to use the above helper as-is, or use the existing AD
LDAP group helpers with %SRC in a similar way. Though the older AD
helpers probably need to use %g macro in the -f filter to specify where
the %DATA portion is to go.

Disclaimer: I have not tried this myself, so YMMV.

Amos


From squid3 at treenet.co.nz  Mon Sep 24 23:16:40 2018
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Tue, 25 Sep 2018 11:16:40 +1200
Subject: [squid-users] Help: squid restarts and squidGuard die
In-Reply-To: <CY1PR16MB045968E551F20C869629CE31B6170@CY1PR16MB0459.namprd16.prod.outlook.com>
References: <CA+d==oEt8J-ie2Fm1SYrHSBKD6Y7TRF-v3iGiMXC7WQ36WV1vw@mail.gmail.com>
 <f8f51c90-6bd7-09d0-a90b-c9552748d813@treenet.co.nz>
 <1537278855873-0.post@n4.nabble.com>
 <daf9e35b-5294-39aa-2116-ce36d663e330@treenet.co.nz>
 <CY1PR16MB045967CC3477DAE98EE094DFB61C0@CY1PR16MB0459.namprd16.prod.outlook.com>
 <20180920111612.GA26863@fantomas.sk>
 <CY1PR16MB0459DBC3A42A489E531F323DB6130@CY1PR16MB0459.namprd16.prod.outlook.com>
 <459c04a0-e09d-1c8f-fea2-12b650eb4f00@treenet.co.nz>
 <CY1PR16MB04597442B5820B3B64E97A53B6120@CY1PR16MB0459.namprd16.prod.outlook.com>
 <CY1PR16MB045968E551F20C869629CE31B6170@CY1PR16MB0459.namprd16.prod.outlook.com>
Message-ID: <646c1349-53e4-6033-10e4-8438060449f6@treenet.co.nz>

On 25/09/18 3:46 AM, Donald Muller wrote:
> I will be downloading the blacklists from the internet and I'm sure that there
> will be sites that I want to whitelist via
> 
> acl whitelist dstdomain "/some folder path/whitelist.acl"
> http_access allow whitelist
> 
> What logging do I need to enable to capture when a site I am trying to access is blacklisted so I can add it to the whitelist?
> 

When your access.log contains DENIED/403 as the transaction status and
no server details it was denied by your policy.

NP: If all you are doing is adding blocked sites to a whitelist, then
its pointless doing the block at all. The best solution there is to
remove the blacklist entirely.

Amos


From squid3 at treenet.co.nz  Tue Sep 25 00:48:02 2018
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Tue, 25 Sep 2018 12:48:02 +1200
Subject: [squid-users] Help: squid restarts and squidGuard die
In-Reply-To: <ca0a95a4-5c2e-6282-5cf2-a0ea217ce78a@urlfilterdb.com>
References: <CA+d==oEt8J-ie2Fm1SYrHSBKD6Y7TRF-v3iGiMXC7WQ36WV1vw@mail.gmail.com>
 <f8f51c90-6bd7-09d0-a90b-c9552748d813@treenet.co.nz>
 <1537278855873-0.post@n4.nabble.com>
 <daf9e35b-5294-39aa-2116-ce36d663e330@treenet.co.nz>
 <69fda549-1455-a336-9bd6-057828f67e0e@urlfilterdb.com>
 <bceeb9cf-128a-79e7-8fc4-e31e9c0e27c9@treenet.co.nz>
 <f926b42a-42eb-9497-8064-b1d87b82e7fb@urlfilterdb.com>
 <22f8fd29-c1c0-f764-2343-2e95600b7cdd@treenet.co.nz>
 <ca0a95a4-5c2e-6282-5cf2-a0ea217ce78a@urlfilterdb.com>
Message-ID: <5dea5687-93e5-da6f-73d8-2e8b78578325@treenet.co.nz>

On 25/09/18 7:07 AM, Marcus Kool wrote:
> The sub-thread starts with "do not use the url rewriter helper because
> of complexity"

The thread started earlier than that, with essentially "move simple
rules to squid.conf"

On 18/09/18 6:38 AM, Amos Jeffries wrote:
>
> I recommend you convert as many of your filtering rules as you can into
> normal Squid ACLs. Traffic which is being blocked for simple reasons can
> be done much more efficiently by Squid than a helper.
>

The statement about the helper being complex came later after a
misunderstanding by the OP about what the tools were used for.

You are paraphrasing in a way which changes the meaning of my actual
statement. I was clearly and explicitly advising the OP to work towards
"less complexity" and pointing out that the helper (any helper) is
complex and to be avoided when a simpler solution is also available.


> and ends with that the (not less complex) external acl helpers are fine
> to use.

They are ... when needed. Having them do everything from src-IP check to
re-authenticating a login Squid already authenticated passed it is
needless extra complexity as a long-term solution.


> And in between there is an attempt to kill the URL rewriter interface.
> 

No, just the use of the rewriters for access control. In the context of
an OP who is using a rewriter for a fairly simple set of blacklist and
whitelist of traffic - which got diverted into a debate of Squid vs
re-writer feature comparisons.

You brought up the topic of removing the interface. As I responded then,
there are still use-cases for it. Just, access control is not one of
those cases.


> It would be a lot less confusing if you started with something like
> ?? I do not like the URL rewriter interface, use the external acl one
> 

That would be only a small amount better (improvement in principle, no
longer destructive for the state lost when re-writing - still complex in
practice). I am pointing the OP at something that should work a bit
better than even that semi-theoretical improvement. They may or may not
end up with a helper still being used, but either way re-assessing this
1980's style config will improve their situation for modern traffic.


>>> ufdbGuard supports dynamic lists of users, domains and source ip
>>> addresses which are updated every X minutes without any service
>>> interruption.
>>
>> So does Squid, via external ACL and/or authentication.
> 
> Aren't you confusing what Squid itself and what Squid+helpers can do?

There is crossover. Though we are delving into realms of principle here.
The data available to the helper running on the URL-rewrite interface is
quite limited - the other interfaces (external ACL in particular) have
wider scope and much more flexibility in what Squid can do with them.

For example SG and ufdbguard may be able to load dynamic lists of users,
but cannot make Squid generate authentication challenge with the correct
parameters to authenticate those users. They can only re-check an
already authenticated username (without access to the password details)
or rewrite/redirect to a third-party server that does so.
 Whereas looking up users in some "dynamic list" without needing a
reconfigure of Squid is pretty much the essence of what auth user/group
helpers do. It is rare to find a never-changing list of users.

Amos


From ygirardin at olfeo.com  Tue Sep 25 10:39:19 2018
From: ygirardin at olfeo.com (Yann Girardin)
Date: Tue, 25 Sep 2018 12:39:19 +0200
Subject: [squid-users] squid interception
Message-ID: <DEE553E0920CFA41BAF85A4B9C6FC3498F27D1054C@EXCHANGESRV1.olfeo-lab.net>

Hello !



We have encountered what we think is a strange behavior of Squid when in interception. We know that it's not a bug but made on purpose, but we question ourself on the why of this choice.



We have a firewall that we have configured to redirect all TCP packets with the destination port set to 80 to the squid box. This redirection is made by changing the destination IP to the address of the Squid box and destination port to 8080. On the box, we set up Squid to listen to port 9090 in interception mode. Moreover, we use some DNAT rules to redirect the traffic from port 8080 to port 9090. Yes, we know that we shouldn't do that, but "we" includes some third parties.



This does not work because Squid takes the destination IP address and try to connect to it. In our case, it tries to connect to itself port 8080 to join the destination HTTP host, where nothing is listening, rejecting the connection and aborting the transaction. There is no process listening on port 8080 on the Squid box.



In detail :

- the box receives the TCP packets and translates traffic from port 8080 to port 9090 in either directions (it works well).

- Squid receives an HTTP request

- Squid does a DNS request on the domain included in the HTTP request (the destination HTTP host)

- Squid tries to connect to the destination IP address of the first packet, in our case the address of the box, instead of the IP got from the DNS request (this is the unexpected behavior)

- The kernel rejects the connection (as expected)

- Squid answers back a connection failure.



My questions are :

- why Squid uses the destination IP instead of the IP of the requested domain included in the HTTP request ?

- why Squid performs a DNS request if it doesn't plan to use its result ?

- Is there a way to configure Squid to use the IP of the domain included in the request instead of the original IP address (in interception mode) ?

- Is there a way to enable a Squid port to react as in interception mode, but without looking for the original IP address ?

- Does translating addresses and ports are inherently a bad practice to avoid when doing interception, or is it just a limitation of Squid ?



Thanks for all your answers :-)

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20180925/3ff55f15/attachment.htm>

From uhlar at fantomas.sk  Tue Sep 25 11:12:27 2018
From: uhlar at fantomas.sk (Matus UHLAR - fantomas)
Date: Tue, 25 Sep 2018 13:12:27 +0200
Subject: [squid-users] squid interception
In-Reply-To: <DEE553E0920CFA41BAF85A4B9C6FC3498F27D1054C@EXCHANGESRV1.olfeo-lab.net>
References: <DEE553E0920CFA41BAF85A4B9C6FC3498F27D1054C@EXCHANGESRV1.olfeo-lab.net>
Message-ID: <20180925111227.GA10306@fantomas.sk>

On 25.09.18 12:39, Yann Girardin wrote:
> We have encountered what we think is a strange behavior of Squid when in
> interception.  We know that it's not a bug but made on purpose, but we
> question ourself on the why of this choice.
>
> We have a firewall that we have configured to redirect all TCP packets
> with the destination port set to 80 to the squid box.  This redirection is
> made by changing the destination IP to the address of the Squid box and

this is wrong way to do interception and it opens door to a security
vulnerability.

squid needs to know the destination IP, otherwise it does not know where it
has to connect.

The Host: header is NOT a reliable info, because it can contain false
information. see the vulnerability info:

https://nvd.nist.gov/vuln/detail/CVE-2009-0801
https://www.kb.cert.org/vuls/id/435052

> destination port to 8080.  On the box, we set up Squid to listen to port
> 9090 in interception mode.  Moreover, we use some DNAT rules to redirect
> the traffic from port 8080 to port 9090.  Yes, we know that we shouldn't
> do that, but "we" includes some third parties.

the proper way to do interception is to forward packets do squid host
without changing the destination I

https://wiki.squid-cache.org/SquidFaq/InterceptionProxy

-- 
Matus UHLAR - fantomas, uhlar at fantomas.sk ; http://www.fantomas.sk/
Warning: I wish NOT to receive e-mail advertising to this address.
Varovanie: na tuto adresu chcem NEDOSTAVAT akukolvek reklamnu postu.
Saving Private Ryan...
Private Ryan exists. Overwrite? (Y/N)


From rousskov at measurement-factory.com  Tue Sep 25 21:40:31 2018
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Tue, 25 Sep 2018 15:40:31 -0600
Subject: [squid-users] About SSL peek-n-splice/bump configurations
In-Reply-To: <003b01d45292$fd11c4d0$f7354e70$@yahoo.com.ar>
References: <000301d43289$14ed4770$3ec7d650$@yahoo.com.ar>
 <019301d4486b$049ebf00$0ddc3d00$@yahoo.com.ar>
 <6b762c34-f0c4-2d9a-6616-432305af160b@treenet.co.nz>
 <009d01d44935$1648ef80$42dace80$@yahoo.com.ar>
 <5400815d-9a82-97a9-5889-0e76c04a9ba0@measurement-factory.com>
 <003501d44aa4$d784e5d0$868eb170$@yahoo.com.ar>
 <500fbad5-d277-965f-d29a-a81c2f343d2e@measurement-factory.com>
 <004401d44b0e$46cf40c0$d46dc240$@yahoo.com.ar>
 <0472b838-415e-e4d2-c71a-7fbce45de926@measurement-factory.com>
 <008e01d44bbf$ccb41e20$661c5a60$@yahoo.com.ar>
 <2b2e8b70-7f45-039c-462f-ba78b2fc1563@measurement-factory.com>
 <006d01d44eaf$55ecc290$01c647b0$@yahoo.com.ar>
 <ee42d35f-2dc0-3bbc-f6c4-a1852374a832@measurement-factory.com>
 <003601d44f61$dfd30e80$9f792b80$@yahoo.com.ar>
 <8c393e26-822f-ddea-50d4-9f80ebc9087f@measurement-factor y.com>
 <002e01d451bc$ec104de0$c430e9a0$@yahoo.com.ar>
 <d4245344-bc04-f8b3-e1ea-fd1956172413@measure ment-factory.com>
 <003b01d45292$fd11c4d0$f7354e70$@yahoo.com.ar>
Message-ID: <80b536df-407d-76b4-bc35-2fdc1524b064@measurement-factory.com>

On 09/22/2018 10:40 AM, Julian Perconti wrote:
>>> # Second rule:
>>> ssl_bump splice noBumpSites
>>>
>>> I think that this rule should implicity match only at step2.
>>
>> I do not know what "implicitly match" means here, but yes, the splice rule
>> may only match at step2 in this configuration:

> When I say "implicit" I want to mean that there is no any step specified in the rule.

Understood. Please avoid that word usage. In this context, implicit
means "without being configured" or "by default". One could say that
"default rules implicitly match", or that "a rule without any ACLs
matches implicitly", but one cannot say that "rule X containing ACL Y
implicitly matched".


>> * It cannot match at step3 because for a splice rule to match at step3 a peek
>> rule has to match at step2, and there is no peek rule that can match at step2
>> in your configuration.

> Althought there is no any peek rule at step2, in the second rule a
> final action is applied to noBumpSites (if match)

Final actions at step2 do not matter when we are talking about what
happens at step3. If a final action is applied at step2, there is no
step3 as far as an ssl_bump configuration is concerned.


> In fact, in case that (for any reason) the 2nd rule can not match,
> there is a explicit stare rule at step2.

Yes, and it will be applied, and it is not a peek rule, so this applied
staring action will prevent splicing at step3.


> So, I think that its almost impossible that splice at step3 happens
> in this configuration for the noBumpSites.

It is impossible for any transaction to be spliced at step3 with this
configuration. Whether the transaction matches or does not match
noBumpSites at any given step is irrelevant for this statement.


> In the worst of the cases, if at rule #2 no match, then noBumpSites
> will be bumped, due to stare at step2.

Correct. There is nothing "worse" about this case though.


>>> In that case is clear "splice is a final action" then no more future checks.

>> The notion of a "final" action does not depend on rule ACLs.

> Here is where I your explanation breaks my head. Here is the most
> important confusion of all of my own other
> confusions/misunderstanding.

Final actions are "bump", "terminate", and "splice". As you can easily
see, this statement does not depend on ACLs.

An action is either final or not, by that action nature/definition. ACLs
are one of the precondition for applying an action, but ACLs do not
affect action "finality".


> In the config I posted, there is a splice action in the middle, and
> only the "noBumpSites" are spliced

Yes.


> And even with the splice action as second rule, the 3rd rule is
> processed (Squid is still processing rules after splice noBumpSites
> ACL).

An action presence in the rules does not, on its own, stop Squid from
processing lower rules. *Applying* a final action does.


> It is checked

Yes, it is checked at stepN when the previous rules do not match at (or
are not applicable to) that step.


>    ssl_bump splice noBumpSites = ssl_bump splice noBumpSites all.

Yes. Adding "and true" to any logical condition does not change that
condition. However, I fear that the above correct equivalence does not
mean what you think it means. It means there is no point in adding
"all". It does not mean that adding "all" changes the meaning of that
ssl_bump rule or of what you are saying.


>> After Squid applies the "splice" action (in whatever context, for whatever reason),
>> SslBump processing for that transaction is over. Same for "bump" and
>> "terminate" actions.

> What do You exactly mean with "for that transaction"? Maybe that rule?

No, I do not mean "that rule". In this context, "transaction" is,
roughly speaking, an "HTTP CONNECT request" or "TLS connection". An
applied final action stops all ssl_bump processing for the corresponding
transaction/request/connection, and not just one ssl_bump rule
processing. That difference is why those actions are called "final".

Alex.


From vh1988 at yahoo.com.ar  Wed Sep 26 17:40:33 2018
From: vh1988 at yahoo.com.ar (Julian Perconti)
Date: Wed, 26 Sep 2018 14:40:33 -0300
Subject: [squid-users] About SSL peek-n-splice/bump configurations
In-Reply-To: <80b536df-407d-76b4-bc35-2fdc1524b064@measurement-factory.com>
References: <000301d43289$14ed4770$3ec7d650$@yahoo.com.ar>
 <019301d4486b$049ebf00$0ddc3d00$@yahoo.com.ar>
 <6b762c34-f0c4-2d9a-6616-432305af160b@treenet.co.nz>
 <009d01d44935$1648ef80$42dace80$@yahoo.com.ar>
 <5400815d-9a82-97a9-5889-0e76c04a9ba0@measurement-factory.com>
 <003501d44aa4$d784e5d0$868eb170$@yahoo.com.ar>
 <500fbad5-d277-965f-d29a-a81c2f343d2e@measurement-factory.com>
 <004401d44b0e$46cf40c0$d46dc240$@yahoo.com.ar>
 <0472b838-415e-e4d2-c71a-7fbce45de926@measurement-factory.com>
 <008e01d44bbf$ccb41e20$661c5a60$@yahoo.com.ar>
 <2b2e8b70-7f45-039c-462f-ba78b2fc1563@measurement-factory.com>
 <006d01d44eaf$55ecc290$01c647b0$@yahoo.com.ar>
 <ee42d35f-2dc0-3bbc-f6c4-a1852374a832@measurement-factory.com>
 <003601d44f61$dfd30e80$9f792b80$@yahoo.com.ar>
 <8c393e26-822f-ddea-50d4-9f80ebc9087f@measurement-factor y.com>
 <002e01d451bc$ec104de0$c430e9a0$@yahoo.com.ar>
 <d4245344-bc04-f8b3-e1ea-fd1956172413@measure ment-factory.com>
 <003b01d45292$fd11c4d0$f7354e70$@yahoo.com.ar>
 <80b536df-407d-76b4-bc35-2fdc1524b 064@measurement-factory.com>
Message-ID: <004701d455c0$0970dba0$1c5292e0$@yahoo.com.ar>

> > When I say "implicit" I want to mean that there is no any step specified in
> the rule.
> 
> Understood. Please avoid that word usage. In this context, implicit means
> "without being configured" or "by default". One could say that "default rules
> implicitly match", or that "a rule without any ACLs matches implicitly", but
> one cannot say that "rule X containing ACL Y implicitly matched".

OK and sorry for that.

> > Althought there is no any peek rule at step2, in the second rule a
> > final action is applied to noBumpSites (if match)
> 
> Final actions at step2 do not matter when we are talking about what happens
> at step3. If a final action is applied at step2, there is no
> step3 as far as an ssl_bump configuration is concerned.

Yes, when a final action is applied at step2, ssl_bump rules are over when the previous step (if it's a final action) has matched.

> It is impossible for any transaction to be spliced at step3 with this
> configuration. Whether the transaction matches or does not match
> noBumpSites at any given step is irrelevant for this statement.

OK: In this configuration it is impossible any kind of splice at step3; but not for step2. 
In fact, noBumpSites are being spliced (at least I can see the TCP_TUNNEL in logs).

> Correct. There is nothing "worse" about this case though.

With the  term "worst" I wanted to mean that my intention is splice sites into the ACL (noBumpSites) , not bump.

> > Here is where I your explanation breaks my head. Here is the most
> > important confusion of all of my own other
> > confusions/misunderstanding.
> 
> Final actions are "bump", "terminate", and "splice". As you can easily see, this
> statement does not depend on ACLs.
> 
> An action is either final or not, by that action nature/definition. ACLs are one
> of the precondition for applying an action, but ACLs do not affect action
> "finality".

Well, Yes.
Strictly speaking final actions (and maybe any action) do not depend on the acl, let's say it is a natural function/behavior of Squid beyond any acl.
However, when a final action is present in a rule and that rule contains an ACL, the final action will apply to that ACL. At least that is the behaviour I see.
If not, Squid would not being splice'ing "noBumpSites" which is an ACL; as he is doing right now.
I say good?

> > And even with the splice action as second rule, the 3rd rule is
> > processed (Squid is still processing rules after splice noBumpSites
> > ACL).
> 
> An action presence in the rules does not, on its own, stop Squid from
> processing lower rules. *Applying* a final action does.

So, why squid process the last rule which stare at step 2? He already applied the splice to the ACL sites.

> >> After Squid applies the "splice" action (in whatever context, for
> >> whatever reason), SslBump processing for that transaction is over.
> >> Same for "bump" and "terminate" actions.
> 
> > What do You exactly mean with "for that transaction"? Maybe that rule?
> 
> No, I do not mean "that rule". In this context, "transaction" is, roughly
> speaking, an "HTTP CONNECT request" or "TLS connection". An applied final
> action stops all ssl_bump processing for the corresponding
> transaction/request/connection, and not just one ssl_bump rule processing.
> That difference is why those actions are called "final".

OK, thank You for that clarification of misinterpreted terms.

So going back to current config:

  ssl_bump peek step1
  ssl_bump splice noBumpSites # I think that here the splice action is applied at step2. Even if there is no step specified. And due to previous rule.
  ssl_bump stare step2

Due to I think that: the splice action happens at step2 (more checks?), and not at step 1 (less checks); This is the config the one of best fit to my necessities.

Quick reminder of the idea/need:
In the most possible "secure" way: bump to all except banks and other sensitive sites; and less possible interference to that sensitive sites.

Just a comment: Squid is working fine,but still the cache.log shows these kind or errors (quite annoying): 

kid1| Error negotiating SSL connection on FD 26: error:00000001:lib(0):func(0):reason(1) (1/-1)
kid1| ERROR: negotiating TLS on FD 31: error:00000000:lib(0):func(0):reason(0) (5/-1/104)

Always or almost always are only those two types of error [reason(1) (1/-1) - (0) (5/-1/104)]

But that is another story. Apart nobody reports problems about the browsing.

> Alex.

Thank You, again.



From rousskov at measurement-factory.com  Wed Sep 26 19:41:58 2018
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Wed, 26 Sep 2018 13:41:58 -0600
Subject: [squid-users] About SSL peek-n-splice/bump configurations
In-Reply-To: <004701d455c0$0970dba0$1c5292e0$@yahoo.com.ar>
References: <000301d43289$14ed4770$3ec7d650$@yahoo.com.ar>
 <009d01d44935$1648ef80$42dace80$@yahoo.com.ar>
 <5400815d-9a82-97a9-5889-0e76c04a9ba0@measurement-factory.com>
 <003501d44aa4$d784e5d0$868eb170$@yahoo.com.ar>
 <500fbad5-d277-965f-d29a-a81c2f343d2e@measurement-factory.com>
 <004401d44b0e$46cf40c0$d46dc240$@yahoo.com.ar>
 <0472b838-415e-e4d2-c71a-7fbce45de926@measurement-factory.com>
 <008e01d44bbf$ccb41e20$661c5a60$@yahoo.com.ar>
 <2b2e8b70-7f45-039c-462f-ba78b2fc1563@measurement-factory.com>
 <006d01d44eaf$55ecc290$01c647b0$@yahoo.com.ar>
 <ee42d35f-2dc0-3bbc-f6c4-a1852374a832@measurement-factory.com>
 <003601d44f61$dfd30e80$9f792b80$@yahoo.com.ar>
 <8c393e26-822f-ddea-50d4-9f80ebc9087f@measurement-factor y.com>
 <002e01d451bc$ec104de0$c430e9a0$@yahoo.com.ar>
 <d4245344-bc04-f8b3-e1ea-fd1956172413@measure ment-factory.com>
 <003b01d45292$fd11c4d0$f7354e70$@yahoo.com.ar>
 <80b536df-407d-76b4-bc35-2fdc1524b 064@measurement-factory.com>
 <004701d455c0$0970dba0$1c5292e0$@yahoo.com.ar>
Message-ID: <17a5afb9-428e-da8c-b0d2-0bdb1af86b54@measurement-factory.com>

On 09/26/2018 11:40 AM, Julian Perconti wrote:

>> It is impossible for any transaction to be spliced at step3 with this
>> configuration. Whether the transaction matches or does not match
>> noBumpSites at any given step is irrelevant for this statement.
> 
> OK: In this configuration it is impossible any kind of splice at step3; but not for step2.

Yes, your configuration makes splicing possible at step2 (and only at
step2).




> Strictly speaking final actions (and maybe any action) do not depend
> on the acl, let's say it is a natural function/behavior of Squid
> beyond any acl.


Correct.


> However, when a final action is present in a rule and that rule
> contains an ACL, the final action will apply to that ACL.

"apply to ACL" does not make sense.

ACLs of a [final] action rule affect when the final action is applied.
They are a necessary (but not sufficient) preconditions for applying the
action.



>> An action presence in the rules does not, on its own, stop Squid from
>> processing lower rules. *Applying* a final action does.

> So, why squid process the last rule which stare at step 2? He already
> applied the splice to the ACL sites.

For your configuration:

* If Squid applied the splice rule, then it will ignore the stare rule.

* If Squid reached but did _not_ apply the splice rule, then it will
apply the stare rule instead.

FWIW, I do not understand why you do not seem to understand this fairly
straightforward algorithm so I cannot explain it better. I can correct
your statements, but I do not know _why_ you keep making statements that
need correction. We are running in circles. It could be just a language
barrier.


> So going back to current config:
> 
>   ssl_bump peek step1
>   ssl_bump splice noBumpSites
>   ssl_bump stare step2

> Due to I think that: the splice action happens at step2 (more
> checks?), and not at step 1 (less checks); 

Correct.


> This is the config the one of best fit to my necessities.

Glad you found what you were looking for.

This is minor, but replacing "step2" in the last/stare rule with "all"
would be slightly better because "all" is simpler and should be faster
to compute than "step2". This minor simplification/optimization will not
change the overall meaning of the configuration.

I added a similar configuration example to Squid wiki at
https://wiki.squid-cache.org/Features/SslPeekAndSplice


HTH,

Alex.


From Ralf.Hildebrandt at charite.de  Thu Sep 27 08:43:20 2018
From: Ralf.Hildebrandt at charite.de (Ralf Hildebrandt)
Date: Thu, 27 Sep 2018 10:43:20 +0200
Subject: [squid-users] Very basic peek & splice
Message-ID: <20180927084320.GA9293@charite.de>

I recompiled my squid-5 with openssl and added

ssl_bump peek all
ssl_bump splice all

to my squid.conf. What logging should I expect to verify it's actually
working?

-- 
Ralf Hildebrandt                   Charite Universit?tsmedizin Berlin
ralf.hildebrandt at charite.de        Campus Benjamin Franklin
https://www.charite.de             Hindenburgdamm 30, 12203 Berlin
Gesch?ftsbereich IT, Abt. Netzwerk fon: +49-30-450.570.155


From squid3 at treenet.co.nz  Thu Sep 27 09:45:45 2018
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 27 Sep 2018 21:45:45 +1200
Subject: [squid-users] Very basic peek & splice
In-Reply-To: <20180927084320.GA9293@charite.de>
References: <20180927084320.GA9293@charite.de>
Message-ID: <d1e18374-f974-9a4a-3200-c07798fca679@treenet.co.nz>

On 27/09/18 8:43 PM, Ralf Hildebrandt wrote:
> I recompiled my squid-5 with openssl and added
> 
> ssl_bump peek all
> ssl_bump splice all
> 
> to my squid.conf. What logging should I expect to verify it's actually
> working?
> 

Depends on what you mean by 'working'.

Splicing will show up as access.log CONNECT messages to raw-IP on port
443 with 0ms duration and probably TCP_NONE status. Followed by CONNECT
from same client IP with either raw-IP or a domain, TCP_TUNNEL status
and non-0 duration.
 These pairs may only be identifiable by using the duration to find that
they started at identical time from the same client. The log entries
will be separated by that duration.


'working' can also mean detecting TLS errors and rejecting them. Which
shows up as https:// requests being bumped and denied with a 5xx error
status.


Amos


From christof.gerber1 at gmail.com  Thu Sep 27 15:56:50 2018
From: christof.gerber1 at gmail.com (Christof Gerber)
Date: Thu, 27 Sep 2018 17:56:50 +0200
Subject: [squid-users] Fetch missing certificate feature of Squid_v4
Message-ID: <CAFyThpJh76KuqwDdbF8obdSJyFVSSt0O2Q-raoM=-wRgo6_8Kw@mail.gmail.com>

Concerning the new feature which fetches the missing intermediate
certificates I have three questions about its implementation and
implications:

1. What happens if the certificate fetch requests runs into a timeout?
Is this prevented somehow?

2. Does Squid also learn intermediate certificates from complete
certificate chains of other requests?

3. Will this feature make it necessary to increase the cache size?

-- 
Christof Gerber
Email: christof.gerber1 at gmail.com


From rousskov at measurement-factory.com  Thu Sep 27 16:07:34 2018
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Thu, 27 Sep 2018 10:07:34 -0600
Subject: [squid-users] Very basic peek & splice
In-Reply-To: <20180927084320.GA9293@charite.de>
References: <20180927084320.GA9293@charite.de>
Message-ID: <17d0b93b-7be7-969c-a59f-8cb4744e6f59@measurement-factory.com>

On 09/27/2018 02:43 AM, Ralf Hildebrandt wrote:
> I recompiled my squid-5 with openssl and added
> 
> ssl_bump peek all
> ssl_bump splice all
> 
> to my squid.conf. What logging should I expect to verify it's actually
> working?

Logging %ssl:bump_mode may be a good idea.

For a particular _spliced_ transaction, logging the server-provided
certificate details (e.g., %ssl::<cert_subject) would confirm that Squid
peeked at the certificate before splicing.

Besides %ssl:bump_mode, reliably distinguishing spliced connections from
bumped connections is difficult AFAICT because Squid does not have a
%code for Squid-sent server certificate details.

Please note that a successful splice using your configuration should
result in two CONNECT access.log entries. I am focusing on the second
one. See Amos response for more details regarding these two entries.


FWIW, I recommend using a few test cases to double check that your
verification method (whatever it is) works well for step3 splicing:

1. Successful splice with a trusted TLS server.
2. Failed splice with an untrusted TLS server.
3. Failed splice with a non-TLS (e.g., an HTTP) server.
4. Failed splice with a TLS server rejecting your TLS client.
5. Failed splice with a down server.
6. Failed splice with a server having an unresolvable DNS name.
...


HTH,

Alex.


From rousskov at measurement-factory.com  Thu Sep 27 16:32:34 2018
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Thu, 27 Sep 2018 10:32:34 -0600
Subject: [squid-users] Fetch missing certificate feature of Squid_v4
In-Reply-To: <CAFyThpJh76KuqwDdbF8obdSJyFVSSt0O2Q-raoM=-wRgo6_8Kw@mail.gmail.com>
References: <CAFyThpJh76KuqwDdbF8obdSJyFVSSt0O2Q-raoM=-wRgo6_8Kw@mail.gmail.com>
Message-ID: <b2292743-03b5-c7dd-d1a2-382274f5f251@measurement-factory.com>

On 09/27/2018 09:56 AM, Christof Gerber wrote:
> Concerning the new feature which fetches the missing intermediate
> certificates I have three questions about its implementation and
> implications:

> 1. What happens if the certificate fetch requests runs into a timeout?

If Squid lacks a certificate required to validate the server, the server
validation will fail. What happens after that probably depends on your
configuration, but bumping the client connection to report the
validation error is typical for SslBump-driven deployments.


> Is this prevented somehow?

Not sure what you mean: No software can prevent external events such as
I/O timeouts.


> 2. Does Squid also learn intermediate certificates from complete
> certificate chains of other requests?

Interesting question. AFAIK, Squid does not cache certificates received
in TLS server Hellos (yet?). The missing certificates are fetched and
cached using the regular Squid HTTP fetching/caching mechanism (as if
somebody else sent a simple GET request for the certificate). There is
no dedicated cache type/system for the certificates. This implies that
the same intermediate certificate, if it was fetched from two different
places/URLs, will be cached twice (by default).

I have CCed Christos that may be able to verify my statements in the
above paragraph.


> 3. Will this feature make it necessary to increase the cache size?

YMMV. By definition, the cache should never be necessary (i.e. required
for correct operation). You should increase the cache size if increasing
the cache size improves performance. This general statement applies to
all features, not just the feature discussed on this thread, of course.

Alex.


From rousskov at measurement-factory.com  Thu Sep 27 17:28:57 2018
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Thu, 27 Sep 2018 11:28:57 -0600
Subject: [squid-users] Fetch missing certificate feature of Squid_v4
In-Reply-To: <CAFyThpKD7E3UjxuJL_xSn6GmXE6uUS9ikNRFt7mE__s=TxPUmQ@mail.gmail.com>
References: <CAFyThpJh76KuqwDdbF8obdSJyFVSSt0O2Q-raoM=-wRgo6_8Kw@mail.gmail.com>
 <b2292743-03b5-c7dd-d1a2-382274f5f251@measurement-factory.com>
 <CAFyThpKD7E3UjxuJL_xSn6GmXE6uUS9ikNRFt7mE__s=TxPUmQ@mail.gmail.com>
Message-ID: <67287ea1-1aaf-1859-70e8-f2bd6a2331ed@measurement-factory.com>

On 09/27/2018 11:12 AM, Christof Gerber wrote:

> I mean what happens if the extra request to the CA to download the
> missing certificate takes ages. Is there a timeout routine running
> which aborts the request if for instance the certificate is not
> downloaded after 5 seconds?

Yes, of course. There are many timeouts at play here. For example,
forward_timeout is used when setting a timeout to negotiate a secure
connection with the origin server (which includes fetching missing
certificates) and read_timeout is a network read timeout applied to
every individual fetching request.

Again, fetching a missing certificate feature reuses the regular "fetch
this URL" functionality in Squid, with all the features/timeouts on that
code path. IIRC, these internal certificate requests even go through
eCAP/ICAP REQMOD services!

Alex.


> On Thu, 27 Sep 2018 at 18:32, Alex Rousskov wrote:
>>
>> On 09/27/2018 09:56 AM, Christof Gerber wrote:
>>> Concerning the new feature which fetches the missing intermediate
>>> certificates I have three questions about its implementation and
>>> implications:
>>
>>> 1. What happens if the certificate fetch requests runs into a timeout?
>>
>> If Squid lacks a certificate required to validate the server, the server
>> validation will fail. What happens after that probably depends on your
>> configuration, but bumping the client connection to report the
>> validation error is typical for SslBump-driven deployments.
>>
>>
>>> Is this prevented somehow?
>>
>> Not sure what you mean: No software can prevent external events such as
>> I/O timeouts.
>>
>>
>>> 2. Does Squid also learn intermediate certificates from complete
>>> certificate chains of other requests?
>>
>> Interesting question. AFAIK, Squid does not cache certificates received
>> in TLS server Hellos (yet?). The missing certificates are fetched and
>> cached using the regular Squid HTTP fetching/caching mechanism (as if
>> somebody else sent a simple GET request for the certificate). There is
>> no dedicated cache type/system for the certificates. This implies that
>> the same intermediate certificate, if it was fetched from two different
>> places/URLs, will be cached twice (by default).
>>
>> I have CCed Christos that may be able to verify my statements in the
>> above paragraph.
>>
>>
>>> 3. Will this feature make it necessary to increase the cache size?
>>
>> YMMV. By definition, the cache should never be necessary (i.e. required
>> for correct operation). You should increase the cache size if increasing
>> the cache size improves performance. This general statement applies to
>> all features, not just the feature discussed on this thread, of course.
>>
>> Alex.
>> _______________________________________________
>> squid-users mailing list
>> squid-users at lists.squid-cache.org
>> http://lists.squid-cache.org/listinfo/squid-users
> 
> 
> 



From brett.anderson.ftw at gmail.com  Thu Sep 27 23:04:16 2018
From: brett.anderson.ftw at gmail.com (Brett)
Date: Thu, 27 Sep 2018 18:04:16 -0500 (CDT)
Subject: [squid-users] Why does this proxy configuration ignore no-cache and
	no-store?
Message-ID: <1538089456992-0.post@n4.nabble.com>

I'm having some trouble because my 4.0.24-VCS squid proxy is caching requests
that it shouldn't be, breaking the website I'm routing through it.

>From the HAR output of the client using the proxy:

Response Headers
Cache-Control	
     no-cache;no-store
Content-Encoding	
     gzip
Content-Type	
     text/html;charset=utf-8
Date	
    Thu, 27 Sep 2018 22:27:17 GMT
Pragma	
    no-cache
Server	
    pache-Coyote/1.1
Vary	
    Accept-Encoding
Age	
    24
Warning	
    110 squid/4.0.24-VCS "Response is stale"
X-Cache	
    HIT from proxy
Via	
    1.1 proxy (squid/4.0.24-VCS)
Connection	
    keep-alive


Note the no-cache;no-store Cache Control headers and then the proxy
returning the result from the cache, and it's awareness of not following
HTTP rules, i.e. "Response is stale"

This would indicate that my configuration is telling the proxy to ignore
these rules. I do have some rules setup for images etc that do override
cache control, but not for html, text etc, which this request was for.
Following is my configuration:

http_port 3128 ssl-bump \
  cert=/apps/server_crt.pem key=/apps/server_key.pem \
  generate-host-certificates=on dynamic_cert_mem_cache_size=4MB
sslcrtd_program /apps/squid/libexec/security_file_certgen -s
/apps/squid/var/lib/ssl_db -M 4MB sslcrtd_children 8 startup=1 idle=1 

acl step1 at_step SslBump1
ssl_bump peek step1
ssl_bump bump all

acl localnet src 10.0.0.0/8     # RFC1918 possible internal network
acl localnet src 172.16.0.0/12  # RFC1918 possible internal network
acl localnet src 192.168.0.0/16 # RFC1918 possible internal network
acl localnet src fc00::/7       # RFC 4193 local private network range
acl localnet src fe80::/10      # RFC 4291 link-local (directly plugged)
machines
acl SSL_ports port 443
acl Safe_ports port 80          # http
acl Safe_ports port 21          # ftp
acl Safe_ports port 443         # https
acl Safe_ports port 70          # gopher
acl Safe_ports port 210         # wais
acl Safe_ports port 280         # http-mgmt
acl Safe_ports port 488         # gss-http
acl Safe_ports port 591         # filemaker
acl Safe_ports port 777         # multiling http
acl Safe_ports port 1025-65535  # unregistered ports
acl CONNECT method CONNECT
http_access deny !Safe_ports
http_access deny CONNECT !SSL_ports
http_access allow localhost manager
http_access deny manager
coredump_dir /apps/squid/var/cache
maximum_object_size 10 GB
cache_dir ufs /apps/squid/var/cache/squid 100 16 256
cache_mem 1024 MB
maximum_object_size_in_memory 512 KB
cache_replacement_policy heap LFUDA
range_offset_limit -1
quick_abort_min -1 KB
offline_mode on
http_access allow localnet
http_access allow localhost
http_access deny all
refresh_pattern ^ftp: 1440 20% 10080
refresh_pattern ^gopher: 1440 0% 1440
refresh_pattern -i \.(gif|png|jpg|jpeg|ico|woff|woff2)$ 10080 90% 43200
override-expire ignore-no-cache ignore-no-store ignore-private
refresh_pattern -i \.(iso|avi|wav|mp3|mp4|mpeg|swf|flv|x-flv)$ 43200 90%
432000 override-expire ignore-no-cache ignore-no-store ignore-private
refresh_pattern -i \.(css|js)$ 1440 40% 40320

I've also tried deleting all of the refresh_pattern statements and I still
get the same outcome. What am I doing wrong?



--
Sent from: http://squid-web-proxy-cache.1019090.n4.nabble.com/Squid-Users-f1019091.html


From squid3 at treenet.co.nz  Fri Sep 28 07:56:07 2018
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Fri, 28 Sep 2018 19:56:07 +1200
Subject: [squid-users] Why does this proxy configuration ignore no-cache
 and no-store?
In-Reply-To: <1538089456992-0.post@n4.nabble.com>
References: <1538089456992-0.post@n4.nabble.com>
Message-ID: <6a8bc753-5abf-6cf2-134b-11858b664dc4@treenet.co.nz>

On 28/09/18 11:04 AM, Brett wrote:
> I'm having some trouble because my 4.0.24-VCS squid proxy is caching requests
> that it shouldn't be, breaking the website I'm routing through it.

NP: please upgrade your proxy that is a beta release. Squid-4 now has
several stable releases.


> 
> From the HAR output of the client using the proxy:
> 
> Response Headers
> Cache-Control	
>      no-cache;no-store

Is the ';' above part of the HAR format or part of the actual headers
received?

...
> Vary	
>     Accept-Encoding
> Age	
>     24
> Warning	
>     110 squid/4.0.24-VCS "Response is stale"
> X-Cache	
>     HIT from proxy
> Via	
>     1.1 proxy (squid/4.0.24-VCS)
...

> 
> Note the no-cache;no-store Cache Control headers and then the proxy
> returning the result from the cache, and it's awareness of not following
> HTTP rules, i.e. "Response is stale"

If your response contains "Cache-Control: no-cache;no-store" then that
actually means just "Cache-Control: no-cache" which tells Squid it *can*
cache the response.

If the response contains "Cache-Control: no-cache, no-store" then that
actually means just "Cache-Control: no-store". More on that below.


> 
> This would indicate that my configuration is telling the proxy to ignore
> these rules. I do have some rules setup for images etc that do override
> cache control, but not for html, text etc, which this request was for.

One thing to be aware of "html, text etc" has no meaning to Squid. It is
working strictly from whether the given regex pattern matches against
the full URL string. Including the query-string part.

That means URLs like "http://example.com/some.html?got=.iso" will match
your pattern.


> Following is my configuration:
> 
...
> offline_mode on
...
> refresh_pattern ^ftp: 1440 20% 10080
> refresh_pattern ^gopher: 1440 0% 1440
> refresh_pattern -i \.(gif|png|jpg|jpeg|ico|woff|woff2)$ 10080 90% 43200
> override-expire ignore-no-cache ignore-no-store ignore-private
> refresh_pattern -i \.(iso|avi|wav|mp3|mp4|mpeg|swf|flv|x-flv)$ 43200 90%
> 432000 override-expire ignore-no-cache ignore-no-store ignore-private
> refresh_pattern -i \.(css|js)$ 1440 40% 40320
> 

NP: ignore-no-cache is no longer supported since version ~3.2. In
HTTP/1.1 compliant proxies like Squid it actually *prevents* caching
which is counter to most intended uses and better done with
cache/store_miss/send_hit directives allow/deny rules instead.


You are also missing the default refresh_pattern line carefully crafted
to make broken CGI scripts and dynamic content behave according to
RFC2616 caching requirements. Without it such broken content will be
cached for very, very long times.

Please restore this line to the end of your refresh_pattern lines:

  refresh_pattern -i (/cgi-bin/\?) 0 0% 0


> I've also tried deleting all of the refresh_pattern statements and I still
> get the same outcome. What am I doing wrong?
> 

Three things that I can see:

1) ignore-no-store tells Squid to ignore Cache-Control:no-store headers.

Since no-store overrides no-cache in HTTP semantics which means
"Cache-Control: no-cache, no-store" is just "Cache-Control: no-store"
... you have told Squid to ignore that header entirely.

Removing that option was the right thing to do. But not sufficient by
itself (or removing the whole line ) to immediately change Squid
behaviour because of the below...


2) offline_mode on tells squid not to revalidate anything.

This directive is badly named and does not do what most people think it
does. It is global in effect. Please see the documentation:
 <http://www.squid-cache.org/Doc/config/offline_mode/>

My advice is do not use this directive unless you are in the process of
a live server migration between two proxies. In which case debugging
weird traffic behaviour is best left until the procedure is completed
and both the directive and defunct proxy removed from the traffic path.


3) override-expire with an age parameter of 43200 minutes.

The specific response you are asking about is not affected by this. But
others using Expires header will be broken in similar ways when this
setting is applied to them.


Amos


From squid3 at treenet.co.nz  Fri Sep 28 07:59:45 2018
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Fri, 28 Sep 2018 19:59:45 +1200
Subject: [squid-users] Why does this proxy configuration ignore no-cache
 and no-store?
In-Reply-To: <6a8bc753-5abf-6cf2-134b-11858b664dc4@treenet.co.nz>
References: <1538089456992-0.post@n4.nabble.com>
 <6a8bc753-5abf-6cf2-134b-11858b664dc4@treenet.co.nz>
Message-ID: <f11cdecb-b176-dd0a-17f6-1feb3ff8fe13@treenet.co.nz>

On 28/09/18 7:56 PM, Amos Jeffries wrote:
> 
> You are also missing the default refresh_pattern line carefully crafted
> to make broken CGI scripts and dynamic content behave according to
> RFC2616 caching requirements. Without it such broken content will be
> cached for very, very long times.
> 
> Please restore this line to the end of your refresh_pattern lines:
> 
>   refresh_pattern -i (/cgi-bin/\?) 0 0% 0
> 

Oops, paste error. That should have been:

 refresh_pattern -i (/cgi-bin/|\?) 0 0% 0


Amos


From ygirardin at olfeo.com  Fri Sep 28 13:27:56 2018
From: ygirardin at olfeo.com (Yann Girardin)
Date: Fri, 28 Sep 2018 15:27:56 +0200
Subject: [squid-users] URL rewriter note
Message-ID: <FBAFB216-9CC2-4854-A3ED-F8F6E363F9AE@olfeo.com>


Hi 

I am using the url_rewrite_program directive on my squid 4 configuration. 

I want to use the note feature to use it in my logformat program. But when I pass %note to it my note looks empty squid send me ?-?. 

My url rewrite program return to squid line that looks something like : 

OK mynote=XX 

Where XX is always an integer. 

Is there something wrong ? 

Thank you

From marko.cupac at mimar.rs  Fri Sep 28 15:56:18 2018
From: marko.cupac at mimar.rs (Marko =?UTF-8?B?Q3VwYcSH?=)
Date: Fri, 28 Sep 2018 17:56:18 +0200
Subject: [squid-users] auth username logging
Message-ID: <20180928175618.47a75232@efreet>

Hi,

I am testing migration of my AD-authenticated (kerberos + ntlm) 3.5
setup to 4.1. I noticed there are no usernames in access.log, just "*"
for served pages, "-" for 407s.

How can I get usernames in my access.log again?

Thank you in advance,

-- 
Before enlightenment - chop wood, draw water.
After  enlightenment - chop wood, draw water.

Marko Cupa?
https://www.mimar.rs/


From rousskov at measurement-factory.com  Fri Sep 28 15:57:20 2018
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Fri, 28 Sep 2018 09:57:20 -0600
Subject: [squid-users] URL rewriter note
In-Reply-To: <FBAFB216-9CC2-4854-A3ED-F8F6E363F9AE@olfeo.com>
References: <FBAFB216-9CC2-4854-A3ED-F8F6E363F9AE@olfeo.com>
Message-ID: <61705e8a-6a2c-a6d6-113a-a3658eeafcad@measurement-factory.com>

On 09/28/2018 07:27 AM, Yann Girardin wrote:
> 
> Hi 
> 
> I am using the url_rewrite_program directive on my squid 4 configuration. 
> 
> I want to use the note feature to use it in my logformat program. But when I pass %note to it my note looks empty squid send me ?-?. 
> 
> My url rewrite program return to squid line that looks something like : 
> 
> OK mynote=XX 
> 
> Where XX is always an integer. 
> 
> Is there something wrong ? 

It is difficult to say what is going on without more information. Please
start by posting your Squid version and the relevant parts of your Squid
configuration.


Thank you,

Alex.




From service.mv at gmail.com  Fri Sep 28 19:51:11 2018
From: service.mv at gmail.com (neok)
Date: Fri, 28 Sep 2018 14:51:11 -0500 (CDT)
Subject: [squid-users] negotiate_kerberos_auth: ERROR
Message-ID: <1538164271714-0.post@n4.nabble.com>

Hello people, in general terms my proxy works quite well. However I tell you
that very eventually, (maybe about 10 times per day based on 15 users using
my test proxy) I get this error in cache.log:


negotiate_kerberos_auth.cc(180): pid=21573 :2018/09/28 14:42:25|
negotiate_kerberos_auth: ERROR: gss_accept_sec_context() failed: Unspecified
GSS failure.  Minor code may provide more information. Request is a replay
2018/09/28 14:42:25 kid1| ERROR: Negotiate Authentication validating user.
Result: {result=BH, notes={message: gss_accept_sec_context() failed:
Unspecified GSS failure.  Minor code may provide more information. Request
is a replay; }}
negotiate_kerberos_auth.cc(180): pid=21573 :2018/09/28 14:42:26|
negotiate_kerberos_auth: ERROR: gss_accept_sec_context() failed: Unspecified
GSS failure.  Minor code may provide more information. Request is a replay
2018/09/28 14:42:26 kid1| ERROR: Negotiate Authentication validating user.
Result: {result=BH, notes={message: gss_accept_sec_context() failed:
Unspecified GSS failure.  Minor code may provide more information. Request
is a replay; }}

I've browse several hours without finding out what causes this error, or if
it's serious, or if I should ignore it.
Could someone with more experience tell me if it's possible what could be
the reason for this error?
My testing environment:

- A VM CentOS 7 Core over VirtualBox 5.2, 1 NIC.
- My VM is attached to my domain W2012R2 (following this post
https://www.rootusers.com/how-to-join-centos-linux-to-an-active-directory-domain/)
to achieve kerberos authentication transparent to the user. SElinux
disabled. Owner permissions to user squid in all folders/files involved.
- squid 3.5.20 installed and working great with Negotiate/NTLM and
Negotiate/Kerberos authentication


squid.conf 
### Negotiate/NTLM and Negotiate/Kerberos authentication 
auth_param negotiate program /usr/sbin/negotiate_wrapper --ntlm
/usr/bin/ntlm_auth --helper-protocol=squid-2.5-ntlmssp --kerberos
/usr/lib64/squid/negotiate_kerberos_auth -r -d -s GSS_C_NO_NAME 
auth_param negotiate children 200 
auth_param negotiate keep_alive on 

Thank you very much indeed.
Cordial greetings
Gabriel




--
Sent from: http://squid-web-proxy-cache.1019090.n4.nabble.com/Squid-Users-f1019091.html


From squid3 at treenet.co.nz  Fri Sep 28 23:17:49 2018
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sat, 29 Sep 2018 11:17:49 +1200
Subject: [squid-users] auth username logging
In-Reply-To: <20180928175618.47a75232@efreet>
References: <20180928175618.47a75232@efreet>
Message-ID: <5b55cbd4-2d65-fd07-4166-891b5b556696@treenet.co.nz>

On 29/09/18 3:56 AM, Marko Cupa? wrote:
> Hi,
> 
> I am testing migration of my AD-authenticated (kerberos + ntlm) 3.5
> setup to 4.1. I noticed there are no usernames in access.log, just "*"
> for served pages, "-" for 407s.
> 
> How can I get usernames in my access.log again?

What is your auth_param config?

It sounds to me like you are using a "Negotiate/NTLM" auth helper for
"NTLM" authentication.

Amos


From squid3 at treenet.co.nz  Fri Sep 28 23:28:56 2018
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sat, 29 Sep 2018 11:28:56 +1200
Subject: [squid-users] negotiate_kerberos_auth: ERROR
In-Reply-To: <1538164271714-0.post@n4.nabble.com>
References: <1538164271714-0.post@n4.nabble.com>
Message-ID: <1492e874-ba5a-d1bc-7b9d-87d54be92981@treenet.co.nz>

On 29/09/18 7:51 AM, neok wrote:
> Hello people, in general terms my proxy works quite well. However I tell you
> that very eventually, (maybe about 10 times per day based on 15 users using
> my test proxy) I get this error in cache.log:
> 
> 
> negotiate_kerberos_auth.cc(180): pid=21573 :2018/09/28 14:42:25|
> negotiate_kerberos_auth: ERROR: gss_accept_sec_context() failed: Unspecified
> GSS failure.  Minor code may provide more information. Request is a replay
> 2018/09/28 14:42:25 kid1| ERROR: Negotiate Authentication validating user.
> Result: {result=BH, notes={message: gss_accept_sec_context() failed:
> Unspecified GSS failure.  Minor code may provide more information. Request
> is a replay; }}
> negotiate_kerberos_auth.cc(180): pid=21573 :2018/09/28 14:42:26|
> negotiate_kerberos_auth: ERROR: gss_accept_sec_context() failed: Unspecified
> GSS failure.  Minor code may provide more information. Request is a replay
> 2018/09/28 14:42:26 kid1| ERROR: Negotiate Authentication validating user.
> Result: {result=BH, notes={message: gss_accept_sec_context() failed:
> Unspecified GSS failure.  Minor code may provide more information. Request
> is a replay; }}
> 
> I've browse several hours without finding out what causes this error, or if
> it's serious, or if I should ignore it.

It is serious.

> Could someone with more experience tell me if it's possible what could be
> the reason for this error?

"Request is a replay", aka "token replay attack".

The client is sending a credentials token which has already been used on
another connection. Such clients are either fatally broken, or malicious.

Negotiate and NTLM credentials authenticate the specific TCP connection
they are used on. They are not permitted to be re-used on other
connections nor changed once authenticated.


PS. please upgrade if you can, there are security issues with 3.5.23 and
older releases. Eliezer provides updated CentOS packages for more recent
Squid versions (see <https://wiki.squid-cache.org/KnowledgeBase/CentOS>).

Amos


From eliezer at ngtech.co.il  Sat Sep 29 06:18:01 2018
From: eliezer at ngtech.co.il (Eliezer Croitoru)
Date: Sat, 29 Sep 2018 09:18:01 +0300
Subject: [squid-users] Help: squid restarts and squidGuard die
In-Reply-To: <CA+d==oEt8J-ie2Fm1SYrHSBKD6Y7TRF-v3iGiMXC7WQ36WV1vw@mail.gmail.com>
References: <CA+d==oEt8J-ie2Fm1SYrHSBKD6Y7TRF-v3iGiMXC7WQ36WV1vw@mail.gmail.com>
Message-ID: <!&!AAAAAAAAAAAuAAAAAAAAAL3wsAcc8JBJkMCbPuiQMGEBAMO2jhD3dRHOtM0AqgC7tuYAAAAAAA4AABAAAAD3jhNc4xlaR6tlvz7g9NgqAQAAAAA=@ngtech.co.il>

Hey Gabriel,

 

The thread seems to me as a milestone in this mailing list and in Squid-Cache history.

>From what I understood there is an issue when SquidGuard receives a specific line from Squid.

In this whole long thread I have not seen any debug logs of what SquidGuard receives from Squid.

It?s crucial to understand what the issue is and why it happens regardless to whether SquidGuard is old or not.

Also it?s not related to an ICAP service or URL rewrite or external acl?

I do not remember by heart what debug log section is relevant but Amos and Alex should be able to direct us towards these.

When you will have the exact line that the url_rewrite helper receives we would be able to know and maybe understand some details.

For some admins this kind of setup is easy but.. any LDAP\NTLM\Kerberos related setup needs to be tested and I believe this is where you are at.

There is a possibility that SquidGuard as a url_rewrite helper doesn?t receive the relevant details it expects such as username or group.

The above can cause this issue.

 

If you can share with us the relevant line that SquidGuard receives and crashes it would help other admins who have yet to encounter it.

 

Eliezer

?         I have a setup of above 800 users but? the cache features are tuned off and it?s only working for ACL checking.

 

 

----

 <http://ngtech.co.il/lmgtfy/> Eliezer Croitoru
Linux System Administrator
Mobile: +972-5-28704261
Email: eliezer at ngtech.co.il



 

From: squid-users [mailto:squid-users-bounces at lists.squid-cache.org] On Behalf Of Service MV
Sent: Monday, September 17, 2018 18:38
To: squid-users at lists.squid-cache.org
Subject: [squid-users] Help: squid restarts and squidGuard die

 

Dear Ones, I draw on your experience in seeking help to determine whether or not it is possible to achieve the configuration I am looking for, due to a strange error I am having.

 

Before commenting on the bug I describe my testing environment:

- A VM CentOS 7 Core over VirtualBox 5.2, 1 NIC.

- My VM is attached to my domain W2012R2 (following this post https://www.rootusers.com/how-to-join-centos-linux-to-an-active-directory-domain/) to achieve kerberos authentication transparent to the user. SElinux disabled. Owner permissions to user squid in all folders/files involved.

- squid 3.5.20 installed and working great with kerberos, NTLM and basic authentication. All authentication mechanisms tested and working great.

- SquidGuard: 1.4 Berkeley DB 5.3.21 installed and working great with blacklists and acl default.

 

My problem starts when I try to use source acl using ldapusersearch in squidGuard... 

 

systemctl status squid:

(squid-1)[12627]: The redirector helpers are crashing too rapidly, need help!

<SNIP>

 

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20180929/1cc3911a/attachment.htm>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: image003.png
Type: image/png
Size: 11308 bytes
Desc: not available
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20180929/1cc3911a/attachment.png>

From marko.cupac at mimar.rs  Sat Sep 29 10:23:07 2018
From: marko.cupac at mimar.rs (Marko =?UTF-8?B?Q3VwYcSH?=)
Date: Sat, 29 Sep 2018 12:23:07 +0200
Subject: [squid-users] auth username logging
In-Reply-To: <5b55cbd4-2d65-fd07-4166-891b5b556696@treenet.co.nz>
References: <20180928175618.47a75232@efreet>
 <5b55cbd4-2d65-fd07-4166-891b5b556696@treenet.co.nz>
Message-ID: <20180929122307.0b5b3f61@efreet>

On Sat, 29 Sep 2018 11:17:49 +1200
Amos Jeffries <squid3 at treenet.co.nz> wrote:

> On 29/09/18 3:56 AM, Marko Cupa? wrote:
> > Hi,
> > 
> > I am testing migration of my AD-authenticated (kerberos + ntlm) 3.5
> > setup to 4.1. I noticed there are no usernames in access.log, just
> > "*" for served pages, "-" for 407s.
> > 
> > How can I get usernames in my access.log again?  
> 
> What is your auth_param config?
> 
> It sounds to me like you are using a "Negotiate/NTLM" auth helper for
> "NTLM" authentication.

Hi,

Here's relevant part of squid.conf:

# AUTHENTICATION HELPERS
auth_param negotiate program \
  /usr/local/libexec/squid/negotiate_wrapper_auth \
    --ntlm /usr/local/bin/ntlm_auth --helper-protocol=gss-spnego \
      --domain=MIMAR \
    --kerberos /usr/local/libexec/squid/negotiate_kerberos_auth \
      -d -r -s GSS_C_NO_NAME
auth_param negotiate children 20 startup=0 idle=1
auth_param negotiate keep_alive on

I am not sure what exactly authenticates, kerberos or NTLM.

Thank you in advance for any pointers,
-- 
Before enlightenment - chop wood, draw water.
After  enlightenment - chop wood, draw water.

Marko Cupa?
https://www.mimar.rs/


From squid3 at treenet.co.nz  Sun Sep 30 07:57:36 2018
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sun, 30 Sep 2018 20:57:36 +1300
Subject: [squid-users] auth username logging
In-Reply-To: <20180929122307.0b5b3f61@efreet>
References: <20180928175618.47a75232@efreet>
 <5b55cbd4-2d65-fd07-4166-891b5b556696@treenet.co.nz>
 <20180929122307.0b5b3f61@efreet>
Message-ID: <d430d132-efd5-cf89-95c7-93e1fcc1ff7a@treenet.co.nz>

On 29/09/18 10:23 PM, Marko Cupa? wrote:
> On Sat, 29 Sep 2018 11:17:49 +1200
> Amos Jeffries <squid3 at treenet.co.nz> wrote:
> 
>> On 29/09/18 3:56 AM, Marko Cupa? wrote:
>>> Hi,
>>>
>>> I am testing migration of my AD-authenticated (kerberos + ntlm) 3.5
>>> setup to 4.1. I noticed there are no usernames in access.log, just
>>> "*" for served pages, "-" for 407s.
>>>
>>> How can I get usernames in my access.log again?  
>>
>> What is your auth_param config?
>>
>> It sounds to me like you are using a "Negotiate/NTLM" auth helper for
>> "NTLM" authentication.
> 
> Hi,
> 
> Here's relevant part of squid.conf:
> 
> # AUTHENTICATION HELPERS
> auth_param negotiate program \
>   /usr/local/libexec/squid/negotiate_wrapper_auth \
>     --ntlm /usr/local/bin/ntlm_auth --helper-protocol=gss-spnego \


--helper-protocol=gss-spnego is telling the samba helper to use
Negotiate protocol, but the wrapper is expecting NTLM protocol and
mapping them.

Please try --helper-protocol=squid-2.5-ntlmssp



>       --domain=MIMAR \
>     --kerberos /usr/local/libexec/squid/negotiate_kerberos_auth \
>       -d -r -s GSS_C_NO_NAME
> auth_param negotiate children 20 startup=0 idle=1
> auth_param negotiate keep_alive on
> 
> I am not sure what exactly authenticates, kerberos or NTLM.
> 
> Thank you in advance for any pointers,
> 


Amos


From ahmed.zaeem at netstream.ps  Sun Sep 30 18:55:51 2018
From: ahmed.zaeem at netstream.ps (--Ahmad--)
Date: Sun, 30 Sep 2018 21:55:51 +0300
Subject: [squid-users] want to change squid name
Message-ID: <46DA2877-D62D-4271-9D3C-E95825FA8174@netstream.ps>

Hey  Folks 
my question may be crazy a little bit .

i want to change everything in squid config files and rename it to ahmad.

so i want change eveverythingeytng in folders , files  from squid  to ?> stinger  and have stinger conf files and run instance as stinger instead of squid .

so i changed recursively everything and replace squid with ?stinger?



here what i made :

find /root/squid-3.5.22 -type f -exec sed -i -e 's/squid/stinger/g' {} \;
find /root/squid-3.5.22 -type f -exec sed -i -e 's/Squid/Stinger/g' {} \;
find /root/squid-3.5.22 -type f -exec sed -i -e 's/SQUID/STINGER/g' {} \;

find . -iname "*squid*" -exec rename squid stinger '{}' \;
find . -iname "*squid*" -exec rename squid stinger '{}' \;
find . -iname "*squid*" -exec rename Squid Stinger '{}' \;

###################





 but compilation give me error 



lo heap.lo iso3307.lo radix.lo rfc1035.lo rfc1123.lo rfc2671.lo rfc3596.lo Splay.lo stub_memaccount.lo util.lo xusleep.lo  
libtool: link: /usr/bin/ar cru .libs/libmiscutil.a .libs/MemPool.o .libs/MemPoolChunked.o .libs/MemPoolMalloc.o .libs/getfullhostname.o .libs/heap.o .libs/iso3307.o .libs/radix.o .libs/rfc1035.o .libs/rfc1123.o .libs/rfc2671.o .libs/rfc3596.o .libs/Splay.o .libs/stub_memaccount.o .libs/util.o .libs/xusleep.o 
libtool: link: ranlib .libs/libmiscutil.a
libtool: link: ( cd ".libs" && rm -f "libmiscutil.la" && ln -s "../libmiscutil.la" "libmiscutil.la" )
make[2]: Leaving directory `/root/squid-3.5.22/lib'
make[1]: Leaving directory `/root/squid-3.5.22/lib'
Making all in libltdl
make[1]: Entering directory `/root/squid-3.5.22/libltdl'
CDPATH="${ZSH_VERSION+.}:" && cd . && /bin/sh /root/squid-3.5.22/cfgaux/missing aclocal-1.15 -I m4
/root/squid-3.5.22/cfgaux/missing: line 81: aclocal-1.15: command not found
WARNING: 'aclocal-1.15' is missing on your system.
         You should only need it if you modified 'acinclude.m4' or
         'configure.ac' or m4 files included by 'configure.ac'.
         The 'aclocal' program is part of the GNU Automake package:
         <http://www.gnu.org/software/automake>
         It also requires GNU Autoconf, GNU m4 and Perl in order to run:
         <http://www.gnu.org/software/autoconf>
         <http://www.gnu.org/software/m4/>
         <http://www.perl.org/>
make[1]: *** [aclocal.m4] Error 127
make[1]: Leaving directory `/root/squid-3.5.22/libltdl'
make: *** [all-recursive] Error 1
[root at li1802-227 squid-3.5.22]# 



any help ?









-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20180930/8671ffe5/attachment.htm>

From rousskov at measurement-factory.com  Sun Sep 30 22:50:50 2018
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Sun, 30 Sep 2018 16:50:50 -0600
Subject: [squid-users] want to change squid name
In-Reply-To: <46DA2877-D62D-4271-9D3C-E95825FA8174@netstream.ps>
References: <46DA2877-D62D-4271-9D3C-E95825FA8174@netstream.ps>
Message-ID: <ee2ce272-33f3-bb7c-4022-9cdd2cd7d00e@measurement-factory.com>

On 09/30/2018 12:55 PM, --Ahmad-- wrote:

> i want to change everything in squid config files and rename it to ahmad.

Generally useful Squid code modifications should be discussed on the
squid-dev mailing list, not squid-users. The modification you are
describing is not generally useful so it is probably out of Squid
Project support scope.

However, if you formulate the actual problem you are trying to solve,
then somebody on this mailing list may know a solution that does not
include blind (and, in some cases, illegal) changes of Squid sources.

What are you trying to accomplish? In other words, what problem do you
think replacing "squid" to "ahmad" in Squid sources would solve?

Alex.


