From johnrefwe at mail.com  Thu Aug  1 02:44:12 2019
From: johnrefwe at mail.com (johnr)
Date: Wed, 31 Jul 2019 21:44:12 -0500 (CDT)
Subject: [squid-users] Blocking CONNECT
Message-ID: <1564627452336-0.post@n4.nabble.com>

Hi,

Squid conf:
acl CONNECT method CONNECT
acl to_bad_ip dst 55.55.2.3
http_access deny CONNECT to_bad_ip

In the above squid config, if I were to try go to https://55.55.2.3:443 I
would get an ACCESS DENIED but squid would not block the CONNECT (it would
respond to 200) and then block the subsequent HTTP request. Is it possible
to tell squid to block the CONNECT? I do server-first SSL bump so if I don't
block the CONNECT squid will reach out to the upstream server which I don't
want it to do. I know this would make it impossible to serve the block page
and have the browser show an error but I don't mind about that.  



--
Sent from: http://squid-web-proxy-cache.1019090.n4.nabble.com/Squid-Users-f1019091.html


From rousskov at measurement-factory.com  Thu Aug  1 03:08:19 2019
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Wed, 31 Jul 2019 23:08:19 -0400
Subject: [squid-users] Blocking CONNECT
In-Reply-To: <1564627452336-0.post@n4.nabble.com>
References: <1564627452336-0.post@n4.nabble.com>
Message-ID: <ab1692d5-adc6-1b3e-548c-80f99b2d9b51@measurement-factory.com>

On 7/31/19 10:44 PM, johnr wrote:

> acl CONNECT method CONNECT
> acl to_bad_ip dst 55.55.2.3
> http_access deny CONNECT to_bad_ip

> In the above squid config, if I were to try go to https://55.55.2.3:443 I
> would get an ACCESS DENIED but squid would not block the CONNECT (it would
> respond to 200) and then block the subsequent HTTP request.

Yes, that is (currently) intentional.


> Is it possible to tell squid to block the CONNECT?

Not for connections that are subject to SslBump processing AFAIK. There
is a known need for a feature that would make such
bumping-to-deliver-CONNECT-error optional, but that feature has not been
sponsored or donated yet (and its design may require a preliminary
discussion on squid-dev). If I am not missing any workarounds, then your
options are outlined at

https://wiki.squid-cache.org/SquidFaq/AboutSquid#How_to_add_a_new_Squid_feature.2C_enhance.2C_of_fix_something.3F


> I do server-first SSL bump so if I don't block the CONNECT squid will
> reach out to the upstream server which I don't want it to do.

Yes, that is one of the reasons why folks want to make
bumping-to-deliver-CONNECT-error optional.


> I know this would make it impossible to serve the block page
> and have the browser show an error but I don't mind about that.  

Yes, thank you for disclosing that understanding.

Alex.


From squid3 at treenet.co.nz  Thu Aug  1 09:07:06 2019
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 1 Aug 2019 21:07:06 +1200
Subject: [squid-users] Reverse Proxy Detected
In-Reply-To: <d5e9f059-cf8f-4a87-a9de-dc0782d1f165@www.fastmail.com>
References: <d5e9f059-cf8f-4a87-a9de-dc0782d1f165@www.fastmail.com>
Message-ID: <d576bb56-784a-bf70-379b-1f1afd028408@treenet.co.nz>

On 1/08/19 9:41 am, creditu wrote:
> We have been using several squid servers in accelerator mode for a 
> number of years mainly for load balancing to send public requests to 
> backend servers.  The requests to the squids typically come via a 
> well known commercial  caching service.   The squids don't do any 
> caching, they just forward requests to the backend.
> 
> Recently the vulnerability scanner that we use changed a plugin from 
> Info level to Moderate for reverse proxy detection. We need to
> mitigate this so the vulnerability scanner doesn't flag for the
> reverse proxy detection.
> 

So the first question is why they did that change. It may be that the
particular Squid version you have is known to have issues they are
warning about.


> On a non-production server I added the following.  This seems to 
> mitigate the vulnerability in the eyes of the scanner.  (I may be 
> able to get away with not including the X-Cache-Lookup  line and 
> still fix the issue.)
> 
> via off reply_header_access X-Cache deny all reply_header_access 
> X-Cache-Lookup deny all
> 
> This removes the headers for both the outgoing traffic to the 
> Internet and the backend traffic to the webservers.  I have not seen 
> any operational impact of doing this, but wanted get some feedback
> on if there is a better way to fix this issue and if I am missing
> any possible implications.
> 

The above are fine.

The "via off" does technically violate a "proxy MUST add" requirement
from RFCs 2616 and 7230. But the HTTPbis WG has been looking into
reducing that to a SHOULD for the next HTTP spec update and the results
looked good. Proxies are already allowed by those RFCs to aggregate
several Via entries together - which is essentially the same as not
adding a new one.


FWIW; All X-* headers are deprecated officially by HTTP and those
X-Cache* ones were only intended to be debug info by their creators back
in 1990's. Though some admin or software apparently place unwarranted
dependency on them for other uses.



> Also, does the following have the same effect as "via off"? 
> reply_header_access Via deny all
> 

No. "via off" will stop Squid adding itself to the Via header. Any
existing header should remain so that other systems outside yours can
still use the feature. This works in both request and reply traffic.

The access denial approach erases other agents Via headers. Rendering
any use of that feature broken for that other software.

Amos


From ahmed.zaeem at netstream.ps  Thu Aug  1 12:55:05 2019
From: ahmed.zaeem at netstream.ps (--Ahmad--)
Date: Thu, 1 Aug 2019 15:55:05 +0300
Subject: [squid-users] logformat for squid5 ?
Message-ID: <EE6D20F3-1CB9-46BA-8B44-B6E4690DB491@netstream.ps>

Hello folks 

any news for logformat directive for squid 5.x ?
or any alternative thing to it ?


From rousskov at measurement-factory.com  Thu Aug  1 13:13:30 2019
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Thu, 1 Aug 2019 09:13:30 -0400
Subject: [squid-users] logformat for squid5 ?
In-Reply-To: <EE6D20F3-1CB9-46BA-8B44-B6E4690DB491@netstream.ps>
References: <EE6D20F3-1CB9-46BA-8B44-B6E4690DB491@netstream.ps>
Message-ID: <a366ac05-91d8-c7de-f344-ff6aff12a00a@measurement-factory.com>

On 8/1/19 8:55 AM, --Ahmad-- wrote:

> any news for logformat directive for squid 5.x ?

There were many logformat-related changes in v5. What specifically are
you looking for?

Alex.


From ahmed.zaeem at netstream.ps  Thu Aug  1 13:23:21 2019
From: ahmed.zaeem at netstream.ps (--Ahmad--)
Date: Thu, 1 Aug 2019 16:23:21 +0300
Subject: [squid-users] logformat for squid5 ?
In-Reply-To: <EE6D20F3-1CB9-46BA-8B44-B6E4690DB491@netstream.ps>
References: <EE6D20F3-1CB9-46BA-8B44-B6E4690DB491@netstream.ps>
Message-ID: <1C2E468A-6E27-4D13-B7C0-E55B01C45424@netstream.ps>

i use :
logformat squid %tl %6tr %>a %>p %>la %>lp %Ss/%03Hs %<st %rm %ru %un %Sh/ %<A %<a %<la

in squid 3.x and its working fine , but in 5.x it dont work as i want 

Thanks 


> On 1 Aug 2019, at 15:55, --Ahmad-- <ahmed.zaeem at netstream.ps> wrote:
> 
> Hello folks 
> 
> any news for logformat directive for squid 5.x ?
> or any alternative thing to it ?
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users



From rousskov at measurement-factory.com  Thu Aug  1 13:55:15 2019
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Thu, 1 Aug 2019 09:55:15 -0400
Subject: [squid-users] logformat for squid5 ?
In-Reply-To: <1C2E468A-6E27-4D13-B7C0-E55B01C45424@netstream.ps>
References: <EE6D20F3-1CB9-46BA-8B44-B6E4690DB491@netstream.ps>
 <1C2E468A-6E27-4D13-B7C0-E55B01C45424@netstream.ps>
Message-ID: <c54548eb-b291-17d2-412b-27db051d31d4@measurement-factory.com>

On 8/1/19 9:23 AM, --Ahmad-- wrote:
> i use :
> logformat squid %tl %6tr %>a %>p %>la %>lp %Ss/%03Hs %<st %rm %ru %un %Sh/ %<A %<a %<la
> 
> in squid 3.x and its working fine , but in 5.x it dont work as i want 

We still do not have enough information to understand the problem you
are trying to solve. Please be specific. For example, describe a
transaction that logs X in v3.5 and Y in v5, and, unless it is really
obvious from X and Y, please explain why you want X and not Y.

Alex.


From chip_pop at hotmail.com  Thu Aug  1 14:35:19 2019
From: chip_pop at hotmail.com (joseph)
Date: Thu, 1 Aug 2019 09:35:19 -0500 (CDT)
Subject: [squid-users] crash Error negotiating SSL connection on FD 54
Message-ID: <1564670119878-0.post@n4.nabble.com>

on solaris crash is it bug to report or ?
cache.log............
2019/07/30 10:10:23 kid1| Error negotiating SSL connection on FD 22:
error:00000001:lib(0):func(0):reason(1) (1/0)
2019/07/30 10:10:37 kid1| Error negotiating SSL connection on FD 54:
error:00000001:lib(0):func(0):reason(1) (1/0)
2019/07/30 10:11:14 kid1| FATAL: Received Segment Violation...dying.
2019/07/30 10:11:14 kid1| Closing HTTP(S) port 0.0.0.0:3126
2019/07/30 10:11:14 kid1| Closing HTTP(S) port 0.0.0.0:3127


t at 1 (l at 1) terminated by signal ABRT (Abort)
0xfffffd7ffe9536ea: __lwp_kill+0x000a:  jae      __lwp_kill+0x18        [
0xfffffd7ffe9536f8, .+0xe ]
(dbx) where                                                                  
current thread: t at 1
=>[1] __lwp_kill(0x1, 0x6, 0xffffffff86785800, 0xfffffd7ffe953fde, 0x0,
0xfffffd7ffe9b3c80), at 0xfffffd7ffe9536ea 
  [2] _thr_kill(), at 0xfffffd7ffe94bfe3 
  [3] raise(), at 0xfffffd7ffe8f8229 
  [4] abort(), at 0xfffffd7ffe8d6bc0 
  [5] death(), at 0x700068 
  [6] __sighndlr(), at 0xfffffd7ffe94dee6 
  [7] call_user_handler(), at 0xfffffd7ffe9427a2 
  [8] sigacthandler(), at 0xfffffd7ffe9429ce 
  ---- called from signal handler with signal 11 (SIGSEGV) ------
  [9] 0x9514b6(0x2b6, 0xfffffd7fff037c30, 0xfffffd7ffe805ea8, 0x7, 0xa,
0xfffffd7fffdfe8d8), at 0x9514b6 
  [10] asn1_enc_save(), at 0xfffffd7ffeee949a 
  [11] 0xfffffd7ffeee42ea(), at 0xfffffd7ffeee42ea 
  [12] 0xfffffd7ffeee4e7d(), at 0xfffffd7ffeee4e7d 
  [13] 0xfffffd7ffeee4b01(), at 0xfffffd7ffeee4b01 
  [14] 0xfffffd7ffeee41ac(), at 0xfffffd7ffeee41ac 
  [15] ASN1_item_ex_d2i(), at 0xfffffd7ffeee49b8 
  [16] ASN1_item_d2i(), at 0xfffffd7ffeee3e73 
  [17] d2i_X509(), at 0xfffffd7ffeeecd90 
  [18] PEM_X509_INFO_read_bio(), at 0xfffffd7ffef870df 
  [19] X509_load_cert_crl_file(), at 0xfffffd7ffefc206f 
  [20] 0xfffffd7ffefc1bdc(), at 0xfffffd7ffefc1bdc 
  [21] X509_LOOKUP_ctrl(), at 0xfffffd7ffefc3a9c 
  [22] X509_STORE_load_locations(), at 0xfffffd7ffefc34fb 
  [23] SSL_CTX_load_verify_locations(), at 0xfffffd7fff12294d 
  [24] Security::PeerOptions::updateContextCa(), at 0x83f18c 
  [25] Security::ServerOptions::updateContextConfig(), at 0x844851 
  [26] Ssl::createSSLContext(), at 0x85d1eb 
  [27] ConnStateData::startPeekAndSplice(), at 0x5ef872 
  [28] httpsSslBumpStep2AccessCheckDone(), at 0x5efe1d 
  [29] ACLChecklist::checkCallback(), at 0x75deba 
  [30] ConnStateData::startPeekAndSplice(), at 0x5ef832 
  [31] ConnStateData::parseTlsHandshake(), at 0x5f08f8 
  [32] ConnStateData::afterClientRead(), at 0x5f0a9d 
  [33] Server::doClientRead(), at 0x7a85dc 
  [34] JobDialer<Server>::dial(), at 0x7a8e5a 
  [35] AsyncCall::make(), at 0x7bea19 
  [36] AsyncCallQueue::fireNext(), at 0x7bfdd3 
  [37] AsyncCallQueue::fire(), at 0x7c0158 
  [38] EventLoop::runOnce(), at 0x632272 
  [39] EventLoop::run(), at 0x632378 
  [40] SquidMain(), at 0x6992b3 
  [41] main(), at 0x959260 
(dbx)



-----
************************** 
***** Crash to the future  ****
**************************
--
Sent from: http://squid-web-proxy-cache.1019090.n4.nabble.com/Squid-Users-f1019091.html


From ahmed.zaeem at netstream.ps  Thu Aug  1 15:40:13 2019
From: ahmed.zaeem at netstream.ps (--Ahmad--)
Date: Thu, 1 Aug 2019 18:40:13 +0300
Subject: [squid-users] logformat for squid5 ?
In-Reply-To: <c54548eb-b291-17d2-412b-27db051d31d4@measurement-factory.com>
References: <EE6D20F3-1CB9-46BA-8B44-B6E4690DB491@netstream.ps>
 <1C2E468A-6E27-4D13-B7C0-E55B01C45424@netstream.ps>
 <c54548eb-b291-17d2-412b-27db051d31d4@measurement-factory.com>
Message-ID: <881D9FBD-2656-45C1-AC81-6294016FE4EE@netstream.ps>

ok in squid 3.x
>> logformat squid %tl %6tr %>a %>p %>la %>lp %Ss/%03Hs %<st %rm %ru %un %Sh/ %<A %<a %<la

check the syntax :

01/Aug/2019:11:29:11 -0400    837 11.11.81.74 50223 22.158.182 11961 TCP_TUNNEL/200 3205 CONNECT www.googletagservices.com:443 mwckpf HIER_DIRECT/ www.googletagservices.com 172.217.15.66 22.22.158.182


lets analyse above .:

1st thing i see the time/date of the request .

then the source ip and source port who hit squid ????>   11.11.81.74 50223
then destination ip and port of squid sender connected to ????> 22.158.182 11961
Dst URL  ?>www.googletagservices.com:443 <http://www.googletagservices.com:443/> 
User of the connection ??> mwckpf
IP resolution of the destination ??????> www.googletagservices.com 172.217.15.66
last thing the external ip address for that connection ???????> 22.22.158.182



Now on squid5.x
i add 
>> logformat squid %tl %6tr %>a %>p %>la %>lp %Ss/%03Hs %<st %rm %ru %un %Sh/ %<A %<a %<la

but the result is as :
1564669418.690    770 18.212.116.217 TCP_TUNNEL/200 40757 CONNECT www.bing.com:443 abc HIER_DIRECT/204.79.197.200 -

as you see , there is no date , so src port no dst ip/dst port .
no external ip 

i would like as possible to see results as the results in 3.5 .

hope that is clear 

Thanks Alex :)



> On 1 Aug 2019, at 16:55, Alex Rousskov <rousskov at measurement-factory.com> wrote:
> 
> On 8/1/19 9:23 AM, --Ahmad-- wrote:
>> i use :
>> logformat squid %tl %6tr %>a %>p %>la %>lp %Ss/%03Hs %<st %rm %ru %un %Sh/ %<A %<a %<la
>> 
>> in squid 3.x and its working fine , but in 5.x it dont work as i want 
> 
> We still do not have enough information to understand the problem you
> are trying to solve. Please be specific. For example, describe a
> transaction that logs X in v3.5 and Y in v5, and, unless it is really
> obvious from X and Y, please explain why you want X and not Y.
> 
> Alex.

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20190801/5d327523/attachment.htm>

From rousskov at measurement-factory.com  Thu Aug  1 15:44:03 2019
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Thu, 1 Aug 2019 11:44:03 -0400
Subject: [squid-users] crash Error negotiating SSL connection on FD 54
In-Reply-To: <1564670119878-0.post@n4.nabble.com>
References: <1564670119878-0.post@n4.nabble.com>
Message-ID: <6d487a29-6d5d-d583-9418-0f925271cd18@measurement-factory.com>

On 8/1/19 10:35 AM, joseph wrote:

> on solaris crash is it bug to report or ?

A Segment Violation crash is virtually always a Squid bug. If you are
not using some dangerous unsupported configuration and/or an unsupported
Squid version, then it is OK to report it.

If you decide to report this problem to Bugzilla, then please post
"*this" values from frames 24, 25, and 27 (if you have that information
available).

Alex.

> cache.log............
> 2019/07/30 10:10:23 kid1| Error negotiating SSL connection on FD 22:
> error:00000001:lib(0):func(0):reason(1) (1/0)
> 2019/07/30 10:10:37 kid1| Error negotiating SSL connection on FD 54:
> error:00000001:lib(0):func(0):reason(1) (1/0)
> 2019/07/30 10:11:14 kid1| FATAL: Received Segment Violation...dying.
> 2019/07/30 10:11:14 kid1| Closing HTTP(S) port 0.0.0.0:3126
> 2019/07/30 10:11:14 kid1| Closing HTTP(S) port 0.0.0.0:3127
> 
> 
> t at 1 (l at 1) terminated by signal ABRT (Abort)
> 0xfffffd7ffe9536ea: __lwp_kill+0x000a:  jae      __lwp_kill+0x18        [
> 0xfffffd7ffe9536f8, .+0xe ]
> (dbx) where                                                                  
> current thread: t at 1
> =>[1] __lwp_kill(0x1, 0x6, 0xffffffff86785800, 0xfffffd7ffe953fde, 0x0,
> 0xfffffd7ffe9b3c80), at 0xfffffd7ffe9536ea 
>   [2] _thr_kill(), at 0xfffffd7ffe94bfe3 
>   [3] raise(), at 0xfffffd7ffe8f8229 
>   [4] abort(), at 0xfffffd7ffe8d6bc0 
>   [5] death(), at 0x700068 
>   [6] __sighndlr(), at 0xfffffd7ffe94dee6 
>   [7] call_user_handler(), at 0xfffffd7ffe9427a2 
>   [8] sigacthandler(), at 0xfffffd7ffe9429ce 
>   ---- called from signal handler with signal 11 (SIGSEGV) ------
>   [9] 0x9514b6(0x2b6, 0xfffffd7fff037c30, 0xfffffd7ffe805ea8, 0x7, 0xa,
> 0xfffffd7fffdfe8d8), at 0x9514b6 
>   [10] asn1_enc_save(), at 0xfffffd7ffeee949a 
>   [11] 0xfffffd7ffeee42ea(), at 0xfffffd7ffeee42ea 
>   [12] 0xfffffd7ffeee4e7d(), at 0xfffffd7ffeee4e7d 
>   [13] 0xfffffd7ffeee4b01(), at 0xfffffd7ffeee4b01 
>   [14] 0xfffffd7ffeee41ac(), at 0xfffffd7ffeee41ac 
>   [15] ASN1_item_ex_d2i(), at 0xfffffd7ffeee49b8 
>   [16] ASN1_item_d2i(), at 0xfffffd7ffeee3e73 
>   [17] d2i_X509(), at 0xfffffd7ffeeecd90 
>   [18] PEM_X509_INFO_read_bio(), at 0xfffffd7ffef870df 
>   [19] X509_load_cert_crl_file(), at 0xfffffd7ffefc206f 
>   [20] 0xfffffd7ffefc1bdc(), at 0xfffffd7ffefc1bdc 
>   [21] X509_LOOKUP_ctrl(), at 0xfffffd7ffefc3a9c 
>   [22] X509_STORE_load_locations(), at 0xfffffd7ffefc34fb 
>   [23] SSL_CTX_load_verify_locations(), at 0xfffffd7fff12294d 
>   [24] Security::PeerOptions::updateContextCa(), at 0x83f18c 
>   [25] Security::ServerOptions::updateContextConfig(), at 0x844851 
>   [26] Ssl::createSSLContext(), at 0x85d1eb 
>   [27] ConnStateData::startPeekAndSplice(), at 0x5ef872 
>   [28] httpsSslBumpStep2AccessCheckDone(), at 0x5efe1d 
>   [29] ACLChecklist::checkCallback(), at 0x75deba 
>   [30] ConnStateData::startPeekAndSplice(), at 0x5ef832 
>   [31] ConnStateData::parseTlsHandshake(), at 0x5f08f8 
>   [32] ConnStateData::afterClientRead(), at 0x5f0a9d 
>   [33] Server::doClientRead(), at 0x7a85dc 
>   [34] JobDialer<Server>::dial(), at 0x7a8e5a 
>   [35] AsyncCall::make(), at 0x7bea19 
>   [36] AsyncCallQueue::fireNext(), at 0x7bfdd3 
>   [37] AsyncCallQueue::fire(), at 0x7c0158 
>   [38] EventLoop::runOnce(), at 0x632272 
>   [39] EventLoop::run(), at 0x632378 
>   [40] SquidMain(), at 0x6992b3 
>   [41] main(), at 0x959260 
> (dbx)
> 



From rousskov at measurement-factory.com  Thu Aug  1 16:01:27 2019
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Thu, 1 Aug 2019 12:01:27 -0400
Subject: [squid-users] logformat for squid5 ?
In-Reply-To: <881D9FBD-2656-45C1-AC81-6294016FE4EE@netstream.ps>
References: <EE6D20F3-1CB9-46BA-8B44-B6E4690DB491@netstream.ps>
 <1C2E468A-6E27-4D13-B7C0-E55B01C45424@netstream.ps>
 <c54548eb-b291-17d2-412b-27db051d31d4@measurement-factory.com>
 <881D9FBD-2656-45C1-AC81-6294016FE4EE@netstream.ps>
Message-ID: <4fe560db-f4c4-657e-e39e-8d362e30ae71@measurement-factory.com>

On 8/1/19 11:40 AM, --Ahmad-- wrote:

> logformat squid %tl %6tr %>a %>p %>la %>lp %Ss/%03Hs %<st %rm %ru %un
> %Sh/ %<A %<a %<la

I suspect your Squid v5 is not using your custom logformat. Squid is
using the default logformat. IIRC, Squid v5 warns if you are trying to
redefine the default logformat called "squid" (which is not supported)
instead of adding and using your own. Check Squid output and cache.log
for WARNING and ERROR messages.

Redefining the default logformat may have "worked" in v3, but you should
not do that (in any Squid version). Instead, define and use your own
logformat. For example:

  logformat myAccessLogFormat %tl %6tr ...
  access_log ... logformat=myAccessLogFormat


HTH,

Alex.


From m.hoffmann.bs at googlemail.com  Fri Aug  2 18:08:50 2019
From: m.hoffmann.bs at googlemail.com (Martin Hoffmann)
Date: Fri, 2 Aug 2019 13:08:50 -0500 (CDT)
Subject: [squid-users] sending certificate chain from squid reverse proxy
In-Reply-To: <58dfb81a-18cd-edad-224c-7bc41d19c265@treenet.co.nz>
References: <20190716123412.5mimbz5vozdnjcik@apple.rat.burntout.org>
 <58dfb81a-18cd-edad-224c-7bc41d19c265@treenet.co.nz>
Message-ID: <1564769330895-0.post@n4.nabble.com>

Any ETA on this?

This would really be a nice feature, since on debian/ubuntu squid comes with
GnuTLS support, while OpenSSL Support means recompiling from source. So for
me, it seems, intermediate certificates is the last thing missing to be able
to use squid/GnuTLS instead of squid/OpenSSL.



--
Sent from: http://squid-web-proxy-cache.1019090.n4.nabble.com/Squid-Users-f1019091.html


From mbile20 at gmail.com  Sat Aug  3 09:45:14 2019
From: mbile20 at gmail.com (Mohamed Ali Ahmed)
Date: Sat, 3 Aug 2019 12:45:14 +0300
Subject: [squid-users] Squid 3.5.27 not caching at all
Message-ID: <CAK4=zavgzFG=yMv=+SCid3FB5VnNfXE5xYs5361r8PeE-c=y3Q@mail.gmail.com>

Hello everyone,
I have set up Squid 3.5.27 on ubuntu 18.04 from the packages. I have made
the minimal change but when i check the access.log i get tcp_miss 200 most
of the websites even visiting the same website over and over again.

I have searched online and i did not come across any which solves my
problem.

this is squid.conf file:

root at squid:/etc/squid# sed 's/#.*//;s/^\s*$//;/^$/d' /etc/squid/squid.conf
 acl NOC src 154.73.26.5
acl SSL_ports port 443
acl Safe_ports port 80
acl Safe_ports port 21
acl Safe_ports port 443
acl Safe_ports port 70
acl Safe_ports port 210
acl Safe_ports port 1025-65535
acl Safe_ports port 280
acl Safe_ports port 488
acl Safe_ports port 591
acl Safe_ports port 777
acl CONNECT method CONNECT
http_access deny !Safe_ports
http_access deny CONNECT !SSL_ports
http_access allow localhost manager
http_access deny manager
http_access allow localhost NOC
http_access allow all
http_port 3128
cache_dir ufs /var/spool/squid 100 16 256
coredump_dir /var/spool/squid
refresh_pattern -i \.(gif|png|jpg|jpeg|ico)$ 3600       90%     43200
refresh_pattern ^ftp:           1440    20%     10080
refresh_pattern ^gopher:        1440    0%      1440
refresh_pattern -i (/cgi-bin/|\?) 0     0%      0
refresh_pattern (Release|Packages(.gz)*)$      0       20%     2880
refresh_pattern .               0       20%     4320
root at squid:/etc/squid#

here's access.log file:

1564825166.134    426 154.73.26.5 TCP_MISS/200 10644 GET
http://somaliren.org/wp-content/plugins/revslider/public/assets/js/extensions/revolution.extension.layeranimation.min.js
- HIER_DIRECT/192.185.31.191 application/javascript
1564825166.142    436 154.73.26.5 TCP_MISS/200 8613 GET
http://somaliren.org/wp-content/plugins/revslider/public/assets/js/extensions/revolution.extension.slideanims.min.js
- HIER_DIRECT/192.185.31.191 application/javascript
1564825166.150    440 154.73.26.5 TCP_MISS/200 8771 GET
http://somaliren.org/wp-content/plugins/revslider/public/assets/js/extensions/revolution.extension.navigation.min.js
- HIER_DIRECT/192.185.31.191 application/javascript
1564825166.156    450 154.73.26.5 TCP_MISS/200 2892 GET
http://somaliren.org/wp-content/plugins/revslider/public/assets/js/extensions/revolution.extension.actions.min.js
- HIER_DIRECT/192.185.31.191 application/javascript
1564825166.804    454 154.73.26.5 TCP_MISS/200 7970 GET
http://somaliren.org/wp-content/plugins/revslider/public/assets/fonts/revicons/revicons.woff?
- HIER_DIRECT/192.185.31.191 font/woff
1564825166.821    437 154.73.26.5 TCP_MISS/200 2982 GET
http://somaliren.org/wp-content/plugins/revslider/public/assets/assets/loader.gif
- HIER_DIRECT/192.185.31.191 image/gif


Here's cach.log file:

root at squid:/etc/squid# tail /var/log/squid/cache.log
2019/08/03 12:05:00 kid1|         0 Objects expired.
2019/08/03 12:05:00 kid1|         0 Objects cancelled.
2019/08/03 12:05:00 kid1|         0 Duplicate URLs purged.
2019/08/03 12:05:00 kid1|         0 Swapfile clashes avoided.
2019/08/03 12:05:00 kid1|   Took 0.10 seconds (  0.00 objects/sec).
2019/08/03 12:05:00 kid1| Beginning Validation Procedure
2019/08/03 12:05:00 kid1|   Completed Validation Procedure
2019/08/03 12:05:00 kid1|   Validated 0 Entries
2019/08/03 12:05:00 kid1|   store_swap_size = 0.00 KB
2019/08/03 12:05:01 kid1| storeLateRelease: released 0 objects

Any help would be appreciated.

thanks,
Mohamed
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20190803/28c6e860/attachment.htm>

From squid3 at treenet.co.nz  Sat Aug  3 10:36:28 2019
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sat, 3 Aug 2019 22:36:28 +1200
Subject: [squid-users] Squid 3.5.27 not caching at all
In-Reply-To: <CAK4=zavgzFG=yMv=+SCid3FB5VnNfXE5xYs5361r8PeE-c=y3Q@mail.gmail.com>
References: <CAK4=zavgzFG=yMv=+SCid3FB5VnNfXE5xYs5361r8PeE-c=y3Q@mail.gmail.com>
Message-ID: <1294aefe-7fb1-e096-3adb-511ccdc6f9be@treenet.co.nz>

On 3/08/19 9:45 pm, Mohamed Ali Ahmed wrote:
> Hello everyone,
> I have set up Squid 3.5.27 on ubuntu 18.04 from the packages. I have
> made the minimal change but when i check the access.log i get tcp_miss
> 200 most of the websites even visiting the same website over and over again.
> 

You have not provided enough information for us to know whether this is
a problem or testing mistake.

What is this unstated "minimal change" you made that made caching
suddenly stop working?

What exactly does "visiting the same website over and over again" mean?
what exact actions did you do?
  - Pressing refresh, Shift+reload, or CTL+refresh in a browser will
*force* a MISS - yet comes under the description you gave.
  - some tools send headers to forbid cached contents being delivered by
default. So cannot be used for this kind of test without special options.


The more specific details you can give, the better anyone help can be.


> I have searched online and i did not come across any which solves my
> problem.?
> 
> this is squid.conf file:
> 
> root at squid:/etc/squid# sed 's/#.*//;s/^\s*$//;/^$/d' /etc/squid/squid.conf
> ?acl NOC src 154.73.26.5
> acl SSL_ports port 443
> acl Safe_ports port 80
> acl Safe_ports port 21
> acl Safe_ports port 443
> acl Safe_ports port 70
> acl Safe_ports port 210
> acl Safe_ports port 1025-65535
> acl Safe_ports port 280
> acl Safe_ports port 488
> acl Safe_ports port 591
> acl Safe_ports port 777
> acl CONNECT method CONNECT
> http_access deny !Safe_ports
> http_access deny CONNECT !SSL_ports
> http_access allow localhost manager
> http_access deny manager
> http_access allow localhost NOC
> http_access allow all

Please do not do that "allow all".

For one thing it makes your NOC ACL pointless, *everybody* is allowed to
use your proxy for whatever they want to do.

If you did it to debug a caching problem, be aware that http_access has
nothing to do with cache.


Amos


From mbile20 at gmail.com  Sat Aug  3 13:18:50 2019
From: mbile20 at gmail.com (Mohamed Ali Ahmed)
Date: Sat, 3 Aug 2019 16:18:50 +0300
Subject: [squid-users] Squid 3.5.27 not caching at all
In-Reply-To: <mailman.1.1564833602.10441.squid-users@lists.squid-cache.org>
References: <mailman.1.1564833602.10441.squid-users@lists.squid-cache.org>
Message-ID: <CAK4=zaui-Uun8a2RDrqapBcN45AeKb4c-MROk0RGwCu0FZAagA@mail.gmail.com>

>
> You have not provided enough information for us to know whether this is
> a problem or testing mistake.
>
> What is this unstated "minimal change" you made that made caching
> suddenly stop working?

 The only change i have made is adding NOC ACL and uncommenting the
cache_dir ufs.

What exactly does "visiting the same website over and over again" mean?
> what exact actions did you do?
>
I expected when i visit a website for the time it will miss and after
retrieving it that squid will cache it internally. if i visit the same
website again, squid to read it from the local cache. i was visiting by
adding a new tap using chrome or visiting from another browser like MS edge
or IE.

Please do not do that "allow all".
>
> For one thing it makes your NOC ACL pointless, *everybody* is allowed to
> use your proxy for whatever they want to do.
>
> If you did it to debug a caching problem, be aware that http_access has
> nothing to do with cache.

Thank you amos, I have changed to  http_access deny all   instead of
http_access allow all
 Please ask me anything else i should share with so that you can help me.
thank you.

>
>

 - Pressing refresh, Shift+reload, or CTL+refresh in a browser will
> *force* a MISS - yet comes under the description you gave.
>   - some tools send headers to forbid cached contents being delivered by
> default. So cannot be used for this kind of test without special options.
>
So what do you recommend i do in order to check if squid is caching or not?



On Sat, Aug 3, 2019 at 3:02 PM <squid-users-request at lists.squid-cache.org>
wrote:

> Send squid-users mailing list submissions to
>         squid-users at lists.squid-cache.org
>
> To subscribe or unsubscribe via the World Wide Web, visit
>         http://lists.squid-cache.org/listinfo/squid-users
> or, via email, send a message with subject or body 'help' to
>         squid-users-request at lists.squid-cache.org
>
> You can reach the person managing the list at
>         squid-users-owner at lists.squid-cache.org
>
> When replying, please edit your Subject line so it is more specific
> than "Re: Contents of squid-users digest..."
>
>
> Today's Topics:
>
>    1. Re: sending certificate chain from squid reverse proxy
>       (Martin Hoffmann)
>    2. Squid 3.5.27 not caching at all (Mohamed Ali Ahmed)
>    3. Re: Squid 3.5.27 not caching at all (Amos Jeffries)
>
>
> ----------------------------------------------------------------------
>
> Message: 1
> Date: Fri, 2 Aug 2019 13:08:50 -0500 (CDT)
> From: Martin Hoffmann <m.hoffmann.bs at googlemail.com>
> To: squid-users at lists.squid-cache.org
> Subject: Re: [squid-users] sending certificate chain from squid
>         reverse proxy
> Message-ID: <1564769330895-0.post at n4.nabble.com>
> Content-Type: text/plain; charset=us-ascii
>
> Any ETA on this?
>
> This would really be a nice feature, since on debian/ubuntu squid comes
> with
> GnuTLS support, while OpenSSL Support means recompiling from source. So for
> me, it seems, intermediate certificates is the last thing missing to be
> able
> to use squid/GnuTLS instead of squid/OpenSSL.
>
>
>
> --
> Sent from:
> http://squid-web-proxy-cache.1019090.n4.nabble.com/Squid-Users-f1019091.html
>
>
> ------------------------------
>
> Message: 2
> Date: Sat, 3 Aug 2019 12:45:14 +0300
> From: Mohamed Ali Ahmed <mbile20 at gmail.com>
> To: squid-users at lists.squid-cache.org
> Subject: [squid-users] Squid 3.5.27 not caching at all
> Message-ID:
>         <CAK4=zavgzFG=yMv=+SCid3FB5VnNfXE5xYs5361r8PeE-c=
> y3Q at mail.gmail.com>
> Content-Type: text/plain; charset="utf-8"
>
> Hello everyone,
> I have set up Squid 3.5.27 on ubuntu 18.04 from the packages. I have made
> the minimal change but when i check the access.log i get tcp_miss 200 most
> of the websites even visiting the same website over and over again.
>
> I have searched online and i did not come across any which solves my
> problem.
>
> this is squid.conf file:
>
> root at squid:/etc/squid# sed 's/#.*//;s/^\s*$//;/^$/d' /etc/squid/squid.conf
>  acl NOC src 154.73.26.5
> acl SSL_ports port 443
> acl Safe_ports port 80
> acl Safe_ports port 21
> acl Safe_ports port 443
> acl Safe_ports port 70
> acl Safe_ports port 210
> acl Safe_ports port 1025-65535
> acl Safe_ports port 280
> acl Safe_ports port 488
> acl Safe_ports port 591
> acl Safe_ports port 777
> acl CONNECT method CONNECT
> http_access deny !Safe_ports
> http_access deny CONNECT !SSL_ports
> http_access allow localhost manager
> http_access deny manager
> http_access allow localhost NOC
> http_access allow all
> http_port 3128
> cache_dir ufs /var/spool/squid 100 16 256
> coredump_dir /var/spool/squid
> refresh_pattern -i \.(gif|png|jpg|jpeg|ico)$ 3600       90%     43200
> refresh_pattern ^ftp:           1440    20%     10080
> refresh_pattern ^gopher:        1440    0%      1440
> refresh_pattern -i (/cgi-bin/|\?) 0     0%      0
> refresh_pattern (Release|Packages(.gz)*)$      0       20%     2880
> refresh_pattern .               0       20%     4320
> root at squid:/etc/squid#
>
> here's access.log file:
>
> 1564825166.134    426 154.73.26.5 TCP_MISS/200 10644 GET
>
> http://somaliren.org/wp-content/plugins/revslider/public/assets/js/extensions/revolution.extension.layeranimation.min.js
> - HIER_DIRECT/192.185.31.191 application/javascript
> 1564825166.142    436 154.73.26.5 TCP_MISS/200 8613 GET
>
> http://somaliren.org/wp-content/plugins/revslider/public/assets/js/extensions/revolution.extension.slideanims.min.js
> - HIER_DIRECT/192.185.31.191 application/javascript
> 1564825166.150    440 154.73.26.5 TCP_MISS/200 8771 GET
>
> http://somaliren.org/wp-content/plugins/revslider/public/assets/js/extensions/revolution.extension.navigation.min.js
> - HIER_DIRECT/192.185.31.191 application/javascript
> 1564825166.156    450 154.73.26.5 TCP_MISS/200 2892 GET
>
> http://somaliren.org/wp-content/plugins/revslider/public/assets/js/extensions/revolution.extension.actions.min.js
> - HIER_DIRECT/192.185.31.191 application/javascript
> 1564825166.804    454 154.73.26.5 TCP_MISS/200 7970 GET
>
> http://somaliren.org/wp-content/plugins/revslider/public/assets/fonts/revicons/revicons.woff
> ?
> - HIER_DIRECT/192.185.31.191 font/woff
> 1564825166.821    437 154.73.26.5 TCP_MISS/200 2982 GET
>
> http://somaliren.org/wp-content/plugins/revslider/public/assets/assets/loader.gif
> - HIER_DIRECT/192.185.31.191 image/gif
>
>
> Here's cach.log file:
>
> root at squid:/etc/squid# tail /var/log/squid/cache.log
> 2019/08/03 12:05:00 kid1|         0 Objects expired.
> 2019/08/03 12:05:00 kid1|         0 Objects cancelled.
> 2019/08/03 12:05:00 kid1|         0 Duplicate URLs purged.
> 2019/08/03 12:05:00 kid1|         0 Swapfile clashes avoided.
> 2019/08/03 12:05:00 kid1|   Took 0.10 seconds (  0.00 objects/sec).
> 2019/08/03 12:05:00 kid1| Beginning Validation Procedure
> 2019/08/03 12:05:00 kid1|   Completed Validation Procedure
> 2019/08/03 12:05:00 kid1|   Validated 0 Entries
> 2019/08/03 12:05:00 kid1|   store_swap_size = 0.00 KB
> 2019/08/03 12:05:01 kid1| storeLateRelease: released 0 objects
>
> Any help would be appreciated.
>
> thanks,
> Mohamed
> -------------- next part --------------
> An HTML attachment was scrubbed...
> URL: <
> http://lists.squid-cache.org/pipermail/squid-users/attachments/20190803/28c6e860/attachment-0001.html
> >
>
> ------------------------------
>
> Message: 3
> Date: Sat, 3 Aug 2019 22:36:28 +1200
> From: Amos Jeffries <squid3 at treenet.co.nz>
> To: squid-users at lists.squid-cache.org
> Subject: Re: [squid-users] Squid 3.5.27 not caching at all
> Message-ID: <1294aefe-7fb1-e096-3adb-511ccdc6f9be at treenet.co.nz>
> Content-Type: text/plain; charset=utf-8
>
> On 3/08/19 9:45 pm, Mohamed Ali Ahmed wrote:
> > Hello everyone,
> > I have set up Squid 3.5.27 on ubuntu 18.04 from the packages. I have
> > made the minimal change but when i check the access.log i get tcp_miss
> > 200 most of the websites even visiting the same website over and over
> again.
> >
>
> You have not provided enough information for us to know whether this is
> a problem or testing mistake.
>
> What is this unstated "minimal change" you made that made caching
> suddenly stop working?
>
> What exactly does "visiting the same website over and over again" mean?
> what exact actions did you do?
>   - Pressing refresh, Shift+reload, or CTL+refresh in a browser will
> *force* a MISS - yet comes under the description you gave.
>   - some tools send headers to forbid cached contents being delivered by
> default. So cannot be used for this kind of test without special options.
>
>
> The more specific details you can give, the better anyone help can be.
>
>
> > I have searched online and i did not come across any which solves my
> > problem.
> >
> > this is squid.conf file:
> >
> > root at squid:/etc/squid# sed 's/#.*//;s/^\s*$//;/^$/d'
> /etc/squid/squid.conf
> >  acl NOC src 154.73.26.5
> > acl SSL_ports port 443
> > acl Safe_ports port 80
> > acl Safe_ports port 21
> > acl Safe_ports port 443
> > acl Safe_ports port 70
> > acl Safe_ports port 210
> > acl Safe_ports port 1025-65535
> > acl Safe_ports port 280
> > acl Safe_ports port 488
> > acl Safe_ports port 591
> > acl Safe_ports port 777
> > acl CONNECT method CONNECT
> > http_access deny !Safe_ports
> > http_access deny CONNECT !SSL_ports
> > http_access allow localhost manager
> > http_access deny manager
> > http_access allow localhost NOC
> > http_access allow all
>
> Please do not do that "allow all".
>
> For one thing it makes your NOC ACL pointless, *everybody* is allowed to
> use your proxy for whatever they want to do.
>
> If you did it to debug a caching problem, be aware that http_access has
> nothing to do with cache.
>
>
> Amos
>
>
> ------------------------------
>
> Subject: Digest Footer
>
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>
>
> ------------------------------
>
> End of squid-users Digest, Vol 60, Issue 4
> ******************************************
>


-- 


Mohamed Ali Ahmed (Bile)
Head of Systems
Somali Research & Education Network (SomaliREN)
+252 615 567671
Skype: bilesuper
Web:   http://www.somaliren.org
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20190803/c29f759a/attachment.htm>

From emz at norma.perm.ru  Sat Aug  3 14:11:21 2019
From: emz at norma.perm.ru (Eugene M. Zheganin)
Date: Sat, 3 Aug 2019 19:11:21 +0500
Subject: [squid-users] cache-peer and tls
Message-ID: <2e7669f1-4cdf-72bf-a87c-556e8d452730@norma.perm.ru>

Hello,


I'm using squid 4.6 and I need to TLS-encrypt the session to the parent 
proxy. I have in config:


cache_peer proxy.foo.bar parent 3129 3130 tls 
tls-cafile=/usr/local/etc/squid/certs/le.pem 
sslcert=/usr/local/etc/letsencrypt/live/vpn.enazadev.ru/cert.pem 
sslkey=/usr/local/etc/letsencrypt/live/vpn.enazadev.ru/privkey.pem 
sslflags=DONT_VERIFY_DOMAIN,DONT_VERIFY_PEER


But no matter what I'm doing, squid keeps telling in logs that he 
doesn't like the peer certificate:


2019/08/03 18:42:24 kid1| ERROR: negotiating TLS on FD 23: 
error:14090086:SSL routines:ssl3_get_server_certificate:certificate 
verify failed (1/-1/0)
2019/08/03 18:42:24 kid1| temporary disabling (Service Unavailable) 
digest from proxy.foo.bar

and then he's going directly bypassing the peer. :/


Is there any way to tell him that I don't care ?

I've also tried to actually tell him about the CA cert with 
tls-cafile=/usr/local/etc/squid/certs/le.pem above, this doesn't work 
either.


Thanks.

Eugene.



From squid3 at treenet.co.nz  Sun Aug  4 02:26:37 2019
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sun, 4 Aug 2019 14:26:37 +1200
Subject: [squid-users] Squid 3.5.27 not caching at all
In-Reply-To: <CAK4=zaui-Uun8a2RDrqapBcN45AeKb4c-MROk0RGwCu0FZAagA@mail.gmail.com>
References: <mailman.1.1564833602.10441.squid-users@lists.squid-cache.org>
 <CAK4=zaui-Uun8a2RDrqapBcN45AeKb4c-MROk0RGwCu0FZAagA@mail.gmail.com>
Message-ID: <552381fb-6ca5-ca44-e416-a7075c22bfe8@treenet.co.nz>

On 4/08/19 1:18 am, Mohamed Ali Ahmed wrote:
>     You have not provided enough information for us to know whether this is
>     a problem or testing mistake.
> 
>     What is this unstated "minimal change" you made that made caching
>     suddenly stop working??
> 
> ?The only change i have made is adding NOC ACL and uncommenting the
> cache_dir ufs.
> 

Okay.

>     What exactly does "visiting the same website over and over again" mean?
>     what exact actions did you do?
> 
> I expected when i visit a website for the time it will miss and after
> retrieving it that squid will cache it internally. if i visit the same
> website again, squid to read it from the local cache. i was visiting by
> adding a new tap using chrome or visiting from another browser like MS
> edge or IE.?
> 

Ah. This website uses "Vary: Accept-Encoding" and each web browser
supports different types of encoding, or has different preference order
for the ones they do share.

So the first request from each browser should be expected to be a MISS.
Only the second use by that particular browser can be expected to HIT or
REFRESH. I'm not sure if Chrome new-tab does the same, it should not but
maybe.
You can add "debug_options 11,2" to get a cache.log trace of the headers
from each test and see if they should have been a MISS, HIT or REFRESH.



>     Please do not do that "allow all".
> 
>     For one thing it makes your NOC ACL pointless, *everybody* is allowed to
>     use your proxy for whatever they want to do.
> 
>     If you did it to debug a caching problem, be aware that http_access has
>     nothing to do with cache.?
> 
> Thank you amos, I have changed to? http_access deny all???instead of
> http_access allow all
> ?Please ask me anything else i should share with so that you can help
> me. thank you.?
> 

The 11,2 debug trace log would help. You may be able to find the problem
yourself from that.


> 
>     ?- Pressing refresh, Shift+reload, or CTL+refresh in a browser will
>     *force* a MISS - yet comes under the description you gave.
>     ? - some tools send headers to forbid cached contents being delivered by
>     default. So cannot be used for this kind of test without special
>     options.??
> 
> So what do you recommend i do in order to check if squid is caching or not?
> ?

If you need to use a browser, then clicking on the address bar and
pressing enter re-fetches that URL without any cache affecting requirements.

Or curl, wget or squidclient tools can be used to test a single URL fetch.

Amos


From squid3 at treenet.co.nz  Sun Aug  4 03:00:52 2019
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sun, 4 Aug 2019 15:00:52 +1200
Subject: [squid-users] cache-peer and tls
In-Reply-To: <2e7669f1-4cdf-72bf-a87c-556e8d452730@norma.perm.ru>
References: <2e7669f1-4cdf-72bf-a87c-556e8d452730@norma.perm.ru>
Message-ID: <53442a63-6456-fe18-2801-4e3e999d8c11@treenet.co.nz>

On 4/08/19 2:11 am, Eugene M. Zheganin wrote:
> Hello,
> 
> 
> I'm using squid 4.6 and I need to TLS-encrypt the session to the parent
> proxy. I have in config:
> 
> 
> cache_peer proxy.foo.bar parent 3129 3130 tls
> tls-cafile=/usr/local/etc/squid/certs/le.pem
> sslcert=/usr/local/etc/letsencrypt/live/vpn.enazadev.ru/cert.pem
> sslkey=/usr/local/etc/letsencrypt/live/vpn.enazadev.ru/privkey.pem
> sslflags=DONT_VERIFY_DOMAIN,DONT_VERIFY_PEER
> 

Please start with "squid -k parse" and update those to the Squid-4 options.

Also, any errors/warnings mentioned about the PEM files contents need to
be fixed.


> 
> But no matter what I'm doing, squid keeps telling in logs that he
> doesn't like the peer certificate:
> 
> 
> 2019/08/03 18:42:24 kid1| ERROR: negotiating TLS on FD 23:
> error:14090086:SSL routines:ssl3_get_server_certificate:certificate
> verify failed (1/-1/0)
> 2019/08/03 18:42:24 kid1| temporary disabling (Service Unavailable)
> digest from proxy.foo.bar
> 
> and then he's going directly bypassing the peer. :/
> 
> 
> Is there any way to tell him that I don't care ?
> 

You really should care. There is no point in TLS to a peer if you are
going to ignore whether the right peer is even being connected to.


Amos


From tarotapprentice at yahoo.com  Sun Aug  4 03:01:21 2019
From: tarotapprentice at yahoo.com (TarotApprentice)
Date: Sun, 4 Aug 2019 13:01:21 +1000
Subject: [squid-users] caching apt package lists/Raspbian
In-Reply-To: <488909992.2609923.1563692884646@mail.yahoo.com>
References: <1454313392.2447317.1563599951391.ref@mail.yahoo.com>
 <1454313392.2447317.1563599951391@mail.yahoo.com>
 <89d5147f-0b4f-c609-6ca2-3837e9ec697b@treenet.co.nz>
 <45004701-F8F9-47F1-9D45-007ECBE81D6A@yahoo.com>
 <cc298ed6-d0ff-ad85-c04a-bd488eb1238a@treenet.co.nz>
 <488909992.2609923.1563692884646@mail.yahoo.com>
Message-ID: <89B96EF2-5A9C-419A-8B4C-82A6F8B42B6B@yahoo.com>

Further to this I did find an issue with the iptables loopback and IPv6 which I corrected.

It still wasn?t caching the packages.xz from either the local machine or others in the local net. I ended up adding a refresh pattern for .gz and .xz which seems to cache them now. I am using 1440 20% 1440 which I thought was fairly conservative.

MarkJ 

> On 21 Jul 2019, at 5:08 pm, TarotApprentice <tarotapprentice at yahoo.com> wrote:
> 
> It whatever Raspbian and the router do by default, although I do use an iptables firewall. I normally don't see any IPv6 from the other Pis, so maybe something to do with localhost and the loopback interface.
> 
> Cheers
> 
> 
> 
> 
> 
> 
> On Sunday, 21 July 2019, 2:45:59 pm AEST, Amos Jeffries <squid3 at treenet.co.nz> wrote: 
> 
> 
> 
> 
> 
>> On 21/07/19 4:20 pm, Mark James wrote:
>> Doing an ?apt update? on the squid machine got another TCP_MISS_ABORTED for ::1 and then subsequent IPv4 requests from other Pis get the TCP_REQUEST_UNMODIFIED.
>> 
> 
> That hints that there is something broken in your local network IPv6
> connectivity. Perhapse ICMPv6 is not working properly?
> 
> 
> Amos
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users



From ahmed.zaeem at netstream.ps  Mon Aug  5 09:33:39 2019
From: ahmed.zaeem at netstream.ps (--Ahmad--)
Date: Mon, 5 Aug 2019 12:33:39 +0300
Subject: [squid-users] outgoing address for DNS queries per instance
Message-ID: <B1D173C1-CEEE-401B-A9B4-4AED1D883D1C@netstream.ps>

Hello folks 
i have 3 squid instances and i want to have different external ip for each squid instance .

i believed acls of dst ip of dns should work .
but i even tested it didnt with the outgoing address .

dns_nameservers 8.8.8.8
acl next dst 8.8.8.8
tcp_outgoing_address 100.100.100.100 next


but the problem when i do debug , the src ip that reach 8.8.8.8 is no 100.100.100.100 and its only the ip address of the main machine .

any help ?

Thanks 



From squid3 at treenet.co.nz  Mon Aug  5 09:57:04 2019
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Mon, 5 Aug 2019 21:57:04 +1200
Subject: [squid-users] outgoing address for DNS queries per instance
In-Reply-To: <B1D173C1-CEEE-401B-A9B4-4AED1D883D1C@netstream.ps>
References: <B1D173C1-CEEE-401B-A9B4-4AED1D883D1C@netstream.ps>
Message-ID: <36097018-32f2-d6fb-aa24-5a51ba2f5548@treenet.co.nz>

On 5/08/19 9:33 pm, --Ahmad-- wrote:
> Hello folks 
> i have 3 squid instances and i want to have different external ip for each squid instance .
> 
> i believed acls of dst ip of dns should work .
> but i even tested it didnt with the outgoing address .
> 
> dns_nameservers 8.8.8.8
> acl next dst 8.8.8.8
> tcp_outgoing_address 100.100.100.100 next
> 
> 
> but the problem when i do debug , the src ip that reach 8.8.8.8 is no 100.100.100.100 and its only the ip address of the main machine .
> 
> any help ?


DNS uses <http://www.squid-cache.org/Doc/config/udp_outgoing_address/>.
Despite the directive name, it is also used for DNS-TCP traffic.

Amos


From ahmed.zaeem at netstream.ps  Mon Aug  5 10:01:13 2019
From: ahmed.zaeem at netstream.ps (--Ahmad--)
Date: Mon, 5 Aug 2019 13:01:13 +0300
Subject: [squid-users] outgoing address for DNS queries per instance
In-Reply-To: <36097018-32f2-d6fb-aa24-5a51ba2f5548@treenet.co.nz>
References: <B1D173C1-CEEE-401B-A9B4-4AED1D883D1C@netstream.ps>
 <36097018-32f2-d6fb-aa24-5a51ba2f5548@treenet.co.nz>
Message-ID: <ECF2804C-522C-4D37-926C-3B3481A22DC6@netstream.ps>

Nice ! 

Thanks Amos ! 

> On 5 Aug 2019, at 12:57, Amos Jeffries <squid3 at treenet.co.nz> wrote:
> 
> On 5/08/19 9:33 pm, --Ahmad-- wrote:
>> Hello folks 
>> i have 3 squid instances and i want to have different external ip for each squid instance .
>> 
>> i believed acls of dst ip of dns should work .
>> but i even tested it didnt with the outgoing address .
>> 
>> dns_nameservers 8.8.8.8
>> acl next dst 8.8.8.8
>> tcp_outgoing_address 100.100.100.100 next
>> 
>> 
>> but the problem when i do debug , the src ip that reach 8.8.8.8 is no 100.100.100.100 and its only the ip address of the main machine .
>> 
>> any help ?
> 
> 
> DNS uses <http://www.squid-cache.org/Doc/config/udp_outgoing_address/ <http://www.squid-cache.org/Doc/config/udp_outgoing_address/>>.
> Despite the directive name, it is also used for DNS-TCP traffic.
> 
> Amos
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org <mailto:squid-users at lists.squid-cache.org>
> http://lists.squid-cache.org/listinfo/squid-users <http://lists.squid-cache.org/listinfo/squid-users>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20190805/7ae4e47a/attachment.htm>

From chirayu.patel at truecomtelesoft.com  Wed Aug  7 06:40:59 2019
From: chirayu.patel at truecomtelesoft.com (Chirayu Patel)
Date: Wed, 7 Aug 2019 12:10:59 +0530
Subject: [squid-users] Squid url rewriters creating issues in case of
	multi-threaded mode
Message-ID: <CAOhxsyziq8VpFafHx86odWmpvsE+p0cKepSvGid-KNVS-R1JcQ@mail.gmail.com>

This is my config :

http_port 3129 intercept
https_port 3131 intercept ssl-bump cert=/etc/ray/certificates/myCA.pem \
    generate-host-certificates=off dynamic_cert_mem_cache_size=2MB
## For Captive Portal
http_port 3132 intercept
https_port 3133 intercept ssl-bump cert=/etc/ray/certificates/myCA.pem \
    generate-host-certificates=off dynamic_cert_mem_cache_size=1MB

#sslcrtd_program /usr/lib/squid/ssl_crtd -s /var/lib/ssl_db -M 4MB
#sslcrtd_children 5

# TLS/SSL bumping definitions
acl tls_s1_connect at_step SslBump1
acl tls_s2_client_hello at_step SslBump2
acl tls_s3_server_hello at_step SslBump3

# TLS/SSL bumping steps
ssl_bump peek tls_s1_connect all # peek at TLS/SSL connect data
ssl_bump splice all # splice: no active bumping
on_unsupported_protocol tunnel all

pinger_enable off
digest_generation off
netdb_filename none
ipcache_size 128
fqdncache_size 128
via off
forwarded_for transparent
httpd_suppress_version_string on
cache deny all
cache_mem 0 MB
memory_pools off
shutdown_lifetime 0 seconds

#logfile_daemon /dev/null
access_log none

#acl good_url dstdomain .yahoo.com
http_access allow all

url_rewrite_program /tmp/squid/urlcat_server
#url_rewrite_bypass on
url_rewrite_children 15 startup=1 idle=1 concurrency=30 queue-size=10000
#url_rewrite_access allow all
#url_rewrite_extras "%>a/%>A %un %>rm bump_mode=%ssl::bump_mode
sni=\"%ssl::>sni\" referer=\"%{Referer}>h\""
url_rewrite_extras "%>a %lp %ssl::>sni"

---------------------
--> I have a single process which receives the requests in multi-threaded
mode

--> The issue I am facing right now is, I am continuously getting these
messages :

Tue Aug  6 15:33:09 2019 daemon.notice squid[7934]: helperHandleRead:
unexpected reply on channel 0 from redirector #Hlpr1 '176 OK'
Tue Aug  6 15:33:09 2019 daemon.notice squid[7934]: helperHandleRead:
unexpected reply on channel 0 from redirector #Hlpr1 '176 OK'
Tue Aug  6 15:33:09 2019 daemon.notice squid[7934]: helperHandleRead:
unexpected reply on channel 0 from redirector #Hlpr1 '178 OK'
Tue Aug  6 15:33:09 2019 daemon.notice squid[7934]: helperHandleRead:
unexpected reply on channel 0 from redirector #Hlpr1 '178 OK'
Tue Aug  6 15:33:09 2019 daemon.notice squid[7934]: helperHandleRead:
unexpected reply on channel 0 from redirector #Hlpr1 '174 OK'
Tue Aug  6 15:33:09 2019 daemon.notice squid[7934]: helperHandleRead:
unexpected reply on channel 0 from redirector #Hlpr1 '180 OK'
Tue Aug  6 15:33:09 2019 daemon.notice squid[7934]: helperHandleRead:
unexpected reply on channel 0 from redirector #Hlpr1 '181 OK'
Tue Aug  6 15:33:11 2019 daemon.notice squid[7934]: helperHandleRead:
unexpected reply on channel 0 from redirector #Hlpr1 '182 OK'
Tue Aug  6 15:33:12 2019 daemon.notice squid[7934]: helperHandleRead:
unexpected reply on channel 0 from redirector #Hlpr1 '183 OK'

--> In single threaded mode, it used to work fine because I used to reply
instantly. Now which multi-threaded mode, I am queuing the requests and
answering them after processing.

--> I am following the convention of concurrency, whereby I am attaching
the id before the response.

--> Does squid not preserve the requests ? I am doubting that when I am
sending the reply, there is no such request existing on squid side and
hence I am getting this reply from squid

Am I missing some config here..
--
Thank You
Chirayu Patel
Truecom Telesoft
+91 8758484287
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20190807/7e437c9d/attachment.htm>

From sebastiaan.wijker at sbdinc.com  Wed Aug  7 09:45:11 2019
From: sebastiaan.wijker at sbdinc.com (gswijker)
Date: Wed, 7 Aug 2019 04:45:11 -0500 (CDT)
Subject: [squid-users] VoIP Software trouble
In-Reply-To: <e70e5122-dd08-4a3b-a775-275b8f7804c8@measurement-factory.com>
References: <1564048816044-0.post@n4.nabble.com>
 <b8ba9feb-58c5-5c22-516d-5df668b140a8@measurement-factory.com>
 <1564386980689-0.post@n4.nabble.com>
 <e70e5122-dd08-4a3b-a775-275b8f7804c8@measurement-factory.com>
Message-ID: <1565171111207-0.post@n4.nabble.com>

Hello Alex,

After I changed the IP of the proxy, the software starts to worked.
The IP address was blocked by mtel.eu.

Thanks for your help.

Best regards,
Sebastiaan



--
Sent from: http://squid-web-proxy-cache.1019090.n4.nabble.com/Squid-Users-f1019091.html


From squid3 at treenet.co.nz  Wed Aug  7 11:45:12 2019
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Wed, 7 Aug 2019 23:45:12 +1200
Subject: [squid-users] Squid url rewriters creating issues in case of
 multi-threaded mode
In-Reply-To: <CAOhxsyziq8VpFafHx86odWmpvsE+p0cKepSvGid-KNVS-R1JcQ@mail.gmail.com>
References: <CAOhxsyziq8VpFafHx86odWmpvsE+p0cKepSvGid-KNVS-R1JcQ@mail.gmail.com>
Message-ID: <952a6077-7d82-3f2c-b27a-2eecbef5c3f9@treenet.co.nz>

On 7/08/19 6:40 pm, Chirayu Patel wrote:
> 
> url_rewrite_program /tmp/squid/urlcat_server
> url_rewrite_children 15 startup=1 idle=1 concurrency=30 queue-size=10000
> url_rewrite_extras "%>a %lp %ssl::>sni"
> 
> ---------------------
> --> I have a single process which receives the requests in
> multi-threaded mode
> 
> --> The issue I am facing right now is, I am continuously getting these
> messages :
> 
> Tue Aug??6 15:33:09 2019 daemon.notice squid[7934]: helperHandleRead:
> unexpected reply on channel 0 from redirector #Hlpr1 '176 OK'


It looks like your helper is trying to indicate channel-ID 176 but
sending something that is not a digit before the ID number.


Can you run the helper manually and send it one or a bunch of requests
to see if it is producing the correct output?

Is there any whitespace or prefix before the channel-ID it is putting on
the line?  The above log entry looks like the helper is producing
something like these:
 "0 178 OK"
 " 178 OK"

Amos


From service.mv at gmail.com  Thu Aug  8 13:44:38 2019
From: service.mv at gmail.com (neok)
Date: Thu, 8 Aug 2019 08:44:38 -0500 (CDT)
Subject: [squid-users] HAProxy + Squid
In-Reply-To: <AM0PR04MB47533E0476C95B3A45900E3D8FC40@AM0PR04MB4753.eurprd04.prod.outlook.com>
References: <CA+d==oF7y8-_gps4jNo1YV1KnWotTK6brmZWKXztCcJUnSF04g@mail.gmail.com>
 <AM0PR04MB47533E0476C95B3A45900E3D8FC40@AM0PR04MB4753.eurprd04.prod.outlook.com>
Message-ID: <1565271878333-0.post@n4.nabble.com>

Hi, I finally did the configuration differently. It's working very well for
me.

In squid.conf configuring:
acl haproxy src x.x.x.x # HAProxy Load Balancer IP
follow_x_forwarded_for allow haproxy

In haproxy.cfg configuring:
defaults
	global log
	mode http
	option httplog
	option dontlognull
        timeout connect 5000
        timeout client 50000
        timeout server 50000
	errorfile 400 /etc/haproxy/errors/400.http
	errorfile 403 /etc/haproxy/errors/403.http
	errorfile 408 /etc/haproxy/errors/408.http
	errorfile 500 /etc/haproxy/errors/500.http
	errorfile 502 /etc/haproxy/errors/502.http
	errorfile 503 /etc/haproxy/errors/503.http
	errorfile 504 /etc/haproxy/errors/504.http

frontend squid
	bind *:3128
	default_backend squid_pool

backend squid_pool
	mode http
	SERVERID cookie insert indirect nocache
	balance source
	hash-type consistent
	option httpclose
	option forwardfor header X-Client
	option forwardfor
	server px1 x.x.x.1:3128 check inter 2000 rise 2 fall 3
	server px2 x.x.x.2:3128 check inter 2000 rise 2 fall 3

Greetings!

Gabriel



--
Sent from: http://squid-web-proxy-cache.1019090.n4.nabble.com/Squid-Users-f1019091.html


From service.mv at gmail.com  Thu Aug  8 13:46:10 2019
From: service.mv at gmail.com (neok)
Date: Thu, 8 Aug 2019 08:46:10 -0500 (CDT)
Subject: [squid-users] HAProxy + Squid
In-Reply-To: <AM0PR04MB47533E0476C95B3A45900E3D8FC40@AM0PR04MB4753.eurprd04.prod.outlook.com>
References: <CA+d==oF7y8-_gps4jNo1YV1KnWotTK6brmZWKXztCcJUnSF04g@mail.gmail.com>
 <AM0PR04MB47533E0476C95B3A45900E3D8FC40@AM0PR04MB4753.eurprd04.prod.outlook.com>
Message-ID: <1565271970049-0.post@n4.nabble.com>

Rafael Akchurin wrote
> Hello Gabriel,
> 
> We do exactly that in our lab, see docs at
> https://docs.diladele.com/administrator_guide_7_0/active_directory_extra/redundancy/haproxy_proxy_protocol.html
> It works perfectly.
> 
> Best regards,
> Rafael Akchurin
> Diladele B.V.
> 
> 
> 
> From: squid-users [mailto:

> squid-users-bounces at .squid-cache

> ] On Behalf Of Service MV
> Sent: Monday, July 22, 2019 4:37 PM
> To: 

> squid-users at .squid-cache

> Subject: [squid-users] HAProxy + Squid
> 
> Hello everyone, I would like to know if the configuration I want to do is
> viable:
> 1 Load Balancer HAProxy configured in TCP mode.
> 2 Squid servers 4.7.2 with negotiate kerberos authentication and LDAP
> group authorizations.
> The idea is that the web clients of my lan point to the IP/Name of the
> Load Balancer and that this distributes the load between the proxy
> servers.
> Attached is a link to a configuration diagram.
> https://cloudcraft.co/view/00ccd7cb-861c-4e70-a38e-980fdd6cfad3?key=iEa-Gyp8R0ZSh-fxDNi58A
> Thank you very much in advance for your comments.
> Best regards
> 
> Gabriel
> 
> 
> _______________________________________________
> squid-users mailing list

> squid-users at .squid-cache

> http://lists.squid-cache.org/listinfo/squid-users

Hi, I finally did the configuration differently. It's working very well for
me.

In squid.conf configuring:
acl haproxy src x.x.x.x # HAProxy Load Balancer IP
follow_x_forwarded_for allow haproxy

In haproxy.cfg configuring:
defaults
        global log
        mode http
        option httplog
        option dontlognull
        timeout connect 5000
        timeout client 50000
        timeout server 50000
        errorfile 400 /etc/haproxy/errors/400.http
        errorfile 403 /etc/haproxy/errors/403.http
        errorfile 408 /etc/haproxy/errors/408.http
        errorfile 500 /etc/haproxy/errors/500.http
        errorfile 502 /etc/haproxy/errors/502.http
        errorfile 503 /etc/haproxy/errors/503.http
        errorfile 504 /etc/haproxy/errors/504.http

frontend squid
        bind *:3128
        default_backend squid_pool

backend squid_pool
        mode http
        SERVERID cookie insert indirect nocache
        balance source
        hash-type consistent
        option httpclose
        option forwardfor header X-Client
        option forwardfor
        server px1 x.x.x.1:3128 check inter 2000 rise 2 fall 3
        server px2 x.x.x.2:3128 check inter 2000 rise 2 fall 3

Greetings!

Gabriel 



--
Sent from: http://squid-web-proxy-cache.1019090.n4.nabble.com/Squid-Users-f1019091.html


From service.mv at gmail.com  Thu Aug  8 13:47:56 2019
From: service.mv at gmail.com (neok)
Date: Thu, 8 Aug 2019 08:47:56 -0500 (CDT)
Subject: [squid-users] HAProxy + Squid
In-Reply-To: <AM0PR04MB47533E0476C95B3A45900E3D8FC40@AM0PR04MB4753.eurprd04.prod.outlook.com>
References: <CA+d==oF7y8-_gps4jNo1YV1KnWotTK6brmZWKXztCcJUnSF04g@mail.gmail.com>
 <AM0PR04MB47533E0476C95B3A45900E3D8FC40@AM0PR04MB4753.eurprd04.prod.outlook.com>
Message-ID: <1565272076348-0.post@n4.nabble.com>

Hi, I finally did the configuration differently. It's working very well for
me.

In squid.conf configuring:
acl haproxy src x.x.x.x # HAProxy Load Balancer IP
follow_x_forwarded_for allow haproxy

In haproxy.cfg configuring:
defaults
        global log
        mode http
        option httplog
        option dontlognull
        timeout connect 5000
        timeout client 50000
        timeout server 50000
        errorfile 400 /etc/haproxy/errors/400.http
        errorfile 403 /etc/haproxy/errors/403.http
        errorfile 408 /etc/haproxy/errors/408.http
        errorfile 500 /etc/haproxy/errors/500.http
        errorfile 502 /etc/haproxy/errors/502.http
        errorfile 503 /etc/haproxy/errors/503.http
        errorfile 504 /etc/haproxy/errors/504.http

frontend squid
        bind *:3128
        default_backend squid_pool

backend squid_pool
        mode http
        SERVERID cookie insert indirect nocache
        balance source
        hash-type consistent
        option httpclose
        option forwardfor header X-Client
        option forwardfor
        server px1 x.x.x.1:3128 check inter 2000 rise 2 fall 3
        server px2 x.x.x.2:3128 check inter 2000 rise 2 fall 3

Greetings!

Gabriel 


Rafael Akchurin wrote
> Hello Gabriel,
> 
> We do exactly that in our lab, see docs at
> https://docs.diladele.com/administrator_guide_7_0/active_directory_extra/redundancy/haproxy_proxy_protocol.html
> It works perfectly.
> 
> Best regards,
> Rafael Akchurin
> Diladele B.V.
> 
> 
> 
> From: squid-users [mailto:

> squid-users-bounces at .squid-cache

> ] On Behalf Of Service MV
> Sent: Monday, July 22, 2019 4:37 PM
> To: 

> squid-users at .squid-cache

> Subject: [squid-users] HAProxy + Squid
> 
> Hello everyone, I would like to know if the configuration I want to do is
> viable:
> 1 Load Balancer HAProxy configured in TCP mode.
> 2 Squid servers 4.7.2 with negotiate kerberos authentication and LDAP
> group authorizations.
> The idea is that the web clients of my lan point to the IP/Name of the
> Load Balancer and that this distributes the load between the proxy
> servers.
> Attached is a link to a configuration diagram.
> https://cloudcraft.co/view/00ccd7cb-861c-4e70-a38e-980fdd6cfad3?key=iEa-Gyp8R0ZSh-fxDNi58A
> Thank you very much in advance for your comments.
> Best regards
> 
> Gabriel
> 
> 
> _______________________________________________
> squid-users mailing list

> squid-users at .squid-cache

> http://lists.squid-cache.org/listinfo/squid-users





--
Sent from: http://squid-web-proxy-cache.1019090.n4.nabble.com/Squid-Users-f1019091.html


From service.mv at gmail.com  Thu Aug  8 13:57:50 2019
From: service.mv at gmail.com (Service MV)
Date: Thu, 8 Aug 2019 10:57:50 -0300
Subject: [squid-users] acl src question
Message-ID: <CA+d==oEDYOBWrO1FBY2M34ivsFw4gApUB=_NCu8_O6N8g8tTCQ@mail.gmail.com>

Hello everyone!

I have a network 192.168.10.0/22
I want to let the IP ranges 192.168.12.1 to 192.168.13.254 through my
proxy, but not the ranges 192.168.10.1 to 192.168.11.254.
If I don't misunderstand the documentation
<http://www.squid-cache.org/Versions/v4/cfgman/acl.html>, the correct way
to do this would be:
acl mylocalnet src 192.168.12.0/24
acl mylocalnet src 192.168.13.0/24
[...]
http_access allow mylocalnet

Is this right?
Thank you

Gabriel
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20190808/7781f212/attachment.htm>

From twk at ncsu.edu  Thu Aug  8 19:29:36 2019
From: twk at ncsu.edu (Tom Karches)
Date: Thu, 8 Aug 2019 15:29:36 -0400
Subject: [squid-users] Problems with squid 3.1 to 3.3 upgrade
Message-ID: <CAGZ9WNo-U1CfbWJuPzE0XxkZavmYDUKj7n+TRgrbK9qbNOhGRA@mail.gmail.com>

I am in the process of upgrading our Squid proxy server from 3.1 (on RHEL6)
to 3.3 (on RHEL7). It is configured as a explicit (not transparent) proxy
that listens on port 3128. Clients are explicitly configured to use the
proxy.

On the 3.3 system with the same squid.conf as the 3.1 system (I have made
changes to fix warnings), the system is able to proxy internal (*.ncsu.edu)
http traffic and https traffic. Anything https outside the ncsu.edu domain
fails.

The system (which does not use caching) was configured to log https
transactions as such :

1565183014.309    230 127.0.0.1 TCP_MISS/200 62539 CONNECT
entrepreneurship.ncsu.edu:443 - DIRECT/152.1.227.116 -

which requires SSL Bumping (I believe), though there is no reference in the
current configs to the use of SSL bumping .

I used curl to test the new proxy. When I attempt to proxy an external
https connection, this is the result :

$ curl --proxy http://127.0.0.1:3128 https://www.google.com
curl: (56) Received HTTP code 503 from proxy after CONNECT

Proxying internal (ncsu.edu) connections this way is working correctly for
http and https

When I change my squid.conf from :

http_port 3128

to

http_port 3128 ssl-bump \
   cert=/etc/pki/ca-trust/extracted/pem/tls-ca-bundle.pem \
   generate-host-certificates=on dynamic_cert_mem_cache_size=4MB

I now get the following error

squid[5796]: FATAL: No valid signing SSL certificate configured for
> HTTP_port [::]:3128


The certs on the new server are newer, but otherwise appear to be correct.

Are there changes in the SSL bump config between 3.1 and 3.3 that would
cause this kind of failure? Where should I be looking for the problem?

No previous experience with squid until this project. I've been doing much
RTM (including the O'Reilly Squid book) searching online and debugging
these past few days. Suggestions appreciated.

Thanks,
Tom

-- 
Thomas Karches
NCSU OIT CSI - Systems Specialist
M.E Student - STEM Education
Hillsborough 319 / 919.515.5508
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20190808/874d8603/attachment.htm>

From rousskov at measurement-factory.com  Thu Aug  8 22:05:18 2019
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Thu, 8 Aug 2019 18:05:18 -0400
Subject: [squid-users] Problems with squid 3.1 to 3.3 upgrade
In-Reply-To: <CAGZ9WNo-U1CfbWJuPzE0XxkZavmYDUKj7n+TRgrbK9qbNOhGRA@mail.gmail.com>
References: <CAGZ9WNo-U1CfbWJuPzE0XxkZavmYDUKj7n+TRgrbK9qbNOhGRA@mail.gmail.com>
Message-ID: <accc24de-c28f-1313-f9e3-dce1f1a26254@measurement-factory.com>

On 8/8/19 3:29 PM, Tom Karches wrote:

> I am in the process of upgrading our Squid proxy server from 3.1 (on
> RHEL6) to 3.3 (on RHEL7).

It could have been worse! For example, you could ask a question about
upgrading Squid from v1.0 to v2.0... I will try to help, but I do not
remember much about v3.3 specifics.


> The system was configured to log https transactions as such:

> 1565183014.309 ? ?230 127.0.0.1 TCP_MISS/200 62539 CONNECT
> entrepreneurship.ncsu.edu:443 - DIRECT/152.1.227.116 -

> which requires SSL Bumping

No, simply logging HTTP CONNECT requests does not require bumping SSL.


> I used curl to test the new proxy. When I attempt to proxy an external
> https connection, this is the result :

> $ curl --proxy http://127.0.0.1:3128 https://www.google.com
> curl: (56) Received HTTP code 503 from proxy after CONNECT

Your Squid told curl that something went wrong. If you look at the
actual response, you may know what went wrong. The same information may
be available in Squid access.log, but the error response may have more
details than a log record. Please share that info here if it does not
point you to a solution.


> http_port 3128 ssl-bump \
> ? ?cert=/etc/pki/ca-trust/extracted/pem/tls-ca-bundle.pem \
> ? ?generate-host-certificates=on dynamic_cert_mem_cache_size=4MB

> I now get the following error

>     squid[5796]: FATAL: No valid signing SSL certificate configured for
>     HTTP_port [::]:3128


Avoid opening the SslBump Pandora box until you have to. If all you need
is CONNECT logging, then you should be able to accomplish what you want
without SslBump pains.


> Where should I be looking for the problem?

In Squid response to curl. You can use curl tracing options or Wireshark
to see it. Squid access.log may have some clues as well.


Go Tuffy!

Alex.


From squid3 at treenet.co.nz  Fri Aug  9 06:18:53 2019
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Fri, 9 Aug 2019 18:18:53 +1200
Subject: [squid-users] acl src question
In-Reply-To: <CA+d==oEDYOBWrO1FBY2M34ivsFw4gApUB=_NCu8_O6N8g8tTCQ@mail.gmail.com>
References: <CA+d==oEDYOBWrO1FBY2M34ivsFw4gApUB=_NCu8_O6N8g8tTCQ@mail.gmail.com>
Message-ID: <9e299de7-0e84-1d79-8a6f-330db3e86cdd@treenet.co.nz>

On 9/08/19 1:57 am, Service MV wrote:
> Hello everyone!
> 
> I have a network 192.168.10.0/22
> I want to let the IP ranges 192.168.12.1 to 192.168.13.254 through my
> proxy, but not the ranges 192.168.10.1 to 192.168.11.254.
> If I don't misunderstand the documentation
> <http://www.squid-cache.org/Versions/v4/cfgman/acl.html>, the correct
> way to do this would be:
> acl mylocalnet src 192.168.12.0/24
> acl mylocalnet src 192.168.13.0/24
> [...]
> http_access allow mylocalnet
> 
> Is this right?

Close. But that would include the machines with *.0 and *.255 address
outside the range you mention wanting to match.

If your needed range does not map to nice CIDR range(s) you can set the
start and end address instead:

 acl mylocalnet src 192.168.12.1-192.168.13.254



PS. setting the LAN range(s) you want to use the proxy is what the
"localnet" ACL is there for. The values provided are just an example of
standardized ranges that will let the proxy work on most networks by
default.
 There is usually no need for a new custom name, just edit the list as
necessary for your policy. Unless you mean something else for this
custom ACL to be doing - in which case you might want to consider using
a name that makes the access rules read in a more easily interpreted way.

Amos


From squid3 at treenet.co.nz  Fri Aug  9 06:45:30 2019
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Fri, 9 Aug 2019 18:45:30 +1200
Subject: [squid-users] HAProxy + Squid
In-Reply-To: <1565271878333-0.post@n4.nabble.com>
References: <CA+d==oF7y8-_gps4jNo1YV1KnWotTK6brmZWKXztCcJUnSF04g@mail.gmail.com>
 <AM0PR04MB47533E0476C95B3A45900E3D8FC40@AM0PR04MB4753.eurprd04.prod.outlook.com>
 <1565271878333-0.post@n4.nabble.com>
Message-ID: <ffa02b37-bd94-3e49-4040-2be2852203b9@treenet.co.nz>

On 9/08/19 1:44 am, neok wrote:
> Hi, I finally did the configuration differently. It's working very well for
> me.
> 

What you are doing is polluting every HTTP message with two new headers.

The way Rafael suggested is more efficient since the PROXY protocol
details are only delivered at the start of each TCP connection, and HTTP
messages do not need to be reformatted at the LB as they pass.

Note that Squid will be extending that XFF header itself anyway. But
with your way the LB IP address will be broadcast to the origin(s) in
the XFF header along with the client address. With PROXY protocol Squid
adds only the client address there.

Amos


From twk at ncsu.edu  Fri Aug  9 13:59:37 2019
From: twk at ncsu.edu (Tom Karches)
Date: Fri, 9 Aug 2019 09:59:37 -0400
Subject: [squid-users] Problems with squid 3.1 to 3.3 upgrade
In-Reply-To: <mailman.4874.1565301757.3120.squid-users@lists.squid-cache.org>
References: <mailman.4874.1565301757.3120.squid-users@lists.squid-cache.org>
Message-ID: <CAGZ9WNoGhDbUuGWgtGJuLG=+zEkTcY-TfdyJmh+bSV+SjeN+EA@mail.gmail.com>

>
>
> On 8/8/19 3:29 PM, Tom Karches wrote:
>
> > I am in the process of upgrading our Squid proxy server from 3.1 (on
> > RHEL6) to 3.3 (on RHEL7).
>
> It could have been worse! For example, you could ask a question about
> upgrading Squid from v1.0 to v2.0... I will try to help, but I do not
> remember much about v3.3 specifics.
>

I realize that it's a bit old. It is the default for RHEL 7 and unless
there is a specific reason to update to the latest version, I usually stick
with the default. The current proxy is 3.1 and totally works for our
application.


>
> No, simply logging HTTP CONNECT requests does not require bumping SSL.
>
>
Great. Don't want to go down that path.



> > I used curl to test the new proxy. When I attempt to proxy an external
> > https connection, this is the result :
>
> > $ curl --proxy http://127.0.0.1:3128 https://www.google.com
> > curl: (56) Received HTTP code 503 from proxy after CONNECT
>
> Your Squid told curl that something went wrong. If you look at the
> actual response, you may know what went wrong. The same information may
> be available in Squid access.log, but the error response may have more
> details than a log record. Please share that info here if it does not
> point you to a solution.
>
> > Where should I be looking for the problem?
>
> In Squid response to curl. You can use curl tracing options or Wireshark
> to see it. Squid access.log may have some clues as well.
>
>
>
>
With this command :
$curl --trace --proxy http://127.0.0.1:3128 https://www.google.com

I get the HTML of the page, with this near the top :
<title>ERROR: The requested URL could not be retrieved</title>
<style type="text/css"><!--

and then :

<div id="content">
<p>The following error was encountered while trying to retrieve the URL: <a
href="/">/</a></p>
<blockquote id="error">
<p><b>Invalid URL</b></p>
</blockquote>

and no 503 error at the end.

Getting this in access.log :
1565358617.666      0 127.0.0.1 TAG_NONE/400 3958 GET / - HIER_NONE/-
text/html

Which seems odd. So the page is being delivered, but I don't see it unless
--trace is turned on.

When I use :
curl --proxy http://127.0.0.1:3128 https://www.google.com

I get this in access.log :
1565358720.756      2 127.0.0.1 TAG_NONE/503 0 CONNECT www.google.com:443 -
HIER_NONE/- -

My http_port directive is set as such :

# Squid normally listens to port 3128
http_port 3128

This is an explicit proxy so everything should be going through 3128.


I don't feel so bad about not figuring this out sooner. There was a thread
with a similar problem on the list (though it was not helpful) where they
were still stuck at this point after a month. I've only spent a week.



Thanks,

Tom
-- 
Thomas Karches
NCSU OIT CSI - Systems Specialist
M.E Student - Technology Education
Hillsborough 319 / 919.515.5508
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20190809/af69ccc6/attachment.htm>

From rousskov at measurement-factory.com  Fri Aug  9 15:38:09 2019
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Fri, 9 Aug 2019 11:38:09 -0400
Subject: [squid-users] Problems with squid 3.1 to 3.3 upgrade
In-Reply-To: <CAGZ9WNoGhDbUuGWgtGJuLG=+zEkTcY-TfdyJmh+bSV+SjeN+EA@mail.gmail.com>
References: <mailman.4874.1565301757.3120.squid-users@lists.squid-cache.org>
 <CAGZ9WNoGhDbUuGWgtGJuLG=+zEkTcY-TfdyJmh+bSV+SjeN+EA@mail.gmail.com>
Message-ID: <90fb19d7-e183-c1fb-c960-25a28447023b@measurement-factory.com>

On 8/9/19 9:59 AM, Tom Karches wrote:

> With this command :
> $curl --trace --proxy http://127.0.0.1:3128 https://www.google.com

> <p><b>Invalid URL</b></p>

Yeah, that command does not do what you think it does. This has bitten
me many times. You may want to remove the file name "--proxy" now :-).

Hint: The --trace option requires an argument.

Alex.


From twk at ncsu.edu  Fri Aug  9 17:37:47 2019
From: twk at ncsu.edu (Tom Karches)
Date: Fri, 9 Aug 2019 13:37:47 -0400
Subject: [squid-users] Problems with squid 3.1 to 3.3 upgrade
In-Reply-To: <90fb19d7-e183-c1fb-c960-25a28447023b@measurement-factory.com>
References: <mailman.4874.1565301757.3120.squid-users@lists.squid-cache.org>
 <CAGZ9WNoGhDbUuGWgtGJuLG=+zEkTcY-TfdyJmh+bSV+SjeN+EA@mail.gmail.com>
 <90fb19d7-e183-c1fb-c960-25a28447023b@measurement-factory.com>
Message-ID: <CAGZ9WNrE_r22_KjrFqJfSBb6HDvCLkjkyq8bjYBewoO=4t-8iw@mail.gmail.com>

On Fri, Aug 9, 2019 at 11:38 AM Alex Rousskov <
rousskov at measurement-factory.com> wrote:

> On 8/9/19 9:59 AM, Tom Karches wrote:
>
> > With this command :
> > $curl --trace --proxy http://127.0.0.1:3128 https://www.google.com
>
> > <p><b>Invalid URL</b></p>
>
> Yeah, that command does not do what you think it does. This has bitten
> me many times. You may want to remove the file name "--proxy" now :-).
>
>
Doh....that was dumb. FWIW, I'm running 3.5.20, not 3.3

Ok, here is the info from the real trace. First time with #dns_v4_first on
commented out, 2nd time "dns_v4_ first on" is active. Difference is with no
"dns_v4_first on" directive, I get a RR_CONNECT_FAIL 111. When active, I
get a RR_CONNECT_FAIL 101.

#dns_v4_first on   commented out
--------------------------------
<= Recv header, 34 bytes (0x22)
0000: 48 54 54 50 2f 31 2e 31 20 35 30 33 20 53 65 72 HTTP/1.1 503 Ser
0010: 76 69 63 65 20 55 6e 61 76 61 69 6c 61 62 6c 65 vice Unavailable
0020: 0d 0a
.
.
<= Recv header, 37 bytes (0x25)
0000: 58 2d 53 71 75 69 64 2d 45 72 72 6f 72 3a 20 45 X-Squid-Error: E
0010: 52 52 5f 43 4f 4e 4e 45 43 54 5f 46 41 49 4c 20 RR_CONNECT_FAIL
0020: 31 31 31 0d 0a                                  111..
.
.
== Info: Received HTTP code 503 from proxy after CONNECT
== Info: Connection #0 to host 127.0.0.1 left intact
curl: (56) Received HTTP code 503 from proxy after CONNECT



dns_v4_first on
---------------------
<= Recv header, 34 bytes (0x22)
0000: 48 54 54 50 2f 31 2e 31 20 35 30 33 20 53 65 72 HTTP/1.1 503 Ser
0010: 76 69 63 65 20 55 6e 61 76 61 69 6c 61 62 6c 65 vice Unavailable
0020: 0d 0a                                           ..

<= Recv header, 37 bytes (0x25)
0000: 58 2d 53 71 75 69 64 2d 45 72 72 6f 72 3a 20 45 X-Squid-Error: E
0010: 52 52 5f 43 4f 4e 4e 45 43 54 5f 46 41 49 4c 20 RR_CONNECT_FAIL
0020: 31 30 31 0d 0a                                  101..
.
.
== Info: Received HTTP code 503 from proxy after CONNECT
== Info: Connection #0 to host 127.0.0.1 left intact
curl: (56) Received HTTP code 503 from proxy after CONNECT

-- 
Thomas Karches
NCSU OIT CSI - Systems Specialist
M.E Student - Technology Education
Hillsborough 319 / 919.515.5508
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20190809/96832a52/attachment.htm>

From rousskov at measurement-factory.com  Fri Aug  9 18:37:30 2019
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Fri, 9 Aug 2019 14:37:30 -0400
Subject: [squid-users] Problems with squid 3.1 to 3.3 upgrade
In-Reply-To: <CAGZ9WNrE_r22_KjrFqJfSBb6HDvCLkjkyq8bjYBewoO=4t-8iw@mail.gmail.com>
References: <mailman.4874.1565301757.3120.squid-users@lists.squid-cache.org>
 <CAGZ9WNoGhDbUuGWgtGJuLG=+zEkTcY-TfdyJmh+bSV+SjeN+EA@mail.gmail.com>
 <90fb19d7-e183-c1fb-c960-25a28447023b@measurement-factory.com>
 <CAGZ9WNrE_r22_KjrFqJfSBb6HDvCLkjkyq8bjYBewoO=4t-8iw@mail.gmail.com>
Message-ID: <a06ad34f-1151-afdd-8a49-98a2d2b5c418@measurement-factory.com>

On 8/9/19 1:37 PM, Tom Karches wrote:
> On Fri, Aug 9, 2019 at 11:38 AM Alex Rousskov wrote:

> Ok, here is the info from the real trace. First time with #dns_v4_first
> on? commented out, 2nd time "dns_v4_ first on" is active. Difference is
> with no "dns_v4_first on" directive, I get a RR_CONNECT_FAIL 111. When
> active, I get a RR_CONNECT_FAIL 101. 

BTW, it may be easier for you to read --trace--ascii output.

Both are ERR_CONNECT_FAIL errors ("connection reset by peer" and
"connection refused"). Your Squid cannot connect to where it needs to
connect in order to establish a TCP tunnel. It could be a Squid
misconfiguration, a routing problem, insufficient capabilities, and many
other things.

I suggest checking cache.log for WARNINGs and ERRORs. After arriving at
a clean cache.log, I would use a packet capture (or similar) to see
where Squid is trying to connect (and which local address it is
connecting from). That information may be enough to figure out why Squid
cannot connect successfully.

Alex.


From twk at ncsu.edu  Fri Aug  9 20:32:59 2019
From: twk at ncsu.edu (Tom Karches)
Date: Fri, 9 Aug 2019 16:32:59 -0400
Subject: [squid-users] Problems with squid 3.1 to 3.3 upgrade
In-Reply-To: <a06ad34f-1151-afdd-8a49-98a2d2b5c418@measurement-factory.com>
References: <mailman.4874.1565301757.3120.squid-users@lists.squid-cache.org>
 <CAGZ9WNoGhDbUuGWgtGJuLG=+zEkTcY-TfdyJmh+bSV+SjeN+EA@mail.gmail.com>
 <90fb19d7-e183-c1fb-c960-25a28447023b@measurement-factory.com>
 <CAGZ9WNrE_r22_KjrFqJfSBb6HDvCLkjkyq8bjYBewoO=4t-8iw@mail.gmail.com>
 <a06ad34f-1151-afdd-8a49-98a2d2b5c418@measurement-factory.com>
Message-ID: <CAGZ9WNqarg5adAoG=zTYOLn6QWeCBQK2y9jJ7HbtLC9v06fQug@mail.gmail.com>

On Fri, Aug 9, 2019 at 2:37 PM Alex Rousskov <
rousskov at measurement-factory.com> wrote:

> On 8/9/19 1:37 PM, Tom Karches wrote:
> > On Fri, Aug 9, 2019 at 11:38 AM Alex Rousskov wrote:
>
> > Ok, here is the info from the real trace. First time with #dns_v4_first
> > on  commented out, 2nd time "dns_v4_ first on" is active. Difference is
> > with no "dns_v4_first on" directive, I get a RR_CONNECT_FAIL 111. When
> > active, I get a RR_CONNECT_FAIL 101.
>
> BTW, it may be easier for you to read --trace--ascii output.
>

I didn't see anything additional using the ascii option, though it is
easier to read


>
> Both are ERR_CONNECT_FAIL errors ("connection reset by peer" and
> "connection refused"). Your Squid cannot connect to where it needs to
> connect in order to establish a TCP tunnel. It could be a Squid
> misconfiguration, a routing problem, insufficient capabilities, and many
> other things.
>
> I suggest checking cache.log for WARNINGs and ERRORs. After arriving at
> a clean cache.log, I would use a packet capture (or similar) to see
> where Squid is trying to connect (and which local address it is
> connecting from). That information may be enough to figure out why Squid
> cannot connect successfully.
>
>
This is what I am seeing from cache.log when I attempt the proxy :



2019/08/09 16:19:08.127 kid1| 33,2| client_side.cc(817) swanSong: local=
127.0.0.1:3128 remote=127.0.0.1:33428 flags=1
2019/08/09 16:19:10.051 kid1| 33,2| client_side.cc(817) swanSong: local=
152.7.114.135:3128 remote=10.50.54.21:43198 flags=1

Right now my debug is set to ALL,1 33,2. Is there a better set of options
to provide me more visibility of what might be wrong?

Here is our config file, in case that helps. If it's something obvious I'm
not seeing. We have some whitelists, but I am running with those turned off
until this is working so I won't include them here. Thanks for the help.

Tom

# squid config file - 2019-08-09
# Timeouts
connect_timeout 2 minutes  # For CDWG Vendor
debug_options ALL,1 33,2

dns_v4_first on

acl SSL_ports port 443
acl SSL_ports port 1443     # b2b-test.apple.com:1443
acl SSL_ports port 3079     # bci.stapleslink.com special port
acl SSL_ports port 4443     # pascal.apple.com:4443
acl SSL_ports port 993      # IMAP from Stat application to Gmail
acl SSL_ports port 22       # Allow SSH and SFTP to proxy/connect
acl SSL_ports port 8443     # redhat cap port


acl Safe_ports port 80          # http
acl Safe_ports port 21          # ftp
acl Safe_ports port 443         # https
acl Safe_ports port 70          # gopher
acl Safe_ports port 210         # wais
acl Safe_ports port 1025-65535  # unregistered ports
acl Safe_ports port 280         # http-mgmt
acl Safe_ports port 488         # gss-http
acl Safe_ports port 591         # filemaker
acl Safe_ports port 777         # multiling http
acl Safe_ports port 5228        # google services
acl Safe_ports port 1935        # cam steamer port
acl Safe_ports port 8443        # redhat cap port
acl CONNECT method CONNECT

#
# Recommended minimum Access Permission configuration:
#
# Only allow cachemgr access from localhost
http_access allow manager localhost
http_access deny manager

# Deny requests to certain unsafe ports
http_access deny !Safe_ports

# Deny CONNECT to other than secure SSL ports
http_access deny CONNECT !SSL_ports

# We strongly recommend the following be uncommented to protect innocent
# web applications running on the proxy server who think the only
# one who can access services on "localhost" is a local user
#http_access deny to_localhost

#
# INSERT YOUR OWN RULE(S) HERE TO ALLOW ACCESS FROM YOUR CLIENTS
#

# Example rule allowing access from your local networks.
# Adapt localnet in the ACL section to list your (internal) IP networks
# from where browsing should be allowed
http_access allow localnet
http_access allow localhost

# And finally deny all other access to this proxy
http_access deny all

icp_access deny all

# Squid normally listens to port 3128
http_port 3128

# Uncomment and adjust the following to add a disk cache directory.
#cache_dir ufs /var/spool/squid 2048 16 256

# Default configuration value for cache_mem
#cache_mem 256 MB
cache deny all

# Leave coredumps in the first cache dir
coredump_dir /var/spool/squid

# Add any of your own refresh_pattern entries above these.
refresh_pattern ^ftp:           1440    20%     10080
refresh_pattern ^gopher:        1440    0%      1440
refresh_pattern -i (/cgi-bin/|\?) 0     0%      0
refresh_pattern .               0       20%     4320

max_filedescriptors 65000

-- 
Thomas Karches
NCSU OIT CSI - Systems Specialist
M.E Student - Technology Education
Hillsborough 319 / 919.515.5508
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20190809/7398ea29/attachment.htm>

From squid3 at treenet.co.nz  Sat Aug 10 06:47:23 2019
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sat, 10 Aug 2019 18:47:23 +1200
Subject: [squid-users] Problems with squid 3.1 to 3.3 upgrade
In-Reply-To: <CAGZ9WNqarg5adAoG=zTYOLn6QWeCBQK2y9jJ7HbtLC9v06fQug@mail.gmail.com>
References: <mailman.4874.1565301757.3120.squid-users@lists.squid-cache.org>
 <CAGZ9WNoGhDbUuGWgtGJuLG=+zEkTcY-TfdyJmh+bSV+SjeN+EA@mail.gmail.com>
 <90fb19d7-e183-c1fb-c960-25a28447023b@measurement-factory.com>
 <CAGZ9WNrE_r22_KjrFqJfSBb6HDvCLkjkyq8bjYBewoO=4t-8iw@mail.gmail.com>
 <a06ad34f-1151-afdd-8a49-98a2d2b5c418@measurement-factory.com>
 <CAGZ9WNqarg5adAoG=zTYOLn6QWeCBQK2y9jJ7HbtLC9v06fQug@mail.gmail.com>
Message-ID: <a68e3318-022e-8211-4417-e826d0ca9741@treenet.co.nz>

On 10/08/19 8:32 am, Tom Karches wrote:
> 
> 
> On Fri, Aug 9, 2019 at 2:37 PM Alex Rousskov wrote:
> 
>     On 8/9/19 1:37 PM, Tom Karches wrote:
>     > On Fri, Aug 9, 2019 at 11:38 AM Alex Rousskov wrote:
> 
>     > Ok, here is the info from the real trace. First time with
>     #dns_v4_first
>     > on? commented out, 2nd time "dns_v4_ first on" is active.
>     Difference is
>     > with no "dns_v4_first on" directive, I get a RR_CONNECT_FAIL 111. When
>     > active, I get a RR_CONNECT_FAIL 101.
> 
>     BTW, it may be easier for you to read --trace--ascii output.
> 
> 
> I didn't see anything additional using the ascii option, though it is
> easier to read
> ?
> 
> 
>     Both are ERR_CONNECT_FAIL errors ("connection reset by peer" and
>     "connection refused"). Your Squid cannot connect to where it needs to
>     connect in order to establish a TCP tunnel. It could be a Squid
>     misconfiguration, a routing problem, insufficient capabilities, and many
>     other things.
> 
>     I suggest checking cache.log for WARNINGs and ERRORs. After arriving at
>     a clean cache.log, I would use a packet capture (or similar) to see
>     where Squid is trying to connect (and which local address it is
>     connecting from). That information may be enough to figure out why Squid
>     cannot connect successfully.
> 
> 
> This is what I am seeing from cache.log when I attempt the proxy :
> 
> ?
> 
> 2019/08/09 16:19:08.127 kid1| 33,2| client_side.cc(817) swanSong:
> local=127.0.0.1:3128 <http://127.0.0.1:3128> remote=127.0.0.1:33428
> <http://127.0.0.1:33428> flags=1
> 
> 2019/08/09 16:19:10.051 kid1| 33,2| client_side.cc(817) swanSong:
> local=152.7.114.135:3128 <http://152.7.114.135:3128>
> remote=10.50.54.21:43198 <http://10.50.54.21:43198> flags=1
> 
> Right now my debug is set to?ALL,1 33,2. Is there a better set of
> options to provide me more visibility of what might be wrong?
> 

11,2 will show the HTTP message headers.

44,2 will show the servers Squid is finding as possible destinations for
the request/tunnel.

5,6 should show the TCP connection attempts activity by Squid.


If it is not clear from that, those should give you hints about lines to
look for (skip to) for searching a much larger ALL,6 trace.


> Here is our config file, in case that helps. If it's something obvious
> I'm not seeing. We have some whitelists, but I am running with those
> turned off until this is working so I won't include them here. Thanks
> for the help.
> 


Few bits of polish. But nothing visible there to indicate what your
problem might be.

I think it is probably a firewall or routing problem for the traffic
leaving the proxy machine.


> Tom
> 
> # squid config file - 2019-08-09
> # Timeouts
> connect_timeout 2 minutes ?# For CDWG Vendor
> debug_options ALL,1 33,2
> 
> dns_v4_first on
> 
> acl SSL_ports port 443
> acl SSL_ports port 1443 ? ? # b2b-test.apple.com:1443
> <http://b2b-test.apple.com:1443>
> acl SSL_ports port 3079 ? ? # bci.stapleslink.com
> <http://bci.stapleslink.com> special port
> acl SSL_ports port 4443 ? ? # pascal.apple.com:4443
> <http://pascal.apple.com:4443>
> acl SSL_ports port 993 ? ? ?# IMAP from Stat application to Gmail
> acl SSL_ports port 22 ? ? ? # Allow SSH and SFTP to proxy/connect
> acl SSL_ports port 8443 ? ? # redhat cap port
> 
> 
> acl Safe_ports port 80 ? ? ? ? ?# http
> acl Safe_ports port 21 ? ? ? ? ?# ftp
> acl Safe_ports port 443 ? ? ? ? # https
> acl Safe_ports port 70 ? ? ? ? ?# gopher
> acl Safe_ports port 210 ? ? ? ? # wais
> acl Safe_ports port 1025-65535 ?# unregistered ports
> acl Safe_ports port 280 ? ? ? ? # http-mgmt
> acl Safe_ports port 488 ? ? ? ? # gss-http
> acl Safe_ports port 591 ? ? ? ? # filemaker
> acl Safe_ports port 777 ? ? ? ? # multiling http

The ports below are all included in the 1024-65535 range. No need to
list them explicitly here.


> acl Safe_ports port 5228 ? ? ? ?# google services
> acl Safe_ports port 1935 ? ? ? ?# cam steamer port
> acl Safe_ports port 8443 ? ? ? ?# redhat cap port
> acl CONNECT method CONNECT
> 
> #
> # Recommended minimum Access Permission configuration:
> #
> # Only allow cachemgr access from localhost
> http_access allow manager localhost
> http_access deny manager
> 

Latest Squid recommendation is to have these manager lines after the
CONNECT !SSL_ports line.


> # Deny requests to certain unsafe ports
> http_access deny !Safe_ports
> 
> # Deny CONNECT to other than secure SSL ports
> http_access deny CONNECT !SSL_ports
> 
> # We strongly recommend the following be uncommented to protect innocent
> # web applications running on the proxy server who think the only
> # one who can access services on "localhost" is a local user
> #http_access deny to_localhost
> 
> #
> # INSERT YOUR OWN RULE(S) HERE TO ALLOW ACCESS FROM YOUR CLIENTS
> #
> 
> # Example rule allowing access from your local networks.
> # Adapt localnet in the ACL section to list your (internal) IP networks
> # from where browsing should be allowed
> http_access allow localnet
> http_access allow localhost
> 
> # And finally deny all other access to this proxy
> http_access deny all
> 
> icp_access deny all

ICP is off by default in modern Squid. No need for the above deny.
> 
> # Squid normally listens to port 3128
> http_port 3128
> 
> # Uncomment and adjust the following to add a disk cache directory.
> #cache_dir ufs /var/spool/squid 2048 16 256
> 
> # Default configuration value for cache_mem
> #cache_mem 256 MB
> cache deny all
> 
> # Leave coredumps in the first cache dir
> coredump_dir /var/spool/squid
> 
> # Add any of your own refresh_pattern entries above these.
> refresh_pattern ^ftp: ? ? ? ? ? 1440 ? ?20% ? ? 10080
> refresh_pattern ^gopher: ? ? ? ?1440 ? ?0% ? ? ?1440
> refresh_pattern -i (/cgi-bin/|\?) 0 ? ? 0% ? ? ?0
> refresh_pattern . ? ? ? ? ? ? ? 0 ? ? ? 20% ? ? 4320
> 
> max_filedescriptors 65000
> 
> -- 
> Thomas Karches
> NCSU OIT CSI -?Systems Specialist
> M.E Student - Technology Education
> Hillsborough 319 / 919.515.5508
> 
> 
> 
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
> 


From squid3 at treenet.co.nz  Sat Aug 10 10:07:43 2019
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sat, 10 Aug 2019 22:07:43 +1200
Subject: [squid-users] sending certificate chain from squid reverse proxy
In-Reply-To: <1564769330895-0.post@n4.nabble.com>
References: <20190716123412.5mimbz5vozdnjcik@apple.rat.burntout.org>
 <58dfb81a-18cd-edad-224c-7bc41d19c265@treenet.co.nz>
 <1564769330895-0.post@n4.nabble.com>
Message-ID: <1ca910fe-7a86-3d4a-1b35-74aa180b5949@treenet.co.nz>

On 3/08/19 6:08 am, Martin Hoffmann wrote:
> Any ETA on this?
> 

Working on it, but no definite ETA sorry.

Amos


From rousskov at measurement-factory.com  Sat Aug 10 17:57:09 2019
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Sat, 10 Aug 2019 13:57:09 -0400
Subject: [squid-users] Problems with squid 3.1 to 3.3 upgrade
In-Reply-To: <CAGZ9WNqarg5adAoG=zTYOLn6QWeCBQK2y9jJ7HbtLC9v06fQug@mail.gmail.com>
References: <mailman.4874.1565301757.3120.squid-users@lists.squid-cache.org>
 <CAGZ9WNoGhDbUuGWgtGJuLG=+zEkTcY-TfdyJmh+bSV+SjeN+EA@mail.gmail.com>
 <90fb19d7-e183-c1fb-c960-25a28447023b@measurement-factory.com>
 <CAGZ9WNrE_r22_KjrFqJfSBb6HDvCLkjkyq8bjYBewoO=4t-8iw@mail.gmail.com>
 <a06ad34f-1151-afdd-8a49-98a2d2b5c418@measurement-factory.com>
 <CAGZ9WNqarg5adAoG=zTYOLn6QWeCBQK2y9jJ7HbtLC9v06fQug@mail.gmail.com>
Message-ID: <83017fd5-bfba-d93d-c8b1-c4810cccfe4f@measurement-factory.com>

On 8/9/19 4:32 PM, Tom Karches wrote:
> Right now my debug is set to?ALL,1 33,2. Is there a better set of
> options to provide me more visibility of what might be wrong?

In theory, yes: You can increase verbosity levels to see exactly what is
going on. However, most people get lost in the debugging noise. FWIW, I
do not recommend using cache.log above ALL,1 (the default) for triaging
connectivity problems like yours by Squid newbies like you. Most likely,
the connectivity problem is outside Squid and can be seen/reproduced
outside Squid.

I would use Wireshark, tcpdump, or a similar packet-level tool to figure
out where Squid is trying to connect and from what address Squid is
trying to connect. If you are not familiar with those basic tools, any
capable local sysadmin can help you get started -- no Squid knowledge is
needed!

Alex.


From leomessi983 at yahoo.com  Sun Aug 11 17:13:07 2019
From: leomessi983 at yahoo.com (leomessi983 at yahoo.com)
Date: Sun, 11 Aug 2019 17:13:07 +0000 (UTC)
Subject: [squid-users] squid Illegal instruction
References: <1655898785.3903982.1565543587896.ref@mail.yahoo.com>
Message-ID: <1655898785.3903982.1565543587896@mail.yahoo.com>



 ..
HiAfter install my own compiled squid in a linux system i got Illegal instruction error when I run squid!
This is my core dump result:
[New LWP 20036]
[Thread debugging using libthread_db enabled]
Using host libthread_db library "/lib/x86_64-linux-gnu/libthread_db.so.1".
Core was generated by `squid'.
Program terminated with signal SIGILL, Illegal instruction.
#0? 0x00000000008b8ac0 in MemPoolMeter::MemPoolMeter() ()

##############################################################
My cpu info:
Architecture:????????? x86_64
CPU op-mode(s):??????? 32-bit, 64-bit
Byte Order:??????????? Little Endian
CPU(s):??????????????? 8
On-line CPU(s) list:?? 0-7
Thread(s) per core:??? 2
Core(s) per socket:??? 4
Socket(s):???????????? 1
NUMA node(s):????????? 1
Vendor ID:???????????? GenuineIntel
CPU family:??????????? 6
Model:???????????????? 44
Model name:??????????? Intel(R) Xeon(R) CPU?????????? E5640? @ 2.67GHz
Stepping:????????????? 2
CPU MHz:?????????????? 2666.509
BogoMIPS:????????????? 5333.01
Virtualization:??????? VT-x
L1d cache:???????????? 32K
L1i cache:???????????? 32K
L2 cache:????????????? 256K
L3 cache:????????????? 12288K
NUMA node0 CPU(s):???? 0-7

############################################################
My compile configuration options:configure options:  '--with-openssl' '--enable-ssl-crtd' '--prefix=/usr' '--enable-linux-netfilter' '--with-netfilter-conntrack' '--exec-prefix=/usr' '--includedir=/usr/include' '--datadir=/usr/share/squid' '--libdir=/usr/lib64' '--libexecdir=/usr/lib64/squid' '--localstatedir=/var' '--sysconfdir=/etc/squid/' '--sharedstatedir=/var/lib/' '--with-logdir=/var/log/squid/' '--enable-ltdl-convenience' '--enable-http-violations'
what is the problem? why I could not run squid on that machine?!
Thank you
  
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20190811/fb3fc949/attachment.htm>

From squid3 at treenet.co.nz  Sun Aug 11 23:26:05 2019
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Mon, 12 Aug 2019 11:26:05 +1200
Subject: [squid-users] squid Illegal instruction
In-Reply-To: <1655898785.3903982.1565543587896@mail.yahoo.com>
References: <1655898785.3903982.1565543587896.ref@mail.yahoo.com>
 <1655898785.3903982.1565543587896@mail.yahoo.com>
Message-ID: <ad1c804d552c73cc9c579ca520fe5163@treenet.co.nz>

On 2019-08-12 05:13, leomessi983 at yahoo.com wrote:
> .
> .
> 
> Hi
> After install my own compiled squid in a linux system i got Illegal
> instruction error when I run squid!
> 

These usually occur due to:

* building on one OS and running on another

* building on/for one CPU architecture and running on another

* compiler bugs

* upgrade of system libraries with an SO bump from those used to build 
Squid.

* CPU or RAM internal errors.



Was this Squid built on the machine you are running it?

What is the OS of the machine you are using? (for both build and run if 
not the same)

Have you done any system upgrades / downgrades between building Squid 
and running it?


Amos


From prudhvi.gnt65 at gmail.com  Mon Aug 12 15:55:23 2019
From: prudhvi.gnt65 at gmail.com (Prudhvisagar Bellamkonda)
Date: Mon, 12 Aug 2019 11:55:23 -0400
Subject: [squid-users] While using icap_service squid working when ip is
 used and failing when domain name is provided
Message-ID: <CAF6Pdhuhq0ZXNAdbesX4OHVPF4hcnxp=pdrXJHNKAFXd30_xBQ@mail.gmail.com>

Hi,
Thanks for checking my message.
 Please check the below configuration, we are running squid 3.5 version.

This service is running on aws its a ui application trying to connect to
virus scanner to scan the uploaded file and send the request to downstream
application if the file is valid.

We implemented squid before the virus scanner

https_port 8443 accel defaultsite=imageuploadqa.com no-vhost
cert=/qa/certificate/imageupload.cer
key=/qa/certificate/private/imageupload.pem
cache_peer imageuploadroute53downstreamappkication.com
<http://imageuploadroute53.com/>. parent 443 0 proxy-only
name=imageuploadAccel ssl sslflags=DONT_VERIFY_PEER
acl imageupload dstdomain imageuploadqa.com
http_access allow imageupload
cache_peer_access imageuploadAccel allow imageupload
cache_peer_access imageuploadAccel deny all
icap_enable on
icap_service service_avi_req reqmod_precache
icap://domainnameofvirusscanner:1344/SYMCScanReqEx-AV bypass=off (not
working, but working when we are trying to use the IP)
adaptation_access service_avi_req allow all
icap_log /var/log/squid/icap.log icap_squid


it also working when "cache_peer_access imageuploadAccel deny all" Line is
removed

Please let me know if am missing any configuration

Thanks.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20190812/ac8f8fda/attachment.htm>

From leomessi983 at yahoo.com  Mon Aug 12 18:45:06 2019
From: leomessi983 at yahoo.com (leomessi983 at yahoo.com)
Date: Mon, 12 Aug 2019 18:45:06 +0000 (UTC)
Subject: [squid-users] squid Illegal instruction
References: <1034079994.4336171.1565635506959.ref@mail.yahoo.com>
Message-ID: <1034079994.4336171.1565635506959@mail.yahoo.com>


hiyes I use it in different machines,but all of them are debian with the same version!At firts I compiled squid in a vmware debian vm ,bit after then I use the created package in my other debian machin in physical systems with different cpu and memory!
Is there any compile option to solve this issue!?How about this option??--disable-arch-native ?
thanks
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20190812/10c21c31/attachment.htm>

From Joseph.Garbacik at netapp.com  Mon Aug 12 19:16:53 2019
From: Joseph.Garbacik at netapp.com (Garbacik, Joe)
Date: Mon, 12 Aug 2019 19:16:53 +0000
Subject: [squid-users] Stuck Filtering HTTPS URL
Message-ID: <29F764E4-AF51-4F35-90C6-AE49564114E0@netapp.com>

I am trying to permit access to something like the following, https://www.example.com/world, without providing access to the whole site.

I have a basic configuration with the pertinent items as follows:
http_port 3128 ssl-bump \
  cert=/etc/squid/ssl_certs.d/myCA.pem \
  generate-host-certificates=on \
  dynamic_cert_mem_cache_size=16MB \
  options=SINGLE_DH_USE,SINGLE_ECDH_USE,NO_SSLv3,CIPHER_SERVER_PREFERENCE cipher=ECDHE-ECDSA-AES128-GCM-SHA256:ECDHE-RSA-AES256-GCM-SHA384:ECDHE-RSA-AES128-GCM-SHA256:DHE-RSA-AES256-GCM-SHA384:DHE-RSA-AES128-GCM-SHA256:ECDHE-RSA-AES256-SHA384:ECDHE-RSA-AES128-SHA256:ECDHE-RSA-AES256-SHA:ECDHE-RSA-AES128-SHA:DHE-RSA-AES256-SHA256:DHE-RSA-AES128-SHA256:DHE-RSA-AES256-SHA:DHE-RSA-AES128-SHA:!ECDHE-RSA-DES-CBC3-SHA:!EDH-RSA-DES-CBC3-SHA:!AES256-GCM-SHA384:!AES128-GCM-SHA256:!AES256-SHA256:!AES128-SHA256:!AES256-SHA:!AES128-SHA:!DES-CBC3-SHA:HIGH:!aNULL:!eNULL:!EXPORT:!DES:!3DES:!MD5:!PSK:!RC4  sslflags=NO_SESSION_REUSE \
  tls-dh=prime256v1:/etc/squid/dhparams.d/dhparam.pem

sslcrtd_program /usr/lib64/squid/security_file_certgen -s /var/cache/squid/ssl_db -M 16MB
acl step1 at_step SslBump1
acl step2 at_step SslBump2
acl step3 at_step SslBump3
ssl_bump peek step1
ssl_bump bump  all

acl DSTDOMAIN_ALLOW dstdomain www.example.com
acl URLPATH_ALLOW urlpath_regex -i ^/world/*
http_access allow SrcSubnet DSTDOMAIN_ALLOW URLPATH_ALLOW
note ruleid Rule-10-GCP.conf  SrcSubnet DSTDOMAIN_ALLOW URLPATH_ALLOW
note ruletype ALLOW  SrcSubnet DSTDOMAIN_ALLOW URLPATH_ALLOW



Dumping the log into debug mode I see that what appears that it can obtain the path but then fails the connection. If I am reading it properly, it seems to fail the SSL connection after decrypting it without passing thru any ACLs. I've tried researching the delated error message from the log. It there a better way to troubleshoot this error or should I not expect to filter a full URL via HTTPS ?

2019/08/12 10:40:29.053 kid1| 23,3| Uri.cc(371) parse: Split URL 'www.example.com:443' into proto='', host='www.example.com', port='443', path=''
?
2019/08/12 10:40:29.055 kid1| 28,5| Acl.cc(124) matches: checking DSTDOMAIN_ALLOW
2019/08/12 10:40:29.055 kid1| 28,3| DomainData.cc(110) match: aclMatchDomainList: checking 'www.example.com'
2019/08/12 10:40:29.055 kid1| 28,3| DomainData.cc(115) match: aclMatchDomainList: 'www.example.com' found
2019/08/12 10:40:29.055 kid1| 28,3| Acl.cc(151) matches: checked: DSTDOMAIN_ALLOW_1 = 1
2019/08/12 10:40:29.055 kid1| 28,5| Acl.cc(124) matches: checking URLPATH_ALLOW
2019/08/12 10:40:29.055 kid1| 28,3| Acl.cc(151) matches: checked: URLPATH_ALLOW = -1
?
2019/08/12 10:40:29.055 kid1| 33,4| ServerBump.cc(26) ServerBump: will peek at www.example.com:443
?
2019/08/12 10:40:29.062 kid1| 83,3| Handshake.cc(497) parseSniExtension: host_name=www.example.com
?.
2019/08/12 10:40:29.062 kid1| 28,3| DomainData.cc(115) match: aclMatchDomainList: 'www.example.com' found
2019/08/12 10:40:29.062 kid1| 28,3| Acl.cc(151) matches: checked: DSTDOMAIN_ALLOW = 1
2019/08/12 10:40:29.062 kid1| 28,5| Acl.cc(124) matches: checking URLPATH_ALLOW
2019/08/12 10:40:29.062 kid1| 28,3| Acl.cc(151) matches: checked: URLPATH_ALLOW = -1
?
2019/08/12 10:40:29.064 kid1| 33,5| client_side.cc(3023) getSslContextStart: SSL crtd request: new_certificate 2999 host=www.example.com
?
2019/08/12 10:40:29.065 kid1| 33,5| client_side.cc(2860) sslCrtdHandleReply: Certificate for www.example.com was successfully recieved from ssl_crtd
?
2019/08/12 10:40:29.081 kid1| 11,2| client_side.cc(1323) parseHttpRequest: HTTP Client REQUEST:
---------
GET /world HTTP/1.1
Host: www.example.com
User-Agent: Mozilla/5.0 (Macintosh; Intel Mac OS X 10.14; rv:68.0) Gecko/20100101 Firefox/68.0
Accept: text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8
Accept-Language: en-US,en;q=0.5
Accept-Encoding: gzip, deflate, br
DNT: 1
Connection: keep-alive
?
2019/08/12 10:40:29.081 kid1| 23,3| Uri.cc(371) parse: Split URL 'https://www.example.com/world' into proto='https', host='www.example.com', port='443', path='/world'
2019/08/12 10:40:29.081 kid1| 33,5| Http1Server.cc(188) buildHttpRequest: normalize 1 Host header using www.example.com
2019/08/12 10:40:29.081 kid1| 33,3| client_side.cc(641) clientSetKeepaliveFlag: http_ver = HTTP/1.1
2019/08/12 10:40:29.081 kid1| 33,3| client_side.cc(642) clientSetKeepaliveFlag: method = GET
2019/08/12 10:40:29.081 kid1| 33,4| client_side.cc(1471) quitAfterError: Will close after error: local=10. 200. 200. 200:3128 remote=10.1.2.3:64913 FD 13 flags=1
2019/08/12 10:40:29.081 kid1| 33,5| client_side.cc(1492) serveDelayedError: Responding with delated error for https://www.example.com/world
2019/08/12 10:40:29.081 kid1| 11,5| HttpRequest.cc(459) detailError: current error details: 1/0
2019/08/12 10:40:29.081 kid1| 33,5| Stream.cc(109) pullData: 0 written 0 into local=10.200. 200. 200:3128 remote=10.1.2.3:64913 FD 13 flags=1
2019/08/12 10:40:29.081 kid1| 33,5| Stream.cc(133) getNextRangeOffset: range: 0; http offset 0; reply 0
2019/08/12 10:40:29.081 kid1| 33,5| store_client.cc(319) doCopy: store_client::doCopy: co: 0, hi: 3760
2019/08/12 10:40:29.081 kid1| 33,3| Pipeline.cc(35) front: Pipeline 0x2c6cb40 front 0x2c71fc0*4
2019/08/12 10:40:29.081 kid1| 33,3| Pipeline.cc(35) front: Pipeline 0x2c6cb40 front 0x2c71fc0*4
2019/08/12 10:40:29.081 kid1| 11,2| Stream.cc(266) sendStartOfMessage: HTTP Client local=10.193.161.197:3128 remote=10.63.200.153:64913 FD 13 flags=1
2019/08/12 10:40:29.081 kid1| 11,2| Stream.cc(267) sendStartOfMessage: HTTP Client REPLY:
---------
HTTP/1.1 403 Forbidden

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20190812/c8e30351/attachment.htm>

From rousskov at measurement-factory.com  Mon Aug 12 20:31:38 2019
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Mon, 12 Aug 2019 16:31:38 -0400
Subject: [squid-users] Stuck Filtering HTTPS URL
In-Reply-To: <29F764E4-AF51-4F35-90C6-AE49564114E0@netapp.com>
References: <29F764E4-AF51-4F35-90C6-AE49564114E0@netapp.com>
Message-ID: <c19456cc-4ce2-83f0-2194-25948a21daf2@measurement-factory.com>

On 8/12/19 3:16 PM, Garbacik, Joe wrote:
> I am trying to permit access to something like the following,
> https://www.example.com/world, without providing access to the whole site.

> 2019/08/12 10:40:29.081 kid1| 33,4| client_side.cc(1471) quitAfterError: Will close after error:...

> HTTP/1.1 403 Forbidden

> http_access allow SrcSubnet DSTDOMAIN_ALLOW URLPATH_ALLOW

You are probably (implicitly) denying the CONNECT request sent by client
to Squid. Squid needs to process that CONNECT request (that does not
have URL paths) before Squid can bump the TLS tunnel (and see in-tunnel
requests with URLs that have paths). If you deny CONNECT, Squid will
bump the client connection and respond with a (delayed) "access denied"
error to the first in-tunnel request, regardless of what that first
in-tunnel request is.

Rule of thumb: Make everything work, including SslBump, _before_
applying custom filtering rules.

Alex.


From squid3 at treenet.co.nz  Tue Aug 13 06:47:02 2019
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Tue, 13 Aug 2019 18:47:02 +1200
Subject: [squid-users] squid Illegal instruction
In-Reply-To: <1034079994.4336171.1565635506959@mail.yahoo.com>
References: <1034079994.4336171.1565635506959.ref@mail.yahoo.com>
 <1034079994.4336171.1565635506959@mail.yahoo.com>
Message-ID: <6cf239f0-e02c-e5af-6cf7-34b2ad218460@treenet.co.nz>

On 13/08/19 6:45 am, leomessi983 wrote:
> 
> hi
> yes I use it in different machines,but all of them are debian with the
> same version!

Okay. That means we can rely on the OS being the same, and _usually_ the
compiler - though it does occasionally have problems.


> At firts I compiled squid in a vmware debian vm ,bit after then I use
> the created package in my other debian machin in physical systems with
> different cpu and memory!

Memory should not matter. So long as there is sufficient to run.

CPU type though does matter a fair amount. If the VM is simulating
different CPU features or type (32bit vs 64bit) than the physical
machines then this problem will occur.

If you do need to isolate the build from the main machine(s) I recommend
a container like Docker rather than a full VM. That way the CPU stuff
which matters here stays the same both in and out of the container but
any environment boundaries and/or installation mess is isolated by the
container.


> 
> Is there any compile option to solve this issue!?
> How about this option??--disable-arch-native ?

That Squid build option will avoid issues sharing one Squid across
different models of almost-same CPUs.

If the CPUs are different architectures then you have to build a binary
specifically for the run-time arch' or it will not run due to the binary
level stuff being different (aka "Illegal Instruction"). That is
cross-compiling - if that is an unfamiliar term and you actually need to
then I suggest you look it up before going any further.


Amos


From squid3 at treenet.co.nz  Tue Aug 13 06:48:59 2019
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Tue, 13 Aug 2019 18:48:59 +1200
Subject: [squid-users] While using icap_service squid working when ip is
 used and failing when domain name is provided
In-Reply-To: <CAF6Pdhuhq0ZXNAdbesX4OHVPF4hcnxp=pdrXJHNKAFXd30_xBQ@mail.gmail.com>
References: <CAF6Pdhuhq0ZXNAdbesX4OHVPF4hcnxp=pdrXJHNKAFXd30_xBQ@mail.gmail.com>
Message-ID: <abe7ed17-ba77-cc3c-8f3e-b7345a89dc4d@treenet.co.nz>

On 13/08/19 3:55 am, Prudhvisagar Bellamkonda wrote:
> Hi,?
> Thanks for checking my message.?
> ?Please check the below configuration, we are running squid 3.5 version.?
> 
> This service is running on aws its a ui application trying to connect to
> virus scanner to scan the uploaded file and send the request to
> downstream application if the file is valid.?
> 
> We implemented squid before the virus scanner?
> ?
> https_port 8443 accel defaultsite=imageuploadqa.com no-vhost

Since this is a reverse-proxy it really should be listening on port 443
unless you have a good reason not to.

Do all these backend systems accept URLs of the form:
  https://imageuploadqa.com:8443/...

FYI: One of the major benefits of reverse-proxy is that they can protect
against garbage traffic for bogus domains etc aimed at your domain. The
no-vhost style config disables that protection completely.
 No matter what URL anyone sends to this proxy it will automatically
force re-write with that scheme://domain:port/ string before any
internal services and even Squids own ACLs get to see the traffic.


> cert=/qa/certificate/imageupload.cer
> key=/qa/certificate/private/imageupload.pem
> cache_peer?imageuploadroute53downstreamappkication.com. parent 443 0 proxy-only
> name=imageuploadAccel ssl sslflags=DONT_VERIFY_PEER

Please remove that DONT_VERIFY_PEER. It is highly dangerous and actually
not useful.

Just add the sslcafile= option with a PEM file containing the CA(s)
which issued that peers X.509 certificate.


> acl imageupload dstdomain?imageuploadqa.com
> http_access allow imageupload
> cache_peer_access imageuploadAccel allow imageupload
> cache_peer_access imageuploadAccel deny all
> icap_enable on
> icap_service service_avi_req reqmod_precache
> icap://domainnameofvirusscanner:1344/SYMCScanReqEx-AV bypass=off (not
> working, but working when we are trying to use the IP)

That is a very strong hint that the problem is DNS related.

Check both A and AAAA are resolving without a timeout or SERVFAIL
result. That the IP(s) produced are all able to be connected to by the
proxy machine OR connection attempts get a quick non-routable ICMP error
back.


> adaptation_access service_avi_req allow all
> icap_log /var/log/squid/icap.log icap_squid
> 
> 
> it also working when "cache_peer_access imageuploadAccel deny all" Line
> is removed

Very Odd. All that line is doing is making it clear to you what the
behaviour is for that peer.

> 
> Please let me know if am missing any configuration?
> 

Please explain "not working" in more detail - what do you see happening
exactly?

Is it;
 * failing to connect?
 * - does the domain name resolve properly when looked up by your Squid?
 * failing to send the ICAP request?
 * failing to get a response?
 * failing to deliver the response it gets?
 * is any of those a timeout or an explicit error seen by Squid?
 * is Squid producing any error message explaining the problem?
 * are there any hints in cache.log?


Lots of details please.


Amos


From PRUDHVI.GNT65 at GMAIL.COM  Tue Aug 13 10:53:23 2019
From: PRUDHVI.GNT65 at GMAIL.COM (prudhvisagar)
Date: Tue, 13 Aug 2019 05:53:23 -0500 (CDT)
Subject: [squid-users] While using icap_service squid working when ip is
 used and failing when domain name is provided
In-Reply-To: <abe7ed17-ba77-cc3c-8f3e-b7345a89dc4d@treenet.co.nz>
References: <CAF6Pdhuhq0ZXNAdbesX4OHVPF4hcnxp=pdrXJHNKAFXd30_xBQ@mail.gmail.com>
 <abe7ed17-ba77-cc3c-8f3e-b7345a89dc4d@treenet.co.nz>
Message-ID: <1565693603003-0.post@n4.nabble.com>

*Thanks a lot for replying back* 

*"From the cache logs i see that its failing:" *

2019/08/13 01:08:02.809| 14,3| Address.cc(389) lookupHostIP: Given Non-IP
'domainnameofvirusscanner': Name or service not known
Address.cc(389) lookupHostIP: Given Non-IP 'domainnameofvirusscanner': Name
or service not known
essential ICAP service is down after an options fetch failure:
icap://domainnameofvirusscanner:1344/SYMCScanReq-AV [down,!opt]

*"At a diffrent location i can also see that internal dns was able to
resolve the addresses. "
"this domain name is a alb in aws, I am able to connect to the instance IP
under the ALB through squid"*

2019/08/13 01:08:02.852| 78,6| dns_internal.cc(1053) idnsCallback: Merging
DNS results domainnameofvirusscanner A has 3 RR, AAAA has 1 RR
2019/08/13 01:08:02.852| 45,9| cbdata.cc(321) cbdataInternalFree:
0x55a43f8b7218
2019/08/13 01:08:02.852| 45,9| cbdata.cc(338) cbdataInternalFree: Freeing
0x55a43f8b7218
2019/08/13 01:08:02.852| 78,6| dns_internal.cc(1086) idnsCallback: Sending 4
(OK) DNS results to caller.
2019/08/13 01:08:02.852| 45,9| cbdata.cc(492) cbdataReferenceValid:
0x55a43f85aaf8
2019/08/13 01:08:02.852| 45,9| cbdata.cc(426) cbdataInternalUnlock:
0x55a43f85aaf8=0
2019/08/13 01:08:02.852| 45,9| cbdata.cc(321) cbdataInternalFree:
0x55a43f85aaf8
2019/08/13 01:08:02.852| 45,9| cbdata.cc(338) cbdataInternalFree: Freeing
0x55a43f85aaf8
2019/08/13 01:08:02.852| 14,3| ipcache.cc(362) ipcacheParse: 4 answers for
'domainnameofvirusscanner'
2019/08/13 01:08:02.852| 14,3| ipcache.cc(420) ipcacheParse:
domainnameofvirusscanner #0 10.55.10.2
2019/08/13 01:08:02.852| 14,3| ipcache.cc(420) ipcacheParse:
domainnameofvirusscanner #1 10.55.10.3

----------

Squid error messages: 

One more error i found was 
2019/08/13 01:08:02.812| 20,3| store.cc(499) setReleaseFlag:
StoreEntry::setReleaseFlag: '[null_store_key]'
----------
2019/08/13 01:08:02.810| 93,5| AsyncJob.cc(123) callStart:
Adaptation::Icap::ServiceRep status in:[down,!opt,fetch]
2019/08/13 01:08:02.810| 45,9| cbdata.cc(492) cbdataReferenceValid:
0x5534xa43f8b9324ddb8
2019/08/13 01:08:02.810| 45,9| cbdata.cc(492) cbdataReferenceValid:
0x5534xa43f8b9324ddb8
2019/08/13 01:08:02.810| 45,9| cbdata.cc(426) cbdataInternalUnlock:
0x5534xa43f8b9324ddb8=0
2019/08/13 01:08:02.810| 45,9| cbdata.cc(449) cbdataInternalUnlock: Freeing
0x5534xa43f8b9324ddb8
2019/08/13 01:08:02.810| 93,3| ServiceRep.cc(534) noteAdaptationAnswer:
failed to fetch options [down,!opt]
2019/08/13 01:08:02.810| 93,8| ServiceRep.cc(448) changeOptions: changes
options from 0 to 0 [down,!opt]
2019/08/13 01:08:02.810| essential ICAP service is down after an options
fetch failure: icap://domainnameofvirusscanner:1344/SYMCScanReq-AV
[down,!opt]
----------

HTTP/1.1 500 Internal Server Error
Server: squid/3.5.20
Mime-Version: 1.0
Date: Tue, 13 Aug 2019 01:08:02 GMT
Content-Type: text/html;charset=utf-8
Content-Length: 3271
X-Squid-Error: ERR_ICAP_FAILURE 0
Vary: Accept-Language
Content-Language: en
X-Cache: MISS from 5e1c4b7853860819
X-Cache-Lookup: NONE from 5e1c4b7854860819:8443
Via: 1.1 5ec4b7886089 (squid/3.5.20)
Connection: keep-alive

----------
2019/08/13 01:08:02.810| 93,5| Xaction.cc(92) disableRepeats:
Adaptation::Icap::ModXact still cannot be repeated because ICAP service is
unusable [G/R job8]
2019/08/13 01:08:02.810| 93,3| ../../../src/base/AsyncJobCalls.h(177) dial:
Adaptation::Icap::ModXact::noteServiceReady threw exception: ICAP service is
unusable


----------



--
Sent from: http://squid-web-proxy-cache.1019090.n4.nabble.com/Squid-Users-f1019091.html


From jmperrote at policia.rionegro.gov.ar  Tue Aug 13 14:44:08 2019
From: jmperrote at policia.rionegro.gov.ar (jmperrote)
Date: Tue, 13 Aug 2019 11:44:08 -0300
Subject: [squid-users] help to disconnect users after determinated time. TTL
In-Reply-To: <26473e6c-7b63-cc26-8f42-5587baaa78e4@policia.rionegro.gov.ar>
References: <26473e6c-7b63-cc26-8f42-5587baaa78e4@policia.rionegro.gov.ar>
Message-ID: <fca32691-03f2-eb6b-ed70-d25aded407c9@policia.rionegro.gov.ar>

Hello, we have a squid reverse proxy, and use the param "auth_param 
basic credentialsttl 10 minutes" to disconnect users that are inactive 
for a time, but this NOT work, because later a users validated on a 
reverse proxy can continue navigating on a reverse proxy even of later 
10 minutes of inactivity.

And the users can continue navigating day to day and not need to 
revalidated if the browser is not closed.

Watching the Cache Manager menu --> Active Cached Usernames --> Check 
TTL, the check TTL is decrecing but when arrive to 0 is continue 
decrecing with - minus values. We observe that when user refresh the 
browser the Check TTL go to the value of credentialsttl setting (in 
seconds) and start to decrecing.


regards.




From emmanuel.fuste at thalesgroup.com  Tue Aug 13 15:09:20 2019
From: emmanuel.fuste at thalesgroup.com (FUSTE Emmanuel)
Date: Tue, 13 Aug 2019 15:09:20 +0000
Subject: [squid-users] help to disconnect users after determinated time.
 TTL
In-Reply-To: <fca32691-03f2-eb6b-ed70-d25aded407c9@policia.rionegro.gov.ar>
References: <26473e6c-7b63-cc26-8f42-5587baaa78e4@policia.rionegro.gov.ar>
 <fca32691-03f2-eb6b-ed70-d25aded407c9@policia.rionegro.gov.ar>
Message-ID: <03be5184-3a85-2a77-f148-dc90e56e195c@thalesgroup.com>

Le 13/08/2019 ? 16:44, jmperrote a ?crit?:
> Hello, we have a squid reverse proxy, and use the param "auth_param 
> basic credentialsttl 10 minutes" to disconnect users that are inactive 
> for a time, but this NOT work, because later a users validated on a 
> reverse proxy can continue navigating on a reverse proxy even of later 
> 10 minutes of inactivity.
>
Hello,
It is not how things works.
You could not achieve what you want with basic auth.
The TTL is the TTL of the cache between the source of authentication 
(file/ldap/sql etc ...) and Squid.
The client authenticate itself on your back at each request because it 
cache auth material. There is no notion of "disconnection" from the 
server side. It could only be a client side policy if implemented in the 
browser.

Emmanuel.

From ser.vieira at gmail.com  Tue Aug 13 19:18:51 2019
From: ser.vieira at gmail.com (=?utf-8?Q?S=C3=A9rgio_Vieira?=)
Date: Tue, 13 Aug 2019 20:18:51 +0100
Subject: [squid-users] squid.config
Message-ID: <5EDBEDD6-93D0-493A-AF56-59CA7F0B2FFB@gmail.com>

Hello,
Regarding squid config file, on MacOS, I can?t add the following parameter: 
strip_query_terms off



I can access the file and edit it, but after restart the file removes the added line...



I have the config file at /Users/sergiovieira/Library/Preferences



Thanks in advance.

Best regards
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20190813/f2f58c49/attachment.htm>

From Antony.Stone at squid.open.source.it  Tue Aug 13 19:54:27 2019
From: Antony.Stone at squid.open.source.it (Antony Stone)
Date: Tue, 13 Aug 2019 21:54:27 +0200
Subject: [squid-users] squid.config
In-Reply-To: <5EDBEDD6-93D0-493A-AF56-59CA7F0B2FFB@gmail.com>
References: <5EDBEDD6-93D0-493A-AF56-59CA7F0B2FFB@gmail.com>
Message-ID: <201908132154.27400.Antony.Stone@squid.open.source.it>

On Tuesday 13 August 2019 at 21:18:51, S?rgio Vieira wrote:

> Hello,
> Regarding squid config file, on MacOS, I can?t add the following parameter:
> strip_query_terms off
> 
> I can access the file and edit it, but after restart the file removes the
> added line...
> 
> I have the config file at /Users/sergiovieira/Library/Preferences

My first questions would be: How did you install Squid on this machine?  What 
instructions did you follow?

That looks like a really unusual path to the squid.conf file to me, but then 
again I don't use a Mac, so maybe it's entirely reasonable.

However, a bit more information would probably be helpful:

 - which version of OSX are you installing on?

 - which version of Squid are you installing?

 - as asked before, how are you installing it?

 - how do you start / stop the Squid process?

Regards,


Antony.

-- 
Schr?dinger's rule of data integrity: the condition of any backup is unknown 
until a restore is attempted.

                                                   Please reply to the list;
                                                         please *don't* CC me.


From ser.vieira at gmail.com  Tue Aug 13 21:11:12 2019
From: ser.vieira at gmail.com (=?utf-8?Q?S=C3=A9rgio_Vieira?=)
Date: Tue, 13 Aug 2019 22:11:12 +0100
Subject: [squid-users] squid.config
In-Reply-To: <201908132154.27400.Antony.Stone@squid.open.source.it>
References: <5EDBEDD6-93D0-493A-AF56-59CA7F0B2FFB@gmail.com>
 <201908132154.27400.Antony.Stone@squid.open.source.it>
Message-ID: <106D610C-42A6-42B0-AB7F-F132E2DBEA45@gmail.com>

Hello,

I followed the instructions on this site: https://howchoo.com/g/mwi3ntu1mjq/how-to-set-up-a-proxy-server-on-mac <https://howchoo.com/g/mwi3ntu1mjq/how-to-set-up-a-proxy-server-on-mac>

Regarding your questions:
- macOS Mojave 10.14.6
- Squid v4.0
- Instructions in the site mentioned above
- I use SquidMan

Why the changes in the config file don?t got permanent?


Thanks in advance.
Regards
 

> On 13 Aug 2019, at 20:54, Antony Stone <Antony.Stone at squid.open.source.it> wrote:
> 
> On Tuesday 13 August 2019 at 21:18:51, S?rgio Vieira wrote:
> 
>> Hello,
>> Regarding squid config file, on MacOS, I can?t add the following parameter:
>> strip_query_terms off
>> 
>> I can access the file and edit it, but after restart the file removes the
>> added line...
>> 
>> I have the config file at /Users/sergiovieira/Library/Preferences
> 
> My first questions would be: How did you install Squid on this machine?  What 
> instructions did you follow?
> 
> That looks like a really unusual path to the squid.conf file to me, but then 
> again I don't use a Mac, so maybe it's entirely reasonable.
> 
> However, a bit more information would probably be helpful:
> 
> - which version of OSX are you installing on?
> 
> - which version of Squid are you installing?
> 
> - as asked before, how are you installing it?
> 
> - how do you start / stop the Squid process?
> 
> Regards,
> 
> 
> Antony.
> 
> -- 
> Schr?dinger's rule of data integrity: the condition of any backup is unknown 
> until a restore is attempted.
> 
>                                                   Please reply to the list;
>                                                         please *don't* CC me.
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20190813/7c9e88a0/attachment.htm>

From PRUDHVI.GNT65 at GMAIL.COM  Wed Aug 14 02:17:36 2019
From: PRUDHVI.GNT65 at GMAIL.COM (prudhvisagar)
Date: Tue, 13 Aug 2019 21:17:36 -0500 (CDT)
Subject: [squid-users] While using icap_service squid working when ip is
 used and failing when domain name is provided
In-Reply-To: <1565693603003-0.post@n4.nabble.com>
References: <CAF6Pdhuhq0ZXNAdbesX4OHVPF4hcnxp=pdrXJHNKAFXd30_xBQ@mail.gmail.com>
 <abe7ed17-ba77-cc3c-8f3e-b7345a89dc4d@treenet.co.nz>
 <1565693603003-0.post@n4.nabble.com>
Message-ID: <1565749056235-0.post@n4.nabble.com>

it also looks like DNS is able to resolve the domainname for the
virusscanner, but looks like by the time its resolved the ICAP_SERVICE
command failed. 

if my understanding is right, Is there any configuration to do the dns
resolving first and before executing the below command 
icap_service service_avi_req reqmod_precache
icap://domainnameofvirusscanner:1344/SYMCScanReqEx-AV bypass=off

Also we are using 3.5, let us know if we have to upgrade ?

Thanks a lot.





--
Sent from: http://squid-web-proxy-cache.1019090.n4.nabble.com/Squid-Users-f1019091.html


From squid3 at treenet.co.nz  Wed Aug 14 05:14:58 2019
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Wed, 14 Aug 2019 17:14:58 +1200
Subject: [squid-users] squid.config
In-Reply-To: <106D610C-42A6-42B0-AB7F-F132E2DBEA45@gmail.com>
References: <5EDBEDD6-93D0-493A-AF56-59CA7F0B2FFB@gmail.com>
 <201908132154.27400.Antony.Stone@squid.open.source.it>
 <106D610C-42A6-42B0-AB7F-F132E2DBEA45@gmail.com>
Message-ID: <bb17ba96-0785-c24a-0be3-9eb21ffa159f@treenet.co.nz>

On 14/08/19 9:11 am, S?rgio Vieira wrote:
> Hello,
> 
> I followed the instructions on this
> site:?https://howchoo.com/g/mwi3ntu1mjq/how-to-set-up-a-proxy-server-on-mac
> 
> Regarding your questions:
> - macOS Mojave 10.14.6
> - Squid v4.0
> - Instructions in the site mentioned above
> - I use SquidMan
> 
> Why the changes in the config file don?t got permanent?
> 

It sounds like SquidMan is writing a new config file on each restart.

AFAIK you have to edit the "Template" file used by SquidMan to retain
any settings in the squid.conf the Squid gets handed.

If that template is what you are already editing (via the SquidMan UI?),
then it is probably a bug in SquidMan and you will need to contact the
author about that.

Amos


From chirayu.patel at truecomtelesoft.com  Wed Aug 14 17:44:52 2019
From: chirayu.patel at truecomtelesoft.com (Chirayu Patel)
Date: Wed, 14 Aug 2019 23:14:52 +0530
Subject: [squid-users] Assertion failure in helper.cc
Message-ID: <CAOhxsyzvAqfcF=jWzd=AXyj1iufdTMAc=quKXq_vO4bY3bW9HA@mail.gmail.com>

Following is the squid config :

http_port 3129 intercept
https_port 3131 intercept ssl-bump cert=/etc/ray/certificates/myCA.pem \
    generate-host-certificates=off dynamic_cert_mem_cache_size=2MB
## For Captive Portal
http_port 3132 intercept
https_port 3133 intercept ssl-bump cert=/etc/ray/certificates/myCA.pem \
    generate-host-certificates=off dynamic_cert_mem_cache_size=1MB

#sslcrtd_program /usr/lib/squid/ssl_crtd -s /var/lib/ssl_db -M 4MB
#sslcrtd_children 5

# TLS/SSL bumping definitions
acl tls_s1_connect at_step SslBump1
acl tls_s2_client_hello at_step SslBump2
acl tls_s3_server_hello at_step SslBump3

# TLS/SSL bumping steps
ssl_bump peek tls_s1_connect all # peek at TLS/SSL connect data
ssl_bump splice all # splice: no active bumping
on_unsupported_protocol tunnel all

pinger_enable off
digest_generation off
netdb_filename none
ipcache_size 128
fqdncache_size 128
via off
forwarded_for transparent
httpd_suppress_version_string on
cache deny all
cache_mem 0 MB
memory_pools off
shutdown_lifetime 0 seconds

#logfile_daemon /dev/null
access_log none

#acl good_url dstdomain .yahoo.com
http_access allow all

url_rewrite_program /tmp/squid/urlcat_server
#url_rewrite_bypass on
url_rewrite_children 1 startup=1 idle=1 concurrency=30 queue-size=10000
on-persistent-overload=ERR
#url_rewrite_access allow all
#url_rewrite_extras "%>a/%>A %un %>rm bump_mode=%ssl::bump_mode
sni=\"%ssl::>sni\" referer=\"%{Referer}>h\""
url_rewrite_extras "%>a %lp %ssl::>sni"

max_filedesc 5120

--------------------------

--> We are getting this error sometimes and squid crashes later :

Tue Aug 13 15:02:12 2019 daemon.notice squid[20079]: helperHandleRead:
unexpected reply on channel 0 from redirector #Hlpr1 '1167 OK status=302
url=https://xxx.xxx.xxx <https://captive.ray.life/>'
Tue Aug 13 15:02:12 2019 daemon.warn squid[20079]: assertion failed:
helper.cc:1039: "skip == 0 && eom == NULL"

--> After this error squid restarts.. What could be the reason behind
this.. Please help !!

--
Thank You
Chirayu Patel
Truecom Telesoft
+91 8758484287
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20190814/97456bcb/attachment.htm>

From rousskov at measurement-factory.com  Wed Aug 14 19:08:14 2019
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Wed, 14 Aug 2019 15:08:14 -0400
Subject: [squid-users] Assertion failure in helper.cc
In-Reply-To: <CAOhxsyzvAqfcF=jWzd=AXyj1iufdTMAc=quKXq_vO4bY3bW9HA@mail.gmail.com>
References: <CAOhxsyzvAqfcF=jWzd=AXyj1iufdTMAc=quKXq_vO4bY3bW9HA@mail.gmail.com>
Message-ID: <5e3d068c-323b-f6b3-ef57-70bf755f519e@measurement-factory.com>

On 8/14/19 1:44 PM, Chirayu Patel wrote:
> url_rewrite_program /tmp/squid/urlcat_server
> url_rewrite_children 1 startup=1 idle=1 concurrency=30 queue-size=10000 on-persistent-overload=ERR
> url_rewrite_extras "%>a %lp %ssl::>sni"

> --> We are getting this error sometimes and squid crashes later :
> 
> Tue Aug 13 15:02:12 2019 squid[20079]: helperHandleRead: unexpected reply on channel 0 from redirector #Hlpr1 '1167 OK status=302 url=https://xxx.xxx.xxx'
> Tue Aug 13 15:02:12 2019 squid[20079]: assertion failed:?helper.cc:1039: "skip == 0 && eom == NULL"??


Most likely, Squid got a malformed (from Squid point of view) helper
response and crashed while handling it. It is not clear to me whether
the response reported on the first line is the one that led to the
crash, but neither of those lines should be printed if everything works
as expected.

The parsing code for helper responses is pretty bad, but it may be
possible to figure out what caused the assertion by examining the core
dump. If you have it (or can change your environment to produce core
dumps), then please post a backtrace from gdb. Squid wiki has
instructions if you are not familiar with the process. You may prefer to
move this triage to Squid Bugzilla though.

Alex.


From jkroneme at asu.edu  Thu Aug 15 08:14:22 2019
From: jkroneme at asu.edu (Joshua Kronemeyer)
Date: Thu, 15 Aug 2019 01:14:22 -0700
Subject: [squid-users] Cache html pages with advertised length of -1?
Message-ID: <CABdjYe15H6svEJgEB+8xtDvrmm6t3PiAOHwTPj80y0zOjxSZLA@mail.gmail.com>

Hello all!

I'm trying to cache some HTML pages with squid, but the pages I'm trying to
cache always advertise size of -1. (Advertised Size/Actual Size i.e.
-1/104732)
The http headers don't include a content-length.

As per the store.log docs "If the advertised length is not zero, but not
equal to the real length, the object will be released from the cache."
Is there a way to get the size of this page in advance so squid can cache
it or ignore the mismatch?

Thanks!
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20190815/42dea9ee/attachment.htm>

From squid3 at treenet.co.nz  Thu Aug 15 11:27:14 2019
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Thu, 15 Aug 2019 23:27:14 +1200
Subject: [squid-users] Cache html pages with advertised length of -1?
In-Reply-To: <CABdjYe15H6svEJgEB+8xtDvrmm6t3PiAOHwTPj80y0zOjxSZLA@mail.gmail.com>
References: <CABdjYe15H6svEJgEB+8xtDvrmm6t3PiAOHwTPj80y0zOjxSZLA@mail.gmail.com>
Message-ID: <df9c9a2c-da71-392c-1ffd-1089366f8944@treenet.co.nz>

On 15/08/19 8:14 pm, Joshua Kronemeyer wrote:
> Hello all!
> 
> I'm trying to cache some HTML pages with squid, but the pages I'm trying
> to cache always advertise size of -1. (Advertised Size/Actual Size i.e.
> -1/104732)
> The http headers don't include a content-length.

So the objects are of unknowable length. Possibly infinite.

In store.log -1 means unknown/undefined. It is only a display value.
Which is correct, advertised size (Content-Length) was not provided.


> As per the store.log docs "If the advertised length is not zero, but not
> equal to the real length, the object will be released from the cache."
> Is there a way to get the size of this page in advance so squid can
> cache it or ignore the mismatch?

That would be the Content-Length header. Which you say the server is not
providing. Nothing anyone but the server admin can do about that.

When the Content-Length is not known that documented calculation is not
possible. Squid does cache objects with unspecified length unless there
is some _other_ reason not to.

Amos


From jmperrote at policia.rionegro.gov.ar  Thu Aug 15 15:30:31 2019
From: jmperrote at policia.rionegro.gov.ar (jmperrote)
Date: Thu, 15 Aug 2019 12:30:31 -0300
Subject: [squid-users] help to disconnect users after determinated time.
 TTL
In-Reply-To: <2d13100e-f4a7-3187-ecbe-67eb8d210310@thalesgroup.com>
References: <26473e6c-7b63-cc26-8f42-5587baaa78e4@policia.rionegro.gov.ar>
 <fca32691-03f2-eb6b-ed70-d25aded407c9@policia.rionegro.gov.ar>
 <03be5184-3a85-2a77-f148-dc90e56e195c@thalesgroup.com>
 <4bf2311a-98e2-529a-99a5-5835e709a6b2@policia.rionegro.gov.ar>
 <2d13100e-f4a7-3187-ecbe-67eb8d210310@thalesgroup.com>
Message-ID: <137be43f-3d9c-5852-0705-2aae26b0413a@policia.rionegro.gov.ar>

Hello Emmanuel, we finish implementing a solution on PHP script, getting 
the TTL time < 0 on the cachemgr, and it work.

The problem is that the param --> auth_param basic credentialsttl 3 
minutes, give this time (180 seconds), but if the user still navigating 
on the site, this value

"Check TTL" is not renewing when the user is navigating, so if the user not aplly any click on the page just when the counter "Check TTL" is 0, the user counter go to < 0.


It is posible introduce any param that tell to squid to renew the counter when a user is betwen the credentialsttl time and still navigating ?

regards.
  



El 13/8/19 a las 12:33, FUSTE Emmanuel escribi?:
> Hello,
>
> Le 13/08/2019 ? 17:06, jmperrote a ?crit?:
>> Hello Emmanuel regards for your answer.
>>
>> We need a solution that if the user do not nothing for about a period
>> of time, for security reason, the reverse proxy request again the
>> authentication, how can resolv that ?
> You need to generate a failed auth to force client cache expiration/auth
> popup.
> So you need to manage your own intermediate cache/TTL in your PHP script.
>
> Put squid credentialttl at 5 minute.
> Squid will call your authenticator two times in ten minutes on an active
> "session" but zero time on a stale one. Issue an auth fail the next time
> even if the auth is ok in this case.
> Disable negative caching on squid to get it work.
>
> But? it is not very robust :
> At startup you will need two auth/popup to successfully connect
> Many pages do requests on your back, reseting the TTL
> Etc ....
>
> As http is stateless, it is more difficult as it sound.
> Perhaps something is doable with? kerberos/ticket authentication scheme,
> but I did not look at.
>
> Emmanuel.
>> We use aut_param basic with php script (ldap repository) for
>> authentication.
>>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20190815/0b091171/attachment.htm>

From rousskov at measurement-factory.com  Thu Aug 15 16:17:44 2019
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Thu, 15 Aug 2019 12:17:44 -0400
Subject: [squid-users] Cache html pages with advertised length of -1?
In-Reply-To: <CABdjYe15H6svEJgEB+8xtDvrmm6t3PiAOHwTPj80y0zOjxSZLA@mail.gmail.com>
References: <CABdjYe15H6svEJgEB+8xtDvrmm6t3PiAOHwTPj80y0zOjxSZLA@mail.gmail.com>
Message-ID: <283ff764-ab75-01a6-e29e-74b9bd2c067a@measurement-factory.com>

On 8/15/19 4:14 AM, Joshua Kronemeyer wrote:

> I'm trying to cache some HTML pages with squid, but the pages I'm trying
> to cache always advertise size of -1. (Advertised Size/Actual Size i.e.
> -1/104732)
> The http headers don't include a content-length.

> As per the store.log docs "If the advertised length is not zero, but not
> equal to the real length, the object will be released from the cache."
> Is there a way to get the size of this page in advance so squid can
> cache it or ignore the mismatch?

IIRC, in many cases, Squid can cache documents of unknown (a priori)
size. The store.log documentation you are looking at is probably talking
about size mismatches, which is a different special/problematic case.
The docs may also be out of sync with the actual logging code (e.g.,
saying "zero" instead of "negative").

You need to figure out whether Squid considers the response cachable.
You can start by using external tools like redbot.org often recommended
on this list. In some cases, looking at Squid debugging logs becomes
necessary to understand why Squid is not caching something (if it does
not) or why Squid is not serving a hit for the cached resource.

Alex.


From service.mv at gmail.com  Thu Aug 15 20:10:25 2019
From: service.mv at gmail.com (Service MV)
Date: Thu, 15 Aug 2019 17:10:25 -0300
Subject: [squid-users] acl src question
In-Reply-To: <9e299de7-0e84-1d79-8a6f-330db3e86cdd@treenet.co.nz>
References: <CA+d==oEDYOBWrO1FBY2M34ivsFw4gApUB=_NCu8_O6N8g8tTCQ@mail.gmail.com>
 <9e299de7-0e84-1d79-8a6f-330db3e86cdd@treenet.co.nz>
Message-ID: <CA+d==oF-TeJ-HrDxOAa2zyPAHHTN=vXb3DCT7fO5Pq949B6=zw@mail.gmail.com>

Thanks Amos. The indication was useful.
Best regards

Gabriel

El vie., 9 ago. 2019 03:19, Amos Jeffries <squid3 at treenet.co.nz> escribi?:

> On 9/08/19 1:57 am, Service MV wrote:
> > Hello everyone!
> >
> > I have a network 192.168.10.0/22
> > I want to let the IP ranges 192.168.12.1 to 192.168.13.254 through my
> > proxy, but not the ranges 192.168.10.1 to 192.168.11.254.
> > If I don't misunderstand the documentation
> > <http://www.squid-cache.org/Versions/v4/cfgman/acl.html>, the correct
> > way to do this would be:
> > acl mylocalnet src 192.168.12.0/24
> > acl mylocalnet src 192.168.13.0/24
> > [...]
> > http_access allow mylocalnet
> >
> > Is this right?
>
> Close. But that would include the machines with *.0 and *.255 address
> outside the range you mention wanting to match.
>
> If your needed range does not map to nice CIDR range(s) you can set the
> start and end address instead:
>
>  acl mylocalnet src 192.168.12.1-192.168.13.254
>
>
>
> PS. setting the LAN range(s) you want to use the proxy is what the
> "localnet" ACL is there for. The values provided are just an example of
> standardized ranges that will let the proxy work on most networks by
> default.
>  There is usually no need for a new custom name, just edit the list as
> necessary for your policy. Unless you mean something else for this
> custom ACL to be doing - in which case you might want to consider using
> a name that makes the access rules read in a more easily interpreted way.
>
> Amos
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20190815/5d7280ef/attachment.htm>

From service.mv at gmail.com  Thu Aug 15 20:46:00 2019
From: service.mv at gmail.com (Service MV)
Date: Thu, 15 Aug 2019 17:46:00 -0300
Subject: [squid-users] HAProxy + Squid
In-Reply-To: <ffa02b37-bd94-3e49-4040-2be2852203b9@treenet.co.nz>
References: <CA+d==oF7y8-_gps4jNo1YV1KnWotTK6brmZWKXztCcJUnSF04g@mail.gmail.com>
 <AM0PR04MB47533E0476C95B3A45900E3D8FC40@AM0PR04MB4753.eurprd04.prod.outlook.com>
 <1565271878333-0.post@n4.nabble.com>
 <ffa02b37-bd94-3e49-4040-2be2852203b9@treenet.co.nz>
Message-ID: <CA+d==oG97Z6C=5H6atM_LpZt4EhjoiFJcRLKVfJ4JkTGPL6JRg@mail.gmail.com>

Thank you, Amos. Taking into account your and Rafael's recommendations, I
configured HAProxy and Squid to use the PROXY protocol instead of
reformatting the messages.
At the moment I disabled authentication, due to internal requirements.
I had a hard time dealing with the HAProxy health checks, but I was able to
fix it.
However, by configuring Squid in this way, I had a last problem that I
didn't expect:
Squid reports the client's IP to my internet gateway instead of their own
IP.
It's true, I want to see my clients' IPs in Squid's log; but I want Squid's
IP to reach my gateway and not my clients'. This way I can make my clients
browse internet only through my proxies.

I'm really not sure if continuing with this configuration I'm doing will be
possible to achieve it.
I appreciate in advance any indication you can give me.


haproxy.cfg
global
    log /dev/log    local0
    log /dev/log    local1 notice
    chroot /var/lib/haproxy
    stats socket /run/haproxy/admin.sock mode 660 level admin expose-fd
listeners
    stats timeout 30s
    user haproxy
    group haproxy
    daemon
    maxconn 4000
    ca-base /etc/ssl/certs
    crt-base /etc/ssl/private
    ssl-default-bind-ciphers ECDH+AESGCM:DH+AESGCM:ECDH+AES256:DH+AES256:
ECDH+AES128:DH+AES:RSA+AESGCM:RSA+AES:!aNULL:!MD5:!DSS
    ssl-default-bind-options no-sslv3

defaults
    log global
    mode tcp
    option tcplog
    option  dontlognull
timeout connect 5000
timeout client 50000
timeout server 50000

frontend squid_in
    bind *:3128
    default_backend squid_pool

backend squid_pool
    balance source
    mode tcp
server px1 x.x.x.1:3128 check port 8181 send-proxy inter 2000 rise 2 fall 3
server px2 x.x.x.2:3128 check port 8181 send-proxy inter 2000 rise 2 fall 3


squid.conf
acl localnet src 192.168.12.1-192.168.13.254 # my clients IP's
acl localnet src 192.168.11.80 # haproxy IP

acl SSL_ports port 443
acl Safe_ports port 80      # http
acl Safe_ports port 21      # ftp
acl Safe_ports port 443     # https
acl Safe_ports port 70      # gopher
acl Safe_ports port 210     # wais
#acl Safe_ports port 1025-65535 # unregistered ports
acl Safe_ports port 280     # http-mgmt
acl Safe_ports port 488     # gss-http
acl Safe_ports port 591     # filemaker
acl Safe_ports port 777     # multiling http
acl Safe_ports port 8181 # haproxy health checks port
acl CONNECT method CONNECT

# list of allowed domains
acl LS_whitedomains dstdomain "/etc/squid/acl/whitedomains.txt"

# list of bloqued domains
acl LS_blackdomains dstdomain "/etc/squid/acl/blackdomains.txt"
acl LS_malicius dstdomain "/etc/squid/acl/malicius.txt"
acl LS_porn dstdomain "/etc/squid/acl/porn.txt"

# Deny requests to certain unsafe ports
http_access deny !Safe_ports

# Deny CONNECT to other than secure SSL ports
http_access deny CONNECT !SSL_ports

# Only allow cachemgr access from localhost
http_access allow localhost manager
http_access deny manager

# We strongly recommend the following be uncommented to protect innocent
web applications running on the proxy server who think the only one who can
access services on "localhost" is a local user
#http_access deny to_localhost
#
# INSERT YOUR OWN RULE(S) HERE TO ALLOW ACCESS FROM YOUR CLIENTS
#
# implementation of access list policies
http_access deny LS_blackdomains
http_access allow LS_whitedomains
http_access deny LS_malicius
http_access deny LS_porn

# limit downloads to 10 Mb/s to the localnet network
delay_pools 1
delay_class 1 2
delay_parameters 1 1310720/1966080 917504/1310720
delay_access 1 allow localnet

# implementation of core access policies
proxy_protocol_access allow localnet
http_access allow localnet
http_access allow localhost

# And finally deny all other access to this proxy
http_access deny all

# Squid normally listens to port 3128
http_port 3128 require-proxy-header

# port used only for haproxy health checks
http_port 8181

# Leave coredumps in the first cache dir
coredump_dir /var/spool/squid

# Add any of your own refresh_pattern entries above these.
refresh_pattern ^ftp:       1440    20% 10080
refresh_pattern ^gopher:    1440    0%  1440
refresh_pattern -i (/cgi-bin/|\?) 0 0%  0
refresh_pattern .       0   20% 4320

# squid customization settings
quick_abort_min 0 KB
quick_abort_max 0 KB
read_timeout 5 minutes
request_timeout 3 minutes
shutdown_lifetime 0 seconds
ipcache_size 2048
fqdncache_size 4096
cache_mgr me at tsa.net
visible_hostname px1
httpd_suppress_version_string on
forwarded_for off


Best regards
Gabriel

El vie., 9 de ago. de 2019 a la(s) 03:45, Amos Jeffries (
squid3 at treenet.co.nz) escribi?:

> On 9/08/19 1:44 am, neok wrote:
> > Hi, I finally did the configuration differently. It's working very well
> for
> > me.
> >
>
> What you are doing is polluting every HTTP message with two new headers.
>
> The way Rafael suggested is more efficient since the PROXY protocol
> details are only delivered at the start of each TCP connection, and HTTP
> messages do not need to be reformatted at the LB as they pass.
>
> Note that Squid will be extending that XFF header itself anyway. But
> with your way the LB IP address will be broadcast to the origin(s) in
> the XFF header along with the client address. With PROXY protocol Squid
> adds only the client address there.
>
> Amos
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20190815/ceb3730b/attachment.htm>

From squid3 at treenet.co.nz  Fri Aug 16 07:18:08 2019
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Fri, 16 Aug 2019 19:18:08 +1200
Subject: [squid-users] HAProxy + Squid
In-Reply-To: <CA+d==oG97Z6C=5H6atM_LpZt4EhjoiFJcRLKVfJ4JkTGPL6JRg@mail.gmail.com>
References: <CA+d==oF7y8-_gps4jNo1YV1KnWotTK6brmZWKXztCcJUnSF04g@mail.gmail.com>
 <AM0PR04MB47533E0476C95B3A45900E3D8FC40@AM0PR04MB4753.eurprd04.prod.outlook.com>
 <1565271878333-0.post@n4.nabble.com>
 <ffa02b37-bd94-3e49-4040-2be2852203b9@treenet.co.nz>
 <CA+d==oG97Z6C=5H6atM_LpZt4EhjoiFJcRLKVfJ4JkTGPL6JRg@mail.gmail.com>
Message-ID: <2209db35-4616-7b8f-dd81-8d4d45c9a59d@treenet.co.nz>

On 16/08/19 8:46 am, Service MV wrote:
> Thank you, Amos. Taking into account your and Rafael's recommendations,
> I configured HAProxy and Squid to use the PROXY protocol instead of
> reformatting the messages.
> At the moment I disabled authentication, due to internal requirements.
> I had a hard time dealing with the HAProxy health checks, but I was able
> to fix it.
> However, by configuring Squid in this way, I had a last problem that I
> didn't expect:
> Squid reports the client's IP to my internet gateway instead of their
> own IP.

Your Squid should be using its own machines default IP to connect at the
TCP level, and you have "forwarded_for off" already to prevent it adding
the X-Forwarded-For header.

Maybe HAProxy is adding it to the headers still. But I do not see the
config option that is supposed to need in your haproxy.cnf

Maybe the server is getting the info some other way directly from the
client?


> 
> squid.conf
> acl localnet src 192.168.12.1-192.168.13.254# my clients IP's
> acl localnet src 192.168.11.80# haproxy IP
> 
...

> # implementation of core access policies
> proxy_protocol_access allow localnet

Careful. Since localnet includes your client IPs this means clients can
connect directly to Squid and send forged PROXY details.

You should have another src ACL that matches only the HAProxy IP. Use
that here.

...
> forwarded_for off
> 


Either "transparent" or "delete" would seem to suite your needs better here.


Amos


From jmperrote at policia.rionegro.gov.ar  Fri Aug 16 12:04:55 2019
From: jmperrote at policia.rionegro.gov.ar (jmperrote)
Date: Fri, 16 Aug 2019 09:04:55 -0300
Subject: [squid-users] helping to setting a grace time for auth_param basic
Message-ID: <29633e10-5d0a-e8e7-b475-5bfebdb9f542@policia.rionegro.gov.ar>

Hello I need helping on setting "grace time" to renew TTL time on a " 
Check TTL" using "auth_param basic program and credentialsttl".

The problem is that I use a "auth_param basic" php script to validate 
users, when validate squid give me a TTL, but is no grace time to renew 
this TTL, if I still connected to the reverse proxy, only the TTL time 
for the user is renewed if click just on the last seconds of TTL counter.

 ??? auth_param basic program /etc/squid/auth/auth.php grace=10
 ??? auth_param basic credentialsttl 60 seconds

Cache Manager menu  <http://10.11.37.46/cgi-bin/cachemgr.cgi?host=10.11.37.46&port=3030&user_name=mesadeayuda at policia.rionegro.gov.ar&operation=menu&auth=>
	
	Cached Usernames: 2 of 7921
	Next Garbage Collection in 643 seconds.

	Type            State     Check TTL Cache TTL Username
	--------------- --------- --------- --------- ------------------------------
	AUTH_BASIC      Ok        58        3598      prueba

When the Check TTL time arrive to 1 if a refresh the client browser (connected to the reverse proxy) the TTL time renew to another 60 seconds, but is not refresh and arrive to 0 the
script PHP request login again. I looking for set a grace time that a user refresh betwen the las 10 seconds can refresh the TTL.

Regards.












-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20190816/0f65b5f5/attachment.htm>

From squid3 at treenet.co.nz  Fri Aug 16 13:28:02 2019
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Sat, 17 Aug 2019 01:28:02 +1200
Subject: [squid-users] help to disconnect users after determinated time.
 TTL
In-Reply-To: <137be43f-3d9c-5852-0705-2aae26b0413a@policia.rionegro.gov.ar>
References: <26473e6c-7b63-cc26-8f42-5587baaa78e4@policia.rionegro.gov.ar>
 <fca32691-03f2-eb6b-ed70-d25aded407c9@policia.rionegro.gov.ar>
 <03be5184-3a85-2a77-f148-dc90e56e195c@thalesgroup.com>
 <4bf2311a-98e2-529a-99a5-5835e709a6b2@policia.rionegro.gov.ar>
 <2d13100e-f4a7-3187-ecbe-67eb8d210310@thalesgroup.com>
 <137be43f-3d9c-5852-0705-2aae26b0413a@policia.rionegro.gov.ar>
Message-ID: <479201a6-d6f1-6cc2-76f1-8d2bda3e66e1@treenet.co.nz>

On 16/08/19 3:30 am, jmperrote wrote:
> Hello Emmanuel, we finish implementing a solution on PHP script, getting
> the TTL time < 0 on the cachemgr, and it work.
> 
> The problem is that the param --> auth_param basic credentialsttl 3
> minutes, give this time (180 seconds), but if the user still navigating
> on the site, this value
> 
> "Check TTL" is not renewing when the user is navigating, so if the user not aplly any click on the page just when the counter "Check TTL" is 0, the user counter go to < 0.
> 
> 
> It is posible introduce any param that tell to squid to renew the counter when a user is betwen the credentialsttl time and still navigating ?

credentiaslttl does not mean what you seem to think it does.

It is just an optimization to reduce the amount of lookups to the
helper. How often they are *checked*.

In your other thread you showed this report:

>
> 	Type            State     Check TTL Cache TTL Username
> 	--------------- --------- --------- ---------
------------------------------
> 	AUTH_BASIC      Ok        58        3598      prueba


Think of credentialsTTL ("Check TTL") hitting 0 as the start of that
"grace period". The cache garbage collection (Cache TTL) defines the end
- when the credentials are completely forgotten by Squid.

As you can see there is already a "grace period" of 3540 seconds on
these credentials.


As Emmanuel said you can fake a sort-of logout by having a custom helper
pretend the credentials have expired suddenly. But that is something
your helper does, not this TTL.

Keep in mind that while Squid is awaiting your helper response all new
HTTP requests using those credentials will be queued up waiting for its
response. When the helper responds its answer will be applied to all
those queued and all future requests until the next credentialsttl
period ends.

Amos


From tarotapprentice at yahoo.com  Sun Aug 18 14:06:26 2019
From: tarotapprentice at yahoo.com (TarotApprentice)
Date: Sun, 18 Aug 2019 14:06:26 +0000 (UTC)
Subject: [squid-users] caching apt package lists/Raspbian
References: <2134002003.589661.1566137186394.ref@mail.yahoo.com>
Message-ID: <2134002003.589661.1566137186394@mail.yahoo.com>

It turns out it still doesn't cache them the Packages.xz. From discussions over on the RaspberryPi forums it seems its hitting the following (this is just the Packages.xz) in order to match their main, contrib, non-free and rpi repos.

$ apt-get --print-uris update

'http://archive.raspberrypi.org/debian/dists/buster/main/binary-armhf/Packages.xz' 
archive.raspberrypi.org_debian_dists_buster_main_binary-armhf_Packages 0

'http://archive.raspberrypi.org/debian/dists/buster/main/binary-all/Packages.xz'
archive.raspberrypi.org_debian_dists_buster_main_binary-all_Packages 0 

'http://raspbian.raspberrypi.org/raspbian/dists/buster/main/binary-armhf/Packages.xz'
raspbian.raspberrypi.org_raspbian_dists_buster_main_binary-armhf_Packages 0 

'http://raspbian.raspberrypi.org/raspbian/dists/buster/main/binary-all/Packages.xz'
raspbian.raspberrypi.org_raspbian_dists_buster_main_binary-all_Packages 0

'http://raspbian.raspberrypi.org/raspbian/dists/buster/contrib/binary-armhf/Packages.xz'
raspbian.raspberrypi.org_raspbian_dists_buster_contrib_binary-armhf_Packages 0

'http://raspbian.raspberrypi.org/raspbian/dists/buster/contrib/binary-all/Packages.xz'
raspbian.raspberrypi.org_raspbian_dists_buster_contrib_binary-all_Packages 0

'http://raspbian.raspberrypi.org/raspbian/dists/buster/non-free/binary-armhf/Packages.xz'
raspbian.raspberrypi.org_raspbian_dists_buster_non-free_binary-armhf_Packages 0

'http://raspbian.raspberrypi.org/raspbian/dists/buster/non-free/binary-all/Packages.xz'
raspbian.raspberrypi.org_raspbian_dists_buster_non-free_binary-all_Packages 0

'http://raspbian.raspberrypi.org/raspbian/dists/buster/rpi/binary-armhf/Packages.xz' 
raspbian.raspberrypi.org_raspbian_dists_buster_rpi_binary-armhf_Packages 0 

'http://raspbian.raspberrypi.org/raspbian/dists/buster/rpi/binary-all/Packages.xz' 
raspbian.raspberrypi.org_raspbian_dists_buster_rpi_binary-all_Packages 0?

I thought I would paste these into redbot. A bunch of them are returning HTTP 404 - Not found responses. That first one for example gives a 404 response. Would that be enough to force squid to have to download the ones it can find each and every time? Any other suggestions on how to debug this?

MarkJ


From chirayu.patel at truecomtelesoft.com  Sun Aug 18 19:14:16 2019
From: chirayu.patel at truecomtelesoft.com (Chirayu Patel)
Date: Mon, 19 Aug 2019 00:44:16 +0530
Subject: [squid-users] Getting lot of client lifetime timeout and
 subsequently running out of file descriptors
Message-ID: <CAOhxsyyqoJ4fHeszb-ih9jgdUKZsCNW=SKX-_5Y9Uo9m6=KY_Q@mail.gmail.com>

Hi,

I am running squid version 4.6 and have set the file descriptors limit to
5000

I get an average of 1 lakh hits daily and in a day or 2 , I start getting
these messages :

Sun Aug 18 15:00:29 2019 daemon.notice squid[4906]: WARNING: Closing client
connection due to lifetime timeout
Sun Aug 18 15:00:29 2019 daemon.notice squid[4906]: 172.217.160.206:443
Sun Aug 18 15:00:32 2019 daemon.notice squid[4906]: WARNING: Closing client
connection due to lifetime timeout
Sun Aug 18 15:00:32 2019 daemon.notice squid[4906]: 52.114.158.52:443
Sun Aug 18 15:00:32 2019 daemon.notice squid[4906]: WARNING: Closing client
connection due to lifetime timeout
Sun Aug 18 15:00:32 2019 daemon.notice squid[4906]: 172.217.160.174:443
Sun Aug 18 15:00:36 2019 daemon.notice squid[4906]: WARNING: Closing client
connection due to lifetime timeout
Sun Aug 18 15:00:36 2019 daemon.notice squid[4906]: 172.217.160.174:443
Sun Aug 18 15:00:37 2019 daemon.notice squid[4906]: WARNING: Closing client
connection due to lifetime timeout
Sun Aug 18 15:00:37 2019 daemon.notice squid[4906]: 172.217.160.174:443
Sun Aug 18 15:00:41 2019 daemon.notice squid[4906]: WARNING: Closing client
connection due to lifetime timeout
Sun Aug 18 15:00:41 2019 daemon.notice squid[4906]: 172.217.160.174:443
Sun Aug 18 15:00:42 2019 daemon.notice squid[4906]: WARNING: Closing client
connection due to lifetime timeout
Sun Aug 18 15:00:42 2019 daemon.notice squid[4906]: 172.217.166.165:443
Sun Aug 18 15:00:44 2019 daemon.notice squid[4906]: WARNING: Closing client
connection due to lifetime timeout
Sun Aug 18 15:00:44 2019 daemon.notice squid[4906]: 52.37.239.109:443
Sun Aug 18 15:00:44 2019 daemon.notice squid[4906]: WARNING: Closing client
connection due to lifetime timeout
Sun Aug 18 15:00:44 2019 daemon.notice squid[4906]: 52.37.239.109:443
Sun Aug 18 15:00:47 2019 daemon.notice squid[4906]: WARNING: Closing client
connection due to lifetime timeout
Sun Aug 18 15:00:47 2019 daemon.notice squid[4906]: 52.37.239.109:443
Sun Aug 18 15:00:47 2019 daemon.notice squid[4906]: WARNING: Closing client
connection due to lifetime timeout
Sun Aug 18 15:00:47 2019 daemon.notice squid[4906]: 52.37.239.109:443
Sun Aug 18 15:00:48 2019 daemon.notice squid[4906]: WARNING: Closing client
connection due to lifetime timeout
Sun Aug 18 15:00:48 2019 daemon.notice squid[4906]: 52.37.239.109:443

------------------------------------------------------------------------------------------------

Squid Config :

http_port 3129 intercept
https_port 3131 intercept ssl-bump cert=/etc/ray/certificates/myCA.pem \
    generate-host-certificates=off dynamic_cert_mem_cache_size=2MB
## For Captive Portal
http_port 3132 intercept
https_port 3133 intercept ssl-bump cert=/etc/ray/certificates/myCA.pem \
    generate-host-certificates=off dynamic_cert_mem_cache_size=1MB

#sslcrtd_program /usr/lib/squid/ssl_crtd -s /var/lib/ssl_db -M 4MB
#sslcrtd_children 5

# TLS/SSL bumping definitions
acl tls_s1_connect at_step SslBump1
acl tls_s2_client_hello at_step SslBump2
acl tls_s3_server_hello at_step SslBump3

# TLS/SSL bumping steps
ssl_bump peek tls_s1_connect all # peek at TLS/SSL connect data
ssl_bump splice all # splice: no active bumping
on_unsupported_protocol tunnel all

pinger_enable off
digest_generation off
netdb_filename none
ipcache_size 128
fqdncache_size 128
via off
forwarded_for transparent
httpd_suppress_version_string on
cache deny all
cache_mem 0 MB
memory_pools off
shutdown_lifetime 0 seconds

#logfile_daemon /dev/null
access_log none

#acl good_url dstdomain .yahoo.com
http_access allow all

url_rewrite_program /tmp/squid/urlcat_server_start.sh
#url_rewrite_bypass on
url_rewrite_children 1 startup=1 idle=1 concurrency=30 queue-size=10000
on-persistent-overload=ERR
#url_rewrite_access allow all
#url_rewrite_extras "%>a/%>A %un %>rm bump_mode=%ssl::bump_mode
sni=\"%ssl::>sni\" referer=\"%{Referer}>h\""
url_rewrite_extras "%>a %lp %ssl::>sni"

max_filedesc 5120
coredump_dir /tmp

-----------------------------------------------------------------------

1. Should i decrease the client_lifetime ? Or should i increase the File
Descriptor limit ? or adjust the timeouts

2. Also, there is a steady increase of memory on the device.. Squid is
currently installed on an Access Point which is  a resource constrained
device.. Is there any way to control it..

--
Thank You
Chirayu Patel
Truecom Telesoft
+91 8758484287
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20190819/37e20dce/attachment.htm>

From chip_pop at hotmail.com  Sun Aug 18 22:30:21 2019
From: chip_pop at hotmail.com (joseph)
Date: Sun, 18 Aug 2019 17:30:21 -0500 (CDT)
Subject: [squid-users] caching apt package lists/Raspbian
In-Reply-To: <2134002003.589661.1566137186394@mail.yahoo.com>
References: <2134002003.589661.1566137186394@mail.yahoo.com>
Message-ID: <1566167421285-0.post@n4.nabble.com>

look at
http://raspbian.raspberrypi.org/raspbian/dists/buster/contrib/

binary-all  dose not exist lol 404 response normal

squid has nothing to do with it so test your link wen you have problem
without squid
squid can not cache ghost



-----
************************** 
***** Crash to the future  ****
**************************
--
Sent from: http://squid-web-proxy-cache.1019090.n4.nabble.com/Squid-Users-f1019091.html


From squid3 at treenet.co.nz  Mon Aug 19 01:57:39 2019
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Mon, 19 Aug 2019 13:57:39 +1200
Subject: [squid-users] Getting lot of client lifetime timeout and
 subsequently running out of file descriptors
In-Reply-To: <CAOhxsyyqoJ4fHeszb-ih9jgdUKZsCNW=SKX-_5Y9Uo9m6=KY_Q@mail.gmail.com>
References: <CAOhxsyyqoJ4fHeszb-ih9jgdUKZsCNW=SKX-_5Y9Uo9m6=KY_Q@mail.gmail.com>
Message-ID: <b455b2d8ffc4f3e8cfdda59c5022320e@treenet.co.nz>

On 2019-08-19 07:14, Chirayu Patel wrote:
> Hi,
> 
> I am running squid version 4.6 and have set the file descriptors limit
> to 5000
> 
> I get an average of 1 lakh hits daily and in a day or 2 , I start
> getting these messages :
> 
> Sun Aug 18 15:00:29 2019 daemon.notice squid[4906]: WARNING: Closing
> client connection due to lifetime timeout
...
> 
> 1. Should i decrease the client_lifetime ? Or should i increase the
> File Descriptor limit ? or adjust the timeouts
> 

I would be looking into why these clients are having such long 
connection times that they hit the 1 day lifetime limit.

If they do not actually need connections open for that whole time, then 
lowering the client_lifetime is safe enough. You will still get these 
log entries, no avoiding that since the clients are holding onto 
connections.

You may be able to raise the FD limits as well. On a limited devices I 
would pair that with lowering the read_ahead_gap to 8 KB.


> 2. Also, there is a steady increase of memory on the device.. Squid is
> currently installed on an Access Point which is  a resource
> constrained device.. Is there any way to control it..
> 

Steadily increasing number of very log-lived client connections is 
likely th esource of that, each will be using up to 256KB of I/O 
buffers. Preventing too many connections in parallel is the best thing 
for that, so you are on the right track trying to remove/reduce these 
ones hitting the lifetime.


Amos


From eli at ChinaBuckets.com  Mon Aug 19 02:41:33 2019
From: eli at ChinaBuckets.com (Eliza)
Date: Mon, 19 Aug 2019 10:41:33 +0800
Subject: [squid-users] squid for live streaming
Message-ID: <8547e87c-327a-5629-b671-0175c6ea46eb@ChinaBuckets.com>

Hello,

Is there any guide for squid as a live steaming proxy? such as RTMP 
protocal etc.

Thanks.


From squid3 at treenet.co.nz  Mon Aug 19 06:03:40 2019
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Mon, 19 Aug 2019 18:03:40 +1200
Subject: [squid-users] squid for live streaming
In-Reply-To: <8547e87c-327a-5629-b671-0175c6ea46eb@ChinaBuckets.com>
References: <8547e87c-327a-5629-b671-0175c6ea46eb@ChinaBuckets.com>
Message-ID: <2a01fb3e-7f5f-86ee-5558-fd5e7f3450c1@treenet.co.nz>

On 19/08/19 2:41 pm, Eliza wrote:
> Hello,
> 
> Is there any guide for squid as a live steaming proxy? such as RTMP
> protocal etc.
> 

Squid only supports HTTP or ICY (SHOUTcast) streaming. There is nothing
special to configure for those to work.

Some RTMP/RTSP clients support tunneling through HTTP proxies. Squid
supports those in its role as the HTTP proxy, but you may need to
identify and add the relevant port(s) to Squid's SSL_Ports and/or
Safe_ports ACLs for their tunnels to be permitted.

Amos


From squid3 at treenet.co.nz  Mon Aug 19 06:43:43 2019
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Mon, 19 Aug 2019 18:43:43 +1200
Subject: [squid-users] caching apt package lists/Raspbian
In-Reply-To: <2134002003.589661.1566137186394@mail.yahoo.com>
References: <2134002003.589661.1566137186394.ref@mail.yahoo.com>
 <2134002003.589661.1566137186394@mail.yahoo.com>
Message-ID: <bb2f0bba-d1ce-2bc7-6345-de104eb2c0c5@treenet.co.nz>

On 19/08/19 2:06 am, TarotApprentice wrote:
> It turns out it still doesn't cache them the Packages.xz. From
> discussions over on the RaspberryPi forums it seems its hitting the
> following (this is just the Packages.xz) in order to match their
> main, contrib, non-free and rpi repos.
> 
> $ apt-get --print-uris update
> 
...

> 
> I thought I would paste these into redbot. A bunch of them are
> returning HTTP 404 - Not found responses. That first one for example
> gives a 404 response. Would that be enough to force squid to have to
> download the ones it can find each and every time?

For those ones yes. As redbot says Squid is allowed to cache them, but
not to use the resulting HIT later. Unless something exceptional occurs
like not being able to contact the origin for an update.


> Any other suggestions on how to debug this?
> 

You can try "debug_options 11,2 22,3 20,3" and see what is going on for
the ones that are not 404.

Amos


From eli at ChinaBuckets.com  Mon Aug 19 09:02:07 2019
From: eli at ChinaBuckets.com (Eliza)
Date: Mon, 19 Aug 2019 17:02:07 +0800
Subject: [squid-users] squid for live streaming
In-Reply-To: <2a01fb3e-7f5f-86ee-5558-fd5e7f3450c1@treenet.co.nz>
References: <8547e87c-327a-5629-b671-0175c6ea46eb@ChinaBuckets.com>
 <2a01fb3e-7f5f-86ee-5558-fd5e7f3450c1@treenet.co.nz>
Message-ID: <3af79e4c-6cda-7697-5d03-ea2d93536e31@ChinaBuckets.com>

Hi amos,

on 2019/8/19 14:03, Amos Jeffries wrote:
> Squid only supports HTTP or ICY (SHOUTcast) streaming. There is nothing
> special to configure for those to work.
> 
> Some RTMP/RTSP clients support tunneling through HTTP proxies. Squid
> supports those in its role as the HTTP proxy, but you may need to
> identify and add the relevant port(s) to Squid's SSL_Ports and/or
> Safe_ports ACLs for their tunnels to be permitted.

got it. thank you.


From twk at ncsu.edu  Mon Aug 19 17:58:07 2019
From: twk at ncsu.edu (Tom Karches)
Date: Mon, 19 Aug 2019 13:58:07 -0400
Subject: [squid-users] Problems with squid 3.1 to 3.3 upgrade
In-Reply-To: <83017fd5-bfba-d93d-c8b1-c4810cccfe4f@measurement-factory.com>
References: <mailman.4874.1565301757.3120.squid-users@lists.squid-cache.org>
 <CAGZ9WNoGhDbUuGWgtGJuLG=+zEkTcY-TfdyJmh+bSV+SjeN+EA@mail.gmail.com>
 <90fb19d7-e183-c1fb-c960-25a28447023b@measurement-factory.com>
 <CAGZ9WNrE_r22_KjrFqJfSBb6HDvCLkjkyq8bjYBewoO=4t-8iw@mail.gmail.com>
 <a06ad34f-1151-afdd-8a49-98a2d2b5c418@measurement-factory.com>
 <CAGZ9WNqarg5adAoG=zTYOLn6QWeCBQK2y9jJ7HbtLC9v06fQug@mail.gmail.com>
 <83017fd5-bfba-d93d-c8b1-c4810cccfe4f@measurement-factory.com>
Message-ID: <CAGZ9WNqMLKQRCHBmX4BSt8E2QWqNzW8oZcyYwZm-60Dig9Q24g@mail.gmail.com>

I thought that this was being caused by a firewall because I was seeing
(requests go out, but blocked coming back) so I had the ports over 1024
opened :

No.     Time           Source                Destination           Protocol
Length Info
    156 16.493132      152.7.114.135         140.211.169.196       TCP
 74     50994 ? 443 [SYN] Seq=0 Win=29200 Len=0 MSS=1460 SACK_PERM=1
TSval=2876033516 TSecr=0 WS=128
    157 16.493333      140.211.169.196       152.7.114.135         TCP
 60     443 ? 50994 [RST, ACK] Seq=1 Ack=1 Win=29200 Len=0
    158 16.493440      152.7.114.135         152.19.134.198        TCP
 74     45744 ? 443 [SYN] Seq=0 Win=29200 Len=0 MSS=1460 SACK_PERM=1
TSval=2876033516 TSecr=0 WS=128
    159 16.493626      152.19.134.198        152.7.114.135         TCP
 60     443 ? 45744 [RST, ACK] Seq=1 Ack=1 Win=29200 Len=0

but now it is still failing and I am also seeing this in access.log. It
looks like connections are going out but not coming back :

2019/08/19 13:17:38.576 kid1| 33,2| client_side.cc(817) swanSong: local=
152.7.114.135:3128 remote=10.50.54.22:57230 flags=1
2019/08/19 13:17:52.050 kid1| 33,2| client_side.cc(817) swanSong: local=
152.7.114.135:3128 remote=10.50.54.21:48557 flags=1
2019/08/19 13:17:53.608 kid1| 33,2| client_side.cc(817) swanSong: local=
152.7.114.135:3128 remote=10.50.54.22:57330 flags=1
2019/08/19 13:18:07.053 kid1| 33,2| client_side.cc(817) swanSong: local=
152.7.114.135:3128 remote=10.50.54.21:48651 flags=1
2019/08/19 13:18:08.725 kid1| 33,2| client_side.cc(817) swanSong: local=
152.7.114.135:3128 remote=10.50.54.22:57426 flags=12

Do you see anything else relevant in here?

Thanks,
Tom

On Sat, Aug 10, 2019 at 1:57 PM Alex Rousskov <
rousskov at measurement-factory.com> wrote:

> On 8/9/19 4:32 PM, Tom Karches wrote:
> > Right now my debug is set to ALL,1 33,2. Is there a better set of
> > options to provide me more visibility of what might be wrong?
>
> In theory, yes: You can increase verbosity levels to see exactly what is
> going on. However, most people get lost in the debugging noise. FWIW, I
> do not recommend using cache.log above ALL,1 (the default) for triaging
> connectivity problems like yours by Squid newbies like you. Most likely,
> the connectivity problem is outside Squid and can be seen/reproduced
> outside Squid.
>
> I would use Wireshark, tcpdump, or a similar packet-level tool to figure
> out where Squid is trying to connect and from what address Squid is
> trying to connect. If you are not familiar with those basic tools, any
> capable local sysadmin can help you get started -- no Squid knowledge is
> needed!
>
> Alex.
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>


-- 
Thomas Karches
NCSU OIT CSI - Systems Specialist
M.E Student - Technology Education
Hillsborough 319 / 919.515.5508
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20190819/7998d77e/attachment.htm>

From rousskov at measurement-factory.com  Mon Aug 19 18:29:07 2019
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Mon, 19 Aug 2019 14:29:07 -0400
Subject: [squid-users] Problems with squid 3.1 to 3.3 upgrade
In-Reply-To: <CAGZ9WNqMLKQRCHBmX4BSt8E2QWqNzW8oZcyYwZm-60Dig9Q24g@mail.gmail.com>
References: <mailman.4874.1565301757.3120.squid-users@lists.squid-cache.org>
 <CAGZ9WNoGhDbUuGWgtGJuLG=+zEkTcY-TfdyJmh+bSV+SjeN+EA@mail.gmail.com>
 <90fb19d7-e183-c1fb-c960-25a28447023b@measurement-factory.com>
 <CAGZ9WNrE_r22_KjrFqJfSBb6HDvCLkjkyq8bjYBewoO=4t-8iw@mail.gmail.com>
 <a06ad34f-1151-afdd-8a49-98a2d2b5c418@measurement-factory.com>
 <CAGZ9WNqarg5adAoG=zTYOLn6QWeCBQK2y9jJ7HbtLC9v06fQug@mail.gmail.com>
 <83017fd5-bfba-d93d-c8b1-c4810cccfe4f@measurement-factory.com>
 <CAGZ9WNqMLKQRCHBmX4BSt8E2QWqNzW8oZcyYwZm-60Dig9Q24g@mail.gmail.com>
Message-ID: <59a4ad33-b119-a16d-ca7d-7eeb0ed1ac04@measurement-factory.com>

On 8/19/19 1:58 PM, Tom Karches wrote:

> It looks like connections are going out but not coming back :
> 
> 2019/08/19 13:17:38.576 kid1| 33,2| client_side.cc(817) swanSong:
> local=152.7.114.135:3128 remote=10.50.54.22:57230 flags=1
> 2019/08/19 13:17:52.050 kid1| 33,2| client_side.cc(817) swanSong:
> local=152.7.114.135:3128 remote=10.50.54.21:48557 flags=1
...
> 2019/08/19 13:18:08.725 kid1| 33,2| client_side.cc(817) swanSong:
> local=152.7.114.135:3128 remote=10.50.54.22:57426 flags=12

> Do you see anything else relevant in here?

No, I do not. These cache.log records are normal/benign AFAICT. See my
earlier warning about using non-default cache.log debugging in your
specific case.

Alex.


> On Sat, Aug 10, 2019 at 1:57 PM Alex Rousskov wrote:
> 
>     On 8/9/19 4:32 PM, Tom Karches wrote:
>     > Right now my debug is set to?ALL,1 33,2. Is there a better set of
>     > options to provide me more visibility of what might be wrong?
> 
>     In theory, yes: You can increase verbosity levels to see exactly what is
>     going on. However, most people get lost in the debugging noise. FWIW, I
>     do not recommend using cache.log above ALL,1 (the default) for triaging
>     connectivity problems like yours by Squid newbies like you. Most likely,
>     the connectivity problem is outside Squid and can be seen/reproduced
>     outside Squid.
> 
>     I would use Wireshark, tcpdump, or a similar packet-level tool to figure
>     out where Squid is trying to connect and from what address Squid is
>     trying to connect. If you are not familiar with those basic tools, any
>     capable local sysadmin can help you get started -- no Squid knowledge is
>     needed!
> 
>     Alex.
>     _______________________________________________
>     squid-users mailing list
>     squid-users at lists.squid-cache.org
>     <mailto:squid-users at lists.squid-cache.org>
>     http://lists.squid-cache.org/listinfo/squid-users
> 
> 
> 
> -- 
> Thomas Karches
> NCSU OIT CSI -?Systems Specialist
> M.E Student - Technology Education
> Hillsborough 319 / 919.515.5508
> 
> 
> 
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
> 



From ilari.laitinen at iki.fi  Tue Aug 20 13:12:12 2019
From: ilari.laitinen at iki.fi (Ilari Laitinen)
Date: Tue, 20 Aug 2019 16:12:12 +0300
Subject: [squid-users] Linearly increasing delays in HTTPS proxy CONNECTS /
	3.5.20
Message-ID: <592B861A-9199-4497-8BFF-9F253F62AFBA@iki.fi>

Hi!

I am experiencing a slowdown between CONNECT requests and corresponding "200 Connection established" responses. This happens when performance testing a load-balanced pair of squid servers with hundreds of simultaneous, independent, proxied HTTPS requests per second per server.

Most new connections get delayed by an increasing amount of time, raising often to more than 5 seconds. This obviously fails the performance test. Existing connections seem to be unaffected. The problematic situation lasts for a couple of seconds with strikingly linearly increasing delays. Expected operation resumes afterwards with a notable spike in traffic. Sometimes the delay stays at a specific level for a couple of seconds before the issue resolves itself.

squid 3.5.20 on CentOS 7

I have recorded the traffic using tcpdump and it boils down to the following. Everything else is very, very fast even during these slowdown periods.


Legend:
?source? is one of the servers where the load test originates
?squid? is the server where squid runs
?target? is a cloud service

source ?> squid [SYN]
source <? squid [SYN, ACK]
source ?> squid [ACK]
source ?> squid [CONNECT target:443 HTTP/1.1]
source <? squid [ACK]

[Unexpected delay here]

squid ?> target [SYN]
squid <? target [SYN, ACK]
squid ?> target [ACK]
source <? squid [HTTP/1.1 200 Connection established]
source ?> squid [ACK]

[Rest of the connection here without such delays]


System load stays at comfortable level (aroung 0.6 even while experiencing the issue), memory is not an issue. From 
SNMP data, I noticed small but consistent spikes in squid's disk cache usage coinciding with the issue at hand. This seemed strange, given there was no other traffic during the tests and proxied HTTPS means there's nothing to cache (right?). I nevertheless tried switching the cache from ufs to aufs and also using the no-cache option with ufs. Didn?t help. (And the spikes remained?)

I tried increasing the net.ipv4.ip_local_port_range sysctl value (it was set to default). That didn?t help, either.

The servers are located in a IPv4-only local network. Every outgoing request is supposed to be IPv4. The servers do have IPv6 interfaces but there is no traffic at all. Squid periodically queries AAAA records. Is it possible that new connections get queued while squid is busy trying to use IPv6 after receiving the new AAAAs? I have very little control over the environment. Is dns_v4_first worth a try in my scenario?

Is there something I?m missing?
What should I look into next? Could setting up "workers N? help, for example?

Best Regards,

-- 
Ilari Laitinen



From rousskov at measurement-factory.com  Tue Aug 20 14:23:05 2019
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Tue, 20 Aug 2019 10:23:05 -0400
Subject: [squid-users] Linearly increasing delays in HTTPS proxy
 CONNECTS / 3.5.20
In-Reply-To: <592B861A-9199-4497-8BFF-9F253F62AFBA@iki.fi>
References: <592B861A-9199-4497-8BFF-9F253F62AFBA@iki.fi>
Message-ID: <d5759d03-ead0-c441-029f-288955ec4cd9@measurement-factory.com>

On 8/20/19 9:12 AM, Ilari Laitinen wrote:

> I am experiencing a slowdown between CONNECT requests and corresponding "200 Connection established" responses.

I assume you are not using any SslBump features but please correct me if
I am wrong.


> source ?> squid [SYN]
> source <? squid [SYN, ACK]
> source ?> squid [ACK]
> source ?> squid [CONNECT target:443 HTTP/1.1]
> source <? squid [ACK]
> 
> [Unexpected delay here]
> 
> squid ?> target [SYN]
> squid <? target [SYN, ACK]
> squid ?> target [ACK]
> source <? squid [HTTP/1.1 200 Connection established]
> source ?> squid [ACK]
> 
> [Rest of the connection here without such delays]


> I noticed small but consistent spikes in squid's disk cache usage
> coinciding with the issue at hand. This seemed strange, given there
> was no other traffic during the tests and proxied HTTPS means there's
> nothing to cache (right?).

Correct. To avoid suspecting disks, configure Squid to log to a
RAM-based partition and remove cache_dirs [until you solve the problem].


> The servers are located in a IPv4-only local network. Every outgoing
> request is supposed to be IPv4. The servers do have IPv6 interfaces
> but there is no traffic at all. Squid periodically queries AAAA
> records. Is it possible that new connections get queued while squid
> is busy trying to use IPv6 after receiving the new AAAAs?

If "no traffic at all" means "zero IPv6 packets", then it is not
possible. Otherwise, it is possible (only the latest Squid (i.e. future
v5) does not have this kind of problem).


> I have very little control over the environment. Is dns_v4_first
> worth a try in my scenario?

It is not a reliable solution, but it would not hurt as far as
performance is concerned.


> What should I look into next?

1. Check system logs.

2. Check atop output while the problem is present. If this is a resource
bottleneck, atop may expose it.

3. If there is IPv6 traffic, to eliminate IPv6 as a suspect, you can
disable IPv6 on the box, use Squid built without IPv6 support, or even
use a DNS forwarder that, for example, rejects all AAAA queries. All
these solutions can and should be validated by examining actual IPv6
traffic. And none of them are needed if there is no IPv6 traffic at all.

4. With delays ranging into _seconds_ it should be fairly easy for a
capable Squid developer to figure out what your Squid is doing by
looking at Squid debugging logs. You can post a link to compressed
cache.log here for analysis, but you should first simplify your workload
so that it has CONNECT tunnels and nothing else (if you have not
already) and enable debugging when the problem is present (e.g., use
"squid -k debug" although it is currently better to send the right
signal manually).


> Could setting up "workers N? help, for example?

The answer depends on your definition of "help": Large number of workers
may mask the problem to the point where it no longer bothers you, but I
would not make the setup a lot more complex until you know where the
current bottleneck is. In fact, I would go into the opposite direction
of making the setup as simple as possible!



HTH,

Alex.


From squid-user at tlinx.org  Wed Aug 21 03:51:59 2019
From: squid-user at tlinx.org (L A Walsh)
Date: Tue, 20 Aug 2019 20:51:59 -0700
Subject: [squid-users] TCP_MISS_ABORTED/503 - -Squid-Error: ERR_DNS_FAIL 0
Message-ID: <5D5CBFDF.7050308@tlinx.org>

Pulled this out of my log.  Downloading lots of files through squid has
the download aborting after about 3k files.  This is the first I've seen
that there's also an associated ERR_DNS_FAIL -- is that a message from
squid's internal resolver? 

1566304848.234      1 192.168.3.1 TCP_MISS_ABORTED/503 4185 GET
http://download.
opensuse.org/tumbleweed/repo/src-oss/src/gdouros-aegean-fonts-9.78-1.6.src.rpm
- HIER_NONE/- text/html [Referer:
http://download.opensuse.org/tumbleweed/repo/src-oss/src/\r\nUser-Agent:
openSuSE_Client\r\nAccept: */*\r\nAccept-Encoding:
identity\r\nConnection: Keep-Alive\r\nProxy-Connection:
Keep-Alive\r\nHost: download.opensuse.org\r\n] [HTTP/1.1 503 Service
Unavailable\r\nServer: squid/4.0.25\r\nMime-Version: 1.0\r\nDate: Tue,
20 Aug 2019 12:40:48 GMT\r\nContent-Type:
text/html;charset=utf-8\r\nContent-Length: 4163\r\nX-Squid-Error:
ERR_DNS_FAIL 0\r\nContent-Language: en\r\n\r]

One thing -- all the files were being downloaded in 1 copy of wget, so
it was
a long connection.

If I don't go through the proxy, it does download, but its hard to see
why DNS
would ahve a problem only 4 hosts are accessed in the download:


   1027 http://download.opensuse.org
      2 http://ftp1.nluug.nl
   2030 http://mirror.sfo12.us.leaseweb.net
     14 http://mirrors.acm.wpi.edu

3073...

    Looks like it's exactly 3k lookups and death on 3k+1

ring any bells?

Thanks






From eli at ChinaBuckets.com  Wed Aug 21 03:56:46 2019
From: eli at ChinaBuckets.com (Eliza)
Date: Wed, 21 Aug 2019 11:56:46 +0800
Subject: [squid-users] TCP_MISS_ABORTED/503 - -Squid-Error: ERR_DNS_FAIL
	0
In-Reply-To: <5D5CBFDF.7050308@tlinx.org>
References: <5D5CBFDF.7050308@tlinx.org>
Message-ID: <0180a87d-f66a-be08-5dce-53e466dc6e45@ChinaBuckets.com>

Hi

on 2019/8/21 11:51, L A Walsh wrote:
> Pulled this out of my log.  Downloading lots of files through squid has
> the download aborting after about 3k files.  This is the first I've seen
> that there's also an associated ERR_DNS_FAIL -- is that a message from
> squid's internal resolver?

Squid may cache the wrong DNS hosts. can you restart squid to give 
another try?

regards.


From squid-user at tlinx.org  Wed Aug 21 04:11:05 2019
From: squid-user at tlinx.org (L A Walsh)
Date: Tue, 20 Aug 2019 21:11:05 -0700
Subject: [squid-users] TCP_MISS_ABORTED/503 - -Squid-Error: ERR_DNS_FAIL
	0
In-Reply-To: <0180a87d-f66a-be08-5dce-53e466dc6e45@ChinaBuckets.com>
References: <5D5CBFDF.7050308@tlinx.org>
 <0180a87d-f66a-be08-5dce-53e466dc6e45@ChinaBuckets.com>
Message-ID: <5D5CC459.3080703@tlinx.org>

On 2019/08/20 20:56, Eliza wrote:
> Hi
>
> on 2019/8/21 11:51, L A Walsh wrote:
>   
>> Pulled this out of my log.  Downloading lots of files through squid has
>> the download aborting after about 3k files.  This is the first I've seen
>> that there's also an associated ERR_DNS_FAIL -- is that a message from
>> squid's internal resolver?
>>     
>
> Squid may cache the wrong DNS hosts. can you restart squid to give 
> another try?
>   
This wasn't just 1 time, but seems to happen each time I try to download
a distro update through squid -- and squid has been restarted multiple
times over
that period -- seems I have set something somewhere to a 3k size.

had 1k for the FQDN in the squid.conf dns section.  but that seems real

This may not be a problem in squid at all -- DNS, my config that's been the
same for months, its just that that's where I'm seeing the evidence of an
error, but it could easily be someplace else.


Was sorta hoping that someone had seen this symptom and had a clue, but
otherwise, I'll try other things.;




From squid3 at treenet.co.nz  Wed Aug 21 11:41:40 2019
From: squid3 at treenet.co.nz (Amos Jeffries)
Date: Wed, 21 Aug 2019 23:41:40 +1200
Subject: [squid-users] TCP_MISS_ABORTED/503 - -Squid-Error: ERR_DNS_FAIL
	0
In-Reply-To: <5D5CBFDF.7050308@tlinx.org>
References: <5D5CBFDF.7050308@tlinx.org>
Message-ID: <43edef18-288e-0bb4-cd9a-f8e55714892d@treenet.co.nz>

On 21/08/19 3:51 pm, L A Walsh wrote:
> Pulled this out of my log.  Downloading lots of files through squid has
> the download aborting after about 3k files.  This is the first I've seen
> that there's also an associated ERR_DNS_FAIL -- is that a message from
> squid's internal resolver?

Indirectly. It will be the result of DNS failure on the domain in that
particular transactions URL.

You mentioned the ABORTED was repeatable. If this DNS_FAIL was not
always present, what was?


> 
> 1566304848.234      1 192.168.3.1 TCP_MISS_ABORTED/503 4185 GET
> http://download.
> opensuse.org/tumbleweed/repo/src-oss/src/gdouros-aegean-fonts-9.78-1.6.src.rpm
> - HIER_NONE/- text/html [Referer:
> http://download.opensuse.org/tumbleweed/repo/src-oss/src/\r\nUser-Agent:
> openSuSE_Client\r\nAccept: */*\r\nAccept-Encoding:
> identity\r\nConnection: Keep-Alive\r\nProxy-Connection:
> Keep-Alive\r\nHost: download.opensuse.org\r\n] [HTTP/1.1 503 Service
> Unavailable\r\nServer: squid/4.0.25\r\nMime-Version: 1.0\r\nDate: Tue,

Please upgrade your Squid. That is a beta release. Squid-4 has been in
stable/production releases for over a year now.


> 20 Aug 2019 12:40:48 GMT\r\nContent-Type:
> text/html;charset=utf-8\r\nContent-Length: 4163\r\nX-Squid-Error:
> ERR_DNS_FAIL 0\r\nContent-Language: en\r\n\r]
> 
> One thing -- all the files were being downloaded in 1 copy of wget, so
> it was
> a long connection.

This makes me suspect the timing may be important. Check if these 3k
transactions all terminated the same amount of time after their TCP
connection was opened by the client.
 If they are or very close to similar timing before the end. Look for
somewhere that may be timing out (could be Squid, or the TCP stack or
any routers along the way.


> 
> If I don't go through the proxy, it does download, but its hard to see
> why DNS
> would ahve a problem only 4 hosts are accessed in the download:
> 

The hostname needs to be looked up on every request (HTTP being
stateless). The Squid internal resolver does cache names for the DNS
provided TTL, when that expires they may need to be re-resolved and hit
a failure then.


> 
>    1027 http://download.opensuse.org
>       2 http://ftp1.nluug.nl
>    2030 http://mirror.sfo12.us.leaseweb.net
>      14 http://mirrors.acm.wpi.edu
> 
> 3073...
> 
>     Looks like it's exactly 3k lookups and death on 3k+1
> 
> ring any bells?

There is nothing in Squid that is tied to a particular number of
transactions. An infinite number of HTTP/1.x requests may be processed
on a TCP connection.

It is more likely to be timing or bandwidth related. Some network level
thing. The error message "page content" Squid produces on that last
request each time would be useful info.

Amos


From squid-user at tlinx.org  Wed Aug 21 18:27:29 2019
From: squid-user at tlinx.org (L A Walsh)
Date: Wed, 21 Aug 2019 11:27:29 -0700
Subject: [squid-users] TCP_MISS_ABORTED/503 - -Squid-Error: ERR_DNS_FAIL
	0
In-Reply-To: <43edef18-288e-0bb4-cd9a-f8e55714892d@treenet.co.nz>
References: <5D5CBFDF.7050308@tlinx.org>
 <43edef18-288e-0bb4-cd9a-f8e55714892d@treenet.co.nz>
Message-ID: <5D5D8D11.8030708@tlinx.org>

On 2019/08/21 04:41, Amos Jeffries wrote:
> On 21/08/19 3:51 pm, L A Walsh wrote:
>   
>> Pulled this out of my log.  Downloading lots of files through squid has
>> the download aborting after about 3k files.  This is the first I've seen
>> that there's also an associated ERR_DNS_FAIL -- is that a message from
>> squid's internal resolver?
>>     
>
> Indirectly. It will be the result of DNS failure on the domain in that
> particular transactions URL.
>
> You mentioned the ABORTED was repeatable. If this DNS_FAIL was not
> always present, what was?
>   
----
    Well not several months ago, but in the past 1-2 seems like it.


>
>   
>> 1566304848.234      1 192.168.3.1 TCP_MISS_ABORTED/503 4185 GET
>> http://download.
>> opensuse.org/tumbleweed/repo/src-oss/src/gdouros-aegean-fonts-9.78-1.6.src.rpm
>> - HIER_NONE/- text/html [Referer:
>> http://download.opensuse.org/tumbleweed/repo/src-oss/src/\r\nUser-Agent:
>> openSuSE_Client\r\nAccept: */*\r\nAccept-Encoding:
>> identity\r\nConnection: Keep-Alive\r\nProxy-Connection:
>> Keep-Alive\r\nHost: download.opensuse.org\r\n] [HTTP/1.1 503 Service
>> Unavailable\r\nServer: squid/4.0.25\r\nMime-Version: 1.0\r\nDate: Tue,
>>     
>
> Please upgrade your Squid. That is a beta release. Squid-4 has been in
> stable/production releases for over a year now.
>   
----
    Yeah, just put together 4.8 and need to install, somehow I'm finding
it hard to believe it is in squid, it's just the piece that is bearing
the "bad news"...but probably pointing at something else.



>
>   
>> 20 Aug 2019 12:40:48 GMT\r\nContent-Type:
>> text/html;charset=utf-8\r\nContent-Length: 4163\r\nX-Squid-Error:
>> ERR_DNS_FAIL 0\r\nContent-Language: en\r\n\r]
>>
>> One thing -- all the files were being downloaded in 1 copy of wget, so
>> it was
>> a long connection.
>>     
>
> This makes me suspect the timing may be important. Check if these 3k
> transactions all terminated the same amount of time after their TCP
> connection was opened by the client.
>  If they are or very close to similar timing before the end. Look for
> somewhere that may be timing out (could be Squid, or the TCP stack or
> any routers along the way.
>   
----
    Don't see anything in the named logs either, but it maybe that it is
not recording these types of events, and my nssswitch for hosts does a
roll-over:

hosts:  files  dns wins

---
but wins isn't recording very much detail. 


>
>   
>> If I don't go through the proxy, it does download, but its hard to see
>> why DNS
>> would ahve a problem only 4 hosts are accessed in the download:
>>
>>     
>
> The hostname needs to be looked up on every request (HTTP being
> stateless). The Squid internal resolver does cache names for the DNS
> provided TTL, when that expires they may need to be re-resolved and hit
> a failure then.
>
>
>   
>>    1027 http://download.opensuse.org
>>       2 http://ftp1.nluug.nl
>>    2030 http://mirror.sfo12.us.leaseweb.net
>>      14 http://mirrors.acm.wpi.edu
>>
>> 3073...
>>
>>     Looks like it's exactly 3k lookups and death on 3k+1
>>
>> ring any bells?
>>     
>
> There is nothing in Squid that is tied to a particular number of
> transactions. An infinite number of HTTP/1.x requests may be processed
> on a TCP connection.
>
> It is more likely to be timing or bandwidth related. Some network level
> thing. The error message "page content" Squid produces on that last
> request each time would be useful info.
>   

---
    Everything was being fetched through wget, so no error page,
exactly, but here is the border in the log of last working to 1st non
working:
--2019-08-20 05:40:31-- 
http://download.opensuse.org/tumbleweed/repo/oss/x86_64/cmus-plugins-all-2.8.0~20190219.ge27e813-1.5.x86_64.rpm
Reusing existing connection to ishtar.sc.tlinx.org:8080.
Proxy request sent, awaiting response... 302 Found
Location:
http://mirror.sfo12.us.leaseweb.net/opensuse/tumbleweed/repo/oss/x86_64/cmus-plugins-all-2.8.0~20190219.ge27e813-1.5.x86_64.rpm
[following]
--2019-08-20 05:40:34-- 
http://mirror.sfo12.us.leaseweb.net/opensuse/tumbleweed/repo/oss/x86_64/cmus-plugins-all-2.8.0~20190219.ge27e813-1.5.x86_64.rpm
Reusing existing connection to ishtar.sc.tlinx.org:8080.
Proxy request sent, awaiting response... 304 Not Modified
File
?tumbleweed/repo/oss/x86_64/cmus-plugins-all-2.8.0~20190219.ge27e813-1.5.x86_64.rpm?
not modified on server. Omitting download.

--2019-08-20 05:40:35-- 
http://download.opensuse.org/tumbleweed/repo/oss/x86_64/cni-0.7.1-1.1.x86_64.rpm
Reusing existing connection to ishtar.sc.tlinx.org:8080.
Proxy request sent, awaiting response... 503 Service Unavailable
2019-08-20 05:40:45 ERROR 503: Service Unavailable.
------------------------------------------------------

So 3072 requests where I got that my connection to the proxy was being
"reused"
and most  of the requests were not modified from a previous run, saw a few
that actually downloaded, but they were in the minority.

Then a 503 -- and answer fills the rest of the log.  It's an alphabetical
download only into the 'c's. 


Well Need to try new squid binary and see if it does anything different.

If I find anything out, will post back.
Thanks all for responses.







From jagadeesh.am at hotmail.com  Thu Aug 22 22:21:48 2019
From: jagadeesh.am at hotmail.com (jagadeesh am)
Date: Thu, 22 Aug 2019 22:21:48 +0000
Subject: [squid-users] SQUID proxy to access web application from another
	subnet
In-Reply-To: <SN6PR10MB2574AA57D01884542C45C7959FA50@SN6PR10MB2574.namprd10.prod.outlook.com>
References: <SN6PR10MB2574AA57D01884542C45C7959FA50@SN6PR10MB2574.namprd10.prod.outlook.com>
Message-ID: <SN6PR10MB257421A71A3AE85A9BAC4A619FA50@SN6PR10MB2574.namprd10.prod.outlook.com>

Hello,

I have one query. Could you please suggest me what to do.

I have a requirement to access a web application running on Server 1 which is connected to Private network 192.168.2.2 network , from Client machine which is connected to Public network (16.x.x.x) using SQUID PROXY.

Is it possible to browse the web application running on server 1 from Client using SQUID?

If yes, could you please guide / suggest me how do I accomplish using SQUID.

Note: SQUID system is connected to both Public and Private network. SQUID is configured on Windows Server 2016.

[cid:image002.png at 01D5590E.37E61F20]



[cid:image003.png at 01D5590E.37E61F20]


Thanks,
Jagadeesh
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20190822/a64e82ef/attachment.htm>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: image002.png
Type: image/png
Size: 10316 bytes
Desc: image002.png
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20190822/a64e82ef/attachment.png>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: image003.png
Type: image/png
Size: 15018 bytes
Desc: image003.png
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20190822/a64e82ef/attachment-0001.png>

From Antony.Stone at squid.open.source.it  Thu Aug 22 22:26:01 2019
From: Antony.Stone at squid.open.source.it (Antony Stone)
Date: Fri, 23 Aug 2019 00:26:01 +0200
Subject: [squid-users] SQUID proxy to access web application from
	another subnet
In-Reply-To: <SN6PR10MB257421A71A3AE85A9BAC4A619FA50@SN6PR10MB2574.namprd10.prod.outlook.com>
References: <SN6PR10MB2574AA57D01884542C45C7959FA50@SN6PR10MB2574.namprd10.prod.outlook.com>
 <SN6PR10MB257421A71A3AE85A9BAC4A619FA50@SN6PR10MB2574.namprd10.prod.outlook.com>
Message-ID: <201908230026.01991.Antony.Stone@squid.open.source.it>

On Friday 23 August 2019 at 00:21:48, jagadeesh am wrote:

> Hello,
> 
> I have one query. Could you please suggest me what to do.

Read the documentation :)

> I have a requirement to access a web application running on Server 1 which
> is connected to Private network 192.168.2.2 network , from Client machine
> which is connected to Public network (16.x.x.x) using SQUID PROXY.
> 
> Is it possible to browse the web application running on server 1 from
> Client using SQUID?

Yes.

> If yes, could you please guide / suggest me how do I accomplish using
> SQUID.

https://wiki.squid-cache.org/SquidFaq/ReverseProxy

> Note: SQUID system is connected to both Public and Private network.

Good.

> SQUID is configured on Windows Server 2016.

Oh well, that'll probably work.


Antony.

-- 
What do you get when you cross a joke with a rhetorical question?

                                                   Please reply to the list;
                                                         please *don't* CC me.


From Randi_indrawan at app.co.id  Fri Aug 23 01:28:27 2019
From: Randi_indrawan at app.co.id (Randi Indrawan)
Date: Fri, 23 Aug 2019 01:28:27 +0000
Subject: [squid-users] AD user Login + Squid Proxy + Automatic Authentication
Message-ID: <SG2PR04MB3365EDFCEECED3EA45C2178CBDA40@SG2PR04MB3365.apcprd04.prod.outlook.com>

So I have setup a squid proxy on a CentOS 7 Server and now the authentication system uses ldap and it works, I can set which groups get access through a proxy

The problem is ... can we setup the proxy read the domain id that is being logged, so the proxy no longer asks for a username and password. All the tutorials I've seen are pop-up messages asking for the username and password. I would like this to happen automatically so when the user logs in they automatically authenticate
Best Regards
Randi Indrawan
DISCLAIMER : The information contained in this communication (including any attachments) is privileged and confidential, and may be legally exempt from disclosure under applicable law. It is intended only for the specific purpose of being used by the individual or entity to whom it is addressed. If you are not the addressee indicated in this message (or are responsible for delivery of the message to such person), you must not disclose, disseminate, distribute, deliver, copy, circulate, rely on or use any of the information contained in this transmission. We apologize if you have received this communication in error; kindly inform the sender accordingly. Please also ensure that this original message and any record of it is permanently deleted from your computer system. We do not give or endorse any opinions, conclusions and other information in this message that do not relate to our official business.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20190823/fdcec9a5/attachment.htm>

From rafael.akchurin at diladele.com  Fri Aug 23 04:30:08 2019
From: rafael.akchurin at diladele.com (Rafael Akchurin)
Date: Fri, 23 Aug 2019 04:30:08 +0000
Subject: [squid-users] AD user Login + Squid Proxy + Automatic
	Authentication
In-Reply-To: <SG2PR04MB3365EDFCEECED3EA45C2178CBDA40@SG2PR04MB3365.apcprd04.prod.outlook.com>
References: <SG2PR04MB3365EDFCEECED3EA45C2178CBDA40@SG2PR04MB3365.apcprd04.prod.outlook.com>
Message-ID: <AM0PR04MB47537A2757D2DC26A445E5B08FA40@AM0PR04MB4753.eurprd04.prod.outlook.com>

Hello Randi,

You seem to be wishing to setup Single-Sign-On. We have a small guide here that might be of some help. It is proven and it definitely works. It involves Microsoft AD and Kerberos https://docs.diladele.com/administrator_guide_stable/active_directory/index.html

The guide involves our Web Safety product but it should not really matter, the pristine Squid will do just fine. You can also use the community version of our UI for Squid, it is completely free so may also be helpful if you need browser management of your proxy box.

Best regards,
Rafael

From: squid-users <squid-users-bounces at lists.squid-cache.org> On Behalf Of Randi Indrawan
Sent: Thursday, August 22, 2019 6:28 PM
To: squid-users at lists.squid-cache.org
Subject: [squid-users] AD user Login + Squid Proxy + Automatic Authentication


So I have setup a squid proxy on a CentOS 7 Server and now the authentication system uses ldap and it works, I can set which groups get access through a proxy

The problem is ... can we setup the proxy read the domain id that is being logged, so the proxy no longer asks for a username and password. All the tutorials I've seen are pop-up messages asking for the username and password. I would like this to happen automatically so when the user logs in they automatically authenticate
Best Regards
Randi Indrawan
DISCLAIMER : The information contained in this communication (including any attachments) is privileged and confidential, and may be legally exempt from disclosure under applicable law. It is intended only for the specific purpose of being used by the individual or entity to whom it is addressed. If you are not the addressee indicated in this message (or are responsible for delivery of the message to such person), you must not disclose, disseminate, distribute, deliver, copy, circulate, rely on or use any of the information contained in this transmission. We apologize if you have received this communication in error; kindly inform the sender accordingly. Please also ensure that this original message and any record of it is permanently deleted from your computer system. We do not give or endorse any opinions, conclusions and other information in this message that do not relate to our official business.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20190823/77a3f87d/attachment.htm>

From belle at bazuin.nl  Fri Aug 23 07:30:20 2019
From: belle at bazuin.nl (=?windows-1252?Q?L.P.H._van_Belle?=)
Date: Fri, 23 Aug 2019 09:30:20 +0200
Subject: [squid-users] FW: AD user Login + Squid Proxy + Automatic
 Authentication
Message-ID: <vmime.5d5f960c.4cae.331bd254678d88cc@ms249-lin-003.rotterdam.bazuin.nl>

?




The most simple way to add SSO. 
?
Install winbind krb5-user, then?your smb.conf,? update this config : 
[global]
??? # Auth-Only setup with winbind. ( no Shares )
????log level = 1
??? workgroup = NTDOM
??? security = ADS
??? realm = YOUR-REALM
??? netbios name = HOSTNAME
?
??? preferred master = no
??? domain master = no
??? host msdfs = no
??? dns proxy = yes
?
??? interfaces = eth0 lo
??? bind interfaces only = yes
?
??? #Add and Update TLS Key
?# Add the root cert and clients certs here, add the rootCA with GPO to the pc's. 
??? tls enabled = yes
??? tls keyfile = /etc/ssl/private/HOSTNAME.key.pem
??? tls certfile = /etc/ssl/certs/HOSTNAME.cert.pem
??? tls cafile = /etc/ssl/certs/ROOT-ca.crt
?
??? ## map id's outside to domain to tdb files.
??? idmap config *: backend = tdb
??? idmap config *: range = 2000-9999
?
??? ## map ids from the domain and (*) the range may not overlap !
??? idmap config NTDOM : backend = rid
??? idmap config NTDOM : schema_mode = rfc2307
??? idmap config NTDOM : range = 10000-3999999
?
?# Samba 4.6+ ( get primary group from AD ) ( Samba AD-Backend )
??? #idmap config NTDOM : unix_nss_info = yes
?# Samba 4.6+ ( get primary group from unix primary group )
??? #idmap config NTDOM : unix_primary_group = yes
###########
?
??? kerberos method = secrets and keytab
??? dedicated keytab file = /etc/krb5.keytab
?
??? # renew the kerberos ticket
??? winbind refresh tickets = yes
?
??? # We strip the domain (NTDOM\username) to username
??? winbind use default domain = yes
?
??? # enable offline logins
??? winbind offline logon = yes
?
??? # check depth of nested groups, ! slows down you samba, if to much groups depth
??? # Not needed on the VPN server.
??? #winbind expand groups = 2
?
??? # user Administrator workaround, without it you are unable to set privileges
??? username map = /etc/samba/samba_usermapping
?
??? # disable usershares creating
??? usershare path =
?
??? # Disable printing completely
??? load printers = no
??? printing = bsd
??? printcap name = /dev/null
??? disable spoolss = yes
?
??? # For ACL support on member servers with shares, OBLIGATES
??? vfs objects = acl_xattr
??? map acl inherit = Yes
??? store dos attributes = Yes
?
######## SHARE DEFINITIONS ################

?
# Next TODO.? Join the AD-DC domain. 
kinit Administrator
net ads join 
?
# setup keytab for squid. 
?
export KRB5_KTNAME=FILE:/etc/squid/HTTP-$(hostname -s).keytab
net ads keytab ADD HTTP/$(hostname -f)
# check keytab file.
klist -ke /etc/squid/HTTP-$(hostname -s).keytab
unset KRB5_KTNAME
?
# set rights.
chgrp proxy /etc/squid/HTTP-$(hostname -s).keytab
chmod g+r /etc/squid/HTTP-$(hostname -s).keytab
?
and use this for auth in squid. 
### negotiate kerberos and ntlm authentication
auth_param negotiate program /usr/lib/squid/negotiate_wrapper_auth \
??? --kerberos /usr/lib/squid/negotiate_kerberos_auth -k /etc/squid/HTTP-hostname.keytab \
????? -s HTTP/hostname.fqdn at REALM \
??? --ntlm /usr/bin/ntlm_auth --helper-protocol=gss-spnego --domain=NTDOM
auth_param negotiate children 30 startup=5 idle=5
auth_param negotiate children 10
auth_param negotiate keep_alive on

If you serve multiple Kerberos realms add a HTTP/fqdn at REALM service principal per realm to
?????? the HTTP.keytab file and use the -s GSS_C_NO_NAME option with negotiate_kerberos_auth. ?
Greetz, 
?
Louis
?
?


Van: squid-users [mailto:squid-users-bounces at lists.squid-cache.org] Namens Randi Indrawan
Verzonden: vrijdag 23 augustus 2019 3:28
Aan: squid-users at lists.squid-cache.org
Onderwerp: [squid-users] AD user Login + Squid Proxy + Automatic Authentication




So I have setup a squid proxy on a CentOS 7 Server and now the authentication system uses ldap and it works, I can set which groups get access through a proxy

The problem is ... can we setup the proxy read the domain id that is being logged, so the proxy no longer asks for a username and password. All the tutorials I've seen are pop-up messages asking for the username and password. I would like this to happen automatically so when the user logs in they automatically authenticate

Best Regards

Randi Indrawan

DISCLAIMER : The information contained in this communication (including any attachments) is privileged and confidential, and may be legally exempt from disclosure under applicable law. It is intended only for the specific purpose of being used by the individual or entity to whom it is addressed. If you are not the addressee indicated in this message (or are responsible for delivery of the message to such person), you must not disclose, disseminate, distribute, deliver, copy, circulate, rely on or use any of the information contained in this transmission. We apologize if you have received this communication in error; kindly inform the sender accordingly. Please also ensure that this original message and any record of it is permanently deleted from your computer system. We do not give or endorse any opinions, conclusions and other information in this message that do not relate to our official business. 
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://lists.squid-cache.org/pipermail/squid-users/attachments/20190823/e0c1047b/attachment.htm>

From jmperrote at policia.rionegro.gov.ar  Wed Aug 28 10:59:21 2019
From: jmperrote at policia.rionegro.gov.ar (jmperrote)
Date: Wed, 28 Aug 2019 07:59:21 -0300
Subject: [squid-users] Help with IP forwarder on squid
Message-ID: <5ccdc292-84ed-aee7-cb7f-9decaf248fcf@policia.rionegro.gov.ar>

Hello we have a reverse proxy squid and on the backend a apache server 
with anti DDOS software.

Any request on the apache comming from the same ip of the reverse proxy 
because it is forwader to the apache backend.

We need that the apache server receip the original ip from client.

We try? the "forwarder_for on" directive at the top of squid.conf but 
not result.


regards.




From uhlar at fantomas.sk  Wed Aug 28 11:40:19 2019
From: uhlar at fantomas.sk (Matus UHLAR - fantomas)
Date: Wed, 28 Aug 2019 13:40:19 +0200
Subject: [squid-users] Help with IP forwarder on squid
In-Reply-To: <5ccdc292-84ed-aee7-cb7f-9decaf248fcf@policia.rionegro.gov.ar>
References: <5ccdc292-84ed-aee7-cb7f-9decaf248fcf@policia.rionegro.gov.ar>
Message-ID: <20190828114019.GB5374@fantomas.sk>

On 28.08.19 07:59, jmperrote wrote:
>Hello we have a reverse proxy squid and on the backend a apache server 
>with anti DDOS software.
>
>Any request on the apache comming from the same ip of the reverse 
>proxy because it is forwader to the apache backend.
>
>We need that the apache server receip the original ip from client.
>
>We try? the "forwarder_for on" directive at the top of squid.conf but 
>not result.

you must configure apache to accept that IP as the original IP.
I think it has mod_remoteip or something like that.

-- 
Matus UHLAR - fantomas, uhlar at fantomas.sk ; http://www.fantomas.sk/
Warning: I wish NOT to receive e-mail advertising to this address.
Varovanie: na tuto adresu chcem NEDOSTAVAT akukolvek reklamnu postu.
- Holmes, what kind of school did you study to be a detective?
- Elementary, Watkins.  -- Daffy Duck & Porky Pig


From jmperrote at policia.rionegro.gov.ar  Wed Aug 28 12:22:57 2019
From: jmperrote at policia.rionegro.gov.ar (jmperrote)
Date: Wed, 28 Aug 2019 09:22:57 -0300
Subject: [squid-users] Help with IP forwarder on squid
In-Reply-To: <20190828114019.GB5374@fantomas.sk>
References: <5ccdc292-84ed-aee7-cb7f-9decaf248fcf@policia.rionegro.gov.ar>
 <20190828114019.GB5374@fantomas.sk>
Message-ID: <c6f7efb6-8ce0-75b7-d063-55db14547fec@policia.rionegro.gov.ar>

Hello Matus thanks for the answer, but on the apache backend server we 
just receip request from the reverse proxy, and we mounted software for 
DDOS on the apache server, so we need to identified the ip from reverse 
proxy for DDOS work.

regards.


El 28/8/19 a las 08:40, Matus UHLAR - fantomas escribi?:
> On 28.08.19 07:59, jmperrote wrote:
>> Hello we have a reverse proxy squid and on the backend a apache 
>> server with anti DDOS software.
>>
>> Any request on the apache comming from the same ip of the reverse 
>> proxy because it is forwader to the apache backend.
>>
>> We need that the apache server receip the original ip from client.
>>
>> We try? the "forwarder_for on" directive at the top of squid.conf but 
>> not result.
>
> you must configure apache to accept that IP as the original IP.
> I think it has mod_remoteip or something like that.
>


From uhlar at fantomas.sk  Wed Aug 28 12:31:42 2019
From: uhlar at fantomas.sk (Matus UHLAR - fantomas)
Date: Wed, 28 Aug 2019 14:31:42 +0200
Subject: [squid-users] Help with IP forwarder on squid
In-Reply-To: <c6f7efb6-8ce0-75b7-d063-55db14547fec@policia.rionegro.gov.ar>
References: <5ccdc292-84ed-aee7-cb7f-9decaf248fcf@policia.rionegro.gov.ar>
 <20190828114019.GB5374@fantomas.sk>
 <c6f7efb6-8ce0-75b7-d063-55db14547fec@policia.rionegro.gov.ar>
Message-ID: <20190828123142.GA6757@fantomas.sk>

On 28.08.19 09:22, jmperrote wrote:
>Hello Matus thanks for the answer, but on the apache backend server we 
>just receip request from the reverse proxy, and we mounted software 
>for DDOS on the apache server, so we need to identified the ip from 
>reverse proxy for DDOS work.

and this is eaxctly why I said you must configure apache to accept
X-Forwarder-For header from squid, so apache knows which real IP connects
from the outside.  And I said the proper module is mod_remoteip or something
similar.

However, yout anti-ddos software should connect to squid, not to apache. Or,
your squid server might be useless in that setup.

>El 28/8/19 a las 08:40, Matus UHLAR - fantomas escribi?:
>>On 28.08.19 07:59, jmperrote wrote:
>>>Hello we have a reverse proxy squid and on the backend a apache 
>>>server with anti DDOS software.
>>>
>>>Any request on the apache comming from the same ip of the reverse 
>>>proxy because it is forwader to the apache backend.
>>>
>>>We need that the apache server receip the original ip from client.
>>>
>>>We try? the "forwarder_for on" directive at the top of squid.conf 
>>>but not result.
>>
>>you must configure apache to accept that IP as the original IP.
>>I think it has mod_remoteip or something like that.

-- 
Matus UHLAR - fantomas, uhlar at fantomas.sk ; http://www.fantomas.sk/
Warning: I wish NOT to receive e-mail advertising to this address.
Varovanie: na tuto adresu chcem NEDOSTAVAT akukolvek reklamnu postu.
"One World. One Web. One Program." - Microsoft promotional advertisement
"Ein Volk, ein Reich, ein Fuhrer!" - Adolf Hitler


From ilari.laitinen at iki.fi  Fri Aug 30 12:16:49 2019
From: ilari.laitinen at iki.fi (Ilari Laitinen)
Date: Fri, 30 Aug 2019 15:16:49 +0300
Subject: [squid-users] Linearly increasing delays in HTTPS proxy
 CONNECTS / 3.5.20
In-Reply-To: <d5759d03-ead0-c441-029f-288955ec4cd9@measurement-factory.com>
References: <592B861A-9199-4497-8BFF-9F253F62AFBA@iki.fi>
 <d5759d03-ead0-c441-029f-288955ec4cd9@measurement-factory.com>
Message-ID: <E8E8363F-0CED-47EC-BFB1-5C5E791C5B64@iki.fi>

>> I noticed small but consistent spikes in squid's disk cache usage
>> coinciding with the issue at hand. This seemed strange, given there
>> was no other traffic during the tests and proxied HTTPS means there's
>> nothing to cache (right?).
> 
> Correct. To avoid suspecting disks, configure Squid to log to a
> RAM-based partition and remove cache_dirs [until you solve the problem].

The behaviour persisted with this configuration. Not a disk bottleneck, then.

>> The servers are located in a IPv4-only local network. Every outgoing
>> request is supposed to be IPv4. The servers do have IPv6 interfaces
>> but there is no traffic at all. Squid periodically queries AAAA
>> records. Is it possible that new connections get queued while squid
>> is busy trying to use IPv6 after receiving the new AAAAs?
> 
> If "no traffic at all" means "zero IPv6 packets", then it is not
> possible. Otherwise, it is possible (only the latest Squid (i.e. future
> v5) does not have this kind of problem).
> 
>> I have very little control over the environment. Is dns_v4_first
>> worth a try in my scenario?
> 
> It is not a reliable solution, but it would not hurt as far as
> performance is concerned.

Tried dns_v4_first, the problem persisted. I also noticed that the platform (in general) tries to resolve ipv6 first, but the TCP dumps have no ipv6 packages at all. This is baffling, because there were indeed some unrelated open ipv6 connections on the Squid server (reported by netstat).

>> What should I look into next?
> 
> 1. Check system logs.

Nothing out of the ordinary.

> 2. Check atop output while the problem is present. If this is a resource
> bottleneck, atop may expose it.

No bottlenecks identified this way.

> 3. If there is IPv6 traffic, to eliminate IPv6 as a suspect, you can
> disable IPv6 on the box, use Squid built without IPv6 support, or even
> use a DNS forwarder that, for example, rejects all AAAA queries. All
> these solutions can and should be validated by examining actual IPv6
> traffic. And none of them are needed if there is no IPv6 traffic at all.

This is something I may need to look into further.

> 4. With delays ranging into _seconds_ it should be fairly easy for a
> capable Squid developer to figure out what your Squid is doing by
> looking at Squid debugging logs. You can post a link to compressed
> cache.log here for analysis, but you should first simplify your workload
> so that it has CONNECT tunnels and nothing else (if you have not
> already) and enable debugging when the problem is present (e.g., use
> "squid -k debug" although it is currently better to send the right
> signal manually).

I unfortunately cannot share the debug log because it contains some sensitive information. We nevertheless recorded what ended up being a huge sample.

I suspect Squid might be waiting for local TCP ports from the kernel (or something related). Why am I thinking this?

Right now, there are four different IP addresses returned for the target cloud service. For practical purposes, they are returned in a random order. The traffic would ideally be spread over all of them. Unfortunately it is evident both from the debug log and from the TCP dump that Squid is using only one of the addresses at a time. The amount of connections in the TIME_WAIT state for that single IP address gets very close to the maximum defined by the net.ipv4.ip_local_port_range sysctl. After a while (a minute or so in the recording) this address changes presumably in response to a new DNS query result.

Could this be the bottleneck? Is there a way to configure Squid to use all returned IP addresses?

One possible workaround that I can think of is setting a short positive_dns_ttl, but this doesn?t fully guarantee an even distribution, now does it?

>> Could setting up "workers N? help, for example?
> 
> The answer depends on your definition of "help": Large number of workers
> may mask the problem to the point where it no longer bothers you, but I
> would not make the setup a lot more complex until you know where the
> current bottleneck is. In fact, I would go into the opposite direction
> of making the setup as simple as possible!

Thanks, this is what I wanted to hear. :)


Best,

-- 
Ilari Laitinen



From rousskov at measurement-factory.com  Fri Aug 30 13:40:11 2019
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Fri, 30 Aug 2019 09:40:11 -0400
Subject: [squid-users] Linearly increasing delays in HTTPS proxy
 CONNECTS / 3.5.20
In-Reply-To: <E8E8363F-0CED-47EC-BFB1-5C5E791C5B64@iki.fi>
References: <592B861A-9199-4497-8BFF-9F253F62AFBA@iki.fi>
 <d5759d03-ead0-c441-029f-288955ec4cd9@measurement-factory.com>
 <E8E8363F-0CED-47EC-BFB1-5C5E791C5B64@iki.fi>
Message-ID: <05ec929a-8e17-1b8a-c95d-02fc299623f2@measurement-factory.com>

On 8/30/19 8:16 AM, Ilari Laitinen wrote:

> I also noticed that the platform (in general) tries to resolve ipv6
> first, but the TCP dumps have no ipv6 packages at all. This is
> baffling, because there were indeed some unrelated open ipv6
> connections on the Squid server (reported by netstat).

You may be able to validate your packet collection rules by adjusting
them to include those known IPv6 connections/ports. Perhaps you are just
not collecting IPv6 traffic.

It is also possible that Squid gets AAAA records but never uses them
because Squid thinks that IPv6 is disabled on your server.


> I unfortunately cannot share the debug log because it contains some
> sensitive information. We nevertheless recorded what ended up being a
> huge sample.

If you hire a Squid developer to help you, they should be willing to
sign a reasonable NDA and/or view data on your servers, without copying.
IMHO, it does not make much sense to sit on a likely valuable direct
information while, at the same time, spending a lot of time to find
distant echoes of that same information elsewhere!


> I suspect Squid might be waiting for local TCP ports from the kernel
> (or something related).

IIRC, ephemeral source port allocator is instantaneous -- Squid either
gets a port or a port allocation error, without waiting. When we
overload the server with high-performance tests (without an explicit
port manager), we see port allocation errors rather than stalled tests.
However, perhaps that is not true in your OS/environment.


> Right now, there are four different IP addresses returned for the
> target cloud service. For practical purposes, they are returned in a
> random order. The traffic would ideally be spread over all of them.
> Unfortunately it is evident both from the debug log and from the TCP
> dump that Squid is using only one of the addresses at a time. The
> amount of connections in the TIME_WAIT state for that single IP
> address gets very close to the maximum defined by the
> net.ipv4.ip_local_port_range sysctl. After a while (a minute or so in
> the recording) this address changes presumably in response to a new
> DNS query result.

In theory, Squid should round-robin across all destination IP addresses
for a single host name. If your Squid v3 does not, it is probably a
Squid bug that can be fixed [by upgrading].

Said that, IIRC, the notion of "round robin" is rather vague in Squid
because there are several places where an IP may be requested for the
same host name inside the same transaction. I would not be surprised if
that low-level round-robin behavior results in the same IP being used
for most transactions in some environments (until an error or a new DNS
query reshuffles the IPs). Debugging logs may expose this problem.


> Could this be the bottleneck?

I would expect that the lack of ports would lead to errors, not stalled
transactions. However, there may be some hidden dependency that I am
missing. For example, lack of ports leads to errors, the errors are not
logged where you can see them, but lead to excessive DNS retries and/or
Squid bugs that lead to delays.


> One possible workaround that I can think of is setting a short
> positive_dns_ttl, but this doesn?t fully guarantee an even
> distribution, now does it?

No, it does not. Moreover, Squid v3 had some TTL handling bugs that were
fixed (in v4 and later code) by the Happy Eyeballs project. Taking all
the known problems into the account, it is difficult for me to predict
the effect of changing TTLs. Said that, it does not hurt to try! Maybe
you will be lucky, and a simple configuration change will remove the
cause of increasing transaction delays.


HTH,

Alex.


From creditu at eml.cc  Fri Aug 30 15:44:56 2019
From: creditu at eml.cc (creditu at eml.cc)
Date: Fri, 30 Aug 2019 09:44:56 -0600
Subject: [squid-users] Advice on Cache Peer ACLs
Message-ID: <e9709e86-2719-4d0a-b163-1c9966a09263@www.fastmail.com>

We use several squid servers in accelerator mode for load balancing to send public requests to backend servers.   The squids don't do any caching, they just forward requests to the backend. 

We have cache_peer directives to send the incoming requests to the backend Apache servers.  What I need to do is send requests to a certain page to a specific backend server and all others to the  other backends.  The site has many pages, subpages etc.  

What I want to do is if someone requests:
https://www.example.com/anything/anything/script.php   or https://origin-www.example.com/anything/anything/etc/etc/script.php

Send the request to only .1, .2,.3.

If someone requests :
https://www.example.com/anything/tst/map2/script.php   or https://origin-www.example.com/anything/anything/tst/map1/etc/script.php

Send that request only to .4 and .5.

It seems to work most of the time, but tailing the access logs on the servers I sometimes see one of the requests for ../tst/map2/... or map1 show up on .1,.2, or .3.  

Is there something I'm missing?

Here is what I have so far.

acl all_requests dstdomain -n www.example.com origin-www.example.com
acl limited  url_regex -i /tst/map1|/tst/map2


cache_peer 192.168.1.1 parent 80 0 no-query no-digest connect-fail-limit=10 weight=1 originserver round-robin
cache_peer_access 192.168.1.1 deny limited
cache_peer_access 192.168.1.1 allow all_requests
cache_peer_access 192.168.1.1 deny all

cache_peer 192.168.1.2 parent 80 0 no-query no-digest connect-fail-limit=10 weight=1 originserver round-robin
cache_peer_access 192.168.1.2 deny limited
cache_peer_access 192.168.1.2 allow all_requests
cache_peer_access 192.168.1.2 deny all

cache_peer 192.168.1.3 parent 80 0 no-query no-digest connect-fail-limit=10 weight=1 originserver round-robin
cache_peer_access 192.168.1.3 deny limited
cache_peer_access 192.168.1.3 allow all_requests
cache_peer_access 192.168.1.3 deny all

cache_peer 192.168.1.4 parent 80 0 no-query no-digest connect-fail-limit=10 weight=1 originserver round-robin
cache_peer_access 192.168.1.4 allow limited
cache_peer_access 192.168.1.4 deny all

cache_peer 192.168.1.5 parent 80 0 no-query no-digest connect-fail-limit=10 weight=1 originserver round-robin
cache_peer_access 192.168.1.5 allow limited
cache_peer_access 192.168.1.5 deny all



From rousskov at measurement-factory.com  Fri Aug 30 17:41:53 2019
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Fri, 30 Aug 2019 13:41:53 -0400
Subject: [squid-users] Advice on Cache Peer ACLs
In-Reply-To: <e9709e86-2719-4d0a-b163-1c9966a09263@www.fastmail.com>
References: <e9709e86-2719-4d0a-b163-1c9966a09263@www.fastmail.com>
Message-ID: <a6e88fdd-229e-6e2e-f13b-34b846f1928f@measurement-factory.com>

On 8/30/19 11:44 AM, creditu at eml.cc wrote:
> We use several squid servers in accelerator mode for load balancing to send public requests to backend servers.   The squids don't do any caching, they just forward requests to the backend. 
> 
> We have cache_peer directives to send the incoming requests to the backend Apache servers.  What I need to do is send requests to a certain page to a specific backend server and all others to the  other backends.  The site has many pages, subpages etc.  
> 
> What I want to do is if someone requests:
> https://www.example.com/anything/anything/script.php   or https://origin-www.example.com/anything/anything/etc/etc/script.php
> 
> Send the request to only .1, .2,.3.
> 
> If someone requests :
> https://www.example.com/anything/tst/map2/script.php   or https://origin-www.example.com/anything/anything/tst/map1/etc/script.php
> 
> Send that request only to .4 and .5.
> 
> It seems to work most of the time, but tailing the access logs on the servers I sometimes see one of the requests for ../tst/map2/... or map1 show up on .1,.2, or .3.  


Do Squid access logs have the corresponding records as well? What cache
peer selection algorithm does Squid record for those misdirected
transactions?


> Is there something I'm missing?

Could Squid go direct to one of those origin servers (e.g., when all
eligible cache peers were down)?

BTW, please note that your cache_peer_access rules look inconsistent:
Your cache_peer_access .1-3 rules require certain domain names but .4-5
rules do not. This does not explain the discrepancy you are describing
above, but you may want to adjust your rules for consistency sake
(either to ignore dstdomain completely or to require correct domains for
all cache peers).


HTH,

Alex.


> acl all_requests dstdomain -n www.example.com origin-www.example.com
> acl limited  url_regex -i /tst/map1|/tst/map2
> 
> 
> cache_peer 192.168.1.1 parent 80 0 no-query no-digest connect-fail-limit=10 weight=1 originserver round-robin
> cache_peer_access 192.168.1.1 deny limited
> cache_peer_access 192.168.1.1 allow all_requests
> cache_peer_access 192.168.1.1 deny all
> 
> cache_peer 192.168.1.2 parent 80 0 no-query no-digest connect-fail-limit=10 weight=1 originserver round-robin
> cache_peer_access 192.168.1.2 deny limited
> cache_peer_access 192.168.1.2 allow all_requests
> cache_peer_access 192.168.1.2 deny all
> 
> cache_peer 192.168.1.3 parent 80 0 no-query no-digest connect-fail-limit=10 weight=1 originserver round-robin
> cache_peer_access 192.168.1.3 deny limited
> cache_peer_access 192.168.1.3 allow all_requests
> cache_peer_access 192.168.1.3 deny all
> 
> cache_peer 192.168.1.4 parent 80 0 no-query no-digest connect-fail-limit=10 weight=1 originserver round-robin
> cache_peer_access 192.168.1.4 allow limited
> cache_peer_access 192.168.1.4 deny all
> 
> cache_peer 192.168.1.5 parent 80 0 no-query no-digest connect-fail-limit=10 weight=1 originserver round-robin
> cache_peer_access 192.168.1.5 allow limited
> cache_peer_access 192.168.1.5 deny all


From squid at buglecreek.com  Fri Aug 30 18:41:10 2019
From: squid at buglecreek.com (squid at buglecreek.com)
Date: Fri, 30 Aug 2019 12:41:10 -0600
Subject: [squid-users] Advice on Cache Peer ACLs
In-Reply-To: <a6e88fdd-229e-6e2e-f13b-34b846f1928f@measurement-factory.com>
References: <e9709e86-2719-4d0a-b163-1c9966a09263@www.fastmail.com>
 <a6e88fdd-229e-6e2e-f13b-34b846f1928f@measurement-factory.com>
Message-ID: <20412edf-620b-465d-845e-746878fa0eac@www.fastmail.com>

Thank you for the reply.  It appears that I had a IP address typo in one of the cache_peer lines that allowed the requests with /tst/map1 or map2 to slip bye.  It appears to be working.  I think you confirmed what I'm trying to do should work. 

One question about your last statement concerning inconsistent  domain names.  All requests will always start with www.example.com /.... or origin-www.example.com/.....  even the ones what I'm trying to send to specific backends using the "limited" acl. 

Are you saying I should have the following for .4 and .5 instead of what I'm currently using?  

 cache_peer 192.168.1.5 parent 80 0 no-query no-digest connect-fail-limit=10 weight=1 originserver round-robin
 cache_peer_access 192.168.1.5 allow limited
cache_peer_access 192.168.1.5 allow all_requests
 cache_peer_access 192.168.1.5 deny all

I was trying to limit the requests to .4 and .5 to only those that contained /tst/map1 or map2.  I thought if I included the "allow all_requests" line in .4 and .5 it would send requests that also did not include /tst/map2 or map2.  For example "origin-www.example.com/hello/test/etc"  could possibly be sent to .4 and .5 as well.  

How do I ensure that www.example.com/tst/map1/..... and map2 only go to .4 and .5 while still correctly being consistent with the domain was you suggested.  Thanks.  

On Fri, Aug 30, 2019, at 11:41 AM, Alex Rousskov wrote:
> On 8/30/19 11:44 AM, creditu at eml.cc wrote:
> > We use several squid servers in accelerator mode for load balancing to send public requests to backend servers.   The squids don't do any caching, they just forward requests to the backend. 
> > 
> > We have cache_peer directives to send the incoming requests to the backend Apache servers.  What I need to do is send requests to a certain page to a specific backend server and all others to the  other backends.  The site has many pages, subpages etc.  
> > 
> > What I want to do is if someone requests:
> > https://www.example.com/anything/anything/script.php   or https://origin-www.example.com/anything/anything/etc/etc/script.php
> > 
> > Send the request to only .1, .2,.3.
> > 
> > If someone requests :
> > https://www.example.com/anything/tst/map2/script.php   or https://origin-www.example.com/anything/anything/tst/map1/etc/script.php
> > 
> > Send that request only to .4 and .5.
> > 
> > It seems to work most of the time, but tailing the access logs on the servers I sometimes see one of the requests for ../tst/map2/... or map1 show up on .1,.2, or .3.  
> 
> 
> Do Squid access logs have the corresponding records as well? What cache
> peer selection algorithm does Squid record for those misdirected
> transactions?
> 
> 
> > Is there something I'm missing?
> 
> Could Squid go direct to one of those origin servers (e.g., when all
> eligible cache peers were down)?
> 
> BTW, please note that your cache_peer_access rules look inconsistent:
> Your cache_peer_access .1-3 rules require certain domain names but .4-5
> rules do not. This does not explain the discrepancy you are describing
> above, but you may want to adjust your rules for consistency sake
> (either to ignore dstdomain completely or to require correct domains for
> all cache peers).
> 
> 
> HTH,
> 
> Alex.
> 
> 
> > acl all_requests dstdomain -n www.example.com origin-www.example.com
> > acl limited  url_regex -i /tst/map1|/tst/map2
> > 
> > 
> > cache_peer 192.168.1.1 parent 80 0 no-query no-digest connect-fail-limit=10 weight=1 originserver round-robin
> > cache_peer_access 192.168.1.1 deny limited
> > cache_peer_access 192.168.1.1 allow all_requests
> > cache_peer_access 192.168.1.1 deny all
> > 
> > cache_peer 192.168.1.2 parent 80 0 no-query no-digest connect-fail-limit=10 weight=1 originserver round-robin
> > cache_peer_access 192.168.1.2 deny limited
> > cache_peer_access 192.168.1.2 allow all_requests
> > cache_peer_access 192.168.1.2 deny all
> > 
> > cache_peer 192.168.1.3 parent 80 0 no-query no-digest connect-fail-limit=10 weight=1 originserver round-robin
> > cache_peer_access 192.168.1.3 deny limited
> > cache_peer_access 192.168.1.3 allow all_requests
> > cache_peer_access 192.168.1.3 deny all
> > 
> > cache_peer 192.168.1.4 parent 80 0 no-query no-digest connect-fail-limit=10 weight=1 originserver round-robin
> > cache_peer_access 192.168.1.4 allow limited
> > cache_peer_access 192.168.1.4 deny all
> > 
> > cache_peer 192.168.1.5 parent 80 0 no-query no-digest connect-fail-limit=10 weight=1 originserver round-robin
> > cache_peer_access 192.168.1.5 allow limited
> > cache_peer_access 192.168.1.5 deny all
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>


From rousskov at measurement-factory.com  Fri Aug 30 20:10:06 2019
From: rousskov at measurement-factory.com (Alex Rousskov)
Date: Fri, 30 Aug 2019 16:10:06 -0400
Subject: [squid-users] Advice on Cache Peer ACLs
In-Reply-To: <20412edf-620b-465d-845e-746878fa0eac@www.fastmail.com>
References: <e9709e86-2719-4d0a-b163-1c9966a09263@www.fastmail.com>
 <a6e88fdd-229e-6e2e-f13b-34b846f1928f@measurement-factory.com>
 <20412edf-620b-465d-845e-746878fa0eac@www.fastmail.com>
Message-ID: <451d2687-72c5-b610-b092-c551eab5258f@measurement-factory.com>

On 8/30/19 2:41 PM, squid at buglecreek.com wrote:

> All requests will always start with www.example.com /.... or origin-www.example.com/

If that is true, why check domain names at all? When you write an ACL
that checks for X, it is reasonable to assume that X may not happen. My
evaluation of your rules made that assumption.

> Are you saying I should have the following for .4 and .5 instead of what I'm currently using?  
> 
>  cache_peer 192.168.1.5 parent 80 0 no-query no-digest connect-fail-limit=10 weight=1 originserver round-robin
>  cache_peer_access 192.168.1.5 allow limited
>  cache_peer_access 192.168.1.5 allow all_requests
>  cache_peer_access 192.168.1.5 deny all

No, the above does not match what you want to achieve AFAICT. If domain
names matter, then you should have something like this:

>  cache_peer_access 192.168.1.5 allow limited all_requests
>  cache_peer_access 192.168.1.5 deny all


Needless to say, the name "all_requests" is very misleading, pointing to
the same inconsistency/problem we are discussing. I will use "myDomains"
below but you may find a better name based on your business logic.

The "limited" name also looks like a poor choice because all ACLs
(except "all") limit matching. I will use mapOneOrTwo below instead.

If you fix the names, the rules become simpler/readable. For example:

cache_peer_access 192.168.1.1 deny mapOneOrTwo
cache_peer_access 192.168.1.1 allow myDomains
cache_peer_access 192.168.1.1 deny all

...

cache_peer_access 192.168.1.5 allow mapOneOrTwo myDomains
cache_peer_access 192.168.1.5 deny all


You can achieve even better clarity if you use negation, but I usually
recommend against negating ACLs:

cache_peer_access 192.168.1.1 allow !mapOneOrTwo myDomains
cache_peer_access 192.168.1.1 deny all

...

cache_peer_access 192.168.1.5 allow mapOneOrTwo myDomains
cache_peer_access 192.168.1.5 deny all



Finally, if domain names do _not_ matter, then you will just have:

cache_peer_access 192.168.1.1 deny mapOneOrTwo
cache_peer_access 192.168.1.1 allow all

...

cache_peer_access 192.168.1.5 allow mapOneOrTwo
cache_peer_access 192.168.1.5 deny all


HTH,

Alex.
P.S. If possible, mapOneOrTwo should be replaced with something more
meaningful according to your business logic.


> On Fri, Aug 30, 2019, at 11:41 AM, Alex Rousskov wrote:
>> On 8/30/19 11:44 AM, creditu at eml.cc wrote:
>>> We use several squid servers in accelerator mode for load balancing to send public requests to backend servers.   The squids don't do any caching, they just forward requests to the backend. 
>>>
>>> We have cache_peer directives to send the incoming requests to the backend Apache servers.  What I need to do is send requests to a certain page to a specific backend server and all others to the  other backends.  The site has many pages, subpages etc.  
>>>
>>> What I want to do is if someone requests:
>>> https://www.example.com/anything/anything/script.php   or https://origin-www.example.com/anything/anything/etc/etc/script.php
>>>
>>> Send the request to only .1, .2,.3.
>>>
>>> If someone requests :
>>> https://www.example.com/anything/tst/map2/script.php   or https://origin-www.example.com/anything/anything/tst/map1/etc/script.php
>>>
>>> Send that request only to .4 and .5.
>>>
>>> It seems to work most of the time, but tailing the access logs on the servers I sometimes see one of the requests for ../tst/map2/... or map1 show up on .1,.2, or .3.  
>>
>>
>> Do Squid access logs have the corresponding records as well? What cache
>> peer selection algorithm does Squid record for those misdirected
>> transactions?
>>
>>
>>> Is there something I'm missing?
>>
>> Could Squid go direct to one of those origin servers (e.g., when all
>> eligible cache peers were down)?
>>
>> BTW, please note that your cache_peer_access rules look inconsistent:
>> Your cache_peer_access .1-3 rules require certain domain names but .4-5
>> rules do not. This does not explain the discrepancy you are describing
>> above, but you may want to adjust your rules for consistency sake
>> (either to ignore dstdomain completely or to require correct domains for
>> all cache peers).
>>
>>
>> HTH,
>>
>> Alex.
>>
>>
>>> acl all_requests dstdomain -n www.example.com origin-www.example.com
>>> acl limited  url_regex -i /tst/map1|/tst/map2
>>>
>>>
>>> cache_peer 192.168.1.1 parent 80 0 no-query no-digest connect-fail-limit=10 weight=1 originserver round-robin
>>> cache_peer_access 192.168.1.1 deny limited
>>> cache_peer_access 192.168.1.1 allow all_requests
>>> cache_peer_access 192.168.1.1 deny all
>>>
>>> cache_peer 192.168.1.2 parent 80 0 no-query no-digest connect-fail-limit=10 weight=1 originserver round-robin
>>> cache_peer_access 192.168.1.2 deny limited
>>> cache_peer_access 192.168.1.2 allow all_requests
>>> cache_peer_access 192.168.1.2 deny all
>>>
>>> cache_peer 192.168.1.3 parent 80 0 no-query no-digest connect-fail-limit=10 weight=1 originserver round-robin
>>> cache_peer_access 192.168.1.3 deny limited
>>> cache_peer_access 192.168.1.3 allow all_requests
>>> cache_peer_access 192.168.1.3 deny all
>>>
>>> cache_peer 192.168.1.4 parent 80 0 no-query no-digest connect-fail-limit=10 weight=1 originserver round-robin
>>> cache_peer_access 192.168.1.4 allow limited
>>> cache_peer_access 192.168.1.4 deny all
>>>
>>> cache_peer 192.168.1.5 parent 80 0 no-query no-digest connect-fail-limit=10 weight=1 originserver round-robin
>>> cache_peer_access 192.168.1.5 allow limited
>>> cache_peer_access 192.168.1.5 deny all



From creditu at eml.cc  Fri Aug 30 20:26:36 2019
From: creditu at eml.cc (creditu at eml.cc)
Date: Fri, 30 Aug 2019 14:26:36 -0600
Subject: [squid-users] Advice on Cache Peer ACLs
In-Reply-To: <451d2687-72c5-b610-b092-c551eab5258f@measurement-factory.com>
References: <e9709e86-2719-4d0a-b163-1c9966a09263@www.fastmail.com>
 <a6e88fdd-229e-6e2e-f13b-34b846f1928f@measurement-factory.com>
 <20412edf-620b-465d-845e-746878fa0eac@www.fastmail.com>
 <451d2687-72c5-b610-b092-c551eab5258f@measurement-factory.com>
Message-ID: <2eb9fd6c-d63e-4b61-aefd-8760a29c624c@www.fastmail.com>

Thanks for the great explanation.  Much appreciated.  

On Fri, Aug 30, 2019, at 2:10 PM, Alex Rousskov wrote:
> On 8/30/19 2:41 PM, squid at buglecreek.com wrote:
> 
> > All requests will always start with www.example.com /.... or origin-www.example.com/
> 
> If that is true, why check domain names at all? When you write an ACL
> that checks for X, it is reasonable to assume that X may not happen. My
> evaluation of your rules made that assumption.
> 
> > Are you saying I should have the following for .4 and .5 instead of what I'm currently using?  
> > 
> >  cache_peer 192.168.1.5 parent 80 0 no-query no-digest connect-fail-limit=10 weight=1 originserver round-robin
> >  cache_peer_access 192.168.1.5 allow limited
> >  cache_peer_access 192.168.1.5 allow all_requests
> >  cache_peer_access 192.168.1.5 deny all
> 
> No, the above does not match what you want to achieve AFAICT. If domain
> names matter, then you should have something like this:
> 
> >  cache_peer_access 192.168.1.5 allow limited all_requests
> >  cache_peer_access 192.168.1.5 deny all
> 
> 
> Needless to say, the name "all_requests" is very misleading, pointing to
> the same inconsistency/problem we are discussing. I will use "myDomains"
> below but you may find a better name based on your business logic.
> 
> The "limited" name also looks like a poor choice because all ACLs
> (except "all") limit matching. I will use mapOneOrTwo below instead.
> 
> If you fix the names, the rules become simpler/readable. For example:
> 
> cache_peer_access 192.168.1.1 deny mapOneOrTwo
> cache_peer_access 192.168.1.1 allow myDomains
> cache_peer_access 192.168.1.1 deny all
> 
> ...
> 
> cache_peer_access 192.168.1.5 allow mapOneOrTwo myDomains
> cache_peer_access 192.168.1.5 deny all
> 
> 
> You can achieve even better clarity if you use negation, but I usually
> recommend against negating ACLs:
> 
> cache_peer_access 192.168.1.1 allow !mapOneOrTwo myDomains
> cache_peer_access 192.168.1.1 deny all
> 
> ...
> 
> cache_peer_access 192.168.1.5 allow mapOneOrTwo myDomains
> cache_peer_access 192.168.1.5 deny all
> 
> 
> 
> Finally, if domain names do _not_ matter, then you will just have:
> 
> cache_peer_access 192.168.1.1 deny mapOneOrTwo
> cache_peer_access 192.168.1.1 allow all
> 
> ...
> 
> cache_peer_access 192.168.1.5 allow mapOneOrTwo
> cache_peer_access 192.168.1.5 deny all
> 
> 
> HTH,
> 
> Alex.
> P.S. If possible, mapOneOrTwo should be replaced with something more
> meaningful according to your business logic.
> 
> 
> > On Fri, Aug 30, 2019, at 11:41 AM, Alex Rousskov wrote:
> >> On 8/30/19 11:44 AM, creditu at eml.cc wrote:
> >>> We use several squid servers in accelerator mode for load balancing to send public requests to backend servers.   The squids don't do any caching, they just forward requests to the backend. 
> >>>
> >>> We have cache_peer directives to send the incoming requests to the backend Apache servers.  What I need to do is send requests to a certain page to a specific backend server and all others to the  other backends.  The site has many pages, subpages etc.  
> >>>
> >>> What I want to do is if someone requests:
> >>> https://www.example.com/anything/anything/script.php   or https://origin-www.example.com/anything/anything/etc/etc/script.php
> >>>
> >>> Send the request to only .1, .2,.3.
> >>>
> >>> If someone requests :
> >>> https://www.example.com/anything/tst/map2/script.php   or https://origin-www.example.com/anything/anything/tst/map1/etc/script.php
> >>>
> >>> Send that request only to .4 and .5.
> >>>
> >>> It seems to work most of the time, but tailing the access logs on the servers I sometimes see one of the requests for ../tst/map2/... or map1 show up on .1,.2, or .3.  
> >>
> >>
> >> Do Squid access logs have the corresponding records as well? What cache
> >> peer selection algorithm does Squid record for those misdirected
> >> transactions?
> >>
> >>
> >>> Is there something I'm missing?
> >>
> >> Could Squid go direct to one of those origin servers (e.g., when all
> >> eligible cache peers were down)?
> >>
> >> BTW, please note that your cache_peer_access rules look inconsistent:
> >> Your cache_peer_access .1-3 rules require certain domain names but .4-5
> >> rules do not. This does not explain the discrepancy you are describing
> >> above, but you may want to adjust your rules for consistency sake
> >> (either to ignore dstdomain completely or to require correct domains for
> >> all cache peers).
> >>
> >>
> >> HTH,
> >>
> >> Alex.
> >>
> >>
> >>> acl all_requests dstdomain -n www.example.com origin-www.example.com
> >>> acl limited  url_regex -i /tst/map1|/tst/map2
> >>>
> >>>
> >>> cache_peer 192.168.1.1 parent 80 0 no-query no-digest connect-fail-limit=10 weight=1 originserver round-robin
> >>> cache_peer_access 192.168.1.1 deny limited
> >>> cache_peer_access 192.168.1.1 allow all_requests
> >>> cache_peer_access 192.168.1.1 deny all
> >>>
> >>> cache_peer 192.168.1.2 parent 80 0 no-query no-digest connect-fail-limit=10 weight=1 originserver round-robin
> >>> cache_peer_access 192.168.1.2 deny limited
> >>> cache_peer_access 192.168.1.2 allow all_requests
> >>> cache_peer_access 192.168.1.2 deny all
> >>>
> >>> cache_peer 192.168.1.3 parent 80 0 no-query no-digest connect-fail-limit=10 weight=1 originserver round-robin
> >>> cache_peer_access 192.168.1.3 deny limited
> >>> cache_peer_access 192.168.1.3 allow all_requests
> >>> cache_peer_access 192.168.1.3 deny all
> >>>
> >>> cache_peer 192.168.1.4 parent 80 0 no-query no-digest connect-fail-limit=10 weight=1 originserver round-robin
> >>> cache_peer_access 192.168.1.4 allow limited
> >>> cache_peer_access 192.168.1.4 deny all
> >>>
> >>> cache_peer 192.168.1.5 parent 80 0 no-query no-digest connect-fail-limit=10 weight=1 originserver round-robin
> >>> cache_peer_access 192.168.1.5 allow limited
> >>> cache_peer_access 192.168.1.5 deny all
> 
> _______________________________________________
> squid-users mailing list
> squid-users at lists.squid-cache.org
> http://lists.squid-cache.org/listinfo/squid-users
>


