<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<HTML>
 <HEAD>
   <TITLE> [squid-users] Linearly increasing delays in HTTPS proxy CONNECTS / 3.5.20
   </TITLE>
   <LINK REL="Index" HREF="index.html" >
   <LINK REL="made" HREF="mailto:squid-users%40lists.squid-cache.org?Subject=Re%3A%20%5Bsquid-users%5D%20Linearly%20increasing%20delays%20in%20HTTPS%20proxy%0A%20CONNECTS%20/%203.5.20&In-Reply-To=%3CE8E8363F-0CED-47EC-BFB1-5C5E791C5B64%40iki.fi%3E">
   <META NAME="robots" CONTENT="index,nofollow">
   <style type="text/css">
       pre {
           white-space: pre-wrap;       /* css-2.1, curent FF, Opera, Safari */
           }
   </style>
   <META http-equiv="Content-Type" content="text/html; charset=us-ascii">
   <LINK REL="Previous"  HREF="020910.html">
   <LINK REL="Next"  HREF="020926.html">
 </HEAD>
 <BODY BGCOLOR="#ffffff">
   <H1>[squid-users] Linearly increasing delays in HTTPS proxy CONNECTS / 3.5.20</H1>
    <B>Ilari Laitinen</B> 
    <A HREF="mailto:squid-users%40lists.squid-cache.org?Subject=Re%3A%20%5Bsquid-users%5D%20Linearly%20increasing%20delays%20in%20HTTPS%20proxy%0A%20CONNECTS%20/%203.5.20&In-Reply-To=%3CE8E8363F-0CED-47EC-BFB1-5C5E791C5B64%40iki.fi%3E"
       TITLE="[squid-users] Linearly increasing delays in HTTPS proxy CONNECTS / 3.5.20">ilari.laitinen at iki.fi
       </A><BR>
    <I>Fri Aug 30 12:16:49 UTC 2019</I>
    <P><UL>
        <LI>Previous message (by thread): <A HREF="020910.html">[squid-users] Linearly increasing delays in HTTPS proxy CONNECTS / 3.5.20
</A></li>
        <LI>Next message (by thread): <A HREF="020926.html">[squid-users] Linearly increasing delays in HTTPS proxy CONNECTS / 3.5.20
</A></li>
         <LI> <B>Messages sorted by:</B> 
              <a href="date.html#20925">[ date ]</a>
              <a href="thread.html#20925">[ thread ]</a>
              <a href="subject.html#20925">[ subject ]</a>
              <a href="author.html#20925">[ author ]</a>
         </LI>
       </UL>
    <HR>  
<!--beginarticle-->
<PRE>&gt;&gt;<i> I noticed small but consistent spikes in squid's disk cache usage
</I>&gt;&gt;<i> coinciding with the issue at hand. This seemed strange, given there
</I>&gt;&gt;<i> was no other traffic during the tests and proxied HTTPS means there's
</I>&gt;&gt;<i> nothing to cache (right?).
</I>&gt;<i> 
</I>&gt;<i> Correct. To avoid suspecting disks, configure Squid to log to a
</I>&gt;<i> RAM-based partition and remove cache_dirs [until you solve the problem].
</I>
The behaviour persisted with this configuration. Not a disk bottleneck, then.

&gt;&gt;<i> The servers are located in a IPv4-only local network. Every outgoing
</I>&gt;&gt;<i> request is supposed to be IPv4. The servers do have IPv6 interfaces
</I>&gt;&gt;<i> but there is no traffic at all. Squid periodically queries AAAA
</I>&gt;&gt;<i> records. Is it possible that new connections get queued while squid
</I>&gt;&gt;<i> is busy trying to use IPv6 after receiving the new AAAAs?
</I>&gt;<i> 
</I>&gt;<i> If &quot;no traffic at all&quot; means &quot;zero IPv6 packets&quot;, then it is not
</I>&gt;<i> possible. Otherwise, it is possible (only the latest Squid (i.e. future
</I>&gt;<i> v5) does not have this kind of problem).
</I>&gt;<i> 
</I>&gt;&gt;<i> I have very little control over the environment. Is dns_v4_first
</I>&gt;&gt;<i> worth a try in my scenario?
</I>&gt;<i> 
</I>&gt;<i> It is not a reliable solution, but it would not hurt as far as
</I>&gt;<i> performance is concerned.
</I>
Tried dns_v4_first, the problem persisted. I also noticed that the platform (in general) tries to resolve ipv6 first, but the TCP dumps have no ipv6 packages at all. This is baffling, because there were indeed some unrelated open ipv6 connections on the Squid server (reported by netstat).

&gt;&gt;<i> What should I look into next?
</I>&gt;<i> 
</I>&gt;<i> 1. Check system logs.
</I>
Nothing out of the ordinary.

&gt;<i> 2. Check atop output while the problem is present. If this is a resource
</I>&gt;<i> bottleneck, atop may expose it.
</I>
No bottlenecks identified this way.

&gt;<i> 3. If there is IPv6 traffic, to eliminate IPv6 as a suspect, you can
</I>&gt;<i> disable IPv6 on the box, use Squid built without IPv6 support, or even
</I>&gt;<i> use a DNS forwarder that, for example, rejects all AAAA queries. All
</I>&gt;<i> these solutions can and should be validated by examining actual IPv6
</I>&gt;<i> traffic. And none of them are needed if there is no IPv6 traffic at all.
</I>
This is something I may need to look into further.

&gt;<i> 4. With delays ranging into _seconds_ it should be fairly easy for a
</I>&gt;<i> capable Squid developer to figure out what your Squid is doing by
</I>&gt;<i> looking at Squid debugging logs. You can post a link to compressed
</I>&gt;<i> cache.log here for analysis, but you should first simplify your workload
</I>&gt;<i> so that it has CONNECT tunnels and nothing else (if you have not
</I>&gt;<i> already) and enable debugging when the problem is present (e.g., use
</I>&gt;<i> &quot;squid -k debug&quot; although it is currently better to send the right
</I>&gt;<i> signal manually).
</I>
I unfortunately cannot share the debug log because it contains some sensitive information. We nevertheless recorded what ended up being a huge sample.

I suspect Squid might be waiting for local TCP ports from the kernel (or something related). Why am I thinking this?

Right now, there are four different IP addresses returned for the target cloud service. For practical purposes, they are returned in a random order. The traffic would ideally be spread over all of them. Unfortunately it is evident both from the debug log and from the TCP dump that Squid is using only one of the addresses at a time. The amount of connections in the TIME_WAIT state for that single IP address gets very close to the maximum defined by the net.ipv4.ip_local_port_range sysctl. After a while (a minute or so in the recording) this address changes presumably in response to a new DNS query result.

Could this be the bottleneck? Is there a way to configure Squid to use all returned IP addresses?

One possible workaround that I can think of is setting a short positive_dns_ttl, but this doesn&#8217;t fully guarantee an even distribution, now does it?

&gt;&gt;<i> Could setting up &quot;workers N&#8221; help, for example?
</I>&gt;<i> 
</I>&gt;<i> The answer depends on your definition of &quot;help&quot;: Large number of workers
</I>&gt;<i> may mask the problem to the point where it no longer bothers you, but I
</I>&gt;<i> would not make the setup a lot more complex until you know where the
</I>&gt;<i> current bottleneck is. In fact, I would go into the opposite direction
</I>&gt;<i> of making the setup as simple as possible!
</I>
Thanks, this is what I wanted to hear. :)


Best,

-- 
Ilari Laitinen


</PRE>

<!--endarticle-->
    <HR>
    <P><UL>
        <!--threads-->
	<LI>Previous message (by thread): <A HREF="020910.html">[squid-users] Linearly increasing delays in HTTPS proxy CONNECTS / 3.5.20
</A></li>
	<LI>Next message (by thread): <A HREF="020926.html">[squid-users] Linearly increasing delays in HTTPS proxy CONNECTS / 3.5.20
</A></li>
         <LI> <B>Messages sorted by:</B> 
              <a href="date.html#20925">[ date ]</a>
              <a href="thread.html#20925">[ thread ]</a>
              <a href="subject.html#20925">[ subject ]</a>
              <a href="author.html#20925">[ author ]</a>
         </LI>
       </UL>

<hr>
<a href="https://lists.squid-cache.org/listinfo/squid-users">More information about the squid-users
mailing list</a><br>
</body></html>
